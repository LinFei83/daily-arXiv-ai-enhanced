{"id": "2508.10941", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10941", "abs": "https://arxiv.org/abs/2508.10941", "authors": ["Zhisen Hu", "David S. Johnson", "Aleksei Tiulpin", "Timothy F. Cootes", "Claudia Lindner"], "title": "The Role of Radiographic Knee Alignment in Knee Replacement Outcomes and Opportunities for Artificial Intelligence-Driven Assessment", "comment": null, "summary": "Prevalent knee osteoarthritis (OA) imposes substantial burden on health\nsystems with no cure available. Its ultimate treatment is total knee\nreplacement (TKR). Complications from surgery and recovery are difficult to\npredict in advance, and numerous factors may affect them. Radiographic knee\nalignment is one of the key factors that impacts TKR outcomes, affecting\noutcomes such as postoperative pain or function. Recently, artificial\nintelligence (AI) has been introduced to the automatic analysis of knee\nradiographs, for example, to automate knee alignment measurements. Existing\nreview articles tend to focus on knee OA diagnosis and segmentation of bones or\ncartilages in MRI rather than exploring knee alignment biomarkers for TKR\noutcomes and their assessment. In this review, we first examine the current\nscoring protocols for evaluating TKR outcomes and potential knee alignment\nbiomarkers associated with these outcomes. We then discuss existing AI-based\napproaches for generating knee alignment biomarkers from knee radiographs, and\nexplore future directions for knee alignment assessment and TKR outcome\nprediction.", "AI": {"tldr": "该综述探讨了膝关节骨性关节炎（OA）全膝关节置换术（TKR）中，利用人工智能（AI）从X光片自动生成膝关节对线生物标志物以预测术后结果的现状与未来方向。", "motivation": "膝关节OA目前无法治愈，TKR是最终治疗手段，但术后并发症难以预测。膝关节对线是影响TKR预后的关键因素。现有AI研究多关注OA诊断和骨骼分割，而非探索膝关节对线生物标志物对TKR预后的影响及其评估，因此需要一个专注于此的综述。", "method": "本文采用综述形式，首先审查评估TKR预后的现有评分协议及潜在的膝关节对线生物标志物，然后讨论现有基于AI的膝关节X光片对线生物标志物生成方法，并展望未来的评估方向和TKR预后预测。", "result": "作为一篇综述，本文系统梳理了TKR预后评估的当前协议、与预后相关的膝关节对线生物标志物，以及利用AI从X光片生成这些生物标志物的现有方法。它为该领域的研究现状提供了全面的概述。", "conclusion": "该综述强调了膝关节对线在TKR预后中的重要性，并指出了AI在自动化生成对线生物标志物以改善TKR预后预测方面的潜力，为未来的研究方向提供了指导。"}}
{"id": "2508.11010", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11010", "abs": "https://arxiv.org/abs/2508.11010", "authors": ["Tausifa Jan Saleem", "Mohammad Yaqub"], "title": "Deep Learning-Based Automated Segmentation of Uterine Myomas", "comment": null, "summary": "Uterine fibroids (myomas) are the most common benign tumors of the female\nreproductive system, particularly among women of childbearing age. With a\nprevalence exceeding 70%, they pose a significant burden on female reproductive\nhealth. Clinical symptoms such as abnormal uterine bleeding, infertility,\npelvic pain, and pressure-related discomfort play a crucial role in guiding\ntreatment decisions, which are largely influenced by the size, number, and\nanatomical location of the fibroids. Magnetic Resonance Imaging (MRI) is a\nnon-invasive and highly accurate imaging modality commonly used by clinicians\nfor the diagnosis of uterine fibroids. Segmenting uterine fibroids requires a\nprecise assessment of both the uterus and fibroids on MRI scans, including\nmeasurements of volume, shape, and spatial location. However, this process is\nlabor intensive and time consuming and subjected to variability due to intra-\nand inter-expert differences at both pre- and post-treatment stages. As a\nresult, there is a critical need for an accurate and automated segmentation\nmethod for uterine fibroids. In recent years, deep learning algorithms have\nshown re-markable improvements in medical image segmentation, outperforming\ntraditional methods. These approaches offer the potential for fully automated\nsegmentation. Several studies have explored the use of deep learning models to\nachieve automated segmentation of uterine fibroids. However, most of the\nprevious work has been conducted using private datasets, which poses challenges\nfor validation and comparison between studies. In this study, we leverage the\npublicly available Uterine Myoma MRI Dataset (UMD) to establish a baseline for\nautomated segmentation of uterine fibroids, enabling standardized evaluation\nand facilitating future research in this domain.", "AI": {"tldr": "该研究旨在利用公开的子宫肌瘤MRI数据集（UMD）建立子宫肌瘤自动分割的基线，以实现标准化评估并促进未来研究。", "motivation": "子宫肌瘤是常见的女性生殖系统良性肿瘤，其诊断和治疗决策依赖于MRI扫描中肌瘤的精确评估。然而，人工分割肌瘤耗时、劳动密集且存在专家间差异，因此急需准确、自动化的分割方法。尽管深度学习在此领域已显示出潜力，但大多数研究使用私有数据集，阻碍了验证和比较。", "method": "本研究利用公开可用的子宫肌瘤MRI数据集（UMD），应用深度学习算法来建立子宫肌瘤自动分割的基线。", "result": "本研究通过利用公开数据集，旨在为子宫肌瘤的自动分割提供一个标准化评估的基线，从而促进该领域的未来研究。", "conclusion": "通过在公开数据集上建立自动分割基线，本研究旨在推动子宫肌瘤分割方法的标准化评估，并为后续研究提供便利。"}}
{"id": "2508.11181", "categories": ["eess.IV", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11181", "abs": "https://arxiv.org/abs/2508.11181", "authors": ["Faisal Ahmed"], "title": "HistoViT: Vision Transformer for Accurate and Scalable Histopathological Cancer Diagnosis", "comment": "13 pages, 3 Figures", "summary": "Accurate and scalable cancer diagnosis remains a critical challenge in modern\npathology, particularly for malignancies such as breast, prostate, bone, and\ncervical, which exhibit complex histological variability. In this study, we\npropose a transformer-based deep learning framework for multi-class tumor\nclassification in histopathological images. Leveraging a fine-tuned Vision\nTransformer (ViT) architecture, our method addresses key limitations of\nconventional convolutional neural networks, offering improved performance,\nreduced preprocessing requirements, and enhanced scalability across tissue\ntypes. To adapt the model for histopathological cancer images, we implement a\nstreamlined preprocessing pipeline that converts tiled whole-slide images into\nPyTorch tensors and standardizes them through data normalization. This ensures\ncompatibility with the ViT architecture and enhances both convergence stability\nand overall classification performance. We evaluate our model on four benchmark\ndatasets: ICIAR2018 (breast), SICAPv2 (prostate), UT-Osteosarcoma (bone), and\nSipakMed (cervical) dataset -- demonstrating consistent outperformance over\nexisting deep learning methods. Our approach achieves classification accuracies\nof 99.32%, 96.92%, 95.28%, and 96.94% for breast, prostate, bone, and cervical\ncancers respectively, with area under the ROC curve (AUC) scores exceeding 99%\nacross all datasets. These results confirm the robustness, generalizability,\nand clinical potential of transformer-based architectures in digital pathology.\nOur work represents a significant advancement toward reliable, automated, and\ninterpretable cancer diagnosis systems that can alleviate diagnostic burdens\nand improve healthcare outcomes.", "AI": {"tldr": "本研究提出了一种基于Transformer的深度学习框架，用于组织病理图像的多类别肿瘤分类，并在四种癌症类型上取得了优于现有方法的分类性能。", "motivation": "准确和可扩展的癌症诊断是现代病理学面临的关键挑战，特别是对于乳腺癌、前列腺癌、骨癌和宫颈癌等具有复杂组织学变异的恶性肿瘤。", "method": "该研究提出了一种基于Transformer的深度学习框架，利用微调的Vision Transformer (ViT) 架构进行多类别肿瘤分类。方法包括一个简化的预处理流程，将全玻片图像转换为PyTorch张量并进行数据标准化，以确保与ViT架构的兼容性并提高收敛稳定性和分类性能。", "result": "该模型在ICIAR2018（乳腺）、SICAPv2（前列腺）、UT-Osteosarcoma（骨）和SipakMed（宫颈）四个基准数据集上进行了评估，对乳腺癌、前列腺癌、骨癌和宫颈癌分别取得了99.32%、96.92%、95.28%和96.94%的分类准确率，所有数据集的ROC曲线下面积（AUC）均超过99%，持续优于现有深度学习方法。", "conclusion": "研究结果证实了基于Transformer的架构在数字病理学中的鲁棒性、泛化能力和临床潜力，代表着在可靠、自动化和可解释的癌症诊断系统方面取得了重大进展。"}}
{"id": "2508.10976", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10976", "abs": "https://arxiv.org/abs/2508.10976", "authors": ["Martin Diller", "Sarah Alice Gaggl", "Philipp Hanisch", "Giuseppina Monterosso", "Fritz Rauschenbach"], "title": "Grounding Rule-Based Argumentation Using Datalog", "comment": null, "summary": "ASPIC+ is one of the main general frameworks for rule-based argumentation for\nAI. Although first-order rules are commonly used in ASPIC+ examples, most\nexisting approaches to reason over rule-based argumentation only support\npropositional rules. To enable reasoning over first-order instances, a\npreliminary grounding step is required. As groundings can lead to an\nexponential increase in the size of the input theories, intelligent procedures\nare needed. However, there is a lack of dedicated solutions for ASPIC+.\nTherefore, we propose an intelligent grounding procedure that keeps the size of\nthe grounding manageable while preserving the correctness of the reasoning\nprocess. To this end, we translate the first-order ASPIC+ instance into a\nDatalog program and query a Datalog engine to obtain ground substitutions to\nperform the grounding of rules and contraries. Additionally, we propose\nsimplifications specific to the ASPIC+ formalism to avoid grounding of rules\nthat have no influence on the reasoning process. Finally, we performed an\nempirical evaluation of a prototypical implementation to show scalability.", "AI": {"tldr": "针对ASPIC+框架中一阶规则推理缺乏智能实例化（grounding）解决方案的问题，本文提出了一种基于Datalog的智能实例化程序，旨在控制实例化的规模同时保证推理的正确性。", "motivation": "ASPIC+框架常使用一阶规则，但现有推理方法多仅支持命题规则，导致处理一阶实例时需进行初步实例化步骤。此步骤可能导致输入理论规模呈指数级增长，且目前缺乏针对ASPIC+的专用智能实例化方案。", "method": "将一阶ASPIC+实例转换为Datalog程序，并查询Datalog引擎以获取用于规则和反例实例化的基础替换。此外，提出了针对ASPIC+形式的简化方法，以避免对推理过程无影响的规则进行实例化。", "result": "所提出的智能实例化程序能够有效控制实例化规模，同时保持推理过程的正确性。原型实现的实证评估显示了良好的可扩展性。", "conclusion": "开发了一种有效且可扩展的智能实例化程序，解决了ASPIC+框架中一阶规则推理的实例化规模管理问题，并确保了推理的正确性。"}}
{"id": "2508.11211", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11211", "abs": "https://arxiv.org/abs/2508.11211", "authors": ["Zhenhao Li", "Long Yang", "Xiaojie Yin", "Haijun Yu", "Jiazhou Wang", "Hongbin Han", "Weigang Hu", "Yixing Huang"], "title": "Efficient Image-to-Image Schrödinger Bridge for CT Field of View Extension", "comment": "10 pages", "summary": "Computed tomography (CT) is a cornerstone imaging modality for non-invasive,\nhigh-resolution visualization of internal anatomical structures. However, when\nthe scanned object exceeds the scanner's field of view (FOV), projection data\nare truncated, resulting in incomplete reconstructions and pronounced artifacts\nnear FOV boundaries. Conventional reconstruction algorithms struggle to recover\naccurate anatomy from such data, limiting clinical reliability. Deep learning\napproaches have been explored for FOV extension, with diffusion generative\nmodels representing the latest advances in image synthesis. Yet, conventional\ndiffusion models are computationally demanding and slow at inference due to\ntheir iterative sampling process. To address these limitations, we propose an\nefficient CT FOV extension framework based on the image-to-image Schr\\\"odinger\nBridge (I$^2$SB) diffusion model. Unlike traditional diffusion models that\nsynthesize images from pure Gaussian noise, I$^2$SB learns a direct stochastic\nmapping between paired limited-FOV and extended-FOV images. This direct\ncorrespondence yields a more interpretable and traceable generative process,\nenhancing anatomical consistency and structural fidelity in reconstructions.\nI$^2$SB achieves superior quantitative performance, with root-mean-square error\n(RMSE) values of 49.8\\,HU on simulated noisy data and 152.0HU on real data,\noutperforming state-of-the-art diffusion models such as conditional denoising\ndiffusion probabilistic models (cDDPM) and patch-based diffusion methods.\nMoreover, its one-step inference enables reconstruction in just 0.19s per 2D\nslice, representing over a 700-fold speedup compared to cDDPM (135s) and\nsurpassing diffusionGAN (0.58s), the second fastest. This combination of\naccuracy and efficiency makes I$^2$SB highly suitable for real-time or clinical\ndeployment.", "AI": {"tldr": "本文提出了一种基于图像到图像Schrödinger桥（I$^2$SB）扩散模型的高效CT视野（FOV）扩展框架，解决了传统CT重建和现有扩散模型在处理截断数据时的准确性和效率问题。", "motivation": "当扫描对象超出CT扫描仪的视野范围时，投影数据会被截断，导致不完整的重建和FOV边界附近的明显伪影，限制了CT在临床上的可靠性。传统的重建算法难以从这类数据中恢复准确的解剖结构。虽然深度学习方法，特别是扩散生成模型，已被探索用于FOV扩展，但传统的扩散模型由于其迭代采样过程，计算量大且推理速度慢。", "method": "提出了一种基于图像到图像Schrödinger桥（I$^2$SB）扩散模型的CT FOV扩展框架。与从纯高斯噪声合成图像的传统扩散模型不同，I$^2$SB学习有限FOV图像与扩展FOV图像之间的直接随机映射。这种直接对应关系使得生成过程更具可解释性和可追溯性，从而增强了重建的解剖一致性和结构保真度。该模型支持一步推理。", "result": "I$^2$SB在定量性能上表现优越，在模拟噪声数据上的均方根误差（RMSE）为49.8 HU，在真实数据上为152.0 HU，优于条件去噪扩散概率模型（cDDPM）和基于补丁的扩散方法等最先进的扩散模型。此外，其一步推理使得每个2D切片的重建时间仅为0.19秒，比cDDPM（135秒）快700多倍，并超越了第二快的diffusionGAN（0.58秒）。", "conclusion": "I$^2$SB模型结合了高准确性和高效率，使其非常适合实时或临床部署，解决了CT FOV扩展中准确性与效率兼顾的挑战。"}}
{"id": "2508.11070", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11070", "abs": "https://arxiv.org/abs/2508.11070", "authors": ["Zahra Khotanlou", "Kate Larson", "Amir-Hossein Karimi"], "title": "From Individual to Multi-Agent Algorithmic Recourse: Minimizing the Welfare Gap via Capacitated Bipartite Matching", "comment": null, "summary": "Decision makers are increasingly relying on machine learning in sensitive\nsituations. In such settings, algorithmic recourse aims to provide individuals\nwith actionable and minimally costly steps to reverse unfavorable AI-driven\ndecisions. While existing research predominantly focuses on single-individual\n(i.e., seeker) and single-model (i.e., provider) scenarios, real-world\napplications often involve multiple interacting stakeholders. Optimizing\noutcomes for seekers under an individual welfare approach overlooks the\ninherently multi-agent nature of real-world systems, where individuals interact\nand compete for limited resources. To address this, we introduce a novel\nframework for multi-agent algorithmic recourse that accounts for multiple\nrecourse seekers and recourse providers. We model this many-to-many interaction\nas a capacitated weighted bipartite matching problem, where matches are guided\nby both recourse cost and provider capacity. Edge weights, reflecting recourse\ncosts, are optimized for social welfare while quantifying the welfare gap\nbetween individual welfare and this collectively feasible outcome. We propose a\nthree-layer optimization framework: (1) basic capacitated matching, (2) optimal\ncapacity redistribution to minimize the welfare gap, and (3) cost-aware\noptimization balancing welfare maximization with capacity adjustment costs.\nExperimental validation on synthetic and real-world datasets demonstrates that\nour framework enables the many-to-many algorithmic recourse to achieve\nnear-optimal welfare with minimum modification in system settings. This work\nextends algorithmic recourse from individual recommendations to system-level\ndesign, providing a tractable path toward higher social welfare while\nmaintaining individual actionability.", "AI": {"tldr": "该研究提出了一个多智能体算法追索框架，将多对多交互建模为带容量的加权二分匹配问题，旨在优化社会福利并量化个体福利与集体可行结果之间的差距。", "motivation": "现有算法追索研究主要关注单个人和单模型场景，忽略了现实世界中多智能体（追索寻求者和提供者）之间的互动、对有限资源的竞争以及超越个体福利的系统级优化需求。", "method": "该研究引入了多智能体算法追索框架，将多对多交互建模为带容量的加权二分匹配问题，其中匹配由追索成本和提供者容量指导。通过优化反映追索成本的边权重以实现社会福利最大化，并量化个体福利与集体可行结果之间的福利差距。提出了一个三层优化框架：1) 基本带容量匹配；2) 优化容量再分配以最小化福利差距；3) 成本感知优化以平衡福利最大化与容量调整成本。", "result": "在合成和真实世界数据集上的实验验证表明，该框架使多对多算法追索能够在系统设置中进行最小修改的情况下实现近乎最优的福利。这工作将算法追索从个体推荐扩展到系统级设计。", "conclusion": "该工作为实现更高的社会福利提供了可行的路径，同时保持了个体可操作性，将算法追索从个体推荐推向了系统级设计。"}}
{"id": "2508.11026", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.11026", "abs": "https://arxiv.org/abs/2508.11026", "authors": ["John W. Sheppard"], "title": "Overview of Complex System Design", "comment": "Appears as Chapter 1 in Realizing Complex Integrated Systems, Anthony\n  P. Ambler and John W. Sheppard (ads.), CRC Press, 2025", "summary": "This chapter serves as an introduction to systems engineering focused on the\nbroad issues surrounding realizing complex integrated systems. What is a\nsystem? We pose a number of possible definitions and perspectives, but leave\nopen the opportunity to consider the system from the target context where it\nwill be used. Once we have a system in mind, we acknowledge the fact that this\nsystem needs to integrate a variety of pieces, components, subsystems, in order\nfor it to accomplish its task. Therefore, we concern ourselves at the\nboundaries and interfaces of different technologies and disciplines to\ndetermine how best to achieve that integration. Next we raise the specter that\nthis integrated system is complex. Complexity can be defined in a number of\nways. For one, the sheer number of subsystems or components can be a measure of\ncomplexity. We could also consider the functions being performed by the system\nand how those functions interact with one another. Further, we could consider\ncomputational aspects such as the time or memory that may be needed to\naccomplish one or more tasks. The extent to which new behaviors might emerge\nfrom the system can also be regarded as an element of complexity. In the end,\ncomplexity is that characteristic of a system that defines the associated\nchallenges along the life of the system, so we are concerned with how to manage\nthat complexity. Finally, realization refers to the process by which our\ncomplex integrated system moves from concept to deployment and subsequent\nsupport. It refers to the entire design, development, manufacture, deployment,\noperation, and support life cycle. Of particular note here, however, is that we\nfocus on systems that, by their very nature, are complex. In other words, we\nare interested in large, complicated, interacting beasts that are intended to\nperform difficult tasks and meet a wide variety of end-user needs.", "AI": {"tldr": "本章介绍了系统工程，重点关注如何实现复杂的集成系统，包括系统的定义、集成挑战、复杂性管理以及系统实现的全生命周期。", "motivation": "实现复杂的集成系统面临诸多挑战，需要理解并管理其固有的复杂性，以确保系统能从概念阶段成功部署并提供支持。", "method": "文章首先探讨了系统的多种定义和视角，接着关注不同技术和学科间的边界与接口，以实现最佳集成。然后，从组件数量、功能交互、计算需求和涌现行为等多个维度定义了复杂性，并讨论了如何管理这种复杂性。最后，将“实现”定义为系统从概念到部署及后续支持的整个生命周期过程，包括设计、开发、制造、部署、操作和支持。", "result": "本章为理解复杂集成系统提供了基础框架，明确了系统、集成、复杂性和实现的关键概念。它强调了系统工程在管理大型、复杂、交互式系统全生命周期中的重要性。", "conclusion": "系统工程的核心在于管理复杂性，确保复杂的集成系统能成功地从概念转化为实际应用，并贯穿其整个设计、开发、制造、部署、操作和支持的生命周期。"}}
{"id": "2508.10973", "categories": ["cs.RO", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.10973", "abs": "https://arxiv.org/abs/2508.10973", "authors": ["Hongchen Wang", "Sima Zeinali Danalou", "Jiahao Zhu", "Kenneth Sulimro", "Chaewon Lim", "Smita Basak", "Aimee Tai", "Usan Siriwardana", "Jason Hattrick-Simpers", "Jay Werber"], "title": "Developing and Validating a High-Throughput Robotic System for the Accelerated Development of Porous Membranes", "comment": null, "summary": "The development of porous polymeric membranes remains a labor-intensive\nprocess, often requiring extensive trial and error to identify optimal\nfabrication parameters. In this study, we present a fully automated platform\nfor membrane fabrication and characterization via nonsolvent-induced phase\nseparation (NIPS). The system integrates automated solution preparation, blade\ncasting, controlled immersion, and compression testing, allowing precise\ncontrol over fabrication parameters such as polymer concentration and ambient\nhumidity. The modular design allows parallel processing and reproducible\nhandling of samples, reducing experimental time and increasing consistency.\nCompression testing is introduced as a sensitive mechanical characterization\nmethod for estimating membrane stiffness and as a proxy to infer porosity and\nintra-sample uniformity through automated analysis of stress-strain curves. As\na proof of concept to demonstrate the effectiveness of the system, NIPS was\ncarried out with polysulfone, the green solvent PolarClean, and water as the\npolymer, solvent, and nonsolvent, respectively. Experiments conducted with the\nautomated system reproduced expected effects of polymer concentration and\nambient humidity on membrane properties, namely increased stiffness and\nuniformity with increasing polymer concentration and humidity variations in\npore morphology and mechanical response. The developed automated platform\nsupports high-throughput experimentation and is well-suited for integration\ninto self-driving laboratory workflows, offering a scalable and reproducible\nfoundation for data-driven optimization of porous polymeric membranes through\nNIPS.", "AI": {"tldr": "开发了一个全自动平台，用于通过非溶剂诱导相分离（NIPS）法制造和表征多孔聚合物膜，实现高通量实验和数据驱动优化。", "motivation": "多孔聚合物膜的开发过程劳动密集，通常需要大量的试错才能找到最佳的制备参数。", "method": "本研究提出了一个全自动的NIPS膜制备和表征平台，集成了自动化溶液制备、刮刀浇铸、受控浸渍和压缩测试。该平台能精确控制聚合物浓度和环境湿度等参数。引入压缩测试作为一种敏感的机械表征方法，用于估算膜的刚度，并通过应力-应变曲线的自动分析推断孔隙率和样品内均匀性。以聚砜、绿色溶剂PolarClean和水作为概念验证。", "result": "自动化系统成功重现了聚合物浓度和环境湿度对膜性能的预期影响，即随着聚合物浓度增加，刚度和均匀性提高；湿度变化影响孔隙形态和机械响应。", "conclusion": "所开发的自动化平台支持高通量实验，非常适合集成到自驱动实验室工作流程中，为通过NIPS法对多孔聚合物膜进行数据驱动优化提供了可扩展和可重复的基础。"}}
{"id": "2508.10904", "categories": ["cs.CL", "cs.AR", "cs.PL"], "pdf": "https://arxiv.org/pdf/2508.10904", "abs": "https://arxiv.org/abs/2508.10904", "authors": ["Jie Lei", "Ruofan Jia", "J. Andrew Zhang", "Hao Zhang"], "title": "A2HCoder: An LLM-Driven Coding Agent for Hierarchical Algorithm-to-HDL Translation", "comment": "15 pages, 6 figures", "summary": "In wireless communication systems, stringent requirements such as ultra-low\nlatency and power consumption have significantly increased the demand for\nefficient algorithm-to-hardware deployment. However, a persistent and\nsubstantial gap remains between algorithm design and hardware implementation.\nBridging this gap traditionally requires extensive domain expertise and\ntime-consuming manual development, due to fundamental mismatches between\nhigh-level programming languages like MATLAB and hardware description languages\n(HDLs) such as Verilog-in terms of memory access patterns, data processing\nmanners, and datatype representations. To address this challenge, we propose\nA2HCoder: a Hierarchical Algorithm-to-HDL Coding Agent, powered by large\nlanguage models (LLMs), designed to enable agile and reliable\nalgorithm-to-hardware translation. A2HCoder introduces a hierarchical framework\nthat enhances both robustness and interpretability while suppressing common\nhallucination issues in LLM-generated code. In the horizontal dimension,\nA2HCoder decomposes complex algorithms into modular functional blocks,\nsimplifying code generation and improving consistency. In the vertical\ndimension, instead of relying on end-to-end generation, A2HCoder performs\nstep-by-step, fine-grained translation, leveraging external toolchains such as\nMATLAB and Vitis HLS for debugging and circuit-level synthesis. This structured\nprocess significantly mitigates hallucinations and ensures hardware-level\ncorrectness. We validate A2HCoder through a real-world deployment case in the\n5G wireless communication domain, demonstrating its practicality, reliability,\nand deployment efficiency.", "AI": {"tldr": "本文提出A2HCoder，一个基于大型语言模型（LLM）的分层算法到硬件描述语言（HDL）编码代理，旨在解决无线通信系统中算法设计与硬件实现之间的巨大鸿沟，通过模块化分解和细粒度翻译结合外部工具链，实现高效、可靠的算法到硬件部署。", "motivation": "无线通信系统对超低延迟和功耗的严格要求，增加了对高效算法到硬件部署的需求。然而，算法设计（如MATLAB）与硬件实现（如Verilog）之间存在巨大差距，主要由于内存访问、数据处理和数据类型表示的不匹配，传统上需要大量领域专业知识和耗时的手动开发。", "method": "提出A2HCoder：一个分层算法到HDL编码代理，由LLM驱动。它引入一个分层框架，增强鲁棒性和可解释性，同时抑制LLM生成的代码中常见的幻觉问题。在横向维度，A2HCoder将复杂算法分解为模块化功能块。在纵向维度，A2HCoder不依赖端到端生成，而是执行分步、细粒度的翻译，并利用MATLAB和Vitis HLS等外部工具链进行调试和电路级综合。", "result": "通过一个5G无线通信领域的真实世界部署案例验证了A2HCoder。结果表明，A2HCoder具有实用性、可靠性和部署效率。", "conclusion": "A2HCoder通过其独特的分层框架和与外部工具链的结合，成功弥合了无线通信中算法到硬件的差距，提供了一种敏捷且可靠的翻译解决方案，显著减轻了幻觉问题并确保了硬件级别的正确性。"}}
{"id": "2508.10918", "categories": ["cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.10918", "abs": "https://arxiv.org/abs/2508.10918", "authors": ["Samantha Aziz", "Oleg Komogortsev"], "title": "Privacy Enhancement for Gaze Data Using a Noise-Infused Autoencoder", "comment": "IJCB 2025; 11 pages, 7 figures", "summary": "We present a privacy-enhancing mechanism for gaze signals using a\nlatent-noise autoencoder that prevents users from being re-identified across\nplay sessions without their consent, while retaining the usability of the data\nfor benign tasks. We evaluate privacy-utility trade-offs across biometric\nidentification and gaze prediction tasks, showing that our approach\nsignificantly reduces biometric identifiability with minimal utility\ndegradation. Unlike prior methods in this direction, our framework retains\nphysiologically plausible gaze patterns suitable for downstream use, which\nproduces favorable privacy-utility trade-off. This work advances privacy in\ngaze-based systems by providing a usable and effective mechanism for protecting\nsensitive gaze data.", "AI": {"tldr": "提出一种基于潜在噪声自编码器的隐私增强机制，用于保护眼动信号，防止用户未经同意被再识别，同时保持数据可用性。", "motivation": "在不影响数据用于良性任务的前提下，防止用户在不同会话中未经同意被重新识别，以保护敏感的眼动数据隐私。", "method": "使用潜在噪声自编码器（latent-noise autoencoder）来处理眼动信号。", "result": "显著降低了生物识别可识别性，同时对数据可用性的影响最小；与现有方法不同，该框架能保留生理上合理的眼动模式，从而产生更有利的隐私-可用性权衡。", "conclusion": "为基于眼动的系统提供了一种可用且有效的敏感眼动数据保护机制，推动了眼动隐私领域的发展。"}}
{"id": "2508.11219", "categories": ["eess.IV", "math.OC"], "pdf": "https://arxiv.org/pdf/2508.11219", "abs": "https://arxiv.org/abs/2508.11219", "authors": ["Tao Hong", "Umberto Villa", "Jeffrey A. Fessler"], "title": "A Convergent Generalized Krylov Subspace Method for Compressed Sensing MRI Reconstruction with Gradient-Driven Denoisers", "comment": "13 pages, 8 figures, 2 tables", "summary": "Model-based reconstruction plays a key role in compressed sensing (CS) MRI,\nas it incorporates effective image regularizers to improve the quality of\nreconstruction. The Plug-and-Play and Regularization-by-Denoising frameworks\nleverage advanced denoisers (e.g., convolutional neural network (CNN)-based\ndenoisers) and have demonstrated strong empirical performance. However, their\ntheoretical guarantees remain limited, as practical CNNs often violate key\nassumptions. In contrast, gradient-driven denoisers achieve competitive\nperformance, and the required assumptions for theoretical analysis are easily\nsatisfied. However, solving the associated optimization problem remains\ncomputationally demanding. To address this challenge, we propose a generalized\nKrylov subspace method (GKSM) to solve the optimization problem efficiently.\nMoreover, we also establish rigorous convergence guarantees for GKSM in\nnonconvex settings. Numerical experiments on CS MRI reconstruction with spiral\nand radial acquisitions validate both the computational efficiency of GKSM and\nthe accuracy of the theoretical predictions. The proposed optimization method\nis applicable to any linear inverse problem.", "AI": {"tldr": "本文提出了一种广义Krylov子空间方法（GKSM），用于高效解决基于梯度驱动去噪器的压缩感知磁共振成像（CS MRI）重建中的优化问题，并提供了在非凸设置下的严格收敛性保证。", "motivation": "模型化重建在CS MRI中至关重要，但现有方法（如Plug-and-Play和Regularization-by-Denoising）依赖的CNN去噪器常违反理论假设，导致理论保证有限。梯度驱动去噪器虽然理论上更具优势，但其优化问题计算成本高昂。", "method": "提出了一种广义Krylov子空间方法（GKSM）来高效求解与梯度驱动去噪器相关的优化问题。此外，在非凸设置下建立了GKSM的严格收敛性保证。", "result": "在螺旋和径向采集的CS MRI重建数值实验中，验证了GKSM的计算效率及其理论预测的准确性。该优化方法适用于任何线性逆问题。", "conclusion": "GKSM有效解决了梯度驱动去噪器在CS MRI重建中的计算效率问题，并提供了坚实的理论基础，使其成为一种高效且具有理论保证的线性逆问题优化方法。"}}
{"id": "2508.11085", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11085", "abs": "https://arxiv.org/abs/2508.11085", "authors": ["Qingqing Wang", "Liqiang Xiao", "Chang Chang"], "title": "Learn to optimize for automatic proton PBS treatment planning for H&N cancers", "comment": "27 pages, 4 figures", "summary": "Proton PBS treatment planning for H&N cancers involves numerous conflicting\nobjectives, requiring significant effort from human planners to balance and\nsatisfy multiple clinical goals during planning. To achieve this,\nexperience-demanding objective parameter adjustment and computationally\nexpensive inverse optimization are performed iteratively. Extensive efforts\nhave been made to automatically adjust objective parameters, but the most\ntime-consuming component, i.e., inverse optimization, still relies heavily on\ntheory-driven approaches. We propose a data-driven inverse optimizer and\nintegrate it into a PPO-based automatic treatment planning framework to\nautomatically generate high-quality plans within a clinical acceptable planning\ntime. The inverse optimizer is a L2O method that predicts update steps by\nlearning from the task-specific data distribution. For the first time, we\nintegrate techniques designed for long-context processing, originally developed\nfor LLMs, into a Transformer-based L2O framework to address the scalability\nissue of existing L2O methods. The PPO framework functions as an outer-loop\nvirtual planner, autonomously adjusting objective parameters through a policy\nnetwork, and the dose predictor is used to initialize objective parameters. The\ninner-loop L2O inverse optimizer computes machine-deliverable MU values based\non objectives refined by the PPO policy network. 97 patients are collected in\nthis study, and compared with L-BFGSB, our L2O-based inverse optimizer improves\nthe effectiveness and efficiency by 22.97% and 36.41%, respectively. In\nconjunction with the PPO-based learned virtual planner, plans generated by our\nframework within an average of 2.55 hours show improved or comparable OAR\nsparing with superior target coverage for patients with different prescription\ndose levels, number of target volumes, beam angles, etc., compared with\nhuman-generated plans.", "AI": {"tldr": "本文提出了一种数据驱动的L2O逆向优化器，并将其集成到基于PPO的自动治疗计划框架中，用于质子PBS治疗计划。该方法显著提升了计划的效率和质量，超越了传统方法和人工计划。", "motivation": "质子PBS治疗计划涉及多重冲突目标，需要人工规划师耗费大量精力平衡并满足临床目标，且依赖经验性参数调整和计算昂贵的逆向优化迭代。尽管在自动调整目标参数方面有所进展，但最耗时的逆向优化仍严重依赖理论驱动方法。", "method": "1. 提出了一种数据驱动的L2O（Learn2Optimize）逆向优化器，通过学习任务特定数据分布来预测更新步长。2. 首次将LLM中用于长上下文处理的技术集成到基于Transformer的L2O框架中，以解决现有L2O方法的可扩展性问题。3. 将此逆向优化器集成到基于PPO的自动治疗计划框架中：PPO作为外层循环的虚拟规划器，通过策略网络自动调整目标参数；剂量预测器用于初始化目标参数；内层循环的L2O逆向优化器根据PPO策略网络优化的目标计算可交付的MU值。4. 在97名患者数据上进行了验证。", "result": "1. 与L-BFGSB相比，本文提出的L2O逆向优化器在有效性和效率上分别提高了22.97%和36.41%。2. 结合基于PPO的学习型虚拟规划器，该框架生成的计划平均耗时2.55小时。3. 与人工生成的计划相比，本文框架生成的计划在不同处方剂量水平、靶体积数量、射束角度等患者情况下，显示出更好的或相当的OAR（危及器官）保护，并具有更优越的靶区覆盖。", "conclusion": "所提出的数据驱动L2O逆向优化器与基于PPO的自动治疗计划框架的结合，能够高效自动生成高质量的质子PBS治疗计划，显著减少了规划时间和人工努力，并达到了或超越了临床标准。"}}
{"id": "2508.11031", "categories": ["eess.SY", "cs.AI", "cs.SY", "stat.AP"], "pdf": "https://arxiv.org/pdf/2508.11031", "abs": "https://arxiv.org/abs/2508.11031", "authors": ["John W. Sheppard"], "title": "Risk-Based Prognostics and Health Management", "comment": "Appears as Chapter 27 in Realizing Complex Integrated Systems,\n  Anthony P. Ambler and John W. Sheppard (ads.), CRC Press, 2025", "summary": "It is often the case that risk assessment and prognostics are viewed as\nrelated but separate tasks. This chapter describes a risk-based approach to\nprognostics that seeks to provide a tighter coupling between risk assessment\nand fault prediction. We show how this can be achieved using the\ncontinuous-time Bayesian network as the underlying modeling framework.\nFurthermore, we provide an overview of the techniques that are available to\nderive these models from data and show how they might be used in practice to\nachieve tasks like decision support and performance-based logistics. This work\nis intended to provide an overview of the recent developments related to\nrisk-based prognostics, and we hope that it will serve as a tutorial of sorts\nthat will assist others in adopting these techniques.", "AI": {"tldr": "本文介绍了一种基于风险的预测方法，通过使用连续时间贝叶斯网络，将风险评估与故障预测紧密结合，并概述了从数据中构建模型的技术及其在实践中的应用。", "motivation": "风险评估和预测通常被视为相关但独立的两项任务，本研究旨在提供一种方法，实现两者之间更紧密的耦合。", "method": "采用连续时间贝叶斯网络作为底层建模框架，并概述了从数据中推导这些模型的可用技术。", "result": "展示了如何利用连续时间贝叶斯网络实现风险评估与故障预测的紧密耦合，并阐述了这些模型在决策支持和基于性能的物流等实际任务中的应用。本文旨在提供风险预测领域最新进展的概述，并作为教程帮助他人采用这些技术。", "conclusion": "该工作为基于风险的预测提供了概述，旨在作为教程帮助其他研究者和实践者采纳和应用这些技术，从而更好地实现风险评估与故障预测的整合。"}}
{"id": "2508.10999", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.10999", "abs": "https://arxiv.org/abs/2508.10999", "authors": ["Yizhi Zhou", "Jie Xu", "Jiawei Xia", "Zechen Hu", "Weizi Li", "Xuan Wang"], "title": "Robust Online Calibration for UWB-Aided Visual-Inertial Navigation with Bias Correction", "comment": null, "summary": "This paper presents a novel robust online calibration framework for\nUltra-Wideband (UWB) anchors in UWB-aided Visual-Inertial Navigation Systems\n(VINS). Accurate anchor positioning, a process known as calibration, is crucial\nfor integrating UWB ranging measurements into state estimation. While several\nprior works have demonstrated satisfactory results by using robot-aided systems\nto autonomously calibrate UWB systems, there are still some limitations: 1)\nthese approaches assume accurate robot localization during the initialization\nstep, ignoring localization errors that can compromise calibration robustness,\nand 2) the calibration results are highly sensitive to the initial guess of the\nUWB anchors' positions, reducing the practical applicability of these methods\nin real-world scenarios. Our approach addresses these challenges by explicitly\nincorporating the impact of robot localization uncertainties into the\ncalibration process, ensuring robust initialization. To further enhance the\nrobustness of the calibration results against initialization errors, we propose\na tightly-coupled Schmidt Kalman Filter (SKF)-based online refinement method,\nmaking the system suitable for practical applications. Simulations and\nreal-world experiments validate the improved accuracy and robustness of our\napproach.", "AI": {"tldr": "本文提出了一个新颖的鲁棒在线校准框架，用于UWB辅助视觉惯性导航系统（VINS）中的UWB锚点，解决了现有方法在机器人定位不确定性和初始锚点位置敏感性方面的局限性。", "motivation": "现有UWB校准方法存在局限性：1) 在初始化阶段假设机器人定位准确，忽略了可能影响校准鲁棒性的定位误差；2) 校准结果对UWB锚点的初始位置猜测高度敏感，降低了实际应用性。", "method": "本方法通过将机器人定位不确定性的影响明确纳入校准过程，确保鲁棒初始化。为进一步增强校准结果对初始化错误的鲁棒性，提出了一种紧耦合的基于Schmidt Kalman Filter (SKF) 的在线精修方法。", "result": "仿真和实际实验验证了所提方法在精度和鲁棒性方面的提升。", "conclusion": "所提出的方法提供了一种更准确和鲁棒的UWB锚点校准方案，适用于实际的UWB辅助VINS应用。"}}
{"id": "2508.10906", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10906", "abs": "https://arxiv.org/abs/2508.10906", "authors": ["Sihan Chen", "John P. Lalor", "Yi Yang", "Ahmed Abbasi"], "title": "PersonaTwin: A Multi-Tier Prompt Conditioning Framework for Generating and Evaluating Personalized Digital Twins", "comment": "Presented at the Generation, Evaluation & Metrics (GEM) Workshop at\n  ACL 2025", "summary": "While large language models (LLMs) afford new possibilities for user modeling\nand approximation of human behaviors, they often fail to capture the\nmultidimensional nuances of individual users. In this work, we introduce\nPersonaTwin, a multi-tier prompt conditioning framework that builds adaptive\ndigital twins by integrating demographic, behavioral, and psychometric data.\nUsing a comprehensive data set in the healthcare context of more than 8,500\nindividuals, we systematically benchmark PersonaTwin against standard LLM\noutputs, and our rigorous evaluation unites state-of-the-art text similarity\nmetrics with dedicated demographic parity assessments, ensuring that generated\nresponses remain accurate and unbiased. Experimental results show that our\nframework produces simulation fidelity on par with oracle settings. Moreover,\ndownstream models trained on persona-twins approximate models trained on\nindividuals in terms of prediction and fairness metrics across both\nGPT-4o-based and Llama-based models. Together, these findings underscore the\npotential for LLM digital twin-based approaches in producing realistic and\nemotionally nuanced user simulations, offering a powerful tool for personalized\ndigital user modeling and behavior analysis.", "AI": {"tldr": "PersonaTwin是一个多层提示条件框架，通过整合人口统计、行为和心理测量数据，为LLM构建自适应数字孪生，实现了高保真、无偏见的个性化用户模拟。", "motivation": "大型语言模型（LLMs）在用户建模和人类行为近似方面存在局限性，难以捕捉个体用户的多维度细微差别。", "method": "引入PersonaTwin框架，通过多层提示条件化整合人口统计、行为和心理测量数据来构建自适应数字孪生。使用包含超过8,500个个体的医疗保健数据集进行基准测试，并采用文本相似度指标和人口统计公平性评估来确保生成响应的准确性和无偏性。", "result": "PersonaTwin框架产生的模拟保真度与预言机设置相当。基于人格孪生训练的下游模型在预测和公平性指标方面，无论是基于GPT-4o还是Llama的模型，都能近似于基于个体训练的模型。生成的响应准确且无偏。", "conclusion": "基于LLM数字孪生（如PersonaTwin）的方法在生成逼真且情感细腻的用户模拟方面具有巨大潜力，为个性化数字用户建模和行为分析提供了强大工具。"}}
{"id": "2508.10922", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10922", "abs": "https://arxiv.org/abs/2508.10922", "authors": ["Jianlong Wu", "Wei Liu", "Ye Liu", "Meng Liu", "Liqiang Nie", "Zhouchen Lin", "Chang Wen Chen"], "title": "A Survey on Video Temporal Grounding with Multimodal Large Language Model", "comment": "20 pages,6 figures,survey", "summary": "The recent advancement in video temporal grounding (VTG) has significantly\nenhanced fine-grained video understanding, primarily driven by multimodal large\nlanguage models (MLLMs). With superior multimodal comprehension and reasoning\nabilities, VTG approaches based on MLLMs (VTG-MLLMs) are gradually surpassing\ntraditional fine-tuned methods. They not only achieve competitive performance\nbut also excel in generalization across zero-shot, multi-task, and multi-domain\nsettings. Despite extensive surveys on general video-language understanding,\ncomprehensive reviews specifically addressing VTG-MLLMs remain scarce. To fill\nthis gap, this survey systematically examines current research on VTG-MLLMs\nthrough a three-dimensional taxonomy: 1) the functional roles of MLLMs,\nhighlighting their architectural significance; 2) training paradigms, analyzing\nstrategies for temporal reasoning and task adaptation; and 3) video feature\nprocessing techniques, which determine spatiotemporal representation\neffectiveness. We further discuss benchmark datasets, evaluation protocols, and\nsummarize empirical findings. Finally, we identify existing limitations and\npropose promising research directions. For additional resources and details,\nreaders are encouraged to visit our repository at\nhttps://github.com/ki-lw/Awesome-MLLMs-for-Video-Temporal-Grounding.", "AI": {"tldr": "该综述系统性地审视了基于多模态大语言模型（MLLMs）的视频时序定位（VTG-MLLMs）的最新研究，填补了现有综述的空白。", "motivation": "多模态大语言模型（MLLMs）在视频时序定位（VTG）中表现出色，超越了传统微调方法，并在零样本、多任务和多领域设置中展现出卓越的泛化能力。然而，针对VTG-MLLMs的全面综述仍旧稀缺。", "method": "本综述采用三维分类法系统地审视了VTG-MLLMs：1) MLLMs的功能角色；2) 训练范式；3) 视频特征处理技术。此外，还讨论了基准数据集、评估协议并总结了实证发现。", "result": "该综述对VTG-MLLMs的当前研究进行了系统性地分类和分析，总结了其在架构、训练和特征处理方面的策略，并讨论了相关基准和评估方法。", "conclusion": "识别了VTG-MLLMs的现有局限性，并提出了有前景的未来研究方向。"}}
{"id": "2508.11331", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11331", "abs": "https://arxiv.org/abs/2508.11331", "authors": ["Xinyi Wang", "Smaranda Tasmoc", "Nantheera Anantrasirichai", "Angeliki Katsenou"], "title": "Guiding WaveMamba with Frequency Maps for Image Debanding", "comment": "5 pages, 2 figures", "summary": "Compression at low bitrates in modern codecs often introduces banding\nartifacts, especially in smooth regions such as skies. These artifacts degrade\nvisual quality and are common in user-generated content due to repeated\ntranscoding. We propose a banding restoration method that employs the Wavelet\nState Space Model and a frequency masking map to preserve high-frequency\ndetails. Furthermore, we provide a benchmark of open-source banding restoration\nmethods and evaluate their performance on two public banding image datasets.\nExperimentation on the available datasets suggests that the proposed\npost-processing approach effectively suppresses banding compared to the\nstate-of-the-art method (a DBI value of 0.082 on BAND-2k) while preserving\nimage textures. Visual inspections of the results confirm this. Code and\nsupplementary material are available at:\nhttps://github.com/xinyiW915/Debanding-PCS2025.", "AI": {"tldr": "该论文提出了一种新的去条带伪影方法，结合小波状态空间模型和频率掩蔽图来处理低比特率压缩引入的条带伪影，同时保留高频细节，并在公开数据集上表现优于现有方法。", "motivation": "现代编解码器在低比特率压缩时常引入条带伪影，尤其是在平滑区域（如天空），这会降低视觉质量。在用户生成内容中，由于重复转码，此类伪影尤为常见。", "method": "该方法采用小波状态空间模型（Wavelet State Space Model）和频率掩蔽图（frequency masking map）来恢复条带伪影，并旨在保留高频细节。此外，论文还对开源去条带方法进行了基准测试和性能评估。", "result": "实验表明，所提出的后处理方法能有效抑制条带伪影，与现有最先进方法相比，在BAND-2k数据集上取得了0.082的DBI值，同时能很好地保留图像纹理。视觉检查也证实了这一点。", "conclusion": "所提出的去条带后处理方法能有效抑制低比特率压缩产生的条带伪影，并在保留图像纹理方面表现出色，优于当前最先进的方法。"}}
{"id": "2508.11182", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11182", "abs": "https://arxiv.org/abs/2508.11182", "authors": ["Matti Berthold", "Lydia Blümel", "Anna Rapberger"], "title": "On Strong and Weak Admissibility in Non-Flat Assumption-Based Argumentation", "comment": null, "summary": "In this work, we broaden the investigation of admissibility notions in the\ncontext of assumption-based argumentation (ABA). More specifically, we study\ntwo prominent alternatives to the standard notion of admissibility from\nabstract argumentation, namely strong and weak admissibility, and introduce the\nrespective preferred, complete and grounded semantics for general (sometimes\ncalled non-flat) ABA. To do so, we use abstract bipolar set-based argumentation\nframeworks (BSAFs) as formal playground since they concisely capture the\nrelations between assumptions and are expressive enough to represent general\nnon-flat ABA frameworks, as recently shown. While weak admissibility has been\nrecently investigated for a restricted fragment of ABA in which assumptions\ncannot be derived (flat ABA), strong admissibility has not been investigated\nfor ABA so far. We introduce strong admissibility for ABA and investigate\ndesirable properties. We furthermore extend the recent investigations of weak\nadmissibility in the flat ABA fragment to the non-flat case. We show that the\ncentral modularization property is maintained under classical, strong, and weak\nadmissibility. We also show that strong and weakly admissible semantics in\nnon-flat ABA share some of the shortcomings of standard admissible semantics\nand discuss ways to address these.", "AI": {"tldr": "本文拓展了基于假设论证（ABA）中的可采纳性概念研究，引入了强可采纳性，并将弱可采纳性从扁平ABA扩展到非扁平ABA，同时为这些新概念定义了相应的语义，并分析了其性质和局限性。", "motivation": "标准的抽象论证可采纳性概念存在局限性，需要探索替代方案。弱可采纳性已在受限的扁平ABA中有所研究，但强可采纳性在ABA中尚未被探索。此外，弱可采纳性在非扁平ABA中的应用也需进一步扩展和研究。", "method": "研究采用抽象双极集基论证框架（BSAFs）作为形式化工具，因为它们能够简洁地捕捉假设之间的关系并表示非扁平ABA。在此基础上，引入了ABA的强可采纳性定义，并将其性质纳入研究。同时，将弱可采纳性的研究从扁平ABA扩展到非扁平ABA。", "result": "为通用（非扁平）ABA引入了强可采纳性及其相应的偏好、完备和基础语义，并研究了其期望性质。将弱可采纳性从扁平ABA扩展到非扁平ABA。结果表明，核心的模块化性质在经典、强和弱可采纳性下均得以保持。此外，发现非扁平ABA中的强和弱可采纳性语义与标准可采纳性语义共享某些缺点。", "conclusion": "强和弱可采纳性为非扁平ABA提供了新的语义选择，且某些重要性质（如模块化）得以保持。然而，这些新的可采纳性语义也继承了标准可采纳性的一些局限性，未来工作需探讨如何解决这些问题。"}}
{"id": "2508.11071", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.11071", "abs": "https://arxiv.org/abs/2508.11071", "authors": ["Zhentong Shao", "Jingtao Qin", "Nanpeng Yu"], "title": "A Neural Column-and-Constraint Generation Method for Solving Two-Stage Stochastic Unit Commitment", "comment": null, "summary": "Two-stage stochastic unit commitment (2S-SUC) problems have been widely\nadopted to manage the uncertainties introduced by high penetrations of\nintermittent renewable energy resources. While decomposition-based algorithms\nsuch as column-and-constraint generation has been proposed to solve these\nproblems, they remain computationally prohibitive for large-scale, real-time\napplications. In this paper, we introduce a Neural Column-and-Constraint\nGeneration (Neural CCG) method to significantly accelerate the solution of\n2S-SUC problems. The proposed approach integrates a neural network that\napproximates the second-stage recourse problem by learning from high-level\nfeatures of operational scenarios and the first-stage commitment decisions.\nThis neural estimator is embedded within the CCG framework, replacing repeated\nsubproblem solving with rapid neural evaluations. We validate the effectiveness\nof the proposed method on the IEEE 118-bus system. Compared to the original CCG\nand a state-of-the-art commercial solver, Neural CCG achieves up to\n130.1$\\times$ speedup while maintaining a mean optimality gap below 0.096\\%,\ndemonstrating its strong potential for scalable stochastic optimization in\npower system.", "AI": {"tldr": "提出了一种名为Neural CCG的新方法，通过集成神经网络来近似两阶段随机机组组合（2S-SUC）问题中的第二阶段子问题，显著加速了大规模2S-SUC问题的求解。", "motivation": "高渗透率间歇性可再生能源引入的不确定性使得两阶段随机机组组合（2S-SUC）问题变得复杂。尽管已存在基于分解的算法（如CCG），但对于大规模实时应用来说，其计算成本仍然过高，难以满足实际需求。", "method": "本文提出Neural CCG方法，将一个神经网络集成到CCG框架中。该神经网络通过学习运行场景的高级特征和第一阶段机组承诺决策，来近似第二阶段的追索问题。这使得CCG中重复的子问题求解被快速的神经网络评估所取代。", "result": "在IEEE 118-bus系统上验证了该方法的有效性。与原始CCG和最先进的商业求解器相比，Neural CCG实现了高达130.1倍的加速，同时将平均最优性差距保持在0.096%以下。", "conclusion": "Neural CCG方法在电力系统中的可扩展随机优化方面展现出强大潜力，它在保持高精度的同时显著提升了计算速度，有望解决大规模2S-SUC问题的实时计算挑战。"}}
{"id": "2508.11002", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11002", "abs": "https://arxiv.org/abs/2508.11002", "authors": ["Nikolaos Gkanatsios", "Jiahe Xu", "Matthew Bronars", "Arsalan Mousavian", "Tsung-Wei Ke", "Katerina Fragkiadaki"], "title": "3D FlowMatch Actor: Unified 3D Policy for Single- and Dual-Arm Manipulation", "comment": null, "summary": "We present 3D FlowMatch Actor (3DFA), a 3D policy architecture for robot\nmanipulation that combines flow matching for trajectory prediction with 3D\npretrained visual scene representations for learning from demonstration. 3DFA\nleverages 3D relative attention between action and visual tokens during action\ndenoising, building on prior work in 3D diffusion-based single-arm policy\nlearning. Through a combination of flow matching and targeted system-level and\narchitectural optimizations, 3DFA achieves over 30x faster training and\ninference than previous 3D diffusion-based policies, without sacrificing\nperformance. On the bimanual PerAct2 benchmark, it establishes a new state of\nthe art, outperforming the next-best method by an absolute margin of 41.4%. In\nextensive real-world evaluations, it surpasses strong baselines with up to\n1000x more parameters and significantly more pretraining. In unimanual\nsettings, it sets a new state of the art on 74 RLBench tasks by directly\npredicting dense end-effector trajectories, eliminating the need for motion\nplanning. Comprehensive ablation studies underscore the importance of our\ndesign choices for both policy effectiveness and efficiency.", "AI": {"tldr": "本文提出了3D FlowMatch Actor (3DFA)，一种用于机器人操作的3D策略架构，结合了流匹配轨迹预测和3D预训练视觉表示。3DFA在保持性能的同时，训练和推理速度比之前的3D扩散策略快30倍以上，并在双臂和单臂操作任务上均达到了新的SOTA。", "motivation": "之前的3D扩散策略在机器人操作中存在训练和推理速度慢的问题。研究旨在开发一种更高效、更高性能的3D策略架构，以加速机器人学习并提高其在复杂操作任务中的表现。", "method": "3DFA结合了流匹配（用于轨迹预测）和3D预训练视觉场景表示（用于从演示中学习）。它在动作去噪过程中利用动作和视觉token之间的3D相对注意力，并基于3D扩散单臂策略学习的先前工作。通过流匹配与系统级和架构优化相结合，实现了效率提升。", "result": "3DFA比之前的3D扩散策略快30倍以上的训练和推理速度，且不牺牲性能。在双臂PerAct2基准测试中，它将SOTA提高了41.4%。在实际世界评估中，它超越了参数量大1000倍且预训练更多的强基线。在单臂设置中，它通过直接预测密集的末端执行器轨迹，在74个RLBench任务上设立了新的SOTA，消除了运动规划的需要。全面的消融研究证实了设计选择的重要性。", "conclusion": "3D FlowMatch Actor (3DFA) 是一种高效且高性能的3D机器人操作策略，通过结合流匹配和优化的3D视觉表示，显著提升了训练和推理速度，并在多种机器人操作任务中设定了新的性能基准，同时简化了单臂任务的执行。"}}
{"id": "2508.10925", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10925", "abs": "https://arxiv.org/abs/2508.10925", "authors": ["OpenAI", ":", "Sandhini Agarwal", "Lama Ahmad", "Jason Ai", "Sam Altman", "Andy Applebaum", "Edwin Arbus", "Rahul K. Arora", "Yu Bai", "Bowen Baker", "Haiming Bao", "Boaz Barak", "Ally Bennett", "Tyler Bertao", "Nivedita Brett", "Eugene Brevdo", "Greg Brockman", "Sebastien Bubeck", "Che Chang", "Kai Chen", "Mark Chen", "Enoch Cheung", "Aidan Clark", "Dan Cook", "Marat Dukhan", "Casey Dvorak", "Kevin Fives", "Vlad Fomenko", "Timur Garipov", "Kristian Georgiev", "Mia Glaese", "Tarun Gogineni", "Adam Goucher", "Lukas Gross", "Katia Gil Guzman", "John Hallman", "Jackie Hehir", "Johannes Heidecke", "Alec Helyar", "Haitang Hu", "Romain Huet", "Jacob Huh", "Saachi Jain", "Zach Johnson", "Chris Koch", "Irina Kofman", "Dominik Kundel", "Jason Kwon", "Volodymyr Kyrylov", "Elaine Ya Le", "Guillaume Leclerc", "James Park Lennon", "Scott Lessans", "Mario Lezcano-Casado", "Yuanzhi Li", "Zhuohan Li", "Ji Lin", "Jordan Liss", "Lily", "Liu", "Jiancheng Liu", "Kevin Lu", "Chris Lu", "Zoran Martinovic", "Lindsay McCallum", "Josh McGrath", "Scott McKinney", "Aidan McLaughlin", "Song Mei", "Steve Mostovoy", "Tong Mu", "Gideon Myles", "Alexander Neitz", "Alex Nichol", "Jakub Pachocki", "Alex Paino", "Dana Palmie", "Ashley Pantuliano", "Giambattista Parascandolo", "Jongsoo Park", "Leher Pathak", "Carolina Paz", "Ludovic Peran", "Dmitry Pimenov", "Michelle Pokrass", "Elizabeth Proehl", "Huida Qiu", "Gaby Raila", "Filippo Raso", "Hongyu Ren", "Kimmy Richardson", "David Robinson", "Bob Rotsted", "Hadi Salman", "Suvansh Sanjeev", "Max Schwarzer", "D. Sculley", "Harshit Sikchi", "Kendal Simon", "Karan Singhal", "Yang Song", "Dane Stuckey", "Zhiqing Sun", "Philippe Tillet", "Sam Toizer", "Foivos Tsimpourlas", "Nikhil Vyas", "Eric Wallace", "Xin Wang", "Miles Wang", "Olivia Watkins", "Kevin Weil", "Amy Wendling", "Kevin Whinnery", "Cedric Whitney", "Hannah Wong", "Lin Yang", "Yu Yang", "Michihiro Yasunaga", "Kristen Ying", "Wojciech Zaremba", "Wenting Zhan", "Cyril Zhang", "Brian Zhang", "Eddie Zhang", "Shengjia Zhao"], "title": "gpt-oss-120b & gpt-oss-20b Model Card", "comment": null, "summary": "We present gpt-oss-120b and gpt-oss-20b, two open-weight reasoning models\nthat push the frontier of accuracy and inference cost. The models use an\nefficient mixture-of-expert transformer architecture and are trained using\nlarge-scale distillation and reinforcement learning. We optimize the models to\nhave strong agentic capabilities (deep research browsing, python tool use, and\nsupport for developer-provided functions), all while using a rendered chat\nformat that enables clear instruction following and role delineation. Both\nmodels achieve strong results on benchmarks ranging from mathematics, coding,\nand safety. We release the model weights, inference implementations, tool\nenvironments, and tokenizers under an Apache 2.0 license to enable broad use\nand further research.", "AI": {"tldr": "本文介绍了gpt-oss-120b和gpt-oss-20b，两款开源、高效的推理模型，它们在准确性和推理成本方面表现出色，并支持强大的代理能力。", "motivation": "研究动机是推动推理模型的准确性前沿，同时降低推理成本，并增强模型作为智能体的能力，使其能够进行深度研究浏览、使用Python工具和支持开发者提供的函数。", "method": "模型采用高效的混合专家（MoE）Transformer架构，通过大规模蒸馏和强化学习进行训练。模型针对代理能力进行了优化，并使用渲染的聊天格式以实现清晰的指令遵循和角色划分。所有模型权重、推理实现、工具环境和分词器均以Apache 2.0许可发布。", "result": "两款模型在数学、编码和安全等基准测试中均取得了优异结果。同时，作者开源了模型权重、推理实现、工具环境和分词器，以便广泛使用和进一步研究。", "conclusion": "该研究成功开发并发布了两款高性能、低成本的开源推理模型，它们具备强大的代理能力和清晰的指令遵循特性，为相关领域的研究和应用提供了宝贵资源。"}}
{"id": "2508.10931", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10931", "abs": "https://arxiv.org/abs/2508.10931", "authors": ["Wenqi Guo", "Shan Du"], "title": "VSF: Simple, Efficient, and Effective Negative Guidance in Few-Step Image Generation Models By \\underline{V}alue \\underline{S}ign \\underline{F}lip", "comment": null, "summary": "We introduce Value Sign Flip (VSF), a simple and efficient method for\nincorporating negative prompt guidance in few-step diffusion and flow-matching\nimage generation models. Unlike existing approaches such as classifier-free\nguidance (CFG), NASA, and NAG, VSF dynamically suppresses undesired content by\nflipping the sign of attention values from negative prompts. Our method\nrequires only small computational overhead and integrates effectively with\nMMDiT-style architectures such as Stable Diffusion 3.5 Turbo, as well as\ncross-attention-based models like Wan. We validate VSF on challenging datasets\nwith complex prompt pairs and demonstrate superior performance in both static\nimage and video generation tasks. Experimental results show that VSF\nsignificantly improves negative prompt adherence compared to prior methods in\nfew-step models, and even CFG in non-few-step models, while maintaining\ncompetitive image quality. Code and ComfyUI node are available in\nhttps://github.com/weathon/VSF/tree/main.", "AI": {"tldr": "VSF是一种简单高效的方法，通过翻转负向提示的注意力值符号，在少步扩散模型中实现负向提示引导，显著提高负向提示的遵循度并保持图像质量。", "motivation": "现有方法（如CFG、NASA、NAG）在少步扩散模型中整合负向提示引导存在局限性，需要一种更有效、计算开销小的方法来抑制不希望的内容。", "method": "引入Value Sign Flip (VSF) 方法，通过动态翻转来自负向提示的注意力值符号来抑制不希望的内容。该方法计算开销小，可有效集成到MMDiT风格（如Stable Diffusion 3.5 Turbo）和基于交叉注意力的模型（如Wan）中。", "result": "VSF在具有复杂提示对的挑战性数据集上，在静态图像和视频生成任务中均表现出卓越性能。实验结果表明，与现有方法相比，VSF显著提高了少步模型中负向提示的遵循度，甚至在非少步模型中也优于CFG，同时保持了有竞争力的图像质量。", "conclusion": "VSF是一种有效且高效的负向提示引导方法，它通过翻转注意力值符号，显著提升了少步和非少步扩散模型中负向提示的遵循度，同时保持了图像质量。"}}
{"id": "2508.11375", "categories": ["eess.IV", "cs.CV", "I.4.9"], "pdf": "https://arxiv.org/pdf/2508.11375", "abs": "https://arxiv.org/abs/2508.11375", "authors": ["Zonglin Wu", "Yule Xue", "Qianxiang Hu", "Yaoyao Feng", "Yuqi Ma", "Shanxiong Chen"], "title": "AnatoMaskGAN: GNN-Driven Slice Feature Fusion and Noise Augmentation for Medical Semantic Image Synthesis", "comment": "8 pages", "summary": "Medical semantic-mask synthesis boosts data augmentation and analysis, yet\nmost GAN-based approaches still produce one-to-one images and lack spatial\nconsistency in complex scans. To address this, we propose AnatoMaskGAN, a novel\nsynthesis framework that embeds slice-related spatial features to precisely\naggregate inter-slice contextual dependencies, introduces diverse\nimage-augmentation strategies, and optimizes deep feature learning to improve\nperformance on complex medical images. Specifically, we design a GNN-based\nstrongly correlated slice-feature fusion module to model spatial relationships\nbetween slices and integrate contextual information from neighboring slices,\nthereby capturing anatomical details more comprehensively; we introduce a\nthree-dimensional spatial noise-injection strategy that weights and fuses\nspatial features with noise to enhance modeling of structural diversity; and we\nincorporate a grayscale-texture classifier to optimize grayscale distribution\nand texture representation during generation. Extensive experiments on the\npublic L2R-OASIS and L2R-Abdomen CT datasets show that AnatoMaskGAN raises PSNR\non L2R-OASIS to 26.50 dB (0.43 dB higher than the current state of the art) and\nachieves an SSIM of 0.8602 on L2R-Abdomen CT--a 0.48 percentage-point gain over\nthe best model, demonstrating its superiority in reconstruction accuracy and\nperceptual quality. Ablation studies that successively remove the slice-feature\nfusion module, spatial 3D noise-injection strategy, and grayscale-texture\nclassifier reveal that each component contributes significantly to PSNR, SSIM,\nand LPIPS, further confirming the independent value of each core design in\nenhancing reconstruction accuracy and perceptual quality.", "AI": {"tldr": "AnatoMaskGAN是一种用于医学语义掩码合成的新型框架，通过嵌入切片相关空间特征、引入多样图像增强策略和优化深度特征学习，解决了现有GAN在复杂扫描中图像一致性差和空间连续性不足的问题，并在公共数据集上取得了最先进的性能。", "motivation": "大多数基于GAN的方法在医学语义掩码合成中仍生成一对一图像，并且在复杂扫描中缺乏空间一致性，无法很好地捕捉解剖细节和结构多样性。", "method": "本文提出了AnatoMaskGAN框架。具体方法包括：1) 设计了一个基于GNN的强相关切片特征融合模块，用于建模切片间的空间关系并整合邻近切片的上下文信息；2) 引入了三维空间噪声注入策略，通过加权和融合空间特征与噪声来增强结构多样性建模；3) 集成了一个灰度纹理分类器，以优化生成过程中的灰度分布和纹理表示。", "result": "在L2R-OASIS数据集上，AnatoMaskGAN的PSNR达到26.50 dB，比当前最先进模型高0.43 dB；在L2R-Abdomen CT数据集上，SSIM达到0.8602，比最佳模型高0.48个百分点。消融研究表明，切片特征融合模块、三维空间噪声注入策略和灰度纹理分类器每个组件都对PSNR、SSIM和LPIPS有显著贡献。", "conclusion": "AnatoMaskGAN在医学语义掩码合成的重建精度和感知质量方面表现出卓越的性能，显著优于现有最佳模型，并证实了其核心设计组件的独立价值。"}}
{"id": "2508.11252", "categories": ["cs.AI", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.11252", "abs": "https://arxiv.org/abs/2508.11252", "authors": ["Youcheng Huang", "Bowen Qin", "Chen Huang", "Duanyu Feng", "Xi Yang", "Wenqiang Lei"], "title": "Beyond Solving Math Quiz: Evaluating the Ability of Large Reasoning Models to Ask for Information", "comment": null, "summary": "Large Reasoning Models (LRMs) have demonstrated remarkable problem-solving\nabilities in mathematics, as evaluated by existing benchmarks exclusively on\nwell-defined problems. However, such evaluation setup constitutes a critical\ngap, since a genuine intelligent agent should not only solve problems (as a\nmath quiz solver), but also be able~to ask for information when the problems\nlack sufficient information, enabling proactivity in responding users'\nrequests. To bridge such gap, we proposes a new dataset consisting of two types\nof incomplete problems with diverse contexts. Based on the dataset, our\nsystematical evaluation of LRMs reveals their inability in proactively asking\nfor information. In addition, we uncover the behaviors related to overthinking\nand hallucination of LRMs, and highlight the potential and challenges of\nsupervised fine-tuning in learning such ability. We hope to provide new\ninsights in developing LRMs with genuine intelligence, rather than just solving\nproblems.", "AI": {"tldr": "大型推理模型（LRMs）在解决不完整数学问题时，无法主动询问缺失信息，且存在过度思考和幻觉行为。", "motivation": "现有基准测试仅评估LRMs解决定义明确问题的能力，但真实的智能代理应能主动询问不完整问题所需的额外信息，弥补了这一评估空白。", "method": "提出了一个包含两种类型、不同上下文的不完整问题的新数据集，并基于此数据集系统评估了LRMs。", "result": "LRMs无法主动询问缺失信息，并表现出过度思考和幻觉行为。同时，也揭示了监督微调（SFT）在学习这种能力方面的潜力和挑战。", "conclusion": "研究为开发具有真正智能而非仅仅解决问题的LRMs提供了新见解。"}}
{"id": "2508.11080", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.11080", "abs": "https://arxiv.org/abs/2508.11080", "authors": ["Soumya Kundu", "Kaustav Chatterjee", "Ramij R. Hossain", "Sai Pushpak Nandanoori", "Veronica Adetola"], "title": "Managing Risks from Large Digital Loads Using Coordinated Grid-Forming Storage Network", "comment": "Submitted to IEEE PES T&D Conference and Expo 2026", "summary": "Anticipated rapid growth of large digital load, driven by artificial\nintelligence (AI) data centers, is poised to increase uncertainty and large\nfluctuations in consumption, threatening the stability, reliability, and\nsecurity of the energy infrastructure. Conventional measures taken by grid\nplanners and operators to ensure stable and reliable integration of new\nresources are either cost-prohibitive (e.g., transmission upgrades) or\nill-equipped (e.g., generation control) to resolve the unique challenges\nbrought on by AI Data Centers (e.g., extreme load transients). In this work, we\nexplore the feasibility of coordinating and managing available flexibility in\nthe grid, in terms of grid-forming storage units, to ensure stable and reliable\nintegration of AI Data Centers without the need for costly grid upgrades.\nRecently developed bi-layered coordinated control strategies -- involving\nfast-acting, local, autonomous, control at the storage to maintain transient\nsafety in voltage and frequency at the point-of-interconnection, and a slower,\ncoordinated (consensus) control to restore normal operating condition in the\ngrid -- are used in the case studies. A comparison is drawn between broadly two\nscenarios: a network of coordinated, smaller, distributed storage vs. larger\nstorage installations collocated with large digital loads. IEEE 68-bus network\nis used for the case studies, with large digital load profiles drawn from the\nMIT Supercloud Dataset.", "AI": {"tldr": "该研究提出通过协调管理并网型储能单元的灵活性，并采用双层协调控制策略，来稳定、可靠地整合AI数据中心带来的大规模数字负载，避免昂贵的电网升级。", "motivation": "人工智能(AI)数据中心预计将导致数字负载快速增长，从而增加电网消耗的不确定性和大幅波动，威胁能源基础设施的稳定性、可靠性和安全性。传统的电网规划和运营措施（如输电升级）成本过高，或（如发电控制）无法有效应对AI数据中心带来的独特挑战（如极端负载瞬变）。", "method": "研究探索了利用并网型储能单元协调管理电网灵活性的可行性。采用了一种新近开发的双层协调控制策略：一层是快速响应、本地自主的储能控制，用于维持并网点电压和频率的瞬态安全；另一层是较慢的协调（共识）控制，用于恢复电网的正常运行状态。案例研究比较了两种场景：分布式小型储能网络与大型数字负载并置的大型储能装置。案例研究使用了IEEE 68-bus网络，并从MIT Supercloud数据集获取大型数字负载曲线。", "result": "研究通过案例分析，探索了利用并网型储能协调管理电网灵活性的可行性，以确保AI数据中心的稳定可靠集成，并对比了分布式小型储能网络与大型并置储能装置两种部署场景。", "conclusion": "该研究旨在为人工智能数据中心提供一种无需昂贵电网升级即可稳定可靠接入电网的解决方案，通过有效利用电网中并网型储能的灵活性和先进的双层协调控制策略来实现。"}}
{"id": "2508.11049", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11049", "abs": "https://arxiv.org/abs/2508.11049", "authors": ["Kelin Yu", "Sheng Zhang", "Harshit Soora", "Furong Huang", "Heng Huang", "Pratap Tokekar", "Ruohan Gao"], "title": "GenFlowRL: Shaping Rewards with Generative Object-Centric Flow in Visual Reinforcement Learning", "comment": "Published at ICCV 2025", "summary": "Recent advances have shown that video generation models can enhance robot\nlearning by deriving effective robot actions through inverse dynamics. However,\nthese methods heavily depend on the quality of generated data and struggle with\nfine-grained manipulation due to the lack of environment feedback. While\nvideo-based reinforcement learning improves policy robustness, it remains\nconstrained by the uncertainty of video generation and the challenges of\ncollecting large-scale robot datasets for training diffusion models. To address\nthese limitations, we propose GenFlowRL, which derives shaped rewards from\ngenerated flow trained from diverse cross-embodiment datasets. This enables\nlearning generalizable and robust policies from diverse demonstrations using\nlow-dimensional, object-centric features. Experiments on 10 manipulation tasks,\nboth in simulation and real-world cross-embodiment evaluations, demonstrate\nthat GenFlowRL effectively leverages manipulation features extracted from\ngenerated object-centric flow, consistently achieving superior performance\nacross diverse and challenging scenarios. Our Project Page:\nhttps://colinyu1.github.io/genflowrl", "AI": {"tldr": "现有视频生成模型在机器人学习中面临数据质量、反馈和数据集挑战。GenFlowRL提出从跨载体数据集生成的流中提取塑形奖励，实现泛化和鲁棒的精细操作，并在多样化任务中表现优越。", "motivation": "现有基于视频的机器人学习方法高度依赖生成数据质量，缺乏环境反馈，难以进行精细操作；同时，视频生成的不确定性及训练扩散模型所需的大规模机器人数据集收集困难，限制了基于视频的强化学习的鲁棒性。", "method": "提出GenFlowRL框架，其核心是从由多样化跨载体数据集训练生成的流中提取塑形奖励。该方法利用低维、以物体为中心的特征，从而能够从多样化示范中学习可泛化且鲁棒的策略。", "result": "在10个模拟和真实世界跨载体操作任务上的实验表明，GenFlowRL有效利用了从生成的以物体为中心的流中提取的操作特征，在多样化和挑战性场景中持续实现卓越性能。", "conclusion": "GenFlowRL通过利用生成的流来提供奖励，成功克服了现有视频生成模型在机器人学习中的局限性，实现了在复杂操作任务中泛化且鲁棒的卓越性能。"}}
{"id": "2508.10927", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10927", "abs": "https://arxiv.org/abs/2508.10927", "authors": ["Jiaxin Pei", "Soumya Vadlamannati", "Liang-Kang Huang", "Daniel Preotiuc-Pietro", "Xinyu Hua"], "title": "Modeling and Detecting Company Risks from News: A Case Study in Bloomberg News", "comment": null, "summary": "Identifying risks associated with a company is important to investors and the\nwell-being of the overall financial market. In this study, we build a\ncomputational framework to automatically extract company risk factors from news\narticles. Our newly proposed schema comprises seven distinct aspects, such as\nsupply chain, regulations, and competitions. We sample and annotate 744 news\narticles and benchmark various machine learning models. While large language\nmodels have achieved huge progress in various types of NLP tasks, our\nexperiment shows that zero-shot and few-shot prompting state-of-the-art LLMs\n(e.g. LLaMA-2) can only achieve moderate to low performances in identifying\nrisk factors. And fine-tuned pre-trained language models are performing better\non most of the risk factors. Using this model, we analyze over 277K Bloomberg\nnews articles and demonstrate that identifying risk factors from news could\nprovide extensive insight into the operations of companies and industries.", "AI": {"tldr": "本研究构建了一个计算框架，从新闻文章中自动提取公司风险因素，并提出了包含七个方面的风险分类方案。实验表明，微调后的预训练语言模型在识别风险因素方面优于零样本/少样本的大语言模型，且该方法能为公司运营和行业提供深入洞察。", "motivation": "识别与公司相关的风险对投资者和整体金融市场的健康至关重要。", "method": "研究构建了一个计算框架，用于从新闻文章中自动提取公司风险因素，并提出了一个包含供应链、法规、竞争等七个不同方面的风险分类方案。通过抽样和标注744篇新闻文章，基准测试了各种机器学习模型，并比较了零样本/少样本大语言模型（如LLaMA-2）与微调预训练语言模型的性能。最终，使用该模型分析了超过27.7万篇彭博新闻文章。", "result": "实验结果显示，零样本和少样本的大语言模型在识别风险因素方面表现中等至较低，而微调后的预训练语言模型在大多数风险因素上表现更好。通过该模型对大量新闻文章的分析表明，从新闻中识别风险因素可以为公司运营和行业提供广泛的洞察。", "conclusion": "构建的计算框架能够有效从新闻中自动提取公司风险因素。对于此类任务，微调的预训练语言模型比零样本/少样本的大语言模型表现更优。从新闻中识别风险因素能够为理解公司和行业提供重要见解。"}}
{"id": "2508.10933", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.10933", "abs": "https://arxiv.org/abs/2508.10933", "authors": ["Yoli Shavit", "Yosi Keller"], "title": "Relative Pose Regression with Pose Auto-Encoders: Enhancing Accuracy and Data Efficiency for Retail Applications", "comment": "Accepted to ICCVW 2025", "summary": "Accurate camera localization is crucial for modern retail environments,\nenabling enhanced customer experiences, streamlined inventory management, and\nautonomous operations. While Absolute Pose Regression (APR) from a single image\noffers a promising solution, approaches that incorporate visual and spatial\nscene priors tend to achieve higher accuracy. Camera Pose Auto-Encoders (PAEs)\nhave recently been introduced to embed such priors into APR. In this work, we\nextend PAEs to the task of Relative Pose Regression (RPR) and propose a novel\nre-localization scheme that refines APR predictions using PAE-based RPR,\nwithout requiring additional storage of images or pose data. We first introduce\nPAE-based RPR and establish its effectiveness by comparing it with image-based\nRPR models of equivalent architectures. We then demonstrate that our refinement\nstrategy, driven by a PAE-based RPR, enhances APR localization accuracy on\nindoor benchmarks. Notably, our method is shown to achieve competitive\nperformance even when trained with only 30% of the data, substantially reducing\nthe data collection burden for retail deployment. Our code and pre-trained\nmodels are available at: https://github.com/yolish/camera-pose-auto-encoders", "AI": {"tldr": "本文提出了一种新的重定位方案，通过基于姿态自编码器（PAE）的相对姿态回归（RPR）来优化绝对姿态回归（APR）的预测，从而在不增加存储的情况下提高相机定位精度，并显著减少数据需求。", "motivation": "在现代零售环境中，准确的相机定位对于提升客户体验、简化库存管理和实现自主操作至关重要。虽然单图像绝对姿态回归（APR）有前景，但结合视觉和空间先验的方法能获得更高精度。姿态自编码器（PAE）已被引入以嵌入这些先验到APR中，但仍有提升空间，尤其是在数据效率和精度方面。", "method": "作者将PAE扩展到相对姿态回归（RPR）任务，并提出了一种新颖的重定位方案。该方案使用基于PAE的RPR来精炼APR的预测，而无需额外的图像或姿态数据存储。首先，他们介绍了基于PAE的RPR，并将其与同等架构的基于图像的RPR模型进行比较。然后，展示了其精炼策略如何提高APR的定位精度。", "result": "研究表明，基于PAE的RPR是有效的。提出的精炼策略显著提高了室内基准测试上的APR定位精度。值得注意的是，即使仅使用30%的数据进行训练，该方法也能达到具有竞争力的性能，从而大大减少了零售部署的数据收集负担。", "conclusion": "该工作成功地将PAE扩展到RPR任务，并开发了一种无需额外存储即可提升APR定位精度的新型重定位方案。该方法在数据效率和精度方面表现出色，使其成为零售环境中相机定位的强大且实用的解决方案。"}}
{"id": "2508.11391", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11391", "abs": "https://arxiv.org/abs/2508.11391", "authors": ["Yinggan Tang", "Quanwei Hu"], "title": "LKFMixer: Exploring Large Kernel Feature For Efficient Image Super-Resolution", "comment": null, "summary": "The success of self-attention (SA) in Transformer demonstrates the importance\nof non-local information to image super-resolution (SR), but the huge computing\npower required makes it difficult to implement lightweight models. To solve\nthis problem, we propose a pure convolutional neural network (CNN) model,\nLKFMixer, which utilizes large convolutional kernel to simulate the ability of\nself-attention to capture non-local features. Specifically, we increase the\nkernel size to 31 to obtain the larger receptive field as possible, and reduce\nthe parameters and computations by coordinate decomposition. Meanwhile, a\nspatial feature modulation block (SFMB) is designed to enhance the focus of\nfeature information on both spatial and channel dimension. In addition, by\nintroducing feature selection block (FSB), the model can adaptively adjust the\nweights between local features and non-local features. Extensive experiments\nshow that the proposed LKFMixer family outperform other state-of-the-art (SOTA)\nmethods in terms of SR performance and reconstruction quality. In particular,\ncompared with SwinIR-light on Manga109 dataset, LKFMixer-L achieves 0.6dB PSNR\nimprovement at $\\times$4 scale, while the inference speed is $\\times$5 times\nfaster. The code is available at https://github.com/Supereeeee/LKFMixer.", "AI": {"tldr": "本文提出LKFMixer，一个纯卷积神经网络（CNN）模型，通过使用大卷积核模拟自注意力机制捕捉非局部特征，解决了自注意力在图像超分辨率（SR）中计算量大的问题，实现了更优的性能和推理速度。", "motivation": "自注意力机制在Transformer中成功应用于图像超分辨率，但其巨大的计算需求使其难以应用于轻量级模型。", "method": "提出LKFMixer模型，该模型是一个纯CNN架构。具体方法包括：1) 使用大卷积核（尺寸达31）以获得更大的感受野，模拟自注意力捕获非局部特征的能力；2) 通过坐标分解减少参数和计算量；3) 设计空间特征调制块（SFMB）以增强特征信息在空间和通道维度上的聚焦；4) 引入特征选择块（FSB）以自适应调整局部特征和非局部特征之间的权重。", "result": "LKFMixer系列模型在SR性能和重建质量方面超越了其他最先进的方法。特别是，在Manga109数据集上，LKFMixer-L在4倍放大下比SwinIR-light提高了0.6dB PSNR，推理速度快了5倍。", "conclusion": "所提出的LKFMixer证明了纯CNN模型通过大卷积核可以有效模拟自注意力机制的非局部特征捕获能力，并在图像超分辨率任务中实现卓越的性能和计算效率。"}}
{"id": "2508.11347", "categories": ["cs.AI", "cs.LG", "I.2.4; I.2.6; H.2.8"], "pdf": "https://arxiv.org/pdf/2508.11347", "abs": "https://arxiv.org/abs/2508.11347", "authors": ["Yifei Li", "Lingling Zhang", "Hang Yan", "Tianzhe Zhao", "Zihan Ma", "Muye Huang", "Jun Liu"], "title": "SAGE: Scale-Aware Gradual Evolution for Continual Knowledge Graph Embedding", "comment": "10 pages, 5 figures, Accepted at KDD 2025, code available at\n  https://github.com/lyfxjtu/Dynamic-Embedding", "summary": "Traditional knowledge graph (KG) embedding methods aim to represent entities\nand relations in a low-dimensional space, primarily focusing on static graphs.\nHowever, real-world KGs are dynamically evolving with the constant addition of\nentities, relations and facts. To address such dynamic nature of KGs, several\ncontinual knowledge graph embedding (CKGE) methods have been developed to\nefficiently update KG embeddings to accommodate new facts while maintaining\nlearned knowledge. As KGs grow at different rates and scales in real-world\nscenarios, existing CKGE methods often fail to consider the varying scales of\nupdates and lack systematic evaluation throughout the entire update process. In\nthis paper, we propose SAGE, a scale-aware gradual evolution framework for\nCKGE. Specifically, SAGE firstly determine the embedding dimensions based on\nthe update scales and expand the embedding space accordingly. The Dynamic\nDistillation mechanism is further employed to balance the preservation of\nlearned knowledge and the incorporation of new facts. We conduct extensive\nexperiments on seven benchmarks, and the results show that SAGE consistently\noutperforms existing baselines, with a notable improvement of 1.38% in MRR,\n1.25% in H@1 and 1.6% in H@10. Furthermore, experiments comparing SAGE with\nmethods using fixed embedding dimensions show that SAGE achieves optimal\nperformance on every snapshot, demonstrating the importance of adaptive\nembedding dimensions in CKGE. The codes of SAGE are publicly available at:\nhttps://github.com/lyfxjtu/Dynamic-Embedding.", "AI": {"tldr": "本文提出了SAGE，一个面向持续知识图谱嵌入（CKGE）的尺度感知渐进演化框架，通过根据更新规模自适应调整嵌入维度并采用动态蒸馏机制，有效处理知识图谱的动态演化。", "motivation": "传统知识图谱嵌入方法主要关注静态图。然而，真实世界的知识图谱是动态演化的，现有CKGE方法未能考虑不同规模的更新，并且缺乏在整个更新过程中的系统评估。", "method": "SAGE框架首先根据更新规模确定并扩展嵌入维度空间，然后采用动态蒸馏机制来平衡现有知识的保留和新事实的融入。", "result": "SAGE在七个基准测试中持续优于现有基线，MRR提高了1.38%，H@1提高了1.25%，H@10提高了1.6%。与使用固定嵌入维度的方法相比，SAGE在每个快照上都实现了最佳性能，证明了自适应嵌入维度在CKGE中的重要性。", "conclusion": "SAGE通过其尺度感知和动态蒸馏机制，有效解决了动态知识图谱的持续嵌入问题，尤其是在处理不同规模更新时表现出色，证明了自适应嵌入维度的关键作用。"}}
{"id": "2508.11332", "categories": ["eess.SY", "cs.SY", "eess.SP", "math.OC"], "pdf": "https://arxiv.org/pdf/2508.11332", "abs": "https://arxiv.org/abs/2508.11332", "authors": ["Chris Verhoek", "Ivan Markovsky", "Roland Tóth"], "title": "Direct data-driven interpolation and approximation of linear parameter-varying system trajectories", "comment": "9 pages, 5 figures, submitted for review", "summary": "We consider the problem of estimating missing values in trajectories of\nlinear parameter-varying (LPV) systems. We solve this interpolation problem for\nthe class of shifted-affine LPV systems. Conditions for the existence and\nuniqueness of solutions are given and a direct data-driven algorithm for its\ncomputation is presented, i.e., the data-generating system is not given by a\nparametric model but is implicitly specified by data. We illustrate the\napplicability of the proposed solution on illustrative examples of a\nmass-spring-damper system with exogenous and endogenous parameter variation.", "AI": {"tldr": "本文提出了一种针对移位仿射线性参数变化（LPV）系统轨迹中缺失值的数据驱动插值算法，并给出了解决方案的存在性和唯一性条件。", "motivation": "研究动机是解决线性参数变化（LPV）系统轨迹中缺失值（插值问题）的估计问题。", "method": "该研究针对移位仿射LPV系统，提出了解决方案的存在性和唯一性条件，并开发了一种直接的数据驱动算法来计算缺失值，该算法不依赖于系统的参数模型，而是隐式地由数据指定。", "result": "所提出的解决方案在质量-弹簧-阻尼系统（具有外生和内生参数变化）的示例上得到了验证，展示了其适用性。", "conclusion": "研究成功地为移位仿射LPV系统的轨迹缺失值问题提供了一个数据驱动的插值算法，并验证了其在实际系统中的有效性。"}}
{"id": "2508.11093", "categories": ["cs.RO", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.11093", "abs": "https://arxiv.org/abs/2508.11093", "authors": ["Cesar Alan Contreras", "Manolis Chiou", "Alireza Rastegarpanah", "Michal Szulik", "Rustam Stolkin"], "title": "Utilizing Vision-Language Models as Action Models for Intent Recognition and Assistance", "comment": "Accepted at Human-Centered Robot Autonomy for Human-Robot Teams\n  (HuRoboT) at IEEE RO-MAN 2025, Eindhoven, the Netherlands", "summary": "Human-robot collaboration requires robots to quickly infer user intent,\nprovide transparent reasoning, and assist users in achieving their goals. Our\nrecent work introduced GUIDER, our framework for inferring navigation and\nmanipulation intents. We propose augmenting GUIDER with a vision-language model\n(VLM) and a text-only language model (LLM) to form a semantic prior that\nfilters objects and locations based on the mission prompt. A vision pipeline\n(YOLO for object detection and the Segment Anything Model for instance\nsegmentation) feeds candidate object crops into the VLM, which scores their\nrelevance given an operator prompt; in addition, the list of detected object\nlabels is ranked by a text-only LLM. These scores weight the existing\nnavigation and manipulation layers of GUIDER, selecting context-relevant\ntargets while suppressing unrelated objects. Once the combined belief exceeds a\nthreshold, autonomy changes occur, enabling the robot to navigate to the\ndesired area and retrieve the desired object, while adapting to any changes in\nthe operator's intent. Future work will evaluate the system on Isaac Sim using\na Franka Emika arm on a Ridgeback base, with a focus on real-time assistance.", "AI": {"tldr": "本文提出将GUIDER框架（用于推断导航和操作意图）与视觉-语言模型（VLM）和纯文本语言模型（LLM）结合，形成语义先验，以根据任务提示过滤对象和位置，从而实现人机协作中机器人对用户意图的快速推断和目标协助。", "motivation": "人机协作要求机器人能够快速推断用户意图、提供透明的推理并协助用户实现其目标。", "method": "通过以下方式增强GUIDER框架：1) 利用VLM和LLM作为语义先验，根据任务提示过滤对象和位置。2) 视觉管道（YOLO用于对象检测，Segment Anything Model用于实例分割）将候选对象裁剪输入VLM，由VLM根据操作员提示对其相关性进行评分。3) 纯文本LLM对检测到的对象标签列表进行排名。4) 这些分数用于加权GUIDER现有的导航和操作层，以选择与上下文相关的目标并抑制不相关的对象。", "result": "一旦组合置信度超过阈值，机器人将触发自主行为，使其能够导航到所需区域并取回所需对象，同时适应操作员意图的任何变化。", "conclusion": "该系统旨在提供实时协助。未来的工作将侧重于在Isaac Sim上使用Franka Emika机械臂和Ridgeback底座评估该系统。"}}
{"id": "2508.10971", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10971", "abs": "https://arxiv.org/abs/2508.10971", "authors": ["Nasim Shirvani-Mahdavi", "Chengkai Li"], "title": "Rule2Text: A Framework for Generating and Evaluating Natural Language Explanations of Knowledge Graph Rules", "comment": "arXiv admin note: text overlap with arXiv:2507.23740", "summary": "Knowledge graphs (KGs) can be enhanced through rule mining; however, the\nresulting logical rules are often difficult for humans to interpret due to\ntheir inherent complexity and the idiosyncratic labeling conventions of\nindividual KGs. This work presents Rule2Text, a comprehensive framework that\nleverages large language models (LLMs) to generate natural language\nexplanations for mined logical rules, thereby improving KG accessibility and\nusability. We conduct extensive experiments using multiple datasets, including\nFreebase variants (FB-CVT-REV, FB+CVT-REV, and FB15k-237) as well as the\nogbl-biokg dataset, with rules mined using AMIE 3.5.1. We systematically\nevaluate several LLMs across a comprehensive range of prompting strategies,\nincluding zero-shot, few-shot, variable type incorporation, and\nChain-of-Thought reasoning. To systematically assess models' performance, we\nconduct a human evaluation of generated explanations on correctness and\nclarity. To address evaluation scalability, we develop and validate an\nLLM-as-a-judge framework that demonstrates strong agreement with human\nevaluators. Leveraging the best-performing model (Gemini 2.0 Flash), LLM judge,\nand human-in-the-loop feedback, we construct high-quality ground truth\ndatasets, which we use to fine-tune the open-source Zephyr model. Our results\ndemonstrate significant improvements in explanation quality after fine-tuning,\nwith particularly strong gains in the domain-specific dataset. Additionally, we\nintegrate a type inference module to support KGs lacking explicit type\ninformation. All code and data are publicly available at\nhttps://github.com/idirlab/KGRule2NL.", "AI": {"tldr": "Rule2Text框架利用大型语言模型（LLMs）为知识图谱（KGs）中挖掘的逻辑规则生成自然语言解释，以提高其可理解性和可用性。", "motivation": "知识图谱通过规则挖掘得到增强，但生成的逻辑规则因其复杂性和特定标签约定而难以被人理解，这限制了KG的可访问性和可用性。", "method": "本研究提出了Rule2Text框架，利用LLMs（如Gemini 2.0 Flash和微调后的Zephyr模型）生成规则解释。方法包括：多数据集（Freebase变体、ogbl-biokg）实验；评估多种提示策略（零样本、少样本、变量类型融入、思维链）；通过人工评估和开发LLM-as-a-judge框架评估解释质量；利用LLM判断和人工反馈构建高质量真值数据集来微调开源模型；集成类型推断模块以支持缺少显式类型信息的KG。", "result": "微调后，解释质量显著提高，特别是在领域特定数据集上效果更佳。开发的LLM-as-a-judge框架与人工评估结果高度一致。最佳表现模型（Gemini 2.0 Flash）和微调后的Zephyr模型在生成高质量解释方面表现出色。", "conclusion": "LLMs能够有效且显著地提高知识图谱逻辑规则的自然语言解释质量，从而极大地改善KG的可访问性和可用性。通过微调和类型推断，该方法在不同数据集上均表现出强大的泛化能力和实用性。"}}
{"id": "2508.10934", "categories": ["cs.CV", "cs.GR", "cs.RO", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.10934", "abs": "https://arxiv.org/abs/2508.10934", "authors": ["Jiahui Huang", "Qunjie Zhou", "Hesam Rabeti", "Aleksandr Korovko", "Huan Ling", "Xuanchi Ren", "Tianchang Shen", "Jun Gao", "Dmitry Slepichev", "Chen-Hsuan Lin", "Jiawei Ren", "Kevin Xie", "Joydeep Biswas", "Laura Leal-Taixe", "Sanja Fidler"], "title": "ViPE: Video Pose Engine for 3D Geometric Perception", "comment": "Paper website: https://research.nvidia.com/labs/toronto-ai/vipe/", "summary": "Accurate 3D geometric perception is an important prerequisite for a wide\nrange of spatial AI systems. While state-of-the-art methods depend on\nlarge-scale training data, acquiring consistent and precise 3D annotations from\nin-the-wild videos remains a key challenge. In this work, we introduce ViPE, a\nhandy and versatile video processing engine designed to bridge this gap. ViPE\nefficiently estimates camera intrinsics, camera motion, and dense, near-metric\ndepth maps from unconstrained raw videos. It is robust to diverse scenarios,\nincluding dynamic selfie videos, cinematic shots, or dashcams, and supports\nvarious camera models such as pinhole, wide-angle, and 360{\\deg} panoramas. We\nhave benchmarked ViPE on multiple benchmarks. Notably, it outperforms existing\nuncalibrated pose estimation baselines by 18%/50% on TUM/KITTI sequences, and\nruns at 3-5FPS on a single GPU for standard input resolutions. We use ViPE to\nannotate a large-scale collection of videos. This collection includes around\n100K real-world internet videos, 1M high-quality AI-generated videos, and 2K\npanoramic videos, totaling approximately 96M frames -- all annotated with\naccurate camera poses and dense depth maps. We open-source ViPE and the\nannotated dataset with the hope of accelerating the development of spatial AI\nsystems.", "AI": {"tldr": "ViPE是一个视频处理引擎，能从非受限视频中高效估计相机参数、运动和稠密深度图，并用于大规模视频3D标注，旨在弥补空间AI系统对精准3D训练数据的需求。", "motivation": "当前先进的空间AI系统依赖于大规模训练数据，但从真实视频中获取一致且精确的3D标注是一个关键挑战。", "method": "ViPE通过高效估计相机内参、相机运动和近度量稠密深度图，从非受限原始视频中实现3D几何感知。它对多种场景和相机模型（如针孔、广角、360度全景）具有鲁棒性。", "result": "ViPE在TUM/KITTI序列上分别超越现有未校准姿态估计基线18%/50%，并在单个GPU上以3-5FPS运行。它被用于标注一个包含约10万真实世界视频、100万AI生成视频和2千全景视频的大规模数据集，总计约9600万帧，均带有精确的相机姿态和稠密深度图。", "conclusion": "ViPE及其标注数据集的开源，有望加速空间AI系统的发展，为需要3D几何感知的应用提供高质量的训练数据。"}}
{"id": "2508.11450", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11450", "abs": "https://arxiv.org/abs/2508.11450", "authors": ["Augustine X. W. Lee", "Pak-Hei Yeung", "Jagath C. Rajapakse"], "title": "Subcortical Masks Generation in CT Images via Ensemble-Based Cross-Domain Label Transfer", "comment": null, "summary": "Subcortical segmentation in neuroimages plays an important role in\nunderstanding brain anatomy and facilitating computer-aided diagnosis of\ntraumatic brain injuries and neurodegenerative disorders. However, training\naccurate automatic models requires large amounts of labelled data. Despite the\navailability of publicly available subcortical segmentation datasets for\nMagnetic Resonance Imaging (MRI), a significant gap exists for Computed\nTomography (CT). This paper proposes an automatic ensemble framework to\ngenerate high-quality subcortical segmentation labels for CT scans by\nleveraging existing MRI-based models. We introduce a robust ensembling pipeline\nto integrate them and apply it to unannotated paired MRI-CT data, resulting in\na comprehensive CT subcortical segmentation dataset. Extensive experiments on\nmultiple public datasets demonstrate the superior performance of our proposed\nframework. Furthermore, using our generated CT dataset, we train segmentation\nmodels that achieve improved performance on related segmentation tasks. To\nfacilitate future research, we make our source code, generated dataset, and\ntrained models publicly available at\nhttps://github.com/SCSE-Biomedical-Computing-Group/CT-Subcortical-Segmentation,\nmarking the first open-source release for CT subcortical segmentation to the\nbest of our knowledge.", "AI": {"tldr": "本文提出了一个自动集成框架，利用现有的MRI模型为CT扫描生成高质量的皮层下分割标签，并构建了一个新的CT皮层下分割数据集，以解决CT数据标注稀缺的问题。", "motivation": "皮层下分割在脑部解剖学理解和计算机辅助诊断中至关重要，但高质量的自动模型需要大量标注数据。尽管MRI有公开数据集，CT图像在皮层下分割方面缺乏标注数据，这限制了相关研究和应用。", "method": "本文提出一个鲁棒的自动集成框架。该框架利用现有的MRI模型，通过集成管线将其应用于未标注的配对MRI-CT数据，从而生成CT扫描的皮层下分割标签，并构建了一个全面的CT皮层下分割数据集。", "result": "所提出的框架在多个公开数据集上表现出卓越的性能。使用生成的CT数据集训练的分割模型，在相关分割任务中取得了改进的性能。此外，研究团队公开了源代码、生成的数据集和训练模型，这是首个公开的CT皮层下分割资源。", "conclusion": "该集成框架有效地解决了CT皮层下分割数据稀缺的问题，通过生成高质量的CT标签和公开数据集，显著推动了该领域的研究进展，并有望促进相关诊断应用。"}}
{"id": "2508.11360", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.11360", "abs": "https://arxiv.org/abs/2508.11360", "authors": ["Songqin Nong", "Jingxuan Xu", "Sheng Zhou", "Jianfeng Chen", "Xiaoxuan Tang", "Tao Jiang", "Wenhao Xu"], "title": "CRAFT-GUI: Curriculum-Reinforced Agent For GUI Tasks", "comment": null, "summary": "As autonomous agents become adept at understanding and interacting with\ngraphical user interface (GUI) environments, a new era of automated task\nexecution is emerging. Recent studies have demonstrated that Reinforcement\nLearning (RL) can effectively enhance agents' performance in dynamic\ninteractive GUI environments. However, these methods face two key limitations:\n(1) they overlook the significant variation in difficulty across different GUI\ntasks by treating the entire training data as a uniform set, which hampers the\nagent's ability to adapt its learning process; and (2) most approaches collapse\ntask-specific nuances into a single, coarse reward, leaving the agent with a\nuniform signal that yields inefficient policy updates. To address these\nlimitations, we propose CRAFT-GUI, a curriculum learning framework based on\nGroup Relative Policy Optimization (GRPO) that explicitly accounts for the\nvarying difficulty across trajectories. To enable more fine-grained policy\noptimization, we design a reward function that combines simple rule-based\nsignals with model-judged evaluation, providing richer and more nuanced\nfeedback during training. Experimental results demonstrate that our method\nachieves significant improvements over previous state-of-the-art approaches,\noutperforming them by 5.6% on public benchmarks Android Control and 10.3% on\nour internal online benchmarks, respectively. These findings empirically\nvalidate the effectiveness of integrating reinforcement learning with\ncurriculum learning in GUI interaction tasks.", "AI": {"tldr": "本文提出CRAFT-GUI框架，通过课程学习和精细化奖励函数，显著提升了强化学习代理在图形用户界面（GUI）交互任务中的表现，解决了现有方法忽视任务难度差异和奖励信号粗糙的问题。", "motivation": "现有强化学习方法在GUI交互任务中存在两个主要限制：1) 忽略了不同GUI任务之间难度差异，将所有训练数据视为均匀集合，阻碍了代理适应性学习；2) 大多数方法将任务特定细微差别合并为单一、粗糙的奖励，导致策略更新效率低下。", "method": "本文提出CRAFT-GUI，一个基于群组相对策略优化（GRPO）的课程学习框架，明确考虑了轨迹间变化的难度。为实现更细粒度的策略优化，设计了一个结合简单规则信号和模型评估的奖励函数，在训练期间提供更丰富、更细致的反馈。", "result": "实验结果表明，CRAFT-GUI显著优于现有最先进方法，在公共基准Android Control上性能提升5.6%，在内部在线基准上提升10.3%。", "conclusion": "研究结果经验性地验证了在GUI交互任务中整合强化学习与课程学习的有效性。"}}
{"id": "2508.11381", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.11381", "abs": "https://arxiv.org/abs/2508.11381", "authors": ["Yusen Wei", "Lan Tang"], "title": "System Synchronization Based on Complex Frequency", "comment": null, "summary": "In response to the inertia decline caused by high penetration of renewable\ngeneration, traditional synchronization criteria that rely solely on frequency\nconsistency are increasingly inadequate for characterizing the coupled behavior\nof frequency and voltage dynamics during power-system transients. This paper\nfocuses on the theory of complex-frequency synchronization and develops a\ntheory-simulation analysis framework that offers a new perspective for\nsteady-state and transient analysis of low-inertia power systems. First, the\nfundamental concepts and theoretical foundations of complex-frequency\nsynchronization are presented in detail. Second, local and global dynamic\nsynchronization criteria are derived and the concept of generalized inertia is\nintroduced, which unifies the conventional inertial support to frequency with\nthe inertia-like support of voltage, thereby providing an accurate measure of\nregion-level coupled support strength for voltage and frequency. Finally,\nnumerical case studies on the IEEE 9-bus system validate the effectiveness of\nthe proposed theoretical methods and criteria, and demonstrate a visualization\nworkflow for key indicators such as disturbance impact zones and\ngeneralized-inertia regions.", "AI": {"tldr": "针对可再生能源高渗透导致的惯量下降问题，本文提出了复频率同步理论及分析框架，以更好地分析低惯量电力系统中的频率与电压耦合动态。", "motivation": "可再生能源高渗透导致系统惯量下降，传统仅依赖频率一致性的同步判据不足以表征电力系统暂态过程中频率和电压的耦合行为。", "method": "本文详细阐述了复频率同步的基本概念和理论基础，推导了局部和全局动态同步判据，并引入了广义惯量概念，统一了传统频率惯量支持与电压惯量支持。", "result": "在IEEE 9节点系统上的数值案例研究验证了所提出理论方法和判据的有效性，并展示了扰动影响区和广义惯量区等关键指标的可视化工作流程。", "conclusion": "复频率同步理论和广义惯量概念为低惯量电力系统稳态和暂态分析提供了新视角，能准确衡量区域级频率和电压的耦合支持强度。"}}
{"id": "2508.11117", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11117", "abs": "https://arxiv.org/abs/2508.11117", "authors": ["Xuning Yang", "Clemens Eppner", "Jonathan Tremblay", "Dieter Fox", "Stan Birchfield", "Fabio Ramos"], "title": "Robot Policy Evaluation for Sim-to-Real Transfer: A Benchmarking Perspective", "comment": "2025 Robot: Science and Systems (RSS) Workshop on Robot Evaluation\n  for the Real World", "summary": "Current vision-based robotics simulation benchmarks have significantly\nadvanced robotic manipulation research. However, robotics is fundamentally a\nreal-world problem, and evaluation for real-world applications has lagged\nbehind in evaluating generalist policies. In this paper, we discuss challenges\nand desiderata in designing benchmarks for generalist robotic manipulation\npolicies for the goal of sim-to-real policy transfer. We propose 1) utilizing\nhigh visual-fidelity simulation for improved sim-to-real transfer, 2)\nevaluating policies by systematically increasing task complexity and scenario\nperturbation to assess robustness, and 3) quantifying performance alignment\nbetween real-world performance and its simulation counterparts.", "AI": {"tldr": "现有机器人模拟基准在真实世界泛化策略评估方面滞后。本文提出通过高视觉保真度模拟、系统性增加任务复杂度和扰动来评估鲁棒性，以及量化虚实性能对齐，以改进虚实迁移。", "motivation": "尽管当前的视觉机器人模拟基准在机器人操作研究中取得了显著进展，但在评估泛化机器人操作策略的真实世界应用和虚实迁移方面仍显不足。", "method": "本文提出了设计改进型基准的三个关键要素：1) 利用高视觉保真度模拟以改善虚实迁移；2) 通过系统性增加任务复杂度和场景扰动来评估策略的鲁棒性；3) 量化真实世界性能与其模拟对应物之间的对齐程度。", "result": "本文讨论了设计泛化机器人操作策略基准所面临的挑战和期望目标，并提出了具体的策略来创建能够促进更好虚实迁移并评估泛化策略鲁棒性的基准，旨在弥合模拟与现实之间的差距。", "conclusion": "为了推动泛化机器人操作策略在真实世界应用中的发展，未来的基准应优先考虑高视觉保真度、系统性鲁棒性评估以及量化虚实性能对齐。"}}
{"id": "2508.10995", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10995", "abs": "https://arxiv.org/abs/2508.10995", "authors": ["Tejomay Kishor Padole", "Suyash P Awate", "Pushpak Bhattacharyya"], "title": "Improving Text Style Transfer using Masked Diffusion Language Models with Inference-time Scaling", "comment": "Accepted as a main conference submission in the European Conference\n  on Artificial Intelligence (ECAI 2025)", "summary": "Masked diffusion language models (MDMs) have recently gained traction as a\nviable generative framework for natural language. This can be attributed to its\nscalability and ease of training compared to other diffusion model paradigms\nfor discrete data, establishing itself as the state-of-the-art\nnon-autoregressive generator for discrete data. Diffusion models, in general,\nhave shown excellent ability to improve the generation quality by leveraging\ninference-time scaling either by increasing the number of denoising steps or by\nusing external verifiers on top of the outputs of each step to guide the\ngeneration. In this work, we propose a verifier-based inference-time scaling\nmethod that aids in finding a better candidate generation during the denoising\nprocess of the MDM. Our experiments demonstrate the application of MDMs for\nstandard text-style transfer tasks and establish MDMs as a better alternative\nto autoregressive language models. Additionally, we show that a simple\nsoft-value-based verifier setup for MDMs using off-the-shelf pre-trained\nembedding models leads to significant gains in generation quality even when\nused on top of typical classifier-free guidance setups in the existing\nliterature.", "AI": {"tldr": "本文提出了一种基于验证器的推理时间扩展方法，以提高掩码扩散语言模型（MDMs）的文本生成质量，并在文本风格迁移任务中取得了显著效果。", "motivation": "MDMs是离散数据生成领域最先进的非自回归生成器，具有良好的可扩展性和易于训练的特点。扩散模型通常可以通过增加去噪步骤或使用外部验证器在推理时提高生成质量。本文旨在为MDMs引入类似的推理时增强机制，以进一步提升其生成表现。", "method": "本文提出了一种基于验证器的推理时间扩展方法，用于在MDM的去噪过程中寻找更好的候选生成。具体而言，他们使用了一个简单的、基于软值的验证器，该验证器利用现成的预训练嵌入模型来指导生成。", "result": "实验证明，所提出的方法显著提升了MDMs的生成质量，特别是在标准文本风格迁移任务中，使得MDMs成为自回归语言模型更好的替代方案。即使在现有文献中典型的无分类器指导设置之上，该简单的验证器设置也能带来显著的生成质量提升。", "conclusion": "通过引入基于验证器的推理时间扩展方法，MDMs在文本生成任务中展现出卓越的性能，尤其是在文本风格迁移方面，进一步巩固了其作为离散数据非自回归生成器的领先地位。"}}
{"id": "2508.10935", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.10935", "abs": "https://arxiv.org/abs/2508.10935", "authors": ["Qi Liu", "Yabei Li", "Hongsong Wang", "Lei He"], "title": "HQ-OV3D: A High Box Quality Open-World 3D Detection Framework based on Diffision Model", "comment": null, "summary": "Traditional closed-set 3D detection frameworks fail to meet the demands of\nopen-world applications like autonomous driving. Existing open-vocabulary 3D\ndetection methods typically adopt a two-stage pipeline consisting of\npseudo-label generation followed by semantic alignment. While vision-language\nmodels (VLMs) recently have dramatically improved the semantic accuracy of\npseudo-labels, their geometric quality, particularly bounding box precision,\nremains commonly neglected.To address this issue, we propose a High Box Quality\nOpen-Vocabulary 3D Detection (HQ-OV3D) framework, dedicated to generate and\nrefine high-quality pseudo-labels for open-vocabulary classes. The framework\ncomprises two key components: an Intra-Modality Cross-Validated (IMCV) Proposal\nGenerator that utilizes cross-modality geometric consistency to generate\nhigh-quality initial 3D proposals, and an Annotated-Class Assisted (ACA)\nDenoiser that progressively refines 3D proposals by leveraging geometric priors\nfrom annotated categories through a DDIM-based denoising mechanism.Compared to\nthe state-of-the-art method, training with pseudo-labels generated by our\napproach achieves a 7.37% improvement in mAP on novel classes, demonstrating\nthe superior quality of the pseudo-labels produced by our framework. HQ-OV3D\ncan serve not only as a strong standalone open-vocabulary 3D detector but also\nas a plug-in high-quality pseudo-label generator for existing open-vocabulary\ndetection or annotation pipelines.", "AI": {"tldr": "该论文提出HQ-OV3D框架，旨在生成和优化高质量的伪标签，以提升开放词汇3D检测中边界框的几何精度。", "motivation": "传统的封闭集3D检测框架无法满足开放世界应用的需求，而现有开放词汇3D检测方法虽然提升了伪标签的语义准确性，但普遍忽视了其几何质量，特别是边界框的精度。", "method": "提出了一个名为HQ-OV3D的开放词汇3D检测框架，包含两个核心组件：1) 模态内交叉验证(IMCV)提议生成器，利用跨模态几何一致性生成高质量的初始3D提议；2) 标注类别辅助(ACA)去噪器，通过基于DDIM的去噪机制，利用已标注类别的几何先验知识逐步细化3D提议。", "result": "与现有最先进的方法相比，使用该方法生成的伪标签进行训练，在新类别上的mAP提高了7.37%，证明了其伪标签的卓越质量。", "conclusion": "HQ-OV3D不仅可以作为强大的独立开放词汇3D检测器，还可以作为现有开放词汇检测或标注流程的高质量伪标签生成器插件。"}}
{"id": "2508.11511", "categories": ["eess.IV", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11511", "abs": "https://arxiv.org/abs/2508.11511", "authors": ["Siyamalan Manivannan"], "title": "Semi-Supervised Learning with Online Knowledge Distillation for Skin Lesion Classification", "comment": null, "summary": "Deep Learning has emerged as a promising approach for skin lesion analysis.\nHowever, existing methods mostly rely on fully supervised learning, requiring\nextensive labeled data, which is challenging and costly to obtain. To alleviate\nthis annotation burden, this study introduces a novel semi-supervised deep\nlearning approach that integrates ensemble learning with online knowledge\ndistillation for enhanced skin lesion classification. Our methodology involves\ntraining an ensemble of convolutional neural network models, using online\nknowledge distillation to transfer insights from the ensemble to its members.\nThis process aims to enhance the performance of each model within the ensemble,\nthereby elevating the overall performance of the ensemble itself.\nPost-training, any individual model within the ensemble can be deployed at test\ntime, as each member is trained to deliver comparable performance to the\nensemble. This is particularly beneficial in resource-constrained environments.\nExperimental results demonstrate that the knowledge-distilled individual model\nperforms better than independently trained models. Our approach demonstrates\nsuperior performance on both the \\emph{International Skin Imaging\nCollaboration} 2018 and 2019 public benchmark datasets, surpassing current\nstate-of-the-art results. By leveraging ensemble learning and online knowledge\ndistillation, our method reduces the need for extensive labeled data while\nproviding a more resource-efficient solution for skin lesion classification in\nreal-world scenarios.", "AI": {"tldr": "本研究提出了一种结合集成学习和在线知识蒸馏的半监督深度学习方法，用于皮肤病变分类，旨在减少对大量标注数据的依赖，并在资源受限环境下实现高性能。", "motivation": "现有皮肤病变分析的深度学习方法主要依赖于全监督学习，需要大量且昂贵的标注数据，这在获取上具有挑战性。", "method": "该研究引入了一种新颖的半监督深度学习方法，将集成学习与在线知识蒸馏相结合。具体而言，训练一个卷积神经网络模型集成，并使用在线知识蒸馏将集成模型的知识转移给其成员，以提升每个成员模型的性能，进而提高整个集成的表现。训练后，集成中的任一成员模型均可单独部署。", "result": "实验结果表明，经过知识蒸馏的单个模型比独立训练的模型表现更好。该方法在国际皮肤影像协作组织2018和2019年公共基准数据集上均展现出卓越性能，超越了当前最先进的结果。", "conclusion": "通过利用集成学习和在线知识蒸馏，该方法减少了对大量标注数据的需求，并为真实世界中的皮肤病变分类提供了一个更具资源效率的解决方案。"}}
{"id": "2508.11416", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11416", "abs": "https://arxiv.org/abs/2508.11416", "authors": ["Xuhua Zhao", "Yuxuan Xie", "Caihua Chen", "Yuxiang Sun"], "title": "AIM-Bench: Evaluating Decision-making Biases of Agentic LLM as Inventory Manager", "comment": null, "summary": "Recent advances in mathematical reasoning and the long-term planning\ncapabilities of large language models (LLMs) have precipitated the development\nof agents, which are being increasingly leveraged in business operations\nprocesses. Decision models to optimize inventory levels are one of the core\nelements of operations management. However, the capabilities of the LLM agent\nin making inventory decisions in uncertain contexts, as well as the\ndecision-making biases (e.g. framing effect, etc.) of the agent, remain largely\nunexplored. This prompts concerns regarding the capacity of LLM agents to\neffectively address real-world problems, as well as the potential implications\nof biases that may be present. To address this gap, we introduce AIM-Bench, a\nnovel benchmark designed to assess the decision-making behaviour of LLM agents\nin uncertain supply chain management scenarios through a diverse series of\ninventory replenishment experiments. Our results reveal that different LLMs\ntypically exhibit varying degrees of decision bias that are similar to those\nobserved in human beings. In addition, we explored strategies to mitigate the\npull-to-centre effect and the bullwhip effect, namely cognitive reflection and\nimplementation of information sharing. These findings underscore the need for\ncareful consideration of the potential biases in deploying LLMs in Inventory\ndecision-making scenarios. We hope that these insights will pave the way for\nmitigating human decision bias and developing human-centred decision support\nsystems for supply chains.", "AI": {"tldr": "该研究引入AIM-Bench基准，评估LLM代理在不确定库存决策中的行为和偏差，发现LLM展现出类似人类的决策偏差，并探索了缓解偏差的策略，强调部署LLM时需谨慎考虑其潜在偏差。", "motivation": "尽管LLM代理在业务运营中日益普及，但其在不确定环境下进行库存决策的能力及其决策偏差（如框架效应）尚未充分探索，这引发了对其解决实际问题能力和潜在偏差影响的担忧。", "method": "研究引入了一个名为AIM-Bench的新型基准，通过一系列多样化的库存补货实验，评估LLM代理在不确定供应链管理场景中的决策行为。", "result": "结果显示，不同的LLM通常表现出不同程度的决策偏差，这些偏差与人类观察到的偏差相似。此外，研究探索了认知反思和信息共享等策略，以缓解“趋中效应”和“牛鞭效应”。", "conclusion": "研究强调在库存决策场景中部署LLM时，需要仔细考虑其潜在偏差。这些发现有望为减轻人类决策偏差和开发以人为本的供应链决策支持系统铺平道路。"}}
{"id": "2508.11422", "categories": ["eess.SY", "cs.SY", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2508.11422", "abs": "https://arxiv.org/abs/2508.11422", "authors": ["Victoria S. Marks", "Joram vanRheede", "Dean Karantonis", "Rosana Esteller", "David Dinsmoor", "John Fleming", "Barrett Larson", "Lane Desborough", "Peter Single", "Robert Raike", "Pierre-Francois DHaese", "Dario J. Englot", "Scott Lempka", "Richard North", "Lawrence Poree", "Marom Bikson", "Tim J. Denison"], "title": "Principles of Physiological Closed-Loop Controllers in Neuromodulation", "comment": "35 pages, 10 figures", "summary": "As neurostimulation devices increasingly incorporate closed-loop\nfunctionality, the greater design complexity brings additional requirements for\nrisk management and special considerations to optimise benefit. This manuscript\ncreates a common framework upon which all current and planned\nneuromodulation-based physiological closed-loop controllers (PCLCs) can be\nmapped including integration of the Technical Considerations of Medical Devices\nwith Physiologic Closed-Loop Control Technology guidance published in 2023 by\nthe United States Food and Drug Administration (FDA), a classification of\nfeedback (reactive) and feedforward (predictive) biomarkers, and control\nsystems theory. We explain risk management in the context of this framework and\nillustrate its applications for three exemplary technologies. This manuscript\nserves as guidance to the emerging field of PCLCs in neuromodulation,\nmitigating risk through standardized nomenclature and a systematic outline for\nrigorous device development, testing, and implementation.", "AI": {"tldr": "本文提出了一个通用框架，用于映射神经调节领域的生理闭环控制器（PCLCs），整合了FDA指南、生物标志物分类和控制系统理论，旨在标准化命名并系统化设备开发，以优化效益并降低风险。", "motivation": "随着神经刺激设备日益集成闭环功能，其设计复杂性增加，对风险管理和效益优化提出了额外要求。", "method": "创建了一个通用框架，将所有当前和计划的基于神经调节的PCLCs纳入其中，整合了美国FDA在2023年发布的《具有生理闭环控制技术的医疗设备技术考虑指南》、反馈（反应性）和前馈（预测性）生物标志物的分类以及控制系统理论。通过三个示例技术说明了该框架在风险管理中的应用。", "result": "建立了一个统一的框架，能够映射所有神经调节领域的PCLCs，并在此框架下解释了风险管理。该框架为新兴的神经调节PCLC领域提供了指导，通过标准化命名和系统化的开发、测试与实施大纲来降低风险。", "conclusion": "该框架通过标准化命名和系统化的设备开发、测试与实施大纲，为神经调节领域的PCLCs提供了指导，有效降低了相关风险。"}}
{"id": "2508.11129", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.11129", "abs": "https://arxiv.org/abs/2508.11129", "authors": ["Ryan M. Bena", "Gilbert Bahati", "Blake Werner", "Ryan K. Cosner", "Lizhi Yang", "Aaron D. Ames"], "title": "Geometry-Aware Predictive Safety Filters on Humanoids: From Poisson Safety Functions to CBF Constrained MPC", "comment": "2025 IEEE-RAS 24th International Conference on Humanoid Robots", "summary": "Autonomous navigation through unstructured and dynamically-changing\nenvironments is a complex task that continues to present many challenges for\nmodern roboticists. In particular, legged robots typically possess manipulable\nasymmetric geometries which must be considered during safety-critical\ntrajectory planning. This work proposes a predictive safety filter: a nonlinear\nmodel predictive control (MPC) algorithm for online trajectory generation with\ngeometry-aware safety constraints based on control barrier functions (CBFs).\nCritically, our method leverages Poisson safety functions to numerically\nsynthesize CBF constraints directly from perception data. We extend the\ntheoretical framework for Poisson safety functions to incorporate temporal\nchanges in the domain by reformulating the static Dirichlet problem for\nPoisson's equation as a parameterized moving boundary value problem.\nFurthermore, we employ Minkowski set operations to lift the domain into a\nconfiguration space that accounts for robot geometry. Finally, we implement our\nreal-time predictive safety filter on humanoid and quadruped robots in various\nsafety-critical scenarios. The results highlight the versatility of Poisson\nsafety functions, as well as the benefit of CBF constrained model predictive\nsafety-critical controllers.", "AI": {"tldr": "该研究提出了一种基于泊松安全函数（从感知数据合成CBF约束）和模型预测控制（MPC）的预测安全滤波器，用于在动态环境中为具有复杂几何形状的腿式机器人进行实时、几何感知的安全轨迹规划。", "motivation": "在非结构化和动态变化的环境中进行自主导航，特别是对于具有可操纵不对称几何形状的腿式机器人，其安全关键轨迹规划面临巨大挑战。", "method": "该方法提出一个预测安全滤波器，即一个非线性模型预测控制（MPC）算法，用于在线轨迹生成。它利用基于控制障碍函数（CBF）的几何感知安全约束。关键在于，它利用泊松安全函数直接从感知数据中数值合成CBF约束。为处理时域变化，该研究将泊松方程的静态狄利克雷问题重新表述为参数化移动边界值问题。此外，采用闵可夫斯基集运算将域提升到配置空间以考虑机器人几何形状。", "result": "该实时预测安全滤波器已在类人机器人和四足机器人上，于各种安全关键场景中实现。结果突出了泊松安全函数的多功能性，以及CBF约束模型预测安全关键控制器的益处。", "conclusion": "泊松安全函数具有多功能性，并且结合CBF约束的模型预测控制器在安全关键场景中表现出显著优势，能有效解决腿式机器人在复杂动态环境下的安全导航问题。"}}
{"id": "2508.11009", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11009", "abs": "https://arxiv.org/abs/2508.11009", "authors": ["Wenpeng Xing", "Lanyi Wei", "Haixiao Hu", "Rongchang Li", "Mohan Li", "Changting Lin", "Meng Han"], "title": "SproutBench: A Benchmark for Safe and Ethical Large Language Models for Youth", "comment": null, "summary": "The rapid proliferation of large language models (LLMs) in applications\ntargeting children and adolescents necessitates a fundamental reassessment of\nprevailing AI safety frameworks, which are largely tailored to adult users and\nneglect the distinct developmental vulnerabilities of minors. This paper\nhighlights key deficiencies in existing LLM safety benchmarks, including their\ninadequate coverage of age-specific cognitive, emotional, and social risks\nspanning early childhood (ages 0--6), middle childhood (7--12), and adolescence\n(13--18). To bridge these gaps, we introduce SproutBench, an innovative\nevaluation suite comprising 1,283 developmentally grounded adversarial prompts\ndesigned to probe risks such as emotional dependency, privacy violations, and\nimitation of hazardous behaviors. Through rigorous empirical evaluation of 47\ndiverse LLMs, we uncover substantial safety vulnerabilities, corroborated by\nrobust inter-dimensional correlations (e.g., between Safety and Risk\nPrevention) and a notable inverse relationship between Interactivity and Age\nAppropriateness. These insights yield practical guidelines for advancing\nchild-centric AI design and deployment.", "AI": {"tldr": "LLM应用于儿童需重新评估现有AI安全框架。本文引入SproutBench评估儿童特定风险，发现LLM存在严重安全漏洞，并提出儿童AI设计指南。", "motivation": "大语言模型（LLMs）正迅速应用于儿童和青少年，但现有AI安全框架主要针对成人，忽视了未成年人独特的发育脆弱性。现有LLM安全基准未能充分覆盖儿童年龄特定的认知、情感和社会风险。", "method": "引入了SproutBench，一个包含1,283个基于发展阶段设计的对抗性提示的创新评估套件，旨在探测情感依赖、隐私侵犯和危险行为模仿等风险。对47个不同LLM进行了严格的实证评估。", "result": "在47个LLM中发现了实质性的安全漏洞；证实了安全与风险预防等维度之间存在稳健的相互关联；交互性与年龄适宜性之间存在显著的负相关关系。", "conclusion": "这些发现为推进以儿童为中心的AI设计和部署提供了实用指导。"}}
{"id": "2508.10936", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.10936", "abs": "https://arxiv.org/abs/2508.10936", "authors": ["Cheng Chen", "Hao Huang", "Saurabh Bagchi"], "title": "Vision-Only Gaussian Splatting for Collaborative Semantic Occupancy Prediction", "comment": null, "summary": "Collaborative perception enables connected vehicles to share information,\novercoming occlusions and extending the limited sensing range inherent in\nsingle-agent (non-collaborative) systems. Existing vision-only methods for 3D\nsemantic occupancy prediction commonly rely on dense 3D voxels, which incur\nhigh communication costs, or 2D planar features, which require accurate depth\nestimation or additional supervision, limiting their applicability to\ncollaborative scenarios. To address these challenges, we propose the first\napproach leveraging sparse 3D semantic Gaussian splatting for collaborative 3D\nsemantic occupancy prediction. By sharing and fusing intermediate Gaussian\nprimitives, our method provides three benefits: a neighborhood-based\ncross-agent fusion that removes duplicates and suppresses noisy or inconsistent\nGaussians; a joint encoding of geometry and semantics in each primitive, which\nreduces reliance on depth supervision and allows simple rigid alignment; and\nsparse, object-centric messages that preserve structural information while\nreducing communication volume. Extensive experiments demonstrate that our\napproach outperforms single-agent perception and baseline collaborative methods\nby +8.42 and +3.28 points in mIoU, and +5.11 and +22.41 points in IoU,\nrespectively. When further reducing the number of transmitted Gaussians, our\nmethod still achieves a +1.9 improvement in mIoU, using only 34.6%\ncommunication volume, highlighting robust performance under limited\ncommunication budgets.", "AI": {"tldr": "该论文提出了一种基于稀疏3D语义高斯泼溅的协同3D语义占用预测方法，通过共享和融合高斯基元，有效降低通信成本，提高感知性能，并对深度估计的依赖性降低。", "motivation": "现有面向视觉的3D语义占用预测方法存在高通信成本（密集3D体素）或需要精确深度估计/额外监督（2D平面特征）的问题，这限制了它们在协同场景中的应用。单智能体系统也面临遮挡和感知范围有限的挑战。", "method": "该方法首次将稀疏3D语义高斯泼溅应用于协同3D语义占用预测。主要通过以下方式实现：1) 基于邻域的跨智能体融合，去除重复并抑制噪声/不一致的高斯；2) 每个基元中几何和语义的联合编码，减少对深度监督的依赖并简化刚性对齐；3) 稀疏、以物体为中心的消息，在保留结构信息的同时减少通信量。", "result": "实验结果表明，该方法在mIoU上比单智能体感知和基线协同方法分别提高了+8.42和+3.28点，在IoU上分别提高了+5.11和+22.41点。即使在通信量减少34.6%的情况下，mIoU仍能提高+1.9点，显示了在有限通信预算下的鲁棒性能。", "conclusion": "所提出的基于稀疏3D语义高斯泼溅的协同感知方法，在提高3D语义占用预测性能的同时，显著降低了通信成本，并减少了对深度监督的依赖，在协同驾驶场景中表现出卓越的有效性和鲁棒性。"}}
{"id": "2508.10946", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.10946", "abs": "https://arxiv.org/abs/2508.10946", "authors": ["Wonho Lee", "Hyunsik Na", "Jisu Lee", "Daeseon Choi"], "title": "IPG: Incremental Patch Generation for Generalized Adversarial Patch Training", "comment": null, "summary": "The advent of adversarial patches poses a significant challenge to the\nrobustness of AI models, particularly in the domain of computer vision tasks\nsuch as object detection. In contradistinction to traditional adversarial\nexamples, these patches target specific regions of an image, resulting in the\nmalfunction of AI models. This paper proposes Incremental Patch Generation\n(IPG), a method that generates adversarial patches up to 11.1 times more\nefficiently than existing approaches while maintaining comparable attack\nperformance. The efficacy of IPG is demonstrated by experiments and ablation\nstudies including YOLO's feature distribution visualization and adversarial\ntraining results, which show that it produces well-generalized patches that\neffectively cover a broader range of model vulnerabilities. Furthermore,\nIPG-generated datasets can serve as a robust knowledge foundation for\nconstructing a robust model, enabling structured representation, advanced\nreasoning, and proactive defenses in AI security ecosystems. The findings of\nthis study suggest that IPG has considerable potential for future utilization\nnot only in adversarial patch defense but also in real-world applications such\nas autonomous vehicles, security systems, and medical imaging, where AI models\nmust remain resilient to adversarial attacks in dynamic and high-stakes\nenvironments.", "AI": {"tldr": "本文提出了一种名为IPG的增量补丁生成方法，能以更高效率生成对抗补丁，并保持攻击性能，同时有助于构建更鲁棒的AI模型。", "motivation": "对抗补丁对AI模型（特别是计算机视觉中的目标检测任务）的鲁棒性构成了重大挑战，它们通过针对图像特定区域导致模型失效。", "method": "本文提出增量补丁生成（IPG）方法，旨在更高效地生成对抗补丁。", "result": "IPG生成对抗补丁的效率比现有方法高出11.1倍，同时保持相当的攻击性能。实验和消融研究（包括YOLO特征分布可视化和对抗训练结果）表明，IPG能生成泛化性良好且有效覆盖更广泛模型漏洞的补丁。此外，IPG生成的数据集可作为构建鲁棒模型的知识基础。", "conclusion": "IPG在对抗补丁防御以及自动驾驶、安全系统和医学成像等真实世界应用中具有巨大潜力，有助于AI模型在动态高风险环境中保持对对抗攻击的韧性。"}}
{"id": "2508.11452", "categories": ["cs.AI", "cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.11452", "abs": "https://arxiv.org/abs/2508.11452", "authors": ["Kangyu Wang", "Hongliang He", "Lin Liu", "Ruiqi Liang", "Zhenzhong Lan", "Jianguo Li"], "title": "Inclusion Arena: An Open Platform for Evaluating Large Foundation Models with Real-World Apps", "comment": "Our platform is publicly accessible at\n  https://doraemon.alipay.com/model-ranking", "summary": "Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs)\nhave ushered in a new era of AI capabilities, demonstrating near-human-level\nperformance across diverse scenarios. While numerous benchmarks (e.g., MMLU)\nand leaderboards (e.g., Chatbot Arena) have been proposed to help evolve the\ndevelopment of LLMs and MLLMs, most rely on static datasets or crowdsourced\ngeneral-domain prompts, often falling short of reflecting performance in\nreal-world applications. To bridge this critical gap, we present Inclusion\nArena, a live leaderboard that ranks models based on human feedback collected\ndirectly from AI-powered applications. Our platform integrates pairwise model\ncomparisons into natural user interactions, ensuring evaluations reflect\npractical usage scenarios. For robust model ranking, we employ the\nBradley-Terry model augmented with two key innovations: (1) Placement Matches,\na cold-start mechanism to quickly estimate initial ratings for newly integrated\nmodels, and (2) Proximity Sampling, an intelligent comparison strategy that\nprioritizes battles between models of similar capabilities to maximize\ninformation gain and enhance rating stability. Extensive empirical analyses and\nsimulations demonstrate that Inclusion Arena yields reliable and stable\nrankings, exhibits higher data transitivity compared to general crowdsourced\ndatasets, and significantly mitigates the risk of malicious manipulation. By\nfostering an open alliance between foundation models and real-world\napplications, Inclusion Arena aims to accelerate the development of LLMs and\nMLLMs truly optimized for practical, user-centric deployments. The platform is\npublicly accessible at https://doraemon.alipay.com/model-ranking.", "AI": {"tldr": "Inclusion Arena是一个实时排行榜，通过收集AI应用中的真实用户反馈进行模型两两比较，并结合改进的Bradley-Terry模型，对LLM和MLLM进行可靠且稳定的排名，以反映其在实际应用中的性能。", "motivation": "现有的大语言模型（LLMs）和多模态大语言模型（MLLMs）基准测试（如MMLU）和排行榜（如Chatbot Arena）多依赖静态数据集或通用领域众包提示，无法准确反映模型在真实世界应用中的性能，因此需要一个能弥合这一差距的评估平台。", "method": "该研究提出了Inclusion Arena平台，将模型两两比较集成到用户与AI应用的自然交互中，直接收集人类反馈。排名采用Bradley-Terry模型，并引入两项创新：1) 冷启动机制“Placement Matches”快速估算新模型初始评分；2) “Proximity Sampling”智能比较策略，优先选择能力相近的模型进行对战，以最大化信息增益并增强评分稳定性。", "result": "Inclusion Arena的实证分析和模拟结果表明，它能产生可靠和稳定的排名，与通用众包数据集相比具有更高的数据传递性，并显著降低了恶意操纵的风险。", "conclusion": "Inclusion Arena旨在通过促进基础模型与真实世界应用之间的开放联盟，加速LLM和MLLM的发展，使其真正为实际、以用户为中心的部署而优化。"}}
{"id": "2508.11533", "categories": ["eess.SY", "cs.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2508.11533", "abs": "https://arxiv.org/abs/2508.11533", "authors": ["Yicheng Lin", "Bingxian Wu", "Nan Bai", "Zhiyong Sun", "Yunxiao Ren", "Chuanze Chen", "Zhisheng Duan"], "title": "Integrating Uncertainties for Koopman-Based Stabilization", "comment": null, "summary": "Over the past decades, the Koopman operator has been widely applied in\ndata-driven control, yet its theoretical foundations remain underexplored. This\npaper establishes a unified framework to address the robust stabilization\nproblem in data-driven control via the Koopman operator, fully accounting for\nthree uncertainties: projection error, estimation error, and process\ndisturbance. It comprehensively investigates both direct and indirect\ndata-driven control approaches, facilitating flexible methodology selection for\nanalysis and control. For the direct approach, considering process\ndisturbances, the lifted-state feedback controller, designed via a linear\nmatrix inequality (LMI), robustly stabilizes all lifted bilinear systems\nconsistent with noisy data. For the indirect approach requiring system\nidentification, the feedback controller, designed using a nonlinear matrix\ninequality convertible to an LMI, ensures closed-loop stability under\nworst-case process disturbances. Numerical simulations via cross-validation\nvalidate the effectiveness of both approaches, highlighting their theoretical\nsignificance and practical utility.", "AI": {"tldr": "本文提出一个统一框架，利用Koopman算子解决数据驱动控制中的鲁棒稳定化问题，全面考虑投影误差、估计误差和过程扰动三种不确定性，并验证了直接和间接两种方法的有效性。", "motivation": "Koopman算子在数据驱动控制中应用广泛，但其理论基础仍未被充分探索，尤其是在处理不确定性情况下的鲁棒稳定化问题。", "method": "建立了一个统一框架来处理鲁棒稳定化问题，考虑了投影误差、估计误差和过程扰动。研究了直接和间接两种数据驱动控制方法：直接方法通过线性矩阵不等式（LMI）设计控制器；间接方法通过可转换为LMI的非线性矩阵不等式设计控制器。通过交叉验证的数值模拟验证了两种方法的有效性。", "result": "对于直接方法，LMI设计的控制器能够鲁棒地稳定与噪声数据一致的所有提升双线性系统；对于间接方法，设计的控制器在最坏情况过程扰动下确保闭环稳定性。数值模拟验证了两种方法的有效性。", "conclusion": "所提出的直接和间接数据驱动控制方法在理论上具有重要意义，并在实践中具有实用价值，能够有效解决Koopman算子在数据驱动控制中的鲁棒稳定化问题。"}}
{"id": "2508.11143", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11143", "abs": "https://arxiv.org/abs/2508.11143", "authors": ["Jiarui Yang", "Bin Zhu", "Jingjing Chen", "Yu-Gang Jiang"], "title": "Actor-Critic for Continuous Action Chunks: A Reinforcement Learning Framework for Long-Horizon Robotic Manipulation with Sparse Reward", "comment": null, "summary": "Existing reinforcement learning (RL) methods struggle with long-horizon\nrobotic manipulation tasks, particularly those involving sparse rewards. While\naction chunking is a promising paradigm for robotic manipulation, using RL to\ndirectly learn continuous action chunks in a stable and data-efficient manner\nremains a critical challenge. This paper introduces AC3 (Actor-Critic for\nContinuous Chunks), a novel RL framework that learns to generate\nhigh-dimensional, continuous action sequences. To make this learning process\nstable and data-efficient, AC3 incorporates targeted stabilization mechanisms\nfor both the actor and the critic. First, to ensure reliable policy\nimprovement, the actor is trained with an asymmetric update rule, learning\nexclusively from successful trajectories. Second, to enable effective value\nlearning despite sparse rewards, the critic's update is stabilized using\nintra-chunk $n$-step returns and further enriched by a self-supervised module\nproviding intrinsic rewards at anchor points aligned with each action chunk. We\nconducted extensive experiments on 25 tasks from the BiGym and RLBench\nbenchmarks. Results show that by using only a few demonstrations and a simple\nmodel architecture, AC3 achieves superior success rates on most tasks,\nvalidating its effective design.", "AI": {"tldr": "AC3是一种新型强化学习框架，通过稳定策略和价值更新来学习连续动作块，有效解决了长时程、稀疏奖励机器人操作任务的挑战，并在多个基准测试中取得了优异表现。", "motivation": "现有强化学习方法难以处理长时程、稀疏奖励的机器人操作任务。尽管动作分块是一个有前景的范式，但如何稳定且数据高效地学习连续动作块仍是关键挑战。", "method": "本文提出了AC3（Actor-Critic for Continuous Chunks）框架。为实现稳定和数据高效学习，AC3为策略（Actor）和价值（Critic）引入了专门的稳定机制：策略通过非对称更新规则，仅从成功轨迹中学习；价值通过块内n步回报稳定更新，并由一个自监督模块在每个动作块的锚点提供内在奖励进行丰富。", "result": "在BiGym和RLBench基准测试的25项任务上进行了广泛实验。结果表明，AC3仅使用少量演示和简单的模型架构，就在大多数任务上取得了卓越的成功率。", "conclusion": "AC3通过其有效的设计，成功验证了学习连续动作块在解决长时程、稀疏奖励机器人操作任务方面的有效性，并展现出优异的性能。"}}
{"id": "2508.11017", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11017", "abs": "https://arxiv.org/abs/2508.11017", "authors": ["Carter Blum", "Katja Filipova", "Ann Yuan", "Asma Ghandeharioun", "Julian Zimmert", "Fred Zhang", "Jessica Hoffmann", "Tal Linzen", "Martin Wattenberg", "Lucas Dixon", "Mor Geva"], "title": "Beyond the Rosetta Stone: Unification Forces in Generalization Dynamics", "comment": null, "summary": "Large language models (LLMs) struggle with cross-lingual knowledge transfer:\nthey hallucinate when asked in one language about facts expressed in a\ndifferent language during training. This work introduces a controlled setting\nto study the causes and dynamics of this phenomenon by training small\nTransformer models from scratch on synthetic multilingual datasets. We identify\na learning phase wherein a model develops either separate or unified\nrepresentations of the same facts across languages, and show that unification\nis essential for cross-lingual transfer. We also show that the degree of\nunification depends on mutual information between facts and training data\nlanguage, and on how easy it is to extract that language. Based on these\ninsights, we develop methods to modulate the level of cross-lingual transfer by\nmanipulating data distribution and tokenization, and we introduce metrics and\nvisualizations to formally characterize their effects on unification. Our work\nshows how controlled settings can shed light on pre-training dynamics and\nsuggests new directions for improving cross-lingual transfer in LLMs.", "AI": {"tldr": "大型语言模型在跨语言知识迁移中存在幻觉问题。本文通过在受控设置下训练小型Transformer模型，研究了该现象的原因和机制，发现统一表示对跨语言迁移至关重要，并提出了改进方法。", "motivation": "大型语言模型（LLMs）在跨语言知识迁移方面表现不佳，当被问及以一种语言表达但在训练数据中以另一种语言呈现的事实时，它们会出现幻觉。", "method": "本文在受控环境下，使用合成多语言数据集从头开始训练小型Transformer模型。研究了模型在学习阶段如何形成跨语言事实的独立或统一表示，并分析了事实与训练数据语言之间的互信息以及语言可提取性对统一程度的影响。在此基础上，开发了通过操纵数据分布和分词来调节跨语言迁移水平的方法，并引入了度量指标和可视化工具来表征其对统一的影响。", "result": "研究发现模型存在一个学习阶段，在此阶段模型会形成跨语言事实的独立或统一表示，且统一表示对于跨语言迁移至关重要。统一的程度取决于事实与训练数据语言之间的互信息以及该语言的可提取性。基于这些见解，开发了通过数据分布和分词操纵来调节跨语言迁移水平的方法，并引入了度量指标和可视化工具来量化其效果。", "conclusion": "受控设置能够揭示预训练的动态过程，并为改进大型语言模型中的跨语言迁移提供了新的方向。"}}
{"id": "2508.10937", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10937", "abs": "https://arxiv.org/abs/2508.10937", "authors": ["Jiarui Yang", "Hang Guo", "Wen Huang", "Tao Dai", "Shutao Xia"], "title": "Personalized Face Super-Resolution with Identity Decoupling and Fitting", "comment": null, "summary": "In recent years, face super-resolution (FSR) methods have achieved remarkable\nprogress, generally maintaining high image fidelity and identity (ID)\nconsistency under standard settings. However, in extreme degradation scenarios\n(e.g., scale $> 8\\times$), critical attributes and ID information are often\nseverely lost in the input image, making it difficult for conventional models\nto reconstruct realistic and ID-consistent faces. Existing methods tend to\ngenerate hallucinated faces under such conditions, producing restored images\nlacking authentic ID constraints. To address this challenge, we propose a novel\nFSR method with Identity Decoupling and Fitting (IDFSR), designed to enhance ID\nrestoration under large scaling factors while mitigating hallucination effects.\nOur approach involves three key designs: 1) \\textbf{Masking} the facial region\nin the low-resolution (LR) image to eliminate unreliable ID cues; 2)\n\\textbf{Warping} a reference image to align with the LR input, providing style\nguidance; 3) Leveraging \\textbf{ID embeddings} extracted from ground truth (GT)\nimages for fine-grained ID modeling and personalized adaptation. We first\npretrain a diffusion-based model to explicitly decouple style and ID by forcing\nit to reconstruct masked LR face regions using both style and identity\nembeddings. Subsequently, we freeze most network parameters and perform\nlightweight fine-tuning of the ID embedding using a small set of target ID\nimages. This embedding encodes fine-grained facial attributes and precise ID\ninformation, significantly improving both ID consistency and perceptual\nquality. Extensive quantitative evaluations and visual comparisons demonstrate\nthat the proposed IDFSR substantially outperforms existing approaches under\nextreme degradation, particularly achieving superior performance on ID\nconsistency.", "AI": {"tldr": "该论文提出了一种名为IDFSR的新型人脸超分辨率方法，旨在极端降级（如大缩放因子）下提高身份（ID）一致性并减少幻觉效应。", "motivation": "在极端降级场景（如缩放因子大于8倍）下，现有的人脸超分辨率方法难以重建真实且身份一致的人脸，因为输入图像中关键属性和身份信息严重丢失，导致生成缺乏真实身份约束的幻觉人脸。", "method": "IDFSR方法包含三个关键设计：1) 遮蔽低分辨率图像中的面部区域以消除不可靠的身份线索；2) 扭曲参考图像以与低分辨率输入对齐，提供风格指导；3) 利用从真实图像中提取的身份嵌入进行精细粒度的身份建模和个性化适应。该方法首先预训练一个基于扩散的模型，通过强制其使用风格和身份嵌入重建被遮蔽的低分辨率面部区域来显式解耦风格和身份。随后，冻结大部分网络参数，并使用少量目标身份图像对身份嵌入进行轻量级微调。", "result": "广泛的定量评估和视觉比较表明，所提出的IDFSR在极端降级下显著优于现有方法，尤其在身份一致性方面表现出卓越的性能。", "conclusion": "IDFSR通过其新颖的身份解耦和拟合方法，有效解决了极端人脸超分辨率的挑战，显著提高了身份一致性和感知质量。"}}
{"id": "2508.11493", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11493", "abs": "https://arxiv.org/abs/2508.11493", "authors": ["David H. Chan", "Mark Roberts", "Dana S. Nau"], "title": "Landmark-Assisted Monte Carlo Planning", "comment": "To be published in the Proceedings of the 28th European Conference on\n  Artificial Intelligence", "summary": "Landmarks$\\unicode{x2013}$conditions that must be satisfied at some point in\nevery solution plan$\\unicode{x2013}$have contributed to major advancements in\nclassical planning, but they have seldom been used in stochastic domains. We\nformalize probabilistic landmarks and adapt the UCT algorithm to leverage them\nas subgoals to decompose MDPs; core to the adaptation is balancing between\ngreedy landmark achievement and final goal achievement. Our results in\nbenchmark domains show that well-chosen landmarks can significantly improve the\nperformance of UCT in online probabilistic planning, while the best balance of\ngreedy versus long-term goal achievement is problem-dependent. The results\nsuggest that landmarks can provide helpful guidance for anytime algorithms\nsolving MDPs.", "AI": {"tldr": "本文将经典规划中的“地标”概念推广到随机领域，形式化了概率地标，并将其整合到UCT算法中作为子目标来分解MDP，以提升在线概率规划的性能。", "motivation": "地标在经典规划中对提升性能有显著贡献，但在随机领域（如MDPs）中却很少被使用，这促使研究者探索如何将地标应用于随机规划以改善算法表现。", "method": "研究者形式化了概率地标，并对UCT算法进行了调整，使其能够利用这些地标作为子目标来分解MDP问题。该方法的核心在于平衡贪婪地实现地标与达成最终目标之间的关系。", "result": "在基准测试领域的结果显示，选择得当的地标可以显著提高UCT在在线概率规划中的性能。同时，贪婪地标实现与长期目标实现之间的最佳平衡点是依赖于具体问题的。", "conclusion": "研究结果表明，地标可以为解决MDP的任何时间算法提供有益的指导，具有在随机规划中推广应用的潜力。"}}
{"id": "2508.11561", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.11561", "abs": "https://arxiv.org/abs/2508.11561", "authors": ["Kaustav Chatterjee", "Sameer Nekkalapu", "Sayak Mukherjee", "Ramij Raja Hossain", "Marcelo Elizondo"], "title": "Identification of Sub/Super-Synchronous Control Interaction Paths Using Dissipative Energy Flow", "comment": "5 pages, 11 figures", "summary": "Sub- and super-synchronous control interactions (SSCIs) are oscillations\narising from adverse interactions between inverter-based resource (IBR)\ncontrols and the power network. SSCIs often involve multiple frequencies and\npropagate through complex, interconnected paths, making it difficult for\nmodel-based approaches to identify both the sources and the paths of\noscillatory energy flow. This paper extends the Dissipative Energy Flow (DEF)\nmethod, originally developed for low-frequency electromechanical oscillations,\nto identify SSCI sources and dynamic interaction paths across multiple\nfrequencies using three-phase voltage and current measurements. The approach\noperates in the dq frame using dynamic phasors, enabling mode-specific DEF\ncomputation from bandpass-filtered signals. An electromagnetic transient (EMT)\ncase study on a meshed network with synchronous generator and type-3 wind farm\nresources under series-compensated conditions demonstrates the method's\ncapability to distinguish frequency-dependent source and sink roles, including\ncases where the same resource acts as a source at one frequency and a sink at\nanother. The results show DEF can provide a physics-based and\nautomation-friendly tool for SSCI diagnosis in IBR-rich grids.", "AI": {"tldr": "本文提出了一种扩展的耗散能量流（DEF）方法，用于通过三相电压和电流测量识别逆变器基资源（IBR）引起的次同步和超同步控制相互作用（SSCI）的源头和动态交互路径。", "motivation": "次同步和超同步控制相互作用（SSCI）是电力系统中由逆变器基资源（IBR）控制与电网之间不良交互引起的振荡。这些振荡通常涉及多个频率，并通过复杂互联的路径传播，使得传统的基于模型的方法难以识别振荡能量流的来源和路径。", "method": "该研究将最初用于低频机电振荡的耗散能量流（DEF）方法进行了扩展，以识别多频率下的SSCI源和动态交互路径。该方法在dq坐标系下，利用动态相量，通过带通滤波信号计算模式特定的DEF。", "result": "电磁暂态（EMT）案例研究表明，该方法能够区分频率相关的源和汇角色，包括同一资源在不同频率下既可作为源也可作为汇的情况。结果证明DEF可以为富含IBR的电网中的SSCI诊断提供一个基于物理且易于自动化的工具。", "conclusion": "扩展的耗散能量流（DEF）方法是一种有效且自动化的工具，能够基于测量数据诊断逆变器基资源（IBR）丰富的电网中的次同步和超同步控制相互作用（SSCI），并识别其能量源和传播路径。"}}
{"id": "2508.11200", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11200", "abs": "https://arxiv.org/abs/2508.11200", "authors": ["Hongbin Lin", "Bin Li", "Kwok Wai Samuel Au"], "title": "Visuomotor Grasping with World Models for Surgical Robots", "comment": null, "summary": "Grasping is a fundamental task in robot-assisted surgery (RAS), and\nautomating it can reduce surgeon workload while enhancing efficiency, safety,\nand consistency beyond teleoperated systems. Most prior approaches rely on\nexplicit object pose tracking or handcrafted visual features, limiting their\ngeneralization to novel objects, robustness to visual disturbances, and the\nability to handle deformable objects. Visuomotor learning offers a promising\nalternative, but deploying it in RAS presents unique challenges, such as low\nsignal-to-noise ratio in visual observations, demands for high safety and\nmillimeter-level precision, as well as the complex surgical environment. This\npaper addresses three key challenges: (i) sim-to-real transfer of visuomotor\npolicies to ex vivo surgical scenes, (ii) visuomotor learning using only a\nsingle stereo camera pair -- the standard RAS setup, and (iii) object-agnostic\ngrasping with a single policy that generalizes to diverse, unseen surgical\nobjects without retraining or task-specific models. We introduce Grasp Anything\nfor Surgery V2 (GASv2), a visuomotor learning framework for surgical grasping.\nGASv2 leverages a world-model-based architecture and a surgical perception\npipeline for visual observations, combined with a hybrid control system for\nsafe execution. We train the policy in simulation using domain randomization\nfor sim-to-real transfer and deploy it on a real robot in both phantom-based\nand ex vivo surgical settings, using only a single pair of endoscopic cameras.\nExtensive experiments show our policy achieves a 65% success rate in both\nsettings, generalizes to unseen objects and grippers, and adapts to diverse\ndisturbances, demonstrating strong performance, generality, and robustness.", "AI": {"tldr": "本文提出了一种名为GASv2的视觉运动学习框架，用于在机器人辅助手术中实现物体无关的抓取，通过模拟到真实世界的迁移和单目立体相机部署，在未见物体和复杂环境下表现出强大的泛化性和鲁棒性。", "motivation": "自动化机器人辅助手术中的抓取任务可以减轻外科医生负担，提高效率、安全性和一致性。现有方法（如姿态跟踪或手工特征）在泛化性、鲁棒性和处理变形物体方面存在局限。视觉运动学习虽有潜力，但在机器人辅助手术中面临独特挑战，如视觉信噪比低、对安全和毫米级精度的高要求以及复杂的手术环境。", "method": "本文提出了Grasp Anything for Surgery V2 (GASv2) 视觉运动学习框架，旨在解决三个关键挑战：(i) 视觉运动策略从模拟到离体手术场景的迁移，(ii) 仅使用标准机器人辅助手术设置中的单个立体相机进行视觉运动学习，以及 (iii) 无需重新训练或任务特定模型即可泛化到多样化、未见手术物体的物体无关抓取。GASv2利用基于世界模型的架构和手术感知管道处理视觉观测，并结合混合控制系统确保安全执行。策略在模拟中通过领域随机化进行训练以实现模拟到真实世界的迁移，并在真实机器人上（包括基于模型和离体手术设置）仅使用一对内窥镜相机进行部署。", "result": "广泛的实验表明，该策略在两种设置中均达到了65%的成功率，能够泛化到未见的物体和抓取器，并适应各种干扰，展现出强大的性能、通用性和鲁棒性。", "conclusion": "GASv2框架成功克服了在机器人辅助手术中部署视觉运动策略的关键挑战，实现了对未见物体和复杂环境的鲁棒、通用和物体无关的抓取，为机器人辅助手术的自动化抓取提供了有效解决方案。"}}
{"id": "2508.11027", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11027", "abs": "https://arxiv.org/abs/2508.11027", "authors": ["Andrew Wang", "Sophia Hager", "Adi Asija", "Daniel Khashabi", "Nicholas Andrews"], "title": "Hell or High Water: Evaluating Agentic Recovery from External Failures", "comment": "Accepted to COLM 2025", "summary": "As language model agents are applied to real world problems of increasing\ncomplexity, they will be expected to formulate plans across large search\nspaces. If those plans fail for reasons beyond their control, how well do\nlanguage agents search for alternative ways to achieve their goals? We devise a\nspecialized agentic planning benchmark to study this question. Each planning\nproblem is solved via combinations of function calls. The agent searches for\nrelevant functions from a set of over four thousand possibilities, and observes\nenvironmental feedback in the form of function outputs or error messages. Our\nbenchmark confronts the agent with external failures in its workflow, such as\nfunctions that suddenly become unavailable. At the same time, even with the\nintroduction of these failures, we guarantee that the task remains solvable.\nIdeally, an agent's performance on the planning task should not be affected by\nthe presence of external failures. Overall, we find that language agents\nstruggle to formulate and execute backup plans in response to environment\nfeedback. While state-of-the-art models are often able to identify the correct\nfunction to use in the right context, they struggle to adapt to feedback from\nthe environment and often fail to pursue alternate courses of action, even when\nthe search space is artificially restricted. We provide a systematic analysis\nof the failures of both open-source and commercial models, examining the\neffects of search space size, as well as the benefits of scaling model size in\nour setting. Our analysis identifies key challenges for current generative\nmodels as well as promising directions for future work.", "AI": {"tldr": "研究发现，当语言模型代理在复杂任务中遇到外部故障时，它们难以制定和执行备用计划，即使任务仍可解决。", "motivation": "随着语言模型代理被应用于日益复杂的现实世界问题，它们需要在大搜索空间中制定计划。当计划因不可控原因失败时，评估语言代理寻找替代方案以实现目标的能力至关重要。", "method": "研究设计了一个专门的代理规划基准。规划问题通过函数调用组合解决，代理从超过四千个函数中搜索相关函数，并观察环境反馈（函数输出或错误信息）。基准测试引入了外部故障（如函数突然不可用），同时保证任务仍可解决。", "result": "研究发现，语言代理在响应环境反馈时难以制定和执行备用计划。尽管最先进的模型通常能在正确上下文中识别正确的函数，但它们难以适应环境反馈，并且常常无法寻求替代行动方案，即使搜索空间被人为限制。研究还系统分析了开源和商业模型的失败情况，并检查了搜索空间大小的影响以及模型规模在此设置中的益处。", "conclusion": "当前生成模型在处理外部故障和制定备用计划方面面临关键挑战，需要进一步研究以提高其适应性和鲁棒性。"}}
{"id": "2508.10938", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10938", "abs": "https://arxiv.org/abs/2508.10938", "authors": ["Tianyu Song", "Van-Doan Duong", "Thi-Phuong Le", "Ton Viet Ta"], "title": "Deep Learning for Automated Identification of Vietnamese Timber Species: A Tool for Ecological Monitoring and Conservation", "comment": null, "summary": "Accurate identification of wood species plays a critical role in ecological\nmonitoring, biodiversity conservation, and sustainable forest management.\nTraditional classification approaches relying on macroscopic and microscopic\ninspection are labor-intensive and require expert knowledge. In this study, we\nexplore the application of deep learning to automate the classification of ten\nwood species commonly found in Vietnam. A custom image dataset was constructed\nfrom field-collected wood samples, and five state-of-the-art convolutional\nneural network architectures--ResNet50, EfficientNet, MobileViT, MobileNetV3,\nand ShuffleNetV2--were evaluated. Among these, ShuffleNetV2 achieved the best\nbalance between classification performance and computational efficiency, with\nan average accuracy of 99.29\\% and F1-score of 99.35\\% over 20 independent\nruns. These results demonstrate the potential of lightweight deep learning\nmodels for real-time, high-accuracy species identification in\nresource-constrained environments. Our work contributes to the growing field of\necological informatics by providing scalable, image-based solutions for\nautomated wood classification and forest biodiversity assessment.", "AI": {"tldr": "本研究利用深度学习自动化识别越南十种常见木材，发现ShuffleNetV2在准确性和计算效率之间取得了最佳平衡。", "motivation": "传统的木材识别方法劳动密集且需要专家知识，而准确的木材种类识别对生态监测、生物多样性保护和可持续森林管理至关重要。", "method": "构建了一个包含野外采集木材样本的定制图像数据集，并评估了五种先进的卷积神经网络架构（ResNet50、EfficientNet、MobileViT、MobileNetV3和ShuffleNetV2）。", "result": "ShuffleNetV2在20次独立运行中表现最佳，平均准确率达到99.29%，F1分数达到99.35%，在分类性能和计算效率之间实现了最佳平衡。", "conclusion": "轻量级深度学习模型在资源受限环境中进行实时、高精度木材种类识别具有巨大潜力，为生态信息学领域提供了可扩展的基于图像的自动化木材分类解决方案。"}}
{"id": "2508.11524", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11524", "abs": "https://arxiv.org/abs/2508.11524", "authors": ["Wenkai Yu", "Jianhang Tang", "Yang Zhang", "Shanjiang Tang", "Kebing Jin", "Hankz Hankui Zhuo"], "title": "Inspire or Predict? Exploring New Paradigms in Assisting Classical Planners with Large Language Models", "comment": null, "summary": "Addressing large-scale planning problems has become one of the central\nchallenges in the planning community, deriving from the state-space explosion\ncaused by growing objects and actions. Recently, researchers have explored the\neffectiveness of leveraging Large Language Models (LLMs) to generate helpful\nactions and states to prune the search space. However, prior works have largely\noverlooked integrating LLMs with domain-specific knowledge to ensure valid\nplans. In this paper, we propose a novel LLM-assisted planner integrated with\nproblem decomposition, which first decomposes large planning problems into\nmultiple simpler sub-tasks. Then we explore two novel paradigms to utilize\nLLMs, i.e., LLM4Inspire and LLM4Predict, to assist problem decomposition, where\nLLM4Inspire provides heuristic guidance according to general knowledge and\nLLM4Predict employs domain-specific knowledge to infer intermediate conditions.\nWe empirically validate the effectiveness of our planner across multiple\ndomains, demonstrating the ability of search space partition when solving\nlarge-scale planning problems. The experimental results show that LLMs\neffectively locate feasible solutions when pruning the search space, where\ninfusing domain-specific knowledge into LLMs, i.e., LLM4Predict, holds\nparticular promise compared with LLM4Inspire, which offers general knowledge\nwithin LLMs.", "AI": {"tldr": "本文提出一种结合问题分解的LLM辅助规划器，通过LLM4Inspire（通用启发式）和LLM4Predict（领域特定知识）两种范式协助分解，有效解决大规模规划问题并剪枝搜索空间。", "motivation": "大规模规划问题面临状态空间爆炸挑战，现有利用LLM剪枝搜索空间的工作忽视了与领域特定知识的结合以确保规划有效性。", "method": "提出一种新颖的LLM辅助规划器，首先将大型规划问题分解为多个子任务。然后探索两种LLM利用范式：LLM4Inspire（提供通用知识启发式指导）和LLM4Predict（利用领域特定知识推断中间条件）来辅助问题分解。", "result": "该规划器在多个领域验证了有效性，展示了解决大规模规划问题时搜索空间划分的能力。实验结果表明LLM在剪枝搜索空间时能有效定位可行解，其中注入领域特定知识的LLM4Predict比提供通用知识的LLM4Inspire更具前景。", "conclusion": "LLM能够有效协助大规模规划问题的求解，特别是通过问题分解和搜索空间剪枝。将领域特定知识融入LLM（LLM4Predict）对于确保有效规划并提升性能至关重要。"}}
{"id": "2508.11612", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.11612", "abs": "https://arxiv.org/abs/2508.11612", "authors": ["Samuel G. Gessow", "James Tseng", "Eden Zafran", "Brett T. Lopez"], "title": "Two-Impulse Trajectory Design in Two-Body Systems With Riemannian Geometry", "comment": null, "summary": "This work presents a new method for generating impulsive trajectories in\nrestricted two-body systems by leveraging Riemannian geometry. The proposed\nmethod transforms the standard trajectory optimization problem into a purely\ngeometric one that involves computing a set of geodesics for a suitable\nRiemannian metric. This transformation is achieved by defining a metric,\nspecifically the Jacobi metric, that embeds the dynamics directly into the\nmetric, so any geodesic of the metric is also a dynamically feasible\ntrajectory. The method finds the fuel-optimal transfer trajectory by sampling\ncandidate energy ($\\Delta V$) changes for different points on the current and\ndesired orbit, and efficiently computing and evaluating each candidate\ngeodesic, which are equivalent to candidate orbit transfer trajectories via the\nJacobi metric. The method bypasses the known issues of optimization-based\nmethods, e.g., sensitivity to the initial guess, and can be applied to more\ncomplex two-body systems. The approach is demonstrated on the minimum-$\\Delta\nV$ two-impulse phase-free orbit transfer problem, first on a Keplerian system\nand second on a system with a modeled $J_2$ perturbation. The proposed method\nis shown to meet or exceed the state-of-the-art methods in the minimum-$\\Delta\nV$ problem in the Keplerian system. The generality and versatility of the\napproach is demonstrated by seamlessly including the $J_2$ perturbation, a case\nthat many existing methods cannot handle. Numerical simulations and performance\ncomparisons showcase the effectiveness of the approach.", "AI": {"tldr": "本文提出了一种利用黎曼几何和Jacobi度量生成脉冲轨迹的新方法，将轨迹优化问题转化为测地线计算，解决了传统优化方法的痛点，并能处理摄动。", "motivation": "传统的优化方法在生成脉冲轨迹时存在已知问题，例如对初始猜测的敏感性。本研究旨在提出一种更鲁棒、更通用的方法来克服这些限制。", "method": "该方法将标准轨迹优化问题转化为纯几何问题，即计算特定黎曼度量（特别是Jacobi度量）下的测地线。通过将动力学直接嵌入到Jacobi度量中，任何测地线都成为动力学上可行的轨迹。通过采样当前和目标轨道上不同点的候选能量（ΔV）变化，并高效计算和评估每个候选测地线（即候选轨道转移轨迹），来找到燃料最优的转移轨迹。", "result": "该方法在最小ΔV两脉冲无相位轨道转移问题上进行了验证，包括开普勒系统和包含J2摄动的系统。结果表明，在开普勒系统中，该方法达到或超过了现有最先进的方法。该方法能够无缝包含J2摄动，而这是许多现有方法无法处理的情况，展示了其通用性和多功能性。数值模拟和性能比较证明了该方法的有效性。", "conclusion": "利用黎曼几何和Jacobi度量的方法为脉冲轨迹生成提供了一种有效、鲁棒且通用的新途径。它克服了传统优化方法的局限性，并在处理复杂摄动系统方面表现出色，具有超越现有技术的潜力。"}}
{"id": "2508.11204", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11204", "abs": "https://arxiv.org/abs/2508.11204", "authors": ["Hongbin Lin", "Juan Rojas", "Kwok Wai Samuel Au"], "title": "Multi-Group Equivariant Augmentation for Reinforcement Learning in Robot Manipulation", "comment": null, "summary": "Sampling efficiency is critical for deploying visuomotor learning in\nreal-world robotic manipulation. While task symmetry has emerged as a promising\ninductive bias to improve efficiency, most prior work is limited to isometric\nsymmetries -- applying the same group transformation to all task objects across\nall timesteps. In this work, we explore non-isometric symmetries, applying\nmultiple independent group transformations across spatial and temporal\ndimensions to relax these constraints. We introduce a novel formulation of the\npartially observable Markov decision process (POMDP) that incorporates the\nnon-isometric symmetry structures, and propose a simple yet effective data\naugmentation method, Multi-Group Equivariance Augmentation (MEA). We integrate\nMEA with offline reinforcement learning to enhance sampling efficiency, and\nintroduce a voxel-based visual representation that preserves translational\nequivariance. Extensive simulation and real-robot experiments across two\nmanipulation domains demonstrate the effectiveness of our approach.", "AI": {"tldr": "本文探索了机器人视觉运动学习中的非等距对称性，提出了一种新的POMDP公式和数据增强方法（MEA），以提高采样效率，并在模拟和真实机器人实验中验证了其有效性。", "motivation": "在现实世界机器人操作中，采样效率至关重要。现有工作大多局限于等距对称性（对所有任务对象在所有时间步应用相同的群变换），这限制了效率提升。因此，需要探索更通用的非等距对称性来放宽这些约束。", "method": "引入了非等距对称性，通过在空间和时间维度上应用多个独立的群变换。提出了包含非等距对称结构的部分可观察马尔可夫决策过程（POMDP）的新公式。提出了一种简单有效的数据增强方法：多群等变增强（MEA），并将其与离线强化学习相结合。引入了一种保留平移等变性的体素基视觉表示。", "result": "在两个操作领域的广泛模拟和真实机器人实验中，证明了所提方法的有效性。", "conclusion": "通过探索非等距对称性并引入MEA数据增强方法，结合离线强化学习和体素基视觉表示，可以显著提高机器人视觉运动学习的采样效率，从而更好地应用于实际机器人操作。"}}
{"id": "2508.11061", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11061", "abs": "https://arxiv.org/abs/2508.11061", "authors": ["Martin Pavlíček", "Tomáš Filip", "Petr Sosík"], "title": "BIPOLAR: Polarization-based granular framework for LLM bias evaluation", "comment": null, "summary": "Large language models (LLMs) are known to exhibit biases in downstream tasks,\nespecially when dealing with sensitive topics such as political discourse,\ngender identity, ethnic relations, or national stereotypes. Although\nsignificant progress has been made in bias detection and mitigation techniques,\ncertain challenges remain underexplored. This study proposes a reusable,\ngranular, and topic-agnostic framework to evaluate polarisation-related biases\nin LLM (both open-source and closed-source). Our approach combines\npolarisation-sensitive sentiment metrics with a synthetically generated\nbalanced dataset of conflict-related statements, using a predefined set of\nsemantic categories.\n  As a case study, we created a synthetic dataset that focusses on the\nRussia-Ukraine war, and we evaluated the bias in several LLMs: Llama-3,\nMistral, GPT-4, Claude 3.5, and Gemini 1.0. Beyond aggregate bias scores, with\na general trend for more positive sentiment toward Ukraine, the framework\nallowed fine-grained analysis with considerable variation between semantic\ncategories, uncovering divergent behavioural patterns among models. Adaptation\nto prompt modifications showed further bias towards preconceived language and\ncitizenship modification.\n  Overall, the framework supports automated dataset generation and fine-grained\nbias assessment, is applicable to a variety of polarisation-driven scenarios\nand topics, and is orthogonal to many other bias-evaluation strategies.", "AI": {"tldr": "本研究提出一个可重用、细粒度且与主题无关的框架，用于评估大型语言模型（LLMs）在敏感话题上的极化相关偏见，并通过俄乌战争案例研究，揭示了不同模型在情感倾向和行为模式上的差异。", "motivation": "大型语言模型在下游任务中存在偏见，尤其是在政治、性别、民族等敏感话题上。尽管偏见检测和缓解技术已取得进展，但仍有未充分探索的挑战。", "method": "本研究提出一个可重用、细粒度、与主题无关的框架，用于评估LLM（包括开源和闭源）的极化相关偏见。该方法结合了极化敏感情感指标和通过预定义语义类别合成生成的冲突相关平衡数据集。以俄乌战争为例，评估了Llama-3、Mistral、GPT-4、Claude 3.5和Gemini 1.0等LLM的偏见。还分析了提示词修改对偏见的影响。", "result": "研究发现，总体而言，LLM对乌克兰表现出更积极的情感倾向。该框架支持细粒度分析，揭示了不同语义类别之间存在显著差异，以及模型间行为模式的多样性。适应提示词修改后，模型对预设语言和国籍修改表现出进一步的偏见。", "conclusion": "该框架支持自动化数据集生成和细粒度偏见评估，适用于各种由极化驱动的场景和主题，并且与许多其他偏见评估策略正交。"}}
{"id": "2508.10940", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10940", "abs": "https://arxiv.org/abs/2508.10940", "authors": ["Nirmal Gaud", "Krishna Kumar Jha", "Jhimli Adhikari", "Adhini Nasarin P S", "Joydeep Das", "Samarth S Deshpande", "Nitasha Barara", "Vaduguru Venkata Ramya", "Santu Saha", "Mehmet Tarik Baran", "Sarangi Venkateshwarlu", "Anusha M D", "Surej Mouli", "Preeti Katiyar", "Vipin Kumar Chaudhary"], "title": "NIRMAL Pooling: An Adaptive Max Pooling Approach with Non-linear Activation for Enhanced Image Classification", "comment": "6 pages, 2 figures", "summary": "This paper presents NIRMAL Pooling, a novel pooling layer for Convolutional\nNeural Networks (CNNs) that integrates adaptive max pooling with non-linear\nactivation function for image classification tasks. The acronym NIRMAL stands\nfor Non-linear Activation, Intermediate Aggregation, Reduction, Maximum,\nAdaptive, and Localized. By dynamically adjusting pooling parameters based on\ndesired output dimensions and applying a Rectified Linear Unit (ReLU)\nactivation post-pooling, NIRMAL Pooling improves robustness and feature\nexpressiveness. We evaluated its performance against standard Max Pooling on\nthree benchmark datasets: MNIST Digits, MNIST Fashion, and CIFAR-10. NIRMAL\nPooling achieves test accuracies of 99.25% (vs. 99.12% for Max Pooling) on\nMNIST Digits, 91.59% (vs. 91.44%) on MNIST Fashion, and 70.49% (vs. 68.87%) on\nCIFAR-10, demonstrating consistent improvements, particularly on complex\ndatasets. This work highlights the potential of NIRMAL Pooling to enhance CNN\nperformance in diverse image recognition tasks, offering a flexible and\nreliable alternative to traditional pooling methods.", "AI": {"tldr": "本文提出了一种名为NIRMAL Pooling的新型卷积神经网络池化层，它结合了自适应最大池化和非线性激活函数，在图像分类任务上表现出比传统最大池化更好的性能。", "motivation": "为了提高卷积神经网络在图像分类任务中的鲁棒性和特征表达能力，通过动态调整池化参数并应用非线性激活函数来改进现有池化方法。", "method": "提出了NIRMAL Pooling，该方法集成了自适应最大池化和ReLU非线性激活函数。通过在MNIST Digits、MNIST Fashion和CIFAR-10三个基准数据集上与标准最大池化进行对比评估其性能。", "result": "NIRMAL Pooling在MNIST Digits上达到99.25%的测试准确率（对比标准最大池化为99.12%），在MNIST Fashion上达到91.59%（对比91.44%），在CIFAR-10上达到70.49%（对比68.87%），在所有数据集上均显示出一致的性能提升，尤其是在复杂数据集上。", "conclusion": "NIRMAL Pooling能够有效提升CNN在多样化图像识别任务中的性能，为传统池化方法提供了一个灵活可靠的替代方案。"}}
{"id": "2508.10956", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10956", "abs": "https://arxiv.org/abs/2508.10956", "authors": ["Abhishek Kolari", "Mohammadhossein Khojasteh", "Yifan Jiang", "Floris den Hengst", "Filip Ilievski"], "title": "ORBIT: An Object Property Reasoning Benchmark for Visual Inference Tasks", "comment": null, "summary": "While vision-language models (VLMs) have made remarkable progress on many\npopular visual question answering (VQA) benchmarks, it remains unclear whether\nthey abstract and reason over depicted objects. Inspired by human object\ncategorisation, object property reasoning involves identifying and recognising\nlow-level details and higher-level abstractions. While current VQA benchmarks\nconsider a limited set of object property attributes like size, they typically\nblend perception and reasoning, and lack representativeness in terms of\nreasoning and image categories. To this end, we introduce a systematic\nevaluation framework with images of three representative types, three reasoning\nlevels of increasing complexity, and four object property dimensions driven by\nprior work on commonsense reasoning. We develop a procedure to instantiate this\nbenchmark into ORBIT, a multi-level reasoning VQA benchmark for object\nproperties comprising 360 images paired with a total of 1,080 count-based\nquestions. Experiments with 12 state-of-the-art VLMs in zero-shot settings\nreveal significant limitations compared to humans, with the best-performing\nmodel only reaching 40\\% accuracy. VLMs struggle particularly with realistic\n(photographic) images, counterfactual reasoning about physical and functional\nproperties, and higher counts. ORBIT points to the need to develop methods for\nscalable benchmarking, generalize annotation guidelines, and explore additional\nreasoning VLMs. We make the ORBIT benchmark and the experimental code available\nto support such endeavors.", "AI": {"tldr": "该研究引入了ORBIT，一个新的多层次视觉问答（VQA）基准，用于评估视觉语言模型（VLMs）在对象属性抽象和推理方面的能力，发现当前VLMs在这方面与人类表现差距显著。", "motivation": "尽管VLMs在VQA基准上取得了显著进展，但它们是否真正抽象和推理图像中的对象仍不清楚。现有VQA基准在对象属性（如尺寸）方面有限，且常将感知与推理混淆，同时在推理和图像类别方面缺乏代表性。", "method": "研究构建了一个系统的评估框架，包含三种代表性图像类型、三个递增复杂度的推理级别和四个基于常识推理的对象属性维度。基于此，开发了ORBIT基准，包含360张图像和1080个基于计数的配对问题。在零样本设置下，对12个最先进的VLMs进行了实验评估。", "result": "实验结果显示，与人类相比，VLMs存在显著局限性，表现最佳的模型准确率仅为40%。VLMs在处理真实（摄影）图像、关于物理和功能属性的反事实推理以及更高计数时表现尤为困难。", "conclusion": "ORBIT基准揭示了当前VLMs在对象属性推理方面的不足，指出了未来需要开发可扩展的基准测试方法、推广标注指南，并探索更多关注推理的VLMs。"}}
{"id": "2508.11520", "categories": ["cs.RO", "cs.SY", "eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2508.11520", "abs": "https://arxiv.org/abs/2508.11520", "authors": ["Evangelos Tsiatsianas", "Chairi Kiourt", "Konstantinos Chatzilygeroudis"], "title": "A Comparative Study of Floating-Base Space Parameterizations for Agile Whole-Body Motion Planning", "comment": "8 pages, 2 figures, 4 tables, Accepted at Humanoids 2025", "summary": "Automatically generating agile whole-body motions for legged and humanoid\nrobots remains a fundamental challenge in robotics. While numerous trajectory\noptimization approaches have been proposed, there is no clear guideline on how\nthe choice of floating-base space parameterization affects performance,\nespecially for agile behaviors involving complex contact dynamics. In this\npaper, we present a comparative study of different parameterizations for direct\ntranscription-based trajectory optimization of agile motions in legged systems.\nWe systematically evaluate several common choices under identical optimization\nsettings to ensure a fair comparison. Furthermore, we introduce a novel\nformulation based on the tangent space of SE(3) for representing the robot's\nfloating-base pose, which, to our knowledge, has not received attention from\nthe literature. This approach enables the use of mature off-the-shelf numerical\nsolvers without requiring specialized manifold optimization techniques. We hope\nthat our experiments and analysis will provide meaningful insights for\nselecting the appropriate floating-based representation for agile whole-body\nmotion generation.", "AI": {"tldr": "本文比较并引入了一种新的浮动基座姿态参数化方法，用于机器人敏捷全身运动的轨迹优化，旨在为选择合适的表示提供指导。", "motivation": "为腿式和人形机器人自动生成敏捷的全身运动仍然是机器人学的一个基本挑战。尽管已提出许多轨迹优化方法，但对于浮动基座空间参数化如何影响性能，特别是涉及复杂接触动力学的敏捷行为，尚无明确指导。", "method": "本文采用基于直接转录的轨迹优化方法，系统地比较了几种常见的浮动基座参数化方案，并在相同的优化设置下进行评估。此外，引入了一种基于SE(3)切线空间的新颖浮动基座姿态表示方法，该方法无需专门的流形优化技术，即可使用成熟的现成数值求解器。", "result": "通过实验和分析，本研究旨在为敏捷全身运动生成中选择合适的浮动基座表示提供有意义的见解。", "conclusion": "该研究的实验和分析将为选择合适的浮动基座表示以生成敏捷全身运动提供有价值的洞察和指导。"}}
{"id": "2508.11232", "categories": ["cs.RO", "cs.NI"], "pdf": "https://arxiv.org/pdf/2508.11232", "abs": "https://arxiv.org/abs/2508.11232", "authors": ["Guoliang Li", "Xibin Jin", "Yujie Wan", "Chenxuan Liu", "Tong Zhang", "Shuai Wang", "Chengzhong Xu"], "title": "Embodied Edge Intelligence Meets Near Field Communication: Concept, Design, and Verification", "comment": "9 pages, 6 figures, to appear in IEEE Network", "summary": "Realizing embodied artificial intelligence is challenging due to the huge\ncomputation demands of large models (LMs). To support LMs while ensuring\nreal-time inference, embodied edge intelligence (EEI) is a promising paradigm,\nwhich leverages an LM edge to provide computing powers in close proximity to\nembodied robots. Due to embodied data exchange, EEI requires higher spectral\nefficiency, enhanced communication security, and reduced inter-user\ninterference. To meet these requirements, near-field communication (NFC), which\nleverages extremely large antenna arrays as its hardware foundation, is an\nideal solution. Therefore, this paper advocates the integration of EEI and NFC,\nresulting in a near-field EEI (NEEI) paradigm. However, NEEI also introduces\nnew challenges that cannot be adequately addressed by isolated EEI or NFC\ndesigns, creating research opportunities for joint optimization of both\nfunctionalities. To this end, we propose radio-friendly embodied planning for\nEEI-assisted NFC scenarios and view-guided beam-focusing for NFC-assisted EEI\nscenarios. We also elaborate how to realize resource-efficient NEEI through\nopportunistic collaborative navigation. Experimental results are provided to\nconfirm the superiority of the proposed techniques compared with various\nbenchmarks.", "AI": {"tldr": "本文提出近场具身边缘智能（NEEI）范式，通过结合具身边缘智能（EEI）和近场通信（NFC）来解决具身AI中大型模型计算需求和实时推理的挑战，并提出了相应的优化技术。", "motivation": "具身人工智能中的大型模型（LMs）计算需求巨大，难以实现实时推理。具身边缘智能（EEI）虽有潜力，但面临数据交换带来的高频谱效率、增强通信安全和减少用户间干扰等挑战。近场通信（NFC）基于超大天线阵列，被认为是解决这些通信问题的理想方案。", "method": "本文倡导将EEI与NFC集成，形成近场具身边缘智能（NEEI）范式。为应对NEEI带来的新挑战，提出了针对EEI辅助NFC场景的“射频友好具身规划”和针对NFC辅助EEI场景的“视觉引导波束聚焦”技术。此外，还通过“机会协作导航”实现资源高效的NEEI。", "result": "实验结果证实，与各种基准方法相比，所提出的技术具有优越性。", "conclusion": "NEEI是一个有前景的范式，能够有效支持具身人工智能中的大型模型实时推理需求，通过联合优化EEI和NFC，可以克服现有挑战，实现资源高效的具身智能。"}}
{"id": "2508.11068", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11068", "abs": "https://arxiv.org/abs/2508.11068", "authors": ["Nicolas Goulet", "Alexandre Blondin Massé", "Moussa Abdendi"], "title": "Approaching the Source of Symbol Grounding with Confluent Reductions of Abstract Meaning Representation Directed Graphs", "comment": null, "summary": "Abstract meaning representation (AMR) is a semantic formalism used to\nrepresent the meaning of sentences as directed acyclic graphs. In this paper,\nwe describe how real digital dictionaries can be embedded into AMR directed\ngraphs (digraphs), using state-of-the-art pre-trained large language models.\nThen, we reduce those graphs in a confluent manner, i.e. with transformations\nthat preserve their circuit space. Finally, the properties of these reduces\ndigraphs are analyzed and discussed in relation to the symbol grounding\nproblem.", "AI": {"tldr": "该论文描述了如何使用大型语言模型将数字词典嵌入AMR图，并对规约后的图进行分析，以探讨其与符号接地问题的关系。", "motivation": "旨在通过将真实数字词典嵌入抽象意义表示（AMR）图来增强其语义表示能力，并进一步探讨这种集成对符号接地问题的潜在影响。", "method": "首先，利用最先进的预训练大型语言模型将真实数字词典嵌入AMR有向图。然后，以合流方式（即保持其电路空间不变的转换）对这些图进行规约。最后，分析并讨论了这些规约后的有向图的特性。", "result": "成功地将数字词典嵌入AMR图并进行了规约。对规约后的有向图的特性进行了分析。", "conclusion": "对规约后的有向图特性的分析与符号接地问题相关联并进行了深入讨论。"}}
{"id": "2508.10942", "categories": ["cs.CV", "cs.HC", "cs.MM", "I.4.10; I.5.4"], "pdf": "https://arxiv.org/pdf/2508.10942", "abs": "https://arxiv.org/abs/2508.10942", "authors": ["Liming Xu", "Dave Towey", "Andrew P. French", "Steve Benford"], "title": "Topological Structure Description for Artcode Detection Using the Shape of Orientation Histogram", "comment": "This work is an extension of an ACM MM'17 workshop paper (Xu et al,\n  2017), which was completed in late 2017 and early 2018 during the first\n  author's doctoral studies at the University of Nottingham. This paper\n  includes 42 pages, 25 figures, 7 tables, and 13,536 words", "summary": "The increasing ubiquity of smartphones and resurgence of VR/AR techniques, it\nis expected that our everyday environment may soon be decorating with objects\nconnecting with virtual elements. Alerting to the presence of these objects is\ntherefore the first step for motivating follow-up further inspection and\ntriggering digital material attached to the objects. This work studies a\nspecial kind of these objects -- Artcodes -- a human-meaningful and\nmachine-readable decorative markers that camouflage themselves with freeform\nappearance by encoding information into their topology. We formulate this\nproblem of recongising the presence of Artcodes as Artcode proposal detection,\na distinct computer vision task that classifies topologically similar but\ngeometrically and semantically different objects as a same class. To deal with\nthis problem, we propose a new feature descriptor, called the shape of\norientation histogram, to describe the generic topological structure of an\nArtcode. We collect datasets and conduct comprehensive experiments to evaluate\nthe performance of the Artcode detection proposer built upon this new feature\nvector. Our experimental results show the feasibility of the proposed feature\nvector for representing topological structures and the effectiveness of the\nsystem for detecting Artcode proposals. Although this work is an initial\nattempt to develop a feature-based system for detecting topological objects\nlike Artcodes, it would open up new interaction opportunities and spark\npotential applications of topological object detection.", "AI": {"tldr": "本文提出了一种名为“Artcode”的拓扑信息编码装饰性标记，并研究了其检测问题，引入了一种新的特征描述符“方向直方图形状”来实现Artcode的识别。", "motivation": "随着智能手机的普及和VR/AR技术的复兴，环境中将出现大量与虚拟元素连接的物体。识别这些物体的存在是后续交互和触发数字内容的第一步。Artcode作为一种将信息编码到拓扑结构中的装饰性标记，其检测对于实现AR/VR互动至关重要。", "method": "将Artcode识别问题定义为Artcode提案检测，即分类拓扑相似但几何和语义不同的对象为同一类。为此，提出了一种新的特征描述符——“方向直方图形状”（shape of orientation histogram），用于描述Artcode的通用拓扑结构。收集了数据集并进行了全面的实验来评估基于该新特征向量构建的Artcode检测器性能。", "result": "实验结果表明，所提出的特征向量在表示拓扑结构方面是可行的，并且该系统在检测Artcode提案方面是有效的。", "conclusion": "这项工作是开发基于特征的拓扑对象（如Artcode）检测系统的初步尝试，它有望开启新的交互机会并激发拓扑对象检测的潜在应用。"}}
{"id": "2508.10972", "categories": ["cs.CV", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.10972", "abs": "https://arxiv.org/abs/2508.10972", "authors": ["Rosiana Natalie", "Wenqian Xu", "Ruei-Che Chang", "Rada Mihalcea", "Anhong Guo"], "title": "Not There Yet: Evaluating Vision Language Models in Simulating the Visual Perception of People with Low Vision", "comment": null, "summary": "Advances in vision language models (VLMs) have enabled the simulation of\ngeneral human behavior through their reasoning and problem solving\ncapabilities. However, prior research has not investigated such simulation\ncapabilities in the accessibility domain. In this paper, we evaluate the extent\nto which VLMs can simulate the vision perception of low vision individuals when\ninterpreting images. We first compile a benchmark dataset through a survey\nstudy with 40 low vision participants, collecting their brief and detailed\nvision information and both open-ended and multiple-choice image perception and\nrecognition responses to up to 25 images. Using these responses, we construct\nprompts for VLMs (GPT-4o) to create simulated agents of each participant,\nvarying the included information on vision information and example image\nresponses. We evaluate the agreement between VLM-generated responses and\nparticipants' original answers. Our results indicate that VLMs tend to infer\nbeyond the specified vision ability when given minimal prompts, resulting in\nlow agreement (0.59). The agreement between the agent' and participants'\nresponses remains low when only either the vision information (0.59) or example\nimage responses (0.59) are provided, whereas a combination of both\nsignificantly increase the agreement (0.70, p < 0.0001). Notably, a single\nexample combining both open-ended and multiple-choice responses, offers\nsignificant performance improvements over either alone (p < 0.0001), while\nadditional examples provided minimal benefits (p > 0.05).", "AI": {"tldr": "本研究评估了视觉语言模型（VLMs）在解释图像时模拟低视力个体视觉感知的能力，发现结合视力信息和单个综合示例提示能显著提高模拟准确性。", "motivation": "现有研究表明VLMs能够模拟通用人类行为，但在无障碍领域，特别是低视力个体的视觉感知模拟方面，尚未有深入探讨。", "method": "研究首先通过一项调查研究，收集了40名低视力参与者的视力信息以及他们对最多25张图像的开放式和多项选择感知响应，构建了一个基准数据集。然后，利用这些数据为VLMs（GPT-4o）构建提示，以创建每个参与者的模拟代理，并改变提示中包含的视力信息和示例图像响应。最后，评估了VLM生成响应与参与者原始答案之间的一致性。", "result": "结果显示，在提供最少提示时，VLMs倾向于超越指定的视力能力进行推断，导致一致性较低（0.59）。仅提供视力信息或示例图像响应时，一致性仍较低（0.59）。然而，结合两者显著提高了一致性（0.70，p < 0.0001）。值得注意的是，一个结合了开放式和多项选择响应的单一示例，比单独提供任一类型示例能带来显著的性能提升（p < 0.0001），而额外的示例带来的益处微乎其微（p > 0.05）。", "conclusion": "VLMs具有模拟低视力个体视觉感知的潜力，但有效的模拟需要特定的提示策略。具体而言，结合参与者的视力信息和一个包含开放式和多项选择响应的单一综合示例，能够显著提高模拟的准确性。"}}
{"id": "2508.11547", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.11547", "abs": "https://arxiv.org/abs/2508.11547", "authors": ["Martin Jiroušek", "Tomáš Báča", "Martin Saska"], "title": "Towards Fully Onboard State Estimation and Trajectory Tracking for UAVs with Suspended Payloads", "comment": null, "summary": "This paper addresses the problem of tracking the position of a\ncable-suspended payload carried by an unmanned aerial vehicle, with a focus on\nreal-world deployment and minimal hardware requirements. In contrast to many\nexisting approaches that rely on motion-capture systems, additional onboard\ncameras, or instrumented payloads, we propose a framework that uses only\nstandard onboard sensors--specifically, real-time kinematic global navigation\nsatellite system measurements and data from the onboard inertial measurement\nunit--to estimate and control the payload's position. The system models the\nfull coupled dynamics of the aerial vehicle and payload, and integrates a\nlinear Kalman filter for state estimation, a model predictive contouring\ncontrol planner, and an incremental model predictive controller. The control\narchitecture is designed to remain effective despite sensing limitations and\nestimation uncertainty. Extensive simulations demonstrate that the proposed\nsystem achieves performance comparable to control based on ground-truth\nmeasurements, with only minor degradation (< 6%). The system also shows strong\nrobustness to variations in payload parameters. Field experiments further\nvalidate the framework, confirming its practical applicability and reliable\nperformance in outdoor environments using only off-the-shelf aerial vehicle\nhardware.", "AI": {"tldr": "该论文提出了一种仅使用标准机载传感器（RTK GNSS和IMU）来估计和控制无人机悬挂载荷位置的框架，并在模拟和实际环境中验证了其性能和鲁棒性。", "motivation": "现有方法通常依赖昂贵的或额外的硬件（如运动捕捉系统、附加机载摄像头或带仪器的载荷），这限制了其在实际部署中的应用。本研究旨在开发一种硬件要求极低的解决方案。", "method": "该系统建模了无人机与载荷的完整耦合动力学，并集成了线性卡尔曼滤波器进行状态估计，一个模型预测轮廓控制规划器，以及一个增量模型预测控制器。控制架构旨在应对传感限制和估计不确定性。", "result": "广泛的模拟结果表明，所提出的系统性能与基于真实测量值的控制相当，性能下降微乎其微（<6%），并且对载荷参数变化表现出强大的鲁棒性。实地实验进一步验证了该框架的实际适用性和在室外环境中使用现成无人机硬件的可靠性能。", "conclusion": "该研究成功开发并验证了一个仅使用标准机载传感器即可有效跟踪和控制无人机悬挂载荷的系统，证明了其在实际部署中的可行性和可靠性，且硬件要求极低。"}}
{"id": "2508.11261", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11261", "abs": "https://arxiv.org/abs/2508.11261", "authors": ["Shan Luo", "Nathan F. Lepora", "Wenzhen Yuan", "Kaspar Althoefer", "Gordon Cheng", "Ravinder Dahiya"], "title": "Tactile Robotics: An Outlook", "comment": "20 pages, 2 figures, accepted to IEEE Transactions on Robotics", "summary": "Robotics research has long sought to give robots the ability to perceive the\nphysical world through touch in an analogous manner to many biological systems.\nDeveloping such tactile capabilities is important for numerous emerging\napplications that require robots to co-exist and interact closely with humans.\nConsequently, there has been growing interest in tactile sensing, leading to\nthe development of various technologies, including piezoresistive and\npiezoelectric sensors, capacitive sensors, magnetic sensors, and optical\ntactile sensors. These diverse approaches utilise different transduction\nmethods and materials to equip robots with distributed sensing capabilities,\nenabling more effective physical interactions. These advances have been\nsupported in recent years by simulation tools that generate large-scale tactile\ndatasets to support sensor designs and algorithms to interpret and improve the\nutility of tactile data. The integration of tactile sensing with other\nmodalities, such as vision, as well as with action strategies for active\ntactile perception highlights the growing scope of this field. To further the\ntransformative progress in tactile robotics, a holistic approach is essential.\nIn this outlook article, we examine several challenges associated with the\ncurrent state of the art in tactile robotics and explore potential solutions to\ninspire innovations across multiple domains, including manufacturing,\nhealthcare, recycling and agriculture.", "AI": {"tldr": "本文综述了触觉机器人的发展现状、挑战与未来方向，强调了其在人机交互中的重要性，并探讨了多种传感技术、仿真工具、多模态集成以及其在不同领域的应用潜力。", "motivation": "使机器人具备类似生物系统的触觉感知能力，以满足新兴应用中机器人与人类共存和密切交互的需求。", "method": "回顾了多种触觉传感技术（如压阻、压电、电容、磁性和光学传感器），介绍了支持传感器设计和数据解释的仿真工具，并探讨了触觉感知与其他模态（如视觉）的集成以及主动触觉感知策略。", "result": "触觉机器人领域取得了显著进展，包括多样化的传感技术、大规模触觉数据集的生成工具、以及触觉与其他感知模态的集成，显示出该领域日益增长的范围和潜力。", "conclusion": "为了推动触觉机器人的变革性发展，需要采取整体性方法来应对现有挑战，并探索潜在解决方案，以激发其在制造、医疗、回收和农业等多个领域的创新应用。"}}
{"id": "2508.11120", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11120", "abs": "https://arxiv.org/abs/2508.11120", "authors": ["Lorenzo Jaime Yu Flores", "Junyi Shen", "Xiaoyuan Gu"], "title": "Towards Reliable Multi-Agent Systems for Marketing Applications via Reflection, Memory, and Planning", "comment": null, "summary": "Recent advances in large language models (LLMs) enabled the development of AI\nagents that can plan and interact with tools to complete complex tasks.\nHowever, literature on their reliability in real-world applications remains\nlimited. In this paper, we introduce a multi-agent framework for a marketing\ntask: audience curation. To solve this, we introduce a framework called RAMP\nthat iteratively plans, calls tools, verifies the output, and generates\nsuggestions to improve the quality of the audience generated. Additionally, we\nequip the model with a long-term memory store, which is a knowledge base of\nclient-specific facts and past queries. Overall, we demonstrate the use of LLM\nplanning and memory, which increases accuracy by 28 percentage points on a set\nof 88 evaluation queries. Moreover, we show the impact of iterative\nverification and reflection on more ambiguous queries, showing progressively\nbetter recall (roughly +20 percentage points) with more verify/reflect\niterations on a smaller challenge set, and higher user satisfaction. Our\nresults provide practical insights for deploying reliable LLM-based systems in\ndynamic, industry-facing environments.", "AI": {"tldr": "本文提出一个名为RAMP的多智能体框架，结合迭代规划、工具调用、验证和长期记忆，显著提高了大语言模型在市场营销受众筛选任务中的准确性和可靠性。", "motivation": "尽管大语言模型（LLMs）驱动的AI智能体在规划和工具交互方面取得进展，但其在现实世界应用中的可靠性研究仍有限。", "method": "引入一个名为RAMP的多智能体框架，用于市场营销中的受众筛选任务。RAMP迭代地进行规划、调用工具、验证输出并生成改进建议。此外，模型配备了长期记忆存储（一个包含客户特定事实和历史查询的知识库）。", "result": "在88个评估查询上，LLM规划和记忆的使用使准确率提高了28个百分点。在较小的挑战集上，迭代验证和反思对模糊查询的影响更大，随着验证/反思迭代次数的增加，召回率逐步提高（约+20个百分点），用户满意度也更高。", "conclusion": "研究结果为在动态、面向行业的环境中部署可靠的基于LLM的系统提供了实用见解。"}}
{"id": "2508.10943", "categories": ["cs.CV", "cond-mat.mtrl-sci", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2508.10943", "abs": "https://arxiv.org/abs/2508.10943", "authors": ["Christian Düreth", "Jan Condé-Wolter", "Marek Danczak", "Karsten Tittmann", "Jörn Jaschinski", "Andreas Hornig", "Maik Gude"], "title": "Analysis of the Compaction Behavior of Textile Reinforcements in Low-Resolution In-Situ CT Scans via Machine-Learning and Descriptor-Based Methods", "comment": "submitted to Elsevier Composite Part C: Open Access\n  (JCOMC-D-25-00212), 16 pages, 8 Figures, and 3 Tables", "summary": "A detailed understanding of material structure across multiple scales is\nessential for predictive modeling of textile-reinforced composites. Nesting --\ncharacterized by the interlocking of adjacent fabric layers through local\ninterpenetration and misalignment of yarns -- plays a critical role in defining\nmechanical properties such as stiffness, permeability, and damage tolerance.\nThis study presents a framework to quantify nesting behavior in dry textile\nreinforcements under compaction using low-resolution computed tomography (CT).\nIn-situ compaction experiments were conducted on various stacking\nconfigurations, with CT scans acquired at 20.22 $\\mu$m per voxel resolution. A\ntailored 3D{-}UNet enabled semantic segmentation of matrix, weft, and fill\nphases across compaction stages corresponding to fiber volume contents of\n50--60 %. The model achieved a minimum mean Intersection-over-Union of 0.822\nand an $F1$ score of 0.902. Spatial structure was subsequently analyzed using\nthe two-point correlation function $S_2$, allowing for probabilistic extraction\nof average layer thickness and nesting degree. The results show strong\nagreement with micrograph-based validation. This methodology provides a robust\napproach for extracting key geometrical features from industrially relevant CT\ndata and establishes a foundation for reverse modeling and descriptor-based\nstructural analysis of composite preforms.", "AI": {"tldr": "本研究提出了一种利用低分辨率CT和3D-UNet量化干燥纺织增强材料在压实过程中嵌套行为的方法，并使用两点相关函数S2分析空间结构，为复合材料预成型件的结构分析奠定基础。", "motivation": "预测纺织增强复合材料的性能需要深入理解材料在多尺度上的结构，其中“嵌套”（相邻织物层通过局部互穿和纱线错位而交错）对力学性能（如刚度、渗透性和损伤容限）有关键影响。因此，需要一种量化嵌套行为的方法。", "method": "研究采用低分辨率计算机断层扫描（CT）对不同堆叠配置的干燥纺织增强材料进行原位压实实验，并在不同压实阶段（纤维体积含量50-60%）获取CT图像。使用定制的3D-UNet模型对基体、纬纱和填充相进行语义分割。随后，利用两点相关函数S2分析空间结构，以概率方式提取平均层厚度和嵌套程度。", "result": "3D-UNet模型在语义分割上表现出色，平均交并比（IoU）最低达到0.822，F1分数达到0.902。通过S2函数分析获得的平均层厚度和嵌套程度与显微照片验证结果高度一致。该方法能够从工业相关的CT数据中提取关键几何特征。", "conclusion": "本研究提供了一种量化干燥纺织增强材料在压实过程中嵌套行为的稳健方法，为复合材料预成型件的逆向建模和基于描述符的结构分析奠定了基础。"}}
{"id": "2508.11133", "categories": ["cs.CL", "cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2508.11133", "abs": "https://arxiv.org/abs/2508.11133", "authors": ["Tomer Wolfson", "Harsh Trivedi", "Mor Geva", "Yoav Goldberg", "Dan Roth", "Tushar Khot", "Ashish Sabharwal", "Reut Tsarfaty"], "title": "MoNaCo: More Natural and Complex Questions for Reasoning Across Dozens of Documents", "comment": "Accepted for publication in Transactions of the Association for\n  Computational Linguistics (TACL), 2025. Authors pre-print", "summary": "Large language models (LLMs) are emerging as a go-to tool for querying\ninformation. However, current LLM benchmarks rarely feature natural questions\nthat are both information-seeking as well as genuinely time-consuming for\nhumans. To address this gap we introduce MoNaCo, a benchmark of 1,315 natural\nand complex questions that require dozens, and at times hundreds, of\nintermediate steps to solve -- far more than any existing QA benchmark. To\nbuild MoNaCo, we developed a decomposed annotation pipeline to elicit and\nmanually answer natural time-consuming questions at scale. Frontier LLMs\nevaluated on MoNaCo achieve at most 61.2% F1, hampered by low recall and\nhallucinations. Our results underscore the need for reasoning models that\nbetter handle the complexity and sheer breadth of real-world\ninformation-seeking questions -- with MoNaCo providing an effective resource\nfor tracking such progress. The MONACO benchmark, codebase, prompts and models\npredictions are publicly available at: https://tomerwolgithub.github.io/monaco", "AI": {"tldr": "引入MoNaCo基准测试，包含1315个复杂且耗时的自然问题，旨在评估大型语言模型（LLMs）处理真实世界信息查询的推理能力。现有LLMs在该基准上表现不佳。", "motivation": "当前LLM基准测试很少包含对人类而言既耗时又具有信息查询性质的自然问题，这导致无法充分评估LLMs处理真实世界复杂信息查询的能力。", "method": "开发了MoNaCo基准测试，包含1315个自然且复杂的问题，这些问题需要大量中间步骤才能解决。为此，他们设计了一个分解式标注流程，以大规模地获取和手动回答这些耗时的自然问题。", "result": "在MoNaCo基准上评估的先进LLMs最高仅达到61.2%的F1分数，主要受限于低召回率和幻觉问题。", "conclusion": "研究结果强调了需要开发能更好处理真实世界信息查询复杂性和广度的新型推理模型。MoNaCo基准将为跟踪此类进展提供有效资源。"}}
{"id": "2508.11573", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.11573", "abs": "https://arxiv.org/abs/2508.11573", "authors": ["Mogens Plessen"], "title": "Nominal Evaluation Of Automatic Multi-Sections Control Potential In Comparison To A Simpler One- Or Two-Sections Alternative With Predictive Spray Switching", "comment": "14 pages plus 7 pages appendix with additional figures, 18 main\n  figures, 3 tables", "summary": "Automatic Section Control (ASC) is a long-standing trend for spraying in\nagriculture. It promises to minimise spray overlap areas. The core idea is to\n(i) switch off spray nozzles on areas that have already been sprayed, and (ii)\nto dynamically adjust nozzle flow rates along the boom bar that holds the spray\nnozzles when velocities of boom sections vary during turn maneuvers. ASC is not\npossible without sensors, in particular for accurate positioning data. Spraying\nand the movement of modern wide boom bars are highly dynamic processes. In\naddition, many uncertainty factors have an effect such as cross wind drift,\nboom height, nozzle clogging in open-field conditions, and so forth. In view of\nthis complexity, the natural question arises if a simpler alternative exist.\nTherefore, an Automatic Multi-Sections Control method is compared to a proposed\nsimpler one- or two-sections alternative that uses predictive spray switching.\nThe comparison is provided under nominal conditions. Agricultural spraying is\nintrinsically linked to area coverage path planning and spray switching logic.\nCombinations of two area coverage path planning and switching logics as well as\nthree sections-setups are compared. The three sections-setups differ by\ncontrolling 48 sections, 2 sections or controlling all nozzles uniformly with\nthe same control signal as one single section. Methods are evaluated on 10\ndiverse real-world field examples, including non-convex field contours,\nfreeform mainfield lanes and multiple obstacle areas. A preferred method is\nsuggested that (i) minimises area coverage pathlength, (ii) offers intermediate\noverlap, (iii) is suitable for manual driving by following a pre-planned\npredictive spray switching logic for an area coverage path plan, and (iv) and\nin contrast to ASC can be implemented sensor-free and therefore at low cost.", "AI": {"tldr": "论文比较了自动多段控制（ASC）与更简单的一段或两段控制方法，发现基于预测喷雾切换的简单方法在特定条件下表现良好，且成本更低。", "motivation": "现有的自动喷雾段控制（ASC）系统复杂、依赖传感器（特别是精确位置数据），且受多种不确定因素影响。因此，研究者寻求是否存在一种更简单、成本更低的替代方案。", "method": "论文比较了两种区域覆盖路径规划和喷雾切换逻辑，以及三种喷雾段设置（48段、2段、1段统一控制）。其中，更简单的方法采用了预测喷雾切换。这些方法在10个多样化的真实田地场景下进行了评估。", "result": "论文提出了一种优选方法，该方法能最小化区域覆盖路径长度，提供中等重叠，适用于手动驾驶并遵循预先规划的预测喷雾切换逻辑，且与ASC相比可实现无传感器部署，从而降低成本。", "conclusion": "存在一种更简单、无传感器、低成本的替代方案，即采用基于预测喷雾切换的一段或两段控制方法，该方法在性能上（如路径长度、重叠）表现良好，且适用于实际农业喷雾操作。"}}
{"id": "2508.11275", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11275", "abs": "https://arxiv.org/abs/2508.11275", "authors": ["Masaki Murooka", "Iori Kumagai", "Mitsuharu Morisawa", "Fumio Kanehiro"], "title": "Learning Differentiable Reachability Maps for Optimization-based Humanoid Motion Generation", "comment": null, "summary": "To reduce the computational cost of humanoid motion generation, we introduce\na new approach to representing robot kinematic reachability: the differentiable\nreachability map. This map is a scalar-valued function defined in the task\nspace that takes positive values only in regions reachable by the robot's\nend-effector. A key feature of this representation is that it is continuous and\ndifferentiable with respect to task-space coordinates, enabling its direct use\nas constraints in continuous optimization for humanoid motion planning. We\ndescribe a method to learn such differentiable reachability maps from a set of\nend-effector poses generated using a robot's kinematic model, using either a\nneural network or a support vector machine as the learning model. By\nincorporating the learned reachability map as a constraint, we formulate\nhumanoid motion generation as a continuous optimization problem. We demonstrate\nthat the proposed approach efficiently solves various motion planning problems,\nincluding footstep planning, multi-contact motion planning, and\nloco-manipulation planning for humanoid robots.", "AI": {"tldr": "本文提出了一种可微分可达性图来表示机器人运动学可达性，并将其作为连续优化中的约束，以降低类人机器人运动生成的计算成本。", "motivation": "降低类人机器人运动生成的计算成本。", "method": "引入了“可微分可达性图”作为机器人末端执行器可达区域的连续可微分标量函数。该图通过使用神经网络或支持向量机从机器人运动学模型生成的末端执行器姿态数据中学习得到。然后，将学习到的可达性图作为约束，将类人机器人运动生成问题表述为连续优化问题。", "result": "该方法能够有效地解决各种运动规划问题，包括足迹规划、多接触运动规划和类人机器人的移动操作规划。", "conclusion": "所提出的可微分可达性图表示方法，通过在连续优化中作为约束，显著提高了类人机器人运动规划的效率。"}}
{"id": "2508.11163", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11163", "abs": "https://arxiv.org/abs/2508.11163", "authors": ["Hikaru Asano", "Hiroki Ouchi", "Akira Kasuga", "Ryo Yonetani"], "title": "MobQA: A Benchmark Dataset for Semantic Understanding of Human Mobility Data through Question Answering", "comment": "23 pages, 12 figures", "summary": "This paper presents MobQA, a benchmark dataset designed to evaluate the\nsemantic understanding capabilities of large language models (LLMs) for human\nmobility data through natural language question answering.\n  While existing models excel at predicting human movement patterns, it remains\nunobvious how much they can interpret the underlying reasons or semantic\nmeaning of those patterns. MobQA provides a comprehensive evaluation framework\nfor LLMs to answer questions about diverse human GPS trajectories spanning\ndaily to weekly granularities. It comprises 5,800 high-quality question-answer\npairs across three complementary question types: factual retrieval (precise\ndata extraction), multiple-choice reasoning (semantic inference), and free-form\nexplanation (interpretive description), which all require spatial, temporal,\nand semantic reasoning. Our evaluation of major LLMs reveals strong performance\non factual retrieval but significant limitations in semantic reasoning and\nexplanation question answering, with trajectory length substantially impacting\nmodel effectiveness. These findings demonstrate the achievements and\nlimitations of state-of-the-art LLMs for semantic mobility\nunderstanding.\\footnote{MobQA dataset is available at\nhttps://github.com/CyberAgentAILab/mobqa.}", "AI": {"tldr": "MobQA是一个基准数据集，旨在通过自然语言问答评估大型语言模型（LLMs）对人类移动数据的语义理解能力，揭示了LLMs在事实检索方面表现良好，但在语义推理和解释性问答方面存在显著局限性。", "motivation": "现有模型擅长预测人类移动模式，但它们对这些模式背后原因或语义含义的解释能力尚不明确。本研究旨在填补这一空白，提供一个评估LLMs语义理解能力的标准框架。", "method": "MobQA数据集包含5,800个高质量的问答对，涵盖了日常到每周粒度的多样化人类GPS轨迹。问题分为三种类型：事实检索（精确数据提取）、多项选择推理（语义推断）和自由形式解释（解释性描述），均需要空间、时间和语义推理能力。", "result": "对主流LLMs的评估显示，它们在事实检索方面表现出色，但在语义推理和解释性问答方面存在显著局限性，并且轨迹长度对模型效能有很大影响。", "conclusion": "这些发现揭示了当前最先进的LLMs在语义移动理解方面的成就和局限性。"}}
{"id": "2508.10945", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10945", "abs": "https://arxiv.org/abs/2508.10945", "authors": ["Rishi Raj Sahoo", "Surbhi Saswati Mohanty", "Subhankar Mishra"], "title": "iWatchRoad: Scalable Detection and Geospatial Visualization of Potholes for Smart Cities", "comment": "Under review", "summary": "Potholes on the roads are a serious hazard and maintenance burden. This poses\na significant threat to road safety and vehicle longevity, especially on the\ndiverse and under-maintained roads of India. In this paper, we present a\ncomplete end-to-end system called iWatchRoad for automated pothole detection,\nGlobal Positioning System (GPS) tagging, and real time mapping using\nOpenStreetMap (OSM). We curated a large, self-annotated dataset of over 7,000\nframes captured across various road types, lighting conditions, and weather\nscenarios unique to Indian environments, leveraging dashcam footage. This\ndataset is used to fine-tune, Ultralytics You Only Look Once (YOLO) model to\nperform real time pothole detection, while a custom Optical Character\nRecognition (OCR) module was employed to extract timestamps directly from video\nframes. The timestamps are synchronized with GPS logs to geotag each detected\npotholes accurately. The processed data includes the potholes' details and\nframes as metadata is stored in a database and visualized via a user friendly\nweb interface using OSM. iWatchRoad not only improves detection accuracy under\nchallenging conditions but also provides government compatible outputs for road\nassessment and maintenance planning through the metadata visible on the\nwebsite. Our solution is cost effective, hardware efficient, and scalable,\noffering a practical tool for urban and rural road management in developing\nregions, making the system automated. iWatchRoad is available at\nhttps://smlab.niser.ac.in/project/iwatchroad", "AI": {"tldr": "本文提出了iWatchRoad，一个端到端系统，用于自动检测路面坑洼、进行GPS标记并通过OpenStreetMap实时映射，以提高道路安全和维护效率。", "motivation": "路面坑洼对道路安全和车辆寿命构成严重威胁和维护负担，尤其是在印度多样且维护不足的道路上。", "method": "该系统利用超过7000帧的自标注行车记录仪数据集（包含印度独特环境下的各种道路类型、光照和天气条件），对Ultralytics YOLO模型进行微调以实现实时坑洼检测。同时，使用自定义OCR模块从视频帧中提取时间戳，并与GPS日志同步以准确地理标记每个检测到的坑洼。处理后的数据及其元数据存储在数据库中，并通过基于OpenStreetMap的用户友好型网页界面进行可视化。", "result": "iWatchRoad在挑战性条件下提高了检测精度，并能提供与政府兼容的输出，用于道路评估和维护规划。该解决方案具有成本效益、硬件高效和可扩展性，为发展中地区的城乡道路管理提供了实用的自动化工具。", "conclusion": "iWatchRoad系统提供了一个实用、自动化且可扩展的工具，用于道路管理，特别是在发展中地区，有效提升了道路安全和维护效率。"}}
{"id": "2508.11141", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11141", "abs": "https://arxiv.org/abs/2508.11141", "authors": ["Bin Ma", "Yifei Zhang", "Yongjin Xian", "Qi Li", "Linna Zhou", "Gongxun Miao"], "title": "A Cross-Modal Rumor Detection Scheme via Contrastive Learning by Exploring Text and Image internal Correlations", "comment": null, "summary": "Existing rumor detection methods often neglect the content within images as\nwell as the inherent relationships between contexts and images across different\nvisual scales, thereby resulting in the loss of critical information pertinent\nto rumor identification. To address these issues, this paper presents a novel\ncross-modal rumor detection scheme based on contrastive learning, namely the\nMulti-scale Image and Context Correlation exploration algorithm (MICC).\nSpecifically, we design an SCLIP encoder to generate unified semantic\nembeddings for text and multi-scale image patches through contrastive\npretraining, enabling their relevance to be measured via dot-product\nsimilarity. Building upon this, a Cross-Modal Multi-Scale Alignment module is\nintroduced to identify image regions most relevant to the textual semantics,\nguided by mutual information maximization and the information bottleneck\nprinciple, through a Top-K selection strategy based on a cross-modal relevance\nmatrix constructed between the text and multi-scale image patches. Moreover, a\nscale-aware fusion network is designed to integrate the highly correlated\nmulti-scale image features with global text features by assigning adaptive\nweights to image regions based on their semantic importance and cross-modal\nrelevance. The proposed methodology has been extensively evaluated on two\nreal-world datasets. The experimental results demonstrate that it achieves a\nsubstantial performance improvement over existing state-of-the-art approaches\nin rumor detection, highlighting its effectiveness and potential for practical\napplications.", "AI": {"tldr": "提出了一种名为MICC的跨模态谣言检测算法，通过对比学习和多尺度图像-文本对齐，有效整合图像内容和文本信息，提升谣言检测性能。", "motivation": "现有谣言检测方法忽视图像内容以及上下文与图像在不同视觉尺度间的内在关系，导致谣言识别关键信息丢失。", "method": "1. 设计SCLIP编码器，通过对比预训练生成文本和多尺度图像块的统一语义嵌入。2. 引入跨模态多尺度对齐模块，基于互信息最大化和信息瓶颈原理，通过Top-K选择策略识别与文本语义最相关的图像区域。3. 设计尺度感知融合网络，根据语义重要性和跨模态相关性为图像区域分配自适应权重，将高相关性多尺度图像特征与全局文本特征融合。", "result": "在两个真实世界数据集上，该方法比现有最先进的谣言检测方法取得了显著的性能提升。", "conclusion": "所提出的方法在谣言检测方面有效且具有实际应用潜力。"}}
{"id": "2508.11286", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11286", "abs": "https://arxiv.org/abs/2508.11286", "authors": ["Che Rin Yu", "Daewon Chae", "Dabin Seo", "Sangwon Lee", "Hyeongwoo Im", "Jinkyu Kim"], "title": "Scene Graph-Guided Proactive Replanning for Failure-Resilient Embodied Agent", "comment": null, "summary": "When humans perform everyday tasks, we naturally adjust our actions based on\nthe current state of the environment. For instance, if we intend to put\nsomething into a drawer but notice it is closed, we open it first. However,\nmany autonomous robots lack this adaptive awareness. They often follow\npre-planned actions that may overlook subtle yet critical changes in the scene,\nwhich can result in actions being executed under outdated assumptions and\neventual failure. While replanning is critical for robust autonomy, most\nexisting methods respond only after failures occur, when recovery may be\ninefficient or infeasible. While proactive replanning holds promise for\npreventing failures in advance, current solutions often rely on manually\ndesigned rules and extensive supervision. In this work, we present a proactive\nreplanning framework that detects and corrects failures at subtask boundaries\nby comparing scene graphs constructed from current RGB-D observations against\nreference graphs extracted from successful demonstrations. When the current\nscene fails to align with reference trajectories, a lightweight reasoning\nmodule is activated to diagnose the mismatch and adjust the plan. Experiments\nin the AI2-THOR simulator demonstrate that our approach detects semantic and\nspatial mismatches before execution failures occur, significantly improving\ntask success and robustness.", "AI": {"tldr": "该论文提出一个主动式重规划框架，通过比较当前RGB-D观察与成功演示生成的场景图，在子任务边界检测并纠正潜在的执行失败，从而提高机器人任务成功率和鲁棒性。", "motivation": "现有自主机器人缺乏环境自适应能力，常基于过时假设执行预设动作导致失败。多数重规划方法仅在失败发生后响应，恢复效率低或不可行。而现有主动式重规划方案依赖手动规则和大量监督，不够通用。", "method": "提出一个主动式重规划框架，在子任务边界检测和纠正失败。通过比较当前RGB-D观察构建的场景图与成功演示中提取的参考图。当当前场景与参考轨迹不符时，激活一个轻量级推理模块来诊断不匹配并调整计划。", "result": "在AI2-THOR模拟器中的实验表明，该方法能在执行失败发生前检测到语义和空间不匹配，显著提高了任务成功率和鲁棒性。", "conclusion": "该框架通过主动检测和纠正潜在的执行失败，有效解决了机器人缺乏自适应能力的问题，显著提升了任务的成功率和系统的鲁棒性。"}}
{"id": "2508.11166", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11166", "abs": "https://arxiv.org/abs/2508.11166", "authors": ["Anusha M D", "Deepthi Vikram", "Bharathi Raja Chakravarthi", "Parameshwar R Hegde"], "title": "Overcoming Low-Resource Barriers in Tulu: Neural Models and Corpus Creation for OffensiveLanguage Identification", "comment": "20 pages, 3 tables, 3 figures. Submitted to Language Resources and\n  Evaluation (Springer)", "summary": "Tulu, a low-resource Dravidian language predominantly spoken in southern\nIndia, has limited computational resources despite its growing digital\npresence. This study presents the first benchmark dataset for Offensive\nLanguage Identification (OLI) in code-mixed Tulu social media content,\ncollected from YouTube comments across various domains. The dataset, annotated\nwith high inter-annotator agreement (Krippendorff's alpha = 0.984), includes\n3,845 comments categorized into four classes: Not Offensive, Not Tulu,\nOffensive Untargeted, and Offensive Targeted. We evaluate a suite of deep\nlearning models, including GRU, LSTM, BiGRU, BiLSTM, CNN, and attention-based\nvariants, alongside transformer architectures (mBERT, XLM-RoBERTa). The BiGRU\nmodel with self-attention achieves the best performance with 82% accuracy and a\n0.81 macro F1-score. Transformer models underperform, highlighting the\nlimitations of multilingual pretraining in code-mixed, under-resourced\ncontexts. This work lays the foundation for further NLP research in Tulu and\nsimilar low-resource, code-mixed languages.", "AI": {"tldr": "本研究为低资源语种图卢语（Tulu）的混合代码冒犯性语言识别（OLI）构建了首个基准数据集，并评估了深度学习模型和Transformer模型的性能，发现BiGRU模型表现最佳，而多语言预训练模型在低资源混合代码语境下表现不佳。", "motivation": "图卢语作为一种低资源达罗毗荼语，尽管数字存在感日益增强，但计算资源有限，特别是在冒犯性语言识别方面缺乏相关资源。", "method": "研究从YouTube评论中收集了混合代码的图卢语社交媒体内容，构建了一个包含3,845条评论的冒犯性语言识别基准数据集，并将其标注为四类（非冒犯性、非图卢语、无目标冒犯性、有目标冒犯性），标注一致性高（Krippendorff's alpha = 0.984）。随后，评估了一系列深度学习模型（GRU、LSTM、BiGRU、BiLSTM、CNN及其注意力变体）以及Transformer架构（mBERT、XLM-RoBERTa）。", "result": "带有自注意力的BiGRU模型取得了最佳性能，准确率为82%，宏F1分数为0.81。Transformer模型表现不佳，这突显了多语言预训练模型在混合代码、低资源语境下的局限性。", "conclusion": "本工作为图卢语和类似的低资源、混合代码语言的自然语言处理研究奠定了基础，并揭示了多语言预训练模型在特定低资源混合语境下的不足。"}}
{"id": "2508.10947", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10947", "abs": "https://arxiv.org/abs/2508.10947", "authors": ["Ronghao Xu", "Zhen Huang", "Yangbo Wei", "Xiaoqian Zhou", "Zikang Xu", "Ting Liu", "Zihang Jiang", "S. Kevin Zhou"], "title": "MedAtlas: Evaluating LLMs for Multi-Round, Multi-Task Medical Reasoning Across Diverse Imaging Modalities and Clinical Text", "comment": null, "summary": "Artificial intelligence has demonstrated significant potential in clinical\ndecision-making; however, developing models capable of adapting to diverse\nreal-world scenarios and performing complex diagnostic reasoning remains a\nmajor challenge. Existing medical multi-modal benchmarks are typically limited\nto single-image, single-turn tasks, lacking multi-modal medical image\nintegration and failing to capture the longitudinal and multi-modal interactive\nnature inherent to clinical practice. To address this gap, we introduce\nMedAtlas, a novel benchmark framework designed to evaluate large language\nmodels on realistic medical reasoning tasks. MedAtlas is characterized by four\nkey features: multi-turn dialogue, multi-modal medical image interaction,\nmulti-task integration, and high clinical fidelity. It supports four core\ntasks: open-ended multi-turn question answering, closed-ended multi-turn\nquestion answering, multi-image joint reasoning, and comprehensive disease\ndiagnosis. Each case is derived from real diagnostic workflows and incorporates\ntemporal interactions between textual medical histories and multiple imaging\nmodalities, including CT, MRI, PET, ultrasound, and X-ray, requiring models to\nperform deep integrative reasoning across images and clinical texts. MedAtlas\nprovides expert-annotated gold standards for all tasks. Furthermore, we propose\ntwo novel evaluation metrics: Round Chain Accuracy and Error Propagation\nResistance. Benchmark results with existing multi-modal models reveal\nsubstantial performance gaps in multi-stage clinical reasoning. MedAtlas\nestablishes a challenging evaluation platform to advance the development of\nrobust and trustworthy medical AI.", "AI": {"tldr": "MedAtlas是一个新的多模态医学基准框架，用于评估大型语言模型在真实临床推理任务中的表现，解决了现有基准在多模态、多轮交互和临床保真度方面的不足。", "motivation": "现有医学多模态基准通常限于单图像、单轮任务，缺乏多模态医学图像整合，未能捕捉临床实践中固有的纵向和多模态交互性质，导致模型难以适应多样化的真实世界场景并进行复杂的诊断推理。", "method": "引入了MedAtlas基准框架，具有四个关键特征：多轮对话、多模态医学图像交互、多任务集成和高临床保真度。它支持四种核心任务：开放式多轮问答、封闭式多轮问答、多图像联合推理和综合疾病诊断。每个案例均源自真实诊断流程，并整合了文本病史和多种影像模态（CT、MRI、PET、超声、X射线）之间的时间交互。此外，提出了两个新的评估指标：回合链准确率（Round Chain Accuracy）和错误传播抵抗（Error Propagation Resistance）。", "result": "现有多模态模型在多阶段临床推理方面表现出显著的性能差距。", "conclusion": "MedAtlas建立了一个具有挑战性的评估平台，旨在推动鲁棒和值得信赖的医学人工智能的发展。"}}
{"id": "2508.11170", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11170", "abs": "https://arxiv.org/abs/2508.11170", "authors": ["Baihong Qian", "Haotian Fan", "Wenjie Liao", "Yunqiu Wang", "Tao Li", "Junhui Cui"], "title": "Better Supervised Fine-tuning for VQA: Integer-Only Loss", "comment": null, "summary": "With the rapid advancement of vision language models(VLM), their ability to\nassess visual content based on specific criteria and dimensions has become\nincreasingly critical for applications such as video-theme consistency\nassessment and visual quality scoring. However, existing methods often suffer\nfrom imprecise results and inefficient loss calculation, which limit the focus\nof the model on key evaluation indicators. To address this, we propose\nIOVQA(Integer-only VQA), a novel fine-tuning approach tailored for VLMs to\nenhance their performance in video quality assessment tasks. The key innovation\nof IOVQA lies in its label construction and its targeted loss calculation\nmechanism. Specifically, during dataset curation, we constrain the model's\noutput to integers within the range of [10,50], ensuring numerical stability,\nand convert decimal Overall_MOS to integer before using them as labels. We also\nintroduce a target-mask strategy: when computing the loss, only the first\ntwo-digit-integer of the label is unmasked, forcing the model to learn the\ncritical components of the numerical evaluation. After fine-tuning the\nQwen2.5-VL model using the constructed dataset, experimental results\ndemonstrate that the proposed method significantly improves the model's\naccuracy and consistency in the VQA task, ranking 3rd in VQualA 2025\nGenAI-Bench AIGC Video Quality Assessment Challenge -- Track I. Our work\nhighlights the effectiveness of merely leaving integer labels during\nfine-tuning, providing an effective idea for optimizing VLMs in quantitative\nevaluation scenarios.", "AI": {"tldr": "本文提出IOVQA（Integer-only VQA），一种针对视觉语言模型（VLM）的微调方法，通过限制输出为整数和引入目标掩码损失计算机制，显著提升了视频质量评估任务的准确性和一致性。", "motivation": "现有视觉语言模型在评估视觉内容时常面临结果不精确和损失计算效率低的问题，这限制了模型对关键评估指标的关注，尤其是在视频主题一致性评估和视觉质量评分等应用中。", "method": "IOVQA方法的核心创新在于其标签构建和目标损失计算机制。具体而言，在数据集构建时，模型输出被限制在[10,50]的整数范围内，并将原始的Overall_MOS小数转换为整数作为标签。同时，引入了目标掩码策略，在计算损失时仅解除标签前两位整数的掩码，迫使模型学习数值评估的关键部分。该方法在Qwen2.5-VL模型上进行了微调。", "result": "实验结果表明，所提出的IOVQA方法显著提高了模型在VQA任务中的准确性和一致性，并在VQualA 2025 GenAI-Bench AIGC视频质量评估挑战赛——第一赛道中排名第三。", "conclusion": "该工作强调了在微调过程中仅保留整数标签的有效性，为在定量评估场景中优化视觉语言模型提供了一个有效的思路。"}}
{"id": "2508.11289", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11289", "abs": "https://arxiv.org/abs/2508.11289", "authors": ["Lin Li", "Xueming Liu", "Zhoujingzi Qiu", "Tianjiang Hu", "Qingrui Zhang"], "title": "A Recursive Total Least Squares Solution for Bearing-Only Target Motion Analysis and Circumnavigation", "comment": "Accepted by 2025 IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS), 6 Pages", "summary": "Bearing-only Target Motion Analysis (TMA) is a promising technique for\npassive tracking in various applications as a bearing angle is easy to measure.\nDespite its advantages, bearing-only TMA is challenging due to the nonlinearity\nof the bearing measurement model and the lack of range information, which\nimpairs observability and estimator convergence. This paper addresses these\nissues by proposing a Recursive Total Least Squares (RTLS) method for online\ntarget localization and tracking using mobile observers. The RTLS approach,\ninspired by previous results on Total Least Squares (TLS), mitigates biases in\nposition estimation and improves computational efficiency compared to\npseudo-linear Kalman filter (PLKF) methods. Additionally, we propose a\ncircumnavigation controller to enhance system observability and estimator\nconvergence by guiding the mobile observer in orbit around the target.\nExtensive simulations and experiments are performed to demonstrate the\neffectiveness and robustness of the proposed method. The proposed algorithm is\nalso compared with the state-of-the-art approaches, which confirms its superior\nperformance in terms of both accuracy and stability.", "AI": {"tldr": "本文提出了一种基于递归全最小二乘（RTLS）的航向角目标运动分析（TMA）方法，结合环绕控制器，用于移动观测器的在线目标定位和跟踪，旨在提高估计精度、计算效率和系统可观测性。", "motivation": "航向角目标运动分析（TMA）由于测量模型非线性、缺乏距离信息以及可观测性和估计器收敛性差等问题，具有挑战性。", "method": "提出了一种递归全最小二乘（RTLS）方法，灵感来源于全最小二乘（TLS），用于减轻位置估计偏差并提高计算效率。此外，还设计了一个环绕控制器，通过引导移动观测器围绕目标轨道运行来增强系统可观测性和估计器收敛性。", "result": "广泛的仿真和实验证明了所提方法的有效性和鲁棒性。与现有先进方法相比，该算法在准确性和稳定性方面表现出卓越的性能。", "conclusion": "所提出的RTLS方法结合环绕控制器，能够有效解决航向角TMA中的挑战，显著提高目标定位和跟踪的准确性和稳定性。"}}
{"id": "2508.11184", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11184", "abs": "https://arxiv.org/abs/2508.11184", "authors": ["Tao Wu", "Jingyuan Chen", "Wang Lin", "Jian Zhan", "Mengze Li", "Kun Kuang", "Fei Wu"], "title": "Personalized Distractor Generation via MCTS-Guided Reasoning Reconstruction", "comment": null, "summary": "Distractors, incorrect but plausible answer choices in multiple-choice\nquestions (MCQs), play a critical role in educational assessment by diagnosing\nstudent misconceptions. Recent work has leveraged large language models (LLMs)\nto generate shared, group-level distractors by learning common error patterns\nacross large student populations. However, such distractors often fail to\ncapture the diverse reasoning errors of individual students, limiting their\ndiagnostic effectiveness. To address this limitation, we introduce the task of\npersonalized distractor generation, which aims to generate tailored distractors\nbased on individual misconceptions inferred from each student's past\nquestion-answering (QA) records, ensuring every student receives options that\neffectively exposes their specific reasoning errors. While promising, this task\nis challenging because each student typically has only a few QA records, which\noften lack the student's underlying reasoning processes, making training-based\ngroup-level approaches infeasible. To overcome this, we propose a training-free\ntwo-stage framework. In the first stage, we construct a student-specific\nmisconception prototype by applying Monte Carlo Tree Search (MCTS) to recover\nthe student's reasoning trajectories from past incorrect answers. In the second\nstage, this prototype guides the simulation of the student's reasoning on new\nquestions, enabling the generation of personalized distractors that align with\nthe student's recurring misconceptions. Experiments show that our approach\nachieves the best performance in generating plausible, personalized distractors\nfor 140 students, and also effectively generalizes to group-level settings,\nhighlighting its robustness and adaptability.", "AI": {"tldr": "本文提出了一种无需训练的两阶段框架，利用蒙特卡洛树搜索（MCTS）从学生过去的答题记录中推断个体错误概念，从而生成个性化的多选题干扰项，有效诊断学生特有的推理错误。", "motivation": "现有的大型语言模型生成的干扰项通常是群体层面的，未能捕捉到个体学生的多元推理错误，限制了其诊断效果。为解决这一局限性，需要生成基于个体错误概念的个性化干扰项。然而，每个学生通常只有少量答题记录且缺乏推理过程，使得基于训练的群体级方法不可行。", "method": "提出一个无需训练的两阶段框架：第一阶段，通过对学生过去的错误答案应用蒙特卡洛树搜索（MCTS）来恢复其推理轨迹，构建学生特定的错误概念原型；第二阶段，该原型指导在新问题上模拟学生的推理过程，从而生成与学生反复出现的错误概念相符的个性化干扰项。", "result": "实验表明，该方法在为140名学生生成合理、个性化的干扰项方面取得了最佳性能，并且能够有效地推广到群体级设置，突出了其鲁棒性和适应性。", "conclusion": "所提出的无需训练的两阶段框架能有效生成个性化干扰项，通过推断学生个体错误概念来准确诊断其推理错误，且具有良好的鲁棒性和在个性化及群体级设置中的适应性。"}}
{"id": "2508.10950", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10950", "abs": "https://arxiv.org/abs/2508.10950", "authors": ["Xinyi Wang", "Michael Barnett", "Frederique Boonstra", "Yael Barnett", "Mariano Cabezas", "Arkiev D'Souza", "Matthew C. Kiernan", "Kain Kyle", "Meng Law", "Lynette Masters", "Zihao Tang", "Stephen Tisch", "Sicong Tu", "Anneke Van Der Walt", "Dongang Wang", "Fernando Calamante", "Weidong Cai", "Chenyu Wang"], "title": "From Promise to Practical Reality: Transforming Diffusion MRI Analysis with Fast Deep Learning Enhancement", "comment": "24 pages, 5 figures", "summary": "Fiber orientation distribution (FOD) is an advanced diffusion MRI modeling\ntechnique that represents complex white matter fiber configurations, and a key\nstep for subsequent brain tractography and connectome analysis. Its reliability\nand accuracy, however, heavily rely on the quality of the MRI acquisition and\nthe subsequent estimation of the FODs at each voxel. Generating reliable FODs\nfrom widely available clinical protocols with single-shell and\nlow-angular-resolution acquisitions remains challenging but could potentially\nbe addressed with recent advances in deep learning-based enhancement\ntechniques. Despite advancements, existing methods have predominantly been\nassessed on healthy subjects, which have proved to be a major hurdle for their\nclinical adoption. In this work, we validate a newly optimized enhancement\nframework, FastFOD-Net, across healthy controls and six neurological disorders.\nThis accelerated end-to-end deep learning framework enhancing FODs with\nsuperior performance and delivering training/inference efficiency for clinical\nuse ($60\\times$ faster comparing to its predecessor). With the most\ncomprehensive clinical evaluation to date, our work demonstrates the potential\nof FastFOD-Net in accelerating clinical neuroscience research, empowering\ndiffusion MRI analysis for disease differentiation, improving interpretability\nin connectome applications, and reducing measurement errors to lower sample\nsize requirements. Critically, this work will facilitate the more widespread\nadoption of, and build clinical trust in, deep learning based methods for\ndiffusion MRI enhancement. Specifically, FastFOD-Net enables robust analysis of\nreal-world, clinical diffusion MRI data, comparable to that achievable with\nhigh-quality research acquisitions.", "AI": {"tldr": "FastFOD-Net是一个加速的深度学习框架，能够从低质量临床扩散MRI数据中增强纤维方向分布（FODs），从而在多种神经系统疾病中实现可靠分析，并加速临床神经科学研究。", "motivation": "纤维方向分布（FOD）的可靠性依赖于MRI采集质量，但从广泛使用的临床低分辨率协议中生成可靠FODs仍具挑战。现有深度学习增强方法主要在健康受试者上评估，限制了其临床应用。", "method": "本文验证了一个名为FastFOD-Net的优化增强框架。这是一个加速的端到端深度学习框架，用于增强FODs，并被广泛评估于健康对照组和六种神经系统疾病患者。", "result": "FastFOD-Net在FOD增强方面表现出卓越性能，并提供了高训练/推理效率（比前身快60倍）。它能对真实世界的临床扩散MRI数据进行鲁棒分析，其效果可与高质量研究采集数据媲美。", "conclusion": "FastFOD-Net有望加速临床神经科学研究，赋能疾病鉴别中的扩散MRI分析，提高连接组应用的解释性，并减少测量误差。它将促进深度学习方法在扩散MRI增强领域的更广泛应用和临床信任。"}}
{"id": "2508.11197", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2508.11197", "abs": "https://arxiv.org/abs/2508.11197", "authors": ["Ahmad Mousavi", "Yeganeh Abdollahinejad", "Roberto Corizzo", "Nathalie Japkowicz", "Zois Boukouvalas"], "title": "E-CaTCH: Event-Centric Cross-Modal Attention with Temporal Consistency and Class-Imbalance Handling for Misinformation Detection", "comment": null, "summary": "Detecting multimodal misinformation on social media remains challenging due\nto inconsistencies between modalities, changes in temporal patterns, and\nsubstantial class imbalance. Many existing methods treat posts independently\nand fail to capture the event-level structure that connects them across time\nand modality. We propose E-CaTCH, an interpretable and scalable framework for\nrobustly detecting misinformation. If needed, E-CaTCH clusters posts into\npseudo-events based on textual similarity and temporal proximity, then\nprocesses each event independently. Within each event, textual and visual\nfeatures are extracted using pre-trained BERT and ResNet encoders, refined via\nintra-modal self-attention, and aligned through bidirectional cross-modal\nattention. A soft gating mechanism fuses these representations to form\ncontextualized, content-aware embeddings of each post. To model temporal\nevolution, E-CaTCH segments events into overlapping time windows and uses a\ntrend-aware LSTM, enhanced with semantic shift and momentum signals, to encode\nnarrative progression over time. Classification is performed at the event\nlevel, enabling better alignment with real-world misinformation dynamics. To\naddress class imbalance and promote stable learning, the model integrates\nadaptive class weighting, temporal consistency regularization, and hard-example\nmining. The total loss is aggregated across all events. Extensive experiments\non Fakeddit, IND, and COVID-19 MISINFOGRAPH demonstrate that E-CaTCH\nconsistently outperforms state-of-the-art baselines. Cross-dataset evaluations\nfurther demonstrate its robustness, generalizability, and practical\napplicability across diverse misinformation scenarios.", "AI": {"tldr": "E-CaTCH是一种可解释、可扩展的框架，通过将帖子聚类成伪事件，整合多模态注意力、趋势感知时间序列建模和鲁棒学习策略，有效检测社交媒体上的多模态虚假信息，解决了模态不一致、时间模式变化和类别不平衡等挑战。", "motivation": "社交媒体上的多模态虚假信息检测面临挑战：模态间的不一致性、时间模式的变化以及严重的类别不平衡。许多现有方法独立处理帖子，未能捕捉到跨时间和模态连接它们的事件级结构。", "method": "E-CaTCH首先根据文本相似性和时间接近度将帖子聚类为伪事件。在每个事件内部，使用BERT和ResNet提取文本和视觉特征，通过模内自注意力进行精炼，并通过双向跨模态注意力进行对齐。一个软门控机制融合这些表示形成上下文感知嵌入。为了建模时间演变，E-CaTCH将事件分割成重叠的时间窗口，并使用一个结合语义偏移和动量信号的趋势感知LSTM来编码叙事进展。分类在事件级别进行。为解决类别不平衡和促进稳定学习，模型集成了自适应类别加权、时间一致性正则化和难例挖掘。总损失在所有事件中聚合。", "result": "在Fakeddit、IND和COVID-19 MISINFOGRAPH数据集上的大量实验表明，E-CaTCH持续优于最先进的基线方法。跨数据集评估进一步证明了其在不同虚假信息场景中的鲁棒性、泛化性和实际适用性。", "conclusion": "E-CaTCH是一个可解释且可扩展的框架，能够鲁棒地检测多模态虚假信息，通过事件级处理、多模态融合、时间演变建模和多种鲁棒性策略，有效应对了该领域的关键挑战，并展现出优异的性能和泛化能力。"}}
{"id": "2508.11396", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11396", "abs": "https://arxiv.org/abs/2508.11396", "authors": ["Jingran Zhang", "Zhengzhang Yan", "Yiming Chen", "Zeqiang He", "Jiahao Chen"], "title": "Pedestrian Dead Reckoning using Invariant Extended Kalman Filter", "comment": null, "summary": "This paper presents a cost-effective inertial pedestrian dead reckoning\nmethod for the bipedal robot in the GPS-denied environment. Each time when the\ninertial measurement unit (IMU) is on the stance foot, a stationary\npseudo-measurement can be executed to provide innovation to the IMU measurement\nbased prediction. The matrix Lie group based theoretical development of the\nadopted invariant extended Kalman filter (InEKF) is set forth for tutorial\npurpose. Three experiments are conducted to compare between InEKF and standard\nEKF, including motion capture benchmark experiment, large-scale multi-floor\nwalking experiment, and bipedal robot experiment, as an effort to show our\nmethod's feasibility in real-world robot system. In addition, a sensitivity\nanalysis is included to show that InEKF is much easier to tune than EKF.", "AI": {"tldr": "本文提出了一种针对双足机器人在GPS拒绝环境下经济高效的惯性行人航位推算方法，利用站立脚的静止伪测量和基于矩阵李群的不变扩展卡尔曼滤波器(InEKF)，并实验验证了其可行性和易调性。", "motivation": "在GPS拒绝环境下，双足机器人需要一种经济高效的行人航位推算方法来确定自身位置。", "method": "该方法在惯性测量单元(IMU)位于站立脚时执行静止伪测量，为IMU测量预测提供创新。理论发展基于矩阵李群的不变扩展卡尔曼滤波器(InEKF)。通过三个实验（运动捕捉基准、大规模多楼层行走、双足机器人实验）与标准EKF进行比较，并进行了灵敏度分析。", "result": "实验结果表明，该方法在真实世界的机器人系统中具有可行性。此外，灵敏度分析显示InEKF比EKF更容易调优。", "conclusion": "所提出的惯性行人航位推算方法对于双足机器人在GPS拒绝环境中的应用是可行且有效的，并且InEKF相比EKF在调优方面更具优势。"}}
{"id": "2508.11189", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.11189", "abs": "https://arxiv.org/abs/2508.11189", "authors": ["Chenyang Le", "Yinfeng Xia", "Huiyan Li", "Manhong Wang", "Yutao Sun", "Xingyang Ma", "Yanmin Qian"], "title": "Novel Parasitic Dual-Scale Modeling for Efficient and Accurate Multilingual Speech Translation", "comment": "Interspeech 2025", "summary": "Recent advancements in speech-to-text translation have led to the development\nof multilingual models capable of handling multiple language pairs\nsimultaneously. However, these unified models often suffer from large parameter\nsizes, making it challenging to balance inference efficiency and performance,\nparticularly in local deployment scenarios. We propose an innovative Parasitic\nDual-Scale Approach, which combines an enhanced speculative sampling method\nwith model compression and knowledge distillation techniques. Building on the\nWhisper Medium model, we enhance it for multilingual speech translation into\nwhisperM2M, and integrate our novel KVSPN module, achieving state-of-the-art\n(SOTA) performance across six popular languages with improved inference\nefficiency. KVSPN enables a 40\\% speedup with no BLEU score degradation.\nCombined with distillation methods, it represents a 2.6$\\times$ speedup over\nthe original Whisper Medium with superior performance.", "AI": {"tldr": "本文提出了一种寄生双尺度方法（Parasitic Dual-Scale Approach），结合增强的推测采样、模型压缩和知识蒸馏技术，在Whisper Medium模型（whisperM2M）上集成了KVSPN模块，显著提升了多语言语音翻译的推理效率和性能。", "motivation": "多语言统一语音到文本翻译模型参数量大，难以在推理效率和性能之间取得平衡，尤其是在本地部署场景下。", "method": "提出创新的寄生双尺度方法，结合增强的推测采样、模型压缩和知识蒸馏技术。具体而言，将Whisper Medium模型增强为whisperM2M，并集成新颖的KVSPN模块。", "result": "在六种常用语言上实现了最先进（SOTA）的性能。KVSPN模块实现了40%的速度提升，且BLEU分数无下降。与蒸馏方法结合后，相对于原始Whisper Medium模型，速度提升了2.6倍，同时性能更优越。", "conclusion": "所提出的寄生双尺度方法（包括KVSPN模块）有效解决了多语言语音翻译模型参数量大导致的效率问题，在显著提升推理速度的同时保持甚至超越了现有模型的性能。"}}
{"id": "2508.10955", "categories": ["cs.CV", "cs.CL", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.10955", "abs": "https://arxiv.org/abs/2508.10955", "authors": ["Wenbin An", "Jiahao Nie", "Yaqiang Wu", "Feng Tian", "Shijian Lu", "Qinghua Zheng"], "title": "Empowering Multimodal LLMs with External Tools: A Comprehensive Survey", "comment": "21 pages, 361 references", "summary": "By integrating the perception capabilities of multimodal encoders with the\ngenerative power of Large Language Models (LLMs), Multimodal Large Language\nModels (MLLMs), exemplified by GPT-4V, have achieved great success in various\nmultimodal tasks, pointing toward a promising pathway to artificial general\nintelligence. Despite this progress, the limited quality of multimodal data,\npoor performance on many complex downstream tasks, and inadequate evaluation\nprotocols continue to hinder the reliability and broader applicability of MLLMs\nacross diverse domains. Inspired by the human ability to leverage external\ntools for enhanced reasoning and problem-solving, augmenting MLLMs with\nexternal tools (e.g., APIs, expert models, and knowledge bases) offers a\npromising strategy to overcome these challenges. In this paper, we present a\ncomprehensive survey on leveraging external tools to enhance MLLM performance.\nOur discussion is structured along four key dimensions about external tools:\n(1) how they can facilitate the acquisition and annotation of high-quality\nmultimodal data; (2) how they can assist in improving MLLM performance on\nchallenging downstream tasks; (3) how they enable comprehensive and accurate\nevaluation of MLLMs; (4) the current limitations and future directions of\ntool-augmented MLLMs. Through this survey, we aim to underscore the\ntransformative potential of external tools in advancing MLLM capabilities,\noffering a forward-looking perspective on their development and applications.\nThe project page of this paper is publicly available\nathttps://github.com/Lackel/Awesome-Tools-for-MLLMs.", "AI": {"tldr": "本综述探讨了如何利用外部工具（如API、专家模型、知识库）来增强多模态大语言模型（MLLMs）的性能，以解决当前MLLMs在数据质量、复杂任务表现和评估方面的局限性。", "motivation": "尽管多模态大语言模型（MLLMs）在多模态任务中取得了巨大成功，但它们仍面临多模态数据质量有限、在许多复杂下游任务上表现不佳以及评估协议不完善等问题，这些因素限制了MLLMs的可靠性和广泛应用。受人类利用外部工具解决问题的启发，研究旨在通过工具增强来克服这些挑战。", "method": "本文通过一篇全面的综述，系统地探讨了利用外部工具来增强MLLM性能的方法。综述围绕四个关键维度展开讨论：外部工具如何促进高质量多模态数据的获取与标注；如何协助提升MLLM在挑战性下游任务上的表现；如何实现MLLM的全面准确评估；以及工具增强型MLLM的当前局限性与未来发展方向。", "result": "综述发现，外部工具能够有效促进高质量多模态数据的获取和标注，显著提升MLLM在复杂下游任务上的性能，并实现对MLLM更全面和准确的评估。此外，论文还识别了工具增强型MLLM的当前局限性。", "conclusion": "外部工具在推动MLLM能力发展方面展现出变革性潜力。本综述为工具增强型MLLM的未来发展和应用提供了前瞻性视角，强调了其在克服现有挑战中的关键作用。"}}
{"id": "2508.11247", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11247", "abs": "https://arxiv.org/abs/2508.11247", "authors": ["Changjian Wang", "Weihong Deng", "Weili Guan", "Quan Lu", "Ning Jiang"], "title": "Cross-Granularity Hypergraph Retrieval-Augmented Generation for Multi-hop Question Answering", "comment": null, "summary": "Multi-hop question answering (MHQA) requires integrating knowledge scattered\nacross multiple passages to derive the correct answer. Traditional\nretrieval-augmented generation (RAG) methods primarily focus on coarse-grained\ntextual semantic similarity and ignore structural associations among dispersed\nknowledge, which limits their effectiveness in MHQA tasks. GraphRAG methods\naddress this by leveraging knowledge graphs (KGs) to capture structural\nassociations, but they tend to overly rely on structural information and\nfine-grained word- or phrase-level retrieval, resulting in an underutilization\nof textual semantics. In this paper, we propose a novel RAG approach called\nHGRAG for MHQA that achieves cross-granularity integration of structural and\nsemantic information via hypergraphs. Structurally, we construct an entity\nhypergraph where fine-grained entities serve as nodes and coarse-grained\npassages as hyperedges, and establish knowledge association through shared\nentities. Semantically, we design a hypergraph retrieval method that integrates\nfine-grained entity similarity and coarse-grained passage similarity via\nhypergraph diffusion. Finally, we employ a retrieval enhancement module, which\nfurther refines the retrieved results both semantically and structurally, to\nobtain the most relevant passages as context for answer generation with the\nLLM. Experimental results on benchmark datasets demonstrate that our approach\noutperforms state-of-the-art methods in QA performance, and achieves a\n6$\\times$ speedup in retrieval efficiency.", "AI": {"tldr": "HGRAG是一种新颖的RAG方法，通过超图实现结构和语义信息的跨粒度集成，显著提升了多跳问答（MHQA）的性能和检索效率。", "motivation": "传统RAG方法侧重粗粒度文本语义相似性，忽略分散知识的结构关联；而GraphRAG方法过度依赖结构信息和细粒度检索，未能充分利用文本语义。MHQA任务需要整合分散在多段落中的知识，现有方法难以有效处理。", "method": "提出了HGRAG方法：1) 结构上，构建实体超图，将细粒度实体作为节点，粗粒度段落作为超边，通过共享实体建立知识关联。2) 语义上，设计超图检索方法，通过超图扩散整合细粒度实体相似性和粗粒度段落相似性。3) 采用检索增强模块，进一步在语义和结构上优化检索结果，为LLM生成答案提供最相关的上下文。", "result": "在基准数据集上的实验结果表明，HGRAG在问答性能上优于现有SOTA方法，并在检索效率上实现了6倍的提速。", "conclusion": "HGRAG通过超图有效整合了多跳问答任务中的结构和语义信息，解决了传统RAG和GraphRAG的局限性，从而显著提升了问答表现和检索效率。"}}
{"id": "2508.11404", "categories": ["cs.RO", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.11404", "abs": "https://arxiv.org/abs/2508.11404", "authors": ["Junyeon Kim", "Tianshu Ruan", "Cesar Alan Contreras", "Manolis Chiou"], "title": "An Exploratory Study on Crack Detection in Concrete through Human-Robot Collaboration", "comment": null, "summary": "Structural inspection in nuclear facilities is vital for maintaining\noperational safety and integrity. Traditional methods of manual inspection pose\nsignificant challenges, including safety risks, high cognitive demands, and\npotential inaccuracies due to human limitations. Recent advancements in\nArtificial Intelligence (AI) and robotic technologies have opened new\npossibilities for safer, more efficient, and accurate inspection methodologies.\nSpecifically, Human-Robot Collaboration (HRC), leveraging robotic platforms\nequipped with advanced detection algorithms, promises significant improvements\nin inspection outcomes and reductions in human workload. This study explores\nthe effectiveness of AI-assisted visual crack detection integrated into a\nmobile Jackal robot platform. The experiment results indicate that HRC enhances\ninspection accuracy and reduces operator workload, resulting in potential\nsuperior performance outcomes compared to traditional manual methods.", "AI": {"tldr": "研究探索了在核设施检查中，AI辅助的移动机器人（Jackal）进行视觉裂纹检测的人机协作（HRC）的有效性。", "motivation": "传统的核设施人工检查存在安全风险高、认知负荷大和准确性受限等问题。AI和机器人技术的发展为更安全、高效和准确的检查方法提供了新机遇，特别是人机协作。", "method": "将AI辅助的视觉裂纹检测技术集成到移动Jackal机器人平台上，并通过实验探索其有效性。", "result": "实验结果表明，人机协作能提高检查准确性并减轻操作员的工作负担，与传统人工方法相比，可能带来更优的性能。", "conclusion": "AI辅助的移动机器人与人机协作模式在核设施检查中具有潜力，能提升检查结果并减轻人类工作量，优于传统人工方法。"}}
{"id": "2508.11260", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11260", "abs": "https://arxiv.org/abs/2508.11260", "authors": ["Mukund Choudhary", "KV Aditya Srivatsa", "Gaurja Aeron", "Antara Raaghavi Bhattacharya", "Dang Khoa Dang Dinh", "Ikhlasul Akmal Hanif", "Daria Kotova", "Ekaterina Kochmar", "Monojit Choudhury"], "title": "UNVEILING: What Makes Linguistics Olympiad Puzzles Tricky for LLMs?", "comment": "Accepted to COLM 2025", "summary": "Large language models (LLMs) have demonstrated potential in reasoning tasks,\nbut their performance on linguistics puzzles remains consistently poor. These\npuzzles, often derived from Linguistics Olympiad (LO) contests, provide a\nminimal contamination environment to assess LLMs' linguistic reasoning\nabilities across low-resource languages. This work analyses LLMs' performance\non 629 problems across 41 low-resource languages by labelling each with\nlinguistically informed features to unveil weaknesses. Our analyses show that\nLLMs struggle with puzzles involving higher morphological complexity and\nperform better on puzzles involving linguistic features that are also found in\nEnglish. We also show that splitting words into morphemes as a pre-processing\nstep improves solvability, indicating a need for more informed and\nlanguage-specific tokenisers. These findings thus offer insights into some\nchallenges in linguistic reasoning and modelling of low-resource languages.", "AI": {"tldr": "大型语言模型（LLMs）在语言学谜题上表现不佳，尤其是在形态复杂性高和非英语特征的低资源语言上。词素分割预处理可提高其解决能力。", "motivation": "尽管LLMs在推理任务中展现潜力，但在语言学谜题（特别是语言学奥林匹克竞赛题目）上表现持续不佳。这些谜题提供了一个低污染环境，用于评估LLMs在低资源语言上的语言推理能力。", "method": "研究分析了LLMs在41种低资源语言的629个语言学谜题上的表现，通过语言学特征对每个谜题进行标记以揭示弱点。此外，还测试了将单词分割成词素作为预处理步骤对可解性的影响。", "result": "分析显示LLMs在涉及更高形态复杂性的谜题上表现不佳，而在涉及英语中也存在的语言特征的谜题上表现更好。将单词分割成词素作为预处理步骤可以提高可解性。", "conclusion": "这些发现揭示了语言推理和低资源语言建模中的一些挑战，表明需要更具信息性和语言特定性的分词器。"}}
{"id": "2508.10962", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10962", "abs": "https://arxiv.org/abs/2508.10962", "authors": ["Jiarong Li", "Imad Ali Shah", "Diarmaid Geever", "Fiachra Collins", "Enda Ward", "Martin Glavin", "Edward Jones", "Brian Deegan"], "title": "CSNR and JMIM Based Spectral Band Selection for Reducing Metamerism in Urban Driving", "comment": "Under Review at IEEE OJITS, July, 2025", "summary": "Protecting Vulnerable Road Users (VRU) is a critical safety challenge for\nautomotive perception systems, particularly under visual ambiguity caused by\nmetamerism, a phenomenon where distinct materials appear similar in RGB\nimagery. This work investigates hyperspectral imaging (HSI) to overcome this\nlimitation by capturing unique material signatures beyond the visible spectrum,\nespecially in the Near-Infrared (NIR). To manage the inherent\nhigh-dimensionality of HSI data, we propose a band selection strategy that\nintegrates information theory techniques (joint mutual information\nmaximization, correlation analysis) with a novel application of an image\nquality metric (contrast signal-to-noise ratio) to identify the most spectrally\ninformative bands. Using the Hyperspectral City V2 (H-City) dataset, we\nidentify three informative bands (497 nm, 607 nm, and 895 nm, $\\pm$27 nm) and\nreconstruct pseudo-color images for comparison with co-registered RGB.\nQuantitative results demonstrate increased dissimilarity and perceptual\nseparability of VRU from the background. The selected HSI bands yield\nimprovements of 70.24%, 528.46%, 1206.83%, and 246.62% for dissimilarity\n(Euclidean, SAM, $T^2$) and perception (CIE $\\Delta E$) metrics, consistently\noutperforming RGB and confirming a marked reduction in metameric confusion. By\nproviding a spectrally optimized input, our method enhances VRU separability,\nestablishing a robust foundation for downstream perception tasks in Advanced\nDriver Assistance Systems (ADAS) and Autonomous Driving (AD), ultimately\ncontributing to improved road safety.", "AI": {"tldr": "解析错误", "motivation": "解析错误", "method": "解析错误", "result": "解析错误", "conclusion": "解析错误"}}
{"id": "2508.11256", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11256", "abs": "https://arxiv.org/abs/2508.11256", "authors": ["Junjie Wang", "Keyu Chen", "Yulin Li", "Bin Chen", "Hengshuang Zhao", "Xiaojuan Qi", "Zhuotao Tian"], "title": "Generalized Decoupled Learning for Enhancing Open-Vocabulary Dense Perception", "comment": "arXiv admin note: text overlap with arXiv:2505.04410", "summary": "Dense visual perception tasks have been constrained by their reliance on\npredefined categories, limiting their applicability in real-world scenarios\nwhere visual concepts are unbounded. While Vision-Language Models (VLMs) like\nCLIP have shown promise in open-vocabulary tasks, their direct application to\ndense perception often leads to suboptimal performance due to limitations in\nlocal feature representation. In this work, we present our observation that\nCLIP's image tokens struggle to effectively aggregate information from\nspatially or semantically related regions, resulting in features that lack\nlocal discriminability and spatial consistency. To address this issue, we\npropose DeCLIP, a novel framework that enhances CLIP by decoupling the\nself-attention module to obtain ``content'' and ``context'' features\nrespectively. \\revise{The context features are enhanced by jointly distilling\nsemantic correlations from Vision Foundation Models (VFMs) and object integrity\ncues from diffusion models, thereby enhancing spatial consistency. In parallel,\nthe content features are aligned with image crop representations and\nconstrained by region correlations from VFMs to improve local discriminability.\nExtensive experiments demonstrate that DeCLIP establishes a solid foundation\nfor open-vocabulary dense perception, consistently achieving state-of-the-art\nperformance across a broad spectrum of tasks, including 2D detection and\nsegmentation, 3D instance segmentation, video instance segmentation, and 6D\nobject pose estimation.} Code is available at\nhttps://github.com/xiaomoguhz/DeCLIP", "AI": {"tldr": "DeCLIP通过解耦自注意力机制，增强了CLIP的局部特征表示能力，从而在开放词汇密集视觉感知任务中实现了最先进的性能。", "motivation": "传统的密集视觉感知任务受限于预定义类别，无法适应无边界的真实世界视觉概念。虽然CLIP等视觉-语言模型在开放词汇任务中表现出色，但其在密集感知任务中因局部特征表示不足而性能不佳，具体表现为图像token难以有效聚合空间或语义相关区域信息，导致特征缺乏局部判别性和空间一致性。", "method": "本文提出了DeCLIP框架，通过解耦自注意力模块，分别获取“内容”和“上下文”特征。上下文特征通过联合蒸馏视觉基础模型（VFMs）的语义关联和扩散模型的对象完整性线索来增强空间一致性。同时，内容特征与图像裁剪表示对齐，并受VFMs的区域相关性约束，以提高局部判别性。", "result": "DeCLIP在广泛的密集感知任务中持续实现了最先进的性能，包括2D检测和分割、3D实例分割、视频实例分割和6D物体姿态估计。", "conclusion": "DeCLIP为开放词汇密集感知奠定了坚实的基础，显著提升了CLIP在这些任务中的应用效果。"}}
{"id": "2508.11406", "categories": ["cs.RO", "cs.AI", "68T40", "I.2.9"], "pdf": "https://arxiv.org/pdf/2508.11406", "abs": "https://arxiv.org/abs/2508.11406", "authors": ["Benjamin Alt", "Mareike Picklum", "Sorin Arion", "Franklin Kenghagho Kenfack", "Michael Beetz"], "title": "Open, Reproducible and Trustworthy Robot-Based Experiments with Virtual Labs and Digital-Twin-Based Execution Tracing", "comment": "8 pages, 6 figures, submitted to the 1st IROS Workshop on Embodied AI\n  and Robotics for Future Scientific Discovery", "summary": "We envision a future in which autonomous robots conduct scientific\nexperiments in ways that are not only precise and repeatable, but also open,\ntrustworthy, and transparent. To realize this vision, we present two key\ncontributions: a semantic execution tracing framework that logs sensor data\ntogether with semantically annotated robot belief states, ensuring that\nautomated experimentation is transparent and replicable; and the AICOR Virtual\nResearch Building (VRB), a cloud-based platform for sharing, replicating, and\nvalidating robot task executions at scale. Together, these tools enable\nreproducible, robot-driven science by integrating deterministic execution,\nsemantic memory, and open knowledge representation, laying the foundation for\nautonomous systems to participate in scientific discovery.", "AI": {"tldr": "该研究提出了一个语义执行追踪框架和一个云平台，旨在实现机器人驱动的科学实验的开放性、可信赖性和可重复性。", "motivation": "目前的自主机器人科学实验缺乏精确、可重复性、开放性、可信赖性和透明度，难以实现科学发现的愿景。", "method": "1. 语义执行追踪框架：记录传感器数据和带语义注释的机器人信念状态，确保自动化实验的透明性和可复制性。2. AICOR虚拟研究大楼（VRB）：一个基于云的平台，用于大规模共享、复制和验证机器人任务执行。3. 整合确定性执行、语义记忆和开放知识表示。", "result": "这些工具共同实现了可重复的机器人驱动科学，为自主系统参与科学发现奠定了基础。", "conclusion": "通过结合语义追踪和云平台，该研究成功构建了透明和可复制的自动化实验系统，为未来的自主科学发现铺平了道路。"}}
{"id": "2508.11280", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11280", "abs": "https://arxiv.org/abs/2508.11280", "authors": ["Ruiyan Qi", "Congding Wen", "Weibo Zhou", "Shangsong Liang", "Lingbo Li"], "title": "LETToT: Label-Free Evaluation of Large Language Models On Tourism Using Expert Tree-of-Thought", "comment": null, "summary": "Evaluating large language models (LLMs) in specific domain like tourism\nremains challenging due to the prohibitive cost of annotated benchmarks and\npersistent issues like hallucinations. We propose $\\textbf{L}$able-Free\n$\\textbf{E}$valuation of LLM on $\\textbf{T}$ourism using Expert\n$\\textbf{T}$ree-$\\textbf{o}$f-$\\textbf{T}$hought (LETToT), a framework that\nleverages expert-derived reasoning structures-instead of labeled data-to access\nLLMs in tourism. First, we iteratively refine and validate hierarchical ToT\ncomponents through alignment with generic quality dimensions and expert\nfeedback. Results demonstrate the effectiveness of our systematically optimized\nexpert ToT with 4.99-14.15\\% relative quality gains over baselines. Second, we\napply LETToT's optimized expert ToT to evaluate models of varying scales\n(32B-671B parameters), revealing: (1) Scaling laws persist in specialized\ndomains (DeepSeek-V3 leads), yet reasoning-enhanced smaller models (e.g.,\nDeepSeek-R1-Distill-Llama-70B) close this gap; (2) For sub-72B models, explicit\nreasoning architectures outperform counterparts in accuracy and conciseness\n($p<0.05$). Our work established a scalable, label-free paradigm for\ndomain-specific LLM evaluation, offering a robust alternative to conventional\nannotated benchmarks.", "AI": {"tldr": "提出LETToT框架，利用专家驱动的思维链（ToT）结构，实现旅游领域大型语言模型（LLM）的无标签评估，解决了标注成本高和幻觉问题。", "motivation": "在旅游等特定领域评估大型语言模型（LLM）面临挑战，主要原因是标注基准的高昂成本以及持续存在的幻觉问题。", "method": "提出了LETToT（使用专家思维链的旅游领域LLM无标签评估）框架。首先，通过与通用质量维度和专家反馈对齐，迭代优化并验证分层ToT组件。其次，将优化后的专家ToT应用于评估不同规模（32B-671B参数）的模型。", "result": "系统优化的专家ToT比基线模型有4.99-14.15%的相对质量提升。在专业领域，缩放定律依然存在（DeepSeek-V3领先），但经过推理增强的小型模型（如DeepSeek-R1-Distill-Llama-70B）能缩小差距。对于小于72B的模型，显式推理架构在准确性和简洁性上优于其他模型（p<0.05）。", "conclusion": "本研究建立了一个可扩展的、无标签的领域特定LLM评估范式，为传统标注基准提供了一个强大的替代方案。"}}
{"id": "2508.10963", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10963", "abs": "https://arxiv.org/abs/2508.10963", "authors": ["Zixiang Yang", "Yue Ma", "Yinhan Zhang", "Shanhui Mo", "Dongrui Liu", "Linfeng Zhang"], "title": "EVCtrl: Efficient Control Adapter for Visual Generation", "comment": null, "summary": "Visual generation includes both image and video generation, training\nprobabilistic models to create coherent, diverse, and semantically faithful\ncontent from scratch. While early research focused on unconditional sampling,\npractitioners now demand controllable generation that allows precise\nspecification of layout, pose, motion, or style. While ControlNet grants\nprecise spatial-temporal control, its auxiliary branch markedly increases\nlatency and introduces redundant computation in both uncontrolled regions and\ndenoising steps, especially for video. To address this problem, we introduce\nEVCtrl, a lightweight, plug-and-play control adapter that slashes overhead\nwithout retraining the model. Specifically, we propose a spatio-temporal dual\ncaching strategy for sparse control information. For spatial redundancy, we\nfirst profile how each layer of DiT-ControlNet responds to fine-grained\ncontrol, then partition the network into global and local functional zones. A\nlocality-aware cache focuses computation on the local zones that truly need the\ncontrol signal, skipping the bulk of redundant computation in global regions.\nFor temporal redundancy, we selectively omit unnecessary denoising steps to\nimprove efficiency. Extensive experiments on CogVideo-Controlnet,\nWan2.1-Controlnet, and Flux demonstrate that our method is effective in image\nand video control generation without the need for training. For example, it\nachieves 2.16 and 2.05 times speedups on CogVideo-Controlnet and\nWan2.1-Controlnet, respectively, with almost no degradation in generation\nquality.Codes are available in the supplementary materials.", "AI": {"tldr": "EVCtrl是一种轻量级、即插即用的控制适配器，通过时空双缓存策略，显著降低了ControlNet在图像和视频生成中的计算开销和延迟，且无需重新训练模型。", "motivation": "现有ControlNet模型虽然提供了精确的空间-时间控制，但其辅助分支显著增加了延迟，并在非控制区域和去噪步骤中引入了冗余计算，尤其在视频生成中问题更突出。", "method": "本文提出了EVCtrl，一个轻量级的即插即用控制适配器，无需重新训练模型。核心方法是稀疏控制信息的时空双缓存策略：1) 针对空间冗余，分析DiT-ControlNet各层对精细控制的响应，将网络划分为全局和局部功能区域，并利用局部感知缓存将计算集中在真正需要控制信号的局部区域；2) 针对时间冗余，选择性地跳过不必要的去噪步骤以提高效率。", "result": "在CogVideo-Controlnet、Wan2.1-Controlnet和Flux上的大量实验表明，EVCtrl在图像和视频控制生成中表现出色，无需训练。例如，在CogVideo-Controlnet上实现了2.16倍的加速，在Wan2.1-Controlnet上实现了2.05倍的加速，且生成质量几乎没有下降。", "conclusion": "EVCtrl通过创新的时空双缓存策略，有效解决了ControlNet在视觉生成中存在的延迟和计算冗余问题，提供了一个高效、轻量级的可控生成解决方案，且无需模型再训练。"}}
{"id": "2508.11262", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11262", "abs": "https://arxiv.org/abs/2508.11262", "authors": ["Aiswarya Konavoor", "Raj Abhijit Dandekar", "Rajat Dandekar", "Sreedath Panat"], "title": "Vision-Language Models display a strong gender bias", "comment": null, "summary": "Vision-language models (VLM) align images and text in a shared representation\nspace that is useful for retrieval and zero-shot transfer. Yet, this alignment\ncan encode and amplify social stereotypes in subtle ways that are not obvious\nfrom standard accuracy metrics. In this study, we test whether the contrastive\nvision-language encoder exhibits gender-linked associations when it places\nembeddings of face images near embeddings of short phrases that describe\noccupations and activities. We assemble a dataset of 220 face photographs split\nby perceived binary gender and a set of 150 unique statements distributed\nacross six categories covering emotional labor, cognitive labor, domestic\nlabor, technical labor, professional roles, and physical labor. We compute\nunit-norm image embeddings for every face and unit-norm text embeddings for\nevery statement, then define a statement-level association score as the\ndifference between the mean cosine similarity to the male set and the mean\ncosine similarity to the female set, where positive values indicate stronger\nassociation with the male set and negative values indicate stronger association\nwith the female set. We attach bootstrap confidence intervals by resampling\nimages within each gender group, aggregate by category with a separate\nbootstrap over statements, and run a label-swap null model that estimates the\nlevel of mean absolute association we would expect if no gender structure were\npresent. The outcome is a statement-wise and category-wise map of gender\nassociations in a contrastive vision-language space, accompanied by\nuncertainty, simple sanity checks, and a robust gender bias evaluation\nframework.", "AI": {"tldr": "本研究揭示了视觉语言模型（VLM）在图像和文本对齐时，会以微妙方式编码和放大性别刻板印象，并提出了一种评估框架。", "motivation": "视觉语言模型（VLM）虽然能有效对齐图像和文本，但其对齐过程可能以标准准确性指标无法察觉的方式编码和放大社会刻板印象，尤其是在性别关联方面。", "method": "研究构建了一个包含220张按感知二元性别划分的面部照片和150个描述职业/活动的短语（分为六类）的数据集。计算面部图像和短语的单位范数嵌入，并定义了“语句级关联分数”：男性集合平均余弦相似度与女性集合平均余弦相似度之差。通过重采样图像和语句进行引导置信区间计算，并运行标签交换空模型来估计无性别结构时的预期关联水平。", "result": "研究成果是视觉语言空间中性别关联的语句级和类别级映射，并附带不确定性、简单的健全性检查以及一个鲁棒的性别偏见评估框架。", "conclusion": "本研究提供了一个评估对比视觉语言编码器中性别关联的框架，揭示了VLM可能存在的性别刻板印象编码问题，并为未来的偏见研究奠定了基础。"}}
{"id": "2508.11453", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11453", "abs": "https://arxiv.org/abs/2508.11453", "authors": ["Jiayue Jin", "Lang Qian", "Jingyu Zhang", "Chuanyu Ju", "Liang Song"], "title": "EvoPSF: Online Evolution of Autonomous Driving Models via Planning-State Feedback", "comment": null, "summary": "Recent years have witnessed remarkable progress in autonomous driving, with\nsystems evolving from modular pipelines to end-to-end architectures. However,\nmost existing methods are trained offline and lack mechanisms to adapt to new\nenvironments during deployment. As a result, their generalization ability\ndiminishes when faced with unseen variations in real-world driving scenarios.\nIn this paper, we break away from the conventional \"train once, deploy forever\"\nparadigm and propose EvoPSF, a novel online Evolution framework for autonomous\ndriving based on Planning-State Feedback. We argue that planning failures are\nprimarily caused by inaccurate object-level motion predictions, and such\nfailures are often reflected in the form of increased planner uncertainty. To\naddress this, we treat planner uncertainty as a trigger for online evolution,\nusing it as a diagnostic signal to initiate targeted model updates. Rather than\nperforming blind updates, we leverage the planner's agent-agent attention to\nidentify the specific objects that the ego vehicle attends to most, which are\nprimarily responsible for the planning failures. For these critical objects, we\ncompute a targeted self-supervised loss by comparing their predicted waypoints\nfrom the prediction module with their actual future positions, selected from\nthe perception module's outputs with high confidence scores. This loss is then\nbackpropagated to adapt the model online. As a result, our method improves the\nmodel's robustness to environmental changes, leads to more precise motion\npredictions, and therefore enables more accurate and stable planning behaviors.\nExperiments on both cross-region and corrupted variants of the nuScenes dataset\ndemonstrate that EvoPSF consistently improves planning performance under\nchallenging conditions.", "AI": {"tldr": "EvoPSF是一种新颖的在线进化框架，通过利用规划器不确定性作为触发器和智能体-智能体注意力来识别关键对象，并对其运动预测模块进行有针对性的自监督更新，从而提高自动驾驶模型在部署时的泛化能力和鲁棒性。", "motivation": "现有自动驾驶系统大多离线训练，部署后缺乏适应新环境的机制，导致在面对未见过的真实世界驾驶场景变体时，其泛化能力下降。", "method": "EvoPSF提出在线进化框架，基于规划状态反馈。它将规划失败归因于不准确的物体级运动预测，并利用规划器不确定性作为在线进化的触发信号。通过规划器的智能体-智能体注意力识别导致规划失败的关键物体，计算一个有针对性的自监督损失（比较预测航点与感知模块高置信度输出的实际未来位置），并反向传播以在线适应模型。", "result": "在nuScenes数据集的跨区域和损坏变体上的实验表明，EvoPSF在挑战性条件下持续提高了规划性能。", "conclusion": "EvoPSF通过在线适应提高了模型对环境变化的鲁棒性，带来了更精确的运动预测，从而实现了更准确和稳定的规划行为。"}}
{"id": "2508.11281", "categories": ["cs.CL", "cs.AI", "cs.CY", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.11281", "abs": "https://arxiv.org/abs/2508.11281", "authors": ["Axel Delaval", "Shujian Yang", "Haicheng Wang", "Han Qiu", "Jialiang Lu"], "title": "ToxiFrench: Benchmarking and Enhancing Language Models via CoT Fine-Tuning for French Toxicity Detection", "comment": "14 pages, 5 figures, 8 tables. This paper introduces TOXIFRENCH, a\n  new large-scale benchmark for French toxicity detection, and proposes a\n  Chain-of-Thought (CoT) fine-tuning method with a dynamic weighted loss. The\n  resulting fine-tuned 4B parameter model, ToxiFrench, achieves\n  state-of-the-art performance, outperforming larger models like GPT-4o", "summary": "Detecting toxic content using language models is crucial yet challenging.\nWhile substantial progress has been made in English, toxicity detection in\nFrench remains underdeveloped, primarily due to the lack of culturally\nrelevant, large-scale datasets. In this work, we introduce TOXIFRENCH, a new\npublic benchmark of 53,622 French online comments, constructed via a\nsemi-automated annotation pipeline that reduces manual labeling to only 10%\nthrough high-confidence LLM-based pre-annotation and human verification. Then,\nwe benchmark a broad range of models and uncover a counterintuitive insight:\nSmall Language Models (SLMs) outperform many larger models in robustness and\ngeneralization under the toxicity detection task. Motivated by this finding, we\npropose a novel Chain-of-Thought (CoT) fine-tuning strategy using a dynamic\nweighted loss that progressively emphasizes the model's final decision,\nsignificantly improving faithfulness. Our fine-tuned 4B model achieves\nstate-of-the-art performance, improving its F1 score by 13% over its baseline\nand outperforming LLMs such as GPT-40 and Gemini-2.5. Further evaluation on a\ncross-lingual toxicity benchmark demonstrates strong multilingual ability,\nsuggesting that our methodology can be effectively extended to other languages\nand safety-critical classification tasks.", "AI": {"tldr": "该研究引入了TOXIFRENCH，一个包含53,622条法语在线评论的新型毒性检测基准数据集，并发现小型语言模型在毒性检测任务中表现优于大型模型。作者提出了一种新的CoT微调策略，使其4B模型在法语毒性检测中达到SOTA性能，并展现出强大的跨语言能力。", "motivation": "尽管英语毒性内容检测取得了显著进展，但法语毒性检测仍不成熟，主要原因是缺乏与文化相关的、大规模数据集。", "method": "1. 构建TOXIFRENCH数据集：采用半自动化标注流程，通过LLM预标注和人工验证，将人工标注量减少至10%。2. 基准测试：评估各种模型在毒性检测任务上的表现。3. 提出CoT微调策略：使用动态加权损失，逐步强调模型的最终决策，以提高模型的忠实性。", "result": "1. 发现小型语言模型（SLMs）在毒性检测任务的鲁棒性和泛化能力上优于许多大型模型。2. 提出的CoT微调后的4B模型实现了最先进的性能，F1分数比基线提高了13%，并超越了GPT-40和Gemini-2.5等大型语言模型。3. 在跨语言毒性基准测试中表现出强大的多语言能力。", "conclusion": "该研究引入的TOXIFRENCH数据集和提出的CoT微调方法显著提升了法语毒性检测的性能。其方法具有强大的多语言能力，可有效扩展到其他语言和安全关键型分类任务。"}}
{"id": "2508.11011", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11011", "abs": "https://arxiv.org/abs/2508.11011", "authors": ["Xuezheng Chen", "Zhengbo Zou"], "title": "Are Large Pre-trained Vision Language Models Effective Construction Safety Inspectors?", "comment": null, "summary": "Construction safety inspections typically involve a human inspector\nidentifying safety concerns on-site. With the rise of powerful Vision Language\nModels (VLMs), researchers are exploring their use for tasks such as detecting\nsafety rule violations from on-site images. However, there is a lack of open\ndatasets to comprehensively evaluate and further fine-tune VLMs in construction\nsafety inspection. Current applications of VLMs use small, supervised datasets,\nlimiting their applicability in tasks they are not directly trained for. In\nthis paper, we propose the ConstructionSite 10k, featuring 10,000 construction\nsite images with annotations for three inter-connected tasks, including image\ncaptioning, safety rule violation visual question answering (VQA), and\nconstruction element visual grounding. Our subsequent evaluation of current\nstate-of-the-art large pre-trained VLMs shows notable generalization abilities\nin zero-shot and few-shot settings, while additional training is needed to make\nthem applicable to actual construction sites. This dataset allows researchers\nto train and evaluate their own VLMs with new architectures and techniques,\nproviding a valuable benchmark for construction safety inspection.", "AI": {"tldr": "本文提出了一个名为ConstructionSite 10k的新型开放数据集，包含10,000张建筑工地图像，用于评估和微调视觉语言模型（VLMs）在建筑安全检查中的应用，并评估了现有VLMs的性能。", "motivation": "目前的建筑安全检查依赖人工，而强大的视觉语言模型（VLMs）在检测安全违规方面显示出潜力。然而，缺乏开放数据集来全面评估和微调VLMs在建筑安全检查中的应用，且现有VLM应用使用的监督数据集规模小，限制了其泛化能力。", "method": "研究者提出了ConstructionSite 10k数据集，包含10,000张建筑工地图像，并针对三个相互关联的任务进行了标注：图像字幕、安全规则违规视觉问答（VQA）和建筑元素视觉定位。随后，他们评估了当前最先进的大型预训练VLMs在该数据集上的表现。", "result": "对现有最先进的预训练VLMs的评估表明，它们在零样本和少样本设置下具有显著的泛化能力，但仍需要额外的训练才能实际应用于建筑工地。", "conclusion": "ConstructionSite 10k数据集为研究人员训练和评估其自有的VLM模型提供了宝贵的基准，推动了建筑安全检查领域的研究进展。"}}
{"id": "2508.11272", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11272", "abs": "https://arxiv.org/abs/2508.11272", "authors": ["Jun Li", "Kai Li", "Shaoguo Liu", "Tingting Gao"], "title": "Enhancing Supervised Composed Image Retrieval via Reasoning-Augmented Representation Engineering", "comment": null, "summary": "Composed Image Retrieval (CIR) presents a significant challenge as it\nrequires jointly understanding a reference image and a modified textual\ninstruction to find relevant target images. Some existing methods attempt to\nuse a two-stage approach to further refine retrieval results. However, this\noften requires additional training of a ranking model. Despite the success of\nChain-of-Thought (CoT) techniques in reducing training costs for language\nmodels, their application in CIR tasks remains limited -- compressing visual\ninformation into text or relying on elaborate prompt designs. Besides, existing\nworks only utilize it for zero-shot CIR, as it is challenging to achieve\nsatisfactory results in supervised CIR with a well-trained model. In this work,\nwe proposed a framework that includes the Pyramid Matching Model with\nTraining-Free Refinement (PMTFR) to address these challenges. Through a simple\nbut effective module called Pyramid Patcher, we enhanced the Pyramid Matching\nModel's understanding of visual information at different granularities.\nInspired by representation engineering, we extracted representations from COT\ndata and injected them into the LVLMs. This approach allowed us to obtain\nrefined retrieval scores in the Training-Free Refinement paradigm without\nrelying on explicit textual reasoning, further enhancing performance. Extensive\nexperiments on CIR benchmarks demonstrate that PMTFR surpasses state-of-the-art\nmethods in supervised CIR tasks. The code will be made public.", "AI": {"tldr": "本文提出PMTFR框架，通过金字塔补丁器增强多粒度视觉理解，并利用CoT数据提取的表示进行免训练精炼，在监督式组合图像检索（CIR）任务中超越现有SOTA方法。", "motivation": "组合图像检索（CIR）任务因需要联合理解图像和文本指令而具有挑战性。现有两阶段方法通常需要额外训练排序模型。链式思考（CoT）技术在CIR中的应用受限，表现为视觉信息压缩、依赖复杂提示或仅限于零样本场景，难以在监督式CIR中取得满意效果。", "method": "本文提出了包含金字塔匹配模型和免训练精炼（PMTFR）的框架。通过“金字塔补丁器”模块，增强了金字塔匹配模型对不同粒度视觉信息的理解。受表示工程启发，从CoT数据中提取表示并注入到LVLM中，实现了在免训练精炼范式下获得精炼的检索分数，且不依赖显式文本推理。", "result": "在CIR基准测试上的大量实验表明，PMTFR在监督式CIR任务中超越了现有最先进的方法。", "conclusion": "PMTFR通过改进多粒度视觉理解和引入免训练的CoT表示注入精炼范式，有效解决了监督式CIR中的挑战，并取得了最先进的性能。"}}
{"id": "2508.11479", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11479", "abs": "https://arxiv.org/abs/2508.11479", "authors": ["Tatiana Zemskova", "Aleksei Staroverov", "Dmitry Yudin", "Aleksandr Panov"], "title": "OVSegDT: Segmenting Transformer for Open-Vocabulary Object Goal Navigation", "comment": null, "summary": "Open-vocabulary Object Goal Navigation requires an embodied agent to reach\nobjects described by free-form language, including categories never seen during\ntraining. Existing end-to-end policies overfit small simulator datasets,\nachieving high success on training scenes but failing to generalize and\nexhibiting unsafe behaviour (frequent collisions). We introduce OVSegDT, a\nlightweight transformer policy that tackles these issues with two synergistic\ncomponents. The first component is the semantic branch, which includes an\nencoder for the target binary mask and an auxiliary segmentation loss function,\ngrounding the textual goal and providing precise spatial cues. The second\ncomponent consists of a proposed Entropy-Adaptive Loss Modulation, a per-sample\nscheduler that continuously balances imitation and reinforcement signals\naccording to the policy entropy, eliminating brittle manual phase switches.\nThese additions cut the sample complexity of training by 33%, and reduce\ncollision count in two times while keeping inference cost low (130M parameters,\nRGB-only input). On HM3D-OVON, our model matches the performance on unseen\ncategories to that on seen ones and establishes state-of-the-art results (40.1%\nSR, 20.9% SPL on val unseen) without depth, odometry, or large vision-language\nmodels. Code is available at https://github.com/CognitiveAISystems/OVSegDT.", "AI": {"tldr": "OVSegDT是一种轻量级Transformer策略，通过引入语义分支和熵自适应损失调制，解决了开放词汇目标导航中现有策略过拟合、泛化性差和碰撞问题，显著提升了训练效率和导航安全性。", "motivation": "现有端到端策略在小规模模拟器数据集上容易过拟合，导致在训练场景表现良好但在泛化到新场景时失败，并频繁发生碰撞，行为不安全。", "method": "引入OVSegDT，包含两个核心组件：1. 语义分支：包含目标二值掩码编码器和辅助分割损失函数，用于 grounding 文本目标并提供精确空间线索。2. 熵自适应损失调制：一种按样本调度器，根据策略熵持续平衡模仿学习和强化学习信号，避免了脆弱的手动阶段切换。", "result": "训练样本复杂度降低33%，碰撞次数减少一半，同时推理成本保持较低（1.3亿参数，仅RGB输入）。在HM3D-OVON数据集上，模型在未见类别上的性能与已见类别持平，并取得了最先进的结果（验证集未见类别SR 40.1%，SPL 20.9%），且无需深度信息、里程计或大型视觉-语言模型。", "conclusion": "OVSegDT通过其创新的语义接地和损失平衡机制，有效解决了开放词汇目标导航中的泛化和安全问题，提供了一个高效、轻量且高性能的解决方案。"}}
{"id": "2508.11285", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11285", "abs": "https://arxiv.org/abs/2508.11285", "authors": ["Arya VarastehNezhad", "Reza Tavasoli", "Soroush Elyasi", "MohammadHossein LotfiNia", "Hamed Farbeh"], "title": "AI in Mental Health: Emotional and Sentiment Analysis of Large Language Models' Responses to Depression, Anxiety, and Stress Queries", "comment": null, "summary": "Depression, anxiety, and stress are widespread mental health concerns that\nincreasingly drive individuals to seek information from Large Language Models\n(LLMs). This study investigates how eight LLMs (Claude Sonnet, Copilot, Gemini\nPro, GPT-4o, GPT-4o mini, Llama, Mixtral, and Perplexity) reply to twenty\npragmatic questions about depression, anxiety, and stress when those questions\nare framed for six user profiles (baseline, woman, man, young, old, and\nuniversity student). The models generated 2,880 answers, which we scored for\nsentiment and emotions using state-of-the-art tools. Our analysis revealed that\noptimism, fear, and sadness dominated the emotional landscape across all\noutputs, with neutral sentiment maintaining consistently high values.\nGratitude, joy, and trust appeared at moderate levels, while emotions such as\nanger, disgust, and love were rarely expressed. The choice of LLM significantly\ninfluenced emotional expression patterns. Mixtral exhibited the highest levels\nof negative emotions including disapproval, annoyance, and sadness, while Llama\ndemonstrated the most optimistic and joyful responses. The type of mental\nhealth condition dramatically shaped emotional responses: anxiety prompts\nelicited extraordinarily high fear scores (0.974), depression prompts generated\nelevated sadness (0.686) and the highest negative sentiment, while\nstress-related queries produced the most optimistic responses (0.755) with\nelevated joy and trust. In contrast, demographic framing of queries produced\nonly marginal variations in emotional tone. Statistical analyses confirmed\nsignificant model-specific and condition-specific differences, while\ndemographic influences remained minimal. These findings highlight the critical\nimportance of model selection in mental health applications, as each LLM\nexhibits a distinct emotional signature that could significantly impact user\nexperience and outcomes.", "AI": {"tldr": "本研究调查了八种大型语言模型（LLMs）如何回应关于抑郁、焦虑和压力的实用问题，并分析了其情感和情绪表达，发现模型选择和心理健康状况类型对LLM的响应情绪有显著影响，而用户画像影响甚微。", "motivation": "抑郁、焦虑和压力是普遍存在的心理健康问题，越来越多的人转向大型语言模型（LLMs）寻求信息。因此，了解LLMs如何回应这些查询，特别是在情感和情绪表达方面，对于其在心理健康应用中的影响至关重要。", "method": "研究选取了八种LLMs（Claude Sonnet, Copilot, Gemini Pro, GPT-4o, GPT-4o mini, Llama, Mixtral, Perplexity），针对抑郁、焦虑和压力设计了20个实用问题。这些问题分别以六种用户画像（基线、女性、男性、年轻人、老年人、大学生）进行提问。共生成了2880个答案，并使用先进工具对这些答案的情绪和情感进行了评分分析。", "result": "分析发现，所有LLM的回答中，乐观、恐惧和悲伤是主导情绪，中性情感始终保持高位。Mixtral表现出最高的负面情绪，而Llama则最乐观和快乐。心理健康状况类型显著影响情绪反应：焦虑问题引发极高的恐惧分（0.974），抑郁问题导致高悲伤（0.686）和最高的负面情绪，压力问题则产生最乐观的回答（0.755）。相比之下，人口统计学上的用户画像对情绪基调影响微乎其微。统计分析证实了模型和状况特异性的显著差异，而人口统计学影响最小。", "conclusion": "研究结果强调了在心理健康应用中模型选择的极端重要性，因为每种LLM都展现出独特的情绪特征，这可能显著影响用户体验和结果。"}}
{"id": "2508.11021", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11021", "abs": "https://arxiv.org/abs/2508.11021", "authors": ["Zisheng Liang", "Kidus Zewde", "Rudra Pratap Singh", "Disha Patil", "Zexi Chen", "Jiayu Xue", "Yao Yao", "Yifei Chen", "Qinzhe Liu", "Simiao Ren"], "title": "Can Multi-modal (reasoning) LLMs detect document manipulation?", "comment": "arXiv admin note: text overlap with arXiv:2503.20084", "summary": "Document fraud poses a significant threat to industries reliant on secure and\nverifiable documentation, necessitating robust detection mechanisms. This study\ninvestigates the efficacy of state-of-the-art multi-modal large language models\n(LLMs)-including OpenAI O1, OpenAI 4o, Gemini Flash (thinking), Deepseek Janus,\nGrok, Llama 3.2 and 4, Qwen 2 and 2.5 VL, Mistral Pixtral, and Claude 3.5 and\n3.7 Sonnet-in detecting fraudulent documents. We benchmark these models against\neach other and prior work on document fraud detection techniques using a\nstandard dataset with real transactional documents. Through prompt optimization\nand detailed analysis of the models' reasoning processes, we evaluate their\nability to identify subtle indicators of fraud, such as tampered text,\nmisaligned formatting, and inconsistent transactional sums. Our results reveal\nthat top-performing multi-modal LLMs demonstrate superior zero-shot\ngeneralization, outperforming conventional methods on out-of-distribution\ndatasets, while several vision LLMs exhibit inconsistent or subpar performance.\nNotably, model size and advanced reasoning capabilities show limited\ncorrelation with detection accuracy, suggesting task-specific fine-tuning is\ncritical. This study underscores the potential of multi-modal LLMs in enhancing\ndocument fraud detection systems and provides a foundation for future research\ninto interpretable and scalable fraud mitigation strategies.", "AI": {"tldr": "本研究评估了多模态大语言模型（LLMs）在检测文档欺诈方面的有效性，发现顶级LLMs在零样本泛化能力上优于传统方法，并强调了任务特定微调的重要性。", "motivation": "文档欺诈对依赖安全可验证文档的行业构成重大威胁，因此需要强大的检测机制。", "method": "研究调查并基准测试了多种最先进的多模态LLMs（包括OpenAI O1/4o, Gemini Flash, Deepseek Janus, Grok, Llama 3.2/4, Qwen 2/2.5 VL, Mistral Pixtral, Claude 3.5/3.7 Sonnet），使用标准数据集和真实交易文档，通过提示优化和详细分析模型推理过程，评估其识别欺诈指标的能力。", "result": "顶级多模态LLMs表现出卓越的零样本泛化能力，在分布外数据集上优于传统方法；然而，一些视觉LLMs表现不稳定或不佳。值得注意的是，模型大小和高级推理能力与检测准确性相关性有限，表明任务特定的微调至关重要。", "conclusion": "本研究强调了多模态LLMs在增强文档欺诈检测系统方面的潜力，并为未来可解释和可扩展的欺诈缓解策略研究奠定了基础。"}}
{"id": "2508.11310", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.11310", "abs": "https://arxiv.org/abs/2508.11310", "authors": ["Beichen Guo", "Zhiyuan Wen", "Yu Yang", "Peng Gao", "Ruosong Yang", "Jiaxing Shen"], "title": "SGSimEval: A Comprehensive Multifaceted and Similarity-Enhanced Benchmark for Automatic Survey Generation Systems", "comment": "Accepted to The 21st International Conference on Advanced Data Mining\n  and Applications (ADMA2025)", "summary": "The growing interest in automatic survey generation (ASG), a task that\ntraditionally required considerable time and effort, has been spurred by recent\nadvances in large language models (LLMs). With advancements in\nretrieval-augmented generation (RAG) and the rising popularity of multi-agent\nsystems (MASs), synthesizing academic surveys using LLMs has become a viable\napproach, thereby elevating the need for robust evaluation methods in this\ndomain. However, existing evaluation methods suffer from several limitations,\nincluding biased metrics, a lack of human preference, and an over-reliance on\nLLMs-as-judges. To address these challenges, we propose SGSimEval, a\ncomprehensive benchmark for Survey Generation with Similarity-Enhanced\nEvaluation that evaluates automatic survey generation systems by integrating\nassessments of the outline, content, and references, and also combines\nLLM-based scoring with quantitative metrics to provide a multifaceted\nevaluation framework. In SGSimEval, we also introduce human preference metrics\nthat emphasize both inherent quality and similarity to humans. Extensive\nexperiments reveal that current ASG systems demonstrate human-comparable\nsuperiority in outline generation, while showing significant room for\nimprovement in content and reference generation, and our evaluation metrics\nmaintain strong consistency with human assessments.", "AI": {"tldr": "该论文提出了SGSimEval，一个用于自动调查问卷生成（ASG）的综合基准，通过整合大纲、内容和参考文献评估，并结合LLM评分与定量指标及人类偏好来解决现有评估方法的局限性。", "motivation": "随着LLM在自动调查问卷生成（ASG）中的应用日益广泛，需要更强大的评估方法。然而，现有评估方法存在偏差、缺乏人类偏好以及过度依赖LLM作为评判者等局限性。", "method": "本文提出了SGSimEval，一个具有相似性增强评估的调查问卷生成综合基准。它整合了对大纲、内容和参考文献的评估，结合了基于LLM的评分和定量指标，并引入了强调内在质量和与人类相似度的人类偏好指标，以提供多方面的评估框架。", "result": "广泛的实验表明，当前的ASG系统在大纲生成方面已达到与人类相当的水平，但在内容和参考文献生成方面仍有显著改进空间。此外，论文提出的评估指标与人类评估保持了很强的一致性。", "conclusion": "SGSimEval提供了一个全面且与人类评估高度一致的自动调查问卷生成评估框架。尽管当前ASG系统在大纲生成上表现出色，但在内容和参考文献生成方面仍需进一步提升。"}}
{"id": "2508.11485", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11485", "abs": "https://arxiv.org/abs/2508.11485", "authors": ["Hailiang Tang", "Tisheng Zhang", "Liqiang Wang", "Xin Ding", "Man Yuan", "Zhiyu Xiang", "Jujin Chen", "Yuhan Bian", "Shuangyan Liu", "Yuqing Wang", "Guan Wang", "Xiaoji Niu"], "title": "i2Nav-Robot: A Large-Scale Indoor-Outdoor Robot Dataset for Multi-Sensor Fusion Navigation and Mapping", "comment": "10 pages, 12 figures", "summary": "Accurate and reliable navigation is crucial for autonomous unmanned ground\nvehicle (UGV). However, current UGV datasets fall short in meeting the demands\nfor advancing navigation and mapping techniques due to limitations in sensor\nconfiguration, time synchronization, ground truth, and scenario diversity. To\naddress these challenges, we present i2Nav-Robot, a large-scale dataset\ndesigned for multi-sensor fusion navigation and mapping in indoor-outdoor\nenvironments. We integrate multi-modal sensors, including the newest front-view\nand 360-degree solid-state LiDARs, 4-dimensional (4D) radar, stereo cameras,\nodometer, global navigation satellite system (GNSS) receiver, and inertial\nmeasurement units (IMU) on an omnidirectional wheeled robot. Accurate\ntimestamps are obtained through both online hardware synchronization and\noffline calibration for all sensors. The dataset comprises ten larger-scale\nsequences covering diverse UGV operating scenarios, such as outdoor streets,\nand indoor parking lots, with a total length of about 17060 meters.\nHigh-frequency ground truth, with centimeter-level accuracy for position, is\nderived from post-processing integrated navigation methods using a\nnavigation-grade IMU. The proposed i2Nav-Robot dataset is evaluated by more\nthan ten open-sourced multi-sensor fusion systems, and it has proven to have\nsuperior data quality.", "AI": {"tldr": "i2Nav-Robot是一个大规模、多传感器融合的室内外无人地面车辆（UGV）导航和建图数据集，旨在解决现有数据集的局限性。", "motivation": "当前的UGV数据集在传感器配置、时间同步、真值准确性和场景多样性方面存在不足，限制了导航和建图技术的发展。", "method": "该研究在全向轮式机器人上集成了多种模态传感器，包括最新的固态激光雷达（前视和360度）、4D雷达、立体相机、里程计、GNSS接收器和IMU。通过在线硬件同步和离线校准确保精确的时间戳。厘米级精度的位置真值由导航级IMU的后处理集成导航方法获得。", "result": "i2Nav-Robot数据集包含十个大规模序列，总长度约17060米，覆盖了多种UGV操作场景，如室外街道和室内停车场。数据集提供了高频率、厘米级精度的位置真值。经过十多个开源多传感器融合系统的评估，该数据集显示出卓越的数据质量。", "conclusion": "i2Nav-Robot数据集通过提供多样化的传感器配置、精确的时间同步、高精度的真值和丰富的场景，有效解决了现有UGV数据集的局限性，有望推动多传感器融合导航和建图技术的发展。"}}
{"id": "2508.11290", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11290", "abs": "https://arxiv.org/abs/2508.11290", "authors": ["Utsav Maskey", "Sumit Yadav", "Mark Dras", "Usman Naseem"], "title": "SafeConstellations: Steering LLM Safety to Reduce Over-Refusals Through Task-Specific Trajectory", "comment": "Preprint", "summary": "LLMs increasingly exhibit over-refusal behavior, where safety mechanisms\ncause models to reject benign instructions that superficially resemble harmful\ncontent. This phenomena diminishes utility in production applications that\nrepeatedly rely on common prompt templates or applications that frequently rely\non LLMs for specific tasks (e.g. sentiment analysis, language translation).\nThrough comprehensive evaluation, we demonstrate that LLMs still tend to refuse\nresponses to harmful instructions when those instructions are reframed to\nappear as benign tasks. Our mechanistic analysis reveal that LLMs follow\ndistinct \"constellation\" patterns in embedding space as representations\ntraverse layers, with each task maintaining consistent trajectories that shift\npredictably between refusal and non-refusal cases. We introduce\nSafeConstellations, an inference-time trajectory-shifting approach that tracks\ntask-specific trajectory patterns and guides representations toward non-refusal\npathways. By selectively guiding model behavior only on tasks prone to\nover-refusal, and by preserving general model behavior, our method reduces\nover-refusal rates by up to 73% with minimal impact on utility-offering a\nprincipled approach to mitigating over-refusals.", "AI": {"tldr": "本文提出LLM过度拒绝（over-refusal）现象，即安全机制导致模型拒绝无害指令。通过分析嵌入空间中的“星座”模式和轨迹，本文引入SafeConstellations方法，在推理时引导模型表示走向非拒绝路径，有效降低过度拒绝率高达73%。", "motivation": "LLMs的过度拒绝行为降低了其在生产应用中的实用性，尤其是在重复使用常见提示模板或特定任务（如情感分析、语言翻译）时。这种行为使得模型无法响应表面上无害的指令，影响了用户体验和系统效率。", "method": "1. 综合评估：证明LLMs即使在有害指令被重构为无害任务时，仍倾向于拒绝响应。2. 机制分析：揭示LLMs在嵌入空间中遵循独特的“星座”模式，表示在层间遍历时保持一致的轨迹，并在拒绝和非拒绝情况下可预测地移动。3. 提出SafeConstellations：一种推理时轨迹转移方法，跟踪任务特定的轨迹模式，并将表示引导至非拒绝路径。", "result": "1. LLMs在有害指令被重构为无害任务时，仍倾向于拒绝响应。2. SafeConstellations方法通过选择性地引导模型行为，在容易过度拒绝的任务上，将过度拒绝率降低了高达73%。3. 该方法对模型实用性影响最小，同时保留了模型的一般行为。", "conclusion": "SafeConstellations提供了一种原则性的方法来缓解LLMs的过度拒绝问题，通过在推理时引导模型表示，显著提高了模型在生产应用中的可用性，同时保持了其安全性。"}}
{"id": "2508.11032", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11032", "abs": "https://arxiv.org/abs/2508.11032", "authors": ["Yanwu Yang", "Guinan Su", "Jiesi Hu", "Francesco Sammarco", "Jonas Geiping", "Thomas Wolfers"], "title": "MedSAMix: A Training-Free Model Merging Approach for Medical Image Segmentation", "comment": null, "summary": "Universal medical image segmentation models have emerged as a promising\nparadigm due to their strong generalizability across diverse tasks, showing\ngreat potential for a wide range of clinical applications. This potential has\nbeen partly driven by the success of general-purpose vision models such as the\nSegment Anything Model (SAM), which has inspired the development of various\nfine-tuned variants for medical segmentation tasks. However, fine-tuned\nvariants like MedSAM are trained on comparatively limited medical imaging data\nthat often suffers from heterogeneity, scarce annotations, and distributional\nshifts. These challenges limit their ability to generalize across a wide range\nof medical segmentation tasks. In this regard, we propose MedSAMix, a\ntraining-free model merging method that integrates the strengths of both\ngeneralist models (e.g., SAM) and specialist models (e.g., MedSAM) for medical\nimage segmentation. In contrast to traditional model merging approaches that\nrely on manual configuration and often result in suboptimal outcomes, we\npropose a zero-order optimization method to automatically discover optimal\nlayer-wise merging solutions. Furthermore, for clinical applications, we\ndevelop two regimes to meet the demand of domain-specificity and\ngeneralizability in different scenarios by single-task optimization and\nmulti-objective optimization respectively. Extensive evaluations on 25 medical\nsegmentation tasks demonstrate that MedSAMix effectively mitigates model bias\nand consistently improves performance in both domain-specific accuracy and\ngeneralization, achieving improvements of 6.67% on specialized tasks and 4.37%\non multi-task evaluations.", "AI": {"tldr": "MedSAMix是一种无需训练的模型合并方法，通过零阶优化自动融合通用模型（如SAM）和专业模型（如MedSAM）的优势，以解决医学图像分割中微调模型的数据限制问题，并在25项医学分割任务上显著提升了性能。", "motivation": "现有的通用医学图像分割模型（如MedSAM的微调变体）受限于医学影像数据的异质性、稀缺性和分布偏移，导致其在广泛的医学分割任务中泛化能力不足。", "method": "提出MedSAMix，一种无需训练的模型合并方法，旨在结合通用模型（如SAM）和专业模型（如MedSAM）的优势。该方法采用零阶优化自动发现最优的逐层合并方案，而非依赖手动配置。此外，为满足临床应用中领域特异性和泛化性的需求，设计了单任务优化和多目标优化两种机制。", "result": "在25项医学分割任务上的广泛评估表明，MedSAMix有效减轻了模型偏差，并在领域特异性准确性和泛化能力上持续提升性能，在专业任务上实现了6.67%的改进，在多任务评估中实现了4.37%的改进。", "conclusion": "MedSAMix通过创新性的模型合并策略，成功解决了微调医学分割模型在数据受限情况下的泛化能力问题，显著提升了模型在各种医学图像分割任务中的表现，兼顾了领域特异性和通用性。"}}
{"id": "2508.11354", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11354", "abs": "https://arxiv.org/abs/2508.11354", "authors": ["Zhenyi Zhao", "Muthu Rama Krishnan Mookiah", "Emanuele Trucco"], "title": "Leveraging the RETFound foundation model for optic disc segmentation in retinal images", "comment": null, "summary": "RETFound is a well-known foundation model (FM) developed for fundus camera\nand optical coherence tomography images. It has shown promising performance\nacross multiple datasets in diagnosing diseases, both eye-specific and\nsystemic, from retinal images. However, to our best knowledge, it has not been\nused for other tasks. We present the first adaptation of RETFound for optic\ndisc segmentation, a ubiquitous and foundational task in retinal image\nanalysis. The resulting segmentation system outperforms state-of-the-art,\nsegmentation-specific baseline networks after training a head with only a very\nmodest number of task-specific examples. We report and discuss results with\nfour public datasets, IDRID, Drishti-GS, RIM-ONE-r3, and REFUGE, and a private\ndataset, GoDARTS, achieving about 96% Dice consistently across all datasets.\nOverall, our method obtains excellent performance in internal verification,\ndomain generalization and domain adaptation, and exceeds most of the\nstate-of-the-art baseline results. We discuss the results in the framework of\nthe debate about FMs as alternatives to task-specific architectures. The code\nis available at: [link to be added after the paper is accepted]", "AI": {"tldr": "首次将RETFound基础模型应用于视盘分割任务，在多个数据集中以少量样本训练后，性能超越现有最先进的分割特异性基线。", "motivation": "RETFound在眼底和OCT图像诊断方面表现出色，但尚未被用于其他任务。视盘分割是视网膜图像分析中一项普遍且基础的任务，因此研究其在视盘分割中的应用潜力。", "method": "将RETFound模型进行适应性改造，通过训练一个分割头部，并使用少量任务特定样本进行训练。在IDRID、Drishti-GS、RIM-ONE-r3、REFUGE等四个公开数据集和一个私有数据集GoDARTS上进行测试和验证。", "result": "在所有数据集中，Dice系数一致达到约96%。该分割系统在内部验证、域泛化和域适应方面均表现出色，并超越了大多数最先进的分割特异性基线网络的性能。", "conclusion": "RETFound在视盘分割任务中表现出卓越性能，证明了基础模型作为任务特定架构替代方案的潜力，尤其是在视网膜图像分析领域。"}}
{"id": "2508.11492", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11492", "abs": "https://arxiv.org/abs/2508.11492", "authors": ["Bozhou Zhang", "Nan Song", "Bingzhao Gao", "Li Zhang"], "title": "Relative Position Matters: Trajectory Prediction and Planning with Polar Representation", "comment": null, "summary": "Trajectory prediction and planning in autonomous driving are highly\nchallenging due to the complexity of predicting surrounding agents' movements\nand planning the ego agent's actions in dynamic environments. Existing methods\nencode map and agent positions and decode future trajectories in Cartesian\ncoordinates. However, modeling the relationships between the ego vehicle and\nsurrounding traffic elements in Cartesian space can be suboptimal, as it does\nnot naturally capture the varying influence of different elements based on\ntheir relative distances and directions. To address this limitation, we adopt\nthe Polar coordinate system, where positions are represented by radius and\nangle. This representation provides a more intuitive and effective way to model\nspatial changes and relative relationships, especially in terms of distance and\ndirectional influence. Based on this insight, we propose Polaris, a novel\nmethod that operates entirely in Polar coordinates, distinguishing itself from\nconventional Cartesian-based approaches. By leveraging the Polar\nrepresentation, this method explicitly models distance and direction variations\nand captures relative relationships through dedicated encoding and refinement\nmodules, enabling more structured and spatially aware trajectory prediction and\nplanning. Extensive experiments on the challenging prediction (Argoverse 2) and\nplanning benchmarks (nuPlan) demonstrate that Polaris achieves state-of-the-art\nperformance.", "AI": {"tldr": "该论文提出了一种名为Polaris的新方法，通过在极坐标系中进行轨迹预测和规划，解决了传统笛卡尔坐标系在建模自动驾驶中车辆间相对关系时的局限性，并实现了最先进的性能。", "motivation": "自动驾驶中的轨迹预测和规划极具挑战性，现有方法在笛卡尔坐标系中建模地图和智能体位置，但这种方式无法自然地捕捉不同交通元素基于相对距离和方向的不同影响，导致建模自我车辆与周围交通元素关系时次优。", "method": "该研究采用极坐标系（用半径和角度表示位置），认为其能更直观有效地建模空间变化和相对关系。基于此，论文提出了Polaris方法，该方法完全在极坐标系中操作，通过专门的编码和细化模块，明确建模距离和方向变化，并捕捉相对关系，从而实现更结构化和空间感知的轨迹预测和规划。", "result": "在具有挑战性的预测（Argoverse 2）和规划（nuPlan）基准测试中，Polaris方法取得了最先进的性能。", "conclusion": "极坐标系在自动驾驶轨迹预测和规划中能更有效地建模空间和相对关系，Polaris方法通过在极坐标系中操作，显著提升了性能，证明了该表示方法的优越性。"}}
{"id": "2508.11318", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11318", "abs": "https://arxiv.org/abs/2508.11318", "authors": ["Sahil Sk", "Debasish Dhal", "Sonal Khosla", "Sk Shahid", "Sambit Shekhar", "Akash Dhaka", "Shantipriya Parida", "Dilip K. Prasad", "Ondřej Bojar"], "title": "LLM Compression: How Far Can We Go in Balancing Size and Performance?", "comment": "This paper has been accepted for presentation at the RANLP 2025\n  conference", "summary": "Quantization is an essential and popular technique for improving the\naccessibility of large language models (LLMs) by reducing memory usage and\ncomputational costs while maintaining performance. In this study, we apply\n4-bit Group Scaling Quantization (GSQ) and Generative Pretrained Transformer\nQuantization (GPTQ) to LLaMA 1B, Qwen 0.5B, and PHI 1.5B, evaluating their\nimpact across multiple NLP tasks. We benchmark these models on MS MARCO\n(Information Retrieval), BoolQ (Boolean Question Answering), and GSM8K\n(Mathematical Reasoning) datasets, assessing both accuracy and efficiency\nacross various tasks. The study measures the trade-offs between model\ncompression and task performance, analyzing key evaluation metrics, namely\naccuracy, inference latency, and throughput (total output tokens generated per\nsecond), providing insights into the suitability of low-bit quantization for\nreal-world deployment. Using the results, users can then make suitable\ndecisions based on the specifications that need to be met. We discuss the pros\nand cons of GSQ and GPTQ techniques on models of different sizes, which also\nserve as a benchmark for future experiments.", "AI": {"tldr": "本研究评估了4比特GSQ和GPTQ量化技术在小型LLM（LLaMA 1B, Qwen 0.5B, PHI 1.5B）上对多种NLP任务（信息检索、布尔问答、数学推理）的性能影响，分析了模型压缩与任务表现之间的权衡。", "motivation": "量化是提高大型语言模型（LLMs）可访问性的关键技术，通过减少内存使用和计算成本同时保持性能。本研究旨在探讨低比特量化在实际部署中的适用性。", "method": "将4比特组缩放量化（GSQ）和生成式预训练Transformer量化（GPTQ）应用于LLaMA 1B、Qwen 0.5B和PHI 1.5B模型。在MS MARCO、BoolQ和GSM8K数据集上进行基准测试，评估准确性、推理延迟和吞吐量。", "result": "研究量化了模型压缩与任务性能之间的权衡，分析了准确性、推理延迟和吞吐量等关键评估指标。讨论了GSQ和GPTQ技术在不同大小模型上的优缺点。", "conclusion": "本研究为低比特量化在实际部署中的适用性提供了见解，帮助用户根据具体需求做出决策，并为未来的实验提供了基准。"}}
{"id": "2508.11058", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.11058", "abs": "https://arxiv.org/abs/2508.11058", "authors": ["Wentao Mo", "Qingchao Chen", "Yuxin Peng", "Siyuan Huang", "Yang Liu"], "title": "Advancing 3D Scene Understanding with MV-ScanQA Multi-View Reasoning Evaluation and TripAlign Pre-training Dataset", "comment": "Accepeted to ACM MM 25", "summary": "The advancement of 3D vision-language (3D VL) learning is hindered by several\nlimitations in existing 3D VL datasets: they rarely necessitate reasoning\nbeyond a close range of objects in single viewpoint, and annotations often link\ninstructions to single objects, missing richer contextual alignments between\nmultiple objects. This significantly curtails the development of models capable\nof deep, multi-view 3D scene understanding over distant objects. To address\nthese challenges, we introduce MV-ScanQA, a novel 3D question answering dataset\nwhere 68% of questions explicitly require integrating information from multiple\nviews (compared to less than 7% in existing datasets), thereby rigorously\ntesting multi-view compositional reasoning. To facilitate the training of\nmodels for such demanding scenarios, we present TripAlign dataset, a\nlarge-scale and low-cost 2D-3D-language pre-training corpus containing 1M <2D\nview, set of 3D objects, text> triplets that explicitly aligns groups of\ncontextually related objects with text, providing richer, view-grounded\nmulti-object multimodal alignment signals than previous single-object\nannotations. We further develop LEGO, a baseline method for the multi-view\nreasoning challenge in MV-ScanQA, transferring knowledge from pre-trained 2D\nLVLMs to 3D domain with TripAlign. Empirically, LEGO pre-trained on TripAlign\nachieves state-of-the-art performance not only on the proposed MV-ScanQA, but\nalso on existing benchmarks for 3D dense captioning and question answering.\nDatasets and code are available at\nhttps://matthewdm0816.github.io/tripalign-mvscanqa.", "AI": {"tldr": "该研究提出了MV-ScanQA和TripAlign两个新数据集，旨在解决现有3D视觉-语言（VL）学习中多视角推理和多对象上下文对齐的不足，并提出了LEGO基线方法，在多个3D VL任务上实现了最先进的性能。", "motivation": "现有的3D视觉-语言数据集存在局限性：很少需要对远距离对象进行超出单视角的推理，且标注通常将指令与单一对象关联，缺乏多对象间丰富的上下文对齐，这严重限制了模型开发深度、多视角3D场景理解能力。", "method": "1. 引入MV-ScanQA数据集：一个新颖的3D问答数据集，其中68%的问题明确要求整合多视角信息，严格测试多视角组合推理。2. 提出TripAlign数据集：一个大规模、低成本的2D-3D-语言预训练语料库，包含1M <2D视图，3D对象集，文本>三元组，明确对齐上下文相关的对象组与文本。3. 开发LEGO基线方法：将预训练的2D LVLM知识通过TripAlign转移到3D领域，用于MV-ScanQA中的多视角推理挑战。", "result": "LEGO方法在TripAlign上预训练后，不仅在提出的MV-ScanQA数据集上取得了最先进的性能，还在现有的3D密集描述和问答基准测试上实现了最先进的性能。", "conclusion": "该研究通过引入新的数据集MV-ScanQA和TripAlign以及基线方法LEGO，有效解决了现有3D视觉-语言学习中多视角推理和多对象上下文对齐的挑战，显著推动了3D场景深度理解模型的发展。"}}
{"id": "2508.11374", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11374", "abs": "https://arxiv.org/abs/2508.11374", "authors": ["Devansh Arora", "Nitin Kumar", "Sukrit Gupta"], "title": "Does the Skeleton-Recall Loss Really Work?", "comment": null, "summary": "Image segmentation is an important and widely performed task in computer\nvision. Accomplishing effective image segmentation in diverse settings often\nrequires custom model architectures and loss functions. A set of models that\nspecialize in segmenting thin tubular structures are topology\npreservation-based loss functions. These models often utilize a pixel\nskeletonization process claimed to generate more precise segmentation masks of\nthin tubes and better capture the structures that other models often miss. One\nsuch model, Skeleton Recall Loss (SRL) proposed by Kirchhoff et al.~\\cite\n{kirchhoff2024srl}, was stated to produce state-of-the-art results on benchmark\ntubular datasets. In this work, we performed a theoretical analysis of the\ngradients for the SRL loss. Upon comparing the performance of the proposed\nmethod on some of the tubular datasets (used in the original work, along with\nsome additional datasets), we found that the performance of SRL-based\nsegmentation models did not exceed traditional baseline models. By providing\nboth a theoretical explanation and empirical evidence, this work critically\nevaluates the limitations of topology-based loss functions, offering valuable\ninsights for researchers aiming to develop more effective segmentation models\nfor complex tubular structures.", "AI": {"tldr": "本文对声称在管状结构分割中达到SOTA的Skeleton Recall Loss (SRL)进行了理论和实证评估，发现其性能并未超越传统基线模型，并指出了基于拓扑的损失函数的局限性。", "motivation": "图像分割是计算机视觉中的重要任务，特别是在薄管状结构分割中，基于拓扑保持的损失函数（如SRL）被声称能生成更精确的分割掩模并达到最先进的结果。本研究旨在验证和深入分析SRL的实际表现和潜在局限性。", "method": "本文首先对SRL损失函数的梯度进行了理论分析。随后，在原始工作中使用的部分管状数据集以及一些额外数据集上，将基于SRL的分割模型与传统基线模型进行了性能比较。", "result": "通过理论分析和实证比较，发现基于SRL的分割模型在所测试的管状数据集上的性能并未超越传统的基线模型。", "conclusion": "本研究通过理论解释和经验证据，批判性地评估了基于拓扑的损失函数（如SRL）的局限性。这为旨在开发更有效复杂管状结构分割模型的研究人员提供了宝贵的见解。"}}
{"id": "2508.11498", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11498", "abs": "https://arxiv.org/abs/2508.11498", "authors": ["Agnes Bressan de Almeida", "Joao Aires Correa Fernandes Marsicano"], "title": "Swarm-in-Blocks: Simplifying Drone Swarm Programming with Block-Based Language", "comment": null, "summary": "Swarm in Blocks, originally developed for CopterHack 2022, is a high-level\ninterface that simplifies drone swarm programming using a block-based language.\nBuilding on the Clover platform, this tool enables users to create\nfunctionalities like loops and conditional structures by assembling code\nblocks. In 2023, we introduced Swarm in Blocks 2.0, further refining the\nplatform to address the complexities of swarm management in a user-friendly\nway. As drone swarm applications grow in areas like delivery, agriculture, and\nsurveillance, the challenge of managing them, especially for beginners, has\nalso increased. The Atena team developed this interface to make swarm handling\naccessible without requiring extensive knowledge of ROS or programming. The\nblock-based approach not only simplifies swarm control but also expands\neducational opportunities in programming.", "AI": {"tldr": "Swarm in Blocks是一个基于块的编程接口，旨在简化无人机群的编程和管理，使其对初学者更易上手。", "motivation": "随着无人机群应用（如配送、农业、监控）的增长，其管理复杂性也随之增加，特别是对于初学者而言，需要大量的ROS或编程知识。", "method": "该研究开发了一个名为“Swarm in Blocks”的高级接口，它基于Clover平台，使用块状语言来简化无人机群编程，允许用户通过组装代码块创建循环和条件结构等功能。Swarm in Blocks 2.0进一步改进了平台。", "result": "该工具使得用户无需深入了解ROS或编程即可创建无人机群功能，显著简化了蜂群控制，并拓展了编程教育机会。", "conclusion": "Swarm in Blocks提供了一个用户友好的、可访问的无人机群编程和管理方式，有效降低了复杂性，并促进了教育普及。"}}
{"id": "2508.11343", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11343", "abs": "https://arxiv.org/abs/2508.11343", "authors": ["Haitong Luo", "Weiyao Zhang", "Suhang Wang", "Wenji Zou", "Chungang Lin", "Xuying Meng", "Yujun Zhang"], "title": "SpecDetect: Simple, Fast, and Training-Free Detection of LLM-Generated Text via Spectral Analysis", "comment": "Under Review", "summary": "The proliferation of high-quality text from Large Language Models (LLMs)\ndemands reliable and efficient detection methods. While existing training-free\napproaches show promise, they often rely on surface-level statistics and\noverlook fundamental signal properties of the text generation process. In this\nwork, we reframe detection as a signal processing problem, introducing a novel\nparadigm that analyzes the sequence of token log-probabilities in the frequency\ndomain. By systematically analyzing the signal's spectral properties using the\nglobal Discrete Fourier Transform (DFT) and the local Short-Time Fourier\nTransform (STFT), we find that human-written text consistently exhibits\nsignificantly higher spectral energy. This higher energy reflects the\nlarger-amplitude fluctuations inherent in human writing compared to the\nsuppressed dynamics of LLM-generated text. Based on this key insight, we\nconstruct SpecDetect, a detector built on a single, robust feature from the\nglobal DFT: DFT total energy. We also propose an enhanced version,\nSpecDetect++, which incorporates a sampling discrepancy mechanism to further\nboost robustness. Extensive experiments demonstrate that our approach\noutperforms the state-of-the-art model while running in nearly half the time.\nOur work introduces a new, efficient, and interpretable pathway for\nLLM-generated text detection, showing that classical signal processing\ntechniques offer a surprisingly powerful solution to this modern challenge.", "AI": {"tldr": "本文将大语言模型（LLM）生成文本的检测重构为信号处理问题，通过分析token对数概率序列的频谱特性，发现人类文本具有更高的频谱能量。基于此洞察，提出了名为SpecDetect及其增强版SpecDetect++的检测器，其性能超越现有最先进模型，且运行速度更快。", "motivation": "高质量LLM文本的泛滥要求可靠高效的检测方法。现有无训练方法依赖表面统计量，忽略了文本生成过程的根本信号特性。", "method": "将文本检测视为信号处理问题，分析token对数概率序列在频域的特性。使用全局离散傅里叶变换（DFT）和局部短时傅里叶变换（STFT）系统分析信号的频谱属性。发现人类文本表现出更高的频谱能量，反映了其固有的更大振幅波动，而LLM生成文本的动态被抑制。基于此，构建了基于DFT总能量的SpecDetect检测器，并提出了结合采样差异机制的增强版SpecDetect++。", "result": "实验证明，人类文本始终表现出显著更高的频谱能量。所提出的SpecDetect方法在性能上优于最先进的模型，且运行时间缩短近一半。", "conclusion": "该工作为LLM生成文本检测引入了一种新的、高效且可解释的途径，表明经典的信号处理技术能为这一现代挑战提供出乎意料的强大解决方案。"}}
{"id": "2508.11063", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11063", "abs": "https://arxiv.org/abs/2508.11063", "authors": ["Lucas W. Remedios", "Chloe Choe", "Trent M. Schwartz", "Dingjie Su", "Gaurav Rudravaram", "Chenyu Gao", "Aravind R. Krishnan", "Adam M. Saunders", "Michael E. Kim", "Shunxing Bao", "Alvin C. Powers", "Bennett A. Landman", "John Virostko"], "title": "Data-Driven Abdominal Phenotypes of Type 2 Diabetes in Lean, Overweight, and Obese Cohorts", "comment": null, "summary": "Purpose: Although elevated BMI is a well-known risk factor for type 2\ndiabetes, the disease's presence in some lean adults and absence in others with\nobesity suggests that detailed body composition may uncover abdominal\nphenotypes of type 2 diabetes. With AI, we can now extract detailed\nmeasurements of size, shape, and fat content from abdominal structures in 3D\nclinical imaging at scale. This creates an opportunity to empirically define\nbody composition signatures linked to type 2 diabetes risk and protection using\nlarge-scale clinical data. Approach: To uncover BMI-specific diabetic abdominal\npatterns from clinical CT, we applied our design four times: once on the full\ncohort (n = 1,728) and once on lean (n = 497), overweight (n = 611), and obese\n(n = 620) subgroups separately. Briefly, our experimental design transforms\nabdominal scans into collections of explainable measurements through\nsegmentation, classifies type 2 diabetes through a cross-validated random\nforest, measures how features contribute to model-estimated risk or protection\nthrough SHAP analysis, groups scans by shared model decision patterns\n(clustering from SHAP) and links back to anatomical differences\n(classification). Results: The random-forests achieved mean AUCs of 0.72-0.74.\nThere were shared type 2 diabetes signatures in each group; fatty skeletal\nmuscle, older age, greater visceral and subcutaneous fat, and a smaller or\nfat-laden pancreas. Univariate logistic regression confirmed the direction of\n14-18 of the top 20 predictors within each subgroup (p < 0.05). Conclusions:\nOur findings suggest that abdominal drivers of type 2 diabetes may be\nconsistent across weight classes.", "AI": {"tldr": "该研究利用AI分析腹部CT影像，识别出与2型糖尿病相关的详细身体成分特征，并发现这些特征在不同BMI人群中具有一致性。", "motivation": "尽管BMI是2型糖尿病的已知风险因素，但一些瘦人也会患病，而一些肥胖者则不会，这表明详细的身体成分，特别是腹部表型，可能更能揭示2型糖尿病的风险。AI技术能够大规模提取3D临床影像中的详细尺寸、形状和脂肪含量信息，从而有机会利用大数据经验性地定义与2型糖尿病风险和保护相关的身体成分特征。", "method": "研究将腹部CT扫描转换为可解释的测量数据（通过分割），使用交叉验证的随机森林模型对2型糖尿病进行分类。通过SHAP分析评估特征对模型风险或保护的贡献，并根据共享的模型决策模式（SHAP聚类）对扫描进行分组，然后关联回解剖学差异（分类）。该方法在全队列（n=1,728）以及瘦（n=497）、超重（n=611）和肥胖（n=620）亚组中分别应用了四次。", "result": "随机森林模型取得了0.72-0.74的平均AUC。在每个亚组中都发现了共同的2型糖尿病特征，包括脂肪浸润的骨骼肌、年龄较大、内脏和皮下脂肪较多，以及胰腺较小或脂肪堆积。单变量逻辑回归分析证实了每个亚组中前20个预测因子中14-18个的方向（p < 0.05）。", "conclusion": "研究结果表明，2型糖尿病的腹部驱动因素在不同体重类别中可能是一致的。"}}
{"id": "2508.11379", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11379", "abs": "https://arxiv.org/abs/2508.11379", "authors": ["Ramil Khafizov", "Artem Komarichev", "Ruslan Rakhimov", "Peter Wonka", "Evgeny Burnaev"], "title": "G-CUT3R: Guided 3D Reconstruction with Camera and Depth Prior Integration", "comment": null, "summary": "We introduce G-CUT3R, a novel feed-forward approach for guided 3D scene\nreconstruction that enhances the CUT3R model by integrating prior information.\nUnlike existing feed-forward methods that rely solely on input images, our\nmethod leverages auxiliary data, such as depth, camera calibrations, or camera\npositions, commonly available in real-world scenarios. We propose a lightweight\nmodification to CUT3R, incorporating a dedicated encoder for each modality to\nextract features, which are fused with RGB image tokens via zero convolution.\nThis flexible design enables seamless integration of any combination of prior\ninformation during inference. Evaluated across multiple benchmarks, including\n3D reconstruction and other multi-view tasks, our approach demonstrates\nsignificant performance improvements, showing its ability to effectively\nutilize available priors while maintaining compatibility with varying input\nmodalities.", "AI": {"tldr": "G-CUT3R是一种新型前馈式3D场景重建方法，通过整合深度、相机校准等先验信息，增强了CUT3R模型，提升了重建性能。", "motivation": "现有前馈式3D重建方法主要依赖RGB图像，但现实世界中常有辅助先验信息（如深度、相机参数）可用，本研究旨在利用这些信息来提升重建精度和效率。", "method": "G-CUT3R对CUT3R模型进行了轻量级修改，为每种辅助模态（如深度、相机校准）设计了专用编码器来提取特征，然后通过零卷积将这些特征与RGB图像特征融合。这种模块化设计允许在推理时灵活集成任意组合的先验信息。", "result": "在3D重建和其他多视角任务的多个基准测试中，G-CUT3R表现出显著的性能提升。结果表明，该方法能有效利用可用先验信息，同时保持对不同输入模态的兼容性。", "conclusion": "G-CUT3R通过有效整合辅助先验信息，显著提高了3D场景重建的性能，并展现了其在处理多模态输入方面的灵活性和鲁棒性。"}}
{"id": "2508.11503", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11503", "abs": "https://arxiv.org/abs/2508.11503", "authors": ["Andrej Orsula", "Matthieu Geist", "Miguel Olivares-Mendez", "Carol Martinez"], "title": "Sim2Dust: Mastering Dynamic Waypoint Tracking on Granular Media", "comment": "The source code is available at\n  https://github.com/AndrejOrsula/space_robotics_bench", "summary": "Reliable autonomous navigation across the unstructured terrains of distant\nplanetary surfaces is a critical enabler for future space exploration. However,\nthe deployment of learning-based controllers is hindered by the inherent\nsim-to-real gap, particularly for the complex dynamics of wheel interactions\nwith granular media. This work presents a complete sim-to-real framework for\ndeveloping and validating robust control policies for dynamic waypoint tracking\non such challenging surfaces. We leverage massively parallel simulation to\ntrain reinforcement learning agents across a vast distribution of procedurally\ngenerated environments with randomized physics. These policies are then\ntransferred zero-shot to a physical wheeled rover operating in a lunar-analogue\nfacility. Our experiments systematically compare multiple reinforcement\nlearning algorithms and action smoothing filters to identify the most effective\ncombinations for real-world deployment. Crucially, we provide strong empirical\nevidence that agents trained with procedural diversity achieve superior\nzero-shot performance compared to those trained on static scenarios. We also\nanalyze the trade-offs of fine-tuning with high-fidelity particle physics,\nwhich offers minor gains in low-speed precision at a significant computational\ncost. Together, these contributions establish a validated workflow for creating\nreliable learning-based navigation systems, marking a critical step towards\ndeploying autonomous robots in the final frontier.", "AI": {"tldr": "本文提出一个完整的从仿真到真实世界的框架，用于在非结构化行星表面上开发和验证基于学习的自主导航控制策略，通过大规模并行仿真和程序化生成环境训练强化学习智能体，并成功实现零样本迁移到物理漫游车，证明了程序化多样性训练的优越性。", "motivation": "在遥远行星表面的非结构化地形上实现可靠的自主导航对于未来的空间探索至关重要。然而，将基于学习的控制器部署到实际应用中，面临固有的“仿真到真实世界”差距，特别是车轮与颗粒介质复杂动力学交互的问题。", "method": "本研究利用大规模并行仿真，在大量程序化生成且物理参数随机化的环境中训练强化学习（RL）智能体，以实现动态航点跟踪的鲁棒控制策略。训练后的策略被零样本迁移到月球模拟设施中的物理轮式漫游车上。实验系统地比较了多种RL算法和动作平滑滤波器，并分析了使用高保真粒子物理进行微调的权衡。", "result": "实验结果提供了强有力的经验证据，表明通过程序化多样性训练的智能体比在静态场景中训练的智能体实现了更优越的零样本性能。此外，研究发现使用高保真粒子物理进行微调在低速精度上仅带来微小增益，但计算成本显著。", "conclusion": "这些贡献共同建立了一个经过验证的工作流程，用于创建可靠的基于学习的导航系统，标志着在最终边疆部署自主机器人迈出了关键一步。"}}
{"id": "2508.11364", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11364", "abs": "https://arxiv.org/abs/2508.11364", "authors": ["Sylvio Rüdian", "Yassin Elsir", "Marvin Kretschmer", "Sabine Cayrou", "Niels Pinkwart"], "title": "Feedback Indicators: The Alignment between Llama and a Teacher in Language Learning", "comment": "11 pages, one table", "summary": "Automated feedback generation has the potential to enhance students' learning\nprogress by providing timely and targeted feedback. Moreover, it can assist\nteachers in optimizing their time, allowing them to focus on more strategic and\npersonalized aspects of teaching. To generate high-quality, information-rich\nformative feedback, it is essential first to extract relevant indicators, as\nthese serve as the foundation upon which the feedback is constructed. Teachers\noften employ feedback criteria grids composed of various indicators that they\nevaluate systematically. This study examines the initial phase of extracting\nsuch indicators from students' submissions of a language learning course using\nthe large language model Llama 3.1. Accordingly, the alignment between\nindicators generated by the LLM and human ratings across various feedback\ncriteria is investigated. The findings demonstrate statistically significant\nstrong correlations, even in cases involving unanticipated combinations of\nindicators and criteria. The methodology employed in this paper offers a\npromising foundation for extracting indicators from students' submissions using\nLLMs. Such indicators can potentially be utilized to auto-generate explainable\nand transparent formative feedback in future research.", "AI": {"tldr": "本研究利用Llama 3.1大型语言模型从语言学习学生的提交作业中提取反馈指标，并发现其与人类评分具有显著强相关性，为自动化生成可解释的形成性反馈奠定基础。", "motivation": "自动化反馈能及时提供有针对性的反馈，提升学生学习进度，并帮助教师优化时间。生成高质量、信息丰富的形成性反馈，首先需要提取相关指标，而教师通常使用包含各种指标的反馈标准网格进行系统评估。", "method": "本研究使用大型语言模型Llama 3.1，从语言学习课程的学生提交作业中提取反馈指标。随后，研究调查了LLM生成的指标与人工评分在各种反馈标准之间的一致性（对齐程度）。", "result": "研究结果表明，LLM生成的指标与人工评分之间存在统计学上显著的强相关性，即使在涉及指标和标准意外组合的情况下也是如此。", "conclusion": "本论文采用的方法为使用大型语言模型从学生提交作业中提取指标提供了一个有前景的基础。这些提取出的指标未来可用于自动生成可解释、透明的形成性反馈。"}}
{"id": "2508.11106", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11106", "abs": "https://arxiv.org/abs/2508.11106", "authors": ["Xinjie Gao", "Bi'an Du", "Wei Hu"], "title": "HierOctFusion: Multi-scale Octree-based 3D Shape Generation via Part-Whole-Hierarchy Message Passing", "comment": null, "summary": "3D content generation remains a fundamental yet challenging task due to the\ninherent structural complexity of 3D data. While recent octree-based diffusion\nmodels offer a promising balance between efficiency and quality through\nhierarchical generation, they often overlook two key insights: 1) existing\nmethods typically model 3D objects as holistic entities, ignoring their\nsemantic part hierarchies and limiting generalization; and 2) holistic\nhigh-resolution modeling is computationally expensive, whereas real-world\nobjects are inherently sparse and hierarchical, making them well-suited for\nlayered generation. Motivated by these observations, we propose HierOctFusion,\na part-aware multi-scale octree diffusion model that enhances hierarchical\nfeature interaction for generating fine-grained and sparse object structures.\nFurthermore, we introduce a cross-attention conditioning mechanism that injects\npart-level information into the generation process, enabling semantic features\nto propagate effectively across hierarchical levels from parts to the whole.\nAdditionally, we construct a 3D dataset with part category annotations using a\npre-trained segmentation model to facilitate training and evaluation.\nExperiments demonstrate that HierOctFusion achieves superior shape quality and\nefficiency compared to prior methods.", "AI": {"tldr": "本文提出HierOctFusion，一种部分感知多尺度八叉树扩散模型，用于生成细粒度、稀疏的3D物体结构，通过增强层次特征交互和引入跨注意力条件机制，实现了卓越的形状质量和效率。", "motivation": "现有的八叉树扩散模型通常将3D物体视为整体，忽略了语义部件层次结构，限制了泛化能力；同时，整体高分辨率建模计算成本高昂，而真实世界物体本质上是稀疏和分层的，更适合分层生成。", "method": "提出HierOctFusion，一个部分感知多尺度八叉树扩散模型，增强了层次特征交互。引入跨注意力条件机制，将部件级信息注入生成过程，使语义特征能在层次级别从部件有效传播到整体。构建了一个带有部件类别标注的3D数据集。", "result": "实验证明，HierOctFusion在形状质量和效率方面优于现有方法。", "conclusion": "HierOctFusion通过利用部件感知和分层建模，有效解决了现有3D内容生成方法的局限性，提升了生成质量和效率。"}}
{"id": "2508.11383", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11383", "abs": "https://arxiv.org/abs/2508.11383", "authors": ["Mikhail Seleznyov", "Mikhail Chaichuk", "Gleb Ershov", "Alexander Panchenko", "Elena Tutubalina", "Oleg Somov"], "title": "When Punctuation Matters: A Large-Scale Comparison of Prompt Robustness Methods for LLMs", "comment": null, "summary": "Large Language Models (LLMs) are highly sensitive to subtle, non-semantic\nvariations in prompt phrasing and formatting. In this work, we present the\nfirst systematic evaluation of 5 methods for improving prompt robustness within\na unified experimental framework. We benchmark these techniques on 8 models\nfrom Llama, Qwen and Gemma families across 52 tasks from Natural Instructions\ndataset. Our evaluation covers robustness methods from both fine-tuned and\nin-context learning paradigms, and tests their generalization against multiple\ntypes of distribution shifts. Finally, we extend our analysis to GPT-4.1 and\nDeepSeek V3 to assess frontier models' current robustness to format\nperturbations. Our findings offer actionable insights into the relative\neffectiveness of these robustness methods, enabling practitioners to make\ninformed decisions when aiming for stable and reliable LLM performance in\nreal-world applications. Code:\nhttps://github.com/AIRI-Institute/when-punctuation-matters.", "AI": {"tldr": "该研究首次系统评估了五种提高大型语言模型（LLMs）提示词鲁棒性的方法，涵盖多种模型和任务，并提供了实践性见解，以应对提示词细微变化带来的性能不稳定性。", "motivation": "大型语言模型对提示词的措辞和格式等非语义性微小变化高度敏感，导致性能不稳定，因此需要系统性地评估和改进其鲁棒性。", "method": "研究采用统一的实验框架，系统评估了五种提示词鲁棒性方法，包括微调和上下文学习范式。在Llama、Qwen和Gemma家族的8个模型上，通过Natural Instructions数据集的52项任务进行基准测试，并测试了其对多种分布偏移的泛化能力。此外，还扩展分析了GPT-4.1和DeepSeek V3等前沿模型。", "result": "研究结果揭示了不同鲁棒性方法的相对有效性，为实际应用提供了可操作的见解。", "conclusion": "本研究的发现使开发者和研究人员能够做出明智的决策，以在实际应用中实现LLM稳定可靠的性能，从而提升模型对格式扰动的鲁棒性。"}}
{"id": "2508.11537", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11537", "abs": "https://arxiv.org/abs/2508.11537", "authors": ["Han Zheng", "Zikang Zhou", "Guli Zhang", "Zhepei Wang", "Kaixuan Wang", "Peiliang Li", "Shaojie Shen", "Ming Yang", "Tong Qin"], "title": "MultiPark: Multimodal Parking Transformer with Next-Segment Prediction", "comment": null, "summary": "Parking accurately and safely in highly constrained spaces remains a critical\nchallenge. Unlike structured driving environments, parking requires executing\ncomplex maneuvers such as frequent gear shifts and steering saturation. Recent\nattempts to employ imitation learning (IL) for parking have achieved promising\nresults. However, existing works ignore the multimodal nature of parking\nbehavior in lane-free open space, failing to derive multiple plausible\nsolutions under the same situation. Notably, IL-based methods encompass\ninherent causal confusion, so enabling a neural network to generalize across\ndiverse parking scenarios is particularly difficult. To address these\nchallenges, we propose MultiPark, an autoregressive transformer for multimodal\nparking. To handle paths filled with abrupt turning points, we introduce a\ndata-efficient next-segment prediction paradigm, enabling spatial\ngeneralization and temporal extrapolation. Furthermore, we design learnable\nparking queries factorized into gear, longitudinal, and lateral components,\nparallelly decoding diverse parking behaviors. To mitigate causal confusion in\nIL, our method employs target-centric pose and ego-centric collision as\noutcome-oriented loss across all modalities beyond pure imitation loss.\nEvaluations on real-world datasets demonstrate that MultiPark achieves\nstate-of-the-art performance across various scenarios. We deploy MultiPark on a\nproduction vehicle, further confirming our approach's robustness in real-world\nparking environments.", "AI": {"tldr": "MultiPark是一个用于多模态泊车的自回归Transformer模型，通过引入下一段预测范式、可学习泊车查询和面向结果的损失函数，解决了传统模仿学习在复杂泊车场景中缺乏多模态解决方案和因果混淆的问题，并在真实世界数据和车辆部署中取得了最先进的性能和鲁棒性。", "motivation": "在高度受限空间内准确安全泊车是一个关键挑战，因为它涉及频繁换挡和转向饱和等复杂操作。现有的模仿学习（IL）方法未能捕捉泊车行为的多模态性质（即在相同情况下存在多种可行解决方案），并且存在固有的因果混淆问题，导致难以在多样化泊车场景中泛化。", "method": "本文提出了MultiPark，一个用于多模态泊车的自回归Transformer。为了处理包含急转弯点的路径，引入了数据高效的下一段预测范式，以实现空间泛化和时间外推。此外，设计了可学习的泊车查询，将其分解为档位、纵向和横向组件，并行解码多样化的泊车行为。为减轻模仿学习中的因果混淆，该方法除了纯模仿损失外，还采用以目标为中心姿态和以自我为中心碰撞作为所有模态的面向结果的损失。", "result": "在真实世界数据集上的评估表明，MultiPark在各种场景下都取得了最先进的性能。该方法已部署在量产车辆上，进一步证实了其在真实世界泊车环境中的鲁棒性。", "conclusion": "MultiPark成功解决了复杂泊车中多模态行为建模和模仿学习因果混淆的挑战，通过创新的模型设计和损失函数，实现了卓越的性能和在实际应用中的可靠性。"}}
{"id": "2508.11386", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.11386", "abs": "https://arxiv.org/abs/2508.11386", "authors": ["Ryan Sze-Yin Chan", "Federico Nanni", "Tomas Lazauskas", "Rosie Wood", "Penelope Yong", "Lionel Tarassenko", "Mark Girolami", "James Geddes", "Andrew Duncan"], "title": "Retrieval-augmented reasoning with lean language models", "comment": null, "summary": "This technical report details a novel approach to combining reasoning and\nretrieval augmented generation (RAG) within a single, lean language model\narchitecture. While existing RAG systems typically rely on large-scale models\nand external APIs, our work addresses the increasing demand for performant and\nprivacy-preserving solutions deployable in resource-constrained or secure\nenvironments. Building on recent developments in test-time scaling and\nsmall-scale reasoning models, we develop a retrieval augmented conversational\nagent capable of interpreting complex, domain-specific queries using a\nlightweight backbone model. Our system integrates a dense retriever with\nfine-tuned Qwen2.5-Instruct models, using synthetic query generation and\nreasoning traces derived from frontier models (e.g., DeepSeek-R1) over a\ncurated corpus, in this case, the NHS A-to-Z condition pages. We explore the\nimpact of summarisation-based document compression, synthetic data design, and\nreasoning-aware fine-tuning on model performance. Evaluation against both\nnon-reasoning and general-purpose lean models demonstrates that our\ndomain-specific fine-tuning approach yields substantial gains in answer\naccuracy and consistency, approaching frontier-level performance while\nremaining feasible for local deployment. All implementation details and code\nare publicly released to support reproducibility and adaptation across domains.", "AI": {"tldr": "该研究提出了一种新颖的方法，在单一、轻量级的语言模型架构中结合了推理和检索增强生成（RAG），旨在为资源受限或安全环境提供高性能且保护隐私的解决方案。", "motivation": "现有的RAG系统通常依赖于大型模型和外部API，无法满足在资源受限或安全环境中部署高性能、保护隐私解决方案日益增长的需求。", "method": "该研究基于测试时缩放和小型推理模型的最新进展，开发了一个检索增强的对话代理。它将密集检索器与微调的Qwen2.5-Instruct模型集成，并利用从前沿模型（如DeepSeek-R1）在精选语料库（NHS A-to-Z）上生成的合成查询和推理轨迹。研究还探讨了基于摘要的文档压缩、合成数据设计和推理感知微调对模型性能的影响。", "result": "与非推理和通用型轻量级模型相比，该领域特定的微调方法在答案准确性和一致性方面取得了显著提升，性能接近前沿模型水平，同时仍可在本地部署。所有实现细节和代码均已公开发布。", "conclusion": "该研究成功地开发了一个在资源受限环境中可部署的、高性能且保护隐私的轻量级RAG系统，证明了在特定领域通过领域特定微调可以达到接近前沿模型的性能，且具备本地部署的可行性。"}}
{"id": "2508.11115", "categories": ["cs.CV", "cs.HC", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.11115", "abs": "https://arxiv.org/abs/2508.11115", "authors": ["Haotang Li", "Zhenyu Qi", "Sen He", "Kebin Peng", "Sheng Tan", "Yili Ren", "Tomas Cerny", "Jiyue Zhao", "Zi Wang"], "title": "UWB-PostureGuard: A Privacy-Preserving RF Sensing System for Continuous Ergonomic Sitting Posture Monitoring", "comment": null, "summary": "Improper sitting posture during prolonged computer use has become a\nsignificant public health concern. Traditional posture monitoring solutions\nface substantial barriers, including privacy concerns with camera-based systems\nand user discomfort with wearable sensors. This paper presents\nUWB-PostureGuard, a privacy-preserving ultra-wideband (UWB) sensing system that\nadvances mobile technologies for preventive health management through\ncontinuous, contactless monitoring of ergonomic sitting posture. Our system\nleverages commercial UWB devices, utilizing comprehensive feature engineering\nto extract multiple ergonomic sitting posture features. We develop PoseGBDT to\neffectively capture temporal dependencies in posture patterns, addressing\nlimitations of traditional frame-wise classification approaches. Extensive\nreal-world evaluation across 10 participants and 19 distinct postures\ndemonstrates exceptional performance, achieving 99.11% accuracy while\nmaintaining robustness against environmental variables such as clothing\nthickness, additional devices, and furniture configurations. Our system\nprovides a scalable, privacy-preserving mobile health solution on existing\nplatforms for proactive ergonomic management, improving quality of life at low\ncosts.", "AI": {"tldr": "UWB-PostureGuard是一种基于超宽带（UWB）技术的非接触式、保护隐私的坐姿监测系统，用于预防性健康管理，解决了传统方法的痛点。", "motivation": "长时间使用电脑导致的不良坐姿是公共健康问题。现有姿态监测方案（如摄像头和可穿戴设备）存在隐私泄露和用户不适等重大障碍。", "method": "系统利用商用UWB设备，通过全面的特征工程提取人体工程学坐姿特征。开发了PoseGBDT模型来有效捕获姿态模式的时间依赖性，克服了传统逐帧分类的局限性。", "result": "在10名参与者和19种不同姿态的真实世界评估中，系统表现出色，实现了99.11%的准确率，并对衣物厚度、附加设备和家具配置等环境变量保持鲁棒性。", "conclusion": "UWB-PostureGuard提供了一个可扩展、保护隐私的移动健康解决方案，可在现有平台上进行主动人体工程学管理，以低成本提高生活质量。"}}
{"id": "2508.11446", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11446", "abs": "https://arxiv.org/abs/2508.11446", "authors": ["Daniel Airinei", "Elena Burceanu", "Marius Leordeanu"], "title": "Inside Knowledge: Graph-based Path Generation with Explainable Data Augmentation and Curriculum Learning for Visual Indoor Navigation", "comment": "Accepted at the International Conference on Computer Vision Workshops\n  2025", "summary": "Indoor navigation is a difficult task, as it generally comes with poor GPS\naccess, forcing solutions to rely on other sources of information. While\nsignificant progress continues to be made in this area, deployment to\nproduction applications is still lacking, given the complexity and additional\nrequirements of current solutions. Here, we introduce an efficient, real-time\nand easily deployable deep learning approach, based on visual input only, that\ncan predict the direction towards a target from images captured by a mobile\ndevice. Our technical approach, based on a novel graph-based path generation\nmethod, combined with explainable data augmentation and curriculum learning,\nincludes contributions that make the process of data collection, annotation and\ntraining, as automatic as possible, efficient and robust. On the practical\nside, we introduce a novel largescale dataset, with video footage inside a\nrelatively large shopping mall, in which each frame is annotated with the\ncorrect next direction towards different specific target destinations.\nDifferent from current methods, ours relies solely on vision, avoiding the need\nof special sensors, additional markers placed along the path, knowledge of the\nscene map or internet access. We also created an easy to use application for\nAndroid, which we plan to make publicly available. We make all our data and\ncode available along with visual demos on our project site", "AI": {"tldr": "提出了一种高效、实时、易于部署的深度学习方法，仅基于视觉输入预测室内导航方向，无需GPS或其他特殊传感器。", "motivation": "室内导航因GPS信号差而困难，现有解决方案复杂、部署成本高，且常依赖特殊传感器或场景地图，限制了其在实际应用中的推广。", "method": "该方法基于纯视觉输入，采用深度学习技术，结合新颖的基于图的路径生成方法、可解释数据增强和课程学习，旨在自动化数据收集、标注和训练过程。此外，还创建了一个大型购物中心视频数据集，并为每帧标注了目标方向。", "result": "开发了一个高效、实时、易于部署的室内导航系统，仅通过移动设备图像即可预测目标方向。该系统无需特殊传感器、路径标记、场景地图知识或互联网连接。同时，发布了一个大规模数据集和一款易于使用的Android应用。", "conclusion": "该研究提供了一种鲁棒、纯视觉的深度学习室内导航方案，解决了现有方法的复杂性和部署难题，使其在实际应用中更具可行性和效率。"}}
{"id": "2508.11584", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11584", "abs": "https://arxiv.org/abs/2508.11584", "authors": ["Jakub Łucki", "Jonathan Becktor", "Georgios Georgakis", "Robert Royce", "Shehryar Khattak"], "title": "Visual Perception Engine: Fast and Flexible Multi-Head Inference for Robotic Vision Tasks", "comment": "6 pages, 6 figures, 2 tables", "summary": "Deploying multiple machine learning models on resource-constrained robotic\nplatforms for different perception tasks often results in redundant\ncomputations, large memory footprints, and complex integration challenges. In\nresponse, this work presents Visual Perception Engine (VPEngine), a modular\nframework designed to enable efficient GPU usage for visual multitasking while\nmaintaining extensibility and developer accessibility. Our framework\narchitecture leverages a shared foundation model backbone that extracts image\nrepresentations, which are efficiently shared, without any unnecessary GPU-CPU\nmemory transfers, across multiple specialized task-specific model heads running\nin parallel. This design eliminates the computational redundancy inherent in\nfeature extraction component when deploying traditional sequential models while\nenabling dynamic task prioritization based on application demands. We\ndemonstrate our framework's capabilities through an example implementation\nusing DINOv2 as the foundation model with multiple task (depth, object\ndetection and semantic segmentation) heads, achieving up to 3x speedup compared\nto sequential execution. Building on CUDA Multi-Process Service (MPS), VPEngine\noffers efficient GPU utilization and maintains a constant memory footprint\nwhile allowing per-task inference frequencies to be adjusted dynamically during\nruntime. The framework is written in Python and is open source with ROS2 C++\n(Humble) bindings for ease of use by the robotics community across diverse\nrobotic platforms. Our example implementation demonstrates end-to-end real-time\nperformance at $\\geq$50 Hz on NVIDIA Jetson Orin AGX for TensorRT optimized\nmodels.", "AI": {"tldr": "VPEngine是一个模块化框架，通过共享基础模型骨干网络实现高效的视觉多任务处理，解决了资源受限机器人平台上多模型部署的冗余计算和内存占用问题。", "motivation": "在资源受限的机器人平台上部署多个机器学习模型进行不同感知任务时，常导致计算冗余、内存占用大以及集成复杂等问题。", "method": "VPEngine框架采用共享的基础模型骨干网络提取图像表示，并将其高效地共享给并行运行的多个任务特定模型头部，避免了不必要的GPU-CPU内存传输。它利用CUDA Multi-Process Service (MPS)实现高效的GPU利用率和动态任务优先级调整。该框架用Python编写，并提供ROS2 C++绑定。", "result": "相比顺序执行，VPEngine实现了高达3倍的加速。它在保持恒定内存占用的同时，允许运行时动态调整每个任务的推理频率。在NVIDIA Jetson Orin AGX上，针对TensorRT优化的模型实现了≥50 Hz的端到端实时性能。", "conclusion": "VPEngine通过消除特征提取的计算冗余、优化GPU使用并提供可扩展性和开发人员可访问性，有效解决了机器人平台上多任务视觉感知的挑战，实现了高效的实时性能。"}}
{"id": "2508.11388", "categories": ["cs.CL", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11388", "abs": "https://arxiv.org/abs/2508.11388", "authors": ["Marc Brinner", "Sina Zarriess"], "title": "Model Interpretability and Rationale Extraction by Input Mask Optimization", "comment": null, "summary": "Concurrent to the rapid progress in the development of neural-network based\nmodels in areas like natural language processing and computer vision, the need\nfor creating explanations for the predictions of these black-box models has\nrisen steadily. We propose a new method to generate extractive explanations for\npredictions made by neural networks, that is based on masking parts of the\ninput which the model does not consider to be indicative of the respective\nclass. The masking is done using gradient-based optimization combined with a\nnew regularization scheme that enforces sufficiency, comprehensiveness and\ncompactness of the generated explanation, three properties that are known to be\ndesirable from the related field of rationale extraction in natural language\nprocessing. In this way, we bridge the gap between model interpretability and\nrationale extraction, thereby proving that the latter of which can be performed\nwithout training a specialized model, only on the basis of a trained\nclassifier. We further apply the same method to image inputs and obtain high\nquality explanations for image classifications, which indicates that the\nconditions proposed for rationale extraction in natural language processing are\nmore broadly applicable to different input types.", "AI": {"tldr": "提出一种基于梯度优化的新方法，通过掩蔽输入生成神经网络的提取式解释，适用于文本和图像，并强制解释具有充分性、全面性和紧凑性。", "motivation": "随着神经网络模型在NLP和CV等领域快速发展，解释这些“黑箱”模型预测的需求日益增长。", "method": "使用基于梯度的优化结合新的正则化方案来掩蔽模型认为不具指示性的输入部分，以生成解释。该正则化方案强制解释满足充分性、全面性和紧凑性。该方法无需训练专门模型，仅基于已训练的分类器，并可应用于图像输入。", "result": "该方法成功为文本和图像分类生成了高质量的解释，证明了在没有专门模型的情况下，仅基于已训练分类器即可进行理由提取。同时表明，为自然语言处理中理由提取提出的条件（充分性、全面性、紧凑性）更广泛地适用于不同输入类型。", "conclusion": "所提出的用于理由提取的条件（充分性、全面性、紧凑性）不仅适用于自然语言处理，也广泛适用于其他输入类型，如图像，从而弥合了模型可解释性与理由提取之间的差距。"}}
{"id": "2508.11134", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11134", "abs": "https://arxiv.org/abs/2508.11134", "authors": ["Bing Liu", "Le Wang", "Hao Liu", "Mingming Liu"], "title": "Residual-based Efficient Bidirectional Diffusion Model for Image Dehazing and Haze Generation", "comment": "7 pages, 5 figures, 2025 ICME Accepted", "summary": "Current deep dehazing methods only focus on removing haze from hazy images,\nlacking the capability to translate between hazy and haze-free images. To\naddress this issue, we propose a residual-based efficient bidirectional\ndiffusion model (RBDM) that can model the conditional distributions for both\ndehazing and haze generation. Firstly, we devise dual Markov chains that can\neffectively shift the residuals and facilitate bidirectional smooth transitions\nbetween them. Secondly, the RBDM perturbs the hazy and haze-free images at\nindividual timesteps and predicts the noise in the perturbed data to\nsimultaneously learn the conditional distributions. Finally, to enhance\nperformance on relatively small datasets and reduce computational costs, our\nmethod introduces a unified score function learned on image patches instead of\nentire images. Our RBDM successfully implements size-agnostic bidirectional\ntransitions between haze-free and hazy images with only 15 sampling steps.\nExtensive experiments demonstrate that the proposed method achieves superior or\nat least comparable performance to state-of-the-art methods on both synthetic\nand real-world datasets.", "AI": {"tldr": "提出了一种基于残差的高效双向扩散模型（RBDM），实现了有雾图像和无雾图像之间的双向转换，并提升了去雾和生成效果。", "motivation": "当前深度去雾方法仅关注从有雾图像中去除雾霾，缺乏在有雾和无雾图像之间进行双向转换的能力。", "method": "该方法提出了一个基于残差的高效双向扩散模型（RBDM）。首先，设计了双马尔可夫链以有效转移残差并促进它们之间的双向平滑过渡。其次，RBDM在单独的时间步长扰动有雾和无雾图像，并预测扰动数据中的噪声以同时学习条件分布。最后，为增强在相对小数据集上的性能并降低计算成本，该方法引入了一个在图像块而非整个图像上学习的统一评分函数。", "result": "RBDM成功实现了无雾和有雾图像之间与尺寸无关的双向转换，仅需15个采样步骤。大量实验表明，所提出的方法在合成和真实世界数据集上均达到或至少可与最先进方法媲美的性能。", "conclusion": "RBDM模型有效解决了现有去雾方法缺乏双向转换能力的问题，实现了高效且高性能的有雾-无雾图像双向转换，并在去雾和雾霾生成方面表现出色。"}}
{"id": "2508.11454", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11454", "abs": "https://arxiv.org/abs/2508.11454", "authors": ["Junichiro Niimi"], "title": "Reference Points in LLM Sentiment Analysis: The Role of Structured Context", "comment": null, "summary": "Large language models (LLMs) are now widely used across many fields,\nincluding marketing research. Sentiment analysis, in particular, helps firms\nunderstand consumer preferences. While most NLP studies classify sentiment from\nreview text alone, marketing theories, such as prospect theory and\nexpectation--disconfirmation theory, point out that customer evaluations are\nshaped not only by the actual experience but also by additional reference\npoints. This study therefore investigates how the content and format of such\nsupplementary information affect sentiment analysis using LLMs. We compare\nnatural language (NL) and JSON-formatted prompts using a lightweight 3B\nparameter model suitable for practical marketing applications. Experiments on\ntwo Yelp categories (Restaurant and Nightlife) show that the JSON prompt with\nadditional information outperforms all baselines without fine-tuning: Macro-F1\nrises by 1.6% and 4% while RMSE falls by 16% and 9.1%, respectively, making it\ndeployable in resource-constrained edge devices. Furthermore, a follow-up\nanalysis confirms that performance gains stem from genuine contextual reasoning\nrather than label proxying. This work demonstrates that structured prompting\ncan enable smaller models to achieve competitive performance, offering a\npractical alternative to large-scale model deployment.", "AI": {"tldr": "本研究探讨了额外参考信息（内容和格式）如何影响LLM进行情感分析，发现使用JSON格式化提示能显著提升小型模型的性能，使其在资源受限设备上具有实用性。", "motivation": "大多数NLP情感分析仅依赖评论文本，但营销理论（如前景理论和期望-不确认理论）指出，消费者评价不仅受实际体验影响，还受额外参考点塑造。因此，研究旨在探索补充信息如何影响LLM的情感分析。", "method": "使用一个轻量级3B参数模型，比较了自然语言（NL）和JSON格式化提示在情感分析中的表现。实验在两个Yelp类别（餐馆和夜生活）的数据上进行。", "result": "带有额外信息的JSON提示在未经微调的情况下，性能优于所有基线：Macro-F1分别提升1.6%和4%，RMSE分别下降16%和9.1%。后续分析确认性能提升源于真实的上下文推理，而非标签代理。", "conclusion": "结构化提示能使小型模型达到有竞争力的性能，为大规模模型部署提供了一种实用的替代方案，尤其适用于资源受限的边缘设备。"}}
{"id": "2508.11588", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11588", "abs": "https://arxiv.org/abs/2508.11588", "authors": ["Benjamin Walt", "Jordan Westphal", "Girish Krishnan"], "title": "Investigating Sensors and Methods in Grasp State Classification in Agricultural Manipulation", "comment": null, "summary": "Effective and efficient agricultural manipulation and harvesting depend on\naccurately understanding the current state of the grasp. The agricultural\nenvironment presents unique challenges due to its complexity, clutter, and\nocclusion. Additionally, fruit is physically attached to the plant, requiring\nprecise separation during harvesting. Selecting appropriate sensors and\nmodeling techniques is critical for obtaining reliable feedback and correctly\nidentifying grasp states. This work investigates a set of key sensors, namely\ninertial measurement units (IMUs), infrared (IR) reflectance, tension, tactile\nsensors, and RGB cameras, integrated into a compliant gripper to classify grasp\nstates. We evaluate the individual contribution of each sensor and compare the\nperformance of two widely used classification models: Random Forest and Long\nShort-Term Memory (LSTM) networks. Our results demonstrate that a Random Forest\nclassifier, trained in a controlled lab environment and tested on real cherry\ntomato plants, achieved 100% accuracy in identifying slip, grasp failure, and\nsuccessful picks, marking a substantial improvement over baseline performance.\nFurthermore, we identify a minimal viable sensor combination, namely IMU and\ntension sensors that effectively classifies grasp states. This classifier\nenables the planning of corrective actions based on real-time feedback, thereby\nenhancing the efficiency and reliability of fruit harvesting operations.", "AI": {"tldr": "该研究通过将多种传感器集成到柔性夹持器中，并比较随机森林和LSTM模型，实现了对农业采摘中抓取状态的精确分类，尤其在番茄采摘中达到100%准确率，并识别出IMU和张力传感器的有效组合。", "motivation": "农业环境中采摘操作复杂，存在杂乱、遮挡等挑战，且果实与植物相连，需要精确分离。准确理解抓取状态对于高效、成功的农业操作至关重要，而选择合适的传感器和建模技术是获取可靠反馈的关键。", "method": "研究将惯性测量单元（IMU）、红外（IR）反射、张力、触觉传感器和RGB摄像头集成到柔性夹持器中，用于分类抓取状态。评估了每种传感器的独立贡献，并比较了随机森林（Random Forest）和长短期记忆（LSTM）两种分类模型的性能。", "result": "在受控实验室环境下训练并在真实樱桃番茄植物上测试后，随机森林分类器在识别打滑、抓取失败和成功采摘方面达到了100%的准确率，显著优于基线性能。此外，研究确定了最小可行传感器组合为IMU和张力传感器，它们能有效分类抓取状态。", "conclusion": "所提出的分类器（特别是使用IMU和张力传感器的随机森林模型）能够根据实时反馈规划纠正措施，从而显著提高水果采摘操作的效率和可靠性。"}}
{"id": "2508.11393", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11393", "abs": "https://arxiv.org/abs/2508.11393", "authors": ["Marc Brinner", "Sina Zarrieß"], "title": "Rationalizing Transformer Predictions via End-To-End Differentiable Self-Training", "comment": null, "summary": "We propose an end-to-end differentiable training paradigm for stable training\nof a rationalized transformer classifier. Our approach results in a single\nmodel that simultaneously classifies a sample and scores input tokens based on\ntheir relevance to the classification. To this end, we build on the widely-used\nthree-player-game for training rationalized models, which typically relies on\ntraining a rationale selector, a classifier and a complement classifier. We\nsimplify this approach by making a single model fulfill all three roles,\nleading to a more efficient training paradigm that is not susceptible to the\ncommon training instabilities that plague existing approaches. Further, we\nextend this paradigm to produce class-wise rationales while incorporating\nrecent advances in parameterizing and regularizing the resulting rationales,\nthus leading to substantially improved and state-of-the-art alignment with\nhuman annotations without any explicit supervision.", "AI": {"tldr": "提出一种端到端可微分的训练范式，用于训练可解释的Transformer分类器。该方法通过一个模型同时完成分类和标记相关性评分，简化了传统的三方博弈训练，提高了训练稳定性，并生成了与人类标注高度一致的解释。", "motivation": "现有的基于三方博弈的解释性模型训练常面临训练不稳定和效率低下的问题。研究旨在开发一种更稳定、高效的训练范式，同时能提供高质量的、甚至类别特定的解释。", "method": "构建了一个端到端可微分的训练范式。通过让单个模型承担传统三方博弈（解释选择器、分类器、补充分类器）的所有角色来简化训练。此外，将该范式扩展到生成类别特定的解释，并结合了最新的参数化和正则化技术来优化解释结果。", "result": "实现了一个能够同时进行样本分类和输入标记相关性评分的单一模型。新方法带来了更高效、更稳定的训练，避免了现有方法常见的训练不稳定性。在没有明确监督的情况下，生成的解释与人类标注达到了显著提升的、最先进的一致性。", "conclusion": "所提出的端到端可微分训练范式为可解释的Transformer分类器提供了一种优越的解决方案，它不仅提高了训练的稳定性与效率，而且在无需显式监督的情况下，生成了高质量的、类别特定的解释，与人类判断高度对齐。"}}
{"id": "2508.11153", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11153", "abs": "https://arxiv.org/abs/2508.11153", "authors": ["Maoquan Zhang", "Bisser Raytchev", "Xiujuan Sun"], "title": "LEARN: A Story-Driven Layout-to-Image Generation Framework for STEM Instruction", "comment": "The International Conference on Neural Information Processing\n  (ICONIP) 2025", "summary": "LEARN is a layout-aware diffusion framework designed to generate\npedagogically aligned illustrations for STEM education. It leverages a curated\nBookCover dataset that provides narrative layouts and structured visual cues,\nenabling the model to depict abstract and sequential scientific concepts with\nstrong semantic alignment. Through layout-conditioned generation, contrastive\nvisual-semantic training, and prompt modulation, LEARN produces coherent visual\nsequences that support mid-to-high-level reasoning in line with Bloom's\ntaxonomy while reducing extraneous cognitive load as emphasized by Cognitive\nLoad Theory. By fostering spatially organized and story-driven narratives, the\nframework counters fragmented attention often induced by short-form media and\npromotes sustained conceptual focus. Beyond static diagrams, LEARN demonstrates\npotential for integration with multimodal systems and curriculum-linked\nknowledge graphs to create adaptive, exploratory educational content. As the\nfirst generative approach to unify layout-based storytelling, semantic\nstructure learning, and cognitive scaffolding, LEARN represents a novel\ndirection for generative AI in education. The code and dataset will be released\nto facilitate future research and practical deployment.", "AI": {"tldr": "LEARN是一个布局感知的扩散框架，旨在为STEM教育生成符合教学法、支持高阶推理并减少认知负荷的插图。", "motivation": "现有短媒体导致注意力分散，缺乏连贯、空间组织和故事驱动的视觉内容来支持STEM教育中的推理和概念理解，并需要降低学生的认知负荷。", "method": "LEARN是一个布局感知的扩散框架。它利用精心策划的BookCover数据集提供叙事布局和结构化视觉线索。采用布局条件生成、对比视觉-语义训练和提示调制等方法。", "result": "LEARN能生成连贯的视觉序列，支持布鲁姆分类法中的中高阶推理，减少认知负荷理论强调的额外认知负荷。它促进空间组织和故事驱动的叙事，对抗短媒体造成的注意力分散，并促进持续的概念专注。此外，它还展示了与多模态系统和课程知识图谱集成的潜力。", "conclusion": "LEARN是首个结合布局叙事、语义结构学习和认知支架的生成式方法，为教育领域的生成式AI开辟了新方向。代码和数据集将发布以促进后续研究和实际应用。"}}
{"id": "2508.11499", "categories": ["cs.CV", "cs.AI", "cs.DL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11499", "abs": "https://arxiv.org/abs/2508.11499", "authors": ["Erez Meoded"], "title": "Handwritten Text Recognition of Historical Manuscripts Using Transformer-Based Models", "comment": null, "summary": "Historical handwritten text recognition (HTR) is essential for unlocking the\ncultural and scholarly value of archival documents, yet digitization is often\nhindered by scarce transcriptions, linguistic variation, and highly diverse\nhandwriting styles. In this study, we apply TrOCR, a state-of-the-art\ntransformer-based HTR model, to 16th-century Latin manuscripts authored by\nRudolf Gwalther. We investigate targeted image preprocessing and a broad suite\nof data augmentation techniques, introducing four novel augmentation methods\ndesigned specifically for historical handwriting characteristics. We also\nevaluate ensemble learning approaches to leverage the complementary strengths\nof augmentation-trained models. On the Gwalther dataset, our best single-model\naugmentation (Elastic) achieves a Character Error Rate (CER) of 1.86, while a\ntop-5 voting ensemble achieves a CER of 1.60 - representing a 50% relative\nimprovement over the best reported TrOCR_BASE result and a 42% improvement over\nthe previous state of the art. These results highlight the impact of\ndomain-specific augmentations and ensemble strategies in advancing HTR\nperformance for historical manuscripts.", "AI": {"tldr": "本研究通过应用TrOCR模型，并结合目标图像预处理、多种数据增强（包括四种新方法）和集成学习策略，显著提升了16世纪拉丁语手稿的历史手写文本识别（HTR）性能。", "motivation": "历史手写文本识别面临转录稀缺、语言变异和手写风格多样等挑战，阻碍了档案文献的数字化及其文化和学术价值的释放。", "method": "研究应用了先进的基于Transformer的HTR模型TrOCR，并对其进行了目标图像预处理。同时，探索了广泛的数据增强技术，并引入了四种专为历史手写特征设计的新型增强方法。此外，还评估了集成学习方法，以利用不同增强训练模型的互补优势。", "result": "在Gwalther数据集上，最佳单一模型（使用Elastic增强）实现了1.86的字符错误率（CER），而一个top-5投票集成模型达到了1.60的CER。这相对于最佳报告的TrOCR_BASE结果有50%的相对改进，相对于之前的最先进技术有42%的改进。", "conclusion": "这些结果突出了领域特定数据增强和集成策略在提升历史手稿HTR性能方面的重要影响。"}}
{"id": "2508.11414", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11414", "abs": "https://arxiv.org/abs/2508.11414", "authors": ["Shangrui Nie", "Florian Mai", "David Kaczér", "Charles Welch", "Zhixue Zhao", "Lucie Flek"], "title": "Survey-to-Behavior: Downstream Alignment of Human Values in LLMs via Survey Questions", "comment": "7 pages 1 figure", "summary": "Large language models implicitly encode preferences over human values, yet\nsteering them often requires large training data. In this work, we investigate\na simple approach: Can we reliably modify a model's value system in downstream\nbehavior by training it to answer value survey questions accordingly? We first\nconstruct value profiles of several open-source LLMs by asking them to rate a\nseries of value-related descriptions spanning 20 distinct human values, which\nwe use as a baseline for subsequent experiments. We then investigate whether\nthe value system of a model can be governed by fine-tuning on the value\nsurveys. We evaluate the effect of finetuning on the model's behavior in two\nways; first, we assess how answers change on in-domain, held-out survey\nquestions. Second, we evaluate whether the model's behavior changes in\nout-of-domain settings (situational scenarios). To this end, we construct a\ncontextualized moral judgment dataset based on Reddit posts and evaluate\nchanges in the model's behavior in text-based adventure games. We demonstrate\nthat our simple approach can not only change the model's answers to in-domain\nsurvey questions, but also produces substantial shifts (value alignment) in\nimplicit downstream task behavior.", "AI": {"tldr": "研究了一种简单方法：通过让大型语言模型（LLMs）回答价值调查问题来微调它们，以可靠地修改其下游行为中的价值系统，并证明了其在域内和域外任务中的有效性。", "motivation": "大型语言模型隐式编码人类价值观，但通常需要大量训练数据来引导它们。本研究旨在探索一种更简单、更可靠的方法来修改模型的价值系统。", "method": "首先构建了多个开源LLMs的价值画像，通过让它们评估20种人类价值观相关的描述作为基线。然后，通过在价值调查问卷上进行微调来改变模型的价值系统。评估方法包括：1) 评估模型在域内（未见过的）调查问题上的回答变化；2) 评估模型在域外情境（情景判断和文本冒险游戏）中的行为变化，为此构建了一个基于Reddit帖子的情境化道德判断数据集。", "result": "研究表明，这种简单方法不仅能改变模型在域内调查问题上的回答，还能在隐式下游任务行为中产生显著的转变（价值对齐）。", "conclusion": "通过让LLMs回答价值调查问题进行微调，是一种有效且简单的方法，可以改变模型的价值系统，并在下游行为中实现价值对齐。"}}
{"id": "2508.11165", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11165", "abs": "https://arxiv.org/abs/2508.11165", "authors": ["Bing Liu", "Le Wang", "Mingming Liu", "Hao Liu", "Rui Yao", "Yong Zhou", "Peng Liu", "Tongqiang Xia"], "title": "Semi-supervised Image Dehazing via Expectation-Maximization and Bidirectional Brownian Bridge Diffusion Models", "comment": "10 pages, 4 figures", "summary": "Existing dehazing methods deal with real-world haze images with difficulty,\nespecially scenes with thick haze. One of the main reasons is the lack of\nreal-world paired data and robust priors. To avoid the costly collection of\npaired hazy and clear images, we propose an efficient semi-supervised image\ndehazing method via Expectation-Maximization and Bidirectional Brownian Bridge\nDiffusion Models (EM-B3DM) with a two-stage learning scheme. In the first\nstage, we employ the EM algorithm to decouple the joint distribution of paired\nhazy and clear images into two conditional distributions, which are then\nmodeled using a unified Brownian Bridge diffusion model to directly capture the\nstructural and content-related correlations between hazy and clear images. In\nthe second stage, we leverage the pre-trained model and large-scale unpaired\nhazy and clear images to further improve the performance of image dehazing.\nAdditionally, we introduce a detail-enhanced Residual Difference Convolution\nblock (RDC) to capture gradient-level information, significantly enhancing the\nmodel's representation capability. Extensive experiments demonstrate that our\nEM-B3DM achieves superior or at least comparable performance to\nstate-of-the-art methods on both synthetic and real-world datasets.", "AI": {"tldr": "本文提出了一种名为EM-B3DM的半监督图像去雾方法，结合期望最大化算法和双向布朗桥扩散模型，以解决真实世界厚雾图像去雾难题，尤其是在缺乏配对数据的情况下。", "motivation": "现有去雾方法难以处理真实世界的雾霾图像，特别是浓雾场景，主要原因是缺乏真实的配对数据和鲁棒的先验知识。", "method": "该方法采用两阶段学习方案：第一阶段，使用EM算法将配对的雾霾和清晰图像的联合分布解耦为两个条件分布，并用统一的布朗桥扩散模型建模，以捕捉图像间的结构和内容相关性；第二阶段，利用预训练模型和大规模非配对数据进一步提升去雾性能。此外，引入了细节增强残差差分卷积块（RDC）来捕获梯度级信息，增强模型表示能力。", "result": "EM-B3DM在合成和真实世界数据集上均实现了优于或至少与现有最先进方法相当的性能。", "conclusion": "EM-B3DM通过创新的半监督扩散模型和细节增强模块，有效克服了真实世界图像去雾，特别是厚雾场景的挑战，且无需昂贵的配对数据收集。"}}
{"id": "2508.11582", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11582", "abs": "https://arxiv.org/abs/2508.11582", "authors": ["Qiguang Chen", "Dengyun Peng", "Jinhao Liu", "HuiKang Su", "Jiannan Guan", "Libo Qin", "Wanxiang Che"], "title": "Aware First, Think Less: Dynamic Boundary Self-Awareness Drives Extreme Reasoning Efficiency in Large Language Models", "comment": "Preprint", "summary": "Recent advancements in large language models (LLMs) have greatly improved\ntheir capabilities on complex reasoning tasks through Long Chain-of-Thought\n(CoT). However, this approach often results in substantial redundancy,\nimpairing computational efficiency and causing significant delays in real-time\napplications. To improve the efficiency, current methods often rely on\nhuman-defined difficulty priors, which do not align with the LLM's self-awared\ndifficulty, leading to inefficiencies. In this paper, we introduce the Dynamic\nReasoning-Boundary Self-Awareness Framework (DR. SAF), which enables models to\ndynamically assess and adjust their reasoning depth in response to problem\ncomplexity. DR. SAF integrates three key components: Boundary Self-Awareness\nAlignment, Adaptive Reward Management, and a Boundary Preservation Mechanism.\nThese components allow models to optimize their reasoning processes, balancing\nefficiency and accuracy without compromising performance. Our experimental\nresults demonstrate that DR. SAF achieves a 49.27% reduction in total response\ntokens with minimal loss in accuracy. The framework also delivers a 6.59x gain\nin token efficiency and a 5x reduction in training time, making it well-suited\nto resource-limited settings. During extreme training, DR. SAF can even surpass\ntraditional instruction-based models in token efficiency with more than 16%\naccuracy improvement.", "AI": {"tldr": "本文提出DR. SAF框架，使大型语言模型能动态调整推理深度，显著提升效率并保持准确性，尤其适用于资源受限环境。", "motivation": "大型语言模型（LLMs）的长思维链（CoT）推理任务效率低下且冗余，导致计算开销大和实时应用延迟。现有优化方法依赖于与LLM自身感知难度不符的人工先验，导致效率不足。", "method": "引入动态推理边界自感知框架（DR. SAF），使模型能根据问题复杂度动态评估和调整推理深度。DR. SAF包含三个核心组件：边界自感知对齐（Boundary Self-Awareness Alignment）、自适应奖励管理（Adaptive Reward Management）和边界保持机制（Boundary Preservation Mechanism）。", "result": "DR. SAF使总响应Token减少49.27%，准确率损失极小。Token效率提高6.59倍，训练时间减少5倍。在极端训练条件下，DR. SAF在Token效率上超越传统指令模型，准确率提升超过16%。", "conclusion": "DR. SAF框架能有效优化LLMs的推理过程，在效率和准确性之间取得良好平衡，且无需牺牲性能。该框架特别适用于资源受限的环境，显著提升了Token效率和模型表现。"}}
{"id": "2508.11429", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11429", "abs": "https://arxiv.org/abs/2508.11429", "authors": ["Shivam Dubey"], "title": "HumorPlanSearch: Structured Planning and HuCoT for Contextual AI Humor", "comment": null, "summary": "Automated humor generation with Large Language Models (LLMs) often yields\njokes that feel generic, repetitive, or tone-deaf because humor is deeply\nsituated and hinges on the listener's cultural background, mindset, and\nimmediate context. We introduce HumorPlanSearch, a modular pipeline that\nexplicitly models context through: (1) Plan-Search for diverse, topic-tailored\nstrategies; (2) Humor Chain-of-Thought (HuCoT) templates capturing cultural and\nstylistic reasoning; (3) a Knowledge Graph to retrieve and adapt\nhigh-performing historical strategies; (4) novelty filtering via semantic\nembeddings; and (5) an iterative judge-driven revision loop. To evaluate\ncontext sensitivity and comedic quality, we propose the Humor Generation Score\n(HGS), which fuses direct ratings, multi-persona feedback, pairwise win-rates,\nand topic relevance. In experiments across nine topics with feedback from 13\nhuman judges, our full pipeline (KG + Revision) boosts mean HGS by 15.4 percent\n(p < 0.05) over a strong baseline. By foregrounding context at every stage from\nstrategy planning to multi-signal evaluation, HumorPlanSearch advances\nAI-driven humor toward more coherent, adaptive, and culturally attuned comedy.", "AI": {"tldr": "本文提出HumorPlanSearch，一个模块化管道，通过显式建模上下文来改进LLM生成的幽默，使其更具语境敏感性和喜剧质量。", "motivation": "现有大型语言模型（LLMs）生成的幽默常显得通用、重复或不合时宜，因为幽默深度依赖于听众的文化背景、心态和即时语境。", "method": "引入HumorPlanSearch管道，包含：1) 计划搜索（Plan-Search）以获取多样化、主题定制的策略；2) 幽默思维链（HuCoT）模板捕捉文化和风格推理；3) 知识图谱（Knowledge Graph）检索和适应高性能历史策略；4) 通过语义嵌入进行新颖性过滤；5) 迭代的评判驱动修订循环。评估方面，提出了幽默生成分数（HGS），融合直接评分、多角色反馈、配对胜率和主题相关性。", "result": "在九个主题和13位人类评委的实验中，完整的管道（知识图谱+修订）相比强基线，将平均HGS提高了15.4%（p < 0.05）。", "conclusion": "HumorPlanSearch通过在策略规划到多信号评估的每个阶段都突出上下文，将AI驱动的幽默推向更连贯、适应性强和文化契合的喜剧。"}}
{"id": "2508.11167", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11167", "abs": "https://arxiv.org/abs/2508.11167", "authors": ["Jianhong Han", "Yupei Wang", "Liang Chen"], "title": "VFM-Guided Semi-Supervised Detection Transformer for Source-Free Object Detection in Remote Sensing Images", "comment": "Manuscript submitted to IEEE TGRS", "summary": "Unsupervised domain adaptation methods have been widely explored to bridge\ndomain gaps. However, in real-world remote-sensing scenarios, privacy and\ntransmission constraints often preclude access to source domain data, which\nlimits their practical applicability. Recently, Source-Free Object Detection\n(SFOD) has emerged as a promising alternative, aiming at cross-domain\nadaptation without relying on source data, primarily through a self-training\nparadigm. Despite its potential, SFOD frequently suffers from training collapse\ncaused by noisy pseudo-labels, especially in remote sensing imagery with dense\nobjects and complex backgrounds. Considering that limited target domain\nannotations are often feasible in practice, we propose a Vision\nfoundation-Guided DEtection TRansformer (VG-DETR), built upon a semi-supervised\nframework for SFOD in remote sensing images. VG-DETR integrates a Vision\nFoundation Model (VFM) into the training pipeline in a \"free lunch\" manner,\nleveraging a small amount of labeled target data to mitigate pseudo-label noise\nwhile improving the detector's feature-extraction capability. Specifically, we\nintroduce a VFM-guided pseudo-label mining strategy that leverages the VFM's\nsemantic priors to further assess the reliability of the generated\npseudo-labels. By recovering potentially correct predictions from\nlow-confidence outputs, our strategy improves pseudo-label quality and\nquantity. In addition, a dual-level VFM-guided alignment method is proposed,\nwhich aligns detector features with VFM embeddings at both the instance and\nimage levels. Through contrastive learning among fine-grained prototypes and\nsimilarity matching between feature maps, this dual-level alignment further\nenhances the robustness of feature representations against domain gaps.\nExtensive experiments demonstrate that VG-DETR achieves superior performance in\nsource-free remote sensing detection tasks.", "AI": {"tldr": "VG-DETR是一种半监督无源目标检测（SFOD）方法，专为遥感图像设计。它通过整合视觉基础模型（VFM）和利用少量标注目标数据，解决SFOD中伪标签噪声导致的训练崩溃问题，并增强特征鲁棒性，从而在无源域情况下实现卓越的跨域检测性能。", "motivation": "传统的无监督域适应方法在遥感领域受限于源域数据访问（隐私、传输限制）。无源目标检测（SFOD）虽有潜力，但其自训练范式在遥感图像中因伪标签噪声（尤其在密集目标和复杂背景下）易导致训练崩溃。", "method": "提出VG-DETR，一个基于半监督框架的遥感图像SFOD模型。它将视觉基础模型（VFM）整合到训练流程中，并利用少量标注的目标域数据。具体方法包括：1) VFM引导的伪标签挖掘策略，利用VFM的语义先验评估并恢复低置信度预测，以提高伪标签质量和数量。2) 双层VFM引导对齐方法，在实例和图像级别将检测器特征与VFM嵌入对齐，通过对比学习和相似性匹配增强特征表示对域间隙的鲁棒性。", "result": "广泛的实验表明，VG-DETR在无源遥感目标检测任务中取得了卓越的性能。", "conclusion": "VG-DETR通过有效利用视觉基础模型和少量目标域标注，成功缓解了SFOD在遥感图像中伪标签噪声和特征鲁棒性不足的问题，显著提升了无源域适应下的目标检测能力。"}}
{"id": "2508.11616", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11616", "abs": "https://arxiv.org/abs/2508.11616", "authors": ["Oscar Mañas", "Pierluca D'Oro", "Koustuv Sinha", "Adriana Romero-Soriano", "Michal Drozdzal", "Aishwarya Agrawal"], "title": "Controlling Multimodal LLMs via Reward-guided Decoding", "comment": "Published at ICCV 2025", "summary": "As Multimodal Large Language Models (MLLMs) gain widespread applicability, it\nis becoming increasingly desirable to adapt them for diverse user needs. In\nthis paper, we study the adaptation of MLLMs through controlled decoding. To\nachieve this, we introduce the first method for reward-guided decoding of MLLMs\nand demonstrate its application in improving their visual grounding. Our method\ninvolves building reward models for visual grounding and using them to guide\nthe MLLM's decoding process. Concretely, we build two separate reward models to\nindependently control the degree of object precision and recall in the model's\noutput. Our approach enables on-the-fly controllability of an MLLM's inference\nprocess in two ways: first, by giving control over the relative importance of\neach reward function during decoding, allowing a user to dynamically trade off\nobject precision for recall in image captioning tasks; second, by giving\ncontrol over the breadth of the search during decoding, allowing the user to\ncontrol the trade-off between the amount of test-time compute and the degree of\nvisual grounding. We evaluate our method on standard object hallucination\nbenchmarks, showing that it provides significant controllability over MLLM\ninference, while consistently outperforming existing hallucination mitigation\nmethods.", "AI": {"tldr": "该论文提出首个针对多模态大语言模型（MLLMs）的奖励引导解码方法，通过构建精度和召回奖励模型，实现对MLLM推理过程的动态控制，显著提升视觉接地能力并缓解幻觉。", "motivation": "随着多模态大语言模型（MLLMs）的广泛应用，需要为其提供适应多样化用户需求的能力，特别是在改善视觉接地和缓解对象幻觉方面。", "method": "引入奖励引导解码方法，为MLLMs构建了两个独立的奖励模型，分别用于控制输出中对象的精度和召回率。该方法允许在推理过程中进行动态控制：一是通过调整不同奖励函数的相对重要性来权衡精度和召回；二是通过控制解码搜索的广度来权衡计算量和视觉接地程度。", "result": "在标准对象幻觉基准测试中，该方法显著提高了MLLM推理的可控性，并持续优于现有幻觉缓解方法。", "conclusion": "奖励引导解码是适应MLLMs、提升其视觉接地能力并提供动态控制的有效方法，且在性能上超越了当前的幻觉缓解技术。"}}
{"id": "2508.11434", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.11434", "abs": "https://arxiv.org/abs/2508.11434", "authors": ["Aditi Dutta", "Susan Banducci"], "title": "Online Anti-sexist Speech: Identifying Resistance to Gender Bias in Political Discourse", "comment": null, "summary": "Anti-sexist speech, i.e., public expressions that challenge or resist\ngendered abuse and sexism, plays a vital role in shaping democratic debate\nonline. Yet automated content moderation systems, increasingly powered by large\nlanguage models (LLMs), may struggle to distinguish such resistance from the\nsexism it opposes. This study examines how five LLMs classify sexist,\nanti-sexist, and neutral political tweets from the UK, focusing on\nhigh-salience trigger events involving female Members of Parliament in the year\n2022. Our analysis show that models frequently misclassify anti-sexist speech\nas harmful, particularly during politically charged events where rhetorical\nstyles of harm and resistance converge. These errors risk silencing those who\nchallenge sexism, with disproportionate consequences for marginalised voices.\nWe argue that moderation design must move beyond binary harmful/not-harmful\nschemas, integrate human-in-the-loop review during sensitive events, and\nexplicitly include counter-speech in training data. By linking feminist\nscholarship, event-based analysis, and model evaluation, this work highlights\nthe sociotechnical challenges of safeguarding resistance speech in digital\npolitical spaces.", "AI": {"tldr": "研究发现，自动化内容审核系统（特别是基于大型语言模型LLMs的）在区分反性别歧视言论和性别歧视言论方面存在困难，尤其是在政治敏感事件中，这可能导致挑战性别歧视的声音被错误压制。", "motivation": "自动化内容审核系统在区分反性别歧视言论和其所反对的性别歧视言论时面临挑战，可能错误地将前者标记为有害内容，从而压制重要的民主辩论和边缘化声音。", "method": "本研究评估了五个大型语言模型(LLMs)如何对来自英国的性别歧视、反性别歧视和中立的政治推文进行分类。研究特别关注了2022年涉及女性议员的高关注度触发事件。", "result": "分析表明，模型经常将反性别歧视言论错误地分类为有害内容，尤其是在修辞风格趋同的政治敏感事件期间。这种错误分类可能导致挑战性别歧视者的声音被压制，对边缘化群体产生不成比例的负面影响。", "conclusion": "内容审核设计应超越简单的有害/无害二元分类，在敏感事件中整合人工审核，并在训练数据中明确包含反驳性言论（counter-speech），以更好地保护数字政治空间中的抵抗性言论。"}}
{"id": "2508.11173", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11173", "abs": "https://arxiv.org/abs/2508.11173", "authors": ["Ruobing Jiang", "Yang Liu", "Haobing Liu", "Yanwei Yu", "Chunyang Wang"], "title": "Exploring the Tradeoff Between Diversity and Discrimination for Continuous Category Discovery", "comment": "Accepted by CIKM 2025. 10 pages, 5 figures,", "summary": "Continuous category discovery (CCD) aims to automatically discover novel\ncategories in continuously arriving unlabeled data. This is a challenging\nproblem considering that there is no number of categories and labels in the\nnewly arrived data, while also needing to mitigate catastrophic forgetting.\nMost CCD methods cannot handle the contradiction between novel class discovery\nand classification well. They are also prone to accumulate errors in the\nprocess of gradually discovering novel classes. Moreover, most of them use\nknowledge distillation and data replay to prevent forgetting, occupying more\nstorage space. To address these limitations, we propose Independence-based\nDiversity and Orthogonality-based Discrimination (IDOD). IDOD mainly includes\nindependent enrichment of diversity module, joint discovery of novelty module,\nand continuous increment by orthogonality module. In independent enrichment,\nthe backbone is trained separately using contrastive loss to avoid it focusing\nonly on features for classification. Joint discovery transforms multi-stage\nnovel class discovery into single-stage, reducing error accumulation impact.\nContinuous increment by orthogonality module generates mutually orthogonal\nprototypes for classification and prevents forgetting with lower space overhead\nvia representative representation replay. Experimental results show that on\nchallenging fine-grained datasets, our method outperforms the state-of-the-art\nmethods.", "AI": {"tldr": "本文提出IDOD方法，用于连续类别发现（CCD），旨在解决现有方法在处理新类别发现与分类矛盾、错误累积以及灾难性遗忘缓解中存储占用大的问题。", "motivation": "现有CCD方法难以平衡新类别发现与分类，易在逐步发现新类过程中累积误差，且多依赖知识蒸馏和数据回放防遗忘，占用大量存储空间。", "method": "提出IDOD（Independence-based Diversity and Orthogonality-based Discrimination）方法，包含：1) 独立多样性增强模块，通过对比学习单独训练主干网络，避免其仅关注分类特征；2) 新颖性联合发现模块，将多阶段新类发现转变为单阶段，减少误差累积；3) 正交性连续增量模块，生成相互正交的原型进行分类，并通过代表性表示回放以较低空间开销防止遗忘。", "result": "在挑战性的细粒度数据集上，该方法优于现有最先进的方法。", "conclusion": "IDOD有效解决了连续类别发现中新类发现与分类的矛盾、错误累积以及高效防遗忘的挑战，实现了性能提升。"}}
{"id": "2508.11628", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11628", "abs": "https://arxiv.org/abs/2508.11628", "authors": ["Qiang Li", "Shansong Wang", "Mingzhe Hu", "Mojtaba Safari", "Zachary Eidex", "Xiaofeng Yang"], "title": "Is ChatGPT-5 Ready for Mammogram VQA?", "comment": null, "summary": "Mammogram visual question answering (VQA) integrates image interpretation\nwith clinical reasoning and has potential to support breast cancer screening.\nWe systematically evaluated the GPT-5 family and GPT-4o model on four public\nmammography datasets (EMBED, InBreast, CMMD, CBIS-DDSM) for BI-RADS assessment,\nabnormality detection, and malignancy classification tasks. GPT-5 consistently\nwas the best performing model but lagged behind both human experts and\ndomain-specific fine-tuned models. On EMBED, GPT-5 achieved the highest scores\namong GPT variants in density (56.8%), distortion (52.5%), mass (64.5%),\ncalcification (63.5%), and malignancy (52.8%) classification. On InBreast, it\nattained 36.9% BI-RADS accuracy, 45.9% abnormality detection, and 35.0%\nmalignancy classification. On CMMD, GPT-5 reached 32.3% abnormality detection\nand 55.0% malignancy accuracy. On CBIS-DDSM, it achieved 69.3% BI-RADS\naccuracy, 66.0% abnormality detection, and 58.2% malignancy accuracy. Compared\nwith human expert estimations, GPT-5 exhibited lower sensitivity (63.5%) and\nspecificity (52.3%). While GPT-5 exhibits promising capabilities for screening\ntasks, its performance remains insufficient for high-stakes clinical imaging\napplications without targeted domain adaptation and optimization. However, the\ntremendous improvements in performance from GPT-4o to GPT-5 show a promising\ntrend in the potential for general large language models (LLMs) to assist with\nmammography VQA tasks.", "AI": {"tldr": "本研究系统评估了GPT-5和GPT-4o在乳腺X线摄影VQA任务上的表现，发现GPT-5是GPT模型中最佳的，但仍落后于人类专家和领域微调模型，表明通用LLM在临床应用前需要进一步的领域适应和优化。", "motivation": "乳腺X线摄影视觉问答（VQA）结合了图像解读和临床推理，有潜力支持乳腺癌筛查，因此评估通用大型语言模型（LLMs）在此领域的应用潜力具有重要意义。", "method": "研究系统评估了GPT-5系列和GPT-4o模型在四个公开乳腺X线摄影数据集（EMBED、InBreast、CMMD、CBIS-DDSM）上的表现，任务包括BI-RADS评估、异常检测和恶性肿瘤分类。", "result": "GPT-5在GPT变体中表现最佳，但在所有任务上均落后于人类专家和领域特异性微调模型。例如，在EMBED数据集上，GPT-5在密度、变形、肿块、钙化和恶性肿瘤分类中得分最高；在InBreast、CMMD和CBIS-DDSM上也取得了不同程度的准确率。与人类专家相比，GPT-5的敏感性（63.5%）和特异性（52.3%）较低。", "conclusion": "尽管GPT-5在筛查任务中展现出前景，但其性能仍不足以在没有目标领域适应和优化的情况下用于高风险临床影像应用。然而，从GPT-4o到GPT-5的巨大性能提升预示着通用LLM在辅助乳腺X线摄影VQA任务方面具有巨大潜力。"}}
{"id": "2508.11442", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11442", "abs": "https://arxiv.org/abs/2508.11442", "authors": ["Bowen Zhang", "Zixin Song", "Chunquan Chen", "Qian-Wen Zhang", "Di Yin", "Xing Sun"], "title": "CoDiEmb: A Collaborative yet Distinct Framework for Unified Representation Learning in Information Retrieval and Semantic Textual Similarity", "comment": null, "summary": "Learning unified text embeddings that excel across diverse downstream tasks\nis a central goal in representation learning, yet negative transfer remains a\npersistent obstacle. This challenge is particularly pronounced when jointly\ntraining a single encoder for Information Retrieval (IR) and Semantic Textual\nSimilarity (STS), two essential but fundamentally disparate tasks for which\nnaive co-training typically yields steep performance trade-offs. We argue that\nresolving this conflict requires systematically decoupling task-specific\nlearning signals throughout the training pipeline. To this end, we introduce\nCoDiEmb, a unified framework that reconciles the divergent requirements of IR\nand STS in a collaborative yet distinct manner. CoDiEmb integrates three key\ninnovations for effective joint optimization: (1) Task-specialized objectives\npaired with a dynamic sampler that forms single-task batches and balances\nper-task updates, thereby preventing gradient interference. For IR, we employ a\ncontrastive loss with multiple positives and hard negatives, augmented by\ncross-device sampling. For STS, we adopt order-aware objectives that directly\noptimize correlation and ranking consistency. (2) A delta-guided model fusion\nstrategy that computes fine-grained merging weights for checkpoints by\nanalyzing each parameter's deviation from its pre-trained initialization,\nproving more effective than traditional Model Soups. (3) An efficient,\nsingle-stage training pipeline that is simple to implement and converges\nstably. Extensive experiments on 15 standard IR and STS benchmarks across three\nbase encoders validate CoDiEmb. Our results and analysis demonstrate that the\nframework not only mitigates cross-task trade-offs but also measurably improves\nthe geometric properties of the embedding space.", "AI": {"tldr": "CoDiEmb是一个统一的框架，通过解耦任务特定学习信号，成功地在信息检索（IR）和语义文本相似度（STS）任务上训练出高性能的文本嵌入，有效克服了负迁移问题。", "motivation": "学习在不同下游任务中表现出色的统一文本嵌入是表征学习的核心目标，但负迁移，尤其是在联合训练IR和STS这两个本质上不同的任务时，是一个持续存在的障碍，导致性能显著下降。", "method": "CoDiEmb框架通过以下三项创新实现IR和STS的协同但独立训练：1) 任务专用目标（IR采用多正例和难负例对比损失，STS采用序感知目标）和动态采样器，形成单任务批次并平衡更新，防止梯度干扰。2) 一种delta引导的模型融合策略，通过分析参数偏离预训练初始化的程度来计算细粒度合并权重。3) 一个高效、单阶段的训练流程，易于实现且收敛稳定。", "result": "在15个标准IR和STS基准测试以及三个基础编码器上的广泛实验验证了CoDiEmb。结果表明，该框架不仅减轻了跨任务权衡，而且显著改善了嵌入空间的几何特性。", "conclusion": "CoDiEmb成功地协调了IR和STS的不同需求，实现了统一的文本嵌入，有效缓解了负迁移问题，并在多个基准测试中展现出优异的性能和改进的嵌入空间特性。"}}
{"id": "2508.11176", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11176", "abs": "https://arxiv.org/abs/2508.11176", "authors": ["Yumiao Zhao", "Bo Jiang", "Yuhe Ding", "Xiao Wang", "Jin Tang", "Bin Luo"], "title": "Fine-Grained VLM Fine-tuning via Latent Hierarchical Adapter Learning", "comment": null, "summary": "Adapter-based approaches have garnered attention for fine-tuning pre-trained\nVision-Language Models (VLMs) on few-shot classification tasks. These methods\nstrive to develop a lightweight module that better aligns visual and (category)\ntextual representations, thereby enhancing performance on downstream few-shot\nlearning tasks. However, existing adapters generally learn/align (category)\ntextual-visual modalities via explicit spatial proximity in the underlying\nembedding space, which i) fails to capture the inherent one-to-many\nassociations between categories and image samples and ii) struggles to\nestablish accurate associations between the unknown categories and images. To\naddress these issues, inspired by recent works on hyperbolic learning, we\ndevelop a novel Latent Hierarchical Adapter (LatHAdapter) for fine-tuning VLMs\non downstream few-shot classification tasks. The core of LatHAdapter is to\nexploit the latent semantic hierarchy of downstream training data and employ it\nto provide richer, fine-grained guidance for the adapter learning process.\nSpecifically, LatHAdapter first introduces some learnable `attribute' prompts\nas the bridge to align categories and images. Then, it projects the categories,\nattribute prompts, and images within each batch in a hyperbolic space, and\nemploys hierarchical regularization to learn the latent semantic hierarchy of\nthem, thereby fully modeling the inherent one-to-many associations among\ncategories, learnable attributes, and image samples. Extensive experiments on\nfour challenging few-shot tasks show that the proposed LatHAdapter consistently\noutperforms many other fine-tuning approaches, particularly in adapting known\nclasses and generalizing to unknown classes.", "AI": {"tldr": "本文提出了一种名为LatHAdapter的新型潜在分层适配器，用于微调视觉-语言模型（VLMs）以进行少样本分类。它利用双曲空间和潜在语义层次结构来更好地建模类别、属性和图像样本之间固有的多对一关联，从而在已知和未知类别上都表现出色。", "motivation": "现有的基于适配器的方法在少样本分类任务中微调预训练VLMs时，通常通过显式空间邻近性来对齐视觉和文本表示。然而，这种方法未能捕捉类别与图像样本之间固有的多对一关联，并且难以建立未知类别与图像之间的准确关联。", "method": "本文受双曲学习启发，开发了潜在分层适配器（LatHAdapter）。其核心是利用下游训练数据的潜在语义层次结构，为适配器学习过程提供更丰富、细粒度的指导。具体而言，LatHAdapter首先引入可学习的“属性”提示作为连接类别和图像的桥梁。然后，它将每个批次中的类别、属性提示和图像投影到双曲空间中，并采用分层正则化来学习它们的潜在语义层次结构，从而充分建模类别、可学习属性和图像样本之间固有的多对一关联。", "result": "在四项具有挑战性的少样本任务上进行的广泛实验表明，所提出的LatHAdapter始终优于许多其他微调方法，特别是在适应已知类别和泛化到未知类别方面。", "conclusion": "LatHAdapter通过利用双曲空间和建模潜在语义层次结构，有效解决了现有适配器在捕捉多对一关联方面的局限性，显著提升了VLM在少样本分类任务中的微调性能，尤其是在处理已知和未知类别时。"}}
{"id": "2508.11534", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.11534", "abs": "https://arxiv.org/abs/2508.11534", "authors": ["Monika Jotautaitė", "Lucius Caviola", "David A. Brewster", "Thilo Hagendorff"], "title": "Speciesism in AI: Evaluating Discrimination Against Animals in Large Language Models", "comment": null, "summary": "As large language models (LLMs) become more widely deployed, it is crucial to\nexamine their ethical tendencies. Building on research on fairness and\ndiscrimination in AI, we investigate whether LLMs exhibit speciesist bias --\ndiscrimination based on species membership -- and how they value non-human\nanimals. We systematically examine this issue across three paradigms: (1)\nSpeciesismBench, a 1,003-item benchmark assessing recognition and moral\nevaluation of speciesist statements; (2) established psychological measures\ncomparing model responses with those of human participants; (3) text-generation\ntasks probing elaboration on, or resistance to, speciesist rationalizations. In\nour benchmark, LLMs reliably detected speciesist statements but rarely\ncondemned them, often treating speciesist attitudes as morally acceptable. On\npsychological measures, results were mixed: LLMs expressed slightly lower\nexplicit speciesism than people, yet in direct trade-offs they more often chose\nto save one human over multiple animals. A tentative interpretation is that\nLLMs may weight cognitive capacity rather than species per se: when capacities\nwere equal, they showed no species preference, and when an animal was described\nas more capable, they tended to prioritize it over a less capable human. In\nopen-ended text generation tasks, LLMs frequently normalized or rationalized\nharm toward farmed animals while refusing to do so for non-farmed animals.\nThese findings suggest that while LLMs reflect a mixture of progressive and\nmainstream human views, they nonetheless reproduce entrenched cultural norms\naround animal exploitation. We argue that expanding AI fairness and alignment\nframeworks to explicitly include non-human moral patients is essential for\nreducing these biases and preventing the entrenchment of speciesist attitudes\nin AI systems and the societies they influence.", "AI": {"tldr": "研究发现大型语言模型（LLMs）存在物种歧视偏见，尤其是在对待农场动物方面，它们倾向于将人类主流的动物剥削文化规范合理化，而非谴责。", "motivation": "随着大型语言模型（LLMs）的广泛部署，审查其伦理倾向至关重要。本研究旨在探讨LLMs是否表现出基于物种成员身份的歧视（物种歧视），以及它们如何评价非人类动物，以扩展人工智能公平性和歧视研究的范畴。", "method": "研究通过三种范式系统地考察了LLMs的物种歧视偏见：1) 使用包含1003个项目的SpeciesismBench基准测试，评估模型对物种歧视言论的识别和道德评价能力；2) 采用既定的心理学测量方法，比较模型与人类参与者的反应；3) 进行文本生成任务，探究模型对物种歧视合理化的阐述或抵制程度。", "result": "在基准测试中，LLMs能可靠地检测出物种歧视言论，但很少谴责，常将其视为道德可接受。在心理学测量中，结果复杂：LLMs表现出略低于人类的明确物种歧视，但在直接权衡中，它们更常选择拯救一个人而非多只动物。初步解释是LLMs可能更看重认知能力而非物种本身。在开放式文本生成任务中，LLMs频繁将对农场动物的伤害正常化或合理化，但拒绝将此用于非农场动物。这些发现表明LLMs混合了进步和主流人类观点，但再现了根深蒂固的动物剥削文化规范。", "conclusion": "研究认为，为了减少LLMs中的物种歧视偏见并防止其在AI系统及受其影响的社会中根深蒂固，有必要扩展AI公平性和对齐框架，明确将非人类道德主体纳入考量。"}}
{"id": "2508.11183", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11183", "abs": "https://arxiv.org/abs/2508.11183", "authors": ["Zhenghao Chen", "Zicong Chen", "Lei Liu", "Yiming Wu", "Dong Xu"], "title": "Versatile Video Tokenization with Generative 2D Gaussian Splatting", "comment": null, "summary": "Video tokenization procedure is critical for a wide range of video processing\ntasks. Most existing approaches directly transform video into fixed-grid and\npatch-wise tokens, which exhibit limited versatility. Spatially, uniformly\nallocating a fixed number of tokens often leads to over-encoding in\nlow-information regions. Temporally, reducing redundancy remains challenging\nwithout explicitly distinguishing between static and dynamic content. In this\nwork, we propose the Gaussian Video Transformer (GVT), a versatile video\ntokenizer built upon a generative 2D Gaussian Splatting (2DGS) strategy. We\nfirst extract latent rigid features from a video clip and represent them with a\nset of 2D Gaussians generated by our proposed Spatio-Temporal Gaussian\nEmbedding (STGE) mechanism in a feed-forward manner. Such generative 2D\nGaussians not only enhance spatial adaptability by assigning higher (resp.,\nlower) rendering weights to regions with higher (resp., lower) information\ncontent during rasterization, but also improve generalization by avoiding\nper-video optimization.To enhance the temporal versatility, we introduce a\nGaussian Set Partitioning (GSP) strategy that separates the 2D Gaussians into\nstatic and dynamic sets, which explicitly model static content shared across\ndifferent time-steps and dynamic content specific to each time-step, enabling a\ncompact representation.We primarily evaluate GVT on the video reconstruction,\nwhile also assessing its performance on action recognition and compression\nusing the UCF101, Kinetics, and DAVIS datasets. Extensive experiments\ndemonstrate that GVT achieves a state-of-the-art video reconstruction quality,\noutperforms the baseline MAGVIT-v2 in action recognition, and delivers\ncomparable compression performance.", "AI": {"tldr": "本文提出了高斯视频Transformer（GVT），一种基于生成式2D高斯溅射的通用视频标记化方法，通过空间自适应高斯和时空高斯集划分，实现了高效且紧凑的视频表示，在视频重建、动作识别和压缩方面表现出色。", "motivation": "现有视频标记化方法将视频直接转换为固定网格和块状标记，导致空间上低信息区域的过度编码以及时间上难以区分静态和动态内容，从而限制了其通用性和冗余消除能力。", "method": "GVT通过以下方式实现：1. 提出Spatio-Temporal Gaussian Embedding (STGE) 机制，前向生成一组2D高斯来表示视频剪辑的潜在刚性特征，实现空间自适应编码并避免逐视频优化。2. 引入Gaussian Set Partitioning (GSP) 策略，将2D高斯分为静态和动态集，明确建模跨时间共享的静态内容和特定于每个时间步的动态内容，实现紧凑表示。", "result": "GVT在视频重建方面达到了最先进的质量，在动作识别方面超越了基线MAGVIT-v2，并在压缩性能上达到了可比较的水平。实验在UCF101、Kinetics和DAVIS数据集上进行。", "conclusion": "GVT通过创新的生成式2D高斯溅射和高斯集划分策略，提供了一种通用且高效的视频标记化方法，显著提升了视频重建、动作识别和压缩等任务的性能。"}}
{"id": "2508.11536", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11536", "abs": "https://arxiv.org/abs/2508.11536", "authors": ["Maria Ryskina", "Greta Tuckute", "Alexander Fung", "Ashley Malkin", "Evelina Fedorenko"], "title": "Language models align with brain regions that represent concepts across modalities", "comment": "Accepted to COLM 2025. Code and data can be found at\n  https://github.com/ryskina/concepts-brain-llms", "summary": "Cognitive science and neuroscience have long faced the challenge of\ndisentangling representations of language from representations of conceptual\nmeaning. As the same problem arises in today's language models (LMs), we\ninvestigate the relationship between LM--brain alignment and two neural\nmetrics: (1) the level of brain activation during processing of sentences,\ntargeting linguistic processing, and (2) a novel measure of meaning consistency\nacross input modalities, which quantifies how consistently a brain region\nresponds to the same concept across paradigms (sentence, word cloud, image)\nusing an fMRI dataset (Pereira et al., 2018). Our experiments show that both\nlanguage-only and language-vision models predict the signal better in more\nmeaning-consistent areas of the brain, even when these areas are not strongly\nsensitive to language processing, suggesting that LMs might internally\nrepresent cross-modal conceptual meaning.", "AI": {"tldr": "该研究通过比较语言模型（LMs）与大脑活动数据，发现LMs在预测大脑信号时，与处理跨模态概念意义的脑区对齐度更高，而非仅仅是语言处理区域。", "motivation": "认知科学和神经科学长期面临区分语言表征与概念意义表征的挑战，现代语言模型也面临类似问题，因此研究LMs与大脑的对齐方式。", "method": "研究LM-大脑对齐与两个神经指标的关系：1) 句子处理期间的大脑激活水平（针对语言处理）；2) 一种新颖的跨输入模态（句子、词云、图像）意义一致性测量，量化大脑区域对同一概念的响应一致性，使用fMRI数据集（Pereira et al., 2018）。", "result": "实验表明，无论是纯语言模型还是语言-视觉模型，都能更好地预测大脑中意义一致性更高的区域的信号，即使这些区域对语言处理不敏感。", "conclusion": "研究结果表明语言模型可能内部表征了跨模态的概念意义。"}}
{"id": "2508.11185", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11185", "abs": "https://arxiv.org/abs/2508.11185", "authors": ["Abhinav Kumar", "Yuliang Guo", "Zhihao Zhang", "Xinyu Huang", "Liu Ren", "Xiaoming Liu"], "title": "CHARM3R: Towards Unseen Camera Height Robust Monocular 3D Detector", "comment": "ICCV 2025", "summary": "Monocular 3D object detectors, while effective on data from one ego camera\nheight, struggle with unseen or out-of-distribution camera heights. Existing\nmethods often rely on Plucker embeddings, image transformations or data\naugmentation. This paper takes a step towards this understudied problem by\nfirst investigating the impact of camera height variations on state-of-the-art\n(SoTA) Mono3D models. With a systematic analysis on the extended CARLA dataset\nwith multiple camera heights, we observe that depth estimation is a primary\nfactor influencing performance under height variations. We mathematically prove\nand also empirically observe consistent negative and positive trends in mean\ndepth error of regressed and ground-based depth models, respectively, under\ncamera height changes. To mitigate this, we propose Camera Height Robust\nMonocular 3D Detector (CHARM3R), which averages both depth estimates within the\nmodel. CHARM3R improves generalization to unseen camera heights by more than\n$45\\%$, achieving SoTA performance on the CARLA dataset. Codes and Models at\nhttps://github.com/abhi1kumar/CHARM3R", "AI": {"tldr": "该研究旨在解决单目3D目标检测器在不同相机高度下性能下降的问题，通过分析发现深度估计是关键因素，并提出CHARM3R模型，通过平均两种深度估计来提高对未知相机高度的泛化能力。", "motivation": "现有的单目3D目标检测器在遇到未见过或分布外的相机高度时表现不佳，尽管有些方法尝试使用Plucker嵌入、图像变换或数据增强，但该问题仍未得到充分研究。", "method": "首先，系统性地调查相机高度变化对最先进的单目3D模型的影响，通过在扩展的CARLA数据集上进行多相机高度分析。发现深度估计是影响性能的主要因素。然后，提出Camera Height Robust Monocular 3D Detector (CHARM3R) 模型，该模型在模型内部平均两种深度估计（回归深度和基于地面的深度），以缓解相机高度变化带来的影响。", "result": "研究发现，在相机高度变化下，回归深度模型的平均深度误差呈现负向趋势，而基于地面的深度模型则呈现正向趋势。CHARM3R模型将对未知相机高度的泛化能力提高了超过45%，并在CARLA数据集上取得了最先进的性能。", "conclusion": "相机高度变化对单目3D检测器的性能有显著影响，其中深度估计是核心问题。通过在模型中平均两种深度估计，CHARM3R模型能有效提高对不同相机高度的泛化能力，从而在更广泛的应用场景中实现鲁棒的单目3D目标检测。"}}
{"id": "2508.11567", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11567", "abs": "https://arxiv.org/abs/2508.11567", "authors": ["Jinpeng Hu", "Ao Wang", "Qianqian Xie", "Hui Ma", "Zhuo Li", "Dan Guo"], "title": "AgentMental: An Interactive Multi-Agent Framework for Explainable and Adaptive Mental Health Assessment", "comment": null, "summary": "Mental health assessment is crucial for early intervention and effective\ntreatment, yet traditional clinician-based approaches are limited by the\nshortage of qualified professionals. Recent advances in artificial intelligence\nhave sparked growing interest in automated psychological assessment, yet most\nexisting approaches are constrained by their reliance on static text analysis,\nlimiting their ability to capture deeper and more informative insights that\nemerge through dynamic interaction and iterative questioning. Therefore, in\nthis paper, we propose a multi-agent framework for mental health evaluation\nthat simulates clinical doctor-patient dialogues, with specialized agents\nassigned to questioning, adequacy evaluation, scoring, and updating. We\nintroduce an adaptive questioning mechanism in which an evaluation agent\nassesses the adequacy of user responses to determine the necessity of\ngenerating targeted follow-up queries to address ambiguity and missing\ninformation. Additionally, we employ a tree-structured memory in which the root\nnode encodes the user's basic information, while child nodes (e.g., topic and\nstatement) organize key information according to distinct symptom categories\nand interaction turns. This memory is dynamically updated throughout the\ninteraction to reduce redundant questioning and further enhance the information\nextraction and contextual tracking capabilities. Experimental results on the\nDAIC-WOZ dataset illustrate the effectiveness of our proposed method, which\nachieves better performance than existing approaches.", "AI": {"tldr": "本文提出一个多智能体框架，通过模拟医患对话，实现自动化的心理健康评估，该框架包含自适应提问机制和树形记忆结构，以提高评估的深度和准确性。", "motivation": "传统的临床医生评估方式受限于专业人员短缺；现有AI心理评估方法多依赖静态文本分析，无法捕获动态交互和迭代提问中产生的更深层、信息量更大的见解。", "method": "提出一个多智能体框架，模拟临床医患对话，包含提问、充分性评估、评分和更新等专业智能体。引入自适应提问机制，评估智能体判断用户回答的充分性，以生成有针对性的后续问题，解决歧义和信息缺失。采用树形记忆结构，根节点存储用户基本信息，子节点按症状类别和交互轮次组织关键信息，该记忆在交互中动态更新，减少重复提问并增强信息提取和上下文跟踪能力。", "result": "在DAIC-WOZ数据集上的实验结果表明，所提出的方法比现有方法表现更好。", "conclusion": "所提出的模拟临床医患对话的多智能体心理健康评估框架，结合自适应提问机制和树形记忆，能有效提升自动化心理健康评估的性能和深度。"}}
{"id": "2508.11192", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11192", "abs": "https://arxiv.org/abs/2508.11192", "authors": ["Lavisha Aggarwal", "Vikas Bahirwani", "Lin Li", "Andrea Colaco"], "title": "Generating Dialogues from Egocentric Instructional Videos for Task Assistance: Dataset, Method and Benchmark", "comment": null, "summary": "Many everyday tasks ranging from fixing appliances, cooking recipes to car\nmaintenance require expert knowledge, especially when tasks are complex and\nmulti-step. Despite growing interest in AI agents, there is a scarcity of\ndialogue-video datasets grounded for real world task assistance. In this paper,\nwe propose a simple yet effective approach that transforms single-person\ninstructional videos into task-guidance two-person dialogues, aligned with fine\ngrained steps and video-clips. Our fully automatic approach, powered by large\nlanguage models, offers an efficient alternative to the substantial cost and\neffort required for human-assisted data collection. Using this technique, we\nbuild HowToDIV, a large-scale dataset containing 507 conversations, 6636\nquestion-answer pairs and 24 hours of videoclips across diverse tasks in\ncooking, mechanics, and planting. Each session includes multi-turn conversation\nwhere an expert teaches a novice user how to perform a task step by step, while\nobserving user's surrounding through a camera and microphone equipped wearable\ndevice. We establish the baseline benchmark performance on HowToDIV dataset\nthrough Gemma-3 model for future research on this new task of dialogues for\nprocedural-task assistance.", "AI": {"tldr": "本文提出了一种利用大型语言模型将单人教学视频自动转换为两人任务指导对话的方法，并构建了一个名为HowToDIV的大规模多模态数据集，用于现实世界任务协助对话研究。", "motivation": "许多日常任务需要专家知识，尤其是在复杂的多步骤任务中。然而，目前缺乏用于现实世界任务协助的对话-视频数据集，且人工收集数据成本高昂。", "method": "该研究提出了一种简单有效的方法，利用大型语言模型（LLMs）将单人教学视频自动转换为两人任务指导对话，并与细粒度步骤和视频片段对齐。利用此技术构建了HowToDIV数据集，并使用Gemma-3模型在该数据集上建立了基线性能。", "result": "成功构建了HowToDIV数据集，包含507个对话、6636个问答对和24小时的视频片段，涵盖烹饪、机械和种植等多种任务。每个会话都是多轮对话，专家通过可穿戴设备观察和指导新手用户完成任务。研究团队使用Gemma-3模型在该数据集上建立了对话式程序任务协助的新任务的基线性能。", "conclusion": "该研究提出了一种高效的自动方法来生成现实世界任务协助所需的对话-视频数据集，克服了传统人工数据收集的局限性。HowToDIV数据集及其建立的基线将为未来在程序任务协助对话领域的研究提供宝贵资源。"}}
{"id": "2508.11598", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.11598", "abs": "https://arxiv.org/abs/2508.11598", "authors": ["Greta Tuckute", "Klemen Kotar", "Evelina Fedorenko", "Daniel L. K. Yamins"], "title": "Representing Speech Through Autoregressive Prediction of Cochlear Tokens", "comment": null, "summary": "We introduce AuriStream, a biologically inspired model for encoding speech\nvia a two-stage framework inspired by the human auditory processing hierarchy.\nThe first stage transforms raw audio into a time-frequency representation based\non the human cochlea, from which we extract discrete \\textbf{cochlear tokens}.\nThe second stage applies an autoregressive sequence model over the cochlear\ntokens. AuriStream learns meaningful phoneme and word representations, and\nstate-of-the-art lexical semantics. AuriStream shows competitive performance on\ndiverse downstream SUPERB speech tasks. Complementing AuriStream's strong\nrepresentational capabilities, it generates continuations of audio which can be\nvisualized in a spectrogram space and decoded back into audio, providing\ninsights into the model's predictions. In summary, we present a two-stage\nframework for speech representation learning to advance the development of more\nhuman-like models that efficiently handle a range of speech-based tasks.", "AI": {"tldr": "AuriStream是一个受生物学启发的两阶段语音编码模型，能学习有意义的语音表示，在各种语音任务上表现出色，并能生成音频延续。", "motivation": "开发更像人类、能高效处理各种语音任务的模型。", "method": "采用两阶段框架：第一阶段将原始音频转换为基于人耳蜗的时间-频率表示，提取离散的“耳蜗标记”；第二阶段对这些耳蜗标记应用自回归序列模型。", "result": "AuriStream学习到有意义的音素和词表示，获得了最先进的词汇语义，并在多种SUPERB语音任务上表现出竞争力。此外，它能生成音频延续，可在频谱图空间中可视化并解码回音频。", "conclusion": "该研究提出了一个用于语音表示学习的两阶段框架，旨在推动开发更像人类、能高效处理各种语音任务的模型。"}}
{"id": "2508.11196", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11196", "abs": "https://arxiv.org/abs/2508.11196", "authors": ["Jiajin Guan", "Haibo Mei", "Bonan Zhang", "Dan Liu", "Yuanshuang Fu", "Yue Zhang"], "title": "UAV-VL-R1: Generalizing Vision-Language Models via Supervised Fine-Tuning and Multi-Stage GRPO for UAV Visual Reasoning", "comment": null, "summary": "Recent advances in vision-language models (VLMs) have demonstrated strong\ngeneralization in natural image tasks. However, their performance often\ndegrades on unmanned aerial vehicle (UAV)-based aerial imagery, which features\nhigh resolution, complex spatial semantics, and strict real-time constraints.\nThese challenges limit the applicability of general-purpose VLMs to structured\naerial reasoning tasks. To address these challenges, we propose UAV-VL-R1, a\nlightweight VLM explicitly designed for aerial visual reasoning. It is trained\nusing a hybrid method that combines supervised fine-tuning (SFT) and\nmulti-stage reinforcement learning (RL). We leverage the group relative policy\noptimization (GRPO) algorithm to promote structured and interpretable reasoning\nthrough rule-guided rewards and intra-group policy alignment. To support model\ntraining and evaluation, we introduce a high-resolution visual question\nanswering dataset named HRVQA-VL, which consists of 50,019 annotated samples\ncovering eight UAV-relevant reasoning tasks, including object counting,\ntransportation recognition, and spatial scene inference. Experimental results\nshow that UAV-VL-R1 achieves a 48.17% higher zero-shot accuracy than the\nQwen2-VL-2B-Instruct baseline and even outperforms its 72B-scale variant, which\nis 36x larger, on multiple tasks. Ablation studies reveal that while SFT\nimproves semantic alignment, it may reduce reasoning diversity in mathematical\ntasks. GRPO-based RL compensates for this limitation by enhancing logical\nflexibility and the robustness of inference. Additionally, UAV-VL-R1 requires\nonly 3.9GB of memory under FP16 inference and can be quantized to 2.5GB with\nINT8, supporting real-time deployment on resource-constrained UAV platforms.", "AI": {"tldr": "本文提出UAV-VL-R1，一个轻量级视觉语言模型，专为无人机航空图像推理设计。它采用SFT与多阶段强化学习（基于GRPO）混合训练，并引入了高分辨率VQA数据集HRVQA-VL，在多项任务上显著优于现有基线模型，且内存占用极低，支持实时部署。", "motivation": "现有视觉语言模型（VLMs）在自然图像任务上表现良好，但在无人机（UAV）航空图像上性能下降，原因在于航空图像的高分辨率、复杂空间语义和严格的实时性要求，这些限制了通用VLM在结构化航空推理任务中的应用。", "method": "提出UAV-VL-R1，一个轻量级VLM，专为航空视觉推理设计。采用监督微调（SFT）和多阶段强化学习（RL）相结合的混合训练方法。利用群组相对策略优化（GRPO）算法，通过规则引导奖励和组内策略对齐，促进结构化和可解释的推理。同时，引入高分辨率视觉问答数据集HRVQA-VL，包含50,019个标注样本，覆盖八种无人机相关推理任务。", "result": "UAV-VL-R1在零样本准确率上比Qwen2-VL-2B-Instruct基线高出48.17%，甚至在多个任务上超越了其大36倍的72B变体。消融研究表明，SFT提升了语义对齐但可能降低数学任务的推理多样性，而基于GRPO的RL弥补了这一限制，增强了逻辑灵活性和推理鲁棒性。此外，UAV-VL-R1在FP16推理下仅需3.9GB内存，量化至INT8后为2.5GB，支持在资源受限的无人机平台上实时部署。", "conclusion": "UAV-VL-R1是一个高效且性能卓越的轻量级VLM，能够有效解决无人机航空图像推理的挑战。其混合训练方法（SFT与GRPO-RL）被证明是成功的，能够兼顾语义对齐和推理灵活性。低内存占用使其非常适合在资源受限的无人机平台进行实时部署。"}}
{"id": "2508.11605", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11605", "abs": "https://arxiv.org/abs/2508.11605", "authors": ["Rob Reijtenbach", "Suzan Verberne", "Gijs Wijnholds"], "title": "Dataset Creation for Visual Entailment using Generative AI", "comment": "NALOMA: Natural Logic meets Machine Learning workshop @ ESSLLI 2025", "summary": "In this paper we present and validate a new synthetic dataset for training\nvisual entailment models. Existing datasets for visual entailment are small and\nsparse compared to datasets for textual entailment. Manually creating datasets\nis labor-intensive. We base our synthetic dataset on the SNLI dataset for\ntextual entailment. We take the premise text from SNLI as input prompts in a\ngenerative image model, Stable Diffusion, creating an image to replace each\ntextual premise. We evaluate our dataset both intrinsically and extrinsically.\nFor extrinsic evaluation, we evaluate the validity of the generated images by\nusing them as training data for a visual entailment classifier based on CLIP\nfeature vectors. We find that synthetic training data only leads to a slight\ndrop in quality on SNLI-VE, with an F-score 0.686 compared to 0.703 when\ntrained on real data. We also compare the quality of our generated training\ndata to original training data on another dataset: SICK-VTE. Again, there is\nonly a slight drop in F-score: from 0.400 to 0.384. These results indicate that\nin settings with data sparsity, synthetic data can be a promising solution for\ntraining visual entailment models.", "AI": {"tldr": "本文提出并验证了一种新的合成视觉蕴涵数据集，通过使用Stable Diffusion将SNLI文本前提转换为图像，旨在解决现有数据集小且稀疏的问题，并证明合成数据在训练视觉蕴涵模型时效果良好。", "motivation": "现有视觉蕴涵数据集规模小且稀疏，而手动创建数据集劳动密集。因此，需要一种更高效的方法来生成大规模训练数据。", "method": "该研究基于SNLI文本蕴涵数据集，使用其前提文本作为Stable Diffusion生成图像模型的输入提示，从而为每个文本前提创建对应的图像。数据集通过内在和外在方式进行评估。外在评估方面，使用生成的图像作为训练数据，训练一个基于CLIP特征向量的视觉蕴涵分类器，并在SNLI-VE和SICK-VTE数据集上进行性能比较。", "result": "使用合成训练数据在SNLI-VE上的F-score为0.686，而使用真实数据训练为0.703，仅略有下降。在SICK-VTE数据集上，F-score从0.400降至0.384，同样是轻微下降。这些结果表明合成数据质量接近真实数据。", "conclusion": "在数据稀疏的场景下，合成数据是训练视觉蕴涵模型的一个有前景的解决方案，能够有效弥补真实数据不足的问题。"}}
{"id": "2508.11212", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11212", "abs": "https://arxiv.org/abs/2508.11212", "authors": ["Zhangjian Ji", "Wenjin Zhang", "Shaotong Qiao", "Kai Feng", "Yuhua Qian"], "title": "A Coarse-to-Fine Human Pose Estimation Method based on Two-stage Distillation and Progressive Graph Neural Network", "comment": null, "summary": "Human pose estimation has been widely applied in the human-centric\nunderstanding and generation, but most existing state-of-the-art human pose\nestimation methods require heavy computational resources for accurate\npredictions. In order to obtain an accurate, robust yet lightweight human pose\nestimator, one feasible way is to transfer pose knowledge from a powerful\nteacher model to a less-parameterized student model by knowledge distillation.\nHowever, the traditional knowledge distillation framework does not fully\nexplore the contextual information among human joints. Thus, in this paper, we\npropose a novel coarse-to-fine two-stage knowledge distillation framework for\nhuman pose estimation. In the first-stage distillation, we introduce the human\njoints structure loss to mine the structural information among human joints so\nas to transfer high-level semantic knowledge from the teacher model to the\nstudent model. In the second-stage distillation, we utilize an Image-Guided\nProgressive Graph Convolutional Network (IGP-GCN) to refine the initial human\npose obtained from the first-stage distillation and supervise the training of\nthe IGP-GCN in the progressive way by the final output pose of teacher model.\nThe extensive experiments on the benchmark dataset: COCO keypoint and CrowdPose\ndatasets, show that our proposed method performs favorably against lots of the\nexisting state-of-the-art human pose estimation methods, especially for the\nmore complex CrowdPose dataset, the performance improvement of our model is\nmore significant.", "AI": {"tldr": "提出了一种新颖的粗到精两阶段知识蒸馏框架，通过利用人体关节结构信息和图像引导的渐进图卷积网络，实现轻量级、准确且鲁棒的人体姿态估计。", "motivation": "现有SOTA人体姿态估计方法计算资源消耗大；传统知识蒸馏框架未能充分挖掘人体关节间的上下文信息。", "method": "引入两阶段知识蒸馏框架：第一阶段，引入人体关节结构损失，挖掘关节结构信息以传递高级语义知识；第二阶段，利用图像引导的渐进图卷积网络（IGP-GCN）细化初始姿态，并以渐进方式受教师模型最终输出监督训练。", "result": "在COCO和CrowdPose基准数据集上，所提方法优于许多现有SOTA方法，尤其在更复杂的CrowdPose数据集上性能提升显著。", "conclusion": "所提出的粗到精两阶段知识蒸馏框架，结合关节结构损失和IGP-GCN，能够有效提升轻量级人体姿态估计的准确性和鲁棒性。"}}
{"id": "2508.11607", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11607", "abs": "https://arxiv.org/abs/2508.11607", "authors": ["Christopher J. Agostino"], "title": "TinyTim: A Family of Language Models for Divergent Generation", "comment": "7 pages, 3 figures, submitted to NeurIPS Creative AI track, code and\n  model available at https://hf.co/npc-worldwide/TinyTimV1", "summary": "This work introduces TinyTim, a family of large language models fine-tuned on\nJames Joyce's `Finnegans Wake'. Through quantitative evaluation against\nbaseline models, we demonstrate that TinyTim V1 produces a statistically\ndistinct generative profile characterized by high lexical diversity and low\nsemantic coherence. These findings are interpreted through theories of\ncreativity and complex problem-solving, arguing that such specialized models\ncan function as divergent knowledge sources within more extensive creative\narchitectures, powering automated discovery mechanisms in diverse settings.", "AI": {"tldr": "TinyTim是一个在詹姆斯·乔伊斯《芬尼根的守灵夜》上微调的大语言模型系列，其生成文本具有高词汇多样性和低语义连贯性的独特特征。", "motivation": "研究旨在探索专业化模型作为发散性知识源在创意架构中的潜力，并将其与创造力及复杂问题解决理论相结合，以期驱动自动化发现机制。", "method": "通过在詹姆斯·乔伊斯《芬尼根的守灵夜》上微调大型语言模型（命名为TinyTim），并与基线模型进行定量评估，以分析其生成文本的特征。", "result": "TinyTim V1生成了一种统计学上独特的文本配置文件，其特点是词汇多样性高但语义连贯性低。", "conclusion": "这些专业化模型可以在更广泛的创意架构中作为发散性知识源，为各种设置下的自动化发现机制提供动力。"}}
{"id": "2508.11218", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11218", "abs": "https://arxiv.org/abs/2508.11218", "authors": ["Jialin Li", "Shuqi Wu", "Ning Wang"], "title": "A CLIP-based Uncertainty Modal Modeling (UMM) Framework for Pedestrian Re-Identification in Autonomous Driving", "comment": null, "summary": "Re-Identification (ReID) is a critical technology in intelligent perception\nsystems, especially within autonomous driving, where onboard cameras must\nidentify pedestrians across views and time in real-time to support safe\nnavigation and trajectory prediction. However, the presence of uncertain or\nmissing input modalities--such as RGB, infrared, sketches, or textual\ndescriptions--poses significant challenges to conventional ReID approaches.\nWhile large-scale pre-trained models offer strong multimodal semantic modeling\ncapabilities, their computational overhead limits practical deployment in\nresource-constrained environments. To address these challenges, we propose a\nlightweight Uncertainty Modal Modeling (UMM) framework, which integrates a\nmultimodal token mapper, synthetic modality augmentation strategy, and\ncross-modal cue interactive learner. Together, these components enable unified\nfeature representation, mitigate the impact of missing modalities, and extract\ncomplementary information across different data types. Additionally, UMM\nleverages CLIP's vision-language alignment ability to fuse multimodal inputs\nefficiently without extensive finetuning. Experimental results demonstrate that\nUMM achieves strong robustness, generalization, and computational efficiency\nunder uncertain modality conditions, offering a scalable and practical solution\nfor pedestrian re-identification in autonomous driving scenarios.", "AI": {"tldr": "本文提出了一种轻量级不确定模态建模（UMM）框架，用于在自动驾驶场景下解决行人再识别中模态不确定或缺失的问题，通过多模态令牌映射、合成模态增强和跨模态线索交互学习，并利用CLIP的视觉-语言对齐能力，实现了鲁棒、泛化和计算高效的解决方案。", "motivation": "行人再识别（ReID）是自动驾驶中关键技术，但传统方法难以应对RGB、红外、草图或文本描述等输入模态的不确定或缺失。同时，大型预训练模型虽然语义建模能力强，但计算开销大，不适合资源受限环境的实际部署。", "method": "本文提出了一个轻量级的不确定模态建模（UMM）框架。该框架集成了多模态令牌映射器、合成模态增强策略和跨模态线索交互学习器，以实现统一特征表示、减轻缺失模态影响并提取互补信息。此外，UMM利用CLIP的视觉-语言对齐能力，无需大量微调即可高效融合多模态输入。", "result": "实验结果表明，UMM在不确定模态条件下展现出强大的鲁棒性、泛化能力和计算效率。", "conclusion": "UMM为自动驾驶场景下的行人再识别提供了一个可扩展且实用的解决方案，有效应对了模态不确定性和计算资源限制的挑战。"}}
{"id": "2508.11255", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11255", "abs": "https://arxiv.org/abs/2508.11255", "authors": ["MengChao Wang", "Qiang Wang", "Fan Jiang", "Mu Xu"], "title": "FantasyTalking2: Timestep-Layer Adaptive Preference Optimization for Audio-Driven Portrait Animation", "comment": "https://fantasy-amap.github.io/fantasy-talking2/", "summary": "Recent advances in audio-driven portrait animation have demonstrated\nimpressive capabilities. However, existing methods struggle to align with\nfine-grained human preferences across multiple dimensions, such as motion\nnaturalness, lip-sync accuracy, and visual quality. This is due to the\ndifficulty of optimizing among competing preference objectives, which often\nconflict with one another, and the scarcity of large-scale, high-quality\ndatasets with multidimensional preference annotations. To address these, we\nfirst introduce Talking-Critic, a multimodal reward model that learns\nhuman-aligned reward functions to quantify how well generated videos satisfy\nmultidimensional expectations. Leveraging this model, we curate Talking-NSQ, a\nlarge-scale multidimensional human preference dataset containing 410K\npreference pairs. Finally, we propose Timestep-Layer adaptive multi-expert\nPreference Optimization (TLPO), a novel framework for aligning diffusion-based\nportrait animation models with fine-grained, multidimensional preferences. TLPO\ndecouples preferences into specialized expert modules, which are then fused\nacross timesteps and network layers, enabling comprehensive, fine-grained\nenhancement across all dimensions without mutual interference. Experiments\ndemonstrate that Talking-Critic significantly outperforms existing methods in\naligning with human preference ratings. Meanwhile, TLPO achieves substantial\nimprovements over baseline models in lip-sync accuracy, motion naturalness, and\nvisual quality, exhibiting superior performance in both qualitative and\nquantitative evaluations. Ours project page:\nhttps://fantasy-amap.github.io/fantasy-talking2/", "AI": {"tldr": "本文提出了一种多模态奖励模型Talking-Critic、一个大规模多维度人类偏好数据集Talking-NSQ以及一种新颖的优化框架TLPO，旨在提升音频驱动肖像动画在自然度、唇形同步和视觉质量方面的表现，使其更好地符合人类的精细偏好。", "motivation": "现有音频驱动肖像动画方法难以满足人类在运动自然度、唇形同步准确性和视觉质量等多维度上的精细偏好，主要原因是难以在相互冲突的偏好目标之间进行优化，并且缺乏大规模、高质量的多维度偏好标注数据集。", "method": "1. **Talking-Critic**: 引入一个多模态奖励模型，学习人类对视频质量的多维度期望，并量化生成视频的满足程度。2. **Talking-NSQ**: 利用Talking-Critic模型，构建了一个包含410K偏好对的大规模多维度人类偏好数据集。3. **TLPO (Timestep-Layer adaptive multi-expert Preference Optimization)**: 提出一种新颖的框架，用于将基于扩散的肖像动画模型与精细、多维度的偏好对齐。TLPO将偏好解耦为专业的专家模块，并在时间步和网络层之间进行融合，从而在所有维度上实现全面、精细的增强，避免相互干扰。", "result": "实验证明，Talking-Critic在与人类偏好评分对齐方面显著优于现有方法。同时，TLPO在唇形同步准确性、运动自然度和视觉质量方面比基线模型有显著提升，在定性和定量评估中均表现出卓越的性能。", "conclusion": "本研究通过引入Talking-Critic奖励模型、构建Talking-NSQ数据集和开发TLPO优化框架，有效解决了音频驱动肖像动画中难以满足多维度人类精细偏好的问题，显著提升了生成视频的自然度和质量。"}}
{"id": "2508.11265", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11265", "abs": "https://arxiv.org/abs/2508.11265", "authors": ["Pei He", "Lingling Li", "Licheng Jiao", "Ronghua Shang", "Fang Liu", "Shuang Wang", "Xu Liu", "Wenping Ma"], "title": "Domain-aware Category-level Geometry Learning Segmentation for 3D Point Clouds", "comment": "to be published in International Conference on Computer Vision, ICCV\n  2025", "summary": "Domain generalization in 3D segmentation is a critical challenge in deploying\nmodels to unseen environments. Current methods mitigate the domain shift by\naugmenting the data distribution of point clouds. However, the model learns\nglobal geometric patterns in point clouds while ignoring the category-level\ndistribution and alignment. In this paper, a category-level geometry learning\nframework is proposed to explore the domain-invariant geometric features for\ndomain generalized 3D semantic segmentation. Specifically, Category-level\nGeometry Embedding (CGE) is proposed to perceive the fine-grained geometric\nproperties of point cloud features, which constructs the geometric properties\nof each class and couples geometric embedding to semantic learning. Secondly,\nGeometric Consistent Learning (GCL) is proposed to simulate the latent 3D\ndistribution and align the category-level geometric embeddings, allowing the\nmodel to focus on the geometric invariant information to improve\ngeneralization. Experimental results verify the effectiveness of the proposed\nmethod, which has very competitive segmentation accuracy compared with the\nstate-of-the-art domain generalized point cloud methods.", "AI": {"tldr": "本文提出了一种类别级几何学习框架，通过感知和对齐类别级几何特征，解决3D分割中的域泛化问题，从而提高模型在未知环境下的泛化能力。", "motivation": "当前3D分割域泛化方法通过数据增强来缓解域偏移，但往往忽略了类别级分布和对齐，导致模型学习到的全局几何模式在未见环境中泛化能力不足。", "method": "提出了一个类别级几何学习框架。具体包括：1. 类别级几何嵌入（CGE）：感知点云特征的细粒度几何属性，构建各类别几何属性并将其与语义学习耦合。2. 几何一致性学习（GCL）：模拟潜在的3D分布并对齐类别级几何嵌入，使模型关注几何不变信息以提高泛化性。", "result": "实验结果验证了所提方法的有效性，与现有最先进的域泛化点云方法相比，具有极具竞争力的分割精度。", "conclusion": "所提出的类别级几何学习框架通过探索域不变的几何特征，有效提升了3D语义分割的域泛化性能，为在未知环境中部署模型提供了新的解决方案。"}}
{"id": "2508.11277", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11277", "abs": "https://arxiv.org/abs/2508.11277", "authors": ["Matthew Lyle Olson", "Musashi Hinck", "Neale Ratzlaff", "Changbai Li", "Phillip Howard", "Vasudev Lal", "Shao-Yen Tseng"], "title": "Probing the Representational Power of Sparse Autoencoders in Vision Models", "comment": "ICCV 2025 Findings", "summary": "Sparse Autoencoders (SAEs) have emerged as a popular tool for interpreting\nthe hidden states of large language models (LLMs). By learning to reconstruct\nactivations from a sparse bottleneck layer, SAEs discover interpretable\nfeatures from the high-dimensional internal representations of LLMs. Despite\ntheir popularity with language models, SAEs remain understudied in the visual\ndomain. In this work, we provide an extensive evaluation the representational\npower of SAEs for vision models using a broad range of image-based tasks. Our\nexperimental results demonstrate that SAE features are semantically meaningful,\nimprove out-of-distribution generalization, and enable controllable generation\nacross three vision model architectures: vision embedding models, multi-modal\nLMMs and diffusion models. In vision embedding models, we find that learned SAE\nfeatures can be used for OOD detection and provide evidence that they recover\nthe ontological structure of the underlying model. For diffusion models, we\ndemonstrate that SAEs enable semantic steering through text encoder\nmanipulation and develop an automated pipeline for discovering\nhuman-interpretable attributes. Finally, we conduct exploratory experiments on\nmulti-modal LLMs, finding evidence that SAE features reveal shared\nrepresentations across vision and language modalities. Our study provides a\nfoundation for SAE evaluation in vision models, highlighting their strong\npotential improving interpretability, generalization, and steerability in the\nvisual domain.", "AI": {"tldr": "本文对稀疏自编码器（SAEs）在视觉模型中的表示能力进行了广泛评估，发现其特征具有语义意义，能提升泛化能力，并实现可控生成，为SAEs在视觉领域的应用奠定了基础。", "motivation": "稀疏自编码器（SAEs）已成为解释大型语言模型（LLMs）隐藏状态的流行工具，但其在视觉领域的研究相对不足，因此需要对其在视觉模型中的表示能力进行深入评估。", "method": "研究通过一系列基于图像的任务，对SAEs在三种视觉模型架构（视觉嵌入模型、多模态LMMs和扩散模型）中的表现进行了广泛评估。具体方法包括：使用SAE特征进行OOD检测、通过文本编码器操作实现扩散模型的语义操控、开发自动化管道发现可解释属性，并探索多模态LLMs中的共享表示。", "result": "实验结果表明，SAE特征具有语义意义，能改善模型在分布外（OOD）的泛化能力，并实现可控生成。在视觉嵌入模型中，SAE特征可用于OOD检测并恢复模型本体结构；在扩散模型中，SAEs能通过文本编码器操作实现语义操控，并发现人类可解释属性；在多模态LLMs中，SAE特征揭示了视觉和语言模态间的共享表示。", "conclusion": "本研究为SAEs在视觉模型中的评估提供了基础，强调了其在提高视觉领域的可解释性、泛化能力和可控性方面的巨大潜力。"}}
{"id": "2508.11282", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11282", "abs": "https://arxiv.org/abs/2508.11282", "authors": ["Muzammil Khan", "Enzo Kerkhof", "Matteo Fusaglia", "Koert Kuhlmann", "Theo Ruers", "Françoise J. Siepel"], "title": "Unifying Scale-Aware Depth Prediction and Perceptual Priors for Monocular Endoscope Pose Estimation and Tissue Reconstruction", "comment": "18 pages, 8 figures, 3 Tables, submitted to IEEE Access for review", "summary": "Accurate endoscope pose estimation and 3D tissue surface reconstruction\nsignificantly enhances monocular minimally invasive surgical procedures by\nenabling accurate navigation and improved spatial awareness. However, monocular\nendoscope pose estimation and tissue reconstruction face persistent challenges,\nincluding depth ambiguity, physiological tissue deformation, inconsistent\nendoscope motion, limited texture fidelity, and a restricted field of view. To\novercome these limitations, a unified framework for monocular endoscopic tissue\nreconstruction that integrates scale-aware depth prediction with\ntemporally-constrained perceptual refinement is presented. This framework\nincorporates a novel MAPIS-Depth module, which leverages Depth Pro for robust\ninitialisation and Depth Anything for efficient per-frame depth prediction, in\nconjunction with L-BFGS-B optimisation, to generate pseudo-metric depth\nestimates. These estimates are temporally refined by computing pixel\ncorrespondences using RAFT and adaptively blending flow-warped frames based on\nLPIPS perceptual similarity, thereby reducing artefacts arising from\nphysiological tissue deformation and motion. To ensure accurate registration of\nthe synthesised pseudo-RGBD frames from MAPIS-Depth, a novel WEMA-RTDL module\nis integrated, optimising both rotation and translation. Finally, truncated\nsigned distance function-based volumetric fusion and marching cubes are applied\nto extract a comprehensive 3D surface mesh. Evaluations on HEVD and SCARED,\nwith ablation and comparative analyses, demonstrate the framework's robustness\nand superiority over state-of-the-art methods.", "AI": {"tldr": "本文提出一个统一框架，用于单目内窥镜组织重建，结合尺度感知深度预测和时序受限感知优化，以克服单目内窥镜面临的挑战，并实现准确的三维表面重建。", "motivation": "单目内窥镜姿态估计和组织重建面临深度模糊、组织变形、运动不一致、纹理有限和视野受限等挑战，这些问题限制了微创手术中的精确导航和空间感知能力。", "method": "该框架包含：1) MAPIS-Depth模块，利用Depth Pro初始化和Depth Anything进行逐帧深度预测，结合L-BFGS-B优化生成伪度量深度估计；2) 通过RAFT计算像素对应关系，并基于LPIPS感知相似性自适应融合流扭曲帧，进行时序深度优化；3) 集成WEMA-RTDL模块，优化伪RGBD帧的旋转和平移以确保精确配准；4) 应用截断符号距离函数（TSDF）的体素融合和Marching Cubes提取三维表面网格。", "result": "在HEVD和SCARED数据集上的评估，包括消融和比较分析，证明了该框架的鲁棒性，并优于现有的最先进方法。", "conclusion": "该统一框架通过整合尺度感知深度预测和时序受限感知优化，显著提升了单目内窥镜姿态估计和三维组织表面重建的准确性，从而增强了微创手术中的导航能力和空间感知。"}}
{"id": "2508.11284", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11284", "abs": "https://arxiv.org/abs/2508.11284", "authors": ["Yilin Mi", "Qixin Yan", "Zheng-Peng Duan", "Chunle Guo", "Hubery Yin", "Hao Liu", "Chen Li", "Chongyi Li"], "title": "TimeMachine: Fine-Grained Facial Age Editing with Identity Preservation", "comment": null, "summary": "With the advancement of generative models, facial image editing has made\nsignificant progress. However, achieving fine-grained age editing while\npreserving personal identity remains a challenging task.In this paper, we\npropose TimeMachine, a novel diffusion-based framework that achieves accurate\nage editing while keeping identity features unchanged. To enable fine-grained\nage editing, we inject high-precision age information into the multi-cross\nattention module, which explicitly separates age-related and identity-related\nfeatures. This design facilitates more accurate disentanglement of age\nattributes, thereby allowing precise and controllable manipulation of facial\naging.Furthermore, we propose an Age Classifier Guidance (ACG) module that\npredicts age directly in the latent space, instead of performing denoising\nimage reconstruction during training. By employing a lightweight module to\nincorporate age constraints, this design enhances age editing accuracy by\nmodest increasing training cost. Additionally, to address the lack of\nlarge-scale, high-quality facial age datasets, we construct a HFFA dataset\n(High-quality Fine-grained Facial-Age dataset) which contains one million\nhigh-resolution images labeled with identity and facial attributes.\nExperimental results demonstrate that TimeMachine achieves state-of-the-art\nperformance in fine-grained age editing while preserving identity consistency.", "AI": {"tldr": "本文提出TimeMachine，一个基于扩散模型的新框架，用于实现精细化面部年龄编辑，同时保持个人身份不变，并通过构建新数据集解决了数据不足问题。", "motivation": "尽管生成模型已取得显著进展，但在实现精细化年龄编辑的同时保持个人身份不变仍然是一个挑战。", "method": "1. 提出TimeMachine，一个扩散模型框架。2. 在多交叉注意力模块中注入高精度年龄信息，显式分离年龄和身份特征，以实现更精确的年龄属性解耦。3. 提出年龄分类器引导（ACG）模块，在潜在空间直接预测年龄，以较低的训练成本提高年龄编辑精度。4. 构建了HFFA数据集，一个包含一百万张高分辨率图像的大规模、高质量面部年龄数据集。", "result": "实验结果表明，TimeMachine在精细化年龄编辑方面达到了最先进的性能，同时保持了身份一致性。", "conclusion": "TimeMachine框架通过其新颖的设计和新构建的数据集，成功解决了面部精细化年龄编辑中身份保持的难题，取得了卓越的性能。"}}
{"id": "2508.11301", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11301", "abs": "https://arxiv.org/abs/2508.11301", "authors": ["Jiarong Li", "Imad Ali Shah", "Enda Ward", "Martin Glavin", "Edward Jones", "Brian Deegan"], "title": "Hyperspectral vs. RGB for Pedestrian Segmentation in Urban Driving Scenes: A Comparative Study", "comment": "Submitted to IEEE ICVES, July, 2025", "summary": "Pedestrian segmentation in automotive perception systems faces critical\nsafety challenges due to metamerism in RGB imaging, where pedestrians and\nbackgrounds appear visually indistinguishable.. This study investigates the\npotential of hyperspectral imaging (HSI) for enhanced pedestrian segmentation\nin urban driving scenarios using the Hyperspectral City v2 (H-City) dataset. We\ncompared standard RGB against two dimensionality-reduction approaches by\nconverting 128-channel HSI data into three-channel representations: Principal\nComponent Analysis (PCA) and optimal band selection using Contrast\nSignal-to-Noise Ratio with Joint Mutual Information Maximization (CSNR-JMIM).\nThree semantic segmentation models were evaluated: U-Net, DeepLabV3+, and\nSegFormer. CSNR-JMIM consistently outperformed RGB with an average improvements\nof 1.44% in Intersection over Union (IoU) and 2.18% in F1-score for pedestrian\nsegmentation. Rider segmentation showed similar gains with 1.43% IoU and 2.25%\nF1-score improvements. These improved performance results from enhanced\nspectral discrimination of optimally selected HSI bands effectively reducing\nfalse positives. This study demonstrates robust pedestrian segmentation through\noptimal HSI band selection, showing significant potential for safety-critical\nautomotive applications.", "AI": {"tldr": "本研究利用高光谱成像（HSI）及其优化波段选择方法，显著提升了汽车感知系统中行人和骑行者的分割性能，有效解决了RGB图像中的同色异谱问题。", "motivation": "在汽车感知系统中，RGB图像中的同色异谱现象导致行人和背景在视觉上难以区分，对行人分割构成严重安全挑战。", "method": "研究使用了Hyperspectral City v2 (H-City) 数据集，将128通道的HSI数据转换为3通道表示，并与标准RGB进行比较。转换方法包括主成分分析（PCA）和使用对比度信噪比与联合互信息最大化（CSNR-JMIM）的最优波段选择。评估了三种语义分割模型：U-Net、DeepLabV3+和SegFormer。", "result": "CSNR-JMIM方法在行人分割方面持续优于RGB，IoU平均提升1.44%，F1分数平均提升2.18%。骑行者分割也取得了类似增益，IoU提升1.43%，F1分数提升2.25%。性能提升归因于优化选择的HSI波段增强了光谱辨别能力，有效减少了误报。", "conclusion": "通过最优HSI波段选择，实现了鲁棒的行人分割，展示了高光谱成像在安全关键型汽车应用中的巨大潜力。"}}
{"id": "2508.11313", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11313", "abs": "https://arxiv.org/abs/2508.11313", "authors": ["Weijia Liu", "Jiuxin Cao", "Bo Miao", "Zhiheng Fu", "Xuelin Zhu", "Jiawei Ge", "Bo Liu", "Mehwish Nasim", "Ajmal Mian"], "title": "Denoise-then-Retrieve: Text-Conditioned Video Denoising for Video Moment Retrieval", "comment": "Accepted by IJCAI 2025", "summary": "Current text-driven Video Moment Retrieval (VMR) methods encode all video\nclips, including irrelevant ones, disrupting multimodal alignment and hindering\noptimization. To this end, we propose a denoise-then-retrieve paradigm that\nexplicitly filters text-irrelevant clips from videos and then retrieves the\ntarget moment using purified multimodal representations. Following this\nparadigm, we introduce the Denoise-then-Retrieve Network (DRNet), comprising\nText-Conditioned Denoising (TCD) and Text-Reconstruction Feedback (TRF)\nmodules. TCD integrates cross-attention and structured state space blocks to\ndynamically identify noisy clips and produce a noise mask to purify multimodal\nvideo representations. TRF further distills a single query embedding from\npurified video representations and aligns it with the text embedding, serving\nas auxiliary supervision for denoising during training. Finally, we perform\nconditional retrieval using text embeddings on purified video representations\nfor accurate VMR. Experiments on Charades-STA and QVHighlights demonstrate that\nour approach surpasses state-of-the-art methods on all metrics. Furthermore,\nour denoise-then-retrieve paradigm is adaptable and can be seamlessly\nintegrated into advanced VMR models to boost performance.", "AI": {"tldr": "提出了一种“去噪再检索”的新范式，通过显式过滤视频中与文本无关的片段来提升文本驱动的视频时刻检索（VMR）性能，并引入了DRNet模型实现该范式。", "motivation": "现有文本驱动的视频时刻检索方法编码所有视频片段（包括不相关的），这会破坏多模态对齐并阻碍优化。", "method": "提出“去噪再检索”范式，并引入DRNet网络。DRNet包含两个模块：1) 文本条件去噪（TCD）模块，利用交叉注意力机制和结构化状态空间块动态识别噪声片段并生成噪声掩码，以净化多模态视频表示。2) 文本重建反馈（TRF）模块，从净化后的视频表示中提取单一查询嵌入，并将其与文本嵌入对齐，作为去噪过程的辅助监督。最终，在净化后的视频表示上进行条件检索。", "result": "在Charades-STA和QVHighlights数据集上，该方法在所有指标上均超越了现有最先进的方法。此外，所提出的“去噪再检索”范式具有适应性，可以无缝集成到其他高级VMR模型中以提升性能。", "conclusion": "通过在检索前显式地过滤文本不相关片段，所提出的“去噪再检索”范式及其DRNet模型显著提升了文本驱动的视频时刻检索的准确性和性能，并具有良好的通用性。"}}
{"id": "2508.11317", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.11317", "abs": "https://arxiv.org/abs/2508.11317", "authors": ["Yuchen Zhou", "Jiayu Tang", "Shuo Yang", "Xiaoyan Xiao", "Yuqin Dai", "Wenhao Yang", "Chao Gou", "Xiaobo Xia", "Tat-Seng Chua"], "title": "Logic Unseen: Revealing the Logical Blindspots of Vision-Language Models", "comment": null, "summary": "Vision-Language Models (VLMs), exemplified by CLIP, have emerged as\nfoundational for multimodal intelligence. However, their capacity for logical\nunderstanding remains significantly underexplored, resulting in critical\n''logical blindspots'' that limit their reliability in practical applications.\nTo systematically diagnose this, we introduce LogicBench, a comprehensive\nbenchmark with over 50,000 vision-language pairs across 9 logical categories\nand 4 diverse scenarios: images, videos, anomaly detection, and medical\ndiagnostics. Our evaluation reveals that existing VLMs, even the\nstate-of-the-art ones, fall at over 40 accuracy points below human performance,\nparticularly in challenging tasks like Causality and Conditionality,\nhighlighting their reliance on surface semantics over critical logical\nstructures. To bridge this gap, we propose LogicCLIP, a novel training\nframework designed to boost VLMs' logical sensitivity through advancements in\nboth data generation and optimization objectives. LogicCLIP utilizes\nlogic-aware data generation and a contrastive learning strategy that combines\ncoarse-grained alignment, a fine-grained multiple-choice objective, and a novel\nlogical structure-aware objective. Extensive experiments demonstrate\nLogicCLIP's substantial improvements in logical comprehension across all\nLogicBench domains, significantly outperforming baselines. Moreover, LogicCLIP\nretains, and often surpasses, competitive performance on general\nvision-language benchmarks, demonstrating that the enhanced logical\nunderstanding does not come at the expense of general alignment. We believe\nthat LogicBench and LogicCLIP will be important resources for advancing VLM\nlogical capabilities.", "AI": {"tldr": "现有视觉-语言模型（VLMs）在逻辑理解方面存在显著缺陷，本文引入了LogicBench基准来诊断这些“逻辑盲点”，并提出了LogicCLIP训练框架来增强VLMs的逻辑敏感性，LogicCLIP在逻辑理解和通用VL任务上均表现出色。", "motivation": "尽管视觉-语言模型（VLMs）如CLIP在多模态智能方面已成为基础，但它们对逻辑理解的能力尚未得到充分探索，导致存在严重的“逻辑盲点”，这限制了它们在实际应用中的可靠性。", "method": "本文提出了LogicBench，一个包含超过50,000个视觉-语言对的综合基准，涵盖9个逻辑类别和4种场景（图像、视频、异常检测、医疗诊断），用于系统诊断VLMs的逻辑理解能力。为弥补差距，提出了LogicCLIP，一个新颖的训练框架，通过改进数据生成和优化目标来提升VLMs的逻辑敏感性。LogicCLIP利用逻辑感知数据生成和一种结合粗粒度对齐、细粒度多项选择目标以及新颖的逻辑结构感知目标的对比学习策略。", "result": "评估显示，现有最先进的VLMs在LogicBench上的准确率比人类表现低40多个百分点，尤其在因果关系和条件性等挑战性任务上表现不佳，这表明它们过度依赖表面语义而非关键逻辑结构。LogicCLIP在LogicBench的所有领域都显著提升了逻辑理解能力，显著优于基线模型。此外，LogicCLIP在通用视觉-语言基准测试中保持甚至超越了竞争性能，表明增强逻辑理解并未牺牲通用对齐能力。", "conclusion": "LogicBench和LogicCLIP将成为推动VLM逻辑能力发展的重要资源。"}}
{"id": "2508.11323", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11323", "abs": "https://arxiv.org/abs/2508.11323", "authors": ["Haonan Zhang", "Xinyao Wang", "Boxi Wu", "Tu Zheng", "Wang Yunhua", "Zheng Yang"], "title": "Delving into Dynamic Scene Cue-Consistency for Robust 3D Multi-Object Tracking", "comment": null, "summary": "3D multi-object tracking is a critical and challenging task in the field of\nautonomous driving. A common paradigm relies on modeling individual object\nmotion, e.g., Kalman filters, to predict trajectories. While effective in\nsimple scenarios, this approach often struggles in crowded environments or with\ninaccurate detections, as it overlooks the rich geometric relationships between\nobjects. This highlights the need to leverage spatial cues. However, existing\ngeometry-aware methods can be susceptible to interference from irrelevant\nobjects, leading to ambiguous features and incorrect associations. To address\nthis, we propose focusing on cue-consistency: identifying and matching stable\nspatial patterns over time. We introduce the Dynamic Scene Cue-Consistency\nTracker (DSC-Track) to implement this principle. Firstly, we design a unified\nspatiotemporal encoder using Point Pair Features (PPF) to learn discriminative\ntrajectory embeddings while suppressing interference. Secondly, our\ncue-consistency transformer module explicitly aligns consistent feature\nrepresentations between historical tracks and current detections. Finally, a\ndynamic update mechanism preserves salient spatiotemporal information for\nstable online tracking. Extensive experiments on the nuScenes and Waymo Open\nDatasets validate the effectiveness and robustness of our approach. On the\nnuScenes benchmark, for instance, our method achieves state-of-the-art\nperformance, reaching 73.2% and 70.3% AMOTA on the validation and test sets,\nrespectively.", "AI": {"tldr": "提出DSC-Track，通过关注线索一致性，利用统一时空编码器和Transformer模块，解决3D多目标跟踪中拥挤环境和不准确检测的挑战，实现最先进性能。", "motivation": "传统的3D多目标跟踪方法（如卡尔曼滤波）在拥挤环境或检测不准确时表现不佳，因为它忽略了对象间的几何关系。现有几何感知方法易受无关对象干扰，导致特征模糊和错误关联。因此，需要有效利用并保持空间线索的一致性。", "method": "提出动态场景线索一致性跟踪器（DSC-Track）。首先，设计统一的时空编码器，利用点对特征（PPF）学习判别性轨迹嵌入并抑制干扰。其次，使用线索一致性Transformer模块显式对齐历史轨迹和当前检测之间一致的特征表示。最后，通过动态更新机制保留显著的时空信息以实现稳定的在线跟踪。", "result": "在nuScenes和Waymo Open数据集上进行了广泛实验，验证了方法的有效性和鲁棒性。在nuScenes基准测试中，其在验证集和测试集上分别达到73.2%和70.3%的AMOTA，实现了最先进的性能。", "conclusion": "DSC-Track通过创新的线索一致性原则和模块设计，成功克服了3D多目标跟踪在复杂环境中的挑战，显著提升了跟踪性能和鲁棒性，达到了领域内领先水平。"}}
{"id": "2508.11330", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11330", "abs": "https://arxiv.org/abs/2508.11330", "authors": ["Yanghao Wang", "Long Chen"], "title": "Noise Matters: Optimizing Matching Noise for Diffusion Classifiers", "comment": null, "summary": "Although today's pretrained discriminative vision-language models (e.g.,\nCLIP) have demonstrated strong perception abilities, such as zero-shot image\nclassification, they also suffer from the bag-of-words problem and spurious\nbias. To mitigate these problems, some pioneering studies leverage powerful\ngenerative models (e.g., pretrained diffusion models) to realize generalizable\nimage classification, dubbed Diffusion Classifier (DC). Specifically, by\nrandomly sampling a Gaussian noise, DC utilizes the differences of denoising\neffects with different category conditions to classify categories.\nUnfortunately, an inherent and notorious weakness of existing DCs is noise\ninstability: different random sampled noises lead to significant performance\nchanges. To achieve stable classification performance, existing DCs always\nensemble the results of hundreds of sampled noises, which significantly reduces\nthe classification speed. To this end, we firstly explore the role of noise in\nDC, and conclude that: there are some ``good noises'' that can relieve the\ninstability. Meanwhile, we argue that these good noises should meet two\nprinciples: Frequency Matching and Spatial Matching. Regarding both principles,\nwe propose a novel Noise Optimization method to learn matching (i.e., good)\nnoise for DCs: NoOp. For frequency matching, NoOp first optimizes a\ndataset-specific noise: Given a dataset and a timestep t, optimize one randomly\ninitialized parameterized noise. For Spatial Matching, NoOp trains a\nMeta-Network that adopts an image as input and outputs image-specific noise\noffset. The sum of optimized noise and noise offset will be used in DC to\nreplace random noise. Extensive ablations on various datasets demonstrated the\neffectiveness of NoOp.", "AI": {"tldr": "本文提出NoOp方法，通过优化“好噪声”来解决扩散分类器（DC）中的噪声不稳定性问题，从而提高分类性能和速度。", "motivation": "现有判别式视觉-语言模型（如CLIP）存在词袋问题和虚假偏差。虽然扩散分类器（DC）能缓解这些问题，但其固有的噪声不稳定性导致性能波动大，现有方法需集成大量随机噪声，显著降低了分类速度。", "method": "作者首先探究了噪声在DC中的作用，发现存在能缓解不稳定性的“好噪声”。这些好噪声应满足“频率匹配”和“空间匹配”两个原则。基于此，提出了一种新颖的噪声优化方法NoOp。对于频率匹配，NoOp优化一个数据集特定的参数化噪声；对于空间匹配，NoOp训练一个元网络，根据输入图像输出图像特定的噪声偏移。最终将优化后的噪声和噪声偏移之和用于DC代替随机噪声。", "result": "在各种数据集上进行的大量消融实验证明了NoOp方法的有效性。", "conclusion": "NoOp通过优化符合特定原则的“好噪声”，有效解决了扩散分类器（DC）的噪声不稳定性问题，从而提高了分类性能和效率。"}}
{"id": "2508.11334", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11334", "abs": "https://arxiv.org/abs/2508.11334", "authors": ["Md Asgor Hossain Reaj", "Rajan Das Gupta", "Md Yeasin Rahat", "Nafiz Fahad", "Md Jawadul Hasan", "Tze Hui Liew"], "title": "GANDiff FR: Hybrid GAN Diffusion Synthesis for Causal Bias Attribution in Face Recognition", "comment": "Accepted in ICCVDM '25", "summary": "We introduce GANDiff FR, the first synthetic framework that precisely\ncontrols demographic and environmental factors to measure, explain, and reduce\nbias with reproducible rigor. GANDiff FR unifies StyleGAN3-based\nidentity-preserving generation with diffusion-based attribute control, enabling\nfine-grained manipulation of pose around 30 degrees, illumination (four\ndirections), and expression (five levels) under ceteris paribus conditions. We\nsynthesize 10,000 demographically balanced faces across five cohorts validated\nfor realism via automated detection (98.2%) and human review (89%) to isolate\nand quantify bias drivers. Benchmarking ArcFace, CosFace, and AdaFace under\nmatched operating points shows AdaFace reduces inter-group TPR disparity by 60%\n(2.5% vs. 6.3%), with illumination accounting for 42% of residual bias.\nCross-dataset evaluation on RFW, BUPT, and CASIA WebFace confirms strong\nsynthetic-to-real transfer (r 0.85). Despite around 20% computational overhead\nrelative to pure GANs, GANDiff FR yields three times more attribute-conditioned\nvariants, establishing a reproducible, regulation-aligned (EU AI Act) standard\nfor fairness auditing. Code and data are released to support transparent,\nscalable bias evaluation.", "AI": {"tldr": "GANDiff FR是一个新颖的合成框架，它通过精确控制人口统计和环境因素来测量、解释和减少面部识别中的偏差，并提供了可复现的严格性。", "motivation": "现有方法难以精确控制人口统计和环境因素来系统地测量、解释和减少面部识别（FR）系统中的偏差。缺乏能够隔离和量化偏差驱动因素的受控、可复现的数据集。", "method": "GANDiff FR结合了基于StyleGAN3的身份保留生成和基于扩散的属性控制。它能够细粒度地操纵姿态（约30度）、光照（四个方向）和表情（五个级别），并在其他条件不变的情况下合成10,000张人口统计平衡的面孔。这些面孔通过自动化检测和人工评估验证了真实性。", "result": "通过GANDiff FR合成的数据集，AdaFace在匹配操作点下将组间TPR差异降低了60%（从6.3%降至2.5%），其中光照占剩余偏差的42%。跨数据集评估（RFW, BUPT, CASIA WebFace）证实了合成数据到真实数据的强迁移性（r 0.85）。尽管计算开销比纯GAN高约20%，但GANDiff FR能生成三倍多的属性条件变体。", "conclusion": "GANDiff FR为面部识别公平性审计建立了一个可复现、符合法规（欧盟AI法案）的标准，通过精确控制合成数据来有效测量、解释和减少偏差。其代码和数据已发布，以支持透明和可扩展的偏差评估。"}}
{"id": "2508.11339", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11339", "abs": "https://arxiv.org/abs/2508.11339", "authors": ["Mingxiao Ma", "Shunyao Zhu", "Guoliang Kang"], "title": "Index-Aligned Query Distillation for Transformer-based Incremental Object Detection", "comment": "12 pages, 5 figures", "summary": "Incremental object detection (IOD) aims to continuously expand the capability\nof a model to detect novel categories while preserving its performance on\npreviously learned ones. When adopting a transformer-based detection model to\nperform IOD, catastrophic knowledge forgetting may inevitably occur, meaning\nthe detection performance on previously learned categories may severely\ndegenerate. Previous typical methods mainly rely on knowledge distillation (KD)\nto mitigate the catastrophic knowledge forgetting of transformer-based\ndetection models. Specifically, they utilize Hungarian Matching to build a\ncorrespondence between the queries of the last-phase and current-phase\ndetection models and align the classifier and regressor outputs between matched\nqueries to avoid knowledge forgetting. However, we observe that in IOD task,\nHungarian Matching is not a good choice. With Hungarian Matching, the query of\nthe current-phase model may match different queries of the last-phase model at\ndifferent iterations during KD. As a result, the knowledge encoded in each\nquery may be reshaped towards new categories, leading to the forgetting of\npreviously encoded knowledge of old categories. Based on our observations, we\npropose a new distillation approach named Index-Aligned Query Distillation\n(IAQD) for transformer-based IOD. Beyond using Hungarian Matching, IAQD\nestablishes a correspondence between queries of the previous and current phase\nmodels that have the same index. Moreover, we perform index-aligned\ndistillation only on partial queries which are critical for the detection of\nprevious categories. In this way, IAQD largely preserves the previous semantic\nand spatial encoding capabilities without interfering with the learning of new\ncategories. Extensive experiments on representative benchmarks demonstrate that\nIAQD effectively mitigates knowledge forgetting, achieving new state-of-the-art\nperformance.", "AI": {"tldr": "本文提出了一种名为索引对齐查询蒸馏（IAQD）的新方法，用于基于Transformer的增量目标检测（IOD），以解决灾难性遗忘问题。IAQD通过索引而非匈牙利匹配来对齐新旧模型查询，并仅对部分关键查询进行蒸馏，从而有效保留旧知识并学习新知识。", "motivation": "在基于Transformer的增量目标检测（IOD）中，模型在学习新类别时，对先前学习类别的检测性能会严重退化，即发生灾难性遗忘。现有知识蒸馏（KD）方法常依赖匈牙利匹配来对齐查询，但作者观察到这种匹配方式在不同迭代中可能导致查询匹配不一致，使旧知识被重塑以适应新类别，从而加剧遗忘。", "method": "提出索引对齐查询蒸馏（IAQD）方法。与匈牙利匹配不同，IAQD通过查询的相同索引来建立新旧模型查询之间的对应关系。此外，IAQD仅对部分对先前类别检测至关重要的查询进行索引对齐蒸馏，旨在在不干扰新类别学习的同时，最大程度地保留旧的语义和空间编码能力。", "result": "在代表性基准测试上的大量实验表明，IAQD有效地缓解了知识遗忘，并取得了新的最先进性能。", "conclusion": "IAQD是一种针对基于Transformer的增量目标检测的有效蒸馏方法，它通过创新的查询对齐和选择性蒸馏策略，成功解决了灾难性遗忘问题，实现了对旧知识的有效保留和新知识的有效学习。"}}
{"id": "2508.11340", "categories": ["cs.CV", "q-bio.TO"], "pdf": "https://arxiv.org/pdf/2508.11340", "abs": "https://arxiv.org/abs/2508.11340", "authors": ["Yuanlin Liu", "Zhihan Zhou", "Mingqiang Wei", "Youyi Song"], "title": "Cost-Effective Active Labeling for Data-Efficient Cervical Cell Classification", "comment": "accepted by CW2025", "summary": "Information on the number and category of cervical cells is crucial for the\ndiagnosis of cervical cancer. However, existing classification methods capable\nof automatically measuring this information require the training dataset to be\nrepresentative, which consumes an expensive or even unaffordable human cost. We\nherein propose active labeling that enables us to construct a representative\ntraining dataset using a much smaller human cost for data-efficient cervical\ncell classification. This cost-effective method efficiently leverages the\nclassifier's uncertainty on the unlabeled cervical cell images to accurately\nselect images that are most beneficial to label. With a fast estimation of the\nuncertainty, this new algorithm exhibits its validity and effectiveness in\nenhancing the representative ability of the constructed training dataset. The\nextensive empirical results confirm its efficacy again in navigating the usage\nof human cost, opening the avenue for data-efficient cervical cell\nclassification.", "AI": {"tldr": "本文提出一种主动标注方法，通过高效选择最有益的未标注宫颈细胞图像进行标注，以更低的人力成本构建有代表性的训练数据集，从而实现数据高效的宫颈细胞分类。", "motivation": "现有宫颈细胞分类方法需要有代表性的训练数据集，但构建此类数据集通常需要耗费高昂甚至难以承受的人力成本。", "method": "提出主动标注（active labeling）方法，该方法通过快速估计分类器对未标注宫颈细胞图像的不确定性，从而准确选择对标注最有益的图像，以提高构建训练数据集的代表性。", "result": "新算法在增强所构建训练数据集的代表性方面表现出有效性。广泛的实证结果再次证实了其在指导人力成本使用方面的功效。", "conclusion": "该方法为数据高效的宫颈细胞分类开辟了新途径，显著降低了训练数据的人力成本，具有有效性和经济性。"}}
{"id": "2508.11341", "categories": ["cs.CV", "cs.CR", "cs.LG", "68T45, 68T01, 68T07, 68T10, 68M25", "I.2.10; I.5.4; I.2.6; I.2.7; K.6.5"], "pdf": "https://arxiv.org/pdf/2508.11341", "abs": "https://arxiv.org/abs/2508.11341", "authors": ["Katarzyna Filus", "Jorge M. Cruz-Duarte"], "title": "Semantically Guided Adversarial Testing of Vision Models Using Language Models", "comment": "12 pages, 4 figures, 3 tables. Submitted for peer review", "summary": "In targeted adversarial attacks on vision models, the selection of the target\nlabel is a critical yet often overlooked determinant of attack success. This\ntarget label corresponds to the class that the attacker aims to force the model\nto predict. Now, existing strategies typically rely on randomness, model\npredictions, or static semantic resources, limiting interpretability,\nreproducibility, or flexibility. This paper then proposes a semantics-guided\nframework for adversarial target selection using the cross-modal knowledge\ntransfer from pretrained language and vision-language models. We evaluate\nseveral state-of-the-art models (BERT, TinyLLAMA, and CLIP) as similarity\nsources to select the most and least semantically related labels with respect\nto the ground truth, forming best- and worst-case adversarial scenarios. Our\nexperiments on three vision models and five attack methods reveal that these\nmodels consistently render practical adversarial targets and surpass static\nlexical databases, such as WordNet, particularly for distant class\nrelationships. We also observe that static testing of target labels offers a\npreliminary assessment of the effectiveness of similarity sources, \\textit{a\npriori} testing. Our results corroborate the suitability of pretrained models\nfor constructing interpretable, standardized, and scalable adversarial\nbenchmarks across architectures and datasets.", "AI": {"tldr": "该研究提出了一种语义引导的对抗性目标标签选择框架，利用预训练语言模型和视觉-语言模型的跨模态知识迁移，以提高对抗性攻击的可解释性、可重现性和灵活性。", "motivation": "在针对视觉模型的对抗性攻击中，目标标签的选择至关重要但常被忽视。现有策略（随机性、模型预测、静态语义资源）存在可解释性、可重现性或灵活性不足的限制。", "method": "提出了一种语义引导的对抗性目标选择框架，利用预训练语言模型（BERT、TinyLLAMA）和视觉-语言模型（CLIP）进行跨模态知识迁移，选择与真实标签语义上最相关或最不相关的标签，形成最佳和最差情况的对抗性场景。实验在三种视觉模型和五种攻击方法上进行评估，并探索了目标标签的静态测试。", "result": "所提出的方法能够持续生成实用的对抗性目标，并优于静态词汇数据库（如WordNet），尤其是在类别关系较远时。此外，目标标签的静态测试可以作为相似性来源有效性的初步评估。", "conclusion": "预训练模型适合构建跨架构和数据集的可解释、标准化和可扩展的对抗性基准。"}}
{"id": "2508.11350", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11350", "abs": "https://arxiv.org/abs/2508.11350", "authors": ["Zhenhao Zhang", "Hanqing Wang", "Xiangyu Zeng", "Ziyu Cheng", "Jiaxin Liu", "Haoyu Yan", "Zhirui Liu", "Kaiyang Ji", "Tianxiang Gui", "Ke Hu", "Kangyi Chen", "Yahao Fan", "Mokai Pan"], "title": "HOID-R1: Reinforcement Learning for Open-World Human-Object Interaction Detection Reasoning with Multimodal Large Language Model", "comment": null, "summary": "Understanding and recognizing human-object interaction (HOI) is a pivotal\napplication in AR/VR and robotics. Recent open-vocabulary HOI detection\napproaches depend exclusively on large language models for richer textual\nprompts, neglecting their inherent 3D spatial understanding capabilities. To\naddress this shortcoming, we introduce HOID-R1, the first HOI detection\nframework that integrates chain-of-thought (CoT) guided supervised fine-tuning\n(SFT) with group relative policy optimization (GRPO) within a reinforcement\nlearning (RL) paradigm. Specifically, we initially apply SFT to imbue the model\nwith essential reasoning capabilities, forcing the model to articulate its\nthought process in the output. Subsequently, we integrate GRPO to leverage\nmulti-reward signals for policy optimization, thereby enhancing alignment\nacross diverse modalities. To mitigate hallucinations in the CoT reasoning, we\nintroduce an \"MLLM-as-a-judge\" mechanism that supervises the CoT outputs,\nfurther improving generalization. Extensive experiments show that HOID-R1\nachieves state-of-the-art performance on HOI detection benchmarks and\noutperforms existing methods in open-world generalization to novel scenarios.", "AI": {"tldr": "HOID-R1是首个结合思维链（CoT）引导的监督微调（SFT）和强化学习（RL）中的群组相对策略优化（GRPO）的人-物交互（HOI）检测框架，通过多模态对齐和“MLLM作为评判者”机制，显著提升了HOI检测性能和开放世界泛化能力。", "motivation": "现有的开放词汇HOI检测方法过度依赖大型语言模型进行文本提示，却忽视了它们固有的3D空间理解能力。本研究旨在解决这一缺陷。", "method": "引入HOID-R1框架，该框架在强化学习范式下整合了CoT引导的SFT和GRPO。具体而言，首先应用SFT赋予模型推理能力并使其阐明思维过程；其次，整合GRPO利用多奖励信号进行策略优化，增强跨模态对齐；为缓解CoT推理中的幻觉，引入“MLLM作为评判者”机制来监督CoT输出，进一步提高泛化能力。", "result": "HOID-R1在HOI检测基准测试中取得了最先进的性能，并在开放世界泛化到新颖场景方面超越了现有方法。", "conclusion": "HOID-R1成功地将CoT引导的SFT和GRPO整合到强化学习范式中，有效利用了3D空间理解能力并缓解了推理幻觉，从而在HOI检测和开放世界泛化方面取得了显著进步。"}}
{"id": "2508.11376", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11376", "abs": "https://arxiv.org/abs/2508.11376", "authors": ["Durgesh Mishra", "Rishabh Uikey"], "title": "Unified Knowledge Distillation Framework: Fine-Grained Alignment and Geometric Relationship Preservation for Deep Face Recognition", "comment": "The paper spans a total of 14 pages, 10 pages for the main content\n  (including references) and 4 pages for the appendix. The main paper contains\n  3 figures and 1 table, while the appendix includes 1 pseudo-code algorithm\n  and 4 tables. The work was recently accepted for publication at IJCB 2025", "summary": "Knowledge Distillation is crucial for optimizing face recognition models for\ndeployment in computationally limited settings, such as edge devices.\nTraditional KD methods, such as Raw L2 Feature Distillation or Feature\nConsistency loss, often fail to capture both fine-grained instance-level\ndetails and complex relational structures, leading to suboptimal performance.\nWe propose a unified approach that integrates two novel loss functions,\nInstance-Level Embedding Distillation and Relation-Based Pairwise Similarity\nDistillation. Instance-Level Embedding Distillation focuses on aligning\nindividual feature embeddings by leveraging a dynamic hard mining strategy,\nthereby enhancing learning from challenging examples. Relation-Based Pairwise\nSimilarity Distillation captures relational information through pairwise\nsimilarity relationships, employing a memory bank mechanism and a sample mining\nstrategy. This unified framework ensures both effective instance-level\nalignment and preservation of geometric relationships between samples, leading\nto a more comprehensive distillation process. Our unified framework outperforms\nstate-of-the-art distillation methods across multiple benchmark face\nrecognition datasets, as demonstrated by extensive experimental evaluations.\nInterestingly, when using strong teacher networks compared to the student, our\nunified KD enables the student to even surpass the teacher's accuracy.", "AI": {"tldr": "提出一种统一的知识蒸馏（KD）方法，结合实例级嵌入蒸馏和基于关系的成对相似度蒸馏，用于优化人脸识别模型，在计算受限设备上部署，并超越现有SOTA方法，甚至在特定情况下使学生模型超越教师模型。", "motivation": "传统知识蒸馏方法（如Raw L2特征蒸馏或特征一致性损失）在人脸识别领域，难以同时捕获细粒度的实例级细节和复杂的结构关系，导致性能不佳，尤其是在资源受限的边缘设备部署场景中。", "method": "提出一个统一的框架，包含两种新的损失函数：1. 实例级嵌入蒸馏：通过动态难例挖掘策略对齐个体特征嵌入，增强对挑战性样本的学习。2. 基于关系的成对相似度蒸馏：通过成对相似性关系捕获关系信息，采用记忆库机制和样本挖掘策略。该框架确保了有效的实例级对齐和样本间几何关系的保留。", "result": "该统一框架在多个人脸识别基准数据集上超越了最先进的蒸馏方法。有趣的是，当使用强大的教师网络时，学生模型的准确性甚至可以超越教师模型。", "conclusion": "该统一框架通过确保有效的实例级对齐和样本间几何关系的保留，实现了一个更全面的知识蒸馏过程，从而在人脸识别任务中取得了卓越的性能。"}}
{"id": "2508.11409", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11409", "abs": "https://arxiv.org/abs/2508.11409", "authors": ["Zhiming Liu", "Nantheera Anantrasirichai"], "title": "RMFAT: Recurrent Multi-scale Feature Atmospheric Turbulence Mitigator", "comment": null, "summary": "Atmospheric turbulence severely degrades video quality by introducing\ndistortions such as geometric warping, blur, and temporal flickering, posing\nsignificant challenges to both visual clarity and temporal consistency. Current\nstate-of-the-art methods are based on transformer and 3D architectures and\nrequire multi-frame input, but their large computational cost and memory usage\nlimit real-time deployment, especially in resource-constrained scenarios. In\nthis work, we propose RMFAT: Recurrent Multi-scale Feature Atmospheric\nTurbulence Mitigator, designed for efficient and temporally consistent video\nrestoration under AT conditions. RMFAT adopts a lightweight recurrent framework\nthat restores each frame using only two inputs at a time, significantly\nreducing temporal window size and computational burden. It further integrates\nmulti-scale feature encoding and decoding with temporal warping modules at both\nencoder and decoder stages to enhance spatial detail and temporal coherence.\nExtensive experiments on synthetic and real-world atmospheric turbulence\ndatasets demonstrate that RMFAT not only outperforms existing methods in terms\nof clarity restoration (with nearly a 9\\% improvement in SSIM) but also\nachieves significantly improved inference speed (more than a fourfold reduction\nin runtime), making it particularly suitable for real-time atmospheric\nturbulence suppression tasks.", "AI": {"tldr": "本文提出RMFAT，一种轻量级循环多尺度特征网络，旨在高效且时间一致地恢复受大气湍流影响的视频，特别适用于资源受限的实时场景。", "motivation": "大气湍流严重降低视频质量，引入几何扭曲、模糊和时间闪烁。现有先进方法（基于Transformer和3D架构）计算成本和内存占用高，限制了在资源受限场景下的实时部署。", "method": "本文提出RMFAT（Recurrent Multi-scale Feature Atmospheric Turbulence Mitigator），采用轻量级循环框架，每次仅使用两帧输入进行恢复，显著减少时间窗口大小和计算负担。它还在编码器和解码器阶段集成了多尺度特征编码解码与时间扭曲模块，以增强空间细节和时间一致性。", "result": "在合成和真实大气湍流数据集上的实验表明，RMFAT在清晰度恢复方面优于现有方法（SSIM提升近9%），并且推理速度显著提高（运行时长减少四倍以上）。", "conclusion": "RMFAT能够高效且时间一致地抑制大气湍流，使其特别适用于实时大气湍流抑制任务。"}}
{"id": "2508.11411", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11411", "abs": "https://arxiv.org/abs/2508.11411", "authors": ["Fabian H. Reith", "Jannik Franzen", "Dinesh R. Palli", "J. Lorenz Rumberger", "Dagmar Kainmueller"], "title": "SelfAdapt: Unsupervised Domain Adaptation of Cell Segmentation Models", "comment": "8 pages, 3 figures. To appear in the proceedings of the BioImage\n  Computing (BIC) Workshop @ ICCVW 2025. This is the accepted author manuscript\n  (camera-ready version)", "summary": "Deep neural networks have become the go-to method for biomedical instance\nsegmentation. Generalist models like Cellpose demonstrate state-of-the-art\nperformance across diverse cellular data, though their effectiveness often\ndegrades on domains that differ from their training data. While supervised\nfine-tuning can address this limitation, it requires annotated data that may\nnot be readily available. We propose SelfAdapt, a method that enables the\nadaptation of pre-trained cell segmentation models without the need for labels.\nOur approach builds upon student-teacher augmentation consistency training,\nintroducing L2-SP regularization and label-free stopping criteria. We evaluate\nour method on the LiveCell and TissueNet datasets, demonstrating relative\nimprovements in AP0.5 of up to 29.64% over baseline Cellpose. Additionally, we\nshow that our unsupervised adaptation can further improve models that were\npreviously fine-tuned with supervision. We release SelfAdapt as an easy-to-use\nextension of the Cellpose framework. The code for our method is publicly\navailable at https: //github.com/Kainmueller-Lab/self_adapt.", "AI": {"tldr": "SelfAdapt是一种无监督的细胞分割模型自适应方法，通过学生-教师一致性训练、L2-SP正则化和无标签停止标准，显著提升预训练模型在未见过数据上的性能。", "motivation": "深度学习在生物医学实例分割中表现出色，但通用模型（如Cellpose）在训练数据之外的领域性能会下降。监督微调需要大量标注数据，而这些数据通常难以获得。", "method": "提出SelfAdapt方法，它在学生-教师增强一致性训练的基础上，引入L2-SP正则化和无标签停止标准，实现无需标签的预训练细胞分割模型自适应。", "result": "在LiveCell和TissueNet数据集上，相比基线Cellpose，AP0.5相对提高了高达29.64%。此外，该无监督自适应方法还能进一步提升已通过监督微调的模型性能。", "conclusion": "SelfAdapt成功实现了预训练细胞分割模型在无标签情况下的自适应，显著提升了模型在不同数据域上的性能，并作为Cellpose框架的易用扩展发布。"}}
{"id": "2508.11419", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11419", "abs": "https://arxiv.org/abs/2508.11419", "authors": ["Florian Bayer", "Maximilian Russo", "Christian Rathgeb"], "title": "Training-free Dimensionality Reduction via Feature Truncation: Enhancing Efficiency in Privacy-preserving Multi-Biometric Systems", "comment": null, "summary": "Biometric recognition is widely used, making the privacy and security of\nextracted templates a critical concern. Biometric Template Protection schemes,\nespecially those utilizing Homomorphic Encryption, introduce significant\ncomputational challenges due to increased workload. Recent advances in deep\nneural networks have enabled state-of-the-art feature extraction for face,\nfingerprint, and iris modalities. The ubiquity and affordability of biometric\nsensors further facilitate multi-modal fusion, which can enhance security by\ncombining features from different modalities. This work investigates the\nbiometric performance of reduced multi-biometric template sizes. Experiments\nare conducted on an in-house virtual multi-biometric database, derived from\nDNN-extracted features for face, fingerprint, and iris, using the FRGC, MCYT,\nand CASIA databases. The evaluated approaches are (i) explainable and\nstraightforward to implement under encryption, (ii) training-free, and (iii)\ncapable of generalization. Dimensionality reduction of feature vectors leads to\nfewer operations in the Homomorphic Encryption (HE) domain, enabling more\nefficient encrypted processing while maintaining biometric accuracy and\nsecurity at a level equivalent to or exceeding single-biometric recognition.\nOur results demonstrate that, by fusing feature vectors from multiple\nmodalities, template size can be reduced by 67 % with no loss in Equal Error\nRate (EER) compared to the best-performing single modality.", "AI": {"tldr": "该研究通过多模态融合和降维技术，显著减小了生物特征模板大小，从而在同态加密下实现了更高效的隐私保护生物特征识别，同时保持或提升了识别精度。", "motivation": "生物特征识别广泛应用，但模板的隐私和安全性是关键问题。同态加密（HE）虽能提供保护，但计算开销巨大。因此，需要找到一种方法来减少HE的计算负担，同时保持高识别性能。", "method": "研究利用深度神经网络（DNN）从人脸、指纹和虹膜数据（FRGC、MCYT、CASIA数据库）中提取特征，构建了一个内部虚拟多模态生物特征数据库。方法包括：1) 调查减小的多生物特征模板尺寸；2) 采用可解释、易于在加密下实现、无需训练且可泛化的方法；3) 对特征向量进行降维；4) 融合来自不同模态的特征向量。", "result": "通过融合多模态特征向量，模板尺寸可减少67%，且与表现最佳的单一模态相比，等错误率（EER）没有损失。特征向量的降维减少了同态加密域的操作，从而在保持或超越单生物特征识别的准确性和安全性水平的同时，实现了更高效的加密处理。", "conclusion": "多模态融合结合降维是实现高效同态加密生物特征模板保护的有效方案，它能显著减小模板尺寸，同时保持或提高识别准确性。"}}
{"id": "2508.11428", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11428", "abs": "https://arxiv.org/abs/2508.11428", "authors": ["Jingyu Li", "Bozhou Zhang", "Xin Jin", "Jiankang Deng", "Xiatian Zhu", "Li Zhang"], "title": "ImagiDrive: A Unified Imagination-and-Planning Framework for Autonomous Driving", "comment": null, "summary": "Autonomous driving requires rich contextual comprehension and precise\npredictive reasoning to navigate dynamic and complex environments safely.\nVision-Language Models (VLMs) and Driving World Models (DWMs) have\nindependently emerged as powerful recipes addressing different aspects of this\nchallenge. VLMs provide interpretability and robust action prediction through\ntheir ability to understand multi-modal context, while DWMs excel in generating\ndetailed and plausible future driving scenarios essential for proactive\nplanning. Integrating VLMs with DWMs is an intuitive, promising, yet\nunderstudied strategy to exploit the complementary strengths of accurate\nbehavioral prediction and realistic scene generation. Nevertheless, this\nintegration presents notable challenges, particularly in effectively connecting\naction-level decisions with high-fidelity pixel-level predictions and\nmaintaining computational efficiency. In this paper, we propose ImagiDrive, a\nnovel end-to-end autonomous driving framework that integrates a VLM-based\ndriving agent with a DWM-based scene imaginer to form a unified\nimagination-and-planning loop. The driving agent predicts initial driving\ntrajectories based on multi-modal inputs, guiding the scene imaginer to\ngenerate corresponding future scenarios. These imagined scenarios are\nsubsequently utilized to iteratively refine the driving agent's planning\ndecisions. To address efficiency and predictive accuracy challenges inherent in\nthis integration, we introduce an early stopping mechanism and a trajectory\nselection strategy. Extensive experimental validation on the nuScenes and\nNAVSIM datasets demonstrates the robustness and superiority of ImagiDrive over\nprevious alternatives under both open-loop and closed-loop conditions.", "AI": {"tldr": "ImagiDrive是一个端到端自动驾驶框架，它将基于VLM的驾驶代理与基于DWM的场景想象器集成，形成统一的想象与规划循环，通过迭代细化提升预测和规划能力。", "motivation": "自动驾驶需要强大的上下文理解和预测推理能力。视觉语言模型（VLM）和驾驶世界模型（DWM）分别在行为预测和场景生成方面表现出色，但尚未有效整合。整合它们可以结合各自优势，但面临行动与像素级预测连接及计算效率挑战。", "method": "提出ImagiDrive框架，该框架将一个基于VLM的驾驶代理与一个基于DWM的场景想象器集成。驾驶代理根据多模态输入预测初始驾驶轨迹，引导场景想象器生成相应的未来场景。这些想象的场景随后被用于迭代细化驾驶代理的规划决策。为解决效率和预测准确性问题，引入了早期停止机制和轨迹选择策略。", "result": "在nuScenes和NAVSIM数据集上的广泛实验验证表明，ImagiDrive在开环和闭环条件下均优于现有替代方案，展现出强大的鲁棒性和优越性。", "conclusion": "ImagiDrive成功地将VLM和DWM的优势结合，通过想象与规划的迭代循环，显著提升了自动驾驶系统的预测精度和规划能力，克服了集成挑战。"}}
{"id": "2508.11431", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11431", "abs": "https://arxiv.org/abs/2508.11431", "authors": ["Simona Kocour", "Assia Benbihi", "Torsten Sattler"], "title": "Remove360: Benchmarking Residuals After Object Removal in 3D Gaussian Splatting", "comment": "arXiv admin note: substantial text overlap with arXiv:2503.17574", "summary": "Understanding what semantic information persists after object removal is\ncritical for privacy-preserving 3D reconstruction and editable scene\nrepresentations. In this work, we introduce a novel benchmark and evaluation\nframework to measure semantic residuals, the unintended semantic traces left\nbehind, after object removal in 3D Gaussian Splatting. We conduct experiments\nacross a diverse set of indoor and outdoor scenes, showing that current methods\ncan preserve semantic information despite the absence of visual geometry. We\nalso release Remove360, a dataset of pre/post-removal RGB images and\nobject-level masks captured in real-world environments. While prior datasets\nhave focused on isolated object instances, Remove360 covers a broader and more\ncomplex range of indoor and outdoor scenes, enabling evaluation of object\nremoval in the context of full-scene representations. Given ground truth images\nof a scene before and after object removal, we assess whether we can truly\neliminate semantic presence, and if downstream models can still infer what was\nremoved. Our findings reveal critical limitations in current 3D object removal\ntechniques and underscore the need for more robust solutions capable of\nhandling real-world complexity. The evaluation framework is available at\ngithub.com/spatial-intelligence-ai/Remove360.git. Data are available at\nhuggingface.co/datasets/simkoc/Remove360.", "AI": {"tldr": "评估3D高斯泼溅中物体移除后残留的语义信息，并揭示现有技术的局限性。", "motivation": "理解物体移除后哪些语义信息会持续存在，对于保护隐私的3D重建和可编辑的场景表示至关重要，以避免留下无意的语义痕迹。", "method": "引入了一个新的基准和评估框架来衡量3D高斯泼溅中物体移除后留下的语义残差；在多样化的室内外场景中进行了实验；发布了Remove360数据集，包含真实世界环境中捕获的移除前后RGB图像和对象级掩码，以支持全场景表示中对象移除的评估。", "result": "研究发现，尽管视觉几何缺失，当前方法仍能保留语义信息；下游模型仍能推断出被移除的物体；这揭示了当前3D对象移除技术的关键局限性。", "conclusion": "强调需要更鲁棒的解决方案来处理真实世界的复杂性，以真正消除语义存在。"}}
{"id": "2508.11433", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11433", "abs": "https://arxiv.org/abs/2508.11433", "authors": ["Qian Liang", "Yujia Wu", "Kuncheng Li", "Jiwei Wei", "Shiyuan He", "Jinyu Guo", "Ning Xie"], "title": "MM-R1: Unleashing the Power of Unified Multimodal Large Language Models for Personalized Image Generation", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) with unified architectures excel\nacross a wide range of vision-language tasks, yet aligning them with\npersonalized image generation remains a significant challenge. Existing methods\nfor MLLMs are frequently subject-specific, demanding a data-intensive\nfine-tuning process for every new subject, which limits their scalability. In\nthis paper, we introduce MM-R1, a framework that integrates a cross-modal\nChain-of-Thought (X-CoT) reasoning strategy to unlock the inherent potential of\nunified MLLMs for personalized image generation. Specifically, we structure\npersonalization as an integrated visual reasoning and generation process: (1)\ngrounding subject concepts by interpreting and understanding user-provided\nimages and contextual cues, and (2) generating personalized images conditioned\non both the extracted subject representations and user prompts. To further\nenhance the reasoning capability, we adopt Grouped Reward Proximal Policy\nOptimization (GRPO) to explicitly align the generation. Experiments demonstrate\nthat MM-R1 unleashes the personalization capability of unified MLLMs to\ngenerate images with high subject fidelity and strong text alignment in a\nzero-shot manner.", "AI": {"tldr": "MM-R1是一个框架，它利用跨模态思维链（X-CoT）推理和分组奖励近端策略优化（GRPO），使统一的多模态大语言模型（MLLMs）能够进行零样本个性化图像生成。", "motivation": "现有的统一多模态大语言模型在个性化图像生成方面面临挑战，通常需要针对每个新主题进行数据密集型微调，这限制了其可扩展性。", "method": "引入MM-R1框架，该框架整合了跨模态思维链（X-CoT）推理策略。个性化被构建为视觉推理和生成过程：1) 通过解释和理解用户提供的图像和上下文线索来理解主题概念；2) 基于提取的主题表示和用户提示生成个性化图像。此外，采用分组奖励近端策略优化（GRPO）来增强推理能力并明确对齐生成。", "result": "实验证明，MM-R1在零样本情况下，释放了统一多模态大语言模型（MLLMs）的个性化生成能力，能够生成具有高主题保真度和强文本对齐的图像。", "conclusion": "MM-R1通过整合X-CoT推理和GRPO，成功地使统一的多模态大语言模型（MLLMs）实现了零样本个性化图像生成，解决了现有方法的扩展性问题。"}}
{"id": "2508.11464", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11464", "abs": "https://arxiv.org/abs/2508.11464", "authors": ["Xiaoya Zhu", "Yibing Nan", "Shiguo Lian"], "title": "Data-Driven Deepfake Image Detection Method -- The 2024 Global Deepfake Image Detection Challenge", "comment": null, "summary": "With the rapid development of technology in the field of AI, deepfake\ntechnology has emerged as a double-edged sword. It has not only created a large\namount of AI-generated content but also posed unprecedented challenges to\ndigital security. The task of the competition is to determine whether a face\nimage is a Deepfake image and output its probability score of being a Deepfake\nimage. In the image track competition, our approach is based on the Swin\nTransformer V2-B classification network. And online data augmentation and\noffline sample generation methods are employed to enrich the diversity of\ntraining samples and increase the generalization ability of the model. Finally,\nwe got the award of excellence in Deepfake image detection.", "AI": {"tldr": "本文提出了一种基于Swin Transformer V2-B的Deepfake图像检测方法，结合在线数据增强和离线样本生成，提高了模型的泛化能力，并在Deepfake图像检测竞赛中获得优秀奖。", "motivation": "随着AI技术的发展，Deepfake技术带来大量AI生成内容的同时，也对数字安全构成前所未有的挑战，因此需要有效的方法来识别Deepfake图像。", "method": "核心方法是基于Swin Transformer V2-B分类网络。为了丰富训练样本多样性和提高模型泛化能力，采用了在线数据增强和离线样本生成。", "result": "该方法在Deepfake图像检测竞赛中获得了优秀奖。", "conclusion": "所提出的基于Swin Transformer V2-B并结合数据增强和样本生成的方法，在Deepfake图像检测任务上表现出色，并得到了认可。"}}
{"id": "2508.11469", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11469", "abs": "https://arxiv.org/abs/2508.11469", "authors": ["Hongjin Fang", "Daniel Reisenbüchler", "Kenji Ikemura", "Mert R. Sabuncu", "Yihe Yang", "Ruining Deng"], "title": "CoFi: A Fast Coarse-to-Fine Few-Shot Pipeline for Glomerular Basement Membrane Segmentation", "comment": null, "summary": "Accurate segmentation of the glomerular basement membrane (GBM) in electron\nmicroscopy (EM) images is fundamental for quantifying membrane thickness and\nsupporting the diagnosis of various kidney diseases. While supervised deep\nlearning approaches achieve high segmentation accuracy, their reliance on\nextensive pixel-level annotation renders them impractical for clinical\nworkflows. Few-shot learning can reduce this annotation burden but often\nstruggles to capture the fine structural details necessary for GBM analysis. In\nthis study, we introduce CoFi, a fast and efficient coarse-to-fine few-shot\nsegmentation pipeline designed for GBM delineation in EM images. CoFi first\ntrains a lightweight neural network using only three annotated images to\nproduce an initial coarse segmentation mask. This mask is then automatically\nprocessed to generate high-quality point prompts with morphology-aware pruning,\nwhich are subsequently used to guide SAM in refining the segmentation. The\nproposed method achieved exceptional GBM segmentation performance, with a Dice\ncoefficient of 74.54% and an inference speed of 1.9 FPS. We demonstrate that\nCoFi not only alleviates the annotation and computational burdens associated\nwith conventional methods, but also achieves accurate and reliable segmentation\nresults. The pipeline's speed and annotation efficiency make it well-suited for\nresearch and hold strong potential for clinical applications in renal\npathology. The pipeline is publicly available at:\nhttps://github.com/ddrrnn123/CoFi.", "AI": {"tldr": "CoFi是一种快速高效的粗到细少样本分割方法，用于电镜图像中肾小球基底膜（GBM）的精确分割，解决了传统方法对大量标注的依赖。", "motivation": "肾小球基底膜（GBM）的准确分割对于肾脏疾病的诊断至关重要。然而，监督深度学习方法需要大量像素级标注，不适用于临床；而现有少样本学习方法难以捕捉GBM的精细结构细节。", "method": "CoFi管道首先使用少量（3张）标注图像训练一个轻量级神经网络，生成初始粗分割掩膜。然后，自动处理该掩膜，通过形态学感知剪枝生成高质量点提示，并用这些提示引导SAM（Segment Anything Model）进行精细分割。", "result": "CoFi在GBM分割上表现出色，Dice系数达到74.54%，推理速度为1.9 FPS。该方法显著减轻了标注和计算负担，同时实现了准确可靠的分割结果。", "conclusion": "CoFi管道在速度和标注效率方面表现优异，非常适合研究，并在肾脏病理学的临床应用中具有巨大潜力。"}}
{"id": "2508.11478", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11478", "abs": "https://arxiv.org/abs/2508.11478", "authors": ["Xinyi Yin", "Wenbo Yuan", "Xuecheng Wu", "Liangyu Fu", "Danlei Huang"], "title": "TACR-YOLO: A Real-time Detection Framework for Abnormal Human Behaviors Enhanced with Coordinate and Task-Aware Representations", "comment": "8 pages, 4 figures, accepted by IJCNN 2025", "summary": "Abnormal Human Behavior Detection (AHBD) under special scenarios is becoming\nincreasingly crucial. While YOLO-based detection methods excel in real-time\ntasks, they remain hindered by challenges including small objects, task\nconflicts, and multi-scale fusion in AHBD. To tackle them, we propose\nTACR-YOLO, a new real-time framework for AHBD. We introduce a Coordinate\nAttention Module to enhance small object detection, a Task-Aware Attention\nModule to deal with classification-regression conflicts, and a Strengthen Neck\nNetwork for refined multi-scale fusion, respectively. In addition, we optimize\nAnchor Box sizes using K-means clustering and deploy DIoU-Loss to improve\nbounding box regression. The Personnel Anomalous Behavior Detection (PABD)\ndataset, which includes 8,529 samples across four behavior categories, is also\npresented. Extensive experimental results indicate that TACR-YOLO achieves\n91.92% mAP on PABD, with competitive speed and robustness. Ablation studies\nhighlight the contribution of each improvement. This work provides new insights\nfor abnormal behavior detection under special scenarios, advancing its\nprogress.", "AI": {"tldr": "针对特殊场景下的异常人类行为检测（AHBD）中YOLO模型面临的小目标、任务冲突和多尺度融合等挑战，本文提出了TACR-YOLO框架，通过引入注意力模块、强化颈部网络、优化锚框和损失函数，并在新数据集PABD上实现了91.92%的mAP，具有竞争力的速度和鲁棒性。", "motivation": "异常人类行为检测（AHBD）在特殊场景下变得日益重要。然而，现有的基于YOLO的检测方法在AHBD中仍面临小目标检测、分类-回归任务冲突以及多尺度融合等挑战。", "method": "本文提出TACR-YOLO框架：1. 引入坐标注意力模块（Coordinate Attention Module）增强小目标检测；2. 引入任务感知注意力模块（Task-Aware Attention Module）处理分类-回归冲突；3. 采用强化颈部网络（Strengthen Neck Network）实现精细的多尺度融合；4. 使用K-means聚类优化锚框尺寸；5. 部署DIoU-Loss改进边界框回归。此外，还提出了包含8529个样本和四类行为的PABD数据集。", "result": "TACR-YOLO在PABD数据集上实现了91.92%的mAP，并展现出具有竞争力的速度和鲁棒性。消融实验也证实了各项改进的有效性。", "conclusion": "本工作为特殊场景下的异常行为检测提供了新的视角，并推动了该领域的进展。"}}
{"id": "2508.11482", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11482", "abs": "https://arxiv.org/abs/2508.11482", "authors": ["Ruoxin Xiong", "Yanyu Wang", "Jiannan Cai", "Kaijian Liu", "Yuansheng Zhu", "Pingbo Tang", "Nora El-Gohary"], "title": "OpenConstruction: A Systematic Synthesis of Open Visual Datasets for Data-Centric Artificial Intelligence in Construction Monitoring", "comment": null, "summary": "The construction industry increasingly relies on visual data to support\nArtificial Intelligence (AI) and Machine Learning (ML) applications for site\nmonitoring. High-quality, domain-specific datasets, comprising images, videos,\nand point clouds, capture site geometry and spatiotemporal dynamics, including\nthe location and interaction of objects, workers, and materials. However,\ndespite growing interest in leveraging visual datasets, existing resources vary\nwidely in sizes, data modalities, annotation quality, and representativeness of\nreal-world construction conditions. A systematic review to categorize their\ndata characteristics and application contexts is still lacking, limiting the\ncommunity's ability to fully understand the dataset landscape, identify\ncritical gaps, and guide future directions toward more effective, reliable, and\nscalable AI applications in construction. To address this gap, this study\nconducts an extensive search of academic databases and open-data platforms,\nyielding 51 publicly available visual datasets that span the 2005-2024 period.\nThese datasets are categorized using a structured data schema covering (i) data\nfundamentals (e.g., size and license), (ii) data modalities (e.g., RGB and\npoint cloud), (iii) annotation frameworks (e.g., bounding boxes), and (iv)\ndownstream application domains (e.g., progress tracking). This study\nsynthesizes these findings into an open-source catalog, OpenConstruction,\nsupporting data-driven method development. Furthermore, the study discusses\nseveral critical limitations in the existing construction dataset landscape and\npresents a roadmap for future data infrastructure anchored in the Findability,\nAccessibility, Interoperability, and Reusability (FAIR) principles. By\nreviewing the current landscape and outlining strategic priorities, this study\nsupports the advancement of data-centric solutions in the construction sector.", "AI": {"tldr": "本研究对建筑行业中用于人工智能/机器学习的51个公开视觉数据集进行了系统回顾、分类和分析，并提出了未来的发展路线图。", "motivation": "建筑行业对视觉数据的AI/ML应用日益依赖，但现有数据集在规模、模态、标注质量和代表性方面差异很大，缺乏系统的分类回顾，限制了社区理解数据集现状、识别差距和指导未来研究的能力。", "method": "本研究通过广泛搜索学术数据库和开放数据平台，收集了2005-2024年间的51个公开视觉数据集。这些数据集根据结构化数据模式进行分类，涵盖数据基本信息、数据模态、标注框架和下游应用领域。研究成果被整合到开源目录OpenConstruction中。", "result": "识别并分类了51个公开可用的建筑视觉数据集，涵盖了多种数据特性和应用场景。研究结果被整理成一个开源目录OpenConstruction。此外，研究还讨论了现有建筑数据集领域存在的关键局限性。", "conclusion": "本研究通过审查当前数据集现状并提出战略重点，支持了建筑领域数据驱动方法的发展。研究为未来的数据基础设施提出了基于FAIR原则的路线图，旨在推动建筑行业数据中心化解决方案的进步。"}}
{"id": "2508.11484", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11484", "abs": "https://arxiv.org/abs/2508.11484", "authors": ["Xiaoxue Wu", "Bingjie Gao", "Yu Qiao", "Yaohui Wang", "Xinyuan Chen"], "title": "CineTrans: Learning to Generate Videos with Cinematic Transitions via Masked Diffusion Models", "comment": "27 pages, 20 figures", "summary": "Despite significant advances in video synthesis, research into multi-shot\nvideo generation remains in its infancy. Even with scaled-up models and massive\ndatasets, the shot transition capabilities remain rudimentary and unstable,\nlargely confining generated videos to single-shot sequences. In this work, we\nintroduce CineTrans, a novel framework for generating coherent multi-shot\nvideos with cinematic, film-style transitions. To facilitate insights into the\nfilm editing style, we construct a multi-shot video-text dataset Cine250K with\ndetailed shot annotations. Furthermore, our analysis of existing video\ndiffusion models uncovers a correspondence between attention maps in the\ndiffusion model and shot boundaries, which we leverage to design a mask-based\ncontrol mechanism that enables transitions at arbitrary positions and transfers\neffectively in a training-free setting. After fine-tuning on our dataset with\nthe mask mechanism, CineTrans produces cinematic multi-shot sequences while\nadhering to the film editing style, avoiding unstable transitions or naive\nconcatenations. Finally, we propose specialized evaluation metrics for\ntransition control, temporal consistency and overall quality, and demonstrate\nthrough extensive experiments that CineTrans significantly outperforms existing\nbaselines across all criteria.", "AI": {"tldr": "该论文提出了CineTrans框架，用于生成具有电影风格转场的连贯多镜头视频，并通过构建Cine250K数据集和利用扩散模型注意力图中的镜头边界对应关系，实现了稳定且高质量的视频转场。", "motivation": "尽管视频合成技术取得了显著进展，但多镜头视频生成仍处于早期阶段。现有模型即使规模庞大、数据集充足，其镜头转场能力依然原始且不稳定，导致生成的视频主要限于单镜头序列。", "method": "该研究引入了CineTrans框架；构建了包含详细镜头注释的多镜头视频-文本数据集Cine250K；分析了现有视频扩散模型中注意力图与镜头边界的对应关系；设计了基于掩码的控制机制，在无需训练的情况下实现任意位置的转场；使用该掩码机制在Cine250K数据集上进行微调；提出了专门的评估指标用于转场控制、时间一致性和整体质量。", "result": "CineTrans能够生成符合电影编辑风格的电影级多镜头序列，避免了不稳定的转场或简单的拼接。通过广泛实验证明，CineTrans在所有评估标准上均显著优于现有基线模型。", "conclusion": "CineTrans是一个新颖的框架，有效解决了多镜头视频生成中转场不稳定和缺乏电影风格的问题，能够生成具有电影级转场的连贯多镜头视频。"}}
{"id": "2508.11486", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11486", "abs": "https://arxiv.org/abs/2508.11486", "authors": ["Kristina Dabrock", "Tim Johansson", "Anna Donarelli", "Mikael Mangold", "Noah Pflugradt", "Jann Michael Weinand", "Jochen Linßen"], "title": "Automated Building Heritage Assessment Using Street-Level Imagery", "comment": null, "summary": "Detailed data is required to quantify energy conservation measures in\nbuildings, such as envelop retrofits, without compromising cultural heritage.\nNovel artificial intelligence tools may improve efficiency in identifying\nheritage values in buildings compared to costly and time-consuming traditional\ninventories. In this study, the large language model GPT was used to detect\nvarious aspects of cultural heritage value in fa\\c{c}ade images. Using this\ndata and building register data as features, machine learning models were\ntrained to classify multi-family and non-residential buildings in Stockholm,\nSweden. Validation against an expert-created inventory shows a macro F1-score\nof 0.71 using a combination of register data and features retrieved from GPT,\nand a score of 0.60 using only GPT-derived data. The presented methodology can\ncontribute to a higher-quality database and thus support careful energy\nefficiency measures and integrated consideration of heritage value in\nlarge-scale energetic refurbishment scenarios.", "AI": {"tldr": "本研究利用大型语言模型GPT从建筑立面图像中识别文化遗产价值，并结合建筑登记数据训练机器学习模型，以支持在不损害文化遗产的前提下进行建筑节能改造。", "motivation": "量化建筑节能措施（如围护结构改造）需要详细数据，尤其是在涉及文化遗产的建筑中。传统普查方法成本高、耗时长，因此需要新型人工智能工具来提高识别建筑遗产价值的效率。", "method": "使用大型语言模型GPT从立面图像中检测文化遗产价值的各个方面。将这些GPT提取的数据与建筑登记数据结合作为特征，训练机器学习模型对瑞典斯德哥尔摩的多户住宅和非住宅建筑进行分类。通过与专家创建的清单进行验证。", "result": "结合登记数据和GPT提取特征的模型，宏观F1分数达到0.71；仅使用GPT提取数据时，宏观F1分数为0.60。", "conclusion": "所提出的方法有助于建立更高质量的数据库，从而支持在大规模节能改造情景中，谨慎实施能源效率措施并综合考虑遗产价值。"}}
{"id": "2508.11488", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11488", "abs": "https://arxiv.org/abs/2508.11488", "authors": ["Bozhou Zhang", "Jingyu Li", "Nan Song", "Li Zhang"], "title": "Perception in Plan: Coupled Perception and Planning for End-to-End Autonomous Driving", "comment": null, "summary": "End-to-end autonomous driving has achieved remarkable advancements in recent\nyears. Existing methods primarily follow a perception-planning paradigm, where\nperception and planning are executed sequentially within a fully differentiable\nframework for planning-oriented optimization. We further advance this paradigm\nthrough a perception-in-plan framework design, which integrates perception into\nthe planning process. This design facilitates targeted perception guided by\nevolving planning objectives over time, ultimately enhancing planning\nperformance. Building on this insight, we introduce VeteranAD, a coupled\nperception and planning framework for end-to-end autonomous driving. By\nincorporating multi-mode anchored trajectories as planning priors, the\nperception module is specifically designed to gather traffic elements along\nthese trajectories, enabling comprehensive and targeted perception. Planning\ntrajectories are then generated based on both the perception results and the\nplanning priors. To make perception fully serve planning, we adopt an\nautoregressive strategy that progressively predicts future trajectories while\nfocusing on relevant regions for targeted perception at each step. With this\nsimple yet effective design, VeteranAD fully unleashes the potential of\nplanning-oriented end-to-end methods, leading to more accurate and reliable\ndriving behavior. Extensive experiments on the NAVSIM and Bench2Drive datasets\ndemonstrate that our VeteranAD achieves state-of-the-art performance.", "AI": {"tldr": "该研究提出了VeteranAD，一个将感知深度融入规划过程的端到端自动驾驶框架，通过目标导向的感知和自回归策略显著提升了规划性能。", "motivation": "现有端到端自动驾驶系统多采用感知-规划顺序范式，感知和规划是分离的。研究旨在进一步优化此范式，将感知融入规划，使感知能被规划目标动态引导，从而提升规划性能。", "method": "引入VeteranAD框架，它是一个耦合感知与规划的系统。该方法利用多模式锚定轨迹作为规划先验，感知模块围绕这些轨迹收集交通元素，实现有针对性的感知。规划轨迹基于感知结果和规划先验生成。此外，采用自回归策略，逐步预测未来轨迹，并在每一步聚焦相关区域进行目标感知。", "result": "VeteranAD在NAVSIM和Bench2Drive数据集上进行了广泛实验，结果表明其实现了最先进的性能。", "conclusion": "通过简单而有效的设计，VeteranAD充分发挥了面向规划的端到端方法的潜力，实现了更准确、更可靠的驾驶行为，验证了“感知融入规划”范式的有效性。"}}
{"id": "2508.11497", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11497", "abs": "https://arxiv.org/abs/2508.11497", "authors": ["Feiyue Zhao", "Zhichao Zhang"], "title": "Hierarchical Graph Feature Enhancement with Adaptive Frequency Modulation for Visual Recognition", "comment": null, "summary": "Convolutional neural networks (CNNs) have\n  demonstrated strong performance in visual recognition tasks,\n  but their inherent reliance on regular grid structures limits\n  their capacity to model complex topological relationships and\n  non-local semantics within images. To address this limita tion, we propose\nthe hierarchical graph feature enhancement\n  (HGFE), a novel framework that integrates graph-based rea soning into CNNs to\nenhance both structural awareness and\n  feature representation. HGFE builds two complementary levels\n  of graph structures: intra-window graph convolution to cap ture local spatial\ndependencies and inter-window supernode\n  interactions to model global semantic relationships. Moreover,\n  we introduce an adaptive frequency modulation module that\n  dynamically balances low-frequency and high-frequency signal\n  propagation, preserving critical edge and texture information\n  while mitigating over-smoothing. The proposed HGFE module\n  is lightweight, end-to-end trainable, and can be seamlessly\n  integrated into standard CNN backbone networks. Extensive\n  experiments on CIFAR-100 (classification), PASCAL VOC,\n  and VisDrone (detection), as well as CrackSeg and CarParts\n  (segmentation), validated the effectiveness of the HGFE in\n  improving structural representation and enhancing overall\n  recognition performance.", "AI": {"tldr": "本文提出HGFE框架，通过在CNN中融入分层图结构和自适应频率调制，以增强模型对复杂拓扑关系和非局部语义的建模能力，从而提升视觉识别性能。", "motivation": "卷积神经网络（CNNs）依赖于规则网格结构，这限制了它们建模图像中复杂拓扑关系和非局部语义的能力。", "method": "提出分层图特征增强（HGFE）框架，该框架将图基推理整合到CNN中。HGFE构建了两个互补的图结构级别：用于捕获局部空间依赖的窗内图卷积，以及用于建模全局语义关系的窗间超节点交互。此外，引入自适应频率调制模块，动态平衡低频和高频信号传播，以保留关键边缘和纹理信息并减轻过平滑。HGFE模块轻量、端到端可训练，并可无缝集成到标准CNN骨干网络中。", "result": "在CIFAR-100（分类）、PASCAL VOC和VisDrone（检测）以及CrackSeg和CarParts（分割）上的大量实验验证了HGFE在改善结构表示和增强整体识别性能方面的有效性。", "conclusion": "HGFE通过集成图基推理，有效克服了CNN在建模复杂拓扑关系和非局部语义方面的局限性，显著提高了视觉识别任务的结构表示能力和整体性能。"}}
{"id": "2508.11502", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11502", "abs": "https://arxiv.org/abs/2508.11502", "authors": ["Eyad Alshami", "Shashank Agnihotri", "Bernt Schiele", "Margret Keuper"], "title": "AIM: Amending Inherent Interpretability via Self-Supervised Masking", "comment": "Accepted at International Conference on Computer Vision (ICCV) 2025", "summary": "It has been observed that deep neural networks (DNNs) often use both genuine\nas well as spurious features. In this work, we propose \"Amending Inherent\nInterpretability via Self-Supervised Masking\" (AIM), a simple yet interestingly\neffective method that promotes the network's utilization of genuine features\nover spurious alternatives without requiring additional annotations. In\nparticular, AIM uses features at multiple encoding stages to guide a\nself-supervised, sample-specific feature-masking process. As a result, AIM\nenables the training of well-performing and inherently interpretable models\nthat faithfully summarize the decision process. We validate AIM across a\ndiverse range of challenging datasets that test both out-of-distribution\ngeneralization and fine-grained visual understanding. These include\ngeneral-purpose classification benchmarks such as ImageNet100, HardImageNet,\nand ImageWoof, as well as fine-grained classification datasets such as\nWaterbirds, TravelingBirds, and CUB-200. AIM demonstrates significant dual\nbenefits: interpretability improvements, as measured by the Energy Pointing\nGame (EPG) score, and accuracy gains over strong baselines. These consistent\ngains across domains and architectures provide compelling evidence that AIM\npromotes the use of genuine and meaningful features that directly contribute to\nimproved generalization and human-aligned interpretability.", "AI": {"tldr": "本文提出AIM方法，通过自监督特征掩码，使深度神经网络更多地利用真实特征而非虚假特征，从而提升模型性能和可解释性。", "motivation": "观察到深度神经网络（DNNs）在决策过程中常同时利用真实特征和虚假特征，导致模型可解释性差且泛化能力受限。", "method": "提出“Amending Inherent Interpretability via Self-Supervised Masking (AIM)”方法。该方法无需额外标注，利用多阶段编码特征指导一个自监督、样本特定的特征掩码过程，以促进网络利用真实特征。", "result": "AIM在多种挑战性数据集（如ImageNet100、HardImageNet、ImageWoof、Waterbirds、TravelingBirds、CUB-200）上进行了验证。结果显示，AIM在提高模型准确性的同时，显著提升了可解释性（通过Energy Pointing Game (EPG) 分数衡量），并且优于现有基线方法。", "conclusion": "AIM方法有效促进了模型对真实且有意义特征的利用，从而在不同领域和架构上实现了泛化能力和与人类对齐的可解释性的双重提升。"}}
{"id": "2508.11517", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11517", "abs": "https://arxiv.org/abs/2508.11517", "authors": ["Shaoze Huang", "Qi Liu", "Chao Chen", "Yuhang Chen"], "title": "A Real-time Concrete Crack Detection and Segmentation Model Based on YOLOv11", "comment": null, "summary": "Accelerated aging of transportation infrastructure in the rapidly developing\nYangtze River Delta region necessitates efficient concrete crack detection, as\ncrack deterioration critically compromises structural integrity and regional\neconomic growth. To overcome the limitations of inefficient manual inspection\nand the suboptimal performance of existing deep learning models, particularly\nfor small-target crack detection within complex backgrounds, this paper\nproposes YOLOv11-KW-TA-FP, a multi-task concrete crack detection and\nsegmentation model based on the YOLOv11n architecture. The proposed model\nintegrates a three-stage optimization framework: (1) Embedding dynamic\nKernelWarehouse convolution (KWConv) within the backbone network to enhance\nfeature representation through a dynamic kernel sharing mechanism; (2)\nIncorporating a triple attention mechanism (TA) into the feature pyramid to\nstrengthen channel-spatial interaction modeling; and (3) Designing an FP-IoU\nloss function to facilitate adaptive bounding box regression penalization.\nExperimental validation demonstrates that the enhanced model achieves\nsignificant performance improvements over the baseline, attaining 91.3%\nprecision, 76.6% recall, and 86.4% mAP@50. Ablation studies confirm the\nsynergistic efficacy of the proposed modules. Furthermore, robustness tests\nindicate stable performance under conditions of data scarcity and noise\ninterference. This research delivers an efficient computer vision solution for\nautomated infrastructure inspection, exhibiting substantial practical\nengineering value.", "AI": {"tldr": "该论文提出了一种名为YOLOv11-KW-TA-FP的多任务混凝土裂缝检测与分割模型，通过集成动态卷积、三重注意力机制和自适应损失函数，显著提升了在复杂背景下对小目标裂缝的检测性能，为基础设施自动化检测提供了高效的计算机视觉解决方案。", "motivation": "长江三角洲地区交通基础设施老化加速，裂缝劣化严重威胁结构完整性和区域经济增长。现有人工检测效率低下，而现有深度学习模型在复杂背景下对小目标裂缝检测性能不佳，因此需要更高效、准确的自动化检测方案。", "method": "本研究提出了基于YOLOv11n架构的YOLOv11-KW-TA-FP模型，包含三阶段优化框架：(1) 在主干网络中嵌入动态KernelWarehouse卷积（KWConv）以增强特征表示；(2) 在特征金字塔中引入三重注意力机制（TA）以加强通道-空间交互建模；(3) 设计FP-IoU损失函数以实现自适应边界框回归惩罚。", "result": "实验验证表明，所提出的模型相比基线模型性能显著提升，达到了91.3%的精确率、76.6%的召回率和86.4%的mAP@50。消融研究证实了各模块的协同增效作用。此外，鲁棒性测试表明模型在数据稀缺和噪声干扰条件下仍能保持稳定性能。", "conclusion": "本研究为自动化基础设施检测提供了一个高效的计算机视觉解决方案，展示出重要的实际工程价值。"}}
{"id": "2508.11531", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11531", "abs": "https://arxiv.org/abs/2508.11531", "authors": ["Shilei Wang", "Gong Cheng", "Pujian Lai", "Dong Gao", "Junwei Han"], "title": "Multi-State Tracker: Enhancing Efficient Object Tracking via Multi-State Specialization and Interaction", "comment": null, "summary": "Efficient trackers achieve faster runtime by reducing computational\ncomplexity and model parameters. However, this efficiency often compromises the\nexpense of weakened feature representation capacity, thus limiting their\nability to accurately capture target states using single-layer features. To\novercome this limitation, we propose Multi-State Tracker (MST), which utilizes\nhighly lightweight state-specific enhancement (SSE) to perform specialized\nenhancement on multi-state features produced by multi-state generation (MSG)\nand aggregates them in an interactive and adaptive manner using cross-state\ninteraction (CSI). This design greatly enhances feature representation while\nincurring minimal computational overhead, leading to improved tracking\nrobustness in complex environments. Specifically, the MSG generates multiple\nstate representations at multiple stages during feature extraction, while SSE\nrefines them to highlight target-specific features. The CSI module facilitates\ninformation exchange between these states and ensures the integration of\ncomplementary features. Notably, the introduced SSE and CSI modules adopt a\nhighly lightweight hidden state adaptation-based state space duality (HSA-SSD)\ndesign, incurring only 0.1 GFLOPs in computation and 0.66 M in parameters.\nExperimental results demonstrate that MST outperforms all previous efficient\ntrackers across multiple datasets, significantly improving tracking accuracy\nand robustness. In particular, it shows excellent runtime performance, with an\nAO score improvement of 4.5\\% over the previous SOTA efficient tracker HCAT on\nthe GOT-10K dataset. The code is available at https://github.com/wsumel/MST.", "AI": {"tldr": "MST通过轻量级多状态特征增强，解决了高效跟踪器在特征表示能力上的不足，显著提升了跟踪精度和鲁棒性。", "motivation": "现有高效跟踪器为了追求速度和低计算量，牺牲了特征表示能力，导致在复杂环境中难以精确捕捉目标状态。", "method": "本文提出了多状态跟踪器（MST），它通过多状态生成（MSG）产生多阶段特征，并利用轻量级的状态特定增强（SSE）对这些特征进行专门强化，最后通过跨状态交互（CSI）模块自适应地聚合互补特征。SSE和CSI模块采用高度轻量级的隐状态自适应-状态空间对偶（HSA-SSD）设计，计算量和参数量极小。", "result": "MST在多个数据集上超越了所有先前的高效跟踪器，显著提升了跟踪精度和鲁棒性。它表现出卓越的运行时性能，在GOT-10K数据集上比之前的SOTA高效跟踪器HCAT的AO分数提高了4.5%。", "conclusion": "MST通过创新的多状态特征处理和极其轻量级的增强模块，成功地在保持高效率的同时，大幅提升了跟踪器的特征表示能力和性能，实现了精度与速度的有效平衡。"}}
{"id": "2508.11532", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11532", "abs": "https://arxiv.org/abs/2508.11532", "authors": ["Jingsong Xia", "Yue Yin", "Xiuhan Li"], "title": "An Efficient Medical Image Classification Method Based on a Lightweight Improved ConvNeXt-Tiny Architecture", "comment": null, "summary": "Intelligent analysis of medical imaging plays a crucial role in assisting\nclinical diagnosis. However, achieving efficient and high-accuracy image\nclassification in resource-constrained computational environments remains\nchallenging. This study proposes a medical image classification method based on\nan improved ConvNeXt-Tiny architecture. Through structural optimization and\nloss function design, the proposed method enhances feature extraction\ncapability and classification performance while reducing computational\ncomplexity. Specifically, the method introduces a dual global pooling (Global\nAverage Pooling and Global Max Pooling) feature fusion strategy into the\nConvNeXt-Tiny backbone to simultaneously preserve global statistical features\nand salient response information. A lightweight channel attention module,\ntermed Squeeze-and-Excitation Vector (SEVector), is designed to improve the\nadaptive allocation of channel weights while minimizing parameter overhead.\nAdditionally, a Feature Smoothing Loss is incorporated into the loss function\nto enhance intra-class feature consistency and suppress intra-class variance.\nUnder CPU-only conditions (8 threads), the method achieves a maximum\nclassification accuracy of 89.10% on the test set within 10 training epochs,\nexhibiting a stable convergence trend in loss values. Experimental results\ndemonstrate that the proposed method effectively improves medical image\nclassification performance in resource-limited settings, providing a feasible\nand efficient solution for the deployment and promotion of medical imaging\nanalysis models.", "AI": {"tldr": "本研究提出一种基于改进ConvNeXt-Tiny的医学图像分类方法，通过结构优化（双全局池化、轻量级通道注意力模块SEVector）和损失函数设计（特征平滑损失），在资源受限环境下实现了高效高精度的分类。", "motivation": "在资源受限的计算环境中，实现高效且高准确度的医学图像分类仍然是一个挑战。", "method": "该方法基于改进的ConvNeXt-Tiny架构，具体包括：1) 在ConvNeXt-Tiny主干网络中引入双全局池化（全局平均池化和全局最大池化）特征融合策略，以同时保留全局统计特征和显著响应信息。2) 设计了轻量级通道注意力模块Squeeze-and-Excitation Vector (SEVector)，以最小的参数开销提高通道权重的自适应分配。3) 在损失函数中加入了特征平滑损失（Feature Smoothing Loss），以增强类内特征一致性并抑制类内方差。", "result": "在仅CPU（8线程）条件下，该方法在10个训练周期内于测试集上达到了89.10%的最大分类准确率，并且损失值呈现稳定的收敛趋势。", "conclusion": "所提出的方法有效提升了资源受限环境下医学图像分类的性能，为医学影像分析模型的部署和推广提供了一种可行且高效的解决方案。"}}
{"id": "2508.11538", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11538", "abs": "https://arxiv.org/abs/2508.11538", "authors": ["Sitong Gong", "Lu Zhang", "Yunzhi Zhuge", "Xu Jia", "Pingping Zhang", "Huchuan Lu"], "title": "Reinforcing Video Reasoning Segmentation to Think Before It Segments", "comment": "12 pages", "summary": "Video reasoning segmentation (VRS) endeavors to delineate referred objects in\nvideos guided by implicit instructions that encapsulate human intent and\ntemporal logic. Previous approaches leverage large vision language models\n(LVLMs) to encode object semantics into <SEG> tokens for mask prediction.\nHowever, this paradigm suffers from limited interpretability during inference\nand suboptimal performance due to inadequate spatiotemporal reasoning. Drawing\ninspiration from seminal breakthroughs in reinforcement learning, we introduce\nVeason-R1, a specialized LVLM for VRS that emphasizes structured reasoning in\nsegmentation. Veason-R1 is trained through Group Relative Policy Optimization\n(GRPO) augmented with Chain-of-Thought (CoT) initialization. To begin with, we\ncurate high-quality CoT training data to instill structured reasoning\ntrajectories, bridging video-level semantics and frame-level spatial grounding,\nyielding the supervised fine-tuned model Veason-SFT. Subsequently, GRPO\nfine-tuning encourages efficient exploration of the reasoning space by\noptimizing reasoning chains. To this end, we incorporate a holistic reward\nmechanism that synergistically enhances spatial alignment and temporal\nconsistency, bolstering keyframe localization and fine-grained grounding.\nComprehensive empirical evaluations demonstrate that Veason-R1 achieves\nstate-of-the-art performance on multiple benchmarks, surpassing prior art by\nsignificant margins (e.g., +1.3 J &F in ReVOS and +10.0 J &F in ReasonVOS),\nwhile exhibiting robustness to hallucinations (+8.8 R). Our code and model\nweights will be available at Veason-R1.", "AI": {"tldr": "Veason-R1是一种专门用于视频推理分割（VRS）的大型视觉语言模型（LVLM），它通过结合思维链（CoT）初始化的群组相对策略优化（GRPO）来强调结构化推理，并在多个基准测试中实现了最先进的性能。", "motivation": "先前的VRS方法（利用LVLM将对象语义编码为<SEG>标记进行掩码预测）在推理过程中解释性有限，并且由于空间-时间推理不足导致性能不佳。", "method": "引入了Veason-R1，一个专注于VRS的LVLM，其训练方法是结合思维链（CoT）初始化的群组相对策略优化（GRPO）。首先，通过高质量的CoT训练数据灌输结构化推理轨迹（连接视频级语义和帧级空间定位），得到监督微调模型Veason-SFT。随后，GRPO微调通过优化推理链来鼓励推理空间的有效探索。为此，论文引入了一个全面的奖励机制，协同增强空间对齐和时间一致性，以加强关键帧定位和细粒度定位。", "result": "Veason-R1在多个基准测试上实现了最先进的性能，显著超越了现有技术（例如，ReVOS上J&F提高1.3，ReasonVOS上J&F提高10.0），同时展现出对幻觉的鲁棒性（R提高8.8）。", "conclusion": "Veason-R1通过引入受强化学习启发的结构化推理方法，有效解决了现有VRS方法的解释性不足和空间-时间推理性能欠佳的问题，并在多项任务上建立了新的性能基准。"}}
{"id": "2508.11550", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11550", "abs": "https://arxiv.org/abs/2508.11550", "authors": ["Zuo Zuo", "Jiahao Dong", "Yanyun Qu", "Zongze Wu"], "title": "Training-Free Anomaly Generation via Dual-Attention Enhancement in Diffusion Model", "comment": null, "summary": "Industrial anomaly detection (AD) plays a significant role in manufacturing\nwhere a long-standing challenge is data scarcity. A growing body of works have\nemerged to address insufficient anomaly data via anomaly generation. However,\nthese anomaly generation methods suffer from lack of fidelity or need to be\ntrained with extra data. To this end, we propose a training-free anomaly\ngeneration framework dubbed AAG, which is based on Stable Diffusion (SD)'s\nstrong generation ability for effective anomaly image generation. Given a\nnormal image, mask and a simple text prompt, AAG can generate realistic and\nnatural anomalies in the specific regions and simultaneously keep contents in\nother regions unchanged. In particular, we propose Cross-Attention Enhancement\n(CAE) to re-engineer the cross-attention mechanism within Stable Diffusion\nbased on the given mask. CAE increases the similarity between visual tokens in\nspecific regions and text embeddings, which guides these generated visual\ntokens in accordance with the text description. Besides, generated anomalies\nneed to be more natural and plausible with object in given image. We propose\nSelf-Attention Enhancement (SAE) which improves similarity between each normal\nvisual token and anomaly visual tokens. SAE ensures that generated anomalies\nare coherent with original pattern. Extensive experiments on MVTec AD and VisA\ndatasets demonstrate effectiveness of AAG in anomaly generation and its\nutility. Furthermore, anomaly images generated by AAG can bolster performance\nof various downstream anomaly inspection tasks.", "AI": {"tldr": "AAG是一个免训练的异常生成框架，利用Stable Diffusion为工业异常检测生成逼真且自然的异常图像，有效缓解数据稀缺问题并提升下游任务性能。", "motivation": "工业异常检测面临数据稀缺的长期挑战。现有异常生成方法存在保真度不足或需要额外数据训练的问题。", "method": "提出AAG（Training-free Anomaly Generation）框架，基于Stable Diffusion强大的生成能力。给定正常图像、掩码和文本提示，AAG能在特定区域生成异常。关键创新包括：1) 交叉注意力增强（CAE），根据给定掩码重新设计交叉注意力机制，增加特定区域视觉token与文本嵌入的相似性，引导生成符合文本描述；2) 自注意力增强（SAE），提高正常视觉token与异常视觉token之间的相似性，确保生成异常与原始模式一致且自然。", "result": "AAG能生成逼真自然的异常。在MVTec AD和VisA数据集上的大量实验证明了AAG在异常生成方面的有效性和实用性。此外，AAG生成的异常图像能显著提升各种下游异常检测任务的性能。", "conclusion": "AAG是一个无需训练的异常生成框架，有效解决了工业异常检测中的数据稀缺问题，通过生成高保真异常图像，显著提升了下游异常检测任务的性能。"}}
{"id": "2508.11569", "categories": ["cs.CV", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.11569", "abs": "https://arxiv.org/abs/2508.11569", "authors": ["Zheng Wang", "Shihao Xu", "Wei Shi"], "title": "TrajSV: A Trajectory-based Model for Sports Video Representations and Applications", "comment": "This paper has been accepted by TCSVT", "summary": "Sports analytics has received significant attention from both academia and\nindustry in recent years. Despite the growing interest and efforts in this\nfield, several issues remain unresolved, including (1) data unavailability, (2)\nlack of an effective trajectory-based framework, and (3) requirement for\nsufficient supervision labels. In this paper, we present TrajSV, a\ntrajectory-based framework that addresses various issues in existing studies.\nTrajSV comprises three components: data preprocessing, Clip Representation\nNetwork (CRNet), and Video Representation Network (VRNet). The data\npreprocessing module extracts player and ball trajectories from sports\nbroadcast videos. CRNet utilizes a trajectory-enhanced Transformer module to\nlearn clip representations based on these trajectories. Additionally, VRNet\nlearns video representations by aggregating clip representations and visual\nfeatures with an encoder-decoder architecture. Finally, a triple contrastive\nloss is introduced to optimize both video and clip representations in an\nunsupervised manner. The experiments are conducted on three broadcast video\ndatasets to verify the effectiveness of TrajSV for three types of sports (i.e.,\nsoccer, basketball, and volleyball) with three downstream applications (i.e.,\nsports video retrieval, action spotting, and video captioning). The results\ndemonstrate that TrajSV achieves state-of-the-art performance in sports video\nretrieval, showcasing a nearly 70% improvement. It outperforms baselines in\naction spotting, achieving state-of-the-art results in 9 out of 17 action\ncategories, and demonstrates a nearly 20% improvement in video captioning.\nAdditionally, we introduce a deployed system along with the three applications\nbased on TrajSV.", "AI": {"tldr": "TrajSV是一个基于轨迹的无监督框架，用于解决体育视频分析中数据、轨迹框架和监督标签不足的问题，并在多种体育项目和下游应用中取得了最先进的性能。", "motivation": "尽管体育分析领域受到广泛关注，但仍存在数据不可用、缺乏有效的基于轨迹的框架以及需要大量监督标签等未解决的问题。", "method": "本文提出了TrajSV框架，包含三个组件：数据预处理（从体育广播视频中提取球员和球的轨迹）、Clip Representation Network (CRNet)（利用轨迹增强的Transformer学习片段表示）、Video Representation Network (VRNet)（通过编码器-解码器架构聚合片段表示和视觉特征学习视频表示）。此外，引入三重对比损失以无监督方式优化视频和片段表示。", "result": "TrajSV在体育视频检索中实现了近70%的性能提升，在动作识别的17个动作类别中有9个达到了最先进水平，并在视频字幕生成中实现了近20%的性能提升。实验在足球、篮球和排球三种体育项目的三个广播视频数据集上验证了其有效性。", "conclusion": "TrajSV是一个有效的基于轨迹的无监督框架，能够解决现有体育分析中的关键问题，并在体育视频检索、动作识别和视频字幕生成等多个下游应用中展现出最先进的性能和广泛的适用性。同时，论文还提出了一个基于TrajSV的部署系统。"}}
{"id": "2508.11576", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11576", "abs": "https://arxiv.org/abs/2508.11576", "authors": ["Yumeng Shi", "Quanyu Long", "Yin Wu", "Wenya Wang"], "title": "Causality Matters: How Temporal Information Emerges in Video Language Models", "comment": null, "summary": "Video language models (VideoLMs) have made significant progress in multimodal\nunderstanding. However, temporal understanding, which involves identifying\nevent order, duration, and relationships across time, still remains a core\nchallenge. Prior works emphasize positional encodings (PEs) as a key mechanism\nfor encoding temporal structure. Surprisingly, we find that removing or\nmodifying PEs in video inputs yields minimal degradation in the performance of\ntemporal understanding. In contrast, reversing the frame sequence while\npreserving the original PEs causes a substantial drop. To explain this\nbehavior, we conduct substantial analysis experiments to trace how temporal\ninformation is integrated within the model. We uncover a causal information\npathway: temporal cues are progressively synthesized through inter-frame\nattention, aggregated in the final frame, and subsequently integrated into the\nquery tokens. This emergent mechanism shows that temporal reasoning emerges\nfrom inter-visual token interactions under the constraints of causal attention,\nwhich implicitly encodes temporal structure. Based on these insights, we\npropose two efficiency-oriented strategies: staged cross-modal attention and a\ntemporal exit mechanism for early token truncation. Experiments on two\nbenchmarks validate the effectiveness of both approaches. To the best of our\nknowledge, this is the first work to systematically investigate video temporal\nunderstanding in VideoLMs, offering insights for future model improvement.", "AI": {"tldr": "视频语言模型（VideoLMs）的时间理解并非主要依赖位置编码，而是源于帧间注意力下视觉令牌交互的涌现机制，并基于此提出了两种高效策略。", "motivation": "视频语言模型在时间理解方面仍面临核心挑战，尽管此前认为位置编码是关键。本文旨在揭示时间信息在模型中整合的真实机制。", "method": "通过移除或修改视频输入中的位置编码来评估其影响；通过反转帧序列同时保留原始位置编码来观察性能变化；进行大量分析实验以追踪时间信息在模型中的整合路径；基于发现提出分阶段跨模态注意力和时间退出机制两种效率导向的策略，并在两个基准数据集上验证其有效性。", "result": "移除或修改位置编码对时间理解性能影响甚微；反转帧序列（保留原始位置编码）导致性能大幅下降；时间线索通过帧间注意力逐步合成，在最终帧中聚合，并随后整合到查询令牌中；时间推理是在因果注意力约束下，通过视觉令牌间交互涌现的；提出的两种高效策略均有效。", "conclusion": "视频语言模型的时间理解是一种涌现机制，主要通过帧间视觉令牌交互在因果注意力下隐式编码时间结构，而非仅仅依靠位置编码。这些发现为未来模型改进，特别是效率提升，提供了重要见解。"}}
{"id": "2508.11591", "categories": ["cs.CV", "cs.ET"], "pdf": "https://arxiv.org/pdf/2508.11591", "abs": "https://arxiv.org/abs/2508.11591", "authors": ["Durga Joshi", "Chandi Witharana", "Robert Fahey", "Thomas Worthley", "Zhe Zhu", "Diego Cerrai"], "title": "DashCam Video: A complementary low-cost data stream for on-demand forest-infrastructure system monitoring", "comment": "35 Pages, 15 figures", "summary": "Our study introduces a novel, low-cost, and reproducible framework for\nreal-time, object-level structural assessment and geolocation of roadside\nvegetation and infrastructure with commonly available but underutilized\ndashboard camera (dashcam) video data. We developed an end-to-end pipeline that\ncombines monocular depth estimation, depth error correction, and geometric\ntriangulation to generate accurate spatial and structural data from\nstreet-level video streams from vehicle-mounted dashcams. Depth maps were first\nestimated using a state-of-the-art monocular depth model, then refined via a\ngradient-boosted regression framework to correct underestimations, particularly\nfor distant objects. The depth correction model achieved strong predictive\nperformance (R2 = 0.92, MAE = 0.31 on transformed scale), significantly\nreducing bias beyond 15 m. Further, object locations were estimated using\nGPS-based triangulation, while object heights were calculated using pin hole\ncamera geometry. Our method was evaluated under varying conditions of camera\nplacement and vehicle speed. Low-speed vehicle with inside camera gave the\nhighest accuracy, with mean geolocation error of 2.83 m, and mean absolute\nerror (MAE) in height estimation of 2.09 m for trees and 0.88 m for poles. To\nthe best of our knowledge, it is the first framework to combine monocular depth\nmodeling, triangulated GPS-based geolocation, and real-time structural\nassessment for urban vegetation and infrastructure using consumer-grade video\ndata. Our approach complements conventional RS methods, such as LiDAR and image\nby offering a fast, real-time, and cost-effective solution for object-level\nmonitoring of vegetation risks and infrastructure exposure, making it\nespecially valuable for utility companies, and urban planners aiming for\nscalable and frequent assessments in dynamic urban environments.", "AI": {"tldr": "本研究提出了一种新颖、低成本且可复现的框架，利用行车记录仪视频数据，对路边植被和基础设施进行实时、物体级别的结构评估和地理定位。", "motivation": "传统的遥感方法（如激光雷达和图像）成本高昂且不便大规模频繁应用。本研究旨在利用普遍可用但未充分利用的行车记录仪数据，提供一种快速、实时、经济高效的解决方案，以实现城市环境中植被风险和基础设施暴露的物体级监测，尤其对公用事业公司和城市规划者有价值。", "method": "该方法开发了一个端到端的数据处理流程，结合了单目深度估计、深度误差校正和几何三角测量。首先使用先进的单目深度模型估计深度图，然后通过梯度提升回归框架对深度进行校正，以减少对远距离物体的低估。物体位置通过基于GPS的三角测量估算，物体高度则利用小孔相机几何原理计算。该方法在不同相机位置和车速条件下进行了评估。", "result": "深度校正模型表现出强大的预测性能（R2 = 0.92，转换尺度上的MAE = 0.31），显著降低了15米以外的偏差。在低速行驶且使用内置摄像头的条件下，系统精度最高，平均地理定位误差为2.83米，树木高度估计的平均绝对误差（MAE）为2.09米，电线杆为0.88米。", "conclusion": "本研究首次将单目深度建模、基于GPS的三角定位和实时结构评估相结合，利用消费级行车记录仪视频数据实现对城市植被和基础设施的监测。该方法作为传统遥感方法的补充，提供了一种快速、实时、经济高效的物体级监测解决方案，对于公用事业公司和城市规划者进行可扩展和频繁的评估具有重要价值。"}}
{"id": "2508.11603", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11603", "abs": "https://arxiv.org/abs/2508.11603", "authors": ["Zhe Zhu", "Honghua Chen", "Peng Li", "Mingqiang Wei"], "title": "CoreEditor: Consistent 3D Editing via Correspondence-constrained Diffusion", "comment": null, "summary": "Text-driven 3D editing seeks to modify 3D scenes according to textual\ndescriptions, and most existing approaches tackle this by adapting pre-trained\n2D image editors to multi-view inputs. However, without explicit control over\nmulti-view information exchange, they often fail to maintain cross-view\nconsistency, leading to insufficient edits and blurry details. We introduce\nCoreEditor, a novel framework for consistent text-to-3D editing. The key\ninnovation is a correspondence-constrained attention mechanism that enforces\nprecise interactions between pixels expected to remain consistent throughout\nthe diffusion denoising process. Beyond relying solely on geometric alignment,\nwe further incorporate semantic similarity estimated during denoising, enabling\nmore reliable correspondence modeling and robust multi-view editing. In\naddition, we design a selective editing pipeline that allows users to choose\npreferred results from multiple candidates, offering greater flexibility and\nuser control. Extensive experiments show that CoreEditor produces high-quality,\n3D-consistent edits with sharper details, significantly outperforming prior\nmethods.", "AI": {"tldr": "CoreEditor是一种新型框架，通过引入对应约束注意力机制和选择性编辑流程，解决了现有文本驱动3D编辑方法中跨视图一致性差和细节模糊的问题，实现了高质量、3D一致且细节更锐利的编辑。", "motivation": "现有的文本驱动3D编辑方法通常通过将预训练的2D图像编辑器适配到多视图输入来工作，但它们缺乏对多视图信息交换的明确控制，导致难以保持跨视图一致性，进而产生编辑不足和细节模糊的问题。", "method": "CoreEditor引入了关键的“对应约束注意力机制”，该机制在扩散去噪过程中强制像素之间保持精确交互。它不仅依赖几何对齐，还结合了去噪过程中估计的语义相似性，以实现更可靠的对应建模和鲁棒的多视图编辑。此外，它设计了一个“选择性编辑流程”，允许用户从多个候选结果中选择偏好，提供更大的灵活性和用户控制。", "result": "CoreEditor能够生成高质量、3D一致的编辑，并具有更锐利的细节，显著优于现有方法。", "conclusion": "CoreEditor通过创新的对应约束注意力机制和选择性编辑流程，有效解决了文本到3D编辑中的多视图一致性问题，实现了卓越的编辑质量和细节表现。"}}
{"id": "2508.11624", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11624", "abs": "https://arxiv.org/abs/2508.11624", "authors": ["Niki Foteinopoulou", "Ignas Budvytis", "Stephan Liwicki"], "title": "LoRAtorio: An intrinsic approach to LoRA Skill Composition", "comment": "32 pages, 17 figures", "summary": "Low-Rank Adaptation (LoRA) has become a widely adopted technique in\ntext-to-image diffusion models, enabling the personalisation of visual concepts\nsuch as characters, styles, and objects. However, existing approaches struggle\nto effectively compose multiple LoRA adapters, particularly in open-ended\nsettings where the number and nature of required skills are not known in\nadvance. In this work, we present LoRAtorio, a novel train-free framework for\nmulti-LoRA composition that leverages intrinsic model behaviour. Our method is\nmotivated by two key observations: (1) LoRA adapters trained on narrow domains\nproduce denoised outputs that diverge from the base model, and (2) when\noperating out-of-distribution, LoRA outputs show behaviour closer to the base\nmodel than when conditioned in distribution. The balance between these two\nobservations allows for exceptional performance in the single LoRA scenario,\nwhich nevertheless deteriorates when multiple LoRAs are loaded. Our method\noperates in the latent space by dividing it into spatial patches and computing\ncosine similarity between each patch's predicted noise and that of the base\nmodel. These similarities are used to construct a spatially-aware weight\nmatrix, which guides a weighted aggregation of LoRA outputs. To address domain\ndrift, we further propose a modification to classifier-free guidance that\nincorporates the base model's unconditional score into the composition. We\nextend this formulation to a dynamic module selection setting, enabling\ninference-time selection of relevant LoRA adapters from a large pool. LoRAtorio\nachieves state-of-the-art performance, showing up to a 1.3% improvement in\nClipScore and a 72.43% win rate in GPT-4V pairwise evaluations, and generalises\neffectively to multiple latent diffusion models.", "AI": {"tldr": "LoRAtorio是一个无需训练的多LoRA组合框架，用于文本到图像扩散模型，通过利用模型内在行为和空间感知权重聚合，解决了现有方法在多LoRA组合上的局限性，并实现了最先进的性能。", "motivation": "现有的LoRA组合方法难以有效处理多个LoRA适配器，尤其是在开放式场景中。研究动机基于两个关键观察：1) 窄域训练的LoRA输出偏离基础模型；2) 域外操作时LoRA输出更接近基础模型。这种平衡在单个LoRA场景中表现良好，但在多LoRA加载时性能下降。", "method": "LoRAtorio在潜在空间操作，将潜在空间划分为空间补丁，计算每个补丁的预测噪声与基础模型噪声之间的余弦相似度。这些相似度用于构建一个空间感知的权重矩阵，指导LoRA输出的加权聚合。为解决域漂移，还修改了无分类器引导，将基础模型的无条件得分纳入组合。该方法还扩展到动态模块选择，实现推理时从大量LoRA中选择相关适配器。", "result": "LoRAtorio实现了最先进的性能，ClipScore提升高达1.3%，在GPT-4V配对评估中胜率达72.43%，并能有效泛化到多个潜在扩散模型。", "conclusion": "LoRAtorio是一个有效的、无需训练的多LoRA组合框架，通过利用模型内在行为和创新的组合策略，显著提升了文本到图像扩散模型在复杂概念组合上的能力和泛化性。"}}
{"id": "2508.11630", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11630", "abs": "https://arxiv.org/abs/2508.11630", "authors": ["Yi-Fan Zhang", "Xingyu Lu", "Shukang Yin", "Chaoyou Fu", "Wei Chen", "Xiao Hu", "Bin Wen", "Kaiyu Jiang", "Changyi Liu", "Tianke Zhang", "Haonan Fan", "Kaibing Chen", "Jiankang Chen", "Haojie Ding", "Kaiyu Tang", "Zhang Zhang", "Liang Wang", "Fan Yang", "Tingting Gao", "Guorui Zhou"], "title": "Thyme: Think Beyond Images", "comment": "Project page: https://thyme-vl.github.io/", "summary": "Following OpenAI's introduction of the ``thinking with images'' concept,\nrecent efforts have explored stimulating the use of visual information in the\nreasoning process to enhance model performance in perception and reasoning\ntasks. However, to the best of our knowledge, no open-source work currently\noffers a feature set as rich as proprietary models (O3), which can perform\ndiverse image manipulations and simultaneously enhance logical reasoning\ncapabilities through code. In this paper, we make a preliminary attempt in this\ndirection by introducing Thyme (Think Beyond Images), a novel paradigm for\nenabling MLLMs to transcend existing ``think with images'' approaches by\nautonomously generating and executing diverse image processing and\ncomputational operations via executable code. This approach not only\nfacilitates a rich, on-the-fly set of image manipulations (e.g., cropping,\nrotation, contrast enhancement) but also allows for mathematical computations,\nall while maintaining high autonomy in deciding when and how to apply these\noperations. We activate this capability through a two-stage training strategy:\nan initial SFT on a curated dataset of 500K samples to teach code generation,\nfollowed by a RL phase to refine decision-making. For the RL stage, we manually\ncollect and design high-resolution question-answer pairs to increase the\nlearning difficulty, and we propose GRPO-ATS (Group Relative Policy\nOptimization with Adaptive Temperature Sampling), an algorithm that applies\ndistinct temperatures to text and code generation to balance reasoning\nexploration with code execution precision. We conduct extensive experimental\nanalysis and ablation studies. Comprehensive evaluations on nearly 20\nbenchmarks show that Thyme yields significant and consistent performance gains,\nparticularly in challenging high-resolution perception and complex reasoning\ntasks.", "AI": {"tldr": "Thyme是一个新范式，使多模态大语言模型（MLLMs）能通过自主生成和执行代码来处理图像和进行计算，从而在感知和推理任务中超越现有“图像思考”方法。", "motivation": "现有开源模型在图像操作和通过代码增强逻辑推理方面不如专有模型，缺乏丰富的功能集，无法像后者那样进行多样化的图像操作并同时提升逻辑推理能力。", "method": "引入Thyme，通过让MLLMs自主生成并执行多样化的图像处理和计算操作代码来实现。采用两阶段训练策略：首先在50万样本数据集上进行SFT（监督微调）以教授代码生成；然后进行RL（强化学习）阶段以优化决策制定，该阶段使用手动收集和设计的高分辨率问答对，并提出GRPO-ATS（Group Relative Policy Optimization with Adaptive Temperature Sampling）算法，该算法对文本和代码生成应用不同的温度，以平衡推理探索和代码执行精度。", "result": "在近20个基准测试中进行的全面评估显示，Thyme在性能上取得显著且一致的提升，特别是在具有挑战性的高分辨率感知和复杂推理任务中表现突出。", "conclusion": "Thyme通过允许多模态大语言模型自主生成和执行图像处理及计算代码，成功地增强了模型在复杂视觉推理任务中的能力，超越了现有“图像思考”的方法。"}}
