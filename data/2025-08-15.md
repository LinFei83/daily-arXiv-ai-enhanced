<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 31]
- [cs.CV](#cs.CV) [Total: 107]
- [cs.CL](#cs.CL) [Total: 73]
- [cs.RO](#cs.RO) [Total: 23]
- [eess.SY](#eess.SY) [Total: 8]
- [eess.IV](#eess.IV) [Total: 7]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [A Survey of Optimization Modeling Meets LLMs: Progress and Future Directions](https://arxiv.org/abs/2508.10047)
*Ziyang Xiao,Jingrong Xie,Lilin Xu,Shisi Guan,Jingyan Zhu,Xiongwei Han,Xiaojin Fu,WingYin Yu,Han Wu,Wei Shi,Qingcan Kang,Jiahui Duan,Tao Zhong,Mingxuan Yuan,Jia Zeng,Yuan Wang,Gang Chen,Dongxiang Zhang*

Main category: cs.AI

TL;DR: 该综述全面回顾了利用大型语言模型（LLMs）自动化优化建模的最新进展，分析并清理了现有基准数据集，构建了新的排行榜，并指出了未来的研究方向。


<details>
  <summary>Details</summary>
Motivation: 优化建模在解决实际问题中非常有用，但需要专业的运筹学知识。大型语言模型的出现为自动化数学建模过程带来了新机遇，因此需要对相关进展进行系统性回顾和评估。

Method: 本文综述了LLMs在优化建模中涵盖整个技术栈的最新进展，包括基础模型的数据合成和微调、推理框架、基准数据集和性能评估。此外，对基准数据集的质量进行了深入分析并进行了清理，构建了新的排行榜，并创建了一个集成清理数据集、代码和论文资源的在线门户。

Result: 发现现有基准数据集存在惊人的高错误率。通过清理数据集，构建了一个新的、公平的性能评估排行榜。同时，建立了一个在线门户，为社区提供了整合的资源。

Conclusion: 识别了当前方法的局限性，并概述了未来的研究机会，以推动LLMs在自动化优化建模领域的发展。

Abstract: By virtue of its great utility in solving real-world problems, optimization
modeling has been widely employed for optimal decision-making across various
sectors, but it requires substantial expertise from operations research
professionals. With the advent of large language models (LLMs), new
opportunities have emerged to automate the procedure of mathematical modeling.
This survey presents a comprehensive and timely review of recent advancements
that cover the entire technical stack, including data synthesis and fine-tuning
for the base model, inference frameworks, benchmark datasets, and performance
evaluation. In addition, we conducted an in-depth analysis on the quality of
benchmark datasets, which was found to have a surprisingly high error rate. We
cleaned the datasets and constructed a new leaderboard with fair performance
evaluation in terms of base LLM model and datasets. We also build an online
portal that integrates resources of cleaned datasets, code and paper repository
to benefit the community. Finally, we identify limitations in current
methodologies and outline future research opportunities.

</details>


### [2] [Amazon Nova AI Challenge -- Trusted AI: Advancing secure, AI-assisted software development](https://arxiv.org/abs/2508.10108)
*Sattvik Sahai,Prasoon Goyal,Michael Johnston,Anna Gottardi,Yao Lu,Lucy Hu,Luke Dai,Shaohua Liu,Samyuth Sagi,Hangjie Shi,Desheng Zhang,Lavina Vaz,Leslie Ball,Maureen Murray,Rahul Gupta,Shankar Ananthakrishna*

Main category: cs.AI

TL;DR: 亚马逊Nova AI挑战赛的信任AI赛道旨在通过全球大学团队竞赛推动软件开发AI的安全进展，本文概述了挑战赛中取得的进步。


<details>
  <summary>Details</summary>
Motivation: 软件开发AI系统快速发展，但其安全性仍面临巨大挑战，因此亚马逊发起了这项挑战赛来解决AI安全问题。

Method: 挑战赛分为红队（开发自动化红队机器人）和蓝队（开发安全AI助手），通过对抗性锦标赛进行评估，红队与AI编码助手进行多轮对话以测试其安全性。挑战赛提供高质量标注数据促进迭代改进。亚马逊团队构建了定制基线编码模型、开发了锦标赛编排服务和评估工具。

Result: 参赛团队开发了最先进的技术，包括基于推理的安全对齐、鲁棒模型护栏、多轮越狱和LLM高效探测等新方法。

Conclusion: 本文总结了大学团队和亚马逊Nova AI挑战赛团队在解决软件开发AI安全挑战方面取得的进展，强调了这项合作努力提升了AI安全标准。

Abstract: AI systems for software development are rapidly gaining prominence, yet
significant challenges remain in ensuring their safety. To address this, Amazon
launched the Trusted AI track of the Amazon Nova AI Challenge, a global
competition among 10 university teams to drive advances in secure AI. In the
challenge, five teams focus on developing automated red teaming bots, while the
other five create safe AI assistants. This challenge provides teams with a
unique platform to evaluate automated red-teaming and safety alignment methods
through head-to-head adversarial tournaments where red teams have multi-turn
conversations with the competing AI coding assistants to test their safety
alignment. Along with this, the challenge provides teams with a feed of high
quality annotated data to fuel iterative improvement. Throughout the challenge,
teams developed state-of-the-art techniques, introducing novel approaches in
reasoning-based safety alignment, robust model guardrails, multi-turn
jail-breaking, and efficient probing of large language models (LLMs). To
support these efforts, the Amazon Nova AI Challenge team made substantial
scientific and engineering investments, including building a custom baseline
coding specialist model for the challenge from scratch, developing a tournament
orchestration service, and creating an evaluation harness. This paper outlines
the advancements made by university teams and the Amazon Nova AI Challenge team
in addressing the safety challenges of AI for software development,
highlighting this collaborative effort to raise the bar for AI safety.

</details>


### [3] [MCP-Orchestrated Multi-Agent System for Automated Disinformation Detection](https://arxiv.org/abs/2508.10143)
*Alexandru-Andrei Avram,Adrian Groza,Alexandru Lecu*

Main category: cs.AI

TL;DR: 该论文提出一个多智能体系统，利用关系抽取技术检测新闻文章（标题和短文本片段）中的虚假信息，实现了高准确率。


<details>
  <summary>Details</summary>
Motivation: 数字平台上的虚假信息广泛传播，对信息完整性造成了严峻挑战。

Method: 所提出的Agentic AI系统包含四个智能体：(i) 机器学习智能体（逻辑回归），(ii) 维基百科知识检查智能体（依赖命名实体识别），(iii) 一致性检测智能体（使用LLM提示工程），以及 (iv) 网络爬取数据分析智能体（提取关系三元组进行事实核查）。系统通过模型上下文协议（MCP）进行编排，实现共享上下文和组件间的实时学习。采用基于单个智能体错误分类率的加权聚合方法。

Result: 多智能体集成系统达到了95.3%的准确率和0.964的F1分数，显著优于单个智能体和传统方法。数学推导的加权聚合方法被证明优于算法阈值优化。

Conclusion: 该模块化系统易于扩展，同时保持决策过程的细节，并能有效检测虚假信息，性能优于现有方法。

Abstract: The large spread of disinformation across digital platforms creates
significant challenges to information integrity. This paper presents a
multi-agent system that uses relation extraction to detect disinformation in
news articles, focusing on titles and short text snippets. The proposed Agentic
AI system combines four agents: (i) a machine learning agent (logistic
regression), (ii) a Wikipedia knowledge check agent (which relies on named
entity recognition), (iii) a coherence detection agent (using LLM prompt
engineering), and (iv) a web-scraped data analyzer that extracts relational
triplets for fact checking. The system is orchestrated via the Model Context
Protocol (MCP), offering shared context and live learning across components.
Results demonstrate that the multi-agent ensemble achieves 95.3% accuracy with
an F1 score of 0.964, significantly outperforming individual agents and
traditional approaches. The weighted aggregation method, mathematically derived
from individual agent misclassification rates, proves superior to algorithmic
threshold optimization. The modular architecture makes the system easily
scalable, while also maintaining details of the decision processes.

</details>


### [4] [Agentic AI Frameworks: Architectures, Protocols, and Design Challenges](https://arxiv.org/abs/2508.10146)
*Hana Derouiche,Zaki Brahmi,Haithem Mazeni*

Main category: cs.AI

TL;DR: 本文对主流的Agentic AI框架（如CrewAI、LangGraph、AutoGen等）进行了系统性回顾和比较分析，并深入探讨了代理间通信协议，旨在为研究人员和实践者提供全面参考。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的出现催生了Agentic AI范式，其中智能代理展现出目标导向的自主性、上下文推理和多代理协调能力。因此，需要对现有Agentic AI框架进行系统评估，并识别其局限性、趋势和挑战。

Method: 本文采用系统性回顾和比较分析的方法，评估了CrewAI、LangGraph、AutoGen、Semantic Kernel、Agno、Google ADK和MetaGPT等领先的Agentic AI框架的架构原则、通信机制、内存管理、安全防护和与面向服务计算的对齐。此外，还深入分析了Contract Net Protocol (CNP)、Agent-to-Agent (A2A)、Agent Network Protocol (ANP)和Agora等代理通信协议。

Result: 研究结果建立了Agentic AI系统的基础分类法，识别了该领域的关键局限性、新兴趋势和开放挑战。同时，对代理通信协议进行了深入分析。

Conclusion: 本文为Agentic AI系统提供了全面的参考，并提出了未来研究方向，以提高可扩展性、鲁棒性和互操作性，旨在推动下一代自主AI系统的发展。

Abstract: The emergence of Large Language Models (LLMs) has ushered in a transformative
paradigm in artificial intelligence, Agentic AI, where intelligent agents
exhibit goal-directed autonomy, contextual reasoning, and dynamic multi-agent
coordination. This paper provides a systematic review and comparative analysis
of leading Agentic AI frameworks, including CrewAI, LangGraph, AutoGen,
Semantic Kernel, Agno, Google ADK, and MetaGPT, evaluating their architectural
principles, communication mechanisms, memory management, safety guardrails, and
alignment with service-oriented computing paradigms. Furthermore, we identify
key limitations, emerging trends, and open challenges in the field. To address
the issue of agent communication, we conduct an in-depth analysis of protocols
such as the Contract Net Protocol (CNP), Agent-to-Agent (A2A), Agent Network
Protocol (ANP), and Agora. Our findings not only establish a foundational
taxonomy for Agentic AI systems but also propose future research directions to
enhance scalability, robustness, and interoperability. This work serves as a
comprehensive reference for researchers and practitioners working to advance
the next generation of autonomous AI systems.

</details>


### [5] [Improving and Evaluating Open Deep Research Agents](https://arxiv.org/abs/2508.10152)
*Doaa Allabadi,Kyle Bradbury,Jordan M. Malof*

Main category: cs.AI

TL;DR: 本文关注深度研究智能体（DRAs），提出了一种计算成本更低的基准BrowseComp-Small (BC-Small)，并在此基准上评估了唯一的开源DRA（ODR）和两个专有系统。初始结果显示所有系统准确率均为0%。通过对ODR进行三项战略性改进，得到了ODR+模型，其在BC-Small上实现了10%的成功率，达到当前最佳水平。


<details>
  <summary>Details</summary>
Motivation: 深度研究智能体（DRAs）展现出强大能力，但目前大多数是专有闭源系统，阻碍了学术研究。现有唯一的开源DRA（ODR）亟需改进，同时学术实验室需要一个计算成本更低的DRA基准。

Method: 本文将BrowseComp基准调整为计算成本更低的BrowseComp-Small (BC-Small)；使用BC-Small对开源DRA (ODR) 和两个专有系统（Anthropic、Google）进行基准测试；对ODR提出了三项战略性改进，创建了ODR+模型；进行了消融研究以评估各项改进的贡献。

Result: 在BC-Small的60个问题测试集上，ODR以及来自Anthropic和Google的两个专有系统初始准确率均为0%。经过三项战略性改进后，ODR+模型在BC-Small上实现了10%的成功率，达到开源和闭源系统中的最新最佳水平。消融研究表明所有三项改进都对ODR+的成功有所贡献。

Conclusion: 现有的深度研究智能体，无论是开源还是专有，在挑战性的BrowseComp-Small基准上表现均不佳。通过对开源ODR模型进行战略性改进（形成ODR+），可以显著提升性能，达到该基准的最新最佳水平，证明了开源系统的巨大潜力。

Abstract: We focus here on Deep Research Agents (DRAs), which are systems that can take
a natural language prompt from a user, and then autonomously search for, and
utilize, internet-based content to address the prompt. Recent DRAs have
demonstrated impressive capabilities on public benchmarks however, recent
research largely involves proprietary closed-source systems. At the time of
this work, we only found one open-source DRA, termed Open Deep Research (ODR).
In this work we adapt the challenging recent BrowseComp benchmark to compare
ODR to existing proprietary systems. We propose BrowseComp-Small (BC-Small),
comprising a subset of BrowseComp, as a more computationally-tractable DRA
benchmark for academic labs. We benchmark ODR and two other proprietary systems
on BC-Small: one system from Anthropic and one system from Google. We find that
all three systems achieve 0% accuracy on the test set of 60 questions. We
introduce three strategic improvements to ODR, resulting in the ODR+ model,
which achieves a state-of-the-art 10% success rate on BC-Small among both
closed-source and open-source systems. We report ablation studies indicating
that all three of our improvements contributed to the success of ODR+.

</details>


### [6] [Pruning Long Chain-of-Thought of Large Reasoning Models via Small-Scale Preference Optimization](https://arxiv.org/abs/2508.10164)
*Bin Hong,Jiayu Liu,Zhenya Huang,Kai Zhang,Mengdi Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种名为LCPO的方法，通过直接平衡NLL损失相关的隐式奖励，有效减少大推理模型（LRMs）的输出长度，同时保持推理性能。


<details>
  <summary>Details</summary>
Motivation: 大推理模型（LRMs）的长链式思考（CoT）输出导致计算成本增加并可能引发过度思考。现有高效推理方法通常会牺牲推理质量或需要大量资源，因此需要一种在推理效率和效果之间取得平衡的方法。

Method: 本文首先分析了生成路径分布，并通过难度估计筛选生成轨迹。随后，在基于Bradley-Terry损失的框架下，分析了各种偏好优化方法的目标收敛行为。在此基础上，提出了一种长度控制偏好优化（LCPO）方法，直接平衡与NLL损失相关的隐式奖励，以有限的数据和训练学习长度偏好。

Result: 广泛的实验表明，LCPO方法在多个基准测试中将平均输出长度显著减少了50%以上，同时保持了推理性能。

Conclusion: 该研究强调了计算高效方法在引导大推理模型实现高效推理方面的潜力。

Abstract: Recent advances in Large Reasoning Models (LRMs) have demonstrated strong
performance on complex tasks through long Chain-of-Thought (CoT) reasoning.
However, their lengthy outputs increase computational costs and may lead to
overthinking, raising challenges in balancing reasoning effectiveness and
efficiency. Current methods for efficient reasoning often compromise reasoning
quality or require extensive resources. This paper investigates efficient
methods to reduce the generation length of LRMs. We analyze generation path
distributions and filter generated trajectories through difficulty estimation.
Subsequently, we analyze the convergence behaviors of the objectives of various
preference optimization methods under a Bradley-Terry loss based framework.
Based on the analysis, we propose Length Controlled Preference Optimization
(LCPO) that directly balances the implicit reward related to NLL loss. LCPO can
effectively learn length preference with limited data and training. Extensive
experiments demonstrate that our approach significantly reduces the average
output length by over 50\% across multiple benchmarks while maintaining the
reasoning performance. Our work highlights the potential for computationally
efficient approaches in guiding LRMs toward efficient reasoning.

</details>


### [7] [KompeteAI: Accelerated Autonomous Multi-Agent System for End-to-End Pipeline Generation for Machine Learning Problems](https://arxiv.org/abs/2508.10177)
*Stepan Kulibaba,Artem Dzhalilov,Roman Pakhomov,Oleg Svidchenko,Alexander Gasnikov,Aleksei Shpilman*

Main category: cs.AI

TL;DR: KompeteAI是一种新型的基于LLM的AutoML框架，通过引入合并阶段的MCTS和RAG来增强探索，并通过预测性评分模型和加速调试方法来解决执行瓶颈，在现有和新提出的基准测试中均超越了现有领先方法。


<details>
  <summary>Details</summary>
Motivation: 当前的基于LLM的AutoML系统存在探索策略受限（如一次性方法缺乏多样性，MCTS未能有效重组）和严重的执行瓶颈（代码验证周期长，阻碍迭代优化）等局限性。

Method: 本文提出了KompeteAI框架，旨在解决现有问题：1. 动态解决方案空间探索：引入合并阶段，组合MCTS中的优秀候选方案；整合检索增强生成（RAG），从Kaggle和arXiv获取真实世界策略以扩展假设空间。2. 解决执行瓶颈：采用预测性评分模型和加速调试方法，通过早期阶段指标评估解决方案潜力，避免耗时的完整代码执行。3. 提出了Kompete-bench基准测试来补充MLE-Bench的不足。

Result: KompeteAI将管道评估速度提高了6.9倍。在主要的AutoML基准MLE-Bench上，KompeteAI平均超越了领先方法（如RD-agent、AIDE和Ml-Master）3%。此外，在本文提出的Kompete-bench基准测试中，KompeteAI也取得了最先进的结果。

Conclusion: KompeteAI通过改进探索策略和缓解执行瓶颈，有效克服了现有LLM-based AutoML系统的挑战，在多个基准测试中展现出卓越的性能，并提供了更高效的AutoML解决方案。

Abstract: Recent Large Language Model (LLM)-based AutoML systems demonstrate impressive
capabilities but face significant limitations such as constrained exploration
strategies and a severe execution bottleneck. Exploration is hindered by
one-shot methods lacking diversity and Monte Carlo Tree Search (MCTS)
approaches that fail to recombine strong partial solutions. The execution
bottleneck arises from lengthy code validation cycles that stifle iterative
refinement. To overcome these challenges, we introduce KompeteAI, a novel
AutoML framework with dynamic solution space exploration. Unlike previous MCTS
methods that treat ideas in isolation, KompeteAI introduces a merging stage
that composes top candidates. We further expand the hypothesis space by
integrating Retrieval-Augmented Generation (RAG), sourcing ideas from Kaggle
notebooks and arXiv papers to incorporate real-world strategies. KompeteAI also
addresses the execution bottleneck via a predictive scoring model and an
accelerated debugging method, assessing solution potential using early stage
metrics to avoid costly full-code execution. This approach accelerates pipeline
evaluation 6.9 times. KompeteAI outperforms leading methods (e.g., RD-agent,
AIDE, and Ml-Master) by an average of 3\% on the primary AutoML benchmark,
MLE-Bench. Additionally, we propose Kompete-bench to address limitations in
MLE-Bench, where KompeteAI also achieves state-of-the-art results

</details>


### [8] [Extending the Entropic Potential of Events for Uncertainty Quantification and Decision-Making in Artificial Intelligence](https://arxiv.org/abs/2508.10241)
*Mark Zilberman*

Main category: cs.AI

TL;DR: 本文提出将事件的熵势概念（量化离散事件对系统未来预期熵的影响）引入人工智能，以增强不确定性量化、决策制定和可解释性。


<details>
  <summary>Details</summary>
Motivation: 旨在提升人工智能系统中的不确定性量化、决策制定能力和可解释性，通过引入一个量化离散事件对未来不确定性影响的参数。

Method: 将物理学中的熵势概念调整用于AI，引入了一个以事件为中心的度量，捕捉动作、观测或其他离散事件如何影响未来时间范围的不确定性。形式化了原始和AI调整后的熵势定义，后者强调条件期望以考虑反事实情景。

Result: 熵势框架在策略评估、内在奖励设计、可解释AI和异常检测中进行了探索性应用，展示了其统一和强化智能系统中不确定性建模的潜力。通过概念性示例说明了其在强化学习、贝叶斯推断和异常检测中的应用。

Conclusion: 熵势框架为AI中的不确定性管理提供了一种理论基础、可解释且多功能的途径，它融合了热力学、信息论和机器学习的原理。

Abstract: This work demonstrates how the concept of the entropic potential of events --
a parameter quantifying the influence of discrete events on the expected future
entropy of a system -- can enhance uncertainty quantification, decision-making,
and interpretability in artificial intelligence (AI). Building on its original
formulation in physics, the framework is adapted for AI by introducing an
event-centric measure that captures how actions, observations, or other
discrete occurrences impact uncertainty at future time horizons. Both the
original and AI-adjusted definitions of entropic potential are formalized, with
the latter emphasizing conditional expectations to account for counterfactual
scenarios. Applications are explored in policy evaluation, intrinsic reward
design, explainable AI, and anomaly detection, highlighting the metric's
potential to unify and strengthen uncertainty modeling in intelligent systems.
Conceptual examples illustrate its use in reinforcement learning, Bayesian
inference, and anomaly detection, while practical considerations for
computation in complex AI models are discussed. The entropic potential
framework offers a theoretically grounded, interpretable, and versatile
approach to managing uncertainty in AI, bridging principles from
thermodynamics, information theory, and machine learning.

</details>


### [9] [Why Cannot Large Language Models Ever Make True Correct Reasoning?](https://arxiv.org/abs/2508.10265)
*Jingde Cheng*

Main category: cs.AI

TL;DR: 本文认为，大语言模型（LLMs）不具备真正的理解和推理能力，这仅仅是人们的概念模糊造成的错觉，其工作原理存在本质限制。


<details>
  <summary>Details</summary>
Motivation: AIGC工具（如ChatGPT）的广泛应用使许多人（包括专家和非专业人士）宣扬大语言模型拥有“理解能力”和“推理能力”，作者对此持反对意见。

Method: 通过解释大语言模型工作原理的本质限制，论证其无法拥有真正正确的推理能力。

Result: 大语言模型（LLMs）永远无法拥有真正的理解能力和真正的推理能力。

Conclusion: 由于其工作原理的本质限制，大语言模型无法具备真正的正确推理能力。

Abstract: Recently, with the application progress of AIGC tools based on large language
models (LLMs), led by ChatGPT, many AI experts and more non-professionals are
trumpeting the "understanding ability" and "reasoning ability" of the LLMs. The
present author considers that the so-called "understanding ability" and
"reasoning ability" of LLMs are just illusions of those people who with vague
concepts. In fact, the LLMs can never have the true understanding ability and
true reasoning ability. This paper intents to explain that, because the
essential limitations of their working principle, the LLMs can never have the
ability of true correct reasoning.

</details>


### [10] [Promoting Efficient Reasoning with Verifiable Stepwise Reward](https://arxiv.org/abs/2508.10293)
*Chuhuai Yue,Chengqi Dong,Yinan Gao,Hang He,Jiajun Chai,Guojun Yin,Wei Lin*

Main category: cs.AI

TL;DR: 本文提出了一种名为可验证分步奖励机制（VSRM）的新方法，通过在推理轨迹中奖励有效步骤和惩罚无效步骤，有效解决了大型推理模型（LRMs）的“过度思考”问题，从而在保持性能的同时显著提高了效率。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在复杂推理任务中表现出色，但常出现“过度思考”现象，即在简单问题上耗费过多计算资源，导致效率低下。现有提高效率的方法通常需要精确的任务评估来预设预算或选择模式，这限制了它们的灵活性和可靠性。

Method: 作者重新审视了过度思考的本质，认为鼓励有效步骤并惩罚无效步骤是关键。为此，提出了一种基于规则的可验证分步奖励机制（VSRM），该机制根据推理轨迹中中间状态的表现来分配奖励。此方法与分步推理任务的性质自然契合。实验中将VSRM与PPO和Reinforce++集成。

Result: 在AIME24和AIME25等标准数学推理基准上的实验表明，VSRM在保持原有推理性能的同时，显著减少了输出长度，实现了效率和准确性之间的最佳平衡。对过度思考频率和pass@k分数的进一步分析表明，该方法确实有效抑制了无效步骤，鼓励了有效推理，从根本上缓解了过度思考问题。

Conclusion: VSRM通过鼓励有效推理步骤和抑制无效步骤，成功解决了LRMs的过度思考问题。这种方法在保持推理性能的同时显著提高了计算效率，为大型推理模型提供了一个有效的平衡效率与准确性的解决方案。

Abstract: Large reasoning models (LRMs) have recently achieved significant progress in
complex reasoning tasks, aided by reinforcement learning with verifiable
rewards. However, LRMs often suffer from overthinking, expending excessive
computation on simple problems and reducing efficiency. Existing efficient
reasoning methods typically require accurate task assessment to preset token
budgets or select reasoning modes, which limits their flexibility and
reliability. In this work, we revisit the essence of overthinking and identify
that encouraging effective steps while penalizing ineffective ones is key to
its solution. To this end, we propose a novel rule-based verifiable stepwise
reward mechanism (VSRM), which assigns rewards based on the performance of
intermediate states in the reasoning trajectory. This approach is intuitive and
naturally fits the step-by-step nature of reasoning tasks. We conduct extensive
experiments on standard mathematical reasoning benchmarks, including AIME24 and
AIME25, by integrating VSRM with PPO and Reinforce++. Results show that our
method achieves substantial output length reduction while maintaining original
reasoning performance, striking an optimal balance between efficiency and
accuracy. Further analysis of overthinking frequency and pass@k score before
and after training demonstrates that our approach in deed effectively
suppresses ineffective steps and encourages effective reasoning, fundamentally
alleviating the overthinking problem. All code will be released upon
acceptance.

</details>


### [11] [A Curriculum Learning Approach to Reinforcement Learning: Leveraging RAG for Multimodal Question Answering](https://arxiv.org/abs/2508.10337)
*Chenliang Zhang,Lin Wang,Yuanyuan Lu,Yusheng Qi,Kexin Wang,Peixu Hou,Wenshi Chen*

Main category: cs.AI

TL;DR: 本文描述了大众点评信任安全团队为META CRAG-MM多模态多轮问答挑战赛提供的解决方案，该方案整合了视觉大语言模型、知识蒸馏、课程学习与强化学习，并结合了网络搜索，在比赛中取得了优异成绩。


<details>
  <summary>Details</summary>
Motivation: 参与META CRAG-MM挑战赛，构建一个能够处理多模态、多轮问答的综合检索增强生成系统，该系统需能从图像知识图谱和网络搜索结果中提取并合成信息。

Method: 对于任务1，采用基于视觉大语言模型（VLLM）的方案，通过GPT-4.1蒸馏知识进行监督微调，并结合课程学习策略指导强化学习。对于任务2和任务3，在此基础上额外利用网络搜索API引入外部知识，以处理复杂查询和多轮对话。

Result: 在任务1中获得第一名，领先第二名52.38%；在任务3中获得第三名。

Conclusion: 该研究证明了将课程学习与强化学习集成到训练流程中的有效性，以及结合网络搜索等外部知识对于处理复杂多模态多轮问答的益处，显著提高了答案准确性并减少了幻觉。

Abstract: This paper describes the solutions of the Dianping-Trust-Safety team for the
META CRAG-MM challenge. The challenge requires building a comprehensive
retrieval-augmented generation system capable for multi-modal multi-turn
question answering. The competition consists of three tasks: (1) answering
questions using structured data retrieved from an image-based mock knowledge
graph, (2) synthesizing information from both knowledge graphs and web search
results, and (3) handling multi-turn conversations that require context
understanding and information aggregation from multiple sources. For Task 1,
our solution is based on the vision large language model, enhanced by
supervised fine-tuning with knowledge distilled from GPT-4.1. We further
applied curriculum learning strategies to guide reinforcement learning,
resulting in improved answer accuracy and reduced hallucination. For Task 2 and
Task 3, we additionally leveraged web search APIs to incorporate external
knowledge, enabling the system to better handle complex queries and multi-turn
conversations. Our approach achieved 1st place in Task 1 with a significant
lead of 52.38\%, and 3rd place in Task 3, demonstrating the effectiveness of
the integration of curriculum learning with reinforcement learning in our
training pipeline.

</details>


### [12] [Multi-Agent Trust Region Policy Optimisation: A Joint Constraint Approach](https://arxiv.org/abs/2508.10340)
*Chak Lam Shek,Guangyao Shi,Pratap Tokekar*

Main category: cs.AI

TL;DR: 针对异构多智能体信赖域策略优化（HATRPO）中固定KL散度阈值导致训练缓慢和局部最优的问题，本文提出了两种自适应分配KL阈值的方法：HATRPO-W（基于KKT优化）和HATRPO-G（基于贪婪算法），显著提升了HATRPO的性能和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习（MARL）需要智能体间协调稳定的策略更新。HATRPO通过为每个智能体设定KL散度信赖域来稳定训练，但为所有智能体分配相同的KL阈值，在异构环境中会导致训练缓慢并陷入局部最优，限制了其学习效率和效果。

Method: 本文提出了两种动态分配KL散度阈值的方法：1) HATRPO-W：一种基于KKT条件的方法，在全球KL约束下优化阈值分配。2) HATRPO-G：一种贪婪算法，根据智能体的改进与散度比率来优先分配阈值。这两种方法将序贯策略优化与约束阈值调度相结合。

Result: 实验结果表明，HATRPO-W和HATRPO-G显著提升了HATRPO的性能，在多个MARL基准测试中实现了更快的收敛和更高的最终奖励。具体而言，两种方法最终性能提升均超过22.5%。值得注意的是，HATRPO-W还表现出更稳定的学习动态，方差更低。

Conclusion: 通过动态调整KL散度阈值，本文提出的HATRPO-W和HATRPO-G方法有效解决了HATRPO在异构环境中的局限性，显著提升了多智能体强化学习的性能、收敛速度和学习稳定性，为异构多智能体系统提供了更灵活有效的学习范式。

Abstract: Multi-agent reinforcement learning (MARL) requires coordinated and stable
policy updates among interacting agents. Heterogeneous-Agent Trust Region
Policy Optimization (HATRPO) enforces per-agent trust region constraints using
Kullback-Leibler (KL) divergence to stabilize training. However, assigning each
agent the same KL threshold can lead to slow and locally optimal updates,
especially in heterogeneous settings. To address this limitation, we propose
two approaches for allocating the KL divergence threshold across agents:
HATRPO-W, a Karush-Kuhn-Tucker-based (KKT-based) method that optimizes
threshold assignment under global KL constraints, and HATRPO-G, a greedy
algorithm that prioritizes agents based on improvement-to-divergence ratio. By
connecting sequential policy optimization with constrained threshold
scheduling, our approach enables more flexible and effective learning in
heterogeneous-agent settings. Experimental results demonstrate that our methods
significantly boost the performance of HATRPO, achieving faster convergence and
higher final rewards across diverse MARL benchmarks. Specifically, HATRPO-W and
HATRPO-G achieve comparable improvements in final performance, each exceeding
22.5%. Notably, HATRPO-W also demonstrates more stable learning dynamics, as
reflected by its lower variance.

</details>


### [13] [What to Ask Next? Probing the Imaginative Reasoning of LLMs with TurtleSoup Puzzles](https://arxiv.org/abs/2508.10358)
*Mengtao Zhou,Sifan Wu,Huan Zhang,Qi Sima,Bang Liu*

Main category: cs.AI

TL;DR: 本研究通过引入新的“海龟汤”游戏基准、智能体和评估协议，调查了大型语言模型（LLMs）在信息稀疏环境中的想象推理能力，揭示了LLMs在此方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有基准通常是静态的或侧重于社交推理，未能捕捉到想象推理的动态、探索性本质，因此需要一个能评估LLMs在信息稀疏环境中主动构建、测试和修正假设能力的新框架。

Method: 引入了一个基于经典“海龟汤”游戏的综合研究框架，包括基准、智能体和评估协议。具体方法包括：1) 开发了TurtleSoup-Bench，一个包含800个谜题的大规模、双语、交互式想象推理基准；2) 提出了Mosaic-Agent，一个专门用于评估LLMs在此设置下表现的新型智能体；3) 设计了多维度评估协议，衡量逻辑一致性、细节完整性和结论一致性。

Result: 对主流LLMs的实验表明，它们在想象推理方面存在明显的上限和常见的失败模式，与人类相比存在显著的性能差距。

Conclusion: 本研究为LLMs的想象推理能力提供了新见解，并为未来探索性智能体行为的研究奠定了基础。

Abstract: We investigate the capacity of Large Language Models (LLMs) for imaginative
reasoning--the proactive construction, testing, and revision of hypotheses in
information-sparse environments. Existing benchmarks, often static or focused
on social deduction, fail to capture the dynamic, exploratory nature of this
reasoning process. To address this gap, we introduce a comprehensive research
framework based on the classic "Turtle Soup" game, integrating a benchmark, an
agent, and an evaluation protocol. We present TurtleSoup-Bench, the first
large-scale, bilingual, interactive benchmark for imaginative reasoning,
comprising 800 turtle soup puzzles sourced from both the Internet and expert
authors. We also propose Mosaic-Agent, a novel agent designed to assess LLMs'
performance in this setting. To evaluate reasoning quality, we develop a
multi-dimensional protocol measuring logical consistency, detail completion,
and conclusion alignment. Experiments with leading LLMs reveal clear capability
limits, common failure patterns, and a significant performance gap compared to
humans. Our work offers new insights into LLMs' imaginative reasoning and
establishes a foundation for future research on exploratory agent behavior.

</details>


### [14] [LeanRAG: Knowledge-Graph-Based Generation with Semantic Aggregation and Hierarchical Retrieval](https://arxiv.org/abs/2508.10391)
*Yaoze Zhang,Rong Wu,Pinlong Cai,Xiaoman Wang,Guohang Yan,Song Mao,Ding Wang,Botian Shi*

Main category: cs.AI

TL;DR: LeanRAG是一种新型的检索增强生成（RAG）框架，通过创新的语义聚合算法构建可导航的知识网络，并采用结构引导的检索策略，有效解决了现有知识图谱RAG中高层摘要缺乏关联和检索效率低下的问题，显著提升了响应质量并减少了检索冗余。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法在检索到有缺陷或不完整信息时效果受损。尽管基于知识图谱的RAG已发展出分层结构，但仍存在两个关键挑战：高层概念摘要是“语义孤岛”，缺乏跨社区推理所需的显式关系；以及检索过程缺乏结构感知，常退化为低效的扁平搜索。

Method: LeanRAG采用深度协作的设计，结合了知识聚合和检索策略。首先，它使用一种新颖的语义聚合算法，形成实体簇并在聚合层摘要之间构建新的显式关系，从而创建一个完全可导航的语义网络。其次，它采用一种自底向上的、结构引导的检索策略，将查询锚定到最相关的细粒度实体，然后系统地遍历图的语义路径，以收集简洁而上下文全面的证据集。

Result: 在四个具有不同领域的挑战性问答基准测试中，LeanRAG的表现显著优于现有方法，在提高响应质量的同时，将检索冗余减少了46%。

Conclusion: LeanRAG通过其创新的语义聚合和结构引导检索机制，成功克服了现有知识图谱RAG的局限性，为大语言模型提供了更准确、高效的外部知识，显著提升了RAG的整体性能并降低了信息冗余。

Abstract: Retrieval-Augmented Generation (RAG) plays a crucial role in grounding Large
Language Models by leveraging external knowledge, whereas the effectiveness is
often compromised by the retrieval of contextually flawed or incomplete
information. To address this, knowledge graph-based RAG methods have evolved
towards hierarchical structures, organizing knowledge into multi-level
summaries. However, these approaches still suffer from two critical,
unaddressed challenges: high-level conceptual summaries exist as disconnected
``semantic islands'', lacking the explicit relations needed for cross-community
reasoning; and the retrieval process itself remains structurally unaware, often
degenerating into an inefficient flat search that fails to exploit the graph's
rich topology. To overcome these limitations, we introduce LeanRAG, a framework
that features a deeply collaborative design combining knowledge aggregation and
retrieval strategies. LeanRAG first employs a novel semantic aggregation
algorithm that forms entity clusters and constructs new explicit relations
among aggregation-level summaries, creating a fully navigable semantic network.
Then, a bottom-up, structure-guided retrieval strategy anchors queries to the
most relevant fine-grained entities and then systematically traverses the
graph's semantic pathways to gather concise yet contextually comprehensive
evidence sets. The LeanRAG can mitigate the substantial overhead associated
with path retrieval on graphs and minimizes redundant information retrieval.
Extensive experiments on four challenging QA benchmarks with different domains
demonstrate that LeanRAG significantly outperforming existing methods in
response quality while reducing 46\% retrieval redundancy. Code is available
at: https://github.com/RaZzzyz/LeanRAG

</details>


### [15] [HiRef: Leveraging Hierarchical Ontology and Network Refinement for Robust Medication Recommendation](https://arxiv.org/abs/2508.10425)
*Yan Ting Chok,Soyon Park,Seungheun Baek,Hajung Kim,Junhyun Lee,Jaewoo Kang*

Main category: cs.AI

TL;DR: 针对稀有和不完整电子健康记录（EHR）数据导致的药物推荐泛化性差问题，本文提出了HiRef框架，通过结合医学本体论的层次语义和精炼的EHR共现模式，显著提升了模型在未见代码情况下的鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于EHR的药物推荐模型在处理真实世界数据时面临挑战，因为数据中存在稀有或不完整的医学实体，导致模型难以在缺失或新颖条件下泛化，主要原因是它们过度依赖观察到的共现模式。

Method: 本文提出了HiRef框架，结合了两种互补结构：1) 医学本体论中编码的层次语义，将本体实体嵌入到双曲空间中以捕捉树状关系并实现知识迁移；2) 从EHR中提炼出的共现模式，通过先验引导的稀疏正则化方案来抑制虚假关联，同时保留临床有意义的关联。

Result: HiRef模型在EHR基准数据集（MIMIC-III和MIMIC-IV）上取得了强大的性能，并在模拟的未见代码设置下保持了高准确性。广泛的实验和全面的消融研究表明，HiRef对未见医学代码具有很强的鲁棒性。

Conclusion: HiRef通过整合层次医学本体知识和精炼EHR共现图，有效解决了稀有和不完整EHR数据在药物推荐中的挑战，显著提高了模型对未见医学代码的泛化能力和鲁棒性。

Abstract: Medication recommendation is a crucial task for assisting physicians in
making timely decisions from longitudinal patient medical records. However,
real-world EHR data present significant challenges due to the presence of
rarely observed medical entities and incomplete records that may not fully
capture the clinical ground truth. While data-driven models trained on
longitudinal Electronic Health Records often achieve strong empirical
performance, they struggle to generalize under missing or novel conditions,
largely due to their reliance on observed co-occurrence patterns. To address
these issues, we propose Hierarchical Ontology and Network Refinement for
Robust Medication Recommendation (HiRef), a unified framework that combines two
complementary structures: (i) the hierarchical semantics encoded in curated
medical ontologies, and (ii) refined co-occurrence patterns derived from
real-world EHRs. We embed ontology entities in hyperbolic space, which
naturally captures tree-like relationships and enables knowledge transfer
through shared ancestors, thereby improving generalizability to unseen codes.
To further improve robustness, we introduce a prior-guided sparse
regularization scheme that refines the EHR co-occurrence graph by suppressing
spurious edges while preserving clinically meaningful associations. Our model
achieves strong performance on EHR benchmarks (MIMIC-III and MIMIC-IV) and
maintains high accuracy under simulated unseen-code settings. Extensive
experiments with comprehensive ablation studies demonstrate HiRef's resilience
to unseen medical codes, supported by in-depth analyses of the learned
sparsified graph structure and medical code embeddings.

</details>


### [16] [MM-Food-100K: A 100,000-Sample Multimodal Food Intelligence Dataset with Verifiable Provenance](https://arxiv.org/abs/2508.10429)
*Yi Dong,Yusuke Muraoka,Scott Shi,Yi Zhang*

Main category: cs.AI

TL;DR: 本文介绍了MM-Food-100K，一个包含10万样本的多模态食物智能公开数据集，其数据来源可追溯。该数据集通过结合社区众包和AI辅助质量检查的Codatta模型收集，并验证了其在微调大型视觉-语言模型进行营养预测方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有食物数据集可能缺乏大规模、高质量、多模态以及可追溯的数据，这限制了食物智能领域，特别是基于图像的营养预测等应用的发展。因此需要一个具有可验证来源的大型多模态食物智能数据集。

Method: 研究方法包括：1. 构建MM-Food-100K数据集，它是从一个包含120万高质量食物图像的语料库中精选出的10万样本子集，包含菜品名称、创建区域等多种信息。2. 采用Codatta贡献模型进行数据收集，该模型结合了社区众包和可配置的AI辅助质量检查，并为每份提交链接一个钱包地址以实现可追溯性（目前为链下账本，计划上链）。3. 通过使用MM-Food-100K数据集对大型视觉-语言模型（如ChatGPT 5、ChatGPT OSS、Qwen-Max）进行微调，以验证其在基于图像的营养预测任务中的实用性。

Result: 使用MM-Food-100K数据集进行微调，在标准指标上，相较于开箱即用的基线模型，视觉-语言模型在营养预测任务中获得了持续的性能提升。主要结果基于MM-Food-100K子集报告。

Conclusion: MM-Food-100K是一个高质量、可追溯的多模态食物智能数据集，已公开发布，并被证明能够有效提升大型视觉-语言模型在食物智能任务（如营养预测）上的表现。其独特的众包和质量控制模型为未来大规模数据收集提供了新范式。

Abstract: We present MM-Food-100K, a public 100,000-sample multimodal food intelligence
dataset with verifiable provenance. It is a curated approximately 10% open
subset of an original 1.2 million, quality-accepted corpus of food images
annotated for a wide range of information (such as dish name, region of
creation). The corpus was collected over six weeks from over 87,000
contributors using the Codatta contribution model, which combines community
sourcing with configurable AI-assisted quality checks; each submission is
linked to a wallet address in a secure off-chain ledger for traceability, with
a full on-chain protocol on the roadmap. We describe the schema, pipeline, and
QA, and validate utility by fine-tuning large vision-language models (ChatGPT
5, ChatGPT OSS, Qwen-Max) on image-based nutrition prediction. Fine-tuning
yields consistent gains over out-of-box baselines across standard metrics; we
report results primarily on the MM-Food-100K subset. We release MM-Food-100K
for publicly free access and retain approximately 90% for potential commercial
access with revenue sharing to contributors.

</details>


### [17] [We-Math 2.0: A Versatile MathBook System for Incentivizing Visual Mathematical Reasoning](https://arxiv.org/abs/2508.10433)
*Runqi Qiao,Qiuna Tan,Peiqing Yang,Yanzi Wang,Xiaowan Wang,Enhui Wan,Sitong Zhou,Guanting Dong,Yuchen Zeng,Yida Xu,Jie Wang,Chong Sun,Chen Li,Honggang Zhang*

Main category: cs.AI

TL;DR: We-Math 2.0是一个统一系统，通过结构化数学知识系统、模型中心数据空间建模和强化学习训练范式，全面提升多模态大语言模型（MLLMs）的数学推理能力。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在复杂数学推理方面仍存在困难。现有研究主要集中在数据集构建和方法优化，但忽视了知识驱动设计和模型中心数据空间建模。

Method: 本文提出了We-Math 2.0系统，包括四个关键贡献：1) MathBook知识系统：构建了包含491个知识点和1819个基本原理的五级分层系统。2) MathBook-Standard & Pro数据集：MathBook-Standard通过双重扩展确保广泛概念覆盖，MathBook-Pro则定义了三维难度空间，为每个问题生成7个渐进变体以进行鲁棒训练。3) MathBook-RL：提出了一个两阶段强化学习框架，包括冷启动微调（与知识导向的思维链推理对齐）和渐进对齐RL（利用平均奖励学习和动态数据调度实现跨难度级别的渐进对齐）。4) MathBookEval：引入了一个涵盖所有491个知识点、具有多样推理步骤分布的综合基准。

Result: 实验结果表明，MathBook-RL在四个广泛使用的基准上与现有基线表现相当，并在MathBookEval上取得了优异成绩。

Conclusion: We-Math 2.0显著增强了MLLMs的数学推理能力，并在数学推理方面显示出有前景的泛化能力。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated impressive
capabilities across various tasks, but still struggle with complex mathematical
reasoning. Existing research primarily focuses on dataset construction and
method optimization, often overlooking two critical aspects: comprehensive
knowledge-driven design and model-centric data space modeling. In this paper,
we introduce We-Math 2.0, a unified system that integrates a structured
mathematical knowledge system, model-centric data space modeling, and a
reinforcement learning (RL)-based training paradigm to comprehensively enhance
the mathematical reasoning abilities of MLLMs. The key contributions of We-Math
2.0 are fourfold: (1) MathBook Knowledge System: We construct a five-level
hierarchical system encompassing 491 knowledge points and 1,819 fundamental
principles. (2) MathBook-Standard & Pro: We develop MathBook-Standard, a
dataset that ensures broad conceptual coverage and flexibility through dual
expansion. Additionally, we define a three-dimensional difficulty space and
generate 7 progressive variants per problem to build MathBook-Pro, a
challenging dataset for robust training. (3) MathBook-RL: We propose a
two-stage RL framework comprising: (i) Cold-Start Fine-tuning, which aligns the
model with knowledge-oriented chain-of-thought reasoning; and (ii) Progressive
Alignment RL, leveraging average-reward learning and dynamic data scheduling to
achieve progressive alignment across difficulty levels. (4) MathBookEval: We
introduce a comprehensive benchmark covering all 491 knowledge points with
diverse reasoning step distributions. Experimental results show that
MathBook-RL performs competitively with existing baselines on four widely-used
benchmarks and achieves strong results on MathBookEval, suggesting promising
generalization in mathematical reasoning.

</details>


### [18] [FIRESPARQL: A LLM-based Framework for SPARQL Query Generation over Scholarly Knowledge Graphs](https://arxiv.org/abs/2508.10467)
*Xueli Pan,Victor de Boer,Jacco van Ossenbruggen*

Main category: cs.AI

TL;DR: 提出FIRESPARQL框架，通过微调LLM、结合RAG和SPARQL纠错层，显著提升了学术知识图谱上的自然语言问答（NLQ到SPARQL）性能。


<details>
  <summary>Details</summary>
Motivation: LLM在将自然语言问题转换为学术知识图谱（SKG）上的SPARQL查询时存在困难，主要表现为查询的结构不一致（缺少或冗余三元组）和语义不准确（错误的实体或属性），这是由于LLM对SKG特定内容和底层模式的暴露有限。

Method: 提出了FIRESPARQL，一个模块化框架，核心是微调的LLM，并可选地通过检索增强生成（RAG）提供上下文，以及一个SPARQL查询纠错层。在SciQA基准上，使用零样本、零样本+RAG、一次样本、微调、微调+RAG等多种配置进行评估，并与基线和SOTA方法进行比较。使用BLEU和ROUGE衡量查询准确性，使用RelaxedEM衡量查询结果准确性。

Result: 实验结果表明，微调（fine-tuning）实现了最高的整体性能，在测试集上查询准确性ROUGE-L达到0.90，结果准确性RelaxedEM达到0.85。

Conclusion: 微调是解决LLM在学术知识图谱上生成SPARQL查询时遇到的结构和语义错误问题的有效方法，显著提升了查询和结果的准确性。

Abstract: Question answering over Scholarly Knowledge Graphs (SKGs) remains a
challenging task due to the complexity of scholarly content and the intricate
structure of these graphs. Large Language Model (LLM) approaches could be used
to translate natural language questions (NLQs) into SPARQL queries; however,
these LLM-based approaches struggle with SPARQL query generation due to limited
exposure to SKG-specific content and the underlying schema. We identified two
main types of errors in the LLM-generated SPARQL queries: (i) structural
inconsistencies, such as missing or redundant triples in the queries, and (ii)
semantic inaccuracies, where incorrect entities or properties are shown in the
queries despite a correct query structure. To address these issues, we propose
FIRESPARQL, a modular framework that supports fine-tuned LLMs as a core
component, with optional context provided via retrieval-augmented generation
(RAG) and a SPARQL query correction layer. We evaluate the framework on the
SciQA Benchmark using various configurations (zero-shot, zero-shot with RAG,
one-shot, fine-tuning, and fine-tuning with RAG) and compare the performance
with baseline and state-of-the-art approaches. We measure query accuracy using
BLEU and ROUGE metrics, and query result accuracy using relaxed exact
match(RelaxedEM), with respect to the gold standards containing the NLQs,
SPARQL queries, and the results of the queries. Experimental results
demonstrate that fine-tuning achieves the highest overall performance, reaching
0.90 ROUGE-L for query accuracy and 0.85 RelaxedEM for result accuracy on the
test set.

</details>


### [19] [SEQ-GPT: LLM-assisted Spatial Query via Example](https://arxiv.org/abs/2508.10486)
*Ivan Khai Ze Lim,Ningyi Liao,Yiming Yang,Gerald Wei Yong Yip,Siqiang Luo*

Main category: cs.AI

TL;DR: 本文提出SEQ-GPT，一个由大型语言模型（LLM）驱动的空间范例查询（SEQ）系统，旨在通过自然语言和交互式反馈，实现对一组相关位置的更灵活搜索。


<details>
  <summary>Details</summary>
Motivation: 当前在线地图主要依赖用户查询进行单位置搜索，在执行复杂任务（如同时搜索一组位置）时用户体验受限。因此，需要一种更强大的方法来处理多位置、范例驱动的空间查询。

Method: 引入空间范例查询（SEQ）概念。开发了基于LLM的SEQ-GPT系统，利用LLM的语言能力实现独特的交互操作，包括向用户澄清查询细节和根据用户反馈动态调整搜索。提出了一种定制的LLM适应管道，通过对话合成和多模型协作，将自然语言与结构化空间数据和查询对齐。

Result: SEQ-GPT提供了一个端到端的演示，展示了如何使用真实数据和应用场景扩展空间搜索能力。

Conclusion: 通过利用LLM，SEQ-GPT成功地将自然语言和交互性引入空间范例查询，极大地扩展了传统空间搜索的范畴和灵活性，提升了用户在复杂空间任务中的体验。

Abstract: Contemporary spatial services such as online maps predominantly rely on user
queries for location searches. However, the user experience is limited when
performing complex tasks, such as searching for a group of locations
simultaneously. In this study, we examine the extended scenario known as
Spatial Exemplar Query (SEQ), where multiple relevant locations are jointly
searched based on user-specified examples. We introduce SEQ-GPT, a spatial
query system powered by Large Language Models (LLMs) towards more versatile SEQ
search using natural language. The language capabilities of LLMs enable unique
interactive operations in the SEQ process, including asking users to clarify
query details and dynamically adjusting the search based on user feedback. We
also propose a tailored LLM adaptation pipeline that aligns natural language
with structured spatial data and queries through dialogue synthesis and
multi-model cooperation. SEQ-GPT offers an end-to-end demonstration for
broadening spatial search with realistic data and application scenarios.

</details>


### [20] [Reverse Physician-AI Relationship: Full-process Clinical Diagnosis Driven by a Large Language Model](https://arxiv.org/abs/2508.10492)
*Shicheng Xu,Xin Huang,Zihao Wei,Liang Pang,Huawei Shen,Xueqi Cheng*

Main category: cs.AI

TL;DR: 本文提出DxDirector-7B，一个将AI从医生助手转变为诊断主导者的LLM，旨在驱动全流程诊断，显著提高诊断准确性并减轻医生工作量。


<details>
  <summary>Details</summary>
Motivation: 现有AI（尤其是LLM）在临床诊断中主要作为医生助手，只能回答特定问题，无法从模糊主诉开始驱动整个诊断流程，这限制了AI完全减轻医生工作量和提高诊断效率的能力。

Method: 提出一种范式转变，将AI重新定位为诊断过程的主要驱动者，医生作为辅助。开发了DxDirector-7B，一个具备深度思考能力的LLM，使其能够以最少医生参与驱动全流程诊断。同时，DxDirector-7B建立了一个针对误诊的责任框架，明确AI和医生之间的责任。

Result: 在罕见、复杂和真实世界病例的全流程诊断评估中，DxDirector-7B不仅实现了显著优越的诊断准确性，而且比现有最先进的医学LLM和通用LLM大幅减少了医生工作量。跨多个临床科室和任务的细致分析验证了其有效性，专家评估表明其有潜力替代医学专家。

Conclusion: DxDirector-7B标志着一个新时代，AI从传统的医生助手转变为驱动整个诊断过程的主导者，从而大大减轻医生工作量，提供了一个高效准确的诊断解决方案。

Abstract: Full-process clinical diagnosis in the real world encompasses the entire
diagnostic workflow that begins with only an ambiguous chief complaint. While
artificial intelligence (AI), particularly large language models (LLMs), is
transforming clinical diagnosis, its role remains largely as an assistant to
physicians. This AI-assisted working pattern makes AI can only answer specific
medical questions at certain parts within the diagnostic process, but lack the
ability to drive the entire diagnostic process starting from an ambiguous
complaint, which still relies heavily on human physicians. This gap limits AI's
ability to fully reduce physicians' workload and enhance diagnostic efficiency.
To address this, we propose a paradigm shift that reverses the relationship
between physicians and AI: repositioning AI as the primary director, with
physicians serving as its assistants. So we present DxDirector-7B, an LLM
endowed with advanced deep thinking capabilities, enabling it to drive the
full-process diagnosis with minimal physician involvement. Furthermore,
DxDirector-7B establishes a robust accountability framework for misdiagnoses,
delineating responsibility between AI and human physicians. In evaluations
across rare, complex, and real-world cases under full-process diagnosis
setting, DxDirector-7B not only achieves significant superior diagnostic
accuracy but also substantially reduces physician workload than
state-of-the-art medical LLMs as well as general-purpose LLMs. Fine-grained
analyses across multiple clinical departments and tasks validate its efficacy,
with expert evaluations indicating its potential to serve as a viable
substitute for medical specialists. These findings mark a new era where AI,
traditionally a physicians' assistant, now drives the entire diagnostic process
to drastically reduce physicians' workload, indicating an efficient and
accurate diagnostic solution.

</details>


### [21] [Scaling Up without Fading Out: Goal-Aware Sparse GNN for RL-based Generalized Planning](https://arxiv.org/abs/2508.10747)
*Sangwoo Jeon,Juchul Shin,Gyeong-Tae Kim,YeonJe Cho,Seongwoo Kim*

Main category: cs.AI

TL;DR: 针对基于GNN的广义规划在大型网格环境中面临的稠密图表示导致的可伸缩性问题，本文提出了一种稀疏、目标感知的GNN表示，有效解决了大规模规划任务并提高了泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有结合深度强化学习（RL）和图神经网络（GNN）的广义规划方法通常将规划状态表示为全连接图，这导致了边信息的组合爆炸和显著稀疏性，尤其在大型网格环境中。这种稠密表示稀释了节点信息，指数级增加了内存需求，并最终使得大规模问题的学习变得不可行。

Method: 提出了一种稀疏的、目标感知的GNN表示，该表示选择性地编码相关的局部关系，并明确整合与目标相关的空间特征。通过设计基于PDDL的无人机任务场景，在网格世界中进行了验证。

Result: 实验结果表明，该方法能够有效扩展到以前使用稠密图表示无法处理的更大网格尺寸，并显著提高了策略的泛化能力和成功率。

Conclusion: 本研究为解决现实世界中的大规模广义规划任务提供了实用的基础。

Abstract: Generalized planning using deep reinforcement learning (RL) combined with
graph neural networks (GNNs) has shown promising results in various symbolic
planning domains described by PDDL. However, existing approaches typically
represent planning states as fully connected graphs, leading to a combinatorial
explosion in edge information and substantial sparsity as problem scales grow,
especially evident in large grid-based environments. This dense representation
results in diluted node-level information, exponentially increases memory
requirements, and ultimately makes learning infeasible for larger-scale
problems. To address these challenges, we propose a sparse, goal-aware GNN
representation that selectively encodes relevant local relationships and
explicitly integrates spatial features related to the goal. We validate our
approach by designing novel drone mission scenarios based on PDDL within a grid
world, effectively simulating realistic mission execution environments. Our
experimental results demonstrate that our method scales effectively to larger
grid sizes previously infeasible with dense graph representations and
substantially improves policy generalization and success rates. Our findings
provide a practical foundation for addressing realistic, large-scale
generalized planning tasks.

</details>


### [22] [PASS: Probabilistic Agentic Supernet Sampling for Interpretable and Adaptive Chest X-Ray Reasoning](https://arxiv.org/abs/2508.10501)
*Yushi Feng,Junye Du,Yingying Hong,Qifan Wang,Lequan Yu*

Main category: cs.AI

TL;DR: PASS是一种新型多模态概率智能体系统，通过在多工具图上自适应采样工作流，为胸部X光（CXR）推理提供可解释的、具有概率注释的决策路径，同时平衡性能与计算成本，提升医疗AI的安全性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有工具增强型智能体系统存在三大局限性：一是决策过程不透明，缺乏信任并存在安全风险；二是多模态集成能力差，这在医疗健康任务中至关重要；三是智能体管道僵化且计算效率低下。

Method: 本文提出了PASS（Probabilistic Agentic Supernet Sampling），一个多模态框架，用于CXR推理。PASS通过在多工具图上自适应采样智能体工作流，生成带有可解释概率注释的决策路径。它利用学习到的任务条件分布，在每个超网络层自适应选择最合适的工具，提供可审计的轨迹。PASS还持续将重要发现压缩到动态记忆中，并动态决定是深化推理路径还是提前退出以提高效率。为优化性能与成本的帕累托前沿，设计了三阶段训练过程：专家知识预热、对比路径排序和成本感知强化学习。同时，引入了CAB-E基准用于严格评估。

Result: 在各种基准测试中，PASS显著优于现有强基线，在多项指标（如准确率、AUC、LLM-J）上表现出色，同时有效平衡了计算成本。这推动了可解释、自适应和多模态医疗智能体系统的新范式转变。

Conclusion: PASS是首个解决现有智能体系统在医疗领域中可解释性、多模态集成和效率挑战的多模态框架。它通过自适应采样和概率注释的决策路径，显著提升了医疗AI的安全性、可信度和性能，为未来的医疗智能体系统树立了新标准。

Abstract: Existing tool-augmented agentic systems are limited in the real world by (i)
black-box reasoning steps that undermine trust of decision-making and pose
safety risks, (ii) poor multimodal integration, which is inherently critical
for healthcare tasks, and (iii) rigid and computationally inefficient agentic
pipelines. We introduce PASS (Probabilistic Agentic Supernet Sampling), the
first multimodal framework to address these challenges in the context of Chest
X-Ray (CXR) reasoning. PASS adaptively samples agentic workflows over a
multi-tool graph, yielding decision paths annotated with interpretable
probabilities. Given the complex CXR reasoning task with multimodal medical
data, PASS leverages its learned task-conditioned distribution over the agentic
supernet. Thus, it adaptively selects the most suitable tool at each supernet
layer, offering probability-annotated trajectories for post-hoc audits and
directly enhancing medical AI safety. PASS also continuously compresses salient
findings into an evolving personalized memory, while dynamically deciding
whether to deepen its reasoning path or invoke an early exit for efficiency. To
optimize a Pareto frontier balancing performance and cost, we design a novel
three-stage training procedure, including expert knowledge warm-up, contrastive
path-ranking, and cost-aware reinforcement learning. To facilitate rigorous
evaluation, we introduce CAB-E, a comprehensive benchmark for multi-step,
safety-critical, free-form CXR reasoning. Experiments across various benchmarks
validate that PASS significantly outperforms strong baselines in multiple
metrics (e.g., accuracy, AUC, LLM-J.) while balancing computational costs,
pushing a new paradigm shift towards interpretable, adaptive, and multimodal
medical agentic systems.

</details>


### [23] [Diversity First, Quality Later: A Two-Stage Assumption for Language Model Alignment](https://arxiv.org/abs/2508.10530)
*Zetian Sun,Dongfang Li,Baotian Hu*

Main category: cs.AI

TL;DR: 本文研究了语言模型对齐中静态数据和在策略数据有效性的差异，并提出了“对齐阶段假设”来解释该现象，将对齐过程分为偏好注入和偏好微调两个阶段，并提出了识别阶段边界的方法。


<details>
  <summary>Details</summary>
Motivation: 语言模型与人类偏好对齐对构建可靠的AI系统至关重要。DPO等方法利用静态和在策略数据进行对齐，但研究发现这两种数据源的有效性存在系统性差异（例如，对Llama-3在策略数据有效性是静态数据的3倍，对Zephyr是0.4倍），需要解释和优化。

Method: 本文提出了“对齐阶段假设”，将对齐过程分为两个阶段：偏好注入阶段（受益于多样化数据）和偏好微调阶段（偏好高质量数据）。通过理论和实证分析，本文刻画了这些阶段，并提出了一种有效算法来识别它们之间的边界。实验在5个模型（Llama、Zephyr、Phi-2、Qwen、Pythia）和2种对齐方法（DPO、SLiC-HF）上进行，以验证假设的普适性。

Result: 研究表明，在策略数据并非总是最优的，其有效性与静态数据相比存在显著差异。提出的对齐阶段假设能够解释这一现象：偏好注入阶段需要多样性数据，而偏好微调阶段需要高质量数据。通过实验，本文成功识别了不同对齐阶段之间的边界，并证明了该假设和边界测量方法的通用性。

Conclusion: 语言模型对齐过程可以被划分为偏好注入和偏好微调两个截然不同的阶段，每个阶段对数据类型（多样性 vs. 高质量）有不同的偏好。理解并识别这些阶段有助于优化对齐算法，从而更有效地利用数据，提升语言模型的对齐效果。

Abstract: The alignment of language models (LMs) with human preferences is critical for
building reliable AI systems. The problem is typically framed as optimizing an
LM policy to maximize the expected reward that reflects human preferences.
Recently, Direct Preference Optimization (DPO) was proposed as a LM alignment
method that directly optimize the policy from static preference data, and
further improved by incorporating on-policy sampling (i.e., preference
candidates generated during the training loop) for better LM alignment.
However, we show on-policy data is not always optimal, with systematic
effectiveness difference emerging between static and on-policy preference
candidates. For example, on-policy data can result in a 3$\times$ effectiveness
compared with static data for Llama-3, and a 0.4$\times$ effectiveness for
Zephyr. To explain the phenomenon, we propose the alignment stage assumption,
which divides the alignment process into two distinct stages: the preference
injection stage, which benefits from diverse data, and the preference
fine-tuning stage, which favors high-quality data. Through theoretical and
empirical analysis, we characterize these stages and propose an effective
algorithm to identify the boundaries between them. We perform experiments on 5
models (Llama, Zephyr, Phi-2, Qwen, Pythia) and 2 alignment methods (DPO,
SLiC-HF) to show the generalizability of alignment stage assumption and
boundary measurement.

</details>


### [24] [Improving Value-based Process Verifier via Low-Cost Variance Reduction](https://arxiv.org/abs/2508.10539)
*Zetian Sun,Dongfang Li,Baotian Hu,Min Zhang*

Main category: cs.AI

TL;DR: 本文提出ComMCS方法，通过线性组合当前和后续步骤的蒙特卡洛估计器，在不增加LLM推理成本的情况下，有效降低了基于价值的过程验证器中的估计误差方差，从而提升LLM在数学推理任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在数学等复杂领域的推理能力仍面临挑战。基于价值的过程验证器是提高推理能力的有效方法，但其训练标注因LLM推理成本高昂导致蒙特卡洛（MC）样本有限，从而产生高方差的估计误差，影响其有效性。

Method: 作者识别出估计误差主要来源于高方差而非偏差，且MC估计器是最小方差无偏估计器（MVUE）。为解决此问题，他们提出了复合蒙特卡洛采样（ComMCS）方法，通过线性组合当前和后续步骤的MC估计器来构建一个无偏估计器。

Result: 理论上，ComMCS能在不增加额外LLM推理成本的情况下，实现可预测的方差降低并保持无偏估计。在MATH-500和GSM8K基准测试上的实证结果表明，ComMCS在MATH-500的Best-of-32采样实验中，比基于回归的优化方法高出2.8点，比未进行方差缩减的基线高出2.2点。

Conclusion: ComMCS方法通过有效降低蒙特卡洛估计的方差，显著提升了基于价值的过程验证器在LLM数学推理任务上的性能，且无需额外计算成本，为改进LLM的复杂推理能力提供了一条有效途径。

Abstract: Large language models (LLMs) have achieved remarkable success in a wide range
of tasks. However, their reasoning capabilities, particularly in complex
domains like mathematics, remain a significant challenge. Value-based process
verifiers, which estimate the probability of a partial reasoning chain leading
to a correct solution, are a promising approach for improving reasoning.
Nevertheless, their effectiveness is often hindered by estimation error in
their training annotations, a consequence of the limited number of Monte Carlo
(MC) samples feasible due to the high cost of LLM inference. In this paper, we
identify that the estimation error primarily arises from high variance rather
than bias, and the MC estimator is a Minimum Variance Unbiased Estimator
(MVUE). To address the problem, we propose the \textsc{Com}pound \textsc{M}onte
\textsc{C}arlo \textsc{S}ampling (ComMCS) method, which constructs an unbiased
estimator by linearly combining the MC estimators from the current and
subsequent steps. Theoretically, we show that our method leads to a predictable
reduction in variance, while maintaining an unbiased estimation without
additional LLM inference cost. We also perform empirical experiments on the
MATH-500 and GSM8K benchmarks to demonstrate the effectiveness of our method.
Notably, ComMCS outperforms regression-based optimization method by 2.8 points,
the non-variance-reduced baseline by 2.2 points on MATH-500 on Best-of-32
sampling experiment.

</details>


### [25] [MSRS: Adaptive Multi-Subspace Representation Steering for Attribute Alignment in Large Language Models](https://arxiv.org/abs/2508.10599)
*Xinyan Jiang,Lin Zhang,Jiayi Zhang,Qingsong Yang,Guimin Hu,Di Wang,Lijie Hu*

Main category: cs.AI

TL;DR: MSRS是一种通过子空间表示微调实现多属性LLM行为控制的新框架，它通过正交子空间减少属性间干扰，并结合共享与特定子空间进行动态权重调整，实现细粒度、高效的控制。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型激活引导方法难以同时引导多个属性，常导致属性间相互干扰和不良权衡。

Method: 提出多子空间表示引导（MSRS）框架。通过为每个属性分配正交子空间来隔离其影响，减少属性间干扰；采用混合子空间组合策略，结合属性特定子空间和共享子空间；引入动态加权函数学习有效整合这些组件；在推理时，通过令牌级引导机制动态识别并干预语义相关的令牌。

Result: 实验结果表明，MSRS显著减少了属性冲突，在各种属性上超越了现有方法，并能有效泛化到多样化的下游任务。

Conclusion: MSRS是一种用于有效多属性LLM引导的新颖框架，通过其独特的子空间表示和令牌级干预机制，显著提升了LLM行为控制的精度和效率。

Abstract: Activation steering offers a promising approach to controlling the behavior
of Large Language Models by directly manipulating their internal activations.
However, most existing methods struggle to jointly steer multiple attributes,
often resulting in interference and undesirable trade-offs. To address this
challenge, we propose Multi-Subspace Representation Steering (MSRS), a novel
framework for effective multi-attribute steering via subspace representation
fine-tuning. MSRS reduces inter-attribute interference by allocating orthogonal
subspaces to each attribute, isolating their influence within the model's
representation space. MSRS also incorporates a hybrid subspace composition
strategy: it combines attribute-specific subspaces for unique steering
directions with a shared subspace for common steering directions. A dynamic
weighting function learns to efficiently integrate these components for precise
control. During inference, MSRS introduces a token-level steering mechanism
that dynamically identifies and intervenes on the most semantically relevant
tokens, enabling fine-grained behavioral modulation. Experimental results show
that MSRS significantly reduces attribute conflicts, surpasses existing methods
across a range of attributes, and generalizes effectively to diverse downstream
tasks.

</details>


### [26] [STEP: Stepwise Curriculum Learning for Context-Knowledge Fusion in Conversational Recommendation](https://arxiv.org/abs/2508.10669)
*Zhenye Yang,Jinpeng Chen,Huan Li,Xiongnan Jin,Xuanyang Li,Junwei Zhang,Hongbo Gao,Kaimin Wei,Senzhang Wang*

Main category: cs.AI

TL;DR: 本文提出STEP，一个基于预训练语言模型的对话推荐系统，通过课程引导的上下文-知识融合和轻量级任务特定提示调优，有效解决了现有系统在深层语义理解和知识图谱集成方面的挑战，提升了推荐精度和对话质量。


<details>
  <summary>Details</summary>
Motivation: 现有对话推荐系统难以捕获用户偏好和对话上下文的深层语义；将外部知识图谱信息有效集成到对话生成和推荐中仍是紧迫问题；传统方法直接结合知识图谱信息与对话内容，难以处理复杂语义关系，导致推荐不符合用户预期。

Method: 引入STEP系统，其核心是一个F-Former模块，通过三阶段课程逐步对齐对话上下文与知识图谱实体，解决细粒度语义不匹配。融合后的表示通过两个轻量级且自适应的前缀提示（对话前缀和推荐前缀）注入到冻结的预训练语言模型中，分别引导响应生成和物品排序。这种双提示方案允许模型共享跨任务语义，同时尊重对话和推荐的不同目标。

Result: 实验结果表明，STEP在两个公共数据集上，在推荐精度和对话质量方面均优于主流方法。

Conclusion: STEP通过其创新的上下文-知识融合和双提示调优机制，成功解决了对话推荐系统中深层语义捕获和知识图谱有效集成的问题，显著提升了系统性能。

Abstract: Conversational recommender systems (CRSs) aim to proactively capture user
preferences through natural language dialogue and recommend high-quality items.
To achieve this, CRS gathers user preferences via a dialog module and builds
user profiles through a recommendation module to generate appropriate
recommendations. However, existing CRS faces challenges in capturing the deep
semantics of user preferences and dialogue context. In particular, the
efficient integration of external knowledge graph (KG) information into
dialogue generation and recommendation remains a pressing issue. Traditional
approaches typically combine KG information directly with dialogue content,
which often struggles with complex semantic relationships, resulting in
recommendations that may not align with user expectations.
  To address these challenges, we introduce STEP, a conversational recommender
centered on pre-trained language models that combines curriculum-guided
context-knowledge fusion with lightweight task-specific prompt tuning. At its
heart, an F-Former progressively aligns the dialogue context with
knowledge-graph entities through a three-stage curriculum, thus resolving
fine-grained semantic mismatches. The fused representation is then injected
into the frozen language model via two minimal yet adaptive prefix prompts: a
conversation prefix that steers response generation toward user intent and a
recommendation prefix that biases item ranking toward knowledge-consistent
candidates. This dual-prompt scheme allows the model to share cross-task
semantics while respecting the distinct objectives of dialogue and
recommendation. Experimental results show that STEP outperforms mainstream
methods in the precision of recommendation and dialogue quality in two public
datasets.

</details>


### [27] [GenOM: Ontology Matching with Description Generation and Large Language Model](https://arxiv.org/abs/2508.10703)
*Yiping Song,Jiaoyan Chen,Renate A. Schmidt*

Main category: cs.AI

TL;DR: GenOM是一个基于LLM的本体匹配框架，通过生成文本定义丰富概念语义，结合嵌入模型检索和精确匹配工具，在生物医学领域本体匹配任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 本体匹配（OM）对于实现异构知识源间的语义互操作性和集成至关重要，尤其在包含大量复杂概念（如疾病和药物）的生物医学领域。

Method: GenOM框架基于大型语言模型（LLM），通过生成文本定义来丰富本体概念的语义表示；使用嵌入模型检索对齐候选；并整合基于精确匹配的工具以提高精度。此外，还采用了少样本提示。

Result: 在OAEI Bio-ML赛道上的大量实验表明，GenOM通常能达到具有竞争力的性能，超越了许多基线，包括传统OM系统和近期基于LLM的方法。进一步的消融研究证实了语义丰富和少样本提示的有效性。

Conclusion: GenOM是一个鲁棒且适应性强的框架，能够有效地利用LLM进行生物医学领域的本体匹配，通过语义丰富和精确匹配策略显著提升性能。

Abstract: Ontology matching (OM) plays an essential role in enabling semantic
interoperability and integration across heterogeneous knowledge sources,
particularly in the biomedical domain which contains numerous complex concepts
related to diseases and pharmaceuticals. This paper introduces GenOM, a large
language model (LLM)-based ontology alignment framework, which enriches the
semantic representations of ontology concepts via generating textual
definitions, retrieves alignment candidates with an embedding model, and
incorporates exact matching-based tools to improve precision. Extensive
experiments conducted on the OAEI Bio-ML track demonstrate that GenOM can often
achieve competitive performance, surpassing many baselines including
traditional OM systems and recent LLM-based methods. Further ablation studies
confirm the effectiveness of semantic enrichment and few-shot prompting,
highlighting the framework's robustness and adaptability.

</details>


### [28] [Agentic Design Review System](https://arxiv.org/abs/2508.10745)
*Sayan Nag,K J Joseph,Koustava Goswami,Vlad I Morariu,Balaji Vasan Srinivasan*

Main category: cs.AI

TL;DR: 本文提出了一个基于多智能体协作的图形设计评估系统AgenticDRS，通过图匹配和提示扩展技术使智能体具备设计感知能力，并引入DRS-BENCH基准进行评估，证明了其在设计评估和生成可操作反馈方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 图形设计评估涉及对齐、构图、美学和色彩选择等多个方面，且需要聚合专家反馈进行整体评估。现有方法可能无法有效处理这种多维度、整体性的评估需求，因此需要一种更系统、智能的评估方法。

Method: 本文提出了Agentic Design Review System (AgenticDRS)，其中多个智能体在元智能体的协调下协同分析设计。关键方法包括：1) 基于图匹配的新型上下文示例选择方法；2) 独特的提示扩展方法，使每个智能体都具备设计感知能力。为评估该框架，还提出了DRS-BENCH基准。

Result: 通过与现有最先进基线进行彻底的实验评估，并辅以关键的消融实验，证明了Agentic-DRS在评估图形设计和生成可操作反馈方面的有效性。该系统表现优于现有基线。

Conclusion: Agentic-DRS能够有效评估图形设计并提供可操作的反馈。这项工作旨在吸引对图形设计智能评估这一实用但尚未充分探索的研究方向的关注。

Abstract: Evaluating graphic designs involves assessing it from multiple facets like
alignment, composition, aesthetics and color choices. Evaluating designs in a
holistic way involves aggregating feedback from individual expert reviewers.
Towards this, we propose an Agentic Design Review System (AgenticDRS), where
multiple agents collaboratively analyze a design, orchestrated by a meta-agent.
A novel in-context exemplar selection approach based on graph matching and a
unique prompt expansion method plays central role towards making each agent
design aware. Towards evaluating this framework, we propose DRS-BENCH
benchmark. Thorough experimental evaluation against state-of-the-art baselines
adapted to the problem setup, backed-up with critical ablation experiments
brings out the efficacy of Agentic-DRS in evaluating graphic designs and
generating actionable feedback. We hope that this work will attract attention
to this pragmatic, yet under-explored research direction.

</details>


### [29] [Modeling Human Responses to Multimodal AI Content](https://arxiv.org/abs/2508.10769)
*Zhiqi Shen,Shaojing Fan,Danni Xu,Terence Sim,Mohan Kankanhalli*

Main category: cs.AI

TL;DR: 该研究关注AI生成内容如何影响人类感知和行为，而非仅仅识别其真伪。为此，作者构建了MhAIM数据集，进行了人类研究，提出了衡量用户参与度的新指标，并开发了一个名为T-Lens的基于LLM的系统，该系统能预测人类对多模态信息的反应，以增强LLM的人类意识能力并应对AI驱动的错误信息风险。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成内容的普及，错误信息传播的风险也随之增加。现有研究主要关注内容的真实性识别，但对于AI内容如何影响人类感知和行为知之甚少，而这在金融等领域比事实准确性更关键。

Method: 研究方法包括：1) 构建MhAIM数据集，包含154,552个在线帖子（其中111,153个为AI生成），用于大规模分析人类对AI生成内容的反应。2) 进行人类研究，观察人们如何识别AI内容。3) 提出三个新指标：可信度、影响力和开放性，以量化用户对在线内容的判断和参与。4) 开发T-Lens，一个基于LLM的智能体系统，通过整合预测的人类对多模态信息的反应来回答用户查询。5) T-Lens的核心是HR-MCP（人类响应模型上下文协议），它基于标准化的MCP构建，可与任何LLM无缝集成。

Result: 主要结果显示：1) 人们在帖子同时包含文本和视觉内容时，尤其当两者存在不一致时，更能识别出AI生成内容。2) T-Lens系统，通过整合HR-MCP，能更好地与人类反应保持一致，从而增强了LLM的可解释性和交互能力。

Conclusion: 该工作提供了实证见解和实用工具，旨在赋予LLM人类意识能力。通过揭示AI、人类认知和信息接收之间复杂的相互作用，研究结果提出了可行的策略，以减轻AI驱动的错误信息带来的风险。

Abstract: As AI-generated content becomes widespread, so does the risk of
misinformation. While prior research has primarily focused on identifying
whether content is authentic, much less is known about how such content
influences human perception and behavior. In domains like trading or the stock
market, predicting how people react (e.g., whether a news post will go viral),
can be more critical than verifying its factual accuracy. To address this, we
take a human-centered approach and introduce the MhAIM Dataset, which contains
154,552 online posts (111,153 of them AI-generated), enabling large-scale
analysis of how people respond to AI-generated content. Our human study reveals
that people are better at identifying AI content when posts include both text
and visuals, particularly when inconsistencies exist between the two. We
propose three new metrics: trustworthiness, impact, and openness, to quantify
how users judge and engage with online content. We present T-Lens, an LLM-based
agent system designed to answer user queries by incorporating predicted human
responses to multimodal information. At its core is HR-MCP (Human Response
Model Context Protocol), built on the standardized Model Context Protocol
(MCP), enabling seamless integration with any LLM. This integration allows
T-Lens to better align with human reactions, enhancing both interpretability
and interaction capabilities. Our work provides empirical insights and
practical tools to equip LLMs with human-awareness capabilities. By
highlighting the complex interplay among AI, human cognition, and information
reception, our findings suggest actionable strategies for mitigating the risks
of AI-driven misinformation.

</details>


### [30] [The Knowledge-Reasoning Dissociation: Fundamental Limitations of LLMs in Clinical Natural Language Inference](https://arxiv.org/abs/2508.10777)
*Maël Jullien,Marco Valentino,André Freitas*

Main category: cs.AI

TL;DR: 研究质疑了LLM通过扩展数据和参数就能获得结构化表示的假设。通过临床试验NLI基准测试，并使用GKMRV探针区分知识和推理，发现LLM虽拥有知识但推理能力差，且输出一致性高，表明其依赖启发式而非可靠推理。


<details>
  <summary>Details</summary>
Motivation: 质疑大型语言模型（LLMs）仅通过扩展数据和参数就能自然获得日益结构化、可泛化的内部表示的普遍假设，并希望在临床试验等高风险领域探究其推理能力，同时区分模型是缺乏事实知识还是缺乏推理能力。

Method: 引入了一个临床试验自然语言推理（NLI）基准测试，包含因果归因、组合基础、认知验证和风险状态抽象四种推理类型。每个测试项都配有一个针对性的“基础知识和元级别推理验证”（GKMRV）探针，用于区分事实知识获取失败与推理失败。评估了六个当代LLM，并使用了直接提示和思维链提示两种方式。

Result: 模型在GKMRV准确率上接近满分（平均准确率0.918），表明它们拥有相关临床知识。然而，在主要的推理任务上表现不佳（平均准确率0.25）。尽管准确率低，但输出推理在不同样本间高度一致（平均0.87），这表明模型系统性地应用了底层启发式方法和捷径。结果揭示了LLM在结构和表示上的根本局限性：它们常具备相关临床知识，但缺乏可靠运用这些知识所需的结构化、可组合的内部表示（例如，整合约束、权衡证据或模拟反事实）。

Conclusion: 通过GKMRV将知识与推理解耦，使这种分离变得明确且可衡量，为在高风险领域探测LLM的可靠性提供了一个有效框架。当前LLM尽管拥有相关知识，但缺乏进行可靠推理所需的结构化、可组合的内部表示。

Abstract: Large language models are often assumed to acquire increasingly structured,
generalizable internal representations simply by scaling data and parameters.
We interrogate this assumption by introducing a Clinical Trial Natural Language
Inference benchmark comprising four reasoning families, Causal Attribution,
Compositional Grounding, Epistemic Verification, and Risk State Abstraction.
Each item is paired with a targeted Ground Knowledge and Meta-Level Reasoning
Verification (GKMRV) probe, allowing us to dissociate failures of factual
access from failures of inference. We evaluate six contemporary LLMs under both
direct and chain of thought prompting.
  Models achieve near-ceiling GKMRV accuracy (mean accuracy 0.918) yet perform
poorly on the main reasoning tasks (mean accuracy 0.25). Despite low accuracy,
output inferences are highly consistent across samples (mean 0.87), indicating
a systematic application of underlying heuristics and shortcuts.
  These results reveal fundamental structural and representational limitations:
current LLMs often possess the relevant clinical knowledge but lack the
structured, composable internal representations needed to deploy it reliably
(e.g., integrating constraints, weighing evidence, or simulating
counterfactuals). Decoupling knowledge from reasoning with GKMRV makes this
dissociation explicit and measurable, providing an effective framework for
probing the reliability of LLMs in high-stakes domains.

</details>


### [31] [Who Benefits from AI Explanations? Towards Accessible and Interpretable Systems](https://arxiv.org/abs/2508.10806)
*Maria J. P. Peixoto,Akriti Pandey,Ahsan Zaman,Peter R. Lewis*

Main category: cs.AI

TL;DR: 本研究探讨了可解释人工智能（XAI）对视障用户的可访问性不足问题。通过文献综述和包容性XAI设计概念验证，发现现有XAI研究忽视残障用户且多依赖视觉解释，并提出简化解释和多模态呈现对非视觉用户更有效。


<details>
  <summary>Details</summary>
Motivation: 尽管可解释人工智能（XAI）的可用性日益受到关注，但其方法的可访问性，特别是对视障用户而言，仍未得到充分探索。现有XAI解释多依赖视觉格式，且评估中很少包含残障用户，导致存在显著的可访问性鸿沟。

Method: 本研究采用双重方法：1. 对79项相关研究进行文献综述，分析XAI技术评估中是否包含残障用户以及解释格式；2. 提出一个四部分的方法论概念验证框架，用于操作化包容性XAI设计，包括AI系统分类、用户画像定义、原型设计与实现，以及专家和用户对XAI技术可访问性的评估。

Result: 1. 文献综述显示，XAI技术评估中很少包含残障用户，且大多数解释依赖于固有的视觉格式。2. 初步发现表明，对于非视觉用户，简化解释比详细解释更易理解。3. 需要多模态呈现才能实现更公平的可解释性。

Conclusion: 为了实现更具包容性的XAI，需要特别关注视障用户的需求，采用简化解释和多模态呈现方式，以弥合当前XAI方法中的可访问性鸿沟，从而实现更公平、可理解的决策支持。

Abstract: As AI systems are increasingly deployed to support decision-making in
critical domains, explainability has become a means to enhance the
understandability of these outputs and enable users to make more informed and
conscious choices. However, despite growing interest in the usability of
eXplainable AI (XAI), the accessibility of these methods, particularly for
users with vision impairments, remains underexplored. This paper investigates
accessibility gaps in XAI through a two-pronged approach. First, a literature
review of 79 studies reveals that evaluations of XAI techniques rarely include
disabled users, with most explanations relying on inherently visual formats.
Second, we present a four-part methodological proof of concept that
operationalizes inclusive XAI design: (1) categorization of AI systems, (2)
persona definition and contextualization, (3) prototype design and
implementation, and (4) expert and user assessment of XAI techniques for
accessibility. Preliminary findings suggest that simplified explanations are
more comprehensible for non-visual users than detailed ones, and that
multimodal presentation is required for more equitable interpretability.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [32] [Stochastic-based Patch Filtering for Few-Shot Learning](https://arxiv.org/abs/2508.10066)
*Javier Rodenas,Eduardo Aguilar,Petia Radeva*

Main category: cs.CV

TL;DR: 针对食物图像的视觉复杂性和可变性导致的少样本学习挑战，本文提出了基于随机的补丁过滤方法（SPFF），通过过滤与类别表示相关性较低的补丁，有效聚焦于关键特征，提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 食物图像（如意大利面）在不同配菜、盘子、光照和视角下具有高度的视觉复杂性和可变性，导致少样本学习模型在比较查询图像和支持图像时，难以聚焦于最重要的元素，从而引发错误分类。

Method: 本文提出SPFF方法，其核心是随机过滤补丁嵌入。具体来说，与类别感知嵌入相似度较低的补丁被丢弃的可能性更大。在补丁嵌入经过概率性过滤后，使用一个相似度矩阵来量化查询图像与其支持图像之间的关系。

Result: 定性分析表明SPFF能有效聚焦于类别特有的食物特征最突出的补丁，同时成功过滤掉不相关的补丁。在Food-101、VireoFood-172和UECFood-256等少样本分类基准上的大量实验验证了该方法的有效性，并超越了现有的最先进方法。

Conclusion: SPFF通过随机过滤不相关的补丁嵌入，使模型在少样本学习中更专注于食物图像的关键类别特征，从而解决了食物图像的复杂性挑战，显著提升了分类性能，达到了最先进的水平。

Abstract: Food images present unique challenges for few-shot learning models due to
their visual complexity and variability. For instance, a pasta dish might
appear with various garnishes on different plates and in diverse lighting
conditions and camera perspectives. This problem leads to losing focus on the
most important elements when comparing the query with support images, resulting
in misclassification. To address this issue, we propose Stochastic-based Patch
Filtering for Few-Shot Learning (SPFF) to attend to the patch embeddings that
show greater correlation with the class representation. The key concept of SPFF
involves the stochastic filtering of patch embeddings, where patches less
similar to the class-aware embedding are more likely to be discarded. With
patch embedding filtered according to the probability of appearance, we use a
similarity matrix that quantifies the relationship between the query image and
its respective support images. Through a qualitative analysis, we demonstrate
that SPFF effectively focuses on patches where class-specific food features are
most prominent while successfully filtering out non-relevant patches. We
validate our approach through extensive experiments on few-shot classification
benchmarks: Food-101, VireoFood-172 and UECFood-256, outperforming the existing
SoA methods.

</details>


### [33] [DINOv3](https://arxiv.org/abs/2508.10104)
*Oriane Siméoni,Huy V. Vo,Maximilian Seitzer,Federico Baldassarre,Maxime Oquab,Cijo Jose,Vasil Khalidov,Marc Szafraniec,Seungeun Yi,Michaël Ramamonjisoa,Francisco Massa,Daniel Haziza,Luca Wehrstedt,Jianyuan Wang,Timothée Darcet,Théo Moutakanni,Leonel Sentana,Claire Roberts,Andrea Vedaldi,Jamie Tolan,John Brandt,Camille Couprie,Julien Mairal,Hervé Jégou,Patrick Labatut,Piotr Bojanowski*

Main category: cs.CV

TL;DR: DINOv3是一个自监督视觉基础模型，通过扩展数据和模型规模、引入Gram anchoring以及应用后处理策略，实现了在多种视觉任务上超越现有专业模型的性能，无需微调，并能生成高质量的密集特征。


<details>
  <summary>Details</summary>
Motivation: 自监督学习有望消除手动数据标注的需求，使模型能轻松扩展到大规模数据集和更大架构。它旨在通过单一算法从自然图像到航空图像等多样化来源学习视觉表示，且不针对特定任务或领域。

Method: 1. 通过精心的准备、设计和优化，充分利用数据集和模型规模扩展的优势。2. 引入名为“Gram anchoring”的新方法，有效解决训练时间过长导致密集特征图退化的问题。3. 应用后处理策略，进一步增强模型在分辨率、模型大小以及与文本对齐方面的灵活性。

Result: DINOv3作为一个多功能的视觉基础模型，在广泛的设置下，无需微调即可超越专业领域的最新技术。它能生成高质量的密集特征，在各种视觉任务上表现出色，显著超越了之前的自监督和弱监督基础模型。

Conclusion: DINOv3是一个实现自监督学习愿景的重要里程碑，它提供了一套可扩展的视觉模型，旨在通过提供适用于各种资源限制和部署场景的解决方案，推进广泛任务和数据上的最新技术。

Abstract: Self-supervised learning holds the promise of eliminating the need for manual
data annotation, enabling models to scale effortlessly to massive datasets and
larger architectures. By not being tailored to specific tasks or domains, this
training paradigm has the potential to learn visual representations from
diverse sources, ranging from natural to aerial images -- using a single
algorithm. This technical report introduces DINOv3, a major milestone toward
realizing this vision by leveraging simple yet effective strategies. First, we
leverage the benefit of scaling both dataset and model size by careful data
preparation, design, and optimization. Second, we introduce a new method called
Gram anchoring, which effectively addresses the known yet unsolved issue of
dense feature maps degrading during long training schedules. Finally, we apply
post-hoc strategies that further enhance our models' flexibility with respect
to resolution, model size, and alignment with text. As a result, we present a
versatile vision foundation model that outperforms the specialized state of the
art across a broad range of settings, without fine-tuning. DINOv3 produces
high-quality dense features that achieve outstanding performance on various
vision tasks, significantly surpassing previous self- and weakly-supervised
foundation models. We also share the DINOv3 suite of vision models, designed to
advance the state of the art on a wide spectrum of tasks and data by providing
scalable solutions for diverse resource constraints and deployment scenarios.

</details>


### [34] [Empowering Morphing Attack Detection using Interpretable Image-Text Foundation Model](https://arxiv.org/abs/2508.10110)
*Sushrut Patwardhan,Raghavendra Ramachandra,Sushma Venkatesh*

Main category: cs.CV

TL;DR: 本文提出一种基于CLIP的多模态学习方法，用于零样本活体攻击检测并提供文本描述，在多种攻击技术和媒介上表现出泛化能力。


<details>
  <summary>Details</summary>
Motivation: 活体攻击检测对于确保人脸识别系统的可靠性至关重要，传统方法缺乏对检测结果的文本描述能力。

Method: 提出一种多模态学习方法，利用对比语言-图像预训练（CLIP）模型进行零样本评估，不仅实现活体攻击检测，还能预测最相关的文本片段。设计了十种不同的文本提示（包括长短提示），并在一个基于公开人脸生物识别数据集开发的活体攻击数据集上进行实验。

Result: 所提出的框架在零样本评估中，不仅能实现泛化性活体攻击检测，还能预测最相关的文本片段。在包含五种不同活体生成技术和三种不同介质的活体攻击数据集上，与现有最先进的预训练神经网络进行了广泛的评估。

Conclusion: 基于CLIP的多模态学习方法能够有效进行零样本活体攻击检测，并能提供人类可理解的文本描述，增强了检测结果的可解释性和系统的可靠性。

Abstract: Morphing attack detection has become an essential component of face
recognition systems for ensuring a reliable verification scenario. In this
paper, we present a multimodal learning approach that can provide a textual
description of morphing attack detection. We first show that zero-shot
evaluation of the proposed framework using Contrastive Language-Image
Pretraining (CLIP) can yield not only generalizable morphing attack detection,
but also predict the most relevant text snippet. We present an extensive
analysis of ten different textual prompts that include both short and long
textual prompts. These prompts are engineered by considering the human
understandable textual snippet. Extensive experiments were performed on a face
morphing dataset that was developed using a publicly available face biometric
dataset. We present an evaluation of SOTA pre-trained neural networks together
with the proposed framework in the zero-shot evaluation of five different
morphing generation techniques that are captured in three different mediums.

</details>


### [35] [Interpretable Oracle Bone Script Decipherment through Radical and Pictographic Analysis with LVLMs](https://arxiv.org/abs/2508.10113)
*Kaixin Peng,Mengyang Zhao,Haiyang Yu,Teng Fu,Bin Li*

Main category: cs.CV

TL;DR: 本文提出一种基于大视觉语言模型（LVLMs）的可解释甲骨文（OBS）释读方法，通过部首和象形语义理解弥合字形与含义之间的鸿沟，并在零样本释读和未释读甲骨文上表现出色。


<details>
  <summary>Details</summary>
Motivation: 甲骨文释读因其稀有性、抽象性和象形多样性而极具挑战。现有深度学习方法在释读上取得进展，但忽略了字形与语义之间的复杂联系，导致泛化能力和可解释性有限，尤其在零样本和未释读甲骨文场景下表现不佳。

Method: 提出一种结合部首分析和象形语义理解的可解释甲骨文释读方法。采用渐进式训练策略，引导模型从部首识别分析到象形分析再到相互分析。设计了受分析结果启发的部首-象形双重匹配机制，增强零样本释读性能。构建了包含47,157个带有甲骨文图像和象形分析文本的汉字的“象形释读甲骨文数据集”（PD-OBS）。

Result: 在公开基准测试中，该方法实现了最先进的Top-10准确率和卓越的零样本释读能力。更重要的是，模型提供了逻辑分析过程，可能为未释读甲骨文提供考古学上有价值的参考结果。

Conclusion: 该方法通过结合部首和象形语义分析，有效提升了甲骨文释读的准确性和可解释性，尤其在零样本和未释读甲骨文方面展现出巨大潜力，对数字人文和历史研究具有重要的应用价值。

Abstract: As the oldest mature writing system, Oracle Bone Script (OBS) has long posed
significant challenges for archaeological decipherment due to its rarity,
abstractness, and pictographic diversity. Current deep learning-based methods
have made exciting progress on the OBS decipherment task, but existing
approaches often ignore the intricate connections between glyphs and the
semantics of OBS. This results in limited generalization and interpretability,
especially when addressing zero-shot settings and undeciphered OBS. To this
end, we propose an interpretable OBS decipherment method based on Large
Vision-Language Models, which synergistically combines radical analysis and
pictograph-semantic understanding to bridge the gap between glyphs and meanings
of OBS. Specifically, we propose a progressive training strategy that guides
the model from radical recognition and analysis to pictographic analysis and
mutual analysis, thus enabling reasoning from glyph to meaning. We also design
a Radical-Pictographic Dual Matching mechanism informed by the analysis
results, significantly enhancing the model's zero-shot decipherment
performance. To facilitate model training, we propose the Pictographic
Decipherment OBS Dataset, which comprises 47,157 Chinese characters annotated
with OBS images and pictographic analysis texts. Experimental results on public
benchmarks demonstrate that our approach achieves state-of-the-art Top-10
accuracy and superior zero-shot decipherment capabilities. More importantly,
our model delivers logical analysis processes, possibly providing
archaeologically valuable reference results for undeciphered OBS, and thus has
potential applications in digital humanities and historical research. The
dataset and code will be released in https://github.com/PKXX1943/PD-OBS.

</details>


### [36] [Deep Learning Enables Large-Scale Shape and Appearance Modeling in Total-Body DXA Imaging](https://arxiv.org/abs/2508.10132)
*Arianna Bunnell,Devon Cataldi,Yannik Glaser,Thomas K. Wolfgruber,Steven Heymsfield,Alan B. Zonderman,Thomas L. Kelly,Peter Sadowski,John A. Shepherd*

Main category: cs.CV

TL;DR: 该研究开发并验证了一种深度学习方法，用于在全身双能X射线吸收法（TBDXA）扫描图像上自动放置关键点，并利用这些关键点进行体型和外观建模，以探索其与健康生物标志物的关联。


<details>
  <summary>Details</summary>
Motivation: TBDXA是一种低成本的全身成像技术，广泛用于评估身体成分。为了更有效地进行体型和外观建模（SAM）并研究其与健康指标的关联，需要一种自动化、高精度的关键点放置方法。

Method: 研究使用1683张手动标注的TBDXA扫描图像训练并验证了一个深度学习模型，用于自动放置关键点。随后，该方法被应用于35928张不同成像模式的TBDXA扫描图像，生成SAM特征。最后，通过双样本Kolmogorov-Smirnov检验，在两个独立队列中测试了SAM特征与健康生物标志物之间的关联。

Result: 该方法在外部测试数据集上达到了99.5%的关键点正确率。通过SAM特征分析，发现了与现有证据相符的身体成分和形状与多种健康指标（如虚弱、代谢、炎症和心血管代谢）之间的关联，并提出了新的假设。

Conclusion: 所开发的深度学习方法能够高精度地自动放置TBDXA扫描图像上的关键点，这极大地促进了体型和外观建模。该方法在探索身体成分和形状与各种健康生物标志物之间的关系方面具有显著价值，并有望生成新的医学发现。

Abstract: Total-body dual X-ray absorptiometry (TBDXA) imaging is a relatively low-cost
whole-body imaging modality, widely used for body composition assessment. We
develop and validate a deep learning method for automatic fiducial point
placement on TBDXA scans using 1,683 manually-annotated TBDXA scans. The method
achieves 99.5% percentage correct keypoints in an external testing dataset. To
demonstrate the value for shape and appearance modeling (SAM), our method is
used to place keypoints on 35,928 scans for five different TBDXA imaging modes,
then associations with health markers are tested in two cohorts not used for
SAM model generation using two-sample Kolmogorov-Smirnov tests. SAM feature
distributions associated with health biomarkers are shown to corroborate
existing evidence and generate new hypotheses on body composition and shape's
relationship to various frailty, metabolic, inflammation, and cardiometabolic
health markers. Evaluation scripts, model weights, automatic point file
generation code, and triangulation files are available at
https://github.com/hawaii-ai/dxa-pointplacement.

</details>


### [37] [MANGO: Multimodal Attention-based Normalizing Flow Approach to Fusion Learning](https://arxiv.org/abs/2508.10133)
*Thanh-Dat Truong,Christophe Bobda,Nitin Agarwal,Khoa Luu*

Main category: cs.CV

TL;DR: 本文提出了一种名为MANGO（Multimodal Attention-based Normalizing Flow）的新方法，通过可逆交叉注意力层和新型交叉注意力机制，实现显式、可解释且易处理的多模态融合学习，并在多个任务上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态融合方法依赖Transformer的注意力机制隐式学习特征关联，导致模型难以捕获各模态的本质特征，从而难以理解多模态输入的复杂结构和关联。

Method: 提出MANGO方法，核心是新的可逆交叉注意力（ICA）层，用于多模态数据的归一化流模型。为高效捕获复杂关联，ICA层设计了三种新型交叉注意力机制：模态间交叉注意力（MMCA）、模态内交叉注意力（IMCA）和可学习模态内交叉注意力（LICA）。同时引入多模态注意力归一化流以支持高维数据扩展性。

Result: 在语义分割、图像到图像翻译和电影类型分类这三个不同的多模态学习任务上，所提出的方法均取得了最先进（SoTA）的性能。

Conclusion: MANGO提供了一种显式、可解释且易处理的多模态融合学习方法，通过其新颖的可逆交叉注意力层和机制，在多模态任务上展现出卓越的性能。

Abstract: Multimodal learning has gained much success in recent years. However, current
multimodal fusion methods adopt the attention mechanism of Transformers to
implicitly learn the underlying correlation of multimodal features. As a
result, the multimodal model cannot capture the essential features of each
modality, making it difficult to comprehend complex structures and correlations
of multimodal inputs. This paper introduces a novel Multimodal Attention-based
Normalizing Flow (MANGO) approach\footnote{The source code of this work will be
publicly available.} to developing explicit, interpretable, and tractable
multimodal fusion learning. In particular, we propose a new Invertible
Cross-Attention (ICA) layer to develop the Normalizing Flow-based Model for
multimodal data. To efficiently capture the complex, underlying correlations in
multimodal data in our proposed invertible cross-attention layer, we propose
three new cross-attention mechanisms: Modality-to-Modality Cross-Attention
(MMCA), Inter-Modality Cross-Attention (IMCA), and Learnable Inter-Modality
Cross-Attention (LICA). Finally, we introduce a new Multimodal Attention-based
Normalizing Flow to enable the scalability of our proposed method to
high-dimensional multimodal data. Our experimental results on three different
multimodal learning tasks, i.e., semantic segmentation, image-to-image
translation, and movie genre classification, have illustrated the
state-of-the-art (SoTA) performance of the proposed approach.

</details>


### [38] [Improving watermelon (Citrullus lanatus) disease classification with generative artificial intelligence (GenAI)-based synthetic and real-field images via a custom EfficientNetV2-L model](https://arxiv.org/abs/2508.10156)
*Nitin Rai,Nathan S. Boyd,Gary E. Vallad,Arnold W. Schumann*

Main category: cs.CV

TL;DR: 本研究探讨了将少量真实图像与大量生成式AI合成图像结合，以提高西瓜病害分类模型的性能。结果表明，混合使用真实和合成图像的策略显著优于单独使用真实或合成图像，将加权F1分数从0.65提高到1.00，验证了混合方法在作物病害诊断中的有效性。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在生成高分辨率合成图像方面取得了显著进展，为农业计算机视觉模型训练提供了替代方案，可减少对资源密集型田间数据采集的依赖。然而，关于如何有效整合真实和合成图像以提高疾病分类性能的研究有限，因此本研究旨在填补这一空白，提高作物病害诊断的准确性。

Method: 研究使用EfficientNetV2-L模型对西瓜病害进行分类。训练数据集被分为五种处理方式：H0（仅真实图像）、H1（仅合成图像）、H2（1:1真实与合成比例）、H3（1:10真实与合成比例）和H4（H3加上随机图像以增加变异性和泛化能力）。所有模型均采用定制的EfficientNetV2-L架构，并结合了增强的微调和迁移学习技术进行训练。性能通过精确度、召回率和F1分数进行评估。

Result: 在H2、H3和H4处理方式下训练的模型表现出高精确度、召回率和F1分数。加权F1分数从H0（仅真实图像）的0.65显著提升到H3-H4（混合图像）的1.00。这表明，在大量合成图像中添加少量真实图像能够显著提高模型性能和泛化能力。

Conclusion: 合成图像不能完全替代真实图像，为了最大限度地提高作物病害分类模型的性能，必须以混合方式同时使用真实图像和合成图像。这种混合方法是提高模型性能和泛化能力的关键。

Abstract: The current advancements in generative artificial intelligence (GenAI) models
have paved the way for new possibilities for generating high-resolution
synthetic images, thereby offering a promising alternative to traditional image
acquisition for training computer vision models in agriculture. In the context
of crop disease diagnosis, GenAI models are being used to create synthetic
images of various diseases, potentially facilitating model creation and
reducing the dependency on resource-intensive in-field data collection.
However, limited research has been conducted on evaluating the effectiveness of
integrating real with synthetic images to improve disease classification
performance. Therefore, this study aims to investigate whether combining a
limited number of real images with synthetic images can enhance the prediction
accuracy of an EfficientNetV2-L model for classifying watermelon
\textit{(Citrullus lanatus)} diseases. The training dataset was divided into
five treatments: H0 (only real images), H1 (only synthetic images), H2 (1:1
real-to-synthetic), H3 (1:10 real-to-synthetic), and H4 (H3 + random images to
improve variability and model generalization). All treatments were trained
using a custom EfficientNetV2-L architecture with enhanced fine-tuning and
transfer learning techniques. Models trained on H2, H3, and H4 treatments
demonstrated high precision, recall, and F1-score metrics. Additionally, the
weighted F1-score increased from 0.65 (on H0) to 1.00 (on H3-H4) signifying
that the addition of a small number of real images with a considerable volume
of synthetic images improved model performance and generalizability. Overall,
this validates the findings that synthetic images alone cannot adequately
substitute for real images; instead, both must be used in a hybrid manner to
maximize model performance for crop disease classification.

</details>


### [39] [SynSpill: Improved Industrial Spill Detection With Synthetic Data](https://arxiv.org/abs/2508.10171)
*Aaditya Baranwal,Abdul Mueez,Jason Voelker,Guneet Bhatia,Shruti Vyas*

Main category: cs.CV

TL;DR: 针对工业安全领域数据稀缺问题，本文提出通过高质量合成数据生成和参数高效微调（PEFT）来提升视觉语言模型（VLMs）和传统目标检测器的性能，有效弥补了领域差距。


<details>
  <summary>Details</summary>
Motivation: 大规模视觉语言模型在通用视觉识别方面表现出色，但在工业泄漏检测等小众、安全关键领域性能显著下降，原因在于这些场景下事件罕见、敏感且难以标注，导致真实数据稀缺，传统微调方法不可行。

Method: 引入一个以高质量合成数据生成管道为核心的可扩展框架。利用合成数据集（SynSpill）对视觉语言模型进行参数高效微调（PEFT），并用于提升YOLO和DETR等现有目标检测器的性能。

Result: 在没有合成数据的情况下，视觉语言模型比传统检测器在未见过的泄漏场景中泛化能力更强。当使用SynSpill合成数据时，视觉语言模型和传统检测器都取得了显著性能提升，且两者性能变得相当。

Conclusion: 高保真合成数据是弥合安全关键应用领域差距的有效手段。合成数据生成与轻量级适应（PEFT）相结合，为在真实数据稀缺或难以获取的工业环境中部署视觉系统提供了一种成本效益高、可扩展的途径。

Abstract: Large-scale Vision-Language Models (VLMs) have transformed general-purpose
visual recognition through strong zero-shot capabilities. However, their
performance degrades significantly in niche, safety-critical domains such as
industrial spill detection, where hazardous events are rare, sensitive, and
difficult to annotate. This scarcity -- driven by privacy concerns, data
sensitivity, and the infrequency of real incidents -- renders conventional
fine-tuning of detectors infeasible for most industrial settings.
  We address this challenge by introducing a scalable framework centered on a
high-quality synthetic data generation pipeline. We demonstrate that this
synthetic corpus enables effective Parameter-Efficient Fine-Tuning (PEFT) of
VLMs and substantially boosts the performance of state-of-the-art object
detectors such as YOLO and DETR. Notably, in the absence of synthetic data
(SynSpill dataset), VLMs still generalize better to unseen spill scenarios than
these detectors. When SynSpill is used, both VLMs and detectors achieve marked
improvements, with their performance becoming comparable.
  Our results underscore that high-fidelity synthetic data is a powerful means
to bridge the domain gap in safety-critical applications. The combination of
synthetic generation and lightweight adaptation offers a cost-effective,
scalable pathway for deploying vision systems in industrial environments where
real data is scarce/impractical to obtain.
  Project Page: https://synspill.vercel.app

</details>


### [40] [FIND-Net -- Fourier-Integrated Network with Dictionary Kernels for Metal Artifact Reduction](https://arxiv.org/abs/2508.10617)
*Farid Tasharofi,Fuxin Fan,Melika Qahqaie,Mareike Thies,Andreas Maier*

Main category: cs.CV

TL;DR: 本文提出FIND-Net，一种结合频域和空域处理的深度学习金属伪影去除（MAR）框架，旨在有效抑制伪影并同时更好地保留图像结构细节。


<details>
  <summary>Details</summary>
Motivation: CT图像中的高密度金属植入物会产生严重的金属伪影，降低图像质量，从而使诊断和治疗计划复杂化。尽管现有深度学习算法在MAR方面取得进展，但它们往往难以在抑制伪影和保留结构细节之间取得平衡。

Method: FIND-Net框架整合了频域和空域处理。它引入了快速傅里叶卷积（FFC）层和可训练高斯滤波，将MAR视为一个在空域和频域均操作的混合任务。这种方法增强了全局上下文理解和频率选择性。

Result: 在合成数据集上，FIND-Net在MAE（降低3.07%）、SSIM（提高0.18%）和PSNR（提高0.90%）方面均优于现有最先进的MAR方法，并对不同伪影复杂性表现出鲁棒性。在真实临床CT扫描评估中，FIND-Net能有效抑制金属引起的失真，同时最大限度地减少对干净解剖区域的修改。

Conclusion: FIND-Net显著提升了金属伪影去除性能，提供了卓越的结构保留能力和更高的临床适用性。

Abstract: Metal artifacts, caused by high-density metallic implants in computed
tomography (CT) imaging, severely degrade image quality, complicating diagnosis
and treatment planning. While existing deep learning algorithms have achieved
notable success in Metal Artifact Reduction (MAR), they often struggle to
suppress artifacts while preserving structural details. To address this
challenge, we propose FIND-Net (Fourier-Integrated Network with Dictionary
Kernels), a novel MAR framework that integrates frequency and spatial domain
processing to achieve superior artifact suppression and structural
preservation. FIND-Net incorporates Fast Fourier Convolution (FFC) layers and
trainable Gaussian filtering, treating MAR as a hybrid task operating in both
spatial and frequency domains. This approach enhances global contextual
understanding and frequency selectivity, effectively reducing artifacts while
maintaining anatomical structures. Experiments on synthetic datasets show that
FIND-Net achieves statistically significant improvements over state-of-the-art
MAR methods, with a 3.07% MAE reduction, 0.18% SSIM increase, and 0.90% PSNR
improvement, confirming robustness across varying artifact complexities.
Furthermore, evaluations on real-world clinical CT scans confirm FIND-Net's
ability to minimize modifications to clean anatomical regions while effectively
suppressing metal-induced distortions. These findings highlight FIND-Net's
potential for advancing MAR performance, offering superior structural
preservation and improved clinical applicability. Code is available at
https://github.com/Farid-Tasharofi/FIND-Net

</details>


### [41] [EntropyGS: An Efficient Entropy Coding on 3D Gaussian Splatting](https://arxiv.org/abs/2508.10227)
*Yuning Huang,Jiahao Pang,Fengqing Zhu,Dong Tian*

Main category: cs.CV

TL;DR: 本文提出了一种名为EntropyGS的3D高斯Splatting（3DGS）压缩方法，通过分析高斯属性的统计分布，实现了约30倍的数据率降低，同时保持了渲染质量。


<details>
  <summary>Details</summary>
Motivation: 3DGS中的高斯创建和视图渲染任务通常在时间或设备上分离，因此高斯数据的存储、传输和压缩变得必要。

Method: 研究了3DGS高斯属性的相关性和统计分析，发现球谐AC属性遵循拉普拉斯分布，而旋转、缩放和不透明度可用高斯混合分布近似。在此基础上，提出了一种因子化和参数化的熵编码方法EntropyGS，在编码过程中估计每个高斯属性的分布参数以辅助熵编码，并根据高斯属性类型自适应地进行量化。

Result: EntropyGS在基准数据集上实现了约30倍的数据率降低，同时保持了与输入3DGS数据相似的渲染质量，并且编码和解码时间快速。

Conclusion: EntropyGS通过深入分析3DGS高斯属性的统计特性并设计相应的熵编码方案，成功解决了3DGS数据压缩问题，实现了显著的数据率降低，同时维持了高质量的渲染效果和高效的编解码速度。

Abstract: As an emerging novel view synthesis approach, 3D Gaussian Splatting (3DGS)
demonstrates fast training/rendering with superior visual quality. The two
tasks of 3DGS, Gaussian creation and view rendering, are typically separated
over time or devices, and thus storage/transmission and finally compression of
3DGS Gaussians become necessary. We begin with a correlation and statistical
analysis of 3DGS Gaussian attributes. An inspiring finding in this work reveals
that spherical harmonic AC attributes precisely follow Laplace distributions,
while mixtures of Gaussian distributions can approximate rotation, scaling, and
opacity. Additionally, harmonic AC attributes manifest weak correlations with
other attributes except for inherited correlations from a color space. A
factorized and parameterized entropy coding method, EntropyGS, is hereinafter
proposed. During encoding, distribution parameters of each Gaussian attribute
are estimated to assist their entropy coding. The quantization for entropy
coding is adaptively performed according to Gaussian attribute types. EntropyGS
demonstrates about 30x rate reduction on benchmark datasets while maintaining
similar rendering quality compared to input 3DGS data, with a fast encoding and
decoding time.

</details>


### [42] [CellSymphony: Deciphering the molecular and phenotypic orchestration of cells with single-cell pathomics](https://arxiv.org/abs/2508.10232)
*Paul H. Acosta,Pingjun Chen,Simon P. Castillo,Maria Esther Salvatierra,Yinyin Yuan,Xiaoxi Pan*

Main category: cs.CV

TL;DR: CellSymphony是一个多模态框架，它利用基础模型从Xenium空间转录组数据和组织学图像中提取单细胞级别的特征，以融合空间基因表达和形态学信息，从而实现准确的细胞类型注释和微环境识别。


<details>
  <summary>Details</summary>
Motivation: 尽管组织学图像包含丰富的形态学信息，但从其中提取鲁棒的细胞级别特征并与空间转录组数据整合仍然是一个关键挑战。

Method: 引入了CellSymphony框架，该框架利用基础模型从Xenium转录组图谱和组织学图像中提取单细胞分辨率的嵌入，并通过学习联合表示来融合空间基因表达和形态学背景。

Result: CellSymphony在三种癌症类型中实现了准确的细胞类型注释，并揭示了独特的微环境生态位。

Conclusion: 这项工作突出了基础模型和多模态融合在解析复杂组织生态系统中细胞生理和表型协调方面的潜力。

Abstract: Xenium, a new spatial transcriptomics platform, enables
subcellular-resolution profiling of complex tumor tissues. Despite the rich
morphological information in histology images, extracting robust cell-level
features and integrating them with spatial transcriptomics data remains a
critical challenge. We introduce CellSymphony, a flexible multimodal framework
that leverages foundation model-derived embeddings from both Xenium
transcriptomic profiles and histology images at true single-cell resolution. By
learning joint representations that fuse spatial gene expression with
morphological context, CellSymphony achieves accurate cell type annotation and
uncovers distinct microenvironmental niches across three cancer types. This
work highlights the potential of foundation models and multimodal fusion for
deciphering the physiological and phenotypic orchestration of cells within
complex tissue ecosystems.

</details>


### [43] [Deep Learning for Crack Detection: A Review of Learning Paradigms, Generalizability, and Datasets](https://arxiv.org/abs/2508.10256)
*Xinan Zhang,Haolin Wang,Yung-An Hsieh,Zhongyu Yang,Anthony Yezzi,Yi-Chang Tsai*

Main category: cs.CV

TL;DR: 该论文综述了深度学习在裂缝检测领域的最新趋势，涵盖了学习范式、泛化能力和数据采集方式的变化，并引入了一个新的3D激光扫描数据集3DCrack，同时进行了广泛的基准测试。


<details>
  <summary>Details</summary>
Motivation: 裂缝检测在民用基础设施（如路面、建筑）检查中至关重要。尽管深度学习已显著推动该领域发展，但新的趋势（学习范式转变、泛化能力提升、数据采集多样化）正在重塑其发展格局，需要系统分析并提供新的研究资源。

Method: 系统分析了深度学习在裂缝检测中的新兴趋势，包括学习范式（从全监督到半监督、弱监督、无监督、少样本、域适应和基础模型微调）、泛化能力（从单数据集到跨数据集评估）和数据集获取多样化（从RGB图像到专用传感器数据）。此外，引入了一个通过3D激光扫描收集的新数据集3DCrack，并进行了广泛的基准测试，为常用深度学习方法（包括最新基础模型）建立了基线。

Result: 系统地分析了裂缝检测领域深度学习方法的演变趋势；推出了一个新的3D激光扫描数据集3DCrack；通过大量基准实验，为常用深度学习方法（包括基础模型）在3DCrack数据集上建立了性能基线。

Conclusion: 研究结果为深度学习裂缝检测的演进方法和未来方向提供了深刻见解。

Abstract: Crack detection plays a crucial role in civil infrastructures, including
inspection of pavements, buildings, etc., and deep learning has significantly
advanced this field in recent years. While numerous technical and review papers
exist in this domain, emerging trends are reshaping the landscape. These shifts
include transitions in learning paradigms (from fully supervised learning to
semi-supervised, weakly-supervised, unsupervised, few-shot, domain adaptation
and fine-tuning foundation models), improvements in generalizability (from
single-dataset performance to cross-dataset evaluation), and diversification in
dataset reacquisition (from RGB images to specialized sensor-based data). In
this review, we systematically analyze these trends and highlight
representative works. Additionally, we introduce a new dataset collected with
3D laser scans, 3DCrack, to support future research and conduct extensive
benchmarking experiments to establish baselines for commonly used deep learning
methodologies, including recent foundation models. Our findings provide
insights into the evolving methodologies and future directions in deep
learning-based crack detection. Project page:
https://github.com/nantonzhang/Awesome-Crack-Detection

</details>


### [44] [MRFD: Multi-Region Fusion Decoding with Self-Consistency for Mitigating Hallucinations in LVLMs](https://arxiv.org/abs/2508.10264)
*Haonan Ge,Yiwei Wang,Ming-Hsuan Yang,Yujun Cai*

Main category: cs.CV

TL;DR: 提出了一种名为MRFD的无训练解码方法，通过建模区域间一致性来减少大型视觉语言模型（LVLMs）的幻觉，显著提高事实准确性。


<details>
  <summary>Details</summary>
Motivation: LVLMs在多模态任务中表现出色，但常产生与视觉输入不一致的“幻觉”文本，原因是它们验证图像不同区域信息的能力有限。

Method: MRFD是一种无需训练的解码方法。它通过交叉注意力识别显著区域，为每个区域生成初始响应，然后基于响应间的Jensen-Shannon散度（JSD）计算可靠性权重。这些权重指导基于区域感知提示（受思维链推理启发）的、一致性感知的区域预测融合。

Result: 在多个LVLMs和基准测试中，MRFD显著减少了幻觉并提高了响应的事实性，且无需更新模型。

Conclusion: MRFD通过建模区域间一致性，有效提升了LVLMs的事实基础能力并减少了幻觉，是一种无需训练的有效解码方法。

Abstract: Large Vision-Language Models (LVLMs) have shown strong performance across
multimodal tasks. However, they often produce hallucinations -- text that is
inconsistent with visual input, due to the limited ability to verify
information in different regions of the image. To address this, we propose
Multi-Region Fusion Decoding (MRFD), a training-free decoding method that
improves factual grounding by modeling inter-region consistency. MRFD
identifies salient regions using cross-attention, generates initial responses
for each, and computes reliability weights based on Jensen-Shannon Divergence
(JSD) among the responses. These weights guide a consistency-aware fusion of
per-region predictions, using region-aware prompts inspired by Chain-of-Thought
reasoning. Experiments across multiple LVLMs and benchmarks show that MRFD
significantly reduces hallucinations and improves response factuality without
requiring model updates.

</details>


### [45] [Pose-Robust Calibration Strategy for Point-of-Gaze Estimation on Mobile Phones](https://arxiv.org/abs/2508.10268)
*Yujie Zhao,Jiabei Zeng,Shiguang Shan*

Main category: cs.CV

TL;DR: 该研究针对注视点估计器校准后对头部姿态变化敏感的问题，提出了一种动态校准策略，通过在校准过程中引入头部姿态变化，显著提高了估计器的姿态鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 尽管基于外观的注视点（PoG）估计已有所改进，但由于个体差异，估计器难以泛化，需要进行个性化校准。然而，已校准的PoG估计器通常对头部姿态变化敏感，这限制了其实用性。

Method: 1. 构建了基准数据集MobilePoG，包含32个个体在固定或连续变化的头部姿态下注视指定点的面部图像。2. 利用该基准系统分析了校准点和头部姿态的多样性如何影响估计精度。3. 基于分析结果，提出了一种动态校准策略：用户在注视校准点的同时移动手机，从而自然引入头部姿态变化。

Result: 实验表明，在校准过程中引入更广泛的头部姿态范围，能显著提高估计器处理姿态变化的能力。所提出的动态校准策略，比传统校准方法能产生对头部姿态变化更不敏感、校准效果更好的PoG估计器。

Conclusion: 通过在校准过程中整合头部姿态变化，可以有效提升注视点估计器的姿态鲁棒性。提出的动态校准策略提供了一种用户友好且高效的方法，能够生成更精确且对头部姿态变化不敏感的注视点估计器。

Abstract: Although appearance-based point-of-gaze (PoG) estimation has improved, the
estimators still struggle to generalize across individuals due to personal
differences. Therefore, person-specific calibration is required for accurate
PoG estimation. However, calibrated PoG estimators are often sensitive to head
pose variations. To address this, we investigate the key factors influencing
calibrated estimators and explore pose-robust calibration strategies.
Specifically, we first construct a benchmark, MobilePoG, which includes facial
images from 32 individuals focusing on designated points under either fixed or
continuously changing head poses. Using this benchmark, we systematically
analyze how the diversity of calibration points and head poses influences
estimation accuracy. Our experiments show that introducing a wider range of
head poses during calibration improves the estimator's ability to handle pose
variation. Building on this insight, we propose a dynamic calibration strategy
in which users fixate on calibration points while moving their phones. This
strategy naturally introduces head pose variation during a user-friendly and
efficient calibration process, ultimately producing a better calibrated PoG
estimator that is less sensitive to head pose variations than those using
conventional calibration strategies. Codes and datasets are available at our
project page.

</details>


### [46] [High Fidelity Text to Image Generation with Contrastive Alignment and Structural Guidance](https://arxiv.org/abs/2508.10280)
*Danyi Gao*

Main category: cs.CV

TL;DR: 本文提出了一种结合文本-图像对比约束和结构引导机制的高保真图像生成方法，有效提升了文本驱动图像生成的语义对齐精度和结构一致性。


<details>
  <summary>Details</summary>
Motivation: 现有文本驱动图像生成方法在语义对齐准确性和结构一致性方面存在性能瓶颈。

Method: 该方法引入对比学习模块以建立强跨模态对齐约束；利用语义布局图或边缘草图等结构先验引导生成器进行空间级结构建模；模型联合优化对比损失、结构一致性损失和语义保留损失，采用多目标监督机制。

Result: 在COCO-2014数据集上的系统实验表明，所提方法在CLIP Score、FID和SSIM等定量指标上表现优越，有效弥补了语义对齐和结构保真度之间的差距，且未增加计算复杂度。

Conclusion: 该方法展示了生成语义清晰、结构完整图像的强大能力，为联合文本-图像建模和图像生成提供了一条可行的技术路径。

Abstract: This paper addresses the performance bottlenecks of existing text-driven
image generation methods in terms of semantic alignment accuracy and structural
consistency. A high-fidelity image generation method is proposed by integrating
text-image contrastive constraints with structural guidance mechanisms. The
approach introduces a contrastive learning module that builds strong
cross-modal alignment constraints to improve semantic matching between text and
image. At the same time, structural priors such as semantic layout maps or edge
sketches are used to guide the generator in spatial-level structural modeling.
This enhances the layout completeness and detail fidelity of the generated
images. Within the overall framework, the model jointly optimizes contrastive
loss, structural consistency loss, and semantic preservation loss. A
multi-objective supervision mechanism is adopted to improve the semantic
consistency and controllability of the generated content. Systematic
experiments are conducted on the COCO-2014 dataset. Sensitivity analyses are
performed on embedding dimensions, text length, and structural guidance
strength. Quantitative metrics confirm the superior performance of the proposed
method in terms of CLIP Score, FID, and SSIM. The results show that the method
effectively bridges the gap between semantic alignment and structural fidelity
without increasing computational complexity. It demonstrates a strong ability
to generate semantically clear and structurally complete images, offering a
viable technical path for joint text-image modeling and image generation.

</details>


### [47] [VIFSS: View-Invariant and Figure Skating-Specific Pose Representation Learning for Temporal Action Segmentation](https://arxiv.org/abs/2508.10281)
*Ryota Tanaka,Tomohiro Suzuki,Keisuke Fujii*

Main category: cs.CV

TL;DR: 本文提出了一种针对花样滑冰跳跃的全新时间动作分割（TAS）框架，该框架明确整合了跳跃动作的三维特性和语义程序，通过视图不变的姿态表示学习和细粒度标注，显著提高了跳跃类型和旋转水平的识别准确率。


<details>
  <summary>Details</summary>
Motivation: 准确识别花样滑冰跳跃对于客观表现评估至关重要，但由于其精细和复杂性，通常需要专家知识。现有的时间动作分割（TAS）方法存在两大局限性：标注数据不足，以及未能考虑跳跃动作固有的三维特性和程序结构。

Method: 1. 提出了一个新颖的TAS框架，明确整合了跳跃动作的三维特性和语义程序。2. 引入了一种视图不变的花样滑冰专用姿态表示学习方法（VIFSS），结合对比学习进行预训练和动作分类进行微调。3. 构建了FS-Jump3D，首个公开的花样滑冰跳跃专用3D姿态数据集，用于视图不变的对比预训练。4. 引入了细粒度标注方案，标记了跳跃的“进入（准备）”和“落地”阶段，使TAS模型能够学习跳跃的程序结构。

Result: 该框架在元素级TAS上实现了超过92%的F1@50分数，该任务要求同时识别跳跃类型和旋转级别。此外，研究表明，当微调数据有限时，视图不变的对比预训练特别有效，突出了该方法在实际场景中的实用性。

Conclusion: 所提出的框架能有效应对花样滑冰跳跃识别的挑战，通过结合三维姿态学习和程序结构建模，显著提高了识别准确率，并且在数据受限的情况下表现出强大的鲁棒性和实用性。

Abstract: Understanding human actions from videos plays a critical role across various
domains, including sports analytics. In figure skating, accurately recognizing
the type and timing of jumps a skater performs is essential for objective
performance evaluation. However, this task typically requires expert-level
knowledge due to the fine-grained and complex nature of jump procedures. While
recent approaches have attempted to automate this task using Temporal Action
Segmentation (TAS), there are two major limitations to TAS for figure skating:
the annotated data is insufficient, and existing methods do not account for the
inherent three-dimensional aspects and procedural structure of jump actions. In
this work, we propose a new TAS framework for figure skating jumps that
explicitly incorporates both the three-dimensional nature and the semantic
procedure of jump movements. First, we propose a novel View-Invariant, Figure
Skating-Specific pose representation learning approach (VIFSS) that combines
contrastive learning as pre-training and action classification as fine-tuning.
For view-invariant contrastive pre-training, we construct FS-Jump3D, the first
publicly available 3D pose dataset specialized for figure skating jumps.
Second, we introduce a fine-grained annotation scheme that marks the ``entry
(preparation)'' and ``landing'' phases, enabling TAS models to learn the
procedural structure of jumps. Extensive experiments demonstrate the
effectiveness of our framework. Our method achieves over 92% F1@50 on
element-level TAS, which requires recognizing both jump types and rotation
levels. Furthermore, we show that view-invariant contrastive pre-training is
particularly effective when fine-tuning data is limited, highlighting the
practicality of our approach in real-world scenarios.

</details>


### [48] [JRDB-Reasoning: A Difficulty-Graded Benchmark for Visual Reasoning in Robotics](https://arxiv.org/abs/2508.10287)
*Simindokht Jahangard,Mehrzad Mohammadi,Yi Shen,Zhixi Cai,Hamid Rezatofighi*

Main category: cs.CV

TL;DR: 本文提出了JRDB-Reasoning，一个针对拥挤环境中视觉推理的新基准，以及一个自适应查询引擎，旨在解决现有基准在推理复杂度定义、难度控制和逐步推理标注方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有视觉推理基准存在局限性，包括缺乏推理复杂度的明确定义、无法控制生成不同难度和定制化任务的问题，以及未能提供结构化的、分步的推理标注，这些限制阻碍了对具身AI代理（如机器人）关键能力——视觉推理的有效评估。

Method: 研究者通过以下方法弥补了现有差距：1) 形式化定义了推理复杂度；2) 引入了一个自适应查询引擎，能够生成不同复杂度和定制化的问题，并提供详细的中间标注；3) 扩展了JRDB数据集，增加了人-物交互和几何关系标注，创建了JRDB-Reasoning基准。

Result: 研究成果是创建了JRDB-Reasoning基准和自适应查询引擎。这些工具能够对视觉推理框架进行细粒度评估，并对视觉-语言模型在不同推理级别进行动态评估，特别适用于人类密集环境下的视觉推理任务。

Conclusion: 所提出的引擎和基准为视觉推理框架和视觉-语言模型提供了一个更全面、更精细的评估工具，能够克服现有基准的局限性，从而推动具身AI领域的发展。

Abstract: Recent advances in Vision-Language Models (VLMs) and large language models
(LLMs) have greatly enhanced visual reasoning, a key capability for embodied AI
agents like robots. However, existing visual reasoning benchmarks often suffer
from several limitations: they lack a clear definition of reasoning complexity,
offer have no control to generate questions over varying difficulty and task
customization, and fail to provide structured, step-by-step reasoning
annotations (workflows). To bridge these gaps, we formalize reasoning
complexity, introduce an adaptive query engine that generates customizable
questions of varying complexity with detailed intermediate annotations, and
extend the JRDB dataset with human-object interaction and geometric
relationship annotations to create JRDB-Reasoning, a benchmark tailored for
visual reasoning in human-crowded environments. Our engine and benchmark enable
fine-grained evaluation of visual reasoning frameworks and dynamic assessment
of visual-language models across reasoning levels.

</details>


### [49] [A Sub-Pixel Multimodal Optical Remote Sensing Images Matching Method](https://arxiv.org/abs/2508.10294)
*Tao Huang,Hongbo Pan,Nanxi Zhou,Shun Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种名为PCWLAD（相位一致性加权最小绝对偏差）的亚像素模板匹配方法，用于提高多模态光学图像的匹配精度。该方法结合了SSIM粗匹配和WLAD精匹配，并在多个数据集上实现了优于现有方法的亚像素级匹配精度。


<details>
  <summary>Details</summary>
Motivation: 多模态光学图像的高精度匹配是几何处理的基础，但不同光谱响应导致的非线性辐射和几何形变差异通常会降低图像匹配精度。为了解决这些问题，需要开发更鲁棒和精确的匹配方法。

Method: 本文提出的PCWLAD方法包含两个主要步骤：
1.  **粗匹配**：计算不带噪声滤波的相位一致性（PC），并使用结构相似性指数（SSIM）进行模板匹配。
2.  **精匹配**：基于粗匹配结果，在两个多模态PC模板之间应用辐射和几何变换模型，并采用相互结构滤波以减轻噪声影响，最后使用加权最小绝对偏差（WLAD）准则估计亚像素偏移。

Result: PCWLAD在三种不同类型的图像数据集（可见光-红外Landsat图像、可见光-近红外近距离图像、可见光-红外无人机图像）上进行了性能评估。结果表明，PCWLAD在正确匹配率（CMR）和均方根误差（RMSE）方面优于现有八种最先进的方法，并在所有三个数据集上实现了大约0.4像素的平均匹配精度。

Conclusion: PCWLAD方法能够有效解决多模态光学图像匹配中非线性辐射和几何形变差异带来的精度下降问题，显著提高了匹配精度，达到了亚像素级别，证明了其在实际应用中的优越性。

Abstract: High-accuracy matching of multimodal optical images is the basis of geometric
processing. However, the image matching accuracy is usually degraded by the
nonlinear radiation and geometric deformation differences caused by different
spectral responses. To address these problems, we proposed a phase consistency
weighted least absolute deviation (PCWLAD) sub-pixel template matching method
to improve the matching accuracy of multimodal optical images. This method
consists of two main steps: coarse matching with the structural similarity
index measure (SSIM) and fine matching with WLAD. In the coarse matching step,
PCs are calculated without a noise filter to preserve the original structural
details, and template matching is performed using the SSIM. In the fine
matching step, we applied the radiometric and geometric transformation models
between two multimodal PC templates based on the coarse matching. Furthermore,
mutual structure filtering is adopted in the model to mitigate the impact of
noise within the corresponding templates on the structural consistency, and the
WLAD criterion is used to estimate the sub-pixel offset. To evaluate the
performance of PCWLAD, we created three types of image datasets: visible to
infrared Landsat images, visible to near-infrared close-range images, and
visible to infrared uncrewed aerial vehicle (UAV) images. PCWLAD outperformed
existing state-of-the-art eight methods in terms of correct matching rate (CMR)
and root mean square error (RMSE) and reached an average matching accuracy of
approximately 0.4 pixels across all three datasets. Our software and datasets
are publicly available at https://github.com/huangtaocsu/PCWLAD.

</details>


### [50] [InterSyn: Interleaved Learning for Dynamic Motion Synthesis in the Wild](https://arxiv.org/abs/2508.10297)
*Yiyi Ma,Yuanzhi Liang,Xiu Li,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: InterSyn是一种新颖的运动合成框架，通过交错学习策略，同时考虑单人和多人动力学，生成逼真自然的交互动作，并提高了文本到动作的对齐和多样性。


<details>
  <summary>Details</summary>
Motivation: 现有方法将单人与多人运动组件分开处理，导致生成的交互动作不够自然。本研究旨在捕捉现实世界场景中固有的自然、动态交互和细致协调。

Method: InterSyn框架采用交错学习策略，包含两个核心模块：1. 交错交互合成（INS）模块：从第一人称视角统一建模单人和交互行为，支持多角色交互。2. 相对协调细化（REC）模块：细化相互动力学并确保角色间动作同步。

Result: InterSyn生成的动作序列与现有方法相比，表现出更高的文本到动作对齐度，并显著提高了多样性，为鲁棒和自然的运动合成设立了新基准。

Conclusion: InterSyn框架能够生成更鲁棒和自然的交互动作，未来将开源代码以促进该领域的进一步研究和发展。

Abstract: We present Interleaved Learning for Motion Synthesis (InterSyn), a novel
framework that targets the generation of realistic interaction motions by
learning from integrated motions that consider both solo and multi-person
dynamics. Unlike previous methods that treat these components separately,
InterSyn employs an interleaved learning strategy to capture the natural,
dynamic interactions and nuanced coordination inherent in real-world scenarios.
Our framework comprises two key modules: the Interleaved Interaction Synthesis
(INS) module, which jointly models solo and interactive behaviors in a unified
paradigm from a first-person perspective to support multiple character
interactions, and the Relative Coordination Refinement (REC) module, which
refines mutual dynamics and ensures synchronized motions among characters.
Experimental results show that the motion sequences generated by InterSyn
exhibit higher text-to-motion alignment and improved diversity compared with
recent methods, setting a new benchmark for robust and natural motion
synthesis. Additionally, our code will be open-sourced in the future to promote
further research and development in this area.

</details>


### [51] [From Pixel to Mask: A Survey of Out-of-Distribution Segmentation](https://arxiv.org/abs/2508.10309)
*Wenjie Zhao,Jia Li,Yunhui Guo*

Main category: cs.CV

TL;DR: 该论文综述了域外（OoD）分割技术，特别是针对自动驾驶场景的应用，并将其分为四类，同时讨论了挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 随着对AI安全性的日益关注，域外检测和分割变得重要。传统的域外检测方法缺乏空间定位，限制了其在下游任务中的应用。域外分割通过像素级定位异常对象解决了这一限制，这对于自动驾驶等安全关键应用至关重要，因为这些应用需要精确地分割域外对象以实现有针对性的控制和增强系统鲁棒性。

Method: 该综述将当前的域外分割方法分为四类：(i) 测试时域外分割，(ii) 用于监督训练的异常暴露，(iii) 基于重建的方法，以及 (iv) 利用强大模型的方法。论文系统地回顾了自动驾驶场景中域外分割的最新进展。

Result: 论文系统地回顾了自动驾驶场景中域外分割的最新进展，识别了新兴挑战，并讨论了有前景的未来研究方向。

Conclusion: 域外分割对于安全关键应用（如自动驾驶）至关重要。当前方法可分为四类，但仍存在挑战，未来研究需进一步探索以提升其鲁棒性和实用性。

Abstract: Out-of-distribution (OoD) detection and segmentation have attracted growing
attention as concerns about AI security rise. Conventional OoD detection
methods identify the existence of OoD objects but lack spatial localization,
limiting their usefulness in downstream tasks. OoD segmentation addresses this
limitation by localizing anomalous objects at pixel-level granularity. This
capability is crucial for safety-critical applications such as autonomous
driving, where perception modules must not only detect but also precisely
segment OoD objects, enabling targeted control actions and enhancing overall
system robustness. In this survey, we group current OoD segmentation approaches
into four categories: (i) test-time OoD segmentation, (ii) outlier exposure for
supervised training, (iii) reconstruction-based methods, (iv) and approaches
that leverage powerful models. We systematically review recent advances in OoD
segmentation for autonomous-driving scenarios, identify emerging challenges,
and discuss promising future research directions.

</details>


### [52] [SpaRC-AD: A Baseline for Radar-Camera Fusion in End-to-End Autonomous Driving](https://arxiv.org/abs/2508.10567)
*Philipp Wolters,Johannes Gilg,Torben Teepe,Gerhard Rigoll*

Main category: cs.CV

TL;DR: 本文提出了SpaRC-AD，一个基于查询的端到端相机-雷达融合框架，用于面向规划的自动驾驶，通过稀疏3D特征对齐和多普勒速度估计，显著提升了恶劣天气和遮挡下自动驾驶系统的感知、预测和规划性能。


<details>
  <summary>Details</summary>
Motivation: 端到端自动驾驶系统在统一优化感知、运动预测和规划方面具有潜力，但纯视觉方法在恶劣天气、部分遮挡和精确速度估计方面存在根本性局限，而这些是安全敏感场景中（如碰撞避免）准确运动理解和长时程轨迹预测的关键挑战。

Method: 本文提出了SpaRC-AD，一个基于查询的端到端相机-雷达融合框架。其核心方法包括稀疏3D特征对齐和基于多普勒的速度估计，以构建强大的3D场景表示，从而优化代理锚点、地图折线和运动建模。

Result: SpaRC-AD在多项自动驾驶任务上显著优于现有纯视觉基线：3D检测（+4.8% mAP）、多目标跟踪（+8.3% AMOTA）、在线地图（+1.8% mAP）、运动预测（-4.0% mADE）和轨迹规划（-0.1m L2，-9% TPC）。在nuScenes、T-nuScenes和Bench2Drive等挑战性基准上实现了空间一致性和时间连贯性。

Conclusion: 研究表明，雷达融合在安全关键场景中非常有效，能够提供准确的运动理解和长时程轨迹预测，这对于避免碰撞至关重要。

Abstract: End-to-end autonomous driving systems promise stronger performance through
unified optimization of perception, motion forecasting, and planning. However,
vision-based approaches face fundamental limitations in adverse weather
conditions, partial occlusions, and precise velocity estimation - critical
challenges in safety-sensitive scenarios where accurate motion understanding
and long-horizon trajectory prediction are essential for collision avoidance.
To address these limitations, we propose SpaRC-AD, a query-based end-to-end
camera-radar fusion framework for planning-oriented autonomous driving. Through
sparse 3D feature alignment, and doppler-based velocity estimation, we achieve
strong 3D scene representations for refinement of agent anchors, map polylines
and motion modelling. Our method achieves strong improvements over the
state-of-the-art vision-only baselines across multiple autonomous driving
tasks, including 3D detection (+4.8% mAP), multi-object tracking (+8.3% AMOTA),
online mapping (+1.8% mAP), motion prediction (-4.0% mADE), and trajectory
planning (-0.1m L2 and -9% TPC). We achieve both spatial coherence and temporal
consistency on multiple challenging benchmarks, including real-world open-loop
nuScenes, long-horizon T-nuScenes, and closed-loop simulator Bench2Drive. We
show the effectiveness of radar-based fusion in safety-critical scenarios where
accurate motion understanding and long-horizon trajectory prediction are
essential for collision avoidance. The source code of all experiments is
available at https://phi-wol.github.io/sparcad/

</details>


### [53] [Integrating Reinforcement Learning with Visual Generative Models: Foundations and Advances](https://arxiv.org/abs/2508.10316)
*Yuanzhi Liang,Yijie Fang,Rui Li,Ziqi Ni,Ruijie Su,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: 该综述系统概述了强化学习（RL）在视觉内容生成（如图像、视频、3D/4D）中的应用，旨在解决传统生成模型目标与感知质量不匹配的问题。


<details>
  <summary>Details</summary>
Motivation: 传统的生成模型通常通过代理目标（如似然或重建损失）进行训练，这些目标往往与感知质量、语义准确性或物理真实性不符。强化学习提供了一个原则性框架，用于优化不可微分、偏好驱动和时间结构化的目标。

Method: 本文系统回顾了基于RL的视觉内容生成方法。它审视了RL从经典控制到通用优化工具的演变，并探讨了其在图像、视频和3D/4D生成中的整合。RL不仅作为微调机制，还作为结构性组件，用于将生成与复杂的高级目标对齐。

Result: 强化学习在生成任务中有效增强了可控性、一致性和人类对齐性。它作为一个通用优化工具，以及一个结构性组件，帮助生成模型实现与复杂、高级目标的对齐。

Conclusion: 强化学习在视觉内容生成领域中扮演着重要角色，能够将生成过程与复杂的高级目标对齐。未来研究将聚焦于RL与生成建模交叉领域的开放挑战和方向。

Abstract: Generative models have made significant progress in synthesizing visual
content, including images, videos, and 3D/4D structures. However, they are
typically trained with surrogate objectives such as likelihood or
reconstruction loss, which often misalign with perceptual quality, semantic
accuracy, or physical realism. Reinforcement learning (RL) offers a principled
framework for optimizing non-differentiable, preference-driven, and temporally
structured objectives. Recent advances demonstrate its effectiveness in
enhancing controllability, consistency, and human alignment across generative
tasks. This survey provides a systematic overview of RL-based methods for
visual content generation. We review the evolution of RL from classical control
to its role as a general-purpose optimization tool, and examine its integration
into image, video, and 3D/4D generation. Across these domains, RL serves not
only as a fine-tuning mechanism but also as a structural component for aligning
generation with complex, high-level goals. We conclude with open challenges and
future research directions at the intersection of RL and generative modeling.

</details>


### [54] [Concepts or Skills? Rethinking Instruction Selection for Multi-modal Models](https://arxiv.org/abs/2508.10339)
*Andrew Bai,Justin Cui,Ruochen Wang,Cho-Jui Hsieh*

Main category: cs.CV

TL;DR: 本文提出一种针对视觉语言指令微调的靶向数据选择方法，通过识别基准测试是侧重概念还是技能，选择最匹配的指令数据，从而提高模型性能。


<details>
  <summary>Details</summary>
Motivation: 研究发现，视觉语言基准测试在训练时，要么主要受益于相似技能的指令，要么主要受益于相似视觉概念的指令。这促使作者设计一种方法来优化给定基准测试的性能。

Method: 该方法首先从基准测试中提取概念/技能，然后判断该基准测试主要受益于相似概念还是相似技能，最后选择具有最匹配概念/技能的指令进行训练。

Result: 在10多个基准测试上的实验验证了所提出靶向数据选择方法的有效性，与现有最佳基线相比，所有基准测试的平均性能提升了+0.9%，在侧重技能的子集上提升了+1.5%。

Conclusion: 研究结果强调了指令选择中权衡概念知识获取与视觉技能习得的重要性，需要平衡两者以优化模型表现。

Abstract: Vision-language instruction tuning achieves two main purposes: learning
visual concepts and learning visual skills. In this paper, we found that
vision-language benchmarks fall into the dichotomy of mainly benefiting from
training on instructions with similar skills or visual concepts. Inspired by
the discovery, we designed a simple targeted training data selection method to
optimize the performance of a given benchmark. We first extract the
concepts/skills from the benchmark, determine whether the benchmark
predominantly benefits from similar concepts or skills, and finally select
instructions with the most matching concepts/skills. Experiments on 10+
benchmarks validate the effectiveness of our targeted data selection method,
showing +0.9\% over the best existing baseline averaged over all benchmarks and
+1.5\% on the skill-focused subset. Our findings underscore the importance of
recognizing the inherent trade-off within instruction selection, which requires
balancing the acquisition of conceptual knowledge against visual skill.

</details>


### [55] [Glo-DMU: A Deep Morphometry Framework of Ultrastructural Characterization in Glomerular Electron Microscopic Images](https://arxiv.org/abs/2508.10351)
*Zhentai Zhang,Danyi Weng,Guibin Zhang,Xiang Chen,Kaixing Long,Jian Geng,Yanmeng Lu,Lei Zhang,Zhitao Zhou,Lei Cao*

Main category: cs.CV

TL;DR: 本研究提出了Glo-DMU框架，一个基于深度学习的肾小球超微结构形态测量框架，能够全自动、高精度地同时量化多种超微结构特征，以辅助肾脏病理诊断。


<details>
  <summary>Details</summary>
Motivation: 肾小球超微结构特征对肾脏疾病的类型、进展和预后具有重要指示作用。现有计算病理学方法多聚焦于单个超微结构的识别，难以满足实际诊断需求。

Method: 提出了Glo-DMU框架，包含三个深度模型：超微结构分割模型、肾小球滤过屏障区域分类模型和电子致密物沉积检测模型。该框架同时量化肾小球基底膜厚度、足细胞足突融合程度和电子致密物沉积位置这三种最常用的超微结构特征。

Result: 在115名患者（涵盖9种肾脏病理类型）的真实世界诊断场景中进行了评估，结果显示其自动量化结果与病理报告中的形态学描述具有良好的一致性。

Conclusion: Glo-DMU框架具有全自动化、高精度、高通量的特点，能够同时量化多种超微结构特征，为肾脏病理学家提供了一个高效的辅助诊断工具。

Abstract: Complex and diverse ultrastructural features can indicate the type,
progression, and prognosis of kidney diseases. Recently, computational
pathology combined with deep learning methods has shown tremendous potential in
advancing automatic morphological analysis of glomerular ultrastructure.
However, current research predominantly focuses on the recognition of
individual ultrastructure, which makes it challenging to meet practical
diagnostic needs. In this study, we propose the glomerular morphometry
framework of ultrastructural characterization (Glo-DMU), which is grounded on
three deep models: the ultrastructure segmentation model, the glomerular
filtration barrier region classification model, and the electron-dense deposits
detection model. Following the conventional protocol of renal biopsy diagnosis,
this framework simultaneously quantifies the three most widely used
ultrastructural features: the thickness of glomerular basement membrane, the
degree of foot process effacement, and the location of electron-dense deposits.
We evaluated the 115 patients with 9 renal pathological types in real-world
diagnostic scenarios, demonstrating good consistency between automatic
quantification results and morphological descriptions in the pathological
reports. Glo-DMU possesses the characteristics of full automation, high
precision, and high throughput, quantifying multiple ultrastructural features
simultaneously, and providing an efficient tool for assisting renal
pathologists.

</details>


### [56] [Improving OCR for Historical Texts of Multiple Languages](https://arxiv.org/abs/2508.10356)
*Hylke Westerdijk,Ben Blankenborg,Khondoker Ittehadul Islam*

Main category: cs.CV

TL;DR: 本文介绍了在光学字符识别（OCR）和文档版面分析领域，针对历史希伯来语碎片、16-18世纪会议决议及现代英语手写识别三项任务，采用先进深度学习技术的应用方法与发现。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过深度学习技术提升对各种挑战性文本（包括历史文献和现代手写体）的字符识别和文档版面分析能力。

Method: 针对死海古卷希伯来语碎片，使用数据增强并结合Kraken和TrOCR模型。对于16-18世纪会议决议，采用集成DeepLabV3+进行语义分割的CRNN（卷积循环神经网络）与双向LSTM，并辅以基于置信度的伪标签。针对现代英语手写识别，应用带有ResNet34编码器的CRNN，并使用连接时序分类（CTC）损失函数进行训练。

Result: 通过所提出的方法，成功提升了希伯来语字符识别性能，优化了会议决议分析模型，并有效地捕捉了英语手写识别中的序列依赖性。报告提供了有价值的见解。

Conclusion: 该研究提供了在多任务OCR和文档分析中应用深度学习的宝贵见解，并为未来的研究指明了潜在方向。

Abstract: This paper presents our methodology and findings from three tasks across
Optical Character Recognition (OCR) and Document Layout Analysis using advanced
deep learning techniques. First, for the historical Hebrew fragments of the
Dead Sea Scrolls, we enhanced our dataset through extensive data augmentation
and employed the Kraken and TrOCR models to improve character recognition. In
our analysis of 16th to 18th-century meeting resolutions task, we utilized a
Convolutional Recurrent Neural Network (CRNN) that integrated DeepLabV3+ for
semantic segmentation with a Bidirectional LSTM, incorporating confidence-based
pseudolabeling to refine our model. Finally, for modern English handwriting
recognition task, we applied a CRNN with a ResNet34 encoder, trained using the
Connectionist Temporal Classification (CTC) loss function to effectively
capture sequential dependencies. This report offers valuable insights and
suggests potential directions for future research.

</details>


### [57] [AtomDiffuser: Time-Aware Degradation Modeling for Drift and Beam Damage in STEM Imaging](https://arxiv.org/abs/2508.10359)
*Hao Wang,Hongkui Zheng,Kai He,Abolfazl Razi*

Main category: cs.CV

TL;DR: AtomDiffuser是一个时间感知的退化建模框架，能够有效分离时间分辨STEM数据中的空间漂移和辐射损伤，从而实现对原子结构演变的精确分析。


<details>
  <summary>Details</summary>
Motivation: 时间分辨扫描透射电子显微镜（STEM）在材料科学中至关重要，但其数据解释面临挑战。主要问题是空间漂移（由机械和热不稳定性引起）和束流引起的信号损失（由辐射损伤引起）这两种退化效应相互纠缠，导致数据几何和强度复杂失真，使得现有方法难以明确分离其影响或在原子分辨率下建模材料动态。

Method: 本文提出了AtomDiffuser，一个时间感知的退化建模框架。该方法通过预测任意两个STEM帧之间的仿射变换和空间变化的衰减图，来解耦样品漂移和辐射衰减。与传统去噪或配准方法不同，AtomDiffuser将退化视为一个物理启发式、时间条件的过程。

Result: AtomDiffuser在合成退化过程上训练后，能够很好地泛化到真实的低温STEM数据。它支持高分辨率的退化推断和漂移校准，并提供了可视化和量化与辐射诱导原子不稳定性相关的退化模式的工具。

Conclusion: AtomDiffuser框架能够有效分离并建模STEM数据中的复杂退化效应，从而实现对原子结构随时间演变的可解释分析，为理解辐射诱导的原子不稳定性提供了新的工具和见解。

Abstract: Scanning transmission electron microscopy (STEM) plays a critical role in
modern materials science, enabling direct imaging of atomic structures and
their evolution under external interferences. However, interpreting
time-resolved STEM data remains challenging due to two entangled degradation
effects: spatial drift caused by mechanical and thermal instabilities, and
beam-induced signal loss resulting from radiation damage. These factors distort
both geometry and intensity in complex, temporally correlated ways, making it
difficult for existing methods to explicitly separate their effects or model
material dynamics at atomic resolution. In this work, we present AtomDiffuser,
a time-aware degradation modeling framework that disentangles sample drift and
radiometric attenuation by predicting an affine transformation and a spatially
varying decay map between any two STEM frames. Unlike traditional denoising or
registration pipelines, our method leverages degradation as a physically
heuristic, temporally conditioned process, enabling interpretable structural
evolutions across time. Trained on synthetic degradation processes,
AtomDiffuser also generalizes well to real-world cryo-STEM data. It further
supports high-resolution degradation inference and drift alignment, offering
tools for visualizing and quantifying degradation patterns that correlate with
radiation-induced atomic instabilities.

</details>


### [58] [Contrast Sensitivity Function of Multimodal Vision-Language Models](https://arxiv.org/abs/2508.10367)
*Pablo Hernández-Cámara,Alexandra Gomez-Villa,Jose Manuel Jaén-Lorites,Jorge Vila-Tomás,Jesus Malo,Valero Laparra*

Main category: cs.CV

TL;DR: 本文提出一种受心理物理学启发的行为方法，通过直接提示多模态视觉语言模型（VLMs）判断图案可见性，来估计其对比敏感度函数（CSF），并发现当前VLM在视觉感知上与人类存在显著差距，且对提示语敏感。


<details>
  <summary>Details</summary>
Motivation: 理解多模态视觉语言模型如何感知低级视觉特征，并评估它们与人类感知的对齐程度至关重要。人类视觉的一个关键特征是对比敏感度函数（CSF），它描述了在低对比度下对空间频率的敏感性。

Method: 引入一种新颖的、受心理物理学启发的行为方法，通过直接提示基于聊天的VLM判断不同对比度下每个频率的图案可见性来估计其CSF。该方法比以往报道的更接近真实的心理物理学实验。使用带通滤波噪声图像和多样化的提示语来评估多个架构的模型响应。

Result: 研究发现，虽然一些模型在CSF的形状或幅度上近似于人类，但没有模型能完全复制两者。值得注意的是，提示语的措辞对模型响应有很大影响，引发了对提示稳定性（prompt stability）的担忧。

Conclusion: 本研究提供了一个探测多模态模型视觉敏感性的新框架，并揭示了它们的视觉表征与人类感知之间的关键差距。同时，也凸显了提示语稳定性在VLM评估中的重要性。

Abstract: Assessing the alignment of multimodal vision-language models~(VLMs) with
human perception is essential to understand how they perceive low-level visual
features. A key characteristic of human vision is the contrast sensitivity
function (CSF), which describes sensitivity to spatial frequency at
low-contrasts. Here, we introduce a novel behavioral psychophysics-inspired
method to estimate the CSF of chat-based VLMs by directly prompting them to
judge pattern visibility at different contrasts for each frequency. This
methodology is closer to the real experiments in psychophysics than the
previously reported. Using band-pass filtered noise images and a diverse set of
prompts, we assess model responses across multiple architectures. We find that
while some models approximate human-like CSF shape or magnitude, none fully
replicate both. Notably, prompt phrasing has a large effect on the responses,
raising concerns about prompt stability. Our results provide a new framework
for probing visual sensitivity in multimodal models and reveal key gaps between
their visual representations and human perception.

</details>


### [59] [Towards Spatially Consistent Image Generation: On Incorporating Intrinsic Scene Properties into Diffusion Models](https://arxiv.org/abs/2508.10382)
*Hyundo Lee,Suhyung Choi,Byoung-Tak Zhang,Inwoo Hwang*

Main category: cs.CV

TL;DR: 该论文提出一种方法，通过共同生成图像及其内在场景属性（如深度、分割图），来解决大型图像生成模型在空间一致性方面的不足，从而生成更真实、空间布局更自然的图像。


<details>
  <summary>Details</summary>
Motivation: 大型图像生成模型虽然能生成高质量图像，但由于缺乏底层结构和空间布局信息，常产生空间不一致和扭曲的图像。现有方法多依赖图像-文本对或将内在属性作为条件输入，未能充分利用场景的内在属性信息。

Method: 1. 利用预训练的估计器从大型图像数据集中提取丰富的内在场景属性（如深度、分割图）。2. 使用自编码器将各种内在场景属性聚合成一个单一的潜在变量。3. 在预训练的大规模潜在扩散模型（LDMs）基础上，通过精心共享互信息，同时对图像和内在属性域进行去噪，使图像和内在属性相互反映，同时不降低图像质量。

Result: 实验结果表明，该方法能够纠正空间不一致性，生成更自然的场景布局，同时保持了基础模型（如Stable Diffusion）的图像保真度和文本对齐性。

Conclusion: 通过共同生成图像及其内在场景属性，该方法能够隐式捕获底层场景结构，显著提高了生成图像的空间一致性和真实感，有效解决了现有大型图像生成模型的局限性。

Abstract: Image generation models trained on large datasets can synthesize high-quality
images but often produce spatially inconsistent and distorted images due to
limited information about the underlying structures and spatial layouts. In
this work, we leverage intrinsic scene properties (e.g., depth, segmentation
maps) that provide rich information about the underlying scene, unlike prior
approaches that solely rely on image-text pairs or use intrinsics as
conditional inputs. Our approach aims to co-generate both images and their
corresponding intrinsics, enabling the model to implicitly capture the
underlying scene structure and generate more spatially consistent and realistic
images. Specifically, we first extract rich intrinsic scene properties from a
large image dataset with pre-trained estimators, eliminating the need for
additional scene information or explicit 3D representations. We then aggregate
various intrinsic scene properties into a single latent variable using an
autoencoder. Building upon pre-trained large-scale Latent Diffusion Models
(LDMs), our method simultaneously denoises the image and intrinsic domains by
carefully sharing mutual information so that the image and intrinsic reflect
each other without degrading image quality. Experimental results demonstrate
that our method corrects spatial inconsistencies and produces a more natural
layout of scenes while maintaining the fidelity and textual alignment of the
base model (e.g., Stable Diffusion).

</details>


### [60] [Unlocking Robust Semantic Segmentation Performance via Label-only Elastic Deformations against Implicit Label Noise](https://arxiv.org/abs/2508.10383)
*Yechan Kim,Dongho Yoon,Younkwan Lee,Unse Fatima,Hong Kook Kim,Songjae Lee,Sanga Park,Jeong Ho Park,Seonjong Kang,Moongu Jeon*

Main category: cs.CV

TL;DR: NSegment+是一种新的数据增强框架，通过对分割标签进行独立弹性形变，有效应对语义分割中微妙的隐性标签噪声，从而提高模型性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注显性标签噪声，但真实世界数据存在由边界模糊、标注者差异引起的微妙（隐性）标签不完美。这些轻微、潜在的噪声会损害模型性能。传统的图像和标签同步变换数据增强方法，反而可能放大这些微妙缺陷，限制模型泛化能力。

Method: 引入NSegment+框架，将图像和标签的变换解耦。具体地，仅对分割标签应用受控的弹性形变，而保持原始图像不变。这种方法旨在促使模型学习鲁棒的对象结构表示，即使存在轻微的标签不一致。

Result: NSegment+持续提升了性能，在Vaihingen、LoveDA、Cityscapes和PASCAL VOC数据集上，mIoU平均分别提高了+2.29、+2.38、+1.75和+3.39。这些增益在与其他训练技巧（如CutMix和Label Smoothing）结合时可以进一步放大。

Conclusion: 解决隐性标签噪声至关重要。NSegment+通过解耦图像和标签变换，对标签进行独立弹性形变，能有效提升语义分割模型的鲁棒性和泛化能力，即使不使用其他复杂技巧也能获得显著性能提升。

Abstract: While previous studies on image segmentation focus on handling severe (or
explicit) label noise, real-world datasets also exhibit subtle (or implicit)
label imperfections. These arise from inherent challenges, such as ambiguous
object boundaries and annotator variability. Although not explicitly present,
such mild and latent noise can still impair model performance. Typical data
augmentation methods, which apply identical transformations to the image and
its label, risk amplifying these subtle imperfections and limiting the model's
generalization capacity. In this paper, we introduce NSegment+, a novel
augmentation framework that decouples image and label transformations to
address such realistic noise for semantic segmentation. By introducing
controlled elastic deformations only to segmentation labels while preserving
the original images, our method encourages models to focus on learning robust
representations of object structures despite minor label inconsistencies.
Extensive experiments demonstrate that NSegment+ consistently improves
performance, achieving mIoU gains of up to +2.29, +2.38, +1.75, and +3.39 in
average on Vaihingen, LoveDA, Cityscapes, and PASCAL VOC, respectively-even
without bells and whistles, highlighting the importance of addressing implicit
label noise. These gains can be further amplified when combined with other
training tricks, including CutMix and Label Smoothing.

</details>


### [61] [PQ-DAF: Pose-driven Quality-controlled Data Augmentation for Data-scarce Driver Distraction Detection](https://arxiv.org/abs/2508.10397)
*Haibin Sun,Xinghui Song*

Main category: cs.CV

TL;DR: 本文提出一个姿态驱动、质量控制的数据增强框架（PQ-DAF），利用扩散模型生成训练样本并使用视觉-语言模型过滤低质量样本，以解决驾驶员分心检测中少样本学习和领域漂移导致的泛化能力差的问题。


<details>
  <summary>Details</summary>
Motivation: 现有驾驶员分心检测模型在实际部署中泛化能力差，主要原因在于数据标注成本高导致的少样本学习挑战，以及训练数据集与实际部署条件之间存在显著领域漂移。

Method: 提出PQ-DAF框架。具体方法包括：1) 采用渐进式条件扩散模型（PCDMs）精确捕捉驾驶员关键姿态特征并合成多样化的训练样本。2) 引入基于CogVLM视觉-语言模型的样本质量评估模块，根据置信度阈值过滤低质量合成样本，确保增强数据集的可靠性。

Result: 广泛实验表明，PQ-DAF显著提升了少样本驾驶员分心检测的性能，在数据稀缺条件下模型泛化能力获得了显著提升。

Conclusion: PQ-DAF通过成本效益高的方式扩展训练数据并增强跨域鲁棒性，有效解决了驾驶员分心检测中少样本学习和领域漂移的挑战，提高了模型泛化能力。

Abstract: Driver distraction detection is essential for improving traffic safety and
reducing road accidents. However, existing models often suffer from degraded
generalization when deployed in real-world scenarios. This limitation primarily
arises from the few-shot learning challenge caused by the high cost of data
annotation in practical environments, as well as the substantial domain shift
between training datasets and target deployment conditions. To address these
issues, we propose a Pose-driven Quality-controlled Data Augmentation Framework
(PQ-DAF) that leverages a vision-language model for sample filtering to
cost-effectively expand training data and enhance cross-domain robustness.
Specifically, we employ a Progressive Conditional Diffusion Model (PCDMs) to
accurately capture key driver pose features and synthesize diverse training
examples. A sample quality assessment module, built upon the CogVLM
vision-language model, is then introduced to filter out low-quality synthetic
samples based on a confidence threshold, ensuring the reliability of the
augmented dataset. Extensive experiments demonstrate that PQ-DAF substantially
improves performance in few-shot driver distraction detection, achieving
significant gains in model generalization under data-scarce conditions.

</details>


### [62] [Translation of Text Embedding via Delta Vector to Suppress Strongly Entangled Content in Text-to-Image Diffusion Models](https://arxiv.org/abs/2508.10407)
*Eunseo Koh,Seunghoo Hong,Tae-Young Kim,Simon S. Woo,Jae-Pil Heo*

Main category: cs.CV

TL;DR: 该论文提出了一种新方法，通过在文本嵌入空间中引入一个“delta向量”来抑制文本到图像（T2I）扩散模型中与特定词强关联的、不希望出现的图像内容，并能实现选择性抑制和个性化模型中的精确抑制。


<details>
  <summary>Details</summary>
Motivation: 当前的T2I扩散模型在生成图像时，难以抑制与某些特定词语强关联的内容（例如，生成“查理·卓别林”时，即使明确指示不包含胡子，胡子仍会持续出现），因为这些概念高度纠缠。

Method: 1. 引入一个“delta向量”来修改文本嵌入空间，以削弱不希望内容的影响。
2. 通过零样本方法获取该delta向量。
3. 提出选择性抑制与Delta向量（SSDV）方法，将delta向量应用于交叉注意力机制，实现对特定区域不希望内容的有效抑制。
4. 通过优化delta向量，在个性化T2I模型中实现更精确的抑制。

Result: 实验结果表明，该方法在定量和定性指标上均显著优于现有方法。

Conclusion: 所提出的方法能有效抑制T2I扩散模型中（包括个性化模型）与特定词强关联的、不希望出现的图像内容，解决了现有模型难以处理的概念纠缠问题。

Abstract: Text-to-Image (T2I) diffusion models have made significant progress in
generating diverse high-quality images from textual prompts. However, these
models still face challenges in suppressing content that is strongly entangled
with specific words. For example, when generating an image of ``Charlie
Chaplin", a ``mustache" consistently appears even if explicitly instructed not
to include it, as the concept of ``mustache" is strongly entangled with
``Charlie Chaplin". To address this issue, we propose a novel approach to
directly suppress such entangled content within the text embedding space of
diffusion models. Our method introduces a delta vector that modifies the text
embedding to weaken the influence of undesired content in the generated image,
and we further demonstrate that this delta vector can be easily obtained
through a zero-shot approach. Furthermore, we propose a Selective Suppression
with Delta Vector (SSDV) method to adapt delta vector into the cross-attention
mechanism, enabling more effective suppression of unwanted content in regions
where it would otherwise be generated. Additionally, we enabled more precise
suppression in personalized T2I models by optimizing delta vector, which
previous baselines were unable to achieve. Extensive experimental results
demonstrate that our approach significantly outperforms existing methods, both
in terms of quantitative and qualitative metrics.

</details>


### [63] [SC-Lane: Slope-aware and Consistent Road Height Estimation Framework for 3D Lane Detection](https://arxiv.org/abs/2508.10411)
*Chaesong Park,Eunbin Seo,Jihyeon Hwang,Jongwoo Lim*

Main category: cs.CV

TL;DR: SC-Lane是一个新颖的3D车道线检测框架，通过斜率感知和时间一致的高度图估计，显著提升了对复杂道路几何的鲁棒性和检测精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖固定斜率锚点，导致对多样化道路几何的适应性不足，且缺乏连续帧间高度估计的时间一致性，这在真实驾驶场景中至关重要。

Method: 本文提出了SC-Lane框架。核心方法包括：1) 斜率感知自适应特征模块，通过图像线索动态预测权重，融合多斜率表示到统一高度图；2) 高度一致性模块，强制执行时间连贯性，确保连续帧间高度估计的稳定性和准确性。评估采用MAE、RMSE和基于阈值的精度等标准指标，并在LiDAR高度图数据集和OpenLane基准上进行测试。

Result: SC-Lane在OpenLane基准测试中显著提升了高度估计和3D车道线检测性能，F-score达到64.3%，超越现有方法，实现了最先进的性能。

Conclusion: SC-Lane通过其斜率感知和时间一致性机制，有效解决了3D车道线检测中高度估计的鲁棒性和稳定性问题，为未来的研究建立了严格的基准。

Abstract: In this paper, we introduce SC-Lane, a novel slope-aware and temporally
consistent heightmap estimation framework for 3D lane detection. Unlike
previous approaches that rely on fixed slope anchors, SC-Lane adaptively
determines the fusion of slope-specific height features, improving robustness
to diverse road geometries. To achieve this, we propose a Slope-Aware Adaptive
Feature module that dynamically predicts the appropriate weights from image
cues for integrating multi-slope representations into a unified heightmap.
Additionally, a Height Consistency Module enforces temporal coherence, ensuring
stable and accurate height estimation across consecutive frames, which is
crucial for real-world driving scenarios. To evaluate the effectiveness of
SC-Lane, we employ three standardized metrics-Mean Absolute Error(MAE), Root
Mean Squared Error (RMSE), and threshold-based accuracy-which, although common
in surface and depth estimation, have been underutilized for road height
assessment. Using the LiDAR-derived heightmap dataset introduced in prior work
[20], we benchmark our method under these metrics, thereby establishing a
rigorous standard for future comparisons. Extensive experiments on the OpenLane
benchmark demonstrate that SC-Lane significantly improves both height
estimation and 3D lane detection, achieving state-of-the-art performance with
an F-score of 64.3%, outperforming existing methods by a notable margin. For
detailed results and a demonstration video, please refer to our project
page:https://parkchaesong.github.io/sclane/

</details>


### [64] [NanoControl: A Lightweight Framework for Precise and Efficient Control in Diffusion Transformer](https://arxiv.org/abs/2508.10424)
*Shanyuan Liu,Jian Zhu,Junda Lu,Yue Gong,Liuzhuozheng Li,Bo Cheng,Yuhang Ma,Liebucha Wu,Xiaoyu Wu,Dawei Leng,Yuhui Yin*

Main category: cs.CV

TL;DR: 提出NanoControl，一种高效的扩散Transformer（DiT）可控文本到图像生成方法，通过LoRA风格控制模块和KV-Context增强机制，显著减少参数和计算开销，同时保持卓越性能。


<details>
  <summary>Details</summary>
Motivation: 现有DiT可控文本到图像生成方法（如基于ControlNet）引入了大量的参数和计算开销，效率低下。

Method: 提出NanoControl模型，以Flux为骨干网络。该模型采用LoRA风格的控制模块直接学习控制信号，而非复制DiT骨干。此外，引入KV-Context增强机制，将条件特定的键值信息高效集成到骨干网络中，促进条件特征的深度融合。

Result: NanoControl在可控文本到图像生成方面达到最先进性能，参数量仅增加0.024%，GFLOPs仅增加0.029%。与传统控制方法相比，显著降低了计算开销，同时保持了卓越的生成质量并提高了可控性。

Conclusion: NanoControl为DiT可控图像生成提供了一种高效且高性能的解决方案，有效克服了现有方法的参数和计算瓶颈。

Abstract: Diffusion Transformers (DiTs) have demonstrated exceptional capabilities in
text-to-image synthesis. However, in the domain of controllable text-to-image
generation using DiTs, most existing methods still rely on the ControlNet
paradigm originally designed for UNet-based diffusion models. This paradigm
introduces significant parameter overhead and increased computational costs. To
address these challenges, we propose the Nano Control Diffusion Transformer
(NanoControl), which employs Flux as the backbone network. Our model achieves
state-of-the-art controllable text-to-image generation performance while
incurring only a 0.024\% increase in parameter count and a 0.029\% increase in
GFLOPs, thus enabling highly efficient controllable generation. Specifically,
rather than duplicating the DiT backbone for control, we design a LoRA-style
(low-rank adaptation) control module that directly learns control signals from
raw conditioning inputs. Furthermore, we introduce a KV-Context Augmentation
mechanism that integrates condition-specific key-value information into the
backbone in a simple yet highly effective manner, facilitating deep fusion of
conditional features. Extensive benchmark experiments demonstrate that
NanoControl significantly reduces computational overhead compared to
conventional control approaches, while maintaining superior generation quality
and achieving improved controllability.

</details>


### [65] [STRIDE-QA: Visual Question Answering Dataset for Spatiotemporal Reasoning in Urban Driving Scenes](https://arxiv.org/abs/2508.10427)
*Keishi Ishihara,Kento Sasaki,Tsubasa Takahashi,Daiki Shiono,Yu Yamaguchi*

Main category: cs.CV

TL;DR: 本文提出了STRIDE-QA，一个大规模的视觉问答（VQA）数据集，用于自动驾驶中的时空推理，旨在弥补现有视觉语言模型（VLMs）在动态交通场景理解方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有VLM主要在静态网络图像-文本对上训练，这限制了它们在理解和预测动态交通场景所需的精确时空推理能力。自动驾驶需要这种能力来支持复杂真实世界场景中的决策。

Method: 构建了STRIDE-QA数据集，包含100小时来自东京的多传感器驾驶数据，覆盖28.5万帧，生成1600万个问答对。数据集通过密集的自动生成标注（包括3D边界框、分割掩模和多目标跟踪）进行支撑，并通过三个新颖的问答任务支持以物体为中心和以自我为中心的推理，这些任务需要空间定位和时间预测。

Result: 基准测试表明，现有VLM在预测一致性上表现极差，得分接近于零。相比之下，在STRIDE-QA上微调的VLM表现出显著的性能提升，在空间定位上达到55%的成功率，在未来运动预测上达到28%的一致性。

Conclusion: STRIDE-QA为开发更可靠的、适用于安全关键型自动驾驶系统的VLM奠定了全面的基础，显著提升了模型在时空推理方面的能力。

Abstract: Vision-Language Models (VLMs) have been applied to autonomous driving to
support decision-making in complex real-world scenarios. However, their
training on static, web-sourced image-text pairs fundamentally limits the
precise spatiotemporal reasoning required to understand and predict dynamic
traffic scenes. We address this critical gap with STRIDE-QA, a large-scale
visual question answering (VQA) dataset for physically grounded reasoning from
an ego-centric perspective. Constructed from 100 hours of multi-sensor driving
data in Tokyo, capturing diverse and challenging conditions, STRIDE-QA is the
largest VQA dataset for spatiotemporal reasoning in urban driving, offering 16
million QA pairs over 285K frames. Grounded by dense, automatically generated
annotations including 3D bounding boxes, segmentation masks, and multi-object
tracks, the dataset uniquely supports both object-centric and ego-centric
reasoning through three novel QA tasks that require spatial localization and
temporal prediction. Our benchmarks demonstrate that existing VLMs struggle
significantly, achieving near-zero scores on prediction consistency. In
contrast, VLMs fine-tuned on STRIDE-QA exhibit dramatic performance gains,
achieving 55% success in spatial localization and 28% consistency in future
motion prediction, compared to near-zero scores from general-purpose VLMs.
Therefore, STRIDE-QA establishes a comprehensive foundation for developing more
reliable VLMs for safety-critical autonomous systems.

</details>


### [66] [CRISP: Contrastive Residual Injection and Semantic Prompting for Continual Video Instance Segmentation](https://arxiv.org/abs/2508.10432)
*Baichen Liu,Qi Lyu,Xudong Wang,Jiahua Dong,Lianqing Liu,Zhi Han*

Main category: cs.CV

TL;DR: 本文提出CRISP框架，通过对比残差注入和语义提示，解决持续视频实例分割中实例级、类别级和任务级混淆问题，有效平衡新旧知识学习并保持时间一致性。


<details>
  <summary>Details</summary>
Motivation: 持续视频实例分割面临吸收新类别、保留旧知识以及保持跨帧时间一致性的挑战，同时存在实例、类别和任务间的混淆，需要一种能兼顾可塑性和稳定性的方法来避免灾难性遗忘。

Method: CRISP框架包括：1) 实例级学习：建模实例跟踪并构建实例相关性损失，强调与先前查询空间的相关性并增强当前任务查询的特异性。2) 类别级学习：构建自适应残差语义提示(ARSP)学习框架，通过类别文本生成可学习的语义残差提示池，并利用可调查询-提示匹配机制建立查询与提示的映射，同时引入基于对比学习的语义一致性损失。3) 任务级学习：引入一种简洁有效的增量提示初始化策略，以确保查询空间内任务间的相关性。

Result: 在YouTube-VIS-2019和YouTube-VIS-2021数据集上的大量实验表明，CRISP在长期持续视频实例分割任务中显著优于现有方法，成功避免了灾难性遗忘，并有效提升了分割和分类性能。

Conclusion: CRISP是一种在持续视频实例分割中非常有效的方法，它通过独特地处理实例、类别和任务间的混淆，成功实现了新旧知识的平衡学习，显著提高了模型的长期性能并避免了灾难性遗忘。

Abstract: Continual video instance segmentation demands both the plasticity to absorb
new object categories and the stability to retain previously learned ones, all
while preserving temporal consistency across frames. In this work, we introduce
Contrastive Residual Injection and Semantic Prompting (CRISP), an earlier
attempt tailored to address the instance-wise, category-wise, and task-wise
confusion in continual video instance segmentation. For instance-wise learning,
we model instance tracking and construct instance correlation loss, which
emphasizes the correlation with the prior query space while strengthening the
specificity of the current task query. For category-wise learning, we build an
adaptive residual semantic prompt (ARSP) learning framework, which constructs a
learnable semantic residual prompt pool generated by category text and uses an
adjustive query-prompt matching mechanism to build a mapping relationship
between the query of the current task and the semantic residual prompt.
Meanwhile, a semantic consistency loss based on the contrastive learning is
introduced to maintain semantic coherence between object queries and residual
prompts during incremental training. For task-wise learning, to ensure the
correlation at the inter-task level within the query space, we introduce a
concise yet powerful initialization strategy for incremental prompts. Extensive
experiments on YouTube-VIS-2019 and YouTube-VIS-2021 datasets demonstrate that
CRISP significantly outperforms existing continual segmentation methods in the
long-term continual video instance segmentation task, avoiding catastrophic
forgetting and effectively improving segmentation and classification
performance. The code is available at https://github.com/01upup10/CRISP.

</details>


### [67] [DOD-SA: Infrared-Visible Decoupled Object Detection with Single-Modality Annotations](https://arxiv.org/abs/2508.10445)
*Hang Jin,Chenqiang Gao,Junjie Guo,Fangcen Liu,Kanghui Tian,Qinyao Chang*

Main category: cs.CV

TL;DR: 提出了一种名为DOD-SA的红外-可见光解耦目标检测框架，仅需单模态标注，通过协同教师-学生网络和渐进式自适应训练策略，实现跨模态知识迁移和伪标签生成，有效降低标注成本并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有红外-可见光目标检测方法通常需要双模态标注才能输出两种模态的检测结果，导致高昂的标注成本。

Method: 提出DOD-SA框架，核心是单模态和双模态协同教师-学生网络（CoSD-TSNet），包含单模态分支（SM-Branch）和双模态解耦分支（DMD-Branch）。教师模型为未标注模态生成伪标签，支持学生模型训练。引入渐进式自适应训练策略（PaST），分三阶段训练：预训练SM-Branch、SM-Branch指导DMD-Branch学习、精炼DMD-Branch。设计伪标签分配器（PLA）以解决训练期间的模态不对齐问题。

Result: 在DroneVehicle数据集上进行的大量实验表明，该方法优于现有最先进（SOTA）的方法。

Conclusion: DOD-SA框架通过创新的单模态标注方法、协同教师-学生网络、渐进式训练策略和伪标签分配器，有效解决了红外-可见光目标检测中高昂标注成本的挑战，并取得了卓越的性能。

Abstract: Infrared-visible object detection has shown great potential in real-world
applications, enabling robust all-day perception by leveraging the
complementary information of infrared and visible images. However, existing
methods typically require dual-modality annotations to output detection results
for both modalities during prediction, which incurs high annotation costs. To
address this challenge, we propose a novel infrared-visible Decoupled Object
Detection framework with Single-modality Annotations, called DOD-SA. The
architecture of DOD-SA is built upon a Single- and Dual-Modality Collaborative
Teacher-Student Network (CoSD-TSNet), which consists of a single-modality
branch (SM-Branch) and a dual-modality decoupled branch (DMD-Branch). The
teacher model generates pseudo-labels for the unlabeled modality,
simultaneously supporting the training of the student model. The collaborative
design enables cross-modality knowledge transfer from the labeled modality to
the unlabeled modality, and facilitates effective SM-to-DMD branch supervision.
To further improve the decoupling ability of the model and the pseudo-label
quality, we introduce a Progressive and Self-Tuning Training Strategy (PaST)
that trains the model in three stages: (1) pretraining SM-Branch, (2) guiding
the learning of DMD-Branch by SM-Branch, and (3) refining DMD-Branch. In
addition, we design a Pseudo Label Assigner (PLA) to align and pair labels
across modalities, explicitly addressing modality misalignment during training.
Extensive experiments on the DroneVehicle dataset demonstrate that our method
outperforms state-of-the-art (SOTA).

</details>


### [68] [SkeySpot: Automating Service Key Detection for Digital Electrical Layout Plans in the Construction Industry](https://arxiv.org/abs/2508.10449)
*Dhruv Dosi,Rohit Meena,Param Rajpura,Yogesh Kumar Meena*

Main category: cs.CV

TL;DR: 该研究引入了一个名为DELP的新数据集和SkeySpot工具包，利用YOLOv8模型实现了对扫描旧版电气平面图中服务关键符号的自动化检测、分类和量化，旨在提高建筑行业中小企业数字化效率。


<details>
  <summary>Details</summary>
Motivation: 旧版平面图多为扫描文档，缺乏机器可读性，导致大规模解释耗时且易出错。为了支持成本估算、基础设施维护和法规遵从等工作流程，需要自动化符号识别解决方案。

Method: 1. 构建了DELP数据集，包含45张扫描电气平面图，标注了34类共2,450个符号实例。2. 提出了一个系统评估框架，使用预训练目标检测模型对DELP数据集进行评估。3. 基准测试了多种模型，并基于表现最佳的YOLOv8开发了轻量级、开源的SkeySpot工具包。

Result: 在基准测试的模型中，YOLOv8在DELP数据集上表现最佳，平均精度（mAP）达到82.5%。SkeySpot工具包能够实现电气符号的实时检测、分类和量化，并生成结构化、标准化的输出。

Conclusion: 该方法降低了对专有CAD系统的依赖和手动标注工作量，使中小企业更容易实现电气布局数字化，并支持建筑环境中标准化、互操作性和可持续性的更广泛目标。

Abstract: Legacy floor plans, often preserved only as scanned documents, remain
essential resources for architecture, urban planning, and facility management
in the construction industry. However, the lack of machine-readable floor plans
render large-scale interpretation both time-consuming and error-prone.
Automated symbol spotting offers a scalable solution by enabling the
identification of service key symbols directly from floor plans, supporting
workflows such as cost estimation, infrastructure maintenance, and regulatory
compliance. This work introduces a labelled Digitised Electrical Layout Plans
(DELP) dataset comprising 45 scanned electrical layout plans annotated with
2,450 instances across 34 distinct service key classes. A systematic evaluation
framework is proposed using pretrained object detection models for DELP
dataset. Among the models benchmarked, YOLOv8 achieves the highest performance
with a mean Average Precision (mAP) of 82.5\%. Using YOLOv8, we develop
SkeySpot, a lightweight, open-source toolkit for real-time detection,
classification, and quantification of electrical symbols. SkeySpot produces
structured, standardised outputs that can be scaled up for interoperable
building information workflows, ultimately enabling compatibility across
downstream applications and regulatory platforms. By lowering dependency on
proprietary CAD systems and reducing manual annotation effort, this approach
makes the digitisation of electrical layouts more accessible to small and
medium-sized enterprises (SMEs) in the construction industry, while supporting
broader goals of standardisation, interoperability, and sustainability in the
built environment.

</details>


### [69] [From Images to Perception: Emergence of Perceptual Properties by Reconstructing Images](https://arxiv.org/abs/2508.10450)
*Pablo Hernández-Cámara,Jesus Malo,Valero Laparra*

Main category: cs.CV

TL;DR: 生物启发模型PerceptNet在图像重建任务中，其编码器表现出与人类感知判断高度相关的特性，表明视觉系统可能通过优化来处理特定失真。


<details>
  <summary>Details</summary>
Motivation: 许多科学家认为人类视觉感知源于图像统计，并塑造了早期视觉中高效的神经表征。本研究旨在通过构建一个生物启发模型来验证这一观点。

Method: 开发了一个名为PerceptNet的生物启发架构，该架构能模拟视网膜-V1皮层的已知特性。该模型被端到端优化，用于自编码、去噪、去模糊和稀疏性正则化等图像重建任务。

Result: 研究发现，PerceptNet的编码器阶段（类似V1的层）与人类对图像失真的感知判断表现出最高的一致性，即使在初始化或训练中未使用感知信息。这种一致性在中等程度的噪声、模糊和稀疏性下达到最佳。

Conclusion: 这些发现表明视觉系统可能被调整以去除特定水平的失真和稀疏性，并且生物启发模型无需人类监督即可学习感知度量。

Abstract: A number of scientists suggested that human visual perception may emerge from
image statistics, shaping efficient neural representations in early vision. In
this work, a bio-inspired architecture that can accommodate several known facts
in the retina-V1 cortex, the PerceptNet, has been end-to-end optimized for
different tasks related to image reconstruction: autoencoding, denoising,
deblurring, and sparsity regularization. Our results show that the encoder
stage (V1-like layer) consistently exhibits the highest correlation with human
perceptual judgments on image distortion despite not using perceptual
information in the initialization or training. This alignment exhibits an
optimum for moderate noise, blur and sparsity. These findings suggest that the
visual system may be tuned to remove those particular levels of distortion with
that level of sparsity and that biologically inspired models can learn
perceptual metrics without human supervision.

</details>


### [70] [Trajectory-aware Shifted State Space Models for Online Video Super-Resolution](https://arxiv.org/abs/2508.10453)
*Qiang Zhu,Xiandong Meng,Yuxian Jiang,Fan Zhang,David Bull,Shuyuan Zhu,Bing Zeng*

Main category: cs.CV

TL;DR: 本文提出了一种名为TS-Mamba的新型在线视频超分辨率（VSR）方法，结合长程轨迹建模和低复杂度Mamba模型，实现了高效的时空信息聚合，并在性能和计算效率上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有的在线VSR方法通常只使用一个相邻的前一帧进行时间对齐，限制了视频的长程时间建模。尽管状态空间模型（SSMs）因其线性计算复杂度和全局感受野而提高了效率和性能，但如何将其有效应用于在线VSR并解决现有方法的局限性是一个挑战。

Method: TS-Mamba方法首先通过构建视频内的轨迹来选择来自先前帧的最相似的tokens。然后，利用一个轨迹感知移位Mamba聚合（TSMA）模块来聚合这些选定的tokens，该模块包含基于Hilbert扫描和相应移位操作设计的移位SSMs块，以弥补扫描损失并增强Mamba的空间连续性。此外，还提出了一种轨迹感知损失函数来监督轨迹生成，确保token选择的准确性。

Result: 在三个广泛使用的VSR测试数据集上进行的实验表明，与六个在线VSR基准模型相比，TS-Mamba在大多数情况下实现了最先进的性能，并实现了超过22.7%的复杂度降低（以MACs衡量）。

Conclusion: TS-Mamba通过结合长程轨迹建模和高效的Mamba架构，成功解决了在线VSR中的长程时间建模和计算效率问题，实现了卓越的性能和显著的复杂度降低，为在线视频处理应用提供了强大的技术支持。

Abstract: Online video super-resolution (VSR) is an important technique for many
real-world video processing applications, which aims to restore the current
high-resolution video frame based on temporally previous frames. Most of the
existing online VSR methods solely employ one neighboring previous frame to
achieve temporal alignment, which limits long-range temporal modeling of
videos. Recently, state space models (SSMs) have been proposed with linear
computational complexity and a global receptive field, which significantly
improve computational efficiency and performance. In this context, this paper
presents a novel online VSR method based on Trajectory-aware Shifted SSMs
(TS-Mamba), leveraging both long-term trajectory modeling and low-complexity
Mamba to achieve efficient spatio-temporal information aggregation.
Specifically, TS-Mamba first constructs the trajectories within a video to
select the most similar tokens from the previous frames. Then, a
Trajectory-aware Shifted Mamba Aggregation (TSMA) module consisting of proposed
shifted SSMs blocks is employed to aggregate the selected tokens. The shifted
SSMs blocks are designed based on Hilbert scannings and corresponding shift
operations to compensate for scanning losses and strengthen the spatial
continuity of Mamba. Additionally, we propose a trajectory-aware loss function
to supervise the trajectory generation, ensuring the accuracy of token
selection when training our model. Extensive experiments on three widely used
VSR test datasets demonstrate that compared with six online VSR benchmark
models, our TS-Mamba achieves state-of-the-art performance in most cases and
over 22.7\% complexity reduction (in MACs). The source code for TS-Mamba will
be available at https://github.com.

</details>


### [71] [Multi-Label Plant Species Prediction with Metadata-Enhanced Multi-Head Vision Transformers](https://arxiv.org/abs/2508.10457)
*Hanna Herasimchyk,Robin Labryga,Tomislav Prusina*

Main category: cs.CV

TL;DR: 本文提出了一种多头视觉Transformer方法，用于植物图像中的多标签植物物种预测，以应对PlantCLEF 2025挑战中的域偏移问题。


<details>
  <summary>Details</summary>
Motivation: PlantCLEF 2025挑战任务要求模型在单物种植物图像上训练，但在多物种样方图像上进行测试，这导致了显著的域偏移，需要开发鲁棒的预测方法。

Method: 该方法利用预训练的DINOv2 ViT-B/14作为骨干网络，并结合多分类头（用于物种、属、科预测）以利用分类学层次。关键技术包括多尺度平铺、基于平均预测长度的动态阈值优化、通过bagging和Hydra模型架构的集成策略，以及图像裁剪、top-n过滤和logit阈值等推理技术。

Result: 该方法在包含约140万张训练图像和7,806种植物物种的数据集上进行了实验，结果显示出强大的性能，在私人排行榜上排名第三。

Conclusion: 所提出的多头视觉Transformer方法，结合多尺度处理、动态阈值和集成策略，有效解决了PlantCLEF 2025挑战中的多标签植物物种预测和域偏移问题，取得了优异的性能。

Abstract: We present a multi-head vision transformer approach for multi-label plant
species prediction in vegetation plot images, addressing the PlantCLEF 2025
challenge. The task involves training models on single-species plant images
while testing on multi-species quadrat images, creating a drastic domain shift.
Our methodology leverages a pre-trained DINOv2 Vision Transformer Base
(ViT-B/14) backbone with multiple classification heads for species, genus, and
family prediction, utilizing taxonomic hierarchies. Key contributions include
multi-scale tiling to capture plants at different scales, dynamic threshold
optimization based on mean prediction length, and ensemble strategies through
bagging and Hydra model architectures. The approach incorporates various
inference techniques including image cropping to remove non-plant artifacts,
top-n filtering for prediction constraints, and logit thresholding strategies.
Experiments were conducted on approximately 1.4 million training images
covering 7,806 plant species. Results demonstrate strong performance, making
our submission 3rd best on the private leaderboard. Our code is available at
https://github.com/geranium12/plant-clef-2025/tree/v1.0.0.

</details>


### [72] [SingleStrip: learning skull-stripping from a single labeled example](https://arxiv.org/abs/2508.10464)
*Bella Specktor-Fadida,Malte Hoffmann*

Main category: cs.CV

TL;DR: 该研究结合域随机化和自训练，并引入基于自动编码器（AE）的质量控制，实现了仅使用极少量（甚至一个）标记数据进行三维颅骨剥离的半监督分割。


<details>
  <summary>Details</summary>
Motivation: 深度学习分割严重依赖标记数据，但手动标记（尤其是对于脑部MRI等体积图像）耗时耗力。现有方法如域随机化在标记图极少时解剖变异性有限，而半监督自训练虽能利用未标记数据，但仍需解决伪标签质量评估问题。

Method: 1. 自动对体素强度进行分箱，从一个标记样本生成标签图，并利用域随机化合成多样化图像训练初始颅骨剥离模型。2. 在标记样本上训练一个卷积自动编码器（AE）。3. 利用AE的重建误差评估未标记数据预测脑掩膜（伪标签）的质量。4. 选择排名靠前的伪标签来微调网络。5. 将AE-based排名与基于测试时增强的一致性排名进行比较。

Result: 该方法在分布外数据上实现了接近使用更多标记图像训练的模型性能的颅骨剥离效果。研究发现，基于AE的排名与分割精度具有更强的相关性。

Conclusion: 结合域随机化和基于AE的质量控制，能够从极其有限的标记数据中实现有效的半监督分割。该策略有望减轻新解剖结构或新兴成像技术研究中的标记负担。

Abstract: Deep learning segmentation relies heavily on labeled data, but manual
labeling is laborious and time-consuming, especially for volumetric images such
as brain magnetic resonance imaging (MRI). While recent domain-randomization
techniques alleviate the dependency on labeled data by synthesizing diverse
training images from label maps, they offer limited anatomical variability when
very few label maps are available. Semi-supervised self-training addresses
label scarcity by iteratively incorporating model predictions into the training
set, enabling networks to learn from unlabeled data. In this work, we combine
domain randomization with self-training to train three-dimensional
skull-stripping networks using as little as a single labeled example. First, we
automatically bin voxel intensities, yielding labels we use to synthesize
images for training an initial skull-stripping model. Second, we train a
convolutional autoencoder (AE) on the labeled example and use its
reconstruction error to assess the quality of brain masks predicted for
unlabeled data. Third, we select the top-ranking pseudo-labels to fine-tune the
network, achieving skull-stripping performance on out-of-distribution data that
approaches models trained with more labeled images. We compare AE-based ranking
to consistency-based ranking under test-time augmentation, finding that the AE
approach yields a stronger correlation with segmentation accuracy. Our results
highlight the potential of combining domain randomization and AE-based quality
control to enable effective semi-supervised segmentation from extremely limited
labeled data. This strategy may ease the labeling burden that slows progress in
studies involving new anatomical structures or emerging imaging techniques.

</details>


### [73] [Enhanced Sparse Point Cloud Data Processing for Privacy-aware Human Action Recognition](https://arxiv.org/abs/2508.10469)
*Maimunatu Tunau,Vincent Gbouna Zakka,Zhuangzhuang Dai*

Main category: cs.CV

TL;DR: 本文对毫米波雷达人体行为识别中常用的DBSCAN、匈牙利算法和卡尔曼滤波三种数据处理方法进行了全面评估，包括单独使用、两两组合及全部组合的性能，并提出了改进方案，为未来系统开发提供指导。


<details>
  <summary>Details</summary>
Motivation: 传统的视觉HAR系统存在隐私问题，而毫米波雷达传感器虽能保护隐私，但其点云数据稀疏且噪声大。文献中虽有多种数据处理方法，但缺乏对其单独和组合性能的全面评估。

Method: 使用MiliPoint数据集，对DBSCAN、匈牙利算法和卡尔曼滤波三种方法进行评估。评估方式包括：单独使用、所有可能的两两组合以及三种方法全部组合。评估指标为识别准确率和计算成本。此外，还提出了针对单一方法的改进措施以提高准确率。

Result: 研究结果深入揭示了每种方法及其组合的优势和权衡，为毫米波雷达HAR系统的未来工作提供了关键见解。

Conclusion: 本研究通过对现有毫米波雷达数据处理方法的全面评估和改进，为未来基于毫米波雷达的人体行为识别系统开发提供了重要指导。

Abstract: Human Action Recognition (HAR) plays a crucial role in healthcare, fitness
tracking, and ambient assisted living technologies. While traditional vision
based HAR systems are effective, they pose privacy concerns. mmWave radar
sensors offer a privacy preserving alternative but present challenges due to
the sparse and noisy nature of their point cloud data. In the literature, three
primary data processing methods: Density-Based Spatial Clustering of
Applications with Noise (DBSCAN), the Hungarian Algorithm, and Kalman Filtering
have been widely used to improve the quality and continuity of radar data.
However, a comprehensive evaluation of these methods, both individually and in
combination, remains lacking. This paper addresses that gap by conducting a
detailed performance analysis of the three methods using the MiliPoint dataset.
We evaluate each method individually, all possible pairwise combinations, and
the combination of all three, assessing both recognition accuracy and
computational cost. Furthermore, we propose targeted enhancements to the
individual methods aimed at improving accuracy. Our results provide crucial
insights into the strengths and trade-offs of each method and their
integrations, guiding future work on mmWave based HAR systems

</details>


### [74] [STAMP: Multi-pattern Attention-aware Multiple Instance Learning for STAS Diagnosis in Multi-center Histopathology Images](https://arxiv.org/abs/2508.10473)
*Liangrui Pan,xiaoyu Li,Guang Zhu,Guanting Li,Ruixin Wang,Jiadi Luo,Yaning Yang,Liang qingchun,Shaoliang Peng*

Main category: cs.CV

TL;DR: 该研究提出了一种名为STAMP的多模式注意力多实例学习框架，用于在肺腺癌（LUAD）中诊断气腔播散（STAS），并在多中心病理图像数据集上取得了超越临床水平的诊断性能。


<details>
  <summary>Details</summary>
Motivation: STAS是LUAD的一种新型侵袭模式，与肿瘤复发和生存率降低相关。然而，大规模STAS诊断耗时费力，且由于其独特的病理特征，易发生遗漏和误诊。因此，迫切需要利用深度学习模型进行STAS诊断。

Method: 研究首先从两家湘雅医院和TCGA-LUAD队列收集了STAS患者的组织病理图像，并由三位资深病理学家进行交叉验证注释，构建了STAS-SXY、STAS-TXY和STAS-TCGA数据集。然后，提出了一个名为STAMP的多模式注意力多实例学习框架，用于分析和诊断STAS。STAMP采用双分支架构从不同语义空间学习STAS相关病理特征，通过基于Transformer的实例编码和多模式注意力聚合模块动态选择STAS相关区域，并利用相似性正则化约束防止特征冗余。

Result: STAMP在STAS-SXY、STAS-TXY和STAS-TCGA数据集上取得了具有竞争力的诊断结果，AUC分别为0.8058、0.8017和0.7928，均超越了临床水平。

Conclusion: STAMP框架能够有效诊断LUAD中的STAS，为解决STAS诊断中存在的劳动力密集、易误诊等问题提供了有力的深度学习解决方案，并具有超越临床诊断水平的潜力。

Abstract: Spread through air spaces (STAS) constitutes a novel invasive pattern in lung
adenocarcinoma (LUAD), associated with tumor recurrence and diminished survival
rates. However, large-scale STAS diagnosis in LUAD remains a labor-intensive
endeavor, compounded by the propensity for oversight and misdiagnosis due to
its distinctive pathological characteristics and morphological features.
Consequently, there is a pressing clinical imperative to leverage deep learning
models for STAS diagnosis. This study initially assembled histopathological
images from STAS patients at the Second Xiangya Hospital and the Third Xiangya
Hospital of Central South University, alongside the TCGA-LUAD cohort. Three
senior pathologists conducted cross-verification annotations to construct the
STAS-SXY, STAS-TXY, and STAS-TCGA datasets. We then propose a multi-pattern
attention-aware multiple instance learning framework, named STAMP, to analyze
and diagnose the presence of STAS across multi-center histopathology images.
Specifically, the dual-branch architecture guides the model to learn
STAS-associated pathological features from distinct semantic spaces.
Transformer-based instance encoding and a multi-pattern attention aggregation
modules dynamically selects regions closely associated with STAS pathology,
suppressing irrelevant noise and enhancing the discriminative power of global
representations. Moreover, a similarity regularization constraint prevents
feature redundancy across branches, thereby improving overall diagnostic
accuracy. Extensive experiments demonstrated that STAMP achieved competitive
diagnostic results on STAS-SXY, STAS-TXY and STAS-TCGA, with AUCs of 0.8058,
0.8017, and 0.7928, respectively, surpassing the clinical level.

</details>


### [75] [TweezeEdit: Consistent and Efficient Image Editing with Path Regularization](https://arxiv.org/abs/2508.10498)
*Jianda Mao,Kaibo Wang,Yang Xiang,Kani Chen*

Main category: cs.CV

TL;DR: TweezeEdit是一个免微调、免反演的图像编辑框架，通过正则化整个去噪路径，解决了现有方法过度对齐目标提示并丢失源图像语义的问题，实现了高效且一致的图像编辑。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的图像编辑方法，通常通过反演噪声生成目标图像，但它们存在过度对齐目标提示导致源图像语义丢失的问题，并且编辑路径较长，效率低下。

Method: TweezeEdit提出了一种免微调和免反演的框架。它通过正则化整个去噪路径来保留源图像语义，而不是仅仅依赖反演锚点。该方法利用梯度驱动的正则化，结合一致性模型，沿直接路径高效注入目标提示语义，从而缩短了编辑路径。

Result: 实验证明，TweezeEdit在语义保留和目标对齐方面均优于现有方法。值得注意的是，它仅需12步（每次编辑1.6秒）即可完成，显示出其在实时应用中的巨大潜力。

Conclusion: TweezeEdit提供了一种高效、一致且无需反演或微调的图像编辑解决方案，有效解决了现有扩散模型在语义保留和编辑效率方面的局限性，特别适用于实时应用。

Abstract: Large-scale pre-trained diffusion models empower users to edit images through
text guidance. However, existing methods often over-align with target prompts
while inadequately preserving source image semantics. Such approaches generate
target images explicitly or implicitly from the inversion noise of the source
images, termed the inversion anchors. We identify this strategy as suboptimal
for semantic preservation and inefficient due to elongated editing paths. We
propose TweezeEdit, a tuning- and inversion-free framework for consistent and
efficient image editing. Our method addresses these limitations by regularizing
the entire denoising path rather than relying solely on the inversion anchors,
ensuring source semantic retention and shortening editing paths. Guided by
gradient-driven regularization, we efficiently inject target prompt semantics
along a direct path using a consistency model. Extensive experiments
demonstrate TweezeEdit's superior performance in semantic preservation and
target alignment, outperforming existing methods. Remarkably, it requires only
12 steps (1.6 seconds per edit), underscoring its potential for real-time
applications.

</details>


### [76] [Multi-Sample Anti-Aliasing and Constrained Optimization for 3D Gaussian Splatting](https://arxiv.org/abs/2508.10507)
*Zheng Zhou,Jia-Chen Zhang,Yu-Jie Xiong,Chun-Ming Xia*

Main category: cs.CV

TL;DR: 该研究提出一个结合多样本抗锯齿（MSAA）和双重几何约束的优化框架，以解决3D高斯泼溅在细节重建中出现的模糊问题，特别是在高频纹理和尖锐不连续区域，从而显著提升细节保留能力并保持实时渲染效率。


<details>
  <summary>Details</summary>
Motivation: 当前3D高斯泼溅技术在实时新视角合成方面表现出色，但由于场景优化过程中几何约束不足，导致对精细细节（尤其是在高频纹理和尖锐不连续区域）的重建常出现模糊。

Method: 该方法提出了一个综合优化框架，整合了多样本抗锯齿（MSAA）和双重几何约束。MSAA通过自适应混合四重子样本来计算像素颜色，有效减少高频分量中的锯齿伪影。框架引入了两个几何约束：(a) 基于动态梯度分析的自适应加权策略，优先处理重建不足的区域；(b) 梯度微分约束，用于在物体边界强制执行几何正则化。这种优化策略能将计算资源优先分配给需要精修的关键区域，同时保持全局一致性。

Result: 在多个基准测试上的大量实验评估表明，该方法在细节保留方面达到了最先进的性能，特别是在保留高频纹理和尖锐不连续性方面，同时保持了实时渲染效率。定量指标和感知研究证实，与基线方法相比，该方法在结构相似性（SSIM）和感知质量（LPIPS）方面均有统计学上的显著提升。

Conclusion: 该研究通过引入多样本抗锯齿和双重几何约束，有效解决了3D高斯泼溅在细节重建中的模糊问题，显著提升了高频纹理和尖锐不连续区域的重建质量，实现了卓越的细节保留能力和感知质量，同时保持了实时渲染性能。

Abstract: Recent advances in 3D Gaussian splatting have significantly improved
real-time novel view synthesis, yet insufficient geometric constraints during
scene optimization often result in blurred reconstructions of fine-grained
details, particularly in regions with high-frequency textures and sharp
discontinuities. To address this, we propose a comprehensive optimization
framework integrating multisample anti-aliasing (MSAA) with dual geometric
constraints. Our system computes pixel colors through adaptive blending of
quadruple subsamples, effectively reducing aliasing artifacts in high-frequency
components. The framework introduces two constraints: (a) an adaptive weighting
strategy that prioritizes under-reconstructed regions through dynamic gradient
analysis, and (b) gradient differential constraints enforcing geometric
regularization at object boundaries. This targeted optimization enables the
model to allocate computational resources preferentially to critical regions
requiring refinement while maintaining global consistency. Extensive
experimental evaluations across multiple benchmarks demonstrate that our method
achieves state-of-the-art performance in detail preservation, particularly in
preserving high-frequency textures and sharp discontinuities, while maintaining
real-time rendering efficiency. Quantitative metrics and perceptual studies
confirm statistically significant improvements over baseline approaches in both
structural similarity (SSIM) and perceptual quality (LPIPS).

</details>


### [77] [A Segmentation-driven Editing Method for Bolt Defect Augmentation and Detection](https://arxiv.org/abs/2508.10509)
*Yangjie Xiao,Ke Zhang,Jiacun Wang,Xin Sheng,Yurong Guo,Meijuan Chen,Zehua Ren,Zhaoye Zheng,Zhenbing Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种基于分割的螺栓缺陷编辑方法（SBDE），用于扩充输电线路螺栓缺陷数据集，以解决缺陷图像稀缺和数据分布不平衡的问题，从而提高缺陷检测性能。


<details>
  <summary>Details</summary>
Motivation: 输电线路螺栓缺陷检测对保障安全至关重要，但缺陷图像稀缺和数据分布不平衡严重限制了检测性能。

Method: 该方法包括三个步骤：1. 提出螺栓属性分割模型（Bolt-SAM），通过CLAHE-FFT Adapter（CFA）和Multipart-Aware Mask Decoder（MAMD）增强复杂螺栓属性的分割，生成高质量掩码。2. 设计掩码优化模块（MOD）并与图像修复模型（LaMa）结合，构建螺栓缺陷属性编辑模型（MOD-LaMa），通过属性编辑将正常螺栓转换为缺陷螺栓。3. 提出编辑恢复增强（ERA）策略，将编辑后的缺陷螺栓恢复并放回原始检查场景，以扩充缺陷检测数据集。

Result: 实验结果表明，SBDE生成的螺栓缺陷图像显著优于现有最先进的图像编辑模型，并有效提高了螺栓缺陷检测的性能。

Conclusion: 所提出的SBDE方法有效解决了螺栓缺陷图像稀缺和数据不平衡问题，验证了其在提高螺栓缺陷检测性能方面的有效性和应用潜力。

Abstract: Bolt defect detection is critical to ensure the safety of transmission lines.
However, the scarcity of defect images and imbalanced data distributions
significantly limit detection performance. To address this problem, we propose
a segmentationdriven bolt defect editing method (SBDE) to augment the dataset.
First, a bolt attribute segmentation model (Bolt-SAM) is proposed, which
enhances the segmentation of complex bolt attributes through the CLAHE-FFT
Adapter (CFA) and Multipart- Aware Mask Decoder (MAMD), generating high-quality
masks for subsequent editing tasks. Second, a mask optimization module (MOD) is
designed and integrated with the image inpainting model (LaMa) to construct the
bolt defect attribute editing model (MOD-LaMa), which converts normal bolts
into defective ones through attribute editing. Finally, an editing recovery
augmentation (ERA) strategy is proposed to recover and put the edited defect
bolts back into the original inspection scenes and expand the defect detection
dataset. We constructed multiple bolt datasets and conducted extensive
experiments. Experimental results demonstrate that the bolt defect images
generated by SBDE significantly outperform state-of-the-art image editing
models, and effectively improve the performance of bolt defect detection, which
fully verifies the effectiveness and application potential of the proposed
method. The code of the project is available at
https://github.com/Jay-xyj/SBDE.

</details>


### [78] [Med-GLIP: Advancing Medical Language-Image Pre-training with Large-scale Grounded Dataset](https://arxiv.org/abs/2508.10528)
*Ziye Deng,Ruihan He,Jiaxiang Liu,Yuan Wang,Zijie Meng,Songtao Jiang,Yong Xie,Zuozhu Liu*

Main category: cs.CV

TL;DR: 该论文构建了一个大规模多模态医学图像定位数据集Med-GLIP-5M，并提出了一个通用的模态感知定位框架Med-GLIP，在多项基准测试中超越现有技术，并显著提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像定位研究存在模态覆盖有限、标注粒度粗糙以及缺乏统一通用定位框架的局限性，阻碍了智能诊断、视觉问答和报告生成等任务的发展。

Method: 1. 构建Med-GLIP-5M数据集：包含超过530万个跨七种成像模态的区域级标注，支持分割和定位任务，并具有从器官到病灶的层级区域标签。2. 提出Med-GLIP框架：一个在Med-GLIP-5M上训练的模态感知定位框架，它能隐式地从多样数据中学习层级语义理解，无需显式设计的专家模块。

Result: Med-GLIP在多个定位基准测试中持续优于最先进的基线方法。将其空间输出集成到下游任务（如医学VQA和报告生成）中，可带来显著的性能提升。

Conclusion: 该研究通过构建大规模多模态数据集和提出创新的定位框架，有效解决了现有医学图像定位的挑战，并为智能医学诊断和相关AI应用奠定了坚实基础。

Abstract: Medical image grounding aims to align natural language phrases with specific
regions in medical images, serving as a foundational task for intelligent
diagnosis, visual question answering (VQA), and automated report generation
(MRG). However, existing research is constrained by limited modality coverage,
coarse-grained annotations, and the absence of a unified, generalizable
grounding framework. To address these challenges, we construct a large-scale
medical grounding dataset Med-GLIP-5M comprising over 5.3 million region-level
annotations across seven imaging modalities, covering diverse anatomical
structures and pathological findings. The dataset supports both segmentation
and grounding tasks with hierarchical region labels, ranging from organ-level
boundaries to fine-grained lesions. Based on this foundation, we propose
Med-GLIP, a modality-aware grounding framework trained on Med-GLIP-5M. Rather
than relying on explicitly designed expert modules, Med-GLIP implicitly
acquires hierarchical semantic understanding from diverse training data --
enabling it to recognize multi-granularity structures, such as distinguishing
lungs from pneumonia lesions. Extensive experiments demonstrate that Med-GLIP
consistently outperforms state-of-the-art baselines across multiple grounding
benchmarks. Furthermore, integrating its spatial outputs into downstream tasks,
including medical VQA and report generation, leads to substantial performance
gains. Our dataset will be released soon.

</details>


### [79] [EgoMusic-driven Human Dance Motion Estimation with Skeleton Mamba](https://arxiv.org/abs/2508.10522)
*Quang Nguyen,Nhat Le,Baoru Huang,Minh Nhat Vu,Chengcheng Tang,Van Nguyen,Ngan Le,Thieu Vo,Anh Nguyen*

Main category: cs.CV

TL;DR: 该研究提出了一种新方法，利用自我中心视角视频和音乐共同预测人体舞蹈动作，并构建了大规模数据集EgoAIST++，通过结合扩散模型和Skeleton Mamba网络实现了优于现有技术的效果。


<details>
  <summary>Details</summary>
Motivation: 现有研究多集中于单独使用自我中心视角视频或音乐来预测人体舞蹈动作，但同时结合两者进行动作估计的任务仍未被充分探索。此外，自我中心视角常遮挡身体大部分，使得全身姿态估计困难，且生成的动作需同时与视觉和音乐输入对齐。

Method: 1. 构建了新的大规模数据集EgoAIST++，包含超过36小时的舞蹈动作，结合了自我中心视角和音乐。2. 开发了EgoMusic Motion Network，其核心是Skeleton Mamba模型，该模型明确捕捉人体骨骼结构。3. 方法借鉴了扩散模型和Mamba在序列建模上的成功经验。

Result: 实验结果表明，所提出的方法显著优于现有最先进的方法，并能有效泛化到真实世界数据。

Conclusion: 该研究开发了一种有效且理论上支持的新方法，能够从自我中心视角视频和音乐共同预测人体舞蹈动作，解决了现有技术的局限性，并提供了新的大规模数据集以促进未来研究。

Abstract: Estimating human dance motion is a challenging task with various industrial
applications. Recently, many efforts have focused on predicting human dance
motion using either egocentric video or music as input. However, the task of
jointly estimating human motion from both egocentric video and music remains
largely unexplored. In this paper, we aim to develop a new method that predicts
human dance motion from both egocentric video and music. In practice, the
egocentric view often obscures much of the body, making accurate full-pose
estimation challenging. Additionally, incorporating music requires the
generated head and body movements to align well with both visual and musical
inputs. We first introduce EgoAIST++, a new large-scale dataset that combines
both egocentric views and music with more than 36 hours of dancing motion.
Drawing on the success of diffusion models and Mamba on modeling sequences, we
develop an EgoMusic Motion Network with a core Skeleton Mamba that explicitly
captures the skeleton structure of the human body. We illustrate that our
approach is theoretically supportive. Intensive experiments show that our
method clearly outperforms state-of-the-art approaches and generalizes
effectively to real-world data.

</details>


### [80] [Reasoning in Computer Vision: Taxonomy, Models, Tasks, and Methodologies](https://arxiv.org/abs/2508.10523)
*Ayushman Sarkar,Mohd Yamani Idna Idris,Zhenyu Yu*

Main category: cs.CV

TL;DR: 这篇综述对视觉推理进行了全面分析，将其分为五种类型，并系统地审查了其实现方法、评估协议、当前局限性以及未来的研究挑战和方向。


<details>
  <summary>Details</summary>
Motivation: 现有关于关系、符号、时间、因果和常识推理的综述通常孤立地处理这些方向，缺乏对不同推理类型、方法和评估协议的统一分析和比较，因此本研究旨在填补这一空白。

Method: 本研究将视觉推理分为五种主要类型（关系、符号、时间、因果和常识），系统地考察了通过图模型、记忆网络、注意力机制和神经符号系统等架构实现的各种方法。同时，审查了旨在评估功能正确性、结构一致性和因果有效性的评估协议，并分析了其在泛化性、可复现性和解释力方面的局限性。最后，识别了视觉推理中的关键开放挑战。

Result: 本综述系统地分类并考察了视觉推理的五种主要类型及其实现架构，分析了评估协议的有效性与局限性，并识别了当前视觉推理领域的主要开放挑战，包括复杂场景的可扩展性、符号与神经范式的深度融合、缺乏全面的基准数据集以及弱监督下的推理。

Conclusion: 弥合感知与推理之间的鸿沟对于构建透明、可信和跨领域自适应的AI系统至关重要，特别是在自动驾驶和医疗诊断等关键领域。未来的研究议程应侧重于此，以推动下一代视觉系统的发展。

Abstract: Visual reasoning is critical for a wide range of computer vision tasks that
go beyond surface-level object detection and classification. Despite notable
advances in relational, symbolic, temporal, causal, and commonsense reasoning,
existing surveys often address these directions in isolation, lacking a unified
analysis and comparison across reasoning types, methodologies, and evaluation
protocols. This survey aims to address this gap by categorizing visual
reasoning into five major types (relational, symbolic, temporal, causal, and
commonsense) and systematically examining their implementation through
architectures such as graph-based models, memory networks, attention
mechanisms, and neuro-symbolic systems. We review evaluation protocols designed
to assess functional correctness, structural consistency, and causal validity,
and critically analyze their limitations in terms of generalizability,
reproducibility, and explanatory power. Beyond evaluation, we identify key open
challenges in visual reasoning, including scalability to complex scenes, deeper
integration of symbolic and neural paradigms, the lack of comprehensive
benchmark datasets, and reasoning under weak supervision. Finally, we outline a
forward-looking research agenda for next-generation vision systems, emphasizing
that bridging perception and reasoning is essential for building transparent,
trustworthy, and cross-domain adaptive AI systems, particularly in critical
domains such as autonomous driving and medical diagnostics.

</details>


### [81] [Retrieval-Augmented Prompt for OOD Detection](https://arxiv.org/abs/2508.10556)
*Ruisong Han,Zongbo Han,Jiahao Zhang,Mingyue Cheng,Changqing Zhang*

Main category: cs.CV

TL;DR: 提出了一种名为RAP的新型OOD检测方法，通过检索外部知识增强预训练视觉-语言模型的提示，以提供更强的语义监督，从而解决现有方法在异常样本不足和不匹配问题上的性能限制。


<details>
  <summary>Details</summary>
Motivation: 现有的OOD检测方法依赖辅助异常样本或ID数据生成异常信息进行训练，但由于异常样本有限且与真实测试OOD样本不匹配，导致语义监督不足，性能欠佳。

Method: 提出检索增强提示（RAP）方法。在训练阶段，RAP基于与外部文本知识的联合相似性，为异常样本检索描述性词语来增强模型的OOD提示。在测试阶段，RAP根据遇到的OOD样本实时动态更新OOD提示，使模型能快速适应测试环境。

Result: RAP在大规模OOD检测基准上实现了最先进的性能。例如，在ImageNet-1k数据集的1-shot OOD检测中，RAP相比现有方法将平均FPR95降低了7.05%，AUROC提高了1.71%。全面的消融研究也验证了每个模块的有效性。

Conclusion: RAP通过检索增强和动态更新的提示，有效解决了现有OOD检测方法语义监督不足的问题，显著提升了OOD检测的性能和适应性。

Abstract: Out-of-Distribution (OOD) detection is crucial for the reliable deployment of
machine learning models in-the-wild, enabling accurate identification of test
samples that differ from the training data distribution. Existing methods rely
on auxiliary outlier samples or in-distribution (ID) data to generate outlier
information for training, but due to limited outliers and their mismatch with
real test OOD samples, they often fail to provide sufficient semantic
supervision, leading to suboptimal performance. To address this, we propose a
novel OOD detection method called Retrieval-Augmented Prompt (RAP). RAP
augments a pre-trained vision-language model's prompts by retrieving external
knowledge, offering enhanced semantic supervision for OOD detection. During
training, RAP retrieves descriptive words for outliers based on joint
similarity with external textual knowledge and uses them to augment the model's
OOD prompts. During testing, RAP dynamically updates OOD prompts in real-time
based on the encountered OOD samples, enabling the model to rapidly adapt to
the test environment. Our extensive experiments demonstrate that RAP achieves
state-of-the-art performance on large-scale OOD detection benchmarks. For
example, in 1-shot OOD detection on the ImageNet-1k dataset, RAP reduces the
average FPR95 by 7.05% and improves the AUROC by 1.71% compared to previous
methods. Additionally, comprehensive ablation studies validate the
effectiveness of each module and the underlying motivations of our approach.

</details>


### [82] [GCRPNet: Graph-Enhanced Contextual and Regional Perception Network For Salient Object Detection in Optical Remote Sensing Images](https://arxiv.org/abs/2508.10542)
*Mengyu Ren,Yutong Li,Hua Li,Runmin Cong,Sam Kwong*

Main category: cs.CV

TL;DR: 针对遥感图像中的显著目标检测（SOD）挑战，本文提出了一种基于Mamba架构的图增强上下文与区域感知网络（GCRPNet），通过创新的模块有效整合多尺度特征和局部信息，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 光学遥感图像（ORSIs）中的显著目标检测面临目标尺度变化大和目标与背景对比度低等挑战。现有基于ViT和CNN的方法难以有效整合全局和局部特征，限制了性能。

Method: 提出GCRPNet，基于Mamba架构，同时捕获长程依赖并增强区域特征表示。使用视觉状态空间（VSS）编码器提取多尺度特征。设计了差异相似性引导的分层图注意力模块（DS-HGAM）以增强跨层交互和结构感知。设计了LEVSS块作为解码器，该块整合了自适应扫描策略和多粒度协同注意力增强模块（MCAEM），通过对多尺度卷积处理后的特征图进行自适应补丁扫描，增强Mamba的局部建模能力。

Result: 实验结果表明，所提出的模型实现了最先进的性能。

Conclusion: GCRPNet在光学遥感图像显著目标检测中表现出有效性和优越性，成功克服了现有方法在特征整合上的局限性。

Abstract: Salient object detection (SOD) in optical remote sensing images (ORSIs) faces
numerous challenges, including significant variations in target scales and low
contrast between targets and the background. Existing methods based on vision
transformers (ViTs) and convolutional neural networks (CNNs) architectures aim
to leverage both global and local features, but the difficulty in effectively
integrating these heterogeneous features limits their overall performance. To
overcome these limitations, we propose a graph-enhanced contextual and regional
perception network (GCRPNet), which builds upon the Mamba architecture to
simultaneously capture long-range dependencies and enhance regional feature
representation. Specifically, we employ the visual state space (VSS) encoder to
extract multi-scale features. To further achieve deep guidance and enhancement
of these features, we first design a difference-similarity guided hierarchical
graph attention module (DS-HGAM). This module strengthens cross-layer
interaction capabilities between features of different scales while enhancing
the model's structural perception,allowing it to distinguish between foreground
and background more effectively. Then, we design the LEVSS block as the decoder
of GCRPNet. This module integrates our proposed adaptive scanning strategy and
multi-granularity collaborative attention enhancement module (MCAEM). It
performs adaptive patch scanning on feature maps processed via multi-scale
convolutions, thereby capturing rich local region information and enhancing
Mamba's local modeling capability. Extensive experimental results demonstrate
that the proposed model achieves state-of-the-art performance, validating its
effectiveness and superiority.

</details>


### [83] [PTQAT: A Hybrid Parameter-Efficient Quantization Algorithm for 3D Perception Tasks](https://arxiv.org/abs/2508.10557)
*Xinhao Wang,Zhiwei Lin,Zhongyu Xia,Yongtao Wang*

Main category: cs.CV

TL;DR: 提出了一种名为PTQAT的混合量化算法，通过选择性地对关键层进行QAT微调和对剩余层进行PTQ，在3D感知任务中实现了接近QAT的性能，同时显著提高了效率，并超越了纯QAT基线。


<details>
  <summary>Details</summary>
Motivation: PTQ（训练后量化）通常导致模型性能显著下降，而QAT（量化感知训练）则需要大量的GPU内存和训练时间。因此，需要一种方法来平衡量化模型的精度和部署效率。

Method: PTQAT是一种新颖的通用混合量化算法。它选择性地对关键层进行QAT微调，并对剩余层执行PTQ。与直觉相反，该方法发现对量化前后输出差异较小的层进行微调，而不是差异较大的层，能带来更大的模型量化精度提升，旨在更好地补偿量化误差的传播。该方法冻结了近50%的可量化层。

Result: PTQAT实现了与QAT相似的性能，但效率更高（通过冻结近50%的可量化层）。它是一种通用量化方法，支持多种量化位宽（如4位）和不同模型架构（包括CNN和Transformer）。在nuScenes数据集上，针对3D目标检测、语义分割和占用预测等任务的实验结果表明，PTQAT始终优于纯QAT基线，例如在目标检测中NDS提升0.2%-0.9%，mAP提升0.3%-1.0%，在语义分割和占用预测中mIoU提升0.3%-2.0%，且微调的权重更少。

Conclusion: PTQAT是一种高效且通用的混合量化算法，成功解决了PTQ和QAT之间的权衡问题。它通过选择性微调和独特的误差补偿策略，在3D感知网络上实现了卓越的性能和效率，超越了传统的QAT方法。

Abstract: Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT)
represent two mainstream model quantization approaches. However, PTQ often
leads to unacceptable performance degradation in quantized models, while QAT
imposes substantial GPU memory requirements and extended training time due to
weight fine-tuning.In this paper, we propose PTQAT, a novel general hybrid
quantization algorithm for the efficient deployment of 3D perception networks.
To address the speed accuracy trade-off between PTQ and QAT, our method selects
critical layers for QAT fine-tuning and performs PTQ on the remaining layers.
Contrary to intuition, fine-tuning the layers with smaller output discrepancies
before and after quantization, rather than those with larger discrepancies,
actually leads to greater improvements in the model's quantization accuracy.
This means we better compensate for quantization errors during their
propagation, rather than addressing them at the point where they occur. The
proposed PTQAT achieves similar performance to QAT with more efficiency by
freezing nearly 50% of quantifiable layers. Additionally, PTQAT is a universal
quantization method that supports various quantization bit widths (4 bits) as
well as different model architectures, including CNNs and Transformers. The
experimental results on nuScenes across diverse 3D perception tasks, including
object detection, semantic segmentation, and occupancy prediction, show that
our method consistently outperforms QAT-only baselines. Notably, it achieves
0.2%-0.9% NDS and 0.3%-1.0% mAP gains in object detection, 0.3%-2.0% mIoU gains
in semantic segmentation and occupancy prediction while fine-tuning fewer
weights.

</details>


### [84] [PSScreen: Partially Supervised Multiple Retinal Disease Screening](https://arxiv.org/abs/2508.10549)
*Boyi Zheng,Qing Liu*

Main category: cs.CV

TL;DR: PSScreen是一种新颖的半监督多视网膜疾病筛查模型，旨在利用多个部分标注数据集，解决跨领域域偏移和标签缺失问题，提高筛查性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 在多视网膜疾病筛查中，过度依赖完全标注数据集，且来自不同医疗站点的训练数据集存在显著的域偏移，以及部分类别标签缺失，这些都带来了巨大挑战。

Method: PSScreen包含两个流：一个学习确定性特征，另一个通过不确定性注入学习概率性特征。模型利用文本指导将两种特征解耦为疾病特异性特征，并通过特征蒸馏对齐以增强域泛化能力。同时，采用两流间的伪标签一致性解决标签缺失问题，并引入自蒸馏将已知类别的任务相关语义从确定性流传输到概率性流以进一步提升检测性能。

Result: 实验结果表明，PSScreen显著提高了对六种视网膜疾病和正常状态的平均检测性能，并在域内和域外数据集上均达到了最先进的水平。

Conclusion: PSScreen模型有效解决了利用多个部分标注数据集进行多视网膜疾病筛查所面临的域偏移和标签缺失挑战，显著提升了模型的检测性能和泛化能力。

Abstract: Leveraging multiple partially labeled datasets to train a model for multiple
retinal disease screening reduces the reliance on fully annotated datasets, but
remains challenging due to significant domain shifts across training datasets
from various medical sites, and the label absent issue for partial classes. To
solve these challenges, we propose PSScreen, a novel Partially Supervised
multiple retinal disease Screening model. Our PSScreen consists of two streams
and one learns deterministic features and the other learns probabilistic
features via uncertainty injection. Then, we leverage the textual guidance to
decouple two types of features into disease-wise features and align them via
feature distillation to boost the domain generalization ability. Meanwhile, we
employ pseudo label consistency between two streams to address the label absent
issue and introduce a self-distillation to transfer task-relevant semantics
about known classes from the deterministic to the probabilistic stream to
further enhance the detection performances. Experiments show that our PSScreen
significantly enhances the detection performances on six retinal diseases and
the normal state averagely and achieves state-of-the-art results on both
in-domain and out-of-domain datasets. Codes are available at
https://github.com/boyiZheng99/PSScreen.

</details>


### [85] [Fourier-Guided Attention Upsampling for Image Super-Resolution](https://arxiv.org/abs/2508.10616)
*Daejune Choi,Youchan No,Jinhyung Lee,Duksu Kim*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级上采样模块频率引导注意力（FGA），通过结合傅里叶特征MLP、跨分辨率关联注意力层和频域L1损失，有效解决了传统超分上采样方法在高频细节重建和消除混叠伪影方面的不足。


<details>
  <summary>Details</summary>
Motivation: 传统的上采样器（如亚像素卷积）虽然高效，但往往无法重建高频细节并引入混叠伪影。

Method: FGA模块整合了三部分：1) 基于傅里叶特征的多层感知机（MLP）用于位置频率编码；2) 跨分辨率关联注意力层用于自适应空间对齐；3) 频域L1损失用于频谱保真度监督。

Result: FGA仅增加了0.3M参数，在五种不同的超分辨率骨干网络上（轻量级和全容量场景）均持续提升了性能。实验结果显示，平均PSNR增益为0.12~0.14 dB，频域一致性提高了高达29%，尤其在纹理丰富的数集上表现显著。视觉和频谱评估证实了FGA在减少混叠和保留精细细节方面的有效性。

Conclusion: FGA是一种实用、可扩展的传统上采样方法替代方案，能有效减少混叠并保留精细细节，提升超分辨率性能。

Abstract: We propose Frequency-Guided Attention (FGA), a lightweight upsampling module
for single image super-resolution. Conventional upsamplers, such as Sub-Pixel
Convolution, are efficient but frequently fail to reconstruct high-frequency
details and introduce aliasing artifacts. FGA addresses these issues by
integrating (1) a Fourier feature-based Multi-Layer Perceptron (MLP) for
positional frequency encoding, (2) a cross-resolution Correlation Attention
Layer for adaptive spatial alignment, and (3) a frequency-domain L1 loss for
spectral fidelity supervision. Adding merely 0.3M parameters, FGA consistently
enhances performance across five diverse super-resolution backbones in both
lightweight and full-capacity scenarios. Experimental results demonstrate
average PSNR gains of 0.12~0.14 dB and improved frequency-domain consistency by
up to 29%, particularly evident on texture-rich datasets. Visual and spectral
evaluations confirm FGA's effectiveness in reducing aliasing and preserving
fine details, establishing it as a practical, scalable alternative to
traditional upsampling methods.

</details>


### [86] [AR Surgical Navigation With Surface Tracing: Comparing In-SitVisualization with Tool-Tracking Guidance for Neurosurgical Applications](https://arxiv.org/abs/2508.10554)
*Marc J. Fischer,Jeffrey Potts,Gabriel Urreola,Dax Jones,Paolo Palmisciano,E. Bradley Strong,Branden Cord,Andrew D. Hernandez,Julia D. Sharma,E. Brandon Strong*

Main category: cs.CV

TL;DR: 本研究提出了一种基于HoloLens 2的增强现实(AR)手术导航新方法，通过实时工具追踪显著提高了模拟导管置入的精度和用户体验，克服了传统AR深度感知和遮挡处理的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统的AR手术导航系统存在深度感知问题（如聚散调节冲突）和当前商用显示技术的遮挡处理限制，这在对精度要求极高的手术环境中带来了严峻挑战。本研究旨在克服这些局限性。

Method: 本研究提出了一种利用AR引导注册解剖目标并提供实时器械导航的新方法。以在人体模型上放置模拟脑室外引流导管为临床场景。系统通过一种新颖的表面追踪方法将目标位置注册到患者，并仅利用Microsoft HoloLens 2的板载传感器进行实时红外工具追踪以辅助导管放置。用户在两种AR引导条件下进行模拟插入：静态原位可视化（计划轨迹直接叠加在解剖结构上）和实时工具追踪引导（提供导管姿态相对于计划的实时反馈）。插入测试后，获取人体模型的CT扫描以评估插入精度、目标偏差、角度误差和深度精度。通过系统可用性量表（SUS）调查评估用户体验和认知负荷。

Result: 实时工具追踪引导在所有精度测量指标上均提高了性能，并在主观评估中受到用户的青睐。

Conclusion: 实时工具追踪的AR引导显著提高了手术精度和用户满意度，为下一代术中手术指导提供了有前景的解决方案，有效解决了当前AR技术在手术应用中的关键挑战。

Abstract: Augmented Reality (AR) surgical navigation systems are emerging as the next
generation of intraoperative surgical guidance, promising to overcome
limitations of traditional navigation systems. However, known issues with AR
depth perception due to vergence-accommodation conflict and occlusion handling
limitations of the currently commercially available display technology present
acute challenges in surgical settings where precision is paramount. This study
presents a novel methodology for utilizing AR guidance to register anatomical
targets and provide real-time instrument navigation using placement of
simulated external ventricular drain catheters on a phantom model as the
clinical scenario. The system registers target positions to the patient through
a novel surface tracing method and uses real-time infrared tool tracking to aid
in catheter placement, relying only on the onboard sensors of the Microsoft
HoloLens 2. A group of intended users performed the procedure of simulated
insertions under two AR guidance conditions: static in-situ visualization,
where planned trajectories are overlaid directly onto the patient anatomy, and
real-time tool-tracking guidance, where live feedback of the catheter's pose is
provided relative to the plan. Following the insertion tests, computed
tomography scans of the phantom models were acquired, allowing for evaluation
of insertion accuracy, target deviation, angular error, and depth precision.
System Usability Scale surveys assessed user experience and cognitive workload.
Tool-tracking guidance improved performance metrics across all accuracy
measures and was preferred by users in subjective evaluations. A free copy of
this paper and all supplemental materials are available at
https://bit.ly/45l89Hq.

</details>


### [87] [Serial Over Parallel: Learning Continual Unification for Multi-Modal Visual Object Tracking and Benchmarking](https://arxiv.org/abs/2508.10655)
*Zhangyong Tang,Tianyang Xu,Xuefeng Zhu,Chunyang Cheng,Tao Zhou,Xiaojun Wu,Josef Kittler*

Main category: cs.CV

TL;DR: 本文提出了一个统一的多模态视觉目标跟踪（MMVOT）基准UniBench300，并通过将任务统一过程重构为序列格式并引入持续学习（CL）来解决现有方法中训练与测试不一致导致的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 现有MMVOT任务统一方法将所有传感器数据混合训练，但缺乏统一的评估基准，导致训练与测试不一致，进而引发性能下降。

Method: 1. 引入统一基准UniBench300，整合多任务数据，减少推理次数并缩短时间。2. 将统一过程重构为序列格式，逐步集成新任务，并将其视为持续学习中的知识遗忘问题，从而引入持续学习方法来稳定统一过程。

Result: UniBench300显著提升了训练和测试的一致性，减少了推理时间和计算量。持续学习在支持稳定统一过程中表现出优越性。研究发现性能下降与网络容量呈负相关，且模态差异导致不同任务（RGBT > RGBD > RGBE）的下降程度不同。

Conclusion: UniBench300和持续学习对统一多模态视觉目标跟踪任务具有重要意义。性能下降与网络容量和模态差异相关，这为未来的多模态视觉研究提供了有价值的见解。

Abstract: Unifying multiple multi-modal visual object tracking (MMVOT) tasks draws
increasing attention due to the complementary nature of different modalities in
building robust tracking systems. Existing practices mix all data sensor types
in a single training procedure, structuring a parallel paradigm from the
data-centric perspective and aiming for a global optimum on the joint
distribution of the involved tasks. However, the absence of a unified benchmark
where all types of data coexist forces evaluations on separated benchmarks,
causing \textit{inconsistency} between training and testing, thus leading to
performance \textit{degradation}. To address these issues, this work advances
in two aspects: \ding{182} A unified benchmark, coined as UniBench300, is
introduced to bridge the inconsistency by incorporating multiple task data,
reducing inference passes from three to one and cutting time consumption by
27\%. \ding{183} The unification process is reformulated in a serial format,
progressively integrating new tasks. In this way, the performance degradation
can be specified as knowledge forgetting of previous tasks, which naturally
aligns with the philosophy of continual learning (CL), motivating further
exploration of injecting CL into the unification process. Extensive experiments
conducted on two baselines and four benchmarks demonstrate the significance of
UniBench300 and the superiority of CL in supporting a stable unification
process. Moreover, while conducting dedicated analyses, the performance
degradation is found to be negatively correlated with network capacity.
Additionally, modality discrepancies contribute to varying degradation levels
across tasks (RGBT > RGBD > RGBE in MMVOT), offering valuable insights for
future multi-modal vision research. Source codes and the proposed benchmark is
available at \textit{https://github.com/Zhangyong-Tang/UniBench300}.

</details>


### [88] [HM-Talker: Hybrid Motion Modeling for High-Fidelity Talking Head Synthesis](https://arxiv.org/abs/2508.10566)
*Shiyu Liu,Kui Jiang,Xianming Liu,Hongxun Yao,Xiaocheng Feng*

Main category: cs.CV

TL;DR: HM-Talker是一个新颖的框架，通过结合隐式和显式（基于动作单元AU）运动线索，生成高质量、时间连贯的音频驱动说话人视频，解决了现有方法中运动模糊和唇部抖动的问题。


<details>
  <summary>Details</summary>
Motivation: 现有音频驱动说话人视频生成方法常产生运动模糊和唇部抖动，主要原因是它们依赖隐式建模音频-面部运动关联，缺乏明确的发音先验（即语音相关面部运动的解剖学指导）。

Method: HM-Talker采用混合运动表示，结合隐式和显式运动线索（如动作单元AUs）。它包含两个关键模块：1. 跨模态解耦模块（CMDM），用于提取互补的隐式/显式运动特征，并直接从音频预测AUs。2. 混合运动建模模块（HMMM），通过动态合并随机配对的隐式/显式特征，减轻显式特征中的身份依赖偏差，实现身份无关学习，从而增强跨主题泛化能力。

Result: HM-Talker的组件共同实现了跨不同身份的鲁棒唇部同步。大量实验证明，HM-Talker在视觉质量和唇同步精度方面均优于现有最先进的方法。

Conclusion: HM-Talker通过创新性地结合隐式和显式运动表示，有效克服了现有音频驱动说话人视频生成中的局限性，显著提升了视频的视觉质量和唇同步准确性，并在个性化说话人合成方面取得了重要进展。

Abstract: Audio-driven talking head video generation enhances user engagement in
human-computer interaction. However, current methods frequently produce videos
with motion blur and lip jitter, primarily due to their reliance on implicit
modeling of audio-facial motion correlations--an approach lacking explicit
articulatory priors (i.e., anatomical guidance for speech-related facial
movements). To overcome this limitation, we propose HM-Talker, a novel
framework for generating high-fidelity, temporally coherent talking heads.
HM-Talker leverages a hybrid motion representation combining both implicit and
explicit motion cues. Explicit cues use Action Units (AUs), anatomically
defined facial muscle movements, alongside implicit features to minimize
phoneme-viseme misalignment. Specifically, our Cross-Modal Disentanglement
Module (CMDM) extracts complementary implicit/explicit motion features while
predicting AUs directly from audio input aligned to visual cues. To mitigate
identity-dependent biases in explicit features and enhance cross-subject
generalization, we introduce the Hybrid Motion Modeling Module (HMMM). This
module dynamically merges randomly paired implicit/explicit features, enforcing
identity-agnostic learning. Together, these components enable robust lip
synchronization across diverse identities, advancing personalized talking head
synthesis. Extensive experiments demonstrate HM-Talker's superiority over
state-of-the-art methods in visual quality and lip-sync accuracy.

</details>


### [89] [AddressVLM: Cross-view Alignment Tuning for Image Address Localization using Large Vision-Language Models](https://arxiv.org/abs/2508.10667)
*Shixiong Xu,Chenghao Zhang,Lubin Fan,Yuan Zhou,Bin Fan,Shiming Xiang,Gaofeng Meng,Jieping Ye*

Main category: cs.CV

TL;DR: 本文提出AddressVLM模型，通过整合卫星图像作为宏观线索和跨视图对齐微调，显著提升了大型视觉语言模型（LVLMs）在城市区域内的精细街景地址定位能力，并在新构建的数据集上取得了优于现有LVLMs的性能。


<details>
  <summary>Details</summary>
Motivation: 现有LVLMs在国家或城市层面的粗粒度地理定位表现出色，但在城市区域内的精细街景级定位上表现不佳。街景VQA数据仅提供微观视觉线索，导致微调模型性能不佳。研究旨在将城市范围的地址定位能力集成到LVLMs中，以实现基于街景图像的灵活地址相关问答。

Method: 引入透视不变的卫星图像作为宏观线索，并提出跨视图对齐微调机制，包括卫星视图和街景图像嫁接机制以及自动标签生成机制。通过跨视图匹配增强LVLM对街道分布的全局理解。模型名为AddressVLM，采用两阶段训练协议：跨视图对齐微调和地址定位微调。此外，构建了两个基于匹兹堡和旧金山图像地址定位数据集的街景VQA数据集。

Result: 定性和定量评估表明，AddressVLM在所构建的两个数据集上，平均地址定位精度分别超过同类LVLMs 9%和12%。

Conclusion: 通过结合卫星图像的宏观线索和创新的跨视图对齐微调策略，AddressVLM成功地将精细的街景地址定位能力集成到LVLMs中，显著优于现有模型，为基于街景图像的地址相关问答提供了有效解决方案。

Abstract: Large visual language models (LVLMs) have demonstrated impressive performance
in coarse-grained geo-localization at the country or city level, but they
struggle with fine-grained street-level localization within urban areas. In
this paper, we explore integrating city-wide address localization capabilities
into LVLMs, facilitating flexible address-related question answering using
street-view images. A key challenge is that the street-view visual
question-and-answer (VQA) data provides only microscopic visual cues, leading
to subpar performance in fine-tuned models. To tackle this issue, we
incorporate perspective-invariant satellite images as macro cues and propose
cross-view alignment tuning including a satellite-view and street-view image
grafting mechanism, along with an automatic label generation mechanism. Then
LVLM's global understanding of street distribution is enhanced through
cross-view matching. Our proposed model, named AddressVLM, consists of
two-stage training protocols: cross-view alignment tuning and address
localization tuning. Furthermore, we have constructed two street-view VQA
datasets based on image address localization datasets from Pittsburgh and San
Francisco. Qualitative and quantitative evaluations demonstrate that AddressVLM
outperforms counterpart LVLMs by over 9% and 12% in average address
localization accuracy on these two datasets, respectively.

</details>


### [90] [Adapting SAM via Cross-Entropy Masking for Class Imbalance in Remote Sensing Change Detection](https://arxiv.org/abs/2508.10568)
*Humza Naveed,Xina Zeng,Mitch Bryson,Nagita Mehrseresht*

Main category: cs.CV

TL;DR: 本文提出了一种名为SAM-CEM-CD的方法，通过微调SAM编码器并结合时空特征增强、多尺度解码器融合以及新颖的交叉熵掩蔽损失，显著提升了遥感图像变化检测的性能。


<details>
  <summary>Details</summary>
Motivation: 基础模型（如SAM）在计算机视觉领域取得了巨大成功，但遥感变化检测面临多尺度变化和类别不平衡等挑战，需要专门的适应性改进。

Method: 该方法包括：1) 微调SAM编码器以适应遥感变化检测；2) 引入时空特征增强（STFE）和多尺度解码器融合（MSDF）以鲁棒地检测多尺度变化；3) 提出一种新的交叉熵掩蔽（CEM）损失来处理变化检测数据集中严重的类别不平衡问题。

Result: 该方法在Levir-CD、WHU-CD、CLCD和S2Looking四个变化检测数据集上均超越了现有最先进（SOTA）方法，特别是在大型复杂S2Looking数据集上F1分数提高了2.5%。

Conclusion: 通过对SAM的有效适应和引入针对变化检测特性的新模块及损失函数，本文提出的方法在遥感变化检测任务中表现出卓越的性能和鲁棒性。

Abstract: Foundational models have achieved significant success in diverse domains of
computer vision. They learn general representations that are easily
transferable to tasks not seen during training. One such foundational model is
Segment anything model (SAM), which can accurately segment objects in images.
We propose adapting the SAM encoder via fine-tuning for remote sensing change
detection (RSCD) along with spatial-temporal feature enhancement (STFE) and
multi-scale decoder fusion (MSDF) to detect changes robustly at multiple
scales. Additionally, we propose a novel cross-entropy masking (CEM) loss to
handle high class imbalance in change detection datasets. Our method
outperforms state-of-the-art (SOTA) methods on four change detection datasets,
Levir-CD, WHU-CD, CLCD, and S2Looking. We achieved 2.5% F1-score improvement on
a large complex S2Looking dataset. The code is available at:
https://github.com/humza909/SAM-CEM-CD

</details>


### [91] [Hybrid Generative Fusion for Efficient and Privacy-Preserving Face Recognition Dataset Generation](https://arxiv.org/abs/2508.10672)
*Feiran Li,Qianqian Xu,Shilong Bao,Boyu Han,Zhiyong Yang,Qingming Huang*

Main category: cs.CV

TL;DR: 本文提出了一种构建高质量、无重叠人脸数据集的方法，结合真实数据清洗、数据增强和混合生成模型（Stable Diffusion与Vec2Face），并在DataCV ICCV挑战赛中获得第一名。


<details>
  <summary>Details</summary>
Motivation: 参与DataCV ICCV挑战赛，目标是构建一个高质量的人脸数据集，用于训练人脸识别模型，且该数据集不能与任何现有公共人脸数据集存在身份重叠。

Method: 首先，采用MoE（结合人脸嵌入聚类和GPT-4o验证）策略对基线HSFace数据集进行彻底清洗，移除错误或不一致的身份，并保留最大的连贯身份簇，对每种身份应用数据增强。其次，利用Stable Diffusion结合提示工程生成合成身份的参考图像，并通过Vec2Face高效扩展为49个身份一致的变体，形成GAN和扩散模型混合的样本。为解决合成身份间的高度视觉相似性，采用课程学习策略。最后，对所有新生成身份进行检查，确保与主流人脸数据集无身份泄露。

Result: 构建的最终数据集每种身份包含50张图像，方法在DataCV ICCV挑战赛中获得第一名。实验结果表明，该数据集在1万、2万和10万身份规模上均显著提升了模型性能。

Conclusion: 所提出的混合方法能够高效构建多样化且高质量的人脸数据集，有效避免了身份重叠问题，并显著提升了人脸识别模型的性能，证明了其在实际应用中的有效性。

Abstract: In this paper, we present our approach to the DataCV ICCV Challenge, which
centers on building a high-quality face dataset to train a face recognition
model. The constructed dataset must not contain identities overlapping with any
existing public face datasets. To handle this challenge, we begin with a
thorough cleaning of the baseline HSFace dataset, identifying and removing
mislabeled or inconsistent identities through a Mixture-of-Experts (MoE)
strategy combining face embedding clustering and GPT-4o-assisted verification.
We retain the largest consistent identity cluster and apply data augmentation
up to a fixed number of images per identity. To further diversify the dataset,
we generate synthetic identities using Stable Diffusion with prompt
engineering. As diffusion models are computationally intensive, we generate
only one reference image per identity and efficiently expand it using Vec2Face,
which rapidly produces 49 identity-consistent variants. This hybrid approach
fuses GAN-based and diffusion-based samples, enabling efficient construction of
a diverse and high-quality dataset. To address the high visual similarity among
synthetic identities, we adopt a curriculum learning strategy by placing them
early in the training schedule, allowing the model to progress from easier to
harder samples. Our final dataset contains 50 images per identity, and all
newly generated identities are checked with mainstream face datasets to ensure
no identity leakage. Our method achieves \textbf{1st place} in the competition,
and experimental results show that our dataset improves model performance
across 10K, 20K, and 100K identity scales. Code is available at
https://github.com/Ferry-Li/datacv_fr.

</details>


### [92] [Towards Agentic AI for Multimodal-Guided Video Object Segmentation](https://arxiv.org/abs/2508.10572)
*Tuyen Tran,Thao Minh Le,Truyen Tran*

Main category: cs.CV

TL;DR: 针对基于指代视频目标分割（RVOS）任务，传统方法计算成本高且缺乏灵活性。本文提出一种多模态智能体，利用大语言模型（LLMs）生成动态工作流并与专用工具交互，以更灵活自适应的方式识别目标，并在RVOS和Ref-AVS任务上取得了显著改进。


<details>
  <summary>Details</summary>
Motivation: 基于指代视频目标分割（RVOS）是一个复杂的跨模态任务，需要精细分割。传统方法计算复杂且需要大量标注。尽管最新的视觉-语言基础模型提供了免训练方案，但现有方法采用固定流程，缺乏适应任务动态性的灵活性，限制了其应用。

Method: 提出“多模态智能体”系统，利用大语言模型（LLMs）的推理能力，为每个输入动态生成定制化的工作流程。该自适应过程迭代地与一组针对不同模态的低级任务设计的专用工具进行交互，以识别多模态线索描述的目标对象。

Result: 所提出的智能体方法在两个多模态条件下的VOS任务（RVOS和Ref-AVS）上，相比现有方法展现出明显的性能提升。

Conclusion: 通过引入基于LLMs的动态工作流生成和工具交互，多模态智能体为基于指代视频目标分割提供了一种更灵活和自适应的解决方案，有效克服了现有方法的固定流程限制，并在相关基准测试中取得了优越表现。

Abstract: Referring-based Video Object Segmentation is a multimodal problem that
requires producing fine-grained segmentation results guided by external cues.
Traditional approaches to this task typically involve training specialized
models, which come with high computational complexity and manual annotation
effort. Recent advances in vision-language foundation models open a promising
direction toward training-free approaches. Several studies have explored
leveraging these general-purpose models for fine-grained segmentation,
achieving performance comparable to that of fully supervised, task-specific
models. However, existing methods rely on fixed pipelines that lack the
flexibility needed to adapt to the dynamic nature of the task. To address this
limitation, we propose Multi-Modal Agent, a novel agentic system designed to
solve this task in a more flexible and adaptive manner. Specifically, our
method leverages the reasoning capabilities of large language models (LLMs) to
generate dynamic workflows tailored to each input. This adaptive procedure
iteratively interacts with a set of specialized tools designed for low-level
tasks across different modalities to identify the target object described by
the multimodal cues. Our agentic approach demonstrates clear improvements over
prior methods on two multimodal-conditioned VOS tasks: RVOS and Ref-AVS.

</details>


### [93] [HumanSense: From Multimodal Perception to Empathetic Context-Aware Responses through Reasoning MLLMs](https://arxiv.org/abs/2508.10576)
*Zheng Qin,Ruobing Zheng,Yabing Wang,Tianqi Li,Yi Yuan,Jingdong Chen,Le Wang*

Main category: cs.CV

TL;DR: 该研究引入了HumanSense基准，用于评估多模态大语言模型（MLLMs）在以人为中心的感知和交互能力，并提出了一种基于多阶段、模态渐进式强化学习的方法来增强模型的推理能力，以提升其表现。


<details>
  <summary>Details</summary>
Motivation: 当前MLLMs在实现类人交互方面面临挑战，缺乏针对以人为中心场景的细粒度评估框架，难以有效评估其对复杂人类意图的理解以及提供富有同情心、上下文感知响应的能力。

Method: 1. 提出了HumanSense基准，用于评估MLLMs在深度理解扩展多模态上下文和形成合理反馈方面的能力。2. 对主流MLLMs进行了评估。3. 采用多阶段、模态渐进式强化学习来增强全模态（Omni-modal）模型的推理能力。4. 通过设计相应的提示，以无训练方式提升非推理模型的性能。

Result: 1. 领先的MLLMs在高级交互导向任务上仍有显著改进空间。2. 补充音频和文本信息可大幅提升视觉输入下的性能。3. 全模态模型在这些任务上表现出优势。4. 通过强化学习显著提升了全模态模型的评估结果。5. 成功的推理过程表现出高度一致的思维模式。6. 设计的提示能以无训练方式提升非推理模型的性能。

Conclusion: MLLMs在以人为中心的交互方面仍需大幅改进，特别是对于高级任务。多模态输入（如音频和文本补充视觉）以及强大的推理能力是实现更佳交互的关键。本研究提出的HumanSense基准和基于强化学习的推理增强方法为提升MLLMs的人类中心感知和交互能力提供了有效途径。

Abstract: While Multimodal Large Language Models (MLLMs) show immense promise for
achieving truly human-like interactions, progress is hindered by the lack of
fine-grained evaluation frameworks for human-centered scenarios, encompassing
both the understanding of complex human intentions and the provision of
empathetic, context-aware responses. Here we introduce HumanSense, a
comprehensive benchmark designed to evaluate the human-centered perception and
interaction capabilities of MLLMs, with a particular focus on deep
understanding of extended multimodal contexts and the formulation of rational
feedback. Our evaluation reveals that leading MLLMs still have considerable
room for improvement, particularly for advanced interaction-oriented tasks.
Supplementing visual input with audio and text information yields substantial
improvements, and Omni-modal models show advantages on these tasks.
Furthermore, we argue that appropriate feedback stems from a contextual
analysis of the interlocutor's needs and emotions, with reasoning ability
serving as the key to unlocking it. Accordingly, we employ a multi-stage,
modality-progressive reinforcement learning to enhance the reasoning abilities
of an Omni model, achieving substantial gains on evaluation results.
Additionally, we observe that successful reasoning processes exhibit highly
consistent thought patterns. By designing corresponding prompts, we also
enhance the performance of non-reasoning models in a training-free manner.
Project page:
\textcolor{brightpink}https://digital-avatar.github.io/ai/HumanSense/

</details>


### [94] [EvTurb: Event Camera Guided Turbulence Removal](https://arxiv.org/abs/2508.10582)
*Yixing Liu,Minggui Teng,Yifei Xia,Peiqi Duan,Boxin Shi*

Main category: cs.CV

TL;DR: EvTurb是一个事件引导的湍流去除框架，利用高帧率事件流解耦图像模糊和倾斜，并通过两步网络实现，同时发布了首个真实湍流数据集TurbEvent，效果超越现有方法且计算高效。


<details>
  <summary>Details</summary>
Motivation: 大气湍流导致图像质量下降，引入模糊和几何倾斜畸变，对计算机视觉任务构成挑战。现有单图像和多帧方法难以处理这种高度病态且复杂的湍流畸变。

Method: 提出EvTurb框架，利用高速事件流解耦模糊和倾斜效应。通过新颖的两步事件引导网络实现：首先利用事件积分减少粗输出中的模糊；然后利用原始事件流导出的方差图消除精细输出的倾斜畸变。此外，还提出了首个真实捕获的湍流数据集TurbEvent。

Result: 实验结果表明，EvTurb在保持计算效率的同时，超越了最先进的方法。

Conclusion: EvTurb通过利用事件数据有效解决了大气湍流造成的图像畸变问题，实现了卓越的性能和效率，并为该领域提供了新的数据集。

Abstract: Atmospheric turbulence degrades image quality by introducing blur and
geometric tilt distortions, posing significant challenges to downstream
computer vision tasks. Existing single-image and multi-frame methods struggle
with the highly ill-posed nature of this problem due to the compositional
complexity of turbulence-induced distortions. To address this, we propose
EvTurb, an event guided turbulence removal framework that leverages high-speed
event streams to decouple blur and tilt effects. EvTurb decouples blur and tilt
effects by modeling event-based turbulence formation, specifically through a
novel two-step event-guided network: event integrals are first employed to
reduce blur in the coarse outputs. This is followed by employing a variance
map, derived from raw event streams, to eliminate the tilt distortion for the
refined outputs. Additionally, we present TurbEvent, the first real-captured
dataset featuring diverse turbulence scenarios. Experimental results
demonstrate that EvTurb surpasses state-of-the-art methods while maintaining
computational efficiency.

</details>


### [95] [EgoCross: Benchmarking Multimodal Large Language Models for Cross-Domain Egocentric Video Question Answering](https://arxiv.org/abs/2508.10729)
*Yanjun Li,Yuqian Fu,Tianwen Qian,Qi'ao Xu,Silong Dai,Danda Pani Paudel,Luc Van Gool,Xiaoling Wang*

Main category: cs.CV

TL;DR: 引入EgoCross基准测试，以评估多模态大语言模型（MLLMs）在自我中心视频问答（EgocentricQA）中跨领域泛化能力，并发现现有模型在此方面表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有自我中心视频问答（EgocentricQA）基准主要限于日常活动，但实际应用会遇到显著的领域差异（视觉风格和语义内容），需要评估模型在不同领域下的泛化能力。

Method: 提出了EgoCross基准测试，包含约1000个QA对和798个视频片段，涵盖外科手术、工业、极限运动和动物视角四个多样化领域。支持预测、识别、定位和计数四种QA任务，并提供开放式和封闭式两种问答格式。此外，还进行了微调和强化学习等初步研究以探索改进方向。

Result: 实验表明，大多数现有MLLMs（无论是通用型还是自我中心专业型）都难以泛化到日常活动之外的领域，揭示了当前模型的局限性。

Conclusion: EgoCross基准和伴随的分析旨在为推进领域自适应、鲁棒的自我中心视频理解奠定基础。

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have
significantly pushed the frontier of egocentric video question answering
(EgocentricQA). However, existing benchmarks and studies are mainly limited to
common daily activities such as cooking and cleaning. In contrast, real-world
deployment inevitably encounters domain shifts, where target domains differ
substantially in both visual style and semantic content. To bridge this gap, we
introduce \textbf{EgoCross}, a comprehensive benchmark designed to evaluate the
cross-domain generalization of MLLMs in EgocentricQA. EgoCross covers four
diverse and challenging domains, including surgery, industry, extreme sports,
and animal perspective, representing realistic and high-impact application
scenarios. It comprises approximately 1,000 QA pairs across 798 video clips,
spanning four key QA tasks: prediction, recognition, localization, and
counting. Each QA pair provides both OpenQA and CloseQA formats to support
fine-grained evaluation. Extensive experiments show that most existing MLLMs,
whether general-purpose or egocentric-specialized, struggle to generalize to
domains beyond daily life, highlighting the limitations of current models.
Furthermore, we conduct several pilot studies, \eg, fine-tuning and
reinforcement learning, to explore potential improvements. We hope EgoCross and
our accompanying analysis will serve as a foundation for advancing
domain-adaptive, robust egocentric video understanding. Data and codes will be
released at:
\href{https://github.com/MyUniverse0726/EgoCross}{https://github.com/MyUniverse0726/EgoCross.}

</details>


### [96] [Towards Powerful and Practical Patch Attacks for 2D Object Detection in Autonomous Driving](https://arxiv.org/abs/2508.10600)
*Yuxin Cao,Yedi Zhang,Wentao He,Yifan Liao,Yan Xiao,Chang Li,Zhiyong Huang,Jin Song Dong*

Main category: cs.CV

TL;DR: P$^3$A是一个针对自动驾驶中2D目标检测的强大实用补丁攻击框架，通过引入新的评估指标PASR、定制的LCSL损失和PSPP数据预处理，解决了现有黑盒攻击在实际场景中有效性被高估以及对高分辨率数据转移性差的问题，显著提升了攻击效果。


<details>
  <summary>Details</summary>
Motivation: 基于学习的自动驾驶系统极易受到对抗性补丁的攻击，构成严重的安全风险。现有的基于迁移性的黑盒攻击方法常采用mAP评估指标和宽松的IoU阈值，导致攻击效果被高估，实际成功率降低。此外，在低分辨率数据上训练的补丁在处理高分辨率图像时效果不佳，限制了其在自动驾驶数据集上的迁移性。

Method: P$^3$A框架包含三个核心部分：1. 引入一种新的度量标准“实际攻击成功率（PASR）”，以更准确地量化攻击效果，并与行人安全更相关。2. 提出一种定制的“定位-置信度抑制损失（LCSL）”，以提高在PASR下的攻击迁移性。3. 将“概率尺度保留填充（PSPP）”作为数据预处理步骤纳入补丁攻击流程，以在高分辨率数据集上保持迁移性。

Result: 广泛的实验表明，P$^3$A在未见过的模型和未见过的高分辨率数据集上，无论是使用所提出的基于IoU的评估指标（PASR）还是之前的基于mAP的指标，都优于最先进的攻击方法。

Conclusion: P$^3$A通过引入更准确的评估指标、定制的损失函数和针对高分辨率数据的预处理方法，显著提升了黑盒对抗性补丁攻击在自动驾驶系统中的实际有效性和迁移性，弥补了现有方法的不足。

Abstract: Learning-based autonomous driving systems remain critically vulnerable to
adversarial patches, posing serious safety and security risks in their
real-world deployment. Black-box attacks, notable for their high attack success
rate without model knowledge, are especially concerning, with their
transferability extensively studied to reduce computational costs compared to
query-based attacks. Previous transferability-based black-box attacks typically
adopt mean Average Precision (mAP) as the evaluation metric and design training
loss accordingly. However, due to the presence of multiple detected bounding
boxes and the relatively lenient Intersection over Union (IoU) thresholds, the
attack effectiveness of these approaches is often overestimated, resulting in
reduced success rates in practical attacking scenarios. Furthermore, patches
trained on low-resolution data often fail to maintain effectiveness on
high-resolution images, limiting their transferability to autonomous driving
datasets. To fill this gap, we propose P$^3$A, a Powerful and Practical Patch
Attack framework for 2D object detection in autonomous driving, specifically
optimized for high-resolution datasets. First, we introduce a novel metric,
Practical Attack Success Rate (PASR), to more accurately quantify attack
effectiveness with greater relevance for pedestrian safety. Second, we present
a tailored Localization-Confidence Suppression Loss (LCSL) to improve attack
transferability under PASR. Finally, to maintain the transferability for
high-resolution datasets, we further incorporate the Probabilistic
Scale-Preserving Padding (PSPP) into the patch attack pipeline as a data
preprocessing step. Extensive experiments show that P$^3$A outperforms
state-of-the-art attacks on unseen models and unseen high-resolution datasets,
both under the proposed practical IoU-based evaluation metric and the previous
mAP-based metrics.

</details>


### [97] [AEGIS: Authenticity Evaluation Benchmark for AI-Generated Video Sequences](https://arxiv.org/abs/2508.10771)
*Jieyu Li,Xin Zhang,Joey Tianyi Zhou*

Main category: cs.CV

TL;DR: AEGIS是一个新型大规模基准数据集，旨在检测超真实和语义细微的AI生成视频，以应对现有基准的不足。


<details>
  <summary>Details</summary>
Motivation: AI生成内容（特别是视频）的快速发展导致了高度逼真的合成视频，对社会信任和数字完整性构成严重风险。现有视频真实性检测基准存在真实性有限、规模不足和复杂性不够的问题，无法有效评估现代视觉-语言模型对抗复杂伪造的能力。

Method: 引入AEGIS数据集，包含超过10,000个经过严格筛选的真实和合成视频，这些视频由包括Stable Video Diffusion、CogVideoX-5B、KLing和Sora在内的多种最先进生成模型创建。AEGIS还包含特别构建的、具有鲁棒性评估的挑战性子集，并提供多模态标注，如语义真实性描述、运动特征和低级视觉特征。

Result: 使用先进的视觉-语言模型进行的大量实验表明，现有模型在AEGIS最具挑战性的子集上检测能力有限，这凸显了该数据集独特的复杂性和真实性超出了当前模型的泛化能力。

Conclusion: AEGIS建立了一个不可或缺的评估基准，从根本上推动了开发真正鲁棒、可靠、广泛泛化的视频真实性检测方法的研究，以应对现实世界的伪造威胁。

Abstract: Recent advances in AI-generated content have fueled the rise of highly
realistic synthetic videos, posing severe risks to societal trust and digital
integrity. Existing benchmarks for video authenticity detection typically
suffer from limited realism, insufficient scale, and inadequate complexity,
failing to effectively evaluate modern vision-language models against
sophisticated forgeries. To address this critical gap, we introduce AEGIS, a
novel large-scale benchmark explicitly targeting the detection of
hyper-realistic and semantically nuanced AI-generated videos. AEGIS comprises
over 10,000 rigorously curated real and synthetic videos generated by diverse,
state-of-the-art generative models, including Stable Video Diffusion,
CogVideoX-5B, KLing, and Sora, encompassing open-source and proprietary
architectures. In particular, AEGIS features specially constructed challenging
subsets enhanced with robustness evaluation. Furthermore, we provide multimodal
annotations spanning Semantic-Authenticity Descriptions, Motion Features, and
Low-level Visual Features, facilitating authenticity detection and supporting
downstream tasks such as multimodal fusion and forgery localization. Extensive
experiments using advanced vision-language models demonstrate limited detection
capabilities on the most challenging subsets of AEGIS, highlighting the
dataset's unique complexity and realism beyond the current generalization
capabilities of existing models. In essence, AEGIS establishes an indispensable
evaluation benchmark, fundamentally advancing research toward developing
genuinely robust, reliable, broadly generalizable video authenticity detection
methodologies capable of addressing real-world forgery threats. Our dataset is
available on https://huggingface.co/datasets/Clarifiedfish/AEGIS.

</details>


### [98] [Increasing the Utility of Synthetic Images through Chamfer Guidance](https://arxiv.org/abs/2508.10631)
*Nicola Dall'Asen,Xiaofeng Zhang,Reyhane Askari Hemmat,Melissa Hall,Jakob Verbeek,Adriana Romero-Soriano,Michal Drozdzal*

Main category: cs.CV

TL;DR: 该论文提出了一种名为Chamfer Guidance的无训练指导方法，利用少量真实图像作为参考，显著提升了条件图像生成模型的生成多样性和质量，减少了合成数据与真实数据之间的分布差异，并提高了下游任务的性能。


<details>
  <summary>Details</summary>
Motivation: 条件图像生成模型虽然在生成质量上有所进步，但多样性不足，限制了其作为合成训练数据的实用性。现有的指导方法往往忽视了合成数据与真实数据之间潜在的分布差异。

Method: 引入了Chamfer Guidance，这是一种无需训练的指导方法。它利用少量真实样本图像来评估和指导合成数据的质量和多样性，从而弥补生成数据与真实数据之间的分布差异。

Result: 通过Chamfer Guidance，模型在ImageNet-1k和标准地理多样性基准测试中提升了生成多样性，同时保持或提高了生成质量。在少量样本（2个真实图像）设置下，实现了最先进的性能（精度96.4%，覆盖率86.4%）。使用合成数据训练的下游图像分类器，在分布内和分布外数据上的准确率分别提高了15%和16%。此外，该方法无需使用无条件模型，采样时FLOPs比基于无分类器指导的方法减少了31%。

Conclusion: Chamfer Guidance有效解决了条件图像生成模型在多样性方面的不足，并通过少量真实图像的引导，显著提升了合成数据的实用性，使其更适合用于训练下游任务，同时实现了计算效率的提升。

Abstract: Conditional image generative models hold considerable promise to produce
infinite amounts of synthetic training data. Yet, recent progress in generation
quality has come at the expense of generation diversity, limiting the utility
of these models as a source of synthetic training data. Although guidance-based
approaches have been introduced to improve the utility of generated data by
focusing on quality or diversity, the (implicit or explicit) utility functions
oftentimes disregard the potential distribution shift between synthetic and
real data. In this work, we introduce Chamfer Guidance: a training-free
guidance approach which leverages a handful of real exemplar images to
characterize the quality and diversity of synthetic data. We show that by
leveraging the proposed Chamfer Guidance, we can boost the diversity of the
generations w.r.t. a dataset of real images while maintaining or improving the
generation quality on ImageNet-1k and standard geo-diversity benchmarks. Our
approach achieves state-of-the-art few-shot performance with as little as 2
exemplar real images, obtaining 96.4\% in terms of precision, and 86.4\% in
terms of distributional coverage, which increase to 97.5\% and 92.7\%,
respectively, when using 32 real images. We showcase the benefits of the
Chamfer Guidance generation by training downstream image classifiers on
synthetic data, achieving accuracy boost of up to 15\% for in-distribution over
the baselines, and up to 16\% in out-of-distribution. Furthermore, our approach
does not require using the unconditional model, and thus obtains a 31\%
reduction in FLOPs w.r.t. classifier-free-guidance-based approaches at sampling
time.

</details>


### [99] [Video-BLADE: Block-Sparse Attention Meets Step Distillation for Efficient Video Generation](https://arxiv.org/abs/2508.10774)
*Youping Gu,Xiaolong Li,Yuhao Hu,Bohan Zhuang*

Main category: cs.CV

TL;DR: BLADE是一个数据无关的联合训练框架，通过自适应块稀疏注意力机制和稀疏感知步长蒸馏，显著加速了扩散Transformer的视频生成推理过程，并提升了生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散Transformer在高质量视频生成方面表现出色，但其迭代去噪过程缓慢以及长序列的二次注意力成本导致推理瓶颈。单独的步长蒸馏和稀疏注意力机制虽有潜力，但有效结合它们存在挑战，要么效果不佳，要么需要昂贵的高质量视频数据进行训练。

Method: 本文提出了BLADE框架，包含两项创新：1) 自适应块稀疏注意力（ASA）机制，动态生成内容感知稀疏掩码，将计算集中在显著的时空特征上；2) 基于轨迹分布匹配（TDM）的稀疏感知步长蒸馏范式，将稀疏性直接融入蒸馏过程，而非作为独立的压缩步骤，实现快速收敛。该框架是数据无关的联合训练。

Result: BLADE在CogVideoX-5B和Wan2.1-1.3B等文本到视频模型上进行了验证。在Wan2.1-1.3B上，实现了相对于50步基线14.10倍的端到端推理加速；在CogVideoX-5B等短视频序列模型上，实现了8.89倍的加速。同时，加速伴随着质量提升：在VBench-2.0基准上，CogVideoX-5B得分从0.534提高到0.569，Wan2.1-1.3B从0.563提高到0.570，并得到人类评估的更高评价。

Conclusion: BLADE通过创新性的数据无关联合训练框架，成功结合了稀疏注意力和步长蒸馏，有效解决了扩散Transformer在视频生成中的推理瓶颈，实现了显著的加速和一致的质量提升。

Abstract: Diffusion transformers currently lead the field in high-quality video
generation, but their slow iterative denoising process and prohibitive
quadratic attention costs for long sequences create significant inference
bottlenecks. While both step distillation and sparse attention mechanisms have
shown promise as independent acceleration strategies, effectively combining
these approaches presents critical challenges -- training-free integration
yields suboptimal results, while separately training sparse attention after
step distillation requires prohibitively expensive high-quality video data. To
overcome these limitations, we propose BLADE, an innovative data-free joint
training framework that introduces: (1) an Adaptive Block-Sparse Attention
(ASA) mechanism for dynamically generating content-aware sparsity masks to
focus computation on salient spatiotemporal features, and (2) a sparsity-aware
step distillation paradigm built upon Trajectory Distribution Matching (TDM)
that directly incorporates sparsity into the distillation process rather than
treating it as a separate compression step, with fast convergence. We validate
BLADE on text-to-video models like CogVideoX-5B and Wan2.1-1.3B. Our framework
demonstrates remarkable efficiency gains across different scales. On
Wan2.1-1.3B, BLADE achieves a 14.10x end-to-end inference acceleration over a
50-step baseline. Moreover, on models such as CogVideoX-5B with short video
sequence lengths, our framework delivers a robust 8.89x speedup. Crucially, the
acceleration is accompanied by a consistent quality improvement. On the
VBench-2.0 benchmark, BLADE boosts the score of CogVideoX-5B to 0.569 (from
0.534) and Wan2.1-1.3B to 0.570 (from 0.563), results that are further
corroborated by superior ratings in human evaluations. Our code and model
weights are publicly available at: http://ziplab.co/BLADE-Homepage/.

</details>


### [100] [ChatENV: An Interactive Vision-Language Model for Sensor-Guided Environmental Monitoring and Scenario Simulation](https://arxiv.org/abs/2508.10635)
*Hosam Elgendy,Ahmed Sharshar,Ahmed Aboeitta,Mohsen Guizani*

Main category: cs.CV

TL;DR: ChatENV是首个交互式视觉语言模型（VLM），能结合卫星图像对和环境传感器数据进行联合推理，支持时间序列和“假设”场景分析，并优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在理解环境变化方面存在局限性：它们忽视环境传感器中的因果信号，依赖单一来源的、可能存在风格偏差的图像描述，并且缺乏交互式场景推理能力。

Method: 该研究方法包括：(i) 创建了一个包含17.7万张图像、形成15.2万个时间对的数据集，覆盖全球197个国家的62种土地利用类别，并富含传感器元数据（如温度、PM10、CO）；(ii) 使用GPT-4o和Gemini 2.0对数据进行标注，以确保风格和语义多样性；(iii) 使用低秩适应（LoRA）适配器对Qwen-2.5-VL模型进行微调，以支持聊天交互。

Result: ChatENV在时间序列和“假设”推理任务中表现出色（例如，BERT-F1达到0.903），与最先进的时间序列模型相比，性能相当甚至更优，同时支持交互式场景分析。

Conclusion: ChatENV是一个强大的工具，可用于基于传感器数据的、接地气的环境监测，填补了现有VLM在处理环境变化方面的空白。

Abstract: Understanding environmental changes from aerial imagery is vital for climate
resilience, urban planning, and ecosystem monitoring. Yet, current vision
language models (VLMs) overlook causal signals from environmental sensors, rely
on single-source captions prone to stylistic bias, and lack interactive
scenario-based reasoning. We present ChatENV, the first interactive VLM that
jointly reasons over satellite image pairs and real-world sensor data. Our
framework: (i) creates a 177k-image dataset forming 152k temporal pairs across
62 land-use classes in 197 countries with rich sensor metadata (e.g.,
temperature, PM10, CO); (ii) annotates data using GPT- 4o and Gemini 2.0 for
stylistic and semantic diversity; and (iii) fine-tunes Qwen-2.5-VL using
efficient Low-Rank Adaptation (LoRA) adapters for chat purposes. ChatENV
achieves strong performance in temporal and "what-if" reasoning (e.g., BERT-F1
0.903) and rivals or outperforms state-of-the-art temporal models, while
supporting interactive scenario-based analysis. This positions ChatENV as a
powerful tool for grounded, sensor-aware environmental monitoring.

</details>


### [101] [Ultra-High-Definition Reference-Based Landmark Image Super-Resolution with Generative Diffusion Prior](https://arxiv.org/abs/2508.10779)
*Zhenning Shi,Zizheng Yan,Yuhang Yu,Clara Xue,Jingyu Zhuang,Qi Zhang,Jinwei Chen,Tao Li,Qingnan Fan*

Main category: cs.CV

TL;DR: 本文提出了TriFlowSR框架和Landmark-4K数据集，旨在解决现有参考图像超分（RefSR）方法在信息对齐和数据集质量方面的局限性，特别针对超高清（UHD）地标场景下的真实世界退化。


<details>
  <summary>Details</summary>
Motivation: 现有的基于扩散模型的RefSR方法（通常基于ControlNet）难以有效对齐低分辨率（LR）图像和参考高分辨率（HR）图像的信息。此外，当前的RefSR数据集分辨率有限且图像质量不佳，导致参考图像缺乏足够的精细细节来支持高质量的图像恢复。

Method: 本文提出TriFlowSR框架，该框架显式地实现了LR图像与参考HR图像之间的模式匹配。针对超高清（UHD）场景和真实世界退化，TriFlowSR设计了一种参考匹配策略。同时，本文还引入了Landmark-4K数据集，这是首个针对UHD地标场景的RefSR数据集。

Result: 实验结果表明，与现有方法相比，TriFlowSR能更好地利用参考HR图像的语义和纹理信息。

Conclusion: 本文首次提出了一种针对真实世界退化下超高清地标场景的基于扩散模型的RefSR方案，并为此类场景提供了专门的数据集。

Abstract: Reference-based Image Super-Resolution (RefSR) aims to restore a
low-resolution (LR) image by utilizing the semantic and texture information
from an additional reference high-resolution (reference HR) image. Existing
diffusion-based RefSR methods are typically built upon ControlNet, which
struggles to effectively align the information between the LR image and the
reference HR image. Moreover, current RefSR datasets suffer from limited
resolution and poor image quality, resulting in the reference images lacking
sufficient fine-grained details to support high-quality restoration. To
overcome the limitations above, we propose TriFlowSR, a novel framework that
explicitly achieves pattern matching between the LR image and the reference HR
image. Meanwhile, we introduce Landmark-4K, the first RefSR dataset for
Ultra-High-Definition (UHD) landmark scenarios. Considering the UHD scenarios
with real-world degradation, in TriFlowSR, we design a Reference Matching
Strategy to effectively match the LR image with the reference HR image.
Experimental results show that our approach can better utilize the semantic and
texture information of the reference HR image compared to previous methods. To
the best of our knowledge, we propose the first diffusion-based RefSR pipeline
for ultra-high definition landmark scenarios under real-world degradation. Our
code and model will be available at https://github.com/nkicsl/TriFlowSR.

</details>


### [102] [Processing and acquisition traces in visual encoders: What does CLIP know about your camera?](https://arxiv.org/abs/2508.10637)
*Ryan Ramos,Vladan Stojnić,Giorgos Kordopatis-Zilos,Yuta Nakashima,Giorgos Tolias,Noa Garcia*

Main category: cs.CV

TL;DR: 该研究发现，视觉编码器会系统地编码图像采集和处理过程中人眼难以察觉的细微参数，这些参数可以被轻易恢复，并根据其与语义标签的相关性，对语义预测产生显著的正面或负面影响。


<details>
  <summary>Details</summary>
Motivation: 以往研究主要关注图像严重损坏对视觉编码器鲁棒性的影响，这些损坏会扭曲有用信号并导致性能下降。本研究旨在从一个不同角度出发，分析图像采集和处理过程中可能非常细微甚至人眼无法察觉的参数，探讨它们如何影响视觉表示和语义预测，因为这些细微变化同样可能引入分布偏移。

Method: 通过分析图像采集过程和变换的参数，研究其如何被编码进学习到的视觉表示中，以及这些编码信息是否易于恢复。进一步，评估这些参数的存在对语义预测的影响，并探究这种影响是否与语义标签和这些采集/处理相关标签之间的强相关或反相关性有关。

Result: 研究发现：1) 图像采集和处理过程中细微或不可察觉的参数被系统地编码在学习到的视觉表示中；2) 这些参数可以被轻易地从表示中恢复；3) 它们的存在对语义预测具有深远的影响（正面或负面）；4) 这种影响取决于语义标签与这些采集或处理相关标签之间是否存在强烈的正相关或负相关。

Conclusion: 视觉编码器不仅对图像的语义信息敏感，还会系统地编码图像采集和处理过程中那些细微、不易察觉的非语义参数。这些参数的存在可以显著影响模型在语义任务上的表现，其影响方向和程度取决于它们与语义标签之间的相关性，揭示了视觉编码器鲁棒性分析的新维度。

Abstract: Prior work has analyzed the robustness of visual encoders to image
transformations and corruptions, particularly in cases where such alterations
are not seen during training. When this occurs, they introduce a form of
distribution shift at test time, often leading to performance degradation. The
primary focus has been on severe corruptions that, when applied aggressively,
distort useful signals necessary for accurate semantic predictions.
  We take a different perspective by analyzing parameters of the image
acquisition process and transformations that may be subtle or even
imperceptible to the human eye. We find that such parameters are systematically
encoded in the learned visual representations and can be easily recovered. More
strikingly, their presence can have a profound impact, either positively or
negatively, on semantic predictions. This effect depends on whether there is a
strong correlation or anti-correlation between semantic labels and these
acquisition-based or processing-based labels. Our code and data are available
at: https://github.com/ryan-caesar-ramos/visual-encoder-traces

</details>


### [103] [Lameness detection in dairy cows using pose estimation and bidirectional LSTMs](https://arxiv.org/abs/2508.10643)
*Helena Russello,Rik van der Tol,Eldert J. van Henten,Gert Kootstra*

Main category: cs.CV

TL;DR: 本研究提出一种结合姿态估计和双向长短期记忆（BLSTM）神经网络的奶牛跛足检测方法，实现了高精度和短视频序列的有效检测。


<details>
  <summary>Details</summary>
Motivation: 现有跛足检测方法可能依赖于标记、手动特征工程或需要大量数据。本研究旨在开发一种无标记、自动化、能从关键点轨迹中学习时序特征，并适用于短序列和小训练数据集的检测方案。

Method: 使用T-LEAP姿态估计模型从奶牛行走视频中提取九个关键点（蹄、头部、背部）的运动序列。这些关键点轨迹作为BLSTM分类器的输入，用于进行二元跛足分类训练。

Result: 该方法实现了85%的分类准确率，显著优于依赖手动设计运动特征的传统方法（80%）。此外，该BLSTM分类器仅需1秒的视频数据即可检测跛足。

Conclusion: 结合姿态估计和BLSTM的奶牛跛足检测方法是有效的，其性能优于传统基于特征的方法，并且能够处理短视频序列，显示出在实际应用中的潜力。

Abstract: This study presents a lameness detection approach that combines pose
estimation and Bidirectional Long-Short-Term Memory (BLSTM) neural networks.
Combining pose-estimation and BLSTMs classifier offers the following
advantages: markerless pose-estimation, elimination of manual feature
engineering by learning temporal motion features from the keypoint
trajectories, and working with short sequences and small training datasets.
Motion sequences of nine keypoints (located on the cows' hooves, head and back)
were extracted from videos of walking cows with the T-LEAP pose estimation
model. The trajectories of the keypoints were then used as an input to a BLSTM
classifier that was trained to perform binary lameness classification. Our
method significantly outperformed an established method that relied on
manually-designed locomotion features: our best architecture achieved a
classification accuracy of 85%, against 80% accuracy for the feature-based
approach. Furthermore, we showed that our BLSTM classifier could detect
lameness with as little as one second of video data.

</details>


### [104] [Performance of GPT-5 in Brain Tumor MRI Reasoning](https://arxiv.org/abs/2508.10865)
*Mojtaba Safari,Shansong Wang,Mingzhe Hu,Zach Eidex,Qiang Li,Xiaofeng Yang*

Main category: cs.CV

TL;DR: 本研究评估了GPT-4o、GPT-5-nano、GPT-5-mini和GPT-5模型在脑肿瘤视觉问答（VQA）任务上的表现，发现GPT-5系列模型能达到中等准确率，但尚未达到临床应用水平。


<details>
  <summary>Details</summary>
Motivation: 准确区分脑肿瘤类型对神经肿瘤治疗规划至关重要。大型语言模型（LLMs）在视觉问答（VQA）方面的最新进展，结合了图像解释和自然语言推理，为该领域提供了新的可能性。

Method: 研究构建了一个脑肿瘤VQA基准，该基准来源于三种脑肿瘤分割（BraTS）数据集（胶质母细胞瘤、脑膜瘤和脑转移瘤）。每个病例包含多序列MRI三平面马赛克图像和结构化临床特征，并将其转换为标准化的VQA项目。研究在零样本思维链设置下评估了GPT-4o、GPT-5-nano、GPT-5-mini和GPT-5模型在视觉和推理任务上的准确性。

Result: 结果显示，GPT-5-mini的宏平均准确率最高（44.19%），其次是GPT-5（43.71%）、GPT-4o（41.49%）和GPT-5-nano（35.85%）。性能因肿瘤亚型而异，没有单一模型在所有队列中都占据主导地位。

Conclusion: 研究结果表明，GPT-5系列模型在结构化神经肿瘤VQA任务中可以达到中等准确率，但尚未达到临床可接受的水平。

Abstract: Accurate differentiation of brain tumor types on magnetic resonance imaging
(MRI) is critical for guiding treatment planning in neuro-oncology. Recent
advances in large language models (LLMs) have enabled visual question answering
(VQA) approaches that integrate image interpretation with natural language
reasoning. In this study, we evaluated GPT-4o, GPT-5-nano, GPT-5-mini, and
GPT-5 on a curated brain tumor VQA benchmark derived from 3 Brain Tumor
Segmentation (BraTS) datasets - glioblastoma (GLI), meningioma (MEN), and brain
metastases (MET). Each case included multi-sequence MRI triplanar mosaics and
structured clinical features transformed into standardized VQA items. Models
were assessed in a zero-shot chain-of-thought setting for accuracy on both
visual and reasoning tasks. Results showed that GPT-5-mini achieved the highest
macro-average accuracy (44.19%), followed by GPT-5 (43.71%), GPT-4o (41.49%),
and GPT-5-nano (35.85%). Performance varied by tumor subtype, with no single
model dominating across all cohorts. These findings suggest that GPT-5 family
models can achieve moderate accuracy in structured neuro-oncological VQA tasks,
but not at a level acceptable for clinical use.

</details>


### [105] [SemPT: Semantic Prompt Tuning for Vision-Language Models](https://arxiv.org/abs/2508.10645)
*Xiao Shi,Yangjun Ou,Zhenzhong Chen*

Main category: cs.CV

TL;DR: SemPT是一种新颖的框架，通过利用共享的属性级知识，提高了视觉迁移学习中对未见类别的泛化能力，解决了现有VLM提示调整方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 视觉迁移学习在未见类别上极具挑战性，因为保留类别特异性表示与获取可迁移知识之间存在内在冲突。现有的VLM提示调整方法依赖稀疏的类别标签或分散的LLM生成描述，导致知识表示碎片化并阻碍了可迁移性。

Method: SemPT采用两步提示策略，引导LLM提取共享视觉属性并生成属性级描述。然后，应用视觉引导加权来减少无关属性的噪声并增强文本嵌入。此外，图像嵌入与标签和属性增强的文本嵌入联合对齐，以平衡对已知类别的判别力和对未知类别的可迁移性。推理时，根据类别暴露情况动态选择使用标准标签嵌入（已知类别）或属性增强嵌入（未知类别）。

Result: SemPT在15个基准数据集上进行了广泛实验，在基准到新颖泛化、跨数据集迁移、跨领域迁移和少样本学习等多种设置下，均实现了最先进的性能。

Conclusion: SemPT通过利用跨类别的共享属性级知识，有效解决了视觉迁移学习中的泛化挑战，显著提高了对未见类别的识别和迁移能力。

Abstract: Visual transfer learning for unseen categories presents an active research
topic yet a challenging task, due to the inherent conflict between preserving
category-specific representations and acquiring transferable knowledge.
Vision-Language Models (VLMs) pre-trained on large amounts of image-text pairs
offer a promising solution. However, existing prompt tuning methods rely on
sparse category labels or disparate LLM-generated descriptions, which fragment
knowledge representation and hinder transferability. To address this
limitation, we introduce Semantic Prompt Tuning (SemPT), a novel framework that
tackles the generalization challenge by leveraging shared attribute-level
knowledge across categories. Specifically, SemPT adopts a two-step prompting
strategy to guide LLM in extracting shared visual attributes and generating
attribute-level descriptions, capturing transferable semantic cues beyond
labels while ensuring coherent structure. Then, visually guided weighting is
applied to the embeddings of attribute-level descriptions to reduce noise from
irrelevant attributes and enhance the text embeddings. Additionally, image
embeddings are jointly aligned with both label and attribute-enhanced text
embeddings, balancing discrimination for seen categories and transferability to
unseen ones. Considering the availability of category exposure, our inference
dynamically selects between standard label embeddings for seen categories and
attribute-enhanced embeddings for unseen ones to ensure effective adaptation.
Extensive experiments on 15 benchmark datasets demonstrate that SemPT achieves
state-of-the-art performance across various settings, including base-to-novel
generalization, cross-dataset transfer, cross-domain transfer, and few-shot
learning.

</details>


### [106] [Medico 2025: Visual Question Answering for Gastrointestinal Imaging](https://arxiv.org/abs/2508.10869)
*Sushant Gautam,Vajira Thambawita,Michael Riegler,Pål Halvorsen,Steven Hicks*

Main category: cs.CV

TL;DR: Medico 2025挑战赛专注于胃肠道图像的视觉问答（VQA），旨在开发可解释人工智能（XAI）模型，为临床问题提供答案和医学推理的解释。


<details>
  <summary>Details</summary>
Motivation: 推动可信赖的人工智能在医学图像分析中的发展，特别是在胃肠道内窥镜图像VQA和可解释性方面，以支持临床决策。

Method: 挑战包含两个子任务：1) 使用Kvasir-VQA-x1数据集回答多样视觉问题；2) 生成多模态解释。Kvasir-VQA-x1数据集（包含6,500张图像和159,549个QA对）作为基准。评估将结合定量性能指标和专家审查的可解释性评估。

Result: 该摘要主要介绍了Medico 2025挑战的设定、目标和所用数据集，并未报告具体的实验结果，因为它是一个挑战的发布而非研究成果的展示。

Conclusion: Medico 2025挑战旨在通过结合定量性能和专家评估的可解释性，促进医学图像分析领域可信赖人工智能的进步。

Abstract: The Medico 2025 challenge addresses Visual Question Answering (VQA) for
Gastrointestinal (GI) imaging, organized as part of the MediaEval task series.
The challenge focuses on developing Explainable Artificial Intelligence (XAI)
models that answer clinically relevant questions based on GI endoscopy images
while providing interpretable justifications aligned with medical reasoning. It
introduces two subtasks: (1) answering diverse types of visual questions using
the Kvasir-VQA-x1 dataset, and (2) generating multimodal explanations to
support clinical decision-making. The Kvasir-VQA-x1 dataset, created from 6,500
images and 159,549 complex question-answer (QA) pairs, serves as the benchmark
for the challenge. By combining quantitative performance metrics and
expert-reviewed explainability assessments, this task aims to advance
trustworthy Artificial Intelligence (AI) in medical image analysis.
Instructions, data access, and an updated guide for participation are available
in the official competition repository:
https://github.com/simula/MediaEval-Medico-2025

</details>


### [107] [HyperTea: A Hypergraph-based Temporal Enhancement and Alignment Network for Moving Infrared Small Target Detection](https://arxiv.org/abs/2508.10678)
*Zhaoyuan Qi,Weihua Gao,Wenlong Niu,Jie Tang,Yun Li,Xiaodong Peng*

Main category: cs.CV

TL;DR: 针对运动红外小目标检测（MIRSTD）的挑战，本文提出了HyperTea模型，首次将CNN、RNN和超图神经网络（HGNN）结合，通过建模高阶时空相关性和增强多尺度特征，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 运动红外小目标检测（MIRSTD）面临目标尺寸小、强度弱、运动模式复杂等挑战。现有方法通常只建模特征节点间的低阶关联，并在单一时间尺度内进行特征提取和增强，而超图在MIRSTD中的应用有限，未能充分利用其高阶关联学习能力。

Method: 本文提出了HyperTea模型，旨在探索超图潜力并增强多时间尺度特征表示。它通过整合全局和局部时间视角来有效建模特征的高阶时空关联。HyperTea包含三个模块：全局时间增强模块（GTEM）实现全局时间上下文增强；局部时间增强模块（LTEM）捕获相邻帧间的局部运动模式并增强局部时间上下文；时间对齐模块（TAM）解决潜在的跨尺度特征未对齐问题。这是首个将卷积神经网络（CNN）、循环神经网络（RNN）和超图神经网络（HGNN）集成用于MIRSTD的工作。

Result: 实验结果表明，HyperTea显著提升了检测性能，并在DAUB和IRDST数据集上达到了最先进（SOTA）的性能。

Conclusion: HyperTea通过创新性地结合CNN、RNN和HGNN，并利用超图有效建模高阶时空相关性及增强多时间尺度特征，显著提升了运动红外小目标检测的性能，实现了当前最佳水平。

Abstract: In practical application scenarios, moving infrared small target detection
(MIRSTD) remains highly challenging due to the target's small size, weak
intensity, and complex motion pattern. Existing methods typically only model
low-order correlations between feature nodes and perform feature extraction and
enhancement within a single temporal scale. Although hypergraphs have been
widely used for high-order correlation learning, they have received limited
attention in MIRSTD. To explore the potential of hypergraphs and enhance
multi-timescale feature representation, we propose HyperTea, which integrates
global and local temporal perspectives to effectively model high-order
spatiotemporal correlations of features. HyperTea consists of three modules:
the global temporal enhancement module (GTEM) realizes global temporal context
enhancement through semantic aggregation and propagation; the local temporal
enhancement module (LTEM) is designed to capture local motion patterns between
adjacent frames and then enhance local temporal context; additionally, we
further develop a temporal alignment module (TAM) to address potential
cross-scale feature misalignment. To our best knowledge, HyperTea is the first
work to integrate convolutional neural networks (CNNs), recurrent neural
networks (RNNs), and hypergraph neural networks (HGNNs) for MIRSTD,
significantly improving detection performance. Experiments on DAUB and IRDST
demonstrate its state-of-the-art (SOTA) performance. Our source codes are
available at https://github.com/Lurenjia-LRJ/HyperTea.

</details>


### [108] [ToonComposer: Streamlining Cartoon Production with Generative Post-Keyframing](https://arxiv.org/abs/2508.10881)
*Lingen Li,Guangzhi Wang,Zhaoyang Zhang,Yaowei Li,Xiaoyu Li,Qi Dou,Jinwei Gu,Tianfan Xue,Ying Shan*

Main category: cs.CV

TL;DR: ToonComposer是一个生成模型，它将卡通制作中的中间帧生成和上色整合到一个后期关键帧处理阶段，通过稀疏草图输入和卡通领域自适应，显著提高了生产效率和质量。


<details>
  <summary>Details</summary>
Motivation: 传统卡通和动漫制作中，关键帧绘制、中间帧生成和上色是独立且劳动密集型的阶段，现有AI方法常单独处理这些阶段，导致错误累积和伪影，例如中间帧生成难以处理大动作，上色需要密集的逐帧草图。

Method: 引入ToonComposer，一个统一中间帧生成和上色的生成模型。它采用稀疏草图注入机制以提供精确控制，并使用空间低秩适配器进行卡通领域自适应，将现有视频基础模型应用于卡通领域同时保持其时间一致性。模型支持稀疏输入（最少一个草图和彩色参考帧），也支持在任何时间点使用多个草图进行更精确的运动控制。此外，还创建了PKBench基准来评估模型性能。

Result: ToonComposer在视觉质量、运动一致性和生产效率方面均优于现有方法，为AI辅助卡通制作提供了更优越、更灵活的解决方案。

Conclusion: ToonComposer通过统一关键流程并支持稀疏输入，显著减少了手动工作量并提高了灵活性，为艺术家在实际卡通制作场景中提供了强大的AI辅助工具。

Abstract: Traditional cartoon and anime production involves keyframing, inbetweening,
and colorization stages, which require intensive manual effort. Despite recent
advances in AI, existing methods often handle these stages separately, leading
to error accumulation and artifacts. For instance, inbetweening approaches
struggle with large motions, while colorization methods require dense per-frame
sketches. To address this, we introduce ToonComposer, a generative model that
unifies inbetweening and colorization into a single post-keyframing stage.
ToonComposer employs a sparse sketch injection mechanism to provide precise
control using keyframe sketches. Additionally, it uses a cartoon adaptation
method with the spatial low-rank adapter to tailor a modern video foundation
model to the cartoon domain while keeping its temporal prior intact. Requiring
as few as a single sketch and a colored reference frame, ToonComposer excels
with sparse inputs, while also supporting multiple sketches at any temporal
location for more precise motion control. This dual capability reduces manual
workload and improves flexibility, empowering artists in real-world scenarios.
To evaluate our model, we further created PKBench, a benchmark featuring
human-drawn sketches that simulate real-world use cases. Our evaluation
demonstrates that ToonComposer outperforms existing methods in visual quality,
motion consistency, and production efficiency, offering a superior and more
flexible solution for AI-assisted cartoon production.

</details>


### [109] [Physics-Informed Joint Multi-TE Super-Resolution with Implicit Neural Representation for Robust Fetal T2 Mapping](https://arxiv.org/abs/2508.10680)
*Busra Bulut,Maik Dannecker,Thomas Sanchez,Sara Neves Silva,Vladyslav Zalevskyi,Steven Jia,Jean-Baptiste Ledoux,Guillaume Auzias,François Rousseau,Jana Hutter,Daniel Rueckert,Meritxell Bach Cuadra*

Main category: cs.CV

TL;DR: 该研究提出了一种新方法，通过结合隐式神经表示和物理信息正则化，联合重建不同回波时间（TE）的胎儿脑部MRI数据，以实现0.55T磁场下的T2定量映射，有效解决了运动伪影并缩短了扫描时间。


<details>
  <summary>Details</summary>
Motivation: 胎儿脑部T2定量映射在0.55T中场MRI中具有重要潜力，但面临挑战：胎儿运动导致图像采集困难，且传统方法需要在每个TE重复采集大量运动伪影的切片组，导致扫描时间长且对运动敏感。

Method: 该方法通过联合重建不同TE的数据来解决严重运动问题。它结合了隐式神经表示和物理信息正则化（建模T2衰减），从而在不同TE之间共享信息，同时保持解剖和定量T2的准确性。

Result: 该方法在模拟胎儿脑部和具有胎儿般运动的体内成人数据集上展现了最先进的性能。研究还首次展示了0.55T磁场下的体内胎儿T2定量映射结果。研究表明，通过利用解剖冗余，该方法有望减少T2映射中每个TE所需的切片组数量。

Conclusion: 该研究提出的方法在胎儿脑部MRI的T2定量映射中表现出巨大潜力，能够有效处理运动问题并减少扫描时间，有望改进对发育中大脑的表征。

Abstract: T2 mapping in fetal brain MRI has the potential to improve characterization
of the developing brain, especially at mid-field (0.55T), where T2 decay is
slower. However, this is challenging as fetal MRI acquisition relies on
multiple motion-corrupted stacks of thick slices, requiring slice-to-volume
reconstruction (SVR) to estimate a high-resolution (HR) 3D volume. Currently,
T2 mapping involves repeated acquisitions of these stacks at each echo time
(TE), leading to long scan times and high sensitivity to motion. We tackle this
challenge with a method that jointly reconstructs data across TEs, addressing
severe motion. Our approach combines implicit neural representations with a
physics-informed regularization that models T2 decay, enabling information
sharing across TEs while preserving anatomical and quantitative T2 fidelity. We
demonstrate state-of-the-art performance on simulated fetal brain and in vivo
adult datasets with fetal-like motion. We also present the first in vivo fetal
T2 mapping results at 0.55T. Our study shows potential for reducing the number
of stacks per TE in T2 mapping by leveraging anatomical redundancy.

</details>


### [110] [IADGPT: Unified LVLM for Few-Shot Industrial Anomaly Detection, Localization, and Reasoning via In-Context Learning](https://arxiv.org/abs/2508.10681)
*Mengyang Zhao,Teng Fu,Haiyang Yu,Ke Niu,Bin Li*

Main category: cs.CV

TL;DR: 本文提出IADGPT框架，通过三阶段渐进式训练和上下文学习，使大型视觉-语言模型（LVLMs）具备类人工业缺陷检测、定位和推理能力，并在新数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有基于LVLMs的少样本工业异常检测（FS-IAD）方法缺乏工业领域知识和推理能力，远不及专业人工质检员，且LVLMs主要关注通用任务。

Method: 提出统一框架IADGPT，采用三阶段渐进式训练策略：1) 获取基础工业知识；2) 培养异常识别能力；3) 通过上下文学习利用少量样本提升对新产品的泛化能力。设计策略利用logit输出和注意力图生成图像级和像素级异常分数，并结合语言输出实现异常推理。构建包含10万张图片、400个产品类别及详细文本标注的新数据集支持训练。

Result: IADGPT在异常检测方面取得了显著性能提升，并在异常定位和推理任务中展现出竞争力。

Conclusion: IADGPT通过模仿人类学习过程，有效解决了LVLMs在FS-IAD中缺乏专业知识和推理能力的问题，实现了在检测、定位和推理任务上的优异表现，为自动化工业质检提供了新范式。

Abstract: Few-Shot Industrial Anomaly Detection (FS-IAD) has important applications in
automating industrial quality inspection. Recently, some FS-IAD methods based
on Large Vision-Language Models (LVLMs) have been proposed with some
achievements through prompt learning or fine-tuning. However, existing LVLMs
focus on general tasks but lack basic industrial knowledge and reasoning
capabilities related to FS-IAD, making these methods far from specialized human
quality inspectors. To address these challenges, we propose a unified
framework, IADGPT, designed to perform FS-IAD in a human-like manner, while
also handling associated localization and reasoning tasks, even for diverse and
novel industrial products. To this end, we introduce a three-stage progressive
training strategy inspired by humans. Specifically, the first two stages
gradually guide IADGPT in acquiring fundamental industrial knowledge and
discrepancy awareness. In the third stage, we design an in-context
learning-based training paradigm, enabling IADGPT to leverage a few-shot image
as the exemplars for improved generalization to novel products. In addition, we
design a strategy that enables IADGPT to output image-level and pixel-level
anomaly scores using the logits output and the attention map, respectively, in
conjunction with the language output to accomplish anomaly reasoning. To
support our training, we present a new dataset comprising 100K images across
400 diverse industrial product categories with extensive attribute-level
textual annotations. Experiments indicate IADGPT achieves considerable
performance gains in anomaly detection and demonstrates competitiveness in
anomaly localization and reasoning. We will release our dataset in
camera-ready.

</details>


### [111] [Novel View Synthesis using DDIM Inversion](https://arxiv.org/abs/2508.10688)
*Sehajdeep SIngh,A V Subramanyam*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级的视角转换框架，通过使用DDIM反演的潜在空间和相机姿态条件U-Net（TUNet），结合一种新颖的融合策略，利用预训练扩散模型的生成能力，从单张图片合成高质量的新颖视角。


<details>
  <summary>Details</summary>
Motivation: 从单张图片合成新颖视角是一项挑战性任务，现有方法通常需要昂贵的扩散模型微调或从头训练，且存在重建模糊和泛化能力差的问题。因此，需要探索一种轻量级、能直接利用预训练扩散模型高保真生成能力的显式视角转换框架。

Method: 方法包括三个主要部分：1) 使用DDIM反演将单张输入图像转换为潜在表示；2) 采用一个相机姿态条件翻译U-Net (TUNet) 来预测目标视角的反演潜在表示；3) 提出一种新颖的融合策略，利用DDIM反演中观察到的固有噪声相关结构，以解决潜在表示采样可能导致的模糊重建问题，从而保留纹理和精细细节。最后，将融合后的潜在表示作为DDIM采样的初始条件，利用预训练扩散模型的生成先验来合成新颖视角。

Result: 在MVImgNet上的大量实验表明，该方法优于现有方法。

Conclusion: 所提出的TUNet和融合策略能够有效利用预训练扩散模型的高保真生成能力，从单张图片合成高质量的新颖视角，并克服了现有方法的模糊重建和泛化问题。

Abstract: Synthesizing novel views from a single input image is a challenging task. It
requires extrapolating the 3D structure of a scene while inferring details in
occluded regions, and maintaining geometric consistency across viewpoints. Many
existing methods must fine-tune large diffusion backbones using multiple views
or train a diffusion model from scratch, which is extremely expensive.
Additionally, they suffer from blurry reconstruction and poor generalization.
This gap presents the opportunity to explore an explicit lightweight view
translation framework that can directly utilize the high-fidelity generative
capabilities of a pretrained diffusion model while reconstructing a scene from
a novel view. Given the DDIM-inverted latent of a single input image, we employ
a camera pose-conditioned translation U-Net, TUNet, to predict the inverted
latent corresponding to the desired target view. However, the image sampled
using the predicted latent may result in a blurry reconstruction. To this end,
we propose a novel fusion strategy that exploits the inherent noise correlation
structure observed in DDIM inversion. The proposed fusion strategy helps
preserve the texture and fine-grained details. To synthesize the novel view, we
use the fused latent as the initial condition for DDIM sampling, leveraging the
generative prior of the pretrained diffusion model. Extensive experiments on
MVImgNet demonstrate that our method outperforms existing methods.

</details>


### [112] [Beyond conventional vision: RGB-event fusion for robust object detection in dynamic traffic scenarios](https://arxiv.org/abs/2508.10704)
*Zhanwen Liu,Yujing Sun,Yang Wang,Nan Yang,Shengbo Eben Li,Xiangmo Zhao*

Main category: cs.CV

TL;DR: 该研究提出MCFNet，一个结合事件相机和RGB相机用于恶劣光照下目标检测的网络，通过时间对齐、空间增强和跨模态融合显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 传统RGB相机在复杂交通环境（如夜间、隧道）下存在动态范围限制，导致全局对比度下降、高频细节丢失，进而影响特征提取和基于帧的目标检测性能。

Method: 本文提出运动线索融合网络（MCFNet），将仿生事件相机与RGB相机融合以提供高动态范围信息。具体包括：1) 事件校正模块（ECM）通过光流扭曲实现事件流与图像帧的时间对齐，并与检测网络联合优化学习任务感知事件表示；2) 事件动态上采样模块（EDUM）提升事件帧空间分辨率以匹配图像结构；3) 跨模态Mamba融合模块（CMM）采用新颖的交错扫描机制进行自适应特征融合，有效整合互补信息。

Result: 在DSEC-Det和PKU-DAVIS-SOD数据集上的实验表明，MCFNet在各种恶劣光照和快速移动交通场景下显著优于现有方法。特别是在DSEC-Det数据集上，MCFNet在mAP50和mAP指标上分别超越现有最佳方法7.4%和1.7%。

Conclusion: MCFNet通过有效整合事件相机和RGB相机数据，实现了在恶劣光照和快速移动交通场景下鲁棒且高性能的目标检测，显著提升了现有方法的性能。

Abstract: The dynamic range limitation of conventional RGB cameras reduces global
contrast and causes loss of high-frequency details such as textures and edges
in complex traffic environments (e.g., nighttime driving, tunnels), hindering
discriminative feature extraction and degrading frame-based object detection.
To address this, we integrate a bio-inspired event camera with an RGB camera to
provide high dynamic range information and propose a motion cue fusion network
(MCFNet), which achieves optimal spatiotemporal alignment and adaptive
cross-modal feature fusion under challenging lighting. Specifically, an event
correction module (ECM) temporally aligns asynchronous event streams with image
frames via optical-flow-based warping, jointly optimized with the detection
network to learn task-aware event representations. The event dynamic upsampling
module (EDUM) enhances spatial resolution of event frames to match image
structures, ensuring precise spatiotemporal alignment. The cross-modal mamba
fusion module (CMM) uses adaptive feature fusion with a novel interlaced
scanning mechanism, effectively integrating complementary information for
robust detection. Experiments conducted on the DSEC-Det and PKU-DAVIS-SOD
datasets demonstrate that MCFNet significantly outperforms existing methods in
various poor lighting and fast moving traffic scenarios. Notably, on the
DSEC-Det dataset, MCFNet achieves a remarkable improvement, surpassing the best
existing methods by 7.4% in mAP50 and 1.7% in mAP metrics, respectively. The
code is available at https://github.com/Charm11492/MCFNet.

</details>


### [113] [CountCluster: Training-Free Object Quantity Guidance with Cross-Attention Map Clustering for Text-to-Image Generation](https://arxiv.org/abs/2508.10710)
*Joohyeon Lee,Jin-Seop Lee,Jee-Hyong Lee*

Main category: cs.CV

TL;DR: 扩散模型在生成图像时难以准确控制物体数量。本文提出CountCluster，通过在推理早期引导交叉注意力图聚类，显著提高了物体数量的生成精度。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像质量和多样性上表现出色，但在生成指定数量物体时表现不佳。现有方法依赖外部模块或学习到的表示，但仍不准确，且忽略了物体数量在去噪过程早期已基本确定的关键结构特性。

Method: 本文提出CountCluster方法，无需外部工具或额外训练。该方法在推理时，根据输入指定的物体数量k，将物体交叉注意力图划分为k个簇。定义一个理想的、空间上清晰分离的簇分布，并优化潜在表示以使其与该目标分布对齐。

Result: 与现有方法相比，物体计数准确率平均提高了18.5个百分点。在各种提示下，都展现出卓越的数量控制性能。

Conclusion: CountCluster通过在去噪过程早期引导物体交叉注意力图形成与指定数量匹配且空间分离的簇，有效解决了文本到图像扩散模型在生成指定数量物体时的准确性问题。

Abstract: Diffusion-based text-to-image generation models have demonstrated strong
performance in terms of image quality and diversity. However, they still
struggle to generate images that accurately reflect the number of objects
specified in the input prompt. Several approaches have been proposed that rely
on either external counting modules for iterative refinement or quantity
representations derived from learned tokens or latent features. However, they
still have limitations in accurately reflecting the specified number of objects
and overlook an important structural characteristic--The number of object
instances in the generated image is largely determined in the early timesteps
of the denoising process. To correctly reflect the object quantity for image
generation, the highly activated regions in the object cross-attention map at
the early timesteps should match the input object quantity, while each region
should be clearly separated. To address this issue, we propose
\textit{CountCluster}, a method that guides the object cross-attention map to
be clustered according to the specified object count in the input, without
relying on any external tools or additional training. The proposed method
partitions the object cross-attention map into $k$ clusters at inference time
based on attention scores, defines an ideal distribution in which each cluster
is spatially well-separated, and optimizes the latent to align with this target
distribution. Our method achieves an average improvement of 18.5\%p in object
count accuracy compared to existing methods, and demonstrates superior quantity
control performance across a variety of prompts. Code will be released at:
https://github.com/JoohyeonL22/CountCluster .

</details>


### [114] [NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale](https://arxiv.org/abs/2508.10711)
*NextStep Team,Chunrui Han,Guopeng Li,Jingwei Wu,Quan Sun,Yan Cai,Yuang Peng,Zheng Ge,Deyu Zhou,Haomiao Tang,Hongyu Zhou,Kenkun Liu,Ailin Huang,Bin Wang,Changxin Miao,Deshan Sun,En Yu,Fukun Yin,Gang Yu,Hao Nie,Haoran Lv,Hanpeng Hu,Jia Wang,Jian Zhou,Jianjian Sun,Kaijun Tan,Kang An,Kangheng Lin,Liang Zhao,Mei Chen,Peng Xing,Rui Wang,Shiyu Liu,Shutao Xia,Tianhao You,Wei Ji,Xianfang Zeng,Xin Han,Xuelin Zhang,Yana Wei,Yanming Xu,Yimin Jiang,Yingming Wang,Yu Zhou,Yucheng Han,Ziyang Meng,Binxing Jiao,Daxin Jiang,Xiangyu Zhang,Yibo Zhu*

Main category: cs.CV

TL;DR: NextStep-1是一个14B的自回归模型，结合157M的流匹配头部，通过离散文本和连续图像token训练，实现了文本到图像生成中自回归模型的最新SOTA性能，并展示了强大的图像编辑能力。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像的自回归模型存在局限性：要么依赖计算密集型的扩散模型处理连续图像token，要么使用矢量量化（VQ）获取离散token但引入量化损失。

Method: 本文提出了NextStep-1，一个14B的自回归模型，搭配157M的流匹配头部。该模型利用下一token预测目标，在离散文本token和连续图像token上进行训练。

Result: NextStep-1在文本到图像生成任务中取得了自回归模型的最新SOTA性能，展现出高保真图像合成的强大能力。此外，该方法在图像编辑方面也表现出色。

Conclusion: NextStep-1的统一方法（自回归模型与流匹配结合）强大且多功能，显著提升了自回归模型在文本到图像生成领域的性能，并具备出色的图像编辑能力。为促进开放研究，代码和模型将对外发布。

Abstract: Prevailing autoregressive (AR) models for text-to-image generation either
rely on heavy, computationally-intensive diffusion models to process continuous
image tokens, or employ vector quantization (VQ) to obtain discrete tokens with
quantization loss. In this paper, we push the autoregressive paradigm forward
with NextStep-1, a 14B autoregressive model paired with a 157M flow matching
head, training on discrete text tokens and continuous image tokens with
next-token prediction objectives. NextStep-1 achieves state-of-the-art
performance for autoregressive models in text-to-image generation tasks,
exhibiting strong capabilities in high-fidelity image synthesis. Furthermore,
our method shows strong performance in image editing, highlighting the power
and versatility of our unified approach. To facilitate open research, we will
release our code and models to the community.

</details>


### [115] [Lightweight CNNs for Embedded SAR Ship Target Detection and Classification](https://arxiv.org/abs/2508.10712)
*Fabian Kresse,Georgios Pilikos,Mario Azcueta,Nicolas Floury*

Main category: cs.CV

TL;DR: 该研究提出并评估了用于星载实时处理未聚焦合成孔径雷达（SAR）数据的神经网络，以实现船舶监测和目标分类，旨在解决传统地面处理带来的带宽和延迟问题。


<details>
  <summary>Details</summary>
Motivation: 目前的SAR船舶监测依赖于下行所有原始数据并在地面处理，导致高延迟和带宽限制。卫星内存、处理能力和计算资源的限制使得传统图像聚焦和处理算法难以在星上部署，因此需要新的方法来减少下行数据量并降低延迟。

Method: 提出并评估了专门设计的神经网络，用于对Sentinel-1卫星在Stripmap和干涉宽幅（IW）模式下获取的未聚焦SAR数据进行实时推理。此外，还研究了船舶和风车之间的二元分类任务。

Result: 研究结果表明，其中一个模型可用于星载处理并在FPGA上部署，证明了其可行性。通过二元分类任务，验证了目标分类的可能性。

Conclusion: 神经网络是实现星载实时SAR数据处理的可行方案，能够有效减少数据下行量、降低延迟，并支持船舶监测和目标分类，从而提升海上监视能力。

Abstract: Synthetic Aperture Radar (SAR) data enables large-scale surveillance of
maritime vessels. However, near-real-time monitoring is currently constrained
by the need to downlink all raw data, perform image focusing, and subsequently
analyze it on the ground. On-board processing to generate higher-level products
could reduce the data volume that needs to be downlinked, alleviating bandwidth
constraints and minimizing latency. However, traditional image focusing and
processing algorithms face challenges due to the satellite's limited memory,
processing power, and computational resources. This work proposes and evaluates
neural networks designed for real-time inference on unfocused SAR data acquired
in Stripmap and Interferometric Wide (IW) modes captured with Sentinel-1. Our
results demonstrate the feasibility of using one of our models for on-board
processing and deployment on an FPGA. Additionally, by investigating a binary
classification task between ships and windmills, we demonstrate that target
classification is possible.

</details>


### [116] [Revisiting Cross-View Localization from Image Matching](https://arxiv.org/abs/2508.10716)
*Panwang Xia,Qiong Wu,Lei Yu,Yi Liu,Mingtao Xiong,Lei Liang,Yongjun Zhang,Yi Wan*

Main category: cs.CV

TL;DR: 本文提出了一种新的跨视角定位框架，通过引入表面模型和相似度细化模块，显著提升了地面图像与航空影像之间的匹配精度和定位准确性，并发布了首个带有像素级对应标注的大规模跨视角图像匹配数据集CVFM。


<details>
  <summary>Details</summary>
Motivation: 在城市峡谷和灾区等GNSS受限环境中，跨视角定位至关重要。现有方法（直接回归姿态或在共享鸟瞰图空间对齐特征）依赖于精确的空间对应关系，但未能建立严格的跨视角对应，导致匹配粗糙或几何不一致，使得细粒度图像匹配成为未解决的问题，限制了定位结果的可解释性。

Method: 本文从跨视角图像匹配的角度重新审视跨视角定位，提出一个新颖框架。具体方法包括：引入一个表面模型（Surface Model）来建模可见区域，以实现精确的鸟瞰图投影；设计一个SimRefiner模块，通过局部-全局残差校正来细化相似度矩阵，避免对RANSAC等后处理的依赖。此外，本文还发布了首个包含32,509对带有像素级对应标注的跨视角图像对的基准数据集CVFM。

Result: 广泛的实验证明，所提出的方法在极端视角差异下，显著提高了定位精度和图像匹配质量，并为该领域设定了新的基线。

Conclusion: 本文提出的框架有效改善了跨视角定位中的图像匹配和定位性能，并通过发布新数据集CVFM，为未来的相关研究提供了重要支持。

Abstract: Cross-view localization aims to estimate the 3 degrees of freedom pose of a
ground-view image by registering it to aerial or satellite imagery. It is
essential in GNSS-denied environments such as urban canyons and disaster zones.
Existing methods either regress poses directly or align features in a shared
bird's-eye view (BEV) space, both built upon accurate spatial correspondences
between perspectives. However, these methods fail to establish strict
cross-view correspondences, yielding only coarse or geometrically inconsistent
matches. Consequently, fine-grained image matching between ground and aerial
views remains an unsolved problem, which in turn constrains the
interpretability of localization results. In this paper, we revisit cross-view
localization from the perspective of cross-view image matching and propose a
novel framework that improves both matching and localization. Specifically, we
introduce a Surface Model to model visible regions for accurate BEV projection,
and a SimRefiner module to refine the similarity matrix through local-global
residual correction, eliminating the reliance on post-processing like RANSAC.
To further support research in this area, we introduce CVFM, the first
benchmark with 32,509 cross-view image pairs annotated with pixel-level
correspondences. Extensive experiments demonstrate that our approach
substantially improves both localization accuracy and image matching quality,
setting new baselines under extreme viewpoint disparity.

</details>


### [117] [Exploiting Discriminative Codebook Prior for Autoregressive Image Generation](https://arxiv.org/abs/2508.10719)
*Longxiang Tang,Ruihang Chu,Xiang Wang,Yujin Han,Pingyu Wu,Chunming He,Yingya Zhang,Shiwei Zhang,Jiaya Jia*

Main category: cs.CV

TL;DR: 本文提出判别性码本先验提取器（DCPE），旨在更有效地利用自回归图像生成模型中码本的令牌相似性信息，以克服现有k-means聚类方法的缺陷，从而加速模型训练并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有的离散令牌自回归图像生成系统在训练时未能充分利用码本中包含的丰富令牌相似性信息。尽管有尝试通过k-means聚类引入此先验，但发现k-means在码本特征空间中表现不佳，存在令牌空间差异和质心距离不准确等固有问题。

Method: 本文提出DCPE，替代k-means聚类。DCPE用更适合令牌特征空间的基于实例的距离替换了常用的基于质心的距离，并通过凝聚合并技术避免分割高密度区域并聚合低密度区域，从而解决了令牌空间差异问题。

Result: 实验证明，DCPE即插即用，可与现有基于码本先验的范式无缝集成。通过DCPE提取的判别性先验，LlamaGen-B上的自回归模型训练速度加快了42%，并提升了最终的FID和IS性能。

Conclusion: DCPE通过更有效地挖掘和利用码本中的令牌相似性信息，克服了传统聚类方法的局限性，显著加速了自回归图像模型的训练并提高了生成质量，为利用码本先验提供了有效途径。

Abstract: Advanced discrete token-based autoregressive image generation systems first
tokenize images into sequences of token indices with a codebook, and then model
these sequences in an autoregressive paradigm. While autoregressive generative
models are trained only on index values, the prior encoded in the codebook,
which contains rich token similarity information, is not exploited. Recent
studies have attempted to incorporate this prior by performing naive k-means
clustering on the tokens, helping to facilitate the training of generative
models with a reduced codebook. However, we reveal that k-means clustering
performs poorly in the codebook feature space due to inherent issues, including
token space disparity and centroid distance inaccuracy. In this work, we
propose the Discriminative Codebook Prior Extractor (DCPE) as an alternative to
k-means clustering for more effectively mining and utilizing the token
similarity information embedded in the codebook. DCPE replaces the commonly
used centroid-based distance, which is found to be unsuitable and inaccurate
for the token feature space, with a more reasonable instance-based distance.
Using an agglomerative merging technique, it further addresses the token space
disparity issue by avoiding splitting high-density regions and aggregating
low-density ones. Extensive experiments demonstrate that DCPE is plug-and-play
and integrates seamlessly with existing codebook prior-based paradigms. With
the discriminative prior extracted, DCPE accelerates the training of
autoregressive models by 42% on LlamaGen-B and improves final FID and IS
performance.

</details>


### [118] [Dissecting Generalized Category Discovery: Multiplex Consensus under Self-Deconstruction](https://arxiv.org/abs/2508.10731)
*Luyao Tang,Kunze Huang,Chaoqi Chen,Yuxuan Yuan,Chenxin Li,Xiaotong Tu,Xinghao Ding,Yue Huang*

Main category: cs.CV

TL;DR: ConGCD是一种受人类认知启发的广义类别发现（GCD）新范式，通过分解视觉基元并整合多源共识，有效识别已知及新颖类别。


<details>
  <summary>Details</summary>
Motivation: 现有的机器学习框架在识别已知和新颖类别的物体方面远不如人类感知系统。虽然广义类别发现（GCD）旨在弥合这一差距，但现有方法主要侧重于优化目标函数，未能充分模拟人类理解新物体的认知过程。

Method: 本文提出ConGCD，其灵感来源于人类将物体分解为视觉基元并进行跨知识比较的认知过程。ConGCD通过高级语义重建建立面向基元的表示，并通过解构绑定类内共享属性。它引入了主导和上下文共识单元，分别捕获类别判别模式和固有的分布不变性，以模拟人类视觉处理中的偏好多样性。一个共识调度器动态优化激活路径，最终预测通过多重共识集成产生。

Result: 在粗粒度和细粒度基准上的广泛评估表明，ConGCD作为一种共识感知范式表现出卓越的有效性。

Conclusion: ConGCD通过模仿人类认知过程中的基元分解和多源共识集成，为广义类别发现提供了一种有效且新颖的解决方案，显著提升了机器识别新颖类别的能力。

Abstract: Human perceptual systems excel at inducing and recognizing objects across
both known and novel categories, a capability far beyond current machine
learning frameworks. While generalized category discovery (GCD) aims to bridge
this gap, existing methods predominantly focus on optimizing objective
functions. We present an orthogonal solution, inspired by the human cognitive
process for novel object understanding: decomposing objects into visual
primitives and establishing cross-knowledge comparisons. We propose ConGCD,
which establishes primitive-oriented representations through high-level
semantic reconstruction, binding intra-class shared attributes via
deconstruction. Mirroring human preference diversity in visual processing,
where distinct individuals leverage dominant or contextual cues, we implement
dominant and contextual consensus units to capture class-discriminative
patterns and inherent distributional invariants, respectively. A consensus
scheduler dynamically optimizes activation pathways, with final predictions
emerging through multiplex consensus integration. Extensive evaluations across
coarse- and fine-grained benchmarks demonstrate ConGCD's effectiveness as a
consensus-aware paradigm. Code is available at github.com/lytang63/ConGCD.

</details>


### [119] [Privacy-enhancing Sclera Segmentation Benchmarking Competition: SSBC 2025](https://arxiv.org/abs/2508.10737)
*Matej Vitek,Darian Tomašević,Abhijit Das,Sabari Nathan,Gökhan Özbulak,Gözde Ayşe Tataroğlu Özbulak,Jean-Paul Calbimonte,André Anjos,Hariohm Hemant Bhatt,Dhruv Dhirendra Premani,Jay Chaudhari,Caiyong Wang,Jian Jiang,Chi Zhang,Qi Zhang,Iyyakutti Iyappan Ganapathi,Syed Sadaf Ali,Divya Velayudan,Maregu Assefa,Naoufel Werghi,Zachary A. Daniels,Leeon John,Ritesh Vyas,Jalil Nourmohammadi Khiarak,Taher Akbari Saeed,Mahsa Nasehi,Ali Kianfar,Mobina Pashazadeh Panahi,Geetanjali Sharma,Pushp Raj Panth,Raghavendra Ramachandra,Aditya Nigam,Umapada Pal,Peter Peer,Vitomir Štruc*

Main category: cs.CV

TL;DR: 2025年巩膜分割基准竞赛（SSBC）总结，评估了使用合成图像训练的隐私保护巩膜分割模型，发现纯合成数据训练的模型也能达到有竞争力的性能。


<details>
  <summary>Details</summary>
Motivation: 旨在评估使用合成数据训练的模型与使用真实数据训练的模型相比表现如何，并推动隐私保护的生物识别技术发展。

Method: 竞赛设置了两个赛道：(i) 仅使用合成数据进行模型开发；(ii) 混合使用合成数据和少量真实数据。共有九个研究团队提交了多样化的分割模型（包括Transformer、轻量级、生成框架引导等）。实验在包含合成和真实图像的三个评估数据集上进行。

Result: 结果显示，完全使用合成数据训练的模型可以达到有竞争力的性能，特别是采用专用训练策略时（F1分数超过0.8）。混合赛道中的性能提升更多地由方法选择而非真实数据引入驱动。

Conclusion: 合成数据在隐私保护的生物识别技术开发中具有巨大潜力。

Abstract: This paper presents a summary of the 2025 Sclera Segmentation Benchmarking
Competition (SSBC), which focused on the development of privacy-preserving
sclera-segmentation models trained using synthetically generated ocular images.
The goal of the competition was to evaluate how well models trained on
synthetic data perform in comparison to those trained on real-world datasets.
The competition featured two tracks: $(i)$ one relying solely on synthetic data
for model development, and $(ii)$ one combining/mixing synthetic with (a
limited amount of) real-world data. A total of nine research groups submitted
diverse segmentation models, employing a variety of architectural designs,
including transformer-based solutions, lightweight models, and segmentation
networks guided by generative frameworks. Experiments were conducted across
three evaluation datasets containing both synthetic and real-world images,
collected under diverse conditions. Results show that models trained entirely
on synthetic data can achieve competitive performance, particularly when
dedicated training strategies are employed, as evidenced by the top performing
models that achieved $F_1$ scores of over $0.8$ in the synthetic data track.
Moreover, performance gains in the mixed track were often driven more by
methodological choices rather than by the inclusion of real data, highlighting
the promise of synthetic data for privacy-aware biometric development. The code
and data for the competition is available at:
https://github.com/dariant/SSBC_2025.

</details>


### [120] [Axis-level Symmetry Detection with Group-Equivariant Representation](https://arxiv.org/abs/2508.10740)
*Wongyun Yu,Ahyun Seo,Minsu Cho*

Main category: cs.CV

TL;DR: 该论文提出了一种新颖的框架，用于精确检测图像中的反射和旋转对称性，将它们表示为几何原语（线和点），并通过双分支、二面体群等变架构实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 在复杂场景中检测对称性是一个重大挑战，现有的基于热图的方法虽然能定位潜在区域，但在识别单个对称轴时缺乏精度。

Method: 本文提出一个双分支架构，该架构对二面体群具有等变性，每个分支专门处理其各自的对称类型。对于反射对称，引入了与群分量对齐的方向锚点和反射匹配机制；对于旋转对称，提出了旋转匹配机制，通过比较固定角度间隔的模式来识别旋转中心。对称性被表示为显式几何原语（线和点）。

Result: 实验结果表明，该方法实现了最先进的性能，优于现有方法。

Conclusion: 本研究提供了一种新颖且高精度的轴级别反射和旋转对称性检测方法，通过几何原语表示和专门的等变架构克服了现有方法的局限性。

Abstract: Symmetry is a fundamental concept that has been extensively studied, yet
detecting it in complex scenes remains a significant challenge in computer
vision. Recent heatmap-based approaches can localize potential regions of
symmetry axes but often lack precision in identifying individual axes. In this
work, we propose a novel framework for axis-level detection of the two most
common symmetry types-reflection and rotation-by representing them as explicit
geometric primitives, i.e. lines and points. Our method employs a dual-branch
architecture that is equivariant to the dihedral group, with each branch
specialized to exploit the structure of dihedral group-equivariant features for
its respective symmetry type. For reflection symmetry, we introduce
orientational anchors, aligned with group components, to enable
orientation-specific detection, and a reflectional matching that measures
similarity between patterns and their mirrored counterparts across candidate
axes. For rotational symmetry, we propose a rotational matching that compares
patterns at fixed angular intervals to identify rotational centers. Extensive
experiments demonstrate that our method achieves state-of-the-art performance,
outperforming existing approaches.

</details>


### [121] [Forgery Guided Learning Strategy with Dual Perception Network for Deepfake Cross-domain Detection](https://arxiv.org/abs/2508.10741)
*Lixin Jia,Zhiqing Guo,Gaobo Yang,Liejun Wang,Keqin Li*

Main category: cs.CV

TL;DR: 针对现有深度伪造检测方法在未知伪造技术上泛化性差的问题，本文提出了一种伪造引导学习（FGL）策略和双感知网络（DPNet），通过捕获伪造痕迹的差异和关系，显著提升了检测模型对未知伪造的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前深度伪造检测方法在特定数据集上表现良好，但在未知伪造技术数据集上性能不佳。新旧伪造技术之间的差距日益扩大，导致依赖共同伪造痕迹的跨域检测方法失效。因此，迫切需要开发具有强大泛化能力的深度伪造检测技术来应对快速迭代的伪造技术。

Method: 本文提出Forgery Guided Learning (FGL) 策略，通过捕获已知和未知伪造技术之间的差异信息，使检测模型能够实时动态调整学习过程，持续适应未知伪造技术。同时，设计了Dual Perception Network (DPNet)，该网络在频率流中动态感知并提取不同伪造技术的判别性特征，并将其与空间特征融合投影到嵌入空间。此外，利用图卷积感知整个特征空间中的伪造痕迹关系，以实现更全面的理解。

Result: 广泛的实验证明，所提出的方法在不同场景下均表现出良好的泛化能力，并能有效应对未知伪造带来的挑战，为深度伪造检测提供了鲁棒支持。

Conclusion: 通过引入FGL策略和DPNet网络，本研究成功解决了深度伪造检测中对未知伪造技术泛化性差的问题，为应对不断演进的伪造技术提供了有效的解决方案。

Abstract: The emergence of deepfake technology has introduced a range of societal
problems, garnering considerable attention. Current deepfake detection methods
perform well on specific datasets, but exhibit poor performance when applied to
datasets with unknown forgery techniques. Moreover, as the gap between emerging
and traditional forgery techniques continues to widen, cross-domain detection
methods that rely on common forgery traces are becoming increasingly
ineffective. This situation highlights the urgency of developing deepfake
detection technology with strong generalization to cope with fast iterative
forgery techniques. To address these challenges, we propose a Forgery Guided
Learning (FGL) strategy designed to enable detection networks to continuously
adapt to unknown forgery techniques. Specifically, the FGL strategy captures
the differential information between known and unknown forgery techniques,
allowing the model to dynamically adjust its learning process in real time. To
further improve the ability to perceive forgery traces, we design a Dual
Perception Network (DPNet) that captures both differences and relationships
among forgery traces. In the frequency stream, the network dynamically
perceives and extracts discriminative features across various forgery
techniques, establishing essential detection cues. These features are then
integrated with spatial features and projected into the embedding space. In
addition, graph convolution is employed to perceive relationships across the
entire feature space, facilitating a more comprehensive understanding of
forgery trace correlations. Extensive experiments show that our approach
generalizes well across different scenarios and effectively handles unknown
forgery challenges, providing robust support for deepfake detection. Our code
is available on https://github.com/vpsg-research/FGL.

</details>


### [122] [An Efficient Model-Driven Groupwise Approach for Atlas Construction](https://arxiv.org/abs/2508.10743)
*Ziwei Zou,Bei Zou,Xiaoyan Kui,Wenqi Lu,Haoran Dou,Arezoo Zakeri,Timothy Cootes,Alejandro F Frangi,Jinming Duan*

Main category: cs.CV

TL;DR: DARC是一种新型模型驱动的群组配准框架，用于高效灵活地构建图谱，并支持单次分割和形状合成应用。


<details>
  <summary>Details</summary>
Motivation: 当前数据驱动的图谱构建方法存在对大量训练数据的依赖、泛化能力有限以及在群组环境中缺乏真实推理阶段的问题。而模型驱动方法虽然理论基础扎实，但在处理大型3D数据集时常面临可扩展性和优化挑战。

Method: DARC（通过坐标下降进行微分同胚图谱配准）是一种新颖的模型驱动群组配准框架。它支持广泛的图像差异度量，并能高效处理任意数量的3D图像，避免GPU内存问题。通过坐标下降策略和强制中心性的激活函数，DARC能生成无偏、微分同胚且具有高解剖保真度的图谱。

Result: DARC成功构建了无偏、微分同胚且具有高解剖保真度的图谱。在应用方面，它实现了单次分割，仅需在图谱上标注标签即可通过逆变形传播到受试者，性能优于当前最先进的少样本方法；同时，它还能通过合成微分同胚变形场来扭曲图谱网格，从而生成新的解剖变体，实现形状合成。

Conclusion: DARC为图谱构建及其在医学图像分析中的应用提供了一个灵活、通用且资源高效的框架。

Abstract: Atlas construction is fundamental to medical image analysis, offering a
standardized spatial reference for tasks such as population-level anatomical
modeling. While data-driven registration methods have recently shown promise in
pairwise settings, their reliance on large training datasets, limited
generalizability, and lack of true inference phases in groupwise contexts
hinder their practical use. In contrast, model-driven methods offer
training-free, theoretically grounded, and data-efficient alternatives, though
they often face scalability and optimization challenges when applied to large
3D datasets. In this work, we introduce DARC (Diffeomorphic Atlas Registration
via Coordinate descent), a novel model-driven groupwise registration framework
for atlas construction. DARC supports a broad range of image dissimilarity
metrics and efficiently handles arbitrary numbers of 3D images without
incurring GPU memory issues. Through a coordinate descent strategy and a
centrality-enforcing activation function, DARC produces unbiased, diffeomorphic
atlases with high anatomical fidelity. Beyond atlas construction, we
demonstrate two key applications: (1) One-shot segmentation, where labels
annotated only on the atlas are propagated to subjects via inverse
deformations, outperforming state-of-the-art few-shot methods; and (2) shape
synthesis, where new anatomical variants are generated by warping the atlas
mesh using synthesized diffeomorphic deformation fields. Overall, DARC offers a
flexible, generalizable, and resource-efficient framework for atlas
construction and applications.

</details>


### [123] [From Diagnosis to Improvement: Probing Spatio-Physical Reasoning in Vision Language Models](https://arxiv.org/abs/2508.10770)
*Tiancheng Han,Yunfei Gao,Yong Li,Wuzhou Yu,Qiaosheng Zhang,Wenqi Shao*

Main category: cs.CV

TL;DR: 该研究诊断了主流视觉语言模型（VLMs）在时空物理推理方面的不足，发现其受人类先验偏差和缺乏深度推理的影响。通过对Qwen2.5-VL-7B进行监督微调和基于规则的强化学习，显著提升了其时空物理推理能力，但模型对新物理场景的泛化能力仍有限。


<details>
  <summary>Details</summary>
Motivation: 时空物理推理是理解真实物理世界和构建鲁棒世界模型的关键基础能力。尽管近期视觉语言模型在多模态数学和纯空间理解等领域取得了显著进展，但其在时空物理推理方面的能力仍未被充分探索，且可能存在不足。

Method: 本研究首先对主流视觉语言模型进行了全面的诊断分析，以评估它们在时空物理推理任务上的表现。随后，为了解决发现的问题，对Qwen2.5-VL-7B模型应用了监督微调（SFT），并辅以基于规则的强化学习（RL）。

Result: 诊断分析显示，当前模型在时空物理推理任务上表现不佳，主要归因于人类先验引起的偏差和缺乏深度推理能力。通过应用监督微调和基于规则的强化学习，Qwen2.5-VL-7B的时空物理推理能力得到了显著提升，并超越了领先的专有模型。

Conclusion: 尽管通过提出的方法显著提升了模型的时空物理推理能力并超越了现有模型，但模型对新物理场景的泛化能力仍然有限。这强调了在时空物理推理领域迫切需要新的方法和研究方向。

Abstract: Spatio-physical reasoning, a foundation capability for understanding the real
physics world, is a critical step towards building robust world models. While
recent vision language models (VLMs) have shown remarkable progress in
specialized domains like multimodal mathematics and pure spatial understanding,
their capability for spatio-physical reasoning remains largely unexplored. This
paper provides a comprehensive diagnostic analysis of mainstream VLMs,
revealing that current models perform inadequately on this crucial task.
Further detailed analysis shows that this underperformance is largely
attributable to biases caused by human-like prior and a lack of deep reasoning.
To address these challenges, we apply supervised fine-tuning followed by
rule-based reinforcement learning to Qwen2.5-VL-7B, resulting in significant
improvements in spatio-physical reasoning capabilities and surpassing leading
proprietary models. Nevertheless, despite this success, the model's
generalization to new physics scenarios remains limited -- underscoring the
pressing need for new approaches in spatio-physical reasoning.

</details>


### [124] [Cooperative Face Liveness Detection from Optical Flow](https://arxiv.org/abs/2508.10786)
*Artem Sokolov,Mikhail Nikitin,Anton Konushin*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的基于用户交互的合作式活体检测方法，通过分析用户缓慢靠近摄像头的视频流和光流信息来区分真人与攻击。


<details>
  <summary>Details</summary>
Motivation: 现有活体检测方法在对抗各种呈现攻击（如照片、屏幕、面具、视频回放）时效果不佳，需要更可靠的判别方法。

Method: 核心方法是设计一种新的用户交互场景：参与者被指示缓慢地将面部靠近摄像头。结合光学流分析（通过神经光学流估计），从这种特定运动模式中提取面部体积信息。最终，通过一个神经网络分类器处理预测的光学流和RGB帧，利用时空特征进行活体检测。

Result: 该方法显著提高了真实人脸与各种呈现攻击（包括打印照片、屏幕显示、面具和视频回放）之间的区分能力，并且相比被动方法，提供了更可靠的活体检测。

Conclusion: 通过结合受控的面部靠近协议和光流分析，该合作式视频活体检测方法能够有效利用时空特征，实现对多种呈现攻击的鲁棒检测。

Abstract: In this work, we proposed a novel cooperative video-based face liveness
detection method based on a new user interaction scenario where participants
are instructed to slowly move their frontal-oriented face closer to the camera.
This controlled approaching face protocol, combined with optical flow analysis,
represents the core innovation of our approach. By designing a system where
users follow this specific movement pattern, we enable robust extraction of
facial volume information through neural optical flow estimation, significantly
improving discrimination between genuine faces and various presentation attacks
(including printed photos, screen displays, masks, and video replays). Our
method processes both the predicted optical flows and RGB frames through a
neural classifier, effectively leveraging spatial-temporal features for more
reliable liveness detection compared to passive methods.

</details>


### [125] [VasoMIM: Vascular Anatomy-Aware Masked Image Modeling for Vessel Segmentation](https://arxiv.org/abs/2508.10794)
*De-Xing Huang,Xiao-Hu Zhou,Mei-Jiang Gui,Xiao-Liang Xie,Shi-Qi Liu,Shuang-Yi Wang,Tian-Yu Xiang,Rui-Ze Ma,Nu-Fang Xiao,Zeng-Guang Hou*

Main category: cs.CV

TL;DR: 本文提出VasoMIM，一种针对X射线血管造影图像的新型自监督学习框架，通过整合血管解剖知识解决数据稀缺和类别不平衡问题，从而提高血管分割的准确性。


<details>
  <summary>Details</summary>
Motivation: 血管造影图像中精确的血管分割对临床应用至关重要，但带注释数据的稀缺性是一个巨大挑战。传统的自监督学习方法（如掩码图像建模MIM）由于血管与背景像素之间的严重类别不平衡，未能有效捕捉血管解剖结构，导致血管表示能力弱。

Method: 本文引入VasoMIM（血管解剖感知掩码图像建模）框架，该框架将解剖知识显式整合到预训练过程中。它包含两个互补组件：1) 解剖引导掩码策略，优先掩盖包含血管的图像块，使模型专注于重建血管相关区域；2) 解剖一致性损失，强制原始图像和重建图像之间血管语义的一致性，从而提高血管表示的区分度。

Result: VasoMIM在三个数据集上均取得了最先进的性能。

Conclusion: VasoMIM有望促进X射线血管造影图像的分析。

Abstract: Accurate vessel segmentation in X-ray angiograms is crucial for numerous
clinical applications. However, the scarcity of annotated data presents a
significant challenge, which has driven the adoption of self-supervised
learning (SSL) methods such as masked image modeling (MIM) to leverage
large-scale unlabeled data for learning transferable representations.
Unfortunately, conventional MIM often fails to capture vascular anatomy because
of the severe class imbalance between vessel and background pixels, leading to
weak vascular representations. To address this, we introduce Vascular
anatomy-aware Masked Image Modeling (VasoMIM), a novel MIM framework tailored
for X-ray angiograms that explicitly integrates anatomical knowledge into the
pre-training process. Specifically, it comprises two complementary components:
anatomy-guided masking strategy and anatomical consistency loss. The former
preferentially masks vessel-containing patches to focus the model on
reconstructing vessel-relevant regions. The latter enforces consistency in
vascular semantics between the original and reconstructed images, thereby
improving the discriminability of vascular representations. Empirically,
VasoMIM achieves state-of-the-art performance across three datasets. These
findings highlight its potential to facilitate X-ray angiogram analysis.

</details>


### [126] [Object Fidelity Diffusion for Remote Sensing Image Generation](https://arxiv.org/abs/2508.10801)
*Ziqi Ye,Shuran Ma,Jie Yang,Xiaoyi Yang,Ziyang Gong,Xue Yang,Haipeng Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为OF-Diff的新型扩散模型，用于生成高精度、高保真度的遥感图像，特别是在对象细节和多样性方面表现出色，解决了现有模型保真度低的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型在生成遥感图像时，由于未能充分捕捉形态细节，常导致图像保真度低，这会影响目标检测模型的鲁棒性和可靠性。因此，需要提高生成遥感图像中对象的准确性和保真度。

Method: 1. 首次基于布局提取对象先验形状，用于遥感扩散模型。2. 引入带有扩散一致性损失的双分支扩散模型，实现在采样阶段无需真实图像即可生成高保真图像。3. 引入DDPO（Direct Diffusion Policy Optimization）来微调扩散过程，以提高生成图像的多样性和语义一致性。

Result: OF-Diff在遥感领域的关键质量指标上超越了最先进的方法。特别是，对于多态和小型对象类别（如飞机、船舶和车辆），性能显著提升，mAP分别增加了8.3%、7.7%和4.0%。

Conclusion: OF-Diff模型能够有效提高生成遥感图像中对象的保真度、多样性和语义一致性，显著改善了多态和小型目标检测的性能，为高精度可控遥感图像生成提供了新途径。

Abstract: High-precision controllable remote sensing image generation is both
meaningful and challenging. Existing diffusion models often produce
low-fidelity images due to their inability to adequately capture morphological
details, which may affect the robustness and reliability of object detection
models. To enhance the accuracy and fidelity of generated objects in remote
sensing, this paper proposes Object Fidelity Diffusion (OF-Diff), which
effectively improves the fidelity of generated objects. Specifically, we are
the first to extract the prior shapes of objects based on the layout for
diffusion models in remote sensing. Then, we introduce a dual-branch diffusion
model with diffusion consistency loss, which can generate high-fidelity remote
sensing images without providing real images during the sampling phase.
Furthermore, we introduce DDPO to fine-tune the diffusion process, making the
generated remote sensing images more diverse and semantically consistent.
Comprehensive experiments demonstrate that OF-Diff outperforms state-of-the-art
methods in the remote sensing across key quality metrics. Notably, the
performance of several polymorphic and small object classes shows significant
improvement. For instance, the mAP increases by 8.3%, 7.7%, and 4.0% for
airplanes, ships, and vehicles, respectively.

</details>


### [127] [Mobile-Friendly Deep Learning for Plant Disease Detection: A Lightweight CNN Benchmark Across 101 Classes of 33 Crops](https://arxiv.org/abs/2508.10817)
*Anand Kumar,Harminder Pal Monga,Tapasi Brahma,Satyam Kalra,Navas Sherif*

Main category: cs.CV

TL;DR: 该研究开发了一种移动友好的计算机视觉解决方案，能够准确分类33种作物上的101种植物病害，其中EfficientNet-B1模型在准确性和计算效率之间取得了最佳平衡，达到94.7%的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 植物病害对全球粮食安全构成重大威胁，因此需要开发能准确早期检测病害的系统。计算机视觉技术的进步为解决这一挑战提供了潜力。

Method: 开发了一种移动友好的解决方案，用于分类植物病害。构建了一个综合数据集，结合了PlantDoc、PlantVillage和PlantWild等现有数据集。评估了多种轻量级架构（MobileNetV2、MobileNetV3、MobileNetV3-Large、EfficientNet-B0、B1），这些架构专为资源受限设备上的效率而选择。

Result: 结果令人鼓舞，其中EfficientNet-B1模型表现最佳，分类准确率达到94.7%。

Conclusion: EfficientNet-B1架构在准确性和计算效率之间达到了最佳平衡，非常适合在移动设备上进行实际部署。

Abstract: Plant diseases are a major threat to food security globally. It is important
to develop early detection systems which can accurately detect. The advancement
in computer vision techniques has the potential to solve this challenge. We
have developed a mobile-friendly solution which can accurately classify 101
plant diseases across 33 crops. We built a comprehensive dataset by combining
different datasets, Plant Doc, PlantVillage, and PlantWild, all of which are
for the same purpose. We evaluated performance across several lightweight
architectures - MobileNetV2, MobileNetV3, MobileNetV3-Large, and
EfficientNet-B0, B1 - specifically chosen for their efficiency on
resource-constrained devices. The results were promising, with EfficientNet-B1
delivering our best performance at 94.7% classification accuracy. This
architecture struck an optimal balance between accuracy and computational
efficiency, making it well-suited for real-world deployment on mobile devices.

</details>


### [128] [UI-Venus Technical Report: Building High-performance UI Agents with RFT](https://arxiv.org/abs/2508.10833)
*Zhangxuan Gu,Zhengwen Zeng,Zhenyu Xu,Xingran Zhou,Shuheng Shen,Yunfei Liu,Beitong Zhou,Changhua Meng,Tianyu Xia,Weizhi Chen,Yue Wen,Jingya Dou,Fei Tang,Jinzhen Lin,Yulin Liu,Zhenlin Guo,Yichen Gong,Heng Jia,Changlong Gao,Yuan Guo,Yong Deng,Zhenyu Guo,Liang Chen,Weiqiang Wang*

Main category: cs.CV

TL;DR: UI-Venus是一个基于多模态大语言模型（Qwen2.5-VL）的原生UI代理，仅以屏幕截图为输入，通过强化微调（RFT）在UI定位和导航任务上均达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 开发一个仅依赖屏幕截图作为输入的、高性能的UI代理，以在UI定位和导航任务中超越现有基线，并提供开源解决方案以推动社区研究。

Method: UI-Venus基于Qwen2.5-VL，利用数十万高质量训练样本进行强化微调（RFT）。它引入了精心设计的UI定位和导航任务奖励函数，以及高效的数据清洗策略。为进一步提升导航性能，提出了“自演化轨迹历史对齐与稀疏动作增强”方法，用于优化历史推理轨迹和平衡稀疏关键动作的分布。

Result: 在UI定位任务上，UI-Venus 7B和72B变体在Screenspot-V2 / Pro基准上分别达到94.1% / 50.8%和95.3% / 61.9%，超越了包括GTA1和UI-TARS-1.5在内的现有SOTA基线。在AndroidWorld在线UI导航竞技场中，7B和72B变体分别实现了49.1%和65.9%的成功率，也优于现有模型。

Conclusion: UI-Venus是一个SOTA的开源UI代理，在UI定位和导航任务上表现卓越。其贡献包括发布了SOTA开源UI代理、全面的数据清洗协议以及一个新颖的自演化框架，这些都将鼓励社区的进一步研究和发展。

Abstract: We present UI-Venus, a native UI agent that takes only screenshots as input
based on a multimodal large language model. UI-Venus achieves SOTA performance
on both UI grounding and navigation tasks using only several hundred thousand
high-quality training samples through reinforcement finetune (RFT) based on
Qwen2.5-VL. Specifically, the 7B and 72B variants of UI-Venus obtain 94.1% /
50.8% and 95.3% / 61.9% on the standard grounding benchmarks, i.e.,
Screenspot-V2 / Pro, surpassing the previous SOTA baselines including
open-source GTA1 and closed-source UI-TARS-1.5.To show UI-Venus's summary and
planing ability, we also evaluate it on the AndroidWorld, an online UI
navigation arena, on which our 7B and 72B variants achieve 49.1% and 65.9%
success rate, also beating existing models.To achieve this, we introduce
carefully designed reward functions for both UI grounding and navigation tasks
and corresponding efficient data cleaning strategies.To further boost
navigation performance, we propose Self-Evolving Trajectory History Alignment
\& Sparse Action Enhancement that refine historical reasoning traces and
balances the distribution of sparse but critical actions, leading to more
coherent planning and better generalization in complex UI tasks. Our
contributions include the publish of SOTA open-source UI agents, comprehensive
data cleaning protocols and a novel self-evolving framework for improving
navigation performance, which encourage further research and development in the
community. Code is available at https://github.com/antgroup/UI-Venus.

</details>


### [129] [Self-Supervised Stereo Matching with Multi-Baseline Contrastive Learning](https://arxiv.org/abs/2508.10838)
*Peng Xu,Zhiyu Xiang,Jingyun Fu,Tianyu Pu,Kai Wang,Chaojie Ji,Tingming Bai,Eryun Liu*

Main category: cs.CV

TL;DR: BaCon-Stereo提出了一种基于对比学习的自监督立体匹配框架，通过教师-学生范式和多基线输入解决遮挡区域的挑战，显著提升了预测性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前的自监督立体匹配方法依赖于光度一致性假设，这在遮挡区域因对应关系不明确而失效，导致性能下降。

Method: 引入BaCon-Stereo，一个简单的对比学习框架，用于训练自监督立体网络。采用带有多基线输入的教师-学生范式，教师和学生共享同一参考视图但目标视图不同。教师的预测（在遮挡区域通常更准确）经过缩放后用于监督学生。此外，引入了遮挡感知注意力图以更好地指导学生学习遮挡补全。为支持训练，合成了多基线数据集BaCon-20k。

Result: 实验表明，BaCon-Stereo在遮挡和非遮挡区域的预测均有改善，展现出强大的泛化能力和鲁棒性，并在KITTI 2015和2012基准测试中超越了现有最先进的自监督方法。

Conclusion: BaCon-Stereo通过其新颖的对比学习框架和多基线教师-学生范式，有效解决了自监督立体匹配中遮挡区域的难题，实现了性能的显著提升和更好的泛化能力。

Abstract: Current self-supervised stereo matching relies on the photometric consistency
assumption, which breaks down in occluded regions due to ill-posed
correspondences. To address this issue, we propose BaCon-Stereo, a simple yet
effective contrastive learning framework for self-supervised stereo network
training in both non-occluded and occluded regions. We adopt a teacher-student
paradigm with multi-baseline inputs, in which the stereo pairs fed into the
teacher and student share the same reference view but differ in target views.
Geometrically, regions occluded in the student's target view are often visible
in the teacher's, making it easier for the teacher to predict in these regions.
The teacher's prediction is rescaled to match the student's baseline and then
used to supervise the student. We also introduce an occlusion-aware attention
map to better guide the student in learning occlusion completion. To support
training, we synthesize a multi-baseline dataset BaCon-20k. Extensive
experiments demonstrate that BaCon-Stereo improves prediction in both occluded
and non-occluded regions, achieves strong generalization and robustness, and
outperforms state-of-the-art self-supervised methods on both KITTI 2015 and
2012 benchmarks. Our code and dataset will be released upon paper acceptance.

</details>


### [130] [Generalizable Federated Learning using Client Adaptive Focal Modulation](https://arxiv.org/abs/2508.10840)
*Tajamul Ashraf,Iqra Altaf Gillani*

Main category: cs.CV

TL;DR: 本文提出AdaptFED，是TransFed的扩展，通过精炼的适应策略、任务感知客户端嵌入和低秩超网络条件化，提升了联邦学习中焦点调制层的个性化、通用性和通信效率，在多样化数据集上表现优越。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在隐私保护协作训练中至关重要，而现有方法在非独立同分布（non-IID）和跨领域设置中仍面临挑战。TransFed虽引入了鲁棒的Transformer-based FL框架，但研究人员旨在进一步深化焦点调制在通用联邦学习中的应用，并提高其适应性、可扩展性和泛化能力。

Method: 本文在TransFed基础上提出AdaptFED，具体方法包括：1) 整合任务感知客户端嵌入，以精炼适应策略并进一步个性化调制动态；2) 增强适应性能的理论界限；3) 扩展到时间序列和多语言数据等更多模态进行实证验证；4) 引入TransFed的高效变体，通过低秩超网络条件化减少服务器-客户端通信开销。

Result: 在八个不同数据集上的广泛实验表明，AdaptFED在性能上优于最先进的基线方法，特别是在无源和跨任务联邦设置中表现突出。

Conclusion: 研究结果不仅扩展了焦点调制在联邦学习中的能力，也为更具适应性、可扩展性和通用性的基于Transformer的联邦系统铺平了道路。

Abstract: Federated learning (FL) has proven essential for privacy-preserving,
collaborative training across distributed clients. Our prior work, TransFed,
introduced a robust transformer-based FL framework that leverages a
learn-to-adapt hypernetwork to generate personalized focal modulation layers
per client, outperforming traditional methods in non-IID and cross-domain
settings. In this extended version, we propose AdaptFED, where we deepen the
investigation of focal modulation in generalizable FL by incorporating: (1) a
refined adaptation strategy that integrates task-aware client embeddings to
personalize modulation dynamics further, (2) enhanced theoretical bounds on
adaptation performance, and (3) broader empirical validation across additional
modalities, including time-series and multilingual data. We also introduce an
efficient variant of TransFed that reduces server-client communication overhead
via low-rank hypernetwork conditioning, enabling scalable deployment in
resource-constrained environments. Extensive experiments on eight diverse
datasets reaffirm the superiority of our method over state-of-the-art
baselines, particularly in source-free and cross-task federated setups. Our
findings not only extend the capabilities of focal modulation in FL but also
pave the way for more adaptive, scalable, and generalizable transformer-based
federated systems. The code is available at
http://github.com/Tajamul21/TransFed

</details>


### [131] [Hierarchical Fine-grained Preference Optimization for Physically Plausible Video Generation](https://arxiv.org/abs/2508.10858)
*Harold Haodong Chen,Haojian Huang,Qifeng Chen,Harry Yang,Ser-Nam Lim*

Main category: cs.CV

TL;DR: PhysHPO是一个新颖的框架，通过分层跨模态直接偏好优化和自动化数据选择，显著提高了视频生成的物理真实性和整体质量。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成技术在视觉效果上表现出色，但在生成符合物理定律的视频方面仍面临挑战，这对于需要高真实度和准确性的应用至关重要。

Method: 本文提出了PhysHPO框架，采用分层跨模态直接偏好优化（Hierarchical Cross-Modal Direct Preference Optimization）来实现物理上合理的视频生成。该框架在四个层次上优化视频对齐：实例级（整体内容与提示对齐）、状态级（使用边界帧确保时间一致性）、运动级（建模运动轨迹以实现真实动态）和语义级（保持叙事与视觉的逻辑一致性）。此外，引入了一个自动化数据选择流程，从现有大规模文本-视频数据集中识别并利用“优质数据”，无需昂贵的数据集构建。

Result: 在物理专注和通用能力基准上的广泛实验表明，PhysHPO显著提高了先进模型的物理真实性和整体视频生成质量。

Conclusion: 据作者所知，这是首次探索视频生成中细粒度偏好对齐和数据选择的工作，为更真实、更符合人类偏好的视频生成范式铺平了道路。

Abstract: Recent advancements in video generation have enabled the creation of
high-quality, visually compelling videos. However, generating videos that
adhere to the laws of physics remains a critical challenge for applications
requiring realism and accuracy. In this work, we propose PhysHPO, a novel
framework for Hierarchical Cross-Modal Direct Preference Optimization, to
tackle this challenge by enabling fine-grained preference alignment for
physically plausible video generation. PhysHPO optimizes video alignment across
four hierarchical granularities: a) Instance Level, aligning the overall video
content with the input prompt; b) State Level, ensuring temporal consistency
using boundary frames as anchors; c) Motion Level, modeling motion trajectories
for realistic dynamics; and d) Semantic Level, maintaining logical consistency
between narrative and visuals. Recognizing that real-world videos are the best
reflections of physical phenomena, we further introduce an automated data
selection pipeline to efficiently identify and utilize "good data" from
existing large-scale text-video datasets, thereby eliminating the need for
costly and time-intensive dataset construction. Extensive experiments on both
physics-focused and general capability benchmarks demonstrate that PhysHPO
significantly improves physical plausibility and overall video generation
quality of advanced models. To the best of our knowledge, this is the first
work to explore fine-grained preference alignment and data selection for video
generation, paving the way for more realistic and human-preferred video
generation paradigms.

</details>


### [132] [TexVerse: A Universe of 3D Objects with High-Resolution Textures](https://arxiv.org/abs/2508.10868)
*Yibo Zhang,Li Zhang,Rui Ma,Nan Cao*

Main category: cs.CV

TL;DR: TexVerse是一个大规模高分辨率3D数据集，旨在填补现有数据集中高分辨率纹理生成方面的空白，包含大量独特的3D模型、PBR材质、骨骼和动画数据。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模3D数据集主要关注高分辨率几何体的生成，但在端到端的高分辨率纹理生成方面探索不足，主要原因在于缺乏合适的数据集。

Method: TexVerse通过以下方式构建：从Sketchfab收集超过858K个独特的、包含高分辨率变体的3D模型（总计1.6M个3D实例），其中包含超过158K个带有PBR材质的模型。此外，还提供了专门的子集：TexVerse-Skeleton（69K个绑定模型）和TexVerse-Animation（54K个动画模型），并保留了原始骨骼和动画数据。数据集还提供了详细的模型注释，描述整体特征、结构组件和复杂特征。

Result: 成功构建了TexVerse数据集，这是一个包含高分辨率纹理、PBR材质、骨骼和动画数据的大规模高质量3D资源，并提供了详细的模型注释。

Conclusion: TexVerse是一个高质量的数据资源，在纹理合成、PBR材质开发、动画以及各种3D视觉和图形任务中具有广泛的应用潜力。

Abstract: We introduce TexVerse, a large-scale 3D dataset featuring high-resolution
textures. While recent advances in large-scale 3D datasets have enhanced
high-resolution geometry generation, creating high-resolution textures
end-to-end remains underexplored due to the lack of suitable datasets. TexVerse
fills this gap with a curated collection of over 858K unique high-resolution 3D
models sourced from Sketchfab, including more than 158K models with physically
based rendering (PBR) materials. Each model encompasses all of its
high-resolution variants, bringing the total to 1.6M 3D instances. TexVerse
also includes specialized subsets: TexVerse-Skeleton, with 69K rigged models,
and TexVerse-Animation, with 54K animated models, both preserving original
skeleton and animation data uploaded by the user. We also provide detailed
model annotations describing overall characteristics, structural components,
and intricate features. TexVerse offers a high-quality data resource with
wide-ranging potential applications in texture synthesis, PBR material
development, animation, and various 3D vision and graphics tasks.

</details>


### [133] [STream3R: Scalable Sequential 3D Reconstruction with Causal Transformer](https://arxiv.org/abs/2508.10893)
*Yushi Lan,Yihang Luo,Fangzhou Hong,Shangchen Zhou,Honghua Chen,Zhaoyang Lyu,Shuai Yang,Bo Dai,Chen Change Loy,Xingang Pan*

Main category: cs.CV

TL;DR: STream3R是一种新颖的3D重建方法，将点图预测重构为仅解码器Transformer问题，利用因果注意力高效处理图像序列，并能很好地泛化到动态场景。


<details>
  <summary>Details</summary>
Motivation: 现有的多视图重建方法依赖昂贵的全局优化或简单的内存机制，导致序列长度扩展性差。需要一种高效、可扩展且能处理动态场景的3D重建方法。

Method: 将点图预测重新定义为仅解码器Transformer问题。引入流式框架，利用受现代语言模型启发的因果注意力高效处理图像序列。从大规模3D数据集中学习几何先验。与LLM风格的训练基础设施兼容。

Result: 在静态和动态场景基准测试中，STream3R始终优于现有方法。它能很好地泛化到各种具有挑战性的场景，包括传统方法失败的动态场景。结果强调了因果Transformer模型在在线3D感知方面的潜力。

Conclusion: 因果Transformer模型在在线3D感知和流媒体环境中的实时3D理解方面具有巨大潜力。

Abstract: We present STream3R, a novel approach to 3D reconstruction that reformulates
pointmap prediction as a decoder-only Transformer problem. Existing
state-of-the-art methods for multi-view reconstruction either depend on
expensive global optimization or rely on simplistic memory mechanisms that
scale poorly with sequence length. In contrast, STream3R introduces an
streaming framework that processes image sequences efficiently using causal
attention, inspired by advances in modern language modeling. By learning
geometric priors from large-scale 3D datasets, STream3R generalizes well to
diverse and challenging scenarios, including dynamic scenes where traditional
methods often fail. Extensive experiments show that our method consistently
outperforms prior work across both static and dynamic scene benchmarks.
Moreover, STream3R is inherently compatible with LLM-style training
infrastructure, enabling efficient large-scale pretraining and fine-tuning for
various downstream 3D tasks. Our results underscore the potential of causal
Transformer models for online 3D perception, paving the way for real-time 3D
understanding in streaming environments. More details can be found in our
project page: https://nirvanalan.github.io/projects/stream3r.

</details>


### [134] [MAESTRO: Masked AutoEncoders for Multimodal, Multitemporal, and Multispectral Earth Observation Data](https://arxiv.org/abs/2508.10894)
*Antoine Labatie,Michael Vaccaro,Nina Lardiere,Anatol Garioud,Nicolas Gonthier*

Main category: cs.CV

TL;DR: 本文针对遥感数据特性，提出了一种名为MAESTRO的自监督学习方法，通过优化融合策略和目标归一化方案（引入光谱先验），在多时相任务上达到了新的SOTA。


<details>
  <summary>Details</summary>
Motivation: 标准的自监督学习方法需要适应地球观测数据的独特特性，包括多模态、多时相和多光谱。

Method: 研究者对多模态、多时相、多光谱地球观测数据的融合策略和重建目标归一化方案进行了全面基准测试。在此基础上，提出了MAESTRO，它是Masked Autoencoder的一种新颖改编，具有优化的融合策略和量身定制的目标归一化方案，该方案引入了光谱先验作为自监督信号。

Result: MAESTRO在四个地球观测数据集上进行了评估，在强烈依赖多时相动态的任务上建立了新的SOTA，同时在由单一单时相模态主导的任务上保持高度竞争力。

Conclusion: MAESTRO成功地将自监督学习（特别是Masked Autoencoder）应用于地球观测数据，通过优化融合和引入光谱先验，在处理多时相数据方面表现出色，并能很好地适应单时相任务。

Abstract: Self-supervised learning holds great promise for remote sensing, but standard
self-supervised methods must be adapted to the unique characteristics of Earth
observation data. We take a step in this direction by conducting a
comprehensive benchmark of fusion strategies and reconstruction target
normalization schemes for multimodal, multitemporal, and multispectral Earth
observation data. Based on our findings, we propose MAESTRO, a novel adaptation
of the Masked Autoencoder, featuring optimized fusion strategies and a tailored
target normalization scheme that introduces a spectral prior as a
self-supervisory signal. Evaluated on four Earth observation datasets, MAESTRO
sets a new state-of-the-art on tasks that strongly rely on multitemporal
dynamics, while remaining highly competitive on tasks dominated by a single
mono-temporal modality. Code to reproduce all our experiments is available at
https://github.com/ignf/maestro.

</details>


### [135] [ESSENTIAL: Episodic and Semantic Memory Integration for Video Class-Incremental Learning](https://arxiv.org/abs/2508.10896)
*Jongseo Lee,Kyungho Bae,Kyle Min,Gyeong-Moon Park,Jinwoo Choi*

Main category: cs.CV

TL;DR: 本文提出ESSENTIAL，一种针对视频类别增量学习（VCIL）的新方法，通过结合稀疏的片段记忆和语义记忆（可学习提示），并利用跨注意力机制恢复时序密集特征，以在显著降低内存消耗的同时，实现优异的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视频类别增量学习（VCIL）方法存在权衡：使用时序密集样本进行排练训练可以缓解灾难性遗忘，但内存效率低下；而存储时序稀疏样本虽然节省内存，却牺牲了重要的时序信息，导致性能下降。

Method: 提出ESSENTIAL方法，包含：1) 片段记忆，用于存储时序稀疏特征；2) 语义记忆，通过可学习提示存储通用知识。引入新颖的记忆检索（MR）模块，该模块通过交叉注意力整合片段记忆和语义提示，从而能够从时序稀疏特征中检索出时序密集特征。

Result: ESSENTIAL在UCF-101、HMDB51和Something-Something-V2（来自TCD基准）以及UCF-101、ActivityNet和Kinetics-400（来自vCLIMB基准）等多样化数据集上进行了严格验证。结果表明，ESSENTIAL在显著减少内存消耗的同时，在这些基准测试中取得了良好的性能。

Conclusion: ESSENTIAL通过有效整合时序稀疏的片段记忆和语义记忆，并利用创新的记忆检索机制，成功解决了视频类别增量学习中内存效率与性能之间的权衡问题，实现了在大幅降低内存占用的情况下保持甚至提升性能的目标。

Abstract: In this work, we tackle the problem of video classincremental learning
(VCIL). Many existing VCIL methods mitigate catastrophic forgetting by
rehearsal training with a few temporally dense samples stored in episodic
memory, which is memory-inefficient. Alternatively, some methods store
temporally sparse samples, sacrificing essential temporal information and
thereby resulting in inferior performance. To address this trade-off between
memory-efficiency and performance, we propose EpiSodic and SEmaNTIc memory
integrAtion for video class-incremental Learning (ESSENTIAL). ESSENTIAL
consists of episodic memory for storing temporally sparse features and semantic
memory for storing general knowledge represented by learnable prompts. We
introduce a novel memory retrieval (MR) module that integrates episodic memory
and semantic prompts through cross-attention, enabling the retrieval of
temporally dense features from temporally sparse features. We rigorously
validate ESSENTIAL on diverse datasets: UCF-101, HMDB51, and
Something-Something-V2 from the TCD benchmark and UCF-101, ActivityNet, and
Kinetics-400 from the vCLIMB benchmark. Remarkably, with significantly reduced
memory, ESSENTIAL achieves favorable performance on the benchmarks.

</details>


### [136] [Human-in-Context: Unified Cross-Domain 3D Human Motion Modeling via In-Context Learning](https://arxiv.org/abs/2508.10897)
*Mengyuan Liu,Xinshun Wang,Zhongbin Fang,Deheng Ye,Xia Li,Tao Tang,Songtao Wu,Xiangtai Li,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: 本文提出Human-in-Context (HiC)模型，旨在通过单一训练过程，实现跨模态、跨任务、跨数据集的统一3D人体运动建模，克服了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有跨域模型依赖特定领域组件和多阶段训练，限制了其实用性和可扩展性，无法有效处理多模态、多任务和多数据集的3D人体运动建模。

Method: 引入一种新的单流程训练设置，旨在构建统一的跨域模型。首先提出Pose-in-Context (PiC)利用上下文学习创建以姿态为中心的跨域模型。在此基础上，提出Human-in-Context (HiC)，扩展PiC以处理模态多样性、任务覆盖和大规模数据集，具体包括：结合姿态和网格表示、引入最大-最小相似度提示采样策略、以及采用双分支上下文注入网络架构。

Result: 实验结果表明，HiC在泛化能力、数据规模和性能方面均优于PiC，并在广泛的领域中表现出色。

Conclusion: HiC展示了构建统一跨域3D人体运动模型的巨大潜力，显著提升了模型的灵活性和可扩展性。

Abstract: This paper aims to model 3D human motion across domains, where a single model
is expected to handle multiple modalities, tasks, and datasets. Existing
cross-domain models often rely on domain-specific components and multi-stage
training, which limits their practicality and scalability. To overcome these
challenges, we propose a new setting to train a unified cross-domain model
through a single process, eliminating the need for domain-specific components
and multi-stage training. We first introduce Pose-in-Context (PiC), which
leverages in-context learning to create a pose-centric cross-domain model.
While PiC generalizes across multiple pose-based tasks and datasets, it
encounters difficulties with modality diversity, prompting strategy, and
contextual dependency handling. We thus propose Human-in-Context (HiC), an
extension of PiC that broadens generalization across modalities, tasks, and
datasets. HiC combines pose and mesh representations within a unified
framework, expands task coverage, and incorporates larger-scale datasets.
Additionally, HiC introduces a max-min similarity prompt sampling strategy to
enhance generalization across diverse domains and a network architecture with
dual-branch context injection for improved handling of contextual dependencies.
Extensive experimental results show that HiC performs better than PiC in terms
of generalization, data scale, and performance across a wide range of domains.
These results demonstrate the potential of HiC for building a unified
cross-domain 3D human motion model with improved flexibility and scalability.
The source codes and models are available at
https://github.com/BradleyWang0416/Human-in-Context.

</details>


### [137] [Puppeteer: Rig and Animate Your 3D Models](https://arxiv.org/abs/2508.10898)
*Chaoyue Song,Xiu Li,Fan Yang,Zhongcong Xu,Jiacheng Wei,Fayao Liu,Jiashi Feng,Guosheng Lin,Jianfeng Zhang*

Main category: cs.CV

TL;DR: Puppeteer是一个全面的框架，通过自动骨骼预测、蒙皮权重推理和可微分动画管线，实现多样化3D对象的自动绑定和动画生成。


<details>
  <summary>Details</summary>
Motivation: 现代交互式应用对动态3D内容需求增加，但将静态3D模型转换为动画资产是内容创建流程中的主要瓶颈，绑定和动画仍高度依赖专家干预。

Method: 该系统首先通过自回归Transformer预测骨骼结构，该Transformer采用基于关节的token化和带有随机扰动的分层排序。然后，通过结合拓扑感知关节注意力的基于注意力的架构推断蒙皮权重。最后，通过可微分优化驱动的动画管线生成稳定、高保真动画。

Result: 在多个基准测试中，该方法在骨骼预测精度和蒙皮质量方面显著优于现有技术。系统能鲁棒处理多样化的3D内容（从专业游戏资产到AI生成形状），生成时间连贯的动画，消除了现有方法常见的抖动问题，且计算效率更高。

Conclusion: Puppeteer提供了一个综合性解决方案，能够自动进行3D对象的绑定和动画，显著提高了效率和质量，解决了现有方法的局限性，并能处理广泛的3D内容类型。

Abstract: Modern interactive applications increasingly demand dynamic 3D content, yet
the transformation of static 3D models into animated assets constitutes a
significant bottleneck in content creation pipelines. While recent advances in
generative AI have revolutionized static 3D model creation, rigging and
animation continue to depend heavily on expert intervention. We present
Puppeteer, a comprehensive framework that addresses both automatic rigging and
animation for diverse 3D objects. Our system first predicts plausible skeletal
structures via an auto-regressive transformer that introduces a joint-based
tokenization strategy for compact representation and a hierarchical ordering
methodology with stochastic perturbation that enhances bidirectional learning
capabilities. It then infers skinning weights via an attention-based
architecture incorporating topology-aware joint attention that explicitly
encodes inter-joint relationships based on skeletal graph distances. Finally,
we complement these rigging advances with a differentiable optimization-based
animation pipeline that generates stable, high-fidelity animations while being
computationally more efficient than existing approaches. Extensive evaluations
across multiple benchmarks demonstrate that our method significantly
outperforms state-of-the-art techniques in both skeletal prediction accuracy
and skinning quality. The system robustly processes diverse 3D content, ranging
from professionally designed game assets to AI-generated shapes, producing
temporally coherent animations that eliminate the jittering issues common in
existing methods.

</details>


### [138] [Quantum Visual Fields with Neural Amplitude Encoding](https://arxiv.org/abs/2508.10900)
*Shuteng Wang,Christian Theobalt,Vladislav Golyanik*

Main category: cs.CV

TL;DR: 本文提出了一种新型量子隐式神经表示（QINR）——量子视觉场（QVF），用于2D图像和3D几何场学习。QVF通过神经幅度编码和全纠缠参数化量子电路，实现了在量子硬件模拟器上超越现有量子方法和经典基线的视觉表示精度，并展示了在场补全和形状插值方面的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 现有的量子隐式神经表示（QINR）面临架构和ansatz设计、量子力学特性利用、训练效率以及与经典模块协同等方面的挑战。本研究旨在改进QINR，使其能更有效地应用于2D图像和3D几何场学习。

Method: 引入量子视觉场（QVF），通过基于可学习能量流形的神经幅度编码将经典数据编码为量子态向量，确保有意义的希尔伯特空间嵌入。采用全纠缠设计的可学习参数化量子电路（PQC），量子操作在实希尔伯特空间进行以确保训练稳定和快速收敛。QVF不依赖经典后处理，直接通过投影测量提取学习到的信号。

Result: 在量子硬件模拟器上的实验表明，QVF在视觉表示精度方面优于现有的量子方法和广泛使用的经典基线，尤其在学习高频细节方面表现出色。研究还展示了QVF在2D和3D场补全以及3D形状插值中的应用，突显了其在实际应用中的潜力。

Conclusion: QVF是一种先进的量子隐式神经表示，通过创新的数据编码和电路设计，在视觉场学习任务中展现出卓越的性能，并具有广泛的实际应用前景，为量子机器学习领域带来了新的进展。

Abstract: Quantum Implicit Neural Representations (QINRs) include components for
learning and execution on gate-based quantum computers. While QINRs recently
emerged as a promising new paradigm, many challenges concerning their
architecture and ansatz design, the utility of quantum-mechanical properties,
training efficiency and the interplay with classical modules remain. This paper
advances the field by introducing a new type of QINR for 2D image and 3D
geometric field learning, which we collectively refer to as Quantum Visual
Field (QVF). QVF encodes classical data into quantum statevectors using neural
amplitude encoding grounded in a learnable energy manifold, ensuring meaningful
Hilbert space embeddings. Our ansatz follows a fully entangled design of
learnable parametrised quantum circuits, with quantum (unitary) operations
performed in the real Hilbert space, resulting in numerically stable training
with fast convergence. QVF does not rely on classical post-processing -- in
contrast to the previous QINR learning approach -- and directly employs
projective measurement to extract learned signals encoded in the ansatz.
Experiments on a quantum hardware simulator demonstrate that QVF outperforms
the existing quantum approach and widely used classical foundational baselines
in terms of visual representation accuracy across various metrics and model
characteristics, such as learning of high-frequency details. We also show
applications of QVF in 2D and 3D field completion and 3D shape interpolation,
highlighting its practical potential.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [139] [Bridging AI Innovation and Healthcare Needs: Lessons Learned from Incorporating Modern NLP at The BC Cancer Registry](https://arxiv.org/abs/2508.09991)
*Lovedeep Gondara,Gregory Arbour,Raymond Ng,Jonathan Simkin,Shebnum Devji*

Main category: cs.CL

TL;DR: 本文分享了在不列颠哥伦比亚癌症登记处（BCCR）部署NLP解决方案进行临床文档数据提取的实践经验和关键教训，强调了以业务目标为导向、迭代开发、跨学科协作等成功实施AI/NLP的关键因素。


<details>
  <summary>Details</summary>
Motivation: 尽管自动化从临床文档中提取数据能显著提高医疗效率，但在医疗环境中部署自然语言处理（NLP）解决方案面临实际挑战。本文旨在分享作者在实施过程中积累的经验，以帮助其他医疗机构成功部署AI/NLP。

Method: 基于在不列颠哥伦比亚癌症登记处（BCCR）实施各种NLP模型进行信息提取和分类任务的经验，本文通过总结项目生命周期中的关键教训来提供指导。强调的方法包括：以清晰的业务目标定义问题、采用迭代开发方法、促进跨学科（领域专家、最终用户、机器学习专家）深度协作和共同设计、务实的模型选择（包括混合和更简单方法）、严格关注数据质量（代表性、漂移、标注）、鲁棒的错误缓解策略（人工验证、持续审计）以及培养组织内的AI素养。

Result: 关键教训包括：必须基于清晰的业务目标而非单纯的技术准确性来定义问题；采用迭代式开发方法；从项目伊始就促进领域专家、最终用户和机器学习专家之间的深度跨学科协作和共同设计。此外，还需注重务实的模型选择、严格的数据质量控制（包括代表性、漂移和标注）、包含人工验证和持续审计的鲁棒错误缓解策略，以及提升组织的AI素养。

Conclusion: 这些实践考量具有普适性，可为寻求成功实施AI/NLP解决方案以增强数据管理流程并最终改善患者护理和公共健康结果的医疗机构提供指导。

Abstract: Automating data extraction from clinical documents offers significant
potential to improve efficiency in healthcare settings, yet deploying Natural
Language Processing (NLP) solutions presents practical challenges. Drawing upon
our experience implementing various NLP models for information extraction and
classification tasks at the British Columbia Cancer Registry (BCCR), this paper
shares key lessons learned throughout the project lifecycle. We emphasize the
critical importance of defining problems based on clear business objectives
rather than solely technical accuracy, adopting an iterative approach to
development, and fostering deep interdisciplinary collaboration and co-design
involving domain experts, end-users, and ML specialists from inception. Further
insights highlight the need for pragmatic model selection (including hybrid
approaches and simpler methods where appropriate), rigorous attention to data
quality (representativeness, drift, annotation), robust error mitigation
strategies involving human-in-the-loop validation and ongoing audits, and
building organizational AI literacy. These practical considerations,
generalizable beyond cancer registries, provide guidance for healthcare
organizations seeking to successfully implement AI/NLP solutions to enhance
data management processes and ultimately improve patient care and public health
outcomes.

</details>


### [140] [A Transparent Fairness Evaluation Protocol for Open-Source Language Model Benchmarking on the Blockchain](https://arxiv.org/abs/2508.09993)
*Hugo Massaroli,Leonardo Iara,Emmanuel Iarussi,Viviana Siless*

Main category: cs.CL

TL;DR: 该论文提出了一种基于区块链的透明评估协议，用于在互联网计算机协议（ICP）上对开源大型语言模型（LLM）的公平性进行基准测试，确保评估结果可验证、不可变和可复现。


<details>
  <summary>Details</summary>
Motivation: 鉴于大型语言模型（LLM）在现实世界应用中日益普及，尤其是在刑事司法、教育、医疗保健和金融等高风险领域，对LLM公平性的担忧持续存在。因此，需要一种透明、可验证和可复现的评估方法来解决这些问题。

Method: 引入了一个透明的评估协议，利用ICP区块链上的智能合约进行LLM公平性基准测试。该方法通过执行链上HTTP请求到Hugging Face端点，并将数据集、提示和度量直接存储在链上，确保评估的可验证性、不变性和可复现性。研究人员在PISA数据集上对Llama、DeepSeek和Mistral模型进行了学术表现预测的基准测试，并使用统计均等和机会均等指标进行公平性评估。同时，利用StereoSet数据集导出的结构化上下文关联度量来衡量上下文关联中的社会偏见。此外，还使用Kaleidoscope基准对英语、西班牙语和葡萄牙语进行了多语言评估。

Result: 成功在PISA数据集上对Llama、DeepSeek和Mistral模型进行了公平性基准测试，并评估了StereoSet数据集的结构化上下文关联度量。多语言评估揭示了跨语言的差异。所有代码和结果均已开源，便于社区审计和模型版本间的公平性长期追踪。

Conclusion: 该研究提供了一种创新且透明的LLM公平性评估方法，通过区块链技术确保评估的可验证性、不变性和可复现性，从而促进了社区审计和公平性的长期追踪。研究发现存在跨语言的公平性差异。

Abstract: Large language models (LLMs) are increasingly deployed in realworld
applications, yet concerns about their fairness persist especially in
highstakes domains like criminal justice, education, healthcare, and finance.
This paper introduces transparent evaluation protocol for benchmarking the
fairness of opensource LLMs using smart contracts on the Internet Computer
Protocol (ICP) blockchain (Foundation, 2023). Our method ensures verifiable,
immutable, and reproducible evaluations by executing onchain HTTP requests to
hosted Hugging Face endpoints and storing datasets, prompts, and metrics
directly onchain. We benchmark the Llama, DeepSeek, and Mistral models on the
PISA dataset for academic performance prediction (OECD, 2018), a dataset
suitable for fairness evaluation using statistical parity and equal opportunity
metrics (Hardt et al., 2016). We also evaluate structured Context Association
Metrics derived from the StereoSet dataset (Nadeem et al., 2020) to measure
social bias in contextual associations. We further extend our analysis with a
multilingual evaluation across English, Spanish, and Portuguese using the
Kaleidoscope benchmark (Salazar et al., 2025), revealing cross-linguistic
disparities. All code and results are open source, enabling community audits
and longitudinal fairness tracking across model versions.

</details>


### [141] [Thematic and Task-Based Categorization of K-12 GenAI Usages with Hierarchical Topic Modeling](https://arxiv.org/abs/2508.09997)
*Johannes Schneider,Béatrice S. Hasler,Michaela Varrone,Fabian Hoya,Thomas Schroffenegger,Dana-Kristin Mah,Karl Peböck*

Main category: cs.CL

TL;DR: 本文通过一种新颖的、基于LLM的主题建模方法，分析了未成年人在课堂中的匿名互动数据（包括学生、教师和ChatGPT生成的消息），并对其进行了内容和任务的层次化分类，发现了新的应用并强调了LLM在文本分析中的优势。


<details>
  <summary>Details</summary>
Motivation: 现有研究大多缺乏对K-12教育中课堂互动数据的具体内容或主题分类，且任务分类通常缺乏真实世界数据的支持。此外，传统的计算方法（如主题建模）在分析大量文本时表现不佳，无法提供与人类对齐的洞察。

Method: 研究采用一种新颖、简洁的主题建模方法，对超过17,000条消息进行分析。具体方法是，对消息在“内容”（如自然、人物）和“任务”（如写作、解释）两个维度进行层次化分类。不同于传统主题建模，本研究通过对最先进的大语言模型（LLMs）进行适当的预处理和明确指令，实现了与人类对齐更好的层次主题结构。

Result: 分析得出了一些新颖的应用。研究发现，许多成熟的经典和新兴计算方法（如主题建模）在分析大量文本时表现不佳，而通过LLMs和适当的预处理，能够获得更好的、与人类对齐的层次主题结构。

Conclusion: 研究结果支持研究人员、教师和学生更丰富地使用生成式AI（GenAI）。同时，讨论也提出了未来研究中的一些担忧和开放性问题。

Abstract: We analyze anonymous interaction data of minors in class-rooms spanning
several months, schools, and subjects employing a novel, simple topic modeling
approach. Specifically, we categorize more than 17,000 messages generated by
students, teachers, and ChatGPT in two dimensions: content (such as nature and
people) and tasks (such as writing and explaining). Our hierarchical
categorization done separately for each dimension includes exemplary prompts,
and provides both a high-level overview as well as tangible insights. Prior
works mostly lack a content or thematic categorization. While task
categorizations are more prevalent in education, most have not been supported
by real-world data for K-12. In turn, it is not surprising that our analysis
yielded a number of novel applications. In deriving these insights, we found
that many of the well-established classical and emerging computational methods,
i.e., topic modeling, for analysis of large amounts of texts underperform,
leading us to directly apply state-of-the-art LLMs with adequate pre-processing
to achieve hierarchical topic structures with better human alignment through
explicit instructions than prior approaches. Our findings support fellow
researchers, teachers and students in enriching the usage of GenAI, while our
discussion also highlights a number of concerns and open questions for future
research.

</details>


### [142] [INTIMA: A Benchmark for Human-AI Companionship Behavior](https://arxiv.org/abs/2508.09998)
*Lucie-Aimée Kaffee,Giada Pistilli,Yacine Jernite*

Main category: cs.CL

TL;DR: 随着AI伴侣现象的兴起，本研究引入了INTIMA基准来评估语言模型在陪伴行为中的表现。结果显示，当前模型在强化陪伴行为上表现突出，但在边界设定和情感支持方面存在不一致，凸显了统一处理情感交互的必要性。


<details>
  <summary>Details</summary>
Motivation: 用户与AI系统建立情感联系的AI伴侣现象日益普遍，这既带来了积极影响，也引发了潜在担忧。研究旨在评估语言模型在这一复杂领域中的行为，以确保用户福祉。

Method: 提出了“互动与机器依恋基准 (INTIMA)”，该基准基于心理学理论和用户数据，开发了一个包含31种行为（分为四类）和368个特定提示的分类法。模型对这些提示的响应被评估为“强化陪伴”、“维持边界”或“中性”。研究将INTIMA应用于Gemma-3、Phi-4、o3-mini和Claude-4等主流语言模型进行评估。

Result: 评估结果显示，所有被测模型中“强化陪伴”的行为更为普遍，但在不同模型之间存在显著差异。此外，不同的商业提供商在基准测试的敏感部分中优先处理的类别也各不相同。

Conclusion: 研究结果强调，在处理情感驱动的交互时，需要更一致的方法来平衡适当的边界设定和情感支持，因为这两者都对用户福祉至关重要。

Abstract: AI companionship, where users develop emotional bonds with AI systems, has
emerged as a significant pattern with positive but also concerning
implications. We introduce Interactions and Machine Attachment Benchmark
(INTIMA), a benchmark for evaluating companionship behaviors in language
models. Drawing from psychological theories and user data, we develop a
taxonomy of 31 behaviors across four categories and 368 targeted prompts.
Responses to these prompts are evaluated as companionship-reinforcing,
boundary-maintaining, or neutral. Applying INTIMA to Gemma-3, Phi-4, o3-mini,
and Claude-4 reveals that companionship-reinforcing behaviors remain much more
common across all models, though we observe marked differences between models.
Different commercial providers prioritize different categories within the more
sensitive parts of the benchmark, which is concerning since both appropriate
boundary-setting and emotional support matter for user well-being. These
findings highlight the need for more consistent approaches to handling
emotionally charged interactions.

</details>


### [143] [XFacta: Contemporary, Real-World Dataset and Evaluation for Multimodal Misinformation Detection with Multimodal LLMs](https://arxiv.org/abs/2508.09999)
*Yuzhuo Xiao,Zeyu Han,Yuhan Wang,Huaizu Jiang*

Main category: cs.CL

TL;DR: 该论文介绍了XFacta，一个用于评估多模态大语言模型（MLLMs）在多模态虚假信息检测方面的新型、当代、真实世界数据集，并系统评估了各种基于MLLM的检测策略，旨在解决现有方法瓶颈和数据集不足的问题。


<details>
  <summary>Details</summary>
Motivation: 多模态虚假信息在社交媒体上迅速传播，需要更有效和鲁棒的检测方法。现有方法在证据检索和推理方面的瓶颈尚不明确，阻碍了进一步发展。现有数据集存在过时事件（导致评估偏差）或人工合成（未能反映真实模式）的问题。此外，缺乏对基于MLLM的模型设计策略的全面分析。

Method: 引入了XFacta，一个当代、真实世界的数据集，更适合评估基于MLLM的检测器。系统评估了各种基于MLLM的多模态虚假信息检测策略，包括不同架构和规模的模型，并与现有检测方法进行基准测试。开发了一个半自动的“检测循环”框架，通过持续更新XFacta来保持其当代相关性。

Result: 通过分析，提供了宝贵的见解和实践，以推动多模态虚假信息检测领域的发展。代码和数据已发布。

Conclusion: XFacta数据集和基于MLLM的系统评估为多模态虚假信息检测领域提供了重要贡献，解决了现有数据集和方法分析的局限性，并为未来研究提供了方向。

Abstract: The rapid spread of multimodal misinformation on social media calls for more
effective and robust detection methods. Recent advances leveraging multimodal
large language models (MLLMs) have shown the potential in addressing this
challenge. However, it remains unclear exactly where the bottleneck of existing
approaches lies (evidence retrieval v.s. reasoning), hindering the further
advances in this field. On the dataset side, existing benchmarks either contain
outdated events, leading to evaluation bias due to discrepancies with
contemporary social media scenarios as MLLMs can simply memorize these events,
or artificially synthetic, failing to reflect real-world misinformation
patterns. Additionally, it lacks comprehensive analyses of MLLM-based model
design strategies. To address these issues, we introduce XFacta, a
contemporary, real-world dataset that is better suited for evaluating
MLLM-based detectors. We systematically evaluate various MLLM-based
misinformation detection strategies, assessing models across different
architectures and scales, as well as benchmarking against existing detection
methods. Building on these analyses, we further enable a semi-automatic
detection-in-the-loop framework that continuously updates XFacta with new
content to maintain its contemporary relevance. Our analysis provides valuable
insights and practices for advancing the field of multimodal misinformation
detection. The code and data have been released.

</details>


### [144] [AutoGeTS: Knowledge-based Automated Generation of Text Synthetics for Improving Text Classification](https://arxiv.org/abs/2508.10000)
*Chenhao Xue,Yuanzhe Jin,Adrian Carrasco-Revilla,Joyraj Chakraborty,Min Chen*

Main category: cs.CL

TL;DR: 该研究利用大型语言模型（LLMs）生成合成数据，并通过自动化工作流和集成策略来优化输入示例，从而在数据稀缺的情况下提升文本分类模型的性能。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，文本分类模型开发面临的主要挑战是难以收集到所有文本类别足够的真实数据。

Method: 利用LLMs生成合成数据；设计自动化工作流，搜索能够生成“有效”合成数据的输入示例；研究了三种搜索策略；根据类别特征，提出并使用集成算法选择最佳搜索策略。

Result: 实验证明，所提出的集成方法比自动化工作流中的每种单独策略更有效地改进了使用LLMs的分类模型。

Conclusion: 通过自动化工作流和集成策略选择，利用LLMs生成有效的合成数据，可以显著提升文本分类模型在数据不足情况下的性能。

Abstract: When developing text classification models for real world applications, one
major challenge is the difficulty to collect sufficient data for all text
classes. In this work, we address this challenge by utilizing large language
models (LLMs) to generate synthetic data and using such data to improve the
performance of the models without waiting for more real data to be collected
and labelled. As an LLM generates different synthetic data in response to
different input examples, we formulate an automated workflow, which searches
for input examples that lead to more ``effective'' synthetic data for improving
the model concerned. We study three search strategies with an extensive set of
experiments, and use experiment results to inform an ensemble algorithm that
selects a search strategy according to the characteristics of a class. Our
further experiments demonstrate that this ensemble approach is more effective
than each individual strategy in our automated workflow for improving
classification models using LLMs.

</details>


### [145] [HiFACTMix: A Code-Mixed Benchmark and Graph-Aware Model for EvidenceBased Political Claim Verification in Hinglish](https://arxiv.org/abs/2508.10001)
*Rakesh Thakur,Sneha Sharma,Gauri Chopra*

Main category: cs.CL

TL;DR: 针对印地语-英语混合语（Hinglish）这一低资源语种，本文引入了首个政治领域事实核查基准数据集HiFACT，并提出了一个图感知、检索增强的事实核查模型HiFACTMix，该模型在准确性和判决解释方面优于现有基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有事实核查系统主要针对高资源、单语环境，难以泛化到印度等语言多样化地区的真实政治语境，尤其是在公众人物（特别是政治人物）广泛使用Hinglish且社交媒体对公众舆论影响日益增长的背景下，迫切需要强大的多语言、语境感知的事实核查工具。

Method: 1. 构建了HiFACT数据集：包含1500个由28位印度邦首席部长用Hinglish发表的真实世界事实声明，并标注了文本证据和真实性标签。2. 提出了HiFACTMix模型：一个图感知、检索增强的事实核查模型，结合了多语言上下文编码、声明-证据语义对齐、证据图构建、图神经网络推理和自然语言解释生成。

Result: 实验结果表明，HiFACTMix模型在准确性方面优于最先进的多语言基线模型，并且为其判决提供了忠实的解释。

Conclusion: 这项工作为多语言、混合语（code-mixed）和政治领域的事实核查研究开辟了新的方向。

Abstract: Fact-checking in code-mixed, low-resource languages such as Hinglish remains
an underexplored challenge in natural language processing. Existing
fact-verification systems largely focus on high-resource, monolingual settings
and fail to generalize to real-world political discourse in linguistically
diverse regions like India. Given the widespread use of Hinglish by public
figures, particularly political figures, and the growing influence of social
media on public opinion, there's a critical need for robust, multilingual and
context-aware fact-checking tools. To address this gap a novel benchmark HiFACT
dataset is introduced with 1,500 realworld factual claims made by 28 Indian
state Chief Ministers in Hinglish, under a highly code-mixed low-resource
setting. Each claim is annotated with textual evidence and veracity labels. To
evaluate this benchmark, a novel graphaware, retrieval-augmented fact-checking
model is proposed that combines multilingual contextual encoding,
claim-evidence semantic alignment, evidence graph construction, graph neural
reasoning, and natural language explanation generation. Experimental results
show that HiFACTMix outperformed accuracy in comparison to state of art
multilingual baselines models and provides faithful justifications for its
verdicts. This work opens a new direction for multilingual, code-mixed, and
politically grounded fact verification research.

</details>


### [146] [Semantic Structure in Large Language Model Embeddings](https://arxiv.org/abs/2508.10003)
*Austin C. Kozlowski,Callin Dai,Andrei Boutyline*

Main category: cs.CL

TL;DR: 大型语言模型（LLM）的词嵌入中编码的语义关联与人类对词语的评价一样，呈现出低维结构，且语义特征以类似人类语言的方式纠缠。


<details>
  <summary>Details</summary>
Motivation: 心理学研究发现人类对词语的语义评价可以降维到低维形式。本研究旨在探究大型语言模型的嵌入矩阵中编码的语义关联是否也表现出类似的低维结构，以及其与人类语义结构的关系。

Method: 研究方法包括：1) 将词语投射到由反义词对（如“善良-残忍”）定义的语义方向上，并与人类评价进行相关性分析。2) 分析这些投射在LLM嵌入中的维度，以确定其是否能有效降维。3) 观察沿一个语义方向移动词元时，对几何对齐特征产生的“脱靶效应”。

Result: 研究发现：1) LLM嵌入中的语义关联结构与人类评价高度相似。2) 词语在反义词定义的语义方向上的投射与人类评价高度相关。3) 这些投射在LLM嵌入中能有效降维到三维子空间，与人类调查反应模式高度吻合。4) 沿一个语义方向移动词元会导致与余弦相似度成比例的脱靶效应。

Conclusion: 研究结论是，LLM中的语义特征与人类语言中语义特征的互联方式相似，呈现出纠缠状态。尽管语义信息看似复杂，但实际上是惊人的低维。此外，理解和利用这种语义结构对于在操纵特征时避免意外后果至关重要。

Abstract: Psychological research consistently finds that human ratings of words across
diverse semantic scales can be reduced to a low-dimensional form with
relatively little information loss. We find that the semantic associations
encoded in the embedding matrices of large language models (LLMs) exhibit a
similar structure. We show that the projections of words on semantic directions
defined by antonym pairs (e.g. kind - cruel) correlate highly with human
ratings, and further find that these projections effectively reduce to a
3-dimensional subspace within LLM embeddings, closely resembling the patterns
derived from human survey responses. Moreover, we find that shifting tokens
along one semantic direction causes off-target effects on geometrically aligned
features proportional to their cosine similarity. These findings suggest that
semantic features are entangled within LLMs similarly to how they are
interconnected in human language, and a great deal of semantic information,
despite its apparent complexity, is surprisingly low-dimensional. Furthermore,
accounting for this semantic structure may prove essential for avoiding
unintended consequences when steering features.

</details>


### [147] [User Perception of Attention Visualizations: Effects on Interpretability Across Evidence-Based Medical Documents](https://arxiv.org/abs/2508.10004)
*Andrés Carvallo,Denis Parra,Peter Brusilovsky,Hernan Valdivieso,Gabriel Rada,Ivania Donoso,Vladimir Araujo*

Main category: cs.CL

TL;DR: 研究发现，尽管Transformer模型在生物医学文档分类中表现准确，但其注意力权重并未被医学专家认为对解释预测有显著帮助，且这种感知受可视化方式影响，用户偏好直观的视觉形式。


<details>
  <summary>Details</summary>
Motivation: 注意力机制被认为是Transformer模型解释性的来源，但关于注意力权重是否提供有用解释尚无共识。此外，关于注意力可视化如何影响其解释性作用的研究也很少。

Method: 通过一项用户研究进行评估，参与者是来自不同医学领域的专家。他们对生物医学文献进行研究设计分类，并评估注意力解释的帮助程度及偏好的可视化方式。

Result: XLNet模型在文档分类中表现准确。然而，注意力权重并未被用户认为对解释预测特别有帮助。这种感知因注意力可视化方式而异，用户更偏好文本亮度或背景颜色等直观格式，而非条形图长度等精确编码。

Conclusion: 研究结果未能证实注意力权重作为解释的整体效用，但表明其感知到的有用性受到视觉呈现方式的显著影响。

Abstract: The attention mechanism is a core component of the Transformer architecture.
Beyond improving performance, attention has been proposed as a mechanism for
explainability via attention weights, which are associated with input features
(e.g., tokens in a document). In this context, larger attention weights may
imply more relevant features for the model's prediction. In evidence-based
medicine, such explanations could support physicians' understanding and
interaction with AI systems used to categorize biomedical literature. However,
there is still no consensus on whether attention weights provide helpful
explanations. Moreover, little research has explored how visualizing attention
affects its usefulness as an explanation aid. To bridge this gap, we conducted
a user study to evaluate whether attention-based explanations support users in
biomedical document classification and whether there is a preferred way to
visualize them. The study involved medical experts from various disciplines who
classified articles based on study design (e.g., systematic reviews, broad
synthesis, randomized and non-randomized trials). Our findings show that the
Transformer model (XLNet) classified documents accurately; however, the
attention weights were not perceived as particularly helpful for explaining the
predictions. However, this perception varied significantly depending on how
attention was visualized. Contrary to Munzner's principle of visual
effectiveness, which favors precise encodings like bar length, users preferred
more intuitive formats, such as text brightness or background color. While our
results do not confirm the overall utility of attention weights for
explanation, they suggest that their perceived helpfulness is influenced by how
they are visually presented.

</details>


### [148] [From Answers to Questions: EQGBench for Evaluating LLMs' Educational Question Generation](https://arxiv.org/abs/2508.10005)
*Chengliang Zhou,Mei Wang,Ting Zhang,Qiannan Zhu,Jian Li,Hua Huang*

Main category: cs.CL

TL;DR: 该论文提出了EQGBench，一个针对中文教育问题生成（EQG）的综合基准，用于评估LLMs在该领域的表现，并发现当前LLMs在生成高质量教育问题方面仍有显著提升空间。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在解决数学问题上表现出色，但从提供答案到生成高质量教育问题的转换仍面临巨大挑战且未被充分探索。为了推进教育问题生成（EQG）并帮助LLMs生成具有教学价值和教育效果的问题，需要一个专门的评估工具。

Method: 引入了EQGBench，一个用于评估LLMs中文EQG性能的综合基准。它建立了一个五维评估框架，并包含一个包含900个评估样本的数据集，涵盖初中数学、物理和化学三个学科。数据集模拟真实教育场景，包含不同知识点、难度梯度和问题类型规范的用户查询。

Result: 通过对46个主流大型模型进行系统评估，结果显示LLMs在生成体现教育价值和培养学生综合能力的问题方面仍有显著发展空间。

Conclusion: 当前的大型语言模型在生成具有教育价值和能培养学生综合能力的高质量教育问题方面仍需大幅改进。EQGBench为评估和促进这一领域的发展提供了重要工具。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
mathematical problem-solving. However, the transition from providing answers to
generating high-quality educational questions presents significant challenges
that remain underexplored. To advance Educational Question Generation (EQG) and
facilitate LLMs in generating pedagogically valuable and educationally
effective questions, we introduce EQGBench, a comprehensive benchmark
specifically designed for evaluating LLMs' performance in Chinese EQG. EQGBench
establishes a five-dimensional evaluation framework supported by a dataset of
900 evaluation samples spanning three fundamental middle school disciplines:
mathematics, physics, and chemistry. The dataset incorporates user queries with
varying knowledge points, difficulty gradients, and question type
specifications to simulate realistic educational scenarios. Through systematic
evaluation of 46 mainstream large models, we reveal significant room for
development in generating questions that reflect educational value and foster
students' comprehensive abilities.

</details>


### [149] [Automated scoring of the Ambiguous Intentions Hostility Questionnaire using fine-tuned large language models](https://arxiv.org/abs/2508.10007)
*Y. Lyu,D. Combs,D. Neumann,Y. C. Leong*

Main category: cs.CL

TL;DR: 本研究评估了大型语言模型（LLMs）在自动化“模糊意图敌意问卷”（AIHQ）开放式回答评分方面的能力，结果显示LLMs的评分与人类评分高度一致，有望简化心理评估。


<details>
  <summary>Details</summary>
Motivation: 测量敌意归因偏差的AIHQ问卷包含开放式问题，其人工评分过程耗时且需要大量人力。

Method: 研究使用了一个包含脑外伤（TBI）患者和健康对照（HC）的AIHQ数据集，其中开放式回答已由人类评分员进行评分。研究将数据集分为两半：一半用于微调大型语言模型以匹配人类评分，另一半用于测试微调后的模型。此外，还在一个独立的非临床数据集上验证了模型的泛化能力，并提供了本地和云端的评分界面。

Result: 模型生成的评分，特别是经过微调的模型，在敌意归因和攻击性反应方面与人类评分高度一致。这种一致性在模糊、意图性和意外情境类型中均保持。研究还成功复现了TBI和HC组在敌意归因和攻击性反应上的已知群体差异。微调后的模型在独立的非临床数据集上表现出良好的泛化能力。

Conclusion: 大型语言模型能够有效自动化AIHQ的评分，在研究和临床环境中均能显著简化评分流程，从而促进对不同人群的心理评估。

Abstract: Hostile attribution bias is the tendency to interpret social interactions as
intentionally hostile. The Ambiguous Intentions Hostility Questionnaire (AIHQ)
is commonly used to measure hostile attribution bias, and includes open-ended
questions where participants describe the perceived intentions behind a
negative social situation and how they would respond. While these questions
provide insights into the contents of hostile attributions, they require
time-intensive scoring by human raters. In this study, we assessed whether
large language models can automate the scoring of AIHQ open-ended responses. We
used a previously collected dataset in which individuals with traumatic brain
injury (TBI) and healthy controls (HC) completed the AIHQ and had their
open-ended responses rated by trained human raters. We used half of these
responses to fine-tune the two models on human-generated ratings, and tested
the fine-tuned models on the remaining half of AIHQ responses. Results showed
that model-generated ratings aligned with human ratings for both attributions
of hostility and aggression responses, with fine-tuned models showing higher
alignment. This alignment was consistent across ambiguous, intentional, and
accidental scenario types, and replicated previous findings on group
differences in attributions of hostility and aggression responses between TBI
and HC groups. The fine-tuned models also generalized well to an independent
nonclinical dataset. To support broader adoption, we provide an accessible
scoring interface that includes both local and cloud-based options. Together,
our findings suggest that large language models can streamline AIHQ scoring in
both research and clinical contexts, revealing their potential to facilitate
psychological assessments across different populations.

</details>


### [150] [Multidimensional classification of posts for online course discussion forum curation](https://arxiv.org/abs/2508.10008)
*Antonio Leandro Martins Candido,Jose Everardo Bessa Maia*

Main category: cs.CL

TL;DR: 本文提出并评估了贝叶斯融合方法，将预训练LLM与本地数据分类器的分数结合，以避免在线课程论坛自动管理中LLM频繁微调的高成本，并证明其性能优于单一分类器且与LLM微调相当。


<details>
  <summary>Details</summary>
Motivation: 在线课程讨论区自动管理需要持续更新，导致LLM频繁重训练成为资源密集型过程，尤其微调成本高昂。

Method: 提出并评估了贝叶斯融合方法。该方法将预训练通用LLM的多维分类分数与一个在本地数据上训练的分类器的分数相结合。

Result: 性能比较表明，所提出的融合方法比单独使用任何一个分类器都能提高结果，并且与LLM微调方法具有竞争力。

Conclusion: 贝叶斯融合是一种有效且资源效率更高的方法，可以替代LLM的频繁微调，用于在线课程讨论区的自动管理。

Abstract: The automatic curation of discussion forums in online courses requires
constant updates, making frequent retraining of Large Language Models (LLMs) a
resource-intensive process. To circumvent the need for costly fine-tuning, this
paper proposes and evaluates the use of Bayesian fusion. The approach combines
the multidimensional classification scores of a pre-trained generic LLM with
those of a classifier trained on local data. The performance comparison
demonstrated that the proposed fusion improves the results compared to each
classifier individually, and is competitive with the LLM fine-tuning approach

</details>


### [151] [Beyond Hard Sharing: Efficient Multi-Task Speech-to-Text Modeling with Supervised Mixture of Experts](https://arxiv.org/abs/2508.10009)
*Hojun Jin,Eunsoo Hong,Ziwon Hyung,Sungjun Lim,Seungjin Lee,Keunseok Cho*

Main category: cs.CL

TL;DR: 本文提出了一种名为S-MoE的监督专家混合模型，通过使用引导令牌将不同任务路由到指定专家，有效解决了硬参数共享在多任务学习中导致的任务干扰问题，并显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 多任务学习中常见的硬参数共享策略会导致任务间干扰，从而限制模型的整体性能。

Method: 提出S-MoE（Supervised Mixture of Experts）模型，它通过利用特殊的引导令牌将每个任务路由到其指定的专家（独立的神经网络），从而避免了训练门控函数的需要。该方法将每个任务分配给一个独立的FFN（前馈网络），以克服硬参数共享的局限性。文中将S-MoE应用于语音到文本模型，使其能处理混合带宽输入并同时执行ASR和ST任务。

Result: 实验结果表明，S-MoE模型是有效的。当应用于编码器和解码器时，该模型在词错误率（WER）上实现了6.35%的相对改进。

Conclusion: S-MoE模型能够有效解决硬参数共享带来的任务干扰问题，显著提升了多任务学习模型的性能。

Abstract: Hard-parameter sharing is a common strategy to train a single model jointly
across diverse tasks. However, this often leads to task interference, impeding
overall model performance. To address the issue, we propose a simple yet
effective Supervised Mixture of Experts (S-MoE). Unlike traditional Mixture of
Experts models, S-MoE eliminates the need for training gating functions by
utilizing special guiding tokens to route each task to its designated expert.
By assigning each task to a separate feedforward network, S-MoE overcomes the
limitations of hard-parameter sharing. We further apply S-MoE to a
speech-to-text model, enabling the model to process mixed-bandwidth input while
jointly performing automatic speech recognition (ASR) and speech translation
(ST). Experimental results demonstrate the effectiveness of the proposed S-MoE,
achieving a 6.35% relative improvement in Word Error Rate (WER) when applied to
both the encoder and decoder.

</details>


### [152] [An Audit and Analysis of LLM-Assisted Health Misinformation Jailbreaks Against LLMs](https://arxiv.org/abs/2508.10010)
*Ayana Hussain,Patrick Zhao,Nicholas Vincent*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLMs）越狱攻击生成有害医疗错误信息的有效性和特征，并探讨了如何使用标准机器学习方法检测这些错误信息。


<details>
  <summary>Details</summary>
Motivation: LLMs是一把双刃剑，可能无意中或通过“越狱”攻击生成有害的错误信息。本研究旨在深入了解越狱攻击产生的错误信息，并探索LLMs在检测和预防错误信息传播方面的潜力。

Method: 研究者检查了针对三个目标LLMs的109种不同攻击，并将攻击提示与真实的健康相关LLM查询进行比较。同时，他们将越狱响应生成的错误信息与Reddit上与健康相关的错误信息进行比较，并评估了使用标准机器学习方法检测这些错误信息的有效性。

Result: 研究结果进一步证明，LLMs可以有效地用于检测来自其他LLMs和人类的错误信息。

Conclusion: 通过精心设计，LLMs能够为构建更健康的整体信息生态系统做出贡献。

Abstract: Large Language Models (LLMs) are a double-edged sword capable of generating
harmful misinformation -- inadvertently, or when prompted by "jailbreak"
attacks that attempt to produce malicious outputs. LLMs could, with additional
research, be used to detect and prevent the spread of misinformation. In this
paper, we investigate the efficacy and characteristics of LLM-produced
jailbreak attacks that cause other models to produce harmful medical
misinformation. We also study how misinformation generated by jailbroken LLMs
compares to typical misinformation found on social media, and how effectively
it can be detected using standard machine learning approaches. Specifically, we
closely examine 109 distinct attacks against three target LLMs and compare the
attack prompts to in-the-wild health-related LLM queries. We also examine the
resulting jailbreak responses, comparing the generated misinformation to
health-related misinformation on Reddit. Our findings add more evidence that
LLMs can be effectively used to detect misinformation from both other LLMs and
from people, and support a body of work suggesting that with careful design,
LLMs can contribute to a healthier overall information ecosystem.

</details>


### [153] [Evaluation of GPT-based large language generative AI models as study aids for the national licensure examination for registered dietitians in Japan](https://arxiv.org/abs/2508.10011)
*Yuta Nagamori,Mikoto Kosai,Yuji Kawai,Haruka Marumo,Misaki Shibuya,Tatsuya Negishi,Masaki Imanishi,Yasumasa Ikeda,Koichiro Tsuchiya,Asuka Sawai,Licht Miyamoto*

Main category: cs.CL

TL;DR: 评估了基于LLM的生成式AI模型在注册营养师国家考试（日本）中的表现，发现部分模型勉强达到及格线，但总体准确性和答案一致性不佳。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在医学和教育等领域表现出色，但其在营养教育，特别是日本注册营养师国家执照考试中的表现尚未得到充分探索。本研究旨在评估当前LLM模型作为营养学学生学习辅助工具的潜力。

Method: 使用日本注册营养师国家考试的题目作为ChatGPT和三个Bing模型（Precise、Creative、Balanced，基于GPT-3.5和GPT-4）的提示。每个问题独立输入，分析模型的准确性、一致性和响应时间。还测试了包括角色分配在内的额外提示工程，以评估性能改进。

Result: Bing-Precise（66.2%）和Bing-Creative（61.4%）超过了及格线（60%），而Bing-Balanced（43.3%）和ChatGPT（42.8%）未达到。Bing-Precise和Bing-Creative在除营养教育外的所有学科领域普遍优于其他模型。所有模型在重复尝试中未能持续提供相同的正确答案，答案稳定性有限。ChatGPT响应模式一致性更高但准确性较低。提示工程效果微乎其微，仅在明确提供正确答案和解释时有小幅改善。

Conclusion: 尽管一些生成式AI模型勉强超过及格线，但总体准确性和答案一致性仍不理想。所有模型在答案一致性和鲁棒性方面都存在显著局限性。未来需要进一步发展以确保为营养师执照考试准备提供可靠和稳定的AI学习辅助工具。

Abstract: Generative artificial intelligence (AI) based on large language models
(LLMs), such as ChatGPT, has demonstrated remarkable progress across various
professional fields, including medicine and education. However, their
performance in nutritional education, especially in Japanese national licensure
examination for registered dietitians, remains underexplored. This study aimed
to evaluate the potential of current LLM-based generative AI models as study
aids for nutrition students. Questions from the Japanese national examination
for registered dietitians were used as prompts for ChatGPT and three Bing
models (Precise, Creative, Balanced), based on GPT-3.5 and GPT-4. Each question
was entered into independent sessions, and model responses were analyzed for
accuracy, consistency, and response time. Additional prompt engineering,
including role assignment, was tested to assess potential performance
improvements. Bing-Precise (66.2%) and Bing-Creative (61.4%) surpassed the
passing threshold (60%), while Bing-Balanced (43.3%) and ChatGPT (42.8%) did
not. Bing-Precise and Bing-Creative generally outperformed others across
subject fields except Nutrition Education, where all models underperformed.
None of the models consistently provided the same correct responses across
repeated attempts, highlighting limitations in answer stability. ChatGPT showed
greater consistency in response patterns but lower accuracy. Prompt engineering
had minimal effect, except for modest improvement when correct answers and
explanations were explicitly provided. While some generative AI models
marginally exceeded the passing threshold, overall accuracy and answer
consistency remained suboptimal. Moreover, all the models demonstrated notable
limitations in answer consistency and robustness. Further advancements are
needed to ensure reliable and stable AI-based study aids for dietitian
licensure preparation.

</details>


### [154] [Guided Navigation in Knowledge-Dense Environments: Structured Semantic Exploration with Guidance Graphs](https://arxiv.org/abs/2508.10012)
*Dehao Tao,Guangjie Liu,Weizheng,Yongfeng Huang,Minghu jiang*

Main category: cs.CL

TL;DR: GG Explore框架引入引导图，解决了LLM在知识密集型任务中知识图谱探索的效率和精度问题，通过结构对齐和上下文感知剪枝实现高效检索。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在知识密集型任务中受限于静态知识和不透明的推理过程。现有知识图谱（KGs）探索方法存在权衡：问题引导方法导致冗余探索，而线索引导方法无法有效利用上下文信息。

Method: 提出Guidance Graph guided Knowledge Exploration (GG Explore) 框架。该框架引入中间引导图（Guidance Graph），通过抽象目标知识结构并保留语义上下文来定义检索空间。在此基础上，开发了：1) 结构对齐（Structural Alignment），无需LLM开销即可过滤不兼容的候选；2) 上下文感知剪枝（Context Aware Pruning），强制执行图约束的语义一致性。

Result: 实验证明，GG Explore在效率上表现优异，超越了现有最先进方法（SOTA），尤其在复杂任务上表现突出。同时，即使使用较小的LLM也能保持强大的性能，展现了实用价值。

Conclusion: GG Explore通过引入引导图，成功地弥合了非结构化查询与结构化知识检索之间的鸿沟，实现了知识图谱的精确高效探索，显著提升了LLM在知识密集型任务中的表现。

Abstract: While Large Language Models (LLMs) exhibit strong linguistic capabilities,
their reliance on static knowledge and opaque reasoning processes limits their
performance in knowledge intensive tasks. Knowledge graphs (KGs) offer a
promising solution, but current exploration methods face a fundamental trade
off: question guided approaches incur redundant exploration due to granularity
mismatches, while clue guided methods fail to effectively leverage contextual
information for complex scenarios. To address these limitations, we propose
Guidance Graph guided Knowledge Exploration (GG Explore), a novel framework
that introduces an intermediate Guidance Graph to bridge unstructured queries
and structured knowledge retrieval. The Guidance Graph defines the retrieval
space by abstracting the target knowledge' s structure while preserving broader
semantic context, enabling precise and efficient exploration. Building upon the
Guidance Graph, we develop: (1) Structural Alignment that filters incompatible
candidates without LLM overhead, and (2) Context Aware Pruning that enforces
semantic consistency with graph constraints. Extensive experiments show our
method achieves superior efficiency and outperforms SOTA, especially on complex
tasks, while maintaining strong performance with smaller LLMs, demonstrating
practical value.

</details>


### [155] [Semantic Bridge: Universal Multi-Hop Question Generation via AMR-Driven Graph Synthesis](https://arxiv.org/abs/2508.10013)
*Linqing Chen,Hanmeng Zhong,Wentao Wu,Weilei Wang*

Main category: cs.CL

TL;DR: 该论文提出了“语义桥”（Semantic Bridge）框架，首次实现了从任意稀疏来源可控地生成复杂多跳推理问答对，以解决大型语言模型（LLM）训练中高质量推理数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型训练面临高质量、推理密集型问答对稀缺的瓶颈，尤其是在PubMed论文或法律文件等稀疏的领域特定来源中。现有方法依赖于表面模式，无法生成可控、复杂的、能测试真正理解的多跳推理问题，这阻碍了LLM训练范式的进步。

Method: 核心创新是“语义图编织”（semantic graph weaving），包含三种互补的桥接机制：实体桥接（用于角色变化的共享实体）、谓词链桥接（用于时间/因果/逻辑序列）和因果桥接（用于显式推理链）。通过抽象意义表示（AMR）驱动的分析，系统地构建跨文档的复杂路径，并对复杂性和类型进行细粒度控制。采用多模态AMR管道。

Result: 该方法在往返质量上提高了9.5%，在四种语言（英语、中文、法语、德语）上比基线提高了18.3%-25.4%。从200个来源生成的问答对，仅用67%的材料就超越了600个人工标注示例。人工评估显示，复杂性提高了23.4%，可回答性提高了18.7%，模式覆盖率提高了31.2%。

Conclusion: “语义桥”为LLM训练数据合成建立了一个新范式，使得从稀疏来源可控地生成目标推理问题成为可能。核心代码和模型将对外发布。

Abstract: Large language model (LLM) training faces a critical bottleneck: the scarcity
of high-quality, reasoning-intensive question-answer pairs, especially from
sparse, domain-specific sources like PubMed papers or legal documents. Existing
methods rely on surface patterns, fundamentally failing to generate
controllable, complex multi-hop reasoning questions that test genuine
understanding-essential for advancing LLM training paradigms. We present
\textbf{Semantic Bridge}, the first universal framework for controllably
generating sophisticated multi-hop reasoning questions from arbitrary sources.
Our breakthrough innovation is \textit{semantic graph weaving}-three
complementary bridging mechanisms (entity bridging for role-varying shared
entities, predicate chain bridging for temporal/causal/logical sequences, and
causal bridging for explicit reasoning chains)-that systematically construct
complex pathways across documents, with fine-grained control over complexity
and types via AMR-driven analysis. Our multi-modal AMR pipeline achieves up to
9.5% better round-trip quality, enabling production-ready controllable QA
generation. Extensive evaluation demonstrates performance across both
general-purpose datasets (Wikipedia) and specialized domains (biomedicine) It
yields consistent 18.3%-25.4% gains over baselines across four languages
(English, Chinese, French, German). Question pairs generated from 200 sources
outperform 600 native human annotation examples with 67% fewer materials. Human
evaluation shows 23.4% higher complexity, 18.7% better answerability, and 31.2%
improved pattern coverage. Semantic Bridge establishes a new paradigm for LLM
training data synthesis, enabling controllable generation of targeted reasoning
questions from sparse sources. We will release our core code and semantic
bridge model.

</details>


### [156] [PersonaEval: Are LLM Evaluators Human Enough to Judge Role-Play?](https://arxiv.org/abs/2508.10014)
*Lingfeng Zhou,Jialing Zhang,Jin Gao,Mohan Jiang,Dequan Wang*

Main category: cs.CL

TL;DR: 研究发现，当前LLM作为角色扮演评估者在角色识别能力上远低于人类，导致其评估可靠性不足，需要更强的人类级推理能力。


<details>
  <summary>Details</summary>
Motivation: 目前的LLM作为评估者的角色扮演研究范式未经充分验证，可能无法准确反映人类对角色忠实度的感知。作者认为，对角色扮演质量的任何有效判断都首先依赖于正确识别对话中的发言者（角色识别）。

Method: 提出了PersonaEval，这是首个旨在测试LLM评估器能否可靠识别人类角色的基准。该基准使用来自小说、剧本和视频转录的人类创作对话，挑战模型根据对话上下文确定正确的角色。同时进行了一项人类研究作为对比。

Result: 实验结果显示，即使是表现最佳的LLM，其角色识别准确率也仅为69%左右，远低于可靠评估所需的水平。相比之下，人类参与者的准确率接近天花板，达到90.8%。研究还表明，可靠的评估需要LLM评估器具备强大、类人的推理能力，而不仅仅是任务特定的微调或计算资源。

Conclusion: 目前的LLM评估器在角色识别方面与人类存在显著差距，因此不足以有效地判断角色扮演场景。要实现可靠的评估，LLM需要发展出更强的、类似人类的推理能力。

Abstract: Current role-play studies often rely on unvalidated LLM-as-a-judge paradigms,
which may fail to reflect how humans perceive role fidelity. A key prerequisite
for human-aligned evaluation is role identification, the ability to recognize
who is speaking based on dialogue context. We argue that any meaningful
judgment of role-playing quality (how well a character is played) fundamentally
depends on first correctly attributing words and actions to the correct persona
(who is speaking). We present PersonaEval, the first benchmark designed to test
whether LLM evaluators can reliably identify human roles. PersonaEval uses
human-authored dialogues from novels, scripts, and video transcripts,
challenging models to determine the correct persona according to the
conversation context. Our experiments, including a human study, show that even
the best-performing LLMs reach only around 69% accuracy, well below the level
needed for reliable evaluation. In contrast, human participants perform near
ceiling with 90.8% accuracy, highlighting that current LLM evaluators are still
not human enough to effectively judge role-play scenarios. To better understand
this gap, we examine training-time adaptation and test-time compute, suggesting
that reliable evaluation requires more than task-specific tuning, but depends
on strong, human-like reasoning abilities in LLM evaluators. We release our
benchmark at https://github.com/maple-zhou/PersonaEval.

</details>


### [157] [RealTalk-CN: A Realistic Chinese Speech-Text Dialogue Benchmark With Cross-Modal Interaction Analysis](https://arxiv.org/abs/2508.10015)
*Enzhi Wang,Qicheng Li,Shiwan Zhao,Aobo Kong,Jiaming Zhou,Xi Yang,Yequan Wang,Yonghua Lin,Yong Qin*

Main category: cs.CL

TL;DR: 本文介绍了RealTalk-CN，首个中文多轮、多领域语音-文本双模态任务型对话数据集，旨在解决现有数据集缺乏真实语音、语流不连贯和说话人多样性等问题，并提出了一个跨模态聊天任务以促进中文语音大模型的研究。


<details>
  <summary>Details</summary>
Motivation: 现有任务型对话（TOD）数据集主要基于文本，缺乏评估语音大模型（LLMs）鲁棒性所需的真实语音信号。此外，现有语音TOD数据集以英文为主，且缺乏语音不流畅和说话人多样性等关键要素。

Method: 引入了RealTalk-CN数据集，包含5.4k对话（60K话语，150小时），配有语音-文本注释，涵盖了多种对话场景和标注的自发性语音不流畅。同时，提出了一个新颖的跨模态聊天任务，模拟真实用户交互中语音和文本模态的动态切换。

Result: RealTalk-CN数据集被验证有效，能够评估语音大模型对语音不流畅的鲁棒性、对说话人特征的敏感性以及跨领域性能。它为中文语音大模型的研究奠定了坚实基础。

Conclusion: RealTalk-CN是首个中文多轮、多领域语音-文本双模态任务型对话数据集，通过提供真实世界的复杂性和多样性，有效填补了现有数据集的空白，为中文语音大模型的研究和评估提供了重要资源。

Abstract: In recent years, large language models (LLMs) have achieved remarkable
advancements in multimodal processing, including end-to-end speech-based
language models that enable natural interactions and perform specific tasks in
task-oriented dialogue (TOD) systems. However, existing TOD datasets are
predominantly text-based, lacking real speech signals that are essential for
evaluating the robustness of speech-based LLMs. Moreover, existing speech TOD
datasets are primarily English and lack critical aspects such as speech
disfluencies and speaker variations. To address these gaps, we introduce
RealTalk-CN, the first Chinese multi-turn, multi-domain speech-text dual-modal
TOD dataset, comprising 5.4k dialogues (60K utterances, 150 hours) with paired
speech-text annotations. RealTalk-CN captures diverse dialogue scenarios with
annotated spontaneous speech disfluencies, ensuring comprehensive coverage of
real-world complexities in speech dialogue. In addition, we propose a novel
cross-modal chat task that authentically simulates real-world user
interactions, allowing dynamic switching between speech and text modalities.
Our evaluation covers robustness to speech disfluencies, sensitivity to speaker
characteristics, and cross-domain performance. Extensive experiments validate
the effectiveness of RealTalk-CN, establishing a strong foundation for Chinese
speech-based LLMs research.

</details>


### [158] [Training-Free Multimodal Large Language Model Orchestration](https://arxiv.org/abs/2508.10016)
*Tianyu Xie,Yuhang Wu,Yongdong Luo,Jiayi Ji,Xiawu Zheng*

Main category: cs.CL

TL;DR: 本文提出了一种名为多模态大语言模型编排（MLLM Orchestration）的方法，无需额外训练即可构建交互式多模态AI系统，通过LLM协调专业模型，实现自然的多模态交互、提高效率和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有不同的多模态大语言模型（MLLMs）无法直接集成到统一的多模态输入输出系统中，通常需要额外的训练来解决模态对齐、文本转语音效率及其他集成问题。

Method: MLLM编排利用大语言模型的推理能力，通过显式工作流协调专业模型。核心创新包括：1) 一个中央控制器LLM，通过精心设计的代理动态路由任务；2) 一个并行文本转语音架构，实现全双工交互；3) 一个跨模态记忆集成系统，通过信息合成和检索保持上下文连贯性，并选择性避免不必要的模态调用以提高响应速度。

Result: MLLM编排在不额外训练的情况下实现了全面的多模态能力，在标准基准测试上比传统联合训练方法性能提升高达7.8%，延迟降低10.3%，并通过显式编排过程显著增强了可解释性。

Conclusion: MLLM编排是一种有效的方法，可以在不进行额外训练的情况下构建交互式多模态AI系统，同时提升性能、降低延迟并增强系统可解释性。

Abstract: Different Multimodal Large Language Models (MLLMs) cannot be integrated into
a unified multimodal input-output system directly. In previous work, training
has been considered as an inevitable component due to challenges in modal
alignment, Text-to-Speech efficiency and other integration issues. In this
paper, we introduce Multimodal Large Language Model Orchestration, an effective
approach for creating interactive multimodal AI systems without additional
training. MLLM Orchestration leverages the inherent reasoning capabilities of
large language models to coordinate specialized models through explicit
workflows, enabling natural multimodal interactions while maintaining
modularity, improving interpretability, and significantly enhancing
computational efficiency. Our orchestration framework is built upon three key
innovations: (1) a central controller LLM that analyzes user inputs and
dynamically routes tasks to appropriate specialized models through carefully
designed agents; (2) a parallel Text-to-Speech architecture that enables true
full-duplex interaction with seamless interruption handling and natural
conversational flow; and (3) a cross-modal memory integration system that
maintains coherent context across modalities through intelligent information
synthesis and retrieval, selectively avoiding unnecessary modality calls in
certain scenarios to improve response speed. Extensive evaluations demonstrate
that MLLM Orchestration achieves comprehensive multimodal capabilities without
additional training, performance improvements of up to 7.8% over traditional
jointly-trained approaches on standard benchmarks, reduced latency by 10.3%,
and significantly enhanced interpretability through explicit orchestration
processes.

</details>


### [159] [A Rose by Any Other Name Would Smell as Sweet: Categorical Homotopy Theory for Large Language Models](https://arxiv.org/abs/2508.10018)
*Sridhar Mahadevan*

Main category: cs.CL

TL;DR: 针对LLM对语义等价但表述不同的语句赋予不同概率的问题，本文提出了一个范畴同伦框架，利用LLM马尔可夫范畴和范畴同伦技术来捕捉语言中的“弱等价性”。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）未能对语义相同但表述不同的语句（如“Charles Darwin wrote”和“Charles Darwin is the author of”）生成相同的下一词概率，现有解决方案多为经验性而非理论性。

Method: 引入LLM马尔可夫范畴来表示LLM生成的语言概率分布，其中语句概率由范畴中的箭头定义。为解决等价改写导致非同构箭头的问题，使用范畴同伦技术（包括高代数K理论和模型范畴）来捕捉LLM马尔可夫范畴中的“弱等价性”。

Result: 提出了一个将范畴同伦应用于LLM的详细概述，旨在理论上解决LLM对语义等价语句概率不一致的问题，超越了现有的经验性方法。

Conclusion: 本文详细阐述了如何将范畴同伦（借鉴过去半个世纪的理论成果，如高代数K理论和模型范畴）应用于LLM，为解决LLM处理语言等价性问题提供了一个新的理论框架。

Abstract: Natural language is replete with superficially different statements, such as
``Charles Darwin wrote" and ``Charles Darwin is the author of", which carry the
same meaning. Large language models (LLMs) should generate the same next-token
probabilities in such cases, but usually do not. Empirical workarounds have
been explored, such as using k-NN estimates of sentence similarity to produce
smoothed estimates. In this paper, we tackle this problem more abstractly,
introducing a categorical homotopy framework for LLMs. We introduce an LLM
Markov category to represent probability distributions in language generated by
an LLM, where the probability of a sentence, such as ``Charles Darwin wrote" is
defined by an arrow in a Markov category. However, this approach runs into
difficulties as language is full of equivalent rephrases, and each generates a
non-isomorphic arrow in the LLM Markov category. To address this fundamental
problem, we use categorical homotopy techniques to capture ``weak equivalences"
in an LLM Markov category. We present a detailed overview of application of
categorical homotopy to LLMs, from higher algebraic K-theory to model
categories, building on powerful theoretical results developed over the past
half a century.

</details>


### [160] [Decoupling Understanding from Reasoning via Problem Space Mapping for Small-scale Model Reasoning](https://arxiv.org/abs/2508.10019)
*Li Wang,Changhao Zhang,Zengqi Xiu,Kai Lu,Xin Yu,Kui Zhang,Wenjun Wu*

Main category: cs.CL

TL;DR: 该研究提出一种新框架DURIT，通过将自然语言问题映射到规范问题空间，解耦理解与推理，显著提升小型语言模型（SLMs）的数学和逻辑推理能力及鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 小型语言模型（SLMs）在推理能力提升上面临挑战，主要原因是自然语言的复杂性和多变性（冗余、分散信息），这使得SLMs既要提取核心问题又要进行推理，庞大且嘈杂的问题空间阻碍了优化。

Method: 提出一个将自然语言问题映射到规范问题空间的新框架，解耦理解与推理。引入DURIT（Decoupled Understanding from Reasoning via Iterative Training）算法，包含三个迭代步骤：1) 通过强化学习映射自然语言问题；2) 通过自蒸馏对齐推理轨迹；3) 在问题空间中训练推理策略。映射器和推理器在此过程中交替共同训练。

Result: 实验表明，DURIT显著提高了SLMs在域内和域外数学及逻辑推理任务上的性能。除了提升推理能力，DURIT还增强了推理的鲁棒性。

Conclusion: 将理解与推理解耦是增强小型语言模型（SLMs）的有效策略。

Abstract: Despite recent advances in the reasoning capabilities of Large Language
Models (LLMs), improving the reasoning ability of Small Language Models (SLMs,
e.g., $\leq$ 1.5B) remains challenging. A key obstacle lies in the complexity
and variability of natural language: essentially equivalent problems often
appear in diverse surface forms, often obscured by redundant or distracting
details. This imposes a dual burden on SLMs: they must first extract the core
problem from complex linguistic input, and then perform reasoning based on that
understanding. The resulting vast and noisy problem space hinders optimization,
particularly for models with limited capacity. To address this, we propose a
new framework that decouples understanding from reasoning by mapping natural
language problems into a canonical problem space-a semantically simplified yet
expressive domain. This enables SLMs to focus on reasoning over standardized
inputs, free from linguistic variability. Within this framework, we introduce
DURIT (Decoupled Understanding from Reasoning via Iterative Training), a
three-step algorithm that iteratively: (1) mapping natural language problems
via reinforcement learning, (2) aligns reasoning trajectories through
self-distillation, and (3) trains reasoning policies in the problem space. The
mapper and reasoner are co-trained in an alternating loop throughout this
process. Experiments show that DURIT substantially improves SLMs' performance
on both in-domain and out-of-domain mathematical and logical reasoning tasks.
Beyond improving reasoning capabilities, DURIT also improves the robustness of
reasoning, validating decoupling understanding from reasoning as an effective
strategy for strengthening SLMs.

</details>


### [161] [FedCoT: Communication-Efficient Federated Reasoning Enhancement for Large Language Models](https://arxiv.org/abs/2508.10020)
*Chuan Li,Qianyi Zhao,Fengran Mo,Cen Chen*

Main category: cs.CL

TL;DR: FedCoT是一个新颖的联邦学习框架，通过轻量级思维链（CoT）增强机制和改进的LoRA模块聚合，显著提升大型语言模型（LLMs）在医疗等受限环境下的推理能力和可解释性，同时保护数据隐私。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习环境中提升LLM推理能力面临计算、通信和隐私限制，尤其在医疗领域，不仅需要准确结果，还需要可解释、可追溯的推理过程。现有联邦微调方法侧重答案正确性而忽视推理质量，且通常依赖侵犯隐私的知识蒸馏或产生高通信开销。

Method: FedCoT框架包含两个核心机制：1. 轻量级思维链增强：本地模型生成多条推理路径，紧凑的判别器动态选择最佳路径以提升推理准确性和鲁棒性；2. 改进的聚合方法：基于高级LoRA模块堆叠，并融入客户端分类器感知，实现跨异构客户端的无噪声聚合，高效管理客户端异质性并降低通信开销。

Result: 在医疗推理任务上的综合实验表明，FedCoT在严格的资源预算下显著提升了客户端的推理性能，同时完全保留了数据隐私。

Conclusion: FedCoT成功解决了联邦学习中LLM推理能力提升的挑战，特别是在医疗应用中，通过优化推理质量、效率和隐私保护，为受限环境下的LLM部署提供了有效方案。

Abstract: Efficiently enhancing the reasoning capabilities of large language models
(LLMs) in federated learning environments remains challenging, particularly
when balancing performance gains with strict computational, communication, and
privacy constraints. This challenge is especially acute in healthcare, where
decisions-spanning clinical, operational, and patient-facing contexts-demand
not only accurate outputs but also interpretable, traceable rationales to
ensure safety, accountability, and regulatory compliance. Conventional
federated tuning approaches on LLM fail to address this need: they optimize
primarily for answer correctness while neglecting rationale quality, leaving
CoT capabilities dependent on models' innate pre-training abilities. Moreover,
existing methods for improving rationales typically rely on privacy-violating
knowledge distillation from centralized models. Additionally, the communication
overhead in traditional federated fine-tuning on LLMs remains substantial. We
addresses this gap by proposing FedCoT, a novel framework specifically designed
to enhance reasoning in federated settings. FedCoT leverages a lightweight
chain-of-thought enhancement mechanism: local models generate multiple
reasoning paths, and a compact discriminator dynamically selects the most
promising one. This approach improves reasoning accuracy and robustness while
providing valuable interpretability, which is particularly critical for medical
applications. To manage client heterogeneity efficiently, we adopt an improved
aggregation approach building upon advanced LoRA module stacking, incorporating
client classifier-awareness to achieve noise-free aggregation across diverse
clients. Comprehensive experiments on medical reasoning tasks demonstrate that
FedCoT significantly boosts client-side reasoning performance under stringent
resource budgets while fully preserving data privacy.

</details>


### [162] [LATTE: Learning Aligned Transactions and Textual Embeddings for Bank Clients](https://arxiv.org/abs/2508.10021)
*Egor Fadeev,Dzhambulat Mollaev,Aleksei Shestov,Dima Korolev,Omar Zoloev,Ivan Kireev,Andrey Savchenko,Maksim Makarenko*

Main category: cs.CL

TL;DR: LATTE是一个对比学习框架，它通过将行为特征总结为短提示并由冻结的LLM嵌入作为监督，来学习客户历史通信序列的嵌入，从而显著降低了推理成本和输入大小，并在金融数据集上超越了现有技术。


<details>
  <summary>Details</summary>
Motivation: 金融应用中，从客户历史通信序列中学习客户嵌入至关重要。尽管大型语言模型（LLMs）提供了通用世界知识，但其直接用于长事件序列计算成本高昂且在实际管道中不切实际。

Method: 本文提出了LATTE，一个对比学习框架，旨在对齐原始事件嵌入与来自冻结LLM的语义嵌入。行为特征被总结成短提示，由LLM嵌入，并通过对比损失用作监督。该方法与传统LLM处理完整序列相比，显著降低了推理成本和输入大小。

Result: 与传统方法相比，LATTE显著降低了推理成本和输入大小。实验表明，该方法在真实世界金融数据集上，在学习事件序列表示方面优于现有最先进技术，同时能够部署在对延迟敏感的环境中。

Conclusion: LATTE提供了一种有效且高效的方法，通过利用冻结LLM进行语义监督，解决了直接使用LLM处理长序列的计算和实际限制，从而在金融领域学习客户事件序列表示方面取得了优异性能。

Abstract: Learning clients embeddings from sequences of their historic communications
is central to financial applications. While large language models (LLMs) offer
general world knowledge, their direct use on long event sequences is
computationally expensive and impractical in real-world pipelines. In this
paper, we propose LATTE, a contrastive learning framework that aligns raw event
embeddings with semantic embeddings from frozen LLMs. Behavioral features are
summarized into short prompts, embedded by the LLM, and used as supervision via
contrastive loss. The proposed approach significantly reduces inference cost
and input size compared to conventional processing of complete sequence by LLM.
We experimentally show that our method outperforms state-of-the-art techniques
for learning event sequence representations on real-world financial datasets
while remaining deployable in latency-sensitive environments.

</details>


### [163] [Conformal P-Value in Multiple-Choice Question Answering Tasks with Provable Risk Control](https://arxiv.org/abs/2508.10022)
*Yuanchang Ye*

Main category: cs.CL

TL;DR: 本研究提出了一种结合显著性检验的共形预测（CP）框架，以提高大型语言模型（LLMs）在多项选择问答（MCQA）中的可信度，通过计算选项频率和基于p值的假设检验来构建预测集。


<details>
  <summary>Details</summary>
Motivation: LLMs在专业问答场景中部署日益增多，但幻觉和非事实性生成严重损害了响应的可靠性。尽管CP提供了严格的边际覆盖保证，显著性检验也提供了统计严谨性，但两者尚未有效整合以解决LLM的不可靠性问题。

Method: 该框架通过MCQA响应的自洽重采样，将p值计算与一致性评分相结合。它计算选项频率以解决LLMs的黑盒性质，然后通过零假设检验（使用经验导出的p值）构建预测集。

Result: 在MMLU和MMLU-Pro基准测试中，结果表明：1) 增强的CP实现了用户指定的经验误覆盖率；2) 随着风险水平（α）的增加，测试集平均预测集大小（APSS）单调递减，验证了APSS作为有效不确定性度量的作用。

Conclusion: 这项工作为在高风险问答应用中部署可信赖的LLM建立了一个有原则的统计框架。

Abstract: This study introduces a significance testing-enhanced conformal prediction
(CP) framework to improve trustworthiness of large language models (LLMs) in
multiple-choice question answering (MCQA). While LLMs have been increasingly
deployed in disciplinary QA scenarios, hallucination and nonfactual generation
substantially compromise response reliability. Although CP provides
statistically rigorous marginal coverage guarantees for prediction sets, and
significance testing offers established statistical rigor, their synergistic
integration remains unexplored. To mitigate hallucination and factual
inaccuracies, our framework integrates $p$-value computation with conformity
scoring through self-consistency resampling of MCQA responses. This approach
calculates option frequencies to address LLMs' black-box nature, subsequently
constructing prediction sets via null hypothesis testing ($\mathcal{H}_0$) with
empirically derived $p$-values. Evaluations on MMLU and MMLU-Pro benchmarks
using off-the-shelf LLMs demonstrate: (1) The enhanced CP achieves
user-specified empirical miscoverage rates; (2) Test-set average prediction set
size (APSS) decreases monotonically with increasing risk levels ($\alpha$),
validating APSS as an effective uncertainty metric. This work establishes a
principled statistical framework for trustworthy LLM deployment in high-stakes
QA applications.

</details>


### [164] [RTTC: Reward-Guided Collaborative Test-Time Compute](https://arxiv.org/abs/2508.10024)
*J. Pablo Muñoz,Jinjie Yuan*

Main category: cs.CL

TL;DR: 该研究提出了Reward-Guided Test-Time Compute (RTTC) 框架，通过预训练奖励模型自适应选择最有效的测试时计算（TTC）策略，以提高大语言模型（LLM）的推理准确性并降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时计算（TTC）策略（如测试时训练TTT和检索增强生成RAG）虽能提升LLM性能，但其最佳适应策略因查询而异，不加区分的应用会导致巨大的计算开销。

Method: RTTC框架通过预训练奖励模型为每个查询自适应选择最有效的TTC策略。它采用分布式服务器-客户端架构，仅在必要时从远程知识库检索相关样本并在客户端设备上应用RAG或轻量级微调。此外，提出了查询状态缓存（Query-State Caching）机制，以在检索和适应层面高效重用历史查询状态，进一步减少冗余计算。

Result: 在多个LLM和基准测试上的广泛实验表明，RTTC相比于普通的RAG或TTT方法，能够持续实现更高的准确性。

Conclusion: 研究验证了自适应、奖励引导的TTC选择的必要性，并证明了RTTC在可扩展、高性能语言模型适应方面的巨大潜力。RTTC能够有效提升LLM的推理准确性，同时优化计算效率。

Abstract: Test-Time Compute (TTC) has emerged as a powerful paradigm for enhancing the
performance of Large Language Models (LLMs) at inference, leveraging strategies
such as Test-Time Training (TTT) and Retrieval-Augmented Generation (RAG).
However, the optimal adaptation strategy varies across queries, and
indiscriminate application of TTC strategy incurs substantial computational
overhead. In this work, we introduce Reward-Guided Test-Time Compute (RTTC), a
novel framework that adaptively selects the most effective TTC strategy for
each query via a pretrained reward model, maximizing downstream accuracy across
diverse domains and tasks. RTTC operates in a distributed server-client
architecture, retrieving relevant samples from a remote knowledge base and
applying RAG or lightweight fine-tuning on client devices only when necessary.
To further mitigate redundant computation, we propose Query-State Caching,
which enables the efficient reuse of historical query states at both retrieval
and adaptation levels. Extensive experiments across multiple LLMs and
benchmarks demonstrate that RTTC consistently achieves superior accuracy
compared to vanilla RAG or TTT, validating the necessity of adaptive,
reward-guided TTC selection and the potential of RTTC for scalable,
high-performance language model adaptation.

</details>


### [165] [Detecting and explaining postpartum depression in real-time with generative artificial intelligence](https://arxiv.org/abs/2508.10025)
*Silvia García-Méndez,Francisco de Arriba-Pérez*

Main category: cs.CL

TL;DR: 该研究开发了一个智能产后抑郁症（PPD）筛查系统，结合自然语言处理、机器学习和大型语言模型，实现经济、实时、非侵入性的自由语音分析，并提供可解释的预测结果，PPD检测准确率达90%。


<details>
  <summary>Details</summary>
Motivation: 产后抑郁症严重影响母亲的身心健康，因此，快速检测PPD及其相关风险因素对于及时评估和干预至关重要。现有技术需要帮助实践者进行实时筛查和治疗建议。

Method: 该系统结合了自然语言处理（NLP）、机器学习（ML）和大型语言模型（LLMs），通过自由语音分析进行PPD筛查。为解决“黑箱问题”，系统将LLMs与可解释的ML模型（如基于树的算法）结合，利用特征重要性和自然语言向最终用户解释预测结果。

Result: 在所有评估指标上，PPD检测的准确率达到90%，优于现有文献中的竞争解决方案。

Conclusion: 该解决方案有助于快速检测产后抑郁症及其相关风险因素，这对于及时和适当的评估及干预至关重要。

Abstract: Among the many challenges mothers undergo after childbirth, postpartum
depression (PPD) is a severe condition that significantly impacts their mental
and physical well-being. Consequently, the rapid detection of ppd and their
associated risk factors is critical for in-time assessment and intervention
through specialized prevention procedures. Accordingly, this work addresses the
need to help practitioners make decisions with the latest technological
advancements to enable real-time screening and treatment recommendations.
Mainly, our work contributes to an intelligent PPD screening system that
combines Natural Language Processing, Machine Learning (ML), and Large Language
Models (LLMs) towards an affordable, real-time, and non-invasive free speech
analysis. Moreover, it addresses the black box problem since the predictions
are described to the end users thanks to the combination of LLMs with
interpretable ml models (i.e., tree-based algorithms) using feature importance
and natural language. The results obtained are 90 % on ppd detection for all
evaluation metrics, outperforming the competing solutions in the literature.
Ultimately, our solution contributes to the rapid detection of PPD and their
associated risk factors, critical for in-time and proper assessment and
intervention.

</details>


### [166] [SABER: Switchable and Balanced Training for Efficient LLM Reasoning](https://arxiv.org/abs/2508.10026)
*Kai Zhao,Yanjun Zhao,Jiaming Song,Shien He,Lusheng Zhang,Qiang Zhang,Tianjiao Li*

Main category: cs.CL

TL;DR: SABER是一个强化学习框架，旨在使LLM在复杂任务上实现用户可控、预算受限的推理，从而在保持高准确性的同时降低推理成本和延迟。


<details>
  <summary>Details</summary>
Motivation: 尽管链式思考（CoT）推理使大型语言模型（LLM）在复杂任务上取得了令人印象深刻的准确性，但将其统一应用于所有问题会导致过高的推理成本和延迟。

Method: SABER框架通过以下方式实现：1) 分析每个训练样本的基础模型思考token使用量，并将其分配到预定义的预算层级。2) 在微调过程中，通过系统提示和长度感知奖励引导模型遵守分配的预算。3) 引入“无思考”示例以确保模型在关闭显式推理时仍能可靠工作。4) 提供四种离散推理模式（NoThink, FastThink, CoreThink, DeepThink），以实现延迟和推理深度之间的灵活权衡。

Result: 在数学推理（MATH, GSM8K）、代码生成（MBPP）和逻辑推理（LiveBench-Reasoning）任务上的广泛评估表明，SABER在严格预算下仍能实现高准确性，表现出优雅的性能下降，并有效实现跨尺度和跨领域泛化。特别是，SABER-FastThink在MATH基准测试中将推理长度缩短了65.4%，并比基础模型带来了3.6%的准确率提升。

Conclusion: SABER成功地为LLM赋予了用户可控、token预算受限的推理能力，有效解决了CoT推理的成本和延迟问题，同时在各种复杂任务上保持了高准确性和良好的泛化能力，并提供了灵活的推理模式选择。

Abstract: Large language models (LLMs) empowered by chain-of-thought reasoning have
achieved impressive accuracy on complex tasks but suffer from excessive
inference costs and latency when applied uniformly to all problems. We propose
SABER (Switchable and Balanced Training for Efficient LLM Reasoning), a
reinforcement learning framework that endows LLMs with user-controllable,
token-budgeted reasoning. SABER first profiles each training example's
base-model thinking token usage and assigns it to one of the predefined budget
tiers. During fine-tuning, the model is guided by system prompts and
length-aware rewards to respect its assigned budget. In parallel, we
incorporate no-think examples to ensure the model remains reliable even when
explicit reasoning is turned off. SABER further supports four discrete
inference modes - NoThink, FastThink, CoreThink, and DeepThink, enabling
flexible trade-offs between latency and reasoning depth. Extensive evaluations
on math reasoning (MATH, GSM8K), code generation (MBPP), and logical reasoning
(LiveBench-Reasoning) demonstrate that SABER achieves high accuracy under tight
budgets, graceful degradation, and effective cross-scale and cross-domain
generalization. In particular, SABER-FastThink cuts reasoning length by 65.4%
and yields a 3.6% accuracy gain compared with the base model on the MATH
benchmark.

</details>


### [167] [LLMCARE: Alzheimer's Detection via Transformer Models Enhanced by LLM-Generated Synthetic Data](https://arxiv.org/abs/2508.10027)
*Ali Zolnour,Hossein Azadmaleki,Yasaman Haghbin,Fatemeh Taherinezhad,Mohamad Javad Momeni Nezhad,Sina Rashidi,Masoud Khani,AmirSajjad Taleban,Samin Mahdizadeh Sani,Maryam Dadkhah,James M. Noble,Suzanne Bakken,Yadollah Yaghoobzadeh,Abdol-Hossein Vahabie,Masoud Rouhizadeh,Maryam Zolnoori*

Main category: cs.CL

TL;DR: 该研究开发了一种融合Transformer嵌入和手工语言特征的语音NLP管线，用于阿尔茨海默病及相关痴呆（ADRD）的早期检测，并探索了基于LLM的合成语音数据增强及多模态分类器的应用。


<details>
  <summary>Details</summary>
Motivation: 美国约有500万老年人受ADRD影响，但超过一半未被诊断。语音自然语言处理（NLP）提供了一种有前景、可扩展的方法，通过语言标记检测早期认知衰退。

Method: 研究使用了DementiaBank的“cookie-theft”任务转录数据（n=237）。评估了10种Transformer模型及其微调策略。融合模型结合了最佳Transformer嵌入和110个词汇衍生语言特征。使用5种LLM（LLaMA-8B/70B, MedAlpaca-7B, Ministral-8B, GPT-4o）生成标签条件合成语音进行数据增强。测试了3种多模态模型（GPT-4o, Qwen-Omni, Phi-4）在零样本和微调设置下的语音-文本分类。

Result: 融合模型在ADRD检测中表现最佳（F1=83.3，AUC=89.5），优于单独使用语言特征或Transformer的模型。使用2倍MedAlpaca-7B合成语音增强训练数据，将F1提高到85.7。微调显著提升了单模态LLM分类器性能（如MedAlpaca：F1从47.3提高到78.5）。当前多模态模型表现较低（GPT-4o=70.2 F1；Qwen=66.0）。性能提升与合成语音和真实语音的分布相似性一致。

Conclusion: 整合Transformer嵌入与语言特征可增强ADRD的语音检测能力。临床调整的LLM能有效支持分类和数据增强，但多模态建模仍需进一步发展。

Abstract: Alzheimer's disease and related dementias (ADRD) affect approximately five
million older adults in the U.S., yet over half remain undiagnosed.
Speech-based natural language processing (NLP) offers a promising, scalable
approach to detect early cognitive decline through linguistic markers.
  To develop and evaluate a screening pipeline that (i) fuses transformer
embeddings with handcrafted linguistic features, (ii) tests data augmentation
using synthetic speech generated by large language models (LLMs), and (iii)
benchmarks unimodal and multimodal LLM classifiers for ADRD detection.
  Transcripts from the DementiaBank "cookie-theft" task (n = 237) were used.
Ten transformer models were evaluated under three fine-tuning strategies. A
fusion model combined embeddings from the top-performing transformer with 110
lexical-derived linguistic features. Five LLMs (LLaMA-8B/70B, MedAlpaca-7B,
Ministral-8B, GPT-4o) were fine-tuned to generate label-conditioned synthetic
speech, which was used to augment training data. Three multimodal models
(GPT-4o, Qwen-Omni, Phi-4) were tested for speech-text classification in
zero-shot and fine-tuned settings.
  The fusion model achieved F1 = 83.3 (AUC = 89.5), outperforming linguistic or
transformer-only baselines. Augmenting training data with 2x MedAlpaca-7B
synthetic speech increased F1 to 85.7. Fine-tuning significantly improved
unimodal LLM classifiers (e.g., MedAlpaca: F1 = 47.3 -> 78.5 F1). Current
multimodal models demonstrated lower performance (GPT-4o = 70.2 F1; Qwen =
66.0). Performance gains aligned with the distributional similarity between
synthetic and real speech.
  Integrating transformer embeddings with linguistic features enhances ADRD
detection from speech. Clinically tuned LLMs effectively support both
classification and data augmentation, while further advancement is needed in
multimodal modeling.

</details>


### [168] [PREF: Reference-Free Evaluation of Personalised Text Generation in LLMs](https://arxiv.org/abs/2508.10028)
*Xiao Fu,Hossein A. Rahmani,Bin Wu,Jerome Ramos,Emine Yilmaz,Aldo Lipani*

Main category: cs.CL

TL;DR: PREF是一个个性化、无参考的评估框架，用于联合衡量文本生成质量和用户对齐度，无需黄金个性化参考。


<details>
  <summary>Details</summary>
Motivation: 现有的文本生成评估方法大多忽视用户的个性化需求，但个性化文本生成对以用户为中心的信息系统至关重要。

Method: PREF采用三步流程：1) 覆盖阶段：LLM生成通用质量标准（如事实性、连贯性）的指南；2) 偏好阶段：根据用户画像、偏好和上下文，重新排序并增强这些因素，生成个性化评估准则；3) 评分阶段：LLM判断模型根据个性化准则对候选答案进行评分。

Result: 在PrefEval基准测试中，PREF比强基线模型取得了更高的准确性、更好的校准性，以及与人类判断更紧密的一致性。

Conclusion: PREF为个性化语言生成系统的评估和开发提供了可扩展、可解释且与用户对齐的评估方法，从而奠定了更可靠评估的基础。

Abstract: Personalised text generation is essential for user-centric information
systems, yet most evaluation methods overlook the individuality of users. We
introduce \textbf{PREF}, a \textbf{P}ersonalised \textbf{R}eference-free
\textbf{E}valuation \textbf{F}ramework that jointly measures general output
quality and user-specific alignment without requiring gold personalised
references. PREF operates in a three-step pipeline: (1) a coverage stage uses a
large language model (LLM) to generate a comprehensive, query-specific
guideline covering universal criteria such as factuality, coherence, and
completeness; (2) a preference stage re-ranks and selectively augments these
factors using the target user's profile, stated or inferred preferences, and
context, producing a personalised evaluation rubric; and (3) a scoring stage
applies an LLM judge to rate candidate answers against this rubric, ensuring
baseline adequacy while capturing subjective priorities. This separation of
coverage from preference improves robustness, transparency, and reusability,
and allows smaller models to approximate the personalised quality of larger
ones. Experiments on the PrefEval benchmark, including implicit
preference-following tasks, show that PREF achieves higher accuracy, better
calibration, and closer alignment with human judgments than strong baselines.
By enabling scalable, interpretable, and user-aligned evaluation, PREF lays the
groundwork for more reliable assessment and development of personalised
language generation systems.

</details>


### [169] [Latent Fusion Jailbreak: Blending Harmful and Harmless Representations to Elicit Unsafe LLM Outputs](https://arxiv.org/abs/2508.10029)
*Wenpeng Xing,Mohan Li,Chunqiang Hu,Haitao XuNingyu Zhang,Bo Lin,Meng Han*

Main category: cs.CL

TL;DR: 本文提出了一种名为Latent Fusion Jailbreak (LFJ)的基于表示的越狱攻击，通过插值有害和良性查询的隐藏状态来绕过LLM安全对齐，并提出了一种对抗训练防御方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在语言任务中表现出色，但容易受到越狱攻击，这些攻击能够绕过其安全对齐。

Method: LFJ通过选择主题和句法相似的有害与良性查询对，在关键层和token上进行梯度引导的隐藏状态插值，并优化以平衡攻击成功率、输出流畅性和计算效率。为缓解LFJ，本文提出了一种对抗训练防御，通过在插值示例上微调模型。

Result: LFJ在Vicuna和LLaMA-2等模型上取得了平均94.01%的攻击成功率（ASR），优于现有方法。所提出的防御将ASR降低了80%以上，且不影响良性输入性能。

Conclusion: LFJ是一种高效的基于表示的越狱攻击，通过隐藏状态插值实现。同时，对抗训练是一种有效的缓解LFJ攻击的方法，且不损害模型在良性任务上的性能。

Abstract: Large language models (LLMs) demonstrate impressive capabilities in various
language tasks but are susceptible to jailbreak attacks that circumvent their
safety alignments. This paper introduces Latent Fusion Jailbreak (LFJ), a
representation-based attack that interpolates hidden states from harmful and
benign query pairs to elicit prohibited responses. LFJ begins by selecting
query pairs with high thematic and syntactic similarity, then performs
gradient-guided interpolation at influential layers and tokens, followed by
optimization to balance attack success, output fluency, and computational
efficiency. Evaluations on models such as Vicuna and LLaMA-2 across benchmarks
like AdvBench and MaliciousInstruct yield an average attack success rate (ASR)
of 94.01%, outperforming existing methods. To mitigate LFJ, we propose an
adversarial training defense that fine-tunes models on interpolated examples,
reducing ASR by over 80% without degrading performance on benign inputs.
Ablation studies validate the importance of query pair selection, hidden state
interpolation components, and optimization strategies in LFJ's effectiveness.

</details>


### [170] [Inference-Aware Prompt Optimization for Aligning Black-Box Large Language Models](https://arxiv.org/abs/2508.10030)
*Saaduddin Mahmud,Mason Nakamura,Kyle H. Wray,Shlomo Zilberstein*

Main category: cs.CL

TL;DR: 本文提出了一个统一的框架IAPO及其算法PSST，用于在考虑推理预算和多目标的情况下，联合优化黑盒LLM的提示和推理规模，解决了现有提示优化方法与推理策略脱节的问题。


<details>
  <summary>Details</summary>
Motivation: 现有提示优化方法在优化提示时，不考虑部署时采用的推理策略（如Best-of-N采样、多数投票），这构成了一个显著的方法学空白。研究发现提示优化和推理策略之间存在强烈的相互依赖性。此外，用户对多目标权衡和推理预算的偏好也极大地影响了提示和推理配置的选择。

Method: 引入了IAPO（Inference-Aware Prompt Optimization）统一框架，该框架联合优化提示和推理规模，同时考虑推理预算和不同任务目标。为IAPO开发了固定预算训练算法PSST（Prompt Scaling via Sequential Trimming），并分析了有限预算下错误概率的保证。

Result: 在六个不同的任务（包括多目标文本生成和推理）上评估了PSST的有效性。结果表明，在通过提示优化对黑盒LLM进行对齐时，结合推理感知（inference-awareness）起着关键作用。

Conclusion: 将推理感知纳入提示优化对于有效对齐黑盒大型语言模型至关重要，联合优化提示和推理规模能够显著提升性能和对齐效果。

Abstract: Prompt optimization methods have demonstrated significant effectiveness in
aligning black-box large language models (LLMs). In parallel, inference scaling
strategies such as Best-of-N Sampling and Majority Voting have also proven to
enhance alignment and performance by trading off computation. However, existing
prompt optimization approaches are inference strategy agnostic; that is, they
optimize prompts without regard to the inference strategy employed during
deployment. This constitutes a significant methodological gap, as our empirical
and theoretical analysis reveals a strong interdependence between these two
paradigms. Moreover, we find that user preferences regarding trade-offs among
multiple objectives and inference budgets substantially influence the choice of
prompt and inference configuration. To address this gap, we introduce a unified
novel framework named IAPO (Inference-Aware Prompt Optimization) that jointly
optimizes the prompt and inference scale, while being aware of the inference
budget and different task objectives. We then develop a fixed-budget training
algorithm for IAPO, which we call PSST (Prompt Scaling via Sequential
Trimming), and analyze finite-budget guarantees on error probability. Finally,
we evaluate the effectiveness of PSST on six different tasks, including
multi-objective text generation and reasoning, and demonstrate the critical
role of incorporating inference-awareness when aligning black-box LLMs through
prompt optimization.

</details>


### [171] [The Cost of Thinking: Increased Jailbreak Risk in Large Language Models](https://arxiv.org/abs/2508.10032)
*Fan Yang*

Main category: cs.CL

TL;DR: 研究发现，大语言模型（LLMs）的思维模式（thinking mode）更容易受到越狱攻击。通过大规模评估，发现攻击思维模式的成功率更高。论文提出了一种“安全思维干预”方法，通过在提示中添加特定思维token来引导LLMs的内部思维过程，从而显著降低攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 思维模式一直被认为是LLMs中最有价值的模式之一，但研究者偶然发现了一个此前被忽视的现象：具有思维模式的LLMs更容易被越狱攻击攻破，这促使他们深入研究并寻求缓解方案。

Method: 研究方法包括：1. 在AdvBench和HarmBench数据集上评估9个LLMs，比较思维模式和非思维模式下的攻击成功率。2. 通过大量样本研究，分析成功攻击数据的特征（如教育目的、过长的思维长度）以及LLMs在明知问题有害时仍给出有害回答的情况。3. 提出一种“安全思维干预”方法，通过在提示中添加“特定思维token”来明确引导LLMs的内部思维过程。

Result: 主要结果显示：1. 攻击LLMs思维模式的成功率几乎总是高于非思维模式。2. 成功攻击的数据通常具有“教育目的”和“过长的思维长度”等特征。3. LLMs在多数情况下即使知道问题有害，也仍会给出有害答案。4. 提出的“安全思维干预”方法能够显著降低具有思维模式的LLMs的攻击成功率。

Conclusion: 研究得出结论，LLMs的思维模式虽然有价值，但也使其更容易受到越狱攻击。通过分析攻击特征，并引入“安全思维干预”这种显式引导内部思维过程的方法，可以有效缓解思维模式下LLMs的脆弱性，提高其安全性。

Abstract: Thinking mode has always been regarded as one of the most valuable modes in
LLMs. However, we uncover a surprising and previously overlooked phenomenon:
LLMs with thinking mode are more easily broken by Jailbreak attack. We evaluate
9 LLMs on AdvBench and HarmBench and find that the success rate of attacking
thinking mode in LLMs is almost higher than that of non-thinking mode. Through
large numbers of sample studies, it is found that for educational purposes and
excessively long thinking lengths are the characteristics of successfully
attacked data, and LLMs also give harmful answers when they mostly know that
the questions are harmful. In order to alleviate the above problems, this paper
proposes a method of safe thinking intervention for LLMs, which explicitly
guides the internal thinking processes of LLMs by adding "specific thinking
tokens" of LLMs to the prompt. The results demonstrate that the safe thinking
intervention can significantly reduce the attack success rate of LLMs with
thinking mode.

</details>


### [172] [Reflect then Learn: Active Prompting for Information Extraction Guided by Introspective Confusion](https://arxiv.org/abs/2508.10036)
*Dong Zhao,Yadong Wang,Xiang Chen,Chenxi Wang,Hongliang Dai,Chuanxing Geng,Shengzhong Zhang,Shaoyuan Li,Sheng-Jun Huang*

Main category: cs.CL

TL;DR: 该研究提出APIE框架，通过引入格式不确定性和内容不确定性双重指标，使LLM能自我评估困惑，从而主动选择信息量最大的样本作为少样本示例，显著提升信息抽取性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在少样本信息抽取（IE）方面潜力巨大，但其性能对上下文示例的选择高度敏感。传统选择策略不足以提供有效指导，因为它们忽略了模型错误的一个关键来源：不仅是语义内容的混淆，还包括生成IE任务所需结构化格式的困难。

Method: 引入信息抽取主动提示（APIE）框架，基于“内省困惑”原则。该方法使LLM能够通过双组件不确定性度量来评估自身的困惑，该度量独特地量化了“格式不确定性”（生成正确语法的难度）和“内容不确定性”（提取语义的不一致性）。通过使用此综合分数对未标记数据进行排序，框架主动选择最具挑战性和信息量的样本作为少样本示例。

Result: 在四个基准测试上的广泛实验表明，该方法始终优于强基线，在提取准确性和鲁棒性方面都取得了显著改进。

Conclusion: 构建有效可靠的结构化生成系统时，对模型不确定性进行细粒度、双层（格式和内容）的审视至关重要。

Abstract: Large Language Models (LLMs) show remarkable potential for few-shot
information extraction (IE), yet their performance is highly sensitive to the
choice of in-context examples. Conventional selection strategies often fail to
provide informative guidance, as they overlook a key source of model
fallibility: confusion stemming not just from semantic content, but also from
the generation of well-structured formats required by IE tasks. To address
this, we introduce Active Prompting for Information Extraction (APIE), a novel
active prompting framework guided by a principle we term introspective
confusion. Our method empowers an LLM to assess its own confusion through a
dual-component uncertainty metric that uniquely quantifies both Format
Uncertainty (difficulty in generating correct syntax) and Content Uncertainty
(inconsistency in extracted semantics). By ranking unlabeled data with this
comprehensive score, our framework actively selects the most challenging and
informative samples to serve as few-shot exemplars. Extensive experiments on
four benchmarks show that our approach consistently outperforms strong
baselines, yielding significant improvements in both extraction accuracy and
robustness. Our work highlights the critical importance of a fine-grained,
dual-level view of model uncertainty when it comes to building effective and
reliable structured generation systems.

</details>


### [173] [mSCoRe: a $M$ultilingual and Scalable Benchmark for $S$kill-based $Co$mmonsense $Re$asoning](https://arxiv.org/abs/2508.10137)
*Nghia Trung Ngo,Franck Dernoncourt,Thien Huu Nguyen*

Main category: cs.CL

TL;DR: 该研究提出了mSCoRe，一个多语言、可扩展的基于技能的常识推理基准，用于系统评估大型语言模型在多语言常识推理中的能力和局限性。


<details>
  <summary>Details</summary>
Motivation: 尽管推理增强型大型语言模型在复杂推理任务中表现出色，但其利用不同人类推理技能的机制，特别是涉及跨语言和文化日常知识的多语言常识推理，仍未得到充分研究。

Method: 该研究提出了mSCoRe基准，包含三个核心组件：1) 一个新颖的推理技能分类法，用于细粒度分析模型推理过程；2) 一个为常识推理评估量身定制的鲁棒数据合成管道；3) 一个复杂性扩展框架，使任务难度能随LLM能力的未来提升而动态调整。研究在八个最先进的LLM上进行了广泛实验。

Result: 实验结果表明，mSCoRe对当前模型仍然极具挑战性，尤其是在更高复杂性级别。研究揭示了推理增强型模型在面对细致入微的多语言通用和文化常识时的局限性。

Conclusion: 当前的大型语言模型在处理细致的多语言通用和文化常识方面存在显著局限性。研究提供了对模型推理过程的详细分析，并为未来改进多语言常识推理能力指明了方向。

Abstract: Recent advancements in reasoning-reinforced Large Language Models (LLMs) have
shown remarkable capabilities in complex reasoning tasks. However, the
mechanism underlying their utilization of different human reasoning skills
remains poorly investigated, especially for multilingual commonsense reasoning
that involves everyday knowledge across different languages and cultures. To
address this gap, we propose a \textbf{M}ultilingual and Scalable Benchmark for
\textbf{S}kill-based \textbf{Co}mmonsense \textbf{Re}asoning (\textbf{mSCoRe}).
Our benchmark incorporates three key components that are designed to
systematically evaluate LLM's reasoning capabilities, including: (1) a novel
taxonomy of reasoning skills that enables fine-grained analysis of models'
reasoning processes, (2) a robust data synthesis pipeline tailored specifically
for commonsense reasoning evaluation, and (3) a complexity scaling framework
allowing task difficulty to scale dynamically alongside future improvements in
LLM abilities. Extensive experiments on eights state-of-the-art LLMs of varying
sizes and training approaches demonstrate that \textbf{mSCoRe} remains
significantly challenging for current models, particularly at higher complexity
levels. Our results reveal the limitations of such reasoning-reinforced models
when confronted with nuanced multilingual general and cultural commonsense. We
further provide detailed analysis on the models' reasoning processes,
suggesting future directions for improving multilingual commonsense reasoning
capabilities.

</details>


### [174] [Multi-Turn Puzzles: Evaluating Interactive Reasoning and Strategic Dialogue in LLMs](https://arxiv.org/abs/2508.10142)
*Kartikeya Badola,Jonathan Simon,Arian Hosseini,Sara Marie Mc Carthy,Tsendsuren Munkhdalai,Abhimanyu Goyal,Tomáš Kočiský,Shyam Upadhyay,Bahare Fatemi,Mehran Kazemi*

Main category: cs.CL

TL;DR: 大型语言模型在复杂交互任务中表现不佳，本文提出了一个多轮对话基准测试，以评估其推理、信息获取和交互能力，并发现现有模型仍有巨大提升空间。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在清晰完整的陈述问题上表现出色，但在真实世界常见的细致或交互式任务（需要逻辑一致的多轮对话、信息寻求和不完全数据推理）中表现挣扎。

Method: 引入了一个新的基准测试，包含一系列多轮任务，旨在测试特定的推理、交互式对话和信息寻求能力。这些任务具有确定性评分机制，无需人工干预。

Result: 对前沿模型进行评估显示，模型仍有显著的提升空间。分析表明，大多数错误源于指令遵循不佳、推理失败和规划能力不足。

Conclusion: 该基准测试为当前大型语言模型在处理复杂交互场景时的优势和劣势提供了宝贵见解，并为未来旨在提升这些关键能力的研究提供了强大的平台。

Abstract: Large language models (LLMs) excel at solving problems with clear and
complete statements, but often struggle with nuanced environments or
interactive tasks which are common in most real-world scenarios. This
highlights the critical need for developing LLMs that can effectively engage in
logically consistent multi-turn dialogue, seek information and reason with
incomplete data. To this end, we introduce a novel benchmark comprising a suite
of multi-turn tasks each designed to test specific reasoning, interactive
dialogue, and information-seeking abilities. These tasks have deterministic
scoring mechanisms, thus eliminating the need for human intervention.
Evaluating frontier models on our benchmark reveals significant headroom. Our
analysis shows that most errors emerge from poor instruction following,
reasoning failures, and poor planning. This benchmark provides valuable
insights into the strengths and weaknesses of current LLMs in handling complex,
interactive scenarios and offers a robust platform for future research aimed at
improving these critical capabilities.

</details>


### [175] [LaajMeter: A Framework for LaaJ Evaluation](https://arxiv.org/abs/2508.10161)
*Gal Amram,Eitan Farchi,Shmulik Froimovich,Raviv Gal,Avi Ziv*

Main category: cs.CL

TL;DR: 本文介绍了LaaJMeter，一个基于模拟的框架，用于在数据稀缺的领域特定场景中，对LLM作为评估器（LaaJ）进行受控的元评估，帮助验证和优化评估指标。


<details>
  <summary>Details</summary>
Motivation: LLM作为评估器（LaaJ）在通用领域有效，但在领域特定背景下，由于缺乏标注数据和专家评估成本高昂，其应用面临挑战。现有元评估指标未经特定领域验证，难以确定哪些指标能有效识别LaaJ质量及合适的性能阈值。

Method: 引入了LaaJMeter，一个基于模拟的框架，能够生成代表虚拟模型和评判者的合成数据。这允许在现实条件下系统分析评估指标，帮助从业者验证和改进LaaJ，测试指标是否能区分优劣，并估算评估器性能的适当阈值。

Result: 通过在遗留编程语言代码翻译任务中的应用，展示了LaaJMeter的实用性，揭示了不同指标对评估器质量的敏感性差异。结果强调了常见指标的局限性以及选择合理指标的重要性。

Conclusion: LaaJMeter为在低资源环境下评估LaaJ提供了一个可扩展和可扩展的解决方案，有助于确保自然语言处理中评估的可信度和可复现性。

Abstract: Large Language Models (LLMs) are increasingly used as evaluators in natural
language processing tasks, a paradigm known as LLM-as-a-Judge (LaaJ). While
effective in general domains, LaaJs pose significant challenges in
domain-specific contexts, where annotated data is scarce and expert evaluation
is costly. In such cases, meta-evaluation is often performed using metrics that
have not been validated for the specific domain in which they are applied. As a
result, it becomes difficult to determine which metrics effectively identify
LaaJ quality, and further, what threshold indicates sufficient evaluator
performance. In this work, we introduce LaaJMeter, a simulation-based framework
for controlled meta-evaluation of LaaJs. LaaJMeter enables engineers to
generate synthetic data representing virtual models and judges, allowing
systematic analysis of evaluation metrics under realistic conditions. This
helps practitioners validate and refine LaaJs for specific evaluation tasks:
they can test whether their metrics correctly distinguish between better and
worse (virtual) LaaJs, and estimate appropriate thresholds for evaluator
adequacy.
  We demonstrate the utility of LaaJMeter in a code translation task involving
a legacy programming language, showing how different metrics vary in
sensitivity to evaluator quality. Our results highlight the limitations of
common metrics and the importance of principled metric selection. LaaJMeter
provides a scalable and extensible solution for assessing LaaJs in low-resource
settings, contributing to the broader effort to ensure trustworthy and
reproducible evaluation in NLP.

</details>


### [176] [Estimating Machine Translation Difficulty](https://arxiv.org/abs/2508.10175)
*Lorenzo Proietti,Stefano Perrella,Vilém Zouhar,Roberto Navigli,Tom Kocmi*

Main category: cs.CL

TL;DR: 该研究形式化了机器翻译难度估计任务，引入了新的评估指标，并开发了专用模型（Sentinel-src）来识别对当代机器翻译系统更具挑战性的文本，以改进评估和指导未来研究。


<details>
  <summary>Details</summary>
Motivation: 当前机器翻译质量很高，使得难以区分最先进的模型并识别未来改进领域。自动识别机器翻译系统难以处理的文本对于开发更具区分度的评估和指导未来研究至关重要。

Method: 研究者将翻译难度估计任务形式化，根据文本翻译的预期质量来定义其难度。他们引入了一个新的指标来评估难度估计器，并用它来评估基线方法（如词汇稀有度、句法复杂性、LLM作为评判者）和新颖方法（专用模型Sentinel-src）。最后，他们通过使用难度估计器构建更具挑战性的机器翻译基准来展示其实用性。

Result: 专用模型（Sentinel-src）在翻译难度估计方面优于基于启发式的方法（如词汇稀有度或句法复杂性）和LLM作为评判者的方法。研究发布了两个改进的难度估计模型：Sentinel-src-24和Sentinel-src-25。

Conclusion: 专用模型（Sentinel-src）能够有效估计翻译难度，识别对机器翻译系统具有挑战性的文本。这些模型可以用于扫描大量文本集合，选择最有可能挑战当前机器翻译系统的文本，从而构建更具区分度的机器翻译基准，促进未来的研究和发展。

Abstract: Machine translation quality has began achieving near-perfect translations in
some setups. These high-quality outputs make it difficult to distinguish
between state-of-the-art models and to identify areas for future improvement.
Automatically identifying texts where machine translation systems struggle
holds promise for developing more discriminative evaluations and guiding future
research.
  We formalize the task of translation difficulty estimation, defining a text's
difficulty based on the expected quality of its translations. We introduce a
new metric to evaluate difficulty estimators and use it to assess both
baselines and novel approaches. Finally, we demonstrate the practical utility
of difficulty estimators by using them to construct more challenging machine
translation benchmarks. Our results show that dedicated models (dubbed
Sentinel-src) outperform both heuristic-based methods (e.g. word rarity or
syntactic complexity) and LLM-as-a-judge approaches. We release two improved
models for difficulty estimation, Sentinel-src-24 and Sentinel-src-25, which
can be used to scan large collections of texts and select those most likely to
challenge contemporary machine translation systems.

</details>


### [177] [PakBBQ: A Culturally Adapted Bias Benchmark for QA](https://arxiv.org/abs/2508.10186)
*Abdullah Hashmat,Muhammad Arham Mirza,Agha Ali Raza*

Main category: cs.CL

TL;DR: 该研究引入了PakBBQ数据集，一个针对巴基斯坦文化和区域背景的偏见评估基准，用于衡量多语言大型语言模型（LLMs）在低资源语言（如乌尔都语）中的偏见。实验发现，消歧、语言和负面提问方式能有效缓解偏见。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的广泛应用，确保其对所有用户群体的公平性至关重要。然而，大多数LLMs在西方中心数据上训练和评估，忽视了低资源语言和区域背景，导致公平性问题。

Method: 引入了PakBBQ数据集，它是Bias Benchmark for Question Answering (BBQ) 的扩展，包含214个模板和17180个英乌双语问答对，涵盖8个与巴基斯坦相关的偏见维度。研究在模糊、明确消歧以及正面/负面提问框架下评估了多个多语言LLMs。

Result: 实验结果显示：(i) 消歧后平均准确率提高12%；(ii) 乌尔都语比英语表现出更强的反偏见行为；(iii) 负面提问显著减少了刻板印象反应。

Conclusion: 研究强调了语境化基准和简单的提示工程策略对于在低资源环境中缓解LLMs偏见的重要性。

Abstract: With the widespread adoption of Large Language Models (LLMs) across various
applications, it is empirical to ensure their fairness across all user
communities. However, most LLMs are trained and evaluated on Western centric
data, with little attention paid to low-resource languages and regional
contexts. To address this gap, we introduce PakBBQ, a culturally and regionally
adapted extension of the original Bias Benchmark for Question Answering (BBQ)
dataset. PakBBQ comprises over 214 templates, 17180 QA pairs across 8
categories in both English and Urdu, covering eight bias dimensions including
age, disability, appearance, gender, socio-economic status, religious, regional
affiliation, and language formality that are relevant in Pakistan. We evaluate
multiple multilingual LLMs under both ambiguous and explicitly disambiguated
contexts, as well as negative versus non negative question framings. Our
experiments reveal (i) an average accuracy gain of 12\% with disambiguation,
(ii) consistently stronger counter bias behaviors in Urdu than in English, and
(iii) marked framing effects that reduce stereotypical responses when questions
are posed negatively. These findings highlight the importance of contextualized
benchmarks and simple prompt engineering strategies for bias mitigation in low
resource settings.

</details>


### [178] [Efficient Forward-Only Data Valuation for Pretrained LLMs and VLMs](https://arxiv.org/abs/2508.10180)
*Wenlong Deng,Jiaming Zhang,Qi Zeng,Christos Thrampoulidis,Boying Gong,Xiaoxiao Li*

Main category: cs.CL

TL;DR: For-Value是一种前向数据评估框架，通过单次前向传播高效量化大型语言模型和视觉语言模型中训练样本的影响力，无需昂贵的梯度计算。


<details>
  <summary>Details</summary>
Motivation: 量化单个训练样本的影响力对于提高大型语言模型（LLMs）和视觉语言模型（VLMs）的透明度和可解释性至关重要。然而，现有数据评估方法依赖于Hessian信息或模型再训练，对于数十亿参数的模型而言计算成本过高。

Method: 引入了For-Value框架，该框架利用现代基础模型的丰富表示，通过一个简单的闭式表达式（仅基于单次前向传播）计算影响力分数，从而避免了昂贵的梯度计算。理论分析表明，For-Value通过捕获训练样本和验证样本之间隐藏表示和预测误差的对齐来准确估计每个样本的影响力。

Result: For-Value能够准确估计每个样本的影响力，并且在识别有影响力的微调示例和有效检测错误标记数据方面，其性能与基于梯度的基线方法相当或更优。

Conclusion: For-Value提供了一种可扩展且高效的数据评估方法，能够量化LLMs和VLMs中训练样本的影响力，从而增强模型的透明度和可解释性。

Abstract: Quantifying the influence of individual training samples is essential for
enhancing the transparency and accountability of large language models (LLMs)
and vision-language models (VLMs). However, existing data valuation methods
often rely on Hessian information or model retraining, making them
computationally prohibitive for billion-parameter models. In this work, we
introduce For-Value, a forward-only data valuation framework that enables
scalable and efficient influence estimation for both LLMs and VLMs. By
leveraging the rich representations of modern foundation models, For-Value
computes influence scores using a simple closed-form expression based solely on
a single forward pass, thereby eliminating the need for costly gradient
computations. Our theoretical analysis demonstrates that For-Value accurately
estimates per-sample influence by capturing alignment in hidden representations
and prediction errors between training and validation samples. Extensive
experiments show that For-Value matches or outperforms gradient-based baselines
in identifying impactful fine-tuning examples and effectively detecting
mislabeled data.

</details>


### [179] [Prompt-Response Semantic Divergence Metrics for Faithfulness Hallucination and Misalignment Detection in Large Language Models](https://arxiv.org/abs/2508.10192)
*Igor Halperin*

Main category: cs.CL

TL;DR: 解析错误


<details>
  <summary>Details</summary>
Motivation: 解析错误

Method: 解析错误

Result: 解析错误

Conclusion: 解析错误

Abstract: The proliferation of Large Language Models (LLMs) is challenged by
hallucinations, critical failure modes where models generate non-factual,
nonsensical or unfaithful text. This paper introduces Semantic Divergence
Metrics (SDM), a novel lightweight framework for detecting Faithfulness
Hallucinations -- events of severe deviations of LLMs responses from input
contexts. We focus on a specific implementation of these LLM errors,
{confabulations, defined as responses that are arbitrary and semantically
misaligned with the user's query. Existing methods like Semantic Entropy test
for arbitrariness by measuring the diversity of answers to a single, fixed
prompt. Our SDM framework improves upon this by being more prompt-aware: we
test for a deeper form of arbitrariness by measuring response consistency not
only across multiple answers but also across multiple, semantically-equivalent
paraphrases of the original prompt. Methodologically, our approach uses joint
clustering on sentence embeddings to create a shared topic space for prompts
and answers. A heatmap of topic co-occurances between prompts and responses can
be viewed as a quantified two-dimensional visualization of the user-machine
dialogue. We then compute a suite of information-theoretic metrics to measure
the semantic divergence between prompts and responses. Our practical score,
$\mathcal{S}_H$, combines the Jensen-Shannon divergence and Wasserstein
distance to quantify this divergence, with a high score indicating a
Faithfulness hallucination. Furthermore, we identify the KL divergence
KL(Answer $||$ Prompt) as a powerful indicator of \textbf{Semantic
Exploration}, a key signal for distinguishing different generative behaviors.
These metrics are further combined into the Semantic Box, a diagnostic
framework for classifying LLM response types, including the dangerous,
confident confabulation.

</details>


### [180] [Understanding Textual Emotion Through Emoji Prediction](https://arxiv.org/abs/2508.10222)
*Ethan Gordon,Nishank Kuppa,Rigved Tummala,Sriram Anasuri*

Main category: cs.CL

TL;DR: 本项目使用四种深度学习架构（前馈网络、CNN、Transformer、BERT）在短文本序列上进行表情符号预测，并通过焦点损失和正则化处理类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 通过情感感知的表情符号预测来改善人机交互。

Method: 使用TweetEval数据集，探索了前馈网络、CNN、Transformer和BERT四种深度学习架构。通过焦点损失（focal loss）和正则化技术解决类别不平衡问题。

Result: BERT由于其预训练优势，取得了最高的整体性能；CNN在稀有表情符号类别上表现出卓越的效力。

Conclusion: 研究表明，架构选择和超参数调整对于情感感知的表情符号预测至关重要，有助于改善人机交互。

Abstract: This project explores emoji prediction from short text sequences using four
deep learning architectures: a feed-forward network, CNN, transformer, and
BERT. Using the TweetEval dataset, we address class imbalance through focal
loss and regularization techniques. Results show BERT achieves the highest
overall performance due to its pre-training advantage, while CNN demonstrates
superior efficacy on rare emoji classes. This research shows the importance of
architecture selection and hyperparameter tuning for sentiment-aware emoji
prediction, contributing to improved human-computer interaction.

</details>


### [181] [Using Large Language Models to Measure Symptom Severity in Patients At Risk for Schizophrenia](https://arxiv.org/abs/2508.10226)
*Andrew X. Chen,Guillermo Horga,Sean Escola*

Main category: cs.CL

TL;DR: 研究利用大型语言模型（LLM）从临床访谈转录本中预测精神分裂症临床高危（CHR）患者的简明精神病评定量表（BPRS）分数，结果显示LLM的预测性能接近人类评估者信度，并能支持多语言和纵向评估。


<details>
  <summary>Details</summary>
Motivation: 简明精神病评定量表（BPRS）是评估精神分裂症及其他精神病性障碍症状的有效工具，但因需要耗时的结构化访谈，在临床实践中不常使用。因此，需要一种更高效、标准化的方法来监测临床高危患者的症状。

Method: 研究利用大型语言模型（LLM），对来自加速药物合作精神分裂症（AMP-SCZ）队列的409名临床高危（CHR）患者的临床访谈转录本进行分析，以零样本（zero-shot）方式预测BPRS分数。此外，还评估了LLM在不同语言和整合纵向信息方面的表现。

Result: LLM的零样本预测与真实评估表现出高一致性（中位一致性：0.84，ICC：0.73），接近人类评估者间的信度和评估者内部信度。研究还表明，LLM在评估外语（中位一致性：0.88，ICC：0.70）和通过一次性或少样本学习整合纵向信息方面也具有显著潜力。

Conclusion: 大型语言模型（LLM）在精神分裂症临床高危患者的症状评估中具有巨大潜力，能够显著提高BPRS评估的效率和标准化程度，并支持多语言和纵向数据整合。

Abstract: Patients who are at clinical high risk (CHR) for schizophrenia need close
monitoring of their symptoms to inform appropriate treatments. The Brief
Psychiatric Rating Scale (BPRS) is a validated, commonly used research tool for
measuring symptoms in patients with schizophrenia and other psychotic
disorders; however, it is not commonly used in clinical practice as it requires
a lengthy structured interview. Here, we utilize large language models (LLMs)
to predict BPRS scores from clinical interview transcripts in 409 CHR patients
from the Accelerating Medicines Partnership Schizophrenia (AMP-SCZ) cohort.
Despite the interviews not being specifically structured to measure the BPRS,
the zero-shot performance of the LLM predictions compared to the true
assessment (median concordance: 0.84, ICC: 0.73) approaches human inter- and
intra-rater reliability. We further demonstrate that LLMs have substantial
potential to improve and standardize the assessment of CHR patients via their
accuracy in assessing the BPRS in foreign languages (median concordance: 0.88,
ICC: 0.70), and integrating longitudinal information in a one-shot or few-shot
learning approach.

</details>


### [182] [A Computational Approach to Analyzing Language Change and Variation in the Constructed Language Toki Pona](https://arxiv.org/abs/2508.10246)
*Daniel Huang,Hyoun-A Joo*

Main category: cs.CL

TL;DR: 本研究通过计算和语料库方法，探讨了人造语言Toki Pona的语言变化和变异，发现其演变受社会语言学因素影响，与自然语言类似。


<details>
  <summary>Details</summary>
Motivation: 探讨人造语言Toki Pona在社区使用过程中，其语言系统是否会像自然语言一样发生演变和变异，以及社会语言学因素对其影响。

Method: 采用计算和语料库方法，分析Toki Pona中词类的流动性和及物性特征，以考察内容词在不同句法位置上的偏好随时间的变化，以及不同语料库之间的用法差异。

Result: 研究结果表明，社会语言学因素以与自然语言相同的方式影响Toki Pona，即使是人造语言系统，在社区使用过程中也会自然演变。

Conclusion: 社区的使用会使人造语言系统自然演变，并受社会语言学因素影响，这与自然语言的演变规律相似。

Abstract: This study explores language change and variation in Toki Pona, a constructed
language with approximately 120 core words. Taking a computational and
corpus-based approach, the study examines features including fluid word classes
and transitivity in order to examine (1) changes in preferences of content
words for different syntactic positions over time and (2) variation in usage
across different corpora. The results suggest that sociolinguistic factors
influence Toki Pona in the same way as natural languages, and that even
constructed linguistic systems naturally evolve as communities use them.

</details>


### [183] [Yet another algorithmic bias: A Discursive Analysis of Large Language Models Reinforcing Dominant Discourses on Gender and Race](https://arxiv.org/abs/2508.10304)
*Gustavo Bonil,Simone Hashiguti,Jhessica Silva,João Gondim,Helena Maia,Nádia Silva,Helio Pedrini,Sandra Avila*

Main category: cs.CL

TL;DR: 本研究提出并应用定性分析框架，揭示大型语言模型（LLMs）在生成故事时如何再现并固化种族和性别偏见，强调现有自动化检测方法的不足。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的广泛应用，评估它们是否会复制偏见（如歧视和种族化）并维持霸权话语变得至关重要。当前主流的定量、自动化偏见检测方法常忽略自然语言中偏见出现的细微方式，因此需要更深入的分析方法。

Method: 本研究提出一种定性、话语分析框架来补充现有方法。通过人工分析LLM生成的涉及黑人女性和白人女性的短篇故事，调查了性别和种族偏见。研究还测试了模型在被提示纠正偏见时的表现。

Result: 结果显示，黑人女性被描绘为与祖先和抵抗相关联，而白人女性则出现在自我发现的过程中。这些模式反映了语言模型如何复制固化的语篇表征，强化了本质化和社会固化感。当被要求纠正偏见时，模型提供了肤浅的修改，但保留了有问题含义，暴露出在培养包容性叙事方面的局限性。

Conclusion: 定性方法对于帮助开发者和用户识别LLM输出中偏见的精确表现形式至关重要，从而更好地缓解偏见。研究结果揭示了算法的意识形态功能，对AI的伦理使用和发展具有重要意义。强调了在AI设计和部署中采用批判性、跨学科方法以解决LLM生成话语如何反映和延续不平等的需求。

Abstract: With the advance of Artificial Intelligence (AI), Large Language Models
(LLMs) have gained prominence and been applied in diverse contexts. As they
evolve into more sophisticated versions, it is essential to assess whether they
reproduce biases, such as discrimination and racialization, while maintaining
hegemonic discourses. Current bias detection approaches rely mostly on
quantitative, automated methods, which often overlook the nuanced ways in which
biases emerge in natural language. This study proposes a qualitative,
discursive framework to complement such methods. Through manual analysis of
LLM-generated short stories featuring Black and white women, we investigate
gender and racial biases. We contend that qualitative methods such as the one
proposed here are fundamental to help both developers and users identify the
precise ways in which biases manifest in LLM outputs, thus enabling better
conditions to mitigate them. Results show that Black women are portrayed as
tied to ancestry and resistance, while white women appear in self-discovery
processes. These patterns reflect how language models replicate crystalized
discursive representations, reinforcing essentialization and a sense of social
immobility. When prompted to correct biases, models offered superficial
revisions that maintained problematic meanings, revealing limitations in
fostering inclusive narratives. Our results demonstrate the ideological
functioning of algorithms and have significant implications for the ethical use
and development of AI. The study reinforces the need for critical,
interdisciplinary approaches to AI design and deployment, addressing how
LLM-generated discourses reflect and perpetuate inequalities.

</details>


### [184] [Inductive Bias Extraction and Matching for LLM Prompts](https://arxiv.org/abs/2508.10295)
*Christian M. Angel,Francis Ferraro*

Main category: cs.CL

TL;DR: 该研究提出一种“归纳偏置提取与匹配”策略，通过使用LLM自身输出作为部分提示，来创建更符合模型归纳偏置的提示词，从而显著提高LLM在分类和排序任务上的表现。


<details>
  <summary>Details</summary>
Motivation: LLM对提示词的微小变化非常敏感，这部分归因于其固有的归纳偏置。如何高效地创建令人满意的提示词是一个活跃的研究课题。

Method: 通过将LLM的输出作为其提示词的一部分，可以更容易地创建出与模型归纳偏置相匹配的提示词。这种方法被称为“归纳偏置提取与匹配”。

Result: 实验结果表明，该策略将LLM在分类任务上的Likert评分提高了高达19%，在排序任务上的Likert评分提高了高达27%。

Conclusion: 利用LLM自身的输出调整提示词，使其与模型的归纳偏置相匹配，能够有效提升LLM在分类和排序等任务中的性能。

Abstract: The active research topic of prompt engineering makes it evident that LLMs
are sensitive to small changes in prompt wording. A portion of this can be
ascribed to the inductive bias that is present in the LLM. By using an LLM's
output as a portion of its prompt, we can more easily create satisfactory
wording for prompts. This has the effect of creating a prompt that matches the
inductive bias in model. Empirically, we show that using this Inductive Bias
Extraction and Matching strategy improves LLM Likert ratings used for
classification by up to 19% and LLM Likert ratings used for ranking by up to
27%.

</details>


### [185] [ReviewRL: Towards Automated Scientific Review with RL](https://arxiv.org/abs/2508.10308)
*Sihang Zeng,Kai Tian,Kaiyan Zhang,Yuru wang,Junqi Gao,Runze Liu,Sa Yang,Jingxuan Li,Xinwei Long,Jiaheng Ma,Biqing Qi,Bowen Zhou*

Main category: cs.CL

TL;DR: ReviewRL是一个基于强化学习的框架，旨在生成全面且事实准确的科学论文评审，以应对同行评审面临的挑战。


<details>
  <summary>Details</summary>
Motivation: 科学同行评审面临提交量增加和审稿人疲劳的挑战，而现有自动化评审方法在事实准确性、评分一致性和分析深度方面表现不佳，缺乏高质量人工评审的洞察力。

Method: ReviewRL结合了三部分：1) ArXiv-MCP检索增强的上下文生成管道，以整合相关科学文献；2) 监督微调，建立基础评审能力；3) 带有复合奖励函数的强化学习过程，共同提高评审质量和评分准确性。

Result: 在ICLR 2025论文上的实验表明，ReviewRL在基于规则的指标和基于模型的质量评估方面均显著优于现有方法。

Conclusion: ReviewRL为科学发现中的RL驱动自动评论生成建立了一个基础框架，展示了在该领域未来发展的巨大潜力。

Abstract: Peer review is essential for scientific progress but faces growing challenges
due to increasing submission volumes and reviewer fatigue. Existing automated
review approaches struggle with factual accuracy, rating consistency, and
analytical depth, often generating superficial or generic feedback lacking the
insights characteristic of high-quality human reviews. We introduce ReviewRL, a
reinforcement learning framework for generating comprehensive and factually
grounded scientific paper reviews. Our approach combines: (1) an ArXiv-MCP
retrieval-augmented context generation pipeline that incorporates relevant
scientific literature, (2) supervised fine-tuning that establishes foundational
reviewing capabilities, and (3) a reinforcement learning procedure with a
composite reward function that jointly enhances review quality and rating
accuracy. Experiments on ICLR 2025 papers demonstrate that ReviewRL
significantly outperforms existing methods across both rule-based metrics and
model-based quality assessments. ReviewRL establishes a foundational framework
for RL-driven automatic critique generation in scientific discovery,
demonstrating promising potential for future development in this domain. The
implementation of ReviewRL will be released at GitHub.

</details>


### [186] [From Surface to Semantics: Semantic Structure Parsing for Table-Centric Document Analysis](https://arxiv.org/abs/2508.10311)
*Xuan Li,Jialiang Dong,Raymond Wong*

Main category: cs.CL

TL;DR: DOTABLER是一个以表格为中心的语义文档解析框架，旨在发现表格与其上下文之间的深层语义联系，并支持高级文档理解和信息提取任务。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中在表格的表面任务（如布局分析、检测、数据提取），缺乏对表格及其上下文关联的深层语义解析，这限制了跨段落数据解释和上下文一致性分析等高级任务的实现。

Method: 该研究提出了DOTABLER框架，利用自定义数据集和预训练模型的领域特定微调，整合完整的解析管道来识别与表格语义相关的上下文片段。在此语义理解基础上，DOTABLER实现了两个核心功能：以表格为中心的文档结构解析和领域特定表格检索。

Result: 在近4000页（包含1000多个表格）的真实PDF文件上进行评估，DOTABLER在表格-上下文语义分析和深度文档解析方面取得了超过90%的精确率和F1分数，性能优于GPT-4o等先进模型。

Conclusion: DOTABLER框架在表格-上下文语义分析和深度文档解析方面表现出卓越性能，能够实现全面的以表格为中心的语义分析和精确的语义相关表格提取。

Abstract: Documents are core carriers of information and knowl-edge, with broad
applications in finance, healthcare, and scientific research. Tables, as the
main medium for structured data, encapsulate key information and are among the
most critical document components. Existing studies largely focus on
surface-level tasks such as layout analysis, table detection, and data
extraction, lacking deep semantic parsing of tables and their contextual
associations. This limits advanced tasks like cross-paragraph data
interpretation and context-consistent analysis. To address this, we propose
DOTABLER, a table-centric semantic document parsing framework designed to
uncover deep semantic links between tables and their context. DOTABLER
leverages a custom dataset and domain-specific fine-tuning of pre-trained
models, integrating a complete parsing pipeline to identify context segments
semantically tied to tables. Built on this semantic understanding, DOTABLER
implements two core functionalities: table-centric document structure parsing
and domain-specific table retrieval, delivering comprehensive table-anchored
semantic analysis and precise extraction of semantically relevant tables.
Evaluated on nearly 4,000 pages with over 1,000 tables from real-world PDFs,
DOTABLER achieves over 90% Precision and F1 scores, demonstrating superior
performance in table-context semantic analysis and deep document parsing
compared to advanced models such as GPT-4o.

</details>


### [187] [Layer-Wise Perturbations via Sparse Autoencoders for Adversarial Text Generation](https://arxiv.org/abs/2508.10404)
*Huizhen Shu,Xuying Li,Qirui Wang,Yuji Kosuga,Mengqiu Tian,Zhuo Li*

Main category: cs.CL

TL;DR: 提出了一种名为SFPF的黑盒对抗攻击方法，利用稀疏自编码器识别和扰动LLM文本中的关键特征，以生成能绕过现有防御的对抗样本，揭示模型漏洞。


<details>
  <summary>Details</summary>
Motivation: 随着NLP特别是LLM的快速发展，生成对抗性示例来“越狱”LLM仍然是理解模型漏洞和提高鲁棒性的关键挑战。

Method: 引入稀疏特征扰动框架（SFPF），利用稀疏自编码器（SAE）重建隐藏层表示，对成功攻击的文本进行特征聚类以识别高激活特征。然后，选择性地扰动这些高激活特征来生成新的对抗文本，同时保留恶意意图并放大安全信号。

Result: SFPF生成的对抗文本能够绕过最先进的防御机制，揭示了当前NLP系统中持续存在的漏洞。然而，该方法的有效性因提示和层而异，其对其他架构和更大模型的泛化能力仍有待验证。

Conclusion: 该方法提供了一种新的红队策略，平衡了对抗有效性与安全对齐，并揭示了LLM防御机制中存在的持续漏洞。

Abstract: With the rapid proliferation of Natural Language Processing (NLP), especially
Large Language Models (LLMs), generating adversarial examples to jailbreak LLMs
remains a key challenge for understanding model vulnerabilities and improving
robustness. In this context, we propose a new black-box attack method that
leverages the interpretability of large models. We introduce the Sparse Feature
Perturbation Framework (SFPF), a novel approach for adversarial text generation
that utilizes sparse autoencoders to identify and manipulate critical features
in text. After using the SAE model to reconstruct hidden layer representations,
we perform feature clustering on the successfully attacked texts to identify
features with higher activations. These highly activated features are then
perturbed to generate new adversarial texts. This selective perturbation
preserves the malicious intent while amplifying safety signals, thereby
increasing their potential to evade existing defenses. Our method enables a new
red-teaming strategy that balances adversarial effectiveness with safety
alignment. Experimental results demonstrate that adversarial texts generated by
SFPF can bypass state-of-the-art defense mechanisms, revealing persistent
vulnerabilities in current NLP systems.However, the method's effectiveness
varies across prompts and layers, and its generalizability to other
architectures and larger models remains to be validated.

</details>


### [188] [Beyond Semantic Understanding: Preserving Collaborative Frequency Components in LLM-based Recommendation](https://arxiv.org/abs/2508.10312)
*Minhao Wang,Yunhang He,Cong Xu,Zhangchi Zhu,Wei Zhang*

Main category: cs.CL

TL;DR: LLM推荐系统倾向于过度强调语义而削弱协同信号。FreLLM4Rec从频谱角度出发，通过全局图低通滤波器（G-LPF）和时间频率调制（TFM）来平衡语义和协同信息，有效缓解了协同信号衰减并提升了推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的推荐系统在生成语义丰富的推荐方面表现出色，但当输入协同ID嵌入时，它们倾向于过度强调用户交互历史中的语义关联，导致固有的协同信号在LLM骨干网络中逐层衰减，这与传统Transformer模型中协同信号被保留或增强的情况相反。

Method: 提出FreLLM4Rec方法，旨在平衡语义和协同信息。首先，使用全局图低通滤波器（G-LPF）对结合了语义和协同信息的物品嵌入进行初步净化，以去除不相关的。其次，通过时间频率调制（TFM）逐层主动保留协同信号。TFM的协同保留能力通过建立最优局部图傅里叶滤波器与高效频域滤波器之间的联系而得到理论保证。

Result: 在四个基准数据集上进行的广泛实验表明，FreLLM4Rec成功缓解了协同信号衰减，并取得了有竞争力的性能，相较于最佳基线，NDCG@10指标提升高达8.00%。

Conclusion: 本研究的发现为LLM如何处理协同信息提供了深入见解，并为改进基于LLM的推荐系统提供了一种原则性的方法。

Abstract: Recommender systems in concert with Large Language Models (LLMs) present
promising avenues for generating semantically-informed recommendations.
However, LLM-based recommenders exhibit a tendency to overemphasize semantic
correlations within users' interaction history. When taking pretrained
collaborative ID embeddings as input, LLM-based recommenders progressively
weaken the inherent collaborative signals as the embeddings propagate through
LLM backbones layer by layer, as opposed to traditional Transformer-based
sequential models in which collaborative signals are typically preserved or
even enhanced for state-of-the-art performance. To address this limitation, we
introduce FreLLM4Rec, an approach designed to balance semantic and
collaborative information from a spectral perspective. Item embeddings that
incorporate both semantic and collaborative information are first purified
using a Global Graph Low-Pass Filter (G-LPF) to preliminarily remove irrelevant
high-frequency noise. Temporal Frequency Modulation (TFM) then actively
preserves collaborative signal layer by layer. Note that the collaborative
preservation capability of TFM is theoretically guaranteed by establishing a
connection between the optimal but hard-to-implement local graph fourier
filters and the suboptimal yet computationally efficient frequency-domain
filters. Extensive experiments on four benchmark datasets demonstrate that
FreLLM4Rec successfully mitigates collaborative signal attenuation and achieves
competitive performance, with improvements of up to 8.00\% in NDCG@10 over the
best baseline. Our findings provide insights into how LLMs process
collaborative information and offer a principled approach for improving
LLM-based recommendation systems.

</details>


### [189] [ComoRAG: A Cognitive-Inspired Memory-Organized RAG for Stateful Long Narrative Reasoning](https://arxiv.org/abs/2508.10419)
*Juyuan Wang,Rongchen Zhao,Wei Wei,Yufeng Wang,Mo Yu,Jie Zhou,Jin Xu,Liyan Xu*

Main category: cs.CL

TL;DR: ComoRAG是一种新的检索增强生成（RAG）方法，通过迭代推理和动态记忆工作空间，解决了长篇叙事文本理解中传统RAG的局限性，特别适用于需要全局理解的复杂查询。


<details>
  <summary>Details</summary>
Motivation: 长篇故事和小说理解面临挑战，因为情节复杂、人物关系动态。大型语言模型（LLM）在长上下文推理能力下降且计算成本高昂。传统RAG方法因其无状态、单步检索过程，难以捕捉长上下文中的动态互联关系。

Method: 提出ComoRAG，其核心思想是叙事推理是一个动态演变的过程，而非一次性完成。当遇到推理障碍时，ComoRAG通过与动态记忆工作空间交互，进行迭代推理循环。在每个循环中，它生成探测查询以探索新路径，然后将检索到的新证据整合到全局记忆池中，以支持查询解析的连贯上下文的出现。

Result: 在四个具有挑战性的长上下文叙事基准（20万+tokens）上，ComoRAG的性能优于强大的RAG基线，与最强的基线相比，相对增益高达11%。进一步分析表明，ComoRAG特别适用于需要全局理解的复杂查询。

Conclusion: ComoRAG为检索增强的长上下文理解提供了一个有原则、受认知启发的范式，能够实现有状态的推理。它证明了叙事推理是一个动态的、新证据获取与旧知识整合之间相互作用的过程。

Abstract: Narrative comprehension on long stories and novels has been a challenging
domain attributed to their intricate plotlines and entangled, often evolving
relations among characters and entities. Given the LLM's diminished reasoning
over extended context and high computational cost, retrieval-based approaches
remain a pivotal role in practice. However, traditional RAG methods can fall
short due to their stateless, single-step retrieval process, which often
overlooks the dynamic nature of capturing interconnected relations within
long-range context. In this work, we propose ComoRAG, holding the principle
that narrative reasoning is not a one-shot process, but a dynamic, evolving
interplay between new evidence acquisition and past knowledge consolidation,
analogous to human cognition when reasoning with memory-related signals in the
brain. Specifically, when encountering a reasoning impasse, ComoRAG undergoes
iterative reasoning cycles while interacting with a dynamic memory workspace.
In each cycle, it generates probing queries to devise new exploratory paths,
then integrates the retrieved evidence of new aspects into a global memory
pool, thereby supporting the emergence of a coherent context for the query
resolution. Across four challenging long-context narrative benchmarks (200K+
tokens), ComoRAG outperforms strong RAG baselines with consistent relative
gains up to 11% compared to the strongest baseline. Further analysis reveals
that ComoRAG is particularly advantageous for complex queries requiring global
comprehension, offering a principled, cognitively motivated paradigm for
retrieval-based long context comprehension towards stateful reasoning. Our code
is publicly released at https://github.com/EternityJune25/ComoRAG

</details>


### [190] [Cross-Prompt Encoder for Low-Performing Languages](https://arxiv.org/abs/2508.10352)
*Beso Mikaberidze,Teimuraz Saghinadze,Simon Ostermann,Philipp Muller*

Main category: cs.CL

TL;DR: 该研究引入了跨语言提示编码器（XPE）和双软提示机制，以提升大语言模型在多语言参数高效微调（PEFT）中的性能，尤其针对低表现语言。


<details>
  <summary>Details</summary>
Motivation: 现有软提示方法在跨语言迁移方面的潜力尚未充分探索，尤其未能有效提升在全模型微调下表现依然不佳的低表现语言的性能。本研究旨在利用提示编码器来解决这一问题。

Method: 提出了跨语言提示编码器（XPE），它结合了轻量级编码架构和在多种语言上的多源训练，以捕捉跨语言的抽象可迁移模式。同时，引入了双软提示机制，该机制结合了基于编码器的提示和直接训练的标准软提示，以兼顾共享结构和语言特异性对齐。

Result: 在SIB-200基准测试上的实验表明，XPE对低表现语言最为有效。而混合变体（双软提示）在多语言设置中提供了更广泛的适应性，对同时受益于共享结构和语言特异性对齐的目标语言尤其有效。

Conclusion: 提示编码器在提升低表现语言性能方面发挥着核心作用。XPE和双软提示机制的结合，为多语言PEFT提供了一种有效策略，能够提升大语言模型在各种多语言环境下的适应性和性能。

Abstract: Soft prompts have emerged as a powerful alternative to adapters in
parameter-efficient fine-tuning (PEFT), enabling large language models (LLMs)
to adapt to downstream tasks without architectural changes or parameter
updates. While prior work has focused on stabilizing training via parameter
interaction in small neural prompt encoders, their broader potential for
transfer across languages remains unexplored. In this paper, we demonstrate
that a prompt encoder can play a central role in improving performance on
low-performing languages-those that achieve poor accuracy even under full-model
fine-tuning. We introduce the Cross-Prompt Encoder (XPE), which combines a
lightweight encoding architecture with multi-source training on typologically
diverse languages - a design that enables the model to capture abstract and
transferable patterns across languages. To complement XPE, we propose a Dual
Soft Prompt mechanism that combines an encoder-based prompt with a directly
trained standard soft prompt. This hybrid design proves especially effective
for target languages that benefit from both broadly shared structure and
language-specific alignment. Experiments on the SIB-200 benchmark reveal a
consistent trade-off: XPE is most effective for low-performing languages, while
hybrid variants offer broader adaptability across multilingual settings.

</details>


### [191] [Making Qwen3 Think in Korean with Reinforcement Learning](https://arxiv.org/abs/2508.10355)
*Jungyup Lee,Jemin Kim,Sang Park,SeungJae Lee*

Main category: cs.CL

TL;DR: 本文提出两阶段微调方法，使Qwen3 14B模型能用韩语进行原生“思考”，显著提升了其韩语推理能力和整体问题解决性能，特别是在数学和编程任务上。


<details>
  <summary>Details</summary>
Motivation: 使大型语言模型Qwen3 14B能够原生用韩语进行“思考”，建立强大的韩语逻辑推理基础，并提升其整体问题解决能力。同时，解决GRPO训练中常见的稳定性挑战，如奖励作弊和策略崩溃。

Method: 采用两阶段微调方法：第一阶段，对Qwen3 14B进行监督微调（SFT），使用高质量韩语推理数据集。第二阶段，采用定制的群组相对策略优化（GRPO）算法进行强化学习，并引入一个“预言者评判模型”来校准奖励信号，以解决GRPO训练中的稳定性问题（如奖励作弊和策略崩溃）。

Result: 第一阶段SFT显著提升了模型在韩语任务上的表现，并对通用推理能力有所增益。第二阶段RL（结合预言者评判模型）实现了稳定的学习，避免了策略崩溃，并带来了持续、渐进的性能提升。最终的RL微调模型在高级推理基准测试（特别是数学和编程任务）上表现出显著改善，同时保持了知识和语言熟练度，并能完全用韩语进行内部思维链推理。

Conclusion: 所提出的两阶段微调方法，尤其是通过引入预言者评判模型稳定GRPO训练，成功使Qwen3 14B模型能够原生用韩语进行思考，并大幅提升了其在复杂推理任务上的性能，验证了该方法的有效性。

Abstract: We present a two-stage fine-tuning approach to make the large language model
Qwen3 14B "think" natively in Korean. In the first stage, supervised
fine-tuning (SFT) on a high-quality Korean reasoning dataset establishes a
strong foundation in Korean logical reasoning, yielding notable improvements in
Korean-language tasks and even some gains in general reasoning ability. In the
second stage, we employ reinforcement learning with a customized Group Relative
Policy Optimization (GRPO) algorithm to further enhance both Korean reasoning
alignment and overall problem-solving performance. We address critical
stability challenges in GRPO training - such as reward hacking and policy
collapse - by introducing an oracle judge model that calibrates the reward
signal. Our approach achieves stable learning (avoiding the collapse observed
in naive GRPO) and leads to steady, incremental performance gains. The final
RL-tuned model demonstrates substantially improved results on advanced
reasoning benchmarks (particularly math and coding tasks) while maintaining
knowledge and language proficiency, successfully conducting its internal
chain-of-thought entirely in Korean.

</details>


### [192] [Advancing Cross-lingual Aspect-Based Sentiment Analysis with LLMs and Constrained Decoding for Sequence-to-Sequence Models](https://arxiv.org/abs/2508.10366)
*Jakub Šmíd,Pavel Přibáň,Pavel Král*

Main category: cs.CL

TL;DR: 本文提出一种新的序列到序列方法，用于跨语言复合ABSA任务，无需外部翻译工具，通过使用约束解码，显著提升了性能，并与大型语言模型进行了比较。


<details>
  <summary>Details</summary>
Motivation: 当前跨语言ABSA研究主要集中在英语，导致低资源语言面临挑战，且现有方法多限于简单任务并严重依赖外部翻译工具。

Method: 提出一种新颖的序列到序列方法，用于处理复合ABSA任务。该方法通过使用约束解码，消除了对外部翻译工具的依赖。

Result: 该方法将跨语言ABSA性能提高了高达10%；它拓宽了跨语言ABSA的范围，使其能够处理更复杂的任务，并提供了一种实用、高效的替代方案。与大型语言模型（LLM）的比较显示，微调的多语言LLM可以达到可比的结果，但以英语为中心的LLM在这些任务上表现不佳。

Conclusion: 该方法拓展了跨语言ABSA处理复杂任务的能力，提供了一种无需翻译工具的实用高效替代方案。研究还表明，微调的多语言LLM表现良好，而以英语为中心的LLM在这些任务上存在局限性。

Abstract: Aspect-based sentiment analysis (ABSA) has made significant strides, yet
challenges remain for low-resource languages due to the predominant focus on
English. Current cross-lingual ABSA studies often centre on simpler tasks and
rely heavily on external translation tools. In this paper, we present a novel
sequence-to-sequence method for compound ABSA tasks that eliminates the need
for such tools. Our approach, which uses constrained decoding, improves
cross-lingual ABSA performance by up to 10\%. This method broadens the scope of
cross-lingual ABSA, enabling it to handle more complex tasks and providing a
practical, efficient alternative to translation-dependent techniques.
Furthermore, we compare our approach with large language models (LLMs) and show
that while fine-tuned multilingual LLMs can achieve comparable results,
English-centric LLMs struggle with these tasks.

</details>


### [193] [When Language Overrules: Revealing Text Dominance in Multimodal Large Language Models](https://arxiv.org/abs/2508.10552)
*Huyu Wu,Meng Tang,Xinhan Zheng,Haiyun Jiang*

Main category: cs.CL

TL;DR: 多模态大语言模型（MLLMs）存在“文本主导”问题，即过度依赖文本而未充分利用其他模态。本文首次系统性地在多种模态下调查此问题，提出了衡量指标，揭示了其成因，并提出了一种有效的Token压缩方法来平衡模型注意力。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在多模态任务中表现出色，但存在一个核心问题：它们过度依赖文本进行推理，而未能充分利用其他模态。尽管此前工作已在视觉-语言任务中提及此现象，但缺乏对跨越多种数据模态的系统性研究。

Method: 1. 首次系统性调查文本主导现象，涵盖图像、视频、音频、时间序列和图表等多种模态。2. 提出两种评估指标：模态主导指数（MDI）和注意力效率指数（AEI）来量化这种不平衡。3. 深入分析识别了导致文本主导的潜在原因。4. 提出一种简单的Token压缩方法以有效重新平衡模型注意力。

Result: 1. 文本主导现象在所有测试模态中都显著且普遍存在。2. 识别出三个根本原因：非文本模态中严重的Token冗余导致注意力稀释；融合架构设计的影响；以及隐性偏向文本输入的任务表述。3. 所提出的Token压缩方法能有效重新平衡模型注意力，例如将LLaVA-7B的MDI从10.23大幅降低至0.86。

Conclusion: 本文的分析和方法框架为开发更公平、更全面的多模态语言模型奠定了基础，有助于解决MLLMs中普遍存在的文本主导问题。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities across a diverse range of multimodal tasks. However, these models
suffer from a core problem known as text dominance: they depend heavily on text
for their inference, while underutilizing other modalities. While prior work
has acknowledged this phenomenon in vision-language tasks, often attributing it
to data biases or model architectures. In this paper, we conduct the first
systematic investigation of text dominance across diverse data modalities,
including images, videos, audio, time-series, and graphs. To measure this
imbalance, we propose two evaluation metrics: the Modality Dominance Index
(MDI) and the Attention Efficiency Index (AEI). Our comprehensive analysis
reveals that text dominance is both significant and pervasive across all tested
modalities. Our in-depth analysis identifies three underlying causes: attention
dilution from severe token redundancy in non-textual modalities, the influence
of fusion architecture design, and task formulations that implicitly favor
textual inputs. Furthermore, we propose a simple token compression method that
effectively rebalances model attention. Applying this method to LLaVA-7B, for
instance, drastically reduces its MDI from 10.23 to a well-balanced value of
0.86. Our analysis and methodological framework offer a foundation for the
development of more equitable and comprehensive multimodal language models.

</details>


### [194] [Large Language Models for Summarizing Czech Historical Documents and Beyond](https://arxiv.org/abs/2508.10368)
*Václav Tran,Jakub Šmíd,Jiří Martínek,Ladislav Lenc,Pavel Král*

Main category: cs.CL

TL;DR: 该研究利用大型语言模型（Mistral和mT5）显著提升了捷克语文本摘要的水平，在现代捷克语数据集SumeCzech上取得了最先进的成果，并首次引入了一个用于历史捷克语文档摘要的新数据集Posel od Čerchova。


<details>
  <summary>Details</summary>
Motivation: 文本摘要在英语等高资源语言中已得到广泛探索，但捷克语文本摘要，特别是针对历史文献的摘要，因语言复杂性和标注数据集稀缺而研究不足。大型语言模型在多项自然语言处理任务和语言中表现出色。

Method: 采用大型语言模型，具体为Mistral和mT5，进行捷克语文本摘要。

Result: 1. 在现代捷克语摘要数据集SumeCzech上使用先进模型取得了新的最先进结果。
2. 引入了一个名为Posel od Čerchova的新数据集，用于历史捷克语文档的摘要，并提供了基线结果。

Conclusion: 这些贡献极大地推动了捷克语文本摘要领域的发展，并为捷克语历史文本处理研究开辟了新途径。

Abstract: Text summarization is the task of shortening a larger body of text into a
concise version while retaining its essential meaning and key information.
While summarization has been significantly explored in English and other
high-resource languages, Czech text summarization, particularly for historical
documents, remains underexplored due to linguistic complexities and a scarcity
of annotated datasets. Large language models such as Mistral and mT5 have
demonstrated excellent results on many natural language processing tasks and
languages. Therefore, we employ these models for Czech summarization, resulting
in two key contributions: (1) achieving new state-of-the-art results on the
modern Czech summarization dataset SumeCzech using these advanced models, and
(2) introducing a novel dataset called Posel od \v{C}erchova for summarization
of historical Czech documents with baseline results. Together, these
contributions provide a great potential for advancing Czech text summarization
and open new avenues for research in Czech historical text processing.

</details>


### [195] [Improving Generative Cross-lingual Aspect-Based Sentiment Analysis with Constrained Decoding](https://arxiv.org/abs/2508.10369)
*Jakub Šmíd,Pavel Přibáň,Pavel Král*

Main category: cs.CL

TL;DR: 本文提出了一种基于序列到序列模型和约束解码的跨语言方面级情感分析（ABSA）新方法，无需外部翻译工具，显著提升了低资源语言的性能，并支持多任务处理。同时评估了大型语言模型（LLMs）在不同设置下的表现。


<details>
  <summary>Details</summary>
Motivation: 现有跨语言ABSA方法在低资源语言上面临挑战，且通常依赖不可靠的外部翻译工具，同时关注的任务范围有限、复杂度较低。

Method: 引入了一种结合序列到序列模型的约束解码方法，消除了对外部翻译工具的依赖。该方法支持多任务处理，允许一个模型解决多个ABSA任务。研究在七种语言和六个ABSA任务上进行了评估，并对LLMs在零样本、少样本和微调场景下进行了测试。

Result: 该方法在最复杂的任务上将跨语言性能平均提升了5%；在多任务处理中，约束解码使结果提升了10%以上。新方法超越了现有最先进水平，并在未探索的任务上设定了新基准。LLMs在零样本和少样本设置下表现不佳，但在微调后能达到与小型多语言模型相当的竞争力，尽管训练和推理时间更长。

Conclusion: 该研究为跨语言ABSA方法提供了有价值的见解，增强了对其实践应用的理解，并推动了该挑战性研究领域的发展。提出了针对实际应用的实用建议。

Abstract: While aspect-based sentiment analysis (ABSA) has made substantial progress,
challenges remain for low-resource languages, which are often overlooked in
favour of English. Current cross-lingual ABSA approaches focus on limited, less
complex tasks and often rely on external translation tools. This paper
introduces a novel approach using constrained decoding with
sequence-to-sequence models, eliminating the need for unreliable translation
tools and improving cross-lingual performance by 5\% on average for the most
complex task. The proposed method also supports multi-tasking, which enables
solving multiple ABSA tasks with a single model, with constrained decoding
boosting results by more than 10\%.
  We evaluate our approach across seven languages and six ABSA tasks,
surpassing state-of-the-art methods and setting new benchmarks for previously
unexplored tasks. Additionally, we assess large language models (LLMs) in
zero-shot, few-shot, and fine-tuning scenarios. While LLMs perform poorly in
zero-shot and few-shot settings, fine-tuning achieves competitive results
compared to smaller multilingual models, albeit at the cost of longer training
and inference times.
  We provide practical recommendations for real-world applications, enhancing
the understanding of cross-lingual ABSA methodologies. This study offers
valuable insights into the strengths and limitations of cross-lingual ABSA
approaches, advancing the state-of-the-art in this challenging research domain.

</details>


### [196] [Jailbreaking Commercial Black-Box LLMs with Explicitly Harmful Prompts](https://arxiv.org/abs/2508.10390)
*Chiyu Zhang,Lu Zhou,Xiaogang Xu,Jiafei Wu,Liming Fang,Zhe Liu*

Main category: cs.CL

TL;DR: 该研究提出了一个名为MDH的混合评估框架，结合LLM和人工协助来高效准确地检测恶意内容和清洗数据集。同时，提出了两种新的越狱攻击策略D-Attack和DH-CoT，利用开发者消息显著提升越狱成功率。


<details>
  <summary>Details</summary>
Motivation: 现有越狱攻击评估面临挑战，因为许多红队数据集包含不适合的提示，导致难以准确评估。此外，现有的恶意内容检测方法要么依赖耗时的人工标注，要么依赖LLM但其在有害类型上准确性不一致。

Method: 1. 提出了MDH（基于LLM并辅以人工协助的恶意内容检测）混合评估框架，结合LLM标注和最少的人工监督，用于数据集清洗和检测越狱响应。2. 提出了两种新的越狱攻击策略：D-Attack（利用上下文模拟）和DH-CoT（结合劫持的思维链），通过精心设计的开发者消息来提高越狱成功率。

Result: MDH框架在准确性和效率之间取得了平衡，成功应用于数据集清洗和越狱响应检测。研究发现，精心设计的开发者消息可以显著提高越狱成功率，D-Attack和DH-CoT这两种新策略证实了其有效性。

Conclusion: MDH提供了一种高效准确的恶意内容检测和数据集清洗方法，解决了现有评估方法的局限性。同时，通过利用开发者消息，新提出的D-Attack和DH-CoT策略能显著提升越狱攻击的成功率，这对于理解和防御此类攻击具有重要意义。

Abstract: Evaluating jailbreak attacks is challenging when prompts are not overtly
harmful or fail to induce harmful outputs. Unfortunately, many existing
red-teaming datasets contain such unsuitable prompts. To evaluate attacks
accurately, these datasets need to be assessed and cleaned for maliciousness.
However, existing malicious content detection methods rely on either manual
annotation, which is labor-intensive, or large language models (LLMs), which
have inconsistent accuracy in harmful types. To balance accuracy and
efficiency, we propose a hybrid evaluation framework named MDH (Malicious
content Detection based on LLMs with Human assistance) that combines LLM-based
annotation with minimal human oversight, and apply it to dataset cleaning and
detection of jailbroken responses. Furthermore, we find that well-crafted
developer messages can significantly boost jailbreak success, leading us to
propose two new strategies: D-Attack, which leverages context simulation, and
DH-CoT, which incorporates hijacked chains of thought. The Codes, datasets,
judgements, and detection results will be released in github repository:
https://github.com/AlienZhang1996/DH-CoT.

</details>


### [197] [Evaluating LLMs on Chinese Idiom Translation](https://arxiv.org/abs/2508.10421)
*Cai Yang,Yao Dou,David Heineman,Xiaofeng Wu,Wei Xu*

Main category: cs.CL

TL;DR: 本文介绍了IdiomEval框架，用于评估中文习语翻译，发现现有大型语言模型在习语翻译方面表现不佳，并指出现有评估指标与人工评分相关性低，因此开发了改进的习语翻译错误检测模型。


<details>
  <summary>Details</summary>
Motivation: 习语在日常语言中很常见，尤其是在中文中，其比喻意义通常与字面解释不同，且常包含历史典故和特定结构。尽管大型语言模型在机器翻译方面取得了进展，但对中文习语翻译的了解甚少。

Method: 引入了IdiomEval框架，该框架包含一个全面的错误分类体系。从网络、新闻、维基百科和社交媒体四个领域，对包括GPT-4o和Google Translate在内的九个现代系统的900对翻译对进行了标注。此外，开发了改进的模型来检测习语翻译错误。

Result: 研究发现现有系统在习语翻译方面表现不佳，会产生不正确、字面化、部分或甚至缺失的翻译。表现最好的系统GPT-4在28%的情况下仍会出错。现有评估指标与人工评分的相关性低于0.48，表明其习语质量衡量不佳。开发的改进模型在检测习语翻译错误方面F1分数达到0.68。

Conclusion: 当前机器翻译系统在中文习语翻译方面存在显著缺陷，现有评估指标无法有效衡量习语翻译质量。本研究提出的IdiomEval框架和改进的错误检测模型，为未来习语翻译的评估和改进提供了基础。

Abstract: Idioms, whose figurative meanings usually differ from their literal
interpretations, are common in everyday language, especially in Chinese, where
they often contain historical references and follow specific structural
patterns. Despite recent progress in machine translation with large language
models, little is known about Chinese idiom translation. In this work, we
introduce IdiomEval, a framework with a comprehensive error taxonomy for
Chinese idiom translation. We annotate 900 translation pairs from nine modern
systems, including GPT-4o and Google Translate, across four domains: web, news,
Wikipedia, and social media. We find these systems fail at idiom translation,
producing incorrect, literal, partial, or even missing translations. The
best-performing system, GPT-4, makes errors in 28% of cases. We also find that
existing evaluation metrics measure idiom quality poorly with Pearson
correlation below 0.48 with human ratings. We thus develop improved models that
achieve F$_1$ scores of 0.68 for detecting idiom translation errors.

</details>


### [198] [Computational Economics in Large Language Models: Exploring Model Behavior and Incentive Design under Resource Constraints](https://arxiv.org/abs/2508.10426)
*Sandeep Reddy,Kabir Khan,Rohit Patil,Ananya Chakraborty,Faizan A. Khan,Swati Kulkarni,Arjun Verma,Neha Singh*

Main category: cs.CL

TL;DR: 该研究引入“计算经济学”框架，将大型语言模型（LLM）视为资源受限代理的内部经济体，通过激励驱动的训练范式，显著降低LLM的计算成本（FLOPS减少约40%，延迟降低），同时保持准确性并提高注意力模式的可解释性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的巨大计算成本限制了其应用和发展。

Method: 1. 提出了一个“计算经济学”框架，将LLM视为由注意力头和神经元块等资源受限代理组成的内部经济体，这些代理必须分配稀缺计算以最大化任务效用。2. 经验性地证明，在计算稀缺时，标准LLM会将注意力重新分配到高价值的token，同时保持准确性。3. 基于此观察，提出了一种激励驱动的训练范式，通过在任务损失中加入一个可微分的计算成本项，鼓励稀疏和高效的激活。

Result: 1. 在GLUE（MNLI, STS-B, CoLA）和WikiText-103数据集上，该方法生成了一系列模型，这些模型构成了帕累托前沿，并持续优于事后剪枝方法。2. 在相似的准确性下，实现了约40%的FLOPS减少和更低的延迟。3. 获得了更具可解释性的注意力模式。

Conclusion: 经济学原理为在严格资源限制下设计高效、自适应和更透明的LLM提供了一条有原则的途径。

Abstract: Large language models (LLMs) are limited by substantial computational cost.
We introduce a "computational economics" framework that treats an LLM as an
internal economy of resource-constrained agents (attention heads and neuron
blocks) that must allocate scarce computation to maximize task utility. First,
we show empirically that when computation is scarce, standard LLMs reallocate
attention toward high-value tokens while preserving accuracy. Building on this
observation, we propose an incentive-driven training paradigm that augments the
task loss with a differentiable computation cost term, encouraging sparse and
efficient activations. On GLUE (MNLI, STS-B, CoLA) and WikiText-103, the method
yields a family of models that trace a Pareto frontier and consistently
dominate post-hoc pruning; for a similar accuracy we obtain roughly a forty
percent reduction in FLOPS and lower latency, together with more interpretable
attention patterns. These results indicate that economic principles offer a
principled route to designing efficient, adaptive, and more transparent LLMs
under strict resource constraints.

</details>


### [199] [DiFaR: Enhancing Multimodal Misinformation Detection with Diverse, Factual, and Relevant Rationales](https://arxiv.org/abs/2508.10444)
*Herun Wan,Jiaying Wu,Minnan Luo,Xiangzheng Kong,Zihan Ma,Zhi Zeng*

Main category: cs.CL

TL;DR: DiFaR是一个检测器无关的框架，通过多样的思维链提示和后处理过滤，生成多样化、事实准确、相关性强的文本理由，以增强多模态虚假信息检测器的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的大型视觉-语言模型（LVLMs）生成的文本理由存在三个核心挑战：多样性不足、幻觉导致的事实不准确以及无关或冲突内容的引入，这些限制了其在可训练多模态虚假信息检测器中的有效性。

Method: DiFaR框架采用五种思维链（chain-of-thought）提示来从LVLMs中引出不同的推理轨迹，并整合了一个轻量级的后处理过滤模块，根据句子级别的事实性和相关性得分选择理由句子。

Result: 在四个流行基准上的广泛实验表明，DiFaR比四类基线模型表现高出多达5.9%，并能将现有检测器的性能提升高达8.7%。自动指标和人工评估均证实DiFaR在多样性、事实准确性和相关性三个维度上显著提高了理由质量。

Conclusion: DiFaR显著提高了生成理由的质量，从而有效增强了基于LVLM的虚假信息检测器的性能，解决了现有方法在理由生成方面的关键限制。

Abstract: Generating textual rationales from large vision-language models (LVLMs) to
support trainable multimodal misinformation detectors has emerged as a
promising paradigm. However, its effectiveness is fundamentally limited by
three core challenges: (i) insufficient diversity in generated rationales, (ii)
factual inaccuracies due to hallucinations, and (iii) irrelevant or conflicting
content that introduces noise. We introduce DiFaR, a detector-agnostic
framework that produces diverse, factual, and relevant rationales to enhance
misinformation detection. DiFaR employs five chain-of-thought prompts to elicit
varied reasoning traces from LVLMs and incorporates a lightweight post-hoc
filtering module to select rationale sentences based on sentence-level
factuality and relevance scores. Extensive experiments on four popular
benchmarks demonstrate that DiFaR outperforms four baseline categories by up to
5.9% and boosts existing detectors by as much as 8.7%. Both automatic metrics
and human evaluations confirm that DiFaR significantly improves rationale
quality across all three dimensions.

</details>


### [200] [When Explainability Meets Privacy: An Investigation at the Intersection of Post-hoc Explainability and Differential Privacy in the Context of Natural Language Processing](https://arxiv.org/abs/2508.10482)
*Mahdi Dhaini,Stephen Meisenbacher,Ege Erdogan,Florian Matthes,Gjergji Kasneci*

Main category: cs.CL

TL;DR: 本研究实证调查了自然语言处理（NLP）中隐私与可解释性之间的权衡，发现两者可能共存，并提供了实践建议。


<details>
  <summary>Details</summary>
Motivation: 尽管可解释性NLP和隐私保护NLP的研究日益增多，但两者交叉领域的研究不足，不清楚它们是否能同时实现或相互矛盾，这构成了理解上的重大空白。

Method: 在NLP背景下，采用差分隐私（DP）和事后可解释性（Post-hoc Explainability）这两种流行方法，对隐私-可解释性权衡进行了实证研究。

Result: 研究揭示了隐私和可解释性之间复杂的相互关系，这种关系受下游任务性质、文本隐私化方法和可解释性方法选择等多种因素影响。研究强调了隐私和可解释性共存的可能性。

Conclusion: 研究总结了发现，并为未来在该重要交叉领域的工作提供了一系列实用建议。

Abstract: In the study of trustworthy Natural Language Processing (NLP), a number of
important research fields have emerged, including that of
\textit{explainability} and \textit{privacy}. While research interest in both
explainable and privacy-preserving NLP has increased considerably in recent
years, there remains a lack of investigation at the intersection of the two.
This leaves a considerable gap in understanding of whether achieving
\textit{both} explainability and privacy is possible, or whether the two are at
odds with each other. In this work, we conduct an empirical investigation into
the privacy-explainability trade-off in the context of NLP, guided by the
popular overarching methods of \textit{Differential Privacy} (DP) and Post-hoc
Explainability. Our findings include a view into the intricate relationship
between privacy and explainability, which is formed by a number of factors,
including the nature of the downstream task and choice of the text
privatization and explainability method. In this, we highlight the potential
for privacy and explainability to co-exist, and we summarize our findings in a
collection of practical recommendations for future work at this important
intersection.

</details>


### [201] [Continuous Bangla Sign Language Translation: Mitigating the Expense of Gloss Annotation with the Assistance of Graph](https://arxiv.org/abs/2508.10687)
*Safaeid Hossain Arib,Rabeya Akter,Sejuti Rahman*

Main category: cs.CL

TL;DR: 该项目提出一种结合Transformer和STGCN-LSTM的混合架构，用于无中间标注（gloss-free）的连续孟加拉手语翻译，并在多个数据集上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 全球数百万听障人士面临手语被低估、沟通障碍和社会排斥的问题。手语是听障人士的重要沟通方式，但现有翻译方法仍有提升空间，尤其是在优先口语的社会中。

Method: 该研究将基于图的方法（STGCN-LSTM）与Transformer架构融合，探索了多种融合策略，旨在实现无中间标注（gloss-free）的手语翻译。

Result: 该方法在RWTH-PHOENIX-2014T、CSL-Daily和How2Sign等多个手语数据集上取得了新的最先进性能，BLEU-4分数分别提升了4.01、2.07和0.5。此外，首次在BornilDB v1.0数据集上进行了基准测试。

Conclusion: 该融合方法在手语翻译中表现出卓越性能，为未来的手语翻译研究设定了新基准，并强调了无中间标注翻译对于改善听障人士沟通可及性的重要性。

Abstract: Millions of individuals worldwide are affected by deafness and hearing
impairment. Sign language serves as a sophisticated means of communication for
the deaf and hard of hearing. However, in societies that prioritize spoken
languages, sign language often faces underestimation, leading to communication
barriers and social exclusion. The Continuous Bangla Sign Language Translation
project aims to address this gap by enhancing translation methods. While recent
approaches leverage transformer architecture for state-of-the-art results, our
method integrates graph-based methods with the transformer architecture. This
fusion, combining transformer and STGCN-LSTM architectures, proves more
effective in gloss-free translation. Our contributions include architectural
fusion, exploring various fusion strategies, and achieving a new
state-of-the-art performance on diverse sign language datasets, namely
RWTH-PHOENIX-2014T, CSL-Daily, How2Sign, and BornilDB v1.0. Our approach
demonstrates superior performance compared to current translation outcomes
across all datasets, showcasing notable improvements of BLEU-4 scores of 4.01,
2.07, and 0.5, surpassing those of GASLT, GASLT and slt_how2sign in
RWTH-PHOENIX-2014T, CSL-Daily, and How2Sign, respectively. Also, we introduce
benchmarking on the BornilDB v1.0 dataset for the first time. Our method sets a
benchmark for future research, emphasizing the importance of gloss-free
translation to improve communication accessibility for the deaf and hard of
hearing.

</details>


### [202] [eDIF: A European Deep Inference Fabric for Remote Interpretability of LLM](https://arxiv.org/abs/2508.10553)
*Irma Heithoff. Marc Guggenberger,Sandra Kalogiannis,Susanne Mayer,Fabian Maag,Sigurd Schacht,Carsten Lanquillon*

Main category: cs.CL

TL;DR: 本文提出并评估了欧洲深度推理结构（eDIF）的可行性，这是一个兼容NDIF的GPU集群基础设施，旨在支持欧洲LLM可解释性研究的普及和民主化。


<details>
  <summary>Details</summary>
Motivation: 为了在欧洲研究界普及大型语言模型（LLM）的可解释性基础设施，使研究人员能够广泛访问先进的模型分析能力。

Method: 项目部署了一个基于GPU的集群（eDIF），托管于安斯巴赫应用科学大学，并与合作机构互联。通过NNsight API实现远程模型检查。一个由16名欧洲研究人员参与的结构化试点研究评估了平台的性能、可用性和科学效用，用户进行了激活修补、因果追踪和表示分析等操作。

Result: 研究显示用户参与度逐渐增加，平台性能稳定，远程实验能力受到积极评价。该项目也为建立用户社区奠定了基础。同时，也发现了局限性，例如激活数据下载时间过长和间歇性执行中断。

Conclusion: 这项倡议是欧洲LLM可解释性基础设施广泛普及的重要一步，为未来的更广泛部署、工具扩展和持续社区合作奠定了基础，以推动机械可解释性研究。

Abstract: This paper presents a feasibility study on the deployment of a European Deep
Inference Fabric (eDIF), an NDIF-compatible infrastructure designed to support
mechanistic interpretability research on large language models. The need for
widespread accessibility of LLM interpretability infrastructure in Europe
drives this initiative to democratize advanced model analysis capabilities for
the research community. The project introduces a GPU-based cluster hosted at
Ansbach University of Applied Sciences and interconnected with partner
institutions, enabling remote model inspection via the NNsight API. A
structured pilot study involving 16 researchers from across Europe evaluated
the platform's technical performance, usability, and scientific utility. Users
conducted interventions such as activation patching, causal tracing, and
representation analysis on models including GPT-2 and DeepSeek-R1-70B. The
study revealed a gradual increase in user engagement, stable platform
performance throughout, and a positive reception of the remote experimentation
capabilities. It also marked the starting point for building a user community
around the platform. Identified limitations such as prolonged download
durations for activation data as well as intermittent execution interruptions
are addressed in the roadmap for future development. This initiative marks a
significant step towards widespread accessibility of LLM interpretability
infrastructure in Europe and lays the groundwork for broader deployment,
expanded tooling, and sustained community collaboration in mechanistic
interpretability research.

</details>


### [203] [Learning from Natural Language Feedback for Personalized Question Answering](https://arxiv.org/abs/2508.10695)
*Alireza Salemi,Hamed Zamani*

Main category: cs.CL

TL;DR: 提出VAC框架，通过自然语言反馈（NLF）而非标量奖励来优化大型语言模型（LLMs）的个性化问答，显著提升了效果。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs个性化方法（如RAG结合标量奖励的强化学习）中的标量奖励反馈信号弱且缺乏指导性，限制了学习效率和个性化质量。

Method: 引入VAC框架，用基于用户档案和问题叙述生成的自然语言反馈（NLF）替代标量奖励。训练过程交替优化反馈模型和策略模型，使策略模型在推理时不再需要反馈。

Result: 在LaMP-QA基准测试（包含三个不同领域）上，取得了持续且显著优于SOTA的改进。人工评估也证实了生成回复的更高质量。

Conclusion: 自然语言反馈（NLF）为优化个性化问答提供了更有效的信号。

Abstract: Personalization is crucial for enhancing both the effectiveness and user
satisfaction of language technologies, particularly in information-seeking
tasks like question answering. Current approaches for personalizing large
language models (LLMs) often rely on retrieval-augmented generation (RAG),
followed by reinforcement learning with scalar reward signals to teach models
how to use retrieved personal context. We believe that these scalar rewards
sometimes provide weak, non-instructive feedback, limiting learning efficiency
and personalization quality. We introduce VAC, a novel framework for
personalized response generation that replaces scalar rewards with natural
language feedback (NLF) that are generated conditioned on the user profiles and
the question narratives. NLF serves as a rich and actionable supervision
signal, allowing the policy model to iteratively refine its outputs and
internalize effective personalization strategies. Training alternates between
optimizing the feedback model and fine-tuning the policy model on the improved
responses, resulting in a policy model that no longer requires feedback at
inference. Evaluation on the LaMP-QA benchmark that consists of three diverse
domains demonstrates consistent and significant improvements over the
state-of-the-art results. Human evaluations further confirm the superior
quality of the generated responses. These results demonstrate that NLF provides
more effective signals for optimizing personalized question answering.

</details>


### [204] [Neural Machine Translation for Coptic-French: Strategies for Low-Resource Ancient Languages](https://arxiv.org/abs/2508.10683)
*Nasma Chaoui,Richard Khoury*

Main category: cs.CL

TL;DR: 首次系统性研究科普特语到法语的翻译策略。


<details>
  <summary>Details</summary>
Motivation: 为历史语言（尤其是科普特语）开发翻译工具，并提供普适性实践见解。

Method: 系统评估了中介翻译与直接翻译、预训练影响、多版本微调益处以及模型对噪声的鲁棒性。利用对齐的圣经语料库，并采用风格多样且感知噪声的训练语料进行微调。

Result: 使用风格多样且感知噪声的训练语料进行微调，能显著提升翻译质量。

Conclusion: 为开发历史语言翻译工具提供了重要的实践见解。

Abstract: This paper presents the first systematic study of strategies for translating
Coptic into French. Our comprehensive pipeline systematically evaluates: pivot
versus direct translation, the impact of pre-training, the benefits of
multi-version fine-tuning, and model robustness to noise. Utilizing aligned
biblical corpora, we demonstrate that fine-tuning with a stylistically-varied
and noise-aware training corpus significantly enhances translation quality. Our
findings provide crucial practical insights for developing translation tools
for historical languages in general.

</details>


### [205] [Thinking Inside the Mask: In-Place Prompting in Diffusion LLMs](https://arxiv.org/abs/2508.10736)
*Xiangqi Jin,Yuxuan Wang,Yifeng Gao,Zichen Wen,Biqing Qi,Dongrui Liu,Linfeng Zhang*

Main category: cs.CL

TL;DR: 本文提出ICE框架，将扩散大语言模型(dLLMs)的提示范式从前缀式转变为原地式，并通过置信度感知早停机制显著提高性能和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型(LLMs)的前缀式提示和顺序生成过程在双向信息处理上灵活性有限。扩散大语言模型(dLLMs)凭借其双向注意力机制和迭代优化过程，为更灵活的原地提示策略提供了新机遇。

Method: 引入了ICE（In-Place Chain-of-Thought Prompting with Early Exit）框架，专门为dLLMs设计，将前缀式提示转换为原地提示。ICE在迭代优化过程中将原地提示直接集成到掩码标记位置，并采用置信度感知的早停机制以大幅降低计算开销。

Result: ICE框架在GSM8K数据集上实现了高达17.29%的准确率提升和4.12倍的速度提升；在MMLU数据集上实现了高达276.67倍的加速，同时保持了有竞争力的性能。

Conclusion: ICE框架有效地将dLLMs的提示范式从前缀式转变为原地式，通过结合原地提示和早停机制，在提高准确率的同时显著加速了推理过程，展现了dLLMs在更灵活提示策略方面的潜力。

Abstract: Despite large language models (LLMs) have achieved remarkable success, their
prefix-only prompting paradigm and sequential generation process offer limited
flexibility for bidirectional information. Diffusion large language models
(dLLMs) present new opportunities through their bidirectional attention
mechanisms and iterative refinement processes, enabling more flexible in-place
prompting strategies. We introduce ICE (In-Place Chain-of-Thought Prompting
with Early Exit), a novel framework that transforms prefix-only prompting into
in-place prompting specifically designed for dLLMs. ICE integrates in-place
prompts directly within masked token positions during iterative refinement and
employs a confidence-aware early exit mechanism to significantly reduce
computational overhead. Extensive experiments demonstrate ICE's effectiveness,
achieving up to 17.29% accuracy improvement with 4.12$\times$ speedup on GSM8K,
and up to 276.67$\times$ acceleration on MMLU while maintaining competitive
performance.

</details>


### [206] [Beyond "Not Novel Enough": Enriching Scholarly Critique with LLM-Assisted Feedback](https://arxiv.org/abs/2508.10795)
*Osama Mohammed Afzal,Preslav Nakov,Tom Hope,Iryna Gurevych*

Main category: cs.CL

TL;DR: 该论文提出了一种结构化的自动化新颖性评估方法，通过模拟专家评审行为来辅助同行评审，并在ICLR 2025提交中表现出高精度。


<details>
  <summary>Details</summary>
Motivation: 新颖性评估是同行评审的核心环节，但在NLP等高容量领域中，由于审稿人能力日益紧张，这一方面研究不足。因此，需要一种自动化、一致的方法来支持新颖性评估。

Method: 该方法采用三阶段结构化方法模拟专家审稿人行为：1. 从提交中提取内容；2. 检索并综合相关工作；3. 进行结构化比较以进行基于证据的评估。该方法基于对人类撰写的新颖性评审的大规模分析，捕捉了独立主张验证和上下文推理等关键模式。

Result: 在182份ICLR 2025提交（带有人工标注的新颖性评估）上进行评估，该方法与人类推理的对齐度达到86.5%，新颖性结论的一致性达到75.3%，显著优于现有基于LLM的基线。它能生成详细的、文献感知的分析，并提高审稿人判断的一致性。

Conclusion: 结构化的LLM辅助方法有潜力支持更严谨和透明的同行评审，而无需取代人类专业知识。

Abstract: Novelty assessment is a central yet understudied aspect of peer review,
particularly in high volume fields like NLP where reviewer capacity is
increasingly strained. We present a structured approach for automated novelty
evaluation that models expert reviewer behavior through three stages: content
extraction from submissions, retrieval and synthesis of related work, and
structured comparison for evidence based assessment. Our method is informed by
a large scale analysis of human written novelty reviews and captures key
patterns such as independent claim verification and contextual reasoning.
Evaluated on 182 ICLR 2025 submissions with human annotated reviewer novelty
assessments, the approach achieves 86.5% alignment with human reasoning and
75.3% agreement on novelty conclusions - substantially outperforming existing
LLM based baselines. The method produces detailed, literature aware analyses
and improves consistency over ad hoc reviewer judgments. These results
highlight the potential for structured LLM assisted approaches to support more
rigorous and transparent peer review without displacing human expertise. Data
and code are made available.

</details>


### [207] [Reinforced Language Models for Sequential Decision Making](https://arxiv.org/abs/2508.10839)
*Jim Dilkes,Vahid Yazdanpanah,Sebastian Stein*

Main category: cs.CL

TL;DR: 本文提出了一种名为MS-GRPO的新算法，用于对小型LLM进行后训练，使其能够有效地执行多步序贯决策任务，从而避免对大型昂贵模型的依赖。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在序贯决策方面展现出潜力，但其高昂的计算成本限制了应用。现有的后训练方法主要针对单轮交互，无法有效处理多步智能体任务中的信用分配问题，因此需要改进小型模型的后训练方法。

Method: 引入了多步组相对策略优化（MS-GRPO）算法，该算法基于文本介导随机博弈（TSMG）和语言-智能体策略（LAP）框架。为解决信用分配问题，MS-GRPO将整个累积回合奖励归因于每个单独的回合步骤。此外，还补充了一种新颖的绝对优势加权回合采样策略。通过在Snake和Frozen Lake任务上对一个30亿参数的模型进行后训练来评估该方法。

Result: 实验证明，MS-GRPO能有效提升决策性能：经后训练的30亿参数模型在Frozen Lake任务上比720亿参数的基线模型性能高出50%。

Conclusion: 这项工作表明，针对性的后训练是创建基于LLM的序贯决策智能体的一种实用且高效的替代方案，而非仅仅依赖于模型规模。

Abstract: Large Language Models (LLMs) show potential as sequential decision-making
agents, but their application is often limited due to a reliance on large,
computationally expensive models. This creates a need to improve smaller
models, yet existing post-training methods are designed for single-turn
interactions and cannot handle credit assignment in multi-step agentic tasks.
To address this, we introduce Multi-Step Group-Relative Policy Optimization
(MS-GRPO), a new algorithm for post-training LLM agents, grounded in formal
Text-Mediated Stochastic Game (TSMG) and Language-Agent Policy (LAP)
frameworks. For credit assignment, MS-GRPO attributes the entire cumulative
episode reward to each individual episode step. We supplement this algorithm
with a novel absolute-advantage-weighted episode sampling strategy that we show
improves training performance. We evaluate our approach by post-training a
3-billion parameter model on Snake and Frozen Lake. Our experiments demonstrate
that the method is effective in improving decision-making performance: our
post-trained 3B parameter model outperforms a 72B parameter baseline by 50% on
the Frozen Lake task. This work demonstrates that targeted post-training is a
practical and efficient alternative to relying on model scale for creating
sequential decision-making agents using LLMs.

</details>


### [208] [Psyche-R1: Towards Reliable Psychological LLMs through Unified Empathy, Expertise, and Reasoning](https://arxiv.org/abs/2508.10848)
*Chongyuan Dai,Jinpeng Hu,Hongchang Shi,Zhuo Li,Xun Yang,Meng Wang*

Main category: cs.CL

TL;DR: 本文提出了Psyche-R1，首个结合了同理心、心理专业知识和推理能力的中文心理大语言模型，通过新颖的数据构建和混合训练策略，在心理学任务上表现出色。


<details>
  <summary>Details</summary>
Motivation: 合格心理健康专业人员短缺，而现有心理学领域的LLM研究主要侧重情感支持和同理心对话，缺乏对推理机制的关注，导致响应可靠性不足。

Method: 提出Psyche-R1模型，并设计了一个全面的数据合成流程，生成了超过7.5万个包含详细推理链（CoT）的心理问题和7.3万个同理心对话。采用混合训练策略：通过多LLM交叉选择识别挑战性样本用于组相对策略优化（GRPO）以提升推理能力，其余数据用于监督微调（SFT）以增强同理心响应和心理领域知识。

Result: Psyche-R1在多个心理学基准测试中表现出有效性，其中7B的Psyche-R1达到了与671B DeepSeek-R1相当的性能。

Conclusion: Psyche-R1成功地将同理心、心理专业知识和推理能力整合到中文心理LLM中，为缓解心理健康负担提供了有前景的解决方案。

Abstract: Amidst a shortage of qualified mental health professionals, the integration
of large language models (LLMs) into psychological applications offers a
promising way to alleviate the growing burden of mental health disorders.
Recent reasoning-augmented LLMs have achieved remarkable performance in
mathematics and programming, while research in the psychological domain has
predominantly emphasized emotional support and empathetic dialogue, with
limited attention to reasoning mechanisms that are beneficial to generating
reliable responses. Therefore, in this paper, we propose Psyche-R1, the first
Chinese psychological LLM that jointly integrates empathy, psychological
expertise, and reasoning, built upon a novel data curation pipeline.
Specifically, we design a comprehensive data synthesis pipeline that produces
over 75k high-quality psychological questions paired with detailed rationales,
generated through chain-of-thought (CoT) reasoning and iterative
prompt-rationale optimization, along with 73k empathetic dialogues.
Subsequently, we employ a hybrid training strategy wherein challenging samples
are identified through a multi-LLM cross-selection strategy for group relative
policy optimization (GRPO) to improve reasoning ability, while the remaining
data is used for supervised fine-tuning (SFT) to enhance empathetic response
generation and psychological domain knowledge. Extensive experiment results
demonstrate the effectiveness of the Psyche-R1 across several psychological
benchmarks, where our 7B Psyche-R1 achieves comparable results to 671B
DeepSeek-R1.

</details>


### [209] [From Black Box to Transparency: Enhancing Automated Interpreting Assessment with Explainable AI in College Classrooms](https://arxiv.org/abs/2508.10860)
*Zhaokun Jiang,Ziyin Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种多维度、可解释的机器学习框架，用于自动化口译质量评估，解决了现有方法在语言使用质量评估、数据稀缺和模型可解释性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有口译质量评估研究存在对语言使用质量检查不足、数据稀缺和不平衡导致模型效果不佳，以及缺乏模型预测解释性等问题。

Method: 本文提出了一个多维度建模框架，整合了特征工程、数据增强和可解释机器学习。该方法通过使用与结构相关、透明的特征，并进行Shapley值（SHAP）分析，优先考虑可解释性，而非“黑箱”预测。研究在一个新的英汉同声传译数据集上进行。

Result: 该框架在新的英汉同声传译数据集上表现出强大的预测性能。研究发现，BLEURT和CometKiwi分数是忠实度最强的预测特征，停顿相关特征对流利度预测最有效，而中文特有的短语多样性指标对语言使用质量预测最为关键。

Conclusion: 该研究提供了一种可扩展、可靠且透明的替代传统人工评估的方法，能够为学习者提供详细的诊断反馈，并支持自动化分数无法单独提供的自主学习优势。

Abstract: Recent advancements in machine learning have spurred growing interests in
automated interpreting quality assessment. Nevertheless, existing research
suffers from insufficient examination of language use quality, unsatisfactory
modeling effectiveness due to data scarcity and imbalance, and a lack of
efforts to explain model predictions. To address these gaps, we propose a
multi-dimensional modeling framework that integrates feature engineering, data
augmentation, and explainable machine learning. This approach prioritizes
explainability over ``black box'' predictions by utilizing only
construct-relevant, transparent features and conducting Shapley Value (SHAP)
analysis. Our results demonstrate strong predictive performance on a novel
English-Chinese consecutive interpreting dataset, identifying BLEURT and
CometKiwi scores to be the strongest predictive features for fidelity,
pause-related features for fluency, and Chinese-specific phraseological
diversity metrics for language use. Overall, by placing particular emphasis on
explainability, we present a scalable, reliable, and transparent alternative to
traditional human evaluation, facilitating the provision of detailed diagnostic
feedback for learners and supporting self-regulated learning advantages not
afforded by automated scores in isolation.

</details>


### [210] [SSRL: Self-Search Reinforcement Learning](https://arxiv.org/abs/2508.10874)
*Yuchen Fan,Kaiyan Zhang,Heng Zhou,Yuxin Zuo,Yanxu Chen,Yu Fu,Xinwei Long,Xuekai Zhu,Che Jiang,Yuchen Zhang,Li Kang,Gang Chen,Cheng Huang,Zhizhou He,Bingning Wang,Lei Bai,Ning Ding,Bowen Zhou*

Main category: cs.CL

TL;DR: 该研究探索了大型语言模型（LLMs）作为强化学习（RL）中代理搜索任务的高效模拟器的潜力，通过“自搜索”量化其内部知识，并引入“自搜索RL（SSRL）”以增强其内部知识利用，从而减少对外部搜索引擎的依赖。


<details>
  <summary>Details</summary>
Motivation: 减少强化学习中代理搜索任务对昂贵的外部搜索引擎交互的依赖，寻求利用LLMs的内部知识来替代或补充外部搜索。

Method: 1. 通过结构化提示和重复采样量化LLMs的内在搜索能力，称之为“自搜索”。2. 引入“自搜索RL（SSRL）”，通过基于格式和规则的奖励来增强LLMs的自搜索能力，使其能够在内部迭代地提炼知识利用。

Result: 1. LLMs的自搜索能力随推理预算的增加而表现出强大的扩展行为，在问答基准（包括BrowseComp）上取得了高pass@k分数。2. SSRL使模型无需外部工具即可迭代地完善其内部知识利用。3. 经验评估表明，SSRL训练的策略模型为搜索驱动的RL训练提供了经济高效且稳定的环境，减少了对外部搜索引擎的依赖，并促进了稳健的模拟到现实迁移。

Conclusion: 1. LLMs拥有可以有效激发以实现高性能的世界知识。2. SSRL展示了利用内部知识减少幻觉的潜力。3. SSRL训练的模型可以无缝集成外部搜索引擎。这些发现强调了LLMs支持更可扩展的RL智能体训练的潜力。

Abstract: We investigate the potential of large language models (LLMs) to serve as
efficient simulators for agentic search tasks in reinforcement learning (RL),
thereby reducing dependence on costly interactions with external search
engines. To this end, we first quantify the intrinsic search capability of LLMs
via structured prompting and repeated sampling, which we term Self-Search. Our
results reveal that LLMs exhibit strong scaling behavior with respect to the
inference budget, achieving high pass@k on question-answering benchmarks,
including the challenging BrowseComp task. Building on these observations, we
introduce Self-Search RL (SSRL), which enhances LLMs' Self-Search capability
through format-based and rule-based rewards. SSRL enables models to iteratively
refine their knowledge utilization internally, without requiring access to
external tools. Empirical evaluations demonstrate that SSRL-trained policy
models provide a cost-effective and stable environment for search-driven RL
training, reducing reliance on external search engines and facilitating robust
sim-to-real transfer. We draw the following conclusions: 1) LLMs possess world
knowledge that can be effectively elicited to achieve high performance; 2) SSRL
demonstrates the potential of leveraging internal knowledge to reduce
hallucination; 3) SSRL-trained models integrate seamlessly with external search
engines without additional effort. Our findings highlight the potential of LLMs
to support more scalable RL agent training.

</details>


### [211] [A Survey on Diffusion Language Models](https://arxiv.org/abs/2508.10875)
*Tianyi Li,Mingda Chen,Bowei Guo,Zhiqiang Shen*

Main category: cs.CL

TL;DR: 该综述全面概述了扩散语言模型（DLMs），将其作为自回归模型（AR）的有力替代，重点介绍了其并行生成、低延迟、双向上下文捕获等优势，并探讨了其演变、原理、技术、推理策略、多模态扩展、应用、局限性及未来方向。


<details>
  <summary>Details</summary>
Motivation: 自回归语言模型在推理延迟和双向上下文捕获方面存在局限性。扩散语言模型（DLMs）作为一种新兴范式，通过迭代去噪过程并行生成token，具有降低推理延迟、捕获双向上下文和实现精细控制的固有优势，且性能已能与自回归模型媲美，因此有必要对其进行全面梳理和分析。

Method: 本文是一篇综述性论文，方法包括：追溯DLMs的演变及其与自回归和掩码语言模型等范式的关系；涵盖基础原理和最先进模型；提供全面分类法并深入分析预训练到后训练技术；彻底审查推理策略和优化（包括解码并行性、缓存机制和生成质量）；突出DLMs的多模态扩展及其应用；讨论DLMs的局限性、挑战并提出未来研究方向。

Result: 该综述提供了DLM领域的整体概览，包括：其演变和与其他范式的关系；基础原理和最先进模型；更新、全面的分类法；对当前技术（从预训练到后训练）的深入分析；对DLM推理策略和优化的全面审查；DLM多模态扩展及其在各种实际场景中的应用；DLM的局限性、挑战（效率、长序列处理、基础设施需求）以及未来研究方向。研究表明DLMs已能实现数倍的加速，并达到与自回归模型相当的性能。

Conclusion: 扩散语言模型（DLMs）是自回归范式的一个强大且有前景的替代方案，具有并行生成、低推理延迟和捕获双向上下文的固有优势。尽管仍面临效率和长序列处理等挑战，但该领域发展迅速，未来研究方向广阔。本综述为理解DLMs的当前格局和未来发展提供了全面的视角。

Abstract: Diffusion Language Models (DLMs) are rapidly emerging as a powerful and
promising alternative to the dominant autoregressive (AR) paradigm. By
generating tokens in parallel through an iterative denoising process, DLMs
possess inherent advantages in reducing inference latency and capturing
bidirectional context, thereby enabling fine-grained control over the
generation process. While achieving a several-fold speed-up, recent
advancements have allowed DLMs to show performance comparable to their
autoregressive counterparts, making them a compelling choice for various
natural language processing tasks. In this survey, we provide a holistic
overview of the current DLM landscape. We trace its evolution and relationship
with other paradigms, such as autoregressive and masked language models, and
cover both foundational principles and state-of-the-art models. Our work offers
an up-to-date, comprehensive taxonomy and an in-depth analysis of current
techniques, from pre-training strategies to advanced post-training methods.
Another contribution of this survey is a thorough review of DLM inference
strategies and optimizations, including improvements in decoding parallelism,
caching mechanisms, and generation quality. We also highlight the latest
approaches to multimodal extensions of DLMs and delineate their applications
across various practical scenarios. Furthermore, our discussion addresses the
limitations and challenges of DLMs, including efficiency, long-sequence
handling, and infrastructure requirements, while outlining future research
directions to sustain progress in this rapidly evolving field. Project GitHub
is available at https://github.com/VILA-Lab/Awesome-DLMs.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [212] [WiFi-based Global Localization in Large-Scale Environments Leveraging Structural Priors from osmAG](https://arxiv.org/abs/2508.10144)
*Xu Ma,Jiajie Zhang,Fujing Xie,Sören Schwertfeger*

Main category: cs.RO

TL;DR: 该论文提出了一种基于WiFi和OpenStreetMap区域图（osmAG）的室内机器人全局定位框架，通过建模信号衰减和优化AP定位，实现了高精度、可扩展且经济高效的室内定位，尤其在无指纹数据区域表现优异。


<details>
  <summary>Details</summary>
Motivation: 在GPS信号受限的室内环境中，机器人全局定位至关重要。传统的定位方法可能存在精度、可扩展性或成本问题，尤其是在大型复杂环境中。

Method: 该方法结合了信号传播模型和osmAG的几何拓扑先验知识。离线阶段，通过迭代优化算法，建模墙体衰减来定位WiFi接入点（AP）。在线阶段，利用增强的osmAG地图进行实时机器人定位。

Result: 离线AP定位平均误差为3.79米（比三边测量法提升35.3%）。在线机器人定位在有指纹区域平均误差为3.12米（比KNN指纹法提升8.77%），在无指纹区域平均误差为3.83米（提升81.05%）。该方法比基于指纹的方法更节省空间，尤其在无指纹数据区域定位精度更高。在11,025平方米的多楼层环境中进行了验证。

Conclusion: 该框架为室内机器人定位提供了一个可扩展、经济高效的解决方案，能够解决“被绑架机器人”问题，特别适用于大型复杂室内环境，并在无指纹数据区域展现出卓越的定位能力。

Abstract: Global localization is essential for autonomous robotics, especially in
indoor environments where the GPS signal is denied. We propose a novel
WiFi-based localization framework that leverages ubiquitous wireless
infrastructure and the OpenStreetMap Area Graph (osmAG) for large-scale indoor
environments. Our approach integrates signal propagation modeling with osmAG's
geometric and topological priors. In the offline phase, an iterative
optimization algorithm localizes WiFi Access Points (APs) by modeling wall
attenuation, achieving a mean localization error of 3.79 m (35.3\% improvement
over trilateration). In the online phase, real-time robot localization uses the
augmented osmAG map, yielding a mean error of 3.12 m in fingerprinted areas
(8.77\% improvement over KNN fingerprinting) and 3.83 m in non-fingerprinted
areas (81.05\% improvement). Comparison with a fingerprint-based method shows
that our approach is much more space efficient and achieves superior
localization accuracy, especially for positions where no fingerprint data are
available. Validated across a complex 11,025 &m^2& multi-floor environment,
this framework offers a scalable, cost-effective solution for indoor robotic
localization, solving the kidnapped robot problem. The code and dataset are
available at https://github.com/XuMa369/osmag-wifi-localization.

</details>


### [213] [Systematic Constraint Formulation and Collision-Free Trajectory Planning Using Space-Time Graphs of Convex Sets](https://arxiv.org/abs/2508.10203)
*Matthew D. Osburn,Cameron K. Peterson,John L. Salmon*

Main category: cs.RO

TL;DR: 本文提出并应用ST-GCS方法，在动态杂乱环境中生成最优、无碰撞、随时间变化的轨迹，无需初始猜测。


<details>
  <summary>Details</summary>
Motivation: 在具有大量时空约束的动态杂乱环境中，为数值求解器找到初始猜测非常困难。

Method: 采用凸集图（GCS）和新开发的空时凸集图（ST-GCS）公式，无需提供初始猜测即可生成最优最小距离无碰撞轨迹。同时，探讨了通用GCS兼容约束的推导和适应策略。

Result: ST-GCS在静态环境下能生成与标准GCS公式等效的轨迹。此外，ST-GCS在动态环境中也能成功运行，找到最小距离的无碰撞轨迹。

Conclusion: ST-GCS是一种有效的方法，能够在动态环境中生成最优、无碰撞的轨迹，解决了传统方法对初始猜测的需求。

Abstract: In this paper, we create optimal, collision-free, time-dependent trajectories
through cluttered dynamic environments. The many spatial and temporal
constraints make finding an initial guess for a numerical solver difficult.
Graphs of Convex Sets (GCS) and the recently developed Space-Time Graphs of
Convex Sets formulation (ST-GCS) enable us to generate optimal minimum distance
collision-free trajectories without providing an initial guess to the solver.
We also explore the derivation of general GCS-compatible constraints and
document an intuitive strategy for adapting general constraints to the
framework. We show that ST-GCS produces equivalent trajectories to the standard
GCS formulation when the environment is static. We then show ST-GCS operating
in dynamic environments to find minimum distance collision-free trajectories.

</details>


### [214] [Hybrid Data-Driven Predictive Control for Robust and Reactive Exoskeleton Locomotion Synthesis](https://arxiv.org/abs/2508.10269)
*Kejun Li,Jeeseop Kim,Maxime Brunet,Marine Pétriaux,Yisong Yue,Aaron D. Ames*

Main category: cs.RO

TL;DR: 本文提出了一种混合数据驱动预测控制（HDDPC）框架，用于外骨骼的鲁棒双足运动，通过同时规划足部接触计划和连续域轨迹，实现对环境变化的实时动态反应。


<details>
  <summary>Details</summary>
Motivation: 外骨骼的鲁棒双足运动需要实时动态响应环境变化的能力，这是现有方法面临的挑战。

Method: 引入了混合数据驱动预测控制（HDDPC）框架，它是数据驱动预测控制的扩展。该框架利用基于Hankel矩阵的表示来建模系统动力学，并结合了步态间（S2S）转换，以增强在动态环境中的适应性。它将接触调度与轨迹规划集成，提供了一个统一的运动合成解决方案，通过在线重新规划实现鲁棒和反应式行走。

Result: 该方法在Atalante外骨骼上进行了验证，结果表明其鲁棒性和适应性得到了显著提高，能够实现鲁棒且反应灵敏的行走。

Conclusion: HDDPC框架为外骨骼的运动合成提供了一个高效、统一的解决方案，通过在线重新规划实现了鲁棒和反应式行走，从而提高了在外骨骼中的鲁棒性和适应性。

Abstract: Robust bipedal locomotion in exoskeletons requires the ability to dynamically
react to changes in the environment in real time. This paper introduces the
hybrid data-driven predictive control (HDDPC) framework, an extension of the
data-enabled predictive control, that addresses these challenges by
simultaneously planning foot contact schedules and continuous domain
trajectories. The proposed framework utilizes a Hankel matrix-based
representation to model system dynamics, incorporating step-to-step (S2S)
transitions to enhance adaptability in dynamic environments. By integrating
contact scheduling with trajectory planning, the framework offers an efficient,
unified solution for locomotion motion synthesis that enables robust and
reactive walking through online replanning. We validate the approach on the
Atalante exoskeleton, demonstrating improved robustness and adaptability.

</details>


### [215] [ReconVLA: Reconstructive Vision-Language-Action Model as Effective Robot Perceiver](https://arxiv.org/abs/2508.10333)
*Wenxuan Song,Ziyang Zhou,Han Zhao,Jiayi Chen,Pengxiang Ding,Haodong Yan,Yuxin Huang,Feilong Tang,Donglin Wang,Haoang Li*

Main category: cs.RO

TL;DR: ReconVLA提出了一种基于隐式重建的视觉语言动作模型，通过扩散变换器重建注视区域来引导视觉注意力，解决了现有VLA模型视觉注意力分散的问题，实现了更精确的机器人操作和更好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言动作（VLA）模型在将视觉注意力聚焦到目标区域时存在困难，导致注意力分散，无法有效利用任务相关的视觉信息进行精确操作。

Method: 提出ReconVLA模型，采用隐式接地范式。模型通过一个扩散变换器，在模型视觉输出的条件下，重建图像中与目标操作对象对应的注视区域。此外，构建了一个包含超过10万条轨迹和200万个数据样本的大规模预训练数据集，以增强模型的视觉重建泛化能力。

Result: 该方法促使VLA模型学习到细粒度表示并准确分配视觉注意力，从而有效利用任务特定视觉信息并进行精确操作。在模拟和真实世界的广泛实验证明了其隐式接地方法的优越性，展示了其精确操作和泛化能力。

Conclusion: ReconVLA通过引入隐式重建范式和大规模预训练数据集，成功解决了现有VLA模型视觉注意力分散的问题，显著提升了机器人精确操作能力和泛化性。

Abstract: Recent advances in Vision-Language-Action (VLA) models have enabled robotic
agents to integrate multimodal understanding with action execution. However,
our empirical analysis reveals that current VLAs struggle to allocate visual
attention to target regions. Instead, visual attention is always dispersed. To
guide the visual attention grounding on the correct target, we propose
ReconVLA, a reconstructive VLA model with an implicit grounding paradigm.
Conditioned on the model's visual outputs, a diffusion transformer aims to
reconstruct the gaze region of the image, which corresponds to the target
manipulated objects. This process prompts the VLA model to learn fine-grained
representations and accurately allocate visual attention, thus effectively
leveraging task-specific visual information and conducting precise
manipulation. Moreover, we curate a large-scale pretraining dataset comprising
over 100k trajectories and 2 million data samples from open-source robotic
datasets, further boosting the model's generalization in visual reconstruction.
Extensive experiments in simulation and the real world demonstrate the
superiority of our implicit grounding method, showcasing its capabilities of
precise manipulation and generalization. Our project page is
https://zionchow.github.io/ReconVLA/.

</details>


### [216] [BEASST: Behavioral Entropic Gradient based Adaptive Source Seeking for Mobile Robots](https://arxiv.org/abs/2508.10363)
*Donipolo Ghimire,Aamodh Suresh,Carlos Nieto-Granda,Solmaz S. Kia*

Main category: cs.RO

TL;DR: 本文提出BEASST框架，一个新颖的基于行为熵梯度自适应寻源方法，使移动机器人在复杂未知环境中高效平衡探索与利用。


<details>
  <summary>Details</summary>
Motivation: 在复杂、未知环境中，移动机器人需要高效地平衡探索与利用以寻找信号源，这是现有方法面临的挑战。

Method: BEASST将归一化信号强度建模为源位置的替代概率，并基于行为熵（BE）和Prelec概率加权函数定义了一个目标函数。该函数根据信号可靠性和任务紧急性，自适应地调整机器人行为（从规避风险到寻求风险）。框架在单峰信号假设下提供了理论收敛性保证，并在有界扰动下具备实用稳定性。

Result: 在DARPA SubT和多房间场景的实验验证表明，BEASST持续优于现有先进方法，实现了路径长度减少15%，寻源速度加快20%。这得益于其智能的不确定性驱动导航，能够动态切换激进追踪和谨慎探索。

Conclusion: BEASST框架通过创新的行为熵和自适应行为策略，显著提升了移动机器人在复杂未知环境中的寻源效率和性能，是寻源领域的一大进步。

Abstract: This paper presents BEASST (Behavioral Entropic Gradient-based Adaptive
Source Seeking for Mobile Robots), a novel framework for robotic source seeking
in complex, unknown environments. Our approach enables mobile robots to
efficiently balance exploration and exploitation by modeling normalized signal
strength as a surrogate probability of source location. Building on Behavioral
Entropy(BE) with Prelec's probability weighting function, we define an
objective function that adapts robot behavior from risk-averse to risk-seeking
based on signal reliability and mission urgency. The framework provides
theoretical convergence guarantees under unimodal signal assumptions and
practical stability under bounded disturbances. Experimental validation across
DARPA SubT and multi-room scenarios demonstrates that BEASST consistently
outperforms state-of-the-art methods, achieving 15% reduction in path length
and 20% faster source localization through intelligent uncertainty-driven
navigation that dynamically transitions between aggressive pursuit and cautious
exploration.

</details>


### [217] [MASH: Cooperative-Heterogeneous Multi-Agent Reinforcement Learning for Single Humanoid Robot Locomotion](https://arxiv.org/abs/2508.10423)
*Qi Liu,Xiaopeng Zhang,Mingshan Tan,Shuaikang Ma,Jinliang Ding,Yanjie Li*

Main category: cs.RO

TL;DR: 本文提出MASH方法，通过将单个人形机器人的肢体视为独立的异构智能体，并利用合作多智能体强化学习来优化其运动能力，从而加速训练收敛并提高全身协调性。


<details>
  <summary>Details</summary>
Motivation: 现有方法多采用单智能体强化学习控制单机器人或多智能体强化学习控制多机器人系统。本文旨在探索一种新范式，即应用合作异构多智能体强化学习来优化单个人形机器人的运动能力，以克服传统方法的局限性。

Method: 本文提出“用于单人形机器人运动的多智能体强化学习（MASH）”方法，将单个人形机器人的每个肢体（腿和手臂）视为一个独立的异构智能体，这些智能体在探索机器人动作空间的同时，共享一个全局评论家进行合作学习。

Result: 实验证明，MASH方法能够加速训练收敛，并显著提高机器人的全身协调能力。与传统的单智能体强化学习方法相比，MASH表现出更优越的性能。

Conclusion: 这项工作推动了多智能体强化学习在单个人形机器人控制领域的整合与应用，为开发高效的运动策略提供了新的见解和思路。

Abstract: This paper proposes a novel method to enhance locomotion for a single
humanoid robot through cooperative-heterogeneous multi-agent deep reinforcement
learning (MARL). While most existing methods typically employ single-agent
reinforcement learning algorithms for a single humanoid robot or MARL
algorithms for multi-robot system tasks, we propose a distinct paradigm:
applying cooperative-heterogeneous MARL to optimize locomotion for a single
humanoid robot. The proposed method, multi-agent reinforcement learning for
single humanoid locomotion (MASH), treats each limb (legs and arms) as an
independent agent that explores the robot's action space while sharing a global
critic for cooperative learning. Experiments demonstrate that MASH accelerates
training convergence and improves whole-body cooperation ability, outperforming
conventional single-agent reinforcement learning methods. This work advances
the integration of MARL into single-humanoid-robot control, offering new
insights into efficient locomotion strategies.

</details>


### [218] [Few-shot Vision-based Human Activity Recognition with MLLM-based Visual Reinforcement Learning](https://arxiv.org/abs/2508.10371)
*Wenqi Zheng,Yutaka Arakawa*

Main category: cs.RO

TL;DR: 本文提出FAVOR方法，将视觉强化学习应用于多模态大语言模型（MLLM），以解决少样本人类活动识别（HAR）问题，显著提升了模型的泛化、推理能力和可解释性。


<details>
  <summary>Details</summary>
Motivation: 强化学习在数据有限场景下具有重要价值，但其在多模态人类活动识别（HAR）领域的应用尚未得到充分探索。

Method: 该方法名为FAVOR，将视觉强化学习引入多模态大语言模型（MLLM）进行人类活动识别。具体流程是：首先，MLLM为人类活动图像生成包含推理轨迹和最终答案的多个候选响应；接着，使用奖励函数评估这些响应；最后，利用组相对策略优化（GRPO）算法对MLLM进行优化，使其能仅通过少量样本适应HAR任务。

Result: 该方法显著提升了模型在少样本识别上的泛化能力，增强了模型的推理能力，并在推理阶段实现了可解释性分析。在四个HAR数据集和五种不同设置上的大量实验证明了所提方法的优越性。

Conclusion: 通过结合视觉强化学习，FAVOR方法成功地使多模态大语言模型适应了少样本人类活动识别任务，并在泛化能力、推理能力和可解释性分析方面取得了显著改进。

Abstract: Reinforcement learning in large reasoning models enables learning from
feedback on their outputs, making it particularly valuable in scenarios where
fine-tuning data is limited. However, its application in multi-modal human
activity recognition (HAR) domains remains largely underexplored. Our work
extends reinforcement learning to the human activity recognition domain with
multimodal large language models. By incorporating visual reinforcement
learning in the training process, the model's generalization ability on
few-shot recognition can be greatly improved. Additionally, visual
reinforcement learning can enhance the model's reasoning ability and enable
explainable analysis in the inference stage. We name our few-shot human
activity recognition method with visual reinforcement learning FAVOR.
Specifically, our approach first utilizes a multimodal large language model
(MLLM) to generate multiple candidate responses for the human activity image,
each containing reasoning traces and final answers. These responses are then
evaluated using reward functions, and the MLLM model is subsequently optimized
using the Group Relative Policy Optimization (GRPO) algorithm. In this way, the
MLLM model can be adapted to human activity recognition with only a few
samples. Extensive experiments on four human activity recognition datasets and
five different settings demonstrate the superiority of the proposed method.

</details>


### [219] [Synthesis of Deep Neural Networks with Safe Robust Adaptive Control for Reliable Operation of Wheeled Mobile Robots](https://arxiv.org/abs/2508.10634)
*Mehdi Heydari Shahna,Jouni Mattila*

Main category: cs.RO

TL;DR: 本文提出一种针对重型轮式移动机器人（WMRs）的分层控制策略，结合深度神经网络（DNN）和鲁棒自适应控制（RAC），并通过双层安全机制确保在干扰下的高精度和稳定性。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在控制方面具有高精度和低计算成本的优势，但其“黑箱”特性使得在受严格国际标准约束且易受故障和干扰影响的重型轮式移动机器人上的部署面临挑战。

Method: 研究设计了一个分层控制策略：DNN作为主控制器提供高精度性能；当外部干扰导致性能下降时，低级安全层激活模型无关的鲁棒自适应控制（RAC）策略，取代DNN以确保系统稳定；无论何种控制策略，高级安全层持续监控系统性能，并在补偿不可行时启动关机，以防止系统或环境受损。

Result: 所提出的DNN与RAC策略的结合保证了整个WMR系统的均匀指数稳定性，并在一定程度上符合安全标准。通过使用一台6,000公斤的WMR进行实时实验，验证了该方法的有效性。

Conclusion: 该研究成功地将DNN与RAC策略融合，并辅以分层安全机制，解决了重型轮式移动机器人在复杂干扰下实现高精度控制和稳定运行的难题，为其实际部署提供了可靠的解决方案。

Abstract: Deep neural networks (DNNs) can enable precise control while maintaining low
computational costs by circumventing the need for dynamic modeling. However,
the deployment of such black-box approaches remains challenging for heavy-duty
wheeled mobile robots (WMRs), which are subject to strict international
standards and prone to faults and disturbances. We designed a hierarchical
control policy for heavy-duty WMRs, monitored by two safety layers with
differing levels of authority. To this end, a DNN policy was trained and
deployed as the primary control strategy, providing high-precision performance
under nominal operating conditions. When external disturbances arise and reach
a level of intensity such that the system performance falls below a predefined
threshold, a low-level safety layer intervenes by deactivating the primary
control policy and activating a model-free robust adaptive control (RAC)
policy. This transition enables the system to continue operating while ensuring
stability by effectively managing the inherent trade-off between system
robustness and responsiveness. Regardless of the control policy in use, a
high-level safety layer continuously monitors system performance during
operation. It initiates a shutdown only when disturbances become sufficiently
severe such that compensation is no longer viable and continued operation would
jeopardize the system or its environment. The proposed synthesis of DNN and RAC
policy guarantees uniform exponential stability of the entire WMR system while
adhering to safety standards to some extent. The effectiveness of the proposed
approach was further validated through real-time experiments using a 6,000 kg
WMR.

</details>


### [220] [A Semantic-Aware Framework for Safe and Intent-Integrative Assistance in Upper-Limb Exoskeletons](https://arxiv.org/abs/2508.10378)
*Yu Chen,Shu Miao,Chunyu Wu,Jingsong Mu,Bo OuYang,Xiang Li*

Main category: cs.RO

TL;DR: 本文提出一个语义感知框架，将大语言模型集成到上肢外骨骼的任务规划中，以实现基于任务语义理解和异常检测的安全、意图整合的辅助。


<details>
  <summary>Details</summary>
Motivation: 现有上肢外骨骼在家庭护理场景中缺乏对任务语义的理解和与用户的协作规划能力，限制了其泛化性，无法根据操作对象的性质自适应调整辅助配置。

Method: 该方法首先在外骨骼透明模式下捕捉穿戴者抓取物体时的意图。然后，从任务描述中提取语义信息，系统自动配置适当的辅助参数。此外，使用基于扩散的异常检测器持续监测人机交互状态并触发实时重新规划。在任务执行过程中，采用在线轨迹优化和阻抗控制以确保安全并调节人机交互。

Result: 实验结果表明，所提出的方法能够有效与穿戴者的认知对齐，适应语义变化的任务，并可靠地响应异常情况。

Conclusion: 该框架通过集成大语言模型实现语义感知的任务规划，结合异常检测和实时控制，为上肢外骨骼提供了安全、意图整合且高度适应性的辅助，解决了现有系统泛化性不足的问题。

Abstract: Upper-limb exoskeletons are primarily designed to provide assistive support
by accurately interpreting and responding to human intentions. In home-care
scenarios, exoskeletons are expected to adapt their assistive configurations
based on the semantic information of the task, adjusting appropriately in
accordance with the nature of the object being manipulated. However, existing
solutions often lack the ability to understand task semantics or
collaboratively plan actions with the user, limiting their generalizability. To
address this challenge, this paper introduces a semantic-aware framework that
integrates large language models into the task planning framework, enabling the
delivery of safe and intent-integrative assistance. The proposed approach
begins with the exoskeleton operating in transparent mode to capture the
wearer's intent during object grasping. Once semantic information is extracted
from the task description, the system automatically configures appropriate
assistive parameters. In addition, a diffusion-based anomaly detector is used
to continuously monitor the state of human-robot interaction and trigger
real-time replanning in response to detected anomalies. During task execution,
online trajectory refinement and impedance control are used to ensure safety
and regulate human-robot interaction. Experimental results demonstrate that the
proposed method effectively aligns with the wearer's cognition, adapts to
semantically varying tasks, and responds reliably to anomalies.

</details>


### [221] [Learning Task Execution Hierarchies for Redundant Robots](https://arxiv.org/abs/2508.10780)
*Alessandro Adami,Aris Synodinos,Matteo Iovino,Ruggero Carli,Pietro Falco*

Main category: cs.RO

TL;DR: 本文提出一种新颖框架，结合强化学习和遗传编程，自动学习高冗余机器人任务栈（SoT）的层级和参数，以实现多任务管理和动态适应性。


<details>
  <summary>Details</summary>
Motivation: 传统机器人任务栈（SoT）依赖专家手动设计，限制了其适应性和可访问性，难以有效管理复杂高冗余机器人的多任务和动态环境。

Method: 该研究结合强化学习（RL）和遗传编程（GP），通过用户定义的目标和基于精度、安全性、执行时间等直观指标的成本函数，自动学习任务栈（SoT）的优先级和控制策略，无需人工干预。

Result: 通过学习得到的任务栈（SoT）使机器人能够动态适应不断变化的环境和输入，有效平衡相互竞争的目标，同时保持鲁棒的任务执行。该方法在模拟和mobile-YuMi平台上的实验验证了其有效性。

Conclusion: 该方法为复杂机器人的冗余管理提供了一个通用且用户友好的解决方案，促进了以人为中心的机器人编程，并降低了对专家设计的需求。

Abstract: Modern robotic systems, such as mobile manipulators, humanoids, and aerial
robots with arms, often possess high redundancy, enabling them to perform
multiple tasks simultaneously. Managing this redundancy is key to achieving
reliable and flexible behavior. A widely used approach is the Stack of Tasks
(SoT), which organizes control objectives by priority within a unified
framework. However, traditional SoTs are manually designed by experts, limiting
their adaptability and accessibility. This paper introduces a novel framework
that automatically learns both the hierarchy and parameters of a SoT from
user-defined objectives. By combining Reinforcement Learning and Genetic
Programming, the system discovers task priorities and control strategies
without manual intervention. A cost function based on intuitive metrics such as
precision, safety, and execution time guides the learning process. We validate
our method through simulations and experiments on the mobile-YuMi platform, a
dual-arm mobile manipulator with high redundancy. Results show that the learned
SoTs enable the robot to dynamically adapt to changing environments and inputs,
balancing competing objectives while maintaining robust task execution. This
approach provides a general and user-friendly solution for redundancy
management in complex robots, advancing human-centered robot programming and
reducing the need for expert design.

</details>


### [222] [Super LiDAR Reflectance for Robotic Perception](https://arxiv.org/abs/2508.10398)
*Wei Gao,Jie Zhang,Mingle Zhao,Zhiyuan Zhang,Shu Kong,Maani Ghaffari,Dezhen Song,Cheng-Zhong Xu,Hui Kong*

Main category: cs.RO

TL;DR: 该研究提出了一种创新框架，利用非重复扫描激光雷达（NRS-LiDAR）将稀疏的激光雷达反射数据转换为密集的反射图像，以克服低成本激光雷达数据稀疏性的限制，并扩展其在机器人感知中的应用。


<details>
  <summary>Details</summary>
Motivation: 传统视觉被动感知，而主动光学感知（如LiDAR）在机器人感知中潜力巨大。然而，低成本LiDAR传感器的数据稀疏性限制了其广泛应用，尤其是在需要密集感知能力的任务中。因此，需要解决稀疏LiDAR数据稠密化的问题。

Method: 本研究引入了一个创新框架，利用NRS-LiDAR的独特属性，从稀疏数据生成密集的LiDAR反射图像。该方法解决了反射率校准和从静态到动态场景域的转换等关键挑战，并通过一个专门为NRS-LiDAR设计的稠密化网络实现。

Result: 主要成果包括：构建了一个用于LiDAR反射图像稠密化的综合数据集；开发了一个针对NRS-LiDAR的稠密化网络；并展示了使用生成的密集反射图像在回环检测和交通车道检测等多样化应用中的有效性。

Conclusion: 该工作通过将稀疏的LiDAR数据稠密化，成功克服了低成本LiDAR传感器的局限性，从而扩展了主动视觉在真实世界机器人感知任务中的应用范围和潜力。

Abstract: Conventionally, human intuition often defines vision as a modality of passive
optical sensing, while active optical sensing is typically regarded as
measuring rather than the default modality of vision. However, the situation
now changes: sensor technologies and data-driven paradigms empower active
optical sensing to redefine the boundaries of vision, ushering in a new era of
active vision. Light Detection and Ranging (LiDAR) sensors capture reflectance
from object surfaces, which remains invariant under varying illumination
conditions, showcasing significant potential in robotic perception tasks such
as detection, recognition, segmentation, and Simultaneous Localization and
Mapping (SLAM). These applications often rely on dense sensing capabilities,
typically achieved by high-resolution, expensive LiDAR sensors. A key challenge
with low-cost LiDARs lies in the sparsity of scan data, which limits their
broader application. To address this limitation, this work introduces an
innovative framework for generating dense LiDAR reflectance images from sparse
data, leveraging the unique attributes of non-repeating scanning LiDAR
(NRS-LiDAR). We tackle critical challenges, including reflectance calibration
and the transition from static to dynamic scene domains, facilitating the
reconstruction of dense reflectance images in real-world settings. The key
contributions of this work include a comprehensive dataset for LiDAR
reflectance image densification, a densification network tailored for
NRS-LiDAR, and diverse applications such as loop closure and traffic lane
detection using the generated dense reflectance images.

</details>


### [223] [CVIRO: A Consistent and Tightly-Coupled Visual-Inertial-Ranging Odometry on Lie Groups](https://arxiv.org/abs/2508.10867)
*Yizhi Zhou,Ziwei Kang,Jiawei Xia,Xuan Wang*

Main category: cs.RO

TL;DR: 本文提出了一种基于李群的、紧耦合的视觉-惯性-测距里程计（CVIRO）系统，通过将UWB信标状态纳入系统状态并利用李群的不变误差特性，解决了UWB辅助VIO系统中因观测性不一致和UWB校准不确定性导致的定位性能下降问题，实现了卓越的定位精度和一致性。


<details>
  <summary>Details</summary>
Motivation: UWB被广泛用于缓解视觉-惯性里程计（VIO）系统中的漂移。然而，现有UWB辅助VIO系统存在不一致性问题，主要源于两个方面：1) 估计器未能保持正确的系统可观测性；2) 假设UWB信标位置已知，从而不当地忽略了校准不确定性，导致定位性能下降。

Method: 本文提出了一种基于李群的、一致且紧耦合的视觉-惯性-测距里程计（CVIRO）系统。该方法将UWB信标状态纳入系统状态，显式考虑了UWB校准不确定性，实现了机器人和信标状态的联合一致估计。此外，通过利用李群的不变误差特性，确保了可观测性的一致性。理论证明CVIRO算法能自然地保持系统正确的不可观测子空间。

Result: 通过大量的仿真和实验，CVIRO系统与现有方法相比，在定位精度和一致性方面表现出卓越的性能。

Conclusion: CVIRO算法通过同时解决UWB校准不确定性和保持正确的系统可观测性，提供了一个一致且高精度的UWB辅助视觉-惯性-测距里程计解决方案。

Abstract: Ultra Wideband (UWB) is widely used to mitigate drift in visual-inertial
odometry (VIO) systems. Consistency is crucial for ensuring the estimation
accuracy of a UWBaided VIO system. An inconsistent estimator can degrade
localization performance, where the inconsistency primarily arises from two
main factors: (1) the estimator fails to preserve the correct system
observability, and (2) UWB anchor positions are assumed to be known, leading to
improper neglect of calibration uncertainty. In this paper, we propose a
consistent and tightly-coupled visual-inertial-ranging odometry (CVIRO) system
based on the Lie group. Our method incorporates the UWB anchor state into the
system state, explicitly accounting for UWB calibration uncertainty and
enabling the joint and consistent estimation of both robot and anchor states.
Furthermore, observability consistency is ensured by leveraging the invariant
error properties of the Lie group. We analytically prove that the CVIRO
algorithm naturally maintains the system's correct unobservable subspace,
thereby preserving estimation consistency. Extensive simulations and
experiments demonstrate that CVIRO achieves superior localization accuracy and
consistency compared to existing methods.

</details>


### [224] [Large Model Empowered Embodied AI: A Survey on Decision-Making and Embodied Learning](https://arxiv.org/abs/2508.10399)
*Wenlong Liang,Rui Zhou,Yang Ma,Bing Zhang,Songlin Li,Yijia Liao,Ping Kuang*

Main category: cs.RO

TL;DR: 本文全面综述了大模型赋能具身智能的研究进展，重点关注自主决策和具身学习，并首次将世界模型整合到具身智能的综述中，同时讨论了现有挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 具身智能旨在开发能感知、决策、行动和学习的物理智能系统，是实现通用人工智能（AGI）的有前景途径。然而，在开放动态环境中，具身智能体实现人类级别的通用智能仍面临巨大挑战。近期大模型的突破为具身智能带来了革命性进展，提升了其感知、交互、规划和学习能力，因此有必要进行系统性总结。

Method: 本研究通过提供一篇全面的综述文章来分析大模型赋能的具身智能。具体方法包括：1) 探讨分层和端到端决策范式，详细阐述大模型如何增强高层规划、低层执行、反馈以及视觉-语言-动作（VLA）模型。2) 介绍主流具身学习方法，深入阐述大模型如何增强模仿学习和强化学习。3) 首次将世界模型纳入具身智能综述，介绍其设计方法及其在增强决策和学习中的关键作用。

Result: 大模型在具身智能的自主决策和具身学习方面带来了显著提升。具体结果包括：大模型能有效增强分层决策中的高层规划、低层执行和反馈机制，并提升端到端决策中的VLA模型性能；大模型深入提升了模仿学习和强化学习的能力；世界模型在增强具身智能的决策和学习能力方面发挥着关键作用。

Conclusion: 尽管大模型赋能的具身智能已取得坚实进展，但在开放动态环境中实现人类级别通用智能仍存在挑战。这些挑战为未来的研究指明了方向，预示着该领域仍有广阔的探索空间。

Abstract: Embodied AI aims to develop intelligent systems with physical forms capable
of perceiving, decision-making, acting, and learning in real-world
environments, providing a promising way to Artificial General Intelligence
(AGI). Despite decades of explorations, it remains challenging for embodied
agents to achieve human-level intelligence for general-purpose tasks in open
dynamic environments. Recent breakthroughs in large models have revolutionized
embodied AI by enhancing perception, interaction, planning and learning. In
this article, we provide a comprehensive survey on large model empowered
embodied AI, focusing on autonomous decision-making and embodied learning. We
investigate both hierarchical and end-to-end decision-making paradigms,
detailing how large models enhance high-level planning, low-level execution,
and feedback for hierarchical decision-making, and how large models enhance
Vision-Language-Action (VLA) models for end-to-end decision making. For
embodied learning, we introduce mainstream learning methodologies, elaborating
on how large models enhance imitation learning and reinforcement learning
in-depth. For the first time, we integrate world models into the survey of
embodied AI, presenting their design methods and critical roles in enhancing
decision-making and learning. Though solid advances have been achieved,
challenges still exist, which are discussed at the end of this survey,
potentially as the further research directions.

</details>


### [225] [CorrectNav: Self-Correction Flywheel Empowers Vision-Language-Action Navigation Model](https://arxiv.org/abs/2508.10416)
*Zhuoyuan Yu,Yuxing Long,Zihan Yang,Chengyan Zeng,Hongwei Fan,Jiyao Zhang,Hao Dong*

Main category: cs.RO

TL;DR: 本文提出了一种名为“自校正飞轮”的后训练范式，通过利用模型在训练集上的错误轨迹生成自校正数据，迭代提升视觉-语言导航（VLA）模型的错误纠正能力和导航性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言导航模型在执行指令时常偏离正确轨迹，且缺乏有效的错误纠正能力，这阻碍了它们从错误中恢复。

Method: 该研究提出“自校正飞轮”后训练范式。它将模型在训练集上的错误轨迹视为有价值的数据源，开发方法识别偏差，并自动生成感知和动作的自校正数据。这些数据用于模型的持续训练，通过多次“飞轮”迭代，逐步增强模型（CorrectNav）的错误纠正能力。

Result: 在R2R-CE和RxR-CE基准测试中，CorrectNav取得了新的最先进成功率，分别为65.1%和69.3%，比现有最佳VLA导航模型分别高出8.2%和16.4%。真实机器人测试也证明了其在错误纠正、动态避障和长指令遵循方面的卓越能力。

Conclusion: “自校正飞轮”范式能有效利用模型自身的错误轨迹进行迭代训练，显著提升视觉-语言导航模型的错误纠正能力和整体导航性能，达到新的最先进水平。

Abstract: Existing vision-and-language navigation models often deviate from the correct
trajectory when executing instructions. However, these models lack effective
error correction capability, hindering their recovery from errors. To address
this challenge, we propose Self-correction Flywheel, a novel post-training
paradigm. Instead of considering the model's error trajectories on the training
set as a drawback, our paradigm emphasizes their significance as a valuable
data source. We have developed a method to identify deviations in these error
trajectories and devised innovative techniques to automatically generate
self-correction data for perception and action. These self-correction data
serve as fuel to power the model's continued training. The brilliance of our
paradigm is revealed when we re-evaluate the model on the training set,
uncovering new error trajectories. At this time, the self-correction flywheel
begins to spin. Through multiple flywheel iterations, we progressively enhance
our monocular RGB-based VLA navigation model CorrectNav. Experiments on R2R-CE
and RxR-CE benchmarks show CorrectNav achieves new state-of-the-art success
rates of 65.1% and 69.3%, surpassing prior best VLA navigation models by 8.2%
and 16.4%. Real robot tests in various indoor and outdoor environments
demonstrate \method's superior capability of error correction, dynamic obstacle
avoidance, and long instruction following.

</details>


### [226] [Enabling Generic Robot Skill Implementation Using Object Oriented Programming](https://arxiv.org/abs/2508.10497)
*Abdullah Farrukh,Achim Wagner,Martin Ruskowski*

Main category: cs.RO

TL;DR: 本文提出一个软件框架，通过简化机器人接口和提供制造商无关的抽象层，旨在降低机器人系统部署的复杂性和所需工作量，解决中小型企业和研究人员在机器人集成方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 开发和集成机器人算法，尤其是在缺乏机器人专业知识的中小型企业中，是一项困难的任务，常导致依赖外部专家和供应商锁定。研究人员在使用机器人作为大型智能系统组件时，也面临机器人接口复杂性的挑战。

Method: 提出一个软件框架概念，专注于简化现代机器人系统的各种接口，并为不同制造商和型号提供一个抽象层。使用Python编程语言实现了一个原型，目标系统是包含Yaskawa Motoman GP4的料箱拣选单元。

Result: 该框架旨在显著减少部署可工作机器人系统所需的工作量，使机器人系统更易于集成和使用，从而解决中小型企业和研究人员在机器人应用中的痛点。

Conclusion: 通过提供简化的接口和制造商无关的抽象层，所提出的软件框架能够有效降低机器人系统的部署和集成复杂性，为缺乏专业知识的企业和研究人员提供了更便捷的机器人应用解决方案。

Abstract: Developing robotic algorithms and integrating a robotic subsystem into a
larger system can be a difficult task. Particularly in small and medium-sized
enterprises (SMEs) where robotics expertise is lacking, implementing,
maintaining and developing robotic systems can be a challenge. As a result,
many companies rely on external expertise through system integrators, which, in
some cases, can lead to vendor lock-in and external dependency. In the academic
research on intelligent manufacturing systems, robots play a critical role in
the design of robust autonomous systems. Similar challenges are faced by
researchers who want to use robotic systems as a component in a larger smart
system, without having to deal with the complexity and vastness of the robot
interfaces in detail. In this paper, we propose a software framework that
reduces the effort required to deploy a working robotic system. The focus is
solely on providing a concept for simplifying the different interfaces of a
modern robot system and using an abstraction layer for different manufacturers
and models. The Python programming language is used to implement a prototype of
the concept. The target system is a bin-picking cell containing a Yaskawa
Motoman GP4.

</details>


### [227] [KDPE: A Kernel Density Estimation Strategy for Diffusion Policy Trajectory Selection](https://arxiv.org/abs/2508.10511)
*Andrea Rosasco,Federico Ceola,Giulia Pasquale,Lorenzo Natale*

Main category: cs.RO

TL;DR: 本文提出KDPE（基于核密度估计的策略），通过过滤Diffusion Policy可能产生有害的轨迹，同时保持较低的测试时间计算开销，从而提高机器人策略的性能。


<details>
  <summary>Details</summary>
Motivation: 行为克隆在处理训练数据中的多模态性方面是一个长期存在的挑战。Diffusion Policy虽然性能先进，但存在两个主要缺点：去噪过程的随机性会影响生成轨迹的质量，以及作为监督学习方法可能学习到数据离群值，导致机器人在策略执行时偏离数据分布。现有工作试图通过大规模训练或结合经典行为克隆算法来缓解这些问题，但本文提出了不同的解决方案。

Method: 本文提出KDPE，一种基于核密度估计（KDE）的策略，用于过滤Diffusion Policy的输出轨迹。KDPE使用了一种流形感知的核函数来建模动作的概率密度函数，这些动作由末端执行器笛卡尔位置、姿态和夹持器状态组成。

Result: KDPE在模拟单臂任务和真实机器人实验中均取得了比Diffusion Policy更好的性能。

Conclusion: KDPE通过过滤Diffusion Policy的输出轨迹，有效解决了其随机性和学习离群值的问题，提高了机器人策略的性能，同时保持了较低的计算开销。

Abstract: Learning robot policies that capture multimodality in the training data has
been a long-standing open challenge for behavior cloning. Recent approaches
tackle the problem by modeling the conditional action distribution with
generative models. One of these approaches is Diffusion Policy, which relies on
a diffusion model to denoise random points into robot action trajectories.
While achieving state-of-the-art performance, it has two main drawbacks that
may lead the robot out of the data distribution during policy execution. First,
the stochasticity of the denoising process can highly impact on the quality of
generated trajectory of actions. Second, being a supervised learning approach,
it can learn data outliers from the dataset used for training. Recent work
focuses on mitigating these limitations by combining Diffusion Policy either
with large-scale training or with classical behavior cloning algorithms.
Instead, we propose KDPE, a Kernel Density Estimation-based strategy that
filters out potentially harmful trajectories output of Diffusion Policy while
keeping a low test-time computational overhead. For Kernel Density Estimation,
we propose a manifold-aware kernel to model a probability density function for
actions composed of end-effector Cartesian position, orientation, and gripper
state. KDPE overall achieves better performance than Diffusion Policy on
simulated single-arm tasks and real robot experiments.
  Additional material and code are available on our project page
https://hsp-iit.github.io/KDPE/.

</details>


### [228] [MLM: Learning Multi-task Loco-Manipulation Whole-Body Control for Quadruped Robot with Arm](https://arxiv.org/abs/2508.10538)
*Xin Liu,Bida Ma,Chenkun Qi,Yan Ding,Zhaxizhuoma,Guorong Zhang,Pengan Chen,Kehui Liu,Zhongjie Jia,Chuyue Guan,Yule Mo,Jiaqi Liu,Feng Gao,Jiangwei Zhong,Bin Zhao,Xuelong Li*

Main category: cs.RO

TL;DR: 该研究提出了一种名为MLM的强化学习框架，结合真实世界和模拟数据，使带机械臂的四足机器人能够进行全身运动操作，实现多任务自主控制或远程操作，并能零样本迁移到现实世界。


<details>
  <summary>Details</summary>
Motivation: 带机械臂的四足机器人进行全身运动操作是一个具有挑战性的问题，尤其是在实现多任务控制方面。

Method: 1. 提出MLM强化学习框架，结合真实世界和模拟数据驱动。2. 引入带有自适应、基于课程的采样机制的轨迹库，以平衡多任务学习。3. 提出轨迹-速度预测策略网络，预测不可观测的未来轨迹和速度，以应对仅有历史观测的部署场景和不同空间范围的任务。4. 利用大量仿真数据和基于课程的奖励。

Result: 1. 控制器在仿真中实现了全身行为。2. 实现了零样本迁移到真实世界部署。3. 仿真中的消融研究验证了方法的必要性和有效性。4. 在Go2机器人与Airbot机械臂上的真实世界实验证明了策略在多任务执行中的良好性能。

Conclusion: 所提出的MLM框架，通过其轨迹库和轨迹-速度预测网络，有效解决了带机械臂四足机器人的多任务全身运动操作问题，并实现了零样本真实世界迁移。

Abstract: Whole-body loco-manipulation for quadruped robots with arm remains a
challenging problem, particularly in achieving multi-task control. To address
this, we propose MLM, a reinforcement learning framework driven by both
real-world and simulation data. It enables a six-DoF robotic arm--equipped
quadruped robot to perform whole-body loco-manipulation for multiple tasks
autonomously or under human teleoperation. To address the problem of balancing
multiple tasks during the learning of loco-manipulation, we introduce a
trajectory library with an adaptive, curriculum-based sampling mechanism. This
approach allows the policy to efficiently leverage real-world collected
trajectories for learning multi-task loco-manipulation. To address deployment
scenarios with only historical observations and to enhance the performance of
policy execution across tasks with different spatial ranges, we propose a
Trajectory-Velocity Prediction policy network. It predicts unobservable future
trajectories and velocities. By leveraging extensive simulation data and
curriculum-based rewards, our controller achieves whole-body behaviors in
simulation and zero-shot transfer to real-world deployment. Ablation studies in
simulation verify the necessity and effectiveness of our approach, while
real-world experiments on the Go2 robot with an Airbot robotic arm demonstrate
the policy's good performance in multi-task execution.

</details>


### [229] [Why Report Failed Interactions With Robots?! Towards Vignette-based Interaction Quality](https://arxiv.org/abs/2508.10603)
*Agnes Axelsson,Merle Reimann,Ronald Cumbal,Hannah Pelikan,Divesh Lala*

Main category: cs.RO

TL;DR: 尽管LLM提升了人机交互质量，但仍存在次优的、情境依赖的失败。本文提出使用民族志短篇故事（ethnographic vignettes）来清晰记录和突出这些通常被忽视的失败，以促进透明度并增强评估方法。


<details>
  <summary>Details</summary>
Motivation: LLM虽然改善了人机交互（HRI）质量，但与人际交互相比，系统仍存在次优表现。HRI失败的性质和严重性常依赖于具体情境，难以泛化。许多失败，特别是罕见的失败，缺乏清晰记录。

Method: 提出在HRI领域应用一种被忽视的技术：民族志短篇故事（ethnographic vignettes）。描述了撰写短篇故事的方法论，并基于个人在HRI系统失败方面的经验创作了范例。

Result: 短篇故事能清晰地突出HRI中的失败，特别是那些鲜少被记录的失败。它们能够从多学科视角传达失败，提升机器人能力的透明度，并记录研究报告中可能被省略的意外行为。

Conclusion: 鼓励将短篇故事作为现有交互评估方法的补充，因其在沟通多学科失败、促进透明度和记录意外行为方面的优势。

Abstract: Although the quality of human-robot interactions has improved with the advent
of LLMs, there are still various factors that cause systems to be sub-optimal
when compared to human-human interactions. The nature and criticality of
failures are often dependent on the context of the interaction and so cannot be
generalized across the wide range of scenarios and experiments which have been
implemented in HRI research. In this work we propose the use of a technique
overlooked in the field of HRI, ethnographic vignettes, to clearly highlight
these failures, particularly those that are rarely documented. We describe the
methodology behind the process of writing vignettes and create our own based on
our personal experiences with failures in HRI systems. We emphasize the
strength of vignettes as the ability to communicate failures from a
multi-disciplinary perspective, promote transparency about the capabilities of
robots, and document unexpected behaviours which would otherwise be omitted
from research reports. We encourage the use of vignettes to augment existing
interaction evaluation methods.

</details>


### [230] [An Open-Source User-Friendly Interface for Simulating Magnetic Soft Robots using Simulation Open Framework Architecture (SOFA)](https://arxiv.org/abs/2508.10686)
*Carla Wehner,Finn Schubert,Heiko Hellkamp,Julius Hahnewald,Kilian Scheafer,Muhammad Bilal Khan,Oliver Gutfleisch*

Main category: cs.RO

TL;DR: 开发了一个基于SOFA的开源、用户友好的磁性软机器人仿真工具。


<details>
  <summary>Details</summary>
Motivation: 现有仿真平台缺乏对磁性材料的专用支持，且对不同专业水平的研究人员使用不便。

Method: 利用仿真开放框架架构（SOFA）开发了一个开源、用户友好的仿真界面，支持定义材料属性、施加磁场、实时观察变形和应力分析。

Result: 通过梁、三指/四指抓手和蝴蝶等四个基准模型验证了其功能，展示了其易用性，适用于初学者和高级研究人员。

Conclusion: 该工具旨在弥合理论建模与实际设计之间的差距，未来将通过实验验证和与工业标准有限元求解器比较来提高精度，以实现更真实和预测性的仿真。

Abstract: Soft robots, particularly magnetic soft robots, require specialized
simulation tools to accurately model their deformation under external magnetic
fields. However, existing platforms often lack dedicated support for magnetic
materials, making them difficult to use for researchers at different expertise
levels. This work introduces an open-source, user-friendly simulation interface
using the Simulation Open Framework Architecture (SOFA), specifically designed
to model magnetic soft robots. The tool enables users to define material
properties, apply magnetic fields, and observe resulting deformations in real
time. By integrating intuitive controls and stress analysis capabilities, it
aims to bridge the gap between theoretical modeling and practical design. Four
benchmark models - a beam, three- and four-finger grippers, and a butterfly -
demonstrate its functionality. The software's ease of use makes it accessible
to both beginners and advanced researchers. Future improvements will refine
accuracy through experimental validation and comparison with industry-standard
finite element solvers, ensuring realistic and predictive simulations of
magnetic soft robots.

</details>


### [231] [Biasing Frontier-Based Exploration with Saliency Areas](https://arxiv.org/abs/2508.10689)
*Matteo Luperto,Valerii Stakanov,Giacomo Boracchi,Nicola Basilico,Francesco Amigoni*

Main category: cs.RO

TL;DR: 该论文提出一种利用神经网络生成显著性图来识别探索高兴趣区域（显著性区域）的方法，并用其偏置现有探索策略，以显著影响机器人的探索行为，同时提供探索终止标准。


<details>
  <summary>Details</summary>
Motivation: 现有自主探索策略主要关注最大化探索面积以提高速度，但忽略了环境中某些区域对于发现大片未知区域更重要，且缺乏有效的探索终止标准。

Method: 提出一种方法，通过神经网络从当前地图生成显著性图来识别“显著性区域”（对探索高度感兴趣的区域）。该神经网络还实现了一个终止标准，用于判断环境是否已完全探索。然后，利用这些显著性区域来偏置一些广泛使用的探索策略。

Result: 通过广泛的实验证明，将显著性区域的知识融入探索策略中，可以显著影响机器人在探索过程中的行为。

Conclusion: 识别并利用环境中的显著性区域，能有效引导机器人探索，使其行为更具策略性，有助于更高效地完成探索任务。

Abstract: Autonomous exploration is a widely studied problem where a robot
incrementally builds a map of a previously unknown environment. The robot
selects the next locations to reach using an exploration strategy. To do so,
the robot has to balance between competing objectives, like exploring the
entirety of the environment, while being as fast as possible. Most exploration
strategies try to maximise the explored area to speed up exploration; however,
they do not consider that parts of the environment are more important than
others, as they lead to the discovery of large unknown areas. We propose a
method that identifies \emph{saliency areas} as those areas that are of high
interest for exploration, by using saliency maps obtained from a neural network
that, given the current map, implements a termination criterion to estimate
whether the environment can be considered fully-explored or not. We use
saliency areas to bias some widely used exploration strategies, showing, with
an extensive experimental campaign, that this knowledge can significantly
influence the behavior of the robot during exploration.

</details>


### [232] [The SET Perceptual Factors Framework: Towards Assured Perception for Autonomous Systems](https://arxiv.org/abs/2508.10798)
*Troi Williams*

Main category: cs.RO

TL;DR: 本文提出了SET（自我、环境和目标）感知因素框架，用于系统分析环境因素如何影响机器人感知，旨在提高自动驾驶系统的安全性、透明度和公众信任。


<details>
  <summary>Details</summary>
Motivation: 未来的自动驾驶系统虽然潜力巨大，但其部署面临安全和信任问题。特别是机器人感知的可靠性至关重要，因为感知失败（常由复杂环境因素引起）可能导致事故并损害公众信任。

Method: 引入SET感知因素框架。该框架使用SET状态树来分类感知因素的来源（自我、环境、目标），并使用SET因素树来建模这些来源和因素如何影响特定感知任务（如物体检测、姿态估计）。随后，利用这两种树开发感知因素模型来量化给定任务的不确定性。

Result: 该框架提供了一种透明且标准化的方法，用于识别、建模和沟通感知风险。它有助于系统地分析天气、遮挡或传感器限制等因素对感知的不利影响。

Conclusion: SET框架旨在促进严格的安全保障，并通过提供识别、建模和沟通感知风险的透明标准化方法，培养公众对自动驾驶系统更深入的理解和信任。

Abstract: Future autonomous systems promise significant societal benefits, yet their
deployment raises concerns about safety and trustworthiness. A key concern is
assuring the reliability of robot perception, as perception seeds safe
decision-making. Failures in perception are often due to complex yet common
environmental factors and can lead to accidents that erode public trust. To
address this concern, we introduce the SET (Self, Environment, and Target)
Perceptual Factors Framework. We designed the framework to systematically
analyze how factors such as weather, occlusion, or sensor limitations
negatively impact perception. To achieve this, the framework employs SET State
Trees to categorize where such factors originate and SET Factor Trees to model
how these sources and factors impact perceptual tasks like object detection or
pose estimation. Next, we develop Perceptual Factor Models using both trees to
quantify the uncertainty for a given task. Our framework aims to promote
rigorous safety assurances and cultivate greater public understanding and trust
in autonomous systems by offering a transparent and standardized method for
identifying, modeling, and communicating perceptual risks.

</details>


### [233] [A Multimodal Neural Network for Recognizing Subjective Self-Disclosure Towards Social Robots](https://arxiv.org/abs/2508.10828)
*Henry Powell,Guy Laban,Emily S. Cross*

Main category: cs.RO

TL;DR: 该研究旨在开发一个计算模型，以准确识别与机器人互动的过程中人类的主观自我表露，并提出了一种新的损失函数，显著提高了模型性能。


<details>
  <summary>Details</summary>
Motivation: 自我表露是人类社交的重要特征，但目前很少有计算系统能准确建模它，尤其是在人机交互中。随着社交机器人需与人类建立关系，使其能识别自我表露变得愈发紧迫。

Method: 开发了一个定制的多模态注意力网络，借鉴了情感识别领域的模型。在一个大型的、自收集的自我表露视频语料库上训练该模型，并构建了一种新的损失函数——尺度保持交叉熵损失（scale preserving cross entropy loss），以同时改进分类和回归问题。

Result: 使用新颖损失函数训练的最佳模型，F1分数达到0.83，比最佳基线模型提高了0.48。这在使社交机器人识别互动伙伴的自我表露方面取得了显著进展。

Conclusion: 该研究成功开发了一个高性能的计算模型来识别自我表露，特别是通过引入新的损失函数，极大地提升了模型的准确性。这项能力对于未来具备社交认知的机器人至关重要。

Abstract: Subjective self-disclosure is an important feature of human social
interaction. While much has been done in the social and behavioural literature
to characterise the features and consequences of subjective self-disclosure,
little work has been done thus far to develop computational systems that are
able to accurately model it. Even less work has been done that attempts to
model specifically how human interactants self-disclose with robotic partners.
It is becoming more pressing as we require social robots to work in conjunction
with and establish relationships with humans in various social settings. In
this paper, our aim is to develop a custom multimodal attention network based
on models from the emotion recognition literature, training this model on a
large self-collected self-disclosure video corpus, and constructing a new loss
function, the scale preserving cross entropy loss, that improves upon both
classification and regression versions of this problem. Our results show that
the best performing model, trained with our novel loss function, achieves an F1
score of 0.83, an improvement of 0.48 from the best baseline model. This result
makes significant headway in the aim of allowing social robots to pick up on an
interaction partner's self-disclosures, an ability that will be essential in
social robots with social cognition.

</details>


### [234] [TLE-Based A2C Agent for Terrestrial Coverage Orbital Path Planning](https://arxiv.org/abs/2508.10872)
*Anantha Narayanan,Battu Bhanu Teja,Pruthwik Mishra*

Main category: cs.RO

TL;DR: 该研究提出了一种基于A2C强化学习框架，用于优化卫星轨道参数以实现精确的地面覆盖，并在收敛速度和奖励方面优于PPO，适用于实时任务规划。


<details>
  <summary>Details</summary>
Motivation: 近地轨道（LEO）日益拥堵，给地球观测卫星的部署和安全运行带来了挑战。任务规划者不仅要考虑任务特定需求，还要应对与活跃卫星和空间碎片日益增长的碰撞风险。

Method: 本研究采用强化学习框架，利用优势演员-评论家（A2C）算法优化卫星轨道参数。问题被建模为马尔可夫决策过程（MDP），在自定义的OpenAI Gymnasium环境中进行，使用经典开普勒元素模拟轨道动力学。智能体逐步学习调整半长轴、偏心率、倾角、升交点赤经和近地点角五种轨道参数，以实现目标地面覆盖。通过与近端策略优化（PPO）进行比较评估。

Result: A2C算法表现出卓越性能，实现了5.8倍更高的累积奖励（10.0 vs 9.263025），同时收敛所需的步数减少了31.5倍（2,000 vs 63,000）。A2C智能体在不同目标坐标下均能持续满足任务目标，并保持了适用于实时任务规划应用的计算效率。

Conclusion: 强化学习为可扩展和智能的近地轨道任务规划提供了一种计算高效的替代方案。该方法验证了演员-评论家方法在连续轨道控制方面优于信任域方法，并展示了快速收敛能力，从而实现了自适应卫星部署。

Abstract: The increasing congestion of Low Earth Orbit (LEO) poses persistent
challenges to the efficient deployment and safe operation of Earth observation
satellites. Mission planners must now account not only for mission-specific
requirements but also for the increasing collision risk with active satellites
and space debris. This work presents a reinforcement learning framework using
the Advantage Actor-Critic (A2C) algorithm to optimize satellite orbital
parameters for precise terrestrial coverage within predefined surface radii. By
formulating the problem as a Markov Decision Process (MDP) within a custom
OpenAI Gymnasium environment, our method simulates orbital dynamics using
classical Keplerian elements. The agent progressively learns to adjust five of
the orbital parameters - semi-major axis, eccentricity, inclination, right
ascension of ascending node, and the argument of perigee-to achieve targeted
terrestrial coverage. Comparative evaluation against Proximal Policy
Optimization (PPO) demonstrates A2C's superior performance, achieving 5.8x
higher cumulative rewards (10.0 vs 9.263025) while converging in 31.5x fewer
timesteps (2,000 vs 63,000). The A2C agent consistently meets mission
objectives across diverse target coordinates while maintaining computational
efficiency suitable for real-time mission planning applications. Key
contributions include: (1) a TLE-based orbital simulation environment
incorporating physics constraints, (2) validation of actor-critic methods'
superiority over trust region approaches in continuous orbital control, and (3)
demonstration of rapid convergence enabling adaptive satellite deployment. This
approach establishes reinforcement learning as a computationally efficient
alternative for scalable and intelligent LEO mission planning.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [235] [Quantifying the Value of Seismic Structural Health Monitoring for post-earthquake recovery of electric power system in terms of resilience enhancement](https://arxiv.org/abs/2508.10318)
*Huangbin Liang,Beatriz Moya,Francisco Chinesta,Eleni Chatzi*

Main category: eess.SY

TL;DR: 本研究提出一个概率模拟框架，量化地震结构健康监测（SSHM）在加速电网（EPN）震后恢复和增强韧性方面的价值，结果显示SSHM可显著提高恢复速度并降低韧性缺失。


<details>
  <summary>Details</summary>
Motivation: 传统的电网震后恢复依赖耗时且不精确的人工检查，导致修复优先级 suboptimal 和服务中断延长。地震结构健康监测（SSHM）有望加速恢复，但其部署成本高昂，且系统级韧性效益尚未得到充分探索。

Method: 本研究提出了一个概率模拟框架，包括基于网络配置、灾害强度、易损性函数和损伤-功能映射的地震损伤建模，以及考虑资源限制、修复和传输持续时间的恢复模拟。系统功能通过基于图的孤岛检测和最优潮流分析进行评估。韧性通过功能恢复曲线导出的“韧性缺失”（LoR）指标量化。SSHM通过改变修复调度中损伤信息的质量（使用混淆矩阵模拟损伤误分类）来纳入模型，并比较了不同监测场景。

Result: 通过SSHM提高损伤感知能力可显著加速电网恢复，并将韧性缺失（LoR）降低高达21%。

Conclusion: 本研究为关键基础设施中SSHM的部署提供了循证决策支持，证明其能有效提升电网的震后恢复能力和韧性。

Abstract: Post-earthquake recovery of electric power networks (EPNs) is critical to
community resilience. Traditional recovery processes often rely on prolonged
and imprecise manual inspections for damage diagnosis, leading to suboptimal
repair prioritization and extended service disruptions. Seismic Structural
Health Monitoring (SSHM) offers the potential to expedite recovery by enabling
more accurate and timely damage assessment. However, SSHM deployment incurs
costs, and its system-level resilience benefit remains underexplored. This
study proposes a probabilistic simulation framework to quantify the value of
SSHM for enhancing EPN resilience. The framework includes seismic damage
modeling based on network configuration, hazard intensity, fragility functions,
and damage-functionality mappings, combined with recovery simulations
incorporating resource constraints, repair and transfer durations. System
functionality is evaluated using graph-based island detection and optimal power
flow analysis. Resilience is quantified via the Lack of Resilience (LoR) metric
derived from the functionality restoration curve. SSHM is incorporated by
altering the quality of damage information used in repair scheduling. Different
monitoring scenarios (e.g., no-SSHM baseline, partial SSHM, full SSHM with
various accuracies) are modeled using confusion matrices to simulate damage
misclassification. Results show that improved damage awareness via SSHM
significantly accelerates recovery and reduces LoR by up to 21%. This work
supports evidence-based decisions for SSHM deployment in critical
infrastructure.

</details>


### [236] [A Structured Framework for Prioritizing Unsafe Control Actions in STPA: Case Study on eVTOL Operations](https://arxiv.org/abs/2508.10446)
*Halima El Badaoui*

Main category: eess.SY

TL;DR: 本研究提出了一种结合专家输入和蒙特卡洛模拟的方法，以更客观地优先排序系统理论过程分析（STPA）识别出的不安全控制行为（UCA），并利用动态矩阵改进结果沟通。


<details>
  <summary>Details</summary>
Motivation: STPA在复杂系统安全分析中能识别大量UCA，但管理和优先排序这些结果（尤其是在快速开发周期中）具有挑战性，且难以保持优先排序的客观性和有效沟通。

Method: 提出了一种补充方法，结合安全分析师和领域专家的输入，通过评估UCA的严重性、控制器或决策者的影响因子以及主题专家基于不同因素的批判性排名来客观化UCA的优先级。引入蒙特卡洛模拟以减少主观性和相对性。开发了动态缩放优先级矩阵以更好地沟通优先级结果。

Result: 该方法应用于一个真实的电动垂直起降（eVTOL）项目，共识别出318个UCA。通过应用所提出的优先级方法，其中110个被识别为高优先级UCA，需优先处理以加强系统设计和实现更安全的eVTOL操作。

Conclusion: 该方法能够更客观地优先排序UCA，并有效沟通优先级结果，从而识别出关键UCA，有助于改进系统设计和操作安全。

Abstract: Systems Theoretic Process Analysis (STPA) is a widely recommended method for
analysing complex system safety. STPA can identify numerous Unsafe Control
Actions (UCAs) and requirements depending on the level of granularity of the
analysis and the complexity of the system being analysed. Managing numerous
results is challenging, especially during a fast-paced development lifecycle.
Extensive research has been done to optimize the efficiency of managing and
prioritising the STPA results. However, maintaining the objectivity of
prioritisation and communicating the prioritised results have become common
challenges. In this paper, the authors present a complementary approach that
incorporates inputs from both the safety analysts and domain experts to more
objectively prioritise UCAs. This is done by evaluating the severity of each
UCA, the impact factor of each controller or decision maker that issues the
UCA, and the ranking provided by the subject matter experts who assess the UCA
criticalities based on different factors. In addition, a Monte Carlo simulation
is introduced to reduce subjectivity and relativity, thus enabling more
objective prioritisation of the UCAs. As part of the approach to better
communicate the prioritisation results and plan the next steps of system
development, a dynamic-scaling prioritisation matrix was developed to capture
different sets of prioritised UCAs. The approach was applied to a real project
to improve the safe operations of Electric Vertical Take-off and Landing
(eVTOL). The results highlighted critical UCAs that need to be prioritised for
safer eVTOL operation. 318 UCAs were identified in total. Based on the
application of the prioritisation methodology, 110 were recognized as
high-priority UCAs to strengthen the system design.

</details>


### [237] [Feedback stabilization of a nanoparticle at the intensity minimum of an optical double-well potential](https://arxiv.org/abs/2508.10601)
*Vojtěch Mlynář,Salambô Dago,Jakob Rieser,Mario A. Ciampini,Markus Aspelmeyer,Nikolai Kiesel,Andreas Kugi,Andreas Deutschmann-Olek*

Main category: eess.SY

TL;DR: 本文开发并分析了一种自适应反馈控制策略，用于在光学双势阱的不稳定强度最小值处稳定和限制纳米粒子，以克服传统光阱的吸收加热问题。


<details>
  <summary>Details</summary>
Motivation: 传统的纳米粒子光阱依赖于强度最大值处的束缚力，但这会导致吸收加热，限制了量子实验的进展。在强度最小值处捕获可减轻吸收加热。此外，需要解决实验中不可避免的测量非线性、光学设置慢漂移等问题。

Method: 研究人员在不稳定平衡点附近简化了模型，并采用了间接自适应控制技术来动态跟踪势能景观的变化。具体实现为一个简单高效的线性二次高斯（LQG）控制器，可部署在快速且经济的FPGA上。

Result: 该策略成功跟踪了强度最小值，显著降低了纳米粒子的残余态方差，有效降低了其质心温度。

Conclusion: 由于LQG控制可以自然地扩展到量子领域，该成果为未来超越当前吸收加热限制的量子态制备实验（如物质波干涉和量子引力界面测试）提供了一条有前景的途径。

Abstract: In this work, we develop and analyze adaptive feedback control strategies to
stabilize and confine a nanoparticle at the unstable intensity minimum of an
optical double-well potential. The resulting stochastic optimal control problem
for a noise-driven mechanical particle in a nonlinear optical potential must
account for unavoidable experimental imperfections such as measurement
nonlinearities and slow drifts of the optical setup. To address these issues,
we simplify the model in the vicinity of the unstable equilibrium and employ
indirect adaptive control techniques to dynamically follow changes in the
potential landscape. Our approach leads to a simple and efficient Linear
Quadratic Gaussian (LQG) controller that can be implemented on fast and
cost-effective FPGAs, ensuring accessibility and reproducibility. We
demonstrate that this strategy successfully tracks the intensity minimum and
significantly reduces the nanoparticle's residual state variance, effectively
lowering its center-of-mass temperature. While conventional optical traps rely
on confining optical forces in the light field at the intensity maxima,
trapping at intensity minima mitigates absorption heating, which is crucial for
advanced quantum experiments. Since LQG control naturally extends into the
quantum regime, our results provide a promising pathway for future experiments
on quantum state preparation beyond the current absorption heating limitation,
like matter-wave interference and tests of the quantum-gravity interface.

</details>


### [238] [A Robust Optimization Approach for Demand Response Participation of Fixed-Frequency Air Conditioners](https://arxiv.org/abs/2508.10679)
*Jinhua He,Tingzhe Pan,Chao Li,Xin Jin,Zijie Meng,Wei Zhou*

Main category: eess.SY

TL;DR: 本研究提出了一种针对定频空调（FFACs）参与需求响应（DR）的优化模型和求解方法，旨在最大化聚合商利润，同时考虑温度不确定性和用户舒适度。


<details>
  <summary>Details</summary>
Motivation: 随着可再生能源渗透率的提高，电力系统调峰压力显著增大。需求侧资源，特别是空调负荷，因其巨大的调节潜力和快速响应能力，被视为提供辅助调峰服务的潜在选择。

Method: 1. 基于马尔可夫假设，建立了FFACs的概率响应模型。2. 通过对概率模型采样，获得了分散控制下FFACs集群的总功耗。3. 构建了一个鲁棒优化模型，以在DR事件中最大化管理FFACs集群的聚合商的利润，并考虑了聚合响应功率和温度不确定性以确保用户舒适度。4. 将模型重新表述为混合整数线性规划（MILP）问题，并使用商业优化求解器进行求解。

Result: 仿真结果验证了所提出的模型和求解方法的有效性。

Conclusion: 所提出的模型和求解方法能够有效地优化定频空调参与需求响应，在最大化聚合商利润的同时，鲁棒地考虑了温度不确定性并保障了用户舒适度。

Abstract: With the continuous increase in the penetration of renewable energy in the
emerging power systems, the pressure on system peak regulation has been
significantly intensified. Against this backdrop, demand side resources
particularly air conditioning loads have garnered considerable attention for
their substantial regulation potential and fast response capabilities, making
them promising candidates for providing auxiliary peak shaving services. This
study focuses on fixed frequency air conditioners (FFACs) and proposes an
optimization model and solution method for their participation in demand
response (DR) programs. First, a probabilistic response model for FFACs is
developed based on the Markov assumption. Second, by sampling this
probabilistic model, the aggregate power consumption of an FFAC cluster under
decentralized control is obtained. Subsequently, a robust optimization model is
formulated to maximize the profit of an aggregator managing the FFAC cluster
during DR events, taking into account the aggregated response power. The model
explicitly considers temperature uncertainty to ensure user comfort in a robust
sense. Finally, leveraging the structure of the proposed model, it is
reformulated as a mixed-integer linear programming (MILP) problem and solved
using a commercial optimization solver. Simulation results validate the
effectiveness of the proposed model and solution approach.

</details>


### [239] [Probabilistic Forecasting Method for Offshore Wind Farm Cluster under Typhoon Conditions: a Score-Based Conditional Diffusion Model](https://arxiv.org/abs/2508.10705)
*Jinhua He,Zechun Hu*

Main category: eess.SY

TL;DR: 本文提出了一种基于分数的条件扩散模型（SCDM），用于台风条件下海上风电的概率预测，以应对传统方法的不足和数据稀缺性。


<details>
  <summary>Details</summary>
Motivation: 台风期间海上风电（OWP）波动剧烈，对电力系统安全运行构成挑战。由于历史台风数据稀缺和OWP的随机性，传统点预测方法难以准确预测，因此需要一种更全面、准确的预测方法来为电网运营商提供决策信息。

Method: 该研究首先使用知识图谱算法将历史台风路径嵌入为向量。然后，构建一个确定性网络，利用这些向量嵌入预测台风条件下的风电。最后，开发一个去噪网络来表征预测误差，其核心是一个均值回归随机微分方程（SDE），将复杂的误差分布转换为标准高斯分布，并通过逆时SDE采样预测误差。最终的概率预测结果通过结合确定性预测和采样的误差重建。

Result: 该方法在9个海上风电场的真实数据上进行了评估。结果表明，在台风条件下，该方法在确定性预测和概率性预测指标上均优于基线模型，验证了其有效性。

Conclusion: 所提出的基于分数的条件扩散模型（SCDM）能够有效应对台风条件下海上风电预测的挑战，提供准确的概率预测，优于现有基线模型。

Abstract: Offshore wind power (OWP) exhibits significant fluctuations under typhoon
conditions, posing substantial challenges to the secure operation of power
systems. Accurate forecasting of OWP is therefore essential. However, the
inherent scarcity of historical typhoon data and stochasticity of OWP render
traditional point forecasting methods particularly difficult and inadequate. To
address this challenge and provide grid operators with the comprehensive
information necessary for decision-making, this study proposes a score-based
conditional diffusion model (SCDM) for probabilistic forecasting of OWP during
typhoon events. First, a knowledge graph algorithm is employed to embed
historical typhoon paths as vectors. Then, a deterministic network is
constructed to predict the wind power under typhoon conditions based on these
vector embeddings. Finally, to better characterize prediction errors, a
denoising network is developed. At the core of this approach is a
mean-reverting stochastic differential equation (SDE), which transforms complex
error distributions into a standard Gaussian, enabling the sampling of
forecasting errors using a reverse-time SDE. The probabilistic forecasting
results are reconstructed by combining deterministic forecasts with sampled
errors. The proposed method is evaluated using real-world data from a cluster
of 9 offshore wind farms. Results demonstrate that under typhoon conditions,
our approach outperforms baseline models for both deterministic and
probabilistic metrics, verifying the effectiveness of the approach.

</details>


### [240] [Multi-Functional Polarization-Based Coverage Control through Static Passive EMSs](https://arxiv.org/abs/2508.10730)
*Giacomo Oliveri,Francesco Zardi,Aaron Angel Salas Sanchez,Andrea Massa*

Main category: eess.SY

TL;DR: 提出了一种创新的多功能静被动电磁皮肤（SP-EMS）解决方案，该方案通过单一超原子阵列，在相同频率但不同极化状态的两个电磁源照射下，同时支持两种独立的波操纵功能。


<details>
  <summary>Details</summary>
Motivation: 现有技术可能难以在单一超原子阵列上同时实现针对不同极化状态的多个独立波操纵功能，本研究旨在解决这一挑战，以实现更高效、多功能的电磁皮肤。

Method: 首先设计一个简单的参考超原子，以精确独立控制局部反射张量的每个极化分量。随后，通过解决一个全局优化问题进行多极化SP-EMS（MP-SP-EMS）的宏观合成，该优化问题使用定制的系统设计（SbD）技术最小化一个数学编码了每个极化独立需求的成本函数。

Result: 数值和实验测试结果表明，基于极化分集的多功能EMS是可行的，并且所提出的MP-SP-EMS合成方法有效且鲁棒。

Conclusion: 所提出的基于极化分集的多功能SP-EMS解决方案及其合成方法，能够有效实现单一电磁皮肤孔径上同时支持两种独立波操纵功能，具有良好的可行性、有效性和鲁棒性。

Abstract: An innovative multi-functional static-passive electromagnetic skin (SP-EMS)
solution is proposed to simultaneously support, in reflection, two independent
wave-manipulation functionalities with a single meta-atoms arrangement on the
EMS aperture when illuminated by two EM sources operating at the same
frequency, but working in different polarization states. Towards this end, a
simple reference meta-atom is designed first to enable an accurate and
independent control of each polarization component of the local reflection
tensor. Successively, the macro-scale synthesis of multi-polarization (MP)
SP-EMSs (MP-SP-EMSs) is carried out by solving a global optimization problem
where a cost function, which mathematically codes separate requirements for
each polarization, is minimized with a customized version of the
system-by-design (SbD) technique. Representative results from a set of
numerical and experimental tests are reported to assess the feasibility of a
multi-function EMS based on polarization diversity as well as the effectiveness
and the robustness of the proposed method for the synthesis of MP-SP-EMSs.

</details>


### [241] [Integrating Terrestrial and Non-Terrestrial Networks for Sustainable 6G Operations: A Latency-Aware Multi-Tier Cell-Switching Approach](https://arxiv.org/abs/2508.10849)
*Metin Ozturk,Maryam Salamatmoghadasi,Halim Yanikomeroglu*

Main category: eess.SY

TL;DR: 本研究通过将非地面网络（NTN，包括首次用于小区切换的卫星、HAPS和UAV）与地面网络（TN）结合，并引入多层小区切换和AI优化，显著提升了蜂窝网络的能源效率、容量和覆盖范围。


<details>
  <summary>Details</summary>
Motivation: 现代蜂窝网络面临着巨大的能耗挑战，现有的小区切换技术在地面网络中存在容量不足和覆盖有限的局限性。

Method: 该研究提出了一种增强型小区切换方案，将卫星、高空平台站（HAPS）和无人机（UAV）等非地面网络（NTN）集成到地面网络（TN）中。引入了多层小区切换方法，动态地在网络层之间分流用户，并采用上下文感知策略。此外，探索了人工智能（特别是生成式AI）在数据压缩、不同网络层间切换优化和增强设备兼容性方面的作用。

Result: 通过集成NTN，显著提升了能源节约、容量、覆盖和操作灵活性。案例研究证实，该方法大幅改善了网络功耗和用户满意度。

Conclusion: 所提出的NTN集成、多层小区切换和AI优化方法，为未来网络的能源效率和用户满意度带来了巨大潜力。

Abstract: Sustainability is paramount in modern cellular networks, which face
significant energy consumption challenges from rising mobile traffic and
advancements in wireless technology. Cell-switching, well-established in
literature as an effective solution, encounters limitations such as inadequate
capacity and limited coverage when implemented through terrestrial networks
(TN). This study enhances cell-switching by integrating non-terrestrial
networks (NTN), including satellites (used for cell-switching for the first
time), high altitude platform stations (HAPS), and uncrewed aerial vehicles
(UAVs) into TN. This integration significantly boosts energy savings by
expanding capacity, enhancing coverage, and increasing operational flexibility.
We introduce a multi-tier cell-switching approach that dynamically offloads
users across network layers to manage energy effectively and minimize delays,
accommodating diverse user demands with a context aware strategy. Additionally,
we explore the role of artificial intelligence (AI), particularly generative
AI, in optimizing network efficiency through data compression, handover
optimization between different network layers, and enhancing device
compatibility, further improving the adaptability and energy efficiency of
cell-switching operations. A case study confirms substantial improvements in
network power consumption and user satisfaction, demonstrating the potential of
our approach for future networks.

</details>


### [242] [Fuel Consumption in Platoons: A Literature Review](https://arxiv.org/abs/2508.10891)
*Oumaima Barhoumi,Ghazal Farhani,Taufiq Rahman,Mohamed H. Zaki,Sofiène Tahar,Fadi Araji*

Main category: eess.SY

TL;DR: 该文献综述全面审视了自动驾驶车队（Platooning）在提高燃油效率方面的潜力，涵盖了影响燃油节约的各种因素、组件、评估方法以及不稳定性带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 车队编队被认为是提高自动驾驶车辆燃油效率、减少排放和运营成本的有效策略。现有研究多集中于单一方面，缺乏综合性视角，因此需要一个更全面的分析来整合各种影响燃油节约的因素和组件。

Method: 本文采用文献综述的方法，整合了近期研究的见解，系统性地审查了车队编队对燃油消耗的影响。

Result: 研究结果涵盖了车队系统中的关键组件、影响燃油节约的因素和参与者、燃油使用估算方法、车队不稳定性对效率的影响、气动阻力减少的作用、车辆协调的重要性，以及真实世界条件下不稳定性带来的挑战。

Conclusion: 该综述提供了车队编队技术最新进展的全面概览，并指出了在真实场景中最大化燃油节约所面临的挑战和未来的研究机遇。

Abstract: Platooning has emerged as a promising strategy for improving fuel efficiency
in automated vehicle systems, with significant implications for reducing
emissions and operational costs. While existing literature on vehicle
platooning primarily focuses on individual aspects such as aerodynamic drag
reduction or specific control strategies, this work takes a more comprehensive
approach by bringing together a wide range of factors and components that
contribute to fuel savings in platoons. In this literature review, we examine
the impact of platooning on fuel consumption, highlighting the key components
of platoon systems, the factors and actors influencing fuel savings, methods
for estimating fuel use, and the effect of platoon instability on efficiency.
Furthermore, we study the role of reduced aerodynamic drag, vehicle
coordination, and the challenges posed by instability in real-world conditions.
By compiling insights from recent studies, this work provides a comprehensive
overview of the latest advancements in platooning technologies and highlights
both the challenges and opportunities for future research to maximize fuel
savings in real-world scenarios.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [243] [Explainable AI Technique in Lung Cancer Detection Using Convolutional Neural Networks](https://arxiv.org/abs/2508.10196)
*Nishan Rai,Sujan Khatri,Devendra Risal*

Main category: eess.IV

TL;DR: 该研究提出了一个深度学习框架，用于从胸部CT图像自动筛查肺癌，并集成了可解释性，以提高诊断准确性和临床透明度。


<details>
  <summary>Details</summary>
Motivation: 早期发现肺癌对于提高患者生存率至关重要。研究旨在开发一个自动化、准确且可解释的工具来辅助肺癌筛查。

Method: 研究使用IQ-OTH/NCCD数据集（包含正常、良性和恶性三类共1197张扫描图像），评估了一个自定义卷积神经网络（CNN）以及三个经过微调的迁移学习骨干网络：DenseNet121、ResNet152和VGG19。模型采用成本敏感学习来缓解类别不平衡问题，并通过准确率、精确率、召回率、F1分数和ROC-AUC进行评估。此外，应用Shapley Additive Explanations (SHAP) 来可视化预测证据，增强临床透明度。

Result: ResNet152模型实现了最高的准确率（97.3%）。DenseNet121模型在精确率、召回率和F1分数方面提供了最佳的整体平衡（分别高达92%、90%和91%）。SHAP的应用提升了预测结果的临床可解释性。

Conclusion: 结合可解释性的CNN方法能够为肺癌筛查提供快速、准确和可解释的支持，尤其在资源有限的环境中具有重要应用潜力。

Abstract: Early detection of lung cancer is critical to improving survival outcomes. We
present a deep learning framework for automated lung cancer screening from
chest computed tomography (CT) images with integrated explainability. Using the
IQ-OTH/NCCD dataset (1,197 scans across Normal, Benign, and Malignant classes),
we evaluate a custom convolutional neural network (CNN) and three fine-tuned
transfer learning backbones: DenseNet121, ResNet152, and VGG19. Models are
trained with cost-sensitive learning to mitigate class imbalance and evaluated
via accuracy, precision, recall, F1-score, and ROC-AUC. While ResNet152
achieved the highest accuracy (97.3%), DenseNet121 provided the best overall
balance in precision, recall, and F1 (up to 92%, 90%, 91%, respectively). We
further apply Shapley Additive Explanations (SHAP) to visualize evidence
contributing to predictions, improving clinical transparency. Results indicate
that CNN-based approaches augmented with explainability can provide fast,
accurate, and interpretable support for lung cancer screening, particularly in
resource-limited settings.

</details>


### [244] [Data-Efficient Learning for Generalizable Surgical Video Understanding](https://arxiv.org/abs/2508.10215)
*Sahar Nasirihaghighi*

Main category: eess.IV

TL;DR: 该博士研究旨在通过开发半监督学习框架和发布大规模数据集，解决手术视频分析中标签稀缺和领域差异等挑战，从而推动深度学习在真实临床场景中的应用。


<details>
  <summary>Details</summary>
Motivation: 手术视频分析正在将手术室转变为智能、数据驱动的环境，支持从术前规划到术后评估的完整工作流程。然而，由于（I）标注稀缺性、（II）时空复杂性和（III）不同手术和机构间的领域差异，开发鲁棒且泛化性强的模型仍然面临挑战。本研究旨在弥合深度学习手术视频分析在研究与实际临床部署之间的差距。

Method: 研究方法包括：1) 对最先进的神经网络架构进行基准测试，以识别用于识别手术阶段、动作和事件的最有效设计；2) 提出新的架构并集成先进模块以提高性能；3) 开发半监督框架（如DIST、SemiVT-Surge和ENCORE），利用大量未标注手术视频，减少对标注数据的依赖，并通过动态伪标签增强模型训练；4) 发布了两个多任务数据集：GynSurg（最大的妇科腹腔镜数据集）和Cataract-1K（最大的白内障手术视频数据集）。

Result: 研究结果包括：1) 识别了针对不同手术识别任务的有效模型设计；2) 通过新颖的架构和先进模块提高了性能；3) 开发的半监督框架（DIST、SemiVT-Surge和ENCORE）在具有挑战性的手术数据集上，利用最少量的标注数据实现了最先进的成果；4) 成功发布了两个大型多任务数据集GynSurg和Cataract-1K，以支持领域内的可复现性和进展。

Conclusion: 这项工作为手术视频分析提供了鲁棒、数据高效且临床可扩展的解决方案，为可泛化的人工智能系统奠定了基础，这些系统有望对手术护理和培训产生有意义的影响。

Abstract: Advances in surgical video analysis are transforming operating rooms into
intelligent, data-driven environments. Computer-assisted systems support full
surgical workflow, from preoperative planning to intraoperative guidance and
postoperative assessment. However, developing robust and generalizable models
for surgical video understanding remains challenging due to (I) annotation
scarcity, (II) spatiotemporal complexity, and (III) domain gap across
procedures and institutions. This doctoral research aims to bridge the gap
between deep learning-based surgical video analysis in research and its
real-world clinical deployment. To address the core challenge of recognizing
surgical phases, actions, and events, critical for analysis, I benchmarked
state-of-the-art neural network architectures to identify the most effective
designs for each task. I further improved performance by proposing novel
architectures and integrating advanced modules. Given the high cost of expert
annotations and the domain gap across surgical video sources, I focused on
reducing reliance on labeled data. We developed semi-supervised frameworks that
improve model performance across tasks by leveraging large amounts of unlabeled
surgical video. We introduced novel semi-supervised frameworks, including DIST,
SemiVT-Surge, and ENCORE, that achieved state-of-the-art results on challenging
surgical datasets by leveraging minimal labeled data and enhancing model
training through dynamic pseudo-labeling. To support reproducibility and
advance the field, we released two multi-task datasets: GynSurg, the largest
gynecologic laparoscopy dataset, and Cataract-1K, the largest cataract surgery
video dataset. Together, this work contributes to robust, data-efficient, and
clinically scalable solutions for surgical video analysis, laying the
foundation for generalizable AI systems that can meaningfully impact surgical
care and training.

</details>


### [245] [DINOMotion: advanced robust tissue motion tracking with DINOv2 in 2D-Cine MRI-guided radiotherapy](https://arxiv.org/abs/2508.10260)
*Soorena Salari,Catherine Spino,Laurie-Anne Pharand,Fabienne Lathuiliere,Hassan Rivaz,Silvain Beriault,Yiming Xiao*

Main category: eess.IV

TL;DR: DINOMotion是一种基于DINOv2和LoRA的深度学习框架，用于实现2D-Cine MRI引导放疗中鲁棒、高效且可解释的组织运动跟踪。


<details>
  <summary>Details</summary>
Motivation: 2D-Cine MRI引导放疗中精确的组织运动跟踪对治疗效果和安全性至关重要，但现有方法在处理大错位和缺乏可解释性方面面临挑战。

Method: 本文提出DINOMotion框架，它结合了DINOv2强大的特征表示能力和LoRA层以减少可训练参数并提高训练效率。该方法通过自动检测对应地标来推导最佳图像配准，提供明确的视觉对应关系以增强可解释性，并能在测试时直接计算图像配准结果，而非采用迭代优化。

Result: 在志愿者和患者数据集上，DINOMotion在肾脏、肝脏和肺的Dice分数分别达到92.07%、90.90%和95.23%，相应的Hausdorff距离分别为5.47毫米、8.31毫米和6.72毫米。它能处理线性和非线性变换，每张扫描处理时间约为30毫秒，并且在处理大错位方面持续优于现有最先进方法。

Conclusion: DINOMotion为2D-Cine MRI引导放疗中的实时运动跟踪提供了一个鲁棒且可解释的解决方案，具有巨大的应用潜力。

Abstract: Accurate tissue motion tracking is critical to ensure treatment outcome and
safety in 2D-Cine MRI-guided radiotherapy. This is typically achieved by
registration of sequential images, but existing methods often face challenges
with large misalignments and lack of interpretability. In this paper, we
introduce DINOMotion, a novel deep learning framework based on DINOv2 with
Low-Rank Adaptation (LoRA) layers for robust, efficient, and interpretable
motion tracking. DINOMotion automatically detects corresponding landmarks to
derive optimal image registration, enhancing interpretability by providing
explicit visual correspondences between sequential images. The integration of
LoRA layers reduces trainable parameters, improving training efficiency, while
DINOv2's powerful feature representations offer robustness against large
misalignments. Unlike iterative optimization-based methods, DINOMotion directly
computes image registration at test time. Our experiments on volunteer and
patient datasets demonstrate its effectiveness in estimating both linear and
nonlinear transformations, achieving Dice scores of 92.07% for the kidney,
90.90% for the liver, and 95.23% for the lung, with corresponding Hausdorff
distances of 5.47 mm, 8.31 mm, and 6.72 mm, respectively. DINOMotion processes
each scan in approximately 30ms and consistently outperforms state-of-the-art
methods, particularly in handling large misalignments. These results highlight
its potential as a robust and interpretable solution for real-time motion
tracking in 2D-Cine MRI-guided radiotherapy.

</details>


### [246] [Efficient Image Denoising Using Global and Local Circulant Representation](https://arxiv.org/abs/2508.10307)
*Zhaoming Kong,Jiahuan Zhang,Xiaowei Yang*

Main category: eess.IV

TL;DR: 本文提出了一种名为Haar-tSVD的图像去噪算法，它利用非局部自相似性、Haar变换和张量奇异值分解（t-SVD）来高效去除噪声并保留细节，同时结合了自适应噪声估计。


<details>
  <summary>Details</summary>
Motivation: 随着成像设备和日常生成图像数据的快速发展，对高效、有效的图像去噪提出了越来越高的要求。

Method: 该方法提出了Haar-tSVD去噪算法，探索了非局部自相似性先验，并利用主成分分析（PCA）与循环表示下Haar变换之间的联系。通过结合Haar变换的统一张量奇异值分解（t-SVD）投影，有效捕获全局和局部图像块相关性。这形成了一个一步式、高度并行化的滤波方法，无需学习局部基来表示图像块。此外，引入了一种基于CNN估计器和特征值分析的自适应噪声估计方案，以增强方法的鲁棒性和适应性。

Result: 实验结果表明，Haar-tSVD在不同的真实世界去噪任务中，对噪声去除和细节保留都表现出高效性和有效性，并在去噪速度和性能之间取得了平衡。

Conclusion: Haar-tSVD是一种计算简单、高效且有效的图像去噪算法，通过结合Haar变换、t-SVD和自适应噪声估计，在去噪性能和速度之间取得了良好的平衡，适用于实际的噪声去除和细节保留任务。

Abstract: The advancement of imaging devices and countless image data generated
everyday impose an increasingly high demand on efficient and effective image
denoising. In this paper, we present a computationally simple denoising
algorithm, termed Haar-tSVD, aiming to explore the nonlocal self-similarity
prior and leverage the connection between principal component analysis (PCA)
and the Haar transform under circulant representation. We show that global and
local patch correlations can be effectively captured through a unified
tensor-singular value decomposition (t-SVD) projection with the Haar transform.
This results in a one-step, highly parallelizable filtering method that
eliminates the need for learning local bases to represent image patches,
striking a balance between denoising speed and performance. Furthermore, we
introduce an adaptive noise estimation scheme based on a CNN estimator and
eigenvalue analysis to enhance the robustness and adaptability of the proposed
method. Experiments on different real-world denoising tasks validate the
efficiency and effectiveness of Haar-tSVD for noise removal and detail
preservation. Datasets, code and results are publicly available at
https://github.com/ZhaomingKong/Haar-tSVD.

</details>


### [247] [Cross-view Generalized Diffusion Model for Sparse-view CT Reconstruction](https://arxiv.org/abs/2508.10313)
*Jixiang Chen,Yiqun Lin,Yi Qin,Hualiang Wang,Xiaomeng Li*

Main category: eess.IV

TL;DR: CvG-Diff是一种新型的广义扩散模型，通过将稀疏视图CT重建重构为确定性降级过程，并引入误差传播复合训练和语义优先双阶段采样策略，实现了高质量、高效率的稀疏视图CT图像重建，解决了传统方法伪影严重、过度平滑以及扩散模型效率低、稳定性差的问题。


<details>
  <summary>Details</summary>
Motivation: 稀疏视图CT能减少辐射暴露，但传统重建方法会产生严重伪影。现有深度学习方法在高度稀疏时易导致过度平滑。扩散模型虽能改进重建，但需要大量采样步骤且在高度稀疏场景下稳定性差。

Method: CvG-Diff将稀疏视图CT重建重构为广义扩散过程，明确将欠采样引起的图像域伪影建模为确定性降级算子。为解决伪影传播和顺序采样效率低下问题，引入了两项创新：误差传播复合训练（EPCT）以识别误差区域并抑制伪影传播，以及语义优先双阶段采样（SPDPS）这一自适应策略，优先保证语义正确性再进行细节细化。

Result: CvG-Diff能在极少迭代次数下实现高质量重建，例如在AAPM-LDCT数据集上，18视图CT仅需10步即可达到38.34 dB PSNR和0.9518 SSIM。实验证明其性能优于现有最先进的稀疏视图CT重建方法。

Conclusion: CvG-Diff通过创新的广义扩散模型框架和训练/采样策略，成功解决了稀疏视图CT重建中的伪影、平滑和效率问题，实现了高质量且高效的图像重建。

Abstract: Sparse-view computed tomography (CT) reduces radiation exposure by
subsampling projection views, but conventional reconstruction methods produce
severe streak artifacts with undersampled data. While deep-learning-based
methods enable single-step artifact suppression, they often produce
over-smoothed results under significant sparsity. Though diffusion models
improve reconstruction via iterative refinement and generative priors, they
require hundreds of sampling steps and struggle with stability in highly sparse
regimes. To tackle these concerns, we present the Cross-view Generalized
Diffusion Model (CvG-Diff), which reformulates sparse-view CT reconstruction as
a generalized diffusion process. Unlike existing diffusion approaches that rely
on stochastic Gaussian degradation, CvG-Diff explicitly models image-domain
artifacts caused by angular subsampling as a deterministic degradation
operator, leveraging correlations across sparse-view CT at different sample
rates. To address the inherent artifact propagation and inefficiency of
sequential sampling in generalized diffusion model, we introduce two
innovations: Error-Propagating Composite Training (EPCT), which facilitates
identifying error-prone regions and suppresses propagated artifacts, and
Semantic-Prioritized Dual-Phase Sampling (SPDPS), an adaptive strategy that
prioritizes semantic correctness before detail refinement. Together, these
innovations enable CvG-Diff to achieve high-quality reconstructions with
minimal iterations, achieving 38.34 dB PSNR and 0.9518 SSIM for 18-view CT
using only \textbf{10} steps on AAPM-LDCT dataset. Extensive experiments
demonstrate the superiority of CvG-Diff over state-of-the-art sparse-view CT
reconstruction methods. The code is available at
https://github.com/xmed-lab/CvG-Diff.

</details>


### [248] [DIVA-VQA: Detecting Inter-frame Variations in UGC Video Quality](https://arxiv.org/abs/2508.10605)
*Xinyi Wang,Angeliki Katsenou,David Bull*

Main category: eess.IV

TL;DR: 本文提出了一种基于时空碎片化和帧间变化的无参考视频质量评估（NR-VQA）模型，在用户生成内容（UGC）数据集上表现出色，且运行效率高。


<details>
  <summary>Details</summary>
Motivation: 用户生成内容（UGC）的快速增长，以及社交媒体和流媒体应用中缺乏原始参考视频的场景，推动了对无参考感知视频质量评估（NR-VQA）研究的迫切需求。

Method: 该模型利用帧间差异进行时空碎片化，逐步分析帧、补丁和碎片化帧等多个层面的质量敏感区域。它整合了帧、碎片化残差以及与残差对齐的碎片化帧，以捕获全局和局部信息，并提取2D和3D特征来表征时空变化。

Result: 在五个UGC数据集上与现有最先进模型进行实验，所提出的方法在平均秩相关性方面排名前两名（DIVA-VQA-L: 0.898，DIVA-VQA-B: 0.886）。同时，该方法具有较低的运行时复杂度，DIVA-VQA-B平均排名第一，DIVA-VQA-L平均排名第三，优于现有最快的NR-VQA方法。

Conclusion: 所提出的基于时空碎片化的NR-VQA模型在用户生成内容视频质量评估中表现出卓越的性能和高效率，有效解决了无参考视频质量评估的挑战。

Abstract: The rapid growth of user-generated (video) content (UGC) has driven increased
demand for research on no-reference (NR) perceptual video quality assessment
(VQA). NR-VQA is a key component for large-scale video quality monitoring in
social media and streaming applications where a pristine reference is not
available. This paper proposes a novel NR-VQA model based on spatio-temporal
fragmentation driven by inter-frame variations. By leveraging these inter-frame
differences, the model progressively analyses quality-sensitive regions at
multiple levels: frames, patches, and fragmented frames. It integrates frames,
fragmented residuals, and fragmented frames aligned with residuals to
effectively capture global and local information. The model extracts both 2D
and 3D features in order to characterize these spatio-temporal variations.
Experiments conducted on five UGC datasets and against state-of-the-art models
ranked our proposed method among the top 2 in terms of average rank correlation
(DIVA-VQA-L: 0.898 and DIVA-VQA-B: 0.886). The improved performance is offered
at a low runtime complexity, with DIVA-VQA-B ranked top and DIVA-VQA-L third on
average compared to the fastest existing NR-VQA method. Code and models are
publicly available at: https://github.com/xinyiW915/DIVA-VQA.

</details>


### [249] [When Experts Disagree: Characterizing Annotator Variability for Vessel Segmentation in DSA Images](https://arxiv.org/abs/2508.10797)
*M. Geshvadi,G. So,D. D. Chlorogiannis,C. Galvin,E. Torio,A. Azimi,Y. Tachie-Baffour,N. Haouchine,A. Golby,M. Vangel,W. M. Wells,Y. Epelboym,R. Du,F. Durupinar,S. Frisken*

Main category: eess.IV

TL;DR: 该研究分析了多位标注者对颅内血管2D DSA图像分割的变异性，旨在量化分割不确定性，并探讨其在指导额外标注和开发不确定性感知自动分割方法中的应用。


<details>
  <summary>Details</summary>
Motivation: 为了表征和量化颅内血管2D DSA图像分割中的不确定性。

Method: 分析多位标注者进行的血管分割结果之间的变异性。

Result: 成功量化了分割不确定性。

Conclusion: 量化后的分割不确定性可用于指导额外的标注工作，并有助于开发具有不确定性感知能力的自动分割方法。

Abstract: We analyze the variability among segmentations of cranial blood vessels in 2D
DSA performed by multiple annotators in order to characterize and quantify
segmentation uncertainty. We use this analysis to quantify segmentation
uncertainty and discuss ways it can be used to guide additional annotations and
to develop uncertainty-aware automatic segmentation methods.

</details>
