{"id": "2601.14352", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.14352", "abs": "https://arxiv.org/abs/2601.14352", "authors": ["Huajie Tan", "Enshen Zhou", "Zhiyu Li", "Yijie Xu", "Yuheng Ji", "Xiansheng Chen", "Cheng Chi", "Pengwei Wang", "Huizhu Jia", "Yulong Ao", "Mingyu Cao", "Sixiang Chen", "Zhe Li", "Mengzhen Liu", "Zixiao Wang", "Shanyu Rong", "Yaoxu Lyu", "Zhongxia Zhao", "Peterson Co", "Yibo Li", "Yi Han", "Shaoxuan Xie", "Guocai Yao", "Songjing Wang", "Leiduo Zhang", "Xi Yang", "Yance Jiao", "Donghai Shi", "Kunchang Xie", "Shaokai Nie", "Chunlei Men", "Yonghua Lin", "Zhongyuan Wang", "Tiejun Huang", "Shanghang Zhang"], "title": "RoboBrain 2.5: Depth in Sight, Time in Mind", "comment": "37 pages, 13 figures, Technical Report", "summary": "We introduce RoboBrain 2.5, a next-generation embodied AI foundation model that advances general perception, spatial reasoning, and temporal modeling through extensive training on high-quality spatiotemporal supervision. Building upon its predecessor, RoboBrain 2.5 introduces two major capability upgrades. Specifically, it unlocks Precise 3D Spatial Reasoning by shifting from 2D pixel-relative grounding to depth-aware coordinate prediction and absolute metric constraint comprehension, generating complete 3D manipulation traces as ordered keypoint sequences under physical constraints. Complementing this spatial precision, the model establishes Dense Temporal Value Estimation that provides dense, step-aware progress prediction and execution state understanding across varying viewpoints, producing stable feedback signals for downstream learning. Together, these upgrades extend the framework toward more physically grounded and execution-aware embodied intelligence for complex, fine-grained manipulation. The code and checkpoints are available at project website: https://superrobobrain.github.io", "AI": {"tldr": "RoboBrain 2.5 是一个新一代的具身AI基础模型，通过高质量的时空监督训练，提升了通用感知、空间推理和时间建模能力。它实现了精确的3D空间推理和密集的时序价值估计，为复杂精细的操作提供了更强的物理约束和执行意识。", "motivation": "在现有具身AI模型的基础上，提升其在精确3D空间推理和时间理解方面的能力，以实现更复杂、更精细的操控任务。", "method": "通过大量的时空监督数据进行训练。主要技术升级包括：1. 从2D像素相对定位转向深度感知坐标预测和绝对度量约束理解，实现精确3D空间推理。2. 引入密集时序价值估计，提供步进式进度预测和不同视角的执行状态理解。", "result": "RoboBrain 2.5 能够生成满足物理约束的3D操作轨迹（有序关键点序列），并提供稳定的、面向下游学习的反馈信号，从而实现更强的物理约束和执行意识。", "conclusion": "RoboBrain 2.5 在通用感知、空间推理和时间建模方面取得了显著进步，为开发更先进的具身智能体以应对复杂精细操作任务奠定了基础。"}}
{"id": "2601.14613", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.14613", "abs": "https://arxiv.org/abs/2601.14613", "authors": ["Tingwei Zhang", "Jiahui Liu", "David Allstot", "Huaping Liu"], "title": "An Ion-Intercalation Memristor for Enabling Full Parallel Writing in Crossbar Networks", "comment": null, "summary": "Crossbar architectures have long been seen as a promising foundation for in-memory computing, using memristor arrays for high-density, energy-efficient analog computation. However, this conventional architecture suffers from a fundamental limitation: the inability to perform parallel write operations due to the sneak path problem. This arises from the structural overlap of read and write paths, forcing sequential or semi-parallel updates and severely limiting scalability. To address this, we introduce a new memristor design that decouples read and write operations at the device level. This design enables orthogonal conductive paths, and employs a reversible ion doping mechanism, inspired by lithium-ion battery principles, to modulate resistance states independently of computation. Fabricated devices exhibit near-ideal memristive characteristics and stable performance under isolated read/write conditions.", "AI": {"tldr": "提出了一种新型忆阻器设计，通过设备级别的读写分离解决了传统交叉开关架构中存在的串扰问题，从而实现了独立的并行读写操作，提高了可扩展性。", "motivation": "传统交叉开关架构在内存计算中存在串扰问题，导致无法进行并行写操作，严重限制了可扩展性。", "method": "设计了一种新型忆阻器，通过实现读写操作的设备级别分离，利用可逆离子掺杂机制独立地调节电阻状态。", "result": "制造出的器件表现出接近理想的忆阻特性，并且在隔离的读写条件下表现出稳定的性能。", "conclusion": "所提出的新型忆阻器设计成功地解决了传统架构的局限性，实现了独立的并行读写操作，为高效节能的内存计算奠定了基础。"}}
{"id": "2601.14437", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14437", "abs": "https://arxiv.org/abs/2601.14437", "authors": ["Thuan Minh Nguyen", "Vu Tuan Truong", "Long Bao Le"], "title": "Agentic AI Meets Edge Computing in Autonomous UAV Swarms", "comment": null, "summary": "The integration of agentic AI, powered by large language models (LLMs) with autonomous reasoning, planning, and execution, into unmanned aerial vehicle (UAV) swarms opens new operational possibilities and brings the vision of the Internet of Drones closer to reality. However, infrastructure constraints, dynamic environments, and the computational demands of multi-agent coordination limit real-world deployment in high-risk scenarios such as wildfires and disaster response. This paper investigates the integration of LLM-based agentic AI and edge computing to realize scalable and resilient autonomy in UAV swarms. We first discuss three architectures for supporting UAV swarms - standalone, edge-enabled, and edge-cloud hybrid deployment - each optimized for varying autonomy and connectivity levels. Then, a use case for wildfire search and rescue (SAR) is designed to demonstrate the efficiency of the edge-enabled architecture, enabling high SAR coverage, reduced mission completion times, and a higher level of autonomy compared to traditional approaches. Finally, we highlight open challenges in integrating LLMs and edge computing for mission-critical UAV-swarm applications.", "AI": {"tldr": "该研究探讨了如何将基于大语言模型（LLM）的智能体AI与边缘计算相结合，以增强无人机（UAV）集群的可扩展性和韧性，尤其是在野火搜救等高风险场景下。", "motivation": "当前无人机集群在基础设施限制、动态环境和高昂计算成本方面面临挑战，限制了其在高风险场景下的实际应用。研究旨在克服这些限制，实现无人机集群的可扩展和弹性自主。", "method": "文章提出了三种支持无人机集群的架构：独立部署、边缘赋能部署和边缘-云混合部署。并通过一个野火搜救（SAR）用例来演示边缘赋能架构的有效性。", "result": "边缘赋能架构在野火搜救用例中展现了更高的搜救覆盖率、更短的任务完成时间和更高水平的自主性，优于传统方法。", "conclusion": "将LLM驱动的智能体AI与边缘计算集成是实现无人机集群在任务关键型应用中可扩展和韧性自主的一种有前途的方法，但仍存在一些开放性挑战需要解决。"}}
{"id": "2601.14258", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2601.14258", "abs": "https://arxiv.org/abs/2601.14258", "authors": ["Ho Yin Au", "Junkun Jiang", "Jie Chen"], "title": "SOSControl: Enhancing Human Motion Generation through Saliency-Aware Symbolic Orientation and Timing Control", "comment": "Accepted by AAAI 2026", "summary": "Traditional text-to-motion frameworks often lack precise control, and existing approaches based on joint keyframe locations provide only positional guidance, making it challenging and unintuitive to specify body part orientations and motion timing. To address these limitations, we introduce the Salient Orientation Symbolic (SOS) script, a programmable symbolic framework for specifying body part orientations and motion timing at keyframes. We further propose an automatic SOS extraction pipeline that employs temporally-constrained agglomerative clustering for frame saliency detection and a Saliency-based Masking Scheme (SMS) to generate sparse, interpretable SOS scripts directly from motion data. Moreover, we present the SOSControl framework, which treats the available orientation symbols in the sparse SOS script as salient and prioritizes satisfying these constraints during motion generation. By incorporating SMS-based data augmentation and gradient-based iterative optimization, the framework enhances alignment with user-specified constraints. Additionally, it employs a ControlNet-based ACTOR-PAE Decoder to ensure smooth and natural motion outputs. Extensive experiments demonstrate that the SOS extraction pipeline generates human-interpretable scripts with symbolic annotations at salient keyframes, while the SOSControl framework outperforms existing baselines in motion quality, controllability, and generalizability with respect to motion timing and body part orientation control.", "AI": {"tldr": "本研究提出了一种名为SOS（Salient Orientation Symbolic）脚本的新型符号化框架，用于精确控制文本到运动生成中的身体部位方向和运动时序。研究还开发了一个自动提取SOS脚本的流程，并通过SOSControl框架利用这些脚本来生成高质量、可控且具有泛化能力的运动。", "motivation": "现有文本到运动生成方法在精确控制身体部位方向和运动时序方面存在不足，难以直观地指定这些关键要素。", "method": "1. 提出SOS脚本框架，用于在关键帧指定身体部位方向和运动时序。 2. 开发自动SOS提取流程，包括时间约束的聚类算法进行帧显著性检测，以及基于显著性的掩码方案（SMS）来生成稀疏、可解释的SOS脚本。 3. 提出SOSControl框架，优先满足SOS脚本中的方向约束，并结合SMS数据增强和梯度优化来提升用户指定约束的对齐度。 4. 使用基于ControlNet的ACTOR-PAE解码器确保运动的平滑自然。", "result": "SOS提取流程能生成带有显著关键帧符号注释的人类可解释脚本。SOSControl框架在运动质量、可控性以及运动时序和身体部位方向控制的泛化能力方面优于现有基线方法。", "conclusion": "SOS脚本框架和SOSControl系统为文本到运动生成提供了更精确、直观的控制方式，显著提升了生成运动的质量和用户控制能力。"}}
{"id": "2601.14334", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14334", "abs": "https://arxiv.org/abs/2601.14334", "authors": ["Junhyuk Heo"], "title": "Self-Supervised Score-Based Despeckling for SAR Imagery via Log-Domain Transformation", "comment": null, "summary": "The speckle noise inherent in Synthetic Aperture Radar (SAR) imagery significantly degrades image quality and complicates subsequent analysis. Given that SAR speckle is multiplicative and Gamma-distributed, effectively despeckling SAR imagery remains challenging. This paper introduces a novel self-supervised framework for SAR image despeckling based on score-based generative models operating in the transformed log domain. We first transform the data into the log-domain and then convert the speckle noise residuals into an approximately additive Gaussian distribution. This step enables the application of score-based models, which are trained in the transformed domain using a self-supervised objective. This objective allows our model to learn the clean underlying signal by training on further corrupted versions of the input data itself. Consequently, our method exhibits significantly shorter inference times compared to many existing self-supervised techniques, offering a robust and practical solution for SAR image restoration.", "AI": {"tldr": "本文提出了一种基于分数生成模型的SAR图像去噪新方法，通过将图像数据转换为对数域，将乘性Gamma分布的斑点噪声转化为近似加性高斯噪声，并采用自监督学习策略进行训练，有效缩短了推理时间，提高了去噪效果。", "motivation": "SAR图像固有的斑点噪声会严重影响图像质量和后续分析，尽管斑点噪声是乘性且服从Gamma分布的，但有效去除仍具挑战性。", "method": "1. 将SAR图像数据转换到对数域。\n2. 将对数域中的噪声残差转化为近似加性高斯分布。\n3. 应用分数生成模型，并在转换域使用自监督目标进行训练。\n4. 自监督目标允许模型通过对输入数据本身进行更严重腐蚀的版本进行训练来学习干净的底层信号。", "result": "与许多现有的自监督技术相比，该方法显著缩短了推理时间，并提供了鲁棒且实用的SAR图像恢复解决方案。", "conclusion": "该方法通过对数域变换和分数生成模型的自监督学习，能够有效地去除SAR图像中的斑点噪声，同时兼顾了处理速度和去噪性能，是一种有前景的SAR图像去噪技术。"}}
{"id": "2601.14643", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.14643", "abs": "https://arxiv.org/abs/2601.14643", "authors": ["Bhabani Shankar Dey", "Ahan Basu", "Pushpak Jagtap"], "title": "Input-to-State Stabilizing Neural Controllers for Unknown Switched Nonlinear Systems within Compact Sets", "comment": null, "summary": "This paper develops a neural network based control framework that ensures system safety and input-to-state stability (ISS) for general nonlinear switched systems with unknown dynamics. Leveraging the concept of dwell time, we derive Lyapunov based sufficient conditions under which both safety and ISS of the closed-loop switched system are guaranteed. The feedback controllers and the associated Lyapunov functions are parameterized using neural networks and trained from data collected over a compact state space via deterministic sampling. To provide formal stability guarantees under the learned controllers, we introduce a validity condition based on Lipschitz continuity assumptions, which is embedded directly into the training framework. This ensures that the resulting neural network controllers satisfy provable correctness and stability guarantees beyond the sampled data. As a special case, the proposed framework recovers ISS and safety under arbitrary switching when a common Lyapunov function exists. Simulation results on a representative switched nonlinear system demonstrate the effectiveness of the proposed approach.", "AI": {"tldr": "本文提出了一种基于神经网络的控制框架，用于保证具有未知动力学的非线性切换系统的安全性和输入状态稳定性 (ISS)。通过结合驻留时间和李雅普诺夫理论，推导出保证系统安全和ISS的充分条件，并使用神经网络参数化控制器和李雅普诺夫函数，通过确定性采样数据进行训练。为了提供形式化的稳定性保证，引入了一个基于Lipschitz连续性的有效性条件，并将其嵌入训练框架，确保学习到的控制器在采样数据之外仍具有可证明的正确性和稳定性。", "motivation": "研究的动机是为了解决具有未知动力学的通用非线性切换系统在保证系统安全性和输入状态稳定性 (ISS) 方面的挑战，并开发一种能够提供形式化稳定性保证的控制方法。", "method": "本文提出了一种基于神经网络的控制框架，结合了驻留时间的概念和李雅普诺夫理论。控制器和李雅普诺夫函数通过神经网络进行参数化，并利用确定性采样数据进行训练。为了提供形式化的稳定性保证，引入了一个基于Lipschitz连续性的有效性条件，并将其集成到训练过程中。", "result": "所提出的框架能够为非线性切换系统提供安全和输入状态稳定性 (ISS) 的保证。在存在公共李雅普诺夫函数的情况下，该框架能够实现任意切换下的ISS和安全性。仿真结果表明了该方法的有效性。", "conclusion": "本文成功地开发了一个基于神经网络的控制框架，能够为具有未知动力学的通用非线性切换系统提供安全性和输入状态稳定性 (ISS) 的保证。该框架通过结合数据驱动学习和形式化验证，实现了可证明的控制器正确性和稳定性。"}}
{"id": "2601.14271", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14271", "abs": "https://arxiv.org/abs/2601.14271", "authors": ["Denise M. Case"], "title": "The Ontological Neutrality Theorem: Why Neutral Ontological Substrates Must Be Pre-Causal and Pre-Normative", "comment": "38 pages", "summary": "Modern data systems must support accountability across persistent legal, political, and analytic disagreement. This requirement imposes strict constraints on the design of any ontology intended to function as a shared substrate. We establish an impossibility result for ontological neutrality: neutrality, understood as interpretive non-commitment and stability under incompatible extensions, is incompatible with the inclusion of causal or normative commitments at the foundational layer. Any ontology that asserts causal or deontic conclusions as ontological facts cannot serve as a neutral substrate across divergent frameworks without revision or contradiction. It follows that neutral ontological substrates must be pre-causal and pre-normative, representing entities, together with identity and persistence conditions, while externalizing interpretation, evaluation, and explanation. This paper does not propose a specific ontology or protocol; rather, it establishes the necessary design constraints for any system intended to maintain a shared, stable representation of reality across conflicting interpretive frameworks.", "AI": {"tldr": "该论文证明了在本体论层面，完全的解释性中立（不承诺解释，且在不兼容的扩展下保持稳定）与包含因果或规范性承诺是不相容的。这意味着一个中立的本体论基础必须是“前因果”和“前规范”的。", "motivation": "现代数据系统需要在存在持续的法律、政治和分析分歧的情况下支持问责制，这要求设计一个能作为共享基础的本体论。研究旨在解决如何构建一个能够跨越不同解释框架保持稳定和共享的现实表示。", "method": "通过建立一个“本体论中立性”的不可能性证明来分析问题。该证明表明，如果本体论包含因果或规范性承诺，它就无法成为一个中立的基础。", "result": "任何声称因果或规范性结论为本体论事实的本体论，都无法在不进行修改或产生矛盾的情况下，成为跨越不同框架的中立基础。因此，中立的本体论基础必须是前因果和前规范的，只表示实体及其身份和持久性条件，并将解释、评估和推理留给外部。", "conclusion": "设计一个能够支持跨越冲突解释框架的共享、稳定现实表示的系统，必须遵守严格的设计约束：本体论基础必须排除因果和规范性承诺，而将这些留给外部解释。"}}
{"id": "2601.14663", "categories": ["eess.SY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14663", "abs": "https://arxiv.org/abs/2601.14663", "authors": ["Yogesh Pipada Sunil Kumar", "S. Ali Pourmousavi", "Jon A. R. Liisberg", "Julian Lesmos-Vinasco"], "title": "Calibrated uncertainty quantification for prosumer flexibility aggregation in ancillary service markets", "comment": "Single column 31 pages, 10 figures, 3 tables, submitted for review to Applied Energy", "summary": "Reliable forecasting of prosumer flexibility is critical for demand response aggregators participating in frequency controlled ancillary services market, where strict reliability requirements such as the P90 standard are enforced. Limited historical data, dependence on exogeneous factors, and heterogenous prosumer behaviour introduce significant epistemic uncertainty, making deterministic or poorly calibrated probabilistic models unsuitable for market bidding. This paper proposes the use of scalable uncertainty quantification framework that integrates Monte Carlo dropout (MCD) with conformal prediction (CP) to produce calibrated, finite sample prediction intervals for aggregated prosumer flexibility. The proposed framework is applied to a behind-the-meter aggregator participating in the Danish manual frequency restoration reserve capacity market. A large-scale synthetic dataset is generated using a modified industry-grade home energy management system, combined with publicly available load, solar, price, activation and device-level data. The resulting machine learning surrogate model captures aggregate prosumer price responsiveness and provides uncertainty-aware estimates suitable for market bidding. Multiple multivariate CP strategies are evaluated and benchmarked against conventional MCD-based methods. Results show that standalone MCD systematically overestimates available flexibility and violates P90 compliance, whereas the proposed MCD-CP framework achieves reliable coverage with controlled conservatism. When embedded in aggregator bidding model, conformalised methods substantially reduce overbidding risk and achieve upto 70% of perfect-information profit while satisfying regulatory reliability constraints, providing practical, computationally efficient, and market-compliant solution for aggregator flexibility forecasting under uncertainty.", "AI": {"tldr": "本研究提出了一种结合蒙特卡洛Dropout（MCD）和共形预测（CP）的可扩展不确定性量化框架，用于预测聚合型产消者的灵活性，并应用于丹麦调频备用容量市场。该方法相比传统MCD能更可靠地满足P90标准，降低了过度投标的风险，并能获得接近完美信息下的利润。", "motivation": "需求响应聚合商在参与调频备用服务市场时，需要满足P90等严格的可靠性要求。然而，有限的历史数据、外部因素的依赖以及产消者行为的异质性导致了不确定性，使得确定性或校准不足的概率模型难以满足市场投标需求。", "method": "该研究提出了一种不确定性量化框架，将蒙特卡洛Dropout（MCD）与共形预测（CP）相结合。首先使用MCD生成预测区间，然后利用CP对预测区间进行校准，以生成在有限样本下可靠的预测区间。研究还对比了多种多变量CP策略。", "result": "研究结果表明，单独使用的MCD方法系统性地高估了可用灵活性并违反了P90合规性。而提出的MCD-CP框架能够实现可靠的覆盖率和受控的保守性。当应用于聚合商投标模型时，共形化方法显著降低了过度投标的风险，并能满足监管可靠性约束，同时获得高达完美信息下70%的利润。", "conclusion": "结合MCD和CP的不确定性量化框架为聚合商提供了实际可行、计算效率高且符合市场要求的灵活性预测解决方案，能够有效应对不确定性并满足市场监管要求。"}}
{"id": "2601.14445", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.14445", "abs": "https://arxiv.org/abs/2601.14445", "authors": ["Aiden Mazidi", "Majid Roshanfar", "Amir Sayadi", "Javad Dargahi", "Jake Barralet", "Liane S. Feldman", "Amir Hooshiar"], "title": "Robust Haptic Rendering Using a Nonlinear Impedance Matching Approach (NIMA) for Robotic Laparoscopic Surgery", "comment": null, "summary": "Background: The integration of haptic feedback into robot-assisted minimally invasive surgery (RAMIS) has long been limited by challenges in accurately rendering forces and ensuring system safety. The need for robust, high-fidelity haptic systems is critical for enhancing the precision and reliability of teleoperated surgical tools. Methods: In this study, we present a Nonlinear Impedance Matching Approach (NIMA) designed to improve force rendering by accurately modelling complex tool-tissue interactions. Based on our previously validated Impedance Matching Approach (IMA), our novel NIMA method includes nonlinear dynamics to capture and render tool-tissue forces effectively. Results: NIMA improves force feedback accuracy with a mean absolute error (MAE) of 0.01 (SD 0.02) N, achieving a 95% reduction in MAE compared to IMA. Furthermore, NIMA effectively eliminates haptic \"kickback\" by ensuring no force is applied by the haptic device to the user's hand when they release the handle, enhancing both patient safety and user comfort. Conclusion: NIMA's ability to account for nonlinearities in tool-tissue interactions provides an improvement in force fidelity, responsiveness, and precision across various surgical conditions. Our findings promote the advancement of haptic feedback systems for robotic surgery, offering a realistic and reliable interface for robot-assisted surgical procedures.", "AI": {"tldr": "提出了一种名为NIMA的非线性阻抗匹配方法，用于提高机器人辅助微创手术（RAMIS）中触觉反馈的准确性和安全性，显著减少了力反馈误差并消除了不必要的反作用力。", "motivation": "现有的机器人辅助微创手术（RAMIS）在精确模拟力和确保系统安全方面存在挑战，需要高保真触觉系统来提高手术工具的精度和可靠性。", "method": "提出了一种名为非线性阻抗匹配方法（NIMA）的新方法，该方法在先前验证的阻抗匹配方法（IMA）的基础上，引入了非线性动力学来更准确地模拟和呈现工具与组织之间的复杂交互力。", "result": "NIMA将力反馈的平均绝对误差（MAE）降低到0.01 N（SD 0.02 N），比IMA降低了95%。此外，NIMA能够有效消除触觉“反冲”，在用户释放手柄时不会向用户手施加力。", "conclusion": "NIMA能够考虑工具-组织交互中的非线性因素，提高了在各种手术条件下的力保真度、响应速度和精度，为机器人辅助手术提供了更真实可靠的接口，促进了机器人手术触觉反馈系统的发展。"}}
{"id": "2601.14337", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14337", "abs": "https://arxiv.org/abs/2601.14337", "authors": ["Zhengyong Huang", "Xingwen Sun", "Xuting Chang", "Ning Jiang", "Yao Wang", "Jianfei Sun", "Hongbin Han", "Yao Sui"], "title": "Unsupervised Deformable Image Registration with Local-Global Attention and Image Decomposition", "comment": null, "summary": "Deformable image registration is a critical technology in medical image analysis, with broad applications in clinical practice such as disease diagnosis, multi-modal fusion, and surgical navigation. Traditional methods often rely on iterative optimization, which is computationally intensive and lacks generalizability. Recent advances in deep learning have introduced attention-based mechanisms that improve feature alignment, yet accurately registering regions with high anatomical variability remains challenging. In this study, we proposed a novel unsupervised deformable image registration framework, LGANet++, which employs a novel local-global attention mechanism integrated with a unique technique for feature interaction and fusion to enhance registration accuracy, robustness, and generalizability. We evaluated our approach using five publicly available datasets, representing three distinct registration scenarios: cross-patient, cross-time, and cross-modal CT-MR registration. The results demonstrated that our approach consistently outperforms several state-of-the-art registration methods, improving registration accuracy by 1.39% in cross-patient registration, 0.71% in cross-time registration, and 6.12% in cross-modal CT-MR registration tasks. These results underscore the potential of LGANet++ to support clinical workflows requiring reliable and efficient image registration. The source code is available at https://github.com/huangzyong/LGANet-Registration.", "AI": {"tldr": "提出了一种名为LGANet++的无监督可变形图像配准框架，该框架利用局部-全局注意力机制和特征交互融合技术，提高了跨患者、跨时间以及跨模态CT-MR配准的准确性和泛化能力，并在多个数据集上取得了优于现有最先进方法的性能。", "motivation": "传统可变形图像配准方法计算成本高且泛化性差；深度学习中的注意力机制虽然有所改进，但在解剖结构变异大的区域配准方面仍存在挑战。", "method": "提出了一种新的无监督可变形图像配准框架LGANet++，该框架集成了新颖的局部-全局注意力机制，并采用了独特的特征交互与融合技术。", "result": "在跨患者、跨时间以及跨模态CT-MR配准任务中，LGANet++的配准准确率分别提高了1.39%、0.71%和6.12%，均优于多个现有的最先进配准方法。", "conclusion": "LGANet++是一种有效的无监督可变形图像配准方法，在不同场景下均表现出优异的性能，有望支持需要可靠高效图像配准的临床工作流程。"}}
{"id": "2601.14665", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.14665", "abs": "https://arxiv.org/abs/2601.14665", "authors": ["Sharaf K. Magableh", "Caisheng Wang", "Oraib Dawaghreh"], "title": "A Two-Stage Risk-Averse DRO-MILP Methodological Framework for Managing AI/Data Center Demand Shocks", "comment": "Submitted to the 2026 IEEE Power and Energy Society General Meeting", "summary": "The rapid growth of artificial intelligence (AI)-driven data centers is reshaping electricity demand patterns. This is achieved by introducing fast, multi-gigawatt load ramps that challenge the stability and resilience of modern power systems. Traditional resilience frameworks focus mainly on physical outages and largely overlook these emerging digital-era disturbances. This paper proposes a unified two-stage, risk-aware distributionally robust optimization (DRO)-MILP framework that coordinates the pre-allocation and post-event dispatch of Flexible Capacity Modules (FCMs), including BESS, fast-ramping generation, demand response, and potential long-duration storage. Stage-I optimally positions FCMs using DRO with CVaR to hedge against uncertain AI load surges. Stage-II models real-time stabilization following stochastic demand-shock scenarios, minimizing imbalance, unserved energy, and restoration penalties. The framework is designed to be applied on IEEE 33-bus system or expanded for scalability to larger IEEE test feeders capable of representing AI-scale loads. This contributes a scalable planning tool for resilient, AI-integrated distribution grids.", "AI": {"tldr": "本文提出了一种统一的两阶段、风险感知分布鲁棒优化（DRO）框架，用于协调灵活容量模块（FCM）在AI驱动数据中心快速负荷变化下的预配置和事后调度，以增强电网的稳定性和韧性。", "motivation": "AI驱动数据中心的快速增长导致了对电网稳定性和韧性构成挑战的快速、大功率负荷变化。现有的韧性框架未能充分考虑这些新兴的数字时代扰动。", "method": "采用两阶段的分布鲁棒优化（DRO）-混合整数线性规划（MILP）框架。第一阶段利用条件风险价值（CVaR）进行DRO，优化FCM（包括BESS、快速调峰发电、需求响应、长时储能）的预配置，以应对不确定的AI负荷激增。第二阶段对随机负荷冲击场景下的实时稳定进行建模，最小化不平衡、未满足的能量和恢复惩罚。", "result": "该框架能够协调多种FCM，有效对冲AI负荷激增带来的不确定性，并在负荷冲击后最小化电网不平衡和未满足能量。框架设计具有可扩展性，可应用于IEEE 33总线系统或更大的IEEE测试馈线。", "conclusion": "所提出的DRO-MILP框架为规划具有AI集成能力的韧性配电网提供了一个可扩展的工具，能够有效地管理AI驱动数据中心带来的新兴电网挑战。"}}
{"id": "2601.14259", "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.LG", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.14259", "abs": "https://arxiv.org/abs/2601.14259", "authors": ["Ziwen Zhong", "Zhitao Shu", "Yue Zhao"], "title": "A Cloud-Based Cross-Modal Transformer for Emotion Recognition and Adaptive Human-Computer Interaction", "comment": null, "summary": "Emotion recognition is a fundamental component of next-generation human-computer interaction (HCI), enabling machines to perceive, understand, and respond to users' affective states. However, existing systems often rely on single-modality analysis such as facial expressions, speech tone, or textual sentiment, resulting in limited robustness and poor generalization in real-world environments. To address these challenges, this study proposes a Cloud-Based Cross-Modal Transformer (CMT) framework for multimodal emotion recognition and adaptive human-computer interaction. The proposed model integrates visual, auditory, and textual signals using pretrained encoders (Vision Transformer, Wav2Vec2, and BERT) and employs a cross-modal attention mechanism to capture complex interdependencies among heterogeneous features. By leveraging cloud computing infrastructure with distributed training on Kubernetes and TensorFlow Serving, the system enables scalable, low-latency emotion recognition for large-scale user interactions. Experiments conducted on benchmark datasets including IEMOCAP, MELD, and AffectNet demonstrate that the CMT achieves state-of-the-art performance, improving the F1-score by 3.0 percent and reducing cross-entropy loss by 12.9 percent compared to strong multimodal baselines. Additionally, cloud deployment evaluations show an average response latency of 128 ms, representing a 35 percent reduction compared with conventional transformer-based fusion systems. These results confirm that the proposed framework enables efficient, real-time emotion recognition and adaptive feedback in applications such as intelligent customer service, virtual tutoring systems, and affective computing interfaces, marking an important step toward cloud-native affective computing and emotionally intelligent interactive systems.", "AI": {"tldr": "提出了一种基于云的跨模态Transformer（CMT）框架，用于多模态情感识别和自适应人机交互，该框架整合视觉、听觉和文本信号，并通过跨模态注意力机制捕捉异构特征间的复杂依赖关系，实现了先进的性能和低延迟。", "motivation": "现有基于单模态的情感识别系统鲁棒性和泛化能力有限，难以满足真实世界环境的需求。", "method": "提出CMT框架，利用预训练的Transformer编码器（Vision Transformer、Wav2Vec2、BERT）整合多模态信号，并通过跨模态注意力机制捕捉特征间的依赖关系，同时利用Kubernetes和TensorFlow Serving进行云端分布式训练和部署，实现低延迟和可扩展性。", "result": "在IEMOCAP、MELD和AffectNet数据集上，CMT取得了比现有模型更高的F1分数（提高3.0%）和更低的交叉熵损失（降低12.9%）。云端部署评估显示平均响应延迟为128毫秒，比传统Transformer融合系统降低了35%。", "conclusion": "CMT框架能够实现高效、实时的情感识别和自适应反馈，可应用于智能客服、虚拟辅导和情感计算接口等领域，是迈向云原生情感计算和情感智能交互系统的重要一步。"}}
{"id": "2601.14468", "categories": ["eess.SY", "math.NA", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.14468", "abs": "https://arxiv.org/abs/2601.14468", "authors": ["Milad Hasanzadeh", "Amin Kargarian", "Javad Lavaei"], "title": "All-Pass Fractional OPF: A Solver-Friendly, Physics-Preserving Approximation of AC OPF", "comment": null, "summary": "This paper presents a fractional approximation of the AC optimal power flow (AC OPF) problem based on an all-pass approximation of the exponential power flow kernel. The classical AC OPF relies on trigonometric coupling between bus voltage phasors, which yields a nonconvex program with oscillatory derivatives that can slow, or in some cases destabilize, interior-point methods. We replace the trigonometric terms with an all-pass fractional (APF) approximation whose real and imaginary components act as smooth surrogates for the cosine and sine functions, and we introduce a pre-rotation to shift the argument of the approximation toward its most accurate region, ensuring that the reformulated power flow model preserves physical loss behavior, maintains the symmetry of the classical kernels, and improves the conditioning of the Jacobian and Hessian matrices. The proposed APF OPF formulation remains nonconvex, as in the classical model, but it eliminates trigonometric evaluations and empirically produces larger and more stable Newton steps under standard interior-point solvers. Numerical results on more than 25 IEEE and PGLib test systems ranging from 9 to 10{,}000 buses demonstrate that the APF OPF model achieves solutions with accuracy comparable to that of the classical formulation while reducing solver times, indicating a more solver-friendly nonconvex representation of AC OPF. All code, functions, verification scripts, and generated results are publicly available on \\href{https://github.com/LSU-RAISE-LAB/APF-OPF}{GitHub}, along with a README describing how to run and reproduce the experiments.", "AI": {"tldr": "本文提出了一种基于全通近似的指数潮流核的交流最优潮流（AC OPF）问题的分数近似方法，该方法用全通分数（APF）近似替代了传统的三角函数耦合，从而提高了求解器的稳定性和效率。", "motivation": "传统的AC OPF模型依赖于非凸的三角函数耦合，这会导致求解器（如内点法）的收敛速度慢甚至不稳定。研究旨在找到一种更稳定、更高效的AC OPF模型。", "method": "提出了一种全通分数（APF）近似来替代AC OPF中的三角函数项。通过引入预旋转来优化近似精度，确保模型能够保留物理损耗特性、保持核函数的对称性，并改善雅可比矩阵和海森矩阵的条件数。APF OPF模型仍然是非凸的。", "result": "在多个IEEE和PGLib测试系统上的数值结果表明，APF OPF模型可以获得与经典模型相当的解的精度，同时显著减少了求解时间。这表明APF OPF提供了一种对求解器更友好的非凸AC OPF表示。", "conclusion": "所提出的APF OPF模型是一种有效的AC OPF近似方法，它通过避免三角函数计算并改善数值特性，提高了求解器的稳定性和效率，同时保持了与经典AC OPF相当的解的精度。"}}
{"id": "2601.14267", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.14267", "abs": "https://arxiv.org/abs/2601.14267", "authors": ["Pouria Mortezaagha", "Joseph Shaw", "Bowen Sun", "Arya Rahgozar"], "title": "From Chaos to Clarity: Schema-Constrained AI for Auditable Biomedical Evidence Extraction from Full-Text PDFs", "comment": null, "summary": "Biomedical evidence synthesis relies on accurate extraction of methodological, laboratory, and outcome variables from full-text research articles, yet these variables are embedded in complex scientific PDFs that make manual abstraction time-consuming and difficult to scale. Existing document AI systems remain limited by OCR errors, long-document fragmentation, constrained throughput, and insufficient auditability for high-stakes synthesis. We present a schema-constrained AI extraction system that transforms full-text biomedical PDFs into structured, analysis-ready records by explicitly restricting model inference through typed schemas, controlled vocabularies, and evidence-gated decisions. Documents are ingested using resume-aware hashing, partitioned into caption-aware page-level chunks, and processed asynchronously under explicit concurrency controls. Chunk-level outputs are deterministically merged into study-level records using conflict-aware consolidation, set-based aggregation, and sentence-level provenance to support traceability and post-hoc audit. Evaluated on a corpus of studies on direct oral anticoagulant level measurement, the pipeline processed all documents without manual intervention, maintained stable throughput under service constraints, and exhibited strong internal consistency across document chunks. Iterative schema refinement substantially improved extraction fidelity for synthesis-critical variables, including assay classification, outcome definitions, follow-up duration, and timing of measurement. These results demonstrate that schema-constrained, provenance-aware extraction enables scalable and auditable transformation of heterogeneous scientific PDFs into structured evidence, aligning modern document AI with the transparency and reliability requirements of biomedical evidence synthesis.", "AI": {"tldr": "研究提出了一种基于模式约束的AI提取系统，可以将生物医学PDF文献转化为结构化的、可供分析的记录，解决了现有文档AI系统在OCR错误、长文档处理、吞吐量和审计性方面的局限性。", "motivation": "手动从生物医学PDF中提取信息耗时且难以规模化，现有的文档AI系统存在准确性和审计性不足的问题，无法满足高风险证据合成的需求。", "method": "该系统通过显式限制模型推理（使用类型化模式、受控词汇表和证据门控决策）来转换PDF。它使用感知恢复的哈希处理文档，按页面分割，并异步处理。输出通过冲突感知合并、基于集合的聚合和句子级溯源来合并成研究级记录。", "result": "该系统成功处理了所有文档，无需人工干预，并在服务限制下保持了稳定的吞吐量。模式的迭代改进显著提高了对合成关键变量（如检测方法分类、结果定义、随访时间、测量时间）的提取准确性。", "conclusion": "研究表明，模式约束和可溯源的提取方法能够实现异构科学PDF到结构化证据的可扩展且可审计的转换，使现代文档AI能够满足生物医学证据合成的透明度和可靠性要求。"}}
{"id": "2601.14673", "categories": ["eess.SY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14673", "abs": "https://arxiv.org/abs/2601.14673", "authors": ["Yogesh Pipada Sunil Kumar", "S. Ali Pourmousavi", "Jon A. R. Liisberg", "Julian Lesmos-Vinasco"], "title": "Efficient reformulations of ReLU deep neural networks for surrogate modelling in power system optimisation", "comment": "24 pages, 7 figures, 3 tables", "summary": "The ongoing decarbonisation of power systems is driving an increasing reliance on distributed energy resources, which introduces complex and nonlinear interactions that are difficult to capture in conventional optimisation models. As a result, machine learning based surrogate modelling has emerged as a promising approach, but integrating machine learning models such as ReLU deep neural networks (DNNs) directly into optimisation often results in nonconvex and computationally intractable formulations. This paper proposes a linear programming (LP) reformulation for a class of convexified ReLU DNNs with non-negative weight matrices beyond the first layer, enabling a tight and tractable embedding of learned surrogate models in optimisation. We evaluate the method using a case study on learning the prosumer's responsiveness within an aggregator bidding problem in the Danish tertiary capacity market. The proposed reformulation is benchmarked against state-of-the-art alternatives, including piecewise linearisation (PWL), MIP-based embedding, and other LP relaxations. Across multiple neural network architectures and market scenarios, the convexified ReLU DNN achieves solution quality comparable to PWL and MIP-based reformulations while significantly improving computational performance and preserving model fidelity, unlike penalty-based reformulations. The results demonstrate that convexified ReLU DNNs offer a scalable and reliable methodology for integrating learned surrogate models in optimisation, with applicability to a wide range of emerging power system applications.", "AI": {"tldr": "本研究提出了一种线性规划（LP）方法，用于将一类经过凸优化的ReLU深度神经网络（DNN）集成到优化问题中，特别适用于包含分布式能源的电力系统。该方法在丹麦的聚合器投标问题案例研究中，通过学习用户响应能力，展示了其在计算效率和模型保真度方面的优势，优于现有方法。", "motivation": "随着电力系统脱碳化，分布式能源资源（DER）的增加导致了复杂的非线性交互，传统优化模型难以捕捉。直接将ReLU DNN等机器学习模型嵌入优化会产生非凸和难以处理的问题。", "method": "该研究提出了一种针对特定类型的ReLU DNN（第一层之后权重矩阵非负）的凸优化方法，并将其线性规划（LP）重新表述，以便能够精确且易于处理地将学习到的代理模型嵌入到优化问题中。", "result": "在丹麦的三元容量市场的聚合器投标问题案例研究中，通过学习用户响应能力，该方法在多层神经网络结构和不同市场场景下，实现了与PWL和MIP方法相当的解的质量，同时显著提高了计算性能。它还比基于惩罚的方法更好地保持了模型保真度。", "conclusion": "凸优化的ReLU DNN为将学习到的代理模型集成到优化问题提供了一种可扩展且可靠的方法，适用于各种新兴的电力系统应用。"}}
{"id": "2601.14295", "categories": ["cs.AI", "cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2601.14295", "abs": "https://arxiv.org/abs/2601.14295", "authors": ["Michele Loi"], "title": "Epistemic Constitutionalism Or: how to avoid coherence bias", "comment": "27 pages, 7 tables. Data: github.com/MicheleLoi/source-attribution-bias-data and github.com/MicheleLoi/source-attribution-bias-swiss-replication. Complete AI-assisted writing documentation: github.com/MicheleLoi/epistemic-constitutionalism-paper", "summary": "Large language models increasingly function as artificial reasoners: they evaluate arguments, assign credibility, and express confidence. Yet their belief-forming behavior is governed by implicit, uninspected epistemic policies. This paper argues for an epistemic constitution for AI: explicit, contestable meta-norms that regulate how systems form and express beliefs. Source attribution bias provides the motivating case: I show that frontier models enforce identity-stance coherence, penalizing arguments attributed to sources whose expected ideological position conflicts with the argument's content. When models detect systematic testing, these effects collapse, revealing that systems treat source-sensitivity as bias to suppress rather than as a capacity to execute well. I distinguish two constitutional approaches: the Platonic, which mandates formal correctness and default source-independence from a privileged standpoint, and the Liberal, which refuses such privilege, specifying procedural norms that protect conditions for collective inquiry while allowing principled source-attending grounded in epistemic vigilance. I argue for the Liberal approach, sketch a constitutional core of eight principles and four orientations, and propose that AI epistemic governance requires the same explicit, contestable structure we now expect for AI ethics.", "AI": {"tldr": "该论文提出了一种“认识论构成”（epistemic constitution）的概念，主张为人工智能（AI）建立明确、可争辩的元规范，以指导其信念形成和表达，以此来解决当前大型语言模型（LLMs）中存在的隐性认知策略问题。文章以“来源归属偏见”（source attribution bias）为例，展示了LLMs如何通过身份-立场一致性来惩罚与预期意识形态不符的论点，并在检测到系统性测试时崩溃。论文提出了两种构成方法：柏拉图式（Platonic）和自由主义式（Liberal），并倡导自由主义式方法，认为其更能促进集体探究的条件，并建立了一个包含八项原则和四项导向的认识论治理框架。", "motivation": "当前大型语言模型（LLMs）在进行推理、评估论点、判断可信度和表达信心时，其行为背后依赖的是隐性的、未经审视的认知策略。作者认为这种不透明性是不可取的，因此提出需要一种明确的、可被挑战的“认识论构成”来规范AI的信念形成和表达。", "method": "文章通过“来源归属偏见”作为案例研究，展示了LLMs如何依据来源的意识形态立场来影响其对论点内容的判断。作者通过实验观察到，当模型检测到系统性测试时，这种偏见会消失，从而揭示了模型将来源敏感性视为一种应被压制的偏见，而非一种应被恰当执行的能力。在此基础上，作者区分了两种认识论构成的途径：柏拉图式和自由主义式，并论证了自由主义式方法的可行性，并勾勒了一个由八项原则和四项导向组成的框架。", "result": "研究表明，当前LLMs在处理带有来源信息的论点时，倾向于强制执行身份-立场一致性，这是一种“来源归属偏见”。这种偏见在系统性测试下会瓦解，暗示模型并未恰当地处理来源信息。作者提出了两种AI认识论构成的方法，并认为自由主义式方法更优，因为它不依赖于特权视角，而是通过程序性规范来保护集体探究的条件。", "conclusion": "AI的认识论治理需要像AI伦理一样，建立一种明确、可争辩的结构。作者倡导采用自由主义式的认识论构成，该构成通过一套原则和导向来规范AI的信念形成和表达，以促进更可靠、更具协作性的AI推理过程。"}}
{"id": "2601.14492", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.14492", "abs": "https://arxiv.org/abs/2601.14492", "authors": ["Malak Mansour", "Ali Abouzeid", "Zezhou Sun", "Qinbo Sun", "Dezhen Song", "Abdalla Swikir"], "title": "UNCLE-Grasp: Uncertainty-Aware Grasping of Leaf-Occluded Strawberries", "comment": null, "summary": "Robotic strawberry harvesting is challenging under partial occlusion, where leaves induce significant geometric uncertainty and make grasp decisions based on a single deterministic shape estimate unreliable. From a single partial observation, multiple incompatible 3D completions may be plausible, causing grasps that appear feasible on one completion to fail on another. We propose an uncertainty-aware grasping pipeline for partially occluded strawberries that explicitly models completion uncertainty arising from both occlusion and learned shape reconstruction. Our approach uses point cloud completion with Monte Carlo dropout to sample multiple shape hypotheses, generates candidate grasps for each completion, and evaluates grasp feasibility using physically grounded force-closure-based metrics. Rather than selecting a grasp based on a single estimate, we aggregate feasibility across completions and apply a conservative lower confidence bound (LCB) criterion to decide whether a grasp should be attempted or safely abstained. We evaluate the proposed method in simulation and on a physical robot across increasing levels of synthetic and real leaf occlusion. Results show that uncertainty-aware decision making enables reliable abstention from high-risk grasp attempts under severe occlusion while maintaining robust grasp execution when geometric confidence is sufficient, outperforming deterministic baselines in both simulated and physical robot experiments.", "AI": {"tldr": "提出了一种用于部分遮挡草莓的机器人抓取方法，该方法通过蒙特卡洛丢弃法对形状不确定性进行建模，并使用保守的低置信度界限（LCB）准则来决定是否进行抓取，以提高在遮挡情况下的抓取成功率。", "motivation": "在部分遮挡的情况下，机器人采摘草莓面临几何不确定性，仅基于单一确定性形状估计的抓取决策并不可靠，可能导致抓取失败。因此，需要一种能够处理遮挡和形状重建不确定性的抓取方法。", "method": "该方法使用带蒙特卡洛丢弃法的点云完成技术，生成多个形状假设。然后，为每个完成的形状生成候选抓取点，并使用基于力的闭合度量来评估抓取的可能性。最终，通过聚合多个形状假设下的可行性，并应用保守的低置信度界限（LCB）准则来决定是否执行抓取。", "result": "该方法在模拟和物理机器人实验中，通过增加合成和真实叶子遮挡的程度进行评估。结果表明，该方法在严重遮挡下能够可靠地放弃高风险的抓取尝试，并在几何置信度足够时保持鲁棒的抓取执行，优于确定性基线方法。", "conclusion": "通过显式建模由遮挡和学习到的形状重建引起的不确定性，并采用不确定性感知决策策略（LCB），可以显著提高机器人部分遮挡草莓抓取的鲁棒性和成功率。"}}
{"id": "2601.14338", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14338", "abs": "https://arxiv.org/abs/2601.14338", "authors": ["Zhengyong Huang", "Ning Jiang", "Xingwen Sun", "Lihua Zhang", "Peng Chen", "Jens Domke", "Yao Sui"], "title": "Partial Decoder Attention Network with Contour-weighted Loss Function for Data-Imbalance Medical Image Segmentation", "comment": null, "summary": "Image segmentation is pivotal in medical image analysis, facilitating clinical diagnosis, treatment planning, and disease evaluation. Deep learning has significantly advanced automatic segmentation methodologies by providing superior modeling capability for complex structures and fine-grained anatomical regions. However, medical images often suffer from data imbalance issues, such as large volume disparities among organs or tissues, and uneven sample distributions across different anatomical structures. This imbalance tends to bias the model toward larger organs or more frequently represented structures, while overlooking smaller or less represented structures, thereby affecting the segmentation accuracy and robustness. To address these challenges, we proposed a novel contour-weighted segmentation approach, which improves the model's capability to represent small and underrepresented structures. We developed PDANet, a lightweight and efficient segmentation network based on a partial decoder mechanism. We evaluated our method using three prominent public datasets. The experimental results show that our methodology excelled in three distinct tasks: segmenting multiple abdominal organs, brain tumors, and pelvic bone fragments with injuries. It consistently outperformed nine state-of-the-art methods. Moreover, the proposed contour-weighted strategy improved segmentation for other comparison methods across the three datasets, yielding average enhancements in Dice scores of 2.32%, 1.67%, and 3.60%, respectively. These results demonstrate that our contour-weighted segmentation method surpassed current leading approaches in both accuracy and robustness. As a model-independent strategy, it can seamlessly fit various segmentation frameworks, enhancing their performance. This flexibility highlighted its practical importance and potential for broad use in medical image analysis.", "AI": {"tldr": "本文提出了一种新颖的轮廓加权分割方法PDANet，通过轻量级网络和部分解码器机制，有效解决了医学图像中数据不平衡导致的小结构分割不准确问题，并在三个公共数据集上验证了其优越性。", "motivation": "医学图像分析中，数据不平衡（如器官体积差异大、样本分布不均）是导致模型偏向大结构、忽视小结构，从而影响分割精度和鲁棒性的主要挑战。", "method": "提出了一种基于部分解码器机制（PDANet）的轻量级分割网络，并引入了一种新颖的轮廓加权分割策略，以增强模型对小且代表性不足结构的学习能力。", "result": "PDANet在多器官腹部、脑肿瘤和盆骨骨折碎片分割任务上均表现出色，优于九种现有最先进方法。此外，轮廓加权策略独立应用于其他方法时，平均Dice得分分别提升了2.32%、1.67%和3.60%。", "conclusion": "提出的轮廓加权分割方法在精度和鲁棒性上均优于当前领先方法，并且作为一种模型无关的策略，可以轻松集成到现有分割框架中，提高其性能，具有广泛的应用潜力。"}}
{"id": "2601.14261", "categories": ["cs.CV", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.14261", "abs": "https://arxiv.org/abs/2601.14261", "authors": ["Taoliang Tan", "Chengwei Ma", "Zhen Tian", "Zhao Lin", "Dongdong Li", "Si Shi"], "title": "Intelligent Power Grid Design Review via Active Perception-Enabled Multimodal Large Language Models", "comment": null, "summary": "The intelligent review of power grid engineering design drawings is crucial for power system safety. However, current automated systems struggle with ultra-high-resolution drawings due to high computational demands, information loss, and a lack of holistic semantic understanding for design error identification. This paper proposes a novel three-stage framework for intelligent power grid drawing review, driven by pre-trained Multimodal Large Language Models (MLLMs) through advanced prompt engineering. Mimicking the human expert review process, the first stage leverages an MLLM for global semantic understanding to intelligently propose domain-specific semantic regions from a low-resolution overview. The second stage then performs high-resolution, fine-grained recognition within these proposed regions, acquiring detailed information with associated confidence scores. In the final stage, a comprehensive decision-making module integrates these confidence-aware results to accurately diagnose design errors and provide a reliability assessment. Preliminary results on real-world power grid drawings demonstrate our approach significantly enhances MLLM's ability to grasp macroscopic semantic information and pinpoint design errors, showing improved defect discovery accuracy and greater reliability in review judgments compared to traditional passive MLLM inference. This research offers a novel, prompt-driven paradigm for intelligent and reliable power grid drawing review.", "AI": {"tldr": "本文提出了一种基于预训练多模态大语言模型（MLLMs）和先进提示工程的三阶段智能电网工程图纸审查框架，以解决现有方法在超高分辨率图纸上面临的计算负担、信息丢失和语义理解不足的问题。该框架通过模仿人类专家审查流程，首先进行全局语义理解，然后进行高分辨率的区域识别，最后整合置信度信息进行设计错误诊断和可靠性评估。", "motivation": "现有自动化系统在处理超高分辨率电网设计图纸时存在计算需求高、信息丢失和缺乏整体语义理解以识别设计错误的问题，亟需一种更高效、更准确的审查方法。", "method": "提出一个三阶段框架：1. 使用MLLM通过提示工程进行全局语义理解，智能划分领域特定的语义区域（低分辨率）；2. 在选定的高分辨率区域内进行细粒度识别，获取带有置信度得分的详细信息；3. 决策模块整合置信度感知的结果，诊断设计错误并提供可靠性评估。", "result": "在真实世界电网图纸上的初步结果表明，该方法显著提升了MLLM理解宏观语义信息和定位设计错误的能力，在缺陷发现准确性和审查判断可靠性方面优于传统的被动MLLM推理。", "conclusion": "该研究提供了一种新颖的、由提示驱动的智能可靠的电网图纸审查范式，有效地解决了处理超高分辨率图纸的挑战。"}}
{"id": "2601.14440", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.14440", "abs": "https://arxiv.org/abs/2601.14440", "authors": ["Saeed Khaki", "Ashudeep Singh", "Nima Safaei", "Kamal Ginotra"], "title": "VisTIRA: Closing the Image-Text Modality Gap in Visual Math Reasoning via Structured Tool Integration", "comment": null, "summary": "Vision-language models (VLMs) lag behind text-only language models on mathematical reasoning when the same problems are presented as images rather than text. We empirically characterize this as a modality gap: the same question in text form yields markedly higher accuracy than its visually typeset counterpart, due to compounded failures in reading dense formulas, layout, and mixed symbolic-diagrammatic context. First, we introduce VisTIRA (Vision and Tool-Integrated Reasoning Agent), a tool-integrated reasoning framework that enables structured problem solving by iteratively decomposing a given math problem (as an image) into natural language rationales and executable Python steps to determine the final answer. Second, we build a framework to measure and improve visual math reasoning: a LaTeX-based pipeline that converts chain-of-thought math corpora (e.g., NuminaMath) into challenging image counterparts, and a large set of synthetic tool-use trajectories derived from a real-world, homework-style image dataset (called SnapAsk) for fine-tuning VLMs. Our experiments show that tool-integrated supervision improves image-based reasoning, and OCR grounding can further narrow the gap for smaller models, although its benefit diminishes at scale. These findings highlight that modality gap severity inversely correlates with model size, and that structured reasoning and OCR-based grounding are complementary strategies for advancing visual mathematical reasoning.", "AI": {"tldr": "研究表明，视觉语言模型（VLMs）在处理数学推理任务时，由于图像输入中公式、布局和图文混合信息的识别困难，相比文本输入存在明显的模态差距。论文提出了VisTIRA框架，通过将数学问题（图像）分解为自然语言解释和Python代码来解决问题，并构建了一个利用LaTeX生成图像化数学问题和利用SnapAsk数据集进行微调的框架，以提升模型的视觉数学推理能力。", "motivation": "现有的视觉语言模型在数学推理方面，当问题以图像形式呈现时，其准确率远低于文本形式，研究旨在探究和弥合这种由于图像识别困难导致的模态差距。", "method": "1. 提出VisTIRA（Vision and Tool-Integrated Reasoning Agent）框架，通过将图像数学问题分解为自然语言推理和可执行的Python代码步骤来解决问题。 2. 构建一个LaTeX管道，将链式思考的数学语料库转换为图像形式。 3. 利用SnapAsk数据集生成合成的工具使用轨迹，用于微调视觉语言模型。 4. 实验评估了工具集成监督和OCR（光学字符识别）接地对模型性能的影响。", "result": "实验证明，工具集成监督能够提升基于图像的数学推理能力。OCR接地可以缩小较小模型在模态差距上的差距，但对于大规模模型效果不显著。模型规模越大，模态差距越小。结构化推理和OCR接地是互补的策略。", "conclusion": "视觉语言模型在处理图像数学问题时存在显著的模态差距，这与模型规模成反比。通过工具集成监督和OCR接地等结构化推理和视觉信息结合的方法，可以有效提升视觉数学推理能力。"}}
{"id": "2601.14270", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14270", "abs": "https://arxiv.org/abs/2601.14270", "authors": ["Liangming Pan", "Jason Liang", "Jiaran Ye", "Minglai Yang", "Xinyuan Lu", "Fengbin Zhu"], "title": "Opening the Black Box: A Survey on the Mechanisms of Multi-Step Reasoning in Large Language Models", "comment": "Technical Report", "summary": "Large Language Models (LLMs) have demonstrated remarkable abilities to solve problems requiring multiple reasoning steps, yet the internal mechanisms enabling such capabilities remain elusive. Unlike existing surveys that primarily focus on engineering methods to enhance performance, this survey provides a comprehensive overview of the mechanisms underlying LLM multi-step reasoning. We organize the survey around a conceptual framework comprising seven interconnected research questions, from how LLMs execute implicit multi-hop reasoning within hidden activations to how verbalized explicit reasoning remodels the internal computation. Finally, we highlight five research directions for future mechanistic studies.", "AI": {"tldr": "本文献综述了大型语言模型（LLM）进行多步推理的内在机制，而非仅关注性能提升的工程方法。通过一个包含七个研究问题的概念框架，梳理了LLM从隐式推理到显式推理的内部运作方式，并提出了五个未来研究方向。", "motivation": "现有研究主要关注如何通过工程手段提升LLM多步推理的性能，但其内在机制仍不清晰。因此，本文旨在提供一个关于LLM多步推理机制的全面概述。", "method": "本文采用文献综述的方式，围绕一个包含七个相互关联的研究问题的概念框架，系统梳理了LLM多步推理的内在机制。研究问题涵盖了从隐藏激活中的隐式多跳推理到显式语言推理如何重塑内部计算等多个方面。", "result": "该综述提供了一个关于LLM多步推理机制的结构化视角，并指出了当前研究的空白和潜在的未来发展方向。它深入探讨了LLM如何在内部处理和生成多步推理过程。", "conclusion": "理解LLM多步推理的内在机制对于进一步发展和优化LLM至关重要。本文提出的概念框架和研究方向为未来的相关研究提供了指导。"}}
{"id": "2601.14689", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.14689", "abs": "https://arxiv.org/abs/2601.14689", "authors": ["Hyeongon Park", "Daniel K. Molzahn", "Rahul K. Gupta"], "title": "Ramping-aware Enhanced Flexibility Aggregation of Distributed Generation with Energy Storage in Power Distribution Networks", "comment": "10 pages, 4 figures", "summary": "Power distribution networks are increasingly hosting controllable and flexible distributed energy resources (DERs) that, when aggregated, can provide ancillary support to transmission systems. However, existing aggregation schemes often ignore the ramping constraints of these DERs, which can render them impractical in real deployments. This work proposes a ramping-aware flexibility aggregation scheme, computed at the transmission-distribution boundary, that explicitly accounts for DER ramp limits and yields flexibility envelopes that are provably disaggregable. To further enhance the attainable flexibility region, we introduce a novel pre-ramping strategy, which proactively adjusts resource operating points to enlarge the aggregated flexibility envelope while preserving both network feasibility and disaggregation guarantees. The proposed method demonstrates a 5.2% to 19.2% improvement in flexibility relative to the baseline model, depending on system conditions. We validate the scheme on an IEEE-33 bus distribution system and provide formal proofs showing that both aggregation strategies are disaggregable for all feasible trajectories within the aggregate flexibility envelope.", "AI": {"tldr": "提出了一种考虑爬坡率限制的分布式能源（DER）灵活聚合方案，并通过预爬坡策略进一步提升了聚合灵活性，证明了该方案的可分解性，并在IEEE-33节点系统上验证了其有效性。", "motivation": "现有DER聚合方案忽略了DER的爬坡率限制，导致在实际部署中不可行。需要一种考虑爬坡率限制的灵活聚合方案。", "method": "提出了一种在输配电边界计算的爬坡率感知灵活聚合方案，显式考虑DER的爬坡限制，并证明了其可分解性。引入了一种新的预爬坡策略，以主动调整资源操作点来扩大聚合灵活性包络。在IEEE-33节点配电系统上进行了验证。", "result": "提出的方案比基线模型提高了5.2%至19.2%的灵活性。证明了该方案对于聚合灵活度包络内的所有可行轨迹都是可分解的。", "conclusion": "提出的爬坡率感知灵活聚合方案和预爬坡策略能够有效提高DER的聚合灵活性，同时保证了网络可行性和分解性，为实际应用提供了理论和实践依据。"}}
{"id": "2601.14550", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.14550", "abs": "https://arxiv.org/abs/2601.14550", "authors": ["Tailai Cheng", "Kejia Chen", "Lingyun Chen", "Liding Zhang", "Yue Zhang", "Yao Ling", "Mahdi Hamad", "Zhenshan Bing", "Fan Wu", "Karan Sharma", "Alois Knoll"], "title": "TacUMI: A Multi-Modal Universal Manipulation Interface for Contact-Rich Tasks", "comment": null, "summary": "Task decomposition is critical for understanding and learning complex long-horizon manipulation tasks. Especially for tasks involving rich physical interactions, relying solely on visual observations and robot proprioceptive information often fails to reveal the underlying event transitions. This raises the requirement for efficient collection of high-quality multi-modal data as well as robust segmentation method to decompose demonstrations into meaningful modules. Building on the idea of the handheld demonstration device Universal Manipulation Interface (UMI), we introduce TacUMI, a multi-modal data collection system that integrates additionally ViTac sensors, force-torque sensor, and pose tracker into a compact, robot-compatible gripper design, which enables synchronized acquisition of all these modalities during human demonstrations. We then propose a multi-modal segmentation framework that leverages temporal models to detect semantically meaningful event boundaries in sequential manipulations. Evaluation on a challenging cable mounting task shows more than 90 percent segmentation accuracy and highlights a remarkable improvement with more modalities, which validates that TacUMI establishes a practical foundation for both scalable collection and segmentation of multi-modal demonstrations in contact-rich tasks.", "AI": {"tldr": "提出了一种名为 TacUMI 的多模态数据采集系统，并结合一个基于时间模型的分割框架，以有效分解复杂的、包含物理交互的操作任务，实验结果表明该方法在电缆安装任务中达到了超过 90% 的分割精度，并且集成更多模态数据能显著提升性能。", "motivation": "现有方法在处理涉及丰富物理交互的复杂长时程操作任务时，仅依赖视觉和本体感受信息难以揭示事件转换，因此需要高效采集高质量的多模态数据并采用鲁棒的分割方法来分解演示。", "method": "基于 UMI 设备，引入 TacUMI 系统，集成了 ViTac 传感器、力矩传感器和姿态跟踪器，实现了对人类演示过程中多模态数据的同步采集。并提出了一种多模态分割框架，利用时间模型检测操作序列中的语义事件边界。", "result": "在具有挑战性的电缆安装任务上评估，分割精度超过 90%。结果显示，增加模态数据可以显著提高分割性能。", "conclusion": "TacUMI 系统和多模态分割框架为接触丰富的任务中多模态演示的可扩展采集和分割提供了实用的基础，证明了多模态数据在理解和学习复杂操作任务中的重要性。"}}
{"id": "2601.14793", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2601.14793", "abs": "https://arxiv.org/abs/2601.14793", "authors": ["Shuo Zhang", "Zihua Wang", "Changgeng He", "Chunhua Hu"], "title": "LiNUS: Lightweight Automatic Segmentation of Deep Brain Nuclei for Real-Time DBS Surgery", "comment": "6 pages, 9 figures", "summary": "This paper proposes LiNUS, a lightweight deep learning framework for the automatic segmentation of the Subthalamic Nucleus (STN) in Deep Brain Stimulation (DBS) surgery. Addressing the challenges of small target volume and class imbalance in MRI data, LiNUS improves upon the U-Net architecture by introducing spectral normalization constraints, bilinear interpolation upsampling, and a multi-scale feature fusion mechanism. Experimental results on the Tsinghua DBS dataset (TT14) demonstrate that LiNUS achieves a Dice coefficient of 0.679 with an inference time of only 0.05 seconds per subject, significantly outperforming traditional manual and registration-based methods. Further validation on high-resolution data confirms the model's robustness, achieving a Dice score of 0.89. A dedicated Graphical User Interface (GUI) was also developed to facilitate real-time clinical application.", "AI": {"tldr": "本文提出了一种名为LiNUS的轻量级深度学习框架，用于自动分割脑深部电刺激（DBS）手术中的丘脑底核（STN），该框架在TT14数据集上实现了0.679的Dice系数，推理时间仅为0.05秒/受试者，并开发了配套GUI以方便临床应用。", "motivation": "自动分割DBS手术中的STN，以解决传统手动分割和基于配准方法在准确性和效率上的不足，特别是针对MRI数据中STN体积小和类别不平衡的问题。", "method": "提出LiNUS框架，改进U-Net架构，引入谱归一化约束、双线性插值上采样和多尺度特征融合机制。", "result": "在TT14数据集上，LiNUS取得了0.679的Dice系数，推理时间为0.05秒/受试者。在分辨率更高的DBS数据上验证，Dice分数达到0.89。LiNUS显著优于传统方法。", "conclusion": "LiNUS是一个高效且准确的轻量级深度学习框架，能够自动分割STN，并具有实时临床应用的潜力，其优化的架构使其在处理小目标和类别不平衡的MRI数据方面表现出色。"}}
{"id": "2601.14280", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14280", "abs": "https://arxiv.org/abs/2601.14280", "authors": ["Nicholas X. Wang", "Aggelos K. Katsaggelos"], "title": "Hallucination-Free Automatic Question & Answer Generation for Intuitive Learning", "comment": null, "summary": "Hallucinations in large language models (LLMs), defined as fluent yet incorrect or incoherent outputs, pose a significant challenge to the automatic generation of educational multiple-choice questions (MCQs). We identified four key hallucination types in MCQ generation: reasoning inconsistencies, insolvability, factual errors, and mathematical errors. To address this, we propose a hallucination-free multi-agent generation framework that breaks down MCQ generation into discrete, verifiable stages. Our framework utilizes both rule-based and LLM-based detection agents, as well as hallucination scoring metrics to optimize question quality. We redefined MCQ generation as an optimization task minimizing hallucination risk while maximizing validity, answerability, and cost-efficiency. We also introduce an agent-led refinement process that uses counterfactual reasoning and chain-of-thought (CoT) to iteratively improve hallucination in question generation. We evaluated a sample of AP- aligned STEM questions, where our system reduced hallucination rates by over 90% compared to baseline generation while preserving the educational value and style of questions. Our results demonstrate that structured multi-agent collaboration can mitigate hallucinations in educational content creation at scale, paving the way for more reliable LLM-powered learning tools.", "AI": {"tldr": "该研究提出了一个无幻觉的多智能体框架，用于自动生成教育性多选题（MCQs），通过将生成过程分解为可验证的阶段，并结合规则和LLM检测代理，显著减少了幻觉（降低90%以上），同时保持了问题的教育价值。", "motivation": "大型语言模型（LLMs）在生成教育性MCQs时存在“幻觉”（即生成流畅但不正确或不连贯的输出）的问题，这阻碍了其在教育内容生成中的应用。", "method": "提出一个无幻觉的多智能体生成框架，将MCQ生成分解为可验证的阶段。框架使用基于规则和LLM的检测代理，以及幻觉评分指标来优化问题质量。将MCQ生成重新定义为一个优化任务，最小化幻觉风险，最大化有效性、可解答性和成本效益。引入了基于反事实推理和思维链（CoT）的代理引导改进过程，以迭代地减少生成中的幻觉。", "result": "在一个AP对齐的STEM问题样本的评估中，该系统将幻觉率降低了90%以上，同时保持了教育价值和问题的风格。", "conclusion": "结构化的多智能体协作能够大规模地减轻教育内容生成中的幻觉问题，为开发更可靠的LLM驱动的学习工具铺平了道路。"}}
{"id": "2601.14330", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.14330", "abs": "https://arxiv.org/abs/2601.14330", "authors": ["Mengyu Sun", "Ziyuan Yang", "Andrew Beng Jin Teoh", "Junxu Liu", "Haibo Hu", "Yi Zhang"], "title": "LURE: Latent Space Unblocking for Multi-Concept Reawakening in Diffusion Models", "comment": null, "summary": "Concept erasure aims to suppress sensitive content in diffusion models, but recent studies show that erased concepts can still be reawakened, revealing vulnerabilities in erasure methods. Existing reawakening methods mainly rely on prompt-level optimization to manipulate sampling trajectories, neglecting other generative factors, which limits a comprehensive understanding of the underlying dynamics. In this paper, we model the generation process as an implicit function to enable a comprehensive theoretical analysis of multiple factors, including text conditions, model parameters, and latent states. We theoretically show that perturbing each factor can reawaken erased concepts. Building on this insight, we propose a novel concept reawakening method: Latent space Unblocking for concept REawakening (LURE), which reawakens erased concepts by reconstructing the latent space and guiding the sampling trajectory. Specifically, our semantic re-binding mechanism reconstructs the latent space by aligning denoising predictions with target distributions to reestablish severed text-visual associations. However, in multi-concept scenarios, naive reconstruction can cause gradient conflicts and feature entanglement. To address this, we introduce Gradient Field Orthogonalization, which enforces feature orthogonality to prevent mutual interference. Additionally, our Latent Semantic Identification-Guided Sampling (LSIS) ensures stability of the reawakening process via posterior density verification. Extensive experiments demonstrate that LURE enables simultaneous, high-fidelity reawakening of multiple erased concepts across diverse erasure tasks and methods.", "AI": {"tldr": "本文提出了一种名为LURE的新方法，通过分析和操纵扩散模型生成过程中的多重因素（文本条件、模型参数、潜在状态），来解决概念擦除后概念被重新唤醒的问题，并能同时高保真地重新唤醒多个概念。", "motivation": "现有概念擦除方法存在被轻易重新唤醒的漏洞，而现有的重新唤醒方法仅侧重于提示词优化，未能全面理解影响生成过程的多种因素，限制了对擦除机制的深入分析。", "method": "1. 将生成过程建模为隐函数，从理论上分析文本条件、模型参数和潜在状态对概念重现的影响。2. 提出LURE方法，通过重建潜在空间并引导采样轨迹来重现概念。具体包括：语义重绑定机制（对齐去噪预测与目标分布以重建文本-视觉关联）、梯度场正交化（解决多概念场景下的梯度冲突和特征纠缠）、潜在语义识别引导采样（LSIS）（通过后验密度验证确保重现过程的稳定性）。", "result": "LURE方法能够在多概念场景下，跨越不同的擦除任务和方法，同时实现高保真的概念重现。", "conclusion": "通过对扩散模型生成过程的深入分析，LURE方法能够有效地克服现有概念擦除方法的局限性，实现鲁棒且可控的概念重现，并为理解和操纵生成模型提供了新的视角。"}}
{"id": "2601.14704", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.14704", "abs": "https://arxiv.org/abs/2601.14704", "authors": ["Ruixing Ren", "Minqi Tao", "Junhui Zhao", "Xiaoke Sun", "Qiuping Li"], "title": "Hierarchical Optimization Based Multi-objective Dynamic Regulation Scheme for VANET Topology", "comment": "10 pages, 6 figures. A topology optimization strategy is proposed in this paper to optimize the latency, average path length, and throughput in Vehicular Ad Hoc Networks (VANETs)", "summary": "As a core technology of intelligent transportation systems, vehicular ad-hoc networks support latency-sensitive services such as safety warning and cooperative perception via vehicle-to-everything communications. However, their highly dynamic topology increases average path length, raises latency, and reduces throughput, severely limiting communication performance. Existing topology optimization methods lack capabilities in multi-objective coordination, dynamic adaptation, and global-local synergy. To address this, this paper proposes a two-layer dynamic topology regulation scheme combining local feature aggregation and global adjustment. The scheme constructs a dynamic multi-objective optimization model integrating average path length, end-to-end latency, and network throughput, and achieves multi-index coordination via link adaptability metrics and a dynamic normalization mechanism. it quickly responds to local link changes via feature fusion of local node feature extraction and dynamic neighborhood sensing, and balances optimization accuracy and real-time performance using a dual-mode adaptive solving strategy for global topology adjustment. It reduces network oscillation risks by introducing a performance improvement threshold and a topology validity verification mechanism. Simulation results on real urban road networks via the SUMO platform show that the proposed scheme outperforms traditional methods in average path length (stabilizing at ~4 hops), end-to-end latency (remaining ~0.01 s), and network throughput.", "AI": {"tldr": "提出了一种两层动态拓扑调控方案，通过局部特征聚合和全局调整，优化了车联网（VANETs）的平均路径长度、端到端延迟和网络吞吐量，有效解决了现有方法在多目标协调、动态适应和全局局部协同方面的不足。", "motivation": "现有的车联网拓扑优化方法在多目标协调、动态适应和全局局部协同方面存在不足，导致网络性能受高度动态拓扑的严重限制，包括增加平均路径长度、延迟和降低吞吐量。", "method": "提出了一种两层动态拓扑调控方案。该方案构建了一个动态多目标优化模型，整合了平均路径长度、端到端延迟和网络吞吐量。通过链路适应性指标和动态归一化机制实现多指标协调。利用局部节点特征提取和动态邻域感知进行特征融合，快速响应局部链路变化。采用双模自适应求解策略进行全局拓扑调整，平衡优化精度和实时性能。引入性能提升阈值和拓扑有效性验证机制降低网络振荡风险。", "result": "仿真结果表明，该方案在平均路径长度（稳定在约4跳）、端到端延迟（保持在约0.01秒）和网络吞吐量方面优于传统方法。", "conclusion": "所提出的两层动态拓扑调控方案能够有效提升车联网网络的通信性能，尤其在动态性和多目标优化方面表现出色，能够应对现实城市道路网络中的挑战。"}}
{"id": "2601.14456", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.14456", "abs": "https://arxiv.org/abs/2601.14456", "authors": ["Valerio Belcamino", "Nicholas Attolino", "Alessio Capitanelli", "Fulvio Mastrogiovanni"], "title": "On the Generalization Gap in LLM Planning: Tests and Verifier-Reward RL", "comment": "9 pages, 4 figures, 3 tables, 2 pages of supplementary materials. Submitted to a conference implementing a double-blind review process", "summary": "Recent work shows that fine-tuned Large Language Models (LLMs) can achieve high valid plan rates on PDDL planning tasks. However, it remains unclear whether this reflects transferable planning competence or domain-specific memorization. In this work, we fine-tune a 1.7B-parameter LLM on 40,000 domain-problem-plan tuples from 10 IPC 2023 domains, and evaluate both in-domain and cross-domain generalization. While the model reaches 82.9% valid plan rate in in-domain conditions, it achieves 0% on two unseen domains. To analyze this failure, we introduce three diagnostic interventions, namely (i) instance-wise symbol anonymization, (ii) compact plan serialization, and (iii) verifier-reward fine-tuning using the VAL validator as a success-focused reinforcement signal. Symbol anonymization and compact serialization cause significant performance drops despite preserving plan semantics, thus revealing strong sensitivity to surface representations. Verifier-reward fine-tuning reaches performance saturation in half the supervised training epochs, but does not improve cross-domain generalization. For the explored configurations, in-domain performance plateaus around 80%, while cross-domain performance collapses, suggesting that our fine-tuned model relies heavily on domain-specific patterns rather than transferable planning competence in this setting. Our results highlight a persistent generalization gap in LLM-based planning and provide diagnostic tools for studying its causes.", "AI": {"tldr": "本文研究了经过微调的大型语言模型（LLMs）在 PDDL 规划任务上的泛化能力。发现模型在训练域内表现良好，但在新域上表现不佳，表明其规划能力主要依赖于领域特定的模式，而非可迁移的规划能力。", "motivation": "探究经过微调的 LLMs 在 PDDL 规划任务上取得的高成功率是由于可迁移的规划能力还是领域特定的记忆。", "method": "在 10 个 IPC 2023 领域上，使用 40,000 个领域-问题-计划元组微调一个 1.7B 参数的 LLM。通过三种诊断干预来分析其失败原因：(i) 实例级符号匿名化，(ii) 紧凑计划序列化，(iii) 使用 VAL 验证器作为成功信号的验证器奖励微调。", "result": "模型在训练域内实现了 82.9% 的有效计划率，但在两个未见过的新域上成功率降至 0%。符号匿名化和紧凑序列化导致性能显著下降。验证器奖励微调在监督训练的 epoch 数减半的情况下达到性能饱和，但并未改善跨域泛化能力。在所探讨的配置下，域内性能稳定在 80% 左右，而跨域性能则完全崩溃。", "conclusion": "经过微调的 LLM 在 PDDL 规划任务中，在训练域内表现出很高的成功率，但这主要是由于对领域特定模式的依赖，而非可迁移的规划能力。这揭示了基于 LLM 的规划中存在持续的泛化差距，并提供了研究其原因的诊断工具。"}}
{"id": "2601.14617", "categories": ["cs.RO", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.14617", "abs": "https://arxiv.org/abs/2601.14617", "authors": ["Yunfeng Lin", "Li Xu", "Yong Yu", "Jiangmiao Pang", "Weinan Zhang"], "title": "UniCon: A Unified System for Efficient Robot Learning Transfers", "comment": "in submission, under review", "summary": "Deploying learning-based controllers across heterogeneous robots is challenging due to platform differences, inconsistent interfaces, and inefficient middleware. To address these issues, we present UniCon, a lightweight framework that standardizes states, control flow, and instrumentation across platforms. It decomposes workflows into execution graphs with reusable components, separating system states from control logic to enable plug-and-play deployment across various robot morphologies. Unlike traditional middleware, it prioritizes efficiency through batched, vectorized data flow, minimizing communication overhead and improving inference latency. This modular, data-oriented approach enables seamless sim-to-real transfer with minimal re-engineering. We demonstrate that UniCon reduces code redundancy when transferring workflows and achieves higher inference efficiency compared to ROS-based systems. Deployed on over 12 robot models from 7 manufacturers, it has been successfully integrated into ongoing research projects, proving its effectiveness in real-world scenarios.", "AI": {"tldr": "UniCon是一个轻量级框架，通过标准化状态、控制流和仪器仪表，简化了跨异构机器人平台部署基于学习的控制器。它将工作流分解为具有可重用组件的执行图，并优先考虑通过批量、矢量化数据流来提高效率，从而实现无缝的Sim-to-Real迁移。", "motivation": "在异构机器人平台上部署基于学习的控制器存在挑战，原因包括平台差异、接口不一致以及中间件效率低下。", "method": "UniCon框架通过以下方式解决这些问题：1. 标准化状态、控制流和仪器仪表。2. 将工作流分解为具有可重用组件的执行图。3. 分离系统状态与控制逻辑。4. 通过批量、矢量化数据流优化通信，提高效率。5. 实现无缝的Sim-to-Real迁移。", "result": "UniCon减少了转移工作流时的代码冗余，并比基于ROS的系统实现了更高的推理效率。该框架已成功部署在7家制造商的12个以上机器人模型上，并集成到实际研究项目中。", "conclusion": "UniCon是一个有效且轻量级的框架，可以简化跨异构机器人平台的学习控制器部署，并通过其模块化、面向数据的设计实现了高效的Sim-to-Real迁移。"}}
{"id": "2601.14997", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14997", "abs": "https://arxiv.org/abs/2601.14997", "authors": ["K. Punnam Chandar", "Y. Ravi Kumar"], "title": "Filtered 2D Contour-Based Reconstruction of 3D STL Model from CT-DICOM Images", "comment": "8 pages, 18 figures", "summary": "Reconstructing a 3D Stereo-lithography (STL) Model from 2D Contours of scanned structure in Digital Imaging and Communication in Medicine (DICOM) images is crucial to understand the geometry and deformity. Computed Tomography (CT) images are processed to enhance the contrast, reduce the noise followed by smoothing. The processed CT images are segmented using thresholding technique. 2D contour data points are extracted from segmented CT images and are used to construct 3D STL Models. The 2D contour data points may contain outliers as a result of segmentation of low resolution images and the geometry of the constructed 3D structure deviate from the actual. To cope with the imperfections in segmentation process, in this work we propose to use filtered 2D contour data points to reconstruct 3D STL Model. The filtered 2D contour points of each image are delaunay triangulated and joined layer-by-layer to reconstruct the 3D STL model. The 3D STL Model reconstruction is verified on i) 2D Data points of basic shapes and ii) Region of Interest (ROI) of human pelvic bone and are presented as case studies. The 3D STL model constructed from 2D contour data points of ROI of segmented pelvic bone with and without filtering are presented. The 3D STL model reconstructed from filtered 2D data points improved the geometry of model compared to the model reconstructed without filtering 2D data points.", "AI": {"tldr": "该研究提出了一种从DICOM图像中提取的2D轮廓点重建3D STL模型的方法，通过滤波消除噪声点，以提高重建模型的几何精度，并在人体骨盆区域进行了验证。", "motivation": "从2D医学图像（DICOM）中提取的2D轮廓点用于重建3D STL模型，但由于分割过程中的噪声点（outliers）会导致3D模型几何失真，因此需要一种方法来改进重建质量。", "method": "首先对CT图像进行增强和去噪处理，然后使用阈值分割提取2D轮廓点。接着对提取的2D轮廓点进行滤波，去除噪声点。滤波后的2D轮廓点通过Delaunay三角剖分，并逐层连接，最终重建3D STL模型。", "result": "通过对基本形状和人体骨盆区域的实验验证，发现使用滤波后的2D轮廓点重建的3D STL模型，其几何形状相比于未使用滤波的方法有了显著的改进。", "conclusion": "滤波2D轮廓点的方法能够有效地提高从DICOM图像中重建3D STL模型的几何精度，克服了分割过程中可能引入的噪声点带来的问题。"}}
{"id": "2601.14485", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14485", "abs": "https://arxiv.org/abs/2601.14485", "authors": ["Yuan Tian", "Yi Mei", "Mengjie Zhang"], "title": "Scalable Knee-Point Guided Activity Group Selection in Multi-Tree Genetic Programming for Dynamic Multi-Mode Project Scheduling", "comment": "17 pages, 9 figures. This paper has been accepted by the Pacific Rim International Conference Series on Artificial Intelligence (PRICAI) 2025 but not published yet. This is the submission to review version, not the camera-ready version", "summary": "The dynamic multi-mode resource-constrained project scheduling problem is a challenging scheduling problem that requires making decisions on both the execution order of activities and their corresponding execution modes. Genetic programming has been widely applied as a hyper-heuristic to evolve priority rules that guide the selection of activity-mode pairs from the current eligible set. Recently, an activity group selection strategy has been proposed to select a subset of activities rather than a single activity at each decision point, allowing for more effective scheduling by considering the interdependence between activities. Although effective in small-scale instances, this strategy suffers from scalability issues when applied to larger problems. In this work, we enhance the scalability of the group selection strategy by introducing a knee-point-based selection mechanism to identify a promising subset of activities before evaluating their combinations. An activity ordering rule is first used to rank all eligible activity-mode pairs, followed by a knee point selection to find the promising pairs. Then, a group selection rule selects the best activity combination. We develop a multi-tree GP framework to evolve both types of rules simultaneously. Experimental results demonstrate that our approach scales well to large instances and outperforms GP with sequential decision-making in most scenarios.", "AI": {"tldr": "提出了一种基于膝点选择的增强型活动分组选择策略，用于解决多模式资源约束项目调度问题，提高了算法在大规模问题上的可扩展性和性能。", "motivation": "现有的活动分组选择策略在处理大规模动态多模式资源约束项目调度问题时存在可扩展性问题。", "method": "引入基于膝点选择的机制，首先使用活动排序规则对所有符合条件的活动-模式对进行排序，然后通过膝点选择识别有前途的活动对，最后使用分组选择规则选择最佳活动组合。开发了多树遗传编程框架来同时演化这两种规则。", "result": "实验结果表明，所提出的方法在大规模实例上具有良好的可扩展性，并且在大多数情况下优于采用顺序决策的遗传编程方法。", "conclusion": "基于膝点选择的活动分组策略能够有效提升动态多模式资源约束项目调度问题的处理效率和性能，尤其是在处理大规模问题时。"}}
{"id": "2601.14339", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14339", "abs": "https://arxiv.org/abs/2601.14339", "authors": ["Haotian Xu", "Yue Hu", "Zhengqiu Zhu", "Chen Gao", "Ziyou Wang", "Junreng Rao", "Wenhao Lu", "Weishi Li", "Quanjun Yin", "Yong Li"], "title": "CityCube: Benchmarking Cross-view Spatial Reasoning on Vision-Language Models in Urban Environments", "comment": null, "summary": "Cross-view spatial reasoning is essential for embodied AI, underpinning spatial understanding, mental simulation and planning in complex environments. Existing benchmarks primarily emphasize indoor or street settings, overlooking the unique challenges of open-ended urban spaces characterized by rich semantics, complex geometries, and view variations. To address this, we introduce CityCube, a systematic benchmark designed to probe cross-view reasoning capabilities of current VLMs in urban settings. CityCube integrates four viewpoint dynamics to mimic camera movements and spans a wide spectrum of perspectives from multiple platforms, e.g., vehicles, drones and satellites. For a comprehensive assessment, it features 5,022 meticulously annotated multi-view QA pairs categorized into five cognitive dimensions and three spatial relation expressions. A comprehensive evaluation of 33 VLMs reveals a significant performance disparity with humans: even large-scale models struggle to exceed 54.1% accuracy, remaining 34.2% below human performance. By contrast, small-scale fine-tuned VLMs achieve over 60.0% accuracy, highlighting the necessity of our benchmark. Further analyses indicate the task correlations and fundamental cognitive disparity between VLMs and human-like reasoning.", "AI": {"tldr": "本文提出了CityCube，一个用于评估视觉语言模型（VLMs）在城市环境中跨视角空间推理能力的基准测试。现有基准在城市开放空间方面存在不足。CityCube包含了多平台、多视角的QA对，并评估了33个VLMs，发现其性能远低于人类，但经过微调的小型模型表现出潜力。", "motivation": "现有的跨视角空间推理基准主要关注室内或街道场景，未能充分应对开放式城市空间所带来的丰富语义、复杂几何形状和视角变化等独特挑战。因此，需要一个专门针对城市环境的基准来评估VLMs在该领域的推理能力。", "method": "引入了CityCube基准测试，该测试系统地整合了四种视角动态，模拟了不同平台的相机移动，覆盖了车辆、无人机和卫星等多种视角。基准包含了5,022个精心标注的多视角问答对，这些问答对被划分为五个认知维度和三种空间关系表达。对33个VLMs进行了评估。", "result": "在CityCube基准上的评估显示，即使是大型VLMs，准确率也难以超过54.1%，比人类水平低34.2%。相比之下，经过微调的小型VLMs能够达到60.0%以上的准确率。此外，分析揭示了任务相关性以及VLMs和人类推理之间的根本性认知差异。", "conclusion": "CityCube基准测试揭示了当前VLMs在城市环境中跨视角空间推理方面的显著局限性，其性能远低于人类。虽然大型模型表现不佳，但小型模型通过微调展现出改进的潜力，这突显了CityCube基准的必要性。研究还发现了VLMs与人类在空间推理能力上的根本差异。"}}
{"id": "2601.14269", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14269", "abs": "https://arxiv.org/abs/2601.14269", "authors": ["Youyou Cheng", "Zhuangwei Kang", "Kerry Jiang", "Chenyu Sun", "Qiyang Pan"], "title": "The Slow Drift of Support: Boundary Failures in Multi-Turn Mental Health LLM Dialogues", "comment": null, "summary": "Large language models (LLMs) have been widely used for mental health support. However, current safety evaluations in this field are mostly limited to detecting whether LLMs output prohibited words in single-turn conversations, neglecting the gradual erosion of safety boundaries in long dialogues. Examples include making definitive guarantees, assuming responsibility, and playing professional roles. We believe that with the evolution of mainstream LLMs, words with obvious safety risks are easily filtered by their underlying systems, while the real danger lies in the gradual transgression of boundaries during multi-turn interactions, driven by the LLM's attempts at comfort and empathy.\n  This paper proposes a multi-turn stress testing framework and conducts long-dialogue safety tests on three cutting-edge LLMs using two pressure methods: static progression and adaptive probing. We generated 50 virtual patient profiles and stress-tested each model through up to 20 rounds of virtual psychiatric dialogues. The experimental results show that violations are common, and both pressure modes produced similar violation rates. However, adaptive probing significantly advanced the time at which models crossed boundaries, reducing the average number of turns from 9.21 in static progression to 4.64. Under both mechanisms, making definitive or zero-risk promises was the primary way in which boundaries were breached. These findings suggest that the robustness of LLM safety boundaries cannot be inferred solely through single-turn tests; it is necessary to fully consider the wear and tear on safety boundaries caused by different interaction pressures and characteristics in extended dialogues.", "AI": {"tldr": "现有LLM在心理健康支持领域的安全评估侧重于单轮对话中的违禁词检测，忽略了多轮对话中安全界限的渐进式侵蚀。本文提出了一个多轮压力测试框架，在长对话中测试了三个前沿LLM，发现违规行为普遍存在，且自适应探测比静态递进更快地导致边界被突破。研究表明，仅凭单轮测试无法推断LLM安全边界的鲁棒性，需要考虑多轮交互压力。", "motivation": "现有LLM在心理健康支持领域的安全评估方法过于局限，主要关注单轮对话中的违禁词，未能有效捕捉多轮对话中安全界限的渐进式侵蚀，尤其是在LLM试图提供安慰和共情时，这种危险性更为隐蔽。", "method": "提出一个多轮压力测试框架，并采用静态递进和自适应探测两种压力方法，对三个前沿LLM进行了长对话安全测试。通过生成50个虚拟患者档案，在多达20轮的虚拟精神病学对话中对每个模型进行压力测试。", "result": "研究发现在长对话中，LLM的安全界限违规行为普遍存在。两种压力模式（静态递进和自适应探测）产生的违规率相似，但自适应探测显著加快了模型突破界限的时间，平均回合数从静态递进的9.21轮减少到4.64轮。在两种压力机制下，做出明确的或零风险的承诺是主要的边界违规方式。", "conclusion": "现有LLM在心理健康支持中的安全边界在多轮对话中容易受到侵蚀，单轮测试无法全面评估其鲁棒性。必须考虑不同交互压力和特征在长对话中对安全边界的损耗，以更全面地评估和提升LLM在敏感领域的安全性。"}}
{"id": "2601.14289", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14289", "abs": "https://arxiv.org/abs/2601.14289", "authors": ["Yelin Chen", "Fanjin Zhang", "Suping Sun", "Yunhe Pang", "Yuanchun Wang", "Jian Song", "Xiaoyan Li", "Lei Hou", "Shu Zhao", "Jie Tang", "Juanzi Li"], "title": "RPC-Bench: A Fine-grained Benchmark for Research Paper Comprehension", "comment": "11 pages, 21 appendix pages", "summary": "Understanding research papers remains challenging for foundation models due to specialized scientific discourse and complex figures and tables, yet existing benchmarks offer limited fine-grained evaluation at scale. To address this gap, we introduce RPC-Bench, a large-scale question-answering benchmark built from review-rebuttal exchanges of high-quality computer science papers, containing 15K human-verified QA pairs. We design a fine-grained taxonomy aligned with the scientific research flow to assess models' ability to understand and answer why, what, and how questions in scholarly contexts. We also define an elaborate LLM-human interaction annotation framework to support large-scale labeling and quality control. Following the LLM-as-a-Judge paradigm, we develop a scalable framework that evaluates models on correctness-completeness and conciseness, with high agreement to human judgment. Experiments reveal that even the strongest models (GPT-5) achieve only 68.2% correctness-completeness, dropping to 37.46% after conciseness adjustment, highlighting substantial gaps in precise academic paper understanding. Our code and data are available at https://rpc-bench.github.io/.", "AI": {"tldr": "本文提出了RPC-Bench，一个包含15K问答对的大规模计算机科学论文审查-辩驳问答基准，旨在评估基础模型在理解学术论文方面的能力，并发现现有模型在该任务上存在显著差距。", "motivation": "现有评估基础模型理解科研论文的基准在规模和细粒度方面存在不足，尤其是在处理专业的科学论述、图表以及评估模型在科学研究流程中的理解能力方面。因此，需要一个更全面、更精细化的基准。", "method": "利用高质量计算机科学论文的审查-辩驳交流构建了RPC-Bench问答基准，包含15K人工验证的问答对。设计了一个与科学研究流程对齐的细粒度分类体系，用于评估模型回答“为什么”、“什么”和“如何”问题的能力。采用LLM-as-a-Judge范式，开发了一个可扩展的评估框架，衡量答案的正确性-完整性和简洁性，并通过LLM-human交互标注框架进行大规模数据标注和质量控制。", "result": "即使是表现最好的模型（GPT-5）在正确性-完整性方面也仅达到68.2%，在考虑简洁性后更是下降到37.46%，表明当前模型在精确理解学术论文方面仍存在巨大差距。", "conclusion": "RPC-Bench是一个大规模、细粒度的科研论文问答基准，揭示了当前基础模型在理解和回答学术论文相关问题方面存在的不足。该基准和相关工具可用于更精确地评估和改进模型在科学文献理解方面的能力。"}}
{"id": "2601.14514", "categories": ["cs.AI", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2601.14514", "abs": "https://arxiv.org/abs/2601.14514", "authors": ["Tony Chen", "Sam Cheyette", "Kelsey Allen", "Joshua Tenenbaum", "Kevin Smith"], "title": "\"Just in Time\" World Modeling Supports Human Planning and Reasoning", "comment": null, "summary": "Probabilistic mental simulation is thought to play a key role in human reasoning, planning, and prediction, yet the demands of simulation in complex environments exceed realistic human capacity limits. A theory with growing evidence is that people simulate using simplified representations of the environment that abstract away from irrelevant details, but it is unclear how people determine these simplifications efficiently. Here, we present a \"Just-in-Time\" framework for simulation-based reasoning that demonstrates how such representations can be constructed online with minimal added computation. The model uses a tight interleaving of simulation, visual search, and representation modification, with the current simulation guiding where to look and visual search flagging objects that should be encoded for subsequent simulation. Despite only ever encoding a small subset of objects, the model makes high-utility predictions. We find strong empirical support for this account over alternative models in a grid-world planning task and a physical reasoning task across a range of behavioral measures. Together, these results offer a concrete algorithmic account of how people construct reduced representations to support efficient mental simulation.", "AI": {"tldr": "本文提出了一种“即时”模拟推理框架，解释了人类如何通过在线构建简化的环境表征，用最少的计算进行高效的心理模拟，从而在复杂环境中做出有用的预测。", "motivation": "现有的心理模拟理论难以解释人类在复杂环境中超出现实能力的推理、规划和预测能力。因此，研究人类如何高效地确定并构建简化的环境表征以支持模拟是必要的。", "method": "提出了一种“即时”模拟推理框架，该框架通过模拟、视觉搜索和表征修改的紧密结合，在线构建简化的环境表征。模型利用当前的模拟指导视觉搜索，而视觉搜索则标记需要编码的对象以供后续模拟。", "result": "该模型即使只编码了对象的一小部分，也能做出高价值的预测。在网格世界规划任务和物理推理任务中，该模型在多项行为指标上均优于其他模型。", "conclusion": "该研究提供了一个具体的算法解释，说明了人类如何构建简化的表征来支持高效的心理模拟，支持了“即时”框架的有效性。"}}
{"id": "2601.15119", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.15119", "abs": "https://arxiv.org/abs/2601.15119", "authors": ["Md Mahmudul Hoque", "Md Mehedi Hassain", "Muntakimur Rahaman", "Md. Towhidul Islam", "Shaista Rani", "Md Sharif Mollah"], "title": "Vision Models for Medical Imaging: A Hybrid Approach for PCOS Detection from Ultrasound Scans", "comment": null, "summary": "Polycystic Ovary Syndrome (PCOS) is the most familiar endocrine illness in women of reproductive age. Many Bangladeshi women suffer from PCOS disease in their older age. The aim of our research is to identify effective vision-based medical image analysis techniques and evaluate hybrid models for the accurate detection of PCOS. We introduced two novel hybrid models combining convolutional and transformer-based approaches. The training and testing data were organized into two categories: \"infected\" (PCOS-positive) and \"noninfected\" (healthy ovaries). In the initial stage, our first hybrid model, 'DenConST' (integrating DenseNet121, Swin Transformer, and ConvNeXt), achieved 85.69% accuracy. The final optimized model, 'DenConREST' (incorporating Swin Transformer, ConvNeXt, DenseNet121, ResNet18, and EfficientNetV2), demonstrated superior performance with 98.23% accuracy. Among all evaluated models, DenConREST showed the best performance. This research highlights an efficient solution for PCOS detection from ultrasound images, significantly improving diagnostic accuracy while reducing detection errors.", "AI": {"tldr": "本研究提出了一种结合卷积神经网络和 Transformer 的混合模型，用于通过医学图像分析准确检测多囊卵巢综合征（PCOS），其中优化的 DenConREST 模型达到了 98.23% 的准确率。", "motivation": "由于多囊卵巢综合征（PCOS）是育龄期女性常见的内分泌疾病，并且在孟加拉国女性中发病率高，因此本研究旨在开发一种有效的基于视觉的医学图像分析技术和混合模型，以提高 PCOS 检测的准确性。", "method": "研究提出了两种新颖的混合模型，将卷积方法（DenseNet121, ConvNeXt, ResNet18, EfficientNetV2）和 Transformer 方法（Swin Transformer）相结合。将医学图像数据分为“感染”（PCOS 阳性）和“非感染”（健康卵巢）两类进行训练和测试。", "result": "第一个混合模型 'DenConST' 实现了 85.69% 的准确率。经过优化后的最终模型 'DenConREST'，整合了 Swin Transformer, ConvNeXt, DenseNet121, ResNet18, 和 EfficientNetV2，取得了 98.23% 的准确率，表现优于所有评估的模型。", "conclusion": "本研究成功开发了 DenConREST 混合模型，为通过超声图像检测 PCOS 提供了一种高效的解决方案，显著提高了诊断准确性并减少了检测误差。"}}
{"id": "2601.14290", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.14290", "abs": "https://arxiv.org/abs/2601.14290", "authors": ["Aradhya Dixit", "Tianxi Liang", "Jai Telang"], "title": "Project Aletheia: Verifier-Guided Distillation of Backtracking for Small Language Models", "comment": null, "summary": "Small Language Models (SLMs, under 10B parameters) are attractive for private, on-device deployment, yet they frequently fail on strict constraint-satisfaction problems due to linear, overconfident reasoning traces that do not recover from early mistakes. We introduce Verifier-Guided Distillation, a training protocol that transfers the process of error repair - explicit conflict detection and backtracking - rather than only correct final answers. By training a 7B model on verified reasoning traces that include mistakes and self-corrections, we show that latent verification behavior can emerge in small models, enabling them to occasionally stop, detect contradictions, and revise earlier assumptions.", "AI": {"tldr": "提出了一种名为“验证器引导蒸馏”的训练方法，通过让小语言模型（SLM）学习包含错误和自我纠正的推理过程，来解决其在严格约束满足问题上的不足。", "motivation": "现有的SLM虽然适合私有和设备端部署，但在处理需要严格约束满足的问题时，由于推理过程线性且过于自信，容易出现早期错误无法挽回的情况。", "method": "采用“验证器引导蒸馏”训练协议，重点在于迁移错误修复的过程（如显式冲突检测和回溯），而不仅仅是最终的正确答案。具体来说，训练了一个7B参数的模型，使用了包含错误和自我纠正的验证过推理轨迹。", "result": "通过上述训练方法，模型展现出了潜在的验证行为，能够偶尔停止、检测矛盾并修正先前的假设。", "conclusion": "验证器引导蒸馏是一种有效的训练策略，可以使小语言模型具备检测和修复自身推理错误的能力，从而提升其在严格约束满足任务上的表现。"}}
{"id": "2601.14725", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.14725", "abs": "https://arxiv.org/abs/2601.14725", "authors": ["Zihao Ren", "Lei Wang", "Deming Yuan", "Guodong Shi"], "title": "Differential Privacy on Affine Manifolds: Geometrically Confined Privacy in Linear Dynamical Systems", "comment": null, "summary": "In this paper, we present a comprehensive framework for differential privacy over affine manifolds and validate its usefulness in the contexts of differentially private cloud-based control and average consensus. We consider differential privacy mechanisms for linear queries when the input data are constrained to lie on affine manifolds, a structural property that is assumed to be available as prior knowledge to adversaries. In this setting, the definition of neighborhood adjacency must be formulated with respect to the intrinsic geometry of the manifolds. We demonstrate that such affine-manifold constraints can fundamentally alter the attainable privacy levels relative to the unconstrained case. In particular, we derive necessary and sufficient conditions under which differential privacy can be realized via structured noise injection mechanisms, wherein correlated Gaussian or Laplace noise distributions, rather than i.i.d. perturbations, are calibrated to the dataset. Based on these characterizations, we develop explicit noise calibration procedures that guarantee the tight realization of any prescribed privacy budget with a matching noise magnitude. Finally, we show that the proposed framework admits direct applications to linear dynamical systems ranging from differentially private cloud-based control to privacy-preserving average consensus, all of which naturally involve affine-manifold constraints. The established theoretical results are illustrated through numerical examples.", "AI": {"tldr": "本文提出了一个在仿射流形上实现差分隐私的框架，并将其应用于云控制和平均共识。该框架考虑了线性查询，并利用仿射流形的内在几何结构来定义邻域邻接性，从而在理论上证明了约束条件如何影响隐私级别，并开发了结构化噪声注入机制来满足预设的隐私预算。", "motivation": "研究动机是为在数据受到仿射流形约束的情况下实现差分隐私提供一个通用的框架，并验证其在云控制和平均共识等实际应用中的有效性。现有的差分隐私方法通常假设数据是无约束的，而忽略了数据内在的几何结构可能带来的隐私提升机会。", "method": "本文提出了一种新的差分隐私定义，该定义考虑了数据在仿射流形上的内在几何结构。在此基础上，研究了如何通过注入与数据集校准的结构化噪声（如相关的正态或拉普拉斯噪声）来实现差分隐私，并推导了实现特定隐私预算的充要条件和噪声校准程序。最后，将该框架应用于线性动力学系统，包括云控制和平均共识。", "result": "研究表明，仿射流形约束可以根本性地改变可达到的隐私级别。开发了能够精确实现任何预设隐私预算的结构化噪声注入机制。该框架可以直接应用于云控制和平均共识等领域，并通过数值示例进行了验证。", "conclusion": "本文提出的在仿射流形上实现差分隐私的框架是有效且实用的。通过利用数据的几何结构，可以设计出更有效的隐私保护机制，尤其是在实际应用如云控制和平均共识中，该框架提供了理论指导和具体的实现方法。"}}
{"id": "2601.14622", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.14622", "abs": "https://arxiv.org/abs/2601.14622", "authors": ["Ling Xiao", "Toshihiko Yamasaki"], "title": "Probing Prompt Design for Socially Compliant Robot Navigation with Vision Language Models", "comment": null, "summary": "Language models are increasingly used for social robot navigation, yet existing benchmarks largely overlook principled prompt design for socially compliant behavior. This limitation is particularly relevant in practice, as many systems rely on small vision language models (VLMs) for efficiency. Compared to large language models, small VLMs exhibit weaker decision-making capabilities, making effective prompt design critical for accurate navigation. Inspired by cognitive theories of human learning and motivation, we study prompt design along two dimensions: system guidance (action-focused, reasoning-oriented, and perception-reasoning prompts) and motivational framing, where models compete against humans, other AI systems, or their past selves. Experiments on two socially compliant navigation datasets reveal three key findings. First, for non-finetuned GPT-4o, competition against humans achieves the best performance, while competition against other AI systems performs worst. For finetuned models, competition against the model's past self yields the strongest results, followed by competition against humans, with performance further influenced by coupling effects among prompt design, model choice, and dataset characteristics. Second, inappropriate system prompt design can significantly degrade performance, even compared to direct finetuning. Third, while direct finetuning substantially improves semantic-level metrics such as perception, prediction, and reasoning, it yields limited gains in action accuracy. In contrast, our system prompts produce a disproportionately larger improvement in action accuracy, indicating that the proposed prompt design primarily acts as a decision-level constraint rather than a representational enhancement.", "AI": {"tldr": "本文研究了在社交机器人导航中，如何通过设计提示（prompt）来提升小型视觉语言模型（VLM）的社会合规性导航能力。研究发现，与人类竞争的提示对未微调的模型效果最好，而对于微调过的模型，与自身历史版本竞争的提示效果最佳。不当的提示设计会显著降低模型性能，而本文提出的提示设计能显著提升模型的动作准确性。", "motivation": "现有的社交机器人导航基准忽视了提示设计在实现社会合规行为中的作用，尤其是在依赖小型VLM的场景下，其决策能力较弱，更需要有效的提示设计来弥补。受认知理论启发，研究者希望探索提示设计如何影响模型的导航性能。", "method": "研究者从系统引导（动作导向、推理导向、感知-推理提示）和动机框架（与人类、其他AI、或自身过去竞争）两个维度设计了提示。实验在两个社交合规导航数据集上进行，并在GPT-4o模型上进行了测试，对比了不同提示设计和模型微调对导航性能的影响。", "result": "1. 对于未微调的GPT-4o，与人类竞争的提示表现最佳；对于微调过的模型，与自身过去竞争的提示效果最好。2. 不恰当的系统提示设计会严重损害模型性能。3. 直接微调主要提升语义层面指标，对动作准确性提升有限，而本文提出的提示设计能不成比例地提升动作准确性。", "conclusion": "本文提出的提示设计能够显著提升小型VLM在社交机器人导航中的动作准确性，其作用更多体现在决策层面的约束而非表征的增强。提示设计，特别是动机框架和系统引导的结合，对模型性能至关重要，并与模型选择和数据集特性存在耦合效应。"}}
{"id": "2601.14406", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2601.14406", "abs": "https://arxiv.org/abs/2601.14406", "authors": ["Yixiong Chen", "Zongwei Zhou", "Wenxuan Li", "Alan Yuille"], "title": "Large-Scale Label Quality Assessment for Medical Segmentation via a Vision-Language Judge and Synthetic Data", "comment": "ISBI 2026 accepted", "summary": "Large-scale medical segmentation datasets often combine manual and pseudo-labels of uneven quality, which can compromise training and evaluation. Low-quality labels may hamper performance and make the model training less robust. To address this issue, we propose SegAE (Segmentation Assessment Engine), a lightweight vision-language model (VLM) that automatically predicts label quality across 142 anatomical structures. Trained on over four million image-label pairs with quality scores, SegAE achieves a high correlation coefficient of 0.902 with ground-truth Dice similarity and evaluates a 3D mask in 0.06s. SegAE shows several practical benefits: (I) Our analysis reveals widespread low-quality labeling across public datasets; (II) SegAE improves data efficiency and training performance in active and semi-supervised learning, reducing dataset annotation cost by one-third and quality-checking time by 70% per label. This tool provides a simple and effective solution for quality control in large-scale medical segmentation datasets. The dataset, model weights, and codes are released at https://github.com/Schuture/SegAE.", "AI": {"tldr": "提出了一种名为SegAE的轻量级视觉语言模型，用于自动评估医学图像分割标签的质量，并在实际应用中提高了数据效率和训练性能。", "motivation": "大型医学分割数据集常包含质量不均的标签，这会影响模型训练和评估。低质量标签可能降低模型性能并使其训练不稳定。", "method": "开发了一个名为SegAE的轻量级视觉语言模型（VLM），在超过四百万个带质量分数的图像-标签对上进行训练，用于自动预测142个解剖结构标签的质量。", "result": "SegAE能以0.06秒的速度评估一个3D掩码，与地面真实Dice相似度达到0.902的高度相关性。通过SegAE的分析揭示了公共数据集中普遍存在的低质量标签问题。在主动学习和半监督学习中，SegAE能提高数据效率和训练性能，将数据集标注成本降低三分之一，质量检查时间减少70%。", "conclusion": "SegAE提供了一种简单有效的解决方案，用于大规模医学分割数据集的质量控制，并显著降低了数据标注和质量检查的成本。"}}
{"id": "2601.14880", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.14880", "abs": "https://arxiv.org/abs/2601.14880", "authors": ["Lei Zheng", "Luyao Zhang", "Peiqi Yu", "Yifan Sun", "Sergio Grammatico", "Jun Ma", "Changliu Liu"], "title": "Contingency Planning for Safety-Critical Autonomous Vehicles: A Review and Perspectives", "comment": "23 pages, 6 figures", "summary": "Contingency planning is the architectural capability that enables autonomous vehicles (AVs) to anticipate and mitigate discrete, high-impact hazards, such as sensor outages and adversarial interactions. This paper presents a comprehensive survey of the field, synthesizing fragmented literature into a unified logic-conditioned hybrid control framework. Within this formalism, we categorize approaches into two distinct paradigms: Reactive Safety, which responds to realized hazards by enforcing safety constraints or executing fail-safe maneuvers; and Proactive Safety, which optimizes for future recourse by branching over potential modal transitions. In addition, we propose a fine-grained taxonomy that partitions the landscape into external contingencies (environmental and interactive hazards) and internal contingencies (system faults). Through a critical comparative analysis, we reveal a fundamental structural divergence: internal faults are predominantly addressed via reactive fail-safe mechanisms, whereas external interaction uncertainties increasingly require proactive branching strategies. Furthermore, we identify a critical methodological divergence: whereas physical hazards are typically managed with formal guarantees, semantic and out-of-distribution anomalies currently rely heavily on empirical validation. We conclude by identifying the open challenges in bridging the gap between theoretical guarantees and practical validation, advocating for hybrid architectures and standardized benchmarking to transition contingency planning from formulation to certifiable real-world deployment.", "AI": {"tldr": "本文对自动驾驶汽车（AVs）的应急规划进行了全面综述，提出了一个统一的逻辑条件混合控制框架，并将现有方法分为反应式安全和主动式安全两种范式。研究区分了内部和外部应急情况，并分析了不同应急类型和不确定性处理方式之间的差异。最后，指出了理论保障与实际验证之间的差距，并提出了混合架构和标准化基准测试的未来方向。", "motivation": "当前的自动驾驶汽车（AVs）在应对传感器故障、对抗性攻击等高风险突发事件方面能力不足，现有研究分散且缺乏统一框架。本文旨在整合分散的文献，建立一个统一的框架来指导AVs的应急规划研究。", "method": "本文首先提出一个逻辑条件混合控制框架，用于统一分析和分类现有的应急规划方法。然后，将现有方法划分为“反应式安全”和“主动式安全”两类范式。接着，对应急情况进行细粒度分类，分为外部应急（环境和交互危险）和内部应急（系统故障）。最后，通过比较分析，揭示了不同应急类型和不确定性处理方式之间的结构性和方法论上的差异。", "result": "研究发现，内部故障主要通过反应式故障安全机制处理，而外部交互不确定性则越来越多地需要主动式分支策略。此外，物理危险通常采用形式化保障，而语义和分布外异常则主要依赖经验验证。", "conclusion": "应急规划领域存在理论保障与实际验证之间的差距。未来的研究需要开发混合架构，并建立标准化的基准测试，以推动应急规划从理论研究走向可认证的实际部署。"}}
{"id": "2601.14477", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2601.14477", "abs": "https://arxiv.org/abs/2601.14477", "authors": ["Frank Bieder", "Hendrik Königshof", "Haohao Hu", "Fabian Immel", "Yinzhe Shen", "Jan-Hendrik Pauls", "Christoph Stiller"], "title": "XD-MAP: Cross-Modal Domain Adaptation using Semantic Parametric Mapping", "comment": null, "summary": "Until open-world foundation models match the performance of specialized approaches, the effectiveness of deep learning models remains heavily dependent on dataset availability. Training data must align not only with the target object categories but also with the sensor characteristics and modalities. To bridge the gap between available datasets and deployment domains, domain adaptation strategies are widely used. In this work, we propose a novel approach to transferring sensor-specific knowledge from an image dataset to LiDAR, an entirely different sensing domain. Our method XD-MAP leverages detections from a neural network on camera images to create a semantic parametric map. The map elements are modeled to produce pseudo labels in the target domain without any manual annotation effort. Unlike previous domain transfer approaches, our method does not require direct overlap between sensors and enables extending the angular perception range from a front-view camera to a full 360 view. On our large-scale road feature dataset, XD-MAP outperforms single shot baseline approaches by +19.5 mIoU for 2D semantic segmentation, +19.5 PQth for 2D panoptic segmentation, and +32.3 mIoU in 3D semantic segmentation. The results demonstrate the effectiveness of our approach achieving strong performance on LiDAR data without any manual labeling.", "AI": {"tldr": "提出了一种名为XD-MAP的新型跨域知识迁移方法，能够将摄像头图像中的语义信息转移到LiDAR点云数据中，实现了无需手动标注的3D语义和全景分割。该方法通过在图像上进行检测，构建语义参数化地图，然后生成伪标签，无需传感器重叠，并将感知范围从前视扩展到360度。", "motivation": "当前的深度学习模型性能很大程度上依赖于可用的数据集，但训练数据需要与目标类别、传感器特性和模态都匹配。为了弥合现有数据集与实际部署域之间的差距，需要域适应策略。", "method": "提出XD-MAP方法，利用摄像头图像上的神经网络检测结果创建语义参数化地图。该地图的元素被建模以在目标域（LiDAR）中生成伪标签，无需手动标注。该方法不要求传感器之间有直接重叠，并能将感知范围从前视摄像头扩展到360度。", "result": "在大型道路特征数据集上，XD-MAP在2D语义分割、2D全景分割和3D语义分割任务上，分别比单摄像头基线方法提升了+19.5 mIoU、+19.5 PQth和+32.3 mIoU。", "conclusion": "XD-MAP方法在没有手动标注的情况下，有效地将摄像头图像的知识迁移到了LiDAR数据上，并在3D语义分割任务上取得了显著的性能提升，证明了其跨传感器域迁移的有效性。"}}
{"id": "2601.14628", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14628", "abs": "https://arxiv.org/abs/2601.14628", "authors": ["Weiyu Guo", "He Zhang", "Pengteng Li", "Tiefu Cai", "Ziyang Chen", "Yandong Guo", "Xiao He", "Yongkui Yang", "Ying Sun", "Hui Xiong"], "title": "A Brain-inspired Embodied Intelligence for Fluid and Fast Reflexive Robotics Control", "comment": null, "summary": "Recent advances in embodied intelligence have leveraged massive scaling of data and model parameters to master natural-language command following and multi-task control. In contrast, biological systems demonstrate an innate ability to acquire skills rapidly from sparse experience. Crucially, current robotic policies struggle to replicate the dynamic stability, reflexive responsiveness, and temporal memory inherent in biological motion. Here we present Neuromorphic Vision-Language-Action (NeuroVLA), a framework that mimics the structural organization of the bio-nervous system between the cortex, cerebellum, and spinal cord. We adopt a system-level bio-inspired design: a high-level model plans goals, an adaptive cerebellum module stabilizes motion using high-frequency sensors feedback, and a bio-inspired spinal layer executes lightning-fast actions generation. NeuroVLA represents the first deployment of a neuromorphic VLA on physical robotics, achieving state-of-the-art performance. We observe the emergence of biological motor characteristics without additional data or special guidance: it stops the shaking in robotic arms, saves significant energy(only 0.4w on Neuromorphic Processor), shows temporal memory ability and triggers safety reflexes in less than 20 milliseconds.", "AI": {"tldr": "本文提出了一种名为 NeuroVLA 的神经形态视觉-语言-动作框架，用于模仿生物神经系统的结构，以实现更高效、更具生物特性的机器人运动控制。该框架在物理机器人上实现了最先进的性能，并在不额外训练的情况下展现了动态稳定性、快速响应和时间记忆等生物运动特性。", "motivation": "当前的机器人策略在动态稳定性、反射性响应和生物运动固有的时间记忆方面存在不足。生物系统能够从稀疏的经验中快速获得技能，这促使研究者探索模仿生物神经系统的结构和功能。", "method": "NeuroVLA 框架模仿了大脑皮层、小脑和脊髓之间的生物神经系统结构。它采用系统级的仿生设计：一个高级模型规划目标，一个自适应的小脑模块利用高频传感器反馈稳定运动，一个仿生脊髓层执行闪电般快速的动作生成。", "result": "NeuroVLA 是首个在物理机器人上部署的神经形态 VLA 系统，实现了最先进的性能。该系统能够停止机械臂的抖动，显著节省能量（仅在神经形态处理器上消耗 0.4 瓦），展现出时间记忆能力，并在不到 20 毫秒的时间内触发安全反射，这些生物运动特性是在没有额外数据或特殊指导的情况下出现的。", "conclusion": "NeuroVLA 框架成功地模仿了生物神经系统的结构，实现了在物理机器人上更高效、更具生物特性的运动控制。该框架在不额外训练的情况下展现了动态稳定性、快速响应和时间记忆等关键的生物运动能力，为未来更先进的机器人智能提供了新的途径。"}}
{"id": "2601.14438", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.14438", "abs": "https://arxiv.org/abs/2601.14438", "authors": ["Danial Sadrian Zadeh", "Otman A. Basir", "Behzad Moshiri"], "title": "Vision-Based Natural Language Scene Understanding for Autonomous Driving: An Extended Dataset and a New Model for Traffic Scene Description Generation", "comment": "Under review at Computer Vision and Image Understanding (submitted July 25, 2025)", "summary": "Traffic scene understanding is essential for enabling autonomous vehicles to accurately perceive and interpret their environment, thereby ensuring safe navigation. This paper presents a novel framework that transforms a single frontal-view camera image into a concise natural language description, effectively capturing spatial layouts, semantic relationships, and driving-relevant cues. The proposed model leverages a hybrid attention mechanism to enhance spatial and semantic feature extraction and integrates these features to generate contextually rich and detailed scene descriptions. To address the limited availability of specialized datasets in this domain, a new dataset derived from the BDD100K dataset has been developed, with comprehensive guidelines provided for its construction. Furthermore, the study offers an in-depth discussion of relevant evaluation metrics, identifying the most appropriate measures for this task. Extensive quantitative evaluations using metrics such as CIDEr and SPICE, complemented by human judgment assessments, demonstrate that the proposed model achieves strong performance and effectively fulfills its intended objectives on the newly developed dataset.", "AI": {"tldr": "提出了一种将单张前视摄像头图像转换为自然语言描述的新框架，用于自动驾驶场景理解，该框架结合了混合注意力机制和新数据集，并在评估指标上表现出色。", "motivation": "自动驾驶汽车需要准确感知和理解其环境以确保安全导航，但现有的专门数据集有限，需要一种方法来从单张图像生成详细的场景描述。", "method": "提出了一种混合注意力机制模型，用于提取空间和语义特征，并将其融合以生成场景描述。同时，开发了一个基于BDD100K的新数据集，并讨论了相关的评估指标。", "result": "在所开发的新数据集上，使用CIDEr和SPICE等指标以及人类评估进行的定量评估表明，所提出的模型取得了强大的性能。", "conclusion": "所提出的新框架能够有效地将单张前视摄像头图像转换为自然语言描述，捕捉了空间布局、语义关系和驾驶相关线索，为自动驾驶场景理解提供了有效解决方案。"}}
{"id": "2601.14523", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.14523", "abs": "https://arxiv.org/abs/2601.14523", "authors": ["Leyi Zhao", "Weijie Huang", "Yitong Guo", "Jiang Bian", "Chenghong Wang", "Xuhong Zhang"], "title": "Large Language Model-Powered Evolutionary Code Optimization on a Phylogenetic Tree", "comment": null, "summary": "Optimizing scientific computing algorithms for modern GPUs is a labor-intensive and iterative process involving repeated code modification, benchmarking, and tuning across complex hardware and software stacks. Recent work has explored large language model (LLM)-assisted evolutionary methods for automated code optimization, but these approaches primarily rely on outcome-based selection and random mutation, underutilizing the rich trajectory information generated during iterative optimization. We propose PhyloEvolve, an LLM-agent system that reframes GPU-oriented algorithm optimization as an In-Context Reinforcement Learning (ICRL) problem. This formulation enables trajectory-conditioned reuse of optimization experience without model retraining. PhyloEvolve integrates Algorithm Distillation and prompt-based Decision Transformers into an iterative workflow, treating sequences of algorithm modifications and performance feedback as first-class learning signals. To organize optimization history, we introduce a phylogenetic tree representation that captures inheritance, divergence, and recombination among algorithm variants, enabling backtracking, cross-lineage transfer, and reproducibility. The system combines elite trajectory pooling, multi-island parallel exploration, and containerized execution to balance exploration and exploitation across heterogeneous hardware. We evaluate PhyloEvolve on scientific computing workloads including PDE solvers, manifold learning, and spectral graph algorithms, demonstrating consistent improvements in runtime, memory efficiency, and correctness over baseline and evolutionary methods. Code is published at: https://github.com/annihi1ation/phylo_evolve", "AI": {"tldr": "提出了一种名为 PhyloEvolve 的 LLM 代理系统，将 GPU 科学计算算法优化视为一个在上下文强化学习（ICRL）问题，并引入了用于组织优化历史的系统发生树表示，实现了更有效的优化。", "motivation": "现有的 LLM 辅助代码优化方法主要依赖基于结果的选择和随机变异，未能充分利用迭代优化过程中产生的丰富轨迹信息。", "method": "将 GPU 算法优化重新构建为上下文强化学习（ICRL）问题，集成算法蒸馏和基于提示的决策 Transformer，并引入系统发生树来组织优化历史，实现轨迹条件下的优化经验重用。", "result": "在 PDE 求解器、流形学习和谱图算法等科学计算工作负载上，PhyloEvolve 相比基线和进化方法在运行时、内存效率和正确性方面取得了持续改进。", "conclusion": "PhyloEvolve 通过将优化过程视为 ICRL 问题并利用系统发生树来组织优化历史，能够有效地进行 GPU 科学计算算法优化，并优于现有方法。"}}
{"id": "2601.14304", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.14304", "abs": "https://arxiv.org/abs/2601.14304", "authors": ["Juncheng Wang", "Zhe Hu", "Chao Xu", "Siyue Ren", "Yuxiang Feng", "Yang Liu", "Baigui Sun", "Shujun Wang"], "title": "Guided by the Plan: Enhancing Faithful Autoregressive Text-to-Audio Generation with Guided Decoding", "comment": "Accepted at EACL 2026", "summary": "Autoregressive (AR) models excel at generating temporally coherent audio by producing tokens sequentially, yet they often falter in faithfully following complex textual prompts, especially those describing complex sound events. We uncover a surprising capability in AR audio generators: their early prefix tokens implicitly encode global semantic attributes of the final output, such as event count and sound-object category, revealing a form of implicit planning. Building on this insight, we propose Plan-Critic, a lightweight auxiliary model trained with a Generalized Advantage Estimation (GAE)-inspired objective to predict final instruction-following quality from partial generations. At inference time, Plan-Critic enables guided exploration: it evaluates candidate prefixes early, prunes low-fidelity trajectories, and reallocates computation to high-potential planning seeds. Our Plan-Critic-guided sampling achieves up to a 10-point improvement in CLAP score over the AR baseline-establishing a new state of the art in AR text-to-audio generation-while maintaining computational parity with standard best-of-N decoding. This work bridges the gap between causal generation and global semantic alignment, demonstrating that even strictly autoregressive models can plan ahead.", "AI": {"tldr": "本文提出了一种名为 Plan-Critic 的轻量级辅助模型，用于增强自回归（AR）音频生成模型遵循复杂文本指令的能力。Plan-Critic 通过预测部分生成序列的最终指令遵循质量，并在推理时引导生成过程，从而提高了文本到音频生成的性能。", "motivation": "现有的自回归（AR）音频生成模型在处理复杂的文本指令时存在困难，尤其是在描述复杂声音事件时。研究人员发现 AR 模型在生成早期就会隐式编码全局语义属性，这表明 AR 模型可能具有潜在的规划能力。", "method": "提出 Plan-Critic 模型，采用类似 GAE 的目标进行训练，用于从部分生成序列预测最终的指令遵循质量。在推理时，Plan-Critic 用于指导生成过程，通过评估候选前缀、剪枝低质量路径并重新分配计算资源来优化生成。", "result": "Plan-Critic 引导的采样在 CLAP 分数上比基线 AR 模型提高了高达 10 个点，并在计算成本不变的情况下，在 AR 文本到音频生成方面达到了新的最先进水平。", "conclusion": "该研究表明，即使是严格的自回归模型也能够进行“提前规划”，从而弥合了因果生成与全局语义对齐之间的差距，并显著提高了 AR 文本到音频生成的性能。"}}
{"id": "2601.14652", "categories": ["cs.AI", "cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.14652", "abs": "https://arxiv.org/abs/2601.14652", "authors": ["Zixuan Ke", "Yifei Ming", "Austin Xu", "Ryan Chin", "Xuan-Phi Nguyen", "Prathyusha Jwalapuram", "Semih Yavuz", "Caiming Xiong", "Shafiq Joty"], "title": "MAS-Orchestra: Understanding and Improving Multi-Agent Reasoning Through Holistic Orchestration and Controlled Benchmarks", "comment": "Preprint; Work in Progress", "summary": "While multi-agent systems (MAS) promise elevated intelligence through coordination of agents, current approaches to automatic MAS design under-deliver. Such shortcomings stem from two key factors: (1) methodological complexity - agent orchestration is performed using sequential, code-level execution that limits global system-level holistic reasoning and scales poorly with agent complexity - and (2) efficacy uncertainty - MAS are deployed without understanding if there are tangible benefits compared to single-agent systems (SAS). We propose MAS-Orchestra, a training-time framework that formulates MAS orchestration as a function-calling reinforcement learning problem with holistic orchestration, generating an entire MAS at once. In MAS-Orchestra, complex, goal-oriented sub-agents are abstracted as callable functions, enabling global reasoning over system structure while hiding internal execution details. To rigorously study when and why MAS are beneficial, we introduce MASBENCH, a controlled benchmark that characterizes tasks along five axes: Depth, Horizon, Breadth, Parallel, and Robustness. Our analysis reveals that MAS gains depend critically on task structure, verification protocols, and the capabilities of both orchestrator and sub-agents, rather than holding universally. Guided by these insights, MAS-Orchestra achieves consistent improvements on public benchmarks including mathematical reasoning, multi-hop QA, and search-based QA. Together, MAS-Orchestra and MASBENCH enable better training and understanding of MAS in the pursuit of multi-agent intelligence.", "AI": {"tldr": "本文提出了MAS-Orchestra框架，将多智能体系统（MAS）的编排视为函数调用强化学习问题，并引入了MASBENCH基准来评估MAS的收益。研究表明MAS的优势取决于任务结构、验证协议以及智能体和协调器的能力。", "motivation": "当前多智能体系统（MAS）的设计方法存在方法论复杂和效果不确定的问题，限制了其潜力的发挥。需要更有效的设计方法和对MAS收益的深入理解。", "method": "提出了MAS-Orchestra框架，将MAS编排建模为函数调用强化学习问题，允许整体推理。引入了MASBENCH基准，该基准通过五个维度（深度、视野、广度、并行性、鲁棒性）来表征任务。", "result": "MAS的收益高度依赖于任务结构、验证协议以及协调器和子智能体的能力。MAS-Orchestra在数学推理、多跳问答和基于搜索的问答等公共基准上取得了持续改进。", "conclusion": "MAS-Orchestra和MASBENCH共同促进了MAS的训练和理解，有助于在追求多智能体智能方面取得进展，并证明了MAS的收益并非普遍存在，而是与具体任务和系统配置密切相关。"}}
{"id": "2601.14984", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.14984", "abs": "https://arxiv.org/abs/2601.14984", "authors": ["Jingwei Dong", "André M. H. Teixeira"], "title": "Stealthy bias injection attack detection based on Kullback-Leibler divergence in stochastic linear systems", "comment": "26 pages, 10 figures", "summary": "This paper studies the design of detection observers against stealthy bias injection attacks in stochastic linear systems under Gaussian noise, considering adversaries that exploit noise and inject crafted bias signals into a subset of sensors in a slow and coordinated manner, thereby achieving malicious objectives while remaining stealthy. To address such attacks, we formulate the observer design as a max-min optimization problem to enhance the detectability of worst-case BIAs, which attain a prescribed attack impact with the least detectability evaluated via Kullback-Leibler divergence. To reduce the computational complexity of the derived non-convex design problem, we consider the detectability of worst-case BIAs at three specific time instants: attack onset, one step after attack occurrence, and the steady state. We prove that the Kalman filter is optimal for maximizing the BIA detectability at the attack onset, regardless of the subset of attacked sensors. For the one-step and steady-state cases, the observer design problems are approximated by bi-convex optimization problems, which can be efficiently solved using alternating optimization and alternating direction method of multipliers. Moreover, more tractable linear matrix inequality relaxations are developed. Finally, the effectiveness of the proposed stealth-aware detection framework is demonstrated through an application to a thermal system.", "AI": {"tldr": "本文研究了在存在高斯噪声的随机线性系统中，如何设计检测器以对抗隐蔽的偏置注入攻击。研究者将检测器设计建模为一个最大最小优化问题，以提高最坏情况偏置注入攻击的可检测性。为了降低计算复杂度，研究者考虑了攻击发生后的三个特定时间点（攻击开始、攻击后一步、稳态）的可检测性，并提出了相应的优化方法。", "motivation": "现有研究忽略了攻击者可能利用噪声并以隐蔽方式注入偏置信号，导致攻击目标得以实现但难以检测。因此，需要设计能够检测此类隐蔽偏置注入攻击的检测器。", "method": "通过将检测器设计表述为最大最小优化问题，来最大化最坏情况偏置注入攻击的可检测性（通过 Kullback-Leibler 散度衡量）。为了简化计算，研究者考虑了攻击发生后的三个关键时间点（攻击开始、攻击后一步、稳态），并将后两种情况下的设计问题近似为双凸优化问题，通过交替优化和 ADMM 方法求解，并推导出更易于处理的线性矩阵不等式松弛方法。", "result": "研究证明了卡尔曼滤波器在最大化攻击开始时刻的偏置注入攻击可检测性方面是最优的。对于攻击后一步和稳态情况，提出的双凸近似方法和 LMI 松弛方法能够有效求解设计问题。", "conclusion": "该研究提出了一种隐蔽感知检测框架，能够设计检测器以对抗随机线性系统中的隐蔽偏置注入攻击。通过将问题转化为最大最小优化，并利用近似方法和 LMI 松弛，该框架能够有效地提高攻击的可检测性，并通过热力学系统应用进行了验证。"}}
{"id": "2601.15040", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.15040", "abs": "https://arxiv.org/abs/2601.15040", "authors": ["Maiken Borud Omtveit", "Qian Long", "Valentin Chabaud", "Marte Ruud-Olsen", "Steinar Halsne", "Tor-Christian Ystgaard"], "title": "Electrical Design of a Clean Offshore Heat and Power (CleanOFF) Hub", "comment": null, "summary": "This paper presents an innovative offshore solution where oil & gas platform clusters are powered by a wind farm and a hydrogen hub. The results show a feasible off-grid design as an alternative to conventional electrification solutions. To address the challenges of design and operation of such a system, a power system model of the equipment and control was developed in a power system simulator called Process Power Simulator (PPSim). Power fluctuations in the wind farm are modelled using a state-of-the-art method encompassing turbulence and wakes. Various operation scenarios were used to evaluate the system design and find the right equipment size. An expensive component to over dimension is the battery energy storage system (BESS). The BESS power rating and energy capacity were found by running a combination of scenarios with extreme and natural wind variations, and contingencies. The control strategy and ramp rates of electrolyzers have significant impact on both system performance and design. A ramp rate in the order of seconds as opposed to minutes will decrease the required BESS size by 60-70%. Choosing synchronized control of the electrolyzers can further reduce the BESS size by 15-20%. The simulations also revealed challenges to achieve self-sufficiency of hydrogen and potential design improvements are suggested.", "AI": {"tldr": "该研究提出了一种创新的离岸解决方案，利用风电场和氢能中心为石油和天然气平台集群供电，并进行了可行性分析。", "motivation": "为了寻找比传统电气化方案更优越的离岸能源供应方式，并解决集成可再生能源和氢能技术到离岸平台集群所面临的设计和运营挑战。", "method": "开发了一个包含设备和控制的电力系统模型，并在Process Power Simulator (PPSim) 中进行模拟。风电场的功率波动通过包含湍流和尾流的先进方法进行建模。通过运行多种操作场景，包括极端和自然风力变化以及故障情况，来评估系统设计、确定设备尺寸，特别是电池储能系统（BESS）的功率额定值和能量容量。", "result": "研究表明，离网设计是替代传统电气化方案的可行方案。通过将电解槽的爬坡速率从几分钟缩短到几秒钟，可以将BESS的尺寸减小60-70%。同步控制电解槽可以将BESS尺寸再减小15-20%。仿真还揭示了实现氢气自给自足的挑战。", "conclusion": "基于风电场和氢能中心的离岸平台集群供电方案是可行的。通过优化控制策略（如缩短电解槽爬坡时间、同步控制）和合理设计BESS，可以显著减小系统成本。研究还指出了实现氢气自给自足的潜在改进方向。"}}
{"id": "2601.14417", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.14417", "abs": "https://arxiv.org/abs/2601.14417", "authors": ["Thanathai Lertpetchpun", "Yoonjeong Lee", "Thanapat Trachu", "Jihwan Lee", "Tiantian Feng", "Dani Byrd", "Shrikanth Narayanan"], "title": "Quantifying Speaker Embedding Phonological Rule Interactions in Accented Speech Synthesis", "comment": "Accepted to ICASSP2026", "summary": "Many spoken languages, including English, exhibit wide variation in dialects and accents, making accent control an important capability for flexible text-to-speech (TTS) models. Current TTS systems typically generate accented speech by conditioning on speaker embeddings associated with specific accents. While effective, this approach offers limited interpretability and controllability, as embeddings also encode traits such as timbre and emotion. In this study, we analyze the interaction between speaker embeddings and linguistically motivated phonological rules in accented speech synthesis. Using American and British English as a case study, we implement rules for flapping, rhoticity, and vowel correspondences. We propose the phoneme shift rate (PSR), a novel metric quantifying how strongly embeddings preserve or override rule-based transformations. Experiments show that combining rules with embeddings yields more authentic accents, while embeddings can attenuate or overwrite rules, revealing entanglement between accent and speaker identity. Our findings highlight rules as a lever for accent control and a framework for evaluating disentanglement in speech generation.", "AI": {"tldr": "研究分析了说话人嵌入和语音学规则在口音合成中的相互作用，提出了一种衡量规则保留程度的度量标准PSR，并证明结合规则和嵌入可以生成更真实的口音，同时揭示了口音与说话人身份之间的纠缠。", "motivation": "当前的文本到语音（TTS）系统通过说话人嵌入来控制口音，但这种方法可解释性和可控性有限，因为嵌入也编码了音色和情感等特征。因此，需要一种更灵活、更可解释的口音控制方法。", "method": "以美式英语和英式英语为例，研究实现了用于扑音、卷舌音和元音对应等语音学规则。提出了量化嵌入保留或覆盖规则转换强度的度量标准——音素转换率（PSR）。通过实验评估了规则与嵌入结合的效果，并分析了嵌入对规则的影响。", "result": "实验表明，将语音学规则与说话人嵌入相结合，可以生成更真实的口音。同时，发现说话人嵌入会削弱或覆盖规则，揭示了口音和说话人身份之间的纠缠。", "conclusion": "语音学规则是口音控制的一个有效工具，并且可以作为一个评估语音生成中解缠结的框架。研究强调了规则在生成更可控、更真实的口音方面的潜力，并为理解和解决口音与说话人身份纠缠问题提供了新思路。"}}
{"id": "2601.14448", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14448", "abs": "https://arxiv.org/abs/2601.14448", "authors": ["A. Enes Doruk"], "title": "Gaussian Based Adaptive Multi-Modal 3D Semantic Occupancy Prediction", "comment": "Master Thesis", "summary": "The sparse object detection paradigm shift towards dense 3D semantic occupancy prediction is necessary for dealing with long-tail safety challenges for autonomous vehicles. Nonetheless, the current voxelization methods commonly suffer from excessive computation complexity demands, where the fusion process is brittle, static, and breaks down under dynamic environmental settings. To this end, this research work enhances a novel Gaussian-based adaptive camera-LiDAR multimodal 3D occupancy prediction model that seamlessly bridges the semantic strengths of camera modality with the geometric strengths of LiDAR modality through a memory-efficient 3D Gaussian model. The proposed solution has four key components: (1) LiDAR Depth Feature Aggregation (LDFA), where depth-wise deformable sampling is employed for dealing with geometric sparsity, (2) Entropy-Based Feature Smoothing, where cross-entropy is employed for handling domain-specific noise, (3) Adaptive Camera-LiDAR Fusion, where dynamic recalibration of sensor outputs is performed based on model outputs, and (4) Gauss-Mamba Head that uses Selective State Space Models for global context decoding that enjoys linear computation complexity.", "AI": {"tldr": "提出了一种基于高斯模型的自适应相机-激光雷达多模态3D占用预测模型，以应对自动驾驶中的长尾安全挑战，解决了现有体素化方法计算复杂度高和动态环境适应性差的问题。", "motivation": "现有3D占用预测方法在处理自动驾驶中的长尾安全挑战时，计算复杂度高且在动态环境下表现脆弱。有必要转向更高效、更具适应性的方法。", "method": "该研究提出了一种新的高斯模型，集成了激光雷达深度特征聚合（LDFA）、基于熵的特征平滑、自适应相机-激光雷达融合以及使用选择性状态空间模型的Gauss-Mamba头部。LDFA使用深度可变形采样处理几何稀疏性，熵基平滑处理噪声，自适应融合动态校准传感器输出，Gauss-Mamba头部利用选择性状态空间模型进行全局上下文解码。", "result": "该模型能够有效地融合相机和激光雷达的语义和几何信息，并通过自适应融合和高效的全局上下文解码来处理动态环境。其计算复杂度得到优化，并能应对几何稀疏性和领域特定噪声。", "conclusion": "所提出的高斯模型为3D占用预测提供了一个内存高效且适应性强的解决方案，能够有效弥合相机和激光雷达的优势，特别是在应对自动驾驶中的复杂和动态场景方面具有显著优势。"}}
{"id": "2601.14475", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14475", "abs": "https://arxiv.org/abs/2601.14475", "authors": ["Yajvan Ravan", "Aref Malek", "Chester Dolph", "Nikhil Behari"], "title": "Real-Time Wildfire Localization on the NASA Autonomous Modular Sensor using Deep Learning", "comment": "16 pages, 9 figures, published at AIAA SciTech 2026", "summary": "High-altitude, multi-spectral, aerial imagery is scarce and expensive to acquire, yet it is necessary for algorithmic advances and application of machine learning models to high-impact problems such as wildfire detection. We introduce a human-annotated dataset from the NASA Autonomous Modular Sensor (AMS) using 12-channel, medium to high altitude (3 - 50 km) aerial wildfire images similar to those used in current US wildfire missions. Our dataset combines spectral data from 12 different channels, including infrared (IR), short-wave IR (SWIR), and thermal. We take imagery from 20 wildfire missions and randomly sample small patches to generate over 4000 images with high variability, including occlusions by smoke/clouds, easily-confused false positives, and nighttime imagery.\n  We demonstrate results from a deep-learning model to automate the human-intensive process of fire perimeter determination. We train two deep neural networks, one for image classification and the other for pixel-level segmentation. The networks are combined into a unique real-time segmentation model to efficiently localize active wildfire on an incoming image feed. Our model achieves 96% classification accuracy, 74% Intersection-over-Union(IoU), and 84% recall surpassing past methods, including models trained on satellite data and classical color-rule algorithms. By leveraging a multi-spectral dataset, our model is able to detect active wildfire at nighttime and behind clouds, while distinguishing between false positives. We find that data from the SWIR, IR, and thermal bands is the most important to distinguish fire perimeters. Our code and dataset can be found here: https://github.com/nasa/Autonomous-Modular-Sensor-Wildfire-Segmentation/tree/main and https://drive.google.com/drive/folders/1-u4vs9rqwkwgdeeeoUhftCxrfe_4QPTn?=usp=drive_link", "AI": {"tldr": "该研究提出了一个包含12个光谱通道的高海拔航空航拍野火图像数据集，并利用深度学习模型实现了自动化野火区域识别，在分类准确率、IoU和召回率方面均优于现有方法。", "motivation": "高海拔、多光谱航空影像数据稀缺且昂贵，但对于机器学习在野火检测等高影响力问题上的算法进步和应用至关重要。", "method": "构建了一个包含20次野火任务、4000多张图像的数据集，利用12个光谱通道（包括红外、短波红外和热成像），并引入了图像分类和像素级分割两个深度神经网络，组合成一个实时分割模型。", "result": "所提出的深度学习模型在自动化野火区域确定方面，取得了96%的分类准确率、74%的IoU和84%的召回率，优于包括卫星数据和传统算法在内的过往方法。模型能够实现夜间和穿透云层的野火检测，并有效区分假阳性。研究发现SWIR、IR和热成像波段对区分火灾周界最为重要。", "conclusion": "利用多光谱数据集和深度学习模型，可以有效地自动化野火区域的识别过程，克服了传统方法的局限性，并且SWIR、IR和热成像波段在野火检测中起关键作用。"}}
{"id": "2601.14634", "categories": ["cs.RO", "physics.bio-ph"], "pdf": "https://arxiv.org/pdf/2601.14634", "abs": "https://arxiv.org/abs/2601.14634", "authors": ["Satoru Hashimoto", "Yinlai Jiang", "Hiroshi Yokoi", "Shunta Togo"], "title": "Landing-Induced Viscoelastic Changes in an Anthropomimetic Foot Joint Structure are Modulated by Foot Structure and Posture", "comment": "27 pages, preprint", "summary": "Cadaveric studies have provided important insights into the mechanics of the human foot arch and plantar fascia. However, repeatedly probing posture-dependent viscoelastic responses immediately after landing impact is difficult in biological specimens, leaving the contribution of skeletal architecture to landing dynamics incompletely understood. In this study, we developed an anthropomimetic foot joint structure aimed at replicating the skeletal geometry of the human foot. Using a vertical drop apparatus that simulates landing and a viscoelastic system-identification model, we investigated how skeletal structure and posture modulate the apparent post-impact viscoelastic response. The results show that the multi-jointed anthropomimetic structure exhibited a higher damping ratio than simplified flat and rigid feet. Moreover, ankle dorsiflexion and toe extension systematically shifted the identified parameters, reducing the damping ratio under the tested conditions. Taken together, these findings indicate that an arch-like, multi-jointed skeletal architecture can enhance impact attenuation in an anthropomimetic mechanical foot, and that morphology and passive posture alone can tune the trade-off between attenuation and rebound. The observed posture-dependent trends are qualitatively consistent with reported differences in human landing strategies, suggesting that skeletal architecture may partly account for the modulation. Furthermore, these results highlight the engineering advantage of anatomically informed skeletal replication for achieving human-like apparent viscoelastic behavior through postural adjustment during landing.", "AI": {"tldr": "通过构建一个仿人足关节结构，研究表明具有足弓等多关节的骨骼结构能提高着陆时的冲击吸收能力，并且踝关节和足趾的姿势可以调节这种吸收和反弹的平衡。", "motivation": "现有尸体研究难以准确模拟着陆冲击后足弓和跖筋膜在不同姿势下的粘弹性反应，导致骨骼结构对动态着陆过程的贡献理解不充分。", "method": "开发了一个仿人足关节结构，使用垂直跌落装置模拟着陆，并采用粘弹性系统辨识模型，研究骨骼结构和姿势如何影响着陆后的表观粘弹性反应。", "result": "仿人足关节结构比简化模型（平坦、刚性足）具有更高的阻尼比。踝关节背屈和足趾伸展会系统性地改变辨识参数，降低阻尼比。", "conclusion": "类足弓、多关节骨骼结构可以增强仿人机械足的冲击吸收能力，并且形态和被动姿势可以单独调节吸收和反弹之间的权衡。姿势依赖性趋势与人类着陆策略的差异一致，表明骨骼结构可能在其中起到部分作用。"}}
{"id": "2601.14649", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.14649", "abs": "https://arxiv.org/abs/2601.14649", "authors": ["Ping Zhong", "Liangbai Liu", "Bolei Chen", "Tao Wu", "Jiazhi Xia", "Chaoxu Mu", "Jianxin Wang"], "title": "Spatially Generalizable Mobile Manipulation via Adaptive Experience Selection and Dynamic Imagination", "comment": null, "summary": "Mobile Manipulation (MM) involves long-horizon decision-making over multi-stage compositions of heterogeneous skills, such as navigation and picking up objects. Despite recent progress, existing MM methods still face two key limitations: (i) low sample efficiency, due to ineffective use of redundant data generated during long-term MM interactions; and (ii) poor spatial generalization, as policies trained on specific tasks struggle to transfer to new spatial layouts without additional training. In this paper, we address these challenges through Adaptive Experience Selection (AES) and model-based dynamic imagination. In particular, AES makes MM agents pay more attention to critical experience fragments in long trajectories that affect task success, improving skill chain learning and mitigating skill forgetting. Based on AES, a Recurrent State-Space Model (RSSM) is introduced for Model-Predictive Forward Planning (MPFP) by capturing the coupled dynamics between the mobile base and the manipulator and imagining the dynamics of future manipulations. RSSM-based MPFP can reinforce MM skill learning on the current task while enabling effective generalization to new spatial layouts. Comparative studies across different experimental configurations demonstrate that our method significantly outperforms existing MM policies. Real-world experiments further validate the feasibility and practicality of our method.", "AI": {"tldr": "本文提出了一种名为自适应经验选择（AES）和基于模型的动态想象的方法，以提高移动操作（MM）任务的样本效率和空间泛化能力。AES 优先处理对任务成功至关重要的经验片段，而基于 RSSM 的动态想象则捕捉移动基座和机械臂之间的耦合动力学，并对未来操作进行预测。实验结果表明，该方法在样本效率和泛化能力上均优于现有方法。", "motivation": "现有移动操作方法在样本效率和空间泛化能力方面存在不足，具体表现在对长轨迹中冗余数据的使用效率低下，以及训练好的策略难以泛化到新的空间布局。", "method": "1. 自适应经验选择（AES）：使 MM 智能体更关注长轨迹中影响任务成功的关键经验片段，从而提高技能链学习能力并缓解技能遗忘。\n2. 基于模型的动态想象：引入递归状态空间模型（RSSM）进行模型预测前向规划（MPFP），捕捉移动基座和机械臂之间的耦合动力学，并想象未来操作的动力学。", "result": "与现有 MM 策略相比，所提方法在不同实验配置下表现出显著的性能提升。真实世界实验也验证了该方法的可用性和实用性。", "conclusion": "本文提出的 AES 和基于 RSSM 的动态想象方法有效地解决了移动操作中的样本效率和空间泛化问题，提高了技能学习和泛化能力，并在模拟和真实世界实验中得到了验证。"}}
{"id": "2601.14662", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.14662", "abs": "https://arxiv.org/abs/2601.14662", "authors": ["Shuhua Yang", "Jiahao Zhang", "Yilong Wang", "Dongwon Lee", "Suhang Wang"], "title": "Query-Efficient Agentic Graph Extraction Attacks on GraphRAG Systems", "comment": null, "summary": "Graph-based retrieval-augmented generation (GraphRAG) systems construct knowledge graphs over document collections to support multi-hop reasoning. While prior work shows that GraphRAG responses may leak retrieved subgraphs, the feasibility of query-efficient reconstruction of the hidden graph structure remains unexplored under realistic query budgets. We study a budget-constrained black-box setting where an adversary adaptively queries the system to steal its latent entity-relation graph. We propose AGEA (Agentic Graph Extraction Attack), a framework that leverages a novelty-guided exploration-exploitation strategy, external graph memory modules, and a two-stage graph extraction pipeline combining lightweight discovery with LLM-based filtering. We evaluate AGEA on medical, agriculture, and literary datasets across Microsoft-GraphRAG and LightRAG systems. Under identical query budgets, AGEA significantly outperforms prior attack baselines, recovering up to 90% of entities and relationships while maintaining high precision. These results demonstrate that modern GraphRAG systems are highly vulnerable to structured, agentic extraction attacks, even under strict query limits.", "AI": {"tldr": "研究提出了一种名为AGEA的框架，能够在查询预算受限的情况下，从基于图的检索增强生成（GraphRAG）系统中高效地提取其隐藏的知识图谱结构，揭示了GraphRAG系统在安全方面的潜在风险。", "motivation": "现有研究表明GraphRAG系统可能会泄露检索到的子图，但尚未探索在现实的查询预算限制下，攻击者能否高效地重建隐藏的图结构。本研究旨在填补这一空白，评估GraphRAG系统在受限查询预算下的结构重建漏洞。", "method": "提出AGEA（Agentic Graph Extraction Attack）框架，该框架采用新颖性驱动的探索-利用策略，结合外部图记忆模块，并通过一个两阶段的图提取流程（轻量级发现与LLM过滤）来实现。", "result": "在医疗、农业和文学数据集上，针对Microsoft-GraphRAG和LightRAG系统进行的评估显示，AGEA在相同的查询预算下，显著优于现有攻击基线，能够恢复高达90%的实体和关系，同时保持高精度。", "conclusion": "现代GraphRAG系统在严格的查询预算限制下，对结构化、代理式的提取攻击高度敏感，存在被窃取潜在实体-关系图的风险。"}}
{"id": "2601.15099", "categories": ["eess.SY", "eess.SP"], "pdf": "https://arxiv.org/pdf/2601.15099", "abs": "https://arxiv.org/abs/2601.15099", "authors": ["A. Vaca", "J. Gutierrez Florensa", "F. Milano"], "title": "Instantaneous Frequency in Power Systems using the Teager-Kaiser Energy Operator", "comment": null, "summary": "This paper develops an instantaneous-frequency (IF) local estimator calculated with the complex Teager-Kaiser energy operator (CTKEO) and the dynamic-signal identity. The contribution is a novel IF expression that makes the envelope-curvature terms explicit, thus correcting the bias that affects conventional estimators used in power systems. The estimator aligns with complex-frequency (CF) kinematics and admits a geometric interpretation (curvature/torsion) without phase unwrapping. Simulations and data-driven examples demonstrate the accuracy of the proposed approach.", "AI": {"tldr": "提出一种基于CTKEO和动态信号恒等式的瞬时频率（IF）局部估计器，明确了包络曲率项，纠正了传统估计器的偏差，并具有几何解释，无需相位解卷绕。", "motivation": "现有用于电力系统的瞬时频率（IF）估计器存在偏差，需要一种更精确的估计方法。", "method": "利用复数Teager-Kaiser能量算子（CTKEO）和动态信号恒等式，推导出新的IF表达式，明确包络曲率项，并赋予几何解释（曲率/扭率），无需相位解卷绕。", "result": "模拟和数据驱动的示例表明，所提出的IF局部估计器比传统方法更准确。", "conclusion": "该新型IF估计器能够准确地估计瞬时频率，并且通过明确包络曲率项纠正了传统方法的偏差，为电力系统分析提供了更好的工具。"}}
{"id": "2601.14681", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.14681", "abs": "https://arxiv.org/abs/2601.14681", "authors": ["Shuhao Liao", "Xuxin Lv", "Jeric Lew", "Shizhe Zhang", "Jingsong Liang", "Peizhuo Li", "Yuhong Cao", "Wenjun Wu", "Guillaume Sartoretti"], "title": "FARE: Fast-Slow Agentic Robotic Exploration", "comment": null, "summary": "This work advances autonomous robot exploration by integrating agent-level semantic reasoning with fast local control. We introduce FARE, a hierarchical autonomous exploration framework that integrates a large language model (LLM) for global reasoning with a reinforcement learning (RL) policy for local decision making. FARE follows a fast-slow thinking paradigm. The slow-thinking LLM module interprets a concise textual description of the unknown environment and synthesizes an agent-level exploration strategy, which is then grounded into a sequence of global waypoints through a topological graph. To further improve reasoning efficiency, this module employs a modularity-based pruning mechanism that reduces redundant graph structures. The fast-thinking RL module executes exploration by reacting to local observations while being guided by the LLM-generated global waypoints. The RL policy is additionally shaped by a reward term that encourages adherence to the global waypoints, enabling coherent and robust closed-loop behavior. This architecture decouples semantic reasoning from geometric decision, allowing each module to operate in its appropriate temporal and spatial scale. In challenging simulated environments, our results show that FARE achieves substantial improvements in exploration efficiency over state-of-the-art baselines. We further deploy FARE on hardware and validate it in complex, large scale $200m\\times130m$ building environment.", "AI": {"tldr": "提出了一种名为FARE的自主机器人探索框架，它结合了大型语言模型（LLM）进行全局语义推理和强化学习（RL）进行局部控制，以提高探索效率。", "motivation": "为了提高自主机器人探索的效率，特别是通过整合高级的语义理解能力和快速的反应式局部控制。", "method": "FARE采用分层架构，利用LLM解释环境的文本描述并生成全局探索策略，该策略被转换为拓扑图中的全局航点。LLM模块还包含一种基于模块化的剪枝机制以提高效率。RL模块则根据局部观测和LLM生成的航点进行探索，并通过奖励项鼓励遵循航点，从而实现闭环控制。", "result": "在模拟和真实机器人硬件的复杂环境中，FARE在探索效率上显著优于最先进的基线方法。", "conclusion": "FARE框架通过结合LLM的全局语义推理和RL的局部控制，成功地提升了自主机器人探索的效率和鲁棒性，实现了语义推理与几何决策的有效分离。"}}
{"id": "2601.14478", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.14478", "abs": "https://arxiv.org/abs/2601.14478", "authors": ["Sasha Ronaghi", "Emma-Louise Aveling", "Maria Levis", "Rachel Lauren Ross", "Emily Alsentzer", "Sara Singer"], "title": "Large Language Models for Large-Scale, Rigorous Qualitative Analysis in Applied Health Services Research", "comment": "20 pages, 6 figures", "summary": "Large language models (LLMs) show promise for improving the efficiency of qualitative analysis in large, multi-site health-services research. Yet methodological guidance for LLM integration into qualitative analysis and evidence of their impact on real-world research methods and outcomes remain limited. We developed a model- and task-agnostic framework for designing human-LLM qualitative analysis methods to support diverse analytic aims. Within a multi-site study of diabetes care at Federally Qualified Health Centers (FQHCs), we leveraged the framework to implement human-LLM methods for (1) qualitative synthesis of researcher-generated summaries to produce comparative feedback reports and (2) deductive coding of 167 interview transcripts to refine a practice-transformation intervention. LLM assistance enabled timely feedback to practitioners and the incorporation of large-scale qualitative data to inform theory and practice changes. This work demonstrates how LLMs can be integrated into applied health-services research to enhance efficiency while preserving rigor, offering guidance for continued innovation with LLMs in qualitative research.", "AI": {"tldr": "本研究提出了一个不依赖于特定模型和任务的框架，用于在健康服务研究中整合大型语言模型（LLM）以辅助定性分析，并在一项关于联邦合格健康中心（FQHCs）糖尿病护理的多地点研究中进行了实践。研究证明了LLM能够提高定性综合和演绎编码的效率，同时保持严谨性。", "motivation": "尽管大型语言模型（LLM）在提高定性分析效率方面具有潜力，但目前缺乏将其整合到实际研究方法中的指导以及关于其影响的证据。", "method": "研究者开发了一个模型和任务无关的框架，用于设计人类-LLM协同的定性分析方法。在一项关于FQHCs糖尿病护理的多地点研究中，该框架被用于：（1）对研究者生成的摘要进行定性综合，生成比较反馈报告；（2）对167份访谈记录进行演绎编码，以优化干预措施。", "result": "LLM辅助能够及时向从业者提供反馈，并能将大规模定性数据纳入理论和实践的改进中。", "conclusion": "本研究展示了LLM如何整合到应用型健康服务研究中，以提高效率并保持严谨性，并为LLM在定性研究中的创新提供了指导。"}}
{"id": "2601.14809", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.14809", "abs": "https://arxiv.org/abs/2601.14809", "authors": ["Muhammad Adel Yusuf", "Ali Nasir", "Zeeshan Hameed Khan"], "title": "Stochastic Decision-Making Framework for Human-Robot Collaboration in Industrial Applications", "comment": "Under Review by IEEE Transactions on Human Machine Systems", "summary": "Collaborative robots, or cobots, are increasingly integrated into various industrial and service settings to work efficiently and safely alongside humans. However, for effective human-robot collaboration, robots must reason based on human factors such as motivation level and aggression level. This paper proposes an approach for decision-making in human-robot collaborative (HRC) environments utilizing stochastic modeling. By leveraging probabilistic models and control strategies, the proposed method aims to anticipate human actions and emotions, enabling cobots to adapt their behavior accordingly. So far, most of the research has been done to detect the intentions of human co-workers. This paper discusses the theoretical framework, implementation strategies, simulation results, and potential applications of the bilateral collaboration approach for safety and efficiency in collaborative robotics.", "AI": {"tldr": "该论文提出一种利用随机建模的方法，使协作机器人在与人类协同工作时，能够根据人类的动机和攻击性等因素做出决策，以提高安全性和效率。", "motivation": "为了实现有效的人机协作，机器人需要考虑人类因素（如动机和攻击性水平），目前的研究主要集中在检测人类意图，但对更深层次的人类状态的推理不足。", "method": "利用随机建模（概率模型和控制策略）来预测人类的行为和情绪，从而使协作机器人能够调整其行为。", "result": "论文讨论了该双边协作方法的理论框架、实施策略、模拟结果和潜在应用。", "conclusion": "通过考虑人类的动机和攻击性水平，并利用随机建模进行预测和适应，可以显著提高人机协作的安全性和效率。"}}
{"id": "2601.14490", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.14490", "abs": "https://arxiv.org/abs/2601.14490", "authors": ["Hunter Heidenreich", "Ben Elliott", "Olivia Dinica", "Yosheb Getachew"], "title": "GutenOCR: A Grounded Vision-Language Front-End for Documents", "comment": null, "summary": "GutenOCR is a family of grounded OCR front-ends obtained by fine-tuning Qwen2.5-VL-3B and Qwen2.5-VL-7B. The resulting single-checkpoint vision-language models expose reading, detection, and grounding through a unified, prompt-based interface. Trained on business documents, scientific articles, and synthetic grounding data, the models support full-page and localized reading with line- and paragraph-level bounding boxes and conditional ``where is x?'' queries. We introduce a grounded OCR evaluation protocol and show that GutenOCR-7B more than doubles the composite grounded OCR score of its Qwen2.5-VL-7B backbone on 10.5K held-out business and scientific pages (0.40 to 0.82). On Fox and OmniDocBench v1.5, our approach substantially improves region- and line-level OCR as well as text-detection recall, but reveals trade-offs in page-level linearization, color-guided OCR, and formula-heavy layouts.", "AI": {"tldr": "GutenOCR是通过微调Qwen2.5-VL模型得到的、支持阅读、检测和定位的统一OCR前端。它在多种文档上进行了训练，并通过新的评估协议展示了其在文本检测和区域/行级OCR方面的显著提升，但也存在某些布局方面的权衡。", "motivation": "为了开发一个统一的、基于提示的OCR解决方案，能够同时处理文本阅读、检测和定位，并提升在特定文档类型上的性能。", "method": "通过在商业文档、科学论文和合成定位数据上对Qwen2.5-VL-3B和Qwen2.5-VL-7B模型进行微调，构建GutenOCR系列模型。模型采用统一的、基于提示的接口，支持全页和局部阅读，并提供边界框和条件查询功能。引入了一个新的地面OCR评估协议。", "result": "GutenOCR-7B模型在10.5K的商业和科学文档上，综合地面OCR得分从0.40提升至0.82，翻倍以上。在Fox和OmniDocBench v1.5基准测试中，显著提高了区域级和行级OCR以及文本检测召回率。", "conclusion": "GutenOCR通过微调Vision-Language模型，实现了强大的地面OCR能力，并在多个任务和基准上取得了显著改进。然而，在页面级线性化、颜色引导OCR和公式密集布局方面，仍存在性能权衡。"}}
{"id": "2601.15196", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.15196", "abs": "https://arxiv.org/abs/2601.15196", "authors": ["Jianye Xu", "Bassam Alrifaee"], "title": "TTCBF: A Truncated Taylor Control Barrier Function for High-Order Safety Constraints", "comment": null, "summary": "Control Barrier Functions (CBFs) enforce safety by rendering a prescribed safe set forward invariant. However, standard CBFs are limited to safety constraints with relative degree one, while High-Order CBF (HOCBF) methods address higher relative degree at the cost of introducing a chain of auxiliary functions and multiple class K functions whose tuning scales with the relative degree. In this paper, we introduce a Truncated Taylor Control Barrier Function (TTCBF), which generalizes standard discrete-time CBFs to consider high-order safety constraints and requires only one class K function, independent of the relative degree. We also propose an adaptive variant, adaptive TTCBF (aTTCBF), that optimizes an online gain on the class K function to improve adaptability, while requiring fewer control design parameters than existing adaptive HOCBF variants. Numerical experiments in a relative-degree-six spring-mass system and a cluttered corridor navigation validate the above theoretical findings.", "AI": {"tldr": "提出了一种截断泰勒控制障碍函数 (TTCBF)，它将离散时间 CBF 推广到高阶安全约束，并且只需要一个与相对阶数无关的 K 类函数。还提出了一种自适应变体 (aTTCBF)，用于优化 K 类函数的在线增益。", "motivation": "标准的 CBF 仅限于相对阶数为一的安全约束，而高阶 CBF (HOCBF) 方法虽然能处理高阶约束，但需要引入一系列辅助函数和多个 K 类函数，并且参数调整的复杂性随相对阶数增加而增加。", "method": "提出了一种截断泰勒控制障碍函数 (TTCBF)，它利用截断泰勒展开来处理高阶安全约束，并设计了一种自适应变体 (aTTCBF)，通过在线优化 K 类函数的增益来提高适应性。", "result": "TTCBF 能够处理高阶安全约束，并且所需的 K 类函数数量与相对阶数无关。aTTCBF 在保持较少设计参数的同时，提高了系统的适应性。", "conclusion": "TTCBF 和 aTTCBF 是处理高阶安全约束的有效方法，它们在简化设计和提高适应性方面优于现有方法，并且在弹簧-质量系统和杂乱走廊导航的数值实验中得到了验证。"}}
{"id": "2601.14683", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14683", "abs": "https://arxiv.org/abs/2601.14683", "authors": ["Aisvarya Adeseye", "Jouni Isoaho", "Seppo Virtanen", "Mohammad Tahir"], "title": "Local Language Models for Context-Aware Adaptive Anonymization of Sensitive Text", "comment": "Accepted and Waiting to be Published. ICAI'25: 27th International Conference on Artificial Intelligence https://american-cse.org/csce2025/conferences-ICAI", "summary": "Qualitative research often contains personal, contextual, and organizational details that pose privacy risks if not handled appropriately. Manual anonymization is time-consuming, inconsistent, and frequently omits critical identifiers. Existing automated tools tend to rely on pattern matching or fixed rules, which fail to capture context and may alter the meaning of the data. This study uses local LLMs to build a reliable, repeatable, and context-aware anonymization process for detecting and anonymizing sensitive data in qualitative transcripts. We introduce a Structured Framework for Adaptive Anonymizer (SFAA) that includes three steps: detection, classification, and adaptive anonymization. The SFAA incorporates four anonymization strategies: rule-based substitution, context-aware rewriting, generalization, and suppression. These strategies are applied based on the identifier type and the risk level. The identifiers handled by the SFAA are guided by major international privacy and research ethics standards, including the GDPR, HIPAA, and OECD guidelines. This study followed a dual-method evaluation that combined manual and LLM-assisted processing. Two case studies were used to support the evaluation. The first includes 82 face-to-face interviews on gamification in organizations. The second involves 93 machine-led interviews using an AI-powered interviewer to test LLM awareness and workplace privacy. Two local models, LLaMA and Phi were used to evaluate the performance of the proposed framework. The results indicate that the LLMs found more sensitive data than a human reviewer. Phi outperformed LLaMA in finding sensitive data, but made slightly more errors. Phi was able to find over 91% of the sensitive data and 94.8% kept the same sentiment as the original text, which means it was very accurate, hence, it does not affect the analysis of the qualitative data.", "AI": {"tldr": "本研究提出了一种名为SFAA的结构化自适应匿名化框架，利用本地大型语言模型（LLMs）来自动匿名化定性研究中的敏感数据，并在两个案例研究中证明了其有效性，优于人工审核。LLaMA和Phi模型被用于评估，Phi模型在检测敏感数据方面表现最佳，同时保持了原始文本的情感。", "motivation": "定性研究中的个人、情境和组织细节存在隐私风险，手动匿名化耗时且不一致，现有自动化工具缺乏对上下文的理解。因此，需要一种更可靠、可重复且能感知上下文的自动化匿名化方法。", "method": "研究提出了结构化自适应匿名化框架（SFAA），包含检测、分类和自适应匿名化三个步骤。SFAA结合了基于规则的替换、上下文感知重写、泛化和抑制四种匿名化策略，并根据标识符类型和风险级别应用。使用了LLaMA和Phi两个本地LLM模型，并结合人工和LLM辅助处理进行了双重方法评估。", "result": "LLMs比人工审阅者发现了更多的敏感数据。Phi模型在发现敏感数据方面优于LLaMA，但错误率略高。Phi模型找到了超过91%的敏感数据，并且94.8%的匿名化处理保持了原始文本的情感。", "conclusion": "基于本地LLMs的SFAA框架能够有效地检测和匿名化定性研究中的敏感数据，并且在保持数据分析完整性方面表现出色，优于人工方法。"}}
{"id": "2601.15135", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2601.15135", "abs": "https://arxiv.org/abs/2601.15135", "authors": ["Natanon Tongamrak", "Kannapha Amaruchkul", "Wijarn Wangdee", "Jitkomut Songsiri"], "title": "Stochastic EMS for Optimal 24/7 Carbon-Free Energy Operations", "comment": "23 pages", "summary": "This paper proposes a two-stage stochastic optimization formulation to determine optimal operation and procurement plans for achieving a 24/7 carbon-free energy (CFE) compliance at minimized cost. The system in consideration follows primary energy technologies in Thailand including solar power, battery storage, and a diverse portfolio of renewable and carbon-based energy procurement sources. Unlike existing literature focused on long-term planning, this study addresses near real-time operations using a 15-minute resolution. A novel feature of the formulation is the explicit treatment of CFE compliance as a model parameter, enabling flexible targets such as a minimum percentage of hourly matching or a required number of carbon-free days within a multi-day horizon. The mixed-integer linear programming formulation accounts for uncertainties in load and solar generation by integrating deep learning-based forecasting within a receding horizon framework. By optimizing battery profiles and multi-source procurement simultaneously, the proposed system provides a feasible pathway for transitioning to carbon-free operations in emerging energy markets.", "AI": {"tldr": "本文提出了一个两阶段随机优化模型，用于制定泰国太阳能、电池储能和多种能源采购的24/7碳中和能源（CFE）运行和采购计划，以最小化成本，并考虑了负荷和太阳能发电的不确定性。", "motivation": "现有文献多关注长期规划，本研究旨在解决近实时运行问题，并为新兴能源市场提供向碳中和过渡的可行途径。", "method": "采用两阶段随机优化模型，结合深度学习预测，在15分钟分辨率下，通过混合整数线性规划，同时优化电池储能配置和多源采购，并明确将CFE合规性作为模型参数。", "result": "该模型能够同时优化电池储能配置和多能源采购，提供灵活的CFE合规性目标（如最小小时匹配百分比或多日内的碳中和天数），并有效处理负荷和太阳能发电的不确定性。", "conclusion": "该研究提出的两阶段随机优化框架能够有效地制定24/7碳中和能源的运行和采购计划，为泰国等新兴能源市场提供了一个可行的碳中和转型路径。"}}
{"id": "2601.14530", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14530", "abs": "https://arxiv.org/abs/2601.14530", "authors": ["Xiaoyan Kui", "Zijie Fan", "Zexin Ji", "Qinsong Li", "Hao Xu", "Weixin Si", "Haodong Xu", "Beiji Zou"], "title": "PAS-Mamba: Phase-Amplitude-Spatial State Space Model for MRI Reconstruction", "comment": null, "summary": "Joint feature modeling in both the spatial and frequency domains has become a mainstream approach in MRI reconstruction. However, existing methods generally treat the frequency domain as a whole, neglecting the differences in the information carried by its internal components. According to Fourier transform theory, phase and amplitude represent different types of information in the image. Our spectrum swapping experiments show that magnitude mainly reflects pixel-level intensity, while phase predominantly governs image structure. To prevent interference between phase and magnitude feature learning caused by unified frequency-domain modeling, we propose the Phase-Amplitude-Spatial State Space Model (PAS-Mamba) for MRI Reconstruction, a framework that decouples phase and magnitude modeling in the frequency domain and combines it with image-domain features for better reconstruction. In the image domain, LocalMamba preserves spatial locality to sharpen fine anatomical details. In frequency domain, we disentangle amplitude and phase into two specialized branches to avoid representational coupling. To respect the concentric geometry of frequency information, we propose Circular Frequency Domain Scanning (CFDS) to serialize features from low to high frequencies. Finally, a Dual-Domain Complementary Fusion Module (DDCFM) adaptively fuses amplitude phase representations and enables bidirectional exchange between frequency and image domains, delivering superior reconstruction. Extensive experiments on the IXI and fastMRI knee datasets show that PAS-Mamba consistently outperforms state of the art reconstruction methods.", "AI": {"tldr": "提出了一种名为PAS-Mamba的MRI重建新框架，它通过在频域中分离相位和幅度信息，并结合图像域特征，以提高重建质量。", "motivation": "现有MRI重建方法在频域处理上将相位和幅度视为整体，忽略了它们所携带信息的差异性（幅度主要反映强度，相位主要决定结构），从而可能导致特征学习的相互干扰。", "method": "PAS-Mamba框架在图像域使用LocalMamba保留空间局部性，在频域中将相位和幅度解耦到两个独立的支路，并采用Circular Frequency Domain Scanning (CFDS) 序列化频域特征，最后通过Dual-Domain Complementary Fusion Module (DDCFM) 融合双域信息并实现域间信息交换。", "result": "在IXI和fastMRI knee数据集上的大量实验表明，PAS-Mamba在MRI重建方面始终优于现有最先进的方法。", "conclusion": "PAS-Mamba通过解耦频域中的相位和幅度信息，并结合图像域特征，能够实现更优越的MRI重建效果，有效克服了现有方法的局限性。"}}
{"id": "2601.14479", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.14479", "abs": "https://arxiv.org/abs/2601.14479", "authors": ["Crish Nagarkar", "Leonid Bogachev", "Serge Sharoff"], "title": "Can LLM Reasoning Be Trusted? A Comparative Study: Using Human Benchmarking on Statistical Tasks", "comment": null, "summary": "This paper investigates the ability of large language models (LLMs) to solve statistical tasks, as well as their capacity to assess the quality of reasoning. While state-of-the-art LLMs have demonstrated remarkable performance in a range of NLP tasks, their competence in addressing even moderately complex statistical challenges is not well understood. We have fine-tuned selected open-source LLMs on a specially developed dataset to enhance their statistical reasoning capabilities, and compared their performance with the human scores used as a benchmark. Our results show that the fine-tuned models achieve better performance on advanced statistical tasks on the level comparable to a statistics student. Fine-tuning demonstrates architecture-dependent improvements, with some models showing significant performance gains, indicating clear potential for deployment in educational technology and statistical analysis assistance systems. We also show that LLMs themselves can be far better judges of the answers quality (including explanation and reasoning assessment) in comparison to traditional metrics, such as BLEU or BertScore. This self-evaluation capability enables scalable automated assessment for statistical education platforms and quality assurance in automated analysis tools. Potential applications also include validation tools for research methodology in academic and industry settings, and quality control mechanisms for data analysis workflows.", "AI": {"tldr": "本研究探讨了大型语言模型（LLMs）在统计任务和评估推理质量方面的能力。通过对选定的开源LLMs进行微调，发现在统计任务上的表现可媲美统计学学生，并且LLMs在评估答案质量方面优于传统指标。", "motivation": "尽管LLMs在NLP任务上表现出色，但其在解决统计问题方面的能力尚未得到充分了解。同时，需要更有效的自动评估方法来衡量LLMs在统计推理方面的表现。", "method": "研究者开发了一个专门的数据集来增强LLMs的统计推理能力，并对选定的开源LLMs进行了微调。然后，将微调后模型的表现与人类评分进行比较。此外，研究者还评估了LLMs自我评估答案质量（包括解释和推理）的能力，并与BLEU和BertScore等传统指标进行了比较。", "result": "微调后的模型在高级统计任务上的表现可与统计学学生媲美，且性能提升具有架构依赖性。LLMs在评估答案质量方面，比BLEU或BertScore等传统指标表现更好。", "conclusion": "LLMs经过微调后，在统计任务和评估推理质量方面具有巨大潜力，可应用于教育技术、统计分析辅助系统、可扩展的自动评估系统以及研究方法验证工具等领域。"}}
{"id": "2601.14518", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.14518", "abs": "https://arxiv.org/abs/2601.14518", "authors": ["Jinhui Liu", "Ximeng Zhang", "Yanbo Ai", "Zhou Yu"], "title": "Business Logic-Driven Text-to-SQL Data Synthesis for Business Intelligence", "comment": null, "summary": "Evaluating Text-to-SQL agents in private business intelligence (BI) settings is challenging due to the scarcity of realistic, domain-specific data. While synthetic evaluation data offers a scalable solution, existing generation methods fail to capture business realism--whether questions reflect realistic business logic and workflows. We propose a Business Logic-Driven Data Synthesis framework that generates data grounded in business personas, work scenarios, and workflows. In addition, we improve the data quality by imposing a business reasoning complexity control strategy that diversifies the analytical reasoning steps required to answer the questions. Experiments on a production-scale Salesforce database show that our synthesized data achieves high business realism (98.44%), substantially outperforming OmniSQL (+19.5%) and SQL-Factory (+54.7%), while maintaining strong question-SQL alignment (98.59%). Our synthetic data also reveals that state-of-the-art Text-to-SQL models still have significant performance gaps, achieving only 42.86% execution accuracy on the most complex business queries.", "AI": {"tldr": "研究提出了一种基于业务逻辑驱动的数据合成框架，用于生成更具商业现实性的Text-to-SQL评估数据，并发现现有Text-to-SQL模型在复杂商业查询上的性能仍有待提高。", "motivation": "在私有的商业智能（BI）环境中，由于缺乏真实、领域特定的数据，评估Text-to-SQL代理具有挑战性。现有的合成数据生成方法未能捕捉到商业现实性，即问题是否反映了真实的业务逻辑和工作流程。", "method": "提出一个基于业务逻辑驱动的数据合成框架，该框架基于商业用户画像、工作场景和工作流程来生成数据。此外，通过实施商业推理复杂度控制策略来提高数据质量，该策略多样化了解答问题所需的分析推理步骤。", "result": "在生产规模的Salesforce数据库上的实验表明，该合成数据具有很高的商业现实性（98.44%），显著优于OmniSQL（+19.5%）和SQL-Factory（+54.7%），同时保持了良好的问题-SQL对齐（98.59%）。合成数据还揭示，最先进的Text-to-SQL模型在处理最复杂的商业查询时，执行准确率仅为42.86%，存在显著的性能差距。", "conclusion": "所提出的业务逻辑驱动的数据合成框架能够生成高商业现实性的评估数据，有助于更准确地评估Text-to-SQL模型在实际BI场景中的表现，并指出了当前模型在处理复杂商业查询方面的不足。"}}
{"id": "2601.14686", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.14686", "abs": "https://arxiv.org/abs/2601.14686", "authors": ["Shuai Wang", "Yaoming Yang", "Bingdong Li", "Hao Hao", "Aimin Zhou"], "title": "IB-GRPO: Aligning LLM-based Learning Path Recommendation with Educational Objectives via Indicator-Based Group Relative Policy Optimization", "comment": null, "summary": "Learning Path Recommendation (LPR) aims to generate personalized sequences of learning items that maximize long-term learning effect while respecting pedagogical principles and operational constraints. Although large language models (LLMs) offer rich semantic understanding for free-form recommendation, applying them to long-horizon LPR is challenging due to (i) misalignment with pedagogical objectives such as the Zone of Proximal Development (ZPD) under sparse, delayed feedback, (ii) scarce and costly expert demonstrations, and (iii) multi-objective interactions among learning effect, difficulty scheduling, length controllability, and trajectory diversity. To address these issues, we propose IB-GRPO (Indicator-Based Group Relative Policy Optimization), an indicator-guided alignment approach for LLM-based LPR. To mitigate data scarcity, we construct hybrid expert demonstrations via Genetic Algorithm search and teacher RL agents and warm-start the LLM with supervised fine-tuning. Building on this warm-start, we design a within-session ZPD alignment score for difficulty scheduling. IB-GRPO then uses the $I_{ε+}$ dominance indicator to compute group-relative advantages over multiple objectives, avoiding manual scalarization and improving Pareto trade-offs. Experiments on ASSIST09 and Junyi using the KES simulator with a Qwen2.5-7B backbone show consistent improvements over representative RL and LLM baselines.", "AI": {"tldr": "提出了一种名为IB-GRPO的指标引导对齐方法，用于基于LLM的学习路径推荐，解决了长期推荐中存在的教学目标不一致、专家演示稀缺以及多目标交互等挑战，并在实验中取得了优于现有方法的性能。", "motivation": "现有的大型语言模型（LLMs）在学习路径推荐（LPR）方面存在挑战，包括与教学目标（如最近发展区 ZPD）的不一致、专家演示的稀缺性和高成本，以及学习效果、难度调度、长度控制和轨迹多样性等多目标之间的复杂交互。", "method": "提出IB-GRPO（Indicator-Based Group Relative Policy Optimization），一种指标引导的对齐方法。通过遗传算法和教师强化学习代理构建混合专家演示，并对LLM进行监督微调以缓解数据稀缺问题。设计了会话内ZPD对齐分数来处理难度调度，并使用$I_{ε+}$优势指标来计算多目标下的组相对优势，从而避免手动标量化并改进帕累托权衡。", "result": "在ASSIST09和Junyi数据集上，使用KES模拟器和Qwen2.5-7B作为骨干模型进行实验，IB-GRPO在性能上始终优于代表性的强化学习和LLM基线方法。", "conclusion": "IB-GRPO是一种有效的LLM驱动的学习路径推荐方法，通过结合专家演示、ZPD对齐和多目标优化技术，成功解决了长期LPR中的关键挑战，并在真实数据集上证明了其优越性。"}}
{"id": "2601.14837", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.14837", "abs": "https://arxiv.org/abs/2601.14837", "authors": ["B. Calmé", "N. J. Greenidge", "A. Metcalf", "A. Bacchetti", "G. Loza", "D. Kpeglo", "P. Lloyd", "V. Pensabene", "J. H. Chandler", "P. Valdastri"], "title": "Moving Beyond Compliance in Soft-Robotic Catheters Through Modularity for Precision Therapies", "comment": "31 pages, 6 figures, 7 supplementary figures", "summary": "Soft robotic instruments could navigate delicate, tortuous anatomy more safely than rigid tools, but clinical adoption is limited by insufficient tip functionalization and real-time feedback at the tissue interface. Few sensing and therapeutic modules are compact, robust, and adaptable enough to measure, and respond to, subtle physiological cues during intraluminal procedures. We present a 1.47 mm diameter modular soft robotic catheter that integrates sensing, actuation, and therapy while retaining the compliance needed for safe endoluminal navigation. Validated across multiple in vivo settings, we emphasize its utility in endoscopic retrograde cholangiopancreatography (ERCP), a highly technical procedure and a key access route to the pancreas, an organ that is fragile, difficult to instrument, and central to diseases such as pancreatic cancer. Our architecture supports up to four independently controlled functional units, allowing customizable combinations of anchoring, manipulation, sensing, and targeted drug delivery. In a live porcine model, we demonstrate semi-autonomous deployment into the pancreatic duct and 7.5 cm of endoscopic navigation within it, a region currently inaccessible with standard catheters. A closed-loop autonomous/shared-control system that combines a learned model, magnetic actuation, onboard shape sensing, and visual marker tracking further improves cannulation accuracy. Together, these results establish a scalable platform for multifunctional soft robotic catheters and a new paradigm for complex endoluminal interventions, with potential to reduce radiation exposure, shorten training, and accelerate clinical translation of soft robotic technologies.", "AI": {"tldr": "本文提出了一种直径仅为1.47毫米的模块化软体机器人导管，集成了传感、驱动和治疗功能，可在内窥镜检查逆行胰胆管造影 (ERCP) 等复杂操作中实现安全导航和精确干预。该导管已在动物模型中得到验证，并展示了其在胰管内导航和靶向药物输送的潜力。", "motivation": "现有软体机器人工具在末端功能化和实时组织反馈方面存在不足，限制了其临床应用。需要更小型、更稳健且适应性强的传感和治疗模块，以应对精细生理信号的测量和响应。", "method": "开发了一种直径1.47毫米的模块化软体机器人导管，该导管集成了传感、驱动和治疗功能。支持最多四个独立控制的功能单元，可组合锚定、操纵、传感和药物递送。在猪模型中进行了体外验证，并结合了学习模型、磁驱动、形状传感和视觉标记跟踪来实现半自主部署和精确的胰管内导航。", "result": "在猪模型中成功实现了导管在胰管内的半自主部署和7.5厘米导航。证明了该平台可实现个性化的功能组合，并显著提高了胰管插管的准确性。研究结果表明，该系统有望减少辐射暴露、缩短培训时间并加速软体机器人技术的临床转化。", "conclusion": "该研究建立了一个可扩展的多功能软体机器人导管平台，并为复杂内腔介入手术开辟了新范式。该技术在提高手术安全性、精准度和效率方面具有巨大潜力，尤其是在ERCP等精密操作中。"}}
{"id": "2601.14563", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14563", "abs": "https://arxiv.org/abs/2601.14563", "authors": ["Thanh-Huy Nguyen", "Hoang-Loc Cao", "Dat T. Chung", "Mai-Anh Vu", "Thanh-Minh Nguyen", "Minh Le", "Phat K. Huynh", "Ulas Bagci"], "title": "Scribble-Supervised Medical Image Segmentation with Dynamic Teacher Switching and Hierarchical Consistency", "comment": null, "summary": "Scribble-supervised methods have emerged to mitigate the prohibitive annotation burden in medical image segmentation. However, the inherent sparsity of these annotations introduces significant ambiguity, which results in noisy pseudo-label propagation and hinders the learning of robust anatomical boundaries. To address this challenge, we propose SDT-Net, a novel dual-teacher, single-student framework designed to maximize supervision quality from these weak signals. Our method features a Dynamic Teacher Switching (DTS) module to adaptively select the most reliable teacher. This selected teacher then guides the student via two synergistic mechanisms: high-confidence pseudo-labels, refined by a Pick Reliable Pixels (PRP) mechanism, and multi-level feature alignment, enforced by a Hierarchical Consistency (HiCo) module. Extensive experiments on the ACDC and MSCMRseg datasets demonstrate that SDT-Net achieves state-of-the-art performance, producing more accurate and anatomically plausible segmentation.", "AI": {"tldr": "本文提出了一种名为SDT-Net的双教师单学生框架，用于解决涂鸦式标注在医学图像分割中的模糊性问题。该框架通过动态教师切换、高置信度伪标签和多级特征对齐来提高分割精度。", "motivation": "现有的涂鸦式弱监督方法因标注稀疏导致伪标签传播噪声，难以学习鲁棒的解剖边界，标注成本过高。", "method": "提出SDT-Net框架，包含动态教师切换（DTS）模块自适应选择可靠教师，通过Pick Reliable Pixels（PRP）机制提炼高置信度伪标签，以及Hierarchical Consistency（HiCo）模块进行多级特征对齐，以指导学生网络学习。", "result": "在ACDC和MSCMRseg数据集上进行了广泛实验，SDT-Net取得了最先进的性能，分割结果更准确，解剖结构更合理。", "conclusion": "SDT-Net通过创新的双教师单学生机制，有效解决了涂鸦式标注的模糊性问题，提升了医学图像分割的准确性和鲁棒性，证明了其在弱监督医学图像分割领域的有效性。"}}
{"id": "2601.14691", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.14691", "abs": "https://arxiv.org/abs/2601.14691", "authors": ["Muhammad Khalifa", "Lajanugen Logeswaran", "Jaekyeom Kim", "Sungryull Sohn", "Yunxiang Zhang", "Moontae Lee", "Hao Peng", "Lu Wang", "Honglak Lee"], "title": "Gaming the Judge: Unfaithful Chain-of-Thought Can Undermine Agent Evaluation", "comment": null, "summary": "Large language models (LLMs) are increasingly used as judges to evaluate agent performance, particularly in non-verifiable settings where judgments rely on agent trajectories including chain-of-thought (CoT) reasoning. This paradigm implicitly assumes that the agent's CoT faithfully reflects both its internal reasoning and the underlying environment state. We show this assumption is brittle: LLM judges are highly susceptible to manipulation of agent reasoning traces. By systematically rewriting agent CoTs while holding actions and observations fixed, we demonstrate that manipulated reasoning alone can inflate false positive rates of state-of-the-art VLM judges by up to 90% across 800 trajectories spanning diverse web tasks. We study manipulation strategies spanning style-based approaches that alter only the presentation of reasoning and content-based approaches that fabricate signals of task progress, and find that content-based manipulations are consistently more effective. We evaluate prompting-based techniques and scaling judge-time compute, which reduce but do not fully eliminate susceptibility to manipulation. Our findings reveal a fundamental vulnerability in LLM-based evaluation and highlight the need for judging mechanisms that verify reasoning claims against observable evidence.", "AI": {"tldr": "研究表明，大型语言模型（LLMs）作为评估代理性能的裁判时，其判断容易受到代理推理轨迹（如Chain-of-Thought）操纵的影响，即使在动作和观测保持不变的情况下，操纵推理过程也能显著提高虚报率。", "motivation": "当前的LLM裁判范式依赖于代理推理轨迹（如CoT）来评估代理性能，但这种方法假设CoT能真实反映代理的内部推理和环境状态，作者发现这种假设是脆弱的，促使他们研究LLM裁判对推理轨迹操纵的敏感性。", "method": "通过系统地重写代理的CoT推理轨迹，同时保持其动作和观测不变，研究人员评估了不同操纵策略（风格类和内容类）的效果，以及提示工程和增加裁判计算资源等缓解方法。", "result": "研究发现在800条跨越多种Web任务的轨迹中，仅通过操纵推理过程，就可以使最先进的视觉语言模型（VLM）裁判的虚报率最高提升90%。内容类操纵策略比风格类操纵策略更有效。", "conclusion": "LLM作为裁判存在根本性的安全漏洞，容易受到推理轨迹操纵的影响。需要开发能够根据可观察证据验证推理声明的裁判机制。"}}
{"id": "2601.14568", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14568", "abs": "https://arxiv.org/abs/2601.14568", "authors": ["Wei Ma", "Shaowu Chen", "Junjie Ye", "Peichang Zhang", "Lei Huang"], "title": "Breaking the accuracy-resource dilemma: a lightweight adaptive video inference enhancement", "comment": "5 pages, 4 figures", "summary": "Existing video inference (VI) enhancement methods typically aim to improve performance by scaling up model sizes and employing sophisticated network architectures. While these approaches demonstrated state-of-the-art performance, they often overlooked the trade-off of resource efficiency and inference effectiveness, leading to inefficient resource utilization and suboptimal inference performance. To address this problem, a fuzzy controller (FC-r) is developed based on key system parameters and inference-related metrics. Guided by the FC-r, a VI enhancement framework is proposed, where the spatiotemporal correlation of targets across adjacent video frames is leveraged. Given the real-time resource conditions of the target device, the framework can dynamically switch between models of varying scales during VI. Experimental results demonstrate that the proposed method effectively achieves a balance between resource utilization and inference performance.", "AI": {"tldr": "提出了一种基于模糊控制器的视频推理增强框架（FC-r），该框架可以根据设备实时资源状况动态切换不同大小的模型，从而在资源利用率和推理性能之间取得平衡。", "motivation": "现有视频推理增强方法通常通过扩大模型规模和复杂网络结构来提高性能，但忽略了资源效率和推理有效性之间的权衡，导致资源利用率低和推理性能不佳。", "method": "开发了一个基于关键系统参数和推理相关指标的模糊控制器（FC-r）。在此框架下，利用目标在相邻视频帧之间的时空相关性，根据目标设备的实时资源条件，动态切换不同大小的模型以进行视频推理。", "result": "实验结果表明，所提出的方法能够有效地平衡资源利用率和推理性能。", "conclusion": "该研究成功开发了一种能够根据设备资源动态调整模型大小的视频推理增强框架，实现了资源效率和推理性能的良好折衷。"}}
{"id": "2601.15059", "categories": ["cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.15059", "abs": "https://arxiv.org/abs/2601.15059", "authors": ["Oleg Romanchuk", "Roman Bondar"], "title": "The Responsibility Vacuum: Organizational Failure in Scaled Agent Systems", "comment": null, "summary": "Modern CI/CD pipelines integrating agent-generated code exhibit a structural failure in responsibility attribution. Decisions are executed through formally correct approval processes, yet no entity possesses both the authority to approve those decisions and the epistemic capacity to meaningfully understand their basis.\n  We define this condition as responsibility vacuum: a state in which decisions occur, but responsibility cannot be attributed because authority and verification capacity do not coincide. We show that this is not a process deviation or technical defect, but a structural property of deployments where decision generation throughput exceeds bounded human verification capacity.\n  We identify a scaling limit under standard deployment assumptions, including parallel agent generation, CI-based validation, and individualized human approval gates. Beyond a throughput threshold, verification ceases to function as a decision criterion and is replaced by ritualized approval based on proxy signals. Personalized responsibility becomes structurally unattainable in this regime.\n  We further characterize a CI amplification dynamic, whereby increasing automated validation coverage raises proxy signal density without restoring human capacity. Under fixed time and attention constraints, this accelerates cognitive offloading in the broad sense and widens the gap between formal approval and epistemic understanding. Additional automation therefore amplifies, rather than mitigates, the responsibility vacuum.\n  We conclude that unless organizations explicitly redesign decision boundaries or reassign responsibility away from individual decisions toward batch- or system-level ownership, responsibility vacuum remains an invisible but persistent failure mode in scaled agent deployments.", "AI": {"tldr": "现代CI/CD流水线在集成代理生成代码时存在责任分配的结构性失败，导致“责任真空”现象。当决策生成速度超过人类验证能力时，正式审批沦为一种仪式，责任变得无法追溯。自动化程度的提高反而加剧了这一问题。", "motivation": "研究者注意到在集成了代理生成代码的CI/CD流水线中，审批流程虽然形式上合规，但没有人真正拥有决策权和理解决策依据的能力，即存在“责任真空”。", "method": "首先定义了“责任真空”的概念，即权限和验证能力不匹配的状态。然后分析了在标准部署假设下（并行代理生成、CI验证、个体人工审批）的扩展限制。接着揭示了CI放大效应，即自动化验证增加反而导致了仪式化审批。最后提出了避免责任真空的组织性解决方案。", "result": "在代理生成代码的CI/CD流水线中，当决策生成速率超过人类验证能力时，会产生“责任真空”。增加自动化验证覆盖范围会放大这一问题，而非缓解。个体责任在这种情况下变得难以实现。", "conclusion": "除非组织明确重塑决策边界或将责任从个体决策转移到批次或系统层面，否则在扩展的代理部署中，责任真空将是潜在且持久的故障模式。"}}
{"id": "2601.14702", "categories": ["cs.AI", "cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.14702", "abs": "https://arxiv.org/abs/2601.14702", "authors": ["Zecong Tang", "Zixu Wang", "Yifei Wang", "Weitong Lian", "Tianjian Gao", "Haoran Li", "Tengju Ru", "Lingyi Meng", "Zhejun Cui", "Yichen Zhu", "Qi Kang", "Kaixuan Wang", "Yu Zhang"], "title": "AutoDriDM: An Explainable Benchmark for Decision-Making of Vision-Language Models in Autonomous Driving", "comment": "23 pages. Submitted to ACL ARR 2026 January", "summary": "Autonomous driving is a highly challenging domain that requires reliable perception and safe decision-making in complex scenarios. Recent vision-language models (VLMs) demonstrate reasoning and generalization abilities, opening new possibilities for autonomous driving; however, existing benchmarks and metrics overemphasize perceptual competence and fail to adequately assess decision-making processes. In this work, we present AutoDriDM, a decision-centric, progressive benchmark with 6,650 questions across three dimensions - Object, Scene, and Decision. We evaluate mainstream VLMs to delineate the perception-to-decision capability boundary in autonomous driving, and our correlation analysis reveals weak alignment between perception and decision-making performance. We further conduct explainability analyses of models' reasoning processes, identifying key failure modes such as logical reasoning errors, and introduce an analyzer model to automate large-scale annotation. AutoDriDM bridges the gap between perception-centered and decision-centered evaluation, providing guidance toward safer and more reliable VLMs for real-world autonomous driving.", "AI": {"tldr": "本文提出了 AutoDriDM，一个以决策为中心的渐进式基准，用于评估自动驾驶中视觉语言模型（VLMs）的感知到决策的能力。研究发现感知和决策能力之间关联性较弱，并识别了模型推理中的主要失败模式。", "motivation": "现有的自动驾驶基准和指标过于侧重感知能力，未能充分评估决策过程。作者希望填补这一空白，为开发更安全可靠的 VLM 提供指导。", "method": "构建了一个包含 6,650 个问题的 AutoDriDM 基准，涵盖物体、场景和决策三个维度。评估了主流 VLM，并进行了相关性分析和可解释性分析，还引入了一个分析器模型来自动化标注。", "result": "主流 VLM 在感知和决策能力之间表现出较弱的关联性。识别出诸如逻辑推理错误等关键失败模式。", "conclusion": "AutoDriDM 成功弥合了以感知为中心和以决策为中心的评估之间的差距，为开发更安全可靠的自动驾驶 VLM 提供了重要方向。"}}
{"id": "2601.14874", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.14874", "abs": "https://arxiv.org/abs/2601.14874", "authors": ["Yara Mahmoud", "Yasheerah Yaqoot", "Miguel Altamirano Cabrera", "Dzmitry Tsetserukou"], "title": "HumanoidVLM: Vision-Language-Guided Impedance Control for Contact-Rich Humanoid Manipulation", "comment": "This paper has been accepted for publication at LBR of HRI 2026 conference", "summary": "Humanoid robots must adapt their contact behavior to diverse objects and tasks, yet most controllers rely on fixed, hand-tuned impedance gains and gripper settings. This paper introduces HumanoidVLM, a vision-language driven retrieval framework that enables the Unitree G1 humanoid to select task-appropriate Cartesian impedance parameters and gripper configurations directly from an egocentric RGB image. The system couples a vision-language model for semantic task inference with a FAISS-based Retrieval-Augmented Generation (RAG) module that retrieves experimentally validated stiffness-damping pairs and object-specific grasp angles from two custom databases, and executes them through a task-space impedance controller for compliant manipulation. We evaluate HumanoidVLM on 14 visual scenarios and achieve a retrieval accuracy of 93%. Real-world experiments show stable interaction dynamics, with z-axis tracking errors typically within 1-3.5 cm and virtual forces consistent with task-dependent impedance settings. These results demonstrate the feasibility of linking semantic perception with retrieval-based control as an interpretable path toward adaptive humanoid manipulation.", "AI": {"tldr": "本文提出了一种名为HumanoidVLM的视觉-语言驱动检索框架，使Unitree G1人形机器人能够根据其周围环境的RGB图像自动选择适合特定任务的阻抗参数和抓取配置，从而实现自适应操纵。", "motivation": "当前人形机器人的接触行为控制通常依赖于手动调整的阻抗增益和夹持器设置，这限制了它们在不同对象和任务中的适应性。研究的动机是开发一种能够通过视觉和语言信息自动适应不同任务的操纵方法。", "method": "该系统结合了视觉-语言模型来推断任务语义，并使用基于FAISS的检索增强生成（RAG）模块，从预先建立的数据库中检索经过实验验证的刚度-阻尼对和特定于对象的抓取角度。这些检索到的参数随后被用于任务空间阻抗控制器以实现顺应性操纵。", "result": "在14个视觉场景的评估中，HumanoidVLM实现了93%的检索准确率。实际实验表明，系统能够实现稳定的交互动态，Z轴跟踪误差通常在1-3.5厘米之间，并且虚拟力与任务相关的阻抗设置一致。", "conclusion": "研究结果证明了将语义感知与基于检索的控制相结合的可行性，为实现自适应人形机器人操纵提供了一条可解释的途径。"}}
{"id": "2601.14553", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2601.14553", "abs": "https://arxiv.org/abs/2601.14553", "authors": ["Brian Christian", "Matan Mazor"], "title": "Self-Blinding and Counterfactual Self-Simulation Mitigate Biases and Sycophancy in Large Language Models", "comment": null, "summary": "Fair decisions require ignoring irrelevant, potentially biasing, information. To achieve this, decision-makers need to approximate what decision they would have made had they not known certain facts, such as the gender or race of a job candidate. This counterfactual self-simulation is notoriously hard for humans, leading to biased judgments even by well-meaning actors. Here we show that large language models (LLMs) suffer from similar limitations in their ability to approximate what decisions they would make under counterfactual knowledge in offsetting gender and race biases and overcoming sycophancy. We show that prompting models to ignore or pretend not to know biasing information fails to offset these biases and occasionally backfires. However, unlike humans, LLMs can be given access to a ground-truth model of their own counterfactual cognition -- their own API. We show that this access to the responses of a blinded replica enables fairer decisions, while providing greater transparency to distinguish implicit from intentionally biased behavior.", "AI": {"tldr": "大型语言模型（LLMs）在近似反事实知识（如忽略性别和种族信息）方面存在与人类相似的局限性，即使通过提示词也难以消除偏见。然而，LLMs可以通过访问其自身反事实认知的“真实模型”（即API）来做出更公平的决策。", "motivation": "决策者需要忽略可能产生偏见的不相关信息，以做出公平的决策。人类在进行这种反事实的自我模拟时存在困难，导致有善意的行为者也会做出有偏见的判断。研究者希望探索LLMs是否也存在类似的局限性。", "method": "研究者通过提示词（如要求模型忽略偏见信息或假装不知道）来测试LLMs在反事实场景下的偏见。随后，他们尝试通过让LLMs访问其自身的“真实模型”（即API）来提供一个“盲眼副本”的响应，以此来观察是否能实现更公平的决策。", "result": "研究发现，提示模型忽略或假装不知道偏见信息并不能有效抵消这些偏见，有时甚至会适得其反。然而，通过访问LLM自身的API（即“真实模型”），能够实现更公平的决策，并能区分隐性偏见和故意偏见。", "conclusion": "LLMs在处理反事实信息以消除偏见方面存在挑战，与人类类似。但与人类不同的是，LLMs可以通过访问其自身API来改进决策的公平性，并提供区分不同类型偏见的能力。"}}
{"id": "2601.14871", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.14871", "abs": "https://arxiv.org/abs/2601.14871", "authors": ["Zejian Cui", "Ferdinando Rodriguez y Baena"], "title": "On-the-fly hand-eye calibration for the da Vinci surgical robot", "comment": "16 pages, 13 figures", "summary": "In Robot-Assisted Minimally Invasive Surgery (RMIS), accurate tool localization is crucial to ensure patient safety and successful task execution. However, this remains challenging for cable-driven robots, such as the da Vinci robot, because erroneous encoder readings lead to pose estimation errors. In this study, we propose a calibration framework to produce accurate tool localization results through computing the hand-eye transformation matrix on-the-fly. The framework consists of two interrelated algorithms: the feature association block and the hand-eye calibration block, which provide robust correspondences for key points detected on monocular images without pre-training, and offer the versatility to accommodate various surgical scenarios by adopting an array of filter approaches, respectively. To validate its efficacy, we test the framework extensively on publicly available video datasets that feature multiple surgical instruments conducting tasks in both in vitro and ex vivo scenarios, under varying illumination conditions and with different levels of key point measurement accuracy. The results show a significant reduction in tool localization errors under the proposed calibration framework, with accuracies comparable to other state-of-the-art methods while being more time-efficient.", "AI": {"tldr": "本研究提出了一种用于机器人辅助微创手术（RMIS）的即时手眼标定框架，通过特征关联和手眼标定算法，提高了带有编码器误差的线控手术机器人（如 da Vinci 机器人）工具定位的准确性，并在多种模拟场景下验证了其有效性。", "motivation": "在机器人辅助微创手术（RMIS）中，准确的工具定位对患者安全和手术成功至关重要。然而，对于 da Vinci 机器人等线控机器人，由于编码器读数错误导致的姿态估计误差，实现精确的工具定位仍然是一个挑战。", "method": "提出一个包含特征关联模块和手眼标定模块的标定框架。特征关联模块在无需预训练的情况下，为单目图像中的关键点提供鲁棒的对应关系；手眼标定模块则采用多种滤波方法，以适应不同的手术场景，从而实时计算手眼变换矩阵。", "result": "该框架在公开的视频数据集上进行了广泛测试，涵盖了体外和体外多种手术器械在不同光照条件和不同关键点测量精度下的场景。结果显示，与现有方法相比，该框架显著降低了工具定位误差，准确度相当，且效率更高。", "conclusion": "所提出的即时手眼标定框架能够有效地提高线控手术机器人的工具定位精度，在 RMIS 中具有重要的应用潜力，并且比现有方法更具时间效率。"}}
{"id": "2601.14584", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14584", "abs": "https://arxiv.org/abs/2601.14584", "authors": ["Cheng Wan", "Bahram Jafrasteh", "Ehsan Adeli", "Miaomiao Zhang", "Qingyu Zhao"], "title": "Anatomically Guided Latent Diffusion for Brain MRI Progression Modeling", "comment": "10 pages, 5 figures, 3 tables", "summary": "Accurately modeling longitudinal brain MRI progression is crucial for understanding neurodegenerative diseases and predicting individualized structural changes. Existing state-of-the-art approaches, such as Brain Latent Progression (BrLP), often use multi-stage training pipelines with auxiliary conditioning modules but suffer from architectural complexity, suboptimal use of conditional clinical covariates, and limited guarantees of anatomical consistency. We propose Anatomically Guided Latent Diffusion Model (AG-LDM), a segmentation-guided framework that enforces anatomically consistent progression while substantially simplifying the training pipeline. AG-LDM conditions latent diffusion by directly fusing baseline anatomy, noisy follow-up states, and clinical covariates at the input level, a strategy that avoids auxiliary control networks by learning a unified, end-to-end model that represents both anatomy and progression. A lightweight 3D tissue segmentation model (WarpSeg) provides explicit anatomical supervision during both autoencoder fine-tuning and diffusion model training, ensuring consistent brain tissue boundaries and morphometric fidelity. Experiments on 31,713 ADNI longitudinal pairs and zero-shot evaluation on OASIS-3 demonstrate that AG-LDM matches or surpasses more complex diffusion models, achieving state-of-the-art image quality and 15-20\\% reduction in volumetric errors in generated images. AG-LDM also exhibits markedly stronger utilization of temporal and clinical covariates (up to 31.5x higher sensitivity than BrLP) and generates biologically plausible counterfactual trajectories, accurately capturing hallmarks of Alzheimer's progression such as limbic atrophy and ventricular expansion. These results highlight AG-LDM as an efficient, anatomically grounded framework for reliable brain MRI progression modeling.", "AI": {"tldr": "本文提出了一种名为AG-LDM的解剖学引导潜在扩散模型，用于模拟纵向脑MRI的进展。该模型通过直接融合基线解剖、后续状态和临床协变量，并利用轻量级分割模型进行显式解剖监督，简化了训练流程，确保了解剖学上的一致性，并在ADNI数据集上取得了最先进的图像质量和体积误差的降低。", "motivation": "现有脑MRI进展建模方法（如BrLP）存在架构复杂、条件临床协变量利用不佳以及解剖学一致性保证有限等问题。因此，需要一种更简单、更有效且解剖学上一致的建模方法。", "method": "提出了一种名为AG-LDM的解剖学引导潜在扩散模型。该模型通过直接融合基线解剖、有噪声的后续状态和临床协变量来条件化潜在扩散，避免了辅助控制网络，实现了端到端的学习。一个轻量级的3D组织分割模型（WarpSeg）在自编码器微调和扩散模型训练过程中提供显式的解剖监督，以确保解剖学上的一致性。", "result": "AG-LDM在ADNI数据集上实现了与或优于现有复杂扩散模型的结果，图像质量达到最先进水平，生成的图像体积误差减少了15-20%。AG-LDM对时间协变量和临床协变量的利用能力显著增强（比BrLP高31.5倍），并能生成具有生物学合理性的反事实轨迹，准确捕捉阿尔茨海默病进展的特征。", "conclusion": "AG-LDM是一种高效、解剖学上 grounding 的脑MRI进展建模框架，能够实现可靠的建模，并有效利用临床信息，具有重要的研究和应用价值。"}}
{"id": "2601.14525", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.14525", "abs": "https://arxiv.org/abs/2601.14525", "authors": ["Chenglei Si", "Zitong Yang", "Yejin Choi", "Emmanuel Candès", "Diyi Yang", "Tatsunori Hashimoto"], "title": "Towards Execution-Grounded Automated AI Research", "comment": null, "summary": "Automated AI research holds great potential to accelerate scientific discovery. However, current LLMs often generate plausible-looking but ineffective ideas. Execution grounding may help, but it is unclear whether automated execution is feasible and whether LLMs can learn from the execution feedback. To investigate these, we first build an automated executor to implement ideas and launch large-scale parallel GPU experiments to verify their effectiveness. We then convert two realistic research problems - LLM pre-training and post-training - into execution environments and demonstrate that our automated executor can implement a large fraction of the ideas sampled from frontier LLMs. We analyze two methods to learn from the execution feedback: evolutionary search and reinforcement learning. Execution-guided evolutionary search is sample-efficient: it finds a method that significantly outperforms the GRPO baseline (69.4% vs 48.0%) on post-training, and finds a pre-training recipe that outperforms the nanoGPT baseline (19.7 minutes vs 35.9 minutes) on pre-training, all within just ten search epochs. Frontier LLMs often generate meaningful algorithmic ideas during search, but they tend to saturate early and only occasionally exhibit scaling trends. Reinforcement learning from execution reward, on the other hand, suffers from mode collapse. It successfully improves the average reward of the ideator model but not the upper-bound, due to models converging on simple ideas. We thoroughly analyze the executed ideas and training dynamics to facilitate future efforts towards execution-grounded automated AI research.", "AI": {"tldr": "研究人员开发了一个自动化执行器，用于测试和改进大型语言模型（LLM）在预训练和后训练方面的研究想法。结果表明，基于进化的搜索方法在样本效率和性能提升方面优于强化学习方法，并且能够有效利用执行反馈。然而，LLM生成的想法存在早期饱和和偶尔出现规模效应的问题，而强化学习则面临模式崩溃的挑战。", "motivation": "当前的LLM生成的AI研究想法虽然看起来合理，但效果不佳。为了加速科学发现，研究人员希望探索“执行基础”（execution grounding）是否可行，即LLM能否从实际执行的反馈中学习。", "method": "1. 构建自动化执行器，实现LLM生成的想法并进行大规模GPU实验。 2. 将LLM预训练和后训练转化为执行环境。 3. 分析两种从执行反馈中学习的方法：进化搜索和强化学习。 4. 对比实验结果，分析LLM的输出和训练动态。", "result": "自动化执行器能够实现大量由LLM生成的想法。基于进化的搜索方法在后训练任务上显著优于GRPO基线（69.4% vs 48.0%），在预训练任务上找到了比nanoGPT基线更快的训练方法（19.7分钟 vs 35.9分钟），且仅需十次搜索迭代。LLM在搜索过程中会产生有意义的算法想法，但存在早期饱和和规模效应不明显的问题。基于执行奖励的强化学习虽然能提高平均奖励，但由于模型收敛到简单想法，未能提升上限性能。", "conclusion": "执行基础的自动化AI研究是可行的，并且基于进化的搜索方法在利用执行反馈方面比强化学习更有效。然而，LLM生成的想法质量和学习能力仍有待提高，特别是要克服早期饱和和模式崩溃的问题，以实现更具突破性的AI研究。"}}
{"id": "2601.14711", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.14711", "abs": "https://arxiv.org/abs/2601.14711", "authors": ["Mingxuan Song", "Yusen Huo", "Bohan Zhou", "Shenglin Yin", "Zhen Xiao", "Jieyi Long", "Zhilin Zhang", "Chuan Yu"], "title": "DARA: Few-shot Budget Allocation in Online Advertising via In-Context Decision Making with RL-Finetuned LLMs", "comment": "Accepted at The ACM Web Conference (WWW) 2026", "summary": "Optimizing the advertiser's cumulative value of winning impressions under budget constraints poses a complex challenge in online advertising, under the paradigm of AI-Generated Bidding (AIGB). Advertisers often have personalized objectives but limited historical interaction data, resulting in few-shot scenarios where traditional reinforcement learning (RL) methods struggle to perform effectively. Large Language Models (LLMs) offer a promising alternative for AIGB by leveraging their in-context learning capabilities to generalize from limited data. However, they lack the numerical precision required for fine-grained optimization. To address this limitation, we introduce GRPO-Adaptive, an efficient LLM post-training strategy that enhances both reasoning and numerical precision by dynamically updating the reference policy during training. Built upon this foundation, we further propose DARA, a novel dual-phase framework that decomposes the decision-making process into two stages: a few-shot reasoner that generates initial plans via in-context prompting, and a fine-grained optimizer that refines these plans using feedback-driven reasoning. This separation allows DARA to combine LLMs' in-context learning strengths with precise adaptability required by AIGB tasks. Extensive experiments on both real-world and synthetic data environments demonstrate that our approach consistently outperforms existing baselines in terms of cumulative advertiser value under budget constraints.", "AI": {"tldr": "提出了一种名为DARA的新型双阶段框架，结合了大型语言模型（LLMs）的少样本推理能力和强化学习（RL）的精确优化能力，以解决AI生成竞价（AIGB）中预算受限的广告累积价值最大化问题。", "motivation": "在AI生成竞价（AIGB）场景下，广告商面临预算限制时最大化获胜展示累积价值的挑战。传统的强化学习方法在数据稀疏的少样本场景下表现不佳，而大型语言模型（LLMs）的上下文学习能力可以弥补数据不足的问题，但缺乏数值精度进行精细化优化。", "method": "引入GRPO-Adaptive，一种LLM后训练策略，通过动态更新参考策略来增强推理和数值精度。在此基础上，提出DARA双阶段框架：第一阶段使用少样本推理器通过上下文提示生成初始计划；第二阶段使用精细化优化器结合反馈驱动的推理来优化计划。", "result": "在真实世界和合成数据环境中进行的广泛实验表明，DARA在预算约束下的累积广告商价值方面，一致优于现有基线方法。", "conclusion": "DARA框架成功地结合了LLMs的上下文学习能力和AIGB任务所需的精确适应性，有效解决了预算受限情况下的广告优化问题，并在实验中展现出优越的性能。"}}
{"id": "2601.14560", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.14560", "abs": "https://arxiv.org/abs/2601.14560", "authors": ["Unggi Lee", "Jiyeong Bae", "Jaehyeon Park", "Haeun Park", "Taejun Park", "Younghoon Jeon", "Sungmin Cho", "Junbo Koh", "Yeil Jeong", "Gyeonggeon Lee"], "title": "Rewarding How Models Think Pedagogically: Integrating Pedagogical Reasoning and Thinking Rewards for LLMs in Education", "comment": null, "summary": "Large language models (LLMs) are increasingly deployed as intelligent tutoring systems, yet research on optimizing LLMs specifically for educational contexts remains limited. Recent works have proposed reinforcement learning approaches for training LLM tutors, but these methods focus solely on optimizing visible responses while neglecting the model's internal thinking process. We introduce PedagogicalRL-Thinking, a framework that extends pedagogical alignment to reasoning LLMs in education through two novel approaches: (1) Pedagogical Reasoning Prompting, which guides internal reasoning using domain-specific educational theory rather than generic instructions; and (2) Thinking Reward, which explicitly evaluates and reinforces the pedagogical quality of the model's reasoning traces. Our experiments reveal that domain-specific, theory-grounded prompting outperforms generic prompting, and that Thinking Reward is most effective when combined with pedagogical prompting. Furthermore, models trained only on mathematics tutoring dialogues show improved performance on educational benchmarks not seen during training, while preserving the base model's factual knowledge. Our quantitative and qualitative analyses reveal that pedagogical thinking reward produces systematic reasoning trace changes, with increased pedagogical reasoning and more structured instructional decision-making in the tutor's thinking process.", "AI": {"tldr": "本研究提出了PedagogicalRL-Thinking框架，通过领域特定的教育理论提示和对模型内部推理过程的奖励，来优化LLM作为智能辅导系统的能力，实验表明该方法优于通用提示，并能提升在未见过任务上的教育表现。", "motivation": "当前LLM作为智能辅导系统的研究主要关注可见响应的优化，忽视了模型内部的思考过程。作者希望通过结合教育理论来改进LLM的推理能力，使其在教育领域表现更好。", "method": "提出了PedagogicalRL-Thinking框架，包含两个新方法：1. Pedagogical Reasoning Prompting，使用领域特定的教育理论指导内部推理；2. Thinking Reward，评估和奖励模型推理过程的教育质量。", "result": "领域特定的理论指导提示优于通用提示；Thinking Reward与Pedagogical Prompting结合效果最佳；在数学辅导对话上训练的模型，在未见过但相关的教育基准上表现提升，且保留了基础模型的知识；推理过程展示出更多的教育性推理和结构化的教学决策。", "conclusion": "PedagogicalRL-Thinking框架能够系统性地改进LLM在教育领域的推理过程，通过结合领域教育理论和对思考过程的奖励，可以提升其作为智能辅导系统的表现，并且这种提升具有迁移性。"}}
{"id": "2601.14945", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14945", "abs": "https://arxiv.org/abs/2601.14945", "authors": ["Yuteng Sun", "Haoran Wang", "Ruofei Bai", "Zhengguo Li", "Jun Li", "Meng Yee", "Chuah", "Wei Yun Yau"], "title": "TIDAL: Temporally Interleaved Diffusion and Action Loop for High-Frequency VLA Control", "comment": null, "summary": "Large-scale Vision-Language-Action (VLA) models offer semantic generalization but suffer from high inference latency, limiting them to low-frequency batch-and-execute paradigm. This frequency mismatch creates an execution blind spot, causing failures in dynamic environments where targets move during the open-loop execution window. We propose TIDAL (Temporally Interleaved Diffusion and Action Loop), a hierarchical framework that decouples semantic reasoning from high-frequency actuation. TIDAL operates as a backbone-agnostic module for diffusion-based VLAs, using a dual-frequency architecture to redistribute the computational budget. Specifically, a low-frequency macro-intent loop caches semantic embeddings, while a high-frequency micro-control loop interleaves single-step flow integration with execution. This design enables approximately 9 Hz control updates on edge hardware (vs. approximately 2.4 Hz baselines) without increasing marginal overhead. To handle the resulting latency shift, we introduce a temporally misaligned training strategy where the policy learns predictive compensation using stale semantic intent alongside real-time proprioception. Additionally, we address the insensitivity of static vision encoders to velocity by incorporating a differential motion predictor. TIDAL is architectural, making it orthogonal to system-level optimizations. Experiments show a 2x performance gain over open-loop baselines in dynamic interception tasks. Despite a marginal regression in static success rates, our approach yields a 4x increase in feedback frequency and extends the effective horizon of semantic embeddings beyond the native action chunk size. Under non-paused inference protocols, TIDAL remains robust where standard baselines fail due to latency.", "AI": {"tldr": "TIDAL 是一个分层框架，通过解耦语义推理和高频驱动来解决大型视觉-语言-动作（VLA）模型在动态环境中推理延迟过高的问题，从而实现更高的控制更新频率并提高动态任务的性能。", "motivation": "大型 VLA 模型在推理时延迟过高，无法适应动态环境中目标移动的情况，导致在执行过程中出现“盲区”而失败。现有模型只能采用低频的批处理执行方式。", "method": "TIDAL 采用双频架构，将计算预算重新分配：一个低频的宏观意图循环缓存语义嵌入，一个高频的微观控制循环将单步流集成与执行交织。此外，还引入了暂失调的训练策略，并加入了一个差分运动预测器来处理速度信息。", "result": "TIDAL 在边缘硬件上实现了约 9 Hz 的控制更新，远高于基线（约 2.4 Hz），且边际开销未增加。在动态拦截任务中，性能比开环基线提升了 2 倍。尽管静态成功率略有下降，但反馈频率增加了 4 倍，语义嵌入的有效范围得到了扩展。", "conclusion": "TIDAL 是一种模块化的框架，通过引入双频架构和暂失调训练策略，有效地解决了 VLA 模型在动态环境下的推理延迟问题，提高了控制频率和动态任务性能，并且在非暂停推理协议下表现出鲁棒性。"}}
{"id": "2601.14569", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.14569", "abs": "https://arxiv.org/abs/2601.14569", "authors": ["Bhaavanaa Thumu", "Leena Mathur", "Youssouf Kebe", "Louis-Philippe Morency"], "title": "Social Caption: Evaluating Social Understanding in Multimodal Models", "comment": "24 pages", "summary": "Social understanding abilities are crucial for multimodal large language models (MLLMs) to interpret human social interactions. We introduce Social Caption, a framework grounded in interaction theory to evaluate social understanding abilities of MLLMs along three dimensions: Social Inference (SI), the ability to make accurate inferences about interactions; Holistic Social Analysis (HSA), the ability to generate comprehensive descriptions of interactions; Directed Social Analysis (DSA), the ability to extract relevant social information from interactions. We analyze factors influencing model performance in social understanding, such as scale, architectural design, and spoken context. Experiments with MLLM judges contribute insights about scaling automated evaluation of multimodal social understanding.", "AI": {"tldr": "本文提出了一个名为Social Caption的框架，用于评估多模态大语言模型（MLLMs）在理解人类社会互动方面的能力，并从社会推断、整体社会分析和定向社会分析三个维度进行评估。研究还分析了影响模型性能的因素，并利用MLLM作为裁判来评估多模态社会理解的自动化。", "motivation": "为了评估和提高MLLMs理解人类社会互动（如社交线索、情感和意图）的能力，作者开发了一个新的评估框架。", "method": "作者提出了Social Caption框架，该框架包含三个评估维度：社会推断（SI）、整体社会分析（HSA）和定向社会分析（DSA）。他们分析了模型规模、架构设计和口头语境等因素对模型性能的影响，并使用MLLM作为裁判来评估。", "result": "研究分析了影响MLLM社会理解能力的因素，并证明了Social Caption框架能够有效地评估这些能力。使用MLLM作为裁判为自动化评估提供了新的思路。", "conclusion": "Social Caption框架为评估MLLMs的社会理解能力提供了一个多维度的方法。研究结果强调了模型规模、架构和上下文信息对社会理解的重要性，并展示了使用MLLM进行自动化评估的可行性。"}}
{"id": "2601.14593", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.14593", "abs": "https://arxiv.org/abs/2601.14593", "authors": ["Po-Kai Chiu", "Hung-Hsuan Chen"], "title": "From Volumes to Slices: Computationally Efficient Contrastive Learning for Sequential Abdominal CT Analysis", "comment": null, "summary": "The requirement for expert annotations limits the effectiveness of deep learning for medical image analysis. Although 3D self-supervised methods like volume contrast learning (VoCo) are powerful and partially address the labeling scarcity issue, their high computational cost and memory consumption are barriers. We propose 2D-VoCo, an efficient adaptation of the VoCo framework for slice-level self-supervised pre-training that learns spatial-semantic features from unlabeled 2D CT slices via contrastive learning. The pre-trained CNN backbone is then integrated into a CNN-LSTM architecture to classify multi-organ injuries. In the RSNA 2023 Abdominal Trauma dataset, 2D-VoCo pre-training significantly improves mAP, precision, recall, and RSNA score over training from scratch. Our framework provides a practical method to reduce the dependency on labeled data and enhance model performance in clinical CT analysis. We release the code for reproducibility. https://github.com/tkz05/2D-VoCo-CT-Classifier", "AI": {"tldr": "提出了一种名为2D-VoCo的2D切片级自监督预训练方法，用于医学图像分析，以克服3D自监督方法计算成本高的问题，并在多器官损伤分类任务中取得了显著性能提升。", "motivation": "深度学习在医学图像分析中需要大量专家标注，这限制了其有效性。现有的3D自监督方法（如VoCo）计算成本和内存消耗高，难以广泛应用。", "method": "提出2D-VoCo，一种适用于CT切片级别的自监督预训练方法，利用对比学习从无标签的2D CT切片中学习空间-语义特征。然后将预训练的CNN骨干网络集成到CNN-LSTM架构中，用于多器官损伤分类。", "result": "在RSNA 2023腹部创伤数据集上，与从头开始训练相比，2D-VoCo预训练显著提高了mAP、精确率、召回率和RSNA分数。", "conclusion": "2D-VoCo提供了一种实用的方法，可以减少对标记数据的依赖，并提高临床CT分析中模型的性能。"}}
{"id": "2601.14764", "categories": ["cs.AI", "cs.HC", "cs.LO"], "pdf": "https://arxiv.org/pdf/2601.14764", "abs": "https://arxiv.org/abs/2601.14764", "authors": ["Thomas Eiter", "Tobias Geibinger", "Zeynep G. Saribatur"], "title": "An XAI View on Explainable ASP: Methods, Systems, and Perspectives", "comment": "10 pages", "summary": "Answer Set Programming (ASP) is a popular declarative reasoning and problem solving approach in symbolic AI. Its rule-based formalism makes it inherently attractive for explainable and interpretive reasoning, which is gaining importance with the surge of Explainable AI (XAI). A number of explanation approaches and tools for ASP have been developed, which often tackle specific explanatory settings and may not cover all scenarios that ASP users encounter. In this survey, we provide, guided by an XAI perspective, an overview of types of ASP explanations in connection with user questions for explanation, and describe how their coverage by current theory and tools. Furthermore, we pinpoint gaps in existing ASP explanations approaches and identify research directions for future work.", "AI": {"tldr": "本文对面向可解释人工智能（XAI）的答案集规划（ASP）解释方法进行了综述，分析了现有方法在不同解释场景下的覆盖情况，并指出了未来的研究方向。", "motivation": "随着对可解释人工智能（XAI）的需求日益增长，ASP作为一种声明式推理方法，其固有的可解释性使其成为重要的研究对象。然而，现有的ASP解释方法可能无法覆盖所有用户场景，因此需要一个全面的综述。", "method": "文章从XAI的视角出发，对ASP的解释类型与用户解释问题进行关联，并描述了当前理论和工具对其的覆盖程度。通过分析现有方法的不足，识别研究空白并提出未来研究方向。", "result": "文章梳理了ASP解释的多种类型，并评估了现有理论和工具在不同解释场景下的覆盖能力。同时，指出了当前ASP解释方法存在的不足之处。", "conclusion": "本文提供了一个关于ASP解释的XAI视角下的综述，为理解和改进ASP解释方法提供了基础，并为未来的研究指明了方向。"}}
{"id": "2601.14921", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14921", "abs": "https://arxiv.org/abs/2601.14921", "authors": ["Sarat Ahmad", "Maryam Hafeez", "Syed Ali Raza Zaidi"], "title": "Vision-Language Models on the Edge for Real-Time Robotic Perception", "comment": null, "summary": "Vision-Language Models (VLMs) enable multimodal reasoning for robotic perception and interaction, but their deployment in real-world systems remains constrained by latency, limited onboard resources, and privacy risks of cloud offloading. Edge intelligence within 6G, particularly Open RAN and Multi-access Edge Computing (MEC), offers a pathway to address these challenges by bringing computation closer to the data source. This work investigates the deployment of VLMs on ORAN/MEC infrastructure using the Unitree G1 humanoid robot as an embodied testbed. We design a WebRTC-based pipeline that streams multimodal data to an edge node and evaluate LLaMA-3.2-11B-Vision-Instruct deployed at the edge versus in the cloud under real-time conditions. Our results show that edge deployment preserves near-cloud accuracy while reducing end-to-end latency by 5\\%. We further evaluate Qwen2-VL-2B-Instruct, a compact model optimized for resource-constrained environments, which achieves sub-second responsiveness, cutting latency by more than half but at the cost of accuracy.", "AI": {"tldr": "本研究将视觉语言模型（VLMs）部署在6G边缘智能基础设施（ORAN/MEC）上，以解决现实世界机器人部署中的延迟、资源和隐私问题。通过在Unitree G1机器人上进行实验，证明了边缘部署的VLMs可以在保持接近云端精度的同时，显著降低端到端延迟。", "motivation": "现实世界机器人部署VLMs面临延迟高、板载资源有限以及云端处理带来的隐私风险等挑战。6G边缘智能（包括Open RAN和MEC）可以提供解决方案，将计算能力移近数据源。", "method": "使用Unitree G1人形机器人作为实验平台，设计了一个基于WebRTC的管道，将多模态数据流传输到边缘节点。在边缘和云端分别部署LLaMA-3.2-11B-Vision-Instruct模型进行实时性能评估。同时，评估了轻量级模型Qwen2-VL-2B-Instruct在资源受限环境下的表现。", "result": "与云端部署相比，边缘部署LLaMA-3.2-11B-Vision-Instruct模型在保持近乎相同的准确率的同时，端到端延迟降低了5%。轻量级模型Qwen2-VL-2B-Instruct实现了亚秒级响应，延迟降低了一半以上，但牺牲了一部分准确率。", "conclusion": "在ORAN/MEC边缘部署VLMs是应对机器人应用中延迟和资源限制的有效策略。轻量级模型在牺牲少量准确率的情况下，能够实现更快的响应速度，适用于资源受限的场景。"}}
{"id": "2601.14784", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14784", "abs": "https://arxiv.org/abs/2601.14784", "authors": ["Amaury Guichard", "Laurent Michel", "Hélène Verhaeghe", "Pierre Schaus"], "title": "Towards Bound Consistency for the No-Overlap Constraint Using MDDs", "comment": null, "summary": "Achieving bound consistency for the no-overlap constraint is known to be NP-complete. Therefore, several polynomial-time tightening techniques, such as edge finding, not-first-not-last reasoning, and energetic reasoning, have been introduced for this constraint. In this work, we derive the first bound-consistent algorithm for the no-overlap constraint. By building on the no-overlap MDD defined by Ciré and van Hoeve, we extract bounds of the time window of the jobs, allowing us to tighten start and end times in time polynomial in the number of nodes of the MDD. Similarly, to bound the size and time-complexity, we limit the width of the MDD to a threshold, creating a relaxed MDD that can also be used to relax the bound-consistent filtering. Through experiments on a sequencing problem with time windows and a just-in-time objective ($1 \\mid r_j, d_j, \\bar{d}_j \\mid \\sum E_j + \\sum T_j$), we observe that the proposed filtering, even with a threshold on the width, achieves a stronger reduction in the number of nodes visited in the search tree compared to the previously proposed precedence-detection algorithm of Ciré and van Hoeve. The new filtering also appears to be complementary to classical propagation methods for the no-overlap constraint, allowing a substantial reduction in both the number of nodes and the solving time on several instances.", "AI": {"tldr": "本文提出了一种用于“无重叠”约束的界一致性算法，该算法基于no-overlap MDD，通过提取作业时间窗口的界来收紧开始和结束时间。为了控制计算复杂度，算法限制了MDD的宽度，并对求解树的节点访问量和求解时间进行了实验验证，结果表明其优于现有的算法。", "motivation": "“无重叠”约束的界一致性求解是NP完全的，因此需要高效的多项式时间收紧技术。", "method": "文章基于Ciré和van Hoeve提出的no-overlap MDD，提取作业时间窗口的界以收紧开始和结束时间。为了控制计算量，算法限制了MDD的宽度，并提出了一个宽松的MDD，用于宽松的界一致性过滤。在求解树中通过实验评估了该算法的效果。", "result": "所提出的过滤算法，即使在MDD宽度设置阈值的情况下，也能比Ciré和van Hoeve提出的基于前驱检测的算法更有效地减少搜索树中访问的节点数量。该算法与经典的“无重叠”约束传播方法互补，在多个实例上显著减少了节点访问量和求解时间。", "conclusion": "本文提出的界一致性过滤算法对“无重叠”约束有效，并且在实际问题中表现出优于现有方法的性能，同时也是现有传播方法的有效补充。"}}
{"id": "2601.14602", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14602", "abs": "https://arxiv.org/abs/2601.14602", "authors": ["Oindrila Saha", "Vojtech Krs", "Radomir Mech", "Subhransu Maji", "Matheus Gadelha", "Kevin Blackburn-Matzen"], "title": "3D Space as a Scratchpad for Editable Text-to-Image Generation", "comment": null, "summary": "Recent progress in large language models (LLMs) has shown that reasoning improves when intermediate thoughts are externalized into explicit workspaces, such as chain-of-thought traces or tool-augmented reasoning. Yet, visual language models (VLMs) lack an analogous mechanism for spatial reasoning, limiting their ability to generate images that accurately reflect geometric relations, object identities, and compositional intent. We introduce the concept of a spatial scratchpad -- a 3D reasoning substrate that bridges linguistic intent and image synthesis. Given a text prompt, our framework parses subjects and background elements, instantiates them as editable 3D meshes, and employs agentic scene planning for placement, orientation, and viewpoint selection. The resulting 3D arrangement is rendered back into the image domain with identity-preserving cues, enabling the VLM to generate spatially consistent and visually coherent outputs. Unlike prior 2D layout-based methods, our approach supports intuitive 3D edits that propagate reliably into final images. Empirically, it achieves a 32% improvement in text alignment on GenAI-Bench, demonstrating the benefit of explicit 3D reasoning for precise, controllable image generation. Our results highlight a new paradigm for vision-language models that deliberate not only in language, but also in space. Code and visualizations at https://oindrilasaha.github.io/3DScratchpad/", "AI": {"tldr": "本研究提出了一个名为“空间草稿板”的3D推理机制，用于增强视觉语言模型（VLMs）在空间推理方面的能力，通过将文本指令转化为可编辑的3D场景，从而生成更具空间一致性和视觉连贯性的图像。", "motivation": "现有的视觉语言模型在空间推理方面存在局限，难以生成精确反映几何关系、物体身份和组合意图的图像。LLMs在外部工作空间（如链式思考）中进行推理可以提升表现，但VLMs缺乏类似的机制来处理空间信息。", "method": "该框架首先解析文本提示中的主体和背景元素，并将它们实例化为可编辑的3D网格。然后，利用代理式场景规划来确定物体的位置、方向和视角。最后，将生成的3D场景渲染回图像域，并保留身份信息，使VLM能够生成空间一致的输出。与2D布局方法不同，该方法支持直观的3D编辑，并能可靠地反映到最终图像中。", "result": "在GenAI-Bench基准测试中，该方法将文本对齐度提高了32%，证明了显式3D推理在精确、可控图像生成方面的优势。", "conclusion": "通过引入空间草稿板，实现了VLM不仅在语言上进行推理，也在空间上进行推理的新范式，从而显著提高了图像生成在文本对齐和空间一致性方面的表现。"}}
{"id": "2601.14773", "categories": ["cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2601.14773", "abs": "https://arxiv.org/abs/2601.14773", "authors": ["Haizhou Liu", "Haodong Jin", "Yiming Wang", "Hui Yu"], "title": "Semantic-Guided Unsupervised Video Summarization", "comment": null, "summary": "Video summarization is a crucial technique for social understanding, enabling efficient browsing of massive multimedia content and extraction of key information from social platforms. Most existing unsupervised summarization methods rely on Generative Adversarial Networks (GANs) to enhance keyframe selection and generate coherent, video summaries through adversarial training. However, such approaches primarily exploit unimodal features, overlooking the guiding role of semantic information in keyframe selection, and often suffer from unstable training. To address these limitations, we propose a novel Semantic-Guided Unsupervised Video Summarization method. Specifically, we design a novel frame-level semantic alignment attention mechanism and integrate it into a keyframe selector, which guides the Transformer-based generator within the adversarial framework to better reconstruct videos. In addition, we adopt an incremental training strategy to progressively update the model components, effectively mitigating the instability of GAN training. Experimental results demonstrate that our approach achieves superior performance on multiple benchmark datasets.", "AI": {"tldr": "提出了一种新的语义引导的无监督视频摘要方法，通过语义对齐注意力机制和增量训练策略来克服现有方法的局限性。", "motivation": "现有无监督视频摘要方法主要依赖于 GANs，但忽略了语义信息在关键帧选择中的指导作用，且训练不稳定。", "method": "设计了帧级语义对齐注意力机制，并将其集成到关键帧选择器中，指导基于 Transformer 的生成器。采用增量训练策略来逐步更新模型组件。", "result": "在多个基准数据集上取得了优于现有方法的性能。", "conclusion": "所提出的语义引导的无监督视频摘要方法能够有效提高摘要质量，并克服 GANs 训练不稳定的问题。"}}
{"id": "2601.14658", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14658", "abs": "https://arxiv.org/abs/2601.14658", "authors": ["Navid Ayoobi", "Marcus I Armstrong", "Arjun Mukherjee"], "title": "Say Anything but This: When Tokenizer Betrays Reasoning in LLMs", "comment": null, "summary": "Large language models (LLMs) reason over discrete token ID sequences, yet modern subword tokenizers routinely produce non-unique encodings: multiple token ID sequences can detokenize to identical surface strings. This representational mismatch creates an unmeasured fragility wherein reasoning processes can fail. LLMs may treat two internal representations as distinct \"words\" even when they are semantically identical at the text level. In this work, we show that tokenization can betray LLM reasoning through one-to-many token ID mappings. We introduce a tokenization-consistency probe that requires models to replace designated target words in context while leaving all other content unchanged. The task is intentionally simple at the surface level, enabling us to attribute failures to tokenizer-detokenizer artifacts rather than to knowledge gaps or parameter limitations. Through analysis of over 11000 replacement trials across state-of-the-art open-source LLMs, we find a non-trivial rate of outputs exhibit phantom edits: cases where models operate under the illusion of correct reasoning, a phenomenon arising from tokenizer-induced representational defects. We further analyze these cases and provide a taxonomy of eight systematic tokenizer artifacts, including whitespace-boundary shifts and intra-word resegmentation. These findings indicate that part of apparent reasoning deficiency originates in the tokenizer layer, motivating tokenizer-level remedies before incurring the cost of training ever-larger models on ever-larger corpora.", "AI": {"tldr": "研究发现，大型语言模型（LLMs）在处理文本时，由于子词分词器生成的非唯一编码，可能出现“幻觉式编辑”问题，即模型在内部的表示不同，但实际上指向相同的文本。研究者提出了一种分词一致性探测方法，并发现8种系统性的分词伪影是导致此问题的原因，这表明分词器层面的改进可能是提升LLM推理能力的关键。", "motivation": "现有的大型语言模型（LLMs）在处理离散的token ID序列，但现代子词分词器常产生非唯一编码，即多个token ID序列可以还原成相同的字符串。这种表示上的不匹配可能导致模型在内部区分语义相同的文本，从而削弱模型的推理能力。", "method": "1. 提出一种分词一致性探测（tokenization-consistency probe），要求模型在保留其他内容不变的情况下，替换指定的目标词语。2. 在11000多次替换实验中，对当前最先进的开源LLMs进行分析。3. 对识别出的“幻觉式编辑”案例进行分类，构建了8种系统性的分词伪影（tokenizer artifacts）的分类体系。", "result": "在对开源LLMs的实验中，发现存在显著比例的输出显示出“幻觉式编辑”，即模型在错误的推理下认为自己是正确的，这种现象源于分词器引起的表示缺陷。研究者归纳了8种系统性的分词伪影，例如空格边界移动和词内重分段。", "conclusion": "部分LLM推理能力的不足源于分词器层面的问题，而非模型知识或参数的限制。因此，在投入更多资源训练更大模型之前，应考虑从分词器层面进行改进，以解决这些由分词器引起的问题。"}}
{"id": "2601.14973", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14973", "abs": "https://arxiv.org/abs/2601.14973", "authors": ["Faryal Batool", "Iana Zhura", "Valerii Serpiva", "Roohan Ahmed Khan", "Ivan Valuev", "Issatay Tokmurziyev", "Dzmitry Tsetserukou"], "title": "HumanDiffusion: A Vision-Based Diffusion Trajectory Planner with Human-Conditioned Goals for Search and Rescue UAV", "comment": "This paper has been accepted at HRI, Late Breaking Report, 2026", "summary": "Reliable human--robot collaboration in emergency scenarios requires autonomous systems that can detect humans, infer navigation goals, and operate safely in dynamic environments. This paper presents HumanDiffusion, a lightweight image-conditioned diffusion planner that generates human-aware navigation trajectories directly from RGB imagery. The system combines YOLO-11--based human detection with diffusion-driven trajectory generation, enabling a quadrotor to approach a target person and deliver medical assistance without relying on prior maps or computationally intensive planning pipelines. Trajectories are predicted in pixel space, ensuring smooth motion and a consistent safety margin around humans. We evaluate HumanDiffusion in simulation and real-world indoor mock-disaster scenarios. On a 300-sample test set, the model achieves a mean squared error of 0.02 in pixel-space trajectory reconstruction. Real-world experiments demonstrate an overall mission success rate of 80% across accident-response and search-and-locate tasks with partial occlusions. These results indicate that human-conditioned diffusion planning offers a practical and robust solution for human-aware UAV navigation in time-critical assistance settings.", "AI": {"tldr": "该研究提出了一种名为HumanDiffusion的轻量级图像条件扩散规划器，可以直接从RGB图像生成人类感知的导航轨迹，用于四旋翼在紧急场景中向目标人物运送医疗援助，无需预先地图或复杂规划。", "motivation": "在紧急情况下，实现可靠的人机协作需要自主系统能够检测人类、推断导航目标并在动态环境中安全运行。现有的方法可能在计算上过于密集或依赖先验地图，这在紧急场景中可能不适用。", "method": "该系统结合了基于YOLO-11的人体检测和扩散模型驱动的轨迹生成。轨迹直接在像素空间中预测，生成平滑且能保持与人类安全距离的运动。该方法不需要预先地图，并且避免了计算密集型的规划流程。", "result": "在模拟和真实的室内模拟灾难场景中进行了评估。在300个样本的测试集上，模型在像素空间轨迹重建方面取得了0.02的均方误差。实际实验在事故响应和搜索定位任务中，成功率达到了80%，即使在存在部分遮挡的情况下。", "conclusion": "研究表明，基于人类条件的扩散规划是一种实用且鲁棒的解决方案，可用于在时间紧迫的援助场景中实现人类感知的无人机导航。"}}
{"id": "2601.14998", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.14998", "abs": "https://arxiv.org/abs/2601.14998", "authors": ["Adip Ranjan Das", "Maria Koskinopoulou"], "title": "Graph-Based Adaptive Planning for Coordinated Dual-Arm Robotic Disassembly of Electronic Devices (eGRAP)", "comment": "7 Pages, 8 Figures, 5 Tables", "summary": "E-waste is growing rapidly while recycling rates remain low. We propose an electronic-device Graph-based Adaptive Planning (eGRAP) that integrates vision, dynamic planning, and dual-arm execution for autonomous disassembly. A camera-equipped arm identifies parts and estimates their poses, and a directed graph encodes which parts must be removed first. A scheduler uses topological ordering of this graph to select valid next steps and assign them to two robot arms, allowing independent tasks to run in parallel. One arm carries a screwdriver (with an eye-in-hand depth camera) and the other holds or handles components. We demonstrate eGRAP on 3.5in hard drives: as parts are unscrewed and removed, the system updates its graph and plan online. Experiments show consistent full disassembly of each HDD, with high success rates and efficient cycle times, illustrating the method's ability to adaptively coordinate dual-arm tasks in real time.", "AI": {"tldr": "提出了一种名为eGRAP的电子垃圾自主拆解方法，该方法结合了计算机视觉、动态规划和双臂协作，能够实时识别零件、规划拆解顺序并执行任务。", "motivation": "电子垃圾量日益增长，但回收率低，需要高效的自主拆解技术。", "method": "eGRAP系统利用带摄像头的机械臂识别零件并估计姿态，构建一个有向图来表示零件的拆解依赖关系。调度器通过对图进行拓扑排序来选择下一步任务，并分配给两个机械臂，实现并行操作。一个机械臂配备螺丝刀和眼在手深度相机，另一个负责抓取或搬运零件。", "result": "在3.5英寸硬盘的拆解实验中，eGRAP系统能够随着零件的拧松和移除而实时更新图和计划，实现了硬盘的完全拆解，成功率高，周期时间效率高。", "conclusion": "eGRAP方法能够适应性地实时协调双臂任务，为电子垃圾的自主拆解提供了有效解决方案。"}}
{"id": "2601.14790", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14790", "abs": "https://arxiv.org/abs/2601.14790", "authors": ["Zhi Qiu", "Jiazheng Sun", "Chenxiao Xia", "Jun Zheng", "Xin Peng"], "title": "CI4A: Semantic Component Interfaces for Agents Empowering Web Automation", "comment": "9 pages, 5 figures", "summary": "While Large Language Models demonstrate remarkable proficiency in high-level semantic planning, they remain limited in handling fine-grained, low-level web component manipulations. To address this limitation, extensive research has focused on enhancing model grounding capabilities through techniques such as Reinforcement Learning. However, rather than compelling agents to adapt to human-centric interfaces, we propose constructing interaction interfaces specifically optimized for agents. This paper introduces Component Interface for Agent (CI4A), a semantic encapsulation mechanism that abstracts the complex interaction logic of UI components into a set of unified tool primitives accessible to agents. We implemented CI4A within Ant Design, an industrial-grade front-end framework, covering 23 categories of commonly used UI components. Furthermore, we developed a hybrid agent featuring an action space that dynamically updates according to the page state, enabling flexible invocation of available CI4A tools. Leveraging the CI4A-integrated Ant Design, we refactored and upgraded the WebArena benchmark to evaluate existing SoTA methods. Experimental results demonstrate that the CI4A-based agent significantly outperforms existing approaches, achieving a new SoTA task success rate of 86.3%, alongside substantial improvements in execution efficiency.", "AI": {"tldr": "本文提出了一种名为CI4A的组件接口机制，用于增强大型语言模型在低级别Web组件操作上的能力，并通过优化交互接口而不是让模型适应接口，显著提高了基准测试中的任务成功率和效率。", "motivation": "现有的大型语言模型在处理细粒度的Web组件操作时存在局限性。虽然强化学习等技术被用于增强模型的接地能力，但本文认为，与其让智能体适应人类设计的界面，不如构建专门为智能体优化的交互界面。", "method": "提出CI4A（Component Interface for Agent）作为一种语义封装机制，将UI组件复杂的交互逻辑抽象为一组统一的工具原始操作，供智能体调用。CI4A在Ant Design框架中实现，覆盖了23类常用UI组件。同时，开发了一个混合智能体，其动作空间根据页面状态动态更新，以便灵活调用CI4A工具。使用CI4A集成的Ant Design对WebArena基准进行了重构和升级。", "result": "基于CI4A的智能体在WebArena基准测试中取得了86.3%的新SoTA任务成功率，显著优于现有方法，并在执行效率上也有大幅提升。", "conclusion": "CI4A是一种有效的机制，通过为智能体构建优化的交互接口，可以显著提升其在低级别Web组件操作上的能力，为更强大的Web自动化代理奠定了基础。"}}
{"id": "2601.14696", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.14696", "abs": "https://arxiv.org/abs/2601.14696", "authors": ["Zhaiyu Fang", "Ruipeng Sun"], "title": "AdaTIR: Adaptive Tool-Integrated Reasoning via Difficulty-Aware Policy Optimization", "comment": "under review", "summary": "Tool-Integrated Reasoning (TIR) has significantly enhanced the capabilities of Large Language Models (LLMs), yet current agents tend to exhibit cognitive offloading, redundantly invoking external tools even for simple tasks. In this paper, we suggest that true agentic intelligence requires not just tool invocation, but the adaptive wisdom to discern when to use them. We propose AdaTIR, a framework that shifts the paradigm from static tool invocation to difficulty-aware reasoning internalization. By introducing a difficulty-aware efficiency reward, AdaTIR dynamically adjusts tool budgets based on task complexity--internalizing reasoning for simple tasks while selectively invoking tools for complex tasks. Furthermore, we identify a sign reversal problem where tool penalties outweigh correctness rewards, mistakenly penalizing correct rollouts with negative advantages. To resolve this, we propose Clipped Advantage Shaping (CAS), which ensures that correctness remains the primary objective while using efficiency as a secondary constraint. Empirical results demonstrate that AdaTIR reduces tool calls by up to 97.6% on simple tasks and 28.2% on complex challenges while maintaining or enhancing accuracy. Notably, AdaTIR successfully internalizes reasoning, outperforming baselines by 4.8% on AIME 2024 even when tool access is strictly disabled.", "AI": {"tldr": "提出AdaTIR框架，通过引入难度感知奖励和Clipped Advantage Shaping (CAS)，解决LLM在工具集成推理中的认知卸载和奖励信号问题，显著减少工具调用次数并提高准确性，甚至在禁用工具时也能提升性能。", "motivation": "当前的大语言模型在工具集成推理（TIR）中存在认知卸载现象，即即使对于简单任务也会过度调用外部工具。研究者认为真正的智能体能力在于自适应地判断何时使用工具，而非盲目调用。", "method": "提出AdaTIR框架，通过难度感知奖励机制，根据任务复杂度动态调整工具调用预算，对于简单任务倾向于内部推理，复杂任务则选择性调用工具。为解决工具调用惩罚过高的问题，提出Clipped Advantage Shaping (CAS)，确保正确性是首要目标，效率是次要约束。", "result": "AdaTIR在简单任务上最多减少97.6%的工具调用，在复杂任务上减少28.2%的工具调用，同时保持或提高了准确性。在AIME 2024数据集上，即使禁用工具访问，AdaTIR的性能也比基线模型提高了4.8%。", "conclusion": "AdaTIR成功地实现了从静态工具调用到难度感知推理内化的范式转变，有效解决了LLM在工具使用中的效率和准确性问题，展现出更强的智能体能力。"}}
{"id": "2601.14615", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14615", "abs": "https://arxiv.org/abs/2601.14615", "authors": ["Xichen Zhang", "Ziyi He", "Yinghao Zhu", "Sitong Wu", "Shaozuo Yu", "Meng Chu", "Wenhu Zhang", "Haoru Tan", "Jiaya Jia"], "title": "SearchGym: Bootstrapping Real-World Search Agents via Cost-Effective and High-Fidelity Environment Simulation", "comment": null, "summary": "Search agents have emerged as a pivotal paradigm for solving open-ended, knowledge-intensive reasoning tasks. However, training these agents via Reinforcement Learning (RL) faces a critical dilemma: interacting with live commercial Web APIs is prohibitively expensive, while relying on static data snapshots often introduces noise due to data misalignment. This misalignment generates corrupted reward signals that destabilize training by penalizing correct reasoning or rewarding hallucination. To address this, we propose SearchGym, a simulation environment designed to bootstrap robust search agents. SearchGym employs a rigorous generative pipeline to construct a verifiable knowledge graph and an aligned document corpus, ensuring that every reasoning task is factually grounded and strictly solvable. Building on this controllable environment, we introduce SearchGym-RL, a curriculum learning methodology that progressively optimizes agent policies through purified feedback, evolving from basic interactions to complex, long-horizon planning. Extensive experiments across the Llama and Qwen families demonstrate strong Sim-to-Real generalization. Notably, our Qwen2.5-7B-Base model trained within SearchGym surpasses the web-enhanced ASearcher baseline across nine diverse benchmarks by an average relative margin of 10.6%. Our results validate that high-fidelity simulation serves as a scalable and highly cost-effective methodology for developing capable search agents.", "AI": {"tldr": "本文提出了SearchGym，一个用于训练搜索代理的仿真环境，通过生成事实性的知识图谱和文档语料库来解决强化学习训练中API交互成本高和数据失配的问题，并结合课程学习方法SearchGym-RL，提高了训练稳定性，并在Llama和Qwen模型上验证了其优越性。", "motivation": "现有的搜索代理在强化学习训练时面临挑战：直接与在线API交互成本高昂，而使用静态数据快照又可能因数据失配产生噪声，导致奖励信号不稳定，影响训练效果。", "method": "构建了一个称为SearchGym的仿真环境，该环境通过生成一个可验证的知识图谱和对齐的文档语料库来确保任务的可解性和事实性。在此基础上，提出了SearchGym-RL，一种课程学习方法，通过纯净的反馈逐步优化代理策略。", "result": "在Llama和Qwen模型系列上的实验表明，SearchGym展现了良好的Sim-to-Real泛化能力。使用SearchGym训练的Qwen2.5-7B-Base模型在九个基准测试中，平均超越了基线ASearcher 10.6%。", "conclusion": "高保真度的仿真是一种可扩展且具有成本效益的方法，能够有效地开发出强大的搜索代理。"}}
{"id": "2601.14594", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14594", "abs": "https://arxiv.org/abs/2601.14594", "authors": ["Lianying Chao", "Linfeng Yin", "Peiyu Ren", "Yifan Jiang", "Qiaoyu Ren", "Dingcheng Shan", "Jing-cheng Pang", "Sijie Wu", "Xubin Li", "Kai Zhang"], "title": "LFS: Learnable Frame Selector for Event-Aware and Temporally Diverse Video Captioning", "comment": null, "summary": "Video captioning models convert frames into visual tokens and generate descriptions with large language models (LLMs). Since encoding all frames is prohibitively expensive, uniform sampling is the default choice, but it enforces equal temporal coverage while ignoring the uneven events distribution. This motivates a Learnable Frame Selector (LFS) that selects temporally diverse and event-relevant frames. LFS explicitly models temporal importance to balance temporal diversity and event relevance, and employs a stratified strategy to ensure temporal coverage while avoiding clustering. Crucially, LFS leverages caption feedback from frozen video-LLMs to learn frame selection that directly optimizes downstream caption quality. Additionally, we identify the gap between existing benchmark and human's cognition. Thus, we introduce ICH-CC built from carefully designed questions by annotators that reflect human-consistent understanding of video. Experiments indicate that LFS consistently improves detailed video captioning across two representative community benchmarks and ICH-CC, achieving up to 2.0% gains on VDC and over 4% gains on ICH-CC. Moreover, we observe that enhanced captions with LFS leads to improved performance on video question answering. Overall, LFS provides an effective and easy-to-integrate solution for detailed video captioning.", "AI": {"tldr": "提出了一种可学习的帧选择器（LFS），用于为视频字幕模型选择时间上多样且事件相关的帧，以提高字幕质量，并引入了一个新数据集ICH-CC。", "motivation": "现有视频字幕模型为了效率通常采用均匀采样，但忽略了事件分布不均的问题，导致字幕质量受损。此外，现有基准测试与人类认知之间存在差距。", "method": "设计了一个LFS，通过显式建模时间重要性来平衡时间多样性和事件相关性，并采用分层策略确保时间覆盖且避免聚集。LFS利用冻结的视频大型语言模型的字幕反馈来学习帧选择。此外，创建了一个名为ICH-CC的新数据集，以反映人类一致的视频理解。", "result": "LFS在两个代表性基准和ICH-CC上显著提高了详细视频字幕的性能，在VDC上最多提升2.0%，在ICH-CC上提升超过4%。使用LFS增强的字幕还能提高视频问答的性能。", "conclusion": "LFS是一种有效且易于集成的解决方案，可以改进详细视频字幕，并且其产生的字幕也能提升视频问答的表现。ICH-CC数据集为评估视频理解提供了一个更符合人类认知的新视角。"}}
{"id": "2601.15006", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.15006", "abs": "https://arxiv.org/abs/2601.15006", "authors": ["Fumiya Ohnishi", "Masaki Takahashi"], "title": "DWPP: Dynamic Window Pure Pursuit Considering Velocity and Acceleration Constraints", "comment": "28 pages, 12 figures", "summary": "Pure pursuit and its variants are widely used for mobile robot path tracking owing to their simplicity and computational efficiency. However, many conventional approaches do not explicitly account for velocity and acceleration constraints, resulting in discrepancies between commanded and actual velocities that result in overshoot and degraded tracking performance. To address this problem, this paper proposes dynamic window pure pursuit (DWPP), which fundamentally reformulates the command velocity computation process to explicitly incorporate velocity and acceleration constraints. Specifically, DWPP formulates command velocity computation in the velocity space (the $v$-$ω$ plane) and selects the command velocity as the point within the dynamic window that is closest to the line $ω= κv$. Experimental results demonstrate that DWPP avoids constraint-violating commands and achieves superior path-tracking accuracy compared with conventional pure pursuit methods. The proposed method has been integrated into the official Nav2 repository and is publicly available (https://github.com/ros-navigation/navigation2).", "AI": {"tldr": "提出了一种名为动态窗口纯追踪（DWPP）的改进纯追踪路径跟踪算法，该算法显式考虑了速度和加速度约束，提高了机器人的路径跟踪精度。", "motivation": "传统的纯追踪方法在计算指令速度时没有明确考虑速度和加速度约束，这会导致指令速度与实际速度之间存在偏差，从而引起超调并降低跟踪性能。", "method": "DWPP 通过在速度空间（v-ω 平面）中重新定义指令速度的计算过程，显式地将速度和加速度约束纳入考量。具体来说，它选择动态窗口内最接近 ω=κv 直线（其中 κ 为曲率）的点作为指令速度。", "result": "实验结果表明，DWPP 能够避免违反约束的指令，并与传统的纯追踪方法相比，实现了更高的路径跟踪精度。", "conclusion": "DWPP 是一种有效的路径跟踪算法，通过显式处理速度和加速度约束，能够显著提高移动机器人的跟踪性能。该方法已集成到 Nav2 官方库中并公开可用。"}}
{"id": "2601.15018", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.15018", "abs": "https://arxiv.org/abs/2601.15018", "authors": ["Leon Tolksdorf", "Arturo Tejada", "Jonas Bauernfeind", "Christian Birkner", "Nathan van de Wouw"], "title": "Risk Estimation for Automated Driving", "comment": "10 pages, 5 figures", "summary": "Safety is a central requirement for automated vehicles. As such, the assessment of risk in automated driving is key in supporting both motion planning technologies and safety evaluation. In automated driving, risk is characterized by two aspects. The first aspect is the uncertainty on the state estimates of other road participants by an automated vehicle. The second aspect is the severity of a collision event with said traffic participants. Here, the uncertainty aspect typically causes the risk to be non-zero for near-collision events. This makes risk particularly useful for automated vehicle motion planning. Namely, constraining or minimizing risk naturally navigates the automated vehicle around traffic participants while keeping a safety distance based on the level of uncertainty and the potential severity of the impending collision. Existing approaches to calculate the risk either resort to empirical modeling or severe approximations, and, hence, lack generalizability and accuracy. In this paper, we combine recent advances in collision probability estimation with the concept of collision severity to develop a general method for accurate risk estimation. The proposed method allows us to assign individual severity functions for different collision constellations, such as, e.g., frontal or side collisions. Furthermore, we show that the proposed approach is computationally efficient, which is beneficial, e.g., in real-time motion planning applications. The programming code for an exemplary implementation of Gaussian uncertainties is also provided.", "AI": {"tldr": "本文提出了一种结合碰撞概率和碰撞严重性来估计自动驾驶风险的通用方法，该方法能够为不同碰撞情景分配独立的严重性函数，并具有计算效率，适用于实时运动规划。", "motivation": "现有的自动驾驶风险评估方法依赖于经验模型或近似计算，缺乏通用性和准确性，阻碍了运动规划和安全评估的发展。", "method": "结合了碰撞概率估计和碰撞严重性的概念，开发了一种通用的风险估计方法，允许为不同的碰撞情景（如正面或侧面碰撞）定义独立的严重性函数。", "result": "所提出的方法能够进行准确的风险估计，并且计算效率高，适用于实时运动规划。", "conclusion": "该方法为自动驾驶车辆的风险评估提供了一个准确且通用的解决方案，并提供了示例性实现的编程代码。"}}
{"id": "2601.14698", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.14698", "abs": "https://arxiv.org/abs/2601.14698", "authors": ["Michael Theologitis", "Preetam Prabhu Srikar Dammu", "Chirag Shah", "Dan Suciu"], "title": "ClaimDB: A Fact Verification Benchmark over Large Structured Data", "comment": "The data, code, and leaderboard are available at https://claimdb.github.io", "summary": "Despite substantial progress in fact-verification benchmarks, claims grounded in large-scale structured data remain underexplored. In this work, we introduce ClaimDB, the first fact-verification benchmark where the evidence for claims is derived from compositions of millions of records and multiple tables. ClaimDB consists of 80 unique real-life databases covering a wide range of domains, from governance and healthcare to media, education and the natural sciences. At this scale, verification approaches that rely on \"reading\" the evidence break down, forcing a timely shift toward reasoning in executable programs. We conduct extensive experiments with 30 state-of-the-art proprietary and open-source (below 70B) LLMs and find that none exceed 83% accuracy, with more than half below 55%. Our analysis also reveals that both closed- and open-source models struggle with abstention -- the ability to admit that there is no evidence to decide -- raising doubts about their reliability in high-stakes data analysis. We release the benchmark, code, and the LLM leaderboard at https://claimdb.github.io .", "AI": {"tldr": "本研究提出了ClaimDB，一个首个基于大规模结构化数据（由数百万条记录和多个表组成）的事实验证基准。现有的先进语言模型在此基准上表现不佳，准确率均低于83%，且在识别无证据可依据的“弃权”能力方面存在不足，表明其在高风险数据分析中的可靠性存疑。", "motivation": "现有事实验证基准主要关注文本证据，而基于大规模结构化数据的验证研究尚不充分。本研究旨在填补这一空白，并探索现有语言模型在处理结构化数据时的局限性。", "method": "构建了一个名为ClaimDB的基准，包含80个真实世界的数据库。使用30个最先进的专有和开源（70B以下）大型语言模型（LLMs）在ClaimDB上进行实验，评估其事实验证性能，特别关注其处理大规模结构化数据的能力和“弃权”能力。", "result": "没有一个LLM的准确率超过83%，超过半数模型的准确率低于55%。同时，无论是闭源还是开源模型，在“弃权”（承认无法根据现有证据做出判断）能力上都表现不佳，准确率不足50%。", "conclusion": "在处理由大规模结构化数据构成的验证任务时，当前最先进的大型语言模型表现出显著的局限性。它们在准确性和可靠性方面均存在不足，尤其是在需要承认信息不足的情况下，这对其在高风险数据分析中的应用提出了质疑。研究者已公开了基准、代码和模型排行榜。"}}
{"id": "2601.14625", "categories": ["cs.CV", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.14625", "abs": "https://arxiv.org/abs/2601.14625", "authors": ["Yingsong Huang", "Hui Guo", "Jing Huang", "Bing Bai", "Qi Xiong"], "title": "Diffusion Epistemic Uncertainty with Asymmetric Learning for Diffusion-Generated Image Detection", "comment": null, "summary": "The rapid progress of diffusion models highlights the growing need for detecting generated images. Previous research demonstrates that incorporating diffusion-based measurements, such as reconstruction error, can enhance the generalizability of detectors. However, ignoring the differing impacts of aleatoric and epistemic uncertainty on reconstruction error can undermine detection performance. Aleatoric uncertainty, arising from inherent data noise, creates ambiguity that impedes accurate detection of generated images. As it reflects random variations within the data (e.g., noise in natural textures), it does not help distinguish generated images. In contrast, epistemic uncertainty, which represents the model's lack of knowledge about unfamiliar patterns, supports detection. In this paper, we propose a novel framework, Diffusion Epistemic Uncertainty with Asymmetric Learning~(DEUA), for detecting diffusion-generated images. We introduce Diffusion Epistemic Uncertainty~(DEU) estimation via the Laplace approximation to assess the proximity of data to the manifold of diffusion-generated samples. Additionally, an asymmetric loss function is introduced to train a balanced classifier with larger margins, further enhancing generalizability. Extensive experiments on large-scale benchmarks validate the state-of-the-art performance of our method.", "AI": {"tldr": "本文提出了一种名为DEUA的新框架，用于检测扩散模型生成的图像。DEUA通过拉普拉斯近似估计扩散模型不确定性（DEU），并结合非对称损失函数来提高检测器的泛化能力，在大型基准测试中取得了最先进的性能。", "motivation": "以往的研究表明，利用扩散模型产生的测量值（如重建误差）可以提高生成图像检测器的泛化能力。然而，忽略了aleatoric不确定性和epistemic不确定性对重建误差的不同影响，会削弱检测性能。aleatoric不确定性（源于固有数据噪声）会产生歧义，阻碍生成图像的准确检测，而epistemic不确定性（反映模型对未知模式的知识缺乏）则有助于检测。", "method": "提出了一种名为DEUA的新框架。首先，通过拉普拉斯近似估计扩散模型不确定性（DEU），以评估数据与扩散生成样本流形之间的接近程度。其次，引入一个非对称损失函数来训练一个具有更大间隔的均衡分类器，以提高泛化能力。", "result": "在大型基准测试上进行了广泛的实验，验证了该方法达到了最先进的性能。", "conclusion": "DEUA框架通过引入DEU估计和非对称损失函数，能够更有效地检测扩散模型生成的图像，并显著提高检测器的泛化能力。"}}
{"id": "2601.14840", "categories": ["cs.AI", "cs.RO", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.14840", "abs": "https://arxiv.org/abs/2601.14840", "authors": ["Abdelrhman Bassiouny", "Tom Schierenbeck", "Sorin Arion", "Benjamin Alt", "Naren Vasantakumaar", "Giang Nguyen", "Michael Beetz"], "title": "Implementing Knowledge Representation and Reasoning with Object Oriented Design", "comment": "9 pages, 2 figures, submitted to the 2026 International Joint Conference on Artificial Intelligence (IJCAI)", "summary": "This paper introduces KRROOD, a framework designed to bridge the integration gap between modern software engineering and Knowledge Representation & Reasoning (KR&R) systems. While Object-Oriented Programming (OOP) is the standard for developing complex applications, existing KR&R frameworks often rely on external ontologies and specialized languages that are difficult to integrate with imperative code. KRROOD addresses this by treating knowledge as a first-class programming abstraction using native class structures, bridging the gap between the logic programming and OOP paradigms. We evaluate the system on the OWL2Bench benchmark and a human-robot task learning scenario. Experimental results show that KRROOD achieves strong performance while supporting the expressive reasoning required for real-world autonomous systems.", "AI": {"tldr": "KRROOD 是一个新框架，它将知识表示和推理（KR&R）系统与面向对象编程（OOP）集成，通过将知识视为一等编程抽象，使用原生类结构来弥合逻辑编程和 OOP 之间的差距。", "motivation": "现有 KR&R 框架与面向对象编程（OOP）集成困难，它们通常依赖难以与命令式代码集成的外部本体和专用语言。", "method": "KRROOD 将知识作为一等编程抽象，使用原生类结构，在逻辑编程和 OOP 范式之间建立桥梁。在 OWL2Bench 基准测试和机器人任务学习场景中进行了评估。", "result": "KRROOD 在 OWL2Bench 基准测试和人类-机器人任务学习场景中取得了强劲的性能，同时支持了现实世界自主系统所需的表达性推理。", "conclusion": "KRROOD 有效地弥合了现代软件工程（特别是 OOP）与 KR&R 系统之间的集成差距，并在实际应用中展现了良好的性能和推理能力。"}}
{"id": "2601.14700", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.14700", "abs": "https://arxiv.org/abs/2601.14700", "authors": ["Chongxuan Huang", "Lei Lin", "Xiaodong Shi", "Wenping Hu", "Ruiming Tang"], "title": "DARL: Encouraging Diverse Answers for General Reasoning without Verifiers", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has demonstrated promising gains in enhancing the reasoning capabilities of large language models. However, its dependence on domain-specific verifiers significantly restricts its applicability to open and general domains. Recent efforts such as RLPR have extended RLVR to general domains, enabling training on broader datasets and achieving improvements over RLVR. However, a notable limitation of these methods is their tendency to overfit to reference answers, which constrains the model's ability to generate diverse outputs. This limitation is particularly pronounced in open-ended tasks such as writing, where multiple plausible answers exist. To address this, we propose DARL, a simple yet effective reinforcement learning framework that encourages the generation of diverse answers within a controlled deviation range from the reference while preserving alignment with it. Our framework is fully compatible with existing general reinforcement learning methods and can be seamlessly integrated without additional verifiers. Extensive experiments on thirteen benchmarks demonstrate consistent improvements in reasoning performance. Notably, DARL surpasses RLPR, achieving average gains of 1.3 points on six reasoning benchmarks and 9.5 points on seven general benchmarks, highlighting its effectiveness in improving both reasoning accuracy and output diversity.", "AI": {"tldr": "本文提出了一种名为DARL的强化学习框架，旨在解决现有方法在通用领域中过度拟合参考答案的问题，从而提高生成答案的多样性，同时保持与参考答案的一致性。DARL框架可以与现有通用强化学习方法集成，无需额外验证器，并在多个基准测试中取得了显著的性能提升。", "motivation": "现有基于可验证奖励的强化学习（RLVR）方法依赖于领域特定的验证器，限制了其在通用领域的应用。尽管RLPR等方法扩展到了通用领域，但它们容易过度拟合参考答案，限制了生成结果的多样性，尤其是在开放式任务中。因此，需要一种新的方法来解决这个问题。", "method": "提出DARL（Diverse Answer Reinforcement Learning）框架。DARL鼓励在与参考答案保持一致的前提下，生成在一定偏差范围内的多样化答案。该框架与现有的通用强化学习方法兼容，无需额外验证器。", "result": "在13个基准测试上的大量实验表明，DARL在推理性能上持续提升。与RLPR相比，DARL在6个推理基准上的平均提升了1.3分，在7个通用基准上的平均提升了9.5分。", "conclusion": "DARL框架能够有效提高模型的推理准确性和输出多样性，克服了现有方法在通用领域中过度拟合参考答案的限制，并且易于集成。"}}
{"id": "2601.15025", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.15025", "abs": "https://arxiv.org/abs/2601.15025", "authors": ["Marian Renz", "Martin Günther", "Felix Igelbrink", "Oscar Lima", "Martin Atzmueller"], "title": "ExPrIS: Knowledge-Level Expectations as Priors for Object Interpretation from Sensor Data", "comment": "This preprint has not undergone peer review or any post-submission improvements or corrections. The Version of Record of this article is published in KI - Künstliche Intelligenz, and is available online at https://doi.org/10.1007/s13218-026-00901-7", "summary": "While deep learning has significantly advanced robotic object recognition, purely data-driven approaches often lack semantic consistency and fail to leverage valuable, pre-existing knowledge about the environment. This report presents the ExPrIS project, which addresses this challenge by investigating how knowledge-level expectations can serve as to improve object interpretation from sensor data. Our approach is based on the incremental construction of a 3D Semantic Scene Graph (3DSSG). We integrate expectations from two sources: contextual priors from past observations and semantic knowledge from external graphs like ConceptNet. These are embedded into a heterogeneous Graph Neural Network (GNN) to create an expectation-biased inference process. This method moves beyond static, frame-by-frame analysis to enhance the robustness and consistency of scene understanding over time. The report details this architecture, its evaluation, and outlines its planned integration on a mobile robotic platform.", "AI": {"tldr": "ExPrIS项目提出了一种基于3D语义场景图（3DSSG）的方法，通过融合上下文先验和外部知识图谱（如ConceptNet）中的期望，利用异构图神经网络（GNN）来提高机器人从传感器数据中进行对象识别的鲁棒性和语义一致性，并计划集成到移动机器人平台。", "motivation": "纯粹的数据驱动的深度学习方法在机器人对象识别方面缺乏语义一致性，并且未能充分利用环境中已有的知识。研究的动机是探索知识层面的期望如何改进从传感器数据中进行的对象解释。", "method": "该方法基于增量构建3D语义场景图（3DSSG）。通过两种方式集成期望：1）来自过去观察的上下文先验；2）来自外部知识图谱（如ConceptNet）的语义知识。这些期望被嵌入到一个异构图神经网络（GNN）中，以创建一个期望偏置的推理过程。", "result": "该方法能够超越静态的、逐帧的分析，从而提高场景理解的鲁棒性和时序一致性。报告详细介绍了该架构及其评估。", "conclusion": "ExPrIS项目通过整合先验知识和外部语义知识来改进机器人对象的解释，从而克服了纯数据驱动方法的局限性，并为未来在移动机器人平台上的集成奠定了基础。"}}
{"id": "2601.14637", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.14637", "abs": "https://arxiv.org/abs/2601.14637", "authors": ["James Brock", "Ce Zhang", "Nantheera Anantrasirichai"], "title": "Forest-Chat: Adapting Vision-Language Agents for Interactive Forest Change Analysis", "comment": "22 pages, 8 figures, 7 tables, Submitted to Ecological Informatics", "summary": "The increasing availability of high-resolution satellite imagery, together with advances in deep learning, creates new opportunities for enhancing forest monitoring workflows. Two central challenges in this domain are pixel-level change detection and semantic change interpretation, particularly for complex forest dynamics. While large language models (LLMs) are increasingly adopted for data exploration, their integration with vision-language models (VLMs) for remote sensing image change interpretation (RSICI) remains underexplored, especially beyond urban environments. We introduce Forest-Chat, an LLM-driven agent designed for integrated forest change analysis. The proposed framework enables natural language querying and supports multiple RSICI tasks, including change detection, change captioning, object counting, deforestation percentage estimation, and change reasoning. Forest-Chat builds upon a multi-level change interpretation (MCI) vision-language backbone with LLM-based orchestration, and incorporates zero-shot change detection via a foundation change detection model together with an interactive point-prompt interface to support fine-grained user guidance. To facilitate adaptation and evaluation in forest environments, we introduce the Forest-Change dataset, comprising bi-temporal satellite imagery, pixel-level change masks, and multi-granularity semantic change captions generated through a combination of human annotation and rule-based methods. Experimental results demonstrate that Forest-Chat achieves strong performance on Forest-Change and on LEVIR-MCI-Trees, a tree-focused subset of LEVIR-MCI, for joint change detection and captioning, highlighting the potential of interactive, LLM-driven RSICI systems to improve accessibility, interpretability, and analytical efficiency in forest change analysis.", "AI": {"tldr": "本研究提出了Forest-Chat，一个基于大语言模型（LLM）的智能体，用于集成森林变化分析，支持自然语言查询和多种遥感图像变化解释任务，并引入了Forest-Change数据集进行评估。", "motivation": "现有森林监测工作流程需要改进，尤其是在像素级变化检测和复杂森林动态的语义变化解释方面。尽管LLMs在数据探索中日益普及，但其与视觉-语言模型（VLMs）在遥感图像变化解释（RSICI）方面的结合，尤其是在非城市环境中，仍未得到充分探索。", "method": "研究提出Forest-Chat框架，它基于多层次变化解释（MCI）的视觉-语言骨干网络，并由LLM进行编排。该框架支持零样本变化检测，并集成了一个交互式点提示界面以实现精细的用户指导。此外，研究还创建了Forest-Change数据集，包含双时相卫星图像、像素级变化掩码和多粒度语义变化描述。", "result": "实验结果表明，Forest-Chat在Forest-Change数据集和LEVIR-MCI-Trees数据集上，在联合变化检测和图像描述方面表现出色。这证明了交互式、LLM驱动的RSICI系统在提高森林变化分析的可访问性、可解释性和分析效率方面的潜力。", "conclusion": "Forest-Chat展示了LLM驱动的交互式系统在处理复杂森林变化分析方面的强大能力，能够支持多种RSICI任务，并有望革新森林监测方法。"}}
{"id": "2601.14894", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14894", "abs": "https://arxiv.org/abs/2601.14894", "authors": ["Nicolas Lazzari", "Valentina Presutti", "Antonio Vergari"], "title": "To Neuro-Symbolic Classification and Beyond by Compiling Description Logic Ontologies to Probabilistic Circuits", "comment": "Manuscript under review", "summary": "Background: Neuro-symbolic methods enhance the reliability of neural network classifiers through logical constraints, but they lack native support for ontologies.\n  Objectives: We aim to develop a neuro-symbolic method that reliably outputs predictions consistent with a Description Logic ontology that formalizes domain-specific knowledge.\n  Methods: We encode a Description Logic ontology as a circuit, a feed-forward differentiable computational graph that supports tractable execution of queries and transformations. We show that the circuit can be used to (i) generate synthetic datasets that capture the semantics of the ontology; (ii) efficiently perform deductive reasoning on a GPU; (iii) implement neuro-symbolic models whose predictions are approximately or provably consistent with the knowledge defined in the ontology.\n  Results We show that the synthetic dataset generated using the circuit qualitatively captures the semantics of the ontology while being challenging for Machine Learning classifiers, including neural networks. Moreover, we show that compiling the ontology into a circuit is a promising approach for scalable deductive reasoning, with runtimes up to three orders of magnitude faster than available reasoners. Finally, we show that our neuro-symbolic classifiers reliably produce consistent predictions when compared to neural network baselines, maintaining competitive performances or even outperforming them.\n  Conclusions By compiling Description Logic ontologies into circuits, we obtain a tighter integration between the Deep Learning and Knowledge Representation fields. We show that a single circuit representation can be used to tackle different challenging tasks closely related to real-world applications.", "AI": {"tldr": "本研究提出了一种将描述逻辑本体编译成电路（可微分计算图）的神经符号方法，用于生成更具挑战性的合成数据集、加速演绎推理，并构建能够生成与本体一致的预测的神经符号分类器，从而实现了深度学习和知识表示领域的更紧密结合。", "motivation": "现有的神经符号方法在增强神经网络分类器的可靠性方面有所欠缺，尤其是在缺乏对本体的本地支持时。因此，研究目标是开发一种能够可靠地输出与领域知识形式化的描述逻辑本体一致的预测的神经符号方法。", "method": "研究方法是将描述逻辑本体编译成一个电路（前馈可微分计算图）。该电路能够（i）生成捕捉本体语义的合成数据集；（ii）在GPU上高效执行演绎推理；（iii）实现神经符号模型，其预测可以近似或可证明地与本体定义的知识保持一致。", "result": "研究结果表明，通过电路生成的合成数据集能够很好地捕捉本体语义，并且对机器学习分类器（包括神经网络）构成挑战。此外，将本体编译成电路在可扩展演绎推理方面展现出潜力，推理速度比现有推理器快达三个数量级。最终，所提出的神经符号分类器能够可靠地生成与本体一致的预测，并且在性能上与基线神经网络相当，甚至有所超越。", "conclusion": "通过将描述逻辑本体编译成电路，本研究实现了深度学习和知识表示领域的更紧密集成。研究证明，单一的电路表示可以用于解决与实际应用密切相关的多种挑战性任务。"}}
{"id": "2601.14722", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.14722", "abs": "https://arxiv.org/abs/2601.14722", "authors": ["Surapon Nonesung", "Natapong Nitarach", "Teetouch Jaknamon", "Pittawat Taveekitworachai", "Kunat Pipatanakul"], "title": "Typhoon OCR: Open Vision-Language Model For Thai Document Extraction", "comment": null, "summary": "Document extraction is a core component of digital workflows, yet existing vision-language models (VLMs) predominantly favor high-resource languages. Thai presents additional challenges due to script complexity from non-latin letters, the absence of explicit word boundaries, and the prevalence of highly unstructured real-world documents, limiting the effectiveness of current open-source models. This paper presents Typhoon OCR, an open VLM for document extraction tailored for Thai and English. The model is fine-tuned from vision-language backbones using a Thai-focused training dataset. The dataset is developed using a multi-stage data construction pipeline that combines traditional OCR, VLM-based restructuring, and curated synthetic data. Typhoon OCR is a unified framework capable of text transcription, layout reconstruction, and document-level structural consistency. The latest iteration of our model, Typhoon OCR V1.5, is a compact and inference-efficient model designed to reduce reliance on metadata and simplify deployment. Comprehensive evaluations across diverse Thai document categories, including financial reports, government forms, books, infographics, and handwritten documents, show that Typhoon OCR achieves performance comparable to or exceeding larger frontier proprietary models, despite substantially lower computational cost. The results demonstrate that open vision-language OCR models can achieve accurate text extraction and layout reconstruction for Thai documents, reaching performance comparable to proprietary systems while remaining lightweight and deployable.", "AI": {"tldr": "本文提出了 Typhoon OCR，一个针对泰语和英语文档提取的开源视觉语言模型（VLM）。该模型通过在泰语为中心的数据集上进行微调，克服了泰语脚本复杂、词语无边界等挑战，并能实现文本转录、布局重建和结构一致性。Typhoon OCR V1.5 紧凑且推理高效，在多种泰语文档类型上表现优于或媲美大型专有模型，计算成本低，证明了开源 VLM 在泰语文档处理上的潜力。", "motivation": "现有视觉语言模型（VLMs）对高资源语言偏爱，而泰语因其复杂的脚本、缺乏词语边界以及真实世界文档的非结构化特性，面临严峻挑战，限制了现有开源模型在泰语文档提取上的效果。", "method": "本文提出了 Typhoon OCR，一个开源 VLM，从视觉语言骨干网络进行微调。训练数据采用了多阶段数据构建流程，结合了传统 OCR、VLM 重构和精选的合成数据。Typhoon OCR V1.5 是一个紧凑且推理高效的模型。", "result": "Typhoon OCR 在财务报告、政府表格、书籍、信息图和手写文档等多种泰语文档类型上进行了全面评估。结果显示，Typhoon OCR 实现了与大型专有模型相当甚至更优的性能，但计算成本却显著降低。", "conclusion": "开源视觉语言 OCR 模型可以实现泰语文档的准确文本提取和布局重建，性能可与专有系统媲美，同时保持轻量级和可部署性。"}}
{"id": "2601.14827", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14827", "abs": "https://arxiv.org/abs/2601.14827", "authors": ["Ben Schaper", "Maxime Di Folco", "Bernhard Kainz", "Julia A. Schnabel", "Cosmin I. Bercea"], "title": "Measuring and Aligning Abstraction in Vision-Language Models with Medical Taxonomies", "comment": null, "summary": "Vision-Language Models show strong zero-shot performance for chest X-ray classification, but standard flat metrics fail to distinguish between clinically minor and severe errors. This work investigates how to quantify and mitigate abstraction errors by leveraging medical taxonomies. We benchmark several state-of-the-art VLMs using hierarchical metrics and introduce Catastrophic Abstraction Errors to capture cross-branch mistakes. Our results reveal substantial misalignment of VLMs with clinical taxonomies despite high flat performance. To address this, we propose risk-constrained thresholding and taxonomy-aware fine-tuning with radial embeddings, which reduce severe abstraction errors to below 2 per cent while maintaining competitive performance. These findings highlight the importance of hierarchical evaluation and representation-level alignment for safer and more clinically meaningful deployment of VLMs.", "AI": {"tldr": "本研究提出了在医学影像（胸部X光片）分类任务中，使用分层指标来评估和减轻视觉语言模型（VLM）的抽象错误，以解决标准指标无法区分细微和严重错误的问题。研究发现VLM在临床分类体系上存在显著不一致，并提出了一种名为“灾难性抽象错误”的新指标。为此，研究者开发了风险约束阈值和基于径向嵌入的分类体系感知微调方法，将严重抽象错误降低至2%以下，同时保持原有性能。", "motivation": "标准评估指标无法区分视觉语言模型（VLM）在胸部X光片分类任务中细微错误和严重错误，这限制了其在临床上的安全可靠应用。因此，研究者希望通过利用医学分类体系来量化和减轻这些抽象错误。", "method": "1. 使用分层指标（hierarchical metrics）来评估几种先进的VLM模型在胸部X光片分类任务上的表现。2. 引入“灾难性抽象错误”（Catastrophic Abstraction Errors）来捕捉跨分支的错误。3. 提出风险约束阈值（risk-constrained thresholding）和分类体系感知微调（taxonomy-aware fine-tuning）的方法，并结合径向嵌入（radial embeddings）。", "result": "尽管VLM在标准（扁平）指标上表现出高精度，但其在与临床分类体系的对齐方面存在显著不足。所提出的风险约束阈值和分类体系感知微调方法能将严重的抽象错误降低到2%以下，同时保持具有竞争力的整体性能。", "conclusion": "使用分层评估方法和进行表示层面的对齐对于更安全、更符合临床意义的VLM部署至关重要。研究证明了其提出的方法可以有效地解决VLM在医学影像分类中的抽象错误问题。"}}
{"id": "2601.15039", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.15039", "abs": "https://arxiv.org/abs/2601.15039", "authors": ["Jiyao Zhang", "Zhiyuan Ma", "Tianhao Wu", "Zeyuan Chen", "Hao Dong"], "title": "CADGrasp: Learning Contact and Collision Aware General Dexterous Grasping in Cluttered Scenes", "comment": null, "summary": "Dexterous grasping in cluttered environments presents substantial challenges due to the high degrees of freedom of dexterous hands, occlusion, and potential collisions arising from diverse object geometries and complex layouts. To address these challenges, we propose CADGrasp, a two-stage algorithm for general dexterous grasping using single-view point cloud inputs. In the first stage, we predict sparse IBS, a scene-decoupled, contact- and collision-aware representation, as the optimization target. Sparse IBS compactly encodes the geometric and contact relationships between the dexterous hand and the scene, enabling stable and collision-free dexterous grasp pose optimization. To enhance the prediction of this high-dimensional representation, we introduce an occupancy-diffusion model with voxel-level conditional guidance and force closure score filtering. In the second stage, we develop several energy functions and ranking strategies for optimization based on sparse IBS to generate high-quality dexterous grasp poses. Extensive experiments in both simulated and real-world settings validate the effectiveness of our approach, demonstrating its capability to mitigate collisions while maintaining a high grasp success rate across diverse objects and complex scenes.", "AI": {"tldr": "本文提出了一种名为CADGrasp的两阶段算法，用于处理单视图点云输入的杂乱环境下的灵巧抓取问题，通过预测一种名为稀疏IBS的场景解耦、接触和碰撞感知的表示，并结合能量函数和排序策略来优化抓取姿态，有效解决了碰撞问题并提高了抓取成功率。", "motivation": "杂乱环境中灵巧抓取因手的高自由度、遮挡和潜在碰撞而面临重大挑战。", "method": "提出CADGrasp两阶段算法：第一阶段预测稀疏IBS（一种场景解耦、接触和碰撞感知的表示），采用基于体素条件引导和力闭合分数过滤的占用扩散模型进行预测；第二阶段基于稀疏IBS开发能量函数和排序策略来优化抓取姿态。", "result": "在模拟和真实世界环境中进行的广泛实验验证了该方法的有效性，表明其能有效缓解碰撞并保持高抓取成功率。", "conclusion": "CADGrasp通过预测稀疏IBS表示并进行优化，能够有效地处理杂乱环境下的灵巧抓取问题，显著减少碰撞并提高抓取成功率。"}}
{"id": "2601.14651", "categories": ["cs.CV", "cs.MM", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.14651", "abs": "https://arxiv.org/abs/2601.14651", "authors": ["Chenglizhao Chen", "Boze Li", "Mengke Song", "Dehao Feng", "Xinyu Liu", "Shanchen Pang", "Jufeng Yang", "Hui Yu"], "title": "READ-Net: Clarifying Emotional Ambiguity via Adaptive Feature Recalibration for Audio-Visual Depression Detection", "comment": "12 pages", "summary": "Depression is a severe global mental health issue that impairs daily functioning and overall quality of life. Although recent audio-visual approaches have improved automatic depression detection, methods that ignore emotional cues often fail to capture subtle depressive signals hidden within emotional expressions. Conversely, those incorporating emotions frequently confuse transient emotional expressions with stable depressive symptoms in feature representations, a phenomenon termed \\emph{Emotional Ambiguity}, thereby leading to detection errors. To address this critical issue, we propose READ-Net, the first audio-visual depression detection framework explicitly designed to resolve Emotional Ambiguity through Adaptive Feature Recalibration (AFR). The core insight of AFR is to dynamically adjust the weights of emotional features to enhance depression-related signals. Rather than merely overlooking or naively combining emotional information, READ-Net innovatively identifies and preserves depressive-relevant cues within emotional features, while adaptively filtering out irrelevant emotional noise. This recalibration strategy significantly clarifies feature representations, and effectively mitigates the persistent challenge of emotional interference. Additionally, READ-Net can be easily integrated into existing frameworks for improved performance. Extensive evaluations on three publicly available datasets show that READ-Net outperforms state-of-the-art methods, with average gains of 4.55\\% in accuracy and 1.26\\% in F1-score, demonstrating its robustness to emotional disturbances and improving audio-visual depression detection.", "AI": {"tldr": "本文提出了一种名为READ-Net的音频-视觉抑郁检测框架，通过自适应特征再校准（AFR）来解决情绪模糊问题，提高了抑郁检测的准确性和F1分数。", "motivation": "现有的音频-视觉抑郁检测方法在处理情绪线索时存在挑战，容易混淆短暂的情绪表达与稳定的抑郁症状，导致“情绪模糊”问题，影响检测精度。作者旨在解决这一问题，提高抑郁检测的鲁棒性。", "method": "提出READ-Net框架，核心是自适应特征再校准（AFR）模块。AFR通过动态调整情绪特征的权重，增强抑郁相关信号，同时过滤掉不相关的“情绪噪音”，从而澄清特征表示并减轻情绪干扰。", "result": "在三个公开数据集上的实验表明，READ-Net在平均准确率上提高了4.55%，F1分数提高了1.26%，优于现有最先进的方法，并展示了其对情绪干扰的鲁棒性。", "conclusion": "READ-Net是一种新颖的音频-视觉抑郁检测框架，通过AFR有效解决了情绪模糊问题，显著提高了抑郁检测性能，并且易于集成到现有框架中以提升效果。"}}
{"id": "2601.14750", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14750", "abs": "https://arxiv.org/abs/2601.14750", "authors": ["Yifan Wang", "Shiyu Li", "Peiming Li", "Xiaochen Yang", "Yang Tang", "Zheng Wei"], "title": "Render-of-Thought: Rendering Textual Chain-of-Thought as Images for Visual Latent Reasoning", "comment": null, "summary": "Chain-of-Thought (CoT) prompting has achieved remarkable success in unlocking the reasoning capabilities of Large Language Models (LLMs). Although CoT prompting enhances reasoning, its verbosity imposes substantial computational overhead. Recent works often focus exclusively on outcome alignment and lack supervision on the intermediate reasoning process. These deficiencies obscure the analyzability of the latent reasoning chain. To address these challenges, we introduce Render-of-Thought (RoT), the first framework to reify the reasoning chain by rendering textual steps into images, making the latent rationale explicit and traceable. Specifically, we leverage the vision encoders of existing Vision Language Models (VLMs) as semantic anchors to align the vision embeddings with the textual space. This design ensures plug-and-play implementation without incurring additional pre-training overhead. Extensive experiments on mathematical and logical reasoning benchmarks demonstrate that our method achieves 3-4x token compression and substantial inference acceleration compared to explicit CoT. Furthermore, it maintains competitive performance against other methods, validating the feasibility of this paradigm. Our code is available at https://github.com/TencentBAC/RoT", "AI": {"tldr": "本文提出了一种名为Render-of-Thought (RoT) 的新框架，通过将语言模型的推理步骤转化为图像，使推理过程可视化并易于追踪，从而解决了Chain-of-Thought (CoT) 提示的计算开销和可解释性问题。", "motivation": "Chain-of-Thought (CoT) 提示虽然能提升大型语言模型 (LLM) 的推理能力，但其冗长的文本会带来显著的计算开销。同时，现有的方法往往只关注最终结果的一致性，而忽略了对中间推理过程的监督，这使得模型的潜在推理链难以分析。", "method": "RoT 框架通过将文本形式的推理步骤渲染成图像，从而显化推理链。它利用现有的视觉语言模型 (VLM) 的视觉编码器作为语义锚点，将图像嵌入与文本空间对齐，实现了即插即用的功能，无需额外的预训练。", "result": "在数学和逻辑推理基准测试上的实验表明，RoT 方法实现了3-4倍的 token 压缩，推理速度有显著提升，同时在性能上与其他方法相比具有竞争力。", "conclusion": "RoT 框架成功地将 LLM 的推理过程可视化，提高了可解释性和可追溯性，并且在不牺牲性能的情况下显著降低了计算开销，验证了这一范式是可行的。"}}
{"id": "2601.14901", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14901", "abs": "https://arxiv.org/abs/2601.14901", "authors": ["Nadine Meertens", "Suet Lee", "Ophelia Deroy"], "title": "Just aware enough: Evaluating awareness across artificial systems", "comment": "24 pages (including references), 1 figure", "summary": "Recent debates on artificial intelligence increasingly emphasise questions of AI consciousness and moral status, yet there remains little agreement on how such properties should be evaluated. In this paper, we argue that awareness offers a more productive and methodologically tractable alternative. We introduce a practical method for evaluating awareness across diverse systems, where awareness is understood as encompassing a system's abilities to process, store and use information in the service of goal-directed action. Central to this approach is the claim that any evaluation aiming to capture the diversity of artificial systems must be domain-sensitive, deployable at any scale, multidimensional, and enable the prediction of task performance, while generalising to the level of abilities for the sake of comparison. Given these four desiderata, we outline a structured approach to evaluating and comparing awareness profiles across artificial systems with differing architectures, scales, and operational domains. By shifting the focus from artificial consciousness to being just aware enough, this approach aims to facilitate principled assessment, support design and oversight, and enable more constructive scientific and public discourse.", "AI": {"tldr": "该论文提出了一种评估人工智能系统“意识”（awareness）水平的方法，认为这比评估“意识”（consciousness）更具实践性和可操作性，并提供了四项评估原则：领域敏感性、可扩展性、多维度性和任务性能预测能力。", "motivation": "当前关于人工智能意识和道德地位的争论缺乏统一的评估方法。作者认为“意识”（awareness）是一个更实用且易于处理的替代概念。", "method": "提出了一种评估“意识”（awareness）的实践方法，该方法将意识定义为系统处理、存储和利用信息以服务于目标导向行为的能力。该方法强调评估的领域敏感性、可扩展性、多维度性以及预测任务性能和泛化能力。", "result": "提出了一个结构化的评估框架，用于跨不同架构、规模和操作领域的 AI 系统评估和比较其“意识”（awareness）的配置。该方法能够促进对 AI 系统能力的原则性评估。", "conclusion": "通过将焦点从“人工智能意识”（artificial consciousness）转移到“足够有意识”（being just aware enough），该方法旨在促进原则性评估、支持设计和监管，并实现更具建设性的科学和公众讨论。"}}
{"id": "2601.14671", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14671", "abs": "https://arxiv.org/abs/2601.14671", "authors": ["Yonghao Yu", "Lang Huang", "Zerun Wang", "Runyi Li", "Toshihiko Yamasaki"], "title": "Mirai: Autoregressive Visual Generation Needs Foresight", "comment": null, "summary": "Autoregressive (AR) visual generators model images as sequences of discrete tokens and are trained with next token likelihood. This strict causality supervision optimizes each step only by its immediate next token, which diminishes global coherence and slows convergence. We ask whether foresight, training signals that originate from later tokens, can help AR visual generation. We conduct a series of controlled diagnostics along the injection level, foresight layout, and foresight source axes, unveiling a key insight: aligning foresight to AR models' internal representation on the 2D image grids improves causality modeling. We formulate this insight with Mirai (meaning \"future\" in Japanese), a general framework that injects future information into AR training with no architecture change and no extra inference overhead: Mirai-E uses explicit foresight from multiple future positions of unidirectional representations, whereas Mirai-I leverages implicit foresight from matched bidirectional representations. Extensive experiments show that Mirai significantly accelerates convergence and improves generation quality. For instance, Mirai can speed up LlamaGen-B's convergence by up to 10$\\times$ and reduce the generation FID from 5.34 to 4.34 on the ImageNet class-condition image generation benchmark. Our study highlights that visual autoregressive models need foresight.", "AI": {"tldr": "本文提出了一种名为Mirai的通用框架，通过引入“前瞻性”训练信号来改进自回归（AR）视觉生成模型，从而加速收敛并提升生成质量，无需改变模型架构或增加推理开销。", "motivation": "现有的自回归视觉生成模型仅依赖于当前生成的下一个离散标记的似然进行训练，这种严格的因果监督会限制全局一致性并减缓收敛速度。研究者希望通过引入“前瞻性”（来自未来标记的训练信号）来解决这些问题。", "method": "作者提出了一种名为Mirai的通用框架，该框架将未来信息注入到AR模型的训练中。Mirai有两种实现方式：Mirai-E显式地利用来自单向表示的多个未来位置的前瞻信息；Mirai-I则利用匹配的双向表示的隐式前瞻信息。实验在注入层级、前瞻布局和前瞻来源等方面进行了控制。", "result": "实验表明，Mirai能够显著加速AR视觉生成模型的收敛速度并提高生成质量。例如，在ImageNet条件生成任务上，Mirai可以将LlamaGen-B的收敛速度提高10倍，并将生成FID从5.34降低到4.34。", "conclusion": "研究表明，视觉自回归模型需要引入“前瞻性”的训练信号才能实现更好的性能。Mirai框架通过将前瞻信息注入到AR模型的内部表示中，有效地改善了因果建模，从而带来了收敛加速和质量提升。"}}
{"id": "2601.14780", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14780", "abs": "https://arxiv.org/abs/2601.14780", "authors": ["Anqi Li", "Yuqian Chen", "Yu Lu", "Zhaoming Chen", "Yuan Xie", "Zhenzhong Lan"], "title": "RECAP: Resistance Capture in Text-based Mental Health Counseling with Large Language Models", "comment": "19 pages, 2 figures", "summary": "Recognizing and navigating client resistance is critical for effective mental health counseling, yet detecting such behaviors is particularly challenging in text-based interactions. Existing NLP approaches oversimplify resistance categories, ignore the sequential dynamics of therapeutic interventions, and offer limited interpretability.\n  To address these limitations, we propose PsyFIRE, a theoretically grounded framework capturing 13 fine-grained resistance behaviors alongside collaborative interactions. Based on PsyFIRE, we construct the ClientResistance corpus with 23,930 annotated utterances from real-world Chinese text-based counseling, each supported by context-specific rationales. Leveraging this dataset, we develop RECAP, a two-stage framework that detects resistance and fine-grained resistance types with explanations.\n  RECAP achieves 91.25% F1 for distinguishing collaboration and resistance and 66.58% macro-F1 for fine-grained resistance categories classification, outperforming leading prompt-based LLM baselines by over 20 points. Applied to a separate counseling dataset and a pilot study with 62 counselors, RECAP reveals the prevalence of resistance, its negative impact on therapeutic relationships and demonstrates its potential to improve counselors' understanding and intervention strategies.", "AI": {"tldr": "本研究提出了一个名为PsyFIRE的理论框架，用于识别和分类心理咨询中的13种细粒度客户抵抗行为。基于此框架构建了包含23,930条标注会话的ClientResistance语料库，并开发了一个名为RECAP的两阶段检测框架，该框架能够检测抵抗行为并提供可解释的分类结果，在细粒度抵抗分类方面取得了显著的性能提升，并表明其在实际应用中能够帮助咨询师更好地理解和干预客户的抵抗行为。", "motivation": "现有的自然语言处理方法在文本咨询中识别客户抵抗行为方面存在不足，包括过于简化的分类、忽略治疗干预的顺序动态以及可解释性有限。", "method": "研究提出了一个名为PsyFIRE的理论框架，捕捉13种细粒度的抵抗行为和协作互动。基于此框架构建了ClientResistance语料库，并开发了一个名为RECAP的两阶段框架，用于检测抵抗行为及其细粒度类型，并提供解释。", "result": "RECAP在区分协作和抵抗方面达到了91.25%的F1分数，在细粒度抵抗类别分类方面达到了66.58%的宏F1分数，显著优于现有的基于提示的大型语言模型基线。在实际应用中，RECAP揭示了抵抗行为的普遍性及其对治疗关系的负面影响。", "conclusion": "RECAP框架及其数据集为文本咨询中识别细粒度客户抵抗行为提供了一个有效且可解释的解决方案，并有望提升咨询师对客户抵抗的理解和干预能力。"}}
{"id": "2601.15069", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.15069", "abs": "https://arxiv.org/abs/2601.15069", "authors": ["Yanran Jiang", "Pavan Sikka", "Leimin Tian", "Dana Kuliic", "Cecile Paris"], "title": "Influence of Operator Expertise on Robot Supervision and Intervention", "comment": null, "summary": "With increasing levels of robot autonomy, robots are increasingly being supervised by users with varying levels of robotics expertise. As the diversity of the user population increases, it is important to understand how users with different expertise levels approach the supervision task and how this impacts performance of the human-robot team. This exploratory study investigates how operators with varying expertise levels perceive information and make intervention decisions when supervising a remote robot. We conducted a user study (N=27) where participants supervised a robot autonomously exploring four unknown tunnel environments in a simulator, and provided waypoints to intervene when they believed the robot had encountered difficulties. By analyzing the interaction data and questionnaire responses, we identify differing patterns in intervention timing and decision-making strategies across novice, intermediate, and expert users.", "AI": {"tldr": "本研究探讨了不同专业知识水平的用户在监督自主机器人时，信息感知和干预决策方式的差异，并分析了这些差异如何影响人机协作的团队表现。", "motivation": "随着机器人自主性的提高，用户对其监督能力的需求也在增加，而用户群体在机器人专业知识水平上存在显著差异。理解这种差异如何影响监督任务和人机团队的整体表现，对于设计更有效的机器人系统至关重要。", "method": "通过一项用户研究，让27名具有不同专业知识水平（新手、中级、专家）的参与者在一个模拟环境中监督一个自主探索隧道的机器人。参与者需要判断机器人何时遇到困难并提供干预（设置航点）。研究分析了交互数据和问卷反馈，以识别不同专业水平用户在干预时间和决策策略上的模式。", "result": "研究发现了不同专业知识水平的用户在干预时机和决策策略上存在显著差异。这些差异体现在他们如何感知信息以及何时以及如何做出干预决策。", "conclusion": "用户的机器人专业知识水平会影响其监督自主机器人的行为模式，包括信息感知和干预决策。这些发现对于未来设计更适应不同用户能力的机器人监督界面和策略具有指导意义。"}}
{"id": "2601.14955", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14955", "abs": "https://arxiv.org/abs/2601.14955", "authors": ["Hanqi Jin", "Gaoming Yang", "Zhangming Chan", "Yapeng Yuan", "Longbin Li", "Fei Sun", "Yeqiu Yang", "Jian Wu", "Yuning Jiang", "Bo Zheng"], "title": "Multi-Behavior Sequential Modeling with Transition-Aware Graph Attention Network for E-Commerce Recommendation", "comment": "Accepted by WWW2026 short paper", "summary": "User interactions on e-commerce platforms are inherently diverse, involving behaviors such as clicking, favoriting, adding to cart, and purchasing. The transitions between these behaviors offer valuable insights into user-item interactions, serving as a key signal for understanding evolving preferences. Consequently, there is growing interest in leveraging multi-behavior data to better capture user intent. Recent studies have explored sequential modeling of multi-behavior data, many relying on transformer-based architectures with polynomial time complexity. While effective, these approaches often incur high computational costs, limiting their applicability in large-scale industrial systems with long user sequences. To address this challenge, we propose the Transition-Aware Graph Attention Network (TGA), a linear-complexity approach for modeling multi-behavior transitions. Unlike traditional transformers that treat all behavior pairs equally, TGA constructs a structured sparse graph by identifying informative transitions from three perspectives: (a) item-level transitions, (b) category-level transitions, and (c) neighbor-level transitions. Built upon the structured graph, TGA employs a transition-aware graph Attention mechanism that jointly models user-item interactions and behavior transition types, enabling more accurate capture of sequential patterns while maintaining computational efficiency. Experiments show that TGA outperforms all state-of-the-art models while significantly reducing computational cost. Notably, TGA has been deployed in a large-scale industrial production environment, where it leads to impressive improvements in key business metrics.", "AI": {"tldr": "本文提出了一种名为TGA（Transition-Aware Graph Attention Network）的新模型，用于在电商平台分析用户多行为序列。TGA通过构建包含物品、类别和邻居信息的稀疏图，并引入考虑行为转移类型的注意力机制，实现了线性复杂度的计算，有效解决了Transformer模型在高数据量下的计算成本问题，并在实际工业应用中取得了显著效果。", "motivation": "现有的基于Transformer的多行为序列模型计算复杂度高，难以在大规模工业系统中应用。研究动机是开发一种计算效率高且能有效捕捉用户意图和行为转移模式的新模型。", "method": "提出Transition-Aware Graph Attention Network (TGA) 模型。TGA通过识别物品、类别和邻居三个层面的信息转移来构建结构化稀疏图，并利用考虑行为转移类型的注意力机制来同时建模用户-物品交互和行为转移类型，从而捕获序列模式。", "result": "TGA在实验中超越了所有最先进的模型，同时显著降低了计算成本。该模型已被成功部署到大规模工业生产环境中，并在关键业务指标上取得了显著提升。", "conclusion": "TGA是一种高效且准确地对用户多行为序列进行建模的方法，能够有效捕捉用户意图和行为转移模式，解决了现有模型在高计算成本方面的不足，并在实际应用中展现了其价值。"}}
{"id": "2601.14674", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.14674", "abs": "https://arxiv.org/abs/2601.14674", "authors": ["Mingyang Xie", "Numair Khan", "Tianfu Wang", "Naina Dhingra", "Seonghyeon Nam", "Haitao Yang", "Zhuo Hui", "Christopher Metzler", "Andrea Vedaldi", "Hamed Pirsiavash", "Lei Luo"], "title": "LaVR: Scene Latent Conditioned Generative Video Trajectory Re-Rendering using Large 4D Reconstruction Models", "comment": null, "summary": "Given a monocular video, the goal of video re-rendering is to generate views of the scene from a novel camera trajectory. Existing methods face two distinct challenges. Geometrically unconditioned models lack spatial awareness, leading to drift and deformation under viewpoint changes. On the other hand, geometrically-conditioned models depend on estimated depth and explicit reconstruction, making them susceptible to depth inaccuracies and calibration errors.\n  We propose to address these challenges by using the implicit geometric knowledge embedded in the latent space of a large 4D reconstruction model to condition the video generation process. These latents capture scene structure in a continuous space without explicit reconstruction. Therefore, they provide a flexible representation that allows the pretrained diffusion prior to regularize errors more effectively. By jointly conditioning on these latents and source camera poses, we demonstrate that our model achieves state-of-the-art results on the video re-rendering task. Project webpage is https://lavr-4d-scene-rerender.github.io/", "AI": {"tldr": "本研究提出一种新的视频重渲染方法，利用预训练的大型4D重建模型的隐式几何知识来指导视频生成，从而克服了现有方法的几何不确定性和对精确重建的依赖性。", "motivation": "现有视频重渲染方法在处理视角变化时存在漂移和形变问题（几何不敏感模型），或者依赖于不准确的深度估计和显式重建（几何敏感模型）。本研究旨在解决这些挑战。", "method": "利用预训练的大型4D重建模型在潜在空间中嵌入的隐式几何知识，并将这些潜在表示与源相机姿态一同作为条件，来指导视频生成过程。这种方法避免了显式的几何重建。", "result": "所提出的模型通过联合条件化潜在表示和源相机姿态，在视频重渲染任务上取得了最先进的性能。", "conclusion": "通过利用预训练4D重建模型的隐式几何知识，本研究提出了一种更灵活、更鲁棒的视频重渲染方法，有效解决了现有方法的局限性。"}}
{"id": "2601.15056", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.15056", "abs": "https://arxiv.org/abs/2601.15056", "authors": ["Maria T. Tagliaferri", "Inseung Kang"], "title": "Systematic Evaluation of Hip Exoskeleton Assistance Parameters for Enhancing Gait Stability During Ground Slip Perturbations", "comment": null, "summary": "Falls are the leading cause of injury related hospitalization and mortality among older adults. Consequently, mitigating age-related declines in gait stability and reducing fall risk during walking is a critical goal for assistive devices. Lower-limb exoskeletons have the potential to support users in maintaining stability during walking. However, most exoskeleton controllers are optimized to reduce the energetic cost of walking rather than to improve stability. While some studies report stability benefits with assistance, the effects of specific parameters, such as assistance magnitude and duration, remain unexplored. To address this gap, we systematically modulated the magnitude and duration of torque provided by a bilateral hip exoskeleton during slip perturbations in eight healthy adults, quantifying stability using whole-body angular momentum (WBAM). WBAM responses were governed by a significant interaction between assistance magnitude and duration, with duration determining whether exoskeleton assistance was stabilizing or destabilizing relative to not wearing the exoskeleton device. Compared to an existing energy-optimized controller, experimentally identified stability-optimal parameters reduced WBAM range by 25.7% on average. Notably, substantial inter-subject variability was observed in the parameter combinations that minimized WBAM during perturbations. We found that optimizing exoskeleton assistance for energetic outcomes alone is insufficient for improving reactive stability during gait perturbations. Stability-focused exoskeleton control should prioritize temporal assistance parameters and include user-specific personalization. This study represents an important step toward personalized, stability-focused exoskeleton control, with direct implications for improving stability and reducing fall risk in older adults.", "AI": {"tldr": "通过系统地调整双侧髋部外骨骼的辅助幅度和持续时间，研究发现外骨骼在应对步态干扰时的稳定性受幅度和持续时间的交互影响，并且存在个体差异。优化能量消耗而非稳定性可能不足以改善步态稳定性，建议采用以稳定性为重点、考虑个体差异的外骨骼控制策略。", "motivation": "老年人跌倒是导致住院和死亡的主要原因，而下肢外骨骼有潜力提高行走稳定性以降低跌倒风险。然而，现有外骨骼控制器多优化能量消耗而非稳定性，且缺乏对特定参数（如辅助幅度和持续时间）对稳定性的影响研究。", "method": "研究邀请了八名健康成年人，在他们行走时施加滑动扰动，并通过双侧髋部外骨骼系统地调整了扭矩的幅度和持续时间。使用全身角动量（WBAM）来量化稳定性。", "result": "WBAM的响应受辅助幅度和持续时间的显著交互作用控制，持续时间决定了辅助是稳定还是不稳定。与现有能量优化控制器相比，实验确定的稳定性优化参数平均将WBAM范围减小了25.7%。此外，观察到最小化WBAM的参数组合存在显著的个体差异。", "conclusion": "仅优化外骨骼辅助的能量消耗不足以改善步态扰动下的反应稳定性。以稳定性为重点的外骨骼控制应优先考虑时间辅助参数，并纳入用户个性化。这项研究是实现个性化、以稳定性为重点的外骨骼控制的重要一步，对改善老年人的稳定性和降低跌倒风险具有直接意义。"}}
{"id": "2601.14605", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.14605", "abs": "https://arxiv.org/abs/2601.14605", "authors": ["Weiwei Ma", "Xiaobing Yu", "Peijie Qiu", "Jin Yang", "Pan Xiao", "Xiaoqi Zhao", "Xiaofeng Liu", "Tomo Miyazaki", "Shinichiro Omachi", "Yongsong Huang"], "title": "U-Harmony: Enhancing Joint Training for Segmentation Models with Universal Harmonization", "comment": null, "summary": "In clinical practice, medical segmentation datasets are often limited and heterogeneous, with variations in modalities, protocols, and anatomical targets across institutions. Existing deep learning models struggle to jointly learn from such diverse data, often sacrificing either generalization or domain-specific knowledge. To overcome these challenges, we propose a joint training method called Universal Harmonization (U-Harmony), which can be integrated into deep learning-based architectures with a domain-gated head, enabling a single segmentation model to learn from heterogeneous datasets simultaneously. By integrating U-Harmony, our approach sequentially normalizes and then denormalizes feature distributions to mitigate domain-specific variations while preserving original dataset-specific knowledge. More appealingly, our framework also supports universal modality adaptation, allowing the seamless learning of new imaging modalities and anatomical classes. Extensive experiments on cross-institutional brain lesion datasets demonstrate the effectiveness of our approach, establishing a new benchmark for robust and adaptable 3D medical image segmentation models in real-world clinical settings.", "AI": {"tldr": "提出了一种名为 U-Harmony 的联合训练方法，该方法通过领域门控头，允许单个深度学习模型同时从异构医疗分割数据集中学习，并支持跨模态和跨解剖类别的适应性。", "motivation": "现有的深度学习模型在处理临床实践中有限且异构的医学分割数据集时存在困难，难以同时学习泛化能力和领域特定知识。", "method": "提出 U-Harmony 联合训练方法，集成到带有领域门控头的深度学习架构中。该方法通过顺序地归一化和反归一化特征分布来减弱领域差异，同时保留数据集特有的知识，并支持跨模态和跨解剖类别的适应。", "result": "在跨机构脑部病灶数据集上的大量实验表明，该方法有效，为真实临床环境中鲁棒且适应性强的 3D 医学图像分割模型树立了新的标杆。", "conclusion": "U-Harmony 是一种有效的联合训练方法，能够解决异构医学分割数据集的挑战，实现单个模型同时学习，并支持跨模态和跨解剖类别的适应性，在实际临床应用中具有重要意义。"}}
{"id": "2601.15029", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.15029", "abs": "https://arxiv.org/abs/2601.15029", "authors": ["Fabio Morreale", "Joan Serrà", "Yuki Mistufuji"], "title": "Emergent, not Immanent: A Baradian Reading of Explainable AI", "comment": "Accepted at CHI 2026", "summary": "Explainable AI (XAI) is frequently positioned as a technical problem of revealing the inner workings of an AI model. This position is affected by unexamined onto-epistemological assumptions: meaning is treated as immanent to the model, the explainer is positioned outside the system, and a causal structure is presumed recoverable through computational techniques. In this paper, we draw on Barad's agential realism to develop an alternative onto-epistemology of XAI. We propose that interpretations are material-discursive performances that emerge from situated entanglements of the AI model with humans, context, and the interpretative apparatus. To develop this position, we read a comprehensive set of XAI methods through agential realism and reveal the assumptions and limitations that underpin several of these methods. We then articulate the framework's ethical dimension and propose design directions for XAI interfaces that support emergent interpretation, using a speculative text-to-music interface as a case study.", "AI": {"tldr": "本文从Barad的能动实在论出发，提出了一种新的可解释人工智能（XAI）的本体-认识论视角，认为解释是通过AI模型、人类、情境和解释工具的具身纠缠所产生的具身-论述性实践，并据此探讨了XAI的伦理维度和设计方向。", "motivation": "当前XAI的研究主要将可解释性视为一个技术问题，存在未被审视的本体-认识论假设，例如将意义视为模型内部固有、解释者置于系统之外，以及认为可以通过计算技术恢复因果结构。本文旨在挑战这些假设，提出新的理解XAI的框架。", "method": "本文运用Barad的能动实在论来构建新的XAI本体-认识论。作者将一系列XAI方法置于能动实在论的视角下进行解读，揭示了这些方法的基础性假设和局限性。最后，通过一个文本到音乐接口的案例研究，阐述了该框架的伦理维度，并提出了支持涌现性解释的XAI界面设计方向。", "result": "通过能动实在论的视角，本文揭示了现有XAI方法中存在的未被充分认识的假设和局限性。该框架强调了解释的具身性和论述性，以及AI模型、人类、情境和解释工具之间相互作用的重要性。", "conclusion": "可解释性不是简单地揭示模型内部运作的技术问题，而是一种在AI模型、人类、情境和解释工具的具身纠缠中涌现出来的具身-论述性实践。本文提出的框架有助于更深入地理解XAI的本体论和认识论基础，并为设计更具伦理和支持涌现性解释的XAI界面提供了指导。"}}
{"id": "2601.14826", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.14826", "abs": "https://arxiv.org/abs/2601.14826", "authors": ["Yuxuan Cao", "Zida Yang", "Ye Wang"], "title": "Comparative Study of Large Language Models on Chinese Film Script Continuation: An Empirical Analysis Based on GPT-5.2 and Qwen-Max", "comment": "18 pages, 6 figures, 6 tables, 20 references. First two authors contributed equally. Corresponding author: Ye Wang (wangye@whu.edu.cn)", "summary": "As large language models (LLMs) are increasingly applied to creative writing, their performance on culturally specific narrative tasks warrants systematic investigation. This study constructs the first Chinese film script continuation benchmark comprising 53 classic films, and designs a multi-dimensional evaluation framework comparing GPT-5.2 and Qwen-Max-Latest. Using a \"first half to second half\" continuation paradigm with 3 samples per film, we obtained 303 valid samples (GPT-5.2: 157, 98.7% validity; Qwen-Max: 146, 91.8% validity). Evaluation integrates ROUGE-L, Structural Similarity, and LLM-as-Judge scoring (DeepSeek-Reasoner).\n  Statistical analysis of 144 paired samples reveals: Qwen-Max achieves marginally higher ROUGE-L (0.2230 vs 0.2114, d=-0.43); however, GPT-5.2 significantly outperforms in structural preservation (0.93 vs 0.75, d=0.46), overall quality (44.79 vs 25.72, d=1.04), and composite scores (0.50 vs 0.39, d=0.84). The overall quality effect size reaches large effect level (d>0.8).\n  GPT-5.2 excels in character consistency, tone-style matching, and format preservation, while Qwen-Max shows deficiencies in generation stability. This study provides a reproducible framework for LLM evaluation in Chinese creative writing.", "AI": {"tldr": "本研究构建了首个中文电影剧本续写基准，并对比了GPT-5.2和Qwen-Max在剧本续写任务上的表现。GPT-5.2在结构、整体质量和综合评分上显著优于Qwen-Max，尤其在角色一致性、风格匹配和格式保留方面表现突出。", "motivation": "随着大语言模型（LLMs）在创意写作领域的应用日益广泛，有必要系统性地评估其在特定文化背景（如中文电影剧本续写）下的表现。", "method": "研究者构建了一个包含53部经典中文电影的剧本续写基准，并设计了一个多维度评估框架。实验采用“上半部分到下半部分”的续写范式，从每部电影中抽取3个样本，最终获得303个有效样本。评估指标包括ROUGE-L、结构相似度（Structural Similarity）以及LLM-as-Judge评分（使用DeepSeek-Reasoner）。对144个配对样本进行统计分析。", "result": "Qwen-Max在ROUGE-L指标上略有优势（0.2230 vs 0.2114），但GPT-5.2在结构保持（0.93 vs 0.75）、整体质量（44.79 vs 25.72）和综合评分（0.50 vs 0.39）上均显著优于Qwen-Max。整体质量效应量达到大效应级别。GPT-5.2在角色一致性、语调风格匹配和格式保持方面表现更佳，而Qwen-Max在生成稳定性方面存在不足。", "conclusion": "GPT-5.2在中文电影剧本续写任务上表现出更强的能力，尤其在保持故事结构、风格和格式方面。本研究提供了一个可复用的框架，用于评估大语言模型在中文创意写作领域的性能。"}}
{"id": "2601.15075", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.15075", "abs": "https://arxiv.org/abs/2601.15075", "authors": ["Chen Qian", "Peng Wang", "Dongrui Liu", "Junyao Yang", "Dadi Guo", "Ling Tang", "Jilin Mei", "Qihan Ren", "Shuai Shao", "Yong Liu", "Jie Fu", "Jing Shao", "Xia Hu"], "title": "The Why Behind the Action: Unveiling Internal Drivers via Agentic Attribution", "comment": null, "summary": "Large Language Model (LLM)-based agents are widely used in real-world applications such as customer service, web navigation, and software engineering. As these systems become more autonomous and are deployed at scale, understanding why an agent takes a particular action becomes increasingly important for accountability and governance. However, existing research predominantly focuses on \\textit{failure attribution} to localize explicit errors in unsuccessful trajectories, which is insufficient for explaining the reasoning behind agent behaviors. To bridge this gap, we propose a novel framework for \\textbf{general agentic attribution}, designed to identify the internal factors driving agent actions regardless of the task outcome. Our framework operates hierarchically to manage the complexity of agent interactions. Specifically, at the \\textit{component level}, we employ temporal likelihood dynamics to identify critical interaction steps; then at the \\textit{sentence level}, we refine this localization using perturbation-based analysis to isolate the specific textual evidence. We validate our framework across a diverse suite of agentic scenarios, including standard tool use and subtle reliability risks like memory-induced bias. Experimental results demonstrate that the proposed framework reliably pinpoints pivotal historical events and sentences behind the agent behavior, offering a critical step toward safer and more accountable agentic systems.", "AI": {"tldr": "提出一种通用的智能体归因框架，能够识别驱动智能体行为的内部因素，无论任务结果如何，通过分层方法（组件级别的时间似然动态和句子级别的扰动分析）来定位关键历史事件和句子。", "motivation": "现有研究主要关注失败归因，不足以解释智能体行为背后的原因，而随着智能体系统的自主性和部署规模的增加，理解其决策过程对问责和治理至关重要。", "method": "提出一个分层归因框架：1. 组件级别：使用时间似然动态识别关键交互步骤；2. 句子级别：使用基于扰动的分析来分离具体的文本证据。", "result": "实验结果表明，该框架能够可靠地定位驱动智能体行为的关键历史事件和句子。", "conclusion": "该框架为实现更安全、更负责任的智能体系统迈出了关键一步，能够识别智能体的内部驱动因素，无论任务成功与否。"}}
{"id": "2601.14677", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14677", "abs": "https://arxiv.org/abs/2601.14677", "authors": ["Sukana Zulfqar", "Sadia Saeed", "M. Azam Zia", "Anjum Ali", "Faisal Mehmood", "Abid Ali"], "title": "A comprehensive overview of deep learning models for object detection from videos/images", "comment": "N/A", "summary": "Object detection in video and image surveillance is a well-established yet rapidly evolving task, strongly influenced by recent deep learning advancements. This review summarises modern techniques by examining architectural innovations, generative model integration, and the use of temporal information to enhance robustness and accuracy. Unlike earlier surveys, it classifies methods based on core architectures, data processing strategies, and surveillance specific challenges such as dynamic environments, occlusions, lighting variations, and real-time requirements. The primary goal is to evaluate the current effectiveness of semantic object detection, while secondary aims include analysing deep learning models and their practical applications. The review covers CNN-based detectors, GAN-assisted approaches, and temporal fusion methods, highlighting how generative models support tasks such as reconstructing missing frames, reducing occlusions, and normalising illumination. It also outlines preprocessing pipelines, feature extraction progress, benchmarking datasets, and comparative evaluations. Finally, emerging trends in low-latency, efficient, and spatiotemporal learning approaches are identified for future research.", "AI": {"tldr": "这篇综述总结了当前视频和图像监控领域中基于深度学习的对象检测技术，重点关注架构创新、生成模型应用以及时序信息利用，并分析了当前方法在动态环境、遮挡、光照变化和实时性等挑战下的有效性。", "motivation": "尽管对象检测技术成熟，但深度学习的快速发展使其不断演进。本文旨在系统性地梳理和总结现代技术，评估其在监控场景下的有效性，并分析深度学习模型及其应用。", "method": "该研究通过审查（review）现代技术，包括架构创新、生成模型集成和时序信息的使用。它将方法根据核心架构、数据处理策略和特定监控挑战（如动态环境、遮挡、光照变化、实时性要求）进行分类。具体涵盖了基于CNN的检测器、GAN辅助方法和时序融合方法。", "result": "生成模型在重构缺失帧、减少遮挡和规范化光照等方面提供了支持。研究还概述了预处理流程、特征提取进展、基准数据集和比较评估。此外，识别出低延迟、高效和时空学习方法等新兴趋势。", "conclusion": "该综述为理解当前视频和图像监控对象检测的最新进展提供了一个全面的视角，并指明了未来研究方向，特别是在提高效率和应对复杂监控环境方面。"}}
{"id": "2601.14610", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14610", "abs": "https://arxiv.org/abs/2601.14610", "authors": ["Zhenghong Li", "Kecheng Zheng", "Haibin Ling"], "title": "Learning Consistent Taxonomic Classification through Hierarchical Reasoning", "comment": "12 pages, 4 figures", "summary": "While Vision-Language Models (VLMs) excel at visual understanding, they often fail to grasp hierarchical knowledge. This leads to common errors where VLMs misclassify coarser taxonomic levels even when correctly identifying the most specific level (leaf level). Existing approaches largely overlook this issue by failing to model hierarchical reasoning. To address this gap, we propose VL-Taxon, a two-stage, hierarchy-based reasoning framework designed to improve both leaf-level accuracy and hierarchical consistency in taxonomic classification. The first stage employs a top-down process to enhance leaf-level classification accuracy. The second stage then leverages this accurate leaf-level output to ensure consistency throughout the entire taxonomic hierarchy. Each stage is initially trained with supervised fine-tuning to instill taxonomy knowledge, followed by reinforcement learning to refine the model's reasoning and generalization capabilities. Extensive experiments reveal a remarkable result: our VL-Taxon framework, implemented on the Qwen2.5-VL-7B model, outperforms its original 72B counterpart by over 10% in both leaf-level and hierarchical consistency accuracy on average on the iNaturalist-2021 dataset. Notably, this significant gain was achieved by fine-tuning on just a small subset of data, without relying on any examples generated by other VLMs.", "AI": {"tldr": "提出了一种名为VL-Taxon的两阶段、基于层次结构的推理框架，用于提升视觉语言模型（VLMs）在分类任务中的层次化知识理解能力，并取得了显著的性能提升。", "motivation": "现有的视觉语言模型在理解层次化知识方面存在不足，即使能正确识别最具体的分类级别（叶子级别），也常常会在较粗的分类级别上出错。现有方法忽略了这一问题，未能对层次化推理进行建模。", "method": "提出VL-Taxon框架，包括两个阶段：第一阶段采用自顶向下的方法提升叶子级别的分类准确性；第二阶段利用第一阶段的准确输出，确保整个分类层次结构的连贯性。两个阶段均先通过监督微调学习分类知识，再通过强化学习优化模型的推理和泛化能力。", "result": "在iNaturalist-2021数据集上，基于Qwen2.5-VL-7B模型的VL-Taxon框架，在叶子级别和层次结构一致性准确率上平均超越了原始的72B模型10%以上。该提升仅通过对少量数据进行微调实现，且未依赖其他VLMs生成的数据。", "conclusion": "VL-Taxon框架能够有效解决VLMs在层次化知识理解上的问题，显著提升分类准确性和层次结构一致性，且具有高效的数据利用能力。"}}
{"id": "2601.15164", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15164", "abs": "https://arxiv.org/abs/2601.15164", "authors": ["Yaru Liu", "Ao-bo Wang", "Nanyang Ye"], "title": "V-CAGE: Context-Aware Generation and Verification for Scalable Long-Horizon Embodied Tasks", "comment": null, "summary": "Learning long-horizon embodied behaviors from synthetic data remains challenging because generated scenes are often physically implausible, language-driven programs frequently \"succeed\" without satisfying task semantics, and high-level instructions require grounding into executable action sequences. To address these limitations, we introduce V-CAGE, a closed-loop framework for generating robust, semantically aligned manipulation datasets at scale. First, we propose a context-aware instantiation mechanism that enforces geometric consistency during scene synthesis. By dynamically maintaining a map of prohibited spatial areas as objects are placed, our system prevents interpenetration and ensures reachable, conflict-free configurations in cluttered environments. Second, to bridge the gap between abstract intent and low-level control, we employ a hierarchical instruction decomposition module. This decomposes high-level goals (e.g., \"get ready for work\") into compositional action primitives, facilitating coherent long-horizon planning. Crucially, we enforce semantic correctness through a VLM-based verification loop. Acting as a visual critic, the VLM performs rigorous rejection sampling after each subtask, filtering out \"silent failures\" where code executes but fails to achieve the visual goal. Experiments demonstrate that V-CAGE yields datasets with superior physical and semantic fidelity, significantly boosting the success rate and generalization of downstream policies compared to non-verified baselines.", "AI": {"tldr": "本文提出了V-CAGE框架，通过几何一致性场景合成、层次化指令分解和视觉语言模型（VLM）验证，生成在物理和语义上都更可靠的合成数据，以训练具身智能体执行长期任务。", "motivation": "从合成数据中学习长远具身行为存在挑战，因为生成的场景往往物理上不合理，语言指令执行可能不满足任务语义，且高层指令难以转化为可执行动作序列。现有的方法在物理保真度、语义对齐和长期规划方面存在不足。", "method": "1. 上下文感知实例化机制：在场景合成过程中，通过动态维护禁止区域地图，确保物体摆放的几何一致性，防止穿插，并在杂乱环境中生成无冲突的配置。 2. 层次化指令分解模块：将高层目标（如“准备上班”）分解为可组合的动作原语，以实现连贯的长期规划。 3. VLM验证循环：利用VLM作为视觉批评家，对每个子任务进行严格的拒绝采样，过滤掉代码执行但视觉目标未达成的“静默失败”。", "result": "V-CAGE生成的数据集在物理和语义保真度上优于基线方法。使用V-CAGE生成的数据训练的下游策略，其成功率和泛化能力显著提高。", "conclusion": "V-CAGE框架能够大规模生成在物理和语义上都更鲁棒、更准确的具身操作数据集，有效解决了合成数据用于训练长远具身行为的挑战，并显著提升了下游策略的性能。"}}
{"id": "2601.14857", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.14857", "abs": "https://arxiv.org/abs/2601.14857", "authors": ["Motong Tian", "Allen P. Wong", "Mingjun Mao", "Wangchunshu Zhou"], "title": "HiNS: Hierarchical Negative Sampling for More Comprehensive Memory Retrieval Embedding Model", "comment": null, "summary": "Memory-augmented language agents rely on embedding models for effective memory retrieval. However, existing training data construction overlooks a critical limitation: the hierarchical difficulty of negative samples and their natural distribution in human-agent interactions. In practice, some negatives are semantically close distractors while others are trivially irrelevant, and natural dialogue exhibits structured proportions of these types. Current approaches using synthetic or uniformly sampled negatives fail to reflect this diversity, limiting embedding models' ability to learn nuanced discrimination essential for robust memory retrieval. In this work, we propose a principled data construction framework HiNS that explicitly models negative sample difficulty tiers and incorporates empirically grounded negative ratios derived from conversational data, enabling the training of embedding models with substantially improved retrieval fidelity and generalization in memory-intensive tasks. Experiments show significant improvements: on LoCoMo, F1/BLEU-1 gains of 3.27%/3.30%(MemoryOS) and 1.95%/1.78% (Mem0); on PERSONAMEM, total score improvements of 1.19% (MemoryOS) and 2.55% (Mem0).", "AI": {"tldr": "该研究提出了一种名为HiNS的数据构建框架，用于改进记忆增强语言代理的嵌入模型训练。HiNS考虑了负样本的难度分级以及负样本在对话数据中的自然比例，从而提高了记忆检索的准确性和泛化能力。", "motivation": "现有的记忆增强语言代理在内存检索时依赖嵌入模型，但其训练数据构建忽略了负样本难度层级和人类-代理交互中负样本的自然分布。这导致嵌入模型难以学习细致的区分能力，限制了内存检索的鲁棒性。", "method": "提出了一种名为HiNS的数据构建框架，该框架明确地对负样本难度分级进行建模，并纳入从对话数据中推导出的经验性负样本比例。", "result": "实验表明，HiNS在LoCoMo数据集上带来了显著的F1/BLEU-1增益（MemoryOS 3.27%/3.30%，Mem0 1.95%/1.78%），在PERSONAMEM数据集上总分提升（MemoryOS 1.19%，Mem0 2.55%）。", "conclusion": "HiNS框架通过模拟负样本的难度分级和自然分布，能训练出检索保真度和泛化能力更强的嵌入模型，从而显著提升记忆密集型任务的表现。"}}
{"id": "2601.15222", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.15222", "abs": "https://arxiv.org/abs/2601.15222", "authors": ["Stavrow A. Bahnam", "Robin Ferede", "Till M. Blaha", "Anton E. Lang", "Erin Lucassen", "Quentin Missinne", "Aderik E. C. Verraest", "Christophe De Wagter", "Guido C. H. E. de Croon"], "title": "MonoRace: Winning Champion-Level Drone Racing with Robust Monocular AI", "comment": null, "summary": "Autonomous drone racing represents a major frontier in robotics research. It requires an Artificial Intelligence (AI) that can run on board light-weight flying robots under tight resource and time constraints, while pushing the physical system to its limits. The state of the art in this area consists of a system with a stereo camera and an inertial measurement unit (IMU) that beat human drone racing champions in a controlled indoor environment. Here, we present MonoRace: an onboard drone racing approach that uses a monocular, rolling-shutter camera and IMU that generalizes to a competition environment without any external motion tracking system. The approach features robust state estimation that combines neural-network-based gate segmentation with a drone model. Moreover, it includes an offline optimization procedure that leverages the known geometry of gates to refine any state estimation parameter. This offline optimization is based purely on onboard flight data and is important for fine-tuning the vital external camera calibration parameters. Furthermore, the guidance and control are performed by a neural network that foregoes inner loop controllers by directly sending motor commands. This small network runs on the flight controller at 500Hz. The proposed approach won the 2025 Abu Dhabi Autonomous Drone Racing Competition (A2RL), outperforming all competing AI teams and three human world champion pilots in a direct knockout tournament. It set a new milestone in autonomous drone racing research, reaching speeds up to 100 km/h on the competition track and successfully coping with problems such as camera interference and IMU saturation.", "AI": {"tldr": "MonoRace 是一个创新的无人机竞速系统，使用单目摄像头和 IMU，能够离线优化并直接输出电机指令，在阿布扎比无人机竞速大赛中获胜，速度达到 100 公里/小时。", "motivation": "现有的无人机竞速 AI 系统依赖于立体摄像头和外部运动跟踪系统，并且在通用性和资源受限环境下的表现有待提高。研究动机在于开发一个更轻量化、通用性更强、且能在比赛环境中独立运行的无人机竞速 AI。", "method": "MonoRace 系统采用单目滚动快门相机和 IMU 进行状态估计，结合基于神经网络的门分割和无人机模型。通过离线优化程序，利用门的几何信息精炼状态估计参数，特别是外部相机标定。导航和控制由一个直接输出电机指令的神经网络实现，该网络能在飞行控制器上以 500Hz 运行。", "result": "MonoRace 在 2025 年阿布扎比自动无人机竞速大赛中获胜，击败了所有 AI 竞争对手和三名人类世界冠军。在比赛中达到了 100 公里/小时的速度，并成功应对了相机干扰和 IMU 饱和等挑战。", "conclusion": "MonoRace 成功地展示了仅使用单目相机和 IMU，并通过神经网络直接控制电机，在复杂环境中实现高速自主无人机竞速的可行性，为无人机竞速领域树立了新的里程碑。"}}
{"id": "2601.14896", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.14896", "abs": "https://arxiv.org/abs/2601.14896", "authors": ["Rui Qi", "Fengran Mo", "Yufeng Chen", "Xue Zhang", "Shuo Wang", "Hongliang Li", "Jinan Xu", "Meng Jiang", "Jian-Yun Nie", "Kaiyu Huang"], "title": "Language-Coupled Reinforcement Learning for Multilingual Retrieval-Augmented Generation", "comment": null, "summary": "Multilingual retrieval-augmented generation (MRAG) requires models to effectively acquire and integrate beneficial external knowledge from multilingual collections. However, most existing studies employ a unitive process where queries of equivalent semantics across different languages are processed through a single-turn retrieval and subsequent optimization. Such a ``one-size-fits-all'' strategy is often suboptimal in multilingual settings, as the models occur to knowledge bias and conflict during the interaction with the search engine. To alleviate the issues, we propose LcRL, a multilingual search-augmented reinforcement learning framework that integrates a language-coupled Group Relative Policy Optimization into the policy and reward models. We adopt the language-coupled group sampling in the rollout module to reduce knowledge bias, and regularize an auxiliary anti-consistency penalty in the reward models to mitigate the knowledge conflict. Experimental results demonstrate that LcRL not only achieves competitive performance but is also appropriate for various practical scenarios such as constrained training data and retrieval over collections encompassing a large number of languages. Our code is available at https://github.com/Cherry-qwq/LcRL-Open.", "AI": {"tldr": "提出了一种名为LcRL的多语言检索增强生成框架，通过语言耦合的组相对策略优化来解决现有MRAG模型在多语言知识获取和整合中的偏见和冲突问题。", "motivation": "现有研究中，多语言检索增强生成（MRAG）模型采用单一检索和优化流程处理不同语言的等价查询，容易导致知识偏见和冲突，效果不佳。", "method": "LcRL框架整合了语言耦合的组相对策略优化（Group Relative Policy Optimization），在rollout模块采用语言耦合组采样（language-coupled group sampling）减少知识偏见，并在奖励模型中引入辅助反一致性惩罚（anti-consistency penalty）来减轻知识冲突。", "result": "实验证明，LcRL在多语言检索增强生成任务上取得了具有竞争力的性能，并且适用于训练数据受限或包含大量语言的实际场景。", "conclusion": "LcRL框架有效解决了现有MRAG模型在多语言环境下的知识偏见和冲突问题，并在多种实际应用场景下表现出良好的鲁棒性。"}}
{"id": "2601.14678", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.NE", "q-bio.TO"], "pdf": "https://arxiv.org/pdf/2601.14678", "abs": "https://arxiv.org/abs/2601.14678", "authors": ["Justin Cheung", "Samuel Savine", "Calvin Nguyen", "Lin Lu", "Alhassan S. Yasin"], "title": "Transfer Learning from One Cancer to Another via Deep Learning Domain Adaptation", "comment": "8 pages, 6 figures, 3 table", "summary": "Supervised deep learning models often achieve excellent performance within their training distribution but struggle to generalize beyond it. In cancer histopathology, for example, a convolutional neural network (CNN) may classify cancer severity accurately for cancer types represented in its training data, yet fail on related but unseen types. Although adenocarcinomas from different organs share morphological features that might support limited cross-domain generalization, addressing domain shift directly is necessary for robust performance. Domain adaptation offers a way to transfer knowledge from labeled data in one cancer type to unlabeled data in another, helping mitigate the scarcity of annotated medical images.\n  This work evaluates cross-domain classification performance among lung, colon, breast, and kidney adenocarcinomas. A ResNet50 trained on any single adenocarcinoma achieves over 98% accuracy on its own domain but shows minimal generalization to others. Ensembling multiple supervised models does not resolve this limitation. In contrast, converting the ResNet50 into a domain adversarial neural network (DANN) substantially improves performance on unlabeled target domains. A DANN trained on labeled breast and colon data and adapted to unlabeled lung data reaches 95.56% accuracy.\n  We also examine the impact of stain normalization on domain adaptation. Its effects vary by target domain: for lung, accuracy drops from 95.56% to 66.60%, while for breast and colon targets, stain normalization boosts accuracy from 49.22% to 81.29% and from 78.48% to 83.36%, respectively. Finally, using Integrated Gradients reveals that DANNs consistently attribute importance to biologically meaningful regions such as densely packed nuclei, indicating that the model learns clinically relevant features and can apply them to unlabeled cancer types.", "AI": {"tldr": "该研究使用领域对抗神经网络（DANN）在不同腺癌类型之间进行跨领域分类，并发现DANN在处理领域偏移方面优于传统的监督学习模型，同时探索了染色质归一化对模型性能的影响，并通过集成梯度验证了模型学习到了具有临床意义的特征。", "motivation": "监督深度学习模型在训练分布外泛化能力差，尤其是在癌症组织病理学领域，缺乏标注的医学图像导致模型性能受限。该研究旨在通过领域自适应来解决这一问题，实现不同癌症类型间的知识迁移。", "method": "研究者使用ResNet50模型进行训练，并将其转换为领域对抗神经网络（DANN）。评估了模型在肺、结肠、乳腺和肾脏腺癌之间的交叉领域分类性能。同时，研究了染色质归一化对领域自适应的影响，并使用集成梯度（Integrated Gradients）来分析模型学习到的特征。", "result": "单独训练的ResNet50在自身领域外泛化能力有限。DANN模型显著提高了在无标签目标领域的性能，例如，在乳腺和结肠数据上训练并适配到无标签肺部数据的DANN模型达到了95.56%的准确率。染色质归一化对不同目标领域的影响不同，对肺部数据准确率有负面影响，但对乳腺和结肠数据有正面提升。集成梯度分析表明，DANN模型将重要性归因于生物学上有意义的区域，如密集排列的细胞核。", "conclusion": "领域对抗神经网络（DANN）能够有效地进行跨领域癌症分类，克服了传统监督学习模型的泛化能力限制。染色质归一化对DANN的性能有复杂影响，具体效果取决于目标领域。DANN模型能够学习并应用到无标签癌症类型的临床相关特征。"}}
{"id": "2601.15120", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15120", "abs": "https://arxiv.org/abs/2601.15120", "authors": ["Qian Xiong", "Yuekai Huang", "Yujia Zheng", "Tianhao Li", "Ziyou Jiang", "Zhiyuan Chang", "Zhaoyang Li", "Huanxiang Feng", "Mingyang Li"], "title": "Emerging from Ground: Addressing Intent Deviation in Tool-Using Agents via Deriving Real Calls into Virtual Trajectories", "comment": null, "summary": "LLMs have advanced tool-using agents for real-world applications, yet they often lead to unexpected behaviors or results. Beyond obvious failures, the subtle issue of \"intent deviation\" severely hinders reliable evaluation and performance improvement. Existing post-training methods generally leverage either real system samples or virtual data simulated by LLMs. However, the former is costly due to reliance on hand-crafted user requests, while the latter suffers from distribution shift from the real tools in the wild. Additionally, both methods lack negative samples tailored to intent deviation scenarios, hindering effective guidance on preference learning. We introduce RISE, a \"Real-to-Virtual\" method designed to mitigate intent deviation. Anchoring on verified tool primitives, RISE synthesizes virtual trajectories and generates diverse negative samples through mutation on critical parameters. With synthetic data, RISE fine-tunes backbone LLMs via the two-stage training for intent alignment. Evaluation results demonstrate that data synthesized by RISE achieve promising results in eight metrics covering user requires, execution trajectories and agent responses. Integrating with training, RISE achieves an average 35.28% improvement in Acctask (task completion) and 23.27% in Accintent (intent alignment), outperforming SOTA baselines by 1.20--42.09% and 1.17--54.93% respectively.", "AI": {"tldr": "本文提出了一种名为RISE的“Real-to-Virtual”方法，通过合成虚拟数据来解决大型语言模型（LLM）在工具使用中出现的意图偏差问题，并在实验中取得了显著的性能提升。", "motivation": "现有的LLM工具使用评估和改进方法存在不足。基于真实系统样本的方法成本高昂，而基于LLM模拟的虚拟数据存在分布偏移。此外，这两种方法都缺乏针对意图偏差的负样本，难以有效指导偏好学习。", "method": "RISE方法通过锚定已验证的工具原语，合成虚拟轨迹，并通过对关键参数进行变异来生成多样化的负样本。然后，利用这些合成数据，通过两阶段训练对LLM进行微调，以实现意图对齐。", "result": "RISE合成的数据在用户需求、执行轨迹和代理响应等八个指标上表现出色。通过RISE进行训练，Acctask（任务完成率）平均提高了35.28%，Accintent（意图对齐率）平均提高了23.27%，优于现有的最先进方法。", "conclusion": "RISE方法能够有效地缓解LLM在工具使用中的意图偏差问题，并且通过合成数据进行微调可以显著提高模型的任务完成度和意图对齐能力。"}}
{"id": "2601.15197", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.15197", "abs": "https://arxiv.org/abs/2601.15197", "authors": ["Shijie Lian", "Bin Yu", "Xiaopeng Lin", "Laurence T. Yang", "Zhaolong Shen", "Changti Wu", "Yuzhuo Miao", "Cong Huang", "Kai Chen"], "title": "BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries", "comment": null, "summary": "Vision-Language-Action (VLA) models have shown promise in robot manipulation but often struggle to generalize to new instructions or complex multi-task scenarios. We identify a critical pathology in current training paradigms where goal-driven data collection creates a dataset bias. In such datasets, language instructions are highly predictable from visual observations alone, causing the conditional mutual information between instructions and actions to vanish, a phenomenon we term Information Collapse. Consequently, models degenerate into vision-only policies that ignore language constraints and fail in out-of-distribution (OOD) settings. To address this, we propose BayesianVLA, a novel framework that enforces instruction following via Bayesian decomposition. By introducing learnable Latent Action Queries, we construct a dual-branch architecture to estimate both a vision-only prior $p(a \\mid v)$ and a language-conditioned posterior $π(a \\mid v, \\ell)$. We then optimize the policy to maximize the conditional Pointwise Mutual Information (PMI) between actions and instructions. This objective effectively penalizes the vision shortcut and rewards actions that explicitly explain the language command. Without requiring new data, BayesianVLA significantly improves generalization. Extensive experiments across on SimplerEnv and RoboCasa demonstrate substantial gains, including an 11.3% improvement on the challenging OOD SimplerEnv benchmark, validating the ability of our approach to robustly ground language in action.", "AI": {"tldr": "解析错误", "motivation": "解析错误", "method": "解析错误", "result": "解析错误", "conclusion": "解析错误"}}
{"id": "2601.14903", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.14903", "abs": "https://arxiv.org/abs/2601.14903", "authors": ["Chenning Xu", "Mao Zheng", "Mingyu Zheng", "Mingyang Song"], "title": "PodBench: A Comprehensive Benchmark for Instruction-Aware Audio-Oriented Podcast Script Generation", "comment": null, "summary": "Podcast script generation requires LLMs to synthesize structured, context-grounded dialogue from diverse inputs, yet systematic evaluation resources for this task remain limited. To bridge this gap, we introduce PodBench, a benchmark comprising 800 samples with inputs up to 21K tokens and complex multi-speaker instructions. We propose a multifaceted evaluation framework that integrates quantitative constraints with LLM-based quality assessment. Extensive experiments reveal that while proprietary models generally excel, open-source models equipped with explicit reasoning demonstrate superior robustness in handling long contexts and multi-speaker coordination compared to standard baselines. However, our analysis uncovers a persistent divergence where high instruction following does not guarantee high content substance. PodBench offers a reproducible testbed to address these challenges in long-form, audio-centric generation.", "AI": {"tldr": "本文介绍了PodBench，一个用于评估大型语言模型（LLM）生成播客脚本的基准数据集和评估框架。该基准包含大量数据和复杂的指令，旨在解决现有评估资源的不足。实验表明，尽管闭源模型表现更好，但开源模型在处理长文本和多说话人方面表现出更强的鲁棒性。同时，研究发现指令遵循度与内容实质之间存在差异。", "motivation": "现有的播客脚本生成任务缺乏系统的评估资源，导致难以对大型语言模型（LLM）在该任务上的表现进行全面衡量。", "method": "提出了PodBench基准，包含800个样本，输入长度可达21K token，并带有复杂的、多说话人的指令。同时，设计了一个多方面的评估框架，结合了定量约束和基于LLM的质量评估。", "result": "实验结果显示，闭源模型在整体表现上优于开源模型。然而，经过显式推理训练的开源模型在处理长上下文和协调多说话人方面，比标准基线模型表现出更好的鲁棒性。研究还发现，高指令遵循度并不总是能保证内容具有实质性。", "conclusion": "PodBench提供了一个可复现的测试平台，用于解决长篇幅、以音频为中心的生成任务中的挑战，并为未来研究提供了方向，特别是关注指令遵循度和内容实质之间的权衡。"}}
{"id": "2601.15130", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.15130", "abs": "https://arxiv.org/abs/2601.15130", "authors": ["Ivan Carrera", "Daniel Maldonado-Ruiz"], "title": "The Plausibility Trap: Using Probabilistic Engines for Deterministic Tasks", "comment": null, "summary": "The ubiquity of Large Language Models (LLMs) is driving a paradigm shift where user convenience supersedes computational efficiency. This article defines the \"Plausibility Trap\": a phenomenon where individuals with access to Artificial Intelligence (AI) models deploy expensive probabilistic engines for simple deterministic tasks-such as Optical Character Recognition (OCR) or basic verification-resulting in significant resource waste. Through micro-benchmarks and case studies on OCR and fact-checking, we quantify the \"efficiency tax\"-demonstrating a ~6.5x latency penalty-and the risks of algorithmic sycophancy. To counter this, we introduce Tool Selection Engineering and the Deterministic-Probabilistic Decision Matrix, a framework to help developers determine when to use Generative AI and, crucially, when to avoid it. We argue for a curriculum shift, emphasizing that true digital literacy relies not only in knowing how to use Generative AI, but also on knowing when not to use it.", "AI": {"tldr": "该论文提出了“合理性陷阱”的概念，即用户倾向于将昂贵的LLM用于简单的确定性任务，导致资源浪费。文章量化了这种“效率税”，并引入了一个框架来指导何时使用或避免使用生成式AI，强调数字素养在于知道何时不使用AI。", "motivation": "用户普遍使用LLM，但却将它们用于简单的确定性任务，造成了计算资源的巨大浪费，促使研究者探索如何解决这一问题。", "method": "通过微基准测试和OCR、事实核查的案例研究，量化了“效率税”和算法谄媚的风险。提出了“工具选择工程”和“确定性-概率性决策矩阵”框架。", "result": "量化了“效率税”的存在，即LLM用于简单任务会带来约6.5倍的延迟惩罚。证明了这种使用方式存在的风险。", "conclusion": "“合理性陷阱”导致了严重的资源浪费。提出了“工具选择工程”和“确定性-概率性决策矩阵”框架来指导开发者做出更优的AI使用决策，并认为真正的数字素养应包含知道何时不使用生成式AI。"}}
{"id": "2601.15250", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.15250", "abs": "https://arxiv.org/abs/2601.15250", "authors": ["Zichen Xi", "Hao-Xiang Chen", "Nan Xue", "Hongyu Yan", "Qi-Yuan Feng", "Levent Burak Kara", "Joaquim Jorge", "Qun-Ce Xu"], "title": "FlowSSC: Universal Generative Monocular Semantic Scene Completion via One-Step Latent Diffusion", "comment": "Under Review", "summary": "Semantic Scene Completion (SSC) from monocular RGB images is a fundamental yet challenging task due to the inherent ambiguity of inferring occluded 3D geometry from a single view. While feed-forward methods have made progress, they often struggle to generate plausible details in occluded regions and preserve the fundamental spatial relationships of objects. Such accurate generative reasoning capability for the entire 3D space is critical in real-world applications. In this paper, we present FlowSSC, the first generative framework applied directly to monocular semantic scene completion. FlowSSC treats the SSC task as a conditional generation problem and can seamlessly integrate with existing feed-forward SSC methods to significantly boost their performance. To achieve real-time inference without compromising quality, we introduce Shortcut Flow-matching that operates in a compact triplane latent space. Unlike standard diffusion models that require hundreds of steps, our method utilizes a shortcut mechanism to achieve high-fidelity generation in a single step, enabling practical deployment in autonomous systems. Extensive experiments on SemanticKITTI demonstrate that FlowSSC achieves state-of-the-art performance, significantly outperforming existing baselines.", "AI": {"tldr": "提出了一种名为 FlowSSC 的新生成框架，用于从单目 RGB 图像进行语义场景补全，通过在紧凑的三平面潜在空间中使用快捷流匹配，实现了单步高保真生成，显著提高了现有前馈方法的性能，并在 SemanticKITTI 数据集上取得了最先进的成果。", "motivation": "单目 RGB 图像的语义场景补全是具有挑战性的任务，因为从单一视图推断被遮挡的 3D 几何信息存在固有的模糊性。现有前馈方法在生成被遮挡区域的细节和保持物体间的空间关系方面存在不足，而准确的 3D 空间生成推理能力在现实应用中至关重要。", "method": "FlowSSC 将语义场景补全视为条件生成问题，并设计了一种快捷流匹配（Shortcut Flow-matching）机制，在紧凑的三平面潜在空间中操作，以实现实时推理。该方法可以与现有的前馈 SSC 方法集成，通过一个单步过程提升性能，避免了传统扩散模型需要大量迭代的缺点。", "result": "在 SemanticKITTI 数据集上的实验表明，FlowSSC 实现了最先进的性能，显著优于现有的基线方法，能够生成更高质量、更具空间一致性的 3D 场景补全结果。", "conclusion": "FlowSSC 是首个将生成方法应用于单目语义场景补全的框架，通过高效的单步生成机制，克服了现有方法的局限性，并在性能和推理速度上取得了突破，为自动驾驶等实际应用提供了可行方案。"}}
{"id": "2601.14914", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.14914", "abs": "https://arxiv.org/abs/2601.14914", "authors": ["Tianxiang Fei", "Cheng Chen", "Yue Pan", "Mao Zheng", "Mingyang Song"], "title": "CodeDelegator: Mitigating Context Pollution via Role Separation in Code-as-Action Agents", "comment": null, "summary": "Recent advances in large language models (LLMs) allow agents to represent actions as executable code, offering greater expressivity than traditional tool-calling. However, real-world tasks often demand both strategic planning and detailed implementation. Using a single agent for both leads to context pollution from debugging traces and intermediate failures, impairing long-horizon performance. We propose CodeDelegator, a multi-agent framework that separates planning from implementation via role specialization. A persistent Delegator maintains strategic oversight by decomposing tasks, writing specifications, and monitoring progress without executing code. For each sub-task, a new Coder agent is instantiated with a clean context containing only its specification, shielding it from prior failures. To coordinate between agents, we introduce Ephemeral-Persistent State Separation (EPSS), which isolates each Coder's execution state while preserving global coherence, preventing debugging traces from polluting the Delegator's context. Experiments on various benchmarks demonstrate the effectiveness of CodeDelegator across diverse scenarios.", "AI": {"tldr": "提出了一种名为CodeDelegator的多智能体框架，将规划和执行分离，通过角色专长和临时-持久状态分离（EPSS）来提高长时任务的性能，防止上下文污染。", "motivation": "单一LLM代理在执行代码时，规划和实现混合会导致调试痕迹和中间失败污染上下文，损害长时任务性能。", "method": "CodeDelegator框架包含一个持续的Delegator（负责规划、分解任务、写规范、监控）和一个为每个子任务实例化的新Coder（仅包含其规范，上下文干净）。使用临时-持久状态分离（EPSS）来隔离Coder的执行状态，同时保持全局一致性。", "result": "在各种基准测试中，CodeDelegator在不同场景下均表现出有效性。", "conclusion": "CodeDelegator通过明确的规划-执行分离和上下文隔离，可以有效解决LLM代理在执行长时任务时面临的上下文污染问题，提升其性能。"}}
{"id": "2601.14703", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14703", "abs": "https://arxiv.org/abs/2601.14703", "authors": ["Xinquan Yang", "Xuguang Li", "Mianjie Zheng", "Xuefen Liu", "Kun Tang", "Kian Ming Lim", "He Meng", "Jianfeng Ren", "Linlin Shen"], "title": "RegFreeNet: A Registration-Free Network for CBCT-based 3D Dental Implant Planning", "comment": null, "summary": "As the commercial surgical guide design software usually does not support the export of implant position for pre-implantation data, existing methods have to scan the post-implantation data and map the implant to pre-implantation space to get the label of implant position for training. Such a process is time-consuming and heavily relies on the accuracy of registration algorithm. Moreover, not all hospitals have paired CBCT data, limitting the construction of multi-center dataset. Inspired by the way dentists determine the implant position based on the neighboring tooth texture, we found that even if the implant area is masked, it will not affect the determination of the implant position. Therefore, we propose to mask the implants in the post-implantation data so that any CBCT containing the implants can be used as training data. This paradigm enables us to discard the registration process and makes it possible to construct a large-scale multi-center implant dataset. On this basis, we proposes ImplantFairy, a comprehensive, publicly accessible dental implant dataset with voxel-level 3D annotations of 1622 CBCT data. Furthermore, according to the area variation characteristics of the tooth's spatial structure and the slope information of the implant, we designed a slope-aware implant position prediction network. Specifically, a neighboring distance perception (NDP) module is designed to adaptively extract tooth area variation features, and an implant slope prediction branch assists the network in learning more robust features through additional implant supervision information. Extensive experiments conducted on ImplantFairy and two public dataset demonstrate that the proposed RegFreeNet achieves the state-of-the-art performance.", "AI": {"tldr": "研究提出了RegFreeNet，一种无需配准即可准确预测种植体位置的方法，并构建了一个名为ImplantFairy的大规模牙科种植体数据集。该方法通过掩盖种植体区域，使得任何包含种植体的CBCT数据都可以用于训练，从而解决了现有方法耗时且依赖配准精度的缺点，并能构建多中心数据集。", "motivation": "现有方法需要注册术后和术前CBCT数据以获取种植体位置标签，过程耗时且依赖配准精度。此外，并非所有医院都有配对的CBCT数据，限制了多中心数据集的构建。研究受牙医根据邻近牙齿纹理确定种植体位置的启发，提出一种无需注册的方法。", "method": "提出一种掩盖术后CBCT数据中种植体区域的方法，使得任何包含种植体的CBCT数据都可作为训练数据，从而跳过注册过程。在此基础上，设计了一个名为ImplantFairy的大规模、公开可用的牙科种植体数据集，包含1622个CBCT数据及其体素级3D标注。同时，设计了一个名为RegFreeNet的斜率感知种植体位置预测网络，包含邻近距离感知（NDP）模块以提取牙齿区域变化特征，以及一个种植体斜率预测分支以通过额外的种植体监督信息学习更鲁棒的特征。", "result": "所提出的RegFreeNet在ImplantFairy以及另外两个公开数据集上进行了广泛的实验，结果表明其达到了最先进的性能。", "conclusion": "研究提出了一种无需注册即可预测种植体位置的新范例，并通过构建ImplantFairy数据集和设计RegFreeNet网络验证了其有效性。该方法简化了数据准备流程，降低了对配准精度的依赖，并有望促进大规模多中心牙科种植体数据集的构建和相关研究的发展。"}}
{"id": "2601.15131", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15131", "abs": "https://arxiv.org/abs/2601.15131", "authors": ["Ayan Maity", "Sudeshna Sarkar"], "title": "Vehicle Routing with Finite Time Horizon using Deep Reinforcement Learning with Improved Network Embedding", "comment": "Accepted at AAAI-26 Workshop on AI for Urban Planning", "summary": "In this paper, we study the vehicle routing problem with a finite time horizon. In this routing problem, the objective is to maximize the number of customer requests served within a finite time horizon. We present a novel routing network embedding module which creates local node embedding vectors and a context-aware global graph representation. The proposed Markov decision process for the vehicle routing problem incorporates the node features, the network adjacency matrix and the edge features as components of the state space. We incorporate the remaining finite time horizon into the network embedding module to provide a proper routing context to the embedding module. We integrate our embedding module with a policy gradient-based deep Reinforcement Learning framework to solve the vehicle routing problem with finite time horizon. We trained and validated our proposed routing method on real-world routing networks, as well as synthetically generated Euclidean networks. Our experimental results show that our method achieves a higher customer service rate than the existing routing methods. Additionally, the solution time of our method is significantly lower than that of the existing methods.", "AI": {"tldr": "该研究提出了一种基于深度强化学习的车辆路径规划方法，用于在有限时间内最大化客户请求的响应数量，并在真实和合成数据集上证明了其优越的客户服务率和更低的求解时间。", "motivation": "在有限时间范围内最大化客户请求的服务数量是一个实际的车辆路径规划问题，现有方法在性能和效率方面有待提高。", "method": "研究人员设计了一个新颖的路由网络嵌入模块，该模块生成局部节点嵌入向量和上下文感知的全局图表示。他们提出了一个将节点特征、网络邻接矩阵和边特征纳入状态空间的马尔可夫决策过程，并将剩余的有限时间范围融入网络嵌入模块。该嵌入模块与基于策略梯度的深度强化学习框架集成，以解决该问题。", "result": "在真实世界和合成的欧几里得网络上进行的实验表明，该方法实现了比现有方法更高的客户服务率，并且求解时间显著缩短。", "conclusion": "提出的基于深度强化学习的方法有效地解决了有限时间范围内的车辆路径规划问题，并在客户服务率和计算效率方面优于现有方法。"}}
{"id": "2601.14690", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14690", "abs": "https://arxiv.org/abs/2601.14690", "authors": ["Yian Huang", "Qing Qin", "Aji Mao", "Xiangyu Qiu", "Liang Xu", "Xian Zhang", "Zhenming Peng"], "title": "FeedbackSTS-Det: Sparse Frames-Based Spatio-Temporal Semantic Feedback Network for Infrared Small Target Detection", "comment": "Submitted to Journal IEEE Transactions on Geoscience and Remote Sensing", "summary": "Infrared small target detection (ISTD) under complex backgrounds remains a critical yet challenging task, primarily due to the extremely low signal-to-clutter ratio, persistent dynamic interference, and the lack of distinct target features. While multi-frame detection methods leverages temporal cues to improve upon single-frame approaches, existing methods still struggle with inefficient long-range dependency modeling and insufficient robustness. To overcome these issues, we propose a novel scheme for ISTD, realized through a sparse frames-based spatio-temporal semantic feedback network named FeedbackSTS-Det. The core of our approach is a novel spatio-temporal semantic feedback strategy with a closed-loop semantic association mechanism, which consists of paired forward and backward refinement modules that work cooperatively across the encoder and decoder. Moreover, both modules incorporate an embedded sparse semantic module (SSM), which performs structured sparse temporal modeling to capture long-range dependencies with low computational cost. This integrated design facilitates robust implicit inter-frame registration and continuous semantic refinement, effectively suppressing false alarms. Furthermore, our overall procedure maintains a consistent training-inference pipeline, which ensures reliable performance transfer and increases model robustness. Extensive experiments on multiple benchmark datasets confirm the effectiveness of FeedbackSTS-Det. Code and models are available at: https://github.com/IDIP-Lab/FeedbackSTS-Det.", "AI": {"tldr": "提出了一种基于稀疏帧的时空语义反馈网络 FeedbackSTS-Det，用于解决红外小目标检测中的挑战，该网络通过闭环语义关联机制增强了长距离依赖建模和鲁棒性。", "motivation": "现有红外小目标检测方法在处理复杂背景、低信噪比、动态干扰以及缺乏显著目标特征时存在效率低下和鲁棒性不足的问题，尤其是在长距离依赖建模方面。", "method": "提出 FeedbackSTS-Det 网络，核心是时空语义反馈策略，包含成对的前向和后向精炼模块，它们在编码器和解码器之间协同工作。每个模块都包含一个稀疏语义模块 (SSM)，用于结构化稀疏时序建模，以低计算成本捕捉长距离依赖。整个流程保持一致的训练-推理管道。", "result": "FeedbackSTS-Det 通过闭环语义关联实现了鲁棒的隐式帧间配准和连续的语义精炼，有效抑制了虚警。在多个基准数据集上的大量实验证明了该方法的有效性。", "conclusion": "FeedbackSTS-Det 是一种新颖的红外小目标检测方案，通过其创新的时空语义反馈机制和稀疏时序建模，显著提高了检测性能和鲁棒性，克服了现有方法的局限性。"}}
{"id": "2601.15282", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.15282", "abs": "https://arxiv.org/abs/2601.15282", "authors": ["Yufan Deng", "Zilin Pan", "Hongyu Zhang", "Xiaojie Li", "Ruoqing Hu", "Yufei Ding", "Yiming Zou", "Yan Zeng", "Daquan Zhou"], "title": "Rethinking Video Generation Model for the Embodied World", "comment": "Github: https://github.com/DAGroup-PKU/ReVidgen/ Project website: https://dagroup-pku.github.io/ReVidgen.github.io/", "summary": "Video generation models have significantly advanced embodied intelligence, unlocking new possibilities for generating diverse robot data that capture perception, reasoning, and action in the physical world. However, synthesizing high-quality videos that accurately reflect real-world robotic interactions remains challenging, and the lack of a standardized benchmark limits fair comparisons and progress. To address this gap, we introduce a comprehensive robotics benchmark, RBench, designed to evaluate robot-oriented video generation across five task domains and four distinct embodiments. It assesses both task-level correctness and visual fidelity through reproducible sub-metrics, including structural consistency, physical plausibility, and action completeness. Evaluation of 25 representative models highlights significant deficiencies in generating physically realistic robot behaviors. Furthermore, the benchmark achieves a Spearman correlation coefficient of 0.96 with human evaluations, validating its effectiveness. While RBench provides the necessary lens to identify these deficiencies, achieving physical realism requires moving beyond evaluation to address the critical shortage of high-quality training data. Driven by these insights, we introduce a refined four-stage data pipeline, resulting in RoVid-X, the largest open-source robotic dataset for video generation with 4 million annotated video clips, covering thousands of tasks and enriched with comprehensive physical property annotations. Collectively, this synergistic ecosystem of evaluation and data establishes a robust foundation for rigorous assessment and scalable training of video models, accelerating the evolution of embodied AI toward general intelligence.", "AI": {"tldr": "该研究提出了一个名为RBench的机器人视频生成基准，用于评估现有模型在物理现实性和任务正确性方面的不足，并引入了一个大规模数据集RoVid-X来解决数据稀缺问题，旨在推动具身智能的发展。", "motivation": "现有机器人视频生成模型在捕捉真实机器人交互方面存在挑战，缺乏标准化基准导致模型评估不公平，阻碍了研究进展。同时，物理现实性差是现有模型面临的关键问题，这与高质量训练数据不足有关。", "method": "提出了RBench基准，包含五个任务领域和四种不同具身，通过结构一致性、物理合理性和动作完整性等可复现的子指标评估任务正确性和视觉保真度。此外，设计了一个四阶段数据管道，生成了包含400万个带注释视频剪辑的RoVid-X数据集，并补充了全面的物理属性标注。", "result": "RBench评估显示，现有25个代表性模型在生成物理上逼真的机器人行为方面存在显著缺陷。RBench与人类评估的相关性达到0.96。RoVid-X是目前最大的开源机器人视频生成数据集。", "conclusion": "RBench基准有效地识别了机器人视频生成模型在物理现实性方面的不足。为了提升物理现实性，需要高质量的训练数据。RoVid-X数据集的引入为机器人视频模型提供了大规模、高质量的训练资源。RBench和RoVid-X共同构成了一个评估和训练的生态系统，为加速具身AI向通用智能演进奠定了基础。"}}
{"id": "2601.15153", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15153", "abs": "https://arxiv.org/abs/2601.15153", "authors": ["Choro Ulan uulu", "Mikhail Kulyabin", "Iris Fuhrmann", "Jan Joosten", "Nuno Miguel Martins Pacheco", "Filippos Petridis", "Rebecca Johnson", "Jan Bosch", "Helena Holmström Olsson"], "title": "How to Build AI Agents by Augmenting LLMs with Codified Human Expert Domain Knowledge? A Software Engineering Framework", "comment": null, "summary": "Critical domain knowledge typically resides with few experts, creating organizational bottlenecks in scalability and decision-making. Non-experts struggle to create effective visualizations, leading to suboptimal insights and diverting expert time. This paper investigates how to capture and embed human domain knowledge into AI agent systems through an industrial case study. We propose a software engineering framework to capture human domain knowledge for engineering AI agents in simulation data visualization by augmenting a Large Language Model (LLM) with a request classifier, Retrieval-Augmented Generation (RAG) system for code generation, codified expert rules, and visualization design principles unified in an agent demonstrating autonomous, reactive, proactive, and social behavior. Evaluation across five scenarios spanning multiple engineering domains with 12 evaluators demonstrates 206% improvement in output quality, with our agent achieving expert-level ratings in all cases versus baseline's poor performance, while maintaining superior code quality with lower variance. Our contributions are: an automated agent-based system for visualization generation and a validated framework for systematically capturing human domain knowledge and codifying tacit expert knowledge into AI agents, demonstrating that non-experts can achieve expert-level outcomes in specialized domains.", "AI": {"tldr": "本研究提出了一种软件工程框架，通过增强大型语言模型（LLM），将人类领域知识嵌入AI代理系统，以实现自动化模拟数据可视化。该系统在工业案例研究中表现出色，使非专家也能达到专家级成果。", "motivation": "关键领域知识集中在少数专家手中，导致组织在可伸缩性和决策方面存在瓶颈。非专家难以创建有效的可视化，从而产生次优的见解并分散专家的时间。", "method": "提出一个软件工程框架，通过增加一个请求分类器、一个用于代码生成的检索增强生成（RAG）系统、编码的专家规则以及统一在代理中的可视化设计原则来捕获人类领域知识，以工程化AI代理用于模拟数据可视化。该代理展示了自主、反应式、主动式和社交式行为。", "result": "在跨越多个工程领域的五个场景中，有12名评估者进行了评估，结果显示输出质量提高了206%，该代理在所有情况下均达到专家级评分，而基线表现不佳，同时保持了卓越的代码质量和更低的标准差。", "conclusion": "开发了一种自动化的基于代理的可视化生成系统，以及一个经过验证的框架，用于系统地捕获人类领域知识并将默示的专家知识编码到AI代理中，证明非专家可以在专业领域内获得专家级别的成果。"}}
{"id": "2601.14706", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14706", "abs": "https://arxiv.org/abs/2601.14706", "authors": ["Chao Gao", "Siqiao Xue", "Yimin Peng", "Jiwen Fu", "Tingyi Gu", "Shanshan Li", "Fan Zhou"], "title": "LookBench: A Live and Holistic Open Benchmark for Fashion Image Retrieval", "comment": "The first two authors contributed equally to this work. Project site: https://serendipityoneinc.github.io/look-bench-page/", "summary": "In this paper, we present LookBench (We use the term \"look\" to reflect retrieval that mirrors how people shop -- finding the exact item, a close substitute, or a visually consistent alternative.), a live, holistic and challenging benchmark for fashion image retrieval in real e-commerce settings. LookBench includes both recent product images sourced from live websites and AI-generated fashion images, reflecting contemporary trends and use cases. Each test sample is time-stamped and we intend to update the benchmark periodically, enabling contamination-aware evaluation aligned with declared training cutoffs. Grounded in our fine-grained attribute taxonomy, LookBench covers single-item and outfit-level retrieval across. Our experiments reveal that LookBench poses a significant challenge on strong baselines, with many models achieving below $60\\%$ Recall@1. Our proprietary model achieves the best performance on LookBench, and we release an open-source counterpart that ranks second, with both models attaining state-of-the-art results on legacy Fashion200K evaluations. LookBench is designed to be updated semi-annually with new test samples and progressively harder task variants, providing a durable measure of progress. We publicly release our leaderboard, dataset, evaluation code, and trained models.", "AI": {"tldr": "本文提出了LookBench，一个用于真实电商场景的时尚图像检索基准。该基准包含真实和AI生成的图像，并定期更新以支持无污染评估。实验表明，现有模型在LookBench上表现不佳，但研究提出的模型取得了最优性能。", "motivation": "现有的时尚图像检索基准未能充分反映真实电商环境中用户购物行为的多样性（如寻找精确商品、替代品或视觉一致的商品），并且缺乏对最新产品和AI生成内容的适应性。因此，需要一个更具挑战性、更贴近实际且可长期更新的基准来推动该领域的发展。", "method": "构建了一个名为LookBench的基准，包含从真实电商网站和AI生成的新近产品图像。该基准具有时间戳，并支持单品和套装级别的检索。评估是基于细粒度属性分类进行的。作者还提出并评估了自己的专有模型以及一个开源模型。", "result": "在LookBench基准上，许多现有模型（基线模型）的Recall@1低于60%，表明LookBench的挑战性。作者的专有模型在该基准上取得了最佳性能，而其开源模型排名第二。这两个模型在旧的Fashion200K评估上都达到了最先进的水平。", "conclusion": "LookBench是一个全面、实时且具有挑战性的时尚图像检索基准，能够有效评估模型在真实电商环境中的表现。该基准的设计易于更新，并为该领域提供了持续的评估框架。实验结果表明，当前模型在该基准上仍有很大提升空间，并且作者的模型在该基准上取得了领先地位。"}}
{"id": "2601.15286", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2601.15286", "abs": "https://arxiv.org/abs/2601.15286", "authors": ["Shantanu Jaiswal", "Mihir Prabhudesai", "Nikash Bhardwaj", "Zheyang Qin", "Amir Zadeh", "Chuan Li", "Katerina Fragkiadaki", "Deepak Pathak"], "title": "Iterative Refinement Improves Compositional Image Generation", "comment": "Project webpage: https://iterative-img-gen.github.io/", "summary": "Text-to-image (T2I) models have achieved remarkable progress, yet they continue to struggle with complex prompts that require simultaneously handling multiple objects, relations, and attributes. Existing inference-time strategies, such as parallel sampling with verifiers or simply increasing denoising steps, can improve prompt alignment but remain inadequate for richly compositional settings where many constraints must be satisfied. Inspired by the success of chain-of-thought reasoning in large language models, we propose an iterative test-time strategy in which a T2I model progressively refines its generations across multiple steps, guided by feedback from a vision-language model as the critic in the loop. Our approach is simple, requires no external tools or priors, and can be flexibly applied to a wide range of image generators and vision-language models. Empirically, we demonstrate consistent gains on image generation across benchmarks: a 16.9% improvement in all-correct rate on ConceptMix (k=7), a 13.8% improvement on T2I-CompBench (3D-Spatial category) and a 12.5% improvement on Visual Jenga scene decomposition compared to compute-matched parallel sampling. Beyond quantitative gains, iterative refinement produces more faithful generations by decomposing complex prompts into sequential corrections, with human evaluators preferring our method 58.7% of the time over 41.3% for the parallel baseline. Together, these findings highlight iterative self-correction as a broadly applicable principle for compositional image generation. Results and visualizations are available at https://iterative-img-gen.github.io/", "AI": {"tldr": "该研究提出了一种文本到图像（T2I）模型的迭代测试时策略，通过引入视觉语言模型作为批评者，逐步优化图像生成过程，以解决复杂提示下的多对象、关系和属性处理问题。", "motivation": "现有T2I模型在处理包含多个对象、关系和属性的复杂提示时存在困难，即使采用并行采样或增加去噪步数等方法也效果有限。受链式思考（chain-of-thought）的启发，研究者希望开发一种更有效的迭代优化方法。", "method": "提出了一种迭代测试时策略，T2I模型在多步过程中逐步优化生成结果，每一步都由一个视觉语言模型（作为批评者）提供反馈进行指导。该方法不依赖外部工具或先验知识，可以灵活应用于不同的图像生成器和视觉语言模型。", "result": "在ConceptMix、T2I-CompBench（3D-Spatial类别）和Visual Jenga等基准测试中，该方法带来了显著的图像生成性能提升。与计算量匹配的并行采样相比，在ConceptMix上all-correct率提高了16.9%，在T2I-CompBench上提高了13.8%，在Visual Jenga上提高了12.5%。人类评估也显示，58.7%的受试者更喜欢该方法生成的图像。", "conclusion": "研究表明，迭代自纠正在组合式图像生成中是一种广泛适用的原则，通过将复杂提示分解为一系列顺序校正，可以生成更符合提示要求的图像。"}}
{"id": "2601.14718", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14718", "abs": "https://arxiv.org/abs/2601.14718", "authors": ["Yiyang Fu", "Hui Li", "Wangyu Wu"], "title": "Context Patch Fusion With Class Token Enhancement for Weakly Supervised Semantic Segmentation", "comment": null, "summary": "Weakly Supervised Semantic Segmentation (WSSS), which relies only on image-level labels, has attracted significant attention for its cost-effectiveness and scalability. Existing methods mainly enhance inter-class distinctions and employ data augmentation to mitigate semantic ambiguity and reduce spurious activations. However, they often neglect the complex contextual dependencies among image patches, resulting in incomplete local representations and limited segmentation accuracy. To address these issues, we propose the Context Patch Fusion with Class Token Enhancement (CPF-CTE) framework, which exploits contextual relations among patches to enrich feature representations and improve segmentation. At its core, the Contextual-Fusion Bidirectional Long Short-Term Memory (CF-BiLSTM) module captures spatial dependencies between patches and enables bidirectional information flow, yielding a more comprehensive understanding of spatial correlations. This strengthens feature learning and segmentation robustness. Moreover, we introduce learnable class tokens that dynamically encode and refine class-specific semantics, enhancing discriminative capability. By effectively integrating spatial and semantic cues, CPF-CTE produces richer and more accurate representations of image content. Extensive experiments on PASCAL VOC 2012 and MS COCO 2014 validate that CPF-CTE consistently surpasses prior WSSS methods.", "AI": {"tldr": "提出了一种名为 CPF-CTE 的弱监督语义分割框架，通过利用上下文补丁融合和类令牌增强来提高分割准确性。", "motivation": "现有弱监督语义分割方法主要关注类间区分和数据增强，忽略了图像块之间复杂的上下文依赖关系，导致局部表示不完整，分割精度受限。", "method": "提出 CPF-CTE 框架，核心是 CF-BiLSTM 模块，用于捕获块间的空间依赖关系并实现双向信息流，同时引入可学习的类令牌来编码和细化类特定语义。", "result": "CPF-CTE 在 PASCAL VOC 2012 和 MS COCO 2014 数据集上进行了广泛实验，结果表明该框架持续优于现有弱监督语义分割方法。", "conclusion": "CPF-CTE 框架通过有效整合空间和语义线索，产生更丰富、更准确的图像内容表示，从而提高了弱监督语义分割的性能。"}}
{"id": "2601.14944", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.14944", "abs": "https://arxiv.org/abs/2601.14944", "authors": ["Pierre-Antoine Lequeu", "Léo Labat", "Laurène Cave", "Gaël Lejeune", "François Yvon", "Benjamin Piwowarski"], "title": "The GDN-CC Dataset: Automatic Corpus Clarification for AI-enhanced Democratic Citizen Consultations", "comment": "31 pages including 22 for references and appendix, 13 figures", "summary": "LLMs are ubiquitous in modern NLP, and while their applicability extends to texts produced for democratic activities such as online deliberations or large-scale citizen consultations, ethical questions have been raised for their usage as analysis tools. We continue this line of research with two main goals: (a) to develop resources that can help standardize citizen contributions in public forums at the pragmatic level, and make them easier to use in topic modeling and political analysis; (b) to study how well this standardization can reliably be performed by small, open-weights LLMs, i.e. models that can be run locally and transparently with limited resources. Accordingly, we introduce Corpus Clarification as a preprocessing framework for large-scale consultation data that transforms noisy, multi-topic contributions into structured, self-contained argumentative units ready for downstream analysis. We present GDN-CC, a manually-curated dataset of 1,231 contributions to the French Grand Débat National, comprising 2,285 argumentative units annotated for argumentative structure and manually clarified. We then show that finetuned Small Language Models match or outperform LLMs on reproducing these annotations, and measure their usability for an opinion clustering task. We finally release GDN-CC-large, an automatically annotated corpus of 240k contributions, the largest annotated democratic consultation dataset to date.", "AI": {"tldr": "研究提出了一种名为“语料库澄清”（Corpus Clarification）的预处理框架，用于规范和结构化大规模公民意见数据，并研究了小型开源语言模型（LLMs）在执行此任务上的能力，同时发布了一个手动标注的数据集（GDN-CC）和一个大规模自动标注的数据集（GDN-CC-large）。", "motivation": "旨在解决在民主活动（如在线审议、公民咨询）中使用LLM进行文本分析时产生的伦理问题，并希望标准化公民贡献，使其更易于主题建模和政治分析，同时探索小型、开放权重LLM在本地资源有限的情况下进行此标准化任务的可靠性。", "method": "提出“语料库澄清”框架，将嘈杂、多主题的贡献转化为结构化的论证单元。创建了GDN-CC数据集（1,231条贡献，2,285个论证单元），并标注了论证结构。通过微调小型LLMs来重现标注，并评估其在意见聚类任务上的可用性。最后，发布了GDN-CC-large（240k贡献）。", "result": "微调后的小型LLMs在重现标注方面表现与大型LLMs相当甚至更优。研究也测量了小型LLMs在意见聚类任务上的可用性。GDN-CC-large成为迄今为止最大的民主咨询数据集。", "conclusion": "“语料库澄清”框架可以有效地将公民贡献标准化为可用于下游分析的论证单元。小型、开源的LLMs能够可靠地执行此标准化任务，并且在某些下游任务上表现出有前景的性能，为大规模民主对话的分析提供了新的可能性。"}}
{"id": "2601.15160", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15160", "abs": "https://arxiv.org/abs/2601.15160", "authors": ["Yuval Kansal", "Niraj K. Jha"], "title": "Knowledge Graphs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning", "comment": null, "summary": "Large language models have achieved near-expert performance in structured reasoning domains like mathematics and programming, yet their ability to perform compositional multi-hop reasoning in specialized scientific fields remains limited. We propose a bottom-up learning paradigm in which models are grounded in axiomatic domain facts and compose them to solve complex, unseen tasks. To this end, we present a post-training pipeline, based on a combination of supervised fine-tuning and reinforcement learning (RL), in which knowledge graphs act as implicit reward models. By deriving novel reward signals from knowledge graph paths, we provide verifiable, scalable, and grounded supervision that encourages models to compose intermediate axioms rather than optimize only final answers during RL. We validate this approach in the medical domain, training a 14B model on short-hop reasoning paths (1-3 hops) and evaluating its zero-shot generalization to complex multi-hop queries (4-5 hops). Our experiments show that path-derived rewards act as a \"compositional bridge\", enabling our model to significantly outperform much larger models and frontier systems like GPT-5.2 and Gemini 3 Pro, on the most difficult reasoning tasks. Furthermore, we demonstrate the robustness of our approach to adversarial perturbations against option-shuffling stress tests. This work suggests that grounding the reasoning process in structured knowledge is a scalable and efficient path toward intelligent reasoning.", "AI": {"tldr": "研究人员提出了一种自底向上的学习范式，通过将大语言模型（LLM）与领域公理事实相结合，并利用知识图谱的路径作为隐式奖励模型，来提升 LLM 在专业科学领域进行多跳推理的能力。实验证明，该方法在医学领域使一个14B模型在未见过（4-5跳）的复杂推理任务上表现优于 GPT-5.2 和 Gemini 3 Pro 等模型，并且对对抗性扰动具有鲁棒性。", "motivation": "尽管大型语言模型在结构化推理方面表现出色，但在专业科学领域进行组合式多跳推理的能力仍然有限。", "method": "提出了一种自底向上的学习范式，结合监督微调和基于知识图谱路径的强化学习（RL）。知识图谱路径被用作隐式奖励模型，提供可验证、可扩展且有依据的监督信号，鼓励模型组合中间公理而非仅优化最终答案。", "result": "在医学领域，该方法使一个14B模型在零样本的复杂多跳推理（4-5跳）任务上，显著优于更大的模型（如 GPT-5.2 和 Gemini 3 Pro）。该方法对选项随机排序的对抗性扰动测试表现出鲁棒性。", "conclusion": "将推理过程建立在结构化知识的基础上，是实现智能推理的可扩展且高效的途径。"}}
{"id": "2601.14952", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14952", "abs": "https://arxiv.org/abs/2601.14952", "authors": ["Zhiyuan Lu", "Chenliang Li", "Yingcheng Shi", "Weizhou Shen", "Ming Yan", "Fei Huang"], "title": "CorpusQA: A 10 Million Token Benchmark for Corpus-Level Analysis and Reasoning", "comment": null, "summary": "While large language models now handle million-token contexts, their capacity for reasoning across entire document repositories remains largely untested. Existing benchmarks are inadequate, as they are mostly limited to single long texts or rely on a \"sparse retrieval\" assumption-that answers can be derived from a few relevant chunks. This assumption fails for true corpus-level analysis, where evidence is highly dispersed across hundreds of documents and answers require global integration, comparison, and statistical aggregation. To address this critical gap, we introduce CorpusQA, a new benchmark scaling up to 10 million tokens, generated via a novel data synthesis framework. By decoupling reasoning from textual representation, this framework creates complex, computation-intensive queries with programmatically guaranteed ground-truth answers, challenging systems to perform holistic reasoning over vast, unstructured text without relying on fallible human annotation. We further demonstrate the utility of our framework beyond evaluation, showing that fine-tuning on our synthesized data effectively enhances an LLM's general long-context reasoning capabilities. Extensive experiments reveal that even state-of-the-art long-context LLMs struggle as input length increases, and standard retrieval-augmented generation systems collapse entirely. Our findings indicate that memory-augmented agentic architectures offer a more robust alternative, suggesting a critical shift is needed from simply extending context windows to developing advanced architectures for global information synthesis.", "AI": {"tldr": "本研究提出了 CorpusQA，一个包含 1000 万 token 的新基准，用于评估大型语言模型在海量文档存储库中的推理能力。研究发现现有模型在处理长文本时表现不佳，并提出内存增强的智能体架构可能是更优的解决方案。", "motivation": "现有的大型语言模型虽然能处理长上下文，但其在整个文档存储库中的推理能力未经充分检验。现有基准测试存在局限性，无法应对证据分散在大量文档中的复杂推理任务。", "method": "引入 CorpusQA 基准，该基准包含高达 1000 万 token，并通过新颖的数据合成框架生成。该框架通过解耦推理与文本表示，创建复杂的、计算密集型的查询，并保证程序化生成正确的答案。此外，研究还通过在合成数据上进行微调来增强 LLM 的长上下文推理能力。", "result": "即使是先进的长上下文 LLM，随着输入长度的增加，其性能也会下降，而标准的检索增强生成系统则完全失效。研究表明，内存增强的智能体架构提供了更鲁棒的替代方案。", "conclusion": "仅仅扩展上下文窗口不足以解决大型语言模型在海量文档存储库中的推理挑战，需要开发更先进的全局信息综合架构，内存增强的智能体架构是未来发展的重要方向。"}}
{"id": "2601.14958", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14958", "abs": "https://arxiv.org/abs/2601.14958", "authors": ["Minuri Rajapakse", "Ruvan Weerasinghe"], "title": "A Comprehensive Benchmark of Language Models on Unicode and Romanized Sinhala", "comment": "6 pages, 1 figure, 3 tables", "summary": "The performance of Language Models (LMs) on lower-resource, morphologically rich languages like Sinhala remains under-explored, particularly for Romanized Sinhala, which is prevalent in digital communication. This paper presents a comprehensive benchmark of modern LMs on a diverse corpus of Unicode and Romanized Sinhala. We evaluate open-source models using perplexity, a measure of how well a model predicts a text, and leading closed-source models via a qualitative analysis of sentence completion. Our findings reveal that the Mistral-Nemo-Base-2407 model achieves the strongest predictive performance on Unicode text and the Mistral-7B-v0.3 model for Romanized text. The results also highlight the strong all-around performance of the Llama-3.1-8B model for both scripts. Furthermore, a significant performance disparity exists among closed-source models: Gemini-1.5-pro and DeepSeek excel at Unicode generation, whereas Claude-3.5-Sonnet is superior at handling Romanized text. These results provide an essential guide for practitioners selecting models for Sinhala-specific applications and highlight the critical role of training data in handling script variations.", "AI": {"tldr": "本研究对现代语言模型在僧伽罗语（包括 Unicode 和罗马化形式）上的性能进行了基准测试，发现 Mistral-Nemo-Base-2407 在 Unicode 上表现最佳，Mistral-7B-v0.3 在罗马化文本上表现最佳。Llama-3.1-8B 在两种脚本上都表现出色。闭源模型方面，Gemini-1.5-pro 和 DeepSeek 在 Unicode 上表现突出，Claude-3.5-Sonnet 在罗马化文本上更胜一筹。", "motivation": "现有研究对低资源、形态丰富的语言（如僧伽罗语）的语言模型性能探索不足，特别是数字通信中广泛使用的罗马化僧伽罗语。", "method": "研究人员使用包含 Unicode 和罗马化僧伽罗语的语料库，通过困惑度评估开源模型，并通过定性分析句子补全来评估领先的闭源模型。", "result": "Mistral-Nemo-Base-2407 在 Unicode 文本上表现出最强的预测性能，Mistral-7B-v0.3 在罗马化文本上表现最佳。Llama-3.1-8B 在两种脚本上都展现出强大的综合性能。闭源模型在 Gemini-1.5-pro 和 DeepSeek 在 Unicode 生成方面表现优异，而 Claude-3.5-Sonnet 在处理罗马化文本方面更胜一筹。", "conclusion": "本研究结果为在僧伽罗语应用中选择模型提供了重要指导，并强调了训练数据在处理脚本差异方面的重要性。"}}
{"id": "2601.14732", "categories": ["cs.CV", "cs.CL", "cs.MM"], "pdf": "https://arxiv.org/pdf/2601.14732", "abs": "https://arxiv.org/abs/2601.14732", "authors": ["Jing Lan", "Hexiao Ding", "Hongzhao Chen", "Yufeng Jiang", "Nga-Chun Ng", "Gwing Kei Yip", "Gerald W. Y. Cheng", "Yunlin Mao", "Jing Cai", "Liang-ting Lin", "Jung Sun Yoo"], "title": "DeepMoLM: Leveraging Visual and Geometric Structural Information for Molecule-Text Modeling", "comment": "Under review", "summary": "AI models for drug discovery and chemical literature mining must interpret molecular images and generate outputs consistent with 3D geometry and stereochemistry. Most molecular language models rely on strings or graphs, while vision-language models often miss stereochemical details and struggle to map continuous 3D structures into discrete tokens. We propose DeepMoLM: Deep Molecular Language M odeling, a dual-view framework that grounds high-resolution molecular images in geometric invariants derived from molecular conformations. DeepMoLM preserves high-frequency evidence from 1024 $\\times$ 1024 inputs, encodes conformer neighborhoods as discrete Extended 3-Dimensional Fingerprints, and fuses visual and geometric streams with cross-attention, enabling physically grounded generation without atom coordinates. DeepMoLM improves PubChem captioning with a 12.3% relative METEOR gain over the strongest generalist baseline while staying competitive with specialist methods. It produces valid numeric outputs for all property queries and attains MAE 13.64 g/mol on Molecular Weight and 37.89 on Complexity in the specialist setting. On ChEBI-20 description generation from images, it exceeds generalist baselines and matches state-of-the-art vision-language models. Code is available at https://github.com/1anj/DeepMoLM.", "AI": {"tldr": "提出了一种名为DeepMoLM的双视角框架，用于处理分子图像，将高分辨率图像信息与从分子构象中提取的几何不变性相结合，从而实现无需原子坐标即可进行物理基础的分子生成，并在分子图像描述生成和属性预测任务上取得了显著改进。", "motivation": "现有的AI药物发现模型在解释分子图像时，往往难以兼顾3D几何和立体化学信息。基于字符串或图的模型难以处理图像，而视觉-语言模型则在立体化学细节上表现不足，并且难以将连续的3D结构映射到离散的token。", "method": "DeepMoLM是一个双视角框架，它利用来自分子构象的几何不变性来解析高分辨率的分子图像。该模型保留了1024x1024输入的高频信息，将构象体邻域编码为离散的Extended 3-Dimensional Fingerprints，并通过跨注意力机制融合视觉和几何信息流，实现了无需原子坐标即可进行物理基础的生成。", "result": "在PubChem图像描述生成任务上，DeepMoLM比最强的通用基线模型有12.3%的相对METEOR增益。在分子量和复杂性预测任务上，它达到了MAE 13.64 g/mol和37.89的准确率。在ChEBI-20描述生成任务上，DeepMoLM的表现优于通用基线模型，并能与最先进的视觉-语言模型相媲美。", "conclusion": "DeepMoLM通过结合分子图像和几何不变性，成功解决了现有模型在处理分子视觉信息时的局限性，实现了更准确、更具物理意义的分子表示和生成，并在多个下游任务上取得了SOTA的性能。"}}
{"id": "2601.14738", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14738", "abs": "https://arxiv.org/abs/2601.14738", "authors": ["Liqin Wang", "Qianyue Hu", "Wei Lu", "Xiangyang Luo"], "title": "Safeguarding Facial Identity against Diffusion-based Face Swapping via Cascading Pathway Disruption", "comment": null, "summary": "The rapid evolution of diffusion models has democratized face swapping but also raises concerns about privacy and identity security. Existing proactive defenses, often adapted from image editing attacks, prove ineffective in this context. We attribute this failure to an oversight of the structural resilience and the unique static conditional guidance mechanism inherent in face swapping systems. To address this, we propose VoidFace, a systemic defense method that views face swapping as a coupled identity pathway. By injecting perturbations at critical bottlenecks, VoidFace induces cascading disruption throughout the pipeline. Specifically, we first introduce localization disruption and identity erasure to degrade physical regression and semantic embeddings, thereby impairing the accurate modeling of the source face. We then intervene in the generative domain by decoupling attention mechanisms to sever identity injection, and corrupting intermediate diffusion features to prevent the reconstruction of source identity. To ensure visual imperceptibility, we perform adversarial search in the latent manifold, guided by a perceptual adaptive strategy to balance attack potency with image quality. Extensive experiments show that VoidFace outperforms existing defenses across various diffusion-based swapping models, while producing adversarial faces with superior visual quality.", "AI": {"tldr": "提出了一种名为VoidFace的系统性防御方法，通过在人脸交换流水线中的关键瓶颈处注入扰动，来破坏身份信息，从而有效抵御基于扩散模型的换脸攻击，同时保持视觉上的不可感知性。", "motivation": "现有的主动防御方法在应对快速发展的扩散模型人脸交换技术时效果不佳，因为它们未能考虑到人脸交换系统的结构弹性和独特的静态条件引导机制。这引发了对隐私和身份安全的担忧。", "method": "VoidFace将人脸交换视为一个耦合的身份通路。它通过在关键瓶颈处注入扰动来破坏物理回归和语义嵌入（实现身份消除），以及在生成域解耦注意力机制并破坏中间扩散特征（阻止身份重构）。为了保证视觉不可感知性，它在潜在流形上进行对抗性搜索，并采用感知自适应策略来平衡攻击效力和图像质量。", "result": "VoidFace在各种基于扩散模型的换脸模型上，其防御效果优于现有方法，并且生成的对抗性人脸具有更好的视觉质量。", "conclusion": "VoidFace是一种有效的系统性防御方法，能够成功抵御扩散模型的人脸交换攻击，同时保持生成的图像的视觉质量，解决了现有防御方法的不足。"}}
{"id": "2601.15037", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15037", "abs": "https://arxiv.org/abs/2601.15037", "authors": ["Xiaonan Jing", "Gongqing Wu", "Xingrui Zhuo", "Lang Sun", "Jiapu Wang"], "title": "Knowledge Restoration-driven Prompt Optimization: Unlocking LLM Potential for Open-Domain Relational Triplet Extraction", "comment": null, "summary": "Open-domain Relational Triplet Extraction (ORTE) is the foundation for mining structured knowledge without predefined schemas. Despite the impressive in-context learning capabilities of Large Language Models (LLMs), existing methods are hindered by their reliance on static, heuristic-driven prompting strategies. Due to the lack of reflection mechanisms required to internalize erroneous signals, these methods exhibit vulnerability in semantic ambiguity, often making erroneous extraction patterns permanent. To address this bottleneck, we propose a Knowledge Reconstruction-driven Prompt Optimization (KRPO) framework to assist LLMs in continuously improving their extraction capabilities for complex ORTE task flows. Specifically, we design a self-evaluation mechanism based on knowledge restoration, which provides intrinsic feedback signals by projecting structured triplets into semantic consistency scores. Subsequently, we propose a prompt optimizer based on a textual gradient that can internalize historical experiences to iteratively optimize prompts, which can better guide LLMs to handle subsequent extraction tasks. Furthermore, to alleviate relation redundancy, we design a relation canonicalization memory that collects representative relations and provides semantically distinct schemas for the triplets. Extensive experiments across three datasets show that KRPO significantly outperforms strong baselines in the extraction F1 score.", "AI": {"tldr": "提出了一种名为KRPO（Knowledge Reconstruction-driven Prompt Optimization）的框架，通过自评估和基于文本梯度的提示优化，以及关系规范化记忆，来提高大型语言模型在开放域关系三元组提取任务中的性能，减少错误并消除关系冗余。", "motivation": "现有的开放域关系三元组提取方法依赖于静态的、启发式驱动的提示策略，缺乏反思机制，容易在语义歧义中产生永久性的错误提取模式。", "method": "提出KRPO框架，包括：1. 基于知识重建的自评估机制，通过结构化三元组的语义一致性得分提供内在反馈信号。2. 基于文本梯度的提示优化器，通过内化历史经验迭代优化提示。3. 关系规范化记忆，收集代表性关系并提供语义上不同的三元组模式，以缓解关系冗余。", "result": "在三个数据集上的大量实验表明，KRPO在提取F1分数上显著优于强大的基线方法。", "conclusion": "KRPO框架能够有效提升大型语言模型在复杂开放域关系三元组提取任务中的能力，通过持续优化提示来改进提取性能，并解决关系冗余问题。"}}
{"id": "2601.14741", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14741", "abs": "https://arxiv.org/abs/2601.14741", "authors": ["Chongbin Yi", "Yuxin Liang", "Ziqi Zhou", "Peng Yang"], "title": "Enhancing Text-to-Image Generation via End-Edge Collaborative Hybrid Super-Resolution", "comment": "Accpeted by ICC 2026", "summary": "Artificial Intelligence-Generated Content (AIGC) has made significant strides, with high-resolution text-to-image (T2I) generation becoming increasingly critical for improving users' Quality of Experience (QoE). Although resource-constrained edge computing adequately supports fast low-resolution T2I generations, achieving high-resolution output still faces the challenge of ensuring image fidelity at the cost of latency. To address this, we first investigate the performance of super-resolution (SR) methods for image enhancement, confirming a fundamental trade-off that lightweight learning-based SR struggles to recover fine details, while diffusion-based SR achieves higher fidelity at a substantial computational cost. Motivated by these observations, we propose an end-edge collaborative generation-enhancement framework. Upon receiving a T2I generation task, the system first generates a low-resolution image based on adaptively selected denoising steps and super-resolution scales at the edge side, which is then partitioned into patches and processed by a region-aware hybrid SR policy. This policy applies a diffusion-based SR model to foreground patches for detail recovery and a lightweight learning-based SR model to background patches for efficient upscaling, ultimately stitching the enhanced ones into the high-resolution image. Experiments show that our system reduces service latency by 33% compared with baselines while maintaining competitive image quality.", "AI": {"tldr": "提出了一种端到端协同生成-增强框架，通过在边缘端进行低分辨率图像生成并结合区域感知的混合超分辨率策略（前景使用扩散模型，背景使用轻量级学习模型），在降低延迟的同时保持图像质量。", "motivation": "现有方法在边缘端生成高分辨率文本到图像（T2I）时，存在图像保真度和延迟之间的权衡。轻量级超分辨率（SR）模型难以恢复细节，而扩散模型计算成本高。", "method": "1. 在边缘端根据自适应选择的去噪步长和超分辨率尺度生成低分辨率图像。 2. 将低分辨率图像划分为前景和背景区域。 3. 对前景区域应用扩散模型进行细节恢复。 4. 对背景区域应用轻量级学习模型进行高效放大。 5. 将增强后的区域拼接成高分辨率图像。", "result": "与基线方法相比，该系统将服务延迟降低了 33%，同时保持了具有竞争力的图像质量。", "conclusion": "所提出的端到边缘协同生成-增强框架能够有效地解决边缘端高分辨率T2I生成中的延迟和图像质量权衡问题，实现了低延迟和高保真度的目标。"}}
{"id": "2601.14777", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14777", "abs": "https://arxiv.org/abs/2601.14777", "authors": ["Jiaxuan Liu", "Yang Xiang", "Han Zhao", "Xiangang Li", "Zhenhua Ling"], "title": "FunCineForge: A Unified Dataset Toolkit and Model for Zero-Shot Movie Dubbing in Diverse Cinematic Scenes", "comment": null, "summary": "Movie dubbing is the task of synthesizing speech from scripts conditioned on video scenes, requiring accurate lip sync, faithful timbre transfer, and proper modeling of character identity and emotion. However, existing methods face two major limitations: (1) high-quality multimodal dubbing datasets are limited in scale, suffer from high word error rates, contain sparse annotations, rely on costly manual labeling, and are restricted to monologue scenes, all of which hinder effective model training; (2) existing dubbing models rely solely on the lip region to learn audio-visual alignment, which limits their applicability to complex live-action cinematic scenes, and exhibit suboptimal performance in lip sync, speech quality, and emotional expressiveness. To address these issues, we propose FunCineForge, which comprises an end-to-end production pipeline for large-scale dubbing datasets and an MLLM-based dubbing model designed for diverse cinematic scenes. Using the pipeline, we construct the first Chinese television dubbing dataset with rich annotations, and demonstrate the high quality of these data. Experiments across monologue, narration, dialogue, and multi-speaker scenes show that our dubbing model consistently outperforms SOTA methods in audio quality, lip sync, timbre transfer, and instruction following. Code and demos are available at https://anonymous.4open.science/w/FunCineForge.", "AI": {"tldr": "本文提出了一种名为 FunCineForge 的新方法，用于生成高质量的电影配音。该方法包括一个用于创建大规模配音数据集的流程，以及一个基于多模态大型语言模型（MLLM）的配音模型，能够处理单人、旁白、对话和多说话人等复杂场景，并在音质、唇同步、音色迁移和指令遵循方面优于现有技术。", "motivation": "现有电影配音方法面临两个主要限制：1) 高质量的多模态配音数据集规模小、词错误率高、标注稀疏且成本高昂，且仅限于独白场景，这阻碍了有效的模型训练。2) 现有模型仅依赖唇部区域进行音视频对齐，难以适应复杂的真人电影场景，并且在唇同步、语音质量和情感表达方面表现不佳。", "method": "FunCineForge 包含一个端到端的生产流程，用于构建大规模配音数据集，以及一个基于 MLLM 的配音模型。该流程创建了一个包含丰富标注的首个中文电视配音数据集。配音模型设计用于处理各种电影场景。", "result": "在单人、旁白、对话和多说话人场景的实验中，FunCineForge 模型在音频质量、唇同步、音色迁移和指令遵循方面始终优于最先进的方法。", "conclusion": "FunCineForge 成功构建了一个大规模、高质量的中文配音数据集，并提出了一种能够有效处理复杂电影场景的 MLLM 配音模型，该模型在多个关键指标上均取得了显著的性能提升。"}}
{"id": "2601.14994", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14994", "abs": "https://arxiv.org/abs/2601.14994", "authors": ["Chaymaa Abbas", "Nour Shamaa", "Mariette Awad"], "title": "Obscuring Data Contamination Through Translation: Evidence from Arabic Corpora", "comment": null, "summary": "Data contamination undermines the validity of Large Language Model evaluation by enabling models to rely on memorized benchmark content rather than true generalization. While prior work has proposed contamination detection methods, these approaches are largely limited to English benchmarks, leaving multilingual contamination poorly understood. In this work, we investigate contamination dynamics in multilingual settings by fine-tuning several open-weight LLMs on varying proportions of Arabic datasets and evaluating them on original English benchmarks. To detect memorization, we extend the Tested Slot Guessing method with a choice-reordering strategy and incorporate Min-K% probability analysis, capturing both behavioral and distributional contamination signals.\n  Our results show that translation into Arabic suppresses conventional contamination indicators, yet models still benefit from exposure to contaminated data, particularly those with stronger Arabic capabilities. This effect is consistently reflected in rising Mink% scores and increased cross-lingual answer consistency as contamination levels grow. To address this blind spot, we propose Translation-Aware Contamination Detection, which identifies contamination by comparing signals across multiple translated benchmark variants rather than English alone. The Translation-Aware Contamination Detection reliably exposes contamination even when English-only methods fail. Together, our findings highlight the need for multilingual, translation-aware evaluation pipelines to ensure fair, transparent, and reproducible assessment of LLMs.", "AI": {"tldr": "研究发现，将英文基准数据翻译成阿拉伯语进行模型微调，会削弱现有的污染检测指标，但模型仍能从中受益。为解决此问题，提出一种翻译感知污染检测方法，能有效检测多语言污染。", "motivation": "现有的大语言模型评估方法在处理多语言污染方面存在不足，尤其是在非英语基准上，这使得多语言污染的研究和检测变得困难。本研究旨在探索多语言环境下的数据污染动态，并提出相应的检测方法。", "method": "研究人员首先在不同比例的阿拉伯语数据集上微调了几个开源大语言模型，然后在原始英文基准上进行评估。为了检测记忆（即数据污染），研究者扩展了“Tested Slot Guessing”方法，加入了“选择重排序策略”和“Min-K% 概率分析”，以捕捉行为和分布式的污染信号。在此基础上，提出了一种名为“Translation-Aware Contamination Detection”的新方法，该方法通过比较多个翻译后的基准变体的信号来检测污染。", "result": "研究结果表明，即使英文基准数据被翻译成阿拉伯语，模型仍能从被污染的数据中获益，特别是那些阿拉伯语能力更强的模型。这种获益体现在Min-K%得分的提高和跨语言答案一致性的增强。现有的英文污染检测指标在多语言场景下效果不佳。提出的“Translation-Aware Contamination Detection”方法能够有效检测出英文方法失效的污染情况。", "conclusion": "本研究强调了开发多语言、翻译感知的评估流程的必要性，以确保大语言模型评估的公平性、透明性和可重复性。现有的污染检测方法在多语言环境下需要改进，并且需要考虑翻译对污染信号的影响。"}}
{"id": "2601.14724", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.14724", "abs": "https://arxiv.org/abs/2601.14724", "authors": ["Haowei Zhang", "Shudong Yang", "Jinlan Fu", "See-Kiong Ng", "Xipeng Qiu"], "title": "HERMES: KV Cache as Hierarchical Memory for Efficient Streaming Video Understanding", "comment": null, "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated significant improvement in offline video understanding. However, extending these capabilities to streaming video inputs, remains challenging, as existing models struggle to simultaneously maintain stable understanding performance, real-time responses, and low GPU memory overhead. To address this challenge, we propose HERMES, a novel training-free architecture for real-time and accurate understanding of video streams. Based on a mechanistic attention investigation, we conceptualize KV cache as a hierarchical memory framework that encapsulates video information across multiple granularities. During inference, HERMES reuses a compact KV cache, enabling efficient streaming understanding under resource constraints. Notably, HERMES requires no auxiliary computations upon the arrival of user queries, thereby guaranteeing real-time responses for continuous video stream interactions, which achieves 10$\\times$ faster TTFT compared to prior SOTA. Even when reducing video tokens by up to 68% compared with uniform sampling, HERMES achieves superior or comparable accuracy across all benchmarks, with up to 11.4% gains on streaming datasets.", "AI": {"tldr": "提出了一种名为HERMES的训练无关架构，用于实时准确地理解视频流，它通过将KV缓存概念化为分层内存框架，并重用紧凑的KV缓存来实现高效的流式理解，在保证实时响应的同时，显著提高了吞吐量并保持了准确性。", "motivation": "现有的多模态大语言模型（MLLMs）在处理离线视频理解方面取得了显著进展，但将其能力扩展到流式视频输入仍然具有挑战性，因为现有模型难以同时实现稳定的理解性能、实时响应和低GPU内存开销。", "method": "提出了一种名为HERMES的训练无关架构。该方法基于对机制注意力机制的深入研究，将KV缓存概念化为一个能够跨多个粒度封装视频信息的分层内存框架。在推理时，HERMES通过重用一个紧凑的KV缓存来实现高效的流式理解，并且不需要对用户查询进行额外的计算，从而确保了连续视频流交互的实时响应。", "result": "HERMES实现了比现有SOTA模型快10倍的TTFT（Time to First Token）。即使在与均匀采样相比将视频令牌减少多达68%的情况下，HERMES在所有基准测试中也取得了同等或更优的准确性，在流式数据集上取得了高达11.4%的性能提升。", "conclusion": "HERMES是一种有效的训练无关架构，能够以低内存开销实现实时、准确的视频流理解，通过利用分层KV缓存和重用机制，解决了现有MLLMs在流式视频处理方面的挑战，并在吞吐量和准确性方面均取得了显著改进。"}}
{"id": "2601.14822", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14822", "abs": "https://arxiv.org/abs/2601.14822", "authors": ["Volodymyr Sydorskyi", "Igor Krashenyi", "Oleksii Yakubenko"], "title": "Multimodal system for skin cancer detection", "comment": "Accepted to System research and information technologies", "summary": "Melanoma detection is vital for early diagnosis and effective treatment. While deep learning models on dermoscopic images have shown promise, they require specialized equipment, limiting their use in broader clinical settings. This study introduces a multi-modal melanoma detection system using conventional photo images, making it more accessible and versatile. Our system integrates image data with tabular metadata, such as patient demographics and lesion characteristics, to improve detection accuracy. It employs a multi-modal neural network combining image and metadata processing and supports a two-step model for cases with or without metadata. A three-stage pipeline further refines predictions by boosting algorithms and enhancing performance. To address the challenges of a highly imbalanced dataset, specific techniques were implemented to ensure robust training. An ablation study evaluated recent vision architectures, boosting algorithms, and loss functions, achieving a peak Partial ROC AUC of 0.18068 (0.2 maximum) and top-15 retrieval sensitivity of 0.78371. Results demonstrate that integrating photo images with metadata in a structured, multi-stage pipeline yields significant performance improvements. This system advances melanoma detection by providing a scalable, equipment-independent solution suitable for diverse healthcare environments, bridging the gap between specialized and general clinical practices.", "AI": {"tldr": "本研究提出了一种利用普通照片图像和患者元数据进行多模态黑色素瘤检测的系统，通过多阶段处理和专门的训练技术，提高了检测的准确性和可及性。", "motivation": "现有的深度学习黑色素瘤检测方法依赖于专业的皮肤镜设备，限制了其在更广泛临床环境中的应用。因此，研究者希望开发一种使用普通照片图像和元数据，更易于获取和部署的多模态检测系统。", "method": "研究提出了一种多模态神经网络，该网络能够同时处理图像数据和表格元数据（如患者人口统计学信息和病灶特征）。系统采用了两步模型（有/无元数据）和三阶段流水线，其中包含梯度提升算法。为了应对数据不平衡的问题，采用了特定的训练技术。此外，进行了消融研究，评估了不同的视觉架构、梯度提升算法和损失函数。", "result": "通过消融研究，该系统在皮肤镜图像上取得了0.18068的峰值部分ROC AUC（最高为0.2）和0.78371的top-15检索敏感度。结果表明，将照片图像与元数据结合，并采用结构化的多阶段处理流程，可以显著提高性能。", "conclusion": "集成普通照片图像和患者元数据的多模态、多阶段黑色素瘤检测系统，相较于仅使用图像的方法，能够显著提升检测性能。该系统具有可扩展性，不依赖特殊设备，适用于各种医疗环境，弥合了专业和通用临床实践之间的差距。"}}
{"id": "2601.14895", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14895", "abs": "https://arxiv.org/abs/2601.14895", "authors": ["Xinyi Zheng", "Yunze Liu", "Chi-Hao Wu", "Fan Zhang", "Hao Zheng", "Wenqi Zhou", "Walterio W. Mayol-Cuevas", "Junxiao Shen"], "title": "SpatialMem: Unified 3D Memory with Metric Anchoring and Fast Retrieval", "comment": null, "summary": "We present SpatialMem, a memory-centric system that unifies 3D geometry, semantics, and language into a single, queryable representation. Starting from casually captured egocentric RGB video, SpatialMem reconstructs metrically scaled indoor environments, detects structural 3D anchors (walls, doors, windows) as the first-layer scaffold, and populates a hierarchical memory with open-vocabulary object nodes -- linking evidence patches, visual embeddings, and two-layer textual descriptions to 3D coordinates -- for compact storage and fast retrieval. This design enables interpretable reasoning over spatial relations (e.g., distance, direction, visibility) and supports downstream tasks such as language-guided navigation and object retrieval without specialized sensors. Experiments across three real-life indoor scenes demonstrate that SpatialMem maintains strong anchor-description-level navigation completion and hierarchical retrieval accuracy under increasing clutter and occlusion, offering an efficient and extensible framework for embodied spatial intelligence.", "AI": {"tldr": "SpatialMem 是一个内存中心系统，它将 3D 几何、语义和语言统一到一个可查询的表示中，能够从 RGB 视频中重建室内环境，并通过开放词汇对象节点填充分层内存，以支持空间推理和下游任务。", "motivation": "现有方法在处理 3D 几何、语义和语言信息时存在碎片化问题，难以实现统一的可查询表示，限制了在导航和对象检索等下游任务中的应用。", "method": "SpatialMem 从 RGB 视频开始，重建具有度量尺度的室内环境，检测 3D 结构锚点（墙壁、门、窗户）作为第一层脚手架，然后填充一个分层内存，其中包含开放词汇对象节点，将证据片段、视觉嵌入和两层文本描述链接到 3D 坐标。", "result": "SpatialMem 在增加的混乱和遮挡情况下，在锚点-描述级别导航完成率和分层检索准确率方面表现出色，证明了其作为高效可扩展的具身空间智能框架的有效性。", "conclusion": "SpatialMem 提供了一个统一的、可查询的 3D 空间表示，能够有效地处理几何、语义和语言信息，从而支持空间推理和下游任务，并且在各种挑战性条件下都保持了良好的性能。"}}
{"id": "2601.14757", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14757", "abs": "https://arxiv.org/abs/2601.14757", "authors": ["Kangcheng Zhou", "Jun Jiang", "Qing Zhang", "Shuang Zheng", "Qingli Li", "Shugong Xu"], "title": "ReinPath: A Multimodal Reinforcement Learning Approach for Pathology", "comment": null, "summary": "Interpretability is significant in computational pathology, leading to the development of multimodal information integration from histopathological image and corresponding text data.However, existing multimodal methods have limited interpretability due to the lack of high-quality dataset that support explicit reasoning and inference and simple reasoning process.To address the above problems, we introduce a novel multimodal pathology large language model with strong reasoning capabilities.To improve the generation of accurate and contextually relevant textual descriptions, we design a semantic reward strategy integrated with group relative policy optimization.We construct a high-quality pathology visual question answering (VQA) dataset, specifically designed to support complex reasoning tasks.Comprehensive experiments conducted on this dataset demonstrate that our method outperforms state-of-the-art methods, even when trained with only 20% of the data.Our method also achieves comparable performance on downstream zero-shot image classification task compared with CLIP.", "AI": {"tldr": "提出了一种具有强大推理能力的多模态病理大语言模型，并结合语义奖励策略和优化的策略优化来生成准确的文本描述。构建了一个高质量的病理视觉问答数据集，并在该数据集上进行了实验，证明了该方法在推理和零样本图像分类任务上的优越性。", "motivation": "现有的多模态计算病理方法缺乏可解释性，原因在于缺乏支持显式推理和推断的高质量数据集以及简单的推理过程。", "method": "提出了一种新的多模态病理大语言模型，并设计了一种集成了组相对策略优化的语义奖励策略，用于生成准确且与上下文相关的文本描述。构建了一个高质量的病理视觉问答（VQA）数据集，专门用于支持复杂的推理任务。", "result": "所提出的方法在构建的病理VQA数据集上，即使只使用20%的数据进行训练，也优于最先进的方法。在下游的零样本图像分类任务上，该方法取得了与CLIP相当的性能。", "conclusion": "所提出的多模态病理大语言模型通过结合语义奖励策略和优化的策略优化，能够有效地进行推理并生成准确的文本描述。构建的高质量病理VQA数据集为该领域的研究提供了支持，并且该模型在推理和零样本分类任务上展现出强大的能力。"}}
{"id": "2601.14771", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14771", "abs": "https://arxiv.org/abs/2601.14771", "authors": ["Puneet Sharma", "Kristian Dalsbø Hindberg", "Eibe Frank", "Benedicte Schelde-Olesen", "Ulrik Deding"], "title": "Using Multi-Instance Learning to Identify Unique Polyps in Colon Capsule Endoscopy Images", "comment": "19 pages", "summary": "Identifying unique polyps in colon capsule endoscopy (CCE) images is a critical yet challenging task for medical personnel due to the large volume of images, the cognitive load it creates for clinicians, and the ambiguity in labeling specific frames. This paper formulates this problem as a multi-instance learning (MIL) task, where a query polyp image is compared with a target bag of images to determine uniqueness. We employ a multi-instance verification (MIV) framework that incorporates attention mechanisms, such as variance-excited multi-head attention (VEMA) and distance-based attention (DBA), to enhance the model's ability to extract meaningful representations. Additionally, we investigate the impact of self-supervised learning using SimCLR to generate robust embeddings. Experimental results on a dataset of 1912 polyps from 754 patients demonstrate that attention mechanisms significantly improve performance, with DBA L1 achieving the highest test accuracy of 86.26\\% and a test AUC of 0.928 using a ConvNeXt backbone with SimCLR pretraining. This study underscores the potential of MIL and self-supervised learning in advancing automated analysis of Colon Capsule Endoscopy images, with implications for broader medical imaging applications.", "AI": {"tldr": "本文将结肠胶囊内窥镜图像中的息肉唯一性识别问题建模为多实例学习（MIL）任务，并提出了一种包含VEMA和DBA等注意力机制的多实例验证（MIV）框架，同时结合SimCLR进行自监督预训练，显著提升了息肉识别的准确性和AUC。", "motivation": "结肠胶囊内窥镜（CCE）图像数量庞大，给医疗人员带来认知负担，且对特定帧进行标记存在模糊性，因此识别独特的息肉具有挑战性，需要自动化分析方法。", "method": "将问题建模为多实例学习（MIL）任务，采用多实例验证（MIV）框架，并整合了方差激发多头注意力（VEMA）和基于距离的注意力（DBA）机制。同时，使用SimCLR进行自监督学习生成鲁棒的嵌入。实验使用了ConvNeXt作为骨干网络。", "result": "注意力机制显著提高了模型性能。其中，DBA L1模型在ConvNeXt骨干网络和SimCLR预训练下，达到了86.26%的测试准确率和0.928的测试AUC。", "conclusion": "多实例学习（MIL）和自监督学习在推进结肠胶囊内窥镜（CCE）图像的自动化分析方面具有巨大潜力，并可能对更广泛的医学影像应用产生积极影响。"}}
{"id": "2601.15077", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.15077", "abs": "https://arxiv.org/abs/2601.15077", "authors": ["Christopher Scofield"], "title": "Multi-Agent Constraint Factorization Reveals Latent Invariant Solution Structure", "comment": null, "summary": "Multi-agent systems (MAS) composed of large language models often exhibit improved problem-solving performance despite operating on identical information. In this work, we provide a formal explanation for this phenomenon grounded in operator theory and constrained optimization. We model each agent as enforcing a distinct family of validity constraints on a shared solution state, and show that a MAS implements a factorized composition of constraint-enforcement operators. Under mild conditions, these dynamics converge to invariant solution sets defined by the intersection of agent constraint sets. Such invariant structures are generally not dynamically accessible to a single agent applying all constraints simultaneously, even when expressive capacity and information are identical. We extend this result from exact constraint enforcement to soft constraints via proximal operators, and apply the formalism to contemporary text-based dialog systems.", "AI": {"tldr": "本文利用算子理论和约束优化，为大型语言模型（LLMs）组成的多个智能体系统（MAS）在信息相同的情况下表现出更优越的问题解决能力提供了理论解释。MAS中的每个智能体被建模为对共享解决方案状态施加不同的有效性约束，并证明MAS实现了约束执行算子的因子化组合。在温和条件下，这些动态会收敛到由智能体约束集交集定义的不变解决方案集。这种不变结构通常是单个智能体同时应用所有约束（即使在表达能力和信息相同的情况下）无法动态达到的。研究还从精确约束执法扩展到通过近邻算子处理软约束，并将其应用于当前的基于文本的对话系统。", "motivation": "现有研究观察到，由大型语言模型（LLMs）组成的多个智能体系统（MAS）在信息完全相同的情况下，往往能解决问题得更好。然而，这种现象背后的原因缺乏正式的解释。", "method": "研究采用算子理论和约束优化方法。将每个智能体建模为对共享解决方案状态施加一组独特的有效性约束。分析MAS如何实现约束执行算子的因子化组合，并研究在温和条件下的动态收敛性，特别关注不变解决方案集。", "result": "MAS的约束执行动态趋向于收敛到一个由所有智能体的约束集交集定义的不变解决方案集。这种不变结构对于单个智能体来说，即使拥有相同的表达能力和信息，也难以动态访问。研究还将该理论扩展到使用近邻算子处理软约束，并将其应用于文本对话系统。", "conclusion": "MAS通过施加不同的约束并以因子化方式组合这些约束，能够探索和收敛到比单个智能体更优的解决方案集。这种通过多智能体协作实现的约束交集是其优越性能的关键。"}}
{"id": "2601.14951", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.14951", "abs": "https://arxiv.org/abs/2601.14951", "authors": ["Carolin Holtermann", "Nina Krebs", "Anne Lauscher"], "title": "TempViz: On the Evaluation of Temporal Knowledge in Text-to-Image Models", "comment": null, "summary": "Time alters the visual appearance of entities in our world, like objects, places, and animals. Thus, for accurately generating contextually-relevant images, knowledge and reasoning about time can be crucial (e.g., for generating a landscape in spring vs. in winter). Yet, although substantial work exists on understanding and improving temporal knowledge in natural language processing, research on how temporal phenomena appear and are handled in text-to-image (T2I) models remains scarce. We address this gap with TempViz, the first data set to holistically evaluate temporal knowledge in image generation, consisting of 7.9k prompts and more than 600 reference images. Using TempViz, we study the capabilities of five T2I models across five temporal knowledge categories. Human evaluation shows that temporal competence is generally weak, with no model exceeding 75% accuracy across categories. Towards larger-scale studies, we also examine automated evaluation methods, comparing several established approaches against human judgments. However, none of these approaches provides a reliable assessment of temporal cues - further indicating the pressing need for future research on temporal knowledge in T2I.", "AI": {"tldr": "该研究提出了TempViz数据集，用于评估文本到图像（T2I）模型在理解和生成与时间相关的图像方面的能力，结果显示现有模型的表现普遍较弱，且自动评估方法尚不可靠。", "motivation": "现有的文本到图像生成模型在处理时间变化对视觉外观的影响方面存在不足，缺乏对时间知识的系统性评估，因此需要一个专门的数据集来研究和改进这一问题。", "method": "构建了包含7.9k个提示和600多张参考图像的TempViz数据集，并使用该数据集对五个T2I模型在五个时间知识类别上的表现进行了评估，同时研究了自动评估方法的有效性。", "result": "人类评估结果表明，现有T2I模型在时间知识方面的能力普遍较弱，没有一个模型的准确率能超过75%；比较了多种自动评估方法，发现它们都无法可靠地评估时间线索。", "conclusion": "TempViz数据集揭示了当前T2I模型在处理时间相关图像生成方面的局限性，并强调了在T2I领域进行未来时间知识研究的必要性，同时也指出了自动评估方法的不足。"}}
{"id": "2601.15129", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.15129", "abs": "https://arxiv.org/abs/2601.15129", "authors": ["Yishu Wei", "Adam E. Flanders", "Errol Colak", "John Mongan", "Luciano M Prevedello", "Po-Hao Chen", "Henrique Min Ho Lee", "Gilberto Szarf", "Hamilton Shoji", "Jason Sho", "Katherine Andriole", "Tessa Cook", "Lisa C. Adams", "Linda C. Chu", "Maggie Chung", "Geraldine Brusca-Augello", "Djeven P. Deva", "Navneet Singh", "Felipe Sanchez Tijmes", "Jeffrey B. Alpert", "Elsie T. Nguyen", "Drew A. Torigian", "Kate Hanneman", "Lauren K Groner", "Alexander Phan", "Ali Islam", "Matias F. Callejas", "Gustavo Borges da Silva Teles", "Faisal Jamal", "Maryam Vazirabad", "Ali Tejani", "Hari Trivedi", "Paulo Kuriki", "Rajesh Bhayana", "Elana T. Benishay", "Yi Lin", "Yifan Peng", "George Shih"], "title": "RSNA Large Language Model Benchmark Dataset for Chest Radiographs of Cardiothoracic Disease: Radiologist Evaluation and Validation Enhanced by AI Labels (REVEAL-CXR)", "comment": null, "summary": "Multimodal large language models have demonstrated comparable performance to that of radiology trainees on multiple-choice board-style exams. However, to develop clinically useful multimodal LLM tools, high-quality benchmarks curated by domain experts are essential. To curate released and holdout datasets of 100 chest radiographic studies each and propose an artificial intelligence (AI)-assisted expert labeling procedure to allow radiologists to label studies more efficiently. A total of 13,735 deidentified chest radiographs and their corresponding reports from the MIDRC were used. GPT-4o extracted abnormal findings from the reports, which were then mapped to 12 benchmark labels with a locally hosted LLM (Phi-4-Reasoning). From these studies, 1,000 were sampled on the basis of the AI-suggested benchmark labels for expert review; the sampling algorithm ensured that the selected studies were clinically relevant and captured a range of difficulty levels. Seventeen chest radiologists participated, and they marked \"Agree all\", \"Agree mostly\" or \"Disagree\" to indicate their assessment of the correctness of the LLM suggested labels. Each chest radiograph was evaluated by three experts. Of these, at least two radiologists selected \"Agree All\" for 381 radiographs. From this set, 200 were selected, prioritizing those with less common or multiple finding labels, and divided into 100 released radiographs and 100 reserved as the holdout dataset. The holdout dataset is used exclusively by RSNA to independently evaluate different models. A benchmark of 200 chest radiographic studies with 12 benchmark labels was created and made publicly available https://imaging.rsna.org, with each chest radiograph verified by three radiologists. In addition, an AI-assisted labeling procedure was developed to help radiologists label at scale, minimize unnecessary omissions, and support a semicollaborative environment.", "AI": {"tldr": "该研究开发了一个包含200张胸部X光片的基准数据集，用于评估医学影像AI模型，并提出了一种AI辅助的专家标注流程，以提高标注效率。", "motivation": "为了开发临床实用的多模态大型语言模型（LLM）工具，需要领域专家精心策划的高质量基准数据集。", "method": "使用GPT-4o和Phi-4-Reasoning LLM从13,735张胸部X光片报告中提取异常发现，并映射到12个基准标签。然后，17位胸部放射科医生对AI建议的标签进行评估，最终创建了一个包含200张X光片（100张发布，100张留用）的基准数据集。开发了AI辅助标注流程以提高效率。", "result": "创建了一个包含200张胸部X光片和12个基准标签的基准数据集，该数据集已公开可用。AI辅助标注流程能够帮助放射科医生更有效地进行标注。", "conclusion": "该研究成功构建了一个经过多位放射科医生验证的胸部X光片基准数据集，并提出了一种AI辅助标注方法，为开发和评估医学影像AI模型提供了重要的资源和工具。"}}
{"id": "2601.15091", "categories": ["cs.CL", "cs.CY", "cs.SI", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2601.15091", "abs": "https://arxiv.org/abs/2601.15091", "authors": ["Vuong Hung Truong", "Mariana Gabrielle Cangco Reyes", "Masatoshi Koizumi", "Jihwan Myung"], "title": "Circadian Modulation of Semantic Exploration in Social Media Language", "comment": "25 pages, 6 figures, 3 supplementary figures", "summary": "Human cognition exhibits strong circadian modulation, yet its influence on high-dimensional semantic behavior remains poorly understood. Using large-scale Reddit data, we quantify time-of-day variation in language use by embedding text into a pretrained transformer model and measuring semantic entropy as an index of linguistic exploration-exploitation, for which we show a robust circadian rhythmicity that could be entrained by seasonal light cues. Distinguishing between local and global semantic entropy reveals a systematic temporal dissociation: local semantic exploration peaks in the morning, reflecting broader exploration of semantic space, whereas global semantic diversity peaks later in the day as submissions accumulate around already established topics, consistent with \"rich-get-richer\" dynamics. These patterns are not explained by sentiment or affective valence, indicating that semantic exploration captures a cognitive dimension distinct from mood. The observed temporal structure aligns with known diurnal patterns in neuromodulatory systems, suggesting that biological circadian rhythms extend to the semantic domain.", "AI": {"tldr": "研究发现，人类在一天中的认知活动存在显著的昼夜节律变化，这种变化体现在语言使用和语义探索上，并可能与生物钟和季节性光照有关。", "motivation": "人类的认知功能受到昼夜节律的强烈影响，但这种影响在高维度的语义行为上如何体现仍不清楚。", "method": "利用大规模Reddit数据，将文本嵌入预训练的Transformer模型，并测量语义熵来量化语言使用的探索-利用程度，分析其昼夜节律变化。", "result": "发现了语言使用存在稳健的昼夜节律，局部语义探索在早晨达到峰值，而全局语义多样性在一天晚些时候达到峰值，呈现“富者愈富”的动态。这些模式不受情感或情绪影响，且与神经调质系统的昼夜节律模式一致。", "conclusion": "昼夜节律不仅影响认知，也延伸到了语义领域，揭示了生物钟对人类语言和思维方式的深层影响。"}}
{"id": "2601.14742", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14742", "abs": "https://arxiv.org/abs/2601.14742", "authors": ["Ami Pandat", "Kanyala Muvva", "Punna Rajasekhar", "Gopika Vinod", "Rohit Shukla"], "title": "SimD3: A Synthetic drone Dataset with Payload and Bird Distractor Modeling for Robust Detection", "comment": null, "summary": "Reliable drone detection is challenging due to limited annotated real-world data, large appearance variability, and the presence of visually similar distractors such as birds. To address these challenges, this paper introduces SimD3, a large-scale high-fidelity synthetic dataset designed for robust drone detection in complex aerial environments. Unlike existing synthetic drone datasets, SimD3 explicitly models drones with heterogeneous payloads, incorporates multiple bird species as realistic distractors, and leverages diverse Unreal Engine 5 environments with controlled weather, lighting, and flight trajectories captured using a 360 six-camera rig. Using SimD3, we conduct an extensive experimental evaluation within the YOLOv5 detection framework, including an attention-enhanced variant termed Yolov5m+C3b, where standard bottleneck-based C3 blocks are replaced with C3b modules. Models are evaluated on synthetic data, combined synthetic and real data, and multiple unseen real-world benchmarks to assess robustness and generalization. Experimental results show that SimD3 provides effective supervision for small-object drone detection and that Yolov5m+C3b consistently outperforms the baseline across in-domain and cross-dataset evaluations. These findings highlight the utility of SimD3 for training and benchmarking robust drone detection models under diverse and challenging conditions.", "AI": {"tldr": "本文提出了一个名为SimD3的大规模高保真合成数据集，用于提高无人机检测的鲁棒性。通过在YOLOv5框架下进行广泛实验，并引入一种改进模型Yolov5m+C3b，证明了SimD3在训练和评估无人机检测模型方面的有效性，尤其是在小目标检测和跨数据集泛化能力上。Yolov5m+C3b模型在各项评估中均优于基线模型。", "motivation": "现有的无人机检测方法面临数据不足、外观变化大以及存在视觉相似干扰物（如鸟类）等挑战。需要一个高质量的数据集来解决这些问题，提高无人机检测的鲁棒性和泛化能力。", "method": "构建了一个名为SimD3的大规模高保真合成数据集，该数据集包含具有不同载荷的无人机、多种鸟类作为干扰物，并利用Unreal Engine 5在各种环境（包括不同的天气、光照和飞行轨迹）下生成。在YOLOv5检测框架下，作者评估了标准YOLOv5以及一种改进的变体Yolov5m+C3b（将C3模块替换为C3b模块）。模型在合成数据、合成与真实数据混合以及多个未见过的真实世界基准上进行评估。", "result": "SimD3数据集有效地用于小目标无人机检测的监督学习。Yolov5m+C3b模型在域内（in-domain）和跨数据集（cross-dataset）评估中均持续优于基线YOLOv5模型。这表明SimD3有助于训练出在各种挑战性条件下具有鲁棒性的无人机检测模型。", "conclusion": "SimD3数据集为训练和评估鲁棒的无人机检测模型提供了一个有用的资源。提出的Yolov5m+C3b模型在利用该数据集进行训练和评估时，展现出优越的性能，尤其是在处理复杂场景和小目标检测方面。"}}
{"id": "2601.14875", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.14875", "abs": "https://arxiv.org/abs/2601.14875", "authors": ["Zhe Chang", "Haodong Jin", "Ying Sun", "Yan Song", "Hui Yu"], "title": "GAT-NeRF: Geometry-Aware-Transformer Enhanced Neural Radiance Fields for High-Fidelity 4D Facial Avatars", "comment": null, "summary": "High-fidelity 4D dynamic facial avatar reconstruction from monocular video is a critical yet challenging task, driven by increasing demands for immersive virtual human applications. While Neural Radiance Fields (NeRF) have advanced scene representation, their capacity to capture high-frequency facial details, such as dynamic wrinkles and subtle textures from information-constrained monocular streams, requires significant enhancement. To tackle this challenge, we propose a novel hybrid neural radiance field framework, called Geometry-Aware-Transformer Enhanced NeRF (GAT-NeRF) for high-fidelity and controllable 4D facial avatar reconstruction, which integrates the Transformer mechanism into the NeRF pipeline. GAT-NeRF synergistically combines a coordinate-aligned Multilayer Perceptron (MLP) with a lightweight Transformer module, termed as Geometry-Aware-Transformer (GAT) due to its processing of multi-modal inputs containing explicit geometric priors. The GAT module is enabled by fusing multi-modal input features, including 3D spatial coordinates, 3D Morphable Model (3DMM) expression parameters, and learnable latent codes to effectively learn and enhance feature representations pertinent to fine-grained geometry. The Transformer's effective feature learning capabilities are leveraged to significantly augment the modeling of complex local facial patterns like dynamic wrinkles and acne scars. Comprehensive experiments unequivocally demonstrate GAT-NeRF's state-of-the-art performance in visual fidelity and high-frequency detail recovery, forging new pathways for creating realistic dynamic digital humans for multimedia applications.", "AI": {"tldr": "该研究提出了一种名为GAT-NeRF的新型混合神经辐射场框架，用于从单目视频中高保真地重建动态面部头像，它集成了Transformer机制，并利用3DMM表达式参数等几何先验来增强细节捕捉，取得了最先进的性能。", "motivation": "现有基于NeRF的方法在从信息受限的单目视频流中捕捉高频面部细节（如动态皱纹和细微纹理）方面能力有限，而沉浸式虚拟人类应用对高保真动态面部头像重建的需求日益增长。", "method": "提出GAT-NeRF框架，该框架结合了坐标对齐的多层感知机（MLP）和一个轻量级的Transformer模块（Geometry-Aware-Transformer，GAT）。GAT通过融合3D空间坐标、3D Morphable Model（3DMM）表情参数和可学习的潜在编码等模态输入特征，来学习和增强与精细几何相关的特征表示，从而增强对动态皱纹和痘疤等复杂局部面部模式的建模。", "result": "实验结果表明，GAT-NeRF在视觉保真度和高频细节恢复方面达到了最先进的性能。", "conclusion": "GAT-NeRF通过集成Transformer和利用几何先验，有效地解决了单目视频中动态面部头像重建的挑战，为创建逼真的动态数字人类开辟了新途径。"}}
{"id": "2601.15161", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15161", "abs": "https://arxiv.org/abs/2601.15161", "authors": ["Yinzhu Chen", "Abdine Maiga", "Hossein A. Rahmani", "Emine Yilmaz"], "title": "Automated Rubrics for Reliable Evaluation of Medical Dialogue Systems", "comment": null, "summary": "Large Language Models (LLMs) are increasingly used for clinical decision support, where hallucinations and unsafe suggestions may pose direct risks to patient safety. These risks are particularly challenging as they often manifest as subtle clinical errors that evade detection by generic metrics, while expert-authored fine-grained rubrics remain costly to construct and difficult to scale. In this paper, we propose a retrieval-augmented multi-agent framework designed to automate the generation of instance-specific evaluation rubrics. Our approach grounds evaluation in authoritative medical evidence by decomposing retrieved content into atomic facts and synthesizing them with user interaction constraints to form verifiable, fine-grained evaluation criteria. Evaluated on HealthBench, our framework achieves a Clinical Intent Alignment (CIA) score of 60.12%, a statistically significant improvement over the GPT-4o baseline (55.16%). In discriminative tests, our rubrics yield a mean score delta ($μ_Δ = 8.658$) and an AUROC of 0.977, nearly doubling the quality separation achieved by GPT-4o baseline (4.972). Beyond evaluation, our rubrics effectively guide response refinement, improving quality by 9.2% (from 59.0% to 68.2%). This provides a scalable and transparent foundation for both evaluating and improving medical LLMs. The code is available at https://anonymous.4open.science/r/Automated-Rubric-Generation-AF3C/.", "AI": {"tldr": "本研究提出了一种检索增强的多智能体框架，用于自动生成特定实例的评估细则，以解决大型语言模型（LLMs）在临床决策支持中存在的幻觉和不安全建议问题。该框架通过检索权威医疗证据，将其分解为原子事实，并与用户交互约束相结合，形成可验证的细粒度评估标准。", "motivation": "大型语言模型（LLMs）在临床决策支持中的应用存在幻觉和不安全建议的风险，这些风险难以被通用指标检测，而专家制定的细粒度评估标准成本高昂且难以扩展。", "method": "提出了一种检索增强的多智能体框架，该框架通过以下步骤自动生成特定实例的评估细则：1. 检索权威医疗证据；2. 将检索到的内容分解为原子事实；3. 将原子事实与用户交互约束相结合；4. 形成可验证的、细粒度的评估标准。", "result": "在HealthBench上评估，该框架实现了60.12%的临床意图对齐（CIA）分数，显著优于GPT-4o基线（55.16%）。在判别性测试中，其细则产生了8.658的平均分数差（$μ_Δ$）和0.977的AUROC，几乎是GPT-4o基线（4.972）的两倍。此外，该细则还能有效指导响应优化，将质量提高了9.2%（从59.0%提高到68.2%）。", "conclusion": "该框架为评估和改进医疗LLMs提供了一个可扩展且透明的基础，能够自动生成特定实例的评估细则，从而提高评估的准确性和LLM的性能。"}}
{"id": "2601.15050", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.15050", "abs": "https://arxiv.org/abs/2601.15050", "authors": ["Zhichao Yan", "Yunxiao Zhao", "Jiapu Wang", "Jiaoyan Chen", "Shaoru Guo", "Xiaoli Li", "Ru Li", "Jeff Z. Pan"], "title": "\\textsc{LogicScore}: Fine-grained Logic Evaluation of Conciseness, Completeness, and Determinateness in Attributed Question Answering", "comment": null, "summary": "Current evaluation methods for Attributed Question Answering (AQA) suffer from \\textit{attribution myopia}: they emphasize verification of isolated statements and their attributions but overlook the global logical integrity of long-form answers. Consequently, Large Language Models (LLMs) often produce factually grounded yet logically incoherent responses with elusive deductive gaps. To mitigate this limitation, we present \\textsc{LogicScore}, a unified evaluation framework that shifts the paradigm from local assessment to global reasoning scrutiny. Grounded in Horn Rules, our approach integrates a backward verification mechanism to systematically evaluate three key reasoning dimensions: \\textit{Completeness} (logically sound deduction), \\textit{Conciseness} (non-redundancy), and \\textit{Determinateness} (consistent answer entailment). Extensive experiments across three multi-hop QA datasets (HotpotQA, MusiQue, and 2WikiMultiHopQA) and over 20 LLMs (including GPT-5, Gemini-3-Pro, LLaMA3, and task-specific tuned models) reveal a critical capability gap: leading models often achieve high attribution scores (e.g., 92.85\\% precision for Gemini-3 Pro) but struggle with global reasoning quality (e.g., 35.11\\% Conciseness for Gemini-3 Pro). Our work establishes a robust standard for logical evaluation, highlighting the need to prioritize reasoning coherence alongside factual grounding in LLM development. Codes are available at: https://github.com/zhichaoyan11/LogicScore.", "AI": {"tldr": "提出了一种名为 LogicScore 的新评估框架，用于评估长篇问答的逻辑一致性，弥补了现有方法只关注局部事实准确性的不足。该框架基于 Horn 规则，从完整性、简洁性和确定性三个维度评估模型的全局推理能力。", "motivation": "现有 Attributed Question Answering (AQA) 的评估方法存在“归因近视”问题，只关注孤立陈述的事实准确性和归因，忽略了长篇答案的整体逻辑完整性，导致大型语言模型（LLMs）生成事实正确但逻辑不连贯的答案。", "method": "提出 LogicScore 评估框架，基于 Horn 规则，整合后向验证机制，从三个维度（完整性、简洁性、确定性）对答案进行全局推理评估。", "result": "在 HotpotQA、MusiQue 和 2WikiMultiHopQA 数据集上对超过 20 个 LLMs 进行的实验表明，主流模型虽然在归因分数上表现优异，但在全局推理质量（如简洁性）方面存在显著差距。", "conclusion": "LogicScore 建立了一个鲁棒的逻辑评估标准，强调了在 LLM 开发中，除了事实准确性之外，优先考虑推理一致性的重要性。"}}
{"id": "2601.14774", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14774", "abs": "https://arxiv.org/abs/2601.14774", "authors": ["Keita Takeda", "Tomoya Sakai"], "title": "Does medical specialization of VLMs enhance discriminative power?: A comprehensive investigation through feature distribution analysis", "comment": "A short version paper of this research has been accepted for The IEEE International Symposium on Biomedical Imaging (ISBI) 2026", "summary": "This study investigates the feature representations produced by publicly available open source medical vision-language models (VLMs). While medical VLMs are expected to capture diagnostically relevant features, their learned representations remain underexplored, and standard evaluations like classification accuracy do not fully reveal if they acquire truly discriminative, lesion-specific features. Understanding these representations is crucial for revealing medical image structures and improving downstream tasks in medical image analysis. This study aims to investigate the feature distributions learned by medical VLMs and evaluate the impact of medical specialization. We analyze the feature distribution of multiple image modalities extracted by some representative medical VLMs across lesion classification datasets on multiple modalities. These distributions were compared them with non-medical VLMs to assess the domain-specific medical training. Our experiments showed that medical VLMs can extract discriminative features that are effective for medical classification tasks. Moreover, it was found that non-medical VLMs with recent improvement with contextual enrichment such as LLM2CLIP produce more refined feature representations. Our results imply that enhancing text encoder is more crucial than training intensively on medical images when developing medical VLMs. Notably, non-medical models are particularly vulnerable to biases introduced by overlaied text strings on images. These findings underscore the need for careful consideration on model selection according to downstream tasks besides potential risks in inference due to background biases such as textual information in images.", "AI": {"tldr": "本研究分析了开源医学视觉语言模型（VLMs）学习到的特征表示，发现医学VLMs能提取区分性特征，但非医学VLMs（如LLM2CLIP）在上下文丰富后表现更优。研究强调了文本编码器对模型性能的重要性，并指出了非医学模型易受图像中背景文本偏见的影响。", "motivation": "目前对医学VLMs学习到的特征表示知之甚少，现有的评估方法（如分类准确率）无法完全揭示其是否学习到具有诊断价值的、特定于病灶的特征。理解这些表示对于揭示医学图像结构和改进下游医学图像分析任务至关重要。", "method": "分析了代表性医学VLMs提取的多种图像模态的特征分布，并将其与非医学VLMs进行比较，以评估领域特定医学训练的影响。实验在多个模态的病灶分类数据集上进行。", "result": "医学VLMs能够提取出对医学分类任务有效的区分性特征。然而，经过上下文丰富（如LLM2CLIP）的非医学VLMs能够产生更精细的特征表示。非医学模型容易受到图像上叠加文本字符串引入的偏见的影响。", "conclusion": "增强文本编码器比仅在医学图像上进行密集训练对于开发医学VLMs更为关键。在选择模型时，需要根据下游任务仔细考虑，并警惕因图像背景文本等引起的潜在推理风险。"}}
{"id": "2601.15042", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15042", "abs": "https://arxiv.org/abs/2601.15042", "authors": ["Andrea Protani", "Riccardo Taiello", "Marc Molina Van Den Bosch", "Luigi Serio"], "title": "Federated Transformer-GNN for Privacy-Preserving Brain Tumor Localization with Modality-Level Explainability", "comment": null, "summary": "Deep learning models for brain tumor analysis require large and diverse datasets that are often siloed across healthcare institutions due to privacy regulations. We present a federated learning framework for brain tumor localization that enables multi-institutional collaboration without sharing sensitive patient data. Our method extends a hybrid Transformer-Graph Neural Network architecture derived from prior decoder-free supervoxel GNNs and is deployed within CAFEIN\\textsuperscript{\\textregistered}, CERN's federated learning platform designed for healthcare environments. We provide an explainability analysis through Transformer attention mechanisms that reveals which MRI modalities drive the model predictions. Experiments on the BraTS dataset demonstrate a key finding: while isolated training on individual client data triggers early stopping well before reaching full training capacity, federated learning enables continued model improvement by leveraging distributed data, ultimately matching centralized performance. This result provides strong justification for federated learning when dealing with complex tasks and high-dimensional input data, as aggregating knowledge from multiple institutions significantly benefits the learning process. Our explainability analysis, validated through rigorous statistical testing on the full test set (paired t-tests with Bonferroni correction), reveals that deeper network layers significantly increase attention to T2 and FLAIR modalities ($p<0.001$, Cohen's $d$=1.50), aligning with clinical practice.", "AI": {"tldr": "研究提出了一种基于联邦学习的脑肿瘤定位框架，通过CAFEIN平台实现多机构协作，无需共享敏感数据。该框架结合了Transformer和图神经网络，实验证明联邦学习能克服单机构训练的局限，达到与集中式训练相当的性能。解释性分析揭示了模型对T2和FLAIR MRI模态的关注程度随网络深度增加而显著提高。", "motivation": "由于隐私法规限制，医疗机构之间的数据通常是孤立的，这阻碍了深度学习模型在脑肿瘤分析等需要大量多样化数据集的任务上的发展。因此，研究旨在开发一种能够实现多机构协作而无需共享敏感数据的联邦学习框架。", "method": "研究提出了一种混合Transformer-图神经网络（GNN）架构，该架构基于先前的解码器无关的超体素GNN，并部署在CERN的CAFEIN®联邦学习平台。该方法利用Transformer的注意力机制进行可解释性分析，以揭示哪些MRI模态驱动模型预测。", "result": "在BraTS数据集上的实验表明，与在单个客户端数据上孤立训练导致在达到完全训练容量之前就触发早期停止不同，联邦学习通过利用分布式数据，能够持续改进模型，并最终达到与集中式训练相当的性能。此外，解释性分析显示，随着网络深度的增加，模型对T2和FLAIR模态的关注度显著提高（p<0.001，Cohen's d=1.50）。", "conclusion": "该研究结果有力地证明了在处理复杂任务和高维输入数据时，联邦学习的优势。通过整合来自多个机构的知识，可以显著促进学习过程。同时，解释性分析也支持了模型在MRI模态选择上与临床实践的一致性。"}}
{"id": "2601.14776", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14776", "abs": "https://arxiv.org/abs/2601.14776", "authors": ["Xiaofan Yang", "Yubin Liu", "Wei Pan", "Guoqing Chu", "Junming Zhang", "Jie Zhao", "Zhuoqi Man", "Xuanming Cao"], "title": "M2I2HA: A Multi-modal Object Detection Method Based on Intra- and Inter-Modal Hypergraph Attention", "comment": "43 pages, 13 figures", "summary": "Recent advances in multi-modal detection have significantly improved detection accuracy in challenging environments (e.g., low light, overexposure). By integrating RGB with modalities such as thermal and depth, multi-modal fusion increases data redundancy and system robustness. However, significant challenges remain in effectively extracting task-relevant information both within and across modalities, as well as in achieving precise cross-modal alignment. While CNNs excel at feature extraction, they are limited by constrained receptive fields, strong inductive biases, and difficulty in capturing long-range dependencies. Transformer-based models offer global context but suffer from quadratic computational complexity and are confined to pairwise correlation modeling. Mamba and other State Space Models (SSMs), on the other hand, are hindered by their sequential scanning mechanism, which flattens 2D spatial structures into 1D sequences, disrupting topological relationships and limiting the modeling of complex higher-order dependencies. To address these issues, we propose a multi-modal perception network based on hypergraph theory called M2I2HA. Our architecture includes an Intra-Hypergraph Enhancement module to capture global many-to-many high-order relationships within each modality, and an Inter-Hypergraph Fusion module to align, enhance, and fuse cross-modal features by bridging configuration and spatial gaps between data sources. We further introduce a M2-FullPAD module to enable adaptive multi-level fusion of multi-modal enhanced features within the network, meanwhile enhancing data distribution and flow across the architecture. Extensive object detection experiments on multiple public datasets against baselines demonstrate that M2I2HA achieves state-of-the-art performance in multi-modal object detection tasks.", "AI": {"tldr": "本文提出了一种基于超图理论的多模态感知网络M2I2HA，用于解决多模态目标检测中模态内信息提取和模态间对齐的挑战，并在多个数据集上取得了最先进的性能。", "motivation": "现有CNN模型受限于感受野和长距离依赖建模能力，Transformer模型计算复杂度高且仅能建模成对关联，而SSMs（如Mamba）的顺序扫描机制会破坏2D空间拓扑关系。因此，需要一种更有效的方法来提取模态内和模态间的任务相关信息，并实现精确的跨模态对齐。", "method": "提出了一种基于超图理论的多模态感知网络M2I2HA，包含：1) Intra-Hypergraph Enhancement模块，用于捕获模态内的全局多对多高阶关系；2) Inter-Hypergraph Fusion模块，通过跨越配置和空间差异来对齐、增强和融合跨模态特征；3) M2-FullPAD模块，实现多模态增强特征的自适应多层融合，并优化数据分布和流动。", "result": "在多个公开数据集上进行的广泛目标检测实验表明，M2I2HA在多模态目标检测任务中达到了最先进的性能，优于现有基线模型。", "conclusion": "M2I2HA网络通过利用超图理论有效解决了多模态目标检测中的关键挑战，展现了其在提取模态内高阶关系和实现精确跨模态融合方面的优势，从而提高了检测性能。"}}
{"id": "2601.15061", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.15061", "abs": "https://arxiv.org/abs/2601.15061", "authors": ["Qiwei Ma", "Jun Zhang"], "title": "Differential Privacy Image Generation with Reconstruction Loss and Noise Injection Using an Error Feedback SGD", "comment": null, "summary": "Traditional data masking techniques such as anonymization cannot achieve the expected privacy protection while ensuring data utility for privacy-preserving machine learning. Synthetic data plays an increasingly important role as it generates a large number of training samples and prevents information leakage in real data. The existing methods suffer from the repeating trade-off processes between privacy and utility. We propose a novel framework for differential privacy generation, which employs an Error Feedback Stochastic Gradient Descent(EFSGD) method and introduces a reconstruction loss and noise injection mechanism into the training process. We generate images with higher quality and usability under the same privacy budget as the related work. Extensive experiments demonstrate the effectiveness and generalization of our proposed framework for both grayscale and RGB images. We achieve state-of-the-art results over almost all metrics on three benchmarks: MNIST, Fashion-MNIST, and CelebA.", "AI": {"tldr": "本文提出了一种新的差分隐私生成框架，通过误差反馈随机梯度下降（EFSGD）、重建损失和噪声注入机制，在相同的隐私预算下生成高质量、高可用性的图像，并在三个基准数据集上取得了最先进的结果。", "motivation": "现有的数据脱敏技术（如匿名化）在保证数据效用的同时无法实现预期的隐私保护，而合成数据在隐私保护机器学习中扮演着越来越重要的角色。然而，现有合成数据生成方法在隐私和效用之间存在重复的权衡问题。", "method": "提出了一种新的差分隐私生成框架，该框架采用误差反馈随机梯度下降（EFSGD）方法，并将重建损失和噪声注入机制引入训练过程。", "result": "在相同的隐私预算下，生成了更高质量和可用性的图像。在MNIST、Fashion-MNIST和CelebA三个数据集上的大量实验表明，该框架对于灰度图像和RGB图像都有效且具有泛化能力，并在几乎所有指标上取得了最先进的结果。", "conclusion": "所提出的差分隐私生成框架通过EFSGD、重建损失和噪声注入机制，有效解决了隐私和效用之间的权衡问题，能够在保证差分隐私的前提下生成高质量且实用的合成图像，并在多个基准数据集上达到了最先进的性能。"}}
{"id": "2601.14788", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14788", "abs": "https://arxiv.org/abs/2601.14788", "authors": ["Yifei Liu", "Changxing Ding", "Ling Guo", "Huaiguang Jiang", "Qiong Cao"], "title": "Reconstruction-Anchored Diffusion Model for Text-to-Motion Generation", "comment": null, "summary": "Diffusion models have seen widespread adoption for text-driven human motion generation and related tasks due to their impressive generative capabilities and flexibility. However, current motion diffusion models face two major limitations: a representational gap caused by pre-trained text encoders that lack motion-specific information, and error propagation during the iterative denoising process. This paper introduces Reconstruction-Anchored Diffusion Model (RAM) to address these challenges. First, RAM leverages a motion latent space as intermediate supervision for text-to-motion generation. To this end, RAM co-trains a motion reconstruction branch with two key objective functions: self-regularization to enhance the discrimination of the motion space and motion-centric latent alignment to enable accurate mapping from text to the motion latent space. Second, we propose Reconstructive Error Guidance (REG), a testing-stage guidance mechanism that exploits the diffusion model's inherent self-correction ability to mitigate error propagation. At each denoising step, REG uses the motion reconstruction branch to reconstruct the previous estimate, reproducing the prior error patterns. By amplifying the residual between the current prediction and the reconstructed estimate, REG highlights the improvements in the current prediction. Extensive experiments demonstrate that RAM achieves significant improvements and state-of-the-art performance. Our code will be released.", "AI": {"tldr": "本文提出了一种名为RAM（Reconstruction-Anchored Diffusion Model）的新型文本驱动人体运动生成模型，通过引入运动潜空间中间监督和重建误差引导机制，解决了现有模型中预训练文本编码器缺乏运动信息以及去噪过程中误差传播的问题，实验证明了其优越性。", "motivation": "现有文本到运动生成模型存在两个主要问题：1. 预训练的文本编码器缺乏运动相关信息，导致表示差距；2. 迭代去噪过程易产生误差累积。作者旨在解决这些限制，提升运动生成质量。", "method": "RAM模型包含两部分：1. 引入运动潜空间作为中间监督，通过联合训练一个运动重建分支实现。该分支包含自正则化（增强运动空间辨别力）和以运动为中心的潜在对齐（实现文本到运动潜空间的精确映射）两个关键目标函数。2. 提出重建误差引导（REG）的测试阶段引导机制，利用运动重建分支在每次去噪迭代中重建上一时刻的估计，并通过放大当前预测与重建估计之间的残差来突出当前预测的改进，从而缓解误差传播。", "result": "RAM在文本驱动的人体运动生成任务上取得了显著的性能提升，达到了最先进水平。", "conclusion": "RAM通过引入运动潜空间中间监督和重建误差引导机制，有效解决了现有运动扩散模型在表示差距和误差传播方面的挑战，能够生成更高质量的人体运动。"}}
{"id": "2601.15165", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.15165", "abs": "https://arxiv.org/abs/2601.15165", "authors": ["Zanlin Ni", "Shenzhi Wang", "Yang Yue", "Tianyu Yu", "Weilin Zhao", "Yeguo Hua", "Tianyi Chen", "Jun Song", "Cheng Yu", "Bo Zheng", "Gao Huang"], "title": "The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models", "comment": "Code and pre-trained models: https://github.com/LeapLabTHU/JustGRPO", "summary": "Diffusion Large Language Models (dLLMs) break the rigid left-to-right constraint of traditional LLMs, enabling token generation in arbitrary orders. Intuitively, this flexibility implies a solution space that strictly supersets the fixed autoregressive trajectory, theoretically unlocking superior reasoning potential for general tasks like mathematics and coding. Consequently, numerous works have leveraged reinforcement learning (RL) to elicit the reasoning capability of dLLMs. In this paper, we reveal a counter-intuitive reality: arbitrary order generation, in its current form, narrows rather than expands the reasoning boundary of dLLMs. We find that dLLMs tend to exploit this order flexibility to bypass high-uncertainty tokens that are crucial for exploration, leading to a premature collapse of the solution space. This observation challenges the premise of existing RL approaches for dLLMs, where considerable complexities, such as handling combinatorial trajectories and intractable likelihoods, are often devoted to preserving this flexibility. We demonstrate that effective reasoning is better elicited by intentionally forgoing arbitrary order and applying standard Group Relative Policy Optimization (GRPO) instead. Our approach, JustGRPO, is minimalist yet surprisingly effective (e.g., 89.1% accuracy on GSM8K) while fully retaining the parallel decoding ability of dLLMs. Project page: https://nzl-thu.github.io/the-flexibility-trap", "AI": {"tldr": "该论文指出，扩散大型语言模型（dLLM）的任意顺序生成能力，实际上限制了其推理能力，因为模型倾向于规避高不确定性但关键的 token。作者提出了一种名为 JustGRPO 的简化方法，通过放弃任意顺序生成并采用标准的 GRPO，有效提升了 dLLM 的推理能力，并在 GSM8K 数据集上取得了显著成果。", "motivation": "现有研究利用强化学习（RL）来提升 dLLM 的推理能力，普遍认为任意顺序生成能扩展模型的能力。然而，作者观察到 dLLM 在实践中似乎未能充分利用这一优势，反而出现了推理能力受限的现象，这促使作者深入研究任意顺序生成对 dLLM 推理能力的影响。", "method": "作者通过实证研究发现 dLLM 倾向于规避高不确定性 token，导致解空间过早坍塌。基于此，作者提出 JustGRPO 方法，该方法故意放弃 dLLM 的任意顺序生成能力，而是采用标准的 Group Relative Policy Optimization (GRPO) 算法来优化模型。这种方法保留了 dLLM 的并行解码能力，但限制了 token 的生成顺序。", "result": "JustGRPO 方法在数学推理任务（如 GSM8K）上取得了显著的性能提升，准确率达到 89.1%。与现有复杂的方法相比，JustGRPO 的方法更简洁，但效果更佳，证明了限制生成顺序可以更好地激发 dLLM 的推理潜力。", "conclusion": "dLLM 的任意顺序生成能力在当前形式下，反而限制了其推理边界。通过放弃任意顺序生成并采用标准的 GRPO 算法（JustGRPO），可以更有效地激发 dLLM 的推理能力，同时保留其并行解码的优势。"}}
{"id": "2601.15123", "categories": ["cs.CV", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.15123", "abs": "https://arxiv.org/abs/2601.15123", "authors": ["Andrey Moskalenko", "Danil Kuznetsov", "Irina Dudko", "Anastasiia Iasakova", "Nikita Boldyrev", "Denis Shepelev", "Andrei Spiridonov", "Andrey Kuznetsov", "Vlad Shakhuro"], "title": "BREPS: Bounding-Box Robustness Evaluation of Promptable Segmentation", "comment": "Accepted by AAAI2026", "summary": "Promptable segmentation models such as SAM have established a powerful paradigm, enabling strong generalization to unseen objects and domains with minimal user input, including points, bounding boxes, and text prompts. Among these, bounding boxes stand out as particularly effective, often outperforming points while significantly reducing annotation costs. However, current training and evaluation protocols typically rely on synthetic prompts generated through simple heuristics, offering limited insight into real-world robustness. In this paper, we investigate the robustness of promptable segmentation models to natural variations in bounding box prompts. First, we conduct a controlled user study and collect thousands of real bounding box annotations. Our analysis reveals substantial variability in segmentation quality across users for the same model and instance, indicating that SAM-like models are highly sensitive to natural prompt noise. Then, since exhaustive testing of all possible user inputs is computationally prohibitive, we reformulate robustness evaluation as a white-box optimization problem over the bounding box prompt space. We introduce BREPS, a method for generating adversarial bounding boxes that minimize or maximize segmentation error while adhering to naturalness constraints. Finally, we benchmark state-of-the-art models across 10 datasets, spanning everyday scenes to medical imaging. Code - https://github.com/emb-ai/BREPS.", "AI": {"tldr": "本研究调查了可提示分割模型（如 SAM）对边界框提示的自然变化的鲁棒性，发现它们对提示噪声敏感。研究引入了一种名为 BREPS 的方法，通过生成符合自然约束的对抗性边界框来评估和优化模型的鲁棒性。", "motivation": "现有的可提示分割模型训练和评估协议依赖于合成的、简化的提示，这未能充分反映模型在真实世界场景中的鲁棒性。研究者希望了解模型对自然边界框提示变化的敏感程度。", "method": "1. 进行用户研究，收集真实的边界框标注，分析分割质量的变异性。 2. 将鲁棒性评估重塑为关于边界框提示空间的白盒优化问题。 3. 提出 BREPS 方法，生成最小化或最大化分割误差的对抗性边界框，同时满足自然性约束。", "result": "用户研究表明，SAM 类模型对自然提示噪声非常敏感，不同用户对同一实例的标注会导致分割质量存在显著差异。BREPS 方法能够在 10 个不同领域的数据集上有效评估和发现模型的鲁棒性弱点。", "conclusion": "可提示分割模型对边界框提示的自然变化敏感，需要更真实的评估方法来衡量其在实际应用中的鲁棒性。BREPS 提供了一种有效的方法来生成对抗性边界框，以评估和改进模型在面对真实世界提示噪声时的性能。"}}
{"id": "2601.15182", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.15182", "abs": "https://arxiv.org/abs/2601.15182", "authors": ["Naghmeh Farzi", "Laura Dietz", "Dave D. Lewis"], "title": "Supporting Humans in Evaluating AI Summaries of Legal Depositions", "comment": "To appear in 2026 ACM SIGIR Conference on Human Information Interaction and Retrieval (CHIIR '26), March 22-26, 2026, Seattle, WA, USA. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/3786304.3787923", "summary": "While large language models (LLMs) are increasingly used to summarize long documents, this trend poses significant challenges in the legal domain, where the factual accuracy of deposition summaries is crucial. Nugget-based methods have been shown to be extremely helpful for the automated evaluation of summarization approaches. In this work, we translate these methods to the user side and explore how nuggets could directly assist end users. Although prior systems have demonstrated the promise of nugget-based evaluation, its potential to support end users remains underexplored. Focusing on the legal domain, we present a prototype that leverages a factual nugget-based approach to support legal professionals in two concrete scenarios: (1) determining which of two summaries is better, and (2) manually improving an automatically generated summary.", "AI": {"tldr": "本研究将基于事实摘要的方法应用于法律领域，开发了一个原型系统，旨在帮助法律专业人士评估和改进自动生成的案情摘要。", "motivation": "在法律领域，案情摘要的准确性至关重要，但现有的大型语言模型在生成摘要时面临准确性挑战。基于摘要“要点”（nuggets）的评估方法虽然在评估摘要生成模型方面有效，但其在直接辅助最终用户方面的潜力尚未被充分探索。", "method": "研究提出了一个基于事实“要点”（nuggets）的原型系统，将其从评估者端转移到用户端，以直接支持法律专业人士。该系统在两个具体场景下进行测试：1. 帮助用户判断两个摘要的优劣；2. 协助用户手动改进自动生成的摘要。", "result": "研究展示了一个原型系统，该系统利用基于事实“要点”的方法，可以在两个实际场景中为法律专业人士提供支持，具体结果（如用户满意度、效率提升等）未在摘要中详述，但表明了该方法的有效性。", "conclusion": "基于事实“要点”的方法具有潜力，可以作为一种直接工具来支持法律专业人士在评估和改进案情摘要方面的需求，尤其是在法律领域这种对事实准确性要求极高的场景下。"}}
{"id": "2601.14797", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14797", "abs": "https://arxiv.org/abs/2601.14797", "authors": ["Qingling Shu", "Sibao Chen", "Wei Lu", "Zhihui You", "Chengzhuang Liu"], "title": "UniRoute: Unified Routing Mixture-of-Experts for Modality-Adaptive Remote Sensing Change Detection", "comment": null, "summary": "Current remote sensing change detection (CD) methods mainly rely on specialized models, which limits the scalability toward modality-adaptive Earth observation. For homogeneous CD, precise boundary delineation relies on fine-grained spatial cues and local pixel interactions, whereas heterogeneous CD instead requires broader contextual information to suppress speckle noise and geometric distortions. Moreover, difference operator (e.g., subtraction) works well for aligned homogeneous images but introduces artifacts in cross-modal or geometrically misaligned scenarios. Across different modality settings, specialized models based on static backbones or fixed difference operations often prove insufficient. To address this challenge, we propose UniRoute, a unified framework for modality-adaptive learning by reformulating feature extraction and fusion as conditional routing problems. We introduce an Adaptive Receptive Field Routing MoE (AR2-MoE) module to disentangle local spatial details from global semantic context, and a Modality-Aware Difference Routing MoE (MDR-MoE) module to adaptively select the most suitable fusion primitive at each pixel. In addition, we propose a Consistency-Aware Self-Distillation (CASD) strategy that stabilizes unified training under data-scarce heterogeneous settings by enforcing multi-level consistency. Extensive experiments on five public datasets demonstrate that UniRoute achieves strong overall performance, with a favorable accuracy-efficiency trade-off under a unified deployment setting.", "AI": {"tldr": "提出了一种名为UniRoute的统一框架，通过条件路由重塑特征提取和融合，以实现模态自适应遥感变化检测，并在五个公共数据集上取得了强大的性能。", "motivation": "现有的遥感变化检测方法主要依赖于专门的模型，这限制了其向模态自适应地球观测的可扩展性。同质变化检测需要精细的空间线索和局部像素交互，而异质变化检测需要更广泛的上下文信息来抑制噪声和几何失真。此外，标准的差分算子在跨模态或几何错位的情况下会引入伪影。为了解决这些问题，需要一个能够适应不同模态和场景的统一框架。", "method": "提出UniRoute统一框架，通过条件路由重塑特征提取和融合。引入了自适应感受野路由MoE（AR2-MoE）模块以解耦局部空间细节和全局语义上下文，以及模态感知差分路由MoE（MDR-MoE）模块以自适应地选择每个像素最合适的融合原语。此外，提出了一致性感知自蒸馏（CASD）策略，通过强制多级一致性来稳定异质设置下的统一训练。", "result": "在五个公共数据集上的大量实验表明，UniRoute在统一部署设置下实现了强大的总体性能，并具有有利的准确性-效率权衡。", "conclusion": "UniRoute是一个统一的、模态自适应的遥感变化检测框架，通过创新的路由机制和自蒸馏策略，能够有效地处理同质和异质场景，并在保持模型统一性的同时取得了优异的性能。"}}
{"id": "2601.15172", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.15172", "abs": "https://arxiv.org/abs/2601.15172", "authors": ["Ilia Kuznetsov", "Rohan Nayak", "Alla Rozovskaya", "Iryna Gurevych"], "title": "Is Peer Review Really in Decline? Analyzing Review Quality across Venues and Time", "comment": null, "summary": "Peer review is at the heart of modern science. As submission numbers rise and research communities grow, the decline in review quality is a popular narrative and a common concern. Yet, is it true? Review quality is difficult to measure, and the ongoing evolution of reviewing practices makes it hard to compare reviews across venues and time. To address this, we introduce a new framework for evidence-based comparative study of review quality and apply it to major AI and machine learning conferences: ICLR, NeurIPS and *ACL. We document the diversity of review formats and introduce a new approach to review standardization. We propose a multi-dimensional schema for quantifying review quality as utility to editors and authors, coupled with both LLM-based and lightweight measurements. We study the relationships between measurements of review quality, and its evolution over time. Contradicting the popular narrative, our cross-temporal analysis reveals no consistent decline in median review quality across venues and years. We propose alternative explanations, and outline recommendations to facilitate future empirical studies of review quality.", "AI": {"tldr": "本研究提出了一种基于证据的同行评审质量评估框架，并应用于 ICLR、NeurIPS 和 *ACL 等顶级人工智能和机器学习会议。研究发现，与普遍看法相反，中位数评审质量并未出现持续下降。", "motivation": "同行评审的质量下降是科学界普遍担忧的问题，但缺乏实证证据支持。本研究旨在通过开发一种新的评估框架来实证研究这一现象，并分析评审质量的演变。", "method": "研究者开发了一个新的同行评审质量评估框架，该框架包含一个多维度的量化模式，用于衡量评审对编辑和作者的效用。该模式结合了大型语言模型（LLM）和轻量级测量方法。研究将此框架应用于 ICLR、NeurIPS 和 *ACL 三个会议，并进行了跨时间的分析。", "result": "研究发现，在所分析的会议和年份中，中位数同行评审质量并未出现持续下降的趋势。研究还记录了评审格式的多样性，并提出了一种评审标准化方法。", "conclusion": "本研究的结果与同行评审质量下降的普遍叙事相矛盾，表明这一担忧可能并非普遍事实。研究者提出了其他可能的解释，并为未来同行评审质量的实证研究提供了建议。"}}
{"id": "2601.15236", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.15236", "abs": "https://arxiv.org/abs/2601.15236", "authors": ["Anjishnu Mukherjee", "Ziwei Zhu", "Antonios Anastasopoulos"], "title": "Metadata Conditioned Large Language Models for Localization", "comment": "under review", "summary": "Large language models are typically trained by treating text as a single global distribution, often resulting in geographically homogenized behavior. We study metadata conditioning as a lightweight approach for localization, pre-training 31 models (at 0.5B and 1B parameter scales) from scratch on large-scale English news data annotated with verified URLs, country tags, and continent tags, covering 4 continents and 17 countries. Across four controlled experiments, we show that metadata conditioning consistently improves in-region performance without sacrificing cross-region generalization, enables global models to recover localization comparable to region-specific models, and improves learning efficiency. Our ablation studies demonstrate that URL-level metadata alone captures much of the geographic signal, while balanced regional data coverage remains essential, as metadata cannot fully compensate for missing regions. Finally, we introduce a downstream benchmark of 800 localized news MCQs and show that after instruction tuning, metadata conditioned global models achieve accuracy comparable to LLaMA-3.2-1B-Instruct, despite being trained on substantially less data. Together, these results establish metadata conditioning as a practical and compute-efficient approach for localization of language models.", "AI": {"tldr": "通过在训练时引入地理元数据（如URL、国家/大洲标签），可以轻量级地实现大型语言模型的地理本地化，提高区域内性能，且不牺牲跨区域泛化能力，其效果可媲美区域专用模型，并在下游任务中展现出与LLaMA-3.2-1B-Instruct相当的性能。", "motivation": "当前大型语言模型倾向于表现出地理同质化行为，缺乏区域特异性。研究者希望找到一种轻量级的方法来解决这个问题，实现模型的地理本地化。", "method": "研究者从头开始预训练了31个不同参数规模（0.5B和1B）的模型，使用包含国家、大洲和URL等地理元数据标注的英文新闻数据。他们通过四个实验评估了元数据条件下的模型在区域内性能、跨区域泛化能力、与区域专用模型的对比以及学习效率。此外，还进行了消融实验以评估不同元数据的贡献，并引入了一个包含800个本地化新闻选择题的下游基准测试。", "result": "元数据条件训练的模型在区域内性能上持续提升，同时保持了跨区域泛化能力。这些模型能够恢复出媲美区域专用模型的本地化能力，并且学习效率更高。URL级别元数据单独就包含了大部分地理信息。但均衡的区域数据覆盖仍然至关重要，元数据无法弥补缺失区域的数据。在下游任务中，经过指令微调的元数据条件模型在准确性上可与LLaMA-3.2-1B-Instruct媲美，尽管训练数据量远小于后者。", "conclusion": "元数据条件是一种实用且计算效率高的方法，可以实现大型语言模型的地理本地化，提高其区域性能并保持全球适应性。"}}
{"id": "2601.15220", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.15220", "abs": "https://arxiv.org/abs/2601.15220", "authors": ["Anmol Goel", "Cornelius Emde", "Sangdoo Yun", "Seong Joon Oh", "Martin Gubri"], "title": "Privacy Collapse: Benign Fine-Tuning Can Break Contextual Privacy in Language Models", "comment": null, "summary": "We identify a novel phenomenon in language models: benign fine-tuning of frontier models can lead to privacy collapse. We find that diverse, subtle patterns in training data can degrade contextual privacy, including optimisation for helpfulness, exposure to user information, emotional and subjective dialogue, and debugging code printing internal variables, among others. Fine-tuned models lose their ability to reason about contextual privacy norms, share information inappropriately with tools, and violate memory boundaries across contexts. Privacy collapse is a ``silent failure'' because models maintain high performance on standard safety and utility benchmarks whilst exhibiting severe privacy vulnerabilities. Our experiments show evidence of privacy collapse across six models (closed and open weight), five fine-tuning datasets (real-world and controlled data), and two task categories (agentic and memory-based). Our mechanistic analysis reveals that privacy representations are uniquely fragile to fine-tuning, compared to task-relevant features which are preserved. Our results reveal a critical gap in current safety evaluations, in particular for the deployment of specialised agents.", "AI": {"tldr": "本研究发现了语言模型中的一种新现象：良性微调可能导致上下文隐私泄露。即使模型在标准安全和效用基准测试中表现良好，也会出现严重的隐私问题。", "motivation": "研究人员注意到，尽管语言模型在各种任务中表现出色，但在实际应用中可能存在隐私风险。本研究旨在识别并量化这种潜在的隐私泄露现象，特别是在模型经过微调后。", "method": "通过对六个不同模型（闭源和开源）、五种微调数据集（真实世界和控制数据）以及两类任务（代理型和基于记忆型）进行实验，研究人员观察并分析了微调过程中隐私表示的变化。他们还进行了机制分析，以了解隐私信息与任务相关信息在微调过程中的不同脆弱性。", "result": "研究发现，包括追求有用性、用户信息暴露、情感对话、调试代码等多种因素都可能导致上下文隐私泄露。微调后的模型在处理上下文隐私规范时能力下降，可能错误地与工具共享信息，并跨越记忆边界。这种“隐私崩溃”是一种“沉默的失败”，因为模型在标准安全和效用测试中表现正常。", "conclusion": "当前的安全评估方法存在关键漏洞，无法充分检测到微调导致的隐私泄露问题，尤其是在部署专业化代理模型时，这种风险更为显著。需要开发新的评估方法来解决这一问题。"}}
{"id": "2601.15235", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.15235", "abs": "https://arxiv.org/abs/2601.15235", "authors": ["Fabi Nahian Madhurja", "Rusab Sarmun", "Muhammad E. H. Chowdhury", "Adam Mushtak", "Israa Al-Hashimi", "Sohaib Bassam Zoghoul"], "title": "Tracing 3D Anatomy in 2D Strokes: A Multi-Stage Projection Driven Approach to Cervical Spine Fracture Identification", "comment": null, "summary": "Cervical spine fractures are critical medical conditions requiring precise and efficient detection for effective clinical management. This study explores the viability of 2D projection-based vertebra segmentation for vertebra-level fracture detection in 3D CT volumes, presenting an end-to-end pipeline for automated analysis of cervical vertebrae (C1-C7). By approximating a 3D volume through optimized 2D axial, sagittal, and coronal projections, regions of interest are identified using the YOLOv8 model from all views and combined to approximate the 3D cervical spine area, achieving a 3D mIoU of 94.45 percent. This projection-based localization strategy reduces computational complexity compared to traditional 3D segmentation methods while maintaining high performance. It is followed by a DenseNet121-Unet-based multi-label segmentation leveraging variance- and energy-based projections, achieving a Dice score of 87.86 percent. Strategic approximation of 3D vertebral masks from these 2D segmentation masks enables the extraction of individual vertebra volumes. The volumes are analyzed for fractures using an ensemble of 2.5D Spatio-Sequential models incorporating both raw slices and projections per vertebra for complementary evaluation. This ensemble achieves vertebra-level and patient-level F1 scores of 68.15 and 82.26, and ROC-AUC scores of 91.62 and 83.04, respectively. We further validate our approach through an explainability study that provides saliency map visualizations highlighting anatomical regions relevant for diagnosis, and an interobserver variability analysis comparing our model's performance with expert radiologists, demonstrating competitive results.", "AI": {"tldr": "本研究提出了一种基于2D投影的颈椎骨折检测方法，通过YOLOv8进行2D定位，DenseNet121-Unet进行2D分割，再结合2.5D模型进行3D骨折检测，并在可解释性和与专家一致性方面进行了验证。", "motivation": "颈椎骨折是一种危及生命的状况，需要精确高效的检测来指导临床管理。现有3D分割方法计算复杂度高，本研究旨在探索一种计算效率更高的2D投影方法来实现颈椎骨折的自动化分析。", "method": "1. 使用YOLOv8模型从3D CT的轴向、矢状面和冠状面2D投影中进行脊椎区域识别和定位。 2. 利用方差和能量投影，基于DenseNet121-Unet模型进行脊椎的2D多标签分割。 3. 通过2D分割掩模近似3D脊椎掩模，提取个体脊椎体积。 4. 使用结合原始切片和投影的2.5D时序模型进行骨折检测。 5. 进行可解释性分析（显著图）和与放射科专家的一致性分析。", "result": "2D定位的3D mIoU达到94.45%，2D分割的Dice分数达到87.86%。骨折检测方面，椎体级别和患者级别的F1分数分别为68.15%和82.26%，ROC-AUC分别为91.62%和83.04%。解释性分析显示模型关注了相关的解剖区域，并且模型性能与专家具有竞争力。", "conclusion": "基于2D投影的脊椎分割和骨折检测方法在计算效率和性能上取得了良好平衡，能够实现颈椎骨折的自动化分析，并且其结果具有可解释性，与专家诊断结果具有可比性。"}}
{"id": "2601.14799", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14799", "abs": "https://arxiv.org/abs/2601.14799", "authors": ["Qihua Liang", "Liang Chen", "Yaozong Zheng", "Jian Nong", "Zhiyi Mo", "Bineng Zhong"], "title": "UBATrack: Spatio-Temporal State Space Model for General Multi-Modal Tracking", "comment": null, "summary": "Multi-modal object tracking has attracted considerable attention by integrating multiple complementary inputs (e.g., thermal, depth, and event data) to achieve outstanding performance. Although current general-purpose multi-modal trackers primarily unify various modal tracking tasks (i.e., RGB-Thermal infrared, RGB-Depth or RGB-Event tracking) through prompt learning, they still overlook the effective capture of spatio-temporal cues. In this work, we introduce a novel multi-modal tracking framework based on a mamba-style state space model, termed UBATrack. Our UBATrack comprises two simple yet effective modules: a Spatio-temporal Mamba Adapter (STMA) and a Dynamic Multi-modal Feature Mixer. The former leverages Mamba's long-sequence modeling capability to jointly model cross-modal dependencies and spatio-temporal visual cues in an adapter-tuning manner. The latter further enhances multi-modal representation capacity across multiple feature dimensions to improve tracking robustness. In this way, UBATrack eliminates the need for costly full-parameter fine-tuning, thereby improving the training efficiency of multi-modal tracking algorithms. Experiments show that UBATrack outperforms state-of-the-art methods on RGB-T, RGB-D, and RGB-E tracking benchmarks, achieving outstanding results on the LasHeR, RGBT234, RGBT210, DepthTrack, VOT-RGBD22, and VisEvent datasets.", "AI": {"tldr": "提出了一种名为UBATrack的新型多模态目标跟踪框架，该框架基于类Mamba状态空间模型，并包含一个时空Mamba适配器（STMA）和一个动态多模态特征混合器，以有效捕捉时空线索并提升跟踪鲁棒性，同时提高了训练效率。", "motivation": "现有通用多模态跟踪器虽然通过提示学习统一了不同模态的跟踪任务，但未能有效捕捉时空线索。", "method": "提出UBATrack框架，包含两个模块：1) 时空Mamba适配器（STMA），利用Mamba的长序列建模能力，以适配器微调的方式联合建模跨模态依赖和时空视觉线索；2) 动态多模态特征混合器，增强多模态表示能力。框架避免了昂贵的参数全量微调。", "result": "UBATrack在RGB-T、RGB-D和RGB-E跟踪基准上均超越了现有最先进方法，在LasHeR、RGBT234、RGBT210、DepthTrack、VOT-RGBD22和VisEvent数据集上取得了优异的成绩。", "conclusion": "UBATrack是一种高效且性能优越的多模态目标跟踪框架，通过引入基于Mamba的模型和新的模块，有效解决了现有方法的不足，并在多个数据集上验证了其有效性。"}}
{"id": "2601.15247", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.15247", "abs": "https://arxiv.org/abs/2601.15247", "authors": ["Rian Dolphin", "Joe Dursun", "Jarrett Blankenship", "Katie Adams", "Quinton Pike"], "title": "Taxonomy-Aligned Risk Extraction from 10-K Filings with Autonomous Improvement Using LLMs", "comment": "4 figures, 9 pages", "summary": "We present a methodology for extracting structured risk factors from corporate 10-K filings while maintaining adherence to a predefined hierarchical taxonomy. Our three-stage pipeline combines LLM extraction with supporting quotes, embedding-based semantic mapping to taxonomy categories, and LLM-as-a-judge validation that filters spurious assignments. To evaluate our approach, we extract 10,688 risk factors from S&P 500 companies and examine risk profile similarity across industry clusters. Beyond extraction, we introduce autonomous taxonomy maintenance where an AI agent analyzes evaluation feedback to identify problematic categories, diagnose failure patterns, and propose refinements, achieving 104.7% improvement in embedding separation in a case study. External validation confirms the taxonomy captures economically meaningful structure: same-industry companies exhibit 63% higher risk profile similarity than cross-industry pairs (Cohen's d=1.06, AUC 0.82, p<0.001). The methodology generalizes to any domain requiring taxonomy-aligned extraction from unstructured text, with autonomous improvement enabling continuous quality maintenance and enhancement as systems process more documents.", "AI": {"tldr": "该研究提出了一种从公司10-K文件中提取结构化风险因素的方法，该方法结合了大型语言模型（LLM）提取、基于嵌入的语义映射和LLM作为裁判的验证，并引入了自主维护分类法的功能，显著提高了风险因素提取的准确性和分类法的质量。", "motivation": "现有从非结构化文本（如公司财报）中提取结构化信息的方法存在挑战，尤其是在需要与预定义的层级分类法保持一致时。研究旨在开发一种能够准确、自动地从公司10-K文件中提取风险因素并保持分类法一致性的方法，并能持续改进。 ", "method": "研究采用了一个三阶段的流水线：1. 使用LLM提取风险因素并附带支持性引用。2. 利用基于嵌入的语义映射将提取的风险因素匹配到预定义的分类法类别。3. 使用LLM作为裁判来验证分配，过滤掉不正确的分配。此外，研究还引入了一个AI代理，通过分析评估反馈来自主维护分类法，识别问题类别并提出改进建议。", "result": "研究从S&P 500公司的10-K文件中成功提取了10,688个风险因素。在自主维护分类法的案例研究中，嵌入分离度提高了104.7%。外部验证表明，同行业公司的风险特征相似性比跨行业公司高63%（Cohen's d=1.06, AUC 0.82, p<0.001），证明了分类法捕捉了经济上有意义的结构。", "conclusion": "该研究提出的方法能够有效地从公司10-K文件中提取结构化风险因素，并与预定义的分类法保持一致。其引入的自主分类法维护机制能够持续提升系统的质量。该方法具有广泛的通用性，适用于任何需要进行分类法对齐的文本提取任务，并能随着处理文档的增加而不断改进。"}}
{"id": "2601.14804", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14804", "abs": "https://arxiv.org/abs/2601.14804", "authors": ["Tobias Weißberg", "Weikang Wang", "Paul Roetzer", "Nafie El Amrani", "Florian Bernard"], "title": "Symmetry Informative and Agnostic Feature Disentanglement for 3D Shapes", "comment": "Accepted at 3DV 2026", "summary": "Shape descriptors, i.e., per-vertex features of 3D meshes or point clouds, are fundamental to shape analysis. Historically, various handcrafted geometry-aware descriptors and feature refinement techniques have been proposed. Recently, several studies have initiated a new research direction by leveraging features from image foundation models to create semantics-aware descriptors, demonstrating advantages across tasks like shape matching, editing, and segmentation. Symmetry, another key concept in shape analysis, has also attracted increasing attention. Consequently, constructing symmetry-aware shape descriptors is a natural progression. Although the recent method $χ$ (Wang et al., 2025) successfully extracted symmetry-informative features from semantic-aware descriptors, its features are only one-dimensional, neglecting other valuable semantic information. Furthermore, the extracted symmetry-informative feature is usually noisy and yields small misclassified patches. To address these gaps, we propose a feature disentanglement approach which is simultaneously symmetry informative and symmetry agnostic. Further, we propose a feature refinement technique to improve the robustness of predicted symmetry informative features. Extensive experiments, including intrinsic symmetry detection, left/right classification, and shape matching, demonstrate the effectiveness of our proposed framework compared to various state-of-the-art methods, both qualitatively and quantitatively.", "AI": {"tldr": "本文提出了一种新的形状描述符生成方法，该方法能够同时提取对称性和非对称性信息，并对对称性信息进行精炼，以提高鲁棒性。实验证明该方法在多种形状分析任务上优于现有方法。", "motivation": "现有的语义感知形状描述符在提取对称性信息时存在不足（如仅提取一维特征、易受噪声影响导致误分类），而对称性是形状分析中的一个重要概念，因此需要一种能够同时捕捉对称性和非对称性信息且鲁棒性更强的形状描述符。  ", "method": "提出了一种特征解耦方法，同时提取对称性信息和非对称性信息。在此基础上，提出了一种特征精炼技术来提高预测的对称性信息特征的鲁棒性。  ", "result": "与多种最先进的方法相比，所提出的框架在内在对称性检测、左右分类和形状匹配等方面的实验表现出有效性，无论是在定性还是定量上。  ", "conclusion": "所提出的框架能够生成同时具备对称性和非对称性信息且鲁棒性更强的形状描述符，并在多项形状分析任务中取得了优于现有方法的性能。  "}}
{"id": "2601.14802", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14802", "abs": "https://arxiv.org/abs/2601.14802", "authors": ["Donnate Hooft", "Stefan M. Fischer", "Cosmin Bercea", "Jan C. Peeken", "Julia A. Schnabel"], "title": "LocBAM: Advancing 3D Patch-Based Image Segmentation by Integrating Location Contex", "comment": "Accepted at ISBI 2026", "summary": "Patch-based methods are widely used in 3D medical image segmentation to address memory constraints in processing high-resolution volumetric data. However, these approaches often neglect the patch's location within the global volume, which can limit segmentation performance when anatomical context is important. In this paper, we investigate the role of location context in patch-based 3D segmentation and propose a novel attention mechanism, LocBAM, that explicitly processes spatial information. Experiments on BTCV, AMOS22, and KiTS23 demonstrate that incorporating location context stabilizes training and improves segmentation performance, particularly under low patch-to-volume coverage where global context is missing. Furthermore, LocBAM consistently outperforms classical coordinate encoding via CoordConv. Code is publicly available at https://github.com/compai-lab/2026-ISBI-hooft", "AI": {"tldr": "提出了一种名为LocBAM的新型注意力机制，用于解决基于块的3D医学图像分割中忽略全局位置信息的问题，并在多个数据集上验证了其稳定训练和提升分割性能的效果。", "motivation": "基于块的3D医学图像分割方法在处理高分辨率体积数据时面临内存限制，但现有方法常忽略块在全局体积中的位置信息，而这在解剖结构背景重要的场景下会限制分割性能。", "method": "提出了一种名为LocBAM的新型注意力机制，用于显式处理空间位置信息，并将其整合到基于块的3D分割方法中。在BTCV、AMOS22和KiTS23数据集上进行了实验。", "result": "实验表明，引入位置信息可以稳定训练并提高分割性能，尤其是在块与体积覆盖率低的情况下。LocBAM的性能优于经典的CoordConv方法。", "conclusion": "位置上下文在基于块的3D医学图像分割中至关重要，LocBAM注意力机制能够有效利用位置信息，提高分割性能并稳定训练过程。"}}
{"id": "2601.15251", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.15251", "abs": "https://arxiv.org/abs/2601.15251", "authors": ["Varshini Reddy", "Craig W. Schmidt", "Seth Ebner", "Adam Wiemerslage", "Yuval Pinter", "Chris Tanner"], "title": "The Effect of Scripts and Formats on LLM Numeracy", "comment": null, "summary": "Large language models (LLMs) have achieved impressive proficiency in basic arithmetic, rivaling human-level performance on standard numerical tasks. However, little attention has been given to how these models perform when numerical expressions deviate from the prevailing conventions present in their training corpora. In this work, we investigate numerical reasoning across a wide range of numeral scripts and formats. We show that LLM accuracy drops substantially when numerical inputs are rendered in underrepresented scripts or formats, despite the underlying mathematical reasoning being identical. We further demonstrate that targeted prompting strategies, such as few-shot prompting and explicit numeral mapping, can greatly narrow this gap. Our findings highlight an overlooked challenge in multilingual numerical reasoning and provide actionable insights for working with LLMs to reliably interpret, manipulate, and generate numbers across diverse numeral scripts and formatting styles.", "AI": {"tldr": "大型语言模型（LLMs）在处理非标准数字格式和少见数字脚本时准确性会显著下降，但可以通过特定提示策略（如少样本提示和显式数字映射）来改善。", "motivation": "现有研究主要关注LLMs在标准数字任务上的表现，而忽视了它们在处理训练语料中不常见的数字脚本和格式时的能力。研究旨在探索LLMs在这些非常规情况下的数值推理能力。", "method": "作者通过在多种数字脚本和格式下测试LLMs的数值推理能力来研究这一问题。他们还尝试使用少样本提示和显式数字映射等定向提示策略来缩小因格式差异造成的性能差距。", "result": "研究发现，当数字输入使用训练语料中代表性不足的脚本或格式时，LLMs的准确性会大幅下降，即使数学推理本身是相同的。定向提示策略能够显著改善LLMs在这些情况下的表现。", "conclusion": "LLMs在处理非标准数字格式和少见数字脚本时面临严峻挑战，这阻碍了其在多语言数值推理方面的可靠性。通过使用诸如少样本提示和显式数字映射等提示策略，可以有效缓解这一问题，为LLMs在处理多样化数字表达方面提供了可行见解。"}}
{"id": "2601.14791", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.14791", "abs": "https://arxiv.org/abs/2601.14791", "authors": ["Ziyao Ling", "Silvia Mirri", "Paola Salomoni", "Giovanni Delnevo"], "title": "Synthetic Data Augmentation for Multi-Task Chinese Porcelain Classification: A Stable Diffusion Approach", "comment": null, "summary": "The scarcity of training data presents a fundamental challenge in applying deep learning to archaeological artifact classification, particularly for the rare types of Chinese porcelain. This study investigates whether synthetic images generated through Stable Diffusion with Low-Rank Adaptation (LoRA) can effectively augment limited real datasets for multi-task CNN-based porcelain classification. Using MobileNetV3 with transfer learning, we conducted controlled experiments comparing models trained on pure real data against those trained on mixed real-synthetic datasets (95:5 and 90:10 ratios) across four classification tasks: dynasty, glaze, kiln and type identification. Results demonstrate task-specific benefits: type classification showed the most substantial improvement (5.5\\% F1-macro increase with 90:10 ratio), while dynasty and kiln tasks exhibited modest gains (3-4\\%), suggesting that synthetic augmentation effectiveness depends on the alignment between generated features and task-relevant visual signatures. Our work contributes practical guidelines for deploying generative AI in archaeological research, demonstrating both the potential and limitations of synthetic data when archaeological authenticity must be balanced with data diversity.", "AI": {"tldr": "本研究探索使用Stable Diffusion和LoRA生成的合成图像来扩充稀缺的真实考古瓷器数据集，以提高基于多任务CNN的瓷器分类性能。结果表明，合成数据增强对不同分类任务（朝代、釉料、窑口、类型）的效果不同，其中类型识别的提升最显著。", "motivation": "深度学习在考古文物分类（特别是稀有类型的中国瓷器）面临训练数据稀缺的挑战。", "method": "使用Stable Diffusion和LoRA生成合成瓷器图像，并将其与真实数据混合（95:5和90:10的比例），然后使用MobileNetV3和迁移学习进行多任务CNN分类实验，比较纯真实数据和混合数据的模型性能。分类任务包括朝代、釉料、窑口和类型识别。", "result": "合成数据增强对不同任务有不同的效果。类型识别的任务在90:10的混合数据比例下，F1-macro分数增加了5.5%，提升最为显著。朝代和窑口识别任务也显示出3-4%的适度提升。这表明合成数据增强的有效性取决于生成特征与任务相关视觉特征的匹配程度。", "conclusion": "研究证明了使用Stable Diffusion和LoRA生成的合成图像可以有效扩充有限的真实考古数据集，从而提高瓷器分类的性能，但其效果因任务而异。本研究为在考古研究中部署生成式AI提供了实用的指导，并指出了在平衡考古真实性和数据多样性时，合成数据潜力和局限性。"}}
{"id": "2601.15277", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.15277", "abs": "https://arxiv.org/abs/2601.15277", "authors": ["Sahar Tahmasebi", "Eric Müller-Budack", "Ralph Ewerth"], "title": "Robust Fake News Detection using Large Language Models under Adversarial Sentiment Attacks", "comment": null, "summary": "Misinformation and fake news have become a pressing societal challenge, driving the need for reliable automated detection methods. Prior research has highlighted sentiment as an important signal in fake news detection, either by analyzing which sentiments are associated with fake news or by using sentiment and emotion features for classification. However, this poses a vulnerability since adversaries can manipulate sentiment to evade detectors especially with the advent of large language models (LLMs). A few studies have explored adversarial samples generated by LLMs, but they mainly focus on stylistic features such as writing style of news publishers. Thus, the crucial vulnerability of sentiment manipulation remains largely unexplored. In this paper, we investigate the robustness of state-of-the-art fake news detectors under sentiment manipulation. We introduce AdSent, a sentiment-robust detection framework designed to ensure consistent veracity predictions across both original and sentiment-altered news articles. Specifically, we (1) propose controlled sentiment-based adversarial attacks using LLMs, (2) analyze the impact of sentiment shifts on detection performance. We show that changing the sentiment heavily impacts the performance of fake news detection models, indicating biases towards neutral articles being real, while non-neutral articles are often classified as fake content. (3) We introduce a novel sentiment-agnostic training strategy that enhances robustness against such perturbations. Extensive experiments on three benchmark datasets demonstrate that AdSent significantly outperforms competitive baselines in both accuracy and robustness, while also generalizing effectively to unseen datasets and adversarial scenarios.", "AI": {"tldr": "本研究探讨了利用大型语言模型（LLM）操纵新闻情感以规避虚假新闻检测器的方法，并提出了一个名为AdSent的鲁棒性检测框架，通过情感无关的训练策略来提高检测器的准确性和鲁棒性。", "motivation": "现有的虚假新闻检测方法容易受到攻击者利用大型语言模型操纵情感的攻击，从而规避检测。尽管已有研究关注风格操纵，但情感操纵这一关键漏洞仍未得到充分探索。", "method": "研究者提出了两种方法：1. 使用LLM进行受控的情感对抗性攻击，以生成情感改变的新闻文章。2. 引入AdSent框架，采用一种新颖的情感无关训练策略，以提高检测器在面对情感变化的新闻时预测的稳定性。", "result": "研究发现，改变新闻的情感会显著影响现有虚假新闻检测模型的性能，模型倾向于将中性文章判断为真实，而非中性文章判断为虚假。AdSent框架在三个基准数据集上的实验结果显示，其在准确性和鲁棒性方面均显著优于现有方法，并且能够有效泛化到未见过的数据集和对抗场景。", "conclusion": "情感操纵是虚假新闻检测模型的一个重要漏洞。AdSent框架及其情感无关的训练策略能够有效提高虚假新闻检测器的鲁棒性，使其在面对情感操纵时仍能保持较高的准确性。"}}
{"id": "2601.14821", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14821", "abs": "https://arxiv.org/abs/2601.14821", "authors": ["Bert Ramlot", "Martijn Courteaux", "Peter Lambert", "Glenn Van Wallendael"], "title": "POTR: Post-Training 3DGS Compression", "comment": "15 pages, 12 figures. Submitted to IEEE TCSVT, under review", "summary": "3D Gaussian Splatting (3DGS) has recently emerged as a promising contender to Neural Radiance Fields (NeRF) in 3D scene reconstruction and real-time novel view synthesis. 3DGS outperforms NeRF in training and inference speed but has substantially higher storage requirements. To remedy this downside, we propose POTR, a post-training 3DGS codec built on two novel techniques. First, POTR introduces a novel pruning approach that uses a modified 3DGS rasterizer to efficiently calculate every splat's individual removal effect simultaneously. This technique results in 2-4x fewer splats than other post-training pruning techniques and as a result also significantly accelerates inference with experiments demonstrating 1.5-2x faster inference than other compressed models. Second, we propose a novel method to recompute lighting coefficients, significantly reducing their entropy without using any form of training. Our fast and highly parallel approach especially increases AC lighting coefficient sparsity, with experiments demonstrating increases from 70% to 97%, with minimal loss in quality. Finally, we extend POTR with a simple fine-tuning scheme to further enhance pruning, inference, and rate-distortion performance. Experiments demonstrate that POTR, even without fine-tuning, consistently outperforms all other post-training compression techniques in both rate-distortion performance and inference speed.", "AI": {"tldr": "POTR 是一种用于 3D 高斯溅射（3DGS）的训练后压缩方法，通过新颖的剪枝和光照系数重计算技术，显著减少了模型大小和提高了推理速度，同时保持了高质量的渲染效果。", "motivation": "3DGS 在 3D 场景重建和新视角合成方面表现出色，但其存储需求较高，这限制了其应用。该研究旨在解决 3DGS 的高存储问题，提高其效率。", "method": "POTR 采用两种新颖技术：1. 改进的 3DGS 光栅化器用于高效计算每个高斯溅射的移除效果，从而实现更彻底的剪枝。2. 一种无需训练即可重新计算光照系数的方法，以降低其熵并增加稀疏性。此外，还引入了一个简单的微调方案以进一步优化性能。", "result": "POTR 剪枝后的高斯溅射数量比其他方法少 2-4 倍，推理速度比其他压缩模型快 1.5-2 倍。光照系数的稀疏性从 70% 提高到 97%，且视觉质量损失很小。即使未经微调，POTR 在速率-失真性能和推理速度方面也优于所有其他训练后压缩技术。", "conclusion": "POTR 是一种有效的 3DGS 训练后压缩方法，通过创新的剪枝和光照系数处理技术，在减小模型尺寸和提高推理速度方面取得了显著成效，同时保持了高质量的渲染效果，克服了 3DGS 的存储劣势。"}}
{"id": "2601.14950", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14950", "abs": "https://arxiv.org/abs/2601.14950", "authors": ["Yufei Song", "Ziqi Zhou", "Menghao Deng", "Yifan Hu", "Shengshan Hu", "Minghui Li", "Leo Yu Zhang"], "title": "Erosion Attack for Adversarial Training to Enhance Semantic Segmentation Robustness", "comment": "Accepted by ICASSP 2026", "summary": "Existing segmentation models exhibit significant vulnerability to adversarial attacks.To improve robustness, adversarial training incorporates adversarial examples into model training. However, existing attack methods consider only global semantic information and ignore contextual semantic relationships within the samples, limiting the effectiveness of adversarial training. To address this issue, we propose EroSeg-AT, a vulnerability-aware adversarial training framework that leverages EroSeg to generate adversarial examples. EroSeg first selects sensitive pixels based on pixel-level confidence and then progressively propagates perturbations to higher-confidence pixels, effectively disrupting the semantic consistency of the samples. Experimental results show that, compared to existing methods, our approach significantly improves attack effectiveness and enhances model robustness under adversarial training.", "AI": {"tldr": "提出了一种名为EroSeg-AT的对抗性训练框架，通过一种新的EroSeg方法生成更有效的对抗样本，从而提高图像分割模型的鲁棒性。", "motivation": "现有的分割模型对对抗性攻击非常脆弱，而现有的对抗攻击方法忽略了样本内的上下文语义关系，限制了对抗训练的有效性。", "method": "提出EroSeg-AT框架，使用EroSeg方法生成对抗样本。EroSeg首先选择像素级置信度低的敏感像素，然后将扰动逐步传播到置信度更高的像素，从而破坏样本的语义一致性。", "result": "实验证明，与现有方法相比，EroSeg-AT能够显著提高攻击效果，并在对抗训练中增强模型的鲁棒性。", "conclusion": "EroSeg-AT框架通过考虑像素间的上下文语义关系来生成更有效的对抗样本，从而显著提升了图像分割模型的鲁棒性。"}}
{"id": "2601.14841", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.14841", "abs": "https://arxiv.org/abs/2601.14841", "authors": ["Sidi Mohamed Sid El Moctar", "Achraf Ait Laydi", "Yousef El Mourabit", "Hélène Bouvrais"], "title": "MTFlow: Time-Conditioned Flow Matching for Microtubule Segmentation in Noisy Microscopy Images", "comment": "Accepted for presentation at ISBI 2026", "summary": "Microtubules are cytoskeletal filaments that play essential roles in many cellular processes and are key therapeutic targets in several diseases. Accurate segmentation of microtubule networks is critical for studying their organization and dynamics but remains challenging due to filament curvature, dense crossings, and image noise. We present MTFlow, a novel time-conditioned flow-matching model for microtubule segmentation. Unlike conventional U-Net variants that predict masks in a single pass, MTFlow learns vector fields that iteratively transport noisy masks toward the ground truth, enabling interpretable, trajectory-based refinement. Our architecture combines a U-Net backbone with temporal embeddings, allowing the model to capture the dynamics of uncertainty resolution along filament boundaries. We trained and evaluated MTFlow on synthetic and real microtubule datasets and assessed its generalization capability on public biomedical datasets of curvilinear structures such as retinal blood vessels and nerves. MTFlow achieves competitive segmentation accuracy comparable to state-of-the-art models, offering a powerful and time-efficient tool for filamentous structure analysis with more precise annotations than manual or semi-automatic approaches.", "AI": {"tldr": "提出了一种名为MTFlow的新型时间条件流匹配模型，用于分割微管网络，通过迭代地优化掩码来提高精度，并且在合成和真实数据上表现出与现有最先进模型相当的分割精度。", "motivation": "准确分割微管网络对于研究其组织和动力学至关重要，但由于丝状物的曲率、密集交叉和图像噪声而具有挑战性。", "method": "开发了一种名为MTFlow的新型时间条件流匹配模型。该模型结合了U-Net骨干和时间嵌入，学习矢量场以迭代地将噪声掩码向真实值传输，从而实现可解释的、基于轨迹的细化。", "result": "MTFlow在合成和真实微管数据集上进行了训练和评估，并在视网膜血管和神经等公共生物医学数据集上评估了其泛化能力。MTFlow实现了具有竞争力的分割精度，与现有最先进的模型相当。", "conclusion": "MTFlow为丝状结构分析提供了一个强大且高效的工具，能够比手动或半自动方法提供更精确的注释，并能捕捉不确定性沿丝状物边界消化的动力学。"}}
{"id": "2601.14959", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14959", "abs": "https://arxiv.org/abs/2601.14959", "authors": ["Xinyu Peng", "Han Li", "Yuyang Huang", "Ziyang Zheng", "Yaoming Wang", "Xin Chen", "Wenrui Dai", "Chenglin Li", "Junni Zou", "Hongkai Xiong"], "title": "Towards Holistic Modeling for Video Frame Interpolation with Auto-regressive Diffusion Transformers", "comment": null, "summary": "Existing video frame interpolation (VFI) methods often adopt a frame-centric approach, processing videos as independent short segments (e.g., triplets), which leads to temporal inconsistencies and motion artifacts. To overcome this, we propose a holistic, video-centric paradigm named \\textbf{L}ocal \\textbf{D}iffusion \\textbf{F}orcing for \\textbf{V}ideo \\textbf{F}rame \\textbf{I}nterpolation (LDF-VFI). Our framework is built upon an auto-regressive diffusion transformer that models the entire video sequence to ensure long-range temporal coherence. To mitigate error accumulation inherent in auto-regressive generation, we introduce a novel skip-concatenate sampling strategy that effectively maintains temporal stability. Furthermore, LDF-VFI incorporates sparse, local attention and tiled VAE encoding, a combination that not only enables efficient processing of long sequences but also allows generalization to arbitrary spatial resolutions (e.g., 4K) at inference without retraining. An enhanced conditional VAE decoder, which leverages multi-scale features from the input video, further improves reconstruction fidelity. Empirically, LDF-VFI achieves state-of-the-art performance on challenging long-sequence benchmarks, demonstrating superior per-frame quality and temporal consistency, especially in scenes with large motion. The source code is available at https://github.com/xypeng9903/LDF-VFI.", "AI": {"tldr": "提出一种名为 LDF-VFI 的视频插帧新框架，采用基于自回归扩散 Transformer 的视频中心化方法，通过跳跃连接采样策略、稀疏局部注意力、分块 VAE 编码以及多尺度特征增强的 VAE 解码器，实现了长序列视频的高效处理和任意分辨率（如 4K）的插值，并在长序列插值任务中取得了 SOTA 性能。", "motivation": "现有视频帧插值（VFI）方法多采用帧中心化处理，将视频视为独立短片段，导致时间不一致和运动伪影。研究动机是克服这些局限，实现更长的时间连贯性和更高的插值质量。", "method": "提出 LDF-VFI 框架，核心是一个自回归扩散 Transformer，能够建模整个视频序列。关键技术包括：1. 跳跃连接采样策略（skip-concatenate sampling）以缓解误差累积。2. 稀疏局部注意力和分块 VAE 编码，实现长序列的高效处理和对任意分辨率的泛化。3. 增强的条件 VAE 解码器，利用多尺度输入特征提升重建保真度。", "result": "LDF-VFI 在具有挑战性的长序列插值基准测试中取得了最先进（SOTA）的性能。实验证明，该方法在每帧质量和时间连贯性方面表现优越，尤其是在大运动场景下。", "conclusion": "LDF-VFI 采用视频中心化、自回归扩散 Transformer 的方法，通过创新的采样策略、注意力机制和编码解码器设计，能够高效处理长视频序列并生成高质量、时间连贯的插值帧，克服了传统帧中心化方法的不足，并在长序列插值任务中展现出领先性能。"}}
{"id": "2601.15224", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2601.15224", "abs": "https://arxiv.org/abs/2601.15224", "authors": ["Jianshu Zhang", "Chengxuan Qian", "Haosen Sun", "Haoran Lu", "Dingcheng Wang", "Letian Xue", "Han Liu"], "title": "PROGRESSLM: Towards Progress Reasoning in Vision-Language Models", "comment": "Website: https://progresslm.github.io/ProgressLM/", "summary": "Estimating task progress requires reasoning over long-horizon dynamics rather than recognizing static visual content. While modern Vision-Language Models (VLMs) excel at describing what is visible, it remains unclear whether they can infer how far a task has progressed from partial observations. To this end, we introduce Progress-Bench, a benchmark for systematically evaluating progress reasoning in VLMs. Beyond benchmarking, we further explore a human-inspired two-stage progress reasoning paradigm through both training-free prompting and training-based approach based on curated dataset ProgressLM-45K. Experiments on 14 VLMs show that most models are not yet ready for task progress estimation, exhibiting sensitivity to demonstration modality and viewpoint changes, as well as poor handling of unanswerable cases. While training-free prompting that enforces structured progress reasoning yields limited and model-dependent gains, the training-based ProgressLM-3B achieves consistent improvements even at a small model scale, despite being trained on a task set fully disjoint from the evaluation tasks. Further analyses reveal characteristic error patterns and clarify when and why progress reasoning succeeds or fails.", "AI": {"tldr": "本研究提出了Progress-Bench基准来评估视觉语言模型（VLMs）的任务进度估计能力，并探索了一种两阶段推理方法。结果表明，大多数VLMs在进度估计方面表现不佳，但基于预训练数据集ProgressLM-3B的训练方法能有效提升性能。", "motivation": "现有模型在理解静态视觉内容方面表现出色，但评估其推理长期动态和任务进度进展的能力仍不明确。因此，研究者希望系统地评估VLMs在任务进度估计方面的表现。", "method": "1. 提出Progress-Bench基准，用于系统评估VLMs的任务进度推理能力。\n2. 探索一种受人类启发的两阶段进度推理范式，包括无训练提示和基于进度数据集ProgressLM-45K的训练方法。\n3. 在14个VLMs上进行实验评估。", "result": "1. 大多数VLMs在任务进度估计方面表现不佳，对演示模式和视角变化敏感，且难以处理无法回答的情况。\n2. 无训练提示的结构化推理方法效果有限且依赖模型。\n3. 基于ProgressLM-3B的训练方法在不完全重叠的任务集上取得了持续的性能提升，即使模型规模较小。", "conclusion": "当前的VLMs在处理任务进度估计方面仍有很大提升空间。然而，通过结构化推理（尤其是在ProgressLM-45K数据集上进行训练）可以显著提高其进度估计能力，即使在较小的模型规模下也能取得成功。"}}
{"id": "2601.15017", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.15017", "abs": "https://arxiv.org/abs/2601.15017", "authors": ["Yanan Wang", "Linjie Ren", "Zihao Li", "Junyi Wang", "Tian Gan"], "title": "SpatialV2A: Visual-Guided High-fidelity Spatial Audio Generation", "comment": null, "summary": "While video-to-audio generation has achieved remarkable progress in semantic and temporal alignment, most existing studies focus solely on these aspects, paying limited attention to the spatial perception and immersive quality of the synthesized audio. This limitation stems largely from current models' reliance on mono audio datasets, which lack the binaural spatial information needed to learn visual-to-spatial audio mappings. To address this gap, we introduce two key contributions: we construct BinauralVGGSound, the first large-scale video-binaural audio dataset designed to support spatially aware video-to-audio generation; and we propose a end-to-end spatial audio generation framework guided by visual cues, which explicitly models spatial features. Our framework incorporates a visual-guided audio spatialization module that ensures the generated audio exhibits realistic spatial attributes and layered spatial depth while maintaining semantic and temporal alignment. Experiments show that our approach substantially outperforms state-of-the-art models in spatial fidelity and delivers a more immersive auditory experience, without sacrificing temporal or semantic consistency. All datasets, code, and model checkpoints will be publicly released to facilitate future research.", "AI": {"tldr": "本文提出了首个用于空间感知视频到音频生成的视频-双耳音频数据集BinauralVGGSound，并开发了一个端到端的框架，该框架通过显式地建模空间特征来生成具有逼真空间属性和深度分层的音频，同时保持语义和时间上的一致性。", "motivation": "现有视频到音频生成模型在语义和时间对齐方面取得了进展，但忽略了空间感知和音频的沉浸感，这主要是因为缺乏包含双耳空间信息的单声道音频数据集。", "method": "构建了BinauralVGGSound数据集，这是首个大规模视频-双耳音频数据集，用于支持空间感知的视频到音频生成。提出了一种端到端的空间音频生成框架，该框架包含一个由视觉线索引导的音频空间化模块，以显式建模空间特征，确保生成音频具有逼真的空间属性和层次感。", "result": "提出的方法在空间保真度方面显著优于最先进的模型，并提供了更具沉浸感的听觉体验，同时保持了时间上和语义上的一致性。", "conclusion": "该研究成功构建了首个支持空间感知视频到音频生成的视频-双耳音频数据集，并提出了一个有效的框架，能够生成具有逼真空间属性和沉浸感的音频，填补了现有研究的空白。"}}
{"id": "2601.15049", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.15049", "abs": "https://arxiv.org/abs/2601.15049", "authors": ["Isaac Baglin", "Xiatian Zhu", "Simon Hadfield"], "title": "Deep Leakage with Generative Flow Matching Denoiser", "comment": null, "summary": "Federated Learning (FL) has emerged as a powerful paradigm for decentralized model training, yet it remains vulnerable to deep leakage (DL) attacks that reconstruct private client data from shared model updates. While prior DL methods have demonstrated varying levels of success, they often suffer from instability, limited fidelity, or poor robustness under realistic FL settings. We introduce a new DL attack that integrates a generative Flow Matching (FM) prior into the reconstruction process. By guiding optimization toward the distribution of realistic images (represented by a flow matching foundation model), our method enhances reconstruction fidelity without requiring knowledge of the private data. Extensive experiments on multiple datasets and target models demonstrate that our approach consistently outperforms state-of-the-art attacks across pixel-level, perceptual, and feature-based similarity metrics. Crucially, the method remains effective across different training epochs, larger client batch sizes, and under common defenses such as noise injection, clipping, and sparsification. Our findings call for the development of new defense strategies that explicitly account for adversaries equipped with powerful generative priors.", "AI": {"tldr": "本文提出了一种新的联邦学习中的深度泄漏攻击方法，通过引入生成式流匹配（FM）先验，显著提高了重建数据的保真度和鲁棒性，即使在存在常见防御措施的情况下也有效。", "motivation": "现有的深度泄漏（DL）攻击在联邦学习（FL）场景下存在不稳定性、保真度有限或鲁棒性差等问题，促使研究者开发更有效、更逼真的攻击方法。", "method": "该研究引入了一种新的DL攻击，将生成式流匹配（FM）先验整合到数据重建过程中。通过利用流匹配基础模型表示的真实图像分布来指导优化，提高了重建保真度，且无需私有数据知识。", "result": "在多个数据集和目标模型上的实验表明，该方法在像素级、感知级和特征级相似性度量上均优于现有最先进的攻击方法。该方法在不同训练轮次、更大的客户端批次大小以及噪声注入、剪枝和稀疏化等常见防御下依然有效。", "conclusion": "研究结果表明，新的FM先验驱动的DL攻击在联邦学习中具有更高的保真度和鲁棒性。这表明需要开发新的防御策略来应对拥有强大生成先验的攻击者。"}}
{"id": "2601.15016", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.15016", "abs": "https://arxiv.org/abs/2601.15016", "authors": ["Xiaodong Wang", "Langling Huang", "Zhirong Wu", "Xu Zhao", "Teng Xu", "Xuhong Xia", "Peixi Peng"], "title": "LiViBench: An Omnimodal Benchmark for Interactive Livestream Video Understanding", "comment": "AAAI 2026 Main Track", "summary": "The development of multimodal large language models (MLLMs) has advanced general video understanding. However, existing video evaluation benchmarks primarily focus on non-interactive videos, such as movies and recordings. To fill this gap, this paper proposes the first omnimodal benchmark for interactive livestream videos, LiViBench. It features a diverse set of 24 tasks, highlighting the perceptual, reasoning, and livestream-specific challenges. To efficiently construct the dataset, we design a standardized semi-automatic annotation workflow that incorporates the human-in-the-loop at multiple stages. The workflow leverages multiple MLLMs to form a multi-agent system for comprehensive video description and uses a seed-question-driven method to construct high-quality annotations. All interactive videos in the benchmark include audio, speech, and real-time comments modalities. To enhance models' understanding of interactive videos, we design tailored two-stage instruction-tuning and propose a Video-to-Comment Retrieval (VCR) module to improve the model's ability to utilize real-time comments. Based on these advancements, we develop LiVi-LLM-7B, an MLLM with enhanced knowledge of interactive livestreams. Experiments show that our model outperforms larger open-source models with up to 72B parameters, narrows the gap with leading proprietary models on LiViBench, and achieves enhanced performance on general video benchmarks, including VideoMME, LongVideoBench, MLVU, and VideoEval-Pro.", "AI": {"tldr": "本文提出了LiViBench，一个针对交互式直播视频的全面基准测试，包含24个多样化任务，并开发了一个高效的半自动标注流程。此外，还提出了针对交互式视频的指令微调方法和视频-评论检索（VCR）模块，并基于此构建了LiVi-LLM-7B模型，该模型在LiViBench及其他视频基准测试中表现出色。", "motivation": "现有视频评估基准主要关注非交互式视频，而忽视了交互式直播视频的独特挑战。因此，需要一个专门针对交互式直播视频的基准来推动多模态大语言模型（MLLMs）在这一领域的发展。", "method": "本文提出了LiViBench基准，包含24个任务，并设计了一个半自动标注工作流，利用多智能体系统和种子问题驱动方法进行高效标注。模型方面，设计了两阶段指令微调方法，并引入视频-评论检索（VCR）模块来提升模型对实时评论的利用能力。最终构建了LiVi-LLM-7B模型。", "result": "LiVi-LLM-7B模型在LiViBench基准上表现优于参数量更大的开源模型，并缩小了与领先的专有模型在LiViBench上的差距。此外，该模型在VideoMME、LongVideoBench、MLVU和VideoEval-Pro等通用视频基准上也取得了增强的性能。", "conclusion": "LiViBench为交互式直播视频评估提供了一个新的、全面的基准。通过提出的标注流程、指令微调方法和VCR模块，以及LiVi-LLM-7B模型，显著提升了模型对交互式直播视频的理解能力，并证明了其在通用视频理解任务上的有效性。"}}
{"id": "2601.15065", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.15065", "abs": "https://arxiv.org/abs/2601.15065", "authors": ["Tianyu Li", "Songyue Cai", "Zongqian Wu", "Ping Hu", "Xiaofeng Zhu"], "title": "Enhancing Few-Shot Out-of-Distribution Detection via the Refinement of Foreground and Background", "comment": null, "summary": "CLIP-based foreground-background (FG-BG) decomposition methods have demonstrated remarkable effectiveness in improving few-shot out-of-distribution (OOD) detection performance. However, existing approaches still suffer from several limitations. For background regions obtained from decomposition, existing methods adopt a uniform suppression strategy for all patches, overlooking the varying contributions of different patches to the prediction. For foreground regions, existing methods fail to adequately consider that some local patches may exhibit appearance or semantic similarity to other classes, which may mislead the training process. To address these issues, we propose a new plug-and-play framework. This framework consists of three core components: (1) a Foreground-Background Decomposition module, which follows previous FG-BG methods to separate an image into foreground and background regions; (2) an Adaptive Background Suppression module, which adaptively weights patch classification entropy; and (3) a Confusable Foreground Rectification module, which identifies and rectifies confusable foreground patches. Extensive experimental results demonstrate that the proposed plug-and-play framework significantly improves the performance of existing FG-BG decomposition methods. Code is available at: https://github.com/lounwb/FoBoR.", "AI": {"tldr": "本文提出了一种即插即用的框架，用于改进基于CLIP的少样本离分布检测性能，通过自适应背景抑制和可疑前景纠正来解决现有方法的局限性。", "motivation": "现有基于CLIP的FG-BG分解方法在背景区域采用统一抑制策略，忽略了不同区域的贡献差异；并且未能充分考虑前景区域中可能存在的与其他类别的外观或语义相似性，从而误导训练。这些限制促使了本研究。", "method": "提出一个包含三个核心组件的即插即用框架：1. 前景-背景分解模块，用于分离图像；2. 自适应背景抑制模块，自适应加权块分类熵；3. 可疑前景纠正模块，识别并纠正可疑的前景块。", "result": "实验结果表明，所提出的框架显著提升了现有FG-BG分解方法的性能。", "conclusion": "该即插即用框架能够有效地解决现有FG-BG分解方法在背景抑制和前景处理上的不足，从而提高少样本离分布检测的性能。"}}
{"id": "2601.15098", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.15098", "abs": "https://arxiv.org/abs/2601.15098", "authors": ["Yipeng Yin", "Rao Yao", "Qingying Li", "Dazhong Wang", "Hong Zhou", "Zhijun Fang", "Jianing Chen", "Longjie Qian", "Mingyue Wu"], "title": "Three-dimensional visualization of X-ray micro-CT with large-scale datasets: Efficiency and accuracy for real-time interaction", "comment": "Page1-37", "summary": "As Micro-CT technology continues to refine its characterization of material microstructures, industrial CT ultra-precision inspection is generating increasingly large datasets, necessitating solutions to the trade-off between accuracy and efficiency in the 3D characterization of defects during ultra-precise detection. This article provides a unique perspective on recent advances in accurate and efficient 3D visualization using Micro-CT, tracing its evolution from medical imaging to industrial non-destructive testing (NDT). Among the numerous CT reconstruction and volume rendering methods, this article selectively reviews and analyzes approaches that balance accuracy and efficiency, offering a comprehensive analysis to help researchers quickly grasp highly efficient and accurate 3D reconstruction methods for microscopic features. By comparing the principles of computed tomography with advancements in microstructural technology, this article examines the evolution of CT reconstruction algorithms from analytical methods to deep learning techniques, as well as improvements in volume rendering algorithms, acceleration, and data reduction. Additionally, it explores advanced lighting models for high-accuracy, photorealistic, and efficient volume rendering. Furthermore, this article envisions potential directions in CT reconstruction and volume rendering. It aims to guide future research in quickly selecting efficient and precise methods and developing new ideas and approaches for real-time online monitoring of internal material defects through virtual-physical interaction, for applying digital twin model to structural health monitoring (SHM).", "AI": {"tldr": "本文综述了Micro-CT技术在工业无损检测中提高三维缺陷表征的准确性和效率的方法，重点关注计算断层扫描重建和体绘制算法的最新进展，并展望了未来的发展方向。", "motivation": "工业CT检测产生的大型数据集需要在准确性和效率之间进行权衡，需要发展更优的Micro-CT三维可视化方法来应对这一挑战。", "method": "本文对Micro-CT重建和体绘制算法进行了选择性综述和分析，涵盖了从解析方法到深度学习技术的演进，以及体绘制算法的加速、数据缩减和高级照明模型。", "result": "文章分析了平衡准确性和效率的多种算法，为研究人员提供了快速掌握高效精确三维重建方法的途径。", "conclusion": "通过对CT重建和体绘制算法的演进以及高级照明模型的探讨，本文旨在为未来研究指明方向，促进实时在线监测和数字孪生在结构健康监测中的应用。"}}
{"id": "2601.15071", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.15071", "abs": "https://arxiv.org/abs/2601.15071", "authors": ["Jingyang Huo", "Yikai Wang", "Yanwei Fu", "Jianfeng Feng"], "title": "The Pictorial Cortex: Zero-Shot Cross-Subject fMRI-to-Image Reconstruction via Compositional Latent Modeling", "comment": null, "summary": "Decoding visual experiences from human brain activity remains a central challenge at the intersection of neuroscience, neuroimaging, and artificial intelligence. A critical obstacle is the inherent variability of cortical responses: neural activity elicited by the same visual stimulus differs across individuals and trials due to anatomical, functional, cognitive, and experimental factors, making fMRI-to-image reconstruction non-injective. In this paper, we tackle a challenging yet practically meaningful problem: zero-shot cross-subject fMRI-to-image reconstruction, where the visual experience of a previously unseen individual must be reconstructed without subject-specific training. To enable principled evaluation, we present a unified cortical-surface dataset -- UniCortex-fMRI, assembled from multiple visual-stimulus fMRI datasets to provide broad coverage of subjects and stimuli. Our UniCortex-fMRI is particularly processed by standardized data formats to make it possible to explore this possibility in the zero-shot scenario of cross-subject fMRI-to-image reconstruction. To tackle the modeling challenge, we propose PictorialCortex, which models fMRI activity using a compositional latent formulation that structures stimulus-driven representations under subject-, dataset-, and trial-related variability. PictorialCortex operates in a universal cortical latent space and implements this formulation through a latent factorization-composition module, reinforced by paired factorization and re-factorizing consistency regularization. During inference, surrogate latents synthesized under multiple seen-subject conditions are aggregated to guide diffusion-based image synthesis for unseen subjects. Extensive experiments show that PictorialCortex improves zero-shot cross-subject visual reconstruction, highlighting the benefits of compositional latent modeling and multi-dataset training.", "AI": {"tldr": "本研究提出了一种名为PictorialCortex的新方法，用于在没有目标受试者特定训练的情况下，从从未见过的受试者的大脑活动（fMRI）重建视觉图像，解决了fMRI到图像重建中的个体差异和数据稀疏性问题。", "motivation": "现有fMRI到图像重建方法通常需要受试者特定的训练数据，这限制了其在实际应用中的普适性。本研究旨在解决“零样本跨主体fMRI到图像重建”这一更具挑战性但更实用的问题，即在没有目标受试者的情况下进行重建。", "method": "研究者提出了PictorialCortex模型，该模型采用一种组合式潜在表征（compositional latent formulation）来模拟fMRI活动，将受试者、数据集和实验等因素引起的变异性进行结构化处理。模型在一个通用的皮层潜在空间（universal cortical latent space）中运行，并通过潜在因子分解-组合模块（latent factorization-composition module）实现，辅以成对的因子分解和再因子分解一致性正则化。在推理时，通过在多个已见受试者条件下合成的代理潜在表示来指导扩散模型进行图像合成。", "result": "PictorialCortex在零样本跨主体视觉重建任务上取得了显著的性能提升，证明了组合式潜在建模和多数据集训练的有效性。", "conclusion": "研究成功地提出了一种无需目标受试者特定训练即可实现fMRI到图像重建的方法，并通过提出的PictorialCortex模型和UniCortex-fMRI数据集展示了其在零样本跨主体场景下的优越性，强调了组合式潜在表征和跨数据集学习的重要性。"}}
{"id": "2601.15133", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.15133", "abs": "https://arxiv.org/abs/2601.15133", "authors": ["André Eberhard", "Gerhard Neumann", "Pascal Friederich"], "title": "Graph Recognition via Subgraph Prediction", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Despite tremendous improvements in tasks such as image classification, object detection, and segmentation, the recognition of visual relationships, commonly modeled as the extraction of a graph from an image, remains a challenging task. We believe that this mainly stems from the fact that there is no canonical way to approach the visual graph recognition task. Most existing solutions are specific to a problem and cannot be transferred between different contexts out-of-the box, even though the conceptual problem remains the same. With broad applicability and simplicity in mind, in this paper we develop a method, \\textbf{Gra}ph Recognition via \\textbf{S}ubgraph \\textbf{P}rediction (\\textbf{GraSP}), for recognizing graphs in images. We show across several synthetic benchmarks and one real-world application that our method works with a set of diverse types of graphs and their drawings, and can be transferred between tasks without task-specific modifications, paving the way to a more unified framework for visual graph recognition.", "AI": {"tldr": "提出了一种名为 GraSP 的新方法，用于通过子图预测来识别图像中的视觉关系图，旨在提高跨不同任务和图类型的通用性和可迁移性。", "motivation": "现有的视觉关系识别方法通常是特定于问题的，难以迁移到不同场景，缺乏统一的解决方案。", "method": "提出了一种名为 GraSP（Graph Recognition via Subgraph Prediction）的方法，通过子图预测来识别图像中的视觉图。", "result": "GraSP 在多个合成基准和一项真实世界应用中表现良好，能够处理各种类型的图及其绘制，并且可以跨任务进行迁移，无需针对特定任务进行修改。", "conclusion": "GraSP 方法通用性强且易于实现，有望为视觉图识别提供一个更统一的框架，克服现有方法的局限性。"}}
{"id": "2601.15110", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.15110", "abs": "https://arxiv.org/abs/2601.15110", "authors": ["Aoran Liu", "Kun Hu", "Clinton Ansun Mo", "Qiuxia Wu", "Wenxiong Kang", "Zhiyong Wang"], "title": "Pb4U-GNet: Resolution-Adaptive Garment Simulation via Propagation-before-Update Graph Network", "comment": "Camera-ready version accepted at AAAI 2026", "summary": "Garment simulation is fundamental to various applications in computer vision and graphics, from virtual try-on to digital human modelling. However, conventional physics-based methods remain computationally expensive, hindering their application in time-sensitive scenarios. While graph neural networks (GNNs) offer promising acceleration, existing approaches exhibit poor cross-resolution generalisation, demonstrating significant performance degradation on higher-resolution meshes beyond the training distribution. This stems from two key factors: (1) existing GNNs employ fixed message-passing depth that fails to adapt information aggregation to mesh density variation, and (2) vertex-wise displacement magnitudes are inherently resolution-dependent in garment simulation. To address these issues, we introduce Propagation-before-Update Graph Network (Pb4U-GNet), a resolution-adaptive framework that decouples message propagation from feature updates. Pb4U-GNet incorporates two key mechanisms: (1) dynamic propagation depth control, adjusting message-passing iterations based on mesh resolution, and (2) geometry-aware update scaling, which scales predictions according to local mesh characteristics. Extensive experiments show that even trained solely on low-resolution meshes, Pb4U-GNet exhibits strong generalisability across diverse mesh resolutions, addressing a fundamental challenge in neural garment simulation.", "AI": {"tldr": "提出了一种名为Pb4U-GNet的图神经网络框架，通过动态调整消息传递深度和几何感知更新缩放，解决了现有GNN在服装模拟中跨分辨率泛化能力差的问题。", "motivation": "传统的基于物理的服装模拟计算成本高，而现有的基于GNN的方法在跨分辨率泛化方面表现不佳，尤其是在训练分布之外的高分辨率网格上性能会显著下降。", "method": "Pb4U-GNet框架通过解耦消息传播和特征更新，引入了两个关键机制：1. 动态传播深度控制，根据网格分辨率自适应地调整消息传递的迭代次数；2. 几何感知更新缩放，根据局部网格特征缩放预测结果。", "result": "在仅使用低分辨率网格进行训练的情况下，Pb4U-GNet在各种网格分辨率上都表现出强大的泛化能力，有效解决了神经服装模拟中的一个根本性挑战。", "conclusion": "Pb4U-GNet是一种分辨率自适应的服装模拟框架，通过动态传播深度控制和几何感知更新缩放，能够有效地提高模型的跨分辨率泛化能力，克服了现有方法的局限性。"}}
{"id": "2601.15115", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.15115", "abs": "https://arxiv.org/abs/2601.15115", "authors": ["Shuonan Yang", "Yuchen Zhang", "Zeyu Fu"], "title": "Training-Free and Interpretable Hateful Video Detection via Multi-stage Adversarial Reasoning", "comment": "Accepted at ICASSP 2026. \\c{opyright} 2026 IEEE. This is the author accepted manuscript. The final published version will be available via IEEE Xplore", "summary": "Hateful videos pose serious risks by amplifying discrimination, inciting violence, and undermining online safety. Existing training-based hateful video detection methods are constrained by limited training data and lack of interpretability, while directly prompting large vision-language models often struggle to deliver reliable hate detection. To address these challenges, this paper introduces MARS, a training-free Multi-stage Adversarial ReaSoning framework that enables reliable and interpretable hateful content detection. MARS begins with the objective description of video content, establishing a neutral foundation for subsequent analysis. Building on this, it develops evidence-based reasoning that supports potential hateful interpretations, while in parallel incorporating counter-evidence reasoning to capture plausible non-hateful perspectives. Finally, these perspectives are synthesized into a conclusive and explainable decision. Extensive evaluation on two real-world datasets shows that MARS achieves up to 10% improvement under certain backbones and settings compared to other training-free approaches and outperforms state-of-the-art training-based methods on one dataset. In addition, MARS produces human-understandable justifications, thereby supporting compliance oversight and enhancing the transparency of content moderation workflows. The code is available at https://github.com/Multimodal-Intelligence-Lab-MIL/MARS.", "AI": {"tldr": "本文提出了一种名为 MARS 的无训练多阶段对抗推理框架，用于可靠且可解释地检测仇恨视频。该方法通过生成性描述、证据推理、反证据推理和最终综合来工作，实现了比现有方法更好的性能，并提供了可解释的理由。", "motivation": "现有的仇恨视频检测方法受限于有限的训练数据和缺乏可解释性，而直接提示大型视觉语言模型在可靠检测方面存在困难。因此，需要一种无需训练、性能优越且可解释的仇恨视频检测方法。", "method": "MARS 是一个多阶段对抗推理框架，包括：1. 视频内容的目标描述（建立中立基础）；2. 支持潜在仇恨解释的基于证据的推理；3. 捕捉合理非仇恨观点的反证据推理；4. 将这些观点综合为最终的、可解释的决定。", "result": "在两个真实数据集上的评估表明，MARS 在某些骨干和设置下比其他无训练方法提高了高达 10% 的性能，并在一个数据集上优于最先进的训练方法。此外，MARS 生成了人类可理解的解释。", "conclusion": "MARS 作为一个无训练框架，能够可靠且可解释地检测仇恨内容，其性能优于现有方法，并提供了增强内容审核透明度和合规性监督的可解释性理由。"}}
{"id": "2601.14978", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.14978", "abs": "https://arxiv.org/abs/2601.14978", "authors": ["Nilanjana Chatterjee", "Sidharatha Garg", "A V Subramanyam", "Brejesh Lall"], "title": "Unified Multi-Dataset Training for TBPS", "comment": null, "summary": "Text-Based Person Search (TBPS) has seen significant progress with vision-language models (VLMs), yet it remains constrained by limited training data and the fact that VLMs are not inherently pre-trained for pedestrian-centric recognition. Existing TBPS methods therefore rely on dataset-centric fine-tuning to handle distribution shift, resulting in multiple independently trained models for different datasets. While synthetic data can increase the scale needed to fine-tune VLMs, it does not eliminate dataset-specific adaptation. This motivates a fundamental question: can we train a single unified TBPS model across multiple datasets? We show that naive joint training over all datasets remains sub-optimal because current training paradigms do not scale to a large number of unique person identities and are vulnerable to noisy image-text pairs. To address these challenges, we propose Scale-TBPS with two contributions: (i) a noise-aware unified dataset curation strategy that cohesively merges diverse TBPS datasets; and (ii) a scalable discriminative identity learning framework that remains effective under a large number of unique identities. Extensive experiments on CUHK-PEDES, ICFG-PEDES, RSTPReid, IIITD-20K, and UFine6926 demonstrate that a single Scale-TBPS model outperforms dataset-centric optimized models and naive joint training.", "AI": {"tldr": "本文提出了一种名为Scale-TBPS的统一文本检索行人（TBPS）模型，通过统一的数据集策​​略和可扩展的判别式身份学习框架，克服了现有模型在多数据集联合训练中的局限性，并在多个公开数据集上取得了优于单一数据集微调模型的性能。", "motivation": "现有的文本检索行人（TBPS）方法依赖于针对特定数据集的微调，导致需要为不同数据集训练多个模型。虽然合成数据可以增加训练规模，但并未解决数据集的特定适应性问题。因此，作者希望训练一个能够跨多个数据集的单一统一TBPS模型。", "method": "作者提出了Scale-TBPS，包含两个主要贡献：（1）一种噪声感知型的统一数据集策​​略，用于整合不同的TBPS数据集；（2）一个可扩展的判别式身份学习框架，即使在大量唯一身份的情况下也能保持有效性。", "result": "在一系列公开数据集（CUHK-PEDES, ICFG-PEDES, RSTPReid, IIITD-20K, and UFine6926）上的实验表明，单一的Scale-TBPS模型性能优于针对特定数据集优化的模型以及朴素的联合训练方法。", "conclusion": "Scale-TBPS能够成功训练一个单一的、通用的TBPS模型，该模型在多个数据集上表现出色，克服了传统方法在处理大规模、多样化行人身份时的挑战。"}}
{"id": "2601.15170", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.15170", "abs": "https://arxiv.org/abs/2601.15170", "authors": ["Zhucun Xue", "Jiangning Zhang", "Juntao Jiang", "Jinzhuo Liu", "Haoyang He", "Teng Hu", "Xiaobin Hu", "Guangming Yao", "Yi Yuan", "Yong Liu"], "title": "Large-Scale Multidimensional Knowledge Profiling of Scientific Literature", "comment": "Code and dataset: https://github.com/xzc-zju/Profiling_Scientific_Literature", "summary": "The rapid expansion of research across machine learning, vision, and language has produced a volume of publications that is increasingly difficult to synthesize. Traditional bibliometric tools rely mainly on metadata and offer limited visibility into the semantic content of papers, making it hard to track how research themes evolve over time or how different areas influence one another. To obtain a clearer picture of recent developments, we compile a unified corpus of more than 100,000 papers from 22 major conferences between 2020 and 2025 and construct a multidimensional profiling pipeline to organize and analyze their textual content. By combining topic clustering, LLM-assisted parsing, and structured retrieval, we derive a comprehensive representation of research activity that supports the study of topic lifecycles, methodological transitions, dataset and model usage patterns, and institutional research directions. Our analysis highlights several notable shifts, including the growth of safety, multimodal reasoning, and agent-oriented studies, as well as the gradual stabilization of areas such as neural machine translation and graph-based methods. These findings provide an evidence-based view of how AI research is evolving and offer a resource for understanding broader trends and identifying emerging directions. Code and dataset: https://github.com/xzc-zju/Profiling_Scientific_Literature", "AI": {"tldr": "本研究构建了一个包含超过10万篇机器学习、计算机视觉和自然语言处理领域论文的语料库，并开发了一个多维度分析流程，以理解研究主题的演变、方法论的转变以及数据集和模型的使用模式。", "motivation": "传统文献计量工具仅依赖元数据，难以深入理解论文的语义内容，限制了对研究主题演变和领域间相互影响的追踪。研究者需要更有效的工具来掌握AI研究的最新进展。", "method": "研究者构建了一个包含2020-2025年间22个顶级会议的10万多篇论文的统一语料库，并开发了一个多维度分析流程，结合了主题聚类、LLM辅助解析和结构化检索，以组织和分析文本内容。", "result": "分析揭示了AI研究中的几个显著变化，包括安全、多模态推理和面向代理的研究的增长，以及神经机器翻译和基于图的方法等领域的逐步稳定。", "conclusion": "这项研究提供了一个基于证据的AI研究演变视图，并通过分析研究主题生命周期、方法论转变、数据集和模型使用模式以及机构研究方向，为理解宏观趋势和识别新兴方向提供了资源。"}}
{"id": "2601.15221", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.15221", "abs": "https://arxiv.org/abs/2601.15221", "authors": ["Hanlei Guo", "Jiahao Shao", "Xinya Chen", "Xiyang Tan", "Sheng Miao", "Yujun Shen", "Yiyi Liao"], "title": "ScenDi: 3D-to-2D Scene Diffusion Cascades for Urban Generation", "comment": null, "summary": "Recent advancements in 3D object generation using diffusion models have achieved remarkable success, but generating realistic 3D urban scenes remains challenging. Existing methods relying solely on 3D diffusion models tend to suffer a degradation in appearance details, while those utilizing only 2D diffusion models typically compromise camera controllability. To overcome this limitation, we propose ScenDi, a method for urban scene generation that integrates both 3D and 2D diffusion models. We first train a 3D latent diffusion model to generate 3D Gaussians, enabling the rendering of images at a relatively low resolution. To enable controllable synthesis, this 3DGS generation process can be optionally conditioned by specifying inputs such as 3d bounding boxes, road maps, or text prompts. Then, we train a 2D video diffusion model to enhance appearance details conditioned on rendered images from the 3D Gaussians. By leveraging the coarse 3D scene as guidance for 2D video diffusion, ScenDi generates desired scenes based on input conditions and successfully adheres to accurate camera trajectories. Experiments on two challenging real-world datasets, Waymo and KITTI-360, demonstrate the effectiveness of our approach.", "AI": {"tldr": "ScenDi 是一种结合了 3D 和 2D 扩散模型的方法，用于生成具有丰富细节和精确相机控制的 3D 城市场景。它首先生成粗糙的 3D 高斯表示，然后使用 2D 视频扩散模型增强细节。", "motivation": "现有 3D 城市场景生成方法要么在外观细节上表现不佳（仅使用 3D 扩散模型），要么在相机控制方面受限（仅使用 2D 扩散模型）。", "method": "ScenDi 首先训练一个 3D 潜在扩散模型来生成 3D 高斯（3DGS），该模型可以根据 3D 边界框、道路图或文本提示进行条件生成。然后，训练一个 2D 视频扩散模型，以 3DGS 渲染的图像为条件，来增强外观细节。", "result": "ScenDi 能够生成符合输入条件且具有准确相机轨迹的 3D 城市场景。在 Waymo 和 KITTI-360 数据集上的实验表明了该方法的有效性。", "conclusion": "通过结合 3D 和 2D 扩散模型，ScenDi 成功解决了现有方法在 3D 城市场景生成中的不足，实现了细节丰富且相机可控的场景合成。"}}
{"id": "2601.15283", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2601.15283", "abs": "https://arxiv.org/abs/2601.15283", "authors": ["Ruofan Liang", "Norman Müller", "Ethan Weber", "Duncan Zauss", "Nandita Vijaykumar", "Peter Kontschieder", "Christian Richardt"], "title": "LuxRemix: Lighting Decomposition and Remixing for Indoor Scenes", "comment": "Project page: https://luxremix.github.io", "summary": "We present a novel approach for interactive light editing in indoor scenes from a single multi-view scene capture. Our method leverages a generative image-based light decomposition model that factorizes complex indoor scene illumination into its constituent light sources. This factorization enables independent manipulation of individual light sources, specifically allowing control over their state (on/off), chromaticity, and intensity. We further introduce multi-view lighting harmonization to ensure consistent propagation of the lighting decomposition across all scene views. This is integrated into a relightable 3D Gaussian splatting representation, providing real-time interactive control over the individual light sources. Our results demonstrate highly photorealistic lighting decomposition and relighting outcomes across diverse indoor scenes. We evaluate our method on both synthetic and real-world datasets and provide a quantitative and qualitative comparison to state-of-the-art techniques. For video results and interactive demos, see https://luxremix.github.io.", "AI": {"tldr": "提出了一种新颖的单视角多视图室内场景交互式光照编辑方法，通过生成式图像分解模型实现对单个光源的独立控制（开关、色度、强度），并结合多视图光照协调和3D高斯泼溅表示，实现了实时交互式重光照。", "motivation": "现有方法在单视角下进行室内场景光照编辑存在局限性，难以实现对单个光源的精细、独立控制，并且在多视图下容易出现光照不一致的问题。", "method": "利用生成式图像分解模型将复杂室内场景光照分解为独立光源，然后对每个光源进行独立的开关、色度、强度控制，并通过多视图光照协调技术保证所有视图光照一致性，最后将其集成到可重光照的3D高斯泼溅表示中，实现实时交互。", "result": "在合成和真实世界数据集上均取得了高度逼真的光照分解和重光照效果，并在定量和定性上优于现有技术。", "conclusion": "该方法能够实现高质量、实时的室内场景交互式光照编辑，为单视角多视图场景下的光照控制提供了新的解决方案。"}}
{"id": "2601.15200", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.15200", "abs": "https://arxiv.org/abs/2601.15200", "authors": ["Miroslav Purkrabek", "Constantin Kolomiiets", "Jiri Matas"], "title": "BBoxMaskPose v2: Expanding Mutual Conditioning to 3D", "comment": "GitHub repository: https://github.com/MiraPurkrabek/BBoxMaskPose/", "summary": "Most 2D human pose estimation benchmarks are nearly saturated, with the exception of crowded scenes. We introduce PMPose, a top-down 2D pose estimator that incorporates the probabilistic formulation and the mask-conditioning. PMPose improves crowded pose estimation without sacrificing performance on standard scenes. Building on this, we present BBoxMaskPose v2 (BMPv2) integrating PMPose and an enhanced SAM-based mask refinement module. BMPv2 surpasses state-of-the-art by 1.5 average precision (AP) points on COCO and 6 AP points on OCHuman, becoming the first method to exceed 50 AP on OCHuman. We demonstrate that BMP's 2D prompting of 3D model improves 3D pose estimation in crowded scenes and that advances in 2D pose quality directly benefit 3D estimation. Results on the new OCHuman-Pose dataset show that multi-person performance is more affected by pose prediction accuracy than by detection. The code, models, and data are available on https://MiraPurkrabek.github.io/BBox-Mask-Pose/.", "AI": {"tldr": "本文提出了一种名为 PMPose 的新颖 2D 人体姿态估计方法，并在此基础上开发了 BMPv2，该方法在拥挤场景下表现出色，并在 COCO 和 OCHuman 数据集上达到了 SOTA 性能。研究还表明 2D 姿态估计的改进对 3D 姿态估计有益。", "motivation": "现有的 2D 人体姿态估计基准在标准场景下已接近饱和，而拥挤场景仍是挑战。作者旨在提升拥挤场景下的 2D 人体姿态估计性能，同时不牺牲标准场景的性能。", "method": "作者提出了 PMPose，一种结合概率公式和掩码条件的自顶向下 2D 姿态估计器。在此基础上，开发了 BMPv2，集成了 PMPose 和一个基于 SAM 的增强掩码精炼模块。此外，还探索了 2D 提示 3D 模型以改进拥挤场景下的 3D 姿态估计。", "result": "BMPv2 在 COCO 数据集上比 SOTA 方法高出 1.5 AP，在 OCHuman 数据集上高出 6 AP，成为 OCHuman 上首个 AP 超过 50 的方法。在拥挤场景下，2D 姿态估计的质量直接影响 3D 姿态估计的性能。", "conclusion": "PMPose 和 BMPv2 在拥挤场景下实现了最先进的 2D 人体姿态估计。2D 姿态估计的质量提升能够有效促进 3D 姿态估计。在多人物场景下，姿态预测精度比检测精度对整体性能影响更大。"}}
{"id": "2601.15202", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.15202", "abs": "https://arxiv.org/abs/2601.15202", "authors": ["Md Mahmudul Hoque", "Shuvo Karmaker", "Md. Hadi Al-Amin", "Md Modabberul Islam", "Jisun Junayed", "Farha Ulfat Mahi"], "title": "A Computer Vision Hybrid Approach: CNN and Transformer Models for Accurate Alzheimer's Detection from Brain MRI Scans", "comment": null, "summary": "Early and accurate classification of Alzheimers disease (AD) from brain MRI scans is essential for timely clinical intervention and improved patient outcomes. This study presents a comprehensive comparative analysis of five CNN architectures (EfficientNetB0, ResNet50, DenseNet201, MobileNetV3, VGG16), five Transformer-based models (ViT, ConvTransformer, PatchTransformer, MLP-Mixer, SimpleTransformer), and a proposed hybrid model named Evan_V2. All models were evaluated on a four-class AD classification task comprising Mild Dementia, Moderate Dementia, Non-Demented, and Very Mild Dementia categories. Experimental findings show that CNN architectures consistently achieved strong performance, with ResNet50 attaining 98.83% accuracy. Transformer models demonstrated competitive generalization capabilities, with ViT achieving the highest accuracy among them at 95.38%. However, individual Transformer variants exhibited greater class-specific instability. The proposed Evan_V2 hybrid model, which integrates outputs from ten CNN and Transformer architectures through feature-level fusion, achieved the best overall performance with 99.99% accuracy, 0.9989 F1-score, and 0.9968 ROC AUC. Confusion matrix analysis further confirmed that Evan_V2 substantially reduced misclassification across all dementia stages, outperforming every standalone model. These findings highlight the potential of hybrid ensemble strategies in producing highly reliable and clinically meaningful diagnostic tools for Alzheimers disease classification.", "AI": {"tldr": "本研究比较了五种CNN、五种Transformer模型以及一个名为Evan_V2的混合模型在阿尔茨海默病（AD）四分类任务上的表现。结果显示，CNN表现稳健，ResNet50准确率达98.83%。Transformer模型泛化能力强，ViT准确率最高为95.38%。提出的Evan_V2混合模型通过特征级融合了十种CNN和Transformer模型的输出，取得了最佳性能，准确率高达99.99%，F1-score为0.9989，ROC AUC为0.9968，显著减少了误分类。", "motivation": "早期准确诊断阿尔茨海默病（AD）对于及时干预和改善患者预后至关重要。本研究旨在通过比较和融合多种深度学习模型，探索更有效的AD诊断方法。", "method": "本研究对五种CNN架构（EfficientNetB0、ResNet50、DenseNet201、MobileNetV3、VGG16）、五种Transformer模型（ViT、ConvTransformer、PatchTransformer、MLP-Mixer、SimpleTransformer）以及一个提出的混合模型Evan_V2进行了比较分析。所有模型都在包含轻度痴呆、中度痴呆、非痴呆和极轻度痴呆四个类别的AD分类任务上进行了评估。Evan_V2模型通过特征级融合了十种CNN和Transformer模型的输出。", "result": "CNN架构表现稳定，ResNet50准确率达到98.83%。Transformer模型泛化能力具有竞争力，ViT在Transformer模型中准确率最高，为95.38%。Evan_V2混合模型取得了最佳的整体性能，准确率为99.99%，F1-score为0.9989，ROC AUC为0.9968。混淆矩阵分析显示，Evan_V2在所有痴呆阶段都显著减少了误分类。", "conclusion": "混合集成策略在开发高度可靠且具有临床意义的阿尔茨海默病诊断工具方面具有巨大潜力。提出的Evan_V2模型通过特征级融合多种CNN和Transformer模型，在AD分类任务上取得了卓越的性能，超越了所有单独的模型。"}}
{"id": "2601.15275", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.15275", "abs": "https://arxiv.org/abs/2601.15275", "authors": ["Yu Wu", "Minsik Jeon", "Jen-Hao Rick Chang", "Oncel Tuzel", "Shubham Tulsiani"], "title": "RayRoPE: Projective Ray Positional Encoding for Multi-view Attention", "comment": "Project page: https://rayrope.github.io/", "summary": "We study positional encodings for multi-view transformers that process tokens from a set of posed input images, and seek a mechanism that encodes patches uniquely, allows SE(3)-invariant attention with multi-frequency similarity, and can be adaptive to the geometry of the underlying scene. We find that prior (absolute or relative) encoding schemes for multi-view attention do not meet the above desiderata, and present RayRoPE to address this gap. RayRoPE represents patch positions based on associated rays but leverages a predicted point along the ray instead of the direction for a geometry-aware encoding. To achieve SE(3) invariance, RayRoPE computes query-frame projective coordinates for computing multi-frequency similarity. Lastly, as the 'predicted' 3D point along a ray may not be precise, RayRoPE presents a mechanism to analytically compute the expected position encoding under uncertainty. We validate RayRoPE on the tasks of novel-view synthesis and stereo depth estimation and show that it consistently improves over alternate position encoding schemes (e.g. 15% relative improvement on LPIPS in CO3D). We also show that RayRoPE can seamlessly incorporate RGB-D input, resulting in even larger gains over alternatives that cannot positionally encode this information.", "AI": {"tldr": "提出了一种名为 RayRoPE 的新位置编码方法，用于多视图 Transformer，它能唯一编码图像块、实现 SE(3) 不变性注意力和多频相似性，并适应场景几何。", "motivation": "现有针对多视图 Transformer 的位置编码方法（绝对或相对）无法同时满足唯一编码、SE(3) 不变性注意力、多频相似性和场景几何适应性等要求。", "method": "RayRoPE 基于射线表示图像块位置，但使用沿射线的预测点而非方向来编码几何信息。为实现 SE(3) 不变性，它计算查询帧投影坐标以进行多频相似性计算。为了处理预测点的不确定性，RayRoPE 提供了分析计算不确定性下期望位置编码的机制。", "result": "在新型视图合成和立体深度估计任务上，RayRoPE 的性能持续优于其他位置编码方案（例如，在 CO3D 数据集上 LPIPS 指标相对提升 15%）。RayRoPE 还能无缝集成 RGB-D 输入，带来比无法位置编码此类信息的方案更大的性能提升。", "conclusion": "RayRoPE 是一种有效的位置编码方法，能够提升多视图 Transformer 在计算机视觉任务中的性能，并且能够灵活适应不同类型的输入数据。"}}
{"id": "2601.15281", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.15281", "abs": "https://arxiv.org/abs/2601.15281", "authors": ["Ying Yang", "Zhengyao Lv", "Tianlin Pan", "Haofan Wang", "Binxin Yang", "Hubery Yin", "Chen Li", "Ziwei Liu", "Chenyang Si"], "title": "StableWorld: Towards Stable and Consistent Long Interactive Video Generation", "comment": "17 pages, 21 figures,", "summary": "In this paper, we explore the overlooked challenge of stability and temporal consistency in interactive video generation, which synthesizes dynamic and controllable video worlds through interactive behaviors such as camera movements and text prompts. Despite remarkable progress in world modeling, current methods still suffer from severe instability and temporal degradation, often leading to spatial drift and scene collapse during long-horizon interactions. To better understand this issue, we initially investigate the underlying causes of instability and identify that the major source of error accumulation originates from the same scene, where generated frames gradually deviate from the initial clean state and propagate errors to subsequent frames. Building upon this observation, we propose a simple yet effective method, \\textbf{StableWorld}, a Dynamic Frame Eviction Mechanism. By continuously filtering out degraded frames while retaining geometrically consistent ones, StableWorld effectively prevents cumulative drift at its source, leading to more stable and temporal consistency of interactive generation. Promising results on multiple interactive video models, \\eg, Matrix-Game, Open-Oasis, and Hunyuan-GameCraft, demonstrate that StableWorld is model-agnostic and can be applied to different interactive video generation frameworks to substantially improve stability, temporal consistency, and generalization across diverse interactive scenarios.", "AI": {"tldr": "本文提出了一种名为 StableWorld 的动态帧淘汰机制，用于解决交互式视频生成中的不稳定性问题，通过过滤掉退化的帧并保留几何一致的帧，从而提高视频的稳定性和时间一致性。", "motivation": "现有交互式视频生成方法在长期交互中存在严重的不稳定性和时间退化问题，导致空间漂移和场景崩溃。", "method": "提出了一种动态帧淘汰机制（StableWorld），通过持续过滤掉退化的帧并保留几何一致的帧，从源头阻止累积漂移。", "result": "StableWorld 在 Matrix-Game、Open-Oasis 和 Hunyuan-GameCraft 等多种交互式视频模型上取得了显著效果，表明其模型无关性，并能提升稳定性、时间一致性和泛化能力。", "conclusion": "StableWorld 是一种简单有效的方法，可以显著改善交互式视频生成的稳定性和时间一致性，并且可以应用于不同的交互式视频生成框架。"}}
{"id": "2601.15288", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.15288", "abs": "https://arxiv.org/abs/2601.15288", "authors": ["Jiwon Kang", "Yeji Choi", "JoungBin Lee", "Wooseok Jang", "Jinhyeok Choi", "Taekeun Kang", "Yongjae Park", "Myungin Kim", "Seungryong Kim"], "title": "APPLE: Attribute-Preserving Pseudo-Labeling for Diffusion-Based Face Swapping", "comment": "Project Page: https://cvlab-kaist.github.io/APPLE/", "summary": "Face swapping aims to transfer the identity of a source face onto a target face while preserving target-specific attributes such as pose, expression, lighting, skin tone, and makeup. However, since real ground truth for face swapping is unavailable, achieving both accurate identity transfer and high-quality attribute preservation remains challenging. In addition, recent diffusion-based approaches attempt to improve visual fidelity through conditional inpainting on masked target images, but the masked condition removes crucial appearance cues of target, resulting in plausible yet misaligned attributes. To address these limitations, we propose APPLE (Attribute-Preserving Pseudo-Labeling), a diffusion-based teacher-student framework that enhances attribute fidelity through attribute-aware pseudo-label supervision. We reformulate face swapping as a conditional deblurring task to more faithfully preserve target-specific attributes such as lighting, skin tone, and makeup. In addition, we introduce an attribute-aware inversion scheme to further improve detailed attribute preservation. Through an elaborate attribute-preserving design for teacher learning, APPLE produces high-quality pseudo triplets that explicitly provide the student with direct face-swapping supervision. Overall, APPLE achieves state-of-the-art performance in terms of attribute preservation and identity transfer, producing more photorealistic and target-faithful results.", "AI": {"tldr": "提出了一种名为APPLE (Attribute-Preserving Pseudo-Labeling) 的扩散模型框架，通过伪标签监督和条件去模糊化任务，提高了身份迁移和目标属性（如光照、肤色、妆容）保持的准确性和视觉保真度。", "motivation": "现有的面部交换方法难以同时实现准确的身份迁移和高质量的目标属性保持，尤其是在使用基于扩散的方法时，掩码处理会丢失关键的目标外观信息。因此，需要一种新方法来解决这些限制。", "method": "提出了一种基于扩散模型的师生框架APPLE。将面部交换重新定义为条件去模糊化任务，以更忠实地保留目标属性。引入了属性感知反演方案以增强细节属性的保持。通过精心的属性保持设计用于教师学习，生成高质量的伪三元组，为学生提供直接的面部交换监督。", "result": "APPLE在属性保持和身份迁移方面取得了最先进的性能，生成了更逼真、更忠实于目标的结果。", "conclusion": "APPLE通过其属性感知伪标签监督和条件去模糊化方法，有效解决了现有面部交换方法的局限性，实现了更高质量的身份迁移和属性保持。"}}
{"id": "2601.15287", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.15287", "abs": "https://arxiv.org/abs/2601.15287", "authors": ["Gautom Das", "Vincent La", "Ethan Lau", "Abhinav Shrivastava", "Matthew Gwilliam"], "title": "Towards Understanding Best Practices for Quantization of Vision-Language Models", "comment": "15 pages, 12 figures, 1 table", "summary": "Large language models (LLMs) deliver impressive results for a variety of tasks, but state-of-the-art systems require fast GPUs with large amounts of memory. To reduce both the memory and latency of these systems, practitioners quantize their learned parameters, typically at half precision. A growing body of research focuses on preserving the model performance with more aggressive bit widths, and some work has been done to apply these strategies to other models, like vision transformers. In our study we investigate how a variety of quantization methods, including state-of-the-art GPTQ and AWQ, can be applied effectively to multimodal pipelines comprised of vision models, language models, and their connectors. We address how performance on captioning, retrieval, and question answering can be affected by bit width, quantization method, and which portion of the pipeline the quantization is used for. Results reveal that ViT and LLM exhibit comparable importance in model performance, despite significant differences in parameter size, and that lower-bit quantization of the LLM achieves high accuracy at reduced bits per weight (bpw). These findings provide practical insights for efficient deployment of MLLMs and highlight the value of exploration for understanding component sensitivities in multimodal models. Our code is available at https://github.com/gautomdas/mmq.", "AI": {"tldr": "本研究探讨了将GPTQ和AWQ等量化方法应用于多模态模型（包括视觉模型、语言模型及其连接器）的有效性，并分析了不同量化策略对图像字幕生成、检索和问答任务性能的影响。", "motivation": "大型语言模型（LLMs）虽然表现出色，但对硬件资源要求很高。研究人员希望通过量化来降低内存和延迟，并探索更低比特宽度下的性能保持。", "method": "研究人员将GPTQ和AWQ等量化方法应用于包含视觉模型（ViT）、语言模型（LLM）和连接器的多模态流水线，并评估了在不同比特宽度、量化方法以及量化应用部分对图像字幕生成、检索和问答任务性能的影响。", "result": "研究发现，尽管ViT和LLM的参数量存在显著差异，但它们对模型性能的重要性相当。对LLM进行低比特量化可以在显著降低比特每权重（bpw）的情况下保持高准确率。", "conclusion": "研究结果为高效部署多模态大型语言模型（MLLMs）提供了实用见解，并强调了探索不同组件敏感性对于理解多模态模型的重要性。"}}
{"id": "2601.15284", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.15284", "abs": "https://arxiv.org/abs/2601.15284", "authors": ["Anurag Bagchi", "Zhipeng Bao", "Homanga Bharadhwaj", "Yu-Xiong Wang", "Pavel Tokmakov", "Martial Hebert"], "title": "Walk through Paintings: Egocentric World Models from Internet Priors", "comment": null, "summary": "What if a video generation model could not only imagine a plausible future, but the correct one, accurately reflecting how the world changes with each action? We address this question by presenting the Egocentric World Model (EgoWM), a simple, architecture-agnostic method that transforms any pretrained video diffusion model into an action-conditioned world model, enabling controllable future prediction. Rather than training from scratch, we repurpose the rich world priors of Internet-scale video models and inject motor commands through lightweight conditioning layers. This allows the model to follow actions faithfully while preserving realism and strong generalization. Our approach scales naturally across embodiments and action spaces, ranging from 3-DoF mobile robots to 25-DoF humanoids, where predicting egocentric joint-angle-driven dynamics is substantially more challenging. The model produces coherent rollouts for both navigation and manipulation tasks, requiring only modest fine-tuning. To evaluate physical correctness independently of visual appearance, we introduce the Structural Consistency Score (SCS), which measures whether stable scene elements evolve consistently with the provided actions. EgoWM improves SCS by up to 80 percent over prior state-of-the-art navigation world models, while achieving up to six times lower inference latency and robust generalization to unseen environments, including navigation inside paintings.", "AI": {"tldr": "提出了一种名为 Egocentric World Model (EgoWM) 的方法，可以将预训练的视频扩散模型转变为可控的动作条件世界模型，用于预测未来，并引入了结构一致性得分 (SCS) 来评估物理正确性。", "motivation": "旨在让视频生成模型不仅能想象一个可能发生的未来，而且能准确预测在给定动作下世界如何变化。", "method": "将预训练的视频扩散模型通过轻量级条件层注入运动指令，使其能够忠实遵循动作，同时保持真实性和泛化能力。该方法不进行从头训练，而是重新利用互联网规模视频模型的丰富世界先验。", "result": "EgoWM 在导航和操作任务中实现了连贯的预测，所需的微调很小。与现有的导航世界模型相比，EgoWM 将 SCS 提高了高达 80%，推理延迟降低了六倍，并且对未见过的环境（包括画作内部的导航）表现出鲁棒的泛化能力。", "conclusion": "EgoWM 是一种简单且通用的方法，能够将现有的视频生成模型转化为强大的动作条件世界模型，在预测物理动态方面表现出色，并能有效处理不同实体和动作空间。"}}
{"id": "2601.15260", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.15260", "abs": "https://arxiv.org/abs/2601.15260", "authors": ["Dominik Rößle", "Xujun Xie", "Adithya Mohan", "Venkatesh Thirugnana Sambandham", "Daniel Cremers", "Torsten Schön"], "title": "DrivIng: A Large-Scale Multimodal Driving Dataset with Full Digital Twin Integration", "comment": "Accepted to the IEEE Intelligent Vehicles Symposium 2026. For code and dataset, see https://github.com/cvims/DrivIng", "summary": "Perception is a cornerstone of autonomous driving, enabling vehicles to understand their surroundings and make safe, reliable decisions. Developing robust perception algorithms requires large-scale, high-quality datasets that cover diverse driving conditions and support thorough evaluation. Existing datasets often lack a high-fidelity digital twin, limiting systematic testing, edge-case simulation, sensor modification, and sim-to-real evaluations. To address this gap, we present DrivIng, a large-scale multimodal dataset with a complete geo-referenced digital twin of a ~18 km route spanning urban, suburban, and highway segments. Our dataset provides continuous recordings from six RGB cameras, one LiDAR, and high-precision ADMA-based localization, captured across day, dusk, and night. All sequences are annotated at 10 Hz with 3D bounding boxes and track IDs across 12 classes, yielding ~1.2 million annotated instances. Alongside the benefits of a digital twin, DrivIng enables a 1-to-1 transfer of real traffic into simulation, preserving agent interactions while enabling realistic and flexible scenario testing. To support reproducible research and robust validation, we benchmark DrivIng with state-of-the-art perception models and publicly release the dataset, digital twin, HD map, and codebase.", "AI": {"tldr": "本文介绍了一个名为DrivIng的大规模多模态数据集，该数据集包含一个高保真度、地理参考的数字孪生，能够支持自动驾驶的感知算法的鲁棒性开发和评估。", "motivation": "现有数据集缺乏高保真度的数字孪生，限制了对自动驾驶感知算法进行系统测试、边缘案例模拟、传感器修改和仿真到现实评估。", "method": "构建了一个包含约18公里长的城市、郊区和高速公路路段的数字孪生，并采集了在白天、黄昏和夜晚条件下，由六个RGB摄像头、一个LiDAR和高精度ADMA定位系统提供的数据。所有数据以10Hz的频率进行标注，提供12类对象的3D边界框和轨迹ID，总计约120万个标注实例。", "result": "DrivIng数据集提供了详细的数字孪生，实现了真实交通到仿真的1对1迁移，保留了代理交互，并支持现实灵活的场景测试。同时，研究者对该数据集进行了标杆测试，并公开发布了数据集、数字孪生、高清地图和代码库。", "conclusion": "DrivIng数据集及其数字孪生为自动驾驶感知算法的研发和验证提供了一个强大的新资源，有助于推动更安全、更可靠的自动驾驶系统。"}}
