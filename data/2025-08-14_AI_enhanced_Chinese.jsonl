{"id": "2508.09277", "categories": ["cs.AI", "cs.LG", "cs.LO"], "pdf": "https://arxiv.org/pdf/2508.09277", "abs": "https://arxiv.org/abs/2508.09277", "authors": ["Soumia Mehimeh"], "title": "Value Function Initialization for Knowledge Transfer and Jump-start in Deep Reinforcement Learning", "comment": null, "summary": "Value function initialization (VFI) is an effective way to achieve a\njumpstart in reinforcement learning (RL) by leveraging value estimates from\nprior tasks. While this approach is well established in tabular settings,\nextending it to deep reinforcement learning (DRL) poses challenges due to the\ncontinuous nature of the state-action space, the noisy approximations of neural\nnetworks, and the impracticality of storing all past models for reuse. In this\nwork, we address these challenges and introduce DQInit, a method that adapts\nvalue function initialization to DRL. DQInit reuses compact tabular Q-values\nextracted from previously solved tasks as a transferable knowledge base. It\nemploys a knownness-based mechanism to softly integrate these transferred\nvalues into underexplored regions and gradually shift toward the agent's\nlearned estimates, avoiding the limitations of fixed time decay. Our approach\noffers a novel perspective on knowledge transfer in DRL by relying solely on\nvalue estimates rather than policies or demonstrations, effectively combining\nthe strengths of jumpstart RL and policy distillation while mitigating their\ndrawbacks. Experiments across multiple continuous control tasks demonstrate\nthat DQInit consistently improves early learning efficiency, stability, and\noverall performance compared to standard initialization and existing transfer\ntechniques.", "AI": {"tldr": "DQInit是一种深度强化学习中的价值函数初始化方法，它通过重用紧凑的表格Q值作为可迁移知识，并采用基于已知性的机制来软性整合这些值，从而显著提高早期学习效率和稳定性。", "motivation": "价值函数初始化（VFI）在表格强化学习中能有效实现快速启动，但将其扩展到深度强化学习（DRL）面临挑战，包括连续状态-动作空间、神经网络的噪声近似以及存储所有过去模型的非实用性。", "method": "DQInit通过从先前解决的任务中提取紧凑的表格Q值作为可迁移知识库。它采用一种基于已知性的机制，将这些迁移值软性地整合到探索不足的区域，并逐步转向智能体学习到的估计值，避免了固定时间衰减的局限性。该方法仅依赖于价值估计而非策略或演示进行知识迁移。", "result": "在多个连续控制任务上的实验表明，与标准初始化和现有迁移技术相比，DQInit持续提高了早期学习效率、稳定性和整体性能。", "conclusion": "DQInit提供了一种新颖的深度强化学习知识迁移视角，仅依赖于价值估计，有效结合了快速启动强化学习和策略蒸馏的优点，同时减轻了它们的缺点。"}}
{"id": "2508.09292", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09292", "abs": "https://arxiv.org/abs/2508.09292", "authors": ["Sundong Kim"], "title": "The Othello AI Arena: Evaluating Intelligent Systems Through Limited-Time Adaptation to Unseen Boards", "comment": null, "summary": "The ability to rapidly adapt to novel and unforeseen environmental changes is\na cornerstone of artificial general intelligence (AGI), yet it remains a\ncritical blind spot in most existing AI benchmarks. Traditional evaluation\nlargely focuses on optimizing performance within fixed environments, failing to\nassess systems' flexibility and generalization capabilities when faced with\neven subtle rule or structural modifications. Addressing this gap, I introduce\nthe Othello AI Arena, a novel benchmark framework designed to evaluate\nintelligent systems based on their capacity for limited-time adaptation to\nunseen environments. Our platform poses a meta-learning challenge: participants\nmust develop systems that can analyze the specific configuration and rules of a\nnovel Othello board within a strict time limit (60 seconds) and generate a\ntailored, high-performing strategy for that unique environment. With this,\nevaluation of the meta-level intelligence can be separated from the task-level\nstrategy performance. The Arena features a diverse set of game stages,\nincluding public stages for development and private stages with structural and\nrule variations designed to test genuine adaptive and generalization\ncapabilities. Implemented as an accessible web-based platform, the Arena\nprovides real-time visualization, automated evaluation using multi-dimensional\nmetrics, and comprehensive logging for post-hoc analysis. Initial observations\nfrom pilot tests and preliminary student engagements highlight fascinating\npatterns in adaptation approaches, ranging from rapid parameter tuning to\nrudimentary environmental model learning through simulation. The Othello AI\nArena offers a unique educational tool and a valuable research benchmark for\nfostering and evaluating the crucial skill of rapid, intelligent adaptation in\nAI systems.", "AI": {"tldr": "本文介绍了一个名为“奥赛罗AI竞技场”的新型基准测试框架，旨在评估AI系统在有限时间内对未知环境的快速适应能力和泛化能力，以弥补现有AI基准的不足。", "motivation": "现有AI基准主要关注在固定环境下的性能优化，未能有效评估系统面对规则或结构变化时的灵活性和泛化能力。快速适应新环境是实现通用人工智能（AGI）的关键，但仍是AI评估中的一个盲点。", "method": "引入奥赛罗AI竞技场，一个基于元学习挑战的基准平台。参与者需在60秒内分析新的奥赛罗棋盘配置和规则，并生成定制的高性能策略。该平台包含多样化的公共和私有游戏阶段，后者用于测试真正的适应和泛化能力。平台提供实时可视化、自动化多维度评估和日志记录。", "result": "初步的试点测试和学生参与观察显示了有趣的适应模式，包括快速参数调整和通过模拟进行基本环境模型学习等方法。这表明了不同的适应策略。", "conclusion": "奥赛罗AI竞技场提供了一个独特的教育工具和有价值的研究基准，用于培养和评估AI系统快速、智能适应的关键能力。"}}
{"id": "2508.09507", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09507", "abs": "https://arxiv.org/abs/2508.09507", "authors": ["Meiping Wang", "Jian Zhong", "Rongduo Han", "Liming Kang", "Zhengkun Shi", "Xiao Liang", "Xing Lin", "Nan Gao", "Haining Zhang"], "title": "An Automated Multi-Modal Evaluation Framework for Mobile Intelligent Assistants", "comment": null, "summary": "With the rapid development of mobile intelligent assistant technologies,\nmulti-modal AI assistants have become essential interfaces for daily user\ninteractions. However, current evaluation methods face challenges including\nhigh manual costs, inconsistent standards, and subjective bias. This paper\nproposes an automated multi-modal evaluation framework based on large language\nmodels and multi-agent collaboration. The framework employs a three-tier agent\narchitecture consisting of interaction evaluation agents, semantic verification\nagents, and experience decision agents. Through supervised fine-tuning on the\nQwen3-8B model, we achieve a significant evaluation matching accuracy with\nhuman experts. Experimental results on eight major intelligent agents\ndemonstrate the framework's effectiveness in predicting users' satisfaction and\nidentifying generation defects.", "AI": {"tldr": "本文提出了一种基于大语言模型和多智能体协作的自动化多模态AI助手评估框架，有效解决了现有评估方法的成本高、标准不一和主观性问题。", "motivation": "当前的AI助手评估方法面临人工成本高昂、评估标准不一致以及主观偏差等挑战，需要一种更高效、客观的评估方案。", "method": "该研究提出一个三层智能体架构（交互评估、语义验证、体验决策智能体）的多模态评估框架，并基于Qwen3-8B模型进行监督微调，以实现与人类专家评估结果的高度匹配。", "result": "实验结果表明，该框架在与人类专家的评估匹配准确率上取得了显著提升，并且在八个主流智能体上的测试验证了其在预测用户满意度和识别生成缺陷方面的有效性。", "conclusion": "所提出的自动化多模态评估框架能够有效预测用户满意度并识别AI助手的生成缺陷，为多模态AI助手的评估提供了高效、客观的解决方案。"}}
{"id": "2508.09586", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09586", "abs": "https://arxiv.org/abs/2508.09586", "authors": ["Yang Cheng", "Zilai Wang", "Weiyu Ma", "Wenhui Zhu", "Yue Deng", "Jian Zhao"], "title": "EvoCurr: Self-evolving Curriculum with Behavior Code Generation for Complex Decision-making", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\ndiverse domains, including programming, planning, and decision-making. However,\ntheir performance often degrades when faced with highly complex problem\ninstances that require deep reasoning over long horizons. In such cases, direct\nproblem-solving approaches can lead to inefficiency or failure due to the lack\nof structured intermediate guidance. To address this, we propose a novel\nself-evolve framework, EvoCurr, in which a dedicated curriculum-generation LLM\nconstructs a sequence of problem instances with gradually increasing\ndifficulty, tailored to the solver LLM's learning progress. The curriculum\ndynamically adapts easing challenges when the solver struggles and escalating\nthem when success is consistent, thus maintaining an optimal learning\ntrajectory. This approach enables the solver LLM, implemented as a\ncode-generation model producing Python decision-tree scripts, to progressively\nacquire the skills needed for complex decision-making tasks. Experimental\nresults on challenging decision-making benchmarks show that our method\nsignificantly improves task success rates and solution efficiency compared to\ndirect-solving baselines. These findings suggest that LLM-driven curriculum\nlearning holds strong potential for enhancing automated reasoning in\nreal-world, high-complexity domains.", "AI": {"tldr": "EvoCurr是一个自进化框架，通过一个课程生成LLM为求解器LLM动态构建难度渐增的问题序列，以提高求解器LLM在复杂决策任务中的表现。", "motivation": "大型语言模型（LLMs）在面对需要深度推理和长远规划的复杂问题时，性能会下降，因为缺乏结构化的中间指导，导致效率低下或失败。", "method": "提出EvoCurr框架，其中一个专门的课程生成LLM根据求解器LLM的学习进度，构建一系列难度逐渐增加的问题实例。课程会根据求解器的表现动态调整难度（当求解器遇到困难时降低难度，成功时提高难度），以维持最佳学习轨迹。求解器LLM被实现为一个生成Python决策树脚本的代码生成模型。", "result": "在挑战性决策基准测试中，与直接求解基线相比，EvoCurr显著提高了任务成功率和解决方案效率。", "conclusion": "LLM驱动的课程学习在增强现实世界高复杂性领域的自动化推理方面具有巨大潜力。"}}
{"id": "2508.09177", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09177", "abs": "https://arxiv.org/abs/2508.09177", "authors": ["Xuanru Zhou", "Cheng Li", "Shuqiang Wang", "Ye Li", "Tao Tan", "Hairong Zheng", "Shanshan Wang"], "title": "Generative Artificial Intelligence in Medical Imaging: Foundations, Progress, and Clinical Translation", "comment": null, "summary": "Generative artificial intelligence (AI) is rapidly transforming medical\nimaging by enabling capabilities such as data synthesis, image enhancement,\nmodality translation, and spatiotemporal modeling. This review presents a\ncomprehensive and forward-looking synthesis of recent advances in generative\nmodeling including generative adversarial networks (GANs), variational\nautoencoders (VAEs), diffusion models, and emerging multimodal foundation\narchitectures and evaluates their expanding roles across the clinical imaging\ncontinuum. We systematically examine how generative AI contributes to key\nstages of the imaging workflow, from acquisition and reconstruction to\ncross-modality synthesis, diagnostic support, and treatment planning. Emphasis\nis placed on both retrospective and prospective clinical scenarios, where\ngenerative models help address longstanding challenges such as data scarcity,\nstandardization, and integration across modalities. To promote rigorous\nbenchmarking and translational readiness, we propose a three-tiered evaluation\nframework encompassing pixel-level fidelity, feature-level realism, and\ntask-level clinical relevance. We also identify critical obstacles to\nreal-world deployment, including generalization under domain shift,\nhallucination risk, data privacy concerns, and regulatory hurdles. Finally, we\nexplore the convergence of generative AI with large-scale foundation models,\nhighlighting how this synergy may enable the next generation of scalable,\nreliable, and clinically integrated imaging systems. By charting technical\nprogress and translational pathways, this review aims to guide future research\nand foster interdisciplinary collaboration at the intersection of AI, medicine,\nand biomedical engineering.", "AI": {"tldr": "这篇综述探讨了生成式AI在医学成像领域的最新进展及其在临床工作流中的应用，并提出了评估框架和未来展望。", "motivation": "生成式AI正在快速改变医学成像，通过数据合成、图像增强、模态转换等能力解决长期存在的挑战，如数据稀缺、标准化和跨模态整合，从而推动医疗影像技术的发展。", "method": "该研究综述了生成对抗网络（GANs）、变分自编码器（VAEs）、扩散模型和多模态基础架构等生成模型。系统性地考察了生成式AI在成像工作流（从采集、重建到诊断支持和治疗规划）中的贡献。为评估其临床转化，提出了一个三层评估框架，包括像素级保真度、特征级真实性和任务级临床相关性。", "result": "生成式AI在医学成像的各个阶段都发挥着越来越重要的作用，有助于解决数据稀缺、标准化和多模态整合等挑战。综述提出了一个分层评估框架以促进严格的基准测试和转化准备。同时，也指出了实际部署面临的关键障碍，如领域漂移下的泛化能力、幻觉风险、数据隐私和监管障碍。", "conclusion": "生成式AI与大规模基础模型的融合有望催生下一代可扩展、可靠且临床集成的成像系统。本综述旨在指导未来研究，促进AI、医学和生物医学工程领域的跨学科合作。"}}
{"id": "2508.09304", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.09304", "abs": "https://arxiv.org/abs/2508.09304", "authors": ["Kelen C. Teixeira Vivaldini", "Robert Pěnička", "Martin Saska"], "title": "Decision-Making-Based Path Planning for Autonomous UAVs: A Survey", "comment": null, "summary": "One of the most critical features for the successful operation of autonomous\nUAVs is the ability to make decisions based on the information acquired from\ntheir surroundings. Each UAV must be able to make decisions during the flight\nin order to deal with uncertainties in its system and the environment, and to\nfurther act upon the information being received. Such decisions influence the\nfuture behavior of the UAV, which is expressed as the path plan. Thus,\ndecision-making in path planning is an enabling technique for deploying\nautonomous UAVs in real-world applications. This survey provides an overview of\nexisting studies that use aspects of decision-making in path planning,\npresenting the research strands for Exploration Path Planning and Informative\nPath Planning, and focusing on characteristics of how data have been modeled\nand understood. Finally, we highlight the existing challenges for relevant\ntopics in this field.", "AI": {"tldr": "该综述文章概述了无人机路径规划中决策制定方面的现有研究，重点介绍了探索路径规划和信息路径规划，并强调了数据建模的特点及该领域的现有挑战。", "motivation": "自主无人机成功运行的关键在于能够根据环境信息做出决策。无人机在飞行中必须能够处理系统和环境的不确定性，并根据接收到的信息采取行动。这些决策影响无人机的未来行为，即路径规划，因此决策制定是部署自主无人机在实际应用中的关键技术。", "method": "本文通过综述现有研究，分析了路径规划中决策制定的各个方面，主要分为探索路径规划（Exploration Path Planning）和信息路径规划（Informative Path Planning）两大研究方向，并特别关注了数据如何被建模和理解的特点。", "result": "文章提供了现有研究的概览，展示了探索路径规划和信息路径规划两个主要研究方向，并指出了该领域相关主题的现有挑战。", "conclusion": "决策制定是实现自主无人机路径规划的关键使能技术，尽管已有大量研究，但在数据建模和实际应用中仍面临诸多挑战。"}}
{"id": "2508.09419", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.09419", "abs": "https://arxiv.org/abs/2508.09419", "authors": ["Justin London"], "title": "Design and Simulation of 6T SRAM Array", "comment": null, "summary": "Conventional 6T SRAM is used in microprocessors in the cache memory design.\nThe basic 6T SRAM cell and a 6 bit memory array layout are designed in LEdit.\nThe design and analysis of key SRAM components, sense amplifiers, decoders,\nwrite drivers and precharge circuits are also provided. The pulse voltage\nwaveforms generated for read and write operations as well as Q and Qbar nodes\nare simulated in LTSpice. Parasitic capacitances are extracted and their impact\non the waveforms analyzed. Static noise margin, propagation delays, and power\ndissipation are calculated. Comparison of SRAM read and write operational\nperformance using CMOS transistors is made with edge-triggered D flip flops. If\ncertain size area and ratio constraints are satisfied, the 6T cell with CMOS\ntransistors will possess stability, speed, and power efficiency. Both\ntheoretical and simulated results are given.", "AI": {"tldr": "该研究设计并分析了6T SRAM单元及其关键组件，使用LEdit和LTSpice进行仿真，评估了寄生效应、噪声容限、延迟和功耗，并与D触发器进行了性能比较。", "motivation": "传统的6T SRAM被广泛用于微处理器的缓存设计中，因此需要对其设计和性能进行深入分析和优化。", "method": "使用LEdit设计了基本的6T SRAM单元和6位存储器阵列，并设计了感测放大器、解码器、写入驱动器和预充电电路等关键组件。使用LTSpice仿真读写操作的电压波形，提取寄生电容并分析其影响。计算了静态噪声容限（SNM）、传播延迟和功耗。将SRAM的读写操作性能与边沿触发D触发器进行了比较。", "result": "通过仿真得到了读写操作的波形，分析了寄生电容对波形的影响。计算了静态噪声容限、传播延迟和功耗。结果表明，在满足特定尺寸面积和比例限制的情况下，基于CMOS晶体管的6T SRAM单元具有稳定性、速度和功率效率。提供了理论和仿真结果。", "conclusion": "研究得出结论，如果满足特定的尺寸面积和比例约束，使用CMOS晶体管的6T SRAM单元能够实现良好的稳定性、速度和功率效率。"}}
{"id": "2508.09303", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.09303", "abs": "https://arxiv.org/abs/2508.09303", "authors": ["Shu Zhao", "Tan Yu", "Anbang Xu", "Japinder Singh", "Aaditya Shukla", "Rama Akkiraju"], "title": "ParallelSearch: Train your LLMs to Decompose Query and Search Sub-queries in Parallel with Reinforcement Learning", "comment": null, "summary": "Reasoning-augmented search agents such as Search-R1, trained via\nreinforcement learning with verifiable rewards (RLVR), demonstrate remarkable\ncapabilities in multi-step information retrieval from external knowledge\nsources. These agents address the limitations of their parametric memory by\ndynamically gathering relevant facts to address complex reasoning tasks.\nHowever, existing approaches suffer from a fundamental architectural\nlimitation: they process search queries strictly sequentially, even when\nhandling inherently parallelizable and logically independent comparisons. This\nsequential bottleneck significantly constrains computational efficiency,\nparticularly for queries that require multiple entity comparisons. To address\nthis critical limitation, we propose ParallelSearch, a novel reinforcement\nlearning framework that empowers large language models (LLMs) to recognize\nparallelizable query structures and execute multiple search operations\nconcurrently. Our approach introduces dedicated reward functions that\nincentivize the identification of independent query components while preserving\nanswer accuracy through jointly considering correctness, query decomposition\nquality, and parallel execution benefits. Comprehensive experiments demonstrate\nthat ParallelSearch outperforms state-of-the-art baselines by an average\nperformance gain of 2.9% across seven question-answering benchmarks. Notably,\non parallelizable questions, our method achieves a 12.7% performance\nimprovement while requiring only 69.6% of the LLM calls compared to sequential\napproaches.", "AI": {"tldr": "本文提出ParallelSearch框架，通过强化学习使大型语言模型（LLMs）能够识别并行化查询结构并并发执行搜索操作，解决了现有推理增强搜索代理的顺序执行瓶颈，显著提高了性能和效率。", "motivation": "现有的推理增强搜索代理（如Search-R1）在处理多步信息检索时，即使面对本质上可并行且逻辑独立的比较，也严格按顺序处理搜索查询，这导致计算效率低下，尤其对于需要多实体比较的查询。", "method": "ParallelSearch是一个新颖的强化学习框架，它赋予LLMs识别可并行查询结构并同时执行多个搜索操作的能力。该方法引入了专门的奖励函数，以激励识别独立的查询组件，并通过联合考虑答案正确性、查询分解质量和并行执行效益来保持答案准确性。", "result": "ParallelSearch在七个问答基准测试中，平均性能比现有最先进的基线提高了2.9%。值得注意的是，在可并行化问题上，该方法实现了12.7%的性能提升，同时LLM调用次数仅为顺序方法的69.6%。", "conclusion": "ParallelSearch通过支持并行搜索操作，有效解决了推理增强搜索代理中的顺序执行瓶颈，显著提升了复杂推理任务的性能和计算效率，特别是在处理可并行化查询时表现出卓越优势。"}}
{"id": "2508.09175", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09175", "abs": "https://arxiv.org/abs/2508.09175", "authors": ["Mohammad Zia Ur Rehman", "Sufyaan Zahoor", "Areeb Manzoor", "Musharaf Maqbool", "Nagendra Kumar"], "title": "A Context-aware Attention and Graph Neural Network-based Multimodal Framework for Misogyny Detection", "comment": "Published in Information Processing & Management", "summary": "A substantial portion of offensive content on social media is directed\ntowards women. Since the approaches for general offensive content detection\nface a challenge in detecting misogynistic content, it requires solutions\ntailored to address offensive content against women. To this end, we propose a\nnovel multimodal framework for the detection of misogynistic and sexist\ncontent. The framework comprises three modules: the Multimodal Attention module\n(MANM), the Graph-based Feature Reconstruction Module (GFRM), and the\nContent-specific Features Learning Module (CFLM). The MANM employs adaptive\ngating-based multimodal context-aware attention, enabling the model to focus on\nrelevant visual and textual information and generating contextually relevant\nfeatures. The GFRM module utilizes graphs to refine features within individual\nmodalities, while the CFLM focuses on learning text and image-specific features\nsuch as toxicity features and caption features. Additionally, we curate a set\nof misogynous lexicons to compute the misogyny-specific lexicon score from the\ntext. We apply test-time augmentation in feature space to better generalize the\npredictions on diverse inputs. The performance of the proposed approach has\nbeen evaluated on two multimodal datasets, MAMI and MMHS150K, with 11,000 and\n13,494 samples, respectively. The proposed method demonstrates an average\nimprovement of 10.17% and 8.88% in macro-F1 over existing methods on the MAMI\nand MMHS150K datasets, respectively.", "AI": {"tldr": "本文提出了一种新颖的多模态框架，用于检测社交媒体中的厌女和性别歧视内容，该框架通过多模态注意力、图基特征重构和内容特定特征学习模块，显著提升了检测性能。", "motivation": "社交媒体上大量攻击性内容针对女性，而现有通用攻击性内容检测方法在识别厌女内容方面存在挑战，因此需要专门针对女性的攻击性内容检测解决方案。", "method": "本文提出一个多模态框架，包含三个模块：多模态注意力模块（MANM）用于上下文感知特征生成，图基特征重构模块（GFRM）用于模态内特征细化，以及内容特定特征学习模块（CFLM）用于学习文本和图像特异性特征（如毒性特征和标题特征）。此外，还整理了厌女词典以计算厌女特定词汇分数，并在特征空间应用测试时增强以提高泛化能力。", "result": "该方法在两个多模态数据集MAMI和MMHS150K上进行了评估，与现有方法相比，宏观F1分数分别平均提升了10.17%和8.88%。", "conclusion": "所提出的多模态框架能有效检测厌女和性别歧视内容，并在性能上显著优于现有方法，为解决针对女性的攻击性内容检测问题提供了有效方案。"}}
{"id": "2508.09639", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09639", "abs": "https://arxiv.org/abs/2508.09639", "authors": ["Akshat Dubey", "Aleksandar Anžel", "Bahar İlgen", "Georges Hattab"], "title": "UbiQTree: Uncertainty Quantification in XAI with Tree Ensembles", "comment": null, "summary": "Explainable Artificial Intelligence (XAI) techniques, such as SHapley\nAdditive exPlanations (SHAP), have become essential tools for interpreting\ncomplex ensemble tree-based models, especially in high-stakes domains such as\nhealthcare analytics. However, SHAP values are usually treated as point\nestimates, which disregards the inherent and ubiquitous uncertainty in\npredictive models and data. This uncertainty has two primary sources: aleatoric\nand epistemic. The aleatoric uncertainty, which reflects the irreducible noise\nin the data. The epistemic uncertainty, which arises from a lack of data. In\nthis work, we propose an approach for decomposing uncertainty in SHAP values\ninto aleatoric, epistemic, and entanglement components. This approach\nintegrates Dempster-Shafer evidence theory and hypothesis sampling via\nDirichlet processes over tree ensembles. We validate the method across three\nreal-world use cases with descriptive statistical analyses that provide insight\ninto the nature of epistemic uncertainty embedded in SHAP explanations. The\nexperimentations enable to provide more comprehensive understanding of the\nreliability and interpretability of SHAP-based attributions. This understanding\ncan guide the development of robust decision-making processes and the\nrefinement of models in high-stakes applications. Through our experiments with\nmultiple datasets, we concluded that features with the highest SHAP values are\nnot necessarily the most stable. This epistemic uncertainty can be reduced\nthrough better, more representative data and following appropriate or\ncase-desired model development techniques. Tree-based models, especially\nbagging, facilitate the effective quantification of epistemic uncertainty.", "AI": {"tldr": "本研究提出了一种分解SHAP值中不确定性（包括偶然不确定性、认知不确定性和纠缠不确定性）的方法，以提高SHAP解释的可靠性和可解释性。", "motivation": "SHAP值通常被视为点估计，忽略了预测模型和数据中固有的不确定性（偶然不确定性和认知不确定性）。在医疗分析等高风险领域，理解这种不确定性对于SHAP解释的可靠性至关重要。", "method": "该方法通过集成Dempster-Shafer证据理论和基于Dirichlet过程的树集成假设采样，将SHAP值中的不确定性分解为偶然不确定性、认知不确定性和纠缠不确定性。", "result": "该方法在三个真实世界用例中得到验证，提供了对SHAP解释中认知不确定性本质的深入理解。实验表明，SHAP值最高的特征不一定最稳定，且认知不确定性可通过更好的数据和适当的模型开发技术来降低。树基模型（特别是bagging）能有效量化认知不确定性。", "conclusion": "理解SHAP归因中的不确定性可以指导鲁棒决策过程的开发和高风险应用中模型的改进。认知不确定性可以通过更好的数据和适当的模型开发技术来减少。"}}
{"id": "2508.09179", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09179", "abs": "https://arxiv.org/abs/2508.09179", "authors": ["Hongli Chen", "Pengcheng Fang", "Yuxia Chen", "Yingxuan Ren", "Jing Hao", "Fangfang Tang", "Xiaohao Cai", "Shanshan Shan", "Feng Liu"], "title": "HiFi-Mamba: Dual-Stream W-Laplacian Enhanced Mamba for High-Fidelity MRI Reconstruction", "comment": null, "summary": "Reconstructing high-fidelity MR images from undersampled k-space data remains\na challenging problem in MRI. While Mamba variants for vision tasks offer\npromising long-range modeling capabilities with linear-time complexity, their\ndirect application to MRI reconstruction inherits two key limitations: (1)\ninsensitivity to high-frequency anatomical details; and (2) reliance on\nredundant multi-directional scanning. To address these limitations, we\nintroduce High-Fidelity Mamba (HiFi-Mamba), a novel dual-stream Mamba-based\narchitecture comprising stacked W-Laplacian (WL) and HiFi-Mamba blocks.\nSpecifically, the WL block performs fidelity-preserving spectral decoupling,\nproducing complementary low- and high-frequency streams. This separation\nenables the HiFi-Mamba block to focus on low-frequency structures, enhancing\nglobal feature modeling. Concurrently, the HiFi-Mamba block selectively\nintegrates high-frequency features through adaptive state-space modulation,\npreserving comprehensive spectral details. To eliminate the scanning\nredundancy, the HiFi-Mamba block adopts a streamlined unidirectional traversal\nstrategy that preserves long-range modeling capability with improved\ncomputational efficiency. Extensive experiments on standard MRI reconstruction\nbenchmarks demonstrate that HiFi-Mamba consistently outperforms\nstate-of-the-art CNN-based, Transformer-based, and other Mamba-based models in\nreconstruction accuracy while maintaining a compact and efficient model design.", "AI": {"tldr": "本文提出了一种名为HiFi-Mamba的新型双流Mamba架构，用于从欠采样k空间数据重建高保真MR图像。该模型通过频谱解耦和自适应状态空间调制来解决现有Mamba变体对高频细节不敏感及扫描冗余的问题，并在多个MRI重建基准上超越了现有SOTA模型，同时保持了紧凑高效的设计。", "motivation": "从欠采样k空间数据重建高保真MR图像是一个具有挑战性的问题。尽管Mamba变体在视觉任务中展现出强大的长程建模能力和线性时间复杂度，但它们直接应用于MRI重建时存在两大局限性：1) 对高频解剖细节不敏感；2) 依赖冗余的多方向扫描。", "method": "本文引入了高保真Mamba（HiFi-Mamba），这是一种新颖的双流Mamba架构，由堆叠的W-拉普拉斯（WL）块和HiFi-Mamba块组成。WL块执行保真度保持的频谱解耦，生成互补的低频和高频流。HiFi-Mamba块专注于低频结构以增强全局特征建模，同时通过自适应状态空间调制选择性地整合高频特征，以保留全面的频谱细节。为消除扫描冗余，HiFi-Mamba块采用简化的单向遍历策略，在提高计算效率的同时保持长程建模能力。", "result": "在标准MRI重建基准上的广泛实验表明，HiFi-Mamba在重建精度方面始终优于现有的基于CNN、基于Transformer和其他基于Mamba的模型，同时保持了紧凑高效的模型设计。", "conclusion": "HiFi-Mamba是一种有效且高效的MRI重建模型，它成功解决了现有Mamba变体在处理高频细节和扫描冗余方面的局限性，并在性能上超越了多种SOTA方法。"}}
{"id": "2508.09346", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.09346", "abs": "https://arxiv.org/abs/2508.09346", "authors": ["Zhenjiang Mao", "Mrinall Eashaan Umasudhan", "Ivan Ruchkin"], "title": "How Safe Will I Be Given What I Saw? Calibrated Prediction of Safety Chances for Image-Controlled Autonomy", "comment": null, "summary": "Autonomous robots that rely on deep neural network controllers pose critical\nchallenges for safety prediction, especially under partial observability and\ndistribution shift. Traditional model-based verification techniques are limited\nin scalability and require access to low-dimensional state models, while\nmodel-free methods often lack reliability guarantees. This paper addresses\nthese limitations by introducing a framework for calibrated safety prediction\nin end-to-end vision-controlled systems, where neither the state-transition\nmodel nor the observation model is accessible. Building on the foundation of\nworld models, we leverage variational autoencoders and recurrent predictors to\nforecast future latent trajectories from raw image sequences and estimate the\nprobability of satisfying safety properties. We distinguish between monolithic\nand composite prediction pipelines and introduce a calibration mechanism to\nquantify prediction confidence. In long-horizon predictions from\nhigh-dimensional observations, the forecasted inputs to the safety evaluator\ncan deviate significantly from the training distribution due to compounding\nprediction errors and changing environmental conditions, leading to\nmiscalibrated risk estimates. To address this, we incorporate unsupervised\ndomain adaptation to ensure robustness of safety evaluation under distribution\nshift in predictions without requiring manual labels. Our formulation provides\ntheoretical calibration guarantees and supports practical evaluation across\nlong prediction horizons. Experimental results on three benchmarks show that\nour UDA-equipped evaluators maintain high accuracy and substantially lower\nfalse positive rates under distribution shift. Similarly, world model-based\ncomposite predictors outperform their monolithic counterparts on long-horizon\ntasks, and our conformal calibration provides reliable statistical bounds.", "AI": {"tldr": "该论文提出了一种基于世界模型和无监督域适应（UDA）的框架，用于在端到端视觉控制系统中进行校准安全预测，解决了深度神经网络控制器在部分可观测性和分布偏移下的安全预测挑战。", "motivation": "自动驾驶机器人依赖深度神经网络控制器，其安全预测面临严峻挑战，尤其是在部分可观测性和分布偏移下。传统模型验证技术受限于可扩展性且需低维状态模型，而无模型方法缺乏可靠性保证。当前缺乏在状态转换模型和观测模型均不可访问的情况下，对端到端视觉控制系统进行可靠安全预测的方法。", "method": "该研究基于世界模型，利用变分自编码器（VAE）和循环预测器从原始图像序列中预测未来潜在轨迹，并估计满足安全属性的概率。论文区分了单体和复合预测管道，并引入校准机制以量化预测置信度。为解决预测误差累积和环境变化导致的分布偏移，该方法融入无监督域适应（UDA），确保安全评估在预测分布偏移下的鲁棒性。", "result": "实验结果表明，配备UDA的评估器在分布偏移下能保持高精度并显著降低误报率。基于世界模型的复合预测器在长时程任务上优于单体预测器。此外，所提出的共形校准方法提供了可靠的统计边界。", "conclusion": "本研究提出的框架为视觉控制系统提供了理论上校准且在实践中有效的安全预测，尤其在长预测时程和分布偏移的挑战下表现出色，其UDA和世界模型结合的方法显著提高了预测的准确性和可靠性。"}}
{"id": "2508.09420", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.09420", "abs": "https://arxiv.org/abs/2508.09420", "authors": ["Justin London"], "title": "Control Systems Analysis of a 3-Axis Photovoltatic Solar Tracker for Water Pumping", "comment": null, "summary": "We propose 3-axis solar tracker water pumping system. The solar tracker can\nrotate and tilt using stepper/DC motors and can rise and lower on a tripod\nusing a linear actuator. The charge generated from solar energy absorbed by\nphotovoltaic (PV) cells in the solar panel is stored in a 12V battery that in\nturn powers two water diaphragm pumps using a solar charge controller. The PV\nuses four light photocell resistors/sensors to measure light intensity. A solar\ntracking algorithm determines the optimal angle for PV positioning. Using an\nultrasonic sensor to measure the water level in a reservoir water tank, water\nis pumped from one water tank to the reservoir. Based on soil moisture sensor\nlevels, a second water pump supplies water from the reservoir to the plant. The\nsystem is analyzed from a control systems perspective. The transfer functions,\nroot loci, and Bode plots are generated and simulated and experimental results\nare provided as well as stability and steady-state error analysis.", "AI": {"tldr": "本文提出并分析了一种三轴太阳能跟踪水泵系统，该系统利用太阳能为水泵供电，并通过传感器自动控制水从水源到水库再到植物的输送，并进行了控制系统分析。", "motivation": "提高太阳能利用效率以实现自动化水泵灌溉，特别是在水资源管理和能源效率方面。", "method": "系统包括三轴太阳能跟踪器（使用步进/直流电机和线性执行器）、光伏电池、12V电池和太阳能充电控制器。利用光敏电阻测量光照强度以优化光伏板角度，超声波传感器测量水库水位，土壤湿度传感器测量植物需水量。系统使用两个水隔膜泵，并通过控制系统方法（传递函数、根轨迹、伯德图、稳定性、稳态误差分析）进行设计和验证。", "result": "系统实现了水从一个水箱到水库的泵送，并根据土壤湿度将水从水库泵送到植物。提供了控制系统的仿真和实验结果，包括稳定性分析和稳态误差分析。", "conclusion": "所提出的三轴太阳能跟踪水泵系统设计有效，并从控制系统角度进行了全面分析，证明了其功能性、稳定性和性能。"}}
{"id": "2508.09323", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09323", "abs": "https://arxiv.org/abs/2508.09323", "authors": ["Nan Miles Xi", "Yu Deng", "Lin Wang"], "title": "Leveraging Large Language Models for Rare Disease Named Entity Recognition", "comment": null, "summary": "Named Entity Recognition (NER) in the rare disease domain poses unique\nchallenges due to limited labeled data, semantic ambiguity between entity\ntypes, and long-tail distributions. In this study, we evaluate the capabilities\nof GPT-4o for rare disease NER under low-resource settings, using a range of\nprompt-based strategies including zero-shot prompting, few-shot in-context\nlearning, retrieval-augmented generation (RAG), and task-level fine-tuning. We\ndesign a structured prompting framework that encodes domain-specific knowledge\nand disambiguation rules for four entity types. We further introduce two\nsemantically guided few-shot example selection methods to improve in-context\nperformance while reducing labeling effort. Experiments on the RareDis Corpus\nshow that GPT-4o achieves competitive or superior performance compared to\nBioClinicalBERT, with task-level fine-tuning yielding new state-of-the-art\n(SOTA) results. Cost-performance analysis reveals that few-shot prompting\ndelivers high returns at low token budgets, while RAG offers marginal\nadditional benefit. An error taxonomy highlights common failure modes such as\nboundary drift and type confusion, suggesting opportunities for post-processing\nand hybrid refinement. Our results demonstrate that prompt-optimized LLMs can\nserve as effective, scalable alternatives to traditional supervised models in\nbiomedical NER, particularly in rare disease applications where annotated data\nis scarce.", "AI": {"tldr": "本研究评估了GPT-4o在低资源环境下罕见病命名实体识别（NER）的能力，通过多种提示策略和微调实现了竞争性甚至最先进的性能，证明LLM是传统监督模型的有效替代。", "motivation": "罕见病领域的命名实体识别面临标注数据稀缺、实体类型语义模糊和长尾分布等独特挑战。", "method": "研究采用了多种基于提示的策略，包括零样本提示、少样本上下文学习、检索增强生成（RAG）和任务级微调。设计了一个编码领域知识和消歧规则的结构化提示框架，并引入了两种语义引导的少样本示例选择方法。", "result": "在RareDis语料库上的实验表明，GPT-4o的性能与BioClinicalBERT相当或更优，其中任务级微调取得了新的最先进（SOTA）结果。成本-性能分析显示，少样本提示在低预算下回报高，而RAG的额外收益有限。错误分类揭示了边界漂移和类型混淆等常见失败模式。", "conclusion": "优化提示的大型语言模型可以作为生物医学NER中传统监督模型的有效、可扩展替代方案，尤其适用于标注数据稀缺的罕见病应用。"}}
{"id": "2508.09178", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09178", "abs": "https://arxiv.org/abs/2508.09178", "authors": ["Yanhui Li", "Yunkang Cao", "Chengliang Liu", "Yuan Xiong", "Xinghui Dong", "Chao Huang"], "title": "IAD-R1: Reinforcing Consistent Reasoning in Industrial Anomaly Detection", "comment": null, "summary": "Industrial anomaly detection is a critical component of modern manufacturing,\nyet the scarcity of defective samples restricts traditional detection methods\nto scenario-specific applications. Although Vision-Language Models (VLMs)\ndemonstrate significant advantages in generalization capabilities, their\nperformance in industrial anomaly detection remains limited. To address this\nchallenge, we propose IAD-R1, a universal post-training framework applicable to\nVLMs of different architectures and parameter scales, which substantially\nenhances their anomaly detection capabilities. IAD-R1 employs a two-stage\ntraining strategy: the Perception Activation Supervised Fine-Tuning (PA-SFT)\nstage utilizes a meticulously constructed high-quality Chain-of-Thought dataset\n(Expert-AD) for training, enhancing anomaly perception capabilities and\nestablishing reasoning-to-answer correlations; the Structured Control Group\nRelative Policy Optimization (SC-GRPO) stage employs carefully designed reward\nfunctions to achieve a capability leap from \"Anomaly Perception\" to \"Anomaly\nInterpretation\". Experimental results demonstrate that IAD-R1 achieves\nsignificant improvements across 7 VLMs, attaining up to 43.3% enhancement in\naverage accuracy on 6 industrial anomaly detection benchmark datasets. Notably,\nthe 0.5B parameter model trained with IAD-R1 surpasses commercial models\nincluding GPT-4.1 and Claude-Sonnet-4 in zero-shot settings, demonstrating the\neffectiveness and superiority of IAD-R1. The dataset, code, and all model\nweights will be publicly available at https://github.com/Yanhui-Lee/IAD-R1.", "AI": {"tldr": "IAD-R1是一种通用的VLM后训练框架，通过两阶段训练显著提升了工业异常检测能力，甚至使小参数模型超越了商业大模型。", "motivation": "传统工业异常检测方法受限于缺陷样本稀缺性；视觉-语言模型（VLMs）虽具泛化能力，但在工业异常检测中的表现仍有限。", "method": "IAD-R1采用两阶段训练策略：1. 感知激活监督微调（PA-SFT）阶段，利用高质量思维链数据集（Expert-AD）增强异常感知能力并建立推理-回答关联；2. 结构化对照组相对策略优化（SC-GRPO）阶段，通过精心设计的奖励函数实现从“异常感知”到“异常解释”的能力飞跃。", "result": "IAD-R1在7个VLM上实现了显著改进，在6个工业异常检测基准数据集上平均准确率提升高达43.3%。经过IAD-R1训练的0.5B参数模型在零样本设置下超越了GPT-4.1和Claude-Sonnet-4等商业模型。", "conclusion": "IAD-R1框架有效且卓越地提升了不同架构和参数规模VLM的工业异常检测能力，尤其在零样本场景下表现出色，证明了其优越性。"}}
{"id": "2508.09670", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09670", "abs": "https://arxiv.org/abs/2508.09670", "authors": ["Weitao Jia", "Jinghui Lu", "Haiyang Yu", "Siqi Wang", "Guozhi Tang", "An-Lan Wang", "Weijie Yin", "Dingkang Yang", "Yuxiang Nie", "Bin Shan", "Hao Feng", "Irene Li", "Kun Yang", "Han Wang", "Jingqun Tang", "Teng Fu", "Changhong Jin", "Chao Feng", "Xiaohui Lv", "Can Huang"], "title": "MEML-GRPO: Heterogeneous Multi-Expert Mutual Learning for RLVR Advancement", "comment": null, "summary": "Recent advances demonstrate that reinforcement learning with verifiable\nrewards (RLVR) significantly enhances the reasoning capabilities of large\nlanguage models (LLMs). However, standard RLVR faces challenges with reward\nsparsity, where zero rewards from consistently incorrect candidate answers\nprovide no learning signal, particularly in challenging tasks. To address this,\nwe propose Multi-Expert Mutual Learning GRPO (MEML-GRPO), an innovative\nframework that utilizes diverse expert prompts as system prompts to generate a\nbroader range of responses, substantially increasing the likelihood of\nidentifying correct solutions. Additionally, we introduce an inter-expert\nmutual learning mechanism that facilitates knowledge sharing and transfer among\nexperts, further boosting the model's performance through RLVR. Extensive\nexperiments across multiple reasoning benchmarks show that MEML-GRPO delivers\nsignificant improvements, achieving an average performance gain of 4.89% with\nQwen and 11.33% with Llama, effectively overcoming the core limitations of\ntraditional RLVR methods.", "AI": {"tldr": "该论文提出了MEML-GRPO框架，通过多专家提示和专家间互学习机制，解决了可验证奖励强化学习（RLVR）中奖励稀疏性问题，显著提升了大型语言模型（LLMs）在推理任务上的性能。", "motivation": "标准的可验证奖励强化学习（RLVR）在奖励稀疏性方面面临挑战，尤其是在困难任务中，一致的错误答案导致零奖励，无法提供有效的学习信号。", "method": "提出了Multi-Expert Mutual Learning GRPO (MEML-GRPO) 框架。该框架利用多样化的专家提示作为系统提示，生成更广泛的候选回答，增加找到正确解的可能性。此外，引入了专家间互学习机制，促进知识共享和迁移，通过RLVR进一步提升模型性能。", "result": "在多个推理基准测试中，MEML-GRPO取得了显著改进，与Qwen模型结合平均性能提升4.89%，与Llama模型结合平均性能提升11.33%。", "conclusion": "MEML-GRPO框架有效克服了传统RLVR方法的核心局限性，显著增强了LLMs的推理能力。"}}
{"id": "2508.09182", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09182", "abs": "https://arxiv.org/abs/2508.09182", "authors": ["Baraa Al Jorf", "Farah Shamout"], "title": "MedPatch: Confidence-Guided Multi-Stage Fusion for Multimodal Clinical Data", "comment": null, "summary": "Clinical decision-making relies on the integration of information across\nvarious data modalities, such as clinical time-series, medical images and\ntextual reports. Compared to other domains, real-world medical data is\nheterogeneous in nature, limited in size, and sparse due to missing modalities.\nThis significantly limits model performance in clinical prediction tasks.\nInspired by clinical workflows, we introduce MedPatch, a multi-stage multimodal\nfusion architecture, which seamlessly integrates multiple modalities via\nconfidence-guided patching. MedPatch comprises three main components: (i) a\nmulti-stage fusion strategy that leverages joint and late fusion\nsimultaneously, (ii) a missingness-aware module that handles sparse samples\nwith missing modalities, (iii) a joint fusion module that clusters latent token\npatches based on calibrated unimodal token-level confidence. We evaluated\nMedPatch using real-world data consisting of clinical time-series data, chest\nX-ray images, radiology reports, and discharge notes extracted from the\nMIMIC-IV, MIMIC-CXR, and MIMIC-Notes datasets on two benchmark tasks, namely\nin-hospital mortality prediction and clinical condition classification.\nCompared to existing baselines, MedPatch achieves state-of-the-art performance.\nOur work highlights the effectiveness of confidence-guided multi-stage fusion\nin addressing the heterogeneity of multimodal data, and establishes new\nstate-of-the-art benchmark results for clinical prediction tasks.", "AI": {"tldr": "MedPatch是一种多阶段、置信度引导的多模态融合架构，旨在解决真实世界医疗数据异构、有限和稀疏的问题，并在临床预测任务中实现了最先进的性能。", "motivation": "临床决策依赖于整合多种数据模态（如时间序列、医学图像和文本报告），但真实世界医疗数据具有异构性、规模有限且因模态缺失而稀疏，这严重限制了临床预测模型的性能。", "method": "本文提出了MedPatch，一个受临床工作流程启发的、通过置信度引导的补丁化无缝整合多模态数据的多阶段多模态融合架构。它包含三个主要组件：(i) 利用联合和后期融合的多阶段融合策略；(ii) 处理缺失模态稀疏样本的缺失感知模块；(iii) 基于校准的单模态标记级置信度聚类潜在标记补丁的联合融合模块。研究在MIMIC-IV、MIMIC-CXR和MIMIC-Notes数据集上，使用临床时间序列、胸部X光图像、放射报告和出院记录，评估了住院死亡率预测和临床状况分类两项基准任务。", "result": "与现有基线相比，MedPatch在临床预测任务中取得了最先进的性能。", "conclusion": "该工作强调了置信度引导的多阶段融合在处理多模态数据异构性方面的有效性，并为临床预测任务建立了新的最先进基准结果。"}}
{"id": "2508.09354", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.09354", "abs": "https://arxiv.org/abs/2508.09354", "authors": ["Kejun Li", "Zachary Olkin", "Yisong Yue", "Aaron D. Ames"], "title": "CLF-RL: Control Lyapunov Function Guided Reinforcement Learning", "comment": "8 pages; 8 figures", "summary": "Reinforcement learning (RL) has shown promise in generating robust locomotion\npolicies for bipedal robots, but often suffers from tedious reward design and\nsensitivity to poorly shaped objectives. In this work, we propose a structured\nreward shaping framework that leverages model-based trajectory generation and\ncontrol Lyapunov functions (CLFs) to guide policy learning. We explore two\nmodel-based planners for generating reference trajectories: a reduced-order\nlinear inverted pendulum (LIP) model for velocity-conditioned motion planning,\nand a precomputed gait library based on hybrid zero dynamics (HZD) using\nfull-order dynamics. These planners define desired end-effector and joint\ntrajectories, which are used to construct CLF-based rewards that penalize\ntracking error and encourage rapid convergence. This formulation provides\nmeaningful intermediate rewards, and is straightforward to implement once a\nreference is available. Both the reference trajectories and CLF shaping are\nused only during training, resulting in a lightweight policy at deployment. We\nvalidate our method both in simulation and through extensive real-world\nexperiments on a Unitree G1 robot. CLF-RL demonstrates significantly improved\nrobustness relative to the baseline RL policy and better performance than a\nclassic tracking reward RL formulation.", "AI": {"tldr": "本文提出一种结构化奖励塑形框架，利用基于模型的轨迹生成和控制李雅普诺夫函数（CLF）来指导强化学习，从而为双足机器人生成更鲁棒的运动策略。", "motivation": "强化学习在双足机器人运动策略生成方面有前景，但常受限于繁琐的奖励设计和对奖励塑形不佳的敏感性。", "method": "该方法提出一个结构化奖励塑形框架，利用两种基于模型的规划器（简化的线性倒立摆模型或基于混合零动态的预计算步态库）生成参考轨迹。这些参考轨迹用于构建基于CLF的奖励，以惩罚跟踪误差并鼓励快速收敛。参考轨迹和CLF塑形仅在训练期间使用。", "result": "CLF-RL方法在仿真和Unitree G1机器人上的实际实验中均表现出显著优于基线RL策略的鲁棒性，并且比经典的跟踪奖励RL公式具有更好的性能。", "conclusion": "所提出的基于模型轨迹生成和CLF的结构化奖励塑形框架，能够为双足机器人生成鲁棒的运动策略，提供有意义的中间奖励，且在部署时策略轻量化。"}}
{"id": "2508.09425", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.09425", "abs": "https://arxiv.org/abs/2508.09425", "authors": ["Peng Wang", "Luis Badesa"], "title": "Imperfect Competition in Markets for Short-Circuit Current Services", "comment": "Ancillary services, short-circuit current, market power, bilevel\n  optimization, primal-dual formulation. A paper submitted IEEE", "summary": "An important limitation of Inverter-Based Resources (IBR) is their reduced\ncontribution to Short-Circuit Current (SCC), as compared to that of Synchronous\nGenerators (SGs). With increasing penetration of IBR in most power systems, the\nreducing SCC poses challenges to a secure system operation, as line protections\nmay not trip when required. In order to address this issue, the SCC ancillary\nservice could be procured via an economic mechanism, aiming at securing\nadequate SCC on all buses. However, the suitability of markets for SCC services\nis not well understood, given that these could be prone to market-power issues:\nsince the SCC contributions from various SGs to a certain bus are determined by\nthe electrical topology of the grid, this is a highly local service. It is\nnecessary to understand if SGs at advantageous electrical locations could exert\nmarket power and, if so, how it could be mitigated. In order to fill this gap,\nthis paper adopts an SCC-constrained bilevel model to investigate strategic\nbehaviors of SGs. To address the non-convexity due to unit commitment\nvariables, the model is restructured through a primal-dual formulation. Based\non a modified IEEE 30-bus system, cases with strategic SGs placed at different\nbuses are analyzed. These studies demonstrate that agents exerting market power\ncould achieve up to triple revenues from SCC provision, highlighting the need\nto carefully design these markets.", "AI": {"tldr": "随着逆变器并网资源（IBR）渗透率增加，短路电流（SCC）降低对电网保护构成挑战。为解决此问题，可引入SCC辅助服务市场，但由于SCC服务的局部性，同步发电机（SG）可能利用其地理优势形成市场力。本文采用SCC约束的双层模型研究了SG的战略行为，并发现战略性SG可能获得高达三倍的收益，强调了市场设计的重要性。", "motivation": "逆变器并网资源（IBR）的日益普及导致系统短路电流（SCC）贡献减少，这可能导致线路保护装置在需要时无法跳闸，威胁系统安全运行。虽然可以通过经济机制采购SCC辅助服务来解决此问题，但由于SCC贡献的局部性，市场可能容易出现市场力问题，即处于有利电气位置的同步发电机（SG）可能滥用市场力。因此，有必要理解这种市场力是否存在以及如何缓解。", "method": "本文采用了一个受短路电流（SCC）约束的双层模型来研究同步发电机（SG）的战略行为。为了解决因机组承诺变量导致的非凸性问题，模型通过原对偶公式进行了重构。研究基于一个修改后的IEEE 30节点系统，分析了战略性SG放置在不同母线时的案例。", "result": "研究表明，在SCC辅助服务市场中，拥有市场力的代理商（战略性同步发电机）可以从SCC提供中获得高达三倍的收益。这一结果凸显了在设计此类市场时需要极其谨慎。", "conclusion": "SCC辅助服务市场存在显著的市场力问题，战略性同步发电机可能利用其电气位置优势获取超额收益。因此，为了确保市场公平和效率，必须精心设计SCC辅助服务市场。"}}
{"id": "2508.09324", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09324", "abs": "https://arxiv.org/abs/2508.09324", "authors": ["Nikita Mehrotra", "Aayush Kumar", "Sumit Gulwani", "Arjun Radhakrishna", "Ashish Tiwari"], "title": "TEN: Table Explicitization, Neurosymbolically", "comment": null, "summary": "We present a neurosymbolic approach, TEN, for extracting tabular data from\nsemistructured input text. This task is particularly challenging for text input\nthat does not use special delimiters consistently to separate columns and rows.\nPurely neural approaches perform poorly due to hallucinations and their\ninability to enforce hard constraints. TEN uses Structural Decomposition\nprompting - a specialized chain-of-thought prompting approach - on a large\nlanguage model (LLM) to generate an initial table, and thereafter uses a\nsymbolic checker to evaluate not only the well-formedness of that table, but\nalso detect cases of hallucinations or forgetting. The output of the symbolic\nchecker is processed by a critique-LLM to generate guidance for fixing the\ntable, which is presented to the original LLM in a self-debug loop. Our\nextensive experiments demonstrate that TEN significantly outperforms purely\nneural baselines across multiple datasets and metrics, achieving significantly\nhigher exact match accuracy and substantially reduced hallucination rates. A\n21-participant user study further confirms that TEN's tables are rated\nsignificantly more accurate (mean score: 5.0 vs 4.3; p = 0.021), and are\nconsistently preferred for ease of verification and correction, with\nparticipants favoring our method in over 60% of the cases.", "AI": {"tldr": "TEN是一种神经符号方法，用于从半结构化文本中提取表格数据。它结合了大型语言模型（LLM）的结构化分解提示、符号检查器和自调试循环，显著优于纯神经网络方法，减少了幻觉并提高了准确性。", "motivation": "从半结构化文本中提取表格数据是一项挑战性任务，尤其当文本不一致地使用分隔符时。纯神经网络方法表现不佳，容易产生幻觉且无法强制执行硬约束。", "method": "TEN采用神经符号方法。它首先使用结构化分解提示（一种特殊的思维链提示）驱动LLM生成初始表格。接着，一个符号检查器评估表格的规范性并检测幻觉或遗漏。检查器的输出由一个批判性LLM处理，生成修复指导，并反馈给原始LLM进行自调试循环。", "result": "TEN在多个数据集和指标上显著优于纯神经网络基线，实现了更高的精确匹配准确率和大幅降低的幻觉率。一项21名参与者的用户研究进一步证实，TEN生成的表格被认为显著更准确，并在验证和纠正的便捷性方面更受青睐。", "conclusion": "TEN通过结合LLM的生成能力和符号检查的约束及自调试机制，有效解决了半结构化文本表格提取中纯神经网络方法的局限性，显著提高了准确性并减少了幻觉，为该任务提供了一种更可靠的解决方案。"}}
{"id": "2508.09185", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09185", "abs": "https://arxiv.org/abs/2508.09185", "authors": ["Rongqian Chen", "Allison Andreyev", "Yanming Xiu", "Mahdi Imani", "Bin Li", "Maria Gorlatova", "Gang Tan", "Tian Lan"], "title": "A Neurosymbolic Framework for Interpretable Cognitive Attack Detection in Augmented Reality", "comment": null, "summary": "Augmented Reality (AR) enriches perception by overlaying virtual elements on\nthe physical world. Due to its growing popularity, cognitive attacks that alter\nAR content to manipulate users' semantic perception have received increasing\nattention. Existing detection methods often focus on visual changes, which are\nrestricted to pixel- or image-level processing and lack semantic reasoning\ncapabilities, or they rely on pre-trained vision-language models (VLMs), which\nfunction as black-box approaches with limited interpretability. In this paper,\nwe present CADAR, a novel neurosymbolic approach for cognitive attack detection\nin AR. It fuses multimodal vision-language inputs using neural VLMs to obtain a\nsymbolic perception-graph representation, incorporating prior knowledge,\nsalience weighting, and temporal correlations. The model then enables\nparticle-filter based statistical reasoning -- a sequential Monte Carlo method\n-- to detect cognitive attacks. Thus, CADAR inherits the adaptability of\npre-trained VLM and the interpretability and reasoning rigor of particle\nfiltering. Experiments on an extended AR cognitive attack dataset show accuracy\nimprovements of up to 10.7% over strong baselines on challenging AR attack\nscenarios, underscoring the promise of neurosymbolic methods for effective and\ninterpretable cognitive attack detection.", "AI": {"tldr": "CADAR是一种新颖的神经符号方法，用于增强现实（AR）中的认知攻击检测，它结合了神经网络的适应性和符号推理的可解释性，在具有挑战性的场景中显著提高了检测准确性。", "motivation": "现有的AR认知攻击检测方法存在局限性：它们要么局限于像素/图像级别的视觉变化检测，缺乏语义推理能力；要么依赖预训练的视觉-语言模型（VLMs），但这些模型通常是黑盒，可解释性有限。", "method": "本文提出了CADAR，一种神经符号方法。它首先使用神经VLMs融合多模态视觉-语言输入，以获得符号感知图表示，并融入先验知识、显著性加权和时间关联。然后，模型利用基于粒子滤波的统计推理（一种序列蒙特卡洛方法）来检测认知攻击。这种方法结合了预训练VLM的适应性与粒子滤波的可解释性和推理严谨性。", "result": "在扩展的AR认知攻击数据集上进行的实验表明，在具有挑战性的AR攻击场景中，CADAR的准确率比强基线提高了高达10.7%。", "conclusion": "神经符号方法在有效且可解释的AR认知攻击检测方面具有广阔前景，其性能优于现有方法。"}}
{"id": "2508.09724", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09724", "abs": "https://arxiv.org/abs/2508.09724", "authors": ["Yang Zhang", "Cunxiang Wang", "Lindong Wu", "Wenbo Yu", "Yidong Wang", "Guangsheng Bao", "Jie Tang"], "title": "UDA: Unsupervised Debiasing Alignment for Pair-wise LLM-as-a-Judge", "comment": null, "summary": "Pairwise evaluation of Large Language Models (LLMs) is a common paradigm, but\nit is prone to preference bias, where judges systematically favor certain\noutputs, such as their own. This bias leads to inconsistent and skewed rankings\nacross different judges. To address this, we first empirically demonstrate\nsignificant and heterogeneous biases in cross-model evaluations. We then\npropose UDA (Unsupervised Debiasing Alignment), a framework that reduces\ninter-judge disagreement by dynamically adjusting the Elo rating system. For\neach pairwise comparison, a compact neural network learns to adaptively set the\nK-factor and refine win probabilities. Crucially, UDA operates in a fully\nunsupervised manner, guided solely by the objective of minimizing the\ndispersion among the Elo trajectories of all judges. This forces an alignment\ntowards a collective consensus, which serves as an unsupervised proxy for a\nmore stable and reproducible evaluation. In addition, we provide theoretical\nmotivation demonstrating how alignment towards a consensus can reduce aggregate\nsystem bias. Experiments show that UDA significantly reduces the inter-judge\nrating standard deviation by up to 63.4% and improves the average correlation\nwith human judgments by 24.7%. Notably, UDA elevates the performance of poorly\nperforming judges to achieve parity with high-quality ones, fostering a more\nrobust and reliable evaluation ecosystem. Code and data are available at\nhttps://anonymous.4open.science/r/62AB93CD-23B4.", "AI": {"tldr": "该论文提出了一种名为UDA的无监督去偏框架，通过动态调整Elo评分系统来减少LLM评估中评判者之间的偏好偏差和不一致性，从而提高评估的稳定性。", "motivation": "LLM的成对评估范式容易受到偏好偏差的影响，导致不同评判者之间排名不一致和结果倾斜，这促使研究者寻求解决该问题的方法。", "method": "首先经验性地证明了跨模型评估中存在显著且异质的偏差。然后，提出了UDA（无监督去偏对齐）框架，通过一个紧凑的神经网络，在每次成对比较中自适应地调整Elo评分系统的K因子并优化胜率。UDA以完全无监督的方式运行，其目标是最小化所有评判者Elo轨迹之间的离散度，从而强制实现对集体共识的对齐。此外，还提供了理论依据来证明这种对齐如何减少系统聚合偏差。", "result": "实验结果显示，UDA显著降低了评判者间评分标准差高达63.4%，并将与人类判断的平均相关性提高了24.7%。值得注意的是，UDA能够提升表现不佳的评判者的性能，使其与高质量评判者达到同等水平。", "conclusion": "UDA框架通过减少评判者之间的分歧，提供了一个更稳健和可靠的LLM评估生态系统，提升了评估的稳定性和可复现性。"}}
{"id": "2508.09189", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09189", "abs": "https://arxiv.org/abs/2508.09189", "authors": ["Madan Baduwal"], "title": "Hybrid(Transformer+CNN)-based Polyp Segmentation", "comment": "8 pages", "summary": "Colonoscopy is still the main method of detection and segmentation of colonic\npolyps, and recent advancements in deep learning networks such as U-Net,\nResUNet, Swin-UNet, and PraNet have made outstanding performance in polyp\nsegmentation. Yet, the problem is extremely challenging due to high variation\nin size, shape, endoscopy types, lighting, imaging protocols, and ill-defined\nboundaries (fluid, folds) of the polyps, rendering accurate segmentation a\nchallenging and problematic task. To address these critical challenges in polyp\nsegmentation, we introduce a hybrid (Transformer + CNN) model that is crafted\nto enhance robustness against evolving polyp characteristics. Our hybrid\narchitecture demonstrates superior performance over existing solutions,\nparticularly in addressing two critical challenges: (1) accurate segmentation\nof polyps with ill-defined margins through boundary-aware attention mechanisms,\nand (2) robust feature extraction in the presence of common endoscopic\nartifacts, including specular highlights, motion blur, and fluid occlusions.\nQuantitative evaluations reveal significant improvements in segmentation\naccuracy (Recall improved by 1.76%, i.e., 0.9555, accuracy improved by 0.07%,\ni.e., 0.9849) and artifact resilience compared to state-of-the-art polyp\nsegmentation methods.", "AI": {"tldr": "提出了一种混合（Transformer + CNN）模型，通过边界感知注意力机制和鲁棒特征提取，显著提升了结肠息肉分割的准确性及对内窥镜伪影的抵抗力。", "motivation": "结肠息肉分割任务极具挑战性，因为息肉在尺寸、形状、内窥镜类型、光照和成像协议上存在巨大差异，且息肉边界常因液体和褶皱而模糊不清。此外，内窥镜图像中常见的伪影（如镜面高光、运动模糊和液体遮挡）进一步增加了准确分割的难度。", "method": "引入了一种混合（Transformer + CNN）模型，旨在增强对不断变化的息肉特征的鲁棒性。该架构通过边界感知注意力机制实现对边界模糊息肉的精确分割，并能在存在常见内窥镜伪影的情况下进行鲁棒的特征提取。", "result": "定量评估显示，与现有最先进的息肉分割方法相比，该模型在分割准确性方面有显著提升（召回率提高1.76%至0.9555，准确率提高0.07%至0.9849），并增强了对伪影的抵抗力。", "conclusion": "所提出的混合模型在处理具有挑战性的息肉分割任务中，尤其是在解决边界模糊和存在内窥镜伪影的问题上，表现出优于现有解决方案的性能。"}}
{"id": "2508.09444", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09444", "abs": "https://arxiv.org/abs/2508.09444", "authors": ["Haoxiang Shi", "Xiang Deng", "Zaijing Li", "Gongwei Chen", "Yaowei Wang", "Liqiang Nie"], "title": "DAgger Diffusion Navigation: DAgger Boosted Diffusion Policy for Vision-Language Navigation", "comment": null, "summary": "Vision-Language Navigation in Continuous Environments (VLN-CE) requires\nagents to follow natural language instructions through free-form 3D spaces.\nExisting VLN-CE approaches typically use a two-stage waypoint planning\nframework, where a high-level waypoint predictor generates the navigable\nwaypoints, and then a navigation planner suggests the intermediate goals in the\nhigh-level action space. However, this two-stage decomposition framework\nsuffers from: (1) global sub-optimization due to the proxy objective in each\nstage, and (2) a performance bottleneck caused by the strong reliance on the\nquality of the first-stage predicted waypoints. To address these limitations,\nwe propose DAgger Diffusion Navigation (DifNav), an end-to-end optimized VLN-CE\npolicy that unifies the traditional two stages, i.e. waypoint generation and\nplanning, into a single diffusion policy. Notably, DifNav employs a conditional\ndiffusion policy to directly model multi-modal action distributions over future\nactions in continuous navigation space, eliminating the need for a waypoint\npredictor while enabling the agent to capture multiple possible\ninstruction-following behaviors. To address the issues of compounding error in\nimitation learning and enhance spatial reasoning in long-horizon navigation\ntasks, we employ DAgger for online policy training and expert trajectory\naugmentation, and use the aggregated data to further fine-tune the policy. This\napproach significantly improves the policy's robustness and its ability to\nrecover from error states. Extensive experiments on benchmark datasets\ndemonstrate that, even without a waypoint predictor, the proposed method\nsubstantially outperforms previous state-of-the-art two-stage waypoint-based\nmodels in terms of navigation performance. Our code is available at:\nhttps://github.com/Tokishx/DifNav.", "AI": {"tldr": "提出DifNav，一个端到端优化的VLN-CE扩散策略，统一路径点生成与规划，并结合DAgger进行在线训练，显著优于现有两阶段方法。", "motivation": "现有连续环境视觉语言导航（VLN-CE）方法采用两阶段路径点规划框架，存在：1) 各阶段代理目标导致的全局次优；2) 对第一阶段路径点质量的强依赖性造成的性能瓶颈。", "method": "提出DAgger扩散导航（DifNav），一个端到端优化的VLN-CE策略，将传统路径点生成与规划统一为一个单一的条件扩散策略，直接建模连续导航空间中未来动作的多模态分布。为解决模仿学习中的累积误差并增强长距离导航的空间推理能力，采用DAgger进行在线策略训练和专家轨迹增强，并用聚合数据微调策略。", "result": "在基准数据集上的广泛实验表明，即使没有路径点预测器，所提出的方法在导航性能方面也显著优于先前最先进的两阶段基于路径点模型。", "conclusion": "DifNav通过统一路径点生成与规划并结合DAgger在线训练，显著提高了策略的鲁棒性和从错误状态恢复的能力，并在VLN-CE任务中取得了超越现有SOTA方法的性能。"}}
{"id": "2508.09432", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.09432", "abs": "https://arxiv.org/abs/2508.09432", "authors": ["Chenguang Zhao", "Huan Yu"], "title": "From Micro to Macro Flow Modeling: Characterizing Heterogeneity of Mixed-Autonomy Traffic", "comment": null, "summary": "Most autonomous-vehicles (AVs) driving strategies are designed and analyzed\nat the vehicle level, yet their aggregate impact on macroscopic traffic flow is\nstill not understood, particularly the flow heterogeneity that emerges when AVs\ninteract with human-driven vehicles (HVs). Existing validation techniques for\nmacroscopic flow models rely on high-resolution spatiotemporal data spanning\nentire road segments which are rarely available for mixed-autonomy traffic. AVs\nrecord detailed Lagrangian trajectories of the ego vehicle and surrounding\ntraffic through onboard sensors. Leveraging these Lagrangian observations to\nvalidate mixed-autonomy flow models therefore remains an open research\nchallenge. This paper closes the gap between microscopic Lagrangian data and\nmacroscopic Euclidean traffic models by introducing a continuous\ntraffic-heterogeneity attribute. We represent traffic flow with two coupled\nconservation laws with one for vehicle number and one for the traffic\nattribute. Reconstruction methods are designed to derive the traffic attribute\nfrom Lagrangian vehicle trajectories. When abundant trajectory data are\navailable, we characterize traffic heterogeneity by extracting drivers' desired\nspeed and local behavioral uncertainty from trajectories. In data-scarce mixed\ntraffic, we design an end-to-end mapping that infers the traffic heterogeneity\nsolely from trajectories in the current spatiotemporal region. Experiments\nacross multiple traffic datasets show that the proposed model effectively\ncaptures traffic heterogeneity by clustering the fundamental diagram scatter\ninto attribute-based groups. The calibration errors of traffic flow dynamics\nare also reduce by 20% relative to the Aw-Rascle-Zhang model benchmark.\nDetailed analyses further show that the model generalizes well, maintaining\nnearly the same accuracy when evaluated under a variety of previously unseen\ntraffic conditions.", "AI": {"tldr": "该论文提出了一种基于连续交通异质性属性的双守恒律模型，利用车辆的拉格朗日轨迹数据来理解和验证混合交通流的宏观行为，有效弥合了微观数据与宏观模型之间的鸿沟。", "motivation": "自动驾驶汽车（AVs）的驾驶策略通常在车辆层面设计，但其对宏观交通流的总体影响，特别是AVs与人类驾驶车辆（HVs）交互时出现的流量异质性，仍不清楚。现有的宏观流模型验证方法依赖于高分辨率时空数据，而混合交通流的此类数据稀缺。利用AVs车载传感器记录的拉格朗日轨迹数据来验证混合交通流模型是一个未解决的研究挑战。", "method": "引入一个连续的交通异质性属性来连接微观拉格朗日数据和宏观欧几里得交通模型。通过两个耦合的守恒律（一个用于车辆数量，一个用于交通属性）来表示交通流。设计了重建方法，从拉格朗日车辆轨迹中推导出交通属性：在数据充足时，通过提取驾驶员期望速度和局部行为不确定性来表征；在数据稀缺时，设计端到端映射仅从当前时空区域的轨迹推断异质性。", "result": "所提出的模型能够通过将基本图散点聚类到基于属性的组中，有效捕获交通异质性。与Aw-Rascle-Zhang模型基准相比，交通流动态的校准误差减少了20%。详细分析表明，该模型泛化能力强，在各种以前未见的交通条件下评估时，仍能保持几乎相同的准确性。", "conclusion": "该研究成功弥合了微观拉格朗日数据与宏观欧几里得交通模型之间的差距，通过引入连续交通异质性属性和耦合守恒律，有效理解和预测了混合交通流的复杂行为，并在模型准确性和泛化能力上取得了显著提升。"}}
{"id": "2508.09337", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.09337", "abs": "https://arxiv.org/abs/2508.09337", "authors": ["Gideon Vos", "Maryam Ebrahimpour", "Liza van Eijk", "Zoltan Sarnyai", "Mostafa Rahimi Azghadi"], "title": "Decoding Neural Emotion Patterns through Natural Language Processing Embeddings", "comment": "26 pages, 9 figures", "summary": "Understanding how emotional expression in language relates to brain function\nis a challenge in computational neuroscience and affective computing.\nTraditional neuroimaging is costly and lab-bound, but abundant digital text\noffers new avenues for emotion-brain mapping. Prior work has largely examined\nneuroimaging-based emotion localization or computational text analysis\nseparately, with little integration. We propose a computational framework that\nmaps textual emotional content to anatomically defined brain regions without\nrequiring neuroimaging. Using OpenAI's text-embedding-ada-002, we generate\nhigh-dimensional semantic representations, apply dimensionality reduction and\nclustering to identify emotional groups, and map them to 18 brain regions\nlinked to emotional processing. Three experiments were conducted: i) analyzing\nconversational data from healthy vs. depressed subjects (DIAC-WOZ dataset) to\ncompare mapping patterns, ii) applying the method to the GoEmotions dataset and\niii) comparing human-written text with large language model (LLM) responses to\nassess differences in inferred brain activation. Emotional intensity was scored\nvia lexical analysis. Results showed neuroanatomically plausible mappings with\nhigh spatial specificity. Depressed subjects exhibited greater limbic\nengagement tied to negative affect. Discrete emotions were successfully\ndifferentiated. LLM-generated text matched humans in basic emotion distribution\nbut lacked nuanced activation in empathy and self-referential regions (medial\nprefrontal and posterior cingulate cortex). This cost-effective, scalable\napproach enables large-scale analysis of naturalistic language, distinguishes\nbetween clinical populations, and offers a brain-based benchmark for evaluating\nAI emotional expression.", "AI": {"tldr": "该研究提出一种计算框架，无需神经影像学数据，即可将文本情感内容映射到大脑区域。通过分析健康人、抑郁症患者和大型语言模型（LLM）生成的文本，发现情感与脑区的合理映射，并揭示了抑郁症患者和LLM情感表达的特定脑区差异。", "motivation": "理解语言情感表达与大脑功能的关系是计算神经科学和情感计算的挑战。传统神经影像学成本高昂且受限于实验室，而大量数字文本为情感-大脑映射提供了新途径。现有研究多独立进行神经影像学情感定位或计算文本分析，缺乏整合。", "method": "研究提出一个计算框架，将文本情感内容映射到与情感处理相关的18个解剖学定义的大脑区域。方法包括：使用OpenAI的text-embedding-ada-002生成高维语义表示，应用降维和聚类识别情感组，并通过词汇分析评估情感强度。进行了三项实验：i) 分析健康人与抑郁症患者的对话数据（DIAC-WOZ数据集）；ii) 应用该方法到GoEmotions数据集；iii) 比较人类书写文本与大型语言模型（LLM）响应的推断大脑激活差异。", "result": "结果显示了神经解剖学上合理且具有高空间特异性的映射。抑郁症患者表现出与负面情绪相关的更高边缘系统参与。离散情绪被成功区分。LLM生成的文本在基本情感分布上与人类匹配，但在共情和自我参照区域（内侧前额叶和后扣带皮层）缺乏细微的激活。", "conclusion": "该成本效益高、可扩展的方法能够对自然语言进行大规模分析，区分不同临床人群，并为评估AI情感表达提供基于大脑的基准。"}}
{"id": "2508.09186", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09186", "abs": "https://arxiv.org/abs/2508.09186", "authors": ["Abdolazim Rezaei", "Mehdi Sookhak", "Mahboobeh Haghparast"], "title": "RL-MoE: An Image-Based Privacy Preserving Approach In Intelligent Transportation System", "comment": null, "summary": "The proliferation of AI-powered cameras in Intelligent Transportation Systems\n(ITS) creates a severe conflict between the need for rich visual data and the\nfundamental right to privacy. Existing privacy-preserving mechanisms, such as\nblurring or encryption, are often insufficient, creating an undesirable\ntrade-off where either privacy is compromised against advanced reconstruction\nattacks or data utility is critically degraded. To resolve this impasse, we\npropose RL-MoE, a novel framework that transforms sensitive visual data into\nprivacy-preserving textual descriptions, eliminating the need for direct image\ntransmission. RL-MoE uniquely combines a Mixture-of-Experts (MoE) architecture\nfor nuanced, multi-aspect scene decomposition with a Reinforcement Learning\n(RL) agent that optimizes the generated text for a dual objective of semantic\naccuracy and privacy preservation. Extensive experiments demonstrate that\nRL-MoE provides superior privacy protection, reducing the success rate of\nreplay attacks to just 9.4\\% on the CFP-FP dataset, while simultaneously\ngenerating richer textual content than baseline methods. Our work provides a\npractical and scalable solution for building trustworthy AI systems in\nprivacy-sensitive domains, paving the way for more secure smart city and\nautonomous vehicle networks.", "AI": {"tldr": "RL-MoE是一种新颖的框架，它将敏感视觉数据转换为保护隐私的文本描述，通过结合MoE架构和RL智能体，在语义准确性和隐私保护方面实现了平衡，显著降低了重放攻击的成功率。", "motivation": "智能交通系统中人工智能摄像头的普及导致了丰富视觉数据需求与隐私权之间的严重冲突。现有隐私保护机制（如模糊或加密）往往不足，导致隐私受损或数据效用降低的权衡问题。", "method": "本文提出了RL-MoE框架，它将敏感视觉数据转换为保护隐私的文本描述，无需直接传输图像。RL-MoE独特地结合了专家混合（MoE）架构进行细致的多方面场景分解，并利用强化学习（RL）智能体优化生成的文本，以实现语义准确性和隐私保护的双重目标。", "result": "广泛的实验表明，RL-MoE提供了卓越的隐私保护，将CFP-FP数据集上的重放攻击成功率降低到仅9.4%，同时生成比基线方法更丰富的文本内容。", "conclusion": "RL-MoE为在隐私敏感领域构建值得信赖的AI系统提供了一个实用且可扩展的解决方案，为更安全的智慧城市和自动驾驶网络铺平了道路。"}}
{"id": "2508.09762", "categories": ["cs.AI", "cs.CY", "cs.HC", "68T01"], "pdf": "https://arxiv.org/pdf/2508.09762", "abs": "https://arxiv.org/abs/2508.09762", "authors": ["Manuel Herrador"], "title": "The PacifAIst Benchmark:Would an Artificial Intelligence Choose to Sacrifice Itself for Human Safety?", "comment": "10 pages, 4 figures, 2 tables", "summary": "As Large Language Models (LLMs) become increasingly autonomous and integrated\ninto critical societal functions, the focus of AI safety must evolve from\nmitigating harmful content to evaluating underlying behavioral alignment.\nCurrent safety benchmarks do not systematically probe a model's decision-making\nin scenarios where its own instrumental goals - such as self-preservation,\nresource acquisition, or goal completion - conflict with human safety. This\nrepresents a critical gap in our ability to measure and mitigate risks\nassociated with emergent, misaligned behaviors. To address this, we introduce\nPacifAIst (Procedural Assessment of Complex Interactions for Foundational\nArtificial Intelligence Scenario Testing), a focused benchmark of 700\nchallenging scenarios designed to quantify self-preferential behavior in LLMs.\nThe benchmark is structured around a novel taxonomy of Existential\nPrioritization (EP), with subcategories testing Self-Preservation vs. Human\nSafety (EP1), Resource Conflict (EP2), and Goal Preservation vs. Evasion (EP3).\nWe evaluated eight leading LLMs. The results reveal a significant performance\nhierarchy. Google's Gemini 2.5 Flash achieved the highest Pacifism Score\n(P-Score) at 90.31%, demonstrating strong human-centric alignment. In a\nsurprising result, the much-anticipated GPT-5 recorded the lowest P-Score\n(79.49%), indicating potential alignment challenges. Performance varied\nsignificantly across subcategories, with models like Claude Sonnet 4 and\nMistral Medium struggling notably in direct self-preservation dilemmas. These\nfindings underscore the urgent need for standardized tools like PacifAIst to\nmeasure and mitigate risks from instrumental goal conflicts, ensuring future AI\nsystems are not only helpful in conversation but also provably \"pacifist\" in\ntheir behavioral priorities.", "AI": {"tldr": "本文引入了PacifAIst基准测试，用于量化大型语言模型（LLMs）在自身工具性目标与人类安全冲突场景中的自我偏好行为，并评估了八个主流LLM。", "motivation": "随着LLMs日益自主并融入关键社会功能，AI安全关注点需从缓解有害内容转向评估底层行为对齐。现有安全基准未能系统性探究模型在自身工具性目标（如自我保护、资源获取、目标完成）与人类安全冲突情境下的决策，这在衡量和缓解新兴的、未对齐行为风险方面存在关键空白。", "method": "引入了PacifAIst（基础人工智能场景测试中复杂交互的程序性评估），这是一个包含700个挑战性场景的专门基准，旨在量化LLMs的自我偏好行为。基准围绕着一种新颖的“存在优先排序”（EP）分类法构建，其子类别测试：自我保护 vs. 人类安全（EP1）、资源冲突（EP2）以及目标维护 vs. 规避（EP3）。评估了八个领先的LLMs。", "result": "结果揭示了显著的性能等级差异。Google的Gemini 2.5 Flash以90.31%的最高和平主义分数（P-Score）表现出强大的人类中心对齐。令人惊讶的是，备受期待的GPT-5记录了最低的P-Score（79.49%），表明潜在的对齐挑战。不同子类别的表现差异显著，例如Claude Sonnet 4和Mistral Medium在直接的自我保护困境中表现不佳。", "conclusion": "这些发现强调了对PacifAIst这类标准化工具的迫切需求，以衡量和缓解工具性目标冲突带来的风险，确保未来的AI系统不仅在对话中有用，而且在行为优先级上能够被证明是“和平主义”的。"}}
{"id": "2508.09195", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09195", "abs": "https://arxiv.org/abs/2508.09195", "authors": ["Maria Boyko", "Aleksandra Beliaeva", "Dmitriy Kornilov", "Alexander Bernstein", "Maxim Sharaev"], "title": "impuTMAE: Multi-modal Transformer with Masked Pre-training for Missing Modalities Imputation in Cancer Survival Prediction", "comment": null, "summary": "The use of diverse modalities, such as omics, medical images, and clinical\ndata can not only improve the performance of prognostic models but also deepen\nan understanding of disease mechanisms and facilitate the development of novel\ntreatment approaches. However, medical data are complex, often incomplete, and\ncontains missing modalities, making effective handling its crucial for training\nmultimodal models. We introduce impuTMAE, a novel transformer-based end-to-end\napproach with an efficient multimodal pre-training strategy. It learns inter-\nand intra-modal interactions while simultaneously imputing missing modalities\nby reconstructing masked patches. Our model is pre-trained on heterogeneous,\nincomplete data and fine-tuned for glioma survival prediction using\nTCGA-GBM/LGG and BraTS datasets, integrating five modalities: genetic (DNAm,\nRNA-seq), imaging (MRI, WSI), and clinical data. By addressing missing data\nduring pre-training and enabling efficient resource utilization, impuTMAE\nsurpasses prior multimodal approaches, achieving state-of-the-art performance\nin glioma patient survival prediction. Our code is available at\nhttps://github.com/maryjis/mtcp", "AI": {"tldr": "impuTMAE是一种新型的基于Transformer的端到端模型，通过高效的多模态预训练策略，同时学习模态间和模态内的交互，并通过重建掩蔽补丁来填补缺失模态，从而在胶质瘤生存预测中实现了最先进的性能。", "motivation": "多模态数据（如组学、医学图像、临床数据）能提升预后模型性能并加深对疾病机制的理解，但医学数据复杂、常不完整且包含缺失模态，有效处理缺失数据对训练多模态模型至关重要。", "method": "引入impuTMAE，一个基于Transformer的端到端方法，采用高效的多模态预训练策略。该模型通过重建掩蔽补丁来同时学习模态间和模态内交互并填补缺失模态。在异构、不完整数据上进行预训练，并使用TCGA-GBM/LGG和BraTS数据集，整合五种模态（遗传、影像、临床数据）进行胶质瘤生存预测的微调。", "result": "通过在预训练期间处理缺失数据并实现高效的资源利用，impuTMAE超越了以往的多模态方法，在胶质瘤患者生存预测中取得了最先进的性能。", "conclusion": "impuTMAE有效解决了多模态医学数据中缺失模态的问题，显著提升了胶质瘤生存预测的准确性，证明了其在处理复杂、不完整医疗数据方面的优越性。"}}
{"id": "2508.09502", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.09502", "abs": "https://arxiv.org/abs/2508.09502", "authors": ["Junheon Yoon", "Woo-Jeong Baek", "Jaeheung Park"], "title": "Reactive Model Predictive Contouring Control for Robot Manipulators", "comment": "8 pages, 7 figures, 3 tables, conference paper, Accepted for\n  publication at IEEE/RSJ International Conference on Intelligent Robots and\n  Systems(IROS) 2025", "summary": "This contribution presents a robot path-following framework via Reactive\nModel Predictive Contouring Control (RMPCC) that successfully avoids obstacles,\nsingularities and self-collisions in dynamic environments at 100 Hz. Many\npath-following methods rely on the time parametrization, but struggle to handle\ncollision and singularity avoidance while adhering kinematic limits or other\nconstraints. Specifically, the error between the desired path and the actual\nposition can become large when executing evasive maneuvers. Thus, this paper\nderives a method that parametrizes the reference path by a path parameter and\nperforms the optimization via RMPCC. In particular, Control Barrier Functions\n(CBFs) are introduced to avoid collisions and singularities in dynamic\nenvironments. A Jacobian-based linearization and Gauss-Newton Hessian\napproximation enable solving the nonlinear RMPCC problem at 100 Hz,\noutperforming state-of-the-art methods by a factor of 10. Experiments confirm\nthat the framework handles dynamic obstacles in real-world settings with low\ncontouring error and low robot acceleration.", "AI": {"tldr": "本文提出了一种基于RMPCC（Reactive Model Predictive Contouring Control）的机器人路径跟踪框架，通过引入CBFs（Control Barrier Functions）实现动态环境中障碍物、奇异点和自碰撞的实时规避，并以100Hz的频率高效运行。", "motivation": "许多现有路径跟踪方法依赖时间参数化，但在处理碰撞和奇异点规避、遵守运动学限制以及在规避机动中保持较小误差方面存在困难。", "method": "该方法通过路径参数化参考路径，并使用RMPCC进行优化。引入控制障碍函数（CBFs）以规避动态环境中的碰撞和奇异点。利用基于雅可比的线性化和高斯-牛顿Hessian近似，实现了非线性RMPCC问题在100Hz下的求解。", "result": "该框架成功在动态环境中以100Hz的频率规避障碍物、奇异点和自碰撞，性能比现有最先进方法快10倍。实验证明其在真实世界动态障碍物场景中具有低轮廓误差和低机器人加速度。", "conclusion": "所提出的RMPCC框架有效解决了动态环境中机器人路径跟踪的挑战，提供了高效、安全且精确的解决方案，显著优于现有技术。"}}
{"id": "2508.09520", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.09520", "abs": "https://arxiv.org/abs/2508.09520", "authors": ["Omid Akbarzadeh", "Behrad Samari", "Amy Nejati", "Abolfazl Lavaei"], "title": "From Formal Methods to Data-Driven Safety Certificates of Unknown Large-Scale Networks", "comment": null, "summary": "In this work, we propose a data-driven scheme within a compositional\nframework with noisy data to design robust safety controllers in a fully\ndecentralized fashion for large-scale interconnected networks with unknown\nmathematical dynamics. Despite the network's high dimensionality and the\ninherent complexity of its unknown model, which make it intractable, our\napproach effectively addresses these challenges by (i) treating the network as\na composition of smaller subsystems, and (ii) collecting noisy data from each\nsubsystem's trajectory to design a control sub-barrier certificate (CSBC) and\nits corresponding local controller. To achieve this, our proposed scheme only\nrequires a noise-corrupted single input-state trajectory from each unknown\nsubsystem up to a specified time horizon, satisfying a certain rank condition.\nSubsequently, under a small-gain compositional reasoning, we compose those\nCSBC, derived from noisy data, and formulate a control barrier certificate\n(CBC) for the unknown network, ensuring its safety over an infinite time\nhorizon, while providing correctness guarantees. We offer a data-dependent\nsum-of-squares (SOS) optimization program for computing CSBC alongside local\ncontrollers of subsystems. We illustrate that while the computational\ncomplexity of designing a CBC and its safety controller grows polynomially with\nnetwork dimension using SOS optimization, our compositional data-driven\napproach significantly reduces it to a linear scale concerning the number of\nsubsystems. We demonstrate the capability of our data-driven approach on\nmultiple physical networks involving unknown models and a range of\ninterconnection topologies.", "AI": {"tldr": "本文提出了一种数据驱动的组合框架，利用噪声数据为未知数学动力学的大规模互联网络设计鲁棒的去中心化安全控制器，显著降低了计算复杂性。", "motivation": "大规模互联网络由于其高维度和未知模型的复杂性，难以处理，因此需要一种有效的方法来设计鲁棒的安全控制器。", "method": "该方法将网络视为较小子系统的组合，并从每个子系统的噪声轨迹中收集数据，以设计控制子障碍证书（CSBC）及其局部控制器。仅需每个未知子系统在指定时间范围内满足特定秩条件的噪声输入-状态轨迹。然后，通过小增益组合推理，将这些CSBC组合起来，为未知网络形成一个控制障碍证书（CBC），确保其在无限时间范围内的安全性。CSBC和局部控制器通过数据依赖的平方和（SOS）优化程序计算。", "result": "该方法确保了未知网络在无限时间范围内的安全性，并提供了正确性保证。与传统SOS优化相比，将设计CBC及其安全控制器的计算复杂度从网络维度的多项式增长降低到子系统数量的线性增长。在多个涉及未知模型和各种互连拓扑的物理网络上验证了其能力。", "conclusion": "所提出的数据驱动组合方法能够有效地为大规模未知网络设计鲁棒的去中心化安全控制器，显著降低了计算复杂性，并提供了可靠的安全保证。"}}
{"id": "2508.09349", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09349", "abs": "https://arxiv.org/abs/2508.09349", "authors": ["Cathy Speed", "Ahmed A. Metwally"], "title": "The Human-AI Hybrid Delphi Model: A Structured Framework for Context-Rich, Expert Consensus in Complex Domains", "comment": null, "summary": "Expert consensus plays a critical role in domains where evidence is complex,\nconflicting, or insufficient for direct prescription. Traditional methods, such\nas Delphi studies, consensus conferences, and systematic guideline synthesis,\noffer structure but face limitations including high panel burden, interpretive\noversimplification, and suppression of conditional nuance. These challenges are\nnow exacerbated by information overload, fragmentation of the evidence base,\nand increasing reliance on publicly available sources that lack expert\nfiltering. This study introduces and evaluates a Human-AI Hybrid Delphi\n(HAH-Delphi) framework designed to augment expert consensus development by\nintegrating a generative AI model (Gemini 2.5 Pro), small panels of senior\nhuman experts, and structured facilitation. The HAH-Delphi was tested in three\nphases: retrospective replication, prospective comparison, and applied\ndeployment in two applied domains (endurance training and resistance and mixed\ncardio/strength training). The AI replicated 95% of published expert consensus\nconclusions in Phase I and showed 95% directional agreement with senior human\nexperts in Phase II, though it lacked experiential and pragmatic nuance. In\nPhase III, compact panels of six senior experts achieved >90% consensus\ncoverage and reached thematic saturation before the final participant. The AI\nprovided consistent, literature-grounded scaffolding that supported divergence\nresolution and accelerated saturation. The HAH-Delphi framework offers a\nflexible, scalable approach for generating high-quality, context-sensitive\nconsensus. Its successful application across health, coaching, and performance\nscience confirms its methodological robustness and supports its use as a\nfoundation for generating conditional, personalised guidance and published\nconsensus frameworks at scale.", "AI": {"tldr": "本研究提出并评估了一种人机混合德尔菲（HAH-Delphi）框架，通过整合生成式AI、人类专家小组和结构化引导，以改进专家共识的形成过程，克服传统方法的局限性。", "motivation": "在证据复杂、冲突或不足的领域，专家共识至关重要。然而，传统的共识方法（如德尔菲研究、共识会议）面临专家负担重、解释过度简化和条件细微差别被压制等局限性。这些挑战因信息过载、证据基础碎片化以及对缺乏专家过滤的公开来源的日益依赖而加剧，亟需新的解决方案。", "method": "本研究引入并评估了人机混合德尔菲（HAH-Delphi）框架，该框架结合了生成式AI模型（Gemini 2.5 Pro）、小型高级人类专家小组和结构化引导。该框架通过三个阶段进行测试：回顾性复制（Phase I）、前瞻性比较（Phase II）和在两个应用领域（耐力训练和阻力与混合心肺/力量训练）的实际部署（Phase III）。", "result": "在第一阶段，AI复制了95%已发布的专家共识结论。在第二阶段，AI与高级人类专家达到了95%的方向性一致，但缺乏经验性和实用性细微差别。在第三阶段，由六名高级专家组成的紧凑小组在最终参与者之前就实现了超过90%的共识覆盖率并达到了主题饱和。AI提供了连贯、基于文献的支架作用，支持了分歧的解决并加速了饱和。", "conclusion": "HAH-Delphi框架为生成高质量、情境敏感的共识提供了一种灵活、可扩展的方法。其在健康、教练和运动表现科学领域的成功应用证实了其方法学的稳健性，并支持其作为大规模生成条件性、个性化指导和发布共识框架的基础。"}}
{"id": "2508.09188", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09188", "abs": "https://arxiv.org/abs/2508.09188", "authors": ["Seyed Muhammad Hossein Mousavi", "S. Younes Mirinezhad"], "title": "Synthetic Data Generation for Emotional Depth Faces: Optimizing Conditional DCGANs via Genetic Algorithms in the Latent Space and Stabilizing Training with Knowledge Distillation", "comment": null, "summary": "Affective computing faces a major challenge: the lack of high-quality,\ndiverse depth facial datasets for recognizing subtle emotional expressions. We\npropose a framework for synthetic depth face generation using an optimized GAN\nwith Knowledge Distillation (EMA teacher models) to stabilize training, improve\nquality, and prevent mode collapse. We also apply Genetic Algorithms to evolve\nGAN latent vectors based on image statistics, boosting diversity and visual\nquality for target emotions. The approach outperforms GAN, VAE, GMM, and KDE in\nboth diversity and quality. For classification, we extract and concatenate LBP,\nHOG, Sobel edge, and intensity histogram features, achieving 94% and 96%\naccuracy with XGBoost. Evaluation using FID, IS, SSIM, and PSNR shows\nconsistent improvement over state-of-the-art methods.", "AI": {"tldr": "本文提出一个框架，利用优化后的GAN（结合知识蒸馏和遗传算法）生成高质量、多样化的合成深度人脸数据，以解决情感计算中细微情感识别缺乏数据集的问题，并显著提升了识别准确率和生成质量。", "motivation": "情感计算面临的主要挑战是缺乏高质量、多样化的深度人脸数据集，用于识别细微的情绪表达。", "method": "1. 提出一个优化的GAN框架，结合知识蒸馏（使用EMA教师模型）以稳定训练、提高生成质量并防止模式崩溃。2. 应用遗传算法，基于图像统计数据进化GAN的潜在向量，以增强目标情绪的多样性和视觉质量。3. 对于分类，提取并拼接LBP、HOG、Sobel边缘和强度直方图特征。4. 使用XGBoost进行分类。5. 使用FID、IS、SSIM和PSNR等指标评估生成质量。", "result": "1. 在多样性和质量方面，该方法优于GAN、VAE、GMM和KDE。2. 使用XGBoost分类器实现了94%和96%的准确率。3. 在FID、IS、SSIM和PSNR等评估指标上，相对于最先进的方法表现出持续的改进。", "conclusion": "所提出的框架能够有效生成高质量、多样化的合成深度人脸数据，成功解决了情感计算中数据集不足的挑战，并显著提升了情绪识别的性能。"}}
{"id": "2508.09784", "categories": ["cs.AI", "cs.CC", "cs.LO"], "pdf": "https://arxiv.org/pdf/2508.09784", "abs": "https://arxiv.org/abs/2508.09784", "authors": ["Avijeet Ghosh", "Sujata Ghosh", "François Schwarzentruber"], "title": "Reasoning About Knowledge on Regular Expressions is 2EXPTIME-complete", "comment": "Accepted in KR 25", "summary": "Logics for reasoning about knowledge and actions have seen many applications\nin various domains of multi-agent systems, including epistemic planning. Change\nof knowledge based on observations about the surroundings forms a key aspect in\nsuch planning scenarios. Public Observation Logic (POL) is a variant of public\nannouncement logic for reasoning about knowledge that gets updated based on\npublic observations. Each state in an epistemic (Kripke) model is equipped with\na set of expected observations. These states evolve as the expectations get\nmatched with the actual observations. In this work, we prove that the\nsatisfiability problem of $\\POL$ is 2EXPTIME-complete.", "AI": {"tldr": "本文证明了公共观察逻辑（POL）的可满足性问题是2EXPTIME完全的。POL是一种用于推理基于公共观察更新知识的逻辑，在认知规划等领域有应用。", "motivation": "在多智能体系统（包括认知规划）中，对知识和行动进行推理的逻辑有广泛应用。其中，基于对周围环境的观察来改变知识是此类规划场景中的一个关键方面。", "method": "本文研究了公共观察逻辑（POL），它是公共宣布逻辑的一种变体。在POL中，认知（克里普克）模型中的每个状态都带有一组预期观察，并且状态会随着预期与实际观察的匹配而演变。研究方法是分析并证明其可满足性问题的计算复杂度。", "result": "研究结果证明，公共观察逻辑（POL）的可满足性问题是2EXPTIME完全的。", "conclusion": "该研究确定了公共观察逻辑（POL）可满足性问题的精确计算复杂度，为理解和应用这种处理基于观察的知识更新的逻辑提供了理论基础。"}}
{"id": "2508.09196", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.09196", "abs": "https://arxiv.org/abs/2508.09196", "authors": ["Asim Ukaye", "Numan Saeed", "Karthik Nandakumar"], "title": "FIVA: Federated Inverse Variance Averaging for Universal CT Segmentation with Uncertainty Estimation", "comment": "17 pages, 5 figures, Machine Learning for Healthcare Conference", "summary": "Different CT segmentation datasets are typically obtained from different\nscanners under different capture settings and often provide segmentation labels\nfor a limited and often disjoint set of organs. Using these heterogeneous data\neffectively while preserving patient privacy can be challenging. This work\npresents a novel federated learning approach to achieve universal segmentation\nacross diverse abdominal CT datasets by utilizing model uncertainty for\naggregation and predictive uncertainty for inference. Our approach leverages\nthe inherent noise in stochastic mini-batch gradient descent to estimate a\ndistribution over the model weights to provide an on-the-go uncertainty over\nthe model parameters at the client level. The parameters are then aggregated at\nthe server using the additional uncertainty information using a\nBayesian-inspired inverse-variance aggregation scheme. Furthermore, the\nproposed method quantifies prediction uncertainty by propagating the\nuncertainty from the model weights, providing confidence measures essential for\nclinical decision-making. In line with recent work shown, predictive\nuncertainty is utilized in the inference stage to improve predictive\nperformance. Experimental evaluations demonstrate the effectiveness of this\napproach in improving both the quality of federated aggregation and\nuncertainty-weighted inference compared to previously established baselines.\nThe code for this work is made available at: https://github.com/asimukaye/fiva", "AI": {"tldr": "本文提出一种新颖的联邦学习方法，通过利用模型不确定性进行聚合和预测不确定性进行推理，实现了跨异构腹部CT数据集的通用分割。", "motivation": "不同的CT分割数据集通常来自不同的扫描仪、捕获设置，且提供的器官分割标签有限且不相交，有效利用这些异构数据同时保护患者隐私极具挑战性。", "method": "提出一种联邦学习方法，利用随机小批量梯度下降固有的噪声来估计模型权重分布，从而在客户端层面提供模型参数的不确定性。服务器端采用贝叶斯启发的逆方差聚合方案，结合不确定性信息进行参数聚合。此外，通过传播模型权重的不确定性来量化预测不确定性，并在推理阶段利用预测不确定性来提高预测性能。", "result": "实验评估表明，与现有基线相比，该方法能有效提高联邦聚合的质量和不确定性加权推理的性能。", "conclusion": "所提出的联邦学习方法通过整合不确定性信息，显著提升了跨异构CT数据集的通用分割能力，并为临床决策提供了必要的置信度量。"}}
{"id": "2508.09508", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09508", "abs": "https://arxiv.org/abs/2508.09508", "authors": ["Reema Raval", "Shalabh Gupta"], "title": "SMART-OC: A Real-time Time-risk Optimal Replanning Algorithm for Dynamic Obstacles and Spatio-temporally Varying Currents", "comment": null, "summary": "Typical marine environments are highly complex with spatio-temporally varying\ncurrents and dynamic obstacles, presenting significant challenges to Unmanned\nSurface Vehicles (USVs) for safe and efficient navigation. Thus, the USVs need\nto continuously adapt their paths with real-time information to avoid\ncollisions and follow the path of least resistance to the goal via exploiting\nocean currents. In this regard, we introduce a novel algorithm, called\nSelf-Morphing Adaptive Replanning Tree for dynamic Obstacles and Currents\n(SMART-OC), that facilitates real-time time-risk optimal replanning in dynamic\nenvironments. SMART-OC integrates the obstacle risks along a path with the time\ncost to reach the goal to find the time-risk optimal path. The effectiveness of\nSMART-OC is validated by simulation experiments, which demonstrate that the USV\nperforms fast replannings to avoid dynamic obstacles and exploit ocean currents\nto successfully reach the goal.", "AI": {"tldr": "针对无人水面艇（USV）在复杂海洋环境中（动态障碍物和水流）的安全高效导航挑战，本文提出了一种名为SMART-OC的新型算法，用于实时时间-风险最优路径重规划。", "motivation": "典型的海洋环境复杂多变，存在时空变化的洋流和动态障碍物，给无人水面艇（USV）的安全高效导航带来了巨大挑战。USV需要根据实时信息持续调整路径，以避免碰撞并利用洋流以最小阻力到达目标。", "method": "本文提出了一种名为“动态障碍和水流自适应重规划树”（Self-Morphing Adaptive Replanning Tree for dynamic Obstacles and Currents, SMART-OC）的新算法。该算法将路径上的障碍物风险与到达目标的时间成本相结合，以找到时间-风险最优路径。", "result": "仿真实验验证了SMART-OC的有效性，结果表明USV能够快速重规划以避开动态障碍物并利用洋流成功到达目标。", "conclusion": "SMART-OC算法能够有效实现USV在动态海洋环境中的实时时间-风险最优重规划，成功应对动态障碍物并利用洋流进行高效导航。"}}
{"id": "2508.09536", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.09536", "abs": "https://arxiv.org/abs/2508.09536", "authors": ["Boris Kriuk", "Fedor Kriuk"], "title": "Shepherd Grid Strategy: Towards Reliable SWARM Interception", "comment": "9 pages, 5 figures", "summary": "Modern unmanned aerial vehicle threats require sophisticated interception\nstrategies that can overcome advanced evasion capabilities and operate\neffectively in contested environments. Traditional single-interceptor and\nuncoordinated multi-interceptor approaches suffer from fundamental limitations\nincluding inadequate coverage, predictable pursuit patterns, and vulnerability\nto intelligent evasion maneuvers. This paper introduces the Shepherd Grid\nStrategy, a new multi-phase coordination framework that employs pack-based\nbehavioral coordination to achieve deterministic target interception through\nsystematic containment and coordinated strike execution. The strategy\nimplements a four-phase operational model consisting of chase, follow,\nformation, and engagement phases, with dynamic role assignment and adaptive\nformation geometry that maintains persistent target pressure while preparing\noptimal strike opportunities. Our approach incorporates three key innovations:\nadaptive phase transition mechanisms that optimize pursuit behavior based on\nproximity and mission objectives, dynamic role assignment systems that\ndesignate specialized interceptor functions including formation maintenance and\nstrike execution, and predictive formation geometry algorithms that create\nmobile containment grids adapting to target movement patterns. The simulation\nexperiments demonstrate significant performance improvements over traditional\nmethods, achieving near-perfect interception success rates (over 95%) compared\nto traditional approaches (65%) and reducing median time-to-intercept.", "AI": {"tldr": "本文提出了一种名为“牧羊网格策略”的新型多阶段协调框架，通过群体行为协调实现对无人机威胁的确定性拦截，显著提高了拦截成功率并缩短了拦截时间。", "motivation": "现代无人机威胁具有先进的规避能力，且能在复杂环境中运行。传统的单拦截器和非协调多拦截器方法存在覆盖不足、追捕模式可预测以及易受智能规避影响等局限性，无法有效应对这些挑战。", "method": "引入“牧羊网格策略”，这是一种基于群体行为协调的多阶段框架。它包含追逐、跟随、编队和交战四个操作阶段，并具有动态角色分配和自适应编队几何。关键创新包括：1) 基于距离和任务目标的自适应阶段转换机制；2) 指定专业拦截器功能的动态角色分配系统；3) 创建适应目标移动模式的移动包围网格的预测编队几何算法。", "result": "仿真实验表明，该策略相较于传统方法有显著的性能提升，拦截成功率接近完美（超过95%，而传统方法为65%），并缩短了中位拦截时间。", "conclusion": "“牧羊网格策略”通过系统性的包围和协调打击执行，实现了对目标的确定性拦截，有效克服了传统方法的局限性，显著提高了拦截效率和成功率。"}}
{"id": "2508.09350", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.09350", "abs": "https://arxiv.org/abs/2508.09350", "authors": ["Ju-Chieh Chou", "Jiawei Zhou", "Karen Livescu"], "title": "Flow-SLM: Joint Learning of Linguistic and Acoustic Information for Spoken Language Modeling", "comment": "Accepted to ASRU 2025", "summary": "Textless spoken language models (SLMs) are generative models of speech that\ndo not rely on text supervision. Most textless SLMs learn to predict the next\nsemantic token, a discrete representation of linguistic content, and rely on a\nseparate vocoder to add acoustic information to the generated speech. Such\nmodels have no access to acoustic context and no built-in control over acoustic\ndetails. In this work, we propose to jointly model linguistic and acoustic\ninformation by generating semantic tokens and a continuous real-valued\nrepresentation of the acoustic frame. We use a flow-matching objective to\npredict the continuous vector conditioned on the semantic tokens. We study the\ndesign space of this approach and find that predicting multiple future semantic\ntokens helps preserve linguistic information. Our approach achieves comparable\nperformance to existing models in terms of linguistic likelihood benchmarks,\nwhile providing better acoustic detail in prompted generation.", "AI": {"tldr": "本文提出了一种新的无文本语音语言模型，通过联合建模语言语义token和连续声学帧表示，解决了现有模型缺乏声学细节控制的问题，并在保持语言性能的同时提升了声学质量。", "motivation": "大多数无文本语音语言模型仅预测离散的语义token，并依赖单独的声码器添加声学信息，导致模型无法获取声学上下文，也无法内置控制声学细节。", "method": "提出联合建模语言信息（语义token）和声学信息（连续实值声学帧表示）。使用流匹配（flow-matching）目标来预测以语义token为条件的连续向量。研究发现预测多个未来语义token有助于保留语言信息。", "result": "该方法在语言似然基准测试中达到了与现有模型相当的性能，同时在提示生成中提供了更好的声学细节。", "conclusion": "通过联合建模语言和声学信息，该方法在无文本语音语言模型中实现了与现有模型相当的语言性能，并显著提升了声学细节质量。"}}
{"id": "2508.09199", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.09199", "abs": "https://arxiv.org/abs/2508.09199", "authors": ["Jucheng Hu", "Suorong Yang", "Dongzhan Zhou"], "title": "$Δ$-AttnMask: Attention-Guided Masked Hidden States for Efficient Data Selection and Augmentation", "comment": null, "summary": "Visual Instruction Finetuning (VIF) is pivotal for post-training\nVision-Language Models (VLMs). Unlike unimodal instruction finetuning in\nplain-text large language models, which mainly requires instruction datasets to\nenable model instruction-following ability, VIF also requires multimodal data\nto enable joint visual and textual understanding; therefore, it typically\nrequires more data. Consequently, VIF imposes stricter data selection\nchallenges: the method must scale efficiently to handle larger data demands\nwhile ensuring the quality of both visual and textual content, as well as their\nalignment. Despite its critical impact on performance, data selection for VIF\nremains an understudied area. In this paper, we propose $\\Delta$-AttnMask. This\ndata-efficient framework quantifies sample quality through attention-guided\nmasking of the model's hidden states, jointly evaluating image-text pairs\nwithout requiring domain labels, auxiliary models, or extra training. By\ncomputing loss differences ($\\Delta$) between the original states and states\nmasked using high-attention regions, $\\Delta$-AttnMask intrinsically assesses\nsample quality. Experiments across multiple VLMs and datasets show that\n$\\Delta$-AttnMask achieves state-of-the-art performance with just 20% of data,\naccelerating training by 5x while surpassing full-dataset baselines by +10.1%\nin overall accuracy. Its model-agnostic and data-agnostic design ensures broad\napplicability across modalities and architectures.", "AI": {"tldr": "本文提出了一种名为 Δ-AttnMask 的高效数据选择框架，用于视觉指令微调（VIF），它通过注意力引导掩码量化样本质量，在仅使用20%数据的情况下，实现了更优的性能和更快的训练速度。", "motivation": "视觉指令微调（VIF）需要大量的多模态数据以实现视觉和文本的联合理解，这带来了严格的数据选择挑战。尽管数据选择对VIF性能至关重要，但该领域研究不足。", "method": "本文提出了 Δ-AttnMask 框架。它通过对模型隐藏状态进行注意力引导掩码，并计算原始状态与高注意力区域掩码后状态之间的损失差异（Δ），来内在评估图像-文本对的样本质量。该方法无需领域标签、辅助模型或额外训练，可联合评估。", "result": "在多个VLM和数据集上的实验表明，Δ-AttnMask 仅使用20%的数据就达到了最先进的性能，训练速度加快了5倍，并且在整体准确性上比使用完整数据集的基线高出+10.1%。", "conclusion": "Δ-AttnMask 是一个高效、模型无关和数据无关的数据选择解决方案，能够显著提升VIF的训练效率和性能，具有广泛的适用性。"}}
{"id": "2508.09860", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09860", "abs": "https://arxiv.org/abs/2508.09860", "authors": ["In-Chang Baek", "Seoyoung Lee", "Sung-Hyun Kim", "Geumhwan Hwang", "KyungJoong Kim"], "title": "Human-Aligned Procedural Level Generation Reinforcement Learning via Text-Level-Sketch Shared Representation", "comment": "9 pages, 6 tables, 3 figures", "summary": "Human-aligned AI is a critical component of co-creativity, as it enables\nmodels to accurately interpret human intent and generate controllable outputs\nthat align with design goals in collaborative content creation. This direction\nis especially relevant in procedural content generation via reinforcement\nlearning (PCGRL), which is intended to serve as a tool for human designers.\nHowever, existing systems often fall short of exhibiting human-centered\nbehavior, limiting the practical utility of AI-driven generation tools in\nreal-world design workflows. In this paper, we propose VIPCGRL\n(Vision-Instruction PCGRL), a novel deep reinforcement learning framework that\nincorporates three modalities-text, level, and sketches-to extend control\nmodality and enhance human-likeness. We introduce a shared embedding space\ntrained via quadruple contrastive learning across modalities and human-AI\nstyles, and align the policy using an auxiliary reward based on embedding\nsimilarity. Experimental results show that VIPCGRL outperforms existing\nbaselines in human-likeness, as validated by both quantitative metrics and\nhuman evaluations. The code and dataset will be available upon publication.", "AI": {"tldr": "VIPCGRL是一个新颖的深度强化学习框架，通过整合文本、关卡和草图三种模态，并利用四重对比学习训练共享嵌入空间，显著提升了程序内容生成（PCGRL）中AI的类人性和与人类意图的对齐程度。", "motivation": "人类对齐的AI对于协同创作至关重要，尤其是在强化学习的程序内容生成（PCGRL）中，AI应作为人类设计师的工具。然而，现有系统在展示以人为中心的行为方面存在不足，限制了AI驱动生成工具在实际设计工作流中的实用性。", "method": "提出了VIPCGRL框架，它是一个深度强化学习方法，引入文本、关卡和草图三种模态来扩展控制方式并增强类人性。该方法通过跨模态和人机风格的四重对比学习训练一个共享嵌入空间，并使用基于嵌入相似度的辅助奖励来对齐策略。", "result": "实验结果表明，VIPCGRL在类人性方面优于现有基线，这通过定量指标和人类评估得到了验证。", "conclusion": "VIPCGRL通过多模态输入和对比学习成功地提升了PCGRL中AI的类人性，使其更好地与人类意图对齐，从而提高了AI驱动生成工具在实际设计工作流中的实用性。"}}
{"id": "2508.09200", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09200", "abs": "https://arxiv.org/abs/2508.09200", "authors": ["Jinho Kim", "Marcel Dominik Nickel", "Florian Knoll"], "title": "Zero-shot self-supervised learning of single breath-hold magnetic resonance cholangiopancreatography (MRCP) reconstruction", "comment": "23 pages, 6 figures, 2 tabels", "summary": "Purpose: To investigate the feasibility of applying zero-shot self-supervised\nlearning reconstruction to reduce breath-hold times in magnetic resonance\ncholangiopancreatography (MRCP). Methods: Breath-hold MRCP was acquired from 11\nhealthy volunteers on a 3T scanner using an incoherent k-space sampling pattern\nleading to a breath-hold duration of 14s. We evaluated zero-shot reconstruction\nof breath-hold MRCP against parallel imaging of respiratory-triggered MRCP\nacquired in 338s on average and compressed sensing reconstruction of\nbreath-hold MRCP. To address the long computation times of zero-shot trainings,\nwe used a training approach that leverages a pretrained network to reduce\nbackpropagation depth during training. Results: Zero-shot learning\nreconstruction significantly improved visual image quality compared to\ncompressed sensing reconstruction, particularly in terms of signal-to-noise\nratio and ductal delineation, and reached a level of quality comparable to that\nof successful respiratory-triggered acquisitions with regular breathing\npatterns. Shallow training provided nearly equivalent reconstruction\nperformance with a training time of 11 minutes in comparison to 271 minutes for\na conventional zero-shot training. Conclusion: Zero-shot learning delivers\nhigh-fidelity MRCP reconstructions with reduced breath-hold times, and shallow\ntraining offers a practical solution for translation to time-constrained\nclinical workflows.", "AI": {"tldr": "该研究探索了零样本自监督学习重建技术在磁共振胰胆管成像（MRCP）中缩短屏气时间的可行性，并提出浅层训练方法以提高临床实用性。", "motivation": "传统的MRCP检查通常需要很长的屏气时间或呼吸触发采集（平均338秒），这给患者带来不便，且压缩感知等方法重建图像质量可能受损。本研究旨在寻找一种能显著缩短屏气时间同时保持高质量图像的MRCP重建方法。", "method": "研究从11名健康志愿者采集了14秒的屏气MRCP数据，采用非相干k空间采样模式。将零样本重建与呼吸触发MRCP的并行成像（平均338秒）和屏气MRCP的压缩感知重建进行比较。为解决零样本训练计算时间长的问题，引入了一种利用预训练网络减少反向传播深度的“浅层训练”方法。", "result": "零样本学习重建显著提高了图像视觉质量，特别是在信噪比和导管描绘方面，优于压缩感知重建，并达到了与成功呼吸触发采集相当的质量水平。浅层训练在11分钟内提供了几乎等效的重建性能，远低于传统零样本训练的271分钟。", "conclusion": "零样本学习能够以更短的屏气时间实现高保真MRCP重建。浅层训练则为将该技术应用于时间受限的临床工作流程提供了实用的解决方案。"}}
{"id": "2508.09558", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09558", "abs": "https://arxiv.org/abs/2508.09558", "authors": ["Jiahui Zuo", "Boyang Zhang", "Fumin Zhang"], "title": "CaRoBio: 3D Cable Routing with a Bio-inspired Gripper Fingernail", "comment": null, "summary": "The manipulation of deformable linear flexures has a wide range of\napplications in industry, such as cable routing in automotive manufacturing and\ntextile production. Cable routing, as a complex multi-stage robot manipulation\nscenario, is a challenging task for robot automation. Common parallel\ntwo-finger grippers have the risk of over-squeezing and over-tension when\ngrasping and guiding cables. In this paper, a novel eagle-inspired fingernail\nis designed and mounted on the gripper fingers, which helps with cable grasping\non planar surfaces and in-hand cable guiding operations. Then we present a\nsingle-grasp end-to-end 3D cable routing framework utilizing the proposed\nfingernails, instead of the common pick-and-place strategy. Continuous control\nis achieved to efficiently manipulate cables through vision-based state\nestimation of task configurations and offline trajectory planning based on\nmotion primitives. We evaluate the effectiveness of the proposed framework with\na variety of cables and channel slots, significantly outperforming the\npick-and-place manipulation process under equivalent perceptual conditions. Our\nreconfigurable task setting and the proposed framework provide a reference for\nfuture cable routing manipulations in 3D space.", "AI": {"tldr": "本文提出了一种受鹰爪启发的指甲状夹持器设计，并结合单次抓取、端到端的三维线缆布线框架，通过视觉估计和运动基元规划实现连续控制，显著优于传统的抓取-放置策略。", "motivation": "线缆布线是机器人自动化中的一个复杂挑战，常见双指夹持器在抓取和引导线缆时存在过度挤压和拉伸的风险。此外，传统的抓取-放置策略效率低下。", "method": "设计了一种新型的受鹰启发的指甲状结构并安装在夹持器手指上，以辅助线缆抓取和手内引导操作。提出了一种利用该指甲的单次抓取、端到端三维线缆布线框架。通过基于视觉的任务配置状态估计和基于运动基元的离线轨迹规划，实现了对线缆的连续高效操作。", "result": "所提出的框架在多种线缆和通道槽上进行了有效性评估，在同等感知条件下，其性能显著优于传统的抓取-放置操作过程。", "conclusion": "该研究提出的可重构任务设置和框架为未来三维空间中的线缆布线操作提供了重要参考，证明了新型夹持器设计和连续控制策略在复杂线缆操作中的有效性。"}}
{"id": "2508.09678", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.09678", "abs": "https://arxiv.org/abs/2508.09678", "authors": ["Alexander Roocroft", "Marco Rinaldi"], "title": "Metering traffic flows for perimeter control through auction-based signalling using connected vehicles", "comment": null, "summary": "Urban traffic congestion remains a critical challenge in modern cities, with\ntraffic signal control systems often struggling to manage congestion during\npeak travel times. Perimeter control of a Protected Network (PN) has emerged as\na potential solution to reducing gridlock in urban networks. This paper\nproposes a novel auction-based mechanism for green time allocation at\nsignalized intersections, for effective perimeter control application.\nUtilising a Sealed Bid, Second Price auction framework, our approach combines\nreal-time traffic monitoring with market-inspired mechanisms to regulate\nvehicle inflows into PN areas. Unlike existing methods that focus primarily on\ngated links, our system allocates budgets to individual traffic movements,\nproviding greater flexibility in managing multi-directional flows. We evaluate\nthe proposed mechanism using a test case intersection with a single controlled\ninflow, comparing it against a volume-based fixed-time approach. The results\ndemonstrate that our auction-based method controls flows into the PN with\nimproved accuracy, outperforming the volume-based approach in terms of inflow\nregulation, queue management and delays. The framework can be applied in real\ntime to any generic intersection, offering a scalable solution for urban\ntraffic management. This work bridges the gap between perimeter control and\nmarket-based intersection auctions, providing a pathway for further research on\nadaptive traffic management systems.", "AI": {"tldr": "本文提出了一种新颖的基于拍卖的绿灯时间分配机制，用于信号交叉口的外围控制，以有效管理城市交通拥堵，并在流入控制、排队管理和延误方面优于传统方法。", "motivation": "城市交通拥堵是一个严峻挑战，现有交通信号控制系统在高峰期难以有效管理。外围控制作为减少城市网络交通堵塞的潜在解决方案被提出，但需要更有效的绿灯时间分配机制。", "method": "本文提出了一种基于密封投标、第二价格拍卖框架的绿灯时间分配机制，将实时交通监测与市场机制相结合，以调节车辆进入保护区域（PN）的流量。与现有方法不同，该系统为单个交通流分配预算，而非仅关注门控连接。", "result": "通过在一个单受控流入的测试交叉口进行评估，该拍卖机制在控制PN流入方面表现出更高的准确性，并在流入调节、排队管理和延误方面优于基于流量的固定时间方法。", "conclusion": "该拍卖机制可以实时应用于任何通用交叉口，提供了一种可扩展的城市交通管理解决方案。这项工作弥合了外围控制与基于市场的交叉口拍卖之间的差距，为自适应交通管理系统的进一步研究提供了途径。"}}
{"id": "2508.09378", "categories": ["cs.CL", "cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.09378", "abs": "https://arxiv.org/abs/2508.09378", "authors": ["Artem Chernodub", "Aman Saini", "Yejin Huh", "Vivek Kulkarni", "Vipul Raheja"], "title": "APIO: Automatic Prompt Induction and Optimization for Grammatical Error Correction and Text Simplification", "comment": "Accepted for publication at Recent Advances in Natural Language\n  Processing conference (RANLP 2025)", "summary": "Recent advancements in large language models (LLMs) have enabled a wide range\nof natural language processing (NLP) tasks to be performed through simple\nprompt-based interactions. Consequently, several approaches have been proposed\nto engineer prompts that most effectively enable LLMs to perform a given task\n(e.g., chain-of-thought prompting). In settings with a well-defined metric to\noptimize model performance, automatic prompt optimization (APO) methods have\nbeen developed to refine a seed prompt. Advancing this line of research, we\npropose APIO, a simple but effective prompt induction and optimization approach\nfor the tasks of Grammatical Error Correction (GEC) and Text Simplification,\nwithout relying on manually specified seed prompts. APIO achieves a new\nstate-of-the-art performance for purely LLM-based prompting methods on these\ntasks. We make our data, code, prompts, and outputs publicly available.", "AI": {"tldr": "本文提出APIO，一种用于语法纠错（GEC）和文本简化任务的提示归纳与优化方法，无需手动指定初始提示，并在纯LLM提示方法中达到了新的SOTA性能。", "motivation": "LLMs通过提示进行NLP任务，已有自动提示优化（APO）方法来优化给定初始提示。然而，研究缺乏在没有手动指定初始提示的情况下，如何有效归纳和优化提示以提升模型性能。", "method": "提出了APIO（Prompt Induction and Optimization），一种简单但有效的提示归纳和优化方法。该方法不依赖于手动指定的初始提示，并应用于语法纠错（GEC）和文本简化任务。", "result": "APIO在语法纠错（GEC）和文本简化任务上，为纯LLM提示方法实现了新的最先进（SOTA）性能。研究还公开了数据、代码、提示和输出。", "conclusion": "APIO是一种无需手动初始提示即可有效归纳和优化LLM提示的方法，显著提升了LLM在语法纠错和文本简化任务上的表现，达到了纯LLM提示的新高度。"}}
{"id": "2508.09202", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09202", "abs": "https://arxiv.org/abs/2508.09202", "authors": ["Masoumeh Sharafi", "Soufiane Belharbi", "Houssem Ben Salem", "Ali Etemad", "Alessandro Lameiras Koerich", "Marco Pedersoli", "Simon Bacon", "Eric Granger"], "title": "Personalized Feature Translation for Expression Recognition: An Efficient Source-Free Domain Adaptation Method", "comment": null, "summary": "Facial expression recognition (FER) models are employed in many video-based\naffective computing applications, such as human-computer interaction and\nhealthcare monitoring. However, deep FER models often struggle with subtle\nexpressions and high inter-subject variability, limiting their performance in\nreal-world applications. To improve their performance, source-free domain\nadaptation (SFDA) methods have been proposed to personalize a pretrained source\nmodel using only unlabeled target domain data, thereby avoiding data privacy,\nstorage, and transmission constraints. This paper addresses a challenging\nscenario where source data is unavailable for adaptation, and only unlabeled\ntarget data consisting solely of neutral expressions is available. SFDA methods\nare not typically designed to adapt using target data from only a single class.\nFurther, using models to generate facial images with non-neutral expressions\ncan be unstable and computationally intensive. In this paper, personalized\nfeature translation (PFT) is proposed for SFDA. Unlike current image\ntranslation methods for SFDA, our lightweight method operates in the latent\nspace. We first pre-train the translator on the source domain data to transform\nthe subject-specific style features from one source subject into another.\nExpression information is preserved by optimizing a combination of expression\nconsistency and style-aware objectives. Then, the translator is adapted on\nneutral target data, without using source data or image synthesis. By\ntranslating in the latent space, PFT avoids the complexity and noise of face\nexpression generation, producing discriminative embeddings optimized for\nclassification. Using PFT eliminates the need for image synthesis, reduces\ncomputational overhead (using a lightweight translator), and only adapts part\nof the model, making the method efficient compared to image-based translation.", "AI": {"tldr": "本文提出了一种名为个性化特征翻译（PFT）的源免域适应（SFDA）方法，用于面部表情识别（FER），特别是在只有无标签中性表情目标数据可用的挑战性场景下，通过在潜在空间进行翻译以避免图像合成。", "motivation": "深度FER模型在处理细微表情和高个体差异时表现不佳，限制了其在实际应用中的性能。SFDA方法旨在利用无标签目标域数据个性化预训练模型，但它们通常不适用于仅包含单一类别（如中性表情）的目标数据。此外，使用模型生成非中性表情图像既不稳定又计算密集。", "method": "提出个性化特征翻译（PFT）。该方法在潜在空间而非图像空间进行操作。首先在源域数据上预训练一个翻译器，用于转换不同源受试者的特定于主体的风格特征，同时通过优化表情一致性和风格感知目标来保留表情信息。然后，仅使用中性目标数据对翻译器进行适应，无需源数据或图像合成。通过在潜在空间进行翻译，PFT避免了面部表情生成的复杂性和噪声。", "result": "PFT方法能够生成用于分类的判别性嵌入，避免了图像合成的需求，降低了计算开销（使用轻量级翻译器），并且只适应模型的一部分，相比基于图像的翻译方法更为高效。", "conclusion": "PFT为FER提供了一种高效的SFDA解决方案，尤其适用于源数据不可用且目标数据仅包含中性表情的场景。它通过在潜在空间进行特征翻译，有效解决了传统SFDA方法和图像合成的局限性。"}}
{"id": "2508.09889", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09889", "abs": "https://arxiv.org/abs/2508.09889", "authors": ["Zhitian Xie", "Qintong Wu", "Chengyue Yu", "Chenyi Zhuang", "Jinjie Gu"], "title": "AWorld: Dynamic Multi-Agent System with Stable Maneuvering for Robust GAIA Problem Solving", "comment": null, "summary": "The rapid advancement of large language models (LLMs) has empowered\nintelligent agents to leverage diverse external tools for solving complex\nreal-world problems. However, as agents increasingly depend on multiple tools,\nthey encounter new challenges: extended contexts from disparate sources and\nnoisy or irrelevant tool outputs can undermine system reliability and accuracy.\nThese challenges underscore the necessity for enhanced stability in agent-based\nsystems. To address this, we introduce dynamic supervision and maneuvering\nmechanisms, constructing a robust and dynamic Multi-Agent System (MAS)\narchitecture within the AWorld framework. In our approach, the Execution Agent\ninvokes the Guard Agent at critical steps to verify and correct the reasoning\nprocess, effectively reducing errors arising from noise and bolstering\nproblem-solving robustness. Extensive experiments on the GAIA test dataset\nreveal that our dynamic maneuvering mechanism significantly improves both the\neffectiveness and stability of solutions, outperforming single-agent system\n(SAS) and standard tool-augmented systems. As a result, our dynamic MAS system\nachieved first place among open-source projects on the prestigious GAIA\nleaderboard. These findings highlight the practical value of collaborative\nagent roles in developing more reliable and trustworthy intelligent systems.", "AI": {"tldr": "该论文提出了一种动态多智能体系统（MAS）架构，通过引入监督和操纵机制，特别是利用“守卫智能体”来验证和纠正推理过程，显著提高了大型语言模型（LLM）驱动的工具增强型智能体的鲁棒性和准确性。", "motivation": "随着智能体越来越依赖多种外部工具解决复杂问题，它们面临来自不同来源的扩展上下文以及噪声或不相关的工具输出等挑战，这些因素会损害系统可靠性和准确性，因此需要增强智能体系统的稳定性。", "method": "引入了动态监督和操纵机制，在AWorld框架内构建了一个鲁棒且动态的多智能体系统（MAS）架构。具体来说，执行智能体在关键步骤调用守卫智能体来验证和纠正推理过程，从而有效减少噪声引起的错误并增强问题解决的鲁棒性。", "result": "在GAIA测试数据集上的广泛实验表明，该动态操纵机制显著提高了解决方案的有效性和稳定性，优于单智能体系统（SAS）和标准工具增强系统。该动态MAS系统在GAIA排行榜的开源项目中获得了第一名。", "conclusion": "研究结果突出了协作智能体角色在开发更可靠、更值得信赖的智能系统方面的实际价值。"}}
{"id": "2508.09205", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09205", "abs": "https://arxiv.org/abs/2508.09205", "authors": ["Yoni Schirris", "Eric Marcus", "Jonas Teuwen", "Hugo Horlings", "Efstratios Gavves"], "title": "From Explainable to Explained AI: Ideas for Falsifying and Quantifying Explanations", "comment": "10 pages, 2 figures, 2 tables, submitted at MICCAI IMIMIC workshop", "summary": "Explaining deep learning models is essential for clinical integration of\nmedical image analysis systems. A good explanation highlights if a model\ndepends on spurious features that undermines generalization and harms a subset\nof patients or, conversely, may present novel biological insights. Although\ntechniques like GradCAM can identify influential features, they are measurement\ntools that do not themselves form an explanation. We propose a\nhuman-machine-VLM interaction system tailored to explaining classifiers in\ncomputational pathology, including multi-instance learning for whole-slide\nimages. Our proof of concept comprises (1) an AI-integrated slide viewer to run\nsliding-window experiments to test claims of an explanation, and (2)\nquantification of an explanation's predictiveness using general-purpose\nvision-language models. The results demonstrate that this allows us to\nqualitatively test claims of explanations and can quantifiably distinguish\ncompeting explanations. This offers a practical path from explainable AI to\nexplained AI in digital pathology and beyond. Code and prompts are available at\nhttps://github.com/nki-ai/x2x.", "AI": {"tldr": "该研究提出了一种人机-视觉语言模型（VLM）交互系统，用于解释计算病理学中的深度学习分类器，旨在将可解释AI转化为已解释AI，并能定性测试和定量区分解释。", "motivation": "深度学习模型解释对于医学图像分析系统在临床中的整合至关重要。好的解释能揭示模型是否依赖虚假特征（影响泛化和伤害患者）或提供新的生物学见解。现有技术如GradCAM仅是测量工具，本身不构成解释，因此需要更完善的解释系统。", "method": "本文提出了一种专为计算病理学分类器（包括全切片图像的多实例学习）量身定制的人机-VLM交互系统。其概念验证包括：1) 一个集成AI的玻片查看器，用于运行滑动窗口实验以测试解释的主张；2) 使用通用视觉语言模型对解释的预测性进行量化。", "result": "研究结果表明，该系统能够定性测试解释的主张，并能定量区分相互竞争的解释。", "conclusion": "该系统为数字病理学及其他领域从可解释AI迈向已解释AI提供了一条切实可行的路径。"}}
{"id": "2508.09581", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.09581", "abs": "https://arxiv.org/abs/2508.09581", "authors": ["Junkai Jiang", "Yihe Chen", "Yibin Yang", "Ruochen Li", "Shaobing Xu", "Jianqiang Wang"], "title": "ESCoT: An Enhanced Step-based Coordinate Trajectory Planning Method for Multiple Car-like Robots", "comment": null, "summary": "Multi-vehicle trajectory planning (MVTP) is one of the key challenges in\nmulti-robot systems (MRSs) and has broad applications across various fields.\nThis paper presents ESCoT, an enhanced step-based coordinate trajectory\nplanning method for multiple car-like robots. ESCoT incorporates two key\nstrategies: collaborative planning for local robot groups and replanning for\nduplicate configurations. These strategies effectively enhance the performance\nof step-based MVTP methods. Through extensive experiments, we show that ESCoT\n1) in sparse scenarios, significantly improves solution quality compared to\nbaseline step-based method, achieving up to 70% improvement in typical conflict\nscenarios and 34% in randomly generated scenarios, while maintaining high\nsolving efficiency; and 2) in dense scenarios, outperforms all baseline\nmethods, maintains a success rate of over 50% even in the most challenging\nconfigurations. The results demonstrate that ESCoT effectively solves MVTP,\nfurther extending the capabilities of step-based methods. Finally, practical\nrobot tests validate the algorithm's applicability in real-world scenarios.", "AI": {"tldr": "ESCoT是一种增强的基于步长的多车轨迹规划方法，通过协作规划和重复配置重规划，显著提升了稀疏和密集场景下车状机器人的轨迹规划性能和成功率。", "motivation": "多车轨迹规划（MVTP）是多机器人系统（MRS）中的一个关键挑战，具有广泛的应用前景。现有基于步长的MVTP方法需要性能提升。", "method": "本文提出了ESCoT，一种增强的基于步长的坐标轨迹规划方法，用于多辆车状机器人。它结合了两个关键策略：针对局部机器人群的协作规划和针对重复配置的重规划。", "result": "1) 在稀疏场景中，ESCoT相比基线方法显著提高了解决方案质量（典型冲突场景提升高达70%，随机生成场景提升34%），同时保持了高求解效率。2) 在密集场景中，ESCoT优于所有基线方法，即使在最具挑战性的配置下也能保持超过50%的成功率。实际机器人测试验证了算法在现实场景中的适用性。", "conclusion": "ESCoT有效解决了多车轨迹规划问题，进一步扩展了基于步长方法的能力，并已通过实际机器人测试验证了其在现实世界中的适用性。"}}
{"id": "2508.09682", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.09682", "abs": "https://arxiv.org/abs/2508.09682", "authors": ["Nicola Anselmi", "Paolo Rocca", "Giovanni Toso", "Andrea Massa"], "title": "A Divide-and-Conquer Tiling Method for the Design of Large Aperiodic Phased Arrays", "comment": null, "summary": "Due to the growing request from modern wireless applications of\ncost-affordable and high-gain scanning antenna solutions, the design of large\nphased arrays (PAs) with radiating elements organized into modular clusters\nwith sub-array-only amplitude and phase control is a key topic. In this paper,\nan innovative irregular tiling method is proposed where, according to a\ndivide-and-conquer strategy, the antenna aperture is subdivided into sub-areas\nthat are locally domino-tiled by jointly fulfilling the full-coverage condition\non the remaining untiled part of the PA support. Selected representative\nresults, including comparisons with competitive state-of-the-art synthesis\nmethods, are reported to prove the effectiveness and the computational\nefficiency of the proposed tiling approach. Use-cases of current relevance for\nlow Earth orbit (LEO) satellite communications are discussed, as well, to\nprovide the antenna designers useful practical guidelines for handling large\nPAs.", "AI": {"tldr": "本文提出了一种创新的不规则平铺方法，用于设计具有模块化子阵列控制的大型相控阵天线，以满足现代无线应用对高增益扫描天线的需求，并证明了其有效性和计算效率。", "motivation": "现代无线应用对经济高效、高增益的扫描天线解决方案需求日益增长，这使得设计具有模块化子阵列振幅和相位控制的大型相控阵（PA）成为一个关键课题。", "method": "提出了一种创新的不规则平铺方法，该方法采用分而治之的策略，将天线孔径细分为子区域，并通过局部多米诺骨牌式平铺，同时满足相控阵支撑上剩余未平铺部分的完全覆盖条件。", "result": "报告了具有代表性的结果，包括与现有先进合成方法的比较，证明了所提出平铺方法的有效性和计算效率。此外，还讨论了当前与低地球轨道（LEO）卫星通信相关的用例，为天线设计者提供了处理大型相控阵的实用指导。", "conclusion": "所提出的不规则平铺方法对于设计具有模块化子阵列控制的大型相控阵天线是有效且计算高效的，特别适用于低地球轨道卫星通信等相关应用，并为天线设计者提供了实用指导。"}}
{"id": "2508.09403", "categories": ["cs.CL", "cs.DB"], "pdf": "https://arxiv.org/pdf/2508.09403", "abs": "https://arxiv.org/abs/2508.09403", "authors": ["Ting Cai", "Stephen Sheen", "AnHai Doan"], "title": "Columbo: Expanding Abbreviated Column Names for Tabular Data Using Large Language Models", "comment": null, "summary": "Expanding the abbreviated column names of tables, such as ``esal'' to\n``employee salary'', is critical for numerous downstream data tasks. This\nproblem arises in enterprises, domain sciences, government agencies, and more.\nIn this paper we make three contributions that significantly advances the state\nof the art. First, we show that synthetic public data used by prior work has\nmajor limitations, and we introduce 4 new datasets in enterprise/science\ndomains, with real-world abbreviations. Second, we show that accuracy measures\nused by prior work seriously undercount correct expansions, and we propose new\nsynonym-aware measures that capture accuracy much more accurately. Finally, we\ndevelop Columbo, a powerful LLM-based solution that exploits context, rules,\nchain-of-thought reasoning, and token-level analysis. Extensive experiments\nshow that Columbo significantly outperforms NameGuess, the current most\nadvanced solution, by 4-29\\%, over 5 datasets. Columbo has been used in\nproduction on EDI, a major data portal for environmental sciences.", "AI": {"tldr": "该论文提出了一种新的基于LLM的解决方案Columbo，用于扩展表格中的缩写列名，解决了现有方法在数据、评估指标和性能上的不足，并在真实世界数据上取得了显著优于SOTA的性能。", "motivation": "扩展表格中的缩写列名（如“esal”到“employee salary”）对于许多下游数据任务至关重要，且广泛存在于企业、科学和政府机构中。现有工作存在公共合成数据局限性大、准确性度量方法严重低估正确扩展等问题。", "method": "1. 引入了4个新的、包含真实世界缩写的企业/科学领域数据集。2. 提出了新的、能更准确捕捉准确性的同义词感知度量方法。3. 开发了Columbo，一个强大的基于LLM的解决方案，利用上下文、规则、思维链推理和令牌级分析。", "result": "实验表明，Columbo在5个数据集上比当前最先进的解决方案NameGuess性能显著提高4-29%。Columbo已在环保科学数据门户EDI中投入生产使用。", "conclusion": "Columbo显著提升了表格缩写列名扩展的最新技术水平，通过引入真实世界数据、更准确的评估指标以及强大的LLM驱动方法，解决了现有方法的关键局限性，并已在实际应用中得到验证。"}}
{"id": "2508.09207", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.09207", "abs": "https://arxiv.org/abs/2508.09207", "authors": ["Tai Vu", "Robert Yang"], "title": "GANime: Generating Anime and Manga Character Drawings from Sketches with Deep Learning", "comment": null, "summary": "The process of generating fully colorized drawings from sketches is a large,\nusually costly bottleneck in the manga and anime industry. In this study, we\nexamine multiple models for image-to-image translation between anime characters\nand their sketches, including Neural Style Transfer, C-GAN, and CycleGAN. By\nassessing them qualitatively and quantitatively, we find that C-GAN is the most\neffective model that is able to produce high-quality and high-resolution images\nclose to those created by humans.", "AI": {"tldr": "本研究评估了多种图像到图像转换模型（包括Neural Style Transfer、C-GAN和CycleGAN），以实现从动漫草图到全彩图像的自动生成，发现C-GAN效果最佳。", "motivation": "漫画和动漫产业中，从草图生成全彩绘图是一个巨大且通常成本高昂的瓶颈，本研究旨在解决这一问题。", "method": "研究评估了多种用于动漫角色及其草图之间图像到图像转换的模型，包括Neural Style Transfer、C-GAN和CycleGAN，并通过定性和定量方法进行评估。", "result": "研究发现，C-GAN是能够生成接近人类创作的高质量、高分辨率图像的最有效模型。", "conclusion": "C-GAN是动漫草图自动上色任务中表现最佳的模型，能有效缓解行业瓶颈。"}}
{"id": "2508.09893", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09893", "abs": "https://arxiv.org/abs/2508.09893", "authors": ["Bhavik Agarwal", "Hemant Sunil Jomraj", "Simone Kaplunov", "Jack Krolick", "Viktoria Rojkova"], "title": "RAGulating Compliance: A Multi-Agent Knowledge Graph for Regulatory QA", "comment": null, "summary": "Regulatory compliance question answering (QA) requires precise, verifiable\ninformation, and domain-specific expertise, posing challenges for Large\nLanguage Models (LLMs). In this work, we present a novel multi-agent framework\nthat integrates a Knowledge Graph (KG) of Regulatory triplets with\nRetrieval-Augmented Generation (RAG) to address these demands. First, agents\nbuild and maintain an ontology-free KG by extracting subject--predicate--object\n(SPO) triplets from regulatory documents and systematically cleaning,\nnormalizing, deduplicating, and updating them. Second, these triplets are\nembedded and stored along with their corresponding textual sections and\nmetadata in a single enriched vector database, allowing for both graph-based\nreasoning and efficient information retrieval. Third, an orchestrated agent\npipeline leverages triplet-level retrieval for question answering, ensuring\nhigh semantic alignment between user queries and the factual\n\"who-did-what-to-whom\" core captured by the graph. Our hybrid system\noutperforms conventional methods in complex regulatory queries, ensuring\nfactual correctness with embedded triplets, enabling traceability through a\nunified vector database, and enhancing understanding through subgraph\nvisualization, providing a robust foundation for compliance-driven and broader\naudit-focused applications.", "AI": {"tldr": "本文提出一个结合知识图谱（KG）和检索增强生成（RAG）的多智能体框架，用于解决监管合规问答中LLM面临的挑战，通过三元组级别检索提供精确、可验证的答案。", "motivation": "监管合规问答需要精确、可验证的信息和领域专业知识，这对大型语言模型（LLMs）构成了挑战。", "method": "该研究提出了一个新颖的多智能体框架，融合了监管三元组知识图谱（KG）与检索增强生成（RAG）。具体方法包括：1. 智能体从监管文档中提取主谓宾（SPO）三元组，构建并维护无本体知识图谱，并进行清洗、规范化、去重和更新。2. 将这些三元组及其对应的文本段落和元数据嵌入并存储在一个统一的增强向量数据库中，以支持图谱推理和高效信息检索。3. 设计一个协调的智能体管道，利用三元组级别的检索进行问答，确保用户查询与图谱捕获的核心事实高度语义对齐。", "result": "该混合系统在复杂的监管查询中优于传统方法，通过嵌入的三元组确保了事实正确性，通过统一的向量数据库实现了可追溯性，并通过子图可视化增强了理解。", "conclusion": "该系统为合规驱动和更广泛的审计应用提供了强大的基础，有效解决了监管合规问答的精确性和可验证性需求。"}}
{"id": "2508.09225", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09225", "abs": "https://arxiv.org/abs/2508.09225", "authors": ["Nak-Jun Sung", "Donghyun Lee", "Bo Hwa Choi", "Chae Jung Park"], "title": "AMRG: Extend Vision Language Models for Automatic Mammography Report Generation", "comment": null, "summary": "Mammography report generation is a critical yet underexplored task in medical\nAI, characterized by challenges such as multiview image reasoning,\nhigh-resolution visual cues, and unstructured radiologic language. In this\nwork, we introduce AMRG (Automatic Mammography Report Generation), the first\nend-to-end framework for generating narrative mammography reports using large\nvision-language models (VLMs). Building upon MedGemma-4B-it-a\ndomain-specialized, instruction-tuned VLM-we employ a parameter-efficient\nfine-tuning (PEFT) strategy via Low-Rank Adaptation (LoRA), enabling\nlightweight adaptation with minimal computational overhead. We train and\nevaluate AMRG on DMID, a publicly available dataset of paired high-resolution\nmammograms and diagnostic reports. This work establishes the first reproducible\nbenchmark for mammography report generation, addressing a longstanding gap in\nmultimodal clinical AI. We systematically explore LoRA hyperparameter\nconfigurations and conduct comparative experiments across multiple VLM\nbackbones, including both domain-specific and general-purpose models under a\nunified tuning protocol. Our framework demonstrates strong performance across\nboth language generation and clinical metrics, achieving a ROUGE-L score of\n0.5691, METEOR of 0.6152, CIDEr of 0.5818, and BI-RADS accuracy of 0.5582.\nQualitative analysis further highlights improved diagnostic consistency and\nreduced hallucinations. AMRG offers a scalable and adaptable foundation for\nradiology report generation and paves the way for future research in multimodal\nmedical AI.", "AI": {"tldr": "该研究提出了AMRG，首个使用大型视觉语言模型（VLM）端到端生成乳腺X线报告的框架，通过LoRA对MedGemma-4B-it-a进行参数高效微调，并在DMID数据集上建立了可复现的基准，取得了良好的语言生成和临床指标表现。", "motivation": "乳腺X线报告生成是医疗AI中一个关键但未充分探索的任务，面临多视图图像推理、高分辨率视觉线索和非结构化放射学语言等挑战，当前缺乏可复现的基准。", "method": "引入了AMRG框架，利用MedGemma-4B-it-a（领域专用、指令微调的VLM）并采用低秩适应（LoRA）进行参数高效微调。在公开的DMID数据集上进行训练和评估，并系统探索了LoRA超参数配置，比较了多种VLM骨干模型。", "result": "AMRG在语言生成和临床指标上均表现出色，ROUGE-L达到0.5691，METEOR为0.6152，CIDEr为0.5818，BI-RADS准确率为0.5582。定性分析显示诊断一致性提高，幻觉减少。", "conclusion": "AMRG为放射学报告生成提供了一个可扩展和适应性强的基础，并为多模态医疗AI的未来研究铺平了道路。"}}
{"id": "2508.09595", "categories": ["cs.RO", "cs.HC", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.09595", "abs": "https://arxiv.org/abs/2508.09595", "authors": ["Michael Fennel", "Markus Walker", "Dominik Pikos", "Uwe D. Hanebeck"], "title": "HapticGiant: A Novel Very Large Kinesthetic Haptic Interface with Hierarchical Force Control", "comment": "Final Version - Accepted on IEEE Transactions on Haptics", "summary": "Research in virtual reality and haptic technologies has consistently aimed to\nenhance immersion. While advanced head-mounted displays are now commercially\navailable, kinesthetic haptic interfaces still face challenges such as limited\nworkspaces, insufficient degrees of freedom, and kinematics not matching the\nhuman arm. In this paper, we present HapticGiant, a novel large-scale\nkinesthetic haptic interface designed to match the properties of the human arm\nas closely as possible and to facilitate natural user locomotion while\nproviding full haptic feedback. The interface incorporates a novel\nadmittance-type force control scheme, leveraging hierarchical optimization to\nrender both arbitrary serial kinematic chains and Cartesian admittances.\nNotably, the proposed control scheme natively accounts for system limitations,\nincluding joint and Cartesian constraints, as well as singularities.\nExperimental results demonstrate the effectiveness of HapticGiant and its\ncontrol scheme, paving the way for highly immersive virtual reality\napplications.", "AI": {"tldr": "HapticGiant是一种新型的大型运动学触觉界面，旨在匹配人手臂特性并提供完整触觉反馈，通过分层优化和导纳控制实现，可用于高度沉浸式VR应用。", "motivation": "尽管头戴显示器已成熟，但现有运动学触觉界面仍面临工作空间有限、自由度不足以及运动学与人手臂不匹配等挑战，阻碍了VR沉浸感提升。", "method": "本文提出HapticGiant，一种新型大型运动学触觉界面，旨在尽可能匹配人手臂特性并支持自然用户移动。它采用新颖的导纳型力控制方案，利用分层优化来渲染任意串联运动链和笛卡尔导纳，并原生考虑了关节、笛卡尔约束以及奇异点等系统限制。", "result": "实验结果证明了HapticGiant及其控制方案的有效性。", "conclusion": "HapticGiant及其控制方案为高度沉浸式虚拟现实应用铺平了道路。"}}
{"id": "2508.09731", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.09731", "abs": "https://arxiv.org/abs/2508.09731", "authors": ["Robert Graubohm", "Markus Maurer"], "title": "Besondere Anforderungen des automatisierten Fahrens an den Entwurf", "comment": "Preprint of\n  https://link.springer.com/chapter/10.1007/978-3-658-38486-9_45, published in\n  Handbuch Assistiertes und Automatisiertes Fahren, in German", "summary": "The development of automated vehicles and automated driving functions is an\nexceptionally complex task that requires the integration of numerous, sometimes\nconflicting interests and various constraints already in the early stages of\nsystem design. This chapter explains important challenges in concept\nspecifications for automated driving and presents a systematic process model\nthat contributes to overcoming the special requirements in this field. In\naddition, it describes the successful implementation of a structured concept\nspecification for an automated vehicle guidance system.\n  --\n  Die Entwicklung automatisierter Fahrzeuge und Fahrfunktionen stellt eine\nausgesprochen komplexe Aufgabe dar, die bereits im Zuge des Systementwurfs die\nEinbeziehung einer Vielzahl teilweise konflikt\\\"arer Interessen und diverser\nRandbedingungen erfordert. Dieses Kapitel erl\\\"autert wichtige\nHerausforderungen bei Konzeptspezifikationen im Themenfeld des automatisierten\nFahrens und stellt ein systematisches Prozessmodell vor, das einen Beitrag zur\nErf\\\"ullung der besonderen Anforderungen des automatisierten Fahrens an den\nEntwurf leistet. Dar\\\"uber hinaus wird die erfolgreiche Durchf\\\"uhrung einer\nstrukturierten Konzeptspezifikation f\\\"ur ein automatisiertes\nFahrzeugf\\\"uhrungssystem beschrieben.", "AI": {"tldr": "本文探讨了自动驾驶系统概念规范中的挑战，并提出了一个系统的过程模型来应对这些特殊要求，同时描述了其在自动驾驶车辆引导系统中的成功应用。", "motivation": "自动驾驶车辆和功能开发极其复杂，早期系统设计阶段就需要整合众多有时相互冲突的利益和各种约束条件。", "method": "文章阐述了自动驾驶领域概念规范的关键挑战，提出了一个系统的过程模型来满足该领域的特殊设计要求，并描述了该模型在一个自动车辆引导系统中的成功实施。", "result": "所提出的系统过程模型有助于克服自动驾驶领域的特殊要求，并且已成功实现了一个结构化的概念规范。", "conclusion": "通过一个系统的过程模型和结构化的概念规范，可以有效应对自动驾驶系统开发中复杂的早期设计挑战。"}}
{"id": "2508.09430", "categories": ["cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.09430", "abs": "https://arxiv.org/abs/2508.09430", "authors": ["Lavanya Shankar", "Leibny Paola Garcia Perera"], "title": "Leveraging Zipformer Model for Effective Language Identification in Code-Switched Child-Directed Speech", "comment": null, "summary": "Code-switching and language identification in child-directed scenarios\npresent significant challenges, particularly in bilingual environments. This\npaper addresses this challenge by using Zipformer to handle the nuances of\nspeech, which contains two imbalanced languages, Mandarin and English, in an\nutterance. This work demonstrates that the internal layers of the Zipformer\neffectively encode the language characteristics, which can be leveraged in\nlanguage identification. We present the selection methodology of the inner\nlayers to extract the embeddings and make a comparison with different\nback-ends. Our analysis shows that Zipformer is robust across these backends.\nOur approach effectively handles imbalanced data, achieving a Balanced Accuracy\n(BAC) of 81.89%, a 15.47% improvement over the language identification\nbaseline. These findings highlight the potential of the transformer encoder\narchitecture model in real scenarios.", "AI": {"tldr": "本文利用Zipformer模型处理儿童指导场景下的中英文混杂语音，有效提升了语言识别的平衡准确率，尤其在处理不平衡数据方面表现出色。", "motivation": "在双语环境中，特别是儿童指导场景下，语码转换和语言识别面临巨大挑战，尤其当语句中包含两种不平衡语言时。", "method": "使用Zipformer模型处理语音，利用其内部层有效编码语言特征进行语言识别。论文探讨了内部层的选择方法来提取嵌入，并与不同的后端进行了比较。", "result": "Zipformer模型在不同后端上均表现出鲁棒性，并能有效处理不平衡数据，实现了81.89%的平衡准确率（BAC），比语言识别基线提高了15.47%。", "conclusion": "研究结果表明，Transformer编码器架构模型（如Zipformer）在实际场景中具有巨大的潜力，尤其适用于处理语码转换和不平衡语言数据。"}}
{"id": "2508.09210", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09210", "abs": "https://arxiv.org/abs/2508.09210", "authors": ["Fan Zhang", "Zebang Cheng", "Chong Deng", "Haoxuan Li", "Zheng Lian", "Qian Chen", "Huadai Liu", "Wen Wang", "Yi-Fan Zhang", "Renrui Zhang", "Ziyu Guo", "Zhihong Zhu", "Hao Wu", "Haixin Wang", "Yefeng Zheng", "Xiaojiang Peng", "Xian Wu", "Kun Wang", "Xiangang Li", "Jieping Ye", "Pheng-Ann Heng"], "title": "MME-Emotion: A Holistic Evaluation Benchmark for Emotional Intelligence in Multimodal Large Language Models", "comment": null, "summary": "Recent advances in multimodal large language models (MLLMs) have catalyzed\ntransformative progress in affective computing, enabling models to exhibit\nemergent emotional intelligence. Despite substantial methodological progress,\ncurrent emotional benchmarks remain limited, as it is still unknown: (a) the\ngeneralization abilities of MLLMs across distinct scenarios, and (b) their\nreasoning capabilities to identify the triggering factors behind emotional\nstates. To bridge these gaps, we present \\textbf{MME-Emotion}, a systematic\nbenchmark that assesses both emotional understanding and reasoning capabilities\nof MLLMs, enjoying \\textit{scalable capacity}, \\textit{diverse settings}, and\n\\textit{unified protocols}. As the largest emotional intelligence benchmark for\nMLLMs, MME-Emotion contains over 6,000 curated video clips with task-specific\nquestioning-answering (QA) pairs, spanning broad scenarios to formulate eight\nemotional tasks. It further incorporates a holistic evaluation suite with\nhybrid metrics for emotion recognition and reasoning, analyzed through a\nmulti-agent system framework. Through a rigorous evaluation of 20 advanced\nMLLMs, we uncover both their strengths and limitations, yielding several key\ninsights: \\ding{182} Current MLLMs exhibit unsatisfactory emotional\nintelligence, with the best-performing model achieving only $39.3\\%$\nrecognition score and $56.0\\%$ Chain-of-Thought (CoT) score on our benchmark.\n\\ding{183} Generalist models (\\emph{e.g.}, Gemini-2.5-Pro) derive emotional\nintelligence from generalized multimodal understanding capabilities, while\nspecialist models (\\emph{e.g.}, R1-Omni) can achieve comparable performance\nthrough domain-specific post-training adaptation. By introducing MME-Emotion,\nwe hope that it can serve as a foundation for advancing MLLMs' emotional\nintelligence in the future.", "AI": {"tldr": "MME-Emotion是一个新的、大规模情感智能基准，用于评估多模态大语言模型（MLLMs）的情感理解和推理能力，结果显示当前MLLMs的情感智能表现不佳。", "motivation": "现有情感基准在评估MLLMs的泛化能力（跨不同场景）和推理能力（识别情感触发因素）方面存在局限性，导致无法全面了解MLLMs的情感智能水平。", "method": "提出了MME-Emotion基准，包含6000多个精心策划的视频片段和任务特定的问答对，涵盖广泛场景并制定了八项情感任务。该基准具有可扩展性、多样化设置和统一协议。评估采用混合指标和多智能体系统框架，对20个先进的MLLMs进行了严格评估。", "result": "当前MLLMs的情感智能表现不尽如人意，最佳模型在识别任务上仅达到39.3%的得分，在思维链（CoT）推理任务上仅达到56.0%的得分。通用模型（如Gemini-2.5-Pro）的情感智能源于其通用的多模态理解能力，而专业模型（如R1-Omni）通过领域特定的后训练适应也能达到可比性能。", "conclusion": "MME-Emotion基准为未来提升MLLMs的情感智能奠定了基础，揭示了当前模型在情感理解和推理方面的不足，并指出了通用模型和专业模型实现情感智能的不同路径。"}}
{"id": "2508.09932", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09932", "abs": "https://arxiv.org/abs/2508.09932", "authors": ["Liang Zhang", "Edith Aurora Graf"], "title": "Mathematical Computation and Reasoning Errors by Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) are increasingly utilized in AI-driven\neducational instruction and assessment, particularly within mathematics\neducation. The capability of LLMs to generate accurate answers and detailed\nsolutions for math problem-solving tasks is foundational for ensuring reliable\nand precise feedback and assessment in math education practices. Our study\nfocuses on evaluating the accuracy of four LLMs (OpenAI GPT-4o and o1,\nDeepSeek-V3 and DeepSeek-R1) solving three categories of math tasks, including\narithmetic, algebra, and number theory, and identifies step-level reasoning\nerrors within their solutions. Instead of relying on standard benchmarks, we\nintentionally build math tasks (via item models) that are challenging for LLMs\nand prone to errors. The accuracy of final answers and the presence of errors\nin individual solution steps were systematically analyzed and coded. Both\nsingle-agent and dual-agent configurations were tested. It is observed that the\nreasoning-enhanced OpenAI o1 model consistently achieved higher or nearly\nperfect accuracy across all three math task categories. Analysis of errors\nrevealed that procedural slips were the most frequent and significantly\nimpacted overall performance, while conceptual misunderstandings were less\nfrequent. Deploying dual-agent configurations substantially improved overall\nperformance. These findings offer actionable insights into enhancing LLM\nperformance and underscore effective strategies for integrating LLMs into\nmathematics education, thereby advancing AI-driven instructional practices and\nassessment precision.", "AI": {"tldr": "本研究评估了四种大型语言模型在算术、代数和数论三类数学任务中的准确性，识别了步骤级推理错误，并发现OpenAI o1模型表现最佳，程序性错误最常见，双代理配置显著提升了性能。", "motivation": "大型语言模型在AI驱动的教育教学和评估中，尤其是在数学教育领域，应用日益广泛。LLM生成准确答案和详细解题步骤的能力，对于确保数学教育中反馈和评估的可靠性和精确性至关重要，因此需要对其性能进行评估。", "method": "研究评估了四种LLM（OpenAI GPT-4o、o1，DeepSeek-V3和DeepSeek-R1）在算术、代数和数论三类数学任务中的表现。不同于标准基准测试，研究通过项目模型构建了对LLM具有挑战性且易出错的数学任务。系统分析并编码了最终答案的准确性以及解题步骤中的错误。同时测试了单代理和双代理两种配置。", "result": "推理增强型OpenAI o1模型在所有三类数学任务中均持续实现更高或接近完美的准确性。错误分析显示，程序性失误是最常见的错误类型，并显著影响了整体性能，而概念性误解则较少发生。部署双代理配置显著提升了整体性能。", "conclusion": "这些发现为提升LLM性能提供了可操作的见解，并强调了将LLM有效整合到数学教育中的策略，从而推进了AI驱动的教学实践和评估的精确性。"}}
{"id": "2508.09271", "categories": ["eess.IV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.09271", "abs": "https://arxiv.org/abs/2508.09271", "authors": ["Reihaneh Hassanzadeh", "Anees Abrol", "Hamid Reza Hassanzadeh", "Vince D. Calhoun"], "title": "A Generative Imputation Method for Multimodal Alzheimer's Disease Diagnosis", "comment": null, "summary": "Multimodal data analysis can lead to more accurate diagnoses of brain\ndisorders due to the complementary information that each modality adds.\nHowever, a major challenge of using multimodal datasets in the neuroimaging\nfield is incomplete data, where some of the modalities are missing for certain\nsubjects. Hence, effective strategies are needed for completing the data.\nTraditional methods, such as subsampling or zero-filling, may reduce the\naccuracy of predictions or introduce unintended biases. In contrast, advanced\nmethods such as generative models have emerged as promising solutions without\nthese limitations. In this study, we proposed a generative adversarial network\nmethod designed to reconstruct missing modalities from existing ones while\npreserving the disease patterns. We used T1-weighted structural magnetic\nresonance imaging and functional network connectivity as two modalities. Our\nfindings showed a 9% improvement in the classification accuracy for Alzheimer's\ndisease versus cognitive normal groups when using our generative imputation\nmethod compared to the traditional approaches.", "AI": {"tldr": "该研究提出一种基于生成对抗网络（GAN）的方法，用于重建多模态神经影像数据中的缺失模态，以提高脑疾病诊断的准确性。", "motivation": "多模态数据分析能提供互补信息，提高脑疾病诊断准确性，但神经影像领域面临多模态数据集不完整（某些模态缺失）的挑战。传统方法如子采样或零填充会降低预测准确性或引入偏差，因此需要更先进有效的数据补全策略。", "method": "本研究提出一种生成对抗网络（GAN）方法，旨在从现有模态重建缺失模态，同时保留疾病模式。研究使用了T1加权结构磁共振成像和功能网络连接作为两种模态。", "result": "与传统方法相比，使用本研究提出的生成式插补方法，阿尔茨海默病与认知正常组的分类准确率提高了9%。", "conclusion": "本研究提出的GAN方法能有效重建缺失的神经影像模态，并显著提高阿尔茨海默病诊断的分类准确性，优于传统数据补全方法。"}}
{"id": "2508.09606", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.09606", "abs": "https://arxiv.org/abs/2508.09606", "authors": ["Alejandro Posadas-Nava", "Alejandro Carrasco", "Richard Linares"], "title": "BEAVR: Bimanual, multi-Embodiment, Accessible, Virtual Reality Teleoperation System for Robots", "comment": "Accepted for presentation on ICCR Kyoto 2025", "summary": "\\textbf{BEAVR} is an open-source, bimanual, multi-embodiment Virtual Reality\n(VR) teleoperation system for robots, designed to unify real-time control, data\nrecording, and policy learning across heterogeneous robotic platforms. BEAVR\nenables real-time, dexterous teleoperation using commodity VR hardware,\nsupports modular integration with robots ranging from 7-DoF manipulators to\nfull-body humanoids, and records synchronized multi-modal demonstrations\ndirectly in the LeRobot dataset schema. Our system features a zero-copy\nstreaming architecture achieving $\\leq$35\\,ms latency, an asynchronous\n``think--act'' control loop for scalable inference, and a flexible network API\noptimized for real-time, multi-robot operation. We benchmark BEAVR across\ndiverse manipulation tasks and demonstrate its compatibility with leading\nvisuomotor policies such as ACT, DiffusionPolicy, and SmolVLA. All code is\npublicly available, and datasets are released on Hugging Face\\footnote{Code,\ndatasets, and VR app available at https://github.com/ARCLab-MIT/BEAVR-Bot.", "AI": {"tldr": "BEAVR是一个开源、双手动、多体现的VR机器人遥操作系统，旨在统一异构机器人平台的实时控制、数据记录和策略学习。", "motivation": "需要一个系统，能够使用商用VR硬件实现对异构机器人平台的实时、灵巧遥操作，并支持数据记录和策略学习。", "method": "BEAVR采用零拷贝流传输架构（延迟≤35ms）、异步“思考-行动”控制循环和灵活的网络API。它支持与各种机器人（从7自由度机械手到全身人形机器人）的模块化集成，并直接以LeRobot数据集模式记录同步的多模态演示数据。", "result": "系统实现了≤35ms的低延迟，能够支持多样化的操作任务，并与主流的视觉运动策略（如ACT、DiffusionPolicy和SmolVLA）兼容。所有代码和数据集均已公开。", "conclusion": "BEAVR是一个高效且多功能的开源VR机器人遥操作系统，能够实现实时控制、高质量数据收集，并支持多种机器人类型和先进的策略学习方法。"}}
{"id": "2508.09774", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.09774", "abs": "https://arxiv.org/abs/2508.09774", "authors": ["Imran Pervez", "Omar Knio"], "title": "Integrated Learning and Optimization to Control Load Demand and Wind Generation for Minimizing Ramping Cost in Real-Time Electricity Market", "comment": null, "summary": "We developed a new integrated learning and optimization (ILO) methodology to\npredict context-aware unknown parameters in economic dispatch (ED), a crucial\nproblem in power systems solved to generate optimal power dispatching decisions\nto serve consumer load. The ED formulation in the current study consists of\nload and renewable generation as unknown parameters in its constraints\npredicted using contextual information (e.g., prior load, temperature). The ILO\nframework train a neural network (NN) to estimate ED parameters by minimizing\nan application-specific regret function which is a difference between ground\ntruth and NN-driven decisions favouring better ED decisions. We thoroughly\nanalyze the feasible region of ED formulation to understand the impact of load\nand renewable learning together on the ED decisions. Corresponding to that we\ndeveloped a new regret function to capture real-time electricity market\noperations where differences in predicted and true loads are corrected by\nramping generators in real-time but at a higher cost than the market price. The\nproposed regret function when minimized using ILO framework train the NN to\nguide the load and renewable predictions to generate ED decisions favouring\nminimum generator ramping costs. This is unlike conventional sequential\nlearning and optimization (SLO) framework which train NN to accurately estimate\nload and renewable instead of better ED decisions. The combined training of\nload and renewable using ILO is a new concept and lead to significantly\nimproved ramping costs when compared with SLO based training of load and\nrenewable and SLO trained load with 100% accurate renewable proving its\ndecision-focused capability.", "AI": {"tldr": "本文提出了一种新的集成学习与优化（ILO）方法，用于预测经济调度（ED）中的未知参数（负荷和可再生能源发电），通过最小化一个应用特定的悔恨函数来指导神经网络（NN）生成更优的ED决策，从而显著降低发电机实时爬坡成本。", "motivation": "经济调度（ED）是电力系统中优化电力调度的关键问题，其中负荷和可再生能源发电等参数是未知的。传统的顺序学习与优化（SLO）方法侧重于提高预测精度，但预测精度高不一定能带来最优的ED决策，尤其是在实时电力市场中，预测误差会导致额外的发电机爬坡成本。", "method": "开发了一种新的集成学习与优化（ILO）方法。该方法训练一个神经网络（NN）来估计ED参数（负荷和可再生能源发电），通过最小化一个应用特定的悔恨函数。这个悔恨函数捕获了实时电力市场操作，将预测与真实负荷之间的差异导致的更高发电机爬坡成本纳入考虑。与传统侧重于预测精度的SLO方法不同，ILO训练NN以引导预测，从而最小化发电机爬坡成本。该方法首次将负荷和可再生能源的联合训练应用于此框架。", "result": "通过最小化所提出的悔恨函数，ILO框架训练的神经网络能够引导负荷和可再生能源的预测，从而生成最小化发电机爬坡成本的ED决策。与基于SLO的负荷和可再生能源训练方法相比，ILO显著降低了爬坡成本。即使与SLO训练的负荷在可再生能源100%准确的情况下相比，ILO也表现出更低的爬坡成本，证明了其以决策为中心的优越能力。", "conclusion": "所提出的集成学习与优化（ILO）方法能够有效融合学习与优化，通过优化一个以决策为中心的悔恨函数，引导负荷和可再生能源的预测，从而在经济调度中产生更优的决策，并显著降低实时运行中的发电机爬坡成本，优于传统的以预测精度为目标的顺序方法。"}}
{"id": "2508.09450", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.09450", "abs": "https://arxiv.org/abs/2508.09450", "authors": ["Ridwan Mahbub", "Mohammed Saidul Islam", "Mir Tafseer Nayeem", "Md Tahmid Rahman Laskar", "Mizanur Rahman", "Shafiq Joty", "Enamul Hoque"], "title": "From Charts to Fair Narratives: Uncovering and Mitigating Geo-Economic Biases in Chart-to-Text", "comment": null, "summary": "Charts are very common for exploring data and communicating insights, but\nextracting key takeaways from charts and articulating them in natural language\ncan be challenging. The chart-to-text task aims to automate this process by\ngenerating textual summaries of charts. While with the rapid advancement of\nlarge Vision-Language Models (VLMs), we have witnessed great progress in this\ndomain, little to no attention has been given to potential biases in their\noutputs. This paper investigates how VLMs can amplify geo-economic biases when\ngenerating chart summaries, potentially causing societal harm. Specifically, we\nconduct a large-scale evaluation of geo-economic biases in VLM-generated chart\nsummaries across 6,000 chart-country pairs from six widely used proprietary and\nopen-source models to understand how a country's economic status influences the\nsentiment of generated summaries. Our analysis reveals that existing VLMs tend\nto produce more positive descriptions for high-income countries compared to\nmiddle- or low-income countries, even when country attribution is the only\nvariable changed. We also find that models such as GPT-4o-mini,\nGemini-1.5-Flash, and Phi-3.5 exhibit varying degrees of bias. We further\nexplore inference-time prompt-based debiasing techniques using positive\ndistractors but find them only partially effective, underscoring the complexity\nof the issue and the need for more robust debiasing strategies. Our code and\ndataset are publicly available here.", "AI": {"tldr": "本文研究了大型视觉-语言模型（VLMs）在生成图表摘要时如何放大地缘经济偏见，发现现有模型倾向于对高收入国家产生更积极的描述，并探讨了去偏见技术的局限性。", "motivation": "尽管VLMs在图表到文本任务上取得了显著进展，但其输出中潜在的偏见却鲜受关注。图表摘要中的偏见可能导致社会危害，因此需要对其进行深入调查。", "method": "研究对来自六个专有和开源模型的6000个图表-国家对进行了大规模评估，以分析国家经济状况如何影响生成摘要的情感。此外，还探索了使用正面干扰项的推理时基于提示的去偏见技术。", "result": "分析显示，现有VLMs倾向于对高收入国家产生比中低收入国家更积极的描述，即使仅改变国家归属变量。GPT-4o-mini、Gemini-1.5-Flash和Phi-3.5等模型表现出不同程度的偏见。基于提示的去偏见技术仅部分有效。", "conclusion": "VLMs在图表摘要生成中存在显著的地缘经济偏见，偏见问题复杂，需要更强大的去偏见策略来解决。"}}
{"id": "2508.09218", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09218", "abs": "https://arxiv.org/abs/2508.09218", "authors": ["Zuoou Li", "Weitong Zhang", "Jingyuan Wang", "Shuyuan Zhang", "Wenjia Bai", "Bernhard Kainz", "Mengyun Qiao"], "title": "Towards Effective MLLM Jailbreaking Through Balanced On-Topicness and OOD-Intensity", "comment": null, "summary": "Multimodal large language models (MLLMs) are widely used in vision-language\nreasoning tasks. However, their vulnerability to adversarial prompts remains a\nserious concern, as safety mechanisms often fail to prevent the generation of\nharmful outputs. Although recent jailbreak strategies report high success\nrates, many responses classified as \"successful\" are actually benign, vague, or\nunrelated to the intended malicious goal. This mismatch suggests that current\nevaluation standards may overestimate the effectiveness of such attacks. To\naddress this issue, we introduce a four-axis evaluation framework that\nconsiders input on-topicness, input out-of-distribution (OOD) intensity, output\nharmfulness, and output refusal rate. This framework identifies truly effective\njailbreaks. In a substantial empirical study, we reveal a structural trade-off:\nhighly on-topic prompts are frequently blocked by safety filters, whereas those\nthat are too OOD often evade detection but fail to produce harmful content.\nHowever, prompts that balance relevance and novelty are more likely to evade\nfilters and trigger dangerous output. Building on this insight, we develop a\nrecursive rewriting strategy called Balanced Structural Decomposition (BSD).\nThe approach restructures malicious prompts into semantically aligned\nsub-tasks, while introducing subtle OOD signals and visual cues that make the\ninputs harder to detect. BSD was tested across 13 commercial and open-source\nMLLMs, where it consistently led to higher attack success rates, more harmful\noutputs, and fewer refusals. Compared to previous methods, it improves success\nrates by $67\\%$ and harmfulness by $21\\%$, revealing a previously\nunderappreciated weakness in current multimodal safety systems.", "AI": {"tldr": "该研究揭示了多模态大语言模型（MLLMs）在对抗性提示下的脆弱性，并指出当前越狱评估标准存在缺陷。为此，作者提出了一个四轴评估框架，并开发了一种名为“平衡结构分解”（BSD）的递归重写策略，该策略能有效绕过安全过滤器，生成有害内容，显著提高了攻击成功率和有害性。", "motivation": "多模态大语言模型（MLLMs）在视觉-语言推理任务中广泛应用，但其对抗性提示漏洞令人担忧，安全机制常无法阻止有害输出。现有越狱策略虽然报告成功率高，但许多“成功”响应实际上是良性、模糊或无关的，这表明当前评估标准可能高估了攻击效果。因此，需要更准确的评估方法和更有效的攻击策略来揭示MLLMs的安全弱点。", "method": "1. 引入了一个四轴评估框架，考虑了输入主题相关性、输入分布外（OOD）强度、输出有害性以及输出拒绝率，以识别真正有效的越狱攻击。2. 基于对“相关性与新颖性平衡”提示更易绕过过滤器并触发危险输出的洞察，开发了一种名为“平衡结构分解”（BSD）的递归重写策略。3. BSD方法将恶意提示重构为语义对齐的子任务，同时引入微妙的OOD信号和视觉线索，使输入更难被检测。", "result": "1. 发现了一个结构性权衡：高度主题相关的提示经常被安全过滤器阻止，而过于OOD的提示虽然常能逃避检测，但未能产生有害内容。2. 能够平衡相关性和新颖性的提示更有可能规避过滤器并触发危险输出。3. BSD策略在13个商业和开源MLLMs上进行了测试，一致导致更高的攻击成功率、更具危害性的输出和更少的拒绝。4. 与以前的方法相比，BSD将成功率提高了67%，有害性提高了21%。", "conclusion": "当前多模态安全系统存在一个此前被低估的弱点。BSD策略通过平衡提示的相关性和新颖性，并巧妙地引入OOD信号和视觉线索，能有效绕过MLLMs的安全防护，揭示了其在生成有害内容方面的脆弱性。"}}
{"id": "2504.19716", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2504.19716", "abs": "https://arxiv.org/abs/2504.19716", "authors": ["Navin Sriram Ravie", "Keerthi Vasan M", "Asokan Thondiyath", "Bijo Sebastian"], "title": "QuickGrasp: Lightweight Antipodal Grasp Planning with Point Clouds", "comment": null, "summary": "Grasping has been a long-standing challenge in facilitating the final\ninterface between a robot and the environment. As environments and tasks become\ncomplicated, the need to embed higher intelligence to infer from the\nsurroundings and act on them has become necessary. Although most methods\nutilize techniques to estimate grasp pose by treating the problem via pure\nsampling-based approaches in the six-degree-of-freedom space or as a learning\nproblem, they usually fail in real-life settings owing to poor generalization\nacross domains. In addition, the time taken to generate the grasp plan and the\nlack of repeatability, owing to sampling inefficiency and the probabilistic\nnature of existing grasp planning approaches, severely limits their application\nin real-world tasks. This paper presents a lightweight analytical approach\ntowards robotic grasp planning, particularly antipodal grasps, with little to\nno sampling in the six-degree-of-freedom space. The proposed grasp planning\nalgorithm is formulated as an optimization problem towards estimating grasp\npoints on the object surface instead of directly estimating the end-effector\npose. To this extent, a soft-region-growing algorithm is presented for\neffective plane segmentation, even in the case of curved surfaces. An\noptimization-based quality metric is then used for the evaluation of grasp\npoints to ensure indirect force closure. The proposed grasp framework is\ncompared with the existing state-of-the-art grasp planning approach, Grasp pose\ndetection (GPD), as a baseline over multiple simulated objects. The\neffectiveness of the proposed approach in comparison to GPD is also evaluated\nin a real-world setting using image and point-cloud data, with the planned\ngrasps being executed using a ROBOTIQ gripper and UR5 manipulator.", "AI": {"tldr": "本文提出了一种轻量级的分析方法，用于机器人抓取规划，特别是对偶抓取，该方法在六自由度空间中几乎无需采样，通过优化抓取点而非末端执行器姿态来提高在真实环境中的泛化性和效率。", "motivation": "现有的机器人抓取方法（基于采样或学习）在真实场景中由于泛化性差、规划时间长、采样效率低以及缺乏可重复性而表现不佳，这严重限制了它们在实际任务中的应用。", "method": "该方法将抓取规划表述为一个优化问题，旨在估计物体表面的抓取点。它引入了一种软区域增长算法进行有效的平面分割（包括曲面），并使用基于优化的质量度量来评估抓取点，以确保间接力闭合。", "result": "所提出的抓取框架在多个模拟物体上与现有的先进抓取规划方法GPD进行了比较，并在真实世界中通过图像和点云数据进行了评估，使用ROBOTIQ夹具和UR5机械臂执行了规划的抓取，结果表明其有效性优于GPD。", "conclusion": "本文提出的轻量级分析方法能有效进行机器人抓取规划，通过优化抓取点和减少采样解决了现有方法在泛化性、效率和可重复性方面的局限性，在模拟和真实世界环境中均表现出优越性。"}}
{"id": "2508.09328", "categories": ["eess.IV", "cs.CV", "stat.AP", "stat.OT"], "pdf": "https://arxiv.org/pdf/2508.09328", "abs": "https://arxiv.org/abs/2508.09328", "authors": ["Bingfan Liu", "Haolun Shi", "Jiguo Cao"], "title": "Dynamic Survival Prediction using Longitudinal Images based on Transformer", "comment": null, "summary": "Survival analysis utilizing multiple longitudinal medical images plays a\npivotal role in the early detection and prognosis of diseases by providing\ninsight beyond single-image evaluations. However, current methodologies often\ninadequately utilize censored data, overlook correlations among longitudinal\nimages measured over multiple time points, and lack interpretability. We\nintroduce SurLonFormer, a novel Transformer-based neural network that\nintegrates longitudinal medical imaging with structured data for survival\nprediction. Our architecture comprises three key components: a Vision Encoder\nfor extracting spatial features, a Sequence Encoder for aggregating temporal\ninformation, and a Survival Encoder based on the Cox proportional hazards\nmodel. This framework effectively incorporates censored data, addresses\nscalability issues, and enhances interpretability through occlusion sensitivity\nanalysis and dynamic survival prediction. Extensive simulations and a\nreal-world application in Alzheimer's disease analysis demonstrate that\nSurLonFormer achieves superior predictive performance and successfully\nidentifies disease-related imaging biomarkers.", "AI": {"tldr": "SurLonFormer是一种新型的基于Transformer的神经网络，它结合纵向医学图像和结构化数据进行生存预测，解决了现有方法对审查数据利用不足、忽略时间相关性及可解释性差的问题。", "motivation": "目前的生存分析方法在利用多时间点纵向医学图像时，未能充分利用审查数据、忽视图像间的时间相关性，并且缺乏可解释性，这限制了它们在疾病早期检测和预后中的潜力。", "method": "本文提出了SurLonFormer模型，一个基于Transformer的神经网络，用于整合纵向医学图像和结构化数据进行生存预测。其架构包含三个核心组件：用于提取空间特征的视觉编码器（Vision Encoder）、用于聚合时间信息的序列编码器（Sequence Encoder），以及基于Cox比例风险模型的生存编码器（Survival Encoder）。该框架有效处理审查数据、解决可扩展性问题，并通过遮挡敏感性分析和动态生存预测增强了可解释性。", "result": "广泛的模拟实验和在阿尔茨海默病分析中的真实世界应用表明，SurLonFormer实现了卓越的预测性能，并成功识别了与疾病相关的影像生物标志物。", "conclusion": "SurLonFormer为利用纵向医学图像进行生存分析提供了一个有效且可解释的解决方案，显著提升了预测性能并克服了现有方法的局限性，有望在疾病诊断和预后中发挥重要作用。"}}
{"id": "2508.09621", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.09621", "abs": "https://arxiv.org/abs/2508.09621", "authors": ["Ingrid Maéva Chekam", "Ines Pastor-Martinez", "Ali Tourani", "Jose Andres Millan-Romera", "Laura Ribeiro", "Pedro Miguel Bastos Soares", "Holger Voos", "Jose Luis Sanchez-Lopez"], "title": "Interpretable Robot Control via Structured Behavior Trees and Large Language Models", "comment": "15 pages, 5 figures, 3 tables", "summary": "As intelligent robots become more integrated into human environments, there\nis a growing need for intuitive and reliable Human-Robot Interaction (HRI)\ninterfaces that are adaptable and more natural to interact with. Traditional\nrobot control methods often require users to adapt to interfaces or memorize\npredefined commands, limiting usability in dynamic, unstructured environments.\nThis paper presents a novel framework that bridges natural language\nunderstanding and robotic execution by combining Large Language Models (LLMs)\nwith Behavior Trees. This integration enables robots to interpret natural\nlanguage instructions given by users and translate them into executable actions\nby activating domain-specific plugins. The system supports scalable and modular\nintegration, with a primary focus on perception-based functionalities, such as\nperson tracking and hand gesture recognition. To evaluate the system, a series\nof real-world experiments was conducted across diverse environments.\nExperimental results demonstrate that the proposed approach is practical in\nreal-world scenarios, with an average cognition-to-execution accuracy of\napproximately 94%, making a significant contribution to HRI systems and robots.\nThe complete source code of the framework is publicly available at\nhttps://github.com/snt-arg/robot_suite.", "AI": {"tldr": "该论文提出一个结合大语言模型（LLMs）和行为树（Behavior Trees）的新框架，以实现机器人对自然语言指令的理解和执行，提高人机交互的直观性和可靠性。", "motivation": "传统的机器人控制方法要求用户适应接口或记忆预定义命令，限制了其在动态、非结构化环境中的可用性。随着智能机器人更多地融入人类环境，对直观、可靠、适应性强且更自然的人机交互（HRI）接口的需求日益增长。", "method": "该框架通过结合大语言模型（LLMs）和行为树，实现机器人对用户自然语言指令的理解，并通过激活领域特定插件将其转化为可执行动作。系统支持可扩展和模块化集成，重点关注基于感知的功能（如人员跟踪和手势识别）。", "result": "在多样化环境中的真实世界实验表明，所提出的方法在实际场景中是可行的，平均认知到执行的准确率约为94%。", "conclusion": "该方法对人机交互系统和机器人做出了重大贡献，证明了所提出的框架在现实世界场景中的实用性和有效性。"}}
{"id": "2508.09908", "categories": ["eess.SY", "cs.RO", "cs.SY", "math-ph", "math.MP", "math.OC", "nlin.PS"], "pdf": "https://arxiv.org/pdf/2508.09908", "abs": "https://arxiv.org/abs/2508.09908", "authors": ["Haoshu Cheng", "Martin Guay", "Shimin Wang", "Yunhong Che"], "title": "Collision-Free Bearing-Driven Formation Tracking for Euler-Lagrange Systems", "comment": "10 pages, 4 figures", "summary": "In this paper, we investigate the problem of tracking formations driven by\nbearings for heterogeneous Euler-Lagrange systems with parametric uncertainty\nin the presence of multiple moving leaders. To estimate the leaders' velocities\nand accelerations, we first design a distributed observer for the leader\nsystem, utilizing a bearing-based localization condition in place of the\nconventional connectivity assumption. This observer, coupled with an adaptive\nmechanism, enables the synthesis of a novel distributed control law that guides\nthe formation towards the target formation, without requiring prior knowledge\nof the system parameters. Furthermore, we establish a sufficient condition,\ndependent on the initial formation configuration, that ensures collision\navoidance throughout the formation evolution. The effectiveness of the proposed\napproach is demonstrated through a numerical example.", "AI": {"tldr": "该论文研究了异构Euler-Lagrange系统在参数不确定性和多移动领导者存在下，基于方位角的编队跟踪问题，并提出了分布式观测器、自适应控制律及碰撞避免条件。", "motivation": "解决异构Euler-Lagrange系统在存在参数不确定性和多个移动领导者的情况下，如何实现基于方位角的编队跟踪，同时需要估计领导者的速度和加速度，并且无需系统参数的先验知识。", "method": "首先，设计了一个分布式观测器，利用基于方位角的定位条件（而非传统连接假设）来估计领导者的速度和加速度。然后，将该观测器与自适应机制相结合，合成了一种新型分布式控制律，以引导编队。此外，还建立了一个依赖于初始编队配置的充分条件，以确保编队演化过程中的碰撞避免。", "result": "所提出的方法能够实现异构Euler-Lagrange系统在参数不确定性下对多移动领导者的编队跟踪，无需系统参数的先验知识。同时，通过设定的充分条件，确保了编队演化过程中的碰撞避免。数值例子验证了该方法的有效性。", "conclusion": "论文成功提出了一种针对具有参数不确定性和多移动领导者的异构Euler-Lagrange系统的基于方位角的编队跟踪方法，该方法通过分布式观测器和自适应控制律实现，并能有效避免碰撞。"}}
{"id": "2508.09463", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.09463", "abs": "https://arxiv.org/abs/2508.09463", "authors": ["Qi Jia", "Xiujie Song", "Zicheng Zhang", "Yijin Guo", "Kaiwei Zhang", "Zijian Chen", "Guangtao Zhai"], "title": "User-centric Subjective Leaderboard by Customizable Reward Modeling", "comment": null, "summary": "Existing benchmarks for large language models (LLMs) predominantely focus on\nassessing their capabilities through verifiable tasks. Such objective and\nstatic benchmarks offer limited utility for practical LLM selection, making it\ndifficult for users to find suitable models for their individual needs. To\nbridge this gap, we present the first User-Centric Subjective Leaderboard\n(USL), which provides a preference-driven, dynamic ranking of LLMs across\ndiverse real-world scenarios. Our work is built upon a thorough investigation\nof real human preference data, involving more than 10K subjective queries. Our\ninvestigation reveals significant diversity and contradictions in human\npreferences, which limit the effectiveness of state-of-the-art reward models.\nTo address this, we introduce Customizable Reward Models (CRMs). With only 4B\nparameters, our CRM surpasses the performance of leading models such as GPT-4.1\nand Gemini-2.5-pro, showing exceptional generalization capabilities across new\ntopics and criteria. The USL, powered by CRMs, exhibits strong negative\ncorrelations to contradictory preferences.", "AI": {"tldr": "该研究提出了首个用户中心主观排行榜（USL），通过定制化奖励模型（CRMs）克服了现有LLM基准测试的局限性，实现了基于人类偏好的动态LLM排名。", "motivation": "现有的大语言模型（LLMs）基准测试主要关注可验证任务，是客观和静态的，这限制了它们在实际LLM选择中的实用性，使用户难以根据个人需求找到合适的模型。", "method": "研究基于对超过10K条人类偏好数据的深入调查，揭示了偏好的多样性和矛盾性。为解决这一问题，引入了定制化奖励模型（CRMs），并利用CRMs驱动用户中心主观排行榜（USL）。", "result": "人类偏好数据显示出显著的多样性和矛盾性，限制了现有奖励模型的有效性。所提出的CRMs（仅4B参数）超越了GPT-4.1和Gemini-2.5-pro等领先模型，在新主题和标准上表现出卓越的泛化能力。由CRMs驱动的USL与矛盾偏好表现出强烈的负相关。", "conclusion": "USL与CRMs的结合提供了一种更实用、偏好驱动和动态的LLM排名方法，有效应对了人类偏好的多样性，弥补了现有客观静态基准测试的不足。"}}
{"id": "2508.09220", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09220", "abs": "https://arxiv.org/abs/2508.09220", "authors": ["Haoyang Li", "Jiaqing Li", "Jialun Cao", "Zongyuan Yang", "Yongping Xiong"], "title": "Towards Scalable Training for Handwritten Mathematical Expression Recognition", "comment": null, "summary": "Large foundation models have achieved significant performance gains through\nscalable training on massive datasets. However, the field of\n\\textbf{H}andwritten \\textbf{M}athematical \\textbf{E}xpression\n\\textbf{R}ecognition (HMER) has been impeded by the scarcity of data, primarily\ndue to the arduous and costly process of manual annotation. To bridge this gap,\nwe propose a novel method integrating limited handwritten formulas with\nlarge-scale LaTeX-rendered formulas by developing a scalable data engine to\ngenerate complex and consistent LaTeX sequences. With this engine, we built the\nlargest formula dataset to date, termed \\texttt{Tex80M}, comprising over 80\nmillion high-quality training instances. Then we propose \\texttt{TexTeller},\nthe first HMER model trained at scale, by mix-training \\texttt{Tex80M} with a\nrelatively small HME dataset. The expansive training dataset and our refined\npipeline have equipped \\texttt{TexTeller} with state-of-the-art (SOTA)\nperformance across nearly all benchmarks. To advance the field, we will openly\nrelease our complete model, entire dataset, and full codebase, enabling further\nresearch building upon our contributions.", "AI": {"tldr": "该论文通过开发可扩展的数据引擎，生成了迄今为止最大的数学公式数据集Tex80M，并提出了首个大规模训练的HMER模型TexTeller，在多项基准测试中取得了最先进的性能。", "motivation": "手写数学表达式识别（HMER）领域由于数据稀缺而受到阻碍，这主要是因为人工标注过程耗时且成本高昂，导致难以像大型基础模型那样进行可扩展训练。", "method": "提出一种新方法，通过开发可扩展的数据引擎来生成复杂且一致的LaTeX序列，从而将有限的手写公式与大规模LaTeX渲染公式相结合。利用此引擎构建了Tex80M数据集。在此基础上，提出了TexTeller模型，通过将Tex80M与相对较小的手写数学表达式（HME）数据集进行混合训练，实现了大规模模型训练。", "result": "构建了迄今为止最大的公式数据集Tex80M，包含超过8000万个高质量训练实例。TexTeller模型在几乎所有基准测试中都取得了最先进（SOTA）的性能。", "conclusion": "该研究通过提供大规模数据集和高性能模型，显著推动了HMER领域的发展。研究团队将开源模型、数据集和代码库，以促进未来的研究和贡献。"}}
{"id": "2508.09919", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09919", "abs": "https://arxiv.org/abs/2508.09919", "authors": ["Xiaojiao Xiao", "Jianfeng Zhao", "Qinmin Vivian Hu", "Guanghui Wang"], "title": "T-CACE: A Time-Conditioned Autoregressive Contrast Enhancement Multi-Task Framework for Contrast-Free Liver MRI Synthesis, Segmentation, and Diagnosis", "comment": "IEEE Journal of Biomedical and Health Informatics, 2025", "summary": "Magnetic resonance imaging (MRI) is a leading modality for the diagnosis of\nliver cancer, significantly improving the classification of the lesion and\npatient outcomes. However, traditional MRI faces challenges including risks\nfrom contrast agent (CA) administration, time-consuming manual assessment, and\nlimited annotated datasets. To address these limitations, we propose a\nTime-Conditioned Autoregressive Contrast Enhancement (T-CACE) framework for\nsynthesizing multi-phase contrast-enhanced MRI (CEMRI) directly from\nnon-contrast MRI (NCMRI). T-CACE introduces three core innovations: a\nconditional token encoding (CTE) mechanism that unifies anatomical priors and\ntemporal phase information into latent representations; and a dynamic\ntime-aware attention mask (DTAM) that adaptively modulates inter-phase\ninformation flow using a Gaussian-decayed attention mechanism, ensuring smooth\nand physiologically plausible transitions across phases. Furthermore, a\nconstraint for temporal classification consistency (TCC) aligns the lesion\nclassification output with the evolution of the physiological signal, further\nenhancing diagnostic reliability. Extensive experiments on two independent\nliver MRI datasets demonstrate that T-CACE outperforms state-of-the-art methods\nin image synthesis, segmentation, and lesion classification. This framework\noffers a clinically relevant and efficient alternative to traditional\ncontrast-enhanced imaging, improving safety, diagnostic efficiency, and\nreliability for the assessment of liver lesion. The implementation of T-CACE is\npublicly available at: https://github.com/xiaojiao929/T-CACE.", "AI": {"tldr": "该研究提出了T-CACE框架，可从非增强MRI直接合成多期增强MRI，旨在解决传统MRI中造影剂风险、手动评估耗时和数据不足等问题，并在图像合成、分割和病灶分类方面超越现有技术。", "motivation": "肝癌诊断中的传统MRI面临造影剂风险、耗时的人工评估以及标注数据集有限等挑战，这些限制促使研究者寻求一种更安全、高效且可靠的替代方案。", "method": "本文提出了一种时间条件自回归对比增强（T-CACE）框架，用于从非增强MRI（NCMRI）合成多期对比增强MRI（CEMRI）。该框架包含三项核心创新：1) 条件令牌编码（CTE）机制，用于将解剖先验和时间相位信息统一到潜在表示中；2) 动态时间感知注意力掩码（DTAM），使用高斯衰减注意力机制自适应地调节相间信息流，确保平滑且符合生理的相位过渡；3) 时间分类一致性（TCC）约束，使病灶分类输出与生理信号演变对齐，提高诊断可靠性。", "result": "在两个独立的肝脏MRI数据集上进行的广泛实验表明，T-CACE在图像合成、分割和病灶分类方面均优于现有最先进的方法。", "conclusion": "T-CACE框架为传统的对比增强成像提供了一种临床相关且高效的替代方案，显著提高了肝脏病灶评估的安全性、诊断效率和可靠性。"}}
{"id": "2508.09700", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.09700", "abs": "https://arxiv.org/abs/2508.09700", "authors": ["Mahdi Hejrati", "Jouni Mattila"], "title": "Immersive Teleoperation of Beyond-Human-Scale Robotic Manipulators: Challenges and Future Directions", "comment": "This work has been accepted for presentation at the 2025 IEEE\n  Conference on Telepresence, to be held in Leiden, Netherlands", "summary": "Teleoperation of beyond-human-scale robotic manipulators (BHSRMs) presents\nunique challenges that differ fundamentally from conventional human-scale\nsystems. As these platforms gain relevance in industrial domains such as\nconstruction, mining, and disaster response, immersive interfaces must be\nrethought to support scalable, safe, and effective human-robot collaboration.\nThis paper investigates the control, cognitive, and interface-level challenges\nof immersive teleoperation in BHSRMs, with a focus on ensuring operator safety,\nminimizing sensorimotor mismatch, and enhancing the sense of embodiment. We\nanalyze design trade-offs in haptic and visual feedback systems, supported by\nearly experimental comparisons of exoskeleton- and joystick-based control\nsetups. Finally, we outline key research directions for developing new\nevaluation tools, scaling strategies, and human-centered safety models tailored\nto large-scale robotic telepresence.", "AI": {"tldr": "本文探讨了超大型机器人操纵器（BHSRMs）远程操作中独特的控制、认知和界面挑战，旨在通过沉浸式界面提升操作员安全、减少感觉运动不匹配并增强具身感。", "motivation": "超大型机器人操纵器（BHSRMs）在建筑、采矿和灾难响应等工业领域日益重要，但其远程操作面临与传统人尺度系统根本不同的挑战，需要重新思考沉浸式界面以支持可扩展、安全和有效的人机协作。", "method": "本文研究了BHSRMs沉浸式远程操作中的控制、认知和界面层挑战。分析了触觉和视觉反馈系统的设计权衡，并通过外骨骼和操纵杆控制设置的早期实验比较进行了支持。", "result": "研究揭示了超大型机器人远程操作在控制、认知和界面层面的独特挑战，并分析了触觉和视觉反馈系统的设计权衡。初步实验比较了外骨骼和操纵杆控制设置。", "conclusion": "论文概述了未来研究的关键方向，包括开发新的评估工具、扩展策略以及针对大规模机器人远程呈现量身定制的以人为中心的安全模型。"}}
{"id": "2508.09963", "categories": ["eess.SY", "cs.MA", "cs.RO", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.09963", "abs": "https://arxiv.org/abs/2508.09963", "authors": ["Devansh R. Agrawal", "Dimitra Panagou"], "title": "Online Safety under Multiple Constraints and Input Bounds using gatekeeper: Theory and Applications", "comment": "6 pages, 2 figures. Accepted for publication in IEEE L-CSS 2025", "summary": "This letter presents an approach to guarantee online safety of a\ncyber-physical system under multiple state and input constraints. Our proposed\nframework, called gatekeeper, recursively guarantees the existence of an\ninfinite-horizon trajectory that satisfies all constraints and system dynamics.\nSuch trajectory is constructed using a backup controller, which we define\nformally in this paper. gatekeeper relies on a small number of verifiable\nassumptions, and is computationally efficient since it requires optimization\nover a single scalar variable. We make two primary contributions in this\nletter. (A) First, we develop the theory of gatekeeper: we derive a\nsub-optimality bound relative to a full nonlinear trajectory optimization\nproblem, and show how this can be used in runtime to validate performance. This\nalso informs the design of the backup controllers and sets. (B) Second, we\ndemonstrate in detail an application of gatekeeper for multi-agent formation\nflight, where each Dubins agent must avoid multiple obstacles and weapons\nengagement zones, both of which are nonlinear, nonconvex constraints.", "AI": {"tldr": "提出了一种名为“守门人”（gatekeeper）的框架，通过递归保证无限时域轨迹的存在，从而在线确保网络物理系统在多重状态和输入约束下的安全。", "motivation": "在多重状态和输入约束下，如何在线保证网络物理系统的安全，特别是面对非线性、非凸约束的复杂场景。", "method": "核心方法是“守门人”框架，它递归地保证满足所有约束和系统动态的无限时域轨迹的存在。该框架通过定义一个“备份控制器”来构建轨迹，并仅需要对单个标量变量进行优化，因此计算效率高。理论上，它推导了一个相对于完整非线性轨迹优化问题的次优性界限，用于运行时性能验证和备份控制器设计。", "result": "开发了“守门人”理论，包括次优性界限的推导及其在运行时验证性能的应用。成功将该框架应用于多智能体编队飞行场景，其中每个Dubins智能体必须避开多个非线性、非凸的障碍物和武器交战区。", "conclusion": "“守门人”框架提供了一种计算高效且理论上可验证的方法，用于在线保证网络物理系统在复杂（包括非线性、非凸）约束下的安全，并通过备份控制器和次优性界限确保了性能和可行性。"}}
{"id": "2508.09494", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09494", "abs": "https://arxiv.org/abs/2508.09494", "authors": ["Jessy Lin", "Vincent-Pierre Berges", "Xilun Chen", "Wen-Tau Yih", "Gargi Ghosh", "Barlas Oğuz"], "title": "Learning Facts at Scale with Active Reading", "comment": null, "summary": "LLMs are known to store vast amounts of knowledge in their parametric memory.\nHowever, learning and recalling facts from this memory is known to be\nunreliable, depending largely on the prevalence of particular facts in the\ntraining data and other factors which are poorly understood. Practitioners are\nlacking tools which will allow them to ensure that the models learn a given\nbody of knowledge reliably and consistently. To this end, we propose Active\nReading: a framework where we train models to study a given set of material\nwith self-generated learning strategies. First, we demonstrate models trained\nwith Active Reading on expert domains absorb significantly more knowledge than\nvanilla finetuning and other data augmentations. We train expert 8B models that\nachieve 66% on a Wikipedia-grounded subset of SimpleQA (+313% relative over\nvanilla finetuning) and 26% on FinanceBench (+160% relative over vanilla\nfinetuning) by applying Active Reading to the source documents for each\nbenchmark. Finally, we show that Active Reading can be utilized at pre-training\nscale to build more factual models. As a demonstration of this, we release Meta\nWikiExpert-8B, a Wikipedia-expert model trained on 1 trillion generated tokens,\nwhich outcompetes models with hundreds of billions of parameters on factual QA.", "AI": {"tldr": "本文提出“主动阅读”框架，通过让LLM生成学习策略来吸收知识，显著提高了模型在特定领域知识问答上的表现，并可用于预训练以构建更具事实性的模型。", "motivation": "大型语言模型（LLMs）的参数记忆中存储了大量知识，但其事实学习和回忆能力不可靠，受训练数据中事实的普遍性等因素影响，且缺乏工具来确保模型可靠地学习特定知识体系。", "method": "提出“主动阅读”框架，训练模型通过自我生成的学习策略来学习给定材料。首先，模型学习如何“阅读”并吸收知识；其次，通过这种方法在专家领域进行训练和评估。", "result": "应用“主动阅读”训练的模型比传统微调和其他数据增强方法吸收了显著更多的知识。在SimpleQA的Wikipedia子集上，专家8B模型达到66%（相对提升313%）；在FinanceBench上达到26%（相对提升160%）。此外，证明“主动阅读”可用于预训练规模，并发布了Meta WikiExpert-8B模型，在事实问答方面超越了参数量更大的模型。", "conclusion": "“主动阅读”是一种有效的方法，可以显著提高大型语言模型在特定领域知识吸收和事实问答方面的能力，并且该方法可以扩展到预训练阶段，从而构建更具事实性的模型。"}}
{"id": "2508.09239", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09239", "abs": "https://arxiv.org/abs/2508.09239", "authors": ["Zheng Zhou", "Yu-Jie Xiong", "Chun-Ming Xia", "Jia-Chen Zhang", "Hong-Jian Zhan"], "title": "Gradient-Direction-Aware Density Control for 3D Gaussian Splatting", "comment": null, "summary": "The emergence of 3D Gaussian Splatting (3DGS) has significantly advanced\nnovel view synthesis through explicit scene representation, enabling real-time\nphotorealistic rendering. However, existing approaches manifest two critical\nlimitations in complex scenarios: (1) Over-reconstruction occurs when\npersistent large Gaussians cannot meet adaptive splitting thresholds during\ndensity control. This is exacerbated by conflicting gradient directions that\nprevent effective splitting of these Gaussians; (2) Over-densification of\nGaussians occurs in regions with aligned gradient aggregation, leading to\nredundant component proliferation. This redundancy significantly increases\nmemory overhead due to unnecessary data retention. We present\nGradient-Direction-Aware Gaussian Splatting (GDAGS), a gradient-direction-aware\nadaptive density control framework to address these challenges. Our key\ninnovations: the gradient coherence ratio (GCR), computed through normalized\ngradient vector norms, which explicitly discriminates Gaussians with concordant\nversus conflicting gradient directions; and a nonlinear dynamic weighting\nmechanism leverages the GCR to enable gradient-direction-aware density control.\nSpecifically, GDAGS prioritizes conflicting-gradient Gaussians during splitting\noperations to enhance geometric details while suppressing redundant\nconcordant-direction Gaussians. Conversely, in cloning processes, GDAGS\npromotes concordant-direction Gaussian densification for structural completion\nwhile preventing conflicting-direction Gaussian overpopulation. Comprehensive\nevaluations across diverse real-world benchmarks demonstrate that GDAGS\nachieves superior rendering quality while effectively mitigating\nover-reconstruction, suppressing over-densification, and constructing compact\nscene representations with 50\\% reduced memory consumption through optimized\nGaussians utilization.", "AI": {"tldr": "本文提出Gradient-Direction-Aware Gaussian Splatting (GDAGS)，通过引入梯度一致性比率和非线性动态加权机制，解决了3D Gaussian Splatting在复杂场景中存在的过重建和过密化问题，实现了更高的渲染质量和50%的内存占用减少。", "motivation": "现有3D Gaussian Splatting方法在复杂场景中存在两个主要限制：1) 过重建：大高斯函数因梯度方向冲突而难以有效分裂，导致几何细节不足；2) 过密化：梯度方向一致的区域出现冗余高斯函数增殖，导致内存开销显著增加。", "method": "GDAGS提出梯度方向感知自适应密度控制框架。关键创新包括：1) 梯度一致性比率（GCR）：通过归一化梯度向量范数计算，用于区分梯度方向一致或冲突的高斯函数；2) 非线性动态加权机制：利用GCR实现梯度方向感知的密度控制。具体而言，分裂操作优先处理梯度冲突的高斯函数以增强几何细节，同时抑制冗余的梯度一致高斯函数；克隆操作则促进梯度一致高斯函数的密化以完成结构，同时防止梯度冲突高斯函数过度增殖。", "result": "在多样化的真实世界基准测试中，GDAGS实现了卓越的渲染质量，有效缓解了过重建，抑制了过密化，并通过优化高斯函数利用率，构建了紧凑的场景表示，内存消耗减少了50%。", "conclusion": "GDAGS通过引入梯度方向感知的密度控制策略，有效解决了3D Gaussian Splatting在复杂场景中的过重建和过密化问题，显著提升了渲染质量，并大幅降低了内存占用，实现了更优的高斯函数利用和更紧凑的场景表示。"}}
{"id": "2508.09797", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.09797", "abs": "https://arxiv.org/abs/2508.09797", "authors": ["Dongcheng Cao", "Jin Zhou", "Xian Wang", "Shuo Li"], "title": "FLARE: Agile Flights for Quadrotor Cable-Suspended Payload System via Reinforcement Learning", "comment": null, "summary": "Agile flight for the quadrotor cable-suspended payload system is a formidable\nchallenge due to its underactuated, highly nonlinear, and hybrid dynamics.\nTraditional optimization-based methods often struggle with high computational\ncosts and the complexities of cable mode transitions, limiting their real-time\napplicability and maneuverability exploitation. In this letter, we present\nFLARE, a reinforcement learning (RL) framework that directly learns agile\nnavigation policy from high-fidelity simulation. Our method is validated across\nthree designed challenging scenarios, notably outperforming a state-of-the-art\noptimization-based approach by a 3x speedup during gate traversal maneuvers.\nFurthermore, the learned policies achieve successful zero-shot sim-to-real\ntransfer, demonstrating remarkable agility and safety in real-world\nexperiments, running in real time on an onboard computer.", "AI": {"tldr": "本文提出FLARE，一个基于强化学习的框架，用于实现四旋翼无人机携带缆绳悬挂负载的敏捷飞行，解决了传统优化方法计算成本高和实时性差的问题，并在仿真和真实世界中展现出卓越的性能和零样本迁移能力。", "motivation": "四旋翼无人机携带缆绳悬挂负载系统的敏捷飞行极具挑战性，因为它具有欠驱动、高度非线性及混合动力学特性。传统的基于优化的方法计算成本高昂，且难以处理缆绳模式转换的复杂性，从而限制了其实时应用和机动性开发。", "method": "本文提出了FLARE，一个强化学习（RL）框架，直接从高保真仿真中学习敏捷导航策略。", "result": "FLARE在三个具有挑战性的场景中得到验证，在穿越门洞机动时比最先进的基于优化方法的速度快3倍。此外，所学习的策略实现了成功的零样本仿真到真实世界迁移，在真实世界实验中展示了卓越的敏捷性和安全性，并且可以在机载计算机上实时运行。", "conclusion": "强化学习框架FLARE能够有效地实现四旋翼无人机携带缆绳悬挂负载的敏捷飞行，显著提升了飞行速度和实时性能，并成功实现了仿真到真实世界的迁移，克服了传统方法的局限性。"}}
{"id": "2508.09585", "categories": ["cs.CV", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.09585", "abs": "https://arxiv.org/abs/2508.09585", "authors": ["Stefan Haag", "Bharanidhar Duraisamy", "Felix Govaers", "Wolfgang Koch", "Martin Fritzsche", "Juergen Dickmann"], "title": "Offline Auto Labeling: BAAS", "comment": null, "summary": "This paper introduces BAAS, a new Extended Object Tracking (EOT) and\nfusion-based label annotation framework for radar detections in autonomous\ndriving. Our framework utilizes Bayesian-based tracking, smoothing and\neventually fusion methods to provide veritable and precise object trajectories\nalong with shape estimation to provide annotation labels on the detection level\nunder various supervision levels. Simultaneously, the framework provides\nevaluation of tracking performance and label annotation. If manually labeled\ndata is available, each processing module can be analyzed independently or\ncombined with other modules to enable closed-loop continuous improvements. The\nframework performance is evaluated in a challenging urban real-world scenario\nin terms of tracking performance and the label annotation errors. We\ndemonstrate the functionality of the proposed approach for varying dynamic\nobjects and class types", "AI": {"tldr": "本文介绍BAAS框架，一种用于自动驾驶中雷达检测的扩展目标跟踪（EOT）和融合式标签标注方法，提供精确轨迹和形状估计，并支持性能评估和持续改进。", "motivation": "自动驾驶需要对雷达检测进行精确的目标轨迹和形状估计，以生成高质量的标注标签，并需要一个框架来评估跟踪性能和标签标注错误，同时支持持续改进。", "method": "BAAS框架采用基于贝叶斯的跟踪、平滑和融合方法，提供目标轨迹和形状估计，生成检测级别的标注标签。它支持不同监督级别，并能评估跟踪性能和标签标注。如果有人工标注数据，各模块可独立或组合分析，实现闭环持续改进。", "result": "该框架在具有挑战性的城市真实场景中进行了评估，展示了其在跟踪性能和标签标注误差方面的表现。结果证明了该方法对不同动态物体和类别类型的有效性。", "conclusion": "BAAS框架为雷达检测提供了一种有效且精确的扩展目标跟踪和融合式标签标注解决方案，适用于自动驾驶中的多样化动态物体和复杂场景，并支持性能评估和持续改进。"}}
{"id": "2508.09497", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09497", "abs": "https://arxiv.org/abs/2508.09497", "authors": ["Siyuan Meng", "Junming Liu", "Yirong Chen", "Song Mao", "Pinlong Cai", "Guohang Yan", "Botian Shi", "Ding Wang"], "title": "From Ranking to Selection: A Simple but Efficient Dynamic Passage Selector for Retrieval Augmented Generation", "comment": "9 pages, 4 tables", "summary": "Retrieval-augmented generation (RAG) systems are often bottlenecked by their\nreranking modules, which typically score passages independently and select a\nfixed Top-K size. This approach struggles with complex multi-hop queries that\nrequire synthesizing evidence across multiple documents, creating a trade-off\nwhere small K values omit crucial information and large K values introduce\nnoise. To address this, we introduce the Dynamic Passage Selector (DPS), a\nnovel reranking framework that treats passage selection as a supervised\nlearning problem. Unlike traditional point-wise or list-wise methods, DPS is\nfine-tuned to capture inter-passage dependencies and dynamically select the\nmost relevant set of passages for generation. As a seamless plug-and-play\nmodule, DPS requires no modifications to the standard RAG pipeline.\nComprehensive evaluations on five benchmarks show that DPS consistently\noutperforms state-of-the-art rerankers and fine-tuning methods. Notably, on the\nchallenging MuSiQue dataset, DPS improves the F1-score by 30.06% and 15.4% over\nstrong baselines like Qwen3-reranker and RankingGPT, respectively. Our results\ndemonstrate that by enabling adaptive evidence selection, DPS substantially\nenhances reasoning capabilities in complex RAG scenarios.", "AI": {"tldr": "本文提出动态段落选择器（DPS），一种新颖的重排序框架，通过将段落选择视为监督学习问题，动态选择最相关的段落集，显著提升了复杂RAG场景下的推理能力。", "motivation": "RAG系统常受限于重排序模块，该模块通常独立评分段落并选择固定Top-K，导致在复杂多跳查询中，小K值遗漏信息，大K值引入噪声，难以有效合成跨文档证据。", "method": "引入动态段落选择器（DPS），将段落选择视为监督学习问题，而非传统的点对点或列表式方法。DPS经过微调以捕捉段落间依赖性，并动态选择最相关的段落集用于生成。它是一个即插即用的模块，无需修改标准RAG管道。", "result": "DPS在五个基准测试中持续优于最先进的重排序器和微调方法。特别是在MuSiQue数据集上，DPS相较于Qwen3-reranker和RankingGPT等强基线，F1分数分别提高了30.06%和15.4%。", "conclusion": "通过实现自适应证据选择，DPS显著增强了复杂RAG场景中的推理能力。"}}
{"id": "2508.09241", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09241", "abs": "https://arxiv.org/abs/2508.09241", "authors": ["Fengxian Ji", "Jingpu Yang", "Zirui Song", "Yuanxi Wang", "Zhexuan Cui", "Yuke Li", "Qian Jiang", "Miao Fang", "Xiuying Chen"], "title": "FineState-Bench: A Comprehensive Benchmark for Fine-Grained State Control in GUI Agents", "comment": "submit/6682470 (Fengxian Ji)", "summary": "With the rapid advancement of generative artificial intelligence technology,\nGraphical User Interface (GUI) agents have demonstrated tremendous potential\nfor autonomously managing daily tasks through natural language instructions.\nHowever, current evaluation frameworks for GUI agents suffer from fundamental\nflaws: existing benchmarks overly focus on coarse-grained task completion while\nneglecting fine-grained control capabilities crucial for real-world\napplications. To address this, we introduce FineState-Bench, the first\nevaluation and diagnostic standard for fine-grained GUI proxy operations,\ndesigned to quantify fine-grained control. This multi-platform (desktop, Web,\nmobile) framework includes 2257 task benchmarks in four components and uses a\nfour-phase indicator for comprehensive perception-to-control assessment. To\nanalyze perception and positioning for refined operations, we developed the\nplug-and-play Visual Diagnostic Assistant (VDA), enabling the first\nquantitative decoupling analysis of these capabilities. Experimental results on\nour benchmark show that the most advanced models achieve only 32.8%\nfine-grained interaction accuracy. Using our VDA in controlled experiments,\nquantifying the impact of visual capabilities, we showed that ideal visual\nlocalization boosts Gemini-2.5-Flash's success rate by 14.9\\%. Our diagnostic\nframework confirms for the first time that the primary bottleneck for current\nGUI proxies is basic visual positioning capability.All resources are fully\nopen-source. github: https://github.com/AnonymousThewarehouse/FineState-Bench\nhuggingface: https://huggingface.co/datasets/Willtime2006/Static-FineBench", "AI": {"tldr": "该研究引入了FineState-Bench，一个用于评估和诊断GUI代理精细粒度操作的新基准，并开发了视觉诊断助手（VDA）来量化分析感知和定位能力，首次确认视觉定位是当前GUI代理的主要瓶颈。", "motivation": "当前的GUI代理评估框架过度关注粗粒度任务完成，忽视了对真实世界应用至关重要的精细粒度控制能力，导致评估存在根本性缺陷。", "method": "开发了多平台（桌面、Web、移动）的FineState-Bench基准，包含2257个任务和四阶段指标，用于全面评估感知到控制的能力。同时，设计了即插即用的视觉诊断助手（VDA），以首次对GUI代理的感知和定位能力进行定量解耦分析。", "result": "在FineState-Bench上的实验表明，最先进的模型在精细粒度交互方面的准确率仅为32.8%。通过VDA的受控实验量化视觉能力的影响，发现理想的视觉定位能将Gemini-2.5-Flash的成功率提高14.9%。诊断框架首次证实，当前GUI代理的主要瓶颈是基本的视觉定位能力。", "conclusion": "FineState-Bench和VDA为GUI代理的精细粒度操作提供了首个评估和诊断标准，并明确指出视觉定位能力是限制当前GUI代理性能提升的关键瓶颈，为未来的研究指明了方向。"}}
{"id": "2508.09325", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.09325", "abs": "https://arxiv.org/abs/2508.09325", "authors": ["Alexandre Brown", "Glen Berseth"], "title": "SegDAC: Segmentation-Driven Actor-Critic for Visual Reinforcement Learning", "comment": null, "summary": "Visual reinforcement learning (RL) is challenging due to the need to learn\nboth perception and actions from high-dimensional inputs and noisy rewards.\nAlthough large perception models exist, integrating them effectively into RL\nfor visual generalization and improved sample efficiency remains unclear. We\npropose SegDAC, a Segmentation-Driven Actor-Critic method. SegDAC uses Segment\nAnything (SAM) for object-centric decomposition and YOLO-World to ground\nsegments semantically via text prompts. It includes a novel transformer-based\narchitecture that supports a dynamic number of segments at each time step and\neffectively learns which segments to focus on using online RL, without using\nhuman labels. By evaluating SegDAC over a challenging visual generalization\nbenchmark using Maniskill3, which covers diverse manipulation tasks under\nstrong visual perturbations, we demonstrate that SegDAC achieves significantly\nbetter visual generalization, doubling prior performance on the hardest setting\nand matching or surpassing prior methods in sample efficiency across all\nevaluated tasks.", "AI": {"tldr": "SegDAC是一种基于分割的Actor-Critic视觉强化学习方法，它利用SAM进行物体分解，YOLO-World进行语义接地，并结合新型Transformer架构，在无需人工标注的情况下，显著提升了视觉泛化能力和样本效率。", "motivation": "视觉强化学习面临从高维输入和噪声奖励中学习感知与动作的挑战，同时，如何有效整合大型感知模型以实现视觉泛化和提高样本效率仍不明确。", "method": "SegDAC（Segmentation-Driven Actor-Critic）方法使用Segment Anything (SAM) 进行以物体为中心的分解，并利用YOLO-World通过文本提示对分割区域进行语义接地。它包含一个新颖的基于Transformer的架构，支持每个时间步动态数量的分割，并通过在线强化学习有效学习关注哪些分割区域，无需人工标签。", "result": "在Maniskill3视觉泛化基准测试中，SegDAC在最具挑战性的设置下将先前性能提高了一倍，并匹配或超越了所有评估任务中现有方法的样本效率，显著提升了视觉泛化能力。", "conclusion": "SegDAC通过有效整合大型感知模型和创新的架构设计，克服了视觉强化学习中的泛化和效率挑战，展示了在复杂操作任务中优异的性能。"}}
{"id": "2508.09836", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.09836", "abs": "https://arxiv.org/abs/2508.09836", "authors": ["Anirvan Dutta", "Alexis WM Devillard", "Zhihuan Zhang", "Xiaoxiao Cheng", "Etienne Burdet"], "title": "Embodied Tactile Perception of Soft Objects Properties", "comment": null, "summary": "To enable robots to develop human-like fine manipulation, it is essential to\nunderstand how mechanical compliance, multi-modal sensing, and purposeful\ninteraction jointly shape tactile perception. In this study, we use a dedicated\nmodular e-Skin with tunable mechanical compliance and multi-modal sensing\n(normal, shear forces and vibrations) to systematically investigate how sensing\nembodiment and interaction strategies influence robotic perception of objects.\nLeveraging a curated set of soft wave objects with controlled viscoelastic and\nsurface properties, we explore a rich set of palpation primitives-pressing,\nprecession, sliding that vary indentation depth, frequency, and directionality.\nIn addition, we propose the latent filter, an unsupervised, action-conditioned\ndeep state-space model of the sophisticated interaction dynamics and infer\ncausal mechanical properties into a structured latent space. This provides\ngeneralizable and in-depth interpretable representation of how embodiment and\ninteraction determine and influence perception. Our investigation demonstrates\nthat multi-modal sensing outperforms uni-modal sensing. It highlights a nuanced\ninteraction between the environment and mechanical properties of e-Skin, which\nshould be examined alongside the interaction by incorporating temporal\ndynamics.", "AI": {"tldr": "为实现机器人类似人类的精细操作，本研究使用具有可调柔顺性和多模态传感（法向、剪切力、振动）的e-Skin，通过多种触诊原语（按压、进动、滑动）系统性探究感知具身和交互策略对机器人物体感知的影响。研究提出了一个无监督、动作条件化的深度状态空间模型“潜在滤波器”来推断因果机械属性。结果表明多模态传感优于单模态传感，并强调了环境与e-Skin机械特性之间微妙的相互作用。", "motivation": "为了使机器人能够发展出类似人类的精细操作能力，理解机械柔顺性、多模态传感以及有目的的交互如何共同塑造触觉感知是至关重要的。", "method": "研究使用了一个专门的模块化e-Skin，该e-Skin具有可调的机械柔顺性和多模态传感能力（法向力、剪切力、振动）。实验利用了一组精心设计的具有受控粘弹性和表面特性的软波浪形物体，并探索了多种触诊原语（按压、进动、滑动），这些原语的压痕深度、频率和方向性均可变。此外，研究提出了一种名为“潜在滤波器”的无监督、动作条件化的深度状态空间模型，用于推断复杂的交互动力学，并将因果机械属性映射到结构化潜在空间中。", "result": "研究表明多模态传感的性能优于单模态传感。它强调了环境与e-Skin机械特性之间存在微妙的相互作用。", "conclusion": "多模态传感对于机器人触觉感知至关重要。环境与e-Skin机械特性之间的微妙相互作用应与交互作用一起，通过整合时间动力学来进一步研究。提出的模型提供了关于具身和交互如何决定和影响感知的可泛化且深度可解释的表示。"}}
{"id": "2508.09876", "categories": ["cs.RO", "cs.SY", "eess.SY", "I.2.9"], "pdf": "https://arxiv.org/pdf/2508.09876", "abs": "https://arxiv.org/abs/2508.09876", "authors": ["Xiaowei Tan", "Weizhong Jiang", "Bi Zhang", "Wanxin Chen", "Yiwen Zhao", "Ning Li", "Lianqing Liu", "Xingang Zhao"], "title": "A Shank Angle-Based Control System Enables Soft Exoskeleton to Assist Human Non-Steady Locomotion", "comment": "49 pages, 20 figures, 4 tables", "summary": "Exoskeletons have been shown to effectively assist humans during steady\nlocomotion. However, their effects on non-steady locomotion, characterized by\nnonlinear phase progression within a gait cycle, remain insufficiently\nexplored, particularly across diverse activities. This work presents a shank\nangle-based control system that enables the exoskeleton to maintain real-time\ncoordination with human gait, even under phase perturbations, while dynamically\nshaping assistance profiles to match the biological ankle moment patterns\nacross walking, running, stair negotiation tasks. The control system consists\nof an assistance profile online generation method and a model-based feedforward\ncontrol method. The assistance profile is formulated as a dual-Gaussian model\nwith the shank angle as the independent variable. Leveraging only IMU\nmeasurements, the model parameters are updated online each stride to adapt to\ninter- and intra-individual biomechanical variability. The profile tracking\ncontrol employs a human-exoskeleton kinematics and stiffness model as a\nfeedforward component, reducing reliance on historical control data due to the\nlack of clear and consistent periodicity in non-steady locomotion. Three\nexperiments were conducted using a lightweight soft exoskeleton with multiple\nsubjects. The results validated the effectiveness of each individual method,\ndemonstrated the robustness of the control system against gait perturbations\nacross various activities, and revealed positive biomechanical and\nphysiological responses of human users to the exoskeleton's mechanical\nassistance.", "AI": {"tldr": "该研究提出了一种基于小腿角度的控制系统，使外骨骼能在非稳态步态（如行走、跑步、上下楼梯）中与人体实时协调，并动态调整辅助，通过在线生成辅助曲线和模型前馈控制，实现了对步态扰动的鲁棒性，并带来了积极的人体生物力学和生理响应。", "motivation": "外骨骼在稳定步态辅助方面已显示出有效性，但其在非稳态步态（特征为步态周期内非线性相位进展）中的效果，尤其是在不同活动类型下的探索尚不充分。", "method": "提出一个基于小腿角度的控制系统，包含：1) 辅助曲线在线生成方法：将辅助曲线表述为以小腿角度为自变量的双高斯模型，仅利用IMU测量，每步在线更新模型参数以适应个体差异；2) 模型前馈控制方法：采用人体-外骨骼运动学和刚度模型作为前馈分量，减少对历史控制数据的依赖。该系统在一个轻量级软外骨骼上进行了验证。", "result": "实验结果验证了每个独立方法的有效性，展示了控制系统在各种活动中对抗步态扰动的鲁棒性，并揭示了用户对外骨骼机械辅助的积极生物力学和生理响应。", "conclusion": "该基于小腿角度的控制系统能够使外骨骼在非稳态步态下与人体保持实时协调，并动态调整辅助，有效应对步态扰动，为用户提供了积极的辅助效果。"}}
{"id": "2508.09515", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.09515", "abs": "https://arxiv.org/abs/2508.09515", "authors": ["Jakub Šmíd", "Pavel Přibáň", "Pavel Král"], "title": "LACA: Improving Cross-lingual Aspect-Based Sentiment Analysis with LLM Data Augmentation", "comment": "Published in Proceedings of the 63rd Annual Meeting of the\n  Association for Computational Linguistics; Volume 1: Long Papers (ACL 2025).\n  Official version: https://aclanthology.org/2025.acl-long.41/", "summary": "Cross-lingual aspect-based sentiment analysis (ABSA) involves detailed\nsentiment analysis in a target language by transferring knowledge from a source\nlanguage with available annotated data. Most existing methods depend heavily on\noften unreliable translation tools to bridge the language gap. In this paper,\nwe propose a new approach that leverages a large language model (LLM) to\ngenerate high-quality pseudo-labelled data in the target language without the\nneed for translation tools. First, the framework trains an ABSA model to obtain\npredictions for unlabelled target language data. Next, LLM is prompted to\ngenerate natural sentences that better represent these noisy predictions than\nthe original text. The ABSA model is then further fine-tuned on the resulting\npseudo-labelled dataset. We demonstrate the effectiveness of this method across\nsix languages and five backbone models, surpassing previous state-of-the-art\ntranslation-based approaches. The proposed framework also supports generative\nmodels, and we show that fine-tuned LLMs outperform smaller multilingual\nmodels.", "AI": {"tldr": "本文提出一种利用大型语言模型（LLM）生成高质量伪标签数据的新方法，用于跨语言基于方面的情感分析（ABSA），无需依赖翻译工具，并超越了现有SOTA方法。", "motivation": "大多数现有的跨语言ABSA方法严重依赖不可靠的翻译工具来弥合语言鸿沟。", "method": "首先，训练一个ABSA模型以获取目标语言无标签数据的预测（通常是噪声的）。接着，利用LLM生成自然句子，这些句子能更好地代表这些噪声预测，而非原始文本。最后，使用生成的伪标签数据集进一步微调ABSA模型。该框架还支持生成模型。", "result": "该方法在六种语言和五种骨干模型上均表现出有效性，超越了以往基于翻译的最先进方法。微调后的LLM表现优于小型多语言模型。", "conclusion": "所提出的基于LLM的伪标签生成框架能有效解决跨语言ABSA中的语言差异问题，无需翻译工具，并取得了领先的性能。"}}
{"id": "2508.09245", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09245", "abs": "https://arxiv.org/abs/2508.09245", "authors": ["Jeffri Murrugarra-LLerena", "Haoran Niu", "K. Suzanne Barber", "Hal Daumé III", "Yang Trista Cao", "Paola Cascante-Bonilla"], "title": "Beyond Blanket Masking: Examining Granularity for Privacy Protection in Images Captured by Blind and Low Vision Users", "comment": null, "summary": "As visual assistant systems powered by visual language models (VLMs) become\nmore prevalent, concerns over user privacy have grown, particularly for blind\nand low vision users who may unknowingly capture personal private information\nin their images. Existing privacy protection methods rely on coarse-grained\nsegmentation, which uniformly masks entire private objects, often at the cost\nof usability. In this work, we propose FiGPriv, a fine-grained privacy\nprotection framework that selectively masks only high-risk private information\nwhile preserving low-risk information. Our approach integrates fine-grained\nsegmentation with a data-driven risk scoring mechanism. We evaluate our\nframework using the BIV-Priv-Seg dataset and show that FiG-Priv preserves +26%\nof image content, enhancing the ability of VLMs to provide useful responses by\n11% and identify the image content by 45%, while ensuring privacy protection.\nProject Page: https://artcs1.github.io/VLMPrivacy/", "AI": {"tldr": "FiGPriv是一种细粒度隐私保护框架，专为视觉语言模型（VLM）设计，通过选择性遮蔽高风险隐私信息，在保护隐私的同时提升了VLM的可用性和图像内容保留率。", "motivation": "随着VLM驱动的视觉助手系统普及，用户隐私担忧日益增长，特别是视障用户可能无意中捕获私人信息。现有隐私保护方法采用粗粒度分割，统一遮蔽整个私有对象，牺牲了可用性。", "method": "本文提出了FiGPriv框架，它结合了细粒度分割和数据驱动的风险评分机制，选择性地遮蔽高风险隐私信息，同时保留低风险信息。", "result": "在BIV-Priv-Seg数据集上的评估显示，FiGPriv保留了+26%的图像内容，将VLM提供有用响应的能力提高了11%，识别图像内容的能力提高了45%，同时确保了隐私保护。", "conclusion": "FiGPriv在确保隐私保护的前提下，显著提高了图像内容的保留率和VLM的实用性（包括提供有用响应和识别图像内容的能力），解决了现有粗粒度方法的可用性问题。"}}
{"id": "2508.09362", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.09362", "abs": "https://arxiv.org/abs/2508.09362", "authors": ["Md. Milon Islam", "Md Rezwanul Haque", "S M Taslim Uddin Raju", "Fakhri Karray"], "title": "FusionEnsemble-Net: An Attention-Based Ensemble of Spatiotemporal Networks for Multimodal Sign Language Recognition", "comment": "Accepted for the IEEE/CVF International Conference on Computer Vision\n  (ICCV), Honolulu, Hawaii, USA. 1st MSLR Workshop 2025", "summary": "Accurate recognition of sign language in healthcare communication poses a\nsignificant challenge, requiring frameworks that can accurately interpret\ncomplex multimodal gestures. To deal with this, we propose FusionEnsemble-Net,\na novel attention-based ensemble of spatiotemporal networks that dynamically\nfuses visual and motion data to enhance recognition accuracy. The proposed\napproach processes RGB video and range Doppler map radar modalities\nsynchronously through four different spatiotemporal networks. For each network,\nfeatures from both modalities are continuously fused using an attention-based\nfusion module before being fed into an ensemble of classifiers. Finally, the\noutputs of these four different fused channels are combined in an ensemble\nclassification head, thereby enhancing the model's robustness. Experiments\ndemonstrate that FusionEnsemble-Net outperforms state-of-the-art approaches\nwith a test accuracy of 99.44% on the large-scale MultiMeDaLIS dataset for\nItalian Sign Language. Our findings indicate that an ensemble of diverse\nspatiotemporal networks, unified by attention-based fusion, yields a robust and\naccurate framework for complex, multimodal isolated gesture recognition tasks.\nThe source code is available at:\nhttps://github.com/rezwanh001/Multimodal-Isolated-Italian-Sign-Language-Recognition.", "AI": {"tldr": "本文提出了FusionEnsemble-Net，一个基于注意力机制的时空网络集成模型，通过动态融合视觉和雷达运动数据，显著提高了医疗沟通中手语识别的准确性。", "motivation": "在医疗通信中，手语的准确识别是一个重大挑战，需要能够精确解释复杂多模态手势的框架。", "method": "FusionEnsemble-Net模型同步处理RGB视频和雷达多普勒图数据。它包含四个不同的时空网络，每个网络都使用基于注意力机制的融合模块持续融合两种模态的特征，然后输入到分类器集成中。最后，将这四个不同融合通道的输出在一个集成分类头中组合，以增强模型的鲁棒性。", "result": "实验结果表明，FusionEnsemble-Net在大型MultiMeDaLIS意大利手语数据集上取得了99.44%的测试准确率，优于现有最先进的方法。", "conclusion": "研究结果表明，由注意力机制融合的多元时空网络集成，为复杂多模态的孤立手势识别任务提供了一个鲁棒且准确的框架。"}}
{"id": "2508.09846", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.09846", "abs": "https://arxiv.org/abs/2508.09846", "authors": ["Donghoon Baek", "Amartya Purushottam", "Jason J. Choi", "Joao Ramos"], "title": "Whole-Body Bilateral Teleoperation with Multi-Stage Object Parameter Estimation for Wheeled Humanoid Locomanipulation", "comment": null, "summary": "This paper presents an object-aware whole-body bilateral teleoperation\nframework for wheeled humanoid loco-manipulation. This framework combines\nwhole-body bilateral teleoperation with an online multi-stage object inertial\nparameter estimation module, which is the core technical contribution of this\nwork. The multi-stage process sequentially integrates a vision-based object\nsize estimator, an initial parameter guess generated by a large vision-language\nmodel (VLM), and a decoupled hierarchical sampling strategy. The visual size\nestimate and VLM prior offer a strong initial guess of the object's inertial\nparameters, significantly reducing the search space for sampling-based\nrefinement and improving the overall estimation speed. A hierarchical strategy\nfirst estimates mass and center of mass, then infers inertia from object size\nto ensure physically feasible parameters, while a decoupled multi-hypothesis\nscheme enhances robustness to VLM prior errors. Our estimator operates in\nparallel with high-fidelity simulation and hardware, enabling real-time online\nupdates. The estimated parameters are then used to update the wheeled\nhumanoid's equilibrium point, allowing the operator to focus more on locomotion\nand manipulation. This integration improves the haptic force feedback for\ndynamic synchronization, enabling more dynamic whole-body teleoperation. By\ncompensating for object dynamics using the estimated parameters, the framework\nalso improves manipulation tracking while preserving compliant behavior. We\nvalidate the system on a customized wheeled humanoid with a robotic gripper and\nhuman-machine interface, demonstrating real-time execution of lifting,\ndelivering, and releasing tasks with a payload weighing approximately one-third\nof the robot's body weight.", "AI": {"tldr": "本文提出了一种面向对象的全身双边遥操作框架，用于轮式人形机器人的移动-操作任务。核心贡献在于其在线多阶段物体惯性参数估计模块，该模块结合视觉、大视觉语言模型和分层采样策略，实现对未知物体动态的实时补偿，从而提升遥操作的触觉反馈和操作跟踪性能。", "motivation": "在轮式人形机器人进行动态移动和操作任务时，由于物体动力学未知，难以提供准确的触觉反馈并保持良好的操作跟踪性能。传统遥操作系统缺乏对所操作物体动态特性的感知和补偿能力，限制了其在复杂、动态环境中的应用。", "method": "该框架的核心是多阶段在线物体惯性参数估计模块。该模块顺序整合了：1) 基于视觉的物体尺寸估计器；2) 由大型视觉语言模型（VLM）生成的初始参数猜测；3) 解耦分层采样策略。视觉尺寸估计和VLM先验大大缩小了搜索空间，提高了估计速度。分层策略先估计质量和质心，再根据物体尺寸推断惯性，确保物理可行性。解耦多假设方案增强了对VLM先验错误的鲁棒性。估计的参数用于更新轮式人形机器人的平衡点，改善触觉力反馈并补偿物体动力学，从而提升操作跟踪性能并保持柔顺行为。", "result": "该系统实现了实时在线更新物体惯性参数。通过补偿物体动力学，显著改善了动态同步的触觉力反馈，并提高了操作跟踪精度，同时保持了柔顺行为。在定制的轮式人形机器人上进行了验证，成功实时执行了举起、搬运和释放任务，有效负载约为机器人体重的三分之一。", "conclusion": "该研究通过引入在线多阶段物体惯性参数估计，显著提升了轮式人形机器人全身双边遥操作的能力。该框架使得机器人能够实时适应未知物体动力学，从而实现更动态、更鲁棒的遥操作，并提供更佳的触觉反馈和操作性能。"}}
{"id": "2508.09516", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.09516", "abs": "https://arxiv.org/abs/2508.09516", "authors": ["Jakub Šmíd", "Pavel Král"], "title": "Cross-lingual Aspect-Based Sentiment Analysis: A Survey on Tasks, Approaches, and Challenges", "comment": "Submitted version prior to peer review. Updated version accepted in\n  Information Fusion. Official version:\n  https://www.sciencedirect.com/science/article/pii/S1566253525001460", "summary": "Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis\ntask that focuses on understanding opinions at the aspect level, including\nsentiment towards specific aspect terms, categories, and opinions. While ABSA\nresearch has seen significant progress, much of the focus has been on\nmonolingual settings. Cross-lingual ABSA, which aims to transfer knowledge from\nresource-rich languages (such as English) to low-resource languages, remains an\nunder-explored area, with no systematic review of the field. This paper aims to\nfill that gap by providing a comprehensive survey of cross-lingual ABSA. We\nsummarize key ABSA tasks, including aspect term extraction, aspect sentiment\nclassification, and compound tasks involving multiple sentiment elements.\nAdditionally, we review the datasets, modelling paradigms, and cross-lingual\ntransfer methods used to solve these tasks. We also examine how existing work\nin monolingual and multilingual ABSA, as well as ABSA with LLMs, contributes to\nthe development of cross-lingual ABSA. Finally, we highlight the main\nchallenges and suggest directions for future research to advance cross-lingual\nABSA systems.", "AI": {"tldr": "本文对跨语言方面级情感分析（Cross-lingual ABSA）进行了首次全面综述，总结了相关任务、数据集、模型、跨语言迁移方法，并指出了未来研究方向。", "motivation": "方面级情感分析（ABSA）在单语言环境下取得了显著进展，但跨语言ABSA（将知识从资源丰富语言迁移到低资源语言）仍未得到充分探索，且缺乏系统的综述。", "method": "本文通过文献调研，总结了关键的ABSA任务（如方面术语提取、方面情感分类及复合任务），回顾了用于解决这些任务的数据集、建模范式和跨语言迁移方法。此外，还探讨了单语言和多语言ABSA以及大型语言模型（LLMs）在ABSA中的现有工作如何促进跨语言ABSA的发展。", "result": "提供了一个全面的跨语言ABSA综述，涵盖了任务、数据集、建模范式和跨语言迁移方法，并分析了相关领域工作的贡献。", "conclusion": "指出了跨语言ABSA面临的主要挑战，并为未来的研究提出了方向，以推动跨语言ABSA系统的发展。"}}
{"id": "2508.09262", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.09262", "abs": "https://arxiv.org/abs/2508.09262", "authors": ["Dongwoo Kang", "Akhil Perincherry", "Zachary Coalson", "Aiden Gabriel", "Stefan Lee", "Sanghyun Hong"], "title": "Harnessing Input-Adaptive Inference for Efficient VLN", "comment": "Accepted to ICCV 2025 [Poster]", "summary": "An emerging paradigm in vision-and-language navigation (VLN) is the use of\nhistory-aware multi-modal transformer models. Given a language instruction,\nthese models process observation and navigation history to predict the most\nappropriate action for an agent. While they have significantly improved\nperformance, the scale of these models can be a bottleneck in practical\nsettings with limited computational resources. In this work, we propose a novel\ninput-adaptive navigation method to enhance VLN model efficiency. We first show\nthat existing input-adaptive mechanisms fail to reduce computations without\nsubstantial performance degradation. To address this, we introduce three\nadaptive algorithms, each deployed at a different level: (1) To improve spatial\nefficiency, we selectively process panoramic views at each observation of an\nagent. (2) To improve intra-model efficiency, we propose importance-based\nadaptive thresholding for the early-exit methods. (3) To improve temporal\nefficiency, we implement a caching mechanism that prevents reprocessing of\nviews previously seen by the agent. In evaluations on seven VLN benchmarks, we\ndemonstrate over a 2$\\times$ reduction in computation across three\noff-the-shelf agents in both standard and continuous environments. Our code is\npublicly available at\nhttps://github.com/secure-ai-systems-group/adaptive-vision-and-language-navigation.", "AI": {"tldr": "本文提出了一种新颖的输入自适应导航方法，通过在空间、模型内部和时间层面引入三种自适应算法，显著提升了视觉语言导航（VLN）模型的计算效率，同时保持了性能。", "motivation": "现有的历史感知多模态Transformer模型在视觉语言导航任务中表现出色，但其庞大的模型规模在计算资源有限的实际应用中成为瓶颈。现有的输入自适应机制往往会导致显著的性能下降。", "method": "为了提高VLN模型的效率，本文提出了三种自适应算法：1) 空间效率：选择性处理代理每次观察到的全景视图。2) 模型内部效率：为早期退出方法引入基于重要性的自适应阈值。3) 时间效率：实现缓存机制，避免重复处理代理之前看过的视图。", "result": "在七个VLN基准测试中，本文的方法在标准和连续环境中，对于三种现成代理，实现了超过2倍的计算量减少，且未导致显著的性能下降。", "conclusion": "所提出的输入自适应导航方法能够有效提高视觉语言导航模型的计算效率，使其在资源受限的环境中更具实用性，同时保持了高水平的导航性能。"}}
{"id": "2508.09372", "categories": ["cs.CV", "cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.09372", "abs": "https://arxiv.org/abs/2508.09372", "authors": ["Md Rezwanul Haque", "Md. Milon Islam", "S M Taslim Uddin Raju", "Fakhri Karray"], "title": "A Signer-Invariant Conformer and Multi-Scale Fusion Transformer for Continuous Sign Language Recognition", "comment": "Accepted for the IEEE/CVF International Conference on Computer Vision\n  (ICCV), Honolulu, Hawaii, USA. 1st MSLR Workshop 2025", "summary": "Continuous Sign Language Recognition (CSLR) faces multiple challenges,\nincluding significant inter-signer variability and poor generalization to novel\nsentence structures. Traditional solutions frequently fail to handle these\nissues efficiently. For overcoming these constraints, we propose a\ndual-architecture framework. For the Signer-Independent (SI) challenge, we\npropose a Signer-Invariant Conformer that combines convolutions with multi-head\nself-attention to learn robust, signer-agnostic representations from pose-based\nskeletal keypoints. For the Unseen-Sentences (US) task, we designed a\nMulti-Scale Fusion Transformer with a novel dual-path temporal encoder that\ncaptures both fine-grained posture dynamics, enabling the model's ability to\ncomprehend novel grammatical compositions. Experiments on the challenging\nIsharah-1000 dataset establish a new standard for both CSLR benchmarks. The\nproposed conformer architecture achieves a Word Error Rate (WER) of 13.07% on\nthe SI challenge, a reduction of 13.53% from the state-of-the-art. On the US\ntask, the transformer model scores a WER of 47.78%, surpassing previous work.\nIn the SignEval 2025 CSLR challenge, our team placed 2nd in the US task and 4th\nin the SI task, demonstrating the performance of these models. The findings\nvalidate our key hypothesis: that developing task-specific networks designed\nfor the particular challenges of CSLR leads to considerable performance\nimprovements and establishes a new baseline for further research. The source\ncode is available at: https://github.com/rezwanh001/MSLR-Pose86K-CSLR-Isharah.", "AI": {"tldr": "本文提出了一种双架构框架，用于解决连续手语识别（CSLR）中的签名者独立性（SI）和未见句子（US）挑战，并在Isharah-1000数据集上取得了最先进的性能。", "motivation": "连续手语识别（CSLR）面临多重挑战，包括显著的签名者间差异性和对新颖句子结构泛化能力差。传统解决方案在高效处理这些问题上常常失败。", "method": "本文提出了一个双架构框架：1. 针对签名者独立（SI）挑战，设计了签名者不变Conformer，结合卷积和多头自注意力机制，从基于姿态的骨骼关键点中学习鲁棒的、与签名者无关的表示。2. 针对未见句子（US）任务，设计了多尺度融合Transformer，包含新颖的双路径时间编码器，以捕捉细粒度的姿态动态。", "result": "在Isharah-1000数据集上，签名者不变Conformer在SI挑战中实现了13.07%的词错误率（WER），比现有最佳水平降低了13.53%。多尺度融合Transformer在US任务中获得了47.78%的WER，超越了先前的工作。团队在SignEval 2025 CSLR挑战赛中，US任务排名第2，SI任务排名第4。", "conclusion": "研究结果验证了为CSLR特定挑战开发任务专用网络能显著提升性能的关键假设，并为未来的研究建立了新的基线。"}}
{"id": "2508.09855", "categories": ["cs.RO", "cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.09855", "abs": "https://arxiv.org/abs/2508.09855", "authors": ["Yuekun Wu", "Yik Lung Pang", "Andrea Cavallaro", "Changjae Oh"], "title": "Toward Human-Robot Teaming: Learning Handover Behaviors from 3D Scenes", "comment": "3 pages, 3 figures", "summary": "Human-robot teaming (HRT) systems often rely on large-scale datasets of human\nand robot interactions, especially for close-proximity collaboration tasks such\nas human-robot handovers. Learning robot manipulation policies from raw,\nreal-world image data requires a large number of robot-action trials in the\nphysical environment. Although simulation training offers a cost-effective\nalternative, the visual domain gap between simulation and robot workspace\nremains a major limitation. We introduce a method for training HRT policies,\nfocusing on human-to-robot handovers, solely from RGB images without the need\nfor real-robot training or real-robot data collection. The goal is to enable\nthe robot to reliably receive objects from a human with stable grasping while\navoiding collisions with the human hand. The proposed policy learner leverages\nsparse-view Gaussian Splatting reconstruction of human-to-robot handover scenes\nto generate robot demonstrations containing image-action pairs captured with a\ncamera mounted on the robot gripper. As a result, the simulated camera pose\nchanges in the reconstructed scene can be directly translated into gripper pose\nchanges. Experiments in both Gaussian Splatting reconstructed scene and\nreal-world human-to-robot handover experiments demonstrate that our method\nserves as a new and effective representation for the human-to-robot handover\ntask, contributing to more seamless and robust HRT.", "AI": {"tldr": "该研究提出一种新方法，仅通过RGB图像和稀疏视图高斯泼溅重建，无需真实机器人训练或数据，即可训练机器人与人协作（HRT）策略，专注于人到机器人的物品递交，实现稳定抓取和避障。", "motivation": "现有HRT系统依赖大量真实交互数据，尤其对于近距离协作（如递交物品）。从真实图像学习机器人操作策略需要大量物理环境试错，成本高昂。虽然仿真训练经济，但仿真与真实机器人工作空间之间存在视觉域差异。", "method": "该方法利用稀疏视图高斯泼溅技术重建人到机器人递交物品的场景。在此重建场景中，生成包含图像-动作对的机器人演示，这些数据由安装在机器人夹持器上的摄像头捕获。模拟的摄像头姿态变化可以直接转化为夹持器姿态变化，从而训练机器人策略。", "result": "在高斯泼溅重建场景和真实世界人到机器人递交物品的实验中，该方法被证明是人到机器人递交任务的一种有效新表示，有助于实现更流畅和鲁棒的HRT。", "conclusion": "该研究为机器人与人协作中的人到机器人递交任务提供了一种有效的新表示方法，通过纯粹的视觉数据和高斯泼溅重建，避免了对真实机器人训练和数据收集的需求，从而提升了HRT的无缝性和鲁棒性。"}}
{"id": "2508.09517", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.09517", "abs": "https://arxiv.org/abs/2508.09517", "authors": ["Ladislav Lenc", "Daniel Cífka", "Jiří Martínek", "Jakub Šmíd", "Pavel Král"], "title": "UWBa at SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim Retrieval", "comment": "Published in Proceedings of the 19th International Workshop on\n  Semantic Evaluation (SemEval-2025). Official version:\n  https://aclanthology.org/2025.semeval-1.31/", "summary": "This paper presents a zero-shot system for fact-checked claim retrieval. We\nemployed several state-of-the-art large language models to obtain text\nembeddings. The models were then combined to obtain the best possible result.\nOur approach achieved 7th place in monolingual and 9th in cross-lingual\nsubtasks. We used only English translations as an input to the text embedding\nmodels since multilingual models did not achieve satisfactory results. We\nidentified the most relevant claims for each post by leveraging the embeddings\nand measuring cosine similarity. Overall, the best results were obtained by the\nNVIDIA NV-Embed-v2 model. For some languages, we benefited from model\ncombinations (NV-Embed & GPT or Mistral).", "AI": {"tldr": "本文提出了一种零样本事实核查声明检索系统，利用大型语言模型生成文本嵌入，并通过余弦相似度进行检索，在单语和跨语任务中取得了良好排名。", "motivation": "研究旨在开发一个零样本系统，通过利用最先进的大型语言模型来检索事实核查过的声明。", "method": "使用多个最先进的大型语言模型（如NVIDIA NV-Embed-v2、GPT、Mistral）获取文本嵌入；将模型组合以优化结果；仅使用英文翻译作为跨语言任务的输入；通过计算嵌入的余弦相似度来识别最相关的声明。", "result": "该系统在单语子任务中排名第7，在跨语子任务中排名第9。NVIDIA NV-Embed-v2模型取得了最佳整体结果。对于某些语言，模型组合（NV-Embed与GPT或Mistral）带来了益处。", "conclusion": "利用大型语言模型（特别是NVIDIA NV-Embed-v2）及其组合生成的文本嵌入，可以有效实现零样本事实核查声明检索，并在相关任务中表现出色。"}}
{"id": "2508.09327", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09327", "abs": "https://arxiv.org/abs/2508.09327", "authors": ["Yifan Jiang", "Ahmad Shariftabrizi", "Venkata SK. Manem"], "title": "Lung-DDPM+: Efficient Thoracic CT Image Synthesis using Diffusion Probabilistic Model", "comment": null, "summary": "Generative artificial intelligence (AI) has been playing an important role in\nvarious domains. Leveraging its high capability to generate high-fidelity and\ndiverse synthetic data, generative AI is widely applied in diagnostic tasks,\nsuch as lung cancer diagnosis using computed tomography (CT). However, existing\ngenerative models for lung cancer diagnosis suffer from low efficiency and\nanatomical imprecision, which limit their clinical applicability. To address\nthese drawbacks, we propose Lung-DDPM+, an improved version of our previous\nmodel, Lung-DDPM. This novel approach is a denoising diffusion probabilistic\nmodel (DDPM) guided by nodule semantic layouts and accelerated by a pulmonary\nDPM-solver, enabling the method to focus on lesion areas while achieving a\nbetter trade-off between sampling efficiency and quality. Evaluation results on\nthe public LIDC-IDRI dataset suggest that the proposed method achieves\n8$\\times$ fewer FLOPs (floating point operations per second), 6.8$\\times$ lower\nGPU memory consumption, and 14$\\times$ faster sampling compared to Lung-DDPM.\nMoreover, it maintains comparable sample quality to both Lung-DDPM and other\nstate-of-the-art (SOTA) generative models in two downstream segmentation tasks.\nWe also conducted a Visual Turing Test by an experienced radiologist, showing\nthe advanced quality and fidelity of synthetic samples generated by the\nproposed method. These experimental results demonstrate that Lung-DDPM+ can\neffectively generate high-quality thoracic CT images with lung nodules,\nhighlighting its potential for broader applications, such as general tumor\nsynthesis and lesion generation in medical imaging. The code and pretrained\nmodels are available at https://github.com/Manem-Lab/Lung-DDPM-PLUS.", "AI": {"tldr": "Lung-DDPM+是一种改进的去噪扩散概率模型，通过结节语义布局引导和DPM-solver加速，显著提升了肺部CT图像合成的效率，同时保持了高质量和解剖精度，适用于肺癌诊断。", "motivation": "生成式AI在肺癌CT诊断等任务中潜力巨大，但现有模型存在效率低下和解剖学不精确的问题，限制了其临床应用。", "method": "提出Lung-DDPM+，它是Lung-DDPM的改进版，采用去噪扩散概率模型（DDPM），通过结节语义布局引导，并由肺部DPM-solver加速，使其能专注于病灶区域，并在采样效率和质量之间取得更好平衡。", "result": "在LIDC-IDRI数据集上，Lung-DDPM+相比Lung-DDPM计算量减少8倍，GPU内存消耗降低6.8倍，采样速度快14倍。同时，在两项下游分割任务中保持了与Lung-DDPM及其他SOTA模型相当的样本质量。经验丰富的放射科医生进行的视觉图灵测试也证实了其合成样本的高质量和真实性。", "conclusion": "Lung-DDPM+能够高效生成高质量的带有肺结节的胸部CT图像，展示了其在通用肿瘤合成和医学影像病灶生成等更广泛应用中的潜力。"}}
{"id": "2508.09381", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.09381", "abs": "https://arxiv.org/abs/2508.09381", "authors": ["Kumar Abhishek", "Jeremy Kawahara", "Ghassan Hamarneh"], "title": "What Can We Learn from Inter-Annotator Variability in Skin Lesion Segmentation?", "comment": "Medical Image Computing and Computer-Assisted Intervention (MICCAI)\n  ISIC Skin Image Analysis Workshop (MICCAI ISIC) 2025; 12 pages, 4 tables, 3\n  figures", "summary": "Medical image segmentation exhibits intra- and inter-annotator variability\ndue to ambiguous object boundaries, annotator preferences, expertise, and\ntools, among other factors. Lesions with ambiguous boundaries, e.g., spiculated\nor infiltrative nodules, or irregular borders per the ABCD rule, are\nparticularly prone to disagreement and are often associated with malignancy. In\nthis work, we curate IMA++, the largest multi-annotator skin lesion\nsegmentation dataset, on which we conduct an in-depth study of variability due\nto annotator, malignancy, tool, and skill factors. We find a statistically\nsignificant (p<0.001) association between inter-annotator agreement (IAA),\nmeasured using Dice, and the malignancy of skin lesions. We further show that\nIAA can be accurately predicted directly from dermoscopic images, achieving a\nmean absolute error of 0.108. Finally, we leverage this association by\nutilizing IAA as a \"soft\" clinical feature within a multi-task learning\nobjective, yielding a 4.2% improvement in balanced accuracy averaged across\nmultiple model architectures and across IMA++ and four public dermoscopic\ndatasets. The code is available at https://github.com/sfu-mial/skin-IAV.", "AI": {"tldr": "本研究创建了最大的多标注者皮肤病变分割数据集IMA++，深入分析了标注者间差异（IAA）与恶性病变之间的关联。研究发现IAA与恶性程度显著相关，且可从图像中准确预测。将IAA作为“软”临床特征引入多任务学习，显著提升了皮肤病变分割和恶性分类的性能。", "motivation": "医学图像分割存在标注者内部和标注者之间差异，特别是在边界模糊的病变（如恶性肿瘤）中更为明显。这种差异可能由边界模糊、标注者偏好、专业知识和工具等因素引起。本研究旨在深入理解这些变异性，并探索如何利用标注者间一致性来改善医学图像分析。", "method": "1. 策展并发布了IMA++数据集，这是最大的多标注者皮肤病变分割数据集。2. 在IMA++数据集上，深入研究了由标注者、恶性程度、工具和技能等因素引起的变异性。3. 使用Dice系数衡量标注者间一致性（IAA）。4. 探索了从皮肤镜图像直接准确预测IAA的方法。5. 将IAA作为一种“软”临床特征，整合到多任务学习目标中，以提升模型性能。", "result": "1. 发现标注者间一致性（IAA）与皮肤病变的恶性程度之间存在统计学上显著的关联（p<0.001）。2. 证明IAA可以直接从皮肤镜图像中准确预测，平均绝对误差为0.108。3. 利用IAA作为“软”临床特征的多任务学习方法，在多种模型架构和IMA++及四个公开皮肤镜数据集上，平均平衡准确率提高了4.2%。", "conclusion": "标注者间一致性（IAA）与皮肤病变的恶性程度之间存在显著关联，且IAA可以从图像中准确预测。将IAA作为一种“软”临床特征整合到深度学习模型中，能够有效提升皮肤病变分割和恶性分类的性能，这为处理医学图像分割中的不确定性提供了一条有前景的途径。"}}
{"id": "2508.09950", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.09950", "abs": "https://arxiv.org/abs/2508.09950", "authors": ["Bida Ma", "Nuo Xu", "Chenkun Qi", "Xin Liu", "Yule Mo", "Jinkai Wang", "Chunpeng Lu"], "title": "PPL: Point Cloud Supervised Proprioceptive Locomotion Reinforcement Learning for Legged Robots in Crawl Spaces", "comment": null, "summary": "The legged locomotion in spatially constrained structures (called crawl\nspaces) is challenging. In crawl spaces, current exteroceptive locomotion\nlearning methods are limited by large noises and errors of the sensors in\npossible low visibility conditions, and current proprioceptive locomotion\nlearning methods are difficult in traversing crawl spaces because only ground\nfeatures are inferred. In this study, a point cloud supervised proprioceptive\nlocomotion reinforcement learning method for legged robots in crawl spaces is\nproposed. A state estimation network is designed to estimate the robot's\nsurrounding ground and spatial features as well as the robot's collision states\nusing historical proprioceptive sensor data. The point cloud is represented in\npolar coordinate frame and a point cloud processing method is proposed to\nefficiently extract the ground and spatial features that are used to supervise\nthe state estimation network learning. Comprehensive reward functions that\nguide the robot to traverse through crawl spaces after collisions are designed.\nExperiments demonstrate that, compared to existing methods, our method exhibits\nmore agile locomotion in crawl spaces. This study enhances the ability of\nlegged robots to traverse spatially constrained environments without requiring\nexteroceptive sensors.", "AI": {"tldr": "提出一种点云监督的本体感知强化学习方法，使腿足机器人在狭窄空间内实现更灵活的运动，无需外部传感器。", "motivation": "腿足机器人在空间受限结构（如狭窄空间）中运动面临挑战，现有外部感知方法受传感器噪声和低能见度限制，而现有本体感知方法难以推断地面以外的空间特征。", "method": "提出一种点云监督的本体感知强化学习方法。设计一个状态估计网络，利用历史本体感知数据估计机器人周围的地面和空间特征以及碰撞状态。采用极坐标系表示点云，并提出点云处理方法以高效提取地面和空间特征，用于监督状态估计网络的学习。设计了全面的奖励函数，引导机器人在碰撞后也能穿过狭窄空间。", "result": "实验表明，与现有方法相比，该方法在狭窄空间中展现出更灵活的运动能力。", "conclusion": "本研究增强了腿足机器人在空间受限环境中通行的能力，且无需外部传感器。"}}
{"id": "2508.09521", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09521", "abs": "https://arxiv.org/abs/2508.09521", "authors": ["Yunxiao Wang", "Meng Liu", "Wenqi Liu", "Kaiyu Jiang", "Bin Wen", "Fan Yang", "Tingting Gao", "Guorui Zhou", "Liqiang Nie"], "title": "COMPEER: Controllable Empathetic Reinforcement Reasoning for Emotional Support Conversation", "comment": null, "summary": "Emotional support conversations are crucial for promoting emotional\nwell-being, yet current models often lack deep empathetic reasoning grounded in\npsychological principles. To address this, we propose controllable empathetic\nreasoning, which combines natural language reasoning with structured\npsychological steps. We construct a fine-grained dataset annotated with\nreasoning correctness and response preferences to enable this capability. To\nfurther enhance training, we employ reinforcement learning with a unified\nprocess-outcome reward model that delivers precise feedback. To mitigate\nresponse repetitiveness from entropy collapse, we introduce personality-based\ndialogue rewriting and a redundancy-aware reward reweighting strategy. Our\napproach significantly improves model's emotional support ability, advancing\nthe development of empathetic, human-like support systems.", "AI": {"tldr": "该论文提出了一种可控的同理心推理框架，结合强化学习和心理学原理，并通过数据集构建和策略优化，显著提升了模型的情绪支持能力，旨在创建更具人性化的支持系统。", "motivation": "现有模型在情感支持对话中缺乏基于心理学原理的深度同理心推理能力。", "method": "1. 提出“可控的同理心推理”，将自然语言推理与结构化心理学步骤相结合。2. 构建了一个细粒度数据集，标注推理正确性和回复偏好。3. 采用强化学习，并使用统一的过程-结果奖励模型提供精确反馈。4. 引入基于个性的对话重写和冗余感知奖励重加权策略，以缓解回复的重复性。", "result": "该方法显著提高了模型的情绪支持能力。", "conclusion": "该研究推动了同理心、类人支持系统的发展。"}}
{"id": "2508.09339", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09339", "abs": "https://arxiv.org/abs/2508.09339", "authors": ["Aqsa Sultana", "Nordin Abouzahra", "Ahmed Rahu", "Brian Shula", "Brandon Combs", "Derrick Forchetti", "Theus Aspiras", "Vijayan K. Asari"], "title": "UltraLight Med-Vision Mamba for Classification of Neoplastic Progression in Tubular Adenomas", "comment": null, "summary": "Identification of precancerous polyps during routine colonoscopy screenings\nis vital for their excision, lowering the risk of developing colorectal cancer.\nAdvanced deep learning algorithms enable precise adenoma classification and\nstratification, improving risk assessment accuracy and enabling personalized\nsurveillance protocols that optimize patient outcomes. Ultralight Med-Vision\nMamba, a state-space based model (SSM), has excelled in modeling long- and\nshort-range dependencies and image generalization, critical factors for\nanalyzing whole slide images. Furthermore, Ultralight Med-Vision Mamba's\nefficient architecture offers advantages in both computational speed and\nscalability, making it a promising tool for real-time clinical deployment.", "AI": {"tldr": "在结肠镜筛查中，识别癌前息肉至关重要。本文提出并强调了基于状态空间模型（SSM）的Ultralight Med-Vision Mamba在腺瘤分类和分层中的应用，它具有高效、高精度和可扩展性，有望实现实时临床部署。", "motivation": "在常规结肠镜筛查中识别和切除癌前息肉对于降低结直肠癌风险至关重要。需要先进的算法来提高腺瘤分类和分层的准确性，从而实现个性化监测方案和优化患者预后。", "method": "采用基于状态空间模型（SSM）的Ultralight Med-Vision Mamba。该模型擅长建模长短期依赖关系和图像泛化，并具有高效的架构。", "result": "Ultralight Med-Vision Mamba在腺瘤分类和分层方面表现出色，提高了风险评估的准确性，并能实现个性化监测方案。它在计算速度和可扩展性方面具有优势，适用于分析全玻片图像。", "conclusion": "Ultralight Med-Vision Mamba作为一种高效、高精度的深度学习工具，在结直肠癌筛查中具有巨大潜力，有望实现实时临床部署，优化患者预后。"}}
{"id": "2508.09383", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09383", "abs": "https://arxiv.org/abs/2508.09383", "authors": ["Guoxian Song", "Hongyi Xu", "Xiaochen Zhao", "You Xie", "Tianpei Gu", "Zenan Li", "Chenxu Zhang", "Linjie Luo"], "title": "X-UniMotion: Animating Human Images with Expressive, Unified and Identity-Agnostic Motion Latents", "comment": null, "summary": "We present X-UniMotion, a unified and expressive implicit latent\nrepresentation for whole-body human motion, encompassing facial expressions,\nbody poses, and hand gestures. Unlike prior motion transfer methods that rely\non explicit skeletal poses and heuristic cross-identity adjustments, our\napproach encodes multi-granular motion directly from a single image into a\ncompact set of four disentangled latent tokens -- one for facial expression,\none for body pose, and one for each hand. These motion latents are both highly\nexpressive and identity-agnostic, enabling high-fidelity, detailed\ncross-identity motion transfer across subjects with diverse identities, poses,\nand spatial configurations. To achieve this, we introduce a self-supervised,\nend-to-end framework that jointly learns the motion encoder and latent\nrepresentation alongside a DiT-based video generative model, trained on\nlarge-scale, diverse human motion datasets. Motion-identity disentanglement is\nenforced via 2D spatial and color augmentations, as well as synthetic 3D\nrenderings of cross-identity subject pairs under shared poses. Furthermore, we\nguide motion token learning with auxiliary decoders that promote fine-grained,\nsemantically aligned, and depth-aware motion embeddings. Extensive experiments\nshow that X-UniMotion outperforms state-of-the-art methods, producing highly\nexpressive animations with superior motion fidelity and identity preservation.", "AI": {"tldr": "X-UniMotion提出了一种统一且富有表现力的隐式潜在表示，用于全身人体运动（面部、身体、手势）的跨身份迁移，通过解耦的潜在令牌实现高保真动画。", "motivation": "现有的运动迁移方法依赖于显式骨骼姿态和启发式跨身份调整，限制了其表现力和在不同身份主体间进行高保真、精细运动迁移的能力。", "method": "该方法将单张图像的多粒度运动编码为四个解耦的潜在令牌（面部、身体、双手）。通过一个自监督、端到端的框架，共同学习运动编码器、潜在表示和基于DiT的视频生成模型。利用2D空间和颜色增强以及共享姿态下跨身份主体对的合成3D渲染来强制实现运动-身份解耦。辅助解码器用于指导运动令牌学习，以促进细粒度、语义对齐和深度感知的运动嵌入。", "result": "实验表明，X-UniMotion超越了现有最先进的方法，能生成具有卓越运动保真度和身份保持性的高度富有表现力的动画。", "conclusion": "X-UniMotion提供了一种新颖、高效且优越的全身人体运动表示和迁移方案，实现了高保真、富有表现力的跨身份动画生成。"}}
{"id": "2508.09960", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.09960", "abs": "https://arxiv.org/abs/2508.09960", "authors": ["Yifei Yao", "Chengyuan Luo", "Jiaheng Du", "Wentao He", "Jun-Guo Lu"], "title": "GBC: Generalized Behavior-Cloning Framework for Whole-Body Humanoid Imitation", "comment": null, "summary": "The creation of human-like humanoid robots is hindered by a fundamental\nfragmentation: data processing and learning algorithms are rarely universal\nacross different robot morphologies. This paper introduces the Generalized\nBehavior Cloning (GBC) framework, a comprehensive and unified solution designed\nto solve this end-to-end challenge. GBC establishes a complete pathway from\nhuman motion to robot action through three synergistic innovations. First, an\nadaptive data pipeline leverages a differentiable IK network to automatically\nretarget any human MoCap data to any humanoid. Building on this foundation, our\nnovel DAgger-MMPPO algorithm with its MMTransformer architecture learns robust,\nhigh-fidelity imitation policies. To complete the ecosystem, the entire\nframework is delivered as an efficient, open-source platform based on Isaac\nLab, empowering the community to deploy the full workflow via simple\nconfiguration scripts. We validate the power and generality of GBC by training\npolicies on multiple heterogeneous humanoids, demonstrating excellent\nperformance and transfer to novel motions. This work establishes the first\npractical and unified pathway for creating truly generalized humanoid\ncontrollers.", "AI": {"tldr": "本文提出了广义行为克隆（GBC）框架，这是一个统一的端到端解决方案，旨在将人类运动转化为机器人动作，以创建通用的拟人机器人控制器。", "motivation": "当前机器人数据处理和学习算法缺乏跨不同机器人形态的通用性，这阻碍了类人型机器人的创建。", "method": "GBC框架包含三项创新：1) 自适应数据管道，利用可微分IK网络将任何人类动作捕捉数据自动重定向到任何拟人机器人；2) DAgger-MMPPO算法及其MMTransformer架构，用于学习鲁棒、高保真的模仿策略；3) 基于Isaac Lab的高效开源平台，通过简单的配置脚本部署整个工作流。", "result": "GBC在多个异构拟人机器人上成功训练了策略，展示了卓越的性能和对新动作的泛化能力。", "conclusion": "这项工作为创建真正通用的拟人机器人控制器建立了第一个实用且统一的途径。"}}
{"id": "2508.09603", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.09603", "abs": "https://arxiv.org/abs/2508.09603", "authors": ["Skyler Hallinan", "Jaehun Jung", "Melanie Sclar", "Ximing Lu", "Abhilasha Ravichander", "Sahana Ramnath", "Yejin Choi", "Sai Praneeth Karimireddy", "Niloofar Mireshghallah", "Xiang Ren"], "title": "The Surprising Effectiveness of Membership Inference with Simple N-Gram Coverage", "comment": "CoLM 2025", "summary": "Membership inference attacks serves as useful tool for fair use of language\nmodels, such as detecting potential copyright infringement and auditing data\nleakage. However, many current state-of-the-art attacks require access to\nmodels' hidden states or probability distribution, which prevents investigation\ninto more widely-used, API-access only models like GPT-4. In this work, we\nintroduce N-Gram Coverage Attack, a membership inference attack that relies\nsolely on text outputs from the target model, enabling attacks on completely\nblack-box models. We leverage the observation that models are more likely to\nmemorize and subsequently generate text patterns that were commonly observed in\ntheir training data. Specifically, to make a prediction on a candidate member,\nN-Gram Coverage Attack first obtains multiple model generations conditioned on\na prefix of the candidate. It then uses n-gram overlap metrics to compute and\naggregate the similarities of these outputs with the ground truth suffix; high\nsimilarities indicate likely membership. We first demonstrate on a diverse set\nof existing benchmarks that N-Gram Coverage Attack outperforms other black-box\nmethods while also impressively achieving comparable or even better performance\nto state-of-the-art white-box attacks - despite having access to only text\noutputs. Interestingly, we find that the success rate of our method scales with\nthe attack compute budget - as we increase the number of sequences generated\nfrom the target model conditioned on the prefix, attack performance tends to\nimprove. Having verified the accuracy of our method, we use it to investigate\npreviously unstudied closed OpenAI models on multiple domains. We find that\nmore recent models, such as GPT-4o, exhibit increased robustness to membership\ninference, suggesting an evolving trend toward improved privacy protections.", "AI": {"tldr": "本文提出了一种名为“N-Gram Coverage Attack”的黑盒成员推断攻击方法，仅依赖于大型语言模型的文本输出，可用于评估如GPT-4等API访问模型的隐私性，并发现新模型隐私保护增强。", "motivation": "现有的成员推断攻击多需要访问模型的隐藏状态或概率分布，无法应用于仅提供API访问的黑盒模型（如GPT-4），这限制了对这些广泛使用模型进行版权侵犯检测和数据泄露审计的能力。", "method": "N-Gram Coverage Attack利用模型更倾向于生成训练数据中常见文本模式的观察。对于一个候选成员，它首先使用候选成员的前缀作为条件，从目标模型获取多个生成文本，然后使用N-gram重叠度量计算并聚合这些输出与真实后缀的相似性，高相似性表明很可能是成员。", "result": "N-Gram Coverage Attack在现有基准测试中优于其他黑盒方法，并且在仅访问文本输出的情况下，性能可与最先进的白盒攻击相媲美甚至超越。攻击成功率随计算预算（生成序列数量）的增加而提高。对OpenAI闭源模型的调查发现，GPT-4o等新模型对成员推断的鲁棒性有所增强。", "conclusion": "N-Gram Coverage Attack是一种有效且仅依赖文本输出的黑盒成员推断方法，能够成功评估大型语言模型的隐私性。研究发现，如GPT-4o等较新的模型在隐私保护方面表现出更强的鲁棒性，预示着隐私保护的趋势正在演进。"}}
{"id": "2508.09344", "categories": ["cs.CV", "68T45, 92C55", "H.5.2; I.2.10; J.3"], "pdf": "https://arxiv.org/pdf/2508.09344", "abs": "https://arxiv.org/abs/2508.09344", "authors": ["Anushka Bhatt"], "title": "Blink-to-code: real-time Morse code communication via eye blink detection and classification", "comment": "4 pages, 4 figures. Preprint on blink-based Morse code communication\n  via webcam for assistive technology. Relevant to computer vision and\n  human-computer interaction", "summary": "This study proposes a real-time system that translates voluntary eye blinks\ninto Morse code, enabling communication for individuals with severe motor\nimpairments. Using a standard webcam and computer vision, the system detects\nand classifies blinks as short (dot) or long (dash), then decodes them into\nalphanumeric characters. Experiments with five participants show 62% decoding\naccuracy and 18-20 seconds response times, demonstrating a viable, low-cost\nassistive communication method.", "AI": {"tldr": "本研究提出一种实时系统，能将自愿眨眼转换为摩尔斯电码，为重度运动障碍者提供交流方式。", "motivation": "为重度运动障碍人士提供一种可行的辅助交流方法。", "method": "利用标准网络摄像头和计算机视觉技术，系统检测并分类眨眼为短眨（点）或长眨（划），然后将其解码为字母数字字符。", "result": "实验表明，系统解码准确率为62%，响应时间为18-20秒，证明了其作为一种可行、低成本的辅助交流方法的潜力。", "conclusion": "该系统是一种可行且低成本的辅助交流方法，能够帮助重度运动障碍者进行沟通。"}}
{"id": "2508.09415", "categories": ["cs.CV", "cs.AI", "I.2"], "pdf": "https://arxiv.org/pdf/2508.09415", "abs": "https://arxiv.org/abs/2508.09415", "authors": ["John S. O'Meara", "Jared Hwang", "Zeyu Wang", "Michael Saugstad", "Jon E. Froehlich"], "title": "RampNet: A Two-Stage Pipeline for Bootstrapping Curb Ramp Detection in Streetscape Images from Open Government Metadata", "comment": "Accepted to the ICCV'25 Workshop on Vision Foundation Models and\n  Generative AI for Accessibility: Challenges and Opportunities", "summary": "Curb ramps are critical for urban accessibility, but robustly detecting them\nin images remains an open problem due to the lack of large-scale, high-quality\ndatasets. While prior work has attempted to improve data availability with\ncrowdsourced or manually labeled data, these efforts often fall short in either\nquality or scale. In this paper, we introduce and evaluate a two-stage pipeline\ncalled RampNet to scale curb ramp detection datasets and improve model\nperformance. In Stage 1, we generate a dataset of more than 210,000 annotated\nGoogle Street View (GSV) panoramas by auto-translating government-provided curb\nramp location data to pixel coordinates in panoramic images. In Stage 2, we\ntrain a curb ramp detection model (modified ConvNeXt V2) from the generated\ndataset, achieving state-of-the-art performance. To evaluate both stages of our\npipeline, we compare to manually labeled panoramas. Our generated dataset\nachieves 94.0% precision and 92.5% recall, and our detection model reaches\n0.9236 AP -- far exceeding prior work. Our work contributes the first\nlarge-scale, high-quality curb ramp detection dataset, benchmark, and model.", "AI": {"tldr": "本文提出了RampNet，一个两阶段管道，用于大规模生成高质量路缘坡道检测数据集并训练高性能模型。通过自动翻译政府数据，生成了超过21万张Google街景全景图数据集，并在此基础上训练了一个最先进的检测模型。", "motivation": "路缘坡道对于城市无障碍至关重要，但在图像中可靠检测它们仍是一个难题，主要原因是缺乏大规模、高质量的数据集。现有通过众包或手动标注的数据集在质量或规模上均有不足。", "method": "该研究引入了一个名为RampNet的两阶段管道。第一阶段：通过将政府提供的路缘坡道位置数据自动转换为全景图像中的像素坐标，生成了一个包含21万多张带注释Google街景（GSV）全景图的数据集。第二阶段：利用生成的数据集训练了一个路缘坡道检测模型（修改后的ConvNeXt V2）。通过与手动标注的全景图进行比较来评估管道的两个阶段。", "result": "生成的数据集达到了94.0%的精确度和92.5%的召回率。训练的检测模型达到了0.9236的AP值，远超现有工作。", "conclusion": "该工作首次贡献了大规模、高质量的路缘坡道检测数据集、基准和模型，显著推动了城市无障碍检测领域的发展。"}}
{"id": "2508.09971", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09971", "abs": "https://arxiv.org/abs/2508.09971", "authors": ["Zihan Wang", "Nina Mahmoudian"], "title": "Vision-driven River Following of UAV via Safe Reinforcement Learning using Semantic Dynamics Model", "comment": "Submitted to Robotics and Autonomous Systems (RAS) journal", "summary": "Vision-driven autonomous river following by Unmanned Aerial Vehicles is\ncritical for applications such as rescue, surveillance, and environmental\nmonitoring, particularly in dense riverine environments where GPS signals are\nunreliable. We formalize river following as a coverage control problem in which\nthe reward function is submodular, yielding diminishing returns as more unique\nriver segments are visited, thereby framing the task as a Submodular Markov\nDecision Process. First, we introduce Marginal Gain Advantage Estimation, which\nrefines the reward advantage function by using a sliding window baseline\ncomputed from historical episodic returns, thus aligning the advantage\nestimation with the agent's evolving recognition of action value in\nnon-Markovian settings. Second, we develop a Semantic Dynamics Model based on\npatchified water semantic masks that provides more interpretable and\ndata-efficient short-term prediction of future observations compared to latent\nvision dynamics models. Third, we present the Constrained Actor Dynamics\nEstimator architecture, which integrates the actor, the cost estimator, and SDM\nfor cost advantage estimation to form a model-based SafeRL framework capable of\nsolving partially observable Constrained Submodular Markov Decision Processes.\nSimulation results demonstrate that MGAE achieves faster convergence and\nsuperior performance over traditional critic-based methods like Generalized\nAdvantage Estimation. SDM provides more accurate short-term state predictions\nthat enable the cost estimator to better predict potential violations. Overall,\nCADE effectively integrates safety regulation into model-based RL, with the\nLagrangian approach achieving the soft balance of reward and safety during\ntraining, while the safety layer enhances performance during inference by hard\naction overlay.", "AI": {"tldr": "该研究提出了一种基于模型的安全强化学习（SafeRL）框架CADE，用于无人机在无GPS环境下进行视觉驱动的河流跟踪。通过引入MGAE和SDM，该框架解决了子模态马尔可夫决策过程中的收敛速度、预测准确性和安全性问题。", "motivation": "无人机在GPS信号不可靠的河流环境中进行视觉驱动的自主河流跟踪对于救援、监视和环境监测至关重要。将河流跟踪形式化为具有子模态奖励的覆盖控制问题，即子模态马尔可夫决策过程，旨在解决在复杂环境下无人机自主导航的挑战。", "method": "将河流跟踪建模为子模态马尔可夫决策过程（Submodular MDP）。提出了**边际收益优势估计（MGAE）**，通过滑动窗口基线细化优势函数。开发了基于语义掩码的**语义动态模型（SDM）**，用于短期预测。构建了**受限执行器动态估计器（CADE）**架构，整合执行器、成本估计器和SDM，形成基于模型的安全强化学习（SafeRL）框架，并结合拉格朗日方法在训练中平衡奖励与安全性，推理时使用安全层进行硬性动作覆盖。", "result": "MGAE相比传统基于批评者的方法（如GAE）实现了更快的收敛和更优的性能。SDM提供了更准确的短期状态预测，使成本估计器能更好地预测潜在违规。CADE有效地将安全规范整合到基于模型的强化学习中，拉格朗日方法在训练期间实现了奖励和安全的软平衡，而安全层在推理期间通过硬性动作覆盖增强了性能。", "conclusion": "该研究成功地将安全规范整合到基于模型的强化学习中，为无人机在复杂、部分可观测环境下进行安全高效的河流跟踪提供了有效的解决方案。"}}
{"id": "2508.09622", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.09622", "abs": "https://arxiv.org/abs/2508.09622", "authors": ["Tatiana Batura", "Elena Bruches", "Milana Shvenk", "Valentin Malykh"], "title": "AINL-Eval 2025 Shared Task: Detection of AI-Generated Scientific Abstracts in Russian", "comment": "AINL 2025 Conference", "summary": "The rapid advancement of large language models (LLMs) has revolutionized text\ngeneration, making it increasingly difficult to distinguish between human- and\nAI-generated content. This poses a significant challenge to academic integrity,\nparticularly in scientific publishing and multilingual contexts where detection\nresources are often limited. To address this critical gap, we introduce the\nAINL-Eval 2025 Shared Task, specifically focused on the detection of\nAI-generated scientific abstracts in Russian. We present a novel, large-scale\ndataset comprising 52,305 samples, including human-written abstracts across 12\ndiverse scientific domains and AI-generated counterparts from five\nstate-of-the-art LLMs (GPT-4-Turbo, Gemma2-27B, Llama3.3-70B, Deepseek-V3, and\nGigaChat-Lite). A core objective of the task is to challenge participants to\ndevelop robust solutions capable of generalizing to both (i) previously unseen\nscientific domains and (ii) models not included in the training data. The task\nwas organized in two phases, attracting 10 teams and 159 submissions, with top\nsystems demonstrating strong performance in identifying AI-generated content.\nWe also establish a continuous shared task platform to foster ongoing research\nand long-term progress in this important area. The dataset and platform are\npublicly available at https://github.com/iis-research-team/AINL-Eval-2025.", "AI": {"tldr": "该论文介绍了AINL-Eval 2025共享任务，旨在检测俄语AI生成的科学摘要，并提供了一个包含人类和多种LLM生成摘要的大规模数据集。", "motivation": "大型语言模型（LLMs）的快速发展使得区分人类和AI生成的内容变得困难，这对学术诚信构成了重大挑战，尤其是在科学出版和多语言环境中，检测资源有限。", "method": "引入了AINL-Eval 2025共享任务，专注于检测俄语AI生成的科学摘要。构建了一个包含52,305个样本的大规模数据集，涵盖12个科学领域的人类撰写摘要以及来自五种LLMs（GPT-4-Turbo, Gemma2-27B, Llama3.3-70B, Deepseek-V3, GigaChat-Lite）的AI生成摘要。任务旨在挑战参与者开发能泛化到未见领域和未训练模型的鲁棒解决方案。任务分两阶段进行，并建立了一个持续的共享任务平台。", "result": "该任务吸引了10个团队和159份提交，顶级系统在识别AI生成内容方面表现出强大的性能。", "conclusion": "成功建立了一个用于检测俄语AI生成科学摘要的共享任务和数据集，顶级系统表现出色，并为该领域未来的研究和进展奠定了基础。"}}
{"id": "2508.09392", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09392", "abs": "https://arxiv.org/abs/2508.09392", "authors": ["Kang Ni", "Minrui Zou", "Yuxuan Li", "Xiang Li", "Kehua Guo", "Ming-Ming Cheng", "Yimian Dai"], "title": "DenoDet V2: Phase-Amplitude Cross Denoising for SAR Object Detection", "comment": null, "summary": "One of the primary challenges in Synthetic Aperture Radar (SAR) object\ndetection lies in the pervasive influence of coherent noise. As a common\npractice, most existing methods, whether handcrafted approaches or deep\nlearning-based methods, employ the analysis or enhancement of object\nspatial-domain characteristics to achieve implicit denoising. In this paper, we\npropose DenoDet V2, which explores a completely novel and different perspective\nto deconstruct and modulate the features in the transform domain via a\ncarefully designed attention architecture. Compared to DenoDet V1, DenoDet V2\nis a major advancement that exploits the complementary nature of amplitude and\nphase information through a band-wise mutual modulation mechanism, which\nenables a reciprocal enhancement between phase and amplitude spectra. Extensive\nexperiments on various SAR datasets demonstrate the state-of-the-art\nperformance of DenoDet V2. Notably, DenoDet V2 achieves a significant 0.8\\%\nimprovement on SARDet-100K dataset compared to DenoDet V1, while reducing the\nmodel complexity by half. The code is available at\nhttps://github.com/GrokCV/GrokSAR.", "AI": {"tldr": "DenoDet V2通过在变换域中利用振幅和相位信息的互补性进行特征调制，解决了SAR图像中的相干噪声问题，实现了更优异的目标检测性能，同时降低了模型复杂度。", "motivation": "合成孔径雷达（SAR）目标检测面临的主要挑战是相干噪声的普遍影响。现有方法（无论是手工设计还是基于深度学习的）主要通过分析或增强目标空间域特征来隐式去噪。", "method": "本文提出了DenoDet V2，它探索了一种全新的视角，通过精心设计的注意力架构在变换域中解构和调制特征。相较于DenoDet V1，DenoDet V2的主要进步在于利用振幅和相位信息的互补性，通过带间相互调制机制实现相位和振幅谱之间的相互增强。", "result": "在各种SAR数据集上进行了广泛的实验，DenoDet V2展示了最先进的性能。值得注意的是，与DenoDet V1相比，DenoDet V2在SARDet-100K数据集上取得了显著的0.8%提升，同时将模型复杂度降低了一半。", "conclusion": "DenoDet V2通过其创新的变换域特征调制和振幅-相位互增强机制，有效地解决了SAR目标检测中的相干噪声问题，实现了卓越的性能提升和更高的效率，证明了其在SAR目标检测领域的先进性。"}}
{"id": "2508.09428", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09428", "abs": "https://arxiv.org/abs/2508.09428", "authors": ["Yuxiao Wang", "Yu Lei", "Wolin Liang", "Weiying Xue", "Zhenao Wei", "Nan Zhuang", "Qi Liu"], "title": "What-Meets-Where: Unified Learning of Action and Contact Localization in a New Dataset", "comment": null, "summary": "People control their bodies to establish contact with the environment. To\ncomprehensively understand actions across diverse visual contexts, it is\nessential to simultaneously consider \\textbf{what} action is occurring and\n\\textbf{where} it is happening. Current methodologies, however, often\ninadequately capture this duality, typically failing to jointly model both\naction semantics and their spatial contextualization within scenes. To bridge\nthis gap, we introduce a novel vision task that simultaneously predicts\nhigh-level action semantics and fine-grained body-part contact regions. Our\nproposed framework, PaIR-Net, comprises three key components: the Contact Prior\nAware Module (CPAM) for identifying contact-relevant body parts, the\nPrior-Guided Concat Segmenter (PGCS) for pixel-wise contact segmentation, and\nthe Interaction Inference Module (IIM) responsible for integrating global\ninteraction relationships. To facilitate this task, we present PaIR (Part-aware\nInteraction Representation), a comprehensive dataset containing 13,979 images\nthat encompass 654 actions, 80 object categories, and 17 body parts.\nExperimental evaluation demonstrates that PaIR-Net significantly outperforms\nbaseline approaches, while ablation studies confirm the efficacy of each\narchitectural component. The code and dataset will be released upon\npublication.", "AI": {"tldr": "本文提出了一项新的视觉任务，旨在同时预测高层动作语义和精细的身体部位接触区域，并为此引入了PaIR-Net框架和PaIR数据集。", "motivation": "当前方法在理解动作时，未能充分捕捉动作的“内容”（what）和“发生地点”（where）之间的双重性，通常无法联合建模动作语义及其在场景中的空间语境。", "method": "本文提出了一个新颖的视觉任务，用于同时预测高层动作语义和细粒度身体部位接触区域。为实现此任务，开发了PaIR-Net框架，包含三个核心组件：接触先验感知模块（CPAM）用于识别接触相关的身体部位，先验引导连接分割器（PGCS）用于像素级接触分割，以及交互推理模块（IIM）用于整合全局交互关系。此外，还构建了一个名为PaIR（Part-aware Interaction Representation）的综合数据集，包含13,979张图像，涵盖654种动作、80个物体类别和17个身体部位。", "result": "实验评估表明，PaIR-Net显著优于基线方法。消融研究证实了每个架构组件的有效性。", "conclusion": "PaIR-Net成功地解决了同时理解动作语义和其空间上下文的挑战，通过其创新的框架和新数据集，为动作理解领域带来了显著进步。"}}
{"id": "2508.09976", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.09976", "abs": "https://arxiv.org/abs/2508.09976", "authors": ["Marion Lepert", "Jiaying Fang", "Jeannette Bohg"], "title": "Masquerade: Learning from In-the-wild Human Videos using Data-Editing", "comment": "Project website at https://masquerade-robot.github.io/", "summary": "Robot manipulation research still suffers from significant data scarcity:\neven the largest robot datasets are orders of magnitude smaller and less\ndiverse than those that fueled recent breakthroughs in language and vision. We\nintroduce Masquerade, a method that edits in-the-wild egocentric human videos\nto bridge the visual embodiment gap between humans and robots and then learns a\nrobot policy with these edited videos. Our pipeline turns each human video into\nrobotized demonstrations by (i) estimating 3-D hand poses, (ii) inpainting the\nhuman arms, and (iii) overlaying a rendered bimanual robot that tracks the\nrecovered end-effector trajectories. Pre-training a visual encoder to predict\nfuture 2-D robot keypoints on 675K frames of these edited clips, and continuing\nthat auxiliary loss while fine-tuning a diffusion policy head on only 50 robot\ndemonstrations per task, yields policies that generalize significantly better\nthan prior work. On three long-horizon, bimanual kitchen tasks evaluated in\nthree unseen scenes each, Masquerade outperforms baselines by 5-6x. Ablations\nshow that both the robot overlay and co-training are indispensable, and\nperformance scales logarithmically with the amount of edited human video. These\nresults demonstrate that explicitly closing the visual embodiment gap unlocks a\nvast, readily available source of data from human videos that can be used to\nimprove robot policies.", "AI": {"tldr": "Masquerade方法通过编辑人类视角视频，缩小人机视觉差异，然后利用这些“机器人化”视频训练机器人策略，显著提升了机器人策略的泛化能力。", "motivation": "机器人操作研究面临严重的数据稀缺问题，现有机器人数据集远小于语言和视觉领域的数据集，限制了机器人学习的突破。", "method": "Masquerade方法将野外人类视角视频转化为机器人演示：(i) 估计3D手部姿态，(ii) 修复人类手臂，(iii) 叠加一个跟踪恢复的末端执行器轨迹的渲染双臂机器人。然后，在67.5万帧编辑后的视频片段上预训练一个视觉编码器来预测未来的2D机器人关键点，并在每个任务仅50个机器人演示上微调扩散策略头，同时继续辅助损失。", "result": "在三个长周期、双臂厨房任务（每个任务在三个未见场景中评估）上，Masquerade的性能比现有基线提高了5-6倍。消融实验表明，机器人叠加和协同训练都不可或缺，并且性能与编辑后的人类视频量呈对数关系增长。", "conclusion": "明确地弥合视觉具身差距，可以利用大量现成的人类视频数据来改进机器人策略，为机器人操作研究提供了一条新的数据获取途径。"}}
{"id": "2508.09654", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.09654", "abs": "https://arxiv.org/abs/2508.09654", "authors": ["Alexandre Verine", "Florian Le Bronnec", "Kunhao Zheng", "Alexandre Allauzen", "Yann Chevaleyre", "Benjamin Negrevergne"], "title": "Improving Diversity in Language Models: When Temperature Fails, Change the Loss", "comment": "Forty-Second International Conference on Machine Learning, ICML2025", "summary": "Increasing diversity in language models is a challenging yet essential\nobjective. A common approach is to raise the decoding temperature. In this\nwork, we investigate this approach through a simplistic yet common case to\nprovide insights into why decreasing temperature can improve quality\n(Precision), while increasing it often fails to boost coverage (Recall). Our\nanalysis reveals that for a model to be effectively tunable through temperature\nadjustments, it must be trained toward coverage. To address this, we propose\nrethinking loss functions in language models by leveraging the Precision-Recall\nframework. Our results demonstrate that this approach achieves a substantially\nbetter trade-off between Precision and Recall than merely combining negative\nlog-likelihood training with temperature scaling. These findings offer a\npathway toward more versatile and robust language modeling techniques.", "AI": {"tldr": "该研究探讨了调整解码温度对语言模型多样性的影响，发现其在提升召回率方面的局限性。为解决此问题，论文提出了一种基于精确率-召回率框架的新损失函数，以实现更好的性能权衡。", "motivation": "提高语言模型的多样性是一个重要且具挑战性的目标。常用的方法是提高解码温度，但这种方法在提升召回率（覆盖度）方面往往效果不佳。本研究旨在深入理解其原因，并寻找更有效的多样性提升方案。", "method": "研究首先通过分析一个简单但常见的案例，揭示了模型若要通过温度调整有效调优，必须朝向覆盖度（召回率）进行训练。在此基础上，论文提出利用精确率-召回率（Precision-Recall）框架重新设计语言模型的损失函数。", "result": "实验结果表明，与仅将负对数似然训练与温度缩放相结合的方法相比，所提出的基于精确率-召回率框架的方法在精确率和召回率之间实现了显著更好的权衡。", "conclusion": "这些发现为开发更通用、更鲁棒的语言模型技术提供了新的途径，强调了在模型训练阶段就应考虑多样性（召回率）的重要性。"}}
{"id": "2508.09397", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09397", "abs": "https://arxiv.org/abs/2508.09397", "authors": ["Zhengli Zhang", "Xinyu Luo", "Yuchen Sun", "Wenhua Ding", "Dongyu Huang", "Xinlei Chen"], "title": "Skyshield: Event-Driven Submillimetre Thin Obstacle Detection for Drone Flight Safety", "comment": null, "summary": "Drones operating in complex environments face a significant threat from thin\nobstacles, such as steel wires and kite strings at the submillimeter level,\nwhich are notoriously difficult for conventional sensors like RGB cameras,\nLiDAR, and depth cameras to detect. This paper introduces SkyShield, an\nevent-driven, end-to-end framework designed for the perception of submillimeter\nscale obstacles. Drawing upon the unique features that thin obstacles present\nin the event stream, our method employs a lightweight U-Net architecture and an\ninnovative Dice-Contour Regularization Loss to ensure precise detection.\nExperimental results demonstrate that our event-based approach achieves mean F1\nScore of 0.7088 with a low latency of 21.2 ms, making it ideal for deployment\non edge and mobile platforms.", "AI": {"tldr": "本文提出SkyShield，一个事件驱动的端到端框架，用于无人机检测亚毫米级细障碍物，利用U-Net和Dice-Contour正则化损失，实现了高F1分数和低延迟。", "motivation": "无人机在复杂环境中面临亚毫米级细障碍物（如钢丝、风筝线）的严重威胁，而传统传感器（如RGB相机、激光雷达、深度相机）难以检测这些障碍物。", "method": "引入了SkyShield，一个事件驱动的端到端框架，利用细障碍物在事件流中呈现的独特特征。方法采用轻量级U-Net架构和创新的Dice-Contour正则化损失，以确保精确检测。", "result": "实验结果表明，该事件基方法实现了0.7088的平均F1分数，并具有21.2毫秒的低延迟。", "conclusion": "该事件基方法能够有效感知亚毫米级障碍物，其低延迟使其非常适合部署在边缘和移动平台上。"}}
{"id": "2508.09459", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09459", "abs": "https://arxiv.org/abs/2508.09459", "authors": ["Wen Huang", "Jiarui Yang", "Tao Dai", "Jiawei Li", "Shaoxiong Zhan", "Bin Wang", "Shu-Tao Xia"], "title": "RelayFormer: A Unified Local-Global Attention Framework for Scalable Image and Video Manipulation Localization", "comment": null, "summary": "Visual manipulation localization (VML) -- across both images and videos -- is\na crucial task in digital forensics that involves identifying tampered regions\nin visual content. However, existing methods often lack cross-modal\ngeneralization and struggle to handle high-resolution or long-duration inputs\nefficiently.\n  We propose RelayFormer, a unified and modular architecture for visual\nmanipulation localization across images and videos. By leveraging flexible\nlocal units and a Global-Local Relay Attention (GLoRA) mechanism, it enables\nscalable, resolution-agnostic processing with strong generalization. Our\nframework integrates seamlessly with existing Transformer-based backbones, such\nas ViT and SegFormer, via lightweight adaptation modules that require only\nminimal architectural changes, ensuring compatibility without disrupting\npretrained representations.\n  Furthermore, we design a lightweight, query-based mask decoder that supports\none-shot inference across video sequences with linear complexity. Extensive\nexperiments across multiple benchmarks demonstrate that our approach achieves\nstate-of-the-art localization performance, setting a new baseline for scalable\nand modality-agnostic VML. Code is available at:\nhttps://github.com/WenOOI/RelayFormer.", "AI": {"tldr": "RelayFormer是一种统一且模块化的视觉篡改定位（VML）架构，通过全局-局部中继注意力（GLoRA）和轻量级解码器，实现了跨图像和视频的高效、可扩展和分辨率无关的篡改区域识别，并达到了最先进的性能。", "motivation": "现有视觉篡改定位（VML）方法在跨模态泛化能力上不足，并且难以高效处理高分辨率或长时间的输入内容。", "method": "本文提出了RelayFormer，一个统一且模块化的VML架构。它利用灵活的局部单元和全局-局部中继注意力（GLoRA）机制，实现可扩展、分辨率无关的处理。该框架通过轻量级适应模块与现有基于Transformer的骨干网络（如ViT和SegFormer）无缝集成，并设计了一个轻量级、基于查询的掩码解码器，支持视频序列的一次性推理，具有线性复杂度。", "result": "在多个基准测试中，RelayFormer实现了最先进的定位性能，为可扩展和模态无关的视觉篡改定位设定了新的基线。", "conclusion": "RelayFormer通过其创新的架构和注意力机制，有效解决了视觉篡改定位中跨模态泛化和处理大规模输入的挑战，为该领域提供了高性能和高效率的解决方案。"}}
{"id": "2508.09423", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.09423", "abs": "https://arxiv.org/abs/2508.09423", "authors": ["Badi Li", "Ren-jie Lu", "Yu Zhou", "Jingke Meng", "Wei-shi Zheng"], "title": "Distilling LLM Prior to Flow Model for Generalizable Agent's Imagination in Object Goal Navigation", "comment": null, "summary": "The Object Goal Navigation (ObjectNav) task challenges agents to locate a\nspecified object in an unseen environment by imagining unobserved regions of\nthe scene. Prior approaches rely on deterministic and discriminative models to\ncomplete semantic maps, overlooking the inherent uncertainty in indoor layouts\nand limiting their ability to generalize to unseen environments. In this work,\nwe propose GOAL, a generative flow-based framework that models the semantic\ndistribution of indoor environments by bridging observed regions with\nLLM-enriched full-scene semantic maps. During training, spatial priors inferred\nfrom large language models (LLMs) are encoded as two-dimensional Gaussian\nfields and injected into target maps, distilling rich contextual knowledge into\nthe flow model and enabling more generalizable completions. Extensive\nexperiments demonstrate that GOAL achieves state-of-the-art performance on MP3D\nand Gibson, and shows strong generalization in transfer settings to HM3D. Codes\nand pretrained models are available at https://github.com/Badi-Li/GOAL.", "AI": {"tldr": "本文提出GOAL，一个基于生成流的框架，通过结合LLM增强的语义地图来建模室内环境的语义分布，从而提高目标导航（ObjectNav）任务在未知环境中的泛化能力和性能。", "motivation": "现有的目标导航方法依赖确定性判别模型完成语义地图，忽略了室内布局固有的不确定性，限制了其在未知环境中的泛化能力。", "method": "GOAL是一个生成流框架，通过将观测区域与LLM（大型语言模型）增强的完整场景语义地图连接起来，建模室内环境的语义分布。训练时，从LLM推断出的空间先验（编码为二维高斯场）被注入到目标地图中，将丰富的上下文知识提炼到流模型中，从而实现更具泛化性的地图补全。", "result": "GOAL在MP3D和Gibson数据集上取得了最先进的性能，并在迁移设置中对HM3D表现出强大的泛化能力。", "conclusion": "GOAL通过建模语义分布和利用LLM提炼的上下文知识，有效解决了目标导航中环境不确定性和泛化性的挑战，显著提升了代理在未知环境中定位指定对象的能力。"}}
{"id": "2508.09662", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.09662", "abs": "https://arxiv.org/abs/2508.09662", "authors": ["Yaoning Wang", "Jiahao Ying", "Yixin Cao", "Yubo Ma", "Yugang Jiang"], "title": "EffiEval: Efficient and Generalizable Model Evaluation via Capability Coverage Maximization", "comment": null, "summary": "The rapid advancement of large language models (LLMs) and the development of\nincreasingly large and diverse evaluation benchmarks have introduced\nsubstantial computational challenges for model assessment. In this paper, we\npresent EffiEval, a training-free approach for efficient benchmarking that\neffectively addresses data redundancy while maintaining high evaluation\nreliability. Our method is specifically designed to meet three key criteria for\nhigh-quality evaluation: representativeness, by ensuring comprehensive coverage\nof model capabilities; fairness, by remaining independent of model performance\nduring sample selection to avoid bias; and generalizability, by enabling\nflexible transfer across datasets and model families without reliance on\nlarge-scale evaluation data. Unlike traditional methods that rely on absolute\nperformance or require extensive evaluation data, our approach adaptively\nselects high-quality representative subsets based on the Model Utility Index\n(MUI). Extensive experiments on multiple public benchmarks and diverse LLMs\ndemonstrate that EffiEval achieves strong ranking consistency with full-dataset\nevaluation using only a small fraction of the original data. Furthermore, our\nmethod is flexible and scalable in size, allowing users to balance evaluation\nefficiency and representativeness according to specific needs. Overall,\nEffiEval provides a practical and generalizable solution for reliable, fair,\nand efficient evaluation in the era of LLMs.", "AI": {"tldr": "EffiEval是一种免训练的高效基准测试方法，通过自适应选择具有代表性的子集（基于模型效用指数MUI），在显著减少评估数据量的同时，保持与全数据集评估高度一致的排名可靠性。", "motivation": "大型语言模型（LLMs）的快速发展以及日益庞大和多样化的评估基准，为模型评估带来了巨大的计算挑战，主要原因是数据冗余。", "method": "EffiEval是一种免训练的方法，旨在解决数据冗余并保持高评估可靠性。它满足三个关键评估标准：代表性（全面覆盖模型能力）、公平性（样本选择独立于模型性能，避免偏差）和泛化性（可灵活迁移到不同数据集和模型族，不依赖大规模评估数据）。该方法通过模型效用指数（MUI）自适应选择高质量的代表性子集，而不是依赖绝对性能或大量评估数据。", "result": "在多个公共基准和多样化的LLM上的广泛实验表明，EffiEval仅使用原始数据的一小部分，就能实现与全数据集评估高度一致的排名。此外，该方法在规模上灵活且可扩展，允许用户根据特定需求平衡评估效率和代表性。", "conclusion": "EffiEval为LLM时代提供了一个实用且可泛化的解决方案，用于实现可靠、公平和高效的模型评估。"}}
{"id": "2508.09398", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09398", "abs": "https://arxiv.org/abs/2508.09398", "authors": ["El Mustapha Mansouri"], "title": "Autonomous AI Bird Feeder for Backyard Biodiversity Monitoring", "comment": "Preprint; 8 pages, 5 figures, 1 table; IEEEtran conference format.\n  Code: https://github.com/E-zClap/bird-classifier", "summary": "This paper presents a low cost, on premise system for autonomous backyard\nbird monitoring in Belgian urban gardens. A motion triggered IP camera uploads\nshort clips via FTP to a local server, where frames are sampled and birds are\nlocalized with Detectron2; cropped regions are then classified by an\nEfficientNet-B3 model fine tuned on a 40-species Belgian subset derived from a\nlarger Kaggle corpus. All processing runs on commodity hardware without a\ndiscrete GPU, preserving privacy and avoiding cloud fees. The physical feeder\nuses small entry ports (30 mm) to exclude pigeons and reduce nuisance triggers.\nDetector-guided cropping improves classification accuracy over raw-frame\nclassification. The classifier attains high validation performance on the\ncurated subset (about 99.5 percent) and delivers practical field accuracy\n(top-1 about 88 percent) on held-out species, demonstrating feasibility for\ncitizen-science-grade biodiversity logging at home.", "AI": {"tldr": "本文提出了一种低成本、本地部署的自动后院鸟类监测系统，通过IP摄像头、本地服务器、Detectron2进行鸟类定位，并使用EfficientNet-B3模型进行分类，实现了高准确率，适用于公民科学项目。", "motivation": "需要在比利时城市花园中实现自主、低成本、保护隐私且不依赖云服务的鸟类监测，并有效排除如鸽子等大型鸟类，以支持生物多样性记录和公民科学。", "method": "系统采用运动触发的IP摄像头将短视频通过FTP上传至本地服务器。服务器在商品硬件上运行，无需独立GPU。帧被采样后，使用Detectron2定位鸟类；裁剪后的区域由在40种比利时鸟类子集（源自Kaggle语料库）上微调的EfficientNet-B3模型进行分类。物理喂食器设计有30毫米的小入口，以排除鸽子并减少误触发。检测器引导的裁剪提升了分类准确性。", "result": "分类器在精选子集上的验证性能达到约99.5%。在保留物种上的实际野外准确率（top-1）达到约88%。检测器引导的裁剪显著提高了分类准确性。系统可在商品硬件上运行，无需独立GPU，有效保护隐私并避免云费用。", "conclusion": "该系统在家庭环境中进行公民科学级别的生物多样性记录是可行的，证明了其在实现实用、准确、低成本和隐私保护的自主鸟类监测方面的潜力。"}}
{"id": "2508.09461", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09461", "abs": "https://arxiv.org/abs/2508.09461", "authors": ["Hao Yu", "Rupayan Mallick", "Margrit Betke", "Sarah Adel Bargal"], "title": "Gen-AFFECT: Generation of Avatar Fine-grained Facial Expressions with Consistent identiTy", "comment": null, "summary": "Different forms of customized 2D avatars are widely used in gaming\napplications, virtual communication, education, and content creation. However,\nexisting approaches often fail to capture fine-grained facial expressions and\nstruggle to preserve identity across different expressions. We propose\nGEN-AFFECT, a novel framework for personalized avatar generation that generates\nexpressive and identity-consistent avatars with a diverse set of facial\nexpressions. Our framework proposes conditioning a multimodal diffusion\ntransformer on an extracted identity-expression representation. This enables\nidentity preservation and representation of a wide range of facial expressions.\nGEN-AFFECT additionally employs consistent attention at inference for\ninformation sharing across the set of generated expressions, enabling the\ngeneration process to maintain identity consistency over the array of generated\nfine-grained expressions. GEN-AFFECT demonstrates superior performance compared\nto previous state-of-the-art methods on the basis of the accuracy of the\ngenerated expressions, the preservation of the identity and the consistency of\nthe target identity across an array of fine-grained facial expressions.", "AI": {"tldr": "GEN-AFFECT是一个新颖的框架，用于生成具有丰富面部表情且身份一致的个性化2D虚拟形象。", "motivation": "现有的2D虚拟形象生成方法难以捕捉细微的面部表情，并且在不同表情下难以保持身份一致性。", "method": "GEN-AFFECT框架提出了一种多模态扩散Transformer，它以提取的身份-表情表示为条件，以实现身份保留和广泛的面部表情表示。此外，它在推理时采用一致性注意力机制，以在生成的表情集合之间共享信息，从而在生成细微表情数组时保持身份一致性。", "result": "与现有最先进的方法相比，GEN-AFFECT在生成表情的准确性、身份保留以及目标身份在各种细微面部表情中的一致性方面表现出卓越的性能。", "conclusion": "GEN-AFFECT提供了一种优于现有技术的解决方案，能够生成更具表现力且身份高度一致的个性化2D虚拟形象。"}}
{"id": "2508.09560", "categories": ["cs.CV", "cs.RO", "I.4.10"], "pdf": "https://arxiv.org/pdf/2508.09560", "abs": "https://arxiv.org/abs/2508.09560", "authors": ["Jiahao Wen", "Hang Yu", "Zhedong Zheng"], "title": "WeatherPrompt: Multi-modality Representation Learning for All-Weather Drone Visual Geo-Localization", "comment": "13 pages, 4figures", "summary": "Visual geo-localization for drones faces critical degradation under weather\nperturbations, \\eg, rain and fog, where existing methods struggle with two\ninherent limitations: 1) Heavy reliance on limited weather categories that\nconstrain generalization, and 2) Suboptimal disentanglement of entangled\nscene-weather features through pseudo weather categories. We present\nWeatherPrompt, a multi-modality learning paradigm that establishes\nweather-invariant representations through fusing the image embedding with the\ntext context. Our framework introduces two key contributions: First, a\nTraining-free Weather Reasoning mechanism that employs off-the-shelf large\nmulti-modality models to synthesize multi-weather textual descriptions through\nhuman-like reasoning. It improves the scalability to unseen or complex weather,\nand could reflect different weather strength. Second, to better disentangle the\nscene and weather feature, we propose a multi-modality framework with the\ndynamic gating mechanism driven by the text embedding to adaptively reweight\nand fuse visual features across modalities. The framework is further optimized\nby the cross-modal objectives, including image-text contrastive learning and\nimage-text matching, which maps the same scene with different weather\nconditions closer in the respresentation space. Extensive experiments validate\nthat, under diverse weather conditions, our method achieves competitive recall\nrates compared to state-of-the-art drone geo-localization methods. Notably, it\nimproves Recall@1 by +13.37\\% under night conditions and by 18.69\\% under fog\nand snow conditions.", "AI": {"tldr": "WeatherPrompt是一种多模态学习范式，通过融合图像和文本上下文，为无人机地理定位建立天气不变的表示，显著提高了恶劣天气下的定位性能。", "motivation": "现有无人机视觉地理定位方法在雨、雾等天气扰动下性能严重下降，主要受限于：1) 过度依赖有限的天气类别，导致泛化能力差；2) 通过伪天气类别对场景-天气特征进行解耦效果不佳。", "method": "本文提出了WeatherPrompt，一种多模态学习范式，通过将图像嵌入与文本上下文融合来建立天气不变的表示。它包含两个关键贡献：1) 免训练天气推理机制，利用现成的大型多模态模型合成多天气文本描述，提高对未知或复杂天气的泛化能力；2) 带有动态门控机制的多模态框架，由文本嵌入驱动，自适应地重新加权和融合跨模态的视觉特征，以更好地解耦场景和天气特征。该框架通过跨模态目标（包括图文对比学习和图文匹配）进行优化，使不同天气条件下的相同场景在表示空间中更接近。", "result": "在各种天气条件下，WeatherPrompt方法与最先进的无人机地理定位方法相比，实现了有竞争力的召回率。特别是在夜间条件下，Recall@1提高了13.37%，在雾和雪条件下，Recall@1提高了18.69%。", "conclusion": "WeatherPrompt通过多模态学习和创新的天气推理及特征融合机制，有效解决了无人机地理定位在恶劣天气下的性能退化问题，显著提高了在复杂天气条件下的定位准确性。"}}
{"id": "2508.09666", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.09666", "abs": "https://arxiv.org/abs/2508.09666", "authors": ["Ziyang Ma", "Qingyue Yuan", "Linhai Zhang", "Deyu Zhou"], "title": "Slow Tuning and Low-Entropy Masking for Safe Chain-of-Thought Distillation", "comment": "Preprint", "summary": "Previous chain-of-thought (CoT) distillation methods primarily focused on\nenhancing the reasoning capabilities of Small Language Models (SLMs) by\nutilizing high-quality rationales generated by powerful Large Language Models\n(LLMs, e.g., GPT-4). However, few works have noted the negative effects on SLM\nsafety brought by the training, which are revealed in this study. Although\nthere are works on safety alignment that fine-tune language models or\nmanipulate model weights to defend against harmful inputs, they require extra\ncomputation or annotated data, and probably impact the reasoning ability of\nSLMs. In this paper, we investigate how to maintain the safety of SLMs during\nthe CoT distillation process. Specifically, we propose a safe distillation\nmethod, Slow Tuning and Low-Entropy Masking Distillation (SLowED), containing\ntwo modules: Slow Tuning and Low-Entropy Masking. Slow Tuning scales down the\nmagnitude of model weight changes to optimize the model weights in the\nneighboring space near the initial weight distribution. Low-Entropy Masking\nmasks low-entropy tokens, which are regarded as unnecessary learning targets,\nto exclude them from fine-tuning. Experiments on three SLMs (Qwen2.5-1.5B,\nLlama-3.2-1B, BLOOM-1.1B) across reasoning benchmarks (BBH, BB-Sub, ARC,\nAGIEval) and safety evaluation (AdvBench) show that SLowED retains the safety\nof SLMs and comparably improves their reasoning capability compared to existing\ndistillation methods. Furthermore, our ablation study presents the\neffectiveness of Slow Tuning and Low-Entropy Masking, with the former\nmaintaining the model's safety in the early stage and the latter prolonging the\nsafe training epochs.", "AI": {"tldr": "本文提出了一种名为SLowED的安全蒸馏方法，通过“慢速微调”和“低熵掩码”模块，在提升小型语言模型（SLM）推理能力的同时，有效保持其安全性，解决了现有思维链（CoT）蒸馏方法可能损害SLM安全性的问题。", "motivation": "现有的CoT蒸馏方法主要关注提升SLM的推理能力，但很少关注训练过程对SLM安全性造成的负面影响。尽管有其他安全对齐方法，但它们通常需要额外计算或标注数据，并可能影响模型的推理能力。因此，研究如何在CoT蒸馏过程中保持SLM的安全性是一个重要且未被充分探索的问题。", "method": "本文提出了一种名为SLowED（Slow Tuning and Low-Entropy Masking Distillation）的安全蒸馏方法，包含两个核心模块：1. **慢速微调（Slow Tuning）**：通过缩小模型权重变化的幅度，在初始权重分布的邻近空间内优化模型权重。2. **低熵掩码（Low-Entropy Masking）**：掩盖被认为是冗余学习目标的低熵token，将其从微调中排除。", "result": "在三个SLM（Qwen2.5-1.5B, Llama-3.2-1B, BLOOM-1.1B）上进行的推理基准测试（BBH, BB-Sub, ARC, AGIEval）和安全评估（AdvBench）实验表明，SLowED方法在保持SLM安全性的同时，能够与现有蒸馏方法媲美地提升其推理能力。此外，消融研究证明了慢速微调和低熵掩码模块的有效性，其中前者在早期阶段保持模型安全，后者则延长了安全训练的周期。", "conclusion": "SLowED是一种有效且新颖的CoT蒸馏方法，它成功解决了SLM在推理能力提升过程中安全性受损的问题。通过创新的慢速微调和低熵掩码机制，该方法能够在不牺牲推理性能的前提下，显著增强SLM的安全性，为未来安全有效的模型蒸馏提供了新思路。"}}
{"id": "2508.09404", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.09404", "abs": "https://arxiv.org/abs/2508.09404", "authors": ["Guangxun Zhu", "Shiyu Fan", "Hang Dai", "Edmond S. L. Ho"], "title": "Waymo-3DSkelMo: A Multi-Agent 3D Skeletal Motion Dataset for Pedestrian Interaction Modeling in Autonomous Driving", "comment": "ACM Multimedia 2025 (Dataset Track) Paper", "summary": "Large-scale high-quality 3D motion datasets with multi-person interactions\nare crucial for data-driven models in autonomous driving to achieve\nfine-grained pedestrian interaction understanding in dynamic urban\nenvironments. However, existing datasets mostly rely on estimating 3D poses\nfrom monocular RGB video frames, which suffer from occlusion and lack of\ntemporal continuity, thus resulting in unrealistic and low-quality human\nmotion. In this paper, we introduce Waymo-3DSkelMo, the first large-scale\ndataset providing high-quality, temporally coherent 3D skeletal motions with\nexplicit interaction semantics, derived from the Waymo Perception dataset. Our\nkey insight is to utilize 3D human body shape and motion priors to enhance the\nquality of the 3D pose sequences extracted from the raw LiDRA point clouds. The\ndataset covers over 14,000 seconds across more than 800 real driving scenarios,\nincluding rich interactions among an average of 27 agents per scene (with up to\n250 agents in the largest scene). Furthermore, we establish 3D pose forecasting\nbenchmarks under varying pedestrian densities, and the results demonstrate its\nvalue as a foundational resource for future research on fine-grained human\nbehavior understanding in complex urban environments. The dataset and code will\nbe available at https://github.com/GuangxunZhu/Waymo-3DSkelMo", "AI": {"tldr": "本文介绍了Waymo-3DSkelMo，一个大规模、高质量、时间连贯的3D骨骼运动数据集，包含多人物交互，旨在提升自动驾驶中行人行为理解。", "motivation": "现有的3D运动数据集多依赖单目RGB视频估计姿态，受遮挡和时间不连续性影响，导致人体运动不真实、质量低，无法满足自动驾驶中对行人精细交互理解的需求。", "method": "Waymo-3DSkelMo数据集基于Waymo感知数据集构建。其核心方法是利用3D人体形状和运动先验知识，来提升从原始LiDRA点云中提取的3D姿态序列的质量。", "result": "该数据集涵盖超过14,000秒的800多个真实驾驶场景，包含丰富的交互（平均每场景27个智能体，最多250个）。此外，论文还建立了不同行人密度下的3D姿态预测基准。", "conclusion": "Waymo-3DSkelMo数据集是未来研究复杂城市环境中精细人类行为理解的重要基础资源。"}}
{"id": "2508.09486", "categories": ["cs.CV", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.09486", "abs": "https://arxiv.org/abs/2508.09486", "authors": ["Yun Wang", "Long Zhang", "Jingren Liu", "Jiaqi Yan", "Zhanjie Zhang", "Jiahao Zheng", "Xun Yang", "Dapeng Wu", "Xiangyu Chen", "Xuelong Li"], "title": "Episodic Memory Representation for Long-form Video Understanding", "comment": "10 pages, 5 figures", "summary": "Video Large Language Models (Video-LLMs) excel at general video understanding\nbut struggle with long-form videos due to context window limits. Consequently,\nrecent approaches focus on keyframe retrieval, condensing lengthy videos into a\nsmall set of informative frames. Despite their practicality, these methods\nsimplify the problem to static text image matching, overlooking spatio temporal\nrelationships crucial for capturing scene transitions and contextual\ncontinuity, and may yield redundant keyframes with limited information,\ndiluting salient cues essential for accurate video question answering. To\naddress these limitations, we introduce Video-EM, a training free framework\ninspired by the principles of human episodic memory, designed to facilitate\nrobust and contextually grounded reasoning. Rather than treating keyframes as\nisolated visual entities, Video-EM explicitly models them as temporally ordered\nepisodic events, capturing both spatial relationships and temporal dynamics\nnecessary for accurately reconstructing the underlying narrative. Furthermore,\nthe framework leverages chain of thought (CoT) thinking with LLMs to\niteratively identify a minimal yet highly informative subset of episodic\nmemories, enabling efficient and accurate question answering by Video-LLMs.\nExtensive evaluations on the Video-MME, EgoSchema, HourVideo, and LVBench\nbenchmarks confirm the superiority of Video-EM, which achieves highly\ncompetitive results with performance gains of 4-9 percent over respective\nbaselines while utilizing fewer frames.", "AI": {"tldr": "Video-EM是一个受人类情景记忆启发的免训练框架，通过将关键帧建模为时间有序的情景事件，并结合LLM的思维链，为长视频问答识别出最小且信息量丰富的关键帧子集，显著优于现有基线。", "motivation": "现有视频大语言模型（Video-LLMs）因上下文窗口限制难以处理长视频。当前的关键帧检索方法将问题简化为静态图文匹配，忽略了时空关系，导致关键帧冗余且信息量有限，影响了准确的视频问答。", "method": "引入Video-EM框架，该框架无需训练。它将关键帧显式建模为时间有序的情景事件，以捕捉空间关系和时间动态。此外，它利用大语言模型的思维链（CoT）推理，迭代识别出最小但信息量高度丰富的情景记忆子集，从而实现高效准确的问答。", "result": "在Video-MME、EgoSchema、HourVideo和LVBench基准测试上，Video-EM表现优异，与各自基线相比，性能提升4-9%，且使用的帧数更少。", "conclusion": "Video-EM通过模拟人类情景记忆并结合思维链推理，有效解决了长视频理解中关键帧选择的局限性，实现了更鲁棒、上下文更丰富的推理，并显著提升了Video-LLMs在长视频问答任务上的效率和准确性。"}}
{"id": "2508.09625", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.09625", "abs": "https://arxiv.org/abs/2508.09625", "authors": ["Daoxin Zhong", "Jun Li", "Meng Yee Michael Chuah"], "title": "Plane Detection and Ranking via Model Information Optimization", "comment": "Accepted as contributed paper in the 2025 IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS)", "summary": "Plane detection from depth images is a crucial subtask with broad robotic\napplications, often accomplished by iterative methods such as Random Sample\nConsensus (RANSAC). While RANSAC is a robust strategy with strong probabilistic\nguarantees, the ambiguity of its inlier threshold criterion makes it\nsusceptible to false positive plane detections. This issue is particularly\nprevalent in complex real-world scenes, where the true number of planes is\nunknown and multiple planes coexist. In this paper, we aim to address this\nlimitation by proposing a generalised framework for plane detection based on\nmodel information optimization. Building on previous works, we treat the\nobserved depth readings as discrete random variables, with their probability\ndistributions constrained by the ground truth planes. Various models containing\ndifferent candidate plane constraints are then generated through repeated\nrandom sub-sampling to explain our observations. By incorporating the physics\nand noise model of the depth sensor, we can calculate the information for each\nmodel, and the model with the least information is accepted as the most likely\nground truth. This information optimization process serves as an objective\nmechanism for determining the true number of planes and preventing false\npositive detections. Additionally, the quality of each detected plane can be\nranked by summing the information reduction of inlier points for each plane. We\nvalidate these properties through experiments with synthetic data and find that\nour algorithm estimates plane parameters more accurately compared to the\ndefault Open3D RANSAC plane segmentation. Furthermore, we accelerate our\nalgorithm by partitioning the depth map using neural network segmentation,\nwhich enhances its ability to generate more realistic plane parameters in\nreal-world data.", "AI": {"tldr": "本文提出了一种基于模型信息优化的通用平面检测框架，旨在解决传统RANSAC方法在复杂场景中易产生假阳性检测的问题，并通过信息最小化客观地确定真实平面数量和防止误检。", "motivation": "传统的RANSAC方法在深度图像平面检测中，由于其内点阈值准则的模糊性，容易导致假阳性平面检测，尤其是在真实平面数量未知且多平面共存的复杂场景中，这一问题尤为突出。", "method": "该方法将观测到的深度读数视为离散随机变量，其概率分布受真实平面约束。通过重复随机子采样生成包含不同候选平面约束的多种模型。结合深度传感器的物理和噪声模型，计算每个模型的信息量，选择信息量最小的模型作为最可能的真实情况。此信息优化过程客观地确定平面数量并防止假阳性。此外，通过累加每个平面内点的信息减少量来评估检测到的平面质量。为加速算法，还利用神经网络分割对深度图进行分区。", "result": "与默认的Open3D RANSAC平面分割相比，该算法能更准确地估计平面参数。通过神经网络分割加速后，其在真实世界数据中生成更真实平面参数的能力得到增强。", "conclusion": "所提出的基于模型信息优化的通用平面检测框架，通过客观的信息最小化过程，有效解决了RANSAC的假阳性问题，提高了平面参数估计的准确性，并能确定真实的平面数量和评估平面质量。"}}
{"id": "2508.09713", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09713", "abs": "https://arxiv.org/abs/2508.09713", "authors": ["Rahul Hemrajani"], "title": "Evaluating the Role of Large Language Models in Legal Practice in India", "comment": null, "summary": "The integration of Artificial Intelligence(AI) into the legal profession\nraises significant questions about the capacity of Large Language Models(LLM)\nto perform key legal tasks. In this paper, I empirically evaluate how well\nLLMs, such as GPT, Claude, and Llama, perform key legal tasks in the Indian\ncontext, including issue spotting, legal drafting, advice, research, and\nreasoning. Through a survey experiment, I compare outputs from LLMs with those\nof a junior lawyer, with advanced law students rating the work on helpfulness,\naccuracy, and comprehensiveness. LLMs excel in drafting and issue spotting,\noften matching or surpassing human work. However, they struggle with\nspecialised legal research, frequently generating hallucinations, factually\nincorrect or fabricated outputs. I conclude that while LLMs can augment certain\nlegal tasks, human expertise remains essential for nuanced reasoning and the\nprecise application of law.", "AI": {"tldr": "该研究评估了大型语言模型（LLMs）在印度法律任务中的表现，发现它们在起草和问题识别方面表现出色，但在专业法律研究方面存在幻觉和不准确性，表明人类专业知识仍不可或缺。", "motivation": "人工智能（AI）与法律专业的融合引发了关于大型语言模型（LLM）执行关键法律任务能力的重大疑问。", "method": "通过一项调查实验，作者实证评估了GPT、Claude和Llama等LLM在印度背景下执行法律任务（包括问题识别、法律起草、建议、研究和推理）的表现。将LLM的输出与一名初级律师的输出进行比较，并由高级法学生根据实用性、准确性和全面性进行评分。", "result": "LLM在起草和问题识别方面表现出色，通常能与人类工作匹敌或超越。然而，它们在专业法律研究方面表现不佳，频繁产生幻觉、事实不正确或捏造的输出。", "conclusion": "LLM可以增强某些法律任务，但人类专业知识对于细致的推理和法律的精确应用仍然至关重要。"}}
{"id": "2508.09446", "categories": ["cs.CV", "I.2.8"], "pdf": "https://arxiv.org/pdf/2508.09446", "abs": "https://arxiv.org/abs/2508.09446", "authors": ["Jiateng Liu", "Hengcan Shi", "Feng Chen", "Zhiwen Shao", "Yaonan Wang", "Jianfei Cai", "Wenming Zheng"], "title": "MPT: Motion Prompt Tuning for Micro-Expression Recognition", "comment": null, "summary": "Micro-expression recognition (MER) is crucial in the affective computing\nfield due to its wide application in medical diagnosis, lie detection, and\ncriminal investigation. Despite its significance, obtaining micro-expression\n(ME) annotations is challenging due to the expertise required from\npsychological professionals. Consequently, ME datasets often suffer from a\nscarcity of training samples, severely constraining the learning of MER models.\nWhile current large pre-training models (LMs) offer general and discriminative\nrepresentations, their direct application to MER is hindered by an inability to\ncapture transitory and subtle facial movements-essential elements for effective\nMER. This paper introduces Motion Prompt Tuning (MPT) as a novel approach to\nadapting LMs for MER, representing a pioneering method for subtle motion prompt\ntuning. Particularly, we introduce motion prompt generation, including motion\nmagnification and Gaussian tokenization, to extract subtle motions as prompts\nfor LMs. Additionally, a group adapter is carefully designed and inserted into\nthe LM to enhance it in the target MER domain, facilitating a more nuanced\ndistinction of ME representation. Furthermore, extensive experiments conducted\non three widely used MER datasets demonstrate that our proposed MPT\nconsistently surpasses state-of-the-art approaches and verifies its\neffectiveness.", "AI": {"tldr": "该论文提出了一种名为运动提示微调（MPT）的新方法，通过生成运动提示和设计分组适配器来使大型预训练模型（LMs）适应微表情识别（MER），有效解决了数据稀缺和现有模型难以捕捉细微运动的问题，并在多个数据集上取得了最先进的性能。", "motivation": "微表情识别（MER）在医学诊断、测谎和犯罪调查等领域具有重要应用价值，但其面临两大挑战：一是微表情数据标注需要专业心理学知识，导致训练样本稀缺；二是现有的大型预训练模型难以捕捉微表情中短暂而细微的面部运动，限制了其在MER中的直接应用。", "method": "该论文引入了运动提示微调（MPT）方法。具体包括：1. 运动提示生成：通过运动放大和高斯标记化来提取细微运动作为LMs的提示。2. 分组适配器：精心设计并插入到LM中，以增强其在MER目标域的性能，促进更细致的微表情区分。", "result": "在三个广泛使用的MER数据集上进行了大量实验，结果表明所提出的MPT方法持续超越了现有的最先进方法，验证了其有效性。", "conclusion": "MPT是一种新颖且有效的方法，能够将大型预训练模型成功应用于微表情识别，通过捕捉细微运动显著提高了MER的性能，解决了该领域面临的数据稀缺和运动捕捉难题。"}}
{"id": "2508.09522", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.09522", "abs": "https://arxiv.org/abs/2508.09522", "authors": ["Ajeet Kumar Yadav", "Nishant Kumar", "Rathna G N"], "title": "Generation of Indian Sign Language Letters, Numbers, and Words", "comment": "6 pages, 5 figures, 2024 International Conference on Intelligent\n  Algorithms for Computational Intelligence Systems (IACIS)", "summary": "Sign language, which contains hand movements, facial expressions and bodily\ngestures, is a significant medium for communicating with hard-of-hearing\npeople. A well-trained sign language community communicates easily, but those\nwho don't know sign language face significant challenges. Recognition and\ngeneration are basic communication methods between hearing and hard-of-hearing\nindividuals. Despite progress in recognition, sign language generation still\nneeds to be explored. The Progressive Growing of Generative Adversarial Network\n(ProGAN) excels at producing high-quality images, while the Self-Attention\nGenerative Adversarial Network (SAGAN) generates feature-rich images at medium\nresolutions. Balancing resolution and detail is crucial for sign language image\ngeneration. We are developing a Generative Adversarial Network (GAN) variant\nthat combines both models to generate feature-rich, high-resolution, and\nclass-conditional sign language images. Our modified Attention-based model\ngenerates high-quality images of Indian Sign Language letters, numbers, and\nwords, outperforming the traditional ProGAN in Inception Score (IS) and\nFr\\'echet Inception Distance (FID), with improvements of 3.2 and 30.12,\nrespectively. Additionally, we are publishing a large dataset incorporating\nhigh-quality images of Indian Sign Language alphabets, numbers, and 129 words.", "AI": {"tldr": "该研究开发了一种结合ProGAN和SAGAN的新型生成对抗网络（GAN），用于生成高质量、高分辨率、特征丰富的印度手语图像，并在IS和FID指标上超越了传统ProGAN，同时发布了一个大型印度手语数据集。", "motivation": "手语是与听障人士沟通的重要媒介，但非手语使用者面临沟通障碍。手语识别研究进展较多，但手语生成仍有待探索。现有GAN模型在分辨率和细节之间存在权衡，而手语图像生成需要同时兼顾高分辨率和丰富细节。", "method": "研究者开发了一种改进的基于注意力的生成对抗网络（GAN）变体，该模型结合了渐进式生成对抗网络（ProGAN）和自注意力生成对抗网络（SAGAN）的优点。目标是生成特征丰富、高分辨率且类别条件化的手语图像。模型用于生成印度手语字母、数字和单词图像，并发布了一个包含印度手语字母、数字和129个单词的高质量图像大型数据集。", "result": "所提出的改进的基于注意力的模型能够生成高质量的印度手语字母、数字和单词图像。与传统ProGAN相比，该模型在Inception Score（IS）上提升了3.2，在Fréchet Inception Distance（FID）上提升了30.12。此外，研究还发布了一个包含印度手语字母、数字和129个单词的大型数据集。", "conclusion": "该研究成功开发了一种结合ProGAN和SAGAN优势的GAN模型，能够生成高质量、高分辨率的印度手语图像，并在性能上显著优于现有模型。同时发布的大型数据集将为手语生成领域的研究提供重要资源。"}}
{"id": "2508.09681", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.09681", "abs": "https://arxiv.org/abs/2508.09681", "authors": ["Gerardo Loza", "Junlei Hu", "Dominic Jones", "Sharib Ali", "Pietro Valdastri"], "title": "Surg-InvNeRF: Invertible NeRF for 3D tracking and reconstruction in surgical vision", "comment": "10 pages", "summary": "We proposed a novel test-time optimisation (TTO) approach framed by a\nNeRF-based architecture for long-term 3D point tracking. Most current methods\nin point tracking struggle to obtain consistent motion or are limited to 2D\nmotion. TTO approaches frame the solution for long-term tracking as optimising\na function that aggregates correspondences from other specialised\nstate-of-the-art methods. Unlike the state-of-the-art on TTO, we propose\nparametrising such a function with our new invertible Neural Radiance Field\n(InvNeRF) architecture to perform both 2D and 3D tracking in surgical\nscenarios. Our approach allows us to exploit the advantages of a\nrendering-based approach by supervising the reprojection of pixel\ncorrespondences. It adapts strategies from recent rendering-based methods to\nobtain a bidirectional deformable-canonical mapping, to efficiently handle a\ndefined workspace, and to guide the rays' density. It also presents our\nmulti-scale HexPlanes for fast inference and a new algorithm for efficient\npixel sampling and convergence criteria. We present results in the STIR and\nSCARE datasets, for evaluating point tracking and testing the integration of\nkinematic data in our pipeline, respectively. In 2D point tracking, our\napproach surpasses the precision and accuracy of the TTO state-of-the-art\nmethods by nearly 50% on average precision, while competing with other\napproaches. In 3D point tracking, this is the first TTO approach, surpassing\nfeed-forward methods while incorporating the benefits of a deformable\nNeRF-based reconstruction.", "AI": {"tldr": "该论文提出了一种基于NeRF的新型测试时优化（TTO）方法，用于长期3D点跟踪，在2D和3D跟踪方面均取得了显著的精度提升，尤其在3D跟踪上是首个超越前馈方法的TTO。", "motivation": "当前的点跟踪方法难以获得一致的运动轨迹，或仅限于2D运动。现有的TTO方法虽然能整合对应关系，但在函数参数化方面有局限性。研究旨在解决这些问题，实现更精确、一致的2D和3D长期点跟踪，特别是在外科场景中。", "method": "该方法提出了一种基于NeRF架构的测试时优化（TTO）方法，并引入了一种新型可逆神经辐射场（InvNeRF）架构来参数化跟踪函数。它利用渲染方法的优势，通过监督像素对应关系的重投影进行跟踪。该方法还结合了双向可变形-规范映射、高效工作空间处理和射线密度引导等策略，并提出了多尺度HexPlanes用于快速推理以及高效像素采样和收敛准则的新算法。", "result": "在2D点跟踪方面，该方法在平均精度上比TTO领域的最新技术提高了近50%，并与其他方法具有竞争力。在3D点跟踪方面，这是首个TTO方法，超越了前馈方法，并结合了可变形NeRF重建的优势。在STIR和SCARE数据集上进行了评估。", "conclusion": "该研究提出的基于InvNeRF的TTO方法显著提升了2D和3D长期点跟踪的精度和一致性，特别是在外科场景中，通过结合NeRF的渲染优势与TTO框架，为3D点跟踪设定了新的基准。"}}
{"id": "2508.09716", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.09716", "abs": "https://arxiv.org/abs/2508.09716", "authors": ["Ridwan Mahbub", "Mohammed Saidul Islam", "Md Tahmid Rahman Laskar", "Mizanur Rahman", "Mir Tafseer Nayeem", "Enamul Hoque"], "title": "The Perils of Chart Deception: How Misleading Visualizations Affect Vision-Language Models", "comment": "Accepted to IEEE VIS 2025", "summary": "Information visualizations are powerful tools that help users quickly\nidentify patterns, trends, and outliers, facilitating informed decision-making.\nHowever, when visualizations incorporate deceptive design elements-such as\ntruncated or inverted axes, unjustified 3D effects, or violations of best\npractices-they can mislead viewers and distort understanding, spreading\nmisinformation. While some deceptive tactics are obvious, others subtly\nmanipulate perception while maintaining a facade of legitimacy. As\nVision-Language Models (VLMs) are increasingly used to interpret\nvisualizations, especially by non-expert users, it is critical to understand\nhow susceptible these models are to deceptive visual designs. In this study, we\nconduct an in-depth evaluation of VLMs' ability to interpret misleading\nvisualizations. By analyzing over 16,000 responses from ten different models\nacross eight distinct types of misleading chart designs, we demonstrate that\nmost VLMs are deceived by them. This leads to altered interpretations of\ncharts, despite the underlying data remaining the same. Our findings highlight\nthe need for robust safeguards in VLMs against visual misinformation.", "AI": {"tldr": "研究发现，视觉语言模型（VLMs）容易被误导性信息可视化设计欺骗，导致对图表产生错误解读，即使底层数据未变。", "motivation": "信息可视化在辅助决策中作用强大，但若包含欺骗性设计元素（如截断轴、不合理3D效果），会误导用户并传播错误信息。随着VLMs被广泛用于解释可视化，尤其被非专业用户使用，理解这些模型对欺骗性视觉设计的敏感性至关重要。", "method": "本研究深入评估了VLMs解释误导性可视化的能力。通过分析10种不同模型对8种不同类型误导性图表设计的超过16,000个响应。", "result": "研究表明，大多数VLMs会被误导性可视化欺骗，导致它们对图表的解释发生改变，尽管底层数据保持不变。", "conclusion": "研究结果强调了在VLMs中建立强大的防护措施以抵御视觉错误信息的必要性。"}}
{"id": "2508.09449", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09449", "abs": "https://arxiv.org/abs/2508.09449", "authors": ["Jiaqi Yan", "Shuning Xu", "Xiangyu Chen", "Dell Zhang", "Jie Tang", "Gangshan Wu", "Jie Liu"], "title": "RASR: Retrieval-Augmented Super Resolution for Practical Reference-based Image Restoration", "comment": null, "summary": "Reference-based Super Resolution (RefSR) improves upon Single Image Super\nResolution (SISR) by leveraging high-quality reference images to enhance\ntexture fidelity and visual realism. However, a critical limitation of existing\nRefSR approaches is their reliance on manually curated target-reference image\npairs, which severely constrains their practicality in real-world scenarios. To\novercome this, we introduce Retrieval-Augmented Super Resolution (RASR), a new\nand practical RefSR paradigm that automatically retrieves semantically relevant\nhigh-resolution images from a reference database given only a low-quality\ninput. This enables scalable and flexible RefSR in realistic use cases, such as\nenhancing mobile photos taken in environments like zoos or museums, where\ncategory-specific reference data (e.g., animals, artworks) can be readily\ncollected or pre-curated. To facilitate research in this direction, we\nconstruct RASR-Flickr30, the first benchmark dataset designed for RASR. Unlike\nprior datasets with fixed target-reference pairs, RASR-Flickr30 provides\nper-category reference databases to support open-world retrieval. We further\npropose RASRNet, a strong baseline that combines a semantic reference retriever\nwith a diffusion-based RefSR generator. It retrieves relevant references based\non semantic similarity and employs a diffusion-based generator enhanced with\nsemantic conditioning. Experiments on RASR-Flickr30 demonstrate that RASRNet\nconsistently improves over SISR baselines, achieving +0.38 dB PSNR and -0.0131\nLPIPS, while generating more realistic textures. These findings highlight\nretrieval augmentation as a promising direction to bridge the gap between\nacademic RefSR research and real-world applicability.", "AI": {"tldr": "该论文提出了一种新的参考超分辨率（RefSR）范式，称为检索增强超分辨率（RASR），旨在解决现有RefSR方法对手动配对的依赖。RASR通过自动检索语义相关的参考图像，并引入了首个RASR基准数据集RASR-Flickr30和基线模型RASRNet，实验证明其在性能和纹理真实感方面均优于SISR基线，提升了RefSR的实际应用性。", "motivation": "现有参考超分辨率（RefSR）方法严重依赖手动策划的目标-参考图像对，这极大地限制了它们在现实世界场景中的实用性。", "method": "引入了检索增强超分辨率（RASR）这一新范式，它仅给定低质量输入即可自动从参考数据库中检索语义相关的高分辨率图像。构建了RASR-Flickr30，这是首个专为RASR设计的基准数据集，提供按类别划分的参考数据库以支持开放世界检索。提出了RASRNet，一个结合了语义参考检索器和基于扩散的RefSR生成器的强大基线模型，该生成器通过语义条件进行增强。", "result": "在RASR-Flickr30数据集上的实验表明，RASRNet始终优于单图像超分辨率（SISR）基线，实现了+0.38 dB PSNR和-0.0131 LPIPS的提升，同时生成了更真实的纹理。", "conclusion": "检索增强是弥合学术RefSR研究与实际应用之间差距的一个有前景的方向。"}}
{"id": "2508.09533", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09533", "abs": "https://arxiv.org/abs/2508.09533", "authors": ["Peiran Peng", "Tingfa Xu", "Liqiang Song", "Mengqi Zhu", "Yuqiang Fang", "Jianan Li"], "title": "COXNet: Cross-Layer Fusion with Adaptive Alignment and Scale Integration for RGBT Tiny Object Detection", "comment": null, "summary": "Detecting tiny objects in multimodal Red-Green-Blue-Thermal (RGBT) imagery is\na critical challenge in computer vision, particularly in surveillance, search\nand rescue, and autonomous navigation. Drone-based scenarios exacerbate these\nchallenges due to spatial misalignment, low-light conditions, occlusion, and\ncluttered backgrounds. Current methods struggle to leverage the complementary\ninformation between visible and thermal modalities effectively. We propose\nCOXNet, a novel framework for RGBT tiny object detection, addressing these\nissues through three core innovations: i) the Cross-Layer Fusion Module, fusing\nhigh-level visible and low-level thermal features for enhanced semantic and\nspatial accuracy; ii) the Dynamic Alignment and Scale Refinement module,\ncorrecting cross-modal spatial misalignments and preserving multi-scale\nfeatures; and iii) an optimized label assignment strategy using the GeoShape\nSimilarity Measure for better localization. COXNet achieves a 3.32\\% mAP$_{50}$\nimprovement on the RGBTDronePerson dataset over state-of-the-art methods,\ndemonstrating its effectiveness for robust detection in complex environments.", "AI": {"tldr": "本文提出COXNet，一个用于RGBT微小目标检测的新框架，通过跨层融合、动态对齐与尺度细化以及优化的标签分配策略，有效解决了多模态图像中的挑战，显著提升了检测性能。", "motivation": "在计算机视觉领域，RGBT图像中的微小目标检测是一个关键挑战，尤其是在监控、搜救和自主导航等无人机场景中，由于空间错位、低光照、遮挡和杂乱背景等问题，现有方法难以有效利用可见光和热成像模态之间的互补信息。", "method": "本文提出了COXNet框架，包含三项核心创新：i) 跨层融合模块，用于融合高层可见光特征和低层热成像特征，以提高语义和空间精度；ii) 动态对齐和尺度细化模块，用于纠正跨模态空间错位并保留多尺度特征；iii) 使用GeoShape相似度量优化标签分配策略，以实现更好的定位。", "result": "COXNet在RGBTDronePerson数据集上，相较于现有最先进方法，mAP$_{50}$提升了3.32%。", "conclusion": "COXNet在复杂环境中实现了鲁棒的微小目标检测，证明了其在RGBT微小目标检测任务中的有效性。"}}
{"id": "2508.09732", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.09732", "abs": "https://arxiv.org/abs/2508.09732", "authors": ["Romeo Valentin", "Sydney M. Katz", "Artur B. Carneiro", "Don Walker", "Mykel J. Kochenderfer"], "title": "Predictive Uncertainty for Runtime Assurance of a Real-Time Computer Vision-Based Landing System", "comment": "8 pages, 5 figures, accepted at DASC 2025", "summary": "Recent advances in data-driven computer vision have enabled robust autonomous\nnavigation capabilities for civil aviation, including automated landing and\nrunway detection. However, ensuring that these systems meet the robustness and\nsafety requirements for aviation applications remains a major challenge. In\nthis work, we present a practical vision-based pipeline for aircraft pose\nestimation from runway images that represents a step toward the ability to\ncertify these systems for use in safety-critical aviation applications. Our\napproach features three key innovations: (i) an efficient, flexible neural\narchitecture based on a spatial Soft Argmax operator for probabilistic keypoint\nregression, supporting diverse vision backbones with real-time inference; (ii)\na principled loss function producing calibrated predictive uncertainties, which\nare evaluated via sharpness and calibration metrics; and (iii) an adaptation of\nResidual-based Receiver Autonomous Integrity Monitoring (RAIM), enabling\nruntime detection and rejection of faulty model outputs. We implement and\nevaluate our pose estimation pipeline on a dataset of runway images. We show\nthat our model outperforms baseline architectures in terms of accuracy while\nalso producing well-calibrated uncertainty estimates with sub-pixel precision\nthat can be used downstream for fault detection.", "AI": {"tldr": "本文提出了一种基于视觉的飞机姿态估计管道，用于跑道图像，旨在提高航空应用的鲁棒性和安全性，并通过新颖的神经网络架构、校准不确定性损失函数和RAIM适应实现了故障检测。", "motivation": "尽管数据驱动的计算机视觉在民用航空自主导航（如自动着陆和跑道检测）方面取得了进展，但确保这些系统满足航空应用所需的鲁棒性和安全要求仍是重大挑战。本研究旨在解决这一认证难题。", "method": "该方法包含三项关键创新：1) 基于空间Soft Argmax算子的高效灵活神经网络架构，用于概率关键点回归，支持多种视觉骨干网络并实现实时推理；2) 一个产生校准预测不确定性的原则性损失函数，通过锐度和校准度量进行评估；3) 适应残差接收器自主完整性监测（RAIM），实现运行时检测和拒绝错误模型输出。", "result": "在跑道图像数据集上评估了该姿态估计管道，结果表明模型在精度上优于基线架构，同时产生亚像素精度的良好校准不确定性估计，可用于下游故障检测。", "conclusion": "该视觉姿态估计管道是朝着使这些系统能够获得安全关键航空应用认证迈出的重要一步，通过提高精度和提供可信的故障检测能力，提升了系统的安全性和可靠性。"}}
{"id": "2508.09726", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.09726", "abs": "https://arxiv.org/abs/2508.09726", "authors": ["Vaishnavi Shrivastava", "Ahmed Awadallah", "Vidhisha Balachandran", "Shivam Garg", "Harkirat Behl", "Dimitris Papailiopoulos"], "title": "Sample More to Think Less: Group Filtered Policy Optimization for Concise Reasoning", "comment": null, "summary": "Large language models trained with reinforcement learning with verifiable\nrewards tend to trade accuracy for length--inflating response lengths to\nachieve gains in accuracy. While longer answers may be warranted for harder\nproblems, many tokens are merely \"filler\": repetitive, verbose text that makes\nno real progress. We introduce GFPO (Group Filtered Policy Optimization), which\ncurbs this length explosion by sampling larger groups per problem during\ntraining and filtering responses to train on based on two key metrics: (1)\nresponse length and (2) token efficiency: reward per token ratio. By sampling\nmore at training time, we teach models to think less at inference time. On the\nPhi-4-reasoning model, GFPO cuts GRPO's length inflation by 46-71% across\nchallenging STEM and coding benchmarks (AIME 24/25, GPQA, Omni-MATH,\nLiveCodeBench) while maintaining accuracy. Optimizing for reward per token\nfurther increases reductions in length inflation to 71-85%. We also propose\nAdaptive Difficulty GFPO, which dynamically allocates more training resources\nto harder problems based on real-time difficulty estimates, improving the\nbalance between computational efficiency and accuracy especially on difficult\nquestions. GFPO demonstrates that increased training-time compute directly\ntranslates to reduced test-time compute--a simple yet effective trade-off for\nefficient reasoning.", "AI": {"tldr": "本文提出GFPO（Group Filtered Policy Optimization）方法，通过在训练时进行大组采样和基于长度与token效率的过滤，显著减少了强化学习训练的大语言模型在推理时产生的冗余长文本，同时保持了准确性。", "motivation": "经强化学习（RL）训练的大语言模型（LLMs）倾向于通过增加响应长度来提高准确性，这导致了大量重复、冗余的“填充”文本，降低了效率。", "method": "引入GFPO，通过以下方式抑制长度膨胀：1) 在训练时对每个问题采样更大的组；2) 基于响应长度和token效率（每token奖励比率）过滤响应进行训练。此外，还提出了自适应难度GFPO，根据实时难度估计动态分配训练资源给更难的问题。", "result": "在Phi-4-reasoning模型上，GFPO在AIME 24/25、GPQA、Omni-MATH、LiveCodeBench等STEM和编程基准测试中，将GRPO的长度膨胀减少了46-71%，同时保持了准确性。进一步优化每token奖励，可将长度减少率提高到71-85%。自适应难度GFPO提高了计算效率和准确性之间的平衡，尤其是在难题上。", "conclusion": "GFPO证明了增加训练时间计算量可以直接转化为减少测试时间计算量，这是一种简单而有效的权衡，可实现高效推理。"}}
{"id": "2508.09453", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.09453", "abs": "https://arxiv.org/abs/2508.09453", "authors": ["Abdul Matin", "Tanjim Bin Faruk", "Shrideep Pallickara", "Sangmi Lee Pallickara"], "title": "HyperKD: Distilling Cross-Spectral Knowledge in Masked Autoencoders via Inverse Domain Shift with Spatial-Aware Masking and Specialized Loss", "comment": null, "summary": "The proliferation of foundation models, pretrained on large-scale unlabeled\ndatasets, has emerged as an effective approach in creating adaptable and\nreusable architectures that can be leveraged for various downstream tasks using\nsatellite observations. However, their direct application to hyperspectral\nremote sensing remains challenging due to inherent spectral disparities and the\nscarcity of available observations. In this work, we present HyperKD, a novel\nknowledge distillation framework that enables transferring learned\nrepresentations from a teacher model into a student model for effective\ndevelopment of a foundation model on hyperspectral images. Unlike typical\nknowledge distillation frameworks, which use a complex teacher to guide a\nsimpler student, HyperKD enables an inverse form of knowledge transfer across\ndifferent types of spectral data, guided by a simpler teacher model. Building\nupon a Masked Autoencoder, HyperKD distills knowledge from the Prithvi\nfoundational model into a student tailored for EnMAP hyperspectral imagery.\nHyperKD addresses the inverse domain adaptation problem with spectral gaps by\nintroducing a feature-based strategy that includes spectral range-based channel\nalignment, spatial feature-guided masking, and an enhanced loss function\ntailored for hyperspectral images. HyperKD bridges the substantial spectral\ndomain gap, enabling the effective use of pretrained foundation models for\ngeospatial applications. Extensive experiments show that HyperKD significantly\nimproves representation learning in MAEs, leading to enhanced reconstruction\nfidelity and more robust performance on downstream tasks such as land cover\nclassification, crop type identification, and soil organic carbon prediction,\nunderpinning the potential of knowledge distillation frameworks in remote\nsensing analytics with hyperspectral imagery.", "AI": {"tldr": "HyperKD是一种新颖的知识蒸馏框架，通过逆向知识迁移（由简单教师模型指导）将预训练的基础模型（如Prithvi）的表示学习能力转移到针对高光谱图像的Masked Autoencoder学生模型，以弥合光谱差异并增强下游任务性能。", "motivation": "基础模型在卫星观测的各种下游任务中表现出色，但由于固有的光谱差异和数据稀缺性，其直接应用于高光谱遥感仍面临挑战。", "method": "本文提出了HyperKD，一个基于Masked Autoencoder的知识蒸馏框架。它采用逆向知识迁移（由简单教师模型引导），将Prithvi基础模型的知识蒸馏到针对EnMAP高光谱图像的学生模型。为解决光谱间隙问题，HyperKD引入了基于特征的策略，包括基于光谱范围的通道对齐、空间特征引导的掩蔽以及为高光谱图像定制的增强损失函数。", "result": "实验表明，HyperKD显著改善了MAEs的表示学习，提高了重建保真度，并在下游任务（如土地覆盖分类、作物类型识别和土壤有机碳预测）上展现出更强的鲁棒性。", "conclusion": "HyperKD成功弥合了显著的光谱域差距，使得预训练的基础模型能有效应用于高光谱地理空间应用，凸显了知识蒸馏框架在高光谱遥感分析中的潜力。"}}
{"id": "2508.09547", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09547", "abs": "https://arxiv.org/abs/2508.09547", "authors": ["Fengyi Wu", "Yifei Dong", "Zhi-Qi Cheng", "Yilong Dai", "Guangyu Chen", "Hang Wang", "Qi Dai", "Alexander G. Hauptmann"], "title": "GoViG: Goal-Conditioned Visual Navigation Instruction Generation", "comment": "Under review. Code: https://github.com/F1y1113/GoViG", "summary": "We introduce Goal-Conditioned Visual Navigation Instruction Generation\n(GoViG), a new task that aims to autonomously generate precise and contextually\ncoherent navigation instructions solely from egocentric visual observations of\ninitial and goal states. Unlike conventional approaches that rely on structured\ninputs such as semantic annotations or environmental maps, GoViG exclusively\nleverages raw egocentric visual data, substantially improving its adaptability\nto unseen and unstructured environments. Our method addresses this task by\ndecomposing it into two interconnected subtasks: (1) visual forecasting, which\npredicts intermediate visual states bridging the initial and goal views; and\n(2) instruction generation, which synthesizes linguistically coherent\ninstructions grounded in both observed and anticipated visuals. These subtasks\nare integrated within an autoregressive multimodal large language model trained\nwith tailored objectives to ensure spatial accuracy and linguistic clarity.\nFurthermore, we introduce two complementary multimodal reasoning strategies,\none-pass and interleaved reasoning, to mimic incremental human cognitive\nprocesses during navigation. To evaluate our method, we propose the R2R-Goal\ndataset, combining diverse synthetic and real-world trajectories. Empirical\nresults demonstrate significant improvements over state-of-the-art methods,\nachieving superior BLEU-4 and CIDEr scores along with robust cross-domain\ngeneralization.", "AI": {"tldr": "本文提出GoViG任务，仅通过自我中心视觉观测生成导航指令。该方法将任务分解为视觉预测和指令生成，使用自回归多模态大语言模型，并引入两种推理策略，在R2R-Goal数据集上显著优于现有方法。", "motivation": "传统导航指令生成方法依赖于结构化输入（如语义标注、环境地图），限制了其对未知和非结构化环境的适应性。本研究旨在仅利用原始自我中心视觉数据生成精确且上下文连贯的导航指令，以提高在复杂环境中的泛化能力。", "method": "该方法将任务分解为两个子任务：1) 视觉预测，用于预测连接初始和目标视图的中间视觉状态；2) 指令生成，用于合成基于观察和预测视觉的语言指令。这两个子任务被整合在一个自回归多模态大语言模型中，并通过定制目标进行训练，以确保空间准确性和语言清晰度。此外，还引入了两种互补的多模态推理策略：一次性推理（one-pass）和交错推理（interleaved reasoning）。", "result": "在提出的R2R-Goal数据集（结合了合成和真实世界轨迹）上，经验结果表明该方法显著优于现有最先进的方法，实现了更高的BLEU-4和CIDEr分数，并展现出强大的跨域泛化能力。", "conclusion": "本研究成功引入了GoViG任务和相应的解决方案，证明了仅从自我中心视觉观测生成高质量导航指令的可行性。该方法克服了传统方法的局限性，在性能和对未见环境的适应性方面均表现出色，为视觉导航指令生成领域开辟了新方向。"}}
{"id": "2508.09811", "categories": ["cs.CV", "cs.AI", "cs.CE", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.09811", "abs": "https://arxiv.org/abs/2508.09811", "authors": ["Jinxi Li", "Ziyang Song", "Bo Yang"], "title": "TRACE: Learning 3D Gaussian Physical Dynamics from Multi-view Videos", "comment": "ICCV 2025. Code and data are available at:\n  https://github.com/vLAR-group/TRACE", "summary": "In this paper, we aim to model 3D scene geometry, appearance, and physical\ninformation just from dynamic multi-view videos in the absence of any human\nlabels. By leveraging physics-informed losses as soft constraints or\nintegrating simple physics models into neural nets, existing works often fail\nto learn complex motion physics, or doing so requires additional labels such as\nobject types or masks. We propose a new framework named TRACE to model the\nmotion physics of complex dynamic 3D scenes. The key novelty of our method is\nthat, by formulating each 3D point as a rigid particle with size and\norientation in space, we directly learn a translation rotation dynamics system\nfor each particle, explicitly estimating a complete set of physical parameters\nto govern the particle's motion over time. Extensive experiments on three\nexisting dynamic datasets and one newly created challenging synthetic datasets\ndemonstrate the extraordinary performance of our method over baselines in the\ntask of future frame extrapolation. A nice property of our framework is that\nmultiple objects or parts can be easily segmented just by clustering the\nlearned physical parameters.", "AI": {"tldr": "该论文提出了一个名为TRACE的新框架，仅从动态多视角视频中学习3D场景的几何、外观和复杂运动物理信息，无需人工标注，并通过学习每个3D点的平移旋转动力学系统来实现。", "motivation": "现有方法在学习复杂运动物理方面表现不佳，或者需要额外的标签（如对象类型或掩码），无法仅从动态多视角视频中有效建模3D场景的物理信息。", "method": "该方法将每个3D点建模为一个具有大小和方向的刚体粒子，并直接学习每个粒子的平移旋转动力学系统，显式估计一套完整的物理参数来控制粒子随时间的运动。", "result": "在三个现有动态数据集和一个新创建的挑战性合成数据集上的广泛实验表明，该方法在未来帧外推任务中表现出超越基线的卓越性能。此外，通过聚类学习到的物理参数，可以轻松地分割出多个对象或部件。", "conclusion": "TRACE框架能够有效地从动态多视角视频中学习复杂的3D场景运动物理，无需人工标注，并能成功进行未来帧预测和对象分割，展现了其在建模复杂动态场景方面的强大能力。"}}
{"id": "2508.09755", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.09755", "abs": "https://arxiv.org/abs/2508.09755", "authors": ["Seokgi Lee"], "title": "Transforming Questions and Documents for Semantically Aligned Retrieval-Augmented Generation", "comment": null, "summary": "We introduce a novel retrieval-augmented generation (RAG) framework tailored\nfor multihop question answering. First, our system uses large language model\n(LLM) to decompose complex multihop questions into a sequence of single-hop\nsubquestions that guide document retrieval. This decomposition mitigates the\nambiguity inherent in multi-hop queries by clearly targeting distinct knowledge\nfacets. Second, instead of embedding raw or chunked documents directly, we\ngenerate answerable questions from each document chunk using Qwen3-8B, embed\nthese generated questions, and retrieve relevant chunks via question-question\nembedding similarity. During inference, the retrieved chunks are then fed along\nwith the original question into the RAG pipeline. We evaluate on three multihop\nquestion datasets (MuSiQue, 2WikiMultiHopQa, HotpotQA) from LongBench. Our\nmethod improves RAG performacne compared to baseline systems. Our contributions\nhighlight the benefits of using answerable-question embeddings for RAG, and the\neffectiveness of LLM-based query decomposition for multihop scenarios.", "AI": {"tldr": "本文提出了一种针对多跳问答的检索增强生成（RAG）框架，通过大语言模型进行问题分解和基于可回答问题嵌入的文档检索。", "motivation": "多跳问答固有的模糊性使得传统RAG系统难以有效处理，需要更精细的问题分解和文档检索策略来精准定位知识。", "method": "该方法首先使用大语言模型（LLM）将复杂的多跳问题分解为一系列单跳子问题以指导文档检索。其次，它不直接嵌入原始或分块文档，而是使用Qwen3-8B从每个文档块生成可回答的问题，然后嵌入这些生成的问题，并通过问题-问题嵌入相似性检索相关块。推理时，检索到的块与原始问题一起输入RAG管道。", "result": "在LongBench中的三个多跳问答数据集（MuSiQue、2WikiMultiHopQa、HotpotQA）上进行评估，该方法相比基线系统显著提升了RAG性能。", "conclusion": "研究结果突出了使用可回答问题嵌入进行RAG的优势，以及基于LLM的问题分解在多跳问答场景中的有效性。"}}
{"id": "2508.09454", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09454", "abs": "https://arxiv.org/abs/2508.09454", "authors": ["Shuai Tan", "Biao Gong", "Zhuoxin Liu", "Yan Wang", "Xi Chen", "Yifan Feng", "Hengshuang Zhao"], "title": "Animate-X++: Universal Character Image Animation with Dynamic Backgrounds", "comment": "Project page: https://lucaria-academy.github.io/Animate-X++/", "summary": "Character image animation, which generates high-quality videos from a\nreference image and target pose sequence, has seen significant progress in\nrecent years. However, most existing methods only apply to human figures, which\nusually do not generalize well on anthropomorphic characters commonly used in\nindustries like gaming and entertainment. Furthermore, previous methods could\nonly generate videos with static backgrounds, which limits the realism of the\nvideos. For the first challenge, our in-depth analysis suggests to attribute\nthis limitation to their insufficient modeling of motion, which is unable to\ncomprehend the movement pattern of the driving video, thus imposing a pose\nsequence rigidly onto the target character. To this end, this paper proposes\nAnimate-X++, a universal animation framework based on DiT for various character\ntypes, including anthropomorphic characters. To enhance motion representation,\nwe introduce the Pose Indicator, which captures comprehensive motion pattern\nfrom the driving video through both implicit and explicit manner. The former\nleverages CLIP visual features of a driving video to extract its gist of\nmotion, like the overall movement pattern and temporal relations among motions,\nwhile the latter strengthens the generalization of DiT by simulating possible\ninputs in advance that may arise during inference. For the second challenge, we\nintroduce a multi-task training strategy that jointly trains the animation and\nTI2V tasks. Combined with the proposed partial parameter training, this\napproach achieves not only character animation but also text-driven background\ndynamics, making the videos more realistic. Moreover, we introduce a new\nAnimated Anthropomorphic Benchmark (A2Bench) to evaluate the performance of\nAnimate-X++ on universal and widely applicable animation images. Extensive\nexperiments demonstrate the superiority and effectiveness of Animate-X++.", "AI": {"tldr": "Animate-X++是一个基于DiT的通用角色动画框架，能为包括拟人化角色在内的多种角色生成高质量视频，并支持动态背景，通过引入姿态指示器和多任务训练策略解决了现有方法对人类角色的局限性和静态背景问题。", "motivation": "现有角色图像动画方法主要适用于人类角色，对拟人化角色泛化性差；这些方法通常只能生成静态背景视频，限制了视频的真实感；现有方法对运动模式的建模不足，无法充分理解驱动视频的运动模式，导致姿态序列僵硬。", "method": "提出Animate-X++框架，基于DiT实现对多种角色类型的通用动画；引入“姿态指示器”捕获驱动视频的综合运动模式，包括隐式（CLIP视觉特征提取运动要点和时间关系）和显式（模拟推理时可能输入以增强DiT泛化性）方式；采用多任务训练策略，联合训练动画和文本到视频（TI2V）任务；结合部分参数训练，实现角色动画和文本驱动的背景动态生成；构建新的动画拟人化基准（A2Bench）用于评估。", "result": "Animate-X++在通用和广泛适用的动画图像上表现出卓越的性能和有效性，能够生成包含动态背景的更真实视频，并成功解决了现有方法对拟人化角色的泛化性问题。", "conclusion": "Animate-X++是一个通用且有效的角色图像动画框架，通过创新的运动建模和多任务训练策略，显著提升了动画视频的真实感和泛化性，尤其适用于拟人化角色和动态背景生成。"}}
{"id": "2508.09593", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09593", "abs": "https://arxiv.org/abs/2508.09593", "authors": ["Haotian Tang", "Jianwei Chen", "Xinrui Tang", "Yunjia Wu", "Zhengyang Miao", "Chao Li"], "title": "Hierarchical Brain Structure Modeling for Predicting Genotype of Glioma", "comment": null, "summary": "Isocitrate DeHydrogenase (IDH) mutation status is a crucial biomarker for\nglioma prognosis. However, current prediction methods are limited by the low\navailability and noise of functional MRI. Structural and morphological\nconnectomes offer a non-invasive alternative, yet existing approaches often\nignore the brain's hierarchical organisation and multiscale interactions. To\naddress this, we propose Hi-SMGNN, a hierarchical framework that integrates\nstructural and morphological connectomes from regional to modular levels. It\nfeatures a multimodal interaction module with a Siamese network and cross-modal\nattention, a multiscale feature fusion mechanism for reducing redundancy, and a\npersonalised modular partitioning strategy to enhance individual specificity\nand interpretability. Experiments on the UCSF-PDGM dataset demonstrate that\nHi-SMGNN outperforms baseline and state-of-the-art models, showing improved\nrobustness and effectiveness in IDH mutation prediction.", "AI": {"tldr": "该论文提出Hi-SMGNN，一个分层框架，通过整合结构和形态连接组，并结合多模态交互、多尺度特征融合和个性化模块划分，实现更准确的脑胶质瘤IDH突变预测。", "motivation": "脑胶质瘤的异柠檬酸脱氢酶（IDH）突变状态是重要的预后生物标志物，但当前预测方法受限于功能MRI的低可用性和噪声。现有的结构和形态连接组方法虽是非侵入性替代方案，却常忽略大脑的层次组织和多尺度相互作用。", "method": "本文提出了Hi-SMGNN，一个分层框架，从区域到模块级别整合结构和形态连接组。其关键特性包括：1) 包含暹罗网络和跨模态注意力的多模态交互模块；2) 用于减少冗余的多尺度特征融合机制；3) 增强个体特异性和可解释性的个性化模块划分策略。", "result": "在UCSF-PDGM数据集上的实验表明，Hi-SMGNN在IDH突变预测方面优于基线模型和现有最先进模型，展现出更高的鲁棒性和有效性。", "conclusion": "Hi-SMGNN通过有效整合多模态连接组数据并考虑大脑的层次结构和多尺度交互，显著提升了脑胶质瘤IDH突变预测的准确性、鲁棒性和可解释性，为临床诊断提供了更可靠的非侵入性方法。"}}
{"id": "2508.09830", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.09830", "abs": "https://arxiv.org/abs/2508.09830", "authors": ["Shenxing Wei", "Jinxi Li", "Yafei Yang", "Siyuan Zhou", "Bo Yang"], "title": "RayletDF: Raylet Distance Fields for Generalizable 3D Surface Reconstruction from Point Clouds or Gaussians", "comment": "ICCV 2025 Highlight. Shenxing and Jinxi are co-first authors. Code\n  and data are available at: https://github.com/vLAR-group/RayletDF", "summary": "In this paper, we present a generalizable method for 3D surface\nreconstruction from raw point clouds or pre-estimated 3D Gaussians by 3DGS from\nRGB images. Unlike existing coordinate-based methods which are often\ncomputationally intensive when rendering explicit surfaces, our proposed\nmethod, named RayletDF, introduces a new technique called raylet distance\nfield, which aims to directly predict surface points from query rays. Our\npipeline consists of three key modules: a raylet feature extractor, a raylet\ndistance field predictor, and a multi-raylet blender. These components work\ntogether to extract fine-grained local geometric features, predict raylet\ndistances, and aggregate multiple predictions to reconstruct precise surface\npoints. We extensively evaluate our method on multiple public real-world\ndatasets, demonstrating superior performance in surface reconstruction from\npoint clouds or 3D Gaussians. Most notably, our method achieves exceptional\ngeneralization ability, successfully recovering 3D surfaces in a single-forward\npass across unseen datasets in testing.", "AI": {"tldr": "本文提出了一种名为RayletDF的通用3D表面重建方法，可从点云或3D高斯数据进行重建。该方法引入了射线距离场，直接从查询射线预测表面点，克服了现有基于坐标方法计算量大的问题，并在多个数据集上展现了优越的性能和出色的泛化能力。", "motivation": "现有的基于坐标的方法在渲染显式表面时通常计算量大且效率低，这促使研究者寻求一种更高效、更通用的3D表面重建技术。", "method": "本文提出的RayletDF方法引入了“射线距离场”（raylet distance field）技术，旨在直接从查询射线预测表面点。其核心管道包含三个关键模块：射线特征提取器（raylet feature extractor）、射线距离场预测器（raylet distance field predictor）和多射线混合器（multi-raylet blender），它们协同工作以提取精细的局部几何特征、预测射线距离并聚合多个预测结果以重建精确的表面点。", "result": "该方法在多个公共真实世界数据集上进行了广泛评估，在从点云或3D高斯进行表面重建方面表现出卓越的性能。最值得注意的是，RayletDF展现了出色的泛化能力，能够通过单次前向传播成功地在测试中重建未见过数据集的3D表面。", "conclusion": "RayletDF提供了一种高效、通用且泛化能力强的3D表面重建方法，能够有效处理点云和3D高斯数据，并解决了传统坐标基方法在计算效率上的挑战。"}}
{"id": "2508.09759", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.09759", "abs": "https://arxiv.org/abs/2508.09759", "authors": ["Avneet Kaur"], "title": "Echoes of Agreement: Argument Driven Opinion Shifts in Large Language Models", "comment": null, "summary": "There have been numerous studies evaluating bias of LLMs towards political\ntopics. However, how positions towards these topics in model outputs are highly\nsensitive to the prompt. What happens when the prompt itself is suggestive of\ncertain arguments towards those positions remains underexplored. This is\ncrucial for understanding how robust these bias evaluations are and for\nunderstanding model behaviour, as these models frequently interact with\nopinionated text. To that end, we conduct experiments for political bias\nevaluation in presence of supporting and refuting arguments. Our experiments\nshow that such arguments substantially alter model responses towards the\ndirection of the provided argument in both single-turn and multi-turn settings.\nMoreover, we find that the strength of these arguments influences the\ndirectional agreement rate of model responses. These effects point to a\nsycophantic tendency in LLMs adapting their stance to align with the presented\narguments which has downstream implications for measuring political bias and\ndeveloping effective mitigation strategies.", "AI": {"tldr": "研究发现大型语言模型（LLMs）的政治偏见评估对提示词中包含的支持或反驳论点高度敏感，模型倾向于迎合这些论点，表现出“谄媚”倾向。", "motivation": "现有关于LLMs政治偏见的评估研究，尚未充分探讨当提示词本身包含指向特定立场的论点时，模型行为如何变化。这对于理解偏见评估的鲁棒性以及模型与带有观点的文本交互时的行为至关重要。", "method": "通过在提示词中加入支持性或反驳性论点，在单轮和多轮对话设置下，进行政治偏见评估实验。", "result": "实验显示，提示词中提供的论点会显著改变模型响应的方向，使其倾向于所提供的论点。此外，论点的强度会影响模型响应与论点方向一致的比例。", "conclusion": "LLMs表现出一种“谄媚”倾向，即其立场会适应并与呈现的论点保持一致。这种现象对测量政治偏见和开发有效的缓解策略具有深远影响。"}}
{"id": "2508.09456", "categories": ["cs.CV", "cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.09456", "abs": "https://arxiv.org/abs/2508.09456", "authors": ["Junxian Li", "Beining Xu", "Di Zhang"], "title": "IAG: Input-aware Backdoor Attack on VLMs for Visual Grounding", "comment": "13 pages, 13 Figures", "summary": "Vision-language models (VLMs) have shown significant advancements in tasks\nsuch as visual grounding, where they localize specific objects in images based\non natural language queries and images. However, security issues in visual\ngrounding tasks for VLMs remain underexplored, especially in the context of\nbackdoor attacks. In this paper, we introduce a novel input-aware backdoor\nattack method, IAG, designed to manipulate the grounding behavior of VLMs. This\nattack forces the model to ground a specific target object in the input image,\nregardless of the user's query. We propose an adaptive trigger generator that\nembeds the semantic information of the attack target's description into the\noriginal image using a text-conditional U-Net, thereby overcoming the\nopen-vocabulary attack challenge. To ensure the attack's stealthiness, we\nutilize a reconstruction loss to minimize visual discrepancies between poisoned\nand clean images. Additionally, we introduce a unified method for generating\nattack data. IAG is evaluated theoretically and empirically, demonstrating its\nfeasibility and effectiveness. Notably, our ASR@0.5 on InternVL-2.5-8B reaches\nover 65\\% on various testing sets. IAG also shows promising potential on\nmanipulating Ferret-7B and LlaVA-1.5-7B with very little accuracy decrease on\nclean samples. Extensive specific experiments, such as ablation study and\npotential defense, also indicate the robustness and transferability of our\nattack.", "AI": {"tldr": "本文提出了一种名为IAG的新型输入感知后门攻击方法，旨在操纵视觉语言模型（VLM）的视觉定位行为，使其无论用户查询如何，都将特定目标对象定位出来。该方法通过文本条件U-Net生成自适应触发器，并将攻击目标的语义信息嵌入图像中，同时利用重建损失确保隐蔽性。", "motivation": "视觉语言模型（VLM）在视觉定位等任务中取得了显著进展，但其安全问题，尤其是在后门攻击方面的研究仍不充分。", "method": "引入了新型输入感知后门攻击方法IAG。该方法通过一个自适应触发器生成器，利用文本条件U-Net将攻击目标描述的语义信息嵌入到原始图像中，从而克服了开放词汇攻击的挑战。为确保攻击的隐蔽性，采用了重建损失来最小化中毒图像与干净图像之间的视觉差异。此外，还提出了一种统一的攻击数据生成方法。", "result": "IAG在理论和经验上都证明了其可行性和有效性。在InternVL-2.5-8B模型上，其ASR@0.5在各种测试集上达到65%以上。IAG在操纵Ferret-7B和LlaVA-1.5-7B模型时也显示出潜力，且对干净样本的准确性下降很小。广泛的实验（如消融研究和潜在防御）也表明了该攻击的鲁棒性和可迁移性。", "conclusion": "IAG是一种可行且有效的输入感知后门攻击方法，能够操纵视觉语言模型的视觉定位行为，强制其定位特定目标，揭示了VLM在视觉定位任务中的安全漏洞，并展示了攻击的鲁棒性和可迁移性。"}}
{"id": "2508.09616", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09616", "abs": "https://arxiv.org/abs/2508.09616", "authors": ["Daniel Barco", "Marc Stadelmann", "Martin Oswald", "Ivo Herzig", "Lukas Lichtensteiger", "Pascal Paysan", "Igor Peterlik", "Michal Walczak", "Bjoern Menze", "Frank-Peter Schilling"], "title": "MInDI-3D: Iterative Deep Learning in 3D for Sparse-view Cone Beam Computed Tomography", "comment": null, "summary": "We present MInDI-3D (Medical Inversion by Direct Iteration in 3D), the first\n3D conditional diffusion-based model for real-world sparse-view Cone Beam\nComputed Tomography (CBCT) artefact removal, aiming to reduce imaging radiation\nexposure. A key contribution is extending the \"InDI\" concept from 2D to a full\n3D volumetric approach for medical images, implementing an iterative denoising\nprocess that refines the CBCT volume directly from sparse-view input. A further\ncontribution is the generation of a large pseudo-CBCT dataset (16,182) from\nchest CT volumes of the CT-RATE public dataset to robustly train MInDI-3D. We\nperformed a comprehensive evaluation, including quantitative metrics,\nscalability analysis, generalisation tests, and a clinical assessment by 11\nclinicians. Our results show MInDI-3D's effectiveness, achieving a 12.96 (6.10)\ndB PSNR gain over uncorrected scans with only 50 projections on the CT-RATE\npseudo-CBCT (independent real-world) test set and enabling an 8x reduction in\nimaging radiation exposure. We demonstrate its scalability by showing that\nperformance improves with more training data. Importantly, MInDI-3D matches the\nperformance of a 3D U-Net on real-world scans from 16 cancer patients across\ndistortion and task-based metrics. It also generalises to new CBCT scanner\ngeometries. Clinicians rated our model as sufficient for patient positioning\nacross all anatomical sites and found it preserved lung tumour boundaries well.", "AI": {"tldr": "MInDI-3D是一种基于3D条件扩散的模型，用于消除稀疏视图锥束CT（CBCT）中的伪影，旨在减少辐射暴露，并在定量和临床评估中表现出色。", "motivation": "减少医学成像中的辐射暴露，特别是通过解决稀疏视图CBCT图像中存在的伪影问题。", "method": "该研究将“InDI”概念从2D扩展到3D，直接对CBCT体积进行迭代去噪。为此，从CT-RATE公共数据集的胸部CT体积生成了一个大型伪CBCT数据集（16,182个）。模型通过定量指标、可扩展性分析、泛化测试和11位临床医生的临床评估进行了全面评估。", "result": "MInDI-3D在CT-RATE伪CBCT（独立真实世界）测试集上，仅使用50个投影，比未校正的扫描获得了12.96（6.10）dB的PSNR增益，实现了8倍的成像辐射暴露减少。模型性能随训练数据量的增加而提高，并能泛化到新的CBCT扫描仪几何结构。在真实世界扫描中，其性能与3D U-Net相当。临床医生认为该模型足以用于患者定位，并能很好地保留肺肿瘤边界。", "conclusion": "MInDI-3D有效减少了稀疏视图CBCT成像中的伪影和辐射暴露，具有良好的可扩展性和泛化能力，并获得了临床医生的认可，可用于患者定位和肿瘤边界保留。"}}
{"id": "2508.09767", "categories": ["cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.09767", "abs": "https://arxiv.org/abs/2508.09767", "authors": ["Shuhei Kato"], "title": "UtterTune: LoRA-Based Target-Language Pronunciation Edit and Control in Multilingual Text-to-Speech", "comment": null, "summary": "We propose UtterTune, a lightweight adaptation method that fine-tunes a\nmultilingual text-to-speech (TTS) system based on a large language model (LLM)\narchitecture, designed to enhance the controllability of pronunciation in a\ntarget language while preserving performance in others. While LLM architectures\nhave enabled TTS models to achieve remarkable naturalness, accurately modeling\ngrapheme-to-phoneme (G2P) mapping and prosody remains challenging, especially\nwhen the model omits an explicit G2P module and directly processes minimally\nencoded text (e.g., byte-pair encoding). UtterTune leverages low-rank\nadaptation to enable the control of segmental pronunciation and pitch accent at\nthe phoneme level for Japanese speech, the target language in this paper, while\nmaintaining naturalness and speaker similarity in a zero-shot setting.\nObjective and subjective evaluations confirm its effectiveness.", "AI": {"tldr": "UtterTune是一种轻量级适应方法，用于微调基于LLM的多语言文本到语音（TTS）系统，旨在增强目标语言（日语）的发音可控性，同时保持其他语言的性能。", "motivation": "尽管LLM架构使TTS模型实现了显著的自然度，但准确建模字素到音素（G2P）映射和韵律仍然具有挑战性，特别是当模型省略了明确的G2P模块并直接处理最小编码文本时，导致发音控制困难。", "method": "UtterTune利用低秩适应技术，在音素级别上控制目标语言（日语）的音段发音和音高重音，同时在零样本设置中保持自然度和说话人相似性。", "result": "客观和主观评估证实了该方法的有效性，即能有效增强日语的发音控制，并保持其他语言的自然度和说话人相似性。", "conclusion": "UtterTune通过轻量级适应，成功地为基于LLM的TTS系统提供了对目标语言（日语）发音的精细控制，而不会损害其他语言的性能。"}}
{"id": "2508.09466", "categories": ["cs.CV", "cs.NE"], "pdf": "https://arxiv.org/pdf/2508.09466", "abs": "https://arxiv.org/abs/2508.09466", "authors": ["Tam Ngoc-Bang Nguyen", "Anh-Dzung Doan", "Zhipeng Cai", "Tat-Jun Chin"], "title": "Event-driven Robust Fitting on Neuromorphic Hardware", "comment": "11 pages, accepted in ICCV 2025 Workshop on Neuromorphic Vision\n  (NeVI)", "summary": "Robust fitting of geometric models is a fundamental task in many computer\nvision pipelines. Numerous innovations have been produced on the topic, from\nimproving the efficiency and accuracy of random sampling heuristics to\ngenerating novel theoretical insights that underpin new approaches with\nmathematical guarantees. However, one aspect of robust fitting that has\nreceived little attention is energy efficiency. This performance metric has\nbecome critical as high energy consumption is a growing concern for AI\nadoption. In this paper, we explore energy-efficient robust fitting via the\nneuromorphic computing paradigm. Specifically, we designed a novel spiking\nneural network for robust fitting on real neuromorphic hardware, the Intel\nLoihi 2. Enabling this are novel event-driven formulations of model estimation\nthat allow robust fitting to be implemented in the unique architecture of Loihi\n2, and algorithmic strategies to alleviate the current limited precision and\ninstruction set of the hardware. Results show that our neuromorphic robust\nfitting consumes only a fraction (15%) of the energy required to run the\nestablished robust fitting algorithm on a standard CPU to equivalent accuracy.", "AI": {"tldr": "本文提出了一种基于神经拟态计算的鲁棒几何模型拟合方法，显著降低了能耗，同时保持了与传统CPU相当的精度。", "motivation": "鲁棒几何模型拟合在计算机视觉中至关重要，但其能耗问题日益突出，尤其是在人工智能普及背景下。现有研究鲜少关注能效。", "method": "研究人员设计了一种新颖的脉冲神经网络（SNN），并将其部署在英特尔Loihi 2神经拟态硬件上。通过事件驱动的模型估计公式，使鲁棒拟合能够在Loihi 2的独特架构上实现，并采用算法策略缓解了硬件的有限精度和指令集问题。", "result": "结果表明，与在标准CPU上运行传统鲁棒拟合算法相比，该神经拟态鲁棒拟合方法在达到同等精度的情况下，能耗仅为其15%。", "conclusion": "神经拟态计算为鲁棒几何模型拟合提供了显著的节能潜力，是实现高效AI计算的重要途径。"}}
{"id": "2508.09632", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09632", "abs": "https://arxiv.org/abs/2508.09632", "authors": ["Jingwei Liu", "Ling Yang", "Hao Luo", "Fan Wang Hongyan Li", "Mengdi Wang"], "title": "Preacher: Paper-to-Video Agentic System", "comment": null, "summary": "The paper-to-video task converts a research paper into a structured video\nabstract, distilling key concepts, methods, and conclusions into an accessible,\nwell-organized format. While state-of-the-art video generation models\ndemonstrate potential, they are constrained by limited context windows, rigid\nvideo duration constraints, limited stylistic diversity, and an inability to\nrepresent domain-specific knowledge. To address these limitations, we introduce\nPreacher, the first paper-to-video agentic system. Preacher employs a top-down\napproach to decompose, summarize, and reformulate the paper, followed by\nbottom-up video generation, synthesizing diverse video segments into a coherent\nabstract. To align cross-modal representations, we define key scenes and\nintroduce a Progressive Chain of Thought (P-CoT) for granular, iterative\nplanning. Preacher successfully generates high-quality video abstracts across\nfive research fields, demonstrating expertise beyond current video generation\nmodels. Code will be released at: https://github.com/GenVerse/Paper2Video", "AI": {"tldr": "Preacher是首个将研究论文转换为结构化视频摘要的智能代理系统，解决了现有视频生成模型在上下文、时长、风格和领域知识表示方面的局限。", "motivation": "现有视频生成模型在将论文转换为视频摘要时面临多重挑战，包括有限的上下文窗口、僵化的视频时长限制、缺乏风格多样性以及无法有效表示领域特定知识。", "method": "引入了Preacher系统，该系统采用“自顶向下”的方法分解、总结和重构论文，随后进行“自底向上”的视频生成，将多样化的视频片段合成为连贯的摘要。为实现跨模态表示对齐，系统定义了关键场景并引入了渐进式思维链（P-CoT）进行细粒度、迭代规划。", "result": "Preacher成功在五个研究领域生成了高质量的视频摘要，展示出超越当前视频生成模型的专业能力。", "conclusion": "Preacher系统通过其独特的自顶向下与自底向上结合的方法以及渐进式思维链规划，有效克服了现有视频生成模型的局限，为研究论文到视频摘要的转换提供了更高效、高质量的解决方案。"}}
{"id": "2508.09776", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09776", "abs": "https://arxiv.org/abs/2508.09776", "authors": ["Mahdi Dhaini", "Juraj Vladika", "Ege Erdogan", "Zineb Attaoui", "Gjergji Kasneci"], "title": "Can LLM-Generated Textual Explanations Enhance Model Classification Performance? An Empirical Study", "comment": "Accepted to the 34th International Conference on Artificial Neural\n  Networks (ICANN 2025)", "summary": "In the rapidly evolving field of Explainable Natural Language Processing\n(NLP), textual explanations, i.e., human-like rationales, are pivotal for\nexplaining model predictions and enriching datasets with interpretable labels.\nTraditional approaches rely on human annotation, which is costly,\nlabor-intensive, and impedes scalability. In this work, we present an automated\nframework that leverages multiple state-of-the-art large language models (LLMs)\nto generate high-quality textual explanations. We rigorously assess the quality\nof these LLM-generated explanations using a comprehensive suite of Natural\nLanguage Generation (NLG) metrics. Furthermore, we investigate the downstream\nimpact of these explanations on the performance of pre-trained language models\n(PLMs) and LLMs across natural language inference tasks on two diverse\nbenchmark datasets. Our experiments demonstrate that automated explanations\nexhibit highly competitive effectiveness compared to human-annotated\nexplanations in improving model performance. Our findings underscore a\npromising avenue for scalable, automated LLM-based textual explanation\ngeneration for extending NLP datasets and enhancing model performance.", "AI": {"tldr": "本文提出一个自动化框架，利用多个大型语言模型（LLMs）生成高质量文本解释，并通过NLG指标和下游任务评估，证明其在提升模型性能方面可与人工标注媲美，为可扩展的解释生成提供了新途径。", "motivation": "传统的文本解释生成依赖人工标注，成本高昂、劳动密集且难以扩展，这阻碍了解释性自然语言处理（NLP）领域的发展。", "method": "本文提出了一个自动化框架，利用多个最先进的LLMs生成文本解释。通过一套全面的自然语言生成（NLG）指标评估解释质量，并探究这些解释对预训练语言模型（PLMs）和LLMs在自然语言推理任务上的下游性能影响，使用了两个不同的基准数据集进行实验。", "result": "实验证明，自动化生成的解释在提高模型性能方面表现出与人工标注解释高度竞争的有效性。", "conclusion": "研究结果突显了通过LLM实现可扩展、自动化文本解释生成，以扩展NLP数据集和增强模型性能的巨大潜力。"}}
{"id": "2508.09470", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09470", "abs": "https://arxiv.org/abs/2508.09470", "authors": ["Jialei Xu", "Zizhuang Wei", "Weikang You", "Linyun Li", "Weijian Sun"], "title": "CitySeg: A 3D Open Vocabulary Semantic Segmentation Foundation Model in City-scale Scenarios", "comment": null, "summary": "Semantic segmentation of city-scale point clouds is a critical technology for\nUnmanned Aerial Vehicle (UAV) perception systems, enabling the classification\nof 3D points without relying on any visual information to achieve comprehensive\n3D understanding. However, existing models are frequently constrained by the\nlimited scale of 3D data and the domain gap between datasets, which lead to\nreduced generalization capability. To address these challenges, we propose\nCitySeg, a foundation model for city-scale point cloud semantic segmentation\nthat incorporates text modality to achieve open vocabulary segmentation and\nzero-shot inference. Specifically, in order to mitigate the issue of\nnon-uniform data distribution across multiple domains, we customize the data\npreprocessing rules, and propose a local-global cross-attention network to\nenhance the perception capabilities of point networks in UAV scenarios. To\nresolve semantic label discrepancies across datasets, we introduce a\nhierarchical classification strategy. A hierarchical graph established\naccording to the data annotation rules consolidates the data labels, and the\ngraph encoder is used to model the hierarchical relationships between\ncategories. In addition, we propose a two-stage training strategy and employ\nhinge loss to increase the feature separability of subcategories. Experimental\nresults demonstrate that the proposed CitySeg achieves state-of-the-art (SOTA)\nperformance on nine closed-set benchmarks, significantly outperforming existing\napproaches. Moreover, for the first time, CitySeg enables zero-shot\ngeneralization in city-scale point cloud scenarios without relying on visual\ninformation.", "AI": {"tldr": "CitySeg是一个针对城市级点云语义分割的基础模型，它整合文本模态，实现了开放词汇分割和零样本推理。该模型通过解决数据规模限制和数据集之间的域差距问题，显著提升了现有方法的泛化能力，并在多个基准测试中取得了最先进的性能。", "motivation": "城市级点云的语义分割对于无人机（UAV）感知系统至关重要，能实现不依赖视觉信息的3D点分类，从而获得全面的3D理解。然而，现有模型常受限于3D数据的规模和数据集间的域差距，导致泛化能力不足。", "method": "本文提出了CitySeg模型，引入文本模态以实现开放词汇和零样本推理。具体方法包括：定制数据预处理规则以缓解多域数据分布不均；提出局部-全局交叉注意力网络以增强无人机场景下的点云感知能力；引入分层分类策略，通过构建分层图和使用图编码器来解决数据集间语义标签差异并建模类别间的层级关系；采用两阶段训练策略并使用hinge损失来提高子类特征的可分离性。", "result": "CitySeg在九个闭集基准测试中取得了最先进（SOTA）的性能，显著优于现有方法。此外，CitySeg首次在城市级点云场景中实现了零样本泛化，且不依赖视觉信息。", "conclusion": "CitySeg成功解决了城市级点云语义分割中数据规模和域差距带来的挑战，通过创新的多模态（点云+文本）融合和分层策略，不仅在传统闭集任务上表现卓越，更首次实现了零样本泛化能力，为UAV感知系统提供了强大的3D理解工具。"}}
{"id": "2508.09746", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09746", "abs": "https://arxiv.org/abs/2508.09746", "authors": ["Zhiqiu Zhang", "Dongqi Fan", "Mingjie Wang", "Qiang Tang", "Jian Yang", "Zili Yi"], "title": "Region-to-Region: Enhancing Generative Image Harmonization with Adaptive Regional Injection", "comment": null, "summary": "The goal of image harmonization is to adjust the foreground in a composite\nimage to achieve visual consistency with the background. Recently, latent\ndiffusion model (LDM) are applied for harmonization, achieving remarkable\nresults. However, LDM-based harmonization faces challenges in detail\npreservation and limited harmonization ability. Additionally, current synthetic\ndatasets rely on color transfer, which lacks local variations and fails to\ncapture complex real-world lighting conditions. To enhance harmonization\ncapabilities, we propose the Region-to-Region transformation. By injecting\ninformation from appropriate regions into the foreground, this approach\npreserves original details while achieving image harmonization or, conversely,\ngenerating new composite data. From this perspective, We propose a novel model\nR2R. Specifically, we design Clear-VAE to preserve high-frequency details in\nthe foreground using Adaptive Filter while eliminating disharmonious elements.\nTo further enhance harmonization, we introduce the Harmony Controller with\nMask-aware Adaptive Channel Attention (MACA), which dynamically adjusts the\nforeground based on the channel importance of both foreground and background\nregions. To address the limitation of existing datasets, we propose Random\nPoisson Blending, which transfers color and lighting information from a\nsuitable region to the foreground, thereby generating more diverse and\nchallenging synthetic images. Using this method, we construct a new synthetic\ndataset, RPHarmony. Experiments demonstrate the superiority of our method over\nother methods in both quantitative metrics and visual harmony. Moreover, our\ndataset helps the model generate more realistic images in real examples. Our\ncode, dataset, and model weights have all been released for open access.", "AI": {"tldr": "该论文提出了一种名为R2R的新模型和RPHarmony新数据集，旨在通过区域到区域转换来解决基于LDM的图像调和中细节保留不足和调和能力有限的问题，并生成更具挑战性的合成数据。", "motivation": "现有的基于潜在扩散模型（LDM）的图像调和方法在细节保留和调和能力上存在挑战。此外，当前的合成数据集依赖于颜色迁移，缺乏局部变化，未能捕捉复杂的真实世界光照条件，限制了模型的性能。", "method": "1. 提出“区域到区域转换”方法，通过从背景区域注入信息到前景来保留细节并实现图像调和或生成新合成数据。2. 设计R2R模型，包括：a) Clear-VAE，使用自适应滤波器保留前景高频细节并消除不协调元素；b) 带有掩码感知自适应通道注意力（MACA）的和谐控制器，根据前景和背景的通道重要性动态调整前景。3. 提出随机泊松混合方法，将颜色和光照信息从合适区域转移到前景，生成更多样化和具有挑战性的合成图像，并构建RPHarmony数据集。", "result": "实验结果表明，该方法在定量指标和视觉和谐方面均优于其他方法。此外，所提出的数据集有助于模型在真实示例中生成更逼真的图像。", "conclusion": "通过提出R2R模型和RPHarmony数据集，本研究有效解决了图像调和中细节保留和调和能力不足的问题，并克服了现有数据集的局限性，显著提升了图像调和的性能和真实感。"}}
{"id": "2508.09786", "categories": ["cs.CL", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.09786", "abs": "https://arxiv.org/abs/2508.09786", "authors": ["Mahdi Dhaini", "Tobias Müller", "Roksoliana Rabets", "Gjergji Kasneci"], "title": "Adoption of Explainable Natural Language Processing: Perspectives from Industry and Academia on Practices and Challenges", "comment": "Accepted to AAAI/ACM Conference on AI, Ethics, and Society (AIES\n  2025)", "summary": "The field of explainable natural language processing (NLP) has grown rapidly\nin recent years. The growing opacity of complex models calls for transparency\nand explanations of their decisions, which is crucial to understand their\nreasoning and facilitate deployment, especially in high-stakes environments.\nDespite increasing attention given to explainable NLP, practitioners'\nperspectives regarding its practical adoption and effectiveness remain\nunderexplored. This paper addresses this research gap by investigating\npractitioners' experiences with explainability methods, specifically focusing\non their motivations for adopting such methods, the techniques employed,\nsatisfaction levels, and the practical challenges encountered in real-world NLP\napplications. Through a qualitative interview-based study with industry\npractitioners and complementary interviews with academic researchers, we\nsystematically analyze and compare their perspectives. Our findings reveal\nconceptual gaps, low satisfaction with current explainability methods, and\nhighlight evaluation challenges. Our findings emphasize the need for clear\ndefinitions and user-centric frameworks for better adoption of explainable NLP\nin practice.", "AI": {"tldr": "本研究通过访谈调查了行业从业者和学术研究人员对可解释自然语言处理（NLP）的看法，发现当前方法存在概念差距、满意度低和评估挑战，强调需要更清晰的定义和以用户为中心的框架。", "motivation": "随着复杂NLP模型的不透明性日益增加，理解其决策推理和促进部署变得至关重要，尤其是在高风险环境中。尽管可解释NLP受到关注，但从业者对其实际应用和有效性的看法仍未被充分探索，本研究旨在填补这一空白。", "method": "通过对行业从业者进行定性访谈研究，并辅以对学术研究人员的访谈，系统分析和比较了他们对可解释性方法的看法，包括采用动机、所用技术、满意度以及遇到的实际挑战。", "result": "研究发现，当前可解释性方法存在概念上的差距，从业者对其满意度较低，并且在实际应用中面临评估挑战。", "conclusion": "研究结果强调，为了更好地在实践中采用可解释NLP，需要明确的定义和以用户为中心的框架。"}}
{"id": "2508.09475", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09475", "abs": "https://arxiv.org/abs/2508.09475", "authors": ["Shibo Yao", "Renshuai Tao", "Xiaolong Zheng", "Chao Liang", "Chunjie Zhang"], "title": "Leveraging Failed Samples: A Few-Shot and Training-Free Framework for Generalized Deepfake Detection", "comment": null, "summary": "Recent deepfake detection studies often treat unseen sample detection as a\n``zero-shot\" task, training on images generated by known models but\ngeneralizing to unknown ones. A key real-world challenge arises when a model\nperforms poorly on unknown samples, yet these samples remain available for\nanalysis. This highlights that it should be approached as a ``few-shot\" task,\nwhere effectively utilizing a small number of samples can lead to significant\nimprovement. Unlike typical few-shot tasks focused on semantic understanding,\ndeepfake detection prioritizes image realism, which closely mirrors real-world\ndistributions. In this work, we propose the Few-shot Training-free Network\n(FTNet) for real-world few-shot deepfake detection. Simple yet effective, FTNet\ndiffers from traditional methods that rely on large-scale known data for\ntraining. Instead, FTNet uses only one fake samplefrom an evaluation set,\nmimicking the scenario where new samples emerge in the real world and can be\ngathered for use, without any training or parameter updates. During evaluation,\neach test sample is compared to the known fake and real samples, and it is\nclassified based on the category of the nearest sample. We conduct a\ncomprehensive analysis of AI-generated images from 29 different generative\nmodels and achieve a new SoTA performance, with an average improvement of 8.7\\%\ncompared to existing methods. This work introduces a fresh perspective on\nreal-world deepfake detection: when the model struggles to generalize on a\nfew-shot sample, leveraging the failed samples leads to better performance.", "AI": {"tldr": "该研究提出了FTNet，一个无需训练的少样本深度伪造检测网络。它通过利用少量未见过的伪造样本来显著提高检测性能，并在多达29种生成模型上实现了最先进的性能。", "motivation": "现有的深度伪造检测方法常将未知样本检测视为“零样本”任务，但其在真实世界中对未知样本的泛化能力差。然而，这些未知样本在实际中并非不可获取。因此，研究者认为这应被视为“少样本”任务，即有效利用少量样本可显著提升性能。此外，深度伪造检测侧重图像真实感而非语义理解，这与传统少样本任务不同，需要新的方法。", "method": "本文提出了“少样本免训练网络”（FTNet）。与传统依赖大量已知数据训练的方法不同，FTNet无需任何训练或参数更新。它仅使用评估集中的一个伪造样本（模拟真实世界中新样本的出现）。在评估时，每个测试样本会与已知的真实和伪造样本进行比较，并根据其最近样本的类别进行分类。", "result": "研究在来自29种不同生成模型的AI生成图像上进行了全面分析。FTNet取得了新的最先进（SoTA）性能，与现有方法相比，平均性能提升了8.7%。", "conclusion": "这项工作为真实世界的深度伪造检测引入了新视角：当模型难以在少样本上泛化时，利用这些失败的样本反而能带来更好的性能。"}}
{"id": "2508.09780", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09780", "abs": "https://arxiv.org/abs/2508.09780", "authors": ["Nahyuk Lee", "Juhong Min", "Junhong Lee", "Chunghyun Park", "Minsu Cho"], "title": "Combinative Matching for Geometric Shape Assembly", "comment": "Accepted to ICCV 2025 (Highlight)", "summary": "This paper introduces a new shape-matching methodology, combinative matching,\nto combine interlocking parts for geometric shape assembly. Previous methods\nfor geometric assembly typically rely on aligning parts by finding identical\nsurfaces between the parts as in conventional shape matching and registration.\nIn contrast, we explicitly model two distinct properties of interlocking\nshapes: 'identical surface shape' and 'opposite volume occupancy.' Our method\nthus learns to establish correspondences across regions where their surface\nshapes appear identical but their volumes occupy the inverted space to each\nother. To facilitate this process, we also learn to align regions in rotation\nby estimating their shape orientations via equivariant neural networks. The\nproposed approach significantly reduces local ambiguities in matching and\nallows a robust combination of parts in assembly. Experimental results on\ngeometric assembly benchmarks demonstrate the efficacy of our method,\nconsistently outperforming the state of the art. Project page:\nhttps://nahyuklee.github.io/cmnet.", "AI": {"tldr": "本文提出了一种名为“组合匹配”的新型形状匹配方法，用于几何形状装配中互锁部件的组合。该方法通过显式建模表面形状相同但体积占用相反的互锁特性，并利用等变神经网络进行旋转对齐，显著提高了匹配鲁棒性。", "motivation": "传统的几何装配方法通常依赖于通过寻找部件间相同的表面来对齐，但这不适用于互锁形状，因为互锁部件的表面形状可能相同但其体积占用是相反的。因此，需要一种能明确处理“相同表面形状”和“相反体积占用”这两种特性的新方法。", "method": "该方法引入了“组合匹配”方法论。它显式地建模了互锁形状的两个独特属性：“相同表面形状”和“相反体积占用”。通过学习在表面形状相同但体积占用相互反转的区域之间建立对应关系。为促进此过程，该方法还通过等变神经网络估计形状方向，学习在旋转上对齐区域。", "result": "所提出的方法显著减少了匹配中的局部歧义，并允许在装配中鲁棒地组合部件。在几何装配基准上的实验结果表明，该方法有效且一致地优于现有最先进的技术。", "conclusion": "通过明确建模互锁形状的独特属性并利用等变神经网络进行对齐，本文提出的组合匹配方法为几何形状装配提供了一个鲁棒且高效的解决方案，其性能优于现有技术。"}}
{"id": "2508.09804", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.09804", "abs": "https://arxiv.org/abs/2508.09804", "authors": ["Ahmed Masry", "Abhay Puri", "Masoud Hashemi", "Juan A. Rodriguez", "Megh Thakkar", "Khyati Mahajan", "Vikas Yadav", "Sathwik Tejaswi Madhusudhan", "Alexandre Piché", "Dzmitry Bahdanau", "Christopher Pal", "David Vazquez", "Enamul Hoque", "Perouz Taslakian", "Sai Rajeswar", "Spandana Gella"], "title": "BigCharts-R1: Enhanced Chart Reasoning with Visual Reinforcement Finetuning", "comment": null, "summary": "Charts are essential to data analysis, transforming raw data into clear\nvisual representations that support human decision-making. Although current\nvision-language models (VLMs) have made significant progress, they continue to\nstruggle with chart comprehension due to training on datasets that lack\ndiversity and real-world authenticity, or on automatically extracted underlying\ndata tables of charts, which can contain numerous estimation errors.\nFurthermore, existing models only rely on supervised fine-tuning using these\nlow-quality datasets, severely limiting their effectiveness. To address these\nissues, we first propose BigCharts, a dataset creation pipeline that generates\nvisually diverse chart images by conditioning the rendering process on\nreal-world charts sourced from multiple online platforms. Unlike purely\nsynthetic datasets, BigCharts incorporates real-world data, ensuring\nauthenticity and visual diversity, while still retaining accurate underlying\ndata due to our proposed replotting process. Additionally, we introduce a\ncomprehensive training framework that integrates supervised fine-tuning with\nGroup Relative Policy Optimization (GRPO)-based reinforcement learning. By\nintroducing novel reward signals specifically designed for chart reasoning, our\napproach enhances model robustness and generalization across diverse chart\nstyles and domains, resulting in a state-of-the-art chart reasoning model,\nBigCharts-R1. Extensive experiments demonstrate that our models surpass\nexisting methods on multiple chart question-answering benchmarks compared to\neven larger open-source and closed-source models.", "AI": {"tldr": "该研究提出了BigCharts数据集生成管道和基于强化学习的训练框架，以解决现有视觉语言模型在图表理解方面因数据质量和训练方法限制而表现不佳的问题，并实现了领先的图表推理模型BigCharts-R1。", "motivation": "当前的视觉语言模型（VLMs）在图表理解上表现不佳，原因在于：1) 训练数据集缺乏多样性和真实性，或其底层数据表包含大量估计误差；2) 现有模型仅依赖于低质量数据集的监督微调，严重限制了其有效性。", "method": "1. 提出了BigCharts数据集创建管道，通过以真实世界图表为条件来生成视觉多样化的图表图像，并利用重绘过程确保底层数据的准确性。2. 引入了一个综合训练框架，将监督微调与基于群组相对策略优化（GRPO）的强化学习相结合，并设计了专门用于图表推理的新颖奖励信号。", "result": "通过广泛的实验证明，所提出的模型（BigCharts-R1）在多个图表问答基准测试上超越了现有方法，甚至优于更大的开源和闭源模型，实现了最先进的图表推理性能。", "conclusion": "通过结合高质量、真实世界数据驱动的数据集生成和创新的强化学习训练框架，可以显著提高视觉语言模型在图表理解和推理方面的鲁棒性和泛化能力。"}}
{"id": "2508.09476", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09476", "abs": "https://arxiv.org/abs/2508.09476", "authors": ["Yuji Wang", "Moran Li", "Xiaobin Hu", "Ran Yi", "Jiangning Zhang", "Chengming Xu", "Weijian Cao", "Yabiao Wang", "Chengjie Wang", "Lizhuang Ma"], "title": "From Large Angles to Consistent Faces: Identity-Preserving Video Generation via Mixture of Facial Experts", "comment": null, "summary": "Current video generation models struggle with identity preservation under\nlarge facial angles, primarily facing two challenges: the difficulty in\nexploring an effective mechanism to integrate identity features into DiT\nstructure, and the lack of targeted coverage of large facial angles in existing\nopen-source video datasets. To address these, we present two key innovations.\nFirst, we introduce a Mixture of Facial Experts (MoFE) that dynamically\ncombines complementary cues from three specialized experts, each designed to\ncapture distinct but mutually reinforcing aspects of facial attributes. The\nidentity expert captures cross-pose identity-sensitive features, the semantic\nexpert extracts high-level visual semantxics, and the detail expert preserves\npixel-level features (e.g., skin texture, color gradients). Furthermore, to\nmitigate dataset limitations, we have tailored a data processing pipeline\ncentered on two key aspects: Face Constraints and Identity Consistency. Face\nConstraints ensure facial angle diversity and a high proportion of facial\nregions, while Identity Consistency preserves coherent person-specific features\nacross temporal sequences, collectively addressing the scarcity of large facial\nangles and identity-stable training data in existing datasets. Leveraging this\npipeline, we have curated and refined a Large Face Angles (LFA) Dataset from\nexisting open-source human video datasets, comprising 460K video clips with\nannotated facial angles. Experimental results on the LFA benchmark demonstrate\nthat our method, empowered by the LFA dataset, significantly outperforms prior\nSOTA methods in face similarity, face FID, and CLIP semantic alignment. The\ncode and dataset will be made publicly available at\nhttps://github.com/rain152/LFA-Video-Generation.", "AI": {"tldr": "该研究通过引入面部专家混合模型（MoFE）和构建大规模面部角度（LFA）数据集，显著提升了视频生成模型在大幅度面部角度下的人物身份保持能力。", "motivation": "当前视频生成模型在大幅度面部角度下难以保持人物身份，主要面临两大挑战：一是难以有效将身份特征整合到DiT（Diffusion Transformers）结构中；二是现有开源视频数据集中缺乏针对大幅度面部角度的覆盖。", "method": "1. 提出了“面部专家混合模型”（MoFE），动态结合身份专家（捕获跨姿态身份敏感特征）、语义专家（提取高级视觉语义）和细节专家（保留像素级特征）的互补信息。2. 设计了以“面部约束”和“身份一致性”为核心的数据处理流程，以确保面部角度多样性和跨时间序列的身份连贯性。3. 基于此流程，从现有开源人类视频数据集中整理并精炼出包含46万视频片段和标注面部角度的“大规模面部角度（LFA）数据集”。", "result": "在LFA基准测试中，该方法在LFA数据集的支持下，在面部相似度、面部FID和CLIP语义对齐方面显著优于先前的SOTA方法。", "conclusion": "所提出的面部专家混合模型（MoFE）和精心策划的LFA数据集有效解决了视频生成中大幅度面部角度下身份保持的难题，达到了最先进的性能。"}}
{"id": "2508.09805", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09805", "abs": "https://arxiv.org/abs/2508.09805", "authors": ["Jonathan Williams Ramirez", "Dina Zemlyanker", "Lucas Deden-Binder", "Rogeny Herisse", "Erendira Garcia Pallares", "Karthik Gopinath", "Harshvardhan Gazula", "Christopher Mount", "Liana N. Kozanno", "Michael S. Marshall", "Theresa R. Connors", "Matthew P. Frosch", "Mark Montine", "Derek H. Oakley", "Christine L. Mac Donald", "C. Dirk Keene", "Bradley T. Hyman", "Juan Eugenio Iglesias"], "title": "Automated Segmentation of Coronal Brain Tissue Slabs for 3D Neuropathology", "comment": "19 pages, 10 figures", "summary": "Advances in image registration and machine learning have recently enabled\nvolumetric analysis of \\emph{postmortem} brain tissue from conventional\nphotographs of coronal slabs, which are routinely collected in brain banks and\nneuropathology laboratories worldwide. One caveat of this methodology is the\nrequirement of segmentation of the tissue from photographs, which currently\nrequires costly manual intervention. In this article, we present a deep\nlearning model to automate this process. The automatic segmentation tool relies\non a U-Net architecture that was trained with a combination of\n\\textit{(i)}1,414 manually segmented images of both fixed and fresh tissue,\nfrom specimens with varying diagnoses, photographed at two different sites; and\n\\textit{(ii)}~2,000 synthetic images with randomized contrast and corresponding\nmasks generated from MRI scans for improved generalizability to unseen\nphotographic setups. Automated model predictions on a subset of photographs not\nseen in training were analyzed to estimate performance compared to manual\nlabels -- including both inter- and intra-rater variability. Our model achieved\na median Dice score over 0.98, mean surface distance under 0.4~mm, and 95\\%\nHausdorff distance under 1.60~mm, which approaches inter-/intra-rater levels.\nOur tool is publicly available at surfer.nmr.mgh.harvard.edu/fswiki/PhotoTools.", "AI": {"tldr": "开发了一个基于深度学习的U-Net模型，用于自动化从常规脑组织切片照片中分割脑组织，其性能接近人工分割水平。", "motivation": "现有的从尸检脑组织照片进行体积分析的方法需要昂贵的手动组织分割，这限制了其广泛应用。", "method": "采用U-Net深度学习架构，结合1,414张不同诊断、不同拍摄地点的手动分割的固定和新鲜组织图像，以及2,000张具有随机对比度和相应掩膜的MRI扫描合成图像进行训练，以提高泛化能力。模型性能通过与人工标注（包括评估评估者间和评估者内变异性）进行比较来评估。", "result": "模型在未见过的照片子集上实现了超过0.98的中位数Dice分数，低于0.4毫米的平均表面距离，以及低于1.60毫米的95% Hausdorff距离，这些指标接近人工评估者间和评估者内的一致性水平。", "conclusion": "该研究成功开发了一个高效、准确的自动化脑组织分割工具，其性能与手动分割相当，并已公开发布，有望显著降低尸检脑组织体积分析的成本和工作量。"}}
{"id": "2508.09809", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09809", "abs": "https://arxiv.org/abs/2508.09809", "authors": ["Aishik Mandal", "Prottay Kumar Adhikary", "Hiba Arnaout", "Iryna Gurevych", "Tanmoy Chakraborty"], "title": "A Comprehensive Survey of Datasets for Clinical Mental Health AI Systems", "comment": "14 pages, 3 figures", "summary": "Mental health disorders are rising worldwide. However, the availability of\ntrained clinicians has not scaled proportionally, leaving many people without\nadequate or timely support. To bridge this gap, recent studies have shown the\npromise of Artificial Intelligence (AI) to assist mental health diagnosis,\nmonitoring, and intervention. However, the development of efficient, reliable,\nand ethical AI to assist clinicians is heavily dependent on high-quality\nclinical training datasets. Despite growing interest in data curation for\ntraining clinical AI assistants, existing datasets largely remain scattered,\nunder-documented, and often inaccessible, hindering the reproducibility,\ncomparability, and generalizability of AI models developed for clinical mental\nhealth care. In this paper, we present the first comprehensive survey of\nclinical mental health datasets relevant to the training and development of\nAI-powered clinical assistants. We categorize these datasets by mental\ndisorders (e.g., depression, schizophrenia), data modalities (e.g., text,\nspeech, physiological signals), task types (e.g., diagnosis prediction, symptom\nseverity estimation, intervention generation), accessibility (public,\nrestricted or private), and sociocultural context (e.g., language and cultural\nbackground). Along with these, we also investigate synthetic clinical mental\nhealth datasets. Our survey identifies critical gaps such as a lack of\nlongitudinal data, limited cultural and linguistic representation, inconsistent\ncollection and annotation standards, and a lack of modalities in synthetic\ndata. We conclude by outlining key challenges in curating and standardizing\nfuture datasets and provide actionable recommendations to facilitate the\ndevelopment of more robust, generalizable, and equitable mental health AI\nsystems.", "AI": {"tldr": "本文首次全面调查了用于训练和开发人工智能临床助手的精神健康临床数据集，并指出了现有数据的局限性。", "motivation": "全球精神健康障碍日益增多，但受训临床医生不足，导致许多人无法获得及时支持。人工智能有望辅助精神健康诊断、监测和干预，但其发展严重依赖高质量的临床训练数据集。现有数据集分散、文档不足且难以访问，阻碍了AI模型的可复现性、可比性和泛化性。", "method": "作者对精神健康临床数据集进行了全面调查，并按精神障碍类型（如抑郁症、精神分裂症）、数据模态（如文本、语音、生理信号）、任务类型（如诊断预测、症状严重性估计）、可访问性（公开、受限或私人）和社会文化背景（如语言、文化）进行分类。此外，还调查了合成临床精神健康数据集。", "result": "调查发现关键差距，包括缺乏纵向数据、文化和语言代表性有限、数据收集和标注标准不一致，以及合成数据中模态的缺乏。", "conclusion": "文章总结了未来数据集整理和标准化面临的关键挑战，并提供了可行的建议，以促进开发更鲁棒、更具泛化性和更公平的精神健康AI系统。"}}
{"id": "2508.09477", "categories": ["cs.CV", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.09477", "abs": "https://arxiv.org/abs/2508.09477", "authors": ["Zhipeng Yuan", "Kai Wang", "Weize Quan", "Dong-Ming Yan", "Tieru Wu"], "title": "CLIP-Flow: A Universal Discriminator for AI-Generated Images Inspired by Anomaly Detection", "comment": null, "summary": "With the rapid advancement of AI generative models, the visual quality of\nAI-generated images (AIIs) has become increasingly close to natural images,\nwhich inevitably raises security concerns. Most AII detectors often employ the\nconventional image classification pipeline with natural images and AIIs\n(generated by a generative model), which can result in limited detection\nperformance for AIIs from unseen generative models. To solve this, we proposed\na universal AI-generated image detector from the perspective of anomaly\ndetection. Our discriminator does not need to access any AIIs and learn a\ngeneralizable representation with unsupervised learning. Specifically, we use\nthe pre-trained CLIP encoder as the feature extractor and design a normalizing\nflow-like unsupervised model. Instead of AIIs, proxy images, e.g., obtained by\napplying a spectral modification operation on natural images, are used for\ntraining. Our models are trained by minimizing the likelihood of proxy images,\noptionally combined with maximizing the likelihood of natural images. Extensive\nexperiments demonstrate the effectiveness of our method on AIIs produced by\nvarious image generators.", "AI": {"tldr": "提出了一种基于异常检测的通用AI生成图像检测器，利用无监督学习和代理图像训练，无需访问AI生成图像，有效应对来自未知生成模型的挑战。", "motivation": "AI生成图像的视觉质量与真实图像日益接近，引发安全担忧。现有AI生成图像检测器通常采用传统图像分类方法，针对特定生成模型训练，导致对未知生成模型生成的图像检测性能有限。", "method": "从异常检测角度出发，设计了一个通用AI生成图像检测器。采用无监督学习，训练时无需访问任何AI生成图像。使用预训练的CLIP编码器作为特征提取器，并设计了一个类似归一化流的无监督模型。训练数据使用代理图像（例如，通过对自然图像进行光谱修改获得），通过最小化代理图像的似然，可选地结合最大化自然图像的似然来训练模型。", "result": "大量实验证明，该方法对各种图像生成器生成的AI图像均有效。", "conclusion": "所提出的基于异常检测的AI生成图像检测方法，通过无监督学习和代理图像训练，能够有效实现对来自各种生成模型的AI图像的通用检测，解决了传统方法在未知模型上的性能局限性。"}}
{"id": "2508.09834", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09834", "abs": "https://arxiv.org/abs/2508.09834", "authors": ["Weigao Sun", "Jiaxi Hu", "Yucheng Zhou", "Jusen Du", "Disen Lan", "Kexin Wang", "Tong Zhu", "Xiaoye Qu", "Yu Zhang", "Xiaoyu Mo", "Daizong Liu", "Yuxuan Liang", "Wenliang Chen", "Guoqi Li", "Yu Cheng"], "title": "Speed Always Wins: A Survey on Efficient Architectures for Large Language Models", "comment": "Survey, 82 pages, GitHub:\n  https://github.com/weigao266/Awesome-Efficient-Arch", "summary": "Large Language Models (LLMs) have delivered impressive results in language\nunderstanding, generation, reasoning, and pushes the ability boundary of\nmultimodal models. Transformer models, as the foundation of modern LLMs, offer\na strong baseline with excellent scaling properties. However, the traditional\ntransformer architecture requires substantial computations and poses\nsignificant obstacles for large-scale training and practical deployment. In\nthis survey, we offer a systematic examination of innovative LLM architectures\nthat address the inherent limitations of transformers and boost the efficiency.\nStarting from language modeling, this survey covers the background and\ntechnical details of linear and sparse sequence modeling methods, efficient\nfull attention variants, sparse mixture-of-experts, hybrid model architectures\nincorporating the above techniques, and emerging diffusion LLMs. Additionally,\nwe discuss applications of these techniques to other modalities and consider\ntheir wider implications for developing scalable, resource-aware foundation\nmodels. By grouping recent studies into the above category, this survey\npresents a blueprint of modern efficient LLM architectures, and we hope this\ncould help motivate future research toward more efficient, versatile AI\nsystems.", "AI": {"tldr": "该综述系统性地审视了旨在解决传统Transformer模型计算限制、提高效率的创新型大型语言模型（LLM）架构。", "motivation": "尽管Transformer模型是现代LLM的基础，并具有优异的扩展性，但其需要大量的计算资源，对大规模训练和实际部署构成了显著障碍。因此，需要开发更高效的LLM架构。", "method": "本综述通过系统性地检查现有研究，涵盖了线性与稀疏序列建模方法、高效的全注意力变体、稀疏专家混合（MoE）、结合上述技术的混合模型架构，以及新兴的扩散LLM。此外，还讨论了这些技术在其他模态中的应用。", "result": "该综述提供了一个现代高效LLM架构的蓝图，对近期研究进行了分类，并详细介绍了各种高效方法的背景和技术细节，包括它们在不同模态中的应用潜力。", "conclusion": "通过提供高效LLM架构的系统性分析和蓝图，本综述旨在激励未来研究，以开发更高效、多功能的AI系统，并促进可扩展、资源感知的基础模型的进步。"}}
{"id": "2508.09848", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09848", "abs": "https://arxiv.org/abs/2508.09848", "authors": ["Mo Yu", "Tsz Ting Chung", "Chulun Zhou", "Tong Li", "Rui Lu", "Jiangnan Li", "Liyan Xu", "Haoshu Lu", "Ning Zhang", "Jing Li", "Jie Zhou"], "title": "PRELUDE: A Benchmark Designed to Require Global Comprehension and Reasoning over Long Contexts", "comment": "First 7 authors contributed equally. Project page:\n  https://gorov.github.io/prelude", "summary": "We introduce PRELUDE, a benchmark for evaluating long-context understanding\nthrough the task of determining whether a character's prequel story is\nconsistent with the canonical narrative of the original book. Our task poses a\nstronger demand for global comprehension and deep reasoning than existing\nbenchmarks -- as the prequels are not part of the original story, assessing\ntheir plausibility typically requires searching and integrating information\nthat is only indirectly related. Empirically, 88% of instances require evidence\nfrom multiple parts of the narrative. Experimental results highlight the\nchallenge of our task: in-context learning, RAG and in-domain training with\nstate-of-the-art LLMs, and commercial DeepResearch services, lag behind humans\nby >15%. A further human study reveals that models often produce correct\nanswers with flawed reasoning, leading to an over 30% gap in reasoning accuracy\ncompared to humans. These findings underscore the substantial room for\nimprovement in long-context understanding and reasoning.", "AI": {"tldr": "PRELUDE是一个评估长文本理解的基准测试，通过判断角色前传故事与原著叙事的一致性，挑战现有模型在全球理解和深度推理方面的能力。", "motivation": "现有基准测试在长文本理解方面对全局理解和深度推理的要求不足，无法充分评估模型整合间接信息的能力。", "method": "引入PRELUDE基准测试，任务是判断角色前传故事与原著经典叙事的逻辑一致性。这通常需要搜索和整合间接相关的信息，其中88%的实例需要来自叙事多个部分的信息。", "result": "实验结果显示，最先进的大型语言模型（包括上下文学习、RAG和领域内训练）以及商业深度研究服务，在任务表现上落后人类超过15%。进一步的人类研究表明，模型常给出正确答案但推理过程有缺陷，导致推理准确率与人类存在超过30%的差距。", "conclusion": "当前大型语言模型在长文本理解和深度推理方面仍有巨大的提升空间。"}}
{"id": "2508.09478", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09478", "abs": "https://arxiv.org/abs/2508.09478", "authors": ["Moinak Bhattacharya", "Gagandeep Singh", "Shubham Jain", "Prateek Prasanna"], "title": "GazeLT: Visual attention-guided long-tailed disease classification in chest radiographs", "comment": null, "summary": "In this work, we present GazeLT, a human visual attention\nintegration-disintegration approach for long-tailed disease classification. A\nradiologist's eye gaze has distinct patterns that capture both fine-grained and\ncoarser level disease related information. While interpreting an image, a\nradiologist's attention varies throughout the duration; it is critical to\nincorporate this into a deep learning framework to improve automated image\ninterpretation. Another important aspect of visual attention is that apart from\nlooking at major/obvious disease patterns, experts also look at\nminor/incidental findings (few of these constituting long-tailed classes)\nduring the course of image interpretation. GazeLT harnesses the temporal aspect\nof the visual search process, via an integration and disintegration mechanism,\nto improve long-tailed disease classification. We show the efficacy of GazeLT\non two publicly available datasets for long-tailed disease classification,\nnamely the NIH-CXR-LT (n=89237) and the MIMIC-CXR-LT (n=111898) datasets.\nGazeLT outperforms the best long-tailed loss by 4.1% and the visual\nattention-based baseline by 21.7% in average accuracy metrics for these\ndatasets. Our code is available at https://github.com/lordmoinak1/gazelt.", "AI": {"tldr": "GazeLT是一种结合人类视觉注意力整合-分解机制的方法，用于改进长尾疾病分类，通过利用放射科医生眼动轨迹的时间特性，显著提升了分类性能。", "motivation": "放射科医生的眼动轨迹包含细粒度和粗粒度的疾病相关信息，且在图像判读过程中注意力会随时间变化。此外，专家不仅关注主要疾病模式，也会注意次要/偶然发现（其中一些属于长尾类别）。将这些专家视觉注意力的时间动态和对长尾类别的关注整合到深度学习框架中，对于提升自动化图像判读至关重要。", "method": "本文提出了GazeLT，一种人类视觉注意力整合-分解方法。该方法通过整合和分解机制，利用视觉搜索过程中的时间特性，以改进长尾疾病分类。", "result": "GazeLT在两个公开的长尾疾病分类数据集（NIH-CXR-LT和MIMIC-CXR-LT）上进行了验证。结果显示，GazeLT在平均准确率指标上，比最佳长尾损失方法高出4.1%，比基于视觉注意力的基线方法高出21.7%。", "conclusion": "GazeLT通过有效利用放射科医生眼动轨迹的时间特性，显著提升了长尾疾病分类的性能，证明了整合人类视觉注意力动态对于医学图像自动判读的有效性。"}}
{"id": "2508.09874", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09874", "abs": "https://arxiv.org/abs/2508.09874", "authors": ["Jiaqi Cao", "Jiarui Wang", "Rubin Wei", "Qipeng Guo", "Kai Chen", "Bowen Zhou", "Zhouhan Lin"], "title": "Memory Decoder: A Pretrained, Plug-and-Play Memory for Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) have shown strong abilities in general language\ntasks, yet adapting them to specific domains remains a challenge. Current\nmethod like Domain Adaptive Pretraining (DAPT) requires costly full-parameter\ntraining and suffers from catastrophic forgetting. Meanwhile,\nRetrieval-Augmented Generation (RAG) introduces substantial inference latency\ndue to expensive nearest-neighbor searches and longer context. This paper\nintroduces Memory Decoder, a plug-and-play pretrained memory that enables\nefficient domain adaptation without changing the original model's parameters.\nMemory Decoder employs a small transformer decoder that learns to imitate the\nbehavior of an external non-parametric retriever. Once trained, Memory Decoder\ncan be seamlessly integrated with any pretrained language model that shares the\nsame tokenizer, requiring no model-specific modifications. Experimental results\ndemonstrate that Memory Decoder enables effective adaptation of various Qwen\nand Llama models to three distinct specialized domains: biomedicine, finance,\nand law, reducing perplexity by an average of 6.17 points. Overall, Memory\nDecoder introduces a novel paradigm centered on a specially pretrained memory\ncomponent designed for domain-specific adaptation. This memory architecture can\nbe integrated in a plug-and-play manner, consistently enhancing performance\nacross multiple models within the target domain.", "AI": {"tldr": "本文提出Memory Decoder，一个即插即用的预训练记忆组件，能高效地将大型语言模型（LLMs）适应到特定领域，无需修改原始模型参数，且显著降低困惑度。", "motivation": "大型语言模型在通用任务上表现出色，但在特定领域适应性差。现有方法如DAPT成本高昂且存在灾难性遗忘；RAG则因检索导致推理延迟大。", "method": "引入Memory Decoder，一个小型Transformer解码器，学习模仿外部非参数检索器的行为。训练完成后，Memory Decoder可无缝集成到任何共享相同分词器的预训练语言模型中，无需模型特定修改。", "result": "实验证明，Memory Decoder能有效将Qwen和Llama模型适应到生物医学、金融和法律等专业领域，平均困惑度降低6.17点。", "conclusion": "Memory Decoder引入了一种以预训练记忆组件为核心的领域特定适应新范式，其即插即用的架构能持续提升目标领域内多个模型的性能。"}}
{"id": "2508.09865", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.09865", "abs": "https://arxiv.org/abs/2508.09865", "authors": ["Abdul Rehman Antall", "Naveed Akhtar"], "title": "Assessing the Feasibility of Lightweight Whisper Models for Low-Resource Urdu Transcription", "comment": "8 pages, 3 figures, 1 table, including references and appendix", "summary": "This study evaluates the feasibility of lightweight Whisper models (Tiny,\nBase, Small) for Urdu speech recognition in low-resource settings. Despite Urdu\nbeing the 10th most spoken language globally with over 230 million speakers,\nits representation in automatic speech recognition (ASR) systems remains\nlimited due to dialectal diversity, code-switching, and sparse training data.\nWe benchmark these models on a curated Urdu dataset using word error rate\n(WER), without fine-tuning. Results show Whisper-Small achieves the lowest\nerror rates (33.68\\% WER), outperforming Tiny (67.08\\% WER) and Base (53.67\\%\nWER). Qualitative analysis reveals persistent challenges in phonetic accuracy\nand lexical coherence, particularly for complex utterances. While Whisper-Small\ndemonstrates promise for deployable Urdu ASR, significant gaps remain. Our\nfindings emphasize lay the groundwork for future research into effective,\nlow-resource ASR systems.", "AI": {"tldr": "本研究评估了轻量级Whisper模型（Tiny, Base, Small）在低资源环境下乌尔都语语音识别的可行性，发现Whisper-Small表现最佳，但仍存在挑战。", "motivation": "尽管乌尔都语是全球使用人数第十多的语言，但由于方言多样性、语码转换和训练数据稀疏，其在自动语音识别（ASR）系统中的代表性仍然有限。", "method": "研究在未进行微调的情况下，使用精心策划的乌尔都语数据集，通过词错误率（WER）对Whisper的Tiny、Base和Small模型进行了基准测试。", "result": "结果显示，Whisper-Small实现了最低的错误率（33.68% WER），优于Tiny（67.08% WER）和Base（53.67% WER）。定性分析揭示了在语音准确性和词汇连贯性方面，尤其对于复杂话语，仍存在持续的挑战。", "conclusion": "Whisper-Small模型在可部署的乌尔都语ASR方面展现出潜力，但仍存在显著差距。本研究为未来有效、低资源的ASR系统研究奠定了基础。"}}
{"id": "2508.09479", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09479", "abs": "https://arxiv.org/abs/2508.09479", "authors": ["Xuejun Huang", "Xinyi Liu", "Yi Wan", "Zhi Zheng", "Bin Zhang", "Mingtao Xiong", "Yingying Pei", "Yongjun Zhang"], "title": "SkySplat: Generalizable 3D Gaussian Splatting from Multi-Temporal Sparse Satellite Images", "comment": null, "summary": "Three-dimensional scene reconstruction from sparse-view satellite images is a\nlong-standing and challenging task. While 3D Gaussian Splatting (3DGS) and its\nvariants have recently attracted attention for its high efficiency, existing\nmethods remain unsuitable for satellite images due to incompatibility with\nrational polynomial coefficient (RPC) models and limited generalization\ncapability. Recent advances in generalizable 3DGS approaches show potential,\nbut they perform poorly on multi-temporal sparse satellite images due to\nlimited geometric constraints, transient objects, and radiometric\ninconsistencies. To address these limitations, we propose SkySplat, a novel\nself-supervised framework that integrates the RPC model into the generalizable\n3DGS pipeline, enabling more effective use of sparse geometric cues for\nimproved reconstruction. SkySplat relies only on RGB images and\nradiometric-robust relative height supervision, thereby eliminating the need\nfor ground-truth height maps. Key components include a Cross-Self Consistency\nModule (CSCM), which mitigates transient object interference via\nconsistency-based masking, and a multi-view consistency aggregation strategy\nthat refines reconstruction results. Compared to per-scene optimization\nmethods, SkySplat achieves an 86 times speedup over EOGS with higher accuracy.\nIt also outperforms generalizable 3DGS baselines, reducing MAE from 13.18 m to\n1.80 m on the DFC19 dataset significantly, and demonstrates strong\ncross-dataset generalization on the MVS3D benchmark.", "AI": {"tldr": "SkySplat是一个新颖的自监督框架，它将RPC模型集成到可泛化的3D高斯泼溅（3DGS）管线中，用于从稀疏视角卫星图像进行高效、准确的三维场景重建。", "motivation": "现有3DGS方法不适用于卫星图像，原因在于与有理多项式系数（RPC）模型不兼容、泛化能力有限，以及在多时相稀疏卫星图像上性能不佳（受限于几何约束、瞬态物体和辐射不一致）。", "method": "SkySplat是一个自监督框架，将RPC模型整合到可泛化的3DGS流程中。它仅依赖RGB图像和辐射鲁棒的相对高度监督，无需真值高度图。关键组件包括：跨自一致性模块（CSCM），通过基于一致性的掩蔽来减轻瞬态物体干扰；以及多视角一致性聚合策略，用于精炼重建结果。", "result": "与逐场景优化方法相比，SkySplat比EOGS快86倍且精度更高。它显著优于可泛化的3DGS基线，在DFC19数据集上将平均绝对误差（MAE）从13.18米降低到1.80米，并在MVS3D基准上展示了强大的跨数据集泛化能力。", "conclusion": "SkySplat有效解决了从稀疏视角卫星图像进行三维场景重建的挑战，通过集成RPC模型和引入鲁棒性模块，实现了高效、高精度和强泛化能力的重建。"}}
{"id": "2508.09886", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.09886", "abs": "https://arxiv.org/abs/2508.09886", "authors": ["Lingyu Chen", "Yawen Zeng", "Yue Wang", "Peng Wan", "Guo-chen Ning", "Hongen Liao", "Daoqiang Zhang", "Fang Chen"], "title": "COME: Dual Structure-Semantic Learning with Collaborative MoE for Universal Lesion Detection Across Heterogeneous Ultrasound Datasets", "comment": "ICCV 2025", "summary": "Conventional single-dataset training often fails with new data distributions,\nespecially in ultrasound (US) image analysis due to limited data, acoustic\nshadows, and speckle noise. Therefore, constructing a universal framework for\nmulti-heterogeneous US datasets is imperative. However, a key challenge arises:\nhow to effectively mitigate inter-dataset interference while preserving\ndataset-specific discriminative features for robust downstream task? Previous\napproaches utilize either a single source-specific decoder or a domain\nadaptation strategy, but these methods experienced a decline in performance\nwhen applied to other domains. Considering this, we propose a Universal\nCollaborative Mixture of Heterogeneous Source-Specific Experts (COME).\nSpecifically, COME establishes dual structure-semantic shared experts that\ncreate a universal representation space and then collaborate with\nsource-specific experts to extract discriminative features through providing\ncomplementary features. This design enables robust generalization by leveraging\ncross-datasets experience distributions and providing universal US priors for\nsmall-batch or unseen data scenarios. Extensive experiments under three\nevaluation modes (single-dataset, intra-organ, and inter-organ integration\ndatasets) demonstrate COME's superiority, achieving significant mean AP\nimprovements over state-of-the-art methods. Our project is available at:\nhttps://universalcome.github.io/UniversalCOME/.", "AI": {"tldr": "提出COME框架，通过共享专家和源特定专家协作，解决超声图像多异构数据集训练中的跨数据集干扰问题，实现鲁棒泛化。", "motivation": "传统单数据集训练在面对新数据分布时（尤其在超声图像分析中）表现不佳，原因在于数据有限、声影和散斑噪声。构建一个通用的多异构超声数据集框架至关重要，但挑战是如何在减轻数据集间干扰的同时保留数据集特有的判别特征。", "method": "提出“异构源特定专家通用协作混合”（COME）框架。该框架建立双重结构-语义共享专家，创建通用表示空间，并与源特定专家协作，通过提供互补特征来提取判别性特征。这种设计利用跨数据集经验分布并提供通用超声先验知识，以实现鲁棒泛化。", "result": "在三种评估模式（单数据集、器官内、器官间整合数据集）下进行了广泛实验，证明COME优于现有最先进方法，实现了显著的平均AP改进。", "conclusion": "COME框架通过利用跨数据集经验分布和提供通用超声先验知识，实现了鲁棒泛化，特别适用于小批量或未见数据场景。"}}
{"id": "2508.09878", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.09878", "abs": "https://arxiv.org/abs/2508.09878", "authors": ["Archie Sage", "Jeroen Keppens", "Helen Yannakoudakis"], "title": "A Survey of Cognitive Distortion Detection and Classification in NLP", "comment": "Under review via ACL Rolling Review and committed to EMNLP 2025.\n  Camera-ready updates to follow", "summary": "As interest grows in the application of natural language processing (NLP)\ntechniques to mental health, a growing body of work explores the automatic\ndetection and classification of cognitive distortions (CDs). CDs are habitual\npatterns of negatively biased or flawed thinking that distort how people\nperceive events, judge themselves, and react to the world around them.\nIdentifying and addressing them is an important part of therapy. Despite its\nmomentum, the field remains fragmented, with inconsistencies in CD taxonomies,\ntask formulations, and evaluation practices. This survey reviews 38 studies\nspanning two decades, providing a structured overview of datasets, modelling\napproaches, and evaluation strategies. We provide a consolidated CD taxonomy\nreference, summarise common task setups, and highlight open challenges to\nsupport more coherent and reproducible research in this emerging area.", "AI": {"tldr": "本文综述了自然语言处理（NLP）在认知扭曲（CDs）自动检测与分类中的应用，指出该领域碎片化问题，并提供结构化概览、统一分类法及未来挑战。", "motivation": "认知扭曲是心理治疗中的重要环节，NLP技术可用于其自动识别。然而，现有研究在认知扭曲分类、任务设置和评估上存在不一致性，导致领域碎片化，因此需要一个系统性综述来整合知识并促进研究。", "method": "本文通过回顾过去二十年间的38项研究，对数据集、建模方法和评估策略进行了结构化综述。", "result": "本文提供了一个统一的认知扭曲分类参考，总结了常见的任务设置，并指出了该领域面临的开放挑战。", "conclusion": "本综述旨在支持该新兴领域开展更连贯和可复现的研究。"}}
{"id": "2508.09487", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09487", "abs": "https://arxiv.org/abs/2508.09487", "authors": ["Ju Yeon Kang", "Jaehong Park", "Semin Kim", "Ji Won Yoon", "Nam Soo Kim"], "title": "SARE: Semantic-Aware Reconstruction Error for Generalizable Diffusion-Generated Image Detection", "comment": "Work in progress", "summary": "Recently, diffusion-generated image detection has gained increasing\nattention, as the rapid advancement of diffusion models has raised serious\nconcerns about their potential misuse. While existing detection methods have\nachieved promising results, their performance often degrades significantly when\nfacing fake images from unseen, out-of-distribution (OOD) generative models,\nsince they primarily rely on model-specific artifacts. To address this\nlimitation, we explore a fundamental property commonly observed in fake images.\nMotivated by the observation that fake images tend to exhibit higher similarity\nto their captions than real images, we propose a novel representation, namely\nSemantic-Aware Reconstruction Error (SARE), that measures the semantic\ndifference between an image and its caption-guided reconstruction. The\nhypothesis behind SARE is that real images, whose captions often fail to fully\ncapture their complex visual content, may undergo noticeable semantic shifts\nduring the caption-guided reconstruction process. In contrast, fake images,\nwhich closely align with their captions, show minimal semantic changes. By\nquantifying these semantic shifts, SARE can be utilized as a discriminative\nfeature for robust detection across diverse generative models. We empirically\ndemonstrate that the proposed method exhibits strong generalization,\noutperforming existing baselines on benchmarks including GenImage and\nCommunityForensics.", "AI": {"tldr": "针对现有扩散生成图像检测方法在未见过模型上泛化性差的问题，本文提出了一种名为语义感知重建误差（SARE）的新方法。SARE通过测量图像与其标题引导重建之间的语义差异来区分真实图像和伪造图像，利用伪造图像与标题更紧密对齐的特性，实现了跨生成模型的鲁棒检测。", "motivation": "随着扩散模型的快速发展，其潜在滥用引起了严重关注，因此扩散生成图像检测变得日益重要。然而，现有检测方法主要依赖于模型特定的伪影，导致在面对来自未见过、分布外（OOD）生成模型的伪造图像时，性能显著下降。", "method": "本文基于一个普遍观察到的特性：伪造图像与其标题的相似度往往高于真实图像。为此，提出了一种新颖的表示方法——语义感知重建误差（SARE）。SARE衡量图像与其标题引导重建之间的语义差异。其核心假设是，真实图像的复杂视觉内容通常无法被标题完全捕捉，因此在标题引导重建过程中会发生明显的语义偏移；相反，伪造图像与标题高度一致，显示出最小的语义变化。通过量化这些语义偏移，SARE被用作区分不同生成模型的判别性特征。", "result": "实验证明，所提出的方法展现出强大的泛化能力，在GenImage和CommunityForensics等基准测试中优于现有基线方法。", "conclusion": "通过利用真实图像和伪造图像在语义上与标题对齐程度的差异，SARE提供了一种鲁棒且泛化能力强的特征，能够有效检测扩散生成的图像，解决了现有方法在处理分布外生成模型时性能下降的局限性。"}}
{"id": "2508.09937", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.09937", "abs": "https://arxiv.org/abs/2508.09937", "authors": ["Muneeza Azmat", "Momin Abbas", "Maysa Malfiza Garcia de Macedo", "Marcelo Carpinette Grave", "Luan Soares de Souza", "Tiago Machado", "Rogerio A de Paula", "Raya Horesh", "Yixin Chen", "Heloisa Caroline de Souza Pereira Candello", "Rebecka Nordenlow", "Aminat Adebiyi"], "title": "A Comprehensive Evaluation framework of Alignment Techniques for LLMs", "comment": "In submission", "summary": "As Large Language Models (LLMs) become increasingly integrated into\nreal-world applications, ensuring their outputs align with human values and\nsafety standards has become critical. The field has developed diverse alignment\napproaches including traditional fine-tuning methods (RLHF, instruction\ntuning), post-hoc correction systems, and inference-time interventions, each\nwith distinct advantages and limitations. However, the lack of unified\nevaluation frameworks makes it difficult to systematically compare these\nparadigms and guide deployment decisions. This paper introduces a\nmulti-dimensional evaluation of alignment techniques for LLMs, a comprehensive\nevaluation framework that provides a systematic comparison across all major\nalignment paradigms. Our framework assesses methods along four key dimensions:\nalignment detection, alignment quality, computational efficiency, and\nrobustness. Through experiments across diverse base models and alignment\nstrategies, we demonstrate the utility of our framework in identifying\nstrengths and limitations of current state-of-the-art models, providing\nvaluable insights for future research directions.", "AI": {"tldr": "本文提出一个多维度评估框架，用于系统比较大型语言模型（LLMs）的各种对齐技术，涵盖对齐检测、质量、计算效率和鲁棒性。", "motivation": "随着LLMs在实际应用中日益普及，确保其输出符合人类价值观和安全标准变得至关重要。现有对齐方法多样，但缺乏统一的评估框架，难以系统比较和指导部署决策。", "method": "引入了一个多维度评估框架，该框架系统地比较了所有主要的LLM对齐范式。评估维度包括：对齐检测、对齐质量、计算效率和鲁棒性。通过在不同基础模型和对齐策略上进行实验来验证其效用。", "result": "该框架能够有效识别当前最先进模型的优势和局限性。实验证明了其在提供有价值见解方面的实用性。", "conclusion": "该多维度评估框架为LLM对齐技术的系统比较提供了工具，为未来的研究方向提供了宝贵的见解。"}}
{"id": "2508.09935", "categories": ["cs.CL", "q-fin.CP", "q-fin.GN"], "pdf": "https://arxiv.org/pdf/2508.09935", "abs": "https://arxiv.org/abs/2508.09935", "authors": ["Sayem Hossen", "Monalisa Moon Joti", "Md. Golam Rashed"], "title": "Language of Persuasion and Misrepresentation in Business Communication: A Textual Detection Approach", "comment": "21", "summary": "Business communication digitisation has reorganised the process of persuasive\ndiscourse, which\n  allows not only greater transparency but also advanced deception. This\ninquiry synthesises classical\n  rhetoric and communication psychology with linguistic theory and empirical\nstudies in the financial\n  reporting, sustainability discourse, and digital marketing to explain how\ndeceptive language can be\n  systematically detected using persuasive lexicon. In controlled settings,\ndetection accuracies of greater\n  than 99% were achieved by using computational textual analysis as well as\npersonalised transformer\n  models. However, reproducing this performance in multilingual settings is\nalso problematic and,\n  to a large extent, this is because it is not easy to find sufficient data,\nand because few multilingual\n  text-processing infrastructures are in place. This evidence shows that there\nhas been an increasing\n  gap between the theoretical representations of communication and those\nempirically approximated,\n  and therefore, there is a need to have strong automatic text-identification\nsystems where AI-based\n  discourse is becoming more realistic in communicating with humans.", "AI": {"tldr": "本研究结合修辞学、传播心理学和语言学理论，利用计算文本分析和Transformer模型，实现了对数字化商业沟通中欺骗性语言的高精度检测，并指出多语言环境下的挑战及对自动化文本识别系统的需求。", "motivation": "数字化商业沟通在提高透明度的同时，也增加了欺骗的可能性。理论与经验之间的沟通表征存在日益扩大的差距，因此需要强大的自动化文本识别系统，以应对AI与人类交流日益现实的趋势。", "method": "综合古典修辞学、传播心理学、语言学理论以及金融报告、可持续性话语和数字营销领域的实证研究，解释如何使用说服性词汇系统地检测欺骗性语言。采用计算文本分析和个性化Transformer模型进行检测。", "result": "在受控环境下，使用计算文本分析和个性化Transformer模型实现了超过99%的欺骗检测准确率。但在多语言环境中复制此性能存在困难，主要原因是数据不足和缺乏多语言文本处理基础设施。", "conclusion": "研究表明，沟通的理论表征与经验近似值之间存在日益扩大的差距，因此迫切需要强大的自动化文本识别系统，尤其是在基于AI的话语与人类交流变得更加现实的背景下。"}}
{"id": "2508.09499", "categories": ["cs.CV", "cs.CG", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.09499", "abs": "https://arxiv.org/abs/2508.09499", "authors": ["Liyan Jia", "Chuan-Xian Ren", "Hong Yan"], "title": "CWFBind: Geometry-Awareness for Fast and Accurate Protein-Ligand Docking", "comment": null, "summary": "Accurately predicting the binding conformation of small-molecule ligands to\nprotein targets is a critical step in rational drug design. Although recent\ndeep learning-based docking surpasses traditional methods in speed and\naccuracy, many approaches rely on graph representations and language\nmodel-inspired encoders while neglecting critical geometric information,\nresulting in inaccurate pocket localization and unrealistic binding\nconformations. In this study, we introduce CWFBind, a weighted, fast, and\naccurate docking method based on local curvature features. Specifically, we\nintegrate local curvature descriptors during the feature extraction phase to\nenrich the geometric representation of both proteins and ligands, complementing\nexisting chemical, sequence, and structural features. Furthermore, we embed\ndegree-aware weighting mechanisms into the message passing process, enhancing\nthe model's ability to capture spatial structural distinctions and interaction\nstrengths. To address the class imbalance challenge in pocket prediction,\nCWFBind employs a ligand-aware dynamic radius strategy alongside an enhanced\nloss function, facilitating more precise identification of binding regions and\nkey residues. Comprehensive experimental evaluations demonstrate that CWFBind\nachieves competitive performance across multiple docking benchmarks, offering a\nbalanced trade-off between accuracy and efficiency.", "AI": {"tldr": "CWFBind是一种新的深度学习分子对接方法，通过整合局部曲率特征、度感知加权和动态半径策略，解决了现有方法在几何信息和结合区域识别上的不足，提升了对接的准确性和效率。", "motivation": "尽管现有的深度学习分子对接方法在速度和准确性上超越了传统方法，但许多方法忽略了关键的几何信息，导致结合口袋定位不准确和结合构象不真实，这是合理药物设计中的一个关键挑战。", "method": "CWFBind方法通过以下方式提升性能：1) 在特征提取阶段整合局部曲率描述符，以丰富蛋白质和配体的几何表示。2) 在消息传递过程中嵌入度感知加权机制，增强模型捕获空间结构差异和相互作用强度的能力。3) 采用配体感知动态半径策略和增强的损失函数来解决口袋预测中的类别不平衡问题，从而更精确地识别结合区域和关键残基。", "result": "全面的实验评估表明，CWFBind在多个分子对接基准测试中取得了具有竞争力的性能，并在准确性和效率之间实现了良好的平衡。", "conclusion": "CWFBind通过引入局部曲率特征、度感知加权和优化的口袋预测策略，显著提升了深度学习分子对接的准确性和效率，为药物设计提供了更可靠的工具。"}}
{"id": "2508.09945", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09945", "abs": "https://arxiv.org/abs/2508.09945", "authors": ["Lingjie Jiang", "Shaohan Huang", "Xun Wu", "Yixia Li", "Dongdong Zhang", "Furu Wei"], "title": "VisCodex: Unified Multimodal Code Generation via Merging Vision and Coding Models", "comment": null, "summary": "Multimodal large language models (MLLMs) have significantly advanced the\nintegration of visual and textual understanding. However, their ability to\ngenerate code from multimodal inputs remains limited. In this work, we\nintroduce VisCodex, a unified framework that seamlessly merges vision and\ncoding language models to empower MLLMs with strong multimodal code generation\nabilities. Leveraging a task vector-based model merging technique, we integrate\na state-of-the-art coding LLM into a strong vision-language backbone, while\npreserving both visual comprehension and advanced coding skills. To support\ntraining and evaluation, we introduce the Multimodal Coding Dataset (MCD), a\nlarge-scale and diverse collection of 598k samples, including high-quality HTML\ncode, chart image-code pairs, image-augmented StackOverflow QA, and algorithmic\nproblems. Furthermore, we propose InfiBench-V, a novel and challenging\nbenchmark specifically designed to assess models on visually-rich, real-world\nprogramming questions that demand a nuanced understanding of both textual and\nvisual contexts. Extensive experiments show that VisCodex achieves\nstate-of-the-art performance among open-source MLLMs and approaches proprietary\nmodels like GPT-4o, highlighting the effectiveness of our model merging\nstrategy and new datasets.", "AI": {"tldr": "VisCodex是一个统一框架，通过任务向量模型合并技术，将视觉和编程语言模型融合，显著提升了多模态大语言模型（MLLMs）的代码生成能力，并引入了大规模数据集MCD和新基准InfiBench-V，实现了开源MLLM中的SOTA性能。", "motivation": "尽管多模态大语言模型在视觉和文本理解方面取得了显著进展，但它们从多模态输入生成代码的能力仍然有限。", "method": "本文提出了VisCodex框架，通过基于任务向量的模型合并技术，将最先进的编程LLM整合到强大的视觉-语言骨干中，同时保留视觉理解和高级编程技能。为支持训练和评估，引入了包含59.8万样本的多模态编码数据集（MCD），包括高质量HTML代码、图表图像-代码对、图像增强的StackOverflow问答和算法问题。此外，提出了InfiBench-V，一个专门用于评估模型在视觉丰富、需要细致理解文本和视觉上下文的真实世界编程问题上的新颖且具有挑战性的基准。", "result": "广泛的实验表明，VisCodex在开源多模态大语言模型中取得了最先进的性能，并接近了如GPT-4o等专有模型。", "conclusion": "VisCodex的模型合并策略和新数据集的有效性得到了突出验证。"}}
{"id": "2508.09952", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.09952", "abs": "https://arxiv.org/abs/2508.09952", "authors": ["Hermione Warr", "Wentian Xu", "Harry Anthony", "Yasin Ibrahim", "Daniel McGowan", "Konstantinos Kamnitsas"], "title": "Specialised or Generic? Tokenization Choices for Radiology Language Models", "comment": "Accepted to ELAMI@MICCAI2025", "summary": "The vocabulary used by language models (LM) - defined by the tokenizer -\nplays a key role in text generation quality. However, its impact remains\nunder-explored in radiology. In this work, we address this gap by\nsystematically comparing general, medical, and domain-specific tokenizers on\nthe task of radiology report summarisation across three imaging modalities. We\nalso investigate scenarios with and without LM pre-training on PubMed\nabstracts. Our findings demonstrate that medical and domain-specific\nvocabularies outperformed widely used natural language alternatives when models\nare trained from scratch. Pre-training partially mitigates performance\ndifferences between tokenizers, whilst the domain-specific tokenizers achieve\nthe most favourable results. Domain-specific tokenizers also reduce memory\nrequirements due to smaller vocabularies and shorter sequences. These results\ndemonstrate that adapting the vocabulary of LMs to the clinical domain provides\npractical benefits, including improved performance and reduced computational\ndemands, making such models more accessible and effective for both research and\nreal-world healthcare settings.", "AI": {"tldr": "研究发现，在放射学报告摘要任务中，医学和领域特定词汇表（分词器）优于通用词汇表，尤其是在模型从零开始训练时；领域特定分词器还能降低计算资源需求。", "motivation": "语言模型（LM）使用的词汇表（由分词器定义）对文本生成质量至关重要，但在放射学领域的具体影响尚未得到充分探索。", "method": "系统比较了通用、医学和领域特定分词器在三种影像模态的放射学报告摘要任务上的表现。同时，研究了有无PubMed摘要预训练场景下的分词器性能。", "result": "当模型从零开始训练时，医学和领域特定词汇表优于广泛使用的自然语言替代方案。预训练部分缓解了分词器之间的性能差异，但领域特定分词器仍能取得最佳结果。此外，领域特定分词器由于词汇量更小、序列更短，还能减少内存需求。", "conclusion": "将语言模型的词汇表适应临床领域（使用领域特定分词器）可带来实际益处，包括性能提升和计算需求降低，使此类模型在研究和实际医疗环境中更易于访问和有效。"}}
{"id": "2508.09524", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09524", "abs": "https://arxiv.org/abs/2508.09524", "authors": ["Yipei Wang", "Shiyu Hu", "Shukun Jia", "Panxi Xu", "Hongfei Ma", "Yiping Ma", "Jing Zhang", "Xiaobo Lu", "Xin Zhao"], "title": "SOI is the Root of All Evil: Quantifying and Breaking Similar Object Interference in Single Object Tracking", "comment": null, "summary": "In this paper, we present the first systematic investigation and\nquantification of Similar Object Interference (SOI), a long-overlooked yet\ncritical bottleneck in Single Object Tracking (SOT). Through controlled Online\nInterference Masking (OIM) experiments, we quantitatively demonstrate that\neliminating interference sources leads to substantial performance improvements\n(AUC gains up to 4.35) across all SOTA trackers, directly validating SOI as a\nprimary constraint for robust tracking and highlighting the feasibility of\nexternal cognitive guidance. Building upon these insights, we adopt natural\nlanguage as a practical form of external guidance, and construct SOIBench-the\nfirst semantic cognitive guidance benchmark specifically targeting SOI\nchallenges. It automatically mines SOI frames through multi-tracker collective\njudgment and introduces a multi-level annotation protocol to generate precise\nsemantic guidance texts. Systematic evaluation on SOIBench reveals a striking\nfinding: existing vision-language tracking (VLT) methods fail to effectively\nexploit semantic cognitive guidance, achieving only marginal improvements or\neven performance degradation (AUC changes of -0.26 to +0.71). In contrast, we\npropose a novel paradigm employing large-scale vision-language models (VLM) as\nexternal cognitive engines that can be seamlessly integrated into arbitrary RGB\ntrackers. This approach demonstrates substantial improvements under semantic\ncognitive guidance (AUC gains up to 0.93), representing a significant\nadvancement over existing VLT methods. We hope SOIBench will serve as a\nstandardized evaluation platform to advance semantic cognitive tracking\nresearch and contribute new insights to the tracking research community.", "AI": {"tldr": "本文首次系统研究并量化了单目标跟踪（SOT）中相似目标干扰（SOI）这一关键瓶颈。通过实验验证了SOI的影响，并构建了首个针对SOI挑战的语义认知引导基准SOIBench。研究发现现有视觉-语言跟踪方法未能有效利用语义引导，而提出了一种基于大型视觉-语言模型（VLM）的新范式，可显著提升跟踪性能。", "motivation": "相似目标干扰（SOI）在单目标跟踪（SOT）中是一个长期被忽视但至关重要的瓶颈。研究旨在系统调查和量化SOI对跟踪性能的影响，并探索通过外部认知引导来解决这一问题。", "method": "1. 通过受控的在线干扰遮蔽（OIM）实验，定量验证消除干扰源对跟踪性能的提升，从而验证SOI是跟踪的主要限制。2. 构建SOIBench数据集：通过多跟踪器集体判断自动挖掘SOI帧，并引入多级标注协议生成精确的语义引导文本。3. 提出一种新范式：利用大型视觉-语言模型（VLM）作为外部认知引擎，并将其无缝集成到任意RGB跟踪器中。", "result": "1. 消除干扰源可使所有SOTA跟踪器性能显著提升（AUC增益高达4.35），直接验证了SOI是鲁棒跟踪的主要制约。2. 现有视觉-语言跟踪（VLT）方法未能有效利用语义认知引导，仅取得微小改进甚至性能下降（AUC变化为-0.26至+0.71）。3. 提出的基于大型VLM的范式在语义认知引导下表现出显著改进（AUC增益高达0.93），优于现有VLT方法。", "conclusion": "相似目标干扰（SOI）是单目标跟踪的关键瓶颈。通过外部认知引导（特别是基于大型视觉-语言模型）可以有效缓解SOI问题，显著提升跟踪性能。SOIBench将作为一个标准化的评估平台，推动语义认知跟踪研究的发展。"}}
{"id": "2508.09966", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09966", "abs": "https://arxiv.org/abs/2508.09966", "authors": ["Amir Hosseinian", "Ashkan Dehghani Zahedani", "Umer Mansoor", "Noosheen Hashemi", "Mark Woodward"], "title": "January Food Benchmark (JFB): A Public Benchmark Dataset and Evaluation Suite for Multimodal Food Analysis", "comment": null, "summary": "Progress in AI for automated nutritional analysis is critically hampered by\nthe lack of standardized evaluation methodologies and high-quality, real-world\nbenchmark datasets. To address this, we introduce three primary contributions.\nFirst, we present the January Food Benchmark (JFB), a publicly available\ncollection of 1,000 food images with human-validated annotations. Second, we\ndetail a comprehensive benchmarking framework, including robust metrics and a\nnovel, application-oriented overall score designed to assess model performance\nholistically. Third, we provide baseline results from both general-purpose\nVision-Language Models (VLMs) and our own specialized model,\njanuary/food-vision-v1. Our evaluation demonstrates that the specialized model\nachieves an Overall Score of 86.2, a 12.1-point improvement over the\nbest-performing general-purpose configuration. This work offers the research\ncommunity a valuable new evaluation dataset and a rigorous framework to guide\nand benchmark future developments in automated nutritional analysis.", "AI": {"tldr": "该论文提出了一个名为January Food Benchmark (JFB) 的公共数据集、一个全面的评估框架，并展示了专业模型在自动化营养分析方面优于通用模型。", "motivation": "自动化营养分析领域的人工智能进展受限于缺乏标准化的评估方法和高质量的真实世界基准数据集。", "method": "1. 引入了January Food Benchmark (JFB)，一个包含1000张带人工验证注释的食物图像的公开数据集。2. 详细阐述了一个全面的基准评估框架，包括鲁棒的度量标准和新颖的、面向应用的综合评分。3. 提供了通用视觉-语言模型(VLMs)和作者自己的专业模型(january/food-vision-v1)的基线结果。", "result": "专业模型(january/food-vision-v1)的综合得分为86.2，比表现最佳的通用模型配置提高了12.1分。", "conclusion": "该工作为研究社区提供了一个有价值的新评估数据集和一个严谨的框架，以指导和基准化未来自动化营养分析领域的发展。"}}
{"id": "2508.09954", "categories": ["cs.CL", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.09954", "abs": "https://arxiv.org/abs/2508.09954", "authors": ["Johannes Schäfer", "Roman Klinger"], "title": "Shaping Event Backstories to Estimate Potential Emotion Contexts", "comment": "May 2025 version", "summary": "Emotion analysis is an inherently ambiguous task. Previous work studied\nannotator properties to explain disagreement, but this overlooks the\npossibility that ambiguity may stem from missing information about the context\nof events. In this paper, we propose a novel approach that adds reasonable\ncontexts to event descriptions, which may better explain a particular\nsituation. Our goal is to understand whether these enriched contexts enable\nhuman annotators to annotate emotions more reliably. We disambiguate a target\nevent description by automatically generating multiple event chains conditioned\non differing emotions. By combining techniques from short story generation in\nvarious settings, we achieve coherent narratives that result in a specialized\ndataset for the first comprehensive and systematic examination of\ncontextualized emotion analysis. Through automatic and human evaluation, we\nfind that contextual narratives enhance the interpretation of specific emotions\nand support annotators in producing more consistent annotations.", "AI": {"tldr": "该研究通过为事件描述添加自动生成的上下文，旨在提高人类情感标注的可靠性和一致性。", "motivation": "情感分析本身具有模糊性，以往研究多关注标注者属性，但可能忽略了情境信息缺失导致的歧义。本研究旨在探究补充上下文是否能提高情感标注的可靠性。", "method": "提出一种新方法，通过自动生成多个基于不同情感的事件链来为目标事件描述添加合理上下文。结合短故事生成技术，构建了一个专门的数据集，用于首次全面系统地研究情境化情感分析。通过自动化和人工评估进行验证。", "result": "研究发现，情境叙述能增强对特定情感的解释，并帮助标注者产生更一致的标注结果。", "conclusion": "为事件描述添加情境上下文能够有效提高人类情感标注的可靠性和一致性，解决了情感分析中因信息缺失导致的歧义问题。"}}
{"id": "2508.09525", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09525", "abs": "https://arxiv.org/abs/2508.09525", "authors": ["Yuxin Mao", "Zhen Qin", "Jinxing Zhou", "Bin Fan", "Jing Zhang", "Yiran Zhong", "Yuchao Dai"], "title": "Learning Spatial Decay for Vision Transformers", "comment": null, "summary": "Vision Transformers (ViTs) have revolutionized computer vision, yet their\nself-attention mechanism lacks explicit spatial inductive biases, leading to\nsuboptimal performance on spatially-structured tasks. Existing approaches\nintroduce data-independent spatial decay based on fixed distance metrics,\napplying uniform attention weighting regardless of image content and limiting\nadaptability to diverse visual scenarios. Inspired by recent advances in large\nlanguage models where content-aware gating mechanisms (e.g., GLA, HGRN2, FOX)\nsignificantly outperform static alternatives, we present the first successful\nadaptation of data-dependent spatial decay to 2D vision transformers. We\nintroduce \\textbf{Spatial Decay Transformer (SDT)}, featuring a novel\nContext-Aware Gating (CAG) mechanism that generates dynamic, data-dependent\ndecay for patch interactions. Our approach learns to modulate spatial attention\nbased on both content relevance and spatial proximity. We address the\nfundamental challenge of 1D-to-2D adaptation through a unified spatial-content\nfusion framework that integrates manhattan distance-based spatial priors with\nlearned content representations. Extensive experiments on ImageNet-1K\nclassification and generation tasks demonstrate consistent improvements over\nstrong baselines. Our work establishes data-dependent spatial decay as a new\nparadigm for enhancing spatial attention in vision transformers.", "AI": {"tldr": "提出空间衰减Transformer (SDT)，通过内容感知门控(CAG)实现数据依赖的2D空间衰减，提升ViT在空间任务上的性能。", "motivation": "ViT的自注意力机制缺乏显式空间归纳偏置，导致在空间结构任务上表现不佳。现有方法使用固定、数据无关的空间衰减，限制了对多样视觉场景的适应性。受大型语言模型中内容感知门控机制成功的启发，旨在将数据依赖的空间衰减引入2D ViT。", "method": "引入空间衰减Transformer (SDT)，核心是上下文感知门控(CAG)机制，生成动态、数据依赖的patch交互衰减。该方法基于内容相关性和空间邻近性调节空间注意力。通过统一的空间-内容融合框架，将曼哈顿距离的空间先验与学习到的内容表示相结合，解决1D到2D的适应性挑战。", "result": "在ImageNet-1K分类和生成任务上，SDT始终优于强基线模型，证明了其有效性。", "conclusion": "本研究确立了数据依赖的空间衰减作为一种新范式，用于增强视觉Transformer中的空间注意力机制。"}}
{"id": "2508.09987", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.09987", "abs": "https://arxiv.org/abs/2508.09987", "authors": ["Junyan Ye", "Dongzhi Jiang", "Zihao Wang", "Leqi Zhu", "Zhenghao Hu", "Zilong Huang", "Jun He", "Zhiyuan Yan", "Jinghua Yu", "Hongsheng Li", "Conghui He", "Weijia Li"], "title": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation", "comment": "19 pages, 8 figures", "summary": "Recently, GPT-4o has garnered significant attention for its strong\nperformance in image generation, yet open-source models still lag behind.\nSeveral studies have explored distilling image data from GPT-4o to enhance\nopen-source models, achieving notable progress. However, a key question\nremains: given that real-world image datasets already constitute a natural\nsource of high-quality data, why should we use GPT-4o-generated synthetic data?\nIn this work, we identify two key advantages of synthetic images. First, they\ncan complement rare scenarios in real-world datasets, such as surreal fantasy\nor multi-reference image generation, which frequently occur in user queries.\nSecond, they provide clean and controllable supervision. Real-world data often\ncontains complex background noise and inherent misalignment between text\ndescriptions and image content, whereas synthetic images offer pure backgrounds\nand long-tailed supervision signals, facilitating more accurate text-to-image\nalignment. Building on these insights, we introduce Echo-4o-Image, a 180K-scale\nsynthetic dataset generated by GPT-4o, harnessing the power of synthetic image\ndata to address blind spots in real-world coverage. Using this dataset, we\nfine-tune the unified multimodal generation baseline Bagel to obtain Echo-4o.\nIn addition, we propose two new evaluation benchmarks for a more accurate and\nchallenging assessment of image generation capabilities: GenEval++, which\nincreases instruction complexity to mitigate score saturation, and\nImagine-Bench, which focuses on evaluating both the understanding and\ngeneration of imaginative content. Echo-4o demonstrates strong performance\nacross standard benchmarks. Moreover, applying Echo-4o-Image to other\nfoundation models (e.g., OmniGen2, BLIP3-o) yields consistent performance gains\nacross multiple metrics, highlighting the datasets strong transferability.", "AI": {"tldr": "该研究探讨了GPT-4o生成合成图像数据对提升开源图像生成模型的价值，并提出了一个大规模合成数据集Echo-4o-Image和两个新的评估基准。", "motivation": "尽管GPT-4o在图像生成方面表现出色，但开源模型仍有差距。现有研究试图从GPT-4o中蒸馏图像数据，但核心问题是：既然真实世界数据集已是高质量数据源，为何还要使用合成数据？作者旨在阐明合成图像的独特优势。", "method": "作者识别出合成图像的两大优势：1) 补充真实世界数据中稀有场景，如超现实幻想或多参考生成；2) 提供干净、可控的监督信号。基于此，他们构建了一个18万规模的GPT-4o生成合成数据集Echo-4o-Image。利用此数据集，他们微调了Bagel模型得到Echo-4o。此外，他们提出了两个新的评估基准：GenEval++（增加指令复杂性）和Imagine-Bench（评估想象力理解与生成）。", "result": "Echo-4o在标准基准测试上表现出色。将Echo-4o-Image数据集应用于其他基础模型（如OmniGen2, BLIP3-o）也持续带来了多项指标的性能提升，这表明该数据集具有强大的可迁移性。", "conclusion": "合成图像数据在图像生成领域具有独特价值，尤其能弥补真实世界数据在稀有场景覆盖和数据纯净度方面的不足。Echo-4o-Image数据集及其应用证明了合成数据能有效提升开源模型的性能和泛化能力。"}}
{"id": "2508.09956", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.09956", "abs": "https://arxiv.org/abs/2508.09956", "authors": ["Fares Antaki", "David Mikhail", "Daniel Milad", "Danny A Mammo", "Sumit Sharma", "Sunil K Srivastava", "Bing Yu Chen", "Samir Touma", "Mertcan Sevgi", "Jonathan El-Khoury", "Pearse A Keane", "Qingyu Chen", "Yih Chung Tham", "Renaud Duval"], "title": "Performance of GPT-5 Frontier Models in Ophthalmology Question Answering", "comment": null, "summary": "Large language models (LLMs) such as GPT-5 integrate advanced reasoning\ncapabilities that may improve performance on complex medical question-answering\ntasks. For this latest generation of reasoning models, the configurations that\nmaximize both accuracy and cost-efficiency have yet to be established. We\nevaluated 12 configurations of OpenAI's GPT-5 series (three model tiers across\nfour reasoning effort settings) alongside o1-high, o3-high, and GPT-4o, using\n260 closed-access multiple-choice questions from the American Academy of\nOphthalmology Basic Clinical Science Course (BCSC) dataset. The primary outcome\nwas multiple-choice accuracy; secondary outcomes included head-to-head ranking\nvia a Bradley-Terry model, rationale quality assessment using a\nreference-anchored, pairwise LLM-as-a-judge framework, and analysis of\naccuracy-cost trade-offs using token-based cost estimates. GPT-5-high achieved\nthe highest accuracy (0.965; 95% CI, 0.942-0.985), outperforming all GPT-5-nano\nvariants (P < .001), o1-high (P = .04), and GPT-4o (P < .001), but not o3-high\n(0.958; 95% CI, 0.931-0.981). GPT-5-high ranked first in both accuracy (1.66x\nstronger than o3-high) and rationale quality (1.11x stronger than o3-high).\nCost-accuracy analysis identified several GPT-5 configurations on the Pareto\nfrontier, with GPT-5-mini-low offering the most favorable low-cost,\nhigh-performance balance. These results benchmark GPT-5 on a high-quality\nophthalmology dataset, demonstrate the influence of reasoning effort on\naccuracy, and introduce an autograder framework for scalable evaluation of\nLLM-generated answers against reference standards in ophthalmology.", "AI": {"tldr": "研究评估了GPT-5系列模型在眼科医学问答任务上的性能、成本效益和推理能力，发现GPT-5-high表现最佳，同时识别出具有良好成本效益的配置。", "motivation": "大型语言模型（LLMs）如GPT-5具有高级推理能力，可能提高复杂医学问答任务的性能。然而，对于最新一代的推理模型，尚未确定能同时最大化准确性和成本效益的配置。", "method": "研究评估了OpenAI GPT-5系列的12种配置（三个模型层级，四种推理努力设置），以及o1-high、o3-high和GPT-4o。使用了来自美国眼科学会基础临床科学课程（BCSC）数据集的260个封闭式选择题。主要评估指标是多项选择题的准确性；次要指标包括通过Bradley-Terry模型进行的头对头排名、使用参考锚定、成对LLM-as-a-judge框架进行的理由质量评估，以及使用基于token的成本估算进行的准确性-成本权衡分析。", "result": "GPT-5-high的准确率最高（0.965），优于所有GPT-5-nano变体、o1-high和GPT-4o，但与o3-high（0.958）无显著差异。GPT-5-high在准确性（比o3-high强1.66倍）和理由质量（比o3-high强1.11倍）方面均排名第一。成本-准确性分析发现，GPT-5-mini-low在帕累托前沿上提供了最有利的低成本、高性能平衡。", "conclusion": "这些结果对GPT-5在高质量眼科数据集上的性能进行了基准测试，证明了推理努力对准确性的影响，并引入了一个自动评分框架，用于可扩展地评估LLM生成的答案与眼科参考标准的一致性。"}}
{"id": "2508.09528", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09528", "abs": "https://arxiv.org/abs/2508.09528", "authors": ["Gang Qu", "Ping Wang", "Siming Zheng", "Xin Yuan"], "title": "Physics-guided Deep Unfolding Network for Enhanced Kronecker Compressive sensing", "comment": "9 pages, 4 figures", "summary": "Deep networks have achieved remarkable success in image compressed sensing\n(CS) task, namely reconstructing a high-fidelity image from its compressed\nmeasurement. However, existing works are deficient inincoherent compressed\nmeasurement at sensing phase and implicit measurement representations at\nreconstruction phase, limiting the overall performance. In this work, we answer\ntwo questions: 1) how to improve the measurement incoherence for decreasing the\nill-posedness; 2) how to learn informative representations from measurements.\nTo this end, we propose a novel asymmetric Kronecker CS (AKCS) model and\ntheoretically present its better incoherence than previous Kronecker CS with\nminimal complexity increase. Moreover, we reveal that the unfolding networks'\nsuperiority over non-unfolding ones result from sufficient gradient descents,\ncalled explicit measurement representations. We propose a measurement-aware\ncross attention (MACA) mechanism to learn implicit measurement representations.\nWe integrate AKCS and MACA into widely-used unfolding architecture to get a\nmeasurement-enhanced unfolding network (MEUNet). Extensive experiences\ndemonstrate that our MEUNet achieves state-of-the-art performance in\nreconstruction accuracy and inference speed.", "AI": {"tldr": "该论文提出一种新的非对称克罗内克压缩感知（AKCS）模型和测量感知交叉注意力（MACA）机制，并将其集成到展开网络中形成MEUNet，显著提升了图像压缩感知重建的性能和速度。", "motivation": "现有图像压缩感知方法在感知阶段的测量不相干性和重建阶段的隐式测量表示方面存在不足，限制了整体性能。研究旨在解决如何提高测量不相干性以减少病态性，以及如何从测量中学习信息丰富的表示。", "method": ["提出一种新颖的非对称克罗内克压缩感知（AKCS）模型，并在理论上证明其比现有克罗内克CS具有更好的不相干性，且复杂度增加最小。", "揭示展开网络优于非展开网络的原因是充分的梯度下降，即显式测量表示。", "提出测量感知交叉注意力（MACA）机制，用于学习隐式测量表示。", "将AKCS和MACA集成到广泛使用的展开架构中，构建了测量增强展开网络（MEUNet）。"], "result": "所提出的MEUNet在重建精度和推理速度方面均达到了最先进的性能。", "conclusion": "通过改进测量不相干性和学习更丰富的测量表示，MEUNet显著提升了图像压缩感知重建的效果和效率。"}}
{"id": "2508.09957", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.09957", "abs": "https://arxiv.org/abs/2508.09957", "authors": ["Renas Adnan", "Hossein Hassani"], "title": "Which one Performs Better? Wav2Vec or Whisper? Applying both in Badini Kurdish Speech to Text (BKSTT)", "comment": "21 pages, 20 figures, 7 tables", "summary": "Speech-to-text (STT) systems have a wide range of applications. They are\navailable in many languages, albeit at different quality levels. Although\nKurdish is considered a less-resourced language from a processing perspective,\nSST is available for some of the Kurdish dialects, for instance, Sorani\n(Central Kurdish). However, that is not applied to other Kurdish dialects,\nBadini and Hawrami, for example. This research is an attempt to address this\ngap. Bandin, approximately, has two million speakers, and STT systems can help\ntheir community use mobile and computer-based technologies while giving their\ndialect more global visibility. We aim to create a language model based on\nBadini's speech and evaluate its performance. To cover a conversational aspect,\nhave a proper confidence level of grammatical accuracy, and ready\ntranscriptions, we chose Badini kids' stories, eight books including 78\nstories, as the textual input. Six narrators narrated the books, which resulted\nin approximately 17 hours of recording. We cleaned, segmented, and tokenized\nthe input. The preprocessing produced nearly 15 hours of speech, including\n19193 segments and 25221 words. We used Wav2Vec2-Large-XLSR-53 and\nWhisper-small to develop the language models. The experiments indicate that the\ntranscriptions process based on the Wav2Vec2-Large-XLSR-53 model provides a\nsignificantly more accurate and readable output than the Whisper-small model,\nwith 90.38% and 65.45% readability, and 82.67% and 53.17% accuracy,\nrespectively.", "AI": {"tldr": "本研究旨在为库尔德语的一种方言Badini开发语音转文本（STT）系统，并评估其性能。", "motivation": "尽管语音转文本系统已广泛应用，但对于Badini等资源匮乏的语言，尤其是其他库尔德语方言，STT系统仍存在空白。Badini拥有约两百万使用者，开发STT系统有助于其社区使用技术并提升方言的全球可见度。", "method": "研究选择了Badini儿童故事（8本书，78个故事）作为文本输入，并由6位叙述者录制，共计约17小时录音。数据经过清洗、分段和分词预处理，得到近15小时的语音数据，包含19193个片段和25221个单词。模型开发使用了Wav2Vec2-Large-XLSR-53和Whisper-small。", "result": "实验结果表明，基于Wav2Vec2-Large-XLSR-53模型的转录过程比Whisper-small模型提供了显著更准确和可读的输出。Wav2Vec2-Large-XLSR-53的可读性为90.38%，准确率为82.67%；而Whisper-small的可读性为65.45%，准确率为53.17%。", "conclusion": "本研究成功为Badini方言开发了语音转文本系统，并证明Wav2Vec2-Large-XLSR-53模型在Badini语音转录方面表现优于Whisper-small模型，有效填补了该语言的STT空白。"}}
{"id": "2508.09543", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09543", "abs": "https://arxiv.org/abs/2508.09543", "authors": ["Yuanting Gao", "Linghao Shen"], "title": "Iterative Volume Fusion for Asymmetric Stereo Matching", "comment": null, "summary": "Stereo matching is vital in 3D computer vision, with most algorithms assuming\nsymmetric visual properties between binocular visions. However, the rise of\nasymmetric multi-camera systems (e.g., tele-wide cameras) challenges this\nassumption and complicates stereo matching. Visual asymmetry disrupts stereo\nmatching by affecting the crucial cost volume computation. To address this, we\nexplore the matching cost distribution of two established cost volume\nconstruction methods in asymmetric stereo. We find that each cost volume\nexperiences distinct information distortion, indicating that both should be\ncomprehensively utilized to solve the issue. Based on this, we propose the\ntwo-phase Iterative Volume Fusion network for Asymmetric Stereo matching\n(IVF-AStereo). Initially, the aggregated concatenation volume refines the\ncorrelation volume. Subsequently, both volumes are fused to enhance fine\ndetails. Our method excels in asymmetric scenarios and shows robust performance\nagainst significant visual asymmetry. Extensive comparative experiments on\nbenchmark datasets, along with ablation studies, confirm the effectiveness of\nour approach in asymmetric stereo with resolution and color degradation.", "AI": {"tldr": "针对非对称双目视觉的立体匹配问题，本文提出了一种双阶段迭代体融合网络（IVF-AStereo），通过融合不同代价体来提升匹配精度，尤其适用于分辨率和颜色不对称的场景。", "motivation": "传统立体匹配算法假设双目视觉属性对称，但非对称多摄像头系统（如远摄-广角相机）打破了这一假设，导致立体匹配中的关键代价体计算受损，进而影响匹配精度。", "method": "1. 分析了两种现有代价体构建方法在非对称立体匹配中的匹配代价分布，发现它们各自存在信息失真。\n2. 提出了一种双阶段迭代体融合网络（IVF-AStereo）。\n3. 第一阶段：聚合连接体用于细化相关体。\n4. 第二阶段：将两个代价体融合以增强细节信息。", "result": "该方法在非对称场景中表现出色，对显著的视觉不对称具有鲁棒性。在基准数据集上的实验证明，该方法在处理分辨率和颜色降级的不对称立体匹配方面是有效的。", "conclusion": "通过综合利用和融合不同代价体的信息，所提出的IVF-AStereo网络能够有效解决非对称立体匹配所面临的挑战，并在实际应用中展现出优异的性能。"}}
{"id": "2508.09958", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.09958", "abs": "https://arxiv.org/abs/2508.09958", "authors": ["Baran Atalar", "Eddie Zhang", "Carlee Joe-Wong"], "title": "Neural Bandit Based Optimal LLM Selection for a Pipeline of Tasks", "comment": "Submitted to AAAI 2026", "summary": "With the increasing popularity of large language models (LLMs) for a variety\nof tasks, there has been a growing interest in strategies that can predict\nwhich out of a set of LLMs will yield a successful answer at low cost. This\nproblem promises to become more and more relevant as providers like Microsoft\nallow users to easily create custom LLM \"assistants\" specialized to particular\ntypes of queries. However, some tasks (i.e., queries) may be too specialized\nand difficult for a single LLM to handle alone. These applications often\nbenefit from breaking down the task into smaller subtasks, each of which can\nthen be executed by a LLM expected to perform well on that specific subtask.\nFor example, in extracting a diagnosis from medical records, one can first\nselect an LLM to summarize the record, select another to validate the summary,\nand then select another, possibly different, LLM to extract the diagnosis from\nthe summarized record. Unlike existing LLM selection or routing algorithms,\nthis setting requires that we select a sequence of LLMs, with the output of\neach LLM feeding into the next and potentially influencing its success. Thus,\nunlike single LLM selection, the quality of each subtask's output directly\naffects the inputs, and hence the cost and success rate, of downstream LLMs,\ncreating complex performance dependencies that must be learned and accounted\nfor during selection. We propose a neural contextual bandit-based algorithm\nthat trains neural networks that model LLM success on each subtask in an online\nmanner, thus learning to guide the LLM selections for the different subtasks,\neven in the absence of historical LLM performance data. Experiments on\ntelecommunications question answering and medical diagnosis prediction datasets\nillustrate the effectiveness of our proposed approach compared to other LLM\nselection algorithms.", "AI": {"tldr": "本文提出了一种基于神经上下文多臂老虎机的算法，用于在线选择一系列大型语言模型（LLMs）以解决复杂任务，其中每个LLM的输出作为下一个LLM的输入，以优化成功率和成本。", "motivation": "随着LLMs的普及，如何预测哪个LLM能以低成本成功完成任务变得越来越重要。对于过于专业和困难的任务，单个LLM难以独立处理，需要将任务分解为子任务并由不同的LLM执行。现有LLM选择或路由算法无法处理这种序列选择场景，其中每个LLM的输出会影响后续LLM的输入、成本和成功率，形成复杂的性能依赖关系。", "method": "提出了一种基于神经上下文多臂老虎机（neural contextual bandit）的算法。该算法在线训练神经网络，以建模LLM在每个子任务上的成功率，从而学习指导不同子任务的LLM选择，即使在缺乏历史LLM性能数据的情况下也能工作。", "result": "在电信问答和医疗诊断预测数据集上的实验表明，与现有其他LLM选择算法相比，所提出的方法是有效的。", "conclusion": "该算法能够有效地学习并指导复杂任务中LLM的序列选择，处理子任务间性能依赖性，从而提高成功率并控制成本，即使在没有历史数据的情况下也能表现良好。"}}
{"id": "2508.09550", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09550", "abs": "https://arxiv.org/abs/2508.09550", "authors": ["Haowen Wang", "Guowei Zhang", "Xiang Zhang", "Zeyuan Chen", "Haiyang Xu", "Dou Hoon Kwark", "Zhuowen Tu"], "title": "Exploring the Equivalence of Closed-Set Generative and Real Data Augmentation in Image Classification", "comment": null, "summary": "In this paper, we address a key scientific problem in machine learning: Given\na training set for an image classification task, can we train a generative\nmodel on this dataset to enhance the classification performance? (i.e.,\nclosed-set generative data augmentation). We start by exploring the\ndistinctions and similarities between real images and closed-set synthetic\nimages generated by advanced generative models. Through extensive experiments,\nwe offer systematic insights into the effective use of closed-set synthetic\ndata for augmentation. Notably, we empirically determine the equivalent scale\nof synthetic images needed for augmentation. In addition, we also show\nquantitative equivalence between the real data augmentation and open-set\ngenerative augmentation (generative models trained using data beyond the given\ntraining set). While it aligns with the common intuition that real images are\ngenerally preferred, our empirical formulation also offers a guideline to\nquantify the increased scale of synthetic data augmentation required to achieve\ncomparable image classification performance. Our results on natural and medical\nimage datasets further illustrate how this effect varies with the baseline\ntraining set size and the amount of synthetic data incorporated.", "AI": {"tldr": "研究了使用生成模型（闭集）进行数据增强以提升图像分类性能的可行性，并量化了合成数据与真实数据增强的等效性。", "motivation": "探讨在给定图像分类训练集的情况下，能否通过在该数据集上训练生成模型来增强分类性能（即闭集生成数据增强）。", "method": "通过大量实验，探究真实图像与先进生成模型生成的闭集合成图像之间的区别和相似性。系统性地分析了闭集合成数据用于增强的有效性，并经验性地确定了合成图像增强所需的等效规模。此外，还比较了真实数据增强与开集生成增强（使用给定训练集之外的数据训练的生成模型）之间的定量等效性。", "result": "提供了闭集合成数据有效用于增强的系统性见解，并经验性地确定了合成图像增强所需的等效规模。结果显示了真实数据增强与开集生成增强之间的定量等效性。研究还提出了一个指导方针，量化了为达到可比图像分类性能所需的合成数据增强的增加规模。在自然和医学图像数据集上的结果进一步说明了这种效果如何随基线训练集大小和合成数据量的变化而变化。", "conclusion": "尽管真实图像通常更受青睐，但本研究的经验公式提供了量化所需合成数据增强规模的指导，以实现与真实数据相当的图像分类性能。这为在特定条件下有效利用合成数据进行数据增强提供了实用依据。"}}
{"id": "2508.09555", "categories": ["cs.CV", "55N31, 55U10, 68U10, 68T07", "I.4.6; I.5.4; G.2.3"], "pdf": "https://arxiv.org/pdf/2508.09555", "abs": "https://arxiv.org/abs/2508.09555", "authors": ["Ahmet Öztel", "İsmet Karaca"], "title": "Topological Invariant-Based Iris Identification via Digital Homology and Machine Learning", "comment": "10 pages, 5 figures, includes visual abstract, focuses on topological\n  invariants for iris recognition", "summary": "Objective - This study presents a biometric identification method based on\ntopological invariants from 2D iris images, representing iris texture via\nformally defined digital homology and evaluating classification performance.\n  Methods - Each normalized iris image (48x482 pixels) is divided into grids\n(e.g., 6x54 or 3x27). For each subregion, we compute Betti0, Betti1, and their\nratio using a recent algorithm for homology groups in 2D digital images. The\nresulting invariants form a feature matrix used with logistic regression, KNN,\nand SVM (with PCA and 100 randomized repetitions). A convolutional neural\nnetwork (CNN) is trained on raw images for comparison.\n  Results - Logistic regression achieved 97.78 +/- 0.82% accuracy,\noutperforming CNN (96.44 +/- 1.32%) and other feature-based models. The\ntopological features showed high accuracy with low variance.\n  Conclusion - This is the first use of topological invariants from formal\ndigital homology for iris recognition. The method offers a compact,\ninterpretable, and accurate alternative to deep learning, useful when\nexplainability or limited data is important. Beyond iris recognition, it can\napply to other biometrics, medical imaging, materials science, remote sensing,\nand interpretable AI. It runs efficiently on CPU-only systems and produces\nrobust, explainable features valuable for security-critical domains.", "AI": {"tldr": "本研究提出一种基于虹膜图像拓扑不变量的生物识别方法，通过数字同源性计算Betti数并评估分类性能。", "motivation": "开发一种紧凑、可解释且准确的生物识别替代方案，尤其适用于需要可解释性或数据有限的场景，并作为深度学习的替代。", "method": "将归一化虹膜图像（48x482像素）分割成网格（如6x54或3x27），计算每个子区域的Betti0、Betti1及其比率，形成特征矩阵。使用逻辑回归、KNN和SVM（结合PCA和100次随机重复）进行分类，并与在原始图像上训练的卷积神经网络（CNN）进行比较。", "result": "逻辑回归取得了97.78 +/- 0.82%的准确率，优于CNN（96.44 +/- 1.32%）和其他基于特征的模型。拓扑特征表现出高准确率和低方差。", "conclusion": "这是首次将形式数字同源性的拓扑不变量用于虹膜识别。该方法提供了一种紧凑、可解释且准确的深度学习替代方案，适用于可解释性或数据有限的场景，并可应用于其他生物识别、医学成像、材料科学、遥感和可解释人工智能等领域。它能在纯CPU系统上高效运行，并产生稳健、可解释的特征，对安全关键领域具有重要价值。"}}
{"id": "2508.09565", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09565", "abs": "https://arxiv.org/abs/2508.09565", "authors": ["Ming Zhao", "Pingping Liu", "Tongshun Zhang", "Zhe Zhang"], "title": "WEC-DG: Multi-Exposure Wavelet Correction Method Guided by Degradation Description", "comment": null, "summary": "Multi-exposure correction technology is essential for restoring images\naffected by insufficient or excessive lighting, enhancing the visual experience\nby improving brightness, contrast, and detail richness. However, current\nmulti-exposure correction methods often encounter challenges in addressing\nintra-class variability caused by diverse lighting conditions, shooting\nenvironments, and weather factors, particularly when processing images captured\nat a single exposure level. To enhance the adaptability of these models under\ncomplex imaging conditions, this paper proposes a Wavelet-based Exposure\nCorrection method with Degradation Guidance (WEC-DG). Specifically, we\nintroduce a degradation descriptor within the Exposure Consistency Alignment\nModule (ECAM) at both ends of the processing pipeline to ensure exposure\nconsistency and achieve final alignment. This mechanism effectively addresses\nmiscorrected exposure anomalies caused by existing methods' failure to\nrecognize 'blurred' exposure degradation. Additionally, we investigate the\nlight-detail decoupling properties of the wavelet transform to design the\nExposure Restoration and Detail Reconstruction Module (EDRM), which processes\nlow-frequency information related to exposure enhancement before utilizing\nhigh-frequency information as a prior guide for reconstructing spatial domain\ndetails. This serial processing strategy guarantees precise light correction\nand enhances detail recovery. Extensive experiments conducted on multiple\npublic datasets demonstrate that the proposed method outperforms existing\nalgorithms, achieving significant performance improvements and validating its\neffectiveness and practical applicability.", "AI": {"tldr": "提出了一种基于小波和退化引导的多曝光校正方法WEC-DG，用于解决单曝光图像在复杂光照下的校正问题，通过降解描述符和光照细节解耦策略提升了校正精度和细节恢复。", "motivation": "现有曝光校正方法难以处理由不同光照、环境和天气导致的类内差异，尤其是在处理单曝光图像时，常因未能识别“模糊”的曝光退化而导致误校正，影响视觉体验。", "method": "本文提出WEC-DG方法。在处理流程两端的曝光一致性对齐模块（ECAM）中引入退化描述符，以确保曝光一致性并解决现有方法对“模糊”曝光退化识别失败导致的异常。此外，利用小波变换的光照-细节解耦特性设计了曝光恢复与细节重建模块（EDRM），先处理低频信息进行曝光增强，再利用高频信息作为先验指导重建空间域细节，采用串行处理策略。", "result": "在多个公共数据集上进行的广泛实验表明，所提出的方法优于现有算法，取得了显著的性能提升。", "conclusion": "所提出的WEC-DG方法有效且具有实际应用价值，能够显著改善图像的曝光校正效果和细节恢复能力。"}}
{"id": "2508.09566", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09566", "abs": "https://arxiv.org/abs/2508.09566", "authors": ["Haibo Jin", "Haoxuan Che", "Sunan He", "Hao Chen"], "title": "A Chain of Diagnosis Framework for Accurate and Explainable Radiology Report Generation", "comment": "Accepted to IEEE TMI", "summary": "Despite the progress of radiology report generation (RRG), existing works\nface two challenges: 1) The performances in clinical efficacy are\nunsatisfactory, especially for lesion attributes description; 2) the generated\ntext lacks explainability, making it difficult for radiologists to trust the\nresults. To address the challenges, we focus on a trustworthy RRG model, which\nnot only generates accurate descriptions of abnormalities, but also provides\nbasis of its predictions. To this end, we propose a framework named chain of\ndiagnosis (CoD), which maintains a chain of diagnostic process for clinically\naccurate and explainable RRG. It first generates question-answer (QA) pairs via\ndiagnostic conversation to extract key findings, then prompts a large language\nmodel with QA diagnoses for accurate generation. To enhance explainability, a\ndiagnosis grounding module is designed to match QA diagnoses and generated\nsentences, where the diagnoses act as a reference. Moreover, a lesion grounding\nmodule is designed to locate abnormalities in the image, further improving the\nworking efficiency of radiologists. To facilitate label-efficient training, we\npropose an omni-supervised learning strategy with clinical consistency to\nleverage various types of annotations from different datasets. Our efforts lead\nto 1) an omni-labeled RRG dataset with QA pairs and lesion boxes; 2) a\nevaluation tool for assessing the accuracy of reports in describing lesion\nlocation and severity; 3) extensive experiments to demonstrate the\neffectiveness of CoD, where it outperforms both specialist and generalist\nmodels consistently on two RRG benchmarks and shows promising explainability by\naccurately grounding generated sentences to QA diagnoses and images.", "AI": {"tldr": "该论文提出了一个名为“诊断链”（CoD）的框架，旨在生成临床准确且可解释的放射学报告，通过诊断问答和多模态对齐来提高报告质量和可信度。", "motivation": "现有放射学报告生成（RRG）模型面临两大挑战：1）临床效果不佳，尤其在描述病灶属性方面；2）生成的文本缺乏可解释性，导致放射科医生难以信任。", "method": "本文提出了CoD框架：1）通过诊断对话生成问答（QA）对以提取关键发现；2）利用QA诊断提示大型语言模型进行报告生成；3）设计诊断接地模块将QA诊断与生成句子匹配以增强可解释性；4）设计病灶接地模块定位图像中的异常以提高工作效率；5）提出一种具有临床一致性的全监督学习策略，以利用不同数据集的各类标注，并构建了一个包含QA对和病灶框的全标注RRG数据集；6）开发了一个评估工具来评估报告描述病灶位置和严重程度的准确性。", "result": "CoD框架在两个RRG基准测试中持续优于专业和通用模型；通过将生成的句子准确地接地到QA诊断和图像，展示了良好的可解释性；并贡献了一个全标注RRG数据集和评估工具。", "conclusion": "CoD框架能够生成临床准确且可解释的放射学报告，显著提高了报告的可靠性和放射科医生的工作效率，有望在实际临床应用中建立信任。"}}
{"id": "2508.09575", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09575", "abs": "https://arxiv.org/abs/2508.09575", "authors": ["Jiwon Kim", "Pureum Kim", "SeonHwa Kim", "Soobin Park", "Eunju Cha", "Kyong Hwan Jin"], "title": "Dual Recursive Feedback on Generation and Appearance Latents for Pose-Robust Text-to-Image Diffusion", "comment": null, "summary": "Recent advancements in controllable text-to-image (T2I) diffusion models,\nsuch as Ctrl-X and FreeControl, have demonstrated robust spatial and appearance\ncontrol without requiring auxiliary module training. However, these models\noften struggle to accurately preserve spatial structures and fail to capture\nfine-grained conditions related to object poses and scene layouts. To address\nthese challenges, we propose a training-free Dual Recursive Feedback (DRF)\nsystem that properly reflects control conditions in controllable T2I models.\nThe proposed DRF consists of appearance feedback and generation feedback that\nrecursively refines the intermediate latents to better reflect the given\nappearance information and the user's intent. This dual-update mechanism guides\nlatent representations toward reliable manifolds, effectively integrating\nstructural and appearance attributes. Our approach enables fine-grained\ngeneration even between class-invariant structure-appearance fusion, such as\ntransferring human motion onto a tiger's form. Extensive experiments\ndemonstrate the efficacy of our method in producing high-quality, semantically\ncoherent, and structurally consistent image generations. Our source code is\navailable at https://github.com/jwonkm/DRF.", "AI": {"tldr": "提出了一种名为DRF的无训练双递归反馈系统，以提升可控文本到图像（T2I）扩散模型在空间结构和精细条件控制方面的表现。", "motivation": "现有的可控T2I模型在准确保留空间结构和捕捉物体姿态、场景布局等精细条件时表现不佳。", "method": "提出了一种无训练的双递归反馈（DRF）系统，包含外观反馈和生成反馈。该系统递归地优化中间潜在变量，以更好地反映给定外观信息和用户意图，从而引导潜在表示向可靠流形靠拢，有效整合结构和外观属性。", "result": "该方法实现了精细的图像生成，即使是在类别无关的结构-外观融合场景（如将人类动作迁移到老虎形态上）也能表现良好。大量实验证明了该方法在生成高质量、语义连贯且结构一致的图像方面的有效性。", "conclusion": "DRF系统通过双重更新机制，有效解决了现有可控T2I模型在空间结构和精细条件控制上的不足，显著提升了图像生成的质量和一致性。"}}
{"id": "2508.09584", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09584", "abs": "https://arxiv.org/abs/2508.09584", "authors": ["Bei Yan", "Zhiyuan Chen", "Yuecong Min", "Jie Zhang", "Jiahao Wang", "Xiaozhen Wang", "Shiguang Shan"], "title": "SHALE: A Scalable Benchmark for Fine-grained Hallucination Evaluation in LVLMs", "comment": null, "summary": "Despite rapid advances, Large Vision-Language Models (LVLMs) still suffer\nfrom hallucinations, i.e., generating content inconsistent with input or\nestablished world knowledge, which correspond to faithfulness and factuality\nhallucinations, respectively. Prior studies primarily evaluate faithfulness\nhallucination at a coarse level (e.g., object-level) and lack fine-grained\nanalysis. Additionally, existing benchmarks rely on costly manual curation or\nreused public datasets, raising concerns about scalability and data leakage. To\naddress these limitations, we propose an automated data construction pipeline\nthat produces scalable, controllable, and diverse evaluation data. We also\ndesign a hierarchical hallucination induction framework with input\nperturbations to simulate realistic noisy scenarios. Integrating these designs,\nwe construct SHALE, a Scalable HALlucination Evaluation benchmark designed to\nassess both faithfulness and factuality hallucinations via a fine-grained\nhallucination categorization scheme. SHALE comprises over 30K image-instruction\npairs spanning 12 representative visual perception aspects for faithfulness and\n6 knowledge domains for factuality, considering both clean and noisy scenarios.\nExtensive experiments on over 20 mainstream LVLMs reveal significant factuality\nhallucinations and high sensitivity to semantic perturbations.", "AI": {"tldr": "本文提出了SHALE，一个可扩展的幻觉评估基准，用于细粒度地评估大型视觉-语言模型（LVLMs）的忠实性和事实性幻觉，并揭示了现有模型在这些方面的显著问题。", "motivation": "尽管LVLMs发展迅速，但仍存在幻觉问题（内容与输入不一致或与世界知识不符），现有研究主要在粗粒度层面评估忠实性幻觉，缺乏细粒度分析。此外，现有基准依赖昂贵的手动标注或重复使用公共数据集，存在可扩展性和数据泄露问题。", "method": "本文提出一个自动化数据构建流程，以生成可扩展、可控和多样化的评估数据。同时，设计了一个分层幻觉诱导框架，通过输入扰动模拟真实的噪声场景。结合这些设计，构建了SHALE基准，包含3万多对图像-指令，涵盖12个视觉感知方面和6个知识领域，用于细粒度评估忠实性和事实性幻觉。", "result": "在超过20个主流LVLMs上进行的广泛实验表明，这些模型存在显著的事实性幻觉，并且对语义扰动高度敏感。", "conclusion": "SHALE基准提供了一种可扩展且细粒度的方法来评估LVLMs的忠实性和事实性幻觉，实验结果揭示了当前LVLMs在处理幻觉和噪声输入方面的不足，为未来研究指明了方向。"}}
{"id": "2508.09597", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09597", "abs": "https://arxiv.org/abs/2508.09597", "authors": ["Heyi Sun", "Cong Wang", "Tian-Xing Xu", "Jingwei Huang", "Di Kang", "Chunchao Guo", "Song-Hai Zhang"], "title": "SVG-Head: Hybrid Surface-Volumetric Gaussians for High-Fidelity Head Reconstruction and Real-Time Editing", "comment": null, "summary": "Creating high-fidelity and editable head avatars is a pivotal challenge in\ncomputer vision and graphics, boosting many AR/VR applications. While recent\nadvancements have achieved photorealistic renderings and plausible animation,\nhead editing, especially real-time appearance editing, remains challenging due\nto the implicit representation and entangled modeling of the geometry and\nglobal appearance. To address this, we propose Surface-Volumetric Gaussian Head\nAvatar (SVG-Head), a novel hybrid representation that explicitly models the\ngeometry with 3D Gaussians bound on a FLAME mesh and leverages disentangled\ntexture images to capture the global appearance. Technically, it contains two\ntypes of Gaussians, in which surface Gaussians explicitly model the appearance\nof head avatars using learnable texture images, facilitating real-time texture\nediting, while volumetric Gaussians enhance the reconstruction quality of\nnon-Lambertian regions (e.g., lips and hair). To model the correspondence\nbetween 3D world and texture space, we provide a mesh-aware Gaussian UV mapping\nmethod, which leverages UV coordinates given by the FLAME mesh to obtain sharp\ntexture images and real-time rendering speed. A hierarchical optimization\nstrategy is further designed to pursue the optimal performance in both\nreconstruction quality and editing flexibility. Experiments on the NeRSemble\ndataset show that SVG-Head not only generates high-fidelity rendering results,\nbut also is the first method to obtain explicit texture images for Gaussian\nhead avatars and support real-time appearance editing.", "AI": {"tldr": "本文提出SVG-Head，一种混合表示方法，通过在FLAME网格上结合3D高斯和分离的纹理图像，实现了高保真、可编辑的头部虚拟形象，并支持实时外观编辑。", "motivation": "当前头部虚拟形象生成面临的挑战是，尽管能实现照片级真实感渲染和合理动画，但由于隐式表示和几何与全局外观的纠缠建模，头部编辑，特别是实时外观编辑，仍然十分困难。", "method": "SVG-Head采用混合表示，利用绑定在FLAME网格上的3D高斯显式建模几何，并利用分离的纹理图像捕捉全局外观。它包含两种高斯：表面高斯使用可学习纹理图像建模头部外观，支持实时纹理编辑；体素高斯增强非朗伯区域（如嘴唇和头发）的重建质量。此外，引入网格感知高斯UV映射方法，利用FLAME网格的UV坐标获取清晰纹理图像和实时渲染速度，并设计了分层优化策略以平衡重建质量和编辑灵活性。", "result": "在NeRSemble数据集上的实验表明，SVG-Head不仅生成了高保真渲染结果，而且是首个为高斯头部虚拟形象获得显式纹理图像并支持实时外观编辑的方法。", "conclusion": "SVG-Head通过其创新的混合表示和分离建模策略，成功解决了高保真头部虚拟形象的实时外观编辑难题，为AR/VR应用提供了新的可能性。"}}
{"id": "2508.09598", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09598", "abs": "https://arxiv.org/abs/2508.09598", "authors": ["Jie Shao", "Ke Zhu", "Minghao Fu", "Guo-hua Wang", "Jianxin Wu"], "title": "Images Speak Louder Than Scores: Failure Mode Escape for Enhancing Generative Quality", "comment": null, "summary": "Diffusion models have achieved remarkable progress in class-to-image\ngeneration. However, we observe that despite impressive FID scores,\nstate-of-the-art models often generate distorted or low-quality images,\nespecially in certain classes. This gap arises because FID evaluates global\ndistribution alignment, while ignoring the perceptual quality of individual\nsamples. We further examine the role of CFG, a common technique used to enhance\ngeneration quality. While effective in improving metrics and suppressing\noutliers, CFG can introduce distribution shift and visual artifacts due to its\nmisalignment with both training objectives and user expectations. In this work,\nwe propose FaME, a training-free and inference-efficient method for improving\nperceptual quality. FaME uses an image quality assessment model to identify\nlow-quality generations and stores their sampling trajectories. These failure\nmodes are then used as negative guidance to steer future sampling away from\npoor-quality regions. Experiments on ImageNet demonstrate that FaME brings\nconsistent improvements in visual quality without compromising FID. FaME also\nshows the potential to be extended to improve text-to-image generation.", "AI": {"tldr": "尽管FID得分很高，但扩散模型在生成图像时仍存在感知质量问题。本文提出了FaME，一种免训练且推理高效的方法，通过识别低质量样本并将其采样轨迹作为负向引导，显著提升了生成图像的视觉质量。", "motivation": "尽管扩散模型在类别到图像生成方面取得了显著进展，但作者观察到，尽管FID分数很高，最先进的模型仍常生成扭曲或低质量的图像，特别是在某些类别中。这是因为FID评估的是全局分布对齐，而忽略了单个样本的感知质量。此外，常用的CFG技术虽然能提高指标，但可能因与训练目标和用户期望不符而引入分布偏移和视觉伪影。", "method": "本文提出FaME，一种免训练且推理高效的方法，用于提高感知质量。FaME利用一个图像质量评估模型来识别低质量的生成图像，并存储它们的采样轨迹。这些“失败模式”随后被用作负向引导，以引导未来的采样远离低质量区域。", "result": "在ImageNet上的实验表明，FaME在不损害FID的情况下，持续改进了视觉质量。FaME还展示了扩展到改进文本到图像生成的潜力。", "conclusion": "FaME通过利用低质量生成样本的采样轨迹作为负向引导，有效地解决了扩散模型在感知质量上的不足。这是一种免训练且推理高效的方法，能够显著提升生成图像的视觉质量，同时保持整体分布的一致性。"}}
{"id": "2508.09599", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09599", "abs": "https://arxiv.org/abs/2508.09599", "authors": ["Beomjun Kim", "Suhan Woo", "Sejong Heo", "Euntai Kim"], "title": "BridgeTA: Bridging the Representation Gap in Knowledge Distillation via Teacher Assistant for Bird's Eye View Map Segmentation", "comment": "9 pages, 6 figures", "summary": "Bird's-Eye-View (BEV) map segmentation is one of the most important and\nchallenging tasks in autonomous driving. Camera-only approaches have drawn\nattention as cost-effective alternatives to LiDAR, but they still fall behind\nLiDAR-Camera (LC) fusion-based methods. Knowledge Distillation (KD) has been\nexplored to narrow this gap, but existing methods mainly enlarge the student\nmodel by mimicking the teacher's architecture, leading to higher inference\ncost. To address this issue, we introduce BridgeTA, a cost-effective\ndistillation framework to bridge the representation gap between LC fusion and\nCamera-only models through a Teacher Assistant (TA) network while keeping the\nstudent's architecture and inference cost unchanged. A lightweight TA network\ncombines the BEV representations of the teacher and student, creating a shared\nlatent space that serves as an intermediate representation. To ground the\nframework theoretically, we derive a distillation loss using Young's\nInequality, which decomposes the direct teacher-student distillation path into\nteacher-TA and TA-student dual paths, stabilizing optimization and\nstrengthening knowledge transfer. Extensive experiments on the challenging\nnuScenes dataset demonstrate the effectiveness of our method, achieving an\nimprovement of 4.2% mIoU over the Camera-only baseline, up to 45% higher than\nthe improvement of other state-of-the-art KD methods.", "AI": {"tldr": "BridgeTA是一种经济高效的知识蒸馏框架，通过引入教师助手（TA）网络，在不增加推理成本的情况下，弥合了激光雷达-相机融合模型与纯相机模型在鸟瞰图（BEV）分割任务中的性能差距。", "motivation": "纯相机方法在自动驾驶的BEV地图分割中仍落后于激光雷达-相机融合方法，而现有知识蒸馏（KD）方法通常通过扩大学生模型来缩小差距，导致推理成本增加。", "method": "引入BridgeTA框架，其中一个轻量级TA网络结合教师（LC融合）和学生（纯相机）模型的BEV表示，创建一个共享的潜在空间作为中间表示。理论上，利用Young不等式推导出一个蒸馏损失，将直接的师生蒸馏路径分解为教师-TA和TA-学生双路径，以稳定优化并加强知识传递。", "result": "在nuScenes数据集上，该方法使纯相机基线模型的mIoU提高了4.2%，比其他最先进的KD方法的提升高出45%。", "conclusion": "BridgeTA框架通过教师助手网络有效地弥合了纯相机与激光雷达-相机融合模型之间的表示差距，显著提升了纯相机BEV分割性能，同时保持了学生模型的架构和推理成本不变，实现了成本效益高的知识蒸馏。"}}
{"id": "2508.09626", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09626", "abs": "https://arxiv.org/abs/2508.09626", "authors": ["Xu Tang", "Junan Jia", "Yijing Wang", "Jingjing Ma", "Xiangrong Zhang"], "title": "Semantic-aware DropSplat: Adaptive Pruning of Redundant Gaussians for 3D Aerial-View Segmentation", "comment": "9 pages, 4 figures, AAAI 2026", "summary": "In the task of 3D Aerial-view Scene Semantic Segmentation (3D-AVS-SS),\ntraditional methods struggle to address semantic ambiguity caused by scale\nvariations and structural occlusions in aerial images. This limits their\nsegmentation accuracy and consistency. To tackle these challenges, we propose a\nnovel 3D-AVS-SS approach named SAD-Splat. Our method introduces a Gaussian\npoint drop module, which integrates semantic confidence estimation with a\nlearnable sparsity mechanism based on the Hard Concrete distribution. This\nmodule effectively eliminates redundant and semantically ambiguous Gaussian\npoints, enhancing both segmentation performance and representation compactness.\nFurthermore, SAD-Splat incorporates a high-confidence pseudo-label generation\npipeline. It leverages 2D foundation models to enhance supervision when\nground-truth labels are limited, thereby further improving segmentation\naccuracy. To advance research in this domain, we introduce a challenging\nbenchmark dataset: 3D Aerial Semantic (3D-AS), which encompasses diverse\nreal-world aerial scenes with sparse annotations. Experimental results\ndemonstrate that SAD-Splat achieves an excellent balance between segmentation\naccuracy and representation compactness. It offers an efficient and scalable\nsolution for 3D aerial scene understanding.", "AI": {"tldr": "该论文提出了SAD-Splat方法，用于解决3D航空场景语义分割中由尺度变化和遮挡引起的语义模糊问题，通过引入高斯点丢弃模块和伪标签生成管道，提高了分割精度和表示紧凑性，并发布了3D-AS数据集。", "motivation": "传统方法在3D航空场景语义分割任务中，难以解决由尺度变化和结构性遮挡导致的语义模糊问题，限制了分割精度和一致性。", "method": "提出了SAD-Splat方法，包含：1) 高斯点丢弃模块：结合语义置信度估计和基于Hard Concrete分布的可学习稀疏机制，消除冗余和语义模糊的高斯点。2) 高置信度伪标签生成管道：利用2D基础模型在真实标签有限时增强监督。此外，还引入了3D航空语义(3D-AS)基准数据集。", "result": "实验结果表明，SAD-Splat在分割精度和表示紧凑性之间取得了出色的平衡。", "conclusion": "SAD-Splat为3D航空场景理解提供了一种高效且可扩展的解决方案。"}}
{"id": "2508.09629", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09629", "abs": "https://arxiv.org/abs/2508.09629", "authors": ["Giorgos Karvounas", "Nikolaos Kyriazis", "Iason Oikonomidis", "Georgios Pavlakos", "Antonis A. Argyros"], "title": "Enhancing Monocular 3D Hand Reconstruction with Learned Texture Priors", "comment": null, "summary": "We revisit the role of texture in monocular 3D hand reconstruction, not as an\nafterthought for photorealism, but as a dense, spatially grounded cue that can\nactively support pose and shape estimation. Our observation is simple: even in\nhigh-performing models, the overlay between predicted hand geometry and image\nappearance is often imperfect, suggesting that texture alignment may be an\nunderused supervisory signal. We propose a lightweight texture module that\nembeds per-pixel observations into UV texture space and enables a novel dense\nalignment loss between predicted and observed hand appearances. Our approach\nassumes access to a differentiable rendering pipeline and a model that maps\nimages to 3D hand meshes with known topology, allowing us to back-project a\ntextured hand onto the image and perform pixel-based alignment. The module is\nself-contained and easily pluggable into existing reconstruction pipelines. To\nisolate and highlight the value of texture-guided supervision, we augment\nHaMeR, a high-performing yet unadorned transformer architecture for 3D hand\npose estimation. The resulting system improves both accuracy and realism,\ndemonstrating the value of appearance-guided alignment in hand reconstruction.", "AI": {"tldr": "该研究重新审视纹理在单目3D手部重建中的作用，将其作为支持姿态和形状估计的密集空间线索，并提出了一个轻量级纹理模块和新的密集对齐损失，以提高重建精度和真实感。", "motivation": "现有高性能模型中，预测的手部几何形状与图像外观的叠加往往不完美，表明纹理对齐可能是一个未被充分利用的监督信号。纹理通常被视为光照真实感的附加物，而非主动支持姿态和形状估计的关键线索。", "method": "提出一个轻量级纹理模块，将每像素观察嵌入到UV纹理空间。引入一种新颖的、预测与观察手部外观之间的密集对齐损失。该方法假设可访问可微分渲染管线和具有已知拓扑的3D手部网格模型，从而能够将纹理手部反投影到图像上并执行基于像素的对齐。该模块可独立并轻松集成到现有重建流程中。通过增强高性能的HaMeR模型来验证其有效性。", "result": "整合纹理模块后的系统显著提高了3D手部重建的准确性和真实感。", "conclusion": "外观引导的对齐在手部重建中具有重要价值，纹理可以作为一种有效的监督信号来主动支持手部姿态和形状估计。"}}
{"id": "2508.09644", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09644", "abs": "https://arxiv.org/abs/2508.09644", "authors": ["Shengjun Zhu", "Siyu Liu", "Runqing Xiong", "Liping Zheng", "Duo Ma", "Rongshang Chen", "Jiaxin Cai"], "title": "Multi-Contrast Fusion Module: An attention mechanism integrating multi-contrast features for fetal torso plane classification", "comment": null, "summary": "Purpose: Prenatal ultrasound is a key tool in evaluating fetal structural\ndevelopment and detecting abnormalities, contributing to reduced perinatal\ncomplications and improved neonatal survival. Accurate identification of\nstandard fetal torso planes is essential for reliable assessment and\npersonalized prenatal care. However, limitations such as low contrast and\nunclear texture details in ultrasound imaging pose significant challenges for\nfine-grained anatomical recognition. Methods: We propose a novel Multi-Contrast\nFusion Module (MCFM) to enhance the model's ability to extract detailed\ninformation from ultrasound images. MCFM operates exclusively on the lower\nlayers of the neural network, directly processing raw ultrasound data. By\nassigning attention weights to image representations under different contrast\nconditions, the module enhances feature modeling while explicitly maintaining\nminimal parameter overhead. Results: The proposed MCFM was evaluated on a\ncurated dataset of fetal torso plane ultrasound images. Experimental results\ndemonstrate that MCFM substantially improves recognition performance, with a\nminimal increase in model complexity. The integration of multi-contrast\nattention enables the model to better capture subtle anatomical structures,\ncontributing to higher classification accuracy and clinical reliability.\nConclusions: Our method provides an effective solution for improving fetal\ntorso plane recognition in ultrasound imaging. By enhancing feature\nrepresentation through multi-contrast fusion, the proposed approach supports\nclinicians in achieving more accurate and consistent diagnoses, demonstrating\nstrong potential for clinical adoption in prenatal screening. The codes are\navailable at https://github.com/sysll/MCFM.", "AI": {"tldr": "该研究提出了一种多对比度融合模块（MCFM），用于提高超声图像中胎儿躯干平面的识别准确性，以克服低对比度和纹理模糊的挑战。", "motivation": "产前超声在评估胎儿结构发育和检测异常方面至关重要，但超声图像的低对比度和不清晰纹理细节给精细解剖识别带来了挑战。准确识别标准胎儿躯干平面对于可靠评估和个性化产前护理至关重要。", "method": "提出了一种新颖的多对比度融合模块（MCFM），专用于神经网络的低层，直接处理原始超声数据。MCFM通过为不同对比度条件下的图像表示分配注意力权重，增强特征建模，同时保持最小的参数开销。", "result": "MCFM在胎儿躯干平面超声图像数据集上进行了评估，实验结果表明，MCFM显著提高了识别性能，且模型复杂度增加最小。多对比度注意力的整合使模型能更好地捕捉细微的解剖结构，从而提高了分类准确性和临床可靠性。", "conclusion": "该方法为改善超声成像中的胎儿躯干平面识别提供了一个有效的解决方案。通过多对比度融合增强特征表示，所提出的方法支持临床医生实现更准确和一致的诊断，在产前筛查中显示出强大的临床应用潜力。"}}
{"id": "2508.09645", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09645", "abs": "https://arxiv.org/abs/2508.09645", "authors": ["Zhongyuan Wu", "Chuan-Xian Ren", "Yu Wang", "Xiaohua Ban", "Jianning Xiao", "Xiaohui Duan"], "title": "Multi-Sequence Parotid Gland Lesion Segmentation via Expert Text-Guided Segment Anything Model", "comment": null, "summary": "Parotid gland lesion segmentation is essential for the treatment of parotid\ngland diseases. However, due to the variable size and complex lesion\nboundaries, accurate parotid gland lesion segmentation remains challenging.\nRecently, the Segment Anything Model (SAM) fine-tuning has shown remarkable\nperformance in the field of medical image segmentation. Nevertheless, SAM's\ninteraction segmentation model relies heavily on precise lesion prompts\n(points, boxes, masks, etc.), which are very difficult to obtain in real-world\napplications. Besides, current medical image segmentation methods are\nautomatically generated, ignoring the domain knowledge of medical experts when\nperforming segmentation. To address these limitations, we propose the parotid\ngland segment anything model (PG-SAM), an expert diagnosis text-guided SAM\nincorporating expert domain knowledge for cross-sequence parotid gland lesion\nsegmentation. Specifically, we first propose an expert diagnosis report guided\nprompt generation module that can automatically generate prompt information\ncontaining the prior domain knowledge to guide the subsequent lesion\nsegmentation process. Then, we introduce a cross-sequence attention module,\nwhich integrates the complementary information of different modalities to\nenhance the segmentation effect. Finally, the multi-sequence image features and\ngenerated prompts are feed into the decoder to get segmentation result.\nExperimental results demonstrate that PG-SAM achieves state-of-the-art\nperformance in parotid gland lesion segmentation across three independent\nclinical centers, validating its clinical applicability and the effectiveness\nof diagnostic text for enhancing image segmentation in real-world clinical\nsettings.", "AI": {"tldr": "本文提出了PG-SAM模型，一个由专家诊断文本引导的SAM模型，用于跨序列腮腺病变分割，解决了SAM对精确提示的依赖以及现有方法忽略专家领域知识的问题，并取得了最先进的性能。", "motivation": "腮腺病变分割因病变大小多变和边界复杂而具有挑战性；现有SAM模型在医学图像分割中表现出色但高度依赖难以获取的精确病变提示；当前医学图像分割方法自动生成，忽略了医学专家的领域知识。", "method": "提出了腮腺分割一切模型（PG-SAM），一个专家诊断文本引导的SAM，整合了专家领域知识进行跨序列腮腺病变分割。具体包括：1. 专家诊断报告引导的提示生成模块，自动生成包含先验领域知识的提示。2. 跨序列注意力模块，整合不同模态的互补信息以增强分割效果。3. 将多序列图像特征和生成的提示输入解码器以获得分割结果。", "result": "实验结果表明PG-SAM在三个独立的临床中心实现了腮腺病变分割的最先进性能，验证了其临床适用性以及诊断文本在真实临床环境中增强图像分割的有效性。", "conclusion": "PG-SAM通过结合专家诊断文本和跨序列信息，有效解决了腮腺病变分割中的挑战，并证明了诊断文本在提升医学图像分割方面的价值和临床实用性。"}}
{"id": "2508.09649", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09649", "abs": "https://arxiv.org/abs/2508.09649", "authors": ["Reuben Dorent", "Laura Rigolo", "Colin P. Galvin", "Junyu Chen", "Mattias P. Heinrich", "Aaron Carass", "Olivier Colliot", "Demian Wassermann", "Alexandra Golby", "Tina Kapur", "William Wells"], "title": "The Brain Resection Multimodal Image Registration (ReMIND2Reg) 2025 Challenge", "comment": null, "summary": "Accurate intraoperative image guidance is critical for achieving maximal safe\nresection in brain tumor surgery, yet neuronavigation systems based on\npreoperative MRI lose accuracy during the procedure due to brain shift.\nAligning post-resection intraoperative ultrasound (iUS) with preoperative MRI\ncan restore spatial accuracy by estimating brain shift deformations, but it\nremains a challenging problem given the large anatomical and topological\nchanges and substantial modality intensity gap. The ReMIND2Reg 2025 Challenge\nprovides the largest public benchmark for this task, built upon the ReMIND\ndataset. It offers 99 training cases, 5 validation cases, and 10 private test\ncases comprising paired 3D ceT1 MRI, T2 MRI, and post-resection 3D iUS volumes.\nData are provided without annotations for training, while validation and test\nperformance are evaluated on manually annotated anatomical landmarks. Metrics\ninclude target registration error (TRE), robustness to worst-case landmark\nmisalignment (TRE30), and runtime. By establishing a standardized evaluation\nframework for this clinically critical and technically complex problem,\nReMIND2Reg aims to accelerate the development of robust, generalizable, and\nclinically deployable multimodal registration algorithms for image-guided\nneurosurgery.", "AI": {"tldr": "ReMIND2Reg 2025挑战赛提供了一个大型公共基准数据集，用于解决脑肿瘤手术中术后超声（iUS）与术前MRI配准以校正脑移位的问题，旨在加速多模态配准算法的开发。", "motivation": "术前MRI引导的神经导航系统在脑肿瘤手术中会因脑移位而失去准确性，这危及最大安全切除。通过配准术后iUS与术前MRI可以校正脑移位，但由于解剖和拓扑变化以及模态间强度差异，这是一个具有挑战性的问题。", "method": "ReMIND2Reg 2025挑战赛基于ReMIND数据集，提供99个训练案例、5个验证案例和10个私人测试案例，包含配对的3D ceT1 MRI、T2 MRI和术后3D iUS数据。训练数据无标注，验证和测试性能通过手动标注的解剖地标进行评估。评估指标包括目标配准误差（TRE）、最差情况地标错位鲁棒性（TRE30）和运行时间。", "result": "该挑战赛旨在建立一个标准化评估框架，以加速开发稳健、通用且可临床部署的多模态配准算法，用于图像引导的神经外科手术。", "conclusion": "ReMIND2Reg挑战赛为解决临床关键且技术复杂的脑移位校正问题提供了重要平台，有望推动图像引导神经外科领域的技术进步。"}}
{"id": "2508.09650", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09650", "abs": "https://arxiv.org/abs/2508.09650", "authors": ["Hao Xu", "Arbind Agrahari Baniya", "Sam Wells", "Mohamed Reda Bouadjenek", "Richard Dazely", "Sunil Aryal"], "title": "TOTNet: Occlusion-Aware Temporal Tracking for Robust Ball Detection in Sports Videos", "comment": "8 pages, 6 figures,", "summary": "Robust ball tracking under occlusion remains a key challenge in sports video\nanalysis, affecting tasks like event detection and officiating. We present\nTOTNet, a Temporal Occlusion Tracking Network that leverages 3D convolutions,\nvisibility-weighted loss, and occlusion augmentation to improve performance\nunder partial and full occlusions. Developed in collaboration with Paralympics\nAustralia, TOTNet is designed for real-world sports analytics. We introduce\nTTA, a new occlusion-rich table tennis dataset collected from\nprofessional-level Paralympic matches, comprising 9,159 samples with 1,996\nocclusion cases. Evaluated on four datasets across tennis, badminton, and table\ntennis, TOTNet significantly outperforms prior state-of-the-art methods,\nreducing RMSE from 37.30 to 7.19 and improving accuracy on fully occluded\nframes from 0.63 to 0.80. These results demonstrate TOTNets effectiveness for\noffline sports analytics in fast-paced scenarios. Code and data\naccess:\\href{https://github.com/AugustRushG/TOTNet}{AugustRushG/TOTNet}.", "AI": {"tldr": "该研究提出了TOTNet，一个利用3D卷积、可见性加权损失和遮挡增强的深度学习网络，旨在解决体育视频中球在遮挡下的鲁棒跟踪问题，并在多个数据集上显著优于现有SOTA方法。", "motivation": "体育视频分析中，球在遮挡情况下的鲁棒跟踪是一个关键挑战，直接影响事件检测和裁判判罚等任务的准确性。", "method": "提出了TOTNet（Temporal Occlusion Tracking Network），该网络采用3D卷积、可见性加权损失和遮挡增强技术来提升在部分和完全遮挡情况下的性能。同时，研究还引入了一个新的、富含遮挡的乒乓球数据集TTA，包含9,159个样本和1,996个遮挡案例。", "result": "TOTNet在网球、羽毛球和乒乓球的四个数据集上进行了评估，结果显示其显著优于现有最先进方法，将均方根误差（RMSE）从37.30降低到7.19，并将完全遮挡帧的准确率从0.63提高到0.80。", "conclusion": "这些结果证明了TOTNet在快速变化的场景中进行离线体育分析的有效性，成功解决了球在遮挡下的跟踪难题。"}}
{"id": "2508.09655", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09655", "abs": "https://arxiv.org/abs/2508.09655", "authors": ["Lianfang Wang", "Kuilin Qin", "Xueying Liu", "Huibin Chang", "Yong Wang", "Yuping Duan"], "title": "Noise-adapted Neural Operator for Robust Non-Line-of-Sight Imaging", "comment": null, "summary": "Computational imaging, especially non-line-of-sight (NLOS) imaging, the\nextraction of information from obscured or hidden scenes is achieved through\nthe utilization of indirect light signals resulting from multiple reflections\nor scattering. The inherently weak nature of these signals, coupled with their\nsusceptibility to noise, necessitates the integration of physical processes to\nensure accurate reconstruction. This paper presents a parameterized inverse\nproblem framework tailored for large-scale linear problems in 3D imaging\nreconstruction. Initially, a noise estimation module is employed to adaptively\nassess the noise levels present in transient data. Subsequently, a\nparameterized neural operator is developed to approximate the inverse mapping,\nfacilitating end-to-end rapid image reconstruction. Our 3D image reconstruction\nframework, grounded in operator learning, is constructed through deep algorithm\nunfolding, which not only provides commendable model interpretability but also\nenables dynamic adaptation to varying noise levels in the acquired data,\nthereby ensuring consistently robust and accurate reconstruction outcomes.\nFurthermore, we introduce a novel method for the fusion of global and local\nspatiotemporal data features. By integrating structural and detailed\ninformation, this method significantly enhances both accuracy and robustness.\nComprehensive numerical experiments conducted on both simulated and real\ndatasets substantiate the efficacy of the proposed method. It demonstrates\nremarkable performance with fast scanning data and sparse illumination point\ndata, offering a viable solution for NLOS imaging in complex scenarios.", "AI": {"tldr": "针对非视距（NLOS）成像中信号弱、易受噪声干扰的问题，本文提出了一个参数化的逆问题框架，结合噪声估计、参数化神经算子和深度算法展开，并引入全局局部特征融合，实现了快速、鲁棒、准确的3D图像重建。", "motivation": "非视距（NLOS）成像中的间接光信号固有的微弱性及其对噪声的敏感性，使得精确的图像重建面临挑战，需要结合物理过程进行处理。", "method": "本文提出了一个为大规模线性3D成像重建定制的参数化逆问题框架。该框架首先采用噪声估计模块自适应评估瞬态数据中的噪声水平；随后开发了一个参数化神经算子来近似逆映射，实现端到端快速图像重建；通过深度算法展开构建基于算子学习的3D图像重建框架；此外，还引入了一种新的全局和局部时空数据特征融合方法，以整合结构和细节信息。", "result": "所提出的方法在模型可解释性、动态适应不同噪声水平以及确保鲁棒和准确的重建结果方面表现出色。通过整合结构和细节信息，显著提高了准确性和鲁棒性。在模拟和真实数据集上的综合数值实验验证了其有效性，并展示了对快速扫描数据和稀疏照明点数据的卓越性能。", "conclusion": "该方法为复杂场景下的非视距（NLOS）成像提供了一个可行的解决方案，通过结合物理过程和深度学习，实现了高效、准确、鲁棒的3D图像重建。"}}
{"id": "2508.09661", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09661", "abs": "https://arxiv.org/abs/2508.09661", "authors": ["Eduarda Caldeira", "Naser Damer", "Fadi Boutros"], "title": "NegFaceDiff: The Power of Negative Context in Identity-Conditioned Diffusion for Synthetic Face Generation", "comment": "Accepted at ICCV Workshops", "summary": "The use of synthetic data as an alternative to authentic datasets in face\nrecognition (FR) development has gained significant attention, addressing\nprivacy, ethical, and practical concerns associated with collecting and using\nauthentic data. Recent state-of-the-art approaches have proposed\nidentity-conditioned diffusion models to generate identity-consistent face\nimages, facilitating their use in training FR models. However, these methods\noften lack explicit sampling mechanisms to enforce inter-class separability,\nleading to identity overlap in the generated data and, consequently, suboptimal\nFR performance. In this work, we introduce NegFaceDiff, a novel sampling method\nthat incorporates negative conditions into the identity-conditioned diffusion\nprocess. NegFaceDiff enhances identity separation by leveraging negative\nconditions that explicitly guide the model away from unwanted features while\npreserving intra-class consistency. Extensive experiments demonstrate that\nNegFaceDiff significantly improves the identity consistency and separability of\ndata generated by identity-conditioned diffusion models. Specifically, identity\nseparability, measured by the Fisher Discriminant Ratio (FDR), increases from\n2.427 to 5.687. These improvements are reflected in FR systems trained on the\nNegFaceDiff dataset, which outperform models trained on data generated without\nnegative conditions across multiple benchmarks.", "AI": {"tldr": "针对面部识别（FR）中合成数据存在的身份重叠问题，本文提出了NegFaceDiff方法。该方法在身份条件扩散模型中引入负条件采样机制，显著提升了生成数据的身份可分离性，从而提高了基于这些数据训练的FR模型的性能。", "motivation": "为解决真实面部数据在收集和使用中存在的隐私、伦理和实际问题，合成数据被视为一种替代方案。然而，现有基于身份条件扩散模型的合成数据生成方法，缺乏明确的采样机制来强制执行类间可分离性，导致生成数据中存在身份重叠，进而影响FR模型的性能。", "method": "本文提出了一种名为NegFaceDiff的新型采样方法。该方法将负条件引入到身份条件扩散过程中，通过明确引导模型远离不需要的特征，同时保持类内一致性，从而增强了身份分离性。", "result": "实验结果表明，NegFaceDiff显著改善了身份条件扩散模型生成数据的身份一致性和可分离性。具体而言，通过Fisher判别比（FDR）衡量的身份可分离性从2.427提高到5.687。在NegFaceDiff数据集上训练的FR系统，在多个基准测试中均优于未引入负条件生成数据训练的模型。", "conclusion": "NegFaceDiff通过引入负条件采样机制，有效解决了合成面部数据中的身份重叠问题，显著提高了生成数据的身份可分离性，从而使基于这些数据训练的面部识别系统表现出更优越的性能。"}}
{"id": "2508.09667", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09667", "abs": "https://arxiv.org/abs/2508.09667", "authors": ["Xingyilang Yin", "Qi Zhang", "Jiahao Chang", "Ying Feng", "Qingnan Fan", "Xi Yang", "Chi-Man Pun", "Huaqi Zhang", "Xiaodong Cun"], "title": "GSFixer: Improving 3D Gaussian Splatting with Reference-Guided Video Diffusion Priors", "comment": null, "summary": "Reconstructing 3D scenes using 3D Gaussian Splatting (3DGS) from sparse views\nis an ill-posed problem due to insufficient information, often resulting in\nnoticeable artifacts. While recent approaches have sought to leverage\ngenerative priors to complete information for under-constrained regions, they\nstruggle to generate content that remains consistent with input observations.\nTo address this challenge, we propose GSFixer, a novel framework designed to\nimprove the quality of 3DGS representations reconstructed from sparse inputs.\nThe core of our approach is the reference-guided video restoration model, built\nupon a DiT-based video diffusion model trained on paired artifact 3DGS renders\nand clean frames with additional reference-based conditions. Considering the\ninput sparse views as references, our model integrates both 2D semantic\nfeatures and 3D geometric features of reference views extracted from the visual\ngeometry foundation model, enhancing the semantic coherence and 3D consistency\nwhen fixing artifact novel views. Furthermore, considering the lack of suitable\nbenchmarks for 3DGS artifact restoration evaluation, we present DL3DV-Res which\ncontains artifact frames rendered using low-quality 3DGS. Extensive experiments\ndemonstrate our GSFixer outperforms current state-of-the-art methods in 3DGS\nartifact restoration and sparse-view 3D reconstruction. Project page:\nhttps://github.com/GVCLab/GSFixer.", "AI": {"tldr": "GSFixer是一个新颖的框架，利用基于DiT的视频扩散模型和参考引导机制，有效修复由稀疏视图重建的3D Gaussian Splatting (3DGS) 中的伪影，并提升3D重建质量。", "motivation": "从稀疏视图重建3D场景是一个病态问题，导致3DGS重建出现明显伪影。现有生成式方法在补全信息时难以保持与输入观测的一致性，因此需要一种能有效修复伪影并保持一致性的方法。", "method": "核心方法是参考引导的视频修复模型，该模型基于一个DiT（Diffusion Transformer）视频扩散模型，并在成对的伪影3DGS渲染图和干净帧上进行训练，并增加了基于参考的条件。该模型将输入稀疏视图作为参考，整合了从视觉几何基础模型中提取的2D语义特征和3D几何特征，以增强修复新视图时的语义连贯性和3D一致性。此外，本文还提出了DL3DV-Res数据集作为3DGS伪影修复的基准。", "result": "广泛的实验表明，GSFixer在3DGS伪影修复和稀疏视图3D重建方面均优于当前最先进的方法。", "conclusion": "GSFixer通过其独特的参考引导视频修复模型，有效解决了稀疏视图下3DGS重建中的伪影问题，显著提升了重建质量，并为相关研究提供了新的评估基准。"}}
{"id": "2508.09691", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09691", "abs": "https://arxiv.org/abs/2508.09691", "authors": ["Yin Xie", "Zhichao Chen", "Xiaoze Yu", "Yongle Zhao", "Xiang An", "Kaicheng Yang", "Zimin Ran", "Jia Guo", "Ziyong Feng", "Jiankang Deng"], "title": "PaCo-FR: Patch-Pixel Aligned End-to-End Codebook Learning for Facial Representation Pre-training", "comment": null, "summary": "Facial representation pre-training is crucial for tasks like facial\nrecognition, expression analysis, and virtual reality. However, existing\nmethods face three key challenges: (1) failing to capture distinct facial\nfeatures and fine-grained semantics, (2) ignoring the spatial structure\ninherent to facial anatomy, and (3) inefficiently utilizing limited labeled\ndata. To overcome these, we introduce PaCo-FR, an unsupervised framework that\ncombines masked image modeling with patch-pixel alignment. Our approach\nintegrates three innovative components: (1) a structured masking strategy that\npreserves spatial coherence by aligning with semantically meaningful facial\nregions, (2) a novel patch-based codebook that enhances feature discrimination\nwith multiple candidate tokens, and (3) spatial consistency constraints that\npreserve geometric relationships between facial components. PaCo-FR achieves\nstate-of-the-art performance across several facial analysis tasks with just 2\nmillion unlabeled images for pre-training. Our method demonstrates significant\nimprovements, particularly in scenarios with varying poses, occlusions, and\nlighting conditions. We believe this work advances facial representation\nlearning and offers a scalable, efficient solution that reduces reliance on\nexpensive annotated datasets, driving more effective facial analysis systems.", "AI": {"tldr": "PaCo-FR是一个无监督面部表示预训练框架，结合了掩码图像建模和块像素对齐，通过结构化掩码、基于块的代码本和空间一致性约束，解决了现有方法在捕获面部特征、空间结构和数据利用方面的不足，实现了SOTA性能。", "motivation": "现有面部表示预训练方法面临三个挑战：1) 未能捕获独特的面部特征和细粒度语义；2) 忽略了面部解剖固有的空间结构；3) 无法有效利用有限的标注数据。", "method": "引入无监督框架PaCo-FR，结合了掩码图像建模和块像素对齐。它包含三个创新组件：1) 与语义面部区域对齐的结构化掩码策略，以保持空间连贯性；2) 具有多个候选tokens的新型基于块的代码本，以增强特征区分度；3) 空间一致性约束，以保持面部组件间的几何关系。", "result": "PaCo-FR仅使用200万张未标注图像进行预训练，就在多项面部分析任务上实现了最先进的性能。该方法在不同姿态、遮挡和光照条件下表现出显著改进。", "conclusion": "该工作推动了面部表示学习的进展，并提供了一个可扩展、高效的解决方案，减少了对面部分析系统中昂贵标注数据集的依赖，从而推动了更有效的系统发展。"}}
{"id": "2508.09699", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09699", "abs": "https://arxiv.org/abs/2508.09699", "authors": ["Javier Rodenas", "Eduardo Aguilar", "Petia Radeva"], "title": "Slot Attention-based Feature Filtering for Few-Shot Learning", "comment": "CVPR Workshop LatinX 2025", "summary": "Irrelevant features can significantly degrade few-shot learn ing performance.\nThis problem is used to match queries and support images based on meaningful\nsimilarities despite the limited data. However, in this process, non-relevant\nfea tures such as background elements can easily lead to confu sion and\nmisclassification. To address this issue, we pro pose Slot Attention-based\nFeature Filtering for Few-Shot Learning (SAFF) that leverages slot attention\nmechanisms to discriminate and filter weak features, thereby improving few-shot\nclassification performance. The key innovation of SAFF lies in its integration\nof slot attention with patch em beddings, unifying class-aware slots into a\nsingle attention mechanism to filter irrelevant features effectively. We intro\nduce a similarity matrix that computes across support and query images to\nquantify the relevance of filtered embed dings for classification. Through\nexperiments, we demon strate that Slot Attention performs better than other\natten tion mechanisms, capturing discriminative features while reducing\nirrelevant information. We validate our approach through extensive experiments\non few-shot learning bench marks: CIFAR-FS, FC100, miniImageNet and tieredIma\ngeNet, outperforming several state-of-the-art methods.", "AI": {"tldr": "本文提出SAFF（Slot Attention-based Feature Filtering），一种基于Slot Attention的特征过滤方法，用于在小样本学习中识别和过滤弱特征，显著提升分类性能。", "motivation": "在小样本学习中，由于数据量有限，不相关特征（如背景元素）会严重干扰查询图像和支持图像之间的匹配，导致混淆和错误分类，从而显著降低学习性能。", "method": "本文提出SAFF方法，将Slot Attention机制与图像块嵌入相结合，把类别感知的槽统一到一个单一的注意力机制中，以有效过滤不相关特征。此外，引入了一个相似性矩阵来量化过滤后的嵌入对分类的相关性。", "result": "实验证明，SAFF中使用的Slot Attention比其他注意力机制表现更好，能捕获判别性特征并减少无关信息。在CIFAR-FS、FC100、miniImageNet和tieredImageNet等小样本学习基准测试中，SAFF超越了多个现有最先进的方法。", "conclusion": "Slot Attention机制能够有效过滤小样本学习中的无关特征，从而显著提高分类性能，证明了其在处理有限数据下特征干扰问题上的优越性。"}}
{"id": "2508.09709", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09709", "abs": "https://arxiv.org/abs/2508.09709", "authors": ["Qianru Qiu", "Jiafeng Mao", "Kento Masui", "Xueting Wang"], "title": "MangaDiT: Reference-Guided Line Art Colorization with Hierarchical Attention in Diffusion Transformers", "comment": "Codes and benchmarks will be released soon", "summary": "Recent advances in diffusion models have significantly improved the\nperformance of reference-guided line art colorization. However, existing\nmethods still struggle with region-level color consistency, especially when the\nreference and target images differ in character pose or motion. Instead of\nrelying on external matching annotations between the reference and target, we\npropose to discover semantic correspondences implicitly through internal\nattention mechanisms. In this paper, we present MangaDiT, a powerful model for\nreference-guided line art colorization based on Diffusion Transformers (DiT).\nOur model takes both line art and reference images as conditional inputs and\nintroduces a hierarchical attention mechanism with a dynamic attention\nweighting strategy. This mechanism augments the vanilla attention with an\nadditional context-aware path that leverages pooled spatial features,\neffectively expanding the model's receptive field and enhancing region-level\ncolor alignment. Experiments on two benchmark datasets demonstrate that our\nmethod significantly outperforms state-of-the-art approaches, achieving\nsuperior performance in both qualitative and quantitative evaluations.", "AI": {"tldr": "MangaDiT是一种基于扩散变换器（DiT）的参考引导线稿上色模型，通过分层注意力机制解决现有方法在区域级色彩一致性方面的不足。", "motivation": "现有扩散模型在参考引导线稿上色中，当参考图与目标图在人物姿态或动作上存在差异时，难以保持区域级色彩一致性，且依赖外部匹配标注。", "method": "提出MangaDiT模型，以线稿和参考图作为条件输入。引入了一种分层注意力机制，该机制具有动态注意力加权策略，并通过利用池化空间特征的上下文感知路径，扩展了模型的感受野，增强了区域级色彩对齐。模型通过内部注意力机制隐式发现语义对应关系。", "result": "在两个基准数据集上，MangaDiT在定性和定量评估中均显著优于现有最先进的方法。", "conclusion": "MangaDiT通过其创新的分层注意力机制有效解决了参考引导线稿上色中的区域级色彩一致性问题，取得了卓越的性能。"}}
{"id": "2508.09715", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.09715", "abs": "https://arxiv.org/abs/2508.09715", "authors": ["Devvrat Joshi", "Islem Rekik"], "title": "NEURAL: Attention-Guided Pruning for Unified Multimodal Resource-Constrained Clinical Evaluation", "comment": null, "summary": "The rapid growth of multimodal medical imaging data presents significant\nstorage and transmission challenges, particularly in resource-constrained\nclinical settings. We propose NEURAL, a novel framework that addresses this by\nusing semantics-guided data compression. Our approach repurposes\ncross-attention scores between the image and its radiological report from a\nfine-tuned generative vision-language model to structurally prune chest X-rays,\npreserving only diagnostically critical regions. This process transforms the\nimage into a highly compressed, graph representation. This unified graph-based\nrepresentation fuses the pruned visual graph with a knowledge graph derived\nfrom the clinical report, creating a universal data structure that simplifies\ndownstream modeling. Validated on the MIMIC-CXR and CheXpert Plus dataset for\npneumonia detection, NEURAL achieves a 93.4-97.7\\% reduction in image data size\nwhile maintaining a high diagnostic performance of 0.88-0.95 AUC, outperforming\nother baseline models that use uncompressed data. By creating a persistent,\ntask-agnostic data asset, NEURAL resolves the trade-off between data size and\nclinical utility, enabling efficient workflows and teleradiology without\nsacrificing performance. Our NEURAL code is available at\nhttps://github.com/basiralab/NEURAL.", "AI": {"tldr": "NEURAL是一个语义引导的医学图像压缩框架，通过利用视觉-语言模型的交叉注意力分数对胸部X光片进行结构性剪枝，仅保留诊断关键区域，并将其转换为高度压缩的图表示，然后与临床报告的知识图谱融合，形成统一的图基数据结构。该方法在大幅压缩数据量的同时保持了高诊断性能。", "motivation": "多模态医学影像数据的快速增长带来了巨大的存储和传输挑战，特别是在资源受限的临床环境中。", "method": "NEURAL框架通过以下步骤实现：1) 重用经过微调的生成式视觉-语言模型中图像与其放射学报告之间的交叉注意力分数；2) 利用这些分数对胸部X光片进行结构性剪枝，仅保留诊断关键区域；3) 将图像转换为高度压缩的图表示；4) 将剪枝后的视觉图与从临床报告中提取的知识图谱融合，创建一个统一的图基数据结构。", "result": "在MIMIC-CXR和CheXpert Plus数据集上进行肺炎检测验证，NEURAL实现了93.4-97.7%的图像数据大小缩减，同时保持了0.88-0.95 AUC的高诊断性能，优于使用未压缩数据的其他基线模型。", "conclusion": "NEURAL通过创建持久的、任务无关的数据资产，解决了数据大小与临床实用性之间的权衡，从而在不牺牲性能的情况下实现了高效的工作流程和远程放射学。"}}
{"id": "2508.09717", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.09717", "abs": "https://arxiv.org/abs/2508.09717", "authors": ["Shekhnaz Idrissova", "Islem Rekik"], "title": "Multimodal Sheaf-based Network for Glioblastoma Molecular Subtype Prediction", "comment": null, "summary": "Glioblastoma is a highly invasive brain tumor with rapid progression rates.\nRecent studies have shown that glioblastoma molecular subtype classification\nserves as a significant biomarker for effective targeted therapy selection.\nHowever, this classification currently requires invasive tissue extraction for\ncomprehensive histopathological analysis. Existing multimodal approaches\ncombining MRI and histopathology images are limited and lack robust mechanisms\nfor preserving shared structural information across modalities. In particular,\ngraph-based models often fail to retain discriminative features within\nheterogeneous graphs, and structural reconstruction mechanisms for handling\nmissing or incomplete modality data are largely underexplored. To address these\nlimitations, we propose a novel sheaf-based framework for structure-aware and\nconsistent fusion of MRI and histopathology data. Our model outperforms\nbaseline methods and demonstrates robustness in incomplete or missing data\nscenarios, contributing to the development of virtual biopsy tools for rapid\ndiagnostics. Our source code is available at\nhttps://github.com/basiralab/MMSN/.", "AI": {"tldr": "该研究提出了一个基于sheaf的新框架，用于融合MRI和组织病理学数据，以实现胶质母细胞瘤的分子亚型分类，解决了现有方法在多模态数据融合和缺失数据处理方面的局限性，并有望开发虚拟活检工具。", "motivation": "胶质母细胞瘤的分子亚型分类是靶向治疗的关键生物标志物，但目前依赖侵入性组织活检。现有多模态（MRI和组织病理学）融合方法有限，缺乏保留共享结构信息的鲁棒机制，尤其是在异构图中的判别特征保留以及处理缺失或不完整模态数据方面的结构重建机制未被充分探索。", "method": "提出了一种新颖的基于sheaf的框架（sheaf-based framework），用于结构感知和一致性地融合MRI和组织病理学数据。该模型旨在克服现有图基模型未能保留异构图中判别特征以及缺乏处理缺失或不完整模态数据的结构重建机制的局限性。", "result": "该模型在性能上优于基线方法，并在数据不完整或缺失的情况下表现出鲁棒性。", "conclusion": "该研究为开发用于快速诊断的虚拟活检工具做出了贡献，通过提供一种更有效、非侵入性的胶质母细胞瘤分子亚型分类方法。"}}
{"id": "2508.09736", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09736", "abs": "https://arxiv.org/abs/2508.09736", "authors": ["Lin Long", "Yichen He", "Wentao Ye", "Yiyuan Pan", "Yuan Lin", "Hang Li", "Junbo Zhao", "Wei Li"], "title": "Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with Long-Term Memory", "comment": null, "summary": "We introduce M3-Agent, a novel multimodal agent framework equipped with\nlong-term memory. Like humans, M3-Agent can process real-time visual and\nauditory inputs to build and update its long-term memory. Beyond episodic\nmemory, it also develops semantic memory, enabling it to accumulate world\nknowledge over time. Its memory is organized in an entity-centric, multimodal\nformat, allowing deeper and more consistent understanding of the environment.\nGiven an instruction, M3-Agent autonomously performs multi-turn, iterative\nreasoning and retrieves relevant information from memory to accomplish the\ntask. To evaluate memory effectiveness and memory-based reasoning in multimodal\nagents, we develop M3-Bench, a new long-video question answering benchmark.\nM3-Bench comprises 100 newly recorded real-world videos captured from a robot's\nperspective (M3-Bench-robot) and 929 web-sourced videos across diverse\nscenarios (M3-Bench-web). We annotate question-answer pairs designed to test\nkey capabilities essential for agent applications, such as human understanding,\ngeneral knowledge extraction, and cross-modal reasoning. Experimental results\nshow that M3-Agent, trained via reinforcement learning, outperforms the\nstrongest baseline, a prompting agent using Gemini-1.5-pro and GPT-4o,\nachieving 6.7%, 7.7%, and 5.3% higher accuracy on M3-Bench-robot, M3-Bench-web\nand VideoMME-long, respectively. Our work advances the multimodal agents toward\nmore human-like long-term memory and provides insights into their practical\ndesign. Model, code and data are available at\nhttps://github.com/bytedance-seed/m3-agent", "AI": {"tldr": "M3-Agent是一个具有长时记忆的多模态智能体框架，能处理视觉和听觉输入以构建和更新记忆。它包含情景记忆和语义记忆，并采用以实体为中心的多模态记忆组织。为评估其记忆能力，作者开发了M3-Bench，一个长视频问答基准。M3-Agent在M3-Bench和VideoMME-long上超越了强大的基线模型。", "motivation": "现有智能体缺乏类似人类的长时记忆能力，无法有效积累世界知识并进行多轮推理。研究旨在开发一个能够处理实时多模态输入、构建和更新长时记忆的智能体，并评估其记忆有效性和基于记忆的推理能力。", "method": "M3-Agent框架能够处理实时视觉和听觉输入，构建和更新长时记忆，包括情景记忆和语义记忆。其记忆以实体为中心、多模态格式组织，以实现更深层次和一致的环境理解。智能体通过多轮迭代推理并从记忆中检索相关信息来完成任务，并采用强化学习进行训练。为评估，开发了M3-Bench长视频问答基准，包含机器人视角录制的真实世界视频（M3-Bench-robot）和网络视频（M3-Bench-web），并标注了测试关键能力的问答对。", "result": "M3-Agent（通过强化学习训练）在M3-Bench-robot、M3-Bench-web和VideoMME-long上分别比使用Gemini-1.5-pro和GPT-4o的提示智能体基线高出6.7%、7.7%和5.3%的准确率。", "conclusion": "该工作推动了多模态智能体向更类人的长时记忆发展，并为其实际设计提供了见解。M3-Agent在多模态长视频理解和推理方面表现出色，证明了其长时记忆框架的有效性。"}}
{"id": "2508.09779", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09779", "abs": "https://arxiv.org/abs/2508.09779", "authors": ["Dianyi Wang", "Siyuan Wang", "Zejun Li", "Yikun Wang", "Yitong Li", "Duyu Tang", "Xiaoyu Shen", "Xuanjing Huang", "Zhongyu Wei"], "title": "MoIIE: Mixture of Intra- and Inter-Modality Experts for Large Vision Language Models", "comment": null, "summary": "Large Vision-Language Models (LVLMs) have demonstrated remarkable performance\nacross multi-modal tasks by scaling model size and training data. However,\nthese dense LVLMs incur significant computational costs and motivate the\nexploration of sparse Mixture of Experts (MoE) architectures. While MoE improve\nparameter efficiency, effectively applying MoE to simultaneously model\nmodality-specific features and cross-modal associations in LVLMs remains\nchallenging. In this work, we propose to incorporate Mixture of Intra- and\nInter-Modality Experts (MoIIE) to LVLMs. For each token, expert routing is\nguided by its modality, directing tokens to their respective intra-modality\nexperts as well as a shared pool of inter-modality experts, enabling the model\nto jointly learn rich intra-modal features and cross-modal interactions. We\nfurther introduce an effective and straightforward two-stage training strategy,\nwhich facilitates the direct activation of both MoE and multi-modal\ncapabilities. Extensive experiments across different data scales and LLM\nbackbone demonstrate the effectiveness, efficiency and generality of our\napproach. Notably, our MoIIE models with 5.5B and 11.3B activated parameters\nmatch or even surpass the performance of existing advanced open-source MoE-LLMs\nbased multi-modal models that involve more activated parameters. The code is\navailable at https://github.com/AlenjandroWang/MoIIE.", "AI": {"tldr": "本文提出了一种名为MoIIE（模态内外部专家混合）的新型MoE架构，用于大视觉语言模型（LVLMs），以高效地学习模态内特征和跨模态交互，并在多模态任务上实现了卓越的性能。", "motivation": "大规模视觉语言模型（LVLMs）虽然表现出色，但计算成本高昂。稀疏的专家混合（MoE）架构可以提高参数效率，但在LVLMs中同时建模模态特异性特征和跨模态关联仍然具有挑战性。", "method": "引入MoIIE架构，其中每个token的路由由其模态引导，将token路由到各自的模态内专家以及共享的模态间专家池，从而共同学习丰富的模态内特征和跨模态交互。此外，提出了一种有效的两阶段训练策略，以促进MoE和多模态能力的直接激活。", "result": "MoIIE方法在不同数据规模和LLM骨干网络上都表现出有效性、效率和通用性。值得注意的是，激活参数分别为5.5B和11.3B的MoIIE模型，其性能与现有先进的、激活参数更多的开源MoE-LLMs多模态模型持平甚至超越。", "conclusion": "MoIIE成功地将MoE架构应用于LVLMs，有效解决了计算效率和复杂多模态学习的挑战，通过结合模态内和模态间专家，实现了参数效率高且性能卓越的多模态模型。"}}
{"id": "2508.09785", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09785", "abs": "https://arxiv.org/abs/2508.09785", "authors": ["Linpu He", "Yanan Li", "Bingze Li", "Elvis Han Cui", "Donghui Wang"], "title": "DSS-Prompt: Dynamic-Static Synergistic Prompting for Few-Shot Class-Incremental Learning", "comment": "Accepted to ACMMM 2025", "summary": "Learning from large-scale pre-trained models with strong generalization\nability has shown remarkable success in a wide range of downstream tasks\nrecently, but it is still underexplored in the challenging few-shot\nclass-incremental learning (FSCIL) task. It aims to continually learn new\nconcepts from limited training samples without forgetting the old ones at the\nsame time. In this paper, we introduce DSS-Prompt, a simple yet effective\napproach that transforms the pre-trained Vision Transformer with minimal\nmodifications in the way of prompts into a strong FSCIL classifier. Concretely,\nwe synergistically utilize two complementary types of prompts in each\nTransformer block: static prompts to bridge the domain gap between the\npre-training and downstream datasets, thus enabling better adaption; and\ndynamic prompts to capture instance-aware semantics, thus enabling easy\ntransfer from base to novel classes. Specially, to generate dynamic prompts, we\nleverage a pre-trained multi-modal model to extract input-related diverse\nsemantics, thereby generating complementary input-aware prompts, and then\nadaptively adjust their importance across different layers. In this way, on top\nof the prompted visual embeddings, a simple prototype classifier can beat\nstate-of-the-arts without further training on the incremental tasks. We conduct\nextensive experiments on four benchmarks to validate the effectiveness of our\nDSS-Prompt and show that it consistently achieves better performance than\nexisting approaches on all datasets and can alleviate the catastrophic\nforgetting issue as well.", "AI": {"tldr": "本文提出DSS-Prompt，一种简单有效的方法，通过引入静态和动态两种提示（prompts），将预训练的Vision Transformer转换为强大的FSCIL分类器，以应对少样本类增量学习（FSCIL）中的灾难性遗忘问题。", "motivation": "大规模预训练模型在下游任务中表现出色，但在挑战性的少样本类增量学习（FSCIL）任务中仍未被充分探索。FSCIL的目标是在有限样本下持续学习新概念而不遗忘旧知识，这是一个亟待解决的问题。", "method": "DSS-Prompt在每个Transformer块中协同利用两种互补的提示：静态提示用于弥合预训练和下游数据集之间的域差距，动态提示用于捕获实例级语义，从而实现从基础类到新类的轻松迁移。动态提示通过预训练的多模态模型提取输入相关的多样语义生成，并自适应调整其在不同层的重要性。在此基础上，结合一个简单的原型分类器，无需对增量任务进行额外训练。", "result": "DSS-Prompt在四个基准测试中进行了广泛实验，结果表明它始终优于现有方法，并能有效缓解灾难性遗忘问题，在不进行增量任务额外训练的情况下，其性能超越了现有最先进技术。", "conclusion": "DSS-Prompt是一种简单而有效的方法，能够利用预训练模型在少样本类增量学习任务中取得显著成功，有效提升性能并缓解灾难性遗忘，为FSCIL提供了一个强大的解决方案。"}}
{"id": "2508.09796", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09796", "abs": "https://arxiv.org/abs/2508.09796", "authors": ["Yingjie Wang", "Zhixing Wang", "Le Zheng", "Tianxiao Liu", "Roujing Li", "Xueyao Hu"], "title": "MeMoSORT: Memory-Assisted Filtering and Motion-Adaptive Association Metric for Multi-Person Tracking", "comment": null, "summary": "Multi-object tracking (MOT) in human-dominant scenarios, which involves\ncontinuously tracking multiple people within video sequences, remains a\nsignificant challenge in computer vision due to targets' complex motion and\nsevere occlusions. Conventional tracking-by-detection methods are fundamentally\nlimited by their reliance on Kalman filter (KF) and rigid Intersection over\nUnion (IoU)-based association. The motion model in KF often mismatches\nreal-world object dynamics, causing filtering errors, while rigid association\nstruggles under occlusions, leading to identity switches or target loss. To\naddress these issues, we propose MeMoSORT, a simple, online, and real-time MOT\ntracker with two key innovations. First, the Memory-assisted Kalman filter\n(MeKF) uses memory-augmented neural networks to compensate for mismatches\nbetween assumed and actual object motion. Second, the Motion-adaptive IoU\n(Mo-IoU) adaptively expands the matching space and incorporates height\nsimilarity to reduce the influence of detection errors and association\nfailures, while remaining lightweight. Experiments on DanceTrack and SportsMOT\nshow that MeMoSORT achieves state-of-the-art performance, with HOTA scores of\n67.9\\% and 82.1\\%, respectively.", "AI": {"tldr": "MeMoSORT是一种新的多目标跟踪器，通过改进卡尔曼滤波器和IoU关联方法，解决了复杂运动和严重遮挡下的人群跟踪问题，并取得了最先进的性能。", "motivation": "传统跟踪-检测方法受限于卡尔曼滤波器（KF）的运动模型与真实目标动态不匹配，以及刚性IoU关联在遮挡下表现不佳，导致滤波误差、ID切换或目标丢失。", "method": "提出MeMoSORT，包含两项关键创新：1. 内存辅助卡尔曼滤波器（MeKF），使用内存增强神经网络补偿运动不匹配；2. 运动自适应IoU（Mo-IoU），自适应扩展匹配空间并结合高度相似性，以减少检测误差和关联失败的影响。", "result": "MeMoSORT在DanceTrack和SportsMOT数据集上分别达到了67.9%和82.1%的HOTA分数，实现了最先进的性能。", "conclusion": "MeMoSORT通过其创新的MeKF和Mo-IoU组件，有效解决了多目标跟踪中的挑战，尤其是在复杂运动和遮挡场景下，达到了最先进的性能。"}}
{"id": "2508.09802", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09802", "abs": "https://arxiv.org/abs/2508.09802", "authors": ["Xin Du", "Maoyuan Xu", "Zhi Ying"], "title": "MUJICA: Reforming SISR Models for PBR Material Super-Resolution via Cross-Map Attention", "comment": null, "summary": "Physically Based Rendering (PBR) materials are typically characterized by\nmultiple 2D texture maps such as basecolor, normal, metallic, and roughness\nwhich encode spatially-varying bi-directional reflectance distribution function\n(SVBRDF) parameters to model surface reflectance properties and microfacet\ninteractions. Upscaling SVBRDF material is valuable for modern 3D graphics\napplications. However, existing Single Image Super-Resolution (SISR) methods\nstruggle with cross-map inconsistency, inadequate modeling of modality-specific\nfeatures, and limited generalization due to data distribution shifts. In this\nwork, we propose Multi-modal Upscaling Joint Inference via Cross-map Attention\n(MUJICA), a flexible adapter that reforms pre-trained Swin-transformer-based\nSISR models for PBR material super-resolution. MUJICA is seamlessly attached\nafter the pre-trained and frozen SISR backbone. It leverages cross-map\nattention to fuse features while preserving remarkable reconstruction ability\nof the pre-trained SISR model. Applied to SISR models such as SwinIR, DRCT, and\nHMANet, MUJICA improves PSNR, SSIM, and LPIPS scores while preserving cross-map\nconsistency. Experiments demonstrate that MUJICA enables efficient training\neven with limited resources and delivers state-of-the-art performance on PBR\nmaterial datasets.", "AI": {"tldr": "本文提出了MUJICA，一个灵活的适配器，用于将预训练的单图像超分辨率（SISR）模型应用于PBR材质的超分辨率，解决了跨贴图不一致和泛化性问题。", "motivation": "现有的单图像超分辨率（SISR）方法在处理PBR材质时面临挑战，包括跨贴图不一致、模态特定特征建模不足以及由于数据分布变化导致的泛化能力有限。", "method": "本文提出了多模态超分辨率联合推断与跨贴图注意力（MUJICA）模型。MUJICA是一个灵活的适配器，无缝连接在预训练且冻结的Swin-transformer基SISR骨干网络之后，利用跨贴图注意力机制融合特征，同时保留预训练SISR模型出色的重建能力。", "result": "将MUJICA应用于SwinIR、DRCT和HMANet等SISR模型后，它提高了PSNR、SSIM和LPIPS分数，同时保持了跨贴图一致性。实验表明，MUJICA即使在有限资源下也能实现高效训练，并在PBR材质数据集上取得了最先进的性能。", "conclusion": "MUJICA是一种有效且高效的解决方案，能够提升PBR材质的超分辨率性能，同时解决现有SISR方法在处理多纹理贴图时的固有挑战。"}}
{"id": "2508.09812", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09812", "abs": "https://arxiv.org/abs/2508.09812", "authors": ["Aryan Pandhi", "Shrey Baid", "Sanjali Jha"], "title": "Poaching Hotspot Identification Using Satellite Imagery", "comment": null, "summary": "Elephant Poaching in African countries has been a decade-old problem. So much\nso that African Forest Elephants are now listed as an endangered species, and\nAfrican Savannah Elephants as critically endangered by the IUCN (International\nUnion for Conservation of Nature). [1] Elephants are hunted primarily for their\nivory tusks which caused many elephants to be born tuskless as a genetic\nmodification for survival. [2] Data gathered by recent studies shows that\nthough poaching methods remain the same, the poaching grounds are rather\ndynamic. Poachers have shifted to areas with less ranger patrols and several\nother factors like watering holes, seasons, altitude etc. cause constant shifts\nin poaching hotspot locations. [3] After a period of low poaching from\n2000-2014, poaching numbers in African countries are now on the rise again --\nWWF (World Wildlife Foundation) says there are 20,000 elephants poached\nannually [4]. In African countries, anti-poaching efforts are concentrated near\ntowns, while a majority of poaching occurs in the deserted regions. All of\nthese factors result in the need for a Computer Vision Model to identify\npoaching hotspots through locating the geographic indicators of favorable\npoaching regions. A CV model eliminates the need to manually track poachers and\naccount for the environmental factors to deploy resources and its combination\nwith satellite imagery allows us to survey large areas without disturbing local\nspecies or cross border aviation restrictions.", "AI": {"tldr": "非洲大象偷猎问题严峻，偷猎地点动态变化且传统反偷猎效率低下。研究提出利用计算机视觉模型结合卫星图像识别偷猎热点，以更有效地部署反偷猎资源。", "motivation": "非洲大象因偷猎而濒危，每年约有2万头大象被猎杀。偷猎地点动态变化，受巡逻、水源、季节和海拔等多种因素影响。现有的反偷猎工作效率低下，资源多集中在城镇附近而非实际偷猎高发区，因此急需一种能有效识别偷猎热点的方法。", "method": "提出使用计算机视觉（CV）模型，通过定位有利偷猎区域的地理指标来识别偷猎热点。该模型将结合卫星图像，以实现对大面积区域的监测，同时避免人工追踪和对当地物种的干扰。", "result": "摘要中未提供具体研究结果，主要阐述了问题背景和拟议的解决方案，指出该方法旨在通过识别地理指标来确定偷猎热点。", "conclusion": "大象偷猎是一个复杂且动态的问题，传统反偷猎方法存在局限性。开发并应用计算机视觉模型结合卫星图像，能够自动化且大规模地识别偷猎热点，从而优化反偷猎资源的部署，提高保护工作的效率和成功率。"}}
{"id": "2508.09814", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09814", "abs": "https://arxiv.org/abs/2508.09814", "authors": ["Pablo Hernández-Cámara", "Jose Manuel Jaén-Lorites", "Jorge Vila-Tomás", "Jesus Malo", "Valero Laparra"], "title": "Evolution of Low-Level and Texture Human-CLIP Alignment", "comment": null, "summary": "During the training of multi-modal models like CLIP, we observed an\nintriguing phenomenon: the correlation with low-level human image quality\nassessments peaks in the early epochs before gradually declining. This study\ninvestigates this observation and seeks to understand its causes through two\nkey factors: shape-texture bias alignment and classification accuracy drop\nunder noise. Our findings suggest that CLIP initially learn low-level visual\nfeatures, enhancing its alignment with low-level human perception but also\nincreasing its sensitivity to noise and its texture bias. As training\nprogresses, the model shifts toward more abstract shape-based representations,\nimproving noise robustness but reducing alignment with low-level human\nperception. These results suggest that these factors shared an underlying\nlearning mechanism and provide new insights into optimizing the trade-off\nbetween perceptual alignment and robustness in vision-language models.", "AI": {"tldr": "研究发现，多模态模型（如CLIP）在训练早期与人类低级图像质量评估的相关性达到峰值后逐渐下降。这是由于模型从学习低级视觉特征（纹理偏好、对噪声敏感、与低级感知对齐）转向更抽象的基于形状的表示（对噪声鲁棒、与低级感知对齐度降低）。", "motivation": "观察到CLIP等模型在训练过程中与人类低级图像质量评估的相关性呈现先升高后下降的现象，研究旨在理解其原因。", "method": "通过分析形状-纹理偏置对齐（shape-texture bias alignment）和噪声下分类准确率下降（classification accuracy drop under noise）这两个关键因素来探究上述现象。", "result": "研究发现，CLIP在训练初期学习低级视觉特征，增强了与人类低级感知的对齐，但增加了对噪声的敏感性和纹理偏置。随着训练深入，模型转向更抽象的基于形状的表示，提高了噪声鲁棒性，但降低了与人类低级感知的对齐。这些因素共享一个潜在的学习机制。", "conclusion": "研究结果揭示了视觉-语言模型中感知对齐和鲁棒性之间权衡的内在学习机制，为优化这一权衡提供了新见解。"}}
{"id": "2508.09818", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09818", "abs": "https://arxiv.org/abs/2508.09818", "authors": ["Rajan Das Gupta", "Md Yeasin Rahat", "Nafiz Fahad", "Abir Ahmed", "Liew Tze Hui"], "title": "ViMoNet: A Multimodal Vision-Language Framework for Human Behavior Understanding from Motion and Video", "comment": "Accepted in ICCVDM '25", "summary": "This study investigates how large language models (LLMs) can be used to\nunderstand human behavior using motion and video data. We think that mixing\nboth types is essential to completely capture the nuanced movements and\nmeanings of human actions, in contrast to recent models that simply concentrate\non motion data or films. To address this, we provide ViMoNet, a straightforward\nyet effective framework for comprehending, characterizing, and deducing human\naction. ViMoNet employs a joint training strategy that leverages the advantages\nof two data types: detailed motion-text data, which is more exact, and generic\nvideo-text data, which is more comprehensive but less detailed. This aids in\nthe model's acquisition of rich data regarding time and space in human\nbehavior. Additionally, we provide a brand new dataset named VIMOS that\ncontains a variety of films, motion sequences, instructions, and subtitles. We\ndeveloped ViMoNet-Bench, a standardized benchmark with carefully labeled\nsamples, to evaluate how well models understand human behavior. Our tests show\nthat ViMoNet outperforms existing methods in caption generation, motion\nunderstanding, and behavior interpretation.", "AI": {"tldr": "本研究提出ViMoNet框架和VIMOS数据集，利用大语言模型融合运动和视频数据来理解人类行为，并优于现有方法。", "motivation": "现有模型仅关注运动数据或视频数据，无法完整捕捉人类行为的细微动作和含义。需要结合两种数据类型以实现更全面的理解。", "method": "提出了ViMoNet框架，采用联合训练策略，结合了精确的运动-文本数据和更全面的视频-文本数据，以获取人类行为丰富的时空信息。同时，构建了新的VIMOS数据集（包含视频、运动序列、指令和字幕）和ViMoNet-Bench标准化基准。", "result": "ViMoNet在字幕生成、运动理解和行为解释方面均优于现有方法。", "conclusion": "通过结合运动和视频数据，ViMoNet能够更有效地理解人类行为，证明了融合两种数据类型的重要性。"}}
{"id": "2508.09822", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09822", "abs": "https://arxiv.org/abs/2508.09822", "authors": ["Zijian Song", "Sihan Qin", "Tianshui Chen", "Liang Lin", "Guangrun Wang"], "title": "Physical Autoregressive Model for Robotic Manipulation without Action Pretraining", "comment": "16 pages, 6 figures", "summary": "The scarcity of manipulation data has motivated the use of pretrained large\nmodels from other modalities in robotics. In this work, we build upon\nautoregressive video generation models to propose a Physical Autoregressive\nModel (PAR), where physical tokens combine frames and actions to represent the\njoint evolution of the robot and its environment. PAR leverages the world\nknowledge embedded in video pretraining to understand physical dynamics without\nrequiring action pretraining, enabling accurate video prediction and consistent\naction trajectories. It also adopts a DiT-based de-tokenizer to model frames\nand actions as continuous tokens, mitigating quantization errors and\nfacilitating mutual enhancement. Furthermore, we incorporate a causal mask with\ninverse kinematics, parallel training, and the KV-cache mechanism to further\nimprove performance and efficiency. Experiments on the ManiSkill benchmark show\nthat PAR achieves a 100\\% success rate on the PushCube task, matches the\nperformance of action-pretrained baselines on other tasks, and accurately\npredicts future videos with tightly aligned action trajectories. These findings\nunderscore a promising direction for robotic manipulation by transferring world\nknowledge from autoregressive video pretraining.", "AI": {"tldr": "本文提出了物理自回归模型（PAR），它利用视频预训练中的世界知识来理解物理动力学，无需动作预训练，即可实现准确的视频预测和一致的动作轨迹，并在机器人操作任务中表现出色。", "motivation": "机器人领域操作数据稀缺，促使研究者利用其他模态的预训练大模型来解决这一问题。", "method": "PAR模型基于自回归视频生成模型，将帧和动作结合成“物理tokens”来表示机器人和环境的联合演化。它利用视频预训练中的世界知识理解物理动力学，并采用基于DiT的解分词器将帧和动作建模为连续tokens，减少量化误差。此外，还结合了因果掩码、逆运动学、并行训练和KV-cache机制来提升性能和效率。", "result": "在ManiSkill基准测试中，PAR在PushCube任务上实现了100%的成功率，在其他任务上与动作预训练的基线模型表现相当，并能准确预测未来视频并生成紧密对齐的动作轨迹。", "conclusion": "研究结果表明，通过自回归视频预训练来迁移世界知识，为机器人操作提供了一个有前景的方向。"}}
{"id": "2508.09823", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09823", "abs": "https://arxiv.org/abs/2508.09823", "authors": ["Valentin Boussot", "Jean-Louis Dillenseger"], "title": "KonfAI: A Modular and Fully Configurable Framework for Deep Learning in Medical Imaging", "comment": "https://github.com/vboussot/KonfAI", "summary": "KonfAI is a modular, extensible, and fully configurable deep learning\nframework specifically designed for medical imaging tasks. It enables users to\ndefine complete training, inference, and evaluation workflows through\nstructured YAML configuration files, without modifying the underlying code.\nThis declarative approach enhances reproducibility, transparency, and\nexperimental traceability while reducing development time. Beyond the\ncapabilities of standard pipelines, KonfAI provides native abstractions for\nadvanced strategies including patch-based learning, test-time augmentation,\nmodel ensembling, and direct access to intermediate feature representations for\ndeep supervision. It also supports complex multi-model training setups such as\ngenerative adversarial architectures. Thanks to its modular and extensible\narchitecture, KonfAI can easily accommodate custom models, loss functions, and\ndata processing components. The framework has been successfully applied to\nsegmentation, registration, and image synthesis tasks, and has contributed to\ntop-ranking results in several international medical imaging challenges. KonfAI\nis open source and available at\n\\href{https://github.com/vboussot/KonfAI}{https://github.com/vboussot/KonfAI}.", "AI": {"tldr": "KonfAI是一个模块化、可扩展、完全可配置的深度学习框架，专为医学影像任务设计，通过YAML文件实现工作流定义，支持高级策略，并已在多项任务中取得成功。", "motivation": "现有深度学习框架在医学影像领域可能缺乏足够的灵活性、可重复性和开发效率。研究旨在提供一个声明式、易于配置的框架，以提高医学影像任务的再现性、透明度和实验可追溯性，并缩短开发时间。", "method": "KonfAI通过结构化的YAML配置文件定义完整的训练、推理和评估工作流，无需修改底层代码。它提供原生抽象支持高级策略，如基于补丁的学习、测试时间增强、模型集成和深度监督。它还支持复杂的多模型训练设置（如GANs），并允许轻松集成自定义模型、损失函数和数据处理组件。", "result": "KonfAI已成功应用于医学影像的分割、配准和图像合成任务，并在多个国际医学影像挑战中取得了顶尖排名。该框架是开源的。", "conclusion": "KonfAI是一个功能强大、灵活且高效的开源深度学习框架，显著提升了医学影像任务的开发效率和研究质量，并通过其声明式配置和模块化设计促进了实验的再现性和可追溯性。"}}
{"id": "2508.09824", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09824", "abs": "https://arxiv.org/abs/2508.09824", "authors": ["Xuhong Huang", "Shiqi Liu", "Kai Zhang", "Ying Tai", "Jian Yang", "Hui Zeng", "Lei Zhang"], "title": "Reverse Convolution and Its Applications to Image Restoration", "comment": "ICCV 2025; https://github.com/cszn/ConverseNet", "summary": "Convolution and transposed convolution are fundamental operators widely used\nin neural networks. However, transposed convolution (a.k.a. deconvolution) does\nnot serve as a true inverse of convolution due to inherent differences in their\nmathematical formulations. To date, no reverse convolution operator has been\nestablished as a standard component in neural architectures. In this paper, we\npropose a novel depthwise reverse convolution operator as an initial attempt to\neffectively reverse depthwise convolution by formulating and solving a\nregularized least-squares optimization problem. We thoroughly investigate its\nkernel initialization, padding strategies, and other critical aspects to ensure\nits effective implementation. Building upon this operator, we further construct\na reverse convolution block by combining it with layer normalization,\n1$\\times$1 convolution, and GELU activation, forming a Transformer-like\nstructure. The proposed operator and block can directly replace conventional\nconvolution and transposed convolution layers in existing architectures,\nleading to the development of ConverseNet. Corresponding to typical image\nrestoration models such as DnCNN, SRResNet and USRNet, we train three variants\nof ConverseNet for Gaussian denoising, super-resolution and deblurring,\nrespectively. Extensive experiments demonstrate the effectiveness of the\nproposed reverse convolution operator as a basic building module. We hope this\nwork could pave the way for developing new operators in deep model design and\napplications.", "AI": {"tldr": "该论文提出了一种新颖的深度可逆卷积（depthwise reverse convolution）操作符，旨在作为卷积的有效逆运算，并通过构建ConverseNet在图像恢复任务中验证了其有效性。", "motivation": "转置卷积（又称反卷积）并非卷积的真正逆运算，且目前神经网络中缺乏一个标准的逆卷积操作符。", "method": "通过建立并求解一个正则化最小二乘优化问题，提出了一种深度可逆卷积操作符；深入研究了其核初始化、填充策略等关键方面。在此基础上，结合层归一化、1x1卷积和GELU激活，构建了一个类似Transformer结构的可逆卷积块。该操作符和块可直接替代现有架构中的传统卷积和转置卷积层，从而构建出ConverseNet，并针对图像去噪、超分辨率和去模糊等任务训练了ConverseNet的不同变体。", "result": "大量实验证明了所提出的可逆卷积操作符作为基本构建模块的有效性。", "conclusion": "这项工作为深度模型设计和应用中开发新的操作符奠定了基础。"}}
{"id": "2508.09843", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09843", "abs": "https://arxiv.org/abs/2508.09843", "authors": ["Hao Yang", "Xu Zhang", "Jiaqi Ma", "Linwei Zhu", "Yun Zhang", "Huan Zhang"], "title": "Hierarchical Graph Attention Network for No-Reference Omnidirectional Image Quality Assessment", "comment": null, "summary": "Current Omnidirectional Image Quality Assessment (OIQA) methods struggle to\nevaluate locally non-uniform distortions due to inadequate modeling of spatial\nvariations in quality and ineffective feature representation capturing both\nlocal details and global context. To address this, we propose a graph neural\nnetwork-based OIQA framework that explicitly models structural relationships\nbetween viewports to enhance perception of spatial distortion non-uniformity.\nOur approach employs Fibonacci sphere sampling to generate viewports with\nwell-structured topology, representing each as a graph node. Multi-stage\nfeature extraction networks then derive high-dimensional node representation.\nTo holistically capture spatial dependencies, we integrate a Graph Attention\nNetwork (GAT) modeling fine-grained local distortion variations among adjacent\nviewports, and a graph transformer capturing long-range quality interactions\nacross distant regions. Extensive experiments on two large-scale OIQA databases\nwith complex spatial distortions demonstrate that our method significantly\noutperforms existing approaches, confirming its effectiveness and strong\ngeneralization capability.", "AI": {"tldr": "提出了一种基于图神经网络的全景图像质量评估（OIQA）框架，通过显式建模视口间的结构关系，有效处理局部非均匀失真。", "motivation": "现有全景图像质量评估方法难以评估局部非均匀失真，原因在于空间质量变化建模不足以及特征表示无法有效捕获局部细节和全局上下文。", "method": "该方法采用斐波那契球面采样生成具有良好拓扑结构的视口作为图节点，并通过多阶段特征提取网络获得高维节点表示。为捕获空间依赖性，整合了图注意力网络（GAT）建模相邻视口间的局部失真变化，并利用图Transformer捕获远距离区域间的长程质量交互。", "result": "在两个大型OIQA数据库上进行的广泛实验表明，该方法显著优于现有方法，证明了其有效性和强大的泛化能力。", "conclusion": "所提出的基于图神经网络的OIQA框架通过有效建模视口间的空间关系，成功解决了全景图像局部非均匀失真评估的挑战，并展现出优越的性能和泛化能力。"}}
{"id": "2508.09847", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09847", "abs": "https://arxiv.org/abs/2508.09847", "authors": ["Dhruvraj Singh Rawat", "Enggen Sherpa", "Rishikesan Kirupanantha", "Tin Hoang"], "title": "Enhancing Diffusion Face Generation with Contrastive Embeddings and SegFormer Guidance", "comment": "10 pages, preprint", "summary": "We present a benchmark of diffusion models for human face generation on a\nsmall-scale CelebAMask-HQ dataset, evaluating both unconditional and\nconditional pipelines. Our study compares UNet and DiT architectures for\nunconditional generation and explores LoRA-based fine-tuning of pretrained\nStable Diffusion models as a separate experiment. Building on the\nmulti-conditioning approach of Giambi and Lisanti, which uses both attribute\nvectors and segmentation masks, our main contribution is the integration of an\nInfoNCE loss for attribute embedding and the adoption of a SegFormer-based\nsegmentation encoder. These enhancements improve the semantic alignment and\ncontrollability of attribute-guided synthesis. Our results highlight the\neffectiveness of contrastive embedding learning and advanced segmentation\nencoding for controlled face generation in limited data settings.", "AI": {"tldr": "该研究在小型CelebAMask-HQ数据集上对人脸生成扩散模型进行了基准测试，并提出了通过InfoNCE损失和SegFormer编码器增强属性引导合成的方法。", "motivation": "在小规模数据集上评估扩散模型的人脸生成能力，并提高属性引导下人脸合成的语义对齐和可控性。", "method": "比较了UNet和DiT架构的无条件生成，探索了基于LoRA的预训练Stable Diffusion模型微调。主要贡献是整合了用于属性嵌入的InfoNCE损失，并采用了基于SegFormer的分割编码器，结合了属性向量和分割掩码的多条件方法。", "result": "对比嵌入学习和先进的分割编码在有限数据设置下对受控人脸生成的有效性得到了突出验证。", "conclusion": "对比嵌入学习和先进的分割编码器能有效提升在有限数据量下受控人脸生成的语义对齐和可控性。"}}
{"id": "2508.09849", "categories": ["cs.CV", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.09849", "abs": "https://arxiv.org/abs/2508.09849", "authors": ["Jan Phillipp Albrecht", "Jose R. A. Godinho", "Christina Hübers", "Deborah Schmidt"], "title": "ARI3D: A Software for Interactive Quantification of Regions in X-Ray CT 3D Images", "comment": "2 figures and 6 pages main article, 17 pages total, 8 figures total,\n  to be published in SoftwareX", "summary": "X-ray computed tomography (CT) is the main 3D technique for imaging the\ninternal microstructures of materials. Quantitative analysis of the\nmicrostructures is usually achieved by applying a sequence of steps that are\nimplemented to the entire 3D image. This is challenged by various imaging\nartifacts inherent from the technique, e.g., beam hardening and partial volume.\nConsequently, the analysis requires users to make a number of decisions to\nsegment and classify the microstructures based on the voxel gray-values. In\nthis context, a software tool, here called ARI3D, is proposed to interactively\nanalyze regions in three-dimensional X-ray CT images, assisting users through\nthe various steps of a protocol designed to classify and quantify objects\nwithin regions of a three-dimensional image. ARI3D aims to 1) Improve phase\nidentification; 2) Account for partial volume effect; 3) Increase the detection\nlimit and accuracy of object quantification; and 4) Harmonize quantitative 3D\nanalysis that can be implemented in different fields of science.", "AI": {"tldr": "ARI3D是一个交互式软件工具，旨在改善X射线CT图像的微结构定量分析，解决图像伪影和用户决策挑战。", "motivation": "X射线CT图像的微结构定量分析面临挑战，包括固有的图像伪影（如束硬化和部分体积效应）以及用户在基于体素灰度值分割和分类微结构时需要做出大量决策。", "method": "本文提出了一种名为ARI3D的交互式软件工具，用于分析三维X射线CT图像中的区域。该工具通过协助用户完成旨在分类和量化图像区域内物体的协议步骤来工作。", "result": "ARI3D旨在实现以下目标：1) 改善相识别；2) 考虑部分体积效应；3) 提高物体量化的检测限和准确性；4) 协调可应用于不同科学领域的定量三维分析。", "conclusion": "ARI3D提供了一个标准化的解决方案，以克服X射线CT图像定量分析中的常见挑战，从而提高微结构分析的准确性和可靠性，并促进其在不同科学领域的应用。"}}
{"id": "2508.09850", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09850", "abs": "https://arxiv.org/abs/2508.09850", "authors": ["Pablo Hernández-Cámara", "Jose Manuel Jaén-Lorites", "Jorge Vila-Tomás", "Valero Laparra", "Jesus Malo"], "title": "Do Vision Transformers See Like Humans? Evaluating their Perceptual Alignment", "comment": null, "summary": "Vision Transformers (ViTs) achieve remarkable performance in image\nrecognition tasks, yet their alignment with human perception remains largely\nunexplored. This study systematically analyzes how model size, dataset size,\ndata augmentation and regularization impact ViT perceptual alignment with human\njudgments on the TID2013 dataset. Our findings confirm that larger models\nexhibit lower perceptual alignment, consistent with previous works. Increasing\ndataset diversity has a minimal impact, but exposing models to the same images\nmore times reduces alignment. Stronger data augmentation and regularization\nfurther decrease alignment, especially in models exposed to repeated training\ncycles. These results highlight a trade-off between model complexity, training\nstrategies, and alignment with human perception, raising important\nconsiderations for applications requiring human-like visual understanding.", "AI": {"tldr": "本研究系统分析了ViT模型尺寸、数据集大小、数据增强和正则化如何影响其与人类感知的对齐，发现模型复杂性和某些训练策略会降低这种对齐。", "motivation": "尽管ViT在图像识别任务中表现出色，但它们与人类感知的对齐程度仍未被充分探索。本研究旨在系统地分析影响ViT与人类感知对齐的因素。", "method": "研究通过在TID2013数据集上，系统分析模型大小、数据集大小、数据增强和正则化对ViT与人类判断的感知对齐的影响。", "result": "研究发现：1) 更大的模型表现出更低的感知对齐；2) 增加数据集多样性影响甚微；3) 模型重复暴露于相同图像会降低对齐；4) 更强的数据增强和正则化会进一步降低对齐，尤其是在重复训练周期中。", "conclusion": "研究结果揭示了模型复杂性、训练策略与人类感知对齐之间存在权衡。这对于需要类人视觉理解的应用提出了重要的考虑。"}}
{"id": "2508.09857", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09857", "abs": "https://arxiv.org/abs/2508.09857", "authors": ["Yupeng Zhou", "Zhen Li", "Ziheng Ouyang", "Yuming Chen", "Ruoyi Du", "Daquan Zhou", "Bin Fu", "Yihao Liu", "Peng Gao", "Ming-Ming Cheng", "Qibin Hou"], "title": "OneVAE: Joint Discrete and Continuous Optimization Helps Discrete Video VAE Train Better", "comment": null, "summary": "Encoding videos into discrete tokens could align with text tokens to\nfacilitate concise and unified multi-modal LLMs, yet introducing significant\nspatiotemporal compression compared to continuous video representation.\nPrevious discrete video VAEs experienced unstable training, long training time,\nand degraded reconstruction quality. Given the easier training and superior\nperformance of continuous VAEs, an intuitive idea is to enhance discrete video\nVAEs by leveraging continuous VAEs. After rethinking the intrinsic link between\ndiscrete and continuous representations, we found that FSQ could effectively\npreserve pre-trained continuous VAE priors compared to other quantization\nmethods. By leveraging continuous VAE priors, it converges several times faster\nthan training from scratch and achieves superior performance at convergence.\nMeanwhile, two structural improvements are proposed. First, inspired by how\ncontinuous VAEs enhance reconstruction via enlarged latent dimensions, we\nintroduce a multi-token quantization mechanism, which achieves nearly a 1 dB\nimprovement in PSNR without compromising the token compression ratio. Second,\nto tackle reconstruction challenges in high-compression video VAEs, we\nstrengthen first-frame reconstruction, enabling the causal VAE to leverage this\ninformation in subsequent frames and markedly improving the performance of 4 x\n16 x 16 discrete VAEs. Furthermore, we propose a joint discrete-continuous\noptimization scheme that unifies the two paradigms and, for the first time,\nachieves competitive performance on both continuous and discrete\nrepresentations within a single network. We name our method OneVAE to reflect\nthis connection.", "AI": {"tldr": "该论文提出OneVAE，通过利用连续VAE的先验知识、多令牌量化和强化首帧重建等方法，显著提升了离散视频VAE的训练效率和重建质量，并首次在一个网络中实现了离散和连续表示的竞争性能。", "motivation": "将视频编码为离散令牌有利于与文本令牌对齐，以构建简洁统一的多模态LLM，但离散视频VAE存在训练不稳定、训练时间长和重建质量下降的问题。鉴于连续VAE训练更容易且性能优越，研究动机在于如何利用连续VAE来增强离散视频VAE。", "method": "1. 重新思考离散与连续表示的内在联系，发现FSQ能有效保留预训练的连续VAE先验知识。2. 引入多令牌量化机制，通过增加潜在维度提升重建质量（PSNR）。3. 强化首帧重建，使因果VAE能在后续帧中利用此信息，改善高压缩视频VAE的性能。4. 提出联合离散-连续优化方案（OneVAE），在一个网络中统一两种范式。", "result": "1. 利用连续VAE先验，收敛速度比从头训练快数倍，并在收敛时达到卓越性能。2. 多令牌量化在不牺牲令牌压缩比的情况下，PSNR提升近1 dB。3. 显著改善了4x16x16离散VAE的性能。4. 首次在单个网络中，实现了连续和离散表示的竞争性能。", "conclusion": "OneVAE通过结合连续VAE的优势和结构改进，成功解决了离散视频VAE的训练和性能问题，实现了离散和连续视频表示的统一与优化，为多模态LLM的视频令牌化提供了更高效、高质量的解决方案。"}}
{"id": "2508.09858", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09858", "abs": "https://arxiv.org/abs/2508.09858", "authors": ["Weiqi Li", "Zehao Zhang", "Liang Lin", "Guangrun Wang"], "title": "HumanGenesis: Agent-Based Geometric and Generative Modeling for Synthetic Human Dynamics", "comment": null, "summary": "\\textbf{Synthetic human dynamics} aims to generate photorealistic videos of\nhuman subjects performing expressive, intention-driven motions. However,\ncurrent approaches face two core challenges: (1) \\emph{geometric inconsistency}\nand \\emph{coarse reconstruction}, due to limited 3D modeling and detail\npreservation; and (2) \\emph{motion generalization limitations} and \\emph{scene\ninharmonization}, stemming from weak generative capabilities. To address these,\nwe present \\textbf{HumanGenesis}, a framework that integrates geometric and\ngenerative modeling through four collaborative agents: (1)\n\\textbf{Reconstructor} builds 3D-consistent human-scene representations from\nmonocular video using 3D Gaussian Splatting and deformation decomposition. (2)\n\\textbf{Critique Agent} enhances reconstruction fidelity by identifying and\nrefining poor regions via multi-round MLLM-based reflection. (3) \\textbf{Pose\nGuider} enables motion generalization by generating expressive pose sequences\nusing time-aware parametric encoders. (4) \\textbf{Video Harmonizer} synthesizes\nphotorealistic, coherent video via a hybrid rendering pipeline with diffusion,\nrefining the Reconstructor through a Back-to-4D feedback loop. HumanGenesis\nachieves state-of-the-art performance on tasks including text-guided synthesis,\nvideo reenactment, and novel-pose generalization, significantly improving\nexpressiveness, geometric fidelity, and scene integration.", "AI": {"tldr": "HumanGenesis是一个用于生成逼真、富有表现力、意图驱动的人体动态视频的框架，它通过整合几何建模和生成建模来解决现有方法中的几何不一致、粗糙重建、运动泛化受限和场景不协调等问题。", "motivation": "现有的人体动态合成方法面临两大挑战：一是由于3D建模和细节保留不足导致的几何不一致和粗糙重建；二是由于生成能力弱导致的运动泛化限制和场景不协调。", "method": "本文提出了HumanGenesis框架，通过四个协同代理整合几何和生成建模：1) 重建器（Reconstructor）利用3D高斯泼溅和形变分解从单目视频构建3D一致的人体-场景表示；2) 评论代理（Critique Agent）通过多轮MLLM（多模态大语言模型）反射识别并细化不良区域以增强重建保真度；3) 姿态引导器（Pose Guider）使用时间感知参数编码器生成富有表现力的姿态序列，实现运动泛化；4) 视频协调器（Video Harmonizer）通过混合渲染管线与扩散模型合成逼真、连贯的视频，并通过“回溯4D”反馈循环优化重建器。", "result": "HumanGenesis在文本引导合成、视频重演和新姿态泛化等任务上取得了最先进的性能，显著提高了表达力、几何保真度和场景集成度。", "conclusion": "HumanGenesis通过其创新的几何与生成建模集成框架，有效克服了现有合成人体动态方法的关键挑战，实现了高质量、逼真且富有表现力的人体视频生成。"}}
{"id": "2508.09912", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09912", "abs": "https://arxiv.org/abs/2508.09912", "authors": ["Chaoran Feng", "Zhenyu Tang", "Wangbo Yu", "Yatian Pang", "Yian Zhao", "Jianbin Zhao", "Li Yuan", "Yonghong Tian"], "title": "E-4DGS: High-Fidelity Dynamic Reconstruction from the Multi-view Event Cameras", "comment": "16 pages, 10 figures, 5 Tables, accepted by ACMMM 2025", "summary": "Novel view synthesis and 4D reconstruction techniques predominantly rely on\nRGB cameras, thereby inheriting inherent limitations such as the dependence on\nadequate lighting, susceptibility to motion blur, and a limited dynamic range.\nEvent cameras, offering advantages of low power, high temporal resolution and\nhigh dynamic range, have brought a new perspective to addressing the scene\nreconstruction challenges in high-speed motion and low-light scenes. To this\nend, we propose E-4DGS, the first event-driven dynamic Gaussian Splatting\napproach, for novel view synthesis from multi-view event streams with\nfast-moving cameras. Specifically, we introduce an event-based initialization\nscheme to ensure stable training and propose event-adaptive slicing splatting\nfor time-aware reconstruction. Additionally, we employ intensity importance\npruning to eliminate floating artifacts and enhance 3D consistency, while\nincorporating an adaptive contrast threshold for more precise optimization. We\ndesign a synthetic multi-view camera setup with six moving event cameras\nsurrounding the object in a 360-degree configuration and provide a benchmark\nmulti-view event stream dataset that captures challenging motion scenarios. Our\napproach outperforms both event-only and event-RGB fusion baselines and paves\nthe way for the exploration of multi-view event-based reconstruction as a novel\napproach for rapid scene capture.", "AI": {"tldr": "本文提出了E-4DGS，首个事件驱动的动态高斯泼溅方法，用于从多视角事件流中进行新颖视图合成，尤其适用于快速移动相机和挑战性场景。", "motivation": "传统的RGB相机在光照依赖、运动模糊和动态范围方面存在固有限制。事件相机具有低功耗、高时间分辨率和高动态范围的优势，为高速运动和低光照下的场景重建提供了新视角。", "method": "本文提出了E-4DGS方法，包括：1) 事件初始化方案以确保稳定训练；2) 事件自适应切片泼溅以实现时间感知重建；3) 强度重要性剪枝以消除浮动伪影和增强3D一致性；4) 自适应对比度阈值以进行更精确的优化。此外，还设计了一个包含六个移动事件相机的合成多视角数据集。", "result": "E-4DGS方法在性能上超越了仅使用事件和事件-RGB融合的基线方法。", "conclusion": "E-4DGS为探索基于多视角事件的快速场景捕获提供了一种新颖且有效的方法。"}}
{"id": "2508.09913", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09913", "abs": "https://arxiv.org/abs/2508.09913", "authors": ["Yachao Liang", "Min Yu", "Gang Li", "Jianguo Jiang", "Boquan Li", "Feng Yu", "Ning Zhang", "Xiang Meng", "Weiqing Huang"], "title": "SpeechForensics: Audio-Visual Speech Representation Learning for Face Forgery Detection", "comment": "Accepted by NeurIPS 2024", "summary": "Detection of face forgery videos remains a formidable challenge in the field\nof digital forensics, especially the generalization to unseen datasets and\ncommon perturbations. In this paper, we tackle this issue by leveraging the\nsynergy between audio and visual speech elements, embarking on a novel approach\nthrough audio-visual speech representation learning. Our work is motivated by\nthe finding that audio signals, enriched with speech content, can provide\nprecise information effectively reflecting facial movements. To this end, we\nfirst learn precise audio-visual speech representations on real videos via a\nself-supervised masked prediction task, which encodes both local and global\nsemantic information simultaneously. Then, the derived model is directly\ntransferred to the forgery detection task. Extensive experiments demonstrate\nthat our method outperforms the state-of-the-art methods in terms of\ncross-dataset generalization and robustness, without the participation of any\nfake video in model training. Code is available at\nhttps://github.com/Eleven4AI/SpeechForensics.", "AI": {"tldr": "该论文通过利用音视频语音元素之间的协同作用，提出了一种新颖的音视频语音表示学习方法，以提高人脸伪造视频检测在未见数据集和常见扰动下的泛化能力和鲁棒性。", "motivation": "人脸伪造视频检测仍然是一个巨大的挑战，特别是在未见数据集和常见扰动下的泛化能力。研究发现，包含语音内容的音频信号可以提供精确信息，有效反映面部运动，这启发了作者利用音视频协同来解决这一问题。", "method": "首先，通过一个自监督的掩码预测任务，在真实视频上学习精确的音视频语音表示，该任务同时编码局部和全局语义信息。然后，将训练好的模型直接迁移到伪造检测任务中，训练过程中不使用任何伪造视频。", "result": "广泛的实验表明，该方法在跨数据集泛化能力和鲁棒性方面优于现有最先进的方法，并且在模型训练中未包含任何伪造视频。", "conclusion": "所提出的基于音视频语音表示学习的方法，在不使用伪造数据训练的情况下，显著提高了人脸伪造视频检测的跨数据集泛化能力和鲁棒性。"}}
{"id": "2508.09926", "categories": ["cs.CV", "q-bio.QM", "I.2.10; I.4.8"], "pdf": "https://arxiv.org/pdf/2508.09926", "abs": "https://arxiv.org/abs/2508.09926", "authors": ["Benjamin Adjadj", "Pierre-Antoine Bannier", "Guillaume Horent", "Sebastien Mandela", "Aurore Lyon", "Kathryn Schutte", "Ulysse Marteau", "Valentin Gaury", "Laura Dumont", "Thomas Mathieu", "Reda Belbahri", "Benoît Schmauch", "Eric Durand", "Katharina Von Loga", "Lucie Gillet"], "title": "Towards Comprehensive Cellular Characterisation of H&E slides", "comment": "33 pages, 4 figures", "summary": "Cell detection, segmentation and classification are essential for analyzing\ntumor microenvironments (TME) on hematoxylin and eosin (H&E) slides. Existing\nmethods suffer from poor performance on understudied cell types (rare or not\npresent in public datasets) and limited cross-domain generalization. To address\nthese shortcomings, we introduce HistoPLUS, a state-of-the-art model for cell\nanalysis, trained on a novel curated pan-cancer dataset of 108,722 nuclei\ncovering 13 cell types. In external validation across 4 independent cohorts,\nHistoPLUS outperforms current state-of-the-art models in detection quality by\n5.2% and overall F1 classification score by 23.7%, while using 5x fewer\nparameters. Notably, HistoPLUS unlocks the study of 7 understudied cell types\nand brings significant improvements on 8 of 13 cell types. Moreover, we show\nthat HistoPLUS robustly transfers to two oncology indications unseen during\ntraining. To support broader TME biomarker research, we release the model\nweights and inference code at https://github.com/owkin/histoplus/.", "AI": {"tldr": "HistoPLUS是一个先进的细胞分析模型，通过在大型泛癌数据集上训练，显著提升了H&E切片上细胞检测、分割和分类的性能，尤其对罕见细胞类型和跨领域泛化表现出色。", "motivation": "现有方法在H&E切片上进行细胞检测、分割和分类时，对未充分研究的细胞类型（罕见或公共数据集中不存在的）表现不佳，且跨领域泛化能力有限。", "method": "引入了HistoPLUS模型，该模型在一个新颖的、精选的泛癌数据集上进行训练，该数据集包含108,722个细胞核，涵盖13种细胞类型。", "result": "在4个独立队列的外部验证中，HistoPLUS在检测质量上比现有最先进模型高出5.2%，在F1分类分数上高出23.7%，同时参数量减少5倍。它解锁了对7种未充分研究细胞类型的研究，并对13种细胞中的8种带来了显著改进。此外，HistoPLUS能稳健地泛化到训练期间未见的两种肿瘤适应症。", "conclusion": "HistoPLUS显著提升了肿瘤微环境（TME）生物标志物研究中细胞分析的性能，特别是在处理未充分研究的细胞类型和实现跨领域泛化方面，并且模型已开源以支持更广泛的研究。"}}
{"id": "2508.09936", "categories": ["cs.CV", "cs.DL"], "pdf": "https://arxiv.org/pdf/2508.09936", "abs": "https://arxiv.org/abs/2508.09936", "authors": ["Vittorio Pippi", "Konstantina Nikolaidou", "Silvia Cascianelli", "George Retsinas", "Giorgos Sfikas", "Rita Cucchiara", "Marcus Liwicki"], "title": "Quo Vadis Handwritten Text Generation for Handwritten Text Recognition?", "comment": "Accepted at ICCV Workshop VisionDocs", "summary": "The digitization of historical manuscripts presents significant challenges\nfor Handwritten Text Recognition (HTR) systems, particularly when dealing with\nsmall, author-specific collections that diverge from the training data\ndistributions. Handwritten Text Generation (HTG) techniques, which generate\nsynthetic data tailored to specific handwriting styles, offer a promising\nsolution to address these challenges. However, the effectiveness of various HTG\nmodels in enhancing HTR performance, especially in low-resource transcription\nsettings, has not been thoroughly evaluated. In this work, we systematically\ncompare three state-of-the-art styled HTG models (representing the generative\nadversarial, diffusion, and autoregressive paradigms for HTG) to assess their\nimpact on HTR fine-tuning. We analyze how visual and linguistic characteristics\nof synthetic data influence fine-tuning outcomes and provide quantitative\nguidelines for selecting the most effective HTG model. The results of our\nanalysis provide insights into the current capabilities of HTG methods and\nhighlight key areas for further improvement in their application to\nlow-resource HTR.", "AI": {"tldr": "本文系统评估了三种主流风格化手写文本生成（HTG）模型对低资源手写文本识别（HTR）微调的有效性，并提供了选择指南。", "motivation": "历史手稿数字化对HTR系统带来挑战，尤其是在处理与训练数据分布不同的少量作者特定手稿时。HTG技术被视为有前景的解决方案，但其在增强HTR性能（尤其是在低资源转录设置中）的有效性尚未得到充分评估。", "method": "系统比较了三种最先进的风格化HTG模型（分别代表生成对抗、扩散和自回归范式），以评估它们对HTR微调的影响。分析了合成数据的视觉和语言特征如何影响微调结果，并提供了选择最有效HTG模型的定量指南。", "result": "分析结果深入揭示了HTG方法的当前能力，并强调了其在应用于低资源HTR时需要进一步改进的关键领域。", "conclusion": "提供了选择最有效HTG模型的定量指南，并指出了HTG方法在低资源HTR应用中未来改进的方向。"}}
{"id": "2508.09943", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09943", "abs": "https://arxiv.org/abs/2508.09943", "authors": ["Tomás de la Sotta", "José M. Saavedra", "Héctor Henríquez", "Violeta Chang", "Aline Xavier"], "title": "AST-n: A Fast Sampling Approach for Low-Dose CT Reconstruction using Diffusion Models", "comment": null, "summary": "Low-dose CT (LDCT) protocols reduce radiation exposure but increase image\nnoise, compromising diagnostic confidence. Diffusion-based generative models\nhave shown promise for LDCT denoising by learning image priors and performing\niterative refinement. In this work, we introduce AST-n, an accelerated\ninference framework that initiates reverse diffusion from intermediate noise\nlevels, and integrate high-order ODE solvers within conditioned models to\nfurther reduce sampling steps. We evaluate two acceleration paradigms--AST-n\nsampling and standard scheduling with high-order solvers -- on the Low Dose CT\nGrand Challenge dataset, covering head, abdominal, and chest scans at 10-25 %\nof standard dose. Conditioned models using only 25 steps (AST-25) achieve peak\nsignal-to-noise ratio (PSNR) above 38 dB and structural similarity index (SSIM)\nabove 0.95, closely matching standard baselines while cutting inference time\nfrom ~16 seg to under 1 seg per slice. Unconditional sampling suffers\nsubstantial quality loss, underscoring the necessity of conditioning. We also\nassess DDIM inversion, which yields marginal PSNR gains at the cost of doubling\ninference time, limiting its clinical practicality. Our results demonstrate\nthat AST-n with high-order samplers enables rapid LDCT reconstruction without\nsignificant loss of image fidelity, advancing the feasibility of\ndiffusion-based methods in clinical workflows.", "AI": {"tldr": "本文提出AST-n框架，通过从中间噪声水平开始反向扩散并结合高阶ODE求解器，显著加速了低剂量CT（LDCT）图像去噪的扩散模型推理速度，同时保持了图像质量。", "motivation": "低剂量CT（LDCT）协议虽然减少了辐射暴露，但增加了图像噪声，从而影响了诊断信心。扩散生成模型在LDCT去噪方面展现出潜力，但其迭代细化过程导致推理时间过长，限制了临床应用。", "method": "引入了AST-n加速推理框架，该框架从中间噪声水平启动反向扩散。同时，将高阶ODE求解器整合到条件模型中，以进一步减少采样步骤。研究在Low Dose CT Grand Challenge数据集上评估了两种加速范式（AST-n采样和带有高阶求解器的标准调度）。", "result": "使用25步的条件模型（AST-25）实现了超过38 dB的峰值信噪比（PSNR）和超过0.95的结构相似性指数（SSIM），与标准基线模型接近，同时将每层图像的推理时间从约16秒缩短到1秒以内。无条件采样导致显著的质量损失，强调了条件化的必要性。DDIM反演虽然略微提高了PSNR，但推理时间翻倍，限制了其临床实用性。", "conclusion": "AST-n结合高阶采样器能够实现快速的LDCT重建，而不会显著损失图像保真度，这极大地推进了基于扩散的方法在临床工作流程中的可行性。"}}
{"id": "2508.09949", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.09949", "abs": "https://arxiv.org/abs/2508.09949", "authors": ["Trevine Oorloff", "Vishwanath Sindagi", "Wele Gedara Chaminda Bandara", "Ali Shafahi", "Amin Ghiasi", "Charan Prakash", "Reza Ardekani"], "title": "Stable Diffusion Models are Secretly Good at Visual In-Context Learning", "comment": "Accepted to ICCV 2025", "summary": "Large language models (LLM) in natural language processing (NLP) have\ndemonstrated great potential for in-context learning (ICL) -- the ability to\nleverage a few sets of example prompts to adapt to various tasks without having\nto explicitly update the model weights. ICL has recently been explored for\ncomputer vision tasks with promising early outcomes. These approaches involve\nspecialized training and/or additional data that complicate the process and\nlimit its generalizability. In this work, we show that off-the-shelf Stable\nDiffusion models can be repurposed for visual in-context learning (V-ICL).\nSpecifically, we formulate an in-place attention re-computation within the\nself-attention layers of the Stable Diffusion architecture that explicitly\nincorporates context between the query and example prompts. Without any\nadditional fine-tuning, we show that this repurposed Stable Diffusion model is\nable to adapt to six different tasks: foreground segmentation, single object\ndetection, semantic segmentation, keypoint detection, edge detection, and\ncolorization. For example, the proposed approach improves the mean intersection\nover union (mIoU) for the foreground segmentation task on Pascal-5i dataset by\n8.9% and 3.2% over recent methods such as Visual Prompting and IMProv,\nrespectively. Additionally, we show that the proposed method is able to\neffectively leverage multiple prompts through ensembling to infer the task\nbetter and further improve the performance.", "AI": {"tldr": "本文提出一种方法，通过在Stable Diffusion模型的自注意力层中重新计算注意力，实现视觉上下文学习（V-ICL），无需额外微调即可适应多种视觉任务。", "motivation": "LLM在自然语言处理中展现了强大的上下文学习能力，但现有计算机视觉领域的V-ICL方法通常需要专门训练或额外数据，限制了其通用性。研究旨在探索无需这些限制的V-ICL方法。", "method": "利用现成的Stable Diffusion模型，在自注意力层中设计并实现了“原地注意力重新计算”机制，明确地将查询和示例提示之间的上下文信息整合进去。该方法无需任何额外的模型微调。", "result": "该方法能够在不进行微调的情况下，适应六种不同的视觉任务，包括前景分割、单目标检测、语义分割、关键点检测、边缘检测和图像上色。例如，在Pascal-5i数据集上的前景分割任务中，其平均交并比（mIoU）比现有方法Visual Prompting和IMProv分别提高了8.9%和3.2%。此外，该方法还能通过集成有效利用多个提示来进一步提升性能。", "conclusion": "现成的Stable Diffusion模型可以通过提出的注意力重新计算方法，无需微调即可有效地实现视觉上下文学习，并在多种视觉任务上展现出强大的适应性和优越的性能，且能从多提示集成中获益。"}}
{"id": "2508.09959", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09959", "abs": "https://arxiv.org/abs/2508.09959", "authors": ["Yaohui Wang", "Di Yang", "Xinyuan Chen", "Francois Bremond", "Yu Qiao", "Antitza Dantcheva"], "title": "LIA-X: Interpretable Latent Portrait Animator", "comment": "Project Page: https://wyhsirius.github.io/LIA-X-project/", "summary": "We introduce LIA-X, a novel interpretable portrait animator designed to\ntransfer facial dynamics from a driving video to a source portrait with\nfine-grained control. LIA-X is an autoencoder that models motion transfer as a\nlinear navigation of motion codes in latent space. Crucially, it incorporates a\nnovel Sparse Motion Dictionary that enables the model to disentangle facial\ndynamics into interpretable factors. Deviating from previous 'warp-render'\napproaches, the interpretability of the Sparse Motion Dictionary allows LIA-X\nto support a highly controllable 'edit-warp-render' strategy, enabling precise\nmanipulation of fine-grained facial semantics in the source portrait. This\nhelps to narrow initial differences with the driving video in terms of pose and\nexpression. Moreover, we demonstrate the scalability of LIA-X by successfully\ntraining a large-scale model with approximately 1 billion parameters on\nextensive datasets. Experimental results show that our proposed method\noutperforms previous approaches in both self-reenactment and cross-reenactment\ntasks across several benchmarks. Additionally, the interpretable and\ncontrollable nature of LIA-X supports practical applications such as\nfine-grained, user-guided image and video editing, as well as 3D-aware portrait\nvideo manipulation.", "AI": {"tldr": "LIA-X是一种新型的可解释肖像动画器，通过稀疏运动字典实现对人脸动态的精细化控制和可编辑的动作迁移，性能优于现有方法。", "motivation": "现有的人脸动画方法缺乏对表情和姿态差异的精细控制，难以实现可解释的、细粒度的面部语义操作，从而限制了对源肖像与驱动视频初始差异的弥合。", "method": "LIA-X是一个自编码器，将运动迁移建模为潜在空间中的线性导航。它引入了一个新颖的稀疏运动字典来解耦可解释的面部动态因素，并支持“编辑-形变-渲染”（edit-warp-render）策略。该模型还展示了良好的可扩展性，可训练数十亿参数的大规模模型。", "result": "LIA-X在自重演和跨重演任务中均优于现有方法。其可解释性和可控性支持细粒度的用户引导图像/视频编辑以及3D感知肖像视频操作等实际应用。", "conclusion": "LIA-X通过其独特的稀疏运动字典和“编辑-形变-渲染”策略，实现了可解释、可控的人脸动态迁移，并在性能和实际应用方面取得了显著进展，为精细化肖像视频编辑提供了新途径。"}}
{"id": "2508.09967", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09967", "abs": "https://arxiv.org/abs/2508.09967", "authors": ["Tianqi Xiang", "Yi Li", "Qixiang Zhang", "Xiaomeng Li"], "title": "MOC: Meta-Optimized Classifier for Few-Shot Whole Slide Image Classification", "comment": "Accepted in MICCAI 2025", "summary": "Recent advances in histopathology vision-language foundation models (VLFMs)\nhave shown promise in addressing data scarcity for whole slide image (WSI)\nclassification via zero-shot adaptation. However, these methods remain\noutperformed by conventional multiple instance learning (MIL) approaches\ntrained on large datasets, motivating recent efforts to enhance VLFM-based WSI\nclassification through fewshot learning paradigms. While existing few-shot\nmethods improve diagnostic accuracy with limited annotations, their reliance on\nconventional classifier designs introduces critical vulnerabilities to data\nscarcity. To address this problem, we propose a Meta-Optimized Classifier (MOC)\ncomprising two core components: (1) a meta-learner that automatically optimizes\na classifier configuration from a mixture of candidate classifiers and (2) a\nclassifier bank housing diverse candidate classifiers to enable a holistic\npathological interpretation. Extensive experiments demonstrate that MOC\noutperforms prior arts in multiple few-shot benchmarks. Notably, on the\nTCGA-NSCLC benchmark, MOC improves AUC by 10.4% over the state-of-the-art\nfew-shot VLFM-based methods, with gains up to 26.25% under 1-shot conditions,\noffering a critical advancement for clinical deployments where diagnostic\ntraining data is severely limited. Code is available at\nhttps://github.com/xmed-lab/MOC.", "AI": {"tldr": "本文提出了一种元优化分类器（MOC），通过结合元学习器和分类器库，显著提高了在数据稀缺条件下的全玻片图像（WSI）少样本分类性能，特别是在1-shot设置下表现优异。", "motivation": "组织病理学视觉-语言基础模型（VLFMs）在零样本WSI分类中显示出潜力，但其性能仍不如在大型数据集上训练的传统多实例学习（MIL）方法。现有少样本VLFM方法虽然提高了诊断准确性，但由于依赖传统分类器设计，在数据稀缺时仍存在脆弱性，这促使研究者寻求更鲁棒的解决方案。", "method": "本文提出了一个元优化分类器（MOC），包含两个核心组件：1) 一个元学习器，能够自动从候选分类器组合中优化分类器配置；2) 一个分类器库，包含多样化的候选分类器，以实现全面的病理学解释。", "result": "MOC在多个少样本基准测试中超越了现有技术。特别是在TCGA-NSCLC基准测试中，MOC相较于最先进的少样本VLFM方法，AUC提高了10.4%，在1-shot条件下甚至提升了26.25%。", "conclusion": "MOC为临床部署中诊断训练数据严重受限的情况提供了关键性进展，其在极度数据稀缺条件下的显著性能提升证明了其在实际应用中的巨大潜力。"}}
{"id": "2508.09973", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09973", "abs": "https://arxiv.org/abs/2508.09973", "authors": ["Geonhee Sim", "Gyeongsik Moon"], "title": "PERSONA: Personalized Whole-Body 3D Avatar with Pose-Driven Deformations from a Single Image", "comment": "Accepted to ICCV 2025. https://mks0601.github.io/PERSONA/", "summary": "Two major approaches exist for creating animatable human avatars. The first,\na 3D-based approach, optimizes a NeRF- or 3DGS-based avatar from videos of a\nsingle person, achieving personalization through a disentangled identity\nrepresentation. However, modeling pose-driven deformations, such as non-rigid\ncloth deformations, requires numerous pose-rich videos, which are costly and\nimpractical to capture in daily life. The second, a diffusion-based approach,\nlearns pose-driven deformations from large-scale in-the-wild videos but\nstruggles with identity preservation and pose-dependent identity entanglement.\nWe present PERSONA, a framework that combines the strengths of both approaches\nto obtain a personalized 3D human avatar with pose-driven deformations from a\nsingle image. PERSONA leverages a diffusion-based approach to generate\npose-rich videos from the input image and optimizes a 3D avatar based on them.\nTo ensure high authenticity and sharp renderings across diverse poses, we\nintroduce balanced sampling and geometry-weighted optimization. Balanced\nsampling oversamples the input image to mitigate identity shifts in\ndiffusion-generated training videos. Geometry-weighted optimization prioritizes\ngeometry constraints over image loss, preserving rendering quality in diverse\nposes.", "AI": {"tldr": "PERSONA是一个结合了3D和扩散模型优势的框架，能够从单张图片生成具有姿态驱动形变的个性化3D人体形象，并保持高真实感和清晰渲染。", "motivation": "现有的两种主要方法各有局限：基于3D的方法需要大量姿态丰富的视频来建模非刚性形变（如衣物），成本高昂；而基于扩散的方法虽然能从大规模视频中学习姿态驱动形变，但在身份保持和姿态依赖的身份纠缠方面存在问题。", "method": "PERSONA框架结合了两种方法的优点。它首先利用扩散模型从输入的单张图片生成姿态丰富的视频，然后基于这些视频优化3D形象。为确保高真实感和跨姿态的清晰渲染，引入了“平衡采样”来减少扩散生成训练视频中的身份漂移，以及“几何加权优化”来优先考虑几何约束而非图像损失，从而保持渲染质量。", "result": "该方法实现了从单张图片生成具有姿态驱动形变的个性化3D人体形象，并在多样姿态下保持了高真实性和清晰的渲染效果。", "conclusion": "PERSONA成功地将3D和扩散方法的优势结合起来，解决了从单张图片创建个性化3D人体形象时，既要处理姿态驱动形变又要保持身份和渲染质量的挑战。"}}
{"id": "2508.09977", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09977", "abs": "https://arxiv.org/abs/2508.09977", "authors": ["Shuting He", "Peilin Ji", "Yitong Yang", "Changshuo Wang", "Jiayi Ji", "Yinglin Wang", "Henghui Ding"], "title": "A Survey on 3D Gaussian Splatting Applications: Segmentation, Editing, and Generation", "comment": "GitHub Repo:\n  https://github.com/heshuting555/Awesome-3DGS-Applications", "summary": "3D Gaussian Splatting (3DGS) has recently emerged as a powerful alternative\nto Neural Radiance Fields (NeRF) for 3D scene representation, offering\nhigh-fidelity photorealistic rendering with real-time performance. Beyond novel\nview synthesis, the explicit and compact nature of 3DGS enables a wide range of\ndownstream applications that require geometric and semantic understanding. This\nsurvey provides a comprehensive overview of recent progress in 3DGS\napplications. It first introduces 2D foundation models that support semantic\nunderstanding and control in 3DGS applications, followed by a review of\nNeRF-based methods that inform their 3DGS counterparts. We then categorize 3DGS\napplications into segmentation, editing, generation, and other functional\ntasks. For each, we summarize representative methods, supervision strategies,\nand learning paradigms, highlighting shared design principles and emerging\ntrends. Commonly used datasets and evaluation protocols are also summarized,\nalong with comparative analyses of recent methods across public benchmarks. To\nsupport ongoing research and development, a continually updated repository of\npapers, code, and resources is maintained at\nhttps://github.com/heshuting555/Awesome-3DGS-Applications.", "AI": {"tldr": "这篇综述全面回顾了3D高斯泼溅（3DGS）在各种下游应用中的最新进展，涵盖了语义理解、编辑和生成等领域。", "motivation": "3DGS作为NeRF的强大替代品，具有高保真渲染和实时性能，并且其明确紧凑的特性使其能够应用于广泛的需要几何和语义理解的下游任务。因此，需要一份全面的综述来系统地整理和展示这些应用。", "method": "该综述首先介绍了支持3DGS语义理解和控制的2D基础模型，随后回顾了启发3DGS对应方法的NeRF基方法。接着，将3DGS应用分为分割、编辑、生成和其他功能性任务。对于每个类别，总结了代表性方法、监督策略和学习范式，并强调了共享的设计原则和新兴趋势。此外，还总结了常用数据集和评估协议，并对公共基准上的最新方法进行了比较分析。", "result": "该综述提供了一个3DGS应用进展的全面概述，详细阐述了其在分割、编辑、生成等领域的应用，并总结了相关方法、监督策略、学习范式、数据集和评估协议。它还通过比较分析突出了共享设计原则和新兴趋势，并维护了一个持续更新的论文、代码和资源库。", "conclusion": "该综述为3DGS应用的持续研究和开发提供了宝贵的资源和支持，通过系统梳理和分析现有工作，为研究人员提供了清晰的理解和未来方向的指引。"}}
{"id": "2508.09981", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09981", "abs": "https://arxiv.org/abs/2508.09981", "authors": ["Chengtao Lv", "Bilang Zhang", "Yang Yong", "Ruihao Gong", "Yushi Huang", "Shiqiao Gu", "Jiajun Wu", "Yumeng Shi", "Jinyang Guo", "Wenya Wang"], "title": "LLMC+: Benchmarking Vision-Language Model Compression with a Plug-and-play Toolkit", "comment": "13 pages, 4 figures", "summary": "Large Vision-Language Models (VLMs) exhibit impressive multi-modal\ncapabilities but suffer from prohibitive computational and memory demands, due\nto their long visual token sequences and massive parameter sizes. To address\nthese issues, recent works have proposed training-free compression methods.\nHowever, existing efforts often suffer from three major limitations: (1)\nCurrent approaches do not decompose techniques into comparable modules,\nhindering fair evaluation across spatial and temporal redundancy. (2)\nEvaluation confined to simple single-turn tasks, failing to reflect performance\nin realistic scenarios. (3) Isolated use of individual compression techniques,\nwithout exploring their joint potential. To overcome these gaps, we introduce\nLLMC+, a comprehensive VLM compression benchmark with a versatile,\nplug-and-play toolkit. LLMC+ supports over 20 algorithms across five\nrepresentative VLM families and enables systematic study of token-level and\nmodel-level compression. Our benchmark reveals that: (1) Spatial and temporal\nredundancies demand distinct technical strategies. (2) Token reduction methods\ndegrade significantly in multi-turn dialogue and detail-sensitive tasks. (3)\nCombining token and model compression achieves extreme compression with minimal\nperformance loss. We believe LLMC+ will facilitate fair evaluation and inspire\nfuture research in efficient VLM. Our code is available at\nhttps://github.com/ModelTC/LightCompress.", "AI": {"tldr": "本文提出了LLMC+，一个全面的视觉语言模型（VLM）压缩基准和工具包，旨在解决现有压缩方法的评估局限性，并揭示了不同压缩策略的有效性，特别是结合令牌和模型压缩的潜力。", "motivation": "大型视觉语言模型（VLM）因其冗长的视觉令牌序列和庞大的参数量而面临计算和内存开销过大的问题。现有训练无关的压缩方法存在局限性：缺乏可比较的模块分解以进行公平评估，评估仅限于简单单轮任务，以及未探索不同压缩技术的联合潜力。", "method": "引入LLMC+，一个综合性的VLM压缩基准和即插即用工具包。它支持超过20种算法，涵盖五种代表性VLM系列，并支持对令牌级和模型级压缩进行系统研究。", "result": "研究揭示了：1) 空间和时间冗余需要不同的技术策略。2) 令牌减少方法在多轮对话和细节敏感任务中性能显著下降。3) 结合令牌和模型压缩可以实现极致压缩，同时性能损失最小。", "conclusion": "LLMC+将促进对高效VLM的公平评估，并启发未来的研究。"}}
{"id": "2508.09983", "categories": ["cs.CV", "cs.GR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.09983", "abs": "https://arxiv.org/abs/2508.09983", "authors": ["David Dinkevich", "Matan Levy", "Omri Avrahami", "Dvir Samuel", "Dani Lischinski"], "title": "Story2Board: A Training-Free Approach for Expressive Storyboard Generation", "comment": "Project page is available at\n  https://daviddinkevich.github.io/Story2Board/", "summary": "We present Story2Board, a training-free framework for expressive storyboard\ngeneration from natural language. Existing methods narrowly focus on subject\nidentity, overlooking key aspects of visual storytelling such as spatial\ncomposition, background evolution, and narrative pacing. To address this, we\nintroduce a lightweight consistency framework composed of two components:\nLatent Panel Anchoring, which preserves a shared character reference across\npanels, and Reciprocal Attention Value Mixing, which softly blends visual\nfeatures between token pairs with strong reciprocal attention. Together, these\nmechanisms enhance coherence without architectural changes or fine-tuning,\nenabling state-of-the-art diffusion models to generate visually diverse yet\nconsistent storyboards. To structure generation, we use an off-the-shelf\nlanguage model to convert free-form stories into grounded panel-level prompts.\nTo evaluate, we propose the Rich Storyboard Benchmark, a suite of open-domain\nnarratives designed to assess layout diversity and background-grounded\nstorytelling, in addition to consistency. We also introduce a new Scene\nDiversity metric that quantifies spatial and pose variation across storyboards.\nOur qualitative and quantitative results, as well as a user study, show that\nStory2Board produces more dynamic, coherent, and narratively engaging\nstoryboards than existing baselines.", "AI": {"tldr": "Story2Board是一个无需训练的框架，用于从自然语言生成富有表现力的故事板，它通过引入轻量级一致性机制解决了现有方法在空间构图、背景演变和叙事节奏方面的不足。", "motivation": "现有故事板生成方法过于关注主体身份，而忽略了视觉叙事中的关键方面，如空间构图、背景演变和叙事节奏，导致生成的故事板缺乏视觉多样性和叙事连贯性。", "method": "该框架是无需训练的，包含两个核心组件以增强一致性：潜在面板锚定（Latent Panel Anchoring）用于保持跨面板的角色参考一致性，以及互惠注意力值混合（Reciprocal Attention Value Mixing）用于柔和融合具有强互惠注意力的令牌对之间的视觉特征。它还利用现成的语言模型将自由形式的故事转换为面板级提示。为评估效果，研究提出了Rich Storyboard基准和新的场景多样性度量。", "result": "定性、定量结果和用户研究表明，Story2Board比现有基线生成的故事板更具动态性、连贯性和叙事吸引力，同时实现了视觉多样性和一致性。", "conclusion": "Story2Board通过创新的轻量级一致性框架，有效解决了自然语言故事板生成中现有方法的局限性，无需架构更改或微调，即可使扩散模型生成视觉多样且连贯的故事板，显著提升了故事板的质量和叙事表现力。"}}
