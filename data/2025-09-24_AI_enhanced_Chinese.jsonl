{"id": "2509.18100", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.18100", "abs": "https://arxiv.org/abs/2509.18100", "authors": ["Shishir Lamichhane", "Anamika Dubey"], "title": "Stochastic Economic Dispatch with Battery Energy Storage considering Wind and Load Uncertainty", "comment": "to be published in NAPS", "summary": "With the integration of renewable energy resources in power systems, managing\noperational flexibility and reliability while minimizing operational costs has\nbecome increasingly challenging. Battery energy storage system (BESS) offers a\npromising solution to address these issues. This paper presents a stochastic\ndynamic economic dispatch with storage (SDED-S) framework to assess the impact\nof BESS in managing uncertainty. The temporal correlation between wind and load\nuncertainties is captured, with scenarios generated using a method inspired by\nstratified and importance sampling. The proposed approach is demonstrated on a\nmodified IEEE 39-bus system, where selected conventional generators are\nconverted to wind power plants. Case studies show that strategic BESS\ndeployment significantly improves system flexibility by reducing renewable\ncurtailments and dispatch costs. Renewable energy curtailments decrease upon\nincreasing BESS size and approach zero depending on wind penetration level.\nHigher wind penetrations result in greater curtailments without storage and\nyield larger cost savings when BESS is deployed, highlighting the growing need\nfor flexibility as renewable energy penetrations increase.", "AI": {"tldr": "本文提出了一个随机动态经济调度与储能（SDED-S）框架，用于评估电池储能系统（BESS）在管理不确定性方面的作用，并证明BESS能显著提高系统灵活性、减少弃风和运行成本，尤其是在高风电渗透率下。", "motivation": "随着可再生能源并网，电力系统在管理运行灵活性、可靠性及最小化运行成本方面面临日益严峻的挑战。电池储能系统（BESS）被认为是解决这些问题的有效方案。", "method": "本文提出了一个随机动态经济调度与储能（SDED-S）框架来评估BESS在管理不确定性方面的作用。该方法捕获了风电和负荷不确定性之间的时间相关性，并使用受分层和重要性采样启发的方法生成场景。该方法在一个修改后的IEEE 39节点系统上进行了演示，其中部分常规发电机被转换为风力发电厂。", "result": "案例研究表明，战略性部署BESS通过减少可再生能源弃用和调度成本，显著提高了系统灵活性。可再生能源弃用量随BESS规模的增加而减少，并可能趋近于零（取决于风电渗透水平）。风电渗透率越高，在没有储能的情况下弃用量越大，部署BESS时节省的成本也越大。", "conclusion": "随着可再生能源渗透率的增加，对灵活性的需求也随之增长。BESS是满足这种需求的有效解决方案，它能显著提升电力系统的灵活性，并通过减少弃风和运行成本来提高经济效益。"}}
{"id": "2509.18224", "categories": ["eess.SY", "cs.RO", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.18224", "abs": "https://arxiv.org/abs/2509.18224", "authors": ["Svyatoslav Covanov", "Cedric Pradalier"], "title": "Reversible Kalman Filter for state estimation with Manifold", "comment": null, "summary": "This work introduces an algorithm for state estimation on manifolds within\nthe framework of the Kalman filter. Its primary objective is to provide a\nmethodology enabling the evaluation of the precision of existing Kalman filter\nvariants with arbitrary accuracy on synthetic data, something that, to the best\nof our knowledge, has not been addressed in prior work. To this end, we develop\na new filter that exhibits favorable numerical properties, thereby correcting\nthe divergences observed in previous Kalman filter variants. In this\nformulation, the achievable precision is no longer constrained by the\nsmall-velocity assumption and is determined solely by sensor noise. In\naddition, this new filter assumes high precision on the sensors, which, in real\nscenarios require a detection step that we define heuristically, allowing one\nto extend this approach to scenarios, using either a 9-axis IMU or a\ncombination of odometry, accelerometer, and pressure sensors. The latter\nconfiguration is designed for the reconstruction of trajectories in underwater\nenvironments.", "AI": {"tldr": "本文提出了一种新的卡尔曼滤波算法，用于流形上的状态估计，旨在以任意精度评估现有卡尔曼滤波变体的性能，并纠正其发散问题，同时扩展其应用场景。", "motivation": "现有卡尔曼滤波变体在合成数据上缺乏以任意精度评估其精度的有效方法，且存在发散问题和小速度假设的限制，这阻碍了其在高精度应用中的表现。", "method": "开发了一种新的卡尔曼滤波器，具有良好的数值特性。该滤波器假设传感器具有高精度，并引入了一个启发式检测步骤，使其能够应用于使用9轴IMU或里程计、加速度计和压力传感器组合的真实场景（例如水下环境）。", "result": "新滤波器纠正了先前卡尔曼滤波器变体中观察到的发散问题。其可实现精度不再受小速度假设的限制，仅由传感器噪声决定。该方法可扩展到使用9轴IMU或特定传感器组合（如用于水下环境的里程计、加速度计和压力传感器）的实际场景。", "conclusion": "本文提出了一种在流形上进行状态估计的新卡尔曼滤波算法，它解决了现有方法的精度评估限制和发散问题，消除了小速度假设，并通过启发式检测步骤扩展了其在多种真实场景（包括水下轨迹重建）中的应用潜力。"}}
{"id": "2509.18292", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.18292", "abs": "https://arxiv.org/abs/2509.18292", "authors": ["Shuaiting Huang", "Haodong Jiang", "Chengcheng Zhao", "Peng Cheng", "Junfeng Wu"], "title": "Fully Distributed State Estimation for Multi-agent Systems and its Application in Cooperative Localization", "comment": null, "summary": "In this paper, we investigate the distributed state estimation problem for a\ncontinuous-time linear multi-agent system (MAS) composed of $\\mathit{m}$ agents\nand monitored by the agents themselves. To address this problem, we propose a\ndistributed observer that enables each agent to reconstruct the state of the\nMAS. The main idea is to let each agent $\\mathit{i}$ recover the state of agent\n$\\mathit{j}$ by using leader-follower consensus rules to track agent\n$\\mathit{j}$'s state estimate, which is generated by agent $\\mathit{j}$ itself\nusing a Luenberger-like estimation rule. Under the assumptions of node-level\nobservability and topological ordering consistency, we show that the estimation\nerror dynamics are stabilizable if and only if the communication graph is\nstrongly connected. Moreover, we discuss the fully distributed design of the\nproposed observer, assuming that the agents only know basic MAS configuration\ninformation, such as the homogeneity and the maximum number of allowable\nagents. This design ensures that the proposed observer functions correctly when\nagents are added or removed. Building on this, we consider cooperative\nlocalization as a distributed estimation problem and develop two fully\ndistributed localization algorithms that allow agents to track their own and\nother agents' positions (and velocities) within the MAS. Finally, we conduct\nsimulations to demonstrate the effectiveness of our proposed theoretical\nresults.", "AI": {"tldr": "本文提出了一种用于连续时间线性多智能体系统（MAS）的分布式状态观测器，使每个智能体能够估计整个系统的状态，并在强连通通信图下证明了其稳定性。", "motivation": "解决由智能体自身监控的连续时间线性多智能体系统的分布式状态估计问题。", "method": "提出了一种分布式观测器。该方法的核心是让每个智能体i通过使用主从共识规则来跟踪智能体j的状态估计（由智能体j自身使用类Luenberger估计规则生成），从而恢复智能体j的状态。此外，讨论了在智能体仅知晓基本MAS配置信息（如同质性和最大允许智能体数量）情况下的全分布式设计，并将其应用于合作定位，开发了两种全分布式定位算法。", "result": "在节点级可观测性和拓扑排序一致性假设下，当且仅当通信图是强连通时，估计误差动力学是可镇定的。所提出的全分布式设计在智能体增减时也能正常工作。开发了两种全分布式定位算法，并仿真验证了理论结果的有效性。", "conclusion": "所提出的分布式观测器能有效解决多智能体系统的分布式状态估计问题，并在特定条件下保证了稳定性。其全分布式设计和在合作定位中的应用也展示了其鲁棒性和有效性。"}}
{"id": "2509.18346", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.18346", "abs": "https://arxiv.org/abs/2509.18346", "authors": ["M Parimi", "Rachit Mehra", "S. R. Wagh", "Amol Yerudkar", "Navdeep Singh"], "title": "On the Dynamics of Acceleration in First order Gradient Methods", "comment": null, "summary": "Ever since the original algorithm by Nesterov (1983), the true nature of the\nacceleration phenomenon has remained elusive, with various interpretations of\nwhy the method is actually faster. The diagnosis of the algorithm through the\nlens of Ordinary Differential Equations (ODEs) and the corresponding dynamical\nsystem formulation to explain the underlying dynamics has a rich history. In\nthe literature, the ODEs that explain algorithms are typically derived by\nconsidering the limiting case of the algorithm maps themselves, that is, an ODE\nformulation follows the development of an algorithm. This obfuscates the\nunderlying higher order principles and thus provides little evidence of the\nworking of the algorithm. Such has been the case with Nesterov algorithm and\nthe various analogies used to describe the acceleration phenomena, viz,\nmomentum associated with the rolling of a Heavy-Ball down a slope, Hessian\ndamping etc. The main focus of our work is to ideate the genesis of the\nNesterov algorithm from the viewpoint of dynamical systems leading to\ndemystifying the mathematical rigour behind the algorithm. Instead of reverse\nengineering ODEs from discrete algorithms, this work explores tools from the\nrecently developed control paradigm titled Passivity and Immersion approach and\nthe Geometric Singular Perturbation theory which are applied to arrive at the\nformulation of a dynamical system that explains and models the acceleration\nphenomena. This perspective helps to gain insights into the various terms\npresent and the sequence of steps used in Nesterovs accelerated algorithm for\nthe smooth strongly convex and the convex case. The framework can also be\nextended to derive the acceleration achieved using the triple momentum method\nand provides justifications for the non-convergence to the optimal solution in\nthe Heavy-Ball method.", "AI": {"tldr": "本文从动力系统视角，利用被动性与浸入方法及几何奇异摄动理论，正向推导并解释了Nesterov加速算法的内在原理和加速现象，而非传统地从离散算法逆向推导ODE。", "motivation": "Nesterov加速算法的加速现象本质一直难以捉摸，传统上从离散算法逆向推导常微分方程（ODE）未能有效揭示其深层原理，导致对加速机制的理解不足。", "method": "本文不采用从离散算法逆向工程ODE的方法，而是利用控制范式中的“被动性与浸入方法”和“几何奇异摄动理论”等工具，正向构建了一个动力系统来解释和建模加速现象。", "result": "该方法成功揭示了Nesterov加速算法（针对平滑强凸和凸情况）中各项和步骤的内在原理。此外，该框架还能用于推导三重动量方法的加速机制，并解释了重球法不收敛到最优解的原因。", "conclusion": "通过从动力系统角度正向推导，本文成功地揭示了Nesterov加速算法背后的数学原理，提供了对加速现象更深层次的理解，并为其他加速方法提供了统一的解释框架。"}}
{"id": "2509.18113", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18113", "abs": "https://arxiv.org/abs/2509.18113", "authors": ["Xin Hu", "Yue Kang", "Guanzi Yao", "Tianze Kang", "Mengjie Wang", "Heyao Liu"], "title": "Dynamic Prompt Fusion for Multi-Task and Cross-Domain Adaptation in LLMs", "comment": null, "summary": "This study addresses the generalization limitations commonly observed in\nlarge language models under multi-task and cross-domain settings. Unlike prior\nmethods such as SPoT, which depends on fixed prompt templates, our study\nintroduces a unified multi-task learning framework with dynamic prompt\nscheduling mechanism. By introducing a prompt pool and a task-aware scheduling\nstrategy, the method dynamically combines and aligns prompts for different\ntasks. This enhances the model's ability to capture semantic differences across\ntasks. During prompt fusion, the model uses task embeddings and a gating\nmechanism to finely control the prompt signals. This ensures alignment between\nprompt content and task-specific demands. At the same time, it builds flexible\nsharing pathways across tasks. In addition, the proposed optimization objective\ncenters on joint multi-task learning. It incorporates an automatic learning\nstrategy for scheduling weights, which effectively mitigates task interference\nand negative transfer. To evaluate the effectiveness of the method, a series of\nsensitivity experiments were conducted. These experiments examined the impact\nof prompt temperature parameters and task number variation. The results confirm\nthe advantages of the proposed mechanism in maintaining model stability and\nenhancing transferability. Experimental findings show that the prompt\nscheduling method significantly improves performance on a range of language\nunderstanding and knowledge reasoning tasks. These results fully demonstrate\nits applicability and effectiveness in unified multi-task modeling and\ncross-domain adaptation.", "AI": {"tldr": "本研究提出了一种动态提示调度机制，用于统一的多任务学习框架，以解决大型语言模型在多任务和跨领域设置中的泛化限制，显著提升了模型性能和可迁移性。", "motivation": "大型语言模型在多任务和跨领域设置中存在泛化能力限制，且现有方法（如SPoT）依赖固定的提示模板，无法有效捕捉任务间的语义差异。", "method": "该研究引入了一个统一的多任务学习框架，包含动态提示调度机制。具体方法包括：构建一个提示池和任务感知调度策略，动态组合和对齐不同任务的提示；在提示融合阶段，利用任务嵌入和门控机制精细控制提示信号，确保提示内容与任务需求对齐，并构建灵活的任务间共享路径；优化目标侧重于联合多任务学习，并整合了调度权重自动学习策略，以减轻任务干扰和负迁移。", "result": "通过一系列敏感性实验（考察提示温度参数和任务数量变化），结果证实了所提出机制在保持模型稳定性、增强可迁移性方面的优势。实验发现，提示调度方法显著提高了模型在多项语言理解和知识推理任务上的性能。", "conclusion": "该动态提示调度方法在统一多任务建模和跨领域适应性方面表现出良好的适用性和有效性，充分证明了其在解决大型语言模型泛化问题上的潜力。"}}
{"id": "2509.18282", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18282", "abs": "https://arxiv.org/abs/2509.18282", "authors": ["Jesse Zhang", "Marius Memmel", "Kevin Kim", "Dieter Fox", "Jesse Thomason", "Fabio Ramos", "Erdem Bıyık", "Abhishek Gupta", "Anqi Li"], "title": "PEEK: Guiding and Minimal Image Representations for Zero-Shot Generalization of Robot Manipulation Policies", "comment": "11 pages", "summary": "Robotic manipulation policies often fail to generalize because they must\nsimultaneously learn where to attend, what actions to take, and how to execute\nthem. We argue that high-level reasoning about where and what can be offloaded\nto vision-language models (VLMs), leaving policies to specialize in how to act.\nWe present PEEK (Policy-agnostic Extraction of Essential Keypoints), which\nfine-tunes VLMs to predict a unified point-based intermediate representation:\n1. end-effector paths specifying what actions to take, and 2. task-relevant\nmasks indicating where to focus. These annotations are directly overlaid onto\nrobot observations, making the representation policy-agnostic and transferable\nacross architectures. To enable scalable training, we introduce an automatic\nannotation pipeline, generating labeled data across 20+ robot datasets spanning\n9 embodiments. In real-world evaluations, PEEK consistently boosts zero-shot\ngeneralization, including a 41.4x real-world improvement for a 3D policy\ntrained only in simulation, and 2-3.5x gains for both large VLAs and small\nmanipulation policies. By letting VLMs absorb semantic and visual complexity,\nPEEK equips manipulation policies with the minimal cues they need--where, what,\nand how. Website at https://peek-robot.github.io/.", "AI": {"tldr": "PEEK通过微调视觉-语言模型（VLMs）来预测统一的基于点的中间表示（末端执行器路径和任务相关掩码），将机器人操纵策略中的高层推理（在哪里、做什么）卸载给VLMs，从而使策略专注于如何行动，显著提高了零样本泛化能力。", "motivation": "机器人操纵策略在泛化方面常常失败，因为它们必须同时学习关注哪里、采取什么行动以及如何执行这些行动，这导致学习任务过于复杂。", "method": "本文提出了PEEK（Policy-agnostic Extraction of Essential Keypoints），它微调视觉-语言模型（VLMs）来预测统一的基于点的中间表示：1) 指定采取什么行动的末端执行器路径，以及2) 指示关注哪里的任务相关掩码。这些注释直接叠加到机器人观测数据上，使表示与策略无关并可在不同架构间转移。为实现可扩展训练，引入了一个自动化标注流程，生成了跨20多个机器人数据集和9种实体的数据。", "result": "在实际世界评估中，PEEK持续提升了零样本泛化能力，包括使仅在仿真中训练的3D策略在实际世界中实现了41.4倍的改进，并使大型视觉-语言-动作模型（VLAs）和小型操纵策略都获得了2-3.5倍的增益。", "conclusion": "通过让VLMs吸收语义和视觉复杂性，PEEK为操纵策略提供了它们所需的最少线索（在哪里、做什么、如何做），从而显著增强了它们的泛化能力。"}}
{"id": "2509.18101", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18101", "abs": "https://arxiv.org/abs/2509.18101", "authors": ["Guanzhong Pan", "Haibo Wang"], "title": "A Cost-Benefit Analysis of On-Premise Large Language Model Deployment: Breaking Even with Commercial LLM Services", "comment": null, "summary": "Large language models (LLMs) are becoming increasingly widespread.\nOrganizations that want to use AI for productivity now face an important\ndecision. They can subscribe to commercial LLM services or deploy models on\ntheir own infrastructure. Cloud services from providers such as OpenAI,\nAnthropic, and Google are attractive because they provide easy access to\nstate-of-the-art models and are easy to scale. However, concerns about data\nprivacy, the difficulty of switching service providers, and long-term operating\ncosts have driven interest in local deployment of open-source models. This\npaper presents a cost-benefit analysis framework to help organizations\ndetermine when on-premise LLM deployment becomes economically viable compared\nto commercial subscription services. We consider the hardware requirements,\noperational expenses, and performance benchmarks of the latest open-source\nmodels, including Qwen, Llama, Mistral, and etc. Then we compare the total cost\nof deploying these models locally with the major cloud providers subscription\nfee. Our findings provide an estimated breakeven point based on usage levels\nand performance needs. These results give organizations a practical framework\nfor planning their LLM strategies.", "AI": {"tldr": "本文提出了一个成本效益分析框架，帮助组织确定何时本地部署开源大型语言模型（LLM）比订阅商业LLM服务更具经济可行性。", "motivation": "随着LLM的普及，组织面临选择商业LLM服务或本地部署的决策。商业服务易于访问和扩展，但存在数据隐私、供应商锁定和长期成本问题；本地部署开源模型则可解决这些问题，因此需要一个框架来评估其经济可行性。", "method": "本文提出了一个成本效益分析框架。该框架考虑了硬件要求、运营支出以及包括Qwen、Llama、Mistral等最新开源模型的性能基准。然后，将本地部署的总成本与主要云服务提供商的订阅费用进行比较。", "result": "研究结果提供了基于使用水平和性能需求的预估盈亏平衡点，即本地部署何时比商业订阅服务更经济。", "conclusion": "这些结果为组织规划其LLM战略提供了一个实用的框架，帮助它们做出明智的部署决策。"}}
{"id": "2509.18159", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18159", "abs": "https://arxiv.org/abs/2509.18159", "authors": ["Akwasi Asare", "Ulas Bagci"], "title": "PolypSeg-GradCAM: Towards Explainable Computer-Aided Gastrointestinal Disease Detection Using U-Net Based Segmentation and Grad-CAM Visualization on the Kvasir Dataset", "comment": null, "summary": "Colorectal cancer (CRC) remains one of the leading causes of cancer-related\nmorbidity and mortality worldwide, with gastrointestinal (GI) polyps serving as\ncritical precursors according to the World Health Organization (WHO). Early and\naccurate segmentation of polyps during colonoscopy is essential for reducing\nCRC progression, yet manual delineation is labor-intensive and prone to\nobserver variability. Deep learning methods have demonstrated strong potential\nfor automated polyp analysis, but their limited interpretability remains a\nbarrier to clinical adoption. In this study, we present PolypSeg-GradCAM, an\nexplainable deep learning framework that integrates the U-Net architecture with\nGradient-weighted Class Activation Mapping (Grad-CAM) for transparent polyp\nsegmentation. The model was trained and evaluated on the Kvasir-SEG dataset of\n1000 annotated endoscopic images. Experimental results demonstrate robust\nsegmentation performance, achieving a mean Intersection over Union (IoU) of\n0.9257 on the test set and consistently high Dice coefficients (F-score > 0.96)\non training and validation sets. Grad-CAM visualizations further confirmed that\npredictions were guided by clinically relevant regions, enhancing transparency\nand trust in the model's decisions. By coupling high segmentation accuracy with\ninterpretability, PolypSeg-GradCAM represents a step toward reliable,\ntrustworthy AI-assisted colonoscopy and improved early colorectal cancer\nprevention.", "AI": {"tldr": "本研究提出PolypSeg-GradCAM，一个结合U-Net和Grad-CAM的可解释深度学习框架，用于结肠镜息肉分割，实现了高精度分割并提高了模型决策的透明度，有助于早期结直肠癌预防。", "motivation": "结直肠癌是全球癌症相关发病率和死亡率的主要原因，胃肠息肉是其关键前兆。结肠镜检查中早期准确分割息肉至关重要，但手动分割耗时且易受观察者差异影响。现有深度学习方法虽有潜力，但其可解释性不足是临床应用的主要障碍。", "method": "本研究提出了PolypSeg-GradCAM，一个可解释的深度学习框架，它将U-Net架构与梯度加权类激活映射（Grad-CAM）相结合，用于透明的息肉分割。模型在包含1000张带注释内窥镜图像的Kvasir-SEG数据集上进行训练和评估。", "result": "实验结果显示出强大的分割性能，在测试集上平均交并比（IoU）达到0.9257，并在训练和验证集上持续保持高Dice系数（F-score > 0.96）。Grad-CAM可视化进一步证实了模型的预测是由临床相关区域指导的，增强了模型决策的透明度和信任度。", "conclusion": "PolypSeg-GradCAM通过将高分割精度与可解释性相结合，代表了迈向可靠、值得信赖的AI辅助结肠镜检查和改进早期结直肠癌预防的重要一步。"}}
{"id": "2509.18402", "categories": ["eess.IV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18402", "abs": "https://arxiv.org/abs/2509.18402", "authors": ["Tingjun Liu", "Chicago Y. Park", "Yuyang Hu", "Hongyu An", "Ulugbek S. Kamilov"], "title": "Measurement Score-Based MRI Reconstruction with Automatic Coil Sensitivity Estimation", "comment": "7 pages, 2 figures. Equal contribution: Tingjun Liu and Chicago Y.\n  Park", "summary": "Diffusion-based inverse problem solvers (DIS) have recently shown outstanding\nperformance in compressed-sensing parallel MRI reconstruction by combining\ndiffusion priors with physical measurement models. However, they typically rely\non pre-calibrated coil sensitivity maps (CSMs) and ground truth images, making\nthem often impractical: CSMs are difficult to estimate accurately under heavy\nundersampling and ground-truth images are often unavailable. We propose\nCalibration-free Measurement Score-based diffusion Model (C-MSM), a new method\nthat eliminates these dependencies by jointly performing automatic CSM\nestimation and self-supervised learning of measurement scores directly from\nk-space data. C-MSM reconstructs images by approximating the full posterior\ndistribution through stochastic sampling over partial measurement posterior\nscores, while simultaneously estimating CSMs. Experiments on the multi-coil\nbrain fastMRI dataset show that C-MSM achieves reconstruction performance close\nto DIS with clean diffusion priors -- even without access to clean training\ndata and pre-calibrated CSMs.", "AI": {"tldr": "现有基于扩散模型的逆问题求解器（DIS）在并行MRI重建中表现出色，但依赖于预校准线圈灵敏度图（CSM）和真实图像，使其不切实际。本文提出C-MSM方法，通过联合自动CSM估计和自监督测量分数学习，消除了这些依赖，并在无需干净训练数据和预校准CSM的情况下，取得了与DIS相近的重建性能。", "motivation": "基于扩散模型的逆问题求解器（DIS）在压缩感知并行MRI重建中表现优异，但其依赖于预校准线圈灵敏度图（CSM）和真实图像。在严重欠采样下，CSM难以精确估计；真实图像通常也无法获取。这些限制使得现有方法在实际应用中不切实际。", "method": "本文提出了无校准测量分数扩散模型（C-MSM）。该方法通过以下方式消除了对预校准CSM和真实图像的依赖：1) 联合执行自动CSM估计；2) 直接从k空间数据进行测量分数的自监督学习。C-MSM通过对部分测量后验分数进行随机采样来近似完整后验分布，从而重建图像，并同时估计CSM。", "result": "在多线圈脑部fastMRI数据集上的实验表明，即使没有干净的训练数据和预校准CSM，C-MSM的重建性能也接近于使用干净扩散先验的DIS方法。", "conclusion": "C-MSM成功解决了现有基于扩散模型的逆问题求解器在并行MRI重建中对预校准CSM和真实图像的依赖问题，显著提高了其实用性，同时保持了高水平的重建性能。"}}
{"id": "2509.18356", "categories": ["eess.SY", "cs.NI", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.18356", "abs": "https://arxiv.org/abs/2509.18356", "authors": ["Darin Jeff", "Eytan Modiano"], "title": "Optimal Service Mode Assignment in a Simple Computation Offloading System: Extended Version", "comment": null, "summary": "We consider a simple computation offloading model where jobs can either be\nfully processed in the cloud or be partially processed at a local server before\nbeing sent to the cloud to complete processing. Our goal is to design a policy\nfor assigning jobs to service modes, i.e., full offloading or partial\noffloading, based on the state of the system, in order to minimize delay in the\nsystem. We show that when the cloud server is idle, the optimal policy is to\nassign the next job in the system queue to the cloud for processing. However,\nwhen the cloud server is busy, we show that, under mild assumptions, the\noptimal policy is of a threshold type, that sends the next job in the system\nqueue to the local server if the queue exceeds a certain threshold. Finally, we\ndemonstrate this policy structure through simulations.", "AI": {"tldr": "本文研究了一种计算卸载模型，旨在通过设计作业分配策略（完全卸载或部分卸载）来最小化系统延迟，并提出了基于云服务器状态的最优策略。", "motivation": "研究动机是为了在计算卸载系统中，通过优化作业分配策略来最小化系统延迟。", "method": "研究方法是设计一种基于系统状态的作业分配策略，将作业分配给两种服务模式：完全云端处理或先本地处理后云端完成。通过理论分析推导出在不同云服务器状态下的最优策略结构，并使用仿真进行验证。", "result": "主要结果是：当云服务器空闲时，最优策略是将下一个作业分配给云端处理；当云服务器繁忙时，在温和假设下，最优策略是阈值类型，即如果系统队列超过某个阈值，则将下一个作业发送到本地服务器处理。", "conclusion": "结论是，最优的计算卸载策略结构取决于云服务器的状态：空闲时直接送云端，繁忙时则采用基于队列长度的阈值策略，该策略结构已通过仿真得到验证。"}}
{"id": "2509.18122", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18122", "abs": "https://arxiv.org/abs/2509.18122", "authors": ["Yue Zhang", "Jiaxin Zhang", "Qiuyu Ren", "Tahsin Saffat", "Xiaoxuan Liu", "Zitong Yang", "Banghua Zhu", "Yi Ma"], "title": "GAUSS: Benchmarking Structured Mathematical Skills for Large Language Models", "comment": "120 pages (including appendix)", "summary": "We introduce \\textbf{GAUSS} (\\textbf{G}eneral \\textbf{A}ssessment of\n\\textbf{U}nderlying \\textbf{S}tructured \\textbf{S}kills in Mathematics), a\nbenchmark that evaluates LLMs' mathematical abilities across twelve core skill\ndimensions, grouped into three domains: knowledge and understanding, problem\nsolving and communication, and meta-skills and creativity. By categorizing\nproblems according to cognitive skills and designing tasks that isolate\nspecific abilities, GAUSS constructs comprehensive, fine-grained, and\ninterpretable profiles of models' mathematical abilities. These profiles\nfaithfully represent their underlying mathematical intelligence. To exemplify\nhow to use the \\textsc{GAUSS} benchmark, we have derived the skill profile of\n\\textsc{GPT-5-thinking}, revealing its strengths and weaknesses as well as its\ndifferences relative to \\textsc{o4-mini-high}, thereby underscoring the value\nof multidimensional, skill-based evaluation.", "AI": {"tldr": "GAUSS是一个评估大型语言模型（LLM）数学能力的基准，通过12个核心技能维度提供全面、细粒度且可解释的模型数学智能剖析。", "motivation": "现有评估方法未能提供LLM底层数学能力的细致、可解释的剖析，无法真实反映其数学智能。", "method": "引入GAUSS基准，将数学能力分为知识与理解、问题解决与沟通、元技能与创造力三个领域共十二个核心技能维度。通过根据认知技能对问题进行分类，并设计隔离特定能力的任务来构建评估体系。", "result": "GAUSS能够构建全面、细粒度且可解释的模型数学能力剖析，忠实地代表其底层数学智能。以GPT-5-thinking为例，揭示了其优缺点以及与o4-mini-high的差异，突显了多维度、基于技能评估的价值。", "conclusion": "GAUSS基准通过多维度、基于技能的评估方法，能够更深入、真实地评估和理解LLM的数学能力及潜在智能。"}}
{"id": "2509.18311", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18311", "abs": "https://arxiv.org/abs/2509.18311", "authors": ["Benjamin A. Christie", "Sagar Parekh", "Dylan P. Losey"], "title": "Fine-Tuning Robot Policies While Maintaining User Privacy", "comment": null, "summary": "Recent works introduce general-purpose robot policies. These policies provide\na strong prior over how robots should behave -- e.g., how a robot arm should\nmanipulate food items. But in order for robots to match an individual person's\nneeds, users typically fine-tune these generalized policies -- e.g., showing\nthe robot arm how to make their own preferred dinners. Importantly, during the\nprocess of personalizing robots, end-users leak data about their preferences,\nhabits, and styles (e.g., the foods they prefer to eat). Other agents can\nsimply roll-out the fine-tuned policy and see these personally-trained\nbehaviors. This leads to a fundamental challenge: how can we develop robots\nthat personalize actions while keeping learning private from external agents?\nWe here explore this emerging topic in human-robot interaction and develop\nPRoP, a model-agnostic framework for personalized and private robot policies.\nOur core idea is to equip each user with a unique key; this key is then used to\nmathematically transform the weights of the robot's network. With the correct\nkey, the robot's policy switches to match that user's preferences -- but with\nincorrect keys, the robot reverts to its baseline behaviors. We show the\ngeneral applicability of our method across multiple model types in imitation\nlearning, reinforcement learning, and classification tasks. PRoP is practically\nadvantageous because it retains the architecture and behaviors of the original\npolicy, and experimentally outperforms existing encoder-based approaches. See\nvideos and code here: https://prop-icra26.github.io.", "AI": {"tldr": "本文提出PRoP框架，通过为每个用户配备独特的密钥来数学转换机器人网络权重，实现个性化且私密的机器人策略，同时防止用户偏好数据泄露给外部代理。", "motivation": "通用机器人策略虽然强大，但用户在个性化微调过程中会泄露个人偏好、习惯等敏感数据。核心挑战在于如何在个性化机器人行为的同时，保护学习过程中的用户隐私不被外部代理获取。", "method": "PRoP（个性化和私密机器人策略）是一个模型无关的框架。其核心思想是为每个用户配备一个唯一的密钥，该密钥用于数学转换机器人网络的权重。使用正确的密钥，机器人策略将匹配用户的偏好；使用不正确的密钥，机器人将恢复到其基线行为。", "result": "PRoP方法在模仿学习、强化学习和分类任务等多种模型类型中显示出普遍适用性。它保留了原始策略的架构和行为，并在实验中优于现有的基于编码器的方法。", "conclusion": "PRoP框架通过独特的密钥转换机制，成功解决了机器人个性化与隐私保护之间的矛盾，实现了私密学习的个性化机器人策略，并展现出实际优势和卓越性能。"}}
{"id": "2509.18123", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18123", "abs": "https://arxiv.org/abs/2509.18123", "authors": ["Yeonju Lee", "Rui Qi Chen", "Joseph Oboamah", "Po Nien Su", "Wei-zhen Liang", "Yeyin Shi", "Lu Gan", "Yongsheng Chen", "Xin Qiao", "Jing Li"], "title": "SPADE: A Large Language Model Framework for Soil Moisture Pattern Recognition and Anomaly Detection in Precision Agriculture", "comment": null, "summary": "Accurate interpretation of soil moisture patterns is critical for irrigation\nscheduling and crop management, yet existing approaches for soil moisture\ntime-series analysis either rely on threshold-based rules or data-hungry\nmachine learning or deep learning models that are limited in adaptability and\ninterpretability. In this study, we introduce SPADE (Soil moisture Pattern and\nAnomaly DEtection), an integrated framework that leverages large language\nmodels (LLMs) to jointly detect irrigation patterns and anomalies in soil\nmoisture time-series data. SPADE utilizes ChatGPT-4.1 for its advanced\nreasoning and instruction-following capabilities, enabling zero-shot analysis\nwithout requiring task-specific annotation or fine-tuning. By converting\ntime-series data into a textual representation and designing domain-informed\nprompt templates, SPADE identifies irrigation events, estimates net irrigation\ngains, detects, classifies anomalies, and produces structured, interpretable\nreports. Experiments were conducted on real-world soil moisture sensor data\nfrom commercial and experimental farms cultivating multiple crops across the\nUnited States. Results demonstrate that SPADE outperforms the existing method\nin anomaly detection, achieving higher recall and F1 scores and accurately\nclassifying anomaly types. Furthermore, SPADE achieved high precision and\nrecall in detecting irrigation events, indicating its strong capability to\ncapture irrigation patterns accurately. SPADE's reports provide\ninterpretability and usability of soil moisture analytics. This study\nhighlights the potential of LLMs as scalable, adaptable tools for precision\nagriculture, which is capable of integrating qualitative knowledge and\ndata-driven reasoning to produce actionable insights for accurate soil moisture\nmonitoring and improved irrigation scheduling from soil moisture time-series\ndata.", "AI": {"tldr": "本研究引入了SPADE框架，利用大型语言模型（LLMs，如ChatGPT-4.1）对土壤湿度时间序列数据进行零样本分析，以联合检测灌溉模式和异常，并生成可解释的报告，在实际应用中表现优于现有方法。", "motivation": "准确解释土壤湿度模式对灌溉调度和作物管理至关重要。然而，现有的土壤湿度时间序列分析方法（如基于阈值的规则或数据密集型机器学习/深度学习模型）在适应性和可解释性方面存在局限。", "method": "SPADE框架利用大型语言模型（具体为ChatGPT-4.1）的推理和指令遵循能力，进行零样本分析。它将时间序列数据转换为文本表示，并设计了领域知情的提示模板。通过这种方式，SPADE能够识别灌溉事件、估算净灌溉增益、检测和分类异常，并生成结构化、可解释的报告。", "result": "SPADE在真实世界的土壤湿度传感器数据上进行了实验。结果表明，在异常检测方面，SPADE优于现有方法，实现了更高的召回率和F1分数，并能准确分类异常类型。此外，SPADE在检测灌溉事件方面也取得了高精度和高召回率。SPADE生成的报告提供了土壤湿度分析的可解释性和可用性。", "conclusion": "本研究强调了大型语言模型作为可扩展、适应性强的工具在精准农业中的潜力。它们能够整合定性知识和数据驱动的推理，为准确的土壤湿度监测和改进的灌溉调度提供可操作的见解。"}}
{"id": "2509.18160", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18160", "abs": "https://arxiv.org/abs/2509.18160", "authors": ["Akwasi Asare", "Isaac Baffour Senkyire", "Emmanuel Freeman", "Simon Hilary Ayinedenaba Aluze-Ele", "Kelvin Kwao"], "title": "PerceptronCARE: A Deep Learning-Based Intelligent Teleopthalmology Application for Diabetic Retinopathy Diagnosis", "comment": null, "summary": "Diabetic retinopathy is a leading cause of vision loss among adults and a\nmajor global health challenge, particularly in underserved regions. This study\npresents PerceptronCARE, a deep learning-based teleophthalmology application\ndesigned for automated diabetic retinopathy detection using retinal images. The\nsystem was developed and evaluated using multiple convolutional neural\nnetworks, including ResNet-18, EfficientNet-B0, and SqueezeNet, to determine\nthe optimal balance between accuracy and computational efficiency. The final\nmodel classifies disease severity with an accuracy of 85.4%, enabling real-time\nscreening in clinical and telemedicine settings. PerceptronCARE integrates\ncloud-based scalability, secure patient data management, and a multi-user\nframework, facilitating early diagnosis, improving doctor-patient interactions,\nand reducing healthcare costs. This study highlights the potential of AI-driven\ntelemedicine solutions in expanding access to diabetic retinopathy screening,\nparticularly in remote and resource-constrained environments.", "AI": {"tldr": "PerceptronCARE是一款基于深度学习的远程眼科应用，利用视网膜图像自动检测糖尿病视网膜病变（DR），旨在提高欠发达地区的筛查可及性。", "motivation": "糖尿病视网膜病变是成年人视力丧失的主要原因，尤其在医疗资源匮乏地区构成严峻的全球健康挑战，亟需可及的筛查解决方案。", "method": "本研究开发并评估了PerceptronCARE系统，使用了多种卷积神经网络（包括ResNet-18、EfficientNet-B0和SqueezeNet）进行模型优化，以在准确性和计算效率之间取得平衡。该应用集成了基于云计算的可扩展性、安全的患者数据管理和多用户框架。", "result": "最终模型在疾病严重程度分类上达到了85.4%的准确率，支持临床和远程医疗环境中的实时筛查。PerceptronCARE的集成功能有助于早期诊断、改善医患互动并降低医疗成本。", "conclusion": "本研究强调了人工智能驱动的远程医疗解决方案在扩大糖尿病视网膜病变筛查覆盖范围方面的潜力，尤其是在偏远和资源受限的环境中。"}}
{"id": "2509.18553", "categories": ["eess.IV", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18553", "abs": "https://arxiv.org/abs/2509.18553", "authors": ["Richa Rawat", "Faisal Ahmed"], "title": "Efficient Breast and Ovarian Cancer Classification via ViT-Based Preprocessing and Transfer Learning", "comment": "10 pages, 3 figures", "summary": "Cancer is one of the leading health challenges for women, specifically breast\nand ovarian cancer. Early detection can help improve the survival rate through\ntimely intervention and treatment. Traditional methods of detecting cancer\ninvolve manually examining mammograms, CT scans, ultrasounds, and other imaging\ntypes. However, this makes the process labor-intensive and requires the\nexpertise of trained pathologists. Hence, making it both time-consuming and\nresource-intensive. In this paper, we introduce a novel vision transformer\n(ViT)-based method for detecting and classifying breast and ovarian cancer. We\nuse a pre-trained ViT-Base-Patch16-224 model, which is fine-tuned for both\nbinary and multi-class classification tasks using publicly available\nhistopathological image datasets. Further, we use a preprocessing pipeline that\nconverts raw histophological images into standardized PyTorch tensors, which\nare compatible with the ViT architecture and also help improve the model\nperformance. We evaluated the performance of our model on two benchmark\ndatasets: the BreakHis dataset for binary classification and the UBC-OCEAN\ndataset for five-class classification without any data augmentation. Our model\nsurpasses existing CNN, ViT, and topological data analysis-based approaches in\nbinary classification. For multi-class classification, it is evaluated against\nrecent topological methods and demonstrates superior performance. Our study\nhighlights the effectiveness of Vision Transformer-based transfer learning\ncombined with efficient preprocessing in oncological diagnostics.", "AI": {"tldr": "本文提出了一种基于Vision Transformer (ViT) 的新方法，用于乳腺癌和卵巢癌的早期检测与分类，通过预训练模型微调和高效预处理，在二分类和多分类任务上均超越了现有方法。", "motivation": "乳腺癌和卵巢癌是女性面临的主要健康挑战。早期检测能显著提高生存率，但传统的手动影像学检查耗时、劳动密集且需要专业病理学家，效率低下。", "method": "研究引入了一种基于ViT的方法，使用预训练的ViT-Base-Patch16-224模型，并对其进行微调以执行二分类和多分类任务。该方法包含一个预处理流程，将原始组织病理学图像转换为标准化的PyTorch张量。模型在BreakHis数据集（二分类）和UBC-OCEAN数据集（五分类）上进行了评估，且未进行数据增强。", "result": "在二分类任务中，该模型超越了现有的CNN、ViT和基于拓扑数据分析的方法。在多分类任务中，它与最新的拓扑方法相比，也表现出卓越的性能。", "conclusion": "该研究强调了结合高效预处理的Vision Transformer迁移学习在肿瘤诊断中的有效性。"}}
{"id": "2509.18371", "categories": ["eess.SY", "cs.MA", "cs.RO", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.18371", "abs": "https://arxiv.org/abs/2509.18371", "authors": ["Eduardo Sebastián", "Maitrayee Keskar", "Eeman Iqbal", "Eduardo Montijano", "Carlos Sagüés", "Nikolay Atanasov"], "title": "Policy Gradient with Self-Attention for Model-Free Distributed Nonlinear Multi-Agent Games", "comment": null, "summary": "Multi-agent games in dynamic nonlinear settings are challenging due to the\ntime-varying interactions among the agents and the non-stationarity of the\n(potential) Nash equilibria. In this paper we consider model-free games, where\nagent transitions and costs are observed without knowledge of the transition\nand cost functions that generate them. We propose a policy gradient approach to\nlearn distributed policies that follow the communication structure in\nmulti-team games, with multiple agents per team. Our formulation is inspired by\nthe structure of distributed policies in linear quadratic games, which take the\nform of time-varying linear feedback gains. In the nonlinear case, we model the\npolicies as nonlinear feedback gains, parameterized by self-attention layers to\naccount for the time-varying multi-agent communication topology. We demonstrate\nthat our distributed policy gradient approach achieves strong performance in\nseveral settings, including distributed linear and nonlinear regulation, and\nsimulated and real multi-robot pursuit-and-evasion games.", "AI": {"tldr": "本文提出了一种模型无关的分布式策略梯度方法，用于动态非线性多智能体博弈。该方法利用自注意力层参数化非线性反馈增益策略，以应对时变通信拓扑，并在多种场景中表现出色。", "motivation": "动态非线性环境下的多智能体博弈面临挑战，原因在于智能体之间相互作用的时变性以及纳什均衡的非平稳性。特别是在模型未知（模型无关）的情况下，仅能观测到智能体状态转移和成本，这使得问题更具挑战性。", "method": "本文提出了一种策略梯度方法，用于学习多团队博弈中的分布式策略，这些策略遵循通信结构。该方法受到线性二次博弈中分布式策略（时变线性反馈增益形式）的启发。在非线性情况下，策略被建模为非线性反馈增益，并通过自注意力层进行参数化，以处理时变的多智能体通信拓扑。", "result": "研究表明，所提出的分布式策略梯度方法在多个设置中取得了强大的性能，包括分布式线性与非线性调节任务，以及模拟和真实的多机器人追捕-规避博弈。", "conclusion": "所提出的模型无关分布式策略梯度方法，通过自注意力层处理非线性动态和时变通信，能够有效学习复杂多智能体博弈中的分布式策略，并在各种实际场景中展现出优异的性能。"}}
{"id": "2509.18156", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18156", "abs": "https://arxiv.org/abs/2509.18156", "authors": ["Haoyu Wang", "Fengze Liu", "Jiayao Zhang", "Dan Roth", "Kyle Richardson"], "title": "Event Causality Identification with Synthetic Control", "comment": null, "summary": "Event causality identification (ECI), a process that extracts causal\nrelations between events from text, is crucial for distinguishing causation\nfrom correlation. Traditional approaches to ECI have primarily utilized\nlinguistic patterns and multi-hop relational inference, risking false causality\nidentification due to informal usage of causality and specious graphical\ninference. In this paper, we adopt the Rubin Causal Model to identify event\ncausality: given two temporally ordered events, we see the first event as the\ntreatment and the second one as the observed outcome. Determining their\ncausality involves manipulating the treatment and estimating the resultant\nchange in the likelihood of the outcome. Given that it is only possible to\nimplement manipulation conceptually in the text domain, as a work-around, we\ntry to find a twin for the protagonist from existing corpora. This twin should\nhave identical life experiences with the protagonist before the treatment but\nundergoes an intervention of treatment. However, the practical difficulty of\nlocating such a match limits its feasibility. Addressing this issue, we use the\nsynthetic control method to generate such a twin' from relevant historical\ndata, leveraging text embedding synthesis and inversion techniques. This\napproach allows us to identify causal relations more robustly than previous\nmethods, including GPT-4, which is demonstrated on a causality benchmark,\nCOPES-hard.", "AI": {"tldr": "本文提出一种基于Rubin因果模型和合成控制方法的新型事件因果识别（ECI）框架，通过生成“合成双胞胎”来模拟反事实情景，从而更鲁棒地识别因果关系，优于传统方法和GPT-4。", "motivation": "传统的事件因果识别方法主要依赖语言模式和多跳关系推理，存在因非正式语言使用和虚假图推理导致错误识别因果关系的风险。区分因果和相关性至关重要。", "method": "该研究采用Rubin因果模型，将两个按时间顺序排列的事件中的第一个视为“处理”，第二个视为“观察结果”。为确定因果关系，需概念性地操纵“处理”并估计结果事件发生可能性的变化。由于在文本领域无法实际操纵，作者提出通过合成控制方法，利用文本嵌入合成和反演技术，从相关历史数据中生成一个“合成双胞胎”，该“双胞胎”在“处理”前具有与主角相同的经历，但经历了不同的干预。", "result": "该方法能够比包括GPT-4在内的现有方法更鲁棒地识别因果关系，并在因果关系基准测试COPES-hard上得到了验证。", "conclusion": "通过结合Rubin因果模型和合成控制方法，该研究提供了一种新颖且更鲁棒的事件因果识别范式，有效克服了传统方法和大型语言模型在区分因果与相关性方面的局限性。"}}
{"id": "2509.18327", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18327", "abs": "https://arxiv.org/abs/2509.18327", "authors": ["Katherine H. Allen", "Chris Rogers", "Elaine S. Short"], "title": "Haptic Communication in Human-Human and Human-Robot Co-Manipulation", "comment": "9 pages, 18 figures, ROMAN 2025", "summary": "When a human dyad jointly manipulates an object, they must communicate about\ntheir intended motion plans. Some of that collaboration is achieved through the\nmotion of the manipulated object itself, which we call \"haptic communication.\"\nIn this work, we captured the motion of human-human dyads moving an object\ntogether with one participant leading a motion plan about which the follower is\nuninformed. We then captured the same human participants manipulating the same\nobject with a robot collaborator. By tracking the motion of the shared object\nusing a low-cost IMU, we can directly compare human-human shared manipulation\nto the motion of those same participants interacting with the robot.\nIntra-study and post-study questionnaires provided participant feedback on the\ncollaborations, indicating that the human-human collaborations are\nsignificantly more fluent, and analysis of the IMU data indicates that it\ncaptures objective differences in the motion profiles of the conditions. The\ndifferences in objective and subjective measures of accuracy and fluency\nbetween the human-human and human-robot trials motivate future research into\nimproving robot assistants for physical tasks by enabling them to send and\nreceive anthropomorphic haptic signals.", "AI": {"tldr": "本研究比较了人-人与人-机器人在共享物体操作中的协作，发现人-人协作更流畅，并通过IMU和问卷数据证实了客观和主观差异，从而启发了改进机器人触觉通信的研究。", "motivation": "了解人类在共同操作物体时如何通过物体运动进行“触觉沟通”，并将其与人-机器人互动进行比较，以期改进机器人协作助手。", "method": "捕捉人类双人组共同操作物体（一人主导，一人不知情）的运动；然后捕捉相同的参与者与机器人协作操作同一物体的运动。使用低成本IMU跟踪共享物体的运动，并使用研究内和研究后问卷收集参与者反馈。最后，直接比较人-人与人-机器人协作的IMU数据和问卷结果。", "result": "问卷调查显示人-人协作显著更流畅。IMU数据捕获了不同条件（人-人 vs. 人-机器人）下运动曲线的客观差异。人-人与人-机器人试验在准确性和流畅性方面的客观和主观测量存在差异。", "conclusion": "人-人与人-机器人试验之间客观和主观测量结果的差异，促使未来研究通过使机器人能够发送和接收拟人化的触觉信号，来改进用于物理任务的机器人助手。"}}
{"id": "2509.18132", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18132", "abs": "https://arxiv.org/abs/2509.18132", "authors": ["Xiuyi Fan"], "title": "Position Paper: Integrating Explainability and Uncertainty Estimation in Medical AI", "comment": "Accepted at the International Joint Conference on Neural Networks,\n  IJCNN 2025", "summary": "Uncertainty is a fundamental challenge in medical practice, but current\nmedical AI systems fail to explicitly quantify or communicate uncertainty in a\nway that aligns with clinical reasoning. Existing XAI works focus on\ninterpreting model predictions but do not capture the confidence or reliability\nof these predictions. Conversely, uncertainty estimation (UE) techniques\nprovide confidence measures but lack intuitive explanations. The disconnect\nbetween these two areas limits AI adoption in medicine. To address this gap, we\npropose Explainable Uncertainty Estimation (XUE) that integrates explainability\nwith uncertainty quantification to enhance trust and usability in medical AI.\nWe systematically map medical uncertainty to AI uncertainty concepts and\nidentify key challenges in implementing XUE. We outline technical directions\nfor advancing XUE, including multimodal uncertainty quantification,\nmodel-agnostic visualization techniques, and uncertainty-aware decision support\nsystems. Lastly, we propose guiding principles to ensure effective XUE\nrealisation. Our analysis highlights the need for AI systems that not only\ngenerate reliable predictions but also articulate confidence levels in a\nclinically meaningful way. This work contributes to the development of\ntrustworthy medical AI by bridging explainability and uncertainty, paving the\nway for AI systems that are aligned with real-world clinical complexities.", "AI": {"tldr": "本文提出可解释不确定性估计（XUE），旨在将可解释性与不确定性量化相结合，以增强医疗AI的信任度和可用性，解决当前医疗AI在量化和传达不确定性方面的不足。", "motivation": "医疗实践中不确定性普遍存在，但现有医疗AI系统未能明确量化或以符合临床推理的方式传达不确定性。现有可解释AI（XAI）侧重于预测解释而非置信度，而不确定性估计（UE）缺乏直观解释，两者脱节限制了AI在医学中的应用。", "method": "本文提出了可解释不确定性估计（XUE）的概念，系统地将医学不确定性映射到AI不确定性概念，识别了实现XUE的关键挑战。同时，概述了推进XUE的技术方向，包括多模态不确定性量化、模型无关可视化技术和不确定性感知决策支持系统，并提出了指导原则。", "result": "分析强调了AI系统不仅需要生成可靠预测，还需要以临床有意义的方式阐明置信水平。通过弥合可解释性与不确定性之间的鸿沟，为开发与真实世界临床复杂性相符的值得信赖的医疗AI系统铺平了道路。", "conclusion": "本研究通过整合可解释性与不确定性，为开发值得信赖的医疗AI做出了贡献，有助于构建与实际临床复杂性对齐的AI系统，从而提升医疗AI的信任度和可用性。"}}
{"id": "2509.18165", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18165", "abs": "https://arxiv.org/abs/2509.18165", "authors": ["Xiuding Cai", "Yaoyao Zhu", "Linjie Fu", "Dong Miao", "Yu Yao"], "title": "Self Identity Mapping", "comment": "Early accepted by Neural Networks 2025", "summary": "Regularization is essential in deep learning to enhance generalization and\nmitigate overfitting. However, conventional techniques often rely on\nheuristics, making them less reliable or effective across diverse settings. We\npropose Self Identity Mapping (SIM), a simple yet effective, data-intrinsic\nregularization framework that leverages an inverse mapping mechanism to enhance\nrepresentation learning. By reconstructing the input from its transformed\noutput, SIM reduces information loss during forward propagation and facilitates\nsmoother gradient flow. To address computational inefficiencies, We instantiate\nSIM as $ \\rho\\text{SIM} $ by incorporating patch-level feature sampling and\nprojection-based method to reconstruct latent features, effectively lowering\ncomplexity. As a model-agnostic, task-agnostic regularizer, SIM can be\nseamlessly integrated as a plug-and-play module, making it applicable to\ndifferent network architectures and tasks.\n  We extensively evaluate $\\rho\\text{SIM}$ across three tasks: image\nclassification, few-shot prompt learning, and domain generalization.\nExperimental results show consistent improvements over baseline methods,\nhighlighting $\\rho\\text{SIM}$'s ability to enhance representation learning\nacross various tasks. We also demonstrate that $\\rho\\text{SIM}$ is orthogonal\nto existing regularization methods, boosting their effectiveness. Moreover, our\nresults confirm that $\\rho\\text{SIM}$ effectively preserves semantic\ninformation and enhances performance in dense-to-dense tasks, such as semantic\nsegmentation and image translation, as well as in non-visual domains including\naudio classification and time series anomaly detection. The code is publicly\navailable at https://github.com/XiudingCai/SIM-pytorch.", "AI": {"tldr": "本文提出了一种名为自身份映射（SIM）的数据内在正则化框架，通过逆映射机制增强表示学习和泛化能力。其高效实现ρSIM通过重建变换后的输出，减少信息损失并促进梯度平滑，在多种任务和领域中显示出显著改进，并能与现有正则化方法正交结合。", "motivation": "传统的深度学习正则化技术常依赖启发式方法，导致在不同设置下可靠性和有效性不足，因此需要更通用和可靠的正则化策略。", "method": "本文提出自身份映射（SIM）框架，利用逆映射机制从变换后的输出重建输入，以减少信息损失并促进平滑梯度流。为解决计算效率问题，SIM被实例化为ρSIM，通过引入补丁级特征采样和基于投影的方法来重建潜在特征，从而降低复杂度。SIM设计为模型和任务无关的即插即用模块。", "result": "ρSIM在图像分类、少样本提示学习和领域泛化等任务中均超越基线方法，持续提升性能。实验证明ρSIM与现有正则化方法正交，能进一步增强其效果。此外，ρSIM能有效保留语义信息，并在语义分割、图像翻译等密集到密集任务以及音频分类和时间序列异常检测等非视觉领域中提升性能。", "conclusion": "自身份映射（SIM）及其高效实现ρSIM是一种简单而有效、数据内在的正则化框架，通过逆映射机制显著增强表示学习和泛化能力。它具有模型和任务无关性，可作为即插即用模块应用于广泛的任务和领域，并能与现有正则化方法协同工作，提供一致的性能提升。"}}
{"id": "2509.18748", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2509.18748", "abs": "https://arxiv.org/abs/2509.18748", "authors": ["Pep Borrell-Tatché", "Till Aczel", "Théo Ladune", "Roger Wattenhofer"], "title": "HyperCool: Reducing Encoding Cost in Overfitted Codecs with Hypernetworks", "comment": null, "summary": "Overfitted image codecs like Cool-chic achieve strong compression by\ntailoring lightweight models to individual images, but their encoding is slow\nand computationally expensive. To accelerate encoding, Non-Overfitted (N-O)\nCool-chic replaces the per-image optimization with a learned inference model,\ntrading compression performance for encoding speed. We introduce HyperCool, a\nhypernetwork architecture that mitigates this trade-off. Building upon the N-O\nCool-chic framework, HyperCool generates content-adaptive parameters for a\nCool-chic decoder in a single forward pass, tailoring the decoder to the input\nimage without requiring per-image fine-tuning. Our method achieves a 4.9% rate\nreduction over N-O Cool-chic with minimal computational overhead. Furthermore,\nthe output of our hypernetwork provides a strong initialization for further\noptimization, reducing the number of steps needed to approach fully overfitted\nmodel performance. With fine-tuning, HEVC-level compression is achieved with\n60.4% of the encoding cost of the fully overfitted Cool-chic. This work\nproposes a practical method to accelerate encoding in overfitted image codecs,\nimproving their viability in scenarios with tight compute budgets.", "AI": {"tldr": "HyperCool提出了一种超网络架构，通过为Cool-chic解码器生成内容自适应参数，显著加速了过拟合图像编码器的编码过程，在提升压缩性能的同时降低了计算成本。", "motivation": "过拟合图像编码器（如Cool-chic）能实现强大的压缩，但其编码过程缓慢且计算成本高昂。非过拟合（N-O）Cool-chic通过牺牲压缩性能来提高编码速度。本研究旨在缓解这种压缩性能与编码速度之间的权衡。", "method": "HyperCool基于N-O Cool-chic框架，采用超网络架构。它通过一次前向传播为Cool-chic解码器生成内容自适应参数，从而根据输入图像调整解码器，而无需进行逐图像微调。此外，超网络的输出还能为进一步优化提供强大的初始化。", "result": "HyperCool相较于N-O Cool-chic实现了4.9%的码率降低，且计算开销极小。超网络的输出为后续优化提供了强大的初始化，减少了达到完全过拟合模型性能所需的步骤。经过微调后，HyperCool在达到HEVC级别压缩性能的同时，仅需完全过拟合Cool-chic编码成本的60.4%。", "conclusion": "本研究提出了一种实用的方法来加速过拟合图像编码器的编码过程，提高了它们在计算预算紧张场景中的可行性。"}}
{"id": "2509.18475", "categories": ["eess.SY", "cs.SY", "math.CT", "math.DS"], "pdf": "https://arxiv.org/pdf/2509.18475", "abs": "https://arxiv.org/abs/2509.18475", "authors": ["Xiaoyan Li", "Evan Patterson", "Patricia L. Mabry", "Nathaniel D. Osgood"], "title": "Compositional System Dynamics: The Higher Mathematics Underlying System Dynamics Diagrams & Practice", "comment": null, "summary": "This work establishes a robust mathematical foundation for compositional\nSystem Dynamics modeling, leveraging category theory to formalize and enhance\nthe representation, analysis, and composition of system models. Here, System\nDynamics diagrams, such as stock & flow diagrams, system structure diagrams,\nand causal loop diagrams, are formulated as categorical constructs, enabling\nscalable, transparent, and systematic reasoning. By encoding these diagrams as\ndata using attributed C-sets and utilizing advanced categorical tools like\nstructured cospans, pushouts, pullbacks, and functor mappings, the framework\nsupports modular composition, stratification, and seamless mapping between\nsyntax and semantics.\n  The approach underwrites traditional practice with firm mathematical\nstructure, facilitates the identification of certain forms of pathways and\nfeedback loops, the detection of simple patterns within complex diagrams,\ncommon structure between diagrams, and structure-preserving mappings between\ndiverse diagram types. Additionally, this framework supports alternative\nsemantics, such as stochastic transition dynamics, extending beyond traditional\nordinary differential equation (ODE) representations. Applications in\ncompositional modeling, modularity, and team-based collaboration demonstrate\nthe practical advantages of this advanced framework.\n  Future directions include integrating dimensional annotations, supporting\nhybrid and agent-based modeling paradigms, and expanding the framework's\napplicability to global and local temporal reasoning through temporal sheaves.\nBy revealing and formalizing the hidden mathematical structure of System\nDynamics diagrams, this work empowers practitioners to tackle complex systems\nwith clarity, scalability, and rigor.", "AI": {"tldr": "该工作利用范畴论为组合系统动力学建模建立了坚实的数学基础，以形式化和增强系统模型的表示、分析和组合。", "motivation": "现有系统动力学建模缺乏强大的数学结构，难以实现可伸缩、透明和系统的推理。研究旨在提供一个严谨的数学框架，以应对复杂系统建模的挑战，并支持模块化组合、分层和多样化的语义表示。", "method": "将系统动力学图（如存量-流量图、因果回路图）形式化为范畴构造，使用带属性的C-集将这些图编码为数据，并利用结构化余跨、推论、拉回和函子映射等高级范畴工具来实现模型的组合和分析。", "result": "该框架支持模块化组合、分层、语法与语义之间的无缝映射，有助于识别路径和反馈回路、检测复杂图中的简单模式、识别图之间的共同结构以及不同图类型之间的结构保持映射。它还支持除传统常微分方程之外的替代语义（如随机转移动力学），并在组合建模、模块化和团队协作中展示了实际优势。", "conclusion": "该工作揭示并形式化了系统动力学图中隐藏的数学结构，为从业者提供了清晰、可伸缩和严谨的工具，以应对复杂系统，并为未来的混合和基于代理建模以及时态推理提供了扩展方向。"}}
{"id": "2509.18158", "categories": ["cs.CL", "cs.LG", "I.2.7"], "pdf": "https://arxiv.org/pdf/2509.18158", "abs": "https://arxiv.org/abs/2509.18158", "authors": ["Seungyoun Yi", "Minsoo Khang", "Sungrae Park"], "title": "ZERA: Zero-init Instruction Evolving Refinement Agent - From Zero Instructions to Structured Prompts via Principle-based Optimization", "comment": "9 pages, 4 figures. To appear in EMNLP 2025 Main Conference (Oral\n  Presentation)", "summary": "Automatic Prompt Optimization (APO) improves large language model (LLM)\nperformance by refining prompts for specific tasks. However, prior APO methods\ntypically focus only on user prompts, rely on unstructured feedback, and\nrequire large sample sizes and long iteration cycles-making them costly and\nbrittle. We propose ZERA (Zero-init Instruction Evolving Refinement Agent), a\nnovel framework that jointly optimizes both system and user prompts through\nprincipled, low-overhead refinement. ZERA scores prompts using eight\ngeneralizable criteria with automatically inferred weights, and revises prompts\nbased on these structured critiques. This enables fast convergence to\nhigh-quality prompts using minimal examples and short iteration cycles. We\nevaluate ZERA across five LLMs and nine diverse datasets spanning reasoning,\nsummarization, and code generation tasks. Experimental results demonstrate\nconsistent improvements over strong baselines. Further ablation studies\nhighlight the contribution of each component to more effective prompt\nconstruction. Our implementation including all prompts is publicly available at\nhttps://github.com/younatics/zera-agent.", "AI": {"tldr": "ZERA是一个新颖的框架，通过联合优化系统和用户提示词，并使用结构化反馈和最少示例，显著改进了大型语言模型（LLM）的自动提示词优化（APO）性能。", "motivation": "现有的自动提示词优化（APO）方法通常只关注用户提示词，依赖非结构化反馈，需要大量样本和漫长的迭代周期，导致成本高昂且脆弱。", "method": "ZERA（Zero-init Instruction Evolving Refinement Agent）框架通过以下方式进行提示词优化：1) 联合优化系统和用户提示词；2) 使用八个可泛化标准对提示词进行评分，并自动推断权重；3) 基于结构化批判修订提示词。这使得它能够以最少的示例和短迭代周期快速收敛到高质量的提示词。", "result": "在五种LLM和九个涵盖推理、摘要和代码生成的不同数据集上，ZERA始终优于强大的基线。进一步的消融研究也强调了每个组件对更有效提示词构建的贡献。", "conclusion": "ZERA是一种有效且低开销的联合提示词优化框架，通过解决现有APO方法的局限性，显著提高了LLM的性能。"}}
{"id": "2509.18330", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18330", "abs": "https://arxiv.org/abs/2509.18330", "authors": ["Marsette Vona"], "title": "The Landform Contextual Mesh: Automatically Fusing Surface and Orbital Terrain for Mars 2020", "comment": null, "summary": "The Landform contextual mesh fuses 2D and 3D data from up to thousands of\nMars 2020 rover images, along with orbital elevation and color maps from Mars\nReconnaissance Orbiter, into an interactive 3D terrain visualization.\nContextual meshes are built automatically for each rover location during\nmission ground data system processing, and are made available to mission\nscientists for tactical and strategic planning in the Advanced Science\nTargeting Tool for Robotic Operations (ASTTRO). A subset of them are also\ndeployed to the \"Explore with Perseverance\" public access website.", "AI": {"tldr": "该研究将火星2020探测器图像与火星勘测轨道器数据融合，自动生成交互式3D地形可视化（地貌情境网格），供任务规划和公众探索使用。", "motivation": "为火星2020任务科学家提供综合的3D地形可视化工具，以便进行战术和战略规划，并向公众开放部分数据。", "method": "将来自数千张火星2020探测器图像的2D和3D数据，以及来自火星勘测轨道器的轨道高程和彩色地图融合。在任务地面数据系统处理过程中自动构建“地貌情境网格”。", "result": "为每个漫游车位置创建了交互式3D地形可视化（情境网格）。这些网格可供任务科学家在ASTTRO工具中进行规划，其中一部分也部署到“与毅力号一起探索”的公众网站。", "conclusion": "地貌情境网格成功地将多种火星数据源整合为可用的3D可视化，有效支持了科学任务规划和公众参与。"}}
{"id": "2509.18168", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18168", "abs": "https://arxiv.org/abs/2509.18168", "authors": ["Dong Liu", "Yanxuan Yu"], "title": "HSGM: Hierarchical Segment-Graph Memory for Scalable Long-Text Semantics", "comment": null, "summary": "Semantic parsing of long documents remains challenging due to quadratic\ngrowth in pairwise composition and memory requirements. We introduce\n\\textbf{Hierarchical Segment-Graph Memory (HSGM)}, a novel framework that\ndecomposes an input of length $N$ into $M$ meaningful segments, constructs\n\\emph{Local Semantic Graphs} on each segment, and extracts compact\n\\emph{summary nodes} to form a \\emph{Global Graph Memory}. HSGM supports\n\\emph{incremental updates} -- only newly arrived segments incur local graph\nconstruction and summary-node integration -- while \\emph{Hierarchical Query\nProcessing} locates relevant segments via top-$K$ retrieval over summary nodes\nand then performs fine-grained reasoning within their local graphs.\n  Theoretically, HSGM reduces worst-case complexity from $O(N^2)$ to\n$O\\!\\left(N\\,k + (N/k)^2\\right)$, with segment size $k \\ll N$, and we derive\nFrobenius-norm bounds on the approximation error introduced by node\nsummarization and sparsification thresholds. Empirically, on three benchmarks\n-- long-document AMR parsing, segment-level semantic role labeling (OntoNotes),\nand legal event extraction -- HSGM achieves \\emph{2--4$\\times$ inference\nspeedup}, \\emph{$>60\\%$ reduction} in peak memory, and \\emph{$\\ge 95\\%$} of\nbaseline accuracy. Our approach unlocks scalable, accurate semantic modeling\nfor ultra-long texts, enabling real-time and resource-constrained NLP\napplications.", "AI": {"tldr": "本文提出分层段图记忆（HSGM）框架，通过将长文档分解为语义段并构建分层图记忆，有效解决了长文档语义解析中二次增长的复杂度和内存问题，实现了可扩展、准确的语义建模。", "motivation": "长文档的语义解析面临挑战，主要原因是成对组合和内存需求呈二次增长，导致计算成本过高。", "method": "HSGM将长文档（长度N）分解为M个有意义的段，在每个段上构建“局部语义图”，并提取紧凑的“摘要节点”以形成“全局图记忆”。它支持增量更新和分层查询处理，通过摘要节点定位相关段，然后在局部图中进行细粒度推理。", "result": "理论上，HSGM将最坏情况下的复杂度从O(N^2)降低到O(N k + (N/k)^2)，并推导了节点摘要和稀疏化阈值引入的近似误差的Frobenius范数界限。经验上，在三个基准测试（长文档AMR解析、段级语义角色标注和法律事件提取）中，HSGM实现了2-4倍的推理速度提升，超过60%的峰值内存减少，以及不低于95%的基线准确率。", "conclusion": "HSGM方法为超长文本的语义建模提供了可扩展、准确的解决方案，使其能够应用于实时和资源受限的自然语言处理场景。"}}
{"id": "2509.18170", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18170", "abs": "https://arxiv.org/abs/2509.18170", "authors": ["Zhanting Zhou", "Jinbo Wang", "Zeqin Wu", "Fengli Zhang"], "title": "MAGIA: Sensing Per-Image Signals from Single-Round Averaged Gradients for Label-Inference-Free Gradient Inversion", "comment": null, "summary": "We study gradient inversion in the challenging single round averaged gradient\nSAG regime where per sample cues are entangled within a single batch mean\ngradient. We introduce MAGIA a momentum based adaptive correction on gradient\ninversion attack a novel label inference free framework that senses latent per\nimage signals by probing random data subsets. MAGIA objective integrates two\ncore innovations 1 a closed form combinatorial rescaling that creates a\nprovably tighter optimization bound and 2 a momentum based mixing of whole\nbatch and subset losses to ensure reconstruction robustness. Extensive\nexperiments demonstrate that MAGIA significantly outperforms advanced methods\nachieving high fidelity multi image reconstruction in large batch scenarios\nwhere prior works fail. This is all accomplished with a computational footprint\ncomparable to standard solvers and without requiring any auxiliary information.", "AI": {"tldr": "MAGIA是一种基于动量的自适应梯度反演攻击修正方法，通过组合重缩放和动量混合，在单轮平均梯度（SAG）和大批量场景下，无需辅助信息即可实现高保真多图像重建，显著优于现有方法。", "motivation": "在单轮平均梯度（SAG）机制中，每个样本的线索纠缠在一个批次平均梯度内，使得梯度反演攻击极具挑战性。现有方法在大批量场景下表现不佳或失效，研究旨在解决这一问题，实现高保真多图像重建。", "method": "引入了MAGIA（基于动量的自适应修正梯度反演攻击），这是一个新颖的无标签推理框架，通过探测随机数据子集来感知潜在的单图像信号。MAGIA的目标函数整合了两项核心创新：1. 闭式组合重缩放，创建了可证明更紧密的优化边界；2. 基于动量的全批次和子集损失混合，以确保重建的鲁棒性。", "result": "MAGIA显著优于先进方法，在现有工作失败的大批量场景中实现了高保真多图像重建。其计算开销与标准求解器相当，且无需任何辅助信息。", "conclusion": "MAGIA提供了一种在具有挑战性的单轮平均梯度（SAG）和大批量场景下进行梯度反演的有效解决方案，通过其创新的优化和混合策略，实现了鲁棒且高保真的图像重建，同时保持了计算效率和独立性。"}}
{"id": "2509.18809", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2509.18809", "abs": "https://arxiv.org/abs/2509.18809", "authors": ["Dehui Yang", "Feng Xi", "Qihao Cao", "Huizhang Yang"], "title": "RFI Removal from SAR Imagery via Sparse Parametric Estimation of LFM Interferences", "comment": null, "summary": "One of the challenges in spaceborne synthetic aperture radar (SAR) is\nmodeling and mitigating radio frequency interference (RFI) artifacts in SAR\nimagery. Linear frequency modulated (LFM) signals have been commonly used for\ncharacterizing the radar interferences in SAR. In this letter, we propose a new\nsignal model that approximates RFI as a mixture of multiple LFM components in\nthe focused SAR image domain. The azimuth and range frequency modulation (FM)\nrates for each LFM component are estimated effectively using a sparse\nparametric representation of LFM interferences with a discretized LFM\ndictionary. This approach is then tested within the recently developed RFI\nsuppression framework using a 2-D SPECtral ANalysis (2-D SPECAN) algorithm\nthrough LFM focusing and notch filtering in the spectral domain [1].\nExperimental studies on Sentinel-1 single-look complex images demonstrate that\nthe proposed LFM model and sparse parametric estimation scheme outperforms\nexisting RFI removal methods.", "AI": {"tldr": "本文提出了一种新的信号模型，将合成孔径雷达（SAR）图像中的射频干扰（RFI）建模为多个线性调频（LFM）分量的混合，并通过稀疏参数估计方法有效去除RFI。", "motivation": "在星载合成孔径雷达（SAR）中，建模和减轻SAR图像中的射频干扰（RFI）伪影是一个重要挑战。", "method": "提出了一种新的信号模型，将聚焦后的SAR图像域中的RFI近似为多个LFM分量的混合。使用离散LFM字典的LFM干扰稀疏参数表示，有效估计每个LFM分量的方位和距离调频（FM）速率。该方法随后在现有RFI抑制框架内，通过2-D SPECAN算法（通过LFM聚焦和频谱域陷波滤波）进行测试。", "result": "在Sentinel-1单视复图像上的实验研究表明，所提出的LFM模型和稀疏参数估计方案优于现有的RFI去除方法。", "conclusion": "新的LFM模型和稀疏参数估计方案能有效建模和抑制SAR图像中的射频干扰，并表现出优于现有方法的性能。"}}
{"id": "2509.18518", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.18518", "abs": "https://arxiv.org/abs/2509.18518", "authors": ["Bai Xue", "Luke Ong", "Dominik Wagner", "Peixin Wang"], "title": "Refined Barrier Conditions for Finite-Time Safety and Reach-Avoid Guarantees in Stochastic Systems", "comment": null, "summary": "Providing finite-time probabilistic safety and reach-avoid guarantees is\ncrucial for safety-critical stochastic systems. Existing barrier certificate\nmethods often rely on a restrictive boundedness assumption for auxiliary\nfunctions, limiting their applicability. This paper presents refined\nbarrier-like conditions that remove this assumption. Specifically, we establish\nconditions for deriving upper bounds on finite-time safety probabilities in\ndiscrete-time systems and lower bounds on finite-time reach-avoid probabilities\nin continuous-time systems. This key relaxation significantly expands the class\nof verifiable systems, especially those with unbounded state spaces, and\nfacilitates the application of advanced optimization techniques, such as\nsemi-definite programming with polynomial functions. The efficacy of our\napproach is validated through numerical examples.", "AI": {"tldr": "本文提出了一种改进的类障碍条件，取消了辅助函数的有界性假设，从而能够为离散时间系统的有限时间安全概率和连续时间系统的有限时间到达-规避概率提供保证。", "motivation": "现有的障碍证书方法常依赖辅助函数的限制性有界性假设，这限制了它们在安全关键型随机系统中的应用范围。", "method": "通过建立新的类障碍条件，本文移除了辅助函数的有界性假设。具体来说，针对离散时间系统推导了有限时间安全概率的上限，针对连续时间系统推导了有限时间到达-规避概率的下限。", "result": "这一关键的放松显著扩展了可验证系统的类别，特别是对于无界状态空间系统，并促进了半定规划等先进优化技术的应用。方法有效性已通过数值例子验证。", "conclusion": "所提出的方法通过移除有界性假设，极大地扩展了障碍证书在随机系统安全性和到达-规避问题中的适用性，尤其是在处理无界状态空间时，并支持更先进的优化技术。"}}
{"id": "2509.18163", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18163", "abs": "https://arxiv.org/abs/2509.18163", "authors": ["Haodong Zhao", "Chenyan Zhao", "Yansi Li", "Zhuosheng Zhang", "Gongshen Liu"], "title": "Thinking in a Crowd: How Auxiliary Information Shapes LLM Reasoning", "comment": "Work in progress", "summary": "The capacity of Large Language Models (LLMs) to reason is fundamental to\ntheir application in complex, knowledge-intensive domains. In real-world\nscenarios, LLMs are often augmented with external information that can be\nhelpful, irrelevant, or even misleading. This paper investigates the causal\nimpact of such auxiliary information on the reasoning process of LLMs with\nexplicit step-by-step thinking capabilities. We introduce SciAux, a new dataset\nderived from ScienceQA, to systematically test the robustness of the model\nagainst these types of information. Our findings reveal a critical\nvulnerability: the model's deliberative \"thinking mode\" is a double-edged\nsword. While helpful context improves accuracy, misleading information causes a\ncatastrophic drop in performance, which is amplified by the thinking process.\nInstead of conferring robustness, thinking reinforces the degree of error when\nprovided with misinformation. This highlights that the challenge is not merely\nto make models \"think\", but to endow them with the critical faculty to evaluate\nthe information upon which their reasoning is based. The SciAux dataset is\navailable at https://huggingface.co/datasets/billhdzhao/SciAux.", "AI": {"tldr": "研究发现，大语言模型（LLMs）的逐步推理能力在面对辅助信息时是把双刃剑：有益信息能提高准确性，但误导信息会导致灾难性的性能下降，且这种下降会被其“思考”过程放大。", "motivation": "大语言模型在复杂领域中的推理能力至关重要，且通常会结合外部信息。然而，这些外部信息可能有用、无关甚至具有误导性。本研究旨在探讨这些辅助信息对具有明确逐步思考能力的大语言模型推理过程的因果影响。", "method": "通过引入一个名为SciAux的新数据集（源自ScienceQA），系统地测试模型对不同类型辅助信息（有益、无关、误导）的鲁棒性，并调查其对大语言模型逐步思考过程的影响。", "result": "研究发现，模型的“思考模式”是一把双刃剑。有益的上下文信息能提高准确性，但误导性信息会导致性能灾难性下降，并且这种下降会被思考过程放大。思考非但没有增强鲁棒性，反而会在接收到错误信息时加剧错误程度。", "conclusion": "挑战并非仅仅在于让模型“思考”，更在于赋予它们评估其推理所依据信息的能力。模型需要具备批判性思维，以甄别和评估辅助信息的质量。"}}
{"id": "2509.18342", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18342", "abs": "https://arxiv.org/abs/2509.18342", "authors": ["Rajitha de Silva", "Jonathan Cox", "James R. Heselden", "Marija Popovic", "Cesar Cadena", "Riccardo Polvara"], "title": "Semantic-Aware Particle Filter for Reliable Vineyard Robot Localisation", "comment": "Sumbitted to ICRA 2026", "summary": "Accurate localisation is critical for mobile robots in structured outdoor\nenvironments, yet LiDAR-based methods often fail in vineyards due to repetitive\nrow geometry and perceptual aliasing. We propose a semantic particle filter\nthat incorporates stable object-level detections, specifically vine trunks and\nsupport poles into the likelihood estimation process. Detected landmarks are\nprojected into a birds eye view and fused with LiDAR scans to generate semantic\nobservations. A key innovation is the use of semantic walls, which connect\nadjacent landmarks into pseudo-structural constraints that mitigate row\naliasing. To maintain global consistency in headland regions where semantics\nare sparse, we introduce a noisy GPS prior that adaptively supports the filter.\nExperiments in a real vineyard demonstrate that our approach maintains\nlocalisation within the correct row, recovers from deviations where AMCL fails,\nand outperforms vision-based SLAM methods such as RTAB-Map.", "AI": {"tldr": "本文提出了一种语义粒子滤波器，通过融合稳定的物体级检测（如藤蔓树干和支撑杆）以及“语义墙”作为结构约束，解决了移动机器人在葡萄园中因LiDAR感知混叠导致的定位失败问题，并在语义稀疏的区域引入噪声GPS先验以保持全局一致性。", "motivation": "移动机器人在结构化的户外环境中（如葡萄园）需要精确的定位，但LiDAR方法常因重复的行几何结构和感知混叠而失效。", "method": "采用语义粒子滤波器，将藤蔓树干和支撑杆等稳定的物体级检测融入似然估计。检测到的地标被投影到鸟瞰图并与LiDAR扫描融合以生成语义观测。关键创新是使用“语义墙”将相邻地标连接成伪结构约束，以缓解行混叠。为在语义稀疏的田头区域保持全局一致性，引入了自适应支持滤波器的带噪声GPS先验。", "result": "在真实葡萄园的实验表明，该方法能够将定位保持在正确的行内，在AMCL失效的情况下能够从偏差中恢复，并且优于RTAB-Map等基于视觉的SLAM方法。", "conclusion": "所提出的语义粒子滤波器通过结合物体级语义信息和自适应GPS先验，有效解决了葡萄园等重复结构环境中机器人定位的挑战，并展现出优于现有方法的性能。"}}
{"id": "2509.18178", "categories": ["cs.AI", "cs.CE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18178", "abs": "https://arxiv.org/abs/2509.18178", "authors": ["Ling Yue", "Nithin Somasekharan", "Tingwen Zhang", "Yadi Cao", "Shaowu Pan"], "title": "Foam-Agent: An End-to-End Composable Multi-Agent Framework for Automating CFD Simulation in OpenFOAM", "comment": null, "summary": "Computational Fluid Dynamics (CFD) is an essential simulation tool in\nengineering, yet its steep learning curve and complex manual setup create\nsignificant barriers. To address these challenges, we introduce Foam-Agent, a\nmulti-agent framework that automates the entire end-to-end OpenFOAM workflow\nfrom a single natural language prompt. Our key innovations address critical\ngaps in existing systems: 1. An Comprehensive End-to-End Simulation Automation:\nFoam-Agent is the first system to manage the full simulation pipeline,\nincluding advanced pre-processing with a versatile Meshing Agent capable of\nhandling external mesh files and generating new geometries via Gmsh, automatic\ngeneration of HPC submission scripts, and post-simulation visualization via\nParaView. 2. Composable Service Architecture: Going beyond a monolithic agent,\nthe framework uses Model Context Protocol (MCP) to expose its core functions as\ndiscrete, callable tools. This allows for flexible integration and use by other\nagentic systems, such as Claude-code, for more exploratory workflows. 3.\nHigh-Fidelity Configuration Generation: We achieve superior accuracy through a\nHierarchical Multi-Index RAG for precise context retrieval and a\ndependency-aware generation process that ensures configuration consistency.\nEvaluated on a benchmark of 110 simulation tasks, Foam-Agent achieves an 88.2%\nsuccess rate with Claude 3.5 Sonnet, significantly outperforming existing\nframeworks (55.5% for MetaOpenFOAM). Foam-Agent dramatically lowers the\nexpertise barrier for CFD, demonstrating how specialized multi-agent systems\ncan democratize complex scientific computing. The code is public at\nhttps://github.com/csml-rpi/Foam-Agent.", "AI": {"tldr": "Foam-Agent是一个多智能体框架，通过自然语言提示实现OpenFOAM计算流体动力学（CFD）模拟的端到端自动化，显著降低了使用门槛。", "motivation": "计算流体动力学（CFD）是工程领域重要的模拟工具，但其学习曲线陡峭且手动设置复杂，构成了显著的使用障碍。", "method": "引入了Foam-Agent，一个多智能体框架，其关键创新包括：1. 全面的端到端模拟自动化，涵盖高级预处理（网格生成、处理外部网格文件）、HPC提交脚本生成和后处理可视化。2. 可组合的服务架构，通过模型上下文协议（MCP）将核心功能暴露为可调用工具，便于集成。3. 高保真配置生成，利用分层多索引RAG进行精确上下文检索和依赖感知生成过程，确保配置一致性。", "result": "在110个模拟任务基准测试中，Foam-Agent与Claude 3.5 Sonnet结合实现了88.2%的成功率，显著优于现有框架（MetaOpenFOAM为55.5%）。", "conclusion": "Foam-Agent极大地降低了CFD的专业知识门槛，展示了专业多智能体系统如何使复杂的科学计算大众化。"}}
{"id": "2509.18174", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18174", "abs": "https://arxiv.org/abs/2509.18174", "authors": ["Khalil Hennara", "Muhammad Hreden", "Mohamed Motasim Hamed", "Ahmad Bastati", "Zeina Aldallal", "Sara Chrouf", "Safwan AlModhayan"], "title": "Baseer: A Vision-Language Model for Arabic Document-to-Markdown OCR", "comment": null, "summary": "Arabic document OCR remains a challenging task due to the language's cursive\nscript, diverse fonts, diacritics, and right-to-left orientation. While modern\nMultimodal Large Language Models (MLLMs) have advanced document understanding\nfor high-resource languages, their performance on Arabic remains limited. In\nthis work, we introduce Baseer, a vision-language model fine- tuned\nspecifically for Arabic document OCR. Leveraging a large-scale dataset\ncombining synthetic and real-world documents, Baseer is trained using a\ndecoder-only fine-tuning strategy to adapt a pre-trained MLLM while preserving\ngeneral visual features. We also present Misraj-DocOCR, a high-quality,\nexpert-verified benchmark designed for rigorous evaluation of Arabic OCR\nsystems. Our experiments show that Baseer significantly outperforms existing\nopen-source and commercial solutions, achieving a WER of 0.25 and establishing\na new state-of-the-art in the domain of Arabic document OCR. Our results\nhighlight the benefits of domain-specific adaptation of general-purpose MLLMs\nand establish a strong baseline for high-accuracy OCR on morphologically rich\nlanguages like Arabic.", "AI": {"tldr": "本文提出了Baseer，一个专门针对阿拉伯语文档OCR进行微调的视觉语言模型，通过结合合成和真实世界数据进行训练，并在新的Misraj-DocOCR基准上显著优于现有解决方案，达到了0.25的词错误率（WER），创造了新的SOTA。", "motivation": "阿拉伯语文档OCR由于其草书字体、多样字体、变音符号和从右到左的书写方向而极具挑战性。尽管现代多模态大型语言模型（MLLM）在处理高资源语言的文档理解方面取得了进展，但它们在阿拉伯语上的表现仍然有限。", "method": "本文引入了Baseer，一个专门为阿拉伯语文档OCR微调的视觉语言模型。通过结合合成和真实世界文档的大规模数据集，Baseer使用仅解码器微调策略进行训练，以适应预训练的MLLM，同时保留通用视觉特征。此外，本文还推出了Misraj-DocOCR，一个高质量、经过专家验证的基准，用于严格评估阿拉伯语OCR系统。", "result": "实验表明，Baseer显著优于现有的开源和商业解决方案，实现了0.25的词错误率（WER），并在阿拉伯语文档OCR领域建立了新的最先进水平。", "conclusion": "研究结果强调了对通用MLLM进行领域特定适应的优势，并为像阿拉伯语这样形态丰富的语言的高精度OCR建立了强大的基线。"}}
{"id": "2509.18815", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2509.18815", "abs": "https://arxiv.org/abs/2509.18815", "authors": ["Shimon Murai", "Fangzheng Lin", "Jiro Katto"], "title": "FlashGMM: Fast Gaussian Mixture Entropy Model for Learned Image Compression", "comment": "Accepted by IEEE VCIP 2025", "summary": "High-performance learned image compression codecs require flexible\nprobability models to fit latent representations. Gaussian Mixture Models\n(GMMs) were proposed to satisfy this demand, but suffer from a significant\nruntime performance bottleneck due to the large Cumulative Distribution\nFunction (CDF) tables that must be built for rANS coding. This paper introduces\na fast coding algorithm that entirely eliminates this bottleneck. By leveraging\nthe CDF's monotonic property, our decoder performs a dynamic binary search to\nfind the correct symbol, eliminating the need for costly table construction and\nlookup. Aided by SIMD optimizations and numerical approximations, our approach\naccelerates the GMM entropy coding process by up to approximately 90x without\ncompromising rate-distortion performance, significantly improving the\npracticality of GMM-based codecs. The implementation will be made publicly\navailable at https://github.com/tokkiwa/FlashGMM.", "AI": {"tldr": "本文提出了一种快速编码算法，通过动态二分搜索消除了高斯混合模型（GMM）在学习型图像压缩中由于CDF表引起的运行时瓶颈，将熵编码速度提升高达90倍，同时保持性能。", "motivation": "高性能学习型图像压缩编解码器需要灵活的概率模型来拟合潜在表示。高斯混合模型（GMM）能满足此需求，但其为rANS编码构建大型累积分布函数（CDF）表导致显著的运行时性能瓶颈。", "method": "该论文引入了一种快速编码算法，通过利用CDF的单调性，解码器执行动态二分搜索来查找正确的符号，从而无需昂贵的CDF表构建和查找。该方法还辅以SIMD优化和数值近似。", "result": "该方法将GMM熵编码过程加速了约90倍，且不影响码率-失真性能。", "conclusion": "该算法显著提高了基于GMM的编解码器的实用性。"}}
{"id": "2509.18526", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.18526", "abs": "https://arxiv.org/abs/2509.18526", "authors": ["Han Zeng", "Haibo Wang", "Luhao Fan", "Bingcheng Zhu", "Xiaohu You", "Zaichen Zhang"], "title": "AI Agent Access (A\\^3) Network: An Embodied, Communication-Aware Multi-Agent Framework for 6G Coverage", "comment": null, "summary": "The vision of 6G communication demands autonomous and resilient networking in\nenvironments without fixed infrastructure. Yet most multi-agent reinforcement\nlearning (MARL) approaches focus on isolated stages - exploration, relay\nformation, or access - under static deployments and centralized control,\nlimiting adaptability. We propose the AI Agent Access (A\\^3) Network, a\nunified, embodied intelligence-driven framework that transforms multi-agent\nnetworking into a dynamic, decentralized, and end-to-end system. Unlike prior\nschemes, the A\\^3 Network integrates exploration, target user access, and\nbackhaul maintenance within a single learning process, while supporting\non-demand agent addition during runtime. Its decentralized policies ensure that\neven a single agent can operate independently with limited observations, while\ncoordinated agents achieve scalable, communication-optimized coverage. By\nembedding link-level communication metrics into actor-critic learning, the A\\^3\nNetwork couples topology formation with robust decision-making. Numerical\nsimulations demonstrate that the A\\^3 Network not only balances exploration and\ncommunication efficiency but also delivers system-level adaptability absent in\nexisting MARL frameworks, offering a new paradigm for 6G multi-agent networks.", "AI": {"tldr": "针对6G无基础设施环境的自主弹性网络需求，本文提出A^3网络，一个统一、去中心化、端到端的多智能体强化学习框架，将探索、用户接入和回传维护整合到单一学习过程，支持运行时智能体添加，并通过通信指标优化拓扑形成，实现了探索与通信效率的平衡以及系统级适应性。", "motivation": "6G通信要求在无固定基础设施的环境中实现自主和弹性的网络。然而，大多数多智能体强化学习（MARL）方法只关注孤立的阶段（如探索、中继形成或接入），且在静态部署和集中控制下运行，这限制了它们的适应性。", "method": "本文提出了AI Agent Access (A^3) 网络，这是一个统一的、具身智能驱动的框架，将多智能体网络转化为动态、去中心化和端到端的系统。它将探索、目标用户接入和回传维护整合到单一学习过程中，并支持在运行时按需添加智能体。A^3网络采用去中心化策略，使单个智能体能够在有限观测下独立运行，同时协调智能体实现可扩展、通信优化的覆盖。通过将链路级通信指标嵌入到Actor-Critic学习中，A^3网络将拓扑形成与鲁棒决策相结合。", "result": "数值模拟表明，A^3网络不仅能在探索和通信效率之间取得平衡，而且还提供了现有MARL框架所缺乏的系统级适应性。", "conclusion": "A^3网络为6G多智能体网络提供了一个新的范式，通过其统一的、去中心化的和端到端的方法，解决了现有MARL方案的局限性。"}}
{"id": "2509.18167", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18167", "abs": "https://arxiv.org/abs/2509.18167", "authors": ["Junlin Wang", "Zehao Wu", "Shaowei Lu", "Yanlan Li", "Xinghao Huang"], "title": "SIRAG: Towards Stable and Interpretable RAG with A Process-Supervised Multi-Agent Framework", "comment": "5 pages,2 figures, IRAC under review", "summary": "Retrieval-Augmented Generation (RAG) enables large language models (LLMs) to\naccess external knowledge sources, but the effectiveness of RAG relies on the\ncoordination between the retriever and the generator. Since these components\nare developed independently, their interaction is often suboptimal: the\nretriever may return irrelevant or redundant documents, while the generator may\nfail to fully leverage retrieved evidence. In this work, we propose a\nprocess-supervised multi-agent framework to bridge the gap between retriever\nand generator. The framework introduces two lightweight agents: a Decision\nMaker, which determines when to continue retrieval or stop for answer\ngeneration, and a Knowledge Selector, which filters retrieved documents to\nretain only the most useful evidence. To provide fine-grained supervision, we\nemploy an LLM-as-a-Judge that evaluates each intermediate action with\nprocess-level rewards, ensuring more accurate credit assignment than relying\nsolely on final answer correctness. We further adopt a tree-structured rollout\nstrategy to explore diverse reasoning paths, and train both agents with\nProximal Policy Optimization (PPO) in an end-to-end manner. Experiments on\nsingle-hop and multi-hop question answering benchmarks show that our approach\nachieves higher accuracy, more stable convergence, and produces more\ninterpretable reasoning trajectories compared with standard RAG baselines.\nImportantly, the proposed framework is modular and plug-and-play, requiring no\nmodification to the retriever or generator, making it practical for real-world\nRAG applications.", "AI": {"tldr": "本文提出了一种过程监督的多智能体框架，通过引入决策者和知识选择器，并使用LLM作为评判者提供细粒度奖励，以优化检索增强生成（RAG）中检索器和生成器之间的协调，从而提高问答准确性。", "motivation": "RAG的效果受限于检索器和生成器之间次优的协调。检索器可能返回不相关或冗余的文档，而生成器可能未能充分利用检索到的证据，因为这两个组件是独立开发的。", "method": "提出了一个过程监督的多智能体框架，包含两个轻量级智能体：一个决策者（决定何时继续检索或停止生成答案）和一个知识选择器（过滤检索到的文档以保留最有用证据）。通过将LLM用作评判者，为每个中间动作提供过程级奖励，以实现更准确的归因。采用树状展开策略探索不同的推理路径，并使用近端策略优化（PPO）端到端训练两个智能体。该框架是模块化和即插即用的，无需修改检索器或生成器。", "result": "在单跳和多跳问答基准测试中，该方法实现了更高的准确性、更稳定的收敛，并产生了比标准RAG基线更可解释的推理轨迹。该框架的模块化和即插即用特性使其适用于实际的RAG应用。", "conclusion": "该框架通过引入智能体和过程级监督，成功弥合了检索器和生成器之间的鸿沟，显著提升了RAG的性能和可解释性，且无需修改现有组件，具有很高的实用价值。"}}
{"id": "2509.18384", "categories": ["cs.RO", "cs.FL"], "pdf": "https://arxiv.org/pdf/2509.18384", "abs": "https://arxiv.org/abs/2509.18384", "authors": ["Yunhao Yang", "Junyuan Hong", "Gabriel Jacob Perin", "Zhiwen Fan", "Li Yin", "Zhangyang Wang", "Ufuk Topcu"], "title": "AD-VF: LLM-Automatic Differentiation Enables Fine-Tuning-Free Robot Planning from Formal Methods Feedback", "comment": null, "summary": "Large language models (LLMs) can translate natural language instructions into\nexecutable action plans for robotics, autonomous driving, and other domains.\nYet, deploying LLM-driven planning in the physical world demands strict\nadherence to safety and regulatory constraints, which current models often\nviolate due to hallucination or weak alignment. Traditional data-driven\nalignment methods, such as Direct Preference Optimization (DPO), require costly\nhuman labeling, while recent formal-feedback approaches still depend on\nresource-intensive fine-tuning. In this paper, we propose LAD-VF, a\nfine-tuning-free framework that leverages formal verification feedback for\nautomated prompt engineering. By introducing a formal-verification-informed\ntext loss integrated with LLM-AutoDiff, LAD-VF iteratively refines prompts\nrather than model parameters. This yields three key benefits: (i) scalable\nadaptation without fine-tuning; (ii) compatibility with modular LLM\narchitectures; and (iii) interpretable refinement via auditable prompts.\nExperiments in robot navigation and manipulation tasks demonstrate that LAD-VF\nsubstantially enhances specification compliance, improving success rates from\n60% to over 90%. Our method thus presents a scalable and interpretable pathway\ntoward trustworthy, formally-verified LLM-driven control systems.", "AI": {"tldr": "LAD-VF是一个无需微调的框架，它利用形式化验证反馈自动进行提示工程，以提高大型语言模型在机器人等领域规划时对安全和规范的依从性，将成功率从60%提升至90%以上。", "motivation": "大型语言模型（LLMs）在将自然语言指令转换为可执行行动计划方面表现出色，但在物理世界部署时，常因幻觉或弱对齐而违反安全和监管约束。传统的对齐方法（如DPO）需要昂贵的人工标注，而现有形式化反馈方法则依赖于资源密集型微调。", "method": "本文提出了LAD-VF框架，它无需微调。该方法利用形式化验证反馈进行自动化提示工程。通过将形式化验证信息文本损失与LLM-AutoDiff结合，LAD-VF迭代地优化提示而非模型参数。", "result": "在机器人导航和操作任务中的实验表明，LAD-VF显著增强了规范依从性，将成功率从60%提高到90%以上。该方法实现了可扩展的适应性、与模块化LLM架构的兼容性以及通过可审计提示进行的可解释优化。", "conclusion": "LAD-VF为构建值得信赖、经过形式化验证的LLM驱动控制系统提供了一条可扩展且可解释的途径。"}}
{"id": "2509.18180", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18180", "abs": "https://arxiv.org/abs/2509.18180", "authors": ["Yang Wang", "Kai Li"], "title": "Large Language Models and Operations Research: A Structured Survey", "comment": null, "summary": "Operations research (OR) provides fundamental methodologies for complex\nsystem decision-making, with established applications in transportation, supply\nchain management, and production scheduling. Traditional approaches, which\ndepend on expert-based modeling and manual parameter adjustment, often face\nchallenges in handling large-scale, dynamic, and multi-constraint problems.\nRecently, large language models (LLMs) have shown potential to address these\nlimitations through semantic understanding, structured generation, and\nreasoning control. LLMs can translate natural language descriptions into\nmathematical models or executable code, generate heuristics, evolve algorithms,\nand directly tackle optimization tasks. This paper surveys recent progress on\nthe integration of LLMs into OR, organizing methods into three main directions:\nautomatic modeling, auxiliary optimization, and direct solving. It further\nreviews evaluation benchmarks and domain-specific applications, and summarizes\nkey open issues such as unstable semantic-to-structure mapping, fragmented\nresearch progress, limited generalization, and insufficient evaluation systems.\nFinally, the survey outlines possible research avenues for advancing the role\nof LLMs in OR.", "AI": {"tldr": "本文综述了大型语言模型（LLMs）在运筹学（OR）中的最新进展，将其整合方法分为自动建模、辅助优化和直接求解三类，并探讨了评估基准、应用、开放性问题及未来研究方向。", "motivation": "传统的运筹学方法在处理大规模、动态、多约束问题时，由于依赖专家建模和手动参数调整，面临诸多挑战。大型语言模型（LLMs）凭借其语义理解、结构化生成和推理能力，有望解决这些局限性。", "method": "本文通过文献综述的方式，将LLMs与OR的整合方法归纳为三个主要方向：自动建模、辅助优化和直接求解。同时，文章还回顾了评估基准和特定领域应用。", "result": "LLMs能够将自然语言描述转化为数学模型或可执行代码，生成启发式算法，演化算法，并直接处理优化任务。然而，研究发现主要存在语义到结构映射不稳定、研究进展碎片化、泛化能力有限以及评估系统不足等关键开放性问题。", "conclusion": "LLMs在运筹学中展现出巨大潜力，但仍面临诸多挑战。本综述总结了当前进展，指出了开放性问题，并为未来LLMs在运筹学领域的深入研究提供了方向。"}}
{"id": "2509.18176", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18176", "abs": "https://arxiv.org/abs/2509.18176", "authors": ["Wendong Yao", "Saeed Azadnejad", "Binhua Huang", "Shane Donohue", "Soumyabrata Dev"], "title": "A Deep Learning Approach for Spatio-Temporal Forecasting of InSAR Ground Deformation in Eastern Ireland", "comment": "This paper is submitted to IEEE Transactions on Geoscience and Remote\n  Sensing", "summary": "Monitoring ground displacement is crucial for urban infrastructure stability\nand mitigating geological hazards. However, forecasting future deformation from\nsparse Interferometric Synthetic Aperture Radar (InSAR) time-series data\nremains a significant challenge. This paper introduces a novel deep learning\nframework that transforms these sparse point measurements into a dense\nspatio-temporal tensor. This methodological shift allows, for the first time,\nthe direct application of advanced computer vision architectures to this\nforecasting problem. We design and implement a hybrid Convolutional Neural\nNetwork and Long-Short Term Memory (CNN-LSTM) model, specifically engineered to\nsimultaneously learn spatial patterns and temporal dependencies from the\ngenerated data tensor. The model's performance is benchmarked against powerful\nmachine learning baselines, Light Gradient Boosting Machine and LASSO\nregression, using Sentinel-1 data from eastern Ireland. Results demonstrate\nthat the proposed architecture provides significantly more accurate and\nspatially coherent forecasts, establishing a new performance benchmark for this\ntask. Furthermore, an interpretability analysis reveals that baseline models\noften default to simplistic persistence patterns, highlighting the necessity of\nour integrated spatio-temporal approach to capture the complex dynamics of\nground deformation. Our findings confirm the efficacy and potential of\nspatio-temporal deep learning for high-resolution deformation forecasting.", "AI": {"tldr": "本文提出了一种新颖的深度学习框架（CNN-LSTM），能将稀疏InSAR数据转换为密集时空张量，首次实现计算机视觉架构直接应用于地面形变预测，并显著提高了预测的准确性和空间一致性。", "motivation": "监测地面位移对城市基础设施稳定和地质灾害防治至关重要。然而，从稀疏的合成孔径雷达干涉测量（InSAR）时间序列数据中预测未来形变是一个重大挑战。", "method": "该研究引入了一个深度学习框架，将稀疏点测量数据转换为密集的时空张量，从而可以直接应用先进的计算机视觉架构。设计并实现了一个混合卷积神经网络和长短期记忆网络（CNN-LSTM）模型，用于同时学习生成数据张量中的空间模式和时间依赖性。模型性能通过与Light Gradient Boosting Machine和LASSO回归等机器学习基线模型进行比较，使用了爱尔兰东部的Sentinel-1数据。", "result": "结果表明，所提出的架构提供了显著更准确和空间一致的预测，为该任务建立了新的性能基准。此外，可解释性分析揭示基线模型常默认简单的持久性模式，凸显了集成时空方法捕捉地面形变复杂动态的必要性。", "conclusion": "研究结果证实了时空深度学习在高分辨率形变预测中的有效性和潜力。"}}
{"id": "2509.19192", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2509.19192", "abs": "https://arxiv.org/abs/2509.19192", "authors": ["Yiyang Liu", "Rongxuan Zhang", "Istvan Gyongy", "Alistair Gorman", "Sarrah M. Patanwala", "Filip Taneski", "Robert K. Henderson"], "title": "An on-chip Pixel Processing Approach with 2.4μs latency for Asynchronous Read-out of SPAD-based dToF Flash LiDARs", "comment": null, "summary": "We propose a fully asynchronous peak detection approach for SPAD-based direct\ntime-of-flight (dToF) flash LiDAR, enabling pixel-wise event-driven depth\nacquisition without global synchronization. By allowing pixels to independently\nreport depth once a sufficient signal-to-noise ratio is achieved, the method\nreduces latency, mitigates motion blur, and increases effective frame rate\ncompared to frame-based systems. The framework is validated under two hardware\nimplementations: an offline 256$\\times$128 SPAD array with PC based processing\nand a real-time FPGA proof-of-concept prototype with 2.4$\\upmu$s latency for\non-chip integration. Experiments demonstrate robust depth estimation,\nreflectivity reconstruction, and dynamic event-based representation under both\nstatic and dynamic conditions. The results confirm that asynchronous operation\nreduces redundant background data and computational load, while remaining\ntunable via simple hyperparameters. These findings establish a foundation for\ncompact, low-latency, event-driven LiDAR architectures suited to robotics,\nautonomous driving, and consumer applications. In addition, we have derived a\nsemi-closed-form solution for the detection probability of the raw-peak finding\nbased LiDAR systems that could benefit both conventional frame-based and\nproposed asynchronous LiDAR systems.", "AI": {"tldr": "本文提出了一种用于SPAD直方图ToF闪光雷达的全异步峰值检测方法，实现了像素级的事件驱动深度采集，无需全局同步，从而降低延迟、减少运动模糊并提高有效帧率。", "motivation": "传统基于帧的LiDAR系统存在高延迟、运动模糊和较低的有效帧率等问题。研究旨在开发一种紧凑、低延迟、事件驱动的LiDAR架构，适用于机器人、自动驾驶和消费级应用。", "method": "该方法提出了一种全异步峰值检测方法，允许每个像素在达到足够的信噪比时独立报告深度信息。通过两种硬件实现进行验证：一个离线256x128 SPAD阵列（PC处理）和一个实时FPGA原型（2.4µs片上集成延迟）。此外，还推导了基于原始峰值查找的LiDAR系统检测概率的半封闭解。", "result": "实验结果表明，在静态和动态条件下，该方法都能实现鲁棒的深度估计、反射率重建和动态事件表示。异步操作减少了冗余背景数据和计算负载，且可通过简单超参数进行调节。FPGA原型实现了2.4µs的低延迟。", "conclusion": "研究结果为紧凑、低延迟、事件驱动的LiDAR架构奠定了基础，适用于机器人、自动驾驶和消费级应用。推导出的检测概率解决方案对传统和提出的异步LiDAR系统均有益处。"}}
{"id": "2509.18624", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.18624", "abs": "https://arxiv.org/abs/2509.18624", "authors": ["Yue Zhang", "Xinzhi Zhong", "Soyoung Ahn", "Yajie Zou", "Zhengbing He"], "title": "Interaction-aware Lane-Changing Early Warning System in Congested Traffic", "comment": null, "summary": "Lane changes (LCs) in congested traffic are complex, multi-vehicle\ninteractive events that pose significant safety concerns. Providing early\nwarnings can enable more proactive driver assistance system and support more\ninformed decision-making for drivers under LCs. This paper presents an\ninteraction-aware Lane-Changing Early Warning (LCEW) system designed to issue\nreliable early warning signals based on future trajectory predictions. We first\ninvestigate the stochastic nature of LCs, characterized by (i) variable-size\nmulti-vehicle interactions and (ii) the direct and indirect risks resulting\nfrom these interactions. To model these stochastic interactions, a Social\nSpatio-Temporal Graph Convolutional Neural Network framework informed by mutual\ninformation (STGCNN-MI) is introduced to predict multi-vehicle trajectories. By\nleveraging a MI-based adjacency matrix, the framework enhances trajectory\nprediction accuracy while providing interpretable representations of vehicle\ninteractions. Then, potential collisions between the LC vehicle and adjacent\nvehicles (direct risks) or among the non-adjacent vehicles (indirect risks) are\nidentified using oriented bounding box detection applied to the predicted\ntrajectories. Finally, a warning signal is generated to inform the LC driver of\nlocation of potential collisions within the predicted time window. Traffic\nsimulation experiments conducted in SUMO demonstrate that the proposed\ninteraction-aware LCEW improves both vehicle-level safety and overall traffic\nefficiency, while also promoting more natural behavioral adaptation.", "AI": {"tldr": "本文提出了一种交互感知的变道预警（LCEW）系统，通过基于互信息（MI）的社交时空图卷积神经网络（STGCNN-MI）预测多车辆轨迹，识别潜在碰撞，从而提供可靠的早期预警信号，以提高交通安全和效率。", "motivation": "拥堵交通中的变道行为复杂，涉及多车辆交互，并带来严重的安全隐患。提供早期预警可以实现更主动的驾驶辅助系统，并支持驾驶员在变道时做出更明智的决策。", "method": "1. 调查变道的随机性，包括可变大小的多车辆交互以及由此产生的直接和间接风险。\n2. 引入基于互信息（MI）的社交时空图卷积神经网络（STGCNN-MI）框架来预测多车辆轨迹，该框架利用MI基邻接矩阵增强预测精度并提供可解释的车辆交互表示。\n3. 使用应用于预测轨迹的定向包围盒检测来识别变道车辆与相邻车辆之间的潜在碰撞（直接风险）或非相邻车辆之间的潜在碰撞（间接风险）。\n4. 生成警告信号，通知变道驾驶员预测时间窗内潜在碰撞的位置。\n5. 在SUMO交通模拟中进行实验验证。", "result": "所提出的交互感知LCEW系统在SUMO交通模拟中表现出显著效果，它同时提高了车辆级别的安全性和整体交通效率，并促进了更自然的驾驶行为适应。", "conclusion": "该交互感知变道早期预警系统通过准确的多车辆未来轨迹预测，能够提供可靠的早期预警信号，有效提升拥堵交通中变道的安全性，并改善整体交通效率和驾驶行为适应性。"}}
{"id": "2509.18175", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18175", "abs": "https://arxiv.org/abs/2509.18175", "authors": ["Aditi Debsharma", "Bhushan Jagyasi", "Surajit Sen", "Priyanka Pandey", "Devicharith Dovari", "Yuvaraj V. C", "Rosalin Parida", "Gopali Contractor"], "title": "ERFC: Happy Customers with Emotion Recognition and Forecasting in Conversation in Call Centers", "comment": "7 pages, 6 Figures, 4 Tables, 18 References", "summary": "Emotion Recognition in Conversation has been seen to be widely applicable in\ncall center analytics, opinion mining, finance, retail, healthcare, and other\nindustries. In a call center scenario, the role of the call center agent is not\njust confined to receiving calls but to also provide good customer experience\nby pacifying the frustration or anger of the customers. This can be achieved by\nmaintaining neutral and positive emotion from the agent. As in any\nconversation, the emotion of one speaker is usually dependent on the emotion of\nother speaker. Hence the positive emotion of an agent, accompanied with the\nright resolution will help in enhancing customer experience. This can change an\nunhappy customer to a happy one. Imparting the right resolution at right time\nbecomes easier if the agent has the insight of the emotion of future\nutterances. To predict the emotions of the future utterances we propose a novel\narchitecture, Emotion Recognition and Forecasting in Conversation. Our proposed\nERFC architecture considers multi modalities, different attributes of emotion,\ncontext and the interdependencies of the utterances of the speakers in the\nconversation. Our intensive experiments on the IEMOCAP dataset have shown the\nfeasibility of the proposed ERFC. This approach can provide a tremendous\nbusiness value for the applications like call center, where the happiness of\ncustomer is utmost important.", "AI": {"tldr": "本文提出了一种新颖的对话情感识别与预测（ERFC）架构，通过考虑多模态、情感属性、上下文和语料依赖性，预测对话中未来话语的情感，以提升客户服务体验。", "motivation": "对话中的情感识别在多个行业有广泛应用，尤其在呼叫中心场景中，客服人员需要通过管理客户情绪来提供良好的客户体验。预测未来话语的情感可以帮助客服人员更及时地提供正确的解决方案，从而将不满意的客户转变为满意的客户。", "method": "提出了一种名为“对话情感识别与预测”（Emotion Recognition and Forecasting in Conversation, ERFC）的新型架构。该架构考虑了多模态信息、情感的不同属性、对话上下文以及对话中说话者话语之间的相互依赖性。", "result": "在IEMOCAP数据集上进行的密集实验表明，所提出的ERFC架构是可行的。", "conclusion": "该ERFC方法可以为呼叫中心等客户满意度至关重要的应用带来巨大的商业价值。"}}
{"id": "2509.18407", "categories": ["cs.RO", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.18407", "abs": "https://arxiv.org/abs/2509.18407", "authors": ["Navya Tiwari", "Joseph Vazhaeparampil", "Victoria Preston"], "title": "Assistive Decision-Making for Right of Way Navigation at Uncontrolled Intersections", "comment": "6 pages, 5 figures. Accepted as a poster at Northeast Robotics\n  Colloquium (NERC 2025). Extended abstract", "summary": "Uncontrolled intersections account for a significant fraction of roadway\ncrashes due to ambiguous right-of-way rules, occlusions, and unpredictable\ndriver behavior. While autonomous vehicle research has explored\nuncertainty-aware decision making, few systems exist to retrofit human-operated\nvehicles with assistive navigation support. We present a driver-assist\nframework for right-of-way reasoning at uncontrolled intersections, formulated\nas a Partially Observable Markov Decision Process (POMDP). Using a custom\nsimulation testbed with stochastic traffic agents, pedestrians, occlusions, and\nadversarial scenarios, we evaluate four decision-making approaches: a\ndeterministic finite state machine (FSM), and three probabilistic planners:\nQMDP, POMCP, and DESPOT. Results show that probabilistic planners outperform\nthe rule-based baseline, achieving up to 97.5 percent collision-free navigation\nunder partial observability, with POMCP prioritizing safety and DESPOT\nbalancing efficiency and runtime feasibility. Our findings highlight the\nimportance of uncertainty-aware planning for driver assistance and motivate\nfuture integration of sensor fusion and environment perception modules for\nreal-time deployment in realistic traffic environments.", "AI": {"tldr": "该研究提出了一个基于POMDP的驾驶辅助框架，用于在非受控交叉口进行路权推理，通过模拟评估发现概率规划器（如POMCP和DESPOT）在部分可观察性下显著优于确定性规则，实现了更高的无碰撞导航率。", "motivation": "非受控交叉口因路权规则模糊、视线遮挡和驾驶员行为不可预测而导致大量交通事故。尽管自动驾驶汽车研究已探索了不确定性感知决策，但现有为人类驾驶车辆提供辅助导航支持的系统较少。", "method": "将非受控交叉口的路权推理问题建模为部分可观察马尔可夫决策过程（POMDP）。使用自定义模拟测试平台，该平台包含随机交通代理、行人、遮挡和对抗性场景。评估了四种决策方法：确定性有限状态机（FSM）以及三种概率规划器：QMDP、POMCP和DESPOT。", "result": "结果表明，概率规划器优于基于规则的基线方法，在部分可观察性下实现了高达97.5%的无碰撞导航。其中，POMCP优先考虑安全性，而DESPOT在效率和运行时可行性之间取得了平衡。", "conclusion": "研究结果强调了不确定性感知规划对于驾驶辅助的重要性，并激励未来将传感器融合和环境感知模块集成，以实现在真实交通环境中的实时部署。"}}
{"id": "2509.18181", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18181", "abs": "https://arxiv.org/abs/2509.18181", "authors": ["Mustafa Sameen", "Xiaojian Zhang", "Xilei Zhao"], "title": "Synthesizing Attitudes, Predicting Actions (SAPA): Behavioral Theory-Guided LLMs for Ridesourcing Mode Choice Modeling", "comment": null, "summary": "Accurate modeling of ridesourcing mode choices is essential for designing and\nimplementing effective traffic management policies for reducing congestion,\nimproving mobility, and allocating resources more efficiently. Existing models\nfor predicting ridesourcing mode choices often suffer from limited predictive\naccuracy due to their inability to capture key psychological factors, and are\nfurther challenged by severe class imbalance, as ridesourcing trips comprise\nonly a small fraction of individuals' daily travel. To address these\nlimitations, this paper introduces the Synthesizing Attitudes, Predicting\nActions (SAPA) framework, a hierarchical approach that uses Large Language\nModels (LLMs) to synthesize theory-grounded latent attitudes to predict\nridesourcing choices. SAPA first uses an LLM to generate qualitative traveler\npersonas from raw travel survey data and then trains a propensity-score model\non demographic and behavioral features, enriched by those personas, to produce\nan individual-level score. Next, the LLM assigns quantitative scores to\ntheory-driven latent variables (e.g., time and cost sensitivity), and a final\nclassifier integrates the propensity score, latent-variable scores (with their\ninteraction terms), and observable trip attributes to predict ridesourcing mode\nchoice. Experiments on a large-scale, multi-year travel survey show that SAPA\nsignificantly outperforms state-of-the-art baselines, improving ridesourcing\nchoice predictions by up to 75.9% in terms of PR-AUC on a held-out test set.\nThis study provides a powerful tool for accurately predicting ridesourcing mode\nchoices, and provides a methodology that is readily transferable to various\napplications.", "AI": {"tldr": "本文提出SAPA框架，利用大型语言模型（LLM）合成基于理论的潜在态度，以显著提高网约车出行模式选择的预测准确性，解决了现有模型在心理因素捕捉和类别不平衡方面的局限。", "motivation": "现有的网约车模式选择预测模型在捕捉关键心理因素方面存在局限，导致预测准确性不足；同时，由于网约车出行占日常出行的比例较小，模型还面临严重的类别不平衡挑战。", "method": "本文引入了“合成态度，预测行动”（SAPA）框架，这是一种分层方法：首先，LLM从原始出行调查数据中生成定性旅客画像；其次，基于人口统计学和行为特征（并结合画像）训练倾向得分模型，生成个体得分；接着，LLM为理论驱动的潜在变量（如时间敏感度、成本敏感度）分配定量分数；最后，一个分类器整合倾向得分、潜在变量得分（及其交互项）和可观测的出行属性，预测网约车模式选择。", "result": "在大规模、多年的出行调查数据上进行的实验表明，SAPA框架显著优于现有最先进的基线模型，在保留测试集上的PR-AUC指标方面，网约车选择预测的准确性提高了高达75.9%。", "conclusion": "本研究提供了一个强大的工具，能够准确预测网约车模式选择，并提出了一种易于推广到各种应用场景的方法论。"}}
{"id": "2509.18177", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18177", "abs": "https://arxiv.org/abs/2509.18177", "authors": ["George Corrêa de Araújo", "Helena de Almeida Maia", "Helio Pedrini"], "title": "A Framework for Generating Artificial Datasets to Validate Absolute and Relative Position Concepts", "comment": "WIP", "summary": "In this paper, we present the Scrapbook framework, a novel methodology\ndesigned to generate extensive datasets for probing the learned concepts of\nartificial intelligence (AI) models. The framework focuses on fundamental\nconcepts such as object recognition, absolute and relative positions, and\nattribute identification. By generating datasets with a large number of\nquestions about individual concepts and a wide linguistic variation, the\nScrapbook framework aims to validate the model's understanding of these basic\nelements before tackling more complex tasks. Our experimental findings reveal\nthat, while contemporary models demonstrate proficiency in recognizing and\nenumerating objects, they encounter challenges in comprehending positional\ninformation and addressing inquiries with additional constraints. Specifically,\nthe MobileVLM-V2 model showed significant answer disagreements and plausible\nwrong answers, while other models exhibited a bias toward affirmative answers\nand struggled with questions involving geometric shapes and positional\ninformation, indicating areas for improvement in understanding and consistency.\nThe proposed framework offers a valuable instrument for generating diverse and\ncomprehensive datasets, which can be utilized to systematically assess and\nenhance the performance of AI models.", "AI": {"tldr": "论文提出了Scrapbook框架，一种生成大规模数据集的新方法，用于探究AI模型对基本概念的理解。研究发现，尽管模型在物体识别和计数方面表现出色，但在理解位置信息和处理带约束的查询时仍面临挑战。", "motivation": "在AI模型处理更复杂的任务之前，需要验证它们对物体识别、绝对和相对位置以及属性识别等基本概念的理解。", "method": "提出了Scrapbook框架，一种新颖的方法，通过生成包含大量关于独立概念和广泛语言变异问题的海量数据集，来探究AI模型的学习概念。", "result": "当代模型在物体识别和枚举方面表现熟练，但在理解位置信息和处理带额外约束的查询时遇到挑战。具体来说，MobileVLM-V2模型显示出显著的答案分歧和看似合理的错误答案，而其他模型则表现出肯定答案的偏向，并在涉及几何形状和位置信息的问题上表现不佳，表明在理解和一致性方面有待改进。", "conclusion": "所提出的Scrapbook框架提供了一个有价值的工具，可用于生成多样化和全面的数据集，从而系统地评估和提升AI模型的性能。"}}
{"id": "2509.19277", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19277", "abs": "https://arxiv.org/abs/2509.19277", "authors": ["Georgii Kolokolnikov", "Marie-Lena Schmalhofer", "Sophie Götz", "Lennart Well", "Said Farschtschi", "Victor-Felix Mautner", "Inka Ristow", "Rene Werner"], "title": "MOIS-SAM2: Exemplar-based Segment Anything Model 2 for multilesion interactive segmentation of neurobromas in whole-body MRI", "comment": null, "summary": "Background and Objectives: Neurofibromatosis type 1 is a genetic disorder\ncharacterized by the development of numerous neurofibromas (NFs) throughout the\nbody. Whole-body MRI (WB-MRI) is the clinical standard for detection and\nlongitudinal surveillance of NF tumor growth. Existing interactive segmentation\nmethods fail to combine high lesion-wise precision with scalability to hundreds\nof lesions. This study proposes a novel interactive segmentation model tailored\nto this challenge.\n  Methods: We introduce MOIS-SAM2, a multi-object interactive segmentation\nmodel that extends the state-of-the-art, transformer-based, promptable Segment\nAnything Model 2 (SAM2) with exemplar-based semantic propagation. MOIS-SAM2 was\ntrained and evaluated on 119 WB-MRI scans from 84 NF1 patients acquired using\nT2-weighted fat-suppressed sequences. The dataset was split at the patient\nlevel into a training set and four test sets (one in-domain and three\nreflecting different domain shift scenarios, e.g., MRI field strength\nvariation, low tumor burden, differences in clinical site and scanner vendor).\n  Results: On the in-domain test set, MOIS-SAM2 achieved a scan-wise DSC of\n0.60 against expert manual annotations, outperforming baseline 3D nnU-Net (DSC:\n0.54) and SAM2 (DSC: 0.35). Performance of the proposed model was maintained\nunder MRI field strength shift (DSC: 0.53) and scanner vendor variation (DSC:\n0.50), and improved in low tumor burden cases (DSC: 0.61). Lesion detection F1\nscores ranged from 0.62 to 0.78 across test sets. Preliminary inter-reader\nvariability analysis showed model-to-expert agreement (DSC: 0.62-0.68),\ncomparable to inter-expert agreement (DSC: 0.57-0.69).\n  Conclusions: The proposed MOIS-SAM2 enables efficient and scalable\ninteractive segmentation of NFs in WB-MRI with minimal user input and strong\ngeneralization, supporting integration into clinical workflows.", "AI": {"tldr": "本研究提出了一种名为MOIS-SAM2的新型多对象交互式分割模型，用于全身MRI中神经纤维瘤(NF)的检测和分割。该模型在精度、可扩展性和泛化能力方面优于现有方法，有望集成到临床工作流程中。", "motivation": "神经纤维瘤病1型(NF1)患者全身MRI中存在大量神经纤维瘤，现有交互式分割方法在处理数百个病灶时，难以兼顾高病灶级精度和可扩展性，因此需要一种新的解决方案。", "method": "研究引入了MOIS-SAM2模型，它在先进的、基于Transformer的、可提示的Segment Anything Model 2 (SAM2)基础上，通过基于范例的语义传播进行了扩展。该模型在来自84名NF1患者的119次全身MRI扫描（T2加权脂肪抑制序列）上进行训练和评估，数据集按患者级别划分为训练集和四个测试集（一个域内测试集和三个反映不同域偏移场景的测试集，如MRI场强、低肿瘤负荷、临床站点和扫描仪供应商差异）。", "result": "在域内测试集上，MOIS-SAM2的扫描级DSC（Dice相似系数）为0.60，优于基线3D nnU-Net（DSC: 0.54）和SAM2（DSC: 0.35）。该模型在MRI场强变化（DSC: 0.53）和扫描仪供应商变化（DSC: 0.50）下仍保持性能，并在低肿瘤负荷情况下有所提升（DSC: 0.61）。病灶检测F1分数在0.62到0.78之间。初步的读者间变异性分析显示，模型与专家之间的一致性（DSC: 0.62-0.68）与专家之间的一致性（DSC: 0.57-0.69）相当。", "conclusion": "所提出的MOIS-SAM2模型能够以最少的用户输入实现全身MRI中神经纤维瘤的高效、可扩展的交互式分割，并具有强大的泛化能力，支持集成到临床工作流程中。"}}
{"id": "2509.18723", "categories": ["eess.SY", "cs.RO", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.18723", "abs": "https://arxiv.org/abs/2509.18723", "authors": ["Jan-Hendrik Ewering", "Alessandro Papa", "Simon F. G. Ehlers", "Thomas Seel", "Michael Meindl"], "title": "Dual Iterative Learning Control for Multiple-Input Multiple-Output Dynamics with Validation in Robotic Systems", "comment": "11 pages, 4 figures", "summary": "Solving motion tasks autonomously and accurately is a core ability for\nintelligent real-world systems. To achieve genuine autonomy across multiple\nsystems and tasks, key challenges include coping with unknown dynamics and\novercoming the need for manual parameter tuning, which is especially crucial in\ncomplex Multiple-Input Multiple-Output (MIMO) systems.\n  This paper presents MIMO Dual Iterative Learning Control (DILC), a novel\ndata-driven iterative learning scheme for simultaneous tracking control and\nmodel learning, without requiring any prior system knowledge or manual\nparameter tuning. The method is designed for repetitive MIMO systems and\nintegrates seamlessly with established iterative learning control methods. We\nprovide monotonic convergence conditions for both reference tracking error and\nmodel error in linear time-invariant systems.\n  The DILC scheme -- rapidly and autonomously -- solves various motion tasks in\nhigh-fidelity simulations of an industrial robot and in multiple nonlinear\nreal-world MIMO systems, without requiring model knowledge or manually tuning\nthe algorithm. In our experiments, many reference tracking tasks are solved\nwithin 10-20 trials, and even complex motions are learned in less than 100\niterations. We believe that, because of its rapid and autonomous learning\ncapabilities, DILC has the potential to serve as an efficient building block\nwithin complex learning frameworks for intelligent real-world systems.", "AI": {"tldr": "本文提出了一种名为MIMO双迭代学习控制（DILC）的新型数据驱动迭代学习方案，用于重复性MIMO系统中的同步跟踪控制和模型学习，无需先验系统知识或手动参数调整，并在模拟和实际系统中展现出快速、自主的学习能力。", "motivation": "在智能真实世界系统中，自主准确地解决运动任务是核心能力。实现跨系统和任务的真正自主性，面临的关键挑战包括应对未知动态以及克服手动参数调优的需求，这在复杂的多输入多输出（MIMO）系统中尤为重要。", "method": "本文提出了MIMO双迭代学习控制（DILC），这是一种新颖的数据驱动迭代学习方案，用于同时进行跟踪控制和模型学习，无需任何先验系统知识或手动参数调优。该方法专为重复性MIMO系统设计，并能与现有迭代学习控制方法无缝集成。文中还提供了线性时不变系统中参考跟踪误差和模型误差的单调收敛条件。", "result": "DILC方案在工业机器人高保真模拟和多个非线性真实世界MIMO系统中，无需模型知识或手动调整算法，即可快速自主地解决各种运动任务。实验表明，许多参考跟踪任务在10-20次试验内即可解决，即使是复杂的运动也能在不到100次迭代中学会。", "conclusion": "鉴于其快速和自主的学习能力，DILC有潜力成为智能真实世界系统中复杂学习框架内的一个高效构建模块。"}}
{"id": "2509.18293", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.18293", "abs": "https://arxiv.org/abs/2509.18293", "authors": ["Jay Patel", "Hrudayangam Mehta", "Jeremy Blackburn"], "title": "Evaluating Large Language Models for Detecting Antisemitism", "comment": "Accepted to EMNLP 2025 Main Conference", "summary": "Detecting hateful content is a challenging and important problem. Automated\ntools, like machine-learning models, can help, but they require continuous\ntraining to adapt to the ever-changing landscape of social media. In this work,\nwe evaluate eight open-source LLMs' capability to detect antisemitic content,\nspecifically leveraging in-context definition as a policy guideline. We explore\nvarious prompting techniques and design a new CoT-like prompt, Guided-CoT.\nGuided-CoT handles the in-context policy well, increasing performance across\nall evaluated models, regardless of decoding configuration, model sizes, or\nreasoning capability. Notably, Llama 3.1 70B outperforms fine-tuned GPT-3.5.\nAdditionally, we examine LLM errors and introduce metrics to quantify semantic\ndivergence in model-generated rationales, revealing notable differences and\nparadoxical behaviors among LLMs. Our experiments highlight the differences\nobserved across LLMs' utility, explainability, and reliability.", "AI": {"tldr": "该研究评估了八个开源大型语言模型（LLM）检测反犹太内容的能力，引入了Guided-CoT提示技术，发现其能显著提升模型性能，并指出Llama 3.1 70B表现优于微调的GPT-3.5。同时，研究还分析了LLM的错误和语义分歧。", "motivation": "检测仇恨内容是一个具有挑战性且重要的问题，自动化工具（如机器学习模型）需要持续训练以适应不断变化的社交媒体环境，尤其是在检测反犹太内容方面。", "method": "研究评估了八个开源LLM检测反犹太内容的能力，将上下文定义作为政策指南。探索了多种提示技术，并设计了一种新的类CoT（Chain-of-Thought）提示——Guided-CoT。此外，还检查了LLM的错误，并引入了量化模型生成理由中语义分歧的指标。", "result": "Guided-CoT提示能够很好地处理上下文策略，提高所有评估模型的性能，无论解码配置、模型大小或推理能力如何。值得注意的是，Llama 3.1 70B的表现优于微调的GPT-3.5。对LLM错误的检查揭示了模型生成理由中显著的语义分歧和矛盾行为。", "conclusion": "实验结果突出了LLM在实用性、可解释性和可靠性方面存在的差异。Guided-CoT提示技术有效提升了LLM在仇恨内容检测任务上的表现，特别是Llama 3.1 70B展现出强大的潜力。"}}
{"id": "2509.18428", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18428", "abs": "https://arxiv.org/abs/2509.18428", "authors": ["Bahey Tharwat", "Yara Nasser", "Ali Abouzeid", "Ian Reid"], "title": "Latent Action Pretraining Through World Modeling", "comment": null, "summary": "Vision-Language-Action (VLA) models have gained popularity for learning\nrobotic manipulation tasks that follow language instructions. State-of-the-art\nVLAs, such as OpenVLA and $\\pi_{0}$, were trained on large-scale, manually\nlabeled action datasets collected through teleoperation. More recent\napproaches, including LAPA and villa-X, introduce latent action representations\nthat enable unsupervised pretraining on unlabeled datasets by modeling abstract\nvisual changes between frames. Although these methods have shown strong\nresults, their large model sizes make deployment in real-world settings\nchallenging. In this work, we propose LAWM, a model-agnostic framework to\npretrain imitation learning models in a self-supervised way, by learning latent\naction representations from unlabeled video data through world modeling. These\nvideos can be sourced from robot recordings or videos of humans performing\nactions with everyday objects. Our framework is designed to be effective for\ntransferring across tasks, environments, and embodiments. It outperforms models\ntrained with ground-truth robotics actions and similar pretraining methods on\nthe LIBERO benchmark and real-world setup, while being significantly more\nefficient and practical for real-world settings.", "AI": {"tldr": "解析错误", "motivation": "解析错误", "method": "解析错误", "result": "解析错误", "conclusion": "解析错误"}}
{"id": "2509.18186", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18186", "abs": "https://arxiv.org/abs/2509.18186", "authors": ["Nursultan Askarbekuly", "Timur Fayzrakhmanov", "Sladjan Babarogić", "Ivan Luković"], "title": "An Outcome-Based Educational Recommender System", "comment": null, "summary": "Most educational recommender systems are tuned and judged on click- or\nrating-based relevance, leaving their true pedagogical impact unclear. We\nintroduce OBER-an Outcome-Based Educational Recommender that embeds learning\noutcomes and assessment items directly into the data schema, so any algorithm\ncan be evaluated on the mastery it fosters. OBER uses a minimalist\nentity-relation model, a log-driven mastery formula, and a plug-in\narchitecture. Integrated into an e-learning system in non-formal domain, it was\nevaluated trough a two-week randomized split test with over 5 700 learners\nacross three methods: fixed expert trajectory, collaborative filtering (CF),\nand knowledge-based (KB) filtering. CF maximized retention, but the fixed path\nachieved the highest mastery. Because OBER derives business, relevance, and\nlearning metrics from the same logs, it lets practitioners weigh relevance and\nengagement against outcome mastery with no extra testing overhead. The\nframework is method-agnostic and readily extensible to future adaptive or\ncontext-aware recommenders.", "AI": {"tldr": "本文介绍了一种名为 OBER 的基于学习成果的教育推荐系统，它能直接评估算法对学习掌握程度的促进作用，并通过随机对照试验发现不同推荐方法在保留率和掌握度上的差异。", "motivation": "大多数教育推荐系统仅基于点击或评分进行调优和评估，其真正的教学效果（即对学习掌握程度的影响）尚不明确。", "method": "引入了 OBER 系统，它将学习成果和评估项直接嵌入数据模式，使得任何算法都能基于学习掌握度进行评估。OBER 采用极简实体关系模型、日志驱动的掌握度公式和插件式架构。在一个非正式领域的电子学习系统中，通过一项为期两周的随机分组测试进行了评估，涉及超过 5700 名学习者，比较了三种方法：固定专家路径、协同过滤（CF）和基于知识（KB）的过滤。", "result": "协同过滤（CF）最大化了学习者保留率，而固定专家路径实现了最高的学习掌握度。OBER 能够从相同的日志中得出业务、相关性和学习指标，使得实践者无需额外测试开销即可权衡相关性和参与度与学习成果的掌握度。", "conclusion": "OBER 提供了一个方法无关且易于扩展的框架，允许教育推荐系统直接评估其教学影响和学习成果掌握度，为未来的自适应或情境感知推荐器奠定基础。"}}
{"id": "2509.18179", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18179", "abs": "https://arxiv.org/abs/2509.18179", "authors": ["Sai Varun Kodathala", "Rakesh Vunnam"], "title": "The Describe-Then-Generate Bottleneck: How VLM Descriptions Alter Image Generation Outcomes", "comment": "13 pages, 7 Figures", "summary": "With the increasing integration of multimodal AI systems in creative\nworkflows, understanding information loss in vision-language-vision pipelines\nhas become important for evaluating system limitations. However, the\ndegradation that occurs when visual content passes through textual\nintermediation remains poorly quantified. In this work, we provide empirical\nanalysis of the describe-then-generate bottleneck, where natural language\nserves as an intermediate representation for visual information. We generated\n150 image pairs through the describe-then-generate pipeline and applied\nexisting metrics (LPIPS, SSIM, and color distance) to measure information\npreservation across perceptual, structural, and chromatic dimensions. Our\nevaluation reveals that 99.3% of samples exhibit substantial perceptual\ndegradation and 91.5% demonstrate significant structural information loss,\nproviding empirical evidence that the describe-then-generate bottleneck\nrepresents a measurable and consistent limitation in contemporary multimodal\nsystems.", "AI": {"tldr": "本文量化了多模态AI系统中“描述-生成”视觉-语言-视觉管道中的信息损失，发现通过文本中介后，视觉内容存在显著的感知和结构退化。", "motivation": "随着多模态AI系统在创意工作流中的集成度提高，理解视觉-语言-视觉管道中的信息损失对于评估系统局限性至关重要。然而，视觉内容通过文本中介时发生的退化尚未得到很好的量化。", "method": "研究通过“描述-生成”管道生成了150对图像，并应用现有指标（LPIPS、SSIM和颜色距离）来衡量感知、结构和色度维度上的信息保留情况。", "result": "评估显示，99.3%的样本表现出显著的感知退化，91.5%的样本表现出明显的结构信息损失。", "conclusion": "“描述-生成”瓶颈代表了当代多模态系统中可衡量且一致的局限性，提供了实证证据证明了这一瓶颈的存在。"}}
{"id": "2509.18182", "categories": ["cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2509.18182", "abs": "https://arxiv.org/abs/2509.18182", "authors": ["Isabelle Tingzon", "Yoji Toriumi", "Caroline Gevaert"], "title": "AI-Derived Structural Building Intelligence for Urban Resilience: An Application in Saint Vincent and the Grenadines", "comment": "Accepted at the 2nd Workshop on Computer Vision for Developing\n  Countries (CV4DC) at ICCV 2025", "summary": "Detailed structural building information is used to estimate potential damage\nfrom hazard events like cyclones, floods, and landslides, making them critical\nfor urban resilience planning and disaster risk reduction. However, such\ninformation is often unavailable in many small island developing states (SIDS)\nin climate-vulnerable regions like the Caribbean. To address this data gap, we\npresent an AI-driven workflow to automatically infer rooftop attributes from\nhigh-resolution satellite imagery, with Saint Vincent and the Grenadines as our\ncase study. Here, we compare the utility of geospatial foundation models\ncombined with shallow classifiers against fine-tuned deep learning models for\nrooftop classification. Furthermore, we assess the impact of incorporating\nadditional training data from neighboring SIDS to improve model performance.\nOur best models achieve F1 scores of 0.88 and 0.83 for roof pitch and roof\nmaterial classification, respectively. Combined with local capacity building,\nour work aims to provide SIDS with novel capabilities to harness AI and Earth\nObservation (EO) data to enable more efficient, evidence-based urban\ngovernance.", "AI": {"tldr": "该研究提出了一种AI驱动的工作流程，利用高分辨率卫星图像自动推断屋顶属性，以解决小岛屿发展中国家（SIDS）在城市韧性规划和灾害风险管理中缺乏建筑结构信息的挑战。", "motivation": "气候脆弱地区（如加勒比海）的许多小岛屿发展中国家（SIDS）缺乏详细的建筑结构信息，而这些信息对于估算灾害（如气旋、洪水、山体滑坡）造成的潜在损害至关重要，是城市韧性规划和灾害风险减轻的关键。", "method": "该研究提出了一种AI驱动的工作流程，利用高分辨率卫星图像自动推断屋顶属性。方法包括比较地理空间基础模型结合浅层分类器与微调深度学习模型在屋顶分类中的效用，并评估纳入邻近SIDS的额外训练数据对模型性能的影响。案例研究地点为圣文森特和格林纳丁斯。", "result": "最佳模型在屋顶坡度分类上达到了0.88的F1分数，在屋顶材料分类上达到了0.83的F1分数。", "conclusion": "该工作旨在结合当地能力建设，为SIDS提供利用AI和地球观测（EO）数据的新能力，以实现更高效、基于证据的城市治理，特别是在灾害风险管理方面。"}}
{"id": "2509.18749", "categories": ["eess.SY", "cs.RO", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.18749", "abs": "https://arxiv.org/abs/2509.18749", "authors": ["Maxwell M. Varley", "Timothy L. Molloy", "Girish N. Nair"], "title": "An Extended Kalman Filter for Systems with Infinite-Dimensional Measurements", "comment": "8 pages", "summary": "This article examines state estimation in discrete-time nonlinear stochastic\nsystems with finite-dimensional states and infinite-dimensional measurements,\nmotivated by real-world applications such as vision-based localization and\ntracking. We develop an extended Kalman filter (EKF) for real-time state\nestimation, with the measurement noise modeled as an infinite-dimensional\nrandom field. When applied to vision-based state estimation, the measurement\nJacobians required to implement the EKF are shown to correspond to image\ngradients. This result provides a novel system-theoretic justification for the\nuse of image gradients as features for vision-based state estimation,\ncontrasting with their (often heuristic) introduction in many computer-vision\npipelines. We demonstrate the practical utility of the EKF on a public\nreal-world dataset involving the localization of an aerial drone using video\nfrom a downward-facing monocular camera. The EKF is shown to outperform\nVINS-MONO, an established visual-inertial odometry algorithm, in some cases\nachieving mean squared error reductions of up to an order of magnitude.", "AI": {"tldr": "本文提出了一种针对具有无限维测量的离散时间非线性随机系统的扩展卡尔曼滤波器（EKF），并证明其测量雅可比矩阵对应于图像梯度，为基于视觉的状态估计中的图像梯度使用提供了理论依据，并在无人机定位任务中表现优于VINS-MONO。", "motivation": "研究动机源于现实世界中基于视觉的定位和跟踪应用，这些应用通常涉及有限维状态和无限维测量（如图像），需要有效的状态估计方法。", "method": "研究开发了一种扩展卡尔曼滤波器（EKF），用于实时状态估计。其中，测量噪声被建模为无限维随机场。在基于视觉的状态估计中，EKF所需的测量雅可比矩阵被证明对应于图像梯度。", "result": "结果表明，该EKF为在基于视觉的状态估计中使用图像梯度作为特征提供了新颖的系统理论依据。在涉及无人机定位的公共真实世界数据集中，该EKF在某些情况下表现优于已有的视觉惯性里程计算法VINS-MONO，均方误差降低可达一个数量级。", "conclusion": "研究结论是，所开发的EKF在理论上为图像梯度在基于视觉的状态估计中的使用提供了坚实基础，并且在实际应用中，尤其是在无人机定位等任务中，展现出显著的性能提升和实用价值。"}}
{"id": "2509.18314", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18314", "abs": "https://arxiv.org/abs/2509.18314", "authors": ["Hieu Tran", "Zonghai Yao", "Hong Yu"], "title": "Exploiting Tree Structure for Credit Assignment in RL Training of LLMs", "comment": "15 pages", "summary": "Reinforcement learning improves LLM reasoning, yet sparse delayed reward over\nlong sequences makes token-level credit assignment the key bottleneck. We study\nthe verifiable-reward setting, where the final answer is checkable and multiple\nresponses can be drawn per prompt. Reasoning tasks in math and medical QA align\nwith this setup, where only a few decision tokens significantly impact the\noutcome. PPO offers token-level advantages with a learned value model, but it\nis complex to train both the actor and critic models simultaneously, and it is\nnot easily generalizable, as the token-level values from the critic model can\nmake training prone to overfitting. GRPO is critic-free and supports verifiable\nrewards, but spreads a single sequence-level return across tokens and ignores\nbranching. We introduce \\textbf{Prefix-to-Tree (P2T)}, a simple procedure that\nconverts a group of responses into a prefix tree and computes\n\\emph{nonparametric} prefix values \\(V(s)\\) by aggregating descendant outcomes.\nBuilt on P2T, we propose \\textbf{TEMPO} (\\emph{\\textbf{T}ree-\\textbf{E}stimated\n\\textbf{M}ean Prefix Value for \\textbf{P}olicy \\textbf{O}ptimization}), a\ncritic-free algorithm that augments the group-relative outcome signal of GRPO\nwith \\emph{branch-gated} temporal-difference corrections derived from the tree.\nAt non-branch tokens, the temporal-difference (TD) term is zero, so TEMPO\nreduces to GRPO; at branching tokens, it supplies precise token-level credit\nwithout a learned value network or extra judges/teachers. On Qwen3-1.7B/4B,\nTEMPO outperforms PPO and GRPO on in-distribution (MATH, MedQA) and\nout-of-distribution (GSM-HARD, AMC23, MedMCQA, MMLU-Medical) benchmarks, and\nreaches higher validation accuracy with roughly the same wall-clock time.", "AI": {"tldr": "解析错误", "motivation": "解析错误", "method": "解析错误", "result": "解析错误", "conclusion": "解析错误"}}
{"id": "2509.18447", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18447", "abs": "https://arxiv.org/abs/2509.18447", "authors": ["Rishabh Madan", "Jiawei Lin", "Mahika Goel", "Angchen Xie", "Xiaoyu Liang", "Marcus Lee", "Justin Guo", "Pranav N. Thakkar", "Rohan Banerjee", "Jose Barreiros", "Kate Tsui", "Tom Silver", "Tapomayukh Bhattacharjee"], "title": "PrioriTouch: Adapting to User Contact Preferences for Whole-Arm Physical Human-Robot Interaction", "comment": "Conference on Robot Learning (CoRL)", "summary": "Physical human-robot interaction (pHRI) requires robots to adapt to\nindividual contact preferences, such as where and how much force is applied.\nIdentifying preferences is difficult for a single contact; with whole-arm\ninteraction involving multiple simultaneous contacts between the robot and\nhuman, the challenge is greater because different body parts can impose\nincompatible force requirements. In caregiving tasks, where contact is frequent\nand varied, such conflicts are unavoidable. With multiple preferences across\nmultiple contacts, no single solution can satisfy all objectives--trade-offs\nare inherent, making prioritization essential. We present PrioriTouch, a\nframework for ranking and executing control objectives across multiple\ncontacts. PrioriTouch can prioritize from a general collection of controllers,\nmaking it applicable not only to caregiving scenarios such as bed bathing and\ndressing but also to broader multi-contact settings. Our method combines a\nnovel learning-to-rank approach with hierarchical operational space control,\nleveraging simulation-in-the-loop rollouts for data-efficient and safe\nexploration. We conduct a user study on physical assistance preferences, derive\npersonalized comfort thresholds, and incorporate them into PrioriTouch. We\nevaluate PrioriTouch through extensive simulation and real-world experiments,\ndemonstrating its ability to adapt to user contact preferences, maintain task\nperformance, and enhance safety and comfort. Website:\nhttps://emprise.cs.cornell.edu/prioritouch.", "AI": {"tldr": "本文提出了PrioriTouch，一个用于多接触物理人机交互(pHRI)的框架，它能对多个接触点上的控制目标进行排序和执行，以适应用户的个性化接触偏好，尤其适用于护理任务。", "motivation": "在物理人机交互中，特别是涉及机器人全身与人体的多点同时接触时，识别和满足个体接触偏好非常困难，因为不同身体部位可能产生不兼容的力学要求。在护理等频繁且多样化的接触任务中，这种冲突不可避免，需要进行权衡和优先级排序。", "method": "PrioriTouch框架结合了一种新颖的“学习排序”方法与分层操作空间控制。它利用“仿真循环”的推出进行数据高效和安全的探索。此外，通过用户研究获取个性化的舒适阈值并将其整合到框架中。", "result": "PrioriTouch在广泛的仿真和真实世界实验中，展示了其适应用户接触偏好、保持任务性能以及提高安全性和舒适度的能力。", "conclusion": "PrioriTouch框架有效解决了多接触pHRI中的挑战，通过优先级排序控制目标来适应个性化偏好，从而提高了机器人在护理等场景中的适应性、性能、安全性和舒适度。"}}
{"id": "2509.18198", "categories": ["cs.AI", "cs.MA", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18198", "abs": "https://arxiv.org/abs/2509.18198", "authors": ["Rui Liu", "Zikang Wang", "Peng Gao", "Yu Shen", "Pratap Tokekar", "Ming Lin"], "title": "MMCD: Multi-Modal Collaborative Decision-Making for Connected Autonomy with Knowledge Distillation", "comment": null, "summary": "Autonomous systems have advanced significantly, but challenges persist in\naccident-prone environments where robust decision-making is crucial. A single\nvehicle's limited sensor range and obstructed views increase the likelihood of\naccidents. Multi-vehicle connected systems and multi-modal approaches,\nleveraging RGB images and LiDAR point clouds, have emerged as promising\nsolutions. However, existing methods often assume the availability of all data\nmodalities and connected vehicles during both training and testing, which is\nimpractical due to potential sensor failures or missing connected vehicles. To\naddress these challenges, we introduce a novel framework MMCD (Multi-Modal\nCollaborative Decision-making) for connected autonomy. Our framework fuses\nmulti-modal observations from ego and collaborative vehicles to enhance\ndecision-making under challenging conditions. To ensure robust performance when\ncertain data modalities are unavailable during testing, we propose an approach\nbased on cross-modal knowledge distillation with a teacher-student model\nstructure. The teacher model is trained with multiple data modalities, while\nthe student model is designed to operate effectively with reduced modalities.\nIn experiments on $\\textit{connected autonomous driving with ground vehicles}$\nand $\\textit{aerial-ground vehicles collaboration}$, our method improves\ndriving safety by up to ${\\it 20.7}\\%$, surpassing the best-existing baseline\nin detecting potential accidents and making safe driving decisions. More\ninformation can be found on our website https://ruiiu.github.io/mmcd.", "AI": {"tldr": "本文提出MMCD框架，用于在互联自动驾驶中实现多模态协同决策，通过跨模态知识蒸馏确保在部分数据模态缺失时仍能稳健运行，显著提升驾驶安全性。", "motivation": "自动驾驶系统在事故多发环境下决策鲁棒性面临挑战，单车传感器范围有限且视线受阻。现有多车辆互联和多模态方法通常假设训练和测试时所有数据模态和互联车辆均可用，这在实际中因传感器故障或车辆缺失而不可行。", "method": "本文引入MMCD（多模态协同决策）框架，融合自我车辆和协作车辆的多模态（RGB图像和LiDAR点云）观测以增强决策。为确保在测试时某些数据模态不可用时的鲁棒性，提出一种基于跨模态知识蒸馏的方法，采用教师-学生模型结构：教师模型使用多种数据模态训练，学生模型则设计为在模态减少的情况下有效运行。", "result": "在“互联自动驾驶（地面车辆）”和“空地车辆协作”实验中，MMCD方法将驾驶安全性提高了高达20.7%，在检测潜在事故和做出安全驾驶决策方面超越了现有最佳基线。", "conclusion": "MMCD框架为互联自动驾驶提供了一个新颖且鲁棒的解决方案，通过有效的多模态融合和知识蒸馏，即使在部分数据模态不可用的情况下也能显著提升驾驶安全性。"}}
{"id": "2509.18183", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18183", "abs": "https://arxiv.org/abs/2509.18183", "authors": ["Jinyue Bian", "Zhaoxing Zhang", "Zhengyu Liang", "Shiwei Zheng", "Shengtao Zhang", "Rong Shen", "Chen Yang", "Anzhou Hou"], "title": "VLA-LPAF: Lightweight Perspective-Adaptive Fusion for Vision-Language-Action to Enable More Unconstrained Robotic Manipulation", "comment": null, "summary": "The Visual-Language-Action (VLA) models can follow text instructions\naccording to visual observations of the surrounding environment. This ability\nto map multimodal inputs to actions is derived from the training of the VLA\nmodel on extensive standard demonstrations. These visual observations captured\nby third-personal global and in-wrist local cameras are inevitably varied in\nnumber and perspective across different environments, resulting in significant\ndifferences in the visual features. This perspective heterogeneity constrains\nthe generality of VLA models. In light of this, we first propose the\nlightweight module VLA-LPAF to foster the perspective adaptivity of VLA models\nusing only 2D data. VLA-LPAF is finetuned using images from a single view and\nfuses other multiview observations in the latent space, which effectively and\nefficiently bridge the gap caused by perspective inconsistency. We instantiate\nour VLA-LPAF framework with the VLA model RoboFlamingo to construct\nRoboFlamingo-LPAF. Experiments show that RoboFlamingo-LPAF averagely achieves\naround 8% task success rate improvement on CALVIN, 15% on LIBERO, and 30% on a\ncustomized simulation benchmark. We also demonstrate the developed viewadaptive\ncharacteristics of the proposed RoboFlamingo-LPAF through real-world tasks.", "AI": {"tldr": "VLA模型在多视角视觉观察下泛化性受限。本文提出轻量级模块VLA-LPAF，通过2D数据和潜在空间融合，有效提升VLA模型的视角适应性，并在多个基准测试中显著提高了任务成功率。", "motivation": "视觉-语言-动作（VLA）模型通过大量演示数据训练，但其视觉观察（来自第三人称全局和腕部局部摄像头）在不同环境中数量和视角各异，导致视觉特征差异显著，从而限制了VLA模型的泛化能力。", "method": "提出VLA-LPAF，一个轻量级模块，仅使用2D数据提升VLA模型的视角适应性。该模块通过单一视角图像进行微调，并在潜在空间中融合其他多视角观察，有效弥合了视角不一致造成的差距。作者将VLA-LPAF与RoboFlamingo模型结合，构建了RoboFlamingo-LPAF。", "result": "实验表明，RoboFlamingo-LPAF在CALVIN上平均提升了约8%的任务成功率，在LIBERO上提升了15%，在一个定制模拟基准上提升了30%。此外，还在真实世界任务中展示了其开发的视角适应性特征。", "conclusion": "VLA-LPAF模块能够有效且高效地弥合因视角不一致造成的差距，显著提高了VLA模型（如RoboFlamingo）在不同环境和摄像头设置下的泛化性和性能。"}}
{"id": "2509.18354", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV", "62H35, 68T07, 62M40, 68T45", "I.2.6; I.2.10; I.4.6; I.4.8; I.5.1; I.5.4"], "pdf": "https://arxiv.org/pdf/2509.18354", "abs": "https://arxiv.org/abs/2509.18354", "authors": ["Mehrdad Moradi", "Shengzhe Chen", "Hao Yan", "Kamran Paynabar"], "title": "A Single Image Is All You Need: Zero-Shot Anomaly Localization Without Training Data", "comment": "12 pages, 10 figures, 1 table. Preprint submitted to a CVF conference", "summary": "Anomaly detection in images is typically addressed by learning from\ncollections of training data or relying on reference samples. In many\nreal-world scenarios, however, such training data may be unavailable, and only\nthe test image itself is provided. We address this zero-shot setting by\nproposing a single-image anomaly localization method that leverages the\ninductive bias of convolutional neural networks, inspired by Deep Image Prior\n(DIP). Our method is named Single Shot Decomposition Network (SSDnet). Our key\nassumption is that natural images often exhibit unified textures and patterns,\nand that anomalies manifest as localized deviations from these repetitive or\nstochastic patterns. To learn the deep image prior, we design a patch-based\ntraining framework where the input image is fed directly into the network for\nself-reconstruction, rather than mapping random noise to the image as done in\nDIP. To avoid the model simply learning an identity mapping, we apply masking,\npatch shuffling, and small Gaussian noise. In addition, we use a perceptual\nloss based on inner-product similarity to capture structure beyond pixel\nfidelity. Our approach needs no external training data, labels, or references,\nand remains robust in the presence of noise or missing pixels. SSDnet achieves\n0.99 AUROC and 0.60 AUPRC on MVTec-AD and 0.98 AUROC and 0.67 AUPRC on the\nfabric dataset, outperforming state-of-the-art methods. The implementation code\nwill be released at https://github.com/mehrdadmoradi124/SSDnet", "AI": {"tldr": "本文提出了一种名为SSDnet的单图像异常定位方法，该方法在零样本设置下，无需外部训练数据或参考，通过利用卷积神经网络的归纳偏置和图像自身的结构特性，实现了对图像中局部异常的有效检测。", "motivation": "传统的图像异常检测方法通常需要大量的训练数据或参考样本。然而，在许多实际场景中，这些数据可能无法获得，只能提供待检测的图像本身。为了解决这种零样本（zero-shot）设置下的挑战，本文提出了新的方法。", "method": "SSDnet方法受Deep Image Prior (DIP)启发，利用卷积神经网络的归纳偏置。核心假设是自然图像通常表现出统一的纹理和模式，而异常表现为这些重复或随机模式的局部偏差。该方法设计了一个基于补丁的训练框架，直接将输入图像送入网络进行自重建，而非像DIP那样将随机噪声映射到图像。为避免模型学习恒等映射，引入了掩码、补丁混洗和高斯噪声。此外，使用基于内积相似度的感知损失来捕获超越像素保真度的结构信息。该方法不需要外部训练数据、标签或参考，并对噪声或缺失像素具有鲁棒性。", "result": "SSDnet在MVTec-AD数据集上取得了0.99的AUROC和0.60的AUPRC，在fabric数据集上取得了0.98的AUROC和0.67的AUPRC，性能优于现有最先进的方法。", "conclusion": "SSDnet是一种无需外部训练数据、标签或参考的单图像异常定位方法，它利用图像自身的结构特性和CNN的归纳偏置，通过自重建和特定的正则化技术，在零样本设置下表现出卓越的性能，并对噪声和缺失像素具有鲁棒性。"}}
{"id": "2509.18769", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.18769", "abs": "https://arxiv.org/abs/2509.18769", "authors": ["Hadi Nemati", "Pedro Sánchez-Martín", "Álvaro Ortega", "Lukas Sigrist", "Luis Rouco"], "title": "Integration of Concentrated Solar Power Plants in Renewable-Only VPP with Electrical and Thermal Demands: A Two-Stage Robust Bidding Approach", "comment": null, "summary": "This paper proposes the integration of Concentrated Solar Power Plant (CSP)\nin the Renewable-only virtual power plant (RVPP) for bidding in the electricity\nday-ahead and secondary reserve markets, as well as trading thermal energy\nthrough a heat purchase agreement. A reformulated two-stage robust optimization\napproach is introduced to account for multiple uncertainties, including\nelectricity prices, non-dispatchable renewable energy sources electrical\nproduction, CSP thermal production, and uncertainties in electrical and thermal\ndemand consumption. The provision of energy and reserve by the thermal storage\nof CSP is modeled using an adjustable approach, which allocates a share of\nenergy for up and down reserves based on the profitability of the RVPP.\nSimulations are conducted for several case studies to demonstrate the\neffectiveness and computational efficiency of the proposed approach under\ndifferent RVPP operator decisions against uncertain parameters and various\ntrading strategies for electricity and thermal energy. The simulation results\nshow that integrating CSP into RVPP enhances RVPP flexibility for both\nelectrical and thermal trading. Furthermore, the results indicate that the\nprofitability of the RVPP increases when all trading options are considered,\nacross different levels of conservatism adopted by the RVPP operator in\nresponse to uncertain parameters.", "AI": {"tldr": "本文提出将聚光太阳能热发电厂（CSP）整合到纯可再生能源虚拟电厂（RVPP）中，通过两阶段鲁棒优化方法应对多重不确定性，以参与日前和二次备用电力市场以及热能交易，结果显示这能增强RVPP的灵活性并提高盈利能力。", "motivation": "研究旨在应对纯可再生能源虚拟电厂在电力和热能市场中面临的多种不确定性（如电价、可再生能源产量、需求等），并探索整合聚光太阳能热发电厂（CSP）如何增强RVPP的灵活性和盈利能力。", "method": "采用重新表述的两阶段鲁棒优化方法，以处理多种不确定性。CSP的热储能通过可调节方法建模，根据RVPP的盈利能力分配能量用于向上和向下备用。通过多个案例研究进行模拟，以验证方法在不同操作决策和交易策略下的有效性和计算效率。", "result": "模拟结果表明，将CSP整合到RVPP中显著增强了RVPP在电力和热能交易方面的灵活性。此外，当考虑所有交易选项时，RVPP的盈利能力有所提高，且这种提升在RVPP运营商针对不确定参数采取不同保守程度时依然成立。", "conclusion": "将聚光太阳能热发电厂（CSP）整合到纯可再生能源虚拟电厂（RVPP）中，通过参与电力和热能市场，能够有效增强RVPP的灵活性并提高其盈利能力，尤其是在面对多重不确定性时。"}}
{"id": "2509.18316", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18316", "abs": "https://arxiv.org/abs/2509.18316", "authors": ["Saksham Khatwani", "He Cheng", "Majid Afshar", "Dmitriy Dligach", "Yanjun Gao"], "title": "Brittleness and Promise: Knowledge Graph Based Reward Modeling for Diagnostic Reasoning", "comment": null, "summary": "Large language models (LLMs) show promise for diagnostic reasoning but often\nlack reliable, knowledge grounded inference. Knowledge graphs (KGs), such as\nthe Unified Medical Language System (UMLS), offer structured biomedical\nknowledge that can support trustworthy reasoning. Prior approaches typically\nintegrate KGs via retrieval augmented generation or fine tuning, inserting KG\ncontent into prompts rather than enabling structured reasoning. We explore an\nalternative paradigm: treating the LLM as a reward model of KG reasoning paths,\nwhere the model learns to judge whether a candidate path leads to correct\ndiagnosis for a given patient input. This approach is inspired by recent work\nthat leverages reward training to enhance model reasoning abilities, and\ngrounded in computational theory, which suggests that verifying a solution is\noften easier than generating one from scratch. It also parallels physicians'\ndiagnostic assessment, where they judge which sequences of findings and\nintermediate conditions most plausibly support a diagnosis. We first\nsystematically evaluate five task formulation for knowledge path judging and\neight training paradigm. Second, we test whether the path judging abilities\ngeneralize to downstream diagnostic tasks, including diagnosis summarization\nand medical question answering. Experiments with three open source\ninstruct-tuned LLMs reveal both promise and brittleness: while specific reward\noptimization and distillation lead to strong path-judging performance, the\ntransferability to downstream tasks remain weak. Our finding provides the first\nsystematic assessment of \"reward model style\" reasoning over clinical KGs,\noffering insights into how structured, reward-based supervision influences\ndiagnostic reasoning in GenAI systems for healthcare.", "AI": {"tldr": "该研究探索了一种新的范式，将大型语言模型（LLM）作为知识图谱（KG）推理路径的奖励模型，以提高诊断推理的可靠性。尽管在路径判断上表现出色，但其到下游诊断任务的泛化能力较弱。", "motivation": "大型语言模型在诊断推理方面有潜力，但缺乏可靠的、基于知识的推理能力。知识图谱（如UMLS）提供了结构化的生物医学知识，可以支持可信赖的推理。现有的KG整合方法（如检索增强生成或微调）主要将KG内容插入提示中，而非实现结构化推理。本文受奖励训练和计算理论（验证解决方案通常比从头生成更容易）的启发，提出了一种将LLM作为KG推理路径奖励模型的替代范式。", "method": "该方法将LLM视为KG推理路径的奖励模型，让模型判断候选路径是否能导致正确的诊断。具体包括：1. 系统评估了五种知识路径判断任务的表述方式。2. 系统评估了八种训练范式。3. 测试了路径判断能力是否能泛化到下游诊断任务，包括诊断摘要和医学问答。实验使用了三个开源的指令微调LLM。", "result": "特定的奖励优化和蒸馏方法能够带来强大的路径判断性能。然而，其向诊断摘要和医学问答等下游任务的泛化能力仍然较弱。", "conclusion": "该研究首次系统评估了在临床知识图谱上采用“奖励模型式”推理的方法，为结构化、基于奖励的监督如何影响医疗领域生成式AI系统的诊断推理提供了见解。"}}
{"id": "2509.18455", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18455", "abs": "https://arxiv.org/abs/2509.18455", "authors": ["Yunshuang Li", "Yiyang Ling", "Gaurav S. Sukhatme", "Daniel Seita"], "title": "Learning Geometry-Aware Nonprehensile Pushing and Pulling with Dexterous Hands", "comment": null, "summary": "Nonprehensile manipulation, such as pushing and pulling, enables robots to\nmove, align, or reposition objects that may be difficult to grasp due to their\ngeometry, size, or relationship to the robot or the environment. Much of the\nexisting work in nonprehensile manipulation relies on parallel-jaw grippers or\ntools such as rods and spatulas. In contrast, multi-fingered dexterous hands\noffer richer contact modes and versatility for handling diverse objects to\nprovide stable support over the objects, which compensates for the difficulty\nof modeling the dynamics of nonprehensile manipulation. Therefore, we propose\nGeometry-aware Dexterous Pushing and Pulling (GD2P) for nonprehensile\nmanipulation with dexterous robotic hands. We study pushing and pulling by\nframing the problem as synthesizing and learning pre-contact dexterous hand\nposes that lead to effective manipulation. We generate diverse hand poses via\ncontact-guided sampling, filter them using physics simulation, and train a\ndiffusion model conditioned on object geometry to predict viable poses. At test\ntime, we sample hand poses and use standard motion planners to select and\nexecute pushing and pulling actions. We perform 840 real-world experiments with\nan Allegro Hand, comparing our method to baselines. The results indicate that\nGD2P offers a scalable route for training dexterous nonprehensile manipulation\npolicies. We further demonstrate GD2P on a LEAP Hand, highlighting its\napplicability to different hand morphologies. Our pre-trained models and\ndataset, including 1.3 million hand poses across 2.3k objects, will be\nopen-source to facilitate further research. Our project website is available\nat: geodex2p.github.io.", "AI": {"tldr": "本文提出了Geometry-aware Dexterous Pushing and Pulling (GD2P) 方法，利用灵巧机械手进行非抓取式推拉操作。通过学习基于物体几何形状的预接触手部姿态，实现对多样物体的有效操作，并在真实世界实验中验证了其可扩展性和通用性。", "motivation": "非抓取式操作（如推拉）能处理难以抓取的物体。现有方法多依赖于平行夹爪或简单工具。灵巧多指机械手提供更丰富的接触模式和多功能性，能为物体提供稳定支撑，弥补非抓取操作动力学建模的难度，因此研究如何利用灵巧手进行非抓取操作具有重要意义。", "method": "该方法将推拉问题框架为合成和学习导致有效操作的预接触灵巧手姿态。具体步骤包括：通过接触引导采样生成多样手部姿态；使用物理模拟过滤这些姿态；训练一个以物体几何形状为条件的扩散模型来预测可行的姿态。在测试时，采样手部姿态并使用标准运动规划器选择和执行推拉动作。", "result": "通过对Allegro手进行了840次真实世界实验，结果表明GD2P为训练灵巧非抓取操作策略提供了一条可扩展的途径。该方法还在LEAP手型上进行了演示，突出了其对不同手部形态的适用性。作者将开源预训练模型和包含2.3k物体、130万手部姿态的数据集。", "conclusion": "GD2P提供了一种可扩展且通用的方法，用于通过灵巧机械手进行几何感知的非抓取式推拉操作。该方法通过学习有效的预接触手部姿态，克服了传统方法的局限性，并在多种手型和真实世界场景中展现出优越的性能和广阔的应用前景。"}}
{"id": "2509.18215", "categories": ["cs.AI", "cs.LO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.18215", "abs": "https://arxiv.org/abs/2509.18215", "authors": ["Timotheus Kampik", "Kristijonas Čyras", "José Ruiz Alarcón"], "title": "Change in Quantitative Bipolar Argumentation: Sufficient, Necessary, and Counterfactual Explanations", "comment": "The publisher's version contains a notation glitch in Example 3, 5th\n  line, first sub-script G should be G'. This has always been G' in authors'\n  version. Thanks to J. Lanser for pointing this out", "summary": "This paper presents a formal approach to explaining change of inference in\nQuantitative Bipolar Argumentation Frameworks (QBAFs). When drawing conclusions\nfrom a QBAF and updating the QBAF to then again draw conclusions (and so on),\nour approach traces changes -- which we call strength inconsistencies -- in the\npartial order over argument strengths that a semantics establishes on some\narguments of interest, called topic arguments. We trace the causes of strength\ninconsistencies to specific arguments, which then serve as explanations. We\nidentify sufficient, necessary, and counterfactual explanations for strength\ninconsistencies and show that strength inconsistency explanations exist if and\nonly if an update leads to strength inconsistency. We define a heuristic-based\napproach to facilitate the search for strength inconsistency explanations, for\nwhich we also provide an implementation.", "AI": {"tldr": "本文提出了一种形式化方法，用于解释量化双极论证框架（QBAFs）中推理变化的原因，通过追溯论证强度偏序的变化（强度不一致性）并将其归因于特定论证。", "motivation": "当QBAF被更新时，其推导出的结论可能会发生变化。理解这些推理变化（特别是论证强度偏序的变化）的原因对于解释QBAF的行为至关重要。", "method": "该研究通过定义“强度不一致性”来追踪QBAF更新后，特定“主题论证”的强度偏序的变化。它将这些不一致性的原因追溯到具体的论证，并识别出充分、必要和反事实的解释。此外，还提出了一种基于启发式的方法来寻找这些解释，并提供了实现。", "result": "研究表明，强度不一致性解释存在，当且仅当更新导致了强度不一致性。同时，提供了一种基于启发式的方法来促进强度不一致性解释的搜索，并提供了相应的实现。", "conclusion": "该论文为解释QBAFs中推理变化提供了一种形式化方法，通过识别和解释论证强度偏序的“强度不一致性”来追溯其原因，并提供了一种实用的启发式搜索方案。"}}
{"id": "2509.18184", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18184", "abs": "https://arxiv.org/abs/2509.18184", "authors": ["Yifeng Cheng", "Alois Knoll", "Hu Cao"], "title": "URNet: Uncertainty-aware Refinement Network for Event-based Stereo Depth Estimation", "comment": "This work is accepted by Visual Intelligence Journal", "summary": "Event cameras provide high temporal resolution, high dynamic range, and low\nlatency, offering significant advantages over conventional frame-based cameras.\nIn this work, we introduce an uncertainty-aware refinement network called URNet\nfor event-based stereo depth estimation. Our approach features a local-global\nrefinement module that effectively captures fine-grained local details and\nlong-range global context. Additionally, we introduce a Kullback-Leibler (KL)\ndivergence-based uncertainty modeling method to enhance prediction reliability.\nExtensive experiments on the DSEC dataset demonstrate that URNet consistently\noutperforms state-of-the-art (SOTA) methods in both qualitative and\nquantitative evaluations.", "AI": {"tldr": "本文提出URNet，一个不确定性感知细化网络，用于基于事件相机的立体深度估计，通过局部-全局细化模块和KL散度不确定性建模，显著优于现有SOTA方法。", "motivation": "事件相机具有高时间分辨率、高动态范围和低延迟的优势，但其在立体深度估计方面的潜力尚未完全开发。提升事件相机在深度估计中的可靠性和准确性是研究的动力。", "method": "本文引入了URNet（不确定性感知细化网络），其核心方法包括：1) 一个局部-全局细化模块，用于捕捉精细局部细节和长距离全局上下文；2) 一个基于Kullback-Leibler（KL）散度的不确定性建模方法，以提高预测的可靠性。", "result": "在DSEC数据集上进行的广泛实验表明，URNet在定性和定量评估中均持续优于现有的最先进（SOTA）方法。", "conclusion": "URNet为事件相机的立体深度估计提供了一种先进且可靠的解决方案，通过其创新的细化模块和不确定性建模，显著提升了深度预测的性能。"}}
{"id": "2509.18566", "categories": ["cs.CV", "cs.RO", "eess.IV"], "pdf": "https://arxiv.org/pdf/2509.18566", "abs": "https://arxiv.org/abs/2509.18566", "authors": ["Xiaoting Yin", "Hao Shi", "Kailun Yang", "Jiajun Zhai", "Shangwei Guo", "Lin Wang", "Kaiwei Wang"], "title": "Event-guided 3D Gaussian Splatting for Dynamic Human and Scene Reconstruction", "comment": null, "summary": "Reconstructing dynamic humans together with static scenes from monocular\nvideos remains difficult, especially under fast motion, where RGB frames suffer\nfrom motion blur. Event cameras exhibit distinct advantages, e.g., microsecond\ntemporal resolution, making them a superior sensing choice for dynamic human\nreconstruction. Accordingly, we present a novel event-guided human-scene\nreconstruction framework that jointly models human and scene from a single\nmonocular event camera via 3D Gaussian Splatting. Specifically, a unified set\nof 3D Gaussians carries a learnable semantic attribute; only Gaussians\nclassified as human undergo deformation for animation, while scene Gaussians\nstay static. To combat blur, we propose an event-guided loss that matches\nsimulated brightness changes between consecutive renderings with the event\nstream, improving local fidelity in fast-moving regions. Our approach removes\nthe need for external human masks and simplifies managing separate Gaussian\nsets. On two benchmark datasets, ZJU-MoCap-Blur and MMHPSD-Blur, it delivers\nstate-of-the-art human-scene reconstruction, with notable gains over strong\nbaselines in PSNR/SSIM and reduced LPIPS, especially for high-speed subjects.", "AI": {"tldr": "本文提出一个事件引导的人体-场景重建框架，利用单目事件相机和3D高斯溅射技术，共同建模动态人体和静态场景，并通过事件引导损失解决运动模糊问题，实现高质量重建。", "motivation": "从单目视频中重建动态人体和静态场景极具挑战，尤其是在快速运动导致RGB帧出现运动模糊时。事件相机具备微秒级时间分辨率，是处理动态人体重建的理想选择。", "method": "该方法提出一个新颖的事件引导人体-场景重建框架，通过3D高斯溅射技术，从单个单目事件相机共同建模人体和场景。统一的3D高斯集合携带可学习的语义属性：被分类为人体的部分进行变形以实现动画，而场景部分保持静态。为对抗模糊，提出一种事件引导损失，将连续渲染帧之间的模拟亮度变化与事件流匹配，以提高快速移动区域的局部保真度。该方法无需外部人体掩码，并简化了不同高斯集合的管理。", "result": "在ZJU-MoCap-Blur和MMHPSD-Blur两个基准数据集上，该方法实现了最先进的人体-场景重建性能，在PSNR/SSIM方面显著优于强基线，并降低了LPIPS，尤其对于高速运动目标表现更佳。", "conclusion": "该研究成功地利用事件相机和3D高斯溅射技术，通过事件引导损失，解决了从单目视频中重建动态人体和静态场景时遇到的运动模糊问题，尤其在快速运动场景下展现出卓越的重建能力和保真度。"}}
{"id": "2509.18935", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.18935", "abs": "https://arxiv.org/abs/2509.18935", "authors": ["Yiqiao Xu", "Quan Wan", "Alessandra Parisio"], "title": "Frequency-Varying Optimization: A Control Framework for New Dynamic Frequency Response Services", "comment": null, "summary": "To address the variability of renewable generation, initiatives have been\nlaunched globally to provide faster and more effective frequency responses. In\nthe UK, the National Energy System Operator (NESO) has introduced a suite of\nthree new dynamic services, where aggregation of assets is expected to play a\nkey role. For an Aggregated Response Unit (ARU), the required level of\nfrequency response varies with grid frequency, resulting in a frequency-varying\nequality constraint that assets should meet collectively. We show that the\noptimal coordination of an ARU constitutes a Frequency-Varying Optimization\n(FVO) problem, in which the optimal trajectory for each asset evolves\ndynamically. To facilitate online optimization, we reformulate the FVO problem\ninto Tracking of the Optimal Trajectory (TOT) problems, with algorithms\nproposed for two scenarios: one where the asset dynamics are negligible, and\nanother where they must be accounted for. Under reasonable conditions, the ARU\nconverges to the optimal trajectory within a fixed time, and within the maximum\ndelivery time requested by NESO. The proposed framework can be readily\ndistributed to coordinate a large number of assets. Numerical results verify\nthe effectiveness and scalability of the proposed control framework.", "AI": {"tldr": "为应对可再生能源发电的波动性，本文提出了一种针对聚合响应单元（ARU）的分布式控制框架，通过将频率变动优化（FVO）问题重构为最优轨迹跟踪（TOT）问题，实现在线优化协调大量资产，以满足电网频率响应需求。", "motivation": "可再生能源发电的波动性促使全球范围内推出更快、更有效的频率响应措施。在英国，国家能源系统运营商（NESO）引入了新的动态服务，其中资产聚合（ARU）扮演关键角色。ARU面临的挑战是，其所需的频率响应水平随电网频率变化，导致资产需要集体满足一个频率变化的等式约束。", "method": "研究将ARU的最佳协调问题构建为频率变动优化（FVO）问题，其中每个资产的最佳轨迹动态演变。为便于在线优化，FVO问题被重新表述为最优轨迹跟踪（TOT）问题，并针对两种场景提出了算法：一是资产动态可忽略不计，二是资产动态必须考虑在内。该框架被设计为易于分布式部署。", "result": "在合理条件下，ARU能在固定时间内收敛到最优轨迹，并满足NESO要求的最大响应交付时间。数值结果验证了所提出的控制框架的有效性和可扩展性。", "conclusion": "所提出的分布式控制框架能够有效且可扩展地协调大量聚合资产，以提供快速的频率响应，满足电网对可再生能源并网的需求，并在规定时间内达到最优性能。"}}
{"id": "2509.18344", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18344", "abs": "https://arxiv.org/abs/2509.18344", "authors": ["Pei-Shuo Wang", "Jian-Jia Chen", "Chun-Che Yang", "Chi-Chih Chang", "Ning-Chi Huang", "Mohamed S. Abdelfattah", "Kai-Chiang Wu"], "title": "Speculate Deep and Accurate: Lossless and Training-Free Acceleration for Offloaded LLMs via Substitute Speculative Decoding", "comment": "Accepted by NeurIPS 2025", "summary": "The immense model sizes of large language models (LLMs) challenge deployment\non memory-limited consumer GPUs. Although model compression and parameter\noffloading are common strategies to address memory limitations, compression can\ndegrade quality, and offloading maintains quality but suffers from slow\ninference. Speculative decoding presents a promising avenue to accelerate\nparameter offloading, utilizing a fast draft model to propose multiple draft\ntokens, which are then verified by the target LLM in parallel with a single\nforward pass. This method reduces the time-consuming data transfers in forward\npasses that involve offloaded weight transfers. Existing methods often rely on\npretrained weights of the same family, but require additional training to align\nwith custom-trained models. Moreover, approaches that involve draft model\ntraining usually yield only modest speedups. This limitation arises from\ninsufficient alignment with the target model, preventing higher token\nacceptance lengths. To address these challenges and achieve greater speedups,\nwe propose SubSpec, a plug-and-play method to accelerate parameter offloading\nthat is lossless and training-free. SubSpec constructs a highly aligned draft\nmodel by generating low-bit quantized substitute layers from offloaded target\nLLM portions. Additionally, our method shares the remaining GPU-resident layers\nand the KV-Cache, further reducing memory overhead and enhance alignment.\nSubSpec achieves a high average acceptance length, delivering 9.1x speedup for\nQwen2.5 7B on MT-Bench (8GB VRAM limit) and an average of 12.5x speedup for\nQwen2.5 32B on popular generation benchmarks (24GB VRAM limit).", "AI": {"tldr": "SubSpec是一种无损、免训练的即插即用方法，通过从卸载的目标LLM部分生成低比特量化替代层来构建高度对齐的草稿模型，并共享GPU驻留层和KV-Cache，显著加速了参数卸载场景下的LLM推理，解决了内存限制消费级GPU上的部署挑战。", "motivation": "大型语言模型（LLMs）的巨大尺寸使得在内存受限的消费级GPU上部署面临挑战。现有策略如模型压缩会降低质量，参数卸载虽然保持质量但推理速度慢。推测解码可以加速参数卸载，但现有方法通常依赖预训练权重或需要额外训练来对齐，且训练草稿模型的方法因对齐不足导致加速效果不明显。", "method": "本文提出了SubSpec，一种无损、免训练的即插即用方法来加速参数卸载。SubSpec通过从卸载的目标LLM部分生成低比特量化替代层来构建高度对齐的草稿模型。此外，该方法还共享剩余的GPU驻留层和KV-Cache，进一步减少内存开销并增强对齐。", "result": "SubSpec实现了较高的平均接受长度，在MT-Bench上为Qwen2.5 7B（8GB显存限制）提供了9.1倍的加速，在流行的生成基准测试中为Qwen2.5 32B（24GB显存限制）平均提供了12.5倍的加速。", "conclusion": "SubSpec通过构建高度对齐的量化替代层草稿模型并共享资源，成功地在不损失质量和无需训练的情况下，显著加速了内存受限GPU上LLM的参数卸载推理，解决了现有方法的局限性。"}}
{"id": "2509.18460", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.18460", "abs": "https://arxiv.org/abs/2509.18460", "authors": ["Haeyoon Han", "Mahdi Taheri", "Soon-Jo Chung", "Fred Y. Hadaegh"], "title": "A Counterfactual Reasoning Framework for Fault Diagnosis in Robot Perception Systems", "comment": null, "summary": "Perception systems provide a rich understanding of the environment for\nautonomous systems, shaping decisions in all downstream modules. Hence,\naccurate detection and isolation of faults in perception systems is important.\nFaults in perception systems pose particular challenges: faults are often tied\nto the perceptual context of the environment, and errors in their multi-stage\npipelines can propagate across modules. To address this, we adopt a\ncounterfactual reasoning approach to propose a framework for fault detection\nand isolation (FDI) in perception systems. As opposed to relying on physical\nredundancy (i.e., having extra sensors), our approach utilizes analytical\nredundancy with counterfactual reasoning to construct perception reliability\ntests as causal outcomes influenced by system states and fault scenarios.\nCounterfactual reasoning generates reliability test results under hypothesized\nfaults to update the belief over fault hypotheses. We derive both passive and\nactive FDI methods. While the passive FDI can be achieved by belief updates,\nthe active FDI approach is defined as a causal bandit problem, where we utilize\nMonte Carlo Tree Search (MCTS) with upper confidence bound (UCB) to find\ncontrol inputs that maximize a detection and isolation metric, designated as\nEffective Information (EI). The mentioned metric quantifies the informativeness\nof control inputs for FDI. We demonstrate the approach in a robot exploration\nscenario, where a space robot performing vision-based navigation actively\nadjusts its attitude to increase EI and correctly isolate faults caused by\nsensor damage, dynamic scenes, and perceptual degradation.", "AI": {"tldr": "该论文提出了一种基于反事实推理的感知系统故障检测与隔离（FDI）框架，利用分析冗余而非物理冗余，并通过主动FDI方法（结合因果多臂老虎机和MCTS）来优化控制输入以提高故障隔离效率。", "motivation": "感知系统为自主系统提供环境理解，其故障会影响所有下游模块的决策。感知系统故障具有挑战性，常与环境感知上下文相关，且多阶段管道中的错误会传播，因此准确检测和隔离感知故障至关重要。", "method": "该研究采用反事实推理方法构建感知系统FDI框架。它利用分析冗余而非物理冗余，将感知可靠性测试构建为受系统状态和故障情景影响的因果结果。反事实推理生成假设故障下的可靠性测试结果以更新对故障假设的信念。论文推导了被动和主动FDI方法：被动FDI通过信念更新实现；主动FDI被定义为因果多臂老虎机问题，利用蒙特卡洛树搜索（MCTS）结合上置信界（UCB）来寻找最大化“有效信息”（EI）的控制输入，EI量化了控制输入对FDI的信息量。", "result": "该方法在一个机器人探索场景中进行了演示。在该场景中，一个执行基于视觉导航的空间机器人主动调整其姿态以增加EI，并正确隔离了由传感器损坏、动态场景和感知退化引起的故障。", "conclusion": "该论文提出了一种新颖的基于反事实推理的感知系统故障检测与隔离框架，通过结合分析冗余和主动FDI（利用因果多臂老虎机和MCTS优化信息量），能够有效地检测和隔离自主系统中的感知故障，并在实际机器人探索场景中得到了验证。"}}
{"id": "2509.18216", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18216", "abs": "https://arxiv.org/abs/2509.18216", "authors": ["Amitava Das"], "title": "nDNA -- the Semantic Helix of Artificial Cognition", "comment": null, "summary": "As AI foundation models grow in capability, a deeper question emerges: What\nshapes their internal cognitive identity -- beyond fluency and output?\nBenchmarks measure behavior, but the soul of a model resides in its latent\ngeometry. In this work, we propose Neural DNA (nDNA) as a semantic-genotypic\nrepresentation that captures this latent identity through the intrinsic\ngeometry of belief. At its core, nDNA is synthesized from three principled and\nindispensable dimensions of latent geometry: spectral curvature, which reveals\nthe curvature of conceptual flow across layers; thermodynamic length, which\nquantifies the semantic effort required to traverse representational\ntransitions through layers; and belief vector field, which delineates the\nsemantic torsion fields that guide a model's belief directional orientations.\nLike biological DNA, it encodes ancestry, mutation, and semantic inheritance,\nfound in finetuning and alignment scars, cultural imprints, and architectural\ndrift. In naming it, we open a new field: Neural Genomics, where models are not\njust tools, but digital semantic organisms with traceable inner cognition.\n  Modeling statement. We read AI foundation models as semantic fluid--dynamics:\nmeaning is transported through layers like fluid in a shaped conduit; nDNA is\nthe physics-grade readout of that flow -- a geometry-first measure of how\nmeaning is bent, paid for, and pushed -- yielding a stable, coordinate-free\nneural DNA fingerprint tied to on-input behavior; with this fingerprint we\ncross into biology: tracing lineages across pretraining, fine-tuning,\nalignment, pruning, distillation, and merges; measuring inheritance between\ncheckpoints; detecting drift as traits shift under new data or objectives; and,\nultimately, studying the evolution of artificial cognition to compare models,\ndiagnose risks, and govern change over time.", "AI": {"tldr": "本文提出“神经DNA”（nDNA）作为一种语义基因型表示，通过模型潜在几何的内在属性捕捉AI基础模型的内部认知身份，并开创“神经基因组学”领域，以研究AI模型的演化、继承和漂移。", "motivation": "随着AI基础模型能力的增强，研究者们希望深入理解其内部认知身份，而非仅仅是外在表现和输出。当前的基准测试只能衡量行为，而模型的“灵魂”存在于其潜在几何中。因此，需要一种方法来捕捉这种超越表面性能的内在身份。", "method": "本文提出神经DNA（nDNA），它是一种语义基因型表示，通过信念的内在几何结构捕捉模型的潜在身份。nDNA由三个核心潜在几何维度合成：谱曲率（揭示跨层的概念流曲率）、热力学长度（量化跨层表征转换所需的语义努力）和信念向量场（描绘引导模型信念方向的语义扭转场）。研究将AI基础模型视为语义流体动力学，nDNA是这种流动的物理级读数。", "result": "nDNA提供了一个稳定、无坐标的神经DNA指纹，可以追溯模型在预训练、微调、对齐、剪枝、蒸馏和合并等过程中的谱系；测量检查点之间的继承性；检测新数据或目标下特征变化的漂移；并最终用于研究人工认知的演化，以比较模型、诊断风险并管理随时间的变化。", "conclusion": "通过引入神经DNA，本文开辟了一个新的领域——神经基因组学，将模型视为具有可追溯内部认知的数字语义有机体。这使得研究者能够深入理解模型的内部机制，进行模型比较、风险诊断和演化治理，从而超越行为测量，探索AI模型的内在“灵魂”。"}}
{"id": "2509.18185", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18185", "abs": "https://arxiv.org/abs/2509.18185", "authors": ["Giammarco La Barbera", "Enzo Bonnot", "Thomas Isla", "Juan Pablo de la Plata", "Joy-Rose Dunoyer de Segonzac", "Jennifer Attali", "Cécile Lozach", "Alexandre Bellucci", "Louis Marcellin", "Laure Fournier", "Sabine Sarnacki", "Pietro Gori", "Isabelle Bloch"], "title": "Visionerves: Automatic and Reproducible Hybrid AI for Peripheral Nervous System Recognition Applied to Endometriosis Cases", "comment": "Computer-Aided Pelvic Imaging for Female Health (CAPI) - Workshop\n  MICCAI 2025", "summary": "Endometriosis often leads to chronic pelvic pain and possible nerve\ninvolvement, yet imaging the peripheral nerves remains a challenge. We\nintroduce Visionerves, a novel hybrid AI framework for peripheral nervous\nsystem recognition from multi-gradient DWI and morphological MRI data. Unlike\nconventional tractography, Visionerves encodes anatomical knowledge through\nfuzzy spatial relationships, removing the need for selection of manual ROIs.\nThe pipeline comprises two phases: (A) automatic segmentation of anatomical\nstructures using a deep learning model, and (B) tractography and nerve\nrecognition by symbolic spatial reasoning. Applied to the lumbosacral plexus in\n10 women with (confirmed or suspected) endometriosis, Visionerves demonstrated\nsubstantial improvements over standard tractography, with Dice score\nimprovements of up to 25% and spatial errors reduced to less than 5 mm. This\nautomatic and reproducible approach enables detailed nerve analysis and paves\nthe way for non-invasive diagnosis of endometriosis-related neuropathy, as well\nas other conditions with nerve involvement.", "AI": {"tldr": "Visionerves是一个混合AI框架，用于从多梯度DWI和形态MRI数据中识别周围神经系统。它通过模糊空间关系编码解剖知识，无需手动选择ROI，显著提高了对腰骶丛的神经识别准确性，为子宫内膜异位症相关神经病变诊断提供了新的非侵入性方法。", "motivation": "子宫内膜异位症常导致慢性盆腔疼痛并可能涉及神经，但周围神经的成像仍然是一个挑战。", "method": "本研究引入了Visionerves，一个混合AI框架，用于从多梯度DWI和形态MRI数据中识别周围神经系统。该框架通过模糊空间关系编码解剖知识，从而消除了手动选择ROI的需要。其流程包括两个阶段：(A) 使用深度学习模型自动分割解剖结构；(B) 通过符号空间推理进行纤维束描记和神经识别。", "result": "将Visionerves应用于10名患有（确诊或疑似）子宫内膜异位症女性的腰骶丛，结果显示其性能较标准纤维束描记法有显著提升：Dice分数提高了高达25%，空间误差减少到小于5毫米。", "conclusion": "Visionerves提供了一种自动且可重复的详细神经分析方法，为子宫内膜异位症相关神经病变以及其他涉及神经的疾病的非侵入性诊断铺平了道路。"}}
{"id": "2509.19073", "categories": ["cs.CV", "eess.IV", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.19073", "abs": "https://arxiv.org/abs/2509.19073", "authors": ["Hung Nguyen", "Runfa Li", "An Le", "Truong Nguyen"], "title": "WaveletGaussian: Wavelet-domain Diffusion for Sparse-view 3D Gaussian Object Reconstruction", "comment": null, "summary": "3D Gaussian Splatting (3DGS) has become a powerful representation for\nimage-based object reconstruction, yet its performance drops sharply in\nsparse-view settings. Prior works address this limitation by employing\ndiffusion models to repair corrupted renders, subsequently using them as pseudo\nground truths for later optimization. While effective, such approaches incur\nheavy computation from the diffusion fine-tuning and repair steps. We present\nWaveletGaussian, a framework for more efficient sparse-view 3D Gaussian object\nreconstruction. Our key idea is to shift diffusion into the wavelet domain:\ndiffusion is applied only to the low-resolution LL subband, while\nhigh-frequency subbands are refined with a lightweight network. We further\npropose an efficient online random masking strategy to curate training pairs\nfor diffusion fine-tuning, replacing the commonly used, but inefficient,\nleave-one-out strategy. Experiments across two benchmark datasets, Mip-NeRF 360\nand OmniObject3D, show WaveletGaussian achieves competitive rendering quality\nwhile substantially reducing training time.", "AI": {"tldr": "针对稀疏视角下的3D高斯泼溅（3DGS）重建问题，本文提出WaveletGaussian框架，通过将扩散模型应用于小波域的低频部分并结合轻量级网络处理高频部分，以及采用高效的在线随机掩码策略，显著提高了训练效率，同时保持了竞争性的渲染质量。", "motivation": "3D高斯泼溅（3DGS）在稀疏视角设置下性能急剧下降。现有方法通过扩散模型修复损坏的渲染图作为伪真值进行优化，但扩散模型的微调和修复步骤计算成本高昂。", "method": "WaveletGaussian框架的核心思想是将扩散过程转移到小波域：扩散仅应用于低分辨率的LL子带，而高频子带则通过轻量级网络进行细化。此外，提出了一种高效的在线随机掩码策略来生成扩散模型微调的训练对，取代了传统但效率低下的“留一法”策略。", "result": "在Mip-NeRF 360和OmniObject3D两个基准数据集上的实验表明，WaveletGaussian在实现具有竞争力的渲染质量的同时，大幅缩短了训练时间。", "conclusion": "WaveletGaussian提供了一种更高效的稀疏视角3D高斯物体重建方法，它通过创新的小波域扩散和高效的掩码策略，显著降低了计算成本，同时保持了高质量的重建效果。"}}
{"id": "2509.18988", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.18988", "abs": "https://arxiv.org/abs/2509.18988", "authors": ["Ziliang Lyu", "Miroslav Krstic", "Kaixin Lu", "Yiguang Hong", "Lihua Xie"], "title": "Adaptive Override Control under High-Relative-Degree Nonovershooting Constraints", "comment": null, "summary": "This paper considers the problem of adaptively overriding unsafe actions of a\nnominal controller in the presence of high-relative-degree nonovershooting\nconstraints and parametric uncertainties. To prevent the design from being\ncoupled with high-order derivatives of the parameter estimation error, we adopt\na modular design approach in which the controller and the parameter identifier\nare designed separately. The controller module ensures that any safety\nviolations caused by parametric uncertainties remain bounded, provided that the\nparameter estimation error and its first-order derivative are either bounded or\nsquare-integrable. The identifier module, in turn, guarantees that these\nrequirements on the parameter estimation error are satisfied. Both theoretical\nanalysis and simulation results demonstrate that the closed-loop safety\nviolation is bounded by a tunable function of the initial estimation error.\nMoreover, as time increases, the parameter estimate converges to the true\nvalue, and the amount of safety violation decreases accordingly.", "AI": {"tldr": "本文提出一种模块化设计方法，用于在存在高相对度非超调约束和参数不确定性的情况下，自适应地覆盖标称控制器的不安全动作，实现安全违规的有界性和参数估计的收敛。", "motivation": "研究动机是为了解决在存在高相对度非超调约束和参数不确定性时，如何自适应地覆盖标称控制器可能产生的不安全动作，同时避免设计与参数估计误差的高阶导数耦合的问题。", "method": "采用模块化设计方法，将控制器和参数识别器分开设计。控制器模块确保在参数估计误差及其一阶导数有界或平方可积的条件下，由参数不确定性引起的安全违规保持有界。识别器模块则保证满足参数估计误差的这些要求。", "result": "理论分析和仿真结果表明，闭环安全违规由初始估计误差的可调函数所限制。此外，随着时间的增加，参数估计收敛到真实值，安全违规量也相应减少。", "conclusion": "该模块化设计方法能够有效处理具有高相对度约束和参数不确定性的自适应安全覆盖问题，确保了安全违规的有界性，且其上限可调并与初始估计误差相关，同时实现了参数估计的收敛和安全违规的减少。"}}
{"id": "2509.18360", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18360", "abs": "https://arxiv.org/abs/2509.18360", "authors": ["Chutong Meng", "Philipp Koehn"], "title": "Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents", "comment": "Accepted by EMNLP 2025 (main)", "summary": "We present Speech Vecalign, a parallel speech document alignment method that\nmonotonically aligns speech segment embeddings and does not depend on text\ntranscriptions. Compared to the baseline method Global Mining, a variant of\nspeech mining, Speech Vecalign produces longer speech-to-speech alignments. It\nalso demonstrates greater robustness than Local Mining, another speech mining\nvariant, as it produces less noise. We applied Speech Vecalign to 3,000 hours\nof unlabeled parallel English-German (En-De) speech documents from VoxPopuli,\nyielding about 1,000 hours of high-quality alignments. We then trained En-De\nspeech-to-speech translation models on the aligned data. Speech Vecalign\nimproves the En-to-De and De-to-En performance over Global Mining by 0.37 and\n0.18 ASR-BLEU, respectively. Moreover, our models match or outperform\nSpeechMatrix model performance, despite using 8 times fewer raw speech\ndocuments.", "AI": {"tldr": "本文提出Speech Vecalign，一种不依赖文本转录的并行语音文档对齐方法。它能生成更长、更鲁棒的对齐结果，并在语音到语音翻译任务中显著优于现有基线方法，甚至在数据量更少的情况下能与先进模型媲美或超越。", "motivation": "研究旨在开发一种不依赖文本转录、能生成高质量、鲁棒且更长语音到语音对齐的并行语音文档对齐方法，以改进语音到语音翻译模型的性能。", "method": "本文提出Speech Vecalign方法，该方法通过单调对齐语音片段嵌入来实现并行语音文档对齐，且不依赖于文本转录。", "result": "1. Speech Vecalign比基线方法Global Mining能产生更长的语音到语音对齐。2. 它比Local Mining更具鲁棒性，产生的噪声更少。3. 应用于3000小时的未标注英德并行语音数据，生成了约1000小时的高质量对齐。4. 基于这些对齐数据训练的英德语音到语音翻译模型，在En-to-De和De-to-En任务上的ASR-BLEU分数分别比Global Mining提高了0.37和0.18。5. 尽管使用了少8倍的原始语音文档，其模型性能与SpeechMatrix模型相当或更优。", "conclusion": "Speech Vecalign是一种高效且鲁棒的并行语音文档对齐方法，无需文本转录即可生成高质量的对齐数据。这些对齐数据能显著提升语音到语音翻译模型的性能，甚至在数据量较少的情况下也能超越现有先进模型。"}}
{"id": "2509.18463", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18463", "abs": "https://arxiv.org/abs/2509.18463", "authors": ["Jannick van Buuren", "Roberto Giglio", "Loris Roveda", "Luka Peternel"], "title": "Robotic Skill Diversification via Active Mutation of Reward Functions in Reinforcement Learning During a Liquid Pouring Task", "comment": null, "summary": "This paper explores how deliberate mutations of reward function in\nreinforcement learning can produce diversified skill variations in robotic\nmanipulation tasks, examined with a liquid pouring use case. To this end, we\ndeveloped a new reward function mutation framework that is based on applying\nGaussian noise to the weights of the different terms in the reward function.\nInspired by the cost-benefit tradeoff model from human motor control, we\ndesigned the reward function with the following key terms: accuracy, time, and\neffort. The study was performed in a simulation environment created in NVIDIA\nIsaac Sim, and the setup included Franka Emika Panda robotic arm holding a\nglass with a liquid that needed to be poured into a container. The\nreinforcement learning algorithm was based on Proximal Policy Optimization. We\nsystematically explored how different configurations of mutated weights in the\nrewards function would affect the learned policy. The resulting policies\nexhibit a wide range of behaviours: from variations in execution of the\noriginally intended pouring task to novel skills useful for unexpected tasks,\nsuch as container rim cleaning, liquid mixing, and watering. This approach\noffers promising directions for robotic systems to perform diversified learning\nof specific tasks, while also potentially deriving meaningful skills for future\ntasks.", "AI": {"tldr": "该研究通过对强化学习中的奖励函数进行故意变异，成功在机器人操作任务中（以倒水为例）生成了多样化的技能变体。", "motivation": "探索如何通过变异强化学习中的奖励函数，使机器人系统在操作任务中产生多样化的技能，从而超越单一的预期行为。", "method": "开发了一个新的奖励函数变异框架，该框架通过对奖励函数中“准确性”、“时间”和“努力”等关键项的权重施加高斯噪声来实现。奖励函数的设计灵感来源于人类运动控制的成本效益权衡模型。实验在NVIDIA Isaac Sim仿真环境中，使用Franka Emika Panda机械臂和Proximal Policy Optimization (PPO)算法进行，系统地探索了不同变异权重配置对学习策略的影响。", "result": "变异后的奖励函数使机器人学习到的策略展现出广泛的行为范围，不仅包括原始倒水任务执行方式的变化，还衍生出诸如容器边缘清洁、液体混合和浇水等意想不到的新颖技能。", "conclusion": "这种方法为机器人系统在特定任务中进行多样化学习提供了有前景的方向，并有可能为未来的任务衍生出有意义的技能，拓展了机器人的应用潜力。"}}
{"id": "2509.18218", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18218", "abs": "https://arxiv.org/abs/2509.18218", "authors": ["Kei-Sing Ng"], "title": "Similarity Field Theory: A Mathematical Framework for Intelligence", "comment": null, "summary": "We posit that persisting and transforming similarity relations form the\nstructural basis of any comprehensible dynamic system. This paper introduces\nSimilarity Field Theory, a mathematical framework that formalizes the\nprinciples governing similarity values among entities and their evolution. We\ndefine: (1) a similarity field $S: U \\times U \\to [0,1]$ over a universe of\nentities $U$, satisfying reflexivity $S(E,E)=1$ and treated as a directed\nrelational field (asymmetry and non-transitivity are allowed); (2) the\nevolution of a system through a sequence $Z_p = (X_p, S^{(p)})$ indexed by\n$p=0,1,2,\\ldots$; (3) concepts $K$ as entities that induce fibers\n$F_{\\alpha}(K) = { E \\in U \\mid S(E,K) \\ge \\alpha }$, i.e., superlevel sets of\nthe unary map $S_K(E) := S(E,K)$; and (4) a generative operator $G$ that\nproduces new entities. Within this framework, we formalize a generative\ndefinition of intelligence: an operator $G$ is intelligent with respect to a\nconcept $K$ if, given a system containing entities belonging to the fiber of\n$K$, it generates new entities that also belong to that fiber. Similarity Field\nTheory thus offers a foundational language for characterizing, comparing, and\nconstructing intelligent systems. We prove two theorems: (i) asymmetry blocks\nmutual inclusion; and (ii) stability requires either an anchor coordinate or\neventual confinement within a level set of $f$. These results ensure that the\nevolution of similarity fields is both constrained and interpretable,\nculminating in an exploration of how the framework allows us to interpret large\nlanguage models and use them as experimental probes into societal cognition.", "AI": {"tldr": "解析错误", "motivation": "解析错误", "method": "解析错误", "result": "解析错误", "conclusion": "解析错误"}}
{"id": "2509.18187", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18187", "abs": "https://arxiv.org/abs/2509.18187", "authors": ["Muhammad Naveed", "Nazia Perwaiz", "Sidra Sultana", "Mohaira Ahmad", "Muhammad Moazam Fraz"], "title": "V-SenseDrive: A Privacy-Preserving Road Video and In-Vehicle Sensor Fusion Framework for Road Safety & Driver Behaviour Modelling", "comment": null, "summary": "Road traffic accidents remain a major public health challenge, particularly\nin countries with heterogeneous road conditions, mixed traffic flow, and\nvariable driving discipline, such as Pakistan. Reliable detection of unsafe\ndriving behaviours is a prerequisite for improving road safety, enabling\nadvanced driver assistance systems (ADAS), and supporting data driven decisions\nin insurance and fleet management. Most of existing datasets originate from the\ndeveloped countries with limited representation of the behavioural diversity\nobserved in emerging economies and the driver's face recording voilates the\nprivacy preservation. We present V-SenseDrive, the first privacy-preserving\nmultimodal driver behaviour dataset collected entirely within the Pakistani\ndriving environment. V-SenseDrive combines smartphone based inertial and GPS\nsensor data with synchronized road facing video to record three target driving\nbehaviours (normal, aggressive, and risky) on multiple types of roads,\nincluding urban arterials, secondary roads, and motorways. Data was gathered\nusing a custom Android application designed to capture high frequency\naccelerometer, gyroscope, and GPS streams alongside continuous video, with all\nsources precisely time aligned to enable multimodal analysis. The focus of this\nwork is on the data acquisition process, covering participant selection,\ndriving scenarios, environmental considerations, and sensor video\nsynchronization techniques. The dataset is structured into raw, processed, and\nsemantic layers, ensuring adaptability for future research in driver behaviour\nclassification, traffic safety analysis, and ADAS development. By representing\nreal world driving in Pakistan, V-SenseDrive fills a critical gap in the global\nlandscape of driver behaviour datasets and lays the groundwork for context\naware intelligent transportation solutions.", "AI": {"tldr": "V-SenseDrive是首个在巴基斯坦驾驶环境中收集的、保护隐私的多模态驾驶行为数据集，结合智能手机传感器数据和路向视频，旨在提升道路安全和支持智能交通系统。", "motivation": "道路交通事故是巴基斯坦等新兴经济体的重大公共卫生挑战，这些国家路况复杂、交通流量混合、驾驶纪律不一。现有数据集多来自发达国家，缺乏对新兴经济体驾驶行为多样性的代表性，且驾驶员面部记录存在隐私问题。因此，需要一个能代表这些环境并保护隐私的数据集。", "method": "研究团队开发了V-SenseDrive数据集。他们使用定制的Android应用程序，结合智能手机的惯性传感器（加速度计、陀螺仪）和GPS数据，以及同步的路向视频，在巴基斯坦的城市主干道、次级道路和高速公路等多种道路类型上，记录了三种目标驾驶行为（正常、激进和危险）。所有数据源均经过精确时间对齐，并结构化为原始、处理和语义层。", "result": "研究成果是V-SenseDrive，这是第一个完全在巴基斯坦驾驶环境中收集的、保护隐私的多模态驾驶行为数据集。它成功地结合了智能手机传感器数据和路向视频，记录了多样化的驾驶行为，并解决了现有数据集在代表性和隐私方面的不足。", "conclusion": "V-SenseDrive数据集填补了全球驾驶行为数据集中在巴基斯坦等新兴经济体方面的关键空白。它为未来的驾驶行为分类、交通安全分析和高级驾驶辅助系统（ADAS）开发奠定了基础，并为情境感知智能交通解决方案提供了支持。"}}
{"id": "2509.19045", "categories": ["eess.SY", "cs.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.19045", "abs": "https://arxiv.org/abs/2509.19045", "authors": ["Dakota Thompson", "Amro M. Farid"], "title": "A Weighted Least Squares Error Hetero-functional Graph State Estimator of the American Multi-modal Energy System", "comment": "27 pages, 3 Tables, 11 Figures", "summary": "As one of the most pressing challenges of the 21st century, global climate\nchange demands a host of changes across at least four critical energy\ninfrastructures: the electric grid, the natural gas system, the oil system, and\nthe coal system. In the context of the United States, this paper refers to this\nsystem-of-systems as ``The American Multi-Modal Energy System (AMES)\". These\ncombined changes necessitate an understanding of the AMES interdependencies,\nboth structurally and behaviorally, to develop and enact effective policies.\nThis work focuses on behavioral analysis methods to provide examples of how to\nanalyze system behavior and the critical matter and energy flows through the\nsystem. Building upon past works, two regions of the AMES are modeled, and\ntheir behavior is analyzed using Hetero-functional Graph Theory (HFGT). More\nspecifically, the work presents a weighted least square error state estimation\nmodel of the AMES. State estimation has played a major role in the operation\nand development of the American Electric Power System. This work extends the\nstate estimation analysis beyond the single-operand electric grid environment\ninto the heterogeneous environment of the AMES. Employing a data-driven and\nmodel-based systems engineering approach in combination with HFGT, a Weighted\nLeast Squares Error Hetero-functional Graph State Estimation (WLSEHFGSE)\noptimization program is developed to estimate the optimal flows of mass and\nenergy through the AMES. This work is the first to integrate state estimation\nmethods with HFGT. Furthermore, it demonstrates how such a WLSEHFGSE recovers\nthe mass and energy flows in a system-of-systems like the AMES with asset-level\ngranularity.", "AI": {"tldr": "为应对气候变化，本文分析了美国多模式能源系统（AMES）的行为和关键物质与能量流。通过将状态估计方法与异质功能图理论（HFGT）结合，首次提出了加权最小二乘误差异质功能图状态估计（WLSEHFGSE）优化程序，以资产级粒度估计AMES中的物质和能量流。", "motivation": "21世纪全球气候变化是紧迫挑战，需要对电力、天然气、石油和煤炭等关键能源基础设施进行变革。为了制定有效的政策，需要理解美国多模式能源系统（AMES）在结构和行为上的相互依赖性。", "method": "本文侧重于行为分析方法，以数据驱动和基于模型的系统工程方法，结合异质功能图理论（HFGT）。具体而言，建模了AMES的两个区域，并开发了一种加权最小二乘误差异质功能图状态估计（WLSEHFGSE）优化程序，以估计通过AMES的物质和能量的最佳流量。该方法将状态估计分析从单一的电网环境扩展到AMES的异质环境。", "result": "开发了WLSEHFGSE优化程序，用于估计通过AMES的最佳物质和能量流。这是首次将状态估计方法与HFGT集成。该方法能够以资产级粒度恢复AMES等系统中的物质和能量流。", "conclusion": "通过将状态估计方法与异质功能图理论（HFGT）相结合，本文提出的WLSEHFGSE方法能够有效地分析和恢复美国多模式能源系统（AMES）中具有资产级粒度的物质和能量流，这对于应对气候变化的政策制定至关重要。"}}
{"id": "2509.18377", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18377", "abs": "https://arxiv.org/abs/2509.18377", "authors": ["Xinlu He", "Yiwen Guan", "Badrivishal Paurana", "Zilin Dai", "Jacob Whitehill"], "title": "Interactive Real-Time Speaker Diarization Correction with Human Feedback", "comment": null, "summary": "Most automatic speech processing systems operate in \"open loop\" mode without\nuser feedback about who said what; yet, human-in-the-loop workflows can\npotentially enable higher accuracy. We propose an LLM-assisted speaker\ndiarization correction system that lets users fix speaker attribution errors in\nreal time. The pipeline performs streaming ASR and diarization, uses an LLM to\ndeliver concise summaries to the users, and accepts brief verbal feedback that\nis immediately incorporated without disrupting interactions. Moreover, we\ndevelop techniques to make the workflow more effective: First, a\nsplit-when-merged (SWM) technique detects and splits multi-speaker segments\nthat the ASR erroneously attributes to just a single speaker. Second, online\nspeaker enrollments are collected based on users' diarization corrections, thus\nhelping to prevent speaker diarization errors from occurring in the future.\nLLM-driven simulations on the AMI test set indicate that our system\nsubstantially reduces DER by 9.92% and speaker confusion error by 44.23%. We\nfurther analyze correction efficacy under different settings, including summary\nvs full transcript display, the number of online enrollments limitation, and\ncorrection frequency.", "AI": {"tldr": "本文提出一个由LLM辅助的实时说话人日志纠正系统，允许用户通过简短的口头反馈修正说话人归属错误，并通过SWM技术和在线说话人注册进一步提高准确性。", "motivation": "大多数自动语音处理系统以“开环”模式运行，缺乏用户反馈，而人机协作的工作流程有可能实现更高的准确性。", "method": "该系统采用流式ASR和说话人日志，使用LLM向用户提供简洁的摘要，并接受即时整合的简短口头反馈。此外，它还开发了两种技术：1) 分割合并（SWM）技术，用于检测并分割被ASR错误归因于单个说话人的多说话人片段；2) 基于用户纠正收集在线说话人注册，以预防未来的说话人日志错误。", "result": "在AMI测试集上，LLM驱动的模拟显示，该系统将DER大幅降低了9.92%，说话人混淆错误降低了44.23%。研究还分析了不同设置下的纠正效果，包括摘要与完整转录显示、在线注册数量限制和纠正频率。", "conclusion": "该LLM辅助的说话人日志纠正系统通过实时用户反馈、SWM技术和在线注册，显著提高了说话人日志的准确性，有效减少了DER和说话人混淆错误。"}}
{"id": "2509.18466", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18466", "abs": "https://arxiv.org/abs/2509.18466", "authors": ["Junnosuke Kamohara", "Feiyang Wu", "Chinmayee Wamorkar", "Seth Hutchinson", "Ye Zhao"], "title": "RL-augmented Adaptive Model Predictive Control for Bipedal Locomotion over Challenging Terrain", "comment": null, "summary": "Model predictive control (MPC) has demonstrated effectiveness for humanoid\nbipedal locomotion; however, its applicability in challenging environments,\nsuch as rough and slippery terrain, is limited by the difficulty of modeling\nterrain interactions. In contrast, reinforcement learning (RL) has achieved\nnotable success in training robust locomotion policies over diverse terrain,\nyet it lacks guarantees of constraint satisfaction and often requires\nsubstantial reward shaping. Recent efforts in combining MPC and RL have shown\npromise of taking the best of both worlds, but they are primarily restricted to\nflat terrain or quadrupedal robots. In this work, we propose an RL-augmented\nMPC framework tailored for bipedal locomotion over rough and slippery terrain.\nOur method parametrizes three key components of\nsingle-rigid-body-dynamics-based MPC: system dynamics, swing leg controller,\nand gait frequency. We validate our approach through bipedal robot simulations\nin NVIDIA IsaacLab across various terrains, including stairs, stepping stones,\nand low-friction surfaces. Experimental results demonstrate that our\nRL-augmented MPC framework produces significantly more adaptive and robust\nbehaviors compared to baseline MPC and RL.", "AI": {"tldr": "本文提出了一种RL增强的MPC框架，专为双足机器人在崎岖和湿滑地形上的运动设计，通过RL参数化MPC的关键组件，实现了比传统MPC和RL更强的适应性和鲁棒性。", "motivation": "模型预测控制（MPC）在双足运动中有效，但在崎岖和湿滑地形中，由于难以建模地形交互而受限。强化学习（RL）在多变地形上训练鲁棒策略方面表现出色，但缺乏约束满足保证且需要大量奖励塑形。现有的MPC与RL结合的方法主要限于平坦地形或四足机器人，缺乏针对双足机器人在复杂地形上的解决方案。", "method": "本文提出了一种RL增强的MPC框架，专门针对双足机器人在崎岖和湿滑地形上的运动。该方法利用RL对基于单刚体动力学的MPC的三个关键组件进行参数化：系统动力学、摆动腿控制器和步态频率。通过在NVIDIA IsaacLab中对双足机器人进行模拟验证，测试了包括楼梯、垫脚石和低摩擦表面在内的各种地形。", "result": "实验结果表明，与基线MPC和RL相比，本文提出的RL增强MPC框架能够产生显著更具适应性和鲁棒性的行为。", "conclusion": "所提出的RL增强MPC框架有效解决了双足机器人在崎岖和湿滑地形上运动的挑战，通过结合MPC的优点和RL的鲁棒性，显著提高了机器人的适应性和性能。"}}
{"id": "2509.18221", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18221", "abs": "https://arxiv.org/abs/2509.18221", "authors": ["Dingxin Lu", "Shurui Wu", "Xinyi Huang"], "title": "Multimodal Health Risk Prediction System for Chronic Diseases via Vision-Language Fusion and Large Language Models", "comment": null, "summary": "With the rising global burden of chronic diseases and the multimodal and\nheterogeneous clinical data (medical imaging, free-text recordings, wearable\nsensor streams, etc.), there is an urgent need for a unified multimodal AI\nframework that can proactively predict individual health risks. We propose\nVL-RiskFormer, a hierarchical stacked visual-language multimodal Transformer\nwith a large language model (LLM) inference head embedded in its top layer. The\nsystem builds on the dual-stream architecture of existing visual-linguistic\nmodels (e.g., PaLM-E, LLaVA) with four key innovations: (i) pre-training with\ncross-modal comparison and fine-grained alignment of radiological images,\nfundus maps, and wearable device photos with corresponding clinical narratives\nusing momentum update encoders and debiased InfoNCE losses; (ii) a time fusion\nblock that integrates irregular visit sequences into the causal Transformer\ndecoder through adaptive time interval position coding; (iii) a disease\nontology map adapter that injects ICD-10 codes into visual and textual channels\nin layers and infers comorbid patterns with the help of a graph attention\nmechanism. On the MIMIC-IV longitudinal cohort, VL-RiskFormer achieved an\naverage AUROC of 0.90 with an expected calibration error of 2.7 percent.", "AI": {"tldr": "VL-RiskFormer是一个分层堆叠的视觉-语言多模态Transformer模型，通过整合多种临床数据，并结合LLM推理头，旨在主动预测个体健康风险。", "motivation": "慢性病负担日益加重，临床数据（医学影像、文本记录、可穿戴设备数据等）呈现多模态和异构性，迫切需要一个统一的多模态AI框架来主动预测个体健康风险。", "method": "本文提出了VL-RiskFormer，一个分层堆叠的视觉-语言多模态Transformer，其顶层嵌入了大型语言模型（LLM）推理头。该系统基于现有视觉-语言模型的双流架构，并引入了四项创新：(i) 使用动量更新编码器和去偏InfoNCE损失，对放射影像、眼底图和可穿戴设备照片与临床叙述进行跨模态比较和细粒度对齐预训练；(ii) 通过自适应时间间隔位置编码，将不规则就诊序列整合到因果Transformer解码器中的时间融合块；(iii) 一个疾病本体图适配器，将ICD-10编码注入视觉和文本通道，并借助图注意力机制推断共病模式。", "result": "在MIMIC-IV纵向队列数据上，VL-RiskFormer实现了平均0.90的AUROC，预期校准误差为2.7%。", "conclusion": "VL-RiskFormer是一个有效且性能优异的多模态AI框架，能够利用多种异构临床数据对个体健康风险进行主动预测，并展示了良好的校准性能。"}}
{"id": "2509.18189", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18189", "abs": "https://arxiv.org/abs/2509.18189", "authors": ["Daxiang Dong", "Mingming Zheng", "Dong Xu", "Bairong Zhuang", "Wenyu Zhang", "Chunhua Luo", "Haoran Wang", "Zijian Zhao", "Jie Li", "Yuxuan Li", "Hanjun Zhong", "Mengyue Liu", "Jieting Chen", "Shupeng Li", "Lun Tian", "Yaping Feng", "Xin Li", "Donggang Jiang", "Yong Chen", "Yehua Xu", "Duohao Qin", "Chen Feng", "Dan Wang", "Henghua Zhang", "Jingjing Ha", "Jinhui He", "Yanfeng Zhai", "Chengxin Zheng", "Jiayi Mao", "Jiacheng Chen", "Ruchang Yao", "Ziye Yuan", "Jianmin Wu", "Guangjun Xie", "Dou Shen"], "title": "Qianfan-VL: Domain-Enhanced Universal Vision-Language Models", "comment": "12 pages", "summary": "We present Qianfan-VL, a series of multimodal large language models ranging\nfrom 3B to 70B parameters, achieving state-of-the-art performance through\ninnovative domain enhancement techniques. Our approach employs multi-stage\nprogressive training and high-precision data synthesis pipelines, which prove\nto be critical technologies for enhancing domain-specific capabilities while\nmaintaining strong general performance. Qianfan-VL achieves comparable results\nto leading open-source models on general benchmarks, with state-of-the-art\nperformance on benchmarks such as CCBench, SEEDBench IMG, ScienceQA, and\nMMStar. The domain enhancement strategy delivers significant advantages in OCR\nand document understanding, validated on both public benchmarks (OCRBench 873,\nDocVQA 94.75%) and in-house evaluations. Notably, Qianfan-VL-8B and 70B\nvariants incorporate long chain-of-thought capabilities, demonstrating superior\nperformance on mathematical reasoning (MathVista 78.6%) and logical inference\ntasks. All models are trained entirely on Baidu's Kunlun P800 chips, validating\nthe capability of large-scale AI infrastructure to train SOTA-level multimodal\nmodels with over 90% scaling efficiency on 5000 chips for a single task. This\nwork establishes an effective methodology for developing domain-enhanced\nmultimodal models suitable for diverse enterprise deployment scenarios.", "AI": {"tldr": "Qianfan-VL是一系列3B到70B参数的多模态大语言模型，通过创新的领域增强技术，在保持强大通用性能的同时，实现了SOTA级的领域特定能力，特别在OCR、文档理解、数学推理和逻辑推理方面表现出色，并验证了大规模AI基础设施的训练能力。", "motivation": "开发具有强大通用性能和卓越领域特定能力的多模态大语言模型，以满足多样化的企业部署场景需求，并探索创新的领域增强技术和高效的大规模训练方法。", "method": "采用多阶段渐进式训练和高精度数据合成流水线，通过领域增强策略提升模型在特定领域的性能。部分模型（8B和70B变体）融入了长链式思考能力。所有模型均在百度昆仑P800芯片上进行训练，并验证了大规模AI基础设施的高效扩展能力。", "result": "Qianfan-VL在通用基准上与领先的开源模型结果相当，并在CCBench、SEEDBench IMG、ScienceQA和MMStar等基准上达到SOTA性能。在OCR和文档理解方面表现出显著优势（OCRBench 873%，DocVQA 94.75%）。8B和70B变体在数学推理（MathVista 78.6%）和逻辑推理任务上表现优异。模型在百度昆仑P800芯片上以超过90%的扩展效率（5000个芯片）成功训练，验证了其大规模AI基础设施的能力。", "conclusion": "这项工作为开发适用于各种企业部署场景的领域增强多模态模型建立了一种有效的方法论，并展示了大规模AI基础设施在训练SOTA级多模态模型方面的强大能力。"}}
{"id": "2509.19079", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.19079", "abs": "https://arxiv.org/abs/2509.19079", "authors": ["Samuel Chamoun", "Christian McDowell", "Robin Buchanan", "Kevin Chan", "Eric Graves", "Yin Sun"], "title": "MAPPO for Edge Server Monitoring", "comment": "6 pages, 4 figures. Accepted to IEEE MILCOM 2025", "summary": "In this paper, we consider a goal-oriented communication problem for edge\nserver monitoring, where jobs arrive intermittently at multiple dispatchers and\nmust be assigned to shared edge servers with finite queues and time-varying\navailability. Accurate knowledge of server status is critical for sustaining\nhigh throughput, yet remains challenging under dynamic workloads and partial\nobservability. To address this challenge, each dispatcher maintains server\nknowledge through two complementary mechanisms: (i) active status queries that\nprovide instantaneous updates at a communication cost, and (ii) job execution\nfeedback that reveals server conditions opportunistically. We formulate a\ncooperative multi-agent distributed decision-making problem in which\ndispatchers jointly optimize query scheduling to balance throughput against\ncommunication overhead. To solve this problem, we propose a Multi-Agent\nProximal Policy Optimization (MAPPO)-based algorithm that leverages centralized\ntraining with decentralized execution (CTDE) to learn distributed\nquery-and-dispatch policies under partial and stale observations. Numerical\nevaluations show that MAPPO achieves superior throughput-cost tradeoffs and\nsignificantly outperforms baseline strategies, achieving on average a 30%\nimprovement over the closest baseline.", "AI": {"tldr": "本文提出了一种基于MAPPO的分布式决策算法，用于边缘服务器监控中的查询调度与任务分派，通过结合主动查询和任务反馈，在动态工作负载下实现了吞吐量与通信开销的优异权衡。", "motivation": "在边缘服务器监控中，任务间歇性到达，需要分配给具有有限队列和时变可用性的共享服务器。准确的服务器状态知识对于维持高吞吐量至关重要，但在动态工作负载和部分可观测性下，获取准确状态具有挑战性。", "method": "每个调度器通过两种机制维护服务器知识：(i) 主动状态查询（有通信成本）和 (ii) 任务执行反馈（机会性揭示状态）。将此问题公式化为合作多智能体分布式决策问题，并提出了一种基于多智能体近端策略优化（MAPPO）的算法，该算法利用集中式训练和分散式执行（CTDE）来学习分布式查询和分派策略。", "result": "数值评估表明，MAPPO算法实现了卓越的吞吐量-成本权衡，显著优于基线策略，平均比最接近的基线提高了30%。", "conclusion": "所提出的MAPPO算法能有效解决边缘服务器监控中的查询调度与任务分派问题，在动态和部分可观测环境下，显著提升了系统性能，优化了吞吐量与通信开销之间的平衡。"}}
{"id": "2509.18395", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18395", "abs": "https://arxiv.org/abs/2509.18395", "authors": ["Minki Hong", "Jangho Choi", "Jihie Kim"], "title": "NormGenesis: Multicultural Dialogue Generation via Exemplar-Guided Social Norm Modeling and Violation Recovery", "comment": "39 pages, 17 figures, EMNLP 2025 Main Conference", "summary": "Social norms govern culturally appropriate behavior in communication,\nenabling dialogue systems to produce responses that are not only coherent but\nalso socially acceptable. We present NormGenesis, a multicultural framework for\ngenerating and annotating socially grounded dialogues across English, Chinese,\nand Korean. To model the dynamics of social interaction beyond static norm\nclassification, we propose a novel dialogue type, Violation-to-Resolution\n(V2R), which models the progression of conversations following norm violations\nthrough recognition and socially appropriate repair. To improve pragmatic\nconsistency in underrepresented languages, we implement an exemplar-based\niterative refinement early in the dialogue synthesis process. This design\nintroduces alignment with linguistic, emotional, and sociocultural expectations\nbefore full dialogue generation begins. Using this framework, we construct a\ndataset of 10,800 multi-turn dialogues annotated at the turn level for norm\nadherence, speaker intent, and emotional response. Human and LLM-based\nevaluations demonstrate that NormGenesis significantly outperforms existing\ndatasets in refinement quality, dialogue naturalness, and generalization\nperformance. We show that models trained on our V2R-augmented data exhibit\nimproved pragmatic competence in ethically sensitive contexts. Our work\nestablishes a new benchmark for culturally adaptive dialogue modeling and\nprovides a scalable methodology for norm-aware generation across linguistically\nand culturally diverse languages.", "AI": {"tldr": "NormGenesis是一个多文化框架，用于生成和标注跨英语、中文和韩语的社会化对话。它引入了V2R对话类型来建模规范违反后的修复过程，并通过早期迭代细化提高了语用一致性。该框架构建了一个大型数据集，并证明了其在对话质量和模型语用能力方面的显著提升。", "motivation": "对话系统需要生成不仅连贯而且符合社会规范的回复，尤其是在多文化背景下。现有方法难以捕捉社会互动的动态性（超越静态规范分类），并且在资源稀缺语言中存在语用一致性问题。", "method": "该研究提出了NormGenesis框架，用于生成和标注英语、中文和韩语的社会化对话。它引入了“违规到解决”（V2R）这一新颖对话类型，以建模规范违反后的识别和修复过程。为提高语用一致性，在对话合成早期采用基于范例的迭代细化方法，以确保语言、情感和社会文化期望的一致性。基于此框架，构建了包含10,800个多轮对话的数据集，并对每轮对话进行了规范依从性、说话者意图和情感反应的标注。", "result": "人类和LLM评估表明，NormGenesis在细化质量、对话自然度和泛化性能方面显著优于现有数据集。在V2R增强数据上训练的模型在伦理敏感语境中表现出改进的语用能力。", "conclusion": "这项工作为文化适应性对话建模建立了新的基准，并为跨语言和文化多样性的规范感知生成提供了一种可扩展的方法。"}}
{"id": "2509.18506", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18506", "abs": "https://arxiv.org/abs/2509.18506", "authors": ["Siyuan Yu", "Congkai Shen", "Yufei Xi", "James Dallas", "Michael Thompson", "John Subosits", "Hiroshi Yasuda", "Tulga Ersal"], "title": "Spatial Envelope MPC: High Performance Driving without a Reference", "comment": null, "summary": "This paper presents a novel envelope based model predictive control (MPC)\nframework designed to enable autonomous vehicles to handle high performance\ndriving across a wide range of scenarios without a predefined reference. In\nhigh performance autonomous driving, safe operation at the vehicle's dynamic\nlimits requires a real time planning and control framework capable of\naccounting for key vehicle dynamics and environmental constraints when\nfollowing a predefined reference trajectory is suboptimal or even infeasible.\nState of the art planning and control frameworks, however, are predominantly\nreference based, which limits their performance in such situations. To address\nthis gap, this work first introduces a computationally efficient vehicle\ndynamics model tailored for optimization based control and a continuously\ndifferentiable mathematical formulation that accurately captures the entire\ndrivable envelope. This novel model and formulation allow for the direct\nintegration of dynamic feasibility and safety constraints into a unified\nplanning and control framework, thereby removing the necessity for predefined\nreferences. The challenge of envelope planning, which refers to maximally\napproximating the safe drivable area, is tackled by combining reinforcement\nlearning with optimization techniques. The framework is validated through both\nsimulations and real world experiments, demonstrating its high performance\nacross a variety of tasks, including racing, emergency collision avoidance and\noff road navigation. These results highlight the framework's scalability and\nbroad applicability across a diverse set of scenarios.", "AI": {"tldr": "本文提出了一种新颖的基于包络线的模型预测控制（MPC）框架，使自动驾驶车辆能够在没有预定义参考的情况下，在各种高难度场景中实现高性能驾驶。", "motivation": "在高性自动驾驶中，车辆在动态极限下安全运行需要实时规划和控制框架，该框架能够考虑关键车辆动力学和环境约束，因为预定义参考轨迹在这种情况下是次优甚至不可行的。然而，现有的规划和控制框架大多是基于参考的，这限制了它们在这种情况下的性能。", "method": "该研究首先引入了一种计算高效的车辆动力学模型，专为基于优化的控制设计，并提出了一种连续可微的数学公式，精确捕捉整个可行驶包络线。这些创新模型和公式允许将动态可行性和安全约束直接集成到一个统一的规划和控制框架中，从而消除了对预定义参考的需求。通过结合强化学习和优化技术来解决包络线规划（即最大程度地逼近安全可行驶区域）的挑战。", "result": "该框架通过仿真和真实世界实验进行了验证，在赛车、紧急避撞和越野导航等各种任务中表现出高性能。这些结果突显了该框架的可扩展性和在不同场景中的广泛适用性。", "conclusion": "该框架能够让自动驾驶车辆在各种场景下实现高性能驾驶，而无需预设参考，展现出卓越的性能、可扩展性和广泛的适用性。"}}
{"id": "2509.18226", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18226", "abs": "https://arxiv.org/abs/2509.18226", "authors": ["Yu Fu", "Linyue Cai", "Ruoyu Wu", "Yong Zhao"], "title": "From \"What to Eat?\" to Perfect Recipe: ChefMind's Chain-of-Exploration for Ambiguous User Intent in Recipe Recommendation", "comment": "5 pages, 3 figures, submitted to icassp 2026", "summary": "Personalized recipe recommendation faces challenges in handling fuzzy user\nintent, ensuring semantic accuracy, and providing sufficient detail coverage.\nWe propose ChefMind, a hybrid architecture combining Chain of Exploration\n(CoE), Knowledge Graph (KG), Retrieval-Augmented Generation (RAG), and a Large\nLanguage Model (LLM). CoE refines ambiguous queries into structured conditions,\nKG offers semantic reasoning and interpretability, RAG supplements contextual\nculinary details, and LLM integrates outputs into coherent recommendations. We\nevaluate ChefMind on the Xiachufang dataset and manually annotated queries,\ncomparing it with LLM-only, KG-only, and RAG-only baselines. Results show that\nChefMind achieves superior performance in accuracy, relevance, completeness,\nand clarity, with an average score of 8.7 versus 6.4-6.7 for ablation models.\nMoreover, it reduces unprocessed queries to 1.6%, demonstrating robustness in\nhandling fuzzy demands.", "AI": {"tldr": "ChefMind是一种结合CoE、KG、RAG和LLM的混合架构，旨在解决个性化食谱推荐中模糊用户意图、语义准确性和细节覆盖不足的问题，并在各项指标上显著优于基线模型。", "motivation": "个性化食谱推荐面临处理模糊用户意图、确保语义准确性以及提供足够细节覆盖的挑战。", "method": "本文提出了ChefMind，一个混合架构，结合了探索链（CoE）用于细化模糊查询，知识图谱（KG）提供语义推理和可解释性，检索增强生成（RAG）补充上下文烹饪细节，以及大型语言模型（LLM）整合输出。该方法在下厨房数据集和手动标注查询上进行了评估，并与仅LLM、仅KG和仅RAG的基线模型进行了比较。", "result": "ChefMind在准确性、相关性、完整性和清晰度方面表现优异，平均得分8.7，而消融模型为6.4-6.7。此外，它将未处理的查询减少到1.6%，显示出处理模糊需求的鲁棒性。", "conclusion": "ChefMind能够有效处理模糊用户需求，提供高质量的个性化食谱推荐，并在多项性能指标上超越现有基线模型。"}}
{"id": "2509.18190", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18190", "abs": "https://arxiv.org/abs/2509.18190", "authors": ["Junseong Shin", "Seungwoo Chung", "Yunjeong Yang", "Tae Hyun Kim"], "title": "HazeFlow: Revisit Haze Physical Model as ODE and Non-Homogeneous Haze Generation for Real-World Dehazing", "comment": null, "summary": "Dehazing involves removing haze or fog from images to restore clarity and\nimprove visibility by estimating atmospheric scattering effects. While deep\nlearning methods show promise, the lack of paired real-world training data and\nthe resulting domain gap hinder generalization to real-world scenarios. In this\ncontext, physics-grounded learning becomes crucial; however, traditional\nmethods based on the Atmospheric Scattering Model (ASM) often fall short in\nhandling real-world complexities and diverse haze patterns. To solve this\nproblem, we propose HazeFlow, a novel ODE-based framework that reformulates ASM\nas an ordinary differential equation (ODE). Inspired by Rectified Flow (RF),\nHazeFlow learns an optimal ODE trajectory to map hazy images to clean ones,\nenhancing real-world dehazing performance with only a single inference step.\nAdditionally, we introduce a non-homogeneous haze generation method using\nMarkov Chain Brownian Motion (MCBM) to address the scarcity of paired\nreal-world data. By simulating realistic haze patterns through MCBM, we enhance\nthe adaptability of HazeFlow to diverse real-world scenarios. Through extensive\nexperiments, we demonstrate that HazeFlow achieves state-of-the-art performance\nacross various real-world dehazing benchmark datasets.", "AI": {"tldr": "HazeFlow是一个基于ODE的去雾框架，将大气散射模型（ASM）重构为ODE，通过学习最优ODE轨迹将雾霾图像映射到清晰图像。它还引入了基于马尔可夫链布朗运动（MCBM）的非均匀雾霾生成方法，以解决真实世界配对数据稀缺问题，并在多个真实世界去雾基准测试中达到了最先进的性能。", "motivation": "深度学习去雾方法面临真实世界配对训练数据不足和领域鸿沟问题，导致泛化能力受限。同时，传统基于物理学的大气散射模型（ASM）难以处理真实世界的复杂性和多样化的雾霾模式。", "method": "本文提出了HazeFlow，一个新颖的基于ODE的框架，将大气散射模型（ASM）重新表述为常微分方程（ODE）。受Rectified Flow启发，HazeFlow学习一个最优的ODE轨迹，以单步推理将雾霾图像映射到清晰图像。此外，为了解决真实世界配对数据稀缺问题，引入了使用马尔可夫链布朗运动（MCBM）的非均匀雾霾生成方法，以模拟逼真的雾霾模式。", "result": "通过广泛的实验，HazeFlow在各种真实世界去雾基准数据集上均实现了最先进的性能。", "conclusion": "HazeFlow通过将ASM重构为ODE并结合创新的数据生成方法，有效解决了真实世界去雾的挑战，显著提升了去雾效果和对多样化真实场景的适应性。"}}
{"id": "2509.19107", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.19107", "abs": "https://arxiv.org/abs/2509.19107", "authors": ["Khan Masood Parvez", "Sk Md Abidar Rahaman", "Ali Shiri Sichani", "Hadi AliAkbarpour"], "title": "AI-Enabled Smart Hygiene System for Real-Time Glucose Detection", "comment": null, "summary": "This research presents a smart urinary health monitoring system incorporating\na coplanar waveguide (CPW)-fed slot-loop antenna biosensor designed to analyse\nvarious urine samples. The antenna demonstrates distinct resonant frequency\nshifts when exposed to five specific urine conditions, deviating from its\nbaseline 1.42 GHz operation. These measurable frequency variations enable the\nantenna to function as an effective microwave sensor for urinary biomarker\ndetection. A potential artificial intelligence-based Convolutional Neural\nNetworks Long Short-Term Memory (CNN-LSTM) framework is also discussed to\novercome the limitations of overlapping frequency responses, aiming to improve\nthe accuracy of health condition detection. These components contribute to the\ndevelopment of a smart toilet system that displays real-time health information\non a wall-mounted urinal screen, without requiring any user effort or\nbehavioural change.", "AI": {"tldr": "本研究提出了一种基于共面波导槽环天线生物传感器的智能泌尿健康监测系统，通过测量尿液样本引起的谐振频率偏移来检测泌尿生物标志物，并讨论了结合CNN-LSTM框架以提高检测准确性，最终应用于智能马桶系统实现无感实时健康监测。", "motivation": "开发一种无需用户努力或行为改变，能够实时、准确监测泌尿健康的系统，并解决传统方法中频率响应重叠导致的检测局限性。", "method": "该研究采用了一种共面波导 (CPW) 馈电的槽环天线生物传感器，用于分析各种尿液样本。该天线通过检测尿液条件引起的谐振频率偏移（偏离基准1.42 GHz）来识别泌尿生物标志物。此外，研究还讨论了一个潜在的基于人工智能的卷积神经网络长短期记忆 (CNN-LSTM) 框架，旨在克服频率响应重叠的限制，从而提高健康状况检测的准确性。这些组件将整合到一个智能马桶系统中，在壁挂式小便器屏幕上显示实时健康信息。", "result": "该天线在暴露于五种特定尿液条件下时，表现出明显的谐振频率偏移，偏离其基线1.42 GHz的运行频率。这些可测量的频率变化使得天线能够作为一种有效的微波传感器，用于泌尿生物标志物的检测。", "conclusion": "这些组件的结合有助于开发一个智能马桶系统，该系统能够在不要求用户付出任何努力或改变行为的情况下，在壁挂式小便器屏幕上显示实时健康信息，从而实现高效、准确的泌尿健康监测，并有望通过AI框架进一步提升检测精度。"}}
{"id": "2509.18401", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18401", "abs": "https://arxiv.org/abs/2509.18401", "authors": ["Armin Tourajmehr", "Mohammad Reza Modarres", "Yadollah Yaghoobzadeh"], "title": "Evaluating the Creativity of LLMs in Persian Literary Text Generation", "comment": null, "summary": "Large language models (LLMs) have demonstrated notable creative abilities in\ngenerating literary texts, including poetry and short stories. However, prior\nresearch has primarily centered on English, with limited exploration of\nnon-English literary traditions and without standardized methods for assessing\ncreativity. In this paper, we evaluate the capacity of LLMs to generate Persian\nliterary text enriched with culturally relevant expressions. We build a dataset\nof user-generated Persian literary spanning 20 diverse topics and assess model\noutputs along four creativity dimensions-originality, fluency, flexibility, and\nelaboration-by adapting the Torrance Tests of Creative Thinking. To reduce\nevaluation costs, we adopt an LLM as a judge for automated scoring and validate\nits reliability against human judgments using intraclass correlation\ncoefficients, observing strong agreement. In addition, we analyze the models'\nability to understand and employ four core literary devices: simile, metaphor,\nhyperbole, and antithesis. Our results highlight both the strengths and\nlimitations of LLMs in Persian literary text generation, underscoring the need\nfor further refinement.", "AI": {"tldr": "本文评估了大型语言模型（LLMs）生成波斯语文学文本的能力，通过构建新数据集、采用适应性的创造力评估维度和使用LLM作为自动评分员，揭示了LLMs在此领域的优缺点。", "motivation": "现有研究主要关注LLMs在英语文学文本生成中的创造力，对非英语文学传统（特别是波斯语）的探索有限，且缺乏标准化的创造力评估方法。", "method": "研究构建了一个包含20个主题的用户生成波斯语文学文本数据集；通过改编托兰斯创造性思维测验，从原创性、流畅性、灵活性和精细化四个维度评估模型输出；采用LLM作为自动评分员以降低评估成本，并使用组内相关系数验证其与人类判断的一致性；此外，还分析了模型理解和运用四种核心文学修辞（明喻、暗喻、夸张、对偶）的能力。", "result": "研究结果突出了LLMs在波斯语文学文本生成方面的优势和局限性。", "conclusion": "LLMs在波斯语文学文本生成方面仍需进一步改进和完善。"}}
{"id": "2509.18576", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18576", "abs": "https://arxiv.org/abs/2509.18576", "authors": ["Zeyi Kang", "Liang He", "Yanxin Zhang", "Zuheng Ming", "Kaixing Zhao"], "title": "LCMF: Lightweight Cross-Modality Mambaformer for Embodied Robotics VQA", "comment": null, "summary": "Multimodal semantic learning plays a critical role in embodied intelligence,\nespecially when robots perceive their surroundings, understand human\ninstructions, and make intelligent decisions. However, the field faces\ntechnical challenges such as effective fusion of heterogeneous data and\ncomputational efficiency in resource-constrained environments. To address these\nchallenges, this study proposes the lightweight LCMF cascaded attention\nframework, introducing a multi-level cross-modal parameter sharing mechanism\ninto the Mamba module. By integrating the advantages of Cross-Attention and\nSelective parameter-sharing State Space Models (SSMs), the framework achieves\nefficient fusion of heterogeneous modalities and semantic complementary\nalignment. Experimental results show that LCMF surpasses existing multimodal\nbaselines with an accuracy of 74.29% in VQA tasks and achieves competitive\nmid-tier performance within the distribution cluster of Large Language Model\nAgents (LLM Agents) in EQA video tasks. Its lightweight design achieves a\n4.35-fold reduction in FLOPs relative to the average of comparable baselines\nwhile using only 166.51M parameters (image-text) and 219M parameters\n(video-text), providing an efficient solution for Human-Robot Interaction (HRI)\napplications in resource-constrained scenarios with strong multimodal decision\ngeneralization capabilities.", "AI": {"tldr": "本研究提出了一种轻量级LCMF级联注意力框架，通过在Mamba模块中引入多级跨模态参数共享机制，有效融合异构数据并提高计算效率，在资源受限的人机交互场景中表现出强大的多模态决策泛化能力。", "motivation": "多模态语义学习在具身智能中至关重要，但面临异构数据有效融合和资源受限环境中计算效率的技术挑战。", "method": "本研究提出了轻量级LCMF级联注意力框架。该框架将多级跨模态参数共享机制引入Mamba模块，并结合了交叉注意力（Cross-Attention）和选择性参数共享状态空间模型（SSMs）的优势，以实现异构模态的高效融合和语义互补对齐。", "result": "实验结果显示，LCMF在VQA任务中超越现有基线，准确率达到74.29%；在EQA视频任务中，其性能在大型语言模型代理（LLM Agents）的分布集群中达到有竞争力的中等水平。其轻量化设计使FLOPs比可比较基线平均降低了4.35倍，参数量仅为166.51M（图像-文本）和219M（视频-文本）。", "conclusion": "LCMF框架为资源受限场景下的人机交互（HRI）应用提供了一个高效的解决方案，具有强大的多模态决策泛化能力。"}}
{"id": "2509.18229", "categories": ["cs.AI", "70, 74, 76, 80"], "pdf": "https://arxiv.org/pdf/2509.18229", "abs": "https://arxiv.org/abs/2509.18229", "authors": ["Anthony Patera", "Rohan Abeyaratne"], "title": "An N-Plus-1 GPT Agency for Critical Solution of Mechanical Engineering Analysis Problems", "comment": null, "summary": "Generative AI, and specifically GPT, can produce a remarkable solution to a\nmechanical engineering analysis problem - but also, on occasion, a flawed\nsolution. For example, an elementary mechanics problem is solved flawlessly in\none GPT instance and incorrectly in a subsequent GPT instance, with a success\nprobability of only 85%. This unreliability renders \"out-of-the-box\" GPT\nunsuitable for deployment in education or engineering practice. We introduce an\n\"N-Plus-1\" GPT Agency for Initial (Low-Cost) Analysis of mechanical engineering\nProblem Statements. Agency first launches N instantiations of Agent Solve to\nyield N independent Proposed Problem Solution Realizations; Agency then invokes\nAgent Compare to summarize and compare the N Proposed Problem Solution\nRealizations and to provide a Recommended Problem Solution. We argue from\nCondorcet's Jury Theorem that, for a Problem Statement characterized by\nper-Solve success probability greater than 1/2 (and N sufficiently large), the\nPredominant (Agent Compare) Proposed Problem Solution will, with high\nprobability, correspond to a Correct Proposed Problem Solution. Furthermore,\nAgent Compare can also incorporate aspects of Secondary (Agent Compare)\nProposed Problem Solutions, in particular when the latter represent alternative\nProblem Statement interpretations - different Mathematical Models - or\nalternative Mathematical Solution Procedures. Comparisons to Grok Heavy, a\ncommercial multi-agent model, show similarities in design and performance, but\nalso important differences in emphasis: our Agency focuses on transparency and\npedagogical value.", "AI": {"tldr": "本研究提出了一种“N-加-1”GPT代理机构，通过聚合多个GPT实例的解决方案来提高生成式AI在机械工程分析问题上的可靠性，以克服单个GPT实例的不稳定性。", "motivation": "生成式AI（尤其是GPT）在解决机械工程分析问题时，有时能给出卓越的解决方案，但有时也会出错（例如，成功率仅为85%）。这种不可靠性使得“开箱即用”的GPT不适合在教育或工程实践中部署。", "method": "引入了一个“N-加-1”GPT代理机构：首先启动N个“Agent Solve”实例以获得N个独立的解决方案；然后调用“Agent Compare”来总结和比较这N个解决方案，并提供一个推荐解决方案。该方法依据孔多塞陪审团定理，认为对于单次求解成功概率大于1/2的问题（且N足够大），多数（Agent Compare）提出的解决方案将以高概率是正确的。此外，Agent Compare还能整合次要解决方案的方面，特别是当它们代表不同的问题解释、数学模型或数学求解程序时。", "result": "通过“N-加-1”代理机构，对于单次求解成功概率大于1/2的问题，主要推荐的解决方案将以高概率是正确的。与商业多代理模型Grok Heavy相比，本机构在设计和性能上相似，但在透明度和教学价值方面有重要区别和侧重。", "conclusion": "所提出的“N-加-1”GPT代理机构能够显著提高GPT在机械工程分析问题上的可靠性，使其更适合在教育和工程实践中部署，并强调了透明度和教学价值。"}}
{"id": "2509.18193", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18193", "abs": "https://arxiv.org/abs/2509.18193", "authors": ["Omar H. Khater", "Abdul Jabbar Siddiqui", "Aiman El-Maleh", "M. Shamim Hossain"], "title": "TinyEcoWeedNet: Edge Efficient Real-Time Aerial Agricultural Weed Detection", "comment": null, "summary": "Deploying deep learning models in agriculture is difficult because edge\ndevices have limited resources, but this work presents a compressed version of\nEcoWeedNet using structured channel pruning, quantization-aware training (QAT),\nand acceleration with NVIDIA's TensorRT on the Jetson Orin Nano. Despite the\nchallenges of pruning complex architectures with residual shortcuts, attention\nmechanisms, concatenations, and CSP blocks, the model size was reduced by up to\n68.5% and computations by 3.2 GFLOPs, while inference speed reached 184 FPS at\nFP16, 28.7% faster than the baseline. On the CottonWeedDet12 dataset, the\npruned EcoWeedNet with a 39.5% pruning ratio outperformed YOLO11n and YOLO12n\n(with only 20% pruning), achieving 83.7% precision, 77.5% recall, and 85.9%\nmAP50, proving it to be both efficient and effective for precision agriculture.", "AI": {"tldr": "该研究通过结构化通道剪枝、量化感知训练和TensorRT加速，成功将EcoWeedNet模型压缩并部署到边缘设备上，显著减小了模型大小和计算量，同时提高了推理速度和精度，使其适用于精准农业。", "motivation": "在农业领域部署深度学习模型面临边缘设备资源有限的挑战。", "method": "采用结构化通道剪枝、量化感知训练（QAT）以及使用NVIDIA TensorRT在Jetson Orin Nano上进行加速。该方法解决了剪枝包含残差快捷连接、注意力机制、拼接和CSP块等复杂架构的挑战。", "result": "模型大小减小了高达68.5%，计算量减少了3.2 GFLOPs。推理速度在FP16下达到184 FPS，比基线快28.7%。在CottonWeedDet12数据集上，剪枝率为39.5%的EcoWeedNet表现优于YOLO11n和YOLO12n（仅20%剪枝），实现了83.7%的精确度、77.5%的召回率和85.9%的mAP50。", "conclusion": "经验证，该压缩版的EcoWeedNet模型在精准农业应用中既高效又有效。"}}
{"id": "2509.19110", "categories": ["eess.SY", "cs.LG", "cs.RO", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.19110", "abs": "https://arxiv.org/abs/2509.19110", "authors": ["Chenxu Ke", "Congling Tian", "Kaichen Xu", "Ye Li", "Lingcong Bao"], "title": "A Fast Initialization Method for Neural Network Controllers: A Case Study of Image-based Visual Servoing Control for the multicopter Interception", "comment": null, "summary": "Reinforcement learning-based controller design methods often require\nsubstantial data in the initial training phase. Moreover, the training process\ntends to exhibit strong randomness and slow convergence. It often requires\nconsiderable time or high computational resources. Another class of\nlearning-based method incorporates Lyapunov stability theory to obtain a\ncontrol policy with stability guarantees. However, these methods generally\nrequire an initially stable neural network control policy at the beginning of\ntraining. Evidently, a stable neural network controller can not only serve as\nan initial policy for reinforcement learning, allowing the training to focus on\nimproving controller performance, but also act as an initial state for\nlearning-based Lyapunov control methods. Although stable controllers can be\ndesigned using traditional control theory, designers still need to have a great\ndeal of control design knowledge to address increasingly complicated control\nproblems. The proposed neural network rapid initialization method in this paper\nachieves the initial training of the neural network control policy by\nconstructing datasets that conform to the stability conditions based on the\nsystem model. Furthermore, using the image-based visual servoing control for\nmulticopter interception as a case study, simulations and experiments were\nconducted to validate the effectiveness and practical performance of the\nproposed method. In the experiment, the trained control policy attains a final\ninterception velocity of 15 m/s.", "AI": {"tldr": "本文提出了一种基于系统模型构建符合稳定性条件数据集的神经网络快速初始化方法，用于稳定控制策略的初始训练，并通过多旋翼拦截案例进行了验证。", "motivation": "现有的强化学习控制器设计方法需要大量数据、训练随机且收敛慢；基于Lyapunov的控制方法需要初始稳定的神经网络策略。一个稳定的初始神经网络控制器可以改善这两种方法的训练效率和性能。传统控制理论在处理复杂问题时需要丰富的专业知识。", "method": "本文提出了一种神经网络快速初始化方法。该方法通过根据系统模型构建符合稳定性条件的数据集，实现神经网络控制策略的初始训练。", "result": "通过对多旋翼拦截的图像视觉伺服控制进行仿真和实验，验证了所提出方法的有效性和实际性能。在实验中，训练后的控制策略实现了15米/秒的最终拦截速度。", "conclusion": "所提出的神经网络快速初始化方法能够有效生成稳定的初始控制策略，从而提高基于学习的控制方法的训练效率和性能，减少对专家知识的依赖。"}}
{"id": "2509.18439", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18439", "abs": "https://arxiv.org/abs/2509.18439", "authors": ["Oscar J. Ponce-Ponte", "David Toro-Tobon", "Luis F. Figueroa", "Michael Gionfriddo", "Megan Branda", "Victor M. Montori", "Saturnino Luz", "Juan P. Brito"], "title": "Developing an AI framework to automatically detect shared decision-making in patient-doctor conversations", "comment": "53 pages, 1 figure, 4 tables, 5 supplementary figures, 13\n  supplementary tables", "summary": "Shared decision-making (SDM) is necessary to achieve patient-centred care.\nCurrently no methodology exists to automatically measure SDM at scale. This\nstudy aimed to develop an automated approach to measure SDM by using language\nmodelling and the conversational alignment (CA) score. A total of 157\nvideo-recorded patient-doctor conversations from a randomized multi-centre\ntrial evaluating SDM decision aids for anticoagulation in atrial fibrillations\nwere transcribed and segmented into 42,559 sentences. Context-response pairs\nand negative sampling were employed to train deep learning (DL) models and\nfine-tuned BERT models via the next sentence prediction (NSP) task. Each\ntop-performing model was used to calculate four types of CA scores. A\nrandom-effects analysis by clinician, adjusting for age, sex, race, and trial\narm, assessed the association between CA scores and SDM outcomes: the\nDecisional Conflict Scale (DCS) and the Observing Patient Involvement in\nDecision-Making 12 (OPTION12) scores. p-values were corrected for multiple\ncomparisons with the Benjamini-Hochberg method. Among 157 patients (34% female,\nmean age 70 SD 10.8), clinicians on average spoke more words than patients\n(1911 vs 773). The DL model without the stylebook strategy achieved a recall@1\nof 0.227, while the fine-tuned BERTbase (110M) achieved the highest recall@1\nwith 0.640. The AbsMax (18.36 SE7.74 p=0.025) and Max CA (21.02 SE7.63 p=0.012)\nscores generated with the DL without stylebook were associated with OPTION12.\nThe Max CA score generated with the fine-tuned BERTbase (110M) was associated\nwith the DCS score (-27.61 SE12.63 p=0.037). BERT model sizes did not have an\nimpact the association between CA scores and SDM. This study introduces an\nautomated, scalable methodology to measure SDM in patient-doctor conversations\nthrough explainable CA scores, with potential to evaluate SDM strategies at\nscale.", "AI": {"tldr": "本研究开发了一种自动、可扩展的方法，利用语言模型和对话对齐（CA）分数来衡量医患对话中的共享决策（SDM），并发现CA分数与SDM结果相关。", "motivation": "共享决策（SDM）对于实现以患者为中心的护理至关重要，但目前缺乏一种能够大规模自动衡量SDM的方法。", "method": "研究使用了157个视频录制的医患对话（共42,559句话），通过上下文-响应对和负采样训练了深度学习（DL）模型和微调的BERT模型，以执行下一句预测（NSP）任务。然后，使用表现最佳的模型计算四种类型的对话对齐（CA）分数。通过调整年龄、性别、种族和试验组的随机效应分析，评估CA分数与SDM结果（决策冲突量表DCS和观察患者参与决策量表OPTION12）之间的关联，并使用Benjamini-Hochberg方法校正p值。", "result": "在157名患者中，临床医生平均比患者说更多的话。微调的BERTbase（110M）模型在recall@1上表现最佳（0.640），优于深度学习模型（0.227）。深度学习模型生成的AbsMax和Max CA分数与OPTION12评分相关联。微调的BERTbase（110M）模型生成的Max CA分数与DCS评分相关联。BERT模型的大小对CA分数与SDM之间的关联没有影响。", "conclusion": "本研究引入了一种自动化、可扩展且可解释的方法，通过对话对齐分数来衡量医患对话中的共享决策，这为大规模评估SDM策略提供了潜力。"}}
{"id": "2509.18592", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.18592", "abs": "https://arxiv.org/abs/2509.18592", "authors": ["Neel P. Bhatt", "Yunhao Yang", "Rohan Siva", "Pranay Samineni", "Daniel Milan", "Zhangyang Wang", "Ufuk Topcu"], "title": "VLN-Zero: Rapid Exploration and Cache-Enabled Neurosymbolic Vision-Language Planning for Zero-Shot Transfer in Robot Navigation", "comment": "Codebase, datasets, and videos for VLN-Zero are available at:\n  https://vln-zero.github.io/", "summary": "Rapid adaptation in unseen environments is essential for scalable real-world\nautonomy, yet existing approaches rely on exhaustive exploration or rigid\nnavigation policies that fail to generalize. We present VLN-Zero, a two-phase\nvision-language navigation framework that leverages vision-language models to\nefficiently construct symbolic scene graphs and enable zero-shot neurosymbolic\nnavigation. In the exploration phase, structured prompts guide VLM-based search\ntoward informative and diverse trajectories, yielding compact scene graph\nrepresentations. In the deployment phase, a neurosymbolic planner reasons over\nthe scene graph and environmental observations to generate executable plans,\nwhile a cache-enabled execution module accelerates adaptation by reusing\npreviously computed task-location trajectories. By combining rapid exploration,\nsymbolic reasoning, and cache-enabled execution, the proposed framework\novercomes the computational inefficiency and poor generalization of prior\nvision-language navigation methods, enabling robust and scalable\ndecision-making in unseen environments. VLN-Zero achieves 2x higher success\nrate compared to state-of-the-art zero-shot models, outperforms most fine-tuned\nbaselines, and reaches goal locations in half the time with 55% fewer VLM calls\non average compared to state-of-the-art models across diverse environments.\nCodebase, datasets, and videos for VLN-Zero are available at:\nhttps://vln-zero.github.io/.", "AI": {"tldr": "VLN-Zero是一种两阶段的视觉-语言导航框架，它利用视觉-语言模型（VLM）构建符号场景图，并通过神经符号规划和缓存执行模块实现零样本导航，显著提升了在未知环境中的成功率和效率。", "motivation": "现有方法依赖详尽探索或僵化导航策略，在未知环境中难以实现快速适应和泛化，这对于可扩展的真实世界自主性至关重要。", "method": "VLN-Zero分为两阶段：1) 探索阶段：利用结构化提示引导VLM进行搜索，构建紧凑的符号场景图。2) 部署阶段：神经符号规划器基于场景图和环境观测生成可执行计划，同时缓存执行模块通过重用先前计算的任务-位置轨迹来加速适应。", "result": "VLN-Zero在成功率上比最先进的零样本模型高出2倍，超越了大多数微调基线，并且在多样化环境中以一半的时间和平均少55%的VLM调用次数到达目标位置。", "conclusion": "该框架结合了快速探索、符号推理和缓存执行，克服了现有视觉-语言导航方法的计算效率低下和泛化能力差的问题，实现了在未知环境中鲁棒且可扩展的决策制定。"}}
{"id": "2509.18230", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18230", "abs": "https://arxiv.org/abs/2509.18230", "authors": ["Zihan Dong", "Xinyu Fan", "Zixiang Tang", "Yunqing Li"], "title": "Towards General Computer Control with Hierarchical Agents and Multi-Level Action Spaces", "comment": null, "summary": "Controlling desktop applications via software remains a fundamental yet\nunder-served problem. Existing multi-modal large language models (MLLMs) ingest\nscreenshots and task instructions to generate keystrokes and mouse events, but\nthey suffer from prohibitive inference latency, poor sample efficiency on\nlong-horizon sparse-reward tasks, and infeasible on-device deployment. We\nintroduce a lightweight hierarchical reinforcement learning framework,\nComputerAgent, that formulates OS control as a two-level option process\n(manager and subpolicy), employs a triple-modal state encoder (screenshot, task\nID, numeric state) to handle visual and contextual diversity, integrates\nmeta-actions with an early-stop mechanism to reduce wasted interactions, and\nuses a compact vision backbone plus small policy networks for on-device\ninference (15M parameters). On a suite of 135 real-world desktop tasks,\nComputerAgent attains 92.1% success on simple tasks (<8 steps) and 58.8% on\nhard tasks (>=8 steps), matching or exceeding 200B-parameter MLLM baselines on\nsimple scenarios while reducing model size by over four orders of magnitude and\nhalving inference time. These results demonstrate that hierarchical RL offers a\npractical, scalable alternative to monolithic MLLM-based automation for\ncomputer control.", "AI": {"tldr": "本文提出了一种轻量级分层强化学习框架ComputerAgent，用于控制桌面应用程序。它通过分层策略、三重模态状态编码、元动作和紧凑模型设计，在保持高成功率的同时，大幅降低了模型规模和推理延迟，为桌面自动化提供了一个可扩展的替代方案。", "motivation": "现有多模态大型语言模型（MLLMs）在控制桌面应用程序时面临高推理延迟、长序列稀疏奖励任务的样本效率低下以及难以在设备上部署等问题。", "method": "ComputerAgent采用以下方法：1) 将操作系统控制建模为两级选项过程（管理器和子策略）的分层强化学习框架。2) 使用三重模态状态编码器（屏幕截图、任务ID、数字状态）来处理视觉和上下文多样性。3) 整合带有提前停止机制的元动作以减少无效交互。4) 采用紧凑的视觉骨干网络和小型策略网络（15M参数）实现设备上推理。", "result": "在135个真实桌面任务的测试中，ComputerAgent在简单任务（<8步）上达到92.1%的成功率，在困难任务（>=8步）上达到58.8%的成功率。在简单场景下，其性能与200B参数的MLLM基线相当或超越，同时将模型大小减少了四个数量级以上，并将推理时间减半。", "conclusion": "这些结果表明，分层强化学习为计算机控制中的整体式MLLM自动化提供了一个实用、可扩展的替代方案。"}}
{"id": "2509.18284", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18284", "abs": "https://arxiv.org/abs/2509.18284", "authors": ["Yi Gu", "Kuniaki Saito", "Jiaxin Ma"], "title": "Learning Contrastive Multimodal Fusion with Improved Modality Dropout for Disease Detection and Prediction", "comment": "MICCAI 2025", "summary": "As medical diagnoses increasingly leverage multimodal data, machine learning\nmodels are expected to effectively fuse heterogeneous information while\nremaining robust to missing modalities. In this work, we propose a novel\nmultimodal learning framework that integrates enhanced modalities dropout and\ncontrastive learning to address real-world limitations such as modality\nimbalance and missingness. Our approach introduces learnable modality tokens\nfor improving missingness-aware fusion of modalities and augments conventional\nunimodal contrastive objectives with fused multimodal representations. We\nvalidate our framework on large-scale clinical datasets for disease detection\nand prediction tasks, encompassing both visual and tabular modalities.\nExperimental results demonstrate that our method achieves state-of-the-art\nperformance, particularly in challenging and practical scenarios where only a\nsingle modality is available. Furthermore, we show its adaptability through\nsuccessful integration with a recent CT foundation model. Our findings\nhighlight the effectiveness, efficiency, and generalizability of our approach\nfor multimodal learning, offering a scalable, low-cost solution with\nsignificant potential for real-world clinical applications. The code is\navailable at https://github.com/omron-sinicx/medical-modality-dropout.", "AI": {"tldr": "本文提出了一种新颖的多模态学习框架，通过增强的模态丢弃和对比学习，有效处理医学诊断中模态不平衡和缺失问题，实现了最先进的性能，特别是在仅有单一模态的挑战性场景下。", "motivation": "随着医学诊断越来越多地利用多模态数据，机器学习模型需要有效融合异构信息，同时对缺失模态保持鲁棒性，以应对现实世界中的模态不平衡和缺失等限制。", "method": "该方法引入了一个新颖的多模态学习框架，集成了增强的模态丢弃和对比学习。它引入了可学习的模态令牌，以改进对缺失模态的融合感知，并通过融合的多模态表示来增强传统的单模态对比目标。", "result": "该框架在大型临床数据集上（包括视觉和表格模态）进行了疾病检测和预测任务的验证。实验结果表明，该方法实现了最先进的性能，尤其是在仅有单一模态的挑战性实际场景中。此外，它还成功地与最近的CT基础模型集成，展示了其适应性。", "conclusion": "研究结果强调了该方法在多模态学习中的有效性、效率和泛化能力，提供了一个可扩展、低成本的解决方案，在实际临床应用中具有巨大潜力。"}}
{"id": "2509.19111", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.19111", "abs": "https://arxiv.org/abs/2509.19111", "authors": ["Michael Ruderman", "Elia Brescia", "Paolo Roberto Massenio", "Giuseppe Leonardo Cascella", "David Naso"], "title": "Robust Synchronous Reference Frame Phase-Looked Loop (PLL) with Feed-Forward Frequency Estimation", "comment": "8 pages, 9 figures", "summary": "Synchronous reference frame phase-looked loop (SRF-PLL) techniques are widely\nused for interfacing and control applications in the power systems and energy\nconversion at large. Since a PLL system synchronizes its output with an\nexogenous harmonic signal, often 3-phases voltage or current, the locking of\nthe frequency and phase angle depends on the performance of the feedback loop\nwith at least two integrator terms, and on the distortions of the measured\ninput quantities. For the conventional SRF-PLL with a proportional-integral\n(PI) control in feedback, we are providing a robust design which maximizes the\nphase margin and uses the normalization scheme for yielding the loop\ninsensitive to the input amplitude variations. The main improvement in the\ntransient behavior and also in tracking of frequency ramps is achieved by using\nthe robust feed-forward frequency estimator, which is model-free and suitable\nfor the noisy and time-varying harmonic signals. The proposed\nfeed-forward-feedback SRF-PLL scheme is experimentally evaluated on the\n3-phases harmonic currents from standard PMSM drives with varying angular\nspeeds and loads. Both, the tracked angular frequency and locked phase angle\nare assessed as performance metrics of the robust SRF-PLL scheme with\nfeedforwarding.", "AI": {"tldr": "本文提出了一种鲁棒的同步参考系锁相环（SRF-PLL）设计，结合了PI控制和无模型前馈频率估计器，以提高在噪声和时变谐波信号下的瞬态性能和频率跟踪能力。", "motivation": "同步参考系锁相环（SRF-PLL）广泛应用于电力系统和能源转换的接口和控制。然而，其频率和相位角的锁定性能受反馈环路和输入信号失真的影响。因此，需要一种鲁棒的设计来改善在噪声和时变谐波信号下的瞬态行为和频率斜坡跟踪。", "method": "该研究采用以下方法：1) 对传统的PI控制SRF-PLL进行了鲁棒设计，旨在最大化相位裕度。2) 使用归一化方案使环路对输入幅度变化不敏感。3) 引入了一个鲁棒的、无模型的前馈频率估计器，以改善瞬态行为和频率斜坡跟踪，该估计器适用于噪声和时变谐波信号。4) 在标准永磁同步电机（PMSM）驱动器（具有变化的角速度和负载）产生的3相谐波电流上对所提出的前馈-反馈SRF-PLL方案进行了实验评估。", "result": "实验结果表明，所提出的前馈-反馈SRF-PLL方案在跟踪角频率和锁定相位角方面均表现出显著的性能提升，尤其是在处理来自PMSM驱动器的有噪声和时变谐波电流时。", "conclusion": "通过结合鲁棒的PI控制和无模型前馈频率估计器，所提出的SRF-PLL方案在噪声和时变谐波信号下展现出更优异的瞬态响应和频率跟踪能力，为电力系统和能源转换应用提供了更可靠的同步解决方案。"}}
{"id": "2509.18458", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50 (Primary) 68T07, 68T05, 68T20, 68T27 (Secondary)", "I.2.7; I.2.6; I.2.4; I.2.8"], "pdf": "https://arxiv.org/pdf/2509.18458", "abs": "https://arxiv.org/abs/2509.18458", "authors": ["Daniel Kaiser", "Arnoldo Frigessi", "Ali Ramezani-Kebrya", "Benjamin Ricaud"], "title": "CogniLoad: A Synthetic Natural Language Reasoning Benchmark With Tunable Length, Intrinsic Difficulty, and Distractor Density", "comment": "29 pages (main: 12 + supplemental material: 17), 6 figures, 4 tables,\n  Code: https://github.com/kaiserdan/cogniload, Data:\n  https://huggingface.co/datasets/cogniloadteam/cogniload", "summary": "Current benchmarks for long-context reasoning in Large Language Models (LLMs)\noften blur critical factors like intrinsic task complexity, distractor\ninterference, and task length. To enable more precise failure analysis, we\nintroduce CogniLoad, a novel synthetic benchmark grounded in Cognitive Load\nTheory (CLT). CogniLoad generates natural-language logic puzzles with\nindependently tunable parameters that reflect CLT's core dimensions: intrinsic\ndifficulty ($d$) controls intrinsic load; distractor-to-signal ratio ($\\rho$)\nregulates extraneous load; and task length ($N$) serves as an operational proxy\nfor conditions demanding germane load. Evaluating 22 SotA reasoning LLMs,\nCogniLoad reveals distinct performance sensitivities, identifying task length\nas a dominant constraint and uncovering varied tolerances to intrinsic\ncomplexity and U-shaped responses to distractor ratios. By offering systematic,\nfactorial control over these cognitive load dimensions, CogniLoad provides a\nreproducible, scalable, and diagnostically rich tool for dissecting LLM\nreasoning limitations and guiding future model development.", "AI": {"tldr": "本文介绍了CogniLoad，一个基于认知负荷理论（CLT）的新型合成基准，用于精确分析大型语言模型（LLMs）在长上下文推理中的失败原因，通过独立调节任务复杂性、干扰和任务长度等参数。", "motivation": "当前的LLM长上下文推理基准往往混淆了内在任务复杂性、干扰物干扰和任务长度等关键因素，导致无法进行精确的失败分析。", "method": "研究者引入了CogniLoad，这是一个基于认知负荷理论（CLT）的新型合成基准。它生成自然语言逻辑谜题，并独立调整反映CLT核心维度的参数：内在难度（d）控制内在负荷；干扰物与信号比（ρ）调节无关负荷；任务长度（N）作为需要相关负荷的条件的操作代理。该方法对22个最先进的推理LLM进行了评估。", "result": "CogniLoad揭示了LLM在长上下文推理中表现出的明显性能敏感性。研究发现任务长度是一个主要的限制因素，并揭示了LLM对内在复杂性的不同容忍度以及对干扰物比例的U形响应。", "conclusion": "CogniLoad通过对认知负荷维度的系统性和因子控制，提供了一个可复现、可扩展且诊断丰富的工具，用于剖析LLM的推理局限性并指导未来的模型开发。"}}
{"id": "2509.18597", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18597", "abs": "https://arxiv.org/abs/2509.18597", "authors": ["Yuan Meng", "Zhenguo Sun", "Max Fest", "Xukun Li", "Zhenshan Bing", "Alois Knoll"], "title": "Growing with Your Embodied Agent: A Human-in-the-Loop Lifelong Code Generation Framework for Long-Horizon Manipulation Skills", "comment": "upload 9 main page - v1", "summary": "Large language models (LLMs)-based code generation for robotic manipulation\nhas recently shown promise by directly translating human instructions into\nexecutable code, but existing methods remain noisy, constrained by fixed\nprimitives and limited context windows, and struggle with long-horizon tasks.\nWhile closed-loop feedback has been explored, corrected knowledge is often\nstored in improper formats, restricting generalization and causing catastrophic\nforgetting, which highlights the need for learning reusable skills. Moreover,\napproaches that rely solely on LLM guidance frequently fail in extremely\nlong-horizon scenarios due to LLMs' limited reasoning capability in the robotic\ndomain, where such issues are often straightforward for humans to identify. To\naddress these challenges, we propose a human-in-the-loop framework that encodes\ncorrections into reusable skills, supported by external memory and\nRetrieval-Augmented Generation with a hint mechanism for dynamic reuse.\nExperiments on Ravens, Franka Kitchen, and MetaWorld, as well as real-world\nsettings, show that our framework achieves a 0.93 success rate (up to 27%\nhigher than baselines) and a 42% efficiency improvement in correction rounds.\nIt can robustly solve extremely long-horizon tasks such as \"build a house\",\nwhich requires planning over 20 primitives.", "AI": {"tldr": "本文提出一个“人在环路”框架，通过将纠正编码为可重用技能，并结合外部记忆和带有提示机制的检索增强生成（RAG），解决了LLM在机器人操作代码生成中存在的噪声、固定原语、有限上下文窗口、长时序任务和泛化性差等问题，显著提高了任务成功率和纠正效率。", "motivation": "现有基于大型语言模型（LLM）的机器人操作代码生成方法存在诸多问题：输出噪声大、受限于固定原语、上下文窗口有限，难以处理长时序任务。虽然探索了闭环反馈，但纠正知识存储格式不当，限制了泛化并导致灾难性遗忘，凸显了学习可重用技能的需求。此外，纯粹依赖LLM指导的方法在极长时序场景中常常失败，因为LLM在机器人领域推理能力有限，而这些问题对人类来说通常很容易识别。", "method": "为解决这些挑战，本文提出一个“人在环路”（human-in-the-loop）框架。该框架将纠正编码为可重用技能，并由外部记忆和带有提示机制的检索增强生成（RAG）提供支持，以实现技能的动态重用。", "result": "在Ravens、Franka Kitchen、MetaWorld以及真实世界环境中的实验表明，该框架实现了0.93的成功率（比基线高出27%），并将纠正轮次的效率提高了42%。它能稳健地解决“建造房屋”等极长时序任务，这些任务需要规划超过20个基本原语。", "conclusion": "本文提出的“人在环路”框架通过将纠正转化为可重用技能，并辅以外部记忆和RAG机制，有效克服了LLM在机器人操作代码生成中面临的噪声、泛化性差和长时序任务挑战，显著提升了任务成功率、纠正效率和对复杂任务的鲁棒性。"}}
{"id": "2509.18234", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18234", "abs": "https://arxiv.org/abs/2509.18234", "authors": ["Yu Gu", "Jingjing Fu", "Xiaodong Liu", "Jeya Maria Jose Valanarasu", "Noel Codella", "Reuben Tan", "Qianchu Liu", "Ying Jin", "Sheng Zhang", "Jinyu Wang", "Rui Wang", "Lei Song", "Guanghui Qin", "Naoto Usuyama", "Cliff Wong", "Cheng Hao", "Hohin Lee", "Praneeth Sanapathi", "Sarah Hilado", "Bian Jiang", "Javier Alvarez-Valle", "Mu Wei", "Jianfeng Gao", "Eric Horvitz", "Matt Lungren", "Hoifung Poon", "Paul Vozila"], "title": "The Illusion of Readiness: Stress Testing Large Frontier Models on Multimodal Medical Benchmarks", "comment": "35 pages", "summary": "Large frontier models like GPT-5 now achieve top scores on medical\nbenchmarks. But our stress tests tell a different story. Leading systems often\nguess correctly even when key inputs like images are removed, flip answers\nunder trivial prompt changes, and fabricate convincing yet flawed reasoning.\nThese aren't glitches; they expose how today's benchmarks reward test-taking\ntricks over medical understanding. We evaluate six flagship models across six\nwidely used benchmarks and find that high leaderboard scores hide brittleness\nand shortcut learning. Through clinician-guided rubric evaluation, we show that\nbenchmarks vary widely in what they truly measure yet are treated\ninterchangeably, masking failure modes. We caution that medical benchmark\nscores do not directly reflect real-world readiness. If we want AI to earn\ntrust in healthcare, we must demand more than leaderboard wins and must hold\nsystems accountable for robustness, sound reasoning, and alignment with real\nmedical demands.", "AI": {"tldr": "领先的医疗AI模型在基准测试中表现出色，但压力测试揭示其脆弱性、捷径学习和缺乏真实医学理解，表明现有基准分数未能反映实际应用能力。", "motivation": "尽管大型模型在医疗基准测试中取得高分，但作者怀疑这些分数是否真正反映了医学理解和实际应用能力，认为现有基准可能奖励了“应试技巧”而非真正的医学知识。", "method": "对六个领先模型在六个常用医疗基准上进行压力测试，并采用临床医生指导的评估标准来分析基准测量的内容。", "result": "领先模型即使在关键输入缺失时也能猜对，在微小提示词变化下答案会翻转，并能编造有说服力但有缺陷的推理。高基准分数掩盖了模型的脆弱性和捷径学习。不同的基准测试衡量内容差异很大，但常被互换使用，掩盖了模型的失效模式。", "conclusion": "医疗基准测试分数不能直接反映AI在现实世界的就绪程度。为了让AI在医疗领域赢得信任，需要超越排行榜的胜利，并要求系统具备鲁棒性、可靠的推理能力以及与真实医疗需求的对齐。"}}
{"id": "2509.18308", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18308", "abs": "https://arxiv.org/abs/2509.18308", "authors": ["Yixin Zhang", "Ryan Chamberlain", "Lawrance Ngo", "Kevin Kramer", "Maciej A. Mazurowski"], "title": "Rethinking Pulmonary Embolism Segmentation: A Study of Current Approaches and Challenges with an Open Weight Model", "comment": "submitted to WACV 2026 application track, model weights available at:\n  https://github.com/mazurowski-lab/PulmonaryEmbolismSegmentation", "summary": "In this study, we curated a densely annotated in-house dataset comprising 490\nCTPA scans. Using this dataset, we systematically evaluated nine widely used\nsegmentation architectures from both the CNN and Vision Transformer (ViT)\nfamilies, initialized with either pretrained or random weights, under a unified\ntesting framework as a performance audit. Our study leads to several important\nobservations: (1) 3D U-Net with a ResNet encoder remains a highly effective\narchitecture for PE segmentation; (2) 3D models are particularly well-suited to\nthis task given the morphological characteristics of emboli; (3) CNN-based\nmodels generally yield superior performance compared to their ViT-based\ncounterparts in PE segmentation; (4) classification-based pretraining, even on\nlarge PE datasets, can adversely impact segmentation performance compared to\ntraining from scratch, suggesting that PE classification and segmentation may\nrely on different sets of discriminative features; (5) different model\narchitectures show a highly consistent pattern of segmentation performance when\ntrained on the same data; and (6) while central and large emboli can be\nsegmented with satisfactory accuracy, distal emboli remain challenging due to\nboth task complexity and the scarcity of high-quality datasets. Besides these\nfindings, our best-performing model achieves a mean Dice score of 0.7131 for\nsegmentation. It detects 181 emboli with 49 false positives and 28 false\nnegatives from 60 in-house testing scans. Its generalizability is further\nvalidated on public datasets.", "AI": {"tldr": "本研究在自建数据集上系统评估了9种分割架构对肺栓塞（PE）的性能，发现3D U-Net（ResNet编码器）表现出色，CNN优于ViT，且分类预训练可能不利于分割，远端栓塞仍是挑战。", "motivation": "本研究旨在利用一个密集标注的自建数据集，系统评估多种广泛使用的分割架构（包括CNN和Vision Transformer）在肺栓塞（PE）分割任务中的性能，并进行统一的性能审计。", "method": "研究构建了一个包含490个CTPA扫描的密集标注内部数据集。在此数据集上，系统评估了9种广泛使用的分割架构（来自CNN和Vision Transformer家族），这些架构使用预训练或随机权重初始化，并在统一的测试框架下进行性能审计。", "result": "主要发现包括：(1) 带有ResNet编码器的3D U-Net仍然是PE分割的高效架构；(2) 鉴于栓塞的形态特征，3D模型特别适合此任务；(3) CNN模型在PE分割中通常优于ViT模型；(4) 基于分类的预训练（即使在大型PE数据集上）可能对分割性能产生负面影响，表明PE分类和分割可能依赖不同的判别特征；(5) 不同模型架构在相同数据上训练时显示出高度一致的分割性能模式；(6) 中心和大型栓塞可以达到满意的分割精度，但远端栓塞由于任务复杂性和高质量数据集稀缺而仍然具有挑战性。最佳模型在分割任务上实现了0.7131的平均Dice分数，并在60个内部测试扫描中检测到181个栓塞，有49个假阳性和28个假阴性，并在公共数据集上验证了其泛化能力。", "conclusion": "本研究得出结论，带有ResNet编码器的3D U-Net是PE分割的有效架构，3D模型适用于此任务，且CNN模型通常优于ViT模型。此外，分类预训练可能不适用于分割任务，远端栓塞的准确分割仍是未来研究的重点挑战。"}}
{"id": "2509.19243", "categories": ["eess.SY", "cs.SY", "econ.TH"], "pdf": "https://arxiv.org/pdf/2509.19243", "abs": "https://arxiv.org/abs/2509.19243", "authors": ["Ahmed S. Alahmed", "Audun Botterud", "Saurabh Amin", "Ali T. Al-Awami"], "title": "Watts and Drops: Co-Scheduling Power and Water in Desalination Plants", "comment": "5 pages, 6 figures. To appear in Proceedings of the 61st Allerton\n  Conference on Communication, Control, and Computing", "summary": "We develop a mathematical framework to jointly schedule water and electricity\nin a profit-maximizing renewable colocated water desalination plant that\nintegrates both thermal and membrane based technologies. The price-taking\ndesalination plant sells desalinated water to a water utility at a given price\nand engages in bidirectional electricity transactions with the grid, purchasing\nor selling power based on its net electricity demand. We show that the optimal\nscheduling policy depends on the plant's internal renewable generation and\nfollows a simple threshold structure. Under the optimal policy, thermal based\nwater output decreases monotonically with renewable output, while membrane\nbased water output increases monotonically. We characterize the structure and\nintuition behind the threshold policy and examine key special properties.", "AI": {"tldr": "本文提出一个数学框架，用于优化结合热能和膜技术的再生能源并置海水淡化厂的水电联合调度，以实现利润最大化。", "motivation": "旨在帮助利用可再生能源、整合热能和膜技术的脱盐厂实现利润最大化，通过优化其水和电力的生产与交易。", "method": "开发了一个数学框架来对价格接受型脱盐厂进行建模，该厂以给定价格向水务公司出售淡化水，并与电网进行双向电力交易，购买或出售电力。", "result": "最优调度策略依赖于工厂内部的可再生能源发电量，并遵循简单的阈值结构。在该最优策略下，基于热能的水产量随可再生能源产量单调递减，而基于膜技术的水产量则单调递增。", "conclusion": "最优调度策略呈现出简单的阈值结构，其特点是基于可再生能源输出，热能和膜技术产水之间存在反向的单调关系，这为利润最大化的水电联合调度提供了清晰的指导。"}}
{"id": "2509.18467", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18467", "abs": "https://arxiv.org/abs/2509.18467", "authors": ["Zeyu Liu", "Souvik Kundu", "Lianghao Jiang", "Anni Li", "Srikanth Ronanki", "Sravan Bodapati", "Gourav Datta", "Peter A. Beerel"], "title": "LAWCAT: Efficient Distillation from Quadratic to Linear Attention with Convolution across Tokens for Long Context Modeling", "comment": "17 pages, 8 figures", "summary": "Although transformer architectures have achieved state-of-the-art performance\nacross diverse domains, their quadratic computational complexity with respect\nto sequence length remains a significant bottleneck, particularly for\nlatency-sensitive long-context applications. While recent linear-complexity\nalternatives are increasingly powerful, effectively training them from scratch\nis still resource-intensive. To overcome these limitations, we propose LAWCAT\n(Linear Attention with Convolution Across Time), a novel linearization\nframework designed to efficiently transfer the capabilities of pre-trained\ntransformers into a performant linear attention architecture. LAWCAT integrates\ncausal Conv1D layers to enhance local dependency modeling and employs\nnormalized gated linear attention to improve generalization across varying\ncontext lengths. Our comprehensive evaluations demonstrate that, distilling\nMistral-7B with only 1K-length sequences yields over 90\\% passkey retrieval\naccuracy up to 22K tokens, significantly extending its effective context\nwindow. Similarly, Llama3.2-1B LAWCAT variant achieves competitive performance\non S-NIAH 1\\&2\\&3 tasks (1K-8K context length) and BABILong benchmark\n(QA2\\&QA3, 0K-16K context length), requiring less than 0.1\\% pre-training\ntokens compared with pre-training models. Furthermore, LAWCAT exhibits faster\nprefill speeds than FlashAttention-2 for sequences exceeding 8K tokens. LAWCAT\nthus provides an efficient pathway to high-performance, long-context linear\nmodels suitable for edge deployment, reducing reliance on extensive\nlong-sequence training data and computational resources.", "AI": {"tldr": "LAWCAT是一种新颖的线性化框架，旨在高效地将预训练Transformer的能力迁移到高性能线性注意力架构，以解决Transformer的二次复杂度瓶颈，并实现长上下文、低资源消耗和边缘部署。", "motivation": "Transformer架构的计算复杂度随序列长度呈二次方增长，这限制了其在延迟敏感的长上下文应用中的使用。尽管存在线性复杂度的替代方案，但从头开始训练它们需要大量资源。", "method": "LAWCAT（Linear Attention with Convolution Across Time）框架：它通过集成因果Conv1D层来增强局部依赖建模，并采用归一化门控线性注意力来提高在不同上下文长度下的泛化能力。其核心思想是将预训练Transformer的能力蒸馏到线性注意力架构中。", "result": "使用1K长度序列蒸馏Mistral-7B，在22K token上实现了超过90%的passkey检索准确率，显著扩展了其有效上下文窗口。Llama3.2-1B LAWCAT变体在S-NIAH 1&2&3 (1K-8K上下文) 和BABILong (QA2&QA3, 0K-16K上下文) 任务上表现出竞争力，且所需预训练token不到预训练模型的0.1%。此外，LAWCAT对超过8K token的序列表现出比FlashAttention-2更快的预填充速度。", "conclusion": "LAWCAT提供了一种高效途径，可以构建适用于边缘部署的高性能、长上下文线性模型，从而减少对大量长序列训练数据和计算资源的依赖。"}}
{"id": "2509.18608", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18608", "abs": "https://arxiv.org/abs/2509.18608", "authors": ["Ana Luiza Mineiro", "Francisco Affonso", "Marcelo Becker"], "title": "End-to-End Crop Row Navigation via LiDAR-Based Deep Reinforcement Learning", "comment": "Accepted to the 22nd International Conference on Advanced Robotics\n  (ICAR 2025). 7 pages", "summary": "Reliable navigation in under-canopy agricultural environments remains a\nchallenge due to GNSS unreliability, cluttered rows, and variable lighting. To\naddress these limitations, we present an end-to-end learning-based navigation\nsystem that maps raw 3D LiDAR data directly to control commands using a deep\nreinforcement learning policy trained entirely in simulation. Our method\nincludes a voxel-based downsampling strategy that reduces LiDAR input size by\n95.83%, enabling efficient policy learning without relying on labeled datasets\nor manually designed control interfaces. The policy was validated in\nsimulation, achieving a 100% success rate in straight-row plantations and\nshowing a gradual decline in performance as row curvature increased, tested\nacross varying sinusoidal frequencies and amplitudes.", "AI": {"tldr": "本文提出一个端到端的基于深度强化学习的导航系统，该系统将原始3D LiDAR数据直接映射到控制命令，并在模拟环境中进行训练，以解决冠层下农业环境中的导航挑战。", "motivation": "冠层下农业环境中的可靠导航面临挑战，主要原因包括GNSS不可靠、行道杂乱以及光照多变。", "method": "该方法采用一个端到端的基于学习的导航系统，直接将原始3D LiDAR数据映射到控制命令。它使用在模拟中完全训练的深度强化学习策略，并包含一个基于体素的下采样策略，将LiDAR输入大小减少95.83%，从而实现高效的策略学习，且不依赖于标注数据集或手动设计的控制接口。", "result": "该策略在模拟中得到验证，在直行种植园中实现了100%的成功率。随着行道曲率的增加（通过不同的正弦频率和振幅测试），性能逐渐下降。", "conclusion": "该研究展示了一个基于模拟训练的端到端深度强化学习系统在冠层下农业导航中的潜力，尤其在直行环境中表现出色，但在处理复杂曲率时性能有待提升。"}}
{"id": "2509.18382", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18382", "abs": "https://arxiv.org/abs/2509.18382", "authors": ["Adarsha Balaji", "Le Chen", "Rajeev Thakur", "Franck Cappello", "Sandeep Madireddy"], "title": "Evaluating the Safety and Skill Reasoning of Large Reasoning Models Under Compute Constraints", "comment": null, "summary": "Test-time compute scaling has demonstrated the ability to improve the\nperformance of reasoning language models by generating longer chain-of-thought\n(CoT) sequences. However, this increase in performance comes with a significant\nincrease in computational cost. In this work, we investigate two compute\nconstraint strategies: (1) reasoning length constraint and (2) model\nquantization, as methods to reduce the compute demand of reasoning models and\nstudy their impact on their safety performance. Specifically, we explore two\napproaches to apply compute constraints to reasoning models: (1) fine-tuning\nreasoning models using a length controlled policy optimization (LCPO) based\nreinforcement learning method to satisfy a user-defined CoT reasoning length,\nand (2) applying quantization to maximize the generation of CoT sequences\nwithin a user-defined compute constraint. Furthermore, we study the trade-off\nbetween the computational efficiency and the safety of the model.", "AI": {"tldr": "本文研究在计算资源受限下，通过限制推理长度和模型量化两种策略，优化语言模型链式思考(CoT)推理的计算效率，并分析其对模型安全性能的影响。", "motivation": "链式思考(CoT)虽然能提升推理语言模型的性能，但会导致显著的计算成本增加，因此需要探索降低计算需求的方法。", "method": "研究了两种计算约束策略：1) 推理长度约束；2) 模型量化。具体实施方法包括：1) 使用基于长度控制策略优化(LCPO)的强化学习方法微调推理模型，以满足用户定义的CoT推理长度；2) 应用量化在用户定义的计算约束内最大化CoT序列的生成。此外，还研究了计算效率与模型安全之间的权衡。", "result": "抽象中未明确给出具体结果，但指出研究了计算约束对推理模型安全性能的影响，并探索了计算效率与模型安全之间的权衡。", "conclusion": "抽象中未提供明确的结论，而是阐述了研究的范围和目标，即在计算受限下提升CoT模型的效率并评估其对安全性的影响。"}}
{"id": "2509.18309", "categories": ["cs.CV", "cs.LG", "I.2.10"], "pdf": "https://arxiv.org/pdf/2509.18309", "abs": "https://arxiv.org/abs/2509.18309", "authors": ["Alessa Carbo", "Eric Nalisnick"], "title": "Improving Handshape Representations for Sign Language Processing: A Graph Neural Network Approach", "comment": null, "summary": "Handshapes serve a fundamental phonological role in signed languages, with\nAmerican Sign Language employing approximately 50 distinct shapes.\nHowever,computational approaches rarely model handshapes explicitly, limiting\nboth recognition accuracy and linguistic analysis.We introduce a novel graph\nneural network that separates temporal dynamics from static handshape\nconfigurations. Our approach combines anatomically-informed graph structures\nwith contrastive learning to address key challenges in handshape recognition,\nincluding subtle interclass distinctions and temporal variations. We establish\nthe first benchmark for structured handshape recognition in signing sequences,\nachieving 46% accuracy across 37 handshape classes (with baseline methods\nachieving 25%).", "AI": {"tldr": "本文提出了一种新颖的图神经网络，结合解剖学图结构和对比学习，显著提高了手语手形识别的准确性，并建立了首个结构化手形识别基准。", "motivation": "手形在手语中扮演着重要的语音学角色（如美国手语有约50种手形），但现有的计算方法很少明确地对手形进行建模，这限制了识别准确性和语言分析能力。研究旨在解决手形识别中微妙的类别间区分和时间变化等关键挑战。", "method": "研究引入了一种新颖的图神经网络，它将时间动态与静态手形配置分离。该方法结合了基于解剖学信息的图结构和对比学习，以应对手形识别中的挑战。", "result": "研究建立了首个手语序列中结构化手形识别的基准，在37个手形类别上实现了46%的准确率，远高于基线方法（25%）。", "conclusion": "该研究提出的图神经网络方法能够有效分离时间动态和静态手形，并通过结合解剖学信息和对比学习，显著提升了手语手形识别的准确性，并为该领域设立了一个新的性能基准。"}}
{"id": "2509.19266", "categories": ["eess.SY", "cs.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.19266", "abs": "https://arxiv.org/abs/2509.19266", "authors": ["Charis Stamouli", "Leonardo F. Toso", "Anastasios Tsiamis", "George J. Pappas", "James Anderson"], "title": "Policy Gradient Bounds in Multitask LQR", "comment": null, "summary": "We analyze the performance of policy gradient in multitask linear quadratic\nregulation (LQR), where the system and cost parameters differ across tasks. The\nmain goal of multitask LQR is to find a controller with satisfactory\nperformance on every task. Prior analyses on relevant contexts fail to capture\nclosed-loop task similarities, resulting in conservative performance\nguarantees. To account for such similarities, we propose bisimulation-based\nmeasures of task heterogeneity. Our measures employ new bisimulation functions\nto bound the cost gradient distance between a pair of tasks in closed loop with\na common stabilizing controller. Employing these measures, we derive\nsuboptimality bounds for both the multitask optimal controller and the\nasymptotic policy gradient controller with respect to each of the tasks. We\nfurther provide conditions under which the policy gradient iterates remain\nstabilizing for every system. For multiple random sets of certain tasks, we\nobserve that our bisimulation-based measures improve upon baseline measures of\ntask heterogeneity dramatically.", "AI": {"tldr": "本文分析了多任务线性二次调节（LQR）中策略梯度（policy gradient）的性能，并提出了基于双模拟（bisimulation）的任务异质性度量方法，以提供更精确的性能保证。", "motivation": "在多任务LQR中，目标是找到一个对所有任务都表现良好的控制器。现有的分析未能捕捉闭环任务的相似性，导致性能保证过于保守。因此，需要一种新的方法来量化并利用这些任务相似性。", "method": "作者提出了基于双模拟的任务异质性度量方法。这些度量利用新的双模拟函数来限制在通用稳定控制器下，一对任务之间闭环成本梯度距离。基于这些度量，作者推导了多任务最优控制器和渐进策略梯度控制器相对于每个任务的次优性界限，并给出了策略梯度迭代保持系统稳定的条件。", "result": "研究结果包括：为多任务最优控制器和渐进策略梯度控制器导出了次优性界限；提供了策略梯度迭代对每个系统保持稳定的条件。在多个随机任务集上，实验观察表明，所提出的基于双模拟的度量方法显著优于基线任务异质性度量方法。", "conclusion": "基于双模拟的度量方法能够有效捕捉多任务LQR中闭环任务的相似性，从而为策略梯度提供了更精确、更不保守的性能保证和稳定性分析。"}}
{"id": "2509.18487", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18487", "abs": "https://arxiv.org/abs/2509.18487", "authors": ["Ben Finkelshtein", "Silviu Cucerzan", "Sujay Kumar Jauhar", "Ryen White"], "title": "Actions Speak Louder than Prompts: A Large-Scale Study of LLMs for Graph Inference", "comment": null, "summary": "Large language models (LLMs) are increasingly used for text-rich graph\nmachine learning tasks such as node classification in high-impact domains like\nfraud detection and recommendation systems. Yet, despite a surge of interest,\nthe field lacks a principled understanding of the capabilities of LLMs in their\ninteraction with graph data. In this work, we conduct a large-scale, controlled\nevaluation across several key axes of variability to systematically assess the\nstrengths and weaknesses of LLM-based graph reasoning methods in text-based\napplications. The axes include the LLM-graph interaction mode, comparing\nprompting, tool-use, and code generation; dataset domains, spanning citation,\nweb-link, e-commerce, and social networks; structural regimes contrasting\nhomophilic and heterophilic graphs; feature characteristics involving both\nshort- and long-text node attributes; and model configurations with varying LLM\nsizes and reasoning capabilities. We further analyze dependencies by\nmethodically truncating features, deleting edges, and removing labels to\nquantify reliance on input types. Our findings provide practical and actionable\nguidance. (1) LLMs as code generators achieve the strongest overall performance\non graph data, with especially large gains on long-text or high-degree graphs\nwhere prompting quickly exceeds the token budget. (2) All interaction\nstrategies remain effective on heterophilic graphs, challenging the assumption\nthat LLM-based methods collapse under low homophily. (3) Code generation is\nable to flexibly adapt its reliance between structure, features, or labels to\nleverage the most informative input type. Together, these findings provide a\ncomprehensive view of the strengths and limitations of current LLM-graph\ninteraction modes and highlight key design principles for future approaches.", "AI": {"tldr": "本文系统评估了大型语言模型（LLMs）在文本丰富图机器学习任务中与图数据交互的能力，发现代码生成模式表现最佳，尤其在长文本或高连接度图上；同时，所有交互策略在异配图上依然有效，且代码生成能灵活调整对结构、特征或标签的依赖。", "motivation": "尽管LLMs在文本丰富的图机器学习任务（如欺诈检测、推荐系统中的节点分类）中应用日益广泛，但目前对LLMs与图数据交互能力的原理性理解尚不足。", "method": "研究进行了一项大规模、受控的评估，涵盖了多个关键变量：LLM-图交互模式（提示、工具使用、代码生成）、数据集领域（引用、网页链接、电商、社交网络）、结构类型（同配图、异配图）、特征特性（短文本、长文本节点属性）以及LLM模型配置（不同大小和推理能力）。此外，通过截断特征、删除边和移除标签来量化LLM对输入类型的依赖性。", "result": "1. LLM作为代码生成器在图数据上取得了最强的整体性能，尤其在长文本或高连接度图上，显著优于提示模式。2. 所有交互策略在异配图上均保持有效，挑战了LLM方法在低同配性下失效的假设。3. 代码生成模式能够灵活调整其对结构、特征或标签的依赖，从而利用最具信息量的输入类型。", "conclusion": "这些发现全面揭示了当前LLM-图交互模式的优势和局限性，并为未来方法的设计提供了关键指导原则和实用的操作性建议。"}}
{"id": "2509.18609", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18609", "abs": "https://arxiv.org/abs/2509.18609", "authors": ["Chengran Yuan", "Zijian Lu", "Zhanqi Zhang", "Yimin Zhao", "Zefan Huang", "Shuo Sun", "Jiawei Sun", "Jiahui Li", "Christina Dao Wen Lee", "Dongen Li", "Marcelo H. Ang Jr"], "title": "PIE: Perception and Interaction Enhanced End-to-End Motion Planning for Autonomous Driving", "comment": null, "summary": "End-to-end motion planning is promising for simplifying complex autonomous\ndriving pipelines. However, challenges such as scene understanding and\neffective prediction for decision-making continue to present substantial\nobstacles to its large-scale deployment. In this paper, we present PIE, a\npioneering framework that integrates advanced perception, reasoning, and\nintention modeling to dynamically capture interactions between the ego vehicle\nand surrounding agents. It incorporates a bidirectional Mamba fusion that\naddresses data compression losses in multimodal fusion of camera and LiDAR\ninputs, alongside a novel reasoning-enhanced decoder integrating Mamba and\nMixture-of-Experts to facilitate scene-compliant anchor selection and optimize\nadaptive trajectory inference. PIE adopts an action-motion interaction module\nto effectively utilize state predictions of surrounding agents to refine ego\nplanning. The proposed framework is thoroughly validated on the NAVSIM\nbenchmark. PIE, without using any ensemble and data augmentation techniques,\nachieves an 88.9 PDM score and 85.6 EPDM score, surpassing the performance of\nprior state-of-the-art methods. Comprehensive quantitative and qualitative\nanalyses demonstrate that PIE is capable of reliably generating feasible and\nhigh-quality ego trajectories.", "AI": {"tldr": "PIE是一个端到端运动规划框架，通过集成先进的感知、推理和意图建模，利用双向Mamba融合和推理增强解码器，动态捕捉自车与周围智能体之间的交互，生成高质量的自车轨迹，并在NAVSIM基准上超越了现有SOTA方法。", "motivation": "端到端运动规划有望简化复杂的自动驾驶流程，但场景理解和有效的预测决策仍然是其大规模部署的重大障碍。", "method": "PIE框架集成了先进的感知、推理和意图建模，以动态捕捉自车与周围智能体之间的交互。它采用了双向Mamba融合来解决相机和激光雷达多模态融合中的数据压缩损失，并引入了一种新颖的推理增强解码器（结合Mamba和Mixture-of-Experts）来促进符合场景的锚点选择和优化自适应轨迹推理。此外，PIE还采用了一个动作-运动交互模块来利用周围智能体的状态预测以完善自车规划。", "result": "PIE在NAVSIM基准上进行了全面验证，在未使用任何集成和数据增强技术的情况下，取得了88.9的PDM分数和85.6的EPDM分数，超越了此前最先进方法的性能。定量和定性分析均表明PIE能够可靠地生成可行且高质量的自车轨迹。", "conclusion": "PIE框架通过创新的感知、推理和意图建模方法，有效解决了端到端运动规划中的挑战，能够可靠地生成高质量的自车轨迹，并在关键指标上达到了新的SOTA水平。"}}
{"id": "2509.18383", "categories": ["cs.AI", "cs.DM", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18383", "abs": "https://arxiv.org/abs/2509.18383", "authors": ["Moran Feldman", "Amin Karbasi"], "title": "Gödel Test: Can Large Language Models Solve Easy Conjectures?", "comment": null, "summary": "Recent announcements from frontier AI model labs have highlighted strong\nresults on high-school and undergraduate math competitions. Yet it remains\nunclear whether large language models can solve new, simple conjectures in more\nadvanced areas of mathematics. We propose the G\\\"odel Test: evaluating whether\na model can produce correct proofs for very simple, previously unsolved\nconjectures. To this end, we study the performance of GPT-5 on five conjectures\nin combinatorial optimization. For each problem, we provided one or two source\npapers from which the conjecture arose, withheld our own conjecture, and then\nassessed the model's reasoning in detail. On the three easier problems, GPT-5\nproduced nearly correct solutions; for Problem 2 it even derived a different\napproximation guarantee that, upon checking, refuted our conjecture while\nproviding a valid solution. The model failed on Problem 4, which required\ncombining results from two papers. On Problem 5, a harder case without a\nvalidated conjecture, GPT-5 proposed the same algorithm we had in mind but\nfailed in the analysis, suggesting the proof is more challenging than expected.\nAlthough our sample is small, the results point to meaningful progress on\nroutine reasoning, occasional flashes of originality, and clear limitations\nwhen cross-paper synthesis is required. GPT-5 may represent an early step\ntoward frontier models eventually passing the G\\\"odel Test.", "AI": {"tldr": "本研究提出了“哥德尔测试”，以评估大型语言模型（如GPT-5）解决组合优化领域中新颖、简单数学猜想的能力，结果显示模型在常规推理方面取得进展，偶有独创性，但在需要跨论文综合时存在明显局限。", "motivation": "现有前沿AI模型在高中和本科数学竞赛中表现出色，但其是否能解决高级数学领域中全新、简单的猜想尚不明确。这促使研究者提出“哥德尔测试”来填补这一评估空白。", "method": "研究者提出了“哥德尔测试”，即评估模型能否为非常简单、先前未解决的猜想提供正确证明。他们将GPT-5应用于组合优化领域的五个猜想，为每个问题提供了一两篇来源论文，但未告知模型自己的猜想，并详细评估了模型的推理过程。", "result": "在三个较简单的问题上，GPT-5给出了几乎正确的解决方案。对于问题2，模型甚至推导出了一个不同的近似保证，该保证在验证后驳斥了研究者的猜想并提供了一个有效解。模型在需要结合两篇论文结果的问题4上失败。在更难且无已验证猜想的问题5上，GPT-5提出了与研究者相同的算法，但在分析中失败。", "conclusion": "尽管样本量小，结果表明GPT-5在常规推理方面取得了显著进展，偶尔展现出独创性，但在需要跨论文综合时存在明显局限性。GPT-5可能代表着前沿模型最终通过“哥德尔测试”的早期一步。"}}
{"id": "2509.18326", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18326", "abs": "https://arxiv.org/abs/2509.18326", "authors": ["Chun Kit Wong", "Anders N. Christensen", "Cosmin I. Bercea", "Julia A. Schnabel", "Martin G. Tolsgaard", "Aasa Feragen"], "title": "Influence of Classification Task and Distribution Shift Type on OOD Detection in Fetal Ultrasound", "comment": "MICCAI 2025", "summary": "Reliable out-of-distribution (OOD) detection is important for safe deployment\nof deep learning models in fetal ultrasound amidst heterogeneous image\ncharacteristics and clinical settings. OOD detection relies on estimating a\nclassification model's uncertainty, which should increase for OOD samples.\nWhile existing research has largely focused on uncertainty quantification\nmethods, this work investigates the impact of the classification task itself.\nThrough experiments with eight uncertainty quantification methods across four\nclassification tasks, we demonstrate that OOD detection performance\nsignificantly varies with the task, and that the best task depends on the\ndefined ID-OOD criteria; specifically, whether the OOD sample is due to: i) an\nimage characteristic shift or ii) an anatomical feature shift. Furthermore, we\nreveal that superior OOD detection does not guarantee optimal abstained\nprediction, underscoring the necessity to align task selection and uncertainty\nstrategies with the specific downstream application in medical image analysis.", "AI": {"tldr": "本研究发现，在胎儿超声图像中，模型分类任务的选择显著影响出分布（OOD）检测的性能，并且最佳任务取决于OOD样本的类型（图像特征或解剖特征偏移）。", "motivation": "在胎儿超声等医疗领域安全部署深度学习模型，可靠的出分布（OOD）检测至关重要，因为图像特征和临床环境存在异质性。现有研究主要关注不确定性量化方法，但本研究认为分类任务本身对OOD检测性能也有重要影响。", "method": "通过八种不确定性量化方法，在四种不同的分类任务上进行了实验，以评估OOD检测性能。研究了两种OOD样本类型：i) 图像特征偏移，ii) 解剖特征偏移。", "result": "OOD检测性能随分类任务的不同而显著变化。最佳任务取决于ID-OOD的定义标准，即OOD样本是由图像特征偏移还是解剖特征偏移引起。此外，卓越的OOD检测性能并不能保证最佳的拒绝预测。", "conclusion": "在医学图像分析中，必须根据特定的下游应用来调整任务选择和不确定性策略，以实现有效的OOD检测和拒绝预测。"}}
{"id": "2509.18666", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.18666", "abs": "https://arxiv.org/abs/2509.18666", "authors": ["Kaizer Rahaman", "Simran Kumari", "Ashish R. Hota"], "title": "Distributionally Robust Safe Motion Planning with Contextual Information", "comment": null, "summary": "We present a distributionally robust approach for collision avoidance by\nincorporating contextual information. Specifically, we embed the conditional\ndistribution of future trajectory of the obstacle conditioned on the motion of\nthe ego agent in a reproducing kernel Hilbert space (RKHS) via the conditional\nkernel mean embedding operator. Then, we define an ambiguity set containing all\ndistributions whose embedding in the RKHS is within a certain distance from the\nempirical estimate of conditional mean embedding learnt from past data.\nConsequently, a distributionally robust collision avoidance constraint is\nformulated, and included in the receding horizon based motion planning\nformulation of the ego agent. Simulation results show that the proposed\napproach is more successful in avoiding collision compared to approaches that\ndo not include contextual information and/or distributional robustness in their\nformulation in several challenging scenarios.", "AI": {"tldr": "本文提出了一种结合上下文信息的分布鲁棒碰撞避免方法，通过条件核均值嵌入在RKHS中建模不确定性，并在运动规划中实现鲁棒性。", "motivation": "现有碰撞避免方法可能未充分利用上下文信息或缺乏对未来轨迹分布不确定性的鲁棒性，导致在复杂场景下效果不佳。", "method": "该方法将障碍物未来轨迹的条件分布（以自我智能体运动为条件）通过条件核均值嵌入算子嵌入到再生核希尔伯特空间（RKHS）中。然后，定义一个模糊集，包含所有其RKHS嵌入与从历史数据学习到的经验条件均值嵌入在一定距离内的分布。最后，将此分布鲁棒碰撞避免约束纳入到自我智能体的滚动时域运动规划公式中。", "result": "仿真结果表明，在多个具有挑战性的场景中，与未包含上下文信息和/或分布鲁棒性的方法相比，所提出的方法在避免碰撞方面更成功。", "conclusion": "通过结合上下文信息和分布鲁棒性，利用条件核均值嵌入在RKHS中建模不确定性，本方法显著提高了碰撞避免的成功率，尤其适用于复杂环境。"}}
{"id": "2509.18514", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18514", "abs": "https://arxiv.org/abs/2509.18514", "authors": ["Mohamad Elzohbi", "Richard Zhao"], "title": "A Rhythm-Aware Phrase Insertion for Classical Arabic Poetry Composition", "comment": "Accepted for the Third Arabic Natural Language Processing Conference\n  (ArabicNLP 2025)", "summary": "This paper presents a methodology for inserting phrases in Arabic poems to\nconform to a specific rhythm using ByT5, a byte-level multilingual\ntransformer-based model. Our work discusses a rule-based grapheme-to-beat\ntransformation tailored for extracting the rhythm from fully diacritized Arabic\nscript. Our approach employs a conditional denoising objective to fine-tune\nByT5, where the model reconstructs masked words to match a target rhythm. We\nadopt a curriculum learning strategy, pre-training on a general Arabic dataset\nbefore fine-tuning on poetic dataset, and explore cross-lingual transfer from\nEnglish to Arabic. Experimental results demonstrate that our models achieve\nhigh rhythmic alignment while maintaining semantic coherence. The proposed\nmodel has the potential to be used in co-creative applications in the process\nof composing classical Arabic poems.", "AI": {"tldr": "本文提出了一种使用ByT5模型向阿拉伯诗歌中插入短语以符合特定韵律的方法，该方法结合了基于规则的韵律提取和条件去噪微调，并在韵律对齐和语义连贯性方面表现出色。", "motivation": "研究动机是协助古典阿拉伯诗歌的创作过程，特别是实现短语插入时与特定韵律的匹配。", "method": "方法包括：1) 针对完全注音的阿拉伯语脚本，采用基于规则的字素到节拍转换来提取韵律。2) 使用ByT5（一个字节级多语言Transformer模型），通过条件去噪目标进行微调，使模型重建被遮蔽的词语以匹配目标韵律。3) 采用课程学习策略，先在通用阿拉伯语数据集上预训练，再在诗歌数据集上微调。4) 探索了从英语到阿拉伯语的跨语言迁移。", "result": "实验结果表明，所提出的模型在保持语义连贯性的同时，实现了高水平的韵律对齐。", "conclusion": "该模型具有在古典阿拉伯诗歌创作过程中应用于协同创意应用的潜力。"}}
{"id": "2509.18610", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18610", "abs": "https://arxiv.org/abs/2509.18610", "authors": ["Maximilian Adang", "JunEn Low", "Ola Shorinwa", "Mac Schwager"], "title": "SINGER: An Onboard Generalist Vision-Language Navigation Policy for Drones", "comment": null, "summary": "Large vision-language models have driven remarkable progress in\nopen-vocabulary robot policies, e.g., generalist robot manipulation policies,\nthat enable robots to complete complex tasks specified in natural language.\nDespite these successes, open-vocabulary autonomous drone navigation remains an\nunsolved challenge due to the scarcity of large-scale demonstrations, real-time\ncontrol demands of drones for stabilization, and lack of reliable external pose\nestimation modules. In this work, we present SINGER for language-guided\nautonomous drone navigation in the open world using only onboard sensing and\ncompute. To train robust, open-vocabulary navigation policies, SINGER leverages\nthree central components: (i) a photorealistic language-embedded flight\nsimulator with minimal sim-to-real gap using Gaussian Splatting for efficient\ndata generation, (ii) an RRT-inspired multi-trajectory generation expert for\ncollision-free navigation demonstrations, and these are used to train (iii) a\nlightweight end-to-end visuomotor policy for real-time closed-loop control.\nThrough extensive hardware flight experiments, we demonstrate superior\nzero-shot sim-to-real transfer of our policy to unseen environments and unseen\nlanguage-conditioned goal objects. When trained on ~700k-1M observation action\npairs of language conditioned visuomotor data and deployed on hardware, SINGER\noutperforms a velocity-controlled semantic guidance baseline by reaching the\nquery 23.33% more on average, and maintains the query in the field of view\n16.67% more on average, with 10% fewer collisions.", "AI": {"tldr": "SINGER是一种基于机载感知和计算的语言引导自主无人机导航系统，通过高斯泼溅模拟器、RRT启发式专家生成数据，并训练轻量级视觉运动策略，实现了卓越的零样本模拟到真实迁移性能。", "motivation": "尽管大型视觉-语言模型在开放词汇机器人策略方面取得了显著进展，但开放词汇自主无人机导航仍是一个未解决的挑战，原因在于缺乏大规模演示数据、无人机稳定所需的实时控制需求以及缺乏可靠的外部姿态估计模块。", "method": "SINGER利用三个核心组件来训练鲁棒的开放词汇导航策略：(i) 一个基于高斯泼溅的逼真语言嵌入飞行模拟器，具有最小的模拟到真实差距，用于高效数据生成；(ii) 一个RRT启发式的多轨迹生成专家，用于提供无碰撞导航演示；(iii) 一个轻量级的端到端视觉运动策略，用于实时闭环控制，该策略通过上述演示数据进行训练。所有操作仅使用机载传感器和计算。", "result": "通过广泛的硬件飞行实验，SINGER展示了其策略在未见环境和未见语言条件目标对象上的卓越零样本模拟到真实迁移能力。在约70万至100万个语言条件视觉运动观测-动作对上进行训练并在硬件上部署时，SINGER在平均到达查询目标方面比基于速度控制的语义引导基线高出23.33%，平均保持查询目标在视野中多16.67%，并且碰撞减少10%。", "conclusion": "SINGER成功解决了开放世界中语言引导自主无人机导航的挑战，通过创新的模拟器和训练方法，实现了卓越的零样本模拟到真实迁移，并在真实硬件上表现出优于基线的导航性能，减少了碰撞并提高了目标达成率和视野保持率。"}}
{"id": "2509.18400", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18400", "abs": "https://arxiv.org/abs/2509.18400", "authors": ["Pritish Yuvraj", "Siva Devarakonda"], "title": "ATLAS: Benchmarking and Adapting LLMs for Global Trade via Harmonized Tariff Code Classification", "comment": null, "summary": "Accurate classification of products under the Harmonized Tariff Schedule\n(HTS) is a critical bottleneck in global trade, yet it has received little\nattention from the machine learning community. Misclassification can halt\nshipments entirely, with major postal operators suspending deliveries to the\nU.S. due to incomplete customs documentation. We introduce the first benchmark\nfor HTS code classification, derived from the U.S. Customs Rulings Online\nSearch System (CROSS). Evaluating leading LLMs, we find that our fine-tuned\nAtlas model (LLaMA-3.3-70B) achieves 40 percent fully correct 10-digit\nclassifications and 57.5 percent correct 6-digit classifications, improvements\nof 15 points over GPT-5-Thinking and 27.5 points over Gemini-2.5-Pro-Thinking.\nBeyond accuracy, Atlas is roughly five times cheaper than GPT-5-Thinking and\neight times cheaper than Gemini-2.5-Pro-Thinking, and can be self-hosted to\nguarantee data privacy in high-stakes trade and compliance workflows. While\nAtlas sets a strong baseline, the benchmark remains highly challenging, with\nonly 40 percent 10-digit accuracy. By releasing both dataset and model, we aim\nto position HTS classification as a new community benchmark task and invite\nfuture work in retrieval, reasoning, and alignment.", "AI": {"tldr": "本文介绍了首个协调关税制度（HTS）编码分类基准，并提出了名为Atlas的微调模型（基于LLaMA-3.3-70B）。该模型在10位和6位分类上均显著优于现有领先LLM，且成本效益更高，同时解决了贸易中的关键瓶颈问题。", "motivation": "HTS产品分类的准确性是全球贸易中的关键瓶颈。错误分类可能导致货物完全停运，甚至邮政运营商暂停对美递送。然而，机器学习社区对这一问题关注甚少。", "method": "研究从美国海关裁决在线搜索系统（CROSS）中构建了首个HTS编码分类基准。评估了领先的大型语言模型（LLM），并开发并微调了Atlas模型（基于LLaMA-3.3-70B）。", "result": "Atlas模型在10位分类中实现了40%的完全正确率，在6位分类中实现了57.5%的正确率，分别比GPT-5-Thinking提高了15个百分点，比Gemini-2.5-Pro-Thinking提高了27.5个百分点。此外，Atlas的成本比GPT-5-Thinking便宜约5倍，比Gemini-2.5-Pro-Thinking便宜约8倍，并支持自托管以保障数据隐私。", "conclusion": "Atlas模型为HTS分类设定了一个强大的基线，但该基准任务仍然极具挑战性（10位准确率仅为40%）。通过发布数据集和模型，本文旨在将HTS分类定位为一个新的社区基准任务，并鼓励未来在检索、推理和对齐方面的研究。"}}
{"id": "2509.18350", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18350", "abs": "https://arxiv.org/abs/2509.18350", "authors": ["Oussema Dhaouadi", "Riccardo Marin", "Johannes Meier", "Jacques Kaiser", "Daniel Cremers"], "title": "OrthoLoC: UAV 6-DoF Localization and Calibration Using Orthographic Geodata", "comment": "Accepted at NeurIPS 2025", "summary": "Accurate visual localization from aerial views is a fundamental problem with\napplications in mapping, large-area inspection, and search-and-rescue\noperations. In many scenarios, these systems require high-precision\nlocalization while operating with limited resources (e.g., no internet\nconnection or GNSS/GPS support), making large image databases or heavy 3D\nmodels impractical. Surprisingly, little attention has been given to leveraging\northographic geodata as an alternative paradigm, which is lightweight and\nincreasingly available through free releases by governmental authorities (e.g.,\nthe European Union). To fill this gap, we propose OrthoLoC, the first\nlarge-scale dataset comprising 16,425 UAV images from Germany and the United\nStates with multiple modalities. The dataset addresses domain shifts between\nUAV imagery and geospatial data. Its paired structure enables fair benchmarking\nof existing solutions by decoupling image retrieval from feature matching,\nallowing isolated evaluation of localization and calibration performance.\nThrough comprehensive evaluation, we examine the impact of domain shifts, data\nresolutions, and covisibility on localization accuracy. Finally, we introduce a\nrefinement technique called AdHoP, which can be integrated with any feature\nmatcher, improving matching by up to 95% and reducing translation error by up\nto 63%. The dataset and code are available at:\nhttps://deepscenario.github.io/OrthoLoC.", "AI": {"tldr": "该论文提出了OrthoLoC，一个用于从空中视角进行视觉定位的大规模数据集，该数据集利用轻量级的正射地理数据，并引入了一种名为AdHoP的匹配优化技术，显著提高了定位精度。", "motivation": "在资源受限（如无网络或GNSS/GPS）的情况下，需要高精度空中视觉定位，但现有的大型图像数据库或3D模型不切实际。同时，轻量级的正射地理数据作为替代方案，却未得到充分利用。", "method": "提出了OrthoLoC数据集，包含16,425张来自德国和美国的无人机图像，具有多种模态和配对结构，旨在解决无人机图像与地理空间数据之间的域偏移问题，并促进公平基准测试。通过综合评估，研究了域偏移、数据分辨率和共视性对定位精度的影响。最后，引入了一种名为AdHoP的精炼技术，可与任何特征匹配器集成。", "result": "OrthoLoC数据集成功解决了无人机图像与地理空间数据之间的域偏移问题，并通过解耦图像检索和特征匹配，实现了对现有解决方案的公平基准测试。AdHoP精炼技术可将匹配性能提高高达95%，并将平移误差降低高达63%。", "conclusion": "OrthoLoC数据集和AdHoP精炼技术为利用轻量级正射地理数据进行高精度空中视觉定位提供了一个新的范式，有效解决了资源受限环境下的定位挑战和域偏移问题，显著提升了定位精度。"}}
{"id": "2509.18676", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.18676", "abs": "https://arxiv.org/abs/2509.18676", "authors": ["Sangjun Noh", "Dongwoo Nam", "Kangmin Kim", "Geonhyup Lee", "Yeonguk Yu", "Raeyoung Kang", "Kyoobin Lee"], "title": "3D Flow Diffusion Policy: Visuomotor Policy Learning via Generating Flow in 3D Space", "comment": "7 main scripts + 2 reference pages", "summary": "Learning robust visuomotor policies that generalize across diverse objects\nand interaction dynamics remains a central challenge in robotic manipulation.\nMost existing approaches rely on direct observation-to-action mappings or\ncompress perceptual inputs into global or object-centric features, which often\noverlook localized motion cues critical for precise and contact-rich\nmanipulation. We present 3D Flow Diffusion Policy (3D FDP), a novel framework\nthat leverages scene-level 3D flow as a structured intermediate representation\nto capture fine-grained local motion cues. Our approach predicts the temporal\ntrajectories of sampled query points and conditions action generation on these\ninteraction-aware flows, implemented jointly within a unified diffusion\narchitecture. This design grounds manipulation in localized dynamics while\nenabling the policy to reason about broader scene-level consequences of\nactions. Extensive experiments on the MetaWorld benchmark show that 3D FDP\nachieves state-of-the-art performance across 50 tasks, particularly excelling\non medium and hard settings. Beyond simulation, we validate our method on eight\nreal-robot tasks, where it consistently outperforms prior baselines in\ncontact-rich and non-prehensile scenarios. These results highlight 3D flow as a\npowerful structural prior for learning generalizable visuomotor policies,\nsupporting the development of more robust and versatile robotic manipulation.\nRobot demonstrations, additional results, and code can be found at\nhttps://sites.google.com/view/3dfdp/home.", "AI": {"tldr": "本文提出了3D流扩散策略（3D FDP），利用场景级3D流作为中间表示来捕获精细的局部运动线索，从而学习鲁棒且可泛化的视觉运动策略，在模拟和真实机器人任务中均表现出色。", "motivation": "机器人操作中学习鲁棒且能泛化到多样物体和交互动态的视觉运动策略是一个核心挑战。现有方法常忽视精确和接触密集型操作所需的局部运动线索。", "method": "本文提出了3D流扩散策略（3D FDP），该框架利用场景级3D流作为结构化的中间表示来捕获精细的局部运动线索。它预测采样查询点的时间轨迹，并在统一的扩散架构中根据这些交互感知流生成动作，从而将操作基于局部动态，并能推理更广泛的场景级动作后果。", "result": "在MetaWorld基准测试中，3D FDP在50项任务中取得了最先进的性能，尤其在中等和困难设置下表现突出。在八项真实机器人任务中，它持续优于现有基线，特别是在接触密集和非抓取场景中。", "conclusion": "研究结果表明，3D流是学习可泛化视觉运动策略的强大结构先验，支持开发更鲁棒和多功能的机器人操作能力。"}}
{"id": "2509.18535", "categories": ["cs.CL", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.18535", "abs": "https://arxiv.org/abs/2509.18535", "authors": ["Mo Mu", "Dianqiao Lei", "Chang Li"], "title": "Trace Is In Sentences: Unbiased Lightweight ChatGPT-Generated Text Detector", "comment": null, "summary": "The widespread adoption of ChatGPT has raised concerns about its misuse,\nhighlighting the need for robust detection of AI-generated text. Current\nword-level detectors are vulnerable to paraphrasing or simple prompts (PSP),\nsuffer from biases induced by ChatGPT's word-level patterns (CWP) and training\ndata content, degrade on modified text, and often require large models or\nonline LLM interaction. To tackle these issues, we introduce a novel task to\ndetect both original and PSP-modified AI-generated texts, and propose a\nlightweight framework that classifies texts based on their internal structure,\nwhich remains invariant under word-level changes. Our approach encodes sentence\nembeddings from pre-trained language models and models their relationships via\nattention. We employ contrastive learning to mitigate embedding biases from\nautoregressive generation and incorporate a causal graph with counterfactual\nmethods to isolate structural features from topic-related biases. Experiments\non two curated datasets, including abstract comparisons and revised life FAQs,\nvalidate the effectiveness of our method.", "AI": {"tldr": "本文提出一个轻量级框架，通过分析文本的内部结构来检测原始及经复述修改的AI生成文本，以解决现有检测器对复述的脆弱性和偏见问题。", "motivation": "ChatGPT的广泛应用引发了对其滥用的担忧，急需鲁棒的AI生成文本检测方法。现有词级检测器易受复述和简单提示（PSP）攻击，存在ChatGPT词级模式（CWP）和训练数据内容导致的偏见，对修改文本性能下降，且常需大型模型或在线LLM交互。", "method": "引入了一个检测原始及PSP修改的AI生成文本的新任务。提出了一个轻量级框架，通过文本内部结构进行分类，该结构在词级变化下保持不变。方法包括：从预训练语言模型编码句子嵌入，通过注意力机制建模它们的关系；采用对比学习减轻自回归生成引起的嵌入偏见；结合因果图和反事实方法，将结构特征与主题相关偏见分离。", "result": "在两个精选数据集（包括摘要比较和修订后的生活常见问题解答）上的实验验证了该方法的有效性。", "conclusion": "所提出的轻量级框架通过关注文本的内部结构并缓解偏见，能有效检测包括复述修改在内的AI生成文本，解决了现有检测器的诸多局限性。"}}
{"id": "2509.18626", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18626", "abs": "https://arxiv.org/abs/2509.18626", "authors": ["Jay Patrikar", "Apoorva Sharma", "Sushant Veer", "Boyi Li", "Sebastian Scherer", "Marco Pavone"], "title": "The Case for Negative Data: From Crash Reports to Counterfactuals for Reasonable Driving", "comment": "8 pages, 5 figures", "summary": "Learning-based autonomous driving systems are trained mostly on incident-free\ndata, offering little guidance near safety-performance boundaries. Real crash\nreports contain precisely the contrastive evidence needed, but they are hard to\nuse: narratives are unstructured, third-person, and poorly grounded to sensor\nviews. We address these challenges by normalizing crash narratives to\nego-centric language and converting both logs and crashes into a unified\nscene-action representation suitable for retrieval. At decision time, our\nsystem adjudicates proposed actions by retrieving relevant precedents from this\nunified index; an agentic counterfactual extension proposes plausible\nalternatives, retrieves for each, and reasons across outcomes before deciding.\nOn a nuScenes benchmark, precedent retrieval substantially improves\ncalibration, with recall on contextually preferred actions rising from 24% to\n53%. The counterfactual variant preserves these gains while sharpening\ndecisions near risk.", "AI": {"tldr": "本文提出一种方法，通过将真实事故报告转换为统一的场景-动作表示，并结合反事实推理，来增强自动驾驶系统在安全边界附近的决策能力和校准性。", "motivation": "自动驾驶系统主要基于无事故数据训练，在安全性能边界附近缺乏指导。真实的碰撞报告包含了必要的对比证据，但其非结构化、第三人称且与传感器视图关联性差的叙述，使其难以直接利用。", "method": "研究方法包括将碰撞叙述规范化为以自我为中心的语言，并将日志和碰撞数据转换为统一的场景-动作表示，以进行检索。在决策时，系统通过从统一索引中检索相关先例来裁决提议的动作；一个代理反事实扩展会提出合理的替代方案，为每个方案检索先例，并在做出决定前推理不同结果。", "result": "在nuScenes基准测试中，先例检索显著提高了校准性，上下文偏好动作的召回率从24%提高到53%。反事实变体在保持这些增益的同时，在风险临近时能使决策更加清晰。", "conclusion": "通过将碰撞报告整合到统一的场景-动作表示中进行先例检索，并结合反事实推理，可以显著改善自动驾驶系统在安全边界附近的决策能力和校准性，尤其是在高风险情境下。"}}
{"id": "2509.18420", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18420", "abs": "https://arxiv.org/abs/2509.18420", "authors": ["Nikolai Skripko"], "title": "Instruction-Following Evaluation in Function Calling for Large Language Models", "comment": null, "summary": "Function calling is a core capability of large language models, essential for\nAI agents. Existing benchmarks such as the Berkeley Function Calling\nLeaderboard (BFCL), tau^2-Bench (arXiv:2506.07982), and ACEBench\n(arXiv:2501.12851) evaluate argument correctness but do not test adherence to\nformat instructions embedded in parameter descriptions, such as enclosing\nvalues in double quotes or using ISO date formats.\n  We introduce IFEval-FC, a benchmark inspired by IFEval (arXiv:2311.07911)\nthat assesses precise instruction following in function calling. IFEval-FC\nencodes verifiable formats directly within JSON schema descriptions, for\nexample specifying that a value must not contain punctuation. It includes 750\ntest cases, each consisting of a function with an embedded format for one of\nits input parameters and a corresponding user query. Evaluation is fully\nalgorithmic, ensuring objectivity, reproducibility, and scalability.\n  Our results show that even state-of-the-art proprietary models, including\nGPT-5 and Claude 4.1 Opus, frequently fail to follow basic formatting rules,\nhighlighting a practical limitation for real-world agent systems. The complete\ncodebase and data are publicly available at\nhttps://github.com/Skripkon/IFEval-FC.", "AI": {"tldr": "IFEval-FC是一个评估大语言模型函数调用中精确指令遵循（特别是格式要求）的新基准，发现现有最先进模型在此方面表现不佳。", "motivation": "现有函数调用基准（如BFCL、tau^2-Bench、ACEBench）主要评估参数正确性，但未能测试模型对嵌入在参数描述中的格式指令（如双引号、ISO日期格式）的遵循情况，这在实际AI代理系统中是一个重要限制。", "method": "本文引入了IFEval-FC基准，灵感来源于IFEval。它将可验证的格式直接编码在JSON schema描述中（例如，指定值不能包含标点符号）。该基准包含750个测试用例，每个用例包括一个函数，其中一个输入参数嵌入了格式要求，以及一个相应的用户查询。评估过程完全是算法化的，确保客观性、可复现性和可扩展性。", "result": "评估结果表明，即使是包括GPT-5和Claude 4.1 Opus在内的最先进的专有模型，也经常未能遵循基本的格式规则。", "conclusion": "模型在函数调用中未能遵循基本格式规则，这突出了当前大语言模型在构建真实世界代理系统时的一个实际限制。"}}
{"id": "2509.18369", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18369", "abs": "https://arxiv.org/abs/2509.18369", "authors": ["Riad Ahmed Anonto", "Sardar Md. Saffat Zabin", "M. Saifur Rahman"], "title": "Align Where the Words Look: Cross-Attention-Guided Patch Alignment with Contrastive and Transport Regularization for Bengali Captioning", "comment": null, "summary": "Grounding vision--language models in low-resource languages remains\nchallenging, as they often produce fluent text about the wrong objects. This\nstems from scarce paired data, translation pivots that break alignment, and\nEnglish-centric pretraining that ignores target-language semantics. We address\nthis with a compute-aware Bengali captioning pipeline trained on LaBSE-verified\nEN--BN pairs and 110k bilingual-prompted synthetic images. A frozen MaxViT\nyields stable visual patches, a Bengali-native mBART-50 decodes, and a\nlightweight bridge links the modalities. Our core novelty is a tri-loss\nobjective: Patch-Alignment Loss (PAL) aligns real and synthetic patch\ndescriptors using decoder cross-attention, InfoNCE enforces global\nreal--synthetic separation, and Sinkhorn-based OT ensures balanced fine-grained\npatch correspondence. This PAL+InfoNCE+OT synergy improves grounding, reduces\nspurious matches, and drives strong gains on Flickr30k-1k (BLEU-4 12.29, METEOR\n27.98, BERTScore-F1 71.20) and MSCOCO-1k (BLEU-4 12.00, METEOR 28.14,\nBERTScore-F1 75.40), outperforming strong CE baselines and narrowing the\nreal--synthetic centroid gap by 41%.", "AI": {"tldr": "该研究提出了一种计算感知的孟加拉语图像字幕生成管道，通过LaBSE验证数据和合成图像进行训练，并引入三元损失函数（PAL+InfoNCE+OT）来改善低资源语言视觉-语言模型的接地能力和减少错误匹配。", "motivation": "在低资源语言中，视觉-语言模型（VLM）的接地（grounding）能力面临挑战，表现为模型常对错误物体生成流畅文本。这源于配对数据稀缺、翻译枢轴破坏对齐以及以英语为中心的预训练忽略目标语言语义等问题。", "method": "该方法构建了一个计算感知的孟加拉语字幕生成管道，使用LaBSE验证的英-孟配对数据和11万张双语提示合成图像进行训练。管道包括：一个冻结的MaxViT用于生成稳定的视觉补丁、一个孟加拉语原生的mBART-50解码器，以及一个轻量级桥接模块连接模态。核心创新是三元损失函数：补丁对齐损失（PAL）使用解码器交叉注意力对齐真实和合成补丁描述符；InfoNCE强制全局真实-合成分离；基于Sinkhorn的OT确保平衡的细粒度补丁对应。", "result": "PAL+InfoNCE+OT的协同作用显著改善了接地能力，减少了虚假匹配。在Flickr30k-1k上，BLEU-4达到12.29，METEOR达到27.98，BERTScore-F1达到71.20；在MSCOCO-1k上，BLEU-4达到12.00，METEOR达到28.14，BERTScore-F1达到75.40。这些结果优于强大的CE基线，并将真实-合成质心差距缩小了41%。", "conclusion": "该研究通过引入创新的三元损失函数和优化的管道，有效解决了低资源语言中视觉-语言模型的接地挑战，显著提高了孟加拉语图像字幕生成的性能，并减少了模型中的错误关联。"}}
{"id": "2509.19246", "categories": ["cs.RO", "cs.MA", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.19246", "abs": "https://arxiv.org/abs/2509.19246", "authors": ["Sinan Oğuz", "Emanuele Garone", "Marco Dorigo", "Mary Katherine Heinrich"], "title": "Proactive-reactive detection and mitigation of intermittent faults in robot swarms", "comment": null, "summary": "Intermittent faults are transient errors that sporadically appear and\ndisappear. Although intermittent faults pose substantial challenges to\nreliability and coordination, existing studies of fault tolerance in robot\nswarms focus instead on permanent faults. One reason for this is that\nintermittent faults are prohibitively difficult to detect in the fully\nself-organized ad-hoc networks typical of robot swarms, as their network\ntopologies are transient and often unpredictable. However, in the recently\nintroduced self-organizing nervous systems (SoNS) approach, robot swarms are\nable to self-organize persistent network structures for the first time, easing\nthe problem of detecting intermittent faults. To address intermittent faults in\nrobot swarms that have persistent networks, we propose a novel\nproactive-reactive strategy to detection and mitigation, based on\nself-organized backup layers and distributed consensus in a multiplex network.\nProactively, the robots self-organize dynamic backup paths before faults occur,\nadapting to changes in the primary network topology and the robots' relative\npositions. Reactively, robots use one-shot likelihood ratio tests to compare\ninformation received along different paths in the multiplex network, enabling\nearly fault detection. Upon detection, communication is temporarily rerouted in\na self-organized way, until the detected fault resolves. We validate the\napproach in representative scenarios of faulty positional data occurring during\nformation control, demonstrating that intermittent faults are prevented from\ndisrupting convergence to desired formations, with high fault detection\naccuracy and low rates of false positives.", "AI": {"tldr": "本文提出了一种针对具有持久网络的机器人群间歇性故障的主动-被动策略，利用自组织备份层和多路网络中的分布式共识进行检测和缓解，有效防止故障中断队形控制。", "motivation": "现有机器人群容错研究主要关注永久性故障而非间歇性故障，因为在自组织临时网络中检测间歇性故障极其困难。自组织神经系统（SoNS）方法允许机器人群自组织持久网络结构，从而简化了间歇性故障的检测问题。", "method": "该方法是一种主动-被动策略，基于自组织备份层和多路网络中的分布式共识：\n1. **主动**：在故障发生前，机器人自组织动态备份路径，以适应主网络拓扑和机器人相对位置的变化。\n2. **被动**：机器人利用一次性似然比检验比较多路网络中沿不同路径接收到的信息，实现早期故障检测。\n3. **缓解**：检测到故障后，通信会以自组织方式暂时重新路由，直到检测到的故障解决。", "result": "该方法在队形控制中出现故障定位数据的代表性场景中得到验证，结果表明：\n- 间歇性故障被有效阻止，未中断收敛到期望队形。\n- 故障检测准确率高。\n- 误报率低。", "conclusion": "所提出的主动-被动策略能够有效检测和缓解具有持久网络的机器人群中的间歇性故障，确保了在故障情况下的鲁棒性收敛和高准确性检测。"}}
{"id": "2509.18536", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18536", "abs": "https://arxiv.org/abs/2509.18536", "authors": ["Jin Young Kim", "Ji Won Yoon"], "title": "CCQA: Generating Question from Solution Can Improve Inference-Time Reasoning in SLMs", "comment": "Published as a main conference paper at EMNLP 2025", "summary": "Recently, inference-time reasoning strategies have further improved the\naccuracy of large language models (LLMs), but their effectiveness on smaller\nmodels remains unclear. Based on the observation that conventional approaches\noften fail to improve performance in this context, we propose\n\\textbf{C}ycle-\\textbf{C}onsistency in \\textbf{Q}uestion \\textbf{A}nswering\n(CCQA), a novel reasoning method that can be effectively applied to SLMs.\nInspired by cycle consistency, CCQA generates a question from each reasoning\npath and answer, evaluates each by its similarity to the original question, and\nthen selects the candidate solution with the highest similarity score as the\nfinal response. Since conventional SLMs struggle to generate accurate questions\nfrom their own reasoning paths and answers, we employ a lightweight Flan-T5\nmodel specialized for question generation to support this process efficiently.\nFrom the experimental results, it is verified that CCQA consistently\noutperforms existing state-of-the-art (SOTA) methods across eight models on\nmathematical and commonsense reasoning benchmarks. Furthermore, our method\nestablishes a new practical baseline for efficient reasoning in SLMs. Source\ncode can be found at https://github.com/scai-research/ccqa_official.", "AI": {"tldr": "本文提出了一种名为CCQA的新型推理方法，通过循环一致性原理，利用问题生成和相似度评估，显著提升了小型语言模型（SLMs）在数学和常识推理任务上的表现。", "motivation": "推理策略能提升大型语言模型（LLMs）的准确性，但其在小型模型（SLMs）上的有效性尚不明确，且传统方法在此背景下往往无法改善性能。", "method": "CCQA方法受循环一致性启发，从每个推理路径和答案生成一个问题，通过评估生成问题与原始问题的相似度来选择最佳候选解决方案。为克服SLMs自身生成准确问题的困难，CCQA采用了一个轻量级的Flan-T5模型专门用于高效的问题生成。", "result": "实验结果表明，CCQA在数学和常识推理基准测试中，始终优于现有最先进（SOTA）方法，并在八个不同模型上得到了验证。此外，该方法为SLMs的有效推理建立了一个新的实用基线。", "conclusion": "CCQA是一种对小型语言模型有效的推理方法，它通过引入循环一致性原则和辅助问题生成模型，显著提升了SLMs在推理任务上的性能，并为SLMs的推理提供了一个新的实用基线。"}}
{"id": "2509.18631", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18631", "abs": "https://arxiv.org/abs/2509.18631", "authors": ["Shuo Cheng", "Liqian Ma", "Zhenyang Chen", "Ajay Mandlekar", "Caelan Garrett", "Danfei Xu"], "title": "Generalizable Domain Adaptation for Sim-and-Real Policy Co-Training", "comment": null, "summary": "Behavior cloning has shown promise for robot manipulation, but real-world\ndemonstrations are costly to acquire at scale. While simulated data offers a\nscalable alternative, particularly with advances in automated demonstration\ngeneration, transferring policies to the real world is hampered by various\nsimulation and real domain gaps. In this work, we propose a unified\nsim-and-real co-training framework for learning generalizable manipulation\npolicies that primarily leverages simulation and only requires a few real-world\ndemonstrations. Central to our approach is learning a domain-invariant,\ntask-relevant feature space. Our key insight is that aligning the joint\ndistributions of observations and their corresponding actions across domains\nprovides a richer signal than aligning observations (marginals) alone. We\nachieve this by embedding an Optimal Transport (OT)-inspired loss within the\nco-training framework, and extend this to an Unbalanced OT framework to handle\nthe imbalance between abundant simulation data and limited real-world examples.\nWe validate our method on challenging manipulation tasks, showing it can\nleverage abundant simulation data to achieve up to a 30% improvement in the\nreal-world success rate and even generalize to scenarios seen only in\nsimulation.", "AI": {"tldr": "本文提出一个统一的模拟-现实协同训练框架，通过学习领域不变特征和使用最优传输（OT）损失对齐观察-动作联合分布，以少量真实世界演示提升机器人操作策略的现实成功率和泛化能力。", "motivation": "机器人操作的行为克隆需要大量真实世界演示，但其获取成本高昂。虽然模拟数据提供了可扩展的替代方案，但模拟与现实之间的领域差异阻碍了策略的有效迁移。", "method": "提出一个统一的模拟-现实协同训练框架，主要利用模拟数据，仅需少量真实世界演示。核心在于学习一个领域不变、任务相关的特征空间。通过对齐跨领域的观察和对应动作的联合分布，而非仅对齐观察（边缘分布），来获取更丰富的信号。为此，在协同训练框架中嵌入了受最优传输（OT）启发的损失，并将其扩展到非平衡OT框架以处理模拟数据和真实世界数据之间的不平衡。", "result": "在具有挑战性的操作任务上验证了该方法，结果显示它能有效利用丰富的模拟数据，将真实世界的成功率提高高达30%，甚至能泛化到仅在模拟中出现过的场景。", "conclusion": "所提出的模拟-现实协同训练框架，结合领域不变特征学习和基于最优传输的联合分布对齐，能够以少量真实世界演示有效利用模拟数据，显著提升机器人操作策略在现实世界中的成功率和泛化能力，有效弥合了模拟与现实之间的领域鸿沟。"}}
{"id": "2509.18436", "categories": ["cs.AI", "cs.CL", "cs.DB"], "pdf": "https://arxiv.org/pdf/2509.18436", "abs": "https://arxiv.org/abs/2509.18436", "authors": ["Hongda Jiang", "Xinyuan Zhang", "Siddhant Garg", "Rishab Arora", "Shiun-Zu Kuo", "Jiayang Xu", "Christopher Brossman", "Yue Liu", "Aaron Colak", "Ahmed Aly", "Anuj Kumar", "Xin Luna Dong"], "title": "Memory-QA: Answering Recall Questions Based on Multimodal Memories", "comment": null, "summary": "We introduce Memory-QA, a novel real-world task that involves answering\nrecall questions about visual content from previously stored multimodal\nmemories. This task poses unique challenges, including the creation of\ntask-oriented memories, the effective utilization of temporal and location\ninformation within memories, and the ability to draw upon multiple memories to\nanswer a recall question. To address these challenges, we propose a\ncomprehensive pipeline, Pensieve, integrating memory-specific augmentation,\ntime- and location-aware multi-signal retrieval, and multi-memory QA\nfine-tuning. We created a multimodal benchmark to illustrate various real\nchallenges in this task, and show the superior performance of Pensieve over\nstate-of-the-art solutions (up to 14% on QA accuracy).", "AI": {"tldr": "本文提出了Memory-QA任务，旨在从多模态记忆中回答回忆性问题。为解决该任务的挑战，作者提出了Pensieve管道，并在新创建的基准测试中，相较于现有最佳方案，问答准确率提升高达14%。", "motivation": "现有研究未能有效解决从存储的多模态记忆中回答回忆性问题。此任务面临独特挑战，包括创建任务导向记忆、有效利用记忆中的时空信息，以及整合多个记忆来回答回忆性问题。", "method": "提出了一个名为Pensieve的综合管道，该管道集成了记忆特异性增强、时间-位置感知的多信号检索，以及多记忆问答微调。同时，创建了一个多模态基准来展示任务中的各种实际挑战。", "result": "Pensieve在所创建的多模态基准测试中，相比于现有最先进的解决方案，问答准确率提升高达14%。", "conclusion": "Pensieve管道能有效应对Memory-QA任务中的独特挑战，并在从先前存储的多模态记忆中回答回忆性问题方面展现出优越的性能。"}}
{"id": "2509.18372", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18372", "abs": "https://arxiv.org/abs/2509.18372", "authors": ["Reeshad Khan", "John Gauch"], "title": "TinyBEV: Cross Modal Knowledge Distillation for Efficient Multi Task Bird's Eye View Perception and Planning", "comment": null, "summary": "We present TinyBEV, a unified, camera only Bird's Eye View (BEV) framework\nthat distills the full-stack capabilities of a large planning-oriented teacher\n(UniAD [19]) into a compact, real-time student model. Unlike prior efficient\ncamera only baselines such as VAD[23] and VADv2[7], TinyBEV supports the\ncomplete autonomy stack 3D detection, HD-map segmentation, motion forecasting,\noccupancy prediction, and goal-directed planning within a streamlined\n28M-parameter backbone, achieving a 78% reduction in parameters over UniAD\n[19]. Our model-agnostic, multi-stage distillation strategy combines\nfeature-level, output-level, and adaptive region-aware supervision to\neffectively transfer high-capacity multi-modal knowledge to a lightweight BEV\nrepresentation. On nuScenes[4], Tiny-BEV achieves 39.0 mAP for detection, 1.08\nminADE for motion forecasting, and a 0.32 collision rate, while running 5x\nfaster (11 FPS) and requiring only camera input. These results demonstrate that\nfull-stack driving intelligence can be retained in resource-constrained\nsettings, bridging the gap between large-scale, multi-modal perception-planning\nmodels and deployment-ready real-time autonomy.", "AI": {"tldr": "TinyBEV是一个紧凑、实时、纯摄像头的鸟瞰图（BEV）框架，它将大型规划导向型教师模型（UniAD）的全栈自动驾驶能力蒸馏到一个参数量为28M的学生模型中，在nuScenes数据集上实现了高性能，同时运行速度快5倍。", "motivation": "现有的高效纯摄像头基线模型（如VAD、VADv2）不支持完整的自动驾驶堆栈。研究动机在于弥合大型多模态感知-规划模型与资源受限环境下可部署的实时自动驾驶系统之间的差距，证明在资源受限的环境中也能保留全栈驾驶智能。", "method": "TinyBEV是一个统一的纯摄像头BEV框架，它将大型规划导向型教师模型（UniAD）的全栈能力（包括3D检测、高清地图分割、运动预测、占用率预测和目标导向规划）蒸馏到一个紧凑的实时学生模型中。该模型采用28M参数的主干网络，比UniAD减少了78%的参数。其模型无关的多阶段蒸馏策略结合了特征级、输出级和自适应区域感知监督，以有效地将高容量的多模态知识转移到轻量级的BEV表示中。", "result": "在nuScenes数据集上，TinyBEV在检测方面达到39.0 mAP，运动预测方面达到1.08 minADE，碰撞率为0.32。同时，它运行速度快5倍（11 FPS），并且仅需要摄像头输入。", "conclusion": "这项研究证明，在资源受限的环境中可以保留全栈驾驶智能，成功弥合了大型多模态感知-规划模型与可部署的实时自动驾驶系统之间的差距。"}}
{"id": "2509.18577", "categories": ["cs.CL", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2509.18577", "abs": "https://arxiv.org/abs/2509.18577", "authors": ["Yeongbin Seo", "Gayoung Kim", "Jaehyung Kim", "Jinyoung Yeo"], "title": "Prior-based Noisy Text Data Filtering: Fast and Strong Alternative For Perplexity", "comment": null, "summary": "As large language models (LLMs) are pretrained on massive web corpora,\ncareful selection of data becomes essential to ensure effective and efficient\nlearning. While perplexity (PPL)-based filtering has shown strong performance,\nit suffers from drawbacks: substantial time costs and inherent unreliability of\nthe model when handling noisy or out-of-distribution samples. In this work, we\npropose a simple yet powerful alternative: a prior-based data filtering method\nthat estimates token priors using corpus-level term frequency statistics,\ninspired by linguistic insights on word roles and lexical density. Our approach\nfilters documents based on the mean and standard deviation of token priors,\nserving as a fast proxy to PPL while requiring no model inference. Despite its\nsimplicity, the prior-based filter achieves the highest average performance\nacross 20 downstream benchmarks, while reducing time cost by over 1000x\ncompared to PPL-based filtering. We further demonstrate its applicability to\nsymbolic languages such as code and math, and its dynamic adaptability to\nmultilingual corpora without supervision", "AI": {"tldr": "本文提出了一种基于词元先验的LLM数据过滤方法，该方法利用语料库级别的词频统计来估计词元先验，作为PPL的快速替代。它在性能上优于PPL，且时间成本降低了1000倍以上。", "motivation": "大型语言模型（LLMs）的预训练数据选择至关重要，但基于困惑度（PPL）的过滤方法存在时间成本高昂和处理噪声或分布外样本时不可靠的缺点。", "method": "受语言学中词语角色和词汇密度启发的见解，研究者提出了一种基于先验的数据过滤方法。该方法使用语料库级别的词频统计来估计词元先验，并根据词元先验的均值和标准差来过滤文档。它无需模型推理，可作为PPL的快速替代。", "result": "该基于先验的过滤器在20个下游基准测试中取得了最高的平均性能，同时与基于PPL的过滤方法相比，时间成本降低了1000倍以上。此外，它还适用于代码和数学等符号语言，并能动态适应多语言语料库而无需监督。", "conclusion": "基于词元先验的数据过滤方法是一种简单而强大的替代方案，它能有效且高效地选择LLM预训练数据，解决了PPL方法的局限性，并具有广泛的适用性。"}}
{"id": "2509.18636", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18636", "abs": "https://arxiv.org/abs/2509.18636", "authors": ["Yuan Zhou", "Jialiang Hou", "Guangtong Xu", "Fei Gao"], "title": "Number Adaptive Formation Flight Planning via Affine Deformable Guidance in Narrow Environments", "comment": null, "summary": "Formation maintenance with varying number of drones in narrow environments\nhinders the convergence of planning to the desired configurations. To address\nthis challenge, this paper proposes a formation planning method guided by\nDeformable Virtual Structures (DVS) with continuous spatiotemporal\ntransformation. Firstly, to satisfy swarm safety distance and preserve\nformation shape filling integrity for irregular formation geometries, we employ\nLloyd algorithm for uniform $\\underline{PA}$rtitioning and Hungarian algorithm\nfor $\\underline{AS}$signment (PAAS) in DVS. Subsequently, a spatiotemporal\ntrajectory involving DVS is planned using primitive-based path search and\nnonlinear trajectory optimization. The DVS trajectory achieves adaptive\ntransitions with respect to a varying number of drones while ensuring\nadaptability to narrow environments through affine transformation. Finally,\neach agent conducts distributed trajectory planning guided by desired\nspatiotemporal positions within the DVS, while incorporating collision\navoidance and dynamic feasibility requirements. Our method enables up to 15\\%\nof swarm numbers to join or leave in cluttered environments while rapidly\nrestoring the desired formation shape in simulation. Compared to cutting-edge\nformation planning method, we demonstrate rapid formation recovery capacity and\nenvironmental adaptability. Real-world experiments validate the effectiveness\nand resilience of our formation planning method.", "AI": {"tldr": "本文提出了一种基于可变形虚拟结构（DVS）的无人机编队规划方法，通过连续时空变换，解决了在狭窄环境中无人机数量变化时编队难以收敛的问题，实现了快速编队恢复和环境适应性。", "motivation": "在狭窄环境中，无人机数量变化时的编队维护会阻碍规划收敛到期望的配置，这是一个亟待解决的挑战。", "method": "1. **DVS与PAAS集成：** 采用可变形虚拟结构（DVS），并通过Lloyd算法进行均匀分区（PA）和匈牙利算法进行分配（AS），即PAAS，以满足群集安全距离并保持不规则编队形状的完整性。 2. **DVS时空轨迹规划：** 利用基于基元的路径搜索和非线性轨迹优化，规划涉及DVS的时空轨迹。该DVS轨迹通过仿射变换实现对无人机数量变化的自适应过渡，并确保对狭窄环境的适应性。 3. **分布式智能体规划：** 每个智能体在DVS内期望的时空位置引导下，进行分布式轨迹规划，同时考虑避碰和动态可行性要求。", "result": "该方法在模拟中能够处理多达15%的无人机加入或离开拥挤环境，并能快速恢复期望的编队形状。与现有先进的编队规划方法相比，本文方法展示了更快的编队恢复能力和环境适应性。实际实验验证了该编队规划方法的有效性和鲁棒性。", "conclusion": "本文提出的基于可变形虚拟结构和PAAS的编队规划方法，通过连续时空变换，能够有效解决无人机数量变化和狭窄环境下的编队维护问题，实现快速编队恢复和优异的环境适应性。"}}
{"id": "2509.18527", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18527", "abs": "https://arxiv.org/abs/2509.18527", "authors": ["Ziwen Chen", "Zhong Wang"], "title": "FERA: Foil Fencing Referee Assistant Using Pose-Based Multi-Label Move Recognition and Rule Reasoning", "comment": null, "summary": "The sport of fencing, like many other sports, faces challenges in refereeing:\nsubjective calls, human errors, bias, and limited availability in practice\nenvironments. We present FERA (Fencing Referee Assistant), a prototype AI\nreferee for foil fencing which integrates pose-based multi-label action\nrecognition and rule-based reasoning. FERA extracts 2D joint positions from\nvideo, normalizes them, computes a 101-dimensional kinematic feature set, and\napplies a Transformer for multi-label move and blade classification. To\ndetermine priority and scoring, FERA applies a distilled language model with\nencoded right-of-way rules, producing both a decision and an explanation for\neach exchange. With limited hand-labeled data, a 5-fold cross-validation\nachieves an average macro-F1 score of 0.549, outperforming multiple baselines,\nincluding a Temporal Convolutional Network (TCN), BiLSTM, and a vanilla\nTransformer. While not ready for deployment, these results demonstrate a\npromising path towards automated referee assistance in foil fencing and new\nopportunities for AI applications, such as coaching in the field of fencing.", "AI": {"tldr": "FERA是一个原型AI裁判系统，用于花剑击剑，它结合了基于姿态的多标签动作识别和基于规则的推理，以解决裁判主观性、错误和可用性等挑战。", "motivation": "击剑运动（以及其他许多运动）在裁判方面面临挑战，包括主观判断、人为错误、偏见以及在训练环境中裁判可用性有限的问题。", "method": "FERA从视频中提取2D关节位置并进行归一化，计算101维运动学特征集，然后使用Transformer进行多标签动作和剑位分类。为了确定优先权和得分，FERA应用了一个经过蒸馏的语言模型，其中编码了优先权规则，为每次交锋生成判决和解释。", "result": "在有限的手动标记数据下，通过5折交叉验证，FERA取得了0.549的平均宏观F1分数，优于包括时间卷积网络（TCN）、BiLSTM和普通Transformer在内的多个基线模型。系统能够生成判决和解释。", "conclusion": "虽然尚未准备好部署，但这些结果展示了在花剑击剑中实现自动化裁判辅助的 promising 途径，并为AI应用（如击剑领域的教练）提供了新机会。"}}
{"id": "2509.18387", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18387", "abs": "https://arxiv.org/abs/2509.18387", "authors": ["Thomas Gossard", "Filip Radovic", "Andreas Ziegler", "Andrea Zell"], "title": "BlurBall: Joint Ball and Motion Blur Estimation for Table Tennis Ball Tracking", "comment": null, "summary": "Motion blur reduces the clarity of fast-moving objects, posing challenges for\ndetection systems, especially in racket sports, where balls often appear as\nstreaks rather than distinct points. Existing labeling conventions mark the\nball at the leading edge of the blur, introducing asymmetry and ignoring\nvaluable motion cues correlated with velocity. This paper introduces a new\nlabeling strategy that places the ball at the center of the blur streak and\nexplicitly annotates blur attributes. Using this convention, we release a new\ntable tennis ball detection dataset. We demonstrate that this labeling approach\nconsistently enhances detection performance across various models. Furthermore,\nwe introduce BlurBall, a model that jointly estimates ball position and motion\nblur attributes. By incorporating attention mechanisms such as\nSqueeze-and-Excitation over multi-frame inputs, we achieve state-of-the-art\nresults in ball detection. Leveraging blur not only improves detection accuracy\nbut also enables more reliable trajectory prediction, benefiting real-time\nsports analytics.", "AI": {"tldr": "本文提出一种新的标注策略，将快速运动模糊的球标注在模糊条纹中心并加入模糊属性，发布了新的乒乓球检测数据集，并引入了BlurBall模型，通过利用模糊信息显著提高了球的检测性能和轨迹预测可靠性。", "motivation": "运动模糊降低了快速移动物体的清晰度，对检测系统构成挑战，尤其是在球类运动中，球常表现为条纹而非清晰点。现有标注约定将球标记在模糊前沿，引入不对称性并忽略了与速度相关的有价值的运动线索。", "method": "引入新的标注策略，将球标记在模糊条纹的中心，并明确标注模糊属性。基于此约定，发布了一个新的乒乓球检测数据集。提出BlurBall模型，该模型联合估计球的位置和运动模糊属性，通过在多帧输入上结合Squeeze-and-Excitation等注意力机制。", "result": "新的标注方法持续提升了各种模型的检测性能。BlurBall模型在球检测方面达到了最先进的水平。利用模糊信息不仅提高了检测精度，还使轨迹预测更加可靠。", "conclusion": "通过引入新的模糊球标注策略和BlurBall模型，该研究有效解决了运动模糊对球检测的挑战，显著提升了检测准确性和轨迹预测的可靠性，对实时体育分析具有重要意义。"}}
{"id": "2509.18585", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18585", "abs": "https://arxiv.org/abs/2509.18585", "authors": ["Yu Chen", "Yifei Han", "Long Zhang", "Yue Du", "Bin Li"], "title": "TsqLoRA: Towards Sensitivity and Quality Low-Rank Adaptation for Efficient Fine-Tuning", "comment": "5 pages, 4 figures, published to ICASSP2026", "summary": "Fine-tuning large pre-trained models for downstream tasks has become a\nfundamental approach in natural language processing. Fully fine-tuning all\nmodel parameters is computationally expensive and memory-intensive, especially\nin resource-constrained environments. Existing parameter-efficient fine-tuning\nmethods reduce the number of trainable parameters but typically overlook the\nvarying sensitivity of different model layers and the importance of training\ndata. In this work, we propose TsqLoRA, a novel method that integrates\ndata-quality-driven selection with sensitivity-aware low-rank adaptation,\nconsisted of two main components: a quality-aware sampling mechanism for\nselecting the most informative training data, and a dynamic rank allocation\nmodule that adjusts the rank of each layer based on its sensitivity to\nparameter updates. The experimental results demonstrate that TsqLoRA improves\nfine-tuning efficiency while maintaining or even improving performance on a\nvariety of NLP tasks. Our code will be available at\nhttps://github.com/Benjamin-Ricky/TsqLoRA.", "AI": {"tldr": "TsqLoRA是一种新颖的参数高效微调方法，它结合了数据质量驱动的采样和敏感度感知的低秩适应，以提高微调效率并保持或提升性能。", "motivation": "在资源受限的环境中，对大型预训练模型进行完全微调计算成本高且内存密集。现有的参数高效微微调方法虽然减少了可训练参数，但通常忽略了模型不同层的敏感性差异以及训练数据的重要性。", "method": "本文提出了TsqLoRA，包含两个主要组件：一个质量感知的采样机制，用于选择信息最丰富的训练数据；一个动态秩分配模块，根据每层对参数更新的敏感性调整其秩。", "result": "实验结果表明，TsqLoRA在提高微调效率的同时，在各种NLP任务上保持甚至提升了性能。", "conclusion": "TsqLoRA通过结合数据质量驱动的采样和敏感度感知的动态秩分配，有效解决了现有参数高效微调方法的局限性，实现了更高效且高性能的模型微调。"}}
{"id": "2509.18644", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18644", "abs": "https://arxiv.org/abs/2509.18644", "authors": ["Juntu Zhao", "Wenbo Lu", "Di Zhang", "Yufeng Liu", "Yushen Liang", "Tianluo Zhang", "Yifeng Cao", "Junyuan Xie", "Yingdong Hu", "Shengjie Wang", "Junliang Guo", "Dequan Wang", "Yang Gao"], "title": "Do You Need Proprioceptive States in Visuomotor Policies?", "comment": "Project page: https://statefreepolicy.github.io", "summary": "Imitation-learning-based visuomotor policies have been widely used in robot\nmanipulation, where both visual observations and proprioceptive states are\ntypically adopted together for precise control. However, in this study, we find\nthat this common practice makes the policy overly reliant on the proprioceptive\nstate input, which causes overfitting to the training trajectories and results\nin poor spatial generalization. On the contrary, we propose the State-free\nPolicy, removing the proprioceptive state input and predicting actions only\nconditioned on visual observations. The State-free Policy is built in the\nrelative end-effector action space, and should ensure the full task-relevant\nvisual observations, here provided by dual wide-angle wrist cameras. Empirical\nresults demonstrate that the State-free policy achieves significantly stronger\nspatial generalization than the state-based policy: in real-world tasks such as\npick-and-place, challenging shirt-folding, and complex whole-body manipulation,\nspanning multiple robot embodiments, the average success rate improves from 0\\%\nto 85\\% in height generalization and from 6\\% to 64\\% in horizontal\ngeneralization. Furthermore, they also show advantages in data efficiency and\ncross-embodiment adaptation, enhancing their practicality for real-world\ndeployment.", "AI": {"tldr": "本研究提出了一种“无状态策略”，仅依赖视觉观测来预测机器人动作，解决了传统仿人学习策略过度依赖本体感受状态导致空间泛化能力差的问题，并在多项实际任务中显著提高了泛化成功率。", "motivation": "研究发现，在基于模仿学习的机器人视觉运动策略中，同时使用视觉观测和本体感受状态会导致策略过度依赖本体感受状态输入，进而造成对训练轨迹的过拟合，并导致空间泛化能力差。", "method": "提出“无状态策略”，移除本体感受状态输入，仅根据视觉观测预测动作。该策略在相对末端执行器动作空间中构建，并利用双广角腕部摄像头提供完整的任务相关视觉观测。", "result": "经验结果表明，“无状态策略”比基于状态的策略具有显著更强的空间泛化能力：在抓取放置、叠衬衫和全身操纵等实际任务中，跨多种机器人实体，高度泛化成功率从0%提高到85%，水平泛化成功率从6%提高到64%。此外，该策略还在数据效率和跨实体适应性方面表现出优势。", "conclusion": "“无状态策略”通过仅依赖视觉观测，显著提升了机器人操纵任务的空间泛化能力、数据效率和跨实体适应性，使其更具实际部署价值。"}}
{"id": "2509.18557", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18557", "abs": "https://arxiv.org/abs/2509.18557", "authors": ["Tom Pawelek", "Raj Patel", "Charlotte Crowell", "Noorbakhsh Amiri", "Sudip Mittal", "Shahram Rahimi", "Andy Perkins"], "title": "LLMZ+: Contextual Prompt Whitelist Principles for Agentic LLMs", "comment": "7 pages, 5 figures, to be published and presented at ICMLA 2025", "summary": "Compared to traditional models, agentic AI represents a highly valuable\ntarget for potential attackers as they possess privileged access to data\nsources and API tools, which are traditionally not incorporated into classical\nagents. Unlike a typical software application residing in a Demilitarized Zone\n(DMZ), agentic LLMs consciously rely on nondeterministic behavior of the AI\n(only defining a final goal, leaving the path selection to LLM). This\ncharacteristic introduces substantial security risk to both operational\nsecurity and information security. Most common existing defense mechanism rely\non detection of malicious intent and preventing it from reaching the LLM agent,\nthus protecting against jailbreak attacks such as prompt injection. In this\npaper, we present an alternative approach, LLMZ+, which moves beyond\ntraditional detection-based approaches by implementing prompt whitelisting.\nThrough this method, only contextually appropriate and safe messages are\npermitted to interact with the agentic LLM. By leveraging the specificity of\ncontext, LLMZ+ guarantees that all exchanges between external users and the LLM\nconform to predefined use cases and operational boundaries. Our approach\nstreamlines the security framework, enhances its long-term resilience, and\nreduces the resources required for sustaining LLM information security. Our\nempirical evaluation demonstrates that LLMZ+ provides strong resilience against\nthe most common jailbreak prompts. At the same time, legitimate business\ncommunications are not disrupted, and authorized traffic flows seamlessly\nbetween users and the agentic LLM. We measure the effectiveness of approach\nusing false positive and false negative rates, both of which can be reduced to\n0 in our experimental setting.", "AI": {"tldr": "本文提出LLMZ+，一种基于提示词白名单的新型安全方法，用于保护具有代理能力的LLM免受越狱攻击，确保只有安全且符合上下文的消息能与LLM交互，并实现了零误报和零漏报。", "motivation": "与传统模型相比，代理式AI因其对数据源和API工具的特权访问以及非确定性行为，成为极具价值的攻击目标，带来重大的运营和信息安全风险。现有防御机制主要依赖恶意意图检测，但不足以有效应对提示注入等越狱攻击。", "method": "LLMZ+采用提示词白名单机制，而非传统的基于检测的方法。它只允许与上下文相关且安全的消息与代理式LLM交互，从而确保外部用户与LLM之间的所有交流都符合预定义的使用场景和操作边界。", "result": "LLMZ+对最常见的越狱提示词表现出强大的抵御能力。同时，它不干扰合法的业务通信，并允许授权流量在用户和代理式LLM之间无缝流动。在实验设置中，误报率和漏报率均可降至0。", "conclusion": "LLMZ+通过实施提示词白名单，简化了安全框架，增强了其长期弹性，并减少了维护LLM信息安全所需的资源。该方法有效提升了代理式LLM的安全性，同时不影响正常业务操作。"}}
{"id": "2509.18388", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18388", "abs": "https://arxiv.org/abs/2509.18388", "authors": ["Binhua Huang", "Ni Wang", "Wendong Yao", "Soumyabrata Dev"], "title": "MVP: Motion Vector Propagation for Zero-Shot Video Object Detection", "comment": "5 pages, 1 figure", "summary": "Running a large open-vocabulary (Open-vocab) detector on every video frame is\naccurate but expensive. We introduce a training-free pipeline that invokes\nOWLv2 only on fixed-interval keyframes and propagates detections to\nintermediate frames using compressed-domain motion vectors (MV). A simple 3x3\ngrid aggregation of motion vectors provides translation and uniform-scale\nupdates, augmented with an area-growth check and an optional single-class\nswitch. The method requires no labels, no fine-tuning, and uses the same prompt\nlist for all open-vocabulary methods. On ILSVRC2015-VID (validation dataset),\nour approach (MVP) attains mAP@0.5=0.609 and mAP@[0.5:0.95]=0.316. At loose\nintersection-over-union (IoU) thresholds it remains close to framewise\nOWLv2-Large (0.747/0.721 at 0.2/0.3 versus 0.784/0.780), reflecting that coarse\nlocalization is largely preserved. Under the same keyframe schedule, MVP\noutperforms tracker-based propagation (MOSSE, KCF, CSRT) at mAP@0.5. A\nsupervised reference (YOLOv12x) reaches 0.631 at mAP@0.5 but requires labeled\ntraining, whereas our method remains label-free and open-vocabulary. These\nresults indicate that compressed-domain propagation is a practical way to\nreduce detector invocations while keeping strong zero-shot coverage in videos.\nOur code and models are available at https://github.com/microa/MVP.", "AI": {"tldr": "该论文提出了一种无需训练的视频开放词汇检测流水线（MVP），通过在关键帧上运行检测器（OWLv2）并利用压缩域运动向量将检测结果传播到中间帧，从而显著降低了计算成本。", "motivation": "在视频的每一帧上运行大型开放词汇检测器虽然准确，但计算成本高昂。", "method": "该方法（MVP）是一种无需训练的流水线，仅在固定间隔的关键帧上调用OWLv2检测器。它利用压缩域运动向量（MV）将检测结果传播到中间帧。具体方法包括：使用简单的3x3网格聚合运动向量以提供平移和均匀缩放更新，并辅以区域增长检查和可选的单类别切换。该方法无需标签、无需微调，并对所有开放词汇方法使用相同的提示列表。", "result": "在ILSVRC2015-VID（验证数据集）上，MVP达到了mAP@0.5=0.609和mAP@[0.5:0.95]=0.316。在宽松的交并比（IoU）阈值下，其性能接近逐帧OWLv2-Large（0.2/0.3 IoU时为0.747/0.721，而OWLv2为0.784/0.780）。在相同的关键帧调度下，MVP在mAP@0.5上优于基于跟踪器的传播方法（MOSSE、KCF、CSRT）。与需要标注训练的监督方法YOLOv12x（mAP@0.5为0.631）相比，MVP保持了无标签和开放词汇的优势。", "conclusion": "这些结果表明，压缩域传播是一种实用的方法，可以减少检测器调用次数，同时在视频中保持强大的零样本覆盖能力。"}}
{"id": "2509.18588", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18588", "abs": "https://arxiv.org/abs/2509.18588", "authors": ["Jiarui Jin", "Haoyu Wang", "Xiang Lan", "Jun Li", "Gaofeng Cheng", "Hongyan Li", "Shenda Hong"], "title": "UniECG: Understanding and Generating ECG in One Unified Model", "comment": null, "summary": "Recent unified models such as GPT-5 have achieved encouraging progress on\nvision-language tasks. However, these unified models typically fail to\ncorrectly understand ECG signals and provide accurate medical diagnoses, nor\ncan they correctly generate ECG signals. To address these limitations, we\npropose UniECG, the first unified model for ECG capable of concurrently\nperforming evidence-based ECG interpretation and text-conditioned ECG\ngeneration tasks. Through a decoupled two-stage training approach, the model\nfirst learns evidence-based interpretation skills (ECG-to-Text), and then\ninjects ECG generation capabilities (Text-to-ECG) via latent space alignment.\nUniECG can autonomously choose to interpret or generate an ECG based on user\ninput, significantly extending the capability boundaries of current ECG models.\nOur code and checkpoints will be made publicly available at\nhttps://github.com/PKUDigitalHealth/UniECG upon acceptance.", "AI": {"tldr": "UniECG是首个统一的ECG模型，能够同时进行基于证据的ECG解读和文本条件的ECG生成。", "motivation": "现有统一模型（如GPT-5）在ECG信号理解、准确医疗诊断和ECG生成方面表现不佳。", "method": "提出UniECG模型，采用解耦的两阶段训练方法：首先学习基于证据的解读技能（ECG-到-文本），然后通过潜在空间对齐注入ECG生成能力（文本-到-ECG）。", "result": "UniECG能根据用户输入自主选择解读或生成ECG，显著扩展了当前ECG模型的能力边界。", "conclusion": "UniECG是首个能够同时进行ECG解读和生成的统一模型，解决了现有模型在ECG任务上的局限性。"}}
{"id": "2509.18648", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18648", "abs": "https://arxiv.org/abs/2509.18648", "authors": ["Yarden As", "Chengrui Qu", "Benjamin Unger", "Dongho Kang", "Max van der Hart", "Laixi Shi", "Stelian Coros", "Adam Wierman", "Andreas Krause"], "title": "SPiDR: A Simple Approach for Zero-Shot Safety in Sim-to-Real Transfer", "comment": null, "summary": "Safety remains a major concern for deploying reinforcement learning (RL) in\nreal-world applications. Simulators provide safe, scalable training\nenvironments, but the inevitable sim-to-real gap introduces additional safety\nconcerns, as policies must satisfy constraints in real-world conditions that\ndiffer from simulation. To address this challenge, robust safe RL techniques\noffer principled methods, but are often incompatible with standard scalable\ntraining pipelines. In contrast, domain randomization, a simple and popular\nsim-to-real technique, stands out as a promising alternative, although it often\nresults in unsafe behaviors in practice. We present SPiDR, short for\nSim-to-real via Pessimistic Domain Randomization -- a scalable algorithm with\nprovable guarantees for safe sim-to-real transfer. SPiDR uses domain\nrandomization to incorporate the uncertainty about the sim-to-real gap into the\nsafety constraints, making it versatile and highly compatible with existing\ntraining pipelines. Through extensive experiments on sim-to-sim benchmarks and\ntwo distinct real-world robotic platforms, we demonstrate that SPiDR\neffectively ensures safety despite the sim-to-real gap while maintaining strong\nperformance.", "AI": {"tldr": "SPiDR提出了一种悲观领域随机化方法，实现了安全且可扩展的仿真到现实迁移，并提供了可证明的安全保证。", "motivation": "强化学习在实际应用中部署时，安全性是一个主要问题。模拟器训练虽然安全可扩展，但仿真到现实的差距会导致策略在真实世界中无法满足安全约束。现有的鲁棒安全强化学习技术通常与可扩展的训练流程不兼容，而常用的领域随机化方法在实践中又常导致不安全行为。", "method": "本文提出了SPiDR（Sim-to-real via Pessimistic Domain Randomization）算法。它利用领域随机化将仿真到现实差距带来的不确定性融入到安全约束中，使其能够兼容现有的训练流程。", "result": "通过在仿真基准和两个不同的真实世界机器人平台上进行大量实验，SPiDR证明了其在存在仿真到现实差距的情况下，能有效确保安全性，同时保持强大的性能。", "conclusion": "SPiDR是一种可扩展的算法，为安全的仿真到现实迁移提供了可证明的保证，并且与现有训练流程高度兼容。"}}
{"id": "2509.18565", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18565", "abs": "https://arxiv.org/abs/2509.18565", "authors": ["Mitchell Piehl", "Dillon Wilson", "Ananya Kalita", "Jugal Kalita"], "title": "Solving Math Word Problems Using Estimation Verification and Equation Generation", "comment": "Accepted to IEEE ICMLA 2025", "summary": "Large Language Models (LLMs) excel at various tasks, including\nproblem-solving and question-answering. However, LLMs often find Math Word\nProblems (MWPs) challenging because solving them requires a range of reasoning\nand mathematical abilities with which LLMs seem to struggle. Recent efforts\nhave helped LLMs solve more complex MWPs with improved prompts. This study\nproposes a novel method that initially prompts an LLM to create equations from\na decomposition of the question, followed by using an external symbolic\nequation solver to produce an answer. To ensure the accuracy of the obtained\nanswer, inspired by an established recommendation of math teachers, the LLM is\ninstructed to solve the MWP a second time, but this time with the objective of\nestimating the correct answer instead of solving it exactly. The estimation is\nthen compared to the generated answer to verify. If verification fails, an\niterative rectification process is employed to ensure the correct answer is\neventually found. This approach achieves new state-of-the-art results on\ndatasets used by prior published research on numeric and algebraic MWPs,\nimproving the previous best results by nearly two percent on average. In\naddition, the approach obtains satisfactory results on trigonometric MWPs, a\ntask not previously attempted to the authors' best knowledge. This study also\nintroduces two new datasets, SVAMPClean and Trig300, to further advance the\ntesting of LLMs' reasoning abilities.", "AI": {"tldr": "本研究提出了一种新颖的方法，结合大型语言模型（LLM）生成方程、外部符号求解器计算答案，并通过LLM估计和迭代纠正来验证答案，从而显著提升了LLM在数学应用题（MWP）上的性能。", "motivation": "大型语言模型（LLM）在解决问题和问答方面表现出色，但在数学应用题（MWP）上仍面临挑战，因为解决MWP需要一系列推理和数学能力。尽管最近的努力通过改进提示词有所帮助，但LLM在此方面仍显不足。", "method": "该方法首先提示LLM根据问题分解创建方程，然后使用外部符号方程求解器来得出答案。为了确保答案的准确性，LLM被指示进行第二次求解，但目标是估计正确答案而非精确求解。估计结果随后与生成的答案进行比较以进行验证。如果验证失败，则采用迭代纠正过程以确保最终找到正确答案。", "result": "该方法在先前研究使用的数值和代数MWP数据集上取得了新的最先进结果，平均比之前最佳结果提高了近2%。此外，它在三角函数MWP上取得了令人满意的结果，这是以前未曾尝试的任务。本研究还引入了两个新数据集：SVAMPClean和Trig300，以进一步推动LLM推理能力的测试。", "conclusion": "本研究提出的方法通过结合外部求解器和独特的两阶段验证与纠正过程，显著提高了LLM在数学应用题上的性能，并在数值、代数和三角函数MWP方面取得了最先进或令人满意的结果，同时引入了新的数据集以促进未来的研究。"}}
{"id": "2509.18390", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18390", "abs": "https://arxiv.org/abs/2509.18390", "authors": ["Zitian Zhang", "Joshua Urban Davis", "Jeanne Phuong Anh Vu", "Jiangtao Kuang", "Jean-François Lalonde"], "title": "Improving the color accuracy of lighting estimation models", "comment": "Project page: https://lvsn.github.io/coloraccuracy", "summary": "Advances in high dynamic range (HDR) lighting estimation from a single image\nhave opened new possibilities for augmented reality (AR) applications.\nPredicting complex lighting environments from a single input image allows for\nthe realistic rendering and compositing of virtual objects. In this work, we\ninvestigate the color robustness of such methods -- an often overlooked yet\ncritical factor for achieving visual realism. While most evaluations conflate\ncolor with other lighting attributes (e.g., intensity, direction), we isolate\ncolor as the primary variable of interest. Rather than introducing a new\nlighting estimation algorithm, we explore whether simple adaptation techniques\ncan enhance the color accuracy of existing models. Using a novel HDR dataset\nfeaturing diverse lighting colors, we systematically evaluate several\nadaptation strategies. Our results show that preprocessing the input image with\na pre-trained white balance network improves color robustness, outperforming\nother strategies across all tested scenarios. Notably, this approach requires\nno retraining of the lighting estimation model. We further validate the\ngenerality of this finding by applying the technique to three state-of-the-art\nlighting estimation methods from recent literature.", "AI": {"tldr": "本文研究了单幅图像高动态范围（HDR）光照估计方法在颜色鲁棒性方面的表现，发现通过预训练的白平衡网络对输入图像进行预处理，可以显著提高现有模型的颜色准确性，且无需重新训练。", "motivation": "高动态范围（HDR）光照估计对于增强现实（AR）应用中的虚拟对象真实渲染至关重要。然而，现有方法在评估时常将颜色与其他光照属性（如强度、方向）混淆，导致颜色鲁棒性这一关键因素被忽视。作者旨在将颜色作为主要变量进行独立研究。", "method": "本文不引入新的光照估计算法，而是探索简单的自适应技术是否能提高现有模型的颜色准确性。为此，研究人员构建了一个包含多种光照颜色的新型HDR数据集，并系统评估了几种自适应策略，特别是使用预训练的白平衡网络对输入图像进行预处理的方法。", "result": "研究结果表明，使用预训练的白平衡网络对输入图像进行预处理，可以显著提高光照估计方法的颜色鲁棒性，在所有测试场景中均优于其他策略。值得注意的是，这种方法无需重新训练光照估计模型。此外，该技术已成功应用于三种最新的SOTA光照估计方法，验证了其普适性。", "conclusion": "简单的白平衡预处理是一种有效且普适的方法，可以显著提高现有单幅图像HDR光照估计模型的颜色准确性和鲁棒性，这对于实现AR应用的视觉真实感至关重要，且无需对原有模型进行重新训练。"}}
{"id": "2509.18632", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18632", "abs": "https://arxiv.org/abs/2509.18632", "authors": ["Nishant Balepur", "Matthew Shu", "Yoo Yeon Sung", "Seraphina Goldfarb-Tarrant", "Shi Feng", "Fumeng Yang", "Rachel Rudinger", "Jordan Lee Boyd-Graber"], "title": "A Good Plan is Hard to Find: Aligning Models with Preferences is Misaligned with What Helps Users", "comment": "EMNLP 2025", "summary": "To assist users in complex tasks, LLMs generate plans: step-by-step\ninstructions towards a goal. While alignment methods aim to ensure LLM plans\nare helpful, they train (RLHF) or evaluate (ChatbotArena) on what users prefer,\nassuming this reflects what helps them. We test this with Planorama: an\ninterface where 126 users answer 300 multi-step questions with LLM plans. We\nget 4388 plan executions and 5584 comparisons to measure plan helpfulness (QA\nsuccess) and user preferences on plans, and recreate the setup in agents and\nreward models to see if they simulate or prefer what helps users. We expose: 1)\nuser/model preferences and agent success do not accurately predict which plans\nhelp users, so common alignment feedback can misalign with helpfulness; 2) this\ngap is not due to user-specific preferences, as users are similarly successful\nwhen using plans they prefer/disprefer; 3) surface-level cues like brevity and\nquestion similarity strongly link to preferences, but such biases fail to\npredict helpfulness. In all, we argue aligning helpful LLMs needs feedback from\nreal user interactions, not just preferences of what looks helpful, so we\ndiscuss the plan NLP researchers can execute to solve this problem.", "AI": {"tldr": "研究发现，大语言模型（LLM）计划的用户偏好和模型偏好并不能准确预测计划的实际帮助程度，常见的对齐反馈可能导致与实际帮助性不符。", "motivation": "当前的LLM对齐方法（如RLHF和ChatbotArena）主要基于用户偏好进行训练或评估，但研究者质疑这种偏好是否真正反映了计划的实际帮助程度。", "method": "研究开发了一个名为Planorama的界面，让126名用户回答300个多步骤问题，并使用LLM生成的计划。共收集了4388次计划执行和5584次比较，以衡量计划的帮助性（QA成功率）和用户偏好。同时，研究人员在代理和奖励模型中重现了该设置，以观察它们是否模拟或偏好对用户有帮助的计划。", "result": "1) 用户/模型偏好和代理成功率并不能准确预测哪些计划对用户有帮助，这表明常见的对齐反馈可能与实际帮助性不符；2) 这种差异并非源于用户特有的偏好，因为用户在使用他们偏好或不偏好的计划时，成功率相似；3) 表面线索（如简洁性、问题相似性）与偏好密切相关，但这些偏见未能预测计划的帮助性。", "conclusion": "为了对齐真正有帮助的LLM，需要来自真实用户交互的反馈，而不仅仅是那些看起来有帮助的偏好。研究者讨论了自然语言处理（NLP）研究人员可以采取的解决方案来解决这个问题。"}}
{"id": "2509.18671", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18671", "abs": "https://arxiv.org/abs/2509.18671", "authors": ["Kaixin Chai", "Hyunjun Lee", "Joseph J. Lim"], "title": "N2M: Bridging Navigation and Manipulation by Learning Pose Preference from Rollout", "comment": null, "summary": "In mobile manipulation, the manipulation policy has strong preferences for\ninitial poses where it is executed. However, the navigation module focuses\nsolely on reaching the task area, without considering which initial pose is\npreferable for downstream manipulation. To address this misalignment, we\nintroduce N2M, a transition module that guides the robot to a preferable\ninitial pose after reaching the task area, thereby substantially improving task\nsuccess rates. N2M features five key advantages: (1) reliance solely on\nego-centric observation without requiring global or historical information; (2)\nreal-time adaptation to environmental changes; (3) reliable prediction with\nhigh viewpoint robustness; (4) broad applicability across diverse tasks,\nmanipulation policies, and robot hardware; and (5) remarkable data efficiency\nand generalizability. We demonstrate the effectiveness of N2M through extensive\nsimulation and real-world experiments. In the PnPCounterToCab task, N2M\nimproves the averaged success rate from 3% with the reachability-based baseline\nto 54%. Furthermore, in the Toybox Handover task, N2M provides reliable\npredictions even in unseen environments with only 15 data samples, showing\nremarkable data efficiency and generalizability.", "AI": {"tldr": "N2M是一个过渡模块，它在机器人到达任务区域后，引导其进入对后续操作更优的初始姿态，从而显著提高移动操作任务的成功率。", "motivation": "在移动操作中，操作策略对初始姿态有强烈偏好，而导航模块仅关注到达任务区域，未考虑下游操作所需的最佳初始姿态，导致导航和操作之间存在错位。", "method": "N2M是一个过渡模块，在机器人到达任务区域后，基于自我中心观察引导机器人到更优的初始姿态。其主要特点包括：仅依赖自我中心观察，无需全局或历史信息；实时适应环境变化；具有高视角鲁棒性的可靠预测；广泛适用于不同任务、操作策略和机器人硬件；以及出色的数据效率和泛化能力。", "result": "在PnPCounterToCab任务中，N2M将平均成功率从可达性基线的3%提高到54%。在Toybox Handover任务中，N2M仅用15个数据样本就能在未见过的环境中提供可靠预测，展现了卓越的数据效率和泛化能力。", "conclusion": "N2M通过在导航和操作之间引入一个过渡模块，有效解决了初始姿态未对齐的问题，显著提高了移动操作任务的成功率，并表现出高可靠性、广泛适用性、数据效率和泛化能力。"}}
{"id": "2509.18633", "categories": ["cs.AI", "q-fin.RM"], "pdf": "https://arxiv.org/pdf/2509.18633", "abs": "https://arxiv.org/abs/2509.18633", "authors": ["Yara Mohajerani"], "title": "Adaptive Learning in Spatial Agent-Based Models for Climate Risk Assessment: A Geospatial Framework with Evolutionary Economic Agents", "comment": "Submitted and accepted to Tackling Climate Change with Machine\n  Learning workshop at NeurIPS 2025. 5 pages, 1 figure. Source code and\n  documentation available at\n  https://github.com/yaramohajerani/spatial-climate-ABM", "summary": "Climate risk assessment requires modelling complex interactions between\nspatially heterogeneous hazards and adaptive economic systems. We present a\nnovel geospatial agent-based model that integrates climate hazard data with\nevolutionary learning for economic agents. Our framework combines Mesa-based\nspatial modelling with CLIMADA climate impact assessment, introducing adaptive\nlearning behaviours that allow firms to evolve strategies for budget\nallocation, pricing, wages, and risk adaptation through fitness-based selection\nand mutation. We demonstrate the framework using riverine flood projections\nunder RCP8.5 until 2100, showing that evolutionary adaptation enables firms to\nconverge with baseline (no hazard) production levels after decades of\ndisruption due to climate stress. Our results reveal systemic risks where even\nagents that are not directly exposed to floods face impacts through supply\nchain disruptions, with the end-of-century average price of goods 5.6% higher\nunder RCP8.5 compared to the baseline. This open-source framework provides\nfinancial institutions and companies with tools to quantify both direct and\ncascading climate risks while evaluating cost-effective adaptation strategies.", "AI": {"tldr": "该论文提出了一种新颖的地理空间基于代理的模型，整合了气候灾害数据和经济代理的演化学习机制，以评估气候风险。研究表明，演化适应能使企业在气候压力后恢复生产，但供应链中断仍导致系统性风险和商品价格上涨。", "motivation": "气候风险评估需要模拟空间异质性灾害与适应性经济系统之间复杂的相互作用。", "method": "研究采用了一种新颖的地理空间基于代理的模型，将基于Mesa的空间建模与CLIMADA气候影响评估相结合。该模型引入了适应性学习行为，使企业通过基于适应度的选择和突变来演化预算分配、定价、工资和风险适应策略。模型使用RCP8.5情景下的河流洪水预测进行演示，模拟至2100年。", "result": "研究结果显示，演化适应使企业在经历数十年的气候压力中断后，能够与基线（无灾害）生产水平趋同。同时，即使未直接暴露于洪水风险的代理，也通过供应链中断面临影响，揭示了系统性风险。在RCP8.5情景下，到本世纪末，商品平均价格比基线高出5.6%。", "conclusion": "该开源框架为金融机构和公司提供了量化直接和级联气候风险的工具，并可用于评估具有成本效益的适应策略。"}}
{"id": "2509.18405", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18405", "abs": "https://arxiv.org/abs/2509.18405", "authors": ["Sourav Halder", "Jinjun Tong", "Xinyu Wu"], "title": "Check Field Detection Agent (CFD-Agent) using Multimodal Large Language and Vision Language Models", "comment": "12 pages, 5 figures, 2 tables", "summary": "Checks remain a foundational instrument in the financial ecosystem,\nfacilitating substantial transaction volumes across institutions. However,\ntheir continued use also renders them a persistent target for fraud,\nunderscoring the importance of robust check fraud detection mechanisms. At the\ncore of such systems lies the accurate identification and localization of\ncritical fields, such as the signature, magnetic ink character recognition\n(MICR) line, courtesy amount, legal amount, payee, and payer, which are\nessential for subsequent verification against reference checks belonging to the\nsame customer. This field-level detection is traditionally dependent on object\ndetection models trained on large, diverse, and meticulously labeled datasets,\na resource that is scarce due to proprietary and privacy concerns. In this\npaper, we introduce a novel, training-free framework for automated check field\ndetection, leveraging the power of a vision language model (VLM) in conjunction\nwith a multimodal large language model (MLLM). Our approach enables zero-shot\ndetection of check components, significantly lowering the barrier to deployment\nin real-world financial settings. Quantitative evaluation of our model on a\nhand-curated dataset of 110 checks spanning multiple formats and layouts\ndemonstrates strong performance and generalization capability. Furthermore,\nthis framework can serve as a bootstrap mechanism for generating high-quality\nlabeled datasets, enabling the development of specialized real-time object\ndetection models tailored to institutional needs.", "AI": {"tldr": "本文提出了一种新颖的、无需训练的支票字段检测框架，结合视觉语言模型（VLM）和多模态大语言模型（MLLM），实现零样本检测，并在小数据集上表现出强大的性能和泛化能力。", "motivation": "支票欺诈是一个持续存在的问题，需要强大的检测机制。核心在于准确识别和定位关键字段（如签名、MICR行、礼节性金额等）。传统方法依赖于大量标注数据，但由于专有性和隐私问题，此类数据稀缺，阻碍了部署。", "method": "该研究引入了一个无需训练的框架，利用视觉语言模型（VLM）和多模态大语言模型（MLLM）的能力。这种方法实现了支票组件的零样本检测，无需预先训练，解决了数据稀缺问题。", "result": "在包含110张支票的手动整理数据集（涵盖多种格式和布局）上进行定量评估，模型展示了强大的性能和泛化能力。此外，该框架还可作为生成高质量标注数据集的引导机制。", "conclusion": "所提出的无需训练的VLM/MLLM框架为支票字段检测提供了一种有效且易于部署的零样本解决方案，显著降低了实际金融环境中的部署门槛，并能辅助生成用于训练专业实时检测模型的数据集。"}}
{"id": "2509.18655", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18655", "abs": "https://arxiv.org/abs/2509.18655", "authors": ["Lingwen Deng", "Yifei Han", "Long Zhang", "Yue Du", "Bin Li"], "title": "Consistency-Aware Parameter-Preserving Knowledge Editing Framework for Multi-Hop Question Answering", "comment": "Submitted to ICASSP 2026", "summary": "Parameter-Preserving Knowledge Editing (PPKE) enables updating models with\nnew or corrected information without retraining or parameter adjustment. Recent\nPPKE approaches based on knowledge graphs (KG) to extend knowledge editing (KE)\ncapabilities to multi-hop question answering (MHQA). However, these methods\noften lack consistency, leading to knowledge contamination, unstable updates,\nand retrieval behaviors that fail to reflect the intended edits. Such\ninconsistencies undermine the reliability of PPKE in multi- hop reasoning. We\npresent CAPE-KG, Consistency-Aware Parameter-Preserving Editing with Knowledge\nGraphs, a novel consistency-aware framework for PPKE on MHQA. CAPE-KG ensures\nKG construction, update, and retrieval are always aligned with the requirements\nof the MHQA task, maintaining coherent reasoning over both unedited and edited\nknowledge. Extensive experiments on the MQuAKE benchmark show accuracy\nimprovements in PPKE performance for MHQA, demonstrating the effectiveness of\naddressing consistency in PPKE.", "AI": {"tldr": "现有基于知识图谱的参数不变知识编辑（PPKE）在多跳问答（MHQA）中存在一致性问题。本文提出了CAPE-KG框架，通过确保知识图谱的构建、更新和检索与MHQA任务一致，显著提高了PPKE的准确性。", "motivation": "现有的基于知识图谱的PPKE方法在扩展到多跳问答时，往往缺乏一致性，导致知识污染、更新不稳定以及检索行为无法反映预期编辑，从而损害了PPKE在多跳推理中的可靠性。", "method": "本文提出了CAPE-KG（Consistency-Aware Parameter-Preserving Editing with Knowledge Graphs），一个新颖的、一致性感知的PPKE框架。CAPE-KG确保知识图谱的构建、更新和检索始终与多跳问答任务的要求保持一致，从而在未经编辑和已编辑的知识上保持连贯的推理。", "result": "在MQuAKE基准测试上进行的广泛实验表明，CAPE-KG显著提高了多跳问答中PPKE的性能准确性。", "conclusion": "解决PPKE中的一致性问题对于提高多跳问答任务的性能是有效的，CAPE-KG框架证明了这一点。"}}
{"id": "2509.18686", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18686", "abs": "https://arxiv.org/abs/2509.18686", "authors": ["Ziyi Xu", "Haohong Lin", "Shiqi Liu", "Ding Zhao"], "title": "Query-Centric Diffusion Policy for Generalizable Robotic Assembly", "comment": "8 pages, 7 figures", "summary": "The robotic assembly task poses a key challenge in building generalist robots\ndue to the intrinsic complexity of part interactions and the sensitivity to\nnoise perturbations in contact-rich settings. The assembly agent is typically\ndesigned in a hierarchical manner: high-level multi-part reasoning and\nlow-level precise control. However, implementing such a hierarchical policy is\nchallenging in practice due to the mismatch between high-level skill queries\nand low-level execution. To address this, we propose the Query-centric\nDiffusion Policy (QDP), a hierarchical framework that bridges high-level\nplanning and low-level control by utilizing queries comprising objects, contact\npoints, and skill information. QDP introduces a query-centric mechanism that\nidentifies task-relevant components and uses them to guide low-level policies,\nleveraging point cloud observations to improve the policy's robustness. We\nconduct comprehensive experiments on the FurnitureBench in both simulation and\nreal-world settings, demonstrating improved performance in skill precision and\nlong-horizon success rate. In the challenging insertion and screwing tasks, QDP\nimproves the skill-wise success rate by over 50% compared to baselines without\nstructured queries.", "AI": {"tldr": "本文提出了一种名为查询中心扩散策略（QDP）的层次化框架，通过利用包含对象、接触点和技能信息的查询来弥合高层规划和低层控制之间的鸿沟，从而提高了机器人装配任务的精度和成功率。", "motivation": "机器人装配任务由于部件交互的复杂性和对接触密集环境中噪声扰动的敏感性，对构建通用机器人提出了关键挑战。传统的层次化策略在实践中因高层技能查询与低层执行之间的不匹配而难以实现。", "method": "本文提出了查询中心扩散策略（QDP），这是一个层次化框架。它利用包含对象、接触点和技能信息的查询来连接高层规划和低层控制。QDP引入了一种查询中心机制，用于识别任务相关组件并指导低层策略，同时利用点云观测来提高策略的鲁棒性。", "result": "在FurnitureBench上的模拟和真实世界实验表明，QDP在技能精度和长周期成功率方面均有显著提升。在具有挑战性的插入和拧螺丝任务中，与没有结构化查询的基线相比，QDP的技能成功率提高了50%以上。", "conclusion": "QDP通过其查询中心机制有效解决了机器人装配中高层规划与低层控制之间的不匹配问题，显著提高了装配任务的鲁棒性和成功率。"}}
{"id": "2509.18667", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18667", "abs": "https://arxiv.org/abs/2509.18667", "authors": ["Qiao Xiao", "Hong Ting Tsang", "Jiaxin Bai"], "title": "TERAG: Token-Efficient Graph-Based Retrieval-Augmented Generation", "comment": "16 pages, 2 figures, 4 tables. Submitted to the 2026 18th\n  International Conference on Machine Learning and Computing (ICMLC 2026),\n  under review", "summary": "Graph-based Retrieval-augmented generation (RAG) has become a widely studied\napproach for improving the reasoning, accuracy, and factuality of Large\nLanguage Models. However, many existing graph-based RAG systems overlook the\nhigh cost associated with LLM token usage during graph construction, hindering\nlarge-scale adoption. To address this, we propose TERAG, a simple yet effective\nframework designed to build informative graphs at a significantly lower cost.\nInspired by HippoRAG, we incorporate Personalized PageRank (PPR) during the\nretrieval phase, and we achieve at least 80% of the accuracy of widely used\ngraph-based RAG methods while consuming only 3%-11% of the output tokens.", "AI": {"tldr": "TERAG是一个成本效益高的图基RAG框架，它在显著降低LLM令牌使用量的同时，保持了与现有方法相当的准确性。", "motivation": "现有的图基RAG系统在图构建过程中LLM令牌使用成本高昂，阻碍了其大规模应用。", "method": "提出了TERAG框架，该框架旨在以显著更低的成本构建信息图。受HippoRAG启发，TERAG在检索阶段融入了个性化PageRank（PPR）。", "result": "TERAG在消耗仅3%-11%输出令牌的情况下，达到了广泛使用的图基RAG方法至少80%的准确率。", "conclusion": "TERAG提供了一种简单而有效的方法，以显著降低的成本构建信息图，同时保持了高准确性，解决了现有图基RAG系统的成本问题。"}}
{"id": "2509.18425", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18425", "abs": "https://arxiv.org/abs/2509.18425", "authors": ["Philip Wootaek Shin", "Jack Sampson", "Vijaykrishnan Narayanan", "Andres Marquez", "Mahantesh Halappanavar"], "title": "Losing the Plot: How VLM responses degrade on imperfect charts", "comment": null, "summary": "Vision language models (VLMs) show strong results on chart understanding, yet\nexisting benchmarks assume clean figures and fact based queries. Real world\ncharts often contain distortions and demand reasoning beyond simple matching.\nWe evaluate ChatGPT 4o, Claude Sonnet 4, and Gemini 2.5 Pro, finding sharp\nperformance drops under corruption or occlusion, with hallucinations such as\nvalue fabrication, trend misinterpretation, and entity confusion becoming more\nfrequent. Models remain overconfident in degraded settings, generating\nplausible but unsupported explanations.\n  To address this gap, we introduce CHART NOISe(Chart Hallucinations, Answers,\nand Reasoning Testing on Noisy and Occluded Input Selections), a dataset\ncombining chart corruptions, occlusions, and exam style multiple choice\nquestions inspired by Korea's CSAT English section. A key innovation is prompt\nreverse inconsistency, where models contradict themselves when asked to confirm\nversus deny the same statement. Our contributions are threefold: (1)\nbenchmarking state of the art VLMs, exposing systematic vulnerabilities in\nchart reasoning; (2) releasing CHART NOISe, the first dataset unifying\ncorruption, occlusion, and reverse inconsistency; and (3) proposing baseline\nmitigation strategies such as quality filtering and occlusion detection.\nTogether, these efforts establish a rigorous testbed for advancing robustness\nand reliability in chart understanding.", "AI": {"tldr": "该研究发现视觉语言模型（VLMs）在处理包含失真和遮挡的真实世界图表时性能显著下降，且易产生幻觉并过度自信。为此，论文引入了CHART NOISe数据集，用于评估模型在噪声环境下的图表理解能力，并提出了缓解策略。", "motivation": "现有图表理解基准通常假设图表清晰且查询基于事实，而现实世界的图表常包含失真和遮挡，需要更深层次的推理。VLMs在此类复杂情境下的表现及其潜在的幻觉问题尚未得到充分评估。", "method": "研究评估了ChatGPT 4o、Claude Sonnet 4和Gemini 2.5 Pro等主流VLM在图表失真或遮挡情况下的表现。随后，引入了CHART NOISe数据集，该数据集结合了图表损坏、遮挡以及受韩国高考英语部分启发的考试风格多项选择题，并创新性地加入了“提示反向不一致性”测试。此外，论文还提出了质量过滤和遮挡检测等基线缓解策略。", "result": "结果显示，在图表损坏或遮挡下，VLMs的性能急剧下降，幻觉（如数值捏造、趋势误解、实体混淆）变得更加频繁。模型在降级设置中仍然过度自信，生成看似合理但无依据的解释。研究还观察到模型在确认与否认同一陈述时存在提示反向不一致性。", "conclusion": "该研究通过对最先进VLMs的基准测试，揭示了它们在图表推理方面的系统性脆弱性；发布了CHART NOISe数据集，这是首个统一损坏、遮挡和反向不一致性的数据集；并提出了基线缓解策略。这些工作共同为提升图表理解的鲁棒性和可靠性建立了严格的测试平台。"}}
{"id": "2509.18658", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18658", "abs": "https://arxiv.org/abs/2509.18658", "authors": ["Huanxin Sheng", "Xinyi Liu", "Hangfeng He", "Jieyu Zhao", "Jian Kang"], "title": "Analyzing Uncertainty of LLM-as-a-Judge: Interval Evaluations with Conformal Prediction", "comment": "To appear in EMNLP 2025. Our code and data are available at\n  \\url{https://github.com/BruceSheng1202/Analyzing_Uncertainty_of_LLM-as-a-Judge", "summary": "LLM-as-a-judge has become a promising paradigm for using large language\nmodels (LLMs) to evaluate natural language generation (NLG), but the\nuncertainty of its evaluation remains underexplored. This lack of reliability\nmay limit its deployment in many applications. This work presents the first\nframework to analyze the uncertainty by offering a prediction interval of\nLLM-based scoring via conformal prediction. Conformal prediction constructs\ncontinuous prediction intervals from a single evaluation run, and we design an\nordinal boundary adjustment for discrete rating tasks. We also suggest a\nmidpoint-based score within the interval as a low-bias alternative to raw model\nscore and weighted average. We perform extensive experiments and analysis,\nwhich show that conformal prediction can provide valid prediction interval with\ncoverage guarantees. We also explore the usefulness of interval midpoint and\njudge reprompting for better judgment.", "AI": {"tldr": "本文提出首个利用共形预测为LLM作为评判者的评分提供预测区间，以量化评估的不确定性，并引入离散评分的序数边界调整和低偏差的中点分数。", "motivation": "LLM作为评判者在自然语言生成评估中前景广阔，但其评估的不确定性尚未得到充分探索，这种可靠性不足限制了其在许多应用中的部署。", "method": "本文提出了一个利用共形预测分析LLM评分不确定性的框架，为LLM的评分提供预测区间。具体方法包括为离散评分任务设计序数边界调整，并建议使用区间中点作为原始模型分数和加权平均的低偏差替代方案。", "result": "实验和分析表明，共形预测可以提供具有覆盖率保证的有效预测区间。研究还探索了区间中点和评判者重新提示（reprompting）对获得更好判断的有用性。", "conclusion": "该研究首次通过共形预测为LLM作为评判者的评估提供了具有覆盖率保证的有效预测区间，从而量化并解决了其不确定性问题，并提出了改进评分准确性的新方法。"}}
{"id": "2509.18734", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18734", "abs": "https://arxiv.org/abs/2509.18734", "authors": ["Nishant Doshi", "Amey Sutvani", "Sanket Gujar"], "title": "Learning Obstacle Avoidance using Double DQN for Quadcopter Navigation", "comment": null, "summary": "One of the challenges faced by Autonomous Aerial Vehicles is reliable\nnavigation through urban environments. Factors like reduction in precision of\nGlobal Positioning System (GPS), narrow spaces and dynamically moving obstacles\nmake the path planning of an aerial robot a complicated task. One of the skills\nrequired for the agent to effectively navigate through such an environment is\nto develop an ability to avoid collisions using information from onboard depth\nsensors. In this paper, we propose Reinforcement Learning of a virtual\nquadcopter robot agent equipped with a Depth Camera to navigate through a\nsimulated urban environment.", "AI": {"tldr": "本文提出使用强化学习训练配备深度摄像头的虚拟四旋翼无人机，以在模拟城市环境中进行导航和避障。", "motivation": "自主飞行器在城市环境中导航面临挑战，包括GPS精度降低、狭窄空间和动态障碍物，使路径规划复杂化。因此，需要开发一种利用机载深度传感器信息进行避障的能力。", "method": "提出一种基于强化学习的方法，训练一个配备深度摄像头的虚拟四旋翼机器人代理，使其在模拟城市环境中导航。", "result": "本文提出了一种方法，但抽象中尚未提供具体的实验结果。", "conclusion": "本文旨在通过强化学习使虚拟四旋翼无人机在模拟城市环境中，利用深度摄像头信息实现可靠的导航和有效的避障。"}}
{"id": "2509.18681", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18681", "abs": "https://arxiv.org/abs/2509.18681", "authors": ["Nicolas Valot", "Louis Fabre", "Benjamin Lesage", "Ammar Mechouche", "Claire Pagetti"], "title": "Implementation of airborne ML models with semantics preservation", "comment": null, "summary": "Machine Learning (ML) may offer new capabilities in airborne systems.\nHowever, as any piece of airborne systems, ML-based systems will be required to\nguarantee their safe operation. Thus, their development will have to be\ndemonstrated to be compliant with the adequate guidance. So far, the European\nUnion Aviation Safety Agency (EASA) has published a concept paper and an\nEUROCAE/SAE group is preparing ED-324. Both approaches delineate high-level\nobjectives to confirm the ML model achieves its intended function and maintains\ntraining performance in the target environment. The paper aims to clarify the\ndifference between an ML model and its corresponding unambiguous description,\nreferred to as the Machine Learning Model Description (MLMD). It then refines\nthe essential notion of semantics preservation to ensure the accurate\nreplication of the model. We apply our contributions to several industrial use\ncases to build and compare several target models.", "AI": {"tldr": "本文旨在明确机器学习模型与其明确描述（MLMD）之间的区别，并细化语义保存的概念，以确保模型在航空系统中的准确复制和安全合规性。", "motivation": "机器学习在航空系统中具有潜力，但必须确保其安全运行并符合相关指导（如EASA概念文件和ED-324）。现有方法提出了高层目标，但缺乏对ML模型及其准确复制的详细说明。", "method": "本文首先阐明了机器学习模型与机器学习模型描述（MLMD）之间的区别。其次，它细化了语义保存这一关键概念，以确保模型的准确复制。最后，将这些贡献应用于多个工业用例，以构建和比较不同的目标模型。", "result": "本文明确了机器学习模型与其明确描述（MLMD）之间的差异，并完善了语义保存的核心概念，以确保模型的精确复制。这些概念被应用于多个工业用例，用于构建和比较不同的目标模型。", "conclusion": "通过区分ML模型和MLMD，并细化语义保存的概念，本文为确保机器学习模型在航空系统中的准确复制和安全合规性提供了关键的基础，这对于满足航空安全标准至关重要。"}}
{"id": "2509.18427", "categories": ["cs.CV", "physics.med-ph"], "pdf": "https://arxiv.org/pdf/2509.18427", "abs": "https://arxiv.org/abs/2509.18427", "authors": ["Xinyang Wu", "Muheng Li", "Xia Li", "Orso Pusterla", "Sairos Safai", "Philippe C. Cattin", "Antony J. Lomax", "Ye Zhang"], "title": "CPT-4DMR: Continuous sPatial-Temporal Representation for 4D-MRI Reconstruction", "comment": null, "summary": "Four-dimensional MRI (4D-MRI) is an promising technique for capturing\nrespiratory-induced motion in radiation therapy planning and delivery.\nConventional 4D reconstruction methods, which typically rely on phase binning\nor separate template scans, struggle to capture temporal variability,\ncomplicate workflows, and impose heavy computational loads. We introduce a\nneural representation framework that considers respiratory motion as a smooth,\ncontinuous deformation steered by a 1D surrogate signal, completely replacing\nthe conventional discrete sorting approach. The new method fuses motion\nmodeling with image reconstruction through two synergistic networks: the\nSpatial Anatomy Network (SAN) encodes a continuous 3D anatomical\nrepresentation, while a Temporal Motion Network (TMN), guided by\nTransformer-derived respiratory signals, produces temporally consistent\ndeformation fields. Evaluation using a free-breathing dataset of 19 volunteers\ndemonstrates that our template- and phase-free method accurately captures both\nregular and irregular respiratory patterns, while preserving vessel and\nbronchial continuity with high anatomical fidelity. The proposed method\nsignificantly improves efficiency, reducing the total processing time from\napproximately five hours required by conventional discrete sorting methods to\njust 15 minutes of training. Furthermore, it enables inference of each 3D\nvolume in under one second. The framework accurately reconstructs 3D images at\nany respiratory state, achieves superior performance compared to conventional\nmethods, and demonstrates strong potential for application in 4D radiation\ntherapy planning and real-time adaptive treatment.", "AI": {"tldr": "本文提出了一种基于神经网络的四维MRI重建框架，通过将呼吸运动建模为由一维信号引导的连续变形，取代了传统的离散排序方法，显著提高了重建精度和效率，适用于放疗规划和实时自适应治疗。", "motivation": "传统的四维MRI重建方法（如相位分箱或独立模板扫描）难以捕捉时间变异性、使工作流程复杂化并产生巨大的计算负担。", "method": "引入了一个神经表示框架，将呼吸运动视为由一维替代信号引导的平滑、连续变形。该方法通过两个协同网络融合运动建模和图像重建：空间解剖网络（SAN）编码连续的三维解剖表示，而时间运动网络（TMN）在Transformer衍生的呼吸信号引导下生成时间一致的形变场。该方法无需模板和相位排序。", "result": "在19名志愿者的自由呼吸数据集上进行评估，结果表明该方法能准确捕捉规则和不规则呼吸模式，同时以高解剖保真度保持血管和支气管的连续性。处理效率显著提高，总处理时间从传统方法的约5小时缩短至仅15分钟的训练时间，每个3D体积的推理时间不到一秒。该框架能准确重建任何呼吸状态下的3D图像，并优于传统方法。", "conclusion": "该框架准确重建了3D图像，性能优于传统方法，在四维放射治疗规划和实时自适应治疗中展现出巨大的应用潜力。"}}
{"id": "2509.18713", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18713", "abs": "https://arxiv.org/abs/2509.18713", "authors": ["Yizhe Huang", "Yang Liu", "Ruiyu Zhao", "Xiaolong Zhong", "Xingming Yue", "Ling Jiang"], "title": "MemOrb: A Plug-and-Play Verbal-Reinforcement Memory Layer for E-Commerce Customer Service", "comment": null, "summary": "Large Language Model-based agents(LLM-based agents) are increasingly deployed\nin customer service, yet they often forget across sessions, repeat errors, and\nlack mechanisms for continual self-improvement. This makes them unreliable in\ndynamic settings where stability and consistency are critical. To better\nevaluate these properties, we emphasize two indicators: task success rate as a\nmeasure of overall effectiveness, and consistency metrics such as Pass$^k$ to\ncapture reliability across multiple trials. To address the limitations of\nexisting approaches, we propose MemOrb, a lightweight and plug-and-play verbal\nreinforcement memory layer that distills multi-turn interactions into compact\nstrategy reflections. These reflections are stored in a shared memory bank and\nretrieved to guide decision-making, without requiring any fine-tuning.\nExperiments show that MemOrb significantly improves both success rate and\nstability, achieving up to a 63 percentage-point gain in multi-turn success\nrate and delivering more consistent performance across repeated trials. Our\nresults demonstrate that structured reflection is a powerful mechanism for\nenhancing long-term reliability of frozen LLM agents in customer service\nscenarios.", "AI": {"tldr": "为解决客户服务中LLM代理的遗忘和重复错误问题，本文提出了MemOrb，一个轻量级的口头强化记忆层，通过结构化策略反思来显著提高任务成功率和稳定性，无需微调。", "motivation": "LLM代理在客户服务中常出现跨会话遗忘、重复错误和缺乏持续自我改进能力，导致在动态环境中不可靠，而这些环境对稳定性和一致性要求很高。", "method": "本文提出了MemOrb，一个轻量级、即插即用的口头强化记忆层。它将多轮交互提炼成紧凑的策略反思，存储在一个共享记忆库中，并在决策时检索以提供指导，无需任何微调。评估指标包括任务成功率（衡量整体有效性）和一致性指标（如Pass$^k$，衡量多次试验的可靠性）。", "result": "实验表明，MemOrb显著提高了任务成功率和稳定性，在多轮成功率方面取得了高达63个百分点的提升，并在重复试验中展现出更一致的性能。", "conclusion": "结构化反思是增强客户服务场景中冻结LLM代理长期可靠性的强大机制。"}}
{"id": "2509.18757", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18757", "abs": "https://arxiv.org/abs/2509.18757", "authors": ["Omar Rayyan", "John Abanes", "Mahmoud Hafez", "Anthony Tzes", "Fares Abu-Dakka"], "title": "MV-UMI: A Scalable Multi-View Interface for Cross-Embodiment Learning", "comment": "For project website and videos, see https https://mv-umi.github.io", "summary": "Recent advances in imitation learning have shown great promise for developing\nrobust robot manipulation policies from demonstrations. However, this promise\nis contingent on the availability of diverse, high-quality datasets, which are\nnot only challenging and costly to collect but are often constrained to a\nspecific robot embodiment. Portable handheld grippers have recently emerged as\nintuitive and scalable alternatives to traditional robotic teleoperation\nmethods for data collection. However, their reliance solely on first-person\nview wrist-mounted cameras often creates limitations in capturing sufficient\nscene contexts. In this paper, we present MV-UMI (Multi-View Universal\nManipulation Interface), a framework that integrates a third-person perspective\nwith the egocentric camera to overcome this limitation. This integration\nmitigates domain shifts between human demonstration and robot deployment,\npreserving the cross-embodiment advantages of handheld data-collection devices.\nOur experimental results, including an ablation study, demonstrate that our\nMV-UMI framework improves performance in sub-tasks requiring broad scene\nunderstanding by approximately 47% across 3 tasks, confirming the effectiveness\nof our approach in expanding the range of feasible manipulation tasks that can\nbe learned using handheld gripper systems, without compromising the\ncross-embodiment advantages inherent to such systems.", "AI": {"tldr": "MV-UMI框架通过结合第三人称视角与第一人称视角，克服了手持式抓手在数据采集中场景上下文不足的限制，显著提升了模仿学习在机器人操作任务上的性能。", "motivation": "模仿学习依赖高质量多样化数据集，但其收集成本高昂且通常受限于特定机器人形态。手持式抓手作为一种直观且可扩展的数据收集方式，其第一人称视角腕部摄像头常限制了对足够场景上下文的捕获，影响了人机演示与机器人部署之间的领域迁移。", "method": "本文提出了MV-UMI（Multi-View Universal Manipulation Interface）框架。该框架将第三人称视角与手持抓手的自我中心（第一人称）摄像头视角相结合，以捕获更丰富的场景上下文。", "result": "实验结果（包括消融研究）表明，MV-UMI框架在需要广泛场景理解的子任务中，跨3个任务的性能平均提高了约47%。这证实了该方法在扩展手持抓手系统可学习操作任务范围方面的有效性。", "conclusion": "MV-UMI框架通过多视角集成，成功克服了手持抓手数据收集的场景上下文限制，有效扩展了可学习操作任务的范围，同时保持了此类系统固有的跨形态优势。"}}
{"id": "2509.18690", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18690", "abs": "https://arxiv.org/abs/2509.18690", "authors": ["Zhiyu Kan", "Wensheng Gan", "Zhenlian Qi", "Philip S. Yu"], "title": "Advances in Large Language Models for Medicine", "comment": "Preprint. 5 figures, 4 tables", "summary": "Artificial intelligence (AI) technology has advanced rapidly in recent years,\nwith large language models (LLMs) emerging as a significant breakthrough. LLMs\nare increasingly making an impact across various industries, with the medical\nfield standing out as the most prominent application area. This paper\nsystematically reviews the up-to-date research progress of LLMs in the medical\nfield, providing an in-depth analysis of training techniques for large medical\nmodels, their adaptation in healthcare settings, related applications, as well\nas their strengths and limitations. Furthermore, it innovatively categorizes\nmedical LLMs into three distinct types based on their training methodologies\nand classifies their evaluation approaches into two categories. Finally, the\nstudy proposes solutions to existing challenges and outlines future research\ndirections based on identified issues in the field of medical LLMs. By\nsystematically reviewing previous and advanced research findings, we aim to\nhighlight the necessity of developing medical LLMs, provide a deeper\nunderstanding of their current state of development, and offer clear guidance\nfor subsequent research.", "AI": {"tldr": "本文系统回顾了大型语言模型（LLMs）在医疗领域的最新研究进展，深入分析其训练、应用、优缺点，并提出分类方法、解决方案及未来研究方向。", "motivation": "鉴于人工智能技术，特别是大型语言模型在医疗领域日益增长的影响力，本文旨在强调开发医疗LLMs的必要性，加深对其当前发展状况的理解，并为后续研究提供明确指导。", "method": "本文采用系统回顾的方法，对医疗领域中LLMs的最新研究进展进行深入分析。具体包括：分析大型医疗模型的训练技术、在医疗环境中的适应性、相关应用、优势与局限性；创新性地根据训练方法将医疗LLMs分为三类，并将评估方法分为两类；最后，针对现有挑战提出解决方案并规划未来研究方向。", "result": "研究结果包括对医疗LLMs训练技术、医疗场景适应性、应用、优势和局限性的深入分析；创新性地将医疗LLMs分为三种类型（基于训练方法）和两种评估方法；并基于已识别的问题提出了现有挑战的解决方案和未来的研究方向。", "conclusion": "通过系统回顾，本文强调了开发医疗LLMs的必要性，提供了对其当前发展状态的深刻理解，并为该领域的后续研究指明了清晰的方向。"}}
{"id": "2509.18451", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18451", "abs": "https://arxiv.org/abs/2509.18451", "authors": ["Prithvi Raj Singh", "Raju Gottumukkala", "Anthony Maida"], "title": "An Analysis of Kalman Filter based Object Tracking Methods for Fast-Moving Tiny Objects", "comment": null, "summary": "Unpredictable movement patterns and small visual mark make precise tracking\nof fast-moving tiny objects like a racquetball one of the challenging problems\nin computer vision. This challenge is particularly relevant for sport robotics\napplications, where lightweight and accurate tracking systems can improve robot\nperception and planning capabilities. While Kalman filter-based tracking\nmethods have shown success in general object tracking scenarios, their\nperformance degrades substantially when dealing with rapidly moving objects\nthat exhibit irregular bouncing behavior. In this study, we evaluate the\nperformance of five state-of-the-art Kalman filter-based tracking\nmethods-OCSORT, DeepOCSORT, ByteTrack, BoTSORT, and StrongSORT-using a custom\ndataset containing 10,000 annotated racquetball frames captured at 720p-1280p\nresolution. We focus our analysis on two critical performance factors:\ninference speed and update frequency per image, examining how these parameters\naffect tracking accuracy and reliability for fast-moving tiny objects. Our\nexperimental evaluation across four distinct scenarios reveals that DeepOCSORT\nachieves the lowest tracking error with an average ADE of 31.15 pixels compared\nto ByteTrack's 114.3 pixels, while ByteTrack demonstrates the fastest\nprocessing at 26.6ms average inference time versus DeepOCSORT's 26.8ms.\nHowever, our results show that all Kalman filter-based trackers exhibit\nsignificant tracking drift with spatial errors ranging from 3-11cm (ADE values:\n31-114 pixels), indicating fundamental limitations in handling the\nunpredictable motion patterns of fast-moving tiny objects like racquetballs.\nOur analysis demonstrates that current tracking approaches require substantial\nimprovements, with error rates 3-4x higher than standard object tracking\nbenchmarks, highlighting the need for specialized methodologies for fast-moving\ntiny object tracking applications.", "AI": {"tldr": "本研究评估了五种基于卡尔曼滤波的先进跟踪方法在追踪快速移动微小物体（如壁球）时的性能。结果显示DeepOCSORT误差最低，ByteTrack速度最快，但所有方法都存在显著的跟踪漂移，表明现有技术对这类物体存在局限性，需要更专业的追踪方法。", "motivation": "由于运动模式不可预测且视觉标记小，精确追踪快速移动的微小物体（如壁球）是计算机视觉中的一个挑战。这对于体育机器人应用尤为重要，但基于卡尔曼滤波的通用跟踪方法在处理快速、不规则弹跳的物体时性能会显著下降。", "method": "研究评估了五种基于卡尔曼滤波的先进跟踪方法：OCSORT、DeepOCSORT、ByteTrack、BoTSORT和StrongSORT。使用了一个包含10,000帧带标注壁球图像（720p-1280p分辨率）的自定义数据集。分析重点关注推理速度和每图像更新频率，并在四种不同场景下进行了实验评估。", "result": "DeepOCSORT实现了最低的跟踪误差（平均ADE为31.15像素），而ByteTrack处理速度最快（平均推理时间26.6毫秒）。然而，所有基于卡尔曼滤波的跟踪器都表现出显著的跟踪漂移，空间误差范围为3-11厘米（ADE值为31-114像素），错误率比标准目标跟踪基准高3-4倍。", "conclusion": "当前基于卡尔曼滤波的跟踪方法在处理快速移动、运动模式不可预测的微小物体时存在根本性局限。现有跟踪方法需要大幅改进，突出表明需要针对快速移动微小目标跟踪应用开发专门的方法。"}}
{"id": "2509.18722", "categories": ["cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.18722", "abs": "https://arxiv.org/abs/2509.18722", "authors": ["Pattara Tipaksorn", "Sumonmas Thatphithakkul", "Vataya Chunwijitra", "Kwanchiva Thangthai"], "title": "LOTUSDIS: A Thai far-field meeting corpus for robust conversational ASR", "comment": null, "summary": "We present LOTUSDIS, a publicly available Thai meeting corpus designed to\nadvance far-field conversational ASR. The dataset comprises 114 hours of\nspontaneous, unscripted dialogue collected in 15-20 minute sessions with three\nparticipants, where overlapping speech is frequent and natural. Speech was\nrecorded simultaneously by nine independent single-channel devices spanning six\nmicrophone types at distances from 0.12 m to 10 m, preserving the authentic\neffects of reverberation, noise, and device coloration without relying on\nmicrophone arrays. We provide standard train, dev, test splits and release a\nreproducible baseline system. We benchmarked several Whisper variants under\nzero-shot and fine-tuned conditions. Off-the-shelf models showed strong\ndegradation with distance, confirming a mismatch between pre-training data and\nThai far-field speech. Fine-tuning on LOTUSDIS dramatically improved\nrobustness: a Thai Whisper baseline reduced overall WER from 64.3 to 38.3 and\nfar-field WER from 81.6 to 49.5, with especially large gains on the most\ndistant microphones. These results underscore the importance of\ndistance-diverse training data for robust ASR. The corpus is available under\nCC-BY-SA 4.0. We also release training and evaluation scripts as a baseline\nsystem to promote reproducible research in this field.", "AI": {"tldr": "该论文发布了一个名为LOTUSDIS的泰语会议语料库，包含114小时远场对话录音，用于提升远场会话式ASR。实验表明，使用LOTUSDIS进行微调可显著提高Whisper模型对远场语音的鲁棒性，尤其是在远距离麦克风上的表现。", "motivation": "现有ASR模型在远场泰语语音识别中表现不佳，且缺乏合适的远场、距离多样化的泰语训练数据，导致预训练数据与远场泰语语音之间存在不匹配。", "method": "构建了LOTUSDIS语料库，包含114小时自发、非脚本对话，由三名参与者在15-20分钟的会话中录制，包含频繁的重叠语音。录音使用九个独立的单通道设备（六种麦克风类型），距离从0.12米到10米不等，保留了混响、噪声和设备音染。提供了标准的训练、开发、测试集划分，并发布了一个可复现的基线系统。在零样本和微调条件下，对多个Whisper变体进行了基准测试。", "result": "现成的模型在识别远距离语音时性能显著下降。在LOTUSDIS上进行微调后，鲁棒性得到极大提升：泰语Whisper基线模型的整体WER从64.3%降至38.3%，远场WER从81.6%降至49.5%，在最远距离麦克风上的提升尤为显著。", "conclusion": "研究结果强调了距离多样化训练数据对于构建鲁棒ASR系统的重要性。LOTUSDIS语料库及其训练和评估脚本已公开发布，以促进该领域的可复现研究。"}}
{"id": "2509.18778", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18778", "abs": "https://arxiv.org/abs/2509.18778", "authors": ["Shijia Ge", "Yinxin Zhang", "Shuzhao Xie", "Weixiang Zhang", "Mingcai Zhou", "Zhi Wang"], "title": "VGGT-DP: Generalizable Robot Control via Vision Foundation Models", "comment": "submitted to AAAI 2026", "summary": "Visual imitation learning frameworks allow robots to learn manipulation\nskills from expert demonstrations. While existing approaches mainly focus on\npolicy design, they often neglect the structure and capacity of visual\nencoders, limiting spatial understanding and generalization. Inspired by\nbiological vision systems, which rely on both visual and proprioceptive cues\nfor robust control, we propose VGGT-DP, a visuomotor policy framework that\nintegrates geometric priors from a pretrained 3D perception model with\nproprioceptive feedback. We adopt the Visual Geometry Grounded Transformer\n(VGGT) as the visual encoder and introduce a proprioception-guided visual\nlearning strategy to align perception with internal robot states, improving\nspatial grounding and closed-loop control. To reduce inference latency, we\ndesign a frame-wise token reuse mechanism that compacts multi-view tokens into\nan efficient spatial representation. We further apply random token pruning to\nenhance policy robustness and reduce overfitting. Experiments on challenging\nMetaWorld tasks show that VGGT-DP significantly outperforms strong baselines\nsuch as DP and DP3, particularly in precision-critical and long-horizon\nscenarios.", "AI": {"tldr": "本文提出VGGT-DP，一个将预训练3D感知模型的几何先验与本体感觉反馈相结合的视觉运动策略框架，通过优化视觉编码器和引入高效机制，显著提升了机器人模仿学习的精度和泛化能力。", "motivation": "现有的视觉模仿学习方法主要侧重于策略设计，但往往忽略了视觉编码器的结构和容量，这限制了机器人的空间理解能力和泛化性。", "method": "本文提出了VGGT-DP框架，它：1) 采用视觉几何基础Transformer (VGGT) 作为视觉编码器；2) 整合了来自预训练3D感知模型的几何先验与本体感觉反馈；3) 引入了本体感觉引导的视觉学习策略，以对齐感知与机器人内部状态；4) 设计了帧级token重用机制以减少推理延迟；5) 应用了随机token剪枝以增强策略鲁棒性并减少过拟合。", "result": "在具有挑战性的MetaWorld任务上的实验表明，VGGT-DP显著优于DP和DP3等强基线模型，尤其在对精度要求高和长时程的场景中表现突出。", "conclusion": "VGGT-DP通过整合几何先验和本体感觉反馈，并优化视觉编码器和推理效率，有效提升了机器人在模仿学习任务中的空间理解、闭环控制、鲁棒性和泛化能力，尤其适用于高精度和长时程操作。"}}
{"id": "2509.18710", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18710", "abs": "https://arxiv.org/abs/2509.18710", "authors": ["Yanjie Fu", "Dongjie Wang", "Wangyang Ying", "Xiangliang Zhang", "Huan Liu", "Jian Pei"], "title": "Autonomous Data Agents: A New Opportunity for Smart Data", "comment": null, "summary": "As data continues to grow in scale and complexity, preparing, transforming,\nand analyzing it remains labor-intensive, repetitive, and difficult to scale.\nSince data contains knowledge and AI learns knowledge from it, the alignment\nbetween AI and data is essential. However, data is often not structured in ways\nthat are optimal for AI utilization. Moreover, an important question arises:\nhow much knowledge can we pack into data through intensive data operations?\nAutonomous data agents (DataAgents), which integrate LLM reasoning with task\ndecomposition, action reasoning and grounding, and tool calling, can\nautonomously interpret data task descriptions, decompose tasks into subtasks,\nreason over actions, ground actions into python code or tool calling, and\nexecute operations. Unlike traditional data management and engineering tools,\nDataAgents dynamically plan workflows, call powerful tools, and adapt to\ndiverse data tasks at scale. This report argues that DataAgents represent a\nparadigm shift toward autonomous data-to-knowledge systems. DataAgents are\ncapable of handling collection, integration, preprocessing, selection,\ntransformation, reweighing, augmentation, reprogramming, repairs, and\nretrieval. Through these capabilities, DataAgents transform complex and\nunstructured data into coherent and actionable knowledge. We first examine why\nthe convergence of agentic AI and data-to-knowledge systems has emerged as a\ncritical trend. We then define the concept of DataAgents and discuss their\narchitectural design, training strategies, as well as the new skills and\ncapabilities they enable. Finally, we call for concerted efforts to advance\naction workflow optimization, establish open datasets and benchmark ecosystems,\nsafeguard privacy, balance efficiency with scalability, and develop trustworthy\nDataAgent guardrails to prevent malicious actions.", "AI": {"tldr": "本文提出数据代理（DataAgents），一种利用大型语言模型（LLM）推理和工具调用的自主系统，旨在将复杂数据转化为可操作的知识，实现数据到知识系统的范式转变。", "motivation": "数据处理、转换和分析过程劳动密集、重复且难以扩展，而数据中包含知识，AI从中学习知识，因此AI与数据的对齐至关重要。然而，数据结构通常不利于AI利用，且如何通过数据操作将更多知识打包到数据中是一个重要问题。", "method": "数据代理（DataAgents）整合了LLM推理、任务分解、行动推理与落地、以及工具调用。它们能自主解释数据任务描述、将任务分解为子任务、推断行动、将行动落地为Python代码或工具调用，并执行操作。与传统工具不同，DataAgents动态规划工作流、调用强大工具并适应大规模多样化数据任务。", "result": "数据代理能够处理数据的收集、集成、预处理、选择、转换、重新加权、增强、重编程、修复和检索等操作。通过这些能力，DataAgents将复杂和非结构化数据转化为连贯且可操作的知识，代表了迈向自主数据到知识系统的范式转变。", "conclusion": "文章呼吁集中精力推进行动工作流优化、建立开放数据集和基准生态系统、保障隐私、平衡效率与可扩展性，并开发可信赖的DataAgent防护措施以防止恶意行为。"}}
{"id": "2509.18473", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18473", "abs": "https://arxiv.org/abs/2509.18473", "authors": ["Binhua Huang", "Wendong Yao", "Shaowu Chen", "Guoxin Wang", "Qingyuan Wang", "Soumyabrata Dev"], "title": "MoCrop: Training Free Motion Guided Cropping for Efficient Video Action Recognition", "comment": "5 pages, 2 figures", "summary": "We introduce MoCrop, a motion-aware adaptive cropping module for efficient\nvideo action recognition in the compressed domain. MoCrop uses motion vectors\nthat are available in H.264 video to locate motion-dense regions and produces a\nsingle clip-level crop that is applied to all I-frames at inference. The module\nis training free, adds no parameters, and can be plugged into diverse\nbackbones. A lightweight pipeline that includes denoising & merge (DM), Monte\nCarlo sampling (MCS), and adaptive cropping (AC) via a motion-density submatrix\nsearch yields robust crops with negligible overhead. On UCF101, MoCrop improves\naccuracy or reduces compute. With ResNet-50, it delivers +3.5% Top-1 accuracy\nat equal FLOPs (attention setting), or +2.4% Top-1 accuracy with 26.5% fewer\nFLOPs (efficiency setting). Applied to CoViAR, it reaches 89.2% Top-1 accuracy\nat the original cost and 88.5% Top-1 accuracy while reducing compute from 11.6\nto 8.5 GFLOPs. Consistent gains on MobileNet-V3, EfficientNet-B1, and Swin-B\nindicate strong generality and make MoCrop practical for real-time deployment\nin the compressed domain. Our code and models are available at\nhttps://github.com/microa/MoCrop.", "AI": {"tldr": "MoCrop是一种运动感知的自适应裁剪模块，利用H.264运动向量在压缩域中高效进行视频动作识别，无需训练，可即插即用，显著提高准确性或降低计算量。", "motivation": "在压缩域中进行高效的视频动作识别，通过关注运动密集区域来减少计算开销并提高识别准确性。", "method": "MoCrop利用H.264视频中可用的运动向量来定位运动密集区域，生成一个适用于推理时所有I帧的片段级裁剪。该模块无需训练，不增加参数，可插入到各种骨干网络中。其轻量级管道包括去噪与合并（DM）、蒙特卡洛采样（MCS）以及通过运动密度子矩阵搜索实现的自适应裁剪（AC）。", "result": "在UCF101数据集上，MoCrop使用ResNet-50时，在相同FLOPs下Top-1准确率提高3.5%，或在FLOPs减少26.5%的情况下Top-1准确率提高2.4%。应用于CoViAR数据集时，在原始成本下达到89.2%的Top-1准确率，或在计算量从11.6 GFLOPs减少到8.5 GFLOPs的情况下达到88.5%的Top-1准确率。在MobileNet-V3、EfficientNet-B1和Swin-B上均表现出一致的提升。", "conclusion": "MoCrop是一个通用且高效的模块，适用于压缩域中的实时视频动作识别部署，能够在可忽略的开销下提高准确性或减少计算量，具有很强的实用性。"}}
{"id": "2509.18742", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18742", "abs": "https://arxiv.org/abs/2509.18742", "authors": ["Yunan Wang", "Jianxin Li", "Ziwei Zhang"], "title": "Global-Recent Semantic Reasoning on Dynamic Text-Attributed Graphs with Large Language Models", "comment": null, "summary": "Dynamic Text-Attribute Graphs (DyTAGs), characterized by time-evolving graph\ninteractions and associated text attributes, are prevalent in real-world\napplications. Existing methods, such as Graph Neural Networks (GNNs) and Large\nLanguage Models (LLMs), mostly focus on static TAGs. Extending these existing\nmethods to DyTAGs is challenging as they largely neglect the recent-global\ntemporal semantics: the recent semantic dependencies among interaction texts\nand the global semantic evolution of nodes over time. Furthermore, applying\nLLMs to the abundant and evolving text in DyTAGs faces efficiency issues. To\ntackle these challenges, we propose Dynamic Global-Recent Adaptive Semantic\nProcessing (DyGRASP), a novel method that leverages LLMs and temporal GNNs to\nefficiently and effectively reason on DyTAGs. Specifically, we first design a\nnode-centric implicit reasoning method together with a sliding window mechanism\nto efficiently capture recent temporal semantics. In addition, to capture\nglobal semantic dynamics of nodes, we leverage explicit reasoning with tailored\nprompts and an RNN-like chain structure to infer long-term semantics. Lastly,\nwe intricately integrate the recent and global temporal semantics as well as\nthe dynamic graph structural information using updating and merging layers.\nExtensive experiments on DyTAG benchmarks demonstrate DyGRASP's superiority,\nachieving up to 34% improvement in Hit@10 for destination node retrieval task.\nBesides, DyGRASP exhibits strong generalization across different temporal GNNs\nand LLMs.", "AI": {"tldr": "本文提出DyGRASP，一种结合大语言模型（LLMs）和时序图神经网络（temporal GNNs）的新方法，能高效有效地处理动态文本属性图（DyTAGs），捕捉近期和全局时间语义。", "motivation": "现有方法（如GNNs和LLMs）主要关注静态文本属性图（TAGs），难以处理DyTAGs中时间演变的图交互和文本属性。它们忽略了近期-全局时间语义（交互文本的近期语义依赖和节点随时间的全局语义演变），且LLMs在处理DyTAGs中大量演变文本时面临效率问题。", "method": "DyGRASP方法包括：1) 采用以节点为中心的隐式推理和滑动窗口机制，高效捕获近期时间语义。2) 利用定制提示和类RNN链式结构的显式推理，推断节点的全局语义动态。3) 通过更新和合并层，精妙地整合近期和全局时间语义以及动态图结构信息。", "result": "在DyTAG基准测试中，DyGRASP表现出卓越性能，在目标节点检索任务中，Hit@10指标最高提升34%。此外，DyGRASP在不同的时序GNNs和LLMs之间展现出强大的泛化能力。", "conclusion": "DyGRASP是一种新颖的方法，能高效且有效地在动态文本属性图上进行推理，通过独特地捕捉近期和全局时间语义，显著优于现有方法并具有良好的泛化性。"}}
{"id": "2509.18786", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18786", "abs": "https://arxiv.org/abs/2509.18786", "authors": ["Johannes A. Gaus", "Loris Schneider", "Yitian Shi", "Jongseok Lee", "Rania Rayyes", "Rudolph Triebel"], "title": "Human-Interpretable Uncertainty Explanations for Point Cloud Registration", "comment": null, "summary": "In this paper, we address the point cloud registration problem, where\nwell-known methods like ICP fail under uncertainty arising from sensor noise,\npose-estimation errors, and partial overlap due to occlusion. We develop a\nnovel approach, Gaussian Process Concept Attribution (GP-CA), which not only\nquantifies registration uncertainty but also explains it by attributing\nuncertainty to well-known sources of errors in registration problems. Our\napproach leverages active learning to discover new uncertainty sources in the\nwild by querying informative instances. We validate GP-CA on three publicly\navailable datasets and in our real-world robot experiment. Extensive ablations\nsubstantiate our design choices. Our approach outperforms other\nstate-of-the-art methods in terms of runtime, high sample-efficiency with\nactive learning, and high accuracy. Our real-world experiment clearly\ndemonstrates its applicability. Our video also demonstrates that GP-CA enables\neffective failure-recovery behaviors, yielding more robust robotic perception.", "AI": {"tldr": "本文提出了一种新颖的高斯过程概念归因（GP-CA）方法，用于解决点云配准中由于传感器噪声、姿态估计误差和部分重叠引起的固有不确定性。GP-CA不仅量化并解释了不确定性来源，还利用主动学习发现新的不确定性。该方法在多个数据集和实际机器人实验中表现出优于现有技术的运行时间、样本效率和准确性，显著提高了机器人感知的鲁棒性。", "motivation": "在点云配准问题中，诸如ICP等知名方法在面临传感器噪声、姿态估计误差和因遮挡导致的部分重叠等不确定性时表现不佳。这促使研究人员开发一种能够有效处理和解释这些不确定性的新方法。", "method": "本文开发了一种名为高斯过程概念归因（GP-CA）的新方法。该方法不仅量化了配准不确定性，还通过将其归因于配准问题中已知的误差来源来解释不确定性。此外，GP-CA利用主动学习通过查询信息实例来发现野外中新的不确定性来源。", "result": "GP-CA在三个公开数据集和实际机器人实验中得到了验证。广泛的消融实验证实了其设计选择。该方法在运行时间、主动学习的高样本效率和高精度方面均优于其他最先进的方法。实际世界的实验清楚地证明了其适用性，并且GP-CA能够实现有效的故障恢复行为，从而产生更鲁棒的机器人感知。", "conclusion": "GP-CA是一种有效且鲁棒的点云配准方法，它通过量化和解释不确定性，并利用主动学习发现新的不确定性来源，显著提高了配准的准确性和效率。其在实际机器人应用中的成功展示了其在增强机器人感知鲁棒性方面的巨大潜力，特别是在故障恢复方面。"}}
{"id": "2509.18771", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18771", "abs": "https://arxiv.org/abs/2509.18771", "authors": ["Xingkun Yin", "Kaibin Huang", "Dong In Kim", "Hongyang Du"], "title": "Experience Scaling: Post-Deployment Evolution For Large Language Models", "comment": null, "summary": "Scaling model size, training data, and compute power have driven advances in\nlarge language models (LLMs), but these approaches are reaching saturation as\nhuman-generated text is exhausted and further gains diminish. We propose\nexperience scaling, a framework for continuous post-deployment evolution for\nLLMs through autonomous interaction with the environment and collaborative\nsharing of accumulated experience. The framework captures raw interactions,\ndistills them into compact, reusable knowledge, and periodically refines stored\ncontent to preserve relevance and efficiency. We validate the framework in\nsimulated real-world scenarios involving generalization to previously unseen\nbut related tasks, repetitive queries, and over-saturated knowledge stores.\nAcross all settings, experience scaling improves accuracy, sustains performance\nover time, and maintains gains when applied to novel situations. These results\ndemonstrate that structured post-deployment learning can extend LLM\ncapabilities beyond the limits of static human-generated data, offering a\nscalable path for continued intelligence progress.", "AI": {"tldr": "本文提出“经验扩展”框架，通过LLM与环境的自主交互和经验共享，实现部署后持续学习，以克服传统扩展方法（模型大小、数据、算力）的局限性。", "motivation": "大型语言模型（LLMs）的进步主要依赖于模型规模、训练数据和计算能力的扩展，但这些方法正因人类生成文本的枯竭而达到饱和，进一步的收益正在减少。", "method": "提出“经验扩展”框架，该框架通过LLM与环境的自主交互和积累经验的协作共享，实现部署后的持续演进。具体包括：捕获原始交互、将其提炼为紧凑可重用的知识，并定期优化存储内容以保持相关性和效率。在模拟真实世界场景中（如泛化到未见但相关的任务、重复查询和过饱和知识库）验证了该框架。", "result": "在所有测试设置中，“经验扩展”框架都提高了准确性，随着时间的推移保持了性能，并在应用于新情境时维持了收益。", "conclusion": "结构化的部署后学习能够将LLM的能力扩展到静态人类生成数据的限制之外，为持续的智能进步提供了一条可扩展的路径。"}}
{"id": "2509.18481", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18481", "abs": "https://arxiv.org/abs/2509.18481", "authors": ["Xinyu Wang", "Zikun Zhou", "Yingjian Li", "Xin An", "Hongpeng Wang"], "title": "Codebook-Based Adaptive Feature Compression With Semantic Enhancement for Edge-Cloud Systems", "comment": null, "summary": "Coding images for machines with minimal bitrate and strong analysis\nperformance is key to effective edge-cloud systems. Several approaches deploy\nan image codec and perform analysis on the reconstructed image. Other methods\ncompress intermediate features using entropy models and subsequently perform\nanalysis on the decoded features. Nevertheless, these methods both perform\npoorly under low-bitrate conditions, as they retain many redundant details or\nlearn over-concentrated symbol distributions. In this paper, we propose a\nCodebook-based Adaptive Feature Compression framework with Semantic\nEnhancement, named CAFC-SE. It maps continuous visual features to discrete\nindices with a codebook at the edge via Vector Quantization (VQ) and\nselectively transmits them to the cloud. The VQ operation that projects feature\nvectors onto the nearest visual primitives enables us to preserve more\ninformative visual patterns under low-bitrate conditions. Hence, CAFC-SE is\nless vulnerable to low-bitrate conditions. Extensive experiments demonstrate\nthe superiority of our method in terms of rate and accuracy.", "AI": {"tldr": "本文提出了一种名为CAFC-SE的码本自适应特征压缩框架，用于在低比特率下为机器编码图像，同时保持强大的分析性能，优于现有方法。", "motivation": "现有图像编码方法（无论是在重建图像上分析还是压缩中间特征）在低比特率条件下表现不佳，因为它们保留了太多冗余细节或学习了过于集中的符号分布，这阻碍了有效的边缘-云系统。", "method": "本文提出了CAFC-SE（Codebook-based Adaptive Feature Compression framework with Semantic Enhancement）。它在边缘端通过矢量量化（VQ）将连续视觉特征映射到离散索引，并使用码本进行选择性传输到云端。VQ操作将特征向量投影到最近的视觉基元上，从而在低比特率条件下保留更多信息丰富的视觉模式。", "result": "广泛的实验证明，CAFC-SE在速率和准确性方面均优于现有方法。它在低比特率条件下受到的影响较小。", "conclusion": "CAFC-SE通过码本和矢量量化机制，成功解决了低比特率下图像编码的挑战，为机器分析提供了更优的性能，使其在边缘-云系统中更具效率和鲁棒性。"}}
{"id": "2509.18750", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18750", "abs": "https://arxiv.org/abs/2509.18750", "authors": ["Julie Kallini", "Dan Jurafsky", "Christopher Potts", "Martijn Bartelds"], "title": "False Friends Are Not Foes: Investigating Vocabulary Overlap in Multilingual Language Models", "comment": null, "summary": "Subword tokenizers trained on multilingual corpora naturally produce\noverlapping tokens across languages. Does token overlap facilitate\ncross-lingual transfer or instead introduce interference between languages?\nPrior work offers mixed evidence, partly due to varied setups and confounders,\nsuch as token frequency or subword segmentation granularity. To address this\nquestion, we devise a controlled experiment where we train bilingual\nautoregressive models on multiple language pairs under systematically varied\nvocabulary overlap settings. Crucially, we explore a new dimension to\nunderstanding how overlap affects transfer: the semantic similarity of tokens\nshared across languages. We first analyze our models' hidden representations\nand find that overlap of any kind creates embedding spaces that capture\ncross-lingual semantic relationships, while this effect is much weaker in\nmodels with disjoint vocabularies. On XNLI and XQuAD, we find that models with\noverlap outperform models with disjoint vocabularies, and that transfer\nperformance generally improves as overlap increases. Overall, our findings\nhighlight the advantages of token overlap in multilingual models and show that\nsubstantial shared vocabulary remains a beneficial design choice for\nmultilingual tokenizers.", "AI": {"tldr": "本研究通过受控实验发现，多语言分词器中的词元重叠有助于跨语言迁移，且重叠程度越高，迁移性能越好，尤其当共享词元具有语义相似性时。", "motivation": "以往关于词元重叠对跨语言迁移影响的研究结果不一，部分原因在于实验设置和混杂因素（如词元频率、子词切分粒度）的差异。本研究旨在明确词元重叠是促进还是阻碍跨语言迁移，并探索共享词元的语义相似性在其中扮演的角色。", "method": "设计受控实验，在不同词汇重叠设置下，训练双语自回归模型。系统性地改变词汇重叠程度，并引入共享词元语义相似性作为新的分析维度。分析模型的隐层表示，并在XNLI和XQuAD数据集上评估模型性能。", "result": "任何形式的词元重叠都能创建捕获跨语言语义关系的嵌入空间，这种效果在词汇不重叠的模型中弱得多。在XNLI和XQuAD任务上，具有重叠词汇的模型优于词汇不重叠的模型，且迁移性能通常随重叠程度的增加而提高。", "conclusion": "词元重叠在多语言模型中具有优势，大量共享词汇仍然是多语言分词器的一种有益设计选择。"}}
{"id": "2509.18793", "categories": ["cs.RO", "cs.MA", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.18793", "abs": "https://arxiv.org/abs/2509.18793", "authors": ["Lukas Zanger", "Bastian Lampe", "Lennart Reiher", "Lutz Eckstein"], "title": "Application Management in C-ITS: Orchestrating Demand-Driven Deployments and Reconfigurations", "comment": "7 pages, 2 figures, 2 tables; Accepted to be published as part of the\n  2025 IEEE International Conference on Intelligent Transportation Systems\n  (ITSC 2025), Gold Coast, Australia, November 18-21, 2025", "summary": "Vehicles are becoming increasingly automated and interconnected, enabling the\nformation of cooperative intelligent transport systems (C-ITS) and the use of\noffboard services. As a result, cloud-native techniques, such as microservices\nand container orchestration, play an increasingly important role in their\noperation. However, orchestrating applications in a large-scale C-ITS poses\nunique challenges due to the dynamic nature of the environment and the need for\nefficient resource utilization. In this paper, we present a demand-driven\napplication management approach that leverages cloud-native techniques -\nspecifically Kubernetes - to address these challenges. Taking into account the\ndemands originating from different entities within the C-ITS, the approach\nenables the automation of processes, such as deployment, reconfiguration,\nupdate, upgrade, and scaling of microservices. Executing these processes on\ndemand can, for example, reduce computing resource consumption and network\ntraffic. A demand may include a request for provisioning an external supporting\nservice, such as a collective environment model. The approach handles changing\nand new demands by dynamically reconciling them through our proposed\napplication management framework built on Kubernetes and the Robot Operating\nSystem (ROS 2). We demonstrate the operation of our framework in the C-ITS use\ncase of collective environment perception and make the source code of the\nprototypical framework publicly available at\nhttps://github.com/ika-rwth-aachen/application_manager .", "AI": {"tldr": "本文提出了一种基于Kubernetes和ROS 2的需求驱动应用管理方法，用于解决协作式智能交通系统（C-ITS）中微服务编排的挑战，实现自动化部署、配置和扩展，以提高资源效率。", "motivation": "随着车辆自动化和互联程度的提高，C-ITS中云原生技术（如微服务和容器编排）的重要性日益增加。然而，在大规模C-ITS中编排应用面临独特挑战，主要源于环境的动态性和对高效资源利用的需求。", "method": "本文提出了一种需求驱动的应用管理方法，利用云原生技术，特别是Kubernetes，并结合机器人操作系统（ROS 2）。该方法根据C-ITS中不同实体的需求，自动化微服务的部署、重新配置、更新、升级和扩展等过程。它通过一个基于Kubernetes和ROS 2构建的应用管理框架，动态协调不断变化和新的需求。", "result": "该方法能够按需执行应用管理过程，从而减少计算资源消耗和网络流量。它能够动态处理不断变化和新的需求。研究人员在C-ITS的集体环境感知用例中展示了该框架的运行效果，并公开了原型框架的源代码。", "conclusion": "本文成功地提出了一个基于Kubernetes和ROS 2的需求驱动应用管理框架，有效应对了C-ITS中动态环境下的应用编排挑战，实现了微服务的自动化管理和高效资源利用。"}}
{"id": "2509.18787", "categories": ["cs.AI", "C.2.4"], "pdf": "https://arxiv.org/pdf/2509.18787", "abs": "https://arxiv.org/abs/2509.18787", "authors": ["Luca Muscariello", "Vijoy Pandey", "Ramiz Polic"], "title": "The AGNTCY Agent Directory Service: Architecture and Implementation", "comment": null, "summary": "The Agent Directory Service (ADS) is a distributed directory for the\ndiscovery of AI agent capabilities, metadata, and provenance. It leverages\ncontent-addressed storage, hierarchical taxonomies, and cryptographic signing\nto enable efficient, verifiable, and multi-dimensional discovery across\nheterogeneous Multi-Agent Systems (MAS). Built on the Open Agentic Schema\nFramework (OASF), ADS decouples capability indexing from content location\nthrough a two-level mapping realized over a Kademlia-based Distributed Hash\nTable (DHT). It reuses mature OCI / ORAS infrastructure for artifact\ndistribution, integrates Sigstore for provenance, and supports schema-driven\nextensibility for emerging agent modalities (LLM prompt agents, MCP servers,\nA2A-enabled components). This paper formalizes the architectural model,\ndescribes storage and discovery layers, explains security and performance\nproperties, and positions ADS within the broader landscape of emerging agent\nregistry and interoperability initiatives.", "AI": {"tldr": "Agent Directory Service (ADS) 是一个分布式目录服务，用于发现AI智能体的能力、元数据和来源，通过内容寻址存储、分层分类和加密签名实现高效、可验证和多维度的发现。", "motivation": "动机是解决异构多智能体系统（MAS）中智能体能力、元数据和来源的发现挑战，需要一个能够实现高效、可验证和多维度发现的分布式目录服务。", "method": "ADS基于Open Agentic Schema Framework (OASF) 构建，通过Kademlia分布式哈希表（DHT）实现两级映射，将能力索引与内容位置解耦。它利用内容寻址存储、分层分类和加密签名，并重用OCI / ORAS基础设施进行工件分发，集成Sigstore进行来源管理，并支持模式驱动的可扩展性，以适应新兴的智能体模式。", "result": "ADS实现了跨异构多智能体系统的高效、可验证和多维度的智能体能力发现。它支持LLM提示智能体、MCP服务器和A2A启用组件等新兴智能体模式，并提供了形式化的架构模型、存储和发现层、安全性和性能属性。", "conclusion": "ADS提供了一个全面的解决方案，用于在复杂多智能体环境中发现智能体能力，并通过其分布式架构、安全特性和对新兴智能体模式的支持，在智能体注册和互操作性领域具有重要地位。"}}
{"id": "2509.18493", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18493", "abs": "https://arxiv.org/abs/2509.18493", "authors": ["Md Mostafijur Rahman", "Radu Marculescu"], "title": "MK-UNet: Multi-kernel Lightweight CNN for Medical Image Segmentation", "comment": "11 pages, 3 figures, Accepted at ICCV 2025 Workshop CVAMD", "summary": "In this paper, we introduce MK-UNet, a paradigm shift towards\nultra-lightweight, multi-kernel U-shaped CNNs tailored for medical image\nsegmentation. Central to MK-UNet is the multi-kernel depth-wise convolution\nblock (MKDC) we design to adeptly process images through multiple kernels,\nwhile capturing complex multi-resolution spatial relationships. MK-UNet also\nemphasizes the images salient features through sophisticated attention\nmechanisms, including channel, spatial, and grouped gated attention. Our\nMK-UNet network, with a modest computational footprint of only 0.316M\nparameters and 0.314G FLOPs, represents not only a remarkably lightweight, but\nalso significantly improved segmentation solution that provides higher accuracy\nover state-of-the-art (SOTA) methods across six binary medical imaging\nbenchmarks. Specifically, MK-UNet outperforms TransUNet in DICE score with\nnearly 333$\\times$ and 123$\\times$ fewer parameters and FLOPs, respectively.\nSimilarly, when compared against UNeXt, MK-UNet exhibits superior segmentation\nperformance, improving the DICE score up to 6.7% margins while operating with\n4.7$\\times$ fewer #Params. Our MK-UNet also outperforms other recent\nlightweight networks, such as MedT, CMUNeXt, EGE-UNet, and Rolling-UNet, with\nmuch lower computational resources. This leap in performance, coupled with\ndrastic computational gains, positions MK-UNet as an unparalleled solution for\nreal-time, high-fidelity medical diagnostics in resource-limited settings, such\nas point-of-care devices. Our implementation is available at\nhttps://github.com/SLDGroup/MK-UNet.", "AI": {"tldr": "本文提出了MK-UNet，一种超轻量级、多核U型卷积神经网络，专为医学图像分割设计，在显著降低计算资源的同时实现了更高的分割精度。", "motivation": "现有医学图像分割方法可能计算成本高昂，难以在资源受限的环境（如即时医疗设备）中实时部署。因此，需要一种既轻量又高效的解决方案。", "method": "MK-UNet的核心是多核深度可分离卷积块（MKDC），它通过多个核处理图像以捕获复杂的多分辨率空间关系。此外，MK-UNet还通过通道、空间和分组门控注意力机制来强调图像的显著特征。", "result": "MK-UNet拥有0.316M参数和0.314G FLOPs的极低计算开销。在六个二元医学图像基准测试中，它在DICE分数上超越了包括TransUNet、UNeXt、MedT等在内的最先进方法，同时参数量和FLOPs分别减少了数百倍（例如，TransUNet的参数减少了约333倍，FLOPs减少了约123倍）。", "conclusion": "MK-UNet以其卓越的性能和极低的计算成本，成为资源受限环境下（如即时医疗设备）实时、高保真医学诊断的无与伦比的解决方案。"}}
{"id": "2509.18762", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18762", "abs": "https://arxiv.org/abs/2509.18762", "authors": ["Yingming Zheng", "Hanqi Li", "Kai Yu", "Lu Chen"], "title": "When Long Helps Short: How Context Length in Supervised Fine-tuning Affects Behavior of Large Language Models", "comment": null, "summary": "Large language models (LLMs) have achieved impressive performance across\nnatural language processing (NLP) tasks. As real-world applications\nincreasingly demand longer context windows, continued pretraining and\nsupervised fine-tuning (SFT) on long-context data has become a common approach.\nWhile the effects of data length in continued pretraining have been extensively\nstudied, their implications for SFT remain unclear. In this work, we\nsystematically investigate how SFT data length influences LLM behavior on\nshort-context tasks. Counterintuitively, we find that long-context SFT improves\nshort-context performance, contrary to the commonly observed degradation from\nlong-context pretraining. To uncover the underlying mechanisms of this\nphenomenon, we first decouple and analyze two key components, Multi-Head\nAttention (MHA) and Feed-Forward Network (FFN), and show that both\nindependently benefit from long-context SFT. We further study their interaction\nand reveal a knowledge preference bias: long-context SFT promotes contextual\nknowledge, while short-context SFT favors parametric knowledge, making\nexclusive reliance on long-context SFT suboptimal. Finally, we demonstrate that\nhybrid training mitigates this bias, offering explainable guidance for\nfine-tuning LLMs.", "AI": {"tldr": "研究发现，长上下文SFT（监督微调）能反直觉地提升LLM在短上下文任务上的性能，这与长上下文预训练的效果相反。这种提升源于MHA和FFN的独立受益，但纯粹的长上下文SFT会产生知识偏好。混合训练能缓解此偏好，提供优化的微调指导。", "motivation": "随着实际应用对LLM长上下文窗口的需求增加，在长上下文数据上进行持续预训练和SFT已成为常见做法。尽管持续预训练中数据长度的影响已被广泛研究，但SFT数据长度对LLM行为，尤其是在短上下文任务上的影响仍不明确。", "method": "本研究系统性地调查了SFT数据长度如何影响LLM在短上下文任务上的表现。首先，解耦并分析了多头注意力（MHA）和前馈网络（FFN）这两个关键组件。其次，研究了它们之间的交互作用，并揭示了知识偏好偏差。最后，通过混合训练来验证其对偏差的缓解作用。", "result": "研究发现，长上下文SFT反直觉地提升了LLM在短上下文任务上的性能，这与长上下文预训练中常见的性能下降现象相反。MHA和FFN均独立受益于长上下文SFT。长上下文SFT促进语境知识，而短上下文SFT偏爱参数知识，导致仅依赖长上下文SFT并非最优。混合训练能有效缓解这种知识偏好偏差。", "conclusion": "长上下文SFT能有效提升LLM在短上下文任务上的性能，通过增强MHA和FFN实现。然而，为避免语境知识与参数知识之间的偏好偏差，建议采用混合训练策略来优化LLM的微调过程，以获得更全面的知识利用能力。"}}
{"id": "2509.18830", "categories": ["cs.RO", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18830", "abs": "https://arxiv.org/abs/2509.18830", "authors": ["Suzannah Wistreich", "Baiyu Shi", "Stephen Tian", "Samuel Clarke", "Michael Nath", "Chengyi Xu", "Zhenan Bao", "Jiajun Wu"], "title": "DexSkin: High-Coverage Conformable Robotic Skin for Learning Contact-Rich Manipulation", "comment": "Accepted to CoRL 2025", "summary": "Human skin provides a rich tactile sensing stream, localizing intentional and\nunintentional contact events over a large and contoured region. Replicating\nthese tactile sensing capabilities for dexterous robotic manipulation systems\nremains a longstanding challenge. In this work, we take a step towards this\ngoal by introducing DexSkin. DexSkin is a soft, conformable capacitive\nelectronic skin that enables sensitive, localized, and calibratable tactile\nsensing, and can be tailored to varying geometries. We demonstrate its efficacy\nfor learning downstream robotic manipulation by sensorizing a pair of parallel\njaw gripper fingers, providing tactile coverage across almost the entire finger\nsurfaces. We empirically evaluate DexSkin's capabilities in learning\nchallenging manipulation tasks that require sensing coverage across the entire\nsurface of the fingers, such as reorienting objects in hand and wrapping\nelastic bands around boxes, in a learning-from-demonstration framework. We then\nshow that, critically for data-driven approaches, DexSkin can be calibrated to\nenable model transfer across sensor instances, and demonstrate its\napplicability to online reinforcement learning on real robots. Our results\nhighlight DexSkin's suitability and practicality for learning real-world,\ncontact-rich manipulation. Please see our project webpage for videos and\nvisualizations: https://dex-skin.github.io/.", "AI": {"tldr": "本文提出DexSkin，一种柔软、可形变的电容式电子皮肤，能实现灵敏、局部化、可校准的触觉传感，并适用于各种几何形状，显著提升机器人学习复杂操作任务的能力。", "motivation": "为灵巧的机器人操作系统复制人类皮肤丰富的触觉感知能力，以定位大面积和异形区域的接触事件，是一个长期存在的挑战。", "method": "引入DexSkin，一种柔软、可形变的电容式电子皮肤，可定制以适应不同几何形状。通过在平行爪夹持器手指上安装DexSkin，覆盖几乎整个手指表面，实现触觉感知。在学习演示框架下，评估DexSkin在学习需要手指整个表面感知覆盖的复杂操作任务（如手内物体重新定向、将橡皮筋缠绕在盒子上）中的能力。展示DexSkin的可校准性，以实现传感器实例间的模型迁移，并证明其适用于真实机器人的在线强化学习。", "result": "DexSkin实现了灵敏、局部化、可校准的触觉感知。它能有效学习需要手指整个表面感知覆盖的复杂操作任务。DexSkin可以校准以实现传感器实例间的模型迁移，并适用于真实机器人的在线强化学习。", "conclusion": "DexSkin非常适合并实用，能够支持机器人学习现实世界中接触丰富的操作任务。"}}
{"id": "2509.18836", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18836", "abs": "https://arxiv.org/abs/2509.18836", "authors": ["Dennis Gross", "Helge Spieker", "Arnaud Gotlieb"], "title": "Bounded PCTL Model Checking of Large Language Model Outputs", "comment": "ICTAI 2025", "summary": "In this paper, we introduce LLMCHECKER, a model-checking-based verification\nmethod to verify the probabilistic computation tree logic (PCTL) properties of\nan LLM text generation process. We empirically show that only a limited number\nof tokens are typically chosen during text generation, which are not always the\nsame. This insight drives the creation of $\\alpha$-$k$-bounded text generation,\nnarrowing the focus to the $\\alpha$ maximal cumulative probability on the\ntop-$k$ tokens at every step of the text generation process. Our verification\nmethod considers an initial string and the subsequent top-$k$ tokens while\naccommodating diverse text quantification methods, such as evaluating text\nquality and biases. The threshold $\\alpha$ further reduces the selected tokens,\nonly choosing those that exceed or meet it in cumulative probability.\nLLMCHECKER then allows us to formally verify the PCTL properties of\n$\\alpha$-$k$-bounded LLMs. We demonstrate the applicability of our method in\nseveral LLMs, including Llama, Gemma, Mistral, Genstruct, and BERT. To our\nknowledge, this is the first time PCTL-based model checking has been used to\ncheck the consistency of the LLM text generation process.", "AI": {"tldr": "本文提出了LLMCHECKER，一种基于模型检查的验证方法，用于验证大型语言模型（LLM）文本生成过程中概率计算树逻辑（PCTL）属性。通过引入$\\alpha$-$k$有界文本生成，LLMCHECKER能形式化验证LLM的PCTL属性。", "motivation": "研究发现LLM文本生成过程中通常只选择有限数量的词元，且这些词元并非总是一致。这种概率性和选择的有限性促使研究者寻求一种形式化方法来验证LLM文本生成过程的属性。", "method": "该方法基于对LLM文本生成过程的观察，即每次生成只选择有限且不总是相同的词元。在此基础上，引入了“$\\alpha$-$k$有界文本生成”概念，将关注点缩小到每一步文本生成中累积概率达到$\\alpha$的最高$k$个词元。LLMCHECKER考虑初始字符串和随后的最高$k$个词元，并能适应各种文本量化方法（如评估文本质量和偏差）。阈值$\\alpha$进一步减少了选定的词元，只选择累积概率超过或达到它的词元。最终，LLMCHECKER形式化验证$\\alpha$-$k$有界LLM的PCTL属性。", "result": "LLMCHECKER能够形式化验证$\\alpha$-$k$有界LLM的PCTL属性。该方法已在Llama、Gemma、Mistral、Genstruct和BERT等多种LLM上验证了其适用性。据作者所知，这是首次将基于PCTL的模型检查用于检查LLM文本生成过程的一致性。", "conclusion": "LLMCHECKER提供了一种新颖的、基于模型检查的验证方法，能够形式化验证LLM文本生成过程的PCTL属性。通过引入$\\alpha$-$k$有界生成概念，该方法有效地处理了LLM生成过程的概率性，并已在多种主流LLM上展示了其广泛适用性，填补了PCTL模型检查在LLM文本生成一致性验证领域的空白。"}}
{"id": "2509.18501", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18501", "abs": "https://arxiv.org/abs/2509.18501", "authors": ["Maximilian Fehrentz", "Alexander Winkler", "Thomas Heiliger", "Nazim Haouchine", "Christian Heiliger", "Nassir Navab"], "title": "BridgeSplat: Bidirectionally Coupled CT and Non-Rigid Gaussian Splatting for Deformable Intraoperative Surgical Navigation", "comment": "Accepted at MICCAI 2025", "summary": "We introduce BridgeSplat, a novel approach for deformable surgical navigation\nthat couples intraoperative 3D reconstruction with preoperative CT data to\nbridge the gap between surgical video and volumetric patient data. Our method\nrigs 3D Gaussians to a CT mesh, enabling joint optimization of Gaussian\nparameters and mesh deformation through photometric supervision. By\nparametrizing each Gaussian relative to its parent mesh triangle, we enforce\nalignment between Gaussians and mesh and obtain deformations that can be\npropagated back to update the CT. We demonstrate BridgeSplat's effectiveness on\nvisceral pig surgeries and synthetic data of a human liver under simulation,\nshowing sensible deformations of the preoperative CT on monocular RGB data.\nCode, data, and additional resources can be found at\nhttps://maxfehrentz.github.io/ct-informed-splatting/ .", "AI": {"tldr": "BridgeSplat是一种新颖的可变形手术导航方法，它将术中3D重建与术前CT数据结合，通过将3D高斯粒子绑定到CT网格并进行联合优化，实现从手术视频到体积患者数据的桥接，并能将变形传播回CT。", "motivation": "该研究的动机是弥合手术视频与体积患者数据之间的鸿沟，以实现可变形的手术导航。", "method": "该方法名为BridgeSplat，将3D高斯粒子绑定到CT网格上，通过光度监督联合优化高斯参数和网格变形。每个高斯粒子相对于其父网格三角形进行参数化，以确保高斯粒子与网格对齐，并使变形能够传播回CT以更新数据。", "result": "BridgeSplat在内脏猪手术和模拟人体肝脏的合成数据上都展示了其有效性，表明它能在单目RGB数据上对术前CT产生合理的变形。", "conclusion": "BridgeSplat成功地通过结合术中重建和术前CT数据，提供了一种有效的可变形手术导航解决方案，能够基于手术视频的变形更新术前CT数据。"}}
{"id": "2509.18775", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18775", "abs": "https://arxiv.org/abs/2509.18775", "authors": ["Wei-Ning Chiu", "Yu-Hsiang Wang", "Andy Hsiao", "Yu-Shiang Huang", "Chuan-Ju Wang"], "title": "Financial Risk Relation Identification through Dual-view Adaptation", "comment": "11 pages, 3 figures, EMNLP 2025 Main Conference", "summary": "A multitude of interconnected risk events -- ranging from regulatory changes\nto geopolitical tensions -- can trigger ripple effects across firms.\nIdentifying inter-firm risk relations is thus crucial for applications like\nportfolio management and investment strategy. Traditionally, such assessments\nrely on expert judgment and manual analysis, which are, however, subjective,\nlabor-intensive, and difficult to scale. To address this, we propose a\nsystematic method for extracting inter-firm risk relations using Form 10-K\nfilings -- authoritative, standardized financial documents -- as our data\nsource. Leveraging recent advances in natural language processing, our approach\ncaptures implicit and abstract risk connections through unsupervised\nfine-tuning based on chronological and lexical patterns in the filings. This\nenables the development of a domain-specific financial encoder with a deeper\ncontextual understanding and introduces a quantitative risk relation score for\ntransparency, interpretable analysis. Extensive experiments demonstrate that\nour method outperforms strong baselines across multiple evaluation settings.", "AI": {"tldr": "该论文提出了一种利用10-K文件和无监督NLP方法，系统地提取企业间风险关系并量化风险关联分数的新方法，优于传统基线。", "motivation": "识别企业间风险关系对投资管理至关重要，但传统方法（如专家判断和人工分析）主观、劳动密集且难以规模化，因此需要一种系统性解决方案。", "method": "该方法以10-K文件为数据源，利用自然语言处理技术，通过基于时间顺序和词汇模式的无监督微调，捕捉隐性和抽象的风险联系。这开发了一个领域特定的金融编码器，并引入了量化的风险关系分数以提高透明度和可解释性。", "result": "广泛的实验表明，该方法在多种评估设置下均优于强大的基线模型。", "conclusion": "该研究提供了一种系统、定量且可解释的方法来提取企业间风险关系，克服了传统评估的局限性，并为投资策略等应用提供了更深入的洞察。"}}
{"id": "2509.18865", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18865", "abs": "https://arxiv.org/abs/2509.18865", "authors": ["Masato Kobayashi", "Thanpimon Buamanee"], "title": "Bi-VLA: Bilateral Control-Based Imitation Learning via Vision-Language Fusion for Action Generation", "comment": null, "summary": "We propose Bilateral Control-Based Imitation Learning via Vision-Language\nFusion for Action Generation (Bi-VLA), a novel framework that extends bilateral\ncontrol-based imitation learning to handle more than one task within a single\nmodel. Conventional bilateral control methods exploit joint angle, velocity,\ntorque, and vision for precise manipulation but require task-specific models,\nlimiting their generality. Bi-VLA overcomes this limitation by utilizing robot\njoint angle, velocity, and torque data from leader-follower bilateral control\nwith visual features and natural language instructions through SigLIP and\nFiLM-based fusion. We validated Bi-VLA on two task types: one requiring\nsupplementary language cues and another distinguishable solely by vision.\nReal-robot experiments showed that Bi-VLA successfully interprets\nvision-language combinations and improves task success rates compared to\nconventional bilateral control-based imitation learning. Our Bi-VLA addresses\nthe single-task limitation of prior bilateral approaches and provides empirical\nevidence that combining vision and language significantly enhances versatility.\nExperimental results validate the effectiveness of Bi-VLA in real-world tasks.\nFor additional material, please visit the website:\nhttps://mertcookimg.github.io/bi-vla/", "AI": {"tldr": "本文提出Bi-VLA框架，通过视觉-语言融合将双边控制的模仿学习扩展到单模型处理多任务，解决了传统方法的任务特异性限制。", "motivation": "传统的双边控制方法虽然能实现精确操作，但需要针对特定任务构建模型，通用性受限，无法在一个模型中处理多个任务。", "method": "Bi-VLA利用领导者-跟随者双边控制中的机器人关节角度、速度和扭矩数据，并结合SigLIP提取的视觉特征以及通过FiLM融合的自然语言指令，实现视觉-语言融合。", "result": "Bi-VLA成功解释了视觉-语言组合，并在需要语言提示和仅通过视觉可区分的两类任务上进行了验证。与传统双边控制模仿学习相比，Bi-VLA显著提高了任务成功率。", "conclusion": "Bi-VLA解决了现有双边方法单任务的局限性，并提供了经验证据，表明结合视觉和语言能显著增强多任务处理的通用性和有效性。"}}
{"id": "2509.18846", "categories": ["cs.AI", "I.2.6; I.2.7; J.3"], "pdf": "https://arxiv.org/pdf/2509.18846", "abs": "https://arxiv.org/abs/2509.18846", "authors": ["Hong-Jie Dai", "Zheng-Hao Li", "An-Tai Lu", "Bo-Tsz Shain", "Ming-Ta Li", "Tatheer Hussain Mir", "Kuang-Te Wang", "Min-I Su", "Pei-Kang Liu", "Ming-Ju Tsai"], "title": "Model selection meets clinical semantics: Optimizing ICD-10-CM prediction via LLM-as-Judge evaluation, redundancy-aware sampling, and section-aware fine-tuning", "comment": "28 Pages, 4 Figures, 2 Tables", "summary": "Accurate International Classification of Diseases (ICD) coding is critical\nfor clinical documentation, billing, and healthcare analytics, yet it remains a\nlabour-intensive and error-prone task. Although large language models (LLMs)\nshow promise in automating ICD coding, their challenges in base model\nselection, input contextualization, and training data redundancy limit their\neffectiveness. We propose a modular framework for ICD-10 Clinical Modification\n(ICD-10-CM) code prediction that addresses these challenges through principled\nmodel selection, redundancy-aware data sampling, and structured input design.\nThe framework integrates an LLM-as-judge evaluation protocol with Plackett-Luce\naggregation to assess and rank open-source LLMs based on their intrinsic\ncomprehension of ICD-10-CM code definitions. We introduced embedding-based\nsimilarity measures, a redundancy-aware sampling strategy to remove\nsemantically duplicated discharge summaries. We leverage structured discharge\nsummaries from Taiwanese hospitals to evaluate contextual effects and examine\nsection-wise content inclusion under universal and section-specific modelling\nparadigms. Experiments across two institutional datasets demonstrate that the\nselected base model after fine-tuning consistently outperforms baseline LLMs in\ninternal and external evaluations. Incorporating more clinical sections\nconsistently improves prediction performance. This study uses open-source LLMs\nto establish a practical and principled approach to ICD-10-CM code prediction.\nThe proposed framework provides a scalable, institution-ready solution for\nreal-world deployment of automated medical coding systems by combining informed\nmodel selection, efficient data refinement, and context-aware prompting.", "AI": {"tldr": "本文提出一个模块化框架，通过原则性模型选择、冗余感知数据采样和结构化输入设计，解决大型语言模型在ICD-10-CM编码预测中的挑战，并使用开源LLM实现了可扩展的自动化医疗编码解决方案。", "motivation": "国际疾病分类（ICD）编码对临床文档、计费和医疗分析至关重要，但它耗时且易出错。尽管大型语言模型（LLMs）在自动化ICD编码方面显示出潜力，但其在基础模型选择、输入情境化和训练数据冗余方面的挑战限制了其有效性。", "method": "该框架包括：1) 结合LLM作为评估器和Plackett-Luce聚合的评估协议，以评估和排名开源LLM的内在理解能力。2) 引入基于嵌入的相似性度量和冗余感知采样策略来去除语义重复的出院摘要。3) 利用台湾医院的结构化出院摘要，通过通用和特定章节建模范式评估上下文效应和章节内容包含。4) 通过精细调整，选择基础模型进行预测。", "result": "在两个机构数据集上的实验表明，经过精细调整后选择的基础模型在内部和外部评估中始终优于基线LLM。纳入更多的临床章节一致地提高了预测性能。", "conclusion": "本研究使用开源LLM建立了一种实用且有原则的ICD-10-CM编码预测方法。所提出的框架通过结合知情的模型选择、高效的数据精炼和上下文感知的提示，为自动化医疗编码系统在实际部署中提供了一个可扩展、机构就绪的解决方案。"}}
{"id": "2509.18502", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18502", "abs": "https://arxiv.org/abs/2509.18502", "authors": ["Wenjie Liu", "Hongmin Liu", "Lixin Zhang", "Bin Fan"], "title": "Source-Free Domain Adaptive Semantic Segmentation of Remote Sensing Images with Diffusion-Guided Label Enrichment", "comment": null, "summary": "Research on unsupervised domain adaptation (UDA) for semantic segmentation of\nremote sensing images has been extensively conducted. However, research on how\nto achieve domain adaptation in practical scenarios where source domain data is\ninaccessible namely, source-free domain adaptation (SFDA) remains limited.\nSelf-training has been widely used in SFDA, which requires obtaining as many\nhigh-quality pseudo-labels as possible to train models on target domain data.\nMost existing methods optimize the entire pseudo-label set to obtain more\nsupervisory information. However, as pseudo-label sets often contain\nsubstantial noise, simultaneously optimizing all labels is challenging. This\nlimitation undermines the effectiveness of optimization approaches and thus\nrestricts the performance of self-training. To address this, we propose a novel\npseudo-label optimization framework called Diffusion-Guided Label Enrichment\n(DGLE), which starts from a few easily obtained high-quality pseudo-labels and\npropagates them to a complete set of pseudo-labels while ensuring the quality\nof newly generated labels. Firstly, a pseudo-label fusion method based on\nconfidence filtering and super-resolution enhancement is proposed, which\nutilizes cross-validation of details and contextual information to obtain a\nsmall number of high-quality pseudo-labels as initial seeds. Then, we leverage\nthe diffusion model to propagate incomplete seed pseudo-labels with irregular\ndistributions due to its strong denoising capability for randomly distributed\nnoise and powerful modeling capacity for complex distributions, thereby\ngenerating complete and high-quality pseudo-labels. This method effectively\navoids the difficulty of directly optimizing the complete set of pseudo-labels,\nsignificantly improves the quality of pseudo-labels, and thus enhances the\nmodel's performance in the target domain.", "AI": {"tldr": "本文提出了一种名为Diffusion-Guided Label Enrichment (DGLE)的新型框架，用于遥感图像语义分割的无源域自适应（SFDA）。该框架通过扩散模型将少量高质量的初始伪标签传播并丰富为完整的、高质量的伪标签集，从而解决了传统自训练方法中伪标签噪声大、难以直接优化完整集的问题。", "motivation": "遥感图像语义分割的无监督域自适应（UDA）研究已广泛进行，但在实际无源域数据可访问的SFDA场景中，研究仍有限。现有SFDA自训练方法通常需要优化包含大量噪声的完整伪标签集，这限制了优化效果和模型性能。", "method": "提出Diffusion-Guided Label Enrichment (DGLE)框架。首先，通过基于置信度过滤和超分辨率增强的伪标签融合方法，利用细节和上下文信息的交叉验证，获取少量高质量的初始伪标签（种子）。然后，利用扩散模型强大的去噪和复杂分布建模能力，将这些不完整、不规则分布的种子伪标签传播并生成完整且高质量的伪标签。", "result": "该方法有效避免了直接优化完整伪标签集的困难，显著提高了伪标签的质量，从而提升了模型在目标域的性能。", "conclusion": "DGLE框架通过从少量高质量伪标签开始，并利用扩散模型进行传播和丰富，成功解决了SFDA中伪标签噪声大、难以优化的问题，显著提升了遥感图像语义分割在无源域自适应场景下的性能。"}}
{"id": "2509.18776", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18776", "abs": "https://arxiv.org/abs/2509.18776", "authors": ["Chen Liang", "Zhaoqi Huang", "Haofen Wang", "Fu Chai", "Chunying Yu", "Huanhuan Wei", "Zhengjie Liu", "Yanpeng Li", "Hongjun Wang", "Ruifeng Luo", "Xianzhong Zhao"], "title": "AECBench: A Hierarchical Benchmark for Knowledge Evaluation of Large Language Models in the AEC Field", "comment": null, "summary": "Large language models (LLMs), as a novel information technology, are seeing\nincreasing adoption in the Architecture, Engineering, and Construction (AEC)\nfield. They have shown their potential to streamline processes throughout the\nbuilding lifecycle. However, the robustness and reliability of LLMs in such a\nspecialized and safety-critical domain remain to be evaluated. To address this\nchallenge, this paper establishes AECBench, a comprehensive benchmark designed\nto quantify the strengths and limitations of current LLMs in the AEC domain.\nThe benchmark defines 23 representative tasks within a five-level\ncognition-oriented evaluation framework encompassing Knowledge Memorization,\nUnderstanding, Reasoning, Calculation, and Application. These tasks were\nderived from authentic AEC practice, with scope ranging from codes retrieval to\nspecialized documents generation. Subsequently, a 4,800-question dataset\nencompassing diverse formats, including open-ended questions, was crafted\nprimarily by engineers and validated through a two-round expert review.\nFurthermore, an LLM-as-a-Judge approach was introduced to provide a scalable\nand consistent methodology for evaluating complex, long-form responses\nleveraging expert-derived rubrics. Through the evaluation of nine LLMs, a clear\nperformance decline across five cognitive levels was revealed. Despite\ndemonstrating proficiency in foundational tasks at the Knowledge Memorization\nand Understanding levels, the models showed significant performance deficits,\nparticularly in interpreting knowledge from tables in building codes, executing\ncomplex reasoning and calculation, and generating domain-specific documents.\nConsequently, this study lays the groundwork for future research and\ndevelopment aimed at the robust and reliable integration of LLMs into\nsafety-critical engineering practices.", "AI": {"tldr": "本研究建立了AECBench基准测试平台，用于评估大型语言模型（LLMs）在建筑、工程和施工（AEC）领域的性能，发现LLMs在复杂推理、计算和专业文档生成方面存在显著不足。", "motivation": "LLMs在AEC领域应用日益广泛，但其在该专业且安全关键领域中的鲁棒性和可靠性尚未得到充分评估。", "method": "本研究建立了AECBench，一个全面的基准测试平台，包含23项代表性任务，涵盖知识记忆、理解、推理、计算和应用五个认知层面。构建了一个包含4800个问题的多格式数据集，并采用“LLM作为评判者”的方法，结合专家制定的评分标准来评估复杂、长篇的回答。", "result": "通过评估九个LLM，发现模型的性能随着认知水平的提高而显著下降。LLMs在知识记忆和理解等基础任务上表现良好，但在解释建筑规范中的表格知识、执行复杂推理和计算，以及生成领域特定文档方面表现出明显的性能缺陷。", "conclusion": "本研究为未来旨在将LLMs稳健可靠地集成到安全关键工程实践中的研究和开发奠定了基础。"}}
{"id": "2509.18937", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18937", "abs": "https://arxiv.org/abs/2509.18937", "authors": ["Yanyuan Qiao", "Kieran Gilday", "Yutong Xie", "Josie Hughes"], "title": "Lang2Morph: Language-Driven Morphological Design of Robotic Hands", "comment": null, "summary": "Designing robotic hand morphologies for diverse manipulation tasks requires\nbalancing dexterity, manufacturability, and task-specific functionality. While\nopen-source frameworks and parametric tools support reproducible design, they\nstill rely on expert heuristics and manual tuning. Automated methods using\noptimization are often compute-intensive, simulation-dependent, and rarely\ntarget dexterous hands. Large language models (LLMs), with their broad\nknowledge of human-object interactions and strong generative capabilities,\noffer a promising alternative for zero-shot design reasoning. In this paper, we\npresent Lang2Morph, a language-driven pipeline for robotic hand design. It uses\nLLMs to translate natural-language task descriptions into symbolic structures\nand OPH-compatible parameters, enabling 3D-printable task-specific\nmorphologies. The pipeline consists of: (i) Morphology Design, which maps tasks\ninto semantic tags, structural grammars, and OPH-compatible parameters; and\n(ii) Selection and Refinement, which evaluates design candidates based on\nsemantic alignment and size compatibility, and optionally applies LLM-guided\nrefinement when needed. We evaluate Lang2Morph across varied tasks, and results\nshow that our approach can generate diverse, task-relevant morphologies. To our\nknowledge, this is the first attempt to develop an LLM-based framework for\ntask-conditioned robotic hand design.", "AI": {"tldr": "Lang2Morph是一个由语言驱动的机器人手部设计流程，它利用大型语言模型（LLMs）将自然语言任务描述转换为可3D打印的、任务特定的手部形态参数。", "motivation": "现有的机器人手部形态设计方法依赖专家启发式、手动调整或计算密集型优化，且很少针对灵巧型手部。大型语言模型（LLMs）在人类-物体交互和生成能力方面的广泛知识为零样本设计推理提供了一种有前景的替代方案。", "method": "Lang2Morph流程包括两部分：(i) 形态设计：LLMs将任务映射到语义标签、结构语法和OPH兼容参数；(ii) 选择与优化：评估设计候选方案的语义对齐和尺寸兼容性，并根据需要应用LLM引导的优化。", "result": "Lang2Morph在各种任务中进行了评估，结果表明该方法能够生成多样化且与任务相关的形态。", "conclusion": "这是首次尝试开发一个基于LLM的框架，用于任务条件下的机器人手部设计。"}}
{"id": "2509.18849", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18849", "abs": "https://arxiv.org/abs/2509.18849", "authors": ["Wenke Huang", "Quan Zhang", "Yiyang Fang", "Jian Liang", "Xuankun Rong", "Huanjin Yao", "Guancheng Wan", "Ke Liang", "Wenwen He", "Mingjun Li", "Leszek Rutkowski", "Mang Ye", "Bo Du", "Dacheng Tao"], "title": "MAPO: Mixed Advantage Policy Optimization", "comment": null, "summary": "Recent advances in reinforcement learning for foundation models, such as\nGroup Relative Policy Optimization (GRPO), have significantly improved the\nperformance of foundation models on reasoning tasks. Notably, the advantage\nfunction serves as a central mechanism in GRPO for ranking the trajectory\nimportance. However, existing explorations encounter both advantage reversion\nand advantage mirror problems, which hinder the reasonable advantage allocation\nacross different query samples. In this work, we propose an easy but effective\nGRPO strategy, Mixed Advantage Policy Optimization (MAPO). We reveal that the\ntrajectory appears with different certainty and propose the advantage percent\ndeviation for samples with high-certainty trajectories. Furthermore, we\ndynamically reweight the advantage function for samples with varying trajectory\ncertainty, thereby adaptively configuring the advantage function to account for\nsample-specific characteristics. Comparison with related state-of-the-art\nmethods, along with ablation studies on different advantage variants, validates\nthe effectiveness of our approach.", "AI": {"tldr": "本文提出了一种名为混合优势策略优化（MAPO）的GRPO策略，通过引入优势百分比偏差和动态重加权来解决现有优势函数在不同查询样本中分配不合理的问题，从而提升基础模型在推理任务上的性能。", "motivation": "现有基于强化学习的GRPO方法在基础模型推理任务中，其优势函数在轨迹重要性排序时存在“优势反转”和“优势镜像”问题，阻碍了优势在不同查询样本间的合理分配。", "method": "本文提出MAPO策略。首先，针对高确定性轨迹的样本，引入优势百分比偏差。其次，根据轨迹确定性的不同，动态重加权优势函数，以自适应地考虑样本特异性。", "result": "通过与现有最先进方法的比较以及对不同优势变体的消融研究，验证了MAPO方法的有效性。", "conclusion": "MAPO是一种简单但有效的GRPO策略，它通过解决优势函数分配问题，显著提升了基础模型在推理任务上的表现。"}}
{"id": "2509.18504", "categories": ["cs.CV", "cs.AI", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.18504", "abs": "https://arxiv.org/abs/2509.18504", "authors": ["Jiaxin Dai", "Xiang Xiang"], "title": "Hyperbolic Coarse-to-Fine Few-Shot Class-Incremental Learning", "comment": null, "summary": "In the field of machine learning, hyperbolic space demonstrates superior\nrepresentation capabilities for hierarchical data compared to conventional\nEuclidean space. This work focuses on the Coarse-To-Fine Few-Shot\nClass-Incremental Learning (C2FSCIL) task. Our study follows the Knowe\napproach, which contrastively learns coarse class labels and subsequently\nnormalizes and freezes the classifier weights of learned fine classes in the\nembedding space. To better interpret the \"coarse-to-fine\" paradigm, we propose\nembedding the feature extractor into hyperbolic space. Specifically, we employ\nthe Poincar\\'e ball model of hyperbolic space, enabling the feature extractor\nto transform input images into feature vectors within the Poincar\\'e ball\ninstead of Euclidean space. We further introduce hyperbolic contrastive loss\nand hyperbolic fully-connected layers to facilitate model optimization and\nclassification in hyperbolic space. Additionally, to enhance performance under\nfew-shot conditions, we implement maximum entropy distribution in hyperbolic\nspace to estimate the probability distribution of fine-class feature vectors.\nThis allows generation of augmented features from the distribution to mitigate\noverfitting during training with limited samples. Experiments on C2FSCIL\nbenchmarks show that our method effectively improves both coarse and fine class\naccuracies.", "AI": {"tldr": "本研究提出在双曲空间中进行粗到细少样本类增量学习（C2FSCIL），通过引入庞加莱球模型、双曲对比损失、双曲全连接层及最大熵分布增强，有效提升粗细粒度分类准确性。", "motivation": "传统欧几里得空间在表示层次数据方面不如双曲空间。为了更好地解释“粗到细”范式，并利用双曲空间对层次数据的优越表示能力，本研究旨在将特征提取器嵌入双曲空间以解决C2FSCIL任务。", "method": "该方法遵循Knowe方法，对比学习粗类别标签，并标准化和冻结已学习细类别的分类器权重。具体地，它将特征提取器嵌入到庞加莱球模型中，使特征向量位于双曲空间。同时，引入双曲对比损失和双曲全连接层进行优化和分类。为应对少样本条件下的过拟合，还在双曲空间中实现了最大熵分布来估计细类别特征向量的概率分布，从而生成增强特征。", "result": "在C2FSCIL基准测试上的实验表明，该方法有效提高了粗类别和细类别的分类准确性。", "conclusion": "通过将特征提取器、损失函数和分类层置于双曲空间，并结合双曲空间中的数据增强策略，本研究提出的方法显著提升了C2FSCIL任务中粗粒度和细粒度分类的性能。"}}
{"id": "2509.18792", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18792", "abs": "https://arxiv.org/abs/2509.18792", "authors": ["Sabri Boughorbel", "Fahim Dalvi", "Nadir Durrani", "Majd Hawasly"], "title": "Beyond the Leaderboard: Understanding Performance Disparities in Large Language Models via Model Diffing", "comment": "12 pages, accepted to the 2025 Conference on Empirical Methods in\n  Natural Language Processing (EMNLP 2025)", "summary": "As fine-tuning becomes the dominant paradigm for improving large language\nmodels (LLMs), understanding what changes during this process is increasingly\nimportant. Traditional benchmarking often fails to explain why one model\noutperforms another. In this work, we use model diffing, a mechanistic\ninterpretability approach, to analyze the specific capability differences\nbetween Gemma-2-9b-it and a SimPO-enhanced variant. Using crosscoders, we\nidentify and categorize latent representations that differentiate the two\nmodels. We find that SimPO acquired latent concepts predominantly enhance\nsafety mechanisms (+32.8%), multilingual capabilities (+43.8%), and\ninstruction-following (+151.7%), while its additional training also reduces\nemphasis on model self-reference (-44.1%) and hallucination management\n(-68.5%). Our analysis shows that model diffing can yield fine-grained insights\nbeyond leaderboard metrics, attributing performance gaps to concrete\nmechanistic capabilities. This approach offers a transparent and targeted\nframework for comparing LLMs.", "AI": {"tldr": "本文使用模型差异分析（一种机械可解释性方法）来揭示大型语言模型微调过程中发生的具体能力变化，并量化了SimPO微调对Gemma-2-9b-it模型的影响。", "motivation": "随着微调成为改进大型语言模型（LLMs）的主导范式，理解此过程中发生的变化变得越来越重要。传统的基准测试通常无法解释为何一个模型优于另一个。", "method": "研究采用模型差异分析（一种机械可解释性方法），并利用跨编码器（crosscoders）识别和分类区分两个模型的潜在表征。具体分析了Gemma-2-9b-it及其SimPO增强变体。", "result": "SimPO主要增强了模型的安全机制（+32.8%）、多语言能力（+43.8%）和指令遵循能力（+151.7%）。同时，它也减少了模型对自我参照的强调（-44.1%）和幻觉管理（-68.5%）。", "conclusion": "模型差异分析能够提供超越排行榜指标的细粒度洞察，将性能差距归因于具体的机械能力，为比较LLMs提供了一个透明且有针对性的框架。"}}
{"id": "2509.18953", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18953", "abs": "https://arxiv.org/abs/2509.18953", "authors": ["Hanqing Liu", "Jiahuan Long", "Junqi Wu", "Jiacheng Hou", "Huili Tang", "Tingsong Jiang", "Weien Zhou", "Wen Yao"], "title": "Eva-VLA: Evaluating Vision-Language-Action Models' Robustness Under Real-World Physical Variations", "comment": null, "summary": "Vision-Language-Action (VLA) models have emerged as promising solutions for\nrobotic manipulation, yet their robustness to real-world physical variations\nremains critically underexplored. To bridge this gap, we propose Eva-VLA, the\nfirst unified framework that systematically evaluates the robustness of VLA\nmodels by transforming discrete physical variations into continuous\noptimization problems. However, comprehensively assessing VLA robustness\npresents two key challenges: (1) how to systematically characterize diverse\nphysical variations encountered in real-world deployments while maintaining\nevaluation reproducibility, and (2) how to discover worst-case scenarios\nwithout prohibitive real-world data collection costs efficiently. To address\nthe first challenge, we decompose real-world variations into three critical\ndomains: object 3D transformations that affect spatial reasoning, illumination\nvariations that challenge visual perception, and adversarial patches that\ndisrupt scene understanding. For the second challenge, we introduce a\ncontinuous black-box optimization framework that transforms discrete physical\nvariations into parameter optimization, enabling systematic exploration of\nworst-case scenarios. Extensive experiments on state-of-the-art OpenVLA models\nacross multiple benchmarks reveal alarming vulnerabilities: all variation types\ntrigger failure rates exceeding 60%, with object transformations causing up to\n97.8% failure in long-horizon tasks. Our findings expose critical gaps between\ncontrolled laboratory success and unpredictable deployment readiness, while the\nEva-VLA framework provides a practical pathway for hardening VLA-based robotic\nmanipulation models against real-world deployment challenges.", "AI": {"tldr": "该论文提出了Eva-VLA框架，首次系统性地评估了视觉-语言-动作（VLA）模型在真实物理变化下的鲁棒性，发现现有模型在各种变化下均表现出高失败率，揭示了从实验室到实际部署的关键差距。", "motivation": "VLA模型在机器人操作方面前景广阔，但其对真实世界物理变化的鲁棒性尚未得到充分探索。现有研究缺乏系统性评估方法，且难以在不产生高昂数据收集成本的情况下发现最坏情况。", "method": "该研究提出了Eva-VLA统一框架，将离散的物理变化转化为连续优化问题。为系统表征变化，将其分解为物体3D变换、光照变化和对抗性补丁。为高效发现最坏情况，引入了连续黑盒优化框架，将离散物理变化转化为参数优化。", "result": "对最先进的OpenVLA模型进行广泛实验发现，所有类型的变化都导致超过60%的故障率。其中，物体变换在长周期任务中导致高达97.8%的故障率，揭示了VLA模型在受控实验室成功与不可预测部署就绪之间存在严重差距。", "conclusion": "研究结果暴露了VLA模型在真实世界部署中的关键漏洞。Eva-VLA框架为增强基于VLA的机器人操作模型以应对实际部署挑战提供了一条实用途径。"}}
{"id": "2509.18864", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18864", "abs": "https://arxiv.org/abs/2509.18864", "authors": ["Yingxin Li", "Jianbo Zhao", "Xueyu Ren", "Jie Tang", "Wangjie You", "Xu Chen", "Kan Zhou", "Chao Feng", "Jiao Ran", "Yuan Meng", "Zhi Wang"], "title": "Conf-Profile: A Confidence-Driven Reasoning Paradigm for Label-Free User Profiling", "comment": null, "summary": "User profiling, as a core technique for user understanding, aims to infer\nstructural attributes from user information. Large Language Models (LLMs)\nprovide a promising avenue for user profiling, yet the progress is hindered by\nthe lack of comprehensive benchmarks. To bridge this gap, we propose\nProfileBench, an industrial benchmark derived from a real-world video platform,\nencompassing heterogeneous user data and a well-structured profiling taxonomy.\nHowever, the profiling task remains challenging due to the difficulty of\ncollecting large-scale ground-truth labels, and the heterogeneous and noisy\nuser information can compromise the reliability of LLMs. To approach label-free\nand reliable user profiling, we propose a Confidence-driven Profile reasoning\nframework Conf-Profile, featuring a two-stage paradigm. We first synthesize\nhigh-quality labels by leveraging advanced LLMs with confidence hints, followed\nby confidence-weighted voting for accuracy improvement and confidence\ncalibration for a balanced distribution. The multiple profile results,\nrationales, and confidence scores are aggregated and distilled into a\nlightweight LLM. We further enhance the reasoning ability via confidence-guided\nunsupervised reinforcement learning, which exploits confidence for difficulty\nfiltering, quasi-ground truth voting, and reward weighting. Experimental\nresults demonstrate that Conf-Profile delivers substantial performance through\nthe two-stage training, improving F1 by 13.97 on Qwen3-8B.", "AI": {"tldr": "本文提出ProfileBench，一个工业级用户画像基准，并引入Conf-Profile，一个置信度驱动的无标签用户画像推理框架。该框架通过两阶段训练，利用LLM合成高质量标签并进行置信度引导的无监督强化学习，显著提升了用户画像的F1分数。", "motivation": "尽管大型语言模型（LLMs）在用户画像方面潜力巨大，但缺乏全面的基准测试，且难以收集大规模的真实标签。此外，异构且嘈杂的用户信息会影响LLMs的可靠性。", "method": "1. **ProfileBench基准**：构建了一个来自真实视频平台的工业级基准，包含异构用户数据和结构化画像分类体系。 2. **Conf-Profile框架**：采用两阶段范式实现无标签和可靠的用户画像推理。  a. **第一阶段（标签合成与聚合）**：利用高级LLM结合置信度提示合成高质量标签，并通过置信度加权投票提高准确性，置信度校准平衡分布。将多个画像结果、推理过程和置信度分数聚合并蒸馏到轻量级LLM中。  b. **第二阶段（置信度引导的无监督强化学习）**：利用置信度进行难度过滤、准真实标签投票和奖励加权，以增强推理能力。", "result": "实验结果表明，Conf-Profile通过两阶段训练取得了显著的性能提升，使Qwen3-8B模型的F1分数提高了13.97。", "conclusion": "本文通过引入ProfileBench基准和Conf-Profile框架，有效解决了LLM在用户画像领域面临的基准缺乏、真实标签稀缺以及数据异构嘈杂等挑战，实现了无标签且可靠的用户画像，并取得了显著的性能提升。"}}
{"id": "2509.18538", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18538", "abs": "https://arxiv.org/abs/2509.18538", "authors": ["Zixin Zhu", "Haoxiang Li", "Xuelu Feng", "He Wu", "Chunming Qiao", "Junsong Yuan"], "title": "GeoRemover: Removing Objects and Their Causal Visual Artifacts", "comment": "Accepted as Spotlight at NeurIPS 2025", "summary": "Towards intelligent image editing, object removal should eliminate both the\ntarget object and its causal visual artifacts, such as shadows and reflections.\nHowever, existing image appearance-based methods either follow strictly\nmask-aligned training and fail to remove these causal effects which are not\nexplicitly masked, or adopt loosely mask-aligned strategies that lack\ncontrollability and may unintentionally over-erase other objects. We identify\nthat these limitations stem from ignoring the causal relationship between an\nobject's geometry presence and its visual effects. To address this limitation,\nwe propose a geometry-aware two-stage framework that decouples object removal\ninto (1) geometry removal and (2) appearance rendering. In the first stage, we\nremove the object directly from the geometry (e.g., depth) using strictly\nmask-aligned supervision, enabling structure-aware editing with strong\ngeometric constraints. In the second stage, we render a photorealistic RGB\nimage conditioned on the updated geometry, where causal visual effects are\nconsidered implicitly as a result of the modified 3D geometry. To guide\nlearning in the geometry removal stage, we introduce a preference-driven\nobjective based on positive and negative sample pairs, encouraging the model to\nremove objects as well as their causal visual artifacts while avoiding new\nstructural insertions. Extensive experiments demonstrate that our method\nachieves state-of-the-art performance in removing both objects and their\nassociated artifacts on two popular benchmarks. The code is available at\nhttps://github.com/buxiangzhiren/GeoRemover.", "AI": {"tldr": "本文提出了一种几何感知的两阶段框架，用于智能图像编辑中的物体移除，旨在同时消除目标物体及其因果视觉伪影（如阴影和反射），解决了现有方法无法有效处理未显式遮罩的伪影或过度擦除的问题。", "motivation": "现有基于图像外观的物体移除方法，要么严格遵循遮罩对齐训练，无法移除未显式遮罩的因果视觉伪影（如阴影和反射）；要么采用宽松的遮罩对齐策略，导致可控性不足，可能意外擦除其他物体。这些局限性源于忽略了物体几何存在与其视觉效果之间的因果关系。", "method": "本文提出了一个几何感知的两阶段框架：1. **几何移除**：使用严格的遮罩对齐监督，直接从几何（例如深度）中移除物体，实现具有强几何约束的结构感知编辑。引入了基于正负样本对的偏好驱动目标，鼓励模型移除物体及其因果视觉伪影，同时避免插入新的结构。2. **外观渲染**：基于更新后的几何渲染逼真的RGB图像，其中因果视觉效果被隐式地视为修改后3D几何的结果。", "result": "在两个流行基准测试中，我们的方法在移除物体及其相关伪影方面达到了最先进的性能。", "conclusion": "通过解耦物体移除为几何移除和外观渲染，并利用物体几何与视觉效果之间的因果关系，本文提出的几何感知两阶段框架有效解决了现有方法的局限性，实现了更智能、更彻底的物体移除，包括其伴随的因果视觉伪影。"}}
{"id": "2509.18813", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18813", "abs": "https://arxiv.org/abs/2509.18813", "authors": ["Liting Zhang", "Shiwan Zhao", "Aobo Kong", "Qicheng Li"], "title": "MAPEX: A Multi-Agent Pipeline for Keyphrase Extraction", "comment": null, "summary": "Keyphrase extraction is a fundamental task in natural language processing.\nHowever, existing unsupervised prompt-based methods for Large Language Models\n(LLMs) often rely on single-stage inference pipelines with uniform prompting,\nregardless of document length or LLM backbone. Such one-size-fits-all designs\nhinder the full exploitation of LLMs' reasoning and generation capabilities,\nespecially given the complexity of keyphrase extraction across diverse\nscenarios. To address these challenges, we propose MAPEX, the first framework\nthat introduces multi-agent collaboration into keyphrase extraction. MAPEX\ncoordinates LLM-based agents through modules for expert recruitment, candidate\nextraction, topic guidance, knowledge augmentation, and post-processing. A\ndual-path strategy dynamically adapts to document length: knowledge-driven\nextraction for short texts and topic-guided extraction for long texts.\nExtensive experiments on six benchmark datasets across three different LLMs\ndemonstrate its strong generalization and universality, outperforming the\nstate-of-the-art unsupervised method by 2.44\\% and standard LLM baselines by\n4.01\\% in F1@5 on average. Code is available at\nhttps://github.com/NKU-LITI/MAPEX.", "AI": {"tldr": "MAPEX是首个将多智能体协作引入关键词提取的框架，通过动态适应文档长度的策略，显著优于现有无监督方法和LLM基线，展现出强大的泛化能力。", "motivation": "现有基于大型语言模型（LLM）的无监督提示式关键词提取方法通常采用单阶段、统一的推理流程，不区分文档长度或LLM模型，这限制了LLM在处理多样化关键词提取场景时的推理和生成能力。", "method": "本文提出了MAPEX框架，引入多智能体协作进行关键词提取。MAPEX通过专家招募、候选提取、主题引导、知识增强和后处理等模块协调基于LLM的智能体。它采用双路径策略：对短文本进行知识驱动提取，对长文本进行主题引导提取，以动态适应文档长度。", "result": "在六个基准数据集和三个不同LLM上的广泛实验表明，MAPEX的平均F1@5指标优于最先进的无监督方法2.44%，优于标准LLM基线4.01%，展现出强大的泛化性和普适性。", "conclusion": "MAPEX通过引入多智能体协作和动态适应文档长度的双路径策略，有效解决了现有无监督关键词提取方法的局限性，显著提升了性能，并证明了其在不同场景下的强大泛化能力和普适性。"}}
{"id": "2509.18954", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18954", "abs": "https://arxiv.org/abs/2509.18954", "authors": ["Minoo Dolatabadi", "Fardin Ayar", "Ehsan Javanmardi", "Manabu Tsukada", "Mahdi Javanmardi"], "title": "Towards Robust LiDAR Localization: Deep Learning-based Uncertainty Estimation", "comment": null, "summary": "LiDAR-based localization and SLAM often rely on iterative matching\nalgorithms, particularly the Iterative Closest Point (ICP) algorithm, to align\nsensor data with pre-existing maps or previous scans. However, ICP is prone to\nerrors in featureless environments and dynamic scenes, leading to inaccurate\npose estimation. Accurately predicting the uncertainty associated with ICP is\ncrucial for robust state estimation but remains challenging, as existing\napproaches often rely on handcrafted models or simplified assumptions.\nMoreover, a few deep learning-based methods for localizability estimation\neither depend on a pre-built map, which may not always be available, or provide\na binary classification of localizable versus non-localizable, which fails to\nproperly model uncertainty. In this work, we propose a data-driven framework\nthat leverages deep learning to estimate the registration error covariance of\nICP before matching, even in the absence of a reference map. By associating\neach LiDAR scan with a reliable 6-DoF error covariance estimate, our method\nenables seamless integration of ICP within Kalman filtering, enhancing\nlocalization accuracy and robustness. Extensive experiments on the KITTI\ndataset demonstrate the effectiveness of our approach, showing that it\naccurately predicts covariance and, when applied to localization using a\npre-built map or SLAM, reduces localization errors and improves robustness.", "AI": {"tldr": "本文提出了一种数据驱动的深度学习框架，用于在匹配前和没有参考地图的情况下，估计激光雷达里程计（ICP）的配准误差协方差，从而提高定位精度和鲁棒性。", "motivation": "传统的ICP算法在无特征或动态场景中容易出错，导致姿态估计不准确。现有的不确定性预测方法依赖于手工模型、简化假设或预建地图，且通常只提供二元分类，无法准确建模不确定性。因此，需要一种更鲁棒、更准确的方法来预测ICP的误差不确定性。", "method": "本文提出了一种数据驱动的深度学习框架，该框架能够在ICP匹配之前，甚至在没有参考地图的情况下，估计每次激光雷达扫描的可靠6自由度误差协方差。这种方法使得ICP能够无缝集成到卡尔曼滤波中，以增强定位精度和鲁棒性。", "result": "在KITTI数据集上进行的广泛实验表明，该方法能够准确预测协方差。当应用于使用预建地图的定位或SLAM时，它能有效减少定位误差并提高系统鲁棒性。", "conclusion": "所提出的数据驱动框架通过在匹配前准确估计ICP的误差协方差，显著提升了激光雷达定位和SLAM的精度与鲁棒性，克服了传统方法和现有深度学习方法的局限性。"}}
{"id": "2509.18868", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18868", "abs": "https://arxiv.org/abs/2509.18868", "authors": ["Dianxing Zhang", "Wendong Li", "Kani Song", "Jiaye Lu", "Gang Li", "Liuchun Yang", "Sheng Li"], "title": "Memory in Large Language Models: Mechanisms, Evaluation and Evolution", "comment": "50 pages, 1 figure, 8 tables This is a survey/framework paper on LLM\n  memory mechanisms and evaluation", "summary": "Under a unified operational definition, we define LLM memory as a persistent\nstate written during pretraining, finetuning, or inference that can later be\naddressed and that stably influences outputs. We propose a four-part taxonomy\n(parametric, contextual, external, procedural/episodic) and a memory quadruple\n(location, persistence, write/access path, controllability). We link mechanism,\nevaluation, and governance via the chain write -> read -> inhibit/update. To\navoid distorted comparisons across heterogeneous setups, we adopt a\nthree-setting protocol (parametric only, offline retrieval, online retrieval)\nthat decouples capability from information availability on the same data and\ntimeline. On this basis we build a layered evaluation: parametric (closed-book\nrecall, edit differential, memorization/privacy), contextual (position curves\nand the mid-sequence drop), external (answer correctness vs snippet\nattribution/faithfulness), and procedural/episodic (cross-session consistency\nand timeline replay, E MARS+). The framework integrates temporal governance and\nleakage auditing (freshness hits, outdated answers, refusal slices) and\nuncertainty reporting via inter-rater agreement plus paired tests with\nmultiple-comparison correction. For updating and forgetting, we present DMM\nGov: coordinating DAPT/TAPT, PEFT, model editing (ROME, MEND, MEMIT, SERAC),\nand RAG to form an auditable loop covering admission thresholds, rollout,\nmonitoring, rollback, and change audits, with specs for timeliness, conflict\nhandling, and long-horizon consistency. Finally, we give four testable\npropositions: minimum identifiability; a minimal evaluation card; causally\nconstrained editing with verifiable forgetting; and when retrieval with\nsmall-window replay outperforms ultra-long-context reading. This yields a\nreproducible, comparable, and governable coordinate system for research and\ndeployment.", "AI": {"tldr": "本文为大型语言模型（LLM）记忆提出了一套统一的操作定义、四部分分类法、记忆四元组、三设置评估协议以及分层评估方法。此外，还引入了DMM Gov框架用于记忆更新与遗忘管理，并提出了四个可测试的命题，旨在为LLM记忆研究与部署提供可复现、可比较和可治理的坐标系。", "motivation": "现有LLM记忆研究缺乏统一的操作定义和分类标准，导致不同设置下的比较困难。同时，需要一个全面的框架来连接记忆的机制、评估和治理（写入、读取、抑制/更新），并有效管理记忆的更新和遗忘，确保可审计性和一致性。", "method": "本文定义了LLM记忆的统一操作定义，并提出了四部分分类法（参数记忆、上下文记忆、外部记忆、程序/情景记忆）和记忆四元组（位置、持久性、写入/访问路径、可控性）。采用三设置协议（仅参数、离线检索、在线检索）以解耦能力与信息可用性。在此基础上构建了分层评估体系，涵盖了不同记忆类型的评估指标。为更新和遗忘，提出了DMM Gov框架，协调DAPT/TAPT、PEFT、模型编辑和RAG，形成可审计的循环。最后，提出了四个可测试的命题。", "result": "本文提供了一个统一的LLM记忆定义和分类系统。建立了一个能够避免异构设置下比较失真的三设置评估协议。开发了一个涵盖参数、上下文、外部和程序/情景记忆的全面分层评估框架。设计了一个集成了时间治理和泄露审计的系统，以及一个用于记忆更新和遗忘的DMM Gov可审计框架。提出了四个具体可测试的命题，旨在指导未来的研究方向。这些成果共同为LLM记忆的研究和部署提供了一个可复现、可比较和可治理的坐标系统。", "conclusion": "通过提供统一的定义、分类、评估协议和治理框架，本文为LLM记忆的研究和实际部署奠定了坚实的基础。该框架使研究人员能够对LLM记忆进行更准确、可比较的评估，并为记忆的更新、遗忘和审计提供了一个可控且可追溯的系统，从而促进了LLM领域的进步和负责任的部署。"}}
{"id": "2509.18546", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18546", "abs": "https://arxiv.org/abs/2509.18546", "authors": ["Yujia Liu", "Dingquan Li", "Tiejun Huang"], "title": "SEGA: A Transferable Signed Ensemble Gaussian Black-Box Attack against No-Reference Image Quality Assessment Models", "comment": null, "summary": "No-Reference Image Quality Assessment (NR-IQA) models play an important role\nin various real-world applications. Recently, adversarial attacks against\nNR-IQA models have attracted increasing attention, as they provide valuable\ninsights for revealing model vulnerabilities and guiding robust system design.\nSome effective attacks have been proposed against NR-IQA models in white-box\nsettings, where the attacker has full access to the target model. However,\nthese attacks often suffer from poor transferability to unknown target models\nin more realistic black-box scenarios, where the target model is inaccessible.\nThis work makes the first attempt to address the challenge of low\ntransferability in attacking NR-IQA models by proposing a transferable Signed\nEnsemble Gaussian black-box Attack (SEGA). The main idea is to approximate the\ngradient of the target model by applying Gaussian smoothing to source models\nand ensembling their smoothed gradients. To ensure the imperceptibility of\nadversarial perturbations, SEGA further removes inappropriate perturbations\nusing a specially designed perturbation filter mask. Experimental results on\nthe CLIVE dataset demonstrate the superior transferability of SEGA, validating\nits effectiveness in enabling successful transfer-based black-box attacks\nagainst NR-IQA models.", "AI": {"tldr": "本文提出了一种名为SEGA的可迁移符号集成高斯黑盒攻击方法，首次解决了对NR-IQA模型进行黑盒攻击时迁移性差的问题，并在CLIVE数据集上验证了其有效性。", "motivation": "现有针对NR-IQA模型的对抗性攻击（尤其是在白盒设置下）在更现实的黑盒场景中对未知目标模型的迁移性较差。研究人员希望解决这一低迁移性挑战，以揭示模型漏洞并指导鲁棒系统设计。", "method": "该研究提出了一种可迁移的符号集成高斯黑盒攻击（SEGA）。其核心思想是通过对源模型应用高斯平滑并集成其平滑后的梯度来近似目标模型的梯度。为了确保对抗性扰动的不可感知性，SEGA还使用专门设计的扰动过滤掩码来去除不适当的扰动。", "result": "在CLIVE数据集上的实验结果表明，SEGA具有卓越的迁移性，验证了其在实现针对NR-IQA模型的成功基于迁移的黑盒攻击方面的有效性。", "conclusion": "SEGA方法有效地解决了对NR-IQA模型进行黑盒攻击时低迁移性的挑战，为揭示模型漏洞和设计更鲁棒的系统提供了有价值的见解。"}}
{"id": "2509.18843", "categories": ["cs.CL", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18843", "abs": "https://arxiv.org/abs/2509.18843", "authors": ["Damian Stachura", "Joanna Konieczna", "Artur Nowak"], "title": "Are Smaller Open-Weight LLMs Closing the Gap to Proprietary Models for Biomedical Question Answering?", "comment": "CLEF 2025 Working Notes, 9-12 September 2025, Madrid, Spain", "summary": "Open-weight versions of large language models (LLMs) are rapidly advancing,\nwith state-of-the-art models like DeepSeek-V3 now performing comparably to\nproprietary LLMs. This progression raises the question of whether small\nopen-weight LLMs are capable of effectively replacing larger closed-source\nmodels. We are particularly interested in the context of biomedical\nquestion-answering, a domain we explored by participating in Task 13B Phase B\nof the BioASQ challenge. In this work, we compare several open-weight models\nagainst top-performing systems such as GPT-4o, GPT-4.1, Claude 3.5 Sonnet, and\nClaude 3.7 Sonnet. To enhance question answering capabilities, we use various\ntechniques including retrieving the most relevant snippets based on embedding\ndistance, in-context learning, and structured outputs. For certain submissions,\nwe utilize ensemble approaches to leverage the diverse outputs generated by\ndifferent models for exact-answer questions. Our results demonstrate that\nopen-weight LLMs are comparable to proprietary ones. In some instances,\nopen-weight LLMs even surpassed their closed counterparts, particularly when\nensembling strategies were applied. All code is publicly available at\nhttps://github.com/evidenceprime/BioASQ-13b.", "AI": {"tldr": "本研究在生物医学问答领域比较了开源大型语言模型（LLMs）与专有LLMs的性能，结果显示开源模型表现相当，甚至在采用集成策略时超越了专有模型。", "motivation": "随着DeepSeek-V3等开源LLMs的快速发展，其性能已可与专有LLMs媲美。这引发了一个问题：小型开源LLMs是否能在生物医学问答等特定领域有效替代大型闭源模型。", "method": "研究团队参与了BioASQ挑战的Task 13B Phase B。他们将多个开源模型与GPT-4o、GPT-4.1、Claude 3.5 Sonnet和Claude 3.7 Sonnet等顶尖专有系统进行了比较。为提升问答能力，采用了多种技术，包括基于嵌入距离检索最相关片段、情境学习和结构化输出。对于某些提交，还利用集成方法来整合不同模型对精确答案问题的输出。", "result": "研究结果表明，开源LLMs与专有LLMs表现相当。在某些情况下，尤其是在应用集成策略时，开源LLMs甚至超越了闭源模型。", "conclusion": "开源LLMs在生物医学问答领域能够有效与专有模型竞争，并且在结合先进技术（如集成策略）时，有时甚至能超越它们。"}}
{"id": "2509.18979", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18979", "abs": "https://arxiv.org/abs/2509.18979", "authors": ["Lorenzo Shaikewitz", "Tim Nguyen", "Luca Carlone"], "title": "Category-Level Object Shape and Pose Estimation in Less Than a Millisecond", "comment": null, "summary": "Object shape and pose estimation is a foundational robotics problem,\nsupporting tasks from manipulation to scene understanding and navigation. We\npresent a fast local solver for shape and pose estimation which requires only\ncategory-level object priors and admits an efficient certificate of global\noptimality. Given an RGB-D image of an object, we use a learned front-end to\ndetect sparse, category-level semantic keypoints on the target object. We\nrepresent the target object's unknown shape using a linear active shape model\nand pose a maximum a posteriori optimization problem to solve for position,\norientation, and shape simultaneously. Expressed in unit quaternions, this\nproblem admits first-order optimality conditions in the form of an eigenvalue\nproblem with eigenvector nonlinearities. Our primary contribution is to solve\nthis problem efficiently with self-consistent field iteration, which only\nrequires computing a 4-by-4 matrix and finding its minimum eigenvalue-vector\npair at each iterate. Solving a linear system for the corresponding Lagrange\nmultipliers gives a simple global optimality certificate. One iteration of our\nsolver runs in about 100 microseconds, enabling fast outlier rejection. We test\nour method on synthetic data and a variety of real-world settings, including\ntwo public datasets and a drone tracking scenario. Code is released at\nhttps://github.com/MIT-SPARK/Fast-ShapeAndPose.", "AI": {"tldr": "本文提出了一种快速的局部求解器，用于基于RGB-D图像和类别级先验知识进行物体形状和姿态估计。该方法利用语义关键点、线性主动形状模型和最大后验优化，通过自洽场迭代高效求解，并提供全局最优性证书。", "motivation": "物体形状和姿态估计是机器人领域的基础问题，支持操作、场景理解和导航等任务。因此，需要一个快速、高效且能利用类别级先验信息来解决此问题的方案。", "method": "该方法首先使用学习到的前端从RGB-D图像中检测目标物体上的稀疏、类别级语义关键点。物体未知形状通过线性主动形状模型表示。然后，构建一个最大后验（MAP）优化问题，以同时求解物体的位置、方向（使用单位四元数）和形状。其主要贡献在于使用自洽场迭代高效求解此问题，每次迭代只需计算一个4x4矩阵并找到其最小特征值-向量对。通过求解相应的拉格朗日乘子线性系统，可获得简单的全局最优性证书。", "result": "该求解器的一次迭代耗时约100微秒，支持快速离群点剔除。该方法在合成数据、两个公共数据集和无人机跟踪场景等多种真实世界环境中进行了测试，并取得了良好效果。相关代码已开源。", "conclusion": "本文提出了一种快速、高效且具备全局最优性证书的物体形状和姿态估计方法。该方法利用类别级先验信息和RGB-D数据，通过创新的自洽场迭代优化，为机器人领域的关键任务提供了强大的支持。"}}
{"id": "2509.18883", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18883", "abs": "https://arxiv.org/abs/2509.18883", "authors": ["Meituan LongCat Team", "Anchun Gui", "Bei Li", "Bingyang Tao", "Bole Zhou", "Borun Chen", "Chao Zhang", "Chao Zhang", "Chengcheng Han", "Chenhui Yang", "Chi Zhang", "Chong Peng", "Chuyu Zhang", "Cong Chen", "Fengcun Li", "Gang Xu", "Guoyuan Lin", "Hao Jiang", "Hao Liang", "Haomin Fu", "Haoxiang Ma", "Hong Liu", "Hongyan Hao", "Hongyin Tang", "Hongyu Zang", "Hongzhi Ni", "Hui Su", "Jiahao Liu", "Jiahuan Li", "Jialin Liu", "Jianfei Zhang", "Jianhao Xu", "Jianing Wang", "Jiaqi Sun", "Jiaqi Zhang", "Jiarong Shi", "Jiawei Yang", "Jingang Wang", "Jinrui Ding", "Jun Kuang", "Jun Xu", "Ke He", "Kefeng Zhang", "Keheng Wang", "Keqing He", "Li Wei", "Liang Shi", "Lin Qiu", "Lingbin Kong", "Lingchuan Liu", "Linsen Guo", "Longfei An", "Mai Xia", "Meng Zhou", "Mengshen Zhu", "Peng Pei", "Pengcheng Jia", "Qi Gu", "Qi Guo", "Qiong Huang", "Quan Chen", "Quanchi Weng", "Rongxiang Weng", "Ruichen Shao", "Rumei Li", "Shanglin Lei", "Shuai Du", "Shuaikang Liu", "Shuang Zhou", "Shuhao Hu", "Siyu Xu", "Songshan Gong", "Tao Liang", "Tianhao Hu", "Wei He", "Wei Shi", "Wei Wang", "Wei Wu", "Wei Zhuo", "Weifeng Tang", "Wenjie Shi", "Wenlong Zhu", "Xi Su", "Xiangcheng Liu", "Xiangyu Xi", "Xiangzhou Huang", "Xiao Liu", "Xiaochen Jiang", "Xiaowei Shi", "Xiaowen Shi", "Xiaoyu Li", "Xin Chen", "Xinyue Zhao", "Xuan Huang", "Xuemiao Zhang", "Xuezhi Cao", "Xunliang Cai", "Yajie Zhang", "Yang Chen", "Yang Liu", "Yang Liu", "Yang Zheng", "Yaoming Wang", "Yaqi Huo", "Yerui Sun", "Yifan Lu", "Yiyang Li", "Youshao Xiao", "Yuanzhe Lei", "Yuchen Xie", "Yueqing Sun", "Yufei Zhang", "Yuhuai Wei", "Yulei Qian", "Yunke Zhao", "Yuqing Ding", "Yuwei Jiang", "Zhaohua Yang", "Zhengyu Chen", "Zhijian Liu", "Zhikang Xia", "Zhongda Su", "Ziran Li", "Ziwen Wang", "Ziyuan Zhuang", "Zongyu Wang", "Zunyuan Yang"], "title": "LongCat-Flash-Thinking Technical Report", "comment": null, "summary": "We present LongCat-Flash-Thinking, an efficient 560-billion-parameter\nopen-source Mixture-of-Experts (MoE) reasoning model. Its advanced capabilities\nare cultivated through a meticulously crafted training process, beginning with\nlong Chain-of-Thought (CoT) data cold-start and culminating in large-scale\nReinforcement Learning (RL). We first employ a well-designed cold-start\ntraining strategy, which significantly enhances the reasoning potential and\nequips the model with specialized skills in both formal and agentic reasoning.\nThen, a core innovation is our domain-parallel training scheme, which decouples\noptimization across distinct domains (e.g., STEM, Code, Agentic) and\nsubsequently fuses the resulting expert models into a single, nearly\nPareto-optimal model. This entire process is powered by our Dynamic\nORchestration for Asynchronous rollout (DORA) system, a large-scale RL\nframework that delivers a greater than threefold training speedup over\nsynchronous methods on tens of thousands of accelerators. As a result,\nLongCat-Flash-Thinking achieves state-of-the-art performance among open-source\nmodels on a suite of complex reasoning tasks. The model exhibits exceptional\nefficiency in agentic reasoning, reducing average token consumption by 64.5%\n(from 19, 653 to 6, 965) on AIME-25, without degrading task accuracy. We\nrelease LongCat-Flash-Thinking to promote further advances in reasoning systems\nand agentic AI research.", "AI": {"tldr": "LongCat-Flash-Thinking是一个560亿参数的开源MoE推理模型，通过CoT冷启动和大规模RL训练，实现了最先进的推理性能和卓越的智能体推理效率。", "motivation": "开发一个高效、强大的开源推理模型，尤其是在复杂推理和智能体推理任务中表现出色，以推动推理系统和智能体AI研究的进展。", "method": "该模型采用560亿参数的专家混合（MoE）架构。训练过程包括：1) 使用长链式思考（CoT）数据进行冷启动训练，以增强推理潜力和在形式化和智能体推理方面的专业技能；2) 核心创新是域并行训练方案，将不同领域（如STEM、代码、智能体）的优化解耦，然后将生成的专家模型融合；3) 整个过程由DORA（Dynamic ORchestration for Asynchronous rollout）系统驱动，这是一个大规模RL框架，在数万个加速器上提供超过同步方法三倍的训练速度。", "result": "LongCat-Flash-Thinking在复杂推理任务套件中，在开源模型中取得了最先进的性能。在智能体推理方面表现出卓越的效率，在AIME-25上将平均token消耗减少了64.5%（从19,653减少到6,965），且未降低任务准确性。", "conclusion": "LongCat-Flash-Thinking是一个高效且性能卓越的开源MoE推理模型，特别擅长智能体推理。该模型的发布旨在促进推理系统和智能体AI研究的进一步发展。"}}
{"id": "2509.18550", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18550", "abs": "https://arxiv.org/abs/2509.18550", "authors": ["Mohammad Junayed Hasan", "Nabeel Mohammed", "Shafin Rahman", "Philipp Koehn"], "title": "HadaSmileNet: Hadamard fusion of handcrafted and deep-learning features for enhancing facial emotion recognition of genuine smiles", "comment": "Accepted to IEEE International Conference on Data Mining (ICDM) 2025.\n  Final version to appear in the conference proceedings", "summary": "The distinction between genuine and posed emotions represents a fundamental\npattern recognition challenge with significant implications for data mining\napplications in social sciences, healthcare, and human-computer interaction.\nWhile recent multi-task learning frameworks have shown promise in combining\ndeep learning architectures with handcrafted D-Marker features for smile facial\nemotion recognition, these approaches exhibit computational inefficiencies due\nto auxiliary task supervision and complex loss balancing requirements. This\npaper introduces HadaSmileNet, a novel feature fusion framework that directly\nintegrates transformer-based representations with physiologically grounded\nD-Markers through parameter-free multiplicative interactions. Through\nsystematic evaluation of 15 fusion strategies, we demonstrate that Hadamard\nmultiplicative fusion achieves optimal performance by enabling direct feature\ninteractions while maintaining computational efficiency. The proposed approach\nestablishes new state-of-the-art results for deep learning methods across four\nbenchmark datasets: UvA-NEMO (88.7 percent, +0.8), MMI (99.7 percent), SPOS\n(98.5 percent, +0.7), and BBC (100 percent, +5.0). Comprehensive computational\nanalysis reveals 26 percent parameter reduction and simplified training\ncompared to multi-task alternatives, while feature visualization demonstrates\nenhanced discriminative power through direct domain knowledge integration. The\nframework's efficiency and effectiveness make it particularly suitable for\npractical deployment in multimedia data mining applications that require\nreal-time affective computing capabilities.", "AI": {"tldr": "HadaSmileNet通过Hadamard乘法融合Transformer特征和D-Markers，实现了高效且最先进的真假笑容情感识别，解决了现有方法计算效率低下的问题。", "motivation": "区分真假情感是模式识别的挑战，对社会科学、医疗保健和人机交互中的数据挖掘应用具有重要意义。尽管现有的多任务学习框架结合深度学习和D-Marker特征在笑容情感识别方面有前景，但它们因辅助任务监督和复杂的损失平衡而导致计算效率低下。", "method": "本文提出了HadaSmileNet，一个新颖的特征融合框架。它通过无参数的乘法交互（特别是Hadamard乘法融合）直接整合基于Transformer的表示和生理学基础的D-Marker特征。该研究系统评估了15种融合策略。", "result": "Hadamard乘法融合实现了最佳性能，通过直接特征交互保持了计算效率。该方法在四个基准数据集上取得了深度学习方法的新SOTA结果：UvA-NEMO (88.7%, +0.8)、MMI (99.7%)、SPOS (98.5%, +0.7) 和 BBC (100%, +5.0)。与多任务替代方案相比，参数减少了26%，训练过程更简化。特征可视化显示通过直接领域知识整合增强了判别力。", "conclusion": "HadaSmileNet框架的效率和有效性使其特别适用于需要实时情感计算能力的多媒体数据挖掘应用中的实际部署。"}}
{"id": "2509.18862", "categories": ["cs.CL", "I.2.7; I.2.1"], "pdf": "https://arxiv.org/pdf/2509.18862", "abs": "https://arxiv.org/abs/2509.18862", "authors": ["Luyan Zhang", "Xinyu Xie"], "title": "Multi-Hierarchical Feature Detection for Large Language Model Generated Text", "comment": "9 pages, 6 tables, empirical study on multi-feature AI text detection", "summary": "With the rapid advancement of large language model technology, there is\ngrowing interest in whether multi-feature approaches can significantly improve\nAI text detection beyond what single neural models achieve. While intuition\nsuggests that combining semantic, syntactic, and statistical features should\nprovide complementary signals, this assumption has not been rigorously tested\nwith modern LLM-generated text. This paper provides a systematic empirical\ninvestigation of multi-hierarchical feature integration for AI text detection,\nspecifically testing whether the computational overhead of combining multiple\nfeature types is justified by performance gains. We implement MHFD\n(Multi-Hierarchical Feature Detection), integrating DeBERTa-based semantic\nanalysis, syntactic parsing, and statistical probability features through\nadaptive fusion. Our investigation reveals important negative results: despite\ntheoretical expectations, multi-feature integration provides minimal benefits\n(0.4-0.5% improvement) while incurring substantial computational costs (4.2x\noverhead), suggesting that modern neural language models may already capture\nmost relevant detection signals efficiently. Experimental results on multiple\nbenchmark datasets demonstrate that the MHFD method achieves 89.7% accuracy in\nin-domain detection and maintains 84.2% stable performance in cross-domain\ndetection, showing modest improvements of 0.4-2.6% over existing methods.", "AI": {"tldr": "本研究系统性地考察了多层次特征集成在AI文本检测中的效果，发现尽管有理论预期，但多特征集成带来的性能提升微乎其微（0.4-0.5%），而计算成本显著增加（4.2倍），表明单一神经模型可能已高效捕获了大部分相关检测信号。", "motivation": "随着大型语言模型技术快速发展，人们对多特征方法是否能显著提升AI文本检测能力产生了兴趣，超越单一神经模型的表现。然而，对于结合语义、句法和统计特征是否能提供互补信号这一假设，尚未对现代LLM生成的文本进行严格测试。", "method": "研究实现了MHFD（多层次特征检测）方法，通过自适应融合集成了基于DeBERTa的语义分析、句法分析和统计概率特征。", "result": "研究揭示了重要的负面结果：尽管有理论预期，多特征集成带来的性能提升微乎其微（0.4-0.5%），却带来了巨大的计算开销（4.2倍）。MHFD方法在域内检测中实现了89.7%的准确率，在跨域检测中保持了84.2%的稳定性能，比现有方法取得了0.4-2.6%的适度改进。", "conclusion": "多特征集成在AI文本检测中的计算开销并未被其微小的性能提升所证明，这表明现代神经语言模型可能已经有效地捕获了大部分相关的检测信号。"}}
{"id": "2509.19012", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19012", "abs": "https://arxiv.org/abs/2509.19012", "authors": ["Dapeng Zhang", "Jin Sun", "Chenghui Hu", "Xiaoyan Wu", "Zhenlong Yuan", "Rui Zhou", "Fei Shen", "Qingguo Zhou"], "title": "Pure Vision Language Action (VLA) Models: A Comprehensive Survey", "comment": null, "summary": "The emergence of Vision Language Action (VLA) models marks a paradigm shift\nfrom traditional policy-based control to generalized robotics, reframing Vision\nLanguage Models (VLMs) from passive sequence generators into active agents for\nmanipulation and decision-making in complex, dynamic environments. This survey\ndelves into advanced VLA methods, aiming to provide a clear taxonomy and a\nsystematic, comprehensive review of existing research. It presents a\ncomprehensive analysis of VLA applications across different scenarios and\nclassifies VLA approaches into several paradigms: autoregression-based,\ndiffusion-based, reinforcement-based, hybrid, and specialized methods; while\nexamining their motivations, core strategies, and implementations in detail. In\naddition, foundational datasets, benchmarks, and simulation platforms are\nintroduced. Building on the current VLA landscape, the review further proposes\nperspectives on key challenges and future directions to advance research in VLA\nmodels and generalizable robotics. By synthesizing insights from over three\nhundred recent studies, this survey maps the contours of this rapidly evolving\nfield and highlights the opportunities and challenges that will shape the\ndevelopment of scalable, general-purpose VLA methods.", "AI": {"tldr": "本综述全面回顾了视觉-语言-动作 (VLA) 模型，提供了清晰的分类法、方法分析、应用场景、基础资源，并提出了未来挑战和方向。", "motivation": "VLA模型的出现标志着机器人控制从传统策略到通用机器人的范式转变，将VLM从被动序列生成器转变为主动智能体。因此，需要一个清晰的分类法和系统全面的综述来理解和推动这一快速发展的领域。", "method": "本研究通过对三百多项近期研究的综合分析，对VLA方法进行了分类（自回归、扩散、强化、混合和专业化范式），详细考察了它们的动机、核心策略和实现。此外，还分析了VLA在不同场景的应用，并介绍了基础数据集、基准和仿真平台。", "result": "本综述提供了VLA方法的清晰分类和系统全面的回顾，分析了其在不同场景中的应用，并详细阐述了各类方法的动机、核心策略和实现。同时，介绍了该领域的基础数据集、基准和仿真平台，并综合了大量最新研究的见解。", "conclusion": "本综述在总结当前VLA发展现状的基础上，提出了VLA模型和通用机器人领域面临的关键挑战和未来的研究方向，为可扩展、通用VLA方法的发展指明了道路。"}}
{"id": "2509.18905", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18905", "abs": "https://arxiv.org/abs/2509.18905", "authors": ["Songsong Yu", "Yuxin Chen", "Hao Ju", "Lianjie Jia", "Fuxi Zhang", "Shaofei Huang", "Yuhan Wu", "Rundi Cui", "Binghao Ran", "Zaibin Zhang", "Zhedong Zheng", "Zhipeng Zhang", "Yifan Wang", "Lin Song", "Lijun Wang", "Yanwei Li", "Ying Shan", "Huchuan Lu"], "title": "How Far are VLMs from Visual Spatial Intelligence? A Benchmark-Driven Perspective", "comment": "a comprehensive visual spatial reasoning evaluation tool, 25 pages,\n  16 figures", "summary": "Visual Spatial Reasoning (VSR) is a core human cognitive ability and a\ncritical requirement for advancing embodied intelligence and autonomous\nsystems. Despite recent progress in Vision-Language Models (VLMs), achieving\nhuman-level VSR remains highly challenging due to the complexity of\nrepresenting and reasoning over three-dimensional space. In this paper, we\npresent a systematic investigation of VSR in VLMs, encompassing a review of\nexisting methodologies across input modalities, model architectures, training\nstrategies, and reasoning mechanisms. Furthermore, we categorize spatial\nintelligence into three levels of capability, ie, basic perception, spatial\nunderstanding, spatial planning, and curate SIBench, a spatial intelligence\nbenchmark encompassing nearly 20 open-source datasets across 23 task settings.\nExperiments with state-of-the-art VLMs reveal a pronounced gap between\nperception and reasoning, as models show competence in basic perceptual tasks\nbut consistently underperform in understanding and planning tasks, particularly\nin numerical estimation, multi-view reasoning, temporal dynamics, and spatial\nimagination. These findings underscore the substantial challenges that remain\nin achieving spatial intelligence, while providing both a systematic roadmap\nand a comprehensive benchmark to drive future research in the field. The\nrelated resources of this study are accessible at\nhttps://sibench.github.io/Awesome-Visual-Spatial-Reasoning/.", "AI": {"tldr": "本文系统性地调查了视觉空间推理（VSR）在视觉语言模型（VLMs）中的表现，回顾了现有方法，将空间智能分为三个能力等级，并构建了包含20个数据集的SIBench基准。实验揭示了当前VLMs在感知和推理之间存在显著差距，尤其在理解和规划任务中表现不佳。", "motivation": "视觉空间推理（VSR）是人类的核心认知能力，也是推动具身智能和自主系统发展的关键。然而，由于三维空间表示和推理的复杂性，尽管视觉语言模型（VLMs）取得了进展，实现人类水平的VSR仍然极具挑战。", "method": "本文首先对VLMs中的VSR进行了系统性调查，涵盖了输入模态、模型架构、训练策略和推理机制等现有方法。其次，将空间智能划分为基本感知、空间理解和空间规划三个能力等级。最后，构建了SIBench空间智能基准，该基准包含近20个开源数据集和23种任务设置，并使用最先进的VLMs进行了实验。", "result": "实验结果表明，VLMs在基本感知任务上表现出能力，但在理解和规划任务中持续表现不佳，尤其是在数值估计、多视角推理、时间动态和空间想象方面。这揭示了模型在感知和推理之间存在显著差距。", "conclusion": "研究结果强调了在实现空间智能方面仍然存在巨大挑战。本文为该领域的未来研究提供了一个系统的路线图和全面的基准，以推动视觉空间推理技术的发展。"}}
{"id": "2509.18571", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18571", "abs": "https://arxiv.org/abs/2509.18571", "authors": ["Yuhan Wang", "Cheng Liu", "Zihan Zhao", "Weichao Wu"], "title": "Live-E2T: Real-time Threat Monitoring in Video via Deduplicated Event Reasoning and Chain-of-Thought", "comment": null, "summary": "Real-time threat monitoring identifies threatening behaviors in video streams\nand provides reasoning and assessment of threat events through explanatory\ntext. However, prevailing methodologies, whether based on supervised learning\nor generative models, struggle to concurrently satisfy the demanding\nrequirements of real-time performance and decision explainability. To bridge\nthis gap, we introduce Live-E2T, a novel framework that unifies these two\nobjectives through three synergistic mechanisms. First, we deconstruct video\nframes into structured Human-Object-Interaction-Place semantic tuples. This\napproach creates a compact, semantically focused representation, circumventing\nthe information degradation common in conventional feature compression. Second,\nan efficient online event deduplication and updating mechanism is proposed to\nfilter spatio-temporal redundancies, ensuring the system's real time\nresponsiveness. Finally, we fine-tune a Large Language Model using a\nChain-of-Thought strategy, endow it with the capability for transparent and\nlogical reasoning over event sequences to produce coherent threat assessment\nreports. Extensive experiments on benchmark datasets, including XD-Violence and\nUCF-Crime, demonstrate that Live-E2T significantly outperforms state-of-the-art\nmethods in terms of threat detection accuracy, real-time efficiency, and the\ncrucial dimension of explainability.", "AI": {"tldr": "Live-E2T是一个新颖的框架，通过结构化语义表示、高效事件去重和基于CoT的LLM推理，实现了实时视频威胁监控的准确性、实时性和可解释性。", "motivation": "现有的视频威胁监控方法（无论是监督学习还是生成模型）难以同时满足实时性能和决策可解释性的高要求。", "method": "Live-E2T框架通过三个协同机制实现目标：1) 将视频帧解构为结构化的“人-物-交互-地点”语义元组，创建紧凑且语义集中的表示。2) 提出高效的在线事件去重和更新机制，过滤时空冗余以确保实时响应。3) 使用思维链（Chain-of-Thought）策略微调大型语言模型（LLM），使其能够对事件序列进行透明且逻辑的推理，生成连贯的威胁评估报告。", "result": "在XD-Violence和UCF-Crime等基准数据集上的广泛实验表明，Live-E2T在威胁检测准确性、实时效率和关键的可解释性方面显著优于现有最先进的方法。", "conclusion": "Live-E2T成功地统一了实时性能和决策可解释性这两个目标，为视频威胁监控提供了一个高效且透明的解决方案。"}}
{"id": "2509.18880", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18880", "abs": "https://arxiv.org/abs/2509.18880", "authors": ["Advik Raj Basani", "Pin-Yu Chen"], "title": "Diversity Boosts AI-Generated Text Detection", "comment": "Project Webpage: https://diveye.vercel.app/", "summary": "Detecting AI-generated text is an increasing necessity to combat misuse of\nLLMs in education, business compliance, journalism, and social media, where\nsynthetic fluency can mask misinformation or deception. While prior detectors\noften rely on token-level likelihoods or opaque black-box classifiers, these\napproaches struggle against high-quality generations and offer little\ninterpretability. In this work, we propose DivEye, a novel detection framework\nthat captures how unpredictability fluctuates across a text using\nsurprisal-based features. Motivated by the observation that human-authored text\nexhibits richer variability in lexical and structural unpredictability than LLM\noutputs, DivEye captures this signal through a set of interpretable statistical\nfeatures. Our method outperforms existing zero-shot detectors by up to 33.2%\nand achieves competitive performance with fine-tuned baselines across multiple\nbenchmarks. DivEye is robust to paraphrasing and adversarial attacks,\ngeneralizes well across domains and models, and improves the performance of\nexisting detectors by up to 18.7% when used as an auxiliary signal. Beyond\ndetection, DivEye provides interpretable insights into why a text is flagged,\npointing to rhythmic unpredictability as a powerful and underexplored signal\nfor LLM detection.", "AI": {"tldr": "本文提出DivEye，一个新颖的AI生成文本检测框架，它利用文本中不可预测性的波动（基于Surprisal特征）来区分人类和LLM生成的内容，并提供可解释的检测结果。", "motivation": "检测AI生成文本对于打击LLM在教育、商业、新闻和社交媒体中的滥用至关重要，因为合成文本的流畅性可能掩盖虚假信息或欺骗。现有的检测器（如基于token似然或黑盒分类器）在面对高质量生成文本时表现不佳，且缺乏可解释性。", "method": "DivEye框架通过使用基于Surprisal的特征来捕捉文本中不可预测性如何波动。其灵感来源于人类创作的文本在词汇和结构不可预测性方面比LLM输出展现出更丰富的变异性。DivEye通过一组可解释的统计特征来捕捉这一信号。", "result": "DivEye的性能优于现有零样本检测器高达33.2%，并在多个基准测试中与微调基线方法表现相当。它对释义和对抗性攻击具有鲁棒性，在不同领域和模型之间泛化良好，并且在作为辅助信号时，能将现有检测器的性能提高高达18.7%。", "conclusion": "DivEye不仅能有效检测AI生成文本，还提供了可解释的洞察，揭示了“节奏性不可预测性”是LLM检测中一个强大且未被充分探索的信号。"}}
{"id": "2509.19023", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19023", "abs": "https://arxiv.org/abs/2509.19023", "authors": ["Shuai Liu", "Meng Cheng Lau"], "title": "Reduced-Order Model-Guided Reinforcement Learning for Demonstration-Free Humanoid Locomotion", "comment": "11 pages, 5 figures, 1 table, Computational Science Graduate Project", "summary": "We introduce Reduced-Order Model-Guided Reinforcement Learning (ROM-GRL), a\ntwo-stage reinforcement learning framework for humanoid walking that requires\nno motion capture data or elaborate reward shaping. In the first stage, a\ncompact 4-DOF (four-degree-of-freedom) reduced-order model (ROM) is trained via\nProximal Policy Optimization. This generates energy-efficient gait templates.\nIn the second stage, those dynamically consistent trajectories guide a\nfull-body policy trained with Soft Actor--Critic augmented by an adversarial\ndiscriminator, ensuring the student's five-dimensional gait feature\ndistribution matches the ROM's demonstrations. Experiments at 1\nmeter-per-second and 4 meter-per-second show that ROM-GRL produces stable,\nsymmetric gaits with substantially lower tracking error than a pure-reward\nbaseline. By distilling lightweight ROM guidance into high-dimensional\npolicies, ROM-GRL bridges the gap between reward-only and imitation-based\nlocomotion methods, enabling versatile, naturalistic humanoid behaviors without\nany human demonstrations.", "AI": {"tldr": "本文提出了ROM-GRL，一个两阶段强化学习框架，用于人形机器人行走。它无需动作捕捉数据或复杂奖励塑形，通过简化模型生成节能步态模板，并用其指导全身策略训练，实现了稳定、对称且低跟踪误差的步态。", "motivation": "现有的人形机器人行走强化学习方法通常依赖于动作捕捉数据或复杂的奖励塑形。研究的动机是开发一种无需人类演示，即可生成通用、自然的人形行为的方法。", "method": "ROM-GRL是一个两阶段框架：\n1. **第一阶段**：通过近端策略优化（PPO）训练一个紧凑的4自由度简化模型（ROM），以生成节能的步态模板。\n2. **第二阶段**：利用这些动态一致的轨迹指导一个全身策略。该策略通过软执行者-评论家（SAC）训练，并辅以一个对抗性判别器，以确保学生策略的五维步态特征分布与ROM的演示相匹配。", "result": "实验结果表明，在1米/秒和4米/秒的速度下，ROM-GRL能够产生稳定、对称的步态，并且跟踪误差显著低于纯奖励基线方法。", "conclusion": "ROM-GRL通过将轻量级ROM的指导提炼到高维策略中，弥合了纯奖励和基于模仿的运动方法之间的差距，从而实现了无需任何人类演示的通用、自然的人形行为。"}}
{"id": "2509.18942", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18942", "abs": "https://arxiv.org/abs/2509.18942", "authors": ["Xiao Han", "Zimo Zhao", "Wanyu Wang", "Maolin Wang", "Zitao Liu", "Yi Chang", "Xiangyu Zhao"], "title": "Data Efficient Adaptation in Large Language Models via Continuous Low-Rank Fine-Tuning", "comment": null, "summary": "Recent advancements in Large Language Models (LLMs) have emphasized the\ncritical role of fine-tuning (FT) techniques in adapting LLMs to specific\ntasks, especially when retraining from scratch is computationally infeasible.\nFine-tuning enables LLMs to leverage task- or domain-specific data, producing\nmodels that more effectively meet the requirements of targeted applications.\nHowever, con- ventional FT approaches often suffer from catastrophic forgetting\nand suboptimal data efficiency, limiting their real-world applicability. To\naddress these challenges, this paper proposes DEAL, a novel framework that\nintegrates Low-Rank Adapta- tion (LoRA) with a continuous fine-tuning strategy.\nBy incorporating knowledge retention and adaptive parameter update modules, the\nframework mitigates the lim- itations of existing FT methods while maintaining\nefficiency in privacy-preserving settings. Experiments on 15 diverse datasets\nshow that DEAL consistently outper- forms baseline methods, yielding\nsubstantial gains in task accuracy and resource efficiency. These findings\ndemonstrate the potential of our approach to advance continual adaptation in\nLLMs by enhancing task performance while improving resource efficiency.", "AI": {"tldr": "本文提出DEAL框架，结合LoRA和持续微调策略，通过知识保留和自适应参数更新模块，有效解决了传统LLM微调中灾难性遗忘和数据效率低下的问题，显著提升了任务准确性和资源效率。", "motivation": "传统LLM微调方法存在灾难性遗忘和数据效率低下等问题，限制了其在实际应用中的效果，尤其是在从头开始训练计算成本过高时，需要更有效的微调技术来适应特定任务。", "method": "DEAL框架整合了低秩自适应（LoRA）与持续微调策略。它通过引入知识保留模块和自适应参数更新模块，来缓解现有微调方法的局限性，并保持隐私保护设置下的效率。", "result": "在15个不同数据集上的实验表明，DEAL始终优于基线方法，在任务准确性和资源效率方面取得了显著提升。", "conclusion": "DEAL方法通过提高任务性能和资源效率，展示了其在LLM持续适应方面的巨大潜力，为LLM的持续适应提供了有效途径。"}}
{"id": "2509.18582", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18582", "abs": "https://arxiv.org/abs/2509.18582", "authors": ["Daiqing Qi", "Handong Zhao", "Jing Shi", "Simon Jenni", "Yifei Fan", "Franck Dernoncourt", "Scott Cohen", "Sheng Li"], "title": "The Photographer Eye: Teaching Multimodal Large Language Models to See and Critique like Photographers", "comment": null, "summary": "While editing directly from life, photographers have found it too difficult\nto see simultaneously both the blue and the sky. Photographer and curator,\nSzarkowski insightfully revealed one of the notable gaps between general and\naesthetic visual understanding: while the former focuses on identifying the\nfactual element in an image (sky), the latter transcends such object\nidentification, viewing it instead as an aesthetic component--a pure color\nblock (blue). Such fundamental distinctions between general (detection,\nlocalization, etc.) and aesthetic (color, lighting, composition, etc.) visual\nunderstanding present a significant challenge for Multimodal Large Language\nModels (MLLMs). Although some recent works have made initial explorations, they\nare often limited to general and basic aesthetic commonsense. As a result, they\nfrequently fall short in real-world scenarios (Fig. 1), which require extensive\nexpertise--including photographic techniques, photo pre/post-processing\nknowledge, and more, to provide a detailed analysis and description. To\nfundamentally enhance the aesthetics understanding of MLLMs, we first introduce\na novel dataset, PhotoCritique, derived from extensive discussions among\nprofessional photographers and enthusiasts, and characterized by the large\nscale, expertise, and diversity. Then, to better learn visual aesthetics from\nPhotoCritique, we furthur propose a novel model, PhotoEye, featuring a\nlanguageguided multi-view vision fusion mechanism to understand image\naesthetics from multiple perspectives. Finally, we present a novel benchmark,\nPhotoBench, a comprehensive and professional benchmark for aesthetic visual\nunderstanding. On existing benchmarks and PhotoBench, our model demonstrates\nclear advantages over existing models.", "AI": {"tldr": "本文旨在通过引入一个专业数据集PhotoCritique、一个多视角融合模型PhotoEye和一个综合基准PhotoBench，显著提升多模态大语言模型（MLLMs）的视觉美学理解能力，以弥补其在区分图像事实元素与美学组件方面的不足。", "motivation": "多模态大语言模型（MLLMs）在理解视觉美学方面存在显著挑战，它们常常将图像中的事实元素（如“天空”）与美学组件（如“蓝色”这一纯色块）混淆。现有工作仅限于一般和基本的审美常识，缺乏摄影技术、图像处理等专业知识，无法提供详细的审美分析，这促使研究者寻求根本性地增强MLLMs的美学理解能力。", "method": "研究者首先引入了一个名为PhotoCritique的新型数据集，该数据集来源于专业摄影师和爱好者的大量讨论，具有大规模、专业性和多样性。其次，为了更好地从PhotoCritique中学习视觉美学，提出了一种名为PhotoEye的新型模型，该模型具有语言引导的多视角视觉融合机制，以从多个角度理解图像美学。最后，提出了一个名为PhotoBench的综合性专业美学视觉理解基准。", "result": "在现有基准和新提出的PhotoBench上，本文提出的模型（PhotoEye）相较于现有模型展现出明显的优势。", "conclusion": "通过引入PhotoCritique数据集、PhotoEye模型和PhotoBench基准，本研究为多模态大语言模型在美学视觉理解方面提供了根本性的增强，使其能够更好地区分图像的事实元素与美学组件，并进行专业的审美分析。"}}
{"id": "2509.18901", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18901", "abs": "https://arxiv.org/abs/2509.18901", "authors": ["Nicholas Popovič", "Michael Färber"], "title": "Extractive Fact Decomposition for Interpretable Natural Language Inference in one Forward Pass", "comment": "EMNLP 2025", "summary": "Recent works in Natural Language Inference (NLI) and related tasks, such as\nautomated fact-checking, employ atomic fact decomposition to enhance\ninterpretability and robustness. For this, existing methods rely on\nresource-intensive generative large language models (LLMs) to perform\ndecomposition. We propose JEDI, an encoder-only architecture that jointly\nperforms extractive atomic fact decomposition and interpretable inference\nwithout requiring generative models during inference. To facilitate training,\nwe produce a large corpus of synthetic rationales covering multiple NLI\nbenchmarks. Experimental results demonstrate that JEDI achieves competitive\naccuracy in distribution and significantly improves robustness out of\ndistribution and in adversarial settings over models based solely on extractive\nrationale supervision. Our findings show that interpretability and robust\ngeneralization in NLI can be realized using encoder-only architectures and\nsynthetic rationales. Code and data available at https://jedi.nicpopovic.com", "AI": {"tldr": "JEDI是一种仅编码器架构，用于自然语言推理（NLI），它联合执行抽取式原子事实分解和可解释推理，无需生成模型。通过合成理由进行训练，JEDI在分布内精度具有竞争力，并显著提高了分布外和对抗性设置下的鲁棒性。", "motivation": "当前自然语言推理（NLI）和事实核查中使用的原子事实分解方法依赖于资源密集型生成式大型语言模型（LLMs），这增加了推理成本和复杂性。研究旨在寻找一种更高效、仅编码器的替代方案，同时保持可解释性和鲁棒性。", "method": "本文提出了JEDI，一个仅编码器架构，它在推理过程中不依赖生成模型，联合执行抽取式原子事实分解和可解释推理。为便于训练，作者生成了一个包含多个NLI基准的大规模合成理由语料库。", "result": "实验结果表明，JEDI在分布内实现了具有竞争力的准确性，并且在分布外和对抗性设置下，相对于仅基于抽取式理由监督的模型，显著提高了鲁棒性。", "conclusion": "研究发现，NLI中的可解释性和鲁棒泛化可以通过仅编码器架构和合成理由来实现。"}}
{"id": "2509.19037", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.19037", "abs": "https://arxiv.org/abs/2509.19037", "authors": ["Qingzheng Cong", "Steven Oh", "Wen Fan", "Shan Luo", "Kaspar Althoefer", "Dandan Zhang"], "title": "TacEva: A Performance Evaluation Framework For Vision-Based Tactile Sensors", "comment": "14 pages, 8 figures. Equal contribution: Qingzheng Cong, Steven Oh,\n  Wen Fan. Corresponding author: Dandan Zhang (d.zhang17@imperial.ac.uk).\n  Additional resources at http://stevenoh2003.github.io/TacEva/", "summary": "Vision-Based Tactile Sensors (VBTSs) are widely used in robotic tasks because\nof the high spatial resolution they offer and their relatively low\nmanufacturing costs. However, variations in their sensing mechanisms,\nstructural dimension, and other parameters lead to significant performance\ndisparities between existing VBTSs. This makes it challenging to optimize them\nfor specific tasks, as both the initial choice and subsequent fine-tuning are\nhindered by the lack of standardized metrics. To address this issue, TacEva is\nintroduced as a comprehensive evaluation framework for the quantitative\nanalysis of VBTS performance. The framework defines a set of performance\nmetrics that capture key characteristics in typical application scenarios. For\neach metric, a structured experimental pipeline is designed to ensure\nconsistent and repeatable quantification. The framework is applied to multiple\nVBTSs with distinct sensing mechanisms, and the results demonstrate its ability\nto provide a thorough evaluation of each design and quantitative indicators for\neach performance dimension. This enables researchers to pre-select the most\nappropriate VBTS on a task by task basis, while also offering\nperformance-guided insights into the optimization of VBTS design. A list of\nexisting VBTS evaluation methods and additional evaluations can be found on our\nwebsite: https://stevenoh2003.github.io/TacEva/", "AI": {"tldr": "由于缺乏标准化指标，视觉触觉传感器（VBTS）的性能评估和优化面临挑战。本文引入TacEva框架，通过定义性能指标和实验流程，实现VBTS的定量分析，辅助其选择和设计优化。", "motivation": "视觉触觉传感器（VBTS）因其传感机制、结构尺寸等参数的差异，导致性能表现不一。现有VBTS缺乏标准化的评估指标，使得针对特定任务进行选择和优化变得困难。", "method": "引入TacEva，一个全面的VBTS性能定量分析评估框架。该框架定义了一套捕获典型应用场景关键特征的性能指标，并为每个指标设计了结构化的实验流程，以确保一致和可重复的量化。", "result": "该框架已应用于多种具有不同传感机制的VBTS，结果表明其能够对每种设计进行彻底评估，并为每个性能维度提供定量指标。", "conclusion": "TacEva框架使研究人员能够根据任务需求预选最合适的VBTS，并为VBTS的设计优化提供性能导向的见解。"}}
{"id": "2509.18970", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18970", "abs": "https://arxiv.org/abs/2509.18970", "authors": ["Xixun Lin", "Yucheng Ning", "Jingwen Zhang", "Yan Dong", "Yilong Liu", "Yongxuan Wu", "Xiaohua Qi", "Nan Sun", "Yanmin Shang", "Pengfei Cao", "Lixin Zou", "Xu Chen", "Chuan Zhou", "Jia Wu", "Shirui Pan", "Bin Wang", "Yanan Cao", "Kai Chen", "Songlin Hu", "Li Guo"], "title": "LLM-based Agents Suffer from Hallucinations: A Survey of Taxonomy, Methods, and Directions", "comment": null, "summary": "Driven by the rapid advancements of Large Language Models (LLMs), LLM-based\nagents have emerged as powerful intelligent systems capable of human-like\ncognition, reasoning, and interaction. These agents are increasingly being\ndeployed across diverse real-world applications, including student education,\nscientific research, and financial analysis. However, despite their remarkable\npotential, LLM-based agents remain vulnerable to hallucination issues, which\ncan result in erroneous task execution and undermine the reliability of the\noverall system design. Addressing this critical challenge requires a deep\nunderstanding and a systematic consolidation of recent advances on LLM-based\nagents. To this end, we present the first comprehensive survey of\nhallucinations in LLM-based agents. By carefully analyzing the complete\nworkflow of agents, we propose a new taxonomy that identifies different types\nof agent hallucinations occurring at different stages. Furthermore, we conduct\nan in-depth examination of eighteen triggering causes underlying the emergence\nof agent hallucinations. Through a detailed review of a large number of\nexisting studies, we summarize approaches for hallucination mitigation and\ndetection, and highlight promising directions for future research. We hope this\nsurvey will inspire further efforts toward addressing hallucinations in\nLLM-based agents, ultimately contributing to the development of more robust and\nreliable agent systems.", "AI": {"tldr": "这篇论文首次全面综述了大型语言模型（LLM）驱动的智能体中出现的幻觉问题，提出了新的分类法、分析了触发原因，并总结了缓解和检测方法。", "motivation": "LLM驱动的智能体在各种实际应用中展现出巨大潜力，但其幻觉问题可能导致任务执行错误，并损害系统可靠性。因此，需要深入理解并系统性地整合LLM智能体幻觉的最新进展。", "method": "通过分析智能体的完整工作流程，提出了一种新的智能体幻觉分类法，识别了不同阶段的幻觉类型。深入研究了导致智能体幻觉出现的18个触发原因。详细回顾了大量现有研究，总结了幻觉缓解和检测方法。", "result": "该论文提供了一份关于LLM驱动智能体中幻觉问题的首次全面综述。提出了一个识别不同阶段幻觉的新分类法，并详细分析了18个触发幻觉的原因。同时，总结了现有的幻觉缓解和检测方法，并指出了未来的研究方向。", "conclusion": "这份综述旨在启发更多研究，以解决LLM驱动智能体中的幻觉问题，最终促进开发更鲁棒和可靠的智能体系统。"}}
{"id": "2509.18591", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18591", "abs": "https://arxiv.org/abs/2509.18591", "authors": ["Pengchao Deng", "Shengqi Chen"], "title": "Enhancing Video Object Segmentation in TrackRAD Using XMem Memory Network", "comment": null, "summary": "This paper presents an advanced tumor segmentation framework for real-time\nMRI-guided radiotherapy, designed for the TrackRAD2025 challenge. Our method\nleverages the XMem model, a memory-augmented architecture, to segment tumors\nacross long cine-MRI sequences. The proposed system efficiently integrates\nmemory mechanisms to track tumor motion in real-time, achieving high\nsegmentation accuracy even under challenging conditions with limited annotated\ndata. Unfortunately, the detailed experimental records have been lost,\npreventing us from reporting precise quantitative results at this stage.\nNevertheless, From our preliminary impressions during development, the\nXMem-based framework demonstrated reasonable segmentation performance and\nsatisfied the clinical real-time requirement. Our work contributes to improving\nthe precision of tumor tracking during MRI-guided radiotherapy, which is\ncrucial for enhancing the accuracy and safety of cancer treatments.", "AI": {"tldr": "本文提出一个基于XMem模型的肿瘤分割框架，用于实时MRI引导的放射治疗，旨在解决TrackRAD2025挑战，并在初步印象中展现了合理的分割性能和实时性。", "motivation": "MRI引导的放射治疗中，实时、精确的肿瘤跟踪对于提高癌症治疗的准确性和安全性至关重要。本研究旨在为TrackRAD2025挑战开发一个先进的肿瘤分割框架。", "method": "该方法利用了XMem模型，一种增强记忆的架构，通过有效整合记忆机制来分割长序列电影式MRI图像中的肿瘤，实现实时肿瘤运动跟踪。", "result": "尽管详细的实验记录已丢失，无法提供精确的定量结果，但根据开发过程中的初步印象，该基于XMem的框架展示了合理的肿瘤分割性能，并满足了临床实时性要求。", "conclusion": "本工作有助于提高MRI引导放射治疗中肿瘤跟踪的精度，这对提升癌症治疗的准确性和安全性至关重要。"}}
{"id": "2509.18987", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18987", "abs": "https://arxiv.org/abs/2509.18987", "authors": ["Abderrahmane Issam", "Yusuf Can Semerci", "Jan Scholtes", "Gerasimos Spanakis"], "title": "DTW-Align: Bridging the Modality Gap in End-to-End Speech Translation with Dynamic Time Warping Alignment", "comment": "Accepted at WMT2025", "summary": "End-to-End Speech Translation (E2E-ST) is the task of translating source\nspeech directly into target text bypassing the intermediate transcription step.\nThe representation discrepancy between the speech and text modalities has\nmotivated research on what is known as bridging the modality gap.\nState-of-the-art methods addressed this by aligning speech and text\nrepresentations on the word or token level. Unfortunately, this requires an\nalignment tool that is not available for all languages. Although this issue has\nbeen addressed by aligning speech and text embeddings using nearest-neighbor\nsimilarity search, it does not lead to accurate alignments. In this work, we\nadapt Dynamic Time Warping (DTW) for aligning speech and text embeddings during\ntraining. Our experiments demonstrate the effectiveness of our method in\nbridging the modality gap in E2E-ST. Compared to previous work, our method\nproduces more accurate alignments and achieves comparable E2E-ST results while\nbeing significantly faster. Furthermore, our method outperforms previous work\nin low resource settings on 5 out of 6 language directions.", "AI": {"tldr": "本文提出了一种基于动态时间规整（DTW）的方法，用于在端到端语音翻译（E2E-ST）训练期间对齐语音和文本嵌入，以弥合模态差距，实现更准确、更快速的对齐，并在低资源环境下表现优异。", "motivation": "端到端语音翻译（E2E-ST）面临语音和文本模态间的表示差异，即“模态鸿沟”。现有方法（如词或token级别对齐）需要并非所有语言都可用的对齐工具，而最近邻相似性搜索虽无需工具但对齐精度不足。", "method": "本文在训练过程中，采用动态时间规整（DTW）来对齐语音和文本的嵌入表示。", "result": "该方法有效弥合了E2E-ST中的模态鸿沟。与现有工作相比，它能生成更准确的对齐，实现可比的E2E-ST结果，同时显著提高速度。此外，在6个语言方向中的5个低资源设置下，该方法优于现有工作。", "conclusion": "所提出的DTW方法能有效、准确且快速地对齐语音和文本嵌入，成功弥合了E2E-ST中的模态差距，尤其在低资源环境下表现出色。"}}
{"id": "2509.19047", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.19047", "abs": "https://arxiv.org/abs/2509.19047", "authors": ["Geonhyup Lee", "Yeongjin Lee", "Kangmin Kim", "Seongju Lee", "Sangjun Noh", "Seunghyeok Back", "Kyoobin Lee"], "title": "ManipForce: Force-Guided Policy Learning with Frequency-Aware Representation for Contact-Rich Manipulation", "comment": "9 pages, 9 figures", "summary": "Contact-rich manipulation tasks such as precision assembly require precise\ncontrol of interaction forces, yet existing imitation learning methods rely\nmainly on vision-only demonstrations. We propose ManipForce, a handheld system\ndesigned to capture high-frequency force-torque (F/T) and RGB data during\nnatural human demonstrations for contact-rich manipulation. Building on these\ndemonstrations, we introduce the Frequency-Aware Multimodal Transformer (FMT).\nFMT encodes asynchronous RGB and F/T signals using frequency- and\nmodality-aware embeddings and fuses them via bi-directional cross-attention\nwithin a transformer diffusion policy. Through extensive experiments on six\nreal-world contact-rich manipulation tasks - such as gear assembly, box\nflipping, and battery insertion - FMT trained on ManipForce demonstrations\nachieves robust performance with an average success rate of 83% across all\ntasks, substantially outperforming RGB-only baselines. Ablation and\nsampling-frequency analyses further confirm that incorporating high-frequency\nF/T data and cross-modal integration improves policy performance, especially in\ntasks demanding high precision and stable contact.", "AI": {"tldr": "该研究提出了ManipForce系统用于采集高频力/扭矩和RGB数据，并基于此数据开发了频率感知多模态Transformer (FMT) 模型，以实现精确的接触式操作，显著优于仅视觉的方法。", "motivation": "现有的模仿学习方法主要依赖于纯视觉演示，难以精确控制交互力，从而无法有效处理需要精确力控制的接触式操作任务。", "method": "研究方法包括：1) 开发ManipForce手持系统，用于在人类演示中捕获高频力/扭矩(F/T)和RGB数据；2) 引入频率感知多模态Transformer (FMT) 模型，该模型利用频率和模态感知的嵌入编码异步RGB和F/T信号，并通过双向交叉注意力在Transformer扩散策略中融合这些信号。", "result": "在六个真实的接触式操作任务（如齿轮组装、翻转盒子、电池插入）中，FMT模型在ManipForce演示数据上训练后，实现了83%的平均成功率，性能显著优于仅RGB的基线方法。消融实验和采样频率分析进一步证实，整合高频F/T数据和跨模态融合显著提升了策略性能，尤其是在需要高精度和稳定接触的任务中。", "conclusion": "该研究表明，通过结合高频力/扭矩数据和多模态学习（特别是使用FMT模型），可以显著提高模仿学习在接触式操作任务中的性能和鲁棒性，尤其对于高精度操作至关重要。"}}
{"id": "2509.18980", "categories": ["cs.AI", "cs.HC", "cs.IR", "H.3.3; H.5.2; I.2.7"], "pdf": "https://arxiv.org/pdf/2509.18980", "abs": "https://arxiv.org/abs/2509.18980", "authors": ["Maxime Manderlier", "Fabian Lecron", "Olivier Vu Thanh", "Nicolas Gillis"], "title": "From latent factors to language: a user study on LLM-generated explanations for an inherently interpretable matrix-based recommender system", "comment": null, "summary": "We investigate whether large language models (LLMs) can generate effective,\nuser-facing explanations from a mathematically interpretable recommendation\nmodel. The model is based on constrained matrix factorization, where user types\nare explicitly represented and predicted item scores share the same scale as\nobserved ratings, making the model's internal representations and predicted\nscores directly interpretable. This structure is translated into natural\nlanguage explanations using carefully designed LLM prompts. Many works in\nexplainable AI rely on automatic evaluation metrics, which often fail to\ncapture users' actual needs and perceptions. In contrast, we adopt a\nuser-centered approach: we conduct a study with 326 participants who assessed\nthe quality of the explanations across five key dimensions-transparency,\neffectiveness, persuasion, trust, and satisfaction-as well as the\nrecommendations themselves.To evaluate how different explanation strategies are\nperceived, we generate multiple explanation types from the same underlying\nmodel, varying the input information provided to the LLM. Our analysis reveals\nthat all explanation types are generally well received, with moderate\nstatistical differences between strategies. User comments further underscore\nhow participants react to each type of explanation, offering complementary\ninsights beyond the quantitative results.", "AI": {"tldr": "本研究探讨大型语言模型（LLMs）如何为数学可解释的推荐模型生成有效的用户解释，并通过用户研究评估其质量。", "motivation": "可解释人工智能（XAI）领域中，许多工作依赖自动化评估指标，这些指标往往未能捕捉用户的实际需求和感知。因此，研究旨在采用以用户为中心的方法来评估解释的质量。", "method": "研究采用了一个基于约束矩阵分解的数学可解释推荐模型，其内部表示和预测分数直接可解释。通过精心设计的LLM提示，将模型结构转换为自然语言解释。进行了一项包含326名参与者的用户研究，评估了五关键维度（透明度、有效性、说服力、信任和满意度）的解释质量以及推荐本身。通过改变提供给LLM的输入信息，生成了多种解释类型以评估不同策略的感知效果。", "result": "分析显示，所有解释类型普遍受到好评，不同解释策略之间存在中等的统计学差异。用户评论进一步强调了参与者对每种解释类型的反应，提供了超越定量结果的补充见解。", "conclusion": "LLMs能够为可解释的推荐模型生成有效的用户导向解释，并且这些解释在用户研究中普遍表现良好。用户中心评估对于理解解释的实际效果至关重要。"}}
{"id": "2509.18593", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18593", "abs": "https://arxiv.org/abs/2509.18593", "authors": ["Xiaoman Wu", "Lubin Gan", "Siying Wu", "Jing Zhang", "Yunwei Ou", "Xiaoyan Sun"], "title": "SSCM: A Spatial-Semantic Consistent Model for Multi-Contrast MRI Super-Resolution", "comment": null, "summary": "Multi-contrast Magnetic Resonance Imaging super-resolution (MC-MRI SR) aims\nto enhance low-resolution (LR) contrasts leveraging high-resolution (HR)\nreferences, shortening acquisition time and improving imaging efficiency while\npreserving anatomical details. The main challenge lies in maintaining\nspatial-semantic consistency, ensuring anatomical structures remain\nwell-aligned and coherent despite structural discrepancies and motion between\nthe target and reference images. Conventional methods insufficiently model\nspatial-semantic consistency and underuse frequency-domain information, which\nleads to poor fine-grained alignment and inadequate recovery of high-frequency\ndetails. In this paper, we propose the Spatial-Semantic Consistent Model\n(SSCM), which integrates a Dynamic Spatial Warping Module for inter-contrast\nspatial alignment, a Semantic-Aware Token Aggregation Block for long-range\nsemantic consistency, and a Spatial-Frequency Fusion Block for fine structure\nrestoration. Experiments on public and private datasets show that SSCM achieves\nstate-of-the-art performance with fewer parameters while ensuring spatially and\nsemantically consistent reconstructions.", "AI": {"tldr": "本文提出了一种名为SSCM的多对比度磁共振成像超分辨率（MC-MRI SR）模型，通过动态空间配准、语义感知聚合和空频融合，在保持空间语义一致性的同时，实现了卓越的图像重建性能。", "motivation": "MC-MRI SR旨在利用高分辨率（HR）参考图像增强低分辨率（LR）对比度，以缩短采集时间并提高成像效率。然而，主要挑战在于如何在目标和参考图像之间存在结构差异和运动的情况下，保持空间语义一致性，确保解剖结构良好对齐和连贯。传统方法在空间语义一致性建模和频率域信息利用方面不足，导致精细对齐不佳和高频细节恢复不足。", "method": "本文提出空间语义一致性模型（SSCM），该模型整合了三个关键模块：1) 动态空间形变模块（Dynamic Spatial Warping Module），用于实现对比度间的空间对齐；2) 语义感知令牌聚合模块（Semantic-Aware Token Aggregation Block），用于实现长距离语义一致性；3) 空频融合模块（Spatial-Frequency Fusion Block），用于恢复精细结构并利用频率域信息。", "result": "在公共和私有数据集上的实验表明，SSCM以更少的参数实现了最先进的性能，同时确保了空间和语义一致的重建效果。", "conclusion": "SSCM通过有效整合动态空间对齐、长距离语义一致性和空频融合，成功解决了MC-MRI SR中保持空间语义一致性和恢复高频细节的挑战，达到了优越且一致的重建效果。"}}
{"id": "2509.19020", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19020", "abs": "https://arxiv.org/abs/2509.19020", "authors": ["Shaomu Tan", "Ryosuke Mitani", "Ritvik Choudhary", "Toshiyuki Sekiya"], "title": "Investigating Test-Time Scaling with Reranking for Machine Translation", "comment": null, "summary": "Scaling model parameters has become the de facto strategy for improving NLP\nsystems, but it comes with substantial computational costs. Test-Time Scaling\n(TTS) offers an alternative by allocating more computation at inference:\ngenerating multiple candidates and selecting the best. While effective in tasks\nsuch as mathematical reasoning, TTS has not been systematically explored for\nmachine translation (MT). In this paper, we present the first systematic study\nof TTS for MT, investigating a simple but practical best-of-N framework on\nWMT24 benchmarks. Our experiments cover six high-resource and one low-resource\nlanguage pairs, five model sizes (3B-72B), and various TTS compute budget (N up\nto 1024). Our results show that a) For high-resource languages, TTS generally\nimproves translation quality according to multiple neural MT evaluation\nmetrics, and our human evaluation confirms these gains; b) Augmenting smaller\nmodels with large $N$ can match or surpass larger models at $N{=}1$ with more\ncompute cost; c) Under fixed compute budgets, larger models are typically more\nefficient, and TTS can degrade quality due to metric blind spots in\nlow-resource cases.", "AI": {"tldr": "本文首次系统研究了机器翻译（MT）中的测试时扩展（TTS）策略，发现其在高质量语言对中能有效提升翻译质量，并探讨了不同模型大小和计算预算下的性能与效率权衡。", "motivation": "扩展模型参数虽然能提升NLP系统性能，但计算成本高昂。测试时扩展（TTS）提供了一种替代方案，即在推理时分配更多计算资源（生成多个候选并选择最佳），但在机器翻译领域尚未得到系统探索。", "method": "研究采用了一个简单实用的“N选一”框架，在WMT24基准上对MT中的TTS进行了首次系统研究。实验涵盖了六个高资源语言对和一个低资源语言对，五种模型大小（3B-72B），以及高达1024的不同TTS计算预算（N值）。", "result": "a) 对于高资源语言，TTS通常能根据多个神经MT评估指标提升翻译质量，并通过人工评估证实了这些提升；b) 使用大N值增强的小模型可以在付出更多计算成本的情况下，达到或超越N=1时的大模型性能；c) 在固定计算预算下，大模型通常更高效，但在低资源情况下，由于评估指标的盲点，TTS可能会降低质量。", "conclusion": "TTS是提升机器翻译质量的有效策略，特别是在高资源语言对中。虽然较小的模型可以通过增加推理计算量来弥补其与大模型之间的差距，但在固定计算预算下，大模型通常更具效率。此外，在低资源场景下，TTS的效果可能受限于评估指标的准确性。"}}
{"id": "2509.19076", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.19076", "abs": "https://arxiv.org/abs/2509.19076", "authors": ["Laura Connolly", "Aravind S. Kumar", "Kapi Ketan Mehta", "Lidia Al-Zogbi", "Peter Kazanzides", "Parvin Mousavi", "Gabor Fichtinger", "Axel Krieger", "Junichi Tokuda", "Russell H. Taylor", "Simon Leonard", "Anton Deguet"], "title": "SlicerROS2: A Research and Development Module for Image-Guided Robotic Interventions", "comment": null, "summary": "Image-guided robotic interventions involve the use of medical imaging in\ntandem with robotics. SlicerROS2 is a software module that combines 3D Slicer\nand robot operating system (ROS) in pursuit of a standard integration approach\nfor medical robotics research. The first release of SlicerROS2 demonstrated the\nfeasibility of using the C++ API from 3D Slicer and ROS to load and visualize\nrobots in real time. Since this initial release, we've rewritten and redesigned\nthe module to offer greater modularity, access to low-level features, access to\n3D Slicer's Python API, and better data transfer protocols. In this paper, we\nintroduce this new design as well as four applications that leverage the core\nfunctionalities of SlicerROS2 in realistic image-guided robotics scenarios.", "AI": {"tldr": "本文介绍了SlicerROS2模块的重新设计版本，该模块旨在为图像引导机器人干预提供标准的集成方法，它结合了3D Slicer和ROS，并提供了增强的模块化、API访问和数据传输功能，并通过四个实际应用进行了演示。", "motivation": "医学机器人研究需要一种标准的集成方法，将医学成像（如3D Slicer）与机器人操作系统（如ROS）结合起来。SlicerROS2的初始版本虽然证明了可行性，但在模块化、低级功能访问、Python API支持和数据传输协议方面仍有改进空间。", "method": "研究人员重新编写并重新设计了SlicerROS2模块，旨在提供更高的模块化、对低级功能的访问、对3D Slicer Python API的访问以及更好的数据传输协议。新设计通过四个利用SlicerROS2核心功能的实际图像引导机器人场景应用进行演示。", "result": "本文介绍了SlicerROS2的新设计，该设计提供了更高的模块化、对低级功能的访问、对3D Slicer Python API的访问以及更好的数据传输协议。此外，通过四个实际的图像引导机器人应用，展示了SlicerROS2核心功能的应用。", "conclusion": "重新设计的SlicerROS2模块提供了一个更强大、更灵活的平台，用于整合3D Slicer和ROS，为医学机器人研究提供了一个标准化的集成方法，并通过实际应用验证了其在图像引导机器人干预中的实用性。"}}
{"id": "2509.18986", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18986", "abs": "https://arxiv.org/abs/2509.18986", "authors": ["Erik Penther", "Michael Grohs", "Jana-Rebecca Rehse"], "title": "Remaining Time Prediction in Outbound Warehouse Processes: A Case Study (Short Paper)", "comment": "Short paper at the ML4PM Workshop 2025, held in conjunction with the\n  ICPM 2025 in Montevideo, Uruguay", "summary": "Predictive process monitoring is a sub-domain of process mining which aims to\nforecast the future of ongoing process executions. One common prediction target\nis the remaining time, meaning the time that will elapse until a process\nexecution is completed. In this paper, we compare four different remaining time\nprediction approaches in a real-life outbound warehouse process of a logistics\ncompany in the aviation business. For this process, the company provided us\nwith a novel and original event log with 169,523 traces, which we can make\npublicly available. Unsurprisingly, we find that deep learning models achieve\nthe highest accuracy, but shallow methods like conventional boosting techniques\nachieve competitive accuracy and require significantly fewer computational\nresources.", "AI": {"tldr": "本文比较了四种剩余时间预测方法在航空物流公司实际出库流程中的表现，发现深度学习模型精度最高，但浅层方法在精度具竞争力且计算资源需求显著更少。", "motivation": "预测性过程监控旨在预测正在进行的过程执行的未来，其中一个常见目标是预测剩余时间。研究者希望在真实场景中比较不同剩余时间预测方法的有效性。", "method": "研究人员比较了四种不同的剩余时间预测方法，并使用一家航空物流公司提供的包含169,523条轨迹的真实出库仓库流程事件日志进行评估。该事件日志已被公开。", "result": "研究发现深度学习模型实现了最高的预测精度，但传统的提升（boosting）技术等浅层方法也取得了具有竞争力的精度，并且需要显著更少的计算资源。", "conclusion": "对于剩余时间预测，深度学习模型虽然精度最高，但浅层方法在精度和计算资源需求之间提供了良好的平衡，可能是在资源受限环境下的实用选择。同时，研究公开了一个新的真实事件日志。"}}
{"id": "2509.18600", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18600", "abs": "https://arxiv.org/abs/2509.18600", "authors": ["Zhuoxiao Chen", "Hongyang Yu", "Ying Xu", "Yadan Luo", "Long Duong", "Yuan-Fang Li"], "title": "OraPO: Oracle-educated Reinforcement Learning for Data-efficient and Factual Radiology Report Generation", "comment": null, "summary": "Radiology report generation (RRG) aims to automatically produce clinically\nfaithful reports from chest X-ray images. Prevailing work typically follows a\nscale-driven paradigm, by multi-stage training over large paired corpora and\noversized backbones, making pipelines highly data- and compute-intensive. In\nthis paper, we propose Oracle-educated GRPO {OraPO) with a FactScore-based\nreward (FactS) to tackle the RRG task under constrained budgets. OraPO enables\nsingle-stage, RL-only training by converting failed GRPO explorations on rare\nor difficult studies into direct preference supervision via a lightweight\noracle step. FactS grounds learning in diagnostic evidence by extracting atomic\nclinical facts and checking entailment against ground-truth labels, yielding\ndense, interpretable sentence-level rewards. Together, OraPO and FactS create a\ncompact and powerful framework that significantly improves learning efficiency\non clinically challenging cases, setting the new SOTA performance on the\nCheXpert Plus dataset (0.341 in F1) with 2--3 orders of magnitude less training\ndata using a small base VLM on modest hardware.", "AI": {"tldr": "本文提出了一种名为OraPO（Oracle-educated GRPO）的新型放射学报告生成（RRG）框架，结合FactScore-based奖励（FactS），实现了在有限资源下高效训练，并在CheXpert Plus数据集上以更少的数据和计算资源达到SOTA性能。", "motivation": "现有的放射学报告生成方法通常遵循规模驱动范式，需要多阶段训练、大量的配对语料库和超大型骨干网络，导致其数据和计算密集度高，难以在资源受限的环境下应用。", "method": "本文提出了Oracle-educated GRPO (OraPO) 和基于FactScore的奖励 (FactS)。OraPO通过将GRPO在稀有或困难案例上的失败探索转化为直接偏好监督，实现了单阶段、仅RL的训练。FactS通过提取原子临床事实并对照真实标签检查蕴涵关系，提供密集、可解释的句子级奖励，将学习建立在诊断证据之上。", "result": "OraPO和FactS的结合显著提高了临床挑战性病例的学习效率，在CheXpert Plus数据集上达到了新的SOTA性能（F1得分为0.341），且仅使用了小型VLM和适度硬件，训练数据量减少了2-3个数量级。", "conclusion": "OraPO和FactS共同创建了一个紧凑而强大的框架，显著提高了放射学报告生成任务中临床挑战性病例的学习效率，以更少的训练数据和计算资源实现了卓越的性能，为资源受限环境下的RRG提供了有效解决方案。"}}
{"id": "2509.19033", "categories": ["cs.CL", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2509.19033", "abs": "https://arxiv.org/abs/2509.19033", "authors": ["Chiara Alzetta", "Serena Auriemma", "Alessandro Bondielli", "Luca Dini", "Chiara Fazzone", "Alessio Miaschi", "Martina Miliani", "Marta Sartor"], "title": "Charting a Decade of Computational Linguistics in Italy: The CLiC-it Corpus", "comment": "Submitted to IJCoL", "summary": "Over the past decade, Computational Linguistics (CL) and Natural Language\nProcessing (NLP) have evolved rapidly, especially with the advent of\nTransformer-based Large Language Models (LLMs). This shift has transformed\nresearch goals and priorities, from Lexical and Semantic Resources to Language\nModelling and Multimodality. In this study, we track the research trends of the\nItalian CL and NLP community through an analysis of the contributions to\nCLiC-it, arguably the leading Italian conference in the field. We compile the\nproceedings from the first 10 editions of the CLiC-it conference (from 2014 to\n2024) into the CLiC-it Corpus, providing a comprehensive analysis of both its\nmetadata, including author provenance, gender, affiliations, and more, as well\nas the content of the papers themselves, which address various topics. Our goal\nis to provide the Italian and international research communities with valuable\ninsights into emerging trends and key developments over time, supporting\ninformed decisions and future directions in the field.", "AI": {"tldr": "本研究通过分析意大利领先的计算语言学与自然语言处理会议CLiC-it在过去十年的会议论文，追踪了意大利该领域的研究趋势，旨在为研究社区提供有价值的洞察。", "motivation": "计算语言学（CL）和自然语言处理（NLP）领域，特别是随着基于Transformer的大型语言模型（LLMs）的出现，在过去十年中迅速发展，改变了研究目标和优先事项。因此，有必要了解这些变化在特定研究社区（如意大利）中的体现。", "method": "研究人员收集了CLiC-it会议前十届（2014-2024年）的论文集，构建了CLiC-it语料库。他们对语料库的元数据（包括作者来源、性别、单位等）和论文内容（涉及的各种主题）进行了全面分析。", "result": "本研究提供了对意大利计算语言学和自然语言处理社区在过去十年中新兴趋势和关键发展的有价值的洞察，但抽象中并未列出具体发现。", "conclusion": "通过提供这些洞察，本研究旨在支持意大利和国际研究社区做出明智的决策，并指导该领域未来的发展方向。"}}
{"id": "2509.19080", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19080", "abs": "https://arxiv.org/abs/2509.19080", "authors": ["Zhennan Jiang", "Kai Liu", "Yuxin Qin", "Shuai Tian", "Yupeng Zheng", "Mingcai Zhou", "Chao Yu", "Haoran Li", "Dongbin Zhao"], "title": "World4RL: Diffusion World Models for Policy Refinement with Reinforcement Learning for Robotic Manipulation", "comment": null, "summary": "Robotic manipulation policies are commonly initialized through imitation\nlearning, but their performance is limited by the scarcity and narrow coverage\nof expert data. Reinforcement learning can refine polices to alleviate this\nlimitation, yet real-robot training is costly and unsafe, while training in\nsimulators suffers from the sim-to-real gap. Recent advances in generative\nmodels have demonstrated remarkable capabilities in real-world simulation, with\ndiffusion models in particular excelling at generation. This raises the\nquestion of how diffusion model-based world models can be combined to enhance\npre-trained policies in robotic manipulation. In this work, we propose\nWorld4RL, a framework that employs diffusion-based world models as\nhigh-fidelity simulators to refine pre-trained policies entirely in imagined\nenvironments for robotic manipulation. Unlike prior works that primarily employ\nworld models for planning, our framework enables direct end-to-end policy\noptimization. World4RL is designed around two principles: pre-training a\ndiffusion world model that captures diverse dynamics on multi-task datasets and\nrefining policies entirely within a frozen world model to avoid online\nreal-world interactions. We further design a two-hot action encoding scheme\ntailored for robotic manipulation and adopt diffusion backbones to improve\nmodeling fidelity. Extensive simulation and real-world experiments demonstrate\nthat World4RL provides high-fidelity environment modeling and enables\nconsistent policy refinement, yielding significantly higher success rates\ncompared to imitation learning and other baselines. More visualization results\nare available at https://world4rl.github.io/.", "AI": {"tldr": "World4RL框架利用基于扩散模型的环境模型作为高保真模拟器，在纯想象环境中优化机器人操作策略，显著提升了策略性能。", "motivation": "机器人操作策略的初始化通常依赖模仿学习，但受限于专家数据的稀缺性和覆盖范围。强化学习可以改进策略，但真实机器人训练成本高昂且不安全，而模拟器训练则面临“模拟到现实”的鸿沟。鉴于生成模型（特别是扩散模型）在真实世界模拟方面的显著进步，研究者旨在探索如何将基于扩散模型的环境模型结合起来，以增强预训练的机器人操作策略。", "method": "提出了World4RL框架，该框架采用基于扩散模型的环境模型作为高保真模拟器，完全在想象环境中精炼预训练策略，实现端到端的策略优化。其设计基于两大原则：1) 预训练一个能在多任务数据集中捕捉多样化动态的扩散环境模型；2) 在冻结的环境模型中完全精炼策略，避免在线的真实世界交互。此外，还设计了一种专为机器人操作定制的two-hot动作编码方案，并采用扩散骨干网络以提高建模保真度。", "result": "广泛的模拟和真实世界实验表明，World4RL提供了高保真的环境建模，并实现了持续的策略精炼，与模仿学习和其他基线相比，成功率显著提高。", "conclusion": "World4RL框架通过将基于扩散模型的环境模型作为高保真模拟器，在纯想象环境中有效精炼机器人操作策略，克服了传统方法的数据限制和“模拟到现实”的挑战，为机器人操作策略的优化提供了一条有效途径。"}}
{"id": "2509.19030", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19030", "abs": "https://arxiv.org/abs/2509.19030", "authors": ["Victoire Hervé", "Henrik Warpefelt", "Christoph Salge"], "title": "Landmarks, Monuments, and Beacons: Understanding Generative Calls to Action", "comment": null, "summary": "Algorithmic evaluation of procedurally generated content struggles to find\nmetrics that align with human experience, particularly for composite artefacts.\nAutomatic decomposition as a possible solution requires concepts that meet a\nrange of properties. To this end, drawing on Games Studies and Game AI\nresearch, we introduce the nested concepts of \\textit{Landmarks},\n\\textit{Monuments}, and \\textit{Beacons}. These concepts are based on the\nartefact's perceivability, evocativeness, and Call to Action, all from a\nplayer-centric perspective. These terms are generic to games and usable across\ngenres. We argue that these entities can be found and evaluated with techniques\ncurrently used in both research and industry, opening a path towards a fully\nautomated decomposition of PCG, and evaluation of the salient sub-components.\nAlthough the work presented here emphasises mixed-initiative PCG and\ncompositional PCG, we believe it applies beyond those domains. With this\napproach, we intend to create a connection between humanities and technical\ngame research and allow for better computational PCG evaluation", "AI": {"tldr": "本文提出“地标(Landmarks)”、“纪念碑(Monuments)”和“灯塔(Beacons)”等嵌套概念，用于从玩家视角自动分解和评估程序生成内容（PCG），以解决当前评估指标与人类体验不符的问题。", "motivation": "当前程序生成内容（PCG）的算法评估，特别是对复合型作品的评估，缺乏与人类体验相符的指标。自动分解作为一种可能的解决方案，需要满足一系列属性的概念。", "method": "借鉴游戏研究和游戏AI研究，引入了“地标”、“纪念碑”和“灯塔”这三个嵌套概念。这些概念基于玩家视角下的可感知性、唤起性和行动召唤，并且通用性强，可跨游戏类型使用。提出这些实体可以通过现有研究和工业技术进行发现和评估。", "result": "这些概念为PCG的完全自动化分解和显著子组件的评估开辟了道路。尽管工作侧重于混合主动式PCG和组合式PCG，但其适用范围更广。这种方法旨在连接人文和技术游戏研究，实现更好的计算PCG评估。", "conclusion": "通过引入玩家中心、可分解的概念（地标、纪念碑、灯塔），本文旨在弥合人文与技术游戏研究之间的鸿沟，从而实现更有效、更符合人类体验的程序生成内容计算评估。"}}
{"id": "2509.18602", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18602", "abs": "https://arxiv.org/abs/2509.18602", "authors": ["Xu Liu", "Yibo Lu", "Xinxian Wang", "Xinyu Wu"], "title": "Training-Free Multi-Style Fusion Through Reference-Based Adaptive Modulation", "comment": "Accepted at ACPR 2025 (oral)", "summary": "We propose Adaptive Multi-Style Fusion (AMSF), a reference-based\ntraining-free framework that enables controllable fusion of multiple reference\nstyles in diffusion models. Most of the existing reference-based methods are\nlimited by (a) acceptance of only one style image, thus prohibiting hybrid\naesthetics and scalability to more styles, and (b) lack of a principled\nmechanism to balance several stylistic influences. AMSF mitigates these\nchallenges by encoding all style images and textual hints with a semantic token\ndecomposition module that is adaptively injected into every cross-attention\nlayer of an frozen diffusion model. A similarity-aware re-weighting module then\nrecalibrates, at each denoising step, the attention allocated to every style\ncomponent, yielding balanced and user-controllable blends without any\nfine-tuning or external adapters. Both qualitative and quantitative evaluations\nshow that AMSF produces multi-style fusion results that consistently outperform\nthe state-of-the-art approaches, while its fusion design scales seamlessly to\ntwo or more styles. These capabilities position AMSF as a practical step toward\nexpressive multi-style generation in diffusion models.", "AI": {"tldr": "AMSF是一个基于参考、免训练的框架，它使扩散模型能够可控地融合多种参考风格，解决了现有方法在多风格融合和风格平衡方面的局限性。", "motivation": "现有的基于参考的方法主要受限于：1) 只能接受一张风格图像，限制了混合美学和风格扩展性；2) 缺乏平衡多种风格影响的原理性机制。", "method": "AMSF通过语义令牌分解模块编码所有风格图像和文本提示，并将其自适应地注入到冻结扩散模型的每个交叉注意力层。此外，一个相似性感知重加权模块在每个去噪步骤重新校准分配给每个风格组件的注意力，从而实现平衡且用户可控的风格融合，无需任何微调或外部适配器。", "result": "定性和定量评估均表明，AMSF生成的多风格融合结果始终优于现有最先进的方法。其融合设计可以无缝扩展到两种或更多风格。", "conclusion": "AMSF的能力使其成为在扩散模型中实现富有表现力的多风格生成的一个实用进展。"}}
{"id": "2509.19094", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.19094", "abs": "https://arxiv.org/abs/2509.19094", "authors": ["Alireza Salemi", "Cheng Li", "Mingyang Zhang", "Qiaozhu Mei", "Zhuowan Li", "Spurthi Amba Hombaiah", "Weize Kong", "Tao Chen", "Hamed Zamani", "Michael Bendersky"], "title": "Pathways of Thoughts: Multi-Directional Thinking for Long-form Personalized Question Answering", "comment": null, "summary": "Personalization is essential for adapting question answering (QA) systems to\nuser-specific information needs, thereby improving both accuracy and user\nsatisfaction. However, personalized QA remains relatively underexplored due to\nchallenges such as inferring preferences from long, noisy, and implicit\ncontexts, and generating responses that are simultaneously correct,\ncontextually appropriate, and aligned with user expectations and background\nknowledge. To address these challenges, we propose Pathways of Thoughts (PoT),\nan inference-stage method that applies to any large language model (LLM)\nwithout requiring task-specific fine-tuning. The approach models the reasoning\nof an LLM as an iterative decision process, where the model dynamically selects\namong cognitive operations such as reasoning, revision, personalization, and\nclarification. This enables exploration of multiple reasoning trajectories,\nproducing diverse candidate responses that capture different perspectives. PoT\nthen aggregates and reweights these candidates according to inferred user\npreferences, yielding a final personalized response that benefits from the\ncomplementary strengths of diverse reasoning paths. Experiments on the LaMP-QA\nbenchmark for personalized QA show that PoT consistently outperforms\ncompetitive baselines, achieving up to a 13.1% relative improvement. Human\nevaluation corroborates these results, with annotators preferring outputs from\nPoT in 66% of cases and reporting ties in only 15% of cases.", "AI": {"tldr": "本文提出了一种名为“思维路径”（PoT）的推理阶段方法，旨在提升大型语言模型（LLM）在个性化问答（QA）中的表现。PoT通过迭代决策过程探索多条推理路径并根据用户偏好聚合候选答案，无需微调即可显著优于基线模型。", "motivation": "个性化问答对于根据用户特定信息需求调整问答系统至关重要，但由于从冗长、嘈杂和隐式上下文中推断偏好以及生成既正确又符合用户期望和背景知识的响应等挑战，该领域仍未得到充分探索。", "method": "PoT是一种推理阶段方法，适用于任何大型语言模型，无需进行特定任务的微调。它将LLM的推理建模为一个迭代决策过程，模型动态选择推理、修订、个性化和澄清等认知操作。这使得模型能够探索多种推理轨迹，生成捕捉不同视角的多元候选响应。随后，PoT根据推断出的用户偏好聚合并重新加权这些候选答案，从而产生最终的个性化响应。", "result": "在个性化问答基准LaMP-QA上的实验表明，PoT始终优于竞争性基线模型，相对改进高达13.1%。人工评估也证实了这些结果，标注者在66%的情况下更倾向于PoT的输出，仅在15%的情况下报告平局。", "conclusion": "PoT通过建模迭代决策过程以探索多样化的推理路径，并根据用户偏好聚合响应，有效解决了个性化问答中的挑战。该方法在无需微调的情况下显著提升了LLM的个性化问答能力，提高了准确性和用户满意度。"}}
{"id": "2509.19102", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19102", "abs": "https://arxiv.org/abs/2509.19102", "authors": ["Hongli Xu", "Lei Zhang", "Xiaoyue Hu", "Boyang Zhong", "Kaixin Bai", "Zoltán-Csaba Márton", "Zhenshan Bing", "Zhaopeng Chen", "Alois Christian Knoll", "Jianwei Zhang"], "title": "FUNCanon: Learning Pose-Aware Action Primitives via Functional Object Canonicalization for Generalizable Robotic Manipulation", "comment": "project website: https://sites.google.com/view/funcanon, 11 pages", "summary": "General-purpose robotic skills from end-to-end demonstrations often leads to\ntask-specific policies that fail to generalize beyond the training\ndistribution. Therefore, we introduce FunCanon, a framework that converts\nlong-horizon manipulation tasks into sequences of action chunks, each defined\nby an actor, verb, and object. These chunks focus policy learning on the\nactions themselves, rather than isolated tasks, enabling compositionality and\nreuse. To make policies pose-aware and category-general, we perform functional\nobject canonicalization for functional alignment and automatic manipulation\ntrajectory transfer, mapping objects into shared functional frames using\naffordance cues from large vision language models. An object centric and action\ncentric diffusion policy FuncDiffuser trained on this aligned data naturally\nrespects object affordances and poses, simplifying learning and improving\ngeneralization ability. Experiments on simulated and real-world benchmarks\ndemonstrate category-level generalization, cross-task behavior reuse, and\nrobust sim2real deployment, showing that functional canonicalization provides a\nstrong inductive bias for scalable imitation learning in complex manipulation\ndomains. Details of the demo and supplemental material are available on our\nproject website https://sites.google.com/view/funcanon.", "AI": {"tldr": "FunCanon是一个机器人框架，它将长周期操作任务分解为动作块序列，并利用大型视觉语言模型进行功能对象规范化，以实现姿态感知和类别通用的策略。其核心FuncDiffuser扩散策略在对齐数据上训练，显著提高了泛化能力、行为复用和sim2real部署。", "motivation": "传统的端到端机器人演示通常导致任务特定的策略，这些策略难以泛化到训练分布之外，无法实现通用机器人技能。", "method": "FunCanon框架将长周期操作任务转换为一系列动作块（由执行者、动词和对象定义）。通过使用大型视觉语言模型提供的可供性线索，进行功能对象规范化，将对象映射到共享的功能框架中，从而实现策略的姿态感知和类别通用性。在此对齐数据上训练了一个以对象和动作为中心的扩散策略FuncDiffuser。", "result": "实验证明，该方法实现了类别级别的泛化、跨任务行为复用以及鲁棒的sim2real部署。功能规范化为复杂操作领域的可扩展模仿学习提供了强大的归纳偏置。", "conclusion": "FunCanon通过其动作块分解、功能对象规范化和FuncDiffuser扩散策略，成功解决了机器人技能泛化性差的问题，为复杂操作任务中的可扩展模仿学习提供了一种有效途径，显著提高了策略的泛化能力和行为复用性。"}}
{"id": "2509.19058", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19058", "abs": "https://arxiv.org/abs/2509.19058", "authors": ["Kwonho Kim", "Heejeong Nam", "Inwoo Hwang", "Sanghack Lee"], "title": "Towards Causal Representation Learning with Observable Sources as Auxiliaries", "comment": null, "summary": "Causal representation learning seeks to recover latent factors that generate\nobservational data through a mixing function. Needing assumptions on latent\nstructures or relationships to achieve identifiability in general, prior works\noften build upon conditional independence given known auxiliary variables.\nHowever, prior frameworks limit the scope of auxiliary variables to be external\nto the mixing function. Yet, in some cases, system-driving latent factors can\nbe easily observed or extracted from data, possibly facilitating\nidentification. In this paper, we introduce a framework of observable sources\nbeing auxiliaries, serving as effective conditioning variables. Our main\nresults show that one can identify entire latent variables up to subspace-wise\ntransformations and permutations using volume-preserving encoders. Moreover,\nwhen multiple known auxiliary variables are available, we offer a\nvariable-selection scheme to choose those that maximize recoverability of the\nlatent factors given knowledge of the latent causal graph. Finally, we\ndemonstrate the effectiveness of our framework through experiments on synthetic\ngraph and image data, thereby extending the boundaries of current approaches.", "AI": {"tldr": "本文提出了一种利用可观测源作为辅助变量的因果表征学习框架，通过体积保持编码器实现了潜在变量的识别，并扩展了现有方法的边界。", "motivation": "以往的因果表征学习框架通常需要关于潜在结构或关系的假设来实现可识别性，并限制辅助变量必须独立于混合函数。然而，在某些情况下，驱动系统的潜在因素本身是可观测或可从数据中提取的，这可能有助于识别。本文旨在利用这些“可观测源”作为有效的辅助条件变量。", "method": "本文引入了一个将可观测源作为辅助变量的框架，这些源充当有效的条件变量。主要方法包括使用体积保持编码器来识别潜在变量，并提出了一种变量选择方案，当存在多个已知辅助变量时，根据潜在因果图的知识选择能最大化潜在因子可恢复性的变量。", "result": "研究结果表明，该框架能够识别整个潜在变量，精度达到子空间变换和置换。此外，所提出的变量选择方案在已知潜在因果图的情况下，能有效选择最大化潜在因子可恢复性的辅助变量。通过在合成图和图像数据上的实验，验证了该框架的有效性。", "conclusion": "本文提出的框架通过利用可观测源作为辅助变量，有效地扩展了现有因果表征学习方法的边界，实现了潜在变量的识别，并在实验中展现了其有效性。"}}
{"id": "2509.18613", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18613", "abs": "https://arxiv.org/abs/2509.18613", "authors": ["Yuzhi Wu", "Li Xiao", "Jun Liu", "Guangfeng Jiang", "XiangGen Xia"], "title": "MLF-4DRCNet: Multi-Level Fusion with 4D Radar and Camera for 3D Object Detection in Autonomous Driving", "comment": null, "summary": "The emerging 4D millimeter-wave radar, measuring the range, azimuth,\nelevation, and Doppler velocity of objects, is recognized for its\ncost-effectiveness and robustness in autonomous driving. Nevertheless, its\npoint clouds exhibit significant sparsity and noise, restricting its standalone\napplication in 3D object detection. Recent 4D radar-camera fusion methods have\nprovided effective perception. Most existing approaches, however, adopt\nexplicit Bird's-Eye-View fusion paradigms originally designed for LiDAR-camera\nfusion, neglecting radar's inherent drawbacks. Specifically, they overlook the\nsparse and incomplete geometry of radar point clouds and restrict fusion to\ncoarse scene-level integration. To address these problems, we propose\nMLF-4DRCNet, a novel two-stage framework for 3D object detection via\nmulti-level fusion of 4D radar and camera images. Our model incorporates the\npoint-, scene-, and proposal-level multi-modal information, enabling\ncomprehensive feature representation. It comprises three crucial components:\nthe Enhanced Radar Point Encoder (ERPE) module, the Hierarchical Scene Fusion\nPooling (HSFP) module, and the Proposal-Level Fusion Enhancement (PLFE) module.\nOperating at the point-level, ERPE densities radar point clouds with 2D image\ninstances and encodes them into voxels via the proposed Triple-Attention Voxel\nFeature Encoder. HSFP dynamically integrates multi-scale voxel features with 2D\nimage features using deformable attention to capture scene context and adopts\npooling to the fused features. PLFE refines region proposals by fusing image\nfeatures, and further integrates with the pooled features from HSFP.\nExperimental results on the View-of-Delft (VoD) and TJ4DRadSet datasets\ndemonstrate that MLF-4DRCNet achieves the state-of-the-art performance.\nNotably, it attains performance comparable to LiDAR-based models on the VoD\ndataset.", "AI": {"tldr": "本文提出MLF-4DRCNet，一个用于3D目标检测的新型两阶段框架，通过点级、场景级和提议级多层次融合4D毫米波雷达和摄像头图像，有效解决了4D雷达点云稀疏和噪声问题，实现了最先进的性能，甚至可与基于激光雷达的模型媲美。", "motivation": "4D毫米波雷达在自动驾驶中具有成本效益和鲁棒性，但其点云稀疏且噪声大，限制了其在3D目标检测中的独立应用。现有的4D雷达-相机融合方法大多采用为激光雷达-相机融合设计的范式，忽略了雷达点云固有的稀疏性和不完整几何结构，且融合仅限于粗略的场景级集成。", "method": "本文提出了MLF-4DRCNet，一个两阶段的3D目标检测框架，通过点级、场景级和提议级多层次融合4D雷达和摄像头图像。它包含三个关键组件：增强雷达点编码器（ERPE）模块在点级通过2D图像实例加密雷达点云，并使用三重注意力体素特征编码器将其编码为体素；分层场景融合池化（HSFP）模块使用可变形注意力动态整合多尺度体素特征和2D图像特征，并对融合特征进行池化；提议级融合增强（PLFE）模块通过融合图像特征细化区域提议，并进一步与HSFP的池化特征进行集成。", "result": "在View-of-Delft (VoD) 和 TJ4DRadSet 数据集上的实验结果表明，MLF-4DRCNet 实现了最先进的性能。值得注意的是，它在 VoD 数据集上取得了与基于激光雷达的模型相当的性能。", "conclusion": "MLF-4DRCNet通过其独特的多层次融合策略，成功解决了4D雷达点云稀疏和噪声的挑战，显著提升了4D雷达-相机融合在3D目标检测中的感知能力，使其性能达到甚至超越现有先进水平，为自动驾驶中的鲁棒感知提供了有效解决方案。"}}
{"id": "2509.19108", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19108", "abs": "https://arxiv.org/abs/2509.19108", "authors": ["Hiram Ring"], "title": "Are most sentences unique? An empirical examination of Chomskyan claims", "comment": null, "summary": "A repeated claim in linguistics is that the majority of linguistic utterances\nare unique. For example, Pinker (1994: 10), summarizing an argument by Noam\nChomsky, states that \"virtually every sentence that a person utters or\nunderstands is a brand-new combination of words, appearing for the first time\nin the history of the universe.\" With the increased availability of large\ncorpora, this is a claim that can be empirically investigated. The current\npaper addresses the question by using the NLTK Python library to parse corpora\nof different genres, providing counts of exact string matches in each. Results\nshow that while completely unique sentences are often the majority of corpora,\nthis is highly constrained by genre, and that duplicate sentences are not an\ninsignificant part of any individual corpus.", "AI": {"tldr": "本文通过语料库分析，实证检验了语言学中“大多数语句都是独一无二的”这一说法，发现虽然独特句通常占多数，但这受语体类型高度制约，且重复句在任何语料库中都并非微不足道。", "motivation": "语言学中有一个反复出现的观点，即绝大多数语言表达都是独一无二的（例如，Pinker总结Chomsky的观点），但随着大型语料库的普及，这一主张现在可以进行实证检验。", "method": "使用NLTK Python库解析不同语体类型的语料库，并统计每个语料库中完全匹配的字符串（即重复句）的数量。", "result": "结果显示，虽然完全独特的句子通常在语料库中占多数，但这受到语体类型的高度制约。同时，重复句在任何单个语料库中都并非微不足道的一部分。", "conclusion": "关于语言表达近乎完全独特的说法需要修正，语体类型在句子的独特性方面扮演着重要角色，且重复句比传统观念认为的更为常见。"}}
{"id": "2509.19105", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.19105", "abs": "https://arxiv.org/abs/2509.19105", "authors": ["Sarvesh Prajapati", "Ananya Trivedi", "Nathaniel Hanson", "Bruce Maxwell", "Taskin Padir"], "title": "Spectral Signature Mapping from RGB Imagery for Terrain-Aware Navigation", "comment": "8 pages, 10 figures, submitted to Robotic Computing & Communication", "summary": "Successful navigation in outdoor environments requires accurate prediction of\nthe physical interactions between the robot and the terrain. To this end,\nseveral methods rely on geometric or semantic labels to classify traversable\nsurfaces. However, such labels cannot distinguish visually similar surfaces\nthat differ in material properties. Spectral sensors enable inference of\nmaterial composition from surface reflectance measured across multiple\nwavelength bands. Although spectral sensing is gaining traction in robotics,\nwidespread deployment remains constrained by the need for custom hardware\nintegration, high sensor costs, and compute-intensive processing pipelines. In\nthis paper, we present RGB Image to Spectral Signature Neural Network (RS-Net),\na deep neural network designed to bridge the gap between the accessibility of\nRGB sensing and the rich material information provided by spectral data. RS-Net\npredicts spectral signatures from RGB patches, which we map to terrain labels\nand friction coefficients. The resulting terrain classifications are integrated\ninto a sampling-based motion planner for a wheeled robot operating in outdoor\nenvironments. Likewise, the friction estimates are incorporated into a\ncontact-force-based MPC for a quadruped robot navigating slippery surfaces.\nThus, we introduce a framework that learns the task-relevant physical property\nonce during training and thereafter relies solely on RGB sensing at test time.\nThe code is available at https://github.com/prajapatisarvesh/RS-Net.", "AI": {"tldr": "本文提出RS-Net，一个深度神经网络，能从RGB图像预测光谱特征，从而推断地形材料属性和摩擦系数。这使得机器人能利用易得的RGB传感器进行材料感知的户外导航。", "motivation": "户外导航需要准确预测机器人与地形的物理交互。现有方法（几何或语义标签）无法区分外观相似但材料属性不同的表面。光谱传感器能提供材料组成信息，但其部署受限于定制硬件、高成本和计算密集型处理。", "method": "本文提出了RS-Net（RGB图像到光谱特征神经网络），一个深度神经网络，用于从RGB图像块预测光谱特征。这些光谱特征被映射到地形标签和摩擦系数。地形分类结果被集成到轮式机器人的基于采样的运动规划器中，摩擦估计则被集成到四足机器人在湿滑表面导航的基于接触力的MPC中。该框架在训练阶段学习任务相关的物理属性，在测试阶段仅依赖RGB传感。", "result": "RS-Net成功地从RGB图像预测了光谱特征，这些特征被用于地形分类和摩擦系数估计。这些信息被有效地集成到运动规划和MPC中，使得轮式和四足机器人在户外环境中能够进行材料感知的导航。", "conclusion": "本文引入了一个框架，它弥合了RGB传感器的可访问性与光谱数据丰富的材料信息之间的鸿沟。通过RS-Net，机器人能够在训练后仅依靠RGB传感在测试时推断出与任务相关的物理属性（如地形和摩擦力），从而实现更智能的户外导航。"}}
{"id": "2509.19077", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19077", "abs": "https://arxiv.org/abs/2509.19077", "authors": ["Zikang Tian", "Shaohui Peng", "Du Huang", "Jiaming Guo", "Ruizhi Chen", "Rui Zhang", "Xishan Zhang", "Yuxuan Guo", "Zidong Du", "Qi Guo", "Ling Li", "Yewen Pu", "Xing Hu", "Yunji Chen"], "title": "Code Driven Planning with Domain-Adaptive Critic", "comment": null, "summary": "Large Language Models (LLMs) have been widely adopted as task planners for AI\nagents in sequential decision-making problems, leveraging their extensive world\nknowledge. However, the gap between their general knowledge and\nenvironment-specific requirements often leads to inaccurate plans. To address\nthis, existing approaches rely on frequent LLM queries to iteratively refine\nplans based on immediate environmental feedback, which incurs substantial query\ncosts. However, this refinement is typically guided by short-term environmental\nfeedback, limiting LLMs from developing plans aligned with long-term rewards.\nWe propose Code Driven Planning with Domain-Adaptive Critic (CoPiC). Instead of\nrelying on frequent queries, CoPiC employs LLMs to generate a diverse set of\nhigh-level planning programs, which iteratively produce and refine candidate\nplans. A trained domain-adaptive critic then evaluates these candidates and\nselects the one most aligned with long-term rewards for execution. Using\nhigh-level planning programs as planner and domain-adaptive critic as\nestimator, CoPiC improves planning while significantly reducing query costs.\nResults in ALFWorld, NetHack, and StarCraft II Unit Building show that CoPiC\noutperforms advanced LLM-based baselines, AdaPlanner and Reflexion, achieving\nan average (1) 23.33% improvement in success rate and (2) 91.27% reduction in\nquery costs.", "AI": {"tldr": "CoPiC提出了一种结合LLM生成高层规划程序和领域自适应评论器的方法，以减少LLM作为规划器时的高查询成本并提升长期奖励对齐的规划能力，显著提高了成功率并降低了查询成本。", "motivation": "LLM作为AI智能体的任务规划器因其通用知识而广泛应用，但其通用知识与环境特定需求之间的差距常导致不准确的计划。现有方法通过频繁查询LLM并基于短期环境反馈迭代优化计划，导致高昂的查询成本，且这种优化往往只关注短期反馈，未能有效对齐长期奖励。", "method": "CoPiC不依赖频繁查询，而是利用LLM生成多样化的“高层规划程序”。这些程序迭代地生成并精炼候选计划。随后，一个经过训练的“领域自适应评论器”评估这些候选计划，并选择与长期奖励最一致的计划执行。通过将高层规划程序作为规划器，领域自适应评论器作为评估器，CoPiC旨在提高规划效率并显著降低查询成本。", "result": "在ALFWorld、NetHack和StarCraft II Unit Building等任务中，CoPiC的表现优于先进的基于LLM的基线方法（AdaPlanner和Reflexion）。它平均实现了23.33%的成功率提升和91.27%的查询成本降低。", "conclusion": "CoPiC通过利用LLM生成高层规划程序并结合领域自适应评论器来评估长期奖励，有效解决了LLM作为规划器时查询成本高和短期反馈限制的问题。该方法在提高规划成功率的同时，显著降低了LLM的查询成本。"}}
{"id": "2509.18619", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18619", "abs": "https://arxiv.org/abs/2509.18619", "authors": ["Yichen Wu", "Xu Liu", "Chenxuan Zhao", "Xinyu Wu"], "title": "Prompt-Guided Dual Latent Steering for Inversion Problems", "comment": "Accepted at DICTA 2025 (oral)", "summary": "Inverting corrupted images into the latent space of diffusion models is\nchallenging. Current methods, which encode an image into a single latent\nvector, struggle to balance structural fidelity with semantic accuracy, leading\nto reconstructions with semantic drift, such as blurred details or incorrect\nattributes. To overcome this, we introduce Prompt-Guided Dual Latent Steering\n(PDLS), a novel, training-free framework built upon Rectified Flow models for\ntheir stable inversion paths. PDLS decomposes the inversion process into two\ncomplementary streams: a structural path to preserve source integrity and a\nsemantic path guided by a prompt. We formulate this dual guidance as an optimal\ncontrol problem and derive a closed-form solution via a Linear Quadratic\nRegulator (LQR). This controller dynamically steers the generative trajectory\nat each step, preventing semantic drift while ensuring the preservation of fine\ndetail without costly, per-image optimization. Extensive experiments on FFHQ-1K\nand ImageNet-1K under various inversion tasks, including Gaussian deblurring,\nmotion deblurring, super-resolution and freeform inpainting, demonstrate that\nPDLS produces reconstructions that are both more faithful to the original image\nand better aligned with the semantic information than single-latent baselines.", "AI": {"tldr": "新型PDLS框架利用整流流模型，通过结构和语义双路径引导，并采用LQR最优控制，解决了扩散模型潜在空间中损坏图像反演时结构保真度与语义准确性之间的矛盾，实现了高质量、无语义漂移的图像重建。", "motivation": "当前将损坏图像反演到扩散模型潜在空间的方法，通常将图像编码为单一潜在向量，难以平衡结构保真度和语义准确性，导致重建图像出现语义漂移（如细节模糊或属性错误）。", "method": "提出PDLS（Prompt-Guided Dual Latent Steering），一个基于整流流模型的新型无训练框架。PDLS将反演过程分解为结构路径（保留源图像完整性）和语义路径（由提示词引导）两个互补流。将这种双重引导表述为最优控制问题，并通过线性二次调节器（LQR）推导出闭式解，从而在每一步动态引导生成轨迹，防止语义漂移并保留细节，无需昂贵的逐图像优化。", "result": "在FFHQ-1K和ImageNet-1K数据集上，针对高斯去模糊、运动去模糊、超分辨率和自由形式修复等多种反演任务进行的广泛实验表明，PDLS生成的重建图像比单一潜在基线方法更忠实于原始图像，并且与语义信息更一致。", "conclusion": "PDLS框架通过其独特的双潜在引导和LQR最优控制机制，成功解决了现有方法在图像反演中结构与语义平衡的挑战，实现了高保真度且语义准确的重建，显著优于传统单一潜在向量方法。"}}
{"id": "2509.19109", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19109", "abs": "https://arxiv.org/abs/2509.19109", "authors": ["Timur Turatali", "Anton Alekseev", "Gulira Jumalieva", "Gulnara Kabaeva", "Sergey Nikolenko"], "title": "Human-Annotated NER Dataset for the Kyrgyz Language", "comment": "Accepted to TurkLang-2025 conference, DOI and copyright will be added\n  upon confirmation of acceptance to publication in IEEE Xplore", "summary": "We introduce KyrgyzNER, the first manually annotated named entity recognition\ndataset for the Kyrgyz language. Comprising 1,499 news articles from the 24.KG\nnews portal, the dataset contains 10,900 sentences and 39,075 entity mentions\nacross 27 named entity classes. We show our annotation scheme, discuss the\nchallenges encountered in the annotation process, and present the descriptive\nstatistics. We also evaluate several named entity recognition models, including\ntraditional sequence labeling approaches based on conditional random fields and\nstate-of-the-art multilingual transformer-based models fine-tuned on our\ndataset. While all models show difficulties with rare entity categories, models\nsuch as the multilingual RoBERTa variant pretrained on a large corpus across\nmany languages achieve a promising balance between precision and recall. These\nfindings emphasize both the challenges and opportunities of using multilingual\npretrained models for processing languages with limited resources. Although the\nmultilingual RoBERTa model performed best, other multilingual models yielded\ncomparable results. This suggests that future work exploring more granular\nannotation schemes may offer deeper insights for Kyrgyz language processing\npipelines evaluation.", "AI": {"tldr": "本文介绍了KyrgyzNER，首个针对吉尔吉斯语的手动标注命名实体识别数据集，包含1,499篇新闻文章、10,900个句子和39,075个实体提及。同时评估了传统CRF和多语言Transformer模型，发现多语言RoBERTa表现最佳，凸显了多语言预训练模型在低资源语言处理中的潜力。", "motivation": "吉尔吉斯语缺乏命名实体识别（NER）数据集，限制了该语言的自然语言处理发展。研究旨在填补这一空白，并探索现有模型在该低资源语言上的表现。", "method": "研究方法包括：1. 构建KyrgyzNER数据集，手动标注来自24.KG新闻门户的1,499篇新闻文章，共包含27类命名实体。2. 详细描述标注方案和遇到的挑战，并提供描述性统计数据。3. 评估多种NER模型，包括基于条件随机场（CRF）的传统序列标注方法和在数据集上微调的最新多语言Transformer模型（如多语言RoBERTa）。", "result": "KyrgyzNER数据集包含10,900个句子和39,075个实体提及。所有评估模型在稀有实体类别上均表现出困难。多语言RoBERTa变体在准确率和召回率之间取得了有希望的平衡，表现最佳，但其他多语言模型也取得了可比结果。这表明多语言预训练模型在处理资源有限的语言方面既有挑战也有机遇。", "conclusion": "多语言预训练模型对低资源语言处理显示出巨大潜力，尤其是多语言RoBERTa模型。未来的工作可以探索更细粒度的标注方案，为吉尔吉斯语处理流程的评估提供更深入的见解。"}}
{"id": "2509.19142", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.19142", "abs": "https://arxiv.org/abs/2509.19142", "authors": ["Kangmin Kim", "Seunghyeok Back", "Geonhyup Lee", "Sangbeom Lee", "Sangjun Noh", "Kyoobin Lee"], "title": "BiGraspFormer: End-to-End Bimanual Grasp Transformer", "comment": "8 pages, 5 figures", "summary": "Bimanual grasping is essential for robots to handle large and complex\nobjects. However, existing methods either focus solely on single-arm grasping\nor employ separate grasp generation and bimanual evaluation stages, leading to\ncoordination problems including collision risks and unbalanced force\ndistribution. To address these limitations, we propose BiGraspFormer, a unified\nend-to-end transformer framework that directly generates coordinated bimanual\ngrasps from object point clouds. Our key idea is the Single-Guided Bimanual\n(SGB) strategy, which first generates diverse single grasp candidates using a\ntransformer decoder, then leverages their learned features through specialized\nattention mechanisms to jointly predict bimanual poses and quality scores. This\nconditioning strategy reduces the complexity of the 12-DoF search space while\nensuring coordinated bimanual manipulation. Comprehensive simulation\nexperiments and real-world validation demonstrate that BiGraspFormer\nconsistently outperforms existing methods while maintaining efficient inference\nspeed (<0.05s), confirming the effectiveness of our framework. Code and\nsupplementary materials are available at https://sites.google.com/bigraspformer", "AI": {"tldr": "本文提出BiGraspFormer，一个统一的端到端Transformer框架，利用“单臂引导双臂”（SGB）策略，直接从点云生成协调的双臂抓取，解决了现有方法的协调性问题。", "motivation": "现有双臂抓取方法存在局限性，要么只关注单臂抓取，要么将抓取生成与双臂评估分开，导致碰撞风险和力分布不均等协调问题。", "method": "BiGraspFormer是一个统一的端到端Transformer框架，直接从物体点云生成协调的双臂抓取。其核心思想是“单臂引导双臂”（SGB）策略：首先使用Transformer解码器生成多样化的单臂抓取候选，然后通过专门的注意力机制利用这些学习到的特征，共同预测双臂姿态和质量得分。这种条件化策略降低了12自由度搜索空间的复杂性，并确保了双臂操作的协调性。", "result": "全面的仿真实验和真实世界验证表明，BiGraspFormer始终优于现有方法，并保持高效的推理速度（<0.05秒）。", "conclusion": "BiGraspFormer是一个有效的框架，能够解决双臂抓取中的协调性问题，并展现出优异的性能和效率。"}}
{"id": "2509.19236", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19236", "abs": "https://arxiv.org/abs/2509.19236", "authors": ["Chunhao Tian", "Yutong Wang", "Xuebo Liu", "Zhexuan Wang", "Liang Ding", "Miao Zhang", "Min Zhang"], "title": "AgentInit: Initializing LLM-based Multi-Agent Systems via Diversity and Expertise Orchestration for Effective and Efficient Collaboration", "comment": "EMNLP 2025 Findings", "summary": "Proper initialization is crucial for any system, particularly in multi-agent\nsystems (MAS), where it plays a pivotal role in determining both the system's\nefficiency and effectiveness. However, existing MAS initialization methods do\nnot fully account for the collaborative needs of the generated agents in\nsubsequent stages. Inspired by the principles of effective team composition, we\npropose AgentInit, which aims to optimize the structure of agent teams.\nSpecifically, in addition to multi-round interactions and reflections between\nagents during agent generation, AgentInit incorporates a Natural Language to\nFormat mechanism to ensure consistency and standardization. Balanced team\nselection strategies using Pareto principles are subsequently applied to\njointly consider agent team diversity and task relevance to promote effective\nand efficient collaboration and enhance overall system performance. Experiments\nshow that AgentInit consistently outperforms state-of-the-art initialization\nmethods and pre-defined strategies across various frameworks and tasks,\nachieving an overall performance improvement of up to 1.2 and 1.6,\nrespectively, while also significantly reducing token consumption. Further\nanalysis confirms its strong transferability to similar tasks and verifies the\neffectiveness of its key components, demonstrating its capability and\nadaptability as a reliable MAS initialization method. Source code and models\nare available at https://github.com/1737423697/AgentInit.", "AI": {"tldr": "AgentInit是一种新的多智能体系统（MAS）初始化方法，它通过优化智能体团队结构，考虑协作需求、采用多轮交互、自然语言到格式机制和帕累托平衡团队选择策略，显著提高了系统性能并降低了token消耗。", "motivation": "现有的多智能体系统（MAS）初始化方法未能充分考虑后续阶段智能体之间的协作需求，这影响了系统的效率和有效性。", "method": "AgentInit通过以下方法优化智能体团队结构：1. 在智能体生成过程中进行多轮交互和反思；2. 引入自然语言到格式（Natural Language to Format）机制以确保一致性和标准化；3. 采用基于帕累托原则的平衡团队选择策略，综合考虑智能体团队的多样性和任务相关性。", "result": "实验表明，AgentInit在各种框架和任务中持续优于现有最先进的初始化方法和预定义策略，性能分别提高了1.2倍和1.6倍，同时显著降低了token消耗。进一步分析证实了其强大的任务迁移能力，并验证了其关键组件的有效性。", "conclusion": "AgentInit被证明是一种能力强、适应性好、可靠的多智能体系统初始化方法，能够有效提升系统性能和协作效率。"}}
{"id": "2509.18638", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18638", "abs": "https://arxiv.org/abs/2509.18638", "authors": ["Yiwei Lyu", "Samir Harake", "Asadur Chowdury", "Soumyanil Banerjee", "Rachel Gologorsky", "Shixuan Liu", "Anna-Katharina Meissner", "Akshay Rao", "Chenhui Zhao", "Akhil Kondepudi", "Cheng Jiang", "Xinhai Hou", "Rushikesh S. Joshi", "Volker Neuschmelting", "Ashok Srinivasan", "Dawn Kleindorfer", "Brian Athey", "Vikas Gulani", "Aditya Pandey", "Honglak Lee", "Todd Hollon"], "title": "Learning neuroimaging models from health system-scale data", "comment": null, "summary": "Neuroimaging is a ubiquitous tool for evaluating patients with neurological\ndiseases. The global demand for magnetic resonance imaging (MRI) studies has\nrisen steadily, placing significant strain on health systems, prolonging\nturnaround times, and intensifying physician burnout \\cite{Chen2017-bt,\nRula2024-qp-1}. These challenges disproportionately impact patients in\nlow-resource and rural settings. Here, we utilized a large academic health\nsystem as a data engine to develop Prima, the first vision language model (VLM)\nserving as an AI foundation for neuroimaging that supports real-world, clinical\nMRI studies as input. Trained on over 220,000 MRI studies, Prima uses a\nhierarchical vision architecture that provides general and transferable MRI\nfeatures. Prima was tested in a 1-year health system-wide study that included\n30K MRI studies. Across 52 radiologic diagnoses from the major neurologic\ndisorders, including neoplastic, inflammatory, infectious, and developmental\nlesions, Prima achieved a mean diagnostic area under the ROC curve of 92.0,\noutperforming other state-of-the-art general and medical AI models. Prima\noffers explainable differential diagnoses, worklist priority for radiologists,\nand clinical referral recommendations across diverse patient demographics and\nMRI systems. Prima demonstrates algorithmic fairness across sensitive groups\nand can help mitigate health system biases, such as prolonged turnaround times\nfor low-resource populations. These findings highlight the transformative\npotential of health system-scale VLMs and Prima's role in advancing AI-driven\nhealthcare.", "AI": {"tldr": "Prima是一个基于视觉语言模型（VLM）的神经影像AI基础，在22万多份MRI研究上训练，并在3万份MRI研究中测试，在52种放射学诊断中实现了92.0的平均诊断AUC，超越了现有模型，并展示了算法公平性，有望缓解医疗系统压力和偏见。", "motivation": "全球对磁共振成像（MRI）研究的需求持续增长，给医疗系统带来巨大压力，导致周转时间延长和医生职业倦怠，尤其是在资源匮乏和农村地区，这些挑战促使研究开发更高效、公平的神经影像评估工具。", "method": "利用大型学术医疗系统作为数据引擎，开发了Prima，这是第一个服务于神经影像的视觉语言模型（VLM）。Prima采用分层视觉架构，在超过22万份MRI研究上进行训练，并在一个为期一年的、包含3万份MRI研究的医疗系统范围内研究中进行了测试。", "result": "Prima在52种主要神经系统疾病的放射学诊断中，实现了92.0的平均诊断曲线下面积（AUC），优于其他最先进的通用和医学AI模型。它能提供可解释的鉴别诊断、放射科医生工作列表优先级和临床转诊建议，并在一系列患者人口统计和MRI系统上表现出算法公平性，有助于缓解医疗系统偏见，如低资源人群的周转时间过长问题。", "conclusion": "研究结果强调了医疗系统规模的视觉语言模型（VLM）的变革潜力，以及Prima在推动AI驱动的医疗保健和缓解医疗系统偏见方面的作用。"}}
{"id": "2509.19125", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19125", "abs": "https://arxiv.org/abs/2509.19125", "authors": ["Kun Zhu", "Lizi Liao", "Yuxuan Gu", "Lei Huang", "Xiaocheng Feng", "Bing Qin"], "title": "Context-Aware Hierarchical Taxonomy Generation for Scientific Papers via LLM-Guided Multi-Aspect Clustering", "comment": "Accepted to EMNLP 2025 Main", "summary": "The rapid growth of scientific literature demands efficient methods to\norganize and synthesize research findings. Existing taxonomy construction\nmethods, leveraging unsupervised clustering or direct prompting of large\nlanguage models (LLMs), often lack coherence and granularity. We propose a\nnovel context-aware hierarchical taxonomy generation framework that integrates\nLLM-guided multi-aspect encoding with dynamic clustering. Our method leverages\nLLMs to identify key aspects of each paper (e.g., methodology, dataset,\nevaluation) and generates aspect-specific paper summaries, which are then\nencoded and clustered along each aspect to form a coherent hierarchy. In\naddition, we introduce a new evaluation benchmark of 156 expert-crafted\ntaxonomies encompassing 11.6k papers, providing the first naturally annotated\ndataset for this task. Experimental results demonstrate that our method\nsignificantly outperforms prior approaches, achieving state-of-the-art\nperformance in taxonomy coherence, granularity, and interpretability.", "AI": {"tldr": "提出了一种新颖的上下文感知分层分类法生成框架，结合LLM引导的多方面编码和动态聚类，解决了现有方法在科学文献组织中缺乏连贯性和粒度的问题，并取得了最先进的性能。", "motivation": "科学文献的快速增长要求高效的方法来组织和整合研究成果。现有分类法构建方法（如无监督聚类或直接LLM提示）往往缺乏连贯性和粒度。", "method": "该方法整合了LLM引导的多方面编码和动态聚类。具体而言，LLM识别每篇论文的关键方面（如方法、数据集、评估），生成特定方面的摘要，然后对这些摘要进行编码并沿每个方面进行聚类，以形成连贯的层次结构。此外，还引入了一个包含156个专家制作分类法和11.6k篇论文的新评估基准。", "result": "实验结果表明，该方法显著优于现有方法，在分类法的连贯性、粒度和可解释性方面达到了最先进的性能。", "conclusion": "所提出的上下文感知分层分类法生成框架，通过结合LLM的多方面编码和动态聚类，有效解决了科学文献组织中的挑战，并在多个关键评估指标上实现了卓越表现。"}}
{"id": "2509.19168", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.19168", "abs": "https://arxiv.org/abs/2509.19168", "authors": ["Mark Gonzales", "Ethan Oh", "Joseph Moore"], "title": "A Multimodal Stochastic Planning Approach for Navigation and Multi-Robot Coordination", "comment": "8 Pages, 7 Figures", "summary": "In this paper, we present a receding-horizon, sampling-based planner capable\nof reasoning over multimodal policy distributions. By using the cross-entropy\nmethod to optimize a multimodal policy under a common cost function, our\napproach increases robustness against local minima and promotes effective\nexploration of the solution space. We show that our approach naturally extends\nto multi-robot collision-free planning, enables agents to share diverse\ncandidate policies to avoid deadlocks, and allows teams to minimize a global\nobjective without incurring the computational complexity of centralized\noptimization. Numerical simulations demonstrate that employing multiple modes\nsignificantly improves success rates in trap environments and in multi-robot\ncollision avoidance. Hardware experiments further validate the approach's\nreal-time feasibility and practical performance.", "AI": {"tldr": "本文提出了一种基于采样、滚动优化的规划器，通过交叉熵方法优化多模态策略分布，提高了鲁棒性、促进了探索，并自然扩展到多机器人无碰撞规划，经仿真和硬件验证有效。", "motivation": "克服局部最优、有效探索解空间，并实现多机器人无碰撞规划，使智能体能够共享多样化候选策略以避免死锁，同时在不增加集中式优化计算复杂性的情况下最小化全局目标。", "method": "采用滚动优化、基于采样的规划器。核心方法是使用交叉熵方法（CEM）在共同成本函数下优化多模态策略分布。该方法自然扩展到多机器人规划，通过智能体共享多样化的候选策略。", "result": "该方法提高了对抗局部最优的鲁棒性，促进了对解空间的有效探索。它自然地扩展到多机器人无碰撞规划，使智能体能够共享多样化候选策略以避免死锁，并允许团队在不产生集中式优化计算复杂性的情况下最小化全局目标。数值模拟表明，采用多模态显著提高了在陷阱环境和多机器人避碰中的成功率。硬件实验进一步验证了该方法的实时可行性和实际性能。", "conclusion": "所提出的基于交叉熵方法优化多模态策略分布的滚动优化规划器，显著提高了规划的鲁棒性和探索能力，特别是在复杂环境和多机器人协作中，并具有良好的实时性和实用性。"}}
{"id": "2509.19265", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19265", "abs": "https://arxiv.org/abs/2509.19265", "authors": ["Saeed Almheiri", "Rania Hossam", "Mena Attia", "Chenxi Wang", "Preslav Nakov", "Timothy Baldwin", "Fajri Koto"], "title": "Cross-Cultural Transfer of Commonsense Reasoning in LLMs: Evidence from the Arab World", "comment": "EMNLP 2025 - Findings", "summary": "Large language models (LLMs) often reflect Western-centric biases, limiting\ntheir effectiveness in diverse cultural contexts. Although some work has\nexplored cultural alignment, the potential for cross-cultural transfer, using\nalignment in one culture to improve performance in others, remains\nunderexplored. This paper investigates cross-cultural transfer of commonsense\nreasoning in the Arab world, where linguistic and historical similarities\ncoexist with local cultural differences. Using a culturally grounded\ncommonsense reasoning dataset covering 13 Arab countries, we evaluate\nlightweight alignment methods such as in-context learning and\ndemonstration-based reinforcement (DITTO), alongside baselines like supervised\nfine-tuning and direct preference optimization. Our results show that merely 12\nculture-specific examples from one country can improve performance in others by\n10\\% on average, within multilingual models. In addition, we demonstrate that\nout-of-culture demonstrations from Indonesia and US contexts can match or\nsurpass in-culture alignment for MCQ reasoning, highlighting cultural\ncommonsense transferability beyond the Arab world. These findings demonstrate\nthat efficient cross-cultural alignment is possible and offer a promising\napproach to adapt LLMs to low-resource cultural settings.", "AI": {"tldr": "本研究探索了大型语言模型在阿拉伯世界的跨文化常识推理能力，发现少量文化特定示例即可显著提升多语言模型在其他阿拉伯国家的表现，并证明了文化常识的跨文化可迁移性。", "motivation": "大型语言模型（LLMs）普遍存在西方中心偏见，限制了其在多元文化背景下的有效性。尽管已有文化对齐研究，但利用单一文化对齐来提升其他文化表现的“跨文化迁移”潜力尚未得到充分探索。", "method": "研究使用了一个涵盖13个阿拉伯国家的文化常识推理数据集。评估了轻量级对齐方法，如情境学习和基于演示的强化（DITTO），并与监督微调和直接偏好优化等基线方法进行了比较。", "result": "结果显示，仅需来自一个国家的12个文化特定示例，即可使多语言模型在其他国家的表现平均提升10%。此外，来自印度尼西亚和美国背景的非本文化演示，在多项选择题推理方面可以达到或超越本文化对齐的效果，突显了阿拉伯世界之外的文化常识可迁移性。", "conclusion": "这些发现表明，高效的跨文化对齐是可行的，为将大型语言模型适应到低资源文化环境提供了一种有前景的方法。"}}
{"id": "2509.18639", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18639", "abs": "https://arxiv.org/abs/2509.18639", "authors": ["Yuanhuiyi Lyu", "Chi Kit Wong", "Chenfei Liao", "Lutao Jiang", "Xu Zheng", "Zexin Lu", "Linfeng Zhang", "Xuming Hu"], "title": "Understanding-in-Generation: Reinforcing Generative Capability of Unified Model via Infusing Understanding into Generation", "comment": null, "summary": "Recent works have made notable advancements in enhancing unified models for\ntext-to-image generation through the Chain-of-Thought (CoT). However, these\nreasoning methods separate the processes of understanding and generation, which\nlimits their ability to guide the reasoning of unified models in addressing the\ndeficiencies of their generative capabilities. To this end, we propose a novel\nreasoning framework for unified models, Understanding-in-Generation (UiG),\nwhich harnesses the robust understanding capabilities of unified models to\nreinforce their performance in image generation. The core insight of our UiG is\nto integrate generative guidance by the strong understanding capabilities\nduring the reasoning process, thereby mitigating the limitations of generative\nabilities. To achieve this, we introduce \"Image Editing\" as a bridge to infuse\nunderstanding into the generation process. Initially, we verify the generated\nimage and incorporate the understanding of unified models into the editing\ninstructions. Subsequently, we enhance the generated image step by step,\ngradually infusing the understanding into the generation process. Our UiG\nframework demonstrates a significant performance improvement in text-to-image\ngeneration over existing text-to-image reasoning methods, e.g., a 3.92% gain on\nthe long prompt setting of the TIIF benchmark. The project code:\nhttps://github.com/QC-LY/UiG", "AI": {"tldr": "本文提出了一种名为Understanding-in-Generation (UiG)的新型推理框架，通过将强大的理解能力融入到生成过程中，并通过图像编辑逐步增强生成图像，显著提升了统一模型在文本到图像生成方面的性能。", "motivation": "现有的文本到图像生成CoT推理方法将理解和生成过程分离，限制了它们在解决统一模型生成能力缺陷方面的指导作用。", "method": "UiG框架的核心在于将统一模型的强大理解能力融入到生成推理过程中，以弥补生成能力的不足。具体通过引入“图像编辑”作为桥梁：首先验证生成的图像，将统一模型的理解融入到编辑指令中；然后逐步增强生成的图像，将理解逐渐注入到生成过程中。", "result": "UiG框架在文本到图像生成方面展现出显著的性能提升，例如在TIIF基准的长提示设置上获得了3.92%的性能增益，优于现有方法。", "conclusion": "UiG框架通过在生成过程中整合理解能力，有效利用了统一模型的优势，显著提高了其文本到图像的生成性能，克服了传统方法中理解与生成分离的局限性。"}}
{"id": "2509.19143", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.19143", "abs": "https://arxiv.org/abs/2509.19143", "authors": ["Alejandro Cuevas", "Saloni Dash", "Bharat Kumar Nayak", "Dan Vann", "Madeleine I. G. Daepp"], "title": "Anecdoctoring: Automated Red-Teaming Across Language and Place", "comment": "To be published in EMNLP 2025", "summary": "Disinformation is among the top risks of generative artificial intelligence\n(AI) misuse. Global adoption of generative AI necessitates red-teaming\nevaluations (i.e., systematic adversarial probing) that are robust across\ndiverse languages and cultures, but red-teaming datasets are commonly US- and\nEnglish-centric. To address this gap, we propose \"anecdoctoring\", a novel\nred-teaming approach that automatically generates adversarial prompts across\nlanguages and cultures. We collect misinformation claims from fact-checking\nwebsites in three languages (English, Spanish, and Hindi) and two geographies\n(US and India). We then cluster individual claims into broader narratives and\ncharacterize the resulting clusters with knowledge graphs, with which we\naugment an attacker LLM. Our method produces higher attack success rates and\noffers interpretability benefits relative to few-shot prompting. Results\nunderscore the need for disinformation mitigations that scale globally and are\ngrounded in real-world adversarial misuse.", "AI": {"tldr": "该研究提出了一种名为“anecdoctoring”的新型红队评估方法，通过自动生成跨语言和文化的对抗性提示，以应对生成式AI在虚假信息传播方面的风险，并克服现有评估数据集以美国和英语为中心的局限性。", "motivation": "生成式AI的滥用是虚假信息传播的主要风险之一。尽管需要跨语言和文化的鲁棒红队评估，但现有红队数据集普遍以美国和英语为中心，无法满足全球化需求。", "method": "研究提出“anecdoctoring”方法。首先，从事实核查网站收集三种语言（英语、西班牙语、印地语）和两个地理区域（美国、印度）的虚假信息声明。然后，将这些声明聚类成更广泛的叙事，并用知识图谱来表征这些聚类。最后，利用这些知识图谱增强一个攻击者大型语言模型（LLM），以自动生成对抗性提示。", "result": "该方法相比于少样本提示，产生了更高的攻击成功率，并提供了更好的可解释性优势。", "conclusion": "研究结果强调了开发全球规模且基于真实世界对抗性滥用情景的虚假信息缓解措施的必要性。"}}
{"id": "2509.19169", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.19169", "abs": "https://arxiv.org/abs/2509.19169", "authors": ["Tianyu Wu", "Xudong Han", "Haoran Sun", "Zishang Zhang", "Bangchao Huang", "Chaoyang Song", "Fang Wan"], "title": "MagiClaw: A Dual-Use, Vision-Based Soft Gripper for Bridging the Human Demonstration to Robotic Deployment Gap", "comment": "8 pages, 4 figures, accepted to Data@CoRL2025 Workshop", "summary": "The transfer of manipulation skills from human demonstration to robotic\nexecution is often hindered by a \"domain gap\" in sensing and morphology. This\npaper introduces MagiClaw, a versatile two-finger end-effector designed to\nbridge this gap. MagiClaw functions interchangeably as both a handheld tool for\nintuitive data collection and a robotic end-effector for policy deployment,\nensuring hardware consistency and reliability. Each finger incorporates a Soft\nPolyhedral Network (SPN) with an embedded camera, enabling vision-based\nestimation of 6-DoF forces and contact deformation. This proprioceptive data is\nfused with exteroceptive environmental sensing from an integrated iPhone, which\nprovides 6D pose, RGB video, and LiDAR-based depth maps. Through a custom iOS\napplication, MagiClaw streams synchronized, multi-modal data for real-time\nteleoperation, offline policy learning, and immersive control via mixed-reality\ninterfaces. We demonstrate how this unified system architecture lowers the\nbarrier to collecting high-fidelity, contact-rich datasets and accelerates the\ndevelopment of generalizable manipulation policies. Please refer to the iOS app\nat https://apps.apple.com/cn/app/magiclaw/id6661033548 for further details.", "AI": {"tldr": "MagiClaw是一种多功能两指末端执行器，旨在通过在数据收集（手持）和机器人执行（机器人末端）之间提供硬件一致性，并集成视觉和力觉传感器，来弥合人类示教到机器人操作的领域鸿沟。", "motivation": "将人类操作技能转移到机器人执行时，由于感知和形态上的“领域鸿沟”常常受到阻碍。", "method": "本文引入了MagiClaw，这是一种两指末端执行器，既可用作手持工具进行数据收集，也可用作机器人末端执行器进行策略部署，确保硬件一致性。每个手指都包含一个嵌入摄像头的软多面体网络（SPN），用于估计6自由度力和接触变形。这些本体感受数据与集成iPhone提供的外部环境感知（6D姿态、RGB视频、LiDAR深度图）融合。通过定制的iOS应用程序，MagiClaw流式传输同步的多模态数据，用于实时遥操作、离线策略学习和混合现实界面控制。", "result": "该统一系统架构降低了收集高保真、接触丰富数据集的门槛，并加速了可泛化操作策略的开发。", "conclusion": "MagiClaw通过其创新的硬件设计和多模态传感融合，成功弥合了人类示教与机器人执行之间的领域鸿沟，从而简化了数据收集并加速了操作策略的开发。"}}
{"id": "2509.18642", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18642", "abs": "https://arxiv.org/abs/2509.18642", "authors": ["Nicolas Toussaint", "Emanuele Colleoni", "Ricardo Sanchez-Matilla", "Joshua Sutcliffe", "Vanessa Thompson", "Muhammad Asad", "Imanol Luengo", "Danail Stoyanov"], "title": "Zero-shot Monocular Metric Depth for Endoscopic Images", "comment": "Accepted at MICCAI 2025 DEMI Workshop", "summary": "Monocular relative and metric depth estimation has seen a tremendous boost in\nthe last few years due to the sharp advancements in foundation models and in\nparticular transformer based networks. As we start to see applications to the\ndomain of endoscopic images, there is still a lack of robust benchmarks and\nhigh-quality datasets in that area. This paper addresses these limitations by\npresenting a comprehensive benchmark of state-of-the-art (metric and relative)\ndepth estimation models evaluated on real, unseen endoscopic images, providing\ncritical insights into their generalisation and performance in clinical\nscenarios. Additionally, we introduce and publish a novel synthetic dataset\n(EndoSynth) of endoscopic surgical instruments paired with ground truth metric\ndepth and segmentation masks, designed to bridge the gap between synthetic and\nreal-world data. We demonstrate that fine-tuning depth foundation models using\nour synthetic dataset boosts accuracy on most unseen real data by a significant\nmargin. By providing both a benchmark and a synthetic dataset, this work\nadvances the field of depth estimation for endoscopic images and serves as an\nimportant resource for future research. Project page, EndoSynth dataset and\ntrained weights are available at https://github.com/TouchSurgery/EndoSynth.", "AI": {"tldr": "本文提出了一项针对内窥镜图像深度估计的综合基准测试，并发布了一个名为EndoSynth的合成数据集，旨在弥补真实数据稀缺的不足，并证明使用该数据集微调模型能显著提高性能。", "motivation": "尽管基础模型（特别是基于Transformer的网络）在单目相对和度量深度估计方面取得了巨大进展，但内窥镜图像领域仍然缺乏鲁棒的基准测试和高质量数据集。", "method": "1. 对最先进的（度量和相对）深度估计模型进行了综合基准测试，并在真实的、未见的内窥镜图像上进行评估。2. 引入并发布了一个新颖的合成数据集（EndoSynth），其中包含内窥镜手术器械，并配有真实度量深度和分割掩模。3. 使用EndoSynth数据集对深度基础模型进行微调。", "result": "通过使用EndoSynth合成数据集对深度基础模型进行微调，在大多数未见的真实数据上显著提高了准确性。", "conclusion": "这项工作通过提供基准测试和合成数据集，推动了内窥镜图像深度估计领域的发展，并为未来的研究提供了重要资源。"}}
{"id": "2509.19163", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19163", "abs": "https://arxiv.org/abs/2509.19163", "authors": ["Chantal Shaib", "Tuhin Chakrabarty", "Diego Garcia-Olano", "Byron C. Wallace"], "title": "Measuring AI \"Slop\" in Text", "comment": null, "summary": "AI \"slop\" is an increasingly popular term used to describe low-quality\nAI-generated text, but there is currently no agreed upon definition of this\nterm nor a means to measure its occurrence. In this work, we develop a taxonomy\nof \"slop\" through interviews with experts in NLP, writing, and philosophy, and\npropose a set of interpretable dimensions for its assessment in text. Through\nspan-level annotation, we find that binary \"slop\" judgments are (somewhat)\nsubjective, but such determinations nonetheless correlate with latent\ndimensions such as coherence and relevance. Our framework can be used to\nevaluate AI-generated text in both detection and binary preference tasks,\npotentially offering new insights into the linguistic and stylistic factors\nthat contribute to quality judgments.", "AI": {"tldr": "本文旨在定义并量化“AI糟粕”（AI slop），通过专家访谈构建分类法，并提出可解释的评估维度，发现“糟粕”判断虽有主观性但与连贯性和相关性相关，并提供评估AI文本的框架。", "motivation": "“AI糟粕”是一个日益流行的术语，用于描述低质量的AI生成文本，但目前尚无公认的定义或衡量其出现的方法。", "method": "通过对自然语言处理、写作和哲学专家的访谈，开发了“糟粕”的分类法；提出了评估文本中“糟粕”的一组可解释维度；进行了跨度级别的标注来分析二元“糟粕”判断的主观性及其与潜在维度（如连贯性和相关性）的关联。", "result": "研究发现，二元“糟粕”判断在一定程度上是主观的，但这些判断与连贯性和相关性等潜在维度存在关联。本文提出的框架可用于检测和二元偏好任务中评估AI生成文本。", "conclusion": "所提出的框架能为AI生成文本的质量判断提供新的见解，特别是关于语言和风格因素如何影响质量评估。这将有助于更好地理解和评估AI生成内容的质量。"}}
{"id": "2509.19261", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.19261", "abs": "https://arxiv.org/abs/2509.19261", "authors": ["Kuanqi Cai", "Chunfeng Wang", "Zeqi Li", "Haowen Yao", "Weinan Chen", "Luis Figueredo", "Aude Billard", "Arash Ajoudani"], "title": "Imitation-Guided Bimanual Planning for Stable Manipulation under Changing External Forces", "comment": null, "summary": "Robotic manipulation in dynamic environments often requires seamless\ntransitions between different grasp types to maintain stability and efficiency.\nHowever, achieving smooth and adaptive grasp transitions remains a challenge,\nparticularly when dealing with external forces and complex motion constraints.\nExisting grasp transition strategies often fail to account for varying external\nforces and do not optimize motion performance effectively. In this work, we\npropose an Imitation-Guided Bimanual Planning Framework that integrates\nefficient grasp transition strategies and motion performance optimization to\nenhance stability and dexterity in robotic manipulation. Our approach\nintroduces Strategies for Sampling Stable Intersections in Grasp Manifolds for\nseamless transitions between uni-manual and bi-manual grasps, reducing\ncomputational costs and regrasping inefficiencies. Additionally, a Hierarchical\nDual-Stage Motion Architecture combines an Imitation Learning-based Global Path\nGenerator with a Quadratic Programming-driven Local Planner to ensure real-time\nmotion feasibility, obstacle avoidance, and superior manipulability. The\nproposed method is evaluated through a series of force-intensive tasks,\ndemonstrating significant improvements in grasp transition efficiency and\nmotion performance. A video demonstrating our simulation results can be viewed\nat\n\\href{https://youtu.be/3DhbUsv4eDo}{\\textcolor{blue}{https://youtu.be/3DhbUsv4eDo}}.", "AI": {"tldr": "本文提出了一种模仿学习引导的双臂规划框架，通过稳定的抓取流形交集采样策略和分层双阶段运动架构，解决了动态环境中机器人抓取类型平滑过渡和运动性能优化的问题，显著提高了抓取过渡效率和运动性能。", "motivation": "在动态环境中，机器人操作需要不同抓取类型之间的无缝过渡以保持稳定性和效率。然而，实现平滑和自适应的抓取过渡仍然是一个挑战，尤其是在处理外部力和复杂运动约束时。现有的抓取过渡策略通常未能考虑变化的外部力，也未能有效地优化运动性能。", "method": "本文提出了一种模仿学习引导的双臂规划框架。该框架整合了高效的抓取过渡策略和运动性能优化。具体方法包括：1) 引入了“抓取流形中稳定交集采样策略”（SSSIGM），用于单手和双手抓取之间的无缝过渡，旨在降低计算成本和重新抓取效率低下问题。2) 提出了一种“分层双阶段运动架构”，该架构结合了基于模仿学习的全局路径生成器和由二次规划驱动的局部规划器，以确保实时运动可行性、避障和卓越的可操作性。", "result": "所提出的方法通过一系列力密集型任务进行评估，结果表明在抓取过渡效率和运动性能方面取得了显著改进。", "conclusion": "该框架通过集成高效的抓取过渡策略和运动性能优化，有效地增强了机器人操作的稳定性和灵活性。"}}
{"id": "2509.18683", "categories": ["cs.CV", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.18683", "abs": "https://arxiv.org/abs/2509.18683", "authors": ["Lanhu Wu", "Zilin Gao", "Hao Fei", "Mong-Li Lee", "Wynne Hsu"], "title": "LEAF-Mamba: Local Emphatic and Adaptive Fusion State Space Model for RGB-D Salient Object Detection", "comment": "Accepted to ACM MM 2025", "summary": "RGB-D salient object detection (SOD) aims to identify the most conspicuous\nobjects in a scene with the incorporation of depth cues. Existing methods\nmainly rely on CNNs, limited by the local receptive fields, or Vision\nTransformers that suffer from the cost of quadratic complexity, posing a\nchallenge in balancing performance and computational efficiency. Recently,\nstate space models (SSM), Mamba, have shown great potential for modeling\nlong-range dependency with linear complexity. However, directly applying SSM to\nRGB-D SOD may lead to deficient local semantics as well as the inadequate\ncross-modality fusion. To address these issues, we propose a Local Emphatic and\nAdaptive Fusion state space model (LEAF-Mamba) that contains two novel\ncomponents: 1) a local emphatic state space module (LE-SSM) to capture\nmulti-scale local dependencies for both modalities. 2) an SSM-based adaptive\nfusion module (AFM) for complementary cross-modality interaction and reliable\ncross-modality integration. Extensive experiments demonstrate that the\nLEAF-Mamba consistently outperforms 16 state-of-the-art RGB-D SOD methods in\nboth efficacy and efficiency. Moreover, our method can achieve excellent\nperformance on the RGB-T SOD task, proving a powerful generalization ability.", "AI": {"tldr": "针对RGB-D显著目标检测中现有方法性能与效率的矛盾，本文提出LEAF-Mamba模型，通过局部强调状态空间模块和基于SSM的自适应融合模块，有效解决了局部语义和跨模态融合问题，实现了卓越的性能和计算效率。", "motivation": "现有的RGB-D显著目标检测（SOD）方法主要依赖CNN（受限于局部感受野）或Vision Transformers（计算复杂度高），难以平衡性能和效率。虽然Mamba等状态空间模型（SSM）在建模长距离依赖方面具有线性复杂度优势，但直接应用于RGB-D SOD可能导致局部语义不足和跨模态融合不充分。", "method": "本文提出了局部强调和自适应融合状态空间模型（LEAF-Mamba），包含两个核心组件：1) 局部强调状态空间模块（LE-SSM），用于捕获多尺度局部依赖；2) 基于SSM的自适应融合模块（AFM），用于实现互补的跨模态交互和可靠的跨模态集成。", "result": "实验结果表明，LEAF-Mamba在有效性和效率方面均持续优于16种最先进的RGB-D SOD方法。此外，该方法在RGB-T SOD任务上也表现出色，证明了其强大的泛化能力。", "conclusion": "LEAF-Mamba通过创新的LE-SSM和AFM模块，成功将状态空间模型应用于RGB-D SOD任务，解决了传统方法的局限性，实现了性能与计算效率的良好平衡，并展现了出色的泛化能力。"}}
{"id": "2509.19170", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19170", "abs": "https://arxiv.org/abs/2509.19170", "authors": ["Natasha Butt", "Ariel Kwiatkowski", "Ismail Labiad", "Julia Kempe", "Yann Ollivier"], "title": "Soft Tokens, Hard Truths", "comment": null, "summary": "The use of continuous instead of discrete tokens during the Chain-of-Thought\n(CoT) phase of reasoning LLMs has garnered attention recently, based on the\nintuition that a continuous mixture of discrete tokens could simulate a\nsuperposition of several reasoning paths simultaneously. Theoretical results\nhave formally proven that continuous tokens have much greater expressivity and\ncan solve specific problems more efficiently. However, practical use of\ncontinuous tokens has been limited by strong training difficulties: previous\nworks either just use continuous tokens at inference time on a pre-trained\ndiscrete-token model, or must distill the continuous CoT from ground-truth\ndiscrete CoTs and face computational costs that limit the CoT to very few\ntokens.\n  This is the first work introducing a scalable method to learn continuous CoTs\nvia reinforcement learning (RL), without distilling from reference discrete\nCoTs. We use \"soft\" tokens: mixtures of tokens together with noise on the input\nembedding to provide RL exploration. Computational overhead is minimal,\nenabling us to learn continuous CoTs with hundreds of tokens. On math reasoning\nbenchmarks with Llama and Qwen models up to 8B, training with continuous CoTs\nmatch discrete-token CoTs for pass@1 and surpass them for pass@32, showing\ngreater CoT diversity. In systematic comparisons, the best-performing scenario\nis to train with continuous CoT tokens then use discrete tokens for inference,\nmeaning the \"soft\" models can be deployed in a standard way. Finally, we show\ncontinuous CoT RL training better preserves the predictions of the base model\non out-of-domain tasks, thus providing a softer touch to the base model.", "AI": {"tldr": "本文提出了一种可扩展的强化学习方法，用于训练大语言模型（LLMs）的连续思维链（CoT），解决了现有方法在训练难度和计算成本上的限制。该方法在数学推理任务上表现出色，提升了CoT多样性，并能更好地保持基础模型的泛化能力。", "motivation": "连续令牌在理论上已被证明比离散令牌具有更强的表达能力，能更有效地解决问题，并能模拟多种推理路径的叠加。然而，现有方法在实际应用中面临严重的训练困难，如只能在推理时使用、需要从离散CoT中蒸馏且计算成本高昂，限制了连续CoT的长度。", "method": "本文首次提出一种可扩展的强化学习（RL）方法来学习连续CoT，而无需从参考离散CoT中蒸馏。该方法使用“软”令牌（令牌混合物加上输入嵌入的噪声）来促进RL探索。计算开销极小，能够学习包含数百个令牌的连续CoT。", "result": "在Llama和Qwen（高达8B参数）的数学推理基准测试中，使用连续CoT训练的模型在pass@1上与离散令牌CoT相当，在pass@32上则超越离散CoT，显示出更强的CoT多样性。最佳方案是使用连续CoT令牌进行训练，然后使用离散令牌进行推理，实现标准部署。此外，连续CoT的RL训练能更好地保留基础模型在域外任务上的预测能力。", "conclusion": "本文提出的基于强化学习的连续CoT训练方法是可扩展的，克服了以往训练连续CoT的难题。它显著提升了CoT的多样性，特别是在高通过率下，并能以标准方式部署。同时，该方法在不损害基础模型在域外任务表现的前提下，为模型训练提供了更“温和”的调整方式。"}}
{"id": "2509.19292", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19292", "abs": "https://arxiv.org/abs/2509.19292", "authors": ["Yang Jin", "Jun Lv", "Han Xue", "Wendi Chen", "Chuan Wen", "Cewu Lu"], "title": "SOE: Sample-Efficient Robot Policy Self-Improvement via On-Manifold Exploration", "comment": null, "summary": "Intelligent agents progress by continually refining their capabilities\nthrough actively exploring environments. Yet robot policies often lack\nsufficient exploration capability due to action mode collapse. Existing methods\nthat encourage exploration typically rely on random perturbations, which are\nunsafe and induce unstable, erratic behaviors, thereby limiting their\neffectiveness. We propose Self-Improvement via On-Manifold Exploration (SOE), a\nframework that enhances policy exploration and improvement in robotic\nmanipulation. SOE learns a compact latent representation of task-relevant\nfactors and constrains exploration to the manifold of valid actions, ensuring\nsafety, diversity, and effectiveness. It can be seamlessly integrated with\narbitrary policy models as a plug-in module, augmenting exploration without\ndegrading the base policy performance. Moreover, the structured latent space\nenables human-guided exploration, further improving efficiency and\ncontrollability. Extensive experiments in both simulation and real-world tasks\ndemonstrate that SOE consistently outperforms prior methods, achieving higher\ntask success rates, smoother and safer exploration, and superior sample\nefficiency. These results establish on-manifold exploration as a principled\napproach to sample-efficient policy self-improvement. Project website:\nhttps://ericjin2002.github.io/SOE", "AI": {"tldr": "本文提出了一种名为SOE（Self-Improvement via On-Manifold Exploration）的框架，通过学习任务相关因素的紧凑潜在表示并将探索限制在有效动作流形上，显著提升了机器人策略的探索能力、安全性和样本效率。", "motivation": "智能体通过探索环境来改进其能力，但机器人策略常因动作模式崩溃而缺乏足够的探索能力。现有鼓励探索的方法通常依赖随机扰动，这既不安全又会导致不稳定和不稳定的行为，从而限制了它们的有效性。", "method": "SOE框架通过学习任务相关因素的紧凑潜在表示，并将探索限制在有效动作的流形上，以确保安全性、多样性和有效性。它可作为即插即用模块与任意策略模型无缝集成，在不降低基础策略性能的情况下增强探索。此外，结构化的潜在空间还支持人机引导探索，进一步提高了效率和可控性。", "result": "在模拟和真实世界任务中的大量实验表明，SOE始终优于现有方法，实现了更高的任务成功率、更平稳和安全的探索以及卓越的样本效率。", "conclusion": "这些结果确立了基于流形的探索作为一种实现样本高效策略自我改进的原则性方法。"}}
{"id": "2509.18692", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18692", "abs": "https://arxiv.org/abs/2509.18692", "authors": ["Xinle Gao", "Linghui Ye", "Zhiyong Xiao"], "title": "Lightweight Vision Transformer with Window and Spatial Attention for Food Image Classification", "comment": null, "summary": "With the rapid development of society and continuous advances in science and\ntechnology, the food industry increasingly demands higher production quality\nand efficiency. Food image classification plays a vital role in enabling\nautomated quality control on production lines, supporting food safety\nsupervision, and promoting intelligent agricultural production. However, this\ntask faces challenges due to the large number of parameters and high\ncomputational complexity of Vision Transformer models. To address these issues,\nwe propose a lightweight food image classification algorithm that integrates a\nWindow Multi-Head Attention Mechanism (WMHAM) and a Spatial Attention Mechanism\n(SAM). The WMHAM reduces computational cost by capturing local and global\ncontextual features through efficient window partitioning, while the SAM\nadaptively emphasizes key spatial regions to improve discriminative feature\nrepresentation. Experiments conducted on the Food-101 and Vireo Food-172\ndatasets demonstrate that our model achieves accuracies of 95.24% and 94.33%,\nrespectively, while significantly reducing parameters and FLOPs compared with\nbaseline methods. These results confirm that the proposed approach achieves an\neffective balance between computational efficiency and classification\nperformance, making it well-suited for deployment in resource-constrained\nenvironments.", "AI": {"tldr": "本文提出了一种轻量级食物图像分类算法，结合了窗口多头注意力机制（WMHAM）和空间注意力机制（SAM），在保持高精度的同时显著降低了计算复杂度和参数量，适用于资源受限环境。", "motivation": "随着社会和科技发展，食品工业对生产质量和效率要求提高，食物图像分类在自动化质检、食品安全监管和智能农业中至关重要。然而，现有的Vision Transformer模型参数多、计算复杂，难以满足实际应用需求。", "method": "提出了一种轻量级食物图像分类算法，集成了窗口多头注意力机制（WMHAM）和空间注意力机制（SAM）。WMHAM通过高效的窗口划分捕捉局部和全局上下文特征以降低计算成本；SAM自适应地强调关键空间区域以增强判别性特征表示。", "result": "在Food-101和Vireo Food-172数据集上，模型分别达到了95.24%和94.33%的准确率，与基线方法相比，显著减少了参数量和FLOPs。", "conclusion": "所提出的方法在计算效率和分类性能之间取得了有效平衡，非常适合部署在资源受限的环境中。"}}
{"id": "2509.19199", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19199", "abs": "https://arxiv.org/abs/2509.19199", "authors": ["Xiaoqian Liu", "Ke Wang", "Yuchuan Wu", "Fei Huang", "Yongbin Li", "Junge Zhang", "Jianbin Jiao"], "title": "Online Process Reward Leanring for Agentic Reinforcement Learning", "comment": "preprint", "summary": "Large language models (LLMs) are increasingly trained with reinforcement\nlearning (RL) as autonomous agents that reason and act over long horizons in\ninteractive environments.\n  However, sparse and sometimes unverifiable rewards make temporal credit\nassignment extremely challenging.\n  Recent work attempts to integrate process supervision into agent learning but\nsuffers from biased annotation, reward hacking, high-variance from overly\nfine-grained signals or failtures when state overlap is rare.\n  We therefore introduce Online Process Reward Learning (OPRL), a general\ncredit-assignment strategy for agentic RL that integrates seamlessly with\nstandard on-policy algorithms without relying on additional rollouts or\nexplicit step labels.\n  In OPRL, we optimize an implicit process reward model (PRM) alternately with\nthe agent's policy to transform trajectory preferences into implicit step\nrewards through a trajectory-based DPO objective.\n  These step rewards are then used to compute step-level advantages, which are\ncombined with episode-level advantages from outcome rewards for policy update,\ncreating a self-reinforcing loop.\n  Theoretical findings guarantee that the learned step rewards are consistent\nwith trajectory preferences and act as potential-based shaping rewards,\nproviding bounded gradients to stabilize training.\n  Empirically, we evaluate OPRL on three distinct agent benmarks, including\nWebShop and VisualSokoban, as well as open-ended social interactions with\nunverfiable rewards in SOTOPIA.\n  Crucially, OPRL shows superior performance over frontier LLMs and strong RL\nbaselines across domains, achieving state-of-the-art results with higher\nsample-efficiency and lower variance during training.\n  Further analysis also demonstrates the efficient exploration by OPRL using\nfewer actions, underscoring its potential for agentic learning in real-world\nscenarios.", "AI": {"tldr": "本文提出了一种名为在线过程奖励学习（OPRL）的新型信用分配策略，用于提升基于强化学习的大型语言模型（LLM）代理的性能，通过将轨迹偏好转化为隐式步骤奖励，解决了稀疏奖励和信用分配的挑战。", "motivation": "将LLM训练为自主代理时，面临稀疏且有时难以验证的奖励导致的信用分配难题。现有集成过程监督的方法存在标注偏差、奖励劫持、信号过细导致的高方差或状态重叠罕见时的失败等问题。", "method": "OPRL与标准在线策略算法无缝集成，无需额外回放或显式步骤标签。它通过基于轨迹的DPO目标，交替优化隐式过程奖励模型（PRM）和代理策略，将轨迹偏好转换为隐式步骤奖励。这些步骤奖励与回合级结果奖励结合，用于策略更新，形成一个自我强化的循环。理论上，学习到的步骤奖励与轨迹偏好一致，并作为基于势能的塑造奖励，提供有界梯度以稳定训练。", "result": "OPRL在WebShop、VisualSokoban以及SOTOPIA等三个不同的代理基准测试中，表现优于前沿LLM和强大的RL基线。它实现了最先进的性能，具有更高的样本效率和更低的训练方差。进一步分析表明，OPRL使用更少的动作进行高效探索。", "conclusion": "OPRL是一种通用且有效的代理RL信用分配策略，能够显著提升LLM代理在交互环境中的性能，具有更高的样本效率和更稳定的训练过程，在现实世界场景中的代理学习方面展现出巨大潜力。"}}
{"id": "2509.19301", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19301", "abs": "https://arxiv.org/abs/2509.19301", "authors": ["Lars Ankile", "Zhenyu Jiang", "Rocky Duan", "Guanya Shi", "Pieter Abbeel", "Anusha Nagabandi"], "title": "Residual Off-Policy RL for Finetuning Behavior Cloning Policies", "comment": null, "summary": "Recent advances in behavior cloning (BC) have enabled impressive visuomotor\ncontrol policies. However, these approaches are limited by the quality of human\ndemonstrations, the manual effort required for data collection, and the\ndiminishing returns from increasing offline data. In comparison, reinforcement\nlearning (RL) trains an agent through autonomous interaction with the\nenvironment and has shown remarkable success in various domains. Still,\ntraining RL policies directly on real-world robots remains challenging due to\nsample inefficiency, safety concerns, and the difficulty of learning from\nsparse rewards for long-horizon tasks, especially for high-degree-of-freedom\n(DoF) systems. We present a recipe that combines the benefits of BC and RL\nthrough a residual learning framework. Our approach leverages BC policies as\nblack-box bases and learns lightweight per-step residual corrections via\nsample-efficient off-policy RL. We demonstrate that our method requires only\nsparse binary reward signals and can effectively improve manipulation policies\non high-degree-of-freedom (DoF) systems in both simulation and the real world.\nIn particular, we demonstrate, to the best of our knowledge, the first\nsuccessful real-world RL training on a humanoid robot with dexterous hands. Our\nresults demonstrate state-of-the-art performance in various vision-based tasks,\npointing towards a practical pathway for deploying RL in the real world.\nProject website: https://residual-offpolicy-rl.github.io", "AI": {"tldr": "该研究提出了一种结合行为克隆（BC）和强化学习（RL）的残差学习框架，以克服各自的局限性，并首次在具有灵巧手的类人机器人上实现了成功的真实世界RL训练。", "motivation": "行为克隆（BC）受限于演示数据质量、收集成本和数据增益递减。强化学习（RL）在真实世界机器人上训练面临样本效率低下、安全问题、稀疏奖励学习困难以及高自由度（DoF）系统挑战。", "method": "该方法通过残差学习框架结合了BC和RL。它利用BC策略作为黑盒基础，并通过样本高效的离策略RL学习轻量级的每步残差校正。", "result": "该方法仅需稀疏的二元奖励信号，就能有效改进仿真和真实世界中高自由度系统的操作策略。它首次在具有灵巧手的类人机器人上实现了成功的真实世界RL训练，并在各种基于视觉的任务中展示了最先进的性能。", "conclusion": "该研究提供了一个在真实世界中部署RL的实用途径，显著提高了高自由度机器人（包括类人机器人）在复杂操作任务中的学习和控制能力。"}}
{"id": "2509.18711", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18711", "abs": "https://arxiv.org/abs/2509.18711", "authors": ["Ke Li", "Di Wang", "Ting Wang", "Fuyu Dong", "Yiming Zhang", "Luyao Zhang", "Xiangyu Wang", "Shaofeng Li", "Quan Wang"], "title": "RSVG-ZeroOV: Exploring a Training-Free Framework for Zero-Shot Open-Vocabulary Visual Grounding in Remote Sensing Images", "comment": null, "summary": "Remote sensing visual grounding (RSVG) aims to localize objects in remote\nsensing images based on free-form natural language expressions. Existing\napproaches are typically constrained to closed-set vocabularies, limiting their\napplicability in open-world scenarios. While recent attempts to leverage\ngeneric foundation models for open-vocabulary RSVG, they overly rely on\nexpensive high-quality datasets and time-consuming fine-tuning. To address\nthese limitations, we propose \\textbf{RSVG-ZeroOV}, a training-free framework\nthat aims to explore the potential of frozen generic foundation models for\nzero-shot open-vocabulary RSVG. Specifically, RSVG-ZeroOV comprises three key\nstages: (i) Overview: We utilize a vision-language model (VLM) to obtain\ncross-attention\\footnote[1]{In this paper, although decoder-only VLMs use\nself-attention over all tokens, we refer to the image-text interaction part as\ncross-attention to distinguish it from pure visual self-attention.}maps that\ncapture semantic correlations between text queries and visual regions. (ii)\nFocus: By leveraging the fine-grained modeling priors of a diffusion model\n(DM), we fill in gaps in structural and shape information of objects, which are\noften overlooked by VLM. (iii) Evolve: A simple yet effective attention\nevolution module is introduced to suppress irrelevant activations, yielding\npurified segmentation masks over the referred objects. Without cumbersome\ntask-specific training, RSVG-ZeroOV offers an efficient and scalable solution.\nExtensive experiments demonstrate that the proposed framework consistently\noutperforms existing weakly-supervised and zero-shot methods.", "AI": {"tldr": "本文提出了RSVG-ZeroOV，一个无需训练的框架，利用冻结的通用基础模型实现零样本开放词汇遥感视觉定位（RSVG），通过结合视觉-语言模型（VLM）和扩散模型（DM）的优势，生成纯净的分割掩码。", "motivation": "现有遥感视觉定位方法受限于封闭词汇，难以应用于开放世界场景；而利用通用基础模型的方法则过度依赖昂贵的高质量数据集和耗时的微调。为了解决这些限制，研究者旨在探索冻结通用基础模型在零样本开放词汇RSVG中的潜力。", "method": "本文提出了RSVG-ZeroOV框架，无需训练，包含三个阶段：(i) 概述（Overview）：利用视觉-语言模型（VLM）获取捕获文本查询与视觉区域语义关联的交叉注意力图。(ii) 聚焦（Focus）：利用扩散模型（DM）的细粒度建模先验，补充VLM常忽略的物体结构和形状信息。(iii) 演化（Evolve）：引入一个简单有效的注意力演化模块，抑制不相关激活，生成纯净的指代对象分割掩码。", "result": "广泛的实验表明，所提出的RSVG-ZeroOV框架持续优于现有的弱监督和零样本方法。", "conclusion": "RSVG-ZeroOV提供了一种高效且可扩展的解决方案，无需繁琐的任务特定训练，即可实现零样本开放词汇遥感视觉定位，充分利用了冻结通用基础模型的潜力。"}}
{"id": "2509.18693", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18693", "abs": "https://arxiv.org/abs/2509.18693", "authors": ["Siyi Chen", "Kai Wang", "Weicong Pang", "Ruiming Yang", "Ziru Chen", "Renjun Gao", "Alexis Kai Hon Lau", "Dasa Gu", "Chenchen Zhang", "Cheng Li"], "title": "OSDA: A Framework for Open-Set Discovery and Automatic Interpretation of Land-cover in Remote Sensing Imagery", "comment": "Project is available at\n  https://anonymous.4open.science/r/openset_remotesensing_tagging-2B5F/README.md", "summary": "Open-set land-cover analysis in remote sensing requires the ability to\nachieve fine-grained spatial localization and semantically open categorization.\nThis involves not only detecting and segmenting novel objects without\ncategorical supervision but also assigning them interpretable semantic labels\nthrough multimodal reasoning. In this study, we introduce OSDA, an integrated\nthree-stage framework for annotation-free open-set land-cover discovery,\nsegmentation, and description. The pipeline consists of: (1) precise discovery\nand mask extraction with a promptable fine-tuned segmentation model (SAM), (2)\nsemantic attribution and contextual description via a two-phase fine-tuned\nmultimodal large language model (MLLM), and (3) LLM-as-judge and manual scoring\nof the MLLMs evaluation. By combining pixel-level accuracy with high-level\nsemantic understanding, OSDA addresses key challenges in open-world remote\nsensing interpretation. Designed to be architecture-agnostic and label-free,\nthe framework supports robust evaluation across diverse satellite imagery\nwithout requiring manual annotation. Our work provides a scalable and\ninterpretable solution for dynamic land-cover monitoring, showing strong\npotential for automated cartographic updating and large-scale earth observation\nanalysis.", "AI": {"tldr": "本研究提出OSDA框架，利用微调分割模型（SAM）和多模态大语言模型（MLLM）实现遥感影像中开放集地物发现、分割和描述，无需人工标注，结合像素级精度与高级语义理解。", "motivation": "遥感领域的开放集地物分析需要精细的空间定位和开放的语义分类能力，包括在无类别监督的情况下检测和分割新物体，并通过多模态推理赋予其可解释的语义标签。", "method": "OSDA是一个三阶段的无标注开放集地物发现、分割和描述框架：1) 使用可提示的微调分割模型（SAM）进行精确发现和掩膜提取；2) 通过两阶段微调的多模态大语言模型（MLLM）进行语义归属和上下文描述；3) 利用LLM作为评估者并进行人工评分来评估MLLM。", "result": "OSDA将像素级精度与高级语义理解相结合，解决了开放世界遥感解释中的关键挑战。该框架与架构无关、无需标签，支持在多样化卫星图像上进行鲁棒评估，无需人工标注。", "conclusion": "本工作为动态地物监测提供了一个可扩展且可解释的解决方案，在自动化制图更新和大规模地球观测分析方面显示出巨大潜力。"}}
{"id": "2509.19212", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19212", "abs": "https://arxiv.org/abs/2509.19212", "authors": ["Zheyuan Liu", "Zhangchen Xu", "Guangyao Dou", "Xiangchi Yuan", "Zhaoxuan Tan", "Radha Poovendran", "Meng Jiang"], "title": "Steering Multimodal Large Language Models Decoding for Context-Aware Safety", "comment": "A lightweight and model-agnostic decoding framework that dynamically\n  adjusts token generation based on multimodal context", "summary": "Multimodal Large Language Models (MLLMs) are increasingly deployed in\nreal-world applications, yet their ability to make context-aware safety\ndecisions remains limited. Existing methods often fail to balance\noversensitivity (unjustified refusals of benign queries) and undersensitivity\n(missed detection of visually grounded risks), leaving a persistent gap in\nsafety alignment. To address this issue, we introduce Safety-aware Contrastive\nDecoding (SafeCoDe), a lightweight and model-agnostic decoding framework that\ndynamically adjusts token generation based on multimodal context. SafeCoDe\noperates in two stages: (1) a contrastive decoding mechanism that highlights\ntokens sensitive to visual context by contrasting real and Gaussian-noised\nimages, and (2) a global-aware token modulation strategy that integrates\nscene-level reasoning with token-level adjustment to adapt refusals according\nto the predicted safety verdict. Extensive experiments across diverse MLLM\narchitectures and safety benchmarks, covering undersensitivity,\noversensitivity, and general safety evaluations, show that SafeCoDe\nconsistently improves context-sensitive refusal behaviors while preserving\nmodel helpfulness.", "AI": {"tldr": "本文提出SafeCoDe，一个轻量级、模型无关的解码框架，通过对比解码和全局感知令牌调制，动态调整多模态大语言模型（MLLMs）的令牌生成，以提高上下文感知的安全决策，平衡过敏性和低敏性，同时保持模型有用性。", "motivation": "多模态大语言模型（MLLMs）在实际应用中部署日益增多，但其进行上下文感知安全决策的能力有限。现有方法难以平衡过敏性（不合理地拒绝良性查询）和低敏性（未能检测到视觉相关的风险），导致安全对齐存在持续差距。", "method": "本文引入Safety-aware Contrastive Decoding (SafeCoDe)，一个轻量级且与模型无关的解码框架，分两阶段动态调整令牌生成：1) 对比解码机制，通过对比真实图像和高斯噪声图像来突出对视觉上下文敏感的令牌；2) 全局感知令牌调制策略，将场景级推理与令牌级调整相结合，根据预测的安全判断来调整拒绝行为。", "result": "在多种MLLM架构和安全基准（涵盖低敏性、过敏性和一般安全评估）上进行的广泛实验表明，SafeCoDe持续改进了上下文敏感的拒绝行为，同时保持了模型的有用性。", "conclusion": "SafeCoDe通过动态调整令牌生成，有效提高了多模态大语言模型（MLLMs）的上下文感知安全决策能力，成功平衡了过敏性和低敏性，同时不牺牲模型的实用性。"}}
{"id": "2509.18754", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18754", "abs": "https://arxiv.org/abs/2509.18754", "authors": ["Yuyang Liu", "Xinyuan Shi", "Bang Yang", "Peilin Zhou", "Jiahua Dong", "Long Chen", "Ian Reid", "Xiaondan Liang"], "title": "COLT: Enhancing Video Large Language Models with Continual Tool Usage", "comment": "16 pages", "summary": "The success of Large Language Models (LLMs) has significantly propelled the\nresearch of video understanding. To harvest the benefits of well-trained expert\nmodels (i.e., tools), video LLMs prioritize the exploration of tool usage\ncapabilities. Existing methods either prompt closed-source LLMs or employ the\ninstruction tuning paradigm for tool-use fine-tuning. These methods, however,\nassume an established repository of fixed tools and struggle to generalize to\nreal-world environments where tool data is perpetually evolving and streaming\nin. To this end, we propose to enhance open-source video LLMs with COntinuaL\nTool usage (termed COLT), which automatically acquires tool-use ability in a\nsuccessive tool stream without suffering 'catastrophic forgetting' of the past\nlearned tools. Specifically, our COLT incorporates a learnable tool codebook as\na tool-specific memory system. Then relevant tools are dynamically selected\nbased on the similarity between user instruction and tool features within the\ncodebook. To unleash the tool usage potential of video LLMs, we collect a\nvideo-centric tool-use instruction tuning dataset VideoToolBench. Extensive\nexperiments on both previous video LLM benchmarks and the tool-use-specific\nVideoToolBench dataset demonstrate the state-of-the-art performance of our\nproposed COLT.", "AI": {"tldr": "本文提出COLT，一种增强开源视频LLM连续工具使用能力的方法，通过可学习的工具码本和动态选择机制，解决了在工具数据不断演进时“灾难性遗忘”的问题，并实现了最先进的性能。", "motivation": "现有视频LLM的工具使用方法（提示闭源LLM或指令微调）假设工具库是固定的，难以适应工具数据不断演进和流式更新的真实世界环境，且面临对过去学习工具的“灾难性遗忘”问题。", "method": "本文提出了COLT（COntinuaL Tool usage）框架，通过引入一个可学习的工具码本作为工具特定的记忆系统。该系统根据用户指令与码本中工具特征的相似性动态选择相关工具。此外，作者还收集了一个以视频为中心的工具使用指令微调数据集VideoToolBench。", "result": "在先前的视频LLM基准测试和VideoToolBench数据集上的大量实验表明，COLT取得了最先进的性能。", "conclusion": "COLT成功地使开源视频LLM能够在不遗忘过去学习工具的情况下，连续地获取和使用新工具，从而显著提升了其在动态环境下的泛化能力和工具使用潜力。"}}
{"id": "2509.18697", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18697", "abs": "https://arxiv.org/abs/2509.18697", "authors": ["Herve Goeau", "Pierre Bonnet", "Alexis Joly"], "title": "Overview of PlantCLEF 2021: cross-domain plant identification", "comment": "15 pages, 6 figures, CLEF 2021 Conference and Labs of the Evaluation\n  Forum, September 21 to 24, 2021, Bucharest, Romania", "summary": "Automated plant identification has improved considerably thanks to recent\nadvances in deep learning and the availability of training data with more and\nmore field photos. However, this profusion of data concerns only a few tens of\nthousands of species, mainly located in North America and Western Europe, much\nless in the richest regions in terms of biodiversity such as tropical\ncountries. On the other hand, for several centuries, botanists have\nsystematically collected, catalogued and stored plant specimens in herbaria,\nespecially in tropical regions, and recent efforts by the biodiversity\ninformatics community have made it possible to put millions of digitised\nrecords online. The LifeCLEF 2021 plant identification challenge (or \"PlantCLEF\n2021\") was designed to assess the extent to which automated identification of\nflora in data-poor regions can be improved by using herbarium collections. It\nis based on a dataset of about 1,000 species mainly focused on the Guiana\nShield of South America, a region known to have one of the highest plant\ndiversities in the world. The challenge was evaluated as a cross-domain\nclassification task where the training set consisted of several hundred\nthousand herbarium sheets and a few thousand photos to allow learning a\ncorrespondence between the two domains. In addition to the usual metadata\n(location, date, author, taxonomy), the training data also includes the values\nof 5 morphological and functional traits for each species. The test set\nconsisted exclusively of photos taken in the field. This article presents the\nresources and evaluations of the assessment carried out, summarises the\napproaches and systems used by the participating research groups and provides\nan analysis of the main results.", "AI": {"tldr": "本文介绍了LifeCLEF 2021植物识别挑战，该挑战旨在通过利用植物标本馆数据，改善生物多样性丰富但数据匮乏地区（如热带国家）的自动化植物识别能力。", "motivation": "当前的深度学习植物识别技术主要依赖于北美和西欧等地区的数据，对热带国家等生物多样性丰富但缺乏野外照片数据的地区效果不佳。然而，这些地区拥有大量的数字化植物标本馆数据，可作为潜在的补充资源。", "method": "LifeCLEF 2021挑战被设计为一个跨领域分类任务，目标是识别约1,000种圭亚那地盾的植物。训练集包含数十万份植物标本馆图像和数千张野外照片，以及元数据和5个形态功能性状。测试集仅包含野外照片。挑战旨在评估通过学习标本馆和野外照片之间的对应关系，来提升识别效果。", "result": "本文介绍了该评估的资源和评价方法，总结了参与研究团队所采用的方法和系统，并对主要结果进行了分析。", "conclusion": "该挑战旨在评估利用植物标本馆藏品，能在多大程度上改善数据匮乏地区植物群的自动化识别。本文将呈现此次评估的发现和主要成果。"}}
{"id": "2509.19224", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19224", "abs": "https://arxiv.org/abs/2509.19224", "authors": ["Tariq Abdul-Quddoos", "Xishuang Dong", "Lijun Qian"], "title": "Systematic Comparative Analysis of Large Pretrained Language Models on Contextualized Medication Event Extraction", "comment": null, "summary": "Attention-based models have become the leading approach in modeling medical\nlanguage for Natural Language Processing (NLP) in clinical notes. These models\noutperform traditional techniques by effectively capturing contextual rep-\nresentations of language. In this research a comparative analysis is done\namongst pre- trained attention based models namely Bert Base, BioBert, two\nvariations of Bio+Clinical Bert, RoBerta, and Clinical Long- former on task\nrelated to Electronic Health Record (EHR) information extraction. The tasks\nfrom Track 1 of Harvard Medical School's 2022 National Clinical NLP Challenges\n(n2c2) are considered for this comparison, with the Contextualized Medication\nEvent Dataset (CMED) given for these task. CMED is a dataset of unstructured\nEHRs and annotated notes that contain task relevant information about the EHRs.\nThe goal of the challenge is to develop effective solutions for extracting\ncontextual information related to patient medication events from EHRs using\ndata driven methods. Each pre-trained model is fine-tuned and applied on CMED\nto perform medication extraction, medical event detection, and\nmulti-dimensional medication event context classification. Pro- cessing methods\nare also detailed for breaking down EHRs for compatibility with the applied\nmodels. Performance analysis has been carried out using a script based on\nconstructing medical terms from the evaluation portion of CMED with metrics\nincluding recall, precision, and F1-Score. The results demonstrate that models\npre-trained on clinical data are more effective in detecting medication and\nmedication events, but Bert Base, pre- trained on general domain data showed to\nbe the most effective for classifying the context of events related to\nmedications.", "AI": {"tldr": "本文对多种预训练注意力模型（如Bert Base, BioBert, RoBerta等）在电子健康记录（EHR）信息抽取任务上进行了比较分析，特别是针对药物事件的检测和上下文分类，发现临床数据预训练的模型在检测方面更有效，而通用领域预训练的Bert Base在上下文分类方面表现最佳。", "motivation": "注意力模型已成为医学语言处理（NLP）领域的主流方法，并在临床笔记中表现出色。研究旨在通过比较分析不同的预训练注意力模型，为电子健康记录（EHR）中的药物事件信息抽取任务开发有效的数据驱动解决方案，以应对哈佛医学院2022年国家临床NLP挑战（n2c2）的Track 1任务。", "method": "研究选取了Bert Base、BioBert、两种Bio+Clinical Bert变体、RoBerta和Clinical Longformer等预训练注意力模型进行比较。这些模型在n2c2挑战提供的上下文药物事件数据集（CMED）上进行微调，以执行药物提取、医学事件检测和多维药物事件上下文分类任务。同时，详细说明了EHR数据处理方法以适应模型。性能评估使用基于CMED评估部分的脚本，通过召回率、精确率和F1-分数等指标构建医学术语进行分析。", "result": "结果表明，在临床数据上预训练的模型在检测药物和药物事件方面更为有效。然而，在通用领域数据上预训练的Bert Base模型在分类与药物相关的事件上下文方面表现出最佳效果。", "conclusion": "研究得出结论，模型预训练的数据领域对其在医学NLP不同子任务中的表现有显著影响：临床数据预训练的模型更擅长检测任务，而通用数据预训练的模型（如Bert Base）在上下文分类任务上可能更具优势。"}}
{"id": "2509.18765", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18765", "abs": "https://arxiv.org/abs/2509.18765", "authors": ["Azad Singh", "Deepak Mishra"], "title": "DiSSECT: Structuring Transfer-Ready Medical Image Representations through Discrete Self-Supervision", "comment": null, "summary": "Self-supervised learning (SSL) has emerged as a powerful paradigm for medical\nimage representation learning, particularly in settings with limited labeled\ndata. However, existing SSL methods often rely on complex architectures,\nanatomy-specific priors, or heavily tuned augmentations, which limit their\nscalability and generalizability. More critically, these models are prone to\nshortcut learning, especially in modalities like chest X-rays, where anatomical\nsimilarity is high and pathology is subtle. In this work, we introduce DiSSECT\n-- Discrete Self-Supervision for Efficient Clinical Transferable\nRepresentations, a framework that integrates multi-scale vector quantization\ninto the SSL pipeline to impose a discrete representational bottleneck. This\nconstrains the model to learn repeatable, structure-aware features while\nsuppressing view-specific or low-utility patterns, improving representation\ntransfer across tasks and domains. DiSSECT achieves strong performance on both\nclassification and segmentation tasks, requiring minimal or no fine-tuning, and\nshows particularly high label efficiency in low-label regimes. We validate\nDiSSECT across multiple public medical imaging datasets, demonstrating its\nrobustness and generalizability compared to existing state-of-the-art\napproaches.", "AI": {"tldr": "DiSSECT是一种新的自监督学习框架，通过多尺度向量量化引入离散表示瓶颈，解决了现有医疗图像SSL方法的复杂性、捷径学习和泛化性问题，实现了高效、可迁移的临床表征学习。", "motivation": "现有医疗图像自监督学习（SSL）方法存在以下问题：依赖复杂架构、解剖学先验或过度调优的数据增强，限制了可扩展性和泛化性；更关键的是，这些模型容易出现捷径学习，尤其是在解剖相似度高、病理特征微妙的胸部X光等模态中。", "method": "本文提出了DiSSECT框架，将多尺度向量量化集成到自监督学习流程中，以施加一个离散的表示瓶颈。这迫使模型学习可重复的、结构感知的特征，同时抑制视图特定或低效用的模式，从而改善跨任务和跨领域的表示迁移。", "result": "DiSSECT在分类和分割任务上均取得了优异性能，仅需极少或无需微调，并在低标签状态下表现出特别高的标签效率。该方法在多个公开医疗影像数据集上得到验证，与现有最先进方法相比，展现出强大的鲁棒性和泛化性。", "conclusion": "DiSSECT通过引入离散自监督学习，有效克服了现有医疗图像SSL方法的局限性，学习到可重复、结构感知的特征，从而提供了高效、可迁移的临床表征，在各种任务和数据集上均表现出强大的性能和泛化能力。"}}
{"id": "2509.18699", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18699", "abs": "https://arxiv.org/abs/2509.18699", "authors": ["Zedong Zhang", "Ying Tai", "Jianjun Qian", "Jian Yang", "Jun Li"], "title": "AGSwap: Overcoming Category Boundaries in Object Fusion via Adaptive Group Swapping", "comment": null, "summary": "Fusing cross-category objects to a single coherent object has gained\nincreasing attention in text-to-image (T2I) generation due to its broad\napplications in virtual reality, digital media, film, and gaming. However,\nexisting methods often produce biased, visually chaotic, or semantically\ninconsistent results due to overlapping artifacts and poor integration.\nMoreover, progress in this field has been limited by the absence of a\ncomprehensive benchmark dataset. To address these problems, we propose\n\\textbf{Adaptive Group Swapping (AGSwap)}, a simple yet highly effective\napproach comprising two key components: (1) Group-wise Embedding Swapping,\nwhich fuses semantic attributes from different concepts through feature\nmanipulation, and (2) Adaptive Group Updating, a dynamic optimization mechanism\nguided by a balance evaluation score to ensure coherent synthesis.\nAdditionally, we introduce \\textbf{Cross-category Object Fusion (COF)}, a\nlarge-scale, hierarchically structured dataset built upon ImageNet-1K and\nWordNet. COF includes 95 superclasses, each with 10 subclasses, enabling\n451,250 unique fusion pairs. Extensive experiments demonstrate that AGSwap\noutperforms state-of-the-art compositional T2I methods, including GPT-Image-1\nusing simple and complex prompts.", "AI": {"tldr": "针对文本到图像生成中跨类别对象融合的挑战，本文提出了AGSwap方法和COF数据集，有效解决了现有方法的局限性，实现了更连贯的图像合成。", "motivation": "文本到图像（T2I）生成中融合跨类别对象具有广泛应用前景，但现有方法常产生有偏差、视觉混乱或语义不一致的结果，主要由于重叠伪影和整合不佳。此外，该领域缺乏全面的基准数据集限制了进展。", "method": "本文提出AGSwap（自适应群组交换）方法，包含两个关键组件：1) 群组式嵌入交换，通过特征操作融合不同概念的语义属性；2) 自适应群组更新，一种由平衡评估分数指导的动态优化机制，确保连贯合成。此外，本文还引入了COF（跨类别对象融合）数据集，这是一个基于ImageNet-1K和WordNet构建的大规模、分层结构数据集，包含95个超类，每个超类有10个子类，支持451,250个独特的融合对。", "result": "广泛实验表明，AGSwap在简单和复杂提示下均优于现有最先进的组合式T2I方法（包括GPT-Image-1）。", "conclusion": "AGSwap提供了一种简单高效的方法来解决跨类别对象融合中的挑战，而COF数据集则为该领域提供了急需的综合基准，共同推动了文本到图像生成中对象融合能力的发展。"}}
{"id": "2509.19228", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19228", "abs": "https://arxiv.org/abs/2509.19228", "authors": ["Gabriele Berton", "Jayakrishnan Unnikrishnan", "Son Tran", "Mubarak Shah"], "title": "CompLLM: Compression for Long Context Q&A", "comment": null, "summary": "Large Language Models (LLMs) face significant computational challenges when\nprocessing long contexts due to the quadratic complexity of self-attention.\nWhile soft context compression methods, which map input text to smaller latent\nrepresentations, have shown promise, their real-world adoption is limited.\nExisting techniques typically compress the context as a single unit, which\nleads to quadratic compression complexity and an inability to reuse\ncomputations across queries with overlapping contexts. In this work, we\nintroduce CompLLM, a soft compression technique designed for practical\ndeployment. Instead of processing the context holistically, CompLLM divides it\ninto segments and compresses each one independently. This simple design choice\nyields three critical properties: efficiency, as the compression step scales\nlinearly with the context length; scalability, enabling models trained on short\nsequences (e.g., 1k tokens) to generalize to contexts of 100k tokens; and\nreusability, allowing compressed segments to be cached and reused across\ndifferent queries. Our experiments show that with a 2x compression rate, at\nhigh context lengths CompLLM speeds up Time To First Token (TTFT) by up to 4x\nand reduces the KV cache size by 50%. Furthermore, CompLLM achieves performance\ncomparable to that obtained with the uncompressed context, and even surpasses\nit on very long sequences, demonstrating its effectiveness and practical\nutility.", "AI": {"tldr": "CompLLM是一种软上下文压缩技术，通过将长上下文分割并独立压缩，解决了LLM处理长上下文的二次复杂度问题，实现了线性扩展、可重用性和与非压缩上下文相当的性能。", "motivation": "大型语言模型（LLM）在处理长上下文时面临计算挑战，因为自注意力机制的复杂度呈二次方增长。现有的软上下文压缩方法通常将整个上下文作为单一单元压缩，导致压缩复杂度仍为二次方，且无法在重叠上下文的查询中重用计算，限制了实际应用。", "method": "CompLLM引入了一种软压缩技术，将长上下文分割成多个段落，并独立压缩每个段落。这种设计实现了三个关键特性：效率（压缩步骤与上下文长度呈线性关系）、可扩展性（短序列训练模型可泛化到长上下文）和可重用性（压缩段可缓存并跨查询重用）。", "result": "实验表明，在2倍压缩率下，CompLLM在长上下文长度上将首个Token生成时间（TTFT）加速高达4倍，并将KV缓存大小减少50%。此外，CompLLM的性能与未压缩上下文相当，甚至在极长序列上超越了未压缩上下文的性能。", "conclusion": "CompLLM通过其分段独立压缩的方法，有效解决了LLM处理长上下文的计算挑战，在保持甚至超越性能的同时，显著提高了效率、可扩展性和计算重用性，展现了其卓越的有效性和实用价值。"}}
{"id": "2509.18801", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18801", "abs": "https://arxiv.org/abs/2509.18801", "authors": ["Kuang Xiaodong", "Li Bingxuan", "Li Yuan", "Rao Fan", "Ma Gege", "Xie Qingguo", "Mok Greta S P", "Liu Huafeng", "Zhu Wentao"], "title": "A Kernel Space-based Multidimensional Sparse Model for Dynamic PET Image Denoising", "comment": null, "summary": "Achieving high image quality for temporal frames in dynamic positron emission\ntomography (PET) is challenging due to the limited statistic especially for the\nshort frames. Recent studies have shown that deep learning (DL) is useful in a\nwide range of medical image denoising tasks. In this paper, we propose a\nmodel-based neural network for dynamic PET image denoising. The inter-frame\nspatial correlation and intra-frame structural consistency in dynamic PET are\nused to establish the kernel space-based multidimensional sparse (KMDS) model.\nWe then substitute the inherent forms of the parameter estimation with neural\nnetworks to enable adaptive parameters optimization, forming the end-to-end\nneural KMDS-Net. Extensive experimental results from simulated and real data\ndemonstrate that the neural KMDS-Net exhibits strong denoising performance for\ndynamic PET, outperforming previous baseline methods. The proposed method may\nbe used to effectively achieve high temporal and spatial resolution for dynamic\nPET. Our source code is available at\nhttps://github.com/Kuangxd/Neural-KMDS-Net/tree/main.", "AI": {"tldr": "本文提出了一种基于模型的神经网络（Neural KMDS-Net），用于动态PET图像去噪，通过利用帧间空间相关性和帧内结构一致性，有效提升了短时帧的图像质量。", "motivation": "动态PET图像，特别是短时帧，由于统计量有限，图像质量差是一个挑战。深度学习在医学图像去噪任务中显示出巨大潜力。", "method": "研究人员提出了一种模型驱动的神经网络，首先利用动态PET中的帧间空间相关性和帧内结构一致性建立了基于核空间的多维稀疏（KMDS）模型。随后，用神经网络替换了参数估计的固有形式，实现了自适应参数优化，从而形成了端到端的Neural KMDS-Net。", "result": "在模拟数据和真实数据上的大量实验结果表明，Neural KMDS-Net在动态PET去噪方面表现出强大的性能，优于现有的基线方法。", "conclusion": "所提出的方法可以有效实现动态PET的高时间分辨率和空间分辨率。"}}
{"id": "2509.18705", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18705", "abs": "https://arxiv.org/abs/2509.18705", "authors": ["Herve Goeau", "Pierre Bonnet", "Alexis Joly"], "title": "Overview of LifeCLEF Plant Identification task 2019: diving into data deficient tropical countries", "comment": "13 pages, 5 figures, CLEF 2019 Conference and Labs of the Evaluation\n  Forum, September 09 to 12, 2019, Lugano, Switzerland", "summary": "Automated identification of plants has improved considerably thanks to the\nrecent progress in deep learning and the availability of training data.\nHowever, this profusion of data only concerns a few tens of thousands of\nspecies, while the planet has nearly 369K. The LifeCLEF 2019 Plant\nIdentification challenge (or \"PlantCLEF 2019\") was designed to evaluate\nautomated identification on the flora of data deficient regions. It is based on\na dataset of 10K species mainly focused on the Guiana shield and the Northern\nAmazon rainforest, an area known to have one of the greatest diversity of\nplants and animals in the world. As in the previous edition, a comparison of\nthe performance of the systems evaluated with the best tropical flora experts\nwas carried out. This paper presents the resources and assessments of the\nchallenge, summarizes the approaches and systems employed by the participating\nresearch groups, and provides an analysis of the main outcomes.", "AI": {"tldr": "LifeCLEF 2019植物识别挑战赛旨在评估深度学习在数据稀缺地区（如圭亚那地盾和亚马逊北部雨林）对10K物种的自动化识别能力，并与人类专家表现进行比较。", "motivation": "尽管深度学习在植物识别方面取得了显著进展，但其成功主要局限于少数物种。全球有近36.9万种植物，但训练数据仅覆盖数万种。因此，需要解决数据稀缺区域和更多物种的自动化识别问题。", "method": "该挑战赛基于一个包含1万个物种的数据集，主要关注圭亚那地盾和亚马逊北部雨林地区的植物群。挑战赛评估了自动化识别系统的性能，并将其与热带植物专家进行比较。本文介绍了挑战赛的资源和评估方法，总结了参与研究团队采用的方法和系统，并分析了主要结果。", "result": "抽象本身并未提供具体的挑战赛结果（例如，哪个系统表现最好或AI与人类表现的差距）。它指出该论文将“总结参与研究团队采用的方法和系统，并分析主要结果”。", "conclusion": "该挑战赛为评估数据稀缺地区植物自动化识别提供了平台，并对自动化系统与人类专家的表现进行了比较。该论文将呈现挑战赛的资源、评估、参与方法和主要成果分析。"}}
{"id": "2509.19249", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19249", "abs": "https://arxiv.org/abs/2509.19249", "authors": ["Siheng Li", "Kejiao Li", "Zenan Xu", "Guanhua Huang", "Evander Yang", "Kun Li", "Haoyuan Wu", "Jiajia Wu", "Zihao Zheng", "Chenchen Zhang", "Kun Shi", "Kyrierl Deng", "Qi Yi", "Ruibin Xiong", "Tingqiang Xu", "Yuhao Jiang", "Jianfeng Yan", "Yuyuan Zeng", "Guanghui Xu", "Jinbao Xue", "Zhijiang Xu", "Zheng Fang", "Shuai Li", "Qibin Liu", "Xiaoxue Li", "Zhuoyu Li", "Yangyu Tao", "Fei Gao", "Cheng Jiang", "Bo Chao Wang", "Kai Liu", "Jianchen Zhu", "Wai Lam", "Wayyt Wang", "Bo Zhou", "Di Wang"], "title": "Reinforcement Learning on Pre-Training Data", "comment": "Work in progress", "summary": "The growing disparity between the exponential scaling of computational\nresources and the finite growth of high-quality text data now constrains\nconventional scaling approaches for large language models (LLMs). To address\nthis challenge, we introduce Reinforcement Learning on Pre-Training data\n(RLPT), a new training-time scaling paradigm for optimizing LLMs. In contrast\nto prior approaches that scale training primarily through supervised learning,\nRLPT enables the policy to autonomously explore meaningful trajectories to\nlearn from pre-training data and improve its capability through reinforcement\nlearning (RL). While existing RL strategies such as reinforcement learning from\nhuman feedback (RLHF) and reinforcement learning with verifiable rewards (RLVR)\nrely on human annotation for reward construction, RLPT eliminates this\ndependency by deriving reward signals directly from pre-training data.\nSpecifically, it adopts a next-segment reasoning objective, rewarding the\npolicy for accurately predicting subsequent text segments conditioned on the\npreceding context. This formulation allows RL to be scaled on pre-training\ndata, encouraging the exploration of richer trajectories across broader\ncontexts and thereby fostering more generalizable reasoning skills. Extensive\nexperiments on both general-domain and mathematical reasoning benchmarks across\nmultiple models validate the effectiveness of RLPT. For example, when applied\nto Qwen3-4B-Base, RLPT yields absolute improvements of $3.0$, $5.1$, $8.1$,\n$6.0$, $6.6$, and $5.3$ on MMLU, MMLU-Pro, GPQA-Diamond, KOR-Bench, AIME24, and\nAIME25, respectively. The results further demonstrate favorable scaling\nbehavior, suggesting strong potential for continued gains with more compute. In\naddition, RLPT provides a solid foundation, extending the reasoning boundaries\nof LLMs and enhancing RLVR performance.", "AI": {"tldr": "RLPT是一种新的LLM训练范式，通过在预训练数据上应用强化学习，解决了高质量文本数据增长有限的问题，并在无需人工标注的情况下显著提升了模型的推理能力。", "motivation": "计算资源呈指数级增长，但高质量文本数据的增长有限，这限制了大型语言模型（LLMs）传统的扩展方法。", "method": "引入了预训练数据上的强化学习（RLPT）。与以往主要通过监督学习扩展训练的方法不同，RLPT使策略能够自主探索有意义的轨迹，从预训练数据中学习并通过强化学习提高能力。RLPT通过直接从预训练数据中获取奖励信号，避免了对人工标注的依赖（与RLHF和RLVR不同）。具体而言，它采用“下一段推理”目标，奖励策略准确预测后续文本段落的能力。", "result": "在通用领域和数学推理基准测试中，RLPT被证明是有效的。例如，应用于Qwen3-4B-Base时，RLPT在MMLU、MMLU-Pro、GPQA-Diamond、KOR-Bench、AIME24和AIME25上分别获得了3.0、5.1、8.1、6.0、6.6和5.3的绝对提升。结果还表明其具有良好的扩展行为，预示着随着计算资源的增加，仍有持续提升的潜力。此外，RLPT为扩展LLM的推理边界和增强RLVR性能提供了坚实基础。", "conclusion": "RLPT是一种有效且可扩展的LLM训练范式，它通过利用预训练数据进行强化学习，解决了数据稀缺问题，提升了模型的泛化推理能力，并为LLM的进一步发展奠定了基础。"}}
{"id": "2509.18847", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.18847", "abs": "https://arxiv.org/abs/2509.18847", "authors": ["Junhao Su", "Yuanliang Wan", "Junwei Yang", "Hengyu Shi", "Tianyang Han", "Junfeng Luo", "Yurui Qiu"], "title": "Failure Makes the Agent Stronger: Enhancing Accuracy through Structured Reflection for Reliable Tool Interactions", "comment": "9pages", "summary": "Tool-augmented large language models (LLMs) are usually trained with\nsupervised imitation or coarse-grained reinforcement learning that optimizes\nsingle tool calls. Current self-reflection practices rely on heuristic prompts\nor one-way reasoning: the model is urged to 'think more' instead of learning\nerror diagnosis and repair. This is fragile in multi-turn interactions; after a\nfailure the model often repeats the same mistake. We propose structured\nreflection, which turns the path from error to repair into an explicit,\ncontrollable, and trainable action. The agent produces a short yet precise\nreflection: it diagnoses the failure using evidence from the previous step and\nthen proposes a correct, executable follow-up call. For training we combine\nDAPO and GSPO objectives with a reward scheme tailored to tool use, optimizing\nthe stepwise strategy Reflect, then Call, then Final. To evaluate, we introduce\nTool-Reflection-Bench, a lightweight benchmark that programmatically checks\nstructural validity, executability, parameter correctness, and result\nconsistency. Tasks are built as mini trajectories of erroneous call,\nreflection, and corrected call, with disjoint train and test splits.\nExperiments on BFCL v3 and Tool-Reflection-Bench show large gains in multi-turn\ntool-call success and error recovery, and a reduction of redundant calls. These\nresults indicate that making reflection explicit and optimizing it directly\nimproves the reliability of tool interaction and offers a reproducible path for\nagents to learn from failure.", "AI": {"tldr": "本文提出了一种名为“结构化反思”的方法，使工具增强型大型语言模型（LLMs）能够明确诊断错误并提出修复方案。通过结合DAPO和GSPO目标以及定制奖励机制进行训练，并在新的Tool-Reflection-Bench基准上评估，显著提升了多轮工具调用成功率和错误恢复能力。", "motivation": "目前的工具增强型LLMs在多轮交互中表现脆弱，常因启发式提示或单向推理的反思机制，在失败后重复犯错，难以从错误中学习。", "method": "研究人员提出了“结构化反思”机制，将从错误到修复的路径转化为明确、可控和可训练的动作。智能体通过证据诊断失败，并提出正确的、可执行的后续调用。训练结合了DAPO和GSPO目标，并设计了针对工具使用的奖励方案，优化“反思、调用、完成”的分步策略。评估引入了Tool-Reflection-Bench基准，以程序化方式检查结构有效性、可执行性、参数正确性和结果一致性。", "result": "在BFCL v3和Tool-Reflection-Bench上的实验显示，该方法在多轮工具调用成功率和错误恢复方面取得了显著提升，并减少了冗余调用。", "conclusion": "明确的反思和直接优化能显著提高工具交互的可靠性，并为智能体从失败中学习提供了可复现的路径。"}}
{"id": "2509.18715", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18715", "abs": "https://arxiv.org/abs/2509.18715", "authors": ["Yingquan Wang", "Pingping Zhang", "Chong Sun", "Dong Wang", "Huchuan Lu"], "title": "What Makes You Unique? Attribute Prompt Composition for Object Re-Identification", "comment": "Accepted by TCSVT2025", "summary": "Object Re-IDentification (ReID) aims to recognize individuals across\nnon-overlapping camera views. While recent advances have achieved remarkable\nprogress, most existing models are constrained to either single-domain or\ncross-domain scenarios, limiting their real-world applicability. Single-domain\nmodels tend to overfit to domain-specific features, whereas cross-domain models\noften rely on diverse normalization strategies that may inadvertently suppress\nidentity-specific discriminative cues. To address these limitations, we propose\nan Attribute Prompt Composition (APC) framework, which exploits textual\nsemantics to jointly enhance discrimination and generalization. Specifically,\nwe design an Attribute Prompt Generator (APG) consisting of a Semantic\nAttribute Dictionary (SAD) and a Prompt Composition Module (PCM). SAD is an\nover-complete attribute dictionary to provide rich semantic descriptions, while\nPCM adaptively composes relevant attributes from SAD to generate discriminative\nattribute-aware features. In addition, motivated by the strong generalization\nability of Vision-Language Models (VLM), we propose a Fast-Slow Training\nStrategy (FSTS) to balance ReID-specific discrimination and generalizable\nrepresentation learning. Specifically, FSTS adopts a Fast Update Stream (FUS)\nto rapidly acquire ReID-specific discriminative knowledge and a Slow Update\nStream (SUS) to retain the generalizable knowledge inherited from the\npre-trained VLM. Through a mutual interaction, the framework effectively\nfocuses on ReID-relevant features while mitigating overfitting. Extensive\nexperiments on both conventional and Domain Generalized (DG) ReID datasets\ndemonstrate that our framework surpasses state-of-the-art methods, exhibiting\nsuperior performances in terms of both discrimination and generalization. The\nsource code is available at https://github.com/AWangYQ/APC.", "AI": {"tldr": "本文提出了一种属性提示组合 (APC) 框架，利用文本语义和视觉语言模型 (VLM) 来共同增强目标重识别 (ReID) 的判别性和泛化性，解决了现有模型在单域或跨域场景中的局限性。", "motivation": "现有目标重识别模型存在局限性：单域模型容易过拟合特定领域特征，而跨域模型可能因归一化策略抑制身份判别性特征，限制了其在真实世界中的适用性。研究旨在解决这些问题，提升模型在判别性和泛化性方面的表现。", "method": "本文提出了一个属性提示组合 (APC) 框架，包含：1) 属性提示生成器 (APG)，由语义属性字典 (SAD) 和提示组合模块 (PCM) 组成，用于从丰富的语义描述中自适应地生成判别性属性感知特征。2) 快慢更新训练策略 (FSTS)，它结合了快速更新流 (FUS) 以学习 ReID 特定的判别知识，以及慢速更新流 (SUS) 以保留预训练 VLM 的通用知识，通过相互作用平衡判别性和泛化性，并减轻过拟合。", "result": "在传统和领域泛化 (DG) ReID 数据集上进行的广泛实验表明，所提出的框架超越了现有最先进的方法，在判别性和泛化性方面均表现出卓越的性能。", "conclusion": "APC 框架通过有效利用文本语义和 VLM 的泛化能力，成功地解决了现有 ReID 模型的局限性，显著提升了目标重识别在判别性和泛化性方面的表现，为跨摄像头识别提供了更鲁棒的解决方案。"}}
{"id": "2509.19269", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19269", "abs": "https://arxiv.org/abs/2509.19269", "authors": ["Nitesh Kumar", "Usashi Chatterjee", "Steven Schockaert"], "title": "Extracting Conceptual Spaces from LLMs Using Prototype Embeddings", "comment": null, "summary": "Conceptual spaces represent entities and concepts using cognitively\nmeaningful dimensions, typically referring to perceptual features. Such\nrepresentations are widely used in cognitive science and have the potential to\nserve as a cornerstone for explainable AI. Unfortunately, they have proven\nnotoriously difficult to learn, although recent LLMs appear to capture the\nrequired perceptual features to a remarkable extent. Nonetheless, practical\nmethods for extracting the corresponding conceptual spaces are currently still\nlacking. While various methods exist for extracting embeddings from LLMs,\nextracting conceptual spaces also requires us to encode the underlying\nfeatures. In this paper, we propose a strategy in which features (e.g.\nsweetness) are encoded by embedding the description of a corresponding\nprototype (e.g. a very sweet food). To improve this strategy, we fine-tune the\nLLM to align the prototype embeddings with the corresponding conceptual space\ndimensions. Our empirical analysis finds this approach to be highly effective.", "AI": {"tldr": "本文提出了一种从大型语言模型（LLMs）中提取概念空间的新策略，通过嵌入原型描述来编码特征，并通过微调LLM来对齐原型嵌入与概念维度，实验证明该方法非常有效。", "motivation": "概念空间在认知科学中广泛应用，并有望成为可解释人工智能（XAI）的基石。然而，它们很难学习。尽管最近的LLMs似乎在很大程度上捕捉到了所需的感知特征，但目前仍然缺乏提取相应概念空间的实用方法。", "method": "该研究提出了一种策略，通过嵌入相应原型（例如，“非常甜的食物”来表示“甜度”）的描述来编码特征。为了改进这一策略，研究人员对LLM进行了微调，以使原型嵌入与相应的概念空间维度对齐。", "result": "实证分析发现，所提出的方法高度有效。", "conclusion": "通过嵌入原型描述和微调LLM对齐原型嵌入与概念维度，可以有效地从LLMs中提取概念空间。"}}
{"id": "2509.18917", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18917", "abs": "https://arxiv.org/abs/2509.18917", "authors": ["Amirhesam Aghanouri", "Cristina Olaverri-Monreal"], "title": "LiDAR Point Cloud Image-based Generation Using Denoising Diffusion Probabilistic Models", "comment": null, "summary": "Autonomous vehicles (AVs) are expected to revolutionize transportation by\nimproving efficiency and safety. Their success relies on 3D vision systems that\neffectively sense the environment and detect traffic agents. Among sensors AVs\nuse to create a comprehensive view of surroundings, LiDAR provides\nhigh-resolution depth data enabling accurate object detection, safe navigation,\nand collision avoidance. However, collecting real-world LiDAR data is\ntime-consuming and often affected by noise and sparsity due to adverse weather\nor sensor limitations. This work applies a denoising diffusion probabilistic\nmodel (DDPM), enhanced with novel noise scheduling and time-step embedding\ntechniques to generate high-quality synthetic data for augmentation, thereby\nimproving performance across a range of computer vision tasks, particularly in\nAV perception. These modifications impact the denoising process and the model's\ntemporal awareness, allowing it to produce more realistic point clouds based on\nthe projection. The proposed method was extensively evaluated under various\nconfigurations using the IAMCV and KITTI-360 datasets, with four performance\nmetrics compared against state-of-the-art (SOTA) methods. The results\ndemonstrate the model's superior performance over most existing baselines and\nits effectiveness in mitigating the effects of noisy and sparse LiDAR data,\nproducing diverse point clouds with rich spatial relationships and structural\ndetail.", "AI": {"tldr": "本文提出了一种改进的去噪扩散概率模型（DDPM），通过新颖的噪声调度和时间步嵌入技术，生成高质量的合成LiDAR数据，以增强自动驾驶汽车（AVs）的3D视觉感知能力，克服了真实世界LiDAR数据噪声和稀疏的挑战。", "motivation": "自动驾驶汽车的成功依赖于有效的3D视觉系统来感知环境和检测交通参与者。然而，收集真实的LiDAR数据耗时且易受恶劣天气或传感器限制导致噪声和稀疏。这促使研究人员寻求生成高质量合成数据的方法来克服这些限制。", "method": "研究人员应用了一种去噪扩散概率模型（DDPM），并对其进行了增强，引入了新颖的噪声调度和时间步嵌入技术。这些修改旨在改善去噪过程和模型的时间感知能力，从而根据投影生成更逼真的点云。", "result": "该方法在IAMCV和KITTI-360数据集上进行了广泛评估，并与现有最先进（SOTA）方法进行了四项性能指标的比较。结果表明，该模型在大多数现有基线中表现优越，并有效缓解了噪声和稀疏LiDAR数据的影响，生成了具有丰富空间关系和结构细节的多样化点云。", "conclusion": "所提出的方法通过生成高质量的合成LiDAR数据，显著提高了自动驾驶汽车的感知性能，有效应对了真实世界LiDAR数据噪声和稀疏的挑战，并且在多个基准测试中优于现有方法。"}}
{"id": "2509.18717", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.18717", "abs": "https://arxiv.org/abs/2509.18717", "authors": ["Tong Zhang", "Kuofeng Gao", "Jiawang Bai", "Leo Yu Zhang", "Xin Yin", "Zonghui Wang", "Shouling Ji", "Wenzhi Chen"], "title": "Pre-training CLIP against Data Poisoning with Optimal Transport-based Matching and Alignment", "comment": null, "summary": "Recent studies have shown that Contrastive Language-Image Pre-training (CLIP)\nmodels are threatened by targeted data poisoning and backdoor attacks due to\nmassive training image-caption pairs crawled from the Internet. Previous\ndefense methods correct poisoned image-caption pairs by matching a new caption\nfor each image. However, the matching process relies solely on the global\nrepresentations of images and captions, overlooking fine-grained features of\nvisual and textual features. It may introduce incorrect image-caption pairs and\nharm the CLIP pre-training. To address their limitations, we propose an Optimal\nTransport-based framework to reconstruct image-caption pairs, named OTCCLIP. We\npropose a new optimal transport-based distance measure between fine-grained\nvisual and textual feature sets and re-assign new captions based on the\nproposed optimal transport distance. Additionally, to further reduce the\nnegative impact of mismatched pairs, we encourage the inter- and intra-modality\nfine-grained alignment by employing optimal transport-based objective\nfunctions. Our experiments demonstrate that OTCCLIP can successfully decrease\nthe attack success rates of poisoning attacks. Also, compared to previous\nmethods, OTCCLIP significantly improves CLIP's zero-shot and linear probing\nperformance trained on poisoned datasets.", "AI": {"tldr": "针对CLIP模型面临的数据投毒和后门攻击威胁，本文提出OTCCLIP框架。该框架利用最优传输（Optimal Transport）机制，通过细粒度视觉和文本特征对图像-标题对进行重建和对齐，有效降低了攻击成功率，并显著提升了CLIP模型在受污染数据集上的零样本和线性探测性能。", "motivation": "CLIP模型因训练数据源自互联网，易受目标数据投毒和后门攻击。现有防御方法仅依赖图像和标题的全局表示来匹配新标题，忽略了细粒度特征，可能引入不正确的图像-标题对，从而损害CLIP预训练效果。", "method": "本文提出了一个基于最优传输的框架OTCCLIP来重建图像-标题对。具体方法包括：1) 提出一种新的基于最优传输的距离度量，用于评估细粒度视觉和文本特征集之间的匹配度。2) 基于该最优传输距离重新分配新的标题。3) 采用基于最优传输的目标函数，鼓励模态内和模态间的细粒度对齐，以进一步减少不匹配对的负面影响。", "result": "实验结果表明，OTCCLIP能够成功降低投毒攻击的成功率。与现有方法相比，OTCCLIP显著提升了CLIP模型在受污染数据集上进行训练后的零样本（zero-shot）和线性探测（linear probing）性能。", "conclusion": "OTCCLIP通过引入基于最优传输的细粒度特征对齐和图像-标题对重建，有效应对了针对CLIP模型的投毒攻击，不仅增强了模型的鲁棒性，还改善了其在受污染数据上的学习性能。"}}
{"id": "2509.19270", "categories": ["cs.CL", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.19270", "abs": "https://arxiv.org/abs/2509.19270", "authors": ["Erik Božík", "Marek Šuppa"], "title": "SloPalSpeech: A 2,8000-Hour Slovak Speech Corpus from Parliamentary Data", "comment": null, "summary": "Automatic Speech Recognition (ASR) for low-resource languages like Slovak is\nhindered by the scarcity of training data. To address this, we introduce\nSloPalSpeech, a new, large-scale Slovak ASR dataset containing 2,806 hours of\nspeech from parliamentary proceedings. We developed a robust processing\npipeline to align and segment long-form recordings into clean, 30-second\naudio-transcript pairs suitable for model training. We use this dataset to\nfine-tune several OpenAI Whisper models (small, medium, large-v3, and\nlarge-v3-turbo), achieving significant Word Error Rate (WER) reductions on\nstandard Slovak benchmarks like Common Voice and FLEURS. For instance, the\nfine-tuned Whisper-small model's WER dropped by up to 70\\%, approaching the\nbaseline performance of the much larger Whisper-large-v3 model. To foster\nfuture research in low-resource speech recognition, we publicly release the\ncomplete SloPalSpeech dataset, the fully segmented transcripts (60 million\nwords), and all our fine-tuned models.", "AI": {"tldr": "该研究引入了一个新的大规模斯洛伐克语ASR数据集SloPalSpeech（2,806小时），并使用它微调了OpenAI Whisper模型，显著降低了标准基准上的词错误率，并公开了数据集和模型。", "motivation": "斯洛伐克语等低资源语言的自动语音识别（ASR）因训练数据稀缺而受阻。", "method": "研究团队开发了一个鲁棒的处理流程，将议会记录中的长篇录音对齐并分割成干净的30秒音频-文本对。然后，他们使用这个名为SloPalSpeech的新数据集微调了多种OpenAI Whisper模型（small, medium, large-v3, large-v3-turbo）。", "result": "微调后的Whisper模型在Common Voice和FLEURS等标准斯洛伐克语基准测试上实现了显著的词错误率（WER）降低。例如，微调后的Whisper-small模型的WER降低了高达70%，接近更大的Whisper-large-v3模型的基线性能。", "conclusion": "该研究通过公开完整的SloPalSpeech数据集、分割后的文本（6000万词）以及所有微调模型，旨在促进低资源语音识别领域的未来研究。"}}
{"id": "2509.18938", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18938", "abs": "https://arxiv.org/abs/2509.18938", "authors": ["Matheus Vinícius Todescato", "Joel Luís Carbonera"], "title": "No Labels Needed: Zero-Shot Image Classification with Collaborative Self-Learning", "comment": "This paper was accepted at International Conference on Tools with\n  Artificial Intelligence (ICTAI) 2025", "summary": "While deep learning, including Convolutional Neural Networks (CNNs) and\nVision Transformers (ViTs), has significantly advanced classification\nperformance, its typical reliance on extensive annotated datasets presents a\nmajor obstacle in many practical scenarios where such data is scarce.\nVision-language models (VLMs) and transfer learning with pre-trained visual\nmodels appear as promising techniques to deal with this problem. This paper\nproposes a novel zero-shot image classification framework that combines a VLM\nand a pre-trained visual model within a self-learning cycle. Requiring only the\nset of class names and no labeled training data, our method utilizes a\nconfidence-based pseudo-labeling strategy to train a lightweight classifier\ndirectly on the test data, enabling dynamic adaptation. The VLM identifies\nhigh-confidence samples, and the pre-trained visual model enhances their visual\nrepresentations. These enhanced features then iteratively train the classifier,\nallowing the system to capture complementary semantic and visual cues without\nsupervision. Notably, our approach avoids VLM fine-tuning and the use of large\nlanguage models, relying on the visual-only model to reduce the dependence on\nsemantic representation. Experimental evaluations on ten diverse datasets\ndemonstrate that our approach outperforms the baseline zero-shot method.", "AI": {"tldr": "本文提出了一种新颖的零样本图像分类框架，结合视觉语言模型（VLM）和预训练视觉模型，通过自学习循环和基于置信度的伪标签策略，在无需标注数据的情况下，直接在测试数据上训练轻量级分类器，实现动态适应和卓越性能。", "motivation": "深度学习模型（如CNN和ViT）在分类方面表现出色，但其对大量标注数据的依赖在许多实际场景中构成重大障碍。视觉语言模型（VLM）和预训练视觉模型的迁移学习是解决此问题的有前景技术，但仍需探索更有效的零样本分类方法。", "method": "该方法提出了一个零样本图像分类框架，仅需类别名称，无需任何标注训练数据。它结合了VLM和预训练视觉模型，采用自学习循环。具体而言，VLM用于识别高置信度样本，预训练视觉模型增强这些样本的视觉表示。然后，这些增强的特征迭代地训练一个轻量级分类器，直接在测试数据上进行动态适应。该方法避免了VLM微调和大型语言模型的使用，通过依赖纯视觉模型来减少对语义表示的依赖。", "result": "在十个不同的数据集上的实验评估表明，所提出的方法优于基线零样本方法。", "conclusion": "该研究成功地提出了一种零样本图像分类框架，通过结合VLM和预训练视觉模型，并利用自学习伪标签策略，在无需标注数据和VLM微调的情况下，实现了对测试数据的动态适应，并显著提升了分类性能。"}}
{"id": "2509.18733", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18733", "abs": "https://arxiv.org/abs/2509.18733", "authors": ["Yilin Gao", "Kangyi Chen", "Zhongxing Peng", "Hengjie Lu", "Shugong Xu"], "title": "Knowledge Transfer from Interaction Learning", "comment": "Accepted by ICCV2025", "summary": "Current visual foundation models (VFMs) face a fundamental limitation in\ntransferring knowledge from vision language models (VLMs), while VLMs excel at\nmodeling cross-modal interactions through unified representation spaces,\nexisting VFMs predominantly adopt result-oriented paradigms that neglect the\nunderlying interaction processes. This representational discrepancy hinders\neffective knowledge transfer and limits generalization across diverse vision\ntasks. We propose Learning from Interactions (LFI), a cognitive-inspired\nframework that addresses this gap by explicitly modeling visual understanding\nas an interactive process. Our key insight is that capturing the dynamic\ninteraction patterns encoded in pre-trained VLMs enables more faithful and\nefficient knowledge transfer to VFMs. The approach centers on two technical\ninnovations, Interaction Queries, which maintain persistent relational\nstructures across network layers, and interaction-based supervision, derived\nfrom the cross-modal attention mechanisms of VLMs. Comprehensive experiments\ndemonstrate consistent improvements across multiple benchmarks, achieving 3.3\nand 1.6mAP/2.4AP absolute gains on TinyImageNet classification and COCO\ndetection/segmentation respectively, with minimal parameter overhead and faster\nconvergence. The framework particularly excels in cross-domain settings,\ndelivering 2.4 and 9.3 zero-shot improvements on PACS and VLCS. Human\nevaluations further confirm its cognitive alignment, outperforming\nresult-oriented methods by 2.7 times in semantic consistency metrics.", "AI": {"tldr": "本文提出了一种名为“从交互中学习”（LFI）的框架，通过明确建模视觉理解为交互过程，解决了视觉基础模型（VFMs）在从视觉语言模型（VLMs）转移知识时存在的局限性，实现了更有效和忠实的知识迁移。", "motivation": "当前的视觉基础模型（VFMs）在从视觉语言模型（VLMs）转移知识方面面临根本性限制。VLMs擅长通过统一表示空间建模跨模态交互，而现有VFMs主要采用以结果为导向的范式，忽略了底层的交互过程。这种表示差异阻碍了有效的知识转移，并限制了在不同视觉任务中的泛化能力。", "method": "本文提出了一种受认知启发的框架LFI，通过明确将视觉理解建模为一个交互过程来解决上述问题。其核心思想是捕获预训练VLM中编码的动态交互模式，以实现更忠实和高效的知识转移。该方法围绕两项技术创新：交互查询（Interaction Queries），用于在网络层之间维持持久的关系结构；以及基于交互的监督，该监督来源于VLMs的跨模态注意力机制。", "result": "全面的实验表明，LFI在多个基准测试中取得了持续改进：在TinyImageNet分类上实现了3.3mAP/2.4AP的绝对增益，在COCO检测/分割上实现了1.6mAP的绝对增益，且参数开销极小，收敛速度更快。该框架在跨领域设置中表现尤为出色，在PACS和VLCS上分别带来了2.4和9.3的零样本改进。人类评估进一步证实了其认知一致性，在语义一致性指标上比以结果为导向的方法高出2.7倍。", "conclusion": "LFI框架通过明确建模视觉理解为交互过程，并利用交互查询和基于交互的监督，有效地解决了VFM从VLM转移知识的难题。它实现了显著的性能提升，尤其是在泛化能力和跨领域设置中，并展现出更高的认知一致性，为未来的视觉基础模型发展提供了新方向。"}}
{"id": "2509.19271", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19271", "abs": "https://arxiv.org/abs/2509.19271", "authors": ["Abdou Karim Kandji", "Frédéric Precioso", "Cheikh Ba", "Samba Ndiaye", "Augustin Ndione"], "title": "WolBanking77: Wolof Banking Speech Intent Classification Dataset", "comment": "10 pages, 7 figures", "summary": "Intent classification models have made a lot of progress in recent years.\nHowever, previous studies primarily focus on high-resource languages datasets,\nwhich results in a gap for low-resource languages and for regions with a high\nrate of illiterate people where languages are more spoken than read or written.\nThis is the case in Senegal, for example, where Wolof is spoken by around 90\\%\nof the population, with an illiteracy rate of 42\\% for the country. Wolof is\nactually spoken by more than 10 million people in West African region. To\ntackle such limitations, we release a Wolof Intent Classification Dataset\n(WolBanking77), for academic research in intent classification. WolBanking77\ncurrently contains 9,791 text sentences in the banking domain and more than 4\nhours of spoken sentences. Experiments on various baselines are conducted in\nthis work, including text and voice state-of-the-art models. The results are\nvery promising on this current dataset. This paper also provides detailed\nanalyses of the contents of the data. We report baseline f1-score and word\nerror rate metrics respectively on NLP and ASR models trained on WolBanking77\ndataset and also comparisons between models. We plan to share and conduct\ndataset maintenance, updates and to release open-source code.", "AI": {"tldr": "该论文发布了一个新的沃洛夫语（Wolof）意图分类数据集WolBanking77，包含文本和语音数据，旨在解决低资源语言和高文盲率地区意图分类的挑战。", "motivation": "以往的意图分类模型主要关注高资源语言数据集，导致低资源语言以及文盲率高、语言以口语为主的地区存在空白。沃洛夫语在西非地区有超过1000万人使用，而塞内加尔的文盲率高达42%，因此急需针对此类语言的数据集。", "method": "研究者发布了Wolof意图分类数据集（WolBanking77），该数据集包含9,791个银行领域的文本句子和超过4小时的口语句子。在此数据集上，他们对文本和语音领域的各种最先进基线模型进行了实验，并提供了详细的数据内容分析。", "result": "在WolBanking77数据集上，各种基线模型的实验结果“非常有前景”。论文报告了NLP模型的F1分数和ASR模型的词错误率（WER）指标，并进行了模型间的比较。", "conclusion": "该论文通过发布WolBanking77数据集，为低资源语言的意图分类研究填补了空白，并展示了在沃洛夫语上进行意图分类的可行性。未来计划进行数据集维护、更新并发布开源代码。"}}
{"id": "2509.19002", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19002", "abs": "https://arxiv.org/abs/2509.19002", "authors": ["Hao Wang", "Eiki Murata", "Lingfang Zhang", "Ayako Sato", "So Fukuda", "Ziqi Yin", "Wentao Hu", "Keisuke Nakao", "Yusuke Nakamura", "Sebastian Zwirner", "Yi-Chia Chen", "Hiroyuki Otomo", "Hiroki Ouchi", "Daisuke Kawahara"], "title": "VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via Travel Video Itinerary Reconstruction", "comment": null, "summary": "Recent advances in multimodal large language models (MLLMs) have\nsignificantly enhanced video understanding capabilities, opening new\npossibilities for practical applications. Yet current video benchmarks focus\nlargely on indoor scenes or short-range outdoor activities, leaving the\nchallenges associated with long-distance travel largely unexplored. Mastering\nextended geospatial-temporal trajectories is critical for next-generation\nMLLMs, underpinning real-world tasks such as embodied-AI planning and\nnavigation. To bridge this gap, we present VIR-Bench, a novel benchmark\nconsisting of 200 travel videos that frames itinerary reconstruction as a\nchallenging task designed to evaluate and push forward MLLMs'\ngeospatial-temporal intelligence. Experimental results reveal that\nstate-of-the-art MLLMs, including proprietary ones, struggle to achieve high\nscores, underscoring the difficulty of handling videos that span extended\nspatial and temporal scales. Moreover, we conduct an in-depth case study in\nwhich we develop a prototype travel-planning agent that leverages the insights\ngained from VIR-Bench. The agent's markedly improved itinerary recommendations\nverify that our evaluation protocol not only benchmarks models effectively but\nalso translates into concrete performance gains in user-facing applications.", "AI": {"tldr": "本文提出了VIR-Bench，一个包含200个旅行视频的新基准，旨在评估和提升多模态大语言模型（MLLMs）在长距离地理空间-时间轨迹理解方面的能力，并将其应用于行程重建任务。", "motivation": "当前的视频基准主要关注室内或短距离户外活动，忽略了长距离旅行带来的挑战。掌握扩展的地理空间-时间轨迹对于下一代MLLMs至关重要，是具身AI规划和导航等实际任务的基础。", "method": "研究者创建了VIR-Bench，一个包含200个旅行视频的新基准，并将行程重建作为一项挑战性任务来评估MLLMs的地理空间-时间智能。此外，他们还开发了一个原型旅行规划代理，利用VIR-Bench的发现来改进行程推荐。", "result": "实验结果显示，包括专有模型在内的最先进MLLMs在VIR-Bench上得分较低，突显了处理跨越扩展空间和时间尺度的视频的难度。原型旅行规划代理显著改进的行程推荐验证了评估协议的有效性，并能转化为面向用户应用的具体性能提升。", "conclusion": "VIR-Bench不仅能有效评估MLLMs在长距离地理空间-时间理解方面的能力，而且其评估协议的洞察力能够转化为实际应用中（如旅行规划代理）的具体性能提升，推动了下一代MLLMs的发展。"}}
{"id": "2509.18738", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18738", "abs": "https://arxiv.org/abs/2509.18738", "authors": ["Ruichao Hou", "Xingyuan Li", "Tongwei Ren", "Dongming Zhou", "Gangshan Wu", "Jinde Cao"], "title": "HyPSAM: Hybrid Prompt-driven Segment Anything Model for RGB-Thermal Salient Object Detection", "comment": null, "summary": "RGB-thermal salient object detection (RGB-T SOD) aims to identify prominent\nobjects by integrating complementary information from RGB and thermal\nmodalities. However, learning the precise boundaries and complete objects\nremains challenging due to the intrinsic insufficient feature fusion and the\nextrinsic limitations of data scarcity. In this paper, we propose a novel\nhybrid prompt-driven segment anything model (HyPSAM), which leverages the\nzero-shot generalization capabilities of the segment anything model (SAM) for\nRGB-T SOD. Specifically, we first propose a dynamic fusion network (DFNet) that\ngenerates high-quality initial saliency maps as visual prompts. DFNet employs\ndynamic convolution and multi-branch decoding to facilitate adaptive\ncross-modality interaction, overcoming the limitations of fixed-parameter\nkernels and enhancing multi-modal feature representation. Moreover, we propose\na plug-and-play refinement network (P2RNet), which serves as a general\noptimization strategy to guide SAM in refining saliency maps by using hybrid\nprompts. The text prompt ensures reliable modality input, while the mask and\nbox prompts enable precise salient object localization. Extensive experiments\non three public datasets demonstrate that our method achieves state-of-the-art\nperformance. Notably, HyPSAM has remarkable versatility, seamlessly integrating\nwith different RGB-T SOD methods to achieve significant performance gains,\nthereby highlighting the potential of prompt engineering in this field. The\ncode and results of our method are available at:\nhttps://github.com/milotic233/HyPSAM.", "AI": {"tldr": "本文提出HyPSAM，一个混合提示驱动的SAM模型，通过动态融合网络生成视觉提示并利用即插即用细化网络结合多种提示引导SAM，以解决RGB-T显著目标检测中边界不精确和特征融合不足的问题。", "motivation": "RGB-T显著目标检测（RGB-T SOD）在整合RGB和热成像模态信息以识别突出物体时，面临由于固有的特征融合不足和数据稀缺性导致的难以学习精确边界和完整物体的问题。", "method": "本文提出混合提示驱动的Segment Anything Model (HyPSAM)。首先，设计动态融合网络（DFNet）生成高质量的初始显著图作为视觉提示，DFNet采用动态卷积和多分支解码实现自适应跨模态交互。其次，提出即插即用细化网络（P2RNet）作为通用优化策略，通过混合提示（文本、掩码和边界框提示）引导SAM细化显著图。", "result": "在三个公共数据集上的大量实验表明，所提出的方法实现了最先进的性能。值得注意的是，HyPSAM具有显著的通用性，可以无缝集成到不同的RGB-T SOD方法中，实现显著的性能提升。", "conclusion": "HyPSAM有效解决了RGB-T SOD的挑战，通过创新的混合提示工程和动态特征融合，实现了优越的性能和通用性，凸显了提示工程在该领域的巨大潜力。"}}
{"id": "2509.19274", "categories": ["cs.CL", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.19274", "abs": "https://arxiv.org/abs/2509.19274", "authors": ["Arijit Maji", "Raghvendra Kumar", "Akash Ghosh", "Anushka", "Nemil Shah", "Abhilekh Borah", "Vanshika Shah", "Nishant Mishra", "Sriparna Saha"], "title": "DRISHTIKON: A Multimodal Multilingual Benchmark for Testing Language Models' Understanding on Indian Culture", "comment": "EMNLP MAINS 2025", "summary": "We introduce DRISHTIKON, a first-of-its-kind multimodal and multilingual\nbenchmark centered exclusively on Indian culture, designed to evaluate the\ncultural understanding of generative AI systems. Unlike existing benchmarks\nwith a generic or global scope, DRISHTIKON offers deep, fine-grained coverage\nacross India's diverse regions, spanning 15 languages, covering all states and\nunion territories, and incorporating over 64,000 aligned text-image pairs. The\ndataset captures rich cultural themes including festivals, attire, cuisines,\nart forms, and historical heritage amongst many more. We evaluate a wide range\nof vision-language models (VLMs), including open-source small and large models,\nproprietary systems, reasoning-specialized VLMs, and Indic-focused models,\nacross zero-shot and chain-of-thought settings. Our results expose key\nlimitations in current models' ability to reason over culturally grounded,\nmultimodal inputs, particularly for low-resource languages and less-documented\ntraditions. DRISHTIKON fills a vital gap in inclusive AI research, offering a\nrobust testbed to advance culturally aware, multimodally competent language\ntechnologies.", "AI": {"tldr": "DRISHTIKON是一个专注于印度文化的多模态、多语言基准测试，旨在评估生成式AI系统的文化理解能力，并揭示了当前模型在处理文化背景输入方面的局限性。", "motivation": "现有基准测试范围通用或全球化，缺乏对印度等多元文化的深入、细致覆盖，无法有效评估生成式AI系统对特定文化的理解能力。", "method": "引入了DRISHTIKON，一个包含15种语言、覆盖印度所有邦和联邦属地、超过64,000个对齐文本-图像对的多模态、多语言基准数据集。该数据集涵盖节日、服饰、美食、艺术形式和历史遗产等丰富的文化主题。作者评估了广泛的视觉-语言模型（VLM），包括开源大小模型、专有系统、推理专业VLM和专注于印度语言的模型，采用零样本和思维链设置。", "result": "评估结果揭示了当前模型在处理基于文化的多模态输入进行推理时存在关键局限性，尤其是在低资源语言和较少记录的传统方面表现不佳。", "conclusion": "DRISHTIKON填补了包容性AI研究中的一个重要空白，提供了一个强大的测试平台，以推动文化感知、多模态能力强的语言技术发展。"}}
{"id": "2509.19090", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19090", "abs": "https://arxiv.org/abs/2509.19090", "authors": ["Guoxin Wang", "Jun Zhao", "Xinyi Liu", "Yanbo Liu", "Xuyang Cao", "Chao Li", "Zhuoyun Liu", "Qintian Sun", "Fangru Zhou", "Haoqiang Xing", "Zhenhong Yang"], "title": "Citrus-V: Advancing Medical Foundation Models with Unified Medical Image Grounding for Clinical Reasoning", "comment": null, "summary": "Medical imaging provides critical evidence for clinical diagnosis, treatment\nplanning, and surgical decisions, yet most existing imaging models are narrowly\nfocused and require multiple specialized networks, limiting their\ngeneralization. Although large-scale language and multimodal models exhibit\nstrong reasoning and multi-task capabilities, real-world clinical applications\ndemand precise visual grounding, multimodal integration, and chain-of-thought\nreasoning. We introduce Citrus-V, a multimodal medical foundation model that\ncombines image analysis with textual reasoning. The model integrates detection,\nsegmentation, and multimodal chain-of-thought reasoning, enabling pixel-level\nlesion localization, structured report generation, and physician-like\ndiagnostic inference in a single framework. We propose a novel multimodal\ntraining approach and release a curated open-source data suite covering\nreasoning, detection, segmentation, and document understanding tasks.\nEvaluations demonstrate that Citrus-V outperforms existing open-source medical\nmodels and expert-level imaging systems across multiple benchmarks, delivering\na unified pipeline from visual grounding to clinical reasoning and supporting\nprecise lesion quantification, automated reporting, and reliable second\nopinions.", "AI": {"tldr": "Citrus-V是一个多模态医疗基础模型，结合图像分析与文本推理，在一个框架内实现病灶定位、报告生成和诊断推理，性能超越现有模型。", "motivation": "现有医学影像模型过于狭窄，泛化能力差，且真实临床应用需要精确的视觉定位、多模态整合和链式推理能力。", "method": "引入Citrus-V，一个整合检测、分割和多模态链式推理的多模态医疗基础模型，能够进行像素级病灶定位、结构化报告生成和医生般的诊断推断。提出了一种新颖的多模态训练方法，并发布了涵盖推理、检测、分割和文档理解任务的开源数据集。", "result": "Citrus-V在多个基准测试中超越了现有的开源医疗模型和专家级影像系统，提供了一个从视觉定位到临床推理的统一流程，支持精确病灶量化、自动化报告和可靠的第二意见。", "conclusion": "Citrus-V成功地将视觉定位与临床推理相结合，提供了一个统一且高性能的医疗影像分析解决方案，有望在临床诊断中发挥重要作用。"}}
{"id": "2509.18743", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18743", "abs": "https://arxiv.org/abs/2509.18743", "authors": ["Susmit Neogi"], "title": "TriFusion-AE: Language-Guided Depth and LiDAR Fusion for Robust Point Cloud Processing", "comment": "39th Conference on Neural Information Processing Systems (NeurIPS\n  2025) Workshop", "summary": "LiDAR-based perception is central to autonomous driving and robotics, yet raw\npoint clouds remain highly vulnerable to noise, occlusion, and adversarial\ncorruptions. Autoencoders offer a natural framework for denoising and\nreconstruction, but their performance degrades under challenging real-world\nconditions. In this work, we propose TriFusion-AE, a multimodal cross-attention\nautoencoder that integrates textual priors, monocular depth maps from\nmulti-view images, and LiDAR point clouds to improve robustness. By aligning\nsemantic cues from text, geometric (depth) features from images, and spatial\nstructure from LiDAR, TriFusion-AE learns representations that are resilient to\nstochastic noise and adversarial perturbations. Interestingly, while showing\nlimited gains under mild perturbations, our model achieves significantly more\nrobust reconstruction under strong adversarial attacks and heavy noise, where\nCNN-based autoencoders collapse. We evaluate on the nuScenes-mini dataset to\nreflect realistic low-data deployment scenarios. Our multimodal fusion\nframework is designed to be model-agnostic, enabling seamless integration with\nany CNN-based point cloud autoencoder for joint representation learning.", "AI": {"tldr": "本文提出TriFusion-AE，一种多模态交叉注意力自编码器，通过整合文本先验、单目深度图和LiDAR点云，显著提升LiDAR点云在强噪声和对抗性攻击下的重建鲁棒性。", "motivation": "LiDAR点云易受噪声、遮挡和对抗性攻击影响，而现有自编码器在复杂真实世界条件下性能下降，需要更鲁棒的感知框架。", "method": "提出TriFusion-AE，一个多模态交叉注意力自编码器。它整合了文本的语义线索、图像的几何（深度）特征和LiDAR的空间结构。通过对齐这些多模态信息，模型学习对随机噪声和对抗性扰动具有弹性的表示。该框架是模型无关的，可与任何基于CNN的点云自编码器集成。", "result": "在轻微扰动下，模型性能提升有限；但在强对抗性攻击和重度噪声下，其重建鲁棒性显著优于传统基于CNN的自编码器（后者会崩溃）。在nuScenes-mini数据集上进行了评估。", "conclusion": "TriFusion-AE通过有效融合文本、图像深度和LiDAR数据，显著增强了LiDAR点云在恶劣条件（如强噪声和对抗性攻击）下的重建鲁棒性，为自动驾驶和机器人技术提供了更可靠的感知基础。"}}
{"id": "2509.19070", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.19070", "abs": "https://arxiv.org/abs/2509.19070", "authors": ["Zijian Ling", "Han Zhang", "Yazhuo Zhou", "Jiahao Cui"], "title": "ColorBlindnessEval: Can Vision-Language Models Pass Color Blindness Tests?", "comment": "Accepted at the Open Science for Foundation Models (SCI-FM) Workshop\n  at ICLR 2025", "summary": "This paper presents ColorBlindnessEval, a novel benchmark designed to\nevaluate the robustness of Vision-Language Models (VLMs) in visually\nadversarial scenarios inspired by the Ishihara color blindness test. Our\ndataset comprises 500 Ishihara-like images featuring numbers from 0 to 99 with\nvarying color combinations, challenging VLMs to accurately recognize numerical\ninformation embedded in complex visual patterns. We assess 9 VLMs using Yes/No\nand open-ended prompts and compare their performance with human participants.\nOur experiments reveal limitations in the models' ability to interpret numbers\nin adversarial contexts, highlighting prevalent hallucination issues. These\nfindings underscore the need to improve the robustness of VLMs in complex\nvisual environments. ColorBlindnessEval serves as a valuable tool for\nbenchmarking and improving the reliability of VLMs in real-world applications\nwhere accuracy is critical.", "AI": {"tldr": "本文提出了ColorBlindnessEval，一个受石原色盲测试启发的视觉对抗场景基准，旨在评估视觉语言模型（VLMs）的鲁棒性。", "motivation": "研究动机是评估VLMs在视觉对抗场景中的鲁棒性，特别是其识别复杂视觉模式中嵌入的数字信息的能力。现有模型可能在这些复杂环境中表现出局限性。", "method": "研究方法包括：1) 创建ColorBlindnessEval数据集，包含500张类石原图片，嵌入0-99的数字，采用不同颜色组合；2) 使用是/否和开放式提示评估9个VLM；3) 将模型性能与人类参与者进行比较。", "result": "实验结果表明，模型在对抗性环境中解释数字的能力存在局限性，并普遍存在幻觉问题。这突显了模型在复杂视觉环境中的不足。", "conclusion": "研究结论强调了提高VLM在复杂视觉环境中鲁棒性的必要性。ColorBlindnessEval是一个有价值的工具，可用于基准测试和提高VLM在准确性至关重要的实际应用中的可靠性。"}}
{"id": "2509.19165", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19165", "abs": "https://arxiv.org/abs/2509.19165", "authors": ["Yun Wang", "Junjie Hu", "Junhui Hou", "Chenghao Zhang", "Renwei Yang", "Dapeng Oliver Wu"], "title": "RoSe: Robust Self-supervised Stereo Matching under Adverse Weather Conditions", "comment": null, "summary": "Recent self-supervised stereo matching methods have made significant\nprogress, but their performance significantly degrades under adverse weather\nconditions such as night, rain, and fog. We identify two primary weaknesses\ncontributing to this performance degradation. First, adverse weather introduces\nnoise and reduces visibility, making CNN-based feature extractors struggle with\ndegraded regions like reflective and textureless areas. Second, these degraded\nregions can disrupt accurate pixel correspondences, leading to ineffective\nsupervision based on the photometric consistency assumption. To address these\nchallenges, we propose injecting robust priors derived from the visual\nfoundation model into the CNN-based feature extractor to improve feature\nrepresentation under adverse weather conditions. We then introduce scene\ncorrespondence priors to construct robust supervisory signals rather than\nrelying solely on the photometric consistency assumption. Specifically, we\ncreate synthetic stereo datasets with realistic weather degradations. These\ndatasets feature clear and adverse image pairs that maintain the same semantic\ncontext and disparity, preserving the scene correspondence property. With this\nknowledge, we propose a robust self-supervised training paradigm, consisting of\ntwo key steps: robust self-supervised scene correspondence learning and adverse\nweather distillation. Both steps aim to align underlying scene results from\nclean and adverse image pairs, thus improving model disparity estimation under\nadverse weather effects. Extensive experiments demonstrate the effectiveness\nand versatility of our proposed solution, which outperforms existing\nstate-of-the-art self-supervised methods. Codes are available at\n\\textcolor{blue}{https://github.com/cocowy1/RoSe-Robust-Self-supervised-Stereo-Matching-under-Adverse-Weather-Conditions}.", "AI": {"tldr": "该论文提出了一种在恶劣天气下进行鲁棒自监督立体匹配的方法，通过引入视觉基础模型先验和场景对应先验，并设计了一个新的训练范式，显著提升了模型性能。", "motivation": "现有的自监督立体匹配方法在恶劣天气（如夜晚、雨、雾）下性能显著下降。主要原因有两点：一是恶劣天气引入的噪声和能见度降低使CNN特征提取器难以处理反光和无纹理区域；二是这些退化区域会破坏准确的像素对应，导致基于光度一致性假设的监督信号失效。", "method": "本文提出以下方法：1) 将来自视觉基础模型的鲁棒先验注入到CNN特征提取器中，以改善恶劣天气下的特征表示。2) 引入场景对应先验来构建鲁棒的监督信号，而非仅仅依赖光度一致性假设。3) 创建了合成立体数据集，包含具有真实天气退化的清晰和恶劣图像对，这些图像对保持相同的语义上下文和视差，从而保留了场景对应属性。4) 提出了一种鲁棒的自监督训练范式，包括两个关键步骤：鲁棒自监督场景对应学习和恶劣天气蒸馏。这两个步骤都旨在对齐来自清晰和恶劣图像对的底层场景结果。", "result": "广泛的实验证明了所提出解决方案的有效性和通用性，其性能优于现有最先进的自监督方法。", "conclusion": "通过结合视觉基础模型的鲁棒先验和场景对应先验，并设计了一个新的自监督训练范式，本文提出的解决方案能够显著提高立体匹配模型在恶劣天气条件下的视差估计精度和鲁棒性。"}}
{"id": "2509.18759", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18759", "abs": "https://arxiv.org/abs/2509.18759", "authors": ["Zhaorui Wang", "Yi Gu", "Deming Zhou", "Renjing Xu"], "title": "FixingGS: Enhancing 3D Gaussian Splatting via Training-Free Score Distillation", "comment": null, "summary": "Recently, 3D Gaussian Splatting (3DGS) has demonstrated remarkable success in\n3D reconstruction and novel view synthesis. However, reconstructing 3D scenes\nfrom sparse viewpoints remains highly challenging due to insufficient visual\ninformation, which results in noticeable artifacts persisting across the 3D\nrepresentation. To address this limitation, recent methods have resorted to\ngenerative priors to remove artifacts and complete missing content in\nunder-constrained areas. Despite their effectiveness, these approaches struggle\nto ensure multi-view consistency, resulting in blurred structures and\nimplausible details. In this work, we propose FixingGS, a training-free method\nthat fully exploits the capabilities of the existing diffusion model for\nsparse-view 3DGS reconstruction enhancement. At the core of FixingGS is our\ndistillation approach, which delivers more accurate and cross-view coherent\ndiffusion priors, thereby enabling effective artifact removal and inpainting.\nIn addition, we propose an adaptive progressive enhancement scheme that further\nrefines reconstructions in under-constrained regions. Extensive experiments\ndemonstrate that FixingGS surpasses existing state-of-the-art methods with\nsuperior visual quality and reconstruction performance. Our code will be\nreleased publicly.", "AI": {"tldr": "针对稀疏视角下3D Gaussian Splatting (3DGS) 重建存在的伪影和多视角不一致性问题，本文提出FixingGS，一种免训练方法。它通过蒸馏现有扩散模型获取更准确和跨视角一致的先验，并结合自适应渐进增强方案，有效去除伪影并提升重建质量。", "motivation": "3DGS在稀疏视角下的3D场景重建仍具挑战性，因视觉信息不足导致伪影。现有利用生成先验的方法虽能去除伪影和补全内容，但难以保证多视角一致性，造成结构模糊和细节不真实。", "method": "本文提出FixingGS，一种免训练方法，充分利用现有扩散模型的能力来增强稀疏视角3DGS重建。核心方法是蒸馏，它能提供更准确且跨视角一致的扩散先验，从而有效去除伪影和进行内容修复。此外，还引入了自适应渐进增强方案，进一步优化约束不足区域的重建。", "result": "广泛的实验证明，FixingGS在视觉质量和重建性能上均超越了现有的最先进方法。", "conclusion": "FixingGS通过其独特的蒸馏方法和自适应渐进增强方案，成功解决了稀疏视角3DGS重建中的伪影和不一致性问题，显著提升了重建的视觉质量和性能。"}}
{"id": "2509.19218", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19218", "abs": "https://arxiv.org/abs/2509.19218", "authors": ["Yunzhi Xu", "Yushuang Ding", "Hu Sun", "Hongxi Zhang", "Li Zhao"], "title": "HyKid: An Open MRI Dataset with Expert-Annotated Multi-Structure and Choroid Plexus in Pediatric Hydrocephalus", "comment": "10 pages, 7 figures", "summary": "Evaluation of hydrocephalus in children is challenging, and the related\nresearch is limited by a lack of publicly available, expert-annotated datasets,\nparticularly those with segmentation of the choroid plexus. To address this, we\npresent HyKid, an open-source dataset from 48 pediatric patients with\nhydrocephalus. 3D MRIs were provided with 1mm isotropic resolution, which was\nreconstructed from routine low-resolution images using a slice-to-volume\nalgorithm. Manually corrected segmentations of brain tissues, including white\nmatter, grey matter, lateral ventricle, external CSF, and the choroid plexus,\nwere provided by an experienced neurologist. Additionally, structured data was\nextracted from clinical radiology reports using a Retrieval-Augmented\nGeneration framework. The strong correlation between choroid plexus volume and\ntotal CSF volume provided a potential biomarker for hydrocephalus evaluation,\nachieving excellent performance in a predictive model (AUC = 0.87). The\nproposed HyKid dataset provided a high-quality benchmark for neuroimaging\nalgorithms development, and it revealed the choroid plexus-related features in\nhydrocephalus assessments. Our datasets are publicly available at\nhttps://www.synapse.org/Synapse:syn68544889.", "AI": {"tldr": "该研究提出了HyKid数据集，这是一个包含48名儿童脑积水患者的开放获取、专家标注的3D MRI数据集，特别包含脉络丛分割，并发现脉络丛体积与脑脊液总体积之间存在强相关性，可作为脑积水评估的潜在生物标志物。", "motivation": "儿童脑积水评估具有挑战性，且相关研究受限于缺乏公开可用的、专家标注的数据集，特别是缺少包含脉络丛分割的数据集。", "method": "研究构建了HyKid数据集，包含48名儿童脑积水患者的3D MRI图像，这些图像通过切片到体积算法从低分辨率图像重建得到1mm各向同性分辨率。经验丰富的神经科医生提供了包括白质、灰质、侧脑室、外部脑脊液和脉络丛在内的脑组织手动校正分割。此外，利用检索增强生成框架从临床放射学报告中提取了结构化数据。", "result": "脉络丛体积与脑脊液总体积之间存在显著相关性，这为脑积水评估提供了一个潜在的生物标志物。基于此的预测模型表现出色（AUC = 0.87）。所提出的HyKid数据集为神经影像算法的开发提供了高质量的基准，并揭示了脉络丛相关特征在脑积水评估中的价值。", "conclusion": "HyKid数据集为脑积水评估和神经影像算法开发提供了宝贵的资源，并强调了脉络丛相关特征在脑积水评估中的重要性，有望成为新的生物标志物。"}}
{"id": "2509.18763", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18763", "abs": "https://arxiv.org/abs/2509.18763", "authors": ["Xijun Wang", "Junyun Huang", "Rayyan Abdalla", "Chengyuan Zhang", "Ruiqi Xian", "Dinesh Manocha"], "title": "Bi-VLM: Pushing Ultra-Low Precision Post-Training Quantization Boundaries in Vision-Language Models", "comment": null, "summary": "We address the critical gap between the computational demands of\nvision-language models and the possible ultra-low-bit weight precision\n(bitwidth $\\leq2$ bits) we can use for higher efficiency. Our work is motivated\nby the substantial computational cost and memory requirements of VLMs, which\nrestrict their applicability in hardware-constrained environments. We propose\nBi-VLM, which separates model weights non-uniformly based on the Gaussian\nquantiles. Our formulation groups the model weights into outlier (salient) and\nmultiple inlier (unsalient) subsets, ensuring that each subset contains a\nproportion of weights corresponding to its quantile in the distribution. We\npropose a saliency-aware hybrid quantization algorithm and use it to quantize\nweights by imposing different constraints on the scaler and binary matrices\nbased on the saliency metric and compression objective. We have evaluated our\napproach on different VLMs. For the language model part of the VLM, our Bi-VLM\noutperforms the SOTA by 3%-47% on the visual question answering task in terms\nof four different benchmarks and three different models. For the overall VLM,\nour Bi-VLM outperforms the SOTA by 4%-45%. We also perform token pruning on the\nquantized models and observe that there is redundancy of image tokens 90% - 99%\nin the quantized models. This helps us to further prune the visual tokens to\nimprove efficiency.", "AI": {"tldr": "该研究提出Bi-VLM，通过基于高斯分位数的非均匀权重分离和显著性感知混合量化算法，实现视觉-语言模型（VLM）的超低比特量化，显著提升效率和性能，并发现量化模型中图像token存在冗余。", "motivation": "视觉-语言模型（VLM）的巨大计算成本和内存需求限制了其在硬件受限环境中的应用，因此需要寻找能大幅提高效率的超低比特权重精度（比特宽度≤2比特）方法。", "method": "研究提出了Bi-VLM，其核心方法包括：1) 基于高斯分位数非均匀地分离模型权重，将其分为异常值（显著）和多个正常值（非显著）子集，确保每个子集包含与其分布分位数相对应的权重比例。2) 提出一种显著性感知混合量化算法，根据显著性指标和压缩目标，对标量和二值矩阵施加不同约束来量化权重。3) 对量化后的模型进行token剪枝，以进一步提高效率。", "result": "实验结果表明：1) 在VLM的语言模型部分，Bi-VLM在视觉问答任务上，相对于现有最先进技术（SOTA），在四种不同基准和三种不同模型上性能提升了3%-47%。2) 对于整体VLM，Bi-VLM的性能优于SOTA 4%-45%。3) 在量化模型上进行token剪枝后，发现图像token存在90%-99%的冗余，这有助于进一步剪枝视觉token以提高效率。", "conclusion": "Bi-VLM成功弥补了VLM计算需求与超低比特权重精度之间的差距，通过其独特的非均匀权重分离和显著性感知混合量化算法，显著提升了VLM在硬件受限环境中的效率和性能。此外，研究还揭示了量化模型中图像token的显著冗余，为未来的效率优化提供了方向。"}}
{"id": "2509.19227", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19227", "abs": "https://arxiv.org/abs/2509.19227", "authors": ["Tongshuai Wu", "Chao Lu", "Ze Song", "Yunlong Lin", "Sizhe Fan", "Xuemei Chen"], "title": "MsFIN: Multi-scale Feature Interaction Network for Traffic Accident Anticipation", "comment": null, "summary": "With the widespread deployment of dashcams and advancements in computer\nvision, developing accident prediction models from the dashcam perspective has\nbecome critical for proactive safety interventions. However, two key challenges\npersist: modeling feature-level interactions among traffic participants (often\noccluded in dashcam views) and capturing complex, asynchronous multi-temporal\nbehavioral cues preceding accidents. To deal with these two challenges, a\nMulti-scale Feature Interaction Network (MsFIN) is proposed for early-stage\naccident anticipation from dashcam videos. MsFIN has three layers for\nmulti-scale feature aggregation, temporal feature processing and multi-scale\nfeature post fusion, respectively. For multi-scale feature aggregation, a\nMulti-scale Module is designed to extract scene representations at short-term,\nmid-term and long-term temporal scales. Meanwhile, the Transformer architecture\nis leveraged to facilitate comprehensive feature interactions. Temporal feature\nprocessing captures the sequential evolution of scene and object features under\ncausal constraints. In the multi-scale feature post fusion stage, the network\nfuses scene and object features across multiple temporal scales to generate a\ncomprehensive risk representation. Experiments on DAD and DADA datasets show\nthat MsFIN significantly outperforms state-of-the-art models with single-scale\nfeature extraction in both prediction correctness and earliness. Ablation\nstudies validate the effectiveness of each module in MsFIN, highlighting how\nthe network achieves superior performance through multi-scale feature fusion\nand contextual interaction modeling.", "AI": {"tldr": "该论文提出了一种名为多尺度特征交互网络（MsFIN）的模型，用于从行车记录仪视频中进行早期事故预测，旨在解决交通参与者特征交互和多时间尺度行为线索捕捉的挑战。", "motivation": "当前的事故预测模型面临两大挑战：一是难以建模交通参与者（在行车记录仪视角下常被遮挡）之间的特征级交互；二是难以捕捉事故发生前复杂、异步的多时间尺度行为线索。", "method": "论文提出MsFIN网络，包含多尺度特征聚合、时序特征处理和多尺度特征后融合三个层。多尺度特征聚合层设计了一个多尺度模块，用于提取短、中、长期时间尺度的场景表示，并利用Transformer架构促进全面的特征交互。时序特征处理层捕获场景和物体特征在因果约束下的序列演变。多尺度特征后融合层则融合多时间尺度的场景和物体特征，以生成全面的风险表示。", "result": "在DAD和DADA数据集上的实验结果表明，MsFIN在预测正确性和提前性方面显著优于现有采用单尺度特征提取的先进模型。消融研究也验证了MsFIN中每个模块的有效性。", "conclusion": "MsFIN通过多尺度特征融合和上下文交互建模，实现了卓越的性能，成功解决了行车记录仪视频中早期事故预测的关键挑战。"}}
{"id": "2509.18779", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18779", "abs": "https://arxiv.org/abs/2509.18779", "authors": ["Hemanth Puppala", "Wayne Sarasua", "Srinivas Biyaguda", "Farhad Farzinpour", "Mashrur Chowdhury"], "title": "Real-time Deer Detection and Warning in Connected Vehicles via Thermal Sensing and Deep Learning", "comment": "Preprint under review in TRR, 20 pages, 9 figures, 4 tables", "summary": "Deer-vehicle collisions represent a critical safety challenge in the United\nStates, causing nearly 2.1 million incidents annually and resulting in\napproximately 440 fatalities, 59,000 injuries, and 10 billion USD in economic\ndamages. These collisions also contribute significantly to declining deer\npopulations. This paper presents a real-time detection and driver warning\nsystem that integrates thermal imaging, deep learning, and\nvehicle-to-everything communication to help mitigate deer-vehicle collisions.\nOur system was trained and validated on a custom dataset of over 12,000 thermal\ndeer images collected in Mars Hill, North Carolina. Experimental evaluation\ndemonstrates exceptional performance with 98.84 percent mean average precision,\n95.44 percent precision, and 95.96 percent recall. The system was field tested\nduring a follow-up visit to Mars Hill and readily sensed deer providing the\ndriver with advanced warning. Field testing validates robust operation across\ndiverse weather conditions, with thermal imaging maintaining between 88 and 92\npercent detection accuracy in challenging scenarios where conventional visible\nlight based cameras achieve less than 60 percent effectiveness. When a high\nprobability threshold is reached sensor data sharing messages are broadcast to\nsurrounding vehicles and roadside units via cellular vehicle to everything\n(CV2X) communication devices. Overall, our system achieves end to end latency\nconsistently under 100 milliseconds from detection to driver alert. This\nresearch establishes a viable technological pathway for reducing deer-vehicle\ncollisions through thermal imaging and connected vehicles.", "AI": {"tldr": "本文提出了一种结合热成像、深度学习和车联网通信的实时鹿检测及驾驶员预警系统，旨在有效减少鹿车碰撞事故。", "motivation": "美国每年发生约210万起鹿车碰撞事故，导致440人死亡、5.9万人受伤、100亿美元经济损失，并对鹿群数量造成显著影响，因此急需解决方案来缓解这一严峻的安全挑战。", "method": "该系统整合了热成像技术、深度学习算法和车联网（V2X）通信。它在一个包含超过12,000张热成像鹿图像的定制数据集上进行了训练和验证。系统通过蜂窝车联网（CV2X）设备将传感器数据共享消息广播给周围车辆和路边单元，并在检测到高概率碰撞风险时向驾驶员发出警报。", "result": "系统表现出色，平均精度达到98.84%，精确度95.44%，召回率95.96%。在实地测试中，它能提供预警，并在恶劣天气条件下保持88%至92%的检测准确率，远优于可见光摄像头（低于60%）。从检测到驾驶员警报的端到端延迟持续低于100毫秒。", "conclusion": "该研究通过热成像和互联车辆，为减少鹿车碰撞事故提供了一条可行的技术途径。"}}
{"id": "2509.19252", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19252", "abs": "https://arxiv.org/abs/2509.19252", "authors": ["Gabriel Maldonado", "Narges Rashvand", "Armin Danesh Pazho", "Ghazal Alinezhad Noghre", "Vinit Katariya", "Hamed Tabkhi"], "title": "Adversarially-Refined VQ-GAN with Dense Motion Tokenization for Spatio-Temporal Heatmaps", "comment": null, "summary": "Continuous human motion understanding remains a core challenge in computer\nvision due to its high dimensionality and inherent redundancy. Efficient\ncompression and representation are crucial for analyzing complex motion\ndynamics. In this work, we introduce an adversarially-refined VQ-GAN framework\nwith dense motion tokenization for compressing spatio-temporal heatmaps while\npreserving the fine-grained traces of human motion. Our approach combines dense\nmotion tokenization with adversarial refinement, which eliminates\nreconstruction artifacts like motion smearing and temporal misalignment\nobserved in non-adversarial baselines. Our experiments on the CMU Panoptic\ndataset provide conclusive evidence of our method's superiority, outperforming\nthe dVAE baseline by 9.31% SSIM and reducing temporal instability by 37.1%.\nFurthermore, our dense tokenization strategy enables a novel analysis of motion\ncomplexity, revealing that 2D motion can be optimally represented with a\ncompact 128-token vocabulary, while 3D motion's complexity demands a much\nlarger 1024-token codebook for faithful reconstruction. These results establish\npractical deployment feasibility across diverse motion analysis applications.\nThe code base for this work is available at\nhttps://github.com/TeCSAR-UNCC/Pose-Quantization.", "AI": {"tldr": "本文提出了一种对抗性优化的VQ-GAN框架，结合密集运动标记化，用于压缩时空人体运动热图，显著提高了重建质量和时间稳定性，并揭示了2D和3D运动的最佳词汇量大小。", "motivation": "连续人体运动理解在计算机视觉中因其高维度和固有冗余性而面临核心挑战。高效的压缩和表示对于分析复杂运动动力学至关重要。", "method": "引入了一个对抗性优化的VQ-GAN框架，并结合密集运动标记化，用于压缩时空热图。该方法通过对抗性优化消除了重建伪影，如运动模糊和时间错位。", "result": "在CMU Panoptic数据集上的实验表明，该方法优于dVAE基线，SSIM提高了9.31%，时间不稳定性降低了37.1%。密集标记化策略还揭示，2D运动可由128个标记的词汇表最佳表示，而3D运动的复杂性需要1024个标记的更大码本进行忠实重建。", "conclusion": "该方法在人体运动理解和表示方面表现出卓越性能，具有实际部署的可行性，并为不同运动分析应用提供了新的见解，特别是关于2D和3D运动的最佳表示复杂性。"}}
{"id": "2509.18796", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18796", "abs": "https://arxiv.org/abs/2509.18796", "authors": ["Danush Kumar Venkatesh", "Stefanie Speidel"], "title": "Towards Application Aligned Synthetic Surgical Image Synthesis", "comment": null, "summary": "The scarcity of annotated surgical data poses a significant challenge for\ndeveloping deep learning systems in computer-assisted interventions. While\ndiffusion models can synthesize realistic images, they often suffer from data\nmemorization, resulting in inconsistent or non-diverse samples that may fail to\nimprove, or even harm, downstream performance. We introduce \\emph{Surgical\nApplication-Aligned Diffusion} (SAADi), a new framework that aligns diffusion\nmodels with samples preferred by downstream models. Our method constructs pairs\nof \\emph{preferred} and \\emph{non-preferred} synthetic images and employs\nlightweight fine-tuning of diffusion models to align the image generation\nprocess with downstream objectives explicitly. Experiments on three surgical\ndatasets demonstrate consistent gains of $7$--$9\\%$ in classification and\n$2$--$10\\%$ in segmentation tasks, with the considerable improvements observed\nfor underrepresented classes. Iterative refinement of synthetic samples further\nboosts performance by $4$--$10\\%$. Unlike baseline approaches, our method\novercomes sample degradation and establishes task-aware alignment as a key\nprinciple for mitigating data scarcity and advancing surgical vision\napplications.", "AI": {"tldr": "SAADi是一种新的扩散模型框架，通过将合成图像与下游模型偏好对齐，有效缓解了外科手术数据稀缺问题，显著提升了分类和分割任务的性能，尤其对于代表性不足的类别。", "motivation": "带注释的外科手术数据稀缺是开发深度学习系统的主要挑战。现有扩散模型虽然能生成逼真图像，但常出现数据记忆问题，导致生成样本不一致或缺乏多样性，可能无法改善甚至损害下游模型的性能。", "method": "该研究引入了“外科应用对齐扩散”（SAADi）框架。它通过构建“偏好”和“非偏好”合成图像对，并对扩散模型进行轻量级微调，使图像生成过程与下游目标明确对齐。此外，还采用了合成样本的迭代细化。", "result": "在三个外科手术数据集上，分类任务实现了7-9%的持续增益，分割任务实现了2-10%的持续增益，尤其在代表性不足的类别上观察到显著改进。合成样本的迭代细化进一步将性能提升了4-10%。与基线方法不同，SAADi克服了样本退化问题。", "conclusion": "SAADi通过建立任务感知对齐作为关键原则，有效缓解了数据稀缺问题，并推动了外科视觉应用的发展，成功解决了现有方法中样本退化的问题。"}}
{"id": "2509.18802", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18802", "abs": "https://arxiv.org/abs/2509.18802", "authors": ["Garam Kim", "Tae Kyeong Jeong", "Juyoun Park"], "title": "Surgical Video Understanding with Label Interpolation", "comment": "8 pages, 10 figures", "summary": "Robot-assisted surgery (RAS) has become a critical paradigm in modern\nsurgery, promoting patient recovery and reducing the burden on surgeons through\nminimally invasive approaches. To fully realize its potential, however, a\nprecise understanding of the visual data generated during surgical procedures\nis essential. Previous studies have predominantly focused on single-task\napproaches, but real surgical scenes involve complex temporal dynamics and\ndiverse instrument interactions that limit comprehensive understanding.\nMoreover, the effective application of multi-task learning (MTL) requires\nsufficient pixel-level segmentation data, which are difficult to obtain due to\nthe high cost and expertise required for annotation. In particular, long-term\nannotations such as phases and steps are available for every frame, whereas\nshort-term annotations such as surgical instrument segmentation and action\ndetection are provided only for key frames, resulting in a significant\ntemporal-spatial imbalance. To address these challenges, we propose a novel\nframework that combines optical flow-based segmentation label interpolation\nwith multi-task learning. optical flow estimated from annotated key frames is\nused to propagate labels to adjacent unlabeled frames, thereby enriching sparse\nspatial supervision and balancing temporal and spatial information for\ntraining. This integration improves both the accuracy and efficiency of\nsurgical scene understanding and, in turn, enhances the utility of RAS.", "AI": {"tldr": "为解决机器人辅助手术中视觉数据理解的挑战，本文提出了一种结合光流分割标签插值与多任务学习的新框架，以应对标注稀疏和时空不平衡问题，从而提高手术场景理解的准确性和效率。", "motivation": "机器人辅助手术（RAS）需要精确理解手术过程中的视觉数据。现有研究多关注单一任务，但真实手术场景复杂且动态。多任务学习（MTL）需要大量的像素级分割数据，但标注成本高昂且专业性强。特别是，长时程标注（如阶段、步骤）适用于所有帧，而短时程标注（如器械分割、动作检测）仅限于关键帧，导致显著的时空不平衡。", "method": "本文提出一个新颖的框架，将基于光流的分割标签插值与多任务学习相结合。利用从已标注关键帧估计的光流，将标签传播到相邻的未标注帧，以此丰富稀疏的空间监督，并平衡训练所需的时空信息。", "result": "这种集成方法提高了手术场景理解的准确性和效率。", "conclusion": "通过改善手术场景理解，该方法增强了机器人辅助手术的实用性。"}}
{"id": "2509.18824", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18824", "abs": "https://arxiv.org/abs/2509.18824", "authors": ["Yanzuo Lu", "Xin Xia", "Manlin Zhang", "Huafeng Kuang", "Jianbin Zheng", "Yuxi Ren", "Xuefeng Xiao"], "title": "Hyper-Bagel: A Unified Acceleration Framework for Multimodal Understanding and Generation", "comment": "Technical Report", "summary": "Unified multimodal models have recently attracted considerable attention for\ntheir remarkable abilities in jointly understanding and generating diverse\ncontent. However, as contexts integrate increasingly numerous interleaved\nmultimodal tokens, the iterative processes of diffusion denoising and\nautoregressive decoding impose significant computational overhead. To address\nthis, we propose Hyper-Bagel, a unified acceleration framework designed to\nsimultaneously speed up both multimodal understanding and generation tasks. Our\napproach uses a divide-and-conquer strategy, employing speculative decoding for\nnext-token prediction and a multi-stage distillation process for diffusion\ndenoising. The framework delivers substantial performance gains, achieving over\na 2x speedup in multimodal understanding. For generative tasks, our resulting\nlossless 6-NFE model yields a 16.67x speedup in text-to-image generation and a\n22x speedup in image editing, all while preserving the high-quality output of\nthe original model. We further develop a highly efficient 1-NFE model that\nenables near real-time interactive editing and generation. By combining\nadvanced adversarial distillation with human feedback learning, this model\nachieves ultimate cost-effectiveness and responsiveness, making complex\nmultimodal interactions seamless and instantaneous.", "AI": {"tldr": "Hyper-Bagel是一个统一加速框架，通过推测解码和多阶段蒸馏，显著加速了多模态理解和生成任务，同时保持了输出质量。", "motivation": "统一多模态模型在理解和生成多样内容方面表现出色，但扩散去噪和自回归解码的迭代过程在处理大量交错多模态token时会产生显著的计算开销。", "method": "该方法采用分治策略，使用推测解码进行下一token预测，并采用多阶段蒸馏过程进行扩散去噪。此外，还结合了先进的对抗性蒸馏和人类反馈学习，以开发高效的1-NFE模型。", "result": "Hyper-Bagel在多模态理解方面实现了超过2倍的加速。在生成任务中，无损6-NFE模型在文本到图像生成中实现了16.67倍加速，在图像编辑中实现了22倍加速，同时保持了原始模型的高质量输出。高效的1-NFE模型实现了近乎实时的交互式编辑和生成，具备极高的成本效益和响应速度。", "conclusion": "Hyper-Bagel框架成功解决了多模态模型中的计算开销问题，通过显著加速理解和生成任务，同时保持高输出质量，实现了无缝且即时的复杂多模态交互。"}}
{"id": "2509.18839", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18839", "abs": "https://arxiv.org/abs/2509.18839", "authors": ["Gianmarco Spinaci", "Lukas Klic", "Giovanni Colavizza"], "title": "Benchmarking Vision-Language and Multimodal Large Language Models in Zero-shot and Few-shot Scenarios: A study on Christian Iconography", "comment": "11 pages, 2 figures", "summary": "This study evaluates the capabilities of Multimodal Large Language Models\n(LLMs) and Vision Language Models (VLMs) in the task of single-label\nclassification of Christian Iconography. The goal was to assess whether\ngeneral-purpose VLMs (CLIP and SigLIP) and LLMs, such as GPT-4o and Gemini 2.5,\ncan interpret the Iconography, typically addressed by supervised classifiers,\nand evaluate their performance. Two research questions guided the analysis:\n(RQ1) How do multimodal LLMs perform on image classification of Christian\nsaints? And (RQ2), how does performance vary when enriching input with\ncontextual information or few-shot exemplars? We conducted a benchmarking study\nusing three datasets supporting Iconclass natively: ArtDL, ICONCLASS, and\nWikidata, filtered to include the top 10 most frequent classes. Models were\ntested under three conditions: (1) classification using class labels, (2)\nclassification with Iconclass descriptions, and (3) few-shot learning with five\nexemplars. Results were compared against ResNet50 baselines fine-tuned on the\nsame datasets. The findings show that Gemini-2.5 Pro and GPT-4o outperformed\nthe ResNet50 baselines. Accuracy dropped significantly on the Wikidata dataset,\nwhere Siglip reached the highest accuracy score, suggesting model sensitivity\nto image size and metadata alignment. Enriching prompts with class descriptions\ngenerally improved zero-shot performance, while few-shot learning produced\nlower results, with only occasional and minimal increments in accuracy. We\nconclude that general-purpose multimodal LLMs are capable of classification in\nvisually complex cultural heritage domains. These results support the\napplication of LLMs as metadata curation tools in digital humanities workflows,\nsuggesting future research on prompt optimization and the expansion of the\nstudy to other classification strategies and models.", "AI": {"tldr": "本研究评估了多模态大语言模型（MLLMs）和视觉语言模型（VLMs）在基督教圣像单标签分类任务中的能力，发现GPT-4o和Gemini-2.5 Pro表现优于ResNet50基线模型，表明它们在视觉复杂文化遗产领域具有分类潜力。", "motivation": "本研究旨在评估通用型VLMs（如CLIP和SigLIP）和LLMs（如GPT-4o和Gemini 2.5）是否能够解释通常由监督分类器处理的基督教圣像学，并评估它们的性能。", "method": "研究进行了一项基准测试，使用了三个原生支持Iconclass的数据集（ArtDL、ICONCLASS和Wikidata），并筛选出前10个最常见的类别。模型在三种条件下进行测试：(1) 使用类别标签分类，(2) 使用Iconclass描述分类，(3) 使用五个范例进行少样本学习。结果与在相同数据集上微调的ResNet50基线进行了比较。研究回答了两个问题：多模态LLMs在基督教圣像图像分类上的表现如何？以及当输入富含上下文信息或少样本示例时，性能如何变化？", "result": "Gemini-2.5 Pro和GPT-4o的表现优于ResNet50基线。在Wikidata数据集上准确率显著下降，SigLIP在该数据集上达到最高准确率，表明模型对图像大小和元数据对齐的敏感性。用类别描述丰富提示通常能提高零样本性能，而少样本学习则产生了较低的结果，仅偶尔有微小准确率提升。", "conclusion": "通用型多模态LLMs能够对视觉复杂的文化遗产领域进行分类。这些结果支持将LLMs作为数字人文工作流程中的元数据整理工具，并建议未来研究进行提示优化以及将研究扩展到其他分类策略和模型。"}}
{"id": "2509.18840", "categories": ["cs.CV", "I.2.10"], "pdf": "https://arxiv.org/pdf/2509.18840", "abs": "https://arxiv.org/abs/2509.18840", "authors": ["Ismael Elsharkawi", "Hossam Sharara", "Ahmed Rafea"], "title": "ViG-LRGC: Vision Graph Neural Networks with Learnable Reparameterized Graph Construction", "comment": "Under Review", "summary": "Image Representation Learning is an important problem in Computer Vision.\nTraditionally, images were processed as grids, using Convolutional Neural\nNetworks or as a sequence of visual tokens, using Vision Transformers.\nRecently, Vision Graph Neural Networks (ViG) have proposed the treatment of\nimages as a graph of nodes; which provides a more intuitive image\nrepresentation. The challenge is to construct a graph of nodes in each layer\nthat best represents the relations between nodes and does not need a\nhyper-parameter search. ViG models in the literature depend on\nnon-parameterized and non-learnable statistical methods that operate on the\nlatent features of nodes to create a graph. This might not select the best\nneighborhood for each node. Starting from k-NN graph construction to HyperGraph\nConstruction and Similarity-Thresholded graph construction, these methods lack\nthe ability to provide a learnable hyper-parameter-free graph construction\nmethod. To overcome those challenges, we present the Learnable Reparameterized\nGraph Construction (LRGC) for Vision Graph Neural Networks. LRGC applies\nkey-query attention between every pair of nodes; then uses soft-threshold\nreparameterization for edge selection, which allows the use of a differentiable\nmathematical model for training. Using learnable parameters to select the\nneighborhood removes the bias that is induced by any clustering or thresholding\nmethods previously introduced in the literature. In addition, LRGC allows\ntuning the threshold in each layer to the training data since the thresholds\nare learnable through training and are not provided as hyper-parameters to the\nmodel. We demonstrate that the proposed ViG-LRGC approach outperforms\nstate-of-the-art ViG models of similar sizes on the ImageNet-1k benchmark\ndataset.", "AI": {"tldr": "本文提出了一种名为可学习重参数化图构建（LRGC）的新方法，用于视觉图神经网络（ViG）中的图像表示学习。LRGC通过键-查询注意力机制和可微分的软阈值重参数化实现可学习、无需超参数的图构建，解决了现有ViG模型图构建方法的局限性，并在ImageNet-1k数据集上超越了现有先进的ViG模型。", "motivation": "图像表示学习是计算机视觉中的一个重要问题。虽然ViG将图像视为节点图提供了一种更直观的表示，但现有的ViG模型在图构建方面存在挑战。它们依赖于非参数化、不可学习的统计方法（如k-NN、超图构建等），这些方法可能无法选择最佳的节点邻域，缺乏提供可学习、无超参数图构建的能力，并且需要超参数搜索。", "method": "本文提出了可学习重参数化图构建（LRGC）方法。LRGC在每对节点之间应用键-查询注意力机制，然后利用软阈值重参数化进行边选择，这使得训练能够使用可微分的数学模型。通过可学习参数来选择邻域，消除了传统聚类或阈值方法引入的偏差。此外，LRGC允许在每个层中根据训练数据调整阈值，因为阈值是通过训练学习的，而不是作为超参数提供给模型。", "result": "所提出的ViG-LRGC方法在ImageNet-1k基准数据集上，性能优于同等规模的现有最先进的ViG模型。", "conclusion": "LRGC成功解决了ViG中图构建的挑战，提供了一种可学习、无超参数的图构建方法。它通过键-查询注意力机制和软阈值重参数化实现了可微分的训练，并允许每层自适应地学习阈值，从而提高了图像表示学习的性能。"}}
{"id": "2509.18891", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18891", "abs": "https://arxiv.org/abs/2509.18891", "authors": ["Xueyu Liu", "Xiaoyi Zhang", "Guangze Shi", "Meilin Liu", "Yexin Lai", "Yongfei Wu", "Mingqiang Wei"], "title": "Attack for Defense: Adversarial Agents for Point Prompt Optimization Empowering Segment Anything Model", "comment": null, "summary": "Prompt quality plays a critical role in the performance of the Segment\nAnything Model (SAM), yet existing approaches often rely on heuristic or\nmanually crafted prompts, limiting scalability and generalization. In this\npaper, we propose Point Prompt Defender, an adversarial reinforcement learning\nframework that adopts an attack-for-defense paradigm to automatically optimize\npoint prompts. We construct a task-agnostic point prompt environment by\nrepresenting image patches as nodes in a dual-space graph, where edges encode\nboth physical and semantic distances. Within this environment, an attacker\nagent learns to activate a subset of prompts that maximally degrade SAM's\nsegmentation performance, while a defender agent learns to suppress these\ndisruptive prompts and restore accuracy. Both agents are trained using Deep\nQ-Networks with a reward signal based on segmentation quality variation. During\ninference, only the defender is deployed to refine arbitrary coarse prompt\nsets, enabling enhanced SAM segmentation performance across diverse tasks\nwithout retraining. Extensive experiments show that Point Prompt Defender\neffectively improves SAM's robustness and generalization, establishing a\nflexible, interpretable, and plug-and-play framework for prompt-based\nsegmentation.", "AI": {"tldr": "本文提出Point Prompt Defender，一个对抗性强化学习框架，通过攻击-防御范式自动优化点提示，以提升Segment Anything Model (SAM)的分割性能、鲁棒性和泛化能力。", "motivation": "SAM的性能严重依赖提示质量，但现有方法多依赖启发式或手动制作的提示，限制了其可扩展性和泛化性。", "method": "构建了一个任务无关的点提示环境，将图像块表示为双空间图中的节点。攻击者智能体学习激活最大程度降低SAM分割性能的提示子集，而防御者智能体学习抑制这些破坏性提示并恢复准确性。两个智能体都使用Deep Q-Networks进行训练，奖励信号基于分割质量的变化。推理时只部署防御者来优化粗略提示集。", "result": "Point Prompt Defender有效提高了SAM的鲁棒性和泛化能力，在不重新训练的情况下增强了SAM在各种任务中的分割性能。", "conclusion": "该框架为基于提示的分割提供了一个灵活、可解释且即插即用的解决方案。"}}
{"id": "2509.18894", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18894", "abs": "https://arxiv.org/abs/2509.18894", "authors": ["Jenna Kline", "Anirudh Potlapally", "Bharath Pillai", "Tanishka Wani", "Rugved Katole", "Vedant Patil", "Penelope Covey", "Hari Subramoni", "Tanya Berger-Wolf", "Christopher Stewart"], "title": "SmartWilds: Multimodal Wildlife Monitoring Dataset", "comment": "8 pages", "summary": "We present the first release of SmartWilds, a multimodal wildlife monitoring\ndataset. SmartWilds is a synchronized collection of drone imagery, camera trap\nphotographs and videos, and bioacoustic recordings collected during summer 2025\nat The Wilds safari park in Ohio. This dataset supports multimodal AI research\nfor comprehensive environmental monitoring, addressing critical needs in\nendangered species research, conservation ecology, and habitat management. Our\npilot deployment captured four days of synchronized monitoring across three\nmodalities in a 220-acre pasture containing Pere David's deer, Sichuan takin,\nPrzewalski's horses, as well as species native to Ohio, including bald eagles,\nwhite-tailed deer, and coyotes. We provide a comparative analysis of sensor\nmodality performance, demonstrating complementary strengths for landuse\npatterns, species detection, behavioral analysis, and habitat monitoring. This\nwork establishes reproducible protocols for multimodal wildlife monitoring\nwhile contributing open datasets to advance conservation computer vision\nresearch. Future releases will include synchronized GPS tracking data from\ntagged individuals, citizen science data, and expanded temporal coverage across\nmultiple seasons.", "AI": {"tldr": "SmartWilds是首个多模态野生动物监测数据集，同步整合了无人机图像、相机陷阱照片/视频和生物声学记录，旨在支持全面的环境监测和保护AI研究。", "motivation": "现有研究在濒危物种、生态保护和栖息地管理方面存在关键需求，需要更全面的环境监测方法，而多模态AI研究可以提供支持。", "method": "数据集于2025年夏季在俄亥俄州The Wilds野生动物园收集，进行为期四天的同步监测。它在220英亩的牧场上，通过无人机影像、相机陷阱（照片和视频）以及生物声学记录三种模态，捕捉了多种动物的数据。", "result": "研究展示了不同传感器模态在土地利用模式、物种检测、行为分析和栖息地监测方面的互补优势。同时，建立了可复现的多模态野生动物监测协议，并贡献了开放数据集。", "conclusion": "SmartWilds数据集及其协议为推进保护计算机视觉研究做出了贡献。未来的版本将进一步扩展，包括GPS追踪数据、公民科学数据和更长的监测时间跨度。"}}
{"id": "2509.18897", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18897", "abs": "https://arxiv.org/abs/2509.18897", "authors": ["Jiayu Wang", "Ruizhi Wang", "Jie Song", "Haofei Zhang", "Mingli Song", "Zunlei Feng", "Li Sun"], "title": "RS3DBench: A Comprehensive Benchmark for 3D Spatial Perception in Remote Sensing", "comment": "26 pages, 4 figures", "summary": "In this paper, we introduce a novel benchmark designed to propel the\nadvancement of general-purpose, large-scale 3D vision models for remote sensing\nimagery. While several datasets have been proposed within the realm of remote\nsensing, many existing collections either lack comprehensive depth information\nor fail to establish precise alignment between depth data and remote sensing\nimages. To address this deficiency, we present a visual Benchmark for 3D\nunderstanding of Remotely Sensed images, dubbed RS3DBench. This dataset\nencompasses 54,951 pairs of remote sensing images and pixel-level aligned depth\nmaps, accompanied by corresponding textual descriptions, spanning a broad array\nof geographical contexts. It serves as a tool for training and assessing 3D\nvisual perception models within remote sensing image spatial understanding\ntasks. Furthermore, we introduce a remotely sensed depth estimation model\nderived from stable diffusion, harnessing its multimodal fusion capabilities,\nthereby delivering state-of-the-art performance on our dataset. Our endeavor\nseeks to make a profound contribution to the evolution of 3D visual perception\nmodels and the advancement of geographic artificial intelligence within the\nremote sensing domain. The dataset, models and code will be accessed on the\nhttps://rs3dbench.github.io.", "AI": {"tldr": "本文提出了一个名为RS3DBench的新型遥感图像3D理解基准数据集，包含大量对齐的遥感图像和深度图，并引入了一个基于稳定扩散的遥感深度估计模型，在该数据集上取得了最先进的性能。", "motivation": "现有的遥感数据集普遍缺乏全面的深度信息，或者深度数据与遥感图像的对齐精度不足，阻碍了通用、大规模遥感3D视觉模型的发展。", "method": "研究者构建了RS3DBench数据集，包含54,951对像素级对齐的遥感图像和深度图，并附带文本描述。此外，他们还提出了一个利用稳定扩散多模态融合能力的遥感深度估计模型。", "result": "RS3DBench数据集为遥感图像空间理解任务中的3D视觉感知模型训练和评估提供了工具。所提出的基于稳定扩散的深度估计模型在该数据集上实现了最先进的性能。", "conclusion": "这项工作旨在为遥感领域3D视觉感知模型的发展和地理人工智能的进步做出重要贡献，并提供了数据集、模型和代码供访问。"}}
{"id": "2509.18898", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18898", "abs": "https://arxiv.org/abs/2509.18898", "authors": ["Pengteng Li", "Yunfan Lu", "Pinhao Song", "Weiyu Guo", "Huizai Yao", "F. Richard Yu", "Hui Xiong"], "title": "DeblurSplat: SfM-free 3D Gaussian Splatting with Event Camera for Robust Deblurring", "comment": null, "summary": "In this paper, we propose the first Structure-from-Motion (SfM)-free\ndeblurring 3D Gaussian Splatting method via event camera, dubbed DeblurSplat.\nWe address the motion-deblurring problem in two ways. First, we leverage the\npretrained capability of the dense stereo module (DUSt3R) to directly obtain\naccurate initial point clouds from blurred images. Without calculating camera\nposes as an intermediate result, we avoid the cumulative errors transfer from\ninaccurate camera poses to the initial point clouds' positions. Second, we\nintroduce the event stream into the deblur pipeline for its high sensitivity to\ndynamic change. By decoding the latent sharp images from the event stream and\nblurred images, we can provide a fine-grained supervision signal for scene\nreconstruction optimization. Extensive experiments across a range of scenes\ndemonstrate that DeblurSplat not only excels in generating high-fidelity novel\nviews but also achieves significant rendering efficiency compared to the SOTAs\nin deblur 3D-GS.", "AI": {"tldr": "本文提出首个无SfM的事件相机去模糊3D高斯泼溅方法DeblurSplat，通过直接生成点云和利用事件流提供精细监督，实现高质量新视图生成和高渲染效率。", "motivation": "解决3D高斯泼溅中的运动去模糊问题，特别是避免传统SfM方法中因不准确相机姿态导致的累积误差传递到初始点云。", "method": "1. 利用预训练的密集立体模块DUSt3R直接从模糊图像获取准确初始点云，跳过相机姿态计算。2. 引入事件相机流，利用其对动态变化的敏感性。3. 从事件流和模糊图像中解码潜在清晰图像，为场景重建优化提供精细监督信号。", "result": "DeblurSplat在各种场景下都能生成高保真度的新颖视图，并且与当前最先进的去模糊3D-GS方法相比，显著提高了渲染效率。", "conclusion": "DeblurSplat是一种有效且高效的、无需SfM的事件相机去模糊3D高斯泼溅方法，在生成质量和渲染效率方面均表现出色。"}}
{"id": "2509.18910", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18910", "abs": "https://arxiv.org/abs/2509.18910", "authors": ["Shuwei Guo", "Simin Luan", "Yan Ke", "Zeyd Boukhers", "John See", "Cong Yang"], "title": "MoiréNet: A Compact Dual-Domain Network for Image Demoiréing", "comment": null, "summary": "Moir\\'e patterns arise from spectral aliasing between display pixel lattices\nand camera sensor grids, manifesting as anisotropic, multi-scale artifacts that\npose significant challenges for digital image demoir\\'eing. We propose\nMoir\\'eNet, a convolutional neural U-Net-based framework that synergistically\nintegrates frequency and spatial domain features for effective artifact\nremoval. Moir\\'eNet introduces two key components: a Directional\nFrequency-Spatial Encoder (DFSE) that discerns moir\\'e orientation via\ndirectional difference convolution, and a Frequency-Spatial Adaptive Selector\n(FSAS) that enables precise, feature-adaptive suppression. Extensive\nexperiments demonstrate that Moir\\'eNet achieves state-of-the-art performance\non public and actively used datasets while being highly parameter-efficient.\nWith only 5.513M parameters, representing a 48% reduction compared to ESDNet-L,\nMoir\\'eNet combines superior restoration quality with parameter efficiency,\nmaking it well-suited for resource-constrained applications including\nsmartphone photography, industrial imaging, and augmented reality.", "AI": {"tldr": "MoiréNet是一个基于U-Net的卷积神经网络框架，它结合了频率和空间域特征，通过引入定向频率-空间编码器（DFSE）和频率-空间自适应选择器（FSAS），实现了对莫尔条纹的有效去除，同时保持了高参数效率和最先进的性能。", "motivation": "莫尔条纹是显示器像素点阵和相机传感器网格之间频谱混叠产生的各向异性、多尺度伪影，对数字图像去莫尔条纹提出了重大挑战。", "method": "该研究提出了MoiréNet，一个基于U-Net的卷积神经网络框架，它协同整合了频率和空间域特征。MoiréNet引入了两个关键组件：1) 定向频率-空间编码器（DFSE），通过定向差分卷积识别莫尔条纹方向；2) 频率-空间自适应选择器（FSAS），实现精确的、特征自适应的抑制。", "result": "MoiréNet在公共和活跃使用的数据集上取得了最先进的性能，并且具有很高的参数效率。它仅有5.513M参数，比ESDNet-L减少了48%，在提供卓越修复质量的同时，也实现了参数效率。", "conclusion": "MoiréNet结合了卓越的修复质量和参数效率，非常适合资源受限的应用，包括智能手机摄影、工业成像和增强现实等。"}}
{"id": "2509.18912", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18912", "abs": "https://arxiv.org/abs/2509.18912", "authors": ["Yunzhe Shen", "Kai Peng", "Leiye Liu", "Wei Ji", "Jingjing Li", "Miao Zhang", "Yongri Piao", "Huchuan Lu"], "title": "Frequency-Domain Decomposition and Recomposition for Robust Audio-Visual Segmentation", "comment": null, "summary": "Audio-visual segmentation (AVS) plays a critical role in multimodal machine\nlearning by effectively integrating audio and visual cues to precisely segment\nobjects or regions within visual scenes. Recent AVS methods have demonstrated\nsignificant improvements. However, they overlook the inherent frequency-domain\ncontradictions between audio and visual modalities--the pervasively interfering\nnoise in audio high-frequency signals vs. the structurally rich details in\nvisual high-frequency signals. Ignoring these differences can result in\nsuboptimal performance. In this paper, we rethink the AVS task from a deeper\nperspective by reformulating AVS task as a frequency-domain decomposition and\nrecomposition problem. To this end, we introduce a novel Frequency-Aware\nAudio-Visual Segmentation (FAVS) framework consisting of two key modules:\nFrequency-Domain Enhanced Decomposer (FDED) module and Synergistic Cross-Modal\nConsistency (SCMC) module. FDED module employs a residual-based iterative\nfrequency decomposition to discriminate modality-specific semantics and\nstructural features, and SCMC module leverages a mixture-of-experts\narchitecture to reinforce semantic consistency and modality-specific feature\npreservation through dynamic expert routing. Extensive experiments demonstrate\nthat our FAVS framework achieves state-of-the-art performance on three\nbenchmark datasets, and abundant qualitative visualizations further verify the\neffectiveness of the proposed FDED and SCMC modules. The code will be released\nas open source upon acceptance of the paper.", "AI": {"tldr": "本文提出了一种名为FAVS的频率感知音视频分割框架，通过将AVS任务重新定义为频域分解和重组问题，解决了音视频模态间固有的频域矛盾，实现了最先进的性能。", "motivation": "现有音视频分割（AVS）方法忽视了音视频模态在频域上的固有矛盾，即音频高频信号中普遍存在的干扰噪声与视觉高频信号中丰富的结构细节之间的差异。忽略这些差异可能导致次优的性能。", "method": "本文将AVS任务重新定义为频域分解和重组问题，并提出了频率感知音视频分割（FAVS）框架。该框架包含两个核心模块：\n1.  **频域增强分解器（FDED）模块**：采用基于残差的迭代频率分解来区分模态特定的语义和结构特征。\n2.  **协同跨模态一致性（SCMC）模块**：利用混合专家架构，通过动态专家路由来增强语义一致性并保留模态特定特征。", "result": "FAVS框架在三个基准数据集上取得了最先进的性能。大量的定性可视化结果进一步验证了所提出的FDED和SCMC模块的有效性。", "conclusion": "本文提出的FAVS框架通过解决音视频模态固有的频域矛盾，显著提升了音视频分割任务的性能，并为多模态学习中的频率处理提供了新的视角。"}}
{"id": "2509.18913", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18913", "abs": "https://arxiv.org/abs/2509.18913", "authors": ["Nguyen Van Tu", "Pham Nguyen Hai Long", "Vo Hoai Viet"], "title": "xAI-CV: An Overview of Explainable Artificial Intelligence in Computer Vision", "comment": null, "summary": "Deep learning has become the de facto standard and dominant paradigm in image\nanalysis tasks, achieving state-of-the-art performance. However, this approach\noften results in \"black-box\" models, whose decision-making processes are\ndifficult to interpret, raising concerns about reliability in critical\napplications. To address this challenge and provide human a method to\nunderstand how AI model process and make decision, the field of xAI has\nemerged. This paper surveys four representative approaches in xAI for visual\nperception tasks: (i) Saliency Maps, (ii) Concept Bottleneck Models (CBM),\n(iii) Prototype-based methods, and (iv) Hybrid approaches. We analyze their\nunderlying mechanisms, strengths and limitations, as well as evaluation\nmetrics, thereby providing a comprehensive overview to guide future research\nand applications.", "AI": {"tldr": "本文综述了四种在视觉感知任务中常用的可解释人工智能（xAI）方法，分析了它们的机制、优缺点和评估指标，旨在指导未来研究和应用。", "motivation": "深度学习模型在图像分析中表现出色，但其“黑箱”特性使得决策过程难以理解，在关键应用中引发可靠性担忧。xAI的出现旨在解决这一挑战，帮助人类理解AI模型的工作方式。", "method": "本文通过调查和分析四种代表性的xAI方法（显著性图、概念瓶颈模型、基于原型的方法和混合方法）在视觉感知任务中的应用，来完成研究。", "result": "本文分析了这些xAI方法的基本机制、优势、局限性以及评估指标，提供了一个全面的概览。", "conclusion": "该综述为未来xAI在视觉感知任务中的研究和应用提供了指导。"}}
{"id": "2509.18919", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18919", "abs": "https://arxiv.org/abs/2509.18919", "authors": ["Chuni Liu", "Hongjie Li", "Jiaqi Du", "Yangyang Hou", "Qian Sun", "Lei Jin", "Ke Xu"], "title": "Advancing Metallic Surface Defect Detection via Anomaly-Guided Pretraining on a Large Industrial Dataset", "comment": null, "summary": "The pretraining-finetuning paradigm is a crucial strategy in metallic surface\ndefect detection for mitigating the challenges posed by data scarcity. However,\nits implementation presents a critical dilemma. Pretraining on natural image\ndatasets such as ImageNet, faces a significant domain gap. Meanwhile, naive\nself-supervised pretraining on in-domain industrial data is often ineffective\ndue to the inability of existing learning objectives to distinguish subtle\ndefect patterns from complex background noise and textures. To resolve this, we\nintroduce Anomaly-Guided Self-Supervised Pretraining (AGSSP), a novel paradigm\nthat explicitly guides representation learning through anomaly priors. AGSSP\nemploys a two-stage framework: (1) it first pretrains the model's backbone by\ndistilling knowledge from anomaly maps, encouraging the network to capture\ndefect-salient features; (2) it then pretrains the detector using pseudo-defect\nboxes derived from these maps, aligning it with localization tasks. To enable\nthis, we develop a knowledge-enhanced method to generate high-quality anomaly\nmaps and collect a large-scale industrial dataset of 120,000 images.\nAdditionally, we present two small-scale, pixel-level labeled metallic surface\ndefect datasets for validation. Extensive experiments demonstrate that AGSSP\nconsistently enhances performance across various settings, achieving up to a\n10\\% improvement in mAP@0.5 and 11.4\\% in mAP@0.5:0.95 compared to\nImageNet-based models. All code, pretrained models, and datasets are publicly\navailable at https://clovermini.github.io/AGSSP-Dev/.", "AI": {"tldr": "本文提出了一种名为异常引导自监督预训练 (AGSSP) 的新范式，通过异常先验显式指导表示学习，解决了金属表面缺陷检测中预训练面临的领域差距和细微缺陷难以识别的问题，显著提升了检测性能。", "motivation": "金属表面缺陷检测中，预训练-微调范式因数据稀缺而重要。然而，在ImageNet等自然图像数据集上预训练存在显著领域差距；而在领域内工业数据上进行朴素自监督预训练则效果不佳，现有学习目标难以区分细微缺陷模式与复杂背景噪声和纹理。", "method": "本文提出了异常引导自监督预训练 (AGSSP)，该方法包含两个阶段：1) 通过从异常图中蒸馏知识来预训练模型骨干网络，促使网络捕获缺陷显著特征；2) 使用从异常图导出的伪缺陷框预训练检测器，使其与定位任务对齐。为此，开发了一种知识增强方法来生成高质量异常图，并收集了一个包含120,000张图像的大规模工业数据集，同时提供了两个小规模、像素级标注的金属表面缺陷数据集用于验证。", "result": "广泛的实验表明，AGSSP在各种设置下持续提升性能，与基于ImageNet的模型相比，mAP@0.5提升高达10%，mAP@0.5:0.95提升高达11.4%。", "conclusion": "AGSSP通过异常先验显式指导表示学习，成功解决了金属表面缺陷检测中预训练的困境，显著提高了检测性能。所有代码、预训练模型和数据集均已公开。"}}
{"id": "2509.18924", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18924", "abs": "https://arxiv.org/abs/2509.18924", "authors": ["Kartik Teotia", "Helge Rhodin", "Mohit Mendiratta", "Hyeongwoo Kim", "Marc Habermann", "Christian Theobalt"], "title": "Audio-Driven Universal Gaussian Head Avatars", "comment": "(SIGGRAPH Asia 2025) Project page:\n  https://kartik-teotia.github.io/UniGAHA/", "summary": "We introduce the first method for audio-driven universal photorealistic\navatar synthesis, combining a person-agnostic speech model with our novel\nUniversal Head Avatar Prior (UHAP). UHAP is trained on cross-identity\nmulti-view videos. In particular, our UHAP is supervised with neutral scan\ndata, enabling it to capture the identity-specific details at high fidelity. In\ncontrast to previous approaches, which predominantly map audio features to\ngeometric deformations only while ignoring audio-dependent appearance\nvariations, our universal speech model directly maps raw audio inputs into the\nUHAP latent expression space. This expression space inherently encodes, both,\ngeometric and appearance variations. For efficient personalization to new\nsubjects, we employ a monocular encoder, which enables lightweight regression\nof dynamic expression variations across video frames. By accounting for these\nexpression-dependent changes, it enables the subsequent model fine-tuning stage\nto focus exclusively on capturing the subject's global appearance and geometry.\nDecoding these audio-driven expression codes via UHAP generates highly\nrealistic avatars with precise lip synchronization and nuanced expressive\ndetails, such as eyebrow movement, gaze shifts, and realistic mouth interior\nappearance as well as motion. Extensive evaluations demonstrate that our method\nis not only the first generalizable audio-driven avatar model that can account\nfor detailed appearance modeling and rendering, but it also outperforms\ncompeting (geometry-only) methods across metrics measuring lip-sync accuracy,\nquantitative image quality, and perceptual realism.", "AI": {"tldr": "该研究提出了一种首个音频驱动的通用逼真头像合成方法，结合了与人无关的语音模型和新型通用头部头像先验（UHAP），能够同时捕捉几何和外观变化，生成高度逼真的头像。", "motivation": "以往的方法主要将音频特征映射到几何变形，却忽略了与音频相关的外观变化。因此，需要一种能够考虑详细外观建模和渲染的、可泛化的音频驱动头像模型。", "method": "该方法引入了通用头部头像先验（UHAP），其在跨身份多视角视频上进行训练，并利用中性扫描数据进行监督，以高保真度捕获身份特定细节。一个通用的语音模型将原始音频输入直接映射到UHAP的潜在表情空间，该空间固有地编码了几何和外观变化。为了高效地对新主体进行个性化，采用单目编码器回归动态表情变化，使后续模型微调专注于捕捉主体的全局外观和几何。通过UHAP解码这些音频驱动的表情代码来生成头像。", "result": "该方法生成了高度逼真的头像，具有精确的唇部同步和细致的表情细节，如眉毛运动、凝视转移以及逼真的口腔内部外观和运动。它是首个能够进行详细外观建模和渲染的可泛化音频驱动头像模型，并且在唇部同步准确性、定量图像质量和感知真实感方面，优于现有的（仅几何）竞争方法。", "conclusion": "该研究成功地提出了一种通用、音频驱动的逼真头像合成方法，该方法能够处理详细的外观建模和渲染，并在唇部同步准确性、图像质量和感知真实感方面超越了现有方法。"}}
{"id": "2509.18926", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18926", "abs": "https://arxiv.org/abs/2509.18926", "authors": ["Pamela Osuna-Vargas", "Altug Kamacioglu", "Dominik F. Aschauer", "Petros E. Vlachos", "Sercan Alipek", "Jochen Triesch", "Simon Rumpel", "Matthias Kaschube"], "title": "SynapFlow: A Modular Framework Towards Large-Scale Analysis of Dendritic Spines", "comment": null, "summary": "Dendritic spines are key structural components of excitatory synapses in the\nbrain. Given the size of dendritic spines provides a proxy for synaptic\nefficacy, their detection and tracking across time is important for studies of\nthe neural basis of learning and memory. Despite their relevance, large-scale\nanalyses of the structural dynamics of dendritic spines in 3D+time microscopy\ndata remain challenging and labor-intense. Here, we present a modular machine\nlearning-based pipeline designed to automate the detection, time-tracking, and\nfeature extraction of dendritic spines in volumes chronically recorded with\ntwo-photon microscopy. Our approach tackles the challenges posed by biological\ndata by combining a transformer-based detection module, a depth-tracking\ncomponent that integrates spatial features, a time-tracking module to associate\n3D spines across time by leveraging spatial consistency, and a feature\nextraction unit that quantifies biologically relevant spine properties. We\nvalidate our method on open-source labeled spine data, and on two complementary\nannotated datasets that we publish alongside this work: one for detection and\ndepth-tracking, and one for time-tracking, which, to the best of our knowledge,\nis the first data of this kind. To encourage future research, we release our\ndata, code, and pre-trained weights at\nhttps://github.com/pamelaosuna/SynapFlow, establishing a baseline for scalable,\nend-to-end analysis of dendritic spine dynamics.", "AI": {"tldr": "该论文提出了一个模块化的机器学习流程，用于自动化检测、时间追踪和提取3D+时间显微镜数据中的树突棘特征，以实现对学习和记忆神经基础的大规模分析。", "motivation": "树突棘是兴奋性突触的关键结构，其大小反映突触效能。检测和追踪树突棘对于研究学习和记忆的神经基础至关重要。然而，对3D+时间显微镜数据中树突棘结构动力学的大规模分析仍然具有挑战性且劳动密集。", "method": "本文提出一个模块化的机器学习流程，包含：基于Transformer的检测模块、整合空间特征的深度追踪组件、利用空间一致性关联3D树突棘的时间追踪模块，以及量化生物学相关棘特性的特征提取单元。该方法在开源标记数据和两个新发布的互补注释数据集（一个用于检测和深度追踪，一个用于时间追踪）上进行了验证。", "result": "该方法在开源标记树突棘数据以及两个新发布的注释数据集上得到了验证，其中包括首次发布的时间追踪数据集。研究团队还发布了数据、代码和预训练权重，为树突棘动力学的可扩展、端到端分析建立了基线。", "conclusion": "该研究提供了一个可扩展、端到端的树突棘动力学分析基线，通过自动化检测、追踪和特征提取过程，解决了大规模分析的挑战，并鼓励未来的相关研究。"}}
{"id": "2509.18956", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18956", "abs": "https://arxiv.org/abs/2509.18956", "authors": ["Zijing Guo", "Yunyang Zhao", "Lin Wang"], "title": "Seeing Through Reflections: Advancing 3D Scene Reconstruction in Mirror-Containing Environments with Gaussian Splatting", "comment": null, "summary": "Mirror-containing environments pose unique challenges for 3D reconstruction\nand novel view synthesis (NVS), as reflective surfaces introduce view-dependent\ndistortions and inconsistencies. While cutting-edge methods such as Neural\nRadiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) excel in typical\nscenes, their performance deteriorates in the presence of mirrors. Existing\nsolutions mainly focus on handling mirror surfaces through symmetry mapping but\noften overlook the rich information carried by mirror reflections. These\nreflections offer complementary perspectives that can fill in absent details\nand significantly enhance reconstruction quality. To advance 3D reconstruction\nin mirror-rich environments, we present MirrorScene3D, a comprehensive dataset\nfeaturing diverse indoor scenes, 1256 high-quality images, and annotated mirror\nmasks, providing a benchmark for evaluating reconstruction methods in\nreflective settings. Building on this, we propose ReflectiveGS, an extension of\n3D Gaussian Splatting that utilizes mirror reflections as complementary\nviewpoints rather than simple symmetry artifacts, enhancing scene geometry and\nrecovering absent details. Experiments on MirrorScene3D show that\nReflectiveGaussian outperforms existing methods in SSIM, PSNR, LPIPS, and\ntraining speed, setting a new benchmark for 3D reconstruction in mirror-rich\nenvironments.", "AI": {"tldr": "该研究提出了MirrorScene3D数据集和ReflectiveGS方法，用于解决镜面环境中3D重建和新视角合成的挑战，通过将镜面反射视为互补视角来增强重建质量。", "motivation": "NeRF和3DGS等先进方法在镜面环境中性能下降，因为反射表面引入了视角依赖的失真和不一致。现有解决方案主要通过对称映射处理镜面，但忽视了镜面反射所携带的丰富信息（即互补视角），这些信息可以填补缺失细节并显著提高重建质量。", "method": "1. 构建了MirrorScene3D数据集：包含多样化的室内场景、1256张高质量图像和标注的镜面掩码，作为评估反射环境中重建方法的基准。2. 提出了ReflectiveGS：作为3D Gaussian Splatting的扩展，它将镜面反射用作互补视角，而非简单的对称伪影，以增强场景几何并恢复缺失细节。", "result": "在MirrorScene3D数据集上的实验表明，ReflectiveGS在SSIM、PSNR、LPIPS和训练速度方面均优于现有方法。", "conclusion": "ReflectiveGS在富含镜面的环境中为3D重建设定了新的基准，通过有效利用镜面反射作为互补视角，显著提升了重建质量和效率。"}}
{"id": "2509.18958", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18958", "abs": "https://arxiv.org/abs/2509.18958", "authors": ["Cristina Iacono", "Mariarosaria Meola", "Federica Conte", "Laura Mecozzi", "Umberto Bracale", "Pietro Falco", "Fanny Ficuciello"], "title": "Generative data augmentation for biliary tract detection on intraoperative images", "comment": null, "summary": "Cholecystectomy is one of the most frequently performed procedures in\ngastrointestinal surgery, and the laparoscopic approach is the gold standard\nfor symptomatic cholecystolithiasis and acute cholecystitis. In addition to the\nadvantages of a significantly faster recovery and better cosmetic results, the\nlaparoscopic approach bears a higher risk of bile duct injury, which has a\nsignificant impact on quality of life and survival. To avoid bile duct injury,\nit is essential to improve the intraoperative visualization of the bile duct.\nThis work aims to address this problem by leveraging a deep-learning approach\nfor the localization of the biliary tract from white-light images acquired\nduring the surgical procedures. To this end, the construction and annotation of\nan image database to train the Yolo detection algorithm has been employed.\nBesides classical data augmentation techniques, the paper proposes Generative\nAdversarial Network (GAN) for the generation of a synthetic portion of the\ntraining dataset. Experimental results have been discussed along with ethical\nconsiderations.", "AI": {"tldr": "该研究利用深度学习（Yolo算法）结合真实与GAN生成的合成数据，旨在从腹腔镜手术的白光图像中定位胆道，以提高术中胆道可视化并降低胆管损伤风险。", "motivation": "腹腔镜胆囊切除术是常见手术，但存在胆管损伤的较高风险，这严重影响患者生活质量和生存。提高术中胆道可视化是避免损伤的关键。", "method": "该研究构建并标注了一个图像数据库，用于训练Yolo检测算法。除了传统的图像数据增强技术外，还提出使用生成对抗网络（GAN）生成部分合成训练数据集。", "result": "摘要中提到已讨论了实验结果和伦理考量，但未具体说明实验结果的量化表现或具体发现。", "conclusion": "该研究致力于通过深度学习方法改善腹腔镜胆囊切除术中胆道的术中可视化，以期降低胆管损伤的风险，并讨论了相关实验结果和伦理问题。"}}
{"id": "2509.18973", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18973", "abs": "https://arxiv.org/abs/2509.18973", "authors": ["Jiabao Chen", "Shan Xiong", "Jialin Peng"], "title": "Prompt-DAS: Annotation-Efficient Prompt Learning for Domain Adaptive Semantic Segmentation of Electron Microscopy Images", "comment": "MICCAI2025", "summary": "Domain adaptive segmentation (DAS) of numerous organelle instances from\nlarge-scale electron microscopy (EM) is a promising way to enable\nannotation-efficient learning. Inspired by SAM, we propose a promptable\nmultitask framework, namely Prompt-DAS, which is flexible enough to utilize any\nnumber of point prompts during the adaptation training stage and testing stage.\nThus, with varying prompt configurations, Prompt-DAS can perform unsupervised\ndomain adaptation (UDA) and weakly supervised domain adaptation (WDA), as well\nas interactive segmentation during testing. Unlike the foundation model SAM,\nwhich necessitates a prompt for each individual object instance, Prompt-DAS is\nonly trained on a small dataset and can utilize full points on all instances,\nsparse points on partial instances, or even no points at all, facilitated by\nthe incorporation of an auxiliary center-point detection task. Moreover, a\nnovel prompt-guided contrastive learning is proposed to enhance discriminative\nfeature learning. Comprehensive experiments conducted on challenging benchmarks\ndemonstrate the effectiveness of the proposed approach over existing UDA, WDA,\nand SAM-based approaches.", "AI": {"tldr": "本文提出Prompt-DAS，一个受SAM启发的、可提示的多任务框架，用于电子显微镜图像中的细胞器域自适应分割。它在训练和测试阶段对点提示的数量具有灵活性，支持无监督、弱监督域自适应以及交互式分割，并通过辅助中心点检测任务和提示引导对比学习，在各种基准测试中优于现有方法。", "motivation": "大规模电子显微镜图像中大量细胞器实例的域自适应分割是实现注释高效学习的有前景方法。受SAM启发，但SAM需要为每个单独对象实例提供提示，这在实际应用中可能效率不高，因此需要一个更灵活且注释高效的框架。", "method": "本文提出了Prompt-DAS，一个可提示的多任务框架，它：1) 借鉴SAM但更灵活，在自适应训练和测试阶段可使用任意数量的点提示；2) 可进行无监督域自适应（UDA）、弱监督域自适应（WDA）以及测试时的交互式分割；3) 通过引入辅助中心点检测任务，可使用所有实例上的完整点、部分实例上的稀疏点，甚至不使用任何点进行训练；4) 提出了一种新颖的提示引导对比学习方法，以增强判别性特征学习。", "result": "在具有挑战性的基准测试上进行的综合实验表明，所提出的Prompt-DAS方法在性能上优于现有的UDA、WDA以及基于SAM的方法。", "conclusion": "Prompt-DAS提供了一种有效且灵活的域自适应分割方法，特别适用于电子显微镜图像中的细胞器分割，通过其独特的提示机制和辅助任务，实现了注释高效的学习和卓越的性能。"}}
{"id": "2509.19003", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19003", "abs": "https://arxiv.org/abs/2509.19003", "authors": ["Honghao Chen", "Xingzhou Lou", "Xiaokun Feng", "Kaiqi Huang", "Xinlong Wang"], "title": "Unveiling Chain of Step Reasoning for Vision-Language Models with Fine-grained Rewards", "comment": "Accepted by NeurIPS 2025", "summary": "Chain of thought reasoning has demonstrated remarkable success in large\nlanguage models, yet its adaptation to vision-language reasoning remains an\nopen challenge with unclear best practices. Existing attempts typically employ\nreasoning chains at a coarse-grained level, which struggles to perform\nfine-grained structured reasoning and, more importantly, are difficult to\nevaluate the reward and quality of intermediate reasoning. In this work, we\ndelve into chain of step reasoning for vision-language models, enabling\nassessing reasoning step quality accurately and leading to effective\nreinforcement learning and inference-time scaling with fine-grained rewards. We\npresent a simple, effective, and fully transparent framework, including the\nstep-level reasoning data, process reward model (PRM), and reinforcement\nlearning training. With the proposed approaches, our models set strong\nbaselines with consistent improvements on challenging vision-language\nbenchmarks. More importantly, we conduct a thorough empirical analysis and\nablation study, unveiling the impact of each component and several intriguing\nproperties of inference-time scaling. We believe this paper serves as a\nbaseline for vision-language models and offers insights into more complex\nmultimodal reasoning. Our dataset, PRM, and code will be available at\nhttps://github.com/baaivision/CoS.", "AI": {"tldr": "本文提出了一种名为“分步链式推理”（Chain of Step, CoS）的新方法，用于视觉-语言模型，通过在更细粒度评估推理步骤质量，实现了有效的强化学习和推理时间扩展，并在挑战性基准测试中取得了显著提升。", "motivation": "链式思考（Chain of Thought）在大型语言模型中表现出色，但其在视觉-语言推理中的应用仍面临挑战，缺乏明确的最佳实践。现有方法通常采用粗粒度推理链，难以进行细粒度结构化推理，更重要的是，难以评估中间推理的奖励和质量。", "method": "本文深入研究了视觉-语言模型的分步链式推理，旨在准确评估推理步骤质量，从而实现有效的强化学习和基于细粒度奖励的推理时间扩展。具体方法包括：构建步级推理数据、开发过程奖励模型（PRM）以及进行强化学习训练，形成了一个简单、有效且完全透明的框架。", "result": "所提出的模型在挑战性的视觉-语言基准测试中建立了强大的基线，并持续取得了改进。更重要的是，通过彻底的实证分析和消融研究，揭示了每个组件的影响以及推理时间扩展的几个有趣特性。", "conclusion": "本文为视觉-语言模型提供了一个强大的基线，并为更复杂的跨模态推理提供了深刻见解。研究中生成的数据集、PRM和代码将对外公开。"}}
{"id": "2509.19028", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19028", "abs": "https://arxiv.org/abs/2509.19028", "authors": ["Ioannis Sarafis", "Alexandros Papadopoulos", "Anastasios Delopoulos"], "title": "Weakly Supervised Food Image Segmentation using Vision Transformers and Segment Anything Model", "comment": "Submitted to the 20th International Workshop on Semantic and Social\n  Media Adaptation & Personalization", "summary": "In this paper, we propose a weakly supervised semantic segmentation approach\nfor food images which takes advantage of the zero-shot capabilities and\npromptability of the Segment Anything Model (SAM) along with the attention\nmechanisms of Vision Transformers (ViTs). Specifically, we use class activation\nmaps (CAMs) from ViTs to generate prompts for SAM, resulting in masks suitable\nfor food image segmentation. The ViT model, a Swin Transformer, is trained\nexclusively using image-level annotations, eliminating the need for pixel-level\nannotations during training. Additionally, to enhance the quality of the\nSAM-generated masks, we examine the use of image preprocessing techniques in\ncombination with single-mask and multi-mask SAM generation strategies. The\nmethodology is evaluated on the FoodSeg103 dataset, generating an average of\n2.4 masks per image (excluding background), and achieving an mIoU of 0.54 for\nthe multi-mask scenario. We envision the proposed approach as a tool to\naccelerate food image annotation tasks or as an integrated component in food\nand nutrition tracking applications.", "AI": {"tldr": "本文提出一种弱监督食物图像语义分割方法，结合SAM的零样本能力与ViT的注意力机制，利用ViT的类别激活图生成SAM提示，实现无需像素级标注的食物分割。", "motivation": "传统语义分割需要大量的像素级标注，成本高昂。本文旨在通过弱监督学习，利用图像级标注来消除对像素级标注的需求，并加速食物图像标注任务或集成到食物营养追踪应用中。", "method": "该方法使用Swin Transformer（一种ViT模型）进行训练，仅需图像级标注。ViT生成的类别激活图（CAMs）被用作Segment Anything Model (SAM) 的提示。为提高SAM生成掩码的质量，研究了图像预处理技术以及单掩码和多掩码SAM生成策略。", "result": "该方法在FoodSeg103数据集上进行了评估，平均每张图像生成2.4个掩码（不包括背景），在多掩码场景下实现了0.54的mIoU。", "conclusion": "所提出的方法可以作为加速食物图像标注任务的工具，或作为食物和营养追踪应用中的集成组件，有效利用弱监督和零样本能力进行食物图像分割。"}}
{"id": "2509.19052", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19052", "abs": "https://arxiv.org/abs/2509.19052", "authors": ["Jierui Qu", "Jianchun Zhao"], "title": "A DyL-Unet framework based on dynamic learning for Temporally Consistent Echocardiographic Segmentation", "comment": null, "summary": "Accurate segmentation of cardiac anatomy in echocardiography is essential for\ncardiovascular diagnosis and treatment. Yet echocardiography is prone to\ndeformation and speckle noise, causing frame-to-frame segmentation jitter. Even\nwith high accuracy in single-frame segmentation, temporal instability can\nweaken functional estimates and impair clinical interpretability. To address\nthese issues, we propose DyL-UNet, a dynamic learning-based temporal\nconsistency U-Net segmentation architecture designed to achieve temporally\nstable and precise echocardiographic segmentation. The framework constructs an\nEcho-Dynamics Graph (EDG) through dynamic learning to extract dynamic\ninformation from videos. DyL-UNet incorporates multiple Swin-Transformer-based\nencoder-decoder branches for processing single-frame images. It further\nintroduces Cardiac Phase-Dynamics Attention (CPDA) at the skip connections,\nwhich uses EDG-encoded dynamic features and cardiac-phase cues to enforce\ntemporal consistency during segmentation. Extensive experiments on the CAMUS\nand EchoNet-Dynamic datasets demonstrate that DyL-UNet maintains segmentation\naccuracy comparable to existing methods while achieving superior temporal\nconsistency, providing a reliable solution for automated clinical\nechocardiography.", "AI": {"tldr": "DyL-UNet是一种基于动态学习的时间一致性U-Net分割架构，旨在实现超声心动图分割的时间稳定性和精确性，通过构建Echo-Dynamics Graph和引入Cardiac Phase-Dynamics Attention来解决帧间抖动问题。", "motivation": "超声心动图分割对心血管诊断和治疗至关重要，但易受形变和散斑噪声影响，导致帧间分割抖动。即使单帧分割精度高，时间不稳定性也会削弱功能评估并损害临床可解释性。", "method": "本文提出了DyL-UNet框架。它通过动态学习构建Echo-Dynamics Graph (EDG) 以提取视频中的动态信息。DyL-UNet包含多个基于Swin-Transformer的编码器-解码器分支来处理单帧图像。此外，在跳跃连接处引入了Cardiac Phase-Dynamics Attention (CPDA)，利用EDG编码的动态特征和心动周期线索来强制执行分割过程中的时间一致性。", "result": "在CAMUS和EchoNet-Dynamic数据集上的大量实验表明，DyL-UNet在保持与现有方法相当的分割精度的同时，实现了卓越的时间一致性。", "conclusion": "DyL-UNet为自动化临床超声心动图提供了一个可靠的解决方案，能够实现稳定且精确的分割。"}}
{"id": "2509.19082", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19082", "abs": "https://arxiv.org/abs/2509.19082", "authors": ["Alexey Nekrasov", "Ali Athar", "Daan de Geus", "Alexander Hermans", "Bastian Leibe"], "title": "3rd Place Report of LSVOS 2025 MeViS Track: Sa2VA-i: Improving Sa2VA Results with Consistent Training and Inference", "comment": null, "summary": "Sa2VA is a recent model for language-guided dense grounding in images and\nvideo that achieves state-of-the-art results on multiple segmentation\nbenchmarks and that has become widely popular. However, we found that Sa2VA\ndoes not perform according to its full potential for referring video object\nsegmentation tasks. We identify inconsistencies between training and inference\nprocedures as the key factor holding it back. To mitigate this issue, we\npropose an improved version of Sa2VA, Sa2VA-i, that rectifies these issues and\nimproves the results. In fact, Sa2VA-i sets a new state of the art for multiple\nvideo benchmarks and achieves improvements of up to +11.6 J&F on MeViS, +1.4 on\nRef-YT-VOS, +3.3 on Ref-DAVIS and +4.1 on ReVOS using the same Sa2VA\ncheckpoints. With our fixes, the Sa2VA-i-1B model even performs on par with the\noriginal Sa2VA-26B model on the MeViS benchmark. We hope that this work will\nshow the importance of seemingly trivial implementation details and that it\nwill provide valuable insights for the referring video segmentation field. We\nprovide the code and updated models at https://github.com/kumuji/sa2va-i", "AI": {"tldr": "Sa2VA模型在指代视频目标分割任务中存在训练与推理不一致的问题，本文提出改进版Sa2VA-i，通过修正这些不一致性显著提升了模型性能，并在多个视频基准测试中刷新了SOTA，证明了实现细节的重要性。", "motivation": "Sa2VA作为一种语言引导的密集接地模型，在图像和视频分割基准测试中取得了最先进的成果并广受欢迎。然而，研究发现Sa2VA在指代视频目标分割任务中未能发挥其全部潜力。主要原因是训练和推理过程中的不一致性。", "method": "通过识别并纠正Sa2VA模型训练和推理过程中的不一致性，提出了一个改进版本Sa2VA-i。", "result": "Sa2VA-i在多个视频基准测试中取得了新的最先进结果，使用相同的Sa2VA检查点，在MeViS上性能提升高达+11.6 J&F，在Ref-YT-VOS上提升+1.4，在Ref-DAVIS上提升+3.3，在ReVOS上提升+4.1。值得注意的是，Sa2VA-i-1B模型在MeViS基准测试上的表现甚至与原始Sa2VA-26B模型相当。", "conclusion": "这项工作强调了看似微不足道的实现细节的重要性，并为指代视频分割领域提供了有价值的见解。作者提供了代码和更新后的模型。"}}
{"id": "2509.19087", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19087", "abs": "https://arxiv.org/abs/2509.19087", "authors": ["Ganesh Mallya", "Yotam Gigi", "Dahun Kim", "Maxim Neumann", "Genady Beryozkin", "Tomer Shekel", "Anelia Angelova"], "title": "Zero-Shot Multi-Spectral Learning: Reimagining a Generalist Multimodal Gemini 2.5 Model for Remote Sensing Applications", "comment": null, "summary": "Multi-spectral imagery plays a crucial role in diverse Remote Sensing\napplications including land-use classification, environmental monitoring and\nurban planning. These images are widely adopted because their additional\nspectral bands correlate strongly with physical materials on the ground, such\nas ice, water, and vegetation. This allows for more accurate identification,\nand their public availability from missions, such as Sentinel-2 and Landsat,\nonly adds to their value. Currently, the automatic analysis of such data is\npredominantly managed through machine learning models specifically trained for\nmulti-spectral input, which are costly to train and support. Furthermore,\nalthough providing a lot of utility for Remote Sensing, such additional inputs\ncannot be used with powerful generalist large multimodal models, which are\ncapable of solving many visual problems, but are not able to understand\nspecialized multi-spectral signals.\n  To address this, we propose a training-free approach which introduces new\nmulti-spectral data in a Zero-Shot-only mode, as inputs to generalist\nmultimodal models, trained on RGB-only inputs. Our approach leverages the\nmultimodal models' understanding of the visual space, and proposes to adapt to\ninputs to that space, and to inject domain-specific information as instructions\ninto the model. We exemplify this idea with the Gemini2.5 model and observe\nstrong Zero-Shot performance gains of the approach on popular Remote Sensing\nbenchmarks for land cover and land use classification and demonstrate the easy\nadaptability of Gemini2.5 to new inputs. These results highlight the potential\nfor geospatial professionals, working with non-standard specialized inputs, to\neasily leverage powerful multimodal models, such as Gemini2.5, to accelerate\ntheir work, benefiting from their rich reasoning and contextual capabilities,\ngrounded in the specialized sensor data.", "AI": {"tldr": "本文提出了一种免训练方法，使仅用RGB图像训练的通用多模态模型（如Gemini 2.5）能够以零样本（Zero-Shot）模式处理多光谱遥感数据，并在土地覆盖分类任务上取得了显著性能提升。", "motivation": "多光谱图像在遥感应用中至关重要，但现有机器学习模型训练成本高昂，且强大的通用多模态模型无法直接理解和处理多光谱信号，限制了其在遥感领域的应用。", "method": "该研究提出了一种免训练方法，将多光谱数据作为通用多模态模型（仅用RGB输入训练）的零样本输入。其核心思想是利用模型对视觉空间的理解，将多光谱输入适配到该空间，并通过指令注入领域特定信息。研究以Gemini 2.5模型为例进行了验证。", "result": "该方法在流行的遥感土地覆盖和土地利用分类基准上取得了显著的零样本性能提升，并证明了Gemini 2.5模型对新输入的易适应性。", "conclusion": "研究结果表明，地理空间专业人员可以轻松利用如Gemini 2.5等强大的多模态模型处理非标准专业输入，从而受益于其丰富的推理和上下文能力，加速工作效率。"}}
{"id": "2509.19096", "categories": ["cs.CV", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.19096", "abs": "https://arxiv.org/abs/2509.19096", "authors": ["Ilhan Skender", "Kailin Tong", "Selim Solmaz", "Daniel Watzenig"], "title": "Investigating Traffic Accident Detection Using Multimodal Large Language Models", "comment": "Accepted for presentation at the 2025 IEEE International Automated\n  Vehicle Validation Conference (IAVVC 2025). Final version to appear in IEEE\n  Xplore", "summary": "Traffic safety remains a critical global concern, with timely and accurate\naccident detection essential for hazard reduction and rapid emergency response.\nInfrastructure-based vision sensors offer scalable and efficient solutions for\ncontinuous real-time monitoring, facilitating automated detection of acci-\ndents directly from captured images. This research investigates the zero-shot\ncapabilities of multimodal large language models (MLLMs) for detecting and\ndescribing traffic accidents using images from infrastructure cameras, thus\nminimizing reliance on extensive labeled datasets. Main contributions include:\n(1) Evaluation of MLLMs using the simulated DeepAccident dataset from CARLA,\nexplicitly addressing the scarcity of diverse, realistic, infrastructure-based\naccident data through controlled simulations; (2) Comparative performance\nanalysis between Gemini 1.5 and 2.0, Gemma 3 and Pixtral models in acci- dent\nidentification and descriptive capabilities without prior fine-tuning; and (3)\nIntegration of advanced visual analytics, specifically YOLO for object\ndetection, Deep SORT for multi- object tracking, and Segment Anything (SAM) for\ninstance segmentation, into enhanced prompts to improve model accuracy and\nexplainability. Key numerical results show Pixtral as the top performer with an\nF1-score of 0.71 and 83% recall, while Gemini models gained precision with\nenhanced prompts (e.g., Gemini 1.5 rose to 90%) but suffered notable F1 and\nrecall losses. Gemma 3 offered the most balanced performance with minimal\nmetric fluctuation. These findings demonstrate the substantial potential of\nintegrating MLLMs with advanced visual analytics techniques, enhancing their\napplicability in real-world automated traffic monitoring systems.", "AI": {"tldr": "本研究探讨了多模态大型语言模型（MLLMs）在零样本交通事故检测中的应用，通过集成先进视觉分析技术（如YOLO、Deep SORT、SAM）来增强其在基础设施摄像头图像上的性能和可解释性。", "motivation": "交通安全是一个全球性问题，及时准确的事故检测对于减少危害和快速响应至关重要。基础设施视觉传感器提供实时监控方案，但依赖大量标注数据集的传统方法存在局限性。本研究旨在利用MLLMs的零样本能力，减少对标注数据的依赖，并解决多样化、真实基础设施事故数据稀缺的问题。", "method": "研究方法包括：1) 使用CARLA模拟的DeepAccident数据集评估MLLMs，以解决实际事故数据稀缺问题；2) 对比Gemini 1.5、2.0、Gemma 3和Pixtral模型在未微调情况下的事故识别和描述能力；3) 将YOLO（目标检测）、Deep SORT（多目标跟踪）和Segment Anything (SAM)（实例分割）等先进视觉分析技术集成到提示词中，以提高模型的准确性和可解释性。", "result": "主要结果显示：Pixtral模型表现最佳，F1-score达到0.71，召回率为83%。Gemini模型在增强提示词后精度显著提高（例如Gemini 1.5提高到90%），但F1-score和召回率有所下降。Gemma 3模型表现最为均衡，各项指标波动最小。", "conclusion": "研究结果表明，将多模态大型语言模型与先进视觉分析技术相结合具有巨大潜力，能有效提升其在现实世界自动化交通监控系统中的应用能力。"}}
{"id": "2509.19115", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19115", "abs": "https://arxiv.org/abs/2509.19115", "authors": ["Görkay Aydemir", "Weidi Xie", "Fatma Güney"], "title": "Track-On2: Enhancing Online Point Tracking with Memory", "comment": null, "summary": "In this paper, we consider the problem of long-term point tracking, which\nrequires consistent identification of points across video frames under\nsignificant appearance changes, motion, and occlusion. We target the online\nsetting, i.e. tracking points frame-by-frame, making it suitable for real-time\nand streaming applications. We extend our prior model Track-On into Track-On2,\na simple and efficient transformer-based model for online long-term tracking.\nTrack-On2 improves both performance and efficiency through architectural\nrefinements, more effective use of memory, and improved synthetic training\nstrategies. Unlike prior approaches that rely on full-sequence access or\niterative updates, our model processes frames causally and maintains temporal\ncoherence via a memory mechanism, which is key to handling drift and occlusions\nwithout requiring future frames. At inference, we perform coarse patch-level\nclassification followed by refinement. Beyond architecture, we systematically\nstudy synthetic training setups and their impact on memory behavior, showing\nhow they shape temporal robustness over long sequences. Through comprehensive\nexperiments, Track-On2 achieves state-of-the-art results across five synthetic\nand real-world benchmarks, surpassing prior online trackers and even strong\noffline methods that exploit bidirectional context. These results highlight the\neffectiveness of causal, memory-based architectures trained purely on synthetic\ndata as scalable solutions for real-world point tracking. Project page:\nhttps://kuis-ai.github.io/track_on2", "AI": {"tldr": "本文提出Track-On2，一个基于Transformer的简单高效模型，用于在线长期点跟踪。它通过因果处理和内存机制处理视频帧，在合成数据上训练，并在多个基准测试中达到了最先进的性能，超越了现有在线和部分离线方法。", "motivation": "研究动机是解决长期点跟踪问题，即在视频帧中持续识别点，即使面临显著的外观变化、运动和遮挡。特别关注在线设置，以适应实时和流媒体应用的需求。", "method": "该研究将之前的Track-On模型扩展为Track-On2，这是一个基于Transformer的在线长期跟踪模型。Track-On2通过架构改进、更有效的内存使用和改进的合成训练策略来提高性能和效率。它以因果方式处理帧，并通过内存机制保持时间一致性，以处理漂移和遮挡，而无需未来帧。推理时，先进行粗略的块级分类，然后进行细化。此外，还系统研究了合成训练设置及其对内存行为的影响。", "result": "Track-On2在五个合成和真实世界基准测试中取得了最先进的成果，超越了之前的在线跟踪器，甚至优于利用双向上下文的强大离线方法。", "conclusion": "研究结论是，纯粹通过合成数据训练的因果、基于内存的架构是解决实际点跟踪问题的有效且可扩展的解决方案。"}}
{"id": "2509.19129", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19129", "abs": "https://arxiv.org/abs/2509.19129", "authors": ["Adam Romlein", "Benjamin X. Hou", "Yuval Boss", "Cynthia L. Christman", "Stacie Koslovsky", "Erin E. Moreland", "Jason Parham", "Anthony Hoogs"], "title": "KAMERA: Enhancing Aerial Surveys of Ice-associated Seals in Arctic Environments", "comment": "Accepted to the IEEE/CVF International Conference on Computer Vision\n  (ICCV 2025)", "summary": "We introduce KAMERA: a comprehensive system for multi-camera, multi-spectral\nsynchronization and real-time detection of seals and polar bears. Utilized in\naerial surveys for ice-associated seals in the Bering, Chukchi, and Beaufort\nseas around Alaska, KAMERA provides up to an 80% reduction in dataset\nprocessing time over previous methods. Our rigorous calibration and hardware\nsynchronization enable using multiple spectra for object detection. All\ncollected data are annotated with metadata so they can be easily referenced\nlater. All imagery and animal detections from a survey are mapped onto a world\nplane for accurate surveyed area estimates and quick assessment of survey\nresults. We hope KAMERA will inspire other mapping and detection efforts in the\nscientific community, with all software, models, and schematics fully\nopen-sourced.", "AI": {"tldr": "KAMERA是一个用于多相机、多光谱同步和实时检测海豹与北极熊的综合系统，可将数据集处理时间减少80%，并已在空中调查中应用。", "motivation": "在白令海、楚科奇海和波弗特海的冰区海豹空中调查中，需要一个更高效、更准确的系统来同步多光谱数据、实时检测动物，并显著减少数据处理时间。", "method": "引入KAMERA系统，该系统采用多相机、多光谱同步技术，进行实时目标检测。通过严格的校准和硬件同步，实现多光谱数据用于目标检测。所有收集的数据都附带元数据，便于后续引用。所有图像和动物检测结果都被映射到世界平面上，以准确估算调查区域和快速评估调查结果。", "result": "KAMERA系统将数据集处理时间比传统方法减少了80%。它使得多光谱数据能够用于目标检测，并能准确估算调查区域和快速评估调查结果。", "conclusion": "KAMERA是一个全面、高效且准确的多相机、多光谱同步系统，用于实时检测海豹和北极熊，显著提高了空中调查的效率。该系统及其所有软件、模型和原理图都已开源，有望启发科学界其他测绘和检测工作。"}}
{"id": "2509.19156", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19156", "abs": "https://arxiv.org/abs/2509.19156", "authors": ["Maurf Hassan", "Steven Davy", "Muhammad Zawish", "Owais Bin Zuber", "Nouman Ashraf"], "title": "NeuCODEX: Edge-Cloud Co-Inference with Spike-Driven Compression and Dynamic Early-Exit", "comment": "This paper was accepted at ICMLA 2025. The official version will\n  appear in IEEE Xplore", "summary": "Spiking Neural Networks (SNNs) offer significant potential for enabling\nenergy-efficient intelligence at the edge. However, performing full SNN\ninference at the edge can be challenging due to the latency and energy\nconstraints arising from fixed and high timestep overheads. Edge-cloud\nco-inference systems present a promising solution, but their deployment is\noften hindered by high latency and feature transmission costs. To address these\nissues, we introduce NeuCODEX, a neuromorphic co-inference architecture that\njointly optimizes both spatial and temporal redundancy. NeuCODEX incorporates a\nlearned spike-driven compression module to reduce data transmission and employs\na dynamic early-exit mechanism to adaptively terminate inference based on\noutput confidence. We evaluated NeuCODEX on both static images (CIFAR10 and\nCaltech) and neuromorphic event streams (CIFAR10-DVS and N-Caltech). To\ndemonstrate practicality, we prototyped NeuCODEX on ResNet-18 and VGG-16\nbackbones in a real edge-to-cloud testbed. Our proposed system reduces data\ntransfer by up to 2048x and edge energy consumption by over 90%, while reducing\nend-to-end latency by up to 3x compared to edge-only inference, all with a\nnegligible accuracy drop of less than 2%. In doing so, NeuCODEX enables\npractical, high-performance SNN deployment in resource-constrained\nenvironments.", "AI": {"tldr": "NeuCODEX是一种神经形态协同推理架构，通过结合尖峰驱动压缩和动态提前退出机制，显著降低了边缘SNN部署的数据传输、能耗和端到端延迟，同时保持了高精度。", "motivation": "SNN在边缘计算中具有节能潜力，但其固定的高时间步开销导致高延迟和能耗。边缘-云协同推理虽有前景，却受限于高延迟和特征传输成本，阻碍了其部署。", "method": "本文提出了NeuCODEX，一个神经形态协同推理架构，它联合优化了空间和时间冗余。该架构包含一个学习型尖峰驱动压缩模块以减少数据传输，并采用动态提前退出机制根据输出置信度自适应地终止推理。", "result": "NeuCODEX在静态图像（CIFAR10、Caltech）和神经形态事件流（CIFAR10-DVS、N-Caltech）上进行了评估，并基于ResNet-18和VGG-16骨干网络在真实的边缘到云测试平台中进行了原型验证。结果显示，与纯边缘推理相比，数据传输减少高达2048倍，边缘能耗降低超过90%，端到端延迟降低高达3倍，而精度损失可忽略不计（低于2%）。", "conclusion": "NeuCODEX使得在资源受限环境中实现实用、高性能的SNN部署成为可能，有效解决了SNN边缘-云协同推理的挑战。"}}
{"id": "2509.19166", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19166", "abs": "https://arxiv.org/abs/2509.19166", "authors": ["Siddharth Gupta", "Jitin Singla"], "title": "YOLO-LAN: Precise Polyp Detection via Optimized Loss, Augmentations and Negatives", "comment": null, "summary": "Colorectal cancer (CRC), a lethal disease, begins with the growth of abnormal\nmucosal cell proliferation called polyps in the inner wall of the colon. When\nleft undetected, polyps can become malignant tumors. Colonoscopy is the\nstandard procedure for detecting polyps, as it enables direct visualization and\nremoval of suspicious lesions. Manual detection by colonoscopy can be\ninconsistent and is subject to oversight. Therefore, object detection based on\ndeep learning offers a better solution for a more accurate and real-time\ndiagnosis during colonoscopy. In this work, we propose YOLO-LAN, a YOLO-based\npolyp detection pipeline, trained using M2IoU loss, versatile data\naugmentations and negative data to replicate real clinical situations. Our\npipeline outperformed existing methods for the Kvasir-seg and BKAI-IGH NeoPolyp\ndatasets, achieving mAP$_{50}$ of 0.9619, mAP$_{50:95}$ of 0.8599 with YOLOv12\nand mAP$_{50}$ of 0.9540, mAP$_{50:95}$ of 0.8487 with YOLOv8 on the Kvasir-seg\ndataset. The significant increase is achieved in mAP$_{50:95}$ score, showing\nthe precision of polyp detection. We show robustness based on polyp size and\nprecise location detection, making it clinically relevant in AI-assisted\ncolorectal screening.", "AI": {"tldr": "本文提出了一种基于YOLO的息肉检测流水线YOLO-LAN，通过M2IoU损失、数据增强和负样本训练，在Kvasir-seg和BKAI-IGH NeoPolyp数据集上超越了现有方法，实现了更精确和鲁棒的息肉检测。", "motivation": "结直肠癌（CRC）是一种致命疾病，源于结肠息肉。结肠镜检查是检测息肉的标准方法，但人工检测存在不一致和遗漏的风险。因此，基于深度学习的目标检测为结肠镜检查期间提供更准确、实时的诊断提供了更好的解决方案。", "method": "研究提出了一种名为YOLO-LAN的基于YOLO的息肉检测流水线。该流水线使用M2IoU损失、多功能数据增强和负样本数据进行训练，以模拟真实的临床情况。实验中使用了YOLOv12和YOLOv8作为基础模型。", "result": "YOLO-LAN在Kvasir-seg和BKAI-IGH NeoPolyp数据集上均优于现有方法。在Kvasir-seg数据集上，使用YOLOv12时，mAP$_{50}$达到0.9619，mAP$_{50:95}$达到0.8599；使用YOLOv8时，mAP$_{50}$达到0.9540，mAP$_{50:95}$达到0.8487。尤其在mAP$_{50:95}$分数上实现了显著提升，表明了息肉检测的精确性。研究还展示了该方法在息肉大小和精确位置检测方面的鲁棒性。", "conclusion": "YOLO-LAN在息肉检测方面表现出卓越的性能，特别是在提高检测精度和鲁棒性方面。这使其在AI辅助的结直肠筛查中具有重要的临床相关性。"}}
{"id": "2509.19183", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19183", "abs": "https://arxiv.org/abs/2509.19183", "authors": ["Mingqi Gao", "Jingkun Chen", "Yunqi Miao", "Gengshen Wu", "Zhijin Qin", "Jungong Han"], "title": "The 1st Solution for MOSEv2 Challenge 2025: Long-term and Concept-aware Video Segmentation via SeC", "comment": null, "summary": "This technical report explores the MOSEv2 track of the LSVOS Challenge, which\ntargets complex semi-supervised video object segmentation. By analysing and\nadapting SeC, an enhanced SAM-2 framework, we conduct a detailed study of its\nlong-term memory and concept-aware memory, showing that long-term memory\npreserves temporal continuity under occlusion and reappearance, while\nconcept-aware memory supplies semantic priors that suppress distractors;\ntogether, these traits directly benefit several MOSEv2's core challenges. Our\nsolution achieves a JF score of 39.89% on the test set, ranking 1st in the\nMOSEv2 track of the LSVOS Challenge.", "AI": {"tldr": "本报告分析并改进了增强型SAM-2框架SeC，以解决LSVOS挑战中MOSEv2赛道的复杂半监督视频对象分割任务，并凭借其长时记忆和概念感知记忆的优势，最终在该赛道中排名第一。", "motivation": "解决LSVOS挑战中MOSEv2赛道的复杂半监督视频对象分割任务所面临的核心挑战，包括处理遮挡、重现以及抑制干扰物等问题。", "method": "通过分析和改进增强型SAM-2框架SeC，并深入研究其长时记忆和概念感知记忆。长时记忆用于在遮挡和重现情况下保持时间连续性，而概念感知记忆则提供语义先验以抑制干扰物。", "result": "研究表明，长时记忆能有效保持遮挡和重现下的时间连续性，概念感知记忆能提供抑制干扰物的语义先验。该解决方案在测试集上取得了39.89%的JF分数，并在LSVOS挑战的MOSEv2赛道中排名第一。", "conclusion": "SeC框架中长时记忆和概念感知记忆的结合，能够有效应对MOSEv2赛道的多个核心挑战，显著提升了复杂半监督视频对象分割的性能，并取得了领先的竞争成绩。"}}
{"id": "2509.19191", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19191", "abs": "https://arxiv.org/abs/2509.19191", "authors": ["Yueyan Li", "Chenggong Zhao", "Zeyuan Zang", "Caixia Yuan", "Xiaojie Wang"], "title": "Reading Images Like Texts: Sequential Image Understanding in Vision-Language Models", "comment": null, "summary": "Vision-Language Models (VLMs) have demonstrated remarkable performance across\na variety of real-world tasks. However, existing VLMs typically process visual\ninformation by serializing images, a method that diverges significantly from\nthe parallel nature of human vision. Moreover, their opaque internal mechanisms\nhinder both deeper understanding and architectural innovation. Inspired by the\ndual-stream hypothesis of human vision, which distinguishes the \"what\" and\n\"where\" pathways, we deconstruct the visual processing in VLMs into object\nrecognition and spatial perception for separate study. For object recognition,\nwe convert images into text token maps and find that the model's perception of\nimage content unfolds as a two-stage process from shallow to deep layers,\nbeginning with attribute recognition and culminating in semantic\ndisambiguation. For spatial perception, we theoretically derive and empirically\nverify the geometric structure underlying the positional representation in\nVLMs. Based on these findings, we introduce an instruction-agnostic token\ncompression algorithm based on a plug-and-play visual decoder to improve\ndecoding efficiency, and a RoPE scaling technique to enhance spatial reasoning.\nThrough rigorous experiments, our work validates these analyses, offering a\ndeeper understanding of VLM internals and providing clear principles for\ndesigning more capable future architectures.", "AI": {"tldr": "本文受人类视觉双流假说启发，解构了视觉语言模型（VLMs）的视觉处理过程，并提出了提高解码效率和空间推理能力的改进方法。", "motivation": "现有VLMs通常以串行方式处理视觉信息，与人类视觉的并行特性大相径庭。此外，其不透明的内部机制阻碍了深入理解和架构创新。", "method": "研究将VLM的视觉处理解构为物体识别（“是什么”）和空间感知（“在哪里”）。对于物体识别，将图像转换为文本token图，分析其从浅层到深层的两阶段感知过程。对于空间感知，理论推导并实证验证了VLM中位置表示的几何结构。基于这些发现，提出了基于即插即用视觉解码器的指令无关token压缩算法以提高解码效率，以及RoPE缩放技术以增强空间推理能力。", "result": "研究验证了对VLM内部机制的分析，揭示了物体识别从属性识别到语义消歧的两阶段过程，并推导验证了空间表示的几何结构。在此基础上，提出的token压缩算法有效提高了解码效率，RoPE缩放技术增强了模型的空间推理能力。", "conclusion": "本研究加深了对VLM内部机制的理解，并为设计更强大的未来VLM架构提供了清晰的原则和指导。"}}
{"id": "2509.19203", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19203", "abs": "https://arxiv.org/abs/2509.19203", "authors": ["Ioanna Ntinou", "Alexandros Xenos", "Yassine Ouali", "Adrian Bulat", "Georgios Tzimiropoulos"], "title": "Vision-Free Retrieval: Rethinking Multimodal Search with Textual Scene Descriptions", "comment": "Accepted at EMNLP 2025", "summary": "Contrastively-trained Vision-Language Models (VLMs), such as CLIP, have\nbecome the standard approach for learning discriminative vision-language\nrepresentations. However, these models often exhibit shallow language\nunderstanding, manifesting bag-of-words behaviour. These limitations are\nreinforced by their dual-encoder design, which induces a modality gap.\nAdditionally, the reliance on vast web-collected data corpora for training\nmakes the process computationally expensive and introduces significant privacy\nconcerns. To address these limitations, in this work, we challenge the\nnecessity of vision encoders for retrieval tasks by introducing a vision-free,\nsingle-encoder retrieval pipeline. Departing from the traditional text-to-image\nretrieval paradigm, we migrate to a text-to-text paradigm with the assistance\nof VLLM-generated structured image descriptions. We demonstrate that this\nparadigm shift has significant advantages, including a substantial reduction of\nthe modality gap, improved compositionality, and better performance on short\nand long caption queries, all attainable with only a few hours of calibration\non two GPUs. Additionally, substituting raw images with textual descriptions\nintroduces a more privacy-friendly alternative for retrieval. To further assess\ngeneralisation and address some of the shortcomings of prior compositionality\nbenchmarks, we release two benchmarks derived from Flickr30k and COCO,\ncontaining diverse compositional queries made of short captions, which we coin\nsubFlickr and subCOCO. Our vision-free retriever matches and often surpasses\ntraditional multimodal models. Importantly, our approach achieves\nstate-of-the-art zero-shot performance on multiple retrieval and\ncompositionality benchmarks, with models as small as 0.3B parameters. Code is\navailable at: https://github.com/IoannaNti/LexiCLIP", "AI": {"tldr": "该研究挑战了视觉编码器在检索任务中的必要性，提出了一种无视觉、单编码器的文本到文本检索方案，通过VLLM生成的图像描述取代原始图像，实现了卓越的性能、更小的模态差距和更好的隐私性。", "motivation": "现有的视觉-语言模型（如CLIP）存在语言理解浅薄（表现为词袋行为）、双编码器设计导致的模态差距、训练成本高昂以及隐私问题等局限性。这些问题促使研究者探索更高效、更深入且更私密友好的检索方法。", "method": "该研究引入了一种无视觉、单编码器的检索流程，将传统的文本到图像检索范式转变为文本到文本范式。具体做法是，利用大型视觉-语言模型（VLLM）生成结构化的图像描述，用这些描述替代原始图像进行检索。同时，发布了两个新的组合性基准测试（subFlickr和subCOCO）来评估泛化能力。", "result": "这种范式转变显著减小了模态差距，提高了组合性，并在短句和长句查询上表现出更好的性能，且仅需少量GPU校准时间。用文本描述替代原始图像还提供了一种更隐私友好的检索方案。该无视觉检索器匹配并经常超越传统多模态模型，在多个检索和组合性基准测试中实现了最先进的零样本性能，甚至使用参数量小至0.3B的模型也能达到。", "conclusion": "研究表明，对于检索任务而言，视觉编码器并非必需。通过VLLM生成的结构化图像描述实现的文本到文本检索范式，不仅能够克服现有视觉-语言模型的局限性，提供卓越的性能和组合性理解，还能显著提升计算效率和数据隐私性，为未来的检索系统开辟了新的方向。"}}
{"id": "2509.19207", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19207", "abs": "https://arxiv.org/abs/2509.19207", "authors": ["Israfel Salazar", "Desmond Elliott", "Yova Kementchedjhieva"], "title": "Long Story Short: Disentangling Compositionality and Long-Caption Understanding in VLMs", "comment": null, "summary": "Contrastive vision-language models (VLMs) have made significant progress in\nbinding visual and textual information, but understanding long, dense captions\nremains an open challenge. We hypothesize that compositionality, the capacity\nto reason about object-attribute bindings and inter-object relationships, is\nkey to understanding longer captions. In this paper, we investigate the\ninteraction between compositionality and long-caption understanding, asking\nwhether training for one property enhances the other. We train and evaluate a\nrange of models that target each of these capabilities. Our results reveal a\nbidirectional relationship: compositional training improves performance on\nlong-caption retrieval, and training on long captions promotes\ncompositionality. However, these gains are sensitive to data quality and model\ndesign. We find that training on poorly structured captions, or with limited\nparameter updates, fails to support generalization. Likewise, strategies that\naim at retaining general alignment, such as freezing positional embeddings, do\nnot improve compositional understanding. Overall, we find that compositional\nunderstanding and long-caption understanding are intertwined capabilities that\ncan be jointly learned through training on dense, grounded descriptions.\nDespite these challenges, we show that models trained on high-quality,\nlong-caption data can achieve strong performance in both tasks, offering\npractical guidance for improving VLM generalization.", "AI": {"tldr": "本文研究了对比视觉-语言模型（VLMs）中组合性与长文本理解之间的关系。研究发现两者之间存在双向促进作用，但效果受数据质量和模型设计的显著影响。高质量、密集的描述性数据有助于同时提升这两种能力。", "motivation": "对比VLMs在绑定视觉和文本信息方面取得了显著进展，但在理解长而密集的图像描述方面仍面临挑战。作者假设组合性（即理解物体属性绑定和物体间关系的能力）是理解长文本的关键。", "method": "研究人员训练并评估了一系列针对组合性和长文本理解能力的模型。他们调查了训练其中一种能力是否能增强另一种能力，并探究了数据质量（例如，结构不良的描述）和模型设计（例如，冻结位置嵌入）对泛化能力的影响。", "result": "结果显示，组合性训练能提升长文本检索的性能，而长文本训练也能促进组合性，两者之间存在双向关系。然而，这些提升对数据质量和模型设计很敏感：使用结构不良的描述或有限的参数更新无法支持泛化；保留通用对齐的策略（如冻结位置嵌入）也无法改善组合性理解。最终，通过高质量、密集的、有基础的描述进行训练的模型，在这两项任务中均能取得优异表现。", "conclusion": "组合性理解和长文本理解是相互交织的能力，可以通过在密集、有基础的描述上进行训练来共同学习。尽管存在挑战，但通过高质量的长文本数据训练模型，可以显著提升VLM的泛化能力，为未来的模型改进提供了实用指导。"}}
{"id": "2509.19208", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19208", "abs": "https://arxiv.org/abs/2509.19208", "authors": ["Earl Ranario", "Ismael Mayanja", "Heesup Yun", "Brian N. Bailey", "J. Mason Earles"], "title": "Enabling Plant Phenotyping in Weedy Environments using Multi-Modal Imagery via Synthetic and Generated Training Data", "comment": null, "summary": "Accurate plant segmentation in thermal imagery remains a significant\nchallenge for high throughput field phenotyping, particularly in outdoor\nenvironments where low contrast between plants and weeds and frequent\nocclusions hinder performance. To address this, we present a framework that\nleverages synthetic RGB imagery, a limited set of real annotations, and\nGAN-based cross-modality alignment to enhance semantic segmentation in thermal\nimages. We trained models on 1,128 synthetic images containing complex mixtures\nof crop and weed plants in order to generate image segmentation masks for crop\nand weed plants. We additionally evaluated the benefit of integrating as few as\nfive real, manually segmented field images within the training process using\nvarious sampling strategies. When combining all the synthetic images with a few\nlabeled real images, we observed a maximum relative improvement of 22% for the\nweed class and 17% for the plant class compared to the full real-data baseline.\nCross-modal alignment was enabled by translating RGB to thermal using\nCycleGAN-turbo, allowing robust template matching without calibration. Results\ndemonstrated that combining synthetic data with limited manual annotations and\ncross-domain translation via generative models can significantly boost\nsegmentation performance in complex field environments for multi-model imagery.", "AI": {"tldr": "该研究提出一个框架，结合合成RGB图像、少量真实标注和基于GAN的跨模态对齐，显著提升了热成像中植物分割的性能，尤其在杂草和植物类别上取得了显著改进。", "motivation": "在户外环境中，热成像中准确的植物分割对于高通量田间表型分析是一个重大挑战，因为植物和杂草之间对比度低且频繁遮挡会阻碍性能。", "method": "该研究提出一个框架，利用合成RGB图像、少量真实标注和基于GAN的跨模态对齐来增强热图像中的语义分割。模型在1,128张包含作物和杂草混合物的合成图像上进行训练。同时，评估了整合少量（低至5张）真实手动分割图像的益处。通过CycleGAN-turbo将RGB图像转换为热图像，实现跨模态对齐，无需校准即可进行鲁棒的模板匹配。", "result": "与仅使用真实数据的基线相比，结合所有合成图像和少量带标注的真实图像，杂草类别的性能最大相对提升了22%，植物类别提升了17%。", "conclusion": "结果表明，将合成数据与有限的手动标注以及通过生成模型进行的跨域转换相结合，可以显著提高复杂田间环境中多模态图像的分割性能。"}}
{"id": "2509.19230", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19230", "abs": "https://arxiv.org/abs/2509.19230", "authors": ["Tianshuo Zhang", "Li Gao", "Siran Peng", "Xiangyu Zhu", "Zhen Lei"], "title": "DevFD: Developmental Face Forgery Detection by Learning Shared and Orthogonal LoRA Subspaces", "comment": "Accepted by NeurIPS 2025", "summary": "The rise of realistic digital face generation and manipulation poses\nsignificant social risks. The primary challenge lies in the rapid and diverse\nevolution of generation techniques, which often outstrip the detection\ncapabilities of existing models. To defend against the ever-evolving new types\nof forgery, we need to enable our model to quickly adapt to new domains with\nlimited computation and data while avoiding forgetting previously learned\nforgery types. In this work, we posit that genuine facial samples are abundant\nand relatively stable in acquisition methods, while forgery faces continuously\nevolve with the iteration of manipulation techniques. Given the practical\ninfeasibility of exhaustively collecting all forgery variants, we frame face\nforgery detection as a continual learning problem and allow the model to\ndevelop as new forgery types emerge. Specifically, we employ a Developmental\nMixture of Experts (MoE) architecture that uses LoRA models as its individual\nexperts. These experts are organized into two groups: a Real-LoRA to learn and\nrefine knowledge of real faces, and multiple Fake-LoRAs to capture incremental\ninformation from different forgery types. To prevent catastrophic forgetting,\nwe ensure that the learning direction of Fake-LoRAs is orthogonal to the\nestablished subspace. Moreover, we integrate orthogonal gradients into the\northogonal loss of Fake-LoRAs, preventing gradient interference throughout the\ntraining process of each task. Experimental results under both the datasets and\nmanipulation types incremental protocols demonstrate the effectiveness of our\nmethod.", "AI": {"tldr": "本文提出一种基于持续学习和专家混合（MoE）架构的方法，通过使用LoRA模型作为专家，实现人脸伪造检测模型对不断演进的新伪造类型快速适应，同时避免灾难性遗忘。", "motivation": "逼真数字人脸生成和操纵技术的快速演进，使得现有检测模型难以跟上，且新伪造类型层出不穷。模型需要能在有限计算和数据下快速适应新领域，同时不忘记已学伪造类型，以应对持续变化的伪造威胁。", "method": "将人脸伪造检测视为一个持续学习问题。采用发展型专家混合（Developmental MoE）架构，其中LoRA模型作为独立专家。专家分为两组：一个Real-LoRA学习真实人脸知识，多个Fake-LoRA捕获不同伪造类型的增量信息。为防止灾难性遗忘，确保Fake-LoRA的学习方向与已建立的子空间正交，并集成正交梯度到Fake-LoRA的正交损失中，以防止训练过程中的梯度干扰。", "result": "在数据集和操纵类型增量协议下的实验结果表明，该方法有效。", "conclusion": "所提出的方法能有效应对人脸伪造检测中伪造技术不断演进的挑战，通过持续学习和特定的MoE架构，实现了对新伪造类型的快速适应和对旧知识的保留。"}}
{"id": "2509.19244", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19244", "abs": "https://arxiv.org/abs/2509.19244", "authors": ["Shufan Li", "Jiuxiang Gu", "Kangning Liu", "Zhe Lin", "Zijun Wei", "Aditya Grover", "Jason Kuen"], "title": "Lavida-O: Elastic Masked Diffusion Models for Unified Multimodal Understanding and Generation", "comment": "32 pages, 15 figures", "summary": "We proposed Lavida-O, a unified multi-modal Masked Diffusion Model (MDM)\ncapable of image understanding and generation tasks. Unlike existing multimodal\ndiffsion language models such as MMaDa and Muddit which only support simple\nimage-level understanding tasks and low-resolution image generation, Lavida-O\nexhibits many new capabilities such as object grounding, image-editing, and\nhigh-resolution (1024px) image synthesis. It is also the first unified MDM that\nuses its understanding capabilities to improve image generation and editing\nresults through planning and iterative self-reflection. To allow effective and\nefficient training and sampling, Lavida-O ntroduces many novel techniques such\nas Elastic Mixture-of-Transformer architecture, universal text conditioning,\nand stratified sampling. \\ours~achieves state-of-the-art performance on a wide\nrange of benchmarks such as RefCOCO object grounding, GenEval text-to-image\ngeneration, and ImgEdit image editing, outperforming existing autoregressive\nand continuous diffusion models such as Qwen2.5-VL and FluxKontext-dev, while\noffering considerable speedup at inference.", "AI": {"tldr": "Lavida-O 是一种统一的多模态掩码扩散模型（MDM），能够处理图像理解和生成任务，支持高分辨率图像合成、对象定位和图像编辑，并通过规划和自我反思提升效果，实现了最先进的性能和推理速度提升。", "motivation": "现有的多模态扩散语言模型（如 MMaDa 和 Muddit）仅支持简单的图像级理解任务和低分辨率图像生成，缺乏更高级的能力如对象定位和高分辨率合成。", "method": "Lavida-O 是一个统一的 MDM，利用其理解能力通过规划和迭代自我反思来改进图像生成和编辑结果。它引入了多种新技术，包括弹性 Transformer 混合架构（Elastic Mixture-of-Transformer）、通用文本条件（universal text conditioning）和分层采样（stratified sampling），以实现有效高效的训练和采样。", "result": "Lavida-O 在 RefCOCO 对象定位、GenEval 文本到图像生成和 ImgEdit 图像编辑等广泛基准测试中取得了最先进的性能，优于现有的自回归和连续扩散模型（如 Qwen2.5-VL 和 FluxKontext-dev），同时显著提高了推理速度。它还展示了对象定位、图像编辑和高分辨率（1024px）图像合成等新能力。", "conclusion": "Lavida-O 是首个统一的多模态掩码扩散模型，通过其创新的架构和训练/采样技术，在图像理解和生成任务中取得了显著进展，实现了多项新能力、最先进的性能和更高的推理效率。"}}
{"id": "2509.19245", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19245", "abs": "https://arxiv.org/abs/2509.19245", "authors": ["Benedetta Liberatori", "Alessandro Conti", "Lorenzo Vaquero", "Yiming Wang", "Elisa Ricci", "Paolo Rota"], "title": "ConViS-Bench: Estimating Video Similarity Through Semantic Concepts", "comment": "Accepted to NeurIPS 2025", "summary": "What does it mean for two videos to be similar? Videos may appear similar\nwhen judged by the actions they depict, yet entirely different if evaluated\nbased on the locations where they were filmed. While humans naturally compare\nvideos by taking different aspects into account, this ability has not been\nthoroughly studied and presents a challenge for models that often depend on\nbroad global similarity scores. Large Multimodal Models (LMMs) with video\nunderstanding capabilities open new opportunities for leveraging natural\nlanguage in comparative video tasks. We introduce Concept-based Video\nSimilarity estimation (ConViS), a novel task that compares pairs of videos by\ncomputing interpretable similarity scores across a predefined set of key\nsemantic concepts. ConViS allows for human-like reasoning about video\nsimilarity and enables new applications such as concept-conditioned video\nretrieval. To support this task, we also introduce ConViS-Bench, a new\nbenchmark comprising carefully annotated video pairs spanning multiple domains.\nEach pair comes with concept-level similarity scores and textual descriptions\nof both differences and similarities. Additionally, we benchmark several\nstate-of-the-art models on ConViS, providing insights into their alignment with\nhuman judgments. Our results reveal significant performance differences on\nConViS, indicating that some concepts present greater challenges for estimating\nvideo similarity. We believe that ConViS-Bench will serve as a valuable\nresource for advancing research in language-driven video understanding.", "AI": {"tldr": "该研究引入了一种名为ConViS的新任务和基准，旨在通过预定义的语义概念集来计算可解释的视频相似度分数，从而实现更接近人类的视频比较，并评估了现有模型在该任务上的表现。", "motivation": "现有视频相似度模型通常依赖宽泛的全局相似度分数，而人类在比较视频时会考虑不同方面（如动作、地点）。这种能力尚未被充分研究，且对模型构成挑战。具有视频理解能力的大型多模态模型（LMMs）为利用自然语言进行视频比较提供了新机遇。", "method": "研究引入了“基于概念的视频相似度估计”（ConViS）任务，通过计算预定义关键语义概念上的可解释相似度分数来比较视频对。为支持此任务，研究还引入了ConViS-Bench基准，包含经过精心标注的视频对，每对都附有概念级相似度分数以及差异和相似性的文本描述。此外，研究还在ConViS上对多个最先进模型进行了基准测试。", "result": "基准测试结果显示，模型在ConViS任务上存在显著的性能差异，表明某些概念在估计视频相似度方面提出了更大的挑战。这揭示了模型与人类判断之间的一致性水平。", "conclusion": "ConViS-Bench将成为推动语言驱动视频理解研究的宝贵资源。该任务和基准有助于实现更细致、更具解释性的视频相似度评估，并可能催生概念条件视频检索等新应用。"}}
{"id": "2509.19258", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19258", "abs": "https://arxiv.org/abs/2509.19258", "authors": ["Dheerendranath Battalapalli", "Apoorva Safai", "Maria Jaramillo", "Hyemin Um", "Gustavo Adalfo Pineda Ortiz", "Ulas Bagci", "Manmeet Singh Ahluwalia", "Marwa Ismail", "Pallavi Tiwari"], "title": "Graph-Radiomic Learning (GrRAiL) Descriptor to Characterize Imaging Heterogeneity in Confounding Tumor Pathologies", "comment": "Under Review: npj Digital Medicine", "summary": "A significant challenge in solid tumors is reliably distinguishing\nconfounding pathologies from malignant neoplasms on routine imaging. While\nradiomics methods seek surrogate markers of lesion heterogeneity on CT/MRI,\nmany aggregate features across the region of interest (ROI) and miss complex\nspatial relationships among varying intensity compositions. We present a new\nGraph-Radiomic Learning (GrRAiL) descriptor for characterizing intralesional\nheterogeneity (ILH) on clinical MRI scans. GrRAiL (1) identifies clusters of\nsub-regions using per-voxel radiomic measurements, then (2) computes\ngraph-theoretic metrics to quantify spatial associations among clusters. The\nresulting weighted graphs encode higher-order spatial relationships within the\nROI, aiming to reliably capture ILH and disambiguate confounding pathologies\nfrom malignancy. To assess efficacy and clinical feasibility, GrRAiL was\nevaluated in n=947 subjects spanning three use cases: differentiating tumor\nrecurrence from radiation effects in glioblastoma (GBM; n=106) and brain\nmetastasis (n=233), and stratifying pancreatic intraductal papillary mucinous\nneoplasms (IPMNs) into no+low vs high risk (n=608). In a multi-institutional\nsetting, GrRAiL consistently outperformed state-of-the-art baselines - Graph\nNeural Networks (GNNs), textural radiomics, and intensity-graph analysis. In\nGBM, cross-validation (CV) and test accuracies for recurrence vs\npseudo-progression were 89% and 78% with >10% test-accuracy gains over\ncomparators. In brain metastasis, CV and test accuracies for recurrence vs\nradiation necrosis were 84% and 74% (>13% improvement). For IPMN risk\nstratification, CV and test accuracies were 84% and 75%, showing >10%\nimprovement.", "AI": {"tldr": "本文提出了一种新的图-放射组学学习（GrRAiL）描述符，通过量化病灶内异质性，在多种肿瘤鉴别诊断任务中（如肿瘤复发与放射效应区分、肿瘤风险分层）显著优于现有基线方法。", "motivation": "在实体瘤影像学中，可靠地区分混淆性病理与恶性肿瘤是一个重大挑战。现有的放射组学方法通常聚合感兴趣区域（ROI）内的特征，从而忽略了不同强度成分之间复杂的空间关系。", "method": "GrRAiL方法首先使用每个体素的放射组学测量来识别子区域簇，然后计算图论指标以量化这些簇之间的空间关联。由此产生的加权图编码了ROI内更高阶的空间关系，旨在可靠地捕捉病灶内异质性（ILH）并区分混淆性病理与恶性肿瘤。", "result": "GrRAiL在n=947名受试者的三个临床应用中进行了评估：区分胶质母细胞瘤（GBM）的肿瘤复发与放射效应（n=106）、区分脑转移瘤的复发与放射性坏死（n=233），以及将胰腺导管内乳头状黏液性肿瘤（IPMNs）分为无/低风险与高风险（n=608）。在多机构环境中，GrRAiL始终优于图神经网络（GNNs）、纹理放射组学和强度图分析等最新基线方法。在GBM中，复发与假性进展的交叉验证和测试准确率分别为89%和78%，比对照组高出10%以上的测试准确率。在脑转移瘤中，复发与放射性坏死的准确率分别为84%和74%（提高13%以上）。对于IPMN风险分层，准确率分别为84%和75%（提高10%以上）。", "conclusion": "GrRAiL能够可靠地捕捉病灶内异质性，并在临床MRI扫描中有效区分混淆性病理与恶性肿瘤，展示了其在多种肿瘤鉴别诊断任务中的优越性能和临床可行性。"}}
{"id": "2509.19259", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19259", "abs": "https://arxiv.org/abs/2509.19259", "authors": ["Markos Diomataris", "Berat Mert Albaba", "Giorgio Becherini", "Partha Ghosh", "Omid Taheri", "Michael J. Black"], "title": "Moving by Looking: Towards Vision-Driven Avatar Motion Generation", "comment": null, "summary": "The way we perceive the world fundamentally shapes how we move, whether it is\nhow we navigate in a room or how we interact with other humans. Current human\nmotion generation methods, neglect this interdependency and use task-specific\n``perception'' that differs radically from that of humans. We argue that the\ngeneration of human-like avatar behavior requires human-like perception.\nConsequently, in this work we present CLOPS, the first human avatar that solely\nuses egocentric vision to perceive its surroundings and navigate. Using vision\nas the primary driver of motion however, gives rise to a significant challenge\nfor training avatars: existing datasets have either isolated human motion,\nwithout the context of a scene, or lack scale. We overcome this challenge by\ndecoupling the learning of low-level motion skills from learning of high-level\ncontrol that maps visual input to motion. First, we train a motion prior model\non a large motion capture dataset. Then, a policy is trained using Q-learning\nto map egocentric visual inputs to high-level control commands for the motion\nprior. Our experiments empirically demonstrate that egocentric vision can give\nrise to human-like motion characteristics in our avatars. For example, the\navatars walk such that they avoid obstacles present in their visual field.\nThese findings suggest that equipping avatars with human-like sensors,\nparticularly egocentric vision, holds promise for training avatars that behave\nlike humans.", "AI": {"tldr": "该研究提出CLOPS，首个仅使用自我中心视觉感知周围环境并导航的人形虚拟形象，通过解耦低级运动技能和高级视觉控制，实现了类人运动特性。", "motivation": "当前人体运动生成方法忽略了感知与运动的相互依赖性，并使用与人类感知差异很大的任务特定“感知”。研究认为，生成类人虚拟形象行为需要类人感知。", "method": "开发了CLOPS，一个仅使用自我中心视觉感知和导航的虚拟形象。为解决训练挑战，将低级运动技能学习与将视觉输入映射到运动的高级控制学习解耦。首先，在大规模动作捕捉数据集上训练运动先验模型；然后，使用Q学习训练策略，将自我中心视觉输入映射到运动先验的高级控制指令。", "result": "实验证明，自我中心视觉能使虚拟形象产生类人运动特性，例如，虚拟形象会避开视野中的障碍物。", "conclusion": "研究结果表明，为虚拟形象配备类人传感器，特别是自我中心视觉，有望训练出行为更像人类的虚拟形象。"}}
{"id": "2509.19282", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19282", "abs": "https://arxiv.org/abs/2509.19282", "authors": ["Bingnan Li", "Chen-Yu Wang", "Haiyang Xu", "Xiang Zhang", "Ethan Armand", "Divyansh Srivastava", "Xiaojun Shan", "Zeyuan Chen", "Jianwen Xie", "Zhuowen Tu"], "title": "OverLayBench: A Benchmark for Layout-to-Image Generation with Dense Overlaps", "comment": "Accepted to NeurIPS 2025 Dataset&Benchmark Track", "summary": "Despite steady progress in layout-to-image generation, current methods still\nstruggle with layouts containing significant overlap between bounding boxes. We\nidentify two primary challenges: (1) large overlapping regions and (2)\noverlapping instances with minimal semantic distinction. Through both\nqualitative examples and quantitative analysis, we demonstrate how these\nfactors degrade generation quality. To systematically assess this issue, we\nintroduce OverLayScore, a novel metric that quantifies the complexity of\noverlapping bounding boxes. Our analysis reveals that existing benchmarks are\nbiased toward simpler cases with low OverLayScore values, limiting their\neffectiveness in evaluating model performance under more challenging\nconditions. To bridge this gap, we present OverLayBench, a new benchmark\nfeaturing high-quality annotations and a balanced distribution across different\nlevels of OverLayScore. As an initial step toward improving performance on\ncomplex overlaps, we also propose CreatiLayout-AM, a model fine-tuned on a\ncurated amodal mask dataset. Together, our contributions lay the groundwork for\nmore robust layout-to-image generation under realistic and challenging\nscenarios. Project link: https://mlpc-ucsd.github.io/OverLayBench.", "AI": {"tldr": "该研究解决了布局到图像生成中边界框重叠严重的问题，提出了量化重叠复杂度的指标OverLayScore和新基准OverLayBench，并提出了初步改进模型CreatiLayout-AM，以促进更鲁棒的生成。", "motivation": "当前的布局到图像生成方法在处理边界框之间存在显著重叠的布局时表现不佳，主要挑战在于大面积重叠区域和语义区分度低的重叠实例，这导致生成质量下降。", "method": "1. 识别并分析了重叠边界框对生成质量的影响。2. 引入了OverLayScore，一个量化重叠边界框复杂度的指标。3. 创建了OverLayBench，一个具有高质量标注和不同OverLayScore值平衡分布的新基准。4. 提出了CreatiLayout-AM，一个在精选无模态掩码数据集上微调的模型，作为解决复杂重叠的初步方案。", "result": "1. 定性和定量分析表明重叠区域和语义区分度低会降低生成质量。2. 发现现有基准偏向于较低OverLayScore值的简单情况，限制了它们在挑战性条件下的评估有效性。3. OverLayBench提供了一个更具挑战性和平衡性的评估平台。4. CreatiLayout-AM作为初步尝试，展示了在复杂重叠场景下改进性能的潜力。", "conclusion": "该研究通过引入OverLayScore和OverLayBench，为在真实和挑战性场景下评估和开发更鲁棒的布局到图像生成方法奠定了基础，并提出了CreatiLayout-AM作为改进复杂重叠处理的初步步骤。"}}
{"id": "2509.19296", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2509.19296", "abs": "https://arxiv.org/abs/2509.19296", "authors": ["Sherwin Bahmani", "Tianchang Shen", "Jiawei Ren", "Jiahui Huang", "Yifeng Jiang", "Haithem Turki", "Andrea Tagliasacchi", "David B. Lindell", "Zan Gojcic", "Sanja Fidler", "Huan Ling", "Jun Gao", "Xuanchi Ren"], "title": "Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model Self-Distillation", "comment": "Project Page: https://research.nvidia.com/labs/toronto-ai/lyra/", "summary": "The ability to generate virtual environments is crucial for applications\nranging from gaming to physical AI domains such as robotics, autonomous\ndriving, and industrial AI. Current learning-based 3D reconstruction methods\nrely on the availability of captured real-world multi-view data, which is not\nalways readily available. Recent advancements in video diffusion models have\nshown remarkable imagination capabilities, yet their 2D nature limits the\napplications to simulation where a robot needs to navigate and interact with\nthe environment. In this paper, we propose a self-distillation framework that\naims to distill the implicit 3D knowledge in the video diffusion models into an\nexplicit 3D Gaussian Splatting (3DGS) representation, eliminating the need for\nmulti-view training data. Specifically, we augment the typical RGB decoder with\na 3DGS decoder, which is supervised by the output of the RGB decoder. In this\napproach, the 3DGS decoder can be purely trained with synthetic data generated\nby video diffusion models. At inference time, our model can synthesize 3D\nscenes from either a text prompt or a single image for real-time rendering. Our\nframework further extends to dynamic 3D scene generation from a monocular input\nvideo. Experimental results show that our framework achieves state-of-the-art\nperformance in static and dynamic 3D scene generation.", "AI": {"tldr": "本文提出一种自蒸馏框架，将视频扩散模型中的隐式3D知识蒸馏到显式3D高斯泼溅（3DGS）表示中，无需多视角训练数据，实现从文本、单图或单目视频实时生成静态和动态3D场景。", "motivation": "生成虚拟环境对游戏、机器人、自动驾驶等领域至关重要。现有基于学习的3D重建方法依赖于难以获取的真实世界多视角数据。视频扩散模型虽有强大想象力但仅限于2D，限制了其在需要与环境交互的3D模拟中的应用。", "method": "提出一个自蒸馏框架。在典型的RGB解码器基础上增加一个3DGS解码器，该3DGS解码器由RGB解码器的输出进行监督。通过这种方式，3DGS解码器可以完全使用视频扩散模型生成的合成数据进行训练。在推理时，模型可以从文本提示或单张图像合成3D场景，并支持实时渲染。框架还扩展到从单目输入视频生成动态3D场景。", "result": "在静态和动态3D场景生成方面，该框架取得了最先进的性能。模型能够在推理时从文本提示、单张图像或单目视频合成3D场景，并支持实时渲染。", "conclusion": "该框架通过自蒸馏方式，成功将2D视频扩散模型的隐式3D知识转化为显式3DGS表示，解决了多视角训练数据需求的问题，并实现了从多种2D输入实时生成高质量的静态和动态3D场景，达到了最先进水平。"}}
{"id": "2509.19297", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19297", "abs": "https://arxiv.org/abs/2509.19297", "authors": ["Weijie Wang", "Yeqing Chen", "Zeyu Zhang", "Hengyu Liu", "Haoxiao Wang", "Zhiyuan Feng", "Wenkang Qin", "Zheng Zhu", "Donny Y. Chen", "Bohan Zhuang"], "title": "VolSplat: Rethinking Feed-Forward 3D Gaussian Splatting with Voxel-Aligned Prediction", "comment": "Project Page: https://lhmd.top/volsplat, Code:\n  https://github.com/ziplab/VolSplat", "summary": "Feed-forward 3D Gaussian Splatting (3DGS) has emerged as a highly effective\nsolution for novel view synthesis. Existing methods predominantly rely on a\npixel-aligned Gaussian prediction paradigm, where each 2D pixel is mapped to a\n3D Gaussian. We rethink this widely adopted formulation and identify several\ninherent limitations: it renders the reconstructed 3D models heavily dependent\non the number of input views, leads to view-biased density distributions, and\nintroduces alignment errors, particularly when source views contain occlusions\nor low texture. To address these challenges, we introduce VolSplat, a new\nmulti-view feed-forward paradigm that replaces pixel alignment with\nvoxel-aligned Gaussians. By directly predicting Gaussians from a predicted 3D\nvoxel grid, it overcomes pixel alignment's reliance on error-prone 2D feature\nmatching, ensuring robust multi-view consistency. Furthermore, it enables\nadaptive control over Gaussian density based on 3D scene complexity, yielding\nmore faithful Gaussian point clouds, improved geometric consistency, and\nenhanced novel-view rendering quality. Experiments on widely used benchmarks\nincluding RealEstate10K and ScanNet demonstrate that VolSplat achieves\nstate-of-the-art performance while producing more plausible and view-consistent\nGaussian reconstructions. In addition to superior results, our approach\nestablishes a more scalable framework for feed-forward 3D reconstruction with\ndenser and more robust representations, paving the way for further research in\nwider communities. The video results, code and trained models are available on\nour project page: https://lhmd.top/volsplat.", "AI": {"tldr": "VolSplat提出了一种新的多视角前馈3D高斯溅射范式，用体素对齐的高斯取代像素对齐的高斯，以解决现有方法的局限性，实现了最先进的新视角合成性能和更鲁棒的3D重建。", "motivation": "现有的前馈3D高斯溅射方法主要依赖于像素对齐的高斯预测，导致重建的3D模型过度依赖输入视图数量、产生视角偏置的密度分布，并在遮挡或低纹理区域引入对齐误差。", "method": "VolSplat引入了一种新的多视角前馈范式，用体素对齐的高斯取代像素对齐。它直接从预测的3D体素网格中预测高斯，避免了易出错的2D特征匹配，确保了鲁棒的多视角一致性。此外，它能根据3D场景复杂性自适应控制高斯密度。", "result": "在RealEstate10K和ScanNet等基准测试中，VolSplat达到了最先进的性能，生成了更合理、视角更一致的高斯重建。它还建立了一个更具可扩展性的前馈3D重建框架，具有更密集、更鲁棒的表示。", "conclusion": "VolSplat通过提出体素对齐的高斯范式，克服了现有像素对齐方法的局限性，实现了卓越的新视角渲染质量和更可靠的3D几何一致性，为未来更广泛的3D重建研究铺平了道路。"}}
{"id": "2509.19300", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19300", "abs": "https://arxiv.org/abs/2509.19300", "authors": ["Chen Chen", "Pengsheng Guo", "Liangchen Song", "Jiasen Lu", "Rui Qian", "Xinze Wang", "Tsu-Jui Fu", "Wei Liu", "Yinfei Yang", "Alex Schwing"], "title": "CAR-Flow: Condition-Aware Reparameterization Aligns Source and Target for Better Flow Matching", "comment": null, "summary": "Conditional generative modeling aims to learn a conditional data distribution\nfrom samples containing data-condition pairs. For this, diffusion and\nflow-based methods have attained compelling results. These methods use a\nlearned (flow) model to transport an initial standard Gaussian noise that\nignores the condition to the conditional data distribution. The model is hence\nrequired to learn both mass transport and conditional injection. To ease the\ndemand on the model, we propose Condition-Aware Reparameterization for Flow\nMatching (CAR-Flow) -- a lightweight, learned shift that conditions the source,\nthe target, or both distributions. By relocating these distributions, CAR-Flow\nshortens the probability path the model must learn, leading to faster training\nin practice. On low-dimensional synthetic data, we visualize and quantify the\neffects of CAR. On higher-dimensional natural image data (ImageNet-256),\nequipping SiT-XL/2 with CAR-Flow reduces FID from 2.07 to 1.68, while\nintroducing less than 0.6% additional parameters.", "AI": {"tldr": "该论文提出了CAR-Flow，一种用于流匹配的条件感知重参数化方法，通过学习轻量级偏移来调整源或目标分布，从而简化模型学习任务，实现更快训练和更优性能，且参数开销极小。", "motivation": "现有的条件生成模型（如扩散模型和流模型）需要学习质量传输和条件注入两项任务，这对模型要求较高。研究旨在减轻模型的学习负担。", "method": "提出条件感知重参数化（CAR-Flow），它是一个轻量级的、学习到的偏移，用于调整源分布、目标分布或两者。通过重新定位这些分布，CAR-Flow缩短了模型需要学习的概率路径。", "result": "在低维合成数据上，CAR-Flow的效果得到了可视化和量化。在ImageNet-256图像数据上，将CAR-Flow集成到SiT-XL/2中，FID从2.07降低到1.68，同时只增加了不到0.6%的额外参数，并且在实践中实现了更快的训练。", "conclusion": "CAR-Flow通过条件感知重参数化有效简化了条件生成模型的学习任务，显著提升了模型性能（降低FID）并加速了训练，同时保持了极低的额外参数开销。"}}
