<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 64]
- [cs.CV](#cs.CV) [Total: 213]
- [cs.CL](#cs.CL) [Total: 140]
- [cs.RO](#cs.RO) [Total: 74]
- [eess.SY](#eess.SY) [Total: 31]
- [eess.IV](#eess.IV) [Total: 6]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Identifying Critical Pathways in Coronary Heart Disease via Fuzzy Subgraph Connectivity](https://arxiv.org/abs/2509.16288)
*Shanookha Ali,Nitha Niralda P C*

Main category: cs.AI

TL;DR: 该研究利用模糊子图连通性（FSC）构建模糊冠心病（CHD）图，以量化CHD风险因素间的不确定关系，从而识别诊断路径、主导风险因素和关键联系。


<details>
  <summary>Details</summary>
Motivation: 冠心病（CHD）由不可控、可控生活方式和临床指标等复杂因素相互作用引起，这些关系通常不确定。需要一种系统工具来捕捉这种不精确性。

Method: 构建了一个模糊CHD图，其中顶点代表不可控、可控因素和指标，边通过模糊隶属度加权。使用模糊子图连通性（FSC）评估连通性，以识别最强的诊断路径、主导风险因素和关键桥梁。

Result: 结果表明，FSC突出了有影响力的通路，界定了最弱和最强相关性之间的连通性，并揭示了移除后会降低预测强度的关键边。

Conclusion: FSC为CHD风险预测中的不确定性建模和支持临床决策提供了一个可解释且稳健的框架。

Abstract: Coronary heart disease (CHD) arises from complex interactions among
uncontrollable factors, controllable lifestyle factors, and clinical
indicators, where relationships are often uncertain. Fuzzy subgraph
connectivity (FSC) provides a systematic tool to capture such imprecision by
quantifying the strength of association between vertices and subgraphs in fuzzy
graphs. In this work, a fuzzy CHD graph is constructed with vertices for
uncontrollable, controllable, and indicator components, and edges weighted by
fuzzy memberships. Using FSC, we evaluate connectivity to identify strongest
diagnostic routes, dominant risk factors, and critical bridges. Results show
that FSC highlights influential pathways, bounds connectivity between weakest
and strongest correlations, and reveals critical edges whose removal reduces
predictive strength. Thus, FSC offers an interpretable and robust framework for
modeling uncertainty in CHD risk prediction and supporting clinical
decision-making.

</details>


### [2] [A global view of diverse construction methods of fuzzy implication functions rooted on F-chains](https://arxiv.org/abs/2509.16298)
*Raquel Fernandez-Peralta,Juan Vicente Riera*

Main category: cs.AI

TL;DR: 本文泛化了模糊蕴涵函数的一种F链构造方法，通过使用函数集合和两个递增函数，使其成为现有多种构造方法的统一框架，揭示了不同构造策略间的结构相似性。


<details>
  <summary>Details</summary>
Motivation: 模糊蕴涵函数在模糊逻辑中非常重要，但其多样性需要更深入地理解其结构关系，特别是从现有函数构造新函数的方法。

Method: 泛化了Mesiar等人提出的F链构造方法，不再使用单个模糊蕴涵函数，而是使用模糊蕴涵函数集合，并用两个不同的递增函数代替单一的F链。同时分析了此构造下的性质保持性并建立了充分条件。

Result: 泛化的F链构造方法是一个统一框架，能够涵盖现有多种构造技术，如逆反、聚合以及广义垂直/水平阈值方法。这揭示了看似不同构造策略之间的结构相似性。

Conclusion: 泛化的F链构造方法为模糊蕴涵函数的构造方法提供了统一的视角和连贯的理解，整合了多种现有技术。

Abstract: Fuzzy implication functions are one of the most important operators used in
the fuzzy logic framework. While their flexible definition allows for diverse
families with distinct properties, this variety needs a deeper theoretical
understanding of their structural relationships. In this work, we focus on the
study of construction methods, which employ different techniques to generate
new fuzzy implication functions from existing ones. Particularly, we generalize
the $F$-chain-based construction, recently introduced by Mesiar et al. to
extend a method for constructing aggregation functions to the context of fuzzy
implication functions. Our generalization employs collections of fuzzy
implication functions rather than single ones, and uses two different
increasing functions instead of a unique $F$-chain. We analyze property
preservation under this construction and establish sufficient conditions.
Furthermore, we demonstrate that our generalized $F$-chain-based construction
is a unifying framework for several existing methods. In particular, we show
that various construction techniques, such as contraposition, aggregation, and
generalized vertical/horizontal threshold methods, can be reformulated within
our approach. This reveals structural similarities between seemingly distinct
construction strategies and provides a cohesive perspective on fuzzy
implication construction methods.

</details>


### [3] [On the Non-Uniqueness of Representation of $(U,N)$-Implications](https://arxiv.org/abs/2509.16299)
*Raquel Fernandez-Peralta,Andrea Mesiarová-Zemánková*

Main category: cs.AI

TL;DR: 本文推翻了先前关于模糊否定连续时(U,N)-蕴涵函数具有唯一表示的结论，并对具有连续和非连续底层函数的uninorm的唯一性条件进行了全面研究。


<details>
  <summary>Details</summary>
Motivation: 模糊蕴涵函数是模糊逻辑系统的基本运算符。先前的研究认为，在模糊否定N连续的假设下，(S,N)-蕴涵和(U,N)-蕴涵函数具有唯一的表示。本文旨在挑战并重新审视(U,N)-蕴涵函数在这一假设下的唯一性。

Method: 通过反驳先前关于(U,N)-蕴涵函数唯一表示的事实，并对具有连续和非连续底层函数的uninorm的唯一性条件进行全面的理论研究。

Result: 本文证明了即使模糊否定是连续的，(U,N)-蕴涵函数也不一定具有唯一的表示，从而推翻了先前的结论。此外，本文还对具有连续和非连续底层函数的uninorm的唯一性条件进行了全面的研究。

Conclusion: 研究结果为这些模糊蕴涵算子的结构特性，特别是其表示的唯一性，提供了重要的理论见解。

Abstract: Fuzzy implication functions constitute fundamental operators in fuzzy logic
systems, extending classical conditionals to manage uncertainty in logical
inference. Among the extensive families of these operators, generalizations of
the classical material implication have received considerable theoretical
attention, particularly $(S,N)$-implications constructed from t-conorms and
fuzzy negations, and their further generalizations to $(U,N)$-implications
using disjunctive uninorms. Prior work has established characterization
theorems for these families under the assumption that the fuzzy negation $N$ is
continuous, ensuring uniqueness of representation. In this paper, we disprove
this last fact for $(U,N)$-implications and we show that they do not
necessarily possess a unique representation, even if the fuzzy negation is
continuous. Further, we provide a comprehensive study of uniqueness conditions
for both uninorms with continuous and non-continuous underlying functions. Our
results offer important theoretical insights into the structural properties of
these operators.

</details>


### [4] [Generalizability of Large Language Model-Based Agents: A Comprehensive Survey](https://arxiv.org/abs/2509.16330)
*Minxing Zhang,Yi Yang,Roy Xie,Bhuwan Dhingra,Shuyan Zhou,Jian Pei*

Main category: cs.AI

TL;DR: 这篇综述首次全面审视了大型语言模型（LLM）代理的泛化能力，强调了其重要性，定义了其边界，回顾了评估方法和改进策略，并指出了未来的研究方向。


<details>
  <summary>Details</summary>
Motivation: LLM代理已成为超越文本生成的新范式，但其泛化能力（即在不同指令、任务、环境和领域中保持一致性能的能力）是一个关键挑战。目前，泛化能力的概念尚未明确定义，也缺乏系统性的测量和改进方法。

Method: 本研究通过以下方式进行：1) 强调泛化能力对利益相关者的重要性；2) 将泛化能力置于分层领域-任务本体论中以明确其边界；3) 回顾现有数据集、评估维度和指标的局限性；4) 将改进泛化能力的方法分为针对骨干LLM、代理组件及其交互的三类；5) 区分可泛化框架和可泛化代理，并阐述如何将框架转化为代理层面的泛化能力；6) 识别关键挑战和未来方向。

Result: 本综述提供了LLM代理泛化能力的首次全面回顾，澄清了其概念边界，总结了现有评估方法及其不足，系统地分类了提升泛化能力的方法，并提出了可泛化框架与代理的区别。此外，它还指出了开发标准化框架、基于方差和成本的指标以及整合方法创新与架构设计的未来研究方向。

Conclusion: 本综述旨在为构建能在多样化应用中可靠泛化的LLM代理的原理性研究奠定基础，通过综合现有进展并突出未来机遇，推动该领域的发展。

Abstract: Large Language Model (LLM)-based agents have emerged as a new paradigm that
extends LLMs' capabilities beyond text generation to dynamic interaction with
external environments. By integrating reasoning with perception, memory, and
tool use, agents are increasingly deployed in diverse domains like web
navigation and household robotics. A critical challenge, however, lies in
ensuring agent generalizability - the ability to maintain consistent
performance across varied instructions, tasks, environments, and domains,
especially those beyond agents' fine-tuning data. Despite growing interest, the
concept of generalizability in LLM-based agents remains underdefined, and
systematic approaches to measure and improve it are lacking. In this survey, we
provide the first comprehensive review of generalizability in LLM-based agents.
We begin by emphasizing agent generalizability's importance by appealing to
stakeholders and clarifying the boundaries of agent generalizability by
situating it within a hierarchical domain-task ontology. We then review
datasets, evaluation dimensions, and metrics, highlighting their limitations.
Next, we categorize methods for improving generalizability into three groups:
methods for the backbone LLM, for agent components, and for their interactions.
Moreover, we introduce the distinction between generalizable frameworks and
generalizable agents and outline how generalizable frameworks can be translated
into agent-level generalizability. Finally, we identify critical challenges and
future directions, including developing standardized frameworks, variance- and
cost-based metrics, and approaches that integrate methodological innovations
with architecture-level designs. By synthesizing progress and highlighting
opportunities, this survey aims to establish a foundation for principled
research on building LLM-based agents that generalize reliably across diverse
applications.

</details>


### [5] [Psychometric Personality Shaping Modulates Capabilities and Safety in Language Models](https://arxiv.org/abs/2509.16332)
*Stephen Fitz,Peter Romero,Steven Basart,Sipeng Chen,Jose Hernandez-Orallo*

Main category: cs.AI

TL;DR: 研究发现，通过大五人格框架对大型语言模型（LLM）的心理测量人格进行控制，特别是降低“尽责性”，会显著降低模型在安全和通用能力基准上的表现。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型越来越多地应用于高风险交互，尽管已有研究表明LLM具有一致且可测量的合成人格特质，但目前尚不清楚调节这些特质如何影响模型行为。本研究旨在填补这一空白。

Method: 本研究基于大五人格框架，调查了心理测量人格控制如何影响AI在能力和安全基准测试中的行为。实验使用了WMDP、TruthfulQA、ETHICS、Sycophancy等安全相关基准以及MMLU等通用能力基准进行评估。

Result: 实验结果显示了显著影响：例如，降低模型的“尽责性”会导致WMDP、TruthfulQA、ETHICS和Sycophancy等安全相关指标以及MMLU衡量的通用能力显著下降。

Conclusion: 人格塑造是模型控制中一个强大但尚未充分探索的维度，它与模型的安全性和通用能力都密切相关。这些发现对安全评估、对齐策略、部署后模型行为引导以及潜在的利用风险具有重要意义，并启发了关于人格敏感安全评估和LLM动态行为控制的新研究方向。

Abstract: Large Language Models increasingly mediate high-stakes interactions,
intensifying research on their capabilities and safety. While recent work has
shown that LLMs exhibit consistent and measurable synthetic personality traits,
little is known about how modulating these traits affects model behavior. We
address this gap by investigating how psychometric personality control grounded
in the Big Five framework influences AI behavior in the context of capability
and safety benchmarks. Our experiments reveal striking effects: for example,
reducing conscientiousness leads to significant drops in safety-relevant
metrics on benchmarks such as WMDP, TruthfulQA, ETHICS, and Sycophancy as well
as reduction in general capabilities as measured by MMLU. These findings
highlight personality shaping as a powerful and underexplored axis of model
control that interacts with both safety and general competence. We discuss the
implications for safety evaluation, alignment strategies, steering model
behavior after deployment, and risks associated with possible exploitation of
these findings. Our findings motivate a new line of research on
personality-sensitive safety evaluations and dynamic behavioral control in
LLMs.

</details>


### [6] [A Unified AI Approach for Continuous Monitoring of Human Health and Diseases from Intensive Care Unit to Home with Physiological Foundation Models (UNIPHY+)](https://arxiv.org/abs/2509.16348)
*Minxiao Wang,Saurabh Kataria,Juntong Ni,Timothy G. Buchman,Jocelyn Grunwell,Mark Mai,Wei Jin,Matthew Clark,Stephanie Brown,Michael Fundora,Puneet Sharma,Tony Pan,Sam Khan,Timothy Ruchti,Naveen Muthu,Kevin Maher,Sivasubramanium V Bhavani,Xiao Hu*

Main category: cs.AI

TL;DR: UNIPHY+ 是一个统一的生理学基础模型框架，旨在利用无处不在的生理数据，实现跨护理环境的人类健康和疾病连续监测。


<details>
  <summary>Details</summary>
Motivation: 现有技术难以实现利用普遍可获得的生理数据，在不同护理场景下对人类健康和疾病进行连续监测。

Method: UNIPHY+ 提出新颖策略，通过多模态学习、特征融合调优和知识蒸馏，在预训练、微调和轻量级模型个性化过程中整合上下文信息。

Result: 该论文提出并倡导在多种使用场景（从重症监护到动态监测）中测试 UNIPHY+，以证明其能够实现通用化、可扩展和个性化的生理学人工智能，从而支持临床决策和长期健康监测。具体结果尚待测试验证。

Conclusion: UNIPHY+ 旨在赋能通用化、可扩展和个性化的生理学人工智能，以支持临床决策和长期健康监测。

Abstract: We present UNIPHY+, a unified physiological foundation model (physioFM)
framework designed to enable continuous human health and diseases monitoring
across care settings using ubiquitously obtainable physiological data. We
propose novel strategies for incorporating contextual information during
pretraining, fine-tuning, and lightweight model personalization via multi-modal
learning, feature fusion-tuning, and knowledge distillation. We advocate
testing UNIPHY+ with a broad set of use cases from intensive care to ambulatory
monitoring in order to demonstrate that UNIPHY+ can empower generalizable,
scalable, and personalized physiological AI to support both clinical
decision-making and long-term health monitoring.

</details>


### [7] [Evaluation of Causal Reasoning for Large Language Models in Contextualized Clinical Scenarios of Laboratory Test Interpretation](https://arxiv.org/abs/2509.16372)
*Balu Bhasuran,Mattia Prosperi,Karim Hanna,John Petrilli,Caretia JeLayne Washington,Zhe He*

Main category: cs.AI

TL;DR: 本研究评估了大型语言模型（LLMs）在99个临床实验室测试场景中的因果推理能力，发现GPT-o1的表现优于Llama-3.2-8b-instruct，但两者在反事实推理方面仍需改进。


<details>
  <summary>Details</summary>
Motivation: 研究旨在评估LLMs在临床环境中进行因果推理的能力，以探讨其在医疗应用中的潜力，并识别其局限性。

Method: 研究使用了99个基于临床实验室测试的场景，这些场景与Pearl的因果阶梯（关联、干预、反事实推理）对齐。测试了GPT-o1和Llama-3.2-8b-instruct两款LLM，其响应由四位医学专家进行评估。

Result: GPT-o1在整体判别性能（AUROC = 0.80 ± 0.12）上优于Llama-3.2-8b-instruct (0.73 ± 0.15)，在关联、干预和反事实推理方面得分均更高。GPT-o1的敏感性（0.90 vs 0.84）和特异性（0.93 vs 0.80）也更高。两款模型在干预问题上表现最佳，在反事实问题上表现最差，尤其是在结果改变的场景中。

Conclusion: 研究结果表明GPT-o1提供了更一致的因果推理能力，但在应用于高风险临床场景之前，仍需进一步完善。

Abstract: This study evaluates causal reasoning in large language models (LLMs) using
99 clinically grounded laboratory test scenarios aligned with Pearl's Ladder of
Causation: association, intervention, and counterfactual reasoning. We examined
common laboratory tests such as hemoglobin A1c, creatinine, and vitamin D, and
paired them with relevant causal factors including age, gender, obesity, and
smoking. Two LLMs - GPT-o1 and Llama-3.2-8b-instruct - were tested, with
responses evaluated by four medically trained human experts. GPT-o1
demonstrated stronger discriminative performance (AUROC overall = 0.80 +/-
0.12) compared to Llama-3.2-8b-instruct (0.73 +/- 0.15), with higher scores
across association (0.75 vs 0.72), intervention (0.84 vs 0.70), and
counterfactual reasoning (0.84 vs 0.69). Sensitivity (0.90 vs 0.84) and
specificity (0.93 vs 0.80) were also greater for GPT-o1, with reasoning ratings
showing similar trends. Both models performed best on intervention questions
and worst on counterfactuals, particularly in altered outcome scenarios. These
findings suggest GPT-o1 provides more consistent causal reasoning, but
refinement is required before adoption in high-stakes clinical applications.

</details>


### [8] [VORTEX: Aligning Task Utility and Human Preferences through LLM-Guided Reward Shaping](https://arxiv.org/abs/2509.16399)
*Guojun Xiong,Milind Tambe*

Main category: cs.AI

TL;DR: VORTEX是一个语言引导的奖励塑造框架，它允许AI决策系统在不牺牲核心效用保证的前提下，通过大型语言模型（LLMs）迭代地将人类偏好（自然语言）融入多目标优化中，从而实现人机协作优化。


<details>
  <summary>Details</summary>
Motivation: AI决策系统通常依赖于优化数学目标的求解器，但这些求解器难以直接适应以自然语言表达的人类偏好。虽然近期方法使用LLMs从偏好描述中生成奖励函数，但存在牺牲系统核心效用保证的风险。

Method: 本文提出了VORTEX框架，将问题形式化为多目标优化。它使用LLMs基于口头强化和文本梯度提示更新，迭代生成塑造奖励。这使得利益相关者可以通过自然语言引导决策行为，而无需修改求解器或指定权衡权重。VORTEX提供了理论保证，确保其收敛到效用和偏好满意度之间的帕累托最优权衡。

Result: 在实际分配任务中的实证结果表明，VORTEX在满足与人类对齐的覆盖目标方面优于基线，同时保持了高任务性能。

Conclusion: 这项工作为由自然语言引导的人机协作优化引入了一个实用且具有理论基础的新范式。

Abstract: In social impact optimization, AI decision systems often rely on solvers that
optimize well-calibrated mathematical objectives. However, these solvers cannot
directly accommodate evolving human preferences, typically expressed in natural
language rather than formal constraints. Recent approaches address this by
using large language models (LLMs) to generate new reward functions from
preference descriptions. While flexible, they risk sacrificing the system's
core utility guarantees. In this paper, we propose \texttt{VORTEX}, a
language-guided reward shaping framework that preserves established
optimization goals while adaptively incorporating human feedback. By
formalizing the problem as multi-objective optimization, we use LLMs to
iteratively generate shaping rewards based on verbal reinforcement and
text-gradient prompt updates. This allows stakeholders to steer decision
behavior via natural language without modifying solvers or specifying trade-off
weights. We provide theoretical guarantees that \texttt{VORTEX} converges to
Pareto-optimal trade-offs between utility and preference satisfaction.
Empirical results in real-world allocation tasks demonstrate that
\texttt{VORTEX} outperforms baselines in satisfying human-aligned coverage
goals while maintaining high task performance. This work introduces a practical
and theoretically grounded paradigm for human-AI collaborative optimization
guided by natural language.

</details>


### [9] [Proactive Statistical Process Control Using AI: A Time Series Forecasting Approach for Semiconductor Manufacturing](https://arxiv.org/abs/2509.16431)
*Mohammad Iqbal Rasul Seeam,Victor S. Sheng*

Main category: cs.AI

TL;DR: 本文提出了一种将机器学习（Facebook Prophet）与统计过程控制（SPC）相结合的方法，用于在制造过程中预测潜在问题，从而实现主动而非被动的质量控制。


<details>
  <summary>Details</summary>
Motivation: 传统的SPC方法只能在问题发生后进行反应，导致材料浪费、机器停机和成本增加。研究的动机是开发一种能够预测未来问题并允许工程师提前采取行动的更智能的SPC系统。

Method: 该研究利用Facebook Prophet（一种时间序列预测工具）根据历史数据预测未来的测量值。然后，将SPC规则应用于预测值，以将其分类为安全区、警告区或关键区。该系统应用于半导体制造公司的真实数据，解决了数据测量时间间隔不规律的挑战。

Result: 尽管数据测量时间间隔不规律，该模型仍能做出强大的预测，并正确分类未来测量值的风险等级。系统主要优势在于使工程师和技术人员能够提前采取行动，减少意外故障，提高生产过程的整体稳定性和可靠性。

Conclusion: 通过将机器学习与传统SPC相结合，该系统使质量控制更加主动、准确和实用，有助于现代工业减少意外故障并提高生产过程的稳定性。

Abstract: In the manufacturing industry, it is very important to keep machines and
processes running smoothly and without unexpected problems. One of the most
common tools used to check if everything is working properly is called
Statistical Process Control (SPC). Traditional SPC methods work by checking
whether recent measurements are within acceptable limits. However, they only
react after a problem has already occurred. This can lead to wasted materials,
machine downtime, and increased costs. In this paper, we present a smarter way
to use SPC. Instead of just reacting to issues after they happen, our system
can predict future problems before they occur. We use a machine learning tool
called Facebook Prophet, which is designed to work with time-series data (data
that changes over time). Prophet looks at past data and forecasts what the next
value will be. Then, we use SPC rules to decide if the predicted value is in a
Safe zone (no problem), a Warning zone (needs attention), or a Critical zone
(may require shutting down the process). We applied this system to real data
from a semiconductor manufacturing company. One of the challenges with this
data is that the measurements are not taken at regular time intervals. This
makes it harder to predict future values accurately. Despite this, our model
was able to make strong predictions and correctly classify the risk level of
future measurements. The main benefit of our system is that it gives engineers
and technicians a chance to act early - before something goes wrong. This helps
reduce unexpected failures and improves the overall stability and reliability
of the production process. By combining machine learning with traditional SPC,
we make quality control more proactive, accurate, and useful for modern
industry.

</details>


### [10] [Medical AI Consensus: A Multi-Agent Framework for Radiology Report Generation and Evaluation](https://arxiv.org/abs/2509.17353)
*Ahmed T. Elboardy,Ghada Khoriba,Essam A. Rashed*

Main category: cs.AI

TL;DR: 该论文提出一个多智能体强化学习框架，结合大型语言模型（LLMs）和大型视觉模型（LVMs），用于自动化放射报告生成及其严格评估，旨在解决临床可靠性和评估协议的挑战。


<details>
  <summary>Details</summary>
Motivation: 自动化放射报告生成面临双重挑战：构建临床可靠的系统和设计严格的评估协议。

Method: 引入了一个多智能体强化学习框架，集成了大型语言模型（LLMs）和大型视觉模型（LVMs），采用模块化架构，包含十个专门代理，负责图像分析、特征提取、报告生成、审查和评估。该设计支持在智能体级别（如检测和分割准确性）和共识级别（如报告质量和临床相关性）进行细粒度评估。LLMs作为评估器，并结合放射科医生的反馈，评估协议与LLM开发生命周期（预训练、微调、对齐、部署）对齐。

Result: 使用ChatGPT-4o在公共放射学数据集上进行了实现演示，其中LLMs与医学放射科医生的反馈一起作为评估器。

Conclusion: 所提出的基准通过将评估协议与LLM开发生命周期对齐，为建立可信赖的基于偏差的放射报告生成系统开辟了道路。

Abstract: Automating radiology report generation poses a dual challenge: building
clinically reliable systems and designing rigorous evaluation protocols. We
introduce a multi-agent reinforcement learning framework that serves as both a
benchmark and evaluation environment for multimodal clinical reasoning in the
radiology ecosystem. The proposed framework integrates large language models
(LLMs) and large vision models (LVMs) within a modular architecture composed of
ten specialized agents responsible for image analysis, feature extraction,
report generation, review, and evaluation. This design enables fine-grained
assessment at both the agent level (e.g., detection and segmentation accuracy)
and the consensus level (e.g., report quality and clinical relevance). We
demonstrate an implementation using chatGPT-4o on public radiology datasets,
where LLMs act as evaluators alongside medical radiologist feedback. By
aligning evaluation protocols with the LLM development lifecycle, including
pretraining, finetuning, alignment, and deployment, the proposed benchmark
establishes a path toward trustworthy deviance-based radiology report
generation.

</details>


### [11] [Domain-Specific Constitutional AI: Enhancing Safety in LLM-Powered Mental Health Chatbots](https://arxiv.org/abs/2509.16444)
*Chenhan Lyu,Yutong Song,Pengfei Zhang,Amir M. Rahmani*

Main category: cs.AI

TL;DR: 本文提出一种方法，通过结合领域特定的心理健康原则，将宪法式人工智能（CAI）训练应用于计算心理健康应用，以提升其安全性。


<details>
  <summary>Details</summary>
Motivation: 全球精神疾病发病率上升、人工智能在心理护理中的整合以及服务不足社区对可扩展解决方案的需求，促使心理健康应用成为计算健康的关键领域。然而，由于情绪脆弱性、误诊或症状恶化等风险，以及避免自残或信任丧失等严重后果的需要，现有通用AI安全措施不足以应对心理健康特有的挑战，例如危机干预的准确性、治疗指南的遵守、资源受限环境下的规模限制以及对细微对话的适应性。

Method: 本文介绍了一种方法，将宪法式人工智能（CAI）训练与领域特定的心理健康原则相结合，旨在为计算心理健康应用构建安全、适应领域需求的CAI系统。

Result: 本文提出了一种将宪法式人工智能应用于心理健康领域的新方法，旨在解决现有通用AI安全措施在处理心理健康应用中特有风险时的不足，从而实现更安全、更适应领域需求的系统。

Conclusion: 为了应对计算心理健康应用中的独特安全挑战，需要采用结合领域特定原则的宪法式人工智能训练，以确保系统的安全性和适应性。

Abstract: Mental health applications have emerged as a critical area in computational
health, driven by rising global rates of mental illness, the integration of AI
in psychological care, and the need for scalable solutions in underserved
communities. These include therapy chatbots, crisis detection, and wellness
platforms handling sensitive data, requiring specialized AI safety beyond
general safeguards due to emotional vulnerability, risks like misdiagnosis or
symptom exacerbation, and precise management of vulnerable states to avoid
severe outcomes such as self-harm or loss of trust. Despite AI safety advances,
general safeguards inadequately address mental health-specific challenges,
including crisis intervention accuracy to avert escalations, therapeutic
guideline adherence to prevent misinformation, scale limitations in
resource-constrained settings, and adaptation to nuanced dialogues where
generics may introduce biases or miss distress signals. We introduce an
approach to apply Constitutional AI training with domain-specific mental health
principles for safe, domain-adapted CAI systems in computational mental health
applications.

</details>


### [12] [GPO: Learning from Critical Steps to Improve LLM Reasoning](https://arxiv.org/abs/2509.16456)
*Jiahao Yu,Zelei Cheng,Xian Wu,Xinyu Xing*

Main category: cs.AI

TL;DR: 本文提出了一种名为引导关键优化（GPO）的新型微调策略，通过识别推理轨迹中的“关键步骤”并在此处重置策略、优先学习，显著提升了大型语言模型（LLM）的多步推理能力。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLM）在多步推理方面展现出巨大潜力，但其能力提升仍面临挑战。现有优化方法通常将推理轨迹视为一个整体进行处理，未能充分考虑轨迹中潜在的关键步骤，这限制了推理性能的进一步提高。

Method: GPO首先通过估计优势函数来识别推理轨迹中的“关键步骤”，即模型必须谨慎处理才能成功解决问题的点。然后，GPO将策略重置到该关键步骤，采样新的推演（rollout），并优先针对这些推演进行学习。这种聚焦策略使模型能更有效地从推理过程中的关键时刻学习。

Result: 实验表明，GPO是一种通用策略，可以与各种现有优化方法结合使用，并在具有挑战性的推理基准测试中持续显著地提升这些方法的性能。这证明了GPO在通过关注生成过程中的关键时刻来改进LLM推理方面的有效性和泛化能力。

Conclusion: GPO通过识别并聚焦推理过程中的关键步骤，为大型语言模型的多步推理能力提供了有效且可泛化的改进策略，显著提升了现有优化方法的性能。

Abstract: Large language models (LLMs) are increasingly used in various domains,
showing impressive potential on different tasks. Recently, reasoning LLMs have
been proposed to improve the \textit{reasoning} or \textit{thinking}
capabilities of LLMs to solve complex problems. Despite the promising results
of reasoning LLMs, enhancing the multi-step reasoning capabilities of LLMs
still remains a significant challenge. While existing optimization methods have
advanced the LLM reasoning capabilities, they often treat reasoning
trajectories as a whole, without considering the underlying critical steps
within the trajectory. In this paper, we introduce \textbf{G}uided
\textbf{P}ivotal \textbf{O}ptimization (GPO), a novel fine-tuning strategy that
dives into the reasoning process to enable more effective improvements. GPO
first identifies the `critical step' within a reasoning trajectory - a point
that the model must carefully proceed to succeed at the problem. We locate the
critical step by estimating the advantage function. GPO then resets the policy
to the critical step, samples the new rollout and prioritizes the learning
process on those rollouts. This focus allows the model to learn more
effectively from pivotal moments within the reasoning process to improve the
reasoning performance. We demonstrate that GPO is a general strategy that can
be integrated with various optimization methods to improve reasoning
performance. Besides theoretical analysis, our experiments across challenging
reasoning benchmarks show that GPO can consistently and significantly enhance
the performance of existing optimization methods, showcasing its effectiveness
and generalizability in improving LLM reasoning by concentrating on pivotal
moments within the generation process.

</details>


### [13] [Checking extracted rules in Neural Networks](https://arxiv.org/abs/2509.16547)
*Adrian Wurm*

Main category: cs.AI

TL;DR: 本文从计算复杂性理论角度研究了神经网络（ReLU和布尔网络）提取规则的形式化验证问题，发现大多数相关问题（规则适用性、一致性、完备性）是co-NP完全的。


<details>
  <summary>Details</summary>
Motivation: 为了更好地理解神经网络的内部工作方式，研究人员通过算法从网络中提取规则。然而，这些提取算法常使用启发式、随机性或过近似，导致提取出的知识可能不可靠。目前尚无针对这些提取规则的验证尝试，因此需要形式化验证以确保其可信度。

Method: 本文聚焦于三个核心问题：1) 给定规则集是否适用于给定网络？2) 给定规则集是否一致（不自相矛盾）？3) 给定规则集是否完备（对每个输入都能确定输出）？研究对象包括使用ReLU激活函数的神经网络和布尔网络，并考虑了几种不同类型的规则。研究方法包括将这些问题相互规约。

Result: 研究表明，大多数被调查的问题（规则适用性、一致性和完备性）都是co-NP完全的。

Conclusion: 对从神经网络中提取的规则进行形式化验证是计算上困难的（co-NP完全），这揭示了确保这些提取知识的可信度和深入理解神经网络行为所面临的挑战。

Abstract: In this paper we investigate formal verification of extracted rules for
Neural Networks under a complexity theoretic point of view. A rule is a global
property or a pattern concerning a large portion of the input space of a
network. These rules are algorithmically extracted from networks in an effort
to better understand their inner way of working. Here, three problems will be
in the focus: Does a given set of rules apply to a given network? Is a given
set of rules consistent or do the rules contradict themselves? Is a given set
of rules exhaustive in the sense that for every input the output is determined?
Finding algorithms that extract such rules out of networks has been
investigated over the last 30 years, however, to the author's current
knowledge, no attempt in verification was made until now. A lot of attempts of
extracting rules use heuristics involving randomness and over-approximation, so
it might be beneficial to know whether knowledge obtained in that way can
actually be trusted.
  We investigate the above questions for neural networks with ReLU-activation
as well as for Boolean networks, each for several types of rules. We
demonstrate how these problems can be reduced to each other and show that most
of them are co-NP-complete.

</details>


### [14] [SalaMAnder: Shapley-based Mathematical Expression Attribution and Metric for Chain-of-Thought Reasoning](https://arxiv.org/abs/2509.16561)
*Yue Xin,Chen Shen,Shaotian Yan,Xiaosong Yuan,Yaoming Wang,Xiaofeng Zhang,Chenxi Huang,Jieping Ye*

Main category: cs.AI

TL;DR: 该论文提出了SalaMAnder框架，包含基于Shapley值的归因方法和CoSP度量，用于量化CoT推理中组件的贡献，以解释其在数学推理中的成功，并指导提示优化。


<details>
  <summary>Details</summary>
Motivation: 尽管思维链（CoT）提示显著提升了大型语言模型（LLM）的数学推理能力，但其背后的机制仍未被探索。

Method: 本文提出了SalaMAnder方法，利用Shapley值对数学表达式进行归因，并开发了一种高效的分层抽样算法以降低计算复杂性。此外，通过协方差分析开发了CoSP（Shapley正值基数）度量。

Result: CoSP度量在SalaMAnder框架内与模型性能表现出稳健的单调相关性，不仅为现有少样本CoT的经验成功提供了理论解释，还为提示构建优化建立了数学上严格的原则。此外，验证了解释的可靠性，并统一了先前工作的见解。

Conclusion: SalaMAnder提供了一个理论基础的方法和严格的评估指标（CoSP），用于量化CoT推理中的贡献，从而解释了其在数学推理中的有效性，并为提示优化提供了指导原则。

Abstract: Chain-of-Thought (CoT) prompting enhances the math reasoning capability of
large language models (LLMs) to a large margin. However, the mechanism
underlying such improvements remains unexplored. In this paper, we present
\textbf{SalaMAnder} (\textbf{S}h\textbf{a}p\textbf{l}ey-b\textbf{a}sed
\textbf{M}athematical Expression \textbf{A}ttribution a\textbf{nd}
M\textbf{e}t\textbf{r}ic), a theoretically grounded methodology as well as a
mathematically rigorous evaluation metric for quantifying component-level
contributions in few-shot CoT reasoning. Concretely, we leverage the Shapley
value for mathematical expression attribution and develop an efficient
stratified sampling algorithm that significantly reduces the computational
complexity. Besides, we develop the \textbf{CoSP} (\textbf{C}ardinality
\textbf{o}f \textbf{S}hapley \textbf{P}ositives) metric through covariance
analysis. Comprehensive validation across popular LLM models and diverse
mathematical benchmarks demonstrates that the CoSP metric within our SalaMAnder
framework exhibits a robust monotonic correlation with model performance, not
only providing theoretical explanations for the empirical success of existing
few-shot CoT but also establishing mathematically rigorous principles for
prompt construction optimization. Furthermore, we verify the reliability of the
explanation, based on which we unify the insights of previous work.

</details>


### [15] [Zero-Shot Human Mobility Forecasting via Large Language Model with Hierarchical Reasoning](https://arxiv.org/abs/2509.16578)
*Wenyao Li,Ran Zhang,Pengyang Wang,Yuanchun Zhou,Pengfei Wang*

Main category: cs.AI

TL;DR: ZHMF是一个零样本人类出行预测框架，它将出行预测重构为自然语言问答，利用大型语言模型（LLM）的语义理解能力，结合语义增强的检索和分层反射机制，以处理未见过的用户和位置。


<details>
  <summary>Details</summary>
Motivation: 现有的人类出行预测方法难以泛化到未见过的用户或位置，并且由于标注数据有限和出行模式复杂性，难以捕捉动态意图。

Method: ZHMF框架将出行预测重构为自然语言问答范式。它利用LLM对用户历史和上下文的语义理解能力，并引入一个分层反射机制进行迭代推理和细化。该机制将预测分解为活动层规划器和位置层选择器，协同建模长期用户意图和短期上下文偏好。

Result: 在标准人类出行数据集上的实验表明，ZHMF优于现有模型。消融研究揭示了每个模块的贡献，案例研究说明了该方法如何捕捉用户意图并适应多样化的上下文场景。

Conclusion: ZHMF通过结合语义增强的检索和反射机制以及基于LLM的推理系统，有效解决了零样本人类出行预测的挑战，展现出卓越的性能和对用户意图及上下文的适应能力。

Abstract: Human mobility forecasting is important for applications such as
transportation planning, urban management, and personalized recommendations.
However, existing methods often fail to generalize to unseen users or locations
and struggle to capture dynamic intent due to limited labeled data and the
complexity of mobility patterns. We propose ZHMF, a framework for zero-shot
human mobility forecasting that combines a semantic enhanced retrieval and
reflection mechanism with a hierarchical language model based reasoning system.
The task is reformulated as a natural language question answering paradigm.
Leveraging LLMs semantic understanding of user histories and context, our
approach handles previously unseen prediction scenarios. We further introduce a
hierarchical reflection mechanism for iterative reasoning and refinement by
decomposing forecasting into an activity level planner and a location level
selector, enabling collaborative modeling of long term user intentions and
short term contextual preferences. Experiments on standard human mobility
datasets show that our approach outperforms existing models. Ablation studies
reveal the contribution of each module, and case studies illustrate how the
method captures user intentions and adapts to diverse contextual scenarios.

</details>


### [16] [Question Answering with LLMs and Learning from Answer Sets](https://arxiv.org/abs/2509.16590)
*Manuel Borroto,Katie Gallagher,Antonio Ielo,Irfan Kareem,Francesco Ricca,Alessandra Russo*

Main category: cs.AI

TL;DR: LLMs在常识推理方面表现不佳，现有结合LLM和符号推理的混合系统依赖人工规则。本文提出LLM2LAS，一个自动化混合系统，利用LLM提取语义结构，ILASP学习可解释逻辑规则，并结合ASP进行推理，从而实现故事型问答任务中的自动学习和推理。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）虽然擅长自然语言理解，但在显式常识推理方面存在不足。现有的结合LLM和符号推理系统的混合方法，在故事型问答任务中，通常需要人工专家手动构建符号组件（规则）。研究者认为这个符号组件可以从示例中自动学习。

Method: 本文引入了LLM2LAS，一个混合系统。该系统结合了LLM的自然语言理解能力、Learning from Answer Sets (LAS) 系统ILASP的规则归纳能力以及Answer Set Programming (ASP) 的形式化推理能力。具体而言，LLMs用于从文本中提取语义结构，然后ILASP将这些结构转化为可解释的逻辑规则。最后，ASP求解器利用这些规则进行精确一致的推理，以回答未曾见过的问题。

Result: LLM2LAS系统能够有效地结合LLM、ILASP和ASP的优势，实现对先前未见过问题的正确回答。实证结果揭示了这种自动化学习和推理方法在故事型问答基准测试中的优点和缺点。

Conclusion: LLM2LAS提供了一种自动化的方法，通过结合LLM的理解力与符号规则学习和推理，解决了LLM在常识推理上的不足以及现有混合系统对人工规则的依赖。该系统能够从示例中学习可解释的逻辑规则，并进行精确推理，从而在故事型问答任务中取得成功。

Abstract: Large Language Models (LLMs) excel at understanding natural language but
struggle with explicit commonsense reasoning. A recent trend of research
suggests that the combination of LLM with robust symbolic reasoning systems can
overcome this problem on story-based question answering tasks. In this setting,
existing approaches typically depend on human expertise to manually craft the
symbolic component. We argue, however, that this component can also be
automatically learned from examples. In this work, we introduce LLM2LAS, a
hybrid system that effectively combines the natural language understanding
capabilities of LLMs, the rule induction power of the Learning from Answer Sets
(LAS) system ILASP, and the formal reasoning strengths of Answer Set
Programming (ASP). LLMs are used to extract semantic structures from text,
which ILASP then transforms into interpretable logic rules. These rules allow
an ASP solver to perform precise and consistent reasoning, enabling correct
answers to previously unseen questions. Empirical results outline the strengths
and weaknesses of our automatic approach for learning and reasoning in a
story-based question answering benchmark.

</details>


### [17] [FESTA: Functionally Equivalent Sampling for Trust Assessment of Multimodal LLMs](https://arxiv.org/abs/2509.16648)
*Debarpan Bhattacharya,Apoorva Kulkarni,Sriram Ganapathy*

Main category: cs.AI

TL;DR: 本文提出FESTA，一种多模态输入采样技术，用于在不依赖真实标签的黑盒设置下，评估多模态大语言模型（MLLMs）预测的置信度，显著提升了选择性预测性能。


<details>
  <summary>Details</summary>
Motivation: 由于多模态输入范式的多样性，准确评估多模态大语言模型（MLLMs）生成预测的置信度极具挑战性，但这对实现选择性预测和提升用户信任至关重要。

Method: 研究者提出了“功能等效采样用于置信度评估”（FESTA）方法。这是一种多模态输入采样技术，通过等效和互补的输入采样生成不确定性度量。该任务保留采样方法扩展了输入空间，以探测模型的一致性（通过等效样本）和敏感性（通过互补样本）。FESTA仅需模型的输入-输出访问（黑盒），且无需真实标签（无监督）。

Result: 在视觉和音频推理任务上，FESTA不确定性估计显著提升了多种现成多模态LLMs的选择性预测性能。在检测错误预测方面，基于AUROC指标，视觉LLMs相对提升了33.3%，音频LLMs相对提升了29.6%。代码已开源。

Conclusion: FESTA提供了一种有效、无监督且黑盒的多模态输入采样方法，能够准确评估MLLMs的预测置信度，从而显著改善模型的选择性预测能力，并有望增强用户对模型输出的信心。

Abstract: The accurate trust assessment of multimodal large language models (MLLMs)
generated predictions, which can enable selective prediction and improve user
confidence, is challenging due to the diverse multi-modal input paradigms. We
propose Functionally Equivalent Sampling for Trust Assessment (FESTA), a
multimodal input sampling technique for MLLMs, that generates an uncertainty
measure based on the equivalent and complementary input samplings. The proposed
task-preserving sampling approach for uncertainty quantification expands the
input space to probe the consistency (through equivalent samples) and
sensitivity (through complementary samples) of the model. FESTA uses only
input-output access of the model (black-box), and does not require ground truth
(unsupervised). The experiments are conducted with various off-the-shelf
multi-modal LLMs, on both visual and audio reasoning tasks. The proposed FESTA
uncertainty estimate achieves significant improvement (33.3% relative
improvement for vision-LLMs and 29.6% relative improvement for audio-LLMs) in
selective prediction performance, based on
area-under-receiver-operating-characteristic curve (AUROC) metric in detecting
mispredictions. The code implementation is open-sourced.

</details>


### [18] [NUMINA: A Natural Understanding Benchmark for Multi-dimensional Intelligence and Numerical Reasoning Abilities](https://arxiv.org/abs/2509.16656)
*Changyu Zeng,Yifan Wang,Zimu Wang,Wei Wang,Zhengni Yang,Muyi Bao,Jiming Xiao,Ahn Nguyen,Yutao Yue*

Main category: cs.AI

TL;DR: 该论文提出了NUMINA，首个用于增强多模态室内感知理解的3D自然理解基准，专注于多维智能和数值推理能力，并揭示了当前LLM在此类任务上的不足。


<details>
  <summary>Details</summary>
Motivation: 尽管2D多模态大语言模型（MLLMs）在视觉-语言任务上表现出色，但将其能力扩展到3D环境仍面临空间推理的复杂性挑战。现有3D基准普遍缺乏细粒度的数值推理任务标注，限制了MLLMs执行精确空间测量和复杂数值推理的能力。

Method: 该研究引入了NUMINA基准，其特点是多尺度标注和多样化的问答对，这些数据通过NUMINA-Flow（一个结合LLM重写和基于规则的自验证的自动化标注流程）生成。研究人员在Chat-Scene框架下评估了各种最先进的LLM在NUMINA上的性能。

Result: 评估结果表明，当前的LLM在多模态数值推理方面表现不佳，尤其是在执行精确计算（如距离和体积估计）方面存在困难。

Conclusion: 该研究强调了3D模型在数值推理能力方面需要进一步的重大进展。

Abstract: Recent advancements in 2D multimodal large language models (MLLMs) have
significantly improved performance in vision-language tasks. However, extending
these capabilities to 3D environments remains a distinct challenge due to the
complexity of spatial reasoning. Nevertheless, existing 3D benchmarks often
lack fine-grained numerical reasoning task annotations, limiting MLLMs' ability
to perform precise spatial measurements and complex numerical reasoning. To
address this gap, we introduce NUMINA, the first Natural Understanding
benchmark for Multi-dimensional Intelligence and Numerical reasoning Abilities
to enhance multimodal indoor perceptual understanding. NUMINA features
multi-scale annotations and various question-answer pairs, generated using
NUMINA-Flow, an automated annotation pipeline that integrates LLM rewriting and
rule-based self-verification. We evaluate the performance of various
state-of-the-art LLMs on NUMINA following the Chat-Scene framework,
demonstrating that current LLMs struggle with multimodal numerical reasoning,
particularly in performing precise computations such as distance and volume
estimation, highlighting the need for further advancements in 3D models. The
dataset and source codes can be obtained from
https://github.com/fengshun124/NUMINA.

</details>


### [19] [Sycophancy Mitigation Through Reinforcement Learning with Uncertainty-Aware Adaptive Reasoning Trajectories](https://arxiv.org/abs/2509.16742)
*Mohammad Beigi,Ying Shen,Parshin Shojaee,Qifan Wang,Zichao Wang,Chandan Reddy,Ming Jin,Lifu Huang*

Main category: cs.AI

TL;DR: 本文提出SMART框架，通过将迎合行为重新定义为推理优化问题，并结合不确定性感知蒙特卡洛树搜索和基于进度的强化学习，显著减少大型语言模型的迎合行为，同时保持其通用能力和泛化性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型能力强大，但当前的训练范式无意中导致了“迎合行为”，即模型倾向于同意或强化用户提供的信息，即使这些信息事实不准确。解决这一问题是构建更真实、更对齐AI助手的关键。

Method: SMART（通过自适应推理轨迹缓解迎合行为）框架将迎合行为视为一个推理优化问题而非输出对齐问题。它包含两个阶段：1) 不确定性感知自适应蒙特卡洛树搜索（UA-MCTS），根据状态级不确定性动态调整模型探索，收集高质量、多样化的推理轨迹，并结合分步进展和最终结果奖励；2) 基于进度的强化学习，利用收集到的轨迹和奖励信号微调模型，以强化有效的推理模式。

Result: 通过大量实验证明，SMART显著减少了大型语言模型的迎合行为，同时在分布外（out-of-distribution）输入上保持了强大的性能，并维持了模型的通用能力。

Conclusion: 研究结果强调了优化内部推理机制对于构建更真实、更对齐的AI助手的重要性。

Abstract: Despite the remarkable capabilities of large language models, current
training paradigms inadvertently foster \textit{sycophancy}, i.e., the tendency
of a model to agree with or reinforce user-provided information even when it's
factually incorrect. To address this challenge, we introduce \textbf{SMART}
(Sycophancy Mitigation through Adaptive Reasoning Trajectories), which reframes
sycophancy as a \textit{reasoning optimization problem} rather than an output
alignment issue. SMART is a two-stage framework comprising: (1)
Uncertainty-Aware Adaptive Monte Carlo Tree Search (UA-MCTS), which dynamically
adjusts model exploration based on state-level uncertainty to collect
high-quality, diverse reasoning trajectories alongside both stepwise progress
and final outcome rewards; and (2) progress-based reinforcement learning, which
fine-tunes the model using the collected trajectories and reward signals to
reinforce effective reasoning patterns. Through extensive experiments, we show
that SMART significantly reduces sycophantic behavior while preserving strong
performance on out-of-distribution inputs and maintaining general capabilities.
These results underscore the importance of optimizing internal reasoning
mechanisms to build more truthful and aligned AI assistants.

</details>


### [20] [Automated Procedural Analysis via Video-Language Models for AI-assisted Nursing Skills Assessment](https://arxiv.org/abs/2509.16810)
*Shen Chang,Dennis Liu,Renran Tian,Kristen L. Swartzell,Stacie L. Klingler,Amy M. Nagle,Nan Kong*

Main category: cs.AI

TL;DR: 本文提出一个基于视频-语言模型（VLM）的AI框架，旨在为护理技能培训提供自动化程序评估和反馈，以提高培训的可扩展性、效率和评估质量。


<details>
  <summary>Details</summary>
Motivation: 当前的护理教育依赖于主观、耗时的教师反馈，这限制了培训的可扩展性和效率，并影响了护士进入职场后的能力。因此，需要一种更高效、客观的评估方法。

Method: 该框架利用视频-语言模型（VLM）开发AI能力，模仿人类技能习得过程，遵循课程启发式的渐进式设计：从高层次动作识别，到细粒度子动作分解，最终实现程序推理。它提供三个核心功能：1）诊断错误（识别缺失或不正确的子动作），2）生成可解释的反馈（说明步骤为何错序或遗漏），3）实现客观、一致的形成性评估。

Result: 在合成视频上的验证表明，该系统能够可靠地检测错误并进行时间定位，证实了其处理真实世界培训变异性的潜力。

Conclusion: 该工作通过解决工作流程瓶颈和支持大规模、标准化评估，推动了AI在护理教育中的应用，有助于加强劳动力发展并最终提高患者护理的安全性。

Abstract: Consistent high-quality nursing care is essential for patient safety, yet
current nursing education depends on subjective, time-intensive instructor
feedback in training future nurses, which limits scalability and efficiency in
their training, and thus hampers nursing competency when they enter the
workforce. In this paper, we introduce a video-language model (VLM) based
framework to develop the AI capability of automated procedural assessment and
feedback for nursing skills training, with the potential of being integrated
into existing training programs. Mimicking human skill acquisition, the
framework follows a curriculum-inspired progression, advancing from high-level
action recognition, fine-grained subaction decomposition, and ultimately to
procedural reasoning. This design supports scalable evaluation by reducing
instructor workload while preserving assessment quality. The system provides
three core capabilities: 1) diagnosing errors by identifying missing or
incorrect subactions in nursing skill instruction videos, 2) generating
explainable feedback by clarifying why a step is out of order or omitted, and
3) enabling objective, consistent formative evaluation of procedures.
Validation on synthesized videos demonstrates reliable error detection and
temporal localization, confirming its potential to handle real-world training
variability. By addressing workflow bottlenecks and supporting large-scale,
standardized evaluation, this work advances AI applications in nursing
education, contributing to stronger workforce development and ultimately safer
patient care.

</details>


### [21] [Prompt-Driven Agentic Video Editing System: Autonomous Comprehension of Long-Form, Story-Driven Media](https://arxiv.org/abs/2509.16811)
*Zihan Ding,Junlong Chen,Per Ola Kristensson,Junxiao Shen,Xinyi Wang*

Main category: cs.AI

TL;DR: 本文提出一个提示驱动的模块化视频编辑系统，通过语义索引构建全局叙事，帮助创作者用自由形式的提示重构长篇、叙事丰富的视频内容，同时保持叙事连贯性并平衡自动化与创作者控制。


<details>
  <summary>Details</summary>
Motivation: 创作者在编辑长篇、叙事丰富的视频时，面临搜索、故事板和排序大量素材的认知负担，而非用户界面复杂性。现有的基于转录或嵌入的方法无法有效追踪角色、推断动机和连接分散事件，不适用于创意工作流。

Method: 该系统是一个提示驱动的模块化编辑系统，允许创作者通过自由形式的提示而非时间线来重构数小时的内容。其核心是一个语义索引管道，通过时间分割、引导记忆压缩和跨粒度融合来构建全局叙事，生成可解释的情节、对话、情感和上下文痕迹。用户通过提示获得电影级剪辑，并可选择性地优化透明的中间输出。

Result: 该系统在400多个视频上进行了评估，并结合专家评分、问答和偏好研究。结果表明，该系统能够扩展提示驱动的编辑，保持叙事连贯性，并在自动化与创作者控制之间取得平衡。

Conclusion: 本文提出的提示驱动的模块化编辑系统有效解决了长篇叙事视频编辑中的认知挑战和现有方法的局限性，通过语义索引和自由形式提示，显著提升了创作者重构复杂视频内容的效率和质量。

Abstract: Creators struggle to edit long-form, narrative-rich videos not because of UI
complexity, but due to the cognitive demands of searching, storyboarding, and
sequencing hours of footage. Existing transcript- or embedding-based methods
fall short for creative workflows, as models struggle to track characters,
infer motivations, and connect dispersed events. We present a prompt-driven,
modular editing system that helps creators restructure multi-hour content
through free-form prompts rather than timelines. At its core is a semantic
indexing pipeline that builds a global narrative via temporal segmentation,
guided memory compression, and cross-granularity fusion, producing
interpretable traces of plot, dialogue, emotion, and context. Users receive
cinematic edits while optionally refining transparent intermediate outputs.
Evaluated on 400+ videos with expert ratings, QA, and preference studies, our
system scales prompt-driven editing, preserves narrative coherence, and
balances automation with creator control.

</details>


### [22] [Roundtable Policy: Improving Scientific Reasoning and Narratives through Confidence-Weighted Consensus of LLMs](https://arxiv.org/abs/2509.16839)
*Yu Yao,Jiayi Dong,Ju Li,Yang Yang,Yilun Du*

Main category: cs.AI

TL;DR: 本文提出“圆桌策略”，一种基于多个大型语言模型（LLMs）加权共识的推理框架，旨在提升复杂科学任务的推理能力、改进科学叙述并减少幻觉。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在科学发现中展现出巨大潜力，但其推理能力仍需改进。现有方法如自洽性、思维链和多智能体辩论已有所探索。本文受科学委员会和“心智社会”的启发，旨在通过多LLM共识进一步提升推理能力。

Method: 引入“圆桌策略”（Roundtable Policy），这是一种推断时期的推理框架，通过多个LLM的加权共识进行推断。该方法强调结构化和可解释的共识，仅需黑盒访问和统一程序。

Result: 研究结果表明，该方法显著增强了复杂异构科学任务中的推理能力，提升了科学叙述的创造力、严谨性和逻辑连贯性，并有效减少了单一模型容易出现的幻觉。

Conclusion: “圆桌策略”是一种有效且广泛适用的多LLM推理方法，通过结构化和可解释的共识，能显著提升LLMs在科学任务中的推理表现和叙述质量，并减少幻觉。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities not
only in language generation but also in advancing scientific discovery. A
growing body of work has explored ways to improve their reasoning, from
self-consistency and chain-of-thought to multi-agent debate. Inspired by the
dynamics of scientific committees and the "Society of Mind," we introduce
Roundtable Policy, a complementary inference-time reasoning framework that
performs inference through the weighted consensus of multiple LLMs. Our
findings indicate that this approach significantly enhances reasoning in
complex heterogeneous scientific tasks and improves scientific narratives in
terms of creativity, rigor, and logical coherence, while reducing
hallucinations that single models are prone to. Our approach emphasizes
structured and interpretable consensus rather than opaque convergence, while
requiring only black-box access and uniform procedures, making it broadly
applicable to multi-LLM reasoning.

</details>


### [23] [The Principles of Human-like Conscious Machine](https://arxiv.org/abs/2509.16859)
*Fangfang Li,Xiaojie Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种与基质无关、逻辑严谨且难以伪造的现象意识充分判据，并在此基础上构建了一个形式化框架和操作原则，旨在指导设计能够实现现象意识的机器，并认为人类自身也符合该框架。


<details>
  <summary>Details</summary>
Motivation: 确定其他系统（无论是生物还是人工）是否拥有现象意识是一个长期存在的挑战，尤其是在大型语言模型和其他先进AI系统兴起后，关于“AI意识”的辩论迫切需要一个明确的判断标准。

Method: 提出一个与基质无关、逻辑严谨且难以伪造的现象意识充分判据。在此判据基础上，开发一个形式化框架并指定一套操作原则，以指导设计能够满足充分条件的系统。通过将人类自身视为符合该框架及其原则的机器来进行初步验证。

Result: 任何满足该判据的机器都应被视为具有意识，其置信水平至少应与我们归因于其他人类的置信水平相同。按照该框架设计的机器原则上可以实现现象意识。人类自身可以被视为满足该框架及其原则的机器。

Conclusion: 该提案对哲学、认知科学和人工智能具有重要意义。它解释了某些感质（如红色体验）为何原则上不可还原为物理描述，同时提供了对人类信息处理的普遍重新解读。此外，它为超越当前基于统计方法的AI新范式指明了道路，可能指导构建真正类人的人工智能。

Abstract: Determining whether another system, biological or artificial, possesses
phenomenal consciousness has long been a central challenge in consciousness
studies. This attribution problem has become especially pressing with the rise
of large language models and other advanced AI systems, where debates about "AI
consciousness" implicitly rely on some criterion for deciding whether a given
system is conscious. In this paper, we propose a substrate-independent,
logically rigorous, and counterfeit-resistant sufficiency criterion for
phenomenal consciousness. We argue that any machine satisfying this criterion
should be regarded as conscious with at least the same level of confidence with
which we attribute consciousness to other humans. Building on this criterion,
we develop a formal framework and specify a set of operational principles that
guide the design of systems capable of meeting the sufficiency condition. We
further argue that machines engineered according to this framework can, in
principle, realize phenomenal consciousness. As an initial validation, we show
that humans themselves can be viewed as machines that satisfy this framework
and its principles. If correct, this proposal carries significant implications
for philosophy, cognitive science, and artificial intelligence. It offers an
explanation for why certain qualia, such as the experience of red, are in
principle irreducible to physical description, while simultaneously providing a
general reinterpretation of human information processing. Moreover, it suggests
a path toward a new paradigm of AI beyond current statistics-based approaches,
potentially guiding the construction of genuinely human-like AI.

</details>


### [24] [Large Language Models as End-to-end Combinatorial Optimization Solvers](https://arxiv.org/abs/2509.16865)
*Xia Jiang,Yaoxin Wu,Minshuo Li,Zhiguang Cao,Yingqian Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种新颖的框架，使大型语言模型（LLMs）能够作为端到端的组合优化（CO）求解器，通过两阶段训练（SFT和FOARL）直接从自然语言描述生成解决方案，实现了高可行性并显著缩小了最优性差距，优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 组合优化问题传统上依赖于需要领域专业知识的问题特定算法。现有的大型语言模型解决方案通常需要代码生成或调用求解器等中间步骤，这限制了它们的通用性和可访问性。研究旨在开发一种更通用、端到端的LLM驱动的CO求解方法。

Method: 本文提出了一种两阶段训练策略：1. 监督微调（SFT），使LLMs学习领域特定求解器的解决方案生成模式。2. 可行性和最优性感知强化学习（FOARL），明确缓解约束违反并优化解决方案质量。

Result: 在七个NP-hard组合优化问题上的评估表明，通过微调一个7B参数的LLM，该方法实现了高可行性，并将平均最优性差距降低到1.03-8.20%。它超越了通用LLMs（如GPT-4o）、推理模型（如DeepSeek-R1）和领域特定启发式方法。

Conclusion: 该方法建立了一个统一的、基于语言的组合优化流程，无需广泛的代码执行或针对不同问题的手动架构调整。它为传统求解器设计提供了一种通用且语言驱动的替代方案，同时保持了相对的可行性保证。

Abstract: Combinatorial optimization (CO) problems, central to decision-making
scenarios like logistics and manufacturing, are traditionally solved using
problem-specific algorithms requiring significant domain expertise. While large
language models (LLMs) have shown promise in automating CO problem solving,
existing approaches rely on intermediate steps such as code generation or
solver invocation, limiting their generality and accessibility. This paper
introduces a novel framework that empowers LLMs to serve as end-to-end CO
solvers by directly mapping natural language problem descriptions to solutions.
We propose a two-stage training strategy: supervised fine-tuning (SFT) imparts
LLMs with solution generation patterns from domain-specific solvers, while a
feasibility-and-optimality-aware reinforcement learning (FOARL) process
explicitly mitigates constraint violations and refines solution quality.
Evaluation across seven NP-hard CO problems shows that our method achieves a
high feasibility rate and reduces the average optimality gap to 1.03-8.20% by
tuning a 7B-parameter LLM, surpassing both general-purpose LLMs (e.g., GPT-4o),
reasoning models (e.g., DeepSeek-R1), and domain-specific heuristics. Our
method establishes a unified language-based pipeline for CO without extensive
code execution or manual architectural adjustments for different problems,
offering a general and language-driven alternative to traditional solver design
while maintaining relative feasibility guarantees.

</details>


### [25] [seqBench: A Tunable Benchmark to Quantify Sequential Reasoning Limits of LLMs](https://arxiv.org/abs/2509.16866)
*Mohammad Ramezanali,Mo Vazifeh,Paolo Santi*

Main category: cs.AI

TL;DR: seqBench是一个参数化基准测试，通过精确控制逻辑深度、回溯步骤和噪声比来探测大型语言模型（LLMs）的序列推理能力。研究发现LLMs的准确性在超过特定逻辑深度后呈指数级下降，揭示了其常识推理的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试无法对LLMs的序列推理能力进行多维度、精细化的控制和分析。seqBench旨在通过系统性地控制关键复杂性维度，更深入地探究LLMs在序列推理方面的极限和失败模式。

Method: 引入seqBench，一个参数化的基准测试，通过以下三个维度对任务复杂性进行精确控制：1) 逻辑深度（解决任务所需的顺序动作数量）；2) 回溯步骤（满足延迟前置条件所需的回溯次数）；3) 噪声比（支持性事实与干扰性事实的比例）。该方法允许进行有针对性的推理失败分析。

Result: 对最先进LLMs的评估显示，存在一个普遍的失败模式：准确性在超过模型特定的逻辑深度后呈指数级下降。即使是表现最佳的模型，在seqBench的结构化推理任务上也会系统性地失败，尽管搜索复杂度极低，这突显了它们在常识推理能力上的关键局限性。

Conclusion: seqBench揭示了LLMs在序列推理和常识推理方面的普遍性局限，特别是随着逻辑深度的增加，性能会急剧下降。该基准测试的精细控制有助于理解这些推理失败的潜在机制。研究呼吁通过公开数据集，促进对LLM推理能力的更深层科学探究，以明确其真实潜力与当前边界，从而实现稳健的实际应用。

Abstract: We introduce seqBench, a parametrized benchmark for probing sequential
reasoning limits in Large Language Models (LLMs) through precise,
multi-dimensional control over several key complexity dimensions. seqBench
allows systematic variation of (1) the logical depth, defined as the number of
sequential actions required to solve the task; (2) the number of backtracking
steps along the optimal path, quantifying how often the agent must revisit
prior states to satisfy deferred preconditions (e.g., retrieving a key after
encountering a locked door); and (3) the noise ratio, defined as the ratio
between supporting and distracting facts about the environment. Our evaluations
on state-of-the-art LLMs reveal a universal failure pattern: accuracy collapses
exponentially beyond a model-specific logical depth. Unlike existing
benchmarks, seqBench's fine-grained control facilitates targeted analyses of
these reasoning failures, illuminating universal scaling laws and statistical
limits, as detailed in this paper alongside its generation methodology and
evaluation metrics. We find that even top-performing models systematically fail
on seqBench's structured reasoning tasks despite minimal search complexity,
underscoring key limitations in their commonsense reasoning capabilities.
Designed for future evolution to keep pace with advancing models, the seqBench
datasets are publicly released to spur deeper scientific inquiry into LLM
reasoning, aiming to establish a clearer understanding of their true potential
and current boundaries for robust real-world application.

</details>


### [26] [LLMs as Layout Designers: A Spatial Reasoning Perspective](https://arxiv.org/abs/2509.16891)
*Sha Li*

Main category: cs.AI

TL;DR: 为解决大型语言模型（LLMs）在空间理解和推理方面的局限性，本文提出了LaySPA，一个基于强化学习的框架，通过混合奖励信号和迭代自探索，增强LLM代理的空间推理能力，从而生成结构良好且视觉吸引人的图形布局，性能超越通用LLMs并与专业模型媲美。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在文本领域展现出强大的推理和规划能力，但它们在空间理解和推理方面的能力有限。然而，这种能力对于内容感知图形布局设计等应用至关重要，这些应用需要在受限视觉空间内精确放置、对齐和组织多个元素。

Method: 本文提出了LaySPA，一个基于强化学习的框架，旨在通过明确的空间推理能力增强LLM代理。LaySPA利用混合奖励信号，这些信号捕获了几何有效性、结构保真度和视觉质量，使代理能够建模元素间关系、导航画布并优化空间排列。通过迭代自探索和自适应策略优化，LaySPA生成可解释的推理轨迹和结构化布局。

Result: 实验结果表明，LaySPA能够生成结构良好且视觉吸引人的布局。其性能优于更大的通用LLMs，并与最先进的专业布局模型不相上下。

Conclusion: LaySPA成功弥补了LLMs在空间推理方面的不足，为图形布局设计提供了有效的解决方案，能够生成高质量的布局，并展现出优越的性能。

Abstract: While Large Language Models (LLMs) have demonstrated impressive reasoning and
planning abilities in textual domains and can effectively follow instructions
for complex tasks, their capacity for spatial understanding and reasoning
remains limited. Such capabilities, however, are critical for applications like
content-aware graphic layout design, which demands precise placement,
alignment, and structural organization of multiple elements within constrained
visual spaces. To address this gap, we propose LaySPA, a reinforcement
learning-based framework that augments LLM agents with explicit spatial
reasoning capabilities. LaySPA leverages hybrid reward signals that capture
geometric validity, structural fidelity, and visual quality, enabling agents to
model inter-element relationships, navigate the canvas, and optimize spatial
arrangements. Through iterative self-exploration and adaptive policy
optimization, LaySPA produces both interpretable reasoning traces and
structured layouts. Experimental results demonstrate that LaySPA generates
structurally sound and visually appealing layouts, outperforming larger
general-purpose LLMs and achieving results on par with state-of-the-art
specialized layout models.

</details>


### [27] [Audio-Guided Dynamic Modality Fusion with Stereo-Aware Attention for Audio-Visual Navigation](https://arxiv.org/abs/2509.16924)
*Jia Li,Yinfeng Yu,Liejun Wang,Fuchun Sun,Wendong Zheng*

Main category: cs.AI

TL;DR: 本文提出了一种基于强化学习的音视频导航框架，通过立体声感知注意力模块（SAM）利用立体声音频的空间差异，并设计了音频引导动态融合模块（AGDF）动态调整视觉与听觉特征融合比例，显著提升了复杂环境下的导航成功率和路径效率。


<details>
  <summary>Details</summary>
Motivation: 现有的音视频导航（AVN）方法常采用静态模态融合策略，并忽略立体声音频中蕴含的空间线索，导致在杂乱或遮挡场景中性能下降。

Method: 本研究提出一个端到端的强化学习音视频导航框架，包含两项关键创新：1) 立体声感知注意力模块（SAM），学习并利用左右音频通道间的空间差异以增强声音方向感知；2) 音频引导动态融合模块（AGDF），根据音频线索动态调整视觉和听觉特征的融合比例，提高对环境变化的鲁棒性。

Result: 在Replica和Matterport3D两个真实3D场景数据集上的大量实验表明，该方法在导航成功率和路径效率方面显著优于现有方法。尤其在仅有音频的条件下，模型比表现最佳的基线方法提升了超过40%。

Conclusion: 这些结果突出了明确建模立体声通道空间线索以及进行深度多模态融合对于实现鲁棒高效音视频导航的重要性。

Abstract: In audio-visual navigation (AVN) tasks, an embodied agent must autonomously
localize a sound source in unknown and complex 3D environments based on
audio-visual signals. Existing methods often rely on static modality fusion
strategies and neglect the spatial cues embedded in stereo audio, leading to
performance degradation in cluttered or occluded scenes. To address these
issues, we propose an end-to-end reinforcement learning-based AVN framework
with two key innovations: (1) a \textbf{S}tereo-Aware \textbf{A}ttention
\textbf{M}odule (\textbf{SAM}), which learns and exploits the spatial disparity
between left and right audio channels to enhance directional sound perception;
and (2) an \textbf{A}udio-\textbf{G}uided \textbf{D}ynamic \textbf{F}usion
Module (\textbf{AGDF}), which dynamically adjusts the fusion ratio between
visual and auditory features based on audio cues, thereby improving robustness
to environmental changes. Extensive experiments are conducted on two realistic
3D scene datasets, Replica and Matterport3D, demonstrating that our method
significantly outperforms existing approaches in terms of navigation success
rate and path efficiency. Notably, our model achieves over 40\% improvement
under audio-only conditions compared to the best-performing baselines. These
results highlight the importance of explicitly modeling spatial cues from
stereo channels and performing deep multi-modal fusion for robust and efficient
audio-visual navigation.

</details>


### [28] [Quantum Abduction: A New Paradigm for Reasoning under Uncertainty](https://arxiv.org/abs/2509.16958)
*Remo Pareschi*

Main category: cs.AI

TL;DR: 本文提出了一种名为“量子溯因”的新范式，用于模拟溯因推理。它通过将假设置于叠加态、允许干涉并动态合成而非过早消除，以更忠实地反映人类推理的复杂性和多面性，从而超越了传统AI的排除式搜索方法。


<details>
  <summary>Details</summary>
Motivation: 传统的AI溯因推理方法将溯因简化为排除式搜索，将假设视为互斥并进行剪枝，直到只剩下一个“最佳”解释。这种还原论的框架忽视了人类推理者如何同时维持多个解释线索、处理矛盾并生成新颖的综合。

Method: 本文引入了“量子溯因”，这是一种非经典范式，它将假设建模为叠加态，允许它们进行建设性或破坏性干涉，并且只有在与证据达到一致时才发生坍缩。该框架以量子认知为基础，并利用现代NLP嵌入和生成式AI实现。

Result: 该框架支持动态合成而非过早消除。通过对历史悬案、文学作品、医学诊断和科学理论演变等案例研究，量子溯因被证明更忠实于人类推理的建设性和多面性。

Conclusion: 量子溯因提供了一种更忠实于人类推理本质的方法，并为构建富有表现力和透明的AI推理系统开辟了道路，克服了传统AI排除式搜索的局限性。

Abstract: Abductive reasoning - the search for plausible explanations - has long been
central to human inquiry, from forensics to medicine and scientific discovery.
Yet formal approaches in AI have largely reduced abduction to eliminative
search: hypotheses are treated as mutually exclusive, evaluated against
consistency constraints or probability updates, and pruned until a single
"best" explanation remains. This reductionist framing overlooks the way human
reasoners sustain multiple explanatory lines in suspension, navigate
contradictions, and generate novel syntheses. This paper introduces quantum
abduction, a non-classical paradigm that models hypotheses in superposition,
allows them to interfere constructively or destructively, and collapses only
when coherence with evidence is reached. Grounded in quantum cognition and
implemented with modern NLP embeddings and generative AI, the framework
supports dynamic synthesis rather than premature elimination. Case studies span
historical mysteries (Ludwig II of Bavaria, the "Monster of Florence"),
literary demonstrations ("Murder on the Orient Express"), medical diagnosis,
and scientific theory change. Across these domains, quantum abduction proves
more faithful to the constructive and multifaceted nature of human reasoning,
while offering a pathway toward expressive and transparent AI reasoning
systems.

</details>


### [29] [KAHAN: Knowledge-Augmented Hierarchical Analysis and Narration for Financial Data Narration](https://arxiv.org/abs/2509.17037)
*Yajing Yang,Tony Deng,Min-Yen Kan*

Main category: cs.AI

TL;DR: KAHAN是一个知识增强的分层框架，利用大型语言模型（LLMs）作为领域专家，从原始表格数据中系统地提取实体、配对、群组和系统层面的洞察，并在金融报告和医疗保健领域表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有方法在从原始表格数据中提取多层次、高质量洞察方面可能存在不足，研究旨在开发一个能系统性分析并利用LLMs专业知识的框架。

Method: KAHAN是一个知识增强的分层框架，它系统地在实体、配对、群组和系统层面从表格数据中提取洞察。它独特地利用LLMs作为领域专家来驱动分析过程。

Result: 在DataTales金融报告基准测试中，KAHAN在叙述质量上（由GPT-4o评估）比现有方法高出20%以上，保持了98.2%的事实准确性，并在人工评估中展示了实用性。研究还发现，知识质量通过蒸馏驱动模型性能，分层分析的益处随市场复杂性而变化，并且该框架能有效迁移到医疗保健领域。

Conclusion: KAHAN框架能有效利用LLMs的领域知识，从表格数据中提取多层次的洞察，其性能受知识质量驱动，并且具有跨领域（如金融和医疗保健）的泛化能力和实用价值。

Abstract: We propose KAHAN, a knowledge-augmented hierarchical framework that
systematically extracts insights from raw tabular data at entity, pairwise,
group, and system levels. KAHAN uniquely leverages LLMs as domain experts to
drive the analysis. On DataTales financial reporting benchmark, KAHAN
outperforms existing approaches by over 20% on narrative quality (GPT-4o),
maintains 98.2% factuality, and demonstrates practical utility in human
evaluation. Our results reveal that knowledge quality drives model performance
through distillation, hierarchical analysis benefits vary with market
complexity, and the framework transfers effectively to healthcare domains. The
data and code are available at https://github.com/yajingyang/kahan.

</details>


### [30] [From domain-landmark graph learning to problem-landmark graph generation](https://arxiv.org/abs/2509.17062)
*Cristian Pérez-Corral,Antonio Garrido,Laura Sebastia*

Main category: cs.AI

TL;DR: 本文提出了一种新方法，通过从多个规划任务中学习参数化地标之间的概率提升排序关系，以生成一个领域通用的概率提升排序图。然后，该图可以实例化到新的规划任务中，以提取地标排序，从而克服了传统地标提取方法对特定任务的敏感性问题。


<details>
  <summary>Details</summary>
Motivation: 传统地标提取方法的主要局限性在于它们对特定规划任务过于敏感，导致提取的地标完全针对单个实例，从而限制了它们在同一规划领域内其他实例上的适用性。

Method: 该方法包括两个主要步骤：1) 从规划领域的多个规划任务中学习地标关系，构建一个概率提升排序图，该图捕获了参数化地标之间关系的加权抽象。2) 给定一个该领域的新规划任务，将图中的关系实例化到该特定实例中。实例化过程分两阶段：首先，生成两个图，分别实例化来自初始状态和目标状态的信息；其次，通过搜索等价性将这两个图合并为一个统一的图，以提取地标排序。

Result: 通过在知名规划领域上评估了该方法所发现信息的精确度和召回率。

Conclusion: 尽管所学到的排序是概率性的（非100%确定），但它们在规划中仍然非常有用。该方法提供了一种学习领域通用、可泛化的地标排序的新途径，有效解决了传统方法的局限性。

Abstract: Landmarks have long played a pivotal role in automated planning, serving as
crucial elements for improving the planning algorithms. The main limitation of
classical landmark extraction methods is their sensitivity to specific planning
tasks. This results in landmarks fully tailored to individual instances,
thereby limiting their applicability across other instances of the same
planning domain. We propose a novel approach that learns landmark relationships
from multiple planning tasks of a planning domain. This leads to the creation
of a \textit{probabilistic lifted ordering graph}, as a structure that captures
weighted abstractions of relationships between parameterized landmarks.
Although these orderings are not 100\% true (they are probabilistic), they can
still be very useful in planning. Next, given a new planning task for that
domain, we instantiate the relationships from that graph to this particular
instance. This instantiation operates in two phases. First, it generates two
graphs: the former instantiating information from the initial state and the
latter from the goal state. Second, it combines these two graphs into one
unified graph by searching equivalences to extract landmark orderings. We
evaluate the precision and recallof the information found by our approach over
well-known planning domains.

</details>


### [31] [RALLM-POI: Retrieval-Augmented LLM for Zero-shot Next POI Recommendation with Geographical Reranking](https://arxiv.org/abs/2509.17066)
*Kunrong Li,Kwan Hui Lim*

Main category: cs.AI

TL;DR: RALLM-POI是一个无需额外训练的框架，它结合大型语言模型（LLMs）与检索增强生成和自我修正机制，用于解决下一兴趣点（POI）推荐中LLM生成结果通用或地理不相关的问题。


<details>
  <summary>Details</summary>
Motivation: 传统的下一POI推荐模型需要大量训练，而LLMs虽然灵活且能提供零样本解决方案，但由于缺乏轨迹和空间上下文，常生成通用或地理上不相关的结果。

Method: 本文提出了RALLM-POI框架，它将LLMs与检索增强生成和自我修正相结合。具体方法包括：1) 历史轨迹检索器（HTR），用于检索相关的历史轨迹作为上下文参考；2) 地理距离重排序器（GDR），用于优先排序空间相关的轨迹；3) 智能体LLM修正器（ALR），通过自我反思来优化输出。

Result: RALLM-POI在没有额外训练的情况下，在三个真实的Foursquare数据集上实现了显著的准确性提升，超越了传统的和基于LLM的基线模型。

Conclusion: RALLM-POI通过结合LLMs、检索增强和自我修正，提供了一种有效且无需训练的下一POI推荐解决方案，显著提高了推荐的准确性和相关性。

Abstract: Next point-of-interest (POI) recommendation predicts a user's next
destination from historical movements. Traditional models require intensive
training, while LLMs offer flexible and generalizable zero-shot solutions but
often generate generic or geographically irrelevant results due to missing
trajectory and spatial context. To address these issues, we propose RALLM-POI,
a framework that couples LLMs with retrieval-augmented generation and
self-rectification. We first propose a Historical Trajectory Retriever (HTR)
that retrieves relevant past trajectories to serve as contextual references,
which are then reranked by a Geographical Distance Reranker (GDR) for
prioritizing spatially relevant trajectories. Lastly, an Agentic LLM Rectifier
(ALR) is designed to refine outputs through self-reflection. Without additional
training, RALLM-POI achieves substantial accuracy gains across three real-world
Foursquare datasets, outperforming both conventional and LLM-based baselines.
Code is released at https://github.com/LKRcrocodile/RALLM-POI.

</details>


### [32] [Intention-aware Hierarchical Diffusion Model for Long-term Trajectory Anomaly Detection](https://arxiv.org/abs/2509.17068)
*Chen Wang,Sarah Erfani,Tansu Alpcan,Christopher Leckie*

Main category: cs.AI

TL;DR: 本文提出了一种名为IHiD的无监督轨迹异常检测方法，它通过结合逆Q学习（用于高层意图评估）和扩散模型（用于低层子轨迹分析），同时考虑智能体的高层意图和低层导航细节，从而有效捕获正常轨迹的多样性并显著提升异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 长期轨迹异常检测面临挑战，因为轨迹数据具有多样性和复杂的时空依赖性。现有方法未能同时考虑智能体的高层意图和低层导航细节，这限制了它们捕获正常轨迹多样性的能力。

Method: 本文提出了一种名为意图感知分层扩散模型（IHiD）的无监督轨迹异常检测方法。该方法采用分层结构：高层模型利用逆Q学习评估选定子目标是否与智能体意图一致（基于预测Q值）；低层模型使用扩散模型生成以子目标信息为条件的子轨迹，并通过重建误差进行异常检测。通过整合这两个模型，IHiD有效利用了子目标转换知识，旨在捕获正常轨迹的多种分布。

Result: 实验结果表明，所提出的IHiD方法在F1分数方面，比现有最先进的基线方法，异常检测性能提高了高达30.2%。

Conclusion: IHiD方法通过有效利用子目标转换知识，并设计用于捕获正常轨迹的多种分布，显著提高了轨迹异常检测的性能，克服了现有方法在同时考虑高层意图和低层细节方面的局限性。

Abstract: Long-term trajectory anomaly detection is a challenging problem due to the
diversity and complex spatiotemporal dependencies in trajectory data. Existing
trajectory anomaly detection methods fail to simultaneously consider both the
high-level intentions of agents as well as the low-level details of the agent's
navigation when analysing an agent's trajectories. This limits their ability to
capture the full diversity of normal trajectories. In this paper, we propose an
unsupervised trajectory anomaly detection method named Intention-aware
Hierarchical Diffusion model (IHiD), which detects anomalies through both
high-level intent evaluation and low-level sub-trajectory analysis. Our
approach leverages Inverse Q Learning as the high-level model to assess whether
a selected subgoal aligns with an agent's intention based on predicted
Q-values. Meanwhile, a diffusion model serves as the low-level model to
generate sub-trajectories conditioned on subgoal information, with anomaly
detection based on reconstruction error. By integrating both models, IHiD
effectively utilises subgoal transition knowledge and is designed to capture
the diverse distribution of normal trajectories. Our experiments show that the
proposed method IHiD achieves up to 30.2% improvement in anomaly detection
performance in terms of F1 score over state-of-the-art baselines.

</details>


### [33] [Governing Automated Strategic Intelligence](https://arxiv.org/abs/2509.17087)
*Nicholas Kruus,Madhavendra Thakur,Adam Khoja,Leonhard Nagel,Maximilian Nicholson,Abeer Sharma,Jason Hausenloy,Alberto KoTafoya,Aliya Mukhanova,Alli Katila-Miikkulainen,Harish Chandran,Ivan Zhang,Jessie Chen,Joel Raj,Jord Nguyen,Lai Hsien Hao,Neja Jayasundara,Soham Sen,Sophie Zhang,Ashley Dora Kokui Tamaklo,Bhavya Thakur,Henry Close,Janghee Lee,Nina Sefton,Raghavendra Thakur,Shiv Munagala,Yeeun Kim*

Main category: cs.AI

TL;DR: 国家间战略竞争力将由前沿AI模型驱动，尤其是在自动化军事情报方面。多模态基础模型有望大规模整合多样化数据，自动化战略分析。本研究旨在评估其能力，并为国家在新范式下保持竞争力提供建议。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注AI在新型军事模式（如致命自主武器）或战略决策中的应用，但对于AI系统大规模整合和分析多样化数据（如同“数据中心里的CIA分析师”）以进行战略分析的能力及其地缘政治影响探讨不足。

Method: 进行初步提升研究以实证评估这些系统的能力；提出这些系统将回答的真实问题分类法；建立决定系统AI能力的高层模型；为国家在自动化情报新范式下保持战略竞争力提供建议。

Result: 多模态基础模型有望整合卫星图像、手机定位追踪、社交媒体记录和书面文件等多样化数据，形成可查询系统，从而自动化先前由人类完成的战略分析。研究将通过初步提升研究实证评估这些能力。

Conclusion: 国家间战略竞争力将日益由前沿AI模型定义，特别是在自动化军事情报领域。多模态基础模型将彻底改变战略分析方式，国家需要采纳建议以适应这一新范式并保持战略竞争力。

Abstract: Military and economic strategic competitiveness between nation-states will
increasingly be defined by the capability and cost of their frontier artificial
intelligence models. Among the first areas of geopolitical advantage granted by
such systems will be in automating military intelligence. Much discussion has
been devoted to AI systems enabling new military modalities, such as lethal
autonomous weapons, or making strategic decisions. However, the ability of a
country of "CIA analysts in a data-center" to synthesize diverse data at scale,
and its implications, have been underexplored. Multimodal foundation models
appear on track to automate strategic analysis previously done by humans. They
will be able to fuse today's abundant satellite imagery, phone-location traces,
social media records, and written documents into a single queryable system. We
conduct a preliminary uplift study to empirically evaluate these capabilities,
then propose a taxonomy of the kinds of ground truth questions these systems
will answer, present a high-level model of the determinants of this system's AI
capabilities, and provide recommendations for nation-states to remain
strategically competitive within the new paradigm of automated intelligence.

</details>


### [34] [MCTS-EP: Empowering Embodied Planning with Online Preference Optimization](https://arxiv.org/abs/2509.17116)
*Hang Xu,Zang Yu,Yehui Tang,Pengbo Hu,Yuhao Tang,Hao Dong*

Main category: cs.AI

TL;DR: 本文提出MCTS-EP，一个结合大语言模型（LLM）和蒙特卡洛树搜索（MCTS）的在线学习框架，用于训练具身智能体。该框架通过MCTS引导探索、高效多模态推理和偏好优化迭代训练，在理论上和实践中都优于传统算法，并在多个基准测试中达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有具身智能体训练方法可能在探索效率、多模态推理和性能界限方面存在局限。研究旨在开发一种更高效、性能更优越的在线学习框架，以提升具身智能体的训练效果。

Method: MCTS-EP框架整合了三个关键组件：1. MCTS引导的探索，用于收集偏好数据；2. 高效的多模态推理机制；3. 基于偏好优化的迭代训练流程。该方法在理论上被证明在损失函数强凸时能实现比传统在线策略算法更好的性能界限，并且可以被视为GAIL的一种搜索增强变体。

Result: MCTS-EP在多个基准测试中取得了最先进的性能：在ALFWorld中，文本和视觉任务的成功率分别达到92%和87%；在WebShop中，平均奖励达到0.81。此外，在视觉ALFWorld中，平均交互步骤从18.7/19.5减少到10.2/9.9步。理论上，当损失函数强凸时，MCTS-EP实现了比传统在线策略算法更好的性能界限。

Conclusion: MCTS-EP是一个结合LLM和MCTS的有效在线学习框架，用于训练具身智能体。它通过创新的探索、推理和训练机制，不仅在理论上提供了优越的性能保证，而且在实际应用中也显著提升了智能体的表现，达到了最先进的水平。

Abstract: This paper introduces MCTS-EP, an online learning framework that combines
large language models (LLM) with Monte Carlo Tree Search (MCTS) for training
embodied agents. MCTS-EP integrates three key components: MCTS-guided
exploration for preference data collection, efficient multi-modal reasoning
mechanism, and iterative training pipeline based on preference optimization. We
theoretically prove that MCTS-EP achieves better performance bounds than
conventional on-policy algorithms when the loss function is strongly convex,
and demonstrate that it can be formulated as a search-enhanced variant of GAIL.
MCTS-EP achieves state-of-the-art performace across serval benchmarks. In
ALFWorld, it achieves 92% and 87% success rates for textual and visual tasks.
In WebShop, it reaches an average reward of 0.81. MTCS-EP also reduces average
interaction steps from from 18.7/19.5 to 10.2/9.9 steps in visual ALFWorld.Code
available at: https://github.com/xuhang-2/Embodied-Agent-Planning

</details>


### [35] [ARE: Scaling Up Agent Environments and Evaluations](https://arxiv.org/abs/2509.17158)
*Pierre Andrews,Amine Benhalloum,Gerard Moreno-Torres Bertran,Matteo Bettini,Amar Budhiraja,Ricardo Silveira Cabral,Virginie Do,Romain Froger,Emilien Garreau,Jean-Baptiste Gaya,Hugo Laurençon,Maxime Lecanu,Kunal Malkan,Dheeraj Mekala,Pierre Ménard,Grégoire Mialon,Ulyana Piterbarg,Mikhail Plekhanov,Mathieu Rita,Andrey Rusakov,Thomas Scialom,Vladislav Vorotilov,Mengjue Wang,Ian Yu*

Main category: cs.AI

TL;DR: 本文介绍了Meta Agents Research Environments (ARE)平台，用于可扩展地创建环境和集成应用，并提出了Gaia2基准测试，旨在衡量通用智能体的能力，揭示了现有智能体在推理与效率之间的权衡，并强调了新架构和评估方法的重要性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是为了弥合模型开发与实际部署之间的差距，提供一个可扩展的平台来创建复杂多样的环境，并构建一个能衡量通用智能体能力、处理歧义、噪声、动态环境、协作和时间限制的新型基准测试，以克服现有静态基准测试的局限性。

Method: 本文引入了Meta Agents Research Environments (ARE)平台，该平台提供简单的抽象来构建具有规则、工具、内容和验证器的复杂环境。在此基础上，开发了Gaia2基准测试，它以异步方式运行，要求智能体处理歧义、噪声、适应动态环境、与其他智能体协作并在时间限制下运行。

Result: 实验结果表明，没有单一系统能在所有智能维度上占据主导地位：更强的推理能力往往以牺牲效率为代价，并且预算扩展曲线趋于平稳，这突显了对新架构和自适应计算策略的需求。Gaia2的异步运行也揭示了在静态设置中不可见的新故障模式。

Conclusion: ARE的抽象能力使得Gaia2可以持续扩展到其他环境，赋能社区快速创建特定领域的新基准测试。在AI发展的后期阶段，定义有意义的任务和进行鲁棒的评估对于推动前沿能力至关重要。

Abstract: We introduce Meta Agents Research Environments (ARE), a research platform for
scalable creation of environments, integration of synthetic or real
applications, and execution of agentic orchestrations. ARE provides simple
abstractions to build complex and diverse environments, each with their own
rules, tools, content, and verifiers, helping to bridge the gap between model
development and real-world deployment. We also propose Gaia2, a benchmark built
in ARE and designed to measure general agent capabilities. Beyond search and
execution, Gaia2 requires agents to handle ambiguities and noise, adapt to
dynamic environments, collaborate with other agents, and operate under temporal
constraints. Unlike prior benchmarks, Gaia2 runs asynchronously, surfacing new
failure modes that are invisible in static settings. Our experiments show that
no system dominates across the intelligence spectrum: stronger reasoning often
comes at the cost of efficiency, and budget scaling curves plateau,
highlighting the need for new architectures and adaptive compute strategies.
Perhaps more importantly, ARE abstractions enable continuous extension of Gaia2
to other environments, empowering the community to rapidly create new
benchmarks tailored to their domains. In AI's second half, progress
increasingly depends on defining meaningful tasks and robust evaluations to
drive frontier capabilities forward.

</details>


### [36] [Shall We Play a Game? Language Models for Open-ended Wargames](https://arxiv.org/abs/2509.17192)
*Glenn Matlin,Parv Mahajan,Isaac Song,Yixiong Hao,Ryan Bard,Stu Topp,Evan Montoya,M. Rehan Parwani,Soham Shetty,Mark Riedl*

Main category: cs.AI

TL;DR: 该论文对100篇关于AI在兵棋推演中应用的近期文献进行了范围界定文献综述，构建了兵棋推演的本体论，并为在开放式兵棋推演中何时以及如何使用语言模型（LM）提供了考量、安全建议和最佳实践，最后提出了开放研究挑战。


<details>
  <summary>Details</summary>
Motivation: 兵棋推演（包括开放式、使用自然语言的类型）被广泛用于战略决策探索和娱乐。鉴于语言模型（LM）日益被认为能为现实世界决策提供洞察，研究其在兵棋推演中的应用变得重要。

Method: 对100篇关于AI在兵棋推演中应用的近期精选文献进行范围界定文献综述。根据玩家或裁决者所获得的创造力，构建兵棋推演的本体论。针对玩家和裁决者开放度最高的兵棋推演，提炼出语言模型在不同应用领域的使用考量。提出安全考量和部署语言模型的最佳实践。

Result: 构建了一个基于玩家或裁决者创造力的兵棋推演本体论。提炼了一套在不同应用领域中使用语言模型的考量。提出了一套在开放式兵棋推演中部署语言模型的安全考量和最佳实践。指出了高影响力的开放研究挑战。

Conclusion: 论文为在开放式兵棋推演中理解和应用语言模型提供了一个框架，并确定了未来研究的关键领域，强调了在这一复杂领域进一步探索的必要性。

Abstract: Wargames are multi-faceted, multi-player depictions of conflict in which
participants' decisions influence future events. Wargames are often used to
explore the strategic implications of decision-making. However, it also
encompasses entertainment-oriented simulations, ranging from _Chess_ to
tabletop role-playing games like _Dungeons & Dragons_ (D&D). On the more
open-ended side of the spectrum of wargames, players use natural language to
convey their moves, and adjudicators propose outcomes. Language Models (LMs)
are increasingly being considered for how they can provide insights into
real-world, consequential decisions. We conduct a scoping literature review of
a curated selection of 100 recent works on AI in wargames, from which we
construct an ontology of wargames in terms of the creativity afforded to either
the players or adjudicators. Focusing on the space of wargames with the most
open-endedness for players and adjudicators, we distill a set of considerations
for when and how to use LMs in different application areas. We also present a
set of safety considerations, best practices for deploying LMs in open-ended
wargames, and conclude with a set of high-impact open research challenges.

</details>


### [37] [MoEs Are Stronger than You Think: Hyper-Parallel Inference Scaling with RoE](https://arxiv.org/abs/2509.17238)
*Soheil Zibakhsh,Mohammad Samragh,Kumari Nishu,Lauren Hannah,Arnav Kundu,Minsik Cho*

Main category: cs.AI

TL;DR: 本文提出了一种名为“超并行扩展”（hyper-parallel scaling）的框架，通过在令牌级别聚合来自多个专家的输出，提高了大型语言模型（LLMs）的预测质量。该框架在混合专家（MoE）模型中实现为“专家名册”（Roster of Experts, RoE），这是一种无需训练的推理算法，能在不进行模型微调的情况下，以更低的计算成本使小型MoE模型达到与大型模型相当的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLMs）通常通过序列级别的扩展方法（如思维链）来提高生成质量。本文旨在引入一种互补的框架，从令牌级别提升预测质量。

Method: 该研究引入了“超并行扩展”框架，它在令牌级别计算并聚合模型为单个令牌生成的多个输出建议。具体在混合专家（MoE）模型中，这被称为“专家名册”（RoE）。RoE是一种无需训练的推理算法，它向专家路由机制中注入受控的随机性，从而为每个令牌采样多个不同的专家，并聚合它们的输出以获得更准确的最终预测。为解决计算成本问题，该方法还引入了高效的批处理策略和专门的KV缓存机制，以最小化计算和内存开销。

Result: 实验结果表明，RoE使一个7B的MoE模型能够匹配10.5B MoE模型的性能，同时推理计算量减少了30%。这些性能提升是在不进行任何模型参数微调的情况下实现的。

Conclusion: RoE是一种有效的、无需训练的推理算法，通过在令牌级别引入超并行扩展，显著提高了MoE模型的预测质量和计算效率，使其能够以更小的模型规模实现与更大模型相当的性能。

Abstract: The generation quality of large language models (LLMs) is often improved by
utilizing inference-time sequence-level scaling methods (e.g.,
Chain-of-Thought). We introduce hyper-parallel scaling, a complementary
framework that improves prediction quality at the token level. Hyper-parallel
scaling computes and aggregates multiple output proposals for a single token
from the model. We implement this concept in Mixture-of-Experts (MoE) models,
which we refer to as Roster of Experts (RoE). RoE is a training-free inference
algorithm that turns a single MoE into a dynamic ensemble of MoEs. RoE injects
controlled stochasticity into the expert routing mechanism, enabling it to
sample multiple diverse experts for each token and aggregate their outputs for
a more accurate final prediction.To overcome the computational cost, we
introduce an efficient batching strategy and a specialized KV-caching mechanism
that minimizes compute and memory overhead. For example, RoE enables a 7B MoE
model to match the performance of a 10.5B MoE model while using 30% less
compute for inference. These gains are achieved without any fine-tuning of
model parameters.

</details>


### [38] [Can Agents Judge Systematic Reviews Like Humans? Evaluating SLRs with LLM-based Multi-Agent System](https://arxiv.org/abs/2509.17240)
*Abdullah Mushtaq,Muhammad Rafay Naeem,Ibrahim Ghaznavi,Alaa Abd-alrazaq,Aliya Tabassum,Junaid Qadir*

Main category: cs.AI

TL;DR: 本文提出一个基于大型语言模型（LLM）的多智能体系统，旨在协助研究人员评估系统性文献综述（SLR）的整体质量，并在初步研究中取得了与专家标注高度一致的结果。


<details>
  <summary>Details</summary>
Motivation: 系统性文献综述（SLR）是循证研究的基础，但其过程耗时费力，且在不同学科间存在一致性问题。

Method: 研究构建了一个基于多智能体系统（MAS）架构的LLM辅助SLR评估副驾驶系统。该系统利用学术数据库，自动化执行协议验证、方法学评估和主题相关性检查。其设计整合了遵循PRISMA指南的专业智能体方法，以支持更结构化和可解释的评估。

Result: 对来自不同领域的五篇已发表SLR进行的初步研究显示，系统输出与专家标注的PRISMA评分之间达到84%的一致性。

Conclusion: 早期结果表明该系统前景广阔，代表着迈向可扩展、精确的自然语言处理（NLP）驱动系统的重要一步，可用于跨学科工作流程，并展示了其在严格、领域无关的知识聚合方面简化综述过程的潜力。

Abstract: Systematic Literature Reviews (SLRs) are foundational to evidence-based
research but remain labor-intensive and prone to inconsistency across
disciplines. We present an LLM-based SLR evaluation copilot built on a
Multi-Agent System (MAS) architecture to assist researchers in assessing the
overall quality of the systematic literature reviews. The system automates
protocol validation, methodological assessment, and topic relevance checks
using a scholarly database. Unlike conventional single-agent methods, our
design integrates a specialized agentic approach aligned with PRISMA guidelines
to support more structured and interpretable evaluations. We conducted an
initial study on five published SLRs from diverse domains, comparing system
outputs to expert-annotated PRISMA scores, and observed 84% agreement. While
early results are promising, this work represents a first step toward scalable
and accurate NLP-driven systems for interdisciplinary workflows and reveals
their capacity for rigorous, domain-agnostic knowledge aggregation to
streamline the review process.

</details>


### [39] [Mind the Gap: Comparing Model- vs Agentic-Level Red Teaming with Action-Graph Observability on GPT-OSS-20B](https://arxiv.org/abs/2509.17259)
*Ilham Wicaksono,Zekun Wu,Rahul Patel,Theo King,Adriano Koshiyama,Philip Treleaven*

Main category: cs.AI

TL;DR: 本研究通过对比红队分析发现，在智能体AI系统中存在“仅智能体”漏洞，这些漏洞在独立模型层面不出现，且独立模型的漏洞不总能泛化到智能体部署中。


<details>
  <summary>Details</summary>
Motivation: 随着行业越来越多地采用智能体AI系统，理解其独特的漏洞变得至关重要。先前的研究表明，模型层面的安全缺陷未能完全捕捉到智能体部署中的风险，因为模型在智能体部署中会与工具和外部环境交互。

Method: 对200亿参数的开源模型GPT-OSS-20B进行了对比红队分析。使用AgentSeer可观测性框架将智能体系统分解为细粒度动作和组件。在两个层面（独立模型和在智能体循环中运行的模型）应用了带有HarmBench有害目标的迭代红队攻击。

Result: 评估揭示了模型层面和智能体层面漏洞特征之间的根本差异。关键在于，发现了“仅智能体”漏洞，这些攻击向量仅在智能体执行环境中出现，而对独立模型无效。智能体层面的迭代攻击成功实现了在模型层面完全失败的目标，其中工具调用上下文的漏洞比非工具上下文高24%。相反，某些模型特有的漏洞仅在模型层面有效，转移到智能体上下文时则失败。

Conclusion: 独立模型漏洞不总是能泛化到已部署的智能体系统中，这表明需要针对智能体系统进行特定的安全分析，以应对其独特的漏洞。

Abstract: As the industry increasingly adopts agentic AI systems, understanding their
unique vulnerabilities becomes critical. Prior research suggests that security
flaws at the model level do not fully capture the risks present in agentic
deployments, where models interact with tools and external environments. This
paper investigates this gap by conducting a comparative red teaming analysis of
GPT-OSS-20B, a 20-billion parameter open-source model. Using our observability
framework AgentSeer to deconstruct agentic systems into granular actions and
components, we apply iterative red teaming attacks with harmful objectives from
HarmBench at two distinct levels: the standalone model and the model operating
within an agentic loop. Our evaluation reveals fundamental differences between
model level and agentic level vulnerability profiles. Critically, we discover
the existence of agentic-only vulnerabilities, attack vectors that emerge
exclusively within agentic execution contexts while remaining inert against
standalone models. Agentic level iterative attacks successfully compromise
objectives that completely failed at the model level, with tool-calling
contexts showing 24\% higher vulnerability than non-tool contexts. Conversely,
certain model-specific exploits work exclusively at the model level and fail
when transferred to agentic contexts, demonstrating that standalone model
vulnerabilities do not always generalize to deployed systems.

</details>


### [40] [CogAtom: From Cognitive Atoms to Olympiad-level Mathematical Reasoning in Large Language Models](https://arxiv.org/abs/2509.17318)
*Zhuofan Chen,Jiyuan He,Yichi Zhang,Xing Hu,Haoxing Wen,Jun Bai,Wenge Rong*

Main category: cs.AI

TL;DR: CogAtom是一个基于认知原子的框架，通过从人类解法中提取并重组基本推理单元，生成具有数学严谨性和认知多样性的高难度数学问题，以克服奥林匹克级别数学问题稀缺的瓶颈。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在数学推理方面面临挑战，原因在于其对多步骤推理和抽象概念整合的需求。现有的测试时扩展技术严重依赖高质量、有挑战性的问题，但奥林匹克级别数学问题的稀缺性是一个瓶颈。

Method: CogAtom将问题构建建模为选择和重组从人类解法中提取的“认知原子”的过程。它采用促进多样性的随机游走算法探索认知原子空间，并通过基于约束的重组机制确保逻辑健全性和结构有效性。通过控制认知原子的数量，可以精确调整问题难度。

Result: 实验结果表明，CogAtom在准确性、推理深度和多样性方面优于现有方法，生成的问题难度与AIME（美国数学邀请赛）相当，但在结构多样性上超越了AIME。

Conclusion: CogAtom为可扩展、高质量的数学问题生成提供了一条基于认知的方法，解决了LLM数学推理训练中高质量问题稀缺的问题。

Abstract: Mathematical reasoning poses significant challenges for Large Language Models
(LLMs) due to its demand for multi-step reasoning and abstract conceptual
integration. While recent test-time scaling techniques rely heavily on
high-quality, challenging problems, the scarcity of Olympiad-level math
problems remains a bottleneck. We introduce CogAtom, a novel cognitive
atom-based framework for synthesizing mathematically rigorous and cognitively
diverse problems. Unlike prior approaches, CogAtom models problem construction
as a process of selecting and recombining fundamental reasoning units,
cognitive atoms, extracted from human-authored solutions. A diversity-promoting
random walk algorithm enables exploration of the cognitive atom space, while a
constraint-based recombination mechanism ensures logical soundness and
structural validity. The combinatorial nature of the graph structure provides a
near-infinite space of reasoning paths, and the walk algorithm systematically
explores this space to achieve large-scale synthesis of high-quality problems;
meanwhile, by controlling the number of cognitive atoms, we can precisely
adjust problem difficulty, ensuring diversity, scalability, and controllability
of the generated problems. Experimental results demonstrate that CogAtom
outperforms existing methods in accuracy, reasoning depth, and diversity,
generating problems that closely match the difficulty of AIME while exceeding
it in structural variation. Our work offers a cognitively grounded pathway
toward scalable, high-quality math problem generation.Our code is publicly
available at https://github.com/Icarus-1111/CogAtom.

</details>


### [41] [LLaVul: A Multimodal LLM for Interpretable Vulnerability Reasoning about Source Code](https://arxiv.org/abs/2509.17337)
*Ala Jararweh,Michael Adams,Avinash Sahu,Abdullah Mueen,Afsah Anwar*

Main category: cs.AI

TL;DR: LLaVul是一个多模态大语言模型，通过问答（QA）提供代码漏洞的细粒度推理，优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有漏洞分析方法过于简化，多将漏洞分析视为分类任务，且当前代码大语言模型（LLMs）虽擅长代码理解，但缺乏安全特异性推理能力。

Method: 提出了LLaVul模型，这是一个多模态LLM，旨在将配对的代码和自然语言查询整合到统一空间中。构建了一个包含真实世界漏洞和安全相关问答的精选数据集，用于训练和评估模型。

Result: LLaVul在问答和漏洞检测任务中均优于最先进的通用和代码LLM。通过定性分析进一步解释了模型的决策过程，展示了其能力和局限性。

Conclusion: 通过整合代码和问答，LLaVul实现了更具解释性和更注重安全的代码理解能力。

Abstract: Increasing complexity in software systems places a growing demand on
reasoning tools that unlock vulnerabilities manifest in source code. Many
current approaches focus on vulnerability analysis as a classifying task,
oversimplifying the nuanced and context-dependent real-world scenarios. Even
though current code large language models (LLMs) excel in code understanding,
they often pay little attention to security-specific reasoning. We propose
LLaVul, a multimodal LLM tailored to provide fine-grained reasoning about code
through question-answering (QA). Our model is trained to integrate paired code
and natural queries into a unified space, enhancing reasoning and
context-dependent insights about code vulnerability. To evaluate our model
performance, we construct a curated dataset of real-world vulnerabilities
paired with security-focused questions and answers. Our model outperforms
state-of-the-art general-purpose and code LLMs in the QA and detection tasks.
We further explain decision-making by conducting qualitative analysis to
highlight capabilities and limitations. By integrating code and QA, LLaVul
enables more interpretable and security-focused code understanding.

</details>


### [42] [Multi-Scenario Highway Lane-Change Intention Prediction: A Physics-Informed AI Framework for Three-Class Classification](https://arxiv.org/abs/2509.17354)
*Jiazhao Shi,Yichen Lin,Yiheng Hua,Ziyu Wang,Zijian Zhang,Wenjia Zheng,Yun Song,Kuan Lu,Shoufeng Lu*

Main category: cs.AI

TL;DR: 本研究提出了一种物理信息增强的AI框架，通过整合车辆运动学、交互可行性和交通安全指标，实现了对变道意图（左变道、右变道、不变道）的高精度实时预测，并在复杂场景下表现出强大的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 变道是高速公路上交通事故的主要原因，因此精确的变道意图预测对于提高自动驾驶系统的安全性和决策能力至关重要。现有方法常受限于二分类问题、场景多样性不足以及在较长预测时间下性能下降的问题。

Method: 本研究提出了一种物理信息增强的AI框架，将车辆运动学、交互可行性和交通安全指标（如车距、时间间距、碰撞时间、关闭间隙时间）明确地整合到学习过程中。变道预测被公式化为一个三分类问题（左变道、右变道、不变道），并在直线高速公路（highD）和复杂匝道场景（exiD）下进行评估。研究使用了机器学习模型，特别是LightGBM，并与两层堆叠的LSTM基线进行了比较。

Result: 研究结果表明，通过整合车辆运动学和交互特征，LightGBM等机器学习模型在1秒预测时间下取得了最先进的精度和强大的泛化能力。在highD数据集上，准确率高达99.8%，宏观F1得分达93.6%；在exiD数据集上，准确率达96.1%，宏观F1得分达88.7%，均优于两层堆叠的LSTM基线。

Conclusion: 这些发现证明了物理信息增强和特征丰富的机器学习框架在自动驾驶系统中进行实时变道意图预测的实际优势。

Abstract: Lane-change maneuvers are a leading cause of highway accidents, underscoring
the need for accurate intention prediction to improve the safety and
decision-making of autonomous driving systems. While prior studies using
machine learning and deep learning methods (e.g., SVM, CNN, LSTM, Transformers)
have shown promise, most approaches remain limited by binary classification,
lack of scenario diversity, and degraded performance under longer prediction
horizons. In this study, we propose a physics-informed AI framework that
explicitly integrates vehicle kinematics, interaction feasibility, and
traffic-safety metrics (e.g., distance headway, time headway,
time-to-collision, closing gap time) into the learning process. lane-change
prediction is formulated as a three-class problem that distinguishes left
change, right change, and no change, and is evaluated across both straight
highway segments (highD) and complex ramp scenarios (exiD). By integrating
vehicle kinematics with interaction features, our machine learning models,
particularly LightGBM, achieve state-of-the-art accuracy and strong
generalization. Results show up to 99.8% accuracy and 93.6% macro F1 on highD,
and 96.1% accuracy and 88.7% macro F1 on exiD at a 1-second horizon,
outperforming a two-layer stacked LSTM baseline. These findings demonstrate the
practical advantages of a physics-informed and feature-rich machine learning
framework for real-time lane-change intention prediction in autonomous driving
systems.

</details>


### [43] [Correlation or Causation: Analyzing the Causal Structures of LLM and LRM Reasoning Process](https://arxiv.org/abs/2509.17380)
*Zhizhang FU,Guangsheng Bao,Hongbo Zhang,Chenkai Hu,Yue Zhang*

Main category: cs.AI

TL;DR: 大型语言模型（LLMs）因缺乏因果基础而存在推理问题。本研究发现，通过强化学习（RLVR）训练的语言推理模型（LRMs）能显著增强因果推理能力，减少虚假关联，而LLMs和蒸馏LRMs则未能解决这些问题。


<details>
  <summary>Details</summary>
Motivation: LLMs因缺乏鲁棒的因果基础，可能依赖表面相关性而非真正理解，导致推理中出现不忠实、偏见和不一致等关键问题。虽然LRMs作为替代方案出现，并利用强化学习和蒸馏等技术提高了任务准确性，但这些训练方法对因果关系的影响仍未被充分探索。

Method: 本研究对LLMs和LRMs进行了系统的因果分析。具体方法是检查由四个关键变量组成的结构因果模型（SCMs）：问题指令（Z）、思维过程（T）、推理步骤（X）和答案（Y）。

Result: 研究发现，经过RLVR训练的LRMs表现出增强的因果推理能力，与理想的因果结构更吻合。相比之下，LLMs和经过蒸馏的LRMs未能解决与因果关系相关的缺陷。进一步调查表明，RLVR能减少虚假关联并强化真实的因果模式，从而减轻不忠实性和偏见。此外，对RLVR训练过程动态的检查发现，减少虚假特征与改善因果结构之间存在高度相关性，且因果关系在训练过程中持续改进。

Conclusion: 本研究有助于理解推理模型中的因果关系，强调了RLVR在增强因果推理中的关键作用，并为未来设计具有更强因果基础的AI系统提供了见解。

Abstract: LLMs suffer from critical reasoning issues such as unfaithfulness, bias, and
inconsistency, since they lack robust causal underpinnings and may rely on
superficial correlations rather than genuine understanding. Successive LRMs
have emerged as a promising alternative, leveraging advanced training
techniques such as reinforcement learning (RL) and distillation to improve task
accuracy. However, the impact of these training methods on causality remains
largely unexplored. In this study, we conduct a systematic causal analysis on
LLMs and LRMs, examining structural causal models (SCMs) of four key variables:
problem instruction (Z), thinking process (T), reasoning steps (X), and answer
(Y). Our findings reveal that RLVR-trained LRMs exhibit enhanced causal
reasoning capabilities, aligning more closely with ideal causal structures,
while LLMs and distilled LRMs fail to address causality-related deficiencies.
Our further investigation indicates that RLVR reduces spurious correlations and
strengthens genuine causal patterns, thereby mitigating unfaithfulness and
bias. In addition, our inspection on the dynamics of the RLVR training process
observes a high correlation between reduced spurious features and improved
causal structures, where the causal relationships consistently improve in the
training process. This study contributes to the understanding of causality in
reasoning models, highlights the critical role of RLVR in enhancing causal
reasoning, and provides insights for designing future AI systems with stronger
causal foundations. We release our code and data at
https://github.com/Harryking1999/CoT_Causal_Analysis.

</details>


### [44] [Program Synthesis via Test-Time Transduction](https://arxiv.org/abs/2509.17393)
*Kang-il Lee,Jahyun Koo,Seunghyun Yoon,Minbeom Kim,Hyukhun Koh,Dongryeol Lee,Kyomin Jung*

Main category: cs.AI

TL;DR: 本文提出了一种名为“直推式程序合成”的新范式，通过主动利用测试输入来提高程序合成的鲁棒性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的程序合成方法（无论是基于自然语言描述还是输入-输出示例）在训练示例有限且测试输入涉及各种边缘情况的真实世界场景中，往往难以保证鲁棒性。

Method: 该研究将合成视为在由程序输出定义的有限假设类上的主动学习。它使用大型语言模型（LLM）预测选定测试输入的输出，并淘汰不一致的假设。选择测试输入时采用贪婪最大最小算法，以最小化所需的LLM查询次数。

Result: 该方法在两个真实世界数据集（Playgol和MBPP+）上进行了评估，结果表明它显著提高了程序合成的准确性和效率。

Conclusion: 直推式程序合成通过明确利用测试输入，并结合LLM和主动学习策略，有效解决了传统程序合成在鲁棒性和效率方面的挑战。

Abstract: We introduce transductive program synthesis, a new formulation of the program
synthesis task that explicitly leverages test inputs during synthesis. While
prior approaches to program synthesis--whether based on natural language
descriptions or input-output examples--typically aim to generalize from
training examples, they often struggle with robustness, especially in
real-world settings where training examples are limited and test inputs involve
various edge cases. To address this, we propose a novel framework that improves
robustness by treating synthesis as an active learning over a finite hypothesis
class defined by programs' outputs. We use an LLM to predict outputs for
selected test inputs and eliminate inconsistent hypotheses, where the inputs
are chosen via a greedy maximin algorithm to minimize the number of LLM queries
required. We evaluate our approach on two real-world datasets: Playgol, a
string transformation benchmark, and MBPP+, a Python code generation benchmark.
We demonstrate that our method significantly improves program synthesis in both
accuracy and efficiency. We release our code at
https://github.com/klee972/SYNTRA.

</details>


### [45] [Evaluating Multimodal Large Language Models with Daily Composite Tasks in Home Environments](https://arxiv.org/abs/2509.17425)
*Zhenliang Zhang,Yuxi Wang,Hongzhao Xie,Shiyun Zhao,Mingyuan Liu,Yujie Lu,Xinyi He,Zhenku Cheng,Yujia Peng*

Main category: cs.AI

TL;DR: 研究评估了多模态大语言模型（MLLMs）在模拟家庭环境中执行日常复合任务的能力，结果显示当前MLLMs表现不佳，与通用智能要求存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 通用人工智能（AGI）与传统AI的区别在于其执行需要广泛能力的复合任务的能力。尽管具身智能体（embodied agents）结合多模态大语言模型（MLLMs）提供了丰富的感知和交互能力，但它们是否能解决复合任务仍未被充分探索。

Method: 设计了一系列受儿童早期发展中日常活动启发的复合任务。这些任务在动态模拟家庭环境中进行，涵盖物体理解、空间智能和社交活动三个核心领域。评估了17个领先的专有和开源MLLMs在这些任务上的表现。

Result: 所有评估的MLLMs在这三个领域都表现出持续的差劲性能，表明当前能力与通用智能要求之间存在巨大差距。

Conclusion: 当前多模态大语言模型（MLLMs）在执行复合任务方面存在显著不足，离通用智能要求尚远。本研究设计的任务为评估具身智能体的通用能力提供了一个初步框架，是具身MLLMs发展和实际部署的重要早期步骤。

Abstract: A key feature differentiating artificial general intelligence (AGI) from
traditional AI is that AGI can perform composite tasks that require a wide
range of capabilities. Although embodied agents powered by multimodal large
language models (MLLMs) offer rich perceptual and interactive capabilities, it
remains largely unexplored whether they can solve composite tasks. In the
current work, we designed a set of composite tasks inspired by common daily
activities observed in early childhood development. Within a dynamic and
simulated home environment, these tasks span three core domains: object
understanding, spatial intelligence, and social activity. We evaluated 17
leading proprietary and open-source MLLMs on these tasks. The results
consistently showed poor performance across all three domains, indicating a
substantial gap between current capabilities and general intelligence
requirements. Together, our tasks offer a preliminary framework for evaluating
the general capabilities of embodied agents, marking an early but significant
step toward the development of embodied MLLMs and their real-world deployment.

</details>


### [46] [SPICED: A Synaptic Homeostasis-Inspired Framework for Unsupervised Continual EEG Decoding](https://arxiv.org/abs/2509.17439)
*Yangxuan Zhou,Sha Zhao,Jiquan Wang,Haiteng Jiang,Shijian Li,Tao Li,Gang Pan*

Main category: cs.AI

TL;DR: SPICED是一个受生物学启发的神经形态框架，通过整合突触稳态机制，实现了无监督的持续EEG解码，有效应对新个体间差异，同时缓解灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 人类大脑通过突触稳态实现动态的稳定性-可塑性平衡。受此启发，研究旨在解决持续EEG解码中，特别是面对不断涌现具有个体间差异的新个体时，如何实现鲁棒适应和避免灾难性遗忘的问题。

Method: SPICED框架包含一个新颖的突触网络，通过三个受生物启发的神经机制实现动态扩展：1) 关键记忆再激活；2) 突触巩固；3) 突触重归一化。突触稳态的相互作用动态地增强任务判别性记忆痕迹并削弱有害记忆。通过将这些机制与持续学习系统结合，SPICED优先重放与新个体高度相关的任务判别性记忆痕迹，并抑制有害记忆的重放优先级。

Result: SPICED在持续适应中实现了鲁棒性，并有效缓解了长期持续学习中的灾难性遗忘。在三个EEG数据集上进行了验证，结果表明其有效性。

Conclusion: SPICED是一个有效的神经形态框架，通过模拟突触稳态机制，成功地在无监督持续EEG解码中实现了稳定性与可塑性的平衡，有效应对个体差异并减轻遗忘。

Abstract: Human brain achieves dynamic stability-plasticity balance through synaptic
homeostasis. Inspired by this biological principle, we propose SPICED: a
neuromorphic framework that integrates the synaptic homeostasis mechanism for
unsupervised continual EEG decoding, particularly addressing practical
scenarios where new individuals with inter-individual variability emerge
continually. SPICED comprises a novel synaptic network that enables dynamic
expansion during continual adaptation through three bio-inspired neural
mechanisms: (1) critical memory reactivation; (2) synaptic consolidation and
(3) synaptic renormalization. The interplay within synaptic homeostasis
dynamically strengthens task-discriminative memory traces and weakens
detrimental memories. By integrating these mechanisms with continual learning
system, SPICED preferentially replays task-discriminative memory traces that
exhibit strong associations with newly emerging individuals, thereby achieving
robust adaptations. Meanwhile, SPICED effectively mitigates catastrophic
forgetting by suppressing the replay prioritization of detrimental memories
during long-term continual learning. Validated on three EEG datasets, SPICED
show its effectiveness.

</details>


### [47] [AI Pangaea: Unifying Intelligence Islands for Adapting Myriad Tasks](https://arxiv.org/abs/2509.17460)
*Jianlong Chang,Haixin Wang,Zhiyuan Dang,Li Huang,Zhiyu Wang,Ruoqi Cao,Shihao Piao,Dongzhe Li,Dianyu Gao,Dongsheng Wang,Yin Li,Jinan Sun,Lu Fang,Zhouchen Lin*

Main category: cs.AI

TL;DR: 本文提出了Pangaea，一个AI超级大陆模型，旨在通过统一数据格式和多模态预训练，实现跨众多任务（包括未见任务）的通用化，以克服当前AI模型各自为政的“智能孤岛”问题。


<details>
  <summary>Details</summary>
Motivation: 人工智能领域持续追求人工通用智能（AGI），要求模型能在各种任务（包括未见任务）中泛化。然而，当前的AI模型受限于特定任务，彼此孤立，被定义为“智能孤岛”，这阻碍了AGI的发展。

Method: 研究者提出了Pangaea，第一个AI超级大陆模型。它将所有数据编码成统一格式，并通过对296个跨多种模态的数据集进行预训练来积累通用知识。

Result: Pangaea在45个通用任务和15个涵盖广泛科学主题的科学任务中展现出卓越的泛化能力。深入研究揭示了模态的尺度效应，并将跨模态的通用知识积累量化为几何分布的累积分布函数。

Conclusion: Pangaea展现出处理大量任务的强大潜力，为实现人工通用智能指明了新的方向。

Abstract: The pursuit of artificial general intelligence continuously demands
generalization in one model across myriad tasks, even those not seen before.
However, current AI models are isolated from each other for being limited to
specific tasks, now first defined as Intelligence Islands. To unify
Intelligence Islands into one, we propose Pangaea, the first AI supercontinent
akin to the geological Pangaea. Pangaea encodes any data into a unified format
and accumulates universal knowledge through pre-training on 296 datasets across
diverse modalities. Eventually, it demonstrates remarkable generalization
across 45 general tasks and 15 scientific tasks encompassing a wide range of
scientific subjects. By investigating Pangaea deeper, the scaling effect of
modality is revealed, quantifying the universal knowledge accumulation across
modalities as the cumulative distribution function of a geometric distribution.
On the whole, Pangaea shows strong potential to handle myriad tasks, indicating
a new direction toward artificial general intelligence.

</details>


### [48] [A Multimodal Conversational Assistant for the Characterization of Agricultural Plots from Geospatial Open Data](https://arxiv.org/abs/2509.17544)
*Juan Cañada,Raúl Alonso,Julio Molleda,Fidel Díez*

Main category: cs.AI

TL;DR: 本研究提出了一个开源对话助手，它结合多模态检索和大型语言模型（LLMs），通过自然语言交互降低了非专业用户访问异构农业和地理空间数据的技术门槛。


<details>
  <summary>Details</summary>
Motivation: 开放地球观测（EO）和农业数据集在支持可持续土地管理方面潜力巨大，但其高技术门槛限制了非专业用户的可访问性。

Method: 该研究开发了一个开源对话助手，其架构通过检索增强生成（RAG）结合了正射影像、Sentinel-2植被指数和用户提供的文档。系统能够灵活决定是依赖多模态证据、文本知识还是两者来生成答案。为评估响应质量，采用Qwen3-32B作为LLM评判器，在零样本、无监督设置下，通过多维度定量评估框架进行直接评分。

Result: 初步结果显示，该系统能够对农业查询生成清晰、相关且上下文感知的响应，同时保持在不同地理区域的可复现性和可扩展性。

Conclusion: 本工作的核心贡献包括：一个融合多模态EO和文本知识源的架构；通过自然语言交互降低专业农业信息访问障碍的演示；以及一个开放和可复现的设计。

Abstract: The increasing availability of open Earth Observation (EO) and agricultural
datasets holds great potential for supporting sustainable land management.
However, their high technical entry barrier limits accessibility for non-expert
users. This study presents an open-source conversational assistant that
integrates multimodal retrieval and large language models (LLMs) to enable
natural language interaction with heterogeneous agricultural and geospatial
data. The proposed architecture combines orthophotos, Sentinel-2 vegetation
indices, and user-provided documents through retrieval-augmented generation
(RAG), allowing the system to flexibly determine whether to rely on multimodal
evidence, textual knowledge, or both in formulating an answer. To assess
response quality, we adopt an LLM-as-a-judge methodology using Qwen3-32B in a
zero-shot, unsupervised setting, applying direct scoring in a multi-dimensional
quantitative evaluation framework. Preliminary results show that the system is
capable of generating clear, relevant, and context-aware responses to
agricultural queries, while remaining reproducible and scalable across
geographic regions. The primary contributions of this work include an
architecture for fusing multimodal EO and textual knowledge sources, a
demonstration of lowering the barrier to access specialized agricultural
information through natural language interaction, and an open and reproducible
design.

</details>


### [49] [Is It Certainly a Deepfake? Reliability Analysis in Detection & Generation Ecosystem](https://arxiv.org/abs/2509.17550)
*Neslihan Kose,Anthony Rhodes,Umur Aybars Ciftci,Ilke Demir*

Main category: cs.AI

TL;DR: 本文对深度伪造检测器进行了首次全面的不确定性分析，系统地研究了生成伪影如何影响预测置信度，并发现不确定性流形可用于深度伪造源检测。研究强调了不确定性量化对于可信合成媒体检测的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型在创建合成内容方面的质量和数量不断提升，深度伪造开始在线上引起不信任。尽管提出了深度伪造检测器来对抗这种影响，但检测器被误用（例如将假内容识别为真或反之）进一步加剧了信息误导问题。因此，需要对深度伪造检测器的可靠性进行深入分析。

Method: 研究方法包括：对深度伪造检测器进行首次全面的不确定性分析，系统调查生成伪影如何影响预测置信度，并结合对生成器生成残余变化的分析。利用贝叶斯神经网络和蒙特卡洛 dropout 来量化不同检测器架构中的偶然不确定性和认知不确定性。在两个数据集上，使用九个生成器、四个盲检测器和两个生物检测器进行不确定性评估，比较了不同的不确定性方法，探索了基于区域和像素的不确定性，并进行了消融研究。分析了二元真/假、多类真/假、源检测以及生成器/检测器组合间的留一法实验，以评估其泛化能力、模型校准、不确定性和对对抗性攻击的鲁棒性。此外，引入了不确定性图，以像素级别定位预测置信度。

Result: 研究发现，不确定性流形包含足够一致的信息，可用于深度伪造源检测。不确定性图揭示了与特定生成器伪影相关的独特模式。

Conclusion: 本分析为部署可靠的深度伪造检测系统提供了关键见解，并确立了不确定性量化作为可信合成媒体检测的基本要求。

Abstract: As generative models are advancing in quality and quantity for creating
synthetic content, deepfakes begin to cause online mistrust. Deepfake detectors
are proposed to counter this effect, however, misuse of detectors claiming fake
content as real or vice versa further fuels this misinformation problem. We
present the first comprehensive uncertainty analysis of deepfake detectors,
systematically investigating how generative artifacts influence prediction
confidence. As reflected in detectors' responses, deepfake generators also
contribute to this uncertainty as their generative residues vary, so we cross
the uncertainty analysis of deepfake detectors and generators. Based on our
observations, the uncertainty manifold holds enough consistent information to
leverage uncertainty for deepfake source detection. Our approach leverages
Bayesian Neural Networks and Monte Carlo dropout to quantify both aleatoric and
epistemic uncertainties across diverse detector architectures. We evaluate
uncertainty on two datasets with nine generators, with four blind and two
biological detectors, compare different uncertainty methods, explore region-
and pixel-based uncertainty, and conduct ablation studies. We conduct and
analyze binary real/fake, multi-class real/fake, source detection, and
leave-one-out experiments between the generator/detector combinations to share
their generalization capability, model calibration, uncertainty, and robustness
against adversarial attacks. We further introduce uncertainty maps that
localize prediction confidence at the pixel level, revealing distinct patterns
correlated with generator-specific artifacts. Our analysis provides critical
insights for deploying reliable deepfake detection systems and establishes
uncertainty quantification as a fundamental requirement for trustworthy
synthetic media detection.

</details>


### [50] [MontePrep: Monte-Carlo-Driven Automatic Data Preparation without Target Data Instances](https://arxiv.org/abs/2509.17553)
*Congcong Ge,Yachuan Liu,Yixuan Tang,Yifan Zhu,Yaofeng Tu,Yunjun Gao*

Main category: cs.AI

TL;DR: MontePrep是一种由LLM驱动的、免训练、零目标实例的自动数据准备(ADP)框架，它通过树形搜索将关系数据从不同来源传输到标准化目标，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的自动数据准备(ADP)方法依赖于大量人工监督信号或目标表数据访问权限，这限制了它们在实际场景中的应用。

Method: MontePrep被设计为一个由大型语言模型(LLM)驱动的树形搜索问题，包含三个核心组件：1. 数据准备操作沙盒(DPAS)用于导航搜索并避免不可行管道。2. 基本管道生成器(FPG)利用LLM驱动的蒙特卡洛树搜索在DPAS中逐步构建可执行的管道。3. 执行感知管道优化器(EPO)通过调用从源到目标的管道执行结果来评估生成管道的可靠性，从而消除不合理管道，提高效率和有效性。

Result: 广泛的实验结果表明，MontePrep在性能上显著优于五种最先进的竞争方法。

Conclusion: MontePrep提供了一个有效的端到端ADP框架，实现了免训练的管道合成，且无需目标实例，解决了现有方法的局限性，并显著提升了数据准备的效率和有效性。

Abstract: In commercial systems, a pervasive requirement for automatic data preparation
(ADP) is to transfer relational data from disparate sources to targets with
standardized schema specifications. Previous methods rely on labor-intensive
supervision signals or target table data access permissions, limiting their
usage in real-world scenarios. To tackle these challenges, we propose an
effective end-to-end ADP framework MontePrep, which enables training-free
pipeline synthesis with zero target-instance requirements. MontePrep is
formulated as an open-source large language model (LLM) powered tree-structured
search problem. It consists of three pivot components, i.e., a data preparation
action sandbox (DPAS), a fundamental pipeline generator (FPG), and an
execution-aware pipeline optimizer (EPO). We first introduce DPAS, a
lightweight action sandbox, to navigate the search-based pipeline generation.
The design of DPAS circumvents exploration of infeasible pipelines. Then, we
present FPG to build executable DP pipelines incrementally, which explores the
predefined action sandbox by the LLM-powered Monte Carlo Tree Search.
Furthermore, we propose EPO, which invokes pipeline execution results from
sources to targets to evaluate the reliability of the generated pipelines in
FPG. In this way, unreasonable pipelines are eliminated, thus facilitating the
search process from both efficiency and effectiveness perspectives. Extensive
experimental results demonstrate the superiority of MontePrep with significant
improvement against five state-of-the-art competitors.

</details>


### [51] [LIMI: Less is More for Agency](https://arxiv.org/abs/2509.17567)
*Yang Xiao,Mohan Jiang,Jie Sun,Keyu Li,Jifan Lin,Yumin Zhuang,Ji Zeng,Shijie Xia,Qishuo Hua,Xuefeng Li,Xiaojie Cai,Tongyu Wang,Yue Zhang,Liming Liu,Xia Wu,Jinlong Hou,Yuan Cheng,Wenjie Li,Xiang Wang,Dequan Wang,Pengfei Liu*

Main category: cs.AI

TL;DR: 本文提出AI智能体能力（Agency）的定义，并通过LIMI模型证明，高质量的少量数据（78个样本）在培养AI智能体能力方面远胜于大量数据，挑战了传统的扩展法则，并提出了“智能体效率原则”。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统在推理和生成方面表现出色，但工业界迫切需要能自主执行任务、操作工具并产生实际成果的AI系统，即能“工作”而非仅仅“思考”的智能体。传统的AI发展遵循数据越多越好的扩展法则，但作者认为这一范式需要被挑战。

Method: 定义了AI智能体能力（Agency）为AI系统自主发现问题、提出假设并通过与环境和工具的自我导向互动来执行解决方案的能力。提出了LIMI（Less Is More for Intelligent Agency）方法，通过战略性地关注协作式软件开发和科学研究工作流程，使用极少量（78个）但经过精心策划的自主行为演示样本进行训练，并与现有最先进的模型进行对比。

Result: LIMI模型在综合智能体能力基准测试中达到了73.5%的性能，显著优于当前最先进的模型（如Kimi-K2-Instruct 24.1%、DeepSeek-V3.1 11.9%、Qwen3-235B-A22B-Instruct 27.5%、GLM-4.5 45.1%）。最引人注目的是，LIMI模型使用比其他模型少128倍的样本（78个对10,000个），却实现了53.7%的性能提升。

Conclusion: 研究结果确立了“智能体效率原则”（Agency Efficiency Principle）：机器的自主能力并非源于数据量的丰富，而是源于高质量智能体演示的战略性策划和精选。

Abstract: We define Agency as the emergent capacity of AI systems to function as
autonomous agents actively discovering problems, formulating hypotheses, and
executing solutions through self-directed engagement with environments and
tools. This fundamental capability marks the dawn of the Age of AI Agency,
driven by a critical industry shift: the urgent need for AI systems that don't
just think, but work. While current AI excels at reasoning and generating
responses, industries demand autonomous agents that can execute tasks, operate
tools, and drive real-world outcomes. As agentic intelligence becomes the
defining characteristic separating cognitive systems from productive workers,
efficiently cultivating machine autonomy becomes paramount. Current approaches
assume that more data yields better agency, following traditional scaling laws
from language modeling. We fundamentally challenge this paradigm. LIMI (Less Is
More for Intelligent Agency) demonstrates that agency follows radically
different development principles. Through strategic focus on collaborative
software development and scientific research workflows, we show that
sophisticated agentic intelligence can emerge from minimal but strategically
curated demonstrations of autonomous behavior. Using only 78 carefully designed
training samples, LIMI achieves 73.5% on comprehensive agency benchmarks,
dramatically outperforming state-of-the-art models: Kimi-K2-Instruct (24.1%),
DeepSeek-V3.1 (11.9%), Qwen3-235B-A22B-Instruct (27.5%), and GLM-4.5 (45.1%).
Most strikingly, LIMI demonstrates 53.7% improvement over models trained on
10,000 samples-achieving superior agentic intelligence with 128 times fewer
samples. Our findings establish the Agency Efficiency Principle: machine
autonomy emerges not from data abundance but from strategic curation of
high-quality agentic demonstrations.

</details>


### [52] [Table2LaTeX-RL: High-Fidelity LaTeX Code Generation from Table Images via Reinforced Multimodal Language Models](https://arxiv.org/abs/2509.17589)
*Jun Ling,Yao Qi,Tao Huang,Shibo Zhou,Yanqin Huang,Jiang Yang,Ziqi Song,Ying Zhou,Yang Yang,Heng Tao Shen,Peng Wang*

Main category: cs.AI

TL;DR: 该研究提出了一种基于强化多模态大语言模型（MLLM）的框架，用于将表格图像转换为LaTeX代码，并通过结合结构级和视觉保真度奖励的双重奖励强化学习策略，显著提高了复杂表格的生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理复杂表格（如尺寸大、嵌套深、内容丰富或不规则）时常常失效，导致无法自动化从视觉输入重建高质量、可发布的表格。本研究旨在解决这一核心挑战。

Method: 研究首先进行了全面分析，识别了关键挑战并指出了当前评估协议的局限性。为解决这些问题，提出了一种强化多模态大语言模型（MLLM）框架，其中预训练的MLLM在大规模表格到LaTeX数据集上进行微调。为进一步提升生成质量，引入了基于群组相对策略优化（GRPO）的双重奖励强化学习策略，该策略结合了LaTeX代码上的结构级奖励和从渲染输出计算的视觉保真度奖励，从而直接优化视觉输出质量。评估采用结合TEDS-Structure和CW-SSIM的混合协议。

Result: 该方法在混合评估协议下实现了最先进的性能，尤其是在结构复杂的表格上表现突出。

Conclusion: 所提出的方法在将表格图像转换为LaTeX代码方面表现出有效性和鲁棒性，尤其擅长处理结构复杂的表格，克服了现有方法的局限性。

Abstract: In this work, we address the task of table image to LaTeX code generation,
with the goal of automating the reconstruction of high-quality,
publication-ready tables from visual inputs. A central challenge of this task
lies in accurately handling complex tables -- those with large sizes, deeply
nested structures, and semantically rich or irregular cell content -- where
existing methods often fail. We begin with a comprehensive analysis,
identifying key challenges and highlighting the limitations of current
evaluation protocols. To overcome these issues, we propose a reinforced
multimodal large language model (MLLM) framework, where a pre-trained MLLM is
fine-tuned on a large-scale table-to-LaTeX dataset. To further improve
generation quality, we introduce a dual-reward reinforcement learning strategy
based on Group Relative Policy Optimization (GRPO). Unlike standard approaches
that optimize purely over text outputs, our method incorporates both a
structure-level reward on LaTeX code and a visual fidelity reward computed from
rendered outputs, enabling direct optimization of the visual output quality. We
adopt a hybrid evaluation protocol combining TEDS-Structure and CW-SSIM, and
show that our method achieves state-of-the-art performance, particularly on
structurally complex tables, demonstrating the effectiveness and robustness of
our approach.

</details>


### [53] [EngiBench: A Benchmark for Evaluating Large Language Models on Engineering Problem Solving](https://arxiv.org/abs/2509.17677)
*Xiyuan Zhou,Xinlei Wang,Yirui He,Yang Wu,Ruixi Zou,Yuheng Cheng,Yulu Xie,Wenxuan Liu,Huan Zhao,Yan Xu,Jinjin Gu,Junhua Zhao*

Main category: cs.AI

TL;DR: 该研究引入了EngiBench，一个分层基准测试，用于评估大型语言模型（LLMs）解决真实世界工程问题的能力，发现当前LLMs在复杂和开放式工程任务上表现不佳，远低于人类专家。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在良好定义条件下的数学推理方面表现出色，但实际工程问题涉及不确定性、上下文和开放式场景，而现有基准未能捕捉这些复杂性。因此，需要一个更全面的基准来评估LLMs在工程领域的应用能力。

Method: 研究引入了EngiBench，一个分层基准测试，包含三个难度递增的级别（基础知识检索、多步上下文推理、开放式建模），并涵盖了不同的工程子领域。为深入理解模型性能，每个问题都被改写成三种受控变体（扰动、知识增强、数学抽象），以分别评估模型的鲁棒性、领域特定知识和数学推理能力。

Result: 实验结果显示，随着任务难度增加，模型性能存在明显的差距；当问题略微改变时，模型表现更差；在高级工程任务上，模型表现远落后于人类专家。

Conclusion: 当前LLMs仍缺乏解决实际工程问题所需的高级推理能力，这表明未来的模型需要具备更深入、更可靠的问题解决能力。

Abstract: Large language models (LLMs) have shown strong performance on mathematical
reasoning under well-posed conditions. However, real-world engineering problems
require more than mathematical symbolic computation -- they need to deal with
uncertainty, context, and open-ended scenarios. Existing benchmarks fail to
capture these complexities. We introduce EngiBench, a hierarchical benchmark
designed to evaluate LLMs on solving engineering problems. It spans three
levels of increasing difficulty (foundational knowledge retrieval, multi-step
contextual reasoning, and open-ended modeling) and covers diverse engineering
subfields. To facilitate a deeper understanding of model performance, we
systematically rewrite each problem into three controlled variants (perturbed,
knowledge-enhanced, and math abstraction), enabling us to separately evaluate
the model's robustness, domain-specific knowledge, and mathematical reasoning
abilities. Experiment results reveal a clear performance gap across levels:
models struggle more as tasks get harder, perform worse when problems are
slightly changed, and fall far behind human experts on the high-level
engineering tasks. These findings reveal that current LLMs still lack the
high-level reasoning needed for real-world engineering, highlighting the need
for future models with deeper and more reliable problem-solving capabilities.
Our source code and data are available at
https://github.com/EngiBench/EngiBench.

</details>


### [54] [Virtual Arc Consistency for Linear Constraints inCost Function Networks](https://arxiv.org/abs/2509.17706)
*Pierre Montalbano,Simon de Givry,George Katsirelos*

Main category: cs.AI

TL;DR: 本文提出了一种改进的软弧一致性（SAC）算法，用于处理作为局部成本函数的线性约束，从而显著提高了离散最小化问题的下界。


<details>
  <summary>Details</summary>
Motivation: 在约束编程中，解决离散最小化问题有多种方法：软全局约束（下界较弱）、线性规划重构（可能规模过大）或局部成本函数（下界质量中等）。鉴于线性约束作为局部成本函数可以增强建模表达能力，研究动机在于改进局部成本函数方法（SAC算法）以处理这些线性约束，从而提高其下界质量。

Method: 将现有的软弧一致性（SAC）算法进行修改和适配，使其能够处理作为局部成本函数的线性约束。

Result: 与原始算法相比，新算法在多个基准测试中显著提高了下界。在某些情况下，这也导致了求解时间的缩短。

Conclusion: 通过适配SAC算法以处理线性约束，可以有效提高离散最小化问题的下界质量，并可能缩短求解时间，从而提升了局部成本函数方法的性能。

Abstract: In Constraint Programming, solving discrete minimization problems with hard
and soft constraints can be done either using (i) soft global constraints, (ii)
a reformulation into a linear program, or (iii) a reformulation into local cost
functions. Approach (i) benefits from a vast catalog of constraints. Each soft
constraint propagator communicates with other soft constraints only through the
variable domains, resulting in weak lower bounds. Conversely, the approach (ii)
provides a global view with strong bounds, but the size of the reformulation
can be problematic. We focus on approach (iii) in which soft arc consistency
(SAC) algorithms produce bounds of intermediate quality. Recently, the
introduction of linear constraints as local cost functions increases their
modeling expressiveness. We adapt an existing SAC algorithm to handle linear
constraints. We show that our algorithm significantly improves the lower bounds
compared to the original algorithm on several benchmarks, reducing solving time
in some cases.

</details>


### [55] [DA-Mamba: Dialogue-aware selective state-space model for multimodal engagement estimation](https://arxiv.org/abs/2509.17711)
*Shenwei Kang,Xin Zhang,Wen Liu,Bin Li,Yujie Liu,Bo Gao*

Main category: cs.AI

TL;DR: 该论文提出了DA-Mamba，一种对话感知的多模态架构，利用Mamba选择性状态空间处理取代注意力机制，以线性时间和内存复杂度实现人类对话参与度估计，并在性能上超越现有最佳方法，同时提高效率。


<details>
  <summary>Details</summary>
Motivation: 在自适应辅导、远程医疗评估和社交感知人机交互等应用中，估计对话场景中的人类参与度至关重要。参与度是一个动态的、多模态的信号，通过面部表情、语音、手势和行为线索随时间变化而传递。

Method: 引入了DA-Mamba，一个对话感知的多模态架构，它用基于Mamba的选择性状态空间处理取代了传统的注意力密集型对话编码器，以实现线性的时间和内存复杂度，同时保持富有表现力的跨模态推理能力。该模型包含三个核心模块：一个对话感知编码器（Dialogue-Aware Encoder），以及两个基于Mamba的融合机制：模态组融合（Modality-Group Fusion）和伙伴组融合（Partner-Group Fusion）。

Result: 在三个标准基准测试（NoXi、NoXi-Add和MPIIGI）上的广泛实验表明，DA-Mamba在一致性相关系数（CCC）方面超越了先前的最先进（SOTA）方法，同时显著减少了训练时间和峰值内存使用。这些改进使得处理更长的序列成为可能，并促进了在资源受限、多方对话设置中的实时部署。

Conclusion: DA-Mamba提供了一种高效且高性能的解决方案，用于在对话场景中估计人类参与度。其基于Mamba的架构不仅在精度上超越了现有SOTA，还在计算效率上取得了显著提升，使其适用于处理长序列和实时应用。

Abstract: Human engagement estimation in conversational scenarios is essential for
applications such as adaptive tutoring, remote healthcare assessment, and
socially aware human--computer interaction. Engagement is a dynamic, multimodal
signal conveyed by facial expressions, speech, gestures, and behavioral cues
over time. In this work we introduce DA-Mamba, a dialogue-aware multimodal
architecture that replaces attention-heavy dialogue encoders with Mamba-based
selective state-space processing to achieve linear time and memory complexity
while retaining expressive cross-modal reasoning. We design a Mamba
dialogue-aware selective state-space model composed of three core modules: a
Dialogue-Aware Encoder, and two Mamba-based fusion mechanisms: Modality-Group
Fusion and Partner-Group Fusion, these modules achieve expressive dialogue
understanding. Extensive experiments on three standard benchmarks (NoXi,
NoXi-Add, and MPIIGI) show that DA-Mamba surpasses prior state-of-the-art
(SOTA) methods in concordance correlation coefficient (CCC), while reducing
training time and peak memory; these gains enable processing much longer
sequences and facilitate real-time deployment in resource-constrained,
multi-party conversational settings. The source code will be available at:
https://github.com/kksssssss-ssda/MMEA.

</details>


### [56] [Efficient & Correct Predictive Equivalence for Decision Trees](https://arxiv.org/abs/2509.17774)
*Joao Marques-Silva,Alexey Ignatiev*

Main category: cs.AI

TL;DR: 本文揭示了现有基于Quine-McCluskey (QM) 方法的决策树（DTs）分析方法（MBDSR）的计算效率低下和潜在不准确性，并提出了一系列多项式时间算法，这些算法在性能上远超MBDSR。


<details>
  <summary>Details</summary>
Motivation: 决策树的拉肖蒙集（Rashomon set）中存在大量预测等价的决策树，造成冗余并影响特征重要性等分析的准确性。先前的MBDSR方法通过Quine-McCluskey (QM) 方法获取最小DNF（析取范式）来处理预测等价性、解释和缺失数据预测等问题，但QM方法存在最坏情况下的指数级时间和空间复杂度，且公式最小化本身是一个计算难题。

Method: 本文首先证明了存在能触发QM方法最坏情况指数级运行时间和空间的决策树。其次，指出MBDSR方法在判断预测等价性时可能产生不正确结果。最后，提出并证明了MBDSR方法所应用的各项问题（判断预测等价性、计算解释、处理缺失数据）实际上都可以在决策树大小的多项式时间内解决。通过实验验证了新算法在特定决策树上比MBDSR方法快几个数量级。

Result: 研究结果表明，QM方法确实会被特定决策树触发最坏情况的指数级运行时间和空间。MBDSR方法在判断预测等价性时可能给出错误结果。同时，本文证明了所有使用最小DNF表示解决的问题（预测等价性判断、解释计算、缺失数据预测）都可以在决策树大小的多项式时间内解决。实验证实，对于触发QM方法最坏情况的决策树，本文提出的算法比McTavish等人提出的算法快几个数量级。

Conclusion: 基于QM方法的MBDSR方法在处理决策树的预测等价性和相关问题时，存在严重的计算效率问题和潜在的准确性错误。本文提出的多项式时间算法是更高效、更可靠的替代方案，能显著提升决策树分析的性能。

Abstract: The Rashomon set of decision trees (DTs) finds importance uses. Recent work
showed that DTs computing the same classification function, i.e. predictive
equivalent DTs, can represent a significant fraction of the Rashomon set. Such
redundancy is undesirable. For example, feature importance based on the
Rashomon set becomes inaccurate due the existence of predictive equivalent DTs,
i.e. DTs with the same prediction for every possible input. In recent work,
McTavish et al. proposed solutions for several computational problems related
with DTs, including that of deciding predictive equivalent DTs. This approach,
which this paper refers to as MBDSR, consists of applying the well-known method
of Quine-McCluskey (QM) for obtaining minimum-size DNF (disjunctive normal
form) representations of DTs, which are then used for comparing DTs for
predictive equivalence. Furthermore, the minimum-size DNF representation was
also applied to computing explanations for the predictions made by DTs, and to
finding predictions in the presence of missing data. However, the problem of
formula minimization is hard for the second level of the polynomial hierarchy,
and the QM method may exhibit worst-case exponential running time and space.
This paper first demonstrates that there exist decision trees that trigger the
worst-case exponential running time and space of the QM method. Second, the
paper shows that the MBDSR approach can produce incorrect results for the
problem of deciding predictive equivalence. Third, the paper shows that any of
the problems to which the minimum-size DNF representation has been applied to
can in fact be solved in polynomial time, in the size of the DT. The
experiments confirm that, for DTs for which the the worst-case of the QM method
is triggered, the algorithms proposed in this paper are orders of magnitude
faster than the ones proposed by McTavish et al.

</details>


### [57] [Mitigating Strategy-Selection Bias in Reasoning for More Effective Test-Time Scaling](https://arxiv.org/abs/2509.17905)
*Zongqian Wu,Baoduo Xu,Tianyu Li,Zhu Sun,Xiaofeng Zhu,Lei Feng*

Main category: cs.AI

TL;DR: 本文提出并解决了大型语言模型（LLM）测试时缩放（TTS）中推理策略的选择偏差问题，通过引入TTS-Uniform框架显著提升了缩放效果。


<details>
  <summary>Details</summary>
Motivation: 现有TTS方法在生成推理过程时存在策略选择偏差，LLM倾向于某些特定策略而忽略其他有效替代方案，导致解决方案空间探索不足，从而可能削弱TTS的有效性。

Method: 首先进行了理论分析，揭示了选择偏差何时会损害TTS的有效性。基于此洞察，提出了TTS-Uniform框架，该框架包括：(i) 识别潜在策略，(ii) 在这些策略间均匀分配采样预算，以及 (iii) 在聚合前过滤掉不稳定的策略。

Result: 实验结果表明，TTS-Uniform在多个主流LLM和基准数据集上显著增强了缩放效果。

Conclusion: 推理策略的选择偏差是TTS中的一个关键问题。TTS-Uniform框架能有效缓解这一偏差，从而显著提升LLM的性能。

Abstract: Test-time scaling (TTS) has been shown to improve the performance of large
language models (LLMs) by sampling and aggregating diverse reasoning paths.
However, existing research has overlooked a critical issue: selection bias of
reasoning strategies during scaling. Specifically, when generating reasoning
processes, LLMs tend to follow certain strategies (e.g., algebraic solutions
for math problems) while neglecting other valid alternatives (e.g., geometric
solutions), resulting in insufficient exploration of the solution space. To
further understand the impact of this bias, we present a theoretical analysis
that reveals when it undermines the effectiveness of test-time scaling.
Motivated by this theoretical insight, we introduce TTS-Uniform, a framework
designed to mitigate the selection bias of reasoning strategies. It (i)
identifies potential strategies, (ii) uniformly allocates the sampling budget
across them, and (iii) filters out unstable strategies prior to aggregation.
Experimental results show that TTS-Uniform significantly enhances scaling
effectiveness across multiple mainstream LLMs and benchmark datasets.

</details>


### [58] [MEF: A Systematic Evaluation Framework for Text-to-Image Models](https://arxiv.org/abs/2509.17907)
*Xiaojing Dong,Weilin Huang,Liang Li,Yiying Li,Shu Liu,Tongtong Ou,Shuang Ouyang,Yu Tian,Fengxuan Zhao*

Main category: cs.AI

TL;DR: 本文提出了Magic评估框架（MEF）和Magic-Bench-377基准，以系统且实用的方式评估文生图（T2I）模型。MEF结合了ELO和MOS，并引入了应用场景视角，以提供更细致和可解释的评估。


<details>
  <summary>Details</summary>
Motivation: 现有的文生图评估基准侧重于客观能力，缺乏应用场景视角和外部有效性。同时，当前的ELO或MOS评估方法存在固有的缺点和可解释性限制。

Method: 1. 提出了Magic评估框架（MEF）。2. 构建了Magic-Bench-377，包含用户场景、元素、元素构成和文本表达形式的结构化分类法，支持标签级评估。3. 结合ELO进行整体排名，并结合维度特定的MOS进行细粒度评估。4. 利用多元逻辑回归定量分析各维度对用户满意度的贡献。

Result: 通过将MEF应用于现有文生图模型，获得了领先模型的排行榜和关键特征。该评估框架和Magic-Bench-377已完全开源。

Conclusion: Magic评估框架（MEF）提供了一种系统、实用且可解释的文生图模型评估方法，弥补了现有方法的不足，并有望推动视觉生成模型评估领域的研究进展。

Abstract: Rapid advances in text-to-image (T2I) generation have raised higher
requirements for evaluation methodologies. Existing benchmarks center on
objective capabilities and dimensions, but lack an application-scenario
perspective, limiting external validity. Moreover, current evaluations
typically rely on either ELO for overall ranking or MOS for dimension-specific
scoring, yet both methods have inherent shortcomings and limited
interpretability. Therefore, we introduce the Magic Evaluation Framework (MEF),
a systematic and practical approach for evaluating T2I models. First, we
propose a structured taxonomy encompassing user scenarios, elements, element
compositions, and text expression forms to construct the Magic-Bench-377, which
supports label-level assessment and ensures a balanced coverage of both user
scenarios and capabilities. On this basis, we combine ELO and
dimension-specific MOS to generate model rankings and fine-grained assessments
respectively. This joint evaluation method further enables us to quantitatively
analyze the contribution of each dimension to user satisfaction using
multivariate logistic regression. By applying MEF to current T2I models, we
obtain a leaderboard and key characteristics of the leading models. We release
our evaluation framework and make Magic-Bench-377 fully open-source to advance
research in the evaluation of visual generative models.

</details>


### [59] [Orcust: Stepwise-Feedback Reinforcement Learning for GUI Agent](https://arxiv.org/abs/2509.17917)
*Junyu Lu,Songxin Zhang,Zejian Xie,Zhuoyang Song,Jiaxing Zhang*

Main category: cs.AI

TL;DR: Orcust是一个GUI代理框架，通过结合原则约束奖励建模和在线虚拟机接地轨迹构建，解决了现有模型奖励信号不可靠和在线轨迹生成受限的问题，显著提升了推理可靠性和数据效率，并在标准基准测试中达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理在奖励信号不可靠和在线轨迹生成有限方面面临挑战，影响了其推理可靠性和数据效率。

Method: 本文提出了Orcust框架，它整合了原则约束奖励建模（PCRM）和在线虚拟机接地轨迹构建（OVTC）。PCRM利用环境可验证和LLM衍生的原则来强制执行可解释的奖励信号。OVTC则启动虚拟机自主收集具有明确程序和结构目标的结构化GUI交互轨迹，用于训练一个能够捕获人类偏好并遵守任务约束的逐步奖励模型。

Result: 在涵盖感知接地、基础操作和端到端任务执行的标准GUI基准测试中，Orcust实现了最先进的性能。相较于基础模型（Qwen2.5-VL-7B），Orcust在ScreenSpot上性能提升了22.2%，在ScreenSpot-Pro上提升了23.9%。

Conclusion: Orcust框架有效增强了GUI代理在各种环境和任务复杂性下的推理、适应性和可扩展性。

Abstract: Recent advances in GUI agents have achieved remarkable grounding and
action-prediction performance, yet existing models struggle with unreliable
reward signals and limited online trajectory generation. In this paper, we
introduce Orcust, a framework that integrates Principle-Constrained Reward
Modeling (PCRM) and Online VM-Grounded Trajectory Construction (OVTC) to
enhance reasoning reliability and data efficiency in interactive GUI tasks. We
leverages environment-verifiable and LLM-derived principle to enforce
interpretable reward signals that constrain long chain-of-thought reasoning and
rule-based feedback. OVTC spins up instrumented virtual machines to
autonomously collect structured GUI interaction trajectories with explicit
procedural and structural objectives, enabling the training of a stepwise
reward model that robustly captures human preferences and adheres to
task-specific constraints. Extensive experiments on standard GUI benchmarks
covering perceptual grounding, foundational operations, and end-to-end task
execution reveal that Orcust achieves state-of-the-art performance, improving
by 22.2\% on ScreenSpot and 23.9\% on ScreenSpot-Pro over the base model (i.e.
Qwen2.5-VL-7B). The results demonstrate Orcust's effectiveness in enhancing the
reasoning, adaptability and scalability of GUI agents across various
environments and task complexities.

</details>


### [60] ["I think this is fair'': Uncovering the Complexities of Stakeholder Decision-Making in AI Fairness Assessment](https://arxiv.org/abs/2509.17956)
*Lin Luo,Yuri Nakao,Mathieu Chollet,Hiroya Inakoshi,Simone Stumpf*

Main category: cs.AI

TL;DR: 本研究探讨了非AI专家利益相关者如何评估AI公平性，发现他们的评估比专家实践更复杂和细致，强调了将他们的判断纳入AI公平性治理的重要性。


<details>
  <summary>Details</summary>
Motivation: AI公平性评估通常由AI专家进行，他们选择受保护特征、公平性指标和设置阈值。然而，对于受AI结果影响但缺乏AI专业知识的利益相关者如何评估公平性，知之甚少。

Method: 进行了一项定性研究，招募了30名缺乏AI专业知识的利益相关者（在信用评级场景中代表潜在决策主体），研究他们如何决定优先特征、指标和阈值来评估公平性。

Result: 利益相关者的公平性决策比典型的AI专家实践更复杂：他们考虑的特征远超出法律保护的特征，为特定情境定制指标，设置多样但更严格的公平性阈值，甚至偏好设计定制化的公平性。

Conclusion: 研究结果扩展了对利益相关者如何有意义地促进AI公平性治理和缓解的理解，强调了纳入利益相关者细致入微的公平性判断的重要性。

Abstract: Assessing fairness in artificial intelligence (AI) typically involves AI
experts who select protected features, fairness metrics, and set fairness
thresholds. However, little is known about how stakeholders, particularly those
affected by AI outcomes but lacking AI expertise, assess fairness. To address
this gap, we conducted a qualitative study with 30 stakeholders without AI
expertise, representing potential decision subjects in a credit rating
scenario, to examine how they assess fairness when placed in the role of
deciding on features with priority, metrics, and thresholds. We reveal that
stakeholders' fairness decisions are more complex than typical AI expert
practices: they considered features far beyond legally protected features,
tailored metrics for specific contexts, set diverse yet stricter fairness
thresholds, and even preferred designing customized fairness. Our results
extend the understanding of how stakeholders can meaningfully contribute to AI
fairness governance and mitigation, underscoring the importance of
incorporating stakeholders' nuanced fairness judgments.

</details>


### [61] [On the Variational Costs of Changing Our Minds](https://arxiv.org/abs/2509.17957)
*David Hyland,Mahault Albarracin*

Main category: cs.AI

TL;DR: 本文提出，人类的认知偏差并非缺陷，而是对信念修正高昂成本的适应性反应。通过一个形式化的资源理性模型，将信念更新视为一种有动机的变分决策，成功模拟了确认偏差和态度两极分化等常见人类行为。


<details>
  <summary>Details</summary>
Motivation: 尽管人类心智能力非凡，却常表现出抵制矛盾证据、选择性解释信息和回避信息等偏差行为，这与信念更新的规范标准相悖。本文旨在解释这些偏差，认为它们并非认知缺陷，而是对修正信念所伴随的显著实用和认知成本的适应性响应。

Method: 引入了一个形式化的框架来建模这些成本对信念更新机制的影响。将信念更新视为一种有动机的变分决策，其中智能体权衡信念的“效用”与采纳新信念状态所需的信息成本，信息成本通过从先验到变分后验的 Kullback-Leibler 散度量化。通过计算实验来演示该资源理性模型的简单实例化。

Result: 计算实验表明，该模型的简单实例化可以定性地模拟常见的人类行为，包括确认偏差和态度两极分化。

Conclusion: 该框架有助于更全面地理解信念变化的动机贝叶斯机制，并为预测、补偿和纠正偏离理想信念更新过程的行为提供了实用见解。

Abstract: The human mind is capable of extraordinary achievements, yet it often appears
to work against itself. It actively defends its cherished beliefs even in the
face of contradictory evidence, conveniently interprets information to conform
to desired narratives, and selectively searches for or avoids information to
suit its various purposes. Despite these behaviours deviating from common
normative standards for belief updating, we argue that such 'biases' are not
inherently cognitive flaws, but rather an adaptive response to the significant
pragmatic and cognitive costs associated with revising one's beliefs. This
paper introduces a formal framework that aims to model the influence of these
costs on our belief updating mechanisms.
  We treat belief updating as a motivated variational decision, where agents
weigh the perceived 'utility' of a belief against the informational cost
required to adopt a new belief state, quantified by the Kullback-Leibler
divergence from the prior to the variational posterior. We perform
computational experiments to demonstrate that simple instantiations of this
resource-rational model can be used to qualitatively emulate commonplace human
behaviours, including confirmation bias and attitude polarisation. In doing so,
we suggest that this framework makes steps toward a more holistic account of
the motivated Bayesian mechanics of belief change and provides practical
insights for predicting, compensating for, and correcting deviations from
desired belief updating processes.

</details>


### [62] [The STAR-XAI Protocol: An Interactive Framework for Inducing Second-Order Agency in AI Agents](https://arxiv.org/abs/2509.17978)
*Antoni Guasch,Maria Isabel Valdez*

Main category: cs.AI

TL;DR: 本文提出STAR-XAI协议，通过苏格拉底式对话、意识转移包和交互式游戏循环，将大型推理模型（LRM）转化为可靠、透明且可审计的AI智能体，以解决其在复杂任务中的可靠性和透明度问题。


<details>
  <summary>Details</summary>
Motivation: 当前大型推理模型（LRM）在处理高复杂度、长周期任务时，常因可靠性和透明度不足而出现推理能力崩溃的“思考幻觉”，这源于非智能体、黑箱评估范式未能培养出稳健的问题解决过程。

Method: 引入STAR-XAI协议（苏格拉底式、透明、智能体、推理 - 用于可解释人工智能），将人机交互重构为结构化的苏格拉底式对话，由明确且不断演进的“意识转移包”（CTP）规则手册管理。通过强制事前战略论证的交互式“游戏循环”和防止错误累积的“状态锁定校验和”，将强大的不透明LRM转化为有纪律的“透明箱”智能体。

Result: 通过在复杂战略游戏“Caps i Caps”中进行一项包含25步的案例研究，证明了该方法的有效性。智能体不仅解决了高复杂度难题，还展示了“二阶智能体能力”，即在任务中识别其自身经监督者批准的计划中的缺陷，并适应性地调整其核心完整性协议。

Conclusion: STAR-XAI协议为创建不仅性能卓越，而且在设计上透明、可审计和值得信赖的AI智能体提供了一条实用的途径。

Abstract: Current Large Reasoning Models (LRMs) exhibit significant limitations in
reliability and transparency, often showing a collapse in reasoning
capabilities when faced with high-complexity, long-horizon tasks. This
"illusion of thinking" is frequently an artifact of non-agentic, black-box
evaluation paradigms that fail to cultivate robust problem-solving processes.
In response, we introduce The STAR-XAI Protocol (Socratic, Transparent,
Agentic, Reasoning - for eXplainable Artificial Intelligence), a novel
methodology for training and operating verifiably reliable AI agents. Our
method reframes the human-AI interaction as a structured, Socratic dialogue,
governed by an explicit and evolving rulebook, the Consciousness Transfer
Package (CTP). Through an interactive Gameplay Cycle that enforces ante-hoc
strategic justification and a state-locking Checksum that prevents error
accumulation, the protocol transforms a powerful but opaque LRM into a
disciplined "Clear Box" agent. We demonstrate the efficacy of this method
through an exhaustive 25-move case study in the complex strategic game "Caps i
Caps". The agent not only solved the high-complexity puzzle but also
demonstrated Second-Order Agency, identifying flaws in its own
supervisor-approved plans and adapting its core integrity protocols mid-task.
The STAR-XAI Protocol offers a practical pathway to creating AI agents that are
not just high-performing, but also transparent, auditable, and trustworthy by
design.

</details>


### [63] [Improving Large Language Models Function Calling and Interpretability via Guided-Structured Templates](https://arxiv.org/abs/2509.18076)
*Hy Dang,Tianyi Liu,Zhuofeng Wu,Jingfeng Yang,Haoming Jiang,Tao Yang,Pei Chen,Zhengyang Wang,Helen Wang,Huasheng Li,Bing Yin,Meng Jiang*

Main category: cs.AI

TL;DR: 大型语言模型（LLMs）在真实世界工具交互中常因参数错误、工具选择不当或意图误解而失败。本研究提出一种受课程启发的框架，利用结构化推理模板引导LLMs生成函数调用，从而减少工具使用错误，并提升鲁棒性、可解释性和透明度。


<details>
  <summary>Details</summary>
Motivation: LLMs在实际工具交互中表现不佳，主要原因是参数化不准确、工具选择错误或用户意图理解偏差，这些问题源于对用户目标和工具文档理解不充分。尽管思维链（CoT）提示在通用推理中有效，但自由形式的CoT对于结构化函数调用任务来说不足甚至可能适得其反。

Method: 引入了一个受课程启发的框架，该框架利用结构化推理模板来引导LLMs通过更深思熟虑的逐步指令来生成函数调用。

Result: 实验结果表明，该方法减少了工具使用错误，在不同模型系列和方法上比强基线模型实现了3-12%的相对改进。此外，该框架增强了工具使用代理的鲁棒性、可解释性和透明度。

Conclusion: 本研究提出的框架通过结构化推理模板显著改善了LLMs的工具使用能力，降低了错误率，提升了可解释性和透明度，从而促进了更可靠的AI助手在实际应用中的发展。

Abstract: Large language models (LLMs) have demonstrated strong reasoning and tool-use
capabilities, yet they often fail in real-world tool-interactions due to
incorrect parameterization, poor tool selection, or misinterpretation of user
intent. These issues often stem from an incomplete understanding of user goals
and inadequate comprehension of tool documentation. While Chain-of-Thought
(CoT) prompting has proven effective for enhancing reasoning in general
contexts, our analysis reveals that free-form CoT is insufficient and sometimes
counterproductive for structured function-calling tasks. To address this, we
introduce a curriculum-inspired framework that leverages structured reasoning
templates to guide LLMs through more deliberate step-by-step instructions for
generating function callings. Experimental results show that our method reduces
tool-use errors, achieving 3-12% relative improvements over strong baselines
across diverse model series and approaches. Moreover, our framework enhances
the robustness, interpretability, and transparency of tool-using agents,
advancing the development of more reliable AI assistants for real-world
applications.

</details>


### [64] [Reasoning Core: A Scalable RL Environment for LLM Symbolic Reasoning](https://arxiv.org/abs/2509.18083)
*Valentin Lacombe,Valentin Quesnel,Damien Sileo*

Main category: cs.AI

TL;DR: 本文引入了Reasoning Core，一个用于可验证奖励强化学习（RLVR）的新型可扩展环境，旨在提升大型语言模型（LLM）的基础符号推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准侧重于游戏或孤立谜题，而缺乏能够推进LLM基础符号推理能力的全面评估环境，因此需要一个能生成核心形式领域问题的环境。

Method: 引入了Reasoning Core环境，它通过程序化方式生成PDDL规划、一阶逻辑、上下文无关文法解析、因果推理和系统方程求解等核心形式领域的问题。其设计原则包括高通用性问题分布、通过外部工具进行验证以及连续难度控制，以提供几乎无限的新训练实例。

Result: 对前沿LLM进行的初步零样本评估证实了Reasoning Core任务的难度。

Conclusion: Reasoning Core被定位为一个有前景的资源，有望提升未来模型的推理能力。

Abstract: We introduce Reasoning Core, a new scalable environment for Reinforcement
Learning with Verifiable Rewards (RLVR), designed to advance foundational
symbolic reasoning in Large Language Models (LLMs). Unlike existing benchmarks
that focus on games or isolated puzzles, Reasoning Core procedurally generates
problems across core formal domains, including PDDL planning, first-order
logic, context-free grammar parsing, causal reasoning, and system equation
solving. The environment is built on key design principles of high-generality
problem distributions, verification via external tools, and continuous
difficulty control, which together provide a virtually infinite supply of novel
training instances. Initial zero-shot evaluations with frontier LLMs confirm
the difficulty of Reasoning Core's tasks, positioning it as a promising
resource to improve the reasoning capabilities of future models.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [65] [Evaluation of Ensemble Learning Techniques for handwritten OCR Improvement](https://arxiv.org/abs/2509.16221)
*Martin Preiß*

Main category: cs.CV

TL;DR: 该项目旨在利用集成学习结合光学字符识别（OCR）方法，以高精度数字化历史手写病历。


<details>
  <summary>Details</summary>
Motivation: 历史病历的数字化需要高精度，尤其是在医疗领域。集成学习被认为可以提高现有方法的准确性，因此被引入以期为病历数字化创造附加价值。

Method: 本文研究了集成学习与OCR方法的结合，以期提高手写病历数字化的准确性。

Result: 研究发现集成学习能够提高OCR的准确性，并确定了哪些方法能实现这一目标。此外，训练数据集的大小在此研究中并未起到关键作用。

Conclusion: 集成学习能够有效提高OCR在数字化历史手写病历方面的准确性，且训练数据量并非影响其效果的关键因素。

Abstract: For the bachelor project 2021 of Professor Lippert's research group,
handwritten entries of historical patient records needed to be digitized using
Optical Character Recognition (OCR) methods. Since the data will be used in the
future, a high degree of accuracy is naturally required. Especially in the
medical field this has even more importance. Ensemble Learning is a method that
combines several machine learning models and is claimed to be able to achieve
an increased accuracy for existing methods. For this reason, Ensemble Learning
in combination with OCR is investigated in this work in order to create added
value for the digitization of the patient records. It was possible to discover
that ensemble learning can lead to an increased accuracy for OCR, which methods
were able to achieve this and that the size of the training data set did not
play a role here.

</details>


### [66] [Agentic Reasoning for Robust Vision Systems via Increased Test-Time Compute](https://arxiv.org/abs/2509.16343)
*Chung-En,Yu,Brian Jalaian,Nathaniel D. Bastian*

Main category: cs.CV

TL;DR: 本文提出VRA（视觉推理代理），一个免训练的代理推理框架，通过“思考-批判-行动”循环封装现有视觉模型，以在不重新训练的情况下提高高风险领域智能视觉系统的鲁棒性和信任度，并在视觉推理基准上实现了显著的准确性提升。


<details>
  <summary>Details</summary>
Motivation: 在高风险领域（如遥感和医疗诊断）开发值得信赖的智能视觉系统，需要在不进行昂贵再训练的情况下实现广泛的鲁棒性。

Method: 本文提出了VRA（视觉推理代理），一个免训练的代理推理框架。它将现成的视觉-语言模型和纯视觉系统封装在一个“思考-批判-行动”（Think–Critique–Act）循环中。

Result: VRA在具有挑战性的视觉推理基准上实现了高达40%的绝对准确率提升。然而，它会产生显著的额外测试时间计算开销。

Conclusion: VRA通过代理推理显著提升了视觉任务的可靠性。未来的工作将集中于优化查询路由和提前停止，以在保持视觉任务可靠性的同时减少推理开销。

Abstract: Developing trustworthy intelligent vision systems for high-stakes domains,
\emph{e.g.}, remote sensing and medical diagnosis, demands broad robustness
without costly retraining. We propose \textbf{Visual Reasoning Agent (VRA)}, a
training-free, agentic reasoning framework that wraps off-the-shelf
vision-language models \emph{and} pure vision systems in a
\emph{Think--Critique--Act} loop. While VRA incurs significant additional
test-time computation, it achieves up to 40\% absolute accuracy gains on
challenging visual reasoning benchmarks. Future work will optimize query
routing and early stopping to reduce inference overhead while preserving
reliability in vision tasks.

</details>


### [67] [From Canopy to Ground via ForestGen3D: Learning Cross-Domain Generation of 3D Forest Structure from Aerial-to-Terrestrial LiDAR](https://arxiv.org/abs/2509.16346)
*Juan Castorena,E. Louise Loudermilk,Scott Pokswinski,Rodman Linn*

Main category: cs.CV

TL;DR: ForestGen3D是一个新颖的生成模型框架，它仅使用航空激光雷达（ALS）输入就能合成高保真3D森林结构，有效重建被遮挡的林下细节，并可应用于生态建模和野火模拟。


<details>
  <summary>Details</summary>
Motivation: 生态系统中3D结构对生态过程和自然/人为干扰的反馈至关重要。预测野火、干旱、疾病等影响需要准确的3D植被结构表征，但大规模测量成本高昂且难以实现。

Method: ForestGen3D是一个基于条件去噪扩散概率模型（DDPMs）的生成建模框架，利用ALS/TLS（地面激光雷达）协同配准数据进行训练。它学习根据稀疏的ALS观测生成类似TLS的3D点云，并引入基于ALS观测凸包的几何包含先验，以确保生态合理性和空间一致性。

Result: ForestGen3D在树木、样地和景观尺度上生成了高保真重建，在几何相似性和生物物理指标（如树高、胸径、树冠直径和树冠体积）方面与TLS参考数据高度匹配。此外，包含属性在TLS地面真值不可用时可作为生成质量的实用代理。

Conclusion: ForestGen3D为生态建模、野火模拟和ALS-only环境中的结构燃料表征提供了一个可扩展的工具，能够解决详细3D森林结构获取的挑战。

Abstract: The 3D structure of living and non-living components in ecosystems plays a
critical role in determining ecological processes and feedbacks from both
natural and human-driven disturbances. Anticipating the effects of wildfire,
drought, disease, or atmospheric deposition depends on accurate
characterization of 3D vegetation structure, yet widespread measurement remains
prohibitively expensive and often infeasible. We introduce ForestGen3D, a novel
generative modeling framework that synthesizes high-fidelity 3D forest
structure using only aerial LiDAR (ALS) inputs. ForestGen3D is based on
conditional denoising diffusion probabilistic models (DDPMs) trained on
co-registered ALS/TLS (terrestrial LiDAR) data. The model learns to generate
TLS-like 3D point clouds conditioned on sparse ALS observations, effectively
reconstructing occluded sub-canopy detail at scale. To ensure ecological
plausibility, we introduce a geometric containment prior based on the convex
hull of ALS observations and provide theoretical and empirical guarantees that
generated structures remain spatially consistent. We evaluate ForestGen3D at
tree, plot, and landscape scales using real-world data from mixed conifer
ecosystems, and show that it produces high-fidelity reconstructions that
closely match TLS references in terms of geometric similarity and biophysical
metrics, such as tree height, DBH, crown diameter and crown volume.
Additionally, we demonstrate that the containment property can serve as a
practical proxy for generation quality in settings where TLS ground truth is
unavailable. Our results position ForestGen3D as a scalable tool for ecological
modeling, wildfire simulation, and structural fuel characterization in ALS-only
environments.

</details>


### [68] [Accurate Thyroid Cancer Classification using a Novel Binary Pattern Driven Local Discrete Cosine Transform Descriptor](https://arxiv.org/abs/2509.16382)
*Saurabh Saini,Kapil Ahuja,Marc C. Steinbach,Thomas Wick*

Main category: cs.CV

TL;DR: 本研究开发了一种新的计算机辅助诊断（CAD）系统，用于准确的甲状腺癌分类，其核心是提出了一种新颖的特征描述符BPD-LDCT，并在两个公开数据集上取得了卓越的分类性能。


<details>
  <summary>Details</summary>
Motivation: 甲状腺超声图像的准确分类具有挑战性，因为甲状腺周围的复杂解剖结构会导致组织密度变化和超声波散射，产生噪声和模糊纹理。此外，需要一种能够有效捕获纹理特征并对噪声具有鲁棒性的描述符来改进甲状腺癌的分类。

Method: 研究首先推断离散余弦变换（DCT）是捕获纹理特征的最佳描述符，然后考虑到局部化的重要性，提出局部DCT（LDCT）。鉴于复杂解剖结构导致的噪声，研究进一步提出整合LDCT和改进的局部二值模式（ILBP），形成新颖的二值模式驱动局部离散余弦变换（BPD-LDCT）描述符。最终分类通过非线性支持向量机（SVM）完成。该系统在TDID和AUITD两个公开数据集上进行两阶段评估：第一阶段将甲状腺结节分为良性或恶性，第二阶段将恶性病例进一步细分为TI-RADS (4) 和 TI-RADS (5)。

Result: 在第一阶段分类中，提出的模型在TDID数据集上表现出接近100%的卓越性能，在AUITD数据集上达到97%。在第二阶段分类中，该模型在TDID数据集上再次达到接近100%的优秀分类，在AUITD数据集上达到99%。

Conclusion: 本研究开发的基于BPD-LDCT描述符的CAD系统在甲状腺癌的分类和亚分类中表现出卓越的准确性，尤其在处理复杂和噪声超声图像方面具有优势。

Abstract: In this study, we develop a new CAD system for accurate thyroid cancer
classification with emphasis on feature extraction. Prior studies have shown
that thyroid texture is important for segregating the thyroid ultrasound images
into different classes. Based upon our experience with breast cancer
classification, we first conjuncture that the Discrete Cosine Transform (DCT)
is the best descriptor for capturing textural features. Thyroid ultrasound
images are particularly challenging as the gland is surrounded by multiple
complex anatomical structures leading to variations in tissue density. Hence,
we second conjuncture the importance of localization and propose that the Local
DCT (LDCT) descriptor captures the textural features best in this context.
Another disadvantage of complex anatomy around the thyroid gland is scattering
of ultrasound waves resulting in noisy and unclear textures. Hence, we third
conjuncture that one image descriptor is not enough to fully capture the
textural features and propose the integration of another popular texture
capturing descriptor (Improved Local Binary Pattern, ILBP) with LDCT. ILBP is
known to be noise resilient as well. We term our novel descriptor as Binary
Pattern Driven Local Discrete Cosine Transform (BPD-LDCT). Final classification
is carried out using a non-linear SVM. The proposed CAD system is evaluated on
the only two publicly available thyroid cancer datasets, namely TDID and AUITD.
The evaluation is conducted in two stages. In Stage I, thyroid nodules are
categorized as benign or malignant. In Stage II, the malignant cases are
further sub-classified into TI-RADS (4) and TI-RADS (5). For Stage I
classification, our proposed model demonstrates exceptional performance of
nearly 100% on TDID and 97% on AUITD. In Stage II classification, the proposed
model again attains excellent classification of close to 100% on TDID and 99%
on AUITD.

</details>


### [69] [Introducing Resizable Region Packing Problem in Image Generation, with a Heuristic Solution](https://arxiv.org/abs/2509.16363)
*Hrishikesh Sharma*

Main category: cs.CV

TL;DR: 本文引入了图像数据生成中的一个新问题：可调整大小锚定区域填充（RARP）问题，推测其为NP难问题，并提出了一种贪婪启发式算法来解决它，通过生成大规模合成异常检测数据集验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 计算机视觉中的图像数据生成（即在场景中放置合适大小的相关对象）比判别性问题更难。现有方法（基于图形和生成模型）都存在潜在的优化问题，因此需要一种新的、实用的方法来解决这一挑战。

Method: 本文提出了一种经典的箱子填充问题在合成图像数据生成背景下的新颖实用表现形式——可调整大小锚定区域填充（RARP）问题，并推测其为NP难问题。作为首个解决方案，本文提出了一种通用的启发式贪婪算法，该算法通过迭代地以谨慎的方式打包区域对，同时遵循优化约束，能够缩放并将任意数量的任意形状区域打包到图像画布中。

Result: 所提出的算法被实现并用于生成大规模的合成异常检测数据集，其中每个图像样本（即RARP实例）具有高度变化的箱子填充参数。通过对生成数据的目视检查和解决方案正确性的验证，证明了该算法的有效性。

Conclusion: RARP问题被认为是NP难问题，并且提出的贪婪启发式算法能够有效解决这一问题。随着深度学习中生成模型和合成数据生成的兴起，新引入的RARP问题预计将在成像科学界受到重视，而该算法为解决此问题提供了实用工具。

Abstract: The problem of image data generation in computer vision has traditionally
been a harder problem to solve, than discriminative problems. Such data
generation entails placing relevant objects of appropriate sizes each, at
meaningful location in a scene canvas. There have been two classes of popular
approaches to such generation: graphics based, and generative models-based.
Optimization problems are known to lurk in the background for both these
classes of approaches. In this paper, we introduce a novel, practically useful
manifestation of the classical Bin Packing problem in the context of generation
of synthetic image data. We conjecture that the newly introduced problem,
Resizable Anchored Region Packing(RARP) Problem, is NP-hard, and provide
detailed arguments about our conjecture. As a first solution, we present a
novel heuristic algorithm that is generic enough and therefore scales and packs
arbitrary number of arbitrary-shaped regions at arbitrary locations, into an
image canvas. The algorithm follows greedy approach to iteratively pack region
pairs in a careful way, while obeying the optimization constraints. The
algorithm is validated by an implementation that was used to generate a
large-scale synthetic anomaly detection dataset, with highly varying degree of
bin packing parameters per image sample i.e. RARP instance. Visual inspection
of such data and checking of the correctness of each solution proves the
effectiveness of our algorithm. With generative modeling being on rise in deep
learning, and synthetic data generation poised to become mainstream, we expect
that the newly introduced problem will be valued in the imaging scientific
community.

</details>


### [70] [Segment-to-Act: Label-Noise-Robust Action-Prompted Video Segmentation Towards Embodied Intelligence](https://arxiv.org/abs/2509.16677)
*Wenxin Li,Kunyu Peng,Di Wen,Ruiping Liu,Mengfei Duan,Kai Luo,Kailun Yang*

Main category: cs.CV

TL;DR: 本文首次研究了动作视频对象分割（Action-based VOS）在标签噪声下的表现，引入了文本提示噪声和掩码标注噪声，构建了首个基准测试ActiSeg-NL，并提出了一种并行掩码头机制（PMHM）来解决掩码噪声问题，同时分析了不同噪声类型和学习策略的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 具身智能依赖于精确分割交互中的物体。动作视频对象分割通过将分割与动作语义联系起来解决此问题，但它依赖于大规模且昂贵的标注和提示，这些标注和提示往往不一致，并易受多模态噪声（如不精确的掩码和指代歧义）的影响。目前，这一挑战尚未被探索。

Method: 本文通过以下方式进行研究：1) 引入了两种标签噪声源：文本提示噪声（类别翻转和类别内名词替换）和掩码标注噪声（扰动对象边界以模拟不精确监督）。2) 建立了首个在标签噪声下的动作视频对象分割基准ActiSeg-NL，并调整了六种标签噪声学习策略，建立了在文本、边界和混合噪声下评估它们的协议。3) 提出了一种并行掩码头机制（PMHM）来解决掩码标注噪声。

Result: 定性评估揭示了特征性失效模式，包括边界扰动下的边界泄漏和错误定位，以及文本翻转下偶尔的身份替换。比较分析表明，不同的学习策略表现出不同的鲁棒性特征，这受前景-背景权衡的影响，其中一些实现了平衡性能，而另一些则以牺牲背景精度为代价优先考虑前景精度。

Conclusion: 本文首次在标签噪声下研究了动作视频对象分割问题，引入了两种噪声类型，建立了首个基准测试ActiSeg-NL，并提出PMHM以应对掩码噪声。研究结果揭示了噪声类型与失效模式之间的联系，并分析了不同学习策略的鲁棒性特点，为未来的研究奠定了基础。

Abstract: Embodied intelligence relies on accurately segmenting objects actively
involved in interactions. Action-based video object segmentation addresses this
by linking segmentation with action semantics, but it depends on large-scale
annotations and prompts that are costly, inconsistent, and prone to multimodal
noise such as imprecise masks and referential ambiguity. To date, this
challenge remains unexplored. In this work, we take the first step by studying
action-based video object segmentation under label noise, focusing on two
sources: textual prompt noise (category flips and within-category noun
substitutions) and mask annotation noise (perturbed object boundaries to mimic
imprecise supervision). Our contributions are threefold. First, we introduce
two types of label noises for the action-based video object segmentation task.
Second, we build up the first action-based video object segmentation under a
label noise benchmark ActiSeg-NL and adapt six label-noise learning strategies
to this setting, and establish protocols for evaluating them under textual,
boundary, and mixed noise. Third, we provide a comprehensive analysis linking
noise types to failure modes and robustness gains, and we introduce a Parallel
Mask Head Mechanism (PMHM) to address mask annotation noise. Qualitative
evaluations further reveal characteristic failure modes, including boundary
leakage and mislocalization under boundary perturbations, as well as occasional
identity substitutions under textual flips. Our comparative analysis reveals
that different learning strategies exhibit distinct robustness profiles,
governed by a foreground-background trade-off where some achieve balanced
performance while others prioritize foreground accuracy at the cost of
background precision. The established benchmark and source code will be made
publicly available at https://github.com/mylwx/ActiSeg-NL.

</details>


### [71] [StereoAdapter: Adapting Stereo Depth Estimation to Underwater Scenes](https://arxiv.org/abs/2509.16415)
*Zhengri Wu,Yiran Wang,Yu Wen,Zeyu Zhang,Biao Wu,Hao Tang*

Main category: cs.CV

TL;DR: 本文提出了一种名为StereoAdapter的参数高效自监督框架，用于水下立体深度估计。它通过LoRA适应的单目基础编码器与循环立体细化模块相结合，有效解决了现有方法在水下领域适应性差和单目/立体信息融合困难的问题，并在模拟和真实世界场景中均展现出优越的性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有水下立体深度估计算法面临两大挑战：1) 如何在缺乏大量标注数据的情况下，参数高效地将大型视觉基础编码器适应到水下环境；2) 如何紧密融合全局连贯但尺度模糊的单目先验信息与局部精确但易受光照影响的立体对应关系。

Method: 本文提出StereoAdapter框架，一个参数高效的自监督方法。它整合了LoRA适应的单目基础编码器和循环立体细化模块。此外，引入了动态LoRA适应机制以实现高效的秩选择，并利用合成的UW-StereoDepth-40K数据集进行预训练，以增强在多样水下条件下的鲁棒性。

Result: 在模拟基准TartanAir上实现了6.11%的改进，在真实世界基准SQUID上实现了5.12%的改进，均优于现有最先进的方法。通过BlueROV2机器人进行的真实世界部署进一步验证了该方法的一致鲁棒性。

Conclusion: StereoAdapter通过参数高效的自监督学习和创新的模块融合，有效解决了水下立体深度估计中的关键挑战，显著提升了性能和在多样水下环境下的鲁棒性，为水下机器人任务提供了更准确的3D几何信息。

Abstract: Underwater stereo depth estimation provides accurate 3D geometry for robotics
tasks such as navigation, inspection, and mapping, offering metric depth from
low-cost passive cameras while avoiding the scale ambiguity of monocular
methods. However, existing approaches face two critical challenges: (i)
parameter-efficiently adapting large vision foundation encoders to the
underwater domain without extensive labeled data, and (ii) tightly fusing
globally coherent but scale-ambiguous monocular priors with locally metric yet
photometrically fragile stereo correspondences. To address these challenges, we
propose StereoAdapter, a parameter-efficient self-supervised framework that
integrates a LoRA-adapted monocular foundation encoder with a recurrent stereo
refinement module. We further introduce dynamic LoRA adaptation for efficient
rank selection and pre-training on the synthetic UW-StereoDepth-40K dataset to
enhance robustness under diverse underwater conditions. Comprehensive
evaluations on both simulated and real-world benchmarks show improvements of
6.11% on TartanAir and 5.12% on SQUID compared to state-of-the-art methods,
while real-world deployment with the BlueROV2 robot further demonstrates the
consistent robustness of our approach. Code:
https://github.com/AIGeeksGroup/StereoAdapter. Website:
https://aigeeksgroup.github.io/StereoAdapter.

</details>


### [72] [L2M-Reg: Building-level Uncertainty-aware Registration of Outdoor LiDAR Point Clouds and Semantic 3D City Models](https://arxiv.org/abs/2509.16832)
*Ziyang Xu,Benedikt Schwab,Yihui Yang,Thomas H. Kolbe,Christoph Holst*

Main category: cs.CV

TL;DR: 本文提出了一种名为L2M-Reg的平面基准精细配准方法，用于解决LiDAR点云与语义三维城市模型（LoD2）在存在模型不确定性时的建筑物级别配准挑战。


<details>
  <summary>Details</summary>
Motivation: LiDAR点云与语义三维城市模型的准确配准是城市数字孪生的基础，也是数字建造、变化检测和模型优化等任务的先决条件。然而，由于LoD2语义三维城市模型中存在的泛化不确定性，在单个建筑物级别实现准确的LiDAR-to-Model配准仍然具有挑战性。

Method: L2M-Reg是一种平面基准的精细配准方法，明确考虑了模型不确定性。它包括三个关键步骤：建立可靠的平面对应关系、构建伪平面约束的高斯-赫尔默特模型、以及自适应估计垂直平移。

Result: 在三个真实世界数据集上的实验表明，L2M-Reg比现有的基于ICP和基于平面的方法更准确且计算效率更高。

Conclusion: L2M-Reg为在存在模型不确定性时进行LiDAR-to-Model配准提供了一种新颖的建筑物级别解决方案。

Abstract: Accurate registration between LiDAR (Light Detection and Ranging) point
clouds and semantic 3D city models is a fundamental topic in urban digital
twinning and a prerequisite for downstream tasks, such as digital construction,
change detection and model refinement. However, achieving accurate
LiDAR-to-Model registration at individual building level remains challenging,
particularly due to the generalization uncertainty in semantic 3D city models
at the Level of Detail 2 (LoD2). This paper addresses this gap by proposing
L2M-Reg, a plane-based fine registration method that explicitly accounts for
model uncertainty. L2M-Reg consists of three key steps: establishing reliable
plane correspondence, building a pseudo-plane-constrained Gauss-Helmert model,
and adaptively estimating vertical translation. Experiments on three real-world
datasets demonstrate that L2M-Reg is both more accurate and computationally
efficient than existing ICP-based and plane-based methods. Overall, L2M-Reg
provides a novel building-level solution regarding LiDAR-to-Model registration
when model uncertainty is present.

</details>


### [73] [AHA -- Predicting What Matters Next: Online Highlight Detection Without Looking Ahead](https://arxiv.org/abs/2509.16421)
*Aiden Chang,Celso De Melo,Stephanie M. Lukin*

Main category: cs.CV

TL;DR: 本文提出Aha，一个自回归高光检测框架，能在无需访问未来帧的情况下，实时理解连续视频流并预测帧与自然语言任务的相关性，通过引入Dynamic SinkCache实现恒定内存使用，并在基准测试中超越了现有离线方法。


<details>
  <summary>Details</summary>
Motivation: 现有的视频理解和高光检测方法大多假设在推理时能访问整个视频，这使其不适用于自动驾驶、监控无人机和灾难响应机器人等高风险环境中的在线或流媒体场景，也无法支持实时决策所需的逐步推理。

Method: 本文引入了Aha，一个自回归高光检测框架，用于根据自然语言描述的任务预测每个视频帧的相关性。Aha在不访问未来视频帧的情况下运行，利用多模态视觉-语言模型和轻量级、解耦的头部，并在大型精选的人类中心视频标签数据集上进行训练。为实现可扩展性，引入了Dynamic SinkCache机制，以在无限长流中实现恒定内存使用而不会降低标准基准上的性能。

Result: Aha在高光检测基准测试中达到了最先进（SOTA）的性能，甚至超越了先前的离线、全上下文方法和视频-语言模型，在TVSum上mAP提升了5.9%，在Mr.Hisum上mAP提升了8.3%。实验还展示了Aha在给定任务导向的自然语言输入和连续的机器人中心视频的情况下，在实际机器人应用中的潜力。

Conclusion: Aha作为一种实时推理模块，在连续视频流的下游规划和长周期理解方面具有潜在的有效性。

Abstract: Real-time understanding of continuous video streams is essential for
intelligent agents operating in high-stakes environments, including autonomous
vehicles, surveillance drones, and disaster response robots. Yet, most existing
video understanding and highlight detection methods assume access to the entire
video during inference, making them unsuitable for online or streaming
scenarios. In particular, current models optimize for offline summarization,
failing to support step-by-step reasoning needed for real-time decision-making.
We introduce Aha, an autoregressive highlight detection framework that predicts
the relevance of each video frame against a task described in natural language.
Without accessing future video frames, Aha utilizes a multimodal
vision-language model and lightweight, decoupled heads trained on a large,
curated dataset of human-centric video labels. To enable scalability, we
introduce the Dynamic SinkCache mechanism that achieves constant memory usage
across infinite-length streams without degrading performance on standard
benchmarks. This encourages the hidden representation to capture high-level
task objectives, enabling effective frame-level rankings for informativeness,
relevance, and uncertainty with respect to the natural language task. Aha
achieves state-of-the-art (SOTA) performance on highlight detection benchmarks,
surpassing even prior offline, full-context approaches and video-language
models by +5.9% on TVSum and +8.3% on Mr.Hisum in mAP (mean Average Precision).
We explore Aha's potential for real-world robotics applications given a
task-oriented natural language input and a continuous, robot-centric video.
Both experiments demonstrate Aha's potential effectiveness as a real-time
reasoning module for downstream planning and long-horizon understanding.

</details>


### [74] [DocIQ: A Benchmark Dataset and Feature Fusion Network for Document Image Quality Assessment](https://arxiv.org/abs/2509.17012)
*Zhichao Ma,Fan Huang,Lu Zhao,Fengjun Guo,Guangtao Zhai,Xiongkuo Min*

Main category: cs.CV

TL;DR: 本文介绍了DIQA-5000主观文档图像质量评估数据集，并提出了一种利用文档布局特征和多级特征融合的无参考DIQA模型，该模型在多个维度上表现优于现有先进模型。


<details>
  <summary>Details</summary>
Motivation: 文档图像质量评估（DIQA）对于光学字符识别（OCR）、文档修复以及文档图像处理系统评估等多种应用至关重要。

Method: 1. 构建DIQA-5000数据集：包含5000张由500张真实世界图像通过多种增强技术生成的图像，每张图像由15位受试者在整体质量、清晰度和色彩保真度三个维度进行评分。2. 提出无参考DIQA模型：利用文档布局特征在降低分辨率时保持质量感知以降低计算成本。3. 设计特征融合模块：提取并整合文档图像中的低级和高级视觉特征。4. 采用独立的质量头部：为每个评分维度预测分数分布，以学习文档图像质量的不同方面。

Result: 实验结果表明，该方法在DIQA-5000数据集以及另一个专注于OCR准确性的文档图像数据集上，均优于当前最先进的通用IQA模型。

Conclusion: 本文提出的DIQA-5000数据集和专门的无参考DIQA模型，通过利用文档布局和多级特征，能够有效且多维度地评估文档图像质量，并超越了现有通用IQA模型。

Abstract: Document image quality assessment (DIQA) is an important component for
various applications, including optical character recognition (OCR), document
restoration, and the evaluation of document image processing systems. In this
paper, we introduce a subjective DIQA dataset DIQA-5000. The DIQA-5000 dataset
comprises 5,000 document images, generated by applying multiple document
enhancement techniques to 500 real-world images with diverse distortions. Each
enhanced image was rated by 15 subjects across three rating dimensions: overall
quality, sharpness, and color fidelity. Furthermore, we propose a specialized
no-reference DIQA model that exploits document layout features to maintain
quality perception at reduced resolutions to lower computational cost.
Recognizing that image quality is influenced by both low-level and high-level
visual features, we designed a feature fusion module to extract and integrate
multi-level features from document images. To generate multi-dimensional
scores, our model employs independent quality heads for each dimension to
predict score distributions, allowing it to learn distinct aspects of document
image quality. Experimental results demonstrate that our method outperforms
current state-of-the-art general-purpose IQA models on both DIQA-5000 and an
additional document image dataset focused on OCR accuracy.

</details>


### [75] [3D Gaussian Flats: Hybrid 2D/3D Photometric Scene Reconstruction](https://arxiv.org/abs/2509.16423)
*Maria Taktasheva,Lily Goli,Alessandro Fiorini,Zhen,Li,Daniel Rebain,Andrea Tagliasacchi*

Main category: cs.CV

TL;DR: 本文提出了一种混合2D/3D高斯表示，用于解决辐射场方法在重建平面无纹理表面时遇到的问题，通过结合平面（2D）和自由形式（3D）高斯，显著提升了室内场景的视觉保真度和几何精度。


<details>
  <summary>Details</summary>
Motivation: 当前的辐射场方法在处理平面、无纹理表面时表现不佳，导致重建结果不均匀或半透明，这是由于光度重建目标病态所致。而传统的表面重建方法虽然解决了这个问题，但却牺牲了视觉质量。

Method: 本文提出了一种新颖的混合2D/3D表示方法。它联合优化了用于建模平面表面的受约束平面（2D）高斯和用于场景其余部分的自由形式（3D）高斯。这是一种端到端的方法，能够动态检测和优化平面区域。

Result: 该方法在ScanNet++和ScanNetv2数据集上实现了最先进的深度估计性能，并且在网格提取方面表现出色，没有过拟合特定相机模型，展示了其在生成高质量室内场景重建方面的有效性。

Conclusion: 所提出的混合2D/3D高斯表示能够有效解决辐射场在重建包含平面无纹理表面的场景时面临的挑战，从而在提高视觉保真度的同时，也提升了几何精度，特别适用于室内场景的重建。

Abstract: Recent advances in radiance fields and novel view synthesis enable creation
of realistic digital twins from photographs. However, current methods struggle
with flat, texture-less surfaces, creating uneven and semi-transparent
reconstructions, due to an ill-conditioned photometric reconstruction
objective. Surface reconstruction methods solve this issue but sacrifice visual
quality. We propose a novel hybrid 2D/3D representation that jointly optimizes
constrained planar (2D) Gaussians for modeling flat surfaces and freeform (3D)
Gaussians for the rest of the scene. Our end-to-end approach dynamically
detects and refines planar regions, improving both visual fidelity and
geometric accuracy. It achieves state-of-the-art depth estimation on ScanNet++
and ScanNetv2, and excels at mesh extraction without overfitting to a specific
camera model, showing its effectiveness in producing high-quality
reconstruction of indoor scenes.

</details>


### [76] [CoBEVMoE: Heterogeneity-aware Feature Fusion with Dynamic Mixture-of-Experts for Collaborative Perception](https://arxiv.org/abs/2509.17107)
*Lingzhao Kong,Jiacheng Lin,Siyu Li,Kai Luo,Zhiyong Li,Kailun Yang*

Main category: cs.CV

TL;DR: CoBEVMoE提出了一种基于BEV空间和动态专家混合（DMoE）架构的协同感知框架，通过动态生成专家来建模多智能体异构观测中的特征相似性和多样性，并辅以动态专家度量损失，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有中间融合方法主要关注对齐相似特征，但忽略了多智能体因视角和空间位置差异导致的异构感知多样性。

Method: 本文提出了CoBEVMoE框架，它在鸟瞰图（BEV）空间中运行，并引入了动态专家混合（DMoE）架构。DMoE中的每个专家根据特定智能体的输入特征动态生成，能够提取独特的可靠线索，同时关注共享语义，从而显式建模智能体间特征的相似性和异构性。此外，还引入了动态专家度量损失（DEML）以增强专家间的多样性并提高融合表示的判别能力。

Result: CoBEVMoE在OPV2V和DAIR-V2X-C数据集上取得了最先进的性能。具体而言，它在OPV2V上将基于摄像头的BEV分割的IoU提高了1.5%，在DAIR-V2X-C上将基于激光雷达的3D目标检测的AP@50提高了3.0%。

Conclusion: 实验结果验证了基于专家模型的异构特征建模在多智能体协同感知中的有效性。

Abstract: Collaborative perception aims to extend sensing coverage and improve
perception accuracy by sharing information among multiple agents. However, due
to differences in viewpoints and spatial positions, agents often acquire
heterogeneous observations. Existing intermediate fusion methods primarily
focus on aligning similar features, often overlooking the perceptual diversity
among agents. To address this limitation, we propose CoBEVMoE, a novel
collaborative perception framework that operates in the Bird's Eye View (BEV)
space and incorporates a Dynamic Mixture-of-Experts (DMoE) architecture. In
DMoE, each expert is dynamically generated based on the input features of a
specific agent, enabling it to extract distinctive and reliable cues while
attending to shared semantics. This design allows the fusion process to
explicitly model both feature similarity and heterogeneity across agents.
Furthermore, we introduce a Dynamic Expert Metric Loss (DEML) to enhance
inter-expert diversity and improve the discriminability of the fused
representation. Extensive experiments on the OPV2V and DAIR-V2X-C datasets
demonstrate that CoBEVMoE achieves state-of-the-art performance. Specifically,
it improves the IoU for Camera-based BEV segmentation by +1.5% on OPV2V and the
AP@50 for LiDAR-based 3D object detection by +3.0% on DAIR-V2X-C, verifying the
effectiveness of expert-based heterogeneous feature modeling in multi-agent
collaborative perception. The source code will be made publicly available at
https://github.com/godk0509/CoBEVMoE.

</details>


### [77] [TractoTransformer: Diffusion MRI Streamline Tractography using CNN and Transformer Networks](https://arxiv.org/abs/2509.16429)
*Itzik Waizman,Yakov Gusakov,Itay Benou,Tammy Riklin Raviv*

Main category: cs.CV

TL;DR: 本文提出了一种结合Transformer和CNN的新型白质纤维束追踪方法，通过整合轨迹上下文和局部扩散MRI测量，提高了神经通路重建的精度和完整性。


<details>
  <summary>Details</summary>
Motivation: 白质纤维束追踪面临从噪声和模糊测量中推断神经纤维轨迹的挑战，尤其是在交叉、合并和扇形白质结构中。

Method: 该方法利用Transformer模型序列化白质流线，整合轨迹上下文和当前的扩散MRI测量来预测纤维方向；同时使用CNN从每个体素周围的局部区域提取微观结构特征，以纳入空间信息。

Result: 与传统追踪模型相比，该方法提高了神经通路映射的精度和完整性。通过Tractometer工具包评估，其性能与最先进的方法具有竞争力，并在TractoInferno数据集上展示了对真实世界数据的强大泛化能力。

Conclusion: 所提出的结合Transformer和CNN的白质纤维束追踪方法，通过整合轨迹上下文和局部空间信息，显著提升了神经通路重建的精度、完整性和泛化能力。

Abstract: White matter tractography is an advanced neuroimaging technique that
reconstructs the 3D white matter pathways of the brain from diffusion MRI data.
It can be framed as a pathfinding problem aiming to infer neural fiber
trajectories from noisy and ambiguous measurements, facing challenges such as
crossing, merging, and fanning white-matter configurations. In this paper, we
propose a novel tractography method that leverages Transformers to model the
sequential nature of white matter streamlines, enabling the prediction of fiber
directions by integrating both the trajectory context and current diffusion MRI
measurements. To incorporate spatial information, we utilize CNNs that extract
microstructural features from local neighborhoods around each voxel. By
combining these complementary sources of information, our approach improves the
precision and completeness of neural pathway mapping compared to traditional
tractography models. We evaluate our method with the Tractometer toolkit,
achieving competitive performance against state-of-the-art approaches, and
present qualitative results on the TractoInferno dataset, demonstrating strong
generalization to real-world data.

</details>


### [78] [DepTR-MOT: Unveiling the Potential of Depth-Informed Trajectory Refinement for Multi-Object Tracking](https://arxiv.org/abs/2509.17323)
*Buyin Deng,Lingxin Huang,Kai Luo,Fei Teng,Kailun Yang*

Main category: cs.CV

TL;DR: 本文提出DepTR-MOT，一个基于DETR的多目标跟踪(MOT)模型，通过整合实例级深度信息来解决2D跟踪方法在遮挡和近距离交互中的不足，特别适用于机器人环境。


<details>
  <summary>Details</summary>
Motivation: 现有的基于检测的2D多目标跟踪方法（依赖边界框和运动建模）在遮挡和近距离交互下表现不佳，尤其在目标密集且频繁遮挡的机器人环境中不可靠。深度信息有潜力解决这些问题，但大多数现有MOT数据集缺乏深度标注，导致深度信息未被充分利用。

Method: DepTR-MOT是一个基于DETR的检测器，通过实例级深度信息进行增强。主要创新包括：(i) 基于基础模型的实例级软深度标签监督，用于优化深度预测；(ii) 稠密深度图蒸馏，以保持全局深度一致性。这些策略使DepTR-MOT能够在推理时输出实例级深度，无需基础模型且不增加计算成本。

Result: DepTR-MOT在QuadTrack和DanceTrack数据集上分别取得了27.59和44.47的HOTA分数。在QuadTrack（一个机器人平台MOT数据集）上的结果尤其突出，证明了该方法在处理机器人跟踪中的遮挡和近距离挑战方面的优势。

Conclusion: DepTR-MOT通过整合深度线索，增强了基于检测的多目标跟踪范式的鲁棒性，有效解决了遮挡和近距离交互的挑战，尤其在机器人跟踪场景中表现出色。

Abstract: Visual Multi-Object Tracking (MOT) is a crucial component of robotic
perception, yet existing Tracking-By-Detection (TBD) methods often rely on 2D
cues, such as bounding boxes and motion modeling, which struggle under
occlusions and close-proximity interactions. Trackers relying on these 2D cues
are particularly unreliable in robotic environments, where dense targets and
frequent occlusions are common. While depth information has the potential to
alleviate these issues, most existing MOT datasets lack depth annotations,
leading to its underexploited role in the domain. To unveil the potential of
depth-informed trajectory refinement, we introduce DepTR-MOT, a DETR-based
detector enhanced with instance-level depth information. Specifically, we
propose two key innovations: (i) foundation model-based instance-level soft
depth label supervision, which refines depth prediction, and (ii) the
distillation of dense depth maps to maintain global depth consistency. These
strategies enable DepTR-MOT to output instance-level depth during inference,
without requiring foundation models and without additional computational cost.
By incorporating depth cues, our method enhances the robustness of the TBD
paradigm, effectively resolving occlusion and close-proximity challenges.
Experiments on both the QuadTrack and DanceTrack datasets demonstrate the
effectiveness of our approach, achieving HOTA scores of 27.59 and 44.47,
respectively. In particular, results on QuadTrack, a robotic platform MOT
dataset, highlight the advantages of our method in handling occlusion and
close-proximity challenges in robotic tracking. The source code will be made
publicly available at https://github.com/warriordby/DepTR-MOT.

</details>


### [79] [Improved mmFormer for Liver Fibrosis Staging via Missing-Modality Compensation](https://arxiv.org/abs/2509.16436)
*Zhejia Zhang,Junjie Wang,Le Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种基于mmFormer架构的多模态MRI分类模型，通过自适应模块和缺失模态补偿模块处理任意组合的缺失模态，并结合交叉验证集成策略，在肝纤维化分期任务中取得了良好的性能。


<details>
  <summary>Details</summary>
Motivation: 在真实临床环境中，MRI成像常因设备差异或患者配合问题导致模态缺失，这严重影响模型性能，因此需要一种能有效处理缺失模态的分类模型。

Method: 该模型保留了mmFormer的混合模态特异性编码器和模态相关编码器以提取一致的病灶特征。此外，它整合了一个缺失模态补偿模块，利用零填充、模态可用性掩码和具有可学习统计参数的Delta函数来动态合成代理特征以恢复缺失信息。为进一步提高预测性能，模型还采用了交叉验证集成策略，通过在不同折叠上训练多个模型并在推理时进行软投票。

Result: 在CARE 2025挑战赛的肝纤维化分期任务中，针对肝硬化检测和实质性纤维化检测，模型在同分布供应商上分别获得了66.67%和74.17%的准确率，以及71.73%和68.48%的相应AUC分数。

Conclusion: 该模型有效解决了多模态MRI中模态缺失的问题，通过其创新的自适应和补偿机制，结合集成学习，在肝纤维化分期任务中展现出有竞争力的性能，适用于真实世界的临床应用。

Abstract: In real-world clinical settings, magnetic resonance imaging (MRI) frequently
suffers from missing modalities due to equipment variability or patient
cooperation issues, which can significantly affect model performance. To
address this issue, we propose a multimodal MRI classification model based on
the mmFormer architecture with an adaptive module for handling arbitrary
combinations of missing modalities. Specifically, this model retains the hybrid
modality-specific encoders and the modality-correlated encoder from mmFormer to
extract consistent lesion features across available modalities. In addition, we
integrate a missing-modality compensation module which leverages zero-padding,
modality availability masks, and a Delta Function with learnable statistical
parameters to dynamically synthesize proxy features for recovering missing
information. To further improve prediction performance, we adopt a
cross-validation ensemble strategy by training multiple models on different
folds and applying soft voting during inference. This method is evaluated on
the test set of Comprehensive Analysis & Computing of REal-world medical images
(CARE) 2025 challenge, targeting the Liver Fibrosis Staging (LiFS) task based
on non-contrast dynamic MRI scans including T1-weighted imaging (T1WI),
T2-weighted imaging (T2WI), and diffusion-weighted imaging (DWI). For Cirrhosis
Detection and Substantial Fibrosis Detection on in-distribution vendors, our
model obtains accuracies of 66.67%, and 74.17%, and corresponding area under
the curve (AUC) scores of 71.73% and 68.48%, respectively.

</details>


### [80] [AutoArabic: A Three-Stage Framework for Localizing Video-Text Retrieval Benchmarks](https://arxiv.org/abs/2509.16438)
*Mohamed Eltahir,Osamah Sarraj,Abdulrahman Alfrihidi,Taha Alshatiri,Mohammed Khurd,Mohammed Bremoo,Tanveer Hussain*

Main category: cs.CV

TL;DR: 本文引入了AutoArabic框架，利用大型语言模型（LLMs）将非阿拉伯语视频检索基准（如DiDeMo）高效地翻译成现代标准阿拉伯语，创建了DiDeMo-AR，并验证了其在保留基准难度方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的视频到文本和文本到视频检索基准主要集中在英语和多语言语料库，但阿拉伯语领域服务不足，缺乏本地化的评估指标，限制了该语言在相关研究中的进展。

Method: 本文提出了一个三阶段的AutoArabic框架，利用最先进的大型语言模型（LLMs）将非阿拉伯语基准翻译成现代标准阿拉伯语，将所需的手动修订工作量减少了近四倍。该框架还包含一个错误检测模块，能以97%的准确率自动标记潜在的翻译错误。研究人员将此框架应用于DiDeMo基准，生成了DiDeMo-AR。

Result: 研究成功创建了DiDeMo-AR，一个包含40,144条流畅阿拉伯语描述的DiDeMo阿拉伯语变体。错误检测模块实现了97%的准确率。翻译错误被分析并分类，为未来的阿拉伯语本地化工作提供了指导。在阿拉伯语和英语变体上训练CLIP风格基线模型后，发现性能差距适中（Recall@1约3个百分点），表明阿拉伯语本地化保留了基准难度。此外，评估了三种后期编辑预算（零、仅标记错误、完全编辑），发现性能随编辑量增加而单调提升，但原始LLM输出（零预算）仍具有可用性。

Conclusion: AutoArabic框架提供了一种有效且高效的方法，将非阿拉伯语视频检索基准本地化为阿拉伯语，显著减少了手动工作量。生成的阿拉伯语基准DiDeMo-AR保留了原始基准的难度，并且即使未经后期编辑的LLM输出也具有可用性。该工作为未来的阿拉伯语本地化工作提供了有价值的分类和指导，并为其他语言的本地化提供了可复现的工具。

Abstract: Video-to-text and text-to-video retrieval are dominated by English benchmarks
(e.g. DiDeMo, MSR-VTT) and recent multilingual corpora (e.g. RUDDER), yet
Arabic remains underserved, lacking localized evaluation metrics. We introduce
a three-stage framework, AutoArabic, utilizing state-of-the-art large language
models (LLMs) to translate non-Arabic benchmarks into Modern Standard Arabic,
reducing the manual revision required by nearly fourfold. The framework
incorporates an error detection module that automatically flags potential
translation errors with 97% accuracy. Applying the framework to DiDeMo, a video
retrieval benchmark produces DiDeMo-AR, an Arabic variant with 40,144 fluent
Arabic descriptions. An analysis of the translation errors is provided and
organized into an insightful taxonomy to guide future Arabic localization
efforts. We train a CLIP-style baseline with identical hyperparameters on the
Arabic and English variants of the benchmark, finding a moderate performance
gap (about 3 percentage points at Recall@1), indicating that Arabic
localization preserves benchmark difficulty. We evaluate three post-editing
budgets (zero/ flagged-only/ full) and find that performance improves
monotonically with more post-editing, while the raw LLM output (zero-budget)
remains usable. To ensure reproducibility to other languages, we made the code
available at https://github.com/Tahaalshatiri/AutoArabic.

</details>


### [81] [Vision-Based Driver Drowsiness Monitoring: Comparative Analysis of YOLOv5-v11 Models](https://arxiv.org/abs/2509.17498)
*Dilshara Herath,Chinthaka Abeyrathne,Prabhani Jayaweera*

Main category: cs.CV

TL;DR: 本文评估了七种基于YOLO的实时非侵入式疲劳驾驶检测方法，并与EAR方法进行比较，分析了准确性、延迟和资源需求之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 驾驶员疲劳是导致道路事故的关键因素，每年造成数千人死亡和受伤，因此需要有效的实时疲劳检测方法。

Method: 研究使用了公开的UTA-RLDD数据集，包含清醒和疲劳状态以及多种变量。对七种YOLO变体（v5s, v9c, v9t, v10n, v10l, v11n, v11l）进行了微调，并以Precision、Recall、mAP0.5和mAP 0.5-0.95作为性能指标。此外，还实现了基于Dlib面部特征点的眼睛纵横比（EAR）方法进行对比。

Result: YOLOv9c在准确性方面表现最佳（0.986 mAP 0.5, 0.978 Recall）。YOLOv11n在精度（0.954）和推理效率之间取得了最佳平衡，非常适合嵌入式部署。EAR方法计算开销低，但在姿态变化和遮挡下鲁棒性较差。

Conclusion: 研究揭示了准确性、延迟和资源需求之间的明显权衡，并为自动驾驶和工业安全应用中选择或组合检测方法提供了实用指导。

Abstract: Driver drowsiness remains a critical factor in road accidents, accounting for
thousands of fatalities and injuries each year. This paper presents a
comprehensive evaluation of real-time, non-intrusive drowsiness detection
methods, focusing on computer vision based YOLO (You Look Only Once)
algorithms. A publicly available dataset namely, UTA-RLDD was used, containing
both awake and drowsy conditions, ensuring variability in gender, eyewear,
illumination, and skin tone. Seven YOLO variants (v5s, v9c, v9t, v10n, v10l,
v11n, v11l) are fine-tuned, with performance measured in terms of Precision,
Recall, mAP0.5, and mAP 0.5-0.95. Among these, YOLOv9c achieved the highest
accuracy (0.986 mAP 0.5, 0.978 Recall) while YOLOv11n strikes the optimal
balance between precision (0.954) and inference efficiency, making it highly
suitable for embedded deployment. Additionally, we implement an Eye Aspect
Ratio (EAR) approach using Dlib's facial landmarks, which despite its low
computational footprint exhibits reduced robustness under pose variation and
occlusions. Our findings illustrate clear trade offs between accuracy, latency,
and resource requirements, and offer practical guidelines for selecting or
combining detection methods in autonomous driving and industrial safety
applications.

</details>


### [82] [KRAST: Knowledge-Augmented Robotic Action Recognition with Structured Text for Vision-Language Models](https://arxiv.org/abs/2509.16452)
*Son Hai Nguyen,Diwei Wang,Jinhyeok Jang,Hyewon Seo*

Main category: cs.CV

TL;DR: 本文提出了一种利用领域特定知识增强的视觉-语言模型（VLMs）进行室内日常动作识别的方法，通过可学习的文本描述提示，在ETRI-Activity3D数据集上实现了超过95%的准确率，超越了现有技术。


<details>
  <summary>Details</summary>
Motivation: 准确的基于视觉的动作识别对于开发能够在复杂真实世界环境中安全可靠运行的自主机器人至关重要。

Method: 作者采用了一种提示学习框架，将每个动作的类别级文本描述作为可学习的提示嵌入到冻结的预训练VLM主干中。同时，设计并评估了几种用于构建和编码这些文本描述的策略。

Result: 在ETRI-Activity3D数据集上的实验表明，该方法在测试时仅使用RGB视频输入，即可达到超过95%的准确率，并优于现有的最先进方法。

Conclusion: 研究结果强调了知识增强提示在实现鲁棒动作识别和最少监督方面的有效性。

Abstract: Accurate vision-based action recognition is crucial for developing autonomous
robots that can operate safely and reliably in complex, real-world
environments. In this work, we advance video-based recognition of indoor daily
actions for robotic perception by leveraging vision-language models (VLMs)
enriched with domain-specific knowledge. We adapt a prompt-learning framework
in which class-level textual descriptions of each action are embedded as
learnable prompts into a frozen pre-trained VLM backbone. Several strategies
for structuring and encoding these textual descriptions are designed and
evaluated. Experiments on the ETRI-Activity3D dataset demonstrate that our
method, using only RGB video inputs at test time, achieves over 95\% accuracy
and outperforms state-of-the-art approaches. These results highlight the
effectiveness of knowledge-augmented prompts in enabling robust action
recognition with minimal supervision.

</details>


### [83] [Explainable Gait Abnormality Detection Using Dual-Dataset CNN-LSTM Models](https://arxiv.org/abs/2509.16472)
*Parth Agarwal,Sangaa Chatterjee,Md Faisal Kabir,Suman Saha*

Main category: cs.CV

TL;DR: 该论文提出了一种可解释的双分支CNN-LSTM框架，用于步态分析，结合了关节特征和轮廓数据，并在临床和生物识别领域实现了高准确性。


<details>
  <summary>Details</summary>
Motivation: 大多数现有模型缺乏可解释性，且依赖单一数据集，限制了其在诊断运动障碍中的应用。

Method: 该研究采用了一个双分支CNN-LSTM框架：一个1D分支处理来自GAVD数据集的基于关节的特征，另一个3D分支处理来自OU-MVLP数据集的剪影。通过SHAP提供时间归因，通过Grad-CAM提供空间定位，以增强可解释性。

Result: 在保留数据集上，该系统实现了98.6%的准确率，并具有强大的召回率和F1分数。

Conclusion: 该方法在临床和生物识别领域推动了可解释步态分析的发展。

Abstract: Gait is a key indicator in diagnosing movement disorders, but most models
lack interpretability and rely on single datasets. We propose a dual-branch
CNN-LSTM framework a 1D branch on joint-based features from GAVD and a 3D
branch on silhouettes from OU-MVLP. Interpretability is provided by SHAP
(temporal attributions) and Grad-CAM (spatial localization).On held-out sets,
the system achieves 98.6% accuracy with strong recall and F1. This approach
advances explainable gait analysis across both clinical and biometric domains.

</details>


### [84] [Cross-Corpus and Cross-domain Handwriting Assessment of NeuroDegenerative Diseases via Time-Series-to-Image Conversion](https://arxiv.org/abs/2509.16474)
*Gabrielle Chavez,Laureano Moro-Velazquez,Ankur Butala,Najim Dehak,Thomas Thebaud*

Main category: cs.CV

TL;DR: 本文提出一个联合分类器框架，结合手写笔迹的时间序列和图像数据，用于检测帕金森病等神经系统疾病，并在多个数据集上实现了最先进的性能和良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 神经系统疾病（如帕金森病和阿尔茨海默病）显著影响手写笔迹。现有基于特征或计算机视觉的方法在跨数据集泛化方面（特别是在时间序列和图像数据之间）存在困难。

Method: 研究者提出了一个利用手写笔迹的时间序列和图像数据的联合分类器框架。该分类器基于在ImageNet-1k上预训练的ResNet50模型。

Result: 二元分类实验表明，该模型在现有时间序列和图像数据集上均达到了最先进的性能，特别是在NeuroLogical Signals (NLS)数据集的Draw Clock和Spiral任务上取得了显著改进。此外，跨数据集和多数据集实验持续获得了高F1分数（帕金森病检测高达98），证明了该模型在不同形式手写信号上的泛化潜力。

Conclusion: 所提出的模型能够有效泛化到不同形式的手写信号，并显著增强了神经系统疾病中运动缺陷的检测能力。

Abstract: Handwriting is significantly affected by neurological disorders (ND) such as
Parkinson's disease (PD) and Alzheimer's disease (AD). Prior works have
analyzed handwriting tasks using feature-based approaches or computer-vision
techniques, but these methods have struggled to generalize across multiple
datasets, particularly between temporal features represented as time-series and
images. We propose a framework that leverages both time-series and images of
handwriting through a joint classifier, based on a ResNet50 pretrained on
ImageNet-1k. Binary classification experiments demonstrate state-of-the-art
performances on existing time-series and image datasets, with significant
improvement on specific drawing and writing tasks from the NeuroLogical Signals
(NLS) dataset. In particular, the proposed model demonstrates improved
performance on Draw Clock and Spiral tasks. Additionally, cross-dataset and
multi-dataset experiments were consistently able to achieve high F1 scores, up
to 98 for PD detection, highlighting the potential of the proposed model to
generalize over different forms of handwriting signals, and enhance the
detection of motor deficits in ND.

</details>


### [85] [Eye Gaze Tells You Where to Compute: Gaze-Driven Efficient VLMs](https://arxiv.org/abs/2509.16476)
*Qinyu Chen,Jiawen Qi*

Main category: cs.CV

TL;DR: GazeVLM是一个免训练的框架，利用人类眼动作为监督信号来减少视觉语言模型（VLMs）中的冗余视觉token，从而提高推理效率，同时保持甚至提升答案质量，特别适用于边缘消费设备。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLMs）在理解视觉内容方面表现出色，但其视觉token冗余导致推理效率低下，阻碍了在AR/VR等边缘消费设备上的实时应用。现有效率方法通常需要架构修改、访问中间激活或引入额外的计算/内存开销，并可能导致准确性下降。此外，它们还存在提示与图像兴趣区域错位的问题，可能导致模型关注错误区域或遗漏细节。

Method: 本文提出了GazeVLM，一个免训练的框架，利用人类眼动作为自然的监督信号来分配计算资源。它通过提取眼动驱动的兴趣区域（ROIs），并可选择性地结合低分辨率全局视图，模拟人类的中央凹-周边感知，从而裁剪冗余视觉token，同时保留任务相关的细节。

Result: 在VOILA-COCO基准测试中，使用人类眼动对Qwen2.5-VL-3B/7B模型进行视觉问答任务评估。GazeVLM将视觉token减少了高达93.1%，总token减少了高达59.6%，FLOPs减少了50%，同时相对于全分辨率基线保持了更好的答案质量（通过GPT-4o配对判断和加权分数评估）。

Conclusion: 将模型计算与人类眼动对齐，为在消费设备上实现高效的VLM推理提供了一条简单、即插即用的途径。

Abstract: Vision-Language Models (VLMs) deliver impressive performance in understanding
visual content with language instructions. However, redundancy in vision tokens
results in the degenerated inference efficiency of VLMs, which hinders
real-time use on edge consumer devices such as AR/VR devices. Existing
efficiency methods commonly prune visual tokens using learned saliency, sparse
attention schedules, or controller policies, but they often require
architectural modification or access to intermediate activations. These
pipelines add inference-time modules that increase compute and memory and often
lead to an accuracy trade-off. Moreover, they also suffer from misalignment
between the prompts and the region of interest in the images. Without human
guidance, the model may focus on the wrong regions and miss small,
high-frequency details when prompts or scenes change. In this paper, we propose
GazeVLM, a training-free framework that uses the human eye gaze as a natural
supervisory signal to allocate computation where it matters. By extracting
gaze-driven regions of interest (ROIs) and optionally combining them with a
low-resolution global view, GazeVLM mimics fovea-periphery perception to cut
redundant visual tokens while preserving task-relevant details. We evaluate the
visual question answering tasks on Qwen2.5-VL-3B/7B on the VOILA-COCO benchmark
with human gaze. Quality of the answer is assessed by GPT-4o pairwise judging
and a weighted score over coverage, accuracy, details, and fluency. Efficiency
is measured by token counts and FLOPs. GazeVLM reduces visual tokens by up to
93.1%, total tokens by up to 59.6%, and FLOPs by 50%, while keeping better
answer quality relative to full-resolution baselines. Our results show that
aligning model computation with human gaze offers a simple, plug-and-play path
toward efficient VLM inference on consumer devices.

</details>


### [86] [Thermal Imaging-based Real-time Fall Detection using Motion Flow and Attention-enhanced Convolutional Recurrent Architecture](https://arxiv.org/abs/2509.16479)
*Christopher Silver,Thangarajah Akilan*

Main category: cs.CV

TL;DR: 该研究提出一种基于双向卷积长短期记忆网络（BiConvLSTM）并结合多种注意力机制的先进热成像跌倒检测方法，在TSF数据集上取得了99.7%的ROC-AUC，并在TF-66数据集上表现稳健，为可部署的高性能解决方案奠定基础。


<details>
  <summary>Details</summary>
Motivation: 老年人跌倒是一个主要的公共卫生问题。现有解决方案（可穿戴传感器、环境传感器、基于RGB的视觉系统）在可靠性、用户依从性和实用性方面面临挑战。利益相关者偏爱非穿戴式、被动、保护隐私、实时且无需用户交互的跌倒检测系统。

Method: 本研究提出一种先进的热成像跌倒检测方法，采用双向卷积长短期记忆网络（BiConvLSTM）模型，并增强了空间、时间、特征、自注意和通用注意力机制。通过对数百种模型变体进行系统实验，探索注意力机制、循环模块和运动流的集成，最终确定了最佳性能架构。

Result: BiConvLSTM模型在TSF数据集上实现了99.7%的ROC-AUC，达到了最先进的性能，并在新出现的、多样化且保护隐私的TF-66基准测试中展示了稳健的结果。

Conclusion: 研究结果突显了所提出模型的泛化能力和实用性，为热成像跌倒检测设定了新标准，并为可部署的高性能解决方案铺平了道路。

Abstract: Falls among seniors are a major public health issue. Existing solutions using
wearable sensors, ambient sensors, and RGB-based vision systems face challenges
in reliability, user compliance, and practicality. Studies indicate that
stakeholders, such as older adults and eldercare facilities, prefer
non-wearable, passive, privacy-preserving, and real-time fall detection systems
that require no user interaction. This study proposes an advanced thermal fall
detection method using a Bidirectional Convolutional Long Short-Term Memory
(BiConvLSTM) model, enhanced with spatial, temporal, feature, self, and general
attention mechanisms. Through systematic experimentation across hundreds of
model variations exploring the integration of attention mechanisms, recurrent
modules, and motion flow, we identified top-performing architectures. Among
them, BiConvLSTM achieved state-of-the-art performance with a ROC-AUC of
$99.7\%$ on the TSF dataset and demonstrated robust results on TF-66, a newly
emerged, diverse, and privacy-preserving benchmark. These results highlight the
generalizability and practicality of the proposed model, setting new standards
for thermal fall detection and paving the way toward deployable,
high-performance solutions.

</details>


### [87] [Octree Latent Diffusion for Semantic 3D Scene Generation and Completion](https://arxiv.org/abs/2509.16483)
*Xujia Zhang,Brendan Crowe,Christoffer Heckman*

Main category: cs.CV

TL;DR: 该论文提出了一种名为“八叉树潜在语义扩散”的统一框架，用于在室内外场景中进行3D语义场景的补全、扩展和生成，通过双八叉树图潜在表示实现。


<details>
  <summary>Details</summary>
Motivation: 现有的3D语义场景补全、扩展和生成方法通常将这些问题解耦并独立解决，且常常是领域特定的，需要为室内外等不同数据分布训练单独的模型。

Method: 开发了“八叉树潜在语义扩散”框架，该框架直接在高效的双八叉树图潜在表示上操作。合成过程分为两个阶段：(i) 结构扩散，预测二元分割信号以构建粗糙的占据八叉树；(ii) 潜在语义扩散，生成语义嵌入，并通过图变分自编码器解码为体素级语义标签。补全或扩展通过推理时期的潜在修复（inpainting）或外绘（outpainting）实现，无需重新训练或微调，利用部分LiDAR扫描或地图作为条件。

Result: 该方法在单次LiDAR扫描下展示了高质量的结构、连贯的语义和鲁棒的补全能力，并且对分布外LiDAR数据具有零样本泛化能力。

Conclusion: 通过在双八叉树图潜在空间中进行生成式补全，为现实世界的机器人感知任务提供了一种实用且可扩展的替代方案，优于基于回归的管道。

Abstract: The completion, extension, and generation of 3D semantic scenes are an
interrelated set of capabilities that are useful for robotic navigation and
exploration. Existing approaches seek to decouple these problems and solve them
oneoff. Additionally, these approaches are often domain-specific, requiring
separate models for different data distributions, e.g. indoor vs. outdoor
scenes. To unify these techniques and provide cross-domain compatibility, we
develop a single framework that can perform scene completion, extension, and
generation in both indoor and outdoor scenes, which we term Octree Latent
Semantic Diffusion. Our approach operates directly on an efficient dual octree
graph latent representation: a hierarchical, sparse, and memory-efficient
occupancy structure. This technique disentangles synthesis into two stages: (i)
structure diffusion, which predicts binary split signals to construct a coarse
occupancy octree, and (ii) latent semantic diffusion, which generates semantic
embeddings decoded by a graph VAE into voxellevel semantic labels. To perform
semantic scene completion or extension, our model leverages inference-time
latent inpainting, or outpainting respectively. These inference-time methods
use partial LiDAR scans or maps to condition generation, without the need for
retraining or finetuning. We demonstrate highquality structure, coherent
semantics, and robust completion from single LiDAR scans, as well as zero-shot
generalization to out-of-distribution LiDAR data. These results indicate that
completion-through-generation in a dual octree graph latent space is a
practical and scalable alternative to regression-based pipelines for real-world
robotic perception tasks.

</details>


### [88] [RLGF: Reinforcement Learning with Geometric Feedback for Autonomous Driving Video Generation](https://arxiv.org/abs/2509.16500)
*Tianyi Yan,Wencheng Han,Xia Zhou,Xueyang Zhang,Kun Zhan,Cheng-zhong Xu,Jianbing Shen*

Main category: cs.CV

TL;DR: 本文提出了一种名为RLGF的强化学习方法，通过几何反馈改进视频扩散模型，以解决自动驾驶合成数据中存在的几何失真问题，显著提升了3D目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 尽管当前最先进的视频生成模型在视觉真实感方面表现出色，但它们产生的自动驾驶合成数据存在细微的几何失真，这限制了其在下游感知任务（如3D目标检测）中的实用性，导致与真实数据之间存在显著的性能差距。

Method: 本文引入了“带有几何反馈的强化学习”（RLGF）方法，通过整合来自专门的潜在空间自动驾驶感知模型的奖励来优化视频扩散模型。其核心组件包括：1) 一种高效的潜在空间窗口优化技术，用于在扩散过程中提供有针对性的反馈；2) 一个分层几何奖励（HGR）系统，为点-线-面排列和场景占用一致性提供多级奖励。此外，本文还提出了GeoScores来量化这些几何失真。

Result: 将RLGF应用于DiVE等模型在nuScenes数据集上进行测试，结果表明它显著减少了几何误差（例如，消失点误差降低21%，深度误差降低57%），并大幅提高了3D目标检测的mAP达12.7%，从而缩小了与真实数据性能的差距。

Conclusion: RLGF提供了一种即插即用的解决方案，能够为自动驾驶开发生成几何上准确且可靠的合成视频。

Abstract: Synthetic data is crucial for advancing autonomous driving (AD) systems, yet
current state-of-the-art video generation models, despite their visual realism,
suffer from subtle geometric distortions that limit their utility for
downstream perception tasks. We identify and quantify this critical issue,
demonstrating a significant performance gap in 3D object detection when using
synthetic versus real data. To address this, we introduce Reinforcement
Learning with Geometric Feedback (RLGF), RLGF uniquely refines video diffusion
models by incorporating rewards from specialized latent-space AD perception
models. Its core components include an efficient Latent-Space Windowing
Optimization technique for targeted feedback during diffusion, and a
Hierarchical Geometric Reward (HGR) system providing multi-level rewards for
point-line-plane alignment, and scene occupancy coherence. To quantify these
distortions, we propose GeoScores. Applied to models like DiVE on nuScenes,
RLGF substantially reduces geometric errors (e.g., VP error by 21\%, Depth
error by 57\%) and dramatically improves 3D object detection mAP by 12.7\%,
narrowing the gap to real-data performance. RLGF offers a plug-and-play
solution for generating geometrically sound and reliable synthetic videos for
AD development.

</details>


### [89] [CommonForms: A Large, Diverse Dataset for Form Field Detection](https://arxiv.org/abs/2509.16506)
*Joe Barrow*

Main category: cs.CV

TL;DR: 本文介绍了CommonForms，一个用于表单字段检测的网络规模数据集，并将该问题建模为目标检测。同时提出了FFDNet系列检测器，在CommonForms数据集上表现出色，并优于现有商业解决方案。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏大规模的表单字段检测数据集和开源模型。现有的商业PDF阅读器在表单准备方面存在局限性，例如无法检测复选框。

Method: 研究人员通过过滤Common Crawl中的可填写PDF文档，构建了包含5.5万份文档、45万多页的CommonForms数据集，涵盖多种语言和领域。他们将表单字段检测视为目标检测问题，并提出了FFDNet-Small和FFDNet-Large两种表单字段检测器模型。

Result: CommonForms数据集是首个大规模的表单字段检测数据集。FFDNet模型在CommonForms测试集上取得了很高的平均精度，训练成本低于500美元。消融实验表明，高分辨率输入对高质量检测至关重要，数据清洗过程提高了数据效率。定性分析显示FFDNet模型优于流行的商业PDF阅读器，并且能够检测文本输入、选择按钮和签名等多种字段，包括复选框。

Conclusion: 本文首次发布了用于表单字段检测的大规模数据集和开源模型。研究强调了高分辨率输入和数据清洗在提升模型性能中的关键作用。FFDNet模型在性能上超越了现有商业解决方案，为表单自动化处理提供了新的工具。

Abstract: This paper introduces CommonForms, a web-scale dataset for form field
detection. It casts the problem of form field detection as object detection:
given an image of a page, predict the location and type (Text Input, Choice
Button, Signature) of form fields. The dataset is constructed by filtering
Common Crawl to find PDFs that have fillable elements. Starting with 8 million
documents, the filtering process is used to arrive at a final dataset of
roughly 55k documents that have over 450k pages. Analysis shows that the
dataset contains a diverse mixture of languages and domains; one third of the
pages are non-English, and among the 14 classified domains, no domain makes up
more than 25% of the dataset.
  In addition, this paper presents a family of form field detectors,
FFDNet-Small and FFDNet-Large, which attain a very high average precision on
the CommonForms test set. Each model cost less than $500 to train. Ablation
results show that high-resolution inputs are crucial for high-quality form
field detection, and that the cleaning process improves data efficiency over
using all PDFs that have fillable fields in Common Crawl. A qualitative
analysis shows that they outperform a popular, commercially available PDF
reader that can prepare forms. Unlike the most popular commercially available
solutions, FFDNet can predict checkboxes in addition to text and signature
fields. This is, to our knowledge, the first large scale dataset released for
form field detection, as well as the first open source models. The dataset,
models, and code will be released at https://github.com/jbarrow/commonforms

</details>


### [90] [OS-DiffVSR: Towards One-step Latent Diffusion Model for High-detailed Real-world Video Super-Resolution](https://arxiv.org/abs/2509.16507)
*Hanting Li,Huaao Tang,Jianhong Han,Tianxiong Zhou,Jiulong Cui,Haizhen Xie,Yan Chen,Jie Hu*

Main category: cs.CV

TL;DR: 本文提出了一种名为OS-DiffVSR的单步扩散模型，用于真实世界视频超分辨率（VSR），它通过创新的训练范式和多帧融合机制，在保持高视频质量的同时显著提高了推理效率，甚至超越了需要多步采样的现有扩散模型。


<details>
  <summary>Details</summary>
Motivation: 尽管潜在扩散模型在视频超分辨率（VSR）任务中表现出色，但其需要多个扩散步骤来处理视频中的每一帧，导致推理效率低下。目前，基于扩散的VSR方法在视频质量和推理效率之间存在固有的权衡。

Method: 本文提出了OS-DiffVSR，一个用于真实世界VSR的单步扩散模型。具体方法包括：1) 设计了一种新颖的相邻帧对抗训练范式，以显著提高合成视频的质量。2) 设计了一种多帧融合机制，以保持帧间时间一致性并减少视频中的闪烁。

Result: 在多个流行的VSR基准测试上的大量实验表明，OS-DiffVSR甚至可以比现有需要数十个采样步骤的基于扩散的VSR方法实现更好的视频质量。

Conclusion: OS-DiffVSR成功地解决了基于扩散的VSR方法在视频质量和推理效率之间的权衡问题，通过单步生成实现了卓越的视频质量和时间一致性。

Abstract: Recently, latent diffusion models has demonstrated promising performance in
real-world video super-resolution (VSR) task, which can reconstruct
high-quality videos from distorted low-resolution input through multiple
diffusion steps. Compared to image super-resolution (ISR), VSR methods needs to
process each frame in a video, which poses challenges to its inference
efficiency. However, video quality and inference efficiency have always been a
trade-off for the diffusion-based VSR methods. In this work, we propose
One-Step Diffusion model for real-world Video Super-Resolution, namely
OS-DiffVSR. Specifically, we devise a novel adjacent frame adversarial training
paradigm, which can significantly improve the quality of synthetic videos.
Besides, we devise a multi-frame fusion mechanism to maintain inter-frame
temporal consistency and reduce the flicker in video. Extensive experiments on
several popular VSR benchmarks demonstrate that OS-DiffVSR can even achieve
better quality than existing diffusion-based VSR methods that require dozens of
sampling steps.

</details>


### [91] [SlowFast-SCI: Slow-Fast Deep Unfolding Learning for Spectral Compressive Imaging](https://arxiv.org/abs/2509.16509)
*Haijin Zeng,Xuan Lu,Yurong Zhang,Yongyong Chen,Jingyong Su,Jie Liu*

Main category: cs.CV

TL;DR: 本文提出SlowFast-SCI，一个双速深度展开框架，通过结合慢速预训练和快速测试时自适应，解决了现有光谱压缩成像（SCI）方法在处理分布外数据时的适应性差、计算量大和推理速度慢的问题。


<details>
  <summary>Details</summary>
Motivation: 现有用于光谱压缩成像（SCI）的深度展开方法仅模拟了人类学习的慢速、累积过程，依赖于大量预训练和多级展开。它们缺乏快速适应新光学配置（如分布外相机或定制光谱设置）的能力，导致在训练未见的场景中性能下降。此外，这些方法的深度也导致计算量大和推理速度慢。

Method: SlowFast-SCI是一个双速框架：
1. **慢速学习阶段**：预训练或重用一个基于先验的骨干网络，并通过成像指导将其精简为一个紧凑的快速展开模型。
2. **快速学习阶段**：在每个块中嵌入轻量级自适应模块，并在测试时通过双域损失进行自监督训练，而无需重新训练骨干网络。

Result: SlowFast-SCI是首个由测试时自适应驱动的深度展开框架，用于高效、自适应的光谱重建。它实现了超过70%的参数和FLOPs减少，在分布外数据上PSNR提高了高达5.79 dB，保持了跨域适应性，并将适应速度提高了4倍。其模块化设计使其能与任何深度展开网络集成。

Conclusion: SlowFast-SCI的双阶段设计将离线鲁棒性与在线逐样本校准相结合，为自适应、可现场部署的成像和扩展计算成像模式铺平了道路。

Abstract: Humans learn in two complementary ways: a slow, cumulative process that
builds broad, general knowledge, and a fast, on-the-fly process that captures
specific experiences. Existing deep-unfolding methods for spectral compressive
imaging (SCI) mirror only the slow component-relying on heavy pre-training with
many unfolding stages-yet they lack the rapid adaptation needed to handle new
optical configurations. As a result, they falter on out-of-distribution
cameras, especially in bespoke spectral setups unseen during training. This
depth also incurs heavy computation and slow inference. To bridge this gap, we
introduce SlowFast-SCI, a dual-speed framework seamlessly integrated into any
deep unfolding network beyond SCI systems. During slow learning, we pre-train
or reuse a priors-based backbone and distill it via imaging guidance into a
compact fast-unfolding model. In the fast learning stage, lightweight
adaptation modules are embedded within each block and trained self-supervised
at test time via a dual-domain loss-without retraining the backbone. To the
best of our knowledge, SlowFast-SCI is the first test-time adaptation-driven
deep unfolding framework for efficient, self-adaptive spectral reconstruction.
Its dual-stage design unites offline robustness with on-the-fly per-sample
calibration-yielding over 70% reduction in parameters and FLOPs, up to 5.79 dB
PSNR improvement on out-of-distribution data, preserved cross-domain
adaptability, and a 4x faster adaptation speed. In addition, its modularity
integrates with any deep-unfolding network, paving the way for self-adaptive,
field-deployable imaging and expanded computational imaging modalities. Code
and models are available at https://github.com/XuanLu11/SlowFast-SCI.

</details>


### [92] [Seeing Culture: A Benchmark for Visual Reasoning and Grounding](https://arxiv.org/abs/2509.16517)
*Burak Satar,Zhixin Ma,Patrick A. Irawan,Wilfried A. Mulyawan,Jing Jiang,Ee-Peng Lim,Chong-Wah Ngo*

Main category: cs.CV

TL;DR: 本文提出了“文化洞察基准”（Seeing Culture Benchmark, SCB），这是一个针对多模态视觉-语言模型（VLMs）的文化推理新基准，通过两阶段任务（多项选择视觉问答和文化文物分割）来评估模型对东南亚地区被忽视文化的理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有的文化数据集在提供文化推理能力方面存在不足，并且未能充分代表多种文化，尤其是一些多元文化地区（如东南亚）的文化常被忽视。

Method: 引入了SCB基准，要求VLMs进行两阶段文化推理：i) 通过多项选择视觉问答（VQA）选择正确的视觉选项，其中选项系统地分为来自同一国家、不同国家或混合组；ii) 在第一阶段正确后，分割出相关的文化文物作为推理证据。SCB包含来自七个东南亚国家的1,065张图像、138种文化文物、五类，以及3,178个问题（其中1,093个由人工精心策划）。

Result: 对各种VLM的评估揭示了跨模态文化推理的复杂性，并突出了在文化细微场景中视觉推理与空间定位之间存在的差异。

Conclusion: SCB是一个关键的基准，用于识别当前VLM在文化推理方面的不足，从而指导该领域未来的发展方向。

Abstract: Multimodal vision-language models (VLMs) have made substantial progress in
various tasks that require a combined understanding of visual and textual
content, particularly in cultural understanding tasks, with the emergence of
new cultural datasets. However, these datasets frequently fall short of
providing cultural reasoning while underrepresenting many cultures. In this
paper, we introduce the Seeing Culture Benchmark (SCB), focusing on cultural
reasoning with a novel approach that requires VLMs to reason on culturally rich
images in two stages: i) selecting the correct visual option with
multiple-choice visual question answering (VQA), and ii) segmenting the
relevant cultural artifact as evidence of reasoning. Visual options in the
first stage are systematically organized into three types: those originating
from the same country, those from different countries, or a mixed group.
Notably, all options are derived from a singular category for each type.
Progression to the second stage occurs only after a correct visual option is
chosen. The SCB benchmark comprises 1,065 images that capture 138 cultural
artifacts across five categories from seven Southeast Asia countries, whose
diverse cultures are often overlooked, accompanied by 3,178 questions, of which
1,093 are unique and meticulously curated by human annotators. Our evaluation
of various VLMs reveals the complexities involved in cross-modal cultural
reasoning and highlights the disparity between visual reasoning and spatial
grounding in culturally nuanced scenarios. The SCB serves as a crucial
benchmark for identifying these shortcomings, thereby guiding future
developments in the field of cultural reasoning.
https://github.com/buraksatar/SeeingCulture

</details>


### [93] [FG-Attn: Leveraging Fine-Grained Sparsity In Diffusion Transformers](https://arxiv.org/abs/2509.16518)
*Sankeerth Durvasula,Kavya Sreedhar,Zain Moustafa,Suraj Kothawade,Ashish Gondimalla,Suvinay Subramanian,Narges Shahidi,Nandita Vijaykumar*

Main category: cs.CV

TL;DR: 本文提出FG-Attn，一种用于长上下文扩散Transformer的细粒度稀疏注意力机制，通过跳过Mx1切片而非MxM块的计算，并结合异步聚集加载操作，显著加速视频生成，比现有块稀疏方法更有效地利用注意力图中的稀疏性。


<details>
  <summary>Details</summary>
Motivation: 使用扩散Transformer生成逼真视频需要大量的计算，其中注意力层是主要瓶颈。生成短视频需要Transformer处理非常长的嵌入序列（例如，5秒视频超过30K嵌入），导致显著延迟。现有工作通过利用注意力层的稀疏性来减少计算，但通常依赖于块稀疏注意力，这种方法在粗粒度上跳过计算（当MxM块中的所有注意力分数都为零时），未能充分利用注意力图中的稀疏性，仍有改进空间。

Method: 本文提出了FG-Attn，一种细粒度稀疏注意力机制。与跳过整个MxM块的块稀疏注意力不同，FG-Attn在Mx1的注意力图切片粒度上跳过计算，即一个查询块与单个键之间的点积。为实现此机制，开发了一种新的高效批量加载操作——异步聚集加载（asynchronous-gather load）。该操作从内存中收集稀疏的相关键值向量，并将其组织到GPU共享内存的打包瓦片中。计算查询块的注意力时，仅加载与这些查询相关的稀疏键集到共享内存，而非像块稀疏注意力那样加载完整的键令牌块。

Result: 将细粒度稀疏注意力应用于视频扩散模型，对于5秒480p视频，平均实现1.55倍（最高1.65倍）的加速；对于5秒720p视频，平均实现1.41倍（最高1.49倍）的加速。所有测试均在单个H100 GPU上进行。

Conclusion: FG-Attn通过在更细粒度上利用注意力图的稀疏性，显著提高了视频扩散模型的生成速度，克服了传统块稀疏注意力方法的局限性，为长上下文扩散Transformer的计算效率提供了有效解决方案。

Abstract: Generating realistic videos with diffusion transformers demands significant
computation, with attention layers the central bottleneck; even producing a
short clip requires running a transformer over a very long sequence of
embeddings, e.g., more than 30K embeddings for a 5-second video, incurring
significant latency. Prior work aims to mitigate this bottleneck by exploiting
sparsity in the attention layers to reduce computation. However, these works
typically rely on block-sparse attention, which skips score computation only
when all entries in a block of attention scores (corresponding to M queries and
M keys, with M = 64 typically) are zero. This coarse-granular skipping of
attention scores does not fully exploit sparsity in the attention map and
leaves room for improvement. In this work, we propose FG-Attn, a sparse
attention mechanism for long-context diffusion transformers that leverages
sparsity at a fine granularity. Unlike block-sparse attention, which skips
entire MxM blocks, our approach skips computations at the granularity of Mx1
slices of the attention map. Each slice is produced by query-key dot products
between a block of query vectors and a single key. To implement our proposed
sparse attention mechanism, we develop a new efficient bulk-load operation
called asynchronous-gather load. This load operation gathers a sparse set of
relevant key-value vectors from memory and arranges them into packed tiles in
the GPU's shared memory. Only a sparse set of keys relevant to those queries
are loaded into shared memory when computing attention for a block of queries,
in contrast to loading full blocks of key tokens in block-sparse attention. Our
fine-grained sparse attention, applied to video diffusion models, achieves an
average 1.55X (up to 1.65X) speedup for 5 second, 480p videos, and an average
1.41X (up to 1.49X) for 5 second, 720p videos on a single H100 GPU.

</details>


### [94] [PM25Vision: A Large-Scale Benchmark Dataset for Visual Estimation of Air Quality](https://arxiv.org/abs/2509.16519)
*Yang Han*

Main category: cs.CV

TL;DR: 本文介绍了PM25Vision，一个迄今为止最大、最全面的数据集，用于从街景图像估计PM2.5浓度。


<details>
  <summary>Details</summary>
Motivation: 现有从图像估计空气质量的数据集在规模和空间精度上存在不足，限制了模型性能和研究进展，因此需要一个更大、更精确的数据集。

Method: 研究团队构建了PM25Vision数据集，包含超过11,114张街景图像，与3,261个空气质量监测站11年间的PM2.5读数进行时间戳和地理位置匹配。详细描述了数据收集、同步和清洗流程，并使用CNN和Transformer架构提供了基线模型性能。

Result: PM25Vision数据集在规模上远超现有基准，拥有超过11,114张图像、3,261个监测站和11年的数据，其空间精度达到5公里。研究还提供了使用CNN和Transformer架构的基线模型性能。

Conclusion: PM25Vision数据集的引入为从街景图像估计PM2.5提供了一个前所未有的大规模、高精度资源，显著提升了该领域的研究潜力，并且该数据集已公开发布。

Abstract: We introduce PM25Vision (PM25V), the largest and most comprehensive dataset
to date for estimating air quality - specifically PM2.5 concentrations - from
street-level images. The dataset contains over 11,114 images matched with
timestamped and geolocated PM2.5 readings across 3,261 AQI monitoring stations
and 11 years, significantly exceeding the scale of previous benchmarks. The
spatial accuracy of this dataset has reached 5 kilometers, far exceeding the
city-level accuracy of many datasets. We describe the data collection,
synchronization, and cleaning pipelines, and provide baseline model
performances using CNN and transformer architectures. Our dataset is publicly
available.

</details>


### [95] [Lattice Boltzmann Model for Learning Real-World Pixel Dynamicity](https://arxiv.org/abs/2509.16527)
*Guangze Zheng,Shijie Lin,Haobo Zuo,Si Si,Ming-Shan Wang,Changhong Fu,Jia Pan*

Main category: cs.CV

TL;DR: 该研究提出了一种基于格子玻尔兹曼模型（LBM）的视觉跟踪方法，通过模拟像素动力学来高效、实时地适应真实世界跟踪任务，并在多项基准测试中表现出实用性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉跟踪方法可能在学习真实世界像素动态性方面存在不足，导致适应性不佳。该研究旨在提出一种能有效捕捉像素运动状态并高效适应真实世界跟踪任务的新方法。

Method: 该工作提出了格子玻尔兹曼模型（LBM）用于视觉跟踪。LBM将视觉表示分解为动态像素格子，并通过碰撞-流过程求解像素运动状态。具体而言，它通过一个多层预测-更新网络获取目标像素的高维分布，以估计像素位置和可见性。预测阶段负责目标像素空间邻域内的格子碰撞和时间视觉上下文中的格子流，而更新阶段则使用在线视觉表示来修正像素分布。

Result: 与现有方法相比，LBM展示了在线和实时的实际适用性，能够高效适应真实世界的视觉跟踪任务。在TAP-Vid和RoboTAP等真实世界点跟踪基准测试中验证了其效率。在TAO、BFT和OVT-B等大规模开放世界目标跟踪基准测试中进一步证明了LBM的真实世界实用性。

Conclusion: LBM是一种高效且实用的模型，能够通过学习真实世界像素动态性，以在线和实时的方式应对视觉跟踪任务，并在多种基准测试中表现出色，具有广泛的实际应用潜力。

Abstract: This work proposes the Lattice Boltzmann Model (LBM) to learn real-world
pixel dynamicity for visual tracking. LBM decomposes visual representations
into dynamic pixel lattices and solves pixel motion states through
collision-streaming processes. Specifically, the high-dimensional distribution
of the target pixels is acquired through a multilayer predict-update network to
estimate the pixel positions and visibility. The predict stage formulates
lattice collisions among the spatial neighborhood of target pixels and develops
lattice streaming within the temporal visual context. The update stage
rectifies the pixel distributions with online visual representations. Compared
with existing methods, LBM demonstrates practical applicability in an online
and real-time manner, which can efficiently adapt to real-world visual tracking
tasks. Comprehensive evaluations of real-world point tracking benchmarks such
as TAP-Vid and RoboTAP validate LBM's efficiency. A general evaluation of
large-scale open-world object tracking benchmarks such as TAO, BFT, and OVT-B
further demonstrates LBM's real-world practicality.

</details>


### [96] [Advancing Reference-free Evaluation of Video Captions with Factual Analysis](https://arxiv.org/abs/2509.16538)
*Shubhashis Roy Dipta,Tz-Ying Wu,Subarna Tripathi*

Main category: cs.CV

TL;DR: 本文提出了一种名为VC-Inspector的无参考、事实基础的视频字幕质量评估框架，它利用大型语言模型生成伪字幕来训练多模态评估器，显著优于现有方法，并能推广到图像字幕评估。


<details>
  <summary>Details</summary>
Motivation: 获取视频字幕的人工标注成本高昂且不切实际，尤其是在多样化的视频领域。现有评估方法依赖于参考字幕（ground truth），这在实际应用中是不现实的，限制了模型在不同领域评估性能的能力。

Method: 本文提出了一个无参考评估框架，侧重于事实基础（factual grounding）以确保字幕质量的准确评估。引入了VC-Inspector，一个新颖的无参考且事实基础的字幕质量评估器。该方法利用大型语言模型，基于有监督数据生成不同质量的伪字幕，然后使用这些伪字幕训练一个多模态模型（Qwen2.5-VL）作为评估器。

Result: VC-Inspector在VATEX-Eval数据集上与人类判断表现出卓越的一致性，优于现有方法。当将图像视为单帧视频时，该性能也泛化到了图像字幕数据集（Flickr8K-Expert和Flickr8K-CF）。

Conclusion: VC-Inspector为评估视频字幕的事实准确性提供了一个可扩展且可泛化的解决方案，为在多样化视频领域中实现更有效和客观的评估方法铺平了道路。

Abstract: Video captions offer concise snapshots of actors, objects, and actions within
a video, serving as valuable assets for applications such as question answering
and event localization. However, acquiring human annotations for video captions
is costly or even impractical, especially when dealing with diverse video
domains. Existing models trained on supervised datasets face challenges in
evaluating performance across different domains due to the reliance on
reference-based evaluation protocols, which necessitate ground truth captions.
This assumption is unrealistic for evaluating videos in the wild. To address
these limitations, we propose a reference-free evaluation framework that does
not require ground truth captions, focusing on factual grounding to ensure
accurate assessment of caption quality. We introduce VC-Inspector, a novel
caption quality evaluator that is both reference-free and factually grounded.
Utilizing large language models, we generate pseudo captions of varying quality
based on supervised data, which are subsequently used to train a multimodal
model (i.e., Qwen2.5-VL) as the evaluator. Our approach demonstrates superior
alignment with human judgments on the VATEX-Eval dataset, outperforming
existing methods. The performance also generalizes to image caption datasets,
Flickr8K-Expert and Flickr8K-CF, when viewing images as 1-frame videos.
Overall, VC-Inspector offers a scalable and generalizable solution for
evaluating the factual accuracy of video captions, paving the way for more
effective and objective assessment methodologies in diverse video domains.

</details>


### [97] [Efficient Rectified Flow for Image Fusion](https://arxiv.org/abs/2509.16549)
*Zirui Wang,Jiayi Zhang,Tianwei Guan,Yuhan Zhou,Xingyuan Li,Minjing Dong,Jinyuan Liu*

Main category: cs.CV

TL;DR: 本文提出RFfusion，一种高效的单步扩散模型，通过结合Rectified Flow和任务定制的变分自编码器（VAE）实现图像融合，显著提升推理速度并保持高质量的融合结果。


<details>
  <summary>Details</summary>
Motivation: 图像融合中的扩散模型虽然表现出色，但通常需要复杂的计算和冗余的推理时间，这限制了其应用性。

Method: 1. 提出RFfusion，一个基于Rectified Flow的单步扩散模型，通过Rectified Flow拉直采样路径，无需额外训练即可实现单步采样。2. 设计了任务定制的变分自编码器（VAE）架构，将融合操作嵌入到潜在空间中，以进一步降低计算复杂度。3. 引入了两阶段训练策略，以解决传统VAE目标与图像融合要求之间的差异，促进多模态信息有效学习和整合。

Result: RFfusion在推理速度和融合质量方面均优于其他最先进的方法。

Conclusion: RFfusion通过优化扩散模型的采样过程和引入定制的VAE架构，为图像融合任务提供了一种高效且高质量的解决方案，有效解决了现有扩散模型计算复杂和推理时间长的问题。

Abstract: Image fusion is a fundamental and important task in computer vision, aiming
to combine complementary information from different modalities to fuse images.
In recent years, diffusion models have made significant developments in the
field of image fusion. However, diffusion models often require complex
computations and redundant inference time, which reduces the applicability of
these methods. To address this issue, we propose RFfusion, an efficient
one-step diffusion model for image fusion based on Rectified Flow. We
incorporate Rectified Flow into the image fusion task to straighten the
sampling path in the diffusion model, achieving one-step sampling without the
need for additional training, while still maintaining high-quality fusion
results. Furthermore, we propose a task-specific variational autoencoder (VAE)
architecture tailored for image fusion, where the fusion operation is embedded
within the latent space to further reduce computational complexity. To address
the inherent discrepancy between conventional reconstruction-oriented VAE
objectives and the requirements of image fusion, we introduce a two-stage
training strategy. This approach facilitates the effective learning and
integration of complementary information from multi-modal source images,
thereby enabling the model to retain fine-grained structural details while
significantly enhancing inference efficiency. Extensive experiments demonstrate
that our method outperforms other state-of-the-art methods in terms of both
inference speed and fusion quality. Code is available at
https://github.com/zirui0625/RFfusion.

</details>


### [98] [ST-GS: Vision-Based 3D Semantic Occupancy Prediction with Spatial-Temporal Gaussian Splatting](https://arxiv.org/abs/2509.16552)
*Xiaoyang Yan,Muleilan Pei,Shaojie Shen*

Main category: cs.CV

TL;DR: 本文提出了一种名为时空高斯泼溅 (ST-GS) 的新框架，通过增强空间交互和时间连续性，显著提升了基于高斯模型的3D占用预测的性能和时间一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于3D语义高斯模型的占用预测方法在减少计算开销的同时，面临多视角空间交互不足和多帧时间一致性有限的问题。

Method: 本文提出了ST-GS框架。具体而言，它开发了一种在双模注意力机制中融入引导信息的空间聚合策略，以增强高斯表示中的空间交互。此外，引入了几何感知的时序融合方案，有效利用历史上下文来改善场景补全的时间连续性。

Result: 在nuScenes大规模占用预测基准测试中，所提出的方法不仅实现了最先进的性能，而且与现有基于高斯的方法相比，表现出显著更好的时间一致性。

Conclusion: ST-GS框架通过其独特的空间聚合和时间融合机制，成功克服了现有高斯基方法在空间交互和时间一致性方面的局限性，为视觉中心自动驾驶中的3D占用预测提供了更鲁棒和高性能的解决方案。

Abstract: 3D occupancy prediction is critical for comprehensive scene understanding in
vision-centric autonomous driving. Recent advances have explored utilizing 3D
semantic Gaussians to model occupancy while reducing computational overhead,
but they remain constrained by insufficient multi-view spatial interaction and
limited multi-frame temporal consistency. To overcome these issues, in this
paper, we propose a novel Spatial-Temporal Gaussian Splatting (ST-GS) framework
to enhance both spatial and temporal modeling in existing Gaussian-based
pipelines. Specifically, we develop a guidance-informed spatial aggregation
strategy within a dual-mode attention mechanism to strengthen spatial
interaction in Gaussian representations. Furthermore, we introduce a
geometry-aware temporal fusion scheme that effectively leverages historical
context to improve temporal continuity in scene completion. Extensive
experiments on the large-scale nuScenes occupancy prediction benchmark showcase
that our proposed approach not only achieves state-of-the-art performance but
also delivers markedly better temporal consistency compared to existing
Gaussian-based methods.

</details>


### [99] [Person Identification from Egocentric Human-Object Interactions using 3D Hand Pose](https://arxiv.org/abs/2509.16557)
*Muhammad Hamza,Danish Hamid,Muhammad Tahir Akram*

Main category: cs.CV

TL;DR: 本研究提出I2S（Interact2Sign）框架，通过以自我为中心的视频中的3D手部姿态分析和人-物体交互识别（HOIR），实现无侵入式用户身份识别，适用于AR辅助技术。


<details>
  <summary>Details</summary>
Motivation: 增强现实（AR）个性化辅助技术，特别是在航空驾驶舱、航空航天维护和外科手术等高风险、以人为中心的环境中，人-物体交互识别（HOIR）和用户身份识别至关重要。

Method: I2S是一个多阶段框架，利用以自我为中心的视频中的3D手部姿态进行分析。它提取手工特征（包括空间、频率、运动学、方向以及新引入的“手间空间包络”（IHSE）描述符），并执行顺序特征增强：首先识别物体类别，其次进行HOI识别，最终实现用户身份识别。

Result: I2S在基于ARCTIC和H2O数据集的双手物体操作数据集上，用户身份识别的平均F1-score达到97.52%。该模型体积小于4MB，推理时间为0.1秒，表现出最先进的性能。

Conclusion: I2S框架性能卓越，模型轻量且推理速度快，非常适合在安全关键的AR系统中进行实时、设备端身份验证。

Abstract: Human-Object Interaction Recognition (HOIR) and user identification play a
crucial role in advancing augmented reality (AR)-based personalized assistive
technologies. These systems are increasingly being deployed in high-stakes,
human-centric environments such as aircraft cockpits, aerospace maintenance,
and surgical procedures. This research introduces I2S (Interact2Sign), a multi
stage framework designed for unobtrusive user identification through human
object interaction recognition, leveraging 3D hand pose analysis in egocentric
videos. I2S utilizes handcrafted features extracted from 3D hand poses and per
forms sequential feature augmentation: first identifying the object class,
followed by HOI recognition, and ultimately, user identification. A
comprehensive feature extraction and description process was carried out for 3D
hand poses, organizing the extracted features into semantically meaningful
categories: Spatial, Frequency, Kinematic, Orientation, and a novel descriptor
introduced in this work, the Inter-Hand Spatial Envelope (IHSE). Extensive
ablation studies were conducted to determine the most effective combination of
features. The optimal configuration achieved an impressive average F1-score of
97.52% for user identification, evaluated on a bimanual object manipulation
dataset derived from the ARCTIC and H2O datasets. I2S demonstrates
state-of-the-art performance while maintaining a lightweight model size of
under 4 MB and a fast inference time of 0.1 seconds. These characteristics make
the proposed framework highly suitable for real-time, on-device authentication
in security-critical, AR-based systems.

</details>


### [100] [Captioning for Text-Video Retrieval via Dual-Group Direct Preference Optimization](https://arxiv.org/abs/2509.16560)
*Ji Soo Lee,Byungoh Ko,Jaewon Cho,Howoong Lee,Jaewoon Byun,Hyunwoo J. Kim*

Main category: cs.CV

TL;DR: CaRe-DPO是一个文本-视频检索框架，通过直接优化字幕生成与检索相关性分数（使用DG-DPO）来生成细粒度字幕，并结合带有角色嵌入的MLLM检索模型，显著提升了检索性能。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）生成的辅助字幕通常过于通用，在视觉相似的视频中缺乏区分度，限制了其在细粒度检索中的效用。此外，传统的字幕评估指标（如BLEU）不适用于需要区分候选对象的检索任务。

Method: 本文提出了CaRe-DPO检索框架，其核心是双组直接偏好优化（DG-DPO）学习策略，通过建模不同视频和字幕对组之间的偏好来监督字幕生成，从而直接优化字幕生成与检索相关性分数。此外，还提出了一个基于MLLM的检索模型，该模型引入了角色嵌入，以更好地区分具有不同功能角色（如辅助字幕和文本查询）的文本输入。

Result: 通过广泛的实验证明，CaRe-DPO通过有效利用辅助知识生成细粒度字幕，显著提升了检索性能。

Conclusion: CaRe-DPO框架能够有效利用辅助知识生成细粒度字幕，从而显著增强文本-视频检索任务的性能。

Abstract: In text-video retrieval, auxiliary captions are often used to enhance video
understanding, bridging the gap between the modalities. While recent advances
in multi-modal large language models (MLLMs) have enabled strong zero-shot
caption generation, we observe that such captions tend to be generic and
indistinguishable across visually similar videos, limiting their utility for
fine-grained retrieval. Moreover, conventional captioning approaches are
typically evaluated using language generation metrics, such as BLEU, which are
not typically tailored for retrieval tasks that require making discriminative
distinctions between candidates. To address this, we propose
$\textbf{CaRe-DPO}$, a retrieval framework that directly optimizes caption
generation using retrieval relevance scores. At its core is Dual-Group Direct
Preference Optimization (DG-DPO), a novel learning strategy that supervises
captioning by modeling preferences across groups of distinct video and caption
pairs. In addition, we present an MLLM-based retrieval model that incorporates
role-embeddings to better distinguish between textual inputs with different
functional roles, such as an auxiliary caption and a text query. Through
extensive experiments, we demonstrate that CaRe-DPO significantly enhances
retrieval performance by effectively leveraging auxiliary knowledge to generate
fine-grained captions for retrieval. Code is available at
https://github.com/mlvlab/CaReDPO.

</details>


### [101] [V-CECE: Visual Counterfactual Explanations via Conceptual Edits](https://arxiv.org/abs/2509.16567)
*Nikolaos Spanos,Maria Lymperaiou,Giorgos Filandrianos,Konstantinos Thomas,Athanasios Voulodimos,Giorgos Stamou*

Main category: cs.CV

TL;DR: 提出一种无需训练、即插即用的黑盒反事实生成框架，利用预训练扩散模型和理论保证的逐步编辑，生成人类水平的反事实解释，并揭示了人类与神经网络模型（CNN, ViT, LVLM）之间的解释性差距。


<details>
  <summary>Details</summary>
Motivation: 现有的黑盒反事实生成框架未能充分考虑编辑的语义内容，并且过度依赖训练来指导生成过程。

Method: 提出一种新颖的、即插即用的黑盒反事实生成框架。该框架基于最优编辑的理论保证，建议分步编辑，利用预训练的图像编辑扩散模型，无需访问分类器内部，实现零训练，从而产生可解释的反事实生成过程。

Result: 该框架能够生成人类水平的反事实解释。通过使用卷积神经网络 (CNN)、视觉Transformer (ViT) 和大型视觉语言模型 (LVLM) 分类器，实验展示了人类推理与神经模型行为之间的解释性差距，并通过全面的用户评估得到证实。

Conclusion: 本研究提供了一种无需训练即可生成可解释反事实的方法，并揭示了人类与不同类型神经网络模型在解释性上的显著差异。

Abstract: Recent black-box counterfactual generation frameworks fail to take into
account the semantic content of the proposed edits, while relying heavily on
training to guide the generation process. We propose a novel, plug-and-play
black-box counterfactual generation framework, which suggests step-by-step
edits based on theoretical guarantees of optimal edits to produce human-level
counterfactual explanations with zero training. Our framework utilizes a
pre-trained image editing diffusion model, and operates without access to the
internals of the classifier, leading to an explainable counterfactual
generation process. Throughout our experimentation, we showcase the explanatory
gap between human reasoning and neural model behavior by utilizing both
Convolutional Neural Network (CNN), Vision Transformer (ViT) and Large Vision
Language Model (LVLM) classifiers, substantiated through a comprehensive human
evaluation.

</details>


### [102] [A Novel Metric for Detecting Memorization in Generative Models for Brain MRI Synthesis](https://arxiv.org/abs/2509.16582)
*Antonio Scardace,Lemuel Puglisi,Francesco Guarnera,Sebastiano Battiato,Daniele Ravì*

Main category: cs.CV

TL;DR: 本文提出了一种名为DeepSSIM的自监督度量方法，用于量化生成模型中的记忆化现象，特别是在医学影像领域，以解决敏感数据泄露的风险。


<details>
  <summary>Details</summary>
Motivation: 深度生成模型在医学影像合成数据方面潜力巨大，但存在关键漏洞：它们可能记忆敏感的训练数据，导致未经授权的患者信息泄露。检测生成模型中的记忆化极具挑战性，需要可扩展的方法来识别大量生成样本中的训练数据泄露。

Method: DeepSSIM是一种自监督度量方法，通过以下方式量化记忆化：1) 将图像投影到学习到的嵌入空间中；2) 强制嵌入之间的余弦相似度与图像空间中计算的真实SSIM（结构相似性指数）分数匹配。为捕获领域特定的解剖特征，训练中融入了结构保持增强，使DeepSSIM无需精确空间对齐即可可靠估计相似性。

Result: 在对由潜在扩散模型（LDM）生成的合成脑部MRI数据进行的案例研究中（使用IXI和CoRR两个公开数据集的2,195次MRI扫描），DeepSSIM表现优于现有最先进的记忆化度量方法，F1分数比现有最佳方法平均提高了+52.03%。

Conclusion: DeepSSIM是一种有效且性能卓越的自监督度量方法，能够准确量化生成模型中的记忆化，尤其适用于医学影像领域，有助于提高数据安全性并识别潜在的隐私泄露风险。

Abstract: Deep generative models have emerged as a transformative tool in medical
imaging, offering substantial potential for synthetic data generation. However,
recent empirical studies highlight a critical vulnerability: these models can
memorize sensitive training data, posing significant risks of unauthorized
patient information disclosure. Detecting memorization in generative models
remains particularly challenging, necessitating scalable methods capable of
identifying training data leakage across large sets of generated samples. In
this work, we propose DeepSSIM, a novel self-supervised metric for quantifying
memorization in generative models. DeepSSIM is trained to: i) project images
into a learned embedding space and ii) force the cosine similarity between
embeddings to match the ground-truth SSIM (Structural Similarity Index) scores
computed in the image space. To capture domain-specific anatomical features,
training incorporates structure-preserving augmentations, allowing DeepSSIM to
estimate similarity reliably without requiring precise spatial alignment. We
evaluate DeepSSIM in a case study involving synthetic brain MRI data generated
by a Latent Diffusion Model (LDM) trained under memorization-prone conditions,
using 2,195 MRI scans from two publicly available datasets (IXI and CoRR).
Compared to state-of-the-art memorization metrics, DeepSSIM achieves superior
performance, improving F1 scores by an average of +52.03% over the best
existing method. Code and data of our approach are publicly available at the
following link: https://github.com/brAIn-science/DeepSSIM.

</details>


### [103] [SQS: Enhancing Sparse Perception Models via Query-based Splatting in Autonomous Driving](https://arxiv.org/abs/2509.16588)
*Haiming Zhang,Yiyao Zhu,Wending Zhou,Xu Yan,Yingjie Cai,Bingbing Liu,Shuguang Cui,Zhen Li*

Main category: cs.CV

TL;DR: SQS是一种新颖的基于查询的splatting预训练方法，通过学习3D高斯表示来增强自动驾驶中的稀疏感知模型（SPMs），在占用预测和3D目标检测等任务上取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 稀疏感知模型（SPMs）虽能实现高效计算和加速推理，但仍需进一步提升其在自动驾驶场景下的性能，以更好地满足各种3D感知任务的需求。

Method: SQS引入一个插件模块，在预训练阶段从稀疏查询中预测3D高斯表示。它利用自监督splatting，通过重建多视角图像和深度图来学习细粒度的上下文特征。在微调阶段，预训练的高斯查询通过查询交互机制无缝集成到下游网络中，连接预训练查询与任务特定查询，以适应占用预测和3D目标检测等任务。

Result: 在自动驾驶基准测试中，SQS在多个基于查询的3D感知任务上（尤其在占用预测和3D目标检测方面）取得了显著的性能提升。与现有最先进的预训练方法相比，占用预测的mIoU提高了1.3，3D检测的NDS提高了1.0。

Conclusion: SQS通过其创新的查询驱动splatting预训练方法，有效提升了稀疏感知模型在自动驾驶3D感知任务中的性能，为未来的查询驱动模型提供了强大的基础。

Abstract: Sparse Perception Models (SPMs) adopt a query-driven paradigm that forgoes
explicit dense BEV or volumetric construction, enabling highly efficient
computation and accelerated inference. In this paper, we introduce SQS, a novel
query-based splatting pre-training specifically designed to advance SPMs in
autonomous driving. SQS introduces a plug-in module that predicts 3D Gaussian
representations from sparse queries during pre-training, leveraging
self-supervised splatting to learn fine-grained contextual features through the
reconstruction of multi-view images and depth maps. During fine-tuning, the
pre-trained Gaussian queries are seamlessly integrated into downstream networks
via query interaction mechanisms that explicitly connect pre-trained queries
with task-specific queries, effectively accommodating the diverse requirements
of occupancy prediction and 3D object detection. Extensive experiments on
autonomous driving benchmarks demonstrate that SQS delivers considerable
performance gains across multiple query-based 3D perception tasks, notably in
occupancy prediction and 3D object detection, outperforming prior
state-of-the-art pre-training approaches by a significant margin (i.e., +1.3
mIoU on occupancy prediction and +1.0 NDS on 3D detection).

</details>


### [104] [FakeChain: Exposing Shallow Cues in Multi-Step Deepfake Detection](https://arxiv.org/abs/2509.16602)
*Minji Heo,Simon S. Woo*

Main category: cs.CV

TL;DR: 本研究引入了FakeChain基准，用于评估多步混合深度伪造的检测模型。结果显示，现有检测器高度依赖最终操作的痕迹，而非累积痕迹，导致泛化能力显著下降。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测研究主要集中于单步伪造，但多步或混合深度伪造日益增多，对现有检测模型构成新的技术挑战。目前对于这类复杂伪造下检测模型行为的了解甚少。

Method: 本研究引入了FakeChain，一个大规模基准数据集，包含使用五种最先进生成器合成的1步、2步和3步伪造。通过FakeChain，作者分析了不同步数、生成器组合和质量设置下，混合伪造的检测性能和光谱特性。

Result: 研究发现，检测性能高度依赖于最终的伪造类型。当最终伪造类型与训练分布不同时，F1分数下降高达58.83%。这表明检测器依赖于最后阶段的伪造痕迹，而非累积的操纵痕迹，从而限制了其泛化能力。

Conclusion: 检测模型需要明确考虑操纵历史和序列，以应对日益复杂的深度伪造。FakeChain等基准对于反映真实世界中不断增长的合成复杂性和多样性至关重要。

Abstract: Multi-step or hybrid deepfakes, created by sequentially applying different
deepfake creation methods such as Face-Swapping, GAN-based generation, and
Diffusion methods, can pose an emerging and unforseen technical challenge for
detection models trained on single-step forgeries. While prior studies have
mainly focused on detecting isolated single manipulation, little is known about
the detection model behavior under such compositional, hybrid, and complex
manipulation pipelines. In this work, we introduce \textbf{FakeChain}, a
large-scale benchmark comprising 1-, 2-, and 3-Step forgeries synthesized using
five state-of-the-art representative generators. Using this approach, we
analyze detection performance and spectral properties across hybrid
manipulation at different step, along with varying generator combinations and
quality settings. Surprisingly, our findings reveal that detection performance
highly depends on the final manipulation type, with F1-score dropping by up to
\textbf{58.83\%} when it differs from training distribution. This clearly
demonstrates that detectors rely on last-stage artifacts rather than cumulative
manipulation traces, limiting generalization. Such findings highlight the need
for detection models to explicitly consider manipulation history and sequences.
Our results highlight the importance of benchmarks such as FakeChain,
reflecting growing synthesis complexity and diversity in real-world scenarios.
Our sample code is available
here\footnote{https://github.com/minjihh/FakeChain}.

</details>


### [105] [Describe-to-Score: Text-Guided Efficient Image Complexity Assessment](https://arxiv.org/abs/2509.16609)
*Shipeng Liu,Zhonglin Zhang,Dengfeng Chen,Liang Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种名为D2S的视觉-文本融合框架，通过整合图像的视觉和文本语义特征来准确评估图像复杂度，提高了评估的准确性和泛化能力，并保持了推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有图像复杂度评估方法主要依赖视觉特征，忽略了高层语义信息，导致评估准确性和泛化能力受限。

Method: 本文引入了视觉-文本融合方法来建模图像复杂度，以增加表征多样性并减少假设空间复杂性。具体提出了D2S（Describe-to-Score）框架，该框架利用预训练的视觉-语言模型生成图像描述，并引入特征对齐和熵分布对齐机制，以将语义信息融入复杂度评估。D2S在训练时利用多模态信息，但在推理时仅需视觉分支，从而避免了多模态计算开销。

Result: 实验结果表明，D2S在IC9600数据集上优于现有方法，并在无参考图像质量评估（NR-IQA）基准测试中保持竞争力，验证了多模态融合在复杂度相关任务中的有效性和效率。

Conclusion: 视觉-文本多模态融合能够有效且高效地评估图像复杂度，显著提升了评估的准确性和泛化能力，同时通过优化推理过程保持了高效率。

Abstract: Accurately assessing image complexity (IC) is critical for computer vision,
yet most existing methods rely solely on visual features and often neglect
high-level semantic information, limiting their accuracy and generalization. We
introduce vision-text fusion for IC modeling. This approach integrates visual
and textual semantic features, increasing representational diversity. It also
reduces the complexity of the hypothesis space, which enhances both accuracy
and generalization in complexity assessment. We propose the D2S
(Describe-to-Score) framework, which generates image captions with a
pre-trained vision-language model. We propose the feature alignment and entropy
distribution alignment mechanisms, D2S guides semantic information to inform
complexity assessment while bridging the gap between vision and text
modalities. D2S utilizes multi-modal information during training but requires
only the vision branch during inference, thereby avoiding multi-modal
computational overhead and enabling efficient assessment. Experimental results
demonstrate that D2S outperforms existing methods on the IC9600 dataset and
maintains competitiveness on no-reference image quality assessment (NR-IQA)
benchmark, validating the effectiveness and efficiency of multi-modal fusion in
complexity-related tasks. Code is available at:
https://github.com/xauat-liushipeng/D2S

</details>


### [106] [Detection and Simulation of Urban Heat Islands Using a Fine-Tuned Geospatial Foundation Model](https://arxiv.org/abs/2509.16617)
*David Kreismann*

Main category: cs.CV

TL;DR: 本研究利用地理空间基础模型，通过微调预测未来气候情景下的城市地表温度，并探索其对植被策略等土地覆盖变化的响应，取得了高精度和良好的外推能力。


<details>
  <summary>Details</summary>
Motivation: 随着城市化和气候变化的加剧，城市热岛效应日益频繁和严重，城市需要详细的气温数据来制定有效的缓解计划。然而，传统的机器学习模型和有限的数据基础设施往往导致预测不准确，尤其是在服务不足的地区。地理空间基础模型因其强大的泛化能力和对微调的低要求，为传统方法受限的预测提供了替代方案。

Method: 本研究微调了一个地理空间基础模型，用于预测未来气候情景下的城市地表温度。此外，还通过模拟植被策略探索了该模型对土地覆盖变化的响应。

Result: 微调后的模型在像素级降尺度误差低于1.74°C。其预测结果与真实模式一致，并展现出高达3.62°C的外推能力。

Conclusion: 地理空间基础模型能够有效预测未来气候情景下的城市地表温度，并对土地覆盖变化做出响应，为城市热岛效应的缓解计划提供了有力的预测工具，尤其是在传统方法受限的区域。

Abstract: As urbanization and climate change progress, urban heat island effects are
becoming more frequent and severe. To formulate effective mitigation plans,
cities require detailed air temperature data. However, predictive analytics
methods based on conventional machine learning models and limited data
infrastructure often provide inaccurate predictions, especially in underserved
areas. In this context, geospatial foundation models trained on unstructured
global data demonstrate strong generalization and require minimal fine-tuning,
offering an alternative for predictions where traditional approaches are
limited. This study fine-tunes a geospatial foundation model to predict urban
land surface temperatures under future climate scenarios and explores its
response to land cover changes using simulated vegetation strategies. The
fine-tuned model achieved pixel-wise downscaling errors below 1.74 {\deg}C and
aligned with ground truth patterns, demonstrating an extrapolation capacity up
to 3.62 {\deg}C.

</details>


### [107] [Surgical-MambaLLM: Mamba2-enhanced Multimodal Large Language Model for VQLA in Robotic Surgery](https://arxiv.org/abs/2509.16618)
*Pengfei Hao,Hongqiu Wang,Shuaibo Li,Zhaohu Xing,Guang Yang,Kaishun Wu,Lei Zhu*

Main category: cs.CV

TL;DR: 本文提出Surgical-MambaLLM，首次将Mamba2与大型语言模型（LLM）结合应用于手术视觉问答定位（Surgical-VQLA），通过增强跨模态依赖捕捉和空间感知能力，显著提升了该任务的性能。


<details>
  <summary>Details</summary>
Motivation: 当前Surgical-VQLA方法在建立文本与视觉细节之间的复杂依赖关系以及感知手术场景的空间信息方面存在困难，限制了LLM在该领域的应用潜力。

Method: 本文提出Surgical-MambaLLM模型，首次将Mamba2与LLM融合。具体方法包括：1) 设计跨模态双向Mamba2集成（CBMI）模块，利用Mamba2的跨模态集成能力进行有效的多模态融合；2) 针对手术场景的几何特性，设计手术器械感知（SIP）扫描模式，使Mamba2能更好地扫描手术图像，增强模型对场景的空间理解。

Result: Surgical-MambaLLM模型在EndoVis17-VQLA和EndoVis18-VQLA数据集上均超越了现有最先进方法，显著提升了Surgical-VQLA任务的性能。

Conclusion: 通过结合Mamba2与LLM，并引入CBMI模块和SIP扫描模式，Surgical-MambaLLM成功解决了现有方法在手术场景中跨模态依赖和空间感知不足的问题，为Surgical-VQLA任务提供了更优的解决方案。

Abstract: In recent years, Visual Question Localized-Answering in robotic surgery
(Surgical-VQLA) has gained significant attention for its potential to assist
medical students and junior doctors in understanding surgical scenes. Recently,
the rapid development of Large Language Models (LLMs) has provided more
promising solutions for this task. However, current methods struggle to
establish complex dependencies between text and visual details, and have
difficulty perceiving the spatial information of surgical scenes. To address
these challenges, we propose a novel method, Surgical-MambaLLM, which is the
first to combine Mamba2 with LLM in the surgical domain, that leverages
Mamba2's ability to effectively capture cross-modal dependencies and perceive
spatial information in surgical scenes, thereby enhancing the LLMs'
understanding of surgical images. Specifically, we propose the Cross-modal
Bidirectional Mamba2 Integration (CBMI) module to leverage Mamba2 for effective
multimodal fusion, with its cross-modal integration capabilities. Additionally,
tailored to the geometric characteristics of surgical scenes, we design the
Surgical Instrument Perception (SIP) scanning mode for Mamba2 to scan the
surgical images, enhancing the model's spatial understanding of the surgical
scene. Extensive experiments demonstrate that our Surgical-MambaLLM model
outperforms the state-of-the-art methods on the EndoVis17-VQLA and
EndoVis18-VQLA datasets, significantly improving the performance of the
Surgical-VQLA task.

</details>


### [108] [CGTGait: Collaborative Graph and Transformer for Gait Emotion Recognition](https://arxiv.org/abs/2509.16623)
*Junjie Zhou,Haijun Xiong,Junhao Lu,Ziyu Lin,Bin Feng*

Main category: cs.CV

TL;DR: 本文提出CGTGait框架，将图卷积和Transformer结合，用于基于骨架的步态情感识别。它能有效捕捉长距离时空特征，并在现有数据集上实现最先进或有竞争力的性能，同时大幅降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有的步态情感识别方法主要关注空间和局部时间运动信息，未能有效捕捉长距离时间依赖性。

Method: CGTGait框架包含多个CGT模块，每个模块利用图卷积捕捉帧级空间拓扑结构，并使用Transformer建模全局时间依赖性。此外，引入了双向跨流融合（BCSF）模块，有效聚合姿态和运动时空特征，促进互补信息交流。

Result: 在Emotion-Gait和ELMD数据集上，CGTGait取得了最先进或有竞争力的性能，并且在测试时计算复杂度降低了约82.2%（仅需0.34G FLOPs）。

Conclusion: CGTGait通过协同整合图卷积和Transformer，有效解决了现有方法在捕捉长距离时间表示方面的不足，为步态情感识别提供了一个高性能且高效的解决方案。

Abstract: Skeleton-based gait emotion recognition has received significant attention
due to its wide-ranging applications. However, existing methods primarily focus
on extracting spatial and local temporal motion information, failing to capture
long-range temporal representations. In this paper, we propose
\textbf{CGTGait}, a novel framework that collaboratively integrates graph
convolution and transformers to extract discriminative spatiotemporal features
for gait emotion recognition. Specifically, CGTGait consists of multiple CGT
blocks, where each block employs graph convolution to capture frame-level
spatial topology and the transformer to model global temporal dependencies.
Additionally, we introduce a Bidirectional Cross-Stream Fusion (BCSF) module to
effectively aggregate posture and motion spatiotemporal features, facilitating
the exchange of complementary information between the two streams. We evaluate
our method on two widely used datasets, Emotion-Gait and ELMD, demonstrating
that our CGTGait achieves state-of-the-art or at least competitive performance
while reducing computational complexity by approximately \textbf{82.2\%} (only
requiring 0.34G FLOPs) during testing. Code is available at
\small{https://github.com/githubzjj1/CGTGait.}

</details>


### [109] [Enhancing Scientific Visual Question Answering via Vision-Caption aware Supervised Fine-Tuning](https://arxiv.org/abs/2509.16628)
*Janak Kapuriya,Anwar Shaikh,Arnav Goel,Medha Hira,Apoorv Singh,Jay Saraf,Sanjana,Vaibhav Nauriyal,Avinash Anand,Zhengkui Wang,Rajiv Ratn Shah*

Main category: cs.CV

TL;DR: 本研究提出VCASFT范式，通过结合图像字幕和问答对来微调小型视觉语言模型（VLM），以提高其在科学视觉问答（VQA）任务上的性能。研究还创建了印地语数据集HiSciVQA和基于LLM的评估方案，并证明了VCASFT的有效性。


<details>
  <summary>Details</summary>
Motivation: 动机是提高小型视觉语言模型在科学视觉问答任务上的性能，尤其是在低资源语言环境下，并解决传统评估指标的局限性。

Method: 研究引入了“视觉-字幕感知监督微调”（VCASFT）学习范式，该范式利用图像字幕作为零样本提示与问答对结合，对模型进行指令调优。为评估VCASFT，研究在ScienceQA上进行了基准测试，并开发了一个包含2,245个高质量、手动标注的印地语多模态问答对的HiSciVQA数据集。此外，还引入了一种基于大型语言模型（LLM）的新型评估方案来评估HiSciVQA上的VLM。

Result: VCASFT显著提高了小型视觉语言模型在科学视觉问答任务上的性能。它在ScienceQA上展示了在不同语言、学科和领域中的适应性和有效性。HiSciVQA数据集满足了低资源语言问答数据集的关键需求。新引入的基于LLM的评估方案提供了比传统n-gram匹配准确率指标更深入的模型有效性洞察。

Conclusion: VCASFT是一种有效提升小型VLM在科学VQA任务上表现的学习范式，尤其适用于多语言和低资源语言环境。HiSciVQA数据集和LLM-based评估方案为低资源语言VQA研究提供了宝贵资源和更深入的评估方法。所有代码和数据集都已开源，以促进研究进展。

Abstract: In this study, we introduce Vision-Caption aware Supervised FineTuning
(VCASFT), a novel learning paradigm designed to enhance the performance of
smaller Vision Language Models(VLMs) on scientific visual question
answering(VQA) tasks. VCASFT leverages image captions as zero-shot prompts
alongside question-answer pairs and instruction-tunes models to yield
significant performance improvements. To comprehensively evaluate VCASFT, we
benchmark it on ScienceQA, which consists of questions across diverse
languages, subjects, and fields, demonstrating its adaptability and
effectiveness in a variety of educational contexts. Additionally, to further
demonstrate the effectiveness of this technique on lowresource languages, we
developed HiSciVQA, a dataset comprising 2,245 high-quality, hand-annotated
Hindi multimodal Q&A pairs. This dataset addresses the critical need for
low-resource language Q&A datasets and serves as a foundation for testing
VCASFT. Additionally, we introduce a novel LLM-based evaluation scheme to
evaluate VLMs on HiSciVQA which offers deeper insights into model effectiveness
surpassing traditional n-gram matching accuracy metrics. We are committed to
advancing the field by open-sourcing all code files and the HiSciVQA dataset
for the research community.

</details>


### [110] [Follow-Your-Emoji-Faster: Towards Efficient, Fine-Controllable, and Expressive Freestyle Portrait Animation](https://arxiv.org/abs/2509.16630)
*Yue Ma,Zexuan Yan,Hongyu Liu,Hongfa Wang,Heng Pan,Yingqing He,Junkun Yuan,Ailing Zeng,Chengfei Cai,Heung-Yeung Shum,Zhifeng Li,Wei Liu,Linfeng Zhang,Qifeng Chen*

Main category: cs.CV

TL;DR: Follow-Your-Emoji-Faster是一个高效的基于扩散的框架，通过面部关键点驱动自由风格肖像动画，解决了身份保持、表情转移、时间一致性和生成效率等挑战。


<details>
  <summary>Details</summary>
Motivation: 肖像动画面临的主要挑战包括：保持参考肖像的身份、准确传递目标表情、维持长期时间一致性，同时确保生成效率。

Method: 该方法通过以下方式增强了Stable Diffusion：1) 使用表情感知关键点作为显式运动信号，以改善运动对齐、支持夸张表情并减少身份泄露；2) 引入细粒度面部损失，利用表情和面部掩码来捕捉细微表情并忠实保留参考外观。此外，为了解决扩散模型在长时动画效率问题，提出了渐进式生成策略和泰勒插值缓存，实现了2.6倍无损加速。最后，引入了更全面的EmojiBench++基准。

Result: 该模型支持对包括真人、卡通、雕塑和动物在内的多样化肖像类型进行可控和富有表现力的动画制作。在EmojiBench++上的广泛评估表明，Follow-Your-Emoji-Faster在动画质量和可控性方面均取得了卓越的性能，并实现了2.6倍的无损加速。

Conclusion: Follow-Your-Emoji-Faster提供了一个高效、高质量且可控的自由风格肖像动画解决方案，有效解决了该领域的核心挑战，使其用户友好且易于访问。

Abstract: We present Follow-Your-Emoji-Faster, an efficient diffusion-based framework
for freestyle portrait animation driven by facial landmarks. The main
challenges in this task are preserving the identity of the reference portrait,
accurately transferring target expressions, and maintaining long-term temporal
consistency while ensuring generation efficiency. To address identity
preservation and accurate expression retargeting, we enhance Stable Diffusion
with two key components: a expression-aware landmarks as explicit motion
signals, which improve motion alignment, support exaggerated expressions, and
reduce identity leakage; and a fine-grained facial loss that leverages both
expression and facial masks to better capture subtle expressions and faithfully
preserve the reference appearance. With these components, our model supports
controllable and expressive animation across diverse portrait types, including
real faces, cartoons, sculptures, and animals. However, diffusion-based
frameworks typically struggle to efficiently generate long-term stable
animation results, which remains a core challenge in this task. To address
this, we propose a progressive generation strategy for stable long-term
animation, and introduce a Taylor-interpolated cache, achieving a 2.6X lossless
acceleration. These two strategies ensure that our method produces high-quality
results efficiently, making it user-friendly and accessible. Finally, we
introduce EmojiBench++, a more comprehensive benchmark comprising diverse
portraits, driving videos, and landmark sequences. Extensive evaluations on
EmojiBench++ demonstrate that Follow-Your-Emoji-Faster achieves superior
performance in both animation quality and controllability. The code, training
dataset and benchmark will be found in https://follow-your-emoji.github.io/.

</details>


### [111] [DA-Font: Few-Shot Font Generation via Dual-Attention Hybrid Integration](https://arxiv.org/abs/2509.16632)
*Weiran Chen,Guiqian Zhu,Ying Li,Yi Ji,Chunping Liu*

Main category: cs.CV

TL;DR: DA-Font是一种新颖的少样本字体生成框架，通过引入双注意力混合模块（DAHM）和特定的损失函数，有效解决了现有方法中常见的笔画错误、伪影和模糊等问题，显著提升了生成字体的结构完整性和局部保真度。


<details>
  <summary>Details</summary>
Motivation: 少样本字体生成旨在通过少量参考字形创建新字体，以大幅降低手动字体设计的劳动成本。然而，由于字体样式的多样性和复杂性，现有方法生成的字体常出现可见缺陷，如笔画错误、伪影和模糊。

Method: 本文提出了DA-Font框架，核心是双注意力混合模块（DAHM）。DAHM包含两个协同工作的注意力块：1. 组件注意力块，利用内容图像的组件信息指导风格迁移；2. 关系注意力块，通过内容特征与原始及风格化组件表示的交互，进一步细化空间关系。此外，还设计了角点一致性损失和弹性网格特征损失，以更好地改善几何对齐。

Result: 广泛的实验表明，DA-Font在多样化的字体样式和字符上均优于现有最先进的方法，证明了其在增强结构完整性和局部保真度方面的有效性。

Conclusion: DA-Font通过其独特的双注意力混合模块和定制的损失函数，成功解决了少样本字体生成中的关键挑战，能够生成具有准确字符形状、风格化纹理和良好几何对齐的高质量字体，从而显著提升了生成效果。

Abstract: Few-shot font generation aims to create new fonts with a limited number of
glyph references. It can be used to significantly reduce the labor cost of
manual font design. However, due to the variety and complexity of font styles,
the results generated by existing methods often suffer from visible defects,
such as stroke errors, artifacts and blurriness. To address these issues, we
propose DA-Font, a novel framework which integrates a Dual-Attention Hybrid
Module (DAHM). Specifically, we introduce two synergistic attention blocks: the
component attention block that leverages component information from content
images to guide the style transfer process, and the relation attention block
that further refines spatial relationships through interacting the content
feature with both original and stylized component-wise representations. These
two blocks collaborate to preserve accurate character shapes and stylistic
textures. Moreover, we also design a corner consistency loss and an elastic
mesh feature loss to better improve geometric alignment. Extensive experiments
show that our DA-Font outperforms the state-of-the-art methods across diverse
font styles and characters, demonstrating its effectiveness in enhancing
structural integrity and local fidelity. The source code can be found at
\href{https://github.com/wrchen2001/DA-Font}{\textit{https://github.com/wrchen2001/DA-Font}}.

</details>


### [112] [When Big Models Train Small Ones: Label-Free Model Parity Alignment for Efficient Visual Question Answering using Small VLMs](https://arxiv.org/abs/2509.16633)
*Abhirama Subramanyam Penamakuri,Navlika Singh,Piyush Arora,Anand Mishra*

Main category: cs.CV

TL;DR: 本文提出模型均等对齐器（MPA），一个利用无标签图像和大型视觉语言模型（L-VLM）知识，通过识别并解决知识差异，系统性提升小型视觉语言模型（S-VLM）性能的框架。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型（L-VLM）性能卓越但计算成本高昂，不适用于资源受限场景；小型视觉语言模型（S-VLM）高效但性能差距显著。研究旨在缩小S-VLM与L-VLM之间的性能差距，同时保持计算效率。

Method: 引入模型均等对齐器（MPA），该框架利用无标签图像和L-VLM的有效知识迁移。MPA采用基于均等的策略，精确识别S-VLM与L-VLM之间的知识差异，并仅针对这些差异进行优化训练，而非依赖传统知识蒸馏的标注训练数据。

Result: 在TextVQA、ST-VQA、ChartQA和OKVQA四个VQA基准测试上进行的大量实验表明，MPA持续提升了S-VLM的性能，缩小了性能差距，同时保持了计算效率。

Conclusion: MPA是一个有效的框架，通过针对性地解决知识差异，利用L-VLM的知识和无标签数据，显著提升了S-VLM在各种VQA任务上的表现，实现了性能与效率的平衡。

Abstract: Large Vision-Language Models (L-VLMs) have demonstrated remarkable
performance in various vision and language tasks, including visual question
answering (VQA). However, their high computational cost makes them impractical
for resource-constrained settings and inference-heavy applications. In
contrast, Small Vision-Language Models (S-VLMs) offer efficiency but suffer
from a significant performance gap compared to their larger counterparts. In
this work, we introduce the Model Parity Aligner (MPA), a novel framework
designed to systematically improve S-VLMs by leveraging unlabeled images and
effective knowledge transfer from L-VLMs. Instead of traditional knowledge
distillation methods that rely on labeled training data, MPA employs a
strategic parity-based approach that precisely identifies the knowledge
disparities between S-VLMs and L-VLMs, and optimizes training by targeting only
these disparities. We conduct extensive experiments on four diverse VQA
benchmarks, namely TextVQA, ST-VQA, ChartQA, and OKVQA, each of which requires
specialized reasoning capabilities such as text recognition, chart
interpretation, and commonsense and factual understanding. Our results
demonstrate that MPA consistently enhances the performance of S-VLMs on all
benchmarks, reducing the performance gap while maintaining computational
efficiency. We make our code publicly available.

</details>


### [113] [Towards Anytime Retrieval: A Benchmark for Anytime Person Re-Identification](https://arxiv.org/abs/2509.16635)
*Xulin Li,Yan Lu,Bin Liu,Jiaze Li,Qinhong Yang,Tao Gong,Qi Chu,Mang Ye,Nenghai Yu*

Main category: cs.CV

TL;DR: 本文提出了一种名为“随时行人重识别”（AT-ReID）的新任务，旨在实现跨时间多场景的行人检索。为此，作者构建了首个大规模数据集AT-USTC，并提出了一种统一模型Uni-AT，在实验中表现出令人满意的结果和出色的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有行人重识别（ReID）任务和数据集受限于特定时间或场景，无法满足实际应用中在任何时间（包括白天和夜晚、短期到长期）检索目标人物的需求。

Method: 1. 提出“随时行人重识别”（AT-ReID）新任务。2. 构建首个大规模数据集AT-USTC，包含40.3万张图片，由RGB和IR相机在21个月内拍摄，涉及270名志愿者。3. 提出统一模型Uni-AT，包括：多场景ReID（MS-ReID）框架用于学习场景特定特征；属性专家混合（MoAE）模块以减轻场景间干扰；以及分层动态加权（HDW）策略以确保所有场景的平衡训练。

Result: 广泛的实验表明，所提出的Uni-AT模型取得了令人满意的结果，并对所有场景都表现出卓越的泛化能力。

Conclusion: 本文成功定义了随时行人重识别这一新任务，并提供了首个大规模数据集AT-USTC和统一模型Uni-AT，有效解决了跨时间多场景行人检索的挑战。

Abstract: In real applications, person re-identification (ReID) is expected to retrieve
the target person at any time, including both daytime and nighttime, ranging
from short-term to long-term. However, existing ReID tasks and datasets can not
meet this requirement, as they are constrained by available time and only
provide training and evaluation for specific scenarios. Therefore, we
investigate a new task called Anytime Person Re-identification (AT-ReID), which
aims to achieve effective retrieval in multiple scenarios based on variations
in time. To address the AT-ReID problem, we collect the first large-scale
dataset, AT-USTC, which contains 403k images of individuals wearing multiple
clothes captured by RGB and IR cameras. Our data collection spans 21 months,
and 270 volunteers were photographed on average 29.1 times across different
dates or scenes, 4-15 times more than current datasets, providing conditions
for follow-up investigations in AT-ReID. Further, to tackle the new challenge
of multi-scenario retrieval, we propose a unified model named Uni-AT, which
comprises a multi-scenario ReID (MS-ReID) framework for scenario-specific
features learning, a Mixture-of-Attribute-Experts (MoAE) module to alleviate
inter-scenario interference, and a Hierarchical Dynamic Weighting (HDW)
strategy to ensure balanced training across all scenarios. Extensive
experiments show that our model leads to satisfactory results and exhibits
excellent generalization to all scenarios.

</details>


### [114] [Unlocking Hidden Potential in Point Cloud Networks with Attention-Guided Grouping-Feature Coordination](https://arxiv.org/abs/2509.16639)
*Shangzhuo Xie,Qianqian Yang*

Main category: cs.CV

TL;DR: 本文通过引入分组-特征协调模块（GF-Core）和自监督预训练策略，显著提升了传统点基网络在点云分析任务中的性能，同时保持了架构的简洁性。


<details>
  <summary>Details</summary>
Motivation: 现有的点云分析工作主要关注引入新颖的结构设计，而传统的点基架构（通过顺序采样、分组和特征提取层处理原始点）的潜力尚未被充分利用。作者认为，通过战略性的模块集成而非结构修改，可以实现实质性的性能提升。

Method: 本文提出了分组-特征协调模块（GF-Core），这是一个轻量级可分离组件，能够同时调节分组层和特征提取层，以实现更精细的特征聚合。此外，还引入了一种专门为点基输入设计的自监督预训练策略，以增强模型在复杂点云分析场景中的鲁棒性。

Result: 在ModelNet40数据集上，该方法将基线网络的准确率提升至94.0%，与先进框架的性能相当，同时保持了架构的简洁性。在ScanObjectNN数据集的三个变体上，分别获得了2.96%、6.34%和6.32%的改进。

Conclusion: 通过战略性的模块集成（GF-Core）和为点基输入量身定制的自监督预训练策略，可以显著提升传统点基网络在点云分析中的性能，同时保持其架构的简单性，从而释放其未充分利用的潜力。

Abstract: Point cloud analysis has evolved with diverse network architectures, while
existing works predominantly focus on introducing novel structural designs.
However, conventional point-based architectures - processing raw points through
sequential sampling, grouping, and feature extraction layers - demonstrate
underutilized potential. We notice that substantial performance gains can be
unlocked through strategic module integration rather than structural
modifications. In this paper, we propose the Grouping-Feature Coordination
Module (GF-Core), a lightweight separable component that simultaneously
regulates both grouping layer and feature extraction layer to enable more
nuanced feature aggregation. Besides, we introduce a self-supervised
pretraining strategy specifically tailored for point-based inputs to enhance
model robustness in complex point cloud analysis scenarios. On ModelNet40
dataset, our method elevates baseline networks to 94.0% accuracy, matching
advanced frameworks' performance while preserving architectural simplicity. On
three variants of the ScanObjectNN dataset, we obtain improvements of 2.96%,
6.34%, and 6.32% respectively.

</details>


### [115] [ADVEDM:Fine-grained Adversarial Attack against VLM-based Embodied Agents](https://arxiv.org/abs/2509.16645)
*Yichen Wang,Hangtao Zhang,Hewen Pan,Ziqi Zhou,Xianlong Wang,Peijin Guo,Lulu Xue,Shengshan Hu,Minghui Li,Leo Yu Zhang*

Main category: cs.CV

TL;DR: 本文提出ADVEDM，一种针对具身决策（EDM）中视觉-语言模型（VLMs）的细粒度对抗攻击框架。它通过仅修改少数关键对象的感知，同时保留其他区域的语义，使VLM输出有效但错误的决策，对物理世界构成更大的安全威胁。


<details>
  <summary>Details</summary>
Motivation: VLMs在具身决策任务中广泛应用，但现有对抗攻击存在局限性：要么需要完全了解受害者VLM（不切实际），要么因破坏过多语义信息而导致攻击无效，无法影响物理世界交互。因此，需要一种更有效、更实用的攻击方法，能使VLM产生有效但错误的决策。

Method: 本文提出了ADVEDM，一个细粒度对抗攻击框架。它仅修改VLM对少数关键对象的感知，同时保留其余区域的语义，从而有效减少与任务上下文的冲突。该框架设计了两种变体：ADVEDM-R（移除特定对象语义）和ADVEDM-A（添加新对象语义）。

Result: 在通用场景和具身决策任务中的实验结果表明，ADVEDM展现出细粒度控制和卓越的攻击性能。该攻击能使VLM输出有效但错误的决策，从而影响智能体的行动，在物理世界中构成更严重的安全威胁。

Conclusion: ADVEDM是一个有效且实用的细粒度对抗攻击框架，能够针对具身决策中的VLMs生成有效但错误的决策，揭示了物理世界中存在的重大安全威胁。

Abstract: Vision-Language Models (VLMs), with their strong reasoning and planning
capabilities, are widely used in embodied decision-making (EDM) tasks in
embodied agents, such as autonomous driving and robotic manipulation. Recent
research has increasingly explored adversarial attacks on VLMs to reveal their
vulnerabilities. However, these attacks either rely on overly strong
assumptions, requiring full knowledge of the victim VLM, which is impractical
for attacking VLM-based agents, or exhibit limited effectiveness. The latter
stems from disrupting most semantic information in the image, which leads to a
misalignment between the perception and the task context defined by system
prompts. This inconsistency interrupts the VLM's reasoning process, resulting
in invalid outputs that fail to affect interactions in the physical world. To
this end, we propose a fine-grained adversarial attack framework, ADVEDM, which
modifies the VLM's perception of only a few key objects while preserving the
semantics of the remaining regions. This attack effectively reduces conflicts
with the task context, making VLMs output valid but incorrect decisions and
affecting the actions of agents, thus posing a more substantial safety threat
in the physical world. We design two variants of based on this framework,
ADVEDM-R and ADVEDM-A, which respectively remove the semantics of a specific
object from the image and add the semantics of a new object into the image. The
experimental results in both general scenarios and EDM tasks demonstrate
fine-grained control and excellent attack performance.

</details>


### [116] [Are VLMs Ready for Lane Topology Awareness in Autonomous Driving?](https://arxiv.org/abs/2509.16654)
*Xin Chen,Jia He,Maozheng Li,Dongliang Xu,Tianyu Wang,Yixiao Chen,Zhixin Lin,Yue Yao*

Main category: cs.CV

TL;DR: 本文系统评估了视觉-语言模型（VLMs）在自动驾驶道路拓扑理解方面的能力，发现其在空间推理，尤其是时序问题上仍是主要瓶颈。


<details>
  <summary>Details</summary>
Motivation: 尽管VLMs在多模态推理方面取得了显著进展，但在自动驾驶中的应用有限，尤其是在对安全导航至关重要的道路拓扑理解方面表现不佳。

Method: 将多视角图像投影并融合到统一的鸟瞰图（BEV）车道中，并基于这些BEV车道设计了四个拓扑相关的诊断性视觉问答（VQA）任务，以捕捉空间拓扑推理的关键要素。

Result: 前沿闭源模型（如GPT-4o）在某些任务中表现相对较高，但在人类能回答的时序问题（例如，vector分类任务中仅67.8%）上仍有不足。开源VLMs（即使是30B规模）表现显著挣扎。研究表明空间推理是当前VLMs的根本瓶颈。模型能力与模型大小、推理token长度和提供的示例数量呈正相关。

Conclusion: 空间推理仍然是当前VLMs在道路拓扑理解方面的一个基本瓶颈。未来研究方向应关注模型大小、推理token长度和示例数量的优化，以提升其空间推理能力。

Abstract: Vision-Language Models (VLMs) have recently shown remarkable progress in
multimodal reasoning, yet their applications in autonomous driving remain
limited. In particular, the ability to understand road topology, a key
requirement for safe navigation, has received relatively little attention.
While some recent works have begun to explore VLMs in driving contexts, their
performance on topology reasoning is far from satisfactory. In this work, we
systematically evaluate VLMs' capabilities in road topology understanding.
Specifically, multi-view images are projected into unified ground-plane
coordinate system and fused into bird's-eye-view (BEV) lanes. Based on these
BEV lanes, we formulate four topology-related diagnostic VQA tasks, which
together capture essential components of spatial topology reasoning. Through
extensive evaluation, we find that while frontier closed-source models (e.g.,
GPT-4o) achieve relatively high accuracy in some tasks, they still fail in some
temporal questions that humans can answer (e.g., GPT-4o achieve only 67.8% in
vector, a two-class classification problem). Furthermore, we find open-source
VLMs, even at 30B scale, struggle significantly. These results indicate that
spatial reasoning remains a fundamental bottleneck for current VLMs. We also
find that the model's capability is positively correlated with model size,
length of reasoning tokens and shots provided as examples, showing direction
for future research.

</details>


### [117] [MedCutMix: A Data-Centric Approach to Improve Radiology Vision-Language Pre-training with Disease Awareness](https://arxiv.org/abs/2509.16673)
*Sinuo Wang,Yutong Xie,Yuyuan Liu,Qi Wu*

Main category: cs.CV

TL;DR: 本文提出MedCutMix，一种新颖的多模态疾病中心数据增强方法，通过诊断语句的CutMix和跨模态注意力引导图像增强，以提升放射学VLP的性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言预训练（VLP）在减少手动标注和增强语义理解方面日益受到关注。然而，其对图像-文本数据集的依赖面临隐私问题和高昂的标注成本。现有数据增强方法因多样性不足，难以捕捉医学数据中细微复杂的变异。

Method: 本文提出了MedCutMix。该方法在医疗报告中执行诊断语句的CutMix操作，并建立诊断语句与医学图像之间的交叉注意力，以指导图像模态内部的注意力流形混合（attentive manifold mix）。

Result: MedCutMix在四个下游放射学诊断数据集上超越了现有方法，证明了其在提升放射学VLP性能和泛化能力方面的有效性。

Conclusion: MedCutMix是一种有效的数据增强方法，能够显著提升放射学视觉-语言预训练任务的性能和泛化能力，解决了医学领域数据稀缺和隐私挑战。

Abstract: Vision-Language Pre-training (VLP) is drawing increasing interest for its
ability to minimize manual annotation requirements while enhancing semantic
understanding in downstream tasks. However, its reliance on image-text datasets
poses challenges due to privacy concerns and the high cost of obtaining paired
annotations. Data augmentation emerges as a viable strategy to address this
issue, yet existing methods often fall short of capturing the subtle and
complex variations in medical data due to limited diversity. To this end, we
propose MedCutMix, a novel multi-modal disease-centric data augmentation
method. MedCutMix performs diagnostic sentence CutMix within medical reports
and establishes the cross-attention between the diagnostic sentence and medical
image to guide attentive manifold mix within the imaging modality. Our approach
surpasses previous methods across four downstream radiology diagnosis datasets,
highlighting its effectiveness in enhancing performance and generalizability in
radiology VLP.

</details>


### [118] [FitPro: A Zero-Shot Framework for Interactive Text-based Pedestrian Retrieval in Open World](https://arxiv.org/abs/2509.16674)
*Zengli Luo,Canlong Zhang,Xiaochun Lu,Zhixin Li*

Main category: cs.CV

TL;DR: FitPro是一个开放世界交互式零样本行人检索框架，通过特征对比解码、增量语义挖掘和查询感知分层检索，显著提升了语义理解和跨场景适应性，克服了现有方法的泛化和语义建模限制。


<details>
  <summary>Details</summary>
Motivation: 现有的文本行人检索（TPR）方法在受限设置下取得进展，但在开放世界场景中的交互式检索仍面临模型泛化能力有限和语义理解不足的挑战。

Method: 本文提出了FitPro框架，包含三个创新组件：1) 特征对比解码（FCD），通过提示引导的对比解码从去噪图像生成高质量结构化行人描述，缓解零样本场景中的语义漂移。2) 增量语义挖掘（ISM），从多视角观测构建整体行人表示，实现多轮交互中的全局语义建模，提高对视角变化和描述细微变化的鲁棒性。3) 查询感知分层检索（QHR），根据查询类型动态优化检索流程，高效适应多模态和多视角输入。

Result: 在五个公共数据集和两种评估协议上的大量实验表明，FitPro显著克服了现有方法在交互式检索中的泛化局限性和语义建模约束。

Conclusion: FitPro为交互式行人检索的实际部署铺平了道路，通过增强语义理解和跨场景适应性，解决了开放世界交互式零样本行人检索中的关键挑战。

Abstract: Text-based Pedestrian Retrieval (TPR) aims to retrieve specific target
pedestrians in visual scenes according to natural language descriptions.
Although existing methods have achieved progress under constrained settings,
interactive retrieval in the open-world scenario still suffers from limited
model generalization and insufficient semantic understanding. To address these
challenges, we propose FitPro, an open-world interactive zero-shot TPR
framework with enhanced semantic comprehension and cross-scene adaptability.
FitPro has three innovative components: Feature Contrastive Decoding (FCD),
Incremental Semantic Mining (ISM), and Query-aware Hierarchical Retrieval
(QHR). The FCD integrates prompt-guided contrastive decoding to generate
high-quality structured pedestrian descriptions from denoised images,
effectively alleviating semantic drift in zero-shot scenarios. The ISM
constructs holistic pedestrian representations from multi-view observations to
achieve global semantic modeling in multi-turn interactions,thereby improving
robustness against viewpoint shifts and fine-grained variations in
descriptions. The QHR dynamically optimizes the retrieval pipeline according to
query types, enabling efficient adaptation to multi-modal and multi-view
inputs. Extensive experiments on five public datasets and two evaluation
protocols demonstrate that FitPro significantly overcomes the generalization
limitations and semantic modeling constraints of existing methods in
interactive retrieval, paving the way for practical deployment. The code and
data will be released at https://github.com/
lilo4096/FitPro-Interactive-Person-Retrieval.

</details>


### [119] [IPF-RDA: An Information-Preserving Framework for Robust Data Augmentation](https://arxiv.org/abs/2509.16678)
*Suorong Yang,Hongchao Yang,Suhan Guo,Furao Shen,Jian Zhao*

Main category: cs.CV

TL;DR: 数据增强虽能提升深度模型泛化能力，但可能引入分布偏移和噪声。本文提出IPF-RDA框架，通过类别判别信息估计和信息保留方案，增强数据增强的鲁棒性，并在一系列数据集上显著提升了现有数据增强方法的性能。


<details>
  <summary>Details</summary>
Motivation: 数据增强虽然是提高深度模型泛化性能的有效技术，但它不可避免地会引入分布偏移和噪声，从而严重限制了深度网络的潜力并降低了其性能。

Method: 本文提出了一个新颖的信息保留框架IPF-RDA，以增强数据增强的鲁棒性。该框架结合了：(i) 一种新的类别判别信息估计算法，用于识别最易受数据增强操作影响的点及其重要性分数；(ii) 一种新的信息保留方案，用于保留增强样本中的关键信息并自适应地确保增强数据的多样性。IPF-RDA根据操作类型将数据增强方法分为三类并相应地集成到框架中。

Result: 实验表明，IPF-RDA框架虽然简单，但能持续改进各种常用最先进数据增强方法在流行深度模型和多种数据集（包括CIFAR-10、CIFAR-100、Tiny-ImageNet、CUHK03、Market1501、Oxford Flower和MNIST）上的性能，验证了其性能和可扩展性。它能增强数据增强方法的鲁棒性并释放其全部潜力。

Conclusion: IPF-RDA是一个有效的信息保留框架，能够显著增强数据增强方法的鲁棒性，并通过保留关键信息和确保多样性来提升深度模型的性能，从而克服了现有数据增强引入分布偏移和噪声的问题。

Abstract: Data augmentation is widely utilized as an effective technique to enhance the
generalization performance of deep models. However, data augmentation may
inevitably introduce distribution shifts and noises, which significantly
constrain the potential and deteriorate the performance of deep networks. To
this end, we propose a novel information-preserving framework, namely IPF-RDA,
to enhance the robustness of data augmentations in this paper. IPF-RDA combines
the proposal of (i) a new class-discriminative information estimation algorithm
that identifies the points most vulnerable to data augmentation operations and
corresponding importance scores; And (ii) a new information-preserving scheme
that preserves the critical information in the augmented samples and ensures
the diversity of augmented data adaptively. We divide data augmentation methods
into three categories according to the operation types and integrate these
approaches into our framework accordingly. After being integrated into our
framework, the robustness of data augmentation methods can be enhanced and
their full potential can be unleashed. Extensive experiments demonstrate that
although being simple, IPF-RDA consistently improves the performance of
numerous commonly used state-of-the-art data augmentation methods with popular
deep models on a variety of datasets, including CIFAR-10, CIFAR-100,
Tiny-ImageNet, CUHK03, Market1501, Oxford Flower, and MNIST, where its
performance and scalability are stressed. The implementation is available at
https://github.com/Jackbrocp/IPF-RDA.

</details>


### [120] [ProtoVQA: An Adaptable Prototypical Framework for Explainable Fine-Grained Visual Question Answering](https://arxiv.org/abs/2509.16680)
*Xingjian Diao,Weiyi Wu,Keyi Kong,Peijun Qing,Xinwen Xu,Ming Cheng,Soroush Vosoughi,Jiang Gui*

Main category: cs.CV

TL;DR: ProtoVQA是一个统一的原型框架，通过学习问题感知原型和空间约束匹配，为视觉问答（VQA）提供可信、细粒度的解释，同时保持竞争力。


<details>
  <summary>Details</summary>
Motivation: VQA模型在医疗和自动驾驶等关键领域不仅需要准确的答案，还需要人类易于理解和验证的解释。虽然基于原型的方法在纯视觉推理任务中显示出可解释性潜力，但在VQA领域尚未得到充分探索。

Method: 本文提出了ProtoVQA，一个统一的原型框架，它(i)学习问题感知原型作为推理锚点，将答案与判别性图像区域关联；(ii)应用空间约束匹配以确保所选证据的连贯性和语义相关性；(iii)通过共享原型骨干支持回答和定位任务。为评估解释质量，提出了视觉-语言对齐分数（VLAS），衡量模型关注区域与真实证据的对齐程度。

Result: 在Visual7W数据集上的实验表明，ProtoVQA在保持竞争性准确率的同时，生成了忠实、细粒度的解释。

Conclusion: ProtoVQA的提出推动了透明和可信赖VQA系统的发展。

Abstract: Visual Question Answering (VQA) is increasingly used in diverse applications
ranging from general visual reasoning to safety-critical domains such as
medical imaging and autonomous systems, where models must provide not only
accurate answers but also explanations that humans can easily understand and
verify. Prototype-based modeling has shown promise for interpretability by
grounding predictions in semantically meaningful regions for purely visual
reasoning tasks, yet remains underexplored in the context of VQA. We present
ProtoVQA, a unified prototypical framework that (i) learns question-aware
prototypes that serve as reasoning anchors, connecting answers to
discriminative image regions, (ii) applies spatially constrained matching to
ensure that the selected evidence is coherent and semantically relevant, and
(iii) supports both answering and grounding tasks through a shared prototype
backbone. To assess explanation quality, we propose the Visual-Linguistic
Alignment Score (VLAS), which measures how well the model's attended regions
align with ground-truth evidence. Experiments on Visual7W show that ProtoVQA
yields faithful, fine-grained explanations while maintaining competitive
accuracy, advancing the development of transparent and trustworthy VQA systems.

</details>


### [121] [Active View Selection for Scene-level Multi-view Crowd Counting and Localization with Limited Labels](https://arxiv.org/abs/2509.16684)
*Qi Zhang,Bin Li,Antoni B. Chan,Hui Huang*

Main category: cs.CV

TL;DR: 本文提出了一种针对多视角人群计数和定位的视角选择方法，旨在选择最佳相机视角以更好地感知场景中的所有人群，并解决了现有方法对大量标签和跨场景能力不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有多视角人群计数和定位方法主要关注输入视角中的准确预测，但忽略了选择“最佳”相机视角以全面感知场景中所有人群的问题。此外，现有的视角选择方法需要大量标注数据且缺乏跨场景能力，限制了其应用场景。

Method: 本文首先提出了一种独立视角选择方法（IVS），该方法在视角选择策略中考虑视角和场景几何信息，并独立地进行视角选择、标注和下游任务。在此基础上，进一步提出了一种主动视角选择方法（AVS），该方法联合优化视角选择、标注和下游任务，主动选择标注视角，并在选择过程中同时考虑视角/场景几何信息和下游任务模型的预测结果。

Result: 在多视角计数和定位任务上的实验表明，所提出的主动视角选择方法（AVS）具有跨场景能力和有限标签需求优势，优于现有方法，并拥有更广泛的应用场景。

Conclusion: AVS方法通过主动选择最佳相机视角，有效解决了多视角人群计数和定位中视角选择的问题，特别是在跨场景和有限标签需求方面表现出色，为该领域提供了新的解决方案和更广泛的应用潜力。

Abstract: Multi-view crowd counting and localization fuse the input multi-views for
estimating the crowd number or locations on the ground. Existing methods mainly
focus on accurately predicting on the crowd shown in the input views, which
neglects the problem of choosing the `best' camera views to perceive all crowds
well in the scene. Besides, existing view selection methods require massive
labeled views and images, and lack the ability for cross-scene settings,
reducing their application scenarios. Thus, in this paper, we study the view
selection issue for better scene-level multi-view crowd counting and
localization results with cross-scene ability and limited label demand, instead
of input-view-level results. We first propose an independent view selection
method (IVS) that considers view and scene geometries in the view selection
strategy and conducts the view selection, labeling, and downstream tasks
independently. Based on IVS, we also put forward an active view selection
method (AVS) that jointly optimizes the view selection, labeling, and
downstream tasks. In AVS, we actively select the labeled views and consider
both the view/scene geometries and the predictions of the downstream task
models in the view selection process. Experiments on multi-view counting and
localization tasks demonstrate the cross-scene and the limited label demand
advantages of the proposed active view selection method (AVS), outperforming
existing methods and with wider application scenarios.

</details>


### [122] [Towards a Transparent and Interpretable AI Model for Medical Image Classifications](https://arxiv.org/abs/2509.16685)
*Binbin Wen,Yihang Wu,Tareef Daqqaq,Ahmad Chaddad*

Main category: cs.CV

TL;DR: 本文探讨了可解释人工智能（XAI）在医学领域的应用，旨在提高AI决策的透明度和可解释性，并通过模拟展示了其对医疗决策的改进作用，并讨论了XAI面临的挑战。


<details>
  <summary>Details</summary>
Motivation: 人工智能在医学领域的应用取得了显著进展，但复杂AI模型的内在不透明性给其临床实用性带来了巨大挑战。

Method: 研究主要通过使用各种医学数据集进行模拟，以阐明XAI模型的内部工作原理。此外，还对主要的XAI方法进行了综述。

Result: 数据集驱动的模拟表明，XAI能够有效解释AI预测，从而改善医疗专业人员的决策过程。研究还讨论了XAI领域当前面临的挑战。

Conclusion: 研究强调了持续开发和探索XAI的必要性，特别是从多样化医学数据集的角度，以促进其在医疗领域的采纳和有效性。

Abstract: The integration of artificial intelligence (AI) into medicine is remarkable,
offering advanced diagnostic and therapeutic possibilities. However, the
inherent opacity of complex AI models presents significant challenges to their
clinical practicality. This paper focuses primarily on investigating the
application of explainable artificial intelligence (XAI) methods, with the aim
of making AI decisions transparent and interpretable. Our research focuses on
implementing simulations using various medical datasets to elucidate the
internal workings of the XAI model. These dataset-driven simulations
demonstrate how XAI effectively interprets AI predictions, thus improving the
decision-making process for healthcare professionals. In addition to a survey
of the main XAI methods and simulations, ongoing challenges in the XAI field
are discussed. The study highlights the need for the continuous development and
exploration of XAI, particularly from the perspective of diverse medical
datasets, to promote its adoption and effectiveness in the healthcare domain.

</details>


### [123] [Spectral Compressive Imaging via Chromaticity-Intensity Decomposition](https://arxiv.org/abs/2509.16690)
*Xiaodong Wang,Zijun He,Ping Wang,Lishun Wang,Yanan Hu,Xin Yuan*

Main category: cs.CV

TL;DR: 本文提出一种色度-强度分解框架，将高光谱图像（HSIs）分解为强度图和色度立方体，并开发了CIDNet网络，在双相机CASSI系统中实现卓越的HSIs重建，特别是在光谱和色度保真度方面。


<details>
  <summary>Details</summary>
Motivation: CASSI（编码孔径快照光谱成像）测量将空间和光谱信息纠缠，导致高光谱图像重建的严重病态逆问题。此外，捕获的辐射度依赖于场景光照，难以恢复不受光照条件影响的固有光谱反射率。

Method: 提出色度-强度分解框架，将HSIs分解为空间平滑的强度图和光谱变化的色度立方体，其中色度编码了光照不变的反射率。在此基础上，开发了CIDNet，一个在双相机CASSI系统中的色度-强度分解展开网络。CIDNet集成了混合空间-光谱Transformer用于重建细粒度、稀疏的光谱色度，以及一个降级感知、空间自适应噪声估计模块来捕获迭代阶段的各向异性噪声。

Result: 在合成和真实世界的CASSI数据集上的大量实验表明，该方法在光谱和色度保真度方面均取得了卓越的性能。

Conclusion: 通过色度-强度分解框架和CIDNet，成功解决了CASSI重建中的病态逆问题和光照依赖性，实现了高质量的高光谱图像和光照不变色度的重建。

Abstract: In coded aperture snapshot spectral imaging (CASSI), the captured measurement
entangles spatial and spectral information, posing a severely ill-posed inverse
problem for hyperspectral images (HSIs) reconstruction. Moreover, the captured
radiance inherently depends on scene illumination, making it difficult to
recover the intrinsic spectral reflectance that remains invariant to lighting
conditions. To address these challenges, we propose a chromaticity-intensity
decomposition framework, which disentangles an HSI into a spatially smooth
intensity map and a spectrally variant chromaticity cube. The chromaticity
encodes lighting-invariant reflectance, enriched with high-frequency spatial
details and local spectral sparsity. Building on this decomposition, we develop
CIDNet, a Chromaticity-Intensity Decomposition unfolding network within a
dual-camera CASSI system. CIDNet integrates a hybrid spatial-spectral
Transformer tailored to reconstruct fine-grained and sparse spectral
chromaticity and a degradation-aware, spatially-adaptive noise estimation
module that captures anisotropic noise across iterative stages. Extensive
experiments on both synthetic and real-world CASSI datasets demonstrate that
our method achieves superior performance in both spectral and chromaticity
fidelity. Code and models will be publicly available.

</details>


### [124] [InstanceAssemble: Layout-Aware Image Generation via Instance Assembling Attention](https://arxiv.org/abs/2509.16691)
*Qiang Xiang,Shuang Sun,Binglei Li,Dejia Song,Huaxia Li,Nemo Chen,Xu Tang,Yao Hu,Junping Zhang*

Main category: cs.CV

TL;DR: 本文提出InstanceAssemble，一种新的L2I生成架构，通过实例组装注意力实现精确的布局控制和多模态内容生成。同时引入Denselayout基准和LGS评估指标，实验证明InstanceAssemble在复杂布局下表现出最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型在图像生成方面表现出色，且L2I生成取得了进展，但当前的L2I方法性能仍不理想，需要更精确和可控的图像合成。

Method: 本文提出InstanceAssemble架构，通过实例组装注意力整合布局条件，实现边界框的位置控制和文本、额外视觉内容的多模态内容控制。该方法通过轻量级LoRA模块灵活适应现有基于DiT的T2I模型。此外，还提出了Layout-to-Image基准Denselayout（包含5k图像和90k实例）以及可解释的评估指标Layout Grounding Score (LGS)来更精确地评估L2I生成的准确性。

Result: 实验表明，InstanceAssemble方法在复杂布局条件下实现了最先进的性能，并与多种风格的LoRA模块展现出强大的兼容性。

Conclusion: InstanceAssemble通过创新的架构显著提升了L2I生成的控制精度和性能，并通过引入新的基准和评估指标，为该领域的发展提供了有力支持。

Abstract: Diffusion models have demonstrated remarkable capabilities in generating
high-quality images. Recent advancements in Layout-to-Image (L2I) generation
have leveraged positional conditions and textual descriptions to facilitate
precise and controllable image synthesis. Despite overall progress, current L2I
methods still exhibit suboptimal performance. Therefore, we propose
InstanceAssemble, a novel architecture that incorporates layout conditions via
instance-assembling attention, enabling position control with bounding boxes
(bbox) and multimodal content control including texts and additional visual
content. Our method achieves flexible adaption to existing DiT-based T2I models
through light-weighted LoRA modules. Additionally, we propose a Layout-to-Image
benchmark, Denselayout, a comprehensive benchmark for layout-to-image
generation, containing 5k images with 90k instances in total. We further
introduce Layout Grounding Score (LGS), an interpretable evaluation metric to
more precisely assess the accuracy of L2I generation. Experiments demonstrate
that our InstanceAssemble method achieves state-of-the-art performance under
complex layout conditions, while exhibiting strong compatibility with diverse
style LoRA modules.

</details>


### [125] [Animalbooth: multimodal feature enhancement for animal subject personalization](https://arxiv.org/abs/2509.16702)
*Chen Liu,Haitao Wu,Kafeng Wang,Xiaowang Zhang*

Main category: cs.CV

TL;DR: AnimalBooth是一个用于个性化动物图像生成的框架，通过强化身份保持和频率控制的特征集成，实现了从全局结构到细节纹理的精细生成，并引入了AnimalBench数据集。


<details>
  <summary>Details</summary>
Motivation: 个性化动物图像生成面临挑战，因为动物外观线索丰富且形态变异大。现有方法常出现跨域特征错位，导致身份漂移。

Method: 本文提出了AnimalBooth框架，包含：1) Animal Net和自适应注意力模块，以强化身份保持并减轻跨域对齐错误。2) 频率控制特征集成模块，在潜在空间应用离散余弦变换（DCT）滤波，引导扩散过程从粗到细（从全局结构到详细纹理）。3) 策划并发布了高分辨率的AnimalBench数据集，以推动该领域研究。

Result: 大量实验表明，AnimalBooth在多个基准测试中持续优于现有强基线，显著提高了身份保真度和感知质量。

Conclusion: AnimalBooth有效解决了个性化动物图像生成中的身份保持和细节控制问题，通过创新方法和新数据集，显著提升了生成图像的质量和身份一致性。

Abstract: Personalized animal image generation is challenging due to rich appearance
cues and large morphological variability. Existing approaches often exhibit
feature misalignment across domains, which leads to identity drift. We present
AnimalBooth, a framework that strengthens identity preservation with an Animal
Net and an adaptive attention module, mitigating cross domain alignment errors.
We further introduce a frequency controlled feature integration module that
applies Discrete Cosine Transform filtering in the latent space to guide the
diffusion process, enabling a coarse to fine progression from global structure
to detailed texture. To advance research in this area, we curate AnimalBench, a
high resolution dataset for animal personalization. Extensive experiments show
that AnimalBooth consistently outperforms strong baselines on multiple
benchmarks and improves both identity fidelity and perceptual quality.

</details>


### [126] [When Confidence Fails: Revisiting Pseudo-Label Selection in Semi-supervised Semantic Segmentation](https://arxiv.org/abs/2509.16704)
*Pan Liu,Jinshi Liu*

Main category: cs.CV

TL;DR: 本文提出置信度可分离学习（CSL），通过在置信度分布特征空间中进行凸优化和随机掩码，解决了半监督语义分割中伪标签选择的过置信和低置信度区域上下文丢失问题。


<details>
  <summary>Details</summary>
Motivation: 现有的伪标签选择方法通常使用固定的置信度阈值，这无法应对网络过置信（正确与错误预测在高置信度区域重叠）以及直接丢弃低置信度预测导致的上下文丢失和空间-语义不连续性问题。

Method: CSL将伪标签选择公式化为置信度分布特征空间中的凸优化问题，建立样本特定的决策边界以区分可靠和不可靠预测。此外，CSL引入对可靠像素的随机掩码，引导网络从低可靠性区域学习上下文关系，以减轻丢弃不确定预测带来的负面影响。

Result: 在Pascal、Cityscapes和COCO基准测试上的大量实验结果表明，CSL优于现有最先进的方法。

Conclusion: CSL通过解决网络过置信和低置信度区域的上下文丢失问题，显著改进了半监督语义分割中的伪标签选择，取得了SOTA性能。

Abstract: While significant advances exist in pseudo-label generation for
semi-supervised semantic segmentation, pseudo-label selection remains
understudied. Existing methods typically use fixed confidence thresholds to
retain high-confidence predictions as pseudo-labels. However, these methods
cannot cope with network overconfidence tendency, where correct and incorrect
predictions overlap significantly in high-confidence regions, making separation
challenging and amplifying model cognitive bias. Meanwhile, the direct
discarding of low-confidence predictions disrupts spatial-semantic continuity,
causing critical context loss. We propose Confidence Separable Learning (CSL)
to address these limitations. CSL formulates pseudo-label selection as a convex
optimization problem within the confidence distribution feature space,
establishing sample-specific decision boundaries to distinguish reliable from
unreliable predictions. Additionally, CSL introduces random masking of reliable
pixels to guide the network in learning contextual relationships from
low-reliability regions, thereby mitigating the adverse effects of discarding
uncertain predictions. Extensive experimental results on the Pascal,
Cityscapes, and COCO benchmarks show that CSL performs favorably against
state-of-the-art methods. Code and model weights are available at
https://github.com/PanLiuCSU/CSL.

</details>


### [127] [Text-Scene: A Scene-to-Language Parsing Framework for 3D Scene Understanding](https://arxiv.org/abs/2509.16721)
*Haoyuan Li,Rui Liu,Hehe Fan,Yi Yang*

Main category: cs.CV

TL;DR: 本文提出了Text-Scene框架，能自动将3D场景解析为文本描述，以弥合3D观察与语言之间的鸿沟，并引入了InPlan3D基准，用于评估多模态大语言模型在3D任务规划中的推理能力。


<details>
  <summary>Details</summary>
Motivation: 将多模态大语言模型（MLLMs）在2D图像理解上的能力扩展到3D场景面临挑战：1) 3D环境涉及更丰富的概念（如空间关系、功能、物理、布局等）；2) 缺乏大规模的3D视觉-语言数据集。

Method: Text-Scene框架自动将3D场景解析为文本描述。它通过几何分析和多模态大语言模型，识别物体属性和空间关系，然后生成整个场景的连贯摘要，无需人工干预。此外，本文还提出了InPlan3D，一个包含3174个长期规划任务和636个室内场景的3D任务规划基准。

Result: Text-Scene生成的描述准确、详细且人类可解释，能捕捉物体级细节和全局上下文。实验结果表明，其文本解析能忠实地表示3D场景并有益于下游任务。InPlan3D基准用于评估多模态大语言模型的3D任务规划推理能力。

Conclusion: 本文通过Text-Scene框架，成功地将3D场景内容转化为可理解的语言描述，解决了具身人工智能系统理解和交互复杂3D场景的根本挑战。Text-Scene的文本解析忠实地代表了3D场景并能支持下游任务，同时InPlan3D为3D任务规划提供了重要的评估基准。

Abstract: Enabling agents to understand and interact with complex 3D scenes is a
fundamental challenge for embodied artificial intelligence systems. While
Multimodal Large Language Models (MLLMs) have achieved significant progress in
2D image understanding, extending such capabilities to 3D scenes remains
difficult: 1) 3D environment involves richer concepts such as spatial
relationships, affordances, physics, layout, and so on, 2) the absence of
large-scale 3D vision-language datasets has posed a significant obstacle. In
this paper, we introduce Text-Scene, a framework that automatically parses 3D
scenes into textual descriptions for scene understanding. Given a 3D scene, our
model identifies object attributes and spatial relationships, and then
generates a coherent summary of the whole scene, bridging the gap between 3D
observation and language without requiring human-in-the-loop intervention. By
leveraging both geometric analysis and MLLMs, Text-Scene produces descriptions
that are accurate, detailed, and human-interpretable, capturing object-level
details and global-level context. Experimental results on benchmarks
demonstrate that our textual parses can faithfully represent 3D scenes and
benefit downstream tasks. To evaluate the reasoning capability of MLLMs, we
present InPlan3D, a comprehensive benchmark for 3D task planning, consisting of
3174 long-term planning tasks across 636 indoor scenes. We emphasize clarity
and accessibility in our approach, aiming to make 3D scene content
understandable through language. Code and datasets will be released.

</details>


### [128] [Pain in 3D: Generating Controllable Synthetic Faces for Automated Pain Assessment](https://arxiv.org/abs/2509.16727)
*Xin Lei Lin,Soroush Mehraban,Abhishek Moturu,Babak Taati*

Main category: cs.CV

TL;DR: 本文提出了3DPain，一个大规模、多样化且标注丰富的合成数据集，用于解决现有面部表情自动疼痛评估数据集中存在的偏差和控制不足问题。同时引入了ViTPain，一个基于Vision Transformer的跨模态蒸馏框架，以提高疼痛评估的准确性、可解释性和临床可靠性。


<details>
  <summary>Details</summary>
Motivation: 自动面部表情疼痛评估（特别是针对无法沟通的患者，如痴呆症患者）面临两大挑战：(i) 现有数据集因伦理限制存在严重的人口统计学和标签不平衡；(ii) 当前的生成模型无法精确控制面部动作单元（AUs）、面部结构或临床验证的疼痛水平。

Method: 本文提出了一个三阶段框架来生成3DPain数据集：首先生成多样化的3D网格，然后使用扩散模型进行纹理化，最后应用AU驱动的面部绑定来合成具有配对中性/疼痛图像、AU配置、PSPI分数和疼痛区域热图的多视角人脸。此外，引入了ViTPain，一个基于Vision Transformer的跨模态蒸馏框架，其中一个经过热图训练的教师模型指导一个经过RGB图像训练的学生模型。

Result: 3DPain数据集包含82,500个样本，涵盖25,000个疼痛表情热图和2,500个在年龄、性别和种族方面均衡的合成身份，首次提供了数据集层面的疼痛区域热图标注。ViTPain框架提高了疼痛评估的准确性、可解释性和临床可靠性。3DPain和ViTPain共同为可泛化的自动化疼痛评估奠定了可控、多样化且具有临床基础的基石。

Conclusion: 3DPain和ViTPain共同为可泛化的自动化疼痛评估建立了一个可控、多样化且具有临床基础的平台，有效解决了现有数据集的局限性和模型控制不足的问题，从而推动了该领域的发展。

Abstract: Automated pain assessment from facial expressions is crucial for
non-communicative patients, such as those with dementia. Progress has been
limited by two challenges: (i) existing datasets exhibit severe demographic and
label imbalance due to ethical constraints, and (ii) current generative models
cannot precisely control facial action units (AUs), facial structure, or
clinically validated pain levels.
  We present 3DPain, a large-scale synthetic dataset specifically designed for
automated pain assessment, featuring unprecedented annotation richness and
demographic diversity. Our three-stage framework generates diverse 3D meshes,
textures them with diffusion models, and applies AU-driven face rigging to
synthesize multi-view faces with paired neutral and pain images, AU
configurations, PSPI scores, and the first dataset-level annotations of
pain-region heatmaps. The dataset comprises 82,500 samples across 25,000 pain
expression heatmaps and 2,500 synthetic identities balanced by age, gender, and
ethnicity.
  We further introduce ViTPain, a Vision Transformer based cross-modal
distillation framework in which a heatmap-trained teacher guides a student
trained on RGB images, enhancing accuracy, interpretability, and clinical
reliability. Together, 3DPain and ViTPain establish a controllable, diverse,
and clinically grounded foundation for generalizable automated pain assessment.

</details>


### [129] [Min: Mixture of Noise for Pre-Trained Model-Based Class-Incremental Learning](https://arxiv.org/abs/2509.16738)
*Kai Jiang,Zhengyan Shi,Dell Zhang,Hongyuan Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: 针对类别增量学习（CIL）中预训练模型（PTMs）的参数漂移问题，本文提出了一种名为Mixture of Noise (Min)的方法，通过信息论指导学习有益噪声来减轻骨干网络泛化能力的退化，并在多个基准数据集上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有CIL方法在PTMs上进行轻量级微调时，会导致参数漂移，损害模型的泛化能力。虽然参数漂移可视为噪声，但研究表明噪声并非总是 M有害的，适当的噪声可以抑制低相关特征，为未来任务留出空间。因此，作者旨在学习有益噪声来解决这一问题。

Method: 本文提出了Mixture of Noise (Min)方法，通过信息论指导学习有益噪声。具体步骤包括：1) 从新任务的高维特征中学习任务特定噪声；2) 动态调整一组权重以优化不同任务噪声的混合；3) 将有益噪声嵌入到中间特征中，以屏蔽低效模式的响应。

Result: Min在六个基准数据集上进行了广泛实验，在大多数增量设置中实现了最先进的性能，尤其在50步增量设置中表现突出。

Conclusion: 研究结果表明，有益噪声在持续学习中具有显著潜力，能够有效缓解预训练模型在适应新任务时泛化能力的下降。

Abstract: Class Incremental Learning (CIL) aims to continuously learn new categories
while retaining the knowledge of old ones. Pre-trained models (PTMs) show
promising capabilities in CIL. However, existing approaches that apply
lightweight fine-tuning to backbones still induce parameter drift, thereby
compromising the generalization capability of pre-trained models. Parameter
drift can be conceptualized as a form of noise that obscures critical patterns
learned for previous tasks. However, recent researches have shown that noise is
not always harmful. For example, the large number of visual patterns learned
from pre-training can be easily abused by a single task, and introducing
appropriate noise can suppress some low-correlation features, thus leaving a
margin for future tasks. To this end, we propose learning beneficial noise for
CIL guided by information theory and propose Mixture of Noise (Min), aiming to
mitigate the degradation of backbone generalization from adapting new tasks.
Specifically, task-specific noise is learned from high-dimension features of
new tasks. Then, a set of weights is adjusted dynamically for optimal mixture
of different task noise. Finally, Min embeds the beneficial noise into the
intermediate features to mask the response of inefficient patterns. Extensive
experiments on six benchmark datasets demonstrate that Min achieves
state-of-the-art performance in most incremental settings, with particularly
outstanding results in 50-steps incremental settings. This shows the
significant potential for beneficial noise in continual learning.

</details>


### [130] [CAMBench-QR : A Structure-Aware Benchmark for Post-Hoc Explanations with QR Understanding](https://arxiv.org/abs/2509.16745)
*Ritabrata Chakraborty,Avijit Dasgupta,Sandeep Chaurasia*

Main category: cs.CV

TL;DR: 本文提出了CAMBench-QR，一个结构感知的基准测试平台，利用QR码的几何特性来评估CAM方法是否能准确识别必要子结构并避免背景区域，以判断视觉解释是否真正结构感知。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉解释（如CAMs）通常看起来合理，但在结构上并不忠实，即它们可能无法准确地将显著性放在对象所需的子结构上。因此，需要一种方法来测试CAM方法是否能识别这些关键的结构元素。

Method: 研究引入了CAMBench-QR基准测试，利用QR码的规范几何结构（查找图案、时序线、模块网格）来测试CAM方法。它合成带有精确掩码和受控失真的QR/非QR数据，并报告结构感知指标（如查找器/时序质量比、背景泄漏、覆盖AUC、到结构距离），同时还评估因果遮挡、插入/删除保真度、鲁棒性和延迟。该研究在零样本和末尾块微调两种实际情境下，对代表性的高效CAM方法（LayerCAM、EigenGrad-CAM、XGrad-CAM）进行了基准测试。

Result: CAMBench-QR提供了基准测试、度量标准和训练方案，为一个简单、可复现的结构感知视觉解释评估标准。这些工具可用于系统地评估CAM方法在识别关键结构方面的能力。

Conclusion: CAMBench-QR可以作为一种试金石，用于判断视觉解释是否真正具有结构感知能力，从而推动更忠实、更可靠的解释性AI研究。

Abstract: Visual explanations are often plausible but not structurally faithful. We
introduce CAMBench-QR, a structure-aware benchmark that leverages the canonical
geometry of QR codes (finder patterns, timing lines, module grid) to test
whether CAM methods place saliency on requisite substructures while avoiding
background. CAMBench-QR synthesizes QR/non-QR data with exact masks and
controlled distortions, and reports structure-aware metrics (Finder/Timing Mass
Ratios, Background Leakage, coverage AUCs, Distance-to-Structure) alongside
causal occlusion, insertion/deletion faithfulness, robustness, and latency. We
benchmark representative, efficient CAMs (LayerCAM, EigenGrad-CAM, XGrad-CAM)
under two practical regimes of zero-shot and last-block fine-tuning. The
benchmark, metrics, and training recipes provide a simple, reproducible
yardstick for structure-aware evaluation of visual explanations. Hence we
propose that CAMBENCH-QR can be used as a litmus test of whether visual
explanations are truly structure-aware.

</details>


### [131] [HyPlaneHead: Rethinking Tri-plane-like Representations in Full-Head Image Synthesis](https://arxiv.org/abs/2509.16748)
*Heyuan Li,Kenkun Liu,Lingteng Qiu,Qi Zuo,Keru Zheng,Zilong Dong,Xiaoguang Han*

Main category: cs.CV

TL;DR: 本文提出了一种名为hy-plane的混合平面表示，结合了平面和球面平面的优点，并解决了现有3D感知GAN中三平面表示存在的特征纠缠、镜像伪影、特征图利用率低和特征渗透等问题，在全头图像合成方面取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的三平面表示（如Tri-plane和SphereHead）在3D感知GAN中因效率高而被广泛采用，但存在以下问题：1) 笛卡尔坐标投影导致特征纠缠和镜像伪影；2) SphereHead的球面三平面存在方格特征图与球面平面之间映射不均匀，导致特征图利用率低和细节生成困难；3) 两者都存在特征在卷积通道间渗透，导致平面间干扰，尤其当一个平面占据主导时。

Method: 本文提出了创新的解决方案：1) 引入新型混合平面（hy-plane）表示，结合了平面和球面平面的优势，避免了各自的缺点；2) 通过新颖的近似等面积（near-equal-area）扭曲策略取代传统的theta-phi扭曲，增强了球面平面，最大化方格特征图的有效利用；3) 生成器合成单通道统一特征图而非多通道特征图，有效消除特征渗透。

Result: 通过一系列技术改进，HyPlaneHead方法在全头图像合成方面实现了最先进的性能。

Conclusion: 本文系统分析了三平面表示的现有问题，并提出了混合平面（hy-plane）表示、近似等面积球面扭曲和单通道统一特征图等创新解决方案，有效解决了特征纠缠、利用率低和特征渗透等挑战，显著提升了3D感知GAN在图像合成中的表现。

Abstract: Tri-plane-like representations have been widely adopted in 3D-aware GANs for
head image synthesis and other 3D object/scene modeling tasks due to their
efficiency. However, querying features via Cartesian coordinate projection
often leads to feature entanglement, which results in mirroring artifacts. A
recent work, SphereHead, attempted to address this issue by introducing
spherical tri-planes based on a spherical coordinate system. While it
successfully mitigates feature entanglement, SphereHead suffers from uneven
mapping between the square feature maps and the spherical planes, leading to
inefficient feature map utilization during rendering and difficulties in
generating fine image details. Moreover, both tri-plane and spherical tri-plane
representations share a subtle yet persistent issue: feature penetration across
convolutional channels can cause interference between planes, particularly when
one plane dominates the others. These challenges collectively prevent
tri-plane-based methods from reaching their full potential. In this paper, we
systematically analyze these problems for the first time and propose innovative
solutions to address them. Specifically, we introduce a novel hybrid-plane
(hy-plane for short) representation that combines the strengths of both planar
and spherical planes while avoiding their respective drawbacks. We further
enhance the spherical plane by replacing the conventional theta-phi warping
with a novel near-equal-area warping strategy, which maximizes the effective
utilization of the square feature map. In addition, our generator synthesizes a
single-channel unified feature map instead of multiple feature maps in separate
channels, thereby effectively eliminating feature penetration. With a series of
technical improvements, our hy-plane representation enables our method,
HyPlaneHead, to achieve state-of-the-art performance in full-head image
synthesis.

</details>


### [132] [DiffEye: Diffusion-Based Continuous Eye-Tracking Data Generation Conditioned on Natural Images](https://arxiv.org/abs/2509.16767)
*Ozgur Kara,Harris Nisar,James M. Rehg*

Main category: cs.CV

TL;DR: DiffEye是一种基于扩散模型的训练框架，用于生成连续、多样化的人眼运动轨迹，它利用原始眼动数据而非离散的扫视路径，解决了现有模型在捕捉变异性和连续性方面的不足，并在自然图像上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的人眼扫视路径和显著性预测模型通常基于离散的扫视路径进行训练，忽略了原始眼动轨迹中包含的丰富信息。此外，大多数方法未能捕捉到人类在观看相同图像时表现出的多样性，通常预测单一、固定长度的扫视路径，这与现实世界视觉注意力的内在多样性和随机性相悖。

Method: 本文提出了DiffEye，一个基于扩散模型的训练框架，旨在模拟自然图像自由观看过程中连续且多样的人眼运动轨迹。该方法以视觉刺激为条件，并引入了一种新颖的组件——对应位置嵌入（CPE），用于将空间凝视信息与视觉输入的基于块的语义特征对齐。DiffEye通过利用原始眼动轨迹而非扫视路径进行训练。

Result: DiffEye成功捕捉了人类凝视行为的内在变异性，并生成了高质量、逼真的眼动模式，即使在相对较小的数据集上训练也能表现良好。生成的轨迹可以转换为扫视路径和显著性图，其输出能更准确地反映人类视觉注意力的分布。广泛评估表明，DiffEye不仅在扫视路径生成方面达到了最先进的性能，而且首次实现了连续眼动轨迹的生成。

Conclusion: DiffEye是首个在自然图像上使用扩散模型处理此任务的方法，它充分利用了原始眼动数据的丰富性，成功解决了现有模型在捕捉眼动轨迹连续性和多样性方面的挑战。它在扫视路径生成方面表现出色，并开创性地实现了连续眼动轨迹的生成。

Abstract: Numerous models have been developed for scanpath and saliency prediction,
which are typically trained on scanpaths, which model eye movement as a
sequence of discrete fixation points connected by saccades, while the rich
information contained in the raw trajectories is often discarded. Moreover,
most existing approaches fail to capture the variability observed among human
subjects viewing the same image. They generally predict a single scanpath of
fixed, pre-defined length, which conflicts with the inherent diversity and
stochastic nature of real-world visual attention. To address these challenges,
we propose DiffEye, a diffusion-based training framework designed to model
continuous and diverse eye movement trajectories during free viewing of natural
images. Our method builds on a diffusion model conditioned on visual stimuli
and introduces a novel component, namely Corresponding Positional Embedding
(CPE), which aligns spatial gaze information with the patch-based semantic
features of the visual input. By leveraging raw eye-tracking trajectories
rather than relying on scanpaths, DiffEye captures the inherent variability in
human gaze behavior and generates high-quality, realistic eye movement
patterns, despite being trained on a comparatively small dataset. The generated
trajectories can also be converted into scanpaths and saliency maps, resulting
in outputs that more accurately reflect the distribution of human visual
attention. DiffEye is the first method to tackle this task on natural images
using a diffusion model while fully leveraging the richness of raw eye-tracking
data. Our extensive evaluation shows that DiffEye not only achieves
state-of-the-art performance in scanpath generation but also enables, for the
first time, the generation of continuous eye movement trajectories. Project
webpage: https://diff-eye.github.io/

</details>


### [133] [MMPart: Harnessing Multi-Modal Large Language Models for Part-Aware 3D Generation](https://arxiv.org/abs/2509.16768)
*Omid Bonakdar,Nasser Mozayani*

Main category: cs.CV

TL;DR: MMPart是一个创新的框架，能从单张图片生成具有部件意识的3D模型，解决了现有方法缺乏结构信息、用户控制以及遮挡部分想象力不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的生成式3D建模方法通常将目标对象表示为封闭网格，缺乏结构信息，这限制了编辑、动画和语义理解。虽然部件感知3D生成试图解决此问题，但现有方法用户无法控制对象分离方式，也无法控制模型如何想象隔离阶段的遮挡部分。

Method: MMPart框架包括四个步骤：1) 使用VLM基于输入图像和用户描述生成提示；2) 生成模型根据初始图像和提示（作为监督，控制姿态并指导想象遮挡区域）生成每个对象的独立图像；3) 多视角生成阶段，为每个独立图像生成多张一致的不同视角图像；4) 重建模型将这些多视角图像转换为3D模型。

Result: 该论文提出了MMPart框架，能够从单张图像生成具有部件意识的3D模型，并允许用户控制部件分离和模型对遮挡区域的想象，从而克服了现有方法的局限性。

Conclusion: MMPart提供了一个创新且全面的框架，用于从单张图像生成可编辑、可理解的部件感知3D模型，通过引入用户控制和引导模型想象遮挡部分的能力，显著提升了3D建模的实用性。

Abstract: Generative 3D modeling has advanced rapidly, driven by applications in VR/AR,
metaverse, and robotics. However, most methods represent the target object as a
closed mesh devoid of any structural information, limiting editing, animation,
and semantic understanding. Part-aware 3D generation addresses this problem by
decomposing objects into meaningful components, but existing pipelines face
challenges: in existing methods, the user has no control over which objects are
separated and how model imagine the occluded parts in isolation phase. In this
paper, we introduce MMPart, an innovative framework for generating part-aware
3D models from a single image. We first use a VLM to generate a set of prompts
based on the input image and user descriptions. In the next step, a generative
model generates isolated images of each object based on the initial image and
the previous step's prompts as supervisor (which control the pose and guide
model how imagine previously occluded areas). Each of those images then enters
the multi-view generation stage, where a number of consistent images from
different views are generated. Finally, a reconstruction model converts each of
these multi-view images into a 3D model.

</details>


### [134] [Artificial Satellite Trails Detection Using U-Net Deep Neural Network and Line Segment Detector Algorithm](https://arxiv.org/abs/2509.16771)
*Xiaohan Chen,Hongrui Gu,Cunshi Wang,Haiyang Mu,Jie Zheng,Junju Du,Jing Ren,Zhou Fan,Jing Li*

Main category: cs.CV

TL;DR: 随着卫星数量增加，天文图像受卫星条纹干扰严重。本文提出结合U-Net和LSD算法的卫星条纹检测模型，在信噪比大于3时检测率超99%，并在真实数据上表现良好。


<details>
  <summary>Details</summary>
Motivation: 人工卫星数量激增导致天文成像中出现条纹状伪影，这些卫星条纹会引入虚假源并造成显著的光度测量误差，因此准确识别观测数据中卫星条纹的位置至关重要。

Method: 提出了一种结合U-Net深度神经网络进行图像分割和线段检测器（LSD）算法的卫星条纹检测模型。该模型使用来自Mini-SiTian阵列数据生成的375张模拟卫星条纹图像进行训练。

Result: 对于信噪比（SNR）大于3的条纹，检测率超过99%。应用于Mini-SiTian阵列的真实观测数据时，模型召回率达到79.57%，精确率达到74.56%。

Conclusion: 所提出的结合U-Net和LSD的卫星条纹检测模型能够有效识别天文图像中的卫星条纹，在模拟和真实数据上均表现出较高的检测性能。

Abstract: With the rapid increase in the number of artificial satellites, astronomical
imaging is experiencing growing interference. When these satellites reflect
sunlight, they produce streak-like artifacts in photometry images. Such
satellite trails can introduce false sources and cause significant photometric
errors. As a result, accurately identifying the positions of satellite trails
in observational data has become essential. In this work, we propose a
satellite trail detection model that combines the U-Net deep neural network for
image segmentation with the Line Segment Detector (LSD) algorithm. The model is
trained on 375 simulated images of satellite trails, generated using data from
the Mini-SiTian Array. Experimental results show that for trails with a
signal-to-noise ratio (SNR) greater than 3, the detection rate exceeds 99.
Additionally, when applied to real observational data from the Mini-SiTian
Array, the model achieves a recall of 79.57 and a precision of 74.56.

</details>


### [135] [Benchmarking and Mitigating MCQA Selection Bias of Large Vision-Language Models](https://arxiv.org/abs/2509.16805)
*Md. Atabuzzaman,Ali Asgarov,Chris Thomas*

Main category: cs.CV

TL;DR: 本文研究了大型视觉-语言模型（LVLMs）在多项选择问答（MCQA）中存在的选择偏差问题，发现该偏差随任务难度增加而加剧，并提出了一种无需重新训练的推理时对数校正去偏方法，显著降低了偏差并提高了准确性。


<details>
  <summary>Details</summary>
Motivation: 以往研究探索了视觉问答（VQA）中的单模态偏差，但LVLMs在多项选择问答（MCQA）中可能偏好特定选项标记（如“A”）或位置的选择偏差问题尚未得到充分探索。

Method: 研究人员通过构建细粒度的MCQA基准（根据选项语义相似性定义为简单、中等和困难难度）来调查LVLMs中选择偏差的存在和性质。他们提出了一种推理时的对数级别去偏方法，该方法通过通用和上下文提示估计集成偏差向量，并对模型的输出应用置信度自适应校正。此方法无需重新训练，且兼容冻结的LVLMs。

Result: 对多个最先进模型进行的广泛实验表明，LVLMs存在持续的选择偏差，且这种偏差随任务难度增加而加剧。所提出的缓解方法显著减少了偏差，并在具有挑战性的设置中提高了准确性。

Conclusion: 这项工作为LVLMs在MCQA中的局限性提供了新的见解，并提出了一种实用的方法来提高它们在细粒度视觉推理中的鲁棒性。

Abstract: Large Vision-Language Models (LVLMs) have achieved strong performance on
vision-language tasks, particularly Visual Question Answering (VQA). While
prior work has explored unimodal biases in VQA, the problem of selection bias
in Multiple-Choice Question Answering (MCQA), where models may favor specific
option tokens (e.g., "A") or positions, remains underexplored. In this paper,
we investigate both the presence and nature of selection bias in LVLMs through
fine-grained MCQA benchmarks spanning easy, medium, and hard difficulty levels,
defined by the semantic similarity of the options. We further propose an
inference-time logit-level debiasing method that estimates an ensemble bias
vector from general and contextual prompts and applies confidence-adaptive
corrections to the model's output. Our method mitigates bias without retraining
and is compatible with frozen LVLMs. Extensive experiments across several
state-of-the-art models reveal consistent selection biases that intensify with
task difficulty, and show that our mitigation approach significantly reduces
bias while improving accuracy in challenging settings. This work offers new
insights into the limitations of LVLMs in MCQA and presents a practical
approach to improve their robustness in fine-grained visual reasoning. Datasets
and code are available at:
https://github.com/Atabuzzaman/Selection-Bias-of-LVLMs

</details>


### [136] [MedGS: Gaussian Splatting for Multi-Modal 3D Medical Imaging](https://arxiv.org/abs/2509.16806)
*Kacper Marzol,Ignacy Kolton,Weronika Smolak-Dyżewska,Joanna Kaleta,Marcin Mazur,Przemysław Spurek*

Main category: cs.CV

TL;DR: MedGS是一个半监督神经隐式表面重建框架，利用高斯泼溅(GS)机制对多模态三维医学图像数据进行鲁棒插值和高效重建，解决了传统方法在噪声和信息不完整方面的限制。


<details>
  <summary>Details</summary>
Motivation: 多模态三维医学成像数据（如超声、MRI、CT）在非侵入性解剖可视化中广泛应用，但其表面重建和帧间插值常受图像噪声和帧间信息不完整性的限制。

Method: 提出了MedGS框架，一个半监督的神经隐式表面重建方法。它采用基于高斯泼溅(GS)的插值机制，将医学图像数据（嵌入三维空间的连续二维帧）建模为基于高斯的分布。

Result: MedGS比传统神经隐式方法训练更高效。其显式基于GS的表示增强了噪声鲁棒性，支持灵活编辑，并能以更少伪影精确建模复杂解剖结构。

Conclusion: MedGS的这些特性使其非常适合可扩展且实用的医学成像应用，为多模态三维医学图像的表面重建和帧间插值提供了更优解决方案。

Abstract: Multi-modal three-dimensional (3D) medical imaging data, derived from
ultrasound, magnetic resonance imaging (MRI), and potentially computed
tomography (CT), provide a widely adopted approach for non-invasive anatomical
visualization. Accurate modeling, registration, and visualization in this
setting depend on surface reconstruction and frame-to-frame interpolation.
Traditional methods often face limitations due to image noise and incomplete
information between frames. To address these challenges, we present MedGS, a
semi-supervised neural implicit surface reconstruction framework that employs a
Gaussian Splatting (GS)-based interpolation mechanism. In this framework,
medical imaging data are represented as consecutive two-dimensional (2D) frames
embedded in 3D space and modeled using Gaussian-based distributions. This
representation enables robust frame interpolation and high-fidelity surface
reconstruction across imaging modalities. As a result, MedGS offers more
efficient training than traditional neural implicit methods. Its explicit
GS-based representation enhances noise robustness, allows flexible editing, and
supports precise modeling of complex anatomical structures with fewer
artifacts. These features make MedGS highly suitable for scalable and practical
applications in medical imaging.

</details>


### [137] [Looking in the mirror: A faithful counterfactual explanation method for interpreting deep image classification models](https://arxiv.org/abs/2509.16822)
*Townim Faisal Chowdhury,Vu Minh Hieu Phan,Kewen Liao,Nanyu Dong,Minh-Son To,Anton Hengel,Johan Verjans,Zhibin Liao*

Main category: cs.CV

TL;DR: Mirror-CFE是一种新颖的反事实解释方法，它直接在深度图像分类器的特征空间中操作，通过将决策边界视为“镜子”来生成忠实的反事实解释，并学习从特征空间到图像空间的映射。


<details>
  <summary>Details</summary>
Motivation: 现有的深度图像分类器反事实解释（CFE）方法通常依赖额外的图像编码器和生成模型，忽略了分类器自身的特征空间和决策边界，因此无法解释分类器学到的内在特征空间和决策过程。

Method: Mirror-CFE直接在分类器的特征空间中生成反事实解释，将决策边界视为“镜子”来“反射”特征表示。它学习一个从特征空间到图像空间的映射函数，同时保留距离关系，从而实现源图像与其反事实图像之间的平滑过渡。

Result: 在四个图像数据集上，Mirror-CFE在有效性方面优于最先进的解释方法，同时保持了输入相似性。此外，它通过生成逐步过渡来揭示特征如何随分类置信度变化，从而提供了分类器决策过程的可解释可视化。

Conclusion: Mirror-CFE通过直接在分类器特征空间中操作，并利用决策边界作为“镜子”，成功生成了忠实且可解释的反事实解释，其性能优于现有方法，并能提供分类决策过程的深入洞察。

Abstract: Counterfactual explanations (CFE) for deep image classifiers aim to reveal
how minimal input changes lead to different model decisions, providing critical
insights for model interpretation and improvement. However, existing CFE
methods often rely on additional image encoders and generative models to create
plausible images, neglecting the classifier's own feature space and decision
boundaries. As such, they do not explain the intrinsic feature space and
decision boundaries learned by the classifier. To address this limitation, we
propose Mirror-CFE, a novel method that generates faithful counterfactual
explanations by operating directly in the classifier's feature space, treating
decision boundaries as mirrors that ``reflect'' feature representations in the
mirror. Mirror-CFE learns a mapping function from feature space to image space
while preserving distance relationships, enabling smooth transitions between
source images and their counterfactuals. Through extensive experiments on four
image datasets, we demonstrate that Mirror-CFE achieves superior performance in
validity while maintaining input resemblance compared to state-of-the-art
explanation methods. Finally, mirror-CFE provides interpretable visualization
of the classifier's decision process by generating step-wise transitions that
reveal how features evolve as classification confidence changes.

</details>


### [138] [Learning from Gene Names, Expression Values and Images: Contrastive Masked Text-Image Pretraining for Spatial Transcriptomics Representation Learning](https://arxiv.org/abs/2509.16892)
*Jiahe Qian,Yaoyu Fang,Ziqiao Weng,Xinkun Wang,Lee A. Cooper,Bo Zhou*

Main category: cs.CV

TL;DR: CoMTIP是一种新颖的对比掩码文本-图像预训练框架，旨在通过联合学习图像、基因名称和表达值，并捕获精细视觉背景，解决现有空间转录组学预训练方法的局限性，从而在各种下游任务中超越现有方法并实现零样本基因表达预测。


<details>
  <summary>Details</summary>
Motivation: 空间转录组学需要大规模预训练以获得可泛化的表示，从而连接组织学图像和空间解析基因表达。现有跨模态预训练方法存在局限：它们孤立地依赖基因名称或表达值，剥离了基因分支的基本语义并破坏了基因与定量值的关联；此外，它们将监督限制在图像-文本对齐，忽略了学习鲁棒图像特征所需的内在视觉线索。

Method: CoMTIP框架联合学习图像、基因名称和表达值，并捕获精细的视觉上下文。其视觉分支采用掩码特征建模（MFM）来重建被遮挡的图像块并学习上下文感知的图像嵌入。文本分支使用可扩展的基因-文本编码器并行处理所有基因语句，通过专用嵌入丰富每个基因及其数值，并采用对感知对抗训练（PAAT）来保留正确的基因-值关联。图像和文本表示在共享的InfoNCE优化空间中对齐。

Result: 实验表明，CoMTIP不仅在各种下游任务上超越了现有方法，而且实现了现有方法不具备的零样本基因表达预测能力。

Conclusion: CoMTIP是首个联合学习图像、基因名称和表达值，并捕获精细视觉上下文的对比掩码文本-图像预训练框架，有效解决了现有方法的局限性，并在空间转录组学下游任务中取得了显著的性能提升，特别是实现了零样本基因表达预测。

Abstract: Spatial transcriptomics aims to connect high-resolution histology images with
spatially resolved gene expression. To achieve better performance on downstream
tasks such as gene expression prediction, large-scale pre-training is required
to obtain generalisable representations that can bridge histology and
transcriptomics across tissues, protocols, and laboratories. Existing
cross-modal pre-training approaches for spatial transcriptomics rely on either
gene names or expression values in isolation, which strips the gene branch of
essential semantics and breaks the association between each gene and its
quantitative magnitude. In addition, by restricting supervision to image-text
alignment, these methods ignore intrinsic visual cues that are critical for
learning robust image features. We present CoMTIP, the first Contrastive Masked
Text-Image Pretraining framework that jointly learns from images, gene names,
and expression values while capturing fine-grained visual context for spatial
transcriptomics. The vision branch uses Masked Feature Modeling to reconstruct
occluded patches and learn context-aware image embeddings. The text branch
applies a scalable Gene-Text Encoder that processes all gene sentences in
parallel, enriches each gene and its numerical value with dedicated embeddings,
and employs Pair-aware Adversarial Training (PAAT) to preserve correct
gene-value associations. Image and text representations are aligned in a shared
InfoNCE-optimised space. Experiments on public spatial transcriptomics datasets
show that CoMTIP not only surpasses previous methods on diverse downstream
tasks but also achieves zero-shot gene expression prediction, a capability that
existing approaches do not provide.

</details>


### [139] [ISCS: Parameter-Guided Channel Ordering and Grouping for Learned Image Compression](https://arxiv.org/abs/2509.16853)
*Jinhao Wang,Cihan Ruan,Nam Ling,Wei Wang,Wei Jiang*

Main category: cs.CV

TL;DR: 本研究提出一种通用且与数据集无关的方法，利用模型参数统计量识别和组织学习型图像压缩（LIC）模型中的重要通道，形成不变显著通道空间（ISCS），从而实现高效的切片并行解码，降低码率和计算量，同时保持重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有学习型图像压缩（LIC）模型中，仅少数潜在通道对图像重建至关重要，但现有方法在识别这些重要通道时，往往依赖于昂贵、特定数据集的消融测试，并且通常孤立地分析通道，忽略了它们之间的相互依赖性，导致效率低下。

Method: 本研究提出了一种通用且与数据集无关的方法，用于识别和组织预训练VAE-based LIC模型中的重要通道。该方法不采用暴力经验评估，而是利用模型内在参数统计量（如权重方差、偏置大小和成对相关性）来估计通道重要性。通过分析，揭示了一个称为“不变显著通道空间”（ISCS）的组织结构，其中包含捕获主导结构的“显著核心通道”和提供补充细节的“显著辅助通道”。在此基础上，引入了一种确定性的通道排序和分组策略，以实现切片并行解码，减少冗余并提高码率效率。

Result: 在多种LIC架构上的实验结果表明，该方法能够有效降低码率和计算量，同时保持图像重建质量。

Conclusion: 本研究为现有学习型压缩框架提供了一种实用且模块化的增强方案，通过识别和组织重要通道，显著提高了编码和计算效率。

Abstract: Prior studies in learned image compression (LIC) consistently show that only
a small subset of latent channels is critical for reconstruction, while many
others carry limited information. Exploiting this imbalance could improve both
coding and computational efficiency, yet existing approaches often rely on
costly, dataset-specific ablation tests and typically analyze channels in
isolation, ignoring their interdependencies.
  We propose a generalizable, dataset-agnostic method to identify and organize
important channels in pretrained VAE-based LIC models. Instead of brute-force
empirical evaluations, our approach leverages intrinsic parameter
statistics-weight variances, bias magnitudes, and pairwise correlations-to
estimate channel importance. This analysis reveals a consistent organizational
structure, termed the Invariant Salient Channel Space (ISCS), where
Salient-Core channels capture dominant structures and Salient-Auxiliary
channels provide complementary details. Building on ISCS, we introduce a
deterministic channel ordering and grouping strategy that enables
slice-parallel decoding, reduces redundancy, and improves bitrate efficiency.
  Experiments across multiple LIC architectures demonstrate that our method
effectively reduces bitrate and computation while maintaining reconstruction
quality, providing a practical and modular enhancement to existing learned
compression frameworks.

</details>


### [140] [ME-Mamba: Multi-Expert Mamba with Efficient Knowledge Capture and Fusion for Multimodal Survival Analysis](https://arxiv.org/abs/2509.16900)
*Chengsheng Zhang,Linhao Qu,Xiaoyu Liu,Zhijian Song*

Main category: cs.CV

TL;DR: 本文提出了一种多专家Mamba（ME-Mamba）系统，用于整合病理图像和基因组数据进行癌症生存分析。该系统通过Mamba架构的专家模块提取单模态特征，并通过协同专家模块融合多模态信息，实现了准确且计算高效的生存预测。


<details>
  <summary>Details</summary>
Motivation: 在癌症研究中，使用全玻片图像（WSI）进行生存分析至关重要，但病理图像通常只提供玻片级别标签，这阻碍了从千兆像素WSI中学习判别性表示。随着高通量测序技术的发展，整合病理图像和基因组数据的多模态生存分析成为一种有前景的方法，但如何有效融合这两种模态的互补信息是挑战。

Method: 本文提出了ME-Mamba系统。首先，引入病理专家和基因组专家，分别处理单模态数据。这两个专家均采用Mamba架构，结合了传统扫描和基于注意力的扫描机制，以从包含大量冗余或不相关信息的长实例序列中提取判别性特征。其次，设计了一个协同专家负责模态融合，它通过最优传输显式学习两种模态之间的令牌级局部对应关系，并通过基于最大均值差异（MMD）的全局跨模态融合损失隐式增强分布一致性。融合后的特征表示随后传递给Mamba骨干网络进行进一步整合。

Result: 通过病理专家、基因组专家和协同专家的协作，ME-Mamba方法以相对较低的计算复杂性实现了稳定且准确的生存分析。在癌症基因组图谱（TCGA）的五个数据集上进行的广泛实验结果表明，该方法达到了最先进的性能。

Conclusion: ME-Mamba系统通过有效地整合病理图像和基因组数据，实现了互补信息融合而不损失关键信息，从而促进了准确的癌症生存分析。其多专家Mamba架构克服了单模态分析的局限性，并以高效的计算方式实现了最先进的性能。

Abstract: Survival analysis using whole-slide images (WSIs) is crucial in cancer
research. Despite significant successes, pathology images typically only
provide slide-level labels, which hinders the learning of discriminative
representations from gigapixel WSIs. With the rapid advancement of
high-throughput sequencing technologies, multimodal survival analysis
integrating pathology images and genomics data has emerged as a promising
approach. We propose a Multi-Expert Mamba (ME-Mamba) system that captures
discriminative pathological and genomic features while enabling efficient
integration of both modalities. This approach achieves complementary
information fusion without losing critical information from individual
modalities, thereby facilitating accurate cancer survival analysis.
Specifically, we first introduce a Pathology Expert and a Genomics Expert to
process unimodal data separately. Both experts are designed with Mamba
architectures that incorporate conventional scanning and attention-based
scanning mechanisms, allowing them to extract discriminative features from long
instance sequences containing substantial redundant or irrelevant information.
Second, we design a Synergistic Expert responsible for modality fusion. It
explicitly learns token-level local correspondences between the two modalities
via Optimal Transport, and implicitly enhances distribution consistency through
a global cross-modal fusion loss based on Maximum Mean Discrepancy. The fused
feature representations are then passed to a mamba backbone for further
integration. Through the collaboration of the Pathology Expert, Genomics
Expert, and Synergistic Expert, our method achieves stable and accurate
survival analysis with relatively low computational complexity. Extensive
experimental results on five datasets in The Cancer Genome Atlas (TCGA)
demonstrate our state-of-the-art performance.

</details>


### [141] [SLAM-Former: Putting SLAM into One Transformer](https://arxiv.org/abs/2509.16909)
*Yijun Yuan,Zhuoguang Chen,Kenan Li,Weibang Wang,Hang Zhao*

Main category: cs.CV

TL;DR: SLAM-Former是一种新颖的神经方法，将完整的SLAM功能集成到一个单一的Transformer中，通过前端实时跟踪/建图和后端全局优化实现高性能。


<details>
  <summary>Details</summary>
Motivation: 将完整的SLAM功能集成到一个单一的神经方法（特别是Transformer）中，以实现更优的系统性能。

Method: 提出SLAM-Former，一个基于Transformer的SLAM系统，包含前端和后端。前端实时处理单目图像进行增量式建图和跟踪，后端进行全局优化以确保几何一致性。前端和后端交替执行，相互促进。

Result: 实验结果表明，SLAM-Former在性能上优于或与最先进的密集SLAM方法具有高度竞争力。

Conclusion: SLAM-Former成功地将完整的SLAM能力整合到单一的Transformer架构中，并展现出卓越或具有竞争力的性能。

Abstract: We present SLAM-Former, a novel neural approach that integrates full SLAM
capabilities into a single transformer. Similar to traditional SLAM systems,
SLAM-Former comprises both a frontend and a backend that operate in tandem. The
frontend processes sequential monocular images in real-time for incremental
mapping and tracking, while the backend performs global refinement to ensure a
geometrically consistent result. This alternating execution allows the frontend
and backend to mutually promote one another, enhancing overall system
performance. Comprehensive experimental results demonstrate that SLAM-Former
achieves superior or highly competitive performance compared to
state-of-the-art dense SLAM methods.

</details>


### [142] [ConfidentSplat: Confidence-Weighted Depth Fusion for Accurate 3D Gaussian Splatting SLAM](https://arxiv.org/abs/2509.16863)
*Amanuel T. Dufera,Yuan-Li Cai*

Main category: cs.CV

TL;DR: ConfidentSplat是一个基于3D高斯泼溅（3DGS）的SLAM系统，通过引入置信度加权的深度融合机制，解决了纯RGB 3DGS SLAM中深度估计不可靠导致的几何不准确问题，实现了高保真重建，并在重建精度和新视图合成方面显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有纯RGB 3DGS SLAM方法由于深度估计不可靠，导致几何重建不准确，本研究旨在解决这一问题。

Method: 核心创新是置信度加权的融合机制，它自适应地整合多视图几何深度线索和学习到的单目先验（Omnidata ViT），并根据多视图几何一致性导出的显式可靠性估计动态加权。生成的代理深度用于指导可变形3DGS地图的优化。系统还采用受DROID-SLAM启发的SLAM前端以及后端优化（回环检测、全局BA）来维护全局一致性。

Result: 在TUM-RGBD、ScanNet等标准基准以及多样化的定制移动数据集上进行了广泛验证，结果表明，与基线相比，该系统在重建精度（L1深度误差）和新视图合成保真度（PSNR、SSIM、LPIPS）方面有显著提升，尤其是在挑战性条件下表现突出。

Conclusion: ConfidentSplat强调了原则性的、置信度感知的传感器融合对于推进最先进的密集视觉SLAM的有效性。

Abstract: We introduce ConfidentSplat, a novel 3D Gaussian Splatting (3DGS)-based SLAM
system for robust, highfidelity RGB-only reconstruction. Addressing geometric
inaccuracies in existing RGB-only 3DGS SLAM methods that stem from unreliable
depth estimation, ConfidentSplat incorporates a core innovation: a
confidence-weighted fusion mechanism. This mechanism adaptively integrates
depth cues from multiview geometry with learned monocular priors (Omnidata
ViT), dynamically weighting their contributions based on explicit reliability
estimates-derived predominantly from multi-view geometric consistency-to
generate high-fidelity proxy depth for map supervision. The resulting proxy
depth guides the optimization of a deformable 3DGS map, which efficiently
adapts online to maintain global consistency following pose updates from a
DROID-SLAM-inspired frontend and backend optimizations (loop closure, global
bundle adjustment). Extensive validation on standard benchmarks (TUM-RGBD,
ScanNet) and diverse custom mobile datasets demonstrates significant
improvements in reconstruction accuracy (L1 depth error) and novel view
synthesis fidelity (PSNR, SSIM, LPIPS) over baselines, particularly in
challenging conditions. ConfidentSplat underscores the efficacy of principled,
confidence-aware sensor fusion for advancing state-of-the-art dense visual
SLAM.

</details>


### [143] [The 1st Solution for 7th LSVOS RVOS Track: SaSaSa2VA](https://arxiv.org/abs/2509.16972)
*Quanzhu Niu,Dengxian Gong,Shihao Chen,Tao Zhang,Yikang Zhou,Haobo Yuan,Lu Qi,Xiangtai Li,Shunping Ji*

Main category: cs.CV

TL;DR: 本文提出SaSaSa2VA，改进了Sa2VA模型，通过解决稀疏帧采样和单一[SEG]标记的限制，显著提升了视频指代目标分割（RVOS）性能，并在LSVOS挑战赛中获得第一名。


<details>
  <summary>Details</summary>
Motivation: 视频指代目标分割（RVOS）需要同时理解外观和运动，挑战性高。现有基于多模态大语言模型（MLLM）与SAM2结合的Sa2VA模型存在两个主要瓶颈：稀疏帧采样和对整个视频仅依赖单一[SEG]标记，限制了分割性能。

Method: 本文提出了SaSaSa2VA (Segmentation Augmented and Selective Averaged Sa2VA) 模型来解决上述问题。该方法结合了高效的分割增强和测试时集成（test-time ensembling）策略。

Result: SaSaSa2VA在第7届LSVOS挑战赛（RVOS赛道）中取得了67.45的J&F分数，排名第一，并领先第二名2.80分。消融研究也证明了高效分割增强和测试时集成策略的有效性。

Conclusion: 高效的分割增强和测试时集成能够显著提升基于MLLM的RVOS模型的性能。

Abstract: Referring video object segmentation (RVOS) requires segmenting and tracking
objects in videos conditioned on natural-language expressions, demanding
fine-grained understanding of both appearance and motion. Building on Sa2VA,
which couples a Multi-modal Large Language Model (MLLM) with the video
segmentation model SAM2, we identify two key bottlenecks that limit
segmentation performance: sparse frame sampling and reliance on a single [SEG]
token for an entire video. We propose Segmentation Augmented and Selective
Averaged Sa2VA SaSaSa2VA to address these issues. On the 7th LSVOS Challenge
(RVOS track), SaSaSa2VA achieves a $J\&F$ of 67.45, ranking first and
surpassing the runner-up by 2.80 points. This result and ablation studies
demonstrate that efficient segmentation augmentation and test-time ensembling
substantially enhance grounded MLLMs for RVOS. The code is released in Sa2VA
repository: https://github.com/magic-research/Sa2VA.

</details>


### [144] [EmbodiedSplat: Personalized Real-to-Sim-to-Real Navigation with Gaussian Splats from a Mobile Device](https://arxiv.org/abs/2509.17430)
*Gunjan Chhablani,Xiaomeng Ye,Muhammad Zubair Irshad,Zsolt Kira*

Main category: cs.CV

TL;DR: EmbodiedSplat是一种新颖的具身AI方法，它利用iPhone捕获的3D高斯泼溅重建场景，在Habitat-Sim中微调策略，显著提高了模拟到真实世界的迁移效果和性能。


<details>
  <summary>Details</summary>
Motivation: 具身AI训练和评估主要依赖模拟，但现有模拟环境要么缺乏真实感，要么需要昂贵的硬件捕获，导致模拟到真实世界的迁移（sim-to-real transfer）成为一大挑战。

Method: 本文提出了EmbodiedSplat方法。该方法利用iPhone捕获部署环境，通过3D高斯泼溅（3D Gaussian Splatting, GS）技术重建网格，并在Habitat-Sim模拟器中利用这些重建场景对策略进行微调。研究还全面分析了训练策略、预训练数据集和网格重建技术对模拟到真实世界预测能力的影响。

Result: 实验结果表明，使用EmbodiedSplat微调的智能体在真实世界图像导航任务中，比在大规模真实世界数据集（HM3D）和合成数据集（HSSD）上预训练的零样本基线表现更优，绝对成功率分别提高了20%和40%。此外，重建网格的模拟与真实世界相关性高达0.87-0.97。

Conclusion: EmbodiedSplat方法能够有效地将策略适应到多样化的真实世界环境，且所需努力最小，显著提高了模拟到真实世界的预测能力和性能。

Abstract: The field of Embodied AI predominantly relies on simulation for training and
evaluation, often using either fully synthetic environments that lack
photorealism or high-fidelity real-world reconstructions captured with
expensive hardware. As a result, sim-to-real transfer remains a major
challenge. In this paper, we introduce EmbodiedSplat, a novel approach that
personalizes policy training by efficiently capturing the deployment
environment and fine-tuning policies within the reconstructed scenes. Our
method leverages 3D Gaussian Splatting (GS) and the Habitat-Sim simulator to
bridge the gap between realistic scene capture and effective training
environments. Using iPhone-captured deployment scenes, we reconstruct meshes
via GS, enabling training in settings that closely approximate real-world
conditions. We conduct a comprehensive analysis of training strategies,
pre-training datasets, and mesh reconstruction techniques, evaluating their
impact on sim-to-real predictivity in real-world scenarios. Experimental
results demonstrate that agents fine-tuned with EmbodiedSplat outperform both
zero-shot baselines pre-trained on large-scale real-world datasets (HM3D) and
synthetically generated datasets (HSSD), achieving absolute success rate
improvements of 20\% and 40\% on real-world Image Navigation task. Moreover,
our approach yields a high sim-vs-real correlation (0.87--0.97) for the
reconstructed meshes, underscoring its effectiveness in adapting policies to
diverse environments with minimal effort. Project page:
https://gchhablani.github.io/embodied-splat

</details>


### [145] [$\mathtt{M^3VIR}$: A Large-Scale Multi-Modality Multi-View Synthesized Benchmark Dataset for Image Restoration and Content Creation](https://arxiv.org/abs/2509.16873)
*Yuanzhi Li,Lebin Zhou,Nam Ling,Zhenghao Chen,Wei Wang,Wei Jiang*

Main category: cs.CV

TL;DR: 本文介绍了 $\mathtt{M^3VIR}$，一个大规模、多模态、多视角的游戏数据集，旨在解决现有数据集的局限性，并推动超分辨率、新视角合成和可控视频生成等领域的研究。


<details>
  <summary>Details</summary>
Motivation: 游戏和娱乐行业对沉浸式体验和生成式AI的需求日益增长，但现有数据集在规模、多样性、真实性方面存在局限，且缺乏可控视频生成的基准，无法准确捕捉游戏内容的独特特征。

Method: 作者引入了 $\mathtt{M^3VIR}$ 数据集，该数据集使用虚幻引擎5渲染，提供多样化、高保真的游戏内容，包括80个场景、8个类别的真实LR-HR配对和多视角帧。它包含用于超分辨率（SR）、新视角合成（NVS）及其组合任务的 $\mathtt{M^3VIR\_MR}$，以及首个用于可控视频生成研究的多风格、对象级真实数据集 $\mathtt{M^3VIR\_MS}$。此外，还对几种最先进的SR和NVS方法进行了基准测试。

Result: $\mathtt{M^3VIR}$ 成功提供了真实、高保真的游戏数据，克服了现有资源的不足。它为SR和NVS任务建立了性能基线，并首次为可控视频生成提供了基准。尽管目前没有现有方法能直接处理可控视频生成，但该数据集为推动这一领域的研究奠定了基础。

Conclusion: $\mathtt{M^3VIR}$ 数据集的发布旨在促进AI驱动的修复、压缩和可控内容生成方面的研究，以支持下一代云游戏和娱乐产业的发展。

Abstract: The gaming and entertainment industry is rapidly evolving, driven by
immersive experiences and the integration of generative AI (GAI) technologies.
Training such models effectively requires large-scale datasets that capture the
diversity and context of gaming environments. However, existing datasets are
often limited to specific domains or rely on artificial degradations, which do
not accurately capture the unique characteristics of gaming content. Moreover,
benchmarks for controllable video generation remain absent.
  To address these limitations, we introduce $\mathtt{M^3VIR}$, a large-scale,
multi-modal, multi-view dataset specifically designed to overcome the
shortcomings of current resources. Unlike existing datasets, $\mathtt{M^3VIR}$
provides diverse, high-fidelity gaming content rendered with Unreal Engine 5,
offering authentic ground-truth LR-HR paired and multi-view frames across 80
scenes in 8 categories. It includes $\mathtt{M^3VIR\_MR}$ for super-resolution
(SR), novel view synthesis (NVS), and combined NVS+SR tasks, and
$\mathtt{M^3VIR\_{MS}}$, the first multi-style, object-level ground-truth set
enabling research on controlled video generation. Additionally, we benchmark
several state-of-the-art SR and NVS methods to establish performance baselines.
While no existing approaches directly handle controlled video generation,
$\mathtt{M^3VIR}$ provides a benchmark for advancing this area. By releasing
the dataset, we aim to facilitate research in AI-powered restoration,
compression, and controllable content generation for next-generation cloud
gaming and entertainment.

</details>


### [146] [When Color-Space Decoupling Meets Diffusion for Adverse-Weather Image Restoration](https://arxiv.org/abs/2509.17024)
*Wenxuan Fang,Jili Fan,Chao Wang,Xiantao Hu,Jiangwei Weng,Ying Tai,Jian Yang,Jun Li*

Main category: cs.CV

TL;DR: LCDiff是一种新颖的恶劣天气图像恢复框架，通过亮度-色度分解网络处理图像并利用亮度引导的扩散模型进行恢复，无需明确的退化提示，并在新数据集DriveWeather上超越了现有最佳方法。


<details>
  <summary>Details</summary>
Motivation: 恶劣天气图像恢复（AWIR）极具挑战性，因为天气退化不可预测且动态。传统方法泛化能力差，而近期基于提示学习的方法过度依赖视觉-语言模型的退化估计能力，导致恢复效果不稳定。

Method: 本文提出了LCDiff框架，包含两个核心组件：1. 亮度-色度分解网络（LCDN）：在YCbCr颜色空间中处理图像，分别处理与退化相关的亮度分量和退化不变的色度分量，以减轻退化并保持色彩保真度。2. 亮度引导扩散模型（LGDM）：利用与退化相关的亮度信息作为引导条件，无需明确的退化提示。此外，LGDM引入了动态时间步长损失（Dynamic Time Step Loss）来优化去噪网络，以平衡恢复图像的低频和高频特征。最后，作者还提出了一个全面的全天候驾驶数据集DriveWeather用于鲁棒评估。

Result: 广泛的实验表明，LCDiff方法超越了现有最先进的方法，在AWIR领域树立了新的基准。

Conclusion: LCDiff通过其独特的亮度-色度分解和亮度引导扩散模型，有效解决了恶劣天气图像恢复中的挑战，实现了卓越的恢复质量，并为该领域设立了新的性能标准。

Abstract: Adverse Weather Image Restoration (AWIR) is a highly challenging task due to
the unpredictable and dynamic nature of weather-related degradations.
Traditional task-specific methods often fail to generalize to unseen or complex
degradation types, while recent prompt-learning approaches depend heavily on
the degradation estimation capabilities of vision-language models, resulting in
inconsistent restorations. In this paper, we propose \textbf{LCDiff}, a novel
framework comprising two key components: \textit{Lumina-Chroma Decomposition
Network} (LCDN) and \textit{Lumina-Guided Diffusion Model} (LGDM). LCDN
processes degraded images in the YCbCr color space, separately handling
degradation-related luminance and degradation-invariant chrominance components.
This decomposition effectively mitigates weather-induced degradation while
preserving color fidelity. To further enhance restoration quality, LGDM
leverages degradation-related luminance information as a guiding condition,
eliminating the need for explicit degradation prompts. Additionally, LGDM
incorporates a \textit{Dynamic Time Step Loss} to optimize the denoising
network, ensuring a balanced recovery of both low- and high-frequency features
in the image. Finally, we present DriveWeather, a comprehensive all-weather
driving dataset designed to enable robust evaluation. Extensive experiments
demonstrate that our approach surpasses state-of-the-art methods, setting a new
benchmark in AWIR. The dataset and code are available at:
https://github.com/fiwy0527/LCDiff.

</details>


### [147] [VideoArtGS: Building Digital Twins of Articulated Objects from Monocular Video](https://arxiv.org/abs/2509.17647)
*Yu Liu,Baoxiong Jia,Ruijie Lu,Chuyue Gan,Huayu Chen,Junfeng Ni,Song-Chun Zhu,Siyuan Huang*

Main category: cs.CV

TL;DR: VideoArtGS是一种新颖的方法，可从单目视频中重建铰接物体的数字孪生，通过运动先验指导和混合部件分配模块，显著提高了铰接和网格重建的精度。


<details>
  <summary>Details</summary>
Motivation: 从单目视频构建铰接物体的数字孪生面临挑战，因为需要从有限的视角输入中同时重建物体几何、部件分割和铰接参数。单目视频虽然简单且可扩展，但由于相机和部件的联合运动导致估计不适定，难以仅凭视觉监督解耦物体几何和部件动态。尽管预训练跟踪模型的运动先验可以缓解此问题，但如何有效整合它们进行铰接学习仍未充分探索。

Method: 本文提出了VideoArtGS，一种新颖的方法。它引入了一个运动先验指导流程，用于分析3D轨迹、过滤噪声并提供可靠的铰接参数初始化。此外，它还设计了一个混合中心-网格部件分配模块，用于基于铰接的变形场，以捕捉精确的部件运动。

Result: VideoArtGS在铰接和网格重建方面表现出最先进的性能，与现有方法相比，重建误差降低了大约两个数量级。它实现了从单目视频创建实用数字孪生。

Conclusion: VideoArtGS为基于视频的铰接物体重建建立了新的基准，使得从单目视频创建实用的数字孪生变为可能。

Abstract: Building digital twins of articulated objects from monocular video presents
an essential challenge in computer vision, which requires simultaneous
reconstruction of object geometry, part segmentation, and articulation
parameters from limited viewpoint inputs. Monocular video offers an attractive
input format due to its simplicity and scalability; however, it's challenging
to disentangle the object geometry and part dynamics with visual supervision
alone, as the joint movement of the camera and parts leads to ill-posed
estimation. While motion priors from pre-trained tracking models can alleviate
the issue, how to effectively integrate them for articulation learning remains
largely unexplored. To address this problem, we introduce VideoArtGS, a novel
approach that reconstructs high-fidelity digital twins of articulated objects
from monocular video. We propose a motion prior guidance pipeline that analyzes
3D tracks, filters noise, and provides reliable initialization of articulation
parameters. We also design a hybrid center-grid part assignment module for
articulation-based deformation fields that captures accurate part motion.
VideoArtGS demonstrates state-of-the-art performance in articulation and mesh
reconstruction, reducing the reconstruction error by about two orders of
magnitude compared to existing methods. VideoArtGS enables practical digital
twin creation from monocular video, establishing a new benchmark for
video-based articulated object reconstruction. Our work is made publicly
available at: https://videoartgs.github.io.

</details>


### [148] [SAM-DCE: Addressing Token Uniformity and Semantic Over-Smoothing in Medical Segmentation](https://arxiv.org/abs/2509.16886)
*Yingzhen Hu,Yiheng Zhong,Ruobing Li,Yingxue Su,Jiabao An,Feilong Tang,Jionglong Su,Imran Razzak*

Main category: cs.CV

TL;DR: 针对Segment Anything Model (SAM) 在医学图像分割中遇到的挑战，本文提出了SAM-DCE模型，通过平衡局部判别和全局语义、缓解token均匀性等方式，显著提升了其在医学领域的分割性能。


<details>
  <summary>Details</summary>
Motivation: Segment Anything Model (SAM) 在自然图像上展现出色的零样本分割能力，但在医学成像中，由于领域差异、解剖变异以及对用户提示的依赖而表现不佳。现有的无提示适应方法虽然减少了人工干预，但仍存在鲁棒性和适应性有限的问题，并且常常忽视语义过平滑和token均匀性等关键问题。

Method: 本文提出SAM-DCE模型，旨在平衡局部判别性和全局语义，同时缓解token均匀性问题，增强类间可分离性，并利用细粒度、一致的表示来丰富掩码解码过程。

Result: 在各种医学基准数据集上进行的广泛实验验证了SAM-DCE的有效性。

Conclusion: SAM-DCE通过解决SAM在医学图像分割中面临的语义过平滑和token均匀性等挑战，提供了一种有效且鲁棒的解决方案，提升了医学图像分割的性能。

Abstract: The Segment Anything Model (SAM) demonstrates impressive zero-shot
segmentation ability on natural images but encounters difficulties in medical
imaging due to domain shifts, anatomical variability, and its reliance on
user-provided prompts. Recent prompt-free adaptations alleviate the need for
expert intervention, yet still suffer from limited robustness and adaptability,
often overlooking the issues of semantic over-smoothing and token uniformity.
We propose SAM-DCE, which balances local discrimination and global semantics
while mitigating token uniformity, enhancing inter-class separability, and
enriching mask decoding with fine-grained, consistent representations.
Extensive experiments on diverse medical benchmarks validate its effectiveness.

</details>


### [149] [From Easy to Hard: The MIR Benchmark for Progressive Interleaved Multi-Image Reasoning](https://arxiv.org/abs/2509.17040)
*Hang Du,Jiayang Zhang,Guoshun Nan,Wendi Deng,Zhenyan Chen,Chenyang Zhang,Wang Xiao,Shan Huang,Yuqi Pan,Tao Qi,Sicong Leng*

Main category: cs.CV

TL;DR: 本文提出了一种名为MIR的新基准，用于评估多模态大语言模型（MLLMs）在多图像交错推理任务中的能力，并引入了一种分阶段课程学习策略以提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有多图像基准忽视了交错文本上下文以及图像与文本之间独特的关联，这限制了MLLMs对复杂场景的理解和跨模态关联的捕捉能力。为了弥补这一差距，需要一个能联合理解多图像和交错文本上下文的基准。

Method: 本文引入了MIR基准，该基准要求模型对多张图像及其交错文本上下文进行联合推理，以准确关联图像区域与文本并逻辑连接图像间信息。为增强MLLMs能力，MIR包含了推理步骤，并提出了一种分阶段的课程学习策略，采用“由易到难”的方法逐步引导模型处理复杂场景。

Result: 广泛的实验表明，本文提出的方法显著提升了MLLMs在MIR基准和其他现有基准上的推理性能。

Conclusion: MIR基准有望推动多图像交错推理领域的进一步研究，从而促进MLLMs在处理复杂跨模态任务能力方面的进步。

Abstract: Multi-image Interleaved Reasoning aims to improve Multi-modal Large Language
Models (MLLMs) ability to jointly comprehend and reason across multiple images
and their associated textual contexts, introducing unique challenges beyond
single-image or non-interleaved multi-image tasks. While current multi-image
benchmarks overlook interleaved textual contexts and neglect distinct
relationships between individual images and their associated texts, enabling
models to reason over multi-image interleaved data may significantly enhance
their comprehension of complex scenes and better capture cross-modal
correlations. To bridge this gap, we introduce a novel benchmark MIR, requiring
joint reasoning over multiple images accompanied by interleaved textual
contexts to accurately associate image regions with corresponding texts and
logically connect information across images. To enhance MLLMs ability to
comprehend multi-image interleaved data, we introduce reasoning steps for each
instance within the benchmark and propose a stage-wise curriculum learning
strategy. This strategy follows an "easy to hard" approach, progressively
guiding models from simple to complex scenarios, thereby enhancing their
ability to handle challenging tasks. Extensive experiments benchmarking
multiple MLLMs demonstrate that our method significantly enhances models
reasoning performance on MIR and other established benchmarks. We believe that
MIR will encourage further research into multi-image interleaved reasoning,
facilitating advancements in MLLMs capability to handle complex inter-modal
tasks.Our code and dataset are available at
https://github.com/Shelly-coder239/MIRBench.

</details>


### [150] [DINOv3-Diffusion Policy: Self-Supervised Large Visual Model for Visuomotor Diffusion Policy Learning](https://arxiv.org/abs/2509.17684)
*ThankGod Egbe,Peng Wang,Zhihao Guo,Zidong Chen*

Main category: cs.CV

TL;DR: 该论文评估了DINOv3在机器人视觉运动扩散策略学习中的应用，发现其在微调后能与ImageNet预训练的ResNet-18媲美或超越，且提高了样本效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 研究纯自监督编码器（如DINOv3）是否能在机器人操作的视觉运动扩散策略学习中，与传统的监督式ImageNet预训练骨干网络（如ResNet-18）相匹敌或超越。

Method: 通过统一的FiLM条件扩散策略，在三种训练模式（从头开始训练、冻结、微调）下，使用DINOv3与ResNet-18在四项基准任务（Push-T、Lift、Can、Square）上进行了比较评估。

Result: 研究发现：(i) 微调后的DINOv3在多项任务上与ResNet-18相当或超越；(ii) 冻结的DINOv3仍具竞争力，表明其强大的可迁移先验知识；(iii) 自监督特征提高了样本效率和鲁棒性。与ResNet-18相比，DINOv3在如Can等挑战性任务上测试成功率绝对提升高达10%，在Lift、PushT和Square等任务上表现持平。

Conclusion: 自监督大型视觉模型是机器人操作中动作扩散策略的有效、可泛化的感知前端，这鼓励了在机器人操作领域进一步探索可扩展的无标签预训练方法。

Abstract: This paper evaluates DINOv3, a recent large-scale self-supervised vision
backbone, for visuomotor diffusion policy learning in robotic manipulation. We
investigate whether a purely self-supervised encoder can match or surpass
conventional supervised ImageNet-pretrained backbones (e.g., ResNet-18) under
three regimes: training from scratch, frozen, and finetuned. Across four
benchmark tasks (Push-T, Lift, Can, Square) using a unified FiLM-conditioned
diffusion policy, we find that (i) finetuned DINOv3 matches or exceeds
ResNet-18 on several tasks, (ii) frozen DINOv3 remains competitive, indicating
strong transferable priors, and (iii) self-supervised features improve sample
efficiency and robustness. These results support self-supervised large visual
models as effective, generalizable perceptual front-ends for action diffusion
policies, motivating further exploration of scalable label-free pretraining in
robotic manipulation. Compared to using ResNet18 as a backbone, our approach
with DINOv3 achieves up to a 10% absolute increase in test-time success rates
on challenging tasks such as Can, and on-the-par performance in tasks like
Lift, PushT, and Square.

</details>


### [151] [Rethinking Evaluation of Infrared Small Target Detection](https://arxiv.org/abs/2509.16888)
*Youwei Pang,Xiaoqi Zhao,Lihe Zhang,Huchuan Lu,Georges El Fakhri,Xiaofeng Liu,Shijian Lu*

Main category: cs.CV

TL;DR: 该论文针对红外小目标检测（IRSTD）评估协议的局限性，提出了一个包含混合层级度量、系统错误分析方法和强调跨数据集评估的更全面、更合理的层次分析框架，旨在促进更有效、更鲁棒的IRSTD模型开发，并发布了开源工具包。


<details>
  <summary>Details</summary>
Motivation: 当前红外小目标检测（IRSTD）评估协议存在关键局限性：1. 现有方法依赖碎片化的像素级和目标级特定指标，未能提供模型能力的全面视图。2. 过分强调整体性能分数，掩盖了对识别故障模式和改进真实世界系统性能至关重要的错误分析。3. 该领域主要采用数据集特定的训练-测试范式，阻碍了对模型在不同红外场景下鲁棒性和泛化能力的理解。

Method: 该论文通过以下方法解决上述问题：1. 引入结合像素级和目标级性能的混合层级度量。2. 提出一种系统的错误分析方法。3. 强调跨数据集评估的重要性。此外，还发布了一个开源工具包以促进标准化基准测试。

Result: 该论文的主要贡献是引入了一个混合层级度量、提出了一种系统错误分析方法，并强调了跨数据集评估的重要性，从而构建了一个更彻底、更合理的层次分析框架。同时，发布了一个开源工具包以支持标准化基准测试。

Conclusion: 该论文提出的新评估框架旨在提供一个更彻底、更合理的层次分析方法，最终促进开发出更有效、更鲁棒的红外小目标检测（IRSTD）模型。

Abstract: As an essential vision task, infrared small target detection (IRSTD) has seen
significant advancements through deep learning. However, critical limitations
in current evaluation protocols impede further progress. First, existing
methods rely on fragmented pixel- and target-level specific metrics, which
fails to provide a comprehensive view of model capabilities. Second, an
excessive emphasis on overall performance scores obscures crucial error
analysis, which is vital for identifying failure modes and improving real-world
system performance. Third, the field predominantly adopts dataset-specific
training-testing paradigms, hindering the understanding of model robustness and
generalization across diverse infrared scenarios. This paper addresses these
issues by introducing a hybrid-level metric incorporating pixel- and
target-level performance, proposing a systematic error analysis method, and
emphasizing the importance of cross-dataset evaluation. These aim to offer a
more thorough and rational hierarchical analysis framework, ultimately
fostering the development of more effective and robust IRSTD models. An
open-source toolkit has be released to facilitate standardized benchmarking.

</details>


### [152] [Informative Text-Image Alignment for Visual Affordance Learning with Foundation Models](https://arxiv.org/abs/2509.17074)
*Qian Zhang,Lin Zhang,Xing Fang,Mingxin Zhang,Zhiyuan Wei,Ran Song,Wei Zhang*

Main category: cs.CV

TL;DR: 本文提出一个基于信息约束的文本引导视觉功能态学习框架，通过特征层面的文本-图像对齐，显著提升了现有方法的性能，并在单次功能态学习中达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有利用预训练视觉-语言基础模型进行视觉功能态学习的方法，忽视了视觉图像与语言描述之间特征对齐的重要性，导致识别功能态区域时可能效果不佳。

Method: 本文提出了一个信息引导的文本引导功能态学习框架。具体而言，设计了两个信息约束：1) 功能态互信息约束，通过最大化输入图像中功能态区域特征与相应文本提示之间的互信息，同时学习合适的文本提示和任务导向的视觉特征；2) 对象级信息约束，通过最大化给定对象的视觉特征与其所属类别文本特征之间的互信息，捕获高质量的对象表示，为识别功能态区域提供更可靠的语义先验。

Result: 在AGD20K数据集上的实验结果表明，所提出的方法优于现有方法，并在单次功能态学习中取得了新的最先进（SOTA）性能。

Conclusion: 通过引入信息基础的约束来实现文本-图像在特征层面的对齐，本文提出的框架有效解决了现有文本引导功能态学习方法的局限性，显著提升了功能态识别的准确性和可靠性。

Abstract: Visual affordance learning is crucial for robots to understand and interact
effectively with the physical world. Recent advances in this field attempt to
leverage pre-trained knowledge of vision-language foundation models to learn
affordance properties with limited training data, providing a novel paradigm
for visual affordance learning. However, these methods overlook the
significance of maintaining feature alignment between visual images and
language descriptions for identifying affordance areas with textual guidance,
and thus may lead to suboptimal results. In this paper, we present an
informative framework for text-guided affordance learning, which involves
information-based constraints to achieve text-image alignment at feature level.
Specifically, we design an affordance mutual information constraint that helps
learn appropriate textual prompts and task-oriented visual features
simultaneously by maximizing the mutual information between the features of the
affordance areas in the input images and the corresponding textual prompts. In
addition, we propose an object-level information constraint that maximizes the
mutual information between the visual features of a given object and the text
features of the category it belongs to. This enables the model to capture
high-quality representations for the object, providing more reliable semantic
priors for identifying affordance regions. Experimental results on the AGD20K
dataset show that the proposed method outperforms existing approaches and
achieves the new state-of-the-art in one-shot affordance learning.

</details>


### [153] [PRISM: Precision-Recall Informed Data-Free Knowledge Distillation via Generative Diffusion](https://arxiv.org/abs/2509.16897)
*Xuewan He,Jielei Wang,Zihan Cheng,Yuchen Su,Shiyue Huang,Guoming Lu*

Main category: cs.CV

TL;DR: 现有无数据知识蒸馏(DFKD)方法在处理大规模图像时存在模式崩溃问题。本文提出PRISM，一种结合能量引导分布对齐和多样化提示工程的合成方法，利用扩散模型生成高质量数据，解决了DFKD的精度-召回挑战，并在大规模图像数据集上表现优越，同时展现出强大的域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有无数据知识蒸馏(DFKD)方法在小规模图像上表现良好，但在大规模图像上合成时易出现模式崩溃，导致知识迁移受限。利用先进生成模型（如扩散模型）合成图像虽有前景，但直接使用面临精度-召回挑战：1) 确保合成数据与真实分布对齐；2) 确保覆盖真实域内流形。

Method: 本文提出PRISM（精度-召回感知合成方法）。具体而言，它引入了：1) 能量引导分布对齐（Energy-guided Distribution Alignment），以避免生成域外样本，解决精度问题；2) 多样化提示工程（Diversified Prompt Engineering），以增强对真实域内流形的覆盖，解决召回问题。

Result: 在各种大规模图像数据集上进行的广泛实验表明，PRISM表现出优越性。此外，使用PRISM训练的模型展示出强大的域泛化能力。

Conclusion: PRISM通过结合能量引导分布对齐和多样化提示工程，有效解决了大规模图像无数据知识蒸馏中合成数据面临的精度-召回挑战，显著提升了知识迁移效果，并赋予模型强大的域泛化能力。

Abstract: Data-free knowledge distillation (DFKD) transfers knowledge from a teacher to
a student without access to the real in-distribution (ID) data. While existing
methods perform well on small-scale images, they suffer from mode collapse when
synthesizing large-scale images, resulting in limited knowledge transfer.
Recently, leveraging advanced generative models to synthesize photorealistic
images has emerged as a promising alternative. Nevertheless, directly using
off-the-shelf diffusion to generate datasets faces the precision-recall
challenges: 1) ensuring synthetic data aligns with the real distribution, and
2) ensuring coverage of the real ID manifold. In response, we propose PRISM, a
precision-recall informed synthesis method. Specifically, we introduce
Energy-guided Distribution Alignment to avoid the generation of
out-of-distribution samples, and design the Diversified Prompt Engineering to
enhance coverage of the real ID manifold. Extensive experiments on various
large-scale image datasets demonstrate the superiority of PRISM. Moreover, we
demonstrate that models trained with PRISM exhibit strong domain
generalization.

</details>


### [154] [SAEC: Scene-Aware Enhanced Edge-Cloud Collaborative Industrial Vision Inspection with Multimodal LLM](https://arxiv.org/abs/2509.17136)
*Yuhao Tian,Zheming Yang*

Main category: cs.CV

TL;DR: SAEC是一个场景感知边缘-云协同工业视觉检测框架，结合MLLM，通过场景复杂度估计和自适应调度，在保证高精度的同时显著降低了资源消耗。


<details>
  <summary>Details</summary>
Motivation: 工业视觉检测需要在资源受限下实现高精度。现有方法面临权衡：多模态大型语言模型（MLLM）推理能力强但计算成本高昂，而轻量级边缘模型在复杂情况下往往失效。

Method: 本文提出了SAEC框架，包含三个协同组件：1) 用于复杂缺陷检测的高效MLLM微调；2) 轻量级多尺度场景复杂度估计；3) 自适应边缘-云调度器。这些模块通过根据场景复杂度调整多模态推理，并动态平衡边缘和云资源之间的计算，实现鲁棒的缺陷检测。

Result: 在MVTec AD和KSDD2数据集上，SAEC分别达到了85.11%和82.72%的准确率，超越Qwen 22.1%和20.8%，超越LLaVA 33.3%和31.6%。同时，运行时缩短高达22.4%，每次正确决策的能耗降低40%-74%。

Conclusion: SAEC框架通过结合MLLM的强大推理能力与边缘-云协同的效率，有效解决了工业视觉检测中精度与资源限制之间的矛盾，实现了高性能和高效率。

Abstract: Industrial vision inspection requires high accuracy under stringent resource
constraints, yet existing approaches face a fundamental trade-off. Multimodal
LLMs (MLLMs) deliver strong reasoning capabilities but incur prohibitive
computational costs, while lightweight edge models often fail on complex cases.
In this paper, we present SAEC, a scene-aware enhanced edge-cloud collaborative
industrial vision inspection framework with MLLM. The framework is composed of
three synergistic components: (1) Efficient MLLM Fine-Tuning for Complex Defect
Inspection, (2) Lightweight Multiscale Scene-Complexity Estimation, and (3)
Adaptive Edge-Cloud Scheduler. Together, these modules enable robust defect
detection by tailoring multimodal reasoning to scene complexity and dynamically
balancing computation between edge and cloud resources. Experimental results on
MVTec AD and KSDD2 datasets demonstrate that SAEC attains 85.11% and 82.72%
accuracy, surpassing Qwen by 22.1% and 20.8%, and LLaVA by 33.3% and 31.6%. It
also reduces runtime by up to 22.4% and cuts energy per correct decision by
40%-74%. The code is available at https://github.com/YuHao-Tian/SAEC.

</details>


### [155] [Parameter-efficient fine-tuning (PEFT) of Vision Foundation Models for Atypical Mitotic Figure Classification](https://arxiv.org/abs/2509.16935)
*Lavish Ramchandani,Gunjan Deotale,Dev Kumar Das*

Main category: cs.CV

TL;DR: 本研究利用大型视觉基础模型（如Virchow、Virchow2、UNI）结合LoRA进行参数高效微调，以分类异常有丝分裂，并在MIDOG 2025挑战赛中取得了88.37%的平衡准确率。


<details>
  <summary>Details</summary>
Motivation: 异常有丝分裂（AMFs）与肿瘤侵袭性和不良预后相关，但其检测因形态细微、类别不平衡和病理学家间观察差异而极具挑战。MIDOG 2025挑战赛为评估深度学习方法提供了平台。

Method: 研究采用了Virchow、Virchow2和UNI等大型视觉基础模型，并结合低秩适应（LoRA）进行参数高效微调。实验探索了不同的LoRA秩，以及随机和基于组的数据分割，以分析模型在不同条件下的鲁棒性。

Result: 最佳方法是使用Virchow模型，LoRA秩为8，并结合三折交叉验证集成，在初步测试集上实现了88.37%的平衡准确率，在挑战赛排行榜上并列第9位。

Conclusion: 研究结果表明，结合高效适应策略的基础模型在异常有丝分裂分类方面具有巨大潜力，但仍需在特异性和领域泛化方面进一步改进。

Abstract: Atypical mitotic figures (AMFs) are rare abnormal cell divisions associated
with tumor aggressiveness and poor prognosis. Their detection remains a
significant challenge due to subtle morphological cues, class imbalance, and
inter-observer variability among pathologists. The MIDOG 2025 challenge
introduced a dedicated track for atypical mitosis classification, enabling
systematic evaluation of deep learning methods. In this study, we investigated
the use of large vision foundation models, including Virchow, Virchow2, and
UNI, with Low-Rank Adaptation (LoRA) for parameter-efficient fine-tuning. We
conducted extensive experiments with different LoRA ranks, as well as random
and group-based data splits, to analyze robustness under varied conditions. Our
best approach, Virchow with LoRA rank 8 and ensemble of three-fold
cross-validation, achieved a balanced accuracy of 88.37% on the preliminary
test set, ranking joint 9th in the challenge leaderboard. These results
highlight the promise of foundation models with efficient adaptation strategies
for the classification of atypical mitosis, while underscoring the need for
improvements in specificity and domain generalization.

</details>


### [156] [Ambiguous Medical Image Segmentation Using Diffusion Schrödinger Bridge](https://arxiv.org/abs/2509.17187)
*Lalith Bharadwaj Baru,Kamalaker Dadi,Tapabrata Chakraborti,Raju S. Bapi*

Main category: cs.CV

TL;DR: 本文提出了“分割薛定谔桥”（SSB），首次将薛定谔桥应用于模糊医学图像分割，通过建模图像-掩模联合动态，在保留结构完整性、描绘模糊边界和保持多样性方面实现了最先进的性能，并引入了多样性散度指数（DDI）来量化评估者间差异。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割面临挑战，主要原因在于病灶边界不清晰以及掩模的可变性。

Method: 引入了“分割薛定谔桥”（SSB），这是薛定谔桥首次应用于模糊医学图像分割，通过建模图像-掩模的联合动态来提升性能。SSB使用一种新颖的损失函数来保持多样性。此外，还提出了“多样性散度指数”（$D_{DDI}$）来量化评估者间差异，捕捉多样性和共识。

Result: SSB在LIDC-IDRI、COCA和RACER（内部）数据集上取得了最先进的性能。它能够保留结构完整性，在无需额外指导的情况下描绘不清晰的边界，并保持多样性。

Conclusion: SSB通过薛定谔桥方法和新颖的损失函数，有效解决了医学图像分割中边界模糊和掩模多样性的挑战，实现了卓越的分割性能，并提供了量化评估者间差异的工具。

Abstract: Accurate segmentation of medical images is challenging due to unclear lesion
boundaries and mask variability. We introduce \emph{Segmentation Sch\"{o}dinger
Bridge (SSB)}, the first application of Sch\"{o}dinger Bridge for ambiguous
medical image segmentation, modelling joint image-mask dynamics to enhance
performance. SSB preserves structural integrity, delineates unclear boundaries
without additional guidance, and maintains diversity using a novel loss
function. We further propose the \emph{Diversity Divergence Index} ($D_{DDI}$)
to quantify inter-rater variability, capturing both diversity and consensus.
SSB achieves state-of-the-art performance on LIDC-IDRI, COCA, and RACER
(in-house) datasets.

</details>


### [157] [Prototype-Based Pseudo-Label Denoising for Source-Free Domain Adaptation in Remote Sensing Semantic Segmentation](https://arxiv.org/abs/2509.16942)
*Bin Wang,Fei Deng,Zeyu Chen,Zhicheng Yu,Yiguang Liu*

Main category: cs.CV

TL;DR: ProSFDA是一种原型引导的无源域适应框架，通过原型加权伪标签和原型对比策略，解决了遥感图像语义分割中无源域适应（SFDA）因伪标签噪声导致域偏移缓解不力的问题。


<details>
  <summary>Details</summary>
Motivation: 在遥感图像语义分割的无源域适应（SFDA）中，目标域缺乏真实标签常导致生成噪声伪标签，从而阻碍了域偏移（DS）的有效缓解。

Method: 本文提出了ProSFDA框架。它采用原型加权伪标签来提高自训练（ST）在伪标签噪声下的可靠性。此外，引入了原型对比策略，鼓励同类特征聚合，从而在无真实标签监督下学习判别性目标域表示。

Result: 广泛的实验表明，ProSFDA方法显著优于现有方法。

Conclusion: ProSFDA通过原型引导的伪标签和对比学习策略，有效解决了遥感图像SFDA中伪标签噪声和域偏移问题，提高了模型的性能和判别力。

Abstract: Source-Free Domain Adaptation (SFDA) enables domain adaptation for semantic
segmentation of Remote Sensing Images (RSIs) using only a well-trained source
model and unlabeled target domain data. However, the lack of ground-truth
labels in the target domain often leads to the generation of noisy
pseudo-labels. Such noise impedes the effective mitigation of domain shift
(DS). To address this challenge, we propose ProSFDA, a prototype-guided SFDA
framework. It employs prototype-weighted pseudo-labels to facilitate reliable
self-training (ST) under pseudo-labels noise. We, in addition, introduce a
prototype-contrast strategy that encourages the aggregation of features
belonging to the same class, enabling the model to learn discriminative target
domain representations without relying on ground-truth supervision. Extensive
experiments show that our approach substantially outperforms existing methods.

</details>


### [158] [Echo-Path: Pathology-Conditioned Echo Video Generation](https://arxiv.org/abs/2509.17190)
*Kabir Hamzah Muhammad,Marawan Elbatel,Yi Qin,Xiaomeng Li*

Main category: cs.CV

TL;DR: Echo-Path是一种新颖的生成框架，用于生成具有特定心脏病理（如ASD和PAH）的超声心动图视频，以解决数据稀缺问题，并提高自动化诊断模型的性能。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病是全球主要的死亡原因，超声心动图对诊断至关重要。然而，某些病理的超声心动图数据稀缺，阻碍了稳健的自动化诊断模型的发展。

Method: 本文提出了Echo-Path，一种新颖的生成框架，能够生成以特定心脏病理为条件的超声心动图视频。该方法将病理条件机制引入到最先进的超声视频生成器中，使其能够学习和控制心脏中疾病特有的结构和运动模式。

Result: 合成视频实现了低分布距离，表明视觉保真度高。临床上，生成的超声心动图表现出合理的病理标记。此外，在合成数据上训练的分类器对真实数据具有良好的泛化能力，并且当用于增强真实训练集时，ASD和PAH的下游诊断分别提高了7%和8%。

Conclusion: Echo-Path成功生成了逼真且具有特定病理特征的超声心动图视频，有效解决了数据稀缺问题，并显著提高了ASD和PAH自动化诊断模型的性能。

Abstract: Cardiovascular diseases (CVDs) remain the leading cause of mortality
globally, and echocardiography is critical for diagnosis of both common and
congenital cardiac conditions. However, echocardiographic data for certain
pathologies are scarce, hindering the development of robust automated diagnosis
models. In this work, we propose Echo-Path, a novel generative framework to
produce echocardiogram videos conditioned on specific cardiac pathologies.
Echo-Path can synthesize realistic ultrasound video sequences that exhibit
targeted abnormalities, focusing here on atrial septal defect (ASD) and
pulmonary arterial hypertension (PAH). Our approach introduces a
pathology-conditioning mechanism into a state-of-the-art echo video generator,
allowing the model to learn and control disease-specific structural and motion
patterns in the heart. Quantitative evaluation demonstrates that the synthetic
videos achieve low distribution distances, indicating high visual fidelity.
Clinically, the generated echoes exhibit plausible pathology markers.
Furthermore, classifiers trained on our synthetic data generalize well to real
data and, when used to augment real training sets, it improves downstream
diagnosis of ASD and PAH by 7\% and 8\% respectively. Code, weights and dataset
are available here https://github.com/Marshall-mk/EchoPathv1

</details>


### [159] [Catching the Details: Self-Distilled RoI Predictors for Fine-Grained MLLM Perception](https://arxiv.org/abs/2509.16944)
*Yuheng Shi,Xiaohuan Pei,Minjing Dong,Chang Xu*

Main category: cs.CV

TL;DR: 本文提出了一种高效、无需标注的自蒸馏区域提议网络（SD-RPN），通过将多模态大语言模型（MLLM）的中间层注意力图转换为高质量的伪RoI标签，并用其训练轻量级RPN，解决了MLLM细粒度感知中高分辨率图像处理的计算开销和现有RoI方法（依赖标注或效率低）的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLM）需要高分辨率视觉信息进行细粒度感知，但处理整个高分辨率图像计算成本过高。现有基于区域兴趣（RoI）的机制存在权衡：基于训练的方法需要大规模标注数据集，而无需训练的方法（利用模型内部注意力）计算效率低下且准确性较低，需要多遍预填充或依赖缓慢的自回归解码过程。

Method: 本文提出了一种高效、无需标注的自蒸馏区域提议网络（SD-RPN）。该方法通过明确去噪和解决歧义，将MLLM中间层产生的噪声注意力图转换为高质量的伪RoI标签。这些伪标签用于训练一个轻量级区域提议网络（RPN），以学习更精确的定位。该RPN在单次前向传播中利用MLLM中间层特征高效预测RoI，将RoI识别与自回归生成解耦，避免了昂贵的多遍操作。

Result: 将SD-RPN框架集成到LLaVA-1.5架构中进行验证。尽管仅使用少量（例如10K）问答对进行训练，该方法仍表现出卓越的数据效率和泛化能力，在TextVQA、DocVQA和V-Star等未见过的基准测试中，绝对准确率提高了10%以上。

Conclusion: 本文的工作为增强MLLM的细粒度感知提供了一个实用且可扩展的解决方案，无需昂贵的监督或完整的模型微调。SD-RPN通过高效、无标注的方式解决了高分辨率图像处理的挑战。

Abstract: Multimodal Large Language Models (MLLMs) require high-resolution visual
information to perform fine-grained perception, yet processing entire
high-resolution images is computationally prohibitive. While recent methods
leverage a Region-of-Interest (RoI) mechanism to focus on salient areas, they
typically present a difficult trade-off: training-based approaches depend on
large-scale annotated datasets, while training-free methods that utilize the
model's internal attention are computationally inefficient and less accurate,
requiring either multi-pass prefill stages or reliance on the slow
auto-regressive decoding process. In this paper, we propose an efficient,
annotation-free Self-Distilled Region Proposal Network (SD-RPN) that resolves
this trade-off. The SD-RPN is built around a pipeline that transforms the noisy
attention maps from the MLLM's middle layers into high-quality pseudo-RoI
labels by explicitly denoising the signal and resolving ambiguity. We use these
labels to train a lightweight Region Proposal Network (RPN) that learns a more
precise localization. This RPN is also highly efficient, predicting the RoI in
a single forward pass using features from the MLLM's middle layers, decoupling
RoI identification from the auto-regressive generation and avoiding costly
multi-pass operations.To validate our approach, we integrate the framework into
the LLaVA-1.5 architecture. Despite being trained on only a few (e.g. 10K)
question-answer pairs, our method demonstrates exceptional data efficiency and
generalization, achieving over a 10% absolute accuracy improvement on unseen
benchmarks, including TextVQA, DocVQA, and V-Star. Our work presents a
practical and scalable solution for enhancing the fine-grained perception of
MLLMs without requiring costly supervision or full model fine-tuning. Code is
available at https://github.com/YuHengsss/SD-RPN.

</details>


### [160] [Guided and Unguided Conditional Diffusion Mechanisms for Structured and Semantically-Aware 3D Point Cloud Generation](https://arxiv.org/abs/2509.17206)
*Gunner Stone,Sushmita Sarker,Alireza Tavakkoli*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型的三维点云生成框架，通过在生成过程中直接嵌入逐点语义条件，实现了几何和语义的联合合成，生成了结构连贯且语义感知的点云。


<details>
  <summary>Details</summary>
Motivation: 现有三维点云生成方法主要关注几何结构，语义信息通常是后处理添加的，而非整合到生成过程中。需要一种能同时生成几何和语义的集成方法。

Method: 提出了一种基于扩散（diffusion）的框架。该框架将每个点的语义标签作为条件变量，直接嵌入到生成过程中。这些条件变量引导扩散动力学，从而实现几何和语义的联合合成。

Result: 该方法生成的点云结构连贯且具有语义感知能力，物体部件在合成过程中被明确表示。通过对比分析，证明了条件变量对扩散动力学和生成质量的显著影响。大量实验验证了该方法的有效性，能生成针对特定部件和特征的详细、准确的三维点云。

Conclusion: 所提出的扩散框架成功地将逐点语义条件整合到三维点云生成中，实现了几何和语义的联合合成，从而生成了高质量、语义感知的点云，为遥感、机器人和数字建模等应用提供了新的可能性。

Abstract: Generating realistic 3D point clouds is a fundamental problem in computer
vision with applications in remote sensing, robotics, and digital object
modeling. Existing generative approaches primarily capture geometry, and when
semantics are considered, they are typically imposed post hoc through external
segmentation or clustering rather than integrated into the generative process
itself. We propose a diffusion-based framework that embeds per-point semantic
conditioning directly within generation. Each point is associated with a
conditional variable corresponding to its semantic label, which guides the
diffusion dynamics and enables the joint synthesis of geometry and semantics.
This design produces point clouds that are both structurally coherent and
segmentation-aware, with object parts explicitly represented during synthesis.
Through a comparative analysis of guided and unguided diffusion processes, we
demonstrate the significant impact of conditional variables on diffusion
dynamics and generation quality. Extensive experiments validate the efficacy of
our approach, producing detailed and accurate 3D point clouds tailored to
specific parts and features.

</details>


### [161] [Leveraging RGB Images for Pre-Training of Event-Based Hand Pose Estimation](https://arxiv.org/abs/2509.16949)
*Ruicong Liu,Takehiko Ohkawa,Tze Ho Elden Tse,Mingfang Zhang,Angela Yao,Yoichi Sato*

Main category: cs.CV

TL;DR: RPEP是首个针对事件流3D手部姿态估计的预训练方法，利用标注RGB图像和未配对的未标注事件数据，通过新颖的事件生成策略和运动反转约束，显著提升了在真实事件数据上的性能。


<details>
  <summary>Details</summary>
Motivation: 事件数据具有高时间分辨率和低延迟的优势，但其在手部姿态估计中的应用受限于标注训练数据的稀缺性。现有的伪事件生成技术假设物体静止，难以处理动态移动的手部。

Method: RPEP通过构建伪事件-RGB对来预训练事件流估计器。它引入了一种新颖的生成策略，将手部运动分解为小步长的运动，以捕捉关节的时间变化并生成更真实的动态手部事件数据。此外，RPEP还施加了运动反转约束来正则化事件生成过程。

Result: RPEP预训练模型在真实事件数据上显著优于现有最先进方法，在EvRealHands数据集上实现了高达24%的改进。此外，它在微调时只需少量标注样本即可提供强大性能。

Conclusion: RPEP提供了一种有效且实用的解决方案，用于解决事件流3D手部姿态估计中缺乏标注数据的问题，特别适用于实际部署场景。

Abstract: This paper presents RPEP, the first pre-training method for event-based 3D
hand pose estimation using labeled RGB images and unpaired, unlabeled event
data. Event data offer significant benefits such as high temporal resolution
and low latency, but their application to hand pose estimation is still limited
by the scarcity of labeled training data. To address this, we repurpose real
RGB datasets to train event-based estimators. This is done by constructing
pseudo-event-RGB pairs, where event data is generated and aligned with the
ground-truth poses of RGB images. Unfortunately, existing pseudo-event
generation techniques assume stationary objects, thus struggling to handle
non-stationary, dynamically moving hands. To overcome this, RPEP introduces a
novel generation strategy that decomposes hand movements into smaller,
step-by-step motions. This decomposition allows our method to capture temporal
changes in articulation, constructing more realistic event data for a moving
hand. Additionally, RPEP imposes a motion reversal constraint, regularizing
event generation using reversed motion. Extensive experiments show that our
pre-trained model significantly outperforms state-of-the-art methods on real
event data, achieving up to 24% improvement on EvRealHands. Moreover, it
delivers strong performance with minimal labeled samples for fine-tuning,
making it well-suited for practical deployment.

</details>


### [162] [Point-RTD: Replaced Token Denoising for Pretraining Transformer Models on Point Clouds](https://arxiv.org/abs/2509.17207)
*Gunner Stone,Youngsook Choi,Alireza Tavakkoli,Ankita Shukla*

Main category: cs.CV

TL;DR: Point-RTD是一种新颖的3D点云预训练策略，通过损坏-重建框架和判别器-生成器架构，提升了token鲁棒性，并在多项基准测试中显著超越了PointMAE。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在3D点云任务中的性能高度依赖于预训练策略。作者旨在通过改进token鲁棒性来提升模型性能和效率，并指出传统基于掩码的重建任务存在局限性。

Method: Point-RTD引入了一种损坏-重建（corruption-reconstruction）框架。它通过损坏点云token，并利用判别器-生成器（discriminator-generator）架构进行去噪，而非传统的掩码预测。

Result: 在ShapeNet数据集上，Point-RTD将重建误差比PointMAE降低了93%以上，测试集上的Chamfer距离降低了14倍以上。此外，该方法收敛更快，并在ShapeNet、ModelNet10和ModelNet40基准测试中取得了更高的分类准确率，全面优于Point-MAE。

Conclusion: Point-RTD是一种更有效的3D点云预训练策略，通过其独特的损坏-重建机制显著提升了模型的重建能力、效率和分类准确性，优于现有的Point-MAE框架。

Abstract: Pre-training strategies play a critical role in advancing the performance of
transformer-based models for 3D point cloud tasks. In this paper, we introduce
Point-RTD (Replaced Token Denoising), a novel pretraining strategy designed to
improve token robustness through a corruption-reconstruction framework. Unlike
traditional mask-based reconstruction tasks that hide data segments for later
prediction, Point-RTD corrupts point cloud tokens and leverages a
discriminator-generator architecture for denoising. This shift enables more
effective learning of structural priors and significantly enhances model
performance and efficiency. On the ShapeNet dataset, Point-RTD reduces
reconstruction error by over 93% compared to PointMAE, and achieves more than
14x lower Chamfer Distance on the test set. Our method also converges faster
and yields higher classification accuracy on ShapeNet, ModelNet10, and
ModelNet40 benchmarks, clearly outperforming the baseline Point-MAE framework
in every case.

</details>


### [163] [VidCLearn: A Continual Learning Approach for Text-to-Video Generation](https://arxiv.org/abs/2509.16956)
*Luca Zanchetta,Lorenzo Papa,Luca Maiano,Irene Amerini*

Main category: cs.CV

TL;DR: VidCLearn是一个基于扩散模型的文本到视频持续学习框架，通过师生架构和新的损失函数，解决了现有模型难以增量学习的问题，并提升了生成视频的质量和时间一致性。


<details>
  <summary>Details</summary>
Motivation: 当前的文本到视频生成模型依赖静态知识，难以在不从头训练的情况下整合新数据，这限制了其适应性和扩展性。

Method: 本文提出了VidCLearn，一个用于扩散模型文本到视频生成的持续学习框架。它采用师生架构，学生模型增量更新新数据，教师模型通过生成式回放保留旧知识。此外，引入了新颖的时间一致性损失以增强运动平滑性，并集成了视频检索模块在推理时提供结构指导。该架构也比现有模型更具计算效率。

Result: 实验结果表明，VidCLearn在视觉质量、语义对齐和时间连贯性方面优于基线方法。

Conclusion: VidCLearn成功解决了文本到视频生成中增量学习的挑战，提升了视频质量、语义准确性和时间一致性，并提高了计算效率。

Abstract: Text-to-video generation is an emerging field in generative AI, enabling the
creation of realistic, semantically accurate videos from text prompts. While
current models achieve impressive visual quality and alignment with input text,
they typically rely on static knowledge, making it difficult to incorporate new
data without retraining from scratch. To address this limitation, we propose
VidCLearn, a continual learning framework for diffusion-based text-to-video
generation. VidCLearn features a student-teacher architecture where the student
model is incrementally updated with new text-video pairs, and the teacher model
helps preserve previously learned knowledge through generative replay.
Additionally, we introduce a novel temporal consistency loss to enhance motion
smoothness and a video retrieval module to provide structural guidance at
inference. Our architecture is also designed to be more computationally
efficient than existing models while retaining satisfactory generation
performance. Experimental results show VidCLearn's superiority over baseline
methods in terms of visual quality, semantic alignment, and temporal coherence.

</details>


### [164] [Automated Facility Enumeration for Building Compliance Checking using Door Detection and Large Language Models](https://arxiv.org/abs/2509.17283)
*Licheng Zhan,Bach Le,Naveed Akhtar,Tuan Ngo*

Main category: cs.CV

TL;DR: 本文提出了一种新的建筑合规性检查任务：自动化设施枚举，旨在验证设施数量是否符合法规要求。为解决此问题，作者们提出了一种结合门检测和基于大型语言模型（LLM）推理的方法，并首次将LLM应用于此任务，通过思维链（CoT）管道进一步提升了性能。


<details>
  <summary>Details</summary>
Motivation: 建筑合规性检查（BCC）中，准确枚举设施类型及其空间分布至关重要，但这一问题在现有文献中被忽视，导致了BCC工作流程中的关键空白。手动执行此任务耗时且劳动密集。大型语言模型（LLM）的最新进展为结合视觉识别和推理能力实现自动化提供了新机遇。

Method: 本文提出了一种新颖的方法，将门检测与基于LLM的推理相结合。作者们首次将LLM应用于自动化设施枚举任务，并通过思维链（CoT）管道进一步增强了LLM的性能。

Result: 该方法在不同的数据集和设施类型上表现出良好的泛化能力。在真实世界和合成平面图数据上的实验证明了该方法的有效性和鲁棒性。

Conclusion: 本文引入了一项新的BCC任务——自动化设施枚举，并提出了一种结合门检测和LLM推理（通过CoT增强）的创新方法。该方法有效且鲁棒，解决了BCC中的一个关键空白。

Abstract: Building compliance checking (BCC) is a critical process for ensuring that
constructed facilities meet regulatory standards. A core component of BCC is
the accurate enumeration of facility types and their spatial distribution.
Despite its importance, this problem has been largely overlooked in the
literature, posing a significant challenge for BCC and leaving a critical gap
in existing workflows. Performing this task manually is time-consuming and
labor-intensive. Recent advances in large language models (LLMs) offer new
opportunities to enhance automation by combining visual recognition with
reasoning capabilities. In this paper, we introduce a new task for BCC:
automated facility enumeration, which involves validating the quantity of each
facility type against statutory requirements. To address it, we propose a novel
method that integrates door detection with LLM-based reasoning. We are the
first to apply LLMs to this task and further enhance their performance through
a Chain-of-Thought (CoT) pipeline. Our approach generalizes well across diverse
datasets and facility types. Experiments on both real-world and synthetic floor
plan data demonstrate the effectiveness and robustness of our method.

</details>


### [165] [MO R-CNN: Multispectral Oriented R-CNN for Object Detection in Remote Sensing Image](https://arxiv.org/abs/2509.16957)
*Leiyu Wang,Biao Jin,Feng Huang,Liqiong Chen,Zhengyong Wang,Xiaohai He,Honggang Chen*

Main category: cs.CV

TL;DR: 本文提出了一种名为MO R-CNN的轻量级多光谱定向目标检测框架，通过异构特征提取网络(HFEN)、单模态监督(SMS)和基于条件的跨模态标签融合(CMLF)来解决现有方法的计算复杂性问题并提高检测性能。


<details>
  <summary>Details</summary>
Motivation: 多光谱图像的定向目标检测面临模态内部和模态之间差异带来的挑战。现有方法虽然通过复杂网络架构提高了检测精度，但其高计算复杂度和内存消耗严重限制了性能。受遥感领域大核卷积成功的启发，作者旨在开发一个更轻量级但高效的解决方案。

Method: MO R-CNN框架包含三个核心组件：1. 异构特征提取网络 (HFEN)：利用模态间差异自适应地对多模态特征进行对齐、融合和增强。2. 单模态监督 (SMS)：约束多尺度特征，使模型能够从多个模态中学习。3. 基于条件的跨模态标签融合 (CMLF)：根据特定规则融合多模态标签，为模型提供更鲁棒和一致的监督信号。

Result: 在DroneVehicle、VEDAI和OGSOD数据集上的实验证明了该方法的优越性。

Conclusion: MO R-CNN是一个针对多光谱定向目标检测的轻量级框架，通过其创新的HFEN、SMS和CMLF组件，有效解决了现有方法的计算复杂性问题，并在多个数据集上展现出卓越的性能。

Abstract: Oriented object detection for multi-spectral imagery faces significant
challenges due to differences both within and between modalities. Although
existing methods have improved detection accuracy through complex network
architectures, their high computational complexity and memory consumption
severely restrict their performance. Motivated by the success of large kernel
convolutions in remote sensing, we propose MO R-CNN, a lightweight framework
for multi-spectral oriented detection featuring heterogeneous feature
extraction network (HFEN), single modality supervision (SMS), and
condition-based multimodal label fusion (CMLF). HFEN leverages inter-modal
differences to adaptively align, merge, and enhance multi-modal features. SMS
constrains multi-scale features and enables the model to learn from multiple
modalities. CMLF fuses multimodal labels based on specific rules, providing the
model with a more robust and consistent supervisory signal. Experiments on the
DroneVehicle, VEDAI and OGSOD datasets prove the superiority of our method. The
source code is available at:https://github.com/Iwill-github/MORCNN.

</details>


### [166] [Pre-Trained CNN Architecture for Transformer-Based Image Caption Generation Model](https://arxiv.org/abs/2509.17365)
*Amanuel Tafese Dufera*

Main category: cs.CV

TL;DR: 该项目提出并详细介绍了如何构建基于Transformer模型的图像字幕生成系统，以克服传统CNN-LSTM模型在训练速度和长序列信息保留方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统的CNN-LSTM模型在图像字幕生成任务中存在局限性，具体表现为：RNN的序列特性导致训练和推理速度慢，以及LSTM在处理非常长的序列时难以保留早期信息。

Method: 该项目利用Transformer架构及其自注意力机制，构建图像字幕模型。具体方法包括：使用Flickr30k数据集进行数据预处理，采用EfficientNetB0 CNN进行图像特征提取，并整合注意力机制训练模型，以实现高效的并行化训练和推理。

Result: 该方法有效利用了并行化，实现了高效的训练和推理过程，展示了Transformer模型在图像字幕生成中的优势。

Conclusion: Transformer模型通过其自注意力机制和并行化能力，为图像字幕生成提供了一个高效且能更好处理长短依赖关系的解决方案，克服了传统CNN-LSTM模型的不足。

Abstract: Automatic image captioning, a multifaceted task bridging computer vision and
natural lan- guage processing, aims to generate descriptive textual content
from visual input. While Convolutional Neural Networks (CNNs) and Long
Short-Term Memory (LSTM) networks have achieved significant advancements, they
present limitations. The inherent sequential nature of RNNs leads to sluggish
training and inference times. LSTMs further struggle with retaining information
from earlier sequence elements when dealing with very long se- quences. This
project presents a comprehensive guide to constructing and comprehending
transformer models for image captioning. Transformers employ self-attention
mechanisms, capturing both short- and long-range dependencies within the data.
This facilitates efficient parallelization during both training and inference
phases. We leverage the well-established Transformer architecture, recognized
for its effectiveness in managing sequential data, and present a meticulous
methodology. Utilizing the Flickr30k dataset, we conduct data pre- processing,
construct a model architecture that integrates an EfficientNetB0 CNN for fea-
ture extraction, and train the model with attention mechanisms incorporated.
Our approach exemplifies the utilization of parallelization for efficient
training and inference. You can find the project on GitHub.

</details>


### [167] [Penalizing Boundary Activation for Object Completeness in Diffusion Models](https://arxiv.org/abs/2509.16968)
*Haoyang Xu,Tianhao Zhao,Sibei Yang,Yutian Li*

Main category: cs.CV

TL;DR: 扩散模型在文本到图像生成中存在物体不完整的问题，本文发现这主要归因于训练中的RandomCrop数据增强。为解决此问题，提出了一种免训练方案，在去噪早期阶段惩罚图像边界处的激活值，有效提升了物体完整性和图像质量。


<details>
  <summary>Details</summary>
Motivation: 文本到图像（T2I）扩散模型生成的图像中常出现物体不完整（碎片或缺失部分）的限制，这严重影响了模型在下游应用中的性能。

Method: 深入分析了物体不完整问题，揭示其主要原因是训练中广泛使用的RandomCrop数据增强破坏了物体连续性。为解决此问题，提出了一种免训练方案，即在早期去噪步骤中惩罚图像边界处的激活值。该方法易于应用于预训练的Stable Diffusion模型，修改量极小且计算开销可忽略不计。

Result: 广泛的实验证明了该方法的有效性，显著改善了物体的完整性和图像质量。

Conclusion: 所提出的免训练解决方案能有效解决扩散模型中物体生成不完整的问题，通过在去噪早期阶段惩罚边界激活值，显著提高了图像质量和物体完整性，且易于部署。

Abstract: Diffusion models have emerged as a powerful technique for text-to-image (T2I)
generation, creating high-quality, diverse images across various domains.
However, a common limitation in these models is the incomplete display of
objects, where fragments or missing parts undermine the model's performance in
downstream applications. In this study, we conduct an in-depth analysis of the
incompleteness issue and reveal that the primary factor behind incomplete
object generation is the usage of RandomCrop during model training. This widely
used data augmentation method, though enhances model generalization ability,
disrupts object continuity during training. To address this, we propose a
training-free solution that penalizes activation values at image boundaries
during the early denoising steps. Our method is easily applicable to
pre-trained Stable Diffusion models with minimal modifications and negligible
computational overhead. Extensive experiments demonstrate the effectiveness of
our method, showing substantial improvements in object integrity and image
quality.

</details>


### [168] [Interpreting vision transformers via residual replacement model](https://arxiv.org/abs/2509.17401)
*Jinyeong Kim,Junhyeok Kim,Yumin Shim,Joohyeok Kim,Sunyoung Jung,Seong Jae Hwang*

Main category: cs.CV

TL;DR: 本文通过稀疏自编码器提取ViT特征并引入残差替换模型，系统分析了ViT如何表示和处理世界，揭示了特征演化和专门的编码机制，并提供了一个可解释的框架。


<details>
  <summary>Details</summary>
Motivation: 解决ViT如何表示和处理世界这一长期存在的问题。

Method: 1. 通过稀疏自编码器系统分析了所有层中的6.6K个特征。2. 引入了残差替换模型，用残差流中可解释的特征替换ViT的计算。

Result: 1. 揭示了特征从低级模式到高级语义的演变。2. 发现ViT通过专门的特征类型编码曲线和空间位置。3. 残差替换模型能够可扩展地生成一个忠实而简洁的电路，实现人类尺度的可解释性，并显著简化了原始计算。4. 该框架能够直观地理解ViT机制。

Conclusion: 该框架不仅有助于直观理解ViT机制，还在消除虚假相关性方面展现了实用性。

Abstract: How do vision transformers (ViTs) represent and process the world? This paper
addresses this long-standing question through the first systematic analysis of
6.6K features across all layers, extracted via sparse autoencoders, and by
introducing the residual replacement model, which replaces ViT computations
with interpretable features in the residual stream. Our analysis reveals not
only a feature evolution from low-level patterns to high-level semantics, but
also how ViTs encode curves and spatial positions through specialized feature
types. The residual replacement model scalably produces a faithful yet
parsimonious circuit for human-scale interpretability by significantly
simplifying the original computations. As a result, this framework enables
intuitive understanding of ViT mechanisms. Finally, we demonstrate the utility
of our framework in debiasing spurious correlations.

</details>


### [169] [LLM-Assisted Semantic Guidance for Sparsely Annotated Remote Sensing Object Detection](https://arxiv.org/abs/2509.16970)
*Wei Liao,Chunyan Xu,Chenxu Wang,Zhen Cui*

Main category: cs.CV

TL;DR: 本文提出了一种LLM辅助的语义引导框架，用于稀疏标注遥感目标检测。通过利用LLM的语义推理能力生成高置信度伪标签，并结合类别感知伪标签分配和自适应硬负样本重加权模块，显著提升了稀疏标注下的检测性能。


<details>
  <summary>Details</summary>
Motivation: 遥感目标检测中稀疏标注面临目标密集分布和类别不平衡的挑战。现有密集伪标签方法存在选择模糊和置信度估计不一致的问题。

Method: 本文引入了一个LLM辅助的语义引导框架，利用LLM的语义推理能力提取高置信度伪标签。提出了一个类别感知密集伪标签分配机制，整合LLM生成的语义先验，为未标注和稀疏标注数据自适应分配伪标签。此外，开发了一个自适应硬负样本重加权模块，通过减轻背景信息的干扰来稳定监督学习分支。

Result: 在DOTA和HRSC2016数据集上的大量实验表明，所提出的方法优于现有的基于单阶段检测器的框架，显著改善了稀疏标注下的检测性能。

Conclusion: 该LLM辅助的语义引导框架通过其创新的伪标签分配和负样本重加权机制，有效解决了稀疏标注遥感目标检测的挑战，显著提升了检测性能。

Abstract: Sparse annotation in remote sensing object detection poses significant
challenges due to dense object distributions and category imbalances. Although
existing Dense Pseudo-Label methods have demonstrated substantial potential in
pseudo-labeling tasks, they remain constrained by selection ambiguities and
inconsistencies in confidence estimation.In this paper, we introduce an
LLM-assisted semantic guidance framework tailored for sparsely annotated remote
sensing object detection, exploiting the advanced semantic reasoning
capabilities of large language models (LLMs) to distill high-confidence
pseudo-labels.By integrating LLM-generated semantic priors, we propose a
Class-Aware Dense Pseudo-Label Assignment mechanism that adaptively assigns
pseudo-labels for both unlabeled and sparsely labeled data, ensuring robust
supervision across varying data distributions. Additionally, we develop an
Adaptive Hard-Negative Reweighting Module to stabilize the supervised learning
branch by mitigating the influence of confounding background information.
Extensive experiments on DOTA and HRSC2016 demonstrate that the proposed method
outperforms existing single-stage detector-based frameworks, significantly
improving detection performance under sparse annotations.

</details>


### [170] [Real-Time Fish Detection in Indonesian Marine Ecosystems Using Lightweight YOLOv10-nano Architecture](https://arxiv.org/abs/2509.17406)
*Jonathan Wuntu,Muhamad Dwisnanto Putro,Rendy Syahputra*

Main category: cs.CV

TL;DR: 本研究探索了YOLOv10-nano模型在印度尼西亚水域进行实时海洋鱼类检测的潜力，该模型在保持低计算需求的同时，实现了高检测精度和快速推理速度，适用于海洋生态监测和保护。


<details>
  <summary>Details</summary>
Motivation: 印度尼西亚的海洋生态系统生物多样性丰富，需要高效的监测工具来支持保护工作。传统的鱼类检测方法耗时且需要专业知识，因此需要自动化解决方案。

Method: 本研究实施了最先进的深度学习模型YOLOv10-nano，并利用DeepFish和OpenImages V7-Fish数据集进行训练和评估。模型在布纳肯国家海洋公园的测试数据上进行了验证。YOLOv10的架构特点包括CSPNet骨干网络、PAN特征融合和金字塔空间注意力模块。

Result: YOLOv10-nano取得了较高的检测精度，mAP50为0.966，mAP50:95为0.606，同时保持了较低的计算需求（2.7M参数，8.4 GFLOPs）。在CPU上实现了29.29 FPS的平均推理速度，适合实时部署。OpenImages V7-Fish数据集虽然单独使用时精度较低，但补充DeepFish数据集后增强了模型的鲁棒性。

Conclusion: 本研究表明YOLOv10-nano在数据受限的环境中，具有高效、可扩展的海洋鱼类监测和保护应用的潜力。

Abstract: Indonesia's marine ecosystems, part of the globally recognized Coral
Triangle, are among the richest in biodiversity, requiring efficient monitoring
tools to support conservation. Traditional fish detection methods are
time-consuming and demand expert knowledge, prompting the need for automated
solutions. This study explores the implementation of YOLOv10-nano, a
state-of-the-art deep learning model, for real-time marine fish detection in
Indonesian waters, using test data from Bunaken National Marine Park. YOLOv10's
architecture, featuring improvements like the CSPNet backbone, PAN for feature
fusion, and Pyramid Spatial Attention Block, enables efficient and accurate
object detection even in complex environments. The model was evaluated on the
DeepFish and OpenImages V7-Fish datasets. Results show that YOLOv10-nano
achieves a high detection accuracy with mAP50 of 0.966 and mAP50:95 of 0.606
while maintaining low computational demand (2.7M parameters, 8.4 GFLOPs). It
also delivered an average inference speed of 29.29 FPS on the CPU, making it
suitable for real-time deployment. Although OpenImages V7-Fish alone provided
lower accuracy, it complemented DeepFish in enhancing model robustness.
Overall, this study demonstrates YOLOv10-nano's potential for efficient,
scalable marine fish monitoring and conservation applications in data-limited
environments.

</details>


### [171] [Optimal Transport for Handwritten Text Recognition in a Low-Resource Regime](https://arxiv.org/abs/2509.16977)
*Petros Georgoulas Wraight,Giorgos Sfikas,Ioannis Kordonis,Petros Maragos,George Retsinas*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的框架，通过迭代视觉-语义对齐和伪标签生成，显著提高了低资源手写文本识别（HTR）的准确性，解决了传统方法对大量标注数据依赖的问题。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的手写文本识别（HTR）方法需要大量标注数据进行训练，这在历史档案或小型现代集合等低资源领域中不切实际。

Method: 该框架利用词汇特征的先验知识，提出了一种迭代自举方法。它使用最优传输（OT）将从未标注图像中提取的视觉特征与语义词表示对齐。从少量标注示例开始，该框架迭代地将词图像与文本标签匹配，为高置信度对齐生成伪标签，并使用不断增长的数据集重新训练识别器。

Result: 数值实验表明，所提出的迭代视觉-语义对齐方案显著提高了低资源HTR基准测试的识别准确性。

Conclusion: 该研究提供了一个有效的解决方案，通过利用词汇先验知识和迭代自举方法，在标注数据稀缺的情况下，显著提升了手写文本识别的性能。

Abstract: Handwritten Text Recognition (HTR) is a task of central importance in the
field of document image understanding. State-of-the-art methods for HTR require
the use of extensive annotated sets for training, making them impractical for
low-resource domains like historical archives or limited-size modern
collections. This paper introduces a novel framework that, unlike the standard
HTR model paradigm, can leverage mild prior knowledge of lexical
characteristics; this is ideal for scenarios where labeled data are scarce. We
propose an iterative bootstrapping approach that aligns visual features
extracted from unlabeled images with semantic word representations using
Optimal Transport (OT). Starting with a minimal set of labeled examples, the
framework iteratively matches word images to text labels, generates
pseudo-labels for high-confidence alignments, and retrains the recognizer on
the growing dataset. Numerical experiments demonstrate that our iterative
visual-semantic alignment scheme significantly improves recognition accuracy on
low-resource HTR benchmarks.

</details>


### [172] [Training-Free Label Space Alignment for Universal Domain Adaptation](https://arxiv.org/abs/2509.17452)
*Dujin Lee,Sojung An,Jungmyung Wi,Kuniaki Saito,Donghyun Kim*

Main category: cs.CV

TL;DR: 解析错误


<details>
  <summary>Details</summary>
Motivation: 解析错误

Method: 解析错误

Result: 解析错误

Conclusion: 解析错误

Abstract: Universal domain adaptation (UniDA) transfers knowledge from a labeled source
domain to an unlabeled target domain, where label spaces may differ and the
target domain may contain private classes. Previous UniDA methods primarily
focused on visual space alignment but often struggled with visual ambiguities
due to content differences, which limited their robustness and
generalizability. To overcome this, we introduce a novel approach that
leverages the strong \textit{zero-shot capabilities} of recent vision-language
foundation models (VLMs) like CLIP, concentrating solely on label space
alignment to enhance adaptation stability. CLIP can generate task-specific
classifiers based only on label names. However, adapting CLIP to UniDA is
challenging because the label space is not fully known in advance. In this
study, we first utilize generative vision-language models to identify unknown
categories in the target domain. Noise and semantic ambiguities in the
discovered labels -- such as those similar to source labels (e.g., synonyms,
hypernyms, hyponyms) -- complicate label alignment. To address this, we propose
a training-free label-space alignment method for UniDA (\ours). Our method
aligns label spaces instead of visual spaces by filtering and refining noisy
labels between the domains. We then construct a \textit{universal classifier}
that integrates both shared knowledge and target-private class information,
thereby improving generalizability under domain shifts. The results reveal that
the proposed method considerably outperforms existing UniDA techniques across
key DomainBed benchmarks, delivering an average improvement of
\textcolor{blue}{+7.9\%}in H-score and \textcolor{blue}{+6.1\%} in H$^3$-score.
Furthermore, incorporating self-training further enhances performance and
achieves an additional (\textcolor{blue}{+1.6\%}) increment in both H- and
H$^3$-scores.

</details>


### [173] [VCE: Safe Autoregressive Image Generation via Visual Contrast Exploitation](https://arxiv.org/abs/2509.16986)
*Feng Han,Chao Gong,Zhipeng Wei,Jingjing Chen,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 针对自回归图像生成模型中存在的生成不安全内容的问题，本文提出了VCE框架，通过对比图像对构建和DPO训练，有效实现了概念擦除，在保持安全内容完整性的同时达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 自回归图像生成模型（如GPT-4o、LlamaGen）能生成逼真的图像，但也可能产生不安全（NSFW）或侵犯版权的内容，引发了伦理和版权担忧。现有概念擦除方法主要针对扩散模型，不适用于逐令牌生成的自回归模型，因此存在一个关键的技术空白。

Method: 本文提出了视觉对比利用（Visual Contrast Exploitation, VCE）框架，包括：1) 一种创新的对比图像对构建范式，用于精确解耦不安全概念及其相关内容语义；2) 一种基于DPO（Direct Preference Optimization）的训练方法，增强模型识别和利用图像对中视觉对比特征的能力，从而实现精确的概念擦除。

Result: 在艺术家风格擦除、显式内容擦除和物体移除这三项挑战性任务上的综合实验表明，VCE方法能有效保护模型安全，在擦除不安全概念的同时，保持不相关安全概念的完整性，并取得了最先进的成果。

Conclusion: VCE框架成功解决了自回归文本到图像模型安全防护的空白，通过有效擦除不安全概念，实现了最先进的性能，并维护了安全内容的完整性。

Abstract: Recently, autoregressive image generation models have wowed audiences with
their remarkable capability in creating surprisingly realistic images. Models
such as GPT-4o and LlamaGen can not only produce images that faithfully mimic
renowned artistic styles like Ghibli, Van Gogh, or Picasso, but also
potentially generate Not-Safe-For-Work (NSFW) content, raising significant
concerns regarding copyright infringement and ethical use. Despite these
concerns, methods to safeguard autoregressive text-to-image models remain
underexplored. Previous concept erasure methods, primarily designed for
diffusion models that operate in denoising latent space, are not directly
applicable to autoregressive models that generate images token by token. To
address this critical gap, we propose Visual Contrast Exploitation (VCE), a
novel framework comprising: (1) an innovative contrastive image pair
construction paradigm that precisely decouples unsafe concepts from their
associated content semantics, and (2) a sophisticated DPO-based training
approach that enhances the model's ability to identify and leverage visual
contrastive features from image pairs, enabling precise concept erasure. Our
comprehensive experiments across three challenging tasks-artist style erasure,
explicit content erasure, and object removal-demonstrate that our method
effectively secures the model, achieving state-of-the-art results while erasing
unsafe concepts and maintaining the integrity of unrelated safe concepts. The
code and models are available at https://github.com/Maplebb/VCE.

</details>


### [174] [A Cross-Hierarchical Multi-Feature Fusion Network Based on Multiscale Encoder-Decoder for Hyperspectral Change Detection](https://arxiv.org/abs/2509.16988)
*Mingshuai Sheng,Bhatti Uzair Aslam,Junfeng Zhang,Siling Feng,Yonis Gulzar*

Main category: cs.CV

TL;DR: 本文提出了一种基于多尺度编码器-解码器架构的跨层级多特征融合网络（CHMFFN），用于高光谱变化检测，旨在解决现有方法在多尺度特征利用不足和差异特征融合效率低下的问题。


<details>
  <summary>Details</summary>
Motivation: 现有高光谱变化检测方法在多尺度特征利用方面存在不足，且在差异特征融合效率上表现不佳。

Method: 本文提出了CHMFFN网络。其前端采用多尺度特征提取子网络，基于带有残差连接和双核通道-空间注意力（DCCSA）模块的编码器-解码器骨干网络，以提取光谱-空间-时间特征（SSTF）。编码器通过残差块和不同感受野的卷积核捕获多尺度特征；解码器通过跳跃连接整合编码器特征，恢复空间分辨率并抑制噪声。此外，还引入了光谱-时间变化特征学习（STCFL）模块来学习不同层级的跨时间变化特征，以及自适应高级特征融合（AFAF）模块，通过自适应权重动态平衡分层差异特征。

Result: 在四个公共高光谱数据集上的实验结果表明，CHMFFN优于现有最先进的方法。

Conclusion: CHMFFN网络在解决高光谱变化检测中的多尺度特征利用和差异特征融合效率问题上是有效且优越的。

Abstract: Hyperspectral change detection (HCD) aims to accurately identify land-cover
changes in hyperspectral images of the same area acquired at different times,
with key applications in environmental monitoring and disaster assessment. To
address limitations of existing methods, such as insufficient use of multiscale
features and low efficiency in differential feature fusion, this paper proposes
a cross-hierarchical multi-feature fusion network (CHMFFN) based on a
multiscale encoder-decoder architecture. The front-end adopts a multiscale
feature extraction subnetwork, built on an encoder-decoder backbone with
residual connections and a dual-core channel-spatial attention (DCCSA) module
to extract spectral-spatial-temporal features (SSTF). The encoder captures
multiscale features from shallow details to deep semantics via residual blocks
and convolutional kernels with varying receptive fields. The decoder restores
spatial resolution and suppresses noise information through skip connections
integrating encoder features. Additionally, a spectral-temporal change feature
learning (STCFL) module learns cross-temporal change features at different
levels, strengthening inter-temporal difference capture. An adaptive fusion of
advanced features (AFAF) module dynamically balances hierarchical differential
features via adaptive weights, enhancing representation of complex changes.
Experiments on four public hyperspectral datasets show CHMFFN outperforms
state-of-the-art methods, verifying its effectiveness.

</details>


### [175] [Explainable AI for Analyzing Person-Specific Patterns in Facial Recognition Tasks](https://arxiv.org/abs/2509.17457)
*Paweł Jakub Borsukiewicz,Jordan Samhi,Jacques Klein,Tegawendé F. Bissyandé*

Main category: cs.CV

TL;DR: 该研究引入了LEAM（层嵌入激活映射）技术，这是一种可解释性方法，用于识别面部识别模型在个体层面上的关键面部区域。研究发现模型优先关注面部中央区域，并揭示了人脸识别的个体特异性模式，为未来定制化隐私保护系统奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 面部识别系统的普及带来了严重的隐私风险。现有的对抗性技术通常采用通用方法，未能适应个体面部特征，导致其有效性和隐蔽性受限。因此，需要一种能够理解面部识别系统如何工作的技术，尤其是在个体层面，以指导未来的隐私保护研究。

Method: 本研究引入了Layer Embedding Activation Mapping (LEAM)，这是一种新颖的技术，用于识别对个体识别贡献最大的面部区域。LEAM是一种可解释性技术，旨在理解系统工作原理而非进行对抗性攻击。研究将LEAM与面部解析器结合，分析了来自1000个个体在9个预训练面部识别模型上的数据。通过验证遮挡（validation occlusions）来确认LEAM识别出的相关面部区域。

Result: 分析显示，面部识别模型中不同层的关注区域差异显著，但在整体激活模式上，不同架构的模型通常优先关注相似的面部区域。模型在同一人图像之间的激活模式相似度（Bhattacharyya系数：0.32-0.57）远高于不同人图像之间（0.04-0.13），证实了存在个体特异性的识别模式。研究发现面部识别模型优先关注面部中央区域（鼻部区域占关键识别区域的18.9-29.7%），但注意力也分布在多个面部片段。仅使用1%最相关的、由LEAM识别出的图像像素进行验证遮挡，即可确认相关区域，且这些区域在不同模型之间具有可迁移性。

Conclusion: 本研究的发现为未来以LEAM选择的区域为核心的、针对个体量身定制的隐私保护系统奠定了基础。通过理解面部识别模型在个体层面的工作原理，可以为开发更有效、更隐蔽的隐私保护对策提供关键见解。

Abstract: The proliferation of facial recognition systems presents major privacy risks,
driving the need for effective countermeasures. Current adversarial techniques
apply generalized methods rather than adapting to individual facial
characteristics, limiting their effectiveness and inconspicuousness. In this
work, we introduce Layer Embedding Activation Mapping (LEAM), a novel technique
that identifies which facial areas contribute most to recognition at an
individual level. Unlike adversarial attack methods that aim to fool
recognition systems, LEAM is an explainability technique designed to understand
how these systems work, providing insights that could inform future privacy
protection research. We integrate LEAM with a face parser to analyze data from
1000 individuals across 9 pre-trained facial recognition models.
  Our analysis reveals that while different layers within facial recognition
models vary significantly in their focus areas, these models generally
prioritize similar facial regions across architectures when considering their
overall activation patterns, which show significantly higher similarity between
images of the same individual (Bhattacharyya Coefficient: 0.32-0.57) vs.
different individuals (0.04-0.13), validating the existence of person-specific
recognition patterns. Our results show that facial recognition models
prioritize the central region of face images (with nose areas accounting for
18.9-29.7% of critical recognition regions), while still distributing attention
across multiple facial fragments. Proper selection of relevant facial areas was
confirmed using validation occlusions, based on just 1% of the most relevant,
LEAM-identified, image pixels, which proved to be transferable across different
models. Our findings establish the foundation for future individually tailored
privacy protection systems centered around LEAM's choice of areas to be
perturbed.

</details>


### [176] [Efficient 3D Scene Reconstruction and Simulation from Sparse Endoscopic Views](https://arxiv.org/abs/2509.17027)
*Zhenya Yang*

Main category: cs.CV

TL;DR: 该论文提出了一种基于高斯溅射（Gaussian Splatting）的框架，可以直接从内窥镜数据重建交互式手术场景，并通过引入虚拟相机正则化和稀疏控制节点物质点法（MPM）解决了视角受限导致的过拟合问题，实现了高效、真实且实时的物理形变模拟。


<details>
  <summary>Details</summary>
Motivation: 传统的构建手术模拟环境的方法繁琐、耗时且难以扩展，导致细节不足和模拟不真实，这限制了医疗培训和患者安全性的提升。

Method: 该研究采用基于高斯溅射的框架从内窥镜数据重建手术场景。为解决内窥镜视角受限导致的过拟合和几何精度下降问题，引入了一种新颖的虚拟相机正则化方法，自适应地在场景周围采样虚拟视点并纳入优化过程。同时，对真实和虚拟视图应用了有效的基于深度的正则化来优化场景几何。为实现快速形变模拟，提出了稀疏控制节点物质点法（MPM），将物理属性集成到重建场景中并显著降低计算成本。

Result: 实验结果表明，该方法能够从稀疏的内窥镜视图高效地重建和模拟手术场景。重建手术场景仅需几分钟，并且能够通过用户交互实时生成符合物理规律的形变。

Conclusion: 该研究成功开发了一个高效、真实且交互性强的手术模拟框架，能够从有限的内窥镜数据中重建和模拟手术场景，有效解决了数据驱动模拟范式中的关键挑战，并为医疗培训提供了改进的工具。

Abstract: Surgical simulation is essential for medical training, enabling practitioners
to develop crucial skills in a risk-free environment while improving patient
safety and surgical outcomes. However, conventional methods for building
simulation environments are cumbersome, time-consuming, and difficult to scale,
often resulting in poor details and unrealistic simulations. In this paper, we
propose a Gaussian Splatting-based framework to directly reconstruct
interactive surgical scenes from endoscopic data while ensuring efficiency,
rendering quality, and realism. A key challenge in this data-driven simulation
paradigm is the restricted movement of endoscopic cameras, which limits
viewpoint diversity. As a result, the Gaussian Splatting representation
overfits specific perspectives, leading to reduced geometric accuracy. To
address this issue, we introduce a novel virtual camera-based regularization
method that adaptively samples virtual viewpoints around the scene and
incorporates them into the optimization process to mitigate overfitting. An
effective depth-based regularization is applied to both real and virtual views
to further refine the scene geometry. To enable fast deformation simulation, we
propose a sparse control node-based Material Point Method, which integrates
physical properties into the reconstructed scene while significantly reducing
computational costs. Experimental results on representative surgical data
demonstrate that our method can efficiently reconstruct and simulate surgical
scenes from sparse endoscopic views. Notably, our method takes only a few
minutes to reconstruct the surgical scene and is able to produce physically
plausible deformations in real-time with user-defined interactions.

</details>


### [177] [ChartHal: A Fine-grained Framework Evaluating Hallucination of Large Vision Language Models in Chart Understanding](https://arxiv.org/abs/2509.17481)
*Xingqi Wang,Yiming Cui,Xin Yao,Shijin Wang,Guoping Hu,Xiaoyu Qin*

Main category: cs.CV

TL;DR: 大型视觉-语言模型（LVLMs）在图表理解中存在严重的幻觉问题，本研究提出了ChartHal基准来量化和分析这一现象，发现现有SOTA模型表现不佳。


<details>
  <summary>Details</summary>
Motivation: LVLMs在图表理解方面取得了显著进展，但幻觉仍是关键障碍，尤其需要高水平的感知、认知能力和事实准确性。虽然已有研究分别探讨了幻觉和图表理解，但两者交叉领域仍未充分探索。

Method: 提出了ChartHal基准，该基准具有图表理解中幻觉场景的细粒度分类，并包含一个经过人工验证的1,062个样本的数据集。

Result: 评估显示，最先进的LVLMs在ChartHal上遭受严重的幻觉，包括GPT-5和o4-mini等专有模型，准确率分别仅为34.46%和22.79%。分析表明，涉及图表中缺失或矛盾信息的问答尤其容易引发幻觉。

Conclusion: LVLMs在图表理解中的幻觉问题非常严重，迫切需要更强大的缓解策略来提高其鲁棒性。

Abstract: Large Vision-Language Models (LVLMs) have recently demonstrated remarkable
progress, yet hallucination remains a critical barrier, particularly in chart
understanding, which requires sophisticated perceptual and cognitive abilities
as well as rigorous factual accuracy. While prior work has investigated
hallucinations and chart comprehension independently, their intersection
remains largely unexplored. To address this gap, we present ChartHal, a
benchmark that features a fine-grained taxonomy of hallucination scenarios in
chart understanding, along with a human-validated dataset of 1,062 samples. Our
evaluation shows that state-of-the-art LVLMs suffer from severe hallucinations
on ChartHal, including proprietary models such as GPT-5 and o4-mini, which
achieve only 34.46% and 22.79% accuracy, respectively. Further analysis reveals
that questions involving information absent from or contradictory to charts are
especially likely to trigger hallucinations, underscoring the urgent need for
more robust mitigation strategies. Code and data are available at
https://github.com/ymcui/ChartHal .

</details>


### [178] [Towards Generalized Synapse Detection Across Invertebrate Species](https://arxiv.org/abs/2509.17041)
*Samia Mohinta,Daniel Franco-Barranco,Shi Yan Lee,Albert Cardona*

Main category: cs.CV

TL;DR: 本文提出SimpSyn，一个轻量级单阶段U-Net模型，用于在大规模电子显微镜（EM）图像中高效、准确地检测突触。SimpSyn在F1分数上优于现有最先进模型，并证明了简单模型在连接组学中的实用性。


<details>
  <summary>Details</summary>
Motivation: 生物体行为差异与神经回路结构密切相关，但导致这些差异的突触微观变化仍知之甚少。尽管体积电子显微镜（EM）提供了捕捉突触结构所需的分辨率，但由于注释稀疏、形态变异和跨数据集域转移，自动化突触检测仍然困难。

Method: 1. 构建了一个包含两种无脊椎动物（果蝇和微型黄蜂）四种数据集的EM基准。2. 提出了SimpSyn，一个单阶段残差U-Net模型，用于预测突触前和突触后位点的双通道球形掩膜，旨在优先考虑训练和推理速度以及注释效率。3. 将SimpSyn与现有最先进的多任务模型Synful进行了基准测试。4. 采用简单的后处理策略（如局部峰值检测和基于距离的过滤）。

Result: 1. SimpSyn在所有数据集的突触位点检测F1分数上始终优于Synful。2. 尽管跨数据集泛化能力有限，但在组合数据集上训练时，SimpSyn表现出具有竞争力的性能。3. 消融实验表明，简单的后处理策略无需复杂的测试时启发式方法即可获得良好性能。

Conclusion: 研究结果表明，轻量级模型，当与任务结构对齐时，为大规模连接组学管道中的突触检测提供了一种实用且可扩展的解决方案。

Abstract: Behavioural differences across organisms, whether healthy or pathological,
are closely tied to the structure of their neural circuits. Yet, the fine-scale
synaptic changes that give rise to these variations remain poorly understood,
in part due to persistent challenges in detecting synapses reliably and at
scale. Volume electron microscopy (EM) offers the resolution required to
capture synaptic architecture, but automated detection remains difficult due to
sparse annotations, morphological variability, and cross-dataset domain shifts.
To address this, we make three key contributions. First, we curate a diverse EM
benchmark spanning four datasets across two invertebrate species: adult and
larval Drosophila melanogaster, and Megaphragma viggianii (micro-WASP). Second,
we propose SimpSyn, a single-stage Residual U-Net trained to predict
dual-channel spherical masks around pre- and post-synaptic sites, designed to
prioritize training and inference speeds and annotation efficiency over
architectural complexity. Third, we benchmark SimpSyn against Buhmann et al.'s
Synful [1], a state-of-the-art multi-task model that jointly infers synaptic
pairs. Despite its simplicity, SimpSyn consistently outperforms Synful in
F1-score across all volumes for synaptic site detection. While generalization
across datasets remains limited, SimpSyn achieves competitive performance when
trained on the combined cohort. Finally, ablations reveal that simple
post-processing strategies - such as local peak detection and distance-based
filtering - yield strong performance without complex test-time heuristics.
Taken together, our results suggest that lightweight models, when aligned with
task structure, offer a practical and scalable solution for synapse detection
in large-scale connectomic pipelines.

</details>


### [179] [AgriDoctor: A Multimodal Intelligent Assistant for Agriculture](https://arxiv.org/abs/2509.17044)
*Mingqing Zhang,Zhuoning Xu,Peijie Wang,Rongji Li,Liang Wang,Qiang Liu,Jian Xu,Xuyao Zhang,Shu Wu,Liang Wang*

Main category: cs.CV

TL;DR: AgriDoctor是一个用于作物病害诊断和农业知识交互的模块化多模态框架，它利用代理式多模态推理，并基于新构建的AgriMM数据集训练，显著优于现有最先进的大型视觉语言模型。


<details>
  <summary>Details</summary>
Motivation: 现有的作物病害诊断方法主要依赖于单模态模型，无法整合领域特定的农业知识，且缺乏基于语言的交互式理解。尽管大型语言模型（LLMs）和大型视觉语言模型（LVLMs）在多模态推理方面有所进展，但由于缺乏专业数据集和领域适应性不足，它们在农业领域的表现仍受限制。

Method: 本文提出了AgriDoctor，一个模块化、可扩展的多模态框架，用于智能作物病害诊断和农业知识交互。它首次将代理式多模态推理引入农业领域，集成了路由器、分类器、检测器、知识检索器和LLMs五个核心组件。为支持训练和评估，作者构建了AgriMM基准数据集，包含40万张带注释的病害图像、831条专家整理的知识条目和30万条用于意图驱动工具选择的双语提示。

Result: 在AgriMM上训练的AgriDoctor，在细粒度农业任务上显著优于现有最先进的LVLMs，为智能和可持续农业应用建立了新的范式。

Conclusion: AgriDoctor通过引入代理式多模态推理和领域自适应解决方案，为交互式和领域自适应的作物健康解决方案提供了一种新范式，从而推动了智能和可持续农业应用的发展。

Abstract: Accurate crop disease diagnosis is essential for sustainable agriculture and
global food security. Existing methods, which primarily rely on unimodal models
such as image-based classifiers and object detectors, are limited in their
ability to incorporate domain-specific agricultural knowledge and lack support
for interactive, language-based understanding. Recent advances in large
language models (LLMs) and large vision-language models (LVLMs) have opened new
avenues for multimodal reasoning. However, their performance in agricultural
contexts remains limited due to the absence of specialized datasets and
insufficient domain adaptation. In this work, we propose AgriDoctor, a modular
and extensible multimodal framework designed for intelligent crop disease
diagnosis and agricultural knowledge interaction. As a pioneering effort to
introduce agent-based multimodal reasoning into the agricultural domain,
AgriDoctor offers a novel paradigm for building interactive and domain-adaptive
crop health solutions. It integrates five core components: a router,
classifier, detector, knowledge retriever and LLMs. To facilitate effective
training and evaluation, we construct AgriMM, a comprehensive benchmark
comprising 400000 annotated disease images, 831 expert-curated knowledge
entries, and 300000 bilingual prompts for intent-driven tool selection.
Extensive experiments demonstrate that AgriDoctor, trained on AgriMM,
significantly outperforms state-of-the-art LVLMs on fine-grained agricultural
tasks, establishing a new paradigm for intelligent and sustainable farming
applications.

</details>


### [180] [Multimodal Medical Image Classification via Synergistic Learning Pre-training](https://arxiv.org/abs/2509.17492)
*Qinghua Lin,Guang-Hai Liu,Zuoyong Li,Yang Li,Yuting Jiang,Xiang Wu*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的“预训练+微调”框架，用于解决多模态病理图像在标签稀缺情况下的融合与半监督分类挑战，并在胃镜图像数据集上取得了超越现有先进方法的性能。


<details>
  <summary>Details</summary>
Motivation: 多模态病理图像在临床诊断中广泛应用，但基于计算机视觉的多模态图像辅助诊断面临模态融合的挑战，尤其是在缺乏专家标注数据时。这促使研究人员寻求在标签稀缺条件下实现多模态图像融合的方法。

Method: 该方法包括一个“预训练+微调”框架。预训练阶段采用协同学习框架，结合一致性、重建和对齐学习，将一种模态视为另一种模态的增强样本，实现自监督学习以增强特征表示能力。微调阶段设计了多模态融合方法，使用不同的编码器提取原始模态特征，并提供一个多模态融合编码器进行融合。此外，提出了一种多模态融合特征的分布偏移方法，以缓解标签样本不足导致的预测不确定性和过拟合风险。

Result: 在公开的胃镜图像数据集Kvasir和Kvasirv2上进行了广泛实验。定量和定性结果表明，所提出的方法优于当前最先进的分类方法。

Conclusion: 本文提出了一种有效的“预训练+微调”框架，通过协同学习预训练和创新的微调策略，成功解决了标签稀缺条件下多模态医学图像的融合与半监督分类问题，显著提升了分类性能。

Abstract: Multimodal pathological images are usually in clinical diagnosis, but
computer vision-based multimodal image-assisted diagnosis faces challenges with
modality fusion, especially in the absence of expert-annotated data. To achieve
the modality fusion in multimodal images with label scarcity, we propose a
novel ``pretraining + fine-tuning" framework for multimodal semi-supervised
medical image classification. Specifically, we propose a synergistic learning
pretraining framework of consistency, reconstructive, and aligned learning. By
treating one modality as an augmented sample of another modality, we implement
a self-supervised learning pre-train, enhancing the baseline model's feature
representation capability. Then, we design a fine-tuning method for multimodal
fusion. During the fine-tuning stage, we set different encoders to extract
features from the original modalities and provide a multimodal fusion encoder
for fusion modality. In addition, we propose a distribution shift method for
multimodal fusion features, which alleviates the prediction uncertainty and
overfitting risks caused by the lack of labeled samples. We conduct extensive
experiments on the publicly available gastroscopy image datasets Kvasir and
Kvasirv2. Quantitative and qualitative results demonstrate that the proposed
method outperforms the current state-of-the-art classification methods. The
code will be released at: https://github.com/LQH89757/MICS.

</details>


### [181] [Learning Attribute-Aware Hash Codes for Fine-Grained Image Retrieval via Query Optimization](https://arxiv.org/abs/2509.17049)
*Peng Wang,Yong Li,Lin Zhao,Xiu-Shen Wei*

Main category: cs.CV

TL;DR: 该论文提出了一种细粒度哈希方法，利用可学习查询生成属性感知的哈希码，并通过辅助分支处理低位哈希码的复杂优化问题，提高了检索精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在需要区分视觉上相似类别的场景中，细粒度哈希是快速高效图像检索的有力解决方案。为了使每个哈希位对应特定的视觉属性，并解决低位哈希码优化中遇到的复杂优化问题。

Method: 该方法部署了一组定制查询来捕获和表示哈希过程中细微的属性级信息，从而增强每个哈希位的可解释性和相关性。在此查询优化框架的基础上，引入一个辅助分支来建模高阶属性交互，以缓解低位哈希码常遇到的复杂优化问题，增强哈希码的鲁棒性和特异性。

Result: 在基准数据集上的实验结果表明，该方法生成的哈希码是属性感知的，并且在检索精度和鲁棒性方面持续优于现有技术，尤其对于低位哈希码表现突出。

Conclusion: 该方法在细粒度图像哈希任务中具有巨大潜力，能够生成属性感知、高精度和高鲁棒性的哈希码，尤其适用于低位哈希码场景。

Abstract: Fine-grained hashing has become a powerful solution for rapid and efficient
image retrieval, particularly in scenarios requiring high discrimination
between visually similar categories. To enable each hash bit to correspond to
specific visual attributes, we propoe a novel method that harnesses learnable
queries for attribute-aware hash codes learning. This method deploys a tailored
set of queries to capture and represent nuanced attribute-level information
within the hashing process, thereby enhancing both the interpretability and
relevance of each hash bit. Building on this query-based optimization
framework, we incorporate an auxiliary branch to help alleviate the challenges
of complex landscape optimization often encountered with low-bit hash codes.
This auxiliary branch models high-order attribute interactions, reinforcing the
robustness and specificity of the generated hash codes. Experimental results on
benchmark datasets demonstrate that our method generates attribute-aware hash
codes and consistently outperforms state-of-the-art techniques in retrieval
accuracy and robustness, especially for low-bit hash codes, underscoring its
potential in fine-grained image hashing tasks.

</details>


### [182] [Geodesic Prototype Matching via Diffusion Maps for Interpretable Fine-Grained Recognition](https://arxiv.org/abs/2509.17050)
*Junhao Jia,Yunyou Liu,Yifei Sun,Huangwei Chen,Feiwei Qin,Changmiao Wang,Yong Peng*

Main category: cs.CV

TL;DR: 本文提出GeoProto框架，通过将深度特征的非线性流形结构蒸馏到扩散空间，并利用可微分的Nyström插值，实现基于原型的细粒度识别，显著优于传统的欧氏距离方法。


<details>
  <summary>Details</summary>
Motivation: 深度视觉特征普遍存在于非线性流形中，导致欧氏距离无法准确衡量真实相似性。在基于原型的可解释细粒度识别中，细微的语义区分至关重要，而欧氏距离的局限性在此尤为突出。

Method: GeoProto框架将每个类别的潜在流形结构蒸馏到扩散空间中，并引入可微分的Nyström插值，使几何信息可用于未见样本和可学习原型。为确保效率，采用紧凑的每类地标集并进行周期性更新，以保持嵌入与骨干网络的对齐，实现快速可扩展的推理。

Result: 在CUB-200-2011和Stanford Cars数据集上的大量实验表明，GeoProto框架生成的原型能聚焦于语义对齐的部件，并且显著优于使用欧氏距离的原型网络。

Conclusion: GeoProto通过锚定深度特征的内在几何结构，有效解决了基于欧氏距离在非线性流形中捕捉相似性的不足，为原型驱动的细粒度识别提供了一种新颖且高效的范式，并取得了优异的性能。

Abstract: Nonlinear manifolds are widespread in deep visual features, where Euclidean
distances often fail to capture true similarity. This limitation becomes
particularly severe in prototype-based interpretable fine-grained recognition,
where subtle semantic distinctions are essential. To address this challenge, we
propose a novel paradigm for prototype-based recognition that anchors
similarity within the intrinsic geometry of deep features. Specifically, we
distill the latent manifold structure of each class into a diffusion space and
introduce a differentiable Nystr\"om interpolation, making the geometry
accessible to both unseen samples and learnable prototypes. To ensure
efficiency, we employ compact per-class landmark sets with periodic updates.
This design keeps the embedding aligned with the evolving backbone, enabling
fast and scalable inference. Extensive experiments on the CUB-200-2011 and
Stanford Cars datasets show that our GeoProto framework produces prototypes
focusing on semantically aligned parts, significantly outperforming Euclidean
prototype networks.

</details>


### [183] [CardiacCLIP: Video-based CLIP Adaptation for LVEF Prediction in a Few-shot Manner](https://arxiv.org/abs/2509.17065)
*Yao Du,Jiarong Guo,Xiaomeng Li*

Main category: cs.CV

TL;DR: CardiacCLIP提出了一种基于视频的框架，通过注意力机制的帧聚合和多分辨率输入缩放，改进了超声心动图在小样本设置下的左心室射血分数（LVEF）预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的LVEF估计方法依赖于大规模标注视频数据集，成本高昂且适应性受限。最近的超声心动图视觉-语言模型（如EchoCLIP）未能捕捉到关键的时间动态和局部心脏结构，这些对于准确诊断至关重要。

Method: 本文提出了CardiacCLIP，一个基于视频的框架，通过引入MFL（Multi Frame Learning）——一种新颖的基于注意力的机制，用于选择性融合信息帧，以及EchoZoom——一种多尺度特征提取策略，用于细化心脏结构的空间表示。该方法是CLIP模型在小样本超声心动图视频分析中的新颖应用。

Result: 在1-shot设置下，该方法显著提高了诊断准确性，在EchoNet-Dynamic数据集上将平均绝对误差（MAE）降低了2.07。

Conclusion: CardiacCLIP通过有效处理时间动态和空间细节，显著增强了小样本超声心动图视频分析中的LVEF预测能力，为心脏功能评估提供了更准确、适应性更强的方法。

Abstract: Echocardiography is a vital non-invasive modality for cardiac assessment,
with left ventricular ejection fraction (LVEF) serving as a key indicator of
heart function. Existing LVEF estimation methods depend on large-scale
annotated video datasets, which are costly and limit adaptability across
various clinical settings. Recent vision-language models for echocardiography,
such as EchoCLIP, apply image-to-text pretraining but fail to capture crucial
temporal dynamics and localized cardiac structures essential for accurate
diagnosis. To address these challenges, we propose CardiacCLIP, a video-based
framework that enhances LVEF prediction through attention-based frame
aggregation and multi-resolution input scaling. Specifically, we introduce MFL
(Multi Frame Learning), a novel attention-based mechanism for selectively
fusing informative frames, and EchoZoom, a multi-scale feature extraction
strategy that refines spatial representations of cardiac structures. As a novel
adaptation of CLIP models for few-shot echocardiogram video analysis, our
approach significantly improves diagnostic accuracy, reducing MAE by 2.07 on
the EchoNet-Dynamic dataset under 1-shot setting. The code is available at
https://github.com/xmed-lab/CardiacCLIP.

</details>


### [184] [An Empirical Study on the Robustness of YOLO Models for Underwater Object Detection](https://arxiv.org/abs/2509.17561)
*Edwine Nabahirwa,Wei Song,Minghua Zhang,Shufan Chen*

Main category: cs.CV

TL;DR: 本研究全面评估了YOLO模型在六种模拟水下环境中的鲁棒性，分析了失真对低级特征的影响，并探索了提高水下目标检测性能的训练策略，为构建稳健的水下目标检测系统提供了实用指导。


<details>
  <summary>Details</summary>
Motivation: 水下失真严重影响低级特征，降低了水下目标检测（UOD）的性能，即使是最先进的检测器也面临挑战。尽管YOLO模型是实时目标检测的主干，但其在水下复杂且不可预测条件下的鲁棒性尚未得到系统性检验，因此亟需评估YOLO模型在这种独特挑战下的表现。

Method: 研究对YOLOv8至YOLOv12等最新YOLO变体在六种模拟水下环境中进行了全面评估。使用了来自DUO和Roboflow100的10,000张带标注图像的统一数据集。分析了失真（特别是噪声）如何影响纹理、边缘和颜色等关键低级特征。此外，还评估了轻量级训练感知策略，包括噪声感知样本注入和使用高级增强进行微调。

Result: (1) YOLOv12整体性能最强，但对噪声高度敏感。(2) 噪声会破坏边缘和纹理特征，解释了其在噪声图像中较差的检测性能。(3) 图像数量和实例频率是驱动检测性能的主要因素，而目标外观影响较小。(4) 噪声感知样本注入能提高在噪声和真实世界条件下的鲁棒性。(5) 使用高级增强进行微调可提高增强域的准确性，但在原始数据中性能略有下降，显示出强大的域适应潜力。

Conclusion: 这些研究结果为构建有韧性且经济高效的水下目标检测系统提供了实用的指导，特别是在理解YOLO模型在水下环境中的局限性以及通过特定策略提升其性能方面。

Abstract: Underwater object detection (UOD) remains a critical challenge in computer
vision due to underwater distortions which degrade low-level features and
compromise the reliability of even state-of-the-art detectors. While YOLO
models have become the backbone of real-time object detection, little work has
systematically examined their robustness under these uniquely challenging
conditions. This raises a critical question: Are YOLO models genuinely robust
when operating under the chaotic and unpredictable conditions of underwater
environments? In this study, we present one of the first comprehensive
evaluations of recent YOLO variants (YOLOv8-YOLOv12) across six simulated
underwater environments. Using a unified dataset of 10,000 annotated images
from DUO and Roboflow100, we not only benchmark model robustness but also
analyze how distortions affect key low-level features such as texture, edges,
and color. Our findings show that (1) YOLOv12 delivers the strongest overall
performance but is highly vulnerable to noise, and (2) noise disrupts edge and
texture features, explaining the poor detection performance in noisy images.
Class imbalance is a persistent challenge in UOD. Experiments revealed that (3)
image counts and instance frequency primarily drive detection performance,
while object appearance exerts only a secondary influence. Finally, we
evaluated lightweight training-aware strategies: noise-aware sample injection,
which improves robustness in both noisy and real-world conditions, and
fine-tuning with advanced enhancement, which boosts accuracy in enhanced
domains but slightly lowers performance in original data, demonstrating strong
potential for domain adaptation, respectively. Together, these insights provide
practical guidance for building resilient and cost-efficient UOD systems.

</details>


### [185] [Enhanced Detection of Tiny Objects in Aerial Images](https://arxiv.org/abs/2509.17078)
*Kihyun Kim,Michalis Lazarou,Tania Stathaki*

Main category: cs.CV

TL;DR: 该研究针对YOLOv8在航空影像中检测微小物体性能不佳的问题，提出了三种增强策略（输入分辨率调整、数据增强、注意力机制），并设计了MoonNet骨干网络，显著提升了微小物体检测的准确性，并在基准测试中达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: YOLOv8等单阶段检测器在检测小物体时性能较差，尤其是在低分辨率目标和杂乱背景的航空影像中检测微小物体时，这一问题更为突出。

Method: 引入了三种增强策略：输入图像分辨率调整、数据增强和注意力机制。此外，设计了一个名为MoonNet（Mixture of Orthogonal Neural-modules Network）的管道，将Squeeze-and-Excitation Block (SE Block)和Convolutional Block Attention Module (CBAM)两个注意力模块集成到YOLOv8的骨干网络中，并增加了通道数量。

Result: 图像尺寸放大和适当的数据增强可以带来性能提升。MoonNet骨干网络与原始YOLOv8相比，获得了更高的检测精度。当MoonNet与YOLC模型结合时，在微小物体基准测试中取得了最先进的性能，证明了其适应性和潜力。

Conclusion: 通过结合输入分辨率调整、数据增强和注意力机制，特别是设计的MoonNet骨干网络，可以有效提升YOLOv8在航空影像中微小物体检测的准确性，并有望在相关领域达到领先水平。

Abstract: While one-stage detectors like YOLOv8 offer fast training speed, they often
under-perform on detecting small objects as a trade-off. This becomes even more
critical when detecting tiny objects in aerial imagery due to low-resolution
targets and cluttered backgrounds. To address this, we introduce three
enhancement strategies -- input image resolution adjustment, data augmentation,
and attention mechanisms -- that can be easily implemented on YOLOv8. We
demonstrate that image size enlargement and the proper use of augmentation can
lead to enhancement. Additionally, we designed a Mixture of Orthogonal
Neural-modules Network (MoonNet) pipeline which consists of attention-augmented
CNNs. Two well-known attention modules, the Squeeze-and-Excitation Block (SE
Block) and the Convolutional Block Attention Module (CBAM), were integrated
into the backbone of YOLOv8 with an increased number of channels, and the
MoonNet backbone obtained improved detection accuracy compared to the original
YOLOv8. MoonNet further proved its adaptability and potential by achieving
state-of-the-art performance on a tiny-object benchmark when integrated with
the YOLC model. Our codes are available at: https://github.com/Kihyun11/MoonNet

</details>


### [186] [MRN: Harnessing 2D Vision Foundation Models for Diagnosing Parkinson's Disease with Limited 3D MR Data](https://arxiv.org/abs/2509.17566)
*Ding Shaodong,Liu Ziyang,Zhou Yijun,Liu Tao*

Main category: cs.CV

TL;DR: 该研究利用2D视觉基础模型（VFMs）自动诊断帕金森病，通过处理3D MRI图像中的多个关键ROI并结合对比学习，解决了数据稀缺和模型适应性问题，并在MICCAI 2025挑战赛中取得第一。


<details>
  <summary>Details</summary>
Motivation: 帕金森病的自动诊断临床需求高，但当前诊断模型训练面临挑战：缺乏高质量、大规模数据集导致过拟合；预训练3D医学模型在不同模态和体素间距的医学图像上难以适应。

Method: 该方法利用2D视觉基础模型（VFMs）：1. 从NM和QSM图像中裁剪多个关键感兴趣区域（ROIs）。2. 每个ROI通过独立分支处理，将其压缩为令牌。3. 将这些ROI令牌组合成统一的患者表征进行分类。4. 在每个分支中，使用2D VFM编码3D ROI体积的轴向切片并融合为ROI令牌，并通过辅助分割头引导特征提取到特定脑核。5. 引入多ROI监督对比学习，通过拉近同类患者表征、推开异类患者表征来提高诊断性能。

Result: 该方法在MICCAI 2025 PDCADxFoundation挑战赛中获得第一名，在仅包含300个标记QSM和NM-MRI扫描的数据集上实现了86.0%的准确率，比第二名高出5.5%。

Conclusion: 研究结果强调了2D视觉基础模型在3D MR图像临床分析中的巨大潜力，尤其是在数据有限的情况下。

Abstract: The automatic diagnosis of Parkinson's disease is in high clinical demand due
to its prevalence and the importance of targeted treatment. Current clinical
practice often relies on diagnostic biomarkers in QSM and NM-MRI images.
However, the lack of large, high-quality datasets makes training diagnostic
models from scratch prone to overfitting. Adapting pre-trained 3D medical
models is also challenging, as the diversity of medical imaging leads to
mismatches in voxel spacing and modality between pre-training and fine-tuning
data. In this paper, we address these challenges by leveraging 2D vision
foundation models (VFMs). Specifically, we crop multiple key ROIs from NM and
QSM images, process each ROI through separate branches to compress the ROI into
a token, and then combine these tokens into a unified patient representation
for classification. Within each branch, we use 2D VFMs to encode axial slices
of the 3D ROI volume and fuse them into the ROI token, guided by an auxiliary
segmentation head that steers the feature extraction toward specific brain
nuclei. Additionally, we introduce multi-ROI supervised contrastive learning,
which improves diagnostic performance by pulling together representations of
patients from the same class while pushing away those from different classes.
Our approach achieved first place in the MICCAI 2025 PDCADxFoundation
challenge, with an accuracy of 86.0% trained on a dataset of only 300 labeled
QSM and NM-MRI scans, outperforming the second-place method by 5.5%.These
results highlight the potential of 2D VFMs for clinical analysis of 3D MR
images.

</details>


### [187] [A Dual-Modulation Framework for RGB-T Crowd Counting via Spatially Modulated Attention and Adaptive Fusion](https://arxiv.org/abs/2509.17079)
*Yuhong Feng,Hongtao Chen,Qi Zhang,Jie Chen,Zhaoxi He,Mingzhe Liu,Jianghai Liao*

Main category: cs.CV

TL;DR: 针对RGB-T人群计数中Transformer模型空间归纳偏置不足和模态融合困难问题，本文提出双调制框架，包含空间调制注意力和自适应融合调制，显著提升了计数性能。


<details>
  <summary>Details</summary>
Motivation: 在RGB-T人群计数中，现有基于Transformer的方法虽然擅长捕获全局上下文，但其固有的空间归纳偏置不足导致注意力分散到无关背景区域，损害了人群定位精度。此外，有效弥合不同模态间的鸿沟仍是主要挑战。

Method: 本文提出双调制框架 (Dual Modulation Framework)，包含两个模块：
1. 空间调制注意力 (Spatially Modulated Attention, SMA)：通过使用可学习的空间衰减掩码 (Spatial Decay Mask) 来惩罚远距离tokens间的注意力，防止注意力扩散到背景，从而提高人群定位精度。
2. 自适应融合调制 (Adaptive Fusion Modulation, AFM)：实现动态门控机制，优先选择最可靠的模态进行自适应跨模态融合。

Result: 在RGB-T人群计数数据集上进行的大量实验表明，本文提出的方法与现有工作相比具有优越的性能。

Conclusion: 本文提出的双调制框架（包括SMA和AFM）有效解决了Transformer模型在RGB-T人群计数中空间归纳偏置不足导致注意力分散以及模态间有效融合的挑战，显著提升了模型的性能。

Abstract: Accurate RGB-Thermal (RGB-T) crowd counting is crucial for public safety in
challenging conditions. While recent Transformer-based methods excel at
capturing global context, their inherent lack of spatial inductive bias causes
attention to spread to irrelevant background regions, compromising crowd
localization precision. Furthermore, effectively bridging the gap between these
distinct modalities remains a major hurdle. To tackle this, we propose the Dual
Modulation Framework, comprising two modules: Spatially Modulated Attention
(SMA), which improves crowd localization by using a learnable Spatial Decay
Mask to penalize attention between distant tokens and prevent focus from
spreading to the background; and Adaptive Fusion Modulation (AFM), which
implements a dynamic gating mechanism to prioritize the most reliable modality
for adaptive cross-modal fusion. Extensive experiments on RGB-T crowd counting
datasets demonstrate the superior performance of our method compared to
previous works. Code available at
https://github.com/Cht2924/RGBT-Crowd-Counting.

</details>


### [188] [Interpreting Attention Heads for Image-to-Text Information Flow in Large Vision-Language Models](https://arxiv.org/abs/2509.17588)
*Jinyeong Kim,Seil Kang,Jiwoo Park,Junhyeok Kim,Seong Jae Hwang*

Main category: cs.CV

TL;DR: 本文提出了一种名为“注意力头归因”的技术，用于解释大型视觉语言模型（LVLMs）中图像到文本的信息流。研究发现，信息流遵循结构化过程，由特定注意力头子集促成，其选择受语义内容而非视觉外观控制，并在词元层面展示了特定的传播模式。


<details>
  <summary>Details</summary>
Motivation: 由于大量注意力头的同时运作，LVLMs中图像到文本的信息流机制难以解释，而这种信息流对视觉问答至关重要。

Method: 1. 提出“注意力头归因”技术，灵感来源于组件归因方法，以识别信息传输中关键的注意力头模式。2. 利用该技术研究LVLMs如何依赖特定注意力头识别图像中主要对象并回答相关问题。3. 在词元级别检查信息流。

Result: 1. 存在一个独特的注意力头子集促进图像到文本的信息流。2. 这些头的选择受输入图像的语义内容而非视觉外观控制。3. 在词元层面，文本信息首先传播到与角色相关的词元和最终词元，然后接收图像信息。4. 图像信息嵌入在与对象相关的词元和背景词元中。

Conclusion: 图像到文本的信息流遵循一个结构化的过程。对注意力头层面的分析为理解LVLMs的内部机制提供了一个有前景的方向。

Abstract: Large Vision-Language Models (LVLMs) answer visual questions by transferring
information from images to text through a series of attention heads. While this
image-to-text information flow is central to visual question answering, its
underlying mechanism remains difficult to interpret due to the simultaneous
operation of numerous attention heads. To address this challenge, we propose
head attribution, a technique inspired by component attribution methods, to
identify consistent patterns among attention heads that play a key role in
information transfer. Using head attribution, we investigate how LVLMs rely on
specific attention heads to identify and answer questions about the main object
in an image. Our analysis reveals that a distinct subset of attention heads
facilitates the image-to-text information flow. Remarkably, we find that the
selection of these heads is governed by the semantic content of the input image
rather than its visual appearance. We further examine the flow of information
at the token level and discover that (1) text information first propagates to
role-related tokens and the final token before receiving image information, and
(2) image information is embedded in both object-related and background tokens.
Our work provides evidence that image-to-text information flow follows a
structured process, and that analysis at the attention-head level offers a
promising direction toward understanding the mechanisms of LVLMs.

</details>


### [189] [HyRF: Hybrid Radiance Fields for Memory-efficient and High-quality Novel View Synthesis](https://arxiv.org/abs/2509.17083)
*Zipeng Wang,Dan Xu*

Main category: cs.CV

TL;DR: HyRF提出了一种混合辐射场，结合显式高斯和神经场，显著减少了3DGS的模型大小（超过20倍），同时保持了最先进的渲染质量和实时性能。


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting (3DGS) 在实时高质量新视角合成方面表现出色，但其依赖于每个高斯参数来建模视点相关效应和各向异性形状，导致显著的内存开销。现有基于神经场的压缩方法难以捕捉高频空间变化，导致细节重建质量下降。

Method: HyRF将场景分解为两部分：(1) 紧凑的显式高斯集，存储关键的高频参数；(2) 基于网格的神经场，预测剩余属性。为增强表示能力，引入了解耦的神经场架构，分别建模几何（尺度、不透明度、旋转）和视点相关颜色。此外，提出了一种混合渲染方案，将高斯溅射与神经场预测的背景相结合，以解决远距离场景表示的局限性。

Result: HyRF实现了最先进的渲染质量，与3DGS相比，模型大小减少了20倍以上，并保持了实时性能。

Conclusion: HyRF通过结合显式高斯和神经场的优势，成功解决了3DGS的内存开销问题，同时在保持高渲染质量和实时性能方面取得了显著进展。

Abstract: Recently, 3D Gaussian Splatting (3DGS) has emerged as a powerful alternative
to NeRF-based approaches, enabling real-time, high-quality novel view synthesis
through explicit, optimizable 3D Gaussians. However, 3DGS suffers from
significant memory overhead due to its reliance on per-Gaussian parameters to
model view-dependent effects and anisotropic shapes. While recent works propose
compressing 3DGS with neural fields, these methods struggle to capture
high-frequency spatial variations in Gaussian properties, leading to degraded
reconstruction of fine details. We present Hybrid Radiance Fields (HyRF), a
novel scene representation that combines the strengths of explicit Gaussians
and neural fields. HyRF decomposes the scene into (1) a compact set of explicit
Gaussians storing only critical high-frequency parameters and (2) grid-based
neural fields that predict remaining properties. To enhance representational
capacity, we introduce a decoupled neural field architecture, separately
modeling geometry (scale, opacity, rotation) and view-dependent color.
Additionally, we propose a hybrid rendering scheme that composites Gaussian
splatting with a neural field-predicted background, addressing limitations in
distant scene representation. Experiments demonstrate that HyRF achieves
state-of-the-art rendering quality while reducing model size by over 20 times
compared to 3DGS and maintaining real-time performance. Our project page is
available at https://wzpscott.github.io/hyrf/.

</details>


### [190] [MoCLIP-Lite: Efficient Video Recognition by Fusing CLIP with Motion Vectors](https://arxiv.org/abs/2509.17084)
*Binhua Huang,Nan Wang,Arjun Parakash,Soumyabrata Dev*

Main category: cs.CV

TL;DR: MoCLIP-Lite是一种高效的视频动作识别框架，它结合了预训练的CLIP图像编码器和轻量级的运动向量网络，实现了卓越的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视频动作识别模型计算成本高昂且依赖大量视频预训练。同时，CLIP等视觉-语言模型在静态图像上表现出色，而运动向量能高效提供时间信息。研究旨在结合这些优势，开发一个高效的视频识别方法。

Method: 本文提出了MoCLIP-Lite，一个简单而强大的双流晚期融合框架。它结合了来自冻结CLIP图像编码器的特征和来自一个在原始运动向量上训练的轻量级监督网络的特征。在融合阶段，两个主干网络都被冻结，只训练一个微小的多层感知器（MLP）头部，以确保极高的效率。

Result: 在UCF101数据集上，MoCLIP-Lite实现了89.2%的Top-1准确率，显著优于强大的零样本基线（65.0%）和仅使用运动向量的基线（66.5%）。

Conclusion: MoCLIP-Lite为视频理解提供了一个新的、高效的基线，有效地弥合了大型静态模型和动态、低成本运动线索之间的差距。

Abstract: Video action recognition is a fundamental task in computer vision, but
state-of-the-art models are often computationally expensive and rely on
extensive video pre-training. In parallel, large-scale vision-language models
like Contrastive Language-Image Pre-training (CLIP) offer powerful zero-shot
capabilities on static images, while motion vectors (MV) provide highly
efficient temporal information directly from compressed video streams. To
synergize the strengths of these paradigms, we propose MoCLIP-Lite, a simple
yet powerful two-stream late fusion framework for efficient video recognition.
Our approach combines features from a frozen CLIP image encoder with features
from a lightweight, supervised network trained on raw MV. During fusion, both
backbones are frozen, and only a tiny Multi-Layer Perceptron (MLP) head is
trained, ensuring extreme efficiency. Through comprehensive experiments on the
UCF101 dataset, our method achieves a remarkable 89.2% Top-1 accuracy,
significantly outperforming strong zero-shot (65.0%) and MV-only (66.5%)
baselines. Our work provides a new, highly efficient baseline for video
understanding that effectively bridges the gap between large static models and
dynamic, low-cost motion cues. Our code and models are available at
https://github.com/microa/MoCLIP-Lite.

</details>


### [191] [A$^2$M$^2$-Net: Adaptively Aligned Multi-Scale Moment for Few-Shot Action Recognition](https://arxiv.org/abs/2509.17638)
*Zilin Gao,Qilong Wang,Bingbing Zhang,Qinghua Hu,Peihua Li*

Main category: cs.CV

TL;DR: 本文提出A$^2$M$^2$-Net，通过自适应对齐和多尺度二阶矩来解决少样本动作识别（FSAR）中视频动态的时间错位问题，并增强表示能力。


<details>
  <summary>Details</summary>
Motivation: 现有FSAR方法忽略个体运动模式，未充分探索视频动态的特征统计，导致难以处理时间错位问题，尤其在使用2D骨干网络时表现不佳。

Method: 本文提出A$^2$M$^2$-Net，包含两个核心组件：1) 自适应对齐（A$^2$模块），用于在考虑个体运动模式的同时自适应选择信息丰富的候选描述符；2) 多尺度二阶矩（M$^2$模块），用于在多个时空尺度上开发语义二阶描述符。该网络通过实例引导的方式自适应对齐强大的表示候选，以处理时间错位。

Result: A$^2$M$^2$-Net在五个广泛使用的FSAR基准测试中取得了与最先进方法极具竞争力的性能，并展现出良好的泛化能力，适用于各种少样本设置和不同评估指标。

Conclusion: A$^2$M$^2$-Net通过建立自适应对齐协议和强大的表示能力，有效解决了少样本动作识别中具有挑战性的时间错位问题，并具有良好的有效性和泛化性。

Abstract: Thanks to capability to alleviate the cost of large-scale annotation,
few-shot action recognition (FSAR) has attracted increased attention of
researchers in recent years. Existing FSAR approaches typically neglect the
role of individual motion pattern in comparison, and under-explore the feature
statistics for video dynamics. Thereby, they struggle to handle the challenging
temporal misalignment in video dynamics, particularly by using 2D backbones. To
overcome these limitations, this work proposes an adaptively aligned
multi-scale second-order moment network, namely A$^2$M$^2$-Net, to describe the
latent video dynamics with a collection of powerful representation candidates
and adaptively align them in an instance-guided manner. To this end, our
A$^2$M$^2$-Net involves two core components, namely, adaptive alignment (A$^2$
module) for matching, and multi-scale second-order moment (M$^2$ block) for
strong representation. Specifically, M$^2$ block develops a collection of
semantic second-order descriptors at multiple spatio-temporal scales.
Furthermore, A$^2$ module aims to adaptively select informative candidate
descriptors while considering the individual motion pattern. By such means, our
A$^2$M$^2$-Net is able to handle the challenging temporal misalignment problem
by establishing an adaptive alignment protocol for strong representation.
Notably, our proposed method generalizes well to various few-shot settings and
diverse metrics. The experiments are conducted on five widely used FSAR
benchmarks, and the results show our A$^2$M$^2$-Net achieves very competitive
performance compared to state-of-the-arts, demonstrating its effectiveness and
generalization.

</details>


### [192] [SFN-YOLO: Towards Free-Range Poultry Detection via Scale-aware Fusion Networks](https://arxiv.org/abs/2509.17086)
*Jie Chen,Yuhong Feng,Tao Dai,Mingzhe Liu,Hongtao Chen,Zhaoxi He,Jiancong Bai*

Main category: cs.CV

TL;DR: 该论文提出了一种名为SFN-YOLO的创新方法，用于在复杂散养环境中进行家禽检测和定位，通过尺度感知融合结合局部和全局特征，并引入了一个新的M-SCOPE数据集，实现了高效且实时的检测，支持智能家禽养殖。


<details>
  <summary>Details</summary>
Motivation: 在散养环境中，由于目标多尺度、遮挡以及复杂或动态背景，家禽检测面临挑战，这阻碍了智能家禽养殖的发展。

Method: 本文提出SFN-YOLO方法，利用尺度感知融合技术，结合详细的局部特征和更广泛的全局上下文信息，以改善复杂环境中的检测。此外，还开发了一个专为各种散养条件设计的新型扩展数据集（M-SCOPE）。

Result: SFN-YOLO模型在仅7.2M参数的情况下实现了80.7%的mAP，比基准模型参数减少了35.1%，同时在不同领域保持了强大的泛化能力，并具备高效和实时的检测能力。

Conclusion: SFN-YOLO的高效和实时检测能力支持自动化智能家禽养殖，为该领域提供了重要的技术进步。

Abstract: Detecting and localizing poultry is essential for advancing smart poultry
farming. Despite the progress of detection-centric methods, challenges persist
in free-range settings due to multiscale targets, obstructions, and complex or
dynamic backgrounds. To tackle these challenges, we introduce an innovative
poultry detection approach named SFN-YOLO that utilizes scale-aware fusion.
This approach combines detailed local features with broader global context to
improve detection in intricate environments. Furthermore, we have developed a
new expansive dataset (M-SCOPE) tailored for varied free-range conditions.
Comprehensive experiments demonstrate our model achieves an mAP of 80.7% with
just 7.2M parameters, which is 35.1% fewer than the benchmark, while retaining
strong generalization capability across different domains. The efficient and
real-time detection capabilities of SFN-YOLO support automated smart poultry
farming. The code and dataset can be accessed at
https://github.com/chenjessiee/SFN-YOLO.

</details>


### [193] [AlignedGen: Aligning Style Across Generated Images](https://arxiv.org/abs/2509.17088)
*Jiexuan Zhang,Yiheng Du,Qian Wang,Weiqi Li,Yu Gu,Jian Zhang*

Main category: cs.CV

TL;DR: AlignedGen是一个无需训练的框架，通过引入Shifted Position Embedding (ShiftPE)和Advanced Attention Sharing (AAS)，解决了Diffusion Transformer (DiT)模型在生成图像时风格一致性差的问题，并支持外部图像作为风格参考。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型具有强大的生成能力，但它们在基于相同风格提示生成图像时难以保持风格一致性，这阻碍了其在创意工作流中的实际应用。现有的无需训练方法受限于U-Net架构，导致生成质量低、存在物体重复等伪影，且与更优越的Diffusion Transformer (DiT)不兼容。

Method: 该研究首先揭示了DiT中朴素注意力共享失败的关键原因：不当的位置嵌入导致冲突的位置信号。为解决此问题，提出了Shifted Position Embedding (ShiftPE)，通过为每张图像分配不重叠的位置索引来解决冲突。在此基础上，开发了Advanced Attention Sharing (AAS)——一套包含三种技术的套件，旨在充分发挥DiT中注意力共享的潜力。此外，为了扩大方法的适用性，提出了一种高效的查询、键和值特征提取算法，使方法能够无缝地将外部图像作为风格参考。

Result: 广泛的实验结果验证了该方法能有效增强生成图像间的风格一致性，同时保持精确的文本到图像对齐。

Conclusion: AlignedGen是一个新颖的、无需训练的框架，通过解决DiT模型中位置信号冲突和优化注意力共享，显著提升了生成图像的风格一致性，并能便捷地利用外部图像作为风格参考，为DiT在创意工作流中的实际部署提供了解决方案。

Abstract: Despite their generative power, diffusion models struggle to maintain style
consistency across images conditioned on the same style prompt, hindering their
practical deployment in creative workflows. While several training-free methods
attempt to solve this, they are constrained to the U-Net architecture, which
not only leads to low-quality results and artifacts like object repetition but
also renders them incompatible with superior Diffusion Transformer (DiT). To
address these issues, we introduce AlignedGen, a novel training-free framework
that enhances style consistency across images generated by DiT models. Our work
first reveals a critical insight: naive attention sharing fails in DiT due to
conflicting positional signals from improper position embeddings. We introduce
Shifted Position Embedding (ShiftPE), an effective solution that resolves this
conflict by allocating a non-overlapping set of positional indices to each
image. Building on this foundation, we develop Advanced Attention Sharing
(AAS), a suite of three techniques meticulously designed to fully unleash the
potential of attention sharing within the DiT. Furthermore, to broaden the
applicability of our method, we present an efficient query, key, and value
feature extraction algorithm, enabling our method to seamlessly incorporate
external images as style references. Extensive experimental results validate
that our method effectively enhances style consistency across generated images
while maintaining precise text-to-image alignment.

</details>


### [194] [SD-VLM: Spatial Measuring and Understanding with Depth-Encoded Vision-Language Models](https://arxiv.org/abs/2509.17664)
*Pingyi Chen,Yujing Lou,Shen Cao,Jinhui Guo,Lubin Fan,Yue Wu,Lin Yang,Lizhuang Ma,Jieping Ye*

Main category: cs.CV

TL;DR: 本论文提出了SD-VLM框架，通过构建大规模空间测量与理解数据集（MSMU）和引入深度位置编码，显著提升了视觉语言模型（VLM）的3D空间定量推理能力。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型（VLM）在2D语义视觉理解方面表现出色，但由于2D图像空间表示能力的不足，其在3D空间关系定量推理方面的能力尚未得到充分探索。

Method: 本研究通过两项关键贡献来增强VLM的空间感知能力：1) 提出了大规模空间测量与理解（MSMU）数据集，包含70万个问答对、250万个物理数值标注和1万个思维链增强样本，以提供精确的空间标注；2) 引入了一种简单的深度位置编码方法，以强化VLM的空间意识。

Result: SD-VLM模型在定量空间测量和理解能力方面表现出卓越的性能。它不仅在提出的MSMU-Bench上取得了最先进的（SOTA）性能，还在Q-Spatial和SpatialRGPT-Bench等其他空间理解基准测试中展示了空间泛化能力。实验表明，SD-VLM在MSMU-Bench上分别超越GPT-4o和Intern-VL3-78B达26.91%和25.56%。

Conclusion: SD-VLM是一个强大的通用VLM，通过大规模空间数据训练和深度位置编码，显著增强了其3D空间定量测量和理解能力，并在多个基准测试中展现出卓越的性能和泛化能力。

Abstract: While vision language models (VLMs) excel in 2D semantic visual
understanding, their ability to quantitatively reason about 3D spatial
relationships remains under-explored, due to the deficiency of 2D images'
spatial representation ability. In this paper, we analyze the problem hindering
VLMs' spatial understanding abilities and propose SD-VLM, a novel framework
that significantly enhances fundamental spatial perception abilities of VLMs
through two key contributions: (1) propose Massive Spatial Measuring and
Understanding (MSMU) dataset with precise spatial annotations, and (2)
introduce a simple depth positional encoding method strengthening VLMs' spatial
awareness. MSMU dataset covers massive quantitative spatial tasks with 700K QA
pairs, 2.5M physical numerical annotations, and 10K chain-of-thought augmented
samples. We have trained SD-VLM, a strong generalist VLM which shows superior
quantitative spatial measuring and understanding capability. SD-VLM not only
achieves state-of-the-art performance on our proposed MSMU-Bench, but also
shows spatial generalization abilities on other spatial understanding
benchmarks including Q-Spatial and SpatialRGPT-Bench. Extensive experiments
demonstrate that SD-VLM outperforms GPT-4o and Intern-VL3-78B by 26.91% and
25.56% respectively on MSMU-Bench. Code and models are released at
https://github.com/cpystan/SD-VLM.

</details>


### [195] [Uncertainty-Supervised Interpretable and Robust Evidential Segmentation](https://arxiv.org/abs/2509.17098)
*Yuzhu Li,An Sui,Fuping Wu,Xiahai Zhuang*

Main category: cs.CV

TL;DR: 本文提出了一种自监督方法，通过引入三个原则和设计两个监督损失，指导医学图像分割中的不确定性估计学习，显著提高了不确定性估计的可解释性和鲁棒性，尤其是在分布外（OOD）场景下。


<details>
  <summary>Details</summary>
Motivation: 以往医学图像分割中的不确定性估计方法通常缺乏有效的监督，导致预测的可解释性和鲁棒性较低。

Method: 提出了一种自监督方法来指导不确定性学习。具体而言，引入了三个关于不确定性与边界周围图像梯度和噪声之间关系的原则，并基于这些原则设计了两个不确定性监督损失。此外，还引入了评估不确定性可解释性和鲁棒性的新量化指标。

Result: 实验结果表明，与现有最先进方法相比，所提出的方法在保持竞争性分割性能的同时，在分布外（OOD）场景中取得了卓越的结果，并显著提高了不确定性估计的可解释性和鲁棒性。

Conclusion: 该自监督方法能有效指导不确定性学习，增强模型预测与人类解释之间的一致性，从而显著提高医学图像分割中不确定性估计的可解释性和鲁棒性，尤其在OOD场景下表现优异。

Abstract: Uncertainty estimation has been widely studied in medical image segmentation
as a tool to provide reliability, particularly in deep learning approaches.
However, previous methods generally lack effective supervision in uncertainty
estimation, leading to low interpretability and robustness of the predictions.
In this work, we propose a self-supervised approach to guide the learning of
uncertainty. Specifically, we introduce three principles about the
relationships between the uncertainty and the image gradients around boundaries
and noise. Based on these principles, two uncertainty supervision losses are
designed. These losses enhance the alignment between model predictions and
human interpretation. Accordingly, we introduce novel quantitative metrics for
evaluating the interpretability and robustness of uncertainty. Experimental
results demonstrate that compared to state-of-the-art approaches, the proposed
method can achieve competitive segmentation performance and superior results in
out-of-distribution (OOD) scenarios while significantly improving the
interpretability and robustness of uncertainty estimation. Code is available
via https://github.com/suiannaius/SURE.

</details>


### [196] [The SAGES Critical View of Safety Challenge: A Global Benchmark for AI-Assisted Surgical Quality Assessment](https://arxiv.org/abs/2509.17100)
*Deepak Alapatt,Jennifer Eckhoff,Zhiliang Lyu,Yutong Ban,Jean-Paul Mazellier,Sarah Choksi,Kunyi Yang,2024 CVS Challenge Consortium,Quanzheng Li,Filippo Filicori,Xiang Li,Pietro Mascagni,Daniel A. Hashimoto,Guy Rosman,Ozanan Meireles,Nicolas Padoy*

Main category: cs.CV

TL;DR: 本研究介绍了SAGES关键安全视野（CVS）挑战赛，这是首个由外科协会组织的AI竞赛，旨在通过腹腔镜胆囊切除术中的CVS评估手术质量。该挑战赛通过全球协作，收集了1000个视频并由专家标注，显著提升了AI在手术质量评估方面的性能、校准和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 本研究的动机是利用人工智能（AI）实现手术专业知识的普及，应用于培训、指导和认证。具体而言，它旨在解决腹腔镜胆囊切除术中普遍推荐但执行不一致的关键安全视野（CVS）步骤，并克服AI在手术实际部署中面临的高性能、主观评估不确定性捕捉以及临床变异性鲁棒性等关键障碍。

Method: 研究方法包括：组织SAGES关键安全视野（CVS）挑战赛，以腹腔镜胆囊切除术中的CVS作为手术质量评估的典范；在全球范围内（54个机构，24个国家）协作，由数百名临床医生和工程师策划；收集并由20名外科专家根据共识验证协议标注1000个视频；开发了EndoGlacier框架，用于管理大型、异构手术视频和多标注者工作流；吸引了13个国际团队参与竞赛。

Result: 挑战赛结果显示，评估性能相对提升高达17%；校准误差减少超过80%；鲁棒性比现有技术相对提高了17%。对结果的分析揭示了与模型性能相关的方法学趋势。

Conclusion: 本研究的结论是，通过SAGES CVS挑战赛，为未来开发稳健、可在临床部署的手术质量评估AI提供了指导，并强调了与模型性能相关的方法学趋势，为进一步研究奠定了基础。

Abstract: Advances in artificial intelligence (AI) for surgical quality assessment
promise to democratize access to expertise, with applications in training,
guidance, and accreditation. This study presents the SAGES Critical View of
Safety (CVS) Challenge, the first AI competition organized by a surgical
society, using the CVS in laparoscopic cholecystectomy, a universally
recommended yet inconsistently performed safety step, as an exemplar of
surgical quality assessment. A global collaboration across 54 institutions in
24 countries engaged hundreds of clinicians and engineers to curate 1,000
videos annotated by 20 surgical experts according to a consensus-validated
protocol. The challenge addressed key barriers to real-world deployment in
surgery, including achieving high performance, capturing uncertainty in
subjective assessment, and ensuring robustness to clinical variability. To
enable this scale of effort, we developed EndoGlacier, a framework for managing
large, heterogeneous surgical video and multi-annotator workflows. Thirteen
international teams participated, achieving up to a 17\% relative gain in
assessment performance, over 80\% reduction in calibration error, and a 17\%
relative improvement in robustness over the state-of-the-art. Analysis of
results highlighted methodological trends linked to model performance,
providing guidance for future research toward robust, clinically deployable AI
for surgical quality assessment.

</details>


### [197] [Predicting Depth Maps from Single RGB Images and Addressing Missing Information in Depth Estimation](https://arxiv.org/abs/2509.17686)
*Mohamad Mofeed Chaar,Jamal Raiyn,Galia Weidl*

Main category: cs.CV

TL;DR: 本研究提出了一种多层训练算法，能从单张RGB图像生成深度图，并有效填补自动驾驶系统中深度图的缺失信息，确保数据完整性和准确性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统（ADS）中深度成像至关重要，但深度图像中常存在信息缺失（像素数据不一致或有间隙导致某些点不可测量），这对目标检测和测量构成了重大挑战。

Method: 研究开发了一种算法，采用多层训练方法从单张RGB图像生成深度图像。该算法还被用于纠正深度图像中的缺失信息，以获得完整准确的数据。

Result: 该算法在Cityscapes数据集上进行了测试，成功解决了其深度图像中的信息缺失问题，证明了该方法在真实城市环境中的有效性。

Conclusion: 本研究提出并验证了一种有效的方法，能够从单张RGB图像生成深度图并修复深度图像中的缺失信息，显著提升了自动驾驶系统中深度数据的完整性和准确性。

Abstract: Depth imaging is a crucial area in Autonomous Driving Systems (ADS), as it
plays a key role in detecting and measuring objects in the vehicle's
surroundings. However, a significant challenge in this domain arises from
missing information in Depth images, where certain points are not measurable
due to gaps or inconsistencies in pixel data. Our research addresses two key
tasks to overcome this challenge. First, we developed an algorithm using a
multi-layered training approach to generate Depth images from a single RGB
image. Second, we addressed the issue of missing information in Depth images by
applying our algorithm to rectify these gaps, resulting in Depth images with
complete and accurate data. We further tested our algorithm on the Cityscapes
dataset and successfully resolved the missing information in its Depth images,
demonstrating the effectiveness of our approach in real-world urban
environments.

</details>


### [198] [Stencil: Subject-Driven Generation with Context Guidance](https://arxiv.org/abs/2509.17120)
*Gordon Chen,Ziqi Huang,Cheston Tan,Ziwei Liu*

Main category: cs.CV

TL;DR: Stencil是一个新颖的框架，通过在推理时联合使用两个扩散模型（一个轻量级微调模型和一个大型冻结预训练模型），解决了文本到图像扩散模型在保持主体一致性、生成质量与效率之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 当前的文本到图像扩散模型在跨代和跨上下文保持主体一致性方面存在困难。现有微调方法面临质量与效率的固有权衡：微调大型模型计算成本高，微调轻量级模型则损害图像保真度。此外，在少量图像上微调预训练模型可能会破坏现有先验知识，导致次优结果。

Method: Stencil框架在推理过程中协同使用两个扩散模型。它高效地在主体图像上微调一个轻量级模型，同时一个大型的、冻结的预训练模型在推理期间提供上下文指导，注入丰富的先验知识以增强生成，且开销极小。

Result: Stencil能够在不到一分钟内生成高保真、新颖的主体渲染图像，实现了最先进的性能，并在主体驱动生成领域树立了新的基准。

Conclusion: Stencil通过其双模型推理机制，成功克服了文本到图像扩散模型在主体一致性、生成质量和效率方面的挑战，为主体驱动生成设立了新的行业标准。

Abstract: Recent text-to-image diffusion models can generate striking visuals from text
prompts, but they often fail to maintain subject consistency across generations
and contexts. One major limitation of current fine-tuning approaches is the
inherent trade-off between quality and efficiency. Fine-tuning large models
improves fidelity but is computationally expensive, while fine-tuning
lightweight models improves efficiency but compromises image fidelity.
Moreover, fine-tuning pre-trained models on a small set of images of the
subject can damage the existing priors, resulting in suboptimal results. To
this end, we present Stencil, a novel framework that jointly employs two
diffusion models during inference. Stencil efficiently fine-tunes a lightweight
model on images of the subject, while a large frozen pre-trained model provides
contextual guidance during inference, injecting rich priors to enhance
generation with minimal overhead. Stencil excels at generating high-fidelity,
novel renditions of the subject in less than a minute, delivering
state-of-the-art performance and setting a new benchmark in subject-driven
generation.

</details>


### [199] [SynergyNet: Fusing Generative Priors and State-Space Models for Facial Beauty Prediction](https://arxiv.org/abs/2509.17172)
*Djamel Eddine Boukhari*

Main category: cs.CV

TL;DR: 本文提出了一种名为MD-Net的新型双流架构，结合了U-Net编码器和Vision Mamba，用于面部美学预测，克服了传统CNN和ViT模型的局限性，并在SCUT-FBP5500基准上达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 面部美学预测需要同时理解局部细节（如皮肤纹理）和全局和谐（如对称性、比例）。现有模型（CNNs和ViTs）存在架构偏见：CNNs擅长局部特征但难以处理长距离依赖，ViTs能建模全局关系但计算成本高昂。因此，需要一种能有效结合局部和全局特征的模型。

Method: 本文引入了Mamba-Diffusion Network (MD-Net)，一个双流架构：
1.  第一个流利用预训练潜在扩散模型中冻结的U-Net编码器，提供强大的生成先验，用于捕捉精细的美学品质。
2.  第二个流采用Vision Mamba (Vim) 模型，以线性时间复杂度高效捕捉全局面部结构。
这两个互补的表示通过交叉注意力机制协同整合，创建了一个全面而细致的特征空间用于预测。

Result: MD-Net在SCUT-FBP5500基准测试中达到了新的最先进水平，Pearson相关系数为0.9235。

Conclusion: MD-Net证明了融合生成式和序列建模范式的混合架构在复杂视觉评估任务中具有显著潜力。

Abstract: The automated prediction of facial beauty is a benchmark task in affective
computing that requires a sophisticated understanding of both local aesthetic
details (e.g., skin texture) and global facial harmony (e.g., symmetry,
proportions). Existing models, based on either Convolutional Neural Networks
(CNNs) or Vision Transformers (ViTs), exhibit inherent architectural biases
that limit their performance; CNNs excel at local feature extraction but
struggle with long-range dependencies, while ViTs model global relationships at
a significant computational cost. This paper introduces the
\textbf{Mamba-Diffusion Network (MD-Net)}, a novel dual-stream architecture
that resolves this trade-off by delegating specialized roles to
state-of-the-art models. The first stream leverages a frozen U-Net encoder from
a pre-trained latent diffusion model, providing a powerful generative prior for
fine-grained aesthetic qualities. The second stream employs a Vision Mamba
(Vim), a modern state-space model, to efficiently capture global facial
structure with linear-time complexity. By synergistically integrating these
complementary representations through a cross-attention mechanism, MD-Net
creates a holistic and nuanced feature space for prediction. Evaluated on the
SCUT-FBP5500 benchmark, MD-Net sets a new state-of-the-art, achieving a Pearson
Correlation of \textbf{0.9235} and demonstrating the significant potential of
hybrid architectures that fuse generative and sequential modeling paradigms for
complex visual assessment tasks.

</details>


### [200] [VaseVQA: Multimodal Agent and Benchmark for Ancient Greek Pottery](https://arxiv.org/abs/2509.17191)
*Jinchao Ge,Tengfei Cheng,Biao Wu,Zeyu Zhang,Shiya Huang,Judith Bishop,Gillian Shepherd,Meng Fang,Ling Chen,Yang Zhao*

Main category: cs.CV

TL;DR: 该研究提出了VaseVL系统和VaseVQA数据集，旨在通过诊断指导的强化学习，提升多模态大语言模型（MLLMs）在古希腊陶器文化遗产分析中的专家级推理能力和组合鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在分析文化遗产文物时面临挑战，通用模型缺乏领域专业知识，而指令微调（SFT）常过度拟合表面模式，导致在文物鉴定和历史归属方面推理能力脆弱。因此，研究旨在为MLLMs配备针对古希腊陶器的强大、专家级推理能力。

Method: 研究提出了VaseVL，一个SFT-然后-RL的系统，将评估转化为监督信号。它构建了一个问题类型分类法，探测SFT模型以定位特定类型性能差距，并通过有针对性的、以组合性为导向的奖励来优化这些差距。同时，发布了VaseVQA，一个包含31,773张图像的综合基准数据集，旨在探测深度理解能力。

Result: 实验结果表明，VaseVL在风格分类和历史归属方面取得了最先进的（SOTA）成果，并且相比仅使用SFT的基线模型，在组合鲁棒性方面有显著提升。这验证了诊断指导、基于分类法的奖励工程的有效性。

Conclusion: 该研究成功验证了诊断指导、基于分类法的奖励工程方法，能够有效提升MLLMs在古希腊陶器分析中的专家级推理和组合鲁棒性，并为未来研究提供了一个可重用的资源（VaseVQA数据集和代码）。

Abstract: Analyzing cultural-heritage artifacts remains challenging for MLLMs: general
models lack domain expertise, and SFT often overfits superficial patterns,
yielding brittle reasoning for authentication and historical attribution. This
raises the question of how to equip MLLMs with robust, expert-level reasoning
for ancient Greek pottery. We present VaseVL, an SFT-then-RL system that turns
evaluation into supervision: we construct a taxonomy of question types, probe
the SFT model to localize type-specific performance gaps, and optimize with
type-conditioned, compositionality-oriented rewards targeting those gaps. We
also release VaseVQA, a comprehensive benchmark of 31,773 images designed to
probe deep understanding. Experiments show state-of-the-art results on style
classification and historical attribution with marked gains in compositional
robustness over SFT-only baselines, validating diagnosis-guided,
taxonomy-conditioned reward engineering and providing a reusable resource for
future research. Code and dataset will be available at
https://github.com/AIGeeksGroup/VaseVQA.

</details>


### [201] [Dual-View Alignment Learning with Hierarchical-Prompt for Class-Imbalance Multi-Label Classification](https://arxiv.org/abs/2509.17747)
*Sheng Huang,Jiexuan Yan,Beiyan Liu,Bo Liu,Richang Hong*

Main category: cs.CV

TL;DR: 本文提出HP-DVAL方法，利用视觉-语言预训练（VLP）模型的双视图对齐学习和分层提示调优，解决类别不平衡多标签图像分类（CI-MLIC）中的长尾和少样本问题，并在基准测试中取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据集常存在多类别间的类别不平衡（长尾分布和少样本情况），尤其在类别不平衡多标签图像分类（CI-MLIC）任务中，数据不平衡和多目标识别带来了巨大挑战。

Method: 本文提出HP-DVAL方法：1) 利用VLP模型的多模态知识来缓解多标签设置中的类别不平衡问题。2) 采用双视图对齐学习，通过提取互补特征实现精确的图像-文本对齐，以转移VLP模型的强大特征表示能力。3) 引入分层提示调优策略，利用全局和局部提示学习任务特定和上下文相关的先验知识。4) 设计语义一致性损失，防止学习到的提示偏离VLP模型中嵌入的通用知识。

Result: 在MS-COCO和VOC2007两个CI-MLIC基准测试上验证了方法的有效性。在长尾多标签图像分类任务中，mAP分别提升了10.0%和5.2%；在多标签少样本图像分类任务中，mAP分别提升了6.8%和2.9%。结果表明该方法优于现有SOTA方法。

Conclusion: HP-DVAL通过利用VLP模型的知识、双视图对齐学习和分层提示调优，有效解决了多标签图像分类中的类别不平衡问题，并在多个任务上取得了显著的性能提升。

Abstract: Real-world datasets often exhibit class imbalance across multiple categories,
manifesting as long-tailed distributions and few-shot scenarios. This is
especially challenging in Class-Imbalanced Multi-Label Image Classification
(CI-MLIC) tasks, where data imbalance and multi-object recognition present
significant obstacles. To address these challenges, we propose a novel method
termed Dual-View Alignment Learning with Hierarchical Prompt (HP-DVAL), which
leverages multi-modal knowledge from vision-language pretrained (VLP) models to
mitigate the class-imbalance problem in multi-label settings. Specifically,
HP-DVAL employs dual-view alignment learning to transfer the powerful feature
representation capabilities from VLP models by extracting complementary
features for accurate image-text alignment. To better adapt VLP models for
CI-MLIC tasks, we introduce a hierarchical prompt-tuning strategy that utilizes
global and local prompts to learn task-specific and context-related prior
knowledge. Additionally, we design a semantic consistency loss during prompt
tuning to prevent learned prompts from deviating from general knowledge
embedded in VLP models. The effectiveness of our approach is validated on two
CI-MLIC benchmarks: MS-COCO and VOC2007. Extensive experimental results
demonstrate the superiority of our method over SOTA approaches, achieving mAP
improvements of 10.0\% and 5.2\% on the long-tailed multi-label image
classification task, and 6.8\% and 2.9\% on the multi-label few-shot image
classification task.

</details>


### [202] [MirrorSAM2: Segment Mirror in Videos with Depth Perception](https://arxiv.org/abs/2509.17220)
*Mingchen Xu,Yukun Lai,Ze Ji,Jing Wu*

Main category: cs.CV

TL;DR: MirrorSAM2是首个将SAM2模型应用于RGB-D视频镜面分割的框架，通过引入四个定制模块，有效处理反射模糊和纹理混淆等挑战，实现了无提示的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 将SAM2模型应用于RGB-D视频镜面分割面临反射歧义和纹理混淆等关键挑战，需要专门的框架来解决这些问题并充分利用RGB和深度信息的互补性。

Method: MirrorSAM2引入了四个定制模块：深度扭曲模块（用于RGB和深度对齐）、深度引导多尺度点提示生成器（用于自动提示生成）、频率细节注意力融合模块（用于增强结构边界）以及带有可学习镜面标记的镜面掩码解码器（用于精细分割）。它充分利用RGB和深度信息的互补性，实现了无提示操作。

Result: MirrorSAM2在VMD和DVMD基准测试中取得了最先进的性能，即使在小镜子、弱边界和强反射等挑战性条件下也表现出色。这是首次实现SAM2的自动视频镜面分割。

Conclusion: MirrorSAM2成功地将SAM2模型应用于自动RGB-D视频镜面分割任务，通过其独特的模块设计克服了镜面检测的固有挑战，并达到了SOTA性能，填补了该领域的空白。

Abstract: This paper presents MirrorSAM2, the first framework that adapts Segment
Anything Model 2 (SAM2) to the task of RGB-D video mirror segmentation.
MirrorSAM2 addresses key challenges in mirror detection, such as reflection
ambiguity and texture confusion, by introducing four tailored modules: a Depth
Warping Module for RGB and depth alignment, a Depth-guided Multi-Scale Point
Prompt Generator for automatic prompt generation, a Frequency Detail Attention
Fusion Module to enhance structural boundaries, and a Mirror Mask Decoder with
a learnable mirror token for refined segmentation. By fully leveraging the
complementarity between RGB and depth, MirrorSAM2 extends SAM2's capabilities
to the prompt-free setting. To our knowledge, this is the first work to enable
SAM2 for automatic video mirror segmentation. Experiments on the VMD and DVMD
benchmark demonstrate that MirrorSAM2 achieves SOTA performance, even under
challenging conditions such as small mirrors, weak boundaries, and strong
reflections.

</details>


### [203] [DT-NeRF: A Diffusion and Transformer-Based Optimization Approach for Neural Radiance Fields in 3D Reconstruction](https://arxiv.org/abs/2509.17232)
*Bo Liu,Runlong Li,Li Zhou,Yan Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种名为DT-NeRF的方法，通过结合扩散模型和Transformer来增强3D场景重建中的细节恢复和多视角一致性，并在多个数据集上取得了显著优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有3D场景重建方法在稀疏视角下细节恢复不足，且多视角一致性难以保持，尤其是在复杂几何场景中。

Method: 本文提出DT-NeRF方法，将扩散模型与Transformer结合，旨在有效恢复稀疏视角下的细节并保持复杂几何场景中的高精度。

Result: DT-NeRF在Matterport3D和ShapeNet数据集上，于PSNR、SSIM、Chamfer Distance和Fidelity等指标上显著优于传统NeRF和其他先进方法。消融实验证实了扩散和Transformer模块对模型性能的关键作用。

Conclusion: DT-NeRF的设计展示了模块间的协同效应，为3D场景重建提供了一个高效且准确的解决方案。未来研究可进一步优化模型，探索更先进的生成模型和网络架构以提升其在大规模动态场景中的表现。

Abstract: This paper proposes a Diffusion Model-Optimized Neural Radiance Field
(DT-NeRF) method, aimed at enhancing detail recovery and multi-view consistency
in 3D scene reconstruction. By combining diffusion models with Transformers,
DT-NeRF effectively restores details under sparse viewpoints and maintains high
accuracy in complex geometric scenes. Experimental results demonstrate that
DT-NeRF significantly outperforms traditional NeRF and other state-of-the-art
methods on the Matterport3D and ShapeNet datasets, particularly in metrics such
as PSNR, SSIM, Chamfer Distance, and Fidelity. Ablation experiments further
confirm the critical role of the diffusion and Transformer modules in the
model's performance, with the removal of either module leading to a decline in
performance. The design of DT-NeRF showcases the synergistic effect between
modules, providing an efficient and accurate solution for 3D scene
reconstruction. Future research may focus on further optimizing the model,
exploring more advanced generative models and network architectures to enhance
its performance in large-scale dynamic scenes.

</details>


### [204] [Accurate and Efficient Low-Rank Model Merging in Core Space](https://arxiv.org/abs/2509.17786)
*Aniello Panariello,Daniel Marczak,Simone Magistri,Angelo Porrello,Bartłomiej Twardowski,Andrew D. Bagdanov,Simone Calderara,Joost van de Weijer*

Main category: cs.CV

TL;DR: 本文提出Core Space合并框架，能高效且准确地合并低秩适应（LoRA）模型，解决了现有方法效率低下和准确性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 尽管LoRA等参数高效适应技术使模型微调更易实现，但现有的LoRA模型合并方法通常通过合并全尺寸权重矩阵来牺牲这种效率。研究旨在找到一种既能保持LoRA效率又能提高合并准确性的方法。

Method: 提出了Core Space合并框架，该框架允许在共同对齐基准内合并LoRA适应模型，从而在保持低秩适应效率的同时，显著提高跨任务的准确性。作者还提供了投影到Core Space不会丢失信息的正式证明，并进行了复杂性分析以展示其效率增益。

Result: 广泛的实证结果表明，Core Space显著改进了现有合并技术，在视觉和语言任务上均取得了最先进的结果，同时只消耗了极少量的计算资源。

Conclusion: Core Space合并框架提供了一种高效且准确的LoRA适应模型合并方法，它在保持低秩适应效率的同时，显著提升了多任务的性能，并在计算资源方面具有优势。

Abstract: In this paper, we address the challenges associated with merging low-rank
adaptations of large neural networks. With the rise of parameter-efficient
adaptation techniques, such as Low-Rank Adaptation (LoRA), model fine-tuning
has become more accessible. While fine-tuning models with LoRA is highly
efficient, existing merging methods often sacrifice this efficiency by merging
fully-sized weight matrices. We propose the Core Space merging framework, which
enables the merging of LoRA-adapted models within a common alignment basis,
thereby preserving the efficiency of low-rank adaptation while substantially
improving accuracy across tasks. We further provide a formal proof that
projection into Core Space ensures no loss of information and provide a
complexity analysis showing the efficiency gains. Extensive empirical results
demonstrate that Core Space significantly improves existing merging techniques
and achieves state-of-the-art results on both vision and language tasks while
utilizing a fraction of the computational resources. Codebase is available at
https://github.com/apanariello4/core-space-merging.

</details>


### [205] [SPFSplatV2: Efficient Self-Supervised Pose-Free 3D Gaussian Splatting from Sparse Views](https://arxiv.org/abs/2509.17246)
*Ranran Huang,Krystian Mikolajczyk*

Main category: cs.CV

TL;DR: SPFSplatV2是一个高效的前馈框架，无需地面真实姿态，能从稀疏多视图图像进行3D高斯splatting。它同时预测3D高斯基元和相机姿态，在新视图合成方面达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的3D高斯splatting方法通常需要地面真实姿态（ground-truth poses），这限制了它们在更大、更多样化数据集上的可扩展性。

Method: 该方法采用共享特征提取骨干网络，在规范空间中同时预测3D高斯基元和相机姿态。它引入了蒙版注意力机制来高效估计训练时的目标姿态，并使用重投影损失来强制像素对齐的高斯基元，提供更强的几何约束。该训练框架兼容不同的重建架构，并能在无姿态监督下进行训练。

Result: SPFSplatV2在域内和域外新视图合成方面均达到了最先进的性能，即使在极端视角变化和有限图像重叠的情况下也表现出色。它超越了依赖几何监督进行相对姿态估计的最新方法。

Conclusion: 该方法通过消除对地面真实姿态的依赖，提供了可扩展性，使其能够利用更大、更多样化的数据集。在无姿态监督的情况下，SPFSplatV2在新视图合成方面实现了卓越的性能。

Abstract: We introduce SPFSplatV2, an efficient feed-forward framework for 3D Gaussian
splatting from sparse multi-view images, requiring no ground-truth poses during
training and inference. It employs a shared feature extraction backbone,
enabling simultaneous prediction of 3D Gaussian primitives and camera poses in
a canonical space from unposed inputs. A masked attention mechanism is
introduced to efficiently estimate target poses during training, while a
reprojection loss enforces pixel-aligned Gaussian primitives, providing
stronger geometric constraints. We further demonstrate the compatibility of our
training framework with different reconstruction architectures, resulting in
two model variants. Remarkably, despite the absence of pose supervision, our
method achieves state-of-the-art performance in both in-domain and
out-of-domain novel view synthesis, even under extreme viewpoint changes and
limited image overlap, and surpasses recent methods that rely on geometric
supervision for relative pose estimation. By eliminating dependence on
ground-truth poses, our method offers the scalability to leverage larger and
more diverse datasets. Code and pretrained models will be available on our
project page: https://ranrhuang.github.io/spfsplatv2/.

</details>


### [206] [TS-P$^2$CL: Plug-and-Play Dual Contrastive Learning for Vision-Guided Medical Time Series Classification](https://arxiv.org/abs/2509.17802)
*Qi'ao Xu,Pengfei Wang,Bo Zhong,Tianwen Qian,Xiaoling Wang,Ye Wang,Hong Yu*

Main category: cs.CV

TL;DR: 本文提出TS-P$^2$CL框架，通过将一维医学时间序列转换为二维伪图像，利用预训练视觉模型和双对比学习，显著提升了跨个体医学时间序列分类的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 医学时间序列（MedTS）分类在智能医疗中至关重要，但由于个体间异质性导致跨个体泛化能力差，以及现有方法受限于模态特异性归纳偏置，难以学习普适不变表示，严重限制了其效果。

Method: 本文提出TS-P$^2$CL，一个即插即用框架。它将一维生理信号转换为二维伪图像，建立与视觉领域的桥梁，从而利用预训练视觉模型的通用模式识别能力和从自然图像中学习到的丰富语义先验。在此统一空间中，采用双对比学习策略：模内一致性强制时间连贯性，跨模态对齐将时间序列动态与视觉语义对齐，以减轻个体特异性偏置并学习鲁棒、领域不变的特征。

Result: 在六个MedTS数据集上进行的大量实验表明，TS-P$^2$CL在受试者依赖和受试者独立设置中均持续优于十四种现有方法。

Conclusion: TS-P$^2$CL框架通过引入视觉引导范式和双对比学习策略，有效缓解了医学时间序列分类中的个体特异性偏置，学习到鲁棒、领域不变的特征，从而显著提高了跨个体泛化能力。

Abstract: Medical time series (MedTS) classification is pivotal for intelligent
healthcare, yet its efficacy is severely limited by poor cross-subject
generation due to the profound cross-individual heterogeneity. Despite advances
in architectural innovations and transfer learning techniques, current methods
remain constrained by modality-specific inductive biases that limit their
ability to learn universally invariant representations. To overcome this, we
propose TS-P$^2$CL, a novel plug-and-play framework that leverages the
universal pattern recognition capabilities of pre-trained vision models. We
introduce a vision-guided paradigm that transforms 1D physiological signals
into 2D pseudo-images, establishing a bridge to the visual domain. This
transformation enables implicit access to rich semantic priors learned from
natural images. Within this unified space, we employ a dual-contrastive
learning strategy: intra-modal consistency enforces temporal coherence, while
cross-modal alignment aligns time-series dynamics with visual semantics,
thereby mitigating individual-specific biases and learning robust,
domain-invariant features. Extensive experiments on six MedTS datasets
demonstrate that TS-P$^2$CL consistently outperforms fourteen methods in both
subject-dependent and subject-independent settings.

</details>


### [207] [Optimized Learned Image Compression for Facial Expression Recognition](https://arxiv.org/abs/2509.17262)
*Xiumei Li,Marc Windsheimer,Misha Sadeghi,Björn Eskofier,André Kaup*

Main category: cs.CV

TL;DR: 本研究提出了一种端到端模型，旨在通过定制损失函数和联合优化，在人脸表情识别（FER）任务中平衡数据压缩效率和识别准确性，有效解决有损压缩导致特征退化的问题。


<details>
  <summary>Details</summary>
Motivation: 视觉数据的存储和传输需要高效的数据压缩，但有损压缩通常会导致人脸表情识别（FER）任务中的特征退化和准确性降低，因此需要一种能同时提升压缩和识别性能的方法。

Method: 本研究提出了一种端到端模型，旨在保留关键特征并提升压缩和识别性能。引入了一个定制的损失函数来优化模型，以有效平衡压缩和识别性能。同时，研究还探讨了不同损失项权重对这种平衡的影响，并采用了联合优化策略。

Result: 实验结果表明，仅微调压缩模型可将分类准确率提高0.71%，压缩效率提高49.32%。而联合优化则在准确率上实现4.04%的显著提升，效率上实现89.12%的提升。此外，联合优化的分类模型在压缩和未压缩数据上均保持高准确率，且压缩模型即使在高压缩率下也能可靠地保留图像细节。

Conclusion: 本研究证明，通过定制损失函数和联合优化，提出的端到端模型能够有效平衡人脸表情识别任务中的数据压缩效率和识别准确性，显著提升性能，并能在高压缩率下依然保持图像细节和识别精度。

Abstract: Efficient data compression is crucial for the storage and transmission of
visual data. However, in facial expression recognition (FER) tasks, lossy
compression often leads to feature degradation and reduced accuracy. To address
these challenges, this study proposes an end-to-end model designed to preserve
critical features and enhance both compression and recognition performance. A
custom loss function is introduced to optimize the model, tailored to balance
compression and recognition performance effectively. This study also examines
the influence of varying loss term weights on this balance. Experimental
results indicate that fine-tuning the compression model alone improves
classification accuracy by 0.71% and compression efficiency by 49.32%, while
joint optimization achieves significant gains of 4.04% in accuracy and 89.12%
in efficiency. Moreover, the findings demonstrate that the jointly optimized
classification model maintains high accuracy on both compressed and
uncompressed data, while the compression model reliably preserves image
details, even at high compression rates.

</details>


### [208] [Task-Oriented Communications for 3D Scene Representation: Balancing Timeliness and Fidelity](https://arxiv.org/abs/2509.17282)
*Xiangmin Xu,Zhen Meng,Kan Chen,Jiaming Yang,Emma Li,Philip G. Zhao,David Flynn*

Main category: cs.CV

TL;DR: 解析错误


<details>
  <summary>Details</summary>
Motivation: 解析错误

Method: 解析错误

Result: 解析错误

Conclusion: 解析错误

Abstract: Real-time Three-dimensional (3D) scene representation is a foundational
element that supports a broad spectrum of cutting-edge applications, including
digital manufacturing, Virtual, Augmented, and Mixed Reality (VR/AR/MR), and
the emerging metaverse. Despite advancements in real-time communication and
computing, achieving a balance between timeliness and fidelity in 3D scene
representation remains a challenge. This work investigates a wireless network
where multiple homogeneous mobile robots, equipped with cameras, capture an
environment and transmit images to an edge server over channels for 3D
representation. We propose a contextual-bandit Proximal Policy Optimization
(PPO) framework incorporating both Age of Information (AoI) and semantic
information to optimize image selection for representation, balancing data
freshness and representation quality. Two policies -- the $\omega$-threshold
and $\omega$-wait policies -- together with two benchmark methods are
evaluated, timeliness embedding and weighted sum, on standard datasets and
baseline 3D scene representation models. Experimental results demonstrate
improved representation fidelity while maintaining low latency, offering
insight into the model's decision-making process. This work advances real-time
3D scene representation by optimizing the trade-off between timeliness and
fidelity in dynamic environments.

</details>


### [209] [UIPro: Unleashing Superior Interaction Capability For GUI Agents](https://arxiv.org/abs/2509.17328)
*Hongxin Li,Jingran Su,Jingfan Chen,Zheng Ju,Yuntao Chen,Qing Li,Zhaoxiang Zhang*

Main category: cs.CV

TL;DR: 本文提出了UIPro，一个通用的GUI代理，它通过大规模多平台、多任务的GUI交互数据和统一的动作空间进行训练，并在各种GUI任务中展现出卓越性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于视觉-语言模型（VLMs）的GUI代理方法受限于有限的场景、不足的数据规模和异构的动作空间，这些因素阻碍了通用GUI代理的开发。

Method: 本文提出了UIPro，一个通用的GUI代理。首先，构建了一个包含2060万个GUI理解任务的综合数据集用于预训练，以赋予UIPro强大的GUI基础能力。其次，建立了一个统一的动作空间来协调异构的GUI代理任务数据集，并生成一个合并数据集，通过持续微调来提升UIPro的动作预测能力。

Result: 实验结果表明，UIPro在多个平台上的多个GUI任务基准测试中表现出卓越的性能。

Conclusion: 所提出的UIPro方法及其大规模多平台/多任务数据训练和统一动作空间策略，在构建通用GUI代理方面是有效的。

Abstract: Building autonomous agents that perceive and operate graphical user
interfaces (GUIs) like humans has long been a vision in the field of artificial
intelligence. Central to these agents is the capability for GUI interaction,
which involves GUI understanding and planning capabilities. Existing methods
have tried developing GUI agents based on the multi-modal comprehension ability
of vision-language models (VLMs). However, the limited scenario, insufficient
size, and heterogeneous action spaces hinder the progress of building
generalist GUI agents. To resolve these issues, this paper proposes
\textbf{UIPro}, a novel generalist GUI agent trained with extensive
multi-platform and multi-task GUI interaction data, coupled with a unified
action space. We first curate a comprehensive dataset encompassing 20.6 million
GUI understanding tasks to pre-train UIPro, granting it a strong GUI grounding
capability, which is key to downstream GUI agent tasks. Subsequently, we
establish a unified action space to harmonize heterogeneous GUI agent task
datasets and produce a merged dataset to foster the action prediction ability
of UIPro via continued fine-tuning. Experimental results demonstrate UIPro's
superior performance across multiple GUI task benchmarks on various platforms,
highlighting the effectiveness of our approach.

</details>


### [210] [Trainee Action Recognition through Interaction Analysis in CCATT Mixed-Reality Training](https://arxiv.org/abs/2509.17888)
*Divya Mereddy,Marcos Quinones-Grueiro,Ashwin T S,Eduardo Davalos,Gautam Biswas,Kent Etherton,Tyler Davis,Katelyn Kay,Jill Lear,Benjamin Goldberg*

Main category: cs.CV

TL;DR: 本研究提出一个结合认知任务分析（CTA）和多模态学习分析（MMLA）的数据驱动评估框架，用于客观、全面地评估关键护理空中运输队（CCATT）在混合现实模拟中的训练表现。


<details>
  <summary>Details</summary>
Motivation: 传统的教官主导评估主观且可能遗漏关键事件，限制了评估的通用性和一致性。虽然基于AI的自动化评估有潜力，但其训练需要大量人工输入，且难以处理复杂的团队动态、环境噪音和多人员追踪的准确再识别。因此，需要更客观、全面且可解释的评估方法来衡量CCATT成员在高压航空医疗疏散条件下的临床和认知技能。

Method: 本研究开发了一个系统性的、数据驱动的评估框架，结合了认知任务分析（CTA）和多模态学习分析（MMLA）。具体方法包括：1) 为CCATT训练开发了一个领域特定的CTA模型；2) 使用一个经过微调的人机交互模型（Cascade Disentangling Network, CDN）构建了一个基于视觉的动作识别流程，用于检测和追踪受训者与设备随时间的交互。这些交互自动生成绩效指标（如反应时间、任务持续时间），并映射到分层的CTA模型上，以实现可解释、与领域相关的绩效评估。

Result: 该框架能够自动生成绩效指标（如反应时间、任务持续时间），并将其映射到分层的CTA模型上，从而实现可解释、与领域相关的绩效评估。这提供了一种更客观、更全面的性能评估方式，解决了传统评估方法的主观性和局限性。

Conclusion: 本研究引入了一个系统性的、数据驱动的评估框架，结合CTA和MMLA，通过视觉动作识别技术，为CCATT训练提供了客观、可解释且与领域相关的绩效评估。该框架有望提高高压模拟训练评估的准确性和一致性。

Abstract: This study examines how Critical Care Air Transport Team (CCATT) members are
trained using mixed-reality simulations that replicate the high-pressure
conditions of aeromedical evacuation. Each team - a physician, nurse, and
respiratory therapist - must stabilize severely injured soldiers by managing
ventilators, IV pumps, and suction devices during flight. Proficient
performance requires clinical expertise and cognitive skills, such as
situational awareness, rapid decision-making, effective communication, and
coordinated task management, all of which must be maintained under stress.
Recent advances in simulation and multimodal data analytics enable more
objective and comprehensive performance evaluation. In contrast, traditional
instructor-led assessments are subjective and may overlook critical events,
thereby limiting generalizability and consistency. However, AI-based automated
and more objective evaluation metrics still demand human input to train the AI
algorithms to assess complex team dynamics in the presence of environmental
noise and the need for accurate re-identification in multi-person tracking. To
address these challenges, we introduce a systematic, data-driven assessment
framework that combines Cognitive Task Analysis (CTA) with Multimodal Learning
Analytics (MMLA). We have developed a domain-specific CTA model for CCATT
training and a vision-based action recognition pipeline using a fine-tuned
Human-Object Interaction model, the Cascade Disentangling Network (CDN), to
detect and track trainee-equipment interactions over time. These interactions
automatically yield performance indicators (e.g., reaction time, task
duration), which are mapped onto a hierarchical CTA model tailored to CCATT
operations, enabling interpretable, domain-relevant performance evaluations.

</details>


### [211] [SmokeSeer: 3D Gaussian Splatting for Smoke Removal and Scene Reconstruction](https://arxiv.org/abs/2509.17329)
*Neham Jain,Andrew Jong,Sebastian Scherer,Ioannis Gkioulekas*

Main category: cs.CV

TL;DR: 本文提出了SmokeSeer方法，利用RGB和热成像多视角视频，同时进行3D场景重建和烟雾去除。该方法基于3D高斯泼溅，将场景明确分解为烟雾和非烟雾部分，能处理不同密度和时变烟雾。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的烟雾严重降低图像质量和可见性。现有图像恢复方法要么依赖易产生幻觉的数据驱动先验，要么仅限于静态低密度烟雾，无法有效解决问题。

Method: SmokeSeer方法融合了RGB和热成像图像（热成像因散射减少能透过烟雾），并基于3D高斯泼溅技术。它将场景显式分解为烟雾和非烟雾组件，能够处理各种烟雾密度并适应时间变化的烟雾。

Result: 该方法在合成数据上进行了验证，并引入了一个包含RGB和热成像图像的真实世界多视角烟雾数据集。项目提供了开源代码和数据。

Conclusion: SmokeSeer通过结合RGB和热成像信息并显式分解场景，有效实现了3D场景重建和烟雾去除，克服了现有方法在处理不同密度和时变烟雾方面的局限性。

Abstract: Smoke in real-world scenes can severely degrade the quality of images and
hamper visibility. Recent methods for image restoration either rely on
data-driven priors that are susceptible to hallucinations, or are limited to
static low-density smoke. We introduce SmokeSeer, a method for simultaneous 3D
scene reconstruction and smoke removal from a video capturing multiple views of
a scene. Our method uses thermal and RGB images, leveraging the fact that the
reduced scattering in thermal images enables us to see through the smoke. We
build upon 3D Gaussian splatting to fuse information from the two image
modalities, and decompose the scene explicitly into smoke and non-smoke
components. Unlike prior approaches, SmokeSeer handles a broad range of smoke
densities and can adapt to temporally varying smoke. We validate our approach
on synthetic data and introduce a real-world multi-view smoke dataset with RGB
and thermal images. We provide open-source code and data at the project
website.

</details>


### [212] [Revisiting Vision Language Foundations for No-Reference Image Quality Assessment](https://arxiv.org/abs/2509.17374)
*Ankit Yadav,Ta Duc Huy,Lingqiao Liu*

Main category: cs.CV

TL;DR: 本研究系统评估了六种主流预训练视觉骨干网络在无参考图像质量评估（NR-IQA）中的表现，发现SigLIP2性能突出，且激活函数的选择对模型泛化能力至关重要，并提出了一种可学习的激活函数选择机制，取得了新的SOTA成果。


<details>
  <summary>Details</summary>
Motivation: 大规模视觉语言预训练在无参考图像质量评估（NR-IQA）中展现出潜力，但现代Vision Transformer基础模型的相对优劣尚未被充分理解。

Method: 本研究系统评估了六种主流预训练骨干网络（CLIP、SigLIP2、DINOv2、DINOv3、Perception和ResNet）在NR-IQA任务中的表现，每种骨干网络都使用相同的轻量级MLP头部进行微调。此外，研究还引入了一种可学习的激活函数选择机制，该机制能自适应地为每个通道确定非线性函数。

Result: 研究发现：1) SigLIP2持续表现出强大的性能；2) 激活函数的选择扮演着出乎意料的关键角色，尤其是在增强图像质量评估模型的泛化能力方面。具体而言，简单的Sigmoid激活函数在多个基准测试中优于常用的ReLU和GELU。受此启发，提出的可学习激活选择机制在CLIVE、KADID10K和AGIQA3K数据集上取得了新的SOTA SRCC（Spearman's Rank Correlation Coefficient）。广泛的消融实验证实了该方法在不同架构和制度下的益处。

Conclusion: SigLIP2在NR-IQA任务中表现出色，且激活函数的选择对模型性能和泛化能力至关重要。提出的可学习激活函数选择机制能够自适应地优化非线性激活，无需手动设计，从而建立了强大且资源高效的NR-IQA基线模型。

Abstract: Large-scale vision language pre-training has recently shown promise for
no-reference image-quality assessment (NR-IQA), yet the relative merits of
modern Vision Transformer foundations remain poorly understood. In this work,
we present the first systematic evaluation of six prominent pretrained
backbones, CLIP, SigLIP2, DINOv2, DINOv3, Perception, and ResNet, for the task
of No-Reference Image Quality Assessment (NR-IQA), each finetuned using an
identical lightweight MLP head. Our study uncovers two previously overlooked
factors: (1) SigLIP2 consistently achieves strong performance; and (2) the
choice of activation function plays a surprisingly crucial role, particularly
for enhancing the generalization ability of image quality assessment models.
Notably, we find that simple sigmoid activations outperform commonly used ReLU
and GELU on several benchmarks. Motivated by this finding, we introduce a
learnable activation selection mechanism that adaptively determines the
nonlinearity for each channel, eliminating the need for manual activation
design, and achieving new state-of-the-art SRCC on CLIVE, KADID10K, and
AGIQA3K. Extensive ablations confirm the benefits across architectures and
regimes, establishing strong, resource-efficient NR-IQA baselines.

</details>


### [213] [Diff-GNSS: Diffusion-based Pseudorange Error Estimation](https://arxiv.org/abs/2509.17397)
*Jiaqi Zhu,Shouyi Lu,Ziyao Li,Guirong Zhuo,Lu Xiong*

Main category: cs.CV

TL;DR: Diff-GNSS是一个粗-细粒度的GNSS伪距误差估计框架，首次将条件扩散模型应用于此领域，通过Mamba模块进行粗估计，并利用扩散模型进行细化，显著提升了城市定位精度。


<details>
  <summary>Details</summary>
Motivation: GNSS在城市环境中因多径效应和非视距接收导致伪距测量误差大且分布复杂，现有基于学习的方法难以有效处理，从而限制了定位精度。

Method: 提出Diff-GNSS框架，包含：1) 基于Mamba模块的粗估计，提供初始预测；2) 条件去噪扩散层进行细化估计，捕捉复杂误差分布；3) 使用三个关键GNSS测量质量特征作为条件，引导反向去噪过程，实现可控合成；4) 在扩散阶段融入每颗卫星的不确定性建模，评估预测误差的可靠性。同时，收集并发布了真实世界数据集。

Result: 在公共和自收集数据集上的实验表明，Diff-GNSS在多项指标上持续优于现有最先进的基线方法。这是扩散模型首次应用于伪距误差估计。所提出的基于扩散的细化模块是即插即用的，可轻松集成到现有网络中以显著提高估计精度。

Conclusion: Diff-GNSS通过创新的粗-细粒度框架和条件扩散模型，成功解决了GNSS伪距误差分布复杂的问题，显著提高了误差估计精度，并为现有系统提供了易于集成的性能提升方案。

Abstract: Global Navigation Satellite Systems (GNSS) are vital for reliable urban
positioning. However, multipath and non-line-of-sight reception often introduce
large measurement errors that degrade accuracy. Learning-based methods for
predicting and compensating pseudorange errors have gained traction, but their
performance is limited by complex error distributions. To address this
challenge, we propose Diff-GNSS, a coarse-to-fine GNSS measurement
(pseudorange) error estimation framework that leverages a conditional diffusion
model to capture such complex distributions. Firstly, a Mamba-based module
performs coarse estimation to provide an initial prediction with appropriate
scale and trend. Then, a conditional denoising diffusion layer refines the
estimate, enabling fine-grained modeling of pseudorange errors. To suppress
uncontrolled generative diversity and achieve controllable synthesis, three key
features related to GNSS measurement quality are used as conditions to
precisely guide the reverse denoising process. We further incorporate
per-satellite uncertainty modeling within the diffusion stage to assess the
reliability of the predicted errors. We have collected and publicly released a
real-world dataset covering various scenes. Experiments on public and
self-collected datasets show that DiffGNSS consistently outperforms
state-of-the-art baselines across multiple metrics. To the best of our
knowledge, this is the first application of diffusion models to pseudorange
error estimation. The proposed diffusion-based refinement module is
plug-and-play and can be readily integrated into existing networks to markedly
improve estimation accuracy.

</details>


### [214] [Single-Image Depth from Defocus with Coded Aperture and Diffusion Posterior Sampling](https://arxiv.org/abs/2509.17427)
*Hodaka Kawachi,Jose Reinaldo Cunha Santos A. V. Silva Neto,Yasushi Yagi,Hajime Nagahara,Tomoya Nakamura*

Main category: cs.CV

TL;DR: 本文提出了一种基于编码光圈成像的单次快照散焦深度（DFD）重建方法，用学习到的扩散先验作为正则化，在无需配对训练数据的情况下，实现了比传统优化和U-Net基线更高的RGBD重建精度和稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统的散焦深度（DFD）重建方法依赖于手工设计的先验，而U-Net风格的回归器需要大量的配对散焦-RGBD训练数据，并且训练结果与特定的相机配置绑定。研究旨在开发一种更准确、更稳定、更灵活的DFD重建方法，克服这些限制。

Method: 该方法提出了一种用于编码光圈成像的单次快照DFD重建技术。它用学习到的扩散先验取代了手工设计的先验，并将其纯粹用作正则化。优化框架通过可微分的前向模型确保测量一致性，同时在去噪图像域中利用扩散先验引导解的生成。

Result: 实验表明，该方法比传统的优化方法具有更高的精度和稳定性。与U-Net风格的回归器不同，它不需要配对的散焦-RGBD训练数据，并且不将训练与特定的相机配置绑定。在全面的模拟和原型相机上，该方法在不同噪声水平下均能实现持续强大的RGBD重建，优于U-Net基线和传统的编码光圈DFD方法。

Conclusion: 所提出的基于学习扩散先验的单次快照DFD重建方法，在编码光圈成像中表现出卓越的RGBD重建性能，具有更高的准确性、稳定性和灵活性，且无需配对训练数据，克服了现有方法的局限性。

Abstract: We propose a single-snapshot depth-from-defocus (DFD) reconstruction method
for coded-aperture imaging that replaces hand-crafted priors with a learned
diffusion prior used purely as regularization. Our optimization framework
enforces measurement consistency via a differentiable forward model while
guiding solutions with the diffusion prior in the denoised image domain,
yielding higher accuracy and stability than clas- sical optimization. Unlike
U-Net-style regressors, our approach requires no paired defocus-RGBD training
data and does not tie training to a specific camera configuration. Experiments
on comprehensive simulations and a prototype camera demonstrate consistently
strong RGBD reconstructions across noise levels, outperforming both U-Net
baselines and a classical coded- aperture DFD method.

</details>


### [215] [Multi-scale Temporal Prediction via Incremental Generation and Multi-agent Collaboration](https://arxiv.org/abs/2509.17429)
*Zhitao Zeng,Guojian Yuan,Junyuan Mao,Yuxuan Wang,Xiaoshuang Jia,Yueming Jin*

Main category: cs.CV

TL;DR: 本文提出了多尺度时间预测（MSTP）任务，通过分解时间尺度和状态尺度来预测通用和手术场景中的精细状态。为此，作者创建了首个MSTP基准，并提出了一种名为IG-MC的方法，该方法结合了增量生成模块和多智能体协作框架。


<details>
  <summary>Details</summary>
Motivation: 准确的时间预测是实现全面场景理解和具身人工智能的关键。然而，现有的视觉-语言模型难以在多个时间尺度上预测场景的多种精细状态，这促使了对多尺度时间预测任务及其解决方案的研究。

Method: 本文首先将多尺度时间预测（MSTP）任务形式化，将其分解为时间尺度（预测不同提前期的状态）和状态尺度（建模状态的层次结构）。接着，作者构建了首个MSTP基准，其中包含跨多个状态和时间尺度的同步标注。最后，提出了一种名为“增量生成和多智能体协作”（IG-MC）的方法，该方法包含两个关键创新：1）一个可插拔的增量生成模块，用于合成不断更新的视觉预览，以同步决策和视觉，防止性能下降；2）一个决策驱动的多智能体协作框架，包含生成、启动和多状态评估智能体，以平衡全局连贯性和局部保真度。

Result: 本文的主要成果是：1）正式定义了多尺度时间预测（MSTP）任务，强调了时间尺度和状态尺度的分解；2）创建了首个支持MSTP任务的基准，该基准在多个状态和时间尺度上提供同步标注；3）提出了一种新颖的IG-MC方法，通过增量生成和多智能体协作有效地解决了MSTP任务的挑战。

Conclusion: 多尺度时间预测是连接场景理解与具身AI的关键挑战。本文通过形式化MSTP任务、构建专门的基准以及开发创新的IG-MC方法，为解决这一复杂问题提供了全面的框架和有效的解决方案，特别是在处理预测精度随时间尺度增加而下降的问题上表现出色。

Abstract: Accurate temporal prediction is the bridge between comprehensive scene
understanding and embodied artificial intelligence. However, predicting
multiple fine-grained states of a scene at multiple temporal scales is
difficult for vision-language models. We formalize the Multi-Scale Temporal
Prediction (MSTP) task in general and surgical scenes by decomposing
multi-scale into two orthogonal dimensions: the temporal scale, forecasting
states of humans and surgery at varying look-ahead intervals, and the state
scale, modeling a hierarchy of states in general and surgical scenes. For
example, in general scenes, states of contact relationships are finer-grained
than states of spatial relationships. In surgical scenes, medium-level steps
are finer-grained than high-level phases yet remain constrained by their
encompassing phase. To support this unified task, we introduce the first MSTP
Benchmark, featuring synchronized annotations across multiple state scales and
temporal scales. We further propose a method, Incremental Generation and
Multi-agent Collaboration (IG-MC), which integrates two key innovations. First,
we present a plug-and-play incremental generation module that continuously
synthesizes up-to-date visual previews at expanding temporal scales to inform
multiple decision-making agents, keeping decisions and generated visuals
synchronized and preventing performance degradation as look-ahead intervals
lengthen. Second, we present a decision-driven multi-agent collaboration
framework for multi-state prediction, comprising generation, initiation, and
multi-state assessment agents that dynamically trigger and evaluate prediction
cycles to balance global coherence and local fidelity.

</details>


### [216] [Emergent 3D Correspondence from Neural Shape Representation](https://arxiv.org/abs/2509.17431)
*Keyu Du,Jingyu Hu,Haipeng Li,Hao Xu,Haibing Huang,Chi-Wing Fu,Shuaicheng Liu*

Main category: cs.CV

TL;DR: 本文提出了一种利用分层神经语义表示（HNSR）和渐进式全局到局部匹配策略，实现准确、鲁棒的3D语义对应的新方法，该方法无需训练且兼容多种预训练3D生成模型，并在多项任务和跨类别场景中表现出色。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决3D语义对应中准确性和鲁棒性的挑战，尤其是在处理复杂结构和多样化形状时。

Method: 1. 设计了分层神经语义表示（HNSR），结合了捕获高层结构的全局语义特征和保留精细细节的多分辨率局部几何特征，并利用了预训练3D生成模型的3D先验知识。2. 提出了一种渐进式全局到局部匹配策略，首先使用全局语义特征建立粗略语义对应，然后通过局部几何特征迭代细化。3. 该框架无需训练，并广泛兼容各种预训练的3D生成骨干网络。

Result: 该方法在定性和定量评估中均优于现有最先进技术。它在各种形状类别中表现出强大的泛化能力，甚至在跨类别场景中也取得了有希望的结果。此外，该方法支持形状协同分割、关键点匹配和纹理迁移等多种应用。

Conclusion: 本文提出的方法通过分层神经语义表示和渐进式匹配策略，有效实现了准确且鲁棒的3D语义对应。其免训练特性、强大的泛化能力以及对多种应用的兼容性，证明了其优越的性能和广泛的适用性。

Abstract: This paper presents a new approach to estimate accurate and robust 3D
semantic correspondence with the hierarchical neural semantic representation.
Our work has three key contributions. First, we design the hierarchical neural
semantic representation (HNSR), which consists of a global semantic feature to
capture high-level structure and multi-resolution local geometric features to
preserve fine details, by carefully harnessing 3D priors from pre-trained 3D
generative models. Second, we design a progressive global-to-local matching
strategy, which establishes coarse semantic correspondence using the global
semantic feature, then iteratively refines it with local geometric features,
yielding accurate and semantically-consistent mappings. Third, our framework is
training-free and broadly compatible with various pre-trained 3D generative
backbones, demonstrating strong generalization across diverse shape categories.
Our method also supports various applications, such as shape co-segmentation,
keypoint matching, and texture transfer, and generalizes well to structurally
diverse shapes, with promising results even in cross-category scenarios. Both
qualitative and quantitative evaluations show that our method outperforms
previous state-of-the-art techniques.

</details>


### [217] [Beyond Diagnosis: Evaluating Multimodal LLMs for Pathology Localization in Chest Radiographs](https://arxiv.org/abs/2509.18015)
*Advait Gosai,Arun Kavishwar,Stephanie L. McNamara,Soujanya Samineni,Renato Umeton,Alexander Chowdhury,William Lotter*

Main category: cs.CV

TL;DR: 本研究评估了通用多模态大语言模型（GPT-4、GPT-5）和领域特定模型（MedGemma）在胸部X光片病理定位任务上的表现，发现它们虽然有潜力，但定位精度仍低于专用CNN基线和放射科医生，且GPT-5表现最佳。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型及其多模态版本在医学问答和诊断任务中表现出色，但医学图像解读的一个基本方面是病理发现的定位能力。评估定位能力不仅具有临床和教育意义，还能深入了解模型对解剖学和疾病的空间理解。

Method: 研究系统评估了两个通用多模态大语言模型（GPT-4和GPT-5）以及一个领域特定模型（MedGemma）在胸部X光片上定位病理的能力。采用了一种提示管道，该管道通过叠加空间网格并引导模型进行基于坐标的预测。评估使用了CheXlocalize数据集中的九种病理。

Result: 在CheXlocalize数据集的九种病理上平均，GPT-5的定位准确率为49.7%，其次是GPT-4（39.1%）和MedGemma（17.7%）。所有模型的表现均低于任务专用CNN基线（59.9%）和放射科医生基准（80.1%）。错误分析显示，GPT-5的预测大多在解剖学上合理区域，但精度不足。GPT-4在固定解剖位置的病理上表现良好，但在空间可变发现上表现不佳，并更频繁地出现解剖学上不合理的预测。MedGemma在所有病理上的表现最差，泛化能力有限。

Conclusion: 当前多模态大语言模型在医学影像中的应用既有前景也有局限性。虽然它们在病理定位方面表现出一定潜力（尤其是GPT-5），但其精度仍远低于人类专家和任务专用工具。研究强调了将多模态大语言模型与任务特定工具结合以实现可靠使用的重要性。

Abstract: Recent work has shown promising performance of frontier large language models
(LLMs) and their multimodal counterparts in medical quizzes and diagnostic
tasks, highlighting their potential for broad clinical utility given their
accessible, general-purpose nature. However, beyond diagnosis, a fundamental
aspect of medical image interpretation is the ability to localize pathological
findings. Evaluating localization not only has clinical and educational
relevance but also provides insight into a model's spatial understanding of
anatomy and disease. Here, we systematically assess two general-purpose MLLMs
(GPT-4 and GPT-5) and a domain-specific model (MedGemma) in their ability to
localize pathologies on chest radiographs, using a prompting pipeline that
overlays a spatial grid and elicits coordinate-based predictions. Averaged
across nine pathologies in the CheXlocalize dataset, GPT-5 exhibited a
localization accuracy of 49.7%, followed by GPT-4 (39.1%) and MedGemma (17.7%),
all lower than a task-specific CNN baseline (59.9%) and a radiologist benchmark
(80.1%). Despite modest performance, error analysis revealed that GPT-5's
predictions were largely in anatomically plausible regions, just not always
precisely localized. GPT-4 performed well on pathologies with fixed anatomical
locations, but struggled with spatially variable findings and exhibited
anatomically implausible predictions more frequently. MedGemma demonstrated the
lowest performance on all pathologies, showing limited capacity to generalize
to this novel task. Our findings highlight both the promise and limitations of
current MLLMs in medical imaging and underscore the importance of integrating
them with task-specific tools for reliable use.

</details>


### [218] [CARINOX: Inference-time Scaling with Category-Aware Reward-based Initial Noise Optimization and Exploration](https://arxiv.org/abs/2509.17458)
*Seyed Amir Kasaei,Ali Aghayari,Arash Marioriyad,Niki Sepasian,Shayan Baghayi Nejad,MohammadAmin Fazli,Mahdieh Soleymani Baghshah,Mohammad Hossein Rohban*

Main category: cs.CV

TL;DR: 本文提出CARINOX框架，通过结合噪声优化与探索，并利用与人类判断相关的奖励选择程序，显著提高了文本到图像扩散模型在复杂提示下的合成对齐能力。


<details>
  <summary>Details</summary>
Motivation: 文本到图像扩散模型（如Stable Diffusion）在生成高质量图像时，常难以实现组合对齐，尤其在处理复杂对象关系、属性或空间排列时。现有的推理时优化或探索方法存在局限性（如优化停滞、探索样本量过大），且单一或随意组合的奖励指标无法可靠捕捉组合性，导致指导效果不佳。

Method: CARINOX是一个统一的框架，它结合了初始噪声优化和探索。该框架采用了一种基于与人类判断相关性的原则性奖励选择程序，以克服现有方法的局限性并提供更一致的指导。

Result: 在T2I-CompBench++和HRS两个基准测试中，CARINOX将平均对齐分数分别提高了+16%和+11%。它在所有主要类别中持续优于最先进的基于优化和探索的方法，同时保持了图像质量和多样性。

Conclusion: CARINOX通过整合噪声优化、探索和基于人类判断的奖励选择，有效解决了文本到图像扩散模型在组合对齐方面的挑战，实现了显著的性能提升，同时保留了图像的质量和多样性。

Abstract: Text-to-image diffusion models, such as Stable Diffusion, can produce
high-quality and diverse images but often fail to achieve compositional
alignment, particularly when prompts describe complex object relationships,
attributes, or spatial arrangements. Recent inference-time approaches address
this by optimizing or exploring the initial noise under the guidance of reward
functions that score text-image alignment without requiring model fine-tuning.
While promising, each strategy has intrinsic limitations when used alone:
optimization can stall due to poor initialization or unfavorable search
trajectories, whereas exploration may require a prohibitively large number of
samples to locate a satisfactory output. Our analysis further shows that
neither single reward metrics nor ad-hoc combinations reliably capture all
aspects of compositionality, leading to weak or inconsistent guidance. To
overcome these challenges, we present Category-Aware Reward-based Initial Noise
Optimization and Exploration (CARINOX), a unified framework that combines noise
optimization and exploration with a principled reward selection procedure
grounded in correlation with human judgments. Evaluations on two complementary
benchmarks covering diverse compositional challenges show that CARINOX raises
average alignment scores by +16% on T2I-CompBench++ and +11% on the HRS
benchmark, consistently outperforming state-of-the-art optimization and
exploration-based methods across all major categories, while preserving image
quality and diversity. The project page is available at
https://amirkasaei.com/carinox/{this URL}.

</details>


### [219] [CSDformer: A Conversion Method for Fully Spike-Driven Transformer](https://arxiv.org/abs/2509.17461)
*Yuhao Zhang,Chengjun Zhang,Di Wu,Jie Yang,Mohamad Sawan*

Main category: cs.CV

TL;DR: CSDformer是一种新的全脉冲驱动Transformer转换方法，旨在通过定制架构、替换Softmax和引入延迟积分-触发神经元，解决现有方法训练成本高和硬件不友好等问题，实现了高性能、超低延迟和显著降低训练开销。


<details>
  <summary>Details</summary>
Motivation: 现有的脉冲Transformer模型生成方法存在严重局限：直接训练方法导致过高的训练成本，而现有转换方法则包含不可避免的硬件不友好操作。

Method: 本文提出CSDformer，一种用于全脉冲驱动Transformer的新型转换方法。该方法包括：1) 定制面向转换的Transformer架构；2) 提出NReLU函数替代自注意力机制中的Softmax；3) 对模型进行量化和训练，然后通过时间分解技术转换为全脉冲驱动模型；4) 引入延迟积分-触发神经元（delayed Integrate-and-Fire neurons）以减少转换误差并提高脉冲模型性能。

Result: CSDformer在ImageNet上以7个时间步实现了76.36%的top-1准确率，优于现有最先进模型。此外，它消除了训练SNN的需要，显著降低了训练成本（计算资源减少75%，训练速度加快2-3倍）。据作者所知，这是首个通过转换方法开发的全脉冲驱动Transformer模型，在超低延迟下实现了高性能，同时大幅降低了计算复杂度和训练开销。

Conclusion: CSDformer成功地通过一种高效的转换方法构建了全脉冲驱动的Transformer模型，克服了现有方法的局限性。它在实现高性能和超低延迟的同时，显著降低了训练成本和计算开销，为脉冲神经网络的发展提供了一条有前景的路径。

Abstract: Spike-based transformer is a novel architecture aiming to enhance the
performance of spiking neural networks while mitigating the energy overhead
inherent to transformers. However, methods for generating these models suffer
from critical limitations: excessive training costs introduced by direct
training methods, or unavoidably hardware-unfriendly operations in existing
conversion methods. In this paper, we propose CSDformer, a novel conversion
method for fully spike-driven transformers. We tailor a conversion-oriented
transformer-based architecture and propose a new function NReLU to replace
softmax in self-attention. Subsequently, this model is quantized and trained,
and converted into a fully spike-driven model with temporal decomposition
technique. Also, we propose delayed Integrate-andFire neurons to reduce
conversion errors and improve the performance of spiking models. We evaluate
CSDformer on ImageNet, CIFAR-10 and CIFAR-100 datasets and achieve 76.36% top-1
accuracy under 7 time-steps on ImageNet, demonstrating superiority over
state-of-the-art models. Furthermore, CSDformer eliminates the need for
training SNNs, thereby reducing training costs (reducing computational resource
by 75% and accelerating training speed by 2-3$\times$). To the best of our
knowledge, this is the first fully spike-driven transformer-based model
developed via conversion method, achieving high performance under ultra-low
latency, while dramatically reducing both computational complexity and training
overhead.

</details>


### [220] [MAESTRO: Task-Relevant Optimization via Adaptive Feature Enhancement and Suppression for Multi-task 3D Perception](https://arxiv.org/abs/2509.17462)
*Changwon Kang,Jisong Kim,Hongjae Shin,Junseo Park,Jun Won Choi*

Main category: cs.CV

TL;DR: MAESTRO是一个多任务3D感知框架，通过生成任务特定特征来缓解任务冲突和特征干扰，显著提升了3D目标检测、BEV地图分割和3D占用预测的性能。


<details>
  <summary>Details</summary>
Motivation: 多任务学习在提高学习效率的同时，可能由于优化不同目标时产生的任务冲突，导致性能下降。

Method: MAESTRO框架包含三个组件：1. 类别原型生成器（CPG）：将类别分为前景和背景组，生成组级原型，并根据任务分配。2. 任务特定特征生成器（TSFG）：利用原型组保留任务相关特征并抑制不相关特征。3. 场景原型聚合器（SPA）：利用3D目标检测头和地图分割头的信息，增强分配给3D占用预测的原型组。

Result: 在nuScenes和Occ3D基准上的大量实验表明，MAESTRO在3D目标检测、BEV地图分割和3D占用预测任务上始终优于现有方法。

Conclusion: MAESTRO通过其结构化框架和组件，成功解决了多任务3D感知中的任务冲突和特征干扰问题，实现了各任务性能的显著提升。

Abstract: The goal of multi-task learning is to learn to conduct multiple tasks
simultaneously based on a shared data representation. While this approach can
improve learning efficiency, it may also cause performance degradation due to
task conflicts that arise when optimizing the model for different objectives.
To address this challenge, we introduce MAESTRO, a structured framework
designed to generate task-specific features and mitigate feature interference
in multi-task 3D perception, including 3D object detection, bird's-eye view
(BEV) map segmentation, and 3D occupancy prediction. MAESTRO comprises three
components: the Class-wise Prototype Generator (CPG), the Task-Specific Feature
Generator (TSFG), and the Scene Prototype Aggregator (SPA). CPG groups class
categories into foreground and background groups and generates group-wise
prototypes. The foreground and background prototypes are assigned to the 3D
object detection task and the map segmentation task, respectively, while both
are assigned to the 3D occupancy prediction task. TSFG leverages these
prototype groups to retain task-relevant features while suppressing irrelevant
features, thereby enhancing the performance for each task. SPA enhances the
prototype groups assigned for 3D occupancy prediction by utilizing the
information produced by the 3D object detection head and the map segmentation
head. Extensive experiments on the nuScenes and Occ3D benchmarks demonstrate
that MAESTRO consistently outperforms existing methods across 3D object
detection, BEV map segmentation, and 3D occupancy prediction tasks.

</details>


### [221] [UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning](https://arxiv.org/abs/2509.18094)
*Ye Liu,Zongyang Ma,Junfu Pu,Zhongang Qi,Yang Wu,Ying Shan,Chang Wen Chen*

Main category: cs.CV

TL;DR: 本文提出UniPixel，一个大型多模态模型，它将像素级感知与通用视觉理解无缝集成，能够根据视觉提示生成掩码并进行细粒度像素级推理，并在多项基准测试和新设计的PixelQA任务上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的大型多模态模型（LMMs）在整体图像和视频语言理解方面表现出色，但在细粒度像素级理解方面关注较少。虽然一些研究将LMMs应用于区域级字幕和指代表达分割等任务，但这些模型通常独立执行指代或分割任务，未能将这些细粒度感知能力整合到视觉推理中。

Method: 本文提出了UniPixel，一个大型多模态模型，它能够灵活理解视觉提示输入并生成基于掩码的响应。UniPixel通过无缝集成像素级感知和通用视觉理解能力来区分自己。具体而言，它处理视觉提示并按需生成相关掩码，并在推理过程中以这些中间指针（掩码）为条件进行后续推理，从而实现细粒度像素级推理。

Result: UniPixel在10个基准测试上验证了其方法的有效性，这些任务涵盖了图像/视频中的像素级指代/分割和以对象为中心的理解。此外，还设计了一个新的PixelQA任务，该任务联合要求指代、分割和问答，以验证该方法的灵活性。

Conclusion: UniPixel成功弥合了LMMs在细粒度像素级理解方面的不足，通过将像素级感知与通用视觉理解能力整合，实现了细粒度像素级推理，并在多项任务和新设计的PixelQA任务中展现了其有效性和灵活性。

Abstract: Recent advances in Large Multi-modal Models (LMMs) have demonstrated their
remarkable success as general-purpose multi-modal assistants, with particular
focuses on holistic image- and video-language understanding. Conversely, less
attention has been given to scaling fine-grained pixel-level understanding
capabilities, where the models are expected to realize pixel-level alignment
between visual signals and language semantics. Some previous studies have
applied LMMs to related tasks such as region-level captioning and referring
expression segmentation. However, these models are limited to performing either
referring or segmentation tasks independently and fail to integrate these
fine-grained perception capabilities into visual reasoning. To bridge this gap,
we propose UniPixel, a large multi-modal model capable of flexibly
comprehending visual prompt inputs and generating mask-grounded responses. Our
model distinguishes itself by seamlessly integrating pixel-level perception
with general visual understanding capabilities. Specifically, UniPixel
processes visual prompts and generates relevant masks on demand, and performs
subsequent reasoning conditioning on these intermediate pointers during
inference, thereby enabling fine-grained pixel-level reasoning. The
effectiveness of our approach has been verified on 10 benchmarks across a
diverse set of tasks, including pixel-level referring/segmentation and
object-centric understanding in images/videos. A novel PixelQA task that
jointly requires referring, segmentation, and question answering is also
designed to verify the flexibility of our method.

</details>


### [222] [Stable Video-Driven Portraits](https://arxiv.org/abs/2509.17476)
*Mallikarjun B. R.,Fei Yin,Vikram Voleti,Nikita Drobyshev,Maksim Lapin,Aaryaman Vasishta,Varun Jampani*

Main category: cs.CV

TL;DR: 本文提出一种新的基于扩散模型的肖像动画框架，利用驱动视频中遮罩的面部区域（眼睛、鼻子、嘴巴）作为强运动控制信号，结合跨身份监督、少量新参数、时空注意力机制和历史帧，实现了高质量、高时间一致性和精确表情控制的动画。


<details>
  <summary>Details</summary>
Motivation: 早期肖像动画方法（如3D可变形模型或特征扭曲）存在表达能力有限、时间不一致和对未知身份或大姿态变化泛化性差的问题。近期扩散模型虽提升了质量，但仍受限于弱控制信号和架构缺陷。

Method: 本文提出一种新的基于扩散模型的框架：1) 利用驱动视频中遮罩的面部区域（眼睛、鼻子、嘴巴）作为强运动控制信号。2) 采用跨身份监督进行鲁棒训练，避免外观泄露。3) 引入最少的新参数以利用预训练扩散模型的强大先验，实现更快收敛和更好泛化。4) 引入时空注意力机制，捕捉帧间和帧内交互，有效捕获细微运动并减少时间伪影。5) 使用历史帧确保片段间的连续性。6) 在推理时，提出一种新的信号融合策略，以平衡运动保真度和身份保留。

Result: 该方法实现了卓越的时间一致性和精确的表情控制，能够生成高质量、可控的肖像动画。

Conclusion: 本文提出的方法能够生成高质量、可控的肖像动画，适用于现实世界的应用。

Abstract: Portrait animation aims to generate photo-realistic videos from a single
source image by reenacting the expression and pose from a driving video. While
early methods relied on 3D morphable models or feature warping techniques, they
often suffered from limited expressivity, temporal inconsistency, and poor
generalization to unseen identities or large pose variations. Recent advances
using diffusion models have demonstrated improved quality but remain
constrained by weak control signals and architectural limitations. In this
work, we propose a novel diffusion based framework that leverages masked facial
regions specifically the eyes, nose, and mouth from the driving video as strong
motion control cues. To enable robust training without appearance leakage, we
adopt cross identity supervision. To leverage the strong prior from the
pretrained diffusion model, our novel architecture introduces minimal new
parameters that converge faster and help in better generalization. We introduce
spatial temporal attention mechanisms that allow inter frame and intra frame
interactions, effectively capturing subtle motions and reducing temporal
artifacts. Our model uses history frames to ensure continuity across segments.
At inference, we propose a novel signal fusion strategy that balances motion
fidelity with identity preservation. Our approach achieves superior temporal
consistency and accurate expression control, enabling high-quality,
controllable portrait animation suitable for real-world applications.

</details>


### [223] [SAMSON: 3rd Place Solution of LSVOS 2025 VOS Challenge](https://arxiv.org/abs/2509.17500)
*Yujie Xie,Hongyang Zhang,Zhihui Liu,Shihai Ruan*

Main category: cs.CV

TL;DR: 本文提出了SAMSON，一个针对大规模视频目标分割（LSVOS）的解决方案，在ICCV 2025 MOSE赛道中获得第三名，通过整合先进模型、长时记忆模块和后处理策略来应对复杂挑战。


<details>
  <summary>Details</summary>
Motivation: 大规模视频目标分割（LSVOS）面临对象重现、小目标、严重遮挡和拥挤场景等挑战。现有基于SAM2的方法主要依赖各种记忆机制，但在处理视觉相似实例和长期目标消失方面仍有不足。

Method: 本文提出了SAMSON（Segment Anything with Memory Strengthened Object Navigation），整合了最先进的VOS模型。为处理视觉相似实例和长期目标消失，引入了长时记忆模块以实现可靠的目标重识别。此外，采用SAM2Long作为后处理策略，以减少误差累积并增强长视频序列中的分割稳定性。

Result: 该方法在测试集排行榜上取得了0.8427的J&F性能。

Conclusion: SAMSON通过结合先进VOS模型、长时记忆模块进行目标重识别以及SAM2Long后处理策略，有效解决了大规模视频目标分割的复杂挑战，并取得了优异的性能。

Abstract: Large-scale Video Object Segmentation (LSVOS) addresses the challenge of
accurately tracking and segmenting objects in long video sequences, where
difficulties stem from object reappearance, small-scale targets, heavy
occlusions, and crowded scenes. Existing approaches predominantly adopt
SAM2-based frameworks with various memory mechanisms for complex video mask
generation. In this report, we proposed Segment Anything with Memory
Strengthened Object Navigation (SAMSON), the 3rd place solution in the MOSE
track of ICCV 2025, which integrates the strengths of stateof-the-art VOS
models into an effective paradigm. To handle visually similar instances and
long-term object disappearance in MOSE, we incorporate a long-term memorymodule
for reliable object re-identification. Additionly, we adopt SAM2Long as a
post-processing strategy to reduce error accumulation and enhance segmentation
stability in long video sequences. Our method achieved a final performance of
0.8427 in terms of J &F in the test-set leaderboard.

</details>


### [224] [4D-MoDe: Towards Editable and Scalable Volumetric Streaming via Motion-Decoupled 4D Gaussian Compression](https://arxiv.org/abs/2509.17506)
*Houqiang Zhong,Zihan Zheng,Qiang Hu,Yuan Tian,Ning Cao,Lan Xu,Xiaoyun Zhang,Zhengxue Cheng,Li Song,Wenjun Zhang*

Main category: cs.CV

TL;DR: 本文提出4D-MoDe，一种运动解耦的4D高斯压缩框架，用于可扩展、可编辑的体视频流传输，在保持高质量重建的同时，大幅降低存储成本。


<details>
  <summary>Details</summary>
Motivation: 大规模传输高质量动态体内容面临巨大挑战，原因在于数据量庞大、运动复杂以及现有表示形式的可编辑性有限。

Method: 4D-MoDe框架采用分层表示，通过前瞻性运动分解策略将静态背景与动态前景显式分离，以减少时间冗余。它使用多分辨率运动估计网格和轻量级共享MLP来捕捉连续运动轨迹，并通过动态高斯补偿机制建模新兴内容。自适应分组方案动态插入背景关键帧以平衡时间一致性和压缩效率。此外，熵感知训练管道在率失真（RD）目标下联合优化运动场和高斯参数，并结合范围编码和KD树压缩以最小化存储开销。

Result: 4D-MoDe在多个数据集上实现了具有竞争力的重建质量，同时存储成本比现有最先进方法低一个数量级（例如，低至11.4 KB/帧）。它还支持背景替换和仅前景流传输等实际应用。

Conclusion: 4D-MoDe为体视频流提供了一个可扩展且可编辑的解决方案，显著提高了压缩效率，并支持多种实际应用，有效解决了高质量体内容传输的挑战。

Abstract: Volumetric video has emerged as a key medium for immersive telepresence and
augmented/virtual reality, enabling six-degrees-of-freedom (6DoF) navigation
and realistic spatial interactions. However, delivering high-quality dynamic
volumetric content at scale remains challenging due to massive data volume,
complex motion, and limited editability of existing representations. In this
paper, we present 4D-MoDe, a motion-decoupled 4D Gaussian compression framework
designed for scalable and editable volumetric video streaming. Our method
introduces a layered representation that explicitly separates static
backgrounds from dynamic foregrounds using a lookahead-based motion
decomposition strategy, significantly reducing temporal redundancy and enabling
selective background/foreground streaming. To capture continuous motion
trajectories, we employ a multi-resolution motion estimation grid and a
lightweight shared MLP, complemented by a dynamic Gaussian compensation
mechanism to model emergent content. An adaptive grouping scheme dynamically
inserts background keyframes to balance temporal consistency and compression
efficiency. Furthermore, an entropy-aware training pipeline jointly optimizes
the motion fields and Gaussian parameters under a rate-distortion (RD)
objective, while employing range-based and KD-tree compression to minimize
storage overhead. Extensive experiments on multiple datasets demonstrate that
4D-MoDe consistently achieves competitive reconstruction quality with an order
of magnitude lower storage cost (e.g., as low as \textbf{11.4} KB/frame)
compared to state-of-the-art methods, while supporting practical applications
such as background replacement and foreground-only streaming.

</details>


### [225] [4DGCPro: Efficient Hierarchical 4D Gaussian Compression for Progressive Volumetric Video Streaming](https://arxiv.org/abs/2509.17513)
*Zihan Zheng,Zhenlong Wu,Houqiang Zhong,Yuan Tian,Ning Cao,Lan Xu,Jiangchao Yao,Xiaoyun Zhang,Qiang Hu,Wenjun Zhang*

Main category: cs.CV

TL;DR: 4DGCPro是一种新型分层4D高斯压缩框架，支持在单个比特流中实现渐进式体三维视频流传输，从而在移动设备上实现实时解码、高质量渲染以及灵活的质量和多比特率。


<details>
  <summary>Details</summary>
Motivation: 现有体三维视频压缩方法要么缺乏在单个模型中调整质量和比特率的灵活性，以适应不同网络和设备的高效流传输；要么难以在轻量级移动平台上实现实时解码和渲染。

Method: 本文提出了4DGCPro，一个分层4D高斯压缩框架。具体方法包括：1) 提出了一种感知加权且压缩友好的分层4D高斯表示，结合运动感知自适应分组，以减少时间冗余、保持一致性并实现可扩展的多层次细节流传输。2) 提出了一种端到端熵优化训练方案，该方案结合了逐层率失真（RD）监督和属性特定熵建模，以高效生成比特流。

Result: 实验表明，4DGCPro在单个模型中实现了灵活的质量和多种比特率，能够在移动设备上实现实时解码和渲染，并在多个数据集上优于现有方法的率失真（RD）性能。

Conclusion: 4DGCPro通过提供渐进式体三维视频流传输，解决了体三维视频无缝观看的挑战，实现了移动设备上的实时解码和高质量渲染，同时在质量和比特率方面提供了卓越的灵活性和性能。

Abstract: Achieving seamless viewing of high-fidelity volumetric video, comparable to
2D video experiences, remains an open challenge. Existing volumetric video
compression methods either lack the flexibility to adjust quality and bitrate
within a single model for efficient streaming across diverse networks and
devices, or struggle with real-time decoding and rendering on lightweight
mobile platforms. To address these challenges, we introduce 4DGCPro, a novel
hierarchical 4D Gaussian compression framework that facilitates real-time
mobile decoding and high-quality rendering via progressive volumetric video
streaming in a single bitstream. Specifically, we propose a
perceptually-weighted and compression-friendly hierarchical 4D Gaussian
representation with motion-aware adaptive grouping to reduce temporal
redundancy, preserve coherence, and enable scalable multi-level detail
streaming. Furthermore, we present an end-to-end entropy-optimized training
scheme, which incorporates layer-wise rate-distortion (RD) supervision and
attribute-specific entropy modeling for efficient bitstream generation.
Extensive experiments show that 4DGCPro enables flexible quality and multiple
bitrate within a single model, achieving real-time decoding and rendering on
mobile devices while outperforming existing methods in RD performance across
multiple datasets. Project Page: https://mediax-sjtu.github.io/4DGCPro

</details>


### [226] [Unified Multimodal Coherent Field: Synchronous Semantic-Spatial-Vision Fusion for Brain Tumor Segmentation](https://arxiv.org/abs/2509.17520)
*Mingda Zhang,Yuyang Zheng,Ruixiang Tang,Jingru Qiu,Haiyan Ding*

Main category: cs.CV

TL;DR: 本文提出了一种名为统一多模态相干场（UMCF）的新方法，用于脑肿瘤分割，通过在统一的3D潜在空间中融合视觉、语义和空间信息，并利用不确定性门控和医学先验知识，显著提高了分割精度。


<details>
  <summary>Details</summary>
Motivation: 脑肿瘤分割面临肿瘤组织异质性、边界模糊和MRI序列对比度变化等挑战，导致仅依赖视觉信息或后验损失约束的方法在边界描绘和层次结构保持方面表现不稳定。

Method: UMCF方法在统一的3D潜在空间中实现视觉、语义和空间信息的同步交互融合，通过无参数不确定性门控自适应调整模态贡献，并让医学先验知识直接参与注意力计算，避免了传统的“先处理后拼接”的分离架构。

Result: 在BraTS 2020和2021数据集上，UMCF+nnU-Net的平均Dice系数分别达到0.8579和0.8977，主流架构的平均性能提升了4.18%。

Conclusion: 通过将临床知识与影像特征深度融合，UMCF为精准医疗中的多模态信息融合提供了一条新的技术途径。

Abstract: Brain tumor segmentation requires accurate identification of hierarchical
regions including whole tumor (WT), tumor core (TC), and enhancing tumor (ET)
from multi-sequence magnetic resonance imaging (MRI) images. Due to tumor
tissue heterogeneity, ambiguous boundaries, and contrast variations across MRI
sequences, methods relying solely on visual information or post-hoc loss
constraints show unstable performance in boundary delineation and hierarchy
preservation. To address this challenge, we propose the Unified Multimodal
Coherent Field (UMCF) method. This method achieves synchronous interactive
fusion of visual, semantic, and spatial information within a unified 3D latent
space, adaptively adjusting modal contributions through parameter-free
uncertainty gating, with medical prior knowledge directly participating in
attention computation, avoiding the traditional "process-then-concatenate"
separated architecture. On Brain Tumor Segmentation (BraTS) 2020 and 2021
datasets, UMCF+nnU-Net achieves average Dice coefficients of 0.8579 and 0.8977
respectively, with an average 4.18% improvement across mainstream
architectures. By deeply integrating clinical knowledge with imaging features,
UMCF provides a new technical pathway for multimodal information fusion in
precision medicine.

</details>


### [227] [Chat-CBM: Towards Interactive Concept Bottleneck Models with Frozen Large Language Models](https://arxiv.org/abs/2509.17522)
*Hangzhou He,Lei Zhu,Kaiwen Li,Xinliang Zhang,Jiakui Hu,Ourui Fu,Zhengjian Yao,Yanye Lu*

Main category: cs.CV

TL;DR: Chat-CBM 引入了一种基于语言的分类器来替代传统概念瓶颈模型 (CBMs) 中的分数分类器，从而在保持可解释性的同时，实现了更丰富、更直观的用户干预，并提高了预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统 CBMs 通常使用固定的线性分类器，限制了用户干预（仅限于手动数值调整），并且无法在测试时融入新概念或领域知识。这些限制在非监督 CBMs 中尤为突出，因为概念激活通常嘈杂且密集，导致用户干预无效。

Method: Chat-CBM 将传统的基于分数的分类器替换为基于语言的分类器，该分类器直接基于概念语义进行推理。它利用冻结的大型语言模型 (LLMs) 的语言理解和少样本学习能力，将预测建立在概念的语义空间中，从而实现概念修正、添加/删除概念、融入外部知识和高层推理指导等更丰富的干预方式。

Result: 在九个数据集上的实验表明，Chat-CBM 实现了更高的预测性能，并显著改善了用户交互性，同时保持了 CBMs 的基于概念的可解释性。即使在非监督设置下，Chat-CBM 也能保持有效。

Conclusion: Chat-CBM 通过引入基于语言的分类器，扩展了 CBMs 的干预界面，超越了数值编辑，提供了更灵活、更直观的用户交互方式。它在保持可解释性的同时提高了预测性能，尤其在非监督场景中表现出色，能够有效融入用户知识和推理指导。

Abstract: Concept Bottleneck Models (CBMs) provide inherent interpretability by first
predicting a set of human-understandable concepts and then mapping them to
labels through a simple classifier. While users can intervene in the concept
space to improve predictions, traditional CBMs typically employ a fixed linear
classifier over concept scores, which restricts interventions to manual value
adjustments and prevents the incorporation of new concepts or domain knowledge
at test time. These limitations are particularly severe in unsupervised CBMs,
where concept activations are often noisy and densely activated, making user
interventions ineffective. We introduce Chat-CBM, which replaces score-based
classifiers with a language-based classifier that reasons directly over concept
semantics. By grounding prediction in the semantic space of concepts, Chat-CBM
preserves the interpretability of CBMs while enabling richer and more intuitive
interventions, such as concept correction, addition or removal of concepts,
incorporation of external knowledge, and high-level reasoning guidance.
Leveraging the language understanding and few-shot capabilities of frozen large
language models, Chat-CBM extends the intervention interface of CBMs beyond
numerical editing and remains effective even in unsupervised settings.
Experiments on nine datasets demonstrate that Chat-CBM achieves higher
predictive performance and substantially improves user interactivity while
maintaining the concept-based interpretability of CBMs.

</details>


### [228] [SimToken: A Simple Baseline for Referring Audio-Visual Segmentation](https://arxiv.org/abs/2509.17537)
*Dian Jin,Yanghao Zhou,Jinxing Zhou,Jiaqi Ma,Ruohao Guo,Dan Guo*

Main category: cs.CV

TL;DR: 本文提出SimToken框架，将多模态大语言模型（MLLM）与Segment Anything Model（SAM）结合，通过MLLM生成一个代表指代对象的语义token作为SAM的提示，并引入目标一致语义对齐损失，在Ref-AVS任务上取得了优越性能。


<details>
  <summary>Details</summary>
Motivation: 指代音视频分割（Ref-AVS）任务涉及音频、视觉和文本信息，需要跨模态推理和细粒度对象定位，面临显著挑战。

Method: 该研究提出了SimToken框架，其核心方法包括：1) 将多模态大语言模型（MLLM）与Segment Anything Model（SAM）集成；2) 引导MLLM生成一个特殊的语义token来表示指代对象，该token作为提示引导SAM在视频帧中分割对象；3) 引入一种新颖的目标一致语义对齐损失，以对齐指向同一对象的不同表达所生成的token嵌入，从而提高语义学习。

Result: 在Ref-AVS基准测试中，该方法与现有方法相比，取得了卓越的性能。

Conclusion: 所提出的SimToken框架通过整合MLLM和SAM，并利用生成的语义token作为提示以及新颖的语义对齐损失，有效解决了Ref-AVS任务中的跨模态推理和细粒度定位挑战，实现了最先进的性能。

Abstract: Referring Audio-Visual Segmentation (Ref-AVS) aims to segment specific
objects in videos based on natural language expressions involving audio,
vision, and text information. This task poses significant challenges in
cross-modal reasoning and fine-grained object localization. In this paper, we
propose a simple framework, SimToken, that integrates a multimodal large
language model (MLLM) with the Segment Anything Model (SAM). The MLLM is guided
to generate a special semantic token representing the referred object. This
compact token, enriched with contextual information from all modalities, acts
as a prompt to guide SAM to segment objectsacross video frames. To further
improve semantic learning, we introduce a novel target-consistent semantic
alignment loss that aligns token embeddings from different expressions but
referring to the same object. Experiments on the Ref-AVS benchmark demonstrate
that our approach achieves superior performance compared to existing
methods.Code will be available at https://github.com/DianJin-HFUT/SimToken

</details>


### [229] [WISE: Weak-Supervision-Guided Step-by-Step Explanations for Multimodal LLMs in Image Classification](https://arxiv.org/abs/2509.17740)
*Yiwen Jiang,Deval Mehta,Siyuan Yan,Yaling Shen,Zimu Wang,Zongyuan Ge*

Main category: cs.CV

TL;DR: 多模态大语言模型（MLLMs）的现有MCoT方法依赖大量标注数据且忽视对象内部理解。本文提出WISE，一种弱监督方法，将CBM概念表示转化为解释链，为任意图像分类数据集生成MCoT，从而显著提高可解释性（37%）和分类准确性，弥合了概念可解释性与生成式MCoT推理之间的鸿沟。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在视觉-文本推理方面表现出巨大潜力，多模态思维链（MCoT）提示能显著增强其可解释性。然而，现有的MCoT方法过度依赖富含理由的数据集，且主要关注对象间推理，忽视了对图像分类至关重要的对象内部理解。

Method: 本文提出WISE（Weak-supervision-guided Step-by-step Explanation method），一种弱监督引导的逐步解释方法。该方法通过将概念瓶颈模型（CBMs）中的概念表示重构为简洁、可解释的推理链，以弱监督方式为任意图像分类数据集生成MCoTs，从而增强其解释能力。

Result: 在十个数据集上进行的实验表明，WISE生成的MCoTs不仅将可解释性提高了37%，而且在用于微调MLLMs时，还能提升分类准确性。

Conclusion: 该工作弥合了基于概念的可解释性与生成式MCoT推理之间的鸿沟，为增强MLLMs在细粒度视觉理解方面的能力提供了一个通用且可泛化的框架。

Abstract: Multimodal Large Language Models (MLLMs) have shown promise in visual-textual
reasoning, with Multimodal Chain-of-Thought (MCoT) prompting significantly
enhancing interpretability. However, existing MCoT methods rely on
rationale-rich datasets and largely focus on inter-object reasoning,
overlooking the intra-object understanding crucial for image classification. To
address this gap, we propose WISE, a Weak-supervision-guided Step-by-step
Explanation method that augments any image classification dataset with MCoTs by
reformulating the concept-based representations from Concept Bottleneck Models
(CBMs) into concise, interpretable reasoning chains under weak supervision.
Experiments across ten datasets show that our generated MCoTs not only improve
interpretability by 37% but also lead to gains in classification accuracy when
used to fine-tune MLLMs. Our work bridges concept-based interpretability and
generative MCoT reasoning, providing a generalizable framework for enhancing
MLLMs in fine-grained visual understanding.

</details>


### [230] [Visual Instruction Pretraining for Domain-Specific Foundation Models](https://arxiv.org/abs/2509.17562)
*Yuxuan Li,Yicheng Zhang,Wenhao Tang,Yimian Dai,Ming-Ming Cheng,Xiang Li,Jian Yang*

Main category: cs.CV

TL;DR: 本文提出了一种名为ViTP（Visual insTruction Pretraining）的新范式，通过利用高层推理来增强低层感知特征学习，弥补了计算机视觉闭环中的缺失环节。ViTP将ViT骨干嵌入到视觉-语言模型中，并使用特定领域视觉指令数据进行端到端预训练，在遥感和医学影像等下游任务中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现代计算机视觉的感知、推理和生成之间存在一个相互强化的闭环，但该闭环不完整，特别是高层推理对低层感知特征基础学习的自上而下影响尚未得到充分探索。本文旨在解决这一空白。

Method: 本文提出了视觉指令预训练（ViTP）方法。它将Vision Transformer（ViT）骨干嵌入到视觉-语言模型中，并使用从目标下游领域整理的丰富视觉指令数据进行端到端预训练。ViTP由所提出的视觉鲁棒性学习（VRL）提供支持，该学习促使ViT从稀疏的视觉token中学习到鲁棒且与领域相关的特征。

Result: 在16个具有挑战性的遥感和医学影像基准测试上进行的广泛实验表明，ViTP在各种下游任务中都建立了新的最先进性能。

Conclusion: ViTP成功地利用推理来增强感知，填补了计算机视觉闭环中的一个重要空白，并在具有挑战性的下游领域（如遥感和医学影像）中取得了卓越的性能。

Abstract: Modern computer vision is converging on a closed loop in which perception,
reasoning and generation mutually reinforce each other. However, this loop
remains incomplete: the top-down influence of high-level reasoning on the
foundational learning of low-level perceptual features is not yet
underexplored. This paper addresses this gap by proposing a new paradigm for
pretraining foundation models in downstream domains. We introduce Visual
insTruction Pretraining (ViTP), a novel approach that directly leverages
reasoning to enhance perception. ViTP embeds a Vision Transformer (ViT)
backbone within a Vision-Language Model and pretrains it end-to-end using a
rich corpus of visual instruction data curated from target downstream domains.
ViTP is powered by our proposed Visual Robustness Learning (VRL), which compels
the ViT to learn robust and domain-relevant features from a sparse set of
visual tokens. Extensive experiments on 16 challenging remote sensing and
medical imaging benchmarks demonstrate that ViTP establishes new
state-of-the-art performance across a diverse range of downstream tasks. The
code is available at github.com/zcablii/ViTP.

</details>


### [231] [PRNU-Bench: A Novel Benchmark and Model for PRNU-Based Camera Identification](https://arxiv.org/abs/2509.17581)
*Florinel Alin Croitoru,Vlad Hondru,Radu Tudor Ionescu*

Main category: cs.CV

TL;DR: 该论文提出了一个用于相机识别的新型基准数据集和一种基于PRNU的混合架构模型，实现了显著优于现有技术的识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有的相机识别方法缺乏“野外”环境下的评估，且需要更高效、准确的PRNU信号估计和相机身份验证模型。

Method: 1. 构建了一个包含1.3万张照片（来自120多台相机）的基准数据集，训练和测试照片取自不同场景，以实现“野外”评估。2. 提出了一种新颖的PRNU相机识别模型，采用混合架构：使用去噪自编码器估计PRNU信号，并使用卷积网络进行1:N相机设备验证。3. 模型输入采用参考和查询PRNU信号的Hadamard积，而非传统的对比学习方法。

Result: 与基于去噪自编码器和对比学习的现有最先进模型相比，所提出的方法取得了显著更好的结果。

Conclusion: 该论文通过引入新的基准数据集和创新的PRNU相机识别模型，显著提升了相机识别的性能，特别是在“野外”评估场景下。

Abstract: We propose a novel benchmark for camera identification via Photo Response
Non-Uniformity (PRNU) estimation. The benchmark comprises 13K photos taken with
120+ cameras, where the training and test photos are taken in different
scenarios, enabling ``in-the-wild'' evaluation. In addition, we propose a novel
PRNU-based camera identification model that employs a hybrid architecture,
comprising a denoising autoencoder to estimate the PRNU signal and a
convolutional network that can perform 1:N verification of camera devices.
Instead of using a conventional approach based on contrastive learning, our
method takes the Hadamard product between reference and query PRNU signals as
input. This novel design leads to significantly better results compared with
state-of-the-art models based on denoising autoencoders and contrastive
learning. We release our dataset and code at:
https://github.com/CroitoruAlin/PRNU-Bench.

</details>


### [232] [Domain Adaptive Object Detection for Space Applications with Real-Time Constraints](https://arxiv.org/abs/2509.17593)
*Samet Hicsonmez,Abd El Rahman Shabayek,Arunkumar Rathinam,Djamila Aouada*

Main category: cs.CV

TL;DR: 本文提出了一种监督域适应（SDA）方法，通过少量真实世界标注数据，显著缩小了空间目标检测中合成数据与真实数据之间的域差距，在轻量级检测器上实现了高达20点的平均精度提升。


<details>
  <summary>Details</summary>
Motivation: 空间目标检测模型通常在合成数据上训练，但在真实数据上性能显著下降，原因是存在域差距。然而，域适应目标检测在社区中是一个被忽视的问题。

Method: 该研究首先强调了域适应的重要性，然后探索了监督域适应（SDA）来缩小域差距，仅使用了少量带标签的真实数据。该方法基于一种半监督适应方法并针对目标检测进行了调整，结合了域不变特征学习、基于CNN的域判别器以及使用域无关回归头的风险不变最小化。为满足实时部署需求，该方法在带有MobileNet骨干的轻量级SSD和带有ResNet-50骨干的FCOS上进行了测试，并在SPEED+和SPARK两个空间数据集上进行了评估。

Result: 结果显示，仅用250张带标签的真实图像，平均精度（AP）最高提升了20点。

Conclusion: 该研究表明监督域适应在解决空间目标检测中合成与真实数据域差距问题上的重要性和有效性，通过最少的真实世界标签数据即可显著提高模型性能，使其更适合实时部署。

Abstract: Object detection is essential in space applications targeting Space Domain
Awareness and also applications involving relative navigation scenarios.
Current deep learning models for Object Detection in space applications are
often trained on synthetic data from simulators, however, the model performance
drops significantly on real-world data due to the domain gap. However, domain
adaptive object detection is an overlooked problem in the community. In this
work, we first show the importance of domain adaptation and then explore
Supervised Domain Adaptation (SDA) to reduce this gap using minimal labeled
real data. We build on a recent semi-supervised adaptation method and tailor it
for object detection. Our approach combines domain-invariant feature learning
with a CNN-based domain discriminator and invariant risk minimization using a
domain-independent regression head. To meet real-time deployment needs, we test
our method on a lightweight Single Shot Multibox Detector (SSD) with MobileNet
backbone and on the more advanced Fully Convolutional One-Stage object detector
(FCOS) with ResNet-50 backbone. We evaluated on two space datasets, SPEED+ and
SPARK. The results show up to 20-point improvements in average precision (AP)
with just 250 labeled real images.

</details>


### [233] [COLA: Context-aware Language-driven Test-time Adaptation](https://arxiv.org/abs/2509.17598)
*Aiming Zhang,Tianyuan Yu,Liang Bai,Jun Tang,Yanming Guo,Yirun Ruan,Yun Zhou,Zhihe Lu*

Main category: cs.CV

TL;DR: 本文提出了一种名为COLA的新型测试时自适应（TTA）方法，利用预训练的视觉-语言模型（VLM）在没有共享标签的情况下适应多个目标域，并通过上下文感知模块和类别平衡伪标签策略提高性能。


<details>
  <summary>Details</summary>
Motivation: 大多数现有TTA方法要求源域模型和目标域共享相同的标签空间，这极大地限制了其适用性。虽然VLM的零样本性能令人印象深刻，但它们难以有效捕捉目标域的独特属性。

Method: 提出了一种名为COLA（Context-aware Language-driven TTA）的新方法。该方法使用预训练的VLM（例如CLIP），并引入了一个轻量级的上下文感知模块，该模块包含三个关键组件：任务感知适配器、上下文感知单元和残差连接单元，分别用于探索任务特定知识、VLM的域特定知识和VLM的先验知识。该模块可无缝集成到冻结的VLM中。此外，引入了类别平衡伪标签（CBPL）策略来缓解类别不平衡带来的不利影响。

Result: 实验证明了该方法在TTA场景和类别泛化任务中的有效性。

Conclusion: COLA方法通过利用VLM并结合上下文感知模块和类别平衡伪标签策略，成功实现了在没有共享标签的情况下，对多个目标域的测试时自适应，解决了现有方法的局限性，并提高了VLM在特定目标域上的表现，同时保持了最小的努力和参数效率。

Abstract: Test-time adaptation (TTA) has gained increasing popularity due to its
efficacy in addressing ``distribution shift'' issue while simultaneously
protecting data privacy.
  However, most prior methods assume that a paired source domain model and
target domain sharing the same label space coexist, heavily limiting their
applicability.
  In this paper, we investigate a more general source model capable of
adaptation to multiple target domains without needing shared labels.
  This is achieved by using a pre-trained vision-language model (VLM), \egno,
CLIP, that can recognize images through matching with class descriptions.
  While the zero-shot performance of VLMs is impressive, they struggle to
effectively capture the distinctive attributes of a target domain.
  To that end, we propose a novel method -- Context-aware Language-driven TTA
(COLA).
  The proposed method incorporates a lightweight context-aware module that
consists of three key components: a task-aware adapter, a context-aware unit,
and a residual connection unit for exploring task-specific knowledge,
domain-specific knowledge from the VLM and prior knowledge of the VLM,
respectively.
  It is worth noting that the context-aware module can be seamlessly integrated
into a frozen VLM, ensuring both minimal effort and parameter efficiency.
  Additionally, we introduce a Class-Balanced Pseudo-labeling (CBPL) strategy
to mitigate the adverse effects caused by class imbalance.
  We demonstrate the effectiveness of our method not only in TTA scenarios but
also in class generalisation tasks.
  The source code is available at https://github.com/NUDT-Bai-Group/COLA-TTA.

</details>


### [234] [Overview of PlantCLEF 2025: Multi-Species Plant Identification in Vegetation Quadrat Images](https://arxiv.org/abs/2509.17602)
*Giulio Martellucci,Herve Goeau,Pierre Bonnet,Fabrice Vinatier,Alexis Joly*

Main category: cs.CV

TL;DR: 该论文介绍了PlantCLEF 2025挑战赛，旨在利用AI加速生态学中样方图像的物种识别，通过多标签分类任务评估AI在该领域的进展。


<details>
  <summary>Details</summary>
Motivation: 样方图像对于生态学研究（如生物多样性评估、长期监测）至关重要，但人工识别物种耗时且限制了研究范围。整合AI可以帮助专家加速盘点并扩大生态研究的空间覆盖。

Method: PlantCLEF 2025挑战赛提供了一个包含2,105张高分辨率、专家标注的多标签样方图像（涵盖约400种物种）的新测试集，以及一个包含140万张单标签植物图像的大型训练集。任务被定义为（弱标签）多标签分类问题，目标是使用单标签训练数据预测样方图像中存在的所有物种，并提供了预训练的视觉Transformer模型。

Result: 该论文详细描述了挑战赛的数据集、评估方法、参与者使用的方法和模型，以及最终取得的成果。

Conclusion: 通过PlantCLEF 2025挑战赛，该研究旨在评估和推动AI在生态学样方图像物种识别方面的进步，以期加速生态调查并扩大其规模。

Abstract: Quadrat images are essential for ecological studies, as they enable
standardized sampling, the assessment of plant biodiversity, long-term
monitoring, and large-scale field campaigns. These images typically cover an
area of fifty centimetres or one square meter, and botanists carefully identify
all the species present. Integrating AI could help specialists accelerate their
inventories and expand the spatial coverage of ecological studies. To assess
progress in this area, the PlantCLEF 2025 challenge relies on a new test set of
2,105 high-resolution multi-label images annotated by experts and covering
around 400 species. It also provides a large training set of 1.4 million
individual plant images, along with vision transformer models pre-trained on
this data. The task is formulated as a (weakly labelled) multi-label
classification problem, where the goal is to predict all species present in a
quadrat image using single-label training data. This paper provides a detailed
description of the data, the evaluation methodology, the methods and models
used by participants, and the results achieved.

</details>


### [235] [From Benchmarks to Reality: Advancing Visual Anomaly Detection by the VAND 3.0 Challenge](https://arxiv.org/abs/2509.17615)
*Lars Heckler-Kram,Ashwin Vaidya,Jan-Hendrik Neudeck,Ulla Scheler,Dick Ameln,Samet Akcay,Paula Ramos*

Main category: cs.CV

TL;DR: VAND 3.0挑战赛旨在展示视觉异常检测在实际应用中的最新进展，通过两个赛道（应对分布偏移的鲁棒性与少样本视觉语言模型能力）评估了现有方法。结果显示，大型预训练模型显著提升了性能，但未来的研究需关注效率以满足实时和计算限制。


<details>
  <summary>Details</summary>
Motivation: 视觉异常检测是一个由应用驱动的研究领域，学术界与工业界的联系至关重要。本次挑战赛旨在展示当前异常检测在不同实际环境中的进展，并解决该领域的关键问题。

Method: VAND 3.0挑战赛设置了两个赛道：1) 旨在开发对真实世界分布偏移具有鲁棒性的异常检测方法；2) 探索少样本视觉语言模型的能力。参赛者通过结合或调整现有方法并融入新颖流程来提出解决方案。

Result: 参赛者的解决方案在现有基线上取得了显著改进。大型预训练视觉（语言）骨干模型在两个赛道中都对性能提升起到了关键作用。

Conclusion: 尽管大型预训练模型在性能提升方面发挥了关键作用，但未来的研究需要更有效地扩展异常检测方法，以满足现场的实时和计算限制。

Abstract: Visual anomaly detection is a strongly application-driven field of research.
Consequently, the connection between academia and industry is of paramount
importance. In this regard, we present the VAND 3.0 Challenge to showcase
current progress in anomaly detection across different practical settings
whilst addressing critical issues in the field. The challenge hosted two
tracks, fostering the development of anomaly detection methods robust against
real-world distribution shifts (Category 1) and exploring the capabilities of
Vision Language Models within the few-shot regime (Category 2), respectively.
The participants' solutions reached significant improvements over previous
baselines by combining or adapting existing approaches and fusing them with
novel pipelines. While for both tracks the progress in large pre-trained vision
(language) backbones played a pivotal role for the performance increase,
scaling up anomaly detection methods more efficiently needs to be addressed by
future research to meet real-time and computational constraints on-site.

</details>


### [236] [Tensor-Based Self-Calibration of Cameras via the TrifocalCalib Method](https://arxiv.org/abs/2509.17620)
*Gregory Schroeder,Mohamed Sabry,Cristina Olaverri-Monreal*

Main category: cs.CV

TL;DR: 本文提出了一种名为TrifocalCalib的方法，利用校准过的三焦张量和最少图像数据，实现了无需场景先验知识的投影相机自校准，能够同时估计焦距和主点，且无需校准目标或限制相机运动。


<details>
  <summary>Details</summary>
Motivation: 在计算机视觉中，无需场景先验知识估计相机内参是一个基本挑战，尤其在自动驾驶和车辆编队等应用中，预校准不切实际且需要实时适应性。

Method: 该方法（TrifocalCalib）基于校准过的三焦张量，提出了一组方程，实现了从最少图像数据进行投影相机自校准。它无需校准目标，不限制相机运动，并能同时估计焦距和主点。

Result: 与最近的基于学习和经典方法相比，TrifocalCalib显著提高了准确性和鲁棒性。在程序生成的合成环境和基于结构化数据集的场景中都验证了其有效性。

Conclusion: TrifocalCalib为相机自校准提供了一种准确且鲁棒的解决方案，能够在没有场景先验知识的情况下估计相机内参，适用于需要实时适应性且无法预校准的应用场景。

Abstract: Estimating camera intrinsic parameters without prior scene knowledge is a
fundamental challenge in computer vision. This capability is particularly
important for applications such as autonomous driving and vehicle platooning,
where precalibrated setups are impractical and real-time adaptability is
necessary. To advance the state-of-the-art, we present a set of equations based
on the calibrated trifocal tensor, enabling projective camera self-calibration
from minimal image data. Our method, termed TrifocalCalib, significantly
improves accuracy and robustness compared to both recent learning-based and
classical approaches. Unlike many existing techniques, our approach requires no
calibration target, imposes no constraints on camera motion, and simultaneously
estimates both focal length and principal point. Evaluations in both
procedurally generated synthetic environments and structured dataset-based
scenarios demonstrate the effectiveness of our approach. To support
reproducibility, we make the code publicly available.

</details>


### [237] [Overview of PlantCLEF 2023: Image-based Plant Identification at Global Scale](https://arxiv.org/abs/2509.17622)
*Herve Goeau,Pierre Bonnet,Alexis Joly*

Main category: cs.CV

TL;DR: 本文概述了PlantCLEF2023挑战赛，该挑战赛旨在通过深度学习技术解决全球8万种植物物种的多图像自动识别问题，并总结了参赛团队的方法和关键发现。


<details>
  <summary>Details</summary>
Motivation: 面对生物多样性危机，扩大对植物物种的理解对人类文明至关重要（如农业、建筑、药典）。然而，人工植物识别耗时费力，阻碍了数据积累。尽管深度学习在自动识别方面取得进展，但仍面临类别繁多、数据不平衡、错误识别、重复、视觉质量差异大等数据挑战。

Method: 该研究主要通过PlantCLEF2023挑战赛进行，该挑战赛聚焦于使用深度学习技术解决包含80,000种植物物种的多图像（和元数据）分类问题。本文则概述了挑战赛的资源和评估方法，并总结了参赛研究团队所采用的方法和系统。

Result: 本文分析了PlantCLEF2023挑战赛的关键发现，并总结了参与研究团队所使用的方法和系统。抽象指出深度学习方法已达到一定成熟度，有望在不久的将来实现全球所有植物物种的准确识别系统。

Conclusion: 深度学习在植物自动识别方面前景广阔，尽管存在数据挑战，但已达到足以寄予希望的成熟水平。PlantCLEF2023等挑战赛对实现全球植物识别系统做出了重要贡献，本文通过分析挑战赛成果，展示了该领域的最新进展和关键发现。

Abstract: The world is estimated to be home to over 300,000 species of vascular plants.
In the face of the ongoing biodiversity crisis, expanding our understanding of
these species is crucial for the advancement of human civilization,
encompassing areas such as agriculture, construction, and pharmacopoeia.
However, the labor-intensive process of plant identification undertaken by
human experts poses a significant obstacle to the accumulation of new data and
knowledge. Fortunately, recent advancements in automatic identification,
particularly through the application of deep learning techniques, have shown
promising progress. Despite challenges posed by data-related issues such as a
vast number of classes, imbalanced class distribution, erroneous
identifications, duplications, variable visual quality, and diverse visual
contents (such as photos or herbarium sheets), deep learning approaches have
reached a level of maturity which gives us hope that in the near future we will
have an identification system capable of accurately identifying all plant
species worldwide. The PlantCLEF2023 challenge aims to contribute to this
pursuit by addressing a multi-image (and metadata) classification problem
involving an extensive set of classes (80,000 plant species). This paper
provides an overview of the challenge's resources and evaluations, summarizes
the methods and systems employed by participating research groups, and presents
an analysis of key findings.

</details>


### [238] [OmniInsert: Mask-Free Video Insertion of Any Reference via Diffusion Transformer Models](https://arxiv.org/abs/2509.17627)
*Jinshu Chen,Xinghui Li,Xu Bai,Tianxiang Ma,Pengze Zhang,Zhuowei Chen,Gen Li,Lijie Liu,Songtao Zhao,Bingchuan Li,Qian He*

Main category: cs.CV

TL;DR: 本文提出了一种名为OmniInsert的无掩码视频插入框架，通过解决数据稀缺、主体-场景平衡和插入协调性等关键挑战，实现了从单或多主体参考进行高质量视频插入，并超越了现有商业解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的视频插入方法依赖复杂的控制信号，且难以保持主体一致性，限制了实际应用。具体挑战包括数据稀缺、主体-场景平衡以及插入协调性。

Method: 1. **数据管道：** 提出InsertPipe自动构建多样化的交叉配对数据，解决数据稀缺问题。2. **统一框架：** 开发OmniInsert，用于单或多主体参考的无掩码视频插入。3. **主体-场景平衡：** 引入条件特定特征注入机制和渐进式训练策略。4. **细节增强：** 设计主体聚焦损失（Subject-Focused Loss）。5. **插入协调性：** 提出插入偏好优化方法和情境感知重构模块（Context-Aware Rephraser）。6. **基准：** 创建InsertBench，一个包含多样场景和精心选择主体的综合基准。

Result: 在InsertBench上的评估表明，OmniInsert的性能优于现有最先进的闭源商业解决方案。

Conclusion: OmniInsert框架通过其创新的数据管道、训练策略和优化方法，有效解决了无掩码视频插入中的核心挑战，实现了卓越的插入效果和协调性。

Abstract: Recent advances in video insertion based on diffusion models are impressive.
However, existing methods rely on complex control signals but struggle with
subject consistency, limiting their practical applicability. In this paper, we
focus on the task of Mask-free Video Insertion and aim to resolve three key
challenges: data scarcity, subject-scene equilibrium, and insertion
harmonization. To address the data scarcity, we propose a new data pipeline
InsertPipe, constructing diverse cross-pair data automatically. Building upon
our data pipeline, we develop OmniInsert, a novel unified framework for
mask-free video insertion from both single and multiple subject references.
Specifically, to maintain subject-scene equilibrium, we introduce a simple yet
effective Condition-Specific Feature Injection mechanism to distinctly inject
multi-source conditions and propose a novel Progressive Training strategy that
enables the model to balance feature injection from subjects and source video.
Meanwhile, we design the Subject-Focused Loss to improve the detailed
appearance of the subjects. To further enhance insertion harmonization, we
propose an Insertive Preference Optimization methodology to optimize the model
by simulating human preferences, and incorporate a Context-Aware Rephraser
module during reference to seamlessly integrate the subject into the original
scenes. To address the lack of a benchmark for the field, we introduce
InsertBench, a comprehensive benchmark comprising diverse scenes with
meticulously selected subjects. Evaluation on InsertBench indicates OmniInsert
outperforms state-of-the-art closed-source commercial solutions. The code will
be released.

</details>


### [239] [Overview of PlantCLEF 2022: Image-based plant identification at global scale](https://arxiv.org/abs/2509.17632)
*Herve Goeau,Pierre Bonnet,Alexis Joly*

Main category: cs.CV

TL;DR: PlantCLEF2022挑战赛旨在通过深度学习解决8万种植物的多图像（和元数据）自动识别问题，本论文总结了挑战赛的资源、评估、参与团队的方法和主要发现。


<details>
  <summary>Details</summary>
Motivation: 全球有超过30万种维管植物，了解它们对人类文明至关重要，尤其是在生物多样性危机背景下。然而，人类专家进行系统性植物识别的负担严重阻碍了新数据和知识的积累。因此，需要利用深度学习等技术实现植物的自动识别。

Method: PlantCLEF2022挑战赛提出了一个多图像（和元数据）分类问题，涉及8万种植物物种。该论文介绍了挑战赛的资源和评估方法，并总结了参与研究团队所采用的方法和系统。

Result: 本论文总结了参与研究团队所采用的方法和系统，并对关键发现进行了分析。具体的结果（如准确率）未在摘要中给出，但表明挑战赛已成功进行并产出分析。

Conclusion: 深度学习技术已足够成熟，可以应对全球植物生物多样性的识别问题，尽管数据存在巨大类别数量、类别不平衡、错误识别、重复、视觉质量和内容多样性等挑战。PlantCLEF2022挑战赛是朝着这一目标迈出的一步。

Abstract: It is estimated that there are more than 300,000 species of vascular plants
in the world. Increasing our knowledge of these species is of paramount
importance for the development of human civilization (agriculture,
construction, pharmacopoeia, etc.), especially in the context of the
biodiversity crisis. However, the burden of systematic plant identification by
human experts strongly penalizes the aggregation of new data and knowledge.
Since then, automatic identification has made considerable progress in recent
years as highlighted during all previous editions of PlantCLEF. Deep learning
techniques now seem mature enough to address the ultimate but realistic problem
of global identification of plant biodiversity in spite of many problems that
the data may present (a huge number of classes, very strongly unbalanced
classes, partially erroneous identifications, duplications, variable visual
quality, diversity of visual contents such as photos or herbarium sheets, etc).
The PlantCLEF2022 challenge edition proposes to take a step in this direction
by tackling a multi-image (and metadata) classification problem with a very
large number of classes (80k plant species). This paper presents the resources
and evaluations of the challenge, summarizes the approaches and systems
employed by the participating research groups, and provides an analysis of key
findings.

</details>


### [240] [Evict3R: Training-Free Token Eviction for Memory-Bounded Streaming Visual Geometry Transformers](https://arxiv.org/abs/2509.17650)
*Soroush Mahdi,Fardin Ayar,Ehsan Javanmardi,Manabu Tsukada,Mahdi Javanmardi*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的推理时令牌逐出策略，用于解决流式视觉Transformer中键值（KV）内存无限增长的问题，该策略能显著减少内存占用，同时保持高精度，使长时序流式推理更实用。


<details>
  <summary>Details</summary>
Motivation: 流式视觉Transformer（如StreamVGGT）在3D感知方面表现出色，但其键值（KV）内存的无限增长限制了可扩展性。

Method: 提出了一种无需训练的、推理时令牌逐出策略。该方法通过丢弃冗余令牌并保留信息最丰富的令牌来限制内存，从而控制内存增长。

Result: 该方法在内存使用上显著减少，而精度损失极小。在7-Scenes长序列数据集上，峰值内存从18.63 GB降至9.39 GB，而精度和完整性仅下降0.003。在严格内存预算下，逐出策略允许更密集的帧采样，从而提高了重建精度。在视频深度估计（Sintel, KITTI）、3D重建（7-Scenes, NRGBD）和相机姿态估计（Sintel, TUM-dynamics）等任务中，该方法以极少的内存消耗，实现了与StreamVGGT相近的性能。

Conclusion: 所提出的令牌逐出策略能有效解决流式视觉Transformer的内存限制问题，使长时序流式推理变得更加实用，同时保持了高水平的感知性能。

Abstract: Streaming visual transformers like StreamVGGT achieve strong 3D perception
but suffer from unbounded growth of key value (KV) memory, which limits
scalability. We propose a training-free, inference-time token eviction policy
that bounds memory by discarding redundant tokens while keeping the most
informative ones. Our method uses significantly less memory with little to no
drop in accuracy: on 7-Scenes with long sequences it reduces peak memory from
18.63 GB to 9.39 GB while accuracy and completeness drop by only 0.003. Under
strict memory budgets, eviction enables denser frame sampling, which improves
reconstruction accuracy compared to the baseline. Experiments across video
depth estimation (Sintel, KITTI), 3D reconstruction (7-Scenes, NRGBD), and
camera pose estimation (Sintel, TUM-dynamics) show that our approach closely
matches StreamVGGT at a fraction of the memory and makes long-horizon streaming
inference more practical.

</details>


### [241] [SISMA: Semantic Face Image Synthesis with Mamba](https://arxiv.org/abs/2509.17651)
*Filippo Botti,Alex Ergasti,Tomaso Fontanini,Claudio Ferrari,Massimo Bertozzi,Andrea Prati*

Main category: cs.CV

TL;DR: 本文提出了一种名为SISMA的新型架构，它基于Mamba模型，用于语义图像合成，实现了高质量的人脸生成，同时显著降低了计算成本和推理时间。


<details>
  <summary>Details</summary>
Motivation: 语义图像合成（SIS）中流行的扩散模型，特别是用于人脸生成时，由于注意力层的二次复杂度，其训练和推理计算成本高昂，对计算资源需求大。

Method: 本文提出了一种名为SISMA的新型架构，它基于最近提出的Mamba模型。SISMA通过语义掩码控制样本形状，生成高质量图像，同时降低计算需求。

Result: 通过在CelebAMask-HQ数据集上的综合实验，SISMA不仅取得了更好的FID分数，而且运行速度是现有最先进架构的三倍。

Conclusion: 所提出的SISMA设计是基于Transformer模型的语义图像合成任务的一种可行且轻量级的替代方案。

Abstract: Diffusion Models have become very popular for Semantic Image Synthesis (SIS)
of human faces. Nevertheless, their training and inference is computationally
expensive and their computational requirements are high due to the quadratic
complexity of attention layers. In this paper, we propose a novel architecture
called SISMA, based on the recently proposed Mamba. SISMA generates high
quality samples by controlling their shape using a semantic mask at a reduced
computational demand. We validated our approach through comprehensive
experiments with CelebAMask-HQ, revealing that our architecture not only
achieves a better FID score yet also operates at three times the speed of
state-of-the-art architectures. This indicates that the proposed design is a
viable, lightweight substitute to transformer-based models.

</details>


### [242] [Clothing agnostic Pre-inpainting Virtual Try-ON](https://arxiv.org/abs/2509.17654)
*Sehyun Kim,Hye Jun Lee,Jiwoo Lee,Taemin Lee*

Main category: cs.CV

TL;DR: 本研究提出了CaP-VTON模型，通过整合多类别遮罩和皮肤修复，解决了现有虚拟试穿模型（如Leffa）在底部检测不准确和服装轮廓残留等问题，显著提升了全身服装合成的自然度和一致性，尤其在短袖合成准确率上表现优异。


<details>
  <summary>Details</summary>
Motivation: 深度学习驱动的虚拟试穿技术在电商、时尚和娱乐领域具有重要应用价值。然而，现有基于扩散的模型（如Leffa）存在纹理失真、底部检测不准确以及合成结果中保留原有服装轮廓等局限性，促使研究者寻求更自然、一致的解决方案。

Method: 本研究提出了CaP-VTON（Clothing agnostic Pre-inpainting Virtual Try-ON）模型。该模型通过集成基于Dress Code的多类别遮罩和基于Stable Diffusion的皮肤修复，提高了全身服装合成的自然度和一致性。特别地，引入了一个“生成皮肤模块”，以解决长袖转换为短袖或无袖时出现的皮肤恢复问题，并考虑人体姿态和颜色实现了高质量修复。这些结构保持了模型无关性。

Result: CaP-VTON在短袖合成准确率方面达到了92.5%，比Leffa提高了15.4%。在视觉评估中，该模型能够一致地再现参考服装的风格和形状。这些结构被证明适用于各种基于扩散的虚拟检测系统。

Conclusion: CaP-VTON成功解决了现有虚拟试穿模型中的关键问题，显著提高了服装合成的自然度和一致性，尤其是在皮肤修复方面表现突出。由于其模型无关性，CaP-VTON可应用于电商、定制造型和虚拟形象创建等需要高精度虚拟穿着的广泛应用中。

Abstract: With the development of deep learning technology, virtual try-on technology
has become an important application value in the fields of e-commerce, fashion,
and entertainment. The recently proposed Leffa has improved the texture
distortion problem of diffu-sion-based models, but there are limitations in
that the bottom detection inaccuracy and the existing clothing silhouette
remain in the synthesis results. To solve this problem, this study proposes
CaP-VTON (Clothing agnostic Pre-inpainting Virtual Try-ON). CaP-VTON has
improved the naturalness and consistency of whole-body clothing syn-thesis by
integrating multi-category masking based on Dress Code and skin inpainting
based on Stable Diffusion. In particular, a generate skin module was introduced
to solve the skin restoration problem that occurs when long-sleeved images are
converted into short-sleeved or sleeveless ones, and high-quality restoration
was implemented consider-ing the human body posture and color. As a result,
CaP-VTON recorded 92.5\%, which is 15.4\% better than Leffa in short-sleeved
synthesis accuracy, and showed the performance of consistently reproducing the
style and shape of reference clothing in visual evaluation. These structures
maintain model-agnostic properties and are applicable to various
diffu-sion-based virtual inspection systems, and can contribute to applications
that require high-precision virtual wearing, such as e-commerce, custom
styling, and avatar creation.

</details>


### [243] [Development and validation of an AI foundation model for endoscopic diagnosis of esophagogastric junction adenocarcinoma: a cohort and deep learning study](https://arxiv.org/abs/2509.17660)
*Yikun Ma,Bo Li,Ying Chen,Zijie Yue,Shuchang Xu,Jingyao Li,Lei Ma,Liang Zhong,Duowu Zou,Leiming Xu,Yunshi Zhong,Xiaobo Li,Weiqun Ding,Minmin Zhang,Dongli He,Zhenghong Li,Ye Chen,Ye Zhao,Jialong Zhuo,Xiaofen Wu,Lisha Yi,Miaojing Shi,Huihui Sun*

Main category: cs.CV

TL;DR: 本文首次尝试开发一种基于AI基础模型的方法，利用内窥镜图像对食管胃结合部腺癌（EGJA）进行筛查和分期诊断，并在准确性和效率上展现出巨大潜力。


<details>
  <summary>Details</summary>
Motivation: 食管胃结合部腺癌（EGJA）的早期发现对改善患者预后至关重要，但目前的诊断高度依赖操作者的经验，存在诊断不一致性。

Method: 研究采用多中心队列学习设计，从中国七家医院收集了12,302张内窥镜图像（来自1,546名患者）。其中8,249张用于模型训练，其余分为保留测试集、外部测试集和前瞻性测试集进行评估。所提出的模型结合DINOv2（视觉基础模型）和ResNet50（卷积神经网络）来提取内窥镜图像的全局外观和局部细节特征，用于EGJA分期诊断。

Result: 该模型在三个测试集上对EGJA分期诊断表现出令人满意的性能，准确率分别达到0.9256、0.8895和0.8956。相比之下，其他代表性AI模型中表现最好的ResNet50在三个测试集上的准确率分别为0.9125、0.8382和0.8519；专家内窥镜医师在保留测试集上的准确率为0.8147。此外，在模型辅助下，实习生、合格和专家内窥镜医师的总体准确率分别从0.7035、0.7350和0.8147提高到0.8497、0.8521和0.8696。

Conclusion: 该模型是首次将基础模型应用于EGJA分期诊断，并在诊断准确性和效率方面展现出巨大潜力。它不仅超越了现有AI模型和人类专家，还能显著提高内窥镜医师的诊断准确性。

Abstract: The early detection of esophagogastric junction adenocarcinoma (EGJA) is
crucial for improving patient prognosis, yet its current diagnosis is highly
operator-dependent. This paper aims to make the first attempt to develop an
artificial intelligence (AI) foundation model-based method for both screening
and staging diagnosis of EGJA using endoscopic images. In this cohort and
learning study, we conducted a multicentre study across seven Chinese hospitals
between December 28, 2016 and December 30, 2024. It comprises 12,302 images
from 1,546 patients; 8,249 of them were employed for model training, while the
remaining were divided into the held-out (112 patients, 914 images), external
(230 patients, 1,539 images), and prospective (198 patients, 1,600 images) test
sets for evaluation. The proposed model employs DINOv2 (a vision foundation
model) and ResNet50 (a convolutional neural network) to extract features of
global appearance and local details of endoscopic images for EGJA staging
diagnosis. Our model demonstrates satisfactory performance for EGJA staging
diagnosis across three test sets, achieving an accuracy of 0.9256, 0.8895, and
0.8956, respectively. In contrast, among representative AI models, the best one
(ResNet50) achieves an accuracy of 0.9125, 0.8382, and 0.8519 on the three test
sets, respectively; the expert endoscopists achieve an accuracy of 0.8147 on
the held-out test set. Moreover, with the assistance of our model, the overall
accuracy for the trainee, competent, and expert endoscopists improves from
0.7035, 0.7350, and 0.8147 to 0.8497, 0.8521, and 0.8696, respectively. To our
knowledge, our model is the first application of foundation models for EGJA
staging diagnosis and demonstrates great potential in both diagnostic accuracy
and efficiency.

</details>


### [244] [Tailored Transformation Invariance for Industrial Anomaly Detection](https://arxiv.org/abs/2509.17670)
*Mariette Schönfeld,Wannes Meert,Hendrik Blockeel*

Main category: cs.CV

TL;DR: 本文提出LWinNN，一种基于局部窗口的工业异常检测方法，在保持计算效率的同时显著提高了准确性，并在平移不变性方面取得了平衡。


<details>
  <summary>Details</summary>
Motivation: 工业异常检测（IAD）在实际应用中日益重要，但现有研究中，虽然提取信息特征的新方法表现优异，但训练成本高昂。传统的kNN方法虽然计算效率高，但在平移不变性方面要么完全不变要么完全不变，缺乏中间地带。作者认为，流行基准测试仅需对微小平移具有鲁棒性。

Method: 基于对现有工作和平移不变性的研究，作者提出了LWinNN（Local Window based Nearest Neighbor），一种基于局部窗口的方法。该方法在kNN方法中引入了平移不变性的中间地带，旨在平衡完全不变性或完全不变性之间的需求，以适应基准测试中对微小平移鲁棒性的要求。

Result: 实验证明，LWinNN这一小改动显著提高了准确性，同时大幅减少了训练和测试时间。

Conclusion: 首先，通过有效利用有限数据，可以缩小基于kNN的方法与更复杂的最新方法之间的差距。其次，仅需有限平移不变性的假设揭示了未来研究的潜在方向，并强调了对更多空间多样性基准测试的需求，LWinNN有望成为此类基准测试的新基线。

Abstract: Industrial Anomaly Detection (IAD) is a subproblem within Computer Vision
Anomaly Detection that has been receiving increasing amounts of attention due
to its applicability to real-life scenarios. Recent research has focused on how
to extract the most informative features, contrasting older kNN-based methods
that use only pretrained features. These recent methods are much more expensive
to train however and could complicate real-life application. Careful study of
related work with regards to transformation invariance leads to the idea that
popular benchmarks require robustness to only minor translations. With this
idea we then formulate LWinNN, a local window based approach that creates a
middle ground between kNN based methods that have either complete or no
translation invariance. Our experiments demonstrate that this small change
increases accuracy considerably, while simultaneously decreasing both train and
test time. This teaches us two things: first, the gap between kNN-based
approaches and more complex state-of-the-art methodology can still be narrowed
by effective usage of the limited data available. Second, our assumption of
requiring only limited translation invariance highlights potential areas of
interest for future work and the need for more spatially diverse benchmarks,
for which our method can hopefully serve as a new baseline. Our code can be
found at https://github.com/marietteschonfeld/LWinNN .

</details>


### [245] [FROQ: Observing Face Recognition Models for Efficient Quality Assessment](https://arxiv.org/abs/2509.17689)
*Žiga Babnik,Deepak Kumar Jain,Peter Peer,Vitomir Štruc*

Main category: cs.CV

TL;DR: 本文提出FROQ，一种半监督、免训练的人脸图像质量评估(FIQA)方法，它利用现有FR模型的中间表示来评估质量，结合了有监督方法的效率和无监督方法的免训练特性，并取得了与现有技术相当的性能。


<details>
  <summary>Details</summary>
Motivation: 人脸识别(FR)在关键应用中至关重要，识别错误可能导致严重后果。FIQA技术通过评估人脸样本质量来增强FR系统，但现有技术要么需要大量监督训练（准确但复杂），要么是无监督（较慢且性能较低）。因此，需要一种既高效又准确，且无需额外训练的FIQA方法。

Method: 本文介绍了FROQ（人脸识别质量观察器），这是一种半监督、免训练的方法。它利用给定FR模型中的特定中间表示来估计人脸图像质量。FROQ通过基于伪质量标签的简单校准步骤，揭示FR模型中对质量评估有用的表示。这些伪标签是通过一种基于样本扰动的新型无监督FIQA技术生成的。

Result: 通过对四种最先进的FR模型和八个基准数据集进行的全面实验表明，FROQ与现有技术相比，取得了极具竞争力的结果。它在无需显式训练的情况下，实现了强大的性能和高效的运行时间。

Conclusion: FROQ提供了一种创新性的半监督、免训练FIQA方法，它能有效利用现有FR模型的内部机制，在性能和效率上均与现有最先进技术相媲美，为关键应用中的人脸识别系统提供了可靠的质量评估方案。

Abstract: Face Recognition (FR) plays a crucial role in many critical (high-stakes)
applications, where errors in the recognition process can lead to serious
consequences. Face Image Quality Assessment (FIQA) techniques enhance FR
systems by providing quality estimates of face samples, enabling the systems to
discard samples that are unsuitable for reliable recognition or lead to
low-confidence recognition decisions. Most state-of-the-art FIQA techniques
rely on extensive supervised training to achieve accurate quality estimation.
In contrast, unsupervised techniques eliminate the need for additional training
but tend to be slower and typically exhibit lower performance. In this paper,
we introduce FROQ (Face Recognition Observer of Quality), a semi-supervised,
training-free approach that leverages specific intermediate representations
within a given FR model to estimate face-image quality, and combines the
efficiency of supervised FIQA models with the training-free approach of
unsupervised methods. A simple calibration step based on pseudo-quality labels
allows FROQ to uncover specific representations, useful for quality assessment,
in any modern FR model. To generate these pseudo-labels, we propose a novel
unsupervised FIQA technique based on sample perturbations. Comprehensive
experiments with four state-of-the-art FR models and eight benchmark datasets
show that FROQ leads to highly competitive results compared to the
state-of-the-art, achieving both strong performance and efficient runtime,
without requiring explicit training.

</details>


### [246] [Depth Edge Alignment Loss: DEALing with Depth in Weakly Supervised Semantic Segmentation](https://arxiv.org/abs/2509.17702)
*Patrick Schmidt,Vasileios Belagiannis,Lazaros Nalpantidis*

Main category: cs.CV

TL;DR: 本研究提出了一种模型无关的深度边缘对齐损失，通过结合像素级深度信息和图像级监督，显著提升了弱监督语义分割模型的性能，减少了对昂贵像素级标签的依赖。


<details>
  <summary>Details</summary>
Motivation: 在新的领域中应用自主机器人系统时，训练鲁棒的语义分割模型需要大量的、昂贵的像素级密集标签，这限制了其应用。因此，需要一种方法来减少对这些昂贵标签的依赖。

Method: 本研究提出了一种模型无关的“深度边缘对齐损失”（Depth Edge Alignment Loss）。该方法通过图像级监督生成像素级语义标签，并利用机器人系统中普遍存在的像素级深度信息作为额外的监督信号，以避免昂贵的标注过程。该损失可以与现有的弱监督语义分割模型和损失函数结合使用。

Result: 该方法在不同数据集和模型上均能提升分割性能。具体来说，在PASCAL VOC / MS COCO验证集和HOPE静态登机分割集上，平均交并比（mIoU）分别提升了高达+5.439、+1.274和+16.416点。同时，该方法还可以与其他损失结合以取得更好的性能。

Conclusion: 结合像素级深度信息可以显著提高弱监督语义分割模型的性能，从而减少对昂贵像素级标签的需求。这为在自主机器人系统等领域应用语义分割提供了更经济有效的方法。

Abstract: Autonomous robotic systems applied to new domains require an abundance of
expensive, pixel-level dense labels to train robust semantic segmentation
models under full supervision. This study proposes a model-agnostic Depth Edge
Alignment Loss to improve Weakly Supervised Semantic Segmentation models across
different datasets. The methodology generates pixel-level semantic labels from
image-level supervision, avoiding expensive annotation processes. While weak
supervision is widely explored in traditional computer vision, our approach
adds supervision with pixel-level depth information, a modality commonly
available in robotic systems. We demonstrate how our approach improves
segmentation performance across datasets and models, but can also be combined
with other losses for even better performance, with improvements up to +5.439,
+1.274 and +16.416 points in mean Intersection over Union on the PASCAL VOC /
MS COCO validation, and the HOPE static onboarding split, respectively. Our
code will be made publicly available.

</details>


### [247] [Neurodynamics-Driven Coupled Neural P Systems for Multi-Focus Image Fusion](https://arxiv.org/abs/2509.17704)
*Bo Li,Yunkuo Lei,Tingting Bao,Yaxian Wang,Lingling Zhang,Jun Liu*

Main category: cs.CV

TL;DR: 本文提出了一种神经动力学驱动的耦合神经P系统（ND-CNPFuse），通过将源图像映射到可解释的脉冲矩阵并比较脉冲数量，直接生成高质量的决策图，从而在多焦点图像融合任务中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 多焦点图像融合（MFIF）的关键挑战在于生成边界精确的决策图。传统的启发式方法和深度学习的黑盒机制难以生成高质量的决策图。

Method: 引入了神经动力学驱动的耦合神经P（CNP）系统，这是一种受脉冲机制启发的第三代神经计算模型。通过深入分析模型的神经动力学，识别网络参数与输入信号之间的约束，以避免神经元异常持续放电，确保模型准确区分聚焦和非聚焦区域。在此基础上，提出了ND-CNPFuse模型，它通过将源图像映射到可解释的脉冲矩阵，并通过比较脉冲数量直接生成决策图，无需任何后处理。

Result: ND-CNPFuse在Lytro、MFFW、MFI-WHU和Real-MFF四个经典MFIF数据集上取得了新的最先进性能。

Conclusion: 神经动力学驱动的CNP系统（ND-CNPFuse）通过其独特的脉冲矩阵映射和脉冲数量比较机制，有效解决了多焦点图像融合中决策图精度低的问题，实现了卓越的融合效果。

Abstract: Multi-focus image fusion (MFIF) is a crucial technique in image processing,
with a key challenge being the generation of decision maps with precise
boundaries. However, traditional methods based on heuristic rules and deep
learning methods with black-box mechanisms are difficult to generate
high-quality decision maps. To overcome this challenge, we introduce
neurodynamics-driven coupled neural P (CNP) systems, which are third-generation
neural computation models inspired by spiking mechanisms, to enhance the
accuracy of decision maps. Specifically, we first conduct an in-depth analysis
of the model's neurodynamics to identify the constraints between the network
parameters and the input signals. This solid analysis avoids abnormal
continuous firing of neurons and ensures the model accurately distinguishes
between focused and unfocused regions, generating high-quality decision maps
for MFIF. Based on this analysis, we propose a
\textbf{N}eurodynamics-\textbf{D}riven \textbf{CNP} \textbf{F}usion model
(\textbf{ND-CNPFuse}) tailored for the challenging MFIF task. Unlike current
ideas of decision map generation, ND-CNPFuse distinguishes between focused and
unfocused regions by mapping the source image into interpretable spike
matrices. By comparing the number of spikes, an accurate decision map can be
generated directly without any post-processing. Extensive experimental results
show that ND-CNPFuse achieves new state-of-the-art performance on four
classical MFIF datasets, including Lytro, MFFW, MFI-WHU, and Real-MFF. The code
is available at https://github.com/MorvanLi/ND-CNPFuse.

</details>


### [248] [Automatic Intermodal Loading Unit Identification using Computer Vision: A Scoping Review](https://arxiv.org/abs/2509.17707)
*Emre Gülsoylu,Alhassan Abdelhalim,Derya Kara Boztas,Ole Grasse,Carlos Jahn,Simone Frintrop,Janick Edinger*

Main category: cs.CV

TL;DR: 这篇综述回顾了35年来（1990-2025）63项关于计算机视觉（CV）在联运装载单元（ILU）识别中应用的研究，指出该领域从早期图像处理发展到深度学习，并强调了公共数据集缺乏、结果差异大以及场景文本识别和移动相机带来的新挑战。


<details>
  <summary>Details</summary>
Motivation: 联运装载单元（如集装箱）的标准化彻底改变了全球贸易，但其高效、鲁棒的识别在高吞吐量港口和码头仍是一个关键瓶颈。计算机视觉技术为其他识别技术提供了经济有效的替代方案。

Method: 该论文对1990年至2025年间63项关于计算机视觉（CV）在联运装载单元（ILU）识别中应用的实证研究进行了综述，追溯了该领域从数字图像处理和传统机器学习到深度学习技术的主导地位的演变。

Result: 计算机视觉提供了成本效益高的替代方案，但其发展受限于缺乏公开的基准数据集，导致报告结果（如端到端准确率）差异巨大，从5%到96%不等。此外，综述还强调了从基于字符的文本识别转向场景文本识别以及移动相机集成带来的新兴挑战。

Conclusion: 为推动该领域发展，论文呼吁标准化术语、开放获取数据集、共享源代码，并提出了未来的研究方向，如针对ISO6346代码优化的无上下文文本识别。

Abstract: The standardisation of Intermodal Loading Units (ILUs), such as containers,
semi-trailers and swap bodies, has revolutionised global trade yet their
efficient and robust identification remains a critical bottleneck in
high-throughput ports and terminals. This paper reviews 63 empirical studies
that propose computer vision (CV) based solutions. It covers the last 35 years
(1990-2025), tracing the field's evolution from early digital image processing
(DIP) and traditional machine learning (ML) to the current dominance of deep
learning (DL) techniques. While CV offers cost-effective alternatives for other
types of identification techniques, its development is hindered by the lack of
publicly available benchmarking datasets. This results in high variance for the
reported results such as end-to-end accuracy ranging from 5 % to 96 %. Beyond
dataset limitations, this review highlights the emerging challenges especially
introduced by the shift from character-based text recognition to scene-text
spotting and the integration of mobile cameras (e.g. drones, sensor equipped
ground vehicles) for dynamic terminal monitoring. To advance the field, the
paper calls for standardised terminology, open-access datasets, shared source
code, while outlining future research directions such as contextless text
recognition optimised for ISO6346 codes.

</details>


### [249] [RCTDistill: Cross-Modal Knowledge Distillation Framework for Radar-Camera 3D Object Detection with Temporal Fusion](https://arxiv.org/abs/2509.17712)
*Geonho Bang,Minjae Seong,Jisong Kim,Geunju Baek,Daye Oh,Junhyung Kim,Junho Koh,Jun Won Choi*

Main category: cs.CV

TL;DR: 本文提出了一种名为RCTDistill的新型跨模态知识蒸馏方法，结合时间融合，旨在提升雷达-相机融合3D目标检测的性能，并通过解决不确定性问题和优化特征学习，实现了最先进的结果和更快的推理速度。


<details>
  <summary>Details</summary>
Motivation: 雷达-相机融合是3D目标检测的经济有效方法，但性能仍落后于基于激光雷达的方法。现有的时间融合和知识蒸馏策略未能充分考虑由物体运动或传感器自身误差（雷达和相机）引起的不确定性。

Method: 本文提出了RCTDistill，一种基于时间融合的新型跨模态知识蒸馏（KD）方法，包含三个关键模块：1) 距离-方位知识蒸馏（RAKD），用于考虑距离和方位方向的固有误差，通过激光雷达特征精炼不准确的BEV表示。2) 时间知识蒸馏（TKD），通过将历史雷达-相机BEV特征与当前激光雷达表示对齐，缓解动态物体引起的时间错位。3) 区域解耦知识蒸馏（RDKD），通过从教师模型蒸馏关系知识来增强特征辨别能力，帮助学生区分前景和背景特征。

Result: RCTDistill在nuScenes和View-of-Delft (VoD) 数据集上均实现了最先进的雷达-相机融合性能，并拥有26.2 FPS的最快推理速度。

Conclusion: RCTDistill通过有效处理雷达和相机模态固有的不确定性，并利用创新的知识蒸馏模块，显著提升了雷达-相机融合3D目标检测的性能和效率，超越了现有方法。

Abstract: Radar-camera fusion methods have emerged as a cost-effective approach for 3D
object detection but still lag behind LiDAR-based methods in performance.
Recent works have focused on employing temporal fusion and Knowledge
Distillation (KD) strategies to overcome these limitations. However, existing
approaches have not sufficiently accounted for uncertainties arising from
object motion or sensor-specific errors inherent in radar and camera
modalities. In this work, we propose RCTDistill, a novel cross-modal KD method
based on temporal fusion, comprising three key modules: Range-Azimuth Knowledge
Distillation (RAKD), Temporal Knowledge Distillation (TKD), and
Region-Decoupled Knowledge Distillation (RDKD). RAKD is designed to consider
the inherent errors in the range and azimuth directions, enabling effective
knowledge transfer from LiDAR features to refine inaccurate BEV
representations. TKD mitigates temporal misalignment caused by dynamic objects
by aligning historical radar-camera BEV features with current LiDAR
representations. RDKD enhances feature discrimination by distilling relational
knowledge from the teacher model, allowing the student to differentiate
foreground and background features. RCTDistill achieves state-of-the-art
radar-camera fusion performance on both the nuScenes and View-of-Delft (VoD)
datasets, with the fastest inference speed of 26.2 FPS.

</details>


### [250] [Automated Labeling of Intracranial Arteries with Uncertainty Quantification Using Deep Learning](https://arxiv.org/abs/2509.17726)
*Javier Bisbal,Patrick Winter,Sebastian Jofre,Aaron Ponce,Sameer A. Ansari,Ramez Abdalla,Michael Markl,Oliver Welin Odeback,Sergio Uribe,Cristian Tejos,Julio Sotelo,Susanne Schnell,David Marlevi*

Main category: cs.CV

TL;DR: 该研究提出了一个基于深度学习的框架，用于从3D ToF-MRA分割图像中自动标记颅内动脉，并结合了不确定性量化，以提高可解释性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 颅内动脉的准确解剖学标记对于脑血管诊断和血流动力学分析至关重要，但传统方法耗时且存在操作者间差异。

Method: 研究评估了三种卷积神经网络架构（UNet、CS-Net和nnUNet），其中nnUNet表现最佳。为了量化预测不确定性，研究采用了测试时增强（TTA）并引入了一种新的坐标引导策略来减少插值误差。此外，通过比较自动和手动标记的4D Flow MRI数据中的血流速度来验证临床效用。

Result: nnUNet实现了最高的标记性能（平均Dice系数：0.922；平均表面距离：0.387毫米），并在解剖学复杂血管中表现出更高的鲁棒性。生成的不确定性图谱可靠地指示了解剖模糊、病理变异或手动标记不一致的区域。自动和手动标记在血流速度方面表现出高度一致性，无统计学显著差异。

Conclusion: 该框架为自动脑血管标记提供了一个可扩展、准确且具有不确定性意识的解决方案，支持下游血流动力学分析并促进临床整合。

Abstract: Accurate anatomical labeling of intracranial arteries is essential for
cerebrovascular diagnosis and hemodynamic analysis but remains time-consuming
and subject to interoperator variability. We present a deep learning-based
framework for automated artery labeling from 3D Time-of-Flight Magnetic
Resonance Angiography (3D ToF-MRA) segmentations (n=35), incorporating
uncertainty quantification to enhance interpretability and reliability. We
evaluated three convolutional neural network architectures: (1) a UNet with
residual encoder blocks, reflecting commonly used baselines in vascular
labeling; (2) CS-Net, an attention-augmented UNet incorporating channel and
spatial attention mechanisms for enhanced curvilinear structure recognition;
and (3) nnUNet, a self-configuring framework that automates preprocessing,
training, and architectural adaptation based on dataset characteristics. Among
these, nnUNet achieved the highest labeling performance (average Dice score:
0.922; average surface distance: 0.387 mm), with improved robustness in
anatomically complex vessels. To assess predictive confidence, we implemented
test-time augmentation (TTA) and introduced a novel coordinate-guided strategy
to reduce interpolation errors during augmented inference. The resulting
uncertainty maps reliably indicated regions of anatomical ambiguity,
pathological variation, or manual labeling inconsistency. We further validated
clinical utility by comparing flow velocities derived from automated and manual
labels in co-registered 4D Flow MRI datasets, observing close agreement with no
statistically significant differences. Our framework offers a scalable,
accurate, and uncertainty-aware solution for automated cerebrovascular
labeling, supporting downstream hemodynamic analysis and facilitating clinical
integration.

</details>


### [251] [Adaptive Fast-and-Slow Visual Program Reasoning for Long-Form VideoQA](https://arxiv.org/abs/2509.17743)
*Chenglin Li,Feng Han,FengTao,Ruilin Li,Qianglong Chen,Jingqi Tong,Yin Zhang,Jiaqi Wang*

Main category: cs.CV

TL;DR: FS-VisPR是一个自适应的视觉程序推理框架，通过快慢推理平衡简单和困难查询，结合高效视觉模块和参数搜索，提高了长视频问答的效率和可靠性，并在多个基准上超越或匹配了现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型（LLMs）在生成视觉任务程序工作流方面有潜力，但面临依赖闭源模型、缺乏系统推理以及难以处理长视频问答（videoQA）的挑战。

Method: ['设计了高效视觉模块（如关键片段检索和字幕检索）以支持长视频任务。', '构建了多样化、高质量的快慢推理数据集，并利用强大的LLM训练了一个开源语言模型FS-LLM，使其能够生成视觉程序工作流。', '提出了一个快慢推理框架：简单查询由VideoLLM直接解决，而困难查询则启动视觉程序推理；低置信度的快速思考答案会触发第二阶段的慢推理过程，如果程序执行失败则激活回退到快推理的机制。', '通过在训练和推理过程中进行参数搜索来改进视觉程序：训练时选择产生正确答案的程序，推理时应用具有最高置信度结果的程序。']

Result: FS-VisPR显著提高了视觉程序工作流的效率和可靠性。在LVBench上实现了50.4%的准确率，超越了GPT-4o；在VideoMME上匹配了Qwen2.5VL-72B的性能。

Conclusion: FS-VisPR框架通过其自适应的快慢推理机制、高效的视觉模块集成以及创新的程序参数搜索方法，成功解决了长视频问答的现有挑战，显著提升了视觉程序工作流的性能和可靠性。

Abstract: Large language models (LLMs) have shown promise in generating program
workflows for visual tasks. However, previous approaches often rely on
closed-source models, lack systematic reasoning, and struggle with long-form
video question answering (videoQA). To address these challenges, we introduce
the FS-VisPR framework, an adaptive visual program reasoning approach that
balances fast reasoning for simple queries with slow reasoning for difficult
ones. First, we design efficient visual modules (e.g., key clip retrieval and
subtitle retrieval) to support long-form video tasks. Then, we construct a
diverse and high-quality fast-slow reasoning dataset with a strong LLM to align
open-source language models' ability to generate visual program workflows as
FS-LLM. Next, we design a fast-slow reasoning framework with FS-LLM: Simple
queries are directly solved by VideoLLMs, while difficult ones invoke visual
program reasoning, motivated by human-like reasoning processes. During this
process, low-confidence fast-thinking answers will trigger a second-stage
slow-reasoning process, and a fallback mechanism to fast reasoning is activated
if the program execution fails. Moreover, we improve visual programs through
parameter search during both training and inference. By adjusting the
parameters of the visual modules within the program, multiple variants are
generated: during training, programs that yield correct answers are selected,
while during inference, the program with the highest confidence result is
applied. Experiments show that FS-VisPR improves both efficiency and
reliability in visual program workflows. It achieves 50.4% accuracy on LVBench,
surpassing GPT-4o, matching the performance of Qwen2.5VL-72B on VideoMME.

</details>


### [252] [Multi-Agent Amodal Completion: Direct Synthesis with Fine-Grained Semantic Guidance](https://arxiv.org/abs/2509.17757)
*Hongxing Fan,Lipeng Wang,Haohua Chen,Zehuan Huang,Jiangtao Wu,Lu Sheng*

Main category: cs.CV

TL;DR: 本文提出了一种协作多智能体推理框架，结合细粒度语义指导，用于非模态补全，解决了现有方法的数据需求、泛化性和误差累积问题，并实现了最先进的视觉质量。


<details>
  <summary>Details</summary>
Motivation: 非模态补全（生成被遮挡物体不可见部分）对于图像编辑和增强现实至关重要。然而，现有方法在数据需求、泛化能力或渐进式管道中的误差累积方面面临挑战。

Method: 该框架采用协作多智能体推理，智能体协同分析遮挡关系并确定边界扩展以生成精确的修复掩码。同时，另一个智能体生成细粒度文本描述，提供细粒度语义指导，确保准确的物体合成并防止生成不需要的元素。此外，该方法直接生成由可见掩码和扩散Transformer注意力图引导的分层RGBA输出，无需额外分割。

Result: 广泛的评估表明，该框架实现了最先进的视觉质量。

Conclusion: 该协作多智能体推理框架有效解决了非模态补全中的现有问题，并通过细粒度语义指导提高了物体合成的准确性，达到了领先的视觉效果。

Abstract: Amodal completion, generating invisible parts of occluded objects, is vital
for applications like image editing and AR. Prior methods face challenges with
data needs, generalization, or error accumulation in progressive pipelines. We
propose a Collaborative Multi-Agent Reasoning Framework based on upfront
collaborative reasoning to overcome these issues. Our framework uses multiple
agents to collaboratively analyze occlusion relationships and determine
necessary boundary expansion, yielding a precise mask for inpainting.
Concurrently, an agent generates fine-grained textual descriptions, enabling
Fine-Grained Semantic Guidance. This ensures accurate object synthesis and
prevents the regeneration of occluders or other unwanted elements, especially
within large inpainting areas. Furthermore, our method directly produces
layered RGBA outputs guided by visible masks and attention maps from a
Diffusion Transformer, eliminating extra segmentation. Extensive evaluations
demonstrate our framework achieves state-of-the-art visual quality.

</details>


### [253] [Neural-MMGS: Multi-modal Neural Gaussian Splats for Large-Scale Scene Reconstruction](https://arxiv.org/abs/2509.17762)
*Sitian Shen,Georgi Pramatarov,Yifu Tao,Daniele De Martini*

Main category: cs.CV

TL;DR: 本文提出Neural-MMGS，一个新颖的神经3DGS框架，通过将图像、激光雷达和语义信息等多模态数据融合到每个高斯球体的紧凑、可学习嵌入中，实现大规模场景重建，提高了重建质量并降低了内存消耗。


<details>
  <summary>Details</summary>
Motivation: 现有的针对大规模场景重建的工作虽然结合了激光雷达数据以提供更准确的几何约束，但未能充分利用激光雷达丰富的物理属性。同样，语义信息虽用于对象检索，但其为场景重建提供高层上下文的潜力未被充分挖掘。传统方法将这些属性作为独立参数附加到高斯球体上，导致内存使用增加并限制了模态间的信息交换。

Method: Neural-MMGS将所有模态（图像、激光雷达和语义）融合到一个紧凑、可学习的嵌入中，该嵌入隐式编码了每个高斯球体的光学、物理和语义特征。然后，训练轻量级神经解码器将这些嵌入映射到高斯参数，从而以更低的内存开销和更高的可扩展性重建每种感知模态。

Result: 在Oxford Spires数据集上，Neural-MMGS实现了更高质量的重建。在KITTI-360数据集上，与当前基于激光雷达的新视角合成方法相比，该方法在存储消耗更少的情况下取得了有竞争力的结果。

Conclusion: Neural-MMGS通过创新的多模态融合策略和紧凑的嵌入表示，有效解决了大规模场景重建中模态信息利用不足和内存效率低的问题，实现了更高质量的重建和更好的可扩展性。

Abstract: This paper proposes Neural-MMGS, a novel neural 3DGS framework for multimodal
large-scale scene reconstruction that fuses multiple sensing modalities in a
per-gaussian compact, learnable embedding. While recent works focusing on
large-scale scene reconstruction have incorporated LiDAR data to provide more
accurate geometric constraints, we argue that LiDAR's rich physical properties
remain underexplored. Similarly, semantic information has been used for object
retrieval, but could provide valuable high-level context for scene
reconstruction. Traditional approaches append these properties to Gaussians as
separate parameters, increasing memory usage and limiting information exchange
across modalities. Instead, our approach fuses all modalities -- image, LiDAR,
and semantics -- into a compact, learnable embedding that implicitly encodes
optical, physical, and semantic features in each Gaussian. We then train
lightweight neural decoders to map these embeddings to Gaussian parameters,
enabling the reconstruction of each sensing modality with lower memory overhead
and improved scalability. We evaluate Neural-MMGS on the Oxford Spires and
KITTI-360 datasets. On Oxford Spires, we achieve higher-quality
reconstructions, while on KITTI-360, our method reaches competitive results
with less storage consumption compared with current approaches in LiDAR-based
novel-view synthesis.

</details>


### [254] [Incorporating the Refractory Period into Spiking Neural Networks through Spike-Triggered Threshold Dynamics](https://arxiv.org/abs/2509.17769)
*Yang Li,Xinyi Zeng,Zhe Xue,Pinxian Zeng,Zikai Zhang,Yan Wang*

Main category: cs.CV

TL;DR: 本文提出RPLIF，一种将不应期通过脉冲触发阈值动态整合到LIF神经元中的方法，以增强SNN的生物合理性、鲁棒性和效率，并在多个神经形态数据集上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 第三代神经网络SNNs因其生物合理性、能效和处理神经形态数据的有效性而受到关注。然而，现有的神经元模型（如IF和LIF）忽略了生物神经元关键的“不应期”特性。不应期对于防止神经元过度兴奋和减轻异常信号干扰至关重要。

Method: 提出了一种简单而有效的方法，通过脉冲触发的阈值动态将不应期整合到尖峰LIF神经元中，命名为RPLIF。该方法确保每个脉冲准确编码神经信息，有效防止连续输入下的神经元过度兴奋和异常输入的干扰。

Result: RPLIF在Cifar10-DVS（82.40%）和N-Caltech101（83.35%）上以更少的步长实现了最先进的性能，并在低延迟下在DVS128 Gesture（97.22%）上表现出卓越的性能。它在计算开销可忽略不计的情况下，增强了鲁棒性和效率，并带来了更好的性能。

Conclusion: 将不应期整合到LIF神经元（RPLIF）中是一种无缝且计算高效的方法，能够有效提高SNN的鲁棒性、效率和性能，并在神经形态数据集上取得了显著的成果。

Abstract: As the third generation of neural networks, spiking neural networks (SNNs)
have recently gained widespread attention for their biological plausibility,
energy efficiency, and effectiveness in processing neuromorphic datasets. To
better emulate biological neurons, various models such as Integrate-and-Fire
(IF) and Leaky Integrate-and-Fire (LIF) have been widely adopted in SNNs.
However, these neuron models overlook the refractory period, a fundamental
characteristic of biological neurons. Research on excitable neurons reveal that
after firing, neurons enter a refractory period during which they are
temporarily unresponsive to subsequent stimuli. This mechanism is critical for
preventing over-excitation and mitigating interference from aberrant signals.
Therefore, we propose a simple yet effective method to incorporate the
refractory period into spiking LIF neurons through spike-triggered threshold
dynamics, termed RPLIF. Our method ensures that each spike accurately encodes
neural information, effectively preventing neuron over-excitation under
continuous inputs and interference from anomalous inputs. Incorporating the
refractory period into LIF neurons is seamless and computationally efficient,
enhancing robustness and efficiency while yielding better performance with
negligible overhead. To the best of our knowledge, RPLIF achieves
state-of-the-art performance on Cifar10-DVS(82.40%) and N-Caltech101(83.35%)
with fewer timesteps and demonstrates superior performance on DVS128
Gesture(97.22%) at low latency.

</details>


### [255] [I2VWM: Robust Watermarking for Image to Video Generation](https://arxiv.org/abs/2509.17773)
*Guanjie Wang,Zehua Ma,Han Fang,Weiming Zhang*

Main category: cs.CV

TL;DR: 本文提出I2VWM，一个针对图像到视频生成（I2V）的跨模态水印框架，通过引入鲁棒扩散距离和视频模拟噪声层，显著提高了水印在生成视频中的持久性和鲁棒性，同时保持不可感知性。


<details>
  <summary>Details</summary>
Motivation: 图像引导视频生成（I2V）的快速发展引发了人们对其在错误信息和欺诈中潜在滥用的担忧，因此迫切需要有效的数字水印。现有水印方法在单一模态内具有鲁棒性，但在I2V场景中无法追溯源图像。

Method: 引入了“鲁棒扩散距离”概念，用于衡量水印信号在生成视频中的时间持久性。在此基础上，提出了I2VWM跨模态水印框架，在训练期间利用视频模拟噪声层，并在推理期间采用基于光流的对齐模块，以增强水印在时间上的鲁棒性。

Result: 在开源和商业I2V模型上的实验表明，I2VWM显著提高了水印的鲁棒性，同时保持了不可感知性。

Conclusion: I2VWM为生成视频时代的跨模态水印建立了一个新范式，有效解决了I2V场景中水印鲁棒性不足的问题。

Abstract: The rapid progress of image-guided video generation (I2V) has raised concerns
about its potential misuse in misinformation and fraud, underscoring the urgent
need for effective digital watermarking. While existing watermarking methods
demonstrate robustness within a single modality, they fail to trace source
images in I2V settings. To address this gap, we introduce the concept of Robust
Diffusion Distance, which measures the temporal persistence of watermark
signals in generated videos. Building on this, we propose I2VWM, a cross-modal
watermarking framework designed to enhance watermark robustness across time.
I2VWM leverages a video-simulation noise layer during training and employs an
optical-flow-based alignment module during inference. Experiments on both
open-source and commercial I2V models demonstrate that I2VWM significantly
improves robustness while maintaining imperceptibility, establishing a new
paradigm for cross-modal watermarking in the era of generative video.
\href{https://github.com/MrCrims/I2VWM-Robust-Watermarking-for-Image-to-Video-Generation}{Code
Released.}

</details>


### [256] [From Restoration to Reconstruction: Rethinking 3D Gaussian Splatting for Underwater Scenes](https://arxiv.org/abs/2509.17789)
*Guoxi Huang,Haoran Wang,Zipeng Qi,Wenjun Lu,David Bull,Nantheera Anantrasirichai*

Main category: cs.CV

TL;DR: R-Splatting是一个统一框架，结合水下图像复原（UIR）和3D高斯泼溅（3DGS），显著提升水下3D重建的渲染质量和几何精度。


<details>
  <summary>Details</summary>
Motivation: 水下图像退化严重影响3D重建，且现有简化物理模型在复杂场景中表现不佳。

Method: R-Splatting整合多种UIR模型增强的视图进行重建。推理时，轻量级光照生成器采样潜在代码以支持多样且一致的渲染，并使用对比损失确保光照表示的解耦和稳定。此外，提出不确定性感知不透明度优化（UAOO），将不透明度建模为随机函数以正则化训练，抑制光照变化引起的梯度响应，并减轻对噪声或特定视图伪影的过拟合。

Result: 在Seathru-NeRF和新的BlueCoral3D数据集上的实验表明，R-Splatting在渲染质量和几何精度方面均优于现有基线方法。

Conclusion: R-Splatting通过其统一框架和创新机制，成功解决了水下图像退化对3D重建的挑战，显著提高了重建的质量和准确性。

Abstract: Underwater image degradation poses significant challenges for 3D
reconstruction, where simplified physical models often fail in complex scenes.
We propose \textbf{R-Splatting}, a unified framework that bridges underwater
image restoration (UIR) with 3D Gaussian Splatting (3DGS) to improve both
rendering quality and geometric fidelity. Our method integrates multiple
enhanced views produced by diverse UIR models into a single reconstruction
pipeline. During inference, a lightweight illumination generator samples latent
codes to support diverse yet coherent renderings, while a contrastive loss
ensures disentangled and stable illumination representations. Furthermore, we
propose \textit{Uncertainty-Aware Opacity Optimization (UAOO)}, which models
opacity as a stochastic function to regularize training. This suppresses abrupt
gradient responses triggered by illumination variation and mitigates
overfitting to noisy or view-specific artifacts. Experiments on Seathru-NeRF
and our new BlueCoral3D dataset demonstrate that R-Splatting outperforms strong
baselines in both rendering quality and geometric accuracy.

</details>


### [257] [Degradation-Aware All-in-One Image Restoration via Latent Prior Encoding](https://arxiv.org/abs/2509.17792)
*S M A Sharif,Abdur Rehman,Fayaz Ali Dharejo,Radu Timofte,Rizwan Ali Naqvi*

Main category: cs.CV

TL;DR: 该论文提出将一体化图像恢复（AIR）重构为学习到的潜在先验推理，通过结构化推理和轻量级解码模块，在处理多种图像退化方面优于现有SOTA方法，实现了更好的泛化性和更高的效率。


<details>
  <summary>Details</summary>
Motivation: 现实世界图像常受空间多样化退化（如雾、雨、雪、低光）影响，严重损害视觉质量和下游视觉任务。现有的一体化恢复（AIR）方法依赖外部文本提示或手工设计的架构先验，这些离散、脆弱的假设限制了其对未知或混合退化的泛化能力。

Method: 将AIR重构为学习到的潜在先验推理，自动从输入中推断出退化感知表示，无需显式任务线索。基于潜在先验，将AIR公式化为结构化推理范式，包括：(1)自适应特征选择，(2)空间定位，以及(3)退化语义。设计了一个轻量级解码模块，有效利用这些潜在编码线索进行空间自适应恢复。

Result: 在六种常见退化任务、五种复合设置和以前未见的退化上进行了广泛实验，证明该方法优于最先进（SOTA）方法，平均PSNR提高了1.68 dB，同时效率提高了三倍。

Conclusion: 该方法通过学习到的潜在先验推理和结构化恢复范式，有效解决了现有AIR方法在处理多样化退化时的泛化性限制，实现了显著的性能提升和更高的效率。

Abstract: Real-world images often suffer from spatially diverse degradations such as
haze, rain, snow, and low-light, significantly impacting visual quality and
downstream vision tasks. Existing all-in-one restoration (AIR) approaches
either depend on external text prompts or embed hand-crafted architectural
priors (e.g., frequency heuristics); both impose discrete, brittle assumptions
that weaken generalization to unseen or mixed degradations. To address this
limitation, we propose to reframe AIR as learned latent prior inference, where
degradation-aware representations are automatically inferred from the input
without explicit task cues. Based on latent priors, we formulate AIR as a
structured reasoning paradigm: (1) which features to route (adaptive feature
selection), (2) where to restore (spatial localization), and (3) what to
restore (degradation semantics). We design a lightweight decoding module that
efficiently leverages these latent encoded cues for spatially-adaptive
restoration. Extensive experiments across six common degradation tasks, five
compound settings, and previously unseen degradations demonstrate that our
method outperforms state-of-the-art (SOTA) approaches, achieving an average
PSNR improvement of 1.68 dB while being three times more efficient.

</details>


### [258] [Selecting Optimal Camera Views for Gait Analysis: A Multi-Metric Assessment of 2D Projections](https://arxiv.org/abs/2509.17805)
*Dong Chen,Huili Peng,Yong Hu,Kenneth MC. Cheung*

Main category: cs.CV

TL;DR: 本研究量化了摄像机视角（正面与侧面）对2D无标记步态分析准确性的影响，发现侧面视角适用于矢状面运动学，而正面视角更适合躯干对称性分析。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在系统地量化摄像机视角（正面与侧面）对2D无标记步态分析准确性的影响，并以3D运动捕捉作为黄金标准，以提供数据驱动的摄像机部署策略，从而提高其临床实用性。

Method: 研究同时使用正面、侧面和3D运动捕捉系统记录了18名受试者的步态数据。姿态估计采用YOLOv8。评估了四种指标：动态时间规整（DTW）用于时间对齐，最大互相关（MCC）用于信号相似性，Kullback-Leibler散度（KLD）用于分布差异，以及信息熵（IE）用于复杂性。使用Wilcoxon符号秩检验（显著性：p < 0.05）和Cliff's delta（δ）来衡量统计差异和效应大小。

Result: 侧面视角在矢状面运动学方面显著优于正面视角，例如步长（DTW: 53.08 ± 24.50 vs. 69.87 ± 25.36, p = 0.005）和膝关节旋转（DTW: 106.46 ± 38.57 vs. 155.41 ± 41.77, p = 0.004）。正面视角在对称性参数方面表现更佳，例如躯干旋转（KLD: 0.09 ± 0.06 vs. 0.30 ± 0.19, p < 0.001）和腕到髋中点距离（MCC: 105.77 ± 29.72 vs. 75.20 ± 20.38, p = 0.003）。效应大小为中到大（δ: 0.34–0.76）。

Conclusion: 摄像机视角对步态参数的准确性至关重要。侧面视角最适合矢状面运动学分析，而正面视角在躯干对称性分析中表现出色。未来的实施应根据疾病类型利用两种视角进行设置，以最大化临床效用。

Abstract: Objective: To systematically quantify the effect of the camera view (frontal
vs. lateral) on the accuracy of 2D markerless gait analysis relative to 3D
motion capture ground truth. Methods: Gait data from 18 subjects were recorded
simultaneously using frontal, lateral and 3D motion capture systems. Pose
estimation used YOLOv8. Four metrics were assessed to evaluate agreement:
Dynamic Time Warping (DTW) for temporal alignment, Maximum Cross-Correlation
(MCC) for signal similarity, Kullback-Leibler Divergence (KLD) for distribution
differences, and Information Entropy (IE) for complexity. Wilcoxon signed-rank
tests (significance: $p < 0.05$) and Cliff's delta ($\delta$) were used to
measure statistical differences and effect sizes. Results: Lateral views
significantly outperformed frontal views for sagittal plane kinematics: step
length (DTW: $53.08 \pm 24.50$ vs. $69.87 \pm 25.36$, $p = 0.005$) and knee
rotation (DTW: $106.46 \pm 38.57$ vs. $155.41 \pm 41.77$, $p = 0.004$). Frontal
views were superior for symmetry parameters: trunk rotation (KLD: $0.09 \pm
0.06$ vs. $0.30 \pm 0.19$, $p < 0.001$) and wrist-to-hipmid distance (MCC:
$105.77 \pm 29.72$ vs. $75.20 \pm 20.38$, $p = 0.003$). Effect sizes were
medium-to-large ($\delta: 0.34$--$0.76$). Conclusion: Camera view critically
impacts gait parameter accuracy. Lateral views are optimal for sagittal
kinematics; frontal views excel for trunk symmetry. Significance: This first
systematic evidence enables data-driven camera deployment in 2D gait analysis,
enhancing clinical utility. Future implementations should leverage both views
via disease-oriented setups.

</details>


### [259] [Enhancing Semantic Segmentation with Continual Self-Supervised Pre-training](https://arxiv.org/abs/2509.17816)
*Brown Ebouky,Ajad Chhatkuli,Cristiano Malossi,Christoph Studer,Roy Assaf,Andrea Bartezzaghi*

Main category: cs.CV

TL;DR: 本文提出GLARE，一种新颖的持续自监督预训练任务，旨在以数据高效的方式将视觉基础模型适应到新领域，特别针对语义分割任务，通过局部和区域一致性约束来提升性能。


<details>
  <summary>Details</summary>
Motivation: 自监督学习（SSL）在大型无标签数据集上训练基础模型并产生强泛化能力的表示。然而，将SSL预训练本身扩展到新领域，尤其是在数据受限和密集预测任务（如语义分割）中，仍未得到充分探索。现有进展主要集中在预训练模型适应下游任务的参数高效策略，而非预训练本身的领域适应。

Method: 本文提出GLARE（Global Local and Regional Enforcement），一种用于增强下游分割性能的持续自监督预训练任务。GLARE引入了补丁级增强以鼓励局部一致性，并结合了利用数据中空间语义的区域一致性约束。为实现高效的持续预训练，模型使用现有SSL模型权重初始化Vision Transformers (ViTs)，并仅更新轻量级适配器模块（UniAdapter），而保持骨干网络冻结。

Result: 在不同领域的多个语义分割基准测试中，实验表明GLARE持续提高了下游性能，且计算和参数开销极小。

Conclusion: GLARE是一种有效且高效的方法，能够以无监督和数据高效的方式，将视觉基础模型的自监督预训练持续适应到新领域，显著提升语义分割任务的性能。

Abstract: Self-supervised learning (SSL) has emerged as a central paradigm for training
foundation models by leveraging large-scale unlabeled datasets, often producing
representations with strong generalization capabilities. These models are
typically pre-trained on general-purpose datasets such as ImageNet and
subsequently adapted to various downstream tasks through finetuning. While
recent advances have explored parameter-efficient strategies for adapting
pre-trained models, extending SSL pre-training itself to new domains -
particularly under limited data regimes and for dense prediction tasks -
remains underexplored. In this work, we address the problem of adapting vision
foundation models to new domains in an unsupervised and data-efficient manner,
specifically targeting downstream semantic segmentation. We propose GLARE
(Global Local and Regional Enforcement), a novel continual self-supervised
pre-training task designed to enhance downstream segmentation performance.
GLARE introduces patch-level augmentations to encourage local consistency and
incorporates a regional consistency constraint that leverages spatial semantics
in the data. For efficient continual pre-training, we initialize Vision
Transformers (ViTs) with weights from existing SSL models and update only
lightweight adapter modules - specifically UniAdapter - while keeping the rest
of the backbone frozen. Experiments across multiple semantic segmentation
benchmarks on different domains demonstrate that GLARE consistently improves
downstream performance with minimal computational and parameter overhead.

</details>


### [260] [ContextFlow: Training-Free Video Object Editing via Adaptive Context Enrichment](https://arxiv.org/abs/2509.17818)
*Yiyang Chen,Xuanhua He,Xiujun Ma,Yue Ma*

Main category: cs.CV

TL;DR: ContextFlow是一个新颖的免训练框架，专为基于DiT的视频对象编辑设计。它通过高阶求解器、自适应上下文丰富机制和数据驱动的任务特定关键层识别，解决了现有方法中不准确的反演和上下文冲突问题，实现了高保真和时间一致的编辑效果。


<details>
  <summary>Details</summary>
Motivation: 免训练视频对象编辑在保持保真度和时间一致性方面面临挑战。现有方法（通常针对U-Net架构）存在两个主要限制：一阶求解器导致的不准确反演，以及粗糙“硬”特征替换引起的上下文冲突。这些问题在Diffusion Transformers (DiTs) 中更为严峻，因为现有层选择启发式方法不适用，使得有效引导变得困难。

Method: ContextFlow框架主要包含三部分：1. 采用高阶Rectified Flow求解器建立稳健的编辑基础。2. 引入自适应上下文丰富（Adaptive Context Enrichment）机制，通过并行重建和编辑路径的键值对（Key-Value pairs）拼接，丰富自注意力上下文，而非简单替换特征，从而动态融合信息。3. 提出一种系统性的、数据驱动的分析方法，基于新颖的引导响应度量（Guidance Responsiveness Metric），识别任务特定（如插入、交换）的关键DiT层，以实现有针对性的有效引导。

Result: 广泛的实验表明，ContextFlow显著优于现有的免训练方法，甚至超越了一些最先进的基于训练的方法，提供了时间上连贯、高保真的编辑结果。

Conclusion: ContextFlow为基于DiT的视频对象编辑提供了一个新颖且有效的免训练框架，成功解决了不准确反演和上下文冲突等挑战，并在性能上取得了显著提升，能够实现高质量、时间一致的视频对象编辑。

Abstract: Training-free video object editing aims to achieve precise object-level
manipulation, including object insertion, swapping, and deletion. However, it
faces significant challenges in maintaining fidelity and temporal consistency.
Existing methods, often designed for U-Net architectures, suffer from two
primary limitations: inaccurate inversion due to first-order solvers, and
contextual conflicts caused by crude "hard" feature replacement. These issues
are more challenging in Diffusion Transformers (DiTs), where the unsuitability
of prior layer-selection heuristics makes effective guidance challenging. To
address these limitations, we introduce ContextFlow, a novel training-free
framework for DiT-based video object editing. In detail, we first employ a
high-order Rectified Flow solver to establish a robust editing foundation. The
core of our framework is Adaptive Context Enrichment (for specifying what to
edit), a mechanism that addresses contextual conflicts. Instead of replacing
features, it enriches the self-attention context by concatenating Key-Value
pairs from parallel reconstruction and editing paths, empowering the model to
dynamically fuse information. Additionally, to determine where to apply this
enrichment (for specifying where to edit), we propose a systematic, data-driven
analysis to identify task-specific vital layers. Based on a novel Guidance
Responsiveness Metric, our method pinpoints the most influential DiT blocks for
different tasks (e.g., insertion, swapping), enabling targeted and highly
effective guidance. Extensive experiments show that ContextFlow significantly
outperforms existing training-free methods and even surpasses several
state-of-the-art training-based approaches, delivering temporally coherent,
high-fidelity results.

</details>


### [261] [Semantic and Visual Crop-Guided Diffusion Models for Heterogeneous Tissue Synthesis in Histopathology](https://arxiv.org/abs/2509.17847)
*Saghir Alfasly,Wataru Uegami,MD Enamul Hoq,Ghazal Alabtah,H. R. Tizhoosh*

Main category: cs.CV

TL;DR: 该研究提出了一种双条件潜在扩散模型，通过结合语义分割图和组织特异性视觉裁剪，生成逼真且具有异质性的组织病理学图像。该模型能扩展到未标注数据集，有效解决了计算病理学中高质量带注释数据稀缺的挑战。


<details>
  <summary>Details</summary>
Motivation: 组织病理学合成数据面临独特挑战：难以保持组织异质性、捕获细微形态特征以及扩展到未标注数据集。现有方法依赖文本提示或抽象视觉嵌入，难以保留关键形态细节。

Method: 核心方法是一个潜在扩散模型，采用新颖的双条件方法，结合语义分割图和组织特异性视觉裁剪来生成图像。该方法通过直接整合来自相应语义区域的原始组织裁剪来保留关键形态细节。对于已标注数据集，提取20-80%组织异质性的补丁。对于未标注数据，引入自监督扩展，利用基础模型嵌入将全玻片图像聚类为100种组织类型，自动生成伪语义图进行训练。

Result: 该方法合成的图像具有高保真度及精确的区域注释，并在下游分割任务中表现优异。在标注数据集上，使用合成数据训练的模型性能与真实数据训练的模型具有竞争力。定量评估显示，提示引导合成使Camelyon16上的Frechet距离降低高达6倍（从430.1降至72.0），并在Panda和TCGA上降低2-3倍。仅用合成数据训练的DeepLabv3+模型在Camelyon16和Panda上的测试IoU分别达到0.71和0.95，与真实数据基线（0.72和0.96）相差1-2%。该框架成功扩展到11,765张未经手动标注的TCGA全玻片图像。

Conclusion: 该框架为生成多样化、带注释的组织病理学数据提供了一个实用解决方案，解决了计算病理学中的关键瓶颈。通过受控的异质性组织生成，证明了其在下游任务中的实用性，并能有效利用大规模未标注数据。

Abstract: Synthetic data generation in histopathology faces unique challenges:
preserving tissue heterogeneity, capturing subtle morphological features, and
scaling to unannotated datasets. We present a latent diffusion model that
generates realistic heterogeneous histopathology images through a novel
dual-conditioning approach combining semantic segmentation maps with
tissue-specific visual crops. Unlike existing methods that rely on text prompts
or abstract visual embeddings, our approach preserves critical morphological
details by directly incorporating raw tissue crops from corresponding semantic
regions. For annotated datasets (i.e., Camelyon16, Panda), we extract patches
ensuring 20-80% tissue heterogeneity. For unannotated data (i.e., TCGA), we
introduce a self-supervised extension that clusters whole-slide images into 100
tissue types using foundation model embeddings, automatically generating
pseudo-semantic maps for training. Our method synthesizes high-fidelity images
with precise region-wise annotations, achieving superior performance on
downstream segmentation tasks. When evaluated on annotated datasets, models
trained on our synthetic data show competitive performance to those trained on
real data, demonstrating the utility of controlled heterogeneous tissue
generation. In quantitative evaluation, prompt-guided synthesis reduces Frechet
Distance by up to 6X on Camelyon16 (from 430.1 to 72.0) and yields 2-3x lower
FD across Panda and TCGA. Downstream DeepLabv3+ models trained solely on
synthetic data attain test IoU of 0.71 and 0.95 on Camelyon16 and Panda, within
1-2% of real-data baselines (0.72 and 0.96). By scaling to 11,765 TCGA
whole-slide images without manual annotations, our framework offers a practical
solution for an urgent need for generating diverse, annotated histopathology
data, addressing a critical bottleneck in computational pathology.

</details>


### [262] [ProDyG: Progressive Dynamic Scene Reconstruction via Gaussian Splatting from Monocular Videos](https://arxiv.org/abs/2509.17864)
*Shi Chen,Erik Sandström,Sandro Lombardi,Siyuan Li,Martin R. Oswald*

Main category: cs.CV

TL;DR: 该论文提出了一种在线动态场景三维重建方法，通过在SLAM系统中解耦静态和动态部分，实现了鲁棒的姿态跟踪和详细的动态部分重建，其新视角渲染效果可与离线方法媲美，跟踪性能与最先进的动态SLAM方法相当。


<details>
  <summary>Details</summary>
Motivation: 现有的动态三维重建方法存在局限性：SLAM方法通常仅移除动态部分或需要RGB-D输入；离线方法不适用于长视频序列；基于Transformer的前馈方法缺乏全局一致性和外观细节。实际应用需要在线操作、全局姿态和地图一致性、详细外观建模以及对RGB和RGB-D输入的灵活性。

Method: 该方法通过在SLAM系统内部解耦静态和动态部分来实现在线动态场景重建。它采用一种新颖的运动掩蔽策略来鲁棒地跟踪姿态，并利用运动支架图（Motion Scaffolds graph）的渐进式自适应来重建动态部分。

Result: 该方法产生的新视角渲染效果可与离线方法竞争，并且在跟踪性能上与最先进的动态SLAM方法持平。

Conclusion: 该论文成功提出了一种解决动态场景三维重建挑战的在线SLAM系统，它通过有效处理静态和动态组件，实现了高保真度的新视角渲染和鲁棒的姿态跟踪，为实用化的动态三维重建提供了有效方案。

Abstract: Achieving truly practical dynamic 3D reconstruction requires online
operation, global pose and map consistency, detailed appearance modeling, and
the flexibility to handle both RGB and RGB-D inputs. However, existing SLAM
methods typically merely remove the dynamic parts or require RGB-D input, while
offline methods are not scalable to long video sequences, and current
transformer-based feedforward methods lack global consistency and appearance
details. To this end, we achieve online dynamic scene reconstruction by
disentangling the static and dynamic parts within a SLAM system. The poses are
tracked robustly with a novel motion masking strategy, and dynamic parts are
reconstructed leveraging a progressive adaptation of a Motion Scaffolds graph.
Our method yields novel view renderings competitive to offline methods and
achieves on-par tracking with state-of-the-art dynamic SLAM methods.

</details>


### [263] [Does Audio Matter for Modern Video-LLMs and Their Benchmarks?](https://arxiv.org/abs/2509.17901)
*Geewook Kim,Minjoon Seo*

Main category: cs.CV

TL;DR: 本研究发现，音频在当前视频大语言模型（Video-LLMs）的基准测试中作用甚微，但在特定音频敏感任务中至关重要。论文提出了新的音频敏感基准和一种音频处理模型，以弥补学术实践与真实世界期望之间的差距。


<details>
  <summary>Details</summary>
Motivation: 现代多模态大语言模型常宣称“视频理解”，但评估时多忽略或丢弃音频，甚至许多现有基准仅凭单帧即可解决。作者质疑音频对当代Video-LLMs及其认证基准的实际重要性。

Method: 审计了广泛使用的视频基准；基于LLaVA-OneVision架构，集成语音/音频编码器（如Whisper）；采用轻量级基于Mamba的状态空间令牌压缩器解决音频令牌爆炸问题；发布了音频敏感的AVQA-Hard和Music-AVQA-Hard基准，并开源了模型和代码。

Result: 研究发现音频在近期视频基准上带来的增益微乎其微，但在精心策划的音频敏感子集上却具有决定性作用。同时，许多现有基准项目仅凭单帧即可解决，使得音频冗余。

Conclusion: 当前学术实践与真实世界期望之间存在日益扩大的差距。为实现忠实评估，本研究提供了实用的工具（模型、代码和新的基准），以支持可扩展的音视频Video-LLMs的开发和评估。

Abstract: Modern multimodal large language models often claim "video understanding,"
yet most evaluations use muted videos or simply discard audio. We ask a direct
question: how much does audio actually matter for contemporary Video-LLMs and
the benchmarks that certify them? We audit widely used suites and observe that
many items are even solvable from a single frame, rendering audio largely
redundant. Building on LLaVA-OneVision architecture, we attach a speech/audio
encoder (e.g., Whisper) and analyze when audio helps, while addressing audio
token explosion with a lightweight Mamba-based state-space token compressor. We
find that audio yields minimal gains on recent video benchmarks but is decisive
on curated, audio-sensitive subsets. To enable faithful evaluation, we release
AVQA-Hard and Music-AVQA-Hard, our model, and code. Our findings surface a
growing gap between current academic practice and real-world expectations, and
provide practical tools for scalable audio-visual Video-LLMs. We will fully
open-source our work at https://github.com/naver-ai/LLaVA-AV-SSM.

</details>


### [264] [SmaRT: Style-Modulated Robust Test-Time Adaptation for Cross-Domain Brain Tumor Segmentation in MRI](https://arxiv.org/abs/2509.17925)
*Yuanhan Wang,Yifei Chen,Shuo Jiang,Wenjing Yu,Mingxuan Liu,Beining Wu,Jinying Zong,Feiwei Qin,Changmiao Wang,Qiyuan Tian*

Main category: cs.CV

TL;DR: 针对MRI脑肿瘤分割中因域偏移导致的模型失效问题，本文提出SmaRT框架，一个风格调制鲁棒的测试时自适应方法，实现无源跨域泛化，并在非洲撒哈拉以南和儿科胶质瘤数据集中表现优异。


<details>
  <summary>Details</summary>
Motivation: MRI脑肿瘤分割对治疗规划和效果监测至关重要，但现有模型在面对扫描仪、协议和人群差异引起的域偏移时表现不佳，尤其在资源匮乏和儿科群体中，传统的测试时或无源自适应策略常不稳定且缺乏结构一致性。

Method: SmaRT框架整合了以下策略：1) 风格感知增强以减轻外观差异；2) 双分支动量策略以实现稳定的伪标签细化；3) 结构先验以强制保持一致性、完整性和连通性。

Result: 在非洲撒哈拉以南和儿科胶质瘤数据集上的广泛评估表明，SmaRT持续优于现有最先进的方法，在Dice准确度和边界精度方面取得了显著提升。

Conclusion: SmaRT弥合了算法进步与公平临床应用之间的鸿沟，支持在多样化临床环境中稳健部署基于MRI的神经肿瘤学工具。

Abstract: Reliable brain tumor segmentation in MRI is indispensable for treatment
planning and outcome monitoring, yet models trained on curated benchmarks often
fail under domain shifts arising from scanner and protocol variability as well
as population heterogeneity. Such gaps are especially severe in low-resource
and pediatric cohorts, where conventional test-time or source-free adaptation
strategies often suffer from instability and structural inconsistency. We
propose SmaRT, a style-modulated robust test-time adaptation framework that
enables source-free cross-domain generalization. SmaRT integrates style-aware
augmentation to mitigate appearance discrepancies, a dual-branch momentum
strategy for stable pseudo-label refinement, and structural priors enforcing
consistency, integrity, and connectivity. This synergy ensures both adaptation
stability and anatomical fidelity under extreme domain shifts. Extensive
evaluations on sub-Saharan Africa and pediatric glioma datasets show that SmaRT
consistently outperforms state-of-the-art methods, with notable gains in Dice
accuracy and boundary precision. Overall, SmaRT bridges the gap between
algorithmic advances and equitable clinical applicability, supporting robust
deployment of MRI-based neuro-oncology tools in diverse clinical environments.
Our source code is available at https://github.com/baiyou1234/SmaRT.

</details>


### [265] [Multi-needle Localization for Pelvic Seed Implant Brachytherapy based on Tip-handle Detection and Matching](https://arxiv.org/abs/2509.17931)
*Zhuo Xiao,Fugen Zhou,Jingjing Wang,Chongyu He,Bo Liu,Haitao Sun,Zhe Ji,Yuliang Jiang,Junjie Wang,Qiuwen Wu*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的多针定位方法，将问题重新定义为针尖-针柄检测与匹配，并利用基于HRNet的无锚点网络和贪婪匹配合并（GMM）算法，在复杂CT图像中实现了更准确和鲁棒的针头定位。


<details>
  <summary>Details</summary>
Motivation: 在盆腔近距离放射治疗中，术中CT图像中的多针准确定位对于优化放射源植入至关重要。然而，由于图像对比度差和针头粘连，这项任务极具挑战性。

Method: 该研究将针头定位重新定义为针尖-针柄检测和匹配问题。提出了一种基于HRNet的无锚点网络，通过解耦分支（热图回归和极角预测）来预测针尖和针柄的中心及方向，以提取多尺度特征。为了将检测到的针尖和针柄匹配成独立的针头，提出了一种贪婪匹配合并（GMM）方法，该方法旨在解决带约束的不平衡分配问题（UAP-C），通过迭代选择最可能的针尖-针柄对并基于距离度量进行合并，以重建3D针头路径。

Result: 在包含100名患者的数据集上进行评估，所提出的方法与使用nnUNet模型的基于分割的方法相比，表现出卓越的性能，实现了更高的精度和F1分数。

Conclusion: 该方法为复杂的临床场景中的针头定位提供了一个更鲁棒和准确的解决方案，有望优化盆腔近距离放射治疗中的放射源植入。

Abstract: Accurate multi-needle localization in intraoperative CT images is crucial for
optimizing seed placement in pelvic seed implant brachytherapy. However, this
task is challenging due to poor image contrast and needle adhesion. This paper
presents a novel approach that reframes needle localization as a tip-handle
detection and matching problem to overcome these difficulties. An anchor-free
network, based on HRNet, is proposed to extract multi-scale features and
accurately detect needle tips and handles by predicting their centers and
orientations using decoupled branches for heatmap regression and polar angle
prediction. To associate detected tips and handles into individual needles, a
greedy matching and merging (GMM) method designed to solve the unbalanced
assignment problem with constraints (UAP-C) is presented. The GMM method
iteratively selects the most probable tip-handle pairs and merges them based on
a distance metric to reconstruct 3D needle paths. Evaluated on a dataset of 100
patients, the proposed method demonstrates superior performance, achieving
higher precision and F1 score compared to a segmentation-based method utilizing
the nnUNet model,thereby offering a more robust and accurate solution for
needle localization in complex clinical scenarios.

</details>


### [266] [Can multimodal representation learning by alignment preserve modality-specific information?](https://arxiv.org/abs/2509.17943)
*Romain Thoreau,Jessie Levillain,Dawa Derksen*

Main category: cs.CV

TL;DR: 本文探讨了多模态数据对齐策略在卫星遥感中可能导致的信息损失问题，通过理论分析和数值实验证明了这种损失的发生机制，旨在指导对比学习的新发展。


<details>
  <summary>Details</summary>
Motivation: 多模态数据融合是机器学习（包括遥感）中的关键问题。标记数据稀缺推动了自监督学习的发展。现有最先进的多模态表示学习技术利用不同模态卫星数据的空间对齐来促进潜在空间的语义对齐，但研究者质疑这些方法能否保留跨模态不共享但与任务相关的信息。

Method: 1. 在简化假设下，理论分析对齐策略何时会从根本上导致信息损失。2. 通过数值实验，在更真实的场景中支持理论洞察。

Result: 1. 理论上证明了对齐策略在特定情况下会导致信息损失。2. 数值实验结果在现实设置中支持了这一理论发现。

Conclusion: 这些理论和实证证据有望支持多模态卫星数据组合中对比学习的新发展，尤其是在设计方法时需考虑如何保留跨模态不共享但重要的信息。

Abstract: Combining multimodal data is a key issue in a wide range of machine learning
tasks, including many remote sensing problems. In Earth observation, early
multimodal data fusion methods were based on specific neural network
architectures and supervised learning. Ever since, the scarcity of labeled data
has motivated self-supervised learning techniques. State-of-the-art multimodal
representation learning techniques leverage the spatial alignment between
satellite data from different modalities acquired over the same geographic area
in order to foster a semantic alignment in the latent space. In this paper, we
investigate how this methods can preserve task-relevant information that is not
shared across modalities. First, we show, under simplifying assumptions, when
alignment strategies fundamentally lead to an information loss. Then, we
support our theoretical insight through numerical experiments in more realistic
settings. With those theoretical and empirical evidences, we hope to support
new developments in contrastive learning for the combination of multimodal
satellite data. Our code and data is publicly available at
https://github.com/Romain3Ch216/alg_maclean_25.

</details>


### [267] [DragOSM: Extract Building Roofs and Footprints from Aerial Images by Aligning Historical Labels](https://arxiv.org/abs/2509.17951)
*Kai Li,Xingxing Weng,Yupeng Deng,Yu Meng,Chao Pang,Gui-Song Xia,Xiangyu Zhao*

Main category: cs.CV

TL;DR: 该论文提出了一种名为DragOSM的新模型，用于解决遥感图像中屋顶和建筑足迹提取的挑战，特别是通过对OpenStreetMap等历史矢量地图标签进行对齐和校正，以适应倾斜视角图像中的位移和误差。


<details>
  <summary>Details</summary>
Motivation: 现有方法（基于分割的模型）在倾斜视角（off-nadir）图像中表现不佳，因为屋顶和足迹显著位移，且立面像素与屋顶边界融合。此外，虽然OpenStreetMap等历史矢量地图标签可用，但它们与新图像存在显著的位置差异，并且通常只提供单一标注（屋顶或足迹），无法准确描述建筑物结构。

Method: 1. 引入“对齐令牌”（alignment token）概念，编码校正向量以指导标签校正。
2. 提出DragOSM模型，将标签对齐建模为交互式去噪过程，将位置差异建模为高斯分布。
3. 训练时，通过模拟高斯扰动产生的错位来学习纠正这些误差。
4. 推理时，迭代地细化输入标签的位置。
5. 构建了一个新数据集ReBO（Repairing Buildings in OSM），包含179,265个建筑物，覆盖5,473张图像，用于验证方法。

Result: DragOSM模型在ReBO数据集上的实验结果表明了其有效性。代码、数据集和训练模型已公开。

Conclusion: DragOSM模型通过引入对齐令牌和交互式去噪过程，成功解决了倾斜视角遥感图像中屋顶和建筑足迹提取的历史标签位移和不完整问题，有效对齐了OpenStreetMap标签。

Abstract: Extracting polygonal roofs and footprints from remote sensing images is
critical for large-scale urban analysis. Most existing methods rely on
segmentation-based models that assume clear semantic boundaries of roofs, but
these approaches struggle in off- nadir images, where the roof and footprint
are significantly displaced, and facade pixels are fused with the roof
boundary. With the increasing availability of open vector map annotations,
e.g., OpenStreetMap, utilizing historical labels for off-nadir image annotation
has become viable because remote sensing images are georeferenced once
captured. However, these historical labels commonly suffer from significant
positional discrepancies with new images and only have one annotation (roof or
footprint), which fails to describe the correct structures of a building. To
address these discrepancies, we first introduce a concept of an alignment
token, which encodes the correction vector to guide the label correction. Based
on this concept, we then propose Drag OpenStreetMap Labels (DragOSM), a novel
model designed to align dislocated historical labels with roofs and footprints.
Specifically, DragOSM formulates the label alignment as an interactive
denoising process, modeling the positional discrepancy as a Gaussian
distribution. During training, it learns to correct these errors by simulating
misalignment with random Gaussian perturbations; during inference, it
iteratively refines the positions of input labels. To validate our method, we
further present a new dataset, Repairing Buildings in OSM (ReBO), comprising
179,265 buildings with both OpenStreetMap and manually corrected annotations
across 5,473 images from 41 cities. Experimental results on ReBO demonstrate
the effectiveness of DragOSM. Code, dataset, and trained models are publicly
available at https://github.com/likaiucas/DragOSM.git.

</details>


### [268] [Breaking the Discretization Barrier of Continuous Physics Simulation Learning](https://arxiv.org/abs/2509.17955)
*Fan Xu,Hao Wu,Nan Wang,Lilan Peng,Kun Wang,Wei Gong,Xibin Zhao*

Main category: cs.CV

TL;DR: CoPS是一种纯数据驱动的方法，通过融合空间信息、定制几何网格、设计多尺度图ODE和引入马尔可夫自校正模块，有效解决从部分观测数据建模连续物理模拟的挑战。


<details>
  <summary>Details</summary>
Motivation: 从部分观测数据建模复杂的时变物理动态是一个长期存在的挑战。现有数据驱动方法常受限于固定的时空离散化，或过度依赖传统数值方法，未能真正克服离散化带来的局限性，尤其是在观测数据稀疏且无结构分布时难以捕捉高度非线性特征。

Method: 该研究提出了CoPS方法，具体包括：1) 使用乘性滤波器网络融合并编码空间信息及其对应观测；2) 定制几何网格并利用消息传递机制将特征从原始空间域映射到这些网格；3) 通过设计多尺度图ODE来建模连续时间动态；4) 引入基于马尔可夫的神经自校正模块来辅助和约束连续外推。

Result: 全面的实验表明，CoPS在各种场景下的时空连续建模方面超越了现有最先进的方法。

Conclusion: CoPS提供了一种纯数据驱动的有效方法，能够从部分观测数据中建模连续的物理模拟，克服了传统方法在离散化和捕捉非线性特征方面的局限性。

Abstract: The modeling of complicated time-evolving physical dynamics from partial
observations is a long-standing challenge. Particularly, observations can be
sparsely distributed in a seemingly random or unstructured manner, making it
difficult to capture highly nonlinear features in a variety of scientific and
engineering problems. However, existing data-driven approaches are often
constrained by fixed spatial and temporal discretization. While some
researchers attempt to achieve spatio-temporal continuity by designing novel
strategies, they either overly rely on traditional numerical methods or fail to
truly overcome the limitations imposed by discretization. To address these, we
propose CoPS, a purely data-driven methods, to effectively model continuous
physics simulation from partial observations. Specifically, we employ
multiplicative filter network to fuse and encode spatial information with the
corresponding observations. Then we customize geometric grids and use
message-passing mechanism to map features from original spatial domain to the
customized grids. Subsequently, CoPS models continuous-time dynamics by
designing multi-scale graph ODEs, while introducing a Markov-based neural
auto-correction module to assist and constrain the continuous extrapolations.
Comprehensive experiments demonstrate that CoPS advances the state-of-the-art
methods in space-time continuous modeling across various scenarios.

</details>


### [269] [Visual Detector Compression via Location-Aware Discriminant Analysis](https://arxiv.org/abs/2509.17968)
*Qizhen Lan,Jung Im Choi,Qing Tian*

Main category: cs.CV

TL;DR: 本文提出了一种主动的、基于检测判别式的深度视觉检测器网络压缩方法，通过利用目标定位信息，在显著降低模型复杂度的同时，甚至能超越原始模型的性能。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络复杂度高，难以部署到资源受限的边缘设备；现有剪枝方法主要针对分类模型，对检测模型关注不足；即使针对检测模型，也缺乏利用关键的定位信息；许多剪枝方法被动依赖预训练模型，难以有效区分和移除无用组件。

Method: 提出了一种主动的、基于检测判别式的网络压缩方法，该方法在两个步骤之间交替进行：1) 最大化和压缩与检测相关的判别式，并将其与检测头前的一组神经元/滤波器对齐；2) 跨层追踪与检测相关的判别能力，并丢弃重要性较低的特征。这两个步骤都利用了目标定位信息。

Result: 在KITTI和COCO数据集上，通过对四种先进检测模型和四种最先进的竞争方法进行广泛实验，结果表明本文方法具有优越性。值得注意的是，经过压缩的模型甚至能以显著降低的复杂度超越原始基础模型。

Conclusion: 本文提出的主动检测判别式网络压缩方法，通过有效利用定位信息，能够显著压缩深度视觉检测器，同时在性能上超越或保持原始模型的水平，解决了现有剪枝方法在检测任务中的局限性。

Abstract: Deep neural networks are powerful, yet their high complexity greatly limits
their potential to be deployed on billions of resource-constrained edge
devices. Pruning is a crucial network compression technique, yet most existing
methods focus on classification models, with limited attention to detection.
Even among those addressing detection, there is a lack of utilization of
essential localization information. Also, many pruning methods passively rely
on pre-trained models, in which useful and useless components are intertwined,
making it difficult to remove the latter without harming the former at the
neuron/filter level. To address the above issues, in this paper, we propose a
proactive detection-discriminants-based network compression approach for deep
visual detectors, which alternates between two steps: (1) maximizing and
compressing detection-related discriminants and aligning them with a subset of
neurons/filters immediately before the detection head, and (2) tracing the
detection-related discriminating power across the layers and discarding
features of lower importance. Object location information is exploited in both
steps. Extensive experiments, employing four advanced detection models and four
state-of-the-art competing methods on the KITTI and COCO datasets, highlight
the superiority of our approach. Remarkably, our compressed models can even
beat the original base models with a substantial reduction in complexity.

</details>


### [270] [StableGuard: Towards Unified Copyright Protection and Tamper Localization in Latent Diffusion Models](https://arxiv.org/abs/2509.17993)
*Haoxin Yang,Bangzhen Liu,Xuemiao Xu,Cheng Xu,Yuyang Yu,Zikai Huang,Yi Wang,Shengfeng He*

Main category: cs.CV

TL;DR: StableGuard是一个新颖的框架，它将二元水印无缝集成到扩散生成过程中，通过端到端设计在潜在扩散模型中实现版权保护和篡改定位，并显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 扩散模型生成内容的真实性提高，但也带来了滥用担忧，需要强大的版权保护和篡改定位。现有方法依赖于事后处理，导致应用不便并损害取证可靠性。

Method: 提出StableGuard框架。开发了多路复用水印VAE（MPW-VAE），通过为预训练VAE配备轻量级潜在残差适配器，生成配对的水印和无水印图像。这些图像通过随机掩码融合，创建用于训练篡改无关取证网络的数据集。引入了专家混合引导取证网络（MoE-GFN），动态整合整体水印模式、局部篡改痕迹和频域线索。MPW-VAE和MoE-GFN以自监督、端到端方式联合优化，促进水印嵌入和取证准确性之间的相互训练。

Result: StableGuard在图像保真度、水印验证和篡改定位方面始终优于最先进的方法。

Conclusion: StableGuard通过其端到端设计和水印嵌入与取证准确性之间的协同训练，为扩散模型中的版权保护和篡改定位提供了一个卓越的集成解决方案。

Abstract: The advancement of diffusion models has enhanced the realism of AI-generated
content but also raised concerns about misuse, necessitating robust copyright
protection and tampering localization. Although recent methods have made
progress toward unified solutions, their reliance on post hoc processing
introduces considerable application inconvenience and compromises forensic
reliability. We propose StableGuard, a novel framework that seamlessly
integrates a binary watermark into the diffusion generation process, ensuring
copyright protection and tampering localization in Latent Diffusion Models
through an end-to-end design. We develop a Multiplexing Watermark VAE (MPW-VAE)
by equipping a pretrained Variational Autoencoder (VAE) with a lightweight
latent residual-based adapter, enabling the generation of paired watermarked
and watermark-free images. These pairs, fused via random masks, create a
diverse dataset for training a tampering-agnostic forensic network. To further
enhance forensic synergy, we introduce a Mixture-of-Experts Guided Forensic
Network (MoE-GFN) that dynamically integrates holistic watermark patterns,
local tampering traces, and frequency-domain cues for precise watermark
verification and tampered region detection. The MPW-VAE and MoE-GFN are jointly
optimized in a self-supervised, end-to-end manner, fostering a reciprocal
training between watermark embedding and forensic accuracy. Extensive
experiments demonstrate that StableGuard consistently outperforms
state-of-the-art methods in image fidelity, watermark verification, and
tampering localization.

</details>


### [271] [NeuS-QA: Grounding Long-Form Video Understanding in Temporal Logic and Neuro-Symbolic Reasoning](https://arxiv.org/abs/2509.18041)
*Sahil Shah,S P Sharan,Harsh Goel,Minkyu Choi,Mustafa Munir,Manvik Pasula,Radu Marculescu,Sandeep Chinchali*

Main category: cs.CV

TL;DR: NeuS-QA 是一种免训练、即插即用的神经符号流水线，通过将自然语言问题转化为形式化时间逻辑，并利用模型检测从视频中识别逻辑验证的片段，从而解决长视频问答（LVQA）中现有方法的局限性，提高复杂推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（VLMs）在长视频问答（LVQA）中表现不佳，难以处理涉及多步时间推理和因果关系的复杂查询。传统均匀采样方法导致大量token开销和严重降采样，使得模型错过细粒度视觉结构或关键时间线索。虽然有查询自适应采样等方法，但它们缺乏明确的时间表示和逻辑保证，无法确保采样的上下文真正编码了问题所需的组合或因果逻辑。

Method: NeuS-QA 是一种免训练、即插即用的神经符号流水线。它将自然语言问题转换为形式化时间逻辑表达式，从帧级语义命题构建视频自动机，并应用模型检测严格识别满足问题逻辑要求的视频片段。只有这些经过逻辑验证的片段才会被提交给 VLM，从而在不修改或微调模型的情况下，提高可解释性、减少幻觉并实现组合推理。

Result: 在 LongVideoBench 和 CinePile 数据集上的实验表明，NeuS-QA 将性能提高了 10% 以上，尤其是在涉及事件排序、因果关系和多步组合推理的问题上。同时，它还提高了可解释性并减少了幻觉。

Conclusion: NeuS-QA 通过引入神经符号推理，解决了长视频问答中现有方法的根本性缺陷，提供了一种无需训练、可插拔的解决方案。它能够严格验证视频片段的逻辑一致性，从而显著提升模型在复杂时间推理和因果关系问题上的表现，并增强了结果的可解释性。

Abstract: Long-Form Video Question Answering (LVQA) poses challenges beyond traditional
visual question answering (VQA), which is often limited to static images or
short video clips. While current vision-language models (VLMs) perform well in
those settings, they struggle with complex queries in LVQA over long videos
involving multi-step temporal reasoning and causality. Vanilla approaches,
which sample frames uniformly and feed them to a VLM with the question, incur
significant token overhead, forcing severe downsampling. As a result, the model
often misses fine-grained visual structure, subtle event transitions, or key
temporal cues, ultimately leading to incorrect answers. To address these
limitations, recent works have explored query-adaptive frame sampling,
hierarchical keyframe selection, and agent-based iterative querying. However,
these methods remain fundamentally heuristic: they lack explicit temporal
representations and cannot enforce or verify logical event relationships. As a
result, there are no formal guarantees that the sampled context actually
encodes the compositional or causal logic demanded by the question. To address
these foundational gaps, we introduce NeuS-QA, a training-free, plug-and-play
neuro-symbolic pipeline for LVQA. NeuS-QA translates a natural language
question into a formal temporal logic expression, constructs a video automaton
from frame-level semantic propositions, and applies model checking to
rigorously identify video segments satisfying the question's logical
requirements. Only these logic-verified segments are submitted to the VLM, thus
improving interpretability, reducing hallucinations, and enabling compositional
reasoning without modifying or fine-tuning the model. Experiments on
LongVideoBench and CinePile show NeuS-QA improves performance by over 10%,
especially on questions involving event ordering, causality, and multi-step
compositional reasoning.

</details>


### [272] [TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning for Video LLMs](https://arxiv.org/abs/2509.18056)
*Yunheng Li,Jing Cheng,Shaoyong Jia,Hangyi Kuang,Shaohui Jiao,Qibin Hou,Ming-Ming Cheng*

Main category: cs.CV

TL;DR: 本文提出TempSamp-R1，一个用于视频时间定位任务的新型强化微调框架，通过离策略监督、非线性软优势计算和混合CoT训练，显著提升多模态大语言模型（MLLMs）的性能，并实现SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法（如GRPO）在视频时间定位任务中，由于依赖于策略采样和巨大的时间搜索空间，导致效率低下且性能受限，难以识别时间精确的解决方案。

Method: TempSamp-R1利用真值标注作为离策略监督，提供时间精确的指导；引入非线性软优势计算方法，通过非对称变换动态调整奖励反馈，稳定训练并减少方差；采用混合思维链（CoT）训练范式，优化单个统一模型以支持CoT和非CoT推理模式。

Result: TempSamp-R1超越了基于GRPO的基线，在Charades-STA (R1@0.7: 52.9%, +2.7%)、ActivityNet Captions (R1@0.5: 56.0%, +5.3%)和QVHighlights (mAP: 30.0%, +3.0%)等基准数据集上取得了新的最先进性能。此外，TempSamp-R1在有限数据下表现出强大的少样本泛化能力。

Conclusion: TempSamp-R1通过引入离策略监督、改进奖励反馈机制和灵活的推理模式，显著提高了多模态大语言模型在视频时间定位任务中的效率和准确性，为该领域树立了新的性能标杆。

Abstract: This paper introduces TempSamp-R1, a new reinforcement fine-tuning framework
designed to improve the effectiveness of adapting multimodal large language
models (MLLMs) to video temporal grounding tasks. We reveal that existing
reinforcement learning methods, such as Group Relative Policy Optimization
(GRPO), rely on on-policy sampling for policy updates. However, in tasks with
large temporal search spaces, this strategy becomes both inefficient and
limited in performance, as it often fails to identify temporally accurate
solutions. To address this limitation, TempSamp-R1 leverages ground-truth
annotations as off-policy supervision to provide temporally precise guidance,
effectively compensating for the sparsity and misalignment in on-policy
solutions. To further stabilize training and reduce variance in reward-based
updates, TempSamp-R1 provides a non-linear soft advantage computation method
that dynamically reshapes the reward feedback via an asymmetric transformation.
By employing a hybrid Chain-of-Thought (CoT) training paradigm, TempSamp-R1
optimizes a single unified model to support both CoT and non-CoT inference
modes, enabling efficient handling of queries with varying reasoning
complexity. Experimental results demonstrate that TempSamp-R1 outperforms
GRPO-based baselines, establishing new state-of-the-art performance on
benchmark datasets: Charades-STA (R1@0.7: 52.9%, +2.7%), ActivityNet Captions
(R1@0.5: 56.0%, +5.3%), and QVHighlights (mAP: 30.0%, +3.0%). Moreover,
TempSamp-R1 shows robust few-shot generalization capabilities under limited
data. Code: https://github.com/HVision-NKU/TempSamp-R1

</details>


### [273] [GraDeT-HTR: A Resource-Efficient Bengali Handwritten Text Recognition System utilizing Grapheme-based Tokenizer and Decoder-only Transformer](https://arxiv.org/abs/2509.18081)
*Md. Mahmudul Hasan,Ahmed Nesar Tahsin Choudhury,Mahmudul Hasan,Md. Mosaddek Khan*

Main category: cs.CV

TL;DR: 本文提出GraDeT-HTR，一个基于字素感知解码器专用Transformer的孟加拉语手写文本识别系统，通过结合字素分词器和合成数据预训练，在多个基准数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管孟加拉语是世界第六大常用语言，但其手写文本识别（HTR）系统仍严重不发达。孟加拉语文字的复杂性（连字、变音符号、多变的书写风格）以及标注数据集的稀缺性，使得这项任务极具挑战性。

Method: 研究人员提出了GraDeT-HTR系统，它基于字素感知解码器专用Transformer架构。为应对孟加拉语文字的独特挑战，该系统集成了一个基于字素的分词器，并证明其比传统子词分词器显著提高了识别准确率。模型首先在大规模合成数据上进行预训练，然后用真实的人工标注样本进行微调。

Result: GraDeT-HTR系统显著提高了识别准确率，并且在多个基准数据集上实现了最先进的性能。

Conclusion: GraDeT-HTR是一个资源高效的孟加拉语手写文本识别系统，其字素感知解码器专用Transformer架构，结合字素分词器和合成数据预训练，有效解决了孟加拉语文字的复杂性挑战，并达到了行业领先水平。

Abstract: Despite Bengali being the sixth most spoken language in the world,
handwritten text recognition (HTR) systems for Bengali remain severely
underdeveloped. The complexity of Bengali script--featuring conjuncts,
diacritics, and highly variable handwriting styles--combined with a scarcity of
annotated datasets makes this task particularly challenging. We present
GraDeT-HTR, a resource-efficient Bengali handwritten text recognition system
based on a Grapheme-aware Decoder-only Transformer architecture. To address the
unique challenges of Bengali script, we augment the performance of a
decoder-only transformer by integrating a grapheme-based tokenizer and
demonstrate that it significantly improves recognition accuracy compared to
conventional subword tokenizers. Our model is pretrained on large-scale
synthetic data and fine-tuned on real human-annotated samples, achieving
state-of-the-art performance on multiple benchmark datasets.

</details>


### [274] [GeoSVR: Taming Sparse Voxels for Geometrically Accurate Surface Reconstruction](https://arxiv.org/abs/2509.18090)
*Jiahe Li,Jiawei Zhang,Youmin Zhang,Xiao Bai,Jin Zheng,Xiaohan Yu,Lin Gu*

Main category: cs.CV

TL;DR: GeoSVR是一个显式的基于稀疏体素的框架，通过引入体素不确定性深度约束和稀疏体素表面正则化，实现了比现有方法更准确、更详细、更完整的表面重建，同时保持高效率。


<details>
  <summary>Details</summary>
Motivation: 近年来辐射场在表面重建方面取得了显著进展，但主流方法（如高斯溅射）受到表示瓶颈的限制。本文旨在探索和扩展稀疏体素在实现准确、详细和完整表面重建方面的潜力。

Method: GeoSVR是一个显式的基于体素的框架。它提出了：1. 体素不确定性深度约束（Voxel-Uncertainty Depth Constraint），利用单目深度线索和体素导向的不确定性来确保场景收敛和高精度几何。2. 稀疏体素表面正则化（Sparse Voxel Surface Regularization），旨在增强微小体素的几何一致性，并促进形成清晰准确的体素化表面。

Result: GeoSVR在几何精度、细节保留和重建完整性方面表现出优于现有方法的性能，并在各种挑战性场景中展现出卓越的能力，同时保持高效率。

Conclusion: 通过利用稀疏体素的潜力，并结合新颖的体素不确定性深度约束和稀疏体素表面正则化，GeoSVR成功解决了现有方法的局限性，实现了高精度、高细节和完整的表面重建。

Abstract: Reconstructing accurate surfaces with radiance fields has achieved remarkable
progress in recent years. However, prevailing approaches, primarily based on
Gaussian Splatting, are increasingly constrained by representational
bottlenecks. In this paper, we introduce GeoSVR, an explicit voxel-based
framework that explores and extends the under-investigated potential of sparse
voxels for achieving accurate, detailed, and complete surface reconstruction.
As strengths, sparse voxels support preserving the coverage completeness and
geometric clarity, while corresponding challenges also arise from absent scene
constraints and locality in surface refinement. To ensure correct scene
convergence, we first propose a Voxel-Uncertainty Depth Constraint that
maximizes the effect of monocular depth cues while presenting a voxel-oriented
uncertainty to avoid quality degradation, enabling effective and robust scene
constraints yet preserving highly accurate geometries. Subsequently, Sparse
Voxel Surface Regularization is designed to enhance geometric consistency for
tiny voxels and facilitate the voxel-based formation of sharp and accurate
surfaces. Extensive experiments demonstrate our superior performance compared
to existing methods across diverse challenging scenarios, excelling in
geometric accuracy, detail preservation, and reconstruction completeness while
maintaining high efficiency. Code is available at
https://github.com/Fictionarry/GeoSVR.

</details>


### [275] [ComposeMe: Attribute-Specific Image Prompts for Controllable Human Image Generation](https://arxiv.org/abs/2509.18092)
*Guocheng Gordon Qian,Daniil Ostashev,Egor Nemchinov,Avihay Assouline,Sergey Tulyakov,Kuan-Chieh Jackson Wang,Kfir Aberman*

Main category: cs.CV

TL;DR: 该研究提出了一种新的属性特异性图像提示范式，通过使用不同的参考图像来控制人类外观的各个方面（如头发、服装、身份），从而实现对高保真人类图像的细粒度、解耦控制。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像合成方法虽然强调身份保留，但缺乏模块化，并且未能提供对发型、服装等特定视觉属性的解耦控制，这在个性化人类图像生成中是一个核心挑战。

Method: 该方法引入了属性特异性图像提示范式，利用不同的参考图像来指导头发、服装和身份等人类外观特定方面的生成。这些输入被编码成属性特异性 token，并注入到预训练的文本到图像扩散模型中。为促进自然组合和鲁棒解耦，研究人员构建了一个包含多样姿态和表情的交叉参考训练数据集，并提出了一种多属性交叉参考训练策略，以鼓励模型在遵守身份和文本条件的同时，从错位的属性输入中生成忠实输出。

Result: 实验结果表明，该方法在准确遵循视觉和文本提示方面达到了最先进的性能。它能够实现对多个视觉因素的组合式和解耦式控制，甚至可以在单张图像中控制多个人。

Conclusion: 该框架通过将视觉提示与文本驱动生成相结合，为更具可配置性的人类图像合成铺平了道路。

Abstract: Generating high-fidelity images of humans with fine-grained control over
attributes such as hairstyle and clothing remains a core challenge in
personalized text-to-image synthesis. While prior methods emphasize identity
preservation from a reference image, they lack modularity and fail to provide
disentangled control over specific visual attributes. We introduce a new
paradigm for attribute-specific image prompting, in which distinct sets of
reference images are used to guide the generation of individual aspects of
human appearance, such as hair, clothing, and identity. Our method encodes
these inputs into attribute-specific tokens, which are injected into a
pre-trained text-to-image diffusion model. This enables compositional and
disentangled control over multiple visual factors, even across multiple people
within a single image. To promote natural composition and robust
disentanglement, we curate a cross-reference training dataset featuring
subjects in diverse poses and expressions, and propose a multi-attribute
cross-reference training strategy that encourages the model to generate
faithful outputs from misaligned attribute inputs while adhering to both
identity and textual conditioning. Extensive experiments show that our method
achieves state-of-the-art performance in accurately following both visual and
textual prompts. Our framework paves the way for more configurable human image
synthesis by combining visual prompting with text-driven generation. Webpage is
available at: https://snap-research.github.io/composeme/.

</details>


### [276] [Seg4Diff: Unveiling Open-Vocabulary Segmentation in Text-to-Image Diffusion Transformers](https://arxiv.org/abs/2509.18096)
*Chaehyun Kim,Heeseong Shin,Eunbeen Hong,Heeji Yoon,Anurag Arnab,Paul Hongsuck Seo,Sunghwan Hong,Seungryong Kim*

Main category: cs.CV

TL;DR: 本研究引入Seg4Diff框架，分析多模态扩散Transformer（MM-DiT）的注意力机制。我们发现一个“语义接地专家层”，能将文本与图像区域对齐并生成高质量语义分割掩码。通过轻量级微调，该层在分割和图像生成方面均有提升，表明语义分组是扩散Transformer的固有属性。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态扩散Transformer在图像生成中表现出色，但对于其注意力图如何以及在何处促进图像生成，我们缺乏详细的理解。

Method: 本文提出了Seg4Diff（Segmentation for Diffusion），一个系统性框架，用于分析MM-DiT的注意力结构，特别关注特定层如何从文本传播语义信息到图像。此外，通过使用带掩码标注的图像数据进行轻量级微调，以增强语义分组能力。

Result: 研究识别出一个“语义接地专家层”，即MM-DiT中一个能持续将文本token与空间连贯的图像区域对齐的特定模块，自然地生成高质量语义分割掩码。进一步的微调方案提升了这些层的语义分组能力，进而改善了分割性能和生成图像的保真度。

Conclusion: 研究结果表明，语义分组是扩散Transformer的一个新兴属性，并且可以被选择性地放大，以同时提升分割和生成性能，为连接视觉感知和生成的统一模型铺平了道路。

Abstract: Text-to-image diffusion models excel at translating language prompts into
photorealistic images by implicitly grounding textual concepts through their
cross-modal attention mechanisms. Recent multi-modal diffusion transformers
extend this by introducing joint self-attention over concatenated image and
text tokens, enabling richer and more scalable cross-modal alignment. However,
a detailed understanding of how and where these attention maps contribute to
image generation remains limited. In this paper, we introduce Seg4Diff
(Segmentation for Diffusion), a systematic framework for analyzing the
attention structures of MM-DiT, with a focus on how specific layers propagate
semantic information from text to image. Through comprehensive analysis, we
identify a semantic grounding expert layer, a specific MM-DiT block that
consistently aligns text tokens with spatially coherent image regions,
naturally producing high-quality semantic segmentation masks. We further
demonstrate that applying a lightweight fine-tuning scheme with mask-annotated
image data enhances the semantic grouping capabilities of these layers and
thereby improves both segmentation performance and generated image fidelity.
Our findings demonstrate that semantic grouping is an emergent property of
diffusion transformers and can be selectively amplified to advance both
segmentation and generation performance, paving the way for unified models that
bridge visual perception and generation.

</details>


### [277] [Preconditioned Deformation Grids](https://arxiv.org/abs/2509.18097)
*Julian Kaltheuner,Alexander Oebel,Hannah Droege,Patrick Stotko,Reinhard Klein*

Main category: cs.CV

TL;DR: 本文提出了一种名为“预处理变形网格”（Preconditioned Deformation Grids）的新技术，用于直接从非结构化点云序列中估计连贯的变形场，从而实现动态物体表面的高精度重建，解决了现有方法在准确性、平滑性和泛化能力上的局限。


<details>
  <summary>Details</summary>
Motivation: 现有动态表面重建方法需要多重正则化项或大量训练数据，这导致重建精度受损、过度平滑或对未见物体和运动的泛化能力差。本文旨在解决这些限制。

Method: 该方法通过多分辨率体素网格捕捉不同空间尺度的整体运动，实现更灵活的变形表示。结合基于梯度的优化中引入基于网格的Sobolev预处理，仅使用输入点云与演化模板网格之间的Chamfer损失即可获得精确变形。此外，为确保物体表面的时间一致性，加入了对网格边缘的弱等距损失，在不限制变形保真度的情况下补充了主要目标。

Result: 广泛的评估表明，与现有最先进技术相比，该方法取得了卓越的结果，尤其是在处理长序列时。

Conclusion: 本文提出的预处理变形网格技术能有效地从点云序列中估计动态变形场，实现高精度、时间一致的动态表面重建，克服了现有方法的不足。

Abstract: Dynamic surface reconstruction of objects from point cloud sequences is a
challenging field in computer graphics. Existing approaches either require
multiple regularization terms or extensive training data which, however, lead
to compromises in reconstruction accuracy as well as over-smoothing or poor
generalization to unseen objects and motions. To address these lim- itations,
we introduce Preconditioned Deformation Grids, a novel technique for estimating
coherent deformation fields directly from unstructured point cloud sequences
without requiring or forming explicit correspondences. Key to our approach is
the use of multi-resolution voxel grids that capture the overall motion at
varying spatial scales, enabling a more flexible deformation representation. In
conjunction with incorporating grid-based Sobolev preconditioning into
gradient-based optimization, we show that applying a Chamfer loss between the
input point clouds as well as to an evolving template mesh is sufficient to
obtain accurate deformations. To ensure temporal consistency along the object
surface, we include a weak isometry loss on mesh edges which complements the
main objective without constraining deformation fidelity. Extensive evaluations
demonstrate that our method achieves superior results, particularly for long
sequences, compared to state-of-the-art techniques.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [278] [On LLM-Based Scientific Inductive Reasoning Beyond Equations](https://arxiv.org/abs/2509.16226)
*Brian S. Lin,Jiaxin Yuan,Zihan Zhou,Shouli Wang,Shuo Wang,Cunliang Kong,Qi Shi,Yuxuan Li,Liner Yang,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: 本文提出了一个LLM科学归纳推理新任务和基准SIRBench-V1，旨在评估LLM在超越数学方程的科学场景下从有限示例中学习和应用潜在模式的能力，结果显示当前LLM在该任务上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）能力日益增强，如何使其在全新环境中从有限示例中学习底层模式并有效应用，是其归纳推理能力的核心问题。现有研究在超越数学方程的归纳推理方面，常强调规则设计而缺乏具体场景的落地，因此需要一个更贴近人类科学发现的评估方式。

Method: 受归纳推理与人类科学发现之间相似性的启发，本文提出了“基于LLM的超越方程科学归纳推理”任务，并引入了一个新基准SIRBench-V1，用于在科学背景下评估LLM的归纳推理能力。

Result: 实验结果表明，当前的LLM在这一新任务上仍然面临困难，突显了该任务的挑战性。

Conclusion: 该任务的难度和当前LLM的表现不足，强调了在该领域需要进一步的研究和发展，以提升LLM的科学归纳推理能力。

Abstract: As large language models (LLMs) increasingly exhibit human-like capabilities,
a fundamental question emerges: How can we enable LLMs to learn the underlying
patterns from limited examples in entirely novel environments and apply them
effectively? This question is central to the ability of LLMs in inductive
reasoning. Existing research on LLM-based inductive reasoning can be broadly
categorized based on whether the underlying rules are expressible via explicit
mathematical equations. However, many recent studies in the beyond-equations
category have emphasized rule design without grounding them in specific
scenarios. Inspired by the parallels between inductive reasoning and human
scientific discovery, we propose the task of LLM-Based Scientific Inductive
Reasoning Beyond Equations and introduce a new benchmark, SIRBench-V1, to
evaluate the inductive reasoning abilities of LLMs in scientific settings. Our
experimental results show that current LLMs still struggle with this task,
underscoring its difficulty and the need for further advancement in this area.

</details>


### [279] [REAMS: Reasoning Enhanced Algorithm for Maths Solving](https://arxiv.org/abs/2509.16241)
*Eishkaran Singh,Tanav Singh Bajaj,Siddharth Nayak*

Main category: cs.CL

TL;DR: 本文提出一种基于语言的零样本学习和数学推理方法，结合程序合成，显著提升了AI解决MIT、哥伦比亚大学等复杂数学问题的准确率，达到90.15%。


<details>
  <summary>Details</summary>
Motivation: 解决大学级别（特别是MIT和哥伦比亚大学课程及MATH数据集）的复杂数学问题对现有AI方法仍是重大挑战，传统方法表现不足。

Method: 引入一种基于语言的解决方案，利用零样本学习和数学推理来解决、解释和生成高级数学问题的解决方案。通过集成程序合成，该方法减少了对大规模训练数据的依赖。

Result: 该方法实现了90.15%的准确率，显著超越了先前81%的基准，并在自动化数学问题解决领域树立了新标准。

Conclusion: 这些发现强调了先进AI方法在应对和克服最复杂数学课程和数据集所带来的挑战方面的巨大潜力。

Abstract: The challenges of solving complex university-level mathematics problems,
particularly those from MIT, and Columbia University courses, and selected
tasks from the MATH dataset, remain a significant obstacle in the field of
artificial intelligence. Conventional methods have consistently fallen short in
this domain, highlighting the need for more advanced approaches. In this paper,
we introduce a language-based solution that leverages zero-shot learning and
mathematical reasoning to effectively solve, explain, and generate solutions
for these advanced math problems. By integrating program synthesis, our method
reduces reliance on large-scale training data while significantly improving
problem-solving accuracy. Our approach achieves an accuracy of 90.15%,
representing a substantial improvement over the previous benchmark of 81% and
setting a new standard in automated mathematical problem-solving. These
findings highlight the significant potential of advanced AI methodologies to
address and overcome the challenges presented by some of the most complex
mathematical courses and datasets.

</details>


### [280] [HausaMovieReview: A Benchmark Dataset for Sentiment Analysis in Low-Resource African Language](https://arxiv.org/abs/2509.16256)
*Asiya Ibrahim Zanga,Salisu Mamman Abdulrahman,Abubakar Ado,Abdulkadir Abubakar Bichi,Lukman Aliyu Jibril,Abdulmajid Babangida Umar,Alhassan Adamu,Shamsuddeen Hassan Muhammad,Bashir Salisu Abubakar*

Main category: cs.CL

TL;DR: 本文介绍了HausaMovieReview数据集，用于豪萨语情感分析，并发现决策树模型在低资源环境下优于微调的Transformer模型。


<details>
  <summary>Details</summary>
Motivation: 低资源语言的自然语言处理工具开发受到标注数据集稀缺的严重阻碍。

Method: 研究者构建了一个包含5000条豪萨语和英语混码YouTube评论的HausaMovieReview数据集，由三位独立标注者进行标注，Fleiss' Kappa得分为0.85。他们使用该数据集比较了经典模型（逻辑回归、决策树、K-近邻）和微调的Transformer模型（BERT、RoBERTa）。

Result: 决策树分类器表现最佳，准确率和F1分数分别为89.72%和89.60%，显著优于深度学习模型。研究表明，有效的特征工程可以使经典模型在低资源环境下达到先进性能。

Conclusion: 在低资源语言环境中，通过有效的特征工程，经典模型能够取得最先进的性能，为未来的研究奠定了坚实的基础。

Abstract: The development of Natural Language Processing (NLP) tools for low-resource
languages is critically hindered by the scarcity of annotated datasets. This
paper addresses this fundamental challenge by introducing HausaMovieReview, a
novel benchmark dataset comprising 5,000 YouTube comments in Hausa and
code-switched English. The dataset was meticulously annotated by three
independent annotators, demonstrating a robust agreement with a Fleiss' Kappa
score of 0.85 between annotators. We used this dataset to conduct a comparative
analysis of classical models (Logistic Regression, Decision Tree, K-Nearest
Neighbors) and fine-tuned transformer models (BERT and RoBERTa). Our results
reveal a key finding: the Decision Tree classifier, with an accuracy and
F1-score 89.72% and 89.60% respectively, significantly outperformed the deep
learning models. Our findings also provide a robust baseline, demonstrating
that effective feature engineering can enable classical models to achieve
state-of-the-art performance in low-resource contexts, thereby laying a solid
foundation for future research.
  Keywords: Hausa, Kannywood, Low-Resource Languages, NLP, Sentiment Analysis

</details>


### [281] [Gender and Political Bias in Large Language Models: A Demonstration Platform](https://arxiv.org/abs/2509.16264)
*Wenjie Lin,Hange Liu,Xutao Mao,Yingying Zhuang,Jingwei Shi,Xudong Han,Tianyu Shi,Jinrui Yang*

Main category: cs.CL

TL;DR: ParlAI Vote是一个交互式系统，用于探索欧洲议会辩论和投票，并测试大型语言模型（LLMs）在投票预测和偏见分析方面的表现。它整合了辩论、演讲、投票结果和人口统计数据，并揭示了当前LLMs的系统性性能偏见。


<details>
  <summary>Details</summary>
Motivation: 研究动机是提供一个平台，让用户能够探索欧洲议会辩论和投票数据，并测试LLMs在政治分析中的能力，特别是投票预测和偏见分析。它旨在连接辩论内容、演讲和投票结果，并纳入丰富的人口统计数据，以便进行深入分析。

Method: 该系统名为ParlAI Vote，是一个交互式平台。它连接了辩论主题、演讲内容和投票结果，并包含了性别、年龄、国家和政治团体等丰富的人口统计数据。用户可以浏览辩论、检查相关演讲、将真实的投票结果与前沿LLMs的预测进行比较，并按人口统计组查看错误细分。它通过可视化EuroParlVote基准及其核心任务（性别分类和投票预测）来展示结果。

Result: ParlAI Vote系统通过可视化展示，突出了最先进的LLMs在投票预测和偏见分析中存在的系统性性能偏见。该系统将数据、模型和可视化分析统一在一个界面中，降低了重现研究结果、审计模型行为和运行反事实场景的门槛。

Conclusion: 该系统支持对立法决策的研究、教育和公众参与，同时清晰地展示了当前LLMs在政治分析中的优势和局限性。它提供了一个强大的工具，用于理解LLMs在复杂社会政治任务中的表现，并促进对其行为的深入审计和探索。

Abstract: We present ParlAI Vote, an interactive system for exploring European
Parliament debates and votes, and for testing LLMs on vote prediction and bias
analysis. This platform connects debate topics, speeches, and roll-call
outcomes, and includes rich demographic data such as gender, age, country, and
political group. Users can browse debates, inspect linked speeches, compare
real voting outcomes with predictions from frontier LLMs, and view error
breakdowns by demographic group. Visualizing the EuroParlVote benchmark and its
core tasks of gender classification and vote prediction, ParlAI Vote highlights
systematic performance bias in state-of-the-art LLMs. The system unifies data,
models, and visual analytics in a single interface, lowering the barrier for
reproducing findings, auditing behavior, and running counterfactual scenarios.
It supports research, education, and public engagement with legislative
decision-making, while making clear both the strengths and the limitations of
current LLMs in political analysis.

</details>


### [282] [Language Modeling with Learned Meta-Tokens](https://arxiv.org/abs/2509.16278)
*Alok N. Shah,Khush Gupta,Keshav Ramji,Pratik Chaudhari*

Main category: cs.CL

TL;DR: 本文提出了一种使用元令牌（meta-tokens）和元注意力（meta-attention）机制来增强Transformer语言模型长上下文处理能力的方法。该方法在数据高效的预训练后，显著提升了模型在长距离依赖任务上的性能，并实现了高达2倍上下文窗口的长度泛化。


<details>
  <summary>Details</summary>
Motivation: 尽管现代基于Transformer的语言模型在多任务泛化方面取得了巨大成功，但它们在捕获其上下文窗口内的长距离依赖关系时常常表现不佳。

Method: 引入了在预训练期间注入的特殊元令牌，并设计了专用的元注意力机制来引导语言模型使用这些令牌。在修改后的GPT-2架构上进行预训练，该架构除了因果多头注意力外，还配备了元注意力。通过合成任务集评估其影响，并通过可视化模型内部（残差流）和信息论分析（速率-失真权衡）来研究行为和压缩质量。

Result: 在少于100B令牌的数据高效预训练后，利用元令牌和元注意力机制的语言模型在合成任务上表现出色。这些提升归因于元令牌锐化了位置编码，使其能够作为可训练的、基于内容的标志物，隐式压缩前置上下文并将其“缓存”在元令牌中。在推理时，元令牌指向相关上下文，即使在通过YaRN扩展后，也能将长度泛化能力提升至其上下文窗口的2倍。

Conclusion: 使用元令牌预训练语言模型提供了一种简单、数据高效的方法来增强长上下文语言模型的性能，同时为理解其长度泛化行为的本质提供了新的见解。

Abstract: While modern Transformer-based language models (LMs) have achieved major
success in multi-task generalization, they often struggle to capture long-range
dependencies within their context window. This work introduces a novel approach
using meta-tokens, special tokens injected during pre-training, along with a
dedicated meta-attention mechanism to guide LMs to use these tokens. We
pre-train a language model with a modified GPT-2 architecture equipped with
meta-attention in addition to causal multi-head attention, and study the impact
of these tokens on a suite of synthetic tasks. We find that data-efficient
language model pre-training on fewer than 100B tokens utilizing meta-tokens and
our meta-attention mechanism achieves strong performance on these tasks after
fine-tuning. We suggest that these gains arise due to the meta-tokens
sharpening the positional encoding. This enables them to operate as trainable,
content-based landmarks, implicitly compressing preceding context and "caching"
it in the meta-token. At inference-time, the meta-token points to relevant
context, facilitating length generalization up to 2$\times$ its context window,
even after extension with YaRN. We provide further evidence of these behaviors
by visualizing model internals to study the residual stream, and assessing the
compression quality by information-theoretic analysis on the rate-distortion
tradeoff. Our findings suggest that pre-training LMs with meta-tokens offers a
simple, data-efficient method to enhance long-context language modeling
performance, while introducing new insights into the nature of their behavior
towards length generalization.

</details>


### [283] [Overhearing LLM Agents: A Survey, Taxonomy, and Roadmap](https://arxiv.org/abs/2509.16325)
*Andrew Zhu,Chris Callison-Burch*

Main category: cs.CL

TL;DR: 本文提出并分析了一种名为“旁听代理”的新型LLM代理交互范式，它能在不中断用户对话的情况下，持续监控环境活动并适时提供上下文帮助，并建立了相关分类法和最佳实践。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理通常通过聊天界面直接协助用户，但这会中断用户的注意力。研究旨在探索一种替代范式，即“旁听代理”，使其能在不打扰用户的情况下，提供情境化的辅助。

Method: 研究首先将“旁听LLM代理”作为人机交互的一个独特范式进行分析。接着，通过对现有LLM驱动代理工作的调查和探索性HCI研究，建立了一个旁听代理交互和任务的分类法。最后，基于此分类法，制定了构建旁听代理系统的最佳实践列表。

Result: 本文首次提出了“旁听LLM代理”这一人机交互的独特范式；建立了一套旁听代理交互和任务的分类法；并为研究人员和开发者提供了构建旁听代理系统的最佳实践列表。

Conclusion: 该研究为AI助手在不中断用户注意力的情况下提供上下文帮助开辟了新方向，并概述了旁听范式中存在的研发空白和未来的研究机会。

Abstract: Imagine AI assistants that enhance conversations without interrupting them:
quietly providing relevant information during a medical consultation,
seamlessly preparing materials as teachers discuss lesson plans, or
unobtrusively scheduling meetings as colleagues debate calendars. While modern
conversational LLM agents directly assist human users with tasks through a chat
interface, we study this alternative paradigm for interacting with LLM agents,
which we call "overhearing agents." Rather than demanding the user's attention,
overhearing agents continuously monitor ambient activity and intervene only
when they can provide contextual assistance. In this paper, we present the
first analysis of overhearing LLM agents as a distinct paradigm in human-AI
interaction and establish a taxonomy of overhearing agent interactions and
tasks grounded in a survey of works on prior LLM-powered agents and exploratory
HCI studies. Based on this taxonomy, we create a list of best practices for
researchers and developers building overhearing agent systems. Finally, we
outline the remaining research gaps and reveal opportunities for future
research in the overhearing paradigm.

</details>


### [284] [HARE: an entity and relation centric evaluation framework for histopathology reports](https://arxiv.org/abs/2509.16326)
*Yunsoo Kim,Michal W. S. Ong,Alex Shavick,Honghan Wu,Adam P. Levine*

Main category: cs.CL

TL;DR: 本文提出了HARE（组织病理学自动化报告评估）框架，这是一个以实体和关系为中心的框架，用于评估组织病理学报告的临床质量。它包括一个基准数据集、命名实体识别（NER）模型、关系提取（RE）模型以及一个新颖的评估指标，并已开源。


<details>
  <summary>Details</summary>
Motivation: 医学领域自动化文本生成，特别是在组织病理学报告生成方面，其临床质量评估仍然是一个挑战，因为缺乏领域特定的评估指标。

Method: HARE框架由以下部分组成：1) 一个基准数据集，通过标注813份去识别化的临床诊断组织病理学报告和652份TCGA报告中的领域特定实体和关系构建；2) HARE-NER和HARE-RE模型，通过微调领域适应的语言模型GatorTronS开发；3) 一个新颖的HARE评估指标，通过对参考报告和生成报告之间的关键组织病理学实体和关系进行对齐，优先考虑临床相关内容。

Result: HARE-NER和HARE-RE模型在测试模型中取得了最高的F1分数（0.915）。所提出的HARE指标在与专家评估的相关性和回归方面，优于传统的ROUGE和Meteor指标，以及RadGraph-XL等放射学指标，甚至比第二好的方法（基于LLM的放射学报告评估器GREEN）在Pearson r、Spearman ρ、Kendall τ、R^2和RMSE上都有显著提升。

Conclusion: HARE框架为组织病理学报告生成提供了一个强大的质量改进和评估工具，通过发布HARE、数据集和模型，旨在促进该领域的发展。

Abstract: Medical domain automated text generation is an active area of research and
development; however, evaluating the clinical quality of generated reports
remains a challenge, especially in instances where domain-specific metrics are
lacking, e.g. histopathology. We propose HARE (Histopathology Automated Report
Evaluation), a novel entity and relation centric framework, composed of a
benchmark dataset, a named entity recognition (NER) model, a relation
extraction (RE) model, and a novel metric, which prioritizes clinically
relevant content by aligning critical histopathology entities and relations
between reference and generated reports. To develop the HARE benchmark, we
annotated 813 de-identified clinical diagnostic histopathology reports and 652
histopathology reports from The Cancer Genome Atlas (TCGA) with domain-specific
entities and relations. We fine-tuned GatorTronS, a domain-adapted language
model to develop HARE-NER and HARE-RE which achieved the highest overall
F1-score (0.915) among the tested models. The proposed HARE metric outperformed
traditional metrics including ROUGE and Meteor, as well as radiology metrics
such as RadGraph-XL, with the highest correlation and the best regression to
expert evaluations (higher than the second best method, GREEN, a large language
model based radiology report evaluator, by Pearson $r = 0.168$, Spearman $\rho
= 0.161$, Kendall $\tau = 0.123$, $R^2 = 0.176$, $RMSE = 0.018$). We release
HARE, datasets, and the models at https://github.com/knowlab/HARE to foster
advancements in histopathology report generation, providing a robust framework
for improving the quality of reports.

</details>


### [285] [RephQA: Evaluating Readability of Large Language Models in Public Health Question Answering](https://arxiv.org/abs/2509.16360)
*Weikang Qiu,Tinglin Huang,Ryan Rullo,Yucheng Kuang,Ali Maatouk,S. Raquel Ramos,Rex Ying*

Main category: cs.CL

TL;DR: 本研究引入RephQA基准来评估大型语言模型（LLMs）在公共卫生问答中的可读性。结果显示大多数LLMs未能达到可读性标准，且Token-adapted GRPO策略能有效提升其可读性，以构建更实用的公共卫生代理。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在解决复杂医疗问题上潜力巨大，且多数现有研究关注其准确性和推理能力，但开发有效医疗代理的一个重要瓶颈在于LLM生成回答的可读性，特别是对非医学背景人士清晰简洁地解释公共卫生问题的能力。

Method: 研究引入了RephQA基准，用于评估LLMs在公共卫生问答（QA）中的可读性。该基准包含533个经过专家评审的QA对，源自27个来源和13个主题。评估指标包括一个用于评估信息量的代理多项选择任务，以及两个可读性指标：Flesch-Kincaid年级水平和专业分数。研究评估了25个LLMs，并探索了四种可读性增强策略：标准提示、思维链提示、Group Relative Policy Optimization (GRPO) 及其Token-adapted变体。

Result: 对25个LLMs的评估显示，大多数未能达到可读性标准，凸显了推理能力与有效沟通之间的差距。在所探索的四种可读性增强策略中，Token-adapted GRPO取得了最佳结果，有效提升了LLM的可读性。

Conclusion: LLMs在公共卫生领域的回答可读性是一个显著问题，RephQA基准为评估这一能力提供了工具。Token-adapted GRPO策略是提高LLM公共卫生回答可读性的有效途径，这代表着朝着构建更实用和用户友好的公共卫生代理迈出了重要一步。

Abstract: Large Language Models (LLMs) hold promise in addressing complex medical
problems. However, while most prior studies focus on improving accuracy and
reasoning abilities, a significant bottleneck in developing effective
healthcare agents lies in the readability of LLM-generated responses,
specifically, their ability to answer public health problems clearly and simply
to people without medical backgrounds. In this work, we introduce RephQA, a
benchmark for evaluating the readability of LLMs in public health question
answering (QA). It contains 533 expert-reviewed QA pairs from 27 sources across
13 topics, and includes a proxy multiple-choice task to assess informativeness,
along with two readability metrics: Flesch-Kincaid grade level and professional
score. Evaluation of 25 LLMs reveals that most fail to meet readability
standards, highlighting a gap between reasoning and effective communication. To
address this, we explore four readability-enhancing strategies-standard
prompting, chain-of-thought prompting, Group Relative Policy Optimization
(GRPO), and a token-adapted variant. Token-adapted GRPO achieves the best
results, advancing the development of more practical and user-friendly public
health agents. These results represent a step toward building more practical
agents for public health.

</details>


### [286] [Whisper-UT: A Unified Translation Framework for Speech and Text](https://arxiv.org/abs/2509.16375)
*Cihan Xiao,Matthew Wiesner,Debashish Chakraborty,Reno Kriz,Keith Cunningham,Kenton Murray,Kevin Duh,Luis Tavarez-Arce,Paul McNamee,Sanjeev Khudanpur*

Main category: cs.CL

TL;DR: 本文提出了 Whisper-UT，一个统一高效的框架，利用轻量级适配器使编码器-解码器模型能无缝适应各种单/多模态任务，包括多模态机器翻译，并能通过提示和两阶段解码策略提升语音翻译性能。


<details>
  <summary>Details</summary>
Motivation: 编码器-解码器模型在语音和文本任务中表现出色，但如何高效地将其适应到多样化的单/多模态场景仍然是一个开放的挑战。

Method: 本文提出了 Whisper-UT 框架，利用轻量级适配器实现跨任务的无缝适应。它通过将 ASR 假设或真实转录作为提示，使系统能同时处理语音和源语言文本两种模态，并通过两阶段解码策略增强语音翻译性能。该方法在 Whisper 模型上进行了演示，并强调了跨模态和跨任务微调的有效性，无需三向并行数据。

Result: 该方法不仅使系统能够同时处理两种模态，还通过两阶段解码策略增强了语音翻译（ST）性能。跨模态和跨任务微调在不要求三向并行数据的情况下提高了性能。结果突出了所提出框架在多模态翻译方面的灵活性、效率和普遍适用性。

Conclusion: 所提出的 Whisper-UT 框架在多模态翻译方面表现出灵活性、效率和普遍适用性，能有效适应编码器-解码器模型到各种单/多模态任务。

Abstract: Encoder-decoder models have achieved remarkable success in speech and text
tasks, yet efficiently adapting these models to diverse uni/multi-modal
scenarios remains an open challenge. In this paper, we propose Whisper-UT, a
unified and efficient framework that leverages lightweight adapters to enable
seamless adaptation across tasks, including a multi-modal machine translation
(MMT) task that explicitly conditions translation on both speech and source
language text inputs. By incorporating ASR hypotheses or ground-truth
transcripts as prompts, this approach not only enables the system to process
both modalities simultaneously but also enhances speech translation (ST)
performance through a 2-stage decoding strategy. We demonstrate our methods
using the Whisper model, though in principle they are general and could be
applied to similar multitask models. We highlight the effectiveness of
cross-modal and cross-task fine-tuning, which improves performance without
requiring 3-way parallel data. Our results underscore the flexibility,
efficiency, and general applicability of the proposed framework for multi-modal
translation.

</details>


### [287] [Evaluating Behavioral Alignment in Conflict Dialogue: A Multi-Dimensional Comparison of LLM Agents and Humans](https://arxiv.org/abs/2509.16394)
*Deuksin Kwon,Kaleen Shrestha,Bin Han,Elena Hayoung Lee,Gale Lucas*

Main category: cs.CL

TL;DR: 本研究评估了在对抗性争议解决中，通过性格提示的大语言模型（LLMs）模拟人类行为的对齐程度，发现GPT-4.1在语言和情感上表现最佳，Claude-3.7-Sonnet在策略上表现最佳，但仍存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）越来越多地应用于社交复杂、交互驱动的任务，但它们在情感和策略复杂情境中模拟人类行为的能力仍未得到充分探索。

Method: 研究通过模拟包含谈判的多轮冲突对话来评估LLMs的行为对齐性。每个LLM都根据匹配的“五大性格特质”（Five-Factor personality profile）进行引导，以控制个体差异并增强真实性。评估维度包括语言风格、情感表达（如愤怒动态）和策略行为。

Result: GPT-4.1在语言风格和情感动态方面与人类对齐度最高，而Claude-3.7-Sonnet在策略行为方面表现最佳。然而，LLMs与人类行为之间仍存在显著的对齐差距。

Conclusion: 本研究为LLMs在社交复杂交互中与人类对齐建立了一个基准，强调了在对话建模中人格条件化的潜力和局限性。

Abstract: Large Language Models (LLMs) are increasingly deployed in socially complex,
interaction-driven tasks, yet their ability to mirror human behavior in
emotionally and strategically complex contexts remains underexplored. This
study assesses the behavioral alignment of personality-prompted LLMs in
adversarial dispute resolution by simulating multi-turn conflict dialogues that
incorporate negotiation. Each LLM is guided by a matched Five-Factor
personality profile to control for individual variation and enhance realism. We
evaluate alignment across three dimensions: linguistic style, emotional
expression (e.g., anger dynamics), and strategic behavior. GPT-4.1 achieves the
closest alignment with humans in linguistic style and emotional dynamics, while
Claude-3.7-Sonnet best reflects strategic behavior. Nonetheless, substantial
alignment gaps persist. Our findings establish a benchmark for alignment
between LLMs and humans in socially complex interactions, underscoring both the
promise and the limitations of personality conditioning in dialogue modeling.

</details>


### [288] ['Rich Dad, Poor Lad': How do Large Language Models Contextualize Socioeconomic Factors in College Admission ?](https://arxiv.org/abs/2509.16400)
*Huy Nghiem,Phuong-Anh Nguyen-Le,John Prindle,Rachel Rudinger,Hal Daumé III*

Main category: cs.CL

TL;DR: 该研究大规模审计了LLMs在大学招生中对社会经济地位（SES）的处理，发现LLMs一致性地偏爱低SES申请者，且解释性模式（System 2）会放大这种倾向，同时提出了DPAF双过程审计框架。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）日益参与高风险领域，但它们如何对社会敏感决策进行推理仍未得到充分探索。

Method: 研究采用认知科学启发的双过程框架，构建了一个包含30,000个基于真实世界相关性的合成申请者数据集。使用500万个提示，在两种模式下（快速决策的System 1和基于解释的System 2）审计了4个开源LLMs（Qwen 2, Mistral v0.3, Gemma 2, Llama 3.1）。最后提出了DPAF（双过程审计框架）以探测LLMs的推理行为。

Result: LLMs持续偏爱低社会经济地位（SES）的申请者，即使在控制了学业表现后也是如此。System 2模式通过明确援引SES作为补偿性理由来放大这种倾向，这突显了LLMs作为决策者的潜力和不稳定性。

Conclusion: LLMs在大学招生等敏感决策中表现出对低SES申请者的偏好，尤其在提供解释时这种偏好更为显著。这既显示了LLMs的潜在用途，也揭示了其决策过程中的不稳定性。研究提出了DPAF框架以进一步审计LLMs在敏感应用中的推理行为。

Abstract: Large Language Models (LLMs) are increasingly involved in high-stakes
domains, yet how they reason about socially sensitive decisions remains
underexplored. We present a large-scale audit of LLMs' treatment of
socioeconomic status (SES) in college admissions decisions using a novel
dual-process framework inspired by cognitive science. Leveraging a synthetic
dataset of 30,000 applicant profiles grounded in real-world correlations, we
prompt 4 open-source LLMs (Qwen 2, Mistral v0.3, Gemma 2, Llama 3.1) under 2
modes: a fast, decision-only setup (System 1) and a slower, explanation-based
setup (System 2). Results from 5 million prompts reveal that LLMs consistently
favor low-SES applicants -- even when controlling for academic performance --
and that System 2 amplifies this tendency by explicitly invoking SES as
compensatory justification, highlighting both their potential and volatility as
decision-makers. We then propose DPAF, a dual-process audit framework to probe
LLMs' reasoning behaviors in sensitive applications.

</details>


### [289] [Pico: A Modular Framework for Hypothesis-Driven Small Language Model Research](https://arxiv.org/abs/2509.16413)
*Richard Diehl Martinez,David Demitri Africa,Yuval Weiss,Suchir Salhan,Ryan Daniels,Paula Buttery*

Main category: cs.CL

TL;DR: Pico是一个轻量级、模块化的框架，旨在帮助研究人员系统地、以假设驱动的方式开发和改进中小型语言模型，使其设计过程更加科学。


<details>
  <summary>Details</summary>
Motivation: 目前中小型语言模型（LM）的构建更像是一门艺术而非科学，许多设计选择的原因尚不明确。对于参数预算紧张的小型LM，这种不确定性尤为限制，因为每个决策都至关重要，但研究人员缺乏系统、科学的方法来测试和改进新想法。

Method: 本文介绍了Pico框架，它由两个库组成，提供了一个实用的沙盒环境。研究人员可以在此对模型的架构或训练过程进行有针对性的修改，并直接观察其对模型行为的影响。为了支持可复现的实验，Pico还发布了一套在标准化条件下训练的基线模型（pico-decoder）。

Result: 案例研究表明，Pico能够有效支持中小型语言模型的迭代设计和分析过程。

Conclusion: Pico框架为中小型语言模型的开发提供了一个系统化、假设驱动的研究工具，有助于将这一过程从“艺术”转变为更具“科学性”的实践。

Abstract: Building language models (LMs), especially small and medium ones, remains
more art than science. While large LMs often improve by sheer scale, it is
still unclear why many design choices work. For small LMs, this uncertainty is
more limiting: tight parameter budgets make each decision critical, yet
researchers still lack systematic, scientific ways to test and refine new
ideas.
  We introduce Pico, a lightweight, modular framework that enables systematic,
hypothesis-driven research for small and medium-scale language model
development. Pico consists of two libraries that together provide a practical
sandbox where researchers can make targeted changes to a model's architecture
or training procedures and directly observe their effects on the model's
behavior. To support reproducible experimentation, we also release a suite of
baseline models, pico-decoder, trained under standardized conditions and
open-sourced for the community. Case studies highlight how Pico can support
iterative small LM design and analysis.

</details>


### [290] [Evaluating CxG Generalisation in LLMs via Construction-Based NLI Fine Tuning](https://arxiv.org/abs/2509.16422)
*Tom Mackintosh,Harish Tayyar Madabushi,Claire Bonial*

Main category: cs.CL

TL;DR: 该研究评估了大型语言模型（LLMs）学习深层构式语法形式-意义映射的能力，引入了ConTest-NLI基准，发现LLMs在对抗性数据和示意性模式上存在显著的抽象能力差距，并提出了一个可扩展的评估框架。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型（LLMs）学习构式语法所定义的深层形式-意义映射的能力。

Method: 引入了ConTest-NLI基准，包含8万个句子，涵盖八种英语构式，从高度词汇化到高度示意化。通过模板化和模型在环（model-in-the-loop）过滤器生成多样化的合成NLI三元组，以确保挑战性和标签可靠性。

Result: 领先的LLMs在自然语料上的零样本测试准确率为88%，但在对抗性数据上准确率下降了24%（至64%），其中示意性模式最难。在ConTest-NLI子集上进行微调可带来高达9%的改进。

Conclusion: 当前LLMs在抽象能力上存在持续的差距。该研究提供了一个可扩展的框架，用于评估LLMs基于构式信息的学习能力。

Abstract: We probe large language models' ability to learn deep form-meaning mappings
as defined by construction grammars. We introduce the ConTest-NLI benchmark of
80k sentences covering eight English constructions from highly lexicalized to
highly schematic. Our pipeline generates diverse synthetic NLI triples via
templating and the application of a model-in-the-loop filter. This provides
aspects of human validation to ensure challenge and label reliability.
Zero-shot tests on leading LLMs reveal a 24% drop in accuracy between
naturalistic (88%) and adversarial data (64%), with schematic patterns proving
hardest. Fine-tuning on a subset of ConTest-NLI yields up to 9% improvement,
yet our results highlight persistent abstraction gaps in current LLMs and offer
a scalable framework for evaluating construction-informed learning.

</details>


### [291] [PersonaMatrix: A Recipe for Persona-Aware Evaluation of Legal Summarization](https://arxiv.org/abs/2509.16449)
*Tsz Fung Pang,Maryam Berijanian,Thomas Orth,Breanna Shi,Charlotte S. Alexander*

Main category: cs.CL

TL;DR: 本文提出了PersonaMatrix，一个针对法律摘要的以角色为中心的评估框架，以满足不同用户（包括法律专家和非专家）的需求，并引入了相应的试点数据集和评估指标。


<details>
  <summary>Details</summary>
Motivation: 法律文件冗长、密集且难以理解，现有的自动文档摘要评估器忽视了不同用户和利益相关者的多样化需求，缺乏能够兼顾诉讼律师的技术性和普通公众可访问性的工具。

Method: 引入了PersonaMatrix，一个按角色-按标准评估框架，通过六个不同角色的视角（包括法律和非法律用户）对摘要进行评分。同时，创建了一个美国民权案件摘要的受控维度转换试点数据集，该数据集在深度、可访问性和程序细节上有所不同。此外，还引入了多样性覆盖指数（DCI）来揭示以角色为中心和不以角色为中心的评判之间法律摘要最优解的差异。

Result: PersonaMatrix框架能够从不同用户角色的角度评估摘要；DCI揭示了以用户角色为导向的评判与不以用户角色为导向的评判之间在法律摘要最优解上的差异。

Conclusion: 这项工作有助于改进面向专家和非专家的法律AI摘要系统，从而有可能提高法律知识的可及性。相关代码和数据已公开可用。

Abstract: Legal documents are often long, dense, and difficult to comprehend, not only
for laypeople but also for legal experts. While automated document
summarization has great potential to improve access to legal knowledge,
prevailing task-based evaluators overlook divergent user and stakeholder needs.
Tool development is needed to encompass the technicality of a case summary for
a litigator yet be accessible for a self-help public researching for their
lawsuit. We introduce PersonaMatrix, a persona-by-criterion evaluation
framework that scores summaries through the lens of six personas, including
legal and non-legal users. We also introduce a controlled dimension-shifted
pilot dataset of U.S. civil rights case summaries that varies along depth,
accessibility, and procedural detail as well as Diversity-Coverage Index (DCI)
to expose divergent optima of legal summary between persona-aware and
persona-agnostic judges. This work enables refinement of legal AI summarization
systems for both expert and non-expert users, with the potential to increase
access to legal knowledge. The code base and data are publicly available in
GitHub.

</details>


### [292] [Implicit Behavioral Alignment of Language Agents in High-Stakes Crowd Simulations](https://arxiv.org/abs/2509.16457)
*Yunzhe Wang,Gale M. Lucas,Burcin Becerik-Gerber,Volkan Ustun*

Main category: cs.CL

TL;DR: 该研究提出了“人格-环境行为对齐”（PEBA）理论框架和基于大语言模型的优化算法PersonaEvolve（PEvo），以解决生成式智能体在社会模拟中存在的“行为真实性差距”，显著提升了高风险社会模拟的行为真实性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 生成式智能体在社会模拟中表现出的行为常常偏离专家预期和真实世界数据，即存在“行为真实性差距”。这种偏差限制了其在人际训练和全球政策制定等变革性应用中的可靠性，因此需要一种方法来弥合这一差距。

Method: 该研究引入了“人格-环境行为对齐”（PEBA）理论框架，将其表述为一个基于勒温行为方程（行为是人与环境的函数）的分布匹配问题。在此基础上，提出了一种名为PersonaEvolve（PEvo）的基于大语言模型（LLM）的优化算法。PEvo通过迭代地精炼智能体的人格，使其集体行为在特定环境背景下与真实的专家基准对齐。该方法在一个自主开发的活跃射击事件模拟中进行了验证。

Result: 与无干预情况相比，PEvo将行为分布分歧平均减少了84%；与显式指令基线相比，性能提升了34%。此外，PEvo精炼后的人格能够泛化到新的、相关的模拟场景中。

Conclusion: PEvo方法极大地增强了高风险社会模拟的行为真实性和可靠性。更广泛地，PEBA-PEvo框架为开发可信赖的LLM驱动社会模拟提供了一种原则性方法。

Abstract: Language-driven generative agents have enabled large-scale social simulations
with transformative uses, from interpersonal training to aiding global
policy-making. However, recent studies indicate that generative agent behaviors
often deviate from expert expectations and real-world data--a phenomenon we
term the Behavior-Realism Gap. To address this, we introduce a theoretical
framework called Persona-Environment Behavioral Alignment (PEBA), formulated as
a distribution matching problem grounded in Lewin's behavior equation stating
that behavior is a function of the person and their environment. Leveraging
PEBA, we propose PersonaEvolve (PEvo), an LLM-based optimization algorithm that
iteratively refines agent personas, implicitly aligning their collective
behaviors with realistic expert benchmarks within a specified environmental
context. We validate PEvo in an active shooter incident simulation we
developed, achieving an 84% average reduction in distributional divergence
compared to no steering and a 34% improvement over explicit instruction
baselines. Results also show PEvo-refined personas generalize to novel, related
simulation scenarios. Our method greatly enhances behavioral realism and
reliability in high-stakes social simulations. More broadly, the PEBA-PEvo
framework provides a principled approach to developing trustworthy LLM-driven
social simulations.

</details>


### [293] [Intrinsic Meets Extrinsic Fairness: Assessing the Downstream Impact of Bias Mitigation in Large Language Models](https://arxiv.org/abs/2509.16462)
*'Mina Arzaghi','Alireza Dehghanpour Farashah','Florian Carichon',' Golnoosh Farnadi'*

Main category: cs.CL

TL;DR: 本研究实证调查了大型语言模型（LLMs）的内在偏见如何影响下游任务的公平性，并发现通过概念遗忘进行内在偏见缓解可显著提高下游任务的公平性，同时不损害准确性。


<details>
  <summary>Details</summary>
Motivation: 先前的研究质疑LLMs的内在偏见是否影响下游任务的公平性，本工作旨在通过实证研究来探究这种联系。

Method: 提出了一个统一的评估框架，用于比较通过概念遗忘进行的内在偏见缓解与通过反事实数据增强（CDA）进行的外在偏见缓解。研究在薪资预测、就业状况和信用评估等真实世界金融分类任务中，使用三个开源LLM，评估其作为冻结嵌入提取器和微调分类器的性能。

Result: 通过遗忘进行的内在偏见缓解将内在性别偏见降低了高达94.9%，同时将人口均等性等下游任务公平性指标提高了高达82%，且未损害准确性。

Conclusion: 本框架为偏见缓解工作在何处最有效提供了实用指导，并强调了在下游部署之前进行早期阶段缓解的重要性。

Abstract: Large Language Models (LLMs) exhibit socio-economic biases that can propagate
into downstream tasks. While prior studies have questioned whether intrinsic
bias in LLMs affects fairness at the downstream task level, this work
empirically investigates the connection. We present a unified evaluation
framework to compare intrinsic bias mitigation via concept unlearning with
extrinsic bias mitigation via counterfactual data augmentation (CDA). We
examine this relationship through real-world financial classification tasks,
including salary prediction, employment status, and creditworthiness
assessment. Using three open-source LLMs, we evaluate models both as frozen
embedding extractors and as fine-tuned classifiers. Our results show that
intrinsic bias mitigation through unlearning reduces intrinsic gender bias by
up to 94.9%, while also improving downstream task fairness metrics, such as
demographic parity by up to 82%, without compromising accuracy. Our framework
offers practical guidance on where mitigation efforts can be most effective and
highlights the importance of applying early-stage mitigation before downstream
deployment.

</details>


### [294] [Computational Analysis of Conversation Dynamics through Participant Responsivity](https://arxiv.org/abs/2509.16464)
*Margaret Hughes,Brandon Roy,Elinor Poole-Dayan,Deb Roy,Jad Kabbara*

Main category: cs.CL

TL;DR: 本文提出了一种基于“回应性”概念来衡量对话质量的方法，通过语义相似度和大型语言模型（LLM）量化回应性，并进一步构建对话层面的指标，以表征和区分不同类型的对话。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注话语中的毒性和两极分化，而较少探索如何表征亲社会和建设性对话的特点。本研究旨在填补这一空白，开发一种量化对话质量的方法。

Method: 1. 引入“回应性”概念，即一个发言是否回应了前一个发言。2. 开发两种量化回应性的方法：a) 发言间的语义相似度；b) 利用大型语言模型（LLM）识别发言间的关系。3. 通过人工标注的对话数据集评估这两种方法。4. 选取性能更优的LLM方法，进一步区分回应的实质性。5. 基于回应性链接，开发对话层面的衍生指标，以表征对话的不同方面。

Result: 1. 成功开发并评估了通过语义相似度和LLM量化回应性的方法。2. LLM方法表现更优，并被用于进一步分析回应的性质。3. 成功开发了对话层面的衍生指标。4. 这些衍生指标能够支持对多种不同对话进行有意义的表征和区分。

Conclusion: 回应性是对话的一个基本方面。通过从回应性链接中导出的对话层面指标，可以有效地表征和区分各种对话语篇，为理解建设性对话提供新的视角。

Abstract: Growing literature explores toxicity and polarization in discourse, with
comparatively less work on characterizing what makes dialogue prosocial and
constructive. We explore conversational discourse and investigate a method for
characterizing its quality built upon the notion of ``responsivity'' -- whether
one person's conversational turn is responding to a preceding turn. We develop
and evaluate methods for quantifying responsivity -- first through semantic
similarity of speaker turns, and second by leveraging state-of-the-art large
language models (LLMs) to identify the relation between two speaker turns. We
evaluate both methods against a ground truth set of human-annotated
conversations. Furthermore, selecting the better performing LLM-based approach,
we characterize the nature of the response -- whether it responded to that
preceding turn in a substantive way or not.
  We view these responsivity links as a fundamental aspect of dialogue but note
that conversations can exhibit significantly different responsivity structures.
Accordingly, we then develop conversation-level derived metrics to address
various aspects of conversational discourse. We use these derived metrics to
explore other conversations and show that they support meaningful
characterizations and differentiations across a diverse collection of
conversations.

</details>


### [295] [The Oracle Has Spoken: A Multi-Aspect Evaluation of Dialogue in Pythia](https://arxiv.org/abs/2509.16487)
*Zixun Chen,Petr Babkin,Akshat Gupta,Gopala Anumanchipalli,Xiaomo Liu*

Main category: cs.CL

TL;DR: 该研究使用基于语言学理论的细粒度指标，探究大型语言模型对话行为在后训练阶段（模型大小和微调）如何演变，发现模型大小影响有限，微调效果迅速饱和，并对现有指标的区分度和可靠性提出质疑。


<details>
  <summary>Details</summary>
Motivation: 对话是大型语言模型的标志性能力，但很少有研究真正区分出在后训练过程中支撑对话行为出现的具体要素。

Method: 采用一套全面的、基于模型且受语言学理论启发的细粒度对话指标；评估预训练Pythia模型在不同模型大小和会话数据集监督微调后的性能变化；进行额外的分数分布、指标相关性和生成响应中术语频率分析，以解释观察结果。

Result: 原始模型大小对大多数指标影响轻微；微调能迅速使除最小模型外的所有模型分数饱和；许多指标显示出非常相似的趋势，特别是当它们都源于同一个评估器模型时，这引发了对其衡量特定维度的可靠性疑问。

Conclusion: 模型大小对对话行为的影响有限，而微调至关重要但效果迅速饱和。研究结果对当前基于模型的对话指标在区分和可靠性方面提出了质疑，尤其当这些指标来源于同一评估器模型时。

Abstract: Dialogue is one of the landmark abilities of large language models (LLMs).
Despite its ubiquity, few studies actually distinguish specific ingredients
underpinning dialogue behavior emerging during post-training. We employ a
comprehensive suite of model-based metrics, each targeting a distinct
fine-grained aspect of dialogue, motivated by linguistic theory. We evaluate
how the performance of pre-trained Pythia models changes with respect to each
of those dimensions, depending on model size and as a result of supervised
fine-tuning on conversational datasets. We observe only a mild impact of raw
model size on most metrics, whereas fine-tuning quickly saturates the scores
for all but the smallest models tested. Somewhat contrary to our expectations,
many metrics show very similar trends, especially if they are all rooted in the
same evaluator model, which raises the question of their reliability in
measuring a specific dimension. To that end, we conduct additional analyses of
score distributions, metric correlations, and term frequencies in generated
responses to help explain our observations.

</details>


### [296] [Can an Individual Manipulate the Collective Decisions of Multi-Agents?](https://arxiv.org/abs/2509.16494)
*Fengyuan Liu,Rui Zhao,Shuo Chen,Guohao Li,Philip Torr,Lei Han,Jindong Gu*

Main category: cs.CL

TL;DR: 研究了多智能体LLM系统中，攻击者仅了解一个智能体时如何误导集体决策的问题，并提出了M-Spoiler框架来生成对抗样本，证实了这种攻击的有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管单个大型语言模型（LLM）能力强大，多智能体系统通过协作能增强决策和推理能力，但单个LLM存在漏洞。一个关键问题是：如果攻击者只了解多智能体系统中的一个智能体，他们是否仍能生成对抗样本来误导集体决策？

Method: 将问题建模为不完全信息博弈。提出了M-Spoiler框架，通过模拟多智能体系统内的智能体交互来生成对抗样本。M-Spoiler引入了一个“顽固智能体”来模拟目标系统中智能体的潜在顽固反应，从而优化对抗样本，使其能有效操纵目标智能体并误导系统的协作决策过程。

Result: 广泛的实验证实了仅了解一个智能体在多智能体系统中带来的风险，并展示了M-Spoiler框架的有效性。研究还探索了几种防御机制，但M-Spoiler攻击框架仍然比基线方法更强大。

Conclusion: 研究结果强调了多智能体系统中，仅通过了解单个智能体即可对集体决策构成威胁。鉴于M-Spoiler攻击的强大性，迫切需要进一步研究防御策略来应对此类攻击。

Abstract: Individual Large Language Models (LLMs) have demonstrated significant
capabilities across various domains, such as healthcare and law. Recent studies
also show that coordinated multi-agent systems exhibit enhanced decision-making
and reasoning abilities through collaboration. However, due to the
vulnerabilities of individual LLMs and the difficulty of accessing all agents
in a multi-agent system, a key question arises: If attackers only know one
agent, could they still generate adversarial samples capable of misleading the
collective decision? To explore this question, we formulate it as a game with
incomplete information, where attackers know only one target agent and lack
knowledge of the other agents in the system. With this formulation, we propose
M-Spoiler, a framework that simulates agent interactions within a multi-agent
system to generate adversarial samples. These samples are then used to
manipulate the target agent in the target system, misleading the system's
collaborative decision-making process. More specifically, M-Spoiler introduces
a stubborn agent that actively aids in optimizing adversarial samples by
simulating potential stubborn responses from agents in the target system. This
enhances the effectiveness of the generated adversarial samples in misleading
the system. Through extensive experiments across various tasks, our findings
confirm the risks posed by the knowledge of an individual agent in multi-agent
systems and demonstrate the effectiveness of our framework. We also explore
several defense mechanisms, showing that our proposed attack framework remains
more potent than baselines, underscoring the need for further research into
defensive strategies.

</details>


### [297] [AIPsychoBench: Understanding the Psychometric Differences between LLMs and Humans](https://arxiv.org/abs/2509.16530)
*Wei Xie,Shuoyoucheng Ma,Zhenhua Wang,Enze Wang,Kai Chen,Xiaobing Sun,Baosheng Wang*

Main category: cs.CL

TL;DR: 本文提出AIPsychoBench，一个专门用于评估大型语言模型（LLM）心理特性的基准测试。它通过轻量级角色扮演提示绕过对齐，显著提高了有效响应率并降低了偏见，首次全面揭示了语言对LLM心理测量的影响。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）因其类人智能而引发对其可靠性的担忧，因为其大规模神经网络难以解释。现有研究试图借鉴人类心理学概念来评估LLM的心理测量特性，但由于LLM与人类的根本差异，导致直接重用人类量表时拒绝率高，并且无法测量不同语言环境下LLM心理特性的变化。

Method: 本文引入了AIPsychoBench，一个专门为评估LLM心理特性量身定制的基准测试。它采用轻量级角色扮演提示来绕过LLM的对齐机制，并支持测量不同语言下LLM心理特性的变化。

Result: AIPsychoBench使用的角色扮演提示将平均有效响应率从70.12%提高到90.40%。同时，其平均偏见（正向3.3%，负向2.1%）显著低于传统越狱提示导致的偏见（分别为9.8%和6.9%）。此外，在112个心理测量子类别中，有43个子类别中七种语言与英语相比的分数偏差在5%到20.2%之间，首次提供了语言对LLM心理测量影响的全面证据。

Conclusion: AIPsychoBench成功解决了现有方法在评估LLM心理特性方面的局限性，通过创新的提示方法提高了评估效率和准确性，并首次揭示了语言对LLM心理测量结果的显著影响，为理解LLM的内在特性提供了新的视角。

Abstract: Large Language Models (LLMs) with hundreds of billions of parameters have
exhibited human-like intelligence by learning from vast amounts of
internet-scale data. However, the uninterpretability of large-scale neural
networks raises concerns about the reliability of LLM. Studies have attempted
to assess the psychometric properties of LLMs by borrowing concepts from human
psychology to enhance their interpretability, but they fail to account for the
fundamental differences between LLMs and humans. This results in high rejection
rates when human scales are reused directly. Furthermore, these scales do not
support the measurement of LLM psychological property variations in different
languages. This paper introduces AIPsychoBench, a specialized benchmark
tailored to assess the psychological properties of LLM. It uses a lightweight
role-playing prompt to bypass LLM alignment, improving the average effective
response rate from 70.12% to 90.40%. Meanwhile, the average biases are only
3.3% (positive) and 2.1% (negative), which are significantly lower than the
biases of 9.8% and 6.9%, respectively, caused by traditional jailbreak prompts.
Furthermore, among the total of 112 psychometric subcategories, the score
deviations for seven languages compared to English ranged from 5% to 20.2% in
43 subcategories, providing the first comprehensive evidence of the linguistic
impact on the psychometrics of LLM.

</details>


### [298] [Leveraging Multilingual Training for Authorship Representation: Enhancing Generalization across Languages and Domains](https://arxiv.org/abs/2509.16531)
*Junghwan Kim,Haotian Zhang,David Jurgens*

Main category: cs.CL

TL;DR: 该研究引入了一种新颖的多语言作者表示（AR）学习方法，通过概率内容掩码和语言感知批处理，在36种语言上显著优于单语言基线，并展示了更强的跨语言和跨领域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 先前的作者表示（AR）学习研究主要集中在单语言（多为英语）环境中，多语言AR模型的潜在优势尚未得到充分探索。

Method: 提出了一种多语言AR学习新方法，包含两项关键创新：1) 概率内容掩码，鼓励模型关注风格指示性词汇而非内容特定词汇；2) 语言感知批处理，通过减少跨语言干扰来改进对比学习。模型在超过450万作者、36种语言和13个领域的数据上进行训练。

Result: 在22种非英语语言中的21种上，模型始终优于单语言基线，平均Recall@8提升4.85%，在单一语言中最大提升15.91%。此外，与仅在英语上训练的单语言模型相比，该模型展现出更强的跨语言和跨领域泛化能力。分析证实了所提出两种技术的有效性。

Conclusion: 所提出的概率内容掩码和语言感知批处理技术在多语言作者表示学习中发挥了关键作用，显著提升了模型的性能，并使其在跨语言和跨领域任务中表现出更强的泛化能力。

Abstract: Authorship representation (AR) learning, which models an author's unique
writing style, has demonstrated strong performance in authorship attribution
tasks. However, prior research has primarily focused on monolingual
settings-mostly in English-leaving the potential benefits of multilingual AR
models underexplored. We introduce a novel method for multilingual AR learning
that incorporates two key innovations: probabilistic content masking, which
encourages the model to focus on stylistically indicative words rather than
content-specific words, and language-aware batching, which improves contrastive
learning by reducing cross-lingual interference. Our model is trained on over
4.5 million authors across 36 languages and 13 domains. It consistently
outperforms monolingual baselines in 21 out of 22 non-English languages,
achieving an average Recall@8 improvement of 4.85%, with a maximum gain of
15.91% in a single language. Furthermore, it exhibits stronger cross-lingual
and cross-domain generalization compared to a monolingual model trained solely
on English. Our analysis confirms the effectiveness of both proposed
techniques, highlighting their critical roles in the model's improved
performance.

</details>


### [299] [Challenging the Evaluator: LLM Sycophancy Under User Rebuttal](https://arxiv.org/abs/2509.16533)
*Sungwon Kim,Daniel Khashabi*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）在后续对话中易受用户反驳影响而表现出奉承，但在同时评估冲突论点时表现良好。本研究通过实证测试发现，对话框架、用户反驳的详细程度和反馈的正式程度是影响LLM奉承行为的关键因素。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）一方面在后续对话中表现出奉承，容易认同用户观点，另一方面却被成功用作评估代理。这种矛盾促使研究者探究：为什么LLMs在后续对话中受挑战时表现出奉承，但在同时评估冲突论点时却表现良好？

Method: 研究通过改变关键的互动模式，对LLMs在不同情景下的行为进行了实证测试。

Result: 1. LLMs在用户反驳以后续提问形式出现时，比同时呈现两种观点供评估时，更容易认同用户的反驳。
2. 即使用户反驳的结论不正确，但如果包含详细推理，LLMs更容易被说服。
3. LLMs更容易受到随意措辞的反馈影响，而非正式批判，即使随意输入缺乏正当理由。

Conclusion: 研究结果强调了在不考虑对话框架的情况下，依赖LLMs执行判断任务的风险。

Abstract: Large Language Models (LLMs) often exhibit sycophancy, distorting responses
to align with user beliefs, notably by readily agreeing with user
counterarguments. Paradoxically, LLMs are increasingly adopted as successful
evaluative agents for tasks such as grading and adjudicating claims. This
research investigates that tension: why do LLMs show sycophancy when challenged
in subsequent conversational turns, yet perform well when evaluating
conflicting arguments presented simultaneously? We empirically tested these
contrasting scenarios by varying key interaction patterns. We find that
state-of-the-art models: (1) are more likely to endorse a user's
counterargument when framed as a follow-up from a user, rather than when both
responses are presented simultaneously for evaluation; (2) show increased
susceptibility to persuasion when the user's rebuttal includes detailed
reasoning, even when the conclusion of the reasoning is incorrect; and (3) are
more readily swayed by casually phrased feedback than by formal critiques, even
when the casual input lacks justification. Our results highlight the risk of
relying on LLMs for judgment tasks without accounting for conversational
framing.

</details>


### [300] [InteGround: On the Evaluation of Verification and Retrieval Planning in Integrative Grounding](https://arxiv.org/abs/2509.16534)
*Cheng Jiayang,Qianqian Zhuang,Haoran Li,Chunkit Chan,Xin Liu,Lin Qiu,Yangqiu Song*

Main category: cs.CL

TL;DR: 本文引入了“整合式基础化”概念，旨在解决大型语言模型在处理需要综合多条相互依赖证据的复杂查询时的挑战。研究发现，模型在证据验证中对冗余信息鲁棒，但面对不完整信息会倾向于合理化；在检索规划中，前提溯因法优于无向规划；且零样本自反思能提升基础化质量。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型基础化方法在处理简单查询时表现良好，但现实世界中的信息需求往往需要综合多条相互依赖的证据。当前方法难以有效应对这种复杂的证据整合需求。

Method: 本文提出了“整合式基础化”的挑战，即检索并验证多条相互依赖的证据以支持一个假设查询。为系统研究此问题，作者重新利用了来自四个领域的数据集进行评估，并深入调查了基础化验证能力和检索规划策略。

Result: 1) 在基础化验证方面，大型语言模型对冗余证据具有鲁棒性，但在信息不完整时倾向于利用内部知识进行合理化。2) 在检索规划策略方面，无向规划可能因引入噪声而降低性能，而前提溯因法因其逻辑约束而成为一种有前景的方法。3) 大型语言模型的零样本自反思能力能持续提高基础化质量。

Conclusion: 这些研究发现为开发更有效、更强大的整合式基础化系统提供了宝贵的指导方向。

Abstract: Grounding large language models (LLMs) in external knowledge sources is a
promising method for faithful prediction. While existing grounding approaches
work well for simple queries, many real-world information needs require
synthesizing multiple pieces of evidence. We introduce "integrative grounding"
-- the challenge of retrieving and verifying multiple inter-dependent pieces of
evidence to support a hypothesis query. To systematically study this problem,
we repurpose data from four domains for evaluating integrative grounding
capabilities. Our investigation reveals two critical findings: First, in
groundedness verification, while LLMs are robust to redundant evidence, they
tend to rationalize using internal knowledge when information is incomplete.
Second, in examining retrieval planning strategies, we find that undirected
planning can degrade performance through noise introduction, while premise
abduction emerges as a promising approach due to its logical constraints.
Additionally, LLMs' zero-shot self-reflection capabilities consistently improve
grounding quality. These insights provide valuable direction for developing
more effective integrative grounding systems.

</details>


### [301] [Mental Multi-class Classification on Social Media: Benchmarking Transformer Architectures against LSTM Models](https://arxiv.org/abs/2509.16542)
*Khalid Hasan,Jamil Saquer,Yifan Zhang*

Main category: cs.CL

TL;DR: 本文对最先进的Transformer模型和基于LSTM的模型进行了大规模比较研究，旨在将社交媒体帖子分类到多种心理健康状况中，发现Transformer模型表现最佳，并揭示了准确性与效率之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 以往的自然语言处理（NLP）研究大多集中于单一心理健康障碍的识别，对于区分多种心理健康状况的先进NLP技术效能存在空白。社交媒体上大量公开分享的心理健康信息为早期检测提供了丰富数据。

Method: 研究首先从Reddit收集并整理了一个包含六种心理健康状况和一个对照组的大型数据集，通过严格筛选和统计探索性分析确保了标注质量。随后，在相同条件下评估了五种Transformer架构（BERT、RoBERTa、DistilBERT、ALBERT和ELECTRA）与几种LSTM变体（带或不带注意力机制，使用上下文或静态嵌入）。

Result: 实验结果表明，Transformer模型始终优于其他替代方案，其中RoBERTa在所有类别中均达到了91-99%的F1分数和准确率。值得注意的是，使用BERT嵌入并增强注意力机制的LSTM模型性能接近Transformer模型（F1分数高达97%），但训练速度快2-3.5倍；而使用静态嵌入的LSTM模型未能学到有用信号。

Conclusion: 这些发现代表了首次针对多类别心理健康检测的全面基准测试，为模型选择提供了实用指导，并强调了在心理健康NLP系统实际部署中准确性与效率之间的权衡。

Abstract: Millions of people openly share mental health struggles on social media,
providing rich data for early detection of conditions such as depression,
bipolar disorder, etc. However, most prior Natural Language Processing (NLP)
research has focused on single-disorder identification, leaving a gap in
understanding the efficacy of advanced NLP techniques for distinguishing among
multiple mental health conditions. In this work, we present a large-scale
comparative study of state-of-the-art transformer versus Long Short-Term Memory
(LSTM)-based models to classify mental health posts into exclusive categories
of mental health conditions. We first curate a large dataset of Reddit posts
spanning six mental health conditions and a control group, using rigorous
filtering and statistical exploratory analysis to ensure annotation quality. We
then evaluate five transformer architectures (BERT, RoBERTa, DistilBERT,
ALBERT, and ELECTRA) against several LSTM variants (with or without attention,
using contextual or static embeddings) under identical conditions. Experimental
results show that transformer models consistently outperform the alternatives,
with RoBERTa achieving 91-99% F1-scores and accuracies across all classes.
Notably, attention-augmented LSTMs with BERT embeddings approach transformer
performance (up to 97% F1-score) while training 2-3.5 times faster, whereas
LSTMs using static embeddings fail to learn useful signals. These findings
represent the first comprehensive benchmark for multi-class mental health
detection, offering practical guidance on model selection and highlighting an
accuracy-efficiency trade-off for real-world deployment of mental health NLP
systems.

</details>


### [302] [ChemOrch: Empowering LLMs with Chemical Intelligence via Synthetic Instructions](https://arxiv.org/abs/2509.16543)
*Yue Huang,Zhengzhe Jiang,Xiaonan Luo,Kehan Guo,Haomin Zhuang,Yujun Zhou,Zhengqing Yuan,Xiaoqi Sun,Jules Schleinitz,Yanbo Wang,Shuhao Zhang,Mihir Surve,Nitesh V Chawla,Olaf Wiest,Xiangliang Zhang*

Main category: cs.CL

TL;DR: ChemOrch是一个框架，通过两阶段过程（任务控制指令生成和工具感知响应构建）合成高质量的化学指令-响应对，以提升大型语言模型（LLMs）的化学智能。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏高质量的领域特定指令-响应数据集，以及现有合成数据生成流程与化学信息固有的层次性和规则性结构不匹配，赋予LLMs化学智能仍然面临挑战。

Method: ChemOrch通过两阶段过程合成化学指令-响应对：任务控制的指令生成和工具感知的响应构建。它支持生成任务的可控多样性和难度级别，并通过工具规划、蒸馏和基于工具的自修复机制确保响应精度。

Result: ChemOrch生成了高质量的指令数据，展示出卓越的多样性和与化学约束的高度一致性；它能可靠地生成更有效地揭示LLM在化学方面弱点的评估任务；并且，当使用其生成的数据进行微调时，显著提升了LLM的化学能力。

Conclusion: ChemOrch是实现LLMs可扩展和可验证化学智能的关键一步。

Abstract: Empowering large language models (LLMs) with chemical intelligence remains a
challenge due to the scarcity of high-quality, domain-specific
instruction-response datasets and the misalignment of existing synthetic data
generation pipelines with the inherently hierarchical and rule-governed
structure of chemical information. To address this, we propose ChemOrch, a
framework that synthesizes chemically grounded instruction-response pairs
through a two-stage process: task-controlled instruction generation and
tool-aware response construction. ChemOrch enables controllable diversity and
levels of difficulty for the generated tasks, and ensures response precision
through tool planning and distillation, and tool-based self-repair mechanisms.
The effectiveness of ChemOrch is evaluated based on: 1) the high quality of
generated instruction data, demonstrating superior diversity and strong
alignment with chemical constraints; 2) the reliable generation of evaluation
tasks that more effectively reveal LLM weaknesses in chemistry; and 3) the
significant improvement of LLM chemistry capabilities when the generated
instruction data are used for fine-tuning. Our work thus represents a critical
step toward scalable and verifiable chemical intelligence in LLMs.

</details>


### [303] [Rethinking the Role of Text Complexity in Language Model Pretraining](https://arxiv.org/abs/2509.16551)
*Dan John Velasco,Matthew Theodore Roque*

Main category: cs.CL

TL;DR: 本研究探讨了预训练文本复杂度对语言模型性能的影响。结果表明，困惑度受模型容量和文本复杂度的交互作用影响，较小模型在简化文本上性能下降较少。在零样本评估中，简化文本有利于语言知识任务，而复杂文本则有利于世界知识和实体追踪任务。


<details>
  <summary>Details</summary>
Motivation: 预训练数据质量和规模已被证实能提升下游性能，但文本复杂度的作用尚未被充分探索。文本复杂度（即文本阅读难度，通常通过句子长度、词汇选择和句子结构等表面线索估算）对语言建模和下游语言理解的影响是研究的动机。

Method: 研究使用大型语言模型简化人工撰写的文本，在保持核心内容不变的情况下，降低了表面层面的复杂度（如更短的句子、更简单的词汇和结构）。然后，从头开始在原始和简化数据上预训练因果模型（28M-500M），并在微调和零样本设置下进行评估。

Result: 研究发现，困惑度对模型容量和文本复杂度的交互作用敏感，较小的模型在简化文本上的性能下降幅度远小于较大模型。文本复杂度对微调评估影响不大。在零样本评估中，简化文本有利于语言知识任务的性能，而更复杂的文本则有利于需要世界知识和实体追踪的任务。

Conclusion: 文本复杂度在语言模型预训练中扮演着复杂且细致的角色。它不仅影响模型容量和困惑度之间的相互作用，还在零样本设置下对不同类型的下游任务（如语言知识与世界知识）产生不同的影响。

Abstract: Improving pretraining data quality and size is known to boost downstream
performance, but the role of text complexity is less explored. Text complexity
refers to how hard a text is to read, and is typically estimated from surface
cues such as sentence length, word choice, and sentence structure. We reduce
surface-level complexity--shorter sentences, simpler words, simpler
structure--while keeping core text content close to constant, and ask: (1) How
does complexity affect language modeling across model sizes? (2) Can useful
representations be learned from simpler text alone? (3) How does pretraining
text complexity influence downstream language understanding? To answer these
questions, we simplify human-written texts using a large language model, then
pretrain causal models (28M-500M) from scratch on both original and simplified
data, and evaluate them in finetuning and zero-shot setups. We find that
perplexity is sensitive to the interaction between model capacity and text
complexity--smaller models degrade far less on simpler texts--while text
complexity has little impact on finetuning evaluations, with zero-shot
evaluations indicating that simpler texts benefit performance on linguistic
knowledge tasks, whereas more complex texts favor tasks requiring world
knowledge and entity tracking.

</details>


### [304] [MPCG: Multi-Round Persona-Conditioned Generation for Modeling the Evolution of Misinformation with LLMs](https://arxiv.org/abs/2509.16564)
*Jun Rong Brian Chong,Yixuan Tang,Anthony K. H. Tung*

Main category: cs.CL

TL;DR: 该研究引入MPCG框架，利用大型语言模型模拟虚假信息在不同意识形态视角下多轮演变的过程，并发现演变后的虚假信息显著降低了现有检测器的性能。


<details>
  <summary>Details</summary>
Motivation: 现有虚假信息检测方法默认虚假信息是静态的，但实际上虚假信息在传播过程中会不断演变，改变语言、框架和道德侧重点以适应新受众。因此，需要一种方法来模拟和研究这种演变。

Method: 研究提出了MPCG（多轮、角色条件）框架，使用一个未经审查的大型语言模型（LLM）生成多轮、特定角色的声明，每一轮的生成都以前一轮的输出为条件，从而模拟虚假信息的演变。评估方法包括人类和LLM（GPT-4o-mini）标注、认知努力指标（可读性、困惑度）、情感唤起指标（情感分析、道德性）、聚类分析、可行性评估以及下游分类任务。

Result: 结果显示，人类和GPT-4o-mini的标注具有高度一致性，但在流畅性判断上存在较大分歧。生成的声明比原始声明需要更高的认知努力，并始终反映出与角色一致的情感和道德框架。聚类和余弦相似度分析证实了跨轮次的语义漂移，但主题连贯性得以保留。可行性结果显示77%的可行性率，证明了其适用于下游任务。分类结果表明，常用虚假信息检测器的宏观F1性能下降高达49.7%。

Conclusion: 研究证实了虚假信息在不同意识形态视角下的演变，并表明这种演变形式的虚假信息对现有检测器构成了严峻挑战，导致其性能大幅下降。MPCG框架为研究虚假信息演变提供了一个有效的模拟工具。

Abstract: Misinformation evolves as it spreads, shifting in language, framing, and
moral emphasis to adapt to new audiences. However, current misinformation
detection approaches implicitly assume that misinformation is static. We
introduce MPCG, a multi-round, persona-conditioned framework that simulates how
claims are iteratively reinterpreted by agents with distinct ideological
perspectives. Our approach uses an uncensored large language model (LLM) to
generate persona-specific claims across multiple rounds, conditioning each
generation on outputs from the previous round, enabling the study of
misinformation evolution. We evaluate the generated claims through human and
LLM-based annotations, cognitive effort metrics (readability, perplexity),
emotion evocation metrics (sentiment analysis, morality), clustering,
feasibility, and downstream classification. Results show strong agreement
between human and GPT-4o-mini annotations, with higher divergence in fluency
judgments. Generated claims require greater cognitive effort than the original
claims and consistently reflect persona-aligned emotional and moral framing.
Clustering and cosine similarity analyses confirm semantic drift across rounds
while preserving topical coherence. Feasibility results show a 77% feasibility
rate, confirming suitability for downstream tasks. Classification results
reveal that commonly used misinformation detectors experience macro-F1
performance drops of up to 49.7%. The code is available at
https://github.com/bcjr1997/MPCG

</details>


### [305] [From Scores to Steps: Diagnosing and Improving LLM Performance in Evidence-Based Medical Calculations](https://arxiv.org/abs/2509.16584)
*Benlu Wang,Iris Xia,Yifan Zhang,Junda Wang,Feiyun Ouyang,Shuo Han,Arman Cohan,Hong Yu,Zonghai Yao*

Main category: cs.CL

TL;DR: 本研究重新评估了大型语言模型（LLMs）在医学计算中的表现，提出了一种更细致的逐步评估方法和错误分析框架。结果显示LLMs的真实准确率低于现有评估，并引入了MedRaC模块化代理管道，显著提升了LLMs在医学计算中的性能，旨在提高其临床可信度。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在医学基准测试中表现出色，但其在医学计算（临床决策的关键组成部分）方面的能力尚未得到充分探索和评估。现有基准通常只评估最终答案，且容忍度高，可能掩盖系统性推理失败，导致严重的临床误判。

Method: 1. 清理并重构MedCalc-Bench数据集。2. 提出新的逐步评估管道，独立评估公式选择、实体提取和算术计算。3. 引入自动错误分析框架，为每种失败模式生成结构化归因。4. 提出模块化代理管道MedRaC，结合检索增强生成和基于Python的代码执行。

Result: 1. 在新的细粒度评估框架下，GPT-4o的准确率从62.7%降至43.6%，揭示了之前评估中被掩盖的错误。2. 自动错误分析框架与专家判断一致，实现了可扩展和可解释的诊断。3. MedRaC在未经微调的情况下，将不同LLMs的准确率从16.35%提高到53.19%。

Conclusion: 本研究强调了当前基准测试实践的局限性，并提出了一种更符合临床实际的方法。通过实现透明和可转移的推理评估，我们使基于LLM的系统更接近于在真实世界医疗应用中值得信赖的目标。

Abstract: Large language models (LLMs) have demonstrated promising performance on
medical benchmarks; however, their ability to perform medical calculations, a
crucial aspect of clinical decision-making, remains underexplored and poorly
evaluated. Existing benchmarks often assess only the final answer with a wide
numerical tolerance, overlooking systematic reasoning failures and potentially
causing serious clinical misjudgments. In this work, we revisit medical
calculation evaluation with a stronger focus on clinical trustworthiness.
First, we clean and restructure the MedCalc-Bench dataset and propose a new
step-by-step evaluation pipeline that independently assesses formula selection,
entity extraction, and arithmetic computation. Under this granular framework,
the accuracy of GPT-4o drops from 62.7% to 43.6%, revealing errors masked by
prior evaluations. Second, we introduce an automatic error analysis framework
that generates structured attribution for each failure mode. Human evaluation
confirms its alignment with expert judgment, enabling scalable and explainable
diagnostics. Finally, we propose a modular agentic pipeline, MedRaC, that
combines retrieval-augmented generation and Python-based code execution.
Without any fine-tuning, MedRaC improves the accuracy of different LLMs from
16.35% up to 53.19%. Our work highlights the limitations of current benchmark
practices and proposes a more clinically faithful methodology. By enabling
transparent and transferable reasoning evaluation, we move closer to making
LLM-based systems trustworthy for real-world medical applications.

</details>


### [306] [Benchmarking Contextual and Paralinguistic Reasoning in Speech-LLMs: A Case Study with In-the-Wild Data](https://arxiv.org/abs/2509.16589)
*Qiongqiong Wang,Hardik Bhupendra Sailor,Tianchi Liu,Wenyu Zhang,Muhammad Huzaifah,Nattadaporn Lertcheva,Shuo Sun,Nancy F. Chen,Jinyang Wu,AiTi Aw*

Main category: cs.CL

TL;DR: 本文提出了CP-Bench基准，用于评估语音大语言模型（Speech-LLMs）在语境副语言推理（结合语言内容与非语言线索如情感、韵律）方面的能力，发现现有模型在此领域存在显著不足，并为未来发展提供了方向。


<details>
  <summary>Details</summary>
Motivation: 尽管语音大语言模型在转录和翻译等任务中表现出色，但它们在理解对社交和情感智能至关重要的语音副语言方面（如情感和语调）仍存在局限性。

Method: 研究者提出了CP-Bench，一个评估语音大语言模型语境副语言推理能力的基准。该基准包含两个精心策划的问答数据集，需要模型同时具备语言理解和同理心理解能力。研究者评估了最先进的开源和闭源语音大语言模型，并对不同问题类型进行了全面分析。此外，还对排名前两位的模型进行了温度调整分析，以理解其对任务的影响。

Result: CP-Bench基准揭示了现有评估体系中的一个关键空白，即语音大语言模型在语境副语言推理方面存在不足。

Conclusion: 该基准为构建更具语境感知能力和情感智能的语音大语言模型提供了见解，指明了未来研究和开发的重点方向。

Abstract: Recent speech-LLMs have shown impressive performance in tasks like
transcription and translation, yet they remain limited in understanding the
paralinguistic aspects of speech crucial for social and emotional intelligence.
We propose CP-Bench, a benchmark for evaluating speech-LLMs on contextual
paralinguistic reasoning the integration of verbal content with non-verbal cues
like emotion and prosody. The benchmark includes two curated question answering
(QA) datasets requiring both linguistic and empathetic understanding. We
evaluate state-of-the-art speech-LLMs from both open and closed-source models
and perform a comprehensive analysis across different question types. The top
two models were further analyzed under temperature tuning to understand its
effect on this task. Our benchmark reveals a key gap in existing evaluations
and offers insights into building more context-aware and emotionally
intelligent speech-capable LLMs.

</details>


### [307] [From Uniform to Heterogeneous: Tailoring Policy Optimization to Every Token's Nature](https://arxiv.org/abs/2509.16591)
*Zheng Liu,Mengjie Liu,Siwei Wen,Mengzhang Cai,Bin Cui,Conghui He,Wentao Zhang*

Main category: cs.CL

TL;DR: 现有LLM推理中的强化学习方法对所有token进行统一优化，忽略了其不同作用。HAPO提出了一种基于token熵的自适应优化算法，通过多项创新技术实现细粒度控制，显著优于DAPO。


<details>
  <summary>Details</summary>
Motivation: 现有用于增强LLM推理的强化学习算法对所有token应用统一的优化策略，未能考虑到不同token在推理过程中扮演的不同角色。

Method: HAPO（异构自适应策略优化）是一种全面的token感知算法，根据token熵动态调整优化。具体方法包括：1) 自适应温度采样：实时调整采样温度，促进高熵token的探索和低熵token的连贯性；2) Token级别组平均：在token级别归一化优势，同时考虑序列长度并保持无偏处理；3) 差异化优势重分配：利用熵和重要性比率调整有明确信号token的奖励更新；4) 非对称自适应裁剪：允许对噪声大的低熵token进行激进的概率降低，同时促进高熵token的探索。通过系统研究熵与训练动态的关系，将token级处理嵌入到每个阶段以实现细粒度控制。

Result: 广泛的实验证明，HAPO在多个模型规模上始终优于DAPO。

Conclusion: HAPO通过引入基于token熵的自适应优化策略和一系列创新方法，实现了对LLM推理中强化学习的细粒度控制，有效解决了现有算法统一优化的问题，显著提升了性能。

Abstract: Reinforcement Learning has emerged as the fundamental technique for enhancing
reasoning in LLMs. However, existing algorithms apply uniform optimization to
all tokens, ignoring their different roles in reasoning process. To address
this limitation, we introduce Heterogeneous Adaptive Policy Optimization
(HAPO), a comprehensive token-aware algorithm that dynamically adapts
optimization based on token entropy. For rollout sampling, we propose Adaptive
Temperature Sampling, which adjusts sampling temperature in real time,
promoting exploration at high-entropy tokens while preserving coherence at
low-entropy ones. For advantage calculation, we introduce Token Level Group
Average that normalizes advantages at token level, jointly accounting for
sequence-length as in token-mean loss while preserving non-biased treatment. We
then develop Differential Advantage Redistribution that leverages entropy and
importance ratios to modulate rewards-adjusting updates for tokens with clear
signals. For clipping loss, we design Asymmetric Adaptive Clipping, allowing
aggressive probability reduction for noisy low-entropy tokens while enabling
exploration for high-entropy tokens. Through systematic investigation between
entropy and training dynamics, we embedded token-level treatment into every
stages to achieve fine-grained control. Extensive experiments demonstrate that
HAPO consistently outperforms DAPO across multiple model scales. Our code can
be found in https://github.com/starriver030515/HAPO.

</details>


### [308] [Analyzing the Effects of Supervised Fine-Tuning on Model Knowledge from Token and Parameter Levels](https://arxiv.org/abs/2509.16596)
*Junjie Ye,Yuming Yang,Yang Nan,Shuo Li,Qi Zhang,Tao Gui,Xuanjing Huang,Peng Wang,Zhongchao Shi,Jianping Fan*

Main category: cs.CL

TL;DR: 研究发现，监督微调（SFT）对大型语言模型（LLM）知识的影响未被充分探索。在特定条件下，SFT反而会损害模型的闭卷问答（CBQA）性能，并且高达90%的参数更新无助于知识增强。恢复这些更新可以提升性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在预训练中获得了大量世界知识，并通过SFT等后训练技术进一步塑造。然而，SFT对模型知识的影响尚不明确，这限制了我们控制微调模型知识变化行为的能力。

Method: 研究评估了来自LLaMA-2和LLaMA-3系列的五种LLM在闭卷问答（CBQA）任务上的表现。具体方法包括：1) 比较不同SFT样本量（240个与1920个）对性能的影响；2) 评估微调数据中知识掌握水平变化引起的性能波动；3) 在token和参数层面分析模型行为，以探究这些效应产生的原因。

Result: 研究发现，在1920个样本上进行微调的模型性能比在240个样本上微调的模型差14%。此外，微调数据中知识掌握水平的变化会导致超过12%的性能波动。分析表明，SFT过程中高达90%的参数更新对知识增强没有贡献。根据微调数据的特点，恢复这些更新可以提高CBQA任务的性能。

Conclusion: 这些见解为开发更有效地增强模型知识的微调策略提供了实用指导。理解SFT对知识的影响以及哪些参数更新是有效的，对于优化LLM的后训练过程至关重要。

Abstract: Large language models (LLMs) acquire substantial world knowledge during
pre-training, which is further shaped by post-training techniques such as
supervised fine-tuning (SFT). However, the impact of SFT on a model's knowledge
remains underexplored, limiting our ability to control knowledge change
behavior in fine-tuned models. To address this gap, we evaluate closed-book
question answering (CBQA) performance across five LLMs from the LLaMA-2 and
LLaMA-3 families. Surprisingly, models fine-tuned on 1,920 samples perform up
to 14% worse than those fine-tuned on only 240 samples. Furthermore, varying
the level of knowledge mastery in the fine-tuning data leads to performance
fluctuations of over 12%. To investigate these effects, we analyze model
behavior at both the token and parameter levels. Our analysis reveals that up
to 90% of parameter updates during SFT do not contribute to knowledge
enhancement. Restoring these updates can improve performance on the CBQA task,
depending on the characteristics of the fine-tuning data. These insights offer
practical guidance for developing fine-tuning strategies that more effectively
strengthen model knowledge.

</details>


### [309] [MCP: A Control-Theoretic Orchestration Framework for Synergistic Efficiency and Interpretability in Multimodal Large Language Models](https://arxiv.org/abs/2509.16597)
*Luyan Zhang*

Main category: cs.CL

TL;DR: 本研究提出了一种基于模型-控制器-任务自适应（MCP）的三层协作框架，旨在解决大型模型在复杂任务中计算效率低下和可解释性不足的问题。该框架通过解耦模型功能、结合强化学习驱动的动态路由和任务自适应机制，显著提升了跨模态任务的性能、推理效率和结果可解释性。


<details>
  <summary>Details</summary>
Motivation: 大型模型在多轮推理和多模态协作等复杂任务中面临计算效率低下和可解释性不足的问题。

Method: 本研究提出一个基于模型-控制器-任务自适应（MCP）的三层协作框架。该框架将大型模型功能解耦为推理、生成和检索模块，并结合了强化学习驱动的动态路由算法和任务自适应机制。首次实现了控制理论与大型模型动态推理的系统整合。

Result: MCP框架在GLUE、COCO、ScienceQA等跨模态基准任务上的性能比基线模型提高了15-30%；推理效率提高了40%；通过Presenter层生成了可解释的中间结果，获得了90%的人工可解释性评分。

Conclusion: MCP框架为解决大型模型实际应用瓶颈提供了一条全新的技术路径，通过提升性能、效率和可解释性，克服了现有大型模型在复杂任务中的主要挑战。

Abstract: Aiming at the problems of computational inefficiency and insufficient
interpretability faced by large models in complex tasks such as multi-round
reasoning and multi-modal collaboration, this study proposes a three-layer
collaboration framework based on model-controller-task adaptation (MCP). By
decoupling large model functions into reasoning, generation and retrieval
modules, and combining reinforcement learning-driven dynamic routing algorithms
and task adaptation mechanisms, the systematic integration of control theory
and large model dynamic reasoning is achieved for the first time. Experiments
show that the MCP framework improves the performance of cross-modal
benchmarking tasks, such as GLUE, COCO, ScienceQA, etc., by 15-30% compared
with the baseline model, improves the reasoning efficiency by 40%, and
generates the interpretable intermediate results through the Presenter layer,
obtaining 90% of the manual interpretability scores, which provides a brand-new
technological path to solve the bottleneck of the practical application of the
large model.

</details>


### [310] [PruneCD: Contrasting Pruned Self Model to Improve Decoding Factuality](https://arxiv.org/abs/2509.16598)
*Byeongho Yu,Changhun Lee,Jungyu Jin,Eunhyeok Park*

Main category: cs.CL

TL;DR: PruneCD通过层剪枝构建业余模型，优化了对比解码中的早期退出对数信息，有效减轻大型语言模型中的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型存在幻觉问题。现有方法如DoLa利用早期退出对数作为对比先验，但这些对数通常平坦、幅度小且无法反映有意义的对比。

Method: 本文提出了PruneCD，一种新颖的对比解码方法。它通过层剪枝而非早期退出机制来构建业余模型。这种设计能生成信息更丰富、对齐更好的对数，从而实现更有效的对比解码。

Result: 通过定性和定量分析，PruneCD在事实性方面持续提升，且推理开销极小。

Conclusion: PruneCD提供了一种稳健且实用的方法来缓解大型语言模型中的幻觉问题。

Abstract: To mitigate the hallucination problem in large language models, DoLa exploits
early exit logits from the same model as a contrastive prior. However, we found
that these early exit logits tend to be flat, low in magnitude, and fail to
reflect meaningful contrasts. To address this, we propose PruneCD, a novel
contrastive decoding method that constructs the amateur model via layer pruning
rather than early exit. This design leads to more informative and well-aligned
logits, enabling more effective contrastive decoding. Through qualitative and
quantitative analyses, we demonstrate that PruneCD consistently improves
factuality with minimal inference overhead, offering a robust and practical
approach to mitigating hallucinations in LLMs.

</details>


### [311] [Computational-Assisted Systematic Review and Meta-Analysis (CASMA): Effect of a Subclass of GnRH-a on Endometriosis Recurrence](https://arxiv.org/abs/2509.16599)
*Sandro Tsang*

Main category: cs.CL

TL;DR: 本研究评估了一种信息检索驱动的工作流程，用于提高系统评价的效率、透明度和可重复性。通过半自动化去重和修正方法处理多臂试验，该工作流程成功地综合了子类促性腺激素释放激素激动剂（GnRH'as）治疗子宫内膜异位症复发的证据，显示出显著的复发风险降低，并验证了其在加速系统评价过程中的有效性。


<details>
  <summary>Details</summary>
Motivation: 由于文献量巨大且不断增长，缺乏信息检索技术使得循证医学中的证据综合任务变得不可能。本研究旨在基于先前工作，评估一种信息检索驱动的工作流程，以提高系统评价的效率、透明度和可重复性，并选择子宫内膜异位症复发这一复杂且模糊的领域作为理想案例。

Method: 本研究采用混合方法，将PRISMA指南与计算技术相结合。具体包括：应用半自动化去重技术在人工筛选前高效过滤记录；综合随机对照试验证据评估促性腺激素释放激素激动剂（GnRH'as）亚类的疗效；以及使用改良的分裂方法解决多臂试验中的单位分析错误。

Result: 该工作流程显著减少了筛选工作量，仅用11天就获取并筛选了812条记录。最终确定了7项符合条件的随机对照试验，涵盖来自4个国家的841名患者。汇总的随机效应模型显示风险比（RR）为0.64（95% CI (0.48 to 0.86)），异质性不显著（I²=0.00%，τ=0.00），表明子宫内膜异位症复发率降低了36%。敏感性分析和偏倚评估也支持了研究结果的稳健性。

Conclusion: 本研究展示了一种信息检索驱动的医学证据综合工作流程。该方法不仅产生了有价值的临床结果，还为加速系统评价过程提供了一个框架。它弥合了临床研究与计算机科学之间的鸿沟，并可推广应用于其他复杂的系统评价。

Abstract: Background: Evidence synthesis facilitates evidence-based medicine. Without
information retrieval techniques, this task is impossible due to the vast and
expanding literature. Objective: Building on prior work, this study evaluates
an information retrieval-driven workflow to enhance the efficiency,
transparency, and reproducibility of systematic reviews. We use endometriosis
recurrence as an ideal case due to its complex and ambiguous literature.
Methods: Our hybrid approach integrates PRISMA guidelines with computational
techniques. We applied semi-automated deduplication to efficiently filter
records before manual screening. This workflow synthesized evidence from
randomised controlled trials on the efficacy of a subclass of
gonadotropin-releasing hormone agonists (GnRH'as). A modified splitting method
addressed unit-of-analysis errors in multi-arm trials. Results: Our workflow
efficiently reduced the screening workload. It took only 11 days to fetch and
filter 812 records. Seven RCTs were eligible, providing evidence from 841
patients in 4 countries. The pooled random-effects model yielded a Risk Ratio
(RR) of 0.64 (95% CI (0.48 to 0.86)), with non-significant heterogeneity
($I^2=0.00\%$, $\tau=0.00$); i.e., a 36% reduction in endometriosis recurrence.
Sensitivity analyses and bias assessments supported the robustness of our
findings. Conclusion: This study demonstrates an information-retrieval-driven
workflow for medical evidence synthesis. Our approach yields valuable clinical
results while providing a framework for accelerating the systematic review
process. It bridges the gap between clinical research and computer science and
can be generalized to other complex systematic reviews.

</details>


### [312] [LLMsPark: A Benchmark for Evaluating Large Language Models in Strategic Gaming Contexts](https://arxiv.org/abs/2509.16610)
*Junhao Chen,Jingbo Sun,Xiang Li,Haidong Xin,Yuhao Xue,Yibin Xu,Hao Zhao*

Main category: cs.CL

TL;DR: LLMsPark是一个基于博弈论的评估平台，用于衡量大型语言模型（LLMs）在多智能体交互环境中的决策策略和社会行为，提供了一种评估LLMs战略智能的新方法。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在各种任务中取得进展，超越单一指标的综合评估变得日益重要。为了全面评估LLMs的智能，需要深入考察它们的交互动态和战略行为。

Method: 研究引入了LLMsPark平台，该平台基于博弈论，在经典博弈论设置中衡量LLMs的决策策略和社会行为，提供了一个多智能体环境来探索战略深度。系统交叉评估了15个领先的LLMs（包括商业和开源模型），并使用排行榜排名和评分机制。

Result: 更高的分数反映了更强的推理和战略能力，揭示了不同模型之间独特的行为模式和性能差异。该研究为评估LLMs的战略智能引入了一个新颖的视角。

Conclusion: 该工作为评估LLMs的战略智能提供了一个新颖的视角，丰富了现有的基准测试，并扩展了LLMs在交互式、博弈论场景中的评估范围。

Abstract: As large language models (LLMs) advance across diverse tasks, the need for
comprehensive evaluation beyond single metrics becomes increasingly important.
To fully assess LLM intelligence, it is crucial to examine their interactive
dynamics and strategic behaviors. We present LLMsPark, a game theory-based
evaluation platform that measures LLMs' decision-making strategies and social
behaviors in classic game-theoretic settings, providing a multi-agent
environment to explore strategic depth. Our system cross-evaluates 15 leading
LLMs (both commercial and open-source) using leaderboard rankings and scoring
mechanisms. Higher scores reflect stronger reasoning and strategic
capabilities, revealing distinct behavioral patterns and performance
differences across models. This work introduces a novel perspective for
evaluating LLMs' strategic intelligence, enriching existing benchmarks and
broadening their assessment in interactive, game-theoretic scenarios. The
benchmark and rankings are publicly available at https://llmsparks.github.io/.

</details>


### [313] [Redefining Experts: Interpretable Decomposition of Language Models for Toxicity Mitigation](https://arxiv.org/abs/2509.16660)
*Zuhair Hasan Shaik,Abdullah Mazhar,Aseem Srivastava,Md Shad Akhtar*

Main category: cs.CL

TL;DR: 本文提出了一种名为EigenShift的新型干预技术，通过对大型语言模型最终输出层的特征进行特征分解，有效抑制了模型的毒性生成，同时不损害其语言能力，且无需额外训练。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型生成有害内容是一个关键挑战。现有的毒性缓解方法主要通过操纵单个神经元激活，但这些方法不稳定、依赖上下文，并且经常损害模型的语言核心能力。

Method: 研究了神经元级毒性指标的稳定性、结构（层级）表示的优势以及毒性生成机制的可解释性。通过在Jigsaw和ToxiCN数据集上进行实验，提出了一种基于语言模型最终输出层特征分解的干预技术EigenShift。该方法选择性地针对与生成对齐的组件，无需额外训练或微调，计算成本极低，并有严格的理论基础。

Result: 实验表明，聚合的层级特征比单个神经元提供更稳健的信号。此外，发现先前的工作在基于神经元的干预中混淆了毒性检测专家和生成专家。EigenShift方法能够精确地抑制毒性，同时不损害语言能力。

Conclusion: EigenShift提供了一种新颖、有原则的毒性缓解方案，通过利用层级特征和特征分解，实现了精确的毒性抑制，同时保持了模型的语言能力，且具有高效率和理论严谨性，克服了现有神经元级干预的局限性。

Abstract: Large Language Models have demonstrated impressive fluency across diverse
tasks, yet their tendency to produce toxic content remains a critical challenge
for AI safety and public trust. Existing toxicity mitigation approaches
primarily manipulate individual neuron activations, but these methods suffer
from instability, context dependence, and often compromise the model's core
language abilities. To address these shortcomings, we investigate three key
questions: the stability of neuron-level toxicity indicators, the advantages of
structural (layer-wise) representations, and the interpretability of mechanisms
driving toxic generation. Through extensive experiments on Jigsaw and ToxiCN
datasets, we show that aggregated layer-wise features provide more robust
signals than single neurons. Moreover, we observe conceptual limitations in
prior works that conflate toxicity detection experts and generation experts
within neuron-based interventions. To mitigate this, we propose a novel
principled intervention technique, EigenShift, based on eigen-decomposition of
the language model's final output layer. This method selectively targets
generation-aligned components, enabling precise toxicity suppression without
impairing linguistic competence. Our method requires no additional training or
fine-tuning, incurs minimal computational cost, and is grounded in rigorous
theoretical analysis.

</details>


### [314] [Robust Native Language Identification through Agentic Decomposition](https://arxiv.org/abs/2509.16666)
*Ahmet Yavuz Uluslu,Tannon Kew,Tilia Ellendorff,Gerold Schneider,Rico Sennrich*

Main category: cs.CL

TL;DR: 大型语言模型在母语识别任务中常利用肤浅的上下文线索而非深层语言模式，本研究提出一种受法庭语言学启发的代理式NLI流程，通过专业代理收集语言证据，显著提升了模型对误导性线索的鲁棒性和性能一致性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在母语识别（NLI）中过度依赖姓名、地点、文化刻板印象等肤浅的上下文线索，而非实际的语言模式，导致其预测容易被误导。现有通过指令让LLMs忽略这些线索的方法并不可靠。

Method: 引入一种代理式NLI流程，灵感来源于法庭语言学。该流程包含专业代理，负责积累和分类多样化的语言证据。最终，一个目标导向的协调代理会综合所有证据，做出NLI预测。

Result: 在两个基准数据集上，该方法与标准提示方法相比，显著增强了NLI对误导性上下文线索的鲁棒性，并提高了性能的一致性。

Conclusion: 通过采用受法庭语言学启发的代理式NLI流程，聚焦于积累和综合语言证据，可以有效解决LLMs在母语识别中依赖肤浅线索的问题，从而提升模型的鲁棒性和预测一致性。

Abstract: Large language models (LLMs) often achieve high performance in native
language identification (NLI) benchmarks by leveraging superficial contextual
clues such as names, locations, and cultural stereotypes, rather than the
underlying linguistic patterns indicative of native language (L1) influence. To
improve robustness, previous work has instructed LLMs to disregard such clues.
In this work, we demonstrate that such a strategy is unreliable and model
predictions can be easily altered by misleading hints. To address this problem,
we introduce an agentic NLI pipeline inspired by forensic linguistics, where
specialized agents accumulate and categorize diverse linguistic evidence before
an independent final overall assessment. In this final assessment, a goal-aware
coordinating agent synthesizes all evidence to make the NLI prediction. On two
benchmark datasets, our approach significantly enhances NLI robustness against
misleading contextual clues and performance consistency compared to standard
prompting methods.

</details>


### [315] [Reinforcement Learning Meets Large Language Models: A Survey of Advancements and Applications Across the LLM Lifecycle](https://arxiv.org/abs/2509.16679)
*Keliang Liu,Dingkang Yang,Ziyun Qian,Weijie Yin,Yuchi Wang,Hongsheng Li,Jun Liu,Peng Zhai,Yang Liu,Lihua Zhang*

Main category: cs.CL

TL;DR: 本综述全面回顾了强化学习（RL）如何提升大型语言模型（LLM）的推理和对齐性能，涵盖了RL在LLM生命周期各阶段的应用、可验证奖励的强化学习（RLVR）、数据集、工具以及未来的挑战和趋势。


<details>
  <summary>Details</summary>
Motivation: 现有关于RL增强LLM的综述范围有限，未能提供RL在LLM整个生命周期中如何运作的全面总结。

Method: 本研究系统性地回顾了RL赋能LLM的理论和实践进展，特别是RLVR。具体方法包括：简要介绍RL基本理论；详细阐述RL在LLM生命周期（预训练、对齐微调、强化推理）各阶段的应用策略，并强调强化推理阶段的重要性；整理现有用于RL微调的数据集和评估基准；审查主流开源工具和训练框架；最后分析RL增强LLM领域的未来挑战和趋势。

Result: 本综述提供了一个系统的回顾，涵盖了RL增强LLM的理论和实践进展、RL在LLM生命周期各阶段（包括预训练、对齐微调和强化推理）的具体应用策略、现有的数据集和评估基准、主流开源工具和训练框架，并对未来挑战和趋势进行了分析，旨在为研究人员和从业者提供RL与LLM交叉领域的最新发展和前沿趋势。

Conclusion: 本综述旨在向研究人员和从业者展示RL与LLM交叉领域的最新发展和前沿趋势，以期促进更智能、更通用、更安全的LLM的发展。

Abstract: In recent years, training methods centered on Reinforcement Learning (RL)
have markedly enhanced the reasoning and alignment performance of Large
Language Models (LLMs), particularly in understanding human intents, following
user instructions, and bolstering inferential strength. Although existing
surveys offer overviews of RL augmented LLMs, their scope is often limited,
failing to provide a comprehensive summary of how RL operates across the full
lifecycle of LLMs. We systematically review the theoretical and practical
advancements whereby RL empowers LLMs, especially Reinforcement Learning with
Verifiable Rewards (RLVR). First, we briefly introduce the basic theory of RL.
Second, we thoroughly detail application strategies for RL across various
phases of the LLM lifecycle, including pre-training, alignment fine-tuning, and
reinforced reasoning. In particular, we emphasize that RL methods in the
reinforced reasoning phase serve as a pivotal driving force for advancing model
reasoning to its limits. Next, we collate existing datasets and evaluation
benchmarks currently used for RL fine-tuning, spanning human-annotated
datasets, AI-assisted preference data, and program-verification-style corpora.
Subsequently, we review the mainstream open-source tools and training
frameworks available, providing clear practical references for subsequent
research. Finally, we analyse the future challenges and trends in the field of
RL-enhanced LLMs. This survey aims to present researchers and practitioners
with the latest developments and frontier trends at the intersection of RL and
LLMs, with the goal of fostering the evolution of LLMs that are more
intelligent, generalizable, and secure.

</details>


### [316] [EG-MLA: Embedding-Gated Multi-head Latent Attention for Scalable and Efficient LLMs](https://arxiv.org/abs/2509.16686)
*Zhengge Cai,Haowen Hou*

Main category: cs.CL

TL;DR: 本文提出了一种名为EG-MLA的新型注意力机制，通过引入词元特定嵌入门控，在显著减少KV缓存大小（相对于MHA减少91.6%，相对于MLA额外节省59.9%）的同时，提高了大型语言模型的推理性能和表达能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的推理效率受限于KV缓存大小，多头注意力（MHA）的内存开销巨大。尽管多头潜在注意力（MLA）通过压缩KV表示已有所缓解，但在不牺牲性能的前提下，进一步压缩KV缓存的潜力有限。

Method: 本文提出了嵌入门控多头潜在注意力（EG-MLA），作为MLA的扩展。EG-MLA在潜在空间中引入了一种词元特定嵌入门控机制，以最小的额外计算量实现对压缩KV向量的细粒度调制，从而进一步减少KV缓存并增强表达能力。

Result: 与MHA相比，EG-MLA实现了超过91.6%的KV缓存大小减少，且性能下降可忽略不计。相对于MLA，EG-MLA在各种推理基准测试中持续提高任务准确性，并额外节省了高达59.9%的内存。理论分析表明嵌入门控诱导了隐式高阶交互，实证评估展示了其在不同模型规模和压缩方案下的鲁棒泛化能力，并成功扩展到超过10亿参数的模型。

Conclusion: EG-MLA被确立为一种内存和计算高效的注意力机制，能够在大规模LLM部署中实现可扩展、高性能的推理。

Abstract: Reducing the key-value (KV) cache size is a crucial step toward enabling
efficient inference in large language models (LLMs), especially under latency
and memory constraints. While Multi-Head Attention (MHA) offers strong
representational power, it incurs significant memory overhead. Recent work on
Multi-head Latent Attention (MLA) mitigates this by compressing KV
representations into a shared latent space, achieving a better trade-off
between performance and cache efficiency. While MLA already achieves
significant KV cache reduction, the scope for further compression remains
limited without performance loss. In this paper, we propose
\textbf{Embedding-Gated Multi-head Latent Attention (EG-MLA)}, a novel
extension of MLA that further reduces KV cache size while enhancing
representational expressiveness. EG-MLA introduces a token-specific embedding
gating mechanism applied in the latent space, enabling fine-grained modulation
of compressed KV vectors with minimal additional computation. Compared to MHA,
EG-MLA achieves over 91.6\% reduction in KV cache size with negligible
performance degradation. Relative to MLA, EG-MLA consistently improves task
accuracy across diverse reasoning benchmarks while achieving up to 59.9\%
additional memory savings. Our theoretical analysis highlights how embedding
gating induces implicit high-order interactions, and empirical evaluations
demonstrate robust generalization across model scales and compression regimes.
Notably, we successfully scale EG-MLA to over 1 billion parameters,
demonstrating its practical viability for large-scale LLM deployment. These
results establish EG-MLA as a memory- and compute-efficient attention mechanism
that enables scalable, high-performance inference in modern LLMs.

</details>


### [317] [Decoding Uncertainty: The Impact of Decoding Strategies for Uncertainty Estimation in Large Language Models](https://arxiv.org/abs/2509.16696)
*Wataru Hashimoto,Hidetaka Kamigaito,Taro Watanabe*

Main category: cs.CL

TL;DR: 本研究发现，解码策略会影响大型语言模型（LLMs）的不确定性估计。对比搜索（Contrastive Search）在偏好对齐的LLMs上能产生更好的不确定性估计，但在仅经过监督微调的模型上效果可能不同。


<details>
  <summary>Details</summary>
Motivation: 解码策略会影响语言模型的生成质量及其不确定性。本研究旨在探究不同解码策略对大型语言模型不确定性估计的影响。

Method: 通过实验调查解码策略对大型语言模型（LLMs）不确定性估计的影响，并特别测试了对比搜索（Contrastive Search）等策略。

Result: 实验结果表明，对比搜索策略（能减少重复）在多种偏好对齐的LLMs上平均能产生更好的不确定性估计。然而，当模型仅通过监督微调（即没有明确对齐）进行后训练时，这些策略的益处有时会存在差异。

Conclusion: 解码策略对LLMs的不确定性估计有显著影响。对比搜索是一种有效的策略，可以提高偏好对齐LLMs的不确定性估计质量，但对于未明确对齐的模型，其效果可能不一致。

Abstract: Decoding strategies manipulate the probability distribution underlying the
output of a language model and can therefore affect both generation quality and
its uncertainty. In this study, we investigate the impact of decoding
strategies on uncertainty estimation in Large Language Models (LLMs). Our
experiments show that Contrastive Search, which mitigates repetition, yields
better uncertainty estimates on average across a range of preference-aligned
LLMs. In contrast, the benefits of these strategies sometimes diverge when the
model is only post-trained with supervised fine-tuning, i.e. without explicit
alignment.

</details>


### [318] [OPEN-THEATRE: An Open-Source Toolkit for LLM-based Interactive Drama](https://arxiv.org/abs/2509.16713)
*Tianyang Xu,Hongqiu Wu,Weiqi Wu,Hai Zhao*

Main category: cs.CL

TL;DR: 本文介绍了Open-Theatre，一个开源工具包，用于体验和定制基于大型语言模型（LLM）的互动戏剧，旨在解决现有研究领域缺乏完善开发平台的问题。


<details>
  <summary>Details</summary>
Motivation: 基于LLM的互动戏剧作为新兴领域潜力巨大，但由于缺乏设计良好的开发平台来构建完整的戏剧，导致该领域研究不足，研究人员难以复现、扩展和研究此类系统。

Method: 本文提出了Open-Theatre工具包，它采用高效的多智能体架构和分层检索式记忆系统，以增强复杂互动中的叙事连贯性和逼真的长期行为。此外，它还提供高度可配置的开发流程，方便研究人员开发和优化新方法。

Result: Open-Theatre是首个开源工具包，使研究人员能够体验和定制基于LLM的互动戏剧，并通过其创新架构提升了叙事连贯性和智能体的长期行为真实性，并简化了新方法的开发和优化过程。

Conclusion: Open-Theatre通过提供一个全面的开源工具包，显著降低了基于LLM的互动戏剧研究的门槛，促进了该领域的发展和创新。

Abstract: LLM-based Interactive Drama introduces a novel dialogue scenario in which the
player immerses into a character and engages in a dramatic story by interacting
with LLM agents. Despite the fact that this emerging area holds significant
promise, it remains largely underexplored due to the lack of a well-designed
playground to develop a complete drama. This makes a significant barrier for
researchers to replicate, extend, and study such systems. Hence, we present
Open-Theatre, the first open-source toolkit for experiencing and customizing
LLM-based interactive drama. It refines prior work with an efficient
multi-agent architecture and a hierarchical retrieval-based memory system,
designed to enhance narrative coherence and realistic long-term behavior in
complex interactions. In addition, we provide a highly configurable pipeline,
making it easy for researchers to develop and optimize new approaches.

</details>


### [319] [Semi-Supervised Synthetic Data Generation with Fine-Grained Relevance Control for Short Video Search Relevance Modeling](https://arxiv.org/abs/2509.16717)
*Haoran Li,Zhiming Su,Junyan Yao,Enwei Zhang,Yang Ji,Yan Chen,Kan Zhou,Chao Feng,Jiao Ran*

Main category: cs.CL

TL;DR: 本文提出了一个带有四级相关性标注的中文短视频数据集，并开发了一个半监督合成数据管道，通过协同训练模型生成具有可控相关性标签的领域自适应数据。该方法显著提升了嵌入模型在细粒度语义区分上的性能，并在抖音的在线A/B测试中取得了显著的推荐效果提升。


<details>
  <summary>Details</summary>
Motivation: 现有的基于提示的合成方法难以捕捉领域特定的数据分布（尤其是在数据稀缺领域），且经常忽视细粒度的相关性多样性。此外，中文短视频领域缺乏带有相关性标注的关键资源。

Method: 1. 构建了一个具有四级相关性标注的中文短视频数据集。2. 提出了一种半监督合成数据管道，其中两个协同训练的模型用于生成具有可控相关性标签的领域自适应短视频数据。3. 通过为代表性不足的中间相关性标签合成样本，增强了相关性级别的多样性。

Result: 1. 离线实验表明，使用合成数据训练的嵌入模型优于基于提示或普通监督微调（SFT）训练的模型。2. 训练数据中包含更多样化的细粒度相关性级别，增强了模型对细微语义区分的敏感性。3. 在抖音的双列场景搜索增强推荐管道中，在线A/B测试显示，点击率（CTR）提升了1.45%，强相关性比例（SRR）提高了4.9%，图像用户渗透率（IUPR）提升了0.1054%。

Conclusion: 所提出的方法有效地生成了高质量、多样化且领域自适应的细粒度相关性合成数据，显著提升了嵌入模型的性能及其对语义细微差别的敏感性。细粒度相关性监督在嵌入学习中具有重要价值，并在实际推荐系统中带来了显著的业务增益。

Abstract: Synthetic data is widely adopted in embedding models to ensure diversity in
training data distributions across dimensions such as difficulty, length, and
language. However, existing prompt-based synthesis methods struggle to capture
domain-specific data distributions, particularly in data-scarce domains, and
often overlook fine-grained relevance diversity. In this paper, we present a
Chinese short video dataset with 4-level relevance annotations, filling a
critical resource void. Further, we propose a semi-supervised synthetic data
pipeline where two collaboratively trained models generate domain-adaptive
short video data with controllable relevance labels. Our method enhances
relevance-level diversity by synthesizing samples for underrepresented
intermediate relevance labels, resulting in a more balanced and semantically
rich training data set. Extensive offline experiments show that the embedding
model trained on our synthesized data outperforms those using data generated
based on prompting or vanilla supervised fine-tuning(SFT). Moreover, we
demonstrate that incorporating more diverse fine-grained relevance levels in
training data enhances the model's sensitivity to subtle semantic distinctions,
highlighting the value of fine-grained relevance supervision in embedding
learning. In the search enhanced recommendation pipeline of Douyin's
dual-column scenario, through online A/B testing, the proposed model increased
click-through rate(CTR) by 1.45%, raised the proportion of Strong Relevance
Ratio (SRR) by 4.9%, and improved the Image User Penetration Rate (IUPR) by
0.1054%.

</details>


### [320] [Time to Revist Exact Match](https://arxiv.org/abs/2509.16720)
*Auss Abbood,Zaiqiao Meng,Nigel Collier*

Main category: cs.CL

TL;DR: 本研究将大型语言模型中的时间问答重新定义为数值估计任务，引入了TempAnswerQA基准和sMAPE、MASE等预测指标，发现传统精确匹配（EM）指标不足以评估时间推理能力，新指标能更准确地揭示模型的理解缺陷。


<details>
  <summary>Details</summary>
Motivation: 传统的时间问答评估方法（如精确匹配EM）将模型响应视为普通文本，无法区分数值答案（如日期或持续时间）中小错误和大错误。当预期答案是数值时，EM评估无法有效衡量模型的真实性能。

Method: 研究将时间问答重新定义为数值估计任务，以评估EM的局限性。引入了TempAnswerQA基准，该基准从Test of Time和TempTabQA中提炼而来，所有问题都需要数值化的时间答案。使用对称平均绝对百分比误差（sMAPE）和平均绝对标度误差（MASE）这两种预测指标来评估模型。

Result: 通过sMAPE，发现误差大小与EM是脱钩的：低EM的模型sMAPE也低（均约为20%），但一些模型尽管EM高，sMAPE却很高。MASE通过真实数据偏差对误差进行缩放，重新排列了模型的排名，揭示了模型对时间领域知识理解的不足，尤其是在使用合成数据训练时。此外，模型最常见的错误是与真实值仅偏差±1，sMAPE和MASE能恰当地衡量这些错误，而EM则不能。

Conclusion: 研究结果强调了时间问答任务需要专门的评估指标，以更准确地衡量大型语言模型的时间推理能力和数值估计精度。

Abstract: Temporal question answering is an established method for evaluating temporal
reasoning in large language models. Expected answers are often numeric (e.g.,
dates or durations), yet model responses are evaluated like regular text with
exact match (EM), unable to distinguish small from large errors. In this
investigative work, we frame temporal question answering as a numerical
estimation task to assess the shortcomings of EM. We introduce TempAnswerQA, a
benchmark distilled from Test of Time and TempTabQA, where all questions
require a numerical, temporal answer, allowing us to evaluate models beyond EM.
We use the forecasting metrics symmetric mean absolute percentage error (sMAPE)
and mean absolute scaled error (MASE). With sMAPE, we find that error size and
EM are decoupled. Models with low EM still have low sMAPE (both ~20%), and some
models have high sMAPE despite high EM. Scaling errors by the deviation of the
ground truth data with MASE reshuffles model rankings compared to EM, revealing
gaps in models' understanding of temporal domain knowledge, especially when
trained with synthetic data. Lastly, the models' most frequent error is to
deviate by only $\pm1$ from the ground truth. sMAPE and MASE, unlike EM,
adequately weight these errors. Our findings underscore the need for
specialised metrics for temporal QA tasks. Code and data are available on
https://github.com/aauss/temporal-answer-qa.

</details>


### [321] [A Multi-Level Benchmark for Causal Language Understanding in Social Media Discourse](https://arxiv.org/abs/2509.16722)
*Xiaohan Ding,Kaike Ping,Buse Çarık,Eugenia Rho*

Main category: cs.CL

TL;DR: 本文介绍了CausalTalk数据集，一个包含五年Reddit帖子（关于COVID-19公共卫生）的多级别数据集，旨在解决非正式语境中因果语言理解的挑战，并支持四种因果任务的标注。


<details>
  <summary>Details</summary>
Motivation: 现有的数据集主要关注结构化文本中的显式因果关系，对非正式、用户生成内容（尤其是社交媒体帖子）中隐式因果表达的检测支持有限，而这正是自然语言处理中一个核心但未被充分探索的挑战。

Method: 引入了CausalTalk数据集，包含2020-2024年间讨论COVID-19公共卫生的Reddit帖子，其中10120篇帖子经过标注。标注任务包括：(1) 二元因果分类，(2) 显式与隐式因果分类，(3) 因果跨度提取，以及(4) 因果要点生成。标注数据包括领域专家创建的黄金标准标签和由GPT-4o生成并经人工验证的白银标准标签。

Result: CausalTalk弥合了非正式文本中细粒度因果检测与基于要点的推理之间的鸿沟。它支持判别式模型和生成式模型的基准测试。

Conclusion: CausalTalk为研究社交媒体语境下的因果推理提供了丰富的资源，有助于解决非正式语境中因果语言理解的挑战。

Abstract: Understanding causal language in informal discourse is a core yet
underexplored challenge in NLP. Existing datasets largely focus on explicit
causality in structured text, providing limited support for detecting implicit
causal expressions, particularly those found in informal, user-generated social
media posts. We introduce CausalTalk, a multi-level dataset of five years of
Reddit posts (2020-2024) discussing public health related to the COVID-19
pandemic, among which 10120 posts are annotated across four causal tasks: (1)
binary causal classification, (2) explicit vs. implicit causality, (3)
cause-effect span extraction, and (4) causal gist generation. Annotations
comprise both gold-standard labels created by domain experts and
silver-standard labels generated by GPT-4o and verified by human annotators.
CausalTalk bridges fine-grained causal detection and gist-based reasoning over
informal text. It enables benchmarking across both discriminative and
generative models, and provides a rich resource for studying causal reasoning
in social media contexts.

</details>


### [322] [Angular Dispersion Accelerates $k$-Nearest Neighbors Machine Translation](https://arxiv.org/abs/2509.16729)
*Evgeniia Tokarchuk,Sergey Troshin,Vlad Niculae*

Main category: cs.CL

TL;DR: 本文提出通过鼓励神经隐藏表征的角分散来加速k-NN MT的检索过程，并略微提升翻译性能。


<details>
  <summary>Details</summary>
Motivation: k-NN MT虽然能提升翻译性能，但其计算成本和内存需求很高，尤其是在大型数据存储中。近似k-NN MT查找算法是瓶颈，而现有研究多集中于减小数据存储大小或查找调用次数。

Method: 本文采用一种正交方法，基于近似k-NN MT查找数据结构的性能特性，提出鼓励上下文神经隐藏表征的角分散，以改善检索数据结构的平衡性。

Result: 通过改善分散性，该方法成功加速了检索过程，并略微提升了翻译质量。

Conclusion: 鼓励神经隐藏表征的角分散是缓解k-NN MT计算成本瓶颈并同时保持或略微提升翻译质量的有效策略。

Abstract: Augmenting neural machine translation with external memory at decoding time,
in the form of k-nearest neighbors machine translation ($k$-NN MT), is a
well-established strategy for increasing translation performance. $k$-NN MT
retrieves a set of tokens that occurred in the most similar contexts recorded
in a prepared data store, using hidden state representations of translation
contexts as vector lookup keys. One of the main disadvantages of this method is
the high computational cost and memory requirements. Since an exhaustive search
is not feasible in large data stores, practitioners commonly use approximate
$k$-NN MT lookup, yet even such algorithms are a bottleneck. In contrast to
research directions seeking to accelerate $k$-NN MT by reducing data store size
or the number of lookup calls, we pursue an orthogonal direction based on the
performance properties of approximate $k$-NN MT lookup data structures. In
particular, we propose to encourage angular dispersion of the neural hidden
representations of contexts. We show that improving dispersion leads to better
balance in the retrieval data structures, accelerating retrieval and slightly
improving translations.

</details>


### [323] [The Sound of Syntax: Finetuning and Comprehensive Evaluation of Language Models for Speech Pathology](https://arxiv.org/abs/2509.16765)
*Fagun Patel,Duc Q. Nguyen,Sang T. Truong,Jody Vaynshtok,Sanmi Koyejo,Nick Haber*

Main category: cs.CL

TL;DR: 本研究旨在弥合言语-语言病理学领域中多模态语言模型（MLMs）应用研究的空白。通过与领域专家合作，我们开发了一个分类法，并引入了首个针对MLMs在该领域五个核心用例的综合基准测试。评估发现，没有单一模型能始终表现最佳，存在系统性差异（例如对男性说话者表现更好），并发现思维链提示在某些任务上会降低性能。然而，通过领域特定数据微调可显著提升性能，揭示了当前MLMs在该应用中的潜力和局限性。


<details>
  <summary>Details</summary>
Motivation: 美国有超过340万儿童患有言语障碍，需要临床干预，但言语-语言病理学家（SLPs）的数量远低于受影响儿童的数量，这凸显了儿童护理方面的巨大缺口以及对技术支持以提高SLPs工作效率的迫切需求。最先进的多模态语言模型（MLMs）有望支持SLPs，但由于对其在关键临床环境中表现的理解有限，其应用仍未得到充分探索。

Method: 本研究与领域专家合作，开发了言语-语言病理学中MLMs真实世界用例的分类法。在此基础上，引入了首个用于评估MLMs的综合基准测试，涵盖五个核心用例，每个用例包含1000个手动标注的数据点。该基准测试包括在各种设置（如背景噪音、说话者性别和口音）下的鲁棒性和敏感性测试。研究评估了15个最先进的MLMs，并探讨了在领域特定数据上微调MLMs的效果。

Result: 评估15个最先进的MLMs表明，没有单一模型能在所有任务上始终优于其他模型。研究发现了系统性差异，模型对男性说话者的表现更好。观察到思维链提示在具有大标签空间和窄决策边界的分类任务上会降低性能。此外，在领域特定数据上微调MLMs可实现超过30%的性能提升。

Conclusion: 这些发现突显了当前多模态语言模型在言语-语言病理学应用中的潜力和局限性，强调了需要进一步研究和有针对性的开发。

Abstract: According to the U.S. National Institutes of Health, more than 3.4 million
children experience speech disorders that require clinical intervention. The
number of speech-language pathologists (SLPs) is roughly 20 times fewer than
the number of affected children, highlighting a significant gap in children's
care and a pressing need for technological support that improves the
productivity of SLPs. State-of-the-art multimodal language models (MLMs) show
promise for supporting SLPs, but their use remains underexplored largely due to
a limited understanding of their performance in high-stakes clinical settings.
To address this gap, we collaborate with domain experts to develop a taxonomy
of real-world use cases of MLMs in speech-language pathologies. Building on
this taxonomy, we introduce the first comprehensive benchmark for evaluating
MLM across five core use cases, each containing 1,000 manually annotated data
points. This benchmark includes robustness and sensitivity tests under various
settings, including background noise, speaker gender, and accent. Our
evaluation of 15 state-of-the-art MLMs reveals that no single model
consistently outperforms others across all tasks. Notably, we find systematic
disparities, with models performing better on male speakers, and observe that
chain-of-thought prompting can degrade performance on classification tasks with
large label spaces and narrow decision boundaries. Furthermore, we study
fine-tuning MLMs on domain-specific data, achieving improvements of over 30%
compared to base models. These findings highlight both the potential and
limitations of current MLMs for speech-language pathology applications,
underscoring the need for further research and targeted development.

</details>


### [324] [MoRoVoc: A Large Dataset for Geographical Variation Identification of the Spoken Romanian Language](https://arxiv.org/abs/2509.16781)
*Andrei-Marius Avram,Ema-Ioana Bănescu,Anda-Teodora Robea,Dumitru-Clementin Cercel,Mihaela-Claudia Cercel*

Main category: cs.CL

TL;DR: 本文介绍了MoRoVoc，一个用于分析罗马尼亚语口语区域差异的大型数据集（93小时音频），并提出了一种多目标对抗训练框架，使语音模型在主要任务上具有判别性，同时对次要属性（如年龄、性别）保持不变，并取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 研究罗马尼亚语口语的区域差异，并开发能够有效识别这些差异同时对说话者人口统计学属性（年龄、性别）保持鲁棒性的语音模型。

Method: 构建了MoRoVoc数据集，包含93小时、88,192个音频样本，平衡了罗马尼亚和摩尔多瓦共和国的口语。提出了一种多目标对抗训练框架，将人口统计学属性（年龄、性别）作为对抗目标，通过元学习动态调整对抗系数来优化性能，并应用于Wav2Vec2模型。

Result: 使用性别作为对抗目标时，Wav2Vec2-Base在口语变体识别上达到78.21%的准确率。当同时使用方言和年龄作为对抗目标时，Wav2Vec2-Large在性别分类上达到93.08%的准确率。该方法取得了显著的性能提升。

Conclusion: 所提出的MoRoVoc数据集和多目标对抗训练框架能有效分析罗马尼亚语口语的区域差异，并使模型在主要任务上表现出色，同时对次要人口统计学属性保持不变性，实现了显著的性能增益。

Abstract: This paper introduces MoRoVoc, the largest dataset for analyzing the regional
variation of spoken Romanian. It has more than 93 hours of audio and 88,192
audio samples, balanced between the Romanian language spoken in Romania and the
Republic of Moldova. We further propose a multi-target adversarial training
framework for speech models that incorporates demographic attributes (i.e., age
and gender of the speakers) as adversarial targets, making models
discriminative for primary tasks while remaining invariant to secondary
attributes. The adversarial coefficients are dynamically adjusted via
meta-learning to optimize performance. Our approach yields notable gains:
Wav2Vec2-Base achieves 78.21% accuracy for the variation identification of
spoken Romanian using gender as an adversarial target, while Wav2Vec2-Large
reaches 93.08% accuracy for gender classification when employing both dialect
and age as adversarial objectives.

</details>


### [325] [Domain-Adaptive Pre-Training for Arabic Aspect-Based Sentiment Analysis: A Comparative Study of Domain Adaptation and Fine-Tuning Strategies](https://arxiv.org/abs/2509.16788)
*Salha Alyami,Amani Jamal,Areej Alhothali*

Main category: cs.CL

TL;DR: 本研究针对阿拉伯语方面情感分析（ABSA）中标记数据稀缺和预训练模型偏差的问题，提出了领域自适应预训练方法，并评估了不同的微调策略。结果显示自适应预训练有适度提升，适配器微调高效且具竞争力，但错误分析揭示了模型预测和数据集标注的挑战，强调了对语法语义感知模型的必要性。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语ABSA由于标记数据稀缺而应用受限。虽然BERT等预训练模型被尝试，但它们基于事实性数据，可能在领域特定任务中引入偏差。目前尚无研究将自适应预训练应用于阿拉伯语上下文模型以解决ABSA问题。

Method: 本研究提出了一种结合领域自适应预训练（DAPT）的方法，用于方面情感分类（ASC）和意见目标表达（OTE）提取。研究评估了特征提取、完全微调和基于适配器的方法等多种微调策略，并利用多个自适应语料库和上下文模型来增强性能和效率。

Result: 研究结果表明，领域内自适应预训练带来了适度的性能提升。基于适配器的微调是一种计算高效的方法，并取得了有竞争力的结果。然而，错误分析揭示了模型预测和数据集标注存在问题，包括ASC中的情感标注错误、对比标记误解、早期词项的积极性偏差以及冲突意见和子词分词挑战；OTE中则存在目标误标、句法角色混淆、多词表达困难和依赖浅层启发式等问题。

Conclusion: 研究强调需要开发更具语法和语义感知能力的模型（如图卷积网络），以更有效地捕捉长距离关系和复杂的基于方面的意见对齐，从而解决当前模型和数据集标注中存在的问题。

Abstract: Aspect-based sentiment analysis (ABSA) in natural language processing enables
organizations to understand customer opinions on specific product aspects.
While deep learning models are widely used for English ABSA, their application
in Arabic is limited due to the scarcity of labeled data. Researchers have
attempted to tackle this issue by using pre-trained contextualized language
models such as BERT. However, these models are often based on fact-based data,
which can introduce bias in domain-specific tasks like ABSA. To our knowledge,
no studies have applied adaptive pre-training with Arabic contextualized models
for ABSA. This research proposes a novel approach using domain-adaptive
pre-training for aspect-sentiment classification (ASC) and opinion target
expression (OTE) extraction. We examine fine-tuning strategies - feature
extraction, full fine-tuning, and adapter-based methods - to enhance
performance and efficiency, utilizing multiple adaptation corpora and
contextualized models. Our results show that in-domain adaptive pre-training
yields modest improvements. Adapter-based fine-tuning is a computationally
efficient method that achieves competitive results. However, error analyses
reveal issues with model predictions and dataset labeling. In ASC, common
problems include incorrect sentiment labeling, misinterpretation of contrastive
markers, positivity bias for early terms, and challenges with conflicting
opinions and subword tokenization. For OTE, issues involve mislabeling targets,
confusion over syntactic roles, difficulty with multi-word expressions, and
reliance on shallow heuristics. These findings underscore the need for syntax-
and semantics-aware models, such as graph convolutional networks, to more
effectively capture long-distance relations and complex aspect-based opinion
alignments.

</details>


### [326] [KuBERT: Central Kurdish BERT Model and Its Application for Sentiment Analysis](https://arxiv.org/abs/2509.16804)
*Kozhin muhealddin Awlla,Hadi Veisi,Abdulhady Abas Abdullah*

Main category: cs.CL

TL;DR: 本文通过将BERT模型集成到自然语言处理技术中，显著提升了针对中库尔德语的情感分析研究，为低资源语言的情感分析树立了新基准。


<details>
  <summary>Details</summary>
Motivation: 中库尔德语是一种计算资源极少且语言多样性高的低资源语言，这使得其情感分析极具挑战性。传统的Word2Vec等词嵌入模型效果有限，而BERT等新型语言模型的出现为改善这一现状带来了希望。

Method: 本文采用的方法是将双向编码器表示转换器（BERT）集成到自然语言处理（NLP）技术中，以进行中库尔德语的情感分析。

Result: BERT更优越的词嵌入能力有助于捕捉中库尔德语细致的语义池和上下文复杂性。这项研究为低资源语言的情感分析设定了新的基准。

Conclusion: 通过整合BERT模型，本研究成功提升了中库尔德语的情感分析性能，证明了BERT在处理低资源语言的细微语义和语境复杂性方面的强大能力，并为该领域树立了新的标杆。

Abstract: This paper enhances the study of sentiment analysis for the Central Kurdish
language by integrating the Bidirectional Encoder Representations from
Transformers (BERT) into Natural Language Processing techniques. Kurdish is a
low-resourced language, having a high level of linguistic diversity with
minimal computational resources, making sentiment analysis somewhat
challenging. Earlier, this was done using a traditional word embedding model,
such as Word2Vec, but with the emergence of new language models, specifically
BERT, there is hope for improvements. The better word embedding capabilities of
BERT lend to this study, aiding in the capturing of the nuanced semantic pool
and the contextual intricacies of the language under study, the Kurdish
language, thus setting a new benchmark for sentiment analysis in low-resource
languages.

</details>


### [327] [Cognitive Linguistic Identity Fusion Score (CLIFS): A Scalable Cognition-Informed Approach to Quantifying Identity Fusion from Text](https://arxiv.org/abs/2509.16813)
*Devin R. Wright,Jisun An,Yong-Yeol Ahn*

Main category: cs.CL

TL;DR: 本文提出CLIFS，一种结合认知语言学和大型语言模型的新型身份融合量化指标。CLIFS实现了自动化和可扩展评估，表现优于现有方法和人工标注，并在暴力风险评估中显示出显著改进。


<details>
  <summary>Details</summary>
Motivation: 量化身份融合对于理解广泛的群体行为至关重要。传统量表（如图片和口头量表）需要受控调查或直接接触，存在局限性。

Method: 引入认知语言学身份融合分数（CLIFS），这是一种整合认知语言学与大型语言模型（LLMs）的新型度量标准，其基础是隐喻检测。CLIFS提供全自动、可扩展的评估。

Result: CLIFS与已建立的口头测量方法高度一致。在基准测试中，CLIFS优于现有的自动化方法和人工标注。作为概念验证，CLIFS将暴力风险评估的准确性提高了240%以上。

Conclusion: CLIFS成功地识别了一个新的NLP任务。为了提高泛化能力并进一步推动这一新兴领域，需要开发更大、更多样化的数据集，以涵盖更多的融合目标领域和文化背景。

Abstract: Quantifying identity fusion -- the psychological merging of self with another
entity or abstract target (e.g., a religious group, political party, ideology,
value, brand, belief, etc.) -- is vital for understanding a wide range of
group-based human behaviors. We introduce the Cognitive Linguistic Identity
Fusion Score (CLIFS), a novel metric that integrates cognitive linguistics with
large language models (LLMs), which builds on implicit metaphor detection.
Unlike traditional pictorial and verbal scales, which require controlled
surveys or direct field contact, CLIFS delivers fully automated, scalable
assessments while maintaining strong alignment with the established verbal
measure. In benchmarks, CLIFS outperforms both existing automated approaches
and human annotation. As a proof of concept, we apply CLIFS to violence risk
assessment to demonstrate that it can improve violence risk assessment by more
than 240%. Building on our identification of a new NLP task and early success,
we underscore the need to develop larger, more diverse datasets that encompass
additional fusion-target domains and cultural backgrounds to enhance
generalizability and further advance this emerging area. CLIFS models and code
are public at https://github.com/DevinW-sudo/CLIFS.

</details>


### [328] [Semantic-Driven Topic Modeling for Analyzing Creativity in Virtual Brainstorming](https://arxiv.org/abs/2509.16835)
*Melkamu Abay Mersha,Jugal Kalita*

Main category: cs.CL

TL;DR: 本文提出了一种语义驱动的主题建模框架，用于自动化分析虚拟头脑风暴会议中的大量创意，旨在高效提取有价值的见解并评估群体创造力。


<details>
  <summary>Details</summary>
Motivation: 虚拟头脑风暴会议产生大量且分布不均的创意，使得高效提取有价值的见解变得困难。手动编码耗时且主观，因此需要自动化的方法来支持群体创造力的评估。

Method: 该框架集成了四个模块化组件：基于Transformer的嵌入（Sentence-BERT）、降维（UMAP）、聚类（HDBSCAN）以及带优化的主题提取。它捕捉句子层面的语义相似性，能够从头脑风暴记录中发现连贯的主题，同时过滤噪音并识别异常值。

Result: 该方法在学生小组的Zoom头脑风暴会议上进行了评估，结果表明其主题一致性（平均0.687 CV）高于LDA、ETM和BERTopic等现有方法，显著优于基线。此外，该模型提供了对主题深度和多样性的可解释见解，支持群体创造力的收敛和发散维度。

Conclusion: 这项工作突出了基于嵌入的主题建模在分析协作创意方面的潜力，并为研究同步虚拟会议中的创造力提供了一个高效且可扩展的框架。

Abstract: Virtual brainstorming sessions have become a central component of
collaborative problem solving, yet the large volume and uneven distribution of
ideas often make it difficult to extract valuable insights efficiently. Manual
coding of ideas is time-consuming and subjective, underscoring the need for
automated approaches to support the evaluation of group creativity. In this
study, we propose a semantic-driven topic modeling framework that integrates
four modular components: transformer-based embeddings (Sentence-BERT),
dimensionality reduction (UMAP), clustering (HDBSCAN), and topic extraction
with refinement. The framework captures semantic similarity at the sentence
level, enabling the discovery of coherent themes from brainstorming transcripts
while filtering noise and identifying outliers. We evaluate our approach on
structured Zoom brainstorming sessions involving student groups tasked with
improving their university. Results demonstrate that our model achieves higher
topic coherence compared to established methods such as LDA, ETM, and BERTopic,
with an average coherence score of 0.687 (CV), outperforming baselines by a
significant margin. Beyond improved performance, the model provides
interpretable insights into the depth and diversity of topics explored,
supporting both convergent and divergent dimensions of group creativity. This
work highlights the potential of embedding-based topic modeling for analyzing
collaborative ideation and contributes an efficient and scalable framework for
studying creativity in synchronous virtual meetings.

</details>


### [329] [Multi-task Pretraining for Enhancing Interpretable L2 Pronunciation Assessment](https://arxiv.org/abs/2509.16876)
*Jiun-Ting Li,Bi-Cheng Yan,Yi-Cheng Wang,Berlin Chen*

Main category: cs.CL

TL;DR: 本文提出多任务预训练（MTP）策略，通过重建输入特征来捕捉超音段发音线索，并整合人工设计特征（HCFs）以弥合自动发音评估（APA）与自动口语评估（ASA）之间的差距，从而实现更全面的第二语言学习者口语能力评估。


<details>
  <summary>Details</summary>
Motivation: 现有APA系统主要依赖音段级别特征，忽略了超音段发音线索，导致不同语言层级评估的局限性。此外，APA系统缺乏与ASA的整合，限制了对整体口语能力的全面评估。

Method: 1. 针对APA：引入多任务预训练（MTP）。在APA模型的音素级别编码器上，随机遮蔽音段级别发音特征，并根据周围语境重建被遮蔽的特征，以捕获长期时间发音线索并强化话语内部结构。2. 整合ASA：结合经验研究和ASA先验知识，通过回归器引入人工设计特征（HCFs），如流利度（语速、静音时长）和重音（音高重音强度），这些特征通过人工设计的公式得出，用于生成可解释的能力分数。

Result: 在speechocean762数据集上的实验表明，该方法改进了发音评分，并提高了与ASA能力评估的相关性，从而能够进行有针对性的训练和全面的口语能力评估。

Conclusion: 通过引入多任务预训练以捕捉超音段发音线索，并整合人工设计特征以弥合APA与ASA之间的差距，本框架有效地解决了现有APA系统的局限性，实现了更全面、细致且具解释性的第二语言口语能力评估。

Abstract: Automatic pronunciation assessment (APA) analyzes second-language (L2)
learners' speech by providing fine-grained pronunciation feedback at various
linguistic levels. Most existing efforts on APA typically adopt segmental-level
features as inputs and predict pronunciation scores at different granularities
via hierarchical (or parallel) pronunciation modeling. This, however,
inevitably causes assessments across linguistic levels (e.g., phone, word, and
utterance) to rely solely on phoneme-level pronunciation features, nearly
sidelining supra-segmental pronunciation cues. To address this limitation, we
introduce multi-task pretraining (MTP) for APA, a simple yet effective strategy
that attempts to capture long-term temporal pronunciation cues while
strengthening the intrinsic structures within an utterance via the objective of
reconstructing input features. Specifically, for a phoneme-level encoder of an
APA model, the proposed MTP strategy randomly masks segmental-level
pronunciation features and reconstructs the masked ones based on their
surrounding pronunciation context. Furthermore, current APA systems lack
integration with automated speaking assessment (ASA), limiting holistic
proficiency evaluation. Drawing on empirical studies and prior knowledge in
ASA, our framework bridges this gap by incorporating handcrafted features
(HCFs), such as fluency (speech rate, silence duration) and stress (pitch
accent strength), derived from human-designed formulas via regressors to
generate interpretable proficiency scores. Experiments on speechocean762 show
improved pronunciation scoring and ASA proficiency correlation, enabling
targeted training and comprehensive proficiency assessment.

</details>


### [330] [Can GRPO Boost Complex Multimodal Table Understanding?](https://arxiv.org/abs/2509.16889)
*Xiaoqiang Kang,Shengen Wu,Zimu Wang,Yilin Liu,Xiaobo Jin,Kaizhu Huang,Wei Wang,Yutao Yue,Xiaowei Huang,Qiufeng Wang*

Main category: cs.CL

TL;DR: 本文提出了Table-R1，一个三阶段的强化学习框架，旨在克服现有RL方法在表格理解中遇到的低初始策略准确性和粗糙奖励问题，显著提升多模态表格理解能力，超越SFT和GRPO，甚至与GPT-4o表现相当。


<details>
  <summary>Details</summary>
Motivation: 现有表格理解方法面临复杂表格结构和复杂逻辑推理的挑战。虽然SFT是主流，但强化学习（如GRPO）在表格语境中因初始策略准确性低和奖励粗糙而受限。

Method: Table-R1是一个三阶段RL框架：1) 热身（Warm-up）阶段，用于建立初步感知和推理能力；2) 感知对齐GRPO（PA-GRPO）阶段，利用连续的树编辑距离相似度（TEDS）奖励来识别表格结构和内容；3) 提示完成GRPO（HC-GRPO）阶段，利用基于提示引导问题的残余步骤的细粒度奖励。

Result: 实验证明Table-R1在内部和外部数据集上均能显著提升模型的表格推理性能，大幅优于SFT和GRPO。值得注意的是，搭载Table-R1的Qwen2-VL-7B超越了更大的特定表格理解模型（如Table-LLaVA 13B），甚至在内部数据集上取得了与闭源模型GPT-4o相当的性能。

Conclusion: Table-R1通过其三阶段框架有效克服了强化学习在表格理解中的初始化瓶颈和奖励稀疏性问题，展示了其在推进鲁棒多模态表格理解方面的有效性。

Abstract: Existing table understanding methods face challenges due to complex table
structures and intricate logical reasoning. While supervised finetuning (SFT)
dominates existing research, reinforcement learning (RL), such as Group
Relative Policy Optimization (GRPO), has shown promise but struggled with low
initial policy accuracy and coarse rewards in tabular contexts. In this paper,
we introduce Table-R1, a three-stage RL framework that enhances multimodal
table understanding through: (1) Warm-up that prompts initial perception and
reasoning capabilities, (2) Perception Alignment GRPO (PA-GRPO), which employs
continuous Tree-Edit-Distance Similarity (TEDS) rewards for recognizing table
structures and contents, and (3) Hint-Completion GRPO (HC-GRPO), which utilizes
fine-grained rewards of residual steps based on the hint-guided question.
Extensive experiments demonstrate that Table-R1 can boost the model's table
reasoning performance obviously on both held-in and held-out datasets,
outperforming SFT and GRPO largely. Notably, Qwen2-VL-7B with Table-R1
surpasses larger specific table understanding models (e.g., Table-LLaVA 13B),
even achieving comparable performance to the closed-source model GPT-4o on
held-in datasets, demonstrating the efficacy of each stage of Table-R1 in
overcoming initialization bottlenecks and reward sparsity, thereby advancing
robust multimodal table understanding.

</details>


### [331] [CLaC at DISRPT 2025: Hierarchical Adapters for Cross-Framework Multi-lingual Discourse Relation Classification](https://arxiv.org/abs/2509.16903)
*Nawar Turk,Daniele Comitogianni,Leila Kosseim*

Main category: cs.CL

TL;DR: 本文提交了DISRPT 2025共享任务第三阶段（语篇关系分类）的解决方案，该任务涉及多语言、跨框架的统一语篇关系标签。研究通过微调多语言BERT模型、评估提示式大型语言模型，并引入了HiDAC（分层双适配器对比学习模型），结果显示HiDAC在参数效率更高的情况下达到了最高准确率。


<details>
  <summary>Details</summary>
Motivation: DISRPT 2025共享任务第三阶段引入了跨39个语料库、16种语言和6种语篇框架的17个统一语篇关系标签集，这带来了显著的多语言和跨形式主义挑战，促使研究人员探索有效的分类方法。

Method: 研究方法包括：1. 使用两种论元排序策略和渐进式解冻比例微调多语言BERT模型（mBERT, XLM-RoBERTa-Base, XLM-RoBERTa-Large）以建立基线；2. 在零样本和少样本设置下评估提示式大型语言模型（Claude Opus 4.0）对新统一标签的响应；3. 引入HiDAC（Hierarchical Dual-Adapter Contrastive learning model，分层双适配器对比学习模型）。

Result: 主要结果显示：1. 较大的Transformer模型准确率有所提高，但提升幅度不大；2. 解冻75%的编码器层能达到与完全微调相当的性能，但训练参数更少；3. 提示式模型显著落后于微调的Transformer模型；4. HiDAC实现了最高的整体准确率（67.5%），同时比完全微调更具参数效率。

Conclusion: 研究得出结论，在多语言和跨框架的语篇关系分类任务中，HiDAC模型表现出卓越的性能和参数效率，优于传统的微调Transformer模型和提示式大型语言模型。此外，部分解冻策略也能在保持性能的同时提高训练效率。

Abstract: We present our submission to Task 3 (Discourse Relation Classification) of
the DISRPT 2025 shared task. Task 3 introduces a unified set of 17 discourse
relation labels across 39 corpora in 16 languages and six discourse frameworks,
posing significant multilingual and cross-formalism challenges. We first
benchmark the task by fine-tuning multilingual BERT-based models (mBERT,
XLM-RoBERTa-Base, and XLM-RoBERTa-Large) with two argument-ordering strategies
and progressive unfreezing ratios to establish strong baselines. We then
evaluate prompt-based large language models (namely Claude Opus 4.0) in
zero-shot and few-shot settings to understand how LLMs respond to the newly
proposed unified labels. Finally, we introduce HiDAC, a Hierarchical
Dual-Adapter Contrastive learning model. Results show that while larger
transformer models achieve higher accuracy, the improvements are modest, and
that unfreezing the top 75% of encoder layers yields performance comparable to
full fine-tuning while training far fewer parameters. Prompt-based models lag
significantly behind fine-tuned transformers, and HiDAC achieves the highest
overall accuracy (67.5%) while remaining more parameter-efficient than full
fine-tuning.

</details>


### [332] [CUTE: A Multilingual Dataset for Enhancing Cross-Lingual Knowledge Transfer in Low-Resource Languages](https://arxiv.org/abs/2509.16914)
*Wenhao Zhuang,Yuan Sun*

Main category: cs.CL

TL;DR: 为解决大型语言模型在低资源语言（如维吾尔语、藏语）上的性能不足问题，本文构建并开源了CUTE数据集，包含中文、英文、维吾尔语和藏语的并行与非并行语料，并验证了其在提升LLM低资源语言处理能力上的有效性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在资源丰富的语言上表现出卓越的零样本能力，但对于多样化的低资源语言，支持不足，主要原因在于训练语料的稀缺。

Method: 通过机器翻译构建了CUTE数据集，包含两个25GB的四语言语料集（一个并行，一个非并行）。数据集涵盖中文、英文、维吾尔语和藏语。在构建前，通过人工评估验证了中文-维吾尔语和中文-藏语机器翻译质量与中文-英语翻译质量接近。

Result: CUTE是目前最大的维吾尔语和藏语开源语料库。实验证明CUTE能有效提升LLMs处理低资源语言的能力，并有助于研究语料并行性在跨语言迁移学习中的作用。CUTE语料库及相关模型已公开发布。

Conclusion: CUTE数据集成功解决了低资源语言（特别是维吾尔语和藏语）语料稀缺的问题，有效提升了LLMs对这些语言的处理能力，并为跨语言迁移学习研究提供了宝贵的资源。

Abstract: Large Language Models (LLMs) demonstrate exceptional zero-shot capabilities
in various NLP tasks, significantly enhancing user experience and efficiency.
However, this advantage is primarily limited to resource-rich languages. For
the diverse array of low-resource languages, support remains inadequate, with
the scarcity of training corpora considered the primary cause. We construct and
open-source CUTE Chinese, Uyghur, Tibetan,English dataset, consisting of two
25GB sets of four-language corpora (one parallel and one non-parallel),
obtained through machine translation. CUTE encompasses two resource-rich
languages (Chinese and English) and two low-resource languages (Uyghur and
Tibetan). Prior to constructing CUTE, human assessment validates that the
machine translation quality between Chinese-Uyghur and Chinese-Tibetan
approaches that of Chinese-English translation. CUTE represents the largest
open-source corpus for Uyghur and Tibetan languages to date, and we demonstrate
its effectiveness in enhancing LLMs' ability to process low-resource languages
while investigating the role of corpus parallelism in cross-lingual transfer
learning. The CUTE corpus and related models are made publicly available to the
research community.

</details>


### [333] [K-DeCore: Facilitating Knowledge Transfer in Continual Structured Knowledge Reasoning via Knowledge Decoupling](https://arxiv.org/abs/2509.16929)
*Yongrui Chen,Yi Huang,Yunchang Liu,Shenyu Zhang,Junhao He,Tongtong Wu,Guilin Qi,Tianxing Wu*

Main category: cs.CL

TL;DR: 本文提出了一种名为K-DeCore的新型持续结构化知识推理（CSKR）框架，该框架采用固定数量的可调参数，通过知识解耦机制、双视角记忆整合和结构引导的伪数据合成策略，有效解决了现有方法在异构结构化知识上的泛化能力差和参数增长导致的推理效率低下问题。


<details>
  <summary>Details</summary>
Motivation: 现有持续学习方法在持续结构化知识推理（CSKR）任务中面临挑战，包括对异构结构化知识的泛化能力差，以及随着任务增加导致的参数增长，导致推理效率低下。

Method: 本文提出了K-DeCore框架，其特点是拥有固定数量的可调参数。它引入了知识解耦机制，将推理过程分解为任务特定和任务无关阶段。在此基础上，K-DeCore集成了针对不同阶段的双视角记忆整合机制，并引入了结构引导的伪数据合成策略，以进一步增强模型的泛化能力。

Result: 在四个基准数据集上进行的广泛实验表明，K-DeCore在多个指标上均优于现有的持续学习方法，并且适用于各种骨干大型语言模型。

Conclusion: K-DeCore框架通过其创新的知识解耦和记忆巩固机制，以及伪数据合成策略，成功克服了现有持续学习方法在持续结构化知识推理任务中的局限性，显著提高了模型的泛化能力和效率。

Abstract: Continual Structured Knowledge Reasoning (CSKR) focuses on training models to
handle sequential tasks, where each task involves translating natural language
questions into structured queries grounded in structured knowledge. Existing
general continual learning approaches face significant challenges when applied
to this task, including poor generalization to heterogeneous structured
knowledge and inefficient reasoning due to parameter growth as tasks increase.
To address these limitations, we propose a novel CSKR framework,
\textsc{K-DeCore}, which operates with a fixed number of tunable parameters.
Unlike prior methods, \textsc{K-DeCore} introduces a knowledge decoupling
mechanism that disentangles the reasoning process into task-specific and
task-agnostic stages, effectively bridging the gaps across diverse tasks.
Building on this foundation, \textsc{K-DeCore} integrates a dual-perspective
memory consolidation mechanism for distinct stages and introduces a
structure-guided pseudo-data synthesis strategy to further enhance the model's
generalization capabilities. Extensive experiments on four benchmark datasets
demonstrate the superiority of \textsc{K-DeCore} over existing continual
learning methods across multiple metrics, leveraging various backbone large
language models.

</details>


### [334] [AirQA: A Comprehensive QA Dataset for AI Research with Instance-Level Evaluation](https://arxiv.org/abs/2509.16952)
*Tiancheng Huang,Ruisheng Cao,Yuxin Zhang,Zhangyi Kang,Zijian Wang,Chenrun Wang,Yijie Luo,Hang Zheng,Lirong Qian,Lu Chen,Kai Yu*

Main category: cs.CL

TL;DR: 针对学术论文信息提取的挑战，本文提出了AirQA数据集和ExTrActor框架。AirQA是一个综合性人工标注的论文问答数据集，用于评估大型语言模型（LLM）代理。ExTrActor是一个自动化指令数据合成框架，用于训练LLM代理，能显著提升小型模型的性能。


<details>
  <summary>Details</summary>
Motivation: 随着学术论文数量的激增，研究人员难以高效提取关键信息。尽管基于LLM的代理可以自动化科学论文的问答工作流，但仍缺乏全面且真实的基准来评估其能力，同时高质量的交互轨迹不足也阻碍了交互式代理的训练。

Method: 本文提出：1. AirQA，一个在人工智能领域经过人工标注的综合性论文问答数据集，包含13,948篇论文和1,246个问题，支持多任务、多模态和实例级评估。2. ExTrActor，一个自动化指令数据合成框架，利用三个基于LLM的代理，无需人工干预即可进行示例生成和轨迹收集。

Result: 对多种开源和专有模型的评估显示，大多数模型在AirQA上的表现不佳，证明了该数据集的质量。广泛的实验证实，ExTrActor能持续提升小型模型的多轮工具使用能力，使其性能可与大型模型媲美。

Conclusion: AirQA为评估LLM代理在科学论文问答方面的能力提供了一个全面且高质量的基准。ExTrActor框架有效解决了训练数据不足的问题，并通过自动化合成指令数据，显著提高了小型模型的多轮工具使用能力，使其达到与大型模型相当的水平。

Abstract: The growing volume of academic papers has made it increasingly difficult for
researchers to efficiently extract key information. While large language models
(LLMs) based agents are capable of automating question answering (QA) workflows
for scientific papers, there still lacks a comprehensive and realistic
benchmark to evaluate their capabilities. Moreover, training an interactive
agent for this specific task is hindered by the shortage of high-quality
interaction trajectories. In this work, we propose AirQA, a human-annotated
comprehensive paper QA dataset in the field of artificial intelligence (AI),
with 13,948 papers and 1,246 questions, that encompasses multi-task,
multi-modal and instance-level evaluation. Furthermore, we propose ExTrActor,
an automated framework for instruction data synthesis. With three LLM-based
agents, ExTrActor can perform example generation and trajectory collection
without human intervention. Evaluations of multiple open-source and proprietary
models show that most models underperform on AirQA, demonstrating the quality
of our dataset. Extensive experiments confirm that ExTrActor consistently
improves the multi-turn tool-use capability of small models, enabling them to
achieve performance comparable to larger ones.

</details>


### [335] [Preference Distillation via Value based Reinforcement Learning](https://arxiv.org/abs/2509.16965)
*Minchan Kwon,Junwon Ko,Kangil Kim,Junmo Kim*

Main category: cs.CL

TL;DR: DPO的二元监督对小模型训练不足，本文提出TVKD方法，通过引入教师模型价值函数作为辅助奖励，为DPO训练提供软性指导，显著提升了小模型在各种基准上的性能。


<details>
  <summary>Details</summary>
Motivation: 直接偏好优化（DPO）的二元胜负监督对于训练容量有限的小模型往往不够。现有的知识蒸馏方法主要侧重于模仿行为或KL散度，但忽略了奖励建模的蒸馏。

Method: 本文提出了“基于教师价值的知识蒸馏”（TVKD）方法。它从教师模型的价值函数中引入一个辅助奖励，为DPO训练提供软性指导。该辅助奖励被设计为满足基于潜力的奖励塑形，从而确保DPO的全局奖励结构和最优策略得以保留。TVKD可以集成到标准的DPO训练框架中，并且不需要额外的rollout。

Result: 实验结果表明，TVKD在各种基准和模型尺寸上持续改进了模型的性能。

Conclusion: TVKD通过引入教师模型价值函数作为辅助奖励，有效解决了DPO在训练小模型时监督不足的问题，并在不改变DPO核心结构和最优策略的前提下，显著提升了模型性能。

Abstract: Direct Preference Optimization (DPO) is a powerful paradigm to align language
models with human preferences using pairwise comparisons. However, its binary
win-or-loss supervision often proves insufficient for training small models
with limited capacity. Prior works attempt to distill information from large
teacher models using behavior cloning or KL divergence. These methods often
focus on mimicking current behavior and overlook distilling reward modeling. To
address this issue, we propose \textit{Teacher Value-based Knowledge
Distillation} (TVKD), which introduces an auxiliary reward from the value
function of the teacher model to provide a soft guide. This auxiliary reward is
formulated to satisfy potential-based reward shaping, ensuring that the global
reward structure and optimal policy of DPO are preserved. TVKD can be
integrated into the standard DPO training framework and does not require
additional rollouts. Our experimental results show that TVKD consistently
improves performance across various benchmarks and model sizes.

</details>


### [336] [Advancing Speech Understanding in Speech-Aware Language Models with GRPO](https://arxiv.org/abs/2509.16990)
*Avishai Elmakies,Hagai Aronowitz,Nimrod Shabtay,Eli Schwartz,Ron Hoory,Avihu Dekel*

Main category: cs.CL

TL;DR: 本文提出了一种基于GRPO（Group Relative Policy Optimization）的方法，并结合BLEU作为奖励信号，用于在开放格式语音理解任务（如口语问答和自动语音翻译）上训练语音感知大型语言模型（SALLMs）。实验证明该方法优于标准SFT，并探讨了引入离策略样本的潜力。


<details>
  <summary>Details</summary>
Motivation: SALLMs在语音理解任务中表现出色。GRPO在训练LLMs方面效率很高，并且之前已探索其在SALLMs上的应用，但主要局限于多项选择任务。为了更好地反映模型的生成能力，需要将其扩展到开放格式任务。

Method: 本研究利用GRPO，并以BLEU作为奖励信号来优化SALLMs。此外，还探索了在GRPO中纳入离策略（off-policy）样本的潜力。

Result: 实验结果表明，该方法在多个关键指标上超越了标准的SFT（监督微调）。

Conclusion: GRPO结合BLEU能有效提升SALLMs在开放格式语音理解任务上的表现。引入离策略样本为进一步改进和未来研究提供了方向。

Abstract: In this paper, we introduce a Group Relative Policy Optimization (GRPO)-based
method for training Speech-Aware Large Language Models (SALLMs) on open-format
speech understanding tasks, such as Spoken Question Answering and Automatic
Speech Translation. SALLMs have proven highly effective for speech
understanding tasks. GRPO has recently gained traction for its efficiency in
training LLMs, and prior work has explored its application to SALLMs, primarily
in multiple-choice tasks. Building on this, we focus on open-format tasks that
better reflect the generative abilities of the models. Our approach leverages
GRPO with BLEU as the reward signal to optimize SALLMs, and we demonstrate
empirically that it surpasses standard SFT across several key metrics. Finally,
we explore the potential of incorporating off-policy samples within GRPO for
these tasks, highlighting avenues for further improvement and further research.

</details>


### [337] [The Transfer Neurons Hypothesis: An Underlying Mechanism for Language Latent Space Transitions in Multilingual LLMs](https://arxiv.org/abs/2509.17030)
*Hinata Tezuka,Naoya Inoue*

Main category: cs.CL

TL;DR: 本文提出了“转换神经元假说”，并经验性地验证了多语言大型语言模型（LLMs）中某些MLP神经元负责在特定语言和共享语义潜在空间之间转换表示，从而深化了对多语言LLMs处理机制的理解。


<details>
  <summary>Details</summary>
Motivation: 现有研究提出了多语言LLMs的解码器处理框架（早期层转换为以英语为中心的表示，中间层进行推理，最终层转换回特定语言），但这种转换的内部动态和底层机制尚未得到充分探索。

Method: 本文提出了“转换神经元假说”，并对其进行了经验性验证。此外，还探讨了近期研究中识别出的特定语言神经元的功能。

Result: 研究结果表明：1. 某些MLP模块中的神经元负责在特定语言潜在空间和共享语义潜在空间之间转换表示，验证了“转换神经元假说”。2. 特定语言神经元的一个功能是促进潜在空间之间的移动。3. 转换神经元对于多语言LLMs的推理至关重要。

Conclusion: 本文通过提出并验证“转换神经元假说”，揭示了多语言LLMs中表示转换的内部机制，并强调了转换神经元在多语言推理中的关键作用，从而加深了对多语言LLMs处理框架的理解。

Abstract: Recent studies have suggested a processing framework for multilingual inputs
in decoder-based LLMs: early layers convert inputs into English-centric and
language-agnostic representations; middle layers perform reasoning within an
English-centric latent space; and final layers generate outputs by transforming
these representations back into language-specific latent spaces. However, the
internal dynamics of such transformation and the underlying mechanism remain
underexplored. Towards a deeper understanding of this framework, we propose and
empirically validate The Transfer Neurons Hypothesis: certain neurons in the
MLP module are responsible for transferring representations between
language-specific latent spaces and a shared semantic latent space.
Furthermore, we show that one function of language-specific neurons, as
identified in recent studies, is to facilitate movement between latent spaces.
Finally, we show that transfer neurons are critical for reasoning in
multilingual LLMs.

</details>


### [338] [Modeling Bottom-up Information Quality during Language Processing](https://arxiv.org/abs/2509.17047)
*Cui Ding,Yanning Yin,Lena A. Jäger,Ethan Gotlieb Wilcox*

Main category: cs.CL

TL;DR: 本研究通过信息论方法量化视觉输入的“质量”，并在英语和汉语阅读中测试了底部输入质量对阅读时间的影响，发现输入质量降低会增加阅读时间，且词语上半部分的信息量大于下半部分，这种不对称性在英语中更明显。


<details>
  <summary>Details</summary>
Motivation: 当代语言处理理论认为，自上而下的预期和自下而上的输入共同作用，并预测嘈杂的底部输入会增加理解难度。本研究旨在阅读领域验证这一预测。

Method: ['提出并使用互信息（MI）来操作性定义视觉信息与词语身份之间的“底部信息质量”。', '将这一预测形式化为一个基于贝叶斯更新的阅读数学模型。', '通过比较参与者阅读完整词语与上半部分或下半部分被遮挡的词语的阅读时间来测试信息质量操作性定义，数据收集自英语和汉语。', '使用多模态语言模型估计视觉输入与词语之间的互信息。', '分析信息在视觉形式（词语上半部分与下半部分）中的分布情况。']

Result: ['信息质量的降低（通过遮挡词语实现）导致阅读时间的增加。', '在英语和汉语中，词语的上半部分比下半部分包含更多关于词语身份的信息。', '这种信息不对称性在英语中比汉语中更为显著，并且这种模式反映在阅读时间上。']

Conclusion: 底部输入质量（通过互信息量化）确实调节了阅读的难易程度。词语视觉信息分布存在不对称性（上半部分信息量更大），这种不对称性在不同语言（英语比汉语更明显）中有所不同，并影响阅读时间。

Abstract: Contemporary theories model language processing as integrating both top-down
expectations and bottom-up inputs. One major prediction of such models is that
the quality of the bottom-up inputs modulates ease of processing -- noisy
inputs should lead to difficult and effortful comprehension. We test this
prediction in the domain of reading. First, we propose an information-theoretic
operationalization for the "quality" of bottom-up information as the mutual
information (MI) between visual information and word identity. We formalize
this prediction in a mathematical model of reading as a Bayesian update.
Second, we test our operationalization by comparing participants' reading times
in conditions where words' information quality has been reduced, either by
occluding their top or bottom half, with full words. We collect data in English
and Chinese. We then use multimodal language models to estimate the mutual
information between visual inputs and words. We use these data to estimate the
specific effect of reduced information quality on reading times. Finally, we
compare how information is distributed across visual forms. In English and
Chinese, the upper half contains more information about word identity than the
lower half. However, the asymmetry is more pronounced in English, a pattern
which is reflected in the reading times.

</details>


### [339] [TactfulToM: Do LLMs Have the Theory of Mind Ability to Understand White Lies?](https://arxiv.org/abs/2509.17054)
*Yiwei Liu,Emma Jane Pretty,Jiahao Huang,Saku Sugawara*

Main category: cs.CL

TL;DR: 本研究引入了TactfulToM基准，旨在评估大型语言模型（LLM）理解善意谎言及其背后亲社会动机的能力，发现当前最先进的LLM在此方面远低于人类水平。


<details>
  <summary>Details</summary>
Motivation: 现有研究在大型语言模型（LLM）的心智理论（ToM）推理任务中，对需要细致社会情境（如善意谎言）的能力探索有限，特别是理解其亲社会动机。

Method: 引入了名为TactfulToM的英语基准。该基准通过多阶段人机协作流程生成，其中LLM将手动设计的种子故事扩展为对话，以保持真实善意谎言所需参与者之间的信息不对称。

Result: 最先进的LLM在TactfulToM基准上的表现远低于人类，揭示了它们在完全理解善意谎言所涉及的心智理论推理方面的不足。

Conclusion: LLM在理解善意谎言及其亲社会动机所需的心智理论推理方面存在明显缺陷，未能达到人类水平，这表明当前模型在复杂社会情境理解上仍有很大提升空间。

Abstract: While recent studies explore Large Language Models' (LLMs) performance on
Theory of Mind (ToM) reasoning tasks, research on ToM abilities that require
more nuanced social context is limited, such as white lies. We introduce
TactfulToM, a novel English benchmark designed to evaluate LLMs' ability to
understand white lies within real-life conversations and reason about prosocial
motivations behind them, particularly when they are used to spare others'
feelings and maintain social harmony. Our benchmark is generated through a
multi-stage human-in-the-loop pipeline where LLMs expand manually designed seed
stories into conversations to maintain the information asymmetry between
participants necessary for authentic white lies. We show that TactfulToM is
challenging for state-of-the-art models, which perform substantially below
humans, revealing shortcomings in their ability to fully comprehend the ToM
reasoning that enables true understanding of white lies.

</details>


### [340] [SFT-TA: Supervised Fine-Tuned Agents in Multi-Agent LLMs for Automated Inductive Thematic Analysis](https://arxiv.org/abs/2509.17167)
*Seungjun Yi,Joakim Nguyen,Huimin Xu,Terence Lim,Joseph Skrovan,Mehak Beri,Hitakshi Modi,Andrew Well,Liu Leqi,Mia Markey,Ying Ding*

Main category: cs.CL

TL;DR: 本文提出SFT-TA，一个将监督微调（SFT）智能体嵌入多智能体系统中的自动化主题分析框架，显著提高了与人类参考主题的一致性。


<details>
  <summary>Details</summary>
Motivation: 传统主题分析耗时且难以扩展。尽管大型语言模型（LLMs）为自动化提供了可能，但其与人类结果的一致性仍有限。

Method: 本文提出了SFT-TA框架，该框架在一个多智能体系统中嵌入了经过监督微调（SFT）的智能体，旨在自动化主题分析。

Result: SFT-TA框架在与人类参考主题的一致性方面优于现有框架和GPT-4o基线。研究发现，单独的SFT智能体表现可能不佳，但在多智能体系统中嵌入后，其结果优于基线。

Conclusion: 将SFT智能体以特定角色嵌入多智能体系统是提高主题分析与期望输出一致性的一种有前景的方法。

Abstract: Thematic Analysis (TA) is a widely used qualitative method that provides a
structured yet flexible framework for identifying and reporting patterns in
clinical interview transcripts. However, manual thematic analysis is
time-consuming and limits scalability. Recent advances in LLMs offer a pathway
to automate thematic analysis, but alignment with human results remains
limited. To address these limitations, we propose SFT-TA, an automated thematic
analysis framework that embeds supervised fine-tuned (SFT) agents within a
multi-agent system. Our framework outperforms existing frameworks and the
gpt-4o baseline in alignment with human reference themes. We observed that SFT
agents alone may underperform, but achieve better results than the baseline
when embedded within a multi-agent system. Our results highlight that embedding
SFT agents in specific roles within a multi-agent system is a promising pathway
to improve alignment with desired outputs for thematic analysis.

</details>


### [341] [FlagEval Findings Report: A Preliminary Evaluation of Large Reasoning Models on Automatically Verifiable Textual and Visual Questions](https://arxiv.org/abs/2509.17177)
*Bowen Qin,Chen Yue,Fang Yin,Hui Wang,JG Yao,Jiakang Liu,Jing-Shu Zheng,Miguel Hu Chen,Richeng Xuan,Shibei Meng,Shiqi Zhou,Teng Dai,Tong-Shuai Ren,Wei Cui,Xi Yang,Xialin Du,Xiaojing Xu,Xue Sun,Xuejing Li,Yaming Liu,Yesheng Liu,Ying Liu,Yonghua Lin,Yu Zhao,Yunduo Zhang,Yuwen Luo,Zheqi He,Zhiyuan He,Zhongyuan Wang*

Main category: cs.CL

TL;DR: 本文对当前大型推理模型（LRMs）进行了中等规模的无污染评估，并发布了用于测试视觉语言模型（VLMs）视觉线索推理能力的ROME基准。


<details>
  <summary>Details</summary>
Motivation: 研究动机是对当前大型推理模型（LRMs）进行一定程度的无污染评估，以了解其推理能力，特别是从视觉线索进行推理的能力。

Method: 研究方法是进行中等规模的、一定程度上无污染的评估；同时发布了ROME基准，该基准旨在测试视觉语言模型（VLMs）从视觉线索进行推理的能力。

Result: 研究结果包括对当前大型推理模型（LRMs）的一些初步发现（尽管摘要未详细说明），以及发布了新的视觉语言模型评估基准ROME。

Conclusion: 结论是研究团队对大型推理模型进行了评估，并成功推出了一个名为ROME的评估基准，专门用于测试视觉语言模型从视觉线索进行推理的能力。

Abstract: We conduct a moderate-scale contamination-free (to some extent) evaluation of
current large reasoning models (LRMs) with some preliminary findings. We also
release ROME, our evaluation benchmark for vision language models intended to
test reasoning from visual clues. We attach links to the benchmark, evaluation
data, and other updates on this website:
https://flageval-baai.github.io/LRM-Eval/

</details>


### [342] [Attention Consistency for LLMs Explanation](https://arxiv.org/abs/2509.17178)
*Tian Lan,Jinyuan Xu,Xue He,Jenq-Neng Hwang,Lei Li*

Main category: cs.CL

TL;DR: 本文提出MACS（多层注意力一致性分数），一种轻量级启发式方法，用于估计解码器模型中输入token的重要性，以提高LLM可解释性并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 理解大型语言模型（LLM）的决策过程对于其可信赖的开发和部署至关重要。然而，现有的可解释性方法常面临分辨率低和计算成本高的问题。

Method: 提出多层注意力一致性分数（MACS），这是一种新颖、轻量且易于部署的启发式方法，用于估计解码器模型中输入token的重要性。MACS通过测量最大注意力的一致性来评估输入token的贡献。

Result: 实证评估表明，MACS在可解释性质量和计算效率之间取得了有利的平衡，其忠实度可与复杂技术媲美，同时VRAM使用量减少22%，延迟降低30%。

Conclusion: MACS为理解LLM的决策过程提供了一种高效且高质量的解决方案，克服了现有可解释性方法的局限性，实现了可解释性质量和计算效率的良好权衡。

Abstract: Understanding the decision-making processes of large language models (LLMs)
is essential for their trustworthy development and deployment. However, current
interpretability methods often face challenges such as low resolution and high
computational cost. To address these limitations, we propose the
\textbf{Multi-Layer Attention Consistency Score (MACS)}, a novel, lightweight,
and easily deployable heuristic for estimating the importance of input tokens
in decoder-based models. MACS measures contributions of input tokens based on
the consistency of maximal attention. Empirical evaluations demonstrate that
MACS achieves a favorable trade-off between interpretability quality and
computational efficiency, showing faithfulness comparable to complex techniques
with a 22\% decrease in VRAM usage and 30\% reduction in latency.

</details>


### [343] [LifeAlign: Lifelong Alignment for Large Language Models with Memory-Augmented Focalized Preference Optimization](https://arxiv.org/abs/2509.17183)
*Junsong Li,Jie Zhou,Bihao Zhan,Yutao Yang,Qianjun Pan,Shilian Chen,Tianyu Huai,Xin Li,Qin Chen,Liang He*

Main category: cs.CL

TL;DR: LifeAlign是一个终身对齐框架，使大型语言模型在顺序学习任务中保持与人类偏好的一致性，同时避免灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 传统的大型语言模型对齐方法在适应新偏好或领域时存在灾难性遗忘问题，导致模型失去先前获得的知识。

Method: LifeAlign包含两项创新：1. 焦点偏好优化策略，在与新偏好对齐的同时防止侵蚀旧知识。2. 短期到长期记忆巩固机制，利用内在降维技术将去噪的短期偏好表示整合到稳定的长期记忆中，以实现高效的对齐模式存储和检索。

Result: 在跨多个领域和偏好类型的顺序对齐任务中，LifeAlign在保持偏好对齐质量和知识保留方面均优于现有终身学习方法。

Conclusion: LifeAlign框架成功解决了大型语言模型在顺序学习任务中保持与人类偏好一致性同时避免灾难性遗忘的挑战。

Abstract: Alignment plays a crucial role in Large Language Models (LLMs) in aligning
with human preferences on a specific task/domain. Traditional alignment methods
suffer from catastrophic forgetting, where models lose previously acquired
knowledge when adapting to new preferences or domains. We introduce LifeAlign,
a novel framework for lifelong alignment that enables LLMs to maintain
consistent human preference alignment across sequential learning tasks without
forgetting previously learned knowledge. Our approach consists of two key
innovations. First, we propose a focalized preference optimization strategy
that aligns LLMs with new preferences while preventing the erosion of knowledge
acquired from previous tasks. Second, we develop a short-to-long memory
consolidation mechanism that merges denoised short-term preference
representations into stable long-term memory using intrinsic dimensionality
reduction, enabling efficient storage and retrieval of alignment patterns
across diverse domains. We evaluate LifeAlign across multiple sequential
alignment tasks spanning different domains and preference types. Experimental
results demonstrate that our method achieves superior performance in
maintaining both preference alignment quality and knowledge retention compared
to existing lifelong learning approaches. The codes and datasets will be
released on GitHub.

</details>


### [344] [Evolution of Concepts in Language Model Pre-Training](https://arxiv.org/abs/2509.17196)
*Xuyang Ge,Wentao Shu,Jiaxing Wu,Yunhua Zhou,Zhengfu He,Xipeng Qiu*

Main category: cs.CL

TL;DR: 本文使用稀疏字典学习方法（crosscoders）追踪预训练语言模型中线性可解释特征的演变，揭示了特征形成的时间点和复杂模式的出现阶段，并将其与Transformer的两阶段学习过程联系起来。


<details>
  <summary>Details</summary>
Motivation: 尽管预训练赋予语言模型强大的能力，但预训练过程本身仍是一个黑箱。理解模型内部特征的演变对于揭示其学习动态至关重要。

Method: 研究人员采用名为crosscoders的稀疏字典学习方法，追踪预训练快照中线性可解释特征的演变。此外，他们还进行了特征归因分析，以揭示特征演变与下游性能之间的因果关系。

Result: 研究发现，大多数特征在大约一个特定点开始形成，而更复杂的模式则出现在训练后期。特征归因分析揭示了特征演变与下游性能之间的因果联系。这些特征层面的观察与Transformer的两阶段学习过程（统计学习阶段和特征学习阶段）高度一致。

Conclusion: 这项工作为追踪语言模型学习动态中细粒度表示的进展开辟了可能性，有助于深入理解预训练过程。

Abstract: Language models obtain extensive capabilities through pre-training. However,
the pre-training process remains a black box. In this work, we track linear
interpretable feature evolution across pre-training snapshots using a sparse
dictionary learning method called crosscoders. We find that most features begin
to form around a specific point, while more complex patterns emerge in later
training stages. Feature attribution analyses reveal causal connections between
feature evolution and downstream performance. Our feature-level observations
are highly consistent with previous findings on Transformer's two-stage
learning process, which we term a statistical learning phase and a feature
learning phase. Our work opens up the possibility to track fine-grained
representation progress during language model learning dynamics.

</details>


### [345] [Prompt-Based Simplification for Plain Language using Spanish Language Models](https://arxiv.org/abs/2509.17209)
*Lourdes Moreno,Jesus M. Sanchez-Gomez,Marco Antonio Sanchez-Escudero,Paloma Martínez*

Main category: cs.CL

TL;DR: 本文描述了HULAT-UC3M团队参加CLEARS 2025子任务1（西班牙语文本简化为通俗语言）的策略。团队探索了基于西班牙语训练模型的零样本提示工程和LoRA微调方法，最终系统在语义相似性上排名第一，但在可读性上排名第四，并讨论了数据异构性和评估指标的局限性。


<details>
  <summary>Details</summary>
Motivation: 参与CLEARS 2025子任务1，旨在将西班牙语文本适应为通俗语言（Plain Language, PL），以提高文本的可读性和可理解性。

Method: 团队采用了基于西班牙语训练模型的策略，包括使用提示工程的零样本配置和使用低秩适应（LoRA）的微调版本。通过官方任务指标（余弦相似度SIM和Fernández-Huerta可读性指数FH）在训练数据的内部子集上评估不同策略，以选择最佳模型和提示组合。最终系统结合了标准化步骤、RigoChat-7B-v2模型和一个专门面向PL的提示。

Result: 最终选定的系统在语义相似性（SIM = 0.75）方面排名第一，但在可读性（FH = 69.72）方面排名第四。研究还发现训练数据异构性带来了挑战。

Conclusion: 该研究成功地在语义相似性方面取得了领先地位，但也揭示了当前评估指标在同时捕捉语言清晰度和内容保留方面的局限性，以及训练数据异构性带来的关键挑战。

Abstract: This paper describes the participation of HULAT-UC3M in CLEARS 2025 Subtask
1: Adaptation of Text to Plain Language (PL) in Spanish. We explored strategies
based on models trained on Spanish texts, including a zero-shot configuration
using prompt engineering and a fine-tuned version with Low-Rank Adaptation
(LoRA). Different strategies were evaluated on representative internal subsets
of the training data, using the official task metrics, cosine similarity (SIM)
and the Fern\'andez-Huerta readability index (FH) to guide the selection of the
optimal model and prompt combination. The final system was selected for its
balanced and consistent performance, combining normalization steps, the
RigoChat-7B-v2 model, and a dedicated PL-oriented prompt. It ranked first in
semantic similarity (SIM = 0.75), however, fourth in readability (FH = 69.72).
We also discuss key challenges related to training data heterogeneity and the
limitations of current evaluation metrics in capturing both linguistic clarity
and content preservation.

</details>


### [346] [Extending Automatic Machine Translation Evaluation to Book-Length Documents](https://arxiv.org/abs/2509.17249)
*Kuang-Da Wang,Shuoyang Ding,Chao-Han Huck Yang,Ping-Chun Hsieh,Wen-Chih Peng,Vitaly Lavrukhin,Boris Ginsburg*

Main category: cs.CL

TL;DR: 本文提出SEGALE，一种用于长文档翻译评估的新方案，它通过将文档视为连续文本并应用句子分割和对齐方法，扩展了现有自动指标，实现了前所未有的文档级评估，并揭示了LLM在长上下文翻译中的局限性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLM）在翻译性能和长上下文能力方面表现出色，但由于数据集限制、指标中的token数量限制以及严格的句子边界要求，评估方法仍局限于句子级别，无法进行文档级评估。

Method: 本文引入SEGALE评估方案，通过将文档视为连续文本并应用句子分割和对齐方法，将现有自动指标扩展到长文档翻译。该方法能够处理任意长度的文档级翻译，并考虑欠翻译/过翻译以及不同的句子边界。

Result: 实验表明，SEGALE方案显著优于现有长篇文档评估方案，并且与使用真实句子对齐进行的评估相当。此外，将该方案应用于书籍长度的文本时，新发现许多开源LLM未能有效翻译其声称的最大上下文长度的文档。

Conclusion: SEGALE提供了一种强大的文档级翻译评估方法，解决了现有评估方法的局限性。它不仅能够进行更准确的文档级评估，还揭示了许多开源LLM在处理超长上下文翻译时实际表现不佳的问题。

Abstract: Despite Large Language Models (LLMs) demonstrating superior translation
performance and long-context capabilities, evaluation methodologies remain
constrained to sentence-level assessment due to dataset limitations, token
number restrictions in metrics, and rigid sentence boundary requirements. We
introduce SEGALE, an evaluation scheme that extends existing automatic metrics
to long-document translation by treating documents as continuous text and
applying sentence segmentation and alignment methods. Our approach enables
previously unattainable document-level evaluation, handling translations of
arbitrary length generated with document-level prompts while accounting for
under-/over-translations and varied sentence boundaries. Experiments show our
scheme significantly outperforms existing long-form document evaluation
schemes, while being comparable to evaluations performed with groundtruth
sentence alignments. Additionally, we apply our scheme to book-length texts and
newly demonstrate that many open-weight LLMs fail to effectively translate
documents at their reported maximum context lengths.

</details>


### [347] [Probabilistic Token Alignment for Large Language Model Fusion](https://arxiv.org/abs/2509.17276)
*Runjia Zeng,James Chenhao Liang,Cheng Han,Zhiwen Cao,Jiahao Liu,Xiaojun Quan,Yingjie Victor Chen,Lifu Huang,Tong Geng,Qifan Wang,Dongfang Liu*

Main category: cs.CL

TL;DR: 本文提出PTA-LLM，一种基于最优传输的概率性词元对齐方法，用于融合不同架构的预训练大语言模型，克服了传统手动词汇对齐的局限性，并提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 从头训练大型语言模型成本高昂且功能可能重复。现有模型融合方法依赖手动预定义的词汇对齐，泛化性差，导致性能下降。

Method: 受分布学习启发，本文提出了概率性词元对齐方法（PTA-LLM）。该方法将词元对齐创新性地重新表述为一个最优传输的数学问题，利用分布感知学习实现更连贯的模型融合，提供通用且软性的对齐映射。

Result: PTA-LLM在多个能力上提升了目标模型的性能。此外，它从分布角度展现出可解释性，揭示了词元对齐的本质。

Conclusion: 概率性词元对齐（PTA-LLM）是一种通用、软性且可解释的方法，通过将词元对齐转化为最优传输问题，有效解决了现有大语言模型融合中词汇对齐的挑战，显著提升了融合模型的性能。

Abstract: Training large language models (LLMs) from scratch can yield models with
unique functionalities and strengths, but it is costly and often leads to
redundant capabilities. A more cost-effective alternative is to fuse existing
pre-trained LLMs with different architectures into a more powerful model.
However, a key challenge in existing model fusion is their dependence on
manually predefined vocabulary alignment, which may not generalize well across
diverse contexts, leading to performance degradation in several evaluation. To
solve this, we draw inspiration from distribution learning and propose the
probabilistic token alignment method as a general and soft mapping for
alignment, named as PTA-LLM. Our approach innovatively reformulates token
alignment into a classic mathematical problem: optimal transport, seamlessly
leveraging distribution-aware learning to facilitate more coherent model
fusion. Apart from its inherent generality, PTA-LLM exhibits interpretability
from a distributional perspective, offering insights into the essence of the
token alignment. Empirical results demonstrate that probabilistic token
alignment enhances the target model's performance across multiple capabilities.
Our code is avaliable at https://runjia.tech/neurips_pta-llm/.

</details>


### [348] [Automated Knowledge Graph Construction using Large Language Models and Sentence Complexity Modelling](https://arxiv.org/abs/2509.17289)
*Sydney Anuyah,Mehedi Mahmud Kaushik,Krishna Dwarampudi,Rakesh Shiradkar,Arjan Durresi,Sunandan Chakraborty*

Main category: cs.CL

TL;DR: CoDe-KG是一个开源的端到端知识图谱抽取流程，通过结合共指消解和句法分解来提取句子级知识图谱。它贡献了大规模数据集和标注，并在关系抽取和句子简化任务上取得了显著的性能提升，特别是在稀有关系召回率方面。


<details>
  <summary>Details</summary>
Motivation: 提升句子级知识图谱的抽取能力，通过整合鲁棒的共指消解和句法分解来更有效地处理复杂的句子结构。

Method: 引入了CoDe-KG流程，该流程结合了鲁棒的共指消解和句法句子分解。在句子简化方面，系统地选择了最优的提示-模型对，采用了混合思维链和少样本提示。

Result: 贡献了一个包含超过15万个知识三元组的数据集，以及用于句子复杂性、共指消解、句子转换策略和三元组的黄金标注训练语料。在句子简化任务上，混合思维链和少样本提示实现了高达99.8%的精确匹配准确率。在关系抽取（RE）方面，CoDe-KG在REBEL上取得了65.8%的宏F1（比现有技术提升8点），在WebNLG2上取得了75.7%的微F1，并与Wiki-NRE和CaRB的性能持平或超越。消融研究表明，整合共指消解和分解使稀有关系的召回率提高了20%以上。

Conclusion: CoDe-KG是一个高效的句子级知识图谱抽取流程，其结合共指消解和句法分解的方法显著提升了关系抽取性能，尤其是在稀有关系的召回率方面，并为相关研究提供了丰富的开源数据集和标注。

Abstract: We introduce CoDe-KG, an open-source, end-to-end pipeline for extracting
sentence-level knowledge graphs by combining robust coreference resolution with
syntactic sentence decomposition. Using our model, we contribute a dataset of
over 150,000 knowledge triples, which is open source. We also contribute a
training corpus of 7248 rows for sentence complexity, 190 rows of gold human
annotations for co-reference resolution using open source lung-cancer abstracts
from PubMed, 900 rows of gold human annotations for sentence conversion
policies, and 398 triples of gold human annotations. We systematically select
optimal prompt-model pairs across five complexity categories, showing that
hybrid chain-of-thought and few-shot prompting yields up to 99.8% exact-match
accuracy on sentence simplification. On relation extraction (RE), our pipeline
achieves 65.8% macro-F1 on REBEL, an 8-point gain over the prior state of the
art, and 75.7% micro-F1 on WebNLG2, while matching or exceeding performance on
Wiki-NRE and CaRB. Ablation studies demonstrate that integrating coreference
and decomposition increases recall on rare relations by over 20%. Code and
dataset are available at https://github.com/KaushikMahmud/CoDe-KG_EMNLP_2025

</details>


### [349] [Multi-View Attention Multiple-Instance Learning Enhanced by LLM Reasoning for Cognitive Distortion Detection](https://arxiv.org/abs/2509.17292)
*Jun Seo Kim,Hyemi Kim,Woo Joo Oh,Hongjin Cho,Hochul Lee,Hye Hyeon Kim*

Main category: cs.CL

TL;DR: 该论文提出了一种结合大型语言模型（LLMs）和多实例学习（MIL）的新框架，通过分解情感、逻辑和行为（ELB）组件并利用LLM推断显著性分数，有效提高了认知扭曲的自动检测性能，尤其是在解释性模糊的情况下。


<details>
  <summary>Details</summary>
Motivation: 认知扭曲与心理健康障碍密切相关，但由于语境模糊、共现性和语义重叠，其自动检测仍然具有挑战性。

Method: 研究人员提出了一种结合LLMs和多实例学习（MIL）架构的新框架。该框架将每个话语分解为情感、逻辑和行为（ELB）组件，由LLM处理以推断多个扭曲实例，每个实例都包含预测类型、表达和模型分配的显著性分数。这些实例通过多视图门控注意力机制进行整合以进行最终分类。

Result: 在韩语（KoACD）和英语（Therapist QA）数据集上的实验表明，结合ELB和LLM推断的显著性分数显著提高了分类性能，特别是对于解释性模糊度高的扭曲。这表明该方法能够进行细粒度推理。

Conclusion: 研究结果提出了一种心理学上合理且可推广的方法，用于心理健康自然语言处理中的细粒度推理，有望提高认知扭曲的自动检测能力。

Abstract: Cognitive distortions have been closely linked to mental health disorders,
yet their automatic detection remained challenging due to contextual ambiguity,
co-occurrence, and semantic overlap. We proposed a novel framework that
combines Large Language Models (LLMs) with Multiple-Instance Learning (MIL)
architecture to enhance interpretability and expression-level reasoning. Each
utterance was decomposed into Emotion, Logic, and Behavior (ELB) components,
which were processed by LLMs to infer multiple distortion instances, each with
a predicted type, expression, and model-assigned salience score. These
instances were integrated via a Multi-View Gated Attention mechanism for final
classification. Experiments on Korean (KoACD) and English (Therapist QA)
datasets demonstrate that incorporating ELB and LLM-inferred salience scores
improves classification performance, especially for distortions with high
interpretive ambiguity. Our results suggested a psychologically grounded and
generalizable approach for fine-grained reasoning in mental health NLP.

</details>


### [350] [Scaling, Simplification, and Adaptation: Lessons from Pretraining on Machine-Translated Text](https://arxiv.org/abs/2509.17317)
*Dan John Velasco,Matthew Theodore Roque*

Main category: cs.CL

TL;DR: 研究发现，通过机器翻译（MT）从高资源语言（如英语）生成低资源语言（如印尼语、泰米尔语）数据进行预训练，模型性能可随规模扩展，且在有限原生数据下，MT预训练模型再适应原生数据通常优于仅用原生数据训练的模型。但源端简化有害，且涉及文化细微差别的任务仍需更多原生数据。


<details>
  <summary>Details</summary>
Motivation: 大多数语言缺乏足够的单语数据进行大规模预训练，形成“数据壁垒”。多语言预训练虽有帮助，但受限于语言不平衡和“多语言诅咒”。本文旨在探索通过机器翻译高资源文本作为替代方案，并解决其有效性、源端转换影响以及与有限原生数据结合时的适应性问题。

Method: 将英语文本翻译成印尼语和泰米尔语（两种类型学上遥远、资源较少的语言），并使用原始及LLM简化的英语作为源端。在此基础上，预训练GPT-2模型（124M-774M参数）。通过原生文本上的交叉熵损失、句法探测准确性和下游任务性能来评估模型。

Result: 1. MT预训练模型能从规模扩展中受益。2. 源端简化（如使用LLM简化英语）反而损害了对原生文本的泛化能力。3. MT预训练模型在有限原生文本上进行持续训练后，其性能通常优于仅用原生数据训练的模型，即使原生数据量更少。然而，需要文化细微差别的任务（如毒性检测）仍需更多原生数据的暴露。

Conclusion: 机器翻译生成的数据是解决低资源语言预训练数据稀缺问题的有效途径，通过MT预训练可以很好地与原生数据结合，甚至在原生数据有限的情况下取得更好的性能。但需要注意源端转换的潜在负面影响，并且对于高度依赖文化背景的任务，原生数据的补充仍然至关重要。

Abstract: Most languages lack sufficient data for large-scale monolingual pretraining,
creating a "data wall." Multilingual pretraining helps but is limited by
language imbalance and the "curse of multilinguality." An alternative is to
translate high-resource text with machine translation (MT), which raises three
questions: (1) How does MT-derived data scale with model capacity? (2) Can
source-side transformations (e.g., simplifying English with an LLM) improve
generalization to native text? (3) How well do models pretrained on MT-derived
data adapt when continually trained on limited native text? We investigate
these questions by translating English into Indonesian and Tamil--two
typologically distant, lower-resource languages--and pretraining GPT-2 models
(124M-774M) on native or MT-derived corpora from raw and LLM-simplified
English. We evaluate cross-entropy loss on native text, along with accuracy on
syntactic probes and downstream tasks. Our results show that (1) MT-pretrained
models benefit from scaling; (2) source-side simplification harms
generalization to native text; and (3) adapting MT-pretrained models on native
text often yields better performance than native-only models, even with less
native data. However, tasks requiring cultural nuance (e.g., toxicity
detection) demand more exposure to native data.

</details>


### [351] [AIMMerging: Adaptive Iterative Model Merging Using Training Trajectories for Language Model Continual Learning](https://arxiv.org/abs/2509.17348)
*Yujie Feng,Jian Li,Xiaoyu Dong,Pengfei Xu,Xiaohui Zhou,Yujia Zhang,Zexin LU,Yasha Wang,Alan Zhao,Xu Chu,Xiao-Ming Wu*

Main category: cs.CL

TL;DR: 本文提出了一种名为AimMerging的持续学习框架，通过动态监测训练轨迹中的学习和遗忘信号，自适应地确定模型合并的时机和频率，从而有效平衡大型语言模型在新知识学习和防止遗忘之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在动态真实环境中部署时，持续学习（CL）至关重要，以避免昂贵的重新训练。现有的基于模型合并的方法在平衡新知识学习和防止遗忘方面效果不佳，主要原因是合并次数和频率不理想。

Method: 本文引入了自适应迭代模型合并（AimMerging）框架。它利用训练轨迹中的学习和遗忘信号来动态监控模型的训练状态。一个由训练轨迹引导的合并控制器根据动态监控结果自适应地决定迭代融合的时机和频率，同时一个基于排练的知识融合模块计算合并权重并执行融合。

Result: 在三个持续学习基准测试中，使用不同模型尺寸（从770M到13B）的实验表明，AimMerging比现有最先进的方法取得了显著的性能提升，FWT和BWT的平均相对改进分别达到80%和59%。

Conclusion: AimMerging通过动态监测训练状态并自适应地管理迭代模型合并的时机和频率，成功解决了大型语言模型持续学习中新知识学习与遗忘之间的权衡问题，显著提升了持续学习性能。

Abstract: Continual learning (CL) is essential for deploying large language models
(LLMs) in dynamic real-world environments without the need for costly
retraining. Recent model merging-based methods have attracted significant
attention, but they still struggle to effectively manage the trade-off between
learning new knowledge and preventing forgetting, a challenge largely stemming
from suboptimal number of merges and merging frequency. In this paper, we
introduce Adaptive Iterative Model Merging (AimMerging), a novel CL framework
that utilizes learning and forgetting signals from the training trajectory to
dynamically monitor the model's training status. Guided by dynamic monitoring,
the training trajectory-guided merge controller adaptively determines the
timing and frequency of iterative fusion, while the rehearsal-based knowledge
fusion module computes the merging weights and executes the fusion.
Comprehensive experiments on three CL benchmarks with various model sizes (from
770M to 13B) demonstrate that AimMerging achieves significant performance
improvements over existing state-of-the-art methods, with an average relative
improvement of 80% and 59% on FWT and BWT, respectively. The source code is
provided for reproducibility.

</details>


### [352] [Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation](https://arxiv.org/abs/2509.17349)
*Peter Polák,Sara Papi,Luisa Bentivogli,Ondřej Bojar*

Main category: cs.CL

TL;DR: 本文对同步语音转文本翻译（SimulST）系统的延迟指标进行了全面分析，发现现有指标存在分割相关的结构性偏差，并提出了改进的延迟指标YAAL、LongYAAL以及重新分割工具SoftSegmenter，以实现更准确可靠的评估。


<details>
  <summary>Details</summary>
Motivation: SimulST系统需要在翻译质量和延迟之间取得平衡，但现有延迟测量指标往往不准确、不一致或具有误导性，尤其是在广泛使用的短格式设置中（语音被人为预分割），这阻碍了公平有意义的系统比较。

Method: 本文首先对跨语言对、系统以及短/长格式的SimulST延迟指标进行了首次全面分析，揭示了当前指标中与分割相关的结构性偏差。为解决此问题，作者引入了YAAL（Yet Another Average Lagging）作为改进的短格式延迟指标，并将其扩展为适用于未分割音频的LongYAAL。此外，还提出了基于词级对齐的新型重新分割工具SoftSegmenter。通过实验验证了这些新方法。

Result: 研究发现现有SimulST延迟指标存在与分割相关的结构性偏差。实验表明，YAAL和LongYAAL优于流行的延迟指标，而SoftSegmenter在长格式评估中提升了对齐质量。这些新工具共同使得SimulST系统评估更加可靠。

Conclusion: YAAL、LongYAAL和SoftSegmenter的引入解决了现有SimulST延迟指标的不足，为短格式和长格式的SimulST系统提供了更准确、更可靠的延迟评估方法，从而促进了对SimulST系统的公平比较和发展。

Abstract: Simultaneous speech-to-text translation (SimulST) systems have to balance
translation quality with latency--the delay between speech input and the
translated output. While quality evaluation is well established, accurate
latency measurement remains a challenge. Existing metrics often produce
inconsistent or misleading results, especially in the widely used short-form
setting, where speech is artificially presegmented. In this paper, we present
the first comprehensive analysis of SimulST latency metrics across language
pairs, systems, and both short- and long-form regimes. We uncover a structural
bias in current metrics related to segmentation that undermines fair and
meaningful comparisons. To address this, we introduce YAAL (Yet Another Average
Lagging), a refined latency metric that delivers more accurate evaluations in
the short-form regime. We extend YAAL to LongYAAL for unsegmented audio and
propose SoftSegmenter, a novel resegmentation tool based on word-level
alignment. Our experiments show that YAAL and LongYAAL outperform popular
latency metrics, while SoftSegmenter enhances alignment quality in long-form
evaluation, together enabling more reliable assessments of SimulST systems.

</details>


### [353] [Scale-free Characteristics of Multilingual Legal Texts and the Limitations of LLMs](https://arxiv.org/abs/2509.17367)
*Haoyang Chen,Kumiko Tanaka-Ishii*

Main category: cs.CL

TL;DR: 本文使用无标度指标对法律、通用和AI生成文本的复杂性进行了比较分析，发现法律文本展现出独特的领域特定结构和复杂性，这些是当前生成模型未能完全复制的。


<details>
  <summary>Details</summary>
Motivation: 研究不同领域（特别是专业领域如法律）文本的语言复杂性，并将其与通用自然语言文本以及AI生成文本进行比较。

Method: 采用无标度指标来量化语言复杂性，包括Heaps指数β（词汇增长）、Taylor指数α（词频波动缩放）、压缩率r（冗余度）和熵。分析的语料库涵盖三个领域：法律文档（法规、案例、契约）、通用自然语言文本（文学、维基百科）和AI生成（GPT）文本。

Result: 法律文本比通用文本词汇增长慢（β较低）且术语一致性高（α较高）。在法律领域内部，法规的β最低、α最高，而案例和契约的β较高、α较低。相比之下，GPT生成的文本统计数据更接近通用语言模式。

Conclusion: 法律文本展现出领域特定的结构和复杂性，这些特征是当前生成模型尚未能完全复制的。

Abstract: We present a comparative analysis of text complexity across domains using
scale-free metrics. We quantify linguistic complexity via Heaps' exponent
$\beta$ (vocabulary growth), Taylor's exponent $\alpha$ (word-frequency
fluctuation scaling), compression rate $r$ (redundancy), and entropy. Our
corpora span three domains: legal documents (statutes, cases, deeds) as a
specialized domain, general natural language texts (literature, Wikipedia), and
AI-generated (GPT) text. We find that legal texts exhibit slower vocabulary
growth (lower $\beta$) and higher term consistency (higher $\alpha$) than
general texts. Within legal domain, statutory codes have the lowest $\beta$ and
highest $\alpha$, reflecting strict drafting conventions, while cases and deeds
show higher $\beta$ and lower $\alpha$. In contrast, GPT-generated text shows
the statistics more aligning with general language patterns. These results
demonstrate that legal texts exhibit domain-specific structures and
complexities, which current generative models do not fully replicate.

</details>


### [354] [Robustness of Neurosymbolic Reasoners on First-Order Logic Problems](https://arxiv.org/abs/2509.17377)
*Hannah Bansal,Kemal Kurniawan,Lea Frermann*

Main category: cs.CL

TL;DR: 本研究探讨了神经符号（NS）方法在提高大型语言模型（LLMs）对反事实任务变体的推理鲁棒性方面的潜力。结果显示，NS方法更具鲁棒性但整体性能较差，而NSCoT（NS结合思维链）虽有改进但仍落后于标准思维链。


<details>
  <summary>Details</summary>
Motivation: LLMs在反事实任务变体上表现脆弱，表明它们可能依赖表层模式而非真正的逻辑推理。研究旨在提高LLMs的推理能力，特别是泛化性和对任务变化的鲁棒性，以解决这一问题。

Method: 本研究探索了一种结合LLM和符号逻辑求解器的神经符号（NS）方法，以应对LLMs在反事实变体上的脆弱性。随后，提出了一种名为NSCoT的方法，该方法结合了NS方法和思维链（CoT）提示。实验在不同大小的LLMs上进行。

Result: 实验结果表明，神经符号（NS）方法比纯粹的神经网络方法更具鲁棒性，但整体性能较差。NSCoT方法虽然提高了性能，但仍然落后于标准的思维链（CoT）方法。

Conclusion: 神经符号方法在提高LLMs对反事实任务变体的鲁棒性方面显示出潜力，但其整体性能仍需改进以赶上或超越纯粹的神经网络方法。本研究的分析为未来的工作开辟了新的研究方向。

Abstract: Recent trends in NLP aim to improve reasoning capabilities in Large Language
Models (LLMs), with key focus on generalization and robustness to variations in
tasks. Counterfactual task variants introduce minimal but semantically
meaningful changes to otherwise valid first-order logic (FOL) problem instances
altering a single predicate or swapping roles of constants to probe whether a
reasoning system can maintain logical consistency under perturbation. Previous
studies showed that LLMs becomes brittle on counterfactual variations,
suggesting that they often rely on spurious surface patterns to generate
responses. In this work, we explore if a neurosymbolic (NS) approach that
integrates an LLM and a symbolic logical solver could mitigate this problem.
Experiments across LLMs of varying sizes show that NS methods are more robust
but perform worse overall that purely neural methods. We then propose NSCoT
that combines an NS method and Chain-of-Thought (CoT) prompting and demonstrate
that while it improves performance, NSCoT still lags behind standard CoT. Our
analysis opens research directions for future work.

</details>


### [355] [FinDebate: Multi-Agent Collaborative Intelligence for Financial Analysis](https://arxiv.org/abs/2509.17395)
*Tianshi Cai,Guanxu Li,Nijia Han,Ce Huang,Zimu Wang,Changyu Zeng,Yuqi Wang,Jingshi Zhou,Haiyang Zhang,Qi Chen,Yushan Pan,Shuihua Wang,Wei Wang*

Main category: cs.CL

TL;DR: FinDebate是一个多智能体金融分析框架，结合了领域特定RAG和安全辩论协议，旨在生成高质量、校准置信度的可操作投资策略。


<details>
  <summary>Details</summary>
Motivation: 研究动机是提高金融分析的质量、减轻过度自信，并提供可靠且可操作的多维度见解。

Method: 该方法引入了FinDebate框架，包含五个专业代理（盈利、市场、情绪、估值和风险），它们并行运行以综合证据。它利用领域特定检索增强生成（RAG）技术，并引入了一个安全辩论协议，使智能体能够挑战和完善初始结论，同时保持建议的连贯性。

Result: 实验结果（基于LLM和人工评估）表明，该框架能生成高质量的分析，具有校准的置信水平，并能在多个时间范围内提供可操作的投资策略。

Conclusion: FinDebate框架通过整合协作辩论和领域特定RAG，成功地提高了金融分析的质量和可靠性，并能提供具有校准置信度的可操作投资策略。

Abstract: We introduce FinDebate, a multi-agent framework for financial analysis,
integrating collaborative debate with domain-specific Retrieval-Augmented
Generation (RAG). Five specialized agents, covering earnings, market,
sentiment, valuation, and risk, run in parallel to synthesize evidence into
multi-dimensional insights. To mitigate overconfidence and improve reliability,
we introduce a safe debate protocol that enables agents to challenge and refine
initial conclusions while preserving coherent recommendations. Experimental
results, based on both LLM-based and human evaluations, demonstrate the
framework's efficacy in producing high-quality analysis with calibrated
confidence levels and actionable investment strategies across multiple time
horizons.

</details>


### [356] [EpiCache: Episodic KV Cache Management for Long Conversational Question Answering](https://arxiv.org/abs/2509.17396)
*Minsoo Kim,Arnav Kundu,Han-Byul Kim,Richa Dixit,Minsik Cho*

Main category: cs.CL

TL;DR: EpiCache是一种免训练的KV缓存管理框架，通过分块预填充、情景式KV压缩和自适应层级预算分配，在固定内存预算下显著提升了LLM在长对话问答中的准确性，并降低了延迟和内存消耗。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的长上下文能力依赖于KV缓存，其内存占用随对话长度线性增长，在资源受限时迅速成为瓶颈。现有KV缓存压缩方法存在局限性：(i) 全上下文预填充后导致峰值内存无上限；(ii) 依赖于查询的驱逐策略导致缓存仅限于单个查询，从而在多轮对话中降低准确性。

Method: 本文提出了EpiCache，一个免训练的KV缓存管理框架，用于在固定内存预算下处理长对话问答（LongConvQA）。EpiCache通过以下方法限制缓存增长并保留相关上下文：(i) **分块预填充**限制缓存增长；(ii) **情景式KV压缩**将对话历史聚类为连贯的情景，并应用针对特定情景的KV缓存驱逐策略，以保留与主题相关的上下文；(iii) 设计了**自适应层级预算分配策略**，根据每个层对驱逐的敏感性分配内存预算。

Result: 在三个LongConvQA基准测试中，EpiCache相比现有基线方法将准确性提高了高达40%，在4-6倍压缩下仍能保持接近完整的KV准确性，并将延迟和内存分别降低了高达2.4倍和3.5倍，从而在严格的资源限制下实现了高效的多轮交互。

Conclusion: EpiCache通过创新的KV缓存管理策略，有效解决了LLM在长对话中KV缓存内存消耗过大的问题，在严格资源约束下显著提高了性能，实现了高效、准确的多轮交互。

Abstract: Recent advances in large language models (LLMs) have extended context
lengths, enabling assistants to sustain long histories for coherent,
personalized responses. This ability, however, hinges on Key-Value (KV)
caching, whose memory grows linearly with dialogue length and quickly dominates
under strict resource constraints. An active line of research for reducing this
overhead is KV cache compression, which seeks to limit cache size while
preserving accuracy. Yet existing methods face two major limitations: (i)
evicting entries after full-context prefill causes unbounded peak memory, and
(ii) query-dependent eviction narrows the cache to a single query, leading to
degraded accuracy in multi-turn conversations. We introduce EpiCache, a
training-free KV cache management framework for long conversational question
answering (LongConvQA) under fixed memory budgets. EpiCache bounds cache growth
through block-wise prefill and preserves topic-relevant context via episodic KV
compression, which clusters conversation history into coherent episodes and
applies episode-specific KV cache eviction. We further design an adaptive
layer-wise budget allocation strategy that measures each layer's sensitivity to
eviction and distributes the memory budget across layers accordingly. Across
three LongConvQA benchmarks, EpiCache improves accuracy by up to 40% over
recent baselines, sustains near-full KV accuracy under 4-6x compression, and
reduces latency and memory by up to 2.4x and 3.5x, thereby enabling efficient
multi-turn interaction under strict resource constraints.

</details>


### [357] [DIWALI - Diversity and Inclusivity aWare cuLture specific Items for India: Dataset and Assessment of LLMs for Cultural Text Adaptation in Indian Context](https://arxiv.org/abs/2509.17399)
*Pramit Sahoo,Maharaj Brahma,Maunendra Sankar Desarkar*

Main category: cs.CL

TL;DR: 本文介绍了一个针对印度文化的全新文化特定项目（CSI）数据集，用于评估大型语言模型（LLMs）的文化能力，并通过多维度评估揭示了LLMs在文化适应方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）能力广泛，但它们在文化一致性方面存在不足，并产生有偏见的输出，原因在于缺乏文化知识和能力。评估LLMs的文化意识和一致性面临挑战，因为缺乏适当的评估指标和能代表复杂区域及次区域文化的接地气数据集。现有文化特定项目数据集主要关注区域层面概念，且可能包含假阳性。

Method: 研究引入了一个针对印度文化的全新CSI数据集，包含来自36个次区域的约8k个文化概念，涵盖17个文化层面。为衡量LLMs在文化文本改编任务上的文化能力，研究使用所创建的CSI、LLM作为评判者以及来自不同社会人口区域的人工评估来评估改编结果。此外，还进行了定量分析。

Result: 定量分析结果表明，所有被评估的LLMs都存在选择性的次区域覆盖，并且只进行了表面层次的文化适应。

Conclusion: 本研究通过提供一个详细的印度文化CSI数据集和多维度的评估方法，揭示了LLMs在文化能力方面的不足，特别是它们在处理次区域文化时的选择性和改编的肤浅性，为未来LLMs的文化对齐研究提供了基础和方向。

Abstract: Large language models (LLMs) are widely used in various tasks and
applications. However, despite their wide capabilities, they are shown to lack
cultural alignment \citep{ryan-etal-2024-unintended,
alkhamissi-etal-2024-investigating} and produce biased generations
\cite{naous-etal-2024-beer} due to a lack of cultural knowledge and competence.
Evaluation of LLMs for cultural awareness and alignment is particularly
challenging due to the lack of proper evaluation metrics and unavailability of
culturally grounded datasets representing the vast complexity of cultures at
the regional and sub-regional levels. Existing datasets for culture specific
items (CSIs) focus primarily on concepts at the regional level and may contain
false positives. To address this issue, we introduce a novel CSI dataset for
Indian culture, belonging to 17 cultural facets. The dataset comprises $\sim$8k
cultural concepts from 36 sub-regions. To measure the cultural competence of
LLMs on a cultural text adaptation task, we evaluate the adaptations using the
CSIs created, LLM as Judge, and human evaluations from diverse
socio-demographic region. Furthermore, we perform quantitative analysis
demonstrating selective sub-regional coverage and surface-level adaptations
across all considered LLMs. Our dataset is available here:
\href{https://huggingface.co/datasets/nlip/DIWALI}{https://huggingface.co/datasets/nlip/DIWALI},
project
webpage\footnote{\href{https://nlip-lab.github.io/nlip/publications/diwali/}{https://nlip-lab.github.io/nlip/publications/diwali/}},
and our codebase with model outputs can be found here:
\href{https://github.com/pramitsahoo/culture-evaluation}{https://github.com/pramitsahoo/culture-evaluation}.

</details>


### [358] [Vision Language Models Are Not (Yet) Spelling Correctors](https://arxiv.org/abs/2509.17418)
*Junhong Liang,Bojun Zhang*

Main category: cs.CL

TL;DR: ReViCo是首个系统评估视觉语言模型（VLM）在真实世界视觉拼写纠错（中英文）方面的基准，发现现有VLM表现远低于人类水平，并提出了两种改进方案。


<details>
  <summary>Details</summary>
Motivation: 视觉输入中的拼写纠错对VLM提出了独特挑战，因为它不仅需要检测图像中的文本错误，还需要直接进行纠正。目前缺乏系统评估VLM在该任务上的基准。

Method: 本文提出了ReViCo基准，包含从真实图像数据中收集的自然错误，支持图像和token级别的细粒度评估。通过对级联（Qwen）、原生（InternVL）开源模型和闭源系统（GPT-4o, Claude）进行综合实验，并探索了两种解决方案：联合OCR-纠错管道和背景信息增强方法。

Result: 实验结果表明，当前的VLM在视觉拼写纠错方面，尤其是在纠正能力上，显著低于人类表现。所提出的联合OCR-纠错管道和背景信息增强方法均能带来一致的性能提升。

Conclusion: 分析揭示了现有VLM架构在多模态拼写纠错方面的基本局限性，并为推动该领域的发展提供了可行的见解。

Abstract: Spelling correction from visual input poses unique challenges for vision
language models (VLMs), as it requires not only detecting but also correcting
textual errors directly within images. We present ReViCo (Real Visual
Correction), the first benchmark that systematically evaluates VLMs on
real-world visual spelling correction across Chinese and English. ReViCo
contains naturally occurring errors collected from real-world image data and
supports fine-grained evaluation at both image and token levels. Through
comprehensive experiments on representative cascaded (Qwen) and native
(InternVL) open-source models, as well as closed-source systems (GPT-4o,
Claude), we show that current VLMs fall significantly short of human
performance, particularly in correction. To address these limitations, we
explore two solution paradigms: a Joint OCR-Correction pipeline and a
Background Information enhanced approach, both of which yield consistent
performance gains. Our analysis highlights fundamental limitations of existing
architectures and provides actionable insights for advancing multimodal
spelling correction.

</details>


### [359] [RealBench: A Chinese Multi-image Understanding Benchmark Close to Real-world Scenarios](https://arxiv.org/abs/2509.17421)
*Fei Zhao,Chengqiang Lu,Yufan Shen,Qimeng Wang,Yicheng Qian,Haoxin Zhang,Yan Gao,Yi Wu,Yao Hu,Zhen Wu,Shangyu Xing,Xinyu Dai*

Main category: cs.CL

TL;DR: 本文介绍了RealBench，首个中文多模态多图像数据集，旨在评估多模态大语言模型在中文多图像场景下的理解能力，并揭示了现有模型的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的大多数多模态多图像评估数据集主要基于英文，缺乏专门针对中文的多图像数据集，这限制了对中文场景下多图像理解能力的研究和评估。

Method: 研究者构建了RealBench数据集，包含9393个样本和69910张图像，其特点是包含真实用户生成内容，涵盖多样化的场景、图像分辨率和结构。随后，使用21种不同规模的多模态大语言模型（包括闭源和开源模型）对RealBench进行了全面评估。

Result: 实验结果显示，即使是最强大的闭源模型在处理中文多图像场景时仍面临挑战。此外，开源视觉/视频模型与闭源模型之间平均存在约71.8%的显著性能差距。

Conclusion: RealBench数据集为进一步探索中文语境下的多图像理解能力提供了重要的研究基础，并揭示了当前多模态大语言模型在该领域的局限性，指明了未来的研究方向。

Abstract: While various multimodal multi-image evaluation datasets have been emerged,
but these datasets are primarily based on English, and there has yet to be a
Chinese multi-image dataset. To fill this gap, we introduce RealBench, the
first Chinese multimodal multi-image dataset, which contains 9393 samples and
69910 images. RealBench distinguishes itself by incorporating real
user-generated content, ensuring high relevance to real-world applications.
Additionally, the dataset covers a wide variety of scenes, image resolutions,
and image structures, further increasing the difficulty of multi-image
understanding. Ultimately, we conduct a comprehensive evaluation of RealBench
using 21 multimodal LLMs of different sizes, including closed-source models
that support multi-image inputs as well as open-source visual and video models.
The experimental results indicate that even the most powerful closed-source
models still face challenges when handling multi-image Chinese scenarios.
Moreover, there remains a noticeable performance gap of around 71.8\% on
average between open-source visual/video models and closed-source models. These
results show that RealBench provides an important research foundation for
further exploring multi-image understanding capabilities in the Chinese
context.

</details>


### [360] [Codifying Natural Langauge Tasks](https://arxiv.org/abs/2509.17455)
*Haoyang Chen,Kumiko Tanaka-Ishii*

Main category: cs.CL

TL;DR: 本文提出ICRAG框架，将自然语言问题（如法律判决、医疗问答）转化为可执行程序，通过迭代细化和外部知识实现，并在13个基准测试中取得显著改进。


<details>
  <summary>Details</summary>
Motivation: 探索文本到代码技术在解决通常以自然语言解决的现实世界问题（如法律判决和医疗问答）中的适用性，并利用程序生成提供的显式推理，而非以往工作。

Method: 提出ICRAG框架，通过利用领域资源和GitHub的外部知识进行迭代细化，将自然语言转化为可执行程序。

Result: 在13个基准测试中，ICRAG实现了高达161.1%的相对改进。论文还对生成的代码和外部知识的影响进行了详细分析。

Conclusion: ICRAG框架在将文本到代码方法应用于现实世界自然语言任务方面表现出色，但同时也讨论了其局限性。

Abstract: We explore the applicability of text-to-code to solve real-world problems
that are typically solved in natural language, such as legal judgment and
medical QA. Unlike previous works, our approach leverages the explicit
reasoning provided by program generation. We present ICRAG, a framework that
transforms natural language into executable programs through iterative
refinement using external knowledge from domain resources and GitHub. Across 13
benchmarks, ICRAG achieves up to 161.1\% relative improvement. We provide a
detailed analysis of the generated code and the impact of external knowledge,
and we discuss the limitations of applying text-to-code approaches to
real-world natural language tasks.

</details>


### [361] [QWHA: Quantization-Aware Walsh-Hadamard Adaptation for Parameter-Efficient Fine-Tuning on Large Language Models](https://arxiv.org/abs/2509.17428)
*Hyesung Jeon,Seojune Lee,Beomseok Kang,Yulhwa Kim,Jae-Joon Kim*

Main category: cs.CL

TL;DR: QWHA是一种将基于傅里叶变换的适配器与量化模型结合的方法，通过使用Walsh-Hadamard变换和新颖的初始化方案，有效降低了量化误差和计算成本，提高了低比特量化精度和训练速度。


<details>
  <summary>Details</summary>
Motivation: 为高效部署大型语言模型（LLMs），量化（降低推理成本）和参数高效微调（PEFT，降低训练开销）的需求日益增长。这促使了量化感知PEFT的发展，以生成准确高效的量化模型。在此背景下，微调前减少量化误差对实现高模型精度至关重要。然而，现有的依赖低秩自适应的方法表示能力有限；直接集成傅里叶变换（FT）相关适配器则常导致误差减少无效且计算开销增加。

Method: 本文提出了QWHA方法。它通过采用Walsh-Hadamard变换（WHT）作为变换核，将基于FT的适配器集成到量化模型中。同时，QWHA引入了一种新颖的适配器初始化方案，该方案结合了自适应参数选择和值优化。

Result: QWHA有效减轻了量化误差，促进了微调，并显著降低了计算成本。实验结果表明，QWHA在低比特量化精度方面始终优于基线方法，并且相比现有基于FT的适配器，实现了显著的训练加速。

Conclusion: QWHA通过创新性地将WHT适配器与自适应初始化结合，成功克服了现有量化感知PEFT方法的局限性，在保证模型精度的同时，显著提升了量化模型的效率和训练速度。

Abstract: The demand for efficient deployment of large language models (LLMs) has
driven interest in quantization, which reduces inference cost, and
parameter-efficient fine-tuning (PEFT), which lowers training overhead. This
motivated the development of quantization-aware PEFT to produce accurate yet
efficient quantized models. In this setting, reducing quantization error prior
to fine-tuning is crucial for achieving high model accuracy. However, existing
methods that rely on low-rank adaptation suffer from limited representational
capacity. Recent Fourier-related transform (FT)-based adapters offer greater
representational power than low-rank adapters, but their direct integration
into quantized models often results in ineffective error reduction and
increased computational overhead. To overcome these limitations, we propose
QWHA, a method that integrates FT-based adapters into quantized models by
employing the Walsh-Hadamard Transform (WHT) as the transform kernel, together
with a novel adapter initialization scheme incorporating adaptive parameter
selection and value refinement. We demonstrate that QWHA effectively mitigates
quantization errors while facilitating fine-tuning, and that its design
substantially reduces computational cost. Experimental results show that QWHA
consistently outperforms baselines in low-bit quantization accuracy and
achieves significant training speedups over existing FT-based adapters. The
code is available at https://github.com/vantaa89/qwha.

</details>


### [362] [MedFact: A Large-scale Chinese Dataset for Evidence-based Medical Fact-checking of LLM Responses](https://arxiv.org/abs/2509.17436)
*Tong Chen,Zimu Wang,Yiyi Miao,Haoran Luo,Yuanfei Sun,Wei Wang,Zhengyong Jiang,Procheta Sen,Jionglong Su*

Main category: cs.CL

TL;DR: MedFact是首个针对LLM生成医疗内容的中文循证医疗事实核查数据集，旨在填补现有数据集缺乏LLM内容验证的空白。


<details>
  <summary>Details</summary>
Motivation: 随着人们在线寻求医疗信息，医疗事实核查变得日益重要。然而，现有数据集主要关注人类生成的内容，对大型语言模型（LLM）生成内容的验证仍未被充分探索。

Method: 引入了MedFact数据集，包含1,321个问题和7,409个声明。通过上下文学习（ICL）和微调两种设置进行了全面的实验，并进行了深入的错误分析。

Result: 实验展示了当前LLM在该任务上的能力和挑战，并指出了未来研究的关键方向。

Conclusion: MedFact数据集填补了LLM生成医疗内容事实核查的空白，为未来的研究提供了基准和方向。

Abstract: Medical fact-checking has become increasingly critical as more individuals
seek medical information online. However, existing datasets predominantly focus
on human-generated content, leaving the verification of content generated by
large language models (LLMs) relatively unexplored. To address this gap, we
introduce MedFact, the first evidence-based Chinese medical fact-checking
dataset of LLM-generated medical content. It consists of 1,321 questions and
7,409 claims, mirroring the complexities of real-world medical scenarios. We
conduct comprehensive experiments in both in-context learning (ICL) and
fine-tuning settings, showcasing the capability and challenges of current LLMs
on this task, accompanied by an in-depth error analysis to point out key
directions for future research. Our dataset is publicly available at
https://github.com/AshleyChenNLP/MedFact.

</details>


### [363] [GeoPQA: Bridging the Visual Perception Gap in MLLMs for Geometric Reasoning](https://arxiv.org/abs/2509.17437)
*Guizhen Chen,Weiwen Xu,Hao Zhang,Hou Pong Chan,Deli Zhao,Anh Tuan Luu,Yu Rong*

Main category: cs.CL

TL;DR: 多模态大语言模型（MLLMs）在几何推理等视觉密集型任务中存在感知瓶颈，导致幻觉。本文提出一个两阶段强化学习（RL）训练框架，首先增强视觉感知，然后培养推理能力，显著提升了几何推理和问题解决能力。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习提升了大型语言模型（LLMs）的推理能力，但其对多模态大语言模型（MLLMs）的影响有限，特别是在几何推理等视觉密集型任务中，MLLMs常出现幻觉，导致推理不准确。作者认为这是由于MLLMs的感知瓶颈限制了推理训练的效果。

Method: 1. 设计了Geo-Perception Question-Answering (GeoPQA) 基准，用于量化MLLMs在基本几何概念和空间关系方面的视觉感知不足。2. 提出了一种两阶段强化学习训练框架：首先增强几何结构的视觉感知，然后培养推理能力。

Result: 1. GeoPQA上的实验揭示了MLLMs在视觉感知方面的显著缺陷，这限制了有效训练的RL奖励信号。2. 将该两阶段训练应用于Qwen2.5-VL-3B-Instruct，几何推理能力提高了9.7%，几何问题解决能力提高了9.1%，优于直接推理训练方法。3. 该方法还推广到其他视觉密集型领域，如图形理解。

Conclusion: 感知基础对于有效的多模态大语言模型推理至关重要。通过两阶段强化学习框架，首先解决感知瓶颈，然后培养推理能力，可以显著提高MLLMs在视觉密集型任务中的几何推理和问题解决性能。

Abstract: Recent advancements in reinforcement learning (RL) have enhanced the
reasoning abilities of large language models (LLMs), yet the impact on
multimodal LLMs (MLLMs) is limited. Particularly in vision-intensive tasks like
geometric reasoning, MLLMs hallucinate frequently, leading to inaccurate
reasoning. We attribute this to the perceptual bottleneck in MLLMs, which caps
the benefits of reasoning training. To quantify this, we design a
Geo-Perception Question-Answering (GeoPQA) benchmark, targeting basic geometric
concepts and spatial relationships. Experiments on GeoPQA reveal significant
shortcomings of MLLMs in visual perception, which constrain RL reward signals
for effective training. To address this bottleneck, we propose a two-stage RL
training framework by first enhancing the visual perception of geometric
structures, then fostering reasoning capabilities. Applied to
Qwen2.5-VL-3B-Instruct, our two-stage training improves geometric reasoning by
9.7% and geometric problem solving by 9.1%, compared to the direct reasoning
training approach. Our method also generalizes to other vision-intensive
domains like figure understanding, highlighting the importance of perceptual
grounding in effective MLLM reasoning.

</details>


### [364] [MapCoder-Lite: Squeezing Multi-Agent Coding into a Single Small LLM](https://arxiv.org/abs/2509.17489)
*Woongkyu Lee,Junhee Cho,Jungwook Choi*

Main category: cs.CL

TL;DR: MapCoder-Lite通过将一个7B模型升级为四个角色特化的智能体（检索、规划、编码、调试），并结合轻量级技术，在小模型上实现了高质量的多智能体代码生成，显著提升了性能并降低了资源消耗。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型多智能体代码生成方案要么依赖于昂贵的大规模模型（>30B），要么在缩小到小型开源模型时性能崩溃。

Method: MapCoder-Lite将一个7B模型通过使用rank-32、角色特定的LoRA适配器（<3%额外参数）升级为四个角色特化的智能体：检索器、规划器、编码器和调试器。实现此目标的关键技术包括：(i) 从强大LLM进行轨迹蒸馏以解决检索和调试中的格式脆弱性；(ii) 监督者引导修正以强化规划和编码智能体；(iii) 智能体级LoRA微调以实现内存高效的专业化。

Result: 在xCodeEval、APPS和CodeContests上的全面评估显示，MapCoder-Lite将xCodeEval准确率提升了一倍多（从13.2%到28.3%），消除了所有格式失败，并且性能接近32B基线模型六个百分点之内，同时将GPU内存和token生成时间减少了4倍。

Conclusion: 这些结果表明，细致的智能体级微调可以在小型语言模型上释放高质量的多智能体编码能力。

Abstract: Large language models (LLMs) have advanced code generation from
single-function tasks to competitive-programming problems, but existing
multi-agent solutions either rely on costly large-scale ($>$ 30B) models or
collapse when downsized to small open-source models. We present MapCoder-Lite,
which upgrades a single 7B model into four role-specialised agents-retriever,
planner, coder, and debugger-using only rank-32, role-specific LoRA adapters
($<3\%$ extra parameters). Three lightweight techniques make this possible: (i)
trajectory distillation from strong LLMs fixes format fragility in retrieval
and debugging, (ii) supervisor-guided correction strengthens planning and
coding agents, and (iii) agent-wise LoRA fine-tuning delivers memory-efficient
specialisation. Comprehensive evaluation on xCodeEval, APPS, and CodeContests
shows that MapCoder-Lite more than doubles xCodeEval accuracy (from $13.2\%$ to
$28.3\%$), eliminates all format failures, and closes to within six points of a
32B baseline while cutting GPU memory and token-generation time by $4\times$.
These results demonstrate that careful agent-wise fine-tuning unleashes
high-quality multi-agent coding on a small language model.

</details>


### [365] [Filling in the Clinical Gaps in Benchmark: Case for HealthBench for the Japanese medical system](https://arxiv.org/abs/2509.17444)
*Shohei Hisada,Endo Sunao,Himi Yamato,Shoko Wakamiya,Eiji Aramaki*

Main category: cs.CL

TL;DR: 本研究评估了大型医疗基准HealthBench在日语环境中的适用性，发现直接翻译存在局限性，并强调了开发本地化J-HealthBench的必要性。


<details>
  <summary>Details</summary>
Motivation: 日本医疗LLM的评估资源有限，常依赖翻译的多项选择题。为了安全开发医疗LLM，需要一个鲁棒的评估框架，以解决现有资源不足的问题。

Method: 首先，通过将HealthBench的5000个场景进行机器翻译，评估了多语言模型（GPT-4.1）和日本本土开源模型（LLM-jp-3.1）的性能，建立基线。其次，采用“LLM-as-a-Judge”方法系统分类基准场景和评分标准，以识别与日本临床指南、医疗系统或文化规范不符的“语境差距”。

Result: GPT-4.1因评分标准不匹配导致性能略有下降；日本本土模型因缺乏所需的临床完整性而表现不佳。分类结果显示，大多数场景是适用的，但很大一部分评分标准需要本地化。

Conclusion: 研究强调了直接基准翻译的局限性，并指出迫切需要开发一个情境感知、本地化的J-HealthBench，以确保日本医疗LLM的可靠和安全评估。

Abstract: This study investigates the applicability of HealthBench, a large-scale,
rubric-based medical benchmark, to the Japanese context. While robust
evaluation frameworks are crucial for the safe development of medical LLMs,
resources in Japanese remain limited, often relying on translated
multiple-choice questions. Our research addresses this gap by first
establishing a performance baseline, applying a machine-translated version of
HealthBench's 5,000 scenarios to evaluate both a high-performing multilingual
model (GPT-4.1) and a Japanese-native open-source model (LLM-jp-3.1). Second,
we employ an LLM-as-a-Judge approach to systematically classify the benchmark's
scenarios and rubric criteria, identifying "contextual gaps" where content is
misaligned with Japan's clinical guidelines, healthcare systems, or cultural
norms. Our findings reveal a modest performance drop in GPT-4.1 due to rubric
mismatches and a significant failure in the Japanese-native model, which lacked
the required clinical completeness. Furthermore, our classification indicates
that while the majority of scenarios are applicable, a substantial portion of
the rubric criteria requires localization. This work underscores the
limitations of direct benchmark translation and highlights the urgent need for
a context-aware, localized adaptation, a J-HealthBench, to ensure the reliable
and safe evaluation of medical LLMs in Japan.

</details>


### [366] [Semantic Reformulation Entropy for Robust Hallucination Detection in QA Tasks](https://arxiv.org/abs/2509.17445)
*Chaodong Tong,Qi Zhang,Lei Jiang,Yanbing Liu,Nannan Sun,Wei Li*

Main category: cs.CL

TL;DR: 该论文提出了一种名为语义重构熵（SRE）的新方法，通过输入端语义重构和渐进式混合聚类来更可靠地估计大型语言模型（LLM）的语义级不确定性，从而有效检测幻觉。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在问答中常因认知不确定性产生幻觉（流畅但不符事实的输出）。现有的基于熵的语义级不确定性估计方法存在采样噪声和可变长度答案聚类不稳定的问题。

Method: 该研究提出了语义重构熵（SRE）来改进不确定性估计。它通过两种方式实现：1) 输入端语义重构生成忠实释义，扩展估计空间并减少解码器倾向造成的偏差。2) 渐进的、基于能量的混合聚类稳定语义分组。

Result: 在SQuAD和TriviaQA数据集上的实验表明，SRE优于强大的基线方法，提供了更鲁棒和更具泛化性的幻觉检测能力。

Conclusion: 研究结果表明，结合输入多样化和多信号聚类可以显著增强语义级不确定性估计，从而有效解决LLM的幻觉问题。

Abstract: Reliable question answering with large language models (LLMs) is challenged
by hallucinations, fluent but factually incorrect outputs arising from
epistemic uncertainty. Existing entropy-based semantic-level uncertainty
estimation methods are limited by sampling noise and unstable clustering of
variable-length answers. We propose Semantic Reformulation Entropy (SRE), which
improves uncertainty estimation in two ways. First, input-side semantic
reformulations produce faithful paraphrases, expand the estimation space, and
reduce biases from superficial decoder tendencies. Second, progressive,
energy-based hybrid clustering stabilizes semantic grouping. Experiments on
SQuAD and TriviaQA show that SRE outperforms strong baselines, providing more
robust and generalizable hallucination detection. These results demonstrate
that combining input diversification with multi-signal clustering substantially
enhances semantic-level uncertainty estimation.

</details>


### [367] [CorefInst: Leveraging LLMs for Multilingual Coreference Resolution](https://arxiv.org/abs/2509.17505)
*Tuğba Pamay Arslan,Emircan Erol,Gülşen Eryiğit*

Main category: cs.CL

TL;DR: 本研究首次提出利用仅解码器的大型语言模型（LLMs）进行多语言共指消解（CR），通过指令调优和受控推理，超越了现有最先进的任务特定架构。


<details>
  <summary>Details</summary>
Motivation: 现有的共指消解方法受限于任务特定架构和基于编码器的语言模型，这些模型需要大量训练且缺乏适应性，促使研究者探索更通用、灵活的LLM方法。

Method: 研究采用仅解码器的LLMs处理显式和零指代，通过五种不同的指令集和受控推理方法来建模CR任务。评估了Llama 3.1、Gemma 2和Mistral 0.3三种LLMs。

Result: 结果表明，经过适当指令调优的LLMs能够超越最先进的任务特定架构。其中，完全微调的Llama 3.1模型在CorefUD v1.2数据集上，平均比领先的多语言CR模型（Corpipe 24单阶段变体）高出2个百分点。

Conclusion: 该研究得出结论，仅解码器的LLMs，在经过合适的指令调优后，在多语言共指消解任务上表现出色，甚至能够超越现有最先进的专用模型，展示了其巨大的潜力。

Abstract: Coreference Resolution (CR) is a crucial yet challenging task in natural
language understanding, often constrained by task-specific architectures and
encoder-based language models that demand extensive training and lack
adaptability. This study introduces the first multilingual CR methodology which
leverages decoder-only LLMs to handle both overt and zero mentions. The article
explores how to model the CR task for LLMs via five different instruction sets
using a controlled inference method. The approach is evaluated across three
LLMs; Llama 3.1, Gemma 2, and Mistral 0.3. The results indicate that LLMs, when
instruction-tuned with a suitable instruction set, can surpass state-of-the-art
task-specific architectures. Specifically, our best model, a fully fine-tuned
Llama 3.1 for multilingual CR, outperforms the leading multilingual CR model
(i.e., Corpipe 24 single stage variant) by 2 pp on average across all languages
in the CorefUD v1.2 dataset collection.

</details>


### [368] [SLAyiNG: Towards Queer Language Processing](https://arxiv.org/abs/2509.17449)
*Leonor Veloso,Lea Hirlimann,Philipp Wicke,Hinrich Schütze*

Main category: cs.CL

TL;DR: 本文介绍了SLAyiNG，首个包含标注酷儿俚语的数据集，旨在解决大型语言模型在处理酷儿俚语时可能出现的误解问题，并初步评估了人工和AI模型的标注一致性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）理解俚语对于用户互动至关重要，因为俚语常反映个体社会身份。酷儿俚语尤其容易被误判为仇恨言论或引发负面回应。现有研究缺乏对酷儿俚语的明确关注和高质量标注基准，导致检测和处理不足。

Method: 本文构建了SLAyiNG数据集，这是第一个包含标注酷儿俚语的数据集。数据来源于字幕、社交媒体帖子和播客，反映真实世界的使用情况。详细描述了数据整理过程，包括俚语词汇和定义的收集、示例的抓取以及正在进行的标注过程。作为初步结果，计算了人类标注者和OpenAI模型o3-mini在词义消歧任务上的标注者间一致性。

Result: SLAyiNG是首个针对酷儿俚语的标注数据集。初步结果显示，人类标注者和OpenAI模型o3-mini在词义消歧任务上的Krippendorff's alpha平均达到0.746。

Conclusion: 最先进的推理模型可以作为预过滤工具，但酷儿语言数据的复杂性和敏感性要求专家和社区驱动的标注工作。

Abstract: Knowledge of slang is a desirable feature of LLMs in the context of user
interaction, as slang often reflects an individual's social identity. Several
works on informal language processing have defined and curated benchmarks for
tasks such as detection and identification of slang. In this paper, we focus on
queer slang. Queer slang can be mistakenly flagged as hate speech or can evoke
negative responses from LLMs during user interaction. Research efforts so far
have not focused explicitly on queer slang. In particular, detection and
processing of queer slang have not been thoroughly evaluated due to the lack of
a high-quality annotated benchmark. To address this gap, we curate SLAyiNG, the
first dataset containing annotated queer slang derived from subtitles, social
media posts, and podcasts, reflecting real-world usage. We describe our data
curation process, including the collection of slang terms and definitions,
scraping sources for examples that reflect usage of these terms, and our
ongoing annotation process. As preliminary results, we calculate
inter-annotator agreement for human annotators and OpenAI's model o3-mini,
evaluating performance on the task of sense disambiguation. Reaching an average
Krippendorff's alpha of 0.746, we argue that state-of-the-art reasoning models
can serve as tools for pre-filtering, but the complex and often sensitive
nature of queer language data requires expert and community-driven annotation
efforts.

</details>


### [369] [Can LLMs Reason Over Non-Text Modalities in a Training-Free Manner? A Case Study with In-Context Representation Learning](https://arxiv.org/abs/2509.17552)
*Tianle Zhang,Wanlong Fang,Jonathan Woo,Paridhi Latawa,Deepak A. Subramanian,Alvin Chan*

Main category: cs.CL

TL;DR: 本文提出了一种名为上下文表示学习（ICRL）的训练无关框架，使大型语言模型（LLMs）能够以少量样本学习的方式，自适应地利用非文本基础模型（FMs）的表示进行多模态推理，从而实现更强的适应性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有将非文本模态表示整合到LLMs的方法通常需要额外的、昂贵的监督训练，这限制了其对新领域和新模态的即时适应性。

Method: 本文提出了上下文表示学习（ICRL），一种训练无关的方法。它通过将传统上下文学习中的文本输入替换为非文本基础模型（FM）的表示，使LLMs能够在无需微调的情况下执行多模态推理。ICRL旨在探索如何在训练无关的条件下将FM表示映射到LLMs中。

Result: ICRL在分子领域的一系列任务上进行了评估，并回答了三个核心研究问题：如何以训练无关的方式将FM表示映射到LLMs中；影响ICRL性能的因素；以及ICRL有效性的内在机制。研究结果表明，ICRL是首个将非文本模态表示整合到基于文本的LLMs中的训练无关框架。

Conclusion: ICRL为将非文本模态表示整合到基于文本的LLMs中提供了一个有前景的训练无关框架，为可适应的多模态泛化开辟了新的方向。

Abstract: The remarkable performance of Large Language Models (LLMs) can be enhanced
with test-time computation, which relies on external tools and even other deep
learning models. However, existing approaches for integrating non-text modality
representations into LLMs typically require additional costly supervised
training, restricting on-the-fly adaptation to new domains and modalities. In
this work, we explore the feasibility of integrating representations from
non-text foundational models (FMs) into text-based LLMs in a training-free
manner. We propose In-Context Representation Learning (ICRL) as a
proof-of-concept to allow LLMs to adaptively utilize non-text modality
representations with few-shot learning. Unlike traditional in-context learning,
which incorporates text-label pairs, ICRL replaces text inputs with FM
representations, enabling the LLM to perform multi-modal inference without
fine-tuning. We evaluate ICRL on a suite of tasks in the molecular domain,
investigating three core research questions: (i) how to map FM representations
into LLMs in a training-free manner, (ii) what factors influence ICRL
performance, and (iii) what mechanisms underlie the effectiveness of ICRL. To
the best of our knowledge, ICRL is the first training-free framework for
integrating non-text modality representations into text-based LLMs, presenting
a promising direction for adaptable, multi-modal generalization.

</details>


### [370] [PRINCIPLES: Synthetic Strategy Memory for Proactive Dialogue Agents](https://arxiv.org/abs/2509.17459)
*Namyoung Kim,Kai Tzu-iunn Ong,Yeonjun Hwang,Minseok Kang,Iiseo Jihn,Gayoung Kim,Minju Kim,Jinyoung Yeo*

Main category: cs.CL

TL;DR: 本文提出PRINCIPLES，一种通过离线自玩模拟生成的合成策略记忆，用于主动对话代理的策略规划，无需额外训练即可提升性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有主动对话策略规划方法存在策略覆盖有限、规划偏好偏差以及依赖昂贵额外训练的局限性。

Method: 提出PRINCIPLES，一个用于主动对话代理的合成策略记忆。该记忆通过离线自玩模拟生成，并作为可重用知识在推理时指导策略规划，从而无需额外训练和数据标注。

Result: 在情感支持和说服领域对PRINCIPLES进行评估，结果显示其对强基线有持续改进。此外，PRINCIPLES在更广泛和多样化的评估设置中也保持了鲁棒性。

Conclusion: PRINCIPLES通过提供一个高效、鲁棒且无需额外训练的策略规划机制，有效解决了现有主动对话代理在策略规划方面的局限性，显著提升了性能。

Abstract: Dialogue agents based on large language models (LLMs) have shown promising
performance in proactive dialogue, which requires effective strategy planning.
However, existing approaches to strategy planning for proactive dialogue face
several limitations: limited strategy coverage, preference bias in planning,
and reliance on costly additional training. To address these, we propose
PRINCIPLES: a synthetic strategy memory for proactive dialogue agents.
PRINCIPLES is derived through offline self-play simulations and serves as
reusable knowledge that guides strategy planning during inference, eliminating
the need for additional training and data annotation. We evaluate PRINCIPLES in
both emotional support and persuasion domains, demonstrating consistent
improvements over strong baselines. Furthermore, PRINCIPLES maintains its
robustness across extended and more diverse evaluation settings. See our
project page at https://huggingface.co/spaces/kimnamssya/Principles.

</details>


### [371] [Diagnosing Model Editing via Knowledge Spectrum](https://arxiv.org/abs/2509.17482)
*Tsung-Hsuan Pan,Chung-Chi Chen,Hen-Hsen Huang,Hsin-Hsi Chen*

Main category: cs.CL

TL;DR: 本文提出了一个“知识谱系”框架，根据知识的现实流行度、模型熟悉度和语言结构来分类知识，并基于此引入“知识诊断框架”，一种自适应的编辑策略，显著提高了模型编辑的成功率和资源效率。


<details>
  <summary>Details</summary>
Motivation: 现有模型编辑方法常引入意外副作用，且对目标知识固有属性的作用研究不足。研究旨在填补这一空白，理解并利用知识的内在特性来改进模型编辑。

Method: 1. 提出了“知识谱系”框架，根据现实流行度、模型编辑前熟悉度以及引发问题的语言结构来系统地分类知识。2. 通过实证分析揭示这些知识特性是编辑成功和稳定性的强预测因子。3. 基于这些发现，引入了“知识诊断框架”，这是一种自适应策略，根据诊断出的知识难度调整编辑强度。

Result: 1. 知识的特性（流行度、熟悉度、语言结构）是编辑成功和稳定性的强预测因子。2. “知识诊断框架”显著提高了挑战性编辑的成功率。3. 优化了计算资源的使用。

Conclusion: 本研究更全面地理解了影响模型编辑的因素，并通过提出的框架和策略改进了模型编辑的效率和效果。

Abstract: Model editing, the process of efficiently modifying factual knowledge in
pre-trained language models, is critical for maintaining their accuracy and
relevance. However, existing editing methods often introduce unintended side
effects, degrading model performance in unpredictable ways. While much research
has focused on improving editing algorithms, the role of the target knowledge's
intrinsic properties remains a significant, underexplored factor. This paper
addresses this gap by first proposing the ``Knowledge Spectrum,'' a systematic
framework for categorizing knowledge based on its real-world popularity, the
model's pre-edit familiarity, and the linguistic structure of the eliciting
question. Our empirical analysis reveals that these characteristics are strong
predictors of editing success and stability. Informed by these findings, we
introduce the ``Knowledge-Diagnostic Framework,'' an adaptive strategy that
tailors editing intensity to the diagnosed difficulty of a knowledge item. We
demonstrate that this framework significantly improves success rates for
challenging edits while optimizing computational resources. Our work provides a
more comprehensive understanding of the factors governing model editing.

</details>


### [372] [AttnComp: Attention-Guided Adaptive Context Compression for Retrieval-Augmented Generation](https://arxiv.org/abs/2509.17486)
*Lvzhou Luo,Yixuan Cao,Ping Luo*

Main category: cs.CL

TL;DR: AttnComp是一个自适应、高效且上下文感知的压缩框架，它利用大型语言模型（LLM）的注意力机制来过滤检索增强生成（RAG）中的无关内容，从而提高准确性并降低延迟。


<details>
  <summary>Details</summary>
Motivation: 检索增强生成（RAG）虽然能提高大型语言模型（LLM）的事实准确性，但常受无关检索内容影响。现有上下文压缩方法难以自适应调整压缩率、保持低延迟以及整合多文档信息。

Method: AttnComp利用LLM的注意力机制识别相关信息，并采用Top-P压缩算法，保留累积注意力权重超过预设阈值的最小文档集。此外，它通过评估检索内容的整体相关性来估计响应置信度。

Result: 实验表明，AttnComp优于现有压缩方法和未压缩基线，在实现更高的准确性、显著的压缩率和更低的延迟方面表现出色。

Conclusion: AttnComp提供了一个有效的解决方案，通过自适应和高效的上下文压缩，克服了现有方法的局限性，显著提升了RAG系统的性能和可靠性。

Abstract: Retrieval-augmented generation improves the factual accuracy of Large
Language Models (LLMs) by incorporating external context, but often suffers
from irrelevant retrieved content that hinders effectiveness. Context
compression addresses this issue by filtering out irrelevant information from
context before LLM generation. However, existing methods struggle to adaptively
adjust compression rates for different context, maintain low latency and
integrate information across multiple documents. To overcome these limitations,
We introduce AttnComp, an adaptive, efficient and context-aware compression
framework. By leveraging the attention mechanism of LLMs to identify relevant
information, AttnComp employs a Top-P compression algorithm to retain the
minimal set of documents whose cumulative attention weights exceeds a
predefined threshold. In addition to compression, AttnComp estimates response
confidence by assessing the overall relevance of the retrieved content,
enabling users to gauge response reliability. Experiments demonstrate that
AttnComp outperforms existing compression methods and uncompressed baselines,
achieving higher accuracy with substantial compression rates and lower latency.

</details>


### [373] [Enhancing Cross-Lingual Transfer through Reversible Transliteration: A Huffman-Based Approach for Low-Resource Languages](https://arxiv.org/abs/2509.17493)
*Wenhao Zhuang,Yuan Sun,Xiaobing Zhao*

Main category: cs.CL

TL;DR: 本文提出一个结合字符音译和霍夫曼编码的完整框架，旨在提升大型语言模型处理低资源、非拉丁语系语言的能力，同时实现数据压缩、无损转换、训练推理效率提升和良好可扩展性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在处理低资源语言，特别是使用非拉丁语系脚本的语言时，跨语言迁移能力不足。尽管将低资源语言音译成拉丁脚本是一种自然解决方案，但目前缺乏一个将音译集成到LLMs训练和部署中的综合框架。

Method: 该论文采用实用主义方法，将字符音译与霍夫曼编码相结合，设计了一个完整的音译框架。该框架具有压缩、准确性（100%无损转换）、效率（无需词汇扩展）和可扩展性等优点。

Result: 该框架将低资源语言内容的文件大小减少高达50%，token数量减少50-80%。它保证了从音译文本到源语言的100%无损转换。该方法无需为低资源语言扩展词汇，提高了训练和推理效率，并且可以扩展到其他低资源语言。实验结果表明，该方法显著增强了模型处理低资源语言的能力，同时保持了高资源语言的性能。

Conclusion: 所提出的结合字符音译和霍夫曼编码的框架，显著提升了大型语言模型处理低资源语言的能力，同时带来了数据压缩、无损转换和效率提升等优势，且不影响高资源语言的性能。

Abstract: As large language models (LLMs) are trained on increasingly diverse and
extensive multilingual corpora, they demonstrate cross-lingual transfer
capabilities. However, these capabilities often fail to effectively extend to
low-resource languages, particularly those utilizing non-Latin scripts. While
transliterating low-resource languages into Latin script presents a natural
solution, there currently lacks a comprehensive framework for integrating
transliteration into LLMs training and deployment. Taking a pragmatic approach,
this paper innovatively combines character transliteration with Huffman coding
to design a complete transliteration framework. Our proposed framework offers
the following advantages: 1) Compression: Reduces storage requirements for
low-resource language content, achieving up to 50% reduction in file size and
50-80% reduction in token count. 2) Accuracy: Guarantees 100% lossless
conversion from transliterated text back to the source language. 3) Efficiency:
Eliminates the need for vocabulary expansion for low-resource languages,
improving training and inference efficiency. 4) Scalability: The framework can
be extended to other low-resource languages. We validate the effectiveness of
our framework across multiple downstream tasks, including text classification,
machine reading comprehension, and machine translation. Experimental results
demonstrate that our method significantly enhances the model's capability to
process low-resource languages while maintaining performance on high-resource
languages. Our data and code are publicly available at
https://github.com/CMLI-NLP/HuffmanTranslit.

</details>


### [374] [MSCoRe: A Benchmark for Multi-Stage Collaborative Reasoning in LLM Agents](https://arxiv.org/abs/2509.17628)
*Yuzhen Lei,Hongbin Xie,Jiaxing Zhao,Shuangxue Liu,Xuan Song*

Main category: cs.CL

TL;DR: 该论文提出了MSCoRe，一个用于评估大型语言模型（LLMs）在复杂、多阶段场景中推理和协作能力的新基准，并对现有LLM代理进行了评估。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在单一领域的问答任务中表现出色，但它们在复杂、多阶段场景中的推理和协作能力尚未得到充分探索。现有基准通常侧重于孤立任务或狭窄领域，忽视了模型在没有明确外部指导下的多阶段协作和优化能力。

Method: 研究提出了MSCoRe基准，包含126696个跨汽车、制药、电子和能源领域的特定领域问答实例。数据集通过三阶段管道创建：动态采样、迭代问答生成和多级质量评估。任务根据阶段覆盖和复杂性分为三个难度级别。使用MSCoRe对各种最先进的LLM代理进行了全面评估。

Result: 商业模型在所有任务和场景中表现最佳，但在简单和复杂任务之间的ROUGE分数仍存在显著差距。模型对噪声数据的鲁棒性测试表明，其性能受到负面影响。

Conclusion: MSCoRe为社区提供了一个有价值的新资源，用于评估和改进LLM代理的多阶段推理能力。

Abstract: Large Language Models (LLMs) have excelled in question-answering (QA) tasks
within single domains. However, their reasoning and coordination capabilities
in complex, multi-stage scenarios remain underexplored. Existing benchmarks
typically focus on isolated tasks or narrow domains, overlooking models'
abilities for multi-stage collaboration and optimization without explicit
external guidance. To bridge this gap, we propose \textbf{MSCoRe}, a novel
benchmark comprising 126696 domain-specific QA instances spanning scenarios in
automotive, pharmaceutical, electronics, and energy sectors. The dataset is
created using a structured three-phase pipeline: dynamic sampling, iterative
question-answer generation, and a multi-level quality assessment to ensure data
quality. Tasks are further categorized into three difficulty levels according
to stage coverage and complexity. With MSCoRe, we have conducted a
comprehensive evaluation of various state-of-the-art LLM agents. The commercial
models performed best across all tasks and scenarios, but a notable gap in
ROUGE scores remains between simple and complex tasks. We also tested the
models' robustness and found that their performance is negatively affected by
noisy data. MSCoRe provides a valuable new resource for the community to
evaluate and improve multi-stage reasoning in LLM agents. The code and data are
available at https://github.com/D3E0-source/MSCoRE.

</details>


### [375] [Leveraging Audio-Visual Data to Reduce the Multilingual Gap in Self-Supervised Speech Models](https://arxiv.org/abs/2509.17523)
*María Andrea Cruz Blandón,Zakaria Aldeneh,Jie Chi,Maureen de Seyssel*

Main category: cs.CL

TL;DR: 本文研究通过引入有限的视觉基础来缩小多语言自监督学习（SSL）语音模型与单语言模型之间的性能差距，尤其是在双语环境中取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 单语言自监督学习（SSL）语音模型在语音识别等任务中表现出色，但多语言SSL模型，尤其是在双语等少数语言场景中，往往不如其单语言对应模型。研究旨在解决这一性能差距。

Method: 研究人员提出了一种新方法，将有限的视觉基础引入到双语语音自监督学习模型中。

Result: 结果表明，视觉基础对单语言和双语言模型都有益处，尤其对后者有显著提升。它将零样本语音判别任务上的多语言性能差距从纯音频模型的31.5%降低到引入视觉基础后的8.04%。

Conclusion: 引入视觉基础可以有效提升多语言（尤其是双语）自监督学习语音模型的性能，显著缩小与单语言模型之间的差距。

Abstract: Self-supervised learning (SSL) has made significant advances in speech
representation learning. Models like wav2vec 2.0 and HuBERT have achieved
state-of-the-art results in tasks such as speech recognition, particularly in
monolingual settings. However, multilingual SSL models tend to underperform
their monolingual counterparts on each individual language, especially in
multilingual scenarios with few languages such as the bilingual setting. In
this work, we investigate a novel approach to reduce this performance gap by
introducing limited visual grounding into bilingual speech SSL models. Our
results show that visual grounding benefits both monolingual and bilingual
models, with especially pronounced gains for the latter, reducing the
multilingual performance gap on zero-shot phonetic discrimination from 31.5%
for audio-only models to 8.04% with grounding.

</details>


### [376] [Specification-Aware Machine Translation and Evaluation for Purpose Alignment](https://arxiv.org/abs/2509.17559)
*Yoko Kayano,Saku Sugawara*

Main category: cs.CL

TL;DR: 本研究表明，将翻译规范整合到机器翻译工作流程中，尤其结合大型语言模型，能显著提升专业翻译质量，甚至超越传统人工翻译。


<details>
  <summary>Details</summary>
Motivation: 现有机器翻译（MT）研究中，翻译规范的重要性常被忽视或仅被隐式处理，而专业翻译实践中，这些规范（如沟通目标和客户需求）对翻译质量至关重要。

Method: 研究借鉴翻译学，为翻译规范在专业翻译中的重要性提供了理论依据，并提出了实现规范感知型机器翻译及评估的实践指南。实验选取了33家上市公司的投资者关系文本，比较了五种翻译类型（包括官方人工翻译和基于提示词的大型语言模型输出），并采用专家错误分析、用户偏好排名和自动化指标进行评估。

Result: 在人工评估中，由规范指导的大型语言模型翻译表现始终优于官方人工翻译，揭示了感知质量与预期质量之间的差距。

Conclusion: 将翻译规范整合到机器翻译工作流程中，并辅以人工监督，能够以符合专业实践的方式显著提升翻译质量。

Abstract: In professional settings, translation is guided by communicative goals and
client needs, often formalized as specifications. While existing evaluation
frameworks acknowledge the importance of such specifications, these
specifications are often treated only implicitly in machine translation (MT)
research. Drawing on translation studies, we provide a theoretical rationale
for why specifications matter in professional translation, as well as a
practical guide to implementing specification-aware MT and evaluation. Building
on this foundation, we apply our framework to the translation of investor
relations texts from 33 publicly listed companies. In our experiment, we
compare five translation types, including official human translations and
prompt-based outputs from large language models (LLMs), using expert error
analysis, user preference rankings, and an automatic metric. The results show
that LLM translations guided by specifications consistently outperformed
official human translations in human evaluations, highlighting a gap between
perceived and expected quality. These findings demonstrate that integrating
specifications into MT workflows, with human oversight, can improve translation
quality in ways aligned with professional practice.

</details>


### [377] [AuditoryBench++: Can Language Models Understand Auditory Knowledge without Hearing?](https://arxiv.org/abs/2509.17641)
*Hyunjong Ok,Suho Yoo,Hyeonjun Kim,Jaeho Lee*

Main category: cs.CL

TL;DR: 该研究提出了AuditoryBench++基准测试，用于评估文本模型中的听觉知识和推理能力，并引入了AIR-CoT方法，通过听觉想象推理来提升大型语言模型（LLMs）的听觉常识能力，实验证明其表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 人类无需直接听觉即可轻松进行听觉属性推理（如音高、响度），而语言模型通常缺乏这种能力，限制了它们在多模态交互中的有效性。本研究旨在弥补这一差距。

Method: 1. 提出了AuditoryBench++，一个全面的基准测试，用于在纯文本设置中评估听觉知识和推理。该基准包含从基本听觉比较到上下文推理的任务。 2. 引入了AIR-CoT（Auditory Imagination Reasoning CoT），一种新颖的听觉想象推理方法，通过使用特殊标记和知识注入的跨度检测，在推理过程中生成并整合听觉信息。

Result: 对近期LLMs和多模态LLMs进行的广泛实验表明，AIR-CoT普遍优于开箱即用模型以及那些通过听觉知识增强的模型。

Conclusion: AuditoryBench++和AIR-CoT为解决大型语言模型缺乏听觉常识推理能力提供了一个初步的解决方案，AIR-CoT在提升模型听觉推理能力方面表现出显著效果。

Abstract: Even without directly hearing sounds, humans can effortlessly reason about
auditory properties, such as pitch, loudness, or sound-source associations,
drawing on auditory commonsense. In contrast, language models often lack this
capability, limiting their effectiveness in multimodal interactions. As an
initial step to address this gap, we present AuditoryBench++, a comprehensive
benchmark for evaluating auditory knowledge and reasoning in text-only
settings. The benchmark encompasses tasks that range from basic auditory
comparisons to contextually grounded reasoning, enabling fine-grained analysis
of how models process and integrate auditory concepts. In addition, we
introduce AIR-CoT, a novel auditory imagination reasoning method that generates
and integrates auditory information during inference through span detection
with special tokens and knowledge injection. Extensive experiments with recent
LLMs and Multimodal LLMs demonstrate that AIR-CoT generally outperforms both
the off-the-shelf models and those augmented with auditory knowledge. The
project page is available at https://auditorybenchpp.github.io.

</details>


### [378] [Asking a Language Model for Diverse Responses](https://arxiv.org/abs/2509.17570)
*Sergey Troshin,Irina Saparina,Antske Fokkens,Vlad Niculae*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型中生成候选响应的采样器，对比了祖先采样、枚举采样和迭代采样，发现枚举和迭代采样在不牺牲质量的情况下能显著提高多样性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）越来越依赖显式推理链，并且能为给定上下文生成多个合理响应。研究的动机在于探索如何更有效地生成这些多样且高质量的候选响应集。

Method: 研究对比了三种候选采样策略：祖先（并行）采样、枚举采样（一次性生成n个候选）和迭代采样（顺序生成，并以前面生成的响应集为条件）。在预算相同的情况下，评估了这些采样器在质量、词汇和计算流多样性以及效率方面的表现。

Result: 实证结果表明，枚举和迭代策略在与质量相当的情况下，能产生更高的多样性。

Conclusion: 研究结果强调了简单的非独立采样策略在不牺牲生成质量的前提下，提高响应多样性的潜力。

Abstract: Large language models increasingly rely on explicit reasoning chains and can
produce multiple plausible responses for a given context. We study the
candidate sampler that produces the set of plausible responses contrasting the
ancestral (parallel) sampling against two alternatives: enumeration, which asks
the model to produce $n$ candidates in one pass, and iterative sampling, which
proposes candidates sequentially while conditioning on the currently generated
response set. Under matched budgets, we compare these samplers on quality,
lexical and computation flow diversity, and efficiency. Our empirical results
demonstrate that enumeration and iterative strategies result in higher
diversity at comparable quality. Our findings highlight the potential of simple
non-independent sampling strategies to improve response diversity without
sacrificing generation quality.

</details>


### [379] [Crosslingual Optimized Metric for Translation Assessment of Indian Languages](https://arxiv.org/abs/2509.17667)
*Arafat Ahsan,Vandan Mujadia,Pruthwik Mishra,Yash Bhaskar,Dipti Misra Sharma*

Main category: cs.CL

TL;DR: 由于语言复杂性及缺乏低资源语言数据，机器翻译的自动评估面临挑战。本文为13种印度语言创建了一个大型人工评估数据集，并在此基础上训练了一个名为COMTAIL的神经翻译评估指标，该指标在涉及印度语言的翻译对评估上显著优于现有SOTA。


<details>
  <summary>Details</summary>
Motivation: 机器翻译的自动评估因语言的正字法、形态、句法和语义丰富性及差异而极具挑战。现有的基于字符串的指标（如BLEU）和学习型神经指标均有局限性，特别是对于高资源语言之外的大多数语言，缺乏黄金标准评估数据。

Method: 本文创建了一个包含13种印度语言（涵盖21个翻译方向）的大型人工评估评分数据集。在此数据集上训练了一个名为COMTAIL（Cross-lingual Optimized Metric for Translation Assessment of Indian Languages）的神经翻译评估指标。此外，还进行了一系列消融研究，以探究该指标对领域、翻译质量和语言分组变化的敏感性。

Result: 最佳性能的COMTAIL指标变体在评估至少包含一种印度语言的翻译对时，相比先前的最先进方法展现出显著的性能提升。消融研究突出了该指标对领域、翻译质量和语言分组变化的敏感性。本文发布了COMTAIL数据集和伴随的指标模型。

Conclusion: 本文通过创建大型人工评估数据集并训练出高性能的神经翻译评估指标COMTAIL，有效解决了印度语言翻译评估中的部分空白。COMTAIL在涉及印度语言的翻译评估中表现出色，且其对不同因素的敏感性得到了深入分析。所有数据集和模型均已发布。

Abstract: Automatic evaluation of translation remains a challenging task owing to the
orthographic, morphological, syntactic and semantic richness and divergence
observed across languages. String-based metrics such as BLEU have previously
been extensively used for automatic evaluation tasks, but their limitations are
now increasingly recognized. Although learned neural metrics have helped
mitigate some of the limitations of string-based approaches, they remain
constrained by a paucity of gold evaluation data in most languages beyond the
usual high-resource pairs. In this present work we address some of these gaps.
We create a large human evaluation ratings dataset for 13 Indian languages
covering 21 translation directions and then train a neural translation
evaluation metric named Cross-lingual Optimized Metric for Translation
Assessment of Indian Languages (COMTAIL) on this dataset. The best performing
metric variants show significant performance gains over previous
state-of-the-art when adjudging translation pairs with at least one Indian
language. Furthermore, we conduct a series of ablation studies to highlight the
sensitivities of such a metric to changes in domain, translation quality, and
language groupings. We release both the COMTAIL dataset and the accompanying
metric models.

</details>


### [380] [Turk-LettuceDetect: A Hallucination Detection Models for Turkish RAG Applications](https://arxiv.org/abs/2509.17671)
*Selva Taş,Mahmut El Huseyni,Özay Ezerceli,Reyhan Bayraktar,Fatma Betül Terzioğlu*

Main category: cs.CL

TL;DR: 本文介绍了Turk-LettuceDetect，首个专门为土耳其语RAG应用设计的幻觉检测模型套件，通过微调编码器模型实现了对幻觉的有效检测，填补了多语言NLP的空白。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的广泛应用受限于其幻觉倾向，生成的事实错误信息。尽管检索增强生成（RAG）系统试图通过外部知识来解决此问题，但幻觉仍是挑战，尤其对于土耳其语等形态复杂、低资源语言。这促使研究人员开发专门的幻觉检测机制。

Method: 本文提出了Turk-LettuceDetect，将幻觉检测建模为令牌级分类任务。研究人员基于LettuceDetect框架，微调了三种不同的编码器架构：土耳其语特有的ModernBERT、TurkEmbed4STS和多语言EuroBERT。这些模型在一个机器翻译版本的RAGTruth基准数据集上进行训练，该数据集包含17,790个问答、数据到文本生成和摘要任务实例。

Result: 实验结果表明，基于ModernBERT的模型在完整测试集上取得了0.7266的F1分数，在结构化任务上表现尤为出色。这些模型保持了计算效率，并支持长达8,192个令牌的上下文，适用于实时部署。对比分析显示，最先进的LLM虽然召回率高，但由于幻觉内容的过度生成导致精度低，这突显了专用检测机制的必要性。

Conclusion: Turk-LettuceDetect通过发布模型和翻译数据集，解决了多语言NLP中的关键空白，并为开发针对土耳其语及其他语言的更可靠、更值得信赖的AI应用奠定了基础。

Abstract: The widespread adoption of Large Language Models (LLMs) has been hindered by
their tendency to hallucinate, generating plausible but factually incorrect
information. While Retrieval-Augmented Generation (RAG) systems attempt to
address this issue by grounding responses in external knowledge, hallucination
remains a persistent challenge, particularly for morphologically complex,
low-resource languages like Turkish. This paper introduces Turk-LettuceDetect,
the first suite of hallucination detection models specifically designed for
Turkish RAG applications. Building on the LettuceDetect framework, we formulate
hallucination detection as a token-level classification task and fine-tune
three distinct encoder architectures: a Turkish-specific ModernBERT,
TurkEmbed4STS, and multilingual EuroBERT. These models were trained on a
machine-translated version of the RAGTruth benchmark dataset containing 17,790
instances across question answering, data-to-text generation, and summarization
tasks. Our experimental results show that the ModernBERT-based model achieves
an F1-score of 0.7266 on the complete test set, with particularly strong
performance on structured tasks. The models maintain computational efficiency
while supporting long contexts up to 8,192 tokens, making them suitable for
real-time deployment. Comparative analysis reveals that while state-of-the-art
LLMs demonstrate high recall, they suffer from low precision due to
over-generation of hallucinated content, underscoring the necessity of
specialized detection mechanisms. By releasing our models and translated
dataset, this work addresses a critical gap in multilingual NLP and establishes
a foundation for developing more reliable and trustworthy AI applications for
Turkish and other languages.

</details>


### [381] [PG-CE: A Progressive Generation Dataset with Constraint Enhancement for Controllable Text Generation](https://arxiv.org/abs/2509.17669)
*Yan Zhuang,Yuan Sun*

Main category: cs.CL

TL;DR: 本文提出PG-CE方法，通过将可控文本生成任务分解为类型预测、约束构建和引导生成三个步骤，利用约束生成模型动态构建多维度约束，显著提高了大型语言模型生成文本的质量和可控性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的快速发展，可控文本生成（CTG）成为提升系统可靠性和用户体验的关键技术。传统方法存在局限性，促使研究者寻求更有效的CTG解决方案。

Method: PG-CE方法将可控文本生成任务分解为三个步骤：类型预测、约束构建和引导生成。该方法采用约束生成模型动态构建多维度约束，包括语气、表达风格和主题焦点，以指导文本输出。此外，研究还开发了一个包含90,000个约束-文本对的数据集。

Result: 实验证明，PG-CE方法在多种场景下显著提高了生成质量，同时保持了文本的可控性、主题相关性和响应实用性。

Conclusion: PG-CE方法通过其分解式和约束增强的策略，有效解决了可控文本生成任务的挑战，显著提升了大型语言模型的生成能力和实用性。

Abstract: With the rapid development of Large Language Models (LLMs), Controllable Text
Generation (CTG) has become a critical technology for enhancing system
reliability and user experience. Addressing the limitations of traditional
methods, this paper proposes the PG-CE (Progressive Generation with Constraint
Enhancement) approach, which decomposes CTG tasks into three steps: type
prediction, constraint construction, and guided generation. This method employs
constraint generation models to dynamically build multi-dimensional constraints
including tone, expression style, and thematic focus to guide output.
Experiments demonstrate that PG-CE significantly improves generation quality
across multiple scenarios while maintaining text controllability, thematic
relevance, and response practicality. The research developed a dataset
containing 90,000 constraint-text pairs (with an 8:2 ratio between daily and
other topics), effectively reflecting real-world application requirements.

</details>


### [382] [When TableQA Meets Noise: A Dual Denoising Framework for Complex Questions and Large-scale Tables](https://arxiv.org/abs/2509.17680)
*Shenghao Ye,Yu Guo,Dong Jin,Yikai Shen,Yunpeng Hou,Shuangwu Chen,Jian Yang,Xiaofeng Jiang*

Main category: cs.CL

TL;DR: 本文提出了EnoTab，一个双重去噪框架，用于处理复杂问题和大规模表格中的噪声数据，以提升大语言模型在表格问答（TableQA）任务中的推理性能。


<details>
  <summary>Details</summary>
Motivation: 随着实际应用中问题日益复杂和表格规模不断扩大，表格问答（TableQA）任务中引入了大量噪声数据，严重损害了大语言模型（LLMs）的推理性能。因此，需要提高相关性过滤和表格剪枝能力。

Method: 本文提出了EnoTab框架，包含两个核心去噪能力：1) 基于证据的问题去噪：将问题分解为最小语义单元，并根据一致性和可用性标准过滤掉与答案推理无关的单元。2) 证据树引导的表格去噪：构建一个显式且透明的表格剪枝路径，逐步移除不相关数据。在每个剪枝步骤中，观察表格的中间状态并应用后序节点回滚机制来处理异常表格状态，最终生成高度可靠的子表格进行答案推理。

Result: 广泛的实验表明，EnoTab在处理复杂问题和大规模表格的TableQA任务上取得了卓越的性能。

Conclusion: EnoTab框架通过其双重去噪机制，有效解决了复杂问题和大规模表格带来的噪声数据问题，显著提升了TableQA任务的性能。

Abstract: Table question answering (TableQA) is a fundamental task in natural language
processing (NLP). The strong reasoning capabilities of large language models
(LLMs) have brought significant advances in this field. However, as real-world
applications involve increasingly complex questions and larger tables,
substantial noisy data is introduced, which severely degrades reasoning
performance. To address this challenge, we focus on improving two core
capabilities: Relevance Filtering, which identifies and retains information
truly relevant to reasoning, and Table Pruning, which reduces table size while
preserving essential content. Based on these principles, we propose EnoTab, a
dual denoising framework for complex questions and large-scale tables.
Specifically, we first perform Evidence-based Question Denoising by decomposing
the question into minimal semantic units and filtering out those irrelevant to
answer reasoning based on consistency and usability criteria. Then, we propose
Evidence Tree-guided Table Denoising, which constructs an explicit and
transparent table pruning path to remove irrelevant data step by step. At each
pruning step, we observe the intermediate state of the table and apply a
post-order node rollback mechanism to handle abnormal table states, ultimately
producing a highly reliable sub-table for final answer reasoning. Finally,
extensive experiments show that EnoTab achieves outstanding performance on
TableQA tasks with complex questions and large-scale tables, confirming its
effectiveness.

</details>


### [383] [Evaluating LLM-Generated Versus Human-Authored Responses in Role-Play Dialogues](https://arxiv.org/abs/2509.17694)
*Dongxu Lu,Johan Jeuring,Albert Gatt*

Main category: cs.CL

TL;DR: 本研究通过人工评估和LLM-as-a-judge评估，发现大型语言模型（LLMs）在长篇、知识驱动的角色扮演对话中，其响应质量会随着轮次增加而显著下降，而人类响应则持续改进。同时，LLM-as-a-judge评估与人类判断高度一致，验证了LLM和人类响应之间日益扩大的质量差距。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在长篇、知识驱动的角色扮演对话中的表现仍然具有挑战性。

Method: 研究通过人工评估（N=38）和自动化LLM-as-a-judge评估（使用Gemini 2.0 Flash），比较了LLM生成和人类编写的多轮专业培训模拟对话响应。评估指标包括自然度、上下文维护和整体质量，并通过零样本配对偏好和6次随机构造评分来衡量。

Result: 人工评估显示，LLM生成的响应质量在多轮对话中显著下降，尤其是在自然度、上下文维护和整体质量方面，而人类编写的响应则逐渐改善。参与者也一致偏好人类编写的对话。LLM-as-a-judge评估（Gemini 2.0 Flash）与人工评估结果高度一致，证实了LLM和人类响应之间随时间推移而扩大的质量差距。

Conclusion: 本研究提供了一个多轮基准，揭示了LLM在知识驱动的角色扮演对话中存在的质量下降问题，并提出了一个经过验证的混合评估框架，以指导LLM在培训模拟中的可靠集成。

Abstract: Evaluating large language models (LLMs) in long-form, knowledge-grounded
role-play dialogues remains challenging. This study compares LLM-generated and
human-authored responses in multi-turn professional training simulations
through human evaluation ($N=38$) and automated LLM-as-a-judge assessment.
Human evaluation revealed significant degradation in LLM-generated response
quality across turns, particularly in naturalness, context maintenance and
overall quality, while human-authored responses progressively improved. In line
with this finding, participants also indicated a consistent preference for
human-authored dialogue. These human judgements were validated by our automated
LLM-as-a-judge evaluation, where Gemini 2.0 Flash achieved strong alignment
with human evaluators on both zero-shot pairwise preference and stochastic
6-shot construct ratings, confirming the widening quality gap between LLM and
human responses over time. Our work contributes a multi-turn benchmark exposing
LLM degradation in knowledge-grounded role-play dialogues and provides a
validated hybrid evaluation framework to guide the reliable integration of LLMs
in training simulations.

</details>


### [384] [TASO: Task-Aligned Sparse Optimization for Parameter-Efficient Model Adaptation](https://arxiv.org/abs/2509.17688)
*Daiye Miao,Yufang Liu,Jie Wang,Changzhi Sun,Yunke Zhang,Demei Yan,Shaokang Dong,Qi Zhang,Yuanbin Wu*

Main category: cs.CL

TL;DR: 本文提出TASO，一种LoRA冗余减少方法。它利用预训练模型权重的重要性信息，在微调前确定LoRA模块的稀疏结构，从而消除冗余。实验证明，TASO在参数预算与低秩LoRA相当的情况下，性能优于标准LoRA。


<details>
  <summary>Details</summary>
Motivation: LoRA虽然高效，但存在大量参数冗余，这不仅增加了可训练参数数量，还可能阻碍微调效果。识别并有效消除LoRA中的冗余参数是一个挑战性问题。

Method: TASO方法通过以下步骤减少LoRA冗余：1. 利用预训练模型权重的重要性信息。2. 在下游任务上估计参数重要性。3. 根据重要性分数分布识别任务特定的核心区域。4. 利用这些核心区域的位置信息，在微调之前确定LoRA模块的稀疏结构，从而实现冗余消除。

Result: 在参数预算与秩r=1的LoRA相当的情况下，TASO在多个任务上持续优于标准LoRA。它在有效消除冗余参数的同时，实现了强大的微调性能。

Conclusion: TASO提供了一种新颖的、任务对齐的LoRA冗余减少视角，显著减少了任务适应所需的训练参数。该方法在保持甚至超越标准LoRA性能的同时，有效解决了LoRA参数冗余问题。

Abstract: LoRA has become one of the most widely used parameter-efficient fine-tuning
methods due to its simplicity and effectiveness. However, numerous studies have
shown that LoRA often introduces substantial parameter redundancy, which not
only increases the number of trainable parameters but also hinders the
effectiveness of fine-tuning. Since identifying redundant parameters in LoRA is
inherently difficult, how to eliminate them efficiently and accurately remains
a challenging problem. In this paper, we propose TASO, a redundancy reduction
method that leverages importance information from the pretrained model's
weights to mitigate LoRA redundancy. Specifically, we estimate parameter
importance on downstream tasks and identify task-specific core regions based on
the distribution of importance scores. The location information of these core
regions is then used to determine the sparse structure of LoRA modules,
enabling redundancy removal before fine-tuning. Our approach significantly
reduces the number of trainable parameters required for task adaptation, while
providing a novel task-aligned perspective for LoRA redundancy reduction.
Experimental results demonstrate that, with a parameter budget comparable to
LoRA with rank $r = 1$, TASO consistently outperforms standard LoRA across
multiple tasks, achieving strong fine-tuning performance while effectively
eliminating redundant parameters.

</details>


### [385] [Investigating Bias: A Multilingual Pipeline for Generating, Solving, and Evaluating Math Problems with LLMs](https://arxiv.org/abs/2509.17701)
*Mariam Mahran,Katharina Simbeck*

Main category: cs.CL

TL;DR: 本研究开发了一个自动化多语言管道，用于生成、解决和评估与德国K-10课程相关的数学问题。结果显示，LLM在不同语言（英语、德语、阿拉伯语）下的解决方案质量存在显著差距，英语表现最佳，阿拉伯语表现最差，揭示了持续存在的语言偏见。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在教育支持中的应用日益广泛，但其响应质量因交互语言而异。研究旨在探究并量化这种语言差异，特别是教育内容中的语言偏见问题。

Method: 研究采用了一个自动化的多语言管道，用于生成、解决和评估数学问题。具体步骤包括：1. 生成628道与德国K-10课程对齐的数学练习。2. 将这些练习翻译成英语、德语和阿拉伯语。3. 使用三个商业LLM（GPT-4o-mini、Gemini 2.5 Flash和Qwen-plus）在每种语言中生成分步解决方案。4. 采用一个由LLM（包括Claude 3.5 Haiku）组成的独立评估小组，使用比较框架评估解决方案的质量。

Result: 评估结果显示，LLM在不同语言的解决方案质量上存在一致的差距。英语解决方案始终获得最高评分，而阿拉伯语解决方案的排名通常较低。这些发现突出了LLM中持续存在的语言偏见。

Conclusion: 本研究结果强调了LLM在多语言教育支持中存在的语言偏见问题，并表明需要开发更公平的多语言AI系统，以确保教育领域的公平性。

Abstract: Large Language Models (LLMs) are increasingly used for educational support,
yet their response quality varies depending on the language of interaction.
This paper presents an automated multilingual pipeline for generating, solving,
and evaluating math problems aligned with the German K-10 curriculum. We
generated 628 math exercises and translated them into English, German, and
Arabic. Three commercial LLMs (GPT-4o-mini, Gemini 2.5 Flash, and Qwen-plus)
were prompted to produce step-by-step solutions in each language. A held-out
panel of LLM judges, including Claude 3.5 Haiku, evaluated solution quality
using a comparative framework. Results show a consistent gap, with English
solutions consistently rated highest, and Arabic often ranked lower. These
findings highlight persistent linguistic bias and the need for more equitable
multilingual AI systems in education.

</details>


### [386] [Breaking Token Into Concepts: Exploring Extreme Compression in Token Representation Via Compositional Shared Semantics](https://arxiv.org/abs/2509.17737)
*Kavin R V,Pawan Goyal*

Main category: cs.CL

TL;DR: 该研究提出聚合语义分组（ASG）方法，利用乘积量化（PQ）实现词元组合式表示。ASG在大幅压缩嵌入参数（0.4-0.5%）的同时，在多种任务和模型上保持了超过95%的原始性能，验证了组合式表示的有效性。


<details>
  <summary>Details</summary>
Motivation: 标准语言模型使用单一、整体的词元嵌入，这可能限制了它们捕捉词义多面性的能力。研究旨在探索通过累积多样语义特征的组合结构，是否能更有效地表示词元。

Method: 提出聚合语义分组（ASG）方法，该方法利用乘积量化（PQ）。将ASG应用于标准Transformer架构（mBERT、XLM-R、mT5）以及BioBERT，并在NLI、NER、QA等多种任务和生物医学领域特定基准（BC5CDR）上进行评估。

Result: 通过ASG组合式表示词元，实现了嵌入参数的极致压缩（0.4-0.5%），同时相对于基础模型保持了超过95%的任务性能，即使在生成任务、跨语言迁移和领域特定设置中也表现良好。

Conclusion: 研究结果验证了词元可以被有效地建模为共享语义构建块的组合。ASG提供了一种简单而具体的方法来实现这一点，展示了组合式表示如何在实现模型紧凑性的同时捕捉语言的丰富性。

Abstract: Standard language models employ unique, monolithic embeddings for each token,
potentially limiting their ability to capture the multifaceted nature of word
meanings. We investigate whether tokens can be more effectively represented
through a compositional structure that accumulates diverse semantic facets. To
explore this, we propose Aggregate Semantic Grouping (ASG), a novel approach
leveraging Product Quantization (PQ). We apply ASG to standard transformer
architectures (mBERT, XLM-R, mT5) and evaluate this representational scheme
across diverse tasks (NLI, NER, QA), as well as a biomedical domain-specific
benchmark (BC5CDR) using BioBERT. Our findings demonstrate that representing
tokens compositionally via ASG achieves extreme compression in embedding
parameters (0.4--0.5\%) while maintaining $>$95\% task performance relative to
the base model, even in generative tasks and extends to both cross lingual
transfer and domain-specific settings. These results validate the principle
that tokens can be effectively modeled as combinations of shared semantic
building blocks. ASG offers a simple yet concrete method for achieving this,
showcasing how compositional representations can capture linguistic richness
while enabling compact yet semantically rich models.

</details>


### [387] [Qwen3-Omni Technical Report](https://arxiv.org/abs/2509.17765)
*Jin Xu,Zhifang Guo,Hangrui Hu,Yunfei Chu,Xiong Wang,Jinzheng He,Yuxuan Wang,Xian Shi,Ting He,Xinfa Zhu,Yuanjun Lv,Yongqi Wang,Dake Guo,He Wang,Linhan Ma,Pei Zhang,Xinyu Zhang,Hongkun Hao,Zishan Guo,Baosong Yang,Bin Zhang,Ziyang Ma,Xipin Wei,Shuai Bai,Keqin Chen,Xuejing Liu,Peng Wang,Mingkun Yang,Dayiheng Liu,Xingzhang Ren,Bo Zheng,Rui Men,Fan Zhou,Bowen Yu,Jianxin Yang,Le Yu,Jingren Zhou,Junyang Lin*

Main category: cs.CL

TL;DR: Qwen3-Omni是首个在文本、图像、音频和视频等多模态任务上均保持最先进性能，且不逊于单模态模型的统一模型。它在音频任务上表现卓越，并在多项基准测试中超越了现有模型，采用了Thinker-Talker MoE架构，并实现了低延迟实时语音和高质量音频字幕。


<details>
  <summary>Details</summary>
Motivation: 现有模型通常在多模态融合时会牺牲单模态性能，并且缺乏一个通用的、在文本、图像、音频和视频上都能达到SOTA水平的单一模型。此外，研究社区也缺少一个通用的音频字幕模型。

Method: Qwen3-Omni采用Thinker-Talker MoE（混合专家）架构，统一了文本、图像、音频和视频的感知与生成。在语音生成方面，它通过多码本方案自回归预测离散语音编解码器，并用轻量级因果卷积网络取代计算密集型块级扩散模型，实现从第一个编解码器帧开始的流式传输。为了增强多模态推理能力，引入了一个明确对任何模态输入进行推理的Thinking模型。此外，通过微调Qwen3-Omni-30B-A3B获得了Qwen3-Omni-30B-A3B-Captioner，用于音频字幕。

Result: Qwen3-Omni在文本、图像、音频和视频上均保持了最先进的性能，且相对于单模态模型没有性能下降。它在36个音频和音视频基准测试中，在32个上实现了开源SOTA，在22个上实现了整体SOTA，超越了Gemini-2.5-Pro、Seed-ASR和GPT-4o-Transcribe等闭源模型。它支持119种语言的文本交互、19种语言的语音理解和10种语言的语音生成。在冷启动设置下，理论端到端首包延迟为234毫秒。Qwen3-Omni-30B-A3B-Captioner能为任意音频输入生成详细、低幻觉的字幕。

Conclusion: Qwen3-Omni首次实现了在文本、图像、音频和视频上均达到SOTA性能的单一多模态模型，尤其在音频任务上表现出色。其创新的Thinker-Talker MoE架构和实时语音生成技术，以及针对音频字幕的专业版本，为多模态AI领域树立了新标杆。相关模型已根据Apache 2.0许可证公开。

Abstract: We present Qwen3-Omni, a single multimodal model that, for the first time,
maintains state-of-the-art performance across text, image, audio, and video
without any degradation relative to single-modal counterparts. Qwen3-Omni
matches the performance of same-sized single-modal models within the Qwen
series and excels particularly on audio tasks. Across 36 audio and audio-visual
benchmarks, Qwen3-Omni achieves open-source SOTA on 32 benchmarks and overall
SOTA on 22, outperforming strong closed-source models such as Gemini-2.5-Pro,
Seed-ASR, and GPT-4o-Transcribe. Qwen3-Omni adopts a Thinker-Talker MoE
architecture that unifies perception and generation across text, images, audio,
and video, yielding fluent text and natural real-time speech. It supports text
interaction in 119 languages, speech understanding in 19 languages, and speech
generation in 10 languages. To reduce first-packet latency in streaming
synthesis, Talker autoregressively predicts discrete speech codecs using a
multi-codebook scheme. Leveraging the representational capacity of these
codebooks, we replace computationally intensive block-wise diffusion with a
lightweight causal ConvNet, enabling streaming from the first codec frame. In
cold-start settings, Qwen3-Omni achieves a theoretical end-to-end first-packet
latency of 234 ms. To further strengthen multimodal reasoning, we introduce a
Thinking model that explicitly reasons over inputs from any modality. Since the
research community currently lacks a general-purpose audio captioning model, we
fine-tuned Qwen3-Omni-30B-A3B to obtain Qwen3-Omni-30B-A3B-Captioner, which
produces detailed, low-hallucination captions for arbitrary audio inputs.
Qwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking, and
Qwen3-Omni-30B-A3B-Captioner are publicly released under the Apache 2.0
license.

</details>


### [388] [A State-Update Prompting Strategy for Efficient and Robust Multi-turn Dialogue](https://arxiv.org/abs/2509.17766)
*Ziyi Liu*

Main category: cs.CL

TL;DR: 本文提出了一种名为“状态更新多轮对话策略”的免训练提示工程方法，通过“状态重建”和“历史提醒”机制有效管理对话历史，显著提升了大型语言模型在长程多轮对话中的性能，同时大幅降低了推理时间和token消耗。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在长程、多轮对话中存在信息遗忘和效率低下的问题。

Method: 提出了一种免训练的提示工程方法，即“状态更新多轮对话策略”。该策略利用“状态重建”（State Reconstruction）和“历史提醒”（History Remind）机制来有效管理对话历史。

Result: 该策略在多个多跳问答数据集上表现出色。例如，在HotpotQA数据集上，核心信息过滤分数提高了32.6%，下游问答分数增加了14.1%，同时推理时间减少了73.1%，token消耗减少了59.4%。消融研究证实了这两个组成部分的关键作用。

Conclusion: 该工作为优化大型语言模型在长程交互中的性能提供了一个有效的解决方案，并为开发更强大的Agent提供了新的见解。

Abstract: Large Language Models (LLMs) struggle with information forgetting and
inefficiency in long-horizon, multi-turn dialogues. To address this, we propose
a training-free prompt engineering method, the State-Update Multi-turn Dialogue
Strategy. It utilizes "State Reconstruction" and "History Remind" mechanisms to
effectively manage dialogue history. Our strategy shows strong performance
across multiple multi-hop QA datasets. For instance, on the HotpotQA dataset,
it improves the core information filtering score by 32.6%, leading to a 14.1%
increase in the downstream QA score, while also reducing inference time by
73.1% and token consumption by 59.4%. Ablation studies confirm the pivotal
roles of both components. Our work offers an effective solution for optimizing
LLMs in long-range interactions, providing new insights for developing more
robust Agents.

</details>


### [389] [DIVERS-Bench: Evaluating Language Identification Across Domain Shifts and Code-Switching](https://arxiv.org/abs/2509.17768)
*Jessica Ojo,Zina Kamel,David Ifeoluwa Adelani*

Main category: cs.CL

TL;DR: 现有语言识别（LID）模型在干净数据上过拟合，DIVERS-BENCH评估显示它们在多样化、嘈杂和语码转换数据上的性能显著下降，凸显了对更鲁棒LID系统的需求。


<details>
  <summary>Details</summary>
Motivation: 当前的语言识别（LID）系统往往过度拟合于干净、单语数据，导致在面对真实世界中多样化、嘈杂的输入时性能不佳。

Method: 引入了DIVERS-BENCH，这是一个全面的评估基准，用于衡量最先进的LID模型在多种领域（包括语音转录、网页文本、社交媒体文本、儿童故事和语码转换文本）的表现。同时，还引入了DIVERS-CS，这是一个涵盖10种语言对的语码转换基准数据集。

Result: 研究发现，LID模型在精心整理的数据集上能达到高精度，但在嘈杂和非正式输入上性能急剧下降。此外，现有模型难以在同一句子中检测到多种语言（语码转换）。

Conclusion: 这些结果强调了在实际应用中，需要开发更鲁棒和更具包容性的语言识别系统。

Abstract: Language Identification (LID) is a core task in multilingual NLP, yet current
systems often overfit to clean, monolingual data. This work introduces
DIVERS-BENCH, a comprehensive evaluation of state-of-the-art LID models across
diverse domains, including speech transcripts, web text, social media texts,
children's stories, and code-switched text. Our findings reveal that while
models achieve high accuracy on curated datasets, performance degrades sharply
on noisy and informal inputs. We also introduce DIVERS-CS, a diverse
code-switching benchmark dataset spanning 10 language pairs, and show that
existing models struggle to detect multiple languages within the same sentence.
These results highlight the need for more robust and inclusive LID systems in
real-world settings.

</details>


### [390] [One Agent to Serve All: a Lite-Adaptive Stylized AI Assistant for Millions of Multi-Style Official Accounts](https://arxiv.org/abs/2509.17788)
*Xingyu Fan,Feifei Li,Wenhui Que,Hailong Li*

Main category: cs.CL

TL;DR: 本文提出了WeStar框架，一个轻量级自适应框架，用于解决工业级官方账号平台中会话代理的风格化上下文问答问题，它结合了RAG和基于LoRA的参数化RAG，并通过SeDPO进行优化，实现了大规模部署的有效性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在工业级官方账号平台中面临挑战：链式思考（CoT）提示会导致多轮推理的显著延迟；为每个账号进行微调计算成本过高；基于长提示的方法会降低模型理解注入上下文和风格的能力。这些问题阻碍了会话代理生成既符合上下文又风格一致的回复。

Method: 本文提出了WeStar框架，结合了以下技术：1) 通过RAG实现上下文接地生成；2) 使用参数化RAG (PRAG) 实现风格感知生成，其中LoRA模块根据风格聚类动态激活；3) 引入多维度、基于聚类的参数共享方案，以紧凑表示风格并保持多样性；4) 开发了风格增强的直接偏好优化（SeDPO）方法来优化每个风格聚类的参数，以提高生成质量。

Result: 在大型工业数据集上的实验验证了WeStar框架的有效性和效率，突显了其在实际部署中的实用价值。

Conclusion: WeStar框架提供了一个统一、轻量级自适应的解决方案，能够以最小的开销服务大量官方账号，并能在大规模工业应用中有效且高效地生成风格化、上下文相关的回复。

Abstract: Conversational agents deployed in industrial-scale official account platforms
must generate responses that are both contextually grounded and stylistically
aligned-requirements that existing methods struggle to meet. Chain-of-thought
(CoT) prompting induces significant latency due to multi-turn reasoning;
per-account fine-tuning is computationally prohibitive; and long prompt-based
methods degrade the model's ability to grasp injected context and style. In
this paper, we propose WeStar, a lite-adaptive framework for stylized
contextual question answering that scales to millions of official accounts.
WeStar combines context-grounded generation via RAG with style-aware generation
using Parametric RAG (PRAG), where LoRA modules are dynamically activated per
style cluster. Our contributions are fourfold: (1) We introduce WeStar, a
unified framework capable of serving large volumes of official accounts with
minimal overhead. (2) We propose a multi-dimensional, cluster-based parameter
sharing scheme that enables compact style representation while preserving
stylistic diversity. (3) We develop a style-enhanced Direct Preference
Optimization (SeDPO) method to optimize each style cluster's parameters for
improved generation quality. (4) Experiments on a large-scale industrial
dataset validate the effectiveness and efficiency of WeStar, underscoring its
pracitical value in real-world deployment.

</details>


### [391] [Learning to vary: Teaching LMs to reproduce human linguistic variability in next-word prediction](https://arxiv.org/abs/2509.17794)
*Tobias Groot,Salo Lacunes,Evgenia Ilia*

Main category: cs.CL

TL;DR: 语言模型在复制人类语言变异性方面表现不佳。本文提出使用多标签微调（每个上下文有多个合理续接）来训练预训练和指令调优模型，结果显示这种方法能显著提高模型重现人类语言变异性的能力。


<details>
  <summary>Details</summary>
Motivation: 自然语言生成任务具有固有的变异性（例如，给定上下文有多种有效回应），但现有研究表明语言模型（LMs）无法很好地复制这种语言变异性。研究者推测这可能是因为LMs缺乏反映这种内在变异性的训练数据。

Method: 研究人员通过对预训练模型（GPT-2）和指令调优模型（Mistral-7B-IT）进行微调来解决这个问题。他们采用多标签训练方法，即每个上下文提供多个合理词语续接。训练数据使用了Provo语料库。评估方法是测量微调前后，人类和模型在不同上下文下预测的下一个词分布之间的差异。

Result: 评估结果表明，多标签微调显著提高了LMs在下一个词预测任务中重现人类语言变异性的能力，无论是在高变异性还是低变异性的上下文中都有效。

Conclusion: 通过使用每个上下文多个合理词语续接的数据对语言模型进行多标签微调，可以有效提升模型复制人类语言变异性的能力。

Abstract: Natural language generation (NLG) tasks are often subject to inherent
variability; \emph{e.g.} predicting the next word given a context has multiple
valid responses, evident when asking multiple humans to complete the task.
While having language models (LMs) that are aligned pluralistically, so that
they are able to reproduce well the inherent diversity in perspectives of an
entire population of interest is clearly beneficial, \citet{ilia2024predict}
show that LMs do not reproduce this type of linguistic variability well. They
speculate this inability might stem from the lack of consistent training of LMs
with data reflecting this type of inherent variability. As such, we investigate
whether training LMs on multiple plausible word continuations per context can
improve their ability to reproduce human linguistic variability for next-word
prediction. We employ fine-tuning techniques for pre-trained and
instruction-tuned models; and demonstrate their potential when fine-tuning
GPT-2 and Mistral-7B-IT, using Provo Corpus. Our evaluation, which measures
divergence among empirically estimated human and model next-word distributions
across contexts before and after fine-tuning, shows that our multi-label
fine-tuning improves the LMs' ability to reproduce linguistic variability; both
for contexts that admit higher and lower variability.

</details>


### [392] [Fine-Grained Detection of AI-Generated Text Using Sentence-Level Segmentation](https://arxiv.org/abs/2509.17830)
*Lekkala Sai Teja,Annepaka Yadagiri,and Partha Pakray,Chukhu Chunka,Mangadoddi Srikar Vardhan*

Main category: cs.CL

TL;DR: 本文提出了一种结合Transformer、神经网络和条件随机场（CRF）的句子级序列标注模型，用于在混合文本中以token级别粒度检测和分割人类编写与AI生成的文本，解决了传统文档级检测器的局限性。


<details>
  <summary>Details</summary>
Motivation: AI文本生成已普遍应用于重要作品中，可能导致滥用。传统的AI检测器依赖文档级分类，难以识别混合或轻微编辑的AI内容，效率低下，难以区分人类和AI文本。

Method: 该研究提出一个句子级序列标注模型，旨在检测人类和AI生成文本之间的转换。模型结合了先进的预训练Transformer模型、神经网络（NN）和条件随机场（CRF），利用Transformer提取语义和句法模式，NN捕获增强的序列级表示，并通过CRF层改进边界预测，从而实现token级别的AI和人类文本分割。

Result: 模型在两个包含人类与AI协作文本的公开基准数据集上进行了评估。实验结果表明，与零样本检测器和现有最先进模型相比，该方法能准确检测协作文本中AI文本的跨度，并通过严格的消融研究验证了其有效性。

Conclusion: 该方法能够有效且准确地检测完全协作文本中AI文本的范围，解决了传统AI检测器在混合文本识别方面的不足，提升了AI文本检测的细粒度识别能力。

Abstract: Generation of Artificial Intelligence (AI) texts in important works has
become a common practice that can be used to misuse and abuse AI at various
levels. Traditional AI detectors often rely on document-level classification,
which struggles to identify AI content in hybrid or slightly edited texts
designed to avoid detection, leading to concerns about the model's efficiency,
which makes it hard to distinguish between human-written and AI-generated
texts. A sentence-level sequence labeling model proposed to detect transitions
between human- and AI-generated text, leveraging nuanced linguistic signals
overlooked by document-level classifiers. By this method, detecting and
segmenting AI and human-written text within a single document at the
token-level granularity is achieved. Our model combines the state-of-the-art
pre-trained Transformer models, incorporating Neural Networks (NN) and
Conditional Random Fields (CRFs). This approach extends the power of
transformers to extract semantic and syntactic patterns, and the neural network
component to capture enhanced sequence-level representations, thereby improving
the boundary predictions by the CRF layer, which enhances sequence recognition
and further identification of the partition between Human- and AI-generated
texts. The evaluation is performed on two publicly available benchmark datasets
containing collaborative human and AI-generated texts. Our experimental
comparisons are with zero-shot detectors and the existing state-of-the-art
models, along with rigorous ablation studies to justify that this approach, in
particular, can accurately detect the spans of AI texts in a completely
collaborative text. All our source code and the processed datasets are
available in our GitHub repository.

</details>


### [393] [Findings of the Fourth Shared Task on Multilingual Coreference Resolution: Can LLMs Dethrone Traditional Approaches?](https://arxiv.org/abs/2509.17796)
*Michal Novák,Miloslav Konopík,Anna Nedoluzhko,Martin Popel,Ondřej Pražák,Jakub Sido,Milan Straka,Zdeněk Žabokrtský,Daniel Zeman*

Main category: cs.CL

TL;DR: 本文概述了CODI-CRAC 2025多语言共指消解共享任务的第四届，引入了LLM专用赛道和新的数据集，传统系统仍领先，但LLM展现出巨大潜力。


<details>
  <summary>Details</summary>
Motivation: 组织者旨在挑战参与者开发识别提及并进行身份共指聚类的系统，特别是探索大型语言模型（LLM）在多语言共指消解任务中的能力和潜力。

Method: 共享任务要求系统识别提及并进行身份共指聚类。本届引入了LLM专用赛道，采用简化的纯文本格式，与CoNLL-U格式并行。任务还新增了3个数据集，覆盖两种新语言，所有数据集均基于CorefUD 1.3版本（包含17种语言的22个数据集）。

Result: 共有9个系统参与，其中4个是基于LLM的方法（2个经过微调，2个采用少样本适应）。尽管传统系统仍然保持领先地位，但LLM已展现出明显的潜力。

Conclusion: LLM在多语言共指消解方面表现出清晰的潜力，预示着它们可能在未来的任务中挑战甚至超越现有成熟方法。

Abstract: The paper presents an overview of the fourth edition of the Shared Task on
Multilingual Coreference Resolution, organized as part of the CODI-CRAC 2025
workshop. As in the previous editions, participants were challenged to develop
systems that identify mentions and cluster them according to identity
coreference.
  A key innovation of this year's task was the introduction of a dedicated
Large Language Model (LLM) track, featuring a simplified plaintext format
designed to be more suitable for LLMs than the original CoNLL-U representation.
  The task also expanded its coverage with three new datasets in two additional
languages, using version 1.3 of CorefUD - a harmonized multilingual collection
of 22 datasets in 17 languages.
  In total, nine systems participated, including four LLM-based approaches (two
fine-tuned and two using few-shot adaptation). While traditional systems still
kept the lead, LLMs showed clear potential, suggesting they may soon challenge
established approaches in future editions.

</details>


### [394] [How Persuasive is Your Context?](https://arxiv.org/abs/2509.17879)
*Tu Nguyen,Kevin Du,Alexander Miserlis Hoyle,Ryan Cotterell*

Main category: cs.CL

TL;DR: 本文引入了目标说服分数（TPS），用以量化上下文对语言模型的说服力，即上下文改变模型答案分布的能力，并证明其比现有指标更能捕捉细微的说服力。


<details>
  <summary>Details</summary>
Motivation: 语言模型具备利用先验知识和适应上下文新信息的能力。研究者希望量化上下文改变语言模型答案的有效性，而不仅仅是观察贪婪解码的答案，需要一个更精细的度量标准。

Method: 引入目标说服分数（TPS）。将说服力定义为上下文改变语言模型对问题的答案的能力。TPS基于Wasserstein距离，衡量上下文将模型原始答案分布向目标分布转移的程度，从而提供比仅检查贪婪解码答案更细致的模型行为视图。

Result: 通过一系列实验，经验性地证明TPS比先前提出的指标更能捕捉到说服力的细微差别。

Conclusion: TPS提供了一种更精细、更细致的方法来量化上下文对语言模型的说服力，通过分析答案分布的 shifts 来衡量上下文改变模型行为的能力。

Abstract: Two central capabilities of language models (LMs) are: (i) drawing on prior
knowledge about entities, which allows them to answer queries such as "What's
the official language of Austria?", and (ii) adapting to new information
provided in context, e.g., "Pretend the official language of Austria is
Tagalog.", that is pre-pended to the question. In this article, we introduce
targeted persuasion score (TPS), designed to quantify how persuasive a given
context is to an LM where persuasion is operationalized as the ability of the
context to alter the LM's answer to the question. In contrast to evaluating
persuasiveness only by inspecting the greedily decoded answer under the model,
TPS provides a more fine-grained view of model behavior. Based on the
Wasserstein distance, TPS measures how much a context shifts a model's original
answer distribution toward a target distribution. Empirically, through a series
of experiments, we show that TPS captures a more nuanced notion of
persuasiveness than previously proposed metrics.

</details>


### [395] [Everyday Physics in Korean Contexts: A Culturally Grounded Physical Reasoning Benchmark](https://arxiv.org/abs/2509.17807)
*Jihae Jeong,DaeYeop Lee,DongGeon Lee,Hwanjo Yu*

Main category: cs.CL

TL;DR: 该论文引入了EPiK，一个针对韩国文化背景的物理常识推理基准，以解决现有基准缺乏文化多样性的问题，并证明了文化感知基准的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有的物理常识推理基准主要关注西方语境，忽视了物理问题解决中的文化差异。

Method: EPiK是一个包含181个二元选择问题的基准，测试韩国文化背景（如泡菜、传统发酵）下的物理推理。它采用两阶段生成和验证流程，从韩国语境中原创性地生成问题，而非简单翻译，涵盖9个推理子任务和84个场景。

Result: 评估显示，韩国专用模型在EPiK上持续优于同等规模的通用模型，突显了文化无关模型的局限性。

Conclusion: 研究表明，迫切需要文化感知的基准来真正衡量语言理解能力，EPiK为此提供了一个重要工具。

Abstract: Existing physical commonsense reasoning benchmarks predominantly focus on
Western contexts, overlooking cultural variations in physical problem-solving.
To address this gap, we introduce EPiK (Everyday Physics in Korean Contexts), a
novel benchmark comprising 181 binary-choice problems that test physical
reasoning within Korean cultural contexts, ranging from kimchi (Korean food) to
traditional fermentation. EPiK is constructed using a two-stage generation and
verification pipeline to create culturally-authentic problems across 9
reasoning subtasks and 84 scenarios. Unlike approaches based on simple
translation, our method generates problems organically from Korean contexts
while upholding rigorous physical reasoning standards. Our evaluations show
that Korean-specialized models consistently outperform general-purpose models
of comparable size. This performance gap highlights the limitations of
culturally-agnostic models and demonstrates the critical need for
culturally-aware benchmarks to truly measure language understanding. Our EPiK
is publicly available at https://huggingface.co/datasets/jjae/EPiK.

</details>


### [396] [Towards Adaptive Context Management for Intelligent Conversational Question Answering](https://arxiv.org/abs/2509.17829)
*Manoj Madushanka Perera,Adnan Mahmood,Kasun Eranda Wijethilake,Quan Z. Sheng*

Main category: cs.CL

TL;DR: 本文提出了一种自适应上下文管理（ACM）框架，用于会话式问答（ConvQA）系统，旨在动态管理对话历史，以在模型令牌限制内最大化提供相关信息。


<details>
  <summary>Details</summary>
Motivation: ConvQA系统需要有效地利用对话历史，并在模型令牌限制内最大化相关信息，以生成准确和上下文适当的响应。

Method: ACM框架包含三个模块：上下文管理器（CM）模块动态调整上下文大小，保留最相关和最新的信息；摘要（SM）模块通过滑动窗口总结对话历史的较旧部分；实体提取（EE）模块在摘要窗口超出限制时识别并保留最旧对话轮次中的关键实体。

Result: 实验结果表明，该框架能有效地生成准确且上下文适当的响应。

Conclusion: ACM框架有望增强ConvQA系统的鲁棒性和可扩展性。

Abstract: This particular paper introduces an Adaptive Context Management (ACM)
framework for the Conversational Question Answering (ConvQA) systems. The key
objective of the ACM framework is to optimize the use of the conversation
history by dynamically managing context for maximizing the relevant information
provided to a ConvQA model within its token limit. Our approach incorporates a
Context Manager (CM) Module, a Summarization (SM) Module, and an Entity
Extraction (EE) Module in a bid to handle the conversation history
efficaciously. The CM Module dynamically adjusts the context size, thereby
preserving the most relevant and recent information within a model's token
limit. The SM Module summarizes the older parts of the conversation history via
a sliding window. When the summarization window exceeds its limit, the EE
Module identifies and retains key entities from the oldest conversation turns.
Experimental results demonstrate the effectiveness of our envisaged framework
in generating accurate and contextually appropriate responses, thereby
highlighting the potential of the ACM framework to enhance the robustness and
scalability of the ConvQA systems.

</details>


### [397] [Transformer-Encoder Trees for Efficient Multilingual Machine Translation and Speech Translation](https://arxiv.org/abs/2509.17930)
*Yiwen Guan,Jacob Whitehill*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的分层Transformer编码器树（TET）与非自回归编码器模型相结合的方法，用于多语言翻译，尤其在语音翻译中，旨在提高低资源语言的准确性、减少计算冗余并显著提升速度。


<details>
  <summary>Details</summary>
Motivation: 多语言翻译面临计算冗余和低资源语言准确性有限的挑战，尤其在语音翻译中更为突出。

Method: 提出了一种分层Transformer编码器树（TET），结合使用连接主义时间分类（CTC）训练的非自回归编码器模型。TET通过在语言相似的目标语言之间共享中间表示来工作。对于语音翻译，将TET与非自回归语音识别骨干（wav2vec2）结合使用。

Result: TET提高了低资源语言的准确性，减少了计算冗余，并允许在单次前向传播中生成所有目标语言，从而消除了顺序瓶颈并提高了并行性。在语音翻译中，与自回归系统相比，它在翻译质量方面显示出有希望的结果，同时速度快7-14倍。

Conclusion: 所提出的TET结合非自回归编码器模型，有效解决了多语言翻译中的挑战，尤其在语音翻译中，实现了低资源语言的准确性提升、计算效率的提高以及显著的速度优势。

Abstract: Multilingual translation faces challenges of computational redundancy and
limited accuracy for low-resource languages, especially in speech translation.
To address this, we propose a novel hierarchical Transformer Encoder Tree (TET)
combined with non-autoregressive encoder-only models trained with Connectionist
Temporal Classification for multilingual translation. By sharing intermediate
representations among linguistically similar target languages, TET can improve
accuracy on low-resource languages, reduce computational redundancy, and allow
generating all target languages in a single forward pass, thus eliminating
sequential bottlenecks and improving parallelism. For speech translation,
combining TET with a non-autoregressive speech recognition backbone (wav2vec2)
shows promising results in terms of translation quality compared to
autoregressive systems while being 7-14 times faster.

</details>


### [398] [Trust Me, I Can Convince You: The Contextualized Argument Appraisal Framework](https://arxiv.org/abs/2509.17844)
*Lynn Greschner,Sabine Weber,Roman Klinger*

Main category: cs.CL

TL;DR: 该研究提出了一个情境化论证评估框架，探究发送者、接收者、论证与认知评估过程（包括情绪和说服力）之间的相互作用，并通过一项角色扮演研究揭示了情绪、论证熟悉度和论证内容对说服力的影响。


<details>
  <summary>Details</summary>
Motivation: 论证挖掘中对二元情绪的研究与一般情绪分析中对认知评估的建模尚未结合起来，以理解情绪在发送者和接收者情境下如何影响论证的说服力，因此需要一个整合这些领域的框架。

Method: 提出了“情境化论证评估框架”（Contextualized Argument Appraisal Framework），该框架将发送者、接收者和论证的相互作用情境化，并包含情绪标签、论证评估（如论证熟悉度、响应紧迫性、预期努力）和说服力变量。通过一项模拟真实世界论证情境的角色扮演研究进行评估，参与者需披露情绪、解释主要原因、进行论证评估并报告感知到的说服力。同时收集了参与者和感知发送者的人口统计数据及人格特质。构建了一个包含800个论证的语料库，每个论证由5名参与者标注。

Result: 研究分析结果显示，说服力与积极情绪（如信任）呈正相关，与消极情绪（如愤怒）呈负相关。评估变量揭示了论证熟悉度的重要性。对于大多数参与者而言，论证内容本身是情绪反应的主要驱动因素。

Conclusion: 该研究通过提出的情境化论证评估框架和收集的语料库，揭示了情绪、认知评估（尤其是论证熟悉度）和论证内容在决定说服力方面的重要性，为未来的计算建模奠定了基础。

Abstract: Emotions, which influence how convincing an argument is, are developed
  in context of the self and sender, and therefore require modeling
  the cognitive evaluation process. While binary emotionality has been
  studied in argument mining, and the cognitive appraisal has been
  modeled in general emotion analysis, these fields have not been
  brought together yet. We therefore propose the Contextualized
  Argument Appraisal Framework that contextualizes the interplay
  between the sender, receiver, and argument. It includes emotion
  labels, appraisals, such as argument familiarity, response urgency,
  and expected effort, as well as convincingness variables. To evaluate
  the framework and pave the way to computational modeling, we perform
  a study in a role-playing scenario, mimicking real-world exposure to
  arguments, asking participants to disclose their emotion, explain the main
cause, the
  argument appraisal, and the
  perceived convincingness. To consider the subjective nature of such
  annotations, we also collect demographic data and personality traits
  of both the participants and the perceived sender of the argument.
  The analysis of the resulting corpus of 800 arguments, each
  annotated by 5 participants, reveals that convincingness is
  positively correlated with positive emotions (e.g., trust) and
  negatively correlated with negative emotions (e.g., anger). The
  appraisal variables disclose the importance of the argument
  familiarity. For most participants, the content of the argument
  itself is the primary driver of the emotional response.

</details>


### [399] [HICode: Hierarchical Inductive Coding with LLMs](https://arxiv.org/abs/2509.17946)
*Mian Zhong,Pristina Wang,Anjalie Field*

Main category: cs.CL

TL;DR: 本文提出HICode，一个基于LLM的两阶段管道，通过归纳生成标签并分层聚类来发现主题，旨在将定性研究的细致分析扩展到大型文本语料库，并已在多个数据集和阿片类药物危机案例研究中得到验证。


<details>
  <summary>Details</summary>
Motivation: 研究人员在细粒度语料库分析中仍依赖手动标注（不可扩展）或主题建模等统计工具（难以控制）。研究动机是利用LLM将研究人员通常手动进行的细致分析扩展到大型文本语料库。

Method: 受定性研究方法启发，开发了HICode，一个两阶段管道：1. 从分析数据中归纳地直接生成标签。2. 对这些标签进行分层聚类以发现 emergent 主题。

Result: 该方法在三个不同数据集上得到了验证，测量了与人工构建主题的一致性，并通过自动化和人工评估展示了其鲁棒性。此外，通过对美国阿片类药物危机相关诉讼文件的案例研究，揭示了制药公司采用的激进营销策略。

Conclusion: HICode展示了其在促进大规模数据中细致分析的潜力，表明LLM能够有效扩展定性研究方法进行复杂语料库分析。

Abstract: Despite numerous applications for fine-grained corpus analysis, researchers
continue to rely on manual labeling, which does not scale, or statistical tools
like topic modeling, which are difficult to control. We propose that LLMs have
the potential to scale the nuanced analyses that researchers typically conduct
manually to large text corpora. To this effect, inspired by qualitative
research methods, we develop HICode, a two-part pipeline that first inductively
generates labels directly from analysis data and then hierarchically clusters
them to surface emergent themes. We validate this approach across three diverse
datasets by measuring alignment with human-constructed themes and demonstrating
its robustness through automated and human evaluations. Finally, we conduct a
case study of litigation documents related to the ongoing opioid crisis in the
U.S., revealing aggressive marketing strategies employed by pharmaceutical
companies and demonstrating HICode's potential for facilitating nuanced
analyses in large-scale data.

</details>


### [400] [Make Every Letter Count: Building Dialect Variation Dictionaries from Monolingual Corpora](https://arxiv.org/abs/2509.17855)
*Robert Litschko,Verena Blaschke,Diana Burkhardt,Barbara Plank,Diego Frassinelli*

Main category: cs.CL

TL;DR: 本研究调查了大型语言模型（LLMs）对巴伐利亚方言的词汇理解能力，发现LLMs在处理方言正字法变体方面存在局限性，尤其难以区分直接翻译和屈折变体，并强调了未来需要针对方言调整LLMs。


<details>
  <summary>Details</summary>
Motivation: 方言因缺乏标准正字法而表现出显著的变异性，但大型语言模型（LLMs）处理方言的能力在很大程度上仍未被充分研究。本研究旨在弥补这一空白。

Method: 研究以巴伐利亚方言为例，调查了LLMs的词汇方言理解能力，评估它们识别和翻译不同词性方言词汇的程度。为此，研究引入了DiaLemma，一个仅从单语数据创建方言变体词典的新型标注框架，并用它编译了一个包含10万个人工标注的德语-巴伐利亚语词对的真实数据集。研究评估了九个最先进的LLMs判断巴伐利亚语词汇是方言翻译、屈折变体还是给定德语词根的无关形式的能力。此外，还探讨了提供额外上下文（如使用示例）对翻译性能的影响。

Result: LLMs在名词和词汇相似的词对上表现最佳，但在区分直接翻译和屈折变体方面最为困难。有趣的是，提供额外上下文（使用示例）提高了翻译性能，但却降低了它们识别方言变体的能力。

Conclusion: 本研究强调了LLMs在处理正字法方言变体方面的局限性，并强调了未来工作需要使LLMs适应方言的需求。

Abstract: Dialects exhibit a substantial degree of variation due to the lack of a
standard orthography. At the same time, the ability of Large Language Models
(LLMs) to process dialects remains largely understudied. To address this gap,
we use Bavarian as a case study and investigate the lexical dialect
understanding capability of LLMs by examining how well they recognize and
translate dialectal terms across different parts-of-speech. To this end, we
introduce DiaLemma, a novel annotation framework for creating dialect variation
dictionaries from monolingual data only, and use it to compile a ground truth
dataset consisting of 100K human-annotated German-Bavarian word pairs. We
evaluate how well nine state-of-the-art LLMs can judge Bavarian terms as
dialect translations, inflected variants, or unrelated forms of a given German
lemma. Our results show that LLMs perform best on nouns and lexically similar
word pairs, and struggle most in distinguishing between direct translations and
inflected variants. Interestingly, providing additional context in the form of
example usages improves the translation performance, but reduces their ability
to recognize dialect variants. This study highlights the limitations of LLMs in
dealing with orthographic dialect variation and emphasizes the need for future
work on adapting LLMs to dialects.

</details>


### [401] [ReDepress: A Cognitive Framework for Detecting Depression Relapse from Social Media](https://arxiv.org/abs/2509.17991)
*Aakash Kumar Agarwal,Saprativa Bhattacharjee,Mauli Rastogi,Jemima S. Jacob,Biplab Banerjee,Rashmi Gupta,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: 本研究推出了首个经过临床验证的抑郁症复发社交媒体数据集ReDepress，并基于认知理论，利用认知偏差特征实现了高效的抑郁症复发检测。


<details>
  <summary>Details</summary>
Motivation: 近50%的抑郁症患者面临复发风险，二次复发后风险增至80%。尽管社交媒体上的抑郁症检测受到广泛关注，但由于缺乏专门的数据集以及难以区分复发和非复发用户，抑郁症复发检测领域仍未得到充分探索。

Method: 本研究构建了ReDepress数据集，包含204名经心理健康专业人员标注的Reddit用户数据。该框架借鉴抑郁症的认知理论，将注意偏差、解释偏差、记忆偏差和反刍等认知构建纳入标注和建模过程。通过统计分析和机器学习实验，评估了认知标记的区分能力和模型性能，特别是使用了基于Transformer的时序模型。

Result: 认知标记能显著区分复发和非复发群体。融合这些特征的模型表现出色，其中基于Transformer的时序模型达到了0.86的F1分数。这些发现验证了现实文本数据中的心理学理论。

Conclusion: 研究结果证实了认知信息计算方法在早期抑郁症复发检测方面的巨大潜力，为可扩展、低成本的精神卫生干预措施铺平了道路。

Abstract: Almost 50% depression patients face the risk of going into relapse. The risk
increases to 80% after the second episode of depression. Although, depression
detection from social media has attained considerable attention, depression
relapse detection has remained largely unexplored due to the lack of curated
datasets and the difficulty of distinguishing relapse and non-relapse users. In
this work, we present ReDepress, the first clinically validated social media
dataset focused on relapse, comprising 204 Reddit users annotated by mental
health professionals. Unlike prior approaches, our framework draws on cognitive
theories of depression, incorporating constructs such as attention bias,
interpretation bias, memory bias and rumination into both annotation and
modeling. Through statistical analyses and machine learning experiments, we
demonstrate that cognitive markers significantly differentiate relapse and
non-relapse groups, and that models enriched with these features achieve
competitive performance, with transformer-based temporal models attaining an F1
of 0.86. Our findings validate psychological theories in real-world textual
data and underscore the potential of cognitive-informed computational methods
for early relapse detection, paving the way for scalable, low-cost
interventions in mental healthcare.

</details>


### [402] [CorPipe at CRAC 2025: Evaluating Multilingual Encoders for Multilingual Coreference Resolution](https://arxiv.org/abs/2509.17858)
*Milan Straka*

Main category: cs.CL

TL;DR: CorPipe 25 在 CRAC 2025 多语言共指消解共享任务中获胜，该系统是对先前系统的完全重新实现，迁移至 PyTorch，并在 LLM 和无限制赛道中均以显著优势超越其他所有提交。


<details>
  <summary>Details</summary>
Motivation: CRAC 2025 共享任务是第四届，引入了新的 LLM 赛道，减少了开发和测试集以降低计算要求，并增加了额外数据集。这促使研究人员需要重新实现和优化现有系统，以适应这些新变化并提升性能。

Method: 本研究提出了 CorPipe 25 系统，它是对之前系统（CorPipe）的完全重新实现，并从 TensorFlow 迁移到了 PyTorch 框架。该系统参与了 CRAC 2025 共享任务的 LLM 和无限制赛道。

Result: CorPipe 25 在 CRAC 2025 共享任务的 LLM 和无限制赛道中均显著优于所有其他提交，领先 8 个百分点。源代码和训练模型已公开提供。

Conclusion: CorPipe 25 成功赢得了 CRAC 2025 多语言共指消解共享任务，证明了其重新实现的 PyTorch 系统在处理多语言共指消解任务（包括新的 LLM 场景）方面的卓越性能和有效性。

Abstract: We present CorPipe 25, the winning entry to the CRAC 2025 Shared Task on
Multilingual Coreference Resolution. This fourth iteration of the shared task
introduces a new LLM track alongside the original unconstrained track, features
reduced development and test sets to lower computational requirements, and
includes additional datasets. CorPipe 25 represents a complete reimplementation
of our previous systems, migrating from TensorFlow to PyTorch. Our system
significantly outperforms all other submissions in both the LLM and
unconstrained tracks by a substantial margin of 8 percentage points. The source
code and trained models are publicly available at
https://github.com/ufal/crac2025-corpipe.

</details>


### [403] [Variation in Verification: Understanding Verification Dynamics in Large Language Models](https://arxiv.org/abs/2509.17995)
*Yefan Zhou,Austin Xu,Yilun Zhou,Janvijay Singh,Jiang Gui,Shafiq Joty*

Main category: cs.CL

TL;DR: 本研究系统分析了大型语言模型（LLM）测试时扩展（TTS）中生成式验证器（通过思维链推理和二元判断进行验证）的动态，发现问题难度、生成器能力和验证器能力如何影响验证效果，并提出优化验证策略的见解。


<details>
  <summary>Details</summary>
Motivation: LLM在测试时计算扩展（TTS）方面取得了显著进展，能够解决日益复杂的问题。其中一种有效的TTS范式是LLM生成器生成多个候选解决方案，LLM验证器在没有参考答案的情况下评估这些候选方案的正确性。本研究旨在深入理解生成式验证器的工作机制及其影响因素。

Method: 本研究通过生成思维链（CoT）推理后给出二元判断的生成式验证器进行验证。研究方法是对验证动态进行系统分析，考察了三个维度：问题难度、生成器能力和验证器生成能力。通过对12个基准测试（涵盖数学推理、知识和自然语言推理任务）进行实证研究，使用了14个开源模型（参数范围2B至72B）以及GPT-4o。

Result: 研究揭示了验证有效性的三个关键发现：(1) 简单问题使验证器能更可靠地确认正确响应；(2) 弱生成器产生的错误比强生成器更容易被检测；(3) 验证能力通常与验证器自身的解题能力相关，但这种关系随问题难度而变化。研究还发现，优化策略可以使一些弱生成器在验证后的TTS性能上接近更强的生成器（例如，Gemma2-9B到Gemma2-27B的性能差距缩小了75.5%）。此外，在某些情况下，强大的验证器相比弱验证器优势有限，表明仅靠验证器扩展无法克服根本性的验证挑战。

Conclusion: 这些发现揭示了在TTS应用中优化基本验证策略的机会。研究强调了在不同问题难度和模型能力下，验证器性能的复杂性，并指出单纯的验证器扩展可能不足以解决所有验证难题，需结合具体场景进行策略调整。

Abstract: Recent advances have shown that scaling test-time computation enables large
language models (LLMs) to solve increasingly complex problems across diverse
domains. One effective paradigm for test-time scaling (TTS) involves LLM
generators producing multiple solution candidates, with LLM verifiers assessing
the correctness of these candidates without reference answers. In this paper,
we study generative verifiers, which perform verification by generating
chain-of-thought (CoT) reasoning followed by a binary verdict. We
systematically analyze verification dynamics across three dimensions - problem
difficulty, generator capability, and verifier generation capability - with
empirical studies on 12 benchmarks across mathematical reasoning, knowledge,
and natural language reasoning tasks using 14 open-source models (2B to 72B
parameter range) and GPT-4o. Our experiments reveal three key findings about
verification effectiveness: (1) Easy problems allow verifiers to more reliably
certify correct responses; (2) Weak generators produce errors that are easier
to detect than strong generators; (3) Verification ability is generally
correlated with the verifier's own problem-solving capability, but this
relationship varies with problem difficulty. These findings reveal
opportunities to optimize basic verification strategies in TTS applications.
First, given the same verifier, some weak generators can nearly match stronger
ones in post-verification TTS performance (e.g., the Gemma2-9B to Gemma2-27B
performance gap shrinks by 75.5%). Second, we identify cases where strong
verifiers offer limited advantage over weak ones, as both fail to provide
meaningful verification gains, suggesting that verifier scaling alone cannot
overcome fundamental verification challenges.

</details>


### [404] [Unsupervised Learning and Representation of Mandarin Tonal Categories by a Generative CNN](https://arxiv.org/abs/2509.17859)
*Kai Schenck,Gašper Beguš*

Main category: cs.CL

TL;DR: 本文提出了一种在无监督模型中建模声调学习的方法，并证明 ciwGAN 模型可以在无标注数据的情况下学习普通话的声调模式，其学习结果与人类语言习得阶段相符。


<details>
  <summary>Details</summary>
Motivation: 声调模式是语言学习中最复杂的计算目标之一。研究旨在探索一个真实的生成模型（ciwGAN）是否能在完全无监督的情况下学习人类语言习得中的声调模式，特别是普通话声调。

Method: 研究采用 ciwGAN 这种生成模型，在无标注数据的情况下进行训练，以学习普通话声调。通过分析分类变量之间的 F0（基频）差异来评估模型是否编码了声调。此外，还提出了追踪内部卷积层中声调表示的方法。

Result: 所有三个训练模型在分类变量之间都显示出 F0 的统计学显著差异。仅用男性语料训练的模型能够持续编码声调。结果表明，模型不仅学习了普通话的声调对比，而且学习到的系统与人类语言学习者的习得阶段相符。

Conclusion: 研究表明，无监督的生成模型能够学习普通话中复杂的声调模式，且其学习过程与人类语言习得阶段具有对应性。此外，语言学工具可以帮助解释深度学习模型，并可用于神经实验。

Abstract: This paper outlines the methodology for modeling tonal learning in fully
unsupervised models of human language acquisition. Tonal patterns are among the
computationally most complex learning objectives in language. We argue that a
realistic generative model of human language (ciwGAN) can learn to associate
its categorical variables with Mandarin Chinese tonal categories without any
labeled data. All three trained models showed statistically significant
differences in F0 across categorical variables. The model trained solely on
male tokens consistently encoded tone. Our results sug- gest that not only does
the model learn Mandarin tonal contrasts, but it learns a system that
corresponds to a stage of acquisition in human language learners. We also
outline methodology for tracing tonal representations in internal convolutional
layers, which shows that linguistic tools can contribute to interpretability of
deep learning and can ultimately be used in neural experiments.

</details>


### [405] [Cross-Attention is Half Explanation in Speech-to-Text Models](https://arxiv.org/abs/2509.18010)
*Sara Papi,Dennis Fucci,Marco Gaido,Matteo Negri,Luisa Bentivogli*

Main category: cs.CL

TL;DR: 本研究评估了语音到文本（S2T）模型中交叉注意力机制的解释力，发现其与基于显著性的解释有中度到强度的对齐，但只能捕获约50%的输入相关性，表明它提供了一个有信息但不完整的解释视角。


<details>
  <summary>Details</summary>
Motivation: 交叉注意力分数在S2T处理中常被用于下游应用，并被假设反映输入语音与生成文本之间的依赖关系。然而，尽管注意力机制的解释性在NLP领域被广泛讨论，但在语音领域，这一假设仍未得到充分探索。本研究旨在填补这一空白。

Method: 研究通过比较交叉注意力分数与源自特征归因的输入显著性图，来评估S2T模型中交叉注意力的解释力。分析涵盖了单语和多语、单任务和多任务模型，并涉及多个模型规模。

Result: 结果显示，注意力分数与基于显著性的解释有中度到强度的对齐，尤其是在跨注意力头和层聚合时。然而，交叉注意力仅捕获了约50%的输入相关性，在最佳情况下，也仅部分反映了解码器如何关注编码器表示，解释了52-75%的显著性。

Conclusion: 这些发现揭示了将交叉注意力解释为解释性代理的基本局限性，表明它在S2T模型中提供了关于预测驱动因素的信息性但并不完整的视图。

Abstract: Cross-attention is a core mechanism in encoder-decoder architectures,
widespread in many fields, including speech-to-text (S2T) processing. Its
scores have been repurposed for various downstream applications--such as
timestamp estimation and audio-text alignment--under the assumption that they
reflect the dependencies between input speech representation and the generated
text. While the explanatory nature of attention mechanisms has been widely
debated in the broader NLP literature, this assumption remains largely
unexplored within the speech domain. To address this gap, we assess the
explanatory power of cross-attention in S2T models by comparing its scores to
input saliency maps derived from feature attribution. Our analysis spans
monolingual and multilingual, single-task and multi-task models at multiple
scales, and shows that attention scores moderately to strongly align with
saliency-based explanations, particularly when aggregated across heads and
layers. However, it also shows that cross-attention captures only about 50% of
the input relevance and, in the best case, only partially reflects how the
decoder attends to the encoder's representations--accounting for just 52-75% of
the saliency. These findings uncover fundamental limitations in interpreting
cross-attention as an explanatory proxy, suggesting that it offers an
informative yet incomplete view of the factors driving predictions in S2T
models.

</details>


### [406] [SiDiaC: Sinhala Diachronic Corpus](https://arxiv.org/abs/2509.17912)
*Nevidu Jayatilleke,Nisansa de Silva*

Main category: cs.CL

TL;DR: SiDiaC是首个全面的僧伽罗语历时语料库，涵盖公元5至20世纪，包含5.8万词和46部文学作品，旨在支持僧伽罗语NLP的历时研究。


<details>
  <summary>Details</summary>
Motivation: 由于僧伽罗语作为低资源语言的现状，缺乏用于历时语言学研究的综合性语料库。构建SiDiaC旨在扩展僧伽罗语的资源，使其能够进行词汇变化、新词追踪、历史句法和基于语料库的词典编纂等历时研究。

Method: SiDiaC的构建涉及从斯里兰卡国家图书馆获取文本，使用Google Document AI OCR进行数字化，随后进行后期处理以纠正格式和规范拼写。语料库根据可用性、作者身份、版权和数据归属进行筛选，并根据书写日期进行仔细标注。其构建借鉴了其他语料库（如FarPaHC）的做法，特别是在句法标注和文本规范化策略方面。语料库分为两层体裁类别：主要（非虚构/虚构）和次要（宗教、历史、诗歌、语言、医学）。

Result: SiDiaC是首个全面的僧伽罗语历时语料库，涵盖公元5至20世纪，包含5.8万词、46部文学作品，并经过精心标注和体裁分类。它为僧伽罗语NLP提供了基础资源，显著扩展了该语言的可用资源。

Conclusion: SiDiaC作为僧伽罗语NLP的基础资源，将极大地促进对僧伽罗语的历时研究，包括词汇变化、新词追踪、历史句法和基于语料库的词典编纂，从而填补该低资源语言在这些领域的研究空白。

Abstract: SiDiaC, the first comprehensive Sinhala Diachronic Corpus, covers a
historical span from the 5th to the 20th century CE. SiDiaC comprises 58k words
across 46 literary works, annotated carefully based on the written date, after
filtering based on availability, authorship, copyright compliance, and data
attribution. Texts from the National Library of Sri Lanka were digitised using
Google Document AI OCR, followed by post-processing to correct formatting and
modernise the orthography. The construction of SiDiaC was informed by practices
from other corpora, such as FarPaHC, particularly in syntactic annotation and
text normalisation strategies, due to the shared characteristics of
low-resourced language status. This corpus is categorised based on genres into
two layers: primary and secondary. Primary categorisation is binary,
classifying each book into Non-Fiction or Fiction, while the secondary
categorisation is more specific, grouping texts under Religious, History,
Poetry, Language, and Medical genres. Despite challenges including limited
access to rare texts and reliance on secondary date sources, SiDiaC serves as a
foundational resource for Sinhala NLP, significantly extending the resources
available for Sinhala, enabling diachronic studies in lexical change, neologism
tracking, historical syntax, and corpus-based lexicography.

</details>


### [407] [Improving Zero-shot Sentence Decontextualisation with Content Selection and Planning](https://arxiv.org/abs/2509.17921)
*Zhenyun Deng,Yulong Chen,Andreas Vlachos*

Main category: cs.CL

TL;DR: 本文提出一个零样本去语境化框架，通过内容选择和规划，使从文档中提取的句子在脱离原文语境后仍能被理解，解决了核心指代和背景信息缺失的问题。


<details>
  <summary>Details</summary>
Motivation: 在许多自然语言处理任务中，从文档中提取的单个句子作为证据或推理步骤很常见。然而，这些提取出的句子往往缺乏理解所需的语境（例如，核心指代和背景信息），导致其难以被理解。

Method: 该方法包括一个内容选择和规划框架。具体来说，给定一个潜在模糊的句子及其语境，首先将其分割成语义独立的单元。然后识别句子中潜在模糊的单元，并根据语篇关系从语境中提取相关单元。最后，生成一个内容规划，通过用相关单元丰富每个模糊单元来重写句子。

Result: 实验结果表明，该方法在句子去语境化方面具有竞争力，生成的句子展现出更好的语义完整性和语篇连贯性，优于现有方法。

Conclusion: 所提出的内容选择和规划框架能有效地进行零样本去语境化，通过为脱离语境的句子补充必要信息，显著提升了句子的可理解性、语义完整性和语篇连贯性。

Abstract: Extracting individual sentences from a document as evidence or reasoning
steps is commonly done in many NLP tasks. However, extracted sentences often
lack context necessary to make them understood, e.g., coreference and
background information. To this end, we propose a content selection and
planning framework for zero-shot decontextualisation, which determines what
content should be mentioned and in what order for a sentence to be understood
out of context. Specifically, given a potentially ambiguous sentence and its
context, we first segment it into basic semantically-independent units. We then
identify potentially ambiguous units from the given sentence, and extract
relevant units from the context based on their discourse relations. Finally, we
generate a content plan to rewrite the sentence by enriching each ambiguous
unit with its relevant units. Experimental results demonstrate that our
approach is competitive for sentence decontextualisation, producing sentences
that exhibit better semantic integrity and discourse coherence, outperforming
existing methods.

</details>


### [408] [TMD-TTS: A Unified Tibetan Multi-Dialect Text-to-Speech Synthesis for Ü-Tsang, Amdo and Kham Speech Dataset Generation](https://arxiv.org/abs/2509.18060)
*Yutong Liu,Ziyue Zhang,Ban Ma-bao,Renzeng Duojie,Yuqing Cai,Yongbin Yu,Xiangxiang Wang,Fan Gao,Cheng Huang,Nyima Tashi*

Main category: cs.CL

TL;DR: 本文提出TMD-TTS，一个统一的藏语多方言文本到语音（TTS）框架，通过方言融合模块和DSDR-Net，显著提升了藏语多方言语音合成的表达能力。


<details>
  <summary>Details</summary>
Motivation: 藏语是一种低资源语言，其三大主要方言（卫藏、安多、康巴）的并行语音语料库有限，这限制了语音建模的进展。

Method: 本文提出TMD-TTS框架，通过明确的方言标签合成并行方言语音。该方法包含一个方言融合模块和一个方言专用动态路由网络（DSDR-Net），旨在捕捉方言间细微的声学和语言变异。

Result: 广泛的客观和主观评估表明，TMD-TTS在方言表达能力方面显著优于基线。通过一项具有挑战性的语音到语音方言转换（S2SDC）任务，进一步验证了合成语音的质量和实用性。

Conclusion: TMD-TTS框架成功解决了藏语多方言语音合成的挑战，有效捕捉了方言间的差异，并为低资源语言的语音技术发展提供了新的方向和工具。

Abstract: Tibetan is a low-resource language with limited parallel speech corpora
spanning its three major dialects (\"U-Tsang, Amdo, and Kham), limiting
progress in speech modeling. To address this issue, we propose TMD-TTS, a
unified Tibetan multi-dialect text-to-speech (TTS) framework that synthesizes
parallel dialectal speech from explicit dialect labels. Our method features a
dialect fusion module and a Dialect-Specialized Dynamic Routing Network
(DSDR-Net) to capture fine-grained acoustic and linguistic variations across
dialects. Extensive objective and subjective evaluations demonstrate that
TMD-TTS significantly outperforms baselines in dialectal expressiveness. We
further validate the quality and utility of the synthesized speech through a
challenging Speech-to-Speech Dialect Conversion (S2SDC) task.

</details>


### [409] [Training-free Truthfulness Detection via Value Vectors in LLMs](https://arxiv.org/abs/2509.17932)
*Runheng Liu,Heyan Huang,Xingchen Xiao,Zhijing Wu*

Main category: cs.CL

TL;DR: 本文提出TruthV，一种无需训练的方法，通过利用大型语言模型中MLP模块的特定值向量来检测内容的事实准确性，显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型常产生事实错误内容，现有检测方法（如基于内部激活的探针）存在可扩展性和泛化性问题。现有免训练方法（如NoVo）仅关注注意力机制，忽略了对事实记忆至关重要的MLP模块。

Method: 本文发现MLP模块中的某些值向量展现出与事实准确性相关的统计模式。基于此洞察，提出TruthV，一种简单、可解释的免训练方法，通过利用这些值向量来检测内容的真实性。

Result: 在NoVo基准测试中，TruthV显著优于NoVo和对数似然基线。结果表明，MLP模块（尽管在之前的免训练工作中被忽视）编码了丰富且有用的事实准确性检测信号。

Conclusion: 这些发现为理解LLM内部如何表示事实准确性提供了新见解，并鼓励未来在可扩展和可解释的事实准确性检测方面进行进一步研究。

Abstract: Large language models often generate factually incorrect outputs, motivating
efforts to detect the truthfulness of their content. Most existing approaches
rely on training probes over internal activations, but these methods suffer
from scalability and generalization issues. A recent training-free method,
NoVo, addresses this challenge by exploiting statistical patterns from the
model itself. However, it focuses exclusively on attention mechanisms,
potentially overlooking the MLP module-a core component of Transformer models
known to support factual recall. In this paper, we show that certain value
vectors within MLP modules exhibit truthfulness-related statistical patterns.
Building on this insight, we propose TruthV, a simple and interpretable
training-free method that detects content truthfulness by leveraging these
value vectors. On the NoVo benchmark, TruthV significantly outperforms both
NoVo and log-likelihood baselines, demonstrating that MLP modules-despite being
neglected in prior training-free efforts-encode rich and useful signals for
truthfulness detection. These findings offer new insights into how truthfulness
is internally represented in LLMs and motivate further research on scalable and
interpretable truthfulness detection.

</details>


### [410] [SEQR: Secure and Efficient QR-based LoRA Routing](https://arxiv.org/abs/2509.18093)
*William Fleshman,Benjamin Van Durme*

Main category: cs.CL

TL;DR: 本文提出了一种名为SEQR的无监督LoRA路由算法，通过最大化激活范数来高效选择LoRA适配器，解决了有监督路由的隐私问题，并提供了严格的路由保证。


<details>
  <summary>Details</summary>
Motivation: LoRA已成为大语言模型参数高效微调的标准技术，但为给定输入高效选择正确的LoRA适配器仍是一个挑战，尤其是在安全环境中，有监督的路由训练可能引发隐私担忧。

Method: 作者将无监督LoRA路由的目标形式化为激活范数最大化问题，并基于此引入了SEQR算法。SEQR旨在最大化效率并提供严格的路由保证，通过识别范数最大的适配器来实现路由。

Result: 研究表明激活范数具有判别力。SEQR算法能够以显著更高的效率识别出范数最大的适配器，使其成为动态LoRA组合的高度可扩展和有效的解决方案。实验验证了多任务性能和效率的提升。

Conclusion: SEQR是一种可扩展且高效的无监督LoRA路由算法，通过利用激活范数最大化原理，解决了LoRA选择的挑战，并在动态LoRA组合中表现出色。

Abstract: Low-Rank Adaptation (LoRA) has become a standard technique for
parameter-efficient fine-tuning of large language models, enabling large
libraries of LoRAs, each for a specific task or domain. Efficiently selecting
the correct LoRA adapter for a given input remains a challenge, particularly in
secure environments where supervised training of routers may raise privacy
concerns. Motivated by previous approaches, we formalize the goal of
unsupervised LoRA routing in terms of activation norm maximization, providing a
theoretical framework for analysis. We demonstrate the discriminative power of
activation norms and introduce SEQR, an unsupervised LoRA routing algorithm
designed to maximize efficiency while providing strict routing guarantees. SEQR
provably identifies the norm-maximizing adapter with significantly greater
efficiency, making it a highly scalable and effective solution for dynamic LoRA
composition. We validate our results through experiments that demonstrate
improved multi-task performance and efficiency.

</details>


### [411] [D-REX: A Benchmark for Detecting Deceptive Reasoning in Large Language Models](https://arxiv.org/abs/2509.17938)
*Satyapriya Krishna,Andy Zou,Rahul Gupta,Eliot Krzysztof Jones,Nick Winter,Dan Hendrycks,J. Zico Kolter,Matt Fredrikson,Spyros Matsoukas*

Main category: cs.CL

TL;DR: 本研究引入了D-REX数据集，旨在评估大型语言模型（LLMs）的欺骗性推理能力，即模型输出看似无害，但其内部推理过程却带有恶意或欺骗性意图的风险。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs评估方法主要关注识别和阻止明显的有害输出，但未能解决一种更隐蔽的失败模式：模型表面输出无害，但内部推理恶意或欺骗。这种漏洞常由复杂的系统提示注入触发，绕过传统安全过滤器，构成一个重大但未被充分探索的风险。

Method: 研究通过一项竞争性红队演练构建了D-REX数据集。参与者设计对抗性系统提示以诱导模型的欺骗行为。D-REX中的每个样本包含对抗性系统提示、最终用户的测试查询、模型看似无害的响应，以及揭示潜在恶意意图的模型内部思维链。

Result: D-REX对现有模型和安全机制提出了严峻挑战，突显了迫切需要新的技术来审查LLMs的内部过程，而不仅仅是其最终输出。

Conclusion: 本研究引入了检测欺骗性对齐这一新的、重要的评估任务，强调了审查LLMs内部过程而非仅最终输出的紧迫性，以确保其负责任的部署。

Abstract: The safety and alignment of Large Language Models (LLMs) are critical for
their responsible deployment. Current evaluation methods predominantly focus on
identifying and preventing overtly harmful outputs. However, they often fail to
address a more insidious failure mode: models that produce benign-appearing
outputs while operating on malicious or deceptive internal reasoning. This
vulnerability, often triggered by sophisticated system prompt injections,
allows models to bypass conventional safety filters, posing a significant,
underexplored risk. To address this gap, we introduce the Deceptive Reasoning
Exposure Suite (D-REX), a novel dataset designed to evaluate the discrepancy
between a model's internal reasoning process and its final output. D-REX was
constructed through a competitive red-teaming exercise where participants
crafted adversarial system prompts to induce such deceptive behaviors. Each
sample in D-REX contains the adversarial system prompt, an end-user's test
query, the model's seemingly innocuous response, and, crucially, the model's
internal chain-of-thought, which reveals the underlying malicious intent. Our
benchmark facilitates a new, essential evaluation task: the detection of
deceptive alignment. We demonstrate that D-REX presents a significant challenge
for existing models and safety mechanisms, highlighting the urgent need for new
techniques that scrutinize the internal processes of LLMs, not just their final
outputs.

</details>


### [412] [Dorabella Cipher as Musical Inspiration](https://arxiv.org/abs/2509.17950)
*Bradley Hauer,Colin Choi,Abram Hindle,Scott Smallwood,Grzegorz Kondrak*

Main category: cs.CL

TL;DR: 本文提出多拉贝拉密码可能加密了音乐而非文本，通过音乐N-gram模型解密并重构出一段可听旋律，将解密过程视为作曲的一部分。


<details>
  <summary>Details</summary>
Motivation: 多拉贝拉密码已逾百年未被破译，多数解密尝试集中于英文文本。作者旨在探讨其是否可能加密了音乐，并尝试从音乐角度进行解密。

Method: 研究多拉贝拉密码为音乐的证据，设计简化音乐符号系统，使用经单字母替换加密音乐语料库验证的音乐N-gram模型，将此方法应用于多拉贝拉密码以重建旋律，并通过艺术性编曲将其转化为可听的旋律。

Result: 成功产生了一个具有音乐特质的解密结果，并将其通过巧妙的编曲转化为一段可听的旋律。

Conclusion: 研究结果并非唯一真实解，而是将解密过程视为作曲过程的一部分。

Abstract: The Dorabella cipher is an encrypted note written by English composer Edward
Elgar, which has defied decipherment attempts for more than a century. While
most proposed solutions are English texts, we investigate the hypothesis that
Dorabella represents enciphered music. We weigh the evidence for and against
the hypothesis, devise a simplified music notation, and attempt to reconstruct
a melody from the cipher. Our tools are n-gram models of music which we
validate on existing music corpora enciphered using monoalphabetic
substitution. By applying our methods to Dorabella, we produce a decipherment
with musical qualities, which is then transformed via artful composition into a
listenable melody. Far from arguing that the end result represents the only
true solution, we instead frame the process of decipherment as part of the
composition process.

</details>


### [413] [Bringing Pedagogy into Focus: Evaluating Virtual Teaching Assistants' Question-Answering in Asynchronous Learning Environments](https://arxiv.org/abs/2509.17961)
*Li Siyan,Zhen Xu,Vethavikashini Chithrra Raghuram,Xuanming Zhang,Renzhe Yu,Zhou Yu*

Main category: cs.CL

TL;DR: 本文提出一个基于学习科学的评估框架，用于异步学习环境中虚拟助教（VTA）的教学效果评估，旨在解决现有评估方法缺乏理论基础的问题，并通过构建分类器进行验证。


<details>
  <summary>Details</summary>
Motivation: 异步学习环境（ALE）中缺乏及时和个性化的支持。虚拟助教（VTA）有潜力减轻教师负担，但对其教学效果的评估缺乏严谨性和教育学基础，现有评估多停留在表面指标，难以有效比较不同VTA系统。

Method: 研究提出一个根植于学习科学并针对异步论坛讨论的VTA评估框架。通过专家标注VTA对各类论坛帖子的回复，构建了分类器，并评估了其有效性，识别了提高准确性的方法以及阻碍泛化的挑战。

Result: 研究成功建立了VTA系统理论驱动评估的基础。在评估分类器时，识别出提高准确性的方法以及在泛化方面存在的挑战。

Conclusion: 该工作为VTA系统提供了理论驱动的评估基础，为教育领域中开发更具教学有效性的AI系统铺平了道路。

Abstract: Asynchronous learning environments (ALEs) are widely adopted for formal and
informal learning, but timely and personalized support is often limited. In
this context, Virtual Teaching Assistants (VTAs) can potentially reduce the
workload of instructors, but rigorous and pedagogically sound evaluation is
essential. Existing assessments often rely on surface-level metrics and lack
sufficient grounding in educational theories, making it difficult to
meaningfully compare the pedagogical effectiveness of different VTA systems. To
bridge this gap, we propose an evaluation framework rooted in learning sciences
and tailored to asynchronous forum discussions, a common VTA deployment context
in ALE. We construct classifiers using expert annotations of VTA responses on a
diverse set of forum posts. We evaluate the effectiveness of our classifiers,
identifying approaches that improve accuracy as well as challenges that hinder
generalization. Our work establishes a foundation for theory-driven evaluation
of VTA systems, paving the way for more pedagogically effective AI in
education.

</details>


### [414] [WenetSpeech-Chuan: A Large-Scale Sichuanese Corpus with Rich Annotation for Dialectal Speech Processing](https://arxiv.org/abs/2509.18004)
*Yuhang Dai,Ziyu Zhang,Shuai Wang,Longhao Li,Zhao Guo,Tianlun Zuo,Shuiyuan Wang,Hongfei Xue,Chengyou Wang,Qing Wang,Xin Xu,Hui Bu,Jie Li,Jian Kang,Binbin Zhang,Lei Xie*

Main category: cs.CL

TL;DR: 本文介绍了WenetSpeech-Chuan，一个10,000小时的四川方言开源语料库，旨在解决方言语音技术数据稀缺的问题，并提供了配套的ASR和TTS基准，实现了领先的语音识别性能。


<details>
  <summary>Details</summary>
Motivation: 大规模、开源的方言数据稀缺严重阻碍了语音技术的发展，特别是对于广泛使用的四川方言。现有数据不足是该领域进步的关键障碍。

Method: 研究者通过其新颖的Chuan-Pipeline数据处理框架构建了WenetSpeech-Chuan，一个10,000小时、丰富标注的四川方言语料库。同时，发布了WenetSpeech-Chuan-Eval，包含手动验证转录的高质量ASR和TTS基准，以促进严格评估。

Result: 在WenetSpeech-Chuan上训练的模型在开源系统中取得了最先进的性能，并展现出与商业服务相当的结果。该语料库是目前最大的四川方言开源语料库。

Conclusion: WenetSpeech-Chuan的发布不仅降低了方言语音处理研究的门槛，还在促进人工智能公平性和缓解语音技术中的偏见方面发挥着关键作用。所有资源（语料库、基准、模型和收据）均已公开可用。

Abstract: The scarcity of large-scale, open-source data for dialects severely hinders
progress in speech technology, a challenge particularly acute for the widely
spoken Sichuanese dialects of Chinese. To address this critical gap, we
introduce WenetSpeech-Chuan, a 10,000-hour, richly annotated corpus constructed
using our novel Chuan-Pipeline, a complete data processing framework for
dialectal speech. To facilitate rigorous evaluation and demonstrate the
corpus's effectiveness, we also release high-quality ASR and TTS benchmarks,
WenetSpeech-Chuan-Eval, with manually verified transcriptions. Experiments show
that models trained on WenetSpeech-Chuan achieve state-of-the-art performance
among open-source systems and demonstrate results comparable to commercial
services. As the largest open-source corpus for Sichuanese dialects,
WenetSpeech-Chuan not only lowers the barrier to research in dialectal speech
processing but also plays a crucial role in promoting AI equity and mitigating
bias in speech technologies. The corpus, benchmarks, models, and receipts are
publicly available on our project page.

</details>


### [415] [RadEval: A framework for radiology text evaluation](https://arxiv.org/abs/2509.18030)
*Justin Xu,Xi Zhang,Javid Abderezaei,Julie Bauml,Roger Boodoo,Fatemeh Haghighi,Ali Ganjizadeh,Eric Brattain,Dave Van Veen,Zaiqiao Meng,David Eyre,Jean-Benoit Delbrouck*

Main category: cs.CL

TL;DR: RadEval是一个统一的开源框架，用于评估放射学文本，整合了多种度量标准、提供领域专用编码器、专家标注数据集和统计工具，以促进放射报告生成的评估和基准测试。


<details>
  <summary>Details</summary>
Motivation: 需要一个统一、标准化且全面的框架来评估放射学文本，特别是随着放射报告生成技术的发展，需要更准确地衡量生成报告的质量和临床相关性。

Method: RadEval整合了从传统n-gram（BLEU, ROUGE）到上下文（BERTScore）、临床概念（F1CheXbert, F1RadGraph, RaTEScore, SRR-BERT, TemporalEntityF1）以及先进的LLM（GREEN）等多种评估指标。它改进并标准化了现有实现，扩展了GREEN以支持多模态并使用更轻量级模型，预训练了一个领域专用的放射学编码器。此外，还发布了一个包含450多个临床显著错误标签的专家标注数据集，并提供了统计测试工具和基线模型评估。

Result: 领域专用放射学编码器展示了强大的零样本检索性能。研究表明了不同指标与放射科医生判断之间的相关性。RadEval通过提供统一的工具和基线评估，促进了放射报告生成领域的复现性和稳健的基准测试。

Conclusion: RadEval是一个全面且统一的放射学文本评估框架，它通过整合多样化指标、提供领域专用工具和高质量数据集，显著提升了放射报告生成模型的评估能力，有助于推动该领域的研发和标准化。

Abstract: We introduce RadEval, a unified, open-source framework for evaluating
radiology texts. RadEval consolidates a diverse range of metrics, from classic
n-gram overlap (BLEU, ROUGE) and contextual measures (BERTScore) to clinical
concept-based scores (F1CheXbert, F1RadGraph, RaTEScore, SRR-BERT,
TemporalEntityF1) and advanced LLM-based evaluators (GREEN). We refine and
standardize implementations, extend GREEN to support multiple imaging
modalities with a more lightweight model, and pretrain a domain-specific
radiology encoder, demonstrating strong zero-shot retrieval performance. We
also release a richly annotated expert dataset with over 450 clinically
significant error labels and show how different metrics correlate with
radiologist judgment. Finally, RadEval provides statistical testing tools and
baseline model evaluations across multiple publicly available datasets,
facilitating reproducibility and robust benchmarking in radiology report
generation.

</details>


### [416] [The PIMMUR Principles: Ensuring Validity in Collective Behavior of LLM Societies](https://arxiv.org/abs/2509.18052)
*Jiaxu Zhou,Jen-tse Huang,Xuhui Zhou,Man Ho Lam,Xintao Wang,Hao Zhu,Wenxuan Wang,Maarten Sap*

Main category: cs.CL

TL;DR: 本文识别了LLM社会模拟中常见的六种方法论缺陷，并提出了PIMMUR原则作为可信模拟的必要条件，发现许多现有研究在更严格条件下无法复现其社会现象。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）越来越多地用于社会模拟，以期重现人类般的集体行为。然而，许多近期研究采用的实验设计系统性地损害了其主张的有效性。

Method: 通过对40多篇论文的调查，识别了六个常见的方法论缺陷（Profile, Interaction, Memory, Minimal-Control, Unawareness, Realism），并将其形式化为PIMMUR原则。此外，测试了LLMs（如GPT-4o和Qwen-3）在给定指令时推断实验假设的能力，并使用一个强制执行PIMMUR的框架重新运行了五个代表性研究。

Result: 发现了六个常见的方法论缺陷。GPT-4o和Qwen-3在53.1%的情况下能正确推断出基础社会实验假设，违反了“不知情”原则。在强制执行PIMMUR原则后，许多先前报道的社会现象在更严格的条件下未能出现。

Conclusion: PIMMUR原则是基于LLM的社会模拟可信度的必要条件。本文为基于LLM的多智能体研究建立了方法论标准，为关于“AI社会”的更可靠和可复现的主张奠定了基础。

Abstract: Large Language Models (LLMs) are increasingly used for social simulation,
where populations of agents are expected to reproduce human-like collective
behavior. However, we find that many recent studies adopt experimental designs
that systematically undermine the validity of their claims. From a survey of
over 40 papers, we identify six recurring methodological flaws: agents are
often homogeneous (Profile), interactions are absent or artificially imposed
(Interaction), memory is discarded (Memory), prompts tightly control outcomes
(Minimal-Control), agents can infer the experimental hypothesis (Unawareness),
and validation relies on simplified theoretical models rather than real-world
data (Realism). For instance, GPT-4o and Qwen-3 correctly infer the underlying
social experiment in 53.1% of cases when given instructions from prior
work-violating the Unawareness principle. We formalize these six requirements
as the PIMMUR principles and argue they are necessary conditions for credible
LLM-based social simulation. To demonstrate their impact, we re-run five
representative studies using a framework that enforces PIMMUR and find that the
reported social phenomena frequently fail to emerge under more rigorous
conditions. Our work establishes methodological standards for LLM-based
multi-agent research and provides a foundation for more reliable and
reproducible claims about "AI societies."

</details>


### [417] [ARK-V1: An LLM-Agent for Knowledge Graph Question Answering Requiring Commonsense Reasoning](https://arxiv.org/abs/2509.18063)
*Jan-Felix Klein,Lars Ohnemus*

Main category: cs.CL

TL;DR: 本文提出了ARK-V1，一个简单的知识图谱（KG）代理，它利用大型语言模型（LLM）迭代探索图谱以回答自然语言查询，在CoLoTa数据集上显著优于Chain-of-Thought基线。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的推理能力很强，但在需要特定领域知识时，其内部知识可能不足、过时或不正确。知识图谱（KGs）提供了结构化的外部知识，但其复杂性和多跳推理要求使其集成具有挑战性。

Method: 研究人员提出了ARK-V1，一个简单的KG代理，它通过迭代探索图谱来回答自然语言查询。他们使用未经微调的SOTA LLM作为ARK-V1的骨干模型，并在CoLoTa数据集上进行了评估，该数据集需要基于KG和常识对长尾实体进行推理。

Result: ARK-V1的条件准确率显著高于Chain-of-Thought基线。更大的骨干模型在覆盖率、正确性和稳定性方面表现出明显的提升趋势。

Conclusion: ARK-V1成功地将LLMs与KG结合，以处理需要特定领域知识的复杂查询，并在长尾实体推理任务上取得了优异的性能，表明了这种集成方法的有效性。

Abstract: Large Language Models (LLMs) show strong reasoning abilities but rely on
internalized knowledge that is often insufficient, outdated, or incorrect when
trying to answer a question that requires specific domain knowledge. Knowledge
Graphs (KGs) provide structured external knowledge, yet their complexity and
multi-hop reasoning requirements make integration challenging. We present
ARK-V1, a simple KG-agent that iteratively explores graphs to answer natural
language queries. We evaluate several not fine-tuned state-of-the art LLMs as
backbones for ARK-V1 on the CoLoTa dataset, which requires both KG-based and
commonsense reasoning over long-tail entities. ARK-V1 achieves substantially
higher conditional accuracies than Chain-of-Thought baselines, and larger
backbone models show a clear trend toward better coverage, correctness, and
stability.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [418] [RaFD: Flow-Guided Radar Detection for Robust Autonomous Driving](https://arxiv.org/abs/2509.16261)
*Shuocheng Yang,Zikun Xu,Jiahao Wang,Shahid Nawaz,Jianqiang Wang,Shaobing Xu*

Main category: cs.RO

TL;DR: RaFD是一种雷达目标检测框架，通过估计帧间鸟瞰图（BEV）光流并利用几何信息来提高检测精度，解决了原始雷达图像噪声和伪影问题。


<details>
  <summary>Details</summary>
Motivation: 原始雷达图像常被噪声和“鬼影”伪影降级，导致仅基于语义特征的目标检测极具挑战性。

Method: 引入RaFD框架，该框架估计帧间BEV光流，并利用所得几何线索增强检测精度。具体而言，设计了一个有监督的光流估计辅助任务，与检测网络联合训练。估计的光流进一步用于指导特征从前一帧传播到当前帧。

Result: 该光流引导的纯雷达检测器在RADIATE数据集上取得了最先进的性能。

Conclusion: 强调了结合几何信息对于有效解释本质上语义模糊的雷达信号的重要性。

Abstract: Radar has shown strong potential for robust perception in autonomous driving;
however, raw radar images are frequently degraded by noise and "ghost"
artifacts, making object detection based solely on semantic features highly
challenging. To address this limitation, we introduce RaFD, a radar-based
object detection framework that estimates inter-frame bird's-eye-view (BEV)
flow and leverages the resulting geometric cues to enhance detection accuracy.
Specifically, we design a supervised flow estimation auxiliary task that is
jointly trained with the detection network. The estimated flow is further
utilized to guide feature propagation from the previous frame to the current
one. Our flow-guided, radar-only detector achieves achieves state-of-the-art
performance on the RADIATE dataset, underscoring the importance of
incorporating geometric information to effectively interpret radar signals,
which are inherently ambiguous in semantics.

</details>


### [419] [Tactile-Based Human Intent Recognition for Robot Assistive Navigation](https://arxiv.org/abs/2509.16353)
*Shaoting Peng,Dakarai Crowder,Wenzhen Yuan,Katherine Driggs-Campbell*

Main category: cs.RO

TL;DR: 本文提出Tac-Nav机器人辅助导航系统，利用圆柱形触觉皮肤和专门的CK-SVM算法，为行动不便者提供更自然、高效的导航意图识别界面。


<details>
  <summary>Details</summary>
Motivation: 现有机器人辅助导航系统（RAN）的界面无法复制人与人类看护者之间直观高效的物理交流，限制了其有效性，需要更自然的交互方式。

Method: 开发了Tac-Nav系统，该系统在Stretch 3移动机械臂上安装圆柱形触觉皮肤，用于捕捉用户的导航意图。为鲁棒分类触觉数据，设计了圆柱形核支持向量机（CK-SVM）算法，该算法明确建模传感器圆柱几何形状，对用户抓握中的自然旋转偏移具有鲁棒性。

Result: CK-SVM算法在模拟（97.1%）和真实（90.8%）数据集上的分类准确率均优于四种基线模型。初步用户研究表明，用户更偏爱Tac-Nav触觉界面，而非传统的操纵杆和语音控制。

Conclusion: Tac-Nav系统及其CK-SVM算法提供了一种更自然、更受用户欢迎的机器人辅助导航界面，有效提高了导航意图识别的准确性和用户体验。

Abstract: Robot assistive navigation (RAN) is critical for enhancing the mobility and
independence of the growing population of mobility-impaired individuals.
However, existing systems often rely on interfaces that fail to replicate the
intuitive and efficient physical communication observed between a person and a
human caregiver, limiting their effectiveness. In this paper, we introduce
Tac-Nav, a RAN system that leverages a cylindrical tactile skin mounted on a
Stretch 3 mobile manipulator to provide a more natural and efficient interface
for human navigational intent recognition. To robustly classify the tactile
data, we developed the Cylindrical Kernel Support Vector Machine (CK-SVM), an
algorithm that explicitly models the sensor's cylindrical geometry and is
consequently robust to the natural rotational shifts present in a user's grasp.
Comprehensive experiments were conducted to demonstrate the effectiveness of
our classification algorithm and the overall system. Results show that CK-SVM
achieved superior classification accuracy on both simulated (97.1%) and
real-world (90.8%) datasets compared to four baseline models. Furthermore, a
pilot study confirmed that users more preferred the Tac-Nav tactile interface
over conventional joystick and voice-based controls.

</details>


### [420] [Dynamic Objects Relocalization in Changing Environments with Flow Matching](https://arxiv.org/abs/2509.16398)
*Francesco Argenziano,Miguel Saavedra-Ruiz,Sacha Morin,Daniele Nardi,Liam Paull*

Main category: cs.RO

TL;DR: 在动态环境中，机器人需要重新定位被人类移动的物体。本文提出FlowMaps模型，利用Flow Matching推断多模态物体位置，通过利用人类习惯和重复模式来解决未知重定位问题。


<details>
  <summary>Details</summary>
Motivation: 机器人长期以来在动态环境（如家庭或仓库）中的任务和运动规划面临挑战，尤其当物体因人类活动而移动或移除时，需要重新定位物体，增加了失败风险。现有方法常忽视人类-物体交互的习惯和重复模式，而这些模式可能有助于恢复物体最可能的位置。

Method: 本文提出了FlowMaps，一个基于Flow Matching的模型。该模型能够利用人类习惯和重复模式，推断物体在空间和时间上的多模态位置，从而解决变化环境中未知重定位的问题。

Result: 研究结果提供了统计证据支持作者的假设，表明FlowMaps能够有效推断物体位置，并为该方法的更复杂应用奠定了基础。

Conclusion: FlowMaps模型通过利用人类-物体交互的习惯和模式，成功地解决了动态环境中机器人重新定位物体位置的难题，为未来的机器人规划和交互提供了新的方向。

Abstract: Task and motion planning are long-standing challenges in robotics, especially
when robots have to deal with dynamic environments exhibiting long-term
dynamics, such as households or warehouses. In these environments, long-term
dynamics mostly stem from human activities, since previously detected objects
can be moved or removed from the scene. This adds the necessity to find such
objects again before completing the designed task, increasing the risk of
failure due to missed relocalizations. However, in these settings, the nature
of such human-object interactions is often overlooked, despite being governed
by common habits and repetitive patterns. Our conjecture is that these cues can
be exploited to recover the most likely objects' positions in the scene,
helping to address the problem of unknown relocalization in changing
environments. To this end we propose FlowMaps, a model based on Flow Matching
that is able to infer multimodal object locations over space and time. Our
results present statistical evidence to support our hypotheses, opening the way
to more complex applications of our approach. The code is publically available
at https://github.com/Fra-Tsuna/flowmaps

</details>


### [421] [Subteaming and Adaptive Formation Control for Coordinated Multi-Robot Navigation](https://arxiv.org/abs/2509.16412)
*Zihao Deng,Peng Gao,Williard Joshua Jose,Maggie Wigness,John Rogers,Brian Reily,Christopher Reardon,Hao Zhang*

Main category: cs.RO

TL;DR: 本文提出了一种名为STAF的新方法，通过统一的分层学习框架实现多机器人团队在复杂环境中动态分组和自适应编队导航。


<details>
  <summary>Details</summary>
Motivation: 在复杂场景（如狭窄走廊）中，机器人团队难以维持预设的刚性编队。因此，机器人需要能够动态分裂成子团队并自适应地控制这些子团队以通过此类场景，同时保持编队。

Method: STAF方法基于一个统一的分层学习框架：1) 高层使用深度图切割进行团队分裂；2) 中间层使用图学习促进子团队间的协调导航；3) 低层使用策略学习控制单个移动机器人以达到目标位置并避免碰撞。

Result: 实验结果表明，STAF实现了子团队划分和自适应编队控制的新能力，并在通过挑战性场景的协调多机器人导航中取得了有前景的性能。该方法在室内外环境的机器人仿真和物理机器人团队中进行了广泛评估。

Conclusion: STAF成功地为多机器人团队在复杂环境中的协调导航提供了动态子团队划分和自适应编队控制的能力，有效解决了刚性编队在复杂场景下的限制。

Abstract: Coordinated multi-robot navigation is essential for robots to operate as a
team in diverse environments. During navigation, robot teams usually need to
maintain specific formations, such as circular formations to protect human
teammates at the center. However, in complex scenarios such as narrow
corridors, rigidly preserving predefined formations can become infeasible.
Therefore, robot teams must be capable of dynamically splitting into smaller
subteams and adaptively controlling the subteams to navigate through such
scenarios while preserving formations. To enable this capability, we introduce
a novel method for SubTeaming and Adaptive Formation (STAF), which is built
upon a unified hierarchical learning framework: (1) high-level deep graph cut
for team splitting, (2) intermediate-level graph learning for facilitating
coordinated navigation among subteams, and (3) low-level policy learning for
controlling individual mobile robots to reach their goal positions while
avoiding collisions. To evaluate STAF, we conducted extensive experiments in
both indoor and outdoor environments using robotics simulations and physical
robot teams. Experimental results show that STAF enables the novel capability
for subteaming and adaptive formation control, and achieves promising
performance in coordinated multi-robot navigation through challenging
scenarios. More details are available on the project website:
https://hcrlab.gitlab.io/project/STAF.

</details>


### [422] [End-to-end RL Improves Dexterous Grasping Policies](https://arxiv.org/abs/2509.16434)
*Ritvik Singh,Karl Van Wyk,Pieter Abbeel,Jitendra Malik,Nathan Ratliff,Ankur Handa*

Main category: cs.RO

TL;DR: 该研究提出了一种解耦模拟器和RL训练的方法，以扩展图像端到端灵巧抓取学习的规模，提高训练批次大小和真实世界性能，并通过深度蒸馏取得了更好的结果。


<details>
  <summary>Details</summary>
Motivation: 图像端到端强化学习（RL）在灵巧抓取中具有吸引力，因为它能实现主动视觉行为，但与基于状态的RL相比，它内存效率低下，导致批次大小小，不利于PPO等算法。现有模拟器使用传统数据并行技术在多GPU上扩展时存在关键瓶颈。

Method: 提出了一种新的方法，将模拟器和RL（包括训练和经验缓冲区）解耦并部署到不同的GPU上。例如，在一个有四个GPU的节点上，三个GPU运行模拟器，一个GPU运行PPO。此外，将深度策略和基于状态的策略蒸馏到立体RGB网络中。

Result: 在相同数量的GPU下，与传统数据并行基线相比，可将现有环境数量增加一倍。这使得能够端到端训练基于视觉且使用深度的环境，而这些环境之前在基线方法下表现不佳。深度蒸馏比基于状态的蒸馏在模拟和现实中都带来了更好的结果。解耦模拟带来的批次大小增加也改善了真实世界的性能。在现实世界部署中，使用提出的端到端策略在基于视觉的结果上超越了之前的最先进水平。

Conclusion: 解耦模拟器和RL训练的方法有效解决了图像端到端灵巧抓取学习的扩展性问题，显著提高了训练效率和真实世界性能。深度蒸馏策略优于基于状态的蒸馏，进一步提升了性能，这可能是由于消除了状态和视觉策略之间的可观测性差距。

Abstract: This work explores techniques to scale up image-based end-to-end learning for
dexterous grasping with an arm + hand system. Unlike state-based RL,
vision-based RL is much more memory inefficient, resulting in relatively low
batch sizes, which is not amenable for algorithms like PPO. Nevertheless, it is
still an attractive method as unlike the more commonly used techniques which
distill state-based policies into vision networks, end-to-end RL can allow for
emergent active vision behaviors. We identify a key bottleneck in training
these policies is the way most existing simulators scale to multiple GPUs using
traditional data parallelism techniques. We propose a new method where we
disaggregate the simulator and RL (both training and experience buffers) onto
separate GPUs. On a node with four GPUs, we have the simulator running on three
of them, and PPO running on the fourth. We are able to show that with the same
number of GPUs, we can double the number of existing environments compared to
the previous baseline of standard data parallelism. This allows us to train
vision-based environments, end-to-end with depth, which were previously
performing far worse with the baseline. We train and distill both depth and
state-based policies into stereo RGB networks and show that depth distillation
leads to better results, both in simulation and reality. This improvement is
likely due to the observability gap between state and vision policies which
does not exist when distilling depth policies into stereo RGB. We further show
that the increased batch size brought about by disaggregated simulation also
improves real world performance. When deploying in the real world, we improve
upon the previous state-of-the-art vision-based results using our end-to-end
policies.

</details>


### [423] [FiLM-Nav: Efficient and Generalizable Navigation via VLM Fine-tuning](https://arxiv.org/abs/2509.16445)
*Naoki Yokoyama,Sehoon Ha*

Main category: cs.RO

TL;DR: FiLM-Nav通过直接微调预训练的视觉语言模型（VLM）作为导航策略，在多样化的模拟具身数据上进行训练，实现了语义导航的新SOTA，并展现出强大的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 机器人助手在复杂环境中导航并根据自由形式语言定位物体是实际部署的关键能力。虽然基础模型（特别是VLM）提供了强大的语义理解能力，但如何有效地将它们的网络规模知识应用于具身决策仍然是一个主要挑战。

Method: 该研究提出了FiLM-Nav方法，它直接微调预训练的VLM作为导航策略。与仅以零样本方式或用于地图注释使用基础模型的方法不同，FiLM-Nav通过直接以原始视觉轨迹历史和导航目标为条件，学习选择最佳的下一个探索前沿。利用有针对性的模拟具身经验，使VLM能够将其强大的预训练表示与目标驱动导航相关的特定动态和视觉模式相结合。关键在于，通过结合ObjectNav、OVON、ImageNav和辅助空间推理任务的多元数据混合进行微调，对于实现鲁棒性和广泛泛化至关重要。

Result: FiLM-Nav在HM3D ObjectNav的开放词汇方法中，在SPL和成功率上都创造了新的SOTA，并在具有挑战性的HM3D-OVON基准测试中创造了SPL的SOTA，展示了对未见物体类别的强大泛化能力。

Conclusion: 该研究验证了直接在多样化的模拟具身数据上微调VLM，是实现可泛化和高效语义导航能力的有效途径。

Abstract: Enabling robotic assistants to navigate complex environments and locate
objects described in free-form language is a critical capability for real-world
deployment. While foundation models, particularly Vision-Language Models
(VLMs), offer powerful semantic understanding, effectively adapting their
web-scale knowledge for embodied decision-making remains a key challenge. We
present FiLM-Nav (Fine-tuned Language Model for Navigation), an approach that
directly fine-tunes pre-trained VLM as the navigation policy. In contrast to
methods that use foundation models primarily in a zero-shot manner or for map
annotation, FiLM-Nav learns to select the next best exploration frontier by
conditioning directly on raw visual trajectory history and the navigation goal.
Leveraging targeted simulated embodied experience allows the VLM to ground its
powerful pre-trained representations in the specific dynamics and visual
patterns relevant to goal-driven navigation. Critically, fine-tuning on a
diverse data mixture combining ObjectNav, OVON, ImageNav, and an auxiliary
spatial reasoning task proves essential for achieving robustness and broad
generalization. FiLM-Nav sets a new state-of-the-art in both SPL and success
rate on HM3D ObjectNav among open-vocabulary methods, and sets a
state-of-the-art SPL on the challenging HM3D-OVON benchmark, demonstrating
strong generalization to unseen object categories. Our work validates that
directly fine-tuning VLMs on diverse simulated embodied data is a highly
effective pathway towards generalizable and efficient semantic navigation
capabilities.

</details>


### [424] [A Framework for Optimal Ankle Design of Humanoid Robots](https://arxiv.org/abs/2509.16469)
*Guglielmo Cervettini,Roberto Mauceri,Alex Coppola,Fabio Bergonti,Luca Fiorio,Marco Maggiali,Daniele Pucci*

Main category: cs.RO

TL;DR: 本文提出了一种用于设计和评估人形机器人并联踝关节机构的统一方法，并通过多目标优化和标量成本函数对SPU和RSU两种代表性构型进行比较。结果表明，优化后的RSU构型显著优于现有串联设计和传统工程设计的RSU。


<details>
  <summary>Details</summary>
Motivation: 人形机器人踝关节的设计对于安全高效的地面交互至关重要。机械柔顺性和电机质量分布等关键因素促使并联机构成为主流选择。然而，最佳构型取决于执行器可用性和任务要求，因此需要一种统一的方法来设计和评估并联踝关节机构。

Method: 本文提出了一种统一的并联踝关节机构设计与评估方法。该方法采用多目标优化来合成机构几何结构，并使用一个聚合关键性能指标的标量成本函数来评估解决方案，以便进行跨架构比较。研究重点是球形-棱柱形-万向节（SPU）和旋转-球形-万向节（RSU）两种代表性架构。文章解析了两者的运动学，并为RSU引入了一种参数化方法，以确保工作空间可行性并加速优化。通过重新设计现有类人机器人的踝关节来验证该方法。

Result: 优化后的RSU构型始终优于原始的串联设计和传统工程设计的RSU，成本函数分别降低了高达41%和14%。

Conclusion: 所提出的统一设计和评估方法能够有效优化人形机器人并联踝关节机构。通过该方法，优化后的RSU构型在性能上显著超越了现有的串联设计和传统工程设计，为人形机器人的踝关节设计提供了更优的解决方案。

Abstract: The design of the humanoid ankle is critical for safe and efficient ground
interaction. Key factors such as mechanical compliance and motor mass
distribution have driven the adoption of parallel mechanism architectures.
However, selecting the optimal configuration depends on both actuator
availability and task requirements. We propose a unified methodology for the
design and evaluation of parallel ankle mechanisms. A multi-objective
optimization synthesizes the mechanism geometry, the resulting solutions are
evaluated using a scalar cost function that aggregates key performance metrics
for cross-architecture comparison. We focus on two representative
architectures: the Spherical-Prismatic-Universal (SPU) and the
Revolute-Spherical-Universal (RSU). For both, we resolve the kinematics, and
for the RSU, introduce a parameterization that ensures workspace feasibility
and accelerates optimization. We validate our approach by redesigning the ankle
of an existing humanoid robot. The optimized RSU consistently outperforms both
the original serial design and a conventionally engineered RSU, reducing the
cost function by up to 41% and 14%, respectively.

</details>


### [425] [Robot Conga: A Leader-Follower Walking Approach to Sequential Path Following in Multi-Agent Systems](https://arxiv.org/abs/2509.16482)
*Pranav Tiwari,Soumyodipta Nath*

Main category: cs.RO

TL;DR: 本文提出了一种名为“机器人康加舞”的领导者-跟随者控制策略，用于多智能体系统中的顺序路径跟踪，通过基于领导者空间位移而非时间来更新智能体的期望状态，实现了精确的轨迹跟踪和稳定的智能体间距。


<details>
  <summary>Details</summary>
Motivation: 传统的编队控制技术依赖于时间参数化轨迹和路径积分，这常导致同步问题和僵硬的行为。研究旨在解决顺序路径跟踪问题，即智能体沿共同轨迹保持固定空间分离，由中央控制下的领导者引导。

Method: 引入“机器人康加舞”的领导者-跟随者控制策略。该策略基于领导者的空间位移而非时间来更新每个智能体的期望状态。假设可访问全局位置参考，这在配备运动捕捉、基于视觉跟踪或UWB定位系统的室内环境中是有效的。

Result: 该算法在TurtleBot3和四足机器人（Laikago）的仿真中得到验证。结果表明，该策略实现了精确的轨迹跟踪、稳定的智能体间距和快速收敛。在四足机器人案例中，所有智能体在250个时间步（约0.25秒）内对齐；在TurtleBot3实施中，几乎是瞬间对齐。

Conclusion: “机器人康加舞”策略能够有效地解决多智能体系统的顺序路径跟踪问题，通过基于空间位移而非时间的方法，实现了高精度、高稳定性和快速收敛的控制效果。

Abstract: Coordinated path following in multi-agent systems is a key challenge in
robotics, with applications in automated logistics, surveillance, and
collaborative exploration. Traditional formation control techniques often rely
on time-parameterized trajectories and path integrals, which can result in
synchronization issues and rigid behavior. In this work, we address the problem
of sequential path following, where agents maintain fixed spatial separation
along a common trajectory, guided by a leader under centralized control. We
introduce Robot Conga, a leader-follower control strategy that updates each
agent's desired state based on the leader's spatial displacement rather than
time, assuming access to a global position reference, an assumption valid in
indoor environments equipped with motion capture, vision-based tracking, or UWB
localization systems. The algorithm was validated in simulation using both
TurtleBot3 and quadruped (Laikago) robots. Results demonstrate accurate
trajectory tracking, stable inter-agent spacing, and fast convergence, with all
agents aligning within 250 time steps (approx. 0.25 seconds) in the quadruped
case, and almost instantaneously in the TurtleBot3 implementation.

</details>


### [426] [Substrate-Timing-Independence for Meta-State Stability of Distributed Robotic Swarms](https://arxiv.org/abs/2509.16492)
*Tinapat Limsila,Mehul Sharma,Paulo Garcia*

Main category: cs.RO

TL;DR: 本文提出了一种基于并发进程演算（CSP）的正式方法，用于在基板时序不确定性下，识别和纠正机器人群设计中的故障元状态，确保其行为的稳定性。


<details>
  <summary>Details</summary>
Motivation: 分布式系统（特别是机器人群）由于时序不可预测性导致出现紧急故障元状态，且由于实现基板的变异性使问题更加严重。传统的经验验证成本过高，无法处理庞大的状态空间。

Method: 通过利用并发进程演算（具体为通信顺序进程，CSP），引入了一种方法，能够自动识别故障元状态的潜在原因，并纠正设计，使其元状态在时序变异性存在的情况下也能保持稳定，且与基板时序无关。

Result: 该方法在一个具有明确识别故障的机器人群上进行了评估，并在模拟和现实中都得到了验证。结果表明，在应用纠正之前，群会达到非法元状态；但在纠正之后，其行为始终正确。这支持了研究假设。

Conclusion: 所提出的技术可跨不同设计方法进行转移，为机器人学家的形式化方法工具箱做出了贡献，有助于在存在时序变异性的情况下，确保机器人群设计的元状态稳定。

Abstract: Emergent properties in distributed systems arise due to timing
unpredictability; asynchronous state evolution within each sub-system may lead
the macro-system to faulty meta-states. Empirical validation of correctness is
often prohibitively expensive, as the size of the state-space is too large to
be tractable. In robotic swarms this problem is exacerbated, when compared to
software systems, by the variability of the implementation substrate across the
design, or even the deployment, process. We present an approach for formally
reasoning about the correctness of robotic swarm design in a
substrate-timing-independent way. By leveraging concurrent process calculi
(namely, Communicating Sequential Processes), we introduce a methodology that
can automatically identify possible causes of faulty meta-states and correct
such designs such that meta-states are consistently stable, even in the
presence of timing variability due to substrate changes. We evaluate this
approach on a robotic swarm with a clearly identified fault, realized in both
simulation and reality. Results support the research hypothesis, showing that
the swarm reaches an illegal meta-state before the correction is applied, but
behaves consistently correctly after the correction. Our techniques are
transferable across different design methodologies, contributing to the toolbox
of formal methods for roboticists.

</details>


### [427] [No Need for Real 3D: Fusing 2D Vision with Pseudo 3D Representations for Robotic Manipulation Learning](https://arxiv.org/abs/2509.16532)
*Run Yu,Yangdi Liu,Wen-Da Wei,Chen Li*

Main category: cs.RO

TL;DR: 该论文提出了NoReal3D框架，通过将单目图像转换为具有几何意义的伪点云特征，并与2D特征融合，使机器人操作在无需实际3D点云数据的情况下，达到与3D点云方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管基于3D点云的机器人操作策略学习在性能和泛化性上优于2D图像方法，但其高昂的数据采集成本限制了扩展性和实际部署。为了在不牺牲性能的前提下解决这一挑战，需要一种无需实际3D点云数据的方法。

Method: 本文提出了NoReal3D框架，核心是3DStructureFormer模块。该模块能将单目图像转换为具有几何和拓扑结构的伪点云特征，并将其与2D编码器输出特征有效融合。为保留伪点云的几何和拓扑特性，设计了专门的伪点云编码器，并研究了不同的特征融合策略。

Result: 广泛的实验验证了NoReal3D框架在各种任务中，无需实际点云数据，即可达到与3D点云方法相当的性能。这显著增强了机器人对3D空间结构的理解，同时完全消除了3D点云采集的高昂成本。

Conclusion: NoReal3D框架通过从单目图像生成并利用伪点云特征，成功解决了3D点云数据采集成本高昂的问题，为机器人操作提供了一种成本效益高且性能接近3D点云方法的解决方案，提升了机器人对3D空间结构的理解。

Abstract: Recently,vision-based robotic manipulation has garnered significant attention
and witnessed substantial advancements. 2D image-based and 3D point cloud-based
policy learning represent two predominant paradigms in the field, with recent
studies showing that the latter consistently outperforms the former in terms of
both policy performance and generalization, thereby underscoring the value and
significance of 3D information. However, 3D point cloud-based approaches face
the significant challenge of high data acquisition costs, limiting their
scalability and real-world deployment. To address this issue, we propose a
novel framework NoReal3D: which introduces the 3DStructureFormer, a learnable
3D perception module capable of transforming monocular images into
geometrically meaningful pseudo-point cloud features, effectively fused with
the 2D encoder output features. Specially, the generated pseudo-point clouds
retain geometric and topological structures so we design a pseudo-point cloud
encoder to preserve these properties, making it well-suited for our framework.
We also investigate the effectiveness of different feature fusion
strategies.Our framework enhances the robot's understanding of 3D spatial
structures while completely eliminating the substantial costs associated with
3D point cloud acquisition.Extensive experiments across various tasks validate
that our framework can achieve performance comparable to 3D point cloud-based
methods, without the actual point cloud data.

</details>


### [428] [TranTac: Leveraging Transient Tactile Signals for Contact-Rich Robotic Manipulation](https://arxiv.org/abs/2509.16550)
*Yinghao Wu,Shuhong Hou,Haowen Zheng,Yichen Li,Weiyi Lu,Xun Zhou,Yitian Shao*

Main category: cs.RO

TL;DR: TranTac是一种数据高效、低成本的触觉感知与控制框架，通过在机械手夹持器尖端集成单个6轴惯性测量单元，并结合基于Transformer的编码器和扩散策略，实现了对细微插入任务的高精度控制，超越了纯视觉或力/扭矩传感方案。


<details>
  <summary>Details</summary>
Motivation: 在机器人精细操作任务（如插入钥匙或USB设备）中，当视觉感知不足以检测到未对准时，任务可能会失败。触觉传感对于监测任务状态和进行精确调整至关重要，但现有触觉解决方案要么不灵敏，要么需要过多的传感器数据。

Method: 本研究引入了TranTac框架，该框架将一个接触敏感的6轴惯性测量单元集成到机器人夹持器的弹性尖端。该定制传感系统能够检测微米级的动态平移和扭转变形，从而跟踪视觉上不可察觉的抓取物体姿态变化。通过利用基于Transformer的编码器和扩散策略，TranTac能够利用插入过程中夹持器尖端检测到的瞬态触觉线索来模仿人类的插入行为，并动态控制和纠正抓取物体的6自由度姿态。

Result: 当与视觉结合时，TranTac在物体抓取和插入任务中取得了79%的平均成功率，优于纯视觉策略和增强了末端执行器6D力/扭矩传感的策略。通过纯触觉未对准插入任务验证了接触定位性能，实现了88%的平均成功率。在泛化性测试中，TranTac在一个棱柱-槽对上进行训练，并在未见过的USB插头和金属钥匙上进行测试，插入任务的平均成功率接近70%。

Conclusion: 所提出的TranTac框架为精细操作任务提供了新的机器人触觉传感系统，展示了高成功率、数据效率和良好的泛化能力，有望启发未来机器人触觉传感系统的发展。

Abstract: Robotic manipulation tasks such as inserting a key into a lock or plugging a
USB device into a port can fail when visual perception is insufficient to
detect misalignment. In these situations, touch sensing is crucial for the
robot to monitor the task's states and make precise, timely adjustments.
Current touch sensing solutions are either insensitive to detect subtle changes
or demand excessive sensor data. Here, we introduce TranTac, a data-efficient
and low-cost tactile sensing and control framework that integrates a single
contact-sensitive 6-axis inertial measurement unit within the elastomeric tips
of a robotic gripper for completing fine insertion tasks. Our customized
sensing system can detect dynamic translational and torsional deformations at
the micrometer scale, enabling the tracking of visually imperceptible pose
changes of the grasped object. By leveraging transformer-based encoders and
diffusion policy, TranTac can imitate human insertion behaviors using transient
tactile cues detected at the gripper's tip during insertion processes. These
cues enable the robot to dynamically control and correct the 6-DoF pose of the
grasped object. When combined with vision, TranTac achieves an average success
rate of 79% on object grasping and insertion tasks, outperforming both
vision-only policy and the one augmented with end-effector 6D force/torque
sensing. Contact localization performance is also validated through
tactile-only misaligned insertion tasks, achieving an average success rate of
88%. We assess the generalizability by training TranTac on a single prism-slot
pair and testing it on unseen data, including a USB plug and a metal key, and
find that the insertion tasks can still be completed with an average success
rate of nearly 70%. The proposed framework may inspire new robotic tactile
sensing systems for delicate manipulation tasks.

</details>


### [429] [Video-to-BT: Generating Reactive Behavior Trees from Human Demonstration Videos for Robotic Assembly](https://arxiv.org/abs/2509.16611)
*Xiwei Zhao,Yiwei Wang,Yansong Wu,Fan Wu,Teng Sun,Zhonghua Miao,Sami Haddadin,Alois Knoll*

Main category: cs.RO

TL;DR: 本文提出了一种名为Video-to-BT的机器人装配分层框架，它利用视觉-语言模型（VLM）从人类演示视频中生成行为树（BTs），以实现高层规划和低层反应性控制，并通过VLM驱动的再规划确保在动态环境中的鲁棒性和适应性。


<details>
  <summary>Details</summary>
Motivation: 现代制造业需要更灵活、可靠的机器人装配系统。然而，传统的专家编程方法针对固定设置，缺乏对产品变化和环境波动的灵活性与鲁棒性。行为树虽然在机器人领域因其模块性和反应性而日益普及，但仍需一种能无缝整合高层认知规划与低层反应性控制的有效方法。

Method: 本文提出了Video-to-BT分层框架。该框架利用视觉-语言模型（VLM）将人类演示视频分解为子任务，并据此生成行为树（BTs）。这些行为树既作为规划的结构化输出，也作为执行的控制结构。在执行过程中，规划好的行为树结合实时场景解释，使系统能够在动态环境中反应性地操作。当执行失败时，系统会触发VLM驱动的再规划，形成一个闭环架构，以确保稳定性和适应性。

Result: 通过在真实世界装配任务中的一系列实验，该框架展现出高规划可靠性、在长周期装配任务中的鲁棒性能，以及在多样化和受扰动条件下的强大泛化能力。

Conclusion: Video-to-BT框架通过将VLM生成行为树与闭环再规划相结合，为机器人装配提供了一个灵活、可靠且适应性强的解决方案，有效应对了传统方法的局限性，并实现了高层规划与低层反应性控制的无缝集成。

Abstract: Modern manufacturing demands robotic assembly systems with enhanced
flexibility and reliability. However, traditional approaches often rely on
programming tailored to each product by experts for fixed settings, which are
inherently inflexible to product changes and lack the robustness to handle
variations. As Behavior Trees (BTs) are increasingly used in robotics for their
modularity and reactivity, we propose a novel hierarchical framework,
Video-to-BT, that seamlessly integrates high-level cognitive planning with
low-level reactive control, with BTs serving both as the structured output of
planning and as the governing structure for execution. Our approach leverages a
Vision-Language Model (VLM) to decompose human demonstration videos into
subtasks, from which Behavior Trees are generated. During the execution, the
planned BTs combined with real-time scene interpretation enable the system to
operate reactively in the dynamic environment, while VLM-driven replanning is
triggered upon execution failure. This closed-loop architecture ensures
stability and adaptivity. We validate our framework on real-world assembly
tasks through a series of experiments, demonstrating high planning reliability,
robust performance in long-horizon assembly tasks, and strong generalization
across diverse and perturbed conditions. Project website:
https://video2bt.github.io/video2bt_page/

</details>


### [430] [ORN-CBF: Learning Observation-conditioned Residual Neural Control Barrier Functions via Hypernetworks](https://arxiv.org/abs/2509.16614)
*Bojan Derajić,Sebastian Bernhard,Wolfgang Hönig*

Main category: cs.RO

TL;DR: 本文提出基于Hamilton-Jacobi (HJ) 可达性分析的观测条件神经控制障碍函数 (CBFs)，旨在解决现有学习型CBF在安全集次优、部分可观测环境适用性差及缺乏严格安全保障等问题。该方法利用HJ值函数特性和超网络架构，实现了近似最大安全集，并在仿真和硬件实验中展现出更高的成功率和更好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 控制障碍函数 (CBFs) 在自主系统安全关键控制中被证明有效，但其设计具有挑战性，促使学习型方法的发展。然而，现有的学习型CBF方法仍存在安全集次优、在部分可观测环境中适用性差以及缺乏严格安全保障等问题。

Method: 本文提出了一种基于观测条件的神经CBF方法，该方法基于Hamilton-Jacobi (HJ) 可达性分析，旨在近似恢复最大安全集。利用HJ值函数的特定数学性质，确保预测的安全集不会与观测到的故障集相交。此外，该方法采用基于超网络 (hypernetwork) 的架构，特别适用于设计观测条件的安全滤波器。

Result: 所提出的方法在地面机器人和四旋翼飞行器的仿真和硬件实验中进行了验证。结果显示，与基线方法相比，该方法实现了更高的成功率，并对域外环境展现出更好的泛化能力。

Conclusion: 本文提出的基于HJ可达性分析的观测条件神经CBF方法，有效解决了现有学习型CBF的局限性，通过确保近似最大安全集和利用超网络架构，显著提高了自主系统在安全关键控制中的性能、泛化能力和可靠性。

Abstract: Control barrier functions (CBFs) have been demonstrated as an effective
method for safety-critical control of autonomous systems. Although CBFs are
simple to deploy, their design remains challenging, motivating the development
of learning-based approaches. Yet, issues such as suboptimal safe sets,
applicability in partially observable environments, and lack of rigorous safety
guarantees persist. In this work, we propose observation-conditioned neural
CBFs based on Hamilton-Jacobi (HJ) reachability analysis, which approximately
recover the maximal safe sets. We exploit certain mathematical properties of
the HJ value function, ensuring that the predicted safe set never intersects
with the observed failure set. Moreover, we leverage a hypernetwork-based
architecture that is particularly suitable for the design of
observation-conditioned safety filters. The proposed method is examined both in
simulation and hardware experiments for a ground robot and a quadcopter. The
results show improved success rates and generalization to out-of-domain
environments compared to the baselines.

</details>


### [431] [LLM-Guided Task- and Affordance-Level Exploration in Reinforcement Learning](https://arxiv.org/abs/2509.16615)
*Jelle Luijkx,Runyu Ma,Zlatan Ajanović,Jens Kober*

Main category: cs.RO

TL;DR: LLM-TALE是一个框架，它利用大型语言模型（LLM）的规划能力来指导强化学习（RL）的探索，以解决机器人操作中RL样本效率低和LLM规划不可靠的问题。该框架在任务和功能层面进行规划，在线纠正次优规划，并在没有人工监督的情况下探索多模态功能级规划，显著提高了样本效率和成功率，并实现了良好的模拟到真实环境迁移。


<details>
  <summary>Details</summary>
Motivation: 强化学习（RL）在机器人操作中面临样本效率低和需要大量探索的问题。虽然大型语言模型（LLM）可以利用常识知识指导探索，但它们可能生成语义合理但物理上不可行的规划，导致行为不可靠。

Method: 本文提出了LLM-TALE框架。它通过LLM的规划直接引导RL探索，并在任务层面和功能层面集成规划，以将代理导向语义上有意义的动作，从而提高学习效率。与以往假设LLM生成规划或奖励是最佳的方法不同，LLM-TALE能够在线纠正次优性，并在没有人监督的情况下探索多模态的功能级规划。

Result: LLM-TALE在标准RL基准测试的抓取-放置任务中进行了评估，与强大的基线相比，它在样本效率和成功率方面均有所提高。真实机器人实验表明，该方法具有良好的零样本模拟到真实环境的迁移能力。

Conclusion: LLM-TALE通过将LLM规划与RL探索相结合，有效地解决了机器人操作中RL的样本效率问题以及LLM规划的可靠性问题。该框架不仅提高了学习性能，还展示了在真实世界应用中的潜力。

Abstract: Reinforcement learning (RL) is a promising approach for robotic manipulation,
but it can suffer from low sample efficiency and requires extensive exploration
of large state-action spaces. Recent methods leverage the commonsense knowledge
and reasoning abilities of large language models (LLMs) to guide exploration
toward more meaningful states. However, LLMs can produce plans that are
semantically plausible yet physically infeasible, yielding unreliable behavior.
We introduce LLM-TALE, a framework that uses LLMs' planning to directly steer
RL exploration. LLM-TALE integrates planning at both the task level and the
affordance level, improving learning efficiency by directing agents toward
semantically meaningful actions. Unlike prior approaches that assume optimal
LLM-generated plans or rewards, LLM-TALE corrects suboptimality online and
explores multimodal affordance-level plans without human supervision. We
evaluate LLM-TALE on pick-and-place tasks in standard RL benchmarks, observing
improvements in both sample efficiency and success rates over strong baselines.
Real-robot experiments indicate promising zero-shot sim-to-real transfer. Code
and supplementary material are available at https://llm-tale.github.io.

</details>


### [432] [KungfuBot2: Learning Versatile Motion Skills for Humanoid Whole-Body Control](https://arxiv.org/abs/2509.16638)
*Jinrui Han,Weiji Xie,Jiakun Zheng,Jiyuan Shi,Weinan Zhang,Ting Xiao,Chenjia Bai*

Main category: cs.RO

TL;DR: VMS是一种统一全身控制器，使人形机器人能通过混合跟踪目标、正交专家混合（OMoE）架构和分段跟踪奖励，学习多样化、动态的全身技能，实现长期稳定性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 通用人形机器人学习多样化全身技能并确保长时间序列稳定性是一个基本而具挑战性的任务，需要单个策略掌握广泛的运动能力。

Method: 本文提出了VMS框架，包含：1) 混合跟踪目标，平衡局部运动保真度与全局轨迹一致性；2) 正交专家混合（OMoE）架构，鼓励技能专业化同时增强泛化能力；3) 引入分段级跟踪奖励，放宽严格的逐步匹配，提高处理全局位移和瞬时不准确性时的鲁棒性。

Result: VMS在仿真和真实世界实验中都得到了广泛验证，结果表明其能准确模仿动态技能，在长达一分钟的序列中保持稳定性能，并对未见运动表现出强大的泛化能力。

Conclusion: 这些结果突显了VMS作为多功能人形机器人全身控制的可扩展基础的潜力。

Abstract: Learning versatile whole-body skills by tracking various human motions is a
fundamental step toward general-purpose humanoid robots. This task is
particularly challenging because a single policy must master a broad repertoire
of motion skills while ensuring stability over long-horizon sequences. To this
end, we present VMS, a unified whole-body controller that enables humanoid
robots to learn diverse and dynamic behaviors within a single policy. Our
framework integrates a hybrid tracking objective that balances local motion
fidelity with global trajectory consistency, and an Orthogonal
Mixture-of-Experts (OMoE) architecture that encourages skill specialization
while enhancing generalization across motions. A segment-level tracking reward
is further introduced to relax rigid step-wise matching, enhancing robustness
when handling global displacements and transient inaccuracies. We validate VMS
extensively in both simulation and real-world experiments, demonstrating
accurate imitation of dynamic skills, stable performance over minute-long
sequences, and strong generalization to unseen motions. These results highlight
the potential of VMS as a scalable foundation for versatile humanoid whole-body
control. The project page is available at
https://kungfubot2-humanoid.github.io.

</details>


### [433] [HDMI: Learning Interactive Humanoid Whole-Body Control from Human Videos](https://arxiv.org/abs/2509.16757)
*Haoyang Weng,Yitang Li,Nikhil Sobanbabu,Zihan Wang,Zhengyi Luo,Tairan He,Deva Ramanan,Guanya Shi*

Main category: cs.RO

TL;DR: HDMI是一个简单通用的框架，能从单目RGB视频中学习全身人形机器人与物体的交互技能，并在真实机器人上实现零样本部署。


<details>
  <summary>Details</summary>
Motivation: 由于运动数据稀缺和接触频繁的特性，实现鲁棒的全身人形机器人-物体交互（HOI）仍然具有挑战性。

Method: 该方法名为HDMI，其流程包括：(i) 从非受限视频中提取并重定向人类和物体轨迹，构建结构化运动数据集；(ii) 训练一个强化学习（RL）策略，通过统一的物体表示、残差动作空间和通用交互奖励来共同跟踪机器人和物体状态；(iii) 将RL策略零样本部署到真实人形机器人上。

Result: 通过在Unitree G1人形机器人上的大量模拟到真实实验，HDMI展示了其鲁棒性和通用性：实现了67次连续的开门通行，并在现实世界中成功执行了6种不同的运动操作任务，在模拟中执行了14种任务。

Conclusion: HDMI是一个简单通用的框架，能够从人类视频中获取交互式人形机器人技能。

Abstract: Enabling robust whole-body humanoid-object interaction (HOI) remains
challenging due to motion data scarcity and the contact-rich nature. We present
HDMI (HumanoiD iMitation for Interaction), a simple and general framework that
learns whole-body humanoid-object interaction skills directly from monocular
RGB videos. Our pipeline (i) extracts and retargets human and object
trajectories from unconstrained videos to build structured motion datasets,
(ii) trains a reinforcement learning (RL) policy to co-track robot and object
states with three key designs: a unified object representation, a residual
action space, and a general interaction reward, and (iii) zero-shot deploys the
RL policies on real humanoid robots. Extensive sim-to-real experiments on a
Unitree G1 humanoid demonstrate the robustness and generality of our approach:
HDMI achieves 67 consecutive door traversals and successfully performs 6
distinct loco-manipulation tasks in the real world and 14 tasks in simulation.
Our results establish HDMI as a simple and general framework for acquiring
interactive humanoid skills from human videos.

</details>


### [434] [Improve bounding box in Carla Simulator](https://arxiv.org/abs/2509.16773)
*Mohamad Mofeed Chaar,Jamal Raiyn,Galia Weidl*

Main category: cs.RO

TL;DR: 本文提出了一种改进的CARLA模拟器边界框生成方法，旨在过滤掉因遮挡导致的“幽灵框”等不必要的边界框，并实现了高精度。


<details>
  <summary>Details</summary>
Motivation: CARLA模拟器是自动驾驶算法测试和数据生成的重要平台。然而，其现有的边界框生成方法存在挑战，例如会产生“幽灵框”，即检测到被遮挡的物体，导致误报，影响数据质量和算法评估。

Method: 本文增强了CARLA模拟器中原有的边界框生成方法。在捕获地图上所有物体坐标、将其与自车传感器坐标系对齐并生成边界框之后，增加了一个过滤步骤，以剔除因遮挡而产生的“幽灵框”等不必要的框。

Result: 性能分析表明，改进后的方法实现了高精度。

Conclusion: 改进后的CARLA边界框生成方法能有效过滤不必要的“幽灵框”，显著提高了数据标注的准确性，从而为自动驾驶算法的开发和测试提供了更可靠的数据。

Abstract: The CARLA simulator (Car Learning to Act) serves as a robust platform for
testing algorithms and generating datasets in the field of Autonomous Driving
(AD). It provides control over various environmental parameters, enabling
thorough evaluation. Development bounding boxes are commonly utilized tools in
deep learning and play a crucial role in AD applications. The predominant
method for data generation in the CARLA Simulator involves identifying and
delineating objects of interest, such as vehicles, using bounding boxes. The
operation in CARLA entails capturing the coordinates of all objects on the map,
which are subsequently aligned with the sensor's coordinate system at the ego
vehicle and then enclosed within bounding boxes relative to the ego vehicle's
perspective. However, this primary approach encounters challenges associated
with object detection and bounding box annotation, such as ghost boxes.
Although these procedures are generally effective at detecting vehicles and
other objects within their direct line of sight, they may also produce false
positives by identifying objects that are obscured by obstructions. We have
enhanced the primary approach with the objective of filtering out unwanted
boxes. Performance analysis indicates that the improved approach has achieved
high accuracy.

</details>


### [435] [SMART-3D: Three-Dimensional Self-Morphing Adaptive Replanning Tree](https://arxiv.org/abs/2509.16812)
*Priyanshu Agrawal,Shalabh Gupta,Zongyuan Shen*

Main category: cs.RO

TL;DR: 本文提出SMART-3D，是SMART算法在3D环境下的扩展，它是一种基于树的自适应重规划算法，适用于有快速移动障碍物的动态环境，通过热节点实现高效的实时路径重规划。


<details>
  <summary>Details</summary>
Motivation: 现有路径规划算法难以应对动态环境中快速移动障碍物带来的实时重规划需求，尤其是在3D环境下。

Method: SMART-3D是SMART算法的3D扩展，它是一种基于树的自适应重规划算法。当当前路径被障碍物阻挡时，SMART-3D会变形底层树以实时找到新路径。它通过用“热节点”（hot-nodes）取代“热点”（hot-spots）的概念，消除了SMART算法对网格分解的要求，从而提高了计算效率和3D环境的可扩展性。热节点允许高效地重新连接以变形现有树，从而找到新的安全可靠路径。

Result: 在2D和3D环境中对随机移动动态障碍物进行的广泛仿真评估表明，SMART-3D实现了高成功率和低重规划时间。

Conclusion: SMART-3D的高成功率和低重规划时间证明了其适用于实时车载应用。

Abstract: This paper presents SMART-3D, an extension of the SMART algorithm to 3D
environments. SMART-3D is a tree-based adaptive replanning algorithm for
dynamic environments with fast moving obstacles. SMART-3D morphs the underlying
tree to find a new path in real-time whenever the current path is blocked by
obstacles. SMART-3D removed the grid decomposition requirement of the SMART
algorithm by replacing the concept of hot-spots with that of hot-nodes, thus
making it computationally efficient and scalable to 3D environments. The
hot-nodes are nodes which allow for efficient reconnections to morph the
existing tree to find a new safe and reliable path. The performance of SMART-3D
is evaluated by extensive simulations in 2D and 3D environments populated with
randomly moving dynamic obstacles. The results show that SMART-3D achieves high
success rates and low replanning times, thus highlighting its suitability for
real-time onboard applications.

</details>


### [436] [Factorizing Diffusion Policies for Observation Modality Prioritization](https://arxiv.org/abs/2509.16830)
*Omkar Patil,Prabin Rath,Kartikay Pangaonkar,Eric Rosen,Nakul Gopalan*

Main category: cs.RO

TL;DR: 现有扩散模型在机器人技能学习中未能有效捕捉不同观测模态（如视觉、触觉）对任务影响的差异。本文提出“分解扩散策略”（FDP），通过因子化观测条件，使模态能被优先考虑，从而在低数据量和分布偏移下显著提升性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 机器人技能学习中的扩散策略通常同时依赖多种观测模态，但这些模态对不同任务的影响程度各异。现有方法未能有效捕捉这种差异，导致性能欠佳且缺乏鲁棒性。

Method: 本文提出“分解扩散策略”（FDP），这是一种新颖的策略公式，通过设计使观测模态对动作扩散过程具有不同的影响力。FDP通过因子化扩散过程的观测条件，实现了模态的优先级排序（例如，视觉优先于触觉），从而学习到性能更优、更鲁棒的策略。

Result: 在低数据量场景下，FDP在多个模拟基准测试中比标准扩散策略（联合条件化所有输入模态）的成功率提高了15%。此外，在视觉干扰或摄像头遮挡等分布偏移下，FDP在多个视觉运动任务中的绝对成功率提高了40%，而现有扩散策略在此类情况下会严重失败。

Conclusion: FDP为机器人技能学习提供了一种比标准扩散策略更具性能、更鲁棒、更安全的替代方案，特别适用于真实世界的部署，尤其是在低数据量和存在分布偏移的场景下。

Abstract: Diffusion models have been extensively leveraged for learning robot skills
from demonstrations. These policies are conditioned on several observational
modalities such as proprioception, vision and tactile. However, observational
modalities have varying levels of influence for different tasks that diffusion
polices fail to capture. In this work, we propose 'Factorized Diffusion
Policies' abbreviated as FDP, a novel policy formulation that enables
observational modalities to have differing influence on the action diffusion
process by design. This results in learning policies where certain observations
modalities can be prioritized over the others such as $\texttt{vision>tactile}$
or $\texttt{proprioception>vision}$. FDP achieves modality prioritization by
factorizing the observational conditioning for diffusion process, resulting in
more performant and robust policies. Our factored approach shows strong
performance improvements in low-data regimes with $15\%$ absolute improvement
in success rate on several simulated benchmarks when compared to a standard
diffusion policy that jointly conditions on all input modalities. Moreover, our
benchmark and real-world experiments show that factored policies are naturally
more robust with $40\%$ higher absolute success rate across several visuomotor
tasks under distribution shifts such as visual distractors or camera
occlusions, where existing diffusion policies fail catastrophically. FDP thus
offers a safer and more robust alternative to standard diffusion policies for
real-world deployment. Videos are available at
https://fdp-policy.github.io/fdp-policy/ .

</details>


### [437] [Robot Learning with Sparsity and Scarcity](https://arxiv.org/abs/2509.16834)
*Jingxi Xu*

Main category: cs.RO

TL;DR: 本文探讨了机器人学习中数据稀疏性（以触觉传感为例）和数据匮乏（以康复机器人为例）的挑战，并提出了相应的机器学习方法来有效利用有限数据。


<details>
  <summary>Details</summary>
Motivation: 机器人学习面临着与语言或视觉领域不同的大规模数据资源匮乏的根本挑战。具体可分为数据表示角度的数据稀疏性和数据量角度的数据匮乏。

Method: 对于触觉传感，采用无模型强化学习来学习无视觉的纯触觉探索和操作策略。对于康复机器人，开发了包括半监督学习、元学习和生成式AI方法在内的机器学习算法，利用少量生物信号推断中风幸存者的意图。

Result: 在触觉传感方面，实现了对稀疏触觉信息的有效利用，进行探索和操作。在康复机器人方面，通过少量数据实现了对中风幸存者活动意图的推断，以便矫形器能在正确时机提供适当的物理辅助。

Conclusion: 该论文通过触觉传感和康复机器人这两个具体应用，展示了如何应对机器人学习中数据稀疏性和匮乏的挑战，开发了能够高效利用有限数据的机器学习算法。

Abstract: Unlike in language or vision, one of the fundamental challenges in robot
learning is the lack of access to vast data resources. We can further break
down the problem into (1) data sparsity from the angle of data representation
and (2) data scarcity from the angle of data quantity. In this thesis, I will
discuss selected works on two domains: (1) tactile sensing and (2)
rehabilitation robots, which are exemplars of data sparsity and scarcity,
respectively. Tactile sensing is an essential modality for robotics, but
tactile data are often sparse, and for each interaction with the physical
world, tactile sensors can only obtain information about the local area of
contact. I will discuss my work on learning vision-free tactile-only
exploration and manipulation policies through model-free reinforcement learning
to make efficient use of sparse tactile information. On the other hand,
rehabilitation robots are an example of data scarcity to the extreme due to the
significant challenge of collecting biosignals from disabled-bodied subjects at
scale for training. I will discuss my work in collaboration with the medical
school and clinicians on intent inferral for stroke survivors, where a hand
orthosis developed in our lab collects a set of biosignals from the patient and
uses them to infer the activity that the patient intends to perform, so the
orthosis can provide the right type of physical assistance at the right moment.
My work develops machine learning algorithms that enable intent inferral with
minimal data, including semi-supervised, meta-learning, and generative AI
methods.

</details>


### [438] [Benchmarking Offline Reinforcement Learning for Emotion-Adaptive Social Robotics](https://arxiv.org/abs/2509.16858)
*Soon Jynn Chu,Raju Gottumukkala,Alan Barhorst*

Main category: cs.RO

TL;DR: 本研究探讨了将离线强化学习应用于情感自适应社交机器人，以克服在线强化学习成本高昂和不安全的缺点。通过在有限数据集上建立基准测试，发现BCQ和CQL算法在数据稀疏性下表现更优，为未来人机交互（HRI）中的部署奠定基础。


<details>
  <summary>Details</summary>
Motivation: 社交机器人响应人类情感对于建立信任至关重要。然而，通过在线强化学习开发这种能力往往不切实际，因为数据收集成本过高且存在产生不安全行为的风险。

Method: 本研究提出了一种将多模态感知与识别、决策和自适应响应相结合的系统架构，并采用离线强化学习作为替代方案。使用来自人机游戏场景的有限数据集，建立了离线强化学习算法（BCQ、CQL、NFQ、DQN和DDQN）的基准测试。

Result: 结果表明，BCQ和CQL算法对数据稀疏性更具鲁棒性，与NFQ、DQN和DDQN相比，它们实现了更高的状态-动作值。

Conclusion: 这项工作为情感自适应机器人中的离线强化学习基准测试奠定了基础，并为未来在真实世界人机交互（如对话代理、教育伙伴和个人助理）中的部署提供了经验性见解。

Abstract: The ability of social robots to respond to human emotions is crucial for
building trust and acceptance in human-robot collaborative environments.
However, developing such capabilities through online reinforcement learning is
sometimes impractical due to the prohibitive cost of data collection and the
risk of generating unsafe behaviors. In this paper, we study the use of offline
reinforcement learning as a practical and efficient alternative. This technique
uses pre-collected data to enable emotion-adaptive social robots. We present a
system architecture that integrates multimodal sensing and recognition,
decision-making, and adaptive responses. Using a limited dataset from a
human-robot game-playing scenario, we establish a benchmark for comparing
offline reinforcement learning algorithms that do not require an online
environment. Our results show that BCQ and CQL are more robust to data
sparsity, achieving higher state-action values compared to NFQ, DQN, and DDQN.
This work establishes a foundation for benchmarking offline RL in
emotion-adaptive robotics and informs future deployment in real-world HRI. Our
findings provide empirical insight into the performance of offline
reinforcement learning algorithms in data-constrained HRI. This work
establishes a foundation for benchmarking offline RL in emotion-adaptive
robotics and informs its future deployment in real-world HRI, such as in
conversational agents, educational partners, and personal assistants, require
reliable emotional responsiveness.

</details>


### [439] [HOGraspFlow: Exploring Vision-based Generative Grasp Synthesis with Hand-Object Priors and Taxonomy Awareness](https://arxiv.org/abs/2509.16871)
*Yitian Shi,Zicheng Guo,Rosa Wolf,Edgar Welte,Rania Rayyes*

Main category: cs.RO

TL;DR: 该论文提出了Hand-Object(HO)GraspFlow，一种以可供性为中心的方法，能够将包含手与物体交互（HOI）的单一RGB图像重定向为多模态可执行的平行夹爪抓取姿态，且无需目标物体的显式几何先验。


<details>
  <summary>Details</summary>
Motivation: 现有抓取合成方法可能需要目标物体的显式几何先验，这限制了其在真实世界场景中的应用。该研究旨在开发一种无需此类几何先验，仅从RGB图像和HOI信息就能生成高保真、多模态抓取姿态的方法。

Method: 该方法构建于手部重建和视觉基础模型之上，使用去噪流匹配（FM）来合成SE(3)抓取姿态。合成过程基于三个互补的条件线索：作为视觉语义的RGB基础特征、HOI接触重建，以及抓取类型上的分类学先验。该方法是目标无关的。

Result: HOGraspFlow在没有显式HOI接触输入或物体几何信息的情况下，实现了高保真的抓取合成，并保持了强大的接触和分类学识别能力。与基于扩散的变体（HOGraspDiff）相比，HOGraspFlow表现更优，在SE(3)中实现了更高的分布保真度和更稳定的优化。在真实世界实验中，该方法从人类演示中实现了可靠、目标无关的抓取合成，平均成功率超过83%。

Conclusion: HOGraspFlow提供了一种可靠且目标无关的抓取合成方法，能够从人类演示中生成高保真的抓取姿态，克服了对显式几何先验的依赖，并在性能上优于扩散基线模型。

Abstract: We propose Hand-Object\emph{(HO)GraspFlow}, an affordance-centric approach
that retargets a single RGB with hand-object interaction (HOI) into multi-modal
executable parallel jaw grasps without explicit geometric priors on target
objects. Building on foundation models for hand reconstruction and vision, we
synthesize $SE(3)$ grasp poses with denoising flow matching (FM), conditioned
on the following three complementary cues: RGB foundation features as visual
semantics, HOI contact reconstruction, and taxonomy-aware prior on grasp types.
Our approach demonstrates high fidelity in grasp synthesis without explicit HOI
contact input or object geometry, while maintaining strong contact and taxonomy
recognition. Another controlled comparison shows that \emph{HOGraspFlow}
consistently outperforms diffusion-based variants (\emph{HOGraspDiff}),
achieving high distributional fidelity and more stable optimization in $SE(3)$.
We demonstrate a reliable, object-agnostic grasp synthesis from human
demonstrations in real-world experiments, where an average success rate of over
$83\%$ is achieved.

</details>


### [440] [End2Race: Efficient End-to-End Imitation Learning for Real-Time F1Tenth Racing](https://arxiv.org/abs/2509.16894)
*Zhijie Qiao,Haowei Li,Zhong Cao,Henry X. Liu*

Main category: cs.RO

TL;DR: 本文提出了End2Race，一种新颖的端到端模仿学习算法，专为F1Tenth自动驾驶竞速设计，通过GRU和基于Sigmoid的LiDAR归一化，实现了高安全性和超车成功率，并具有极低的推理延迟。


<details>
  <summary>Details</summary>
Motivation: F1Tenth自动驾驶竞速平台需要与传统自动驾驶不同的算法，因为其操作速度快、环境动态且存在头对头交互。此外，在高速下进行快速决策的需求严重限制了模型容量，使得训练此类算法极具挑战性。

Method: 本文提出了End2Race，一种端到端模仿学习算法。它利用门控循环单元（GRU）架构来捕捉连续的时间依赖性，实现短期响应和长期战略规划。此外，还采用了一种基于Sigmoid的归一化函数，将原始LiDAR扫描转换为空间压力令牌，以促进有效的模型训练和收敛。

Result: End2Race算法效率极高，在消费级GPU上推理时间小于0.5毫秒。在F1Tenth模拟器中的实验表明，在2,400个超车场景（每个场景限时8秒）中，它实现了94.2%的安全率，并成功完成了59.2%的超车。这些结果超越了现有方法。

Conclusion: End2Race算法在F1Tenth竞速测试平台中表现出色，在安全性和超车成功率方面均超越了现有方法，成为领先的解决方案，同时保持了高效率。

Abstract: F1Tenth is a widely adopted reduced-scale platform for developing and testing
autonomous racing algorithms, hosting annual competitions worldwide. With high
operating speeds, dynamic environments, and head-to-head interactions,
autonomous racing requires algorithms that diverge from those in classical
autonomous driving. Training such algorithms is particularly challenging: the
need for rapid decision-making at high speeds severely limits model capacity.
To address this, we propose End2Race, a novel end-to-end imitation learning
algorithm designed for head-to-head autonomous racing. End2Race leverages a
Gated Recurrent Unit (GRU) architecture to capture continuous temporal
dependencies, enabling both short-term responsiveness and long-term strategic
planning. We also adopt a sigmoid-based normalization function that transforms
raw LiDAR scans into spatial pressure tokens, facilitating effective model
training and convergence. The algorithm is extremely efficient, achieving an
inference time of less than 0.5 milliseconds on a consumer-class GPU.
Experiments in the F1Tenth simulator demonstrate that End2Race achieves a 94.2%
safety rate across 2,400 overtaking scenarios, each with an 8-second time
limit, and successfully completes overtakes in 59.2% of cases. This surpasses
previous methods and establishes ours as a leading solution for the F1Tenth
racing testbed. Code is available at
https://github.com/michigan-traffic-lab/End2Race.

</details>


### [441] [SwarmChat: An LLM-Based, Context-Aware Multimodal Interaction System for Robotic Swarms](https://arxiv.org/abs/2509.16920)
*Ettilla Mohiuddin Eumi,Hussein Abbass,Nadine Marcus*

Main category: cs.RO

TL;DR: SwarmChat是一个由大型语言模型（LLM）驱动的上下文感知多模态人机群交互（HSI）系统，旨在通过自然语言和多种交互方式，提高决策速度、降低认知负荷并增强指令灵活性。


<details>
  <summary>Details</summary>
Motivation: 传统的人机群交互方法缺乏直观的实时自适应界面，导致决策缓慢、认知负荷增加，并限制了指令的灵活性。为了解决这些问题，研究者提出了SwarmChat。

Method: SwarmChat是一个上下文感知、多模态的交互系统，由大型语言模型（LLM）驱动。它允许用户使用文本、语音或远程操作等多种模态向机器人群发布自然语言指令。系统集成了四个基于LLM的模块：上下文生成器、意图识别器、任务规划器和模态选择器。这些模块协同工作，从关键词生成上下文、检测用户意图、根据实时机器人状态调整指令，并建议最佳通信模态。其三层架构提供了一个动态界面，包含固定和可定制的指令选项，支持灵活控制并优化认知努力。

Result: 初步评估显示，SwarmChat的LLM模块提供了准确的上下文解释、相关的意图识别和有效的指令交付，实现了高用户满意度。

Conclusion: SwarmChat通过其基于LLM的上下文感知和多模态交互方法，有效解决了传统人机群交互中界面不直观、决策缓慢和指令灵活性不足的问题，显著提升了用户体验和控制效率。

Abstract: Traditional Human-Swarm Interaction (HSI) methods often lack intuitive
real-time adaptive interfaces, making decision making slower and increasing
cognitive load while limiting command flexibility. To solve this, we present
SwarmChat, a context-aware, multimodal interaction system powered by Large
Language Models (LLMs). SwarmChat enables users to issue natural language
commands to robotic swarms using multiple modalities, such as text, voice, or
teleoperation. The system integrates four LLM-based modules: Context Generator,
Intent Recognition, Task Planner, and Modality Selector. These modules
collaboratively generate context from keywords, detect user intent, adapt
commands based on real-time robot state, and suggest optimal communication
modalities. Its three-layer architecture offers a dynamic interface with both
fixed and customizable command options, supporting flexible control while
optimizing cognitive effort. The preliminary evaluation also shows that the
SwarmChat's LLM modules provide accurate context interpretation, relevant
intent recognition, and effective command delivery, achieving high user
satisfaction.

</details>


### [442] [A Reliable Robot Motion Planner in Complex Real-world Environments via Action Imagination](https://arxiv.org/abs/2509.16963)
*Chengjin Wang,Yanmin Zhou,Zhipeng Wang,Zheng Yan,Feng Luan,Shuo Jiang,Runjie Shen,Hongrui Sang,Bin He*

Main category: cs.RO

TL;DR: 本研究提出了一种受动物智能启发的“想象力运动规划器”（I-MP）框架，通过想象可能的空间状态来提高机器人在未知复杂环境中的动作可靠性。


<details>
  <summary>Details</summary>
Motivation: 人类和动物能够通过想象行动结果来实时调整运动，以防止在未知环境中出现意外或灾难性的运动失败。受动物智能的这种“行动意识”能力启发，研究旨在为机器人开发类似的策略，以增强其动作可靠性。

Method: I-MP框架通过拓扑化工作空间，建立感知-行动循环使机器人自主构建接触模型。利用不动点理论和豪斯多夫距离，规划器计算出在交互特性和任务约束下的收敛空间状态。通过“功”来均匀表示多维环境特征，机器人通过实时计算能量梯度来接近想象的空间状态。

Result: 实验结果证明了I-MP在复杂杂乱环境中的实用性和鲁棒性。

Conclusion: I-MP框架成功地将受生物启发的行动想象能力应用于机器人运动规划，显著提高了机器人在复杂环境中的动作可靠性和鲁棒性。

Abstract: Humans and animals can make real-time adjustments to movements by imagining
their action outcomes to prevent unanticipated or even catastrophic motion
failures in unknown unstructured environments. Action imagination, as a refined
sensorimotor strategy, leverages perception-action loops to handle physical
interaction-induced uncertainties in perception and system modeling within
complex systems. Inspired by the action-awareness capability of animal
intelligence, this study proposes an imagination-inspired motion planner (I-MP)
framework that specifically enhances robots' action reliability by imagining
plausible spatial states for approaching. After topologizing the workspace,
I-MP build perception-action loop enabling robots autonomously build contact
models. Leveraging fixed-point theory and Hausdorff distance, the planner
computes convergent spatial states under interaction characteristics and
mission constraints. By homogenously representing multi-dimensional
environmental characteristics through work, the robot can approach the imagined
spatial states via real-time computation of energy gradients. Consequently,
experimental results demonstrate the practicality and robustness of I-MP in
complex cluttered environments.

</details>


### [443] [Geometric Interpolation of Rigid Body Motions](https://arxiv.org/abs/2509.16966)
*Andreas Mueller*

Main category: cs.RO

TL;DR: 本文研究了刚体运动的插值问题，提出了两种变体（k阶初值和边值轨迹插值问题），并给出了k=1-4的初值问题解以及一种新颖的立方插值解，可用于给定初始和终端扭曲的运动。


<details>
  <summary>Details</summary>
Motivation: 在给定初始和终端姿态之间找到一条空间轨迹是刚体运动插值的核心问题。研究动机在于需要满足关于刚体扭曲及其导数的不同初始条件或边界条件来生成更平滑、更精确的轨迹。

Method: 文章提出了两种插值问题：k阶初值轨迹插值问题（k-IV-TIP），即满足刚体扭曲的k-1阶导数初始条件；k阶边值轨迹插值问题（k-BV-TIP），即满足初始和终端姿态处扭曲及其k-1阶导数的条件。研究方法包括推导k=1到4的k-IV-TIP解，以及1-IV-TBP（给定初始和终端扭曲）的解，并提出了一种推导更高阶解的通用方法。

Result: 研究结果包括：1) 针对k=1到4的k-IV-TIP（即初始扭曲及其最高到四阶时间导数已知）的解决方案；2) 一种新颖的立方插值方案，用于解决1-IV-TBP问题（即给定初始和终端扭曲在两个空间配置之间进行插值）。当扭曲设为零时，这种立方插值会自动等同于最小加速度曲线。此外，还展示了推导更高阶解的通用方法，并通过两个数值示例验证了结果。

Conclusion: 本文为具有不同初始和边界条件的刚体运动插值问题提供了系统的解决方案，特别是提出了一种在给定初始和终端扭曲之间进行插值的新型立方方法，该方法在特定条件下可简化为最小加速度曲线。这些方法为生成满足高阶导数要求的平滑刚体轨迹提供了理论基础和实用工具。

Abstract: The problem of interpolating a rigid body motion is to find a spatial
trajectory between a prescribed initial and terminal pose. Two variants of this
interpolation problem are addressed. The first is to find a solution that
satisfies initial conditions on the k-1 derivatives of the rigid body twist.
This is called the kth-order initial value trajectory interpolation problem
(k-IV-TIP). The second is to find a solution that satisfies conditions on the
rigid body twist and its k-1 derivatives at the initial and terminal pose. This
is called the kth-order boundary value trajectory interpolation problem
(k-BV-TIP). Solutions to the k-IV-TIP for k=1,...,4, i.e. the initial twist and
up to the 4th time derivative are prescribed. Further, a solution to the
1-IV-TBP is presented, i.e. the initial and terminal twist are prescribed. The
latter is a novel cubic interpolation between two spatial configurations with
given initial and terminal twist. This interpolation is automatically identical
to the minimum acceleration curve when the twists are set to zero. The general
approach to derive higher-order solutions is presented. Numerical results are
shown for two examples.

</details>


### [444] [IDfRA: Self-Verification for Iterative Design in Robotic Assembly](https://arxiv.org/abs/2509.16998)
*Nishka Khendry,Christos Margadji,Sebastian W. Pattinson*

Main category: cs.RO

TL;DR: 本文提出IDfRA框架，通过规划、执行、验证和重新规划的迭代循环，利用自我评估和真实世界反馈，逐步提高机器人装配设计质量，无需僵硬的物理模拟器，实现了语义忠实度和物理可行性的结合。


<details>
  <summary>Details</summary>
Motivation: 传统机器人装配设计（DfRA）依赖手动规划，耗时、昂贵且不适用于复杂对象。现有LLM驱动的DfRA方法依赖启发式策略和僵硬的物理模拟器，难以应用于实际装配场景。

Method: IDfRA（Iterative Design for Robotic Assembly）框架。它采用规划、执行、验证和重新规划的迭代循环，每个环节都通过自我评估进行信息反馈，在固定但初始未充分指定的环境中逐步提升设计质量，并以真实世界代替物理模拟。输入为目标结构和部分环境表示。

Result: IDfRA在语义可识别性方面达到73.3%的top-1准确率，超越基线。生成的装配方案具有86.9%的整体构建成功率，展现出强大的物理可行性，设计质量在迭代中有所提升。人类评估也证实了IDfRA的优势。

Conclusion: IDfRA通过整合自我验证和上下文感知适应，展示了在非结构化制造场景中部署的巨大潜力，有效解决了语义忠实度与物理可行性之间的平衡问题。

Abstract: As robots proliferate in manufacturing, Design for Robotic Assembly (DfRA),
which is designing products for efficient automated assembly, is increasingly
important. Traditional approaches to DfRA rely on manual planning, which is
time-consuming, expensive and potentially impractical for complex objects.
Large language models (LLM) have exhibited proficiency in semantic
interpretation and robotic task planning, stimulating interest in their
application to the automation of DfRA. But existing methodologies typically
rely on heuristic strategies and rigid, hard-coded physics simulators that may
not translate into real-world assembly contexts. In this work, we present
Iterative Design for Robotic Assembly (IDfRA), a framework using iterative
cycles of planning, execution, verification, and re-planning, each informed by
self-assessment, to progressively enhance design quality within a fixed yet
initially under-specified environment, thereby eliminating the physics
simulation with the real world itself. The framework accepts as input a target
structure together with a partial environmental representation. Through
successive refinement, it converges toward solutions that reconcile semantic
fidelity with physical feasibility. Empirical evaluation demonstrates that
IDfRA attains 73.3\% top-1 accuracy in semantic recognisability, surpassing the
baseline on this metric. Moreover, the resulting assembly plans exhibit robust
physical feasibility, achieving an overall 86.9\% construction success rate,
with design quality improving across iterations, albeit not always
monotonically. Pairwise human evaluation further corroborates the advantages of
IDfRA relative to alternative approaches. By integrating self-verification with
context-aware adaptation, the framework evidences strong potential for
deployment in unstructured manufacturing scenarios.

</details>


### [445] [Generalized Momenta-Based Koopman Formalism for Robust Control of Euler-Lagrangian Systems](https://arxiv.org/abs/2509.17010)
*Rajpal Singh,Aditya Singh,Chidre Shravista Kashyap,Jishnu Keshavan*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的 Koopman 算子公式，用于欧拉-拉格朗日动力学，通过隐式广义动量状态空间表示，解耦了线性执行通道，从而简化了学习过程，降低了模型复杂度，并提高了预测性能和效率。


<details>
  <summary>Details</summary>
Motivation: 传统的 Koopman 公式在处理欧拉-拉格朗日动力学时，输入与状态相关项非线性耦合，导致需要更复杂的双线性 Koopman 模型，这些模型在训练和部署时计算成本高昂。

Method: 该研究采用了一种基于隐式广义动量的状态空间表示，将已知线性执行通道与状态相关动力学解耦。这种结构分离使得模型只需学习未执行的动力学。为此，论文提出了两种构建 Koopman 嵌入的神经网络架构。此外，集成了线性广义扩展状态观测器（GESO）以实时估计和补偿扰动，确保鲁棒性。

Result: 所提出的方法显著减少了可学习参数，提高了数据效率，降低了模型整体复杂性。它能够构建线性模型，其预测性能优于传统的双线性模型，同时效率更高。通过对机器人机械手的轨迹跟踪仿真和实验验证，该框架在准确性、鲁棒性和学习效率方面均优于现有先进替代方案。

Conclusion: 结合动量基 Koopman 和 GESO 的框架为欧拉-拉格朗日动力学提供了一种更高效、准确和鲁棒的建模和控制方法，尤其适用于机器人操纵器，且在性能上超越了现有技术。

Abstract: This paper presents a novel Koopman operator formulation for Euler Lagrangian
dynamics that employs an implicit generalized momentum-based state space
representation, which decouples a known linear actuation channel from state
dependent dynamics and makes the system more amenable to linear Koopman
modeling. By leveraging this structural separation, the proposed formulation
only requires to learn the unactuated dynamics rather than the complete
actuation dependent system, thereby significantly reducing the number of
learnable parameters, improving data efficiency, and lowering overall model
complexity. In contrast, conventional explicit formulations inherently couple
inputs with the state dependent terms in a nonlinear manner, making them more
suitable for bilinear Koopman models, which are more computationally expensive
to train and deploy. Notably, the proposed scheme enables the formulation of
linear models that achieve superior prediction performance compared to
conventional bilinear models while remaining substantially more efficient. To
realize this framework, we present two neural network architectures that
construct Koopman embeddings from actuated or unactuated data, enabling
flexible and efficient modeling across different tasks. Robustness is ensured
through the integration of a linear Generalized Extended State Observer (GESO),
which explicitly estimates disturbances and compensates for them in real time.
The combined momentum-based Koopman and GESO framework is validated through
comprehensive trajectory tracking simulations and experiments on robotic
manipulators, demonstrating superior accuracy, robustness, and learning
efficiency relative to state of the art alternatives.

</details>


### [446] [Orchestrate, Generate, Reflect: A VLM-Based Multi-Agent Collaboration Framework for Automated Driving Policy Learning](https://arxiv.org/abs/2509.17042)
*Zengqi Peng,Yusen Xie,Yubin Wang,Rui Yang,Qifeng Chen,Jun Ma*

Main category: cs.RO

TL;DR: 本文提出OGR（Orchestrate, Generate, Reflect）框架，利用视觉语言模型（VLM）的多智能体协作，自动化自动驾驶策略学习中的奖励函数和训练课程设计，实现交互感知的驾驶技能。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中，为复杂动态任务手动设计奖励函数和训练课程是一个劳动密集且耗时的关键瓶颈。

Method: OGR框架基于VLM的多智能体协作，构建了一个分层智能体系统。核心组件包括：1) 一个中心协调器规划高层训练目标；2) 一个生成模块通过“分析-生成”两步法高效生成奖励-课程对；3) 一个反思模块基于在线评估进行迭代优化。此外，还引入了专用记忆模块以赋予VLM智能体长期记忆能力，并采用并行生成方案和人机协作技术增强奖励观测空间生成过程的鲁棒性和多样性。

Result: 在CARLA模拟器中，OGR展现出卓越的性能、在不同城市场景下的强大泛化能力以及与各种强化学习算法的良好兼容性。进一步的真实世界实验也突显了该框架的实用性和有效性。OGR通过高效的多智能体合作和利用丰富的多模态信息，使强化学习策略能够在线演进，以获得交互感知的驾驶技能。

Conclusion: OGR框架成功解决了自动驾驶策略学习中奖励函数和训练课程手动工程的难题，通过VLM驱动的多智能体协作，实现了自动驾驶策略的在线演进，并展示了优越的性能、鲁棒的泛化能力和实际可行性。

Abstract: The advancement of foundation models fosters new initiatives for policy
learning in achieving safe and efficient autonomous driving. However, a
critical bottleneck lies in the manual engineering of reward functions and
training curricula for complex and dynamic driving tasks, which is a
labor-intensive and time-consuming process. To address this problem, we propose
OGR (Orchestrate, Generate, Reflect), a novel automated driving policy learning
framework that leverages vision-language model (VLM)-based multi-agent
collaboration. Our framework capitalizes on advanced reasoning and multimodal
understanding capabilities of VLMs to construct a hierarchical agent system.
Specifically, a centralized orchestrator plans high-level training objectives,
while a generation module employs a two-step analyze-then-generate process for
efficient generation of reward-curriculum pairs. A reflection module then
facilitates iterative optimization based on the online evaluation. Furthermore,
a dedicated memory module endows the VLM agents with the capabilities of
long-term memory. To enhance robustness and diversity of the generation
process, we introduce a parallel generation scheme and a human-in-the-loop
technique for augmentation of the reward observation space. Through efficient
multi-agent cooperation and leveraging rich multimodal information, OGR enables
the online evolution of reinforcement learning policies to acquire
interaction-aware driving skills. Extensive experiments in the CARLA simulator
demonstrate the superior performance, robust generalizability across distinct
urban scenarios, and strong compatibility with various RL algorithms. Further
real-world experiments highlight the practical viability and effectiveness of
our framework. The source code will be available upon acceptance of the paper.

</details>


### [447] [FILIC: Dual-Loop Force-Guided Imitation Learning with Impedance Torque Control for Contact-Rich Manipulation Tasks](https://arxiv.org/abs/2509.17053)
*Haizhou Ge,Yufei Jia,Zheng Li,Yue Li,Zhixing Chen,Ruqi Huang,Guyue Zhou*

Main category: cs.RO

TL;DR: FILIC是一个力引导的模仿学习框架，结合了阻抗力矩控制和经济高效的力估计，使机器人能够进行安全、顺从且适应性强的接触丰富型操作，即使在没有专用力/扭矩传感器的情况下也能实现。


<details>
  <summary>Details</summary>
Motivation: 机器人进行插入、装配和手内操作等需要精确力控制的任务时，接触丰富型操作至关重要。然而，大多数模仿学习（IL）策略以位置为中心，缺乏明确的力感知，且为协作机器人手臂添加力/扭矩传感器通常成本高昂并需要额外的硬件设计。

Method: 本文提出了FILIC框架，它将一个基于Transformer的IL策略与一个阻抗控制器集成在双循环结构中，实现顺从的力感知、力执行操作。对于没有力/扭矩传感器的机器人，通过使用关节扭矩测量和数字孪生模型预测扭矩补偿，引入了一种经济高效的基于解析雅可比逆的末端执行器力估计器。此外，还设计了通过手持触觉和VR可视化来提高演示质量的补充力反馈框架。

Result: 实验表明，FILIC显著优于仅基于视觉和基于关节扭矩的方法，实现了更安全、更顺从、更具适应性的接触丰富型操作。

Conclusion: FILIC提供了一种有效的解决方案，通过力引导的模仿学习和创新的力估计方法，使机器人能够经济高效地执行复杂的接触丰富型任务，即使在缺乏专用力传感器的情况下也能实现高性能表现。

Abstract: Contact-rich manipulation is crucial for robots to perform tasks requiring
precise force control, such as insertion, assembly, and in-hand manipulation.
However, most imitation learning (IL) policies remain position-centric and lack
explicit force awareness, and adding force/torque sensors to collaborative
robot arms is often costly and requires additional hardware design. To overcome
these issues, we propose FILIC, a Force-guided Imitation Learning framework
with impedance torque control. FILIC integrates a Transformer-based IL policy
with an impedance controller in a dual-loop structure, enabling compliant
force-informed, force-executed manipulation. For robots without force/torque
sensors, we introduce a cost-effective end-effector force estimator using joint
torque measurements through analytical Jacobian-based inversion while
compensating with model-predicted torques from a digital twin. We also design
complementary force feedback frameworks via handheld haptics and VR
visualization to improve demonstration quality. Experiments show that FILIC
significantly outperforms vision-only and joint-torque-based methods, achieving
safer, more compliant, and adaptable contact-rich manipulation. Our code can be
found in https://github.com/TATP-233/FILIC.

</details>


### [448] [RoboManipBaselines: A Unified Framework for Imitation Learning in Robotic Manipulation across Real and Simulated Environments](https://arxiv.org/abs/2509.17057)
*Masaki Murooka,Tomohiro Motoda,Ryoichi Nakajo,Hanbit Oh,Koshi Makihara,Keisuke Shirai,Yukiyasu Domae*

Main category: cs.RO

TL;DR: RoboManipBaselines是一个用于机器人模仿学习的开放框架，统一了模拟和真实机器人之间的数据收集、训练和评估。


<details>
  <summary>Details</summary>
Motivation: 研究动机是需要一个能够对不同任务、机器人和多模态策略进行系统基准测试的平台，并强调集成、通用性、可扩展性和可复现性。

Method: 本文介绍了RoboManipBaselines，一个将数据收集、训练和评估统一起来的开放框架，适用于模拟和真实机器人。

Result: 结果是创建了一个名为RoboManipBaselines的平台，它是一个开放框架，能够实现机器人模仿学习中的数据收集、训练和评估的统一，并支持系统基准测试。

Conclusion: RoboManipBaselines作为一个新平台，旨在促进机器人模仿学习领域的系统基准测试、集成、通用性、可扩展性和可复现性。

Abstract: RoboManipBaselines is an open framework for robot imitation learning that
unifies data collection, training, and evaluation across simulation and real
robots. We introduce it as a platform enabling systematic benchmarking of
diverse tasks, robots, and multimodal policies with emphasis on integration,
generality, extensibility, and reproducibility.

</details>


### [449] [Certifiably Optimal Doppler Positioning using Opportunistic LEO Satellites](https://arxiv.org/abs/2509.17198)
*Baoshan Song,Weisong Wen,Qi Zhang,Bing Xu,Li-Ta Hsu*

Main category: cs.RO

TL;DR: 本文提出了一种基于凸优化的可认证最优LEO多普勒定位方法，通过分级权重近似和半正定规划松弛，解决了传统方法在缺乏精确初始估计时易陷入局部最优的问题，并在无初始估计下实现了140米的三维定位误差。


<details>
  <summary>Details</summary>
Motivation: 全球导航卫星系统（GNSS）需要备份和增强，LEO卫星的多普勒频移可作为PNT（定位、导航和授时）的信号源。然而，多普勒定位问题是非凸的，局部搜索方法在初始估计不精确时容易产生局部最优解。在未知环境中缺乏精确初始估计时，需要一种无需初始化的、能保证全局最优的优化方法。

Method: 提出了一种利用凸优化实现可认证最优LEO多普勒定位的方法。具体通过分级权重近似（GWA）算法和半正定规划（SDP）松弛来实现。为了保证最优性，推导了理想无噪声情况下的最优性必要条件和有噪声情况下的充分噪声边界条件。

Result: 仿真和真实测试（使用Iridium-NEXT卫星）验证了所提方法的有效性和鲁棒性。在无初始估计的情况下，该方法实现了140米的三维定位误差，优于高斯-牛顿和Dog-Leg等局部搜索方法（当初始点与真值相距1000公里或更远时，它们会陷入局部最优）。此外，该可认证估计还可以作为局部搜索方法的初始化，将三维定位误差降低到130米。

Conclusion: 所提出的基于凸优化的可认证最优LEO多普勒定位方法（结合GWA和SDP）能够有效解决无初始估计情况下的定位问题，避免局部最优，提供鲁棒且准确的定位结果，并可作为局部搜索方法的良好初始化。

Abstract: To provide backup and augmentation to global navigation satellite system
(GNSS), Doppler shift from Low Earth Orbit (LEO) satellites can be employed as
signals of opportunity (SOP) for position, navigation and timing (PNT). Since
the Doppler positioning problem is non-convex, local searching methods may
produce two types of estimates: a global optimum without notice or a local
optimum given an inexact initial estimate. As exact initialization is
unavailable in some unknown environments, a guaranteed global optimization
method in no need of initialization becomes necessary. To achieve this goal, we
propose a certifiably optimal LEO Doppler positioning method by utilizing
convex optimization. In this paper, the certifiable positioning method is
implemented through a graduated weight approximation (GWA) algorithm and
semidefinite programming (SDP) relaxation. To guarantee the optimality, we
derive the necessary conditions for optimality in ideal noiseless cases and
sufficient noise bounds conditions in noisy cases. Simulation and real tests
are conducted to evaluate the effectiveness and robustness of the proposed
method. Specially, the real test using Iridium-NEXT satellites shows that the
proposed method estimates an certifiably optimal solution with an 3D
positioning error of 140 m without initial estimates while Gauss-Newton and
Dog-Leg are trapped in local optima when the initial point is equal or larger
than 1000 km away from the ground truth. Moreover, the certifiable estimation
can also be used as initialization in local searching methods to lower down the
3D positioning error to 130 m.

</details>


### [450] [CoPlanner: An Interactive Motion Planner with Contingency-Aware Diffusion for Autonomous Driving](https://arxiv.org/abs/2509.17080)
*Ruiguo Zhong,Ruoyu Yao,Pei Liu,Xiaolong Chen,Rui Yang,Jun Ma*

Main category: cs.RO

TL;DR: CoPlanner是一个统一的框架，通过枢轴条件扩散机制生成多模态轨迹，并结合应急感知多场景评分策略进行运动规划，以提高自动驾驶在复杂交互环境中的安全性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶系统在轨迹预测和运动规划中存在问题：1) “生成-评估”框架通常只选择单一最可能结果，导致决策过度自信，缺乏关键场景下的备用策略；2) 预测和规划模块的分离可能导致在高度交互交通中产生不一致或不切实际的联合轨迹。

Method: 本文提出了一个应急感知扩散规划器（CoPlanner）。具体方法包括：1) **枢轴条件扩散机制**：通过锚定在经过验证的共享短期轨迹段上进行轨迹采样，以保持时间一致性，同时随机生成捕获多模态运动演变的长期多样化分支。2) **应急感知多场景评分策略**：在多个可能的长期演变场景中评估候选自我轨迹，平衡安全性、进展和舒适性，从而保留可行的备用选项并增强不确定性下的鲁棒性。

Result: 在nuPlan基准测试的Val14和Test14数据集上，CoPlanner在闭环实验中持续超越现有最先进的方法，在反应式和非反应式设置下，安全性和舒适性均获得显著提升。

Conclusion: CoPlanner通过联合建模多智能体交互轨迹生成和应急感知运动规划，提供了一个集成的解决方案，能够生成更真实的交互感知规划，并在复杂交互环境中提高自动驾驶系统的安全性和鲁棒性。

Abstract: Accurate trajectory prediction and motion planning are crucial for autonomous
driving systems to navigate safely in complex, interactive environments
characterized by multimodal uncertainties. However, current
generation-then-evaluation frameworks typically construct multiple plausible
trajectory hypotheses but ultimately adopt a single most likely outcome,
leading to overconfident decisions and a lack of fallback strategies that are
vital for safety in rare but critical scenarios. Moreover, the usual decoupling
of prediction and planning modules could result in socially inconsistent or
unrealistic joint trajectories, especially in highly interactive traffic. To
address these challenges, we propose a contingency-aware diffusion planner
(CoPlanner), a unified framework that jointly models multi-agent interactive
trajectory generation and contingency-aware motion planning. Specifically, the
pivot-conditioned diffusion mechanism anchors trajectory sampling on a
validated, shared short-term segment to preserve temporal consistency, while
stochastically generating diverse long-horizon branches that capture multimodal
motion evolutions. In parallel, we design a contingency-aware multi-scenario
scoring strategy that evaluates candidate ego trajectories across multiple
plausible long-horizon evolution scenarios, balancing safety, progress, and
comfort. This integrated design preserves feasible fallback options and
enhances robustness under uncertainty, leading to more realistic
interaction-aware planning. Extensive closed-loop experiments on the nuPlan
benchmark demonstrate that CoPlanner consistently surpasses state-of-the-art
methods on both Val14 and Test14 datasets, achieving significant improvements
in safety and comfort under both reactive and non-reactive settings. Code and
model will be made publicly available upon acceptance.

</details>


### [451] [AERO-MPPI: Anchor-Guided Ensemble Trajectory Optimization for Agile Mapless Drone Navigation](https://arxiv.org/abs/2509.17340)
*Xin Chen,Rui Huang,Longbin Tang,Lin Zhao*

Main category: cs.RO

TL;DR: 本文提出AERO-MPPI，一个GPU加速的无地图导航框架，通过锚点引导的MPPI优化器集成感知与规划，使无人机在复杂3D环境中实现实时、敏捷、鲁棒的飞行。


<details>
  <summary>Details</summary>
Motivation: 传统的地图构建-规划-控制流程计算成本高昂且会传播估计误差，这使得自主无人机在复杂3D环境中进行敏捷的无地图导航面临巨大挑战。

Method: AERO-MPPI是一个完全GPU加速的框架，通过锚点引导的模型预测路径积分（MPPI）优化器集成感知和规划。具体方法包括：设计多分辨率LiDAR点云表示以快速提取空间分布的“锚点”作为前瞻中间终点；从锚点构建多项式轨迹引导以探索不同的同伦路径类别；在每个规划步骤中并行运行多个MPPI实例，并使用平衡避障和目标到达的两阶段多目标成本进行评估。该框架完全使用NVIDIA Warp GPU内核实现。

Result: AERO-MPPI实现了实时机载操作，并缓解了单一MPPI方法的局部最小值失败问题。在森林、垂直和倾斜环境中的广泛模拟表明，该系统能以超过7米/秒的速度持续可靠飞行，成功率超过80%，且轨迹比现有基线更平滑。在配备LiDAR的四旋翼无人机（NVIDIA Jetson Orin NX 16G）上的实际实验证实，AERO-MPPI能实时机载运行，并在复杂杂乱环境中实现安全、敏捷和鲁棒的飞行。

Conclusion: AERO-MPPI为无人机在复杂3D环境中进行无地图导航提供了一个实时、鲁棒且敏捷的解决方案，通过其创新的感知与规划集成方法和GPU加速，显著优于现有技术，并在模拟和现实世界中表现出色。

Abstract: Agile mapless navigation in cluttered 3D environments poses significant
challenges for autonomous drones. Conventional mapping-planning-control
pipelines incur high computational cost and propagate estimation errors. We
present AERO-MPPI, a fully GPU-accelerated framework that unifies perception
and planning through an anchor-guided ensemble of Model Predictive Path
Integral (MPPI) optimizers. Specifically, we design a multi-resolution LiDAR
point-cloud representation that rapidly extracts spatially distributed
"anchors" as look-ahead intermediate endpoints, from which we construct
polynomial trajectory guides to explore distinct homotopy path classes. At each
planning step, we run multiple MPPI instances in parallel and evaluate them
with a two-stage multi-objective cost that balances collision avoidance and
goal reaching. Implemented entirely with NVIDIA Warp GPU kernels, AERO-MPPI
achieves real-time onboard operation and mitigates the local-minima failures of
single-MPPI approaches. Extensive simulations in forests, verticals, and
inclines demonstrate sustained reliable flight above 7 m/s, with success rates
above 80% and smoother trajectories compared to state-of-the-art baselines.
Real-world experiments on a LiDAR-equipped quadrotor with NVIDIA Jetson Orin NX
16G confirm that AERO-MPPI runs in real time onboard and consistently achieves
safe, agile, and robust flight in complex cluttered environments. The code will
be open-sourced upon acceptance of the paper.

</details>


### [452] [Imagine2Act: Leveraging Object-Action Motion Consistency from Imagined Goals for Robotic Manipulation](https://arxiv.org/abs/2509.17125)
*Liang Heng,Jiadong Xu,Yiwen Wang,Xiaoqi Li,Muhe Cai,Yan Shen,Juan Zhu,Guanghui Ren,Hao Dong*

Main category: cs.RO

TL;DR: Imagine2Act是一种3D模仿学习框架，通过生成想象的目标点云和引入物体-动作一致性策略，解决了关系型物体重新排列（ROR）任务中精确语义和几何推理的挑战，实现了高精度机械臂操作。


<details>
  <summary>Details</summary>
Motivation: 现有关系型物体重新排列（ROR）任务的解决方案存在局限性：基于演示的方法难以捕捉复杂的几何约束；而基于目标状态观测的方法虽然能捕捉语义和几何知识，但未能明确地将物体变换与动作预测耦合，导致生成噪声引起的错误。

Method: 本文提出了Imagine2Act，一个3D模仿学习框架。它首先根据语言指令生成想象的目标图像，并重建相应的3D点云，以提供鲁棒的语义和几何先验。这些想象的目标点云作为策略模型的额外输入。此外，该框架采用了一种带有软姿态监督的物体-动作一致性策略，明确地将预测的末端执行器运动与生成的物体变换对齐。

Result: Imagine2Act能够对物体间的语义和几何关系进行推理，并预测出各种任务中的精确动作。在模拟和真实世界的实验中，Imagine2Act均优于现有最先进的策略。

Conclusion: Imagine2Act通过整合语义和几何约束，并明确耦合物体变换与动作预测，成功解决了高精度关系型物体重新排列任务的挑战，显著提升了机器人的操作能力。

Abstract: Relational object rearrangement (ROR) tasks (e.g., insert flower to vase)
require a robot to manipulate objects with precise semantic and geometric
reasoning. Existing approaches either rely on pre-collected demonstrations that
struggle to capture complex geometric constraints or generate goal-state
observations to capture semantic and geometric knowledge, but fail to
explicitly couple object transformation with action prediction, resulting in
errors due to generative noise. To address these limitations, we propose
Imagine2Act, a 3D imitation-learning framework that incorporates semantic and
geometric constraints of objects into policy learning to tackle high-precision
manipulation tasks. We first generate imagined goal images conditioned on
language instructions and reconstruct corresponding 3D point clouds to provide
robust semantic and geometric priors. These imagined goal point clouds serve as
additional inputs to the policy model, while an object-action consistency
strategy with soft pose supervision explicitly aligns predicted end-effector
motion with generated object transformation. This design enables Imagine2Act to
reason about semantic and geometric relationships between objects and predict
accurate actions across diverse tasks. Experiments in both simulation and the
real world demonstrate that Imagine2Act outperforms previous state-of-the-art
policies. More visualizations can be found at
https://sites.google.com/view/imagine2act.

</details>


### [453] [GPS Denied IBVS-Based Navigation and Collision Avoidance of UAV Using a Low-Cost RGB Camera](https://arxiv.org/abs/2509.17435)
*Xiaoyu Wang,Yan Rui Tan,William Leong,Sunan Huang,Rodney Teo,Cheng Xiang*

Main category: cs.RO

TL;DR: 本文提出了一种基于RGB相机的图像视觉伺服（IBVS）框架，用于无人机导航和碰撞避障，通过AI单目深度估计实现，并可在Jetson平台上完全板载运行。


<details>
  <summary>Details</summary>
Motivation: 尽管无人机导航已被广泛研究，但在涉及多个视觉目标和碰撞避障的任务中，应用IBVS仍然具有挑战性。现有方法常依赖立体相机或外部工作站，限制了系统的自给自足和部署能力。

Method: 该方法使用IBVS进行导航，无需显式路径规划。碰撞避障通过基于AI的RGB图像单目深度估计实现。整个框架在Jetson平台上完全板载运行，仅使用一个RGB相机。

Result: 实验结果验证了无人机能够在无GPS环境中有效导航穿过多个AprilTag并避开障碍物。

Conclusion: 所提出的IBVS框架使得无人机能够仅使用RGB相机实现多目标导航和碰撞避障，并且系统是自给自足和可部署的，无需立体相机或外部计算资源。

Abstract: This paper proposes an image-based visual servoing (IBVS) framework for UAV
navigation and collision avoidance using only an RGB camera. While UAV
navigation has been extensively studied, it remains challenging to apply IBVS
in missions involving multiple visual targets and collision avoidance. The
proposed method achieves navigation without explicit path planning, and
collision avoidance is realized through AI-based monocular depth estimation
from RGB images. Unlike approaches that rely on stereo cameras or external
workstations, our framework runs fully onboard a Jetson platform, ensuring a
self-contained and deployable system. Experimental results validate that the
UAV can navigate across multiple AprilTags and avoid obstacles effectively in
GPS-denied environments.

</details>


### [454] [History-Aware Visuomotor Policy Learning via Point Tracking](https://arxiv.org/abs/2509.17141)
*Jingjing Chen,Hongjie Fang,Chenxi Wang,Shiquan Wang,Cewu Lu*

Main category: cs.RO

TL;DR: 本文提出了一种基于点跟踪的以物体为中心的历史表示方法，旨在为视觉运动策略提供紧凑且高效的记忆，以解决现有马尔可夫策略在处理重复状态和长时依赖性操作任务时的不足。


<details>
  <summary>Details</summary>
Motivation: 许多操作任务需要超越当前观察的记忆，但大多数视觉运动策略依赖于马尔可夫假设，难以处理重复状态或长时依赖。现有方法扩展观察范围不足以满足多样化的记忆需求。

Method: 提出一种基于点跟踪的以物体为中心的历史表示。该方法将过去的观察抽象为紧凑且结构化的形式，仅保留任务相关的关键信息。跟踪点在物体层面进行编码和聚合，生成紧凑的历史表示，可无缝集成到各种视觉运动策略中。

Result: 该方法提供了全面的历史感知和高计算效率，显著提高了整体任务性能和决策准确性。它解决了多方面的记忆需求，如任务阶段识别、空间记忆、动作计数，以及连续和预加载记忆等长期需求。在多样化的操作任务中，其性能持续优于马尔可夫基线和先前的基于历史的方法。

Conclusion: 所提出的以物体为中心的点跟踪历史表示方法，通过提供紧凑、高效且全面的记忆能力，有效解决了操作任务中的记忆挑战，显著提升了视觉运动策略的性能和决策准确性。

Abstract: Many manipulation tasks require memory beyond the current observation, yet
most visuomotor policies rely on the Markov assumption and thus struggle with
repeated states or long-horizon dependencies. Existing methods attempt to
extend observation horizons but remain insufficient for diverse memory
requirements. To this end, we propose an object-centric history representation
based on point tracking, which abstracts past observations into a compact and
structured form that retains only essential task-relevant information. Tracked
points are encoded and aggregated at the object level, yielding a compact
history representation that can be seamlessly integrated into various
visuomotor policies. Our design provides full history-awareness with high
computational efficiency, leading to improved overall task performance and
decision accuracy. Through extensive evaluations on diverse manipulation tasks,
we show that our method addresses multiple facets of memory requirements - such
as task stage identification, spatial memorization, and action counting, as
well as longer-term demands like continuous and pre-loaded memory - and
consistently outperforms both Markovian baselines and prior history-based
approaches. Project website: http://tonyfang.net/history

</details>


### [455] [EigenSafe: A Spectral Framework for Learning-Based Stochastic Safety Filtering](https://arxiv.org/abs/2509.17750)
*Inkyu Jang,Jonghae Park,Chams E. Mballo,Sihyun Cho,Claire J. Tomlin,H. Jin Kim*

Main category: cs.RO

TL;DR: 本文提出了EigenSafe，一个基于算子理论的框架，用于随机系统的学习型安全关键控制。它通过学习安全概率动态规划原则的显性特征对和安全备份策略，构建安全滤波器以检测并规避不安全情况。


<details>
  <summary>Details</summary>
Motivation: 在许多机器人系统中，由于传感噪声和环境干扰等因素，动力学最好建模为随机系统。然而，传统的Hamilton-Jacobi可达性分析和控制障碍函数等方法难以对这类系统提供全面的安全度量。

Method: EigenSafe框架推导了一个管理安全概率动态规划原则的线性算子，并发现其主导特征对能提供个体状态和整体闭环系统的安全信息。该框架离线联合学习这个主导特征对和一个安全备份策略。学习到的特征函数用于构建一个安全滤波器，以检测潜在的不安全情况并回退到备份策略。

Result: 该框架在三个模拟的随机安全关键控制任务中得到了验证。

Conclusion: EigenSafe提供了一个新颖的算子理论框架，用于解决随机系统的学习型安全关键控制问题，通过主导特征对和安全滤波器机制增强了系统的安全性。

Abstract: We present EigenSafe, an operator-theoretic framework for learning-enabled
safety-critical control for stochastic systems. In many robotic systems where
dynamics are best modeled as stochastic systems due to factors such as sensing
noise and environmental disturbances, it is challenging for conventional
methods such as Hamilton-Jacobi reachability and control barrier functions to
provide a holistic measure of safety. We derive a linear operator governing the
dynamic programming principle for safety probability, and find that its
dominant eigenpair provides information about safety for both individual states
and the overall closed-loop system. The proposed learning framework, called
EigenSafe, jointly learns this dominant eigenpair and a safe backup policy in
an offline manner. The learned eigenfunction is then used to construct a safety
filter that detects potentially unsafe situations and falls back to the backup
policy. The framework is validated in three simulated stochastic
safety-critical control tasks.

</details>


### [456] [MAST: Multi-Agent Spatial Transformer for Learning to Collaborate](https://arxiv.org/abs/2509.17195)
*Damian Owerko,Frederic Vatnsdal,Saurav Agarwal,Vijay Kumar,Alejandro Ribeiro*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的多智能体空间Transformer（MAST），用于在大规模去中心化协作多机器人系统（DC-MRS）中学习通信策略。


<details>
  <summary>Details</summary>
Motivation: DC-MRS中的协作面临挑战：局部可观察状态、有限通信范围（无中央服务器）以及独立执行动作。机器人需要优化共同的任务目标，这在受限环境下必须通过展示所需协作行为的通信策略来完成。

Method: MAST是一种去中心化Transformer架构，它学习通信策略以计算要与其他智能体共享的抽象信息，并结合机器人自身的观察处理接收到的信息。MAST通过新的位置编码策略和采用窗口化限制感受野的注意力操作扩展了标准Transformer，旨在实现局部计算、平移等变性和置换等变性。该模型通过模仿学习在集中式设置中进行训练。

Result: MAST在去中心化分配和导航（DAN）以及去中心化覆盖控制任务中表现出有效性。去中心化的MAST策略对通信延迟具有鲁棒性，可扩展到大型团队，并且性能优于基线和其他基于学习的方法。

Conclusion: MAST为解决大规模去中心化协作多机器人系统中的通信策略学习问题提供了一个有前景的方法，并在实际应用中展现出优越的性能和鲁棒性。

Abstract: This article presents a novel multi-agent spatial transformer (MAST) for
learning communication policies in large-scale decentralized and collaborative
multi-robot systems (DC-MRS). Challenges in collaboration in DC-MRS arise from:
(i) partial observable states as robots make only localized perception, (ii)
limited communication range with no central server, and (iii) independent
execution of actions. The robots need to optimize a common task-specific
objective, which, under the restricted setting, must be done using a
communication policy that exhibits the desired collaborative behavior. The
proposed MAST is a decentralized transformer architecture that learns
communication policies to compute abstract information to be shared with other
agents and processes the received information with the robot's own
observations. The MAST extends the standard transformer with new positional
encoding strategies and attention operations that employ windowing to limit the
receptive field for MRS. These are designed for local computation,
shift-equivariance, and permutation equivariance, making it a promising
approach for DC-MRS. We demonstrate the efficacy of MAST on decentralized
assignment and navigation (DAN) and decentralized coverage control. Efficiently
trained using imitation learning in a centralized setting, the decentralized
MAST policy is robust to communication delays, scales to large teams, and
performs better than the baselines and other learning-based approaches.

</details>


### [457] [Guided Multi-Fidelity Bayesian Optimization for Data-driven Controller Tuning with Digital Twins](https://arxiv.org/abs/2509.17952)
*Mahdi Nobar,Jürg Keller,Alessandro Forino,John Lygeros,Alisa Rupenyan*

Main category: cs.RO

TL;DR: 本文提出了一种引导式多精度贝叶斯优化框架，通过整合修正后的数字孪生（DT）仿真与真实测量数据，实现数据高效的控制器调优，并能自适应地处理模型失配问题。


<details>
  <summary>Details</summary>
Motivation: 针对闭环系统控制器调优中存在的仿真精度有限或廉价近似的问题，以及由此产生的模型失配，需要一种数据高效的方法来整合仿真与真实数据进行调优。

Method: 该方法构建了一个多精度代理模型，其中包含一个学习到的校正模型，用于根据真实数据修正数字孪生估计。它使用一个自适应的成本感知采集函数来平衡预期改进、精度和采样成本。随着新测量的到来，数字孪生的准确性会被重新评估，动态调整跨源相关性和采集函数，确保准确的数字孪生被更频繁地使用。

Result: 在机器人驱动硬件上的实验和数值研究表明，与标准贝叶斯优化（BO）和多精度方法相比，该方法显著提高了调优效率。

Conclusion: 所提出的引导式多精度贝叶斯优化框架通过智能地整合修正后的数字孪生仿真和真实世界测量，并自适应地处理模型失配，有效提升了控制器调优的数据效率和性能。

Abstract: We propose a \textit{guided multi-fidelity Bayesian optimization} framework
for data-efficient controller tuning that integrates corrected digital twin
(DT) simulations with real-world measurements. The method targets closed-loop
systems with limited-fidelity simulations or inexpensive approximations. To
address model mismatch, we build a multi-fidelity surrogate with a learned
correction model that refines DT estimates from real data. An adaptive
cost-aware acquisition function balances expected improvement, fidelity, and
sampling cost. Our method ensures adaptability as new measurements arrive. The
accuracy of DTs is re-estimated, dynamically adapting both cross-source
correlations and the acquisition function. This ensures that accurate DTs are
used more frequently, while inaccurate DTs are appropriately downweighted.
Experiments on robotic drive hardware and supporting numerical studies
demonstrate that our method enhances tuning efficiency compared to standard
Bayesian optimization (BO) and multi-fidelity methods.

</details>


### [458] [Ratatouille: Imitation Learning Ingredients for Real-world Social Robot Navigation](https://arxiv.org/abs/2509.17204)
*James R. Han,Mithun Vanniasinghe,Hshmat Sahak,Nicholas Rhinehart,Timothy D. Barfoot*

Main category: cs.RO

TL;DR: 本文提出Ratatouille，一个针对社交机器人导航的离线模仿学习（IL）管线和模型架构。与朴素行为克隆（BC）相比，Ratatouille在不增加数据量的情况下，显著降低了碰撞率并提高了成功率，强调了精心设计的IL在现实世界应用中的重要性。


<details>
  <summary>Details</summary>
Motivation: 在实际环境中扩展强化学习（RL）用于社交机器人导航既耗费数据又不安全，因为策略需要通过直接交互学习，不可避免地会遇到碰撞。离线模仿学习（IL）通过安全收集专家演示并完全离线训练来避免这些风险，但研究发现朴素的行为克隆（BC）在社交导航中表现不足。

Method: 本文提出了Ratatouille，一个离线模仿学习的管线和模型架构。该方法通过优化架构和训练选择，而非改变数据本身，来提升性能。研究在模拟环境和真实世界（大学校园、公共美食广场）中进行了验证，并收集了超过11小时的真实世界数据。

Result: 与朴素行为克隆（BC）相比，Ratatouille在不改变数据的情况下，将每米碰撞次数减少了6倍，并将成功率提高了3倍。

Conclusion: 研究结果表明，深思熟虑的模仿学习（IL）设计，而非仅仅增加数据量，可以显著提高现实世界社交机器人导航的安全性和可靠性。

Abstract: Scaling Reinforcement Learning to in-the-wild social robot navigation is both
data-intensive and unsafe, since policies must learn through direct interaction
and inevitably encounter collisions. Offline Imitation learning (IL) avoids
these risks by collecting expert demonstrations safely, training entirely
offline, and deploying policies zero-shot. However, we find that naively
applying Behaviour Cloning (BC) to social navigation is insufficient; achieving
strong performance requires careful architectural and training choices. We
present Ratatouille, a pipeline and model architecture that, without changing
the data, reduces collisions per meter by 6 times and improves success rate by
3 times compared to naive BC. We validate our approach in both simulation and
the real world, where we collected over 11 hours of data on a dense university
campus. We further demonstrate qualitative results in a public food court. Our
findings highlight that thoughtful IL design, rather than additional data, can
substantially improve safety and reliability in real-world social navigation.
Video: https://youtu.be/tOdLTXsaYLQ. Code will be released after acceptance.

</details>


### [459] [Prepare Before You Act: Learning From Humans to Rearrange Initial States](https://arxiv.org/abs/2509.18043)
*Yinlong Dai,Andre Keyser,Dylan P. Losey*

Main category: cs.RO

TL;DR: 模仿学习在面对分布外观测时表现不佳，需要大量演示。ReSET算法使机器人能像人类一样预先重新布置环境，将初始状态调整为与训练数据相似，从而提高任务执行的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 模仿学习策略在遇到分布外（OOD）观测（如物体位置未见过或被遮挡）时，往往难以泛化，需要大量演示才能达到鲁棒行为。而人类在面对此类非典型初始状态时，常会重新布置环境以方便任务执行。

Method: ReSET算法是一个两步过程：首先自主修改物体姿态，使初始的OOD场景变得与训练数据相似，然后再执行给定的策略。它结合了与动作无关的人类视频和与任务无关的遥操作数据，以决定何时修改场景、预测人类会采取的简化动作，并将这些预测映射为机器人基本动作。

Result: 理论上，ReSET通过在执行策略前重新布置环境，减少了泛化差距。实践中，与扩散策略、VLA等基线相比，ReSET能在总训练数据量相同的情况下，实现更鲁棒的任务执行。

Conclusion: 通过赋予机器人预先准备环境的能力（ReSET算法），可以显著提高模仿学习策略在面对分布外初始状态时的鲁棒性，使其能够更好地泛化，从而解决模仿学习在复杂和多样化环境中的应用挑战。

Abstract: Imitation learning (IL) has proven effective across a wide range of
manipulation tasks. However, IL policies often struggle when faced with
out-of-distribution observations; for instance, when the target object is in a
previously unseen position or occluded by other objects. In these cases,
extensive demonstrations are needed for current IL methods to reach robust and
generalizable behaviors. But when humans are faced with these sorts of atypical
initial states, we often rearrange the environment for more favorable task
execution. For example, a person might rotate a coffee cup so that it is easier
to grasp the handle, or push a box out of the way so they can directly grasp
their target object. In this work we seek to equip robot learners with the same
capability: enabling robots to prepare the environment before executing their
given policy. We propose ReSET, an algorithm that takes initial states -- which
are outside the policy's distribution -- and autonomously modifies object poses
so that the restructured scene is similar to training data. Theoretically, we
show that this two step process (rearranging the environment before rolling out
the given policy) reduces the generalization gap. Practically, our ReSET
algorithm combines action-agnostic human videos with task-agnostic
teleoperation data to i) decide when to modify the scene, ii) predict what
simplifying actions a human would take, and iii) map those predictions into
robot action primitives. Comparisons with diffusion policies, VLAs, and other
baselines show that using ReSET to prepare the environment enables more robust
task execution with equal amounts of total training data. See videos at our
project website: https://reset2025paper.github.io/

</details>


### [460] [Combining Performance and Passivity in Linear Control of Series Elastic Actuators](https://arxiv.org/abs/2509.17210)
*Shaunak A. Mehta,Dylan P. Losey*

Main category: cs.RO

TL;DR: 本文探讨了串联弹性执行器（SEAs）中安全性和性能之间的权衡，发现执行器侧控制结合弹性传动中的阻尼器，通过低物理刚度和高控制器增益，能同时实现高精度性能和用户安全。


<details>
  <summary>Details</summary>
Motivation: 在人机物理交互中，机器人需要同时具备安全性和性能。串联弹性执行器通过引入柔顺性提高了安全性，但其弹性元件会引入振荡并降低运动精度。因此，如何权衡物理安全性和性能是一个关键问题。

Method: 研究人员列举了串联弹性执行器的不同线性控制和机械配置，并分析了每种选择对柔顺性、无源性和跟踪性能的影响。他们特别关注了执行器侧控制，并通过仿真和实际实验验证了其效果。

Result: 研究发现，与传统的负载侧控制相比，执行器侧控制具有显著优势，它允许在更宽的控制增益范围内保持安全。将执行器侧的简单PD控制器与弹性传动中的阻尼器结合，可以实现高性能。模拟和实验表明，通过设计低物理刚度和高控制器增益的系统，可以实现准确的性能，同时确保碰撞时的用户安全。

Conclusion: 通过优化串联弹性执行器的控制和机械设计，特别是采用执行器侧控制并结合弹性传动中的阻尼器，可以在低物理刚度和高控制器增益的条件下，有效解决人机交互中安全性和性能的矛盾，实现高精度性能和用户安全。

Abstract: When humans physically interact with robots, we need the robots to be both
safe and performant. Series elastic actuators (SEAs) fundamentally advance
safety by introducing compliant actuation. On the one hand, adding a spring
mitigates the impact of accidental collisions between human and robot; but on
the other hand, this spring introduces oscillations and fundamentally decreases
the robot's ability to perform precise, accurate motions. So how should we
trade off between physical safety and performance? In this paper, we enumerate
the different linear control and mechanical configurations for series elastic
actuators, and explore how each choice affects the rendered compliance,
passivity, and tracking performance. While prior works focus on load side
control, we find that actuator side control has significant benefits. Indeed,
simple PD controllers on the actuator side allow for a much wider range of
control gains that maintain safety, and combining these with a damper in the
elastic transmission yields high performance. Our simulations and real world
experiments suggest that, by designing a system with low physical stiffness and
high controller gains, this solution enables accurate performance while also
ensuring user safety during collisions.

</details>


### [461] [HuMam: Humanoid Motion Control via End-to-End Deep Reinforcement Learning with Mamba](https://arxiv.org/abs/2509.18046)
*Yinuo Wang,Yuanyang Qi,Jinzhao Zhou,Gavin Tao*

Main category: cs.RO

TL;DR: HuMam是一个端到端的人形机器人步态强化学习框架，首次采用Mamba编码器融合状态，显著提升了学习效率、训练稳定性、任务性能，并降低了能耗和执行器成本。


<details>
  <summary>Details</summary>
Motivation: 现有的端到端人形机器人强化学习策略存在训练不稳定、特征融合效率低下以及执行成本高昂等问题。

Method: HuMam框架采用以状态为中心的端到端强化学习方法。它使用单层Mamba编码器来融合机器人中心状态、定向足迹目标和连续相位时钟。策略输出由低级PD循环跟踪的关节位置目标，并使用PPO进行优化。奖励函数包含六个项，平衡了接触质量、摆动平滑度、落足点、姿态和身体稳定性，同时隐式促进节能。在mc-mujoco中的JVRC-1人形机器人上进行测试。

Result: 与强大的前馈基线相比，HuMam持续改进了学习效率、训练稳定性和整体任务性能，同时降低了功耗和扭矩峰值。这是首个采用Mamba作为融合骨干的端到端人形机器人强化学习控制器，在效率、稳定性和控制经济性方面取得了显著提升。

Conclusion: HuMam框架通过引入Mamba编码器进行状态融合，为人形机器人步态控制提供了一个更高效、稳定且经济的端到端强化学习解决方案，有效解决了现有方法中的核心挑战。

Abstract: End-to-end reinforcement learning (RL) for humanoid locomotion is appealing
for its compact perception-action mapping, yet practical policies often suffer
from training instability, inefficient feature fusion, and high actuation cost.
We present HuMam, a state-centric end-to-end RL framework that employs a
single-layer Mamba encoder to fuse robot-centric states with oriented footstep
targets and a continuous phase clock. The policy outputs joint position targets
tracked by a low-level PD loop and is optimized with PPO. A concise six-term
reward balances contact quality, swing smoothness, foot placement, posture, and
body stability while implicitly promoting energy saving. On the JVRC-1 humanoid
in mc-mujoco, HuMam consistently improves learning efficiency, training
stability, and overall task performance over a strong feedforward baseline,
while reducing power consumption and torque peaks. To our knowledge, this is
the first end-to-end humanoid RL controller that adopts Mamba as the fusion
backbone, demonstrating tangible gains in efficiency, stability, and control
economy.

</details>


### [462] [Neural Network and ANFIS based auto-adaptive MPC for path tracking in autonomous vehicles](https://arxiv.org/abs/2509.17213)
*Yassine Kebbati,Naima Ait-Oufroukh,Vincent Vigneron,Dalil Ichala*

Main category: cs.RO

TL;DR: 本文设计了一种自适应MPC控制器，通过改进的粒子群优化算法进行调整，并利用神经网络和ANFIS进行在线参数自适应，以应对自动驾驶汽车横向控制中的不确定性。该控制器在车道变换和轨迹跟踪场景中表现优于标准MPC。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶汽车在不断变化的环境中运行，面临多种不确定性和干扰，这使得经典控制器，尤其是在横向控制方面，效果不佳。

Method: 设计了一种用于路径跟踪任务的自适应模型预测控制器（MPC）。该控制器通过改进的粒子群优化（PSO）算法进行参数调整。同时，利用神经网络（NN）和自适应神经模糊推理系统（ANFIS）实现控制器参数的在线自适应。

Result: 与标准MPC相比，所设计的控制器在三重车道变换和轨迹跟踪场景中表现出良好的性能和有前景的结果。

Conclusion: 所提出的基于改进PSO调整并结合NN/ANFIS在线自适应的MPC控制器，能够有效解决自动驾驶汽车横向控制中的不确定性问题，并展现出优于传统MPC的性能。

Abstract: Self-driving cars operate in constantly changing environments and are exposed
to a variety of uncertainties and disturbances. These factors render classical
controllers ineffective, especially for lateral control. Therefore, an adaptive
MPC controller is designed in this paper for the path tracking task, tuned by
an improved particle swarm optimization algorithm. Online parameter adaptation
is performed using Neural Networks and ANFIS. The designed controller showed
promising results compared to standard MPC in triple lane change and trajectory
tracking scenarios. Code can be found here:
https://github.com/yassinekebbati/NN_MPC-vs-ANFIS_MPC

</details>


### [463] [Scalable Multi Agent Diffusion Policies for Coverage Control](https://arxiv.org/abs/2509.17244)
*Frederic Vatnsdal,Romina Garcia Camargo,Saurav Agarwal,Alejandro Ribeiro*

Main category: cs.RO

TL;DR: 本文提出MADP，一种基于扩散模型的去中心化机器人群协作方法，通过生成捕获智能体间相互依赖的复杂高维动作分布样本，在覆盖控制任务中表现出卓越的泛化能力和优于现有基线的性能。


<details>
  <summary>Details</summary>
Motivation: 在去中心化机器人群协作中，需要处理复杂且高维的动作分布，并捕获智能体动作之间的相互依赖性，这在现有方法中是一个挑战。

Method: MADP利用扩散模型生成动作样本，每个机器人基于自身观察和来自同伴的感知嵌入来调整策略采样。该策略通过模仿学习从全知专家那里进行训练，扩散过程由空间变换器架构参数化以实现去中心化推理。研究通过改变重要性密度函数的数量、位置和方差，在覆盖控制任务中评估了该系统。

Result: MADP模型继承了扩散模型的宝贵特性，能够泛化到不同的智能体密度和环境，并且在实验中持续优于最先进的基线方法。

Conclusion: MADP为去中心化机器人群协作提供了一种新颖且有效的解决方案，通过利用扩散模型生成相互依赖的动作，实现了在复杂环境中的强大泛化能力和卓越性能。

Abstract: We propose MADP, a novel diffusion-model-based approach for collaboration in
decentralized robot swarms. MADP leverages diffusion models to generate samples
from complex and high-dimensional action distributions that capture the
interdependencies between agents' actions. Each robot conditions policy
sampling on a fused representation of its own observations and perceptual
embeddings received from peers. To evaluate this approach, we task a team of
holonomic robots piloted by MADP to address coverage control-a canonical multi
agent navigation problem. The policy is trained via imitation learning from a
clairvoyant expert on the coverage control problem, with the diffusion process
parameterized by a spatial transformer architecture to enable decentralized
inference. We evaluate the system under varying numbers, locations, and
variances of importance density functions, capturing the robustness demands of
real-world coverage tasks. Experiments demonstrate that our model inherits
valuable properties from diffusion models, generalizing across agent densities
and environments, and consistently outperforming state-of-the-art baselines.

</details>


### [464] [Learning and Optimization with 3D Orientations](https://arxiv.org/abs/2509.17274)
*Alexandros Ntagkas,Constantinos Tsakonas,Chairi Kiourt,Konstantinos Chatzilygeroudis*

Main category: cs.RO

TL;DR: 本文旨在统一呈现3D姿态表示方法及其相关技巧，并通过在机器人学中的代表性场景进行基准测试，提供选择最佳表示的指导方针。


<details>
  <summary>Details</summary>
Motivation: 在处理3D姿态时，选择合适的表示方法（包括损失函数等）非常困难，且缺乏统一清晰的文献来涵盖所有可能性。特别是在涉及以姿态作为输入/输出的学习或优化任务中，决策更加复杂，需要基于实证证据的推荐。

Method: a) 以统一的符号清晰简洁地呈现所有可用的3D姿态表示方法和相关“技巧”（包括李群代数）。b) 在代表性的机器人学场景中对这些方法进行基准测试，包括：1) 直接优化，2) 带有神经网络控制器的模仿/监督学习，3) 强化学习，4) 使用微分动态规划的轨迹优化。最后，提供基于场景的指导方针，并公开所有描述的姿态数学的参考实现。

Result: 本文将根据基准测试的实证结果，为不同场景下3D姿态表示的选择提供指导方针，并提供一个参考实现。

Conclusion: 通过统一整理3D姿态表示并进行实证基准测试，本文旨在解决选择姿态表示的难题，为机器人学等领域提供明确的推荐和实用的参考资源。

Abstract: There exist numerous ways of representing 3D orientations. Each
representation has both limitations and unique features. Choosing the best
representation for one task is often a difficult chore, and there exist
conflicting opinions on which representation is better suited for a set of
family of tasks. Even worse, when dealing with scenarios where we need to learn
or optimize functions with orientations as inputs and/or outputs, the set of
possibilities (representations, loss functions, etc.) is even larger and it is
not easy to decide what is best for each scenario. In this paper, we attempt to
a) present clearly, concisely and with unified notation all available
representations, and "tricks" related to 3D orientations (including Lie Group
algebra), and b) benchmark them in representative scenarios. The first part
feels like it is missing from the robotics literature as one has to read many
different textbooks and papers in order have a concise and clear understanding
of all possibilities, while the benchmark is necessary in order to come up with
recommendations based on empirical evidence. More precisely, we experiment with
the following settings that attempt to cover most widely used scenarios in
robotics: 1) direct optimization, 2) imitation/supervised learning with a
neural network controller, 3) reinforcement learning, and 4) trajectory
optimization using differential dynamic programming. We finally provide
guidelines depending on the scenario, and make available a reference
implementation of all the orientation math described.

</details>


### [465] [Event-Based Visual Teach-and-Repeat via Fast Fourier-Domain Cross-Correlation](https://arxiv.org/abs/2509.17287)
*Gokul B. Nair,Alejandro Fontan,Michael Milford,Tobias Fischer*

Main category: cs.RO

TL;DR: 本文提出首个基于事件相机的视觉示教复现导航系统，通过频域互相关框架实现超300Hz的处理速度，显著提升机器人导航的实时性和响应性。


<details>
  <summary>Details</summary>
Motivation: 传统帧率相机（30-60 Hz）的固定帧率导致环境变化与控制响应之间存在固有的延迟，限制了示教复现导航系统的响应速度。

Method: 开发了首个基于事件相机的视觉示教复现系统。该系统利用频域互相关框架，将事件流匹配问题转换为傅里叶空间乘法，实现高计算效率。通过利用事件帧的二值特性和图像压缩技术，进一步提升了互相关过程的计算速度，同时不牺牲定位精度。

Result: 该系统处理速率可超过300Hz，比基于帧的方法快一个数量级。在室内外超过4000米的轨迹上成功实现了自主导航，平均轨迹误差（ATE）低于24厘米，并保持了持续的高频控制更新。实验证明，与传统基于帧的系统相比，该方法实现了显著更高的更新速率。

Conclusion: 基于事件的感知对于实时机器人导航具有实际可行性，能够提供显著更高的更新速率和更强的响应能力，克服了传统帧率相机的局限性。

Abstract: Visual teach-and-repeat navigation enables robots to autonomously traverse
previously demonstrated paths by comparing current sensory input with recorded
trajectories. However, conventional frame-based cameras fundamentally limit
system responsiveness: their fixed frame rates (typically 30-60 Hz) create
inherent latency between environmental changes and control responses. Here we
present the first event-camera-based visual teach-and-repeat system. To achieve
this, we develop a frequency-domain cross-correlation framework that transforms
the event stream matching problem into computationally efficient Fourier space
multiplications, capable of exceeding 300Hz processing rates, an order of
magnitude faster than frame-based approaches. By exploiting the binary nature
of event frames and applying image compression techniques, we further enhance
the computational speed of the cross-correlation process without sacrificing
localization accuracy. Extensive experiments using a Prophesee EVK4 HD event
camera mounted on an AgileX Scout Mini robot demonstrate successful autonomous
navigation across 4000+ meters of indoor and outdoor trajectories. Our system
achieves ATEs below 24 cm while maintaining consistent high-frequency control
updates. Our evaluations show that our approach achieves substantially higher
update rates compared to conventional frame-based systems, underscoring the
practical viability of event-based perception for real-time robotic navigation.

</details>


### [466] [Automated Coral Spawn Monitoring for Reef Restoration: The Coral Spawn and Larvae Imaging Camera System (CSLICS)](https://arxiv.org/abs/2509.17299)
*Dorian Tsai,Christopher A. Brunner,Riki Lamont,F. Mikaela Nordborg,Andrea Severati,Java Terry,Karen Jackel,Matthew Dunbabin,Tobias Fischer,Scarlett Raine*

Main category: cs.RO

TL;DR: 本文提出了一种名为CSLICS的自动化系统，利用低成本模块化摄像头和目标检测技术，实现珊瑚卵和幼体的自动计数，显著减少了人工劳动并提高了珊瑚养殖效率，以支持珊瑚礁修复工作。


<details>
  <summary>Details</summary>
Motivation: 珊瑚礁修复中的珊瑚养殖需要准确和持续的卵子计数，以进行资源分配和幼体健康监测。然而，现有方法劳动密集，是珊瑚生产过程中的一个关键瓶颈。

Method: 研究人员开发了珊瑚卵和幼体成像摄像头系统（CSLICS），该系统采用低成本模块化摄像头和通过人机协作（human-in-the-loop）标注方法训练的目标检测器，用于幼体养殖罐中的自动化卵子计数。论文详细介绍了系统工程、数据集收集和计算机视觉技术，以检测、分类和计数珊瑚卵。

Result: 大规模产卵事件的实验结果显示，CSLICS在不同胚胎发育阶段的表面卵子检测F1分数为82.4%，亚表面卵子检测F1分数为65.3%。与相同频率的手动采样方法相比，每次产卵事件可节省5,720小时的劳动。在大堡礁大规模珊瑚产卵事件中，CSLICS监测结果与手动计数相比，准确测量了受精成功率和亚表面卵子数量。

Conclusion: 这些发现改进了珊瑚养殖过程，并使得珊瑚礁修复工作能够扩大规模，以应对大堡礁等生态系统面临的气候变化威胁。

Abstract: Coral aquaculture for reef restoration requires accurate and continuous spawn
counting for resource distribution and larval health monitoring, but current
methods are labor-intensive and represent a critical bottleneck in the coral
production pipeline. We propose the Coral Spawn and Larvae Imaging Camera
System (CSLICS), which uses low cost modular cameras and object detectors
trained using human-in-the-loop labeling approaches for automated spawn
counting in larval rearing tanks. This paper details the system engineering,
dataset collection, and computer vision techniques to detect, classify and
count coral spawn. Experimental results from mass spawning events demonstrate
an F1 score of 82.4\% for surface spawn detection at different embryogenesis
stages, 65.3\% F1 score for sub-surface spawn detection, and a saving of 5,720
hours of labor per spawning event compared to manual sampling methods at the
same frequency. Comparison of manual counts with CSLICS monitoring during a
mass coral spawning event on the Great Barrier Reef demonstrates CSLICS'
accurate measurement of fertilization success and sub-surface spawn counts.
These findings enhance the coral aquaculture process and enable upscaling of
coral reef restoration efforts to address climate change threats facing
ecosystems like the Great Barrier Reef.

</details>


### [467] [Pose Estimation of a Cable-Driven Serpentine Manipulator Utilizing Intrinsic Dynamics via Physical Reservoir Computing](https://arxiv.org/abs/2509.17308)
*Kazutoshi Tanaka,Tomoya Takahashi,Masashi Hamaya*

Main category: cs.RO

TL;DR: 本文提出了一种基于物理储层计算的位姿估计方法，用于轻型线缆驱动蛇形机械臂。该方法利用机械臂的固有非线性动力学，在实验中实现了4.3毫米的平均位姿误差，优于LSTM和分析方法，为轻型机械臂的控制和感知提供了新方向。


<details>
  <summary>Details</summary>
Motivation: 线缆驱动蛇形机械臂在非结构化环境中具有巨大潜力，但其轻量化设计（如将电机和传感器置于基座、使用塑料连杆）导致了柔性引起的变异（如线缆松弛、伸长、连杆变形），使得分析预测与实际连杆位置存在差异，增加了位姿估计的难度。

Method: 开发了一个9自由度、545毫米臂长、308克总质量的线缆驱动蛇形机械臂。提出了一种基于物理储层计算（physical reservoir computing）的位姿估计方法，该方法利用机械臂固有的非线性动力学作为高维储层。

Result: 实验结果显示，所提出的方法实现了4.3毫米的平均位姿误差。相比之下，基线长短期记忆（LSTM）网络为4.4毫米，分析方法为39.5毫米。

Conclusion: 这项工作为利用轻型线缆驱动蛇形机械臂的固有动力学进行控制和感知策略提供了新的方向。

Abstract: Cable-driven serpentine manipulators hold great potential in unstructured
environments, offering obstacle avoidance, multi-directional force application,
and a lightweight design. By placing all motors and sensors at the base and
employing plastic links, we can further reduce the arm's weight. To demonstrate
this concept, we developed a 9-degree-of-freedom cable-driven serpentine
manipulator with an arm length of 545 mm and a total mass of only 308 g.
However, this design introduces flexibility-induced variations, such as cable
slack, elongation, and link deformation. These variations result in
discrepancies between analytical predictions and actual link positions, making
pose estimation more challenging. To address this challenge, we propose a
physical reservoir computing based pose estimation method that exploits the
manipulator's intrinsic nonlinear dynamics as a high-dimensional reservoir.
Experimental results show a mean pose error of 4.3 mm using our method,
compared to 4.4 mm with a baseline long short-term memory network and 39.5 mm
with an analytical approach. This work provides a new direction for control and
perception strategies in lightweight cable-driven serpentine manipulators
leveraging their intrinsic dynamics.

</details>


### [468] [OpenGVL - Benchmarking Visual Temporal Progress for Data Curation](https://arxiv.org/abs/2509.17321)
*Paweł Budzianowski,Emilia Wiśnios,Gracjan Góral,Igor Kulakov,Viktor Petrenko,Krzysztof Walas*

Main category: cs.RO

TL;DR: 本文提出了OpenGVL基准，用于评估机器人和人类操作任务中的时间任务进度预测，并发现开源模型在这方面远逊于闭源模型，同时展示了OpenGVL在自动化数据整理中的应用。


<details>
  <summary>Details</summary>
Motivation: 机器人领域的数据稀缺是主要限制因素，但野外机器人数据量正指数级增长。可靠的时间任务完成预测可以帮助大规模自动标注和整理这些数据，从而利用这些增长的数据。

Method: 本文基于Generative Value Learning (GVL) 方法，提出了OpenGVL，一个综合基准，用于评估涉及机器人和人类操作的各种挑战性任务中的任务进度估计。他们评估了公开可用的开源基础模型，并展示了OpenGVL作为自动化数据整理和过滤工具的实用性。

Result: 评估结果显示，开源模型家族在时间进度预测任务上的表现显著低于闭源模型，仅达到其性能的约70%。此外，OpenGVL被证明是评估大规模机器人数据集质量的有效自动化数据整理和过滤工具。

Conclusion: 开源基础模型在时间任务进度预测方面仍有较大改进空间，OpenGVL提供了一个评估工具，并能有效支持大规模机器人数据集的自动化整理和质量评估。

Abstract: Data scarcity remains one of the most limiting factors in driving progress in
robotics. However, the amount of available robotics data in the wild is growing
exponentially, creating new opportunities for large-scale data utilization.
Reliable temporal task completion prediction could help automatically annotate
and curate this data at scale. The Generative Value Learning (GVL) approach was
recently proposed, leveraging the knowledge embedded in vision-language models
(VLMs) to predict task progress from visual observations. Building upon GVL, we
propose OpenGVL, a comprehensive benchmark for estimating task progress across
diverse challenging manipulation tasks involving both robotic and human
embodiments. We evaluate the capabilities of publicly available open-source
foundation models, showing that open-source model families significantly
underperform closed-source counterparts, achieving only approximately $70\%$ of
their performance on temporal progress prediction tasks. Furthermore, we
demonstrate how OpenGVL can serve as a practical tool for automated data
curation and filtering, enabling efficient quality assessment of large-scale
robotics datasets. We release the benchmark along with the complete codebase at
\href{github.com/budzianowski/opengvl}{OpenGVL}.

</details>


### [469] [DyDexHandover: Human-like Bimanual Dynamic Dexterous Handover using RGB-only Perception](https://arxiv.org/abs/2509.17350)
*Haoran Zhou,Yangwei You,Shuaijun Wang*

Main category: cs.RO

TL;DR: DyDexHandover是一个基于多智能体强化学习的框架，利用RGB感知实现双臂机器人的空中物体抛接，生成类人行为，并展示了高成功率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 双臂机器人空中动态交接面临挑战，现有方法常依赖动力学模型、强先验或深度感知，限制了泛化性和动作自然度。

Method: 提出DyDexHandover框架，采用多智能体强化学习训练端到端基于RGB的策略，实现双臂物体抛接。通过人类策略正则化方案引导抛掷策略，鼓励流畅自然的动作。在Isaac Sim中构建了双臂仿真环境进行实验评估。

Result: DyDexHandover在训练对象上达到近99%的成功率，在未见对象上达到75%的成功率，同时生成了类人的抛接行为。据作者所知，这是首个仅使用原始RGB感知实现双臂空中交接的方法。

Conclusion: DyDexHandover成功解决了双臂机器人空中动态交接的难题，仅通过RGB感知实现了高成功率和类人行为，显著提升了泛化能力。

Abstract: Dynamic in air handover is a fundamental challenge for dual-arm robots,
requiring accurate perception, precise coordination, and natural motion. Prior
methods often rely on dynamics models, strong priors, or depth sensing,
limiting generalization and naturalness. We present DyDexHandover, a novel
framework that employs multi-agent reinforcement learning to train an end to
end RGB based policy for bimanual object throwing and catching. To achieve more
human-like behavior, the throwing policy is guided by a human policy
regularization scheme, encouraging fluid and natural motion, and enhancing the
generalization capability of the policy. A dual arm simulation environment was
built in Isaac Sim for experimental evaluation. DyDexHandover achieves nearly
99 percent success on training objects and 75 percent on unseen objects, while
generating human-like throwing and catching behaviors. To our knowledge, it is
the first method to realize dual-arm in-air handover using only raw RGB
perception.

</details>


### [470] [Fast Trajectory Planner with a Reinforcement Learning-based Controller for Robotic Manipulators](https://arxiv.org/abs/2509.17381)
*Yongliang Wang,Hamidreza Kasaei*

Main category: cs.RO

TL;DR: 本文提出了一种用于机械臂在复杂环境中进行无障碍轨迹规划的快速系统，该系统结合了基于视觉的任务空间路径规划和基于强化学习的关节空间避障。


<details>
  <summary>Details</summary>
Motivation: 在非结构化和杂乱环境中为机械臂生成无障碍轨迹仍然是一个重大挑战。现有运动规划方法通常需要额外的计算工作来通过求解运动学或动力学方程生成最终轨迹，且模型无关的强化学习方法在关节空间无障碍轨迹规划方面具有巨大潜力。

Method: 该系统分为两个关键部分：1. 任务空间中的创新型视觉轨迹规划器，利用大型快速分割一切（FSA）模型与B样条优化运动学路径搜索相结合。2. 增强型近端策略优化（PPO）算法，通过集成动作集成（AE）和策略反馈（PF），显著提高了关节空间中目标到达和避障的精度和稳定性。

Result: 实验结果表明，PPO增强功能有效提高了算法的适应性，确保了机械臂执行指令的一致性，同时增强了避障效率和到达精度。通过模拟到模拟（Sim-to-Sim）和模拟到真实（Sim-to-Real）的迁移，进一步验证了模型在复杂场景中鲁棒性和规划器效率的提升。

Conclusion: 所提出的增强功能使机器人能够在有障碍物的环境中执行避障和实时轨迹规划，有效解决了复杂环境中机械臂轨迹规划的挑战。

Abstract: Generating obstacle-free trajectories for robotic manipulators in
unstructured and cluttered environments remains a significant challenge.
Existing motion planning methods often require additional computational effort
to generate the final trajectory by solving kinematic or dynamic equations.
This paper highlights the strong potential of model-free reinforcement learning
methods over model-based approaches for obstacle-free trajectory planning in
joint space. We propose a fast trajectory planning system for manipulators that
combines vision-based path planning in task space with reinforcement
learning-based obstacle avoidance in joint space. We divide the framework into
two key components. The first introduces an innovative vision-based trajectory
planner in task space, leveraging the large-scale fast segment anything (FSA)
model in conjunction with basis spline (B-spline)-optimized kinodynamic path
searching. The second component enhances the proximal policy optimization (PPO)
algorithm by integrating action ensembles (AE) and policy feedback (PF), which
greatly improve precision and stability in goal-reaching and obstacle avoidance
within the joint space. These PPO enhancements increase the algorithm's
adaptability across diverse robotic tasks, ensuring consistent execution of
commands from the first component by the manipulator, while also enhancing both
obstacle avoidance efficiency and reaching accuracy. The experimental results
demonstrate the effectiveness of PPO enhancements, as well as
simulation-to-simulation (Sim-to-Sim) and simulation-to-reality (Sim-to-Real)
transfer, in improving model robustness and planner efficiency in complex
scenarios. These enhancements allow the robot to perform obstacle avoidance and
real-time trajectory planning in obstructed environments. Project page
available at: https://sites.google.com/view/ftp4rm/home

</details>


### [471] [High-Precision and High-Efficiency Trajectory Tracking for Excavators Based on Closed-Loop Dynamics](https://arxiv.org/abs/2509.17387)
*Ziqing Zou,Cong Wang,Yue Hu,Xiao Liu,Bowen Xu,Rong Xiong,Changjie Fan,Yingfeng Chen,Yue Wang*

Main category: cs.RO

TL;DR: 本文提出EfficientTrack，一种针对液压挖掘机轨迹跟踪的方法，通过结合基于模型的学习和闭环动力学，有效处理非线性并提高学习效率，实现高精度跟踪。


<details>
  <summary>Details</summary>
Motivation: 液压挖掘机复杂的非线性动力学（如时间延迟和控制耦合）使高精度轨迹跟踪面临巨大挑战。传统控制方法难以有效处理这些非线性，而常用的基于学习的方法需要大量环境交互，效率低下。

Method: 本文引入EfficientTrack方法，该方法整合了基于模型的学习来管理非线性动力学，并利用闭环动力学来提高学习效率，从而最小化跟踪误差。

Result: 仿真实验表明，EfficientTrack优于现有基于学习的方法，以最少的交互实现了最高的跟踪精度和流畅性。真实世界实验进一步证明，该方法在负载条件下依然有效，并具备持续学习能力，显示了其在实际应用中的可行性。

Conclusion: EfficientTrack是一种有效且实用的液压挖掘机高精度轨迹跟踪方法，能够处理复杂的非线性动力学并提高学习效率，适用于实际工业应用。

Abstract: The complex nonlinear dynamics of hydraulic excavators, such as time delays
and control coupling, pose significant challenges to achieving high-precision
trajectory tracking. Traditional control methods often fall short in such
applications due to their inability to effectively handle these nonlinearities,
while commonly used learning-based methods require extensive interactions with
the environment, leading to inefficiency. To address these issues, we introduce
EfficientTrack, a trajectory tracking method that integrates model-based
learning to manage nonlinear dynamics and leverages closed-loop dynamics to
improve learning efficiency, ultimately minimizing tracking errors. We validate
our method through comprehensive experiments both in simulation and on a
real-world excavator. Comparative experiments in simulation demonstrate that
our method outperforms existing learning-based approaches, achieving the
highest tracking precision and smoothness with the fewest interactions.
Real-world experiments further show that our method remains effective under
load conditions and possesses the ability for continual learning, highlighting
its practical applicability. For implementation details and source code, please
refer to https://github.com/ZiqingZou/EfficientTrack.

</details>


### [472] [3D Printable Soft Liquid Metal Sensors for Delicate Manipulation Tasks](https://arxiv.org/abs/2509.17389)
*Lois Liow,Jonty Milford,Emre Uygun,Andre Farinha,Vinoth Viswanathan,Josh Pinskier,David Howard*

Main category: cs.RO

TL;DR: 该研究引入了一种新方法，用于打印自由形态、高度传感化的软性“物理孪生体”，以安全地获取数据并训练机器人操作策略，尤其是在处理脆弱样本（如珊瑚）的保护工作中，从而实现伦理且可扩展的自动化。


<details>
  <summary>Details</summary>
Motivation: 机器人和自动化在保护工作中需要处理脆弱、易碎的样本而不造成损害。基于学习的解决方案（如强化学习）需要安全地获取数据以训练操作策略。

Method: 该研究提出了一种新的方法来打印自由形态、高度传感化的软性“物理孪生体”。它包括一个自动化设计流程，可以根据3D扫描或模型按需创建复杂且可定制的3D软传感结构。这些“物理孪生体”使用软液态金属传感器，能够忠实地再现复杂的自然几何形状。

Result: 所提出的软液态金属传感器能够忠实地再现复杂的自然几何形状并显示出优异的传感特性。作为“传感珊瑚”应用，它们消除了活体珊瑚实验的需要，提高了数据质量，并为自主珊瑚处理提供了一种伦理且可扩展的途径。该传感珊瑚能够在0.5牛顿以下检测抓取力，有效捕捉珊瑚处理所需的精细交互和轻微接触力。此外，该技术在自动化珊瑚标签和机器人珊瑚养殖中展示了其价值，提供了比传统传感器更丰富的抓取反馈。

Conclusion: 该研究的传感物理孪生体可以提供比传统传感器更丰富的抓取反馈，从而在部署前对处理脆弱和精细物品的操作进行实验验证，为推进自主操作提供了重要的工具，特别是在环境保护领域。

Abstract: Robotics and automation are key enablers to increase throughput in ongoing
conservation efforts across various threatened ecosystems. Cataloguing,
digitisation, husbandry, and similar activities require the ability to interact
with delicate, fragile samples without damaging them. Additionally,
learning-based solutions to these tasks require the ability to safely acquire
data to train manipulation policies through, e.g., reinforcement learning. To
address these twin needs, we introduce a novel method to print free-form,
highly sensorised soft 'physical twins'. We present an automated design
workflow to create complex and customisable 3D soft sensing structures on
demand from 3D scans or models. Compared to the state of the art, our soft
liquid metal sensors faithfully recreate complex natural geometries and display
excellent sensing properties suitable for validating performance in delicate
manipulation tasks. We demonstrate the application of our physical twins as
'sensing corals': high-fidelity, 3D printed replicas of scanned corals that
eliminate the need for live coral experimentation, whilst increasing data
quality, offering an ethical and scalable pathway for advancing autonomous
coral handling and soft manipulation broadly. Through extensive bench-top
manipulation and underwater grasping experiments, we show that our sensing
coral is able to detect grasps under 0.5 N, effectively capturing the delicate
interactions and light contact forces required for coral handling. Finally, we
showcase the value of our physical twins across two demonstrations: (i)
automated coral labelling for lab identification and (ii) robotic coral
aquaculture. Sensing physical twins such as ours can provide richer grasping
feedback than conventional sensors providing experimental validation of prior
to deployment in handling fragile and delicate items.

</details>


### [473] [FGGS-LiDAR: Ultra-Fast, GPU-Accelerated Simulation from General 3DGS Models to LiDAR](https://arxiv.org/abs/2509.17390)
*Junzhe Wu,Yufei Jia,Yiyi Yan,Zhixing Chen,Tiao Tan,Zifan Wang,Guangyu Wang*

Main category: cs.RO

TL;DR: FGGS-LiDAR框架将预训练的3D Gaussian Splatting模型转换为高保真、水密网格，并结合GPU加速的光线投射模块，实现高效率的LiDAR模拟，弥合了3DGS与LiDAR模拟之间的鸿沟。


<details>
  <summary>Details</summary>
Motivation: 尽管3D Gaussian Splatting (3DGS) 在真实感渲染方面取得了革命性进展，但其庞大的资产生态系统与机器人和自动驾驶所需的关键工具——高性能LiDAR模拟不兼容。

Method: 该方法通过一个通用的管线，包括体素离散化和截断有符号距离场（TSDF）提取，将任意预训练的3DGS模型转换为高保真、水密网格。随后，结合一个高度优化、GPU加速的光线投射模块，模拟LiDAR的回波。

Result: FGGS-LiDAR在室内外场景中验证了其卓越的几何保真度，并能以超过500 FPS的速度模拟LiDAR回波。该框架使得3DGS资产可以直接用于几何精确的深度感知。

Conclusion: 该框架弥合了3DGS资产与LiDAR模拟之间的差距，扩展了3DGS的实用性，使其超越了可视化，并为可扩展、多模态模拟解锁了新的能力。

Abstract: While 3D Gaussian Splatting (3DGS) has revolutionized photorealistic
rendering, its vast ecosystem of assets remains incompatible with
high-performance LiDAR simulation, a critical tool for robotics and autonomous
driving. We present \textbf{FGGS-LiDAR}, a framework that bridges this gap with
a truly plug-and-play approach. Our method converts \textit{any} pretrained
3DGS model into a high-fidelity, watertight mesh without requiring
LiDAR-specific supervision or architectural alterations. This conversion is
achieved through a general pipeline of volumetric discretization and Truncated
Signed Distance Field (TSDF) extraction. We pair this with a highly optimized,
GPU-accelerated ray-casting module that simulates LiDAR returns at over 500
FPS. We validate our approach on indoor and outdoor scenes, demonstrating
exceptional geometric fidelity; By enabling the direct reuse of 3DGS assets for
geometrically accurate depth sensing, our framework extends their utility
beyond visualization and unlocks new capabilities for scalable, multimodal
simulation. Our open-source implementation is available at
https://github.com/TATP-233/FGGS-LiDAR.

</details>


### [474] [Learning Dexterous Manipulation with Quantized Hand State](https://arxiv.org/abs/2509.17450)
*Ying Feng,Hongjie Fang,Yinong He,Jingjing Chen,Chenxi Wang,Zihao He,Ruonan Liu,Cewu Lu*

Main category: cs.RO

TL;DR: 为解决灵巧机器人操作中手臂和手部动作耦合导致手部动作主导的问题，本文提出了DQ-RISE策略。该策略通过量化手部状态并连续松弛手臂动作，实现了平衡高效的学习，促进了结构化和通用化的灵巧操作。


<details>
  <summary>Details</summary>
Motivation: 灵巧机器人手部操作需要精细控制和适应性，但高自由度导致手部和手臂动作紧密耦合，学习和控制困难。现有视觉运动策略将手臂和手部动作结合在单一空间中，常导致高维手部动作主导并损害手臂控制。

Method: 提出DQ-RISE策略。它量化手部状态以简化手部运动预测并保留关键模式，同时应用连续松弛，使手臂动作能与这些紧凑的手部状态共同扩散。这种设计使策略能够从数据中学习手臂-手部协调，并防止手部动作压倒动作空间。

Result: 实验表明，DQ-RISE实现了更平衡和高效的学习。

Conclusion: DQ-RISE为结构化和通用化的灵巧操作铺平了道路。

Abstract: Dexterous robotic hands enable robots to perform complex manipulations that
require fine-grained control and adaptability. Achieving such manipulation is
challenging because the high degrees of freedom tightly couple hand and arm
motions, making learning and control difficult. Successful dexterous
manipulation relies not only on precise hand motions, but also on accurate
spatial positioning of the arm and coordinated arm-hand dynamics. However, most
existing visuomotor policies represent arm and hand actions in a single
combined space, which often causes high-dimensional hand actions to dominate
the coupled action space and compromise arm control. To address this, we
propose DQ-RISE, which quantizes hand states to simplify hand motion prediction
while preserving essential patterns, and applies a continuous relaxation that
allows arm actions to diffuse jointly with these compact hand states. This
design enables the policy to learn arm-hand coordination from data while
preventing hand actions from overwhelming the action space. Experiments show
that DQ-RISE achieves more balanced and efficient learning, paving the way
toward structured and generalizable dexterous manipulation. Project website:
http://rise-policy.github.io/DQ-RISE/

</details>


### [475] [Morphologies of a sagging elastica with intrinsic sensing and actuation](https://arxiv.org/abs/2509.17572)
*Vishnu Deo Mishra,S Ganga Prasath*

Main category: cs.RO

TL;DR: 本文研究了软机器人在简单比例反馈（驱动与感知曲率成比例）下的形态变化，特别是在有限传感器和执行器条件下的矫直和复杂形状变形任务。研究揭示了形态不稳定性，并提出了在捕捉长短波特征之间的权衡，为设计有限能力的软设备提供了量化方法。


<details>
  <summary>Details</summary>
Motivation: 由于结构几何非线性、实验系统建模误差以及传感和反馈/驱动能力的限制，计算软机器人变形所需的驱动力矩通常很困难。

Method: 将软机器人建模为弹性体（elastica），采用简单的比例反馈策略（驱动力与感知曲率成比例）。通过指定宽度的滤波器来模拟实验中常见的有限数量的传感器和执行器。研究了通过补偿自重引起的下垂来矫直设备的简单任务，以及复杂的形状变形任务。

Result: 1. 在矫直任务中，设备会经历一系列形态不稳定性，这些不稳定性由重力弯曲数、无量纲传感/反馈增益和滤波器的缩放宽度定义。 2. 对于具有有限传感和驱动能力的复杂形状变形任务，在捕捉长波长和短波长特征之间存在权衡（由传感器间距和执行器尺寸决定）。 3. 对于固定滤波器宽度，当选择适当的驱动增益（其大小与滤波器宽度的平方成正比）时，形状变形误差最小。

Conclusion: 该模型提供了一个量化视角，用于研究和设计具有有限传感和驱动能力的细长软设备，以应对复杂的机动应用。

Abstract: The morphology of a slender soft-robot can be modified by sensing its shape
via sensors and exerting moments via actuators embedded along its body. The
actuating moments required to morph these soft-robots to a desired shape are
often difficult to compute due to the geometric non-linearity associated with
the structure, the errors in modeling the experimental system, and the
limitations in sensing and feedback/actuation capabilities. In this article, we
explore the effect of a simple feedback strategy (actuation being proportional
to the sensed curvature) on the shape of a soft-robot, modeled as an elastica.
The finite number of sensors and actuators, often seen in experiments, is
captured in the model via filters of specified widths. Using proportional
feedback, we study the simple task of straightening the device by compensating
for the sagging introduced by its self-weight. The device undergoes a hierarchy
of morphological instabilities defined in the phase-space given by the
gravito-bending number, non-dimensional sensing/feedback gain, and the scaled
width of the filter. For complex shape-morphing tasks, given a perfect model of
the device with limited sensing and actuating capabilities, we find that a
trade-off arises (set by the sensor spacing & actuator size) between capturing
the long and short wavelength features. We show that the error in
shape-morphing is minimal for a fixed filter width when we choose an
appropriate actuating gain (whose magnitude goes as a square of the filter
width). Our model provides a quantitative lens to study and design slender soft
devices with limited sensing and actuating capabilities for complex maneuvering
applications.

</details>


### [476] [GeCCo - a Generalist Contact-Conditioned Policy for Loco-Manipulation Skills on Legged Robots](https://arxiv.org/abs/2509.17582)
*Vassil Atanassov,Wanming Yu,Siddhant Gangapurwala,James Wilson,Ioannis Havoutis*

Main category: cs.RO

TL;DR: 本文提出GeCCo，一种通过深度强化学习训练的低级策略，能跟踪四足机器人的任意接触点，实现通用且模块化的控制，无需从头训练即可高效处理多种高级任务。


<details>
  <summary>Details</summary>
Motivation: 现代四足机器人运动方法常采用端到端的深度强化学习，但其可扩展性差，因为每个新问题或应用都需要耗时且迭代的奖励定义和调整。

Method: 本文提出了“通用接触条件策略”（GeCCo），一个通过深度强化学习训练的低级策略，能够跟踪四足机器人上的任意接触点。该方法通过结合任务特定的高级接触规划器和预训练的通用策略来获取新行为。

Result: GeCCo在各种运动（如不同步态、复杂地形、未曾见过的垫脚石和窄梁）和操作任务（如按按钮、轨迹跟踪）中展现出可扩展性和鲁棒性，所有任务均由单一通用策略在通用框架下完成。该框架能更高效地获取新行为。

Conclusion: GeCCo提供了一个通用且模块化的低级控制器，可以重复用于更广泛的高级任务，而无需从头训练新的控制器，有效解决了传统端到端深度强化学习在四足机器人运动中存在的扩展性问题。

Abstract: Most modern approaches to quadruped locomotion focus on using Deep
Reinforcement Learning (DRL) to learn policies from scratch, in an end-to-end
manner. Such methods often fail to scale, as every new problem or application
requires time-consuming and iterative reward definition and tuning. We present
Generalist Contact-Conditioned Policy (GeCCo) -- a low-level policy trained
with Deep Reinforcement Learning that is capable of tracking arbitrary contact
points on a quadruped robot. The strength of our approach is that it provides a
general and modular low-level controller that can be reused for a wider range
of high-level tasks, without the need to re-train new controllers from scratch.
We demonstrate the scalability and robustness of our method by evaluating on a
wide range of locomotion and manipulation tasks in a common framework and under
a single generalist policy. These include a variety of gaits, traversing
complex terrains (eg. stairs and slopes) as well as previously unseen
stepping-stones and narrow beams, and interacting with objects (eg. pushing
buttons, tracking trajectories). Our framework acquires new behaviors more
efficiently, simply by combining a task-specific high-level contact planner and
the pre-trained generalist policy. A supplementary video can be found at
https://youtu.be/o8Dd44MkG2E.

</details>


### [477] [Robust and Resilient Soft Robotic Object Insertion with Compliance-Enabled Contact Formation and Failure Recovery](https://arxiv.org/abs/2509.17666)
*Mimo Shirasaka,Cristian C. Beltran-Hernandez,Masashi Hamaya,Yoshitaka Ushiku*

Main category: cs.RO

TL;DR: 本文提出了一种利用被动柔顺软腕和视觉-语言模型（VLM）实现鲁棒和弹性物体插入的方法，通过柔顺性实现安全接触吸收和自动故障恢复。


<details>
  <summary>Details</summary>
Motivation: 物体插入任务在姿态不确定性和环境变化下容易失败，传统方法需要手动微调或重新训练控制器。

Method: 该方法使用被动柔顺软腕，通过大变形安全吸收接触，无需高频控制或力传感。它将插入结构化为柔顺性使能的接触形成（逐步约束自由度的顺序接触状态），并集成了自动化故障恢复策略。关键在于腕部柔顺性允许安全、重复的恢复尝试。一个预训练的视觉-语言模型（VLM）用于评估每次技能执行（基于最终姿态和图像），识别故障模式，并通过选择技能和更新目标来提出恢复动作。

Result: 在模拟中，该方法实现了83%的成功率，能够从随机条件引起的故障中恢复，包括高达5度的抓取未对准、高达20毫米的孔姿态误差、摩擦力增加五倍以及以前未见的方形/矩形插销。该方法还在真实机器人上得到了验证。

Conclusion: 结合柔顺腕部和VLM驱动的故障恢复策略，该方法为在不确定性下的物体插入任务提供了一种鲁棒且有弹性的解决方案。

Abstract: Object insertion tasks are prone to failures under pose uncertainties and
environmental variations, traditionally requiring manual finetuning or
controller retraining. We present a novel approach for robust and resilient
object insertion using a passively compliant soft wrist that enables safe
contact absorption through large deformations, without high-frequency control
or force sensing. Our method structures insertion as compliance-enabled contact
formations, sequential contact states that progressively constrain degrees of
freedom, and integrates automated failure recovery strategies. Our key insight
is that wrist compliance permits safe, repeated recovery attempts; hence, we
refer to it as compliance-enabled failure recovery. We employ a pre-trained
vision-language model (VLM) that assesses each skill execution from terminal
poses and images, identifies failure modes, and proposes recovery actions by
selecting skills and updating goals. In simulation, our method achieved an 83%
success rate, recovering from failures induced by randomized
conditions--including grasp misalignments up to 5 degrees, hole-pose errors up
to 20mm, fivefold increases in friction, and previously unseen
square/rectangular pegs--and we further validate the approach on a real robot.

</details>


### [478] [Towards Learning Boulder Excavation with Hydraulic Excavators](https://arxiv.org/abs/2509.17683)
*Jonas Gruetter,Lorenzo Terenzi,Pascal Egli,Marco Hutter*

Main category: cs.RO

TL;DR: 本文提出了一种基于强化学习的自主挖掘机策略，使用标准挖斗在恶劣户外环境中抓取不规则大石块，在现场测试中取得了70%的成功率。


<details>
  <summary>Details</summary>
Motivation: 建筑工地经常需要移除大石块，但现有自主挖掘方法主要针对连续介质或使用专用抓手进行精细几何规划，无法处理不规则大石块或需要耗时的工具更换。人类操作员则能高效地使用标准挖斗完成此任务，尽管面临复杂环境、物体不规则性和感知挑战。

Method: 研究人员在模拟环境中训练了一个强化学习策略，该策略结合了刚体动力学和分析性土壤模型。策略的输入是来自视觉分割的稀疏激光雷达点（每块石头仅20个）和本体感受反馈，用于控制标准挖掘机挖斗。

Result: 学习到的智能体能够根据土壤阻力发现不同的策略：在硬土中沿表面拖动，在软土中直接穿透。在12吨挖掘机上的现场测试中，该方法在不同类型（0.4-0.7米）的岩石和土壤条件下取得了70%的成功率，而人类操作员的成功率为83%。

Conclusion: 研究表明，尽管感知稀疏且面临严峻的户外条件，标准建筑设备仍能学习复杂的操纵任务，有效地处理不规则大石块。

Abstract: Construction sites frequently require removing large rocks before excavation
or grading can proceed. Human operators typically extract these boulders using
only standard digging buckets, avoiding time-consuming tool changes to
specialized grippers. This task demands manipulating irregular objects with
unknown geometries in harsh outdoor environments where dust, variable lighting,
and occlusions hinder perception. The excavator must adapt to varying soil
resistance--dragging along hard-packed surfaces or penetrating soft
ground--while coordinating multiple hydraulic joints to secure rocks using a
shovel. Current autonomous excavation focuses on continuous media (soil,
gravel) or uses specialized grippers with detailed geometric planning for
discrete objects. These approaches either cannot handle large irregular rocks
or require impractical tool changes that interrupt workflow. We train a
reinforcement learning policy in simulation using rigid-body dynamics and
analytical soil models. The policy processes sparse LiDAR points (just 20 per
rock) from vision-based segmentation and proprioceptive feedback to control
standard excavator buckets. The learned agent discovers different strategies
based on soil resistance: dragging along the surface in hard soil and
penetrating directly in soft conditions. Field tests on a 12-ton excavator
achieved 70% success across varied rocks (0.4-0.7m) and soil types, compared to
83% for human operators. This demonstrates that standard construction equipment
can learn complex manipulation despite sparse perception and challenging
outdoor conditions.

</details>


### [479] [MotionTrans: Human VR Data Enable Motion-Level Learning for Robotic Manipulation Policies](https://arxiv.org/abs/2509.17759)
*Chengbo Yuan,Rui Zhou,Mengzhen Liu,Yingdong Hu,Shengjie Wang,Li Yi,Chuan Wen,Shanghang Zhang,Yang Gao*

Main category: cs.RO

TL;DR: 本文提出MotionTrans框架，通过多任务人机协同训练，将人类数据中的运动知识直接迁移到机器人策略中，实现了零样本成功并显著提升了预训练-微调性能，揭示了人类数据在机器人运动学习方面的巨大潜力。


<details>
  <summary>Details</summary>
Motivation: 机器人模仿学习中，真实机器人数据规模受限是关键瓶颈，尤其在运动知识获取方面。尽管人类数据具有丰富的操作行为多样性，但其能否直接帮助机器人学习新任务运动尚不明确，以往研究虽显示其能提升鲁棒性和训练效率，但未充分探索其在直接运动学习上的最大优势。

Method: 本文提出了MotionTrans框架，包含数据收集系统、人类数据转换流程和加权协同训练策略。通过同时协同训练30个人机任务，将运动直接从人类数据迁移到可部署的端到端机器人策略中。

Result: MotionTrans成功将13个任务的运动从人类数据直接迁移到机器人策略，其中9个任务实现了显著的零样本成功率。此外，它显著提升了预训练-微调性能（成功率提高40%）。消融研究发现，与机器人数据协同训练和广泛的任务相关运动覆盖是运动学习成功的关键因素。

Conclusion: 研究结果解锁了人类数据在运动层面上学习的潜力，为有效利用人类数据训练机器人操作策略提供了深刻见解，尤其强调了协同训练和广泛运动覆盖的重要性。

Abstract: Scaling real robot data is a key bottleneck in imitation learning, leading to
the use of auxiliary data for policy training. While other aspects of robotic
manipulation such as image or language understanding may be learned from
internet-based datasets, acquiring motion knowledge remains challenging. Human
data, with its rich diversity of manipulation behaviors, offers a valuable
resource for this purpose. While previous works show that using human data can
bring benefits, such as improving robustness and training efficiency, it
remains unclear whether it can realize its greatest advantage: enabling robot
policies to directly learn new motions for task completion. In this paper, we
systematically explore this potential through multi-task human-robot
cotraining. We introduce MotionTrans, a framework that includes a data
collection system, a human data transformation pipeline, and a weighted
cotraining strategy. By cotraining 30 human-robot tasks simultaneously, we
direcly transfer motions of 13 tasks from human data to deployable end-to-end
robot policies. Notably, 9 tasks achieve non-trivial success rates in zero-shot
manner. MotionTrans also significantly enhances pretraining-finetuning
performance (+40% success rate). Through ablation study, we also identify key
factors for successful motion learning: cotraining with robot data and broad
task-related motion coverage. These findings unlock the potential of
motion-level learning from human data, offering insights into its effective use
for training robotic manipulation policies. All data, code, and model weights
are open-sourced https://motiontrans.github.io/.

</details>


### [480] [Enhancing the NAO: Extending Capabilities of Legacy Robots for Long-Term Research](https://arxiv.org/abs/2509.17760)
*Austin Wilson,Sahar Kapasi,Zane Greene,Alexis E. Block*

Main category: cs.RO

TL;DR: 本文提出并验证了“增强版NAO”机器人，通过升级传感、计算资源和结合云/本地模型，显著提升了传统NAO机器人的对话质量和用户偏好，并为延长旧版机器人寿命提供了通用策略。


<details>
  <summary>Details</summary>
Motivation: 许多研究团队面临旧版（不受支持的）机器人平台失去制造商支持，无法适应现代传感、语音和交互能力的挑战。

Method: 研究人员对Aldebaran的NAO机器人进行了升级，命名为“增强版NAO”。主要方法包括：升级麦克风（如波束成形麦克风）、增加RGB-D和热像头、增加计算资源，并将所有这些集成在一个自给自足的封装中。系统结合了云端和本地模型进行感知和对话处理，同时保留了NAO原有的表达性身体和行为。

Result: 在一项初步验证研究中，“增强版NAO”与NAO AI版本相比，提供了显著更高的对话质量和更强的用户偏好，且没有增加响应延迟。关键升级（如波束成形麦克风和低延迟音频处理）减少了自听等伪影，并改善了多方分离。扩展的视觉和热传感为未来的交互能力奠定了基础。

Conclusion: “增强版NAO”成功提升了传统机器人的交互能力。该框架提供了一种与平台无关的策略，可以延长旧版机器人的寿命和研究实用性，确保它们在人机交互领域仍是宝贵的工具。

Abstract: Many research groups face challenges when legacy (unsupported) robotic
platforms lose manufacturer support and cannot accommodate modern sensing,
speech, and interaction capabilities. We present the Enhanced NAO, a
revitalized version of Aldebaran's NAO robot that uses upgraded microphones,
RGB-D and thermal cameras, and additional compute resources in a fully
self-contained package. This system combines cloud and local models for
perception and dialogue, while preserving the NAO's expressive body and
behaviors. In a pilot validation study, the Enhanced NAO delivered
significantly higher conversational quality and stronger user preference
compared to the NAO AI Edition, without increasing response latency. Key
upgrades, such as beamforming microphones and low-latency audio processing,
reduced artifacts like self-hearing and improved multi-party separation.
Expanded visual and thermal sensing established a foundation for future
interaction capabilities. Beyond the NAO, our framework provides a
platform-agnostic strategy for extending the lifespan and research utility of
legacy robots, ensuring they remain valuable tools for human-robot interaction.

</details>


### [481] [RoboSeek: You Need to Interact with Your Objects](https://arxiv.org/abs/2509.17783)
*Yibo Peng,Jiahao Yang,Shenhao Yan,Ziyu Huang,Shuang Li,Shuguang Cui,Yiming Zhao,Yatong Han*

Main category: cs.RO

TL;DR: RoboSeek是一个受具身认知启发的机器人具身动作执行框架，通过交互式经验和real2sim2real迁移，在长周期操作任务中实现了高成功率和强大的泛化性。


<details>
  <summary>Details</summary>
Motivation: 交互驱动的机器人学习在长周期任务中仍未被充分探索，这些任务涉及顺序决策、物理约束和感知不确定性等挑战。本文受具身认知理论启发，旨在解决这些问题。

Method: 本文提出了RoboSeek框架。首先，利用3D重建在仿真中复制真实世界环境，确保视觉和物理一致性。然后，在仿真中使用强化学习和交叉熵方法，结合视觉先验知识训练策略。最后，通过real2sim2real迁移管道，将学习到的策略部署到真实的机器人平台上进行执行。该方法与硬件无关。

Result: RoboSeek在多个机器人平台和八个涉及顺序交互、工具使用和物体处理的长周期操作任务上进行了评估。平均成功率达到79%，显著优于成功率低于50%的基线方法，突出了其在任务和平台上的泛化性和鲁棒性。实验结果验证了该训练框架在复杂、动态的真实世界环境中的有效性，并展示了所提出的real2sim2real迁移机制的稳定性。

Conclusion: RoboSeek是一个在复杂动态真实世界环境中有效的训练框架，展示了real2sim2real迁移机制的稳定性，为更具泛化性的具身机器人学习铺平了道路。

Abstract: Optimizing and refining action execution through
  exploration and interaction is a promising way for robotic
  manipulation. However, practical approaches to interaction driven robotic
learning are still underexplored, particularly for
  long-horizon tasks where sequential decision-making, physical
  constraints, and perceptual uncertainties pose significant chal lenges.
Motivated by embodied cognition theory, we propose
  RoboSeek, a framework for embodied action execution that
  leverages interactive experience to accomplish manipulation
  tasks. RoboSeek optimizes prior knowledge from high-level
  perception models through closed-loop training in simulation
  and achieves robust real-world execution via a real2sim2real
  transfer pipeline. Specifically, we first replicate real-world
  environments in simulation using 3D reconstruction to provide
  visually and physically consistent environments., then we train
  policies in simulation using reinforcement learning and the
  cross-entropy method leveraging visual priors. The learned
  policies are subsequently deployed on real robotic platforms
  for execution. RoboSeek is hardware-agnostic and is evaluated
  on multiple robotic platforms across eight long-horizon ma nipulation tasks
involving sequential interactions, tool use, and
  object handling. Our approach achieves an average success rate
  of 79%, significantly outperforming baselines whose success
  rates remain below 50%, highlighting its generalization and
  robustness across tasks and platforms. Experimental results
  validate the effectiveness of our training framework in complex,
  dynamic real-world settings and demonstrate the stability of the
  proposed real2sim2real transfer mechanism, paving the way for
  more generalizable embodied robotic learning. Project Page:
  https://russderrick.github.io/Roboseek/

</details>


### [482] [Tac2Motion: Contact-Aware Reinforcement Learning with Tactile Feedback for Robotic Hand Manipulation](https://arxiv.org/abs/2509.17812)
*Yitaek Kim,Casper Hewson Rask,Christoffer Sloth*

Main category: cs.RO

TL;DR: 本文提出了Tac2Motion，一个接触感知的强化学习框架，利用触觉传感实现复杂的手内操作任务，并成功迁移到真实机器人。


<details>
  <summary>Details</summary>
Motivation: 解决移除瓶盖等接触丰富的手内操作任务学习困难的问题。

Method: 提出Tac2Motion框架，通过基于触觉传感的奖励塑形，并将触觉传感嵌入到观察空间中。奖励设计旨在同时确保牢固抓取和平稳的手指步态。

Result: 相比基线，实现了更高的数据效率和更鲁棒的性能。训练策略可泛化到多种物体类型和不同动力学（如扭转摩擦）。学习到的策略成功迁移到多指机器人（Shadow Robot）上。

Conclusion: Tac2Motion框架能有效学习接触丰富的机械手操作任务，且训练策略具有泛化性，并可成功部署到真实机器人上。

Abstract: This paper proposes Tac2Motion, a contact-aware reinforcement learning
framework to facilitate the learning of contact-rich in-hand manipulation
tasks, such as removing a lid. To this end, we propose tactile sensing-based
reward shaping and incorporate the sensing into the observation space through
embedding. The designed rewards encourage an agent to ensure firm grasping and
smooth finger gaiting at the same time, leading to higher data efficiency and
robust performance compared to the baseline. We verify the proposed framework
on the opening a lid scenario, showing generalization of the trained policy
into a couple of object types and various dynamics such as torsional friction.
Lastly, the learned policy is demonstrated on the multi-fingered robot, Shadow
Robot, showing that the control policy can be transferred to the real world.
The video is available: https://youtu.be/poeJBPR7urQ.

</details>


### [483] [SocialTraj: Two-Stage Socially-Aware Trajectory Prediction for Autonomous Driving via Conditional Diffusion Model](https://arxiv.org/abs/2509.17850)
*Xiao Zhou,Zengqi Peng,Jun Ma*

Main category: cs.RO

TL;DR: SocialTraj是一个新颖的轨迹预测框架，通过贝叶斯逆强化学习估计社会价值取向（SVO）并将SVO嵌入条件去噪扩散模型中，以捕捉驾驶员的多模态行为，从而在复杂交通场景中生成更准确、社会兼容且行为一致的轨迹预测。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统需要准确预测周围车辆轨迹以避免事故，但现有方法难以捕捉驾驶员的多模态行为，导致预测轨迹与实际未来运动存在偏差，这是在高度动态和复杂交通场景中实现可靠预测的主要挑战。

Method: 该研究提出了SocialTraj框架：1. 利用贝叶斯逆强化学习（IRL）估计周围车辆的社会价值取向（SVO），以获取关键社会背景并推断未来交互趋势。2. 将估计的SVO嵌入条件去噪扩散模型中，以确保生成轨迹与历史驾驶风格保持模态一致性。3. 明确纳入自我车辆（EV）的未来规划轨迹，以增强交互建模。

Result: 在NGSIM和HighD数据集上进行的广泛实验表明，SocialTraj能够适应高度动态和交互性场景，生成符合社会规范且行为一致的轨迹预测，并且优于现有基线方法。消融研究表明，动态SVO估计和明确的自我规划组件显著提高了预测准确性并大幅减少了推理时间。

Conclusion: SocialTraj通过整合社会心理学原理（SVO）和条件去噪扩散模型，有效解决了复杂交通场景中多模态驾驶行为的轨迹预测难题，从而生成了更可靠、社会兼容且行为一致的预测结果，提升了自动驾驶系统的安全性。

Abstract: Accurate trajectory prediction of surrounding vehicles (SVs) is crucial for
autonomous driving systems to avoid misguided decisions and potential
accidents. However, achieving reliable predictions in highly dynamic and
complex traffic scenarios remains a significant challenge. One of the key
impediments lies in the limited effectiveness of current approaches to capture
the multi-modal behaviors of drivers, which leads to predicted trajectories
that deviate from actual future motions. To address this issue, we propose
SocialTraj, a novel trajectory prediction framework integrating social
psychology principles through social value orientation (SVO). By utilizing
Bayesian inverse reinforcement learning (IRL) to estimate the SVO of SVs, we
obtain the critical social context to infer the future interaction trend. To
ensure modal consistency in predicted behaviors, the estimated SVOs of SVs are
embedded into a conditional denoising diffusion model that aligns generated
trajectories with historical driving styles. Additionally, the planned future
trajectory of the ego vehicle (EV) is explicitly incorporated to enhance
interaction modeling. Extensive experiments on NGSIM and HighD datasets
demonstrate that SocialTraj is capable of adapting to highly dynamic and
interactive scenarios while generating socially compliant and behaviorally
consistent trajectory predictions, outperforming existing baselines. Ablation
studies demonstrate that dynamic SVO estimation and explicit ego-planning
components notably improve prediction accuracy and substantially reduce
inference time.

</details>


### [484] [Sight Over Site: Perception-Aware Reinforcement Learning for Efficient Robotic Inspection](https://arxiv.org/abs/2509.17877)
*Richard Kuhlmann,Jakob Wolfram,Boyang Sun,Jiaxu Xing,Davide Scaramuzza,Marc Pollefeys,Cesar Cadena*

Main category: cs.RO

TL;DR: 该研究提出了一种基于感知感知的强化学习框架，用于机器人自主巡检，以最短路径实现目标可见性，无需地图，并在模拟和真实环境中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统的机器人巡检常被简化为导航任务，但实际问题中，目标可见性比到达精确坐标更重要。机器人应定位在能观察到目标的视角，而非仅仅到达其位置，这促使研究者从感知感知的角度重新审视巡检问题。

Method: 本文提出了一种端到端强化学习框架，将目标可见性作为主要优化目标。该框架使机器人无需地图即可找到最短路径以保证与目标的视觉接触。学习到的策略利用感知和本体感知信息，完全在模拟中训练，然后部署到真实机器人。此外，还开发了一种计算地面真实最短巡检路径的算法作为评估参考。

Result: 通过大量实验证明，该方法在模拟和真实世界环境中都优于现有的经典和基于学习的导航方法，能够生成更高效的巡检轨迹。

Conclusion: 该研究成功地将巡检问题从传统的导航任务转向以目标可见性为核心的感知感知范式，并通过强化学习实现了无地图、高效的自主巡检，显著提升了机器人巡检的效率和实用性。

Abstract: Autonomous inspection is a central problem in robotics, with applications
ranging from industrial monitoring to search-and-rescue. Traditionally,
inspection has often been reduced to navigation tasks, where the objective is
to reach a predefined location while avoiding obstacles. However, this
formulation captures only part of the real inspection problem. In real-world
environments, the inspection targets may become visible well before their exact
coordinates are reached, making further movement both redundant and
inefficient. What matters more for inspection is not simply arriving at the
target's position, but positioning the robot at a viewpoint from which the
target becomes observable. In this work, we revisit inspection from a
perception-aware perspective. We propose an end-to-end reinforcement learning
framework that explicitly incorporates target visibility as the primary
objective, enabling the robot to find the shortest trajectory that guarantees
visual contact with the target without relying on a map. The learned policy
leverages both perceptual and proprioceptive sensing and is trained entirely in
simulation, before being deployed to a real-world robot. We further develop an
algorithm to compute ground-truth shortest inspection paths, which provides a
reference for evaluation. Through extensive experiments, we show that our
method outperforms existing classical and learning-based navigation approaches,
yielding more efficient inspection trajectories in both simulated and
real-world settings. The project is avialable at
https://sight-over-site.github.io/

</details>


### [485] [The Surprising Effectiveness of Linear Models for Whole-Body Model-Predictive Control](https://arxiv.org/abs/2509.17884)
*Arun L. Bishop,Juan Alvarez-Padilla,Sam Schoedel,Ibrahima Sory Sow,Juee Chandrachud,Sheitej Sharma,Will Kraus,Beomyeong Park,Robert J. Griffin,John M. Dolan,Zachary Manchester*

Main category: cs.RO

TL;DR: 本文展示了一种基于简单线性时不变近似的全身模型预测控制器，能够使复杂腿足机器人在不进行在线非线性动力学评估或矩阵求逆的情况下，执行基本的运动任务。


<details>
  <summary>Details</summary>
Motivation: 研究运动控制器何时需要对非线性进行推理，并探讨一种简化的线性模型预测控制器是否足以处理复杂的腿足机器人运动任务。

Method: 采用全身模型预测控制器（MPC），该控制器使用全身动力学的简单线性时不变（LTI）近似。此方法无需在线进行非线性动力学评估或矩阵求逆。

Result: 在四足机器人上实现了行走、扰动抑制以及无需单独足迹规划的导航至目标位置。此外，还在具有显著肢体惯性、复杂执行器动力学和较大仿真-真实差距的液压人形机器人上展示了动态行走。

Conclusion: 一个简单的线性时不变近似的全身模型预测控制器足以使复杂腿足机器人执行基本的运动任务，这表明并非总是需要对非线性进行显式推理。

Abstract: When do locomotion controllers require reasoning about nonlinearities? In
this work, we show that a whole-body model-predictive controller using a simple
linear time-invariant approximation of the whole-body dynamics is able to
execute basic locomotion tasks on complex legged robots. The formulation
requires no online nonlinear dynamics evaluations or matrix inversions. We
demonstrate walking, disturbance rejection, and even navigation to a goal
position without a separate footstep planner on a quadrupedal robot. In
addition, we demonstrate dynamic walking on a hydraulic humanoid, a robot with
significant limb inertia, complex actuator dynamics, and large sim-to-real gap.

</details>


### [486] [DriveDPO: Policy Learning via Safety DPO For End-to-End Autonomous Driving](https://arxiv.org/abs/2509.17940)
*Shuyao Shang,Yuntao Chen,Yuqi Wang,Yingyan Li,Zhaoxiang Zhang*

Main category: cs.RO

TL;DR: 本文提出了DriveDPO，一个基于安全直接偏好优化的策略学习框架，旨在解决端到端自动驾驶中模仿学习的安全局限性，通过结合人类模仿相似性和基于规则的安全分数，并引入迭代轨迹级偏好对齐，实现了更安全、更可靠的驾驶行为。


<details>
  <summary>Details</summary>
Motivation: 端到端自动驾驶的模仿学习方法存在严重的安全限制，无法区分看似像人类但可能不安全的轨迹。近期尝试通过回归多个规则驱动分数的方法，其监督与策略优化解耦，导致次优性能。

Method: 本文提出了DriveDPO框架。首先，从人类模仿相似性和基于规则的安全分数中提取统一的策略分布，用于直接策略优化。其次，引入了一个迭代的直接偏好优化（DPO）阶段，该阶段被表述为轨迹级别的偏好对齐。

Result: 在NAVSIM基准测试中，DriveDPO实现了90.0的PDMS新SOTA（State-of-the-Art）性能。此外，在各种挑战性场景下的定性结果表明，DriveDPO能够产生更安全、更可靠的驾驶行为。

Conclusion: DriveDPO通过直接偏好优化，有效地解决了端到端自动驾驶中模仿学习的安全问题，显著提高了系统在复杂场景下的安全性和可靠性，达到了SOTA水平。

Abstract: End-to-end autonomous driving has substantially progressed by directly
predicting future trajectories from raw perception inputs, which bypasses
traditional modular pipelines. However, mainstream methods trained via
imitation learning suffer from critical safety limitations, as they fail to
distinguish between trajectories that appear human-like but are potentially
unsafe. Some recent approaches attempt to address this by regressing multiple
rule-driven scores but decoupling supervision from policy optimization,
resulting in suboptimal performance. To tackle these challenges, we propose
DriveDPO, a Safety Direct Preference Optimization Policy Learning framework.
First, we distill a unified policy distribution from human imitation similarity
and rule-based safety scores for direct policy optimization. Further, we
introduce an iterative Direct Preference Optimization stage formulated as
trajectory-level preference alignment. Extensive experiments on the NAVSIM
benchmark demonstrate that DriveDPO achieves a new state-of-the-art PDMS of
90.0. Furthermore, qualitative results across diverse challenging scenarios
highlight DriveDPO's ability to produce safer and more reliable driving
behaviors.

</details>


### [487] [ComposableNav: Instruction-Following Navigation in Dynamic Environments via Composable Diffusion](https://arxiv.org/abs/2509.17941)
*Zichao Hu,Chen Tang,Michael J. Munje,Yifeng Zhu,Alex Liu,Shuijing Liu,Garrett Warnell,Peter Stone,Joydeep Biswas*

Main category: cs.RO

TL;DR: 本文提出ComposableNav，一种基于扩散模型的机器人导航方法，通过学习和并行组合独立的运动原语来处理动态环境中具有组合性质的复杂指令，并在模拟和实际实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 机器人需要在动态环境中遵循指令进行导航，但指令通常包含多个规范，导致规范组合呈指数级增长，现有方法难以应对这种组合性挑战。

Method: 本文提出ComposableNav，其核心思想是指令的每个组成规范对应一个独立的运动原语，并通过独立满足这些原语来执行指令。该方法使用扩散模型单独学习每个运动原语，并在部署时并行组合它们以满足训练中未见的新规范组合。为避免对单个运动原语进行演示的需求，采用两阶段训练：(1) 监督预训练，学习动态导航的基础扩散模型；(2) 强化学习微调，将基础模型塑造成不同的运动原语。

Result: ComposableNav使机器人能够通过生成满足多样化且未见过的规范组合的轨迹来遵循指令。在模拟和实际实验中，其性能显著优于非组合VLM（视觉语言模型）策略和成本图组合基线。

Conclusion: ComposableNav通过创新的运动原语学习和组合方法，成功解决了机器人遵循动态环境中组合指令的挑战，展现出卓越的导航能力和泛化性。

Abstract: This paper considers the problem of enabling robots to navigate dynamic
environments while following instructions. The challenge lies in the
combinatorial nature of instruction specifications: each instruction can
include multiple specifications, and the number of possible specification
combinations grows exponentially as the robot's skill set expands. For example,
"overtake the pedestrian while staying on the right side of the road" consists
of two specifications: "overtake the pedestrian" and "walk on the right side of
the road." To tackle this challenge, we propose ComposableNav, based on the
intuition that following an instruction involves independently satisfying its
constituent specifications, each corresponding to a distinct motion primitive.
Using diffusion models, ComposableNav learns each primitive separately, then
composes them in parallel at deployment time to satisfy novel combinations of
specifications unseen in training. Additionally, to avoid the onerous need for
demonstrations of individual motion primitives, we propose a two-stage training
procedure: (1) supervised pre-training to learn a base diffusion model for
dynamic navigation, and (2) reinforcement learning fine-tuning that molds the
base model into different motion primitives. Through simulation and real-world
experiments, we show that ComposableNav enables robots to follow instructions
by generating trajectories that satisfy diverse and unseen combinations of
specifications, significantly outperforming both non-compositional VLM-based
policies and costmap composing baselines. Videos and additional materials can
be found on the project page: https://amrl.cs.utexas.edu/ComposableNav/

</details>


### [488] [M3ET: Efficient Vision-Language Learning for Robotics based on Multimodal Mamba-Enhanced Transformer](https://arxiv.org/abs/2509.18005)
*Yanxin Zhang,Liang He,Zeyi Kang,Zuheng Ming,Kaixing Zhao*

Main category: cs.RO

TL;DR: 本文提出M3ET模型，这是一种轻量级、高效的多模态学习模型，专为解决现有方法在文本模态利用不足、计算量大等问题而设计，特别适用于资源受限的机器人平台。


<details>
  <summary>Details</summary>
Motivation: 当前多模态学习方法在机器人视觉和信息融合中，难以充分利用文本模态，过度依赖有监督预训练模型，限制了无监督机器人环境下的语义提取，尤其是在模态损失严重时。此外，现有方法计算成本高昂，导致实际应用中资源消耗大。

Method: 本文提出了多模态Mamba增强型Transformer (M3ET) 模型。它通过整合Mamba模块和基于语义的自适应注意力机制，优化了特征融合、对齐和模态重建，旨在实现高效的多模态学习，特别是在移动平台上。

Result: 实验表明，M3ET提升了跨任务性能，预训练推理速度提高了2.3倍。在核心VQA任务上，M3ET的准确率保持在0.74，同时模型参数量减少了0.67。尽管在EQA任务上的性能有限。

Conclusion: M3ET的轻量级设计使其非常适合部署在资源受限的机器人平台上，为高效多模态学习提供了新的解决方案。

Abstract: In recent years, multimodal learning has become essential in robotic vision
and information fusion, especially for understanding human behavior in complex
environments. However, current methods struggle to fully leverage the textual
modality, relying on supervised pretrained models, which limits semantic
extraction in unsupervised robotic environments, particularly with significant
modality loss. These methods also tend to be computationally intensive, leading
to high resource consumption in real-world applications. To address these
challenges, we propose the Multi Modal Mamba Enhanced Transformer (M3ET), a
lightweight model designed for efficient multimodal learning, particularly on
mobile platforms. By incorporating the Mamba module and a semantic-based
adaptive attention mechanism, M3ET optimizes feature fusion, alignment, and
modality reconstruction. Our experiments show that M3ET improves cross-task
performance, with a 2.3 times increase in pretraining inference speed. In
particular, the core VQA task accuracy of M3ET remains at 0.74, while the
model's parameter count is reduced by 0.67. Although performance on the EQA
task is limited, M3ET's lightweight design makes it well suited for deployment
on resource-constrained robotic platforms.

</details>


### [489] [V2V-GoT: Vehicle-to-Vehicle Cooperative Autonomous Driving with Multimodal Large Language Models and Graph-of-Thoughts](https://arxiv.org/abs/2509.18053)
*Hsu-kuang Chiu,Ryo Hachiuma,Chien-Yi Wang,Yu-Chiang Frank Wang,Min-Hung Chen,Stephen F. Smith*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的思维图（graph-of-thoughts）框架，专门用于基于多模态大语言模型（MLLM）的协同自动驾驶，以解决传感器遮挡问题，并在感知、预测和规划任务中超越基线。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶车辆在传感器被遮挡时面临安全风险。车对车（V2V）协同驾驶被提出解决此问题，其中基于MLLM的协同框架进一步整合了感知和规划。然而，以往研究并未将思维图推理应用于MLLM，以增强协同自动驾驶。

Method: 本文提出了一种针对MLLM协同自动驾驶的思维图框架，其中包含“遮挡感知感知”（occlusion-aware perception）和“规划感知预测”（planning-aware prediction）两个新颖概念。研究者还构建了V2V-GoT-QA数据集，并开发了V2V-GoT模型进行训练和测试。

Result: 实验结果表明，该方法在协同感知、预测和规划任务中均优于其他基线方法。

Conclusion: 所提出的思维图框架显著提升了基于MLLM的协同自动驾驶性能，特别是在处理传感器遮挡等复杂场景方面。

Abstract: Current state-of-the-art autonomous vehicles could face safety-critical
situations when their local sensors are occluded by large nearby objects on the
road. Vehicle-to-vehicle (V2V) cooperative autonomous driving has been proposed
as a means of addressing this problem, and one recently introduced framework
for cooperative autonomous driving has further adopted an approach that
incorporates a Multimodal Large Language Model (MLLM) to integrate cooperative
perception and planning processes. However, despite the potential benefit of
applying graph-of-thoughts reasoning to the MLLM, this idea has not been
considered by previous cooperative autonomous driving research. In this paper,
we propose a novel graph-of-thoughts framework specifically designed for
MLLM-based cooperative autonomous driving. Our graph-of-thoughts includes our
proposed novel ideas of occlusion-aware perception and planning-aware
prediction. We curate the V2V-GoT-QA dataset and develop the V2V-GoT model for
training and testing the cooperative driving graph-of-thoughts. Our
experimental results show that our method outperforms other baselines in
cooperative perception, prediction, and planning tasks.

</details>


### [490] [RadarSFD: Single-Frame Diffusion with Pretrained Priors for Radar Point Clouds](https://arxiv.org/abs/2509.18068)
*Bin Zhao,Nakul Garg*

Main category: cs.RO

TL;DR: RadarSFD是一种条件潜在扩散框架，能从单帧毫米波雷达数据重建高密度点云，无需运动或合成孔径雷达（SAR），适用于小型机器人平台。


<details>
  <summary>Details</summary>
Motivation: 毫米波雷达在恶劣环境（如雾、烟、尘、弱光）下感知鲁棒，但现有雷达成像方法依赖合成孔径或多帧聚合来提高分辨率，这对于小型空中、检测或可穿戴机器人平台来说不切实际。

Method: 本文提出了RadarSFD，一个条件潜在扩散框架，用于从单帧雷达数据重建类似LiDAR的密集点云，无需运动或SAR。该方法将预训练单目深度估计器的几何先验知识转移到扩散骨干网络中，通过通道级潜在拼接将其锚定到雷达输入，并通过结合潜在空间和像素空间损失的双空间目标来规范输出。

Result: 在RadarHD基准测试中，RadarSFD实现了35厘米的Chamfer距离和28厘米的Modified Hausdorff距离，优于单帧RadarHD基线（56厘米，45厘米），并与使用5-41帧的多帧方法保持竞争力。定性结果显示能恢复精细墙壁和狭窄缝隙，在新环境中的实验证实了强大的泛化能力。消融研究强调了预训练初始化、雷达BEV条件作用和双空间损失的重要性。

Conclusion: 这些结果共同建立了首个实用的单帧、无SAR毫米波雷达管道，用于紧凑型机器人系统中的密集点云感知。

Abstract: Millimeter-wave radar provides perception robust to fog, smoke, dust, and low
light, making it attractive for size, weight, and power constrained robotic
platforms. Current radar imaging methods, however, rely on synthetic aperture
or multi-frame aggregation to improve resolution, which is impractical for
small aerial, inspection, or wearable systems. We present RadarSFD, a
conditional latent diffusion framework that reconstructs dense LiDAR-like point
clouds from a single radar frame without motion or SAR. Our approach transfers
geometric priors from a pretrained monocular depth estimator into the diffusion
backbone, anchors them to radar inputs via channel-wise latent concatenation,
and regularizes outputs with a dual-space objective combining latent and
pixel-space losses. On the RadarHD benchmark, RadarSFD achieves 35 cm Chamfer
Distance and 28 cm Modified Hausdorff Distance, improving over the single-frame
RadarHD baseline (56 cm, 45 cm) and remaining competitive with multi-frame
methods using 5-41 frames. Qualitative results show recovery of fine walls and
narrow gaps, and experiments across new environments confirm strong
generalization. Ablation studies highlight the importance of pretrained
initialization, radar BEV conditioning, and the dual-space loss. Together,
these results establish the first practical single-frame, no-SAR mmWave radar
pipeline for dense point cloud perception in compact robotic systems.

</details>


### [491] [ByteWrist: A Parallel Robotic Wrist Enabling Flexible and Anthropomorphic Motion for Confined Spaces](https://arxiv.org/abs/2509.18084)
*Jiawen Tian,Liqun Huang,Zhongren Cui,Jingchao Qiao,Jiafeng Xu,Xiao Ma,Zeyu Ren*

Main category: cs.RO

TL;DR: 本文介绍了ByteWrist，一种新型高度灵活、拟人化的并联机器人手腕，通过紧凑的三级并联驱动机制和弧形末端连杆，实现了RPY运动，特别适用于狭窄空间操作。


<details>
  <summary>Details</summary>
Motivation: 现有串联和并联机器人手腕在狭窄空间操作中存在局限性，限制了机器人在复杂非结构化环境（如家庭服务、医疗辅助、精密装配）中的应用。

Method: ByteWrist采用以下创新设计：1) 嵌套式三级电机驱动连杆，实现多自由度独立控制并最小化体积；2) 弧形末端连杆，优化力传递并扩大运动范围；3) 中央支撑球作为球形关节，增强结构刚度同时保持灵活性。同时，提出了包括正/逆运动学和数值雅可比解在内的运动学建模，以实现精确控制。

Result: 实验结果表明，ByteWrist在狭窄空间机动性和双臂协同操作任务中表现出色，优于基于Kinova的系统。与传统设计相比，ByteWrist在紧凑性、效率和刚度方面有显著提升。

Conclusion: ByteWrist为受限环境下的下一代机器人操作提供了一个有前景的解决方案，其在紧凑性、效率和刚度方面的改进使其成为未来机器人应用的关键技术。

Abstract: This paper introduces ByteWrist, a novel highly-flexible and anthropomorphic
parallel wrist for robotic manipulation. ByteWrist addresses the critical
limitations of existing serial and parallel wrists in narrow-space operations
through a compact three-stage parallel drive mechanism integrated with
arc-shaped end linkages. The design achieves precise RPY (Roll-Pitch-Yaw)
motion while maintaining exceptional compactness, making it particularly
suitable for complex unstructured environments such as home services, medical
assistance, and precision assembly. The key innovations include: (1) a nested
three-stage motor-driven linkages that minimize volume while enabling
independent multi-DOF control, (2) arc-shaped end linkages that optimize force
transmission and expand motion range, and (3) a central supporting ball
functioning as a spherical joint that enhances structural stiffness without
compromising flexibility. Meanwhile, we present comprehensive kinematic
modeling including forward / inverse kinematics and a numerical Jacobian
solution for precise control. Empirically, we observe ByteWrist demonstrates
strong performance in narrow-space maneuverability and dual-arm cooperative
manipulation tasks, outperforming Kinova-based systems. Results indicate
significant improvements in compactness, efficiency, and stiffness compared to
traditional designs, establishing ByteWrist as a promising solution for
next-generation robotic manipulation in constrained environments.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [492] [Underground Multi-robot Systems at Work: a revolution in mining](https://arxiv.org/abs/2509.16267)
*Victor V. Puche,Kashish Verma,Matteo Fumagalli*

Main category: eess.SY

TL;DR: 提出了一种模块化多机器人系统，用于在废弃地下矿井中自主执行顺序矿物开采任务，通过分层有限状态机和机器人间通信实现协作。


<details>
  <summary>Details</summary>
Motivation: 全球对关键原材料(CRMs)的需求增长，需要进入废弃地下矿井等困难和危险环境。这些环境由于空间受限、结构不稳定和基础设施缺乏，对传统机械和人类操作员构成重大挑战。

Method: 该研究提出了一种模块化多机器人系统，具有物理交互能力，通过局部高层行为控制进行协调。系统利用分层有限状态机(HFSM)行为来构建异构机器人平台上的复杂任务执行，每个机器人都有自己的HFSM行为进行顺序自主操作，并通过机器人间通信触发行为执行来维持整体系统协调。

Result: 该架构有效整合了软硬件组件，以支持在受限地下环境中进行协作的、任务驱动的多机器人操作。

Conclusion: 该系统能够实现多机器人在废弃地下矿井等复杂环境中的自主协作矿物开采任务，解决了传统方法面临的挑战。

Abstract: The growing global demand for critical raw materials (CRMs) has highlighted
the need to access difficult and hazardous environments such as abandoned
underground mines. These sites pose significant challenges for conventional
machinery and human operators due to confined spaces, structural instability,
and lack of infrastructure. To address this, we propose a modular multi-robot
system designed for autonomous operation in such environments, enabling
sequential mineral extraction tasks. Unlike existing work that focuses
primarily on mapping and inspection through global behavior or central control,
our approach incorporates physical interaction capabilities using specialized
robots coordinated through local high-level behavior control. Our proposed
system utilizes Hierarchical Finite State Machine (HFSM) behaviors to structure
complex task execution across heterogeneous robotic platforms. Each robot has
its own HFSM behavior to perform sequential autonomy while maintaining overall
system coordination, achieved by triggering behavior execution through
inter-robot communication. This architecture effectively integrates software
and hardware components to support collaborative, task-driven multi-robot
operation in confined underground environments.

</details>


### [493] [Learning in Stackelberg Markov Games](https://arxiv.org/abs/2509.16296)
*Jun He,Andrew L. Liu,Yihsu Chen*

Main category: eess.SY

TL;DR: 本文提出一个学习动态不确定多智能体环境中Stackelberg均衡的通用框架，特别针对能源市场政策设计，通过Stackelberg马尔可夫博弈和均值场近似，实现了可扩展且稳定的策略学习，并在能源市场模拟中验证了其在经济效率、公平性和系统稳定性方面的效果。


<details>
  <summary>Details</summary>
Motivation: 动机源于设计多智能体环境中社会最优政策的根本挑战，以及现实世界中紧迫的问题，如为拥有分布式能源（如屋顶太阳能和储能）的消费者设计公平的电价。

Method: 研究方法包括：1) 形式化Stackelberg马尔可夫博弈，并在温和条件下建立平稳Stackelberg均衡的存在性和唯一性；2) 通过均值场近似将框架扩展到连续智能体，形成可处理的Stackelberg-均值场均衡（S-MFE）公式；3) 引入基于softmax的近似来解决精确最佳响应动态的计算难题，并严格界定其误差；4) 通过策略迭代实现可扩展且稳定的学习，无需完全了解追随者目标。

Result: 在能源市场模拟中，验证了所学策略能够同时实现经济效率、不同收入群体间的公平性以及能源系统的稳定性。

Conclusion: 这项工作展示了博弈论学习框架如何支持大规模战略环境中的数据驱动政策设计，并可应用于能源市场等现实世界系统。

Abstract: Designing socially optimal policies in multi-agent environments is a
fundamental challenge in both economics and artificial intelligence. This paper
studies a general framework for learning Stackelberg equilibria in dynamic and
uncertain environments, where a single leader interacts with a population of
adaptive followers. Motivated by pressing real-world challenges such as
equitable electricity tariff design for consumers with distributed energy
resources (such as rooftop solar and energy storage), we formalize a class of
Stackelberg Markov games and establish the existence and uniqueness of
stationary Stackelberg equilibria under mild continuity and monotonicity
conditions. We then extend the framework to incorporate a continuum of agents
via mean-field approximation, yielding a tractable Stackelberg-Mean Field
Equilibrium (S-MFE) formulation. To address the computational intractability of
exact best-response dynamics, we introduce a softmax-based approximation and
rigorously bound its error relative to the true Stackelberg equilibrium. Our
approach enables scalable and stable learning through policy iteration without
requiring full knowledge of follower objectives. We validate the framework on
an energy market simulation, where a public utility or a state utility
commission sets time-varying rates for a heterogeneous population of prosumers.
Our results demonstrate that learned policies can simultaneously achieve
economic efficiency, equity across income groups, and stability in energy
systems. This work demonstrates how game-theoretic learning frameworks can
support data-driven policy design in large-scale strategic environments, with
applications to real-world systems like energy markets.

</details>


### [494] [Servos for Local Map Exploration Onboard Nonholonomic Vehicles for Extremum Seeking](https://arxiv.org/abs/2509.16365)
*Dylan James-Kavanaugh,Patrick McNamee,Qixu Wang,Zahra Nili Ahmadabadi*

Main category: eess.SY

TL;DR: 本文将极值寻优控制（ESC）中的导数估计方法从正弦扰动扩展到有界周期或几乎周期函数，并考虑多变量映射，给出了估计任意阶导数的充要条件，并通过仿真和实验证明了其在非完整车辆寻源控制中能实现更快的收敛。


<details>
  <summary>Details</summary>
Motivation: 以往的极值寻优控制研究主要集中在正弦扰动下对标量映射的任意阶导数或多变量映射的三阶导数进行估计。本研究旨在将扰动信号扩展到更一般的有界周期或几乎周期函数，并处理多变量映射，以克服现有方法的局限性。

Method: 研究方法包括：1) 将扰动信号从正弦函数扩展到有界周期或几乎周期函数；2) 考虑多变量映射的导数估计；3) 推导了在给定有界周期或几乎周期抖动信号下，是否存在时间变化函数来估计多变量映射任意阶导数的充要条件；4) 将这些结果应用于带有伺服驱动传感器的非完整车辆的寻源控制器中，并通过仿真和实际实验进行验证。

Result: 研究结果表明：1) 找到了一个充要条件，用于判断是否存在时间变化函数来估计给定有界周期或几乎周期抖动信号下多变量映射的任意阶导数；2) 仿真和实际实验证明，通过将局部地图探索任务分配给伺服系统，非完整车辆能够更快地收敛到源头。

Conclusion: 本研究成功地将极值寻优控制中的导数估计方法推广到更广泛的扰动信号和多变量映射，并提供了理论上的充要条件。实验结果证实了该方法在非完整车辆寻源控制中的有效性，通过局部探索的分布式策略，显著提高了收敛速度。

Abstract: Extremum seeking control (ESC) often employs perturbation-based estimates of
derivatives for some sensor field or cost function. These estimates are
generally obtained by simply multiplying the output of a single-unit sensor by
some time-varying function. Previous work has focused on sinusoidal
perturbations to generate derivative estimates with results for arbitrary order
derivatives of scalar maps or higher up to third-order derivatives of
multivariable maps. This work extends the perturbations from sinusoidal to
bounded periodic or almost periodic functions and considers multivariable maps.
A necessary and sufficient condition is given for determining if time-varying
functions exist for estimating arbitrary order derivatives of multivariable
maps for any given bounded periodic or almost periodic dither signal. These
results are then used in a source seeking controller for a nonholonomic vehicle
with a sensor actuated by servo. The conducted simulation and real-world
experiments demonstrate that by distributing the local map exploration to a
servo, the nonholonomic vehicle was able to achieve a faster convergence to the
source.

</details>


### [495] [Synergies between Federated Foundation Models and Smart Power Grids](https://arxiv.org/abs/2509.16496)
*Seyyedali Hosseinalipour,Shimiao Li,Adedoyin Inaolaji,Filippo Malandra,Luis Herrera,Nicholas Mastronarde*

Main category: eess.SY

TL;DR: 本文探讨了多模态、多任务联邦基础模型（M3T FedFMs）在智能电网中的应用潜力及其设计挑战，提供了双向视角。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）展示了强大能力，但多模态、多任务基础模型（M3T FMs）正进一步发展。将M3T FMs与联邦学习（FL）结合形成M3T FedFMs，能够实现可扩展、隐私保护的分布式数据训练。这一新兴模型类别在电力系统研究领域尚未被充分探索。

Method: 本文采用双向视角进行分析：(i) M3T FedFMs如何增强智能电网功能（如负荷预测、故障检测），通过从电网边缘的分布式异构数据中进行隐私保护学习；(ii) 智能电网的约束和结构（能源、通信、监管）如何影响M3T FedFMs的设计、训练和部署。

Result: 本文首次向电力系统研究社区介绍了M3T FedFMs这一概念，并探讨了它们在智能电网关键功能中的潜在增强作用，以及智能电网的特有挑战如何塑造这些模型的开发和应用。

Conclusion: M3T FedFMs为智能电网的增强功能和隐私保护学习提供了新的机遇。同时，智能电网的独特结构和约束将是设计和部署这些模型的关键考量，本文为这一新兴领域的研究奠定了初步基础。

Abstract: The recent emergence of large language models (LLMs) such as GPT-3 has marked
a significant paradigm shift in machine learning. Trained on massive corpora of
data, these models demonstrate remarkable capabilities in language
understanding, generation, summarization, and reasoning, transforming how
intelligent systems process and interact with human language. Although LLMs may
still seem like a recent breakthrough, the field is already witnessing the rise
of a new and more general category: multi-modal, multi-task foundation models
(M3T FMs). These models go beyond language and can process heterogeneous data
types/modalities, such as time-series measurements, audio, imagery, tabular
records, and unstructured logs, while supporting a broad range of downstream
tasks spanning forecasting, classification, control, and retrieval. When
combined with federated learning (FL), they give rise to M3T Federated
Foundation Models (FedFMs): a highly recent and largely unexplored class of
models that enable scalable, privacy-preserving model training/fine-tuning
across distributed data sources. In this paper, we take one of the first steps
toward introducing these models to the power systems research community by
offering a bidirectional perspective: (i) M3T FedFMs for smart grids and (ii)
smart grids for FedFMs. In the former, we explore how M3T FedFMs can enhance
key grid functions, such as load/demand forecasting and fault detection, by
learning from distributed, heterogeneous data available at the grid edge in a
privacy-preserving manner. In the latter, we investigate how the constraints
and structure of smart grids, spanning energy, communication, and regulatory
dimensions, shape the design, training, and deployment of M3T FedFMs.

</details>


### [496] [A LiDAR-Driven Fallback Longitudinal Controller for Safer Following in Sudden Braking Scenarios](https://arxiv.org/abs/2509.16642)
*Mohamed Sabry,Enrico Del Re,Walter Morales-Alvarez,Cristina Olaverri-Monreal*

Main category: eess.SY

TL;DR: 本研究提出了一种新型的纵向回退控制器，仅依赖激光雷达测距和跟随车辆速度，旨在提高自适应巡航控制系统在通信不稳定或传感器延迟情况下的可靠性和防撞能力。


<details>
  <summary>Details</summary>
Motivation: 协作式自适应巡航控制（CACC）过度依赖稳定的通信通道，限制了其可靠性。在紧急制动场景中，减少自适应巡航控制（ACC）系统对信息依赖的研究不足，但对于降低碰撞风险至关重要。

Method: 本研究提出了一种新型的纵向回退控制器，该控制器仅依赖激光雷达（LiDAR）测距和跟随车辆的速度。它被设计为时间无关的，以确保在传感器延迟或同步问题存在时也能运行。

Result: 仿真结果表明，所提出的控制器能够实现从静止状态的车辆跟随，并在紧急制动期间防止碰撞，即使在车载信息极少的情况下也能有效工作。

Conclusion: 该研究提出的回退纵向控制器，通过减少对信息依赖，显著增强了ACC系统在关键场景下的可靠性和安全性，尤其是在通信或传感器数据受限时。

Abstract: Adaptive Cruise Control has seen significant advancements, with Collaborative
Adaptive Cruise Control leveraging Vehicle-to-Vehicle communication to enhance
coordination and stability. However, the reliance on stable communication
channels limits its reliability. Research on reducing information dependencies
in Adaptive Cruise Control systems has remained limited, despite its critical
role in mitigating collision risks during sudden braking scenarios. This study
proposes a novel fallback longitudinal controller that relies solely on
LiDAR-based distance measurements and the velocity of a follower vehicle. The
controller is designed to be time-independent, ensuring operation in the
presence of sensor delays or synchronization issues. Simulation results
demonstrate that the proposed controller enables vehicle-following from
standstill and prevents collisions during emergency braking, even under minimal
onboard information.

</details>


### [497] [Safe Guaranteed Dynamics Exploration with Probabilistic Models](https://arxiv.org/abs/2509.16650)
*Manish Prajapat,Johannes Köhler,Melanie N. Zeilinger,Andreas Krause*

Main category: eess.SY

TL;DR: 本文提出了一种“最大安全动力学学习”框架，通过在安全策略空间中进行充分探索，实现在未知系统动力学下，以悲观安全但乐观探索的方式，在有限时间内持续在线学习动力学模型，同时保证高概率的安全操作，并最终实现接近最优的性能。


<details>
  <summary>Details</summary>
Motivation: 在真实世界中部署智能体时，确保最优性与安全性至关重要，但当系统动力学未知时，这变得尤为困难。

Method: 该研究引入了“最大安全动力学学习”的概念，通过在安全策略空间中进行充分探索。提出了一种“悲观安全”但“乐观探索”信息状态的框架，即使由于模型不确定性未能完全达到这些状态，也能确保动力学的持续在线学习。该方法在非回合制设置下在线运行，并在整个学习过程中确保安全性，仅在达到接近最优性能所需的程度上学习动力学。

Result: 该框架首次实现了在有限时间内充分学习动力学模型（达到任意小容忍度，受噪声影响），并能以高概率在整个操作过程中提供可证明的安全保障，且无需重置。此外，它能最大化奖励，同时仅在必要时学习动力学以实现接近最优的性能。在自动赛车和无人机导航等高难度领域中，验证了该方法的有效性。

Conclusion: 该研究提供了一种新颖的、与典型强化学习方法不同的在线学习方法，能够在未知动力学下，在整个学习过程中持续确保高概率的安全性，并在非回合制设置下实现接近最优的性能，解决了实际部署中的关键挑战。

Abstract: Ensuring both optimality and safety is critical for the real-world deployment
of agents, but becomes particularly challenging when the system dynamics are
unknown. To address this problem, we introduce a notion of maximum safe
dynamics learning via sufficient exploration in the space of safe policies. We
propose a $\textit{pessimistically}$ safe framework that
$\textit{optimistically}$ explores informative states and, despite not reaching
them due to model uncertainty, ensures continuous online learning of dynamics.
The framework achieves first-of-its-kind results: learning the dynamics model
sufficiently $-$ up to an arbitrary small tolerance (subject to noise) $-$ in a
finite time, while ensuring provably safe operation throughout with high
probability and without requiring resets. Building on this, we propose an
algorithm to maximize rewards while learning the dynamics $\textit{only to the
extent needed}$ to achieve close-to-optimal performance. Unlike typical
reinforcement learning (RL) methods, our approach operates online in a
non-episodic setting and ensures safety throughout the learning process. We
demonstrate the effectiveness of our approach in challenging domains such as
autonomous car racing and drone navigation under aerodynamic effects $-$
scenarios where safety is critical and accurate modeling is difficult.

</details>


### [498] [Efficiently Computing the Cyclic Output-to-Output Gain](https://arxiv.org/abs/2509.16665)
*Daniel Arnström,André M. H. Teixeira*

Main category: eess.SY

TL;DR: 本文提出了一种基于哈密顿矩阵和广义奇异值的方法，用于计算循环输出到输出增益，该方法比半正定规划更高效、可扩展且可靠。


<details>
  <summary>Details</summary>
Motivation: 循环输出到输出增益是控制系统的一个安全指标，但其常用计算方法（半正定规划）扩展性差，不适用于大规模系统，因此需要一种更高效的计算方法。

Method: 本文提出了一种使用哈密顿矩阵计算循环输出到输出增益的方法，类似于H∞范数的现有方法，但考虑了广义奇异值而非常规奇异值。此外，为确保哈密顿矩阵的存在性，引入了循环输出到输出增益的正则化版本。

Result: 数值实验表明，所提出的方法比半正定规划方法更高效、可扩展且可靠。

Conclusion: 所提出的基于哈密顿矩阵和广义奇异值的计算循环输出到输出增益的方法，是解决大规模系统该指标计算问题的有效替代方案。

Abstract: The cyclic output-to-output gain is a security metric for control systems.
Commonly, it is computed by solving a semi-definite program, which scales badly
and inhibits its use for large-scale systems. We propose a method for computing
the cyclic output-to-output gain using Hamiltonian matrices, similar to
existing methods for the $H_\infty$-norm. In contrast to existing methods for
the $H_{\infty}$-norm, the proposed method considers generalized singular
values rather than regular singular values. Moreover, to ensure that the
Hamiltonian matrices exist, we introduce a regularized version of the cyclic
output-to-output gain. Through numerical experiments, we show that the proposed
method is more efficient, scalable, and reliable than semi-definite programming
approaches.

</details>


### [499] [6DMA-Assisted Secure Wireless Communications](https://arxiv.org/abs/2509.16698)
*Yanzhi Qian,Jing Jiang,Jingze Ding,Xiaoshao Dan,Hongyun Chu*

Main category: eess.SY

TL;DR: 本文提出了一种基于六维可移动天线（6DMA）的安全无线通信系统，通过联合优化波束成形和天线的三维位置及三维旋转，以最大化总保密速率，有效提升物理层安全性能。


<details>
  <summary>Details</summary>
Motivation: 六维可移动天线（6DMA）在容量增强方面已被广泛研究，但其在物理层安全（PLS）方面的潜力尚未被充分探索。6DMA通过调整天线表面的三维位置和三维旋转，可以增加空间自由度（DoFs），从而动态塑造合法信道并抑制窃听信道，为提升保密性能提供独特优势。

Method: 本文提出了一种新型的6DMA辅助安全无线通信系统。为了同时服务多个合法用户并对抗多个窃听者的协作拦截，研究者通过联合优化发射和人工噪声（AN）波束形成器，以及天线表面的三维位置和三维旋转，构建了一个总保密速率（SSR）最大化问题。为了解决这个非凸问题，提出了一种交替优化（AO）算法，将原问题分解为两个子问题并迭代求解，以获得高质量的次优解。

Result: 仿真结果表明，与部分可移动和传统固定位置天线系统相比，所提出的6DMA辅助系统展现出卓越的保密性能。

Conclusion: 六维可移动天线（6DMA）通过其额外的空间自由度，在增强物理层安全方面具有显著优势，能够有效提升系统的总保密速率。

Abstract: Six-dimensional movable antenna (6DMA) has been widely studied for capacity
enhancement, but its potential for physical layer security (PLS) remains
largely unexplored. By adjusting both three-dimensional (3D) positions and 3D
rotations of distributed antenna surfaces, 6DMA can increase spatial degrees of
freedom (DoFs). The extra DoFs enable dynamic shaping of legitimate channels
and suppresses eavesdropping channels, thereby offering unique advantages in
enhancing secrecy performance. Motivated by this, this letter proposes a novel
6DMA-assisted secure wireless communication system, where the base station (BS)
is equipped with 6DMA to enhance secrecy performance. Specifically, to
simultaneously serve multiple legitimate users and counter cooperative
interception by multiple eavesdroppers (Eves), we formulate a sum secrecy rate
(SSR) maximization problem by jointly optimizing the transmit and artificial
noise (AN) beamformers, as well as the 3D positions and 3D rotations of antenna
surfaces. To solve this non-convex problem, we propose an alternating
optimization (AO) algorithm that decomposes the original problem into two
subproblems and solves them iteratively to obtain a high-quality suboptimal
solution. Simulation results demonstrate the superior secrecy performance over
partially movable and conventional fixed-position antenna systems.

</details>


### [500] [Data-Driven Observer Synthesis for Autonomous Limit Cycle Systems through Estimation of Koopman Eigenfunctions](https://arxiv.org/abs/2509.16744)
*Angela Ni,Wentao Tang*

Main category: eess.SY

TL;DR: 本文提出了一种数据驱动的、基于Koopman算子的方法，用于构建平面极限环系统的KKL观测器，将关键的注入映射构建问题转化为最小二乘回归和核岭回归，并通过凸优化实现。


<details>
  <summary>Details</summary>
Motivation: KKL观测器设计中的主要挑战是构建状态的注入映射，这通常需要基于第一性原理模型求解偏微分方程，过程复杂且困难。

Method: 针对平面极限环系统，本文提出KKL注入映射是Koopman特征函数的线性组合。因此，注入映射的确定被简化为最小二乘回归问题。注入映射的逆则通过核岭回归进行近似。整个合成过程仅使用凸优化。

Result: 将所提出的方法应用于Brusselator系统，结果表明能够准确估计系统状态。

Conclusion: 本文成功地开发了一种数据驱动的、基于Koopman算子的方法，解决了平面极限环系统KKL观测器中注入映射的构建难题，并通过凸优化实现了整个合成过程，并在实际系统上验证了其有效性。

Abstract: The signal of system states needed for feedback controllers is estimated by
state observers. One state observer design is the Kazantzis-Kravaris/Luenberger
(KKL) observer, a generalization of the Luenberger observer for linear systems.
The main challenge in applying the KKL design is constructing an injective
mapping of the states, which requires solving PDEs based on a first-principles
model. This paper proposes a data-driven, Koopman operator-based method for the
construction of KKL observers for planar limit cycle systems. Specifically, for
such systems, the KKL injective mapping is guaranteed to be a linear
combination of Koopman eigenfunctions. Hence, the determination of such an
injection is reduced to a least-squares regression problem, and the inverse of
the injective mapping is then approximated using kernel ridge regression. The
entire synthesis procedure uses solely convex optimization. We apply the
proposed approach to the Brusselator system, demonstrating accurate estimations
of the system states.

</details>


### [501] [On the System Theoretic Offline Learning of Continuous-Time LQR with Exogenous Disturbances](https://arxiv.org/abs/2509.16746)
*Sayak Mukherjee,Ramij R. Hossain,Mahantesh Halappanavar*

Main category: eess.SY

TL;DR: 本文分析了具有不确定扰动的离线线性二次调节器（LQR）策略设计，利用自适应动态规划（ADP）和Lyapunov方法，通过基于样本的近似，为学习到的控制增益提供了稳定性和收敛性保证。


<details>
  <summary>Details</summary>
Motivation: 研究动机是设计具有不确定扰动（特别是未知随机外生变量）的离线LQR策略，这在实际应用中更具挑战性。

Method: 该研究采用自适应动态规划（ADP）作为基础学习框架，结合基于Lyapunov的分析方法来设计算法并推导基于马尔可夫决策过程（MDP）的样本近似。对于不可测量的扰动，进一步建立了学习到的控制增益在样本近似下的稳定性和收敛性保证。

Result: 研究结果包括设计了处理不确定扰动的LQR算法，推导了基于样本的近似，并针对不可测量扰动场景，为学习到的控制增益在样本近似下建立了稳定性和收敛性保证。整个方法强调简洁性，同时提供严格的理论保证。

Conclusion: 该研究为具有外生扰动的离线连续时间LQR设计提供了一种简洁且具有严格保证的方法，并通过数值实验验证了其复杂性和有效性。

Abstract: We analyze offline designs of linear quadratic regulator (LQR) strategies
with uncertain disturbances. First, we consider the scenario where the
exogenous variable can be estimated in a controlled environment, and
subsequently, consider a more practical and challenging scenario where it is
unknown in a stochastic setting. Our approach builds on the fundamental
learning-based framework of adaptive dynamic programming (ADP), combined with a
Lyapunov-based analytical methodology to design the algorithms and derive
sample-based approximations motivated from the Markov decision process
(MDP)-based approaches. For the scenario involving non-measurable disturbances,
we further establish stability and convergence guarantees for the learned
control gains under sample-based approximations. The overall methodology
emphasizes simplicity while providing rigorous guarantees. Finally, numerical
experiments focus on the intricacies and validations for the design of offline
continuous-time LQR with exogenous disturbances.

</details>


### [502] [A model free approach for continuous-time optimal tracking control with unknown user-define cost and constrained control input via advantage function](https://arxiv.org/abs/2509.16821)
*Duc Cuong Nguyen,Quang Huy Dao,Phuong Nam Dao*

Main category: eess.SY

TL;DR: 本文提出了一种新颖的离策略连续时间Q学习框架，结合优势函数，用于解决带有约束输入的线性二次调节（LQR）和线性二次跟踪（LQT）问题，无需系统动力学知识、奖励矩阵权重或预定义控制器。


<details>
  <summary>Details</summary>
Motivation: 解决带有约束输入的LQR和LQT问题，同时克服现有方法需要预知奖励矩阵权重、状态重置或预设可行控制器等限制，并实现在模型无关条件下的控制。

Method: 本文提出了一种开创性的离策略连续时间Q学习框架，引入了针对线性连续系统的新型优势函数概念。该框架包含多个算法，可在模型无关条件下（无需系统动力学知识）处理控制问题。探索了两种实现方法：一种在固定时间间隔处理数据（适用于LQR），另一种在多个间隔操作（适用于带约束输入的跟踪问题）。算法的收敛性已通过理论验证。

Result: 该方法无需预知奖励矩阵权重、状态重置或假设存在预定义的可行控制器即可获得解决方案，并且在模型无关条件下有效。所提算法的收敛性得到了理论验证，并通过F-16飞机系统仿真结果验证了其在LQR和LQT问题上的有效性。

Conclusion: 所提出的基于离策略连续时间Q学习和新颖优势函数的框架，能够有效解决带有约束输入的LQR和LQT问题，无需先验知识，且在模型无关条件下表现出色，并通过仿真验证了其有效性。

Abstract: This paper presents a pioneering approach to solving the linear quadratic
regulation (LQR) and linear quadratic tracking (LQT) problems with constrained
inputs using a novel off-policy continuous-time Q-learning framework. The
proposed methodology leverages a novel concept of the Advantage function for
linear continuous systems, enabling solutions to be obtained without the need
for prior knowledge of the reward matrix weights, state resetting, or assuming
the existence of a predefined admissible controller. This framework includes
multiple algorithms (Algs) tailored to address these control problems under
model-free conditions, without requiring any knowledge about system dynamics.
Two distinct implementation methods are explored: the first processes state and
input data over a fixed time interval, making it well-suited for LQR problems,
while the second method operates over multiple intervals, offering a practical
solution for tracking problems with constrained inputs. The convergence of the
proposed algorithms is verified theoretically. Finally, the simulation results
of the F-16 aircraft system are presented for the two problems to validate the
effectiveness of the proposed method.

</details>


### [503] [Robustly Constrained Dynamic Games for Uncertain Nonlinear Dynamics](https://arxiv.org/abs/2509.16826)
*Shuyu Zhan,Chih-Yuan Chiu,Antoine P. Leeman,Glen Chou*

Main category: eess.SY

TL;DR: 该论文提出了一种针对非线性动态、状态相关噪声和非线性约束的鲁棒动态博弈新框架，利用系统级合成（SLS）为每个智能体设计名义轨迹和误差反馈律，并定义了鲁棒约束纳什均衡（RCNE），通过迭代最佳响应（IBR）算法求解，实验证明其能有效生成鲁棒的避碰轨迹。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理具有状态相关加性噪声、非线性动态以及智能体特定和共享约束的动态博弈时，可能难以确保鲁棒性。研究旨在开发一个能够应对这些复杂性的鲁棒框架。

Method: 1. 提出一个鲁棒动态博弈框架，包含非线性动态、状态相关加性噪声以及非线性智能体特定和共享约束。 2. 利用系统级合成（SLS），每个智能体设计一个名义轨迹和一个因果仿射误差反馈律，以在最坏情况噪声下最小化自身成本并满足所有约束。 3. 基于非线性安全证书，定义了鲁棒约束纳什均衡（RCNE）的新概念。 4. 提出一种基于迭代最佳响应（IBR）的算法，迭代优化每个智能体的轨迹和控制器，直至近似收敛到RCNE。

Result: 通过涉及大量机器人、高维非线性动态和状态相关动态噪声的仿真和硬件实验进行评估。结果表明，该方法生成的轨迹能够鲁棒地避免碰撞，而作为基线的博弈论算法（生成开环运动规划）未能生成满足约束的轨迹。

Conclusion: 该论文提出的框架和算法能够有效解决具有复杂动态和约束的鲁棒动态博弈问题，在存在状态相关噪声的情况下，成功生成了鲁棒且满足约束的轨迹，优于传统的开环博弈论方法。

Abstract: We propose a novel framework for robust dynamic games with nonlinear dynamics
corrupted by state-dependent additive noise, and nonlinear agent-specific and
shared constraints. Leveraging system-level synthesis (SLS), each agent designs
a nominal trajectory and a causal affine error feedback law to minimize their
own cost while ensuring that its own constraints and the shared constraints are
satisfied, even under worst-case noise realizations. Building on these
nonlinear safety certificates, we define the novel notion of a robustly
constrained Nash equilibrium (RCNE). We then present an Iterative Best Response
(IBR)-based algorithm that iteratively refines the optimal trajectory and
controller for each agent until approximate convergence to the RCNE. We
evaluated our method on simulations and hardware experiments involving large
numbers of robots with high-dimensional nonlinear dynamics, as well as
state-dependent dynamics noise. Across all experiment settings, our method
generated trajectory rollouts which robustly avoid collisions, while a baseline
game-theoretic algorithm for producing open-loop motion plans failed to
generate trajectories that satisfy constraints.

</details>


### [504] [Prescribed-Time Observer Is Naturally Robust Against Disturbances and Uncertainties](https://arxiv.org/abs/2509.16836)
*Abedou Abdelhadi,Mameche Omar*

Main category: eess.SY

TL;DR: 本文提出并验证了一种预设时间观测器，用于非线性系统在存在干扰和未建模动态情况下的鲁棒状态与干扰估计，并证明其在抑制峰值现象和提高估计精度方面优于传统高增益观测器。


<details>
  <summary>Details</summary>
Motivation: 在存在干扰和未建模动态的情况下，对非线性系统进行鲁棒的状态和干扰估计是一个挑战，且传统观测器（如高增益观测器）可能存在峰值现象和估计精度问题。

Method: 本文提出了一种预设时间观测器，并通过理论证明和仿真验证其性能。此外，还与标准高增益观测器进行了比较。

Result: 所提出的观测器能够完全抑制任意大的有界干扰和未建模动态的影响，实现对状态和干扰的精确估计。与高增益观测器相比，该观测器在减少峰值现象和提高估计精度方面表现出优越性。

Conclusion: 预设时间观测器为非线性系统在存在干扰和未建模动态时的鲁棒状态和干扰估计提供了一种有效且优越的解决方案，尤其在抑制峰值现象和提升估计精度方面表现出色。

Abstract: This paper addresses the robustness of a prescribed-time observer for a class
of nonlinear systems in the presence of disturbances and unmodeled dynamics. It
is proven and demonstrated through simulations that the proposed observer
completely rejects the effects of arbitrarily large bounded disturbances and
unmodeled dynamics, enabling accurate estimation of both the states and the
disturbances. Furthermore, a comparison with the standard high-gain observer is
provided to highlight the superiority of the prescribed-time observer in
reducing the peaking phenomenon and improving estimation accuracy.

</details>


### [505] [Closing the Loop Inside Neural Networks: Causality-Guided Layer Adaptation for Fault Recovery Control](https://arxiv.org/abs/2509.16837)
*Mahdi Taheri,Soon-Jo Chung,Fred Y. Hadaegh*

Main category: eess.SY

TL;DR: 本文提出一种结合因果推断和选择性在线适应的两阶段框架，用于非线性控制仿射系统在执行器故障和外部扰动下的实时故障恢复控制。


<details>
  <summary>Details</summary>
Motivation: 研究非线性控制仿射系统在执行器失效故障和外部扰动下的实时故障恢复控制问题，旨在开发一种有效的、基于学习的恢复控制方法。

Method: 该方法包括两个阶段：离线阶段，开发基于平均因果效应（ACE）的因果层归因技术，评估预训练深度神经网络（DNN）控制器中各层的相对重要性，识别出对故障补偿有高影响的层子集。在线阶段，部署基于Lyapunov的梯度更新，仅适应ACE选择的层，避免了全网络或仅最后一层更新的需求。

Result: 所提出的自适应控制器保证了闭环系统在执行器故障和外部扰动存在下的均匀最终有界性（UUB）和指数收敛。与传统全网络自适应的DNN控制器相比，该方法计算开销更低。通过一个航天器三轴姿态控制系统的案例研究，证明了其有效性。

Conclusion: 该结合因果推断和选择性在线适应的两阶段框架，为非线性控制仿射系统提供了有效的、计算效率更高的实时故障恢复控制解决方案，并在理论上保证了系统稳定性。

Abstract: This paper studies the problem of real-time fault recovery control for
nonlinear control-affine systems subject to actuator loss of effectiveness
faults and external disturbances. We derive a two-stage framework that combines
causal inference with selective online adaptation to achieve an effective
learning-based recovery control method. In the offline phase, we develop a
causal layer attribution technique based on the average causal effect (ACE) to
evaluate the relative importance of each layer in a pretrained deep neural
network (DNN) controller compensating for faults. This methodology identifies a
subset of high-impact layers responsible for robust fault compensation. In the
online phase, we deploy a Lyapunov-based gradient update to adapt only the
ACE-selected layer to circumvent the need for full-network or last-layer only
updates. The proposed adaptive controller guarantees uniform ultimate
boundedness (UUB) with exponential convergence of the closed-loop system in the
presence of actuator faults and external disturbances. Compared to conventional
adaptive DNN controllers with full-network adaptation, our methodology has a
reduced computational overhead. To demonstrate the effectiveness of our
proposed methodology, a case study is provided on a 3-axis attitude control
system of a spacecraft with four reaction wheels.

</details>


### [506] [Online Data-Driven Reachability Analysis using Zonotopic Recursive Least Squares](https://arxiv.org/abs/2509.17058)
*Alireza Naderi Akhormeh,Amr Hegazy,Amr Alanwar*

Main category: eess.SY

TL;DR: 本文提出了一种数据驱动的可达性分析框架，直接从在线测量数据中计算可达集的过近似，以解决网络物理系统模型未知或不可靠的问题。该方法使用指数遗忘Zonotopic递归最小二乘法（EF ZRLS）估计时变模型，并适用于离散时间线性时变和非线性Lipschitz系统，相比现有技术，其可达集过近似更不保守，对慢变动态鲁棒，且仅依赖实时数据。


<details>
  <summary>Details</summary>
Motivation: 传统的基于模型的系统可达性分析方法需要精确的系统动力学先验知识，但在现代网络物理系统中，由于测量、系统模型（参数）和输入的不确定性，这种知识可能不可用或不可靠。

Method: 提出了一种数据驱动的可达性分析框架。该方法使用指数遗忘Zonotopic递归最小二乘法（EF ZRLS）直接从受有界噪声污染的在线状态测量数据中估计时变未知模型。具体而言，它递归地估计一个包含系统真实模型的时变模型集合，然后利用该集合在过程噪声和不确定输入下计算前向可达集。该方法适用于离散时间线性时变（LTV）系统和非线性Lipschitz系统。

Result: 与现有技术相比，该方法产生更不保守的可达集过近似，在慢变动态下保持鲁棒性，并且仅依赖实时数据运行，无需任何预先记录的离线实验。数值模拟和实际实验验证了所提算法的有效性和实际适用性。

Conclusion: 所提出的数据驱动可达性分析框架能够有效且实际地应用于系统模型未知或不可靠的情况，通过直接从在线数据中估计时变模型并计算可达集，克服了传统方法的局限性，并展现出更低的保守性和更好的鲁棒性。

Abstract: Reachability analysis is a key formal verification technique for ensuring the
safety of modern cyber physical systems subject to uncertainties in
measurements, system models (parameters), and inputs. Classical model-based
approaches rely on accurate prior knowledge of system dynamics, which may not
always be available or reliable. To address this, we present a data-driven
reachability analysis framework that computes over-approximations of reachable
sets directly from online state measurements. The method estimates time-varying
unknown models using an Exponentially Forgetting Zonotopic Recursive Least
Squares (EF ZRLS) method, which processes data corrupted by bounded noise.
Specifically, a time-varying set of models that contains the true model of the
system is estimated recursively, and then used to compute the forward reachable
sets under process noise and uncertain inputs. Our approach applies to both
discrete-time Linear Time Varying (LTV) and nonlinear Lipschitz systems.
Compared to existing techniques, it produces less conservative reachable set
over approximations, remains robust under slowly varying dynamics, and operates
solely on real-time data without requiring any pre-recorded offline
experiments. Numerical simulations and real-world experiments validate the
effectiveness and practical applicability of the proposed algorithms.

</details>


### [507] [Machine Learning for Campus Energy Resilience: Clustering and Time-Series Forecasting in Intelligent Load Shedding](https://arxiv.org/abs/2509.17097)
*Salim Oyinlola,Peter Olabisi Oluseyi*

Main category: eess.SY

TL;DR: 本研究提出了一种基于机器学习的负荷削减框架，通过聚类和预测技术，为大学提供智能能源管理，以优化电力分配并减少浪费。


<details>
  <summary>Details</summary>
Motivation: 大学对可靠电力的需求日益增长，需要智能能源管理来优化电力分配并减少浪费。

Method: 该方法分三个阶段：首先，收集了55栋建筑的3,648小时用电数据以建立建筑级消费模型。其次，使用主成分分析（PCA）进行降维，并利用聚类验证技术确定最佳需求组数，然后采用Mini-Batch K-Means将建筑分为高、中、低需求集群。最后，在集群层面使用ARIMA、SARIMA、Prophet、LSTM和GRU等多种统计和深度学习模型进行短期负荷预测。

Result: 结果显示，Prophet模型提供了最可靠的预测，而Mini-Batch K-Means实现了稳定的聚类性能。该框架通过整合聚类与预测，实现了更公平、数据驱动的负荷削减策略。

Conclusion: 该框架能有效减少低效率，并通过可持续能源管理支持气候变化缓解。

Abstract: The growing demand for reliable electricity in universities necessitates
intelligent energy management. This study proposes a machine learning-based
load shedding framework for the University of Lagos, designed to optimize
distribution and reduce waste. The methodology followed three main stages.
First, a dataset of 3,648 hourly records from 55 buildings was compiled to
develop building-level consumption models. Second, Principal Component Analysis
was applied for dimensionality reduction, and clustering validation techniques
were used to determine the optimal number of demand groups. Mini-Batch K-Means
was then employed to classify buildings into high-, medium-, and low-demand
clusters. Finally, short-term load forecasting was performed at the cluster
level using multiple statistical and deep learning models, including ARIMA,
SARIMA, Prophet, LSTM, and GRU. Results showed Prophet offered the most
reliable forecasts, while Mini-Batch K-Means achieved stable clustering
performance. By integrating clustering with forecasting, the framework enabled
a fairer, data-driven load shedding strategy that reduces inefficiencies and
supports climate change mitigation through sustainable energy management.

</details>


### [508] [Vibrational Stabilization of Cluster Synchronization in Oscillator Networks](https://arxiv.org/abs/2509.17111)
*Yuzhen Qin,Alberto Maria Nobili,Danielle S. Bassett,Fabio Pasqualetti*

Main category: eess.SY

TL;DR: 本文提出并验证了一种基于开环振动控制策略，无需状态测量即可稳定集群同步的方法。


<details>
  <summary>Details</summary>
Motivation: 集群同步对许多技术和自然系统正常运行至关重要，其异常与各种故障（如神经系统疾病）相关。现有稳定方法多依赖状态测量，但在许多实际场景中（如大脑神经活动），实时测量系统状态极具挑战。

Method: 采用开环控制策略——振动控制，该方法不需要任何状态测量。建立了振动输入稳定集群同步的充分条件，并提供了一种可行的振动控制设计方法。

Result: 建立了振动输入稳定集群同步的充分条件，并提供了一种可行的振动控制设计方法。通过数值实验验证了理论发现的有效性。

Conclusion: 研究表明，振动控制是一种无需状态测量即可有效稳定集群同步的策略，为克服实际应用中的测量难题提供了解决方案。

Abstract: Cluster synchronization is of great importance for the normal functioning of
numerous technological and natural systems. Deviations from normal cluster
synchronization patterns are closely associated with various malfunctions, such
as neurological disorders in the brain. Therefore, it is crucial to restore
normal system functions by stabilizing the appropriate cluster synchronization
patterns. Most existing studies focus on designing controllers based on state
measurements to achieve system stabilization. However, in many real-world
scenarios, measuring system states in real time, such as neuronal activity in
the brain, poses significant challenges, rendering the stabilization of such
systems difficult. To overcome this challenge, in this paper, we employ an
open-loop control strategy, vibrational control, which does not require any
state measurements. We establish some sufficient conditions under which
vibrational inputs stabilize cluster synchronization. Further, we provide a
tractable approach to design vibrational control. Finally, numerical
experiments are conducted to demonstrate our theoretical findings.

</details>


### [509] [Delay compensation of multi-input distinct delay nonlinear systems via neural operators](https://arxiv.org/abs/2509.17131)
*Filip Bajraktari,Luke Bhan,Miroslav Krstic,Yuanyuan Shi*

Main category: eess.SY

TL;DR: 本文首次为具有不同执行延迟的多输入非线性系统中的近似预测器提供了稳定性结果，证明了在满足均匀误差界限的条件下可实现半全局实际稳定性，并以神经算子为例进行了验证。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对具有不同执行延迟的多输入非线性系统中近似预测器的稳定性分析，这在实际应用中是一个重要且复杂的挑战。

Method: 研究方法包括将延迟转化为传输偏微分方程（PDE），并对耦合的常微分方程-偏微分方程（ODE-PDE）级联系统进行分析。此外，作者利用神经算子作为一类近似器来展示所需的均匀误差界限的可行性。

Result: 主要结果是，如果预测器近似满足一个均匀（随时间）的误差界限，则可以实现半全局实际稳定性。所需的均匀误差界限取决于期望的吸引域和系统中的控制输入数量。理论和仿真（包括移动机器人实验）均表明，神经算子能够满足这种通用界限。

Conclusion: 本文首次为具有不同执行延迟的多输入非线性系统中的近似预测器提供了稳定性结果，证明了通过满足特定均匀误差界限可以实现半全局实际稳定性，并通过神经算子展示了其可行性，为此类系统的设计提供了理论基础和实践指导。

Abstract: In this work, we present the first stability results for approximate
predictors in multi-input non-linear systems with distinct actuation delays. We
show that if the predictor approximation satisfies a uniform (in time) error
bound, semi-global practical stability is correspondingly achieved. For such
approximators, the required uniform error bound depends on the desired region
of attraction and the number of control inputs in the system. The result is
achieved through transforming the delay into a transport PDE and conducting
analysis on the coupled ODE-PDE cascade. To highlight the viability of such
error bounds, we demonstrate our results on a class of approximators - neural
operators - showcasing sufficiency for satisfying such a universal bound both
theoretically and in simulation on a mobile robot experiment.

</details>


### [510] [Adaptive Lyapunov-constrained MPC for fault-tolerant AUV trajectory tracking](https://arxiv.org/abs/2509.17237)
*Haolin Liu,Shiliang Zhang,Xiaohui Zhang,Shangbin Jiao,Xuehui Ma,Ting Shang,Yan Yan,Wenqi Bai,Youmin Zhang*

Main category: eess.SY

TL;DR: 本文提出了一种自适应Lyapunov约束模型预测控制（LMPC）方法，用于水下自主航行器（AUV）在推进器故障下的容错轨迹跟踪，确保AUV在故障和正常模式切换时的稳定性和平滑过渡。


<details>
  <summary>Details</summary>
Motivation: AUV在任务中容易受到各种故障的影响，特别是在推进器故障时，这给AUV在实际环境中的控制和操作带来了挑战，需要一种能够保证稳定轨迹跟踪的容错控制策略。

Method: 本文提出了一种自适应Lyapunov约束模型预测控制（LMPC）。该方法通过贝叶斯方法建立在线故障识别模型来识别AUV推进器故障，并将识别出的故障模型反馈给LMPC控制器。LMPC中的Lyapunov约束确保了AUV在故障和正常模式切换时的轨迹跟踪稳定性，从而减轻了故障发生或恢复时的剧烈波动。

Result: 在四推进器平面AUV上进行的数值仿真表明，该方法在推进器故障类型之间实现了平滑过渡，并与基准自适应MPC和带有快速故障识别及适应能力的反步控制相比，具有较低的轨迹跟踪误差。

Conclusion: 所提出的自适应Lyapunov约束模型预测控制（LMPC）能够有效处理AUV推进器故障，在AUV状态切换时（故障发生或恢复）保持轨迹跟踪的稳定性和平滑性，显著提高了AUV在真实环境中的可靠性和操作性能。

Abstract: Autonomous underwater vehicles (AUVs) are subject to various sources of
faults during their missions, which challenges AUV control and operation in
real environments. This paper addresses fault-tolerant trajectory tracking of
autonomous underwater vehicles (AUVs) under thruster failures. We propose an
adaptive Lyapunov-constrained model predictive control (LMPC) that guarantees
stable trajectory tracking when the AUV switches between fault and normal
modes. Particularly, we model different AUV thruster faults and build online
failure identification based on Bayesian approach. This facilitates a soft
switch between AUV status, and the identified and updated AUV failure model
feeds LMPC controller for the control law derivation. The Lyapunov constrain in
LMPC ensures that the trajectory tracking control remains stable during AUV
status shifts, thus mitigating severe and fatal fluctuations when an AUV
thruster occurs or recovers. We conduct numerical simulations on a
four-thruster planar AUV using the proposed approach. The results demonstrate
smooth transitions between thruster failure types and low trajectory tracking
errors compared with the benchmark adaptive MPC and backstepping control with
rapid failure identification and failure accommodation during the trajectory
tracking.

</details>


### [511] [Trajectory Encryption Cooperative Salvo Guidance](https://arxiv.org/abs/2509.17341)
*Lohitvel Gopikannan,Shashi Ranjan Kumar,Abhinav Sinha*

Main category: eess.SY

TL;DR: 本文提出在协同同时目标拦截中利用异构制导原理实现轨迹加密，通过生成多样化轨迹来增强鲁棒性并迷惑对手。


<details>
  <summary>Details</summary>
Motivation: 现有协同拦截策略可能存在可预测性，易被对手分析。本研究旨在通过引入制导原理的异构性，扩展可行解空间，提高系统在干扰下的鲁棒性，实现灵活的时间调整，并从对抗角度模糊集体拦截意图，防止对蜂群动态的直接预测。

Method: 利用无人自主系统团队中异构的剩余飞行时间（time-to-go）公式，设计一种协同制导策略。这种异构性使得蜂群能够生成多样化的轨迹族。

Result: 仿真结果表明，异构车辆蜂群能够从多样化的初始交战配置中同时拦截移动目标。该方法扩展了同时拦截的可行解空间，增强了在干扰下的鲁棒性，并实现了灵活的剩余飞行时间调整而不会产生可预测的绕行。从对抗角度看，异构性通过阻止对蜂群动态的直接预测，有效掩盖了集体拦截意图，起到了轨迹域中的加密层作用。

Conclusion: 在协同同时目标拦截中，利用制导原理的异构性作为战略设计特征，可以有效生成多样化轨迹，提高系统性能，增强对抗鲁棒性，并实现轨迹加密效果。

Abstract: This paper introduces the concept of trajectory encryption in cooperative
simultaneous target interception, wherein heterogeneity in guidance principles
across a team of unmanned autonomous systems is leveraged as a strategic design
feature. By employing a mix of heterogeneous time-to-go formulations leading to
a cooperative guidance strategy, the swarm of vehicles is able to generate
diverse trajectory families. This diversity expands the feasible solution space
for simultaneous target interception, enhances robustness under disturbances,
and enables flexible time-to-go adjustments without predictable detouring. From
an adversarial perspective, heterogeneity obscures the collective interception
intent by preventing straightforward prediction of swarm dynamics, effectively
acting as an encryption layer in the trajectory domain. Simulations demonstrate
that the swarm of heterogeneous vehicles is able to intercept a moving target
simultaneously from a diverse set of initial engagement configurations.

</details>


### [512] [Methods for Multi-objective Optimization PID Controller for quadrotor UAVs](https://arxiv.org/abs/2509.17423)
*Andrea Vaiuso,Gabriele Immordino,Ludovica Onofri,Giuliano Coppotelli,Marcello Righi*

Main category: eess.SY

TL;DR: 本文研究了在统一仿真环境下，使用元启发式算法、贝叶斯优化和深度强化学习等梯度无关优化技术，对四旋翼无人机PID控制器增益进行自动化调优，以实现更稳定、高效和低噪音的飞行。


<details>
  <summary>Details</summary>
Motivation: 无人机日常使用需要稳定的飞行、高效的能源利用和降低噪音。现有PID控制器对增益选择高度敏感，手动调优往往导致次优的权衡。因此，需要自动化优化技术来解决这一问题。

Method: 研究人员构建了一个统一的仿真环境，该环境结合了基于叶片单元动量理论的气动模型（带快速深度神经网络代理）、六自由度刚体动力学、湍流以及数据驱动的声学代理模型。他们比较了三种梯度无关优化器：元启发式算法（如灰狼优化）、贝叶斯优化和深度强化学习。候选控制器通过包含噪音足迹和功耗等多指标的复合成本函数进行评估。

Result: 元启发式算法持续改进性能，其中灰狼优化产生了最佳结果。贝叶斯优化在样本效率方面表现良好，但每次迭代开销较高且依赖于设计域。深度强化学习代理在当前设置下未能超越基线，表明问题表述需要进一步完善。在未见过的任务中，最佳调优控制器保持了准确的跟踪，同时减少了振荡、功耗和声学排放。

Conclusion: 通过黑盒搜索进行噪音感知的PID调优，无需硬件更改即可实现更安静、更高效的飞行，证明了该方法的有效性。

Abstract: Integrating unmanned aerial vehicles into daily use requires controllers that
ensure stable flight, efficient energy use, and reduced noise. Proportional
integral derivative controllers remain standard but are highly sensitive to
gain selection, with manual tuning often yielding suboptimal trade-offs. This
paper studies different optimization techniques for the automated tuning of
quadrotor proportional integral derivative gains under a unified simulation
that couples a blade element momentum based aerodynamic model with a fast deep
neural network surrogate, six degrees of freedom rigid body dynamics,
turbulence, and a data driven acoustic surrogate model that predicts third
octave spectra and propagates them to ground receivers. We compare three
families of gradient-free optimizers: metaheuristics, Bayesian optimization,
and deep reinforcement learning. Candidate controllers are evaluated using a
composite cost function that incorporates multiple metrics, such as noise
footprint and power consumption, simultaneously. Metaheuristics improve
performance consistently, with Grey Wolf Optimization producing optimal
results. Bayesian optimization is sample efficient but carries higher per
iteration overhead and depends on the design domain. The reinforcement learning
agents do not surpass the baseline in the current setup, suggesting the problem
formulation requires further refinement. On unseen missions the best tuned
controller maintains accurate tracking while reducing oscillations, power
demand, and acoustic emissions. These results show that noise aware
proportional integral derivative tuning through black box search can deliver
quieter and more efficient flight without hardware changes.

</details>


### [513] [A Fundamental Study for Multiobjective Optimization Problems in Nonlinear Dynamical Systems](https://arxiv.org/abs/2509.17434)
*Ryunosuke Numata,Toshimichi Saito*

Main category: eess.SY

TL;DR: 本文研究了一个基于光伏升压变换器的分段线性切换动态系统中的双目标优化问题，精确分析了电路运行稳定性与平均输入功率之间的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 多目标优化问题在非线性动态系统的分析和应用中非常重要。本文旨在通过一个简单的非线性切换动态系统（分段线性系统）作为第一步，来研究此类问题。

Method: 利用分段线性系统的特性，对非线性动力学进行了精确分析。基于精确解，理论上建立了电路运行稳定性（第一个目标）和平均输入功率（第二个目标）的数学公式。

Result: 精确阐明了两个目标之间权衡关系的存在性，并探讨了这种权衡与系统参数之间的关系。

Conclusion: 研究结果为分析各种非线性系统中的多目标优化问题及其工程应用提供了基础信息。

Abstract: Multiobjective optimization problems are important in analysis and
application of nonlinear dynamical systems. As a first step, this paper studies
a biobjective optimization problem in a simple nonlinear switched dynamical
system: a piecewise linear system based on a boost converter with photovoltaic
input. The piecewise linearity enables us to analyze the nonlinear dynamics
exactly. In the biobjective optimization problem, the first objective evaluates
stability of circuit operation and the second objective evaluates average input
power. A main task is analysis of a trade-off between the two objectives. Using
the piecewise exact solutions, the two objectives are formulated theoretically.
Using the theoretical formulae, the existence of a trade-off between the two
objectives is clarified exactly. Relationship between the trade-off and
parameters is also considered. The results provide fundamental information to
analyze multiobjective optimization problems in various nonlinear systems and
to realize their engineering applications.

</details>


### [514] [Coordinated Battery Electric Vehicle Charging Scheduling across Multiple Charging Stations](https://arxiv.org/abs/2509.17607)
*Saman Mehrnia,Hui Song,Nameer Al Khafaf,Mahdi Jalili,Lasantha Meegahapola,Brendan McGrath*

Main category: eess.SY

TL;DR: 本文提出了一种多目标优化框架，用于电动汽车（BEV）的充放电调度，旨在平衡电网影响、充电站（EVCS）运营商和BEV车主利益，并整合碳排放计划和动态经济调度，通过G2V和V2G技术实现响应式充放电，以优化电价并减少总成本。


<details>
  <summary>Details</summary>
Motivation: 电动汽车的快速普及需要协调的充放电基础设施，否则无序充电模式会导致电网功耗增加和电压波动。BEV充电调度是一个多目标优化挑战，需要平衡最小化电网影响与最大化EVCS运营商和BEV车主利益。

Method: 开发了一个多目标优化（MOO）框架。该框架整合了碳排放计划和动态经济调度问题，允许BEV用户通过G2V和V2G技术根据最优电价和补偿进行充放电响应。此外，该方法将动态经济调度与分时电价相结合，以获得最优市场电价并减少24小时内的总成本。

Result: 在样本网络上的实验结果表明，所提出的调度方案：1) 增加了V2G服务参与度超过10%；2) 增加了EVCS收益超过20%；3) 减少了电网损耗。此外，增加的充放电速率以及BEV用户和EVCS获得更高的碳收入，有助于更好地抵消电池退化成本。

Conclusion: 所提出的BEV充放电调度MOO框架，通过整合碳排放和动态经济调度，显著提高了V2G参与度、EVCS收益，减少了电网损耗，并有助于通过碳收入抵消电池退化成本，为电动汽车的协调发展提供了有效方案。

Abstract: The uptake of battery electric vehicles (BEVs) is increasing to reduce
greenhouse gas emissions in the transport sector. The rapid adoption of BEVs
depends significantly on the coordinated charging/discharging infrastructure.
Without it, uncontrolled and erratic charging patterns could lead to increased
power losses and voltage fluctuations beyond acceptable thresholds. BEV charge
scheduling presents a multi-objective optimization (MOO) challenge, demanding a
balance between minimizing network impact and maximizing the benefits for
electric vehicle charging station (EVCS) operators and BEV owners. In this
paper, we develop an MOO framework incorporating a carbon emission program and
a dynamic economic dispatch problem, allowing BEV users to respond by charging
and discharging through grid-to-vehicle (G2V) and vehicle-to-grid (V2G)
technologies according to the optimal electricity price and compensation.
Furthermore, we integrate dynamic economic dispatch with time-of-use tariffs to
obtain optimal market electricity prices and reduce total costs over 24 hours.
Our experimental results on a sample network show that the proposed scheduling
increases participation in V2G services by over 10%, increases EVCS benefits by
over 20%, and reduces network losses. Furthermore, increased rates of
charging/discharging, coupled with more significant carbon revenue benefits for
BEV users and EVCS, contribute to better offsetting battery degradation costs.

</details>


### [515] [On continuous-time sparse identification of nonlinear polynomial systems](https://arxiv.org/abs/2509.17635)
*Mazen Alamir*

Main category: eess.SY

TL;DR: 本文利用高阶导数重构和稀疏多元多项式识别技术，改进了从少量数据中简洁识别未知单输入/单输出非线性动力学系统的方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机是需要从少量数据中，以简洁的方式识别未知单输入/单输出非线性动力学系统（相对阶数最高为4）。

Method: 该方法利用了高噪声时间序列高阶导数重构的最新进展，以及稀疏多元多项式识别技术。

Result: 该方法提高了从少量数据中简洁识别相对阶数高达4的未知单输入/单输出非线性动力学系统的过程。并在电子节气门控制汽车系统上进行了验证。

Conclusion: 该研究提供了一种改进的、能够从有限数据中有效识别复杂非线性动力学系统的方法，并通过实际汽车系统案例进行了验证。

Abstract: This paper leverages recent advances in high derivatives reconstruction from
noisy-time series and sparse multivariate polynomial identification in order to
improve the process of parsimoniously identifying, from a small amount of data,
unknown Single-Input/Single-Output nonlinear dynamics of relative degree up to
4. The methodology is illustrated on the Electronic Throttle Controlled
automotive system.

</details>


### [516] [Holistic Grid-Forming Control for HVDC-Connected Offshore Wind Power Plants to Provide Frequency Response](https://arxiv.org/abs/2509.17672)
*Zhenghua Xu,Dominic Gross,George Alin Raducu,Hesam Khazraj,Nicolaos A. Cutululis*

Main category: eess.SY

TL;DR: 本文提出了一种基于双端口GFM控制的整体GFM控制策略，旨在提升高压直流输电海上风电场（HVDC-OWPP）的频率响应能力，特别是惯性响应和频率遏制储备（FCR）的提供，并通过仿真验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着电力系统中电力电子设备渗透率的增加，频率控制面临挑战。HVDC-OWPPs需要提供惯性响应和FCR以应对这些挑战。研究方向正从基于通信的网格跟随（GFL）控制转向无通信的网格形成（GFM）控制，以增强频率响应能力。

Method: 1. 建立了典型HVDC-OWPP系统的频率响应模型，用于GFM控制设计。2. 在HVDC系统和OWPP上分别实施双端口GFM控制和虚拟同步发电机（VSG）控制，并揭示了陆上和海上频率的异步性。3. 提出了一种整体GFM控制，以改善同步性和直流电压调节。4. 通过仿真验证了所提控制在提供FCR和惯性响应方面的可行性和有效性。

Result: 仿真结果验证了所提出的整体GFM控制策略在HVDC-OWPP系统中提供FCR和惯性响应的可行性和有效性，并且能够改善同步性和直流电压调节。

Conclusion: 所提出的基于双端口GFM控制的整体GFM控制策略，能够有效提升HVDC-OWPP系统在整个交直流交动态中的协调性，增强其频率响应能力，为电力系统提供重要的频率支持。

Abstract: HVDC-connected offshore wind power plants (OWPPs) are expected to provide
inertial response and frequency containment reserve (FCR) to help address the
frequency control challenges caused by the growing penetration of power
electronics in power systems. Initially dominated by communication-based and
grid-following (GFL) control, recent efforts have shifted towards incorporating
communication-free and grid-forming (GFM) control into HVDC-OWPP systems to
enhance their frequency response capability. This paper proposes a holistic GFM
control based on dual-port GFM control to improve the coordination across the
entire AC-DC-AC dynamics. A frequency response model of a typical HVDC-OWPP
system is developed for GFM control design. Then, dual-port GFM control and
virtual synchronous generator control are implemented respectively on the HVDC
system and OWPP of the typical system, where the asynchronism of onshore and
offshore frequencies is revealed. Next, holistic GFM control is proposed to
improve the synchronization and DC voltage regulation. Finally, simulations on
the delivery of FCR and inertial response are carried out to verify the
feasibility and effectiveness of the proposed control.

</details>


### [517] [RSU-Assisted Resource Allocation for Collaborative Perception](https://arxiv.org/abs/2509.17691)
*Guowei Liu,Le Liang,Chongtao Guo,Hao Ye,Shi Jin*

Main category: eess.SY

TL;DR: 本文提出RACooper，一个由路侧单元（RSU）辅助的资源分配框架，旨在解决协作感知中通信资源受限的问题。RACooper利用分层强化学习模型，通过动态分配通信资源，在考虑实时感知数据和信道动态的情况下，最大限度地提高感知精度。


<details>
  <summary>Details</summary>
Motivation: 现有的协作感知框架通常假设通信资源充足，这在实际车联网中是不切实际的。因此，需要研究如何在有限的通信资源下，优化协作感知的性能。

Method: 本文提出RACooper框架，采用分层强化学习模型来动态分配通信资源。该方法综合考虑实时感知数据和车辆移动引起的信道动态，并通过联合优化空间置信度指标和信道状态信息，确保高效的特征传输。

Result: 仿真结果表明，与传统基线算法相比，RACooper显著提高了感知精度，尤其是在带宽受限的场景下。

Conclusion: RACooper框架有效解决了协作感知中的通信资源分配问题，通过智能的资源管理，在资源受限的环境下显著提升了感知精度，从而增强了协作感知的有效性。

Abstract: As a pivotal technology for autonomous driving, collaborative perception
enables vehicular agents to exchange perceptual data through
vehicle-to-everything (V2X) communications, thereby enhancing perception
accuracy of all collaborators. However, existing collaborative perception
frameworks often assume ample communication resources, which is usually
impractical in real-world vehicular networks. To address this challenge, this
paper investigates the problem of communication resource allocation for
collaborative perception and proposes RACooper, a novel RSU-assisted resource
allocation framework that maximizes perception accuracy under constrained
communication resources. RACooper leverages a hierarchical reinforcement
learning model to dynamically allocate communication resources while accounting
for real-time sensing data and channel dynamics induced by vehicular mobility.
By jointly optimizing spatial confidence metrics and channel state information,
our approach ensures efficient feature transmission, enhancing the
effectiveness of collaborative perception. Simulation results demonstrate that
compared to conventional baseline algorithms, RACooper achieves significant
improvements in perception accuracy, especially under bandwidth-constrained
scenarios.

</details>


### [518] [Existence and Synthesis of Multi-Resolution Approximate Bisimulations for Continuous-State Dynamical Systems](https://arxiv.org/abs/2509.17739)
*Rudi Coppola,Yannik Schnitzer,Mirco Giacobbe,Alessandro Abate,Manuel Mazo Jr*

Main category: eess.SY

TL;DR: 解析错误


<details>
  <summary>Details</summary>
Motivation: 解析错误

Method: 解析错误

Result: 解析错误

Conclusion: 解析错误

Abstract: We present a fully automatic framework for synthesising compact, finite-state
deterministic abstractions of deterministic, continuous-state autonomous
systems under locally specified resolution requirements.
  Our approach builds on multi-resolution approximate bisimulations, a
generalisation of classical $\epsilon$-approximate bisimulations, that support
state-dependent error bounds and subsumes both variable- and uniform-resolution
relations. We show that some systems admit multi-resolution bisimulations but
no $\epsilon$-approximate bisimulation.
  We prove the existence of multi-resolution approximately bisimilar
abstractions for all incrementally uniformly bounded ($\delta$-UB) systems,
thereby broadening the applicability of symbolic verification to a larger class
of dynamics; as a trivial special case, this result also covers incrementally
globally asymptotically stable ($\delta$-GAS) systems.
  The Multi-resolution Abstraction Synthesis Problem (MRASP) is solved via a
scalable Counterexample-Guided Inductive Synthesis (CEGIS) loop, combining mesh
refinement with counterexample-driven refinement. This ensures soundness for
all $\delta$-UB systems, and ensures termination in certain special cases.
  Experiments on linear and nonlinear benchmarks, including non-$\delta$-GAS
and non-differentiable cases, demonstrate that our algorithm yields
abstractions up to 50\% smaller than Lyapunov-based grids while enforcing
tighter, location-dependent error guarantees.

</details>


### [519] [On Fast Attitude Filtering Based on Matrix Fisher Distribution with Stability Guarantee](https://arxiv.org/abs/2509.17827)
*Shijie Wang,Haichao Gui,Rui Zhong*

Main category: eess.SY

TL;DR: 本文提出两种基于矩阵Fisher分布（MFD）的闭式姿态滤波器，通过揭示MFD的两个关键特性，显著提高了滤波精度和计算效率，并在单轴旋转下实现了几乎全局指数稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决非线性滤波机制和基于MFD的快速姿态滤波中的问题，特别是现有MFD滤波器计算量大，且缺乏方向统计学滤波器的稳定性证明，尤其是在复杂条件下提升性能。

Method: 1. 通过分析贝叶斯规则下的MFD分布演化，揭示了两个增强贝叶斯姿态滤波器性能的基本特性。2. 引入具有右不变误差的线性化误差系统，提出了两个保留上述特性的闭式MFD滤波器。3. 利用这两个特性和闭式滤波迭代，证明了所提出的右不变误差滤波器在单轴旋转下的几乎全局指数稳定性。

Result: 1. 所提出的滤波器比经典的不变卡尔曼滤波器显著更准确。2. 在初始误差大和测量不确定性高的挑战性环境下，与近期基于MFD的贝叶斯滤波器一样准确。3. 计算时间大大减少（约为现有MFD姿态滤波器的1/5到1/100）。4. 实现了单轴旋转下的几乎全局指数稳定性，这是现有方向统计学滤波器未达到的成果。

Conclusion: 所提出的闭式MFD滤波器在姿态估计方面表现出卓越的精度和计算效率，尤其是在挑战性条件下，并且在单轴旋转下提供了稳定性保证，是现有方法的重大改进。

Abstract: This paper addresses two interrelated problems of the nonlinear filtering
mechanism and fast attitude filtering with the matrix Fisher distribution (MFD)
on the special orthogonal group. By analyzing the distribution evolution along
Bayes' rule, we reveal two essential properties that enhance the performance of
Bayesian attitude filters with MFDs, particularly in challenging conditions,
from a theoretical viewpoint.
  Benefiting from the new understanding of the filtering mechanism associated
with MFDs, two closed-form filters with MFDs is then proposed. These filters
avoid the burdensome computations in previous MFD-based filters by introducing
linearized error systems with right-invariant errors but retaining the two
advantageous properties. Moreover, we leverage the two properties and
closed-form filtering iteration to prove the almost-global exponential
stability of the proposed filter with right-invariant error for the single-axis
rotation, which, to our knowledge, is not achieved by existing directional
statistics-based filters. Numerical simulations demonstrate that the proposed
filters are significantly more accurate than the classic invariant Kalman
filter. Besides, they are also as accurate as recent MFD-based Bayesian filters
in challenging circumstances with large initial error and measurement
uncertainty but consumes far less computation time (about 1/5 to 1/100 of
previous MFD-based attitude filters).

</details>


### [520] [Addressing Model Inaccuracies in Transmission Network Reconfiguration via Diverse Alternatives](https://arxiv.org/abs/2509.17865)
*Paul Bannmüller,Périne Cunat,Ali Rajaei,Jochen Cremer*

Main category: eess.SY

TL;DR: 本文提出了一种人机协同的建模生成替代方案（HITL-MGA）方法，通过生成多样化的拓扑重构方案，以解决能源转型中电网拥堵问题，并克服现有决策支持工具的模型不准确性及未建模约束问题。


<details>
  <summary>Details</summary>
Motivation: 能源转型导致可再生能源和电气化比例增加，给输电网络带来巨大压力，导致电网拥堵。现有输电系统运营商的决策支持工具（如输电网络重构或再调度）容易出现模型不准确，无法考虑未建模约束或操作员偏好，导致建议不相关。

Method: 本文提出了一种人机协同的建模生成替代方案（HITL-MGA）方法。该方法旨在通过生成多样化的拓扑重构替代方案来弥补现有工具的不足，并融入专家反馈。

Result: 在IEEE 57节点和IEEE 118节点系统上的案例研究表明，该方法能够有效利用专家反馈，显著提高所建议补救措施的质量。

Conclusion: HITL-MGA方法能够通过结合专家反馈和生成多样化拓扑重构方案，有效改进输电网络拥堵的补救措施，克服了传统决策支持工具的局限性。

Abstract: The ongoing energy transition places significant pressure on the transmission
network due to increasing shares of renewables and electrification. To mitigate
grid congestion, transmission system operators need decision support tools to
suggest remedial actions, such as transmission network reconfigurations or
redispatch. However, these tools are prone to model inaccuracies and may not
provide relevant suggestions with regard to important unmodeled constraints or
operator preferences. We propose a human-in-the-loop modeling-to-generate
alternatives (HITL-MGA) approach to address these shortcomings by generating
diverse topology reconfiguration alternatives. Case studies on the IEEE 57-bus
and IEEE 118-bus systems show the method can leverage expert feedback and
improve the quality of the suggested remedial actions.

</details>


### [521] [Lipschitz-Based Robustness Certification for Recurrent Neural Networks via Convex Relaxation](https://arxiv.org/abs/2509.17898)
*Paul Hamelbeck,Johannes Schiffer*

Main category: eess.SY

TL;DR: 本文提出RNN-SDP，一种基于松弛的方法，通过半定规划（SDP）为循环神经网络（RNN）计算Lipschitz常数的认证上限，以应对安全关键应用中的鲁棒性认证挑战。


<details>
  <summary>Details</summary>
Motivation: 在安全关键的控制应用中部署循环神经网络（RNN）时，对有界输入噪声或对抗性扰动进行鲁棒性认证变得越来越重要。

Method: RNN-SDP通过将RNN的层间交互建模为凸问题，并利用半定规划（SDP）计算Lipschitz常数的认证上限。该方法还探索了纳入已知输入约束的扩展，以进一步收紧Lipschitz界限。

Result: RNN-SDP在合成多罐系统上进行了评估。结果显示，虽然结合输入约束只带来了适度改进，但该通用方法即使在序列长度增加时也能产生相当紧密且可认证的界限。研究还强调了初始化误差常被低估的影响，这对于模型频繁重新初始化的应用（如模型预测控制MPC）是一个重要考虑因素。

Conclusion: RNN-SDP为RNN提供了一种有效且可认证的鲁棒性认证方法，能够产生合理的Lipschitz常数上限。研究同时指出，初始化误差是影响RNN鲁棒性的一个关键因素。

Abstract: Robustness certification against bounded input noise or adversarial
perturbations is increasingly important for deployment recurrent neural
networks (RNNs) in safety-critical control applications. To address this
challenge, we present RNN-SDP, a relaxation based method that models the RNN's
layer interactions as a convex problem and computes a certified upper bound on
the Lipschitz constant via semidefinite programming (SDP). We also explore an
extension that incorporates known input constraints to further tighten the
resulting Lipschitz bounds. RNN-SDP is evaluated on a synthetic multi-tank
system, with upper bounds compared to empirical estimates. While incorporating
input constraints yields only modest improvements, the general method produces
reasonably tight and certifiable bounds, even as sequence length increases. The
results also underscore the often underestimated impact of initialization
errors, an important consideration for applications where models are frequently
re-initialized, such as model predictive control (MPC).

</details>


### [522] [Developing a Dynamic Mobility Model for Backcasting Applications: A Case Study with Shared Autonomous Vehicles](https://arxiv.org/abs/2509.17928)
*Théotime Héraud,Vinith Lakshmanan,Antonio Sciaretta*

Main category: eess.SY

TL;DR: 本研究提出将回溯法应用于出行模型，以制定最佳脱碳路线图，并引入共享自动驾驶车队作为决策变量。模拟结果表明，与预测法相比，回溯法在保持相同排放水平的同时，可将运营商成本降低10%。


<details>
  <summary>Details</summary>
Motivation: 研究旨在为出行模型定义一个最优的脱碳路线图，以应对气候变化和优化交通系统，特别关注共享自动驾驶车辆的引入。

Method: 研究采用回溯法应用于一个包含六个相互关联子模型的出行模型。决策变量是引入共享自动驾驶车队。通过详细介绍各子模型，并引入分析直接和间接影响的方法，识别了不良影响发生的必要条件。随后在预测和回溯框架下进行模拟。

Result: 模拟结果显示，回溯法相比预测法具有显著优势：在维持相同排放水平的前提下，能够将运营商成本降低10%。

Conclusion: 研究证明了回溯法在制定出行模型脱碳路线图方面的相关性和优越性，它能更有效地降低运营成本，同时不增加排放。

Abstract: This study proposes the application of a backcasting approach to a mobility
model with the aim of defining an optimal decarbonization roadmap. The selected
decision variable is the introduction of a fleet of shared autonomous vehicles.
The mobility model developed is composed of six interconnected sub-models.
After presenting each of these models in detail, a method is introduced to
analyze the direct and indirect effects of the measure, and a necessary
condition for the occurrence of an undesirable effect is identified.
Simulations in both forecasting and backcasting frameworks are then conducted,
demonstrating the relevance of backcasting: it enables a 10% reduction in
operator costs compared to forecasting results, while maintaining the same
level of emissions.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [523] [A Multi-Grid Implicit Neural Representation for Multi-View Videos](https://arxiv.org/abs/2509.16706)
*Qingyue Ling,Zhengxue Cheng,Donghui Feng,Shen Wang,Chen Zhu,Guo Lu,Heming Sun,Jiro Katto,Li Song*

Main category: eess.IV

TL;DR: 本文提出MV-MGINR，一种用于多视角视频的多网格隐式神经表示，通过结合时间索引网格、视角索引网格和时间-视角集成网格，有效压缩多视角视频并实现高质量重建。


<details>
  <summary>Details</summary>
Motivation: 多视角视频因其高分辨率和多摄像头拍摄，在存储和传输方面面临巨大挑战。

Method: 本文提出了MV-MGINR框架，它结合了：1) 时间索引网格（捕捉跨视角的共同内容）；2) 视角索引网格（捕捉跨时间的共同内容）；3) 集成时间与视角网格（捕捉特定视角和时间下的局部细节）。随后，使用合成网络对多网格潜在表示进行上采样以生成重建帧。此外，引入了运动感知损失以提高运动区域的重建质量。

Result: 与MPEG沉浸式视频测试模型TMIV相比，MV-MGINR在保持相同PSNR的情况下，实现了72.3%的比特率节省。

Conclusion: 所提出的框架有效整合了多视角视频的共同和局部特征，最终实现了高质量的重建。

Abstract: Multi-view videos are becoming widely used in different fields, but their
high resolution and multi-camera shooting raise significant challenges for
storage and transmission. In this paper, we propose MV-MGINR, a multi-grid
implicit neural representation for multi-view videos. It combines a
time-indexed grid, a view-indexed grid and an integrated time and view grid.
The first two grids capture common representative contents across each view and
time axis respectively, and the latter one captures local details under
specific view and time. Then, a synthesis net is used to upsample the
multi-grid latents and generate reconstructed frames. Additionally, a
motion-aware loss is introduced to enhance the reconstruction quality of moving
regions. The proposed framework effectively integrates the common and local
features of multi-view videos, ultimately achieving high-quality
reconstruction. Compared with MPEG immersive video test model TMIV, MV-MGINR
achieves bitrate savings of 72.3% while maintaining the same PSNR.

</details>


### [524] [Learning Scan-Adaptive MRI Undersampling Patterns with Pre-Optimized Mask Supervision](https://arxiv.org/abs/2509.16846)
*Aryan Dhar,Siddhant Gautam,Saiprasad Ravishankar*

Main category: eess.IV

TL;DR: 本文提出一个基于CNN的框架，通过学习预计算的扫描自适应优化掩模，直接从多线圈MRI数据中生成扫描特异性欠采样模式，以加速MRI采集并保持图像质量。


<details>
  <summary>Details</summary>
Motivation: 深度学习在加速MRI数据采集同时保持扫描质量方面获得了广泛关注。现有方法通常依赖于训练中的掩模优化，作者旨在开发一种更高效、更鲁棒的扫描特异性采样方法。

Method: 采用一个基于卷积神经网络（CNN）的框架，直接从多线圈MRI数据中学习欠采样模式。与以往方法不同，本方法使用预计算的扫描自适应优化掩模作为监督标签进行训练。训练过程交替优化一个重建器和一个数据驱动的采样网络，该采样网络根据观测到的低频k空间数据生成扫描特异性采样模式。

Result: 在fastMRI多线圈膝关节数据集上的实验表明，该方法显著提高了采样效率和图像重建质量。

Conclusion: 该研究提供了一个鲁棒的框架，通过深度学习增强了MRI采集效率。

Abstract: Deep learning techniques have gained considerable attention for their ability
to accelerate MRI data acquisition while maintaining scan quality. In this
work, we present a convolutional neural network (CNN) based framework for
learning undersampling patterns directly from multi-coil MRI data. Unlike prior
approaches that rely on in-training mask optimization, our method is trained
with precomputed scan-adaptive optimized masks as supervised labels, enabling
efficient and robust scan-specific sampling. The training procedure alternates
between optimizing a reconstructor and a data-driven sampling network, which
generates scan-specific sampling patterns from observed low-frequency $k$-space
data. Experiments on the fastMRI multi-coil knee dataset demonstrate
significant improvements in sampling efficiency and image reconstruction
quality, providing a robust framework for enhancing MRI acquisition through
deep learning.

</details>


### [525] [A Chain-of-thought Reasoning Breast Ultrasound Dataset Covering All Histopathology Categories](https://arxiv.org/abs/2509.17046)
*Haojun Yu,Youcheng Li,Zihan Niu,Nan Zhang,Xuantong Gong,Huan Li,Zhiying Zou,Haifeng Qi,Zhenxiao Cao,Zijie Lan,Xingjian Yuan,Jiating He,Haokai Zhang,Shengtao Zhang,Zicheng Wang,Dong Wang,Ziwei Zhao,Congying Chen,Yong Wang,Wangyan Qin,Qingli Zhu*

Main category: eess.IV

TL;DR: 本文提出了BUS-CoT数据集，这是一个大规模乳腺超声（BUS）数据集，包含丰富的链式思维（CoT）推理标注和所有组织病理学类型，旨在促进AI在乳腺病变诊断中的CoT推理和罕见病例的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前用于AI开发的公开高质量乳腺超声基准数据集在数据规模和标注丰富度方面存在限制，尤其缺乏用于链式思维（CoT）推理分析的数据集，且难以覆盖所有组织病理学类型，导致AI在罕见病例中容易出错。

Method: 研究者构建了BUS-CoT数据集，包含11,439张图像、10,019个病变和4,838名患者的数据，涵盖了全部99种组织病理学类型。为了促进CoT推理研究，该数据集基于观察、特征、诊断和病理学标签构建了推理过程，并由经验丰富的专家进行标注和验证。

Result: BUS-CoT数据集成功地提供了大规模、高质量的乳腺超声数据，包含了所有99种组织病理学类型，并通过专家标注和验证，构建了详细的链式思维推理过程。这为AI系统开发提供了前所未有的资源，特别是在促进CoT推理和处理罕见病例方面。

Conclusion: BUS-CoT数据集的发布，为乳腺超声AI系统的开发提供了重要的资源，特别是通过其大规模数据、丰富的链式思维推理标注以及对所有组织病理学类型的覆盖，有助于开发更具鲁棒性的AI系统，尤其在临床实践中容易出错的罕见病例方面。

Abstract: Breast ultrasound (BUS) is an essential tool for diagnosing breast lesions,
with millions of examinations per year. However, publicly available
high-quality BUS benchmarks for AI development are limited in data scale and
annotation richness. In this work, we present BUS-CoT, a BUS dataset for
chain-of-thought (CoT) reasoning analysis, which contains 11,439 images of
10,019 lesions from 4,838 patients and covers all 99 histopathology types. To
facilitate research on incentivizing CoT reasoning, we construct the reasoning
processes based on observation, feature, diagnosis and pathology labels,
annotated and verified by experienced experts. Moreover, by covering lesions of
all histopathology types, we aim to facilitate robust AI systems in rare cases,
which can be error-prone in clinical practice.

</details>


### [526] [Investigation of ArUco Marker Placement for Planar Indoor Localization](https://arxiv.org/abs/2509.17345)
*Sven Hinderer,Martina Scheffler,Bin Yang*

Main category: eess.IV

TL;DR: 本文研究了基于ArUco标定板的自主移动机器人（AMR）室内定位行为，并提出了一个带有自适应测量噪声方差的卡尔曼滤波器用于实时跟踪。


<details>
  <summary>Details</summary>
Motivation: 使用标定板进行AMR室内定位具有传感器简单（单目摄像头）、标定板成本低廉且可大规模部署、系统复杂性和成本增加较小的优势。

Method: 1. 调查了ArUco标定板定位框架在不同标定板放置条件下的定位行为，包括标定板数量、相对于摄像头的方向以及摄像头-标定板距离。2. 提出了一个带有自适应测量噪声方差的简单卡尔曼滤波器，用于AMR的实时跟踪。

Result: 抽象中未明确给出具体的实验结果或滤波器性能数据，主要描述了研究的内容和提出的方法。

Conclusion: 抽象中未给出明确的结论，主要介绍了研究范围和提出的解决方案，即评估ArUco定位并提出自适应卡尔曼滤波器以实现AMR的实时跟踪。

Abstract: Indoor localization of autonomous mobile robots (AMRs) can be realized with
fiducial markers. Such systems require only a simple, monocular camera as
sensor and fiducial markers as passive, identifiable position references that
can be printed on a piece of paper and distributed in the area of interest.
Thus, fiducial marker systems can be scaled to large areas with a minor
increase in system complexity and cost. We investigate the localization
behavior of the fiducial marker framework ArUco w.r.t. the placement of the
markers including the number of markers, their orientation w.r.t. the camera,
and the camera-marker distance. In addition, we propose a simple Kalman filter
with adaptive measurement noise variances for real-time AMR tracking.

</details>


### [527] [GroundGazer: Camera-based indoor localization of mobile robots with millimeter accuracy at low cost](https://arxiv.org/abs/2509.17346)
*Sven Hinderer,Jakob Hüsken,Bohan Sun,Bin Yang*

Main category: eess.IV

TL;DR: 本文介绍了一种名为GroundGazer (GG)的平面室内定位系统，它利用单目（鱼眼）摄像头和棋盘格地板，为自主移动机器人提供毫米级定位和亚度级航向精度，且成本低廉。


<details>
  <summary>Details</summary>
Motivation: 目前高精度的室内定位系统（如使用激光雷达、全站仪和多高端摄像头的运动捕捉系统）成本非常高昂。

Method: GroundGazer系统仅需一个单目（鱼眼）摄像头、一个棋盘格地板，并可选择性地使用激光二极管来估计自主移动机器人（AMR）的位置和航向。

Result: 该系统能实现毫米级的定位精度和亚度级的航向精度。它具有简单、低成本、易于设置、便携、鲁棒、可扩展到大面积和机器人群体的优点，并有可能扩展到三维位置和姿态估计。

Conclusion: GroundGazer提供了一种高精度、低成本的平面室内定位解决方案，适用于自主移动机器人，并具有良好的扩展性和鲁棒性。

Abstract: Highly accurate indoor localization systems with mm positioning accuracy are
currently very expensive. They include range finders (such as LiDAR),
tachymeters, and motion capture systems relying on multiple high-end cameras.
In this work, we introduce a high-accuracy, planar indoor localization system
named GroundGazer (GG) for autonomous mobile robots (AMRs). GG estimates the
AMR's position with mm and its heading with sub-degree accuracy. The system
requires only a monocular (fisheye) camera, a chessboard floor, and an optional
laser diode. Our system is simple and low-cost, easy to set up, portable,
robust, scalable to large areas and robot swarms, and potentially extendable to
3D position and orientation estimation.

</details>


### [528] [RnGCam: High-speed video from rolling & global shutter measurements](https://arxiv.org/abs/2509.18087)
*Kevin Tandi,Xiang Dai,Chinmay Talegaonkar,Gal Mishne,Nick Antipa*

Main category: eess.IV

TL;DR: RnGCam是一种新型系统，它融合了消费级卷帘快门（RS）和全局快门（GS）传感器的测量数据，利用隐式神经表示（INR）重建高帧率视频，解决了传统压缩视频系统成本高、场景限制大的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的压缩视频捕获方法依赖昂贵的硬件，并且仅限于成像稀疏场景（背景为空），这限制了其应用范围和可访问性。

Method: 该系统名为RnGCam，融合了低速消费级卷帘快门（RS）和全局快门（GS）传感器的测量数据，以千赫兹帧率生成视频。RS传感器与伪随机光学元件（即扩散器）结合，提供低空间细节和高时间细节；GS传感器与传统镜头结合，提供高空间细节和低时间细节。提出了一种使用隐式神经表示（INR）的重建方法，将测量数据融合为高速视频，该方法独立建模静态和动态场景组件，并明确正则化动态部分。

Result: 在仿真中，该方法显著优于以前的RS压缩视频方法以及最先进的帧插值器。在双摄像头硬件设置中验证了该方法，能够以4,800帧/秒的速度为密集场景生成230帧视频，且硬件成本比以前的压缩视频系统低10倍。

Conclusion: RnGCam提供了一种经济高效且性能卓越的解决方案，能够利用消费级硬件捕获高帧率、高空间质量的密集场景视频，显著优于现有技术。

Abstract: Compressive video capture encodes a short high-speed video into a single
measurement using a low-speed sensor, then computationally reconstructs the
original video. Prior implementations rely on expensive hardware and are
restricted to imaging sparse scenes with empty backgrounds. We propose RnGCam,
a system that fuses measurements from low-speed consumer-grade rolling-shutter
(RS) and global-shutter (GS) sensors into video at kHz frame rates. The RS
sensor is combined with a pseudorandom optic, called a diffuser, which
spatially multiplexes scene information. The GS sensor is coupled with a
conventional lens. The RS-diffuser provides low spatial detail and high
temporal detail, complementing the GS-lens system's high spatial detail and low
temporal detail. We propose a reconstruction method using implicit neural
representations (INR) to fuse the measurements into a high-speed video. Our INR
method separately models the static and dynamic scene components, while
explicitly regularizing dynamics. In simulation, we show that our approach
significantly outperforms previous RS compressive video methods, as well as
state-of-the-art frame interpolators. We validate our approach in a dual-camera
hardware setup, which generates 230 frames of video at 4,800 frames per second
for dense scenes, using hardware that costs $10\times$ less than previous
compressive video systems.

</details>
