{"id": "2512.11333", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.11333", "abs": "https://arxiv.org/abs/2512.11333", "authors": ["Kai Kang", "Xiaoyu Peng", "Kui Luo", "Xi Ru", "Feng Liu"], "title": "Controlled Evolution-Based Day-Ahead Robust Dispatch Considering Frequency Security with Frequency Regulation Loads and Curtailable Loads", "comment": "2025 IEEE Power & Energy Society General Meeting (PESGM)", "summary": "With the extensive integration of volatile and uncertain renewable energy, power systems face significant challenges in primary frequency regulation due to instantaneous power fluctuations. However, the maximum frequency deviation constraint is inherently non-convex, and commonly used two-stage dispatch methods overlook causality, potentially resulting in infeasible day-ahead decisions. This paper presents a controlled evolution-based day-ahead robust dispatch method to address these issues. First, we suggest the convex relaxation technique to transform the maximum frequency deviation constraint to facilitate optimization. Then, an evolution-based robust dispatch framework is introduced to align day-ahead decisions with intraday strategies, ensuring both frequency security and power supply reliability. Additionally, a novel controlled evolution-based algorithm is developed to solve this framework efficiently. Case studies on a modified IEEE 14-bus system demonstrate the superiority of the proposed method in enhancing frequency security and system reliability.", "AI": {"tldr": "本文提出了一种基于受控演化的日前鲁棒调度方法，通过凸松弛处理非凸频率偏差约束，并引入演化框架解决日前与日内策略的因果关系，以提高含可再生能源电力系统的频率安全性和供电可靠性。", "motivation": "随着波动和不确定性可再生能源的广泛整合，电力系统在一次调频方面面临严峻挑战，尤其是在瞬时功率波动方面。然而，最大频率偏差约束本质上是非凸的，并且常用的两阶段调度方法忽略了因果关系，可能导致不可行的日前决策。", "method": "首先，采用凸松弛技术将最大频率偏差约束转换为可优化形式。其次，引入了一个基于演化的鲁棒调度框架，旨在使日前决策与日内策略保持一致，确保频率安全性和电力供应可靠性。此外，开发了一种新颖的基于受控演化的算法来高效求解该框架。", "result": "在修改后的IEEE 14节点系统上的案例研究表明，所提出的方法在增强频率安全性和系统可靠性方面具有优越性。", "conclusion": "所提出的基于受控演化的日前鲁棒调度方法能有效解决含可再生能源电力系统的频率调节挑战，通过处理非凸约束和考虑决策因果关系，显著提升了系统的频率安全性和供电可靠性。"}}
{"id": "2512.11047", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11047", "abs": "https://arxiv.org/abs/2512.11047", "authors": ["Haoran Jiang", "Jin Chen", "Qingwen Bu", "Li Chen", "Modi Shi", "Yanjie Zhang", "Delong Li", "Chuanzhe Suo", "Chuang Wang", "Zhihui Peng", "Hongyang Li"], "title": "WholeBodyVLA: Towards Unified Latent VLA for Whole-Body Loco-Manipulation Control", "comment": null, "summary": "Humanoid robots require precise locomotion and dexterous manipulation to perform challenging loco-manipulation tasks. Yet existing approaches, modular or end-to-end, are deficient in manipulation-aware locomotion. This confines the robot to a limited workspace, preventing it from performing large-space loco-manipulation. We attribute this to: (1) the challenge of acquiring loco-manipulation knowledge due to the scarcity of humanoid teleoperation data, and (2) the difficulty of faithfully and reliably executing locomotion commands, stemming from the limited precision and stability of existing RL controllers. To acquire richer loco-manipulation knowledge, we propose a unified latent learning framework that enables Vision-Language-Action (VLA) system to learn from low-cost action-free egocentric videos. Moreover, an efficient human data collection pipeline is devised to augment the dataset and scale the benefits. To more precisely execute the desired locomotion commands, we present a loco-manipulation-oriented (LMO) RL policy specifically tailored for accurate and stable core loco-manipulation movements, such as advancing, turning, and squatting. Building on these components, we introduce WholeBodyVLA, a unified framework for humanoid loco-manipulation. To the best of our knowledge, WholeBodyVLA is one of its kind enabling large-space humanoid loco-manipulation. It is verified via comprehensive experiments on the AgiBot X2 humanoid, outperforming prior baseline by 21.3%. It also demonstrates strong generalization and high extensibility across a broad range of tasks.", "AI": {"tldr": "该研究提出了WholeBodyVLA，一个统一的类人机器人运动-操作框架，通过从无动作的第一视角视频中学习并结合专门的强化学习策略，实现了大空间类人机器人运动-操作，解决了现有方法在操作感知运动和控制器精度方面的不足。", "motivation": "现有的类人机器人运动-操作方法（无论是模块化还是端到端）在操作感知运动方面存在不足，导致机器人工作空间受限，无法执行大空间运动-操作任务。这主要是由于：1) 缺乏类人机器人遥操作数据，难以获取运动-操作知识；2) 现有强化学习控制器精度和稳定性有限，难以忠实可靠地执行运动指令。", "method": "为获取更丰富的运动-操作知识，提出了一种统一的潜在学习框架，使视觉-语言-动作（VLA）系统能够从低成本、无动作的第一视角视频中学习。同时，设计了一个高效的人类数据收集流程来扩充数据集。为更精确地执行所需的运动指令，提出了一种面向运动-操作（LMO）的强化学习策略，专门针对精确稳定的核心运动-操作动作（如前进、转弯、下蹲）进行优化。将这些组件整合，构建了WholeBodyVLA统一框架。", "result": "WholeBodyVLA在AgiBot X2类人机器人上通过综合实验验证，性能优于现有基线21.3%。它还展示了强大的泛化能力和在各种任务中的高度可扩展性。据作者所知，这是首个实现大空间类人机器人运动-操作的框架。", "conclusion": "该研究成功构建了WholeBodyVLA，一个能够实现大空间类人机器人运动-操作的统一框架，解决了现有方法在操作感知运动和控制器执行精度方面的挑战。通过结合从无动作视频中学习的VLA系统和专门的LMO强化学习策略，显著提升了类人机器人在复杂任务中的表现和泛化能力。"}}
{"id": "2512.11015", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.11015", "abs": "https://arxiv.org/abs/2512.11015", "authors": ["Anoop Krishnan"], "title": "Leveraging Text Guidance for Enhancing Demographic Fairness in Gender Classification", "comment": null, "summary": "In the quest for fairness in artificial intelligence, novel approaches to enhance it in facial image based gender classification algorithms using text guided methodologies are presented. The core methodology involves leveraging semantic information from image captions during model training to improve generalization capabilities. Two key strategies are presented: Image Text Matching (ITM) guidance and Image Text fusion. ITM guidance trains the model to discern fine grained alignments between images and texts to obtain enhanced multimodal representations. Image text fusion combines both modalities into comprehensive representations for improved fairness. Exensive experiments conducted on benchmark datasets demonstrate these approaches effectively mitigate bias and improve accuracy across gender racial groups compared to existing methods. Additionally, the unique integration of textual guidance underscores an interpretable and intuitive training paradigm for computer vision systems. By scrutinizing the extent to which semantic information reduces disparities, this research offers valuable insights into cultivating more equitable facial analysis algorithms. The proposed methodologies contribute to addressing the pivotal challenge of demographic bias in gender classification from facial images. Furthermore, this technique operates in the absence of demographic labels and is application agnostic.", "AI": {"tldr": "该研究提出使用文本指导方法（图像-文本匹配指导和图像-文本融合）来提升基于面部图像的性别分类算法的公平性，有效减轻了人口统计学偏见并提高了准确性，且无需人口统计学标签。", "motivation": "在人工智能中寻求公平性，特别是解决面部图像性别分类算法中存在的偏见问题，是促使这项研究的关键因素。研究旨在通过新颖方法提升算法的公平性。", "method": "核心方法是利用图像字幕中的语义信息来指导模型训练，以提高泛化能力。具体包括两种策略：1. 图像-文本匹配（ITM）指导：训练模型辨别图像和文本之间的细粒度对齐，以获得增强的多模态表示。2. 图像-文本融合：将图像和文本两种模态结合成全面的表示，以改善公平性。", "result": "在基准数据集上进行的广泛实验表明，这些方法与现有方法相比，能有效减轻偏见并提高跨性别种族群体的准确性。此外，该技术无需人口统计学标签即可操作，并且与应用无关。", "conclusion": "该研究通过独特地整合文本指导，为计算机视觉系统提供了一种可解释且直观的训练范式。通过审视语义信息在减少差异方面的作用，为培养更公平的面部分析算法提供了宝贵见解，并为解决面部图像性别分类中的人口统计学偏见这一关键挑战做出了贡献。"}}
{"id": "2512.11086", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2512.11086", "abs": "https://arxiv.org/abs/2512.11086", "authors": ["Randy Palamar", "Darren Dahunsi", "Tyler Henry", "Mohammad Rahim Sobhani", "Roger Zemp"], "title": "An Open Source Realtime GPU Beamformer for Row-Column and Top Orthogonal to Bottom Electrode (TOBE) Arrays", "comment": "17 pages, 11 figures. for mentioned datasets, videos, and files see: https://drive.google.com/drive/folders/1ZqfkJjFfx6JA0gwoHjLdohN1dp-p2Yfu?usp=sharing", "summary": "Research ultrasound platforms have enabled many next-generation imaging sequences but have lacked realtime navigation capabilities for emerging 2D arrays such as row-column arrays (RCAs). We present an open-source, GPU-accelerated reconstruction and rendering software suite integrated with a programmable ultrasound platform and novel electrostrictive Top-Orthogonal-to-Bottom-Electrode (TOBE) arrays. The system supports advanced real-time modes, including cross-plane aperture-encoded synthetic-aperture imaging and aperture-encoded volumetric scanning. TOBE-enabled methods demonstrate improved image quality and expanded field of view compared with conventional RCA techniques. The software implements beamforming and rendering kernels using OpenGL compute shaders and is designed for maximum data throughput helping to minimize stalls and latency. Accompanying sample datasets and example scripts for offline reconstruction are provided to facilitate external testing.", "AI": {"tldr": "本文介绍了一个开源、GPU加速的超声重建和渲染软件套件，它与可编程超声平台和新型TOBE阵列集成，实现了2D阵列的实时导航，并提供改进的图像质量和更宽的视野。", "motivation": "研究型超声平台虽然支持许多下一代成像序列，但缺乏针对新兴2D阵列（如行-列阵列，RCAs）的实时导航能力。", "method": "开发了一个开源、GPU加速的重建和渲染软件套件，并将其与可编程超声平台和新型电致伸缩TOBE（Top-Orthogonal-to-Bottom-Electrode）阵列集成。该系统使用OpenGL计算着色器实现波束形成和渲染内核，支持跨平面孔径编码合成孔径成像和孔径编码容积扫描等高级实时模式，旨在最大化数据吞吐量以减少延迟。", "result": "该系统支持高级实时模式，并且与传统RCA技术相比，TOBE阵列实现的方法展示了改进的图像质量和扩展的视野。软件设计实现了最大数据吞吐量，有助于最小化停顿和延迟。此外，还提供了样本数据集和离线重建示例脚本以方便外部测试。", "conclusion": "所提出的系统（包括软件和TOBE阵列）为2D超声阵列提供了实时导航能力，显著提升了图像质量并扩展了视野，同时通过开源方式和提供的资源促进了进一步的研究和测试。"}}
{"id": "2512.10967", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2512.10967", "abs": "https://arxiv.org/abs/2512.10967", "authors": ["Subham Kumar", "Prakrithi Shivaprakash", "Abhishek Manoharan", "Astut Kurariya", "Diptadhi Mukherjee", "Lekhansh Shukla", "Animesh Mukherjee", "Prabhat Chand", "Pratima Murthy"], "title": "ASR Under the Stethoscope: Evaluating Biases in Clinical Speech Recognition across Indian Languages", "comment": null, "summary": "Automatic Speech Recognition (ASR) is increasingly used to document clinical encounters, yet its reliability in multilingual and demographically diverse Indian healthcare contexts remains largely unknown. In this study, we conduct the first systematic audit of ASR performance on real world clinical interview data spanning Kannada, Hindi, and Indian English, comparing leading models including Indic Whisper, Whisper, Sarvam, Google speech to text, Gemma3n, Omnilingual, Vaani, and Gemini. We evaluate transcription accuracy across languages, speakers, and demographic subgroups, with a particular focus on error patterns affecting patients vs. clinicians and gender based or intersectional disparities. Our results reveal substantial variability across models and languages, with some systems performing competitively on Indian English but failing on code mixed or vernacular speech. We also uncover systematic performance gaps tied to speaker role and gender, raising concerns about equitable deployment in clinical settings. By providing a comprehensive multilingual benchmark and fairness analysis, our work highlights the need for culturally and demographically inclusive ASR development for healthcare ecosystem in India.", "AI": {"tldr": "首次系统评估了ASR在印度多语言临床场景中的表现，发现模型和语言之间存在显著差异，并存在与说话者角色和性别相关的系统性性能差距。", "motivation": "自动语音识别（ASR）在临床中日益普及，但其在印度多语言和人口多样化的医疗环境中的可靠性尚不清楚。", "method": "对涵盖卡纳达语、印地语和印度英语的真实世界临床访谈数据进行了首次系统性ASR性能审计。比较了包括Indic Whisper、Whisper、Sarvam、Google speech to text、Gemma3n、Omnilingual、Vaani和Gemini在内的领先模型。评估了跨语言、说话者和人口统计亚群的转录准确性，特别关注影响患者与临床医生的错误模式以及基于性别的或交叉的差异。", "result": "结果显示模型和语言之间存在显著差异，一些系统在印度英语上表现出色，但在混合语或方言语音上失败。还发现与说话者角色和性别相关的系统性性能差距。", "conclusion": "这项工作引发了对ASR在临床环境中公平部署的担忧，并强调了为印度医疗生态系统开发文化和人口统计学上包容的ASR的必要性。"}}
{"id": "2512.11308", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.11308", "abs": "https://arxiv.org/abs/2512.11308", "authors": ["Kazuyoshi Fukuda", "Masaki Inoue", "Riko Asanaka"], "title": "Gig-work Management System with Chance-Constraints Verification Algorithm", "comment": "6 pages, 5 figures, submitted to IFAC World Congress 2026", "summary": "This paper proposes the framework of an efficient gig-work management system. A gig-work management system recommends one-off tasks with information about task hours and wages to gig-workers. To enable effective management, this paper develops a model of gig-workers' decision-making. Then, based on the model, we formulate an optimization problem to determine the optimal task hours and wages. The formulated problem belongs to the class of chance-constrained model predictive control (CC-MPC) problems. To efficiently solve the CC-MPC problem, we develop an approximate solution algorithm with guaranteed confidence levels. Finally, we develop gig-worker models based on data collected through crowdsourcing.", "AI": {"tldr": "本文提出了一个高效的零工管理系统框架，通过建模零工决策并利用机会约束模型预测控制（CC-MPC）优化任务时间和工资，并开发了近似求解算法。", "motivation": "为了实现对零工任务的有效管理，即向零工推荐一次性任务并提供任务时长和薪资信息，需要一个高效的零工管理系统。", "method": "本文首先建立了零工决策模型，然后基于该模型将任务时长和薪资的确定问题表述为机会约束模型预测控制（CC-MPC）问题。为高效求解CC-MPC问题，开发了一种具有置信水平保证的近似求解算法。最后，基于众包数据建立了零工模型。", "result": "开发了一个具有置信水平保证的近似求解算法，用于高效解决所提出的CC-MPC问题。同时，基于众包数据成功建立了零工模型。", "conclusion": "本文提出了一个高效的零工管理系统框架，通过建模零工决策、将任务优化问题公式化为CC-MPC并开发相应的近似求解算法，实现了对零工任务的有效管理，并利用众包数据验证了零工模型。"}}
{"id": "2512.11228", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.11228", "abs": "https://arxiv.org/abs/2512.11228", "authors": ["Navneet Kaur", "Christopher J. Adams", "William E. Singhose", "Santosh Devasia"], "title": "Mitigating Dynamic Tip-Over during Mobile Crane Slewing using Input Shaping", "comment": "Submitted to conference", "summary": "Payload swing during rapid slewing of mobile cranes poses a safety risk, as it generates overturning moments that can lead to tip-over accidents of mobile cranes. Currently, to limit the risk of tip-over, mobile crane operators are forced to either reduce the slewing speed (which lowers productivity) or reduce the load being carried to reduce the induced moments. Both of these approaches reduce productivity. This paper seeks to enable rapid slewing without compromising safety by applying input shaping to the crane-slewing commands generated by the operator. A key advantage of this approach is that the input shaper requires only the information about the rope length, and does not require detailed mobile crane dynamics. Simulations and experiments show that the proposed method reduces residual payload swing and enables significantly higher slewing speeds without tip over, reducing slewing completion time by at least 38% compared to unshaped control. Human control with input shaping improves task completion time by 13%, reduces the peak swing by 18%, and reduces the potential of collisions by 82% when compared to unshaped control. Moreover, shaped control with a human had no tip-over, whereas large swing led to tip-over without input shaping. Thereby, the proposed method substantially recovers the operational-safety envelope of mobile cranes (designed to avoid tip-over using static analysis) that would otherwise be lost in dynamic conditions. Videos and demonstrations are available at https://youtu.be/dVy3bbIhrBU.", "AI": {"tldr": "该研究提出使用输入整形技术来减少移动起重机快速回转时的负载摆动，从而提高操作速度、安全性和生产力，同时避免倾翻风险。", "motivation": "移动起重机在快速回转时，负载摆动会产生倾覆力矩，导致起重机倾翻事故，存在安全隐患。为降低风险，操作员被迫降低回转速度或减少负载，这两种方法都会降低生产力。本研究旨在实现在不影响安全的前提下进行快速回转。", "method": "该方法通过对操作员生成的起重机回转指令应用输入整形技术。这种方法的关键优势在于，输入整形器仅需要绳索长度信息，而不需要详细的移动起重机动力学模型。", "result": "模拟和实验表明，所提出的方法减少了残余负载摆动，并实现了显著更高的回转速度而不会倾翻。与未整形控制相比，回转完成时间至少缩短了38%。结合人工控制，输入整形使任务完成时间缩短13%，峰值摆动减少18%，碰撞可能性降低82%。此外，带输入整形的人工控制未发生倾翻，而无输入整形的大摆动导致了倾翻。因此，该方法显著恢复了移动起重机在动态条件下原本会丧失的操作安全范围。", "conclusion": "所提出的输入整形方法能有效减少移动起重机在快速回转时的负载摆动，显著提高操作速度和安全性，通过恢复起重机的动态安全包络，从而提高生产力并预防倾翻事故。"}}
{"id": "2512.11468", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.11468", "abs": "https://arxiv.org/abs/2512.11468", "authors": ["Alejandra Sandoval-Carranza", "Juan E. Machado", "Johannes Schiffer"], "title": "An Input-Output Data-Driven Dissipativity Approach for Compositional Stability Certification of Interconnected LTI MIMO Systems", "comment": null, "summary": "We propose an input-output data-driven framework for certifying the stability of interconnected multiple-input-multiple-output linear time-invariant discrete-time systems via QSR-dissipativity. That is, by using measured input-output trajectories of each subsystem, we verify dissipative properties and extract local passivity indices without requiring an explicit model identification.These passivity indices are then used to derive conditions under which the equilibrium of the interconnected system is stable. In particular, the framework identifies how the lack of passivity in some subsystems can be compensated by surpluses in others. The proposed approach enables a compositional stability analysis by combining subsystem-level conditions into a criterion valid for the overall interconnected system. We illustrate via a numerical case study, how to compute channel-wise passivity indices and infer stability guarantees directly from data with the proposed method.", "AI": {"tldr": "本文提出了一种数据驱动的框架，通过QSR耗散性，利用测量到的输入输出数据来验证互联多输入多输出线性时不变离散时间系统的稳定性，无需显式模型识别。", "motivation": "研究动机是为了在不进行显式模型识别的情况下，直接从输入输出数据中验证互联系统的稳定性，并实现组合式稳定性分析。", "method": "该方法通过使用每个子系统的测量输入输出轨迹来验证耗散性并提取局部无源性指标（通过QSR耗散性）。然后，这些无源性指标被用于推导互联系统平衡点稳定的条件，并识别某些子系统无源性不足如何被其他子系统的盈余所补偿。", "result": "该框架能够直接从数据中计算通道级无源性指标，并推断稳定性保证。通过数值案例研究，证明了该方法可以实现组合式稳定性分析，将子系统级别的条件组合成适用于整个互联系统的准则。", "conclusion": "该研究成功开发了一种数据驱动的、组合式的稳定性分析方法，通过从输入输出数据中提取无源性指标，为互联系统的整体稳定性提供了保证，且无需显式模型识别。"}}
{"id": "2512.11244", "categories": ["eess.SY", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2512.11244", "abs": "https://arxiv.org/abs/2512.11244", "authors": ["Taishi Kotsuka", "Enoch Yeung"], "title": "Model Reduction of Multicellular Communication Systems via Singular Perturbation: Sender Receiver Systems", "comment": null, "summary": "We investigate multicellular sender receiver systems embedded in hydrogel beads, where diffusible signals mediate interactions among heterogeneous cells. Such systems are modeled by PDE ODE couplings that combine three dimensional diffusion with nonlinear intracellular dynamics, making analysis and simulation challenging. We show that the diffusion dynamics converges exponentially to a quasi steady spatial profile and use singular perturbation theory to reduce the model to a finite dimensional multiagent network. A closed form communication matrix derived from the spherical Green's function captures the effective sender receiver coupling. Numerical results show the reduced model closely matches the full dynamics while enabling scalable simulation of large cell populations.", "AI": {"tldr": "该研究通过奇异摄动理论将水凝胶珠中异质细胞间信号传递的复杂PDE-ODE模型简化为有限维多智能体网络，实现了大规模细胞群的可扩展模拟，且与完整模型高度匹配。", "motivation": "分析和模拟水凝胶珠中通过可扩散信号相互作用的多细胞发送-接收系统具有挑战性，因为它们涉及三维扩散与非线性细胞内动力学的偏微分方程-常微分方程（PDE-ODE）耦合。", "method": "研究方法包括：1) 使用PDE-ODE耦合模型描述系统；2) 证明扩散动力学指数收敛到准稳态空间分布；3) 利用奇异摄动理论将模型简化为有限维多智能体网络；4) 从球形格林函数推导出闭合形式的通信矩阵，以捕捉有效的发送-接收耦合。", "result": "研究结果表明：1) 扩散动力学以指数速度收敛到准稳态空间分布；2) 简化的模型（有限维多智能体网络）与完整动力学高度匹配；3) 简化模型能够对大型细胞群进行可扩展模拟。", "conclusion": "通过奇异摄动理论将复杂的PDE-ODE耦合模型简化为有限维多智能体网络是有效的，它能准确捕捉细胞间的有效耦合，并支持大规模多细胞发送-接收系统的可扩展模拟。"}}
{"id": "2512.11134", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2512.11134", "abs": "https://arxiv.org/abs/2512.11134", "authors": ["Juan Merlos", "Fabien Racapé", "Hyomin Choi", "Mateen Ulhaq", "Hari Kalva"], "title": "Feature Compression for Machines with Range-Based Channel Truncation and Frame Packing", "comment": "10 pages, 8 figures. Extended version of the paper with the same title presented at IEEE DCC 2025", "summary": "This paper proposes a method that enhances the compression performance of the current model under development for the upcoming MPEG standard on Feature Coding for Machines (FCM). This standard aims at providing inter-operable compressed bitstreams of features in the context of split computing, i.e., when the inference of a large computer vision neural-network (NN)-based model is split between two devices. Intermediate features can consist of multiple 3D tensors that can be reduced and entropy coded to limit the required bandwidth of such transmission. In the envisioned design for the MPEG-FCM standard, intermediate feature tensors may be reduced using Neural layers before being converted into 2D video frames that can be coded using existing video compression standards. This paper introduces an additional channel truncation and packing method which enables the system to preserve the relevant channels, depending on the statistics of the features at inference time, while preserving the computer vision task performance at the receiver. Implemented within the MPEG-FCM test model, the proposed method yields an average reduction in rate by 10.59% for a given accuracy on multiple computer vision tasks and datasets.", "AI": {"tldr": "本文提出了一种针对MPEG机器特征编码(FCM)标准的新方法，通过通道截断和打包，在保持计算机视觉任务性能的同时，显著提升了中间特征的压缩效率。", "motivation": "MPEG-FCM标准旨在为分层计算场景（即大型计算机视觉神经网络模型推理在两设备间拆分）提供可互操作的压缩特征码流。为限制传输带宽，需要对中间特征进行高效压缩。现有设计将3D张量转换为2D视频帧进行压缩，但仍有提升空间。", "method": "在MPEG-FCM测试模型中，本文引入了一种额外的通道截断和打包方法。该方法根据推理时特征的统计数据，保留相关通道，并将它们打包，从而在保持接收端计算机视觉任务性能的前提下，优化压缩。", "result": "将所提出的方法在MPEG-FCM测试模型中实施后，在多个计算机视觉任务和数据集上，以给定精度为前提，平均码率降低了10.59%。", "conclusion": "所提出的通道截断和打包方法，通过智能地管理特征通道，显著提高了MPEG-FCM标准下中间特征的压缩性能，同时确保了计算机视觉任务的准确性。"}}
{"id": "2512.11481", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.11481", "abs": "https://arxiv.org/abs/2512.11481", "authors": ["Severin Beger", "Sandra Hirche"], "title": "A Robust Model Predictive Control Method for Networked Control Systems", "comment": "Accepted for publication in the Proceedings of the 63rd IEEE Conference on Decision and Control", "summary": "Robustly compensating network constraints such as delays and packet dropouts in networked control systems is crucial for remotely controlling dynamical systems. This work proposes a novel prediction consistent method to cope with delays and packet losses as encountered in UDP-type communication systems. The augmented control system preserves all properties of the original model predictive control method under the network constraints. Furthermore, we propose to use linear tube MPC with the novel method and show that the system converges robustly to the origin under mild conditions. We illustrate this with simulation examples of a cart pole and a continuous stirred tank reactor.", "AI": {"tldr": "本文提出一种新颖的预测一致性方法，结合线性管MPC，以鲁棒地补偿网络化控制系统中的延迟和丢包问题，并确保系统收敛。", "motivation": "在网络化控制系统中，鲁棒地补偿网络约束（如延迟和丢包）对于远程控制动态系统至关重要。", "method": "提出了一种新颖的预测一致性方法来处理UDP类型通信系统中的延迟和丢包。该方法增强了控制系统，并与线性管模型预测控制（MPC）结合使用。", "result": "增强的控制系统在网络约束下保留了原始模型预测控制的所有特性。结合线性管MPC，系统在温和条件下能鲁棒地收敛到原点。通过倒立摆和连续搅拌釜反应器仿真进行了验证。", "conclusion": "所提出的预测一致性方法与线性管MPC相结合，能够鲁棒地应对网络化控制系统中的延迟和丢包，并确保系统鲁棒收敛到原点。"}}
{"id": "2512.11745", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11745", "abs": "https://arxiv.org/abs/2512.11745", "authors": ["Liqiang Huang", "Rachel W. Mills", "Saikiran Mandula", "Lin Bai", "Mahtab Jeyhani", "John Redell", "Hien Van Nguyen", "Saurabh Prasad", "Dragan Maric", "Badrinath Roysam"], "title": "mViSE: A Visual Search Engine for Analyzing Multiplex IHC Brain Tissue Images", "comment": null, "summary": "Whole-slide multiplex imaging of brain tissue generates massive information-dense images that are challenging to analyze and require custom software. We present an alternative query-driven programming-free strategy using a multiplex visual search engine (mViSE) that learns the multifaceted brain tissue chemoarchitecture, cytoarchitecture, and myeloarchitecture. Our divide-and-conquer strategy organizes the data into panels of related molecular markers and uses self-supervised learning to train a multiplex encoder for each panel with explicit visual confirmation of successful learning. Multiple panels can be combined to process visual queries for retrieving similar communities of individual cells or multicellular niches using information-theoretic methods. The retrievals can be used for diverse purposes including tissue exploration, delineating brain regions and cortical cell layers, profiling and comparing brain regions without computer programming. We validated mViSE's ability to retrieve single cells, proximal cell pairs, tissue patches, delineate cortical layers, brain regions and sub-regions. mViSE is provided as an open-source QuPath plug-in.", "AI": {"tldr": "mViSE是一个多重视觉搜索引擎，通过查询驱动、无需编程的策略，学习和分析全玻片多重脑组织图像，用于检索相似细胞群或多细胞微环境。", "motivation": "全玻片多重脑组织成像生成的信息密集型图像分析极具挑战性，且通常需要定制软件，这促使研究人员寻求一种无需编程的替代方案。", "method": "该研究提出一种查询驱动、无需编程的策略，即多重视觉搜索引擎（mViSE）。它采用分而治之的方法，将数据组织成相关分子标记的面板，并使用自监督学习为每个面板训练一个多重编码器，并通过明确的视觉确认来验证学习效果。多个面板可以结合起来，使用信息论方法处理视觉查询，以检索相似的个体细胞群或多细胞微环境。", "result": "mViSE能够成功检索单个细胞、邻近细胞对、组织斑块，并能描绘皮层层、大脑区域和子区域。它还支持组织探索、大脑区域的描绘和比较，所有这些都无需计算机编程。mViSE作为开源的QuPath插件提供。", "conclusion": "mViSE提供了一种有效的、无需编程的策略来分析复杂的多重脑组织图像，能够学习和利用组织的多方面化学结构、细胞结构和髓鞘结构，并已通过多种任务验证了其能力，为脑组织分析提供了一个易于使用的工具。"}}
{"id": "2512.10996", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.10996", "abs": "https://arxiv.org/abs/2512.10996", "authors": ["Seonok Kim"], "title": "MedBioRAG: Semantic Search and Retrieval-Augmented Generation with Large Language Models for Medical and Biological QA", "comment": "Submitted to ACL 2025. 9 pages, 4 figures, 5 tables (including 2 appendix tables)", "summary": "Recent advancements in retrieval-augmented generation (RAG) have significantly enhanced the ability of large language models (LLMs) to perform complex question-answering (QA) tasks. In this paper, we introduce MedBioRAG, a retrieval-augmented model designed to improve biomedical QA performance through a combination of semantic and lexical search, document retrieval, and supervised fine-tuning. MedBioRAG efficiently retrieves and ranks relevant biomedical documents, enabling precise and context-aware response generation. We evaluate MedBioRAG across text retrieval, close-ended QA, and long-form QA tasks using benchmark datasets such as NFCorpus, TREC-COVID, MedQA, PubMedQA, and BioASQ. Experimental results demonstrate that MedBioRAG outperforms previous state-of-the-art (SoTA) models and the GPT-4o base model in all evaluated tasks. Notably, our approach improves NDCG and MRR scores for document retrieval, while achieving higher accuracy in close-ended QA and ROUGE scores in long-form QA. Our findings highlight the effectiveness of semantic search-based retrieval and LLM fine-tuning in biomedical applications.", "AI": {"tldr": "MedBioRAG是一个结合语义和词汇搜索、文档检索及监督微调的检索增强生成（RAG）模型，显著提升了生物医学问答（QA）性能，在多项基准测试中超越了现有最先进模型和GPT-4o。", "motivation": "检索增强生成（RAG）在复杂问答任务中显著提升了大型语言模型（LLM）的能力，但生物医学问答领域仍有改进空间。", "method": "本文提出了MedBioRAG模型，通过结合语义和词汇搜索、文档检索以及监督微调来提升生物医学问答性能。该模型能高效检索和排序相关生物医学文档，实现精确和上下文感知的响应生成。MedBioRAG在NFCorpus、TREC-COVID、MedQA、PubMedQA和BioASQ等基准数据集上进行了文本检索、封闭式问答和长篇问答任务的评估。", "result": "实验结果表明，MedBioRAG在所有评估任务中均优于先前的最先进（SoTA）模型和GPT-4o基础模型。具体而言，它提高了文档检索的NDCG和MRR分数，在封闭式问答中实现了更高的准确性，并在长篇问答中获得了更高的ROUGE分数。", "conclusion": "研究结果强调了基于语义搜索的检索和LLM微调在生物医学应用中的有效性。"}}
{"id": "2512.11125", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.11125", "abs": "https://arxiv.org/abs/2512.11125", "authors": ["Benedictus C. G. Cinun", "Tua A. Tamba", "Immanuel R. Santjoko", "Xiaofeng Wang", "Michael A. Gunarso", "Bin Hu"], "title": "Design and Experimental Validation of Closed-Form CBF-Based Safe Control for Stewart Platform Under Multiple Constraints", "comment": "9 pages", "summary": "This letter presents a closed-form solution of Control Barrier Function (CBF) framework for enforcing safety constraints on a Stewart robotic platform. The proposed method simultaneously handles multiple position and velocity constraints through an explicit closed-form control law, eliminating the need to solve a Quadratic Program (QP) at every control step and enabling efficient real-time implementation. This letter derives necessary and sufficient conditions under which the closed-form expression remains non-singular, thereby ensuring well-posedness of the CBF solution to multi-constraint problem. The controller is validated in both simulation and hardware experiments on a custom-built Stewart platform prototype, demonstrating safetyguaranteed performance that is comparable to the QP-based formulation, while reducing computation time by more than an order of magnitude. The results confirm that the proposed approach provides a reliable and computationally lightweight framework for real-time safe control of parallel robotic systems. The experimental videos are available on the project website. (https://nail-uh.github.io/StewartPlatformSafeControl.github.io/)", "AI": {"tldr": "本文提出了一种针对Stewart平台安全控制的闭式控制障碍函数（CBF）解决方案，无需每步求解二次规划（QP），即可同时处理多个位置和速度约束，显著提升计算效率。", "motivation": "在Stewart机器人平台上实现实时、高效且安全的控制，同时处理多个安全约束是一个挑战。传统的QP-based CBF方法在每个控制步骤都需要求解QP，计算成本高，难以满足实时性要求。", "method": "研究提出了一种CBF框架的闭式解，用于强制执行Stewart平台的安全约束。该方法通过一个显式的闭式控制律同时处理多个位置和速度约束，避免了QP求解。此外，还推导了闭式表达式保持非奇异的充要条件，确保了多约束CBF解的适定性。", "result": "通过仿真和硬件实验，该控制器在保证安全性能方面与基于QP的方案相当，但计算时间减少了一个数量级以上。结果证实了所提方法为并行机器人系统的实时安全控制提供了一个可靠且计算开销低的框架。", "conclusion": "所提出的闭式CBF解决方案为Stewart平台提供了一种高效、可靠且计算量轻的实时安全控制框架，有效解决了多约束处理和实时性问题，并已通过仿真和硬件实验验证其性能。"}}
{"id": "2512.11080", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.11080", "abs": "https://arxiv.org/abs/2512.11080", "authors": ["Cedric-Pascal Sommer", "Robert J. Wood", "Justin Werfel"], "title": "Taxonomy and Modular Tool System for Versatile and Effective Non-Prehensile Manipulations", "comment": "34 pages, 10 figures, 2 tables, supplementary videos: https://youtu.be/Hcefy53PY0M, https://youtu.be/nFF9k91hsfU, https://youtu.be/EulPLskNIZQ", "summary": "General-purpose robotic end-effectors of limited complexity, like the parallel-jaw gripper, are appealing for their balance of simplicity and effectiveness in a wide range of manipulation tasks. However, while many such manipulators offer versatility in grasp-like interactions, they are not optimized for non-prehensile actions like pressing, rubbing, or scraping -- manipulations needed for many common tasks. To perform such tasks, humans use a range of different body parts or tools with different rigidity, friction, etc., according to the properties most effective for a given task. Here, we discuss a taxonomy for the key properties of a non-actuated end-effector, laying the groundwork for a systematic understanding of the affordances of non-prehensile manipulators. We then present a modular tool system, based on the taxonomy, that can be used by a standard two-fingered gripper to extend its versatility and effectiveness in performing such actions. We demonstrate the application of the tool system in aerospace and household scenarios that require a range of non-prehensile and prehensile manipulations.", "AI": {"tldr": "通用机器人夹具擅长抓取但缺乏非抓取操作能力。本文提出了一种非驱动末端执行器分类法和基于该分类法的模块化工具系统，以扩展标准夹具在按压、摩擦等非抓取任务中的效用，并在实际场景中进行了演示。", "motivation": "现有的通用机器人末端执行器（如平行夹爪）在抓取任务中表现良好，但对于按压、摩擦、刮擦等非抓取操作并不优化，而这些操作是许多常见任务所必需的。人类在执行此类任务时会根据需要使用具有不同刚度和摩擦力的工具或身体部位。", "method": "1. 讨论并建立了一套非驱动末端执行器关键属性的分类法，旨在系统理解非抓取操纵器的功能。2. 基于该分类法，设计并提出了一种模块化工具系统。3. 该系统旨在与标准两指夹具配合使用，以增强其执行非抓取动作的能力。", "result": "所提出的模块化工具系统能够有效扩展标准两指夹具在执行非抓取动作时的多功能性和有效性。研究在需要一系列非抓取和抓取操作的航空航天和家庭场景中，成功展示了该工具系统的实际应用。", "conclusion": "通过引入非驱动末端执行器分类法和基于此的模块化工具系统，可以显著提升通用机器人夹具在非抓取任务中的能力，使其在更广泛的实际应用场景中展现出更高的通用性和效率。"}}
{"id": "2512.11016", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11016", "abs": "https://arxiv.org/abs/2512.11016", "authors": ["Haolin Yang", "Jiayuan Rao", "Haoning Wu", "Weidi Xie"], "title": "SoccerMaster: A Vision Foundation Model for Soccer Understanding", "comment": null, "summary": "Soccer understanding has recently garnered growing research interest due to its domain-specific complexity and unique challenges. Unlike prior works that typically rely on isolated, task-specific expert models, this work aims to propose a unified model to handle diverse soccer visual understanding tasks, ranging from fine-grained perception (e.g., athlete detection) to semantic reasoning (e.g., event classification). Specifically, our contributions are threefold: (i) we present SoccerMaster, the first soccer-specific vision foundation model that unifies diverse understanding tasks within a single framework via supervised multi-task pretraining; (ii) we develop an automated data curation pipeline to generate scalable spatial annotations, and integrate them with various existing soccer video datasets to construct SoccerFactory, a comprehensive pretraining data resource; and (iii) we conduct extensive evaluations demonstrating that SoccerMaster consistently outperforms task-specific expert models across diverse downstream tasks, highlighting its breadth and superiority. The data, code, and model will be publicly available.", "AI": {"tldr": "本文提出了SoccerMaster，首个足球领域专用视觉基础模型，通过监督多任务预训练统一处理从精细感知到语义推理的多种足球视觉理解任务，并在综合数据集SoccerFactory上训练，表现优于传统任务专用模型。", "motivation": "由于足球领域固有的复杂性和独特挑战，以及现有研究多依赖于孤立、任务专用的专家模型，本文旨在提出一个统一模型，能够处理从细粒度感知（如运动员检测）到语义推理（如事件分类）的各种足球视觉理解任务。", "method": "1. 提出了SoccerMaster，首个足球专用视觉基础模型，通过监督多任务预训练在一个单一框架内统一处理多种理解任务。 2. 开发了自动化数据整理流水线，以生成可扩展的空间标注，并将其与现有足球视频数据集整合，构建了全面的预训练数据资源SoccerFactory。 3. 进行了广泛评估，以证明SoccerMaster的性能。", "result": "SoccerMaster在各种下游任务中始终优于任务专用的专家模型，展示了其广泛性和优越性。", "conclusion": "SoccerMaster作为统一的足球视觉理解模型，在性能和任务处理广度上均表现出色。相关数据、代码和模型将公开可用。"}}
{"id": "2512.11492", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.11492", "abs": "https://arxiv.org/abs/2512.11492", "authors": ["Severin Beger", "Yihui Lin", "Katarina Stanojevic", "Sandra Hirche"], "title": "Optimal Delay Compensation in Networked Predictive Control", "comment": "Submitted to the 2026 IFAC World Congress", "summary": "Networked Predictive Control is widely used to mitigate the effect of delays and dropouts in Networked Control Systems, particularly when these exceed the sampling time. A key design choice of these methods is the delay bound, which determines the prediction horizon and the robustness to information loss. This work develops a systematic method to select the optimal bound by quantifying the trade-off between prediction errors and open-loop operation caused by communication losses. Simulation studies demonstrate the performance gains achieved with the optimal bound.", "AI": {"tldr": "本文提出了一种系统方法，用于在网络化预测控制中选择最优延迟边界，以平衡预测误差和通信丢失导致的开环操作之间的权衡，从而提高系统性能。", "motivation": "网络化控制系统（NCS）中存在的延迟和数据丢失（尤其是当它们超过采样时间时）对系统性能有显著影响。网络化预测控制（NPC）常用于缓解这些问题，而延迟边界是NPC设计中的一个关键选择，它决定了预测范围和对信息丢失的鲁棒性。", "method": "研究开发了一种系统方法来选择最优的延迟边界。该方法通过量化预测误差与通信丢失引起的开环操作之间的权衡来确定最优边界。", "result": "仿真研究表明，使用最优延迟边界可以实现性能增益。", "conclusion": "所提出的系统方法能够有效地选择网络化预测控制中的最优延迟边界，从而在预测误差和开环操作之间取得平衡，并显著提升系统性能。"}}
{"id": "2512.11057", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11057", "abs": "https://arxiv.org/abs/2512.11057", "authors": ["Marshal Ashif Shawkat", "Moidul Hasan", "Taufiq Hasan"], "title": "Weakly Supervised Tuberculosis Localization in Chest X-rays through Knowledge Distillation", "comment": "18 pages, 9 figures, 4 tables", "summary": "Tuberculosis (TB) remains one of the leading causes of mortality worldwide, particularly in resource-limited countries. Chest X-ray (CXR) imaging serves as an accessible and cost-effective diagnostic tool but requires expert interpretation, which is often unavailable. Although machine learning models have shown high performance in TB classification, they often depend on spurious correlations and fail to generalize. Besides, building large datasets featuring high-quality annotations for medical images demands substantial resources and input from domain specialists, and typically involves several annotators reaching agreement, which results in enormous financial and logistical expenses. This study repurposes knowledge distillation technique to train CNN models reducing spurious correlations and localize TB-related abnormalities without requiring bounding-box annotations. By leveraging a teacher-student framework with ResNet50 architecture, the proposed method trained on TBX11k dataset achieve impressive 0.2428 mIOU score. Experimental results further reveal that the student model consistently outperforms the teacher, underscoring improved robustness and potential for broader clinical deployment in diverse settings.", "AI": {"tldr": "本研究利用知识蒸馏技术训练CNN模型，用于胸部X光片上的结核病（TB）检测，旨在减少虚假相关性并实现异常定位，无需边界框标注。学生模型表现优于教师模型，显示出更强的鲁棒性。", "motivation": "结核病是全球主要死因之一，尤其是在资源匮乏地区。胸部X光片（CXR）是一种可及且经济的诊断工具，但需要专家解读，而这往往不可得。现有机器学习模型虽然性能高，但常依赖虚假相关性且泛化能力差。此外，构建高质量标注的医学图像数据集成本高昂且耗时。", "method": "本研究重新利用知识蒸馏技术，采用基于ResNet50架构的师生框架来训练CNN模型。该方法旨在减少虚假相关性，并在不要求边界框标注的情况下定位结核病相关的异常。模型在TBX11k数据集上进行训练。", "result": "所提出的方法在TBX11k数据集上实现了0.2428的mIOU分数。实验结果进一步表明，学生模型始终优于教师模型，突显了其改进的鲁棒性。", "conclusion": "该研究表明，通过知识蒸馏训练的学生模型在结核病检测中表现出更高的鲁棒性，并具有在不同临床环境中广泛部署的潜力。"}}
{"id": "2512.11173", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.11173", "abs": "https://arxiv.org/abs/2512.11173", "authors": ["Tzu-Hsien Lee", "Fidan Mahmudova", "Karthik Desingh"], "title": "Learning Category-level Last-meter Navigation from RGB Demonstrations of a Single-instance", "comment": null, "summary": "Achieving precise positioning of the mobile manipulator's base is essential for successful manipulation actions that follow. Most of the RGB-based navigation systems only guarantee coarse, meter-level accuracy, making them less suitable for the precise positioning phase of mobile manipulation. This gap prevents manipulation policies from operating within the distribution of their training demonstrations, resulting in frequent execution failures. We address this gap by introducing an object-centric imitation learning framework for last-meter navigation, enabling a quadruped mobile manipulator robot to achieve manipulation-ready positioning using only RGB observations from its onboard cameras. Our method conditions the navigation policy on three inputs: goal images, multi-view RGB observations from the onboard cameras, and a text prompt specifying the target object. A language-driven segmentation module and a spatial score-matrix decoder then supply explicit object grounding and relative pose reasoning. Using real-world data from a single object instance within a category, the system generalizes to unseen object instances across diverse environments with challenging lighting and background conditions. To comprehensively evaluate this, we introduce two metrics: an edge-alignment metric, which uses ground truth orientation, and an object-alignment metric, which evaluates how well the robot visually faces the target. Under these metrics, our policy achieves 73.47% success in edge-alignment and 96.94% success in object-alignment when positioning relative to unseen target objects. These results show that precise last-meter navigation can be achieved at a category-level without depth, LiDAR, or map priors, enabling a scalable pathway toward unified mobile manipulation. Project page: https://rpm-lab-umn.github.io/category-level-last-meter-nav/", "AI": {"tldr": "该研究提出了一种基于RGB图像的对象中心模仿学习框架，使移动机械臂能够仅通过机载摄像头实现精确的最后一米导航，从而为后续的机械臂操作做好准备，并能泛化到未见过的同类别物体。", "motivation": "大多数基于RGB的导航系统只能提供米级的粗略精度，不足以满足移动机械臂操作所需的精确对准，这导致机械臂操作策略无法在其训练分布内有效执行，频繁出现故障。", "method": "该方法引入了一个对象中心模仿学习框架，用于最后一米导航。导航策略以目标图像、机载摄像头的多视角RGB观测以及指定目标对象的文本提示为条件。一个语言驱动的分割模块和一个空间分数矩阵解码器提供显式的对象定位和相对姿态推理。系统使用单一对象实例的真实世界数据进行训练，并泛化到不同环境和挑战性照明条件下的未见对象实例。", "result": "研究引入了边缘对齐和对象对齐两个新指标。在定位未见过的目标对象时，该策略在边缘对齐方面达到73.47%的成功率，在对象对齐方面达到96.94%的成功率。结果表明，无需深度信息、激光雷达或地图先验，即可实现类别级别的精确最后一米导航。", "conclusion": "该研究证明了仅使用RGB观测，可以在类别级别实现精确的最后一米导航，从而为统一的移动机械臂操作提供了一条可扩展的途径，无需依赖深度传感器、激光雷达或地图先验。"}}
{"id": "2512.11187", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11187", "abs": "https://arxiv.org/abs/2512.11187", "authors": ["Haohui Zhang", "Wouter van Heeswijk", "Xinyu Hu", "Neil Yorke-Smith", "Martijn Mes"], "title": "Deep Learning--Accelerated Multi-Start Large Neighborhood Search for Real-time Freight Bundling", "comment": null, "summary": "Online Freight Exchange Systems (OFEX) play a crucial role in modern freight logistics by facilitating real-time matching between shippers and carrier. However, efficient combinatorial bundling of transporation jobs remains a bottleneck. We model the OFEX combinatorial bundling problem as a multi-commodity one-to-one pickup-and-delivery selective traveling salesperson problem (m1-PDSTSP), which optimizes revenue-driven freight bundling under capacity, precedence, and route-length constraints. The key challenge is to couple combinatorial bundle selection with pickup-and-delivery routing under sub-second latency. We propose a learning--accelerated hybrid search pipeline that pairs a Transformer Neural Network-based constructive policy with an innovative Multi-Start Large Neighborhood Search (MSLNS) metaheuristic within a rolling-horizon scheme in which the platform repeatedly freezes the current marketplace into a static snapshot and solves it under a short time budget. This pairing leverages the low-latency, high-quality inference of the learning-based constructor alongside the robustness of improvement search; the multi-start design and plausible seeds help LNS to explore the solution space more efficiently. Across benchmarks, our method outperforms state-of-the-art neural combinatorial optimization and metaheuristic baselines in solution quality with comparable time, achieving an optimality gap of less than 2\\% in total revenue relative to the best available exact baseline method. To our knowledge, this is the first work to establish that a Deep Neural Network-based constructor can reliably provide high-quality seeds for (multi-start) improvement heuristics, with applicability beyond the \\textit{m1-PDSTSP} to a broad class of selective traveling salesperson problems and pickup and delivery problems.", "AI": {"tldr": "本研究针对在线货运交易系统中的组合捆绑问题，提出了一个学习加速的混合搜索管道，结合Transformer神经网络和多起点大邻域搜索，以在亚秒级延迟下高效优化货运捆绑，并取得了超越现有技术水平的性能。", "motivation": "在线货运交易系统在现代货运物流中至关重要，但高效的运输作业组合捆绑仍然是一个瓶颈。现有方法难以在亚秒级延迟内有效解决组合捆绑和取送货路径规划的耦合问题。", "method": "将在线货运交易系统组合捆绑问题建模为多商品一对一取送货选择性旅行商问题（m1-PDSTSP）。提出了一种学习加速的混合搜索管道，该管道在一个滚动视窗方案中运行，将基于Transformer神经网络的构造策略与创新的多起点大邻域搜索（MSLNS）元启发式算法相结合。Transformer网络提供低延迟、高质量的初始解（种子），MSLNS则在此基础上进行鲁棒的改进搜索。", "result": "在基准测试中，该方法在解决方案质量方面优于最先进的神经组合优化和元启发式基线，同时保持了可比的运行时间。相对于现有最佳精确基线方法，总收入的最优性差距小于2%。", "conclusion": "本研究首次证明了基于深度神经网络的构造器能够可靠地为（多起点）改进启发式算法提供高质量的初始解，其适用性超越了m1-PDSTSP，可推广到广泛的选择性旅行商问题和取送货问题。"}}
{"id": "2512.11213", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.11213", "abs": "https://arxiv.org/abs/2512.11213", "authors": ["Dongwon Jung", "Peng Shi", "Yi Zhang"], "title": "FutureWeaver: Planning Test-Time Compute for Multi-Agent Systems with Modularized Collaboration", "comment": null, "summary": "Scaling test-time computation improves large language model performance without additional training. Recent work demonstrates that techniques such as repeated sampling, self-verification, and self-reflection can significantly enhance task success by allocating more inference-time compute. However, applying these techniques across multiple agents in a multi-agent system is difficult: there does not exist principled mechanisms to allocate compute to foster collaboration among agents, to extend test-time scaling to collaborative interactions, or to distribute compute across agents under explicit budget constraints. To address this gap, we propose FutureWeaver, a framework for planning and optimizing test-time compute allocation in multi-agent systems under fixed budgets. FutureWeaver introduces modularized collaboration, formalized as callable functions that encapsulate reusable multi-agent workflows. These modules are automatically derived through self-play reflection by abstracting recurring interaction patterns from past trajectories. Building on these modules, FutureWeaver employs a dual-level planning architecture that optimizes compute allocation by reasoning over the current task state while also speculating on future steps. Experiments on complex agent benchmarks demonstrate that FutureWeaver consistently outperforms baselines across diverse budget settings, validating its effectiveness for multi-agent collaboration in inference-time optimization.", "AI": {"tldr": "FutureWeaver是一个框架，用于在固定预算下规划和优化多智能体系统中的测试时间计算分配，通过模块化协作和双层规划显著提高协作性能。", "motivation": "现有技术如重复采样、自验证和自反思虽能通过增加推理计算提升大型语言模型性能，但难以应用于多智能体系统。目前缺乏原则性的机制来分配计算以促进智能体间的协作，将测试时间扩展应用于协作交互，或在明确的预算约束下在智能体之间分配计算。", "method": "FutureWeaver框架提出模块化协作，将其形式化为可调用函数，封装可重用的多智能体工作流。这些模块通过自博弈反思，从过去的轨迹中抽象出重复的交互模式自动生成。在此基础上，FutureWeaver采用双层规划架构，通过推理当前任务状态并推测未来步骤来优化计算分配。", "result": "在复杂智能体基准测试中，FutureWeaver在不同的预算设置下始终优于基线，验证了其在多智能体协作推理时间优化方面的有效性。", "conclusion": "FutureWeaver框架通过其模块化协作和双层规划方法，有效解决了多智能体系统中测试时间计算分配和优化的问题，显著提升了协作性能。"}}
{"id": "2512.11169", "categories": ["cs.AI", "cs.LG", "eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.11169", "abs": "https://arxiv.org/abs/2512.11169", "authors": ["Akhil S Anand", "Elias Aarekol", "Martin Mziray Dalseg", "Magnus Stalhane", "Sebastien Gros"], "title": "CORL: Reinforcement Learning of MILP Policies Solved via Branch and Bound", "comment": null, "summary": "Combinatorial sequential decision making problems are typically modeled as mixed integer linear programs (MILPs) and solved via branch and bound (B&B) algorithms. The inherent difficulty of modeling MILPs that accurately represent stochastic real world problems leads to suboptimal performance in the real world. Recently, machine learning methods have been applied to build MILP models for decision quality rather than how accurately they model the real world problem. However, these approaches typically rely on supervised learning, assume access to true optimal decisions, and use surrogates for the MILP gradients. In this work, we introduce a proof of concept CORL framework that end to end fine tunes an MILP scheme using reinforcement learning (RL) on real world data to maximize its operational performance. We enable this by casting an MILP solved by B&B as a differentiable stochastic policy compatible with RL. We validate the CORL method in a simple illustrative combinatorial sequential decision making example.", "AI": {"tldr": "本文提出CORL框架，通过强化学习端到端地微调混合整数线性规划（MILP）方案，以最大化其在真实世界数据上的操作性能，其核心是将分支定界（B&B）求解的MILP建模为可微分的随机策略。", "motivation": "组合序列决策问题通常建模为MILP并通过B&B算法求解，但其难以准确表示随机真实世界问题，导致实际性能不佳。现有的机器学习方法构建MILP模型时，常依赖监督学习、假设可获取真实最优决策，并使用MILP梯度的替代品。", "method": "引入了一个名为CORL的概念验证框架。该框架使用强化学习在真实世界数据上端到端地微调MILP方案，以最大化其操作性能。实现方式是将通过B&B求解的MILP建模为一个与强化学习兼容的可微分随机策略。", "result": "CORL方法在一个简单的组合序列决策示例中得到了验证，证明了其概念的可行性。", "conclusion": "CORL框架通过将B&B求解的MILP转化为可微分的随机策略，展示了使用强化学习端到端微调MILP方案以提升其在实际操作中性能的潜力。"}}
{"id": "2512.11076", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.11076", "abs": "https://arxiv.org/abs/2512.11076", "authors": ["Jack Brady", "Andrew Dailey", "Kristen Schang", "Zo Vic Shong"], "title": "E-CHUM: Event-based Cameras for Human Detection and Urban Monitoring", "comment": null, "summary": "Understanding human movement and city dynamics has always been challenging. From traditional methods of manually observing the city's inhabitant, to using cameras, to now using sensors and more complex technology, the field of urban monitoring has evolved greatly. Still, there are more that can be done to unlock better practices for understanding city dynamics. This paper surveys how the landscape of urban dynamics studying has evolved with a particular focus on event-based cameras. Event-based cameras capture changes in light intensity instead of the RGB values that traditional cameras do. They offer unique abilities, like the ability to work in low-light, that can make them advantageous compared to other sensors. Through an analysis of event-based cameras, their applications, their advantages and challenges, and machine learning applications, we propose event-based cameras as a medium for capturing information to study urban dynamics. They offer the ability to capture important information while maintaining privacy. We also suggest multi-sensor fusion of event-based cameras and other sensors in the study of urban dynamics. Combining event-based cameras and infrared, event-LiDAR, or vibration has to potential to enhance the ability of event-based cameras and overcome the challenges that event-based cameras have.", "AI": {"tldr": "本文综述了事件相机在城市动态研究中的应用前景，并提出将其作为一种新的信息捕获媒介，结合多传感器融合技术，以更好地理解城市运动和隐私保护。", "motivation": "理解人类活动和城市动态一直充满挑战，传统方法和现有技术仍有局限。事件相机具有低光照工作等独特优势，有望克服现有传感器的不足，提供一种新的研究城市动态的方法。", "method": "本文通过对城市动态研究发展历程的调查，重点分析了事件相机的工作原理、应用、优缺点以及机器学习应用。在此基础上，提出了事件相机在城市动态研究中的潜力。", "result": "研究提出将事件相机作为捕获城市动态信息的有效媒介，其能够在捕获重要信息的同时保护隐私。此外，还建议将事件相机与其他传感器（如红外、事件LiDAR或振动传感器）进行多传感器融合，以增强其能力并克服单一事件相机的挑战。", "conclusion": "事件相机具有独特的能力，结合多传感器融合技术，有望显著提升城市动态研究的水平，克服现有挑战，并在数据捕获和隐私保护之间取得平衡。"}}
{"id": "2512.10968", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2512.10968", "abs": "https://arxiv.org/abs/2512.10968", "authors": ["Alvin Nahabwe", "Sulaiman Kagumire", "Denis Musinguzi", "Bruno Beijuka", "Jonah Mubuuke Kyagaba", "Peter Nabende", "Andrew Katumba", "Joyce Nakatumba-Nabende"], "title": "Benchmarking Automatic Speech Recognition Models for African Languages", "comment": "19 pages, 8 figures, Deep Learning Indiba, Proceedings of Machine Learning Research", "summary": "Automatic speech recognition (ASR) for African languages remains constrained by limited labeled data and the lack of systematic guidance on model selection, data scaling, and decoding strategies. Large pre-trained systems such as Whisper, XLS-R, MMS, and W2v-BERT have expanded access to ASR technology, but their comparative behavior in African low-resource contexts has not been studied in a unified and systematic way. In this work, we benchmark four state-of-the-art ASR models across 13 African languages, fine-tuning them on progressively larger subsets of transcribed data ranging from 1 to 400 hours. Beyond reporting error rates, we provide new insights into why models behave differently under varying conditions. We show that MMS and W2v-BERT are more data efficient in very low-resource regimes, XLS-R scales more effectively as additional data becomes available, and Whisper demonstrates advantages in mid-resource conditions. We also analyze where external language model decoding yields improvements and identify cases where it plateaus or introduces additional errors, depending on the alignment between acoustic and text resources. By highlighting the interaction between pre-training coverage, model architecture, dataset domain, and resource availability, this study offers practical and insights into the design of ASR systems for underrepresented languages.", "AI": {"tldr": "本研究系统性地基准测试了四种最先进的ASR模型在13种非洲语言中的表现，探究了在不同数据量和解码策略下模型行为差异的原因，为低资源语言的ASR系统设计提供了实用见解。", "motivation": "非洲语言的自动语音识别（ASR）受限于标注数据稀缺以及模型选择、数据扩展和解码策略缺乏系统性指导。虽然大型预训练系统（如Whisper、XLS-R、MMS、W2v-BERT）提高了ASR技术的可及性，但它们在非洲低资源语境中的比较行为尚未得到统一和系统的研究。", "method": "本研究在13种非洲语言上对四种最先进的ASR模型（Whisper、XLS-R、MMS和W2v-BERT）进行了基准测试。模型在1到400小时不同规模的转录数据子集上进行微调。除了报告错误率外，还深入分析了模型在不同条件下表现差异的原因。同时，分析了外部语言模型解码何时能带来改进，以及何时会停滞或引入额外错误。", "result": "MMS和W2v-BERT在极低资源环境下数据效率更高；XLS-R随着数据量的增加扩展性更强；Whisper在中等资源条件下表现出优势。外部语言模型解码的改进取决于声学和文本资源之间的一致性，有时会停滞或引入额外错误。", "conclusion": "本研究通过强调预训练覆盖范围、模型架构、数据集领域和资源可用性之间的相互作用，为欠代表语言的ASR系统设计提供了实用的见解。"}}
{"id": "2512.10999", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.10999", "abs": "https://arxiv.org/abs/2512.10999", "authors": ["Xin Sun", "Zhongqi Chen", "Xing Zheng", "Qiang Liu", "Shu Wu", "Bowen Song", "Zilei Wang", "Weiqiang Wang", "Liang Wang"], "title": "KBQA-R1: Reinforcing Large Language Models for Knowledge Base Question Answering", "comment": null, "summary": "Knowledge Base Question Answering (KBQA) challenges models to bridge the gap between natural language and strict knowledge graph schemas by generating executable logical forms. While Large Language Models (LLMs) have advanced this field, current approaches often struggle with a dichotomy of failure: they either generate hallucinated queries without verifying schema existence or exhibit rigid, template-based reasoning that mimics synthesized traces without true comprehension of the environment. To address these limitations, we present \\textbf{KBQA-R1}, a framework that shifts the paradigm from text imitation to interaction optimization via Reinforcement Learning. Treating KBQA as a multi-turn decision process, our model learns to navigate the knowledge base using a list of actions, leveraging Group Relative Policy Optimization (GRPO) to refine its strategies based on concrete execution feedback rather than static supervision. Furthermore, we introduce \\textbf{Referenced Rejection Sampling (RRS)}, a data synthesis method that resolves cold-start challenges by strictly aligning reasoning traces with ground-truth action sequences. Extensive experiments on WebQSP, GrailQA, and GraphQuestions demonstrate that KBQA-R1 achieves state-of-the-art performance, effectively grounding LLM reasoning in verifiable execution.", "AI": {"tldr": "本文提出了KBQA-R1框架，通过强化学习将知识库问答（KBQA）从文本模仿转变为交互优化，并引入了Referenced Rejection Sampling (RRS)数据合成方法，在多个数据集上实现了最先进的性能。", "motivation": "当前基于大型语言模型（LLM）的KBQA方法存在两大问题：一是生成幻觉查询，未能验证模式存在性；二是推理僵化，过度依赖模板而缺乏对环境的真实理解。这些局限性促使研究者寻求更有效的方法。", "method": "KBQA-R1将KBQA视为多轮决策过程，通过强化学习（RL）进行交互优化。它利用Group Relative Policy Optimization (GRPO)基于具体的执行反馈而非静态监督来优化策略。此外，为解决冷启动问题，本文引入了Referenced Rejection Sampling (RRS)数据合成方法，严格将推理轨迹与真实动作序列对齐。", "result": "在WebQSP、GrailQA和GraphQuestions等数据集上的广泛实验表明，KBQA-R1达到了最先进的性能，并有效地将LLM的推理建立在可验证的执行基础之上。", "conclusion": "KBQA-R1通过将范式从文本模仿转向交互优化，并结合强化学习和创新的数据合成方法，成功克服了现有LLM-based KBQA的局限性，实现了卓越的性能，并确保了LLM推理的可验证性。"}}
{"id": "2512.11270", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11270", "abs": "https://arxiv.org/abs/2512.11270", "authors": ["Hong Je-Gal", "Chan-Bin Yi", "Hyun-Suk Lee"], "title": "A-LAMP: Agentic LLM-Based Framework for Automated MDP Modeling and Policy Generation", "comment": "NeurIPS 2025 Workshop: Multi-Turn Interactions in Large Language Models. 26 pages, 8 figures", "summary": "Applying reinforcement learning (RL) to real-world tasks requires converting informal descriptions into a formal Markov decision process (MDP), implementing an executable environment, and training a policy agent. Automating this process is challenging due to modeling errors, fragile code, and misaligned objectives, which often impede policy training. We introduce an agentic large language model (LLM)-based framework for automated MDP modeling and policy generation (A-LAMP), that automatically translates free-form natural language task descriptions into an MDP formulation and trained policy. The framework decomposes modeling, coding, and training into verifiable stages, ensuring semantic alignment throughout the pipeline. Across both classic control and custom RL domains, A-LAMP consistently achieves higher policy generation capability than a single state-of-the-art LLM model. Notably, even its lightweight variant, which is built on smaller language models, approaches the performance of much larger models. Failure analysis reveals why these improvements occur. In addition, a case study also demonstrates that A-LAMP generates environments and policies that preserve the task's optimality, confirming its correctness and reliability.", "AI": {"tldr": "本文提出了一个基于代理大型语言模型（LLM）的框架A-LAMP，能自动将自由形式的自然语言任务描述转化为马尔可夫决策过程（MDP）和训练好的策略，解决了真实世界强化学习（RL）应用中的建模和训练挑战。", "motivation": "将强化学习应用于现实世界任务面临挑战：需要将非正式描述转化为形式化的MDP，实现可执行环境，并训练策略代理。此过程常因建模错误、脆弱代码和目标不一致而受阻，影响策略训练。", "method": "A-LAMP是一个基于代理LLM的框架，用于自动化MDP建模和策略生成。它将建模、编码和训练分解为可验证的阶段，确保整个流程中的语义对齐。", "result": "在经典控制和自定义RL领域，A-LAMP始终比单一的最先进LLM模型展现出更高的策略生成能力。值得注意的是，其基于小型语言模型的轻量级变体也能接近大型模型的性能。失败分析揭示了这些改进的原因。案例研究还证明A-LAMP生成的环境和策略能保持任务的最优性。", "conclusion": "A-LAMP框架能够可靠地自动化MDP建模和策略生成，即使是其轻量级版本也表现出色，证明了其在解决真实世界RL应用挑战方面的有效性和鲁棒性。"}}
{"id": "2512.11493", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.11493", "abs": "https://arxiv.org/abs/2512.11493", "authors": ["Vandana Narri", "Jonah J. Glunt", "Joshua A. Robbins", "Jonas Mårtensson", "Herschel C. Pangborn", "Karl H. Johansson"], "title": "Shared Situational Awareness Using Hybrid Zonotopes with Confidence Metric", "comment": null, "summary": "Situational awareness for connected and automated vehicles describes the ability to perceive and predict the behavior of other road-users in the near surroundings. However, pedestrians can become occluded by vehicles or infrastructure, creating significant safety risks due to limited visibility. Vehicle-to-everything communication enables the sharing of perception data between connected road-users, allowing for a more comprehensive awareness. The main challenge is how to fuse perception data when measurements are inconsistent with the true locations of pedestrians. Inconsistent measurements can occur due to sensor noise, false positives, or communication issues. This paper employs set-based estimation with constrained zonotopes to compute a confidence metric for the measurement set from each sensor. These sets and their confidences are then fused using hybrid zonotopes. This method can account for inconsistent measurements, enabling reliable and robust fusion of the sensor data. The effectiveness of the proposed method is demonstrated in both simulation and real experiments.", "AI": {"tldr": "本文提出一种基于混合多面体的集合估计方法，用于融合来自V2X通信的感知数据，以提高互联自动驾驶汽车（CAV）的态势感知能力，尤其是在测量数据不一致的情况下。", "motivation": "互联自动驾驶汽车需要感知并预测其他道路使用者的行为，但行人可能被遮挡，导致安全风险。V2X通信可以共享感知数据以提供更全面的感知，但传感器噪声、误报或通信问题可能导致测量数据不一致，这是融合感知数据的主要挑战。", "method": "该方法利用受限多面体（constrained zonotopes）进行基于集合的估计，为每个传感器的测量集合计算置信度指标。然后，使用混合多面体（hybrid zonotopes）融合这些集合及其置信度。", "result": "所提出的方法能够处理不一致的测量数据，实现了可靠且鲁棒的传感器数据融合。该方法的有效性已在仿真和实际实验中得到验证。", "conclusion": "该研究提供了一种有效融合传感器数据的方法，即使在测量不一致的情况下也能提高CAV的态势感知能力，从而增强道路安全性。"}}
{"id": "2512.11060", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11060", "abs": "https://arxiv.org/abs/2512.11060", "authors": ["Chenjun Li", "Cheng Wan", "Laurin Lux", "Alexander Berger", "Richard B. Rosen", "Martin J. Menten", "Johannes C. Paetzold"], "title": "Synthetic Vasculature and Pathology Enhance Vision-Language Model Reasoning", "comment": "23 pages, 8 figures, 6 tables. Full paper under review for MIDL 2026 (Medical Imaging with Deep Learning)", "summary": "Vision-Language Models (VLMs) offer a promising path toward interpretable medical diagnosis by allowing users to ask about clinical explanations alongside predictions and across different modalities. However, training VLMs for detailed reasoning requires large-scale image-text datasets. In many specialized domains, for example in reading Optical Coherence Tomography Angiography (OCTA) images, such precise text with grounded description of pathologies is scarce or even non-existent. To overcome this bottleneck, we introduce Synthetic Vasculature Reasoning (SVR), a framework that controllably synthesizes images and corresponding text, specifically: realistic retinal vasculature with Diabetic Retinopathy (DR) features: capillary dropout, microaneurysms, neovascularization, and tortuosity, while automatically generating granular reasoning texts. Based on this we curate OCTA-100K-SVR, an OCTA image-reasoning dataset with 100,000 pairs. Our experiments show that a general-purpose VLM (Qwen3-VL-8b) trained on the dataset achieves a zero-shot balanced classification accuracy of 89.67% on real OCTA images, outperforming supervised baselines. Through human expert evaluation we also demonstrate that it significantly enhances explanation quality and pathology localization on clinical data.", "AI": {"tldr": "针对专业医学领域（如OCTA）缺乏详细图像-文本数据的问题，本研究引入了合成血管推理（SVR）框架，可控地生成包含糖尿病视网膜病变（DR）特征的OCTA图像及其详细推理文本，并构建了OCTA-100K-SVR数据集。在此数据集上训练的VLM在真实OCTA图像上实现了高零样本准确率，并显著提升了解释质量和病理定位能力。", "motivation": "视觉-语言模型（VLMs）在可解释医学诊断方面具有巨大潜力，但其详细推理能力需要大规模图像-文本数据集。在许多专业领域，如光学相干断层扫描血管造影（OCTA）图像判读中，缺乏带有病理学精确描述的文本数据。", "method": "引入了合成血管推理（SVR）框架，该框架能够可控地合成具有糖尿病视网膜病变（DR）特征（如毛细血管闭塞、微动脉瘤、新生血管、血管迂曲）的真实视网膜血管图像，并自动生成细粒度的推理文本。基于此，构建了包含100,000对OCTA图像-推理文本的OCTA-100K-SVR数据集。使用Qwen3-VL-8b通用VLM在此数据集上进行训练。", "result": "在真实OCTA图像上，经过训练的VLM实现了89.67%的零样本平衡分类准确率，优于有监督基线模型。通过人类专家评估，该模型显著提高了临床数据上的解释质量和病理定位能力。", "conclusion": "SVR框架及其生成的OCTA-100K-SVR数据集有效解决了专业医学领域VLM训练数据稀缺的瓶颈。基于合成数据训练的VLM在真实OCTA图像上展现出强大的零样本性能、改进的解释质量和病理定位能力，证明了合成数据在可解释医学诊断中的价值。"}}
{"id": "2512.11013", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.11013", "abs": "https://arxiv.org/abs/2512.11013", "authors": ["Pawel Batorski", "Paul Swoboda"], "title": "PIAST: Rapid Prompting with In-context Augmentation for Scarce Training data", "comment": null, "summary": "LLMs are highly sensitive to prompt design, but handcrafting effective prompts is difficult and often requires intricate crafting of few-shot examples. We propose a fast automatic prompt construction algorithm that augments human instructions by generating a small set of few shot examples. Our method iteratively replaces/drops/keeps few-shot examples using Monte Carlo Shapley estimation of example utility. For faster execution, we use aggressive subsampling and a replay buffer for faster evaluations. Our method can be run using different compute time budgets. On a limited budget, we outperform existing automatic prompting methods on text simplification and GSM8K and obtain second best results on classification and summarization. With an extended, but still modest compute budget we set a new state of the art among automatic prompting methods on classification, simplification and GSM8K. Our results show that carefully constructed examples, rather than exhaustive instruction search, are the dominant lever for fast and data efficient prompt engineering. Our code is available at https://github.com/Batorskq/PIAST.", "AI": {"tldr": "本文提出了一种名为PIAST的快速自动提示构建算法，通过生成少量上下文示例来增强人类指令。该方法利用蒙特卡洛Shapley估计迭代优化示例效用，并在适度计算预算下在多个任务上取得了最先进或领先的结果。", "motivation": "大型语言模型（LLMs）对提示设计高度敏感，但手工制作有效的提示（尤其是包含少量示例的提示）非常困难且耗时。", "method": "本文提出了一种名为PIAST的快速自动提示构建算法。该方法通过生成少量上下文示例来增强人类指令。它利用蒙特卡洛Shapley估计的示例效用，迭代地替换/删除/保留这些示例。为了加快执行速度，该方法采用了激进的二次抽样和重放缓冲区，并且可以在不同的计算时间预算下运行。", "result": "在有限预算下，该方法在文本简化和GSM8K任务上优于现有自动提示方法，并在分类和摘要任务上取得次优结果。在扩展但仍适度的计算预算下，该方法在分类、简化和GSM8K任务上刷新了自动提示方法的最新技术水平。研究结果表明，精心构建的示例而非穷尽的指令搜索，是实现快速且数据高效提示工程的主要驱动力。", "conclusion": "精心构建的少量示例是快速、数据高效提示工程的关键因素。本文提出的自动提示构建算法（PIAST）能够以适度的计算资源实现最先进的性能，证明了示例构造的重要性。"}}
{"id": "2512.11121", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.11121", "abs": "https://arxiv.org/abs/2512.11121", "authors": ["Yuyang Hu", "Mojtaba Sahraee-Ardakan", "Arpit Bansal", "Kangfu Mei", "Christian Qi", "Peyman Milanfar", "Mauricio Delbracio"], "title": "Learning from a Generative Oracle: Domain Adaptation for Restoration", "comment": null, "summary": "Pre-trained image restoration models often fail on real-world, out-of-distribution degradations due to significant domain gaps. Adapting to these unseen domains is challenging, as out-of-distribution data lacks ground truth, and traditional adaptation methods often require complex architectural changes. We propose LEGO (Learning from a Generative Oracle), a practical three-stage framework for post-training domain adaptation without paired data. LEGO converts this unsupervised challenge into a tractable pseudo-supervised one. First, we obtain initial restorations from the pre-trained model. Second, we leverage a frozen, large-scale generative oracle to refine these estimates into high-quality pseudo-ground-truths. Third, we fine-tune the original model using a mixed-supervision strategy combining in-distribution data with these new pseudo-pairs. This approach adapts the model to the new distribution without sacrificing its original robustness or requiring architectural modifications. Experiments demonstrate that LEGO effectively bridges the domain gap, significantly improving performance on diverse real-world benchmarks.", "AI": {"tldr": "LEGO是一种三阶段框架，通过利用大型生成式预言机生成伪真实标签，将无监督域适应问题转化为可处理的伪监督问题，从而在不修改模型架构的情况下，帮助预训练图像修复模型适应真实世界的分布外退化。", "motivation": "预训练图像修复模型在面对真实世界中分布外（out-of-distribution）的退化时表现不佳，因为存在显著的域间隙。适应这些未见过的域具有挑战性，因为缺乏真实标签，且传统适应方法常需要复杂的架构修改。", "method": "该方法提出LEGO（Learning from a Generative Oracle），一个实用的三阶段后训练域适应框架，无需配对数据：1. 从预训练模型获得初始修复结果。2. 利用一个冻结的大规模生成式预言机将这些估计细化为高质量的伪真实标签。3. 使用混合监督策略（结合域内数据和新的伪配对）微调原始模型。", "result": "实验证明，LEGO有效弥合了域间隙，显著提高了模型在各种真实世界基准上的性能。", "conclusion": "LEGO方法在不牺牲模型原有鲁棒性或无需架构修改的情况下，成功使模型适应新的数据分布，将无监督挑战转化为可处理的伪监督挑战。"}}
{"id": "2512.11271", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11271", "abs": "https://arxiv.org/abs/2512.11271", "authors": ["Yuxing Chen", "Basem Suleiman", "Qifan Chen"], "title": "TriFlow: A Progressive Multi-Agent Framework for Intelligent Trip Planning", "comment": "4 pages, 3 figures", "summary": "Real-world trip planning requires transforming open-ended user requests into executable itineraries under strict spatial, temporal, and budgetary constraints while aligning with user preferences. Existing LLM-based agents struggle with constraint satisfaction, tool coordination, and efficiency, often producing infeasible or costly plans. To address these limitations, we present TriFlow, a progressive multi-agent framework that unifies structured reasoning and language-based flexibility through a three-stage pipeline of retrieval, planning, and governance. By this design, TriFlow progressively narrows the search space, assembles constraint-consistent itineraries via rule-LLM collaboration, and performs bounded iterative refinement to ensure global feasibility and personalisation. Evaluations on TravelPlanner and TripTailor benchmarks demonstrated state-of-the-art results, achieving 91.1% and 97% final pass rates, respectively, with over 10x runtime efficiency improvement compared to current SOTA.", "AI": {"tldr": "TriFlow是一个渐进式多智能体框架，通过检索、规划和治理三阶段流程，结合结构化推理和语言灵活性，解决了现有大型语言模型（LLM）在旅行规划中约束满足、工具协调和效率低下的问题，实现了最先进的性能和显著的运行效率提升。", "motivation": "现实世界旅行规划需要将开放式用户请求转化为可执行行程，同时满足严格的空间、时间、预算约束和用户偏好。现有基于LLM的智能体在约束满足、工具协调和效率方面表现不佳，常产生不可行或成本过高的计划。", "method": "TriFlow提出一个渐进式多智能体框架，通过检索、规划和治理三个阶段的流程，统一了结构化推理和基于语言的灵活性。该框架逐步缩小搜索空间，通过规则-LLM协作组装符合约束的行程，并执行有界迭代优化以确保全局可行性和个性化。", "result": "在TravelPlanner和TripTailor基准测试中，TriFlow取得了最先进的结果，最终通过率分别达到91.1%和97%，并且运行效率比当前最先进方法提高了10倍以上。", "conclusion": "TriFlow通过其渐进式多智能体设计，有效克服了现有LLM在旅行规划中的局限性，显著提高了行程规划的约束满足度、可行性和个性化水平，同时大幅提升了运行效率，为真实世界旅行规划提供了强大且高效的解决方案。"}}
{"id": "2512.11218", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11218", "abs": "https://arxiv.org/abs/2512.11218", "authors": ["Kechun Xu", "Zhenjie Zhu", "Anzhe Chen", "Shuqi Zhao", "Qing Huang", "Yifei Yang", "Haojian Lu", "Rong Xiong", "Masayoshi Tomizuka", "Yue Wang"], "title": "Seeing to Act, Prompting to Specify: A Bayesian Factorization of Vision Language Action Policy", "comment": null, "summary": "The pursuit of out-of-distribution generalization in Vision-Language-Action (VLA) models is often hindered by catastrophic forgetting of the Vision-Language Model (VLM) backbone during fine-tuning. While co-training with external reasoning data helps, it requires experienced tuning and data-related overhead. Beyond such external dependencies, we identify an intrinsic cause within VLA datasets: modality imbalance, where language diversity is much lower than visual and action diversity. This imbalance biases the model toward visual shortcuts and language forgetting. To address this, we introduce BayesVLA, a Bayesian factorization that decomposes the policy into a visual-action prior, supporting seeing-to-act, and a language-conditioned likelihood, enabling prompt-to-specify. This inherently preserves generalization and promotes instruction following. We further incorporate pre- and post-contact phases to better leverage pre-trained foundation models. Information-theoretic analysis formally validates our effectiveness in mitigating shortcut learning. Extensive experiments show superior generalization to unseen instructions, objects, and environments compared to existing methods. Project page is available at: https://xukechun.github.io/papers/BayesVLA.", "AI": {"tldr": "本文提出BayesVLA，通过贝叶斯分解和引入接触阶段，解决视觉-语言-动作（VLA）模型在微调过程中灾难性遗忘和模态不平衡问题，从而显著提升泛化能力和指令遵循性。", "motivation": "VLA模型在微调时，视觉-语言模型（VLM）骨干容易发生灾难性遗忘，即使共同训练也需大量调优。主要原因在于VLA数据集中存在模态不平衡，即语言多样性远低于视觉和动作多样性，导致模型偏向视觉捷径而忽视语言信息。", "method": "引入BayesVLA，采用贝叶斯分解将策略分解为：1. 视觉-动作先验（支持“看即行动”）；2. 语言条件似然（支持“提示即规范”）。这种设计旨在固有地保持泛化能力并促进指令遵循。此外，还融入了接触前和接触后阶段，以更好地利用预训练基础模型。通过信息论分析形式化验证了其缓解捷径学习的有效性。", "result": "实验结果表明，BayesVLA在面对未见指令、物体和环境时，泛化能力优于现有方法。信息论分析也正式验证了其在减轻捷径学习方面的有效性。", "conclusion": "BayesVLA通过其独特的贝叶斯分解架构和阶段集成，成功解决了VLA模型中的灾难性遗忘和模态不平衡问题，显著提高了模型对新情境的泛化能力和指令遵循能力。"}}
{"id": "2512.11061", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11061", "abs": "https://arxiv.org/abs/2512.11061", "authors": ["Felix O'Mahony", "Roberto Cipolla", "Ayush Tewari"], "title": "VDAWorld: World Modelling via VLM-Directed Abstraction and Simulation", "comment": "Website: https://felixomahony.github.io/vdaworld/", "summary": "Generative video models, a leading approach to world modeling, face fundamental limitations. They often violate physical and logical rules, lack interactivity, and operate as opaque black boxes ill-suited for building structured, queryable worlds. To overcome these challenges, we propose a new paradigm focused on distilling an image caption pair into a tractable, abstract representation optimized for simulation. We introduce VDAWorld, a framework where a Vision-Language Model (VLM) acts as an intelligent agent to orchestrate this process. The VLM autonomously constructs a grounded (2D or 3D) scene representation by selecting from a suite of vision tools, and accordingly chooses a compatible physics simulator (e.g., rigid body, fluid) to act upon it. VDAWorld can then infer latent dynamics from the static scene to predict plausible future states. Our experiments show that this combination of intelligent abstraction and adaptive simulation results in a versatile world model capable of producing high quality simulations across a wide range of dynamic scenarios.", "AI": {"tldr": "为克服生成视频模型在世界建模中的局限性，本文提出了VDAWorld框架。该框架通过视觉-语言模型（VLM）将图像-字幕对提炼为可处理的抽象表示，并利用VLM智能构建场景并选择合适的物理模拟器，从而实现高质量的动态场景模拟。", "motivation": "现有的生成视频模型作为世界建模方法，存在违反物理和逻辑规则、缺乏交互性以及作为不透明黑盒难以构建结构化、可查询世界等基本局限性。", "method": "VDAWorld框架将图像-字幕对提炼为用于模拟的抽象表示。其中，VLM作为智能代理，自主选择视觉工具构建具象的（2D或3D）场景表示，并相应选择兼容的物理模拟器（如刚体、流体）进行作用。VDAWorld随后从静态场景推断潜在动态以预测未来状态。", "result": "智能抽象和自适应模拟的结合，使得VDAWorld能够生成跨越广泛动态场景的高质量模拟，证明其作为通用世界模型的有效性。", "conclusion": "VDAWorld通过智能抽象和自适应模拟的新范式，成功克服了传统生成视频模型的局限性，构建了一个多功能的世界模型，能够产生高质量的动态模拟。"}}
{"id": "2512.11531", "categories": ["eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.11531", "abs": "https://arxiv.org/abs/2512.11531", "authors": ["Luis Romero-Ben", "Bernat Joseph-Duran", "David Sunyer", "Gabriela Cembrano", "Jordi Meseguer", "Vicenç Puig", "Alejandro Carrasco"], "title": "Data-driven control-oriented modelling for MPC-based control of urban drainage systems", "comment": "This work has been submitted to IFAC WC 2026 for review. It has 7 pages and 2 figures", "summary": "This article presents a data-driven, control-oriented modelling methodology for urban drainage systems (UDS). The proposed framework requires three main key components: input-output data from the element to be modelled, expert knowledge to define the model structure, and data-fitting techniques to obtain optimal parameters. The methodology is evaluated using a realistic benchmark from an UDS in Madrid, Spain. The results show high model accuracy and improved performance within a MPC scheme, reducing discharge and increasing treatment facilities utilization.", "AI": {"tldr": "本文提出了一种数据驱动的、面向控制的城市排水系统（UDS）建模方法，并通过在MPC方案中的评估，展示了其高精度和改进的性能。", "motivation": "开发一种有效的、面向控制的城市排水系统建模方法，以优化系统管理，减少排放并提高处理设施的利用率。", "method": "该方法包含三个关键组成部分：待建模元素的输入-输出数据、用于定义模型结构的专家知识，以及用于获取最优参数的数据拟合技术。该方法通过西班牙马德里一个城市排水系统的真实基准进行了评估。", "result": "结果显示模型具有高精度，并在模型预测控制（MPC）方案中表现出改进的性能，从而减少了排放并增加了处理设施的利用率。", "conclusion": "所提出的数据驱动、面向控制的建模方法能有效应用于城市排水系统，显著提高模型精度和控制性能，优化系统操作。"}}
{"id": "2512.11612", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.11612", "abs": "https://arxiv.org/abs/2512.11612", "authors": ["Chunyi Li", "Rui Qing", "Jianbo Zhang", "Yuan Tian", "Xiangyang Zhu", "Zicheng Zhang", "Xiaohong Liu", "Weisi Lin", "Guangtao Zhai"], "title": "Embodied Image Compression", "comment": "15 pages, 12 figures, 3 tables", "summary": "Image Compression for Machines (ICM) has emerged as a pivotal research direction in the field of visual data compression. However, with the rapid evolution of machine intelligence, the target of compression has shifted from task-specific virtual models to Embodied agents operating in real-world environments. To address the communication constraints of Embodied AI in multi-agent systems and ensure real-time task execution, this paper introduces, for the first time, the scientific problem of Embodied Image Compression. We establish a standardized benchmark, EmbodiedComp, to facilitate systematic evaluation under ultra-low bitrate conditions in a closed-loop setting. Through extensive empirical studies in both simulated and real-world settings, we demonstrate that existing Vision-Language-Action models (VLAs) fail to reliably perform even simple manipulation tasks when compressed below the Embodied bitrate threshold. We anticipate that EmbodiedComp will catalyze the development of domain-specific compression tailored for Embodied agents , thereby accelerating the Embodied AI deployment in the Real-world.", "AI": {"tldr": "本文首次提出了具身图像压缩的科学问题，并建立了EmbodiedComp基准。研究发现，现有视觉-语言-动作模型在具身比特率阈值以下无法可靠执行任务，该基准有望推动具身AI在现实世界的部署。", "motivation": "随着机器学习智能的快速发展，图像压缩的目标已从特定任务的虚拟模型转向在真实世界环境中操作的具身智能体。为解决多智能体系统中具身AI的通信限制并确保实时任务执行，亟需研究具身图像压缩问题。", "method": "本文首次引入了具身图像压缩的科学问题。为此，建立了一个标准化基准EmbodiedComp，用于在闭环设置和超低比特率条件下进行系统评估。通过在模拟和真实世界环境中的大量实证研究来验证其有效性。", "result": "通过广泛的实证研究表明，当压缩率低于具身比特率阈值时，现有的视觉-语言-动作（VLA）模型甚至无法可靠地执行简单的操作任务。", "conclusion": "EmbodiedComp基准有望催化针对具身智能体的领域特定压缩技术的发展，从而加速具身AI在现实世界中的部署。"}}
{"id": "2512.11079", "categories": ["cs.CL", "cs.CY", "stat.AP", "stat.OT"], "pdf": "https://arxiv.org/pdf/2512.11079", "abs": "https://arxiv.org/abs/2512.11079", "authors": ["Alan Gerber", "Sam Cooperman"], "title": "Applying NLP to iMessages: Understanding Topic Avoidance, Responsiveness, and Sentiment", "comment": "11 pages, 18 figures, https://github.com/Alanshnir/imessage-analyzer/blob/main/Research/NLP-iMessage-Analyzer%20Findings.pdf", "summary": "What is your messaging data used for? While many users do not often think about the information companies can gather based off of their messaging platform of choice, it is nonetheless important to consider as society increasingly relies on short-form electronic communication. While most companies keep their data closely guarded, inaccessible to users or potential hackers, Apple has opened a door to their walled-garden ecosystem, providing iMessage users on Mac with one file storing all their messages and attached metadata. With knowledge of this locally stored file, the question now becomes: What can our data do for us? In the creation of our iMessage text message analyzer, we set out to answer five main research questions focusing on topic modeling, response times, reluctance scoring, and sentiment analysis. This paper uses our exploratory data to show how these questions can be answered using our analyzer and its potential in future studies on iMessage data.", "AI": {"tldr": "本文开发了一个iMessage文本消息分析器，利用Mac本地存储的iMessage数据，探索了主题建模、响应时间、犹豫度评分和情感分析等方面的用户洞察。", "motivation": "随着社会对短形式电子通信的日益依赖，用户很少关注消息平台的数据使用情况。苹果在Mac上提供了iMessage消息及其元数据的本地文件存储，这为分析这些数据以回答“我们的数据能为我们做什么？”的问题提供了机会。", "method": "研究者创建了一个iMessage文本消息分析器，并提出了五个主要研究问题，涉及主题建模、响应时间、犹豫度评分和情感分析。本文使用探索性数据来展示其分析器如何回答这些问题。", "result": "本文展示了所开发的分析器如何利用探索性数据来回答关于iMessage数据的五个研究问题（包括主题建模、响应时间、犹豫度评分和情感分析），并证明了其在未来研究中的潜力。", "conclusion": "该iMessage文本消息分析器能够从本地存储的iMessage数据中提取有价值的洞察，为未来的iMessage数据研究提供了有潜力的工具和方法。"}}
{"id": "2512.11074", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2512.11074", "abs": "https://arxiv.org/abs/2512.11074", "authors": ["Christopher Driggers-Ellis", "Detravious Brinkley", "Ray Chen", "Aashish Dhawan", "Daisy Zhe Wang", "Christan Grant"], "title": "MultiScript30k: Leveraging Multilingual Embeddings to Extend Cross Script Parallel Data", "comment": "7 pages, 2 figures, 5 tables. Not published at any conference at this time", "summary": "Multi30k is frequently cited in the multimodal machine translation (MMT) literature, offering parallel text data for training and fine-tuning deep learning models. However, it is limited to four languages: Czech, English, French, and German. This restriction has led many researchers to focus their investigations only on these languages. As a result, MMT research on diverse languages has been stalled because the official Multi30k dataset only represents European languages in Latin scripts. Previous efforts to extend Multi30k exist, but the list of supported languages, represented language families, and scripts is still very short. To address these issues, we propose MultiScript30k, a new Multi30k dataset extension for global languages in various scripts, created by translating the English version of Multi30k (Multi30k-En) using NLLB200-3.3B. The dataset consists of over \\(30000\\) sentences and provides translations of all sentences in Multi30k-En into Ar, Es, Uk, Zh\\_Hans and Zh\\_Hant. Similarity analysis shows that Multi30k extension consistently achieves greater than \\(0.8\\) cosine similarity and symmetric KL divergence less than \\(0.000251\\) for all languages supported except Zh\\_Hant which is comparable to the previous Multi30k extensions ArEnMulti30k and Multi30k-Uk. COMETKiwi scores reveal mixed assessments of MultiScript30k as a translation of Multi30k-En in comparison to the related work. ArEnMulti30k scores nearly equal MultiScript30k-Ar, but Multi30k-Uk scores $6.4\\%$ greater than MultiScript30k-Uk per split.", "AI": {"tldr": "该研究提出了MultiScript30k，一个扩展Multi30k数据集的新版本，包含了更多全球语言和不同文字系统，以促进多模态机器翻译对多样化语言的研究。", "motivation": "现有的Multi30k数据集仅限于四种欧洲语言和拉丁文字，限制了多模态机器翻译（MMT）研究对多样化语言的探索。尽管存在一些扩展，但支持的语言家族和文字系统仍然非常有限。", "method": "通过使用NLLB200-3.3B模型将Multi30k的英文版本（Multi30k-En）翻译成阿拉伯语（Ar）、西班牙语（Es）、乌克兰语（Uk）、简体中文（Zh_Hans）和繁体中文（Zh_Hant），创建了包含超过30000个句子的MultiScript30k数据集。", "result": "相似性分析显示，除繁体中文外，所有支持语言的余弦相似度均大于0.8，对称KL散度小于0.000251，与之前的Multi30k扩展（如ArEnMulti30k和Multi30k-Uk）相当。COMETKiwi评分结果好坏参半：MultiScript30k-Ar与ArEnMulti30k分数几乎相等，但MultiScript30k-Uk比Multi30k-Uk低6.4%。", "conclusion": "MultiScript30k成功地将Multi30k数据集扩展到了更多全球语言和不同的文字系统，解决了现有数据集语言多样性不足的问题。尽管翻译质量评估结果（COMETKiwi）显示部分语言可能存在提升空间，但该数据集为促进多模态机器翻译对非欧洲语言的研究提供了宝贵的资源。"}}
{"id": "2512.11421", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11421", "abs": "https://arxiv.org/abs/2512.11421", "authors": ["Gonca Gürsun"], "title": "Towards Trustworthy Multi-Turn LLM Agents via Behavioral Guidance", "comment": "Accepted to AAAI 2026 Workshop on Trust and Control in Agentic AI (TrustAgent)", "summary": "Large Language Models demonstrate strong reasoning and generation abilities, yet their behavior in multi-turn tasks often lacks reliability and verifiability. We present a task completion framework that enables LLM-based agents to act under explicit behavioral guidance in environments described by reinforcement learning formalisms with defined observation, action, and reward signals.\n  The framework integrates three components: a lightweight task profiler that selects reasoning and generation strategies, a reasoning module that learns verifiable observation - action mappings, and a generation module that enforces constraint-compliant outputs through validation or deterministic synthesis. We show that as the agent interacts with the environment, these components co-evolve, yielding trustworthy behavior.", "AI": {"tldr": "本文提出一个任务完成框架，使基于大型语言模型（LLM）的智能体能够在多轮任务中，在明确行为指导下，表现出可靠且可验证的行为。", "motivation": "大型语言模型在多轮任务中的行为往往缺乏可靠性和可验证性，尽管它们具有强大的推理和生成能力。", "method": "该框架包含三个协同进化的组件：一个轻量级任务分析器（选择推理和生成策略），一个推理模块（学习可验证的观察-行动映射），以及一个生成模块（通过验证或确定性合成强制输出符合约束）。", "result": "随着智能体与环境的交互，这些组件共同演化，从而产生值得信赖的行为。", "conclusion": "该框架通过集成任务分析、可验证推理和约束生成，使LLM智能体能够在强化学习形式化的环境中，在明确行为指导下实现可靠和可验证的任务完成。"}}
{"id": "2512.11715", "categories": ["cs.CV", "cs.MM", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.11715", "abs": "https://arxiv.org/abs/2512.11715", "authors": ["Wei Chow", "Linfeng Li", "Lingdong Kong", "Zefeng Li", "Qi Xu", "Hang Song", "Tian Ye", "Xian Wang", "Jinbin Bai", "Shilin Xu", "Xiangtai Li", "Junting Pan", "Shaoteng Liu", "Ran Zhou", "Tianshu Yang", "Songhua Liu"], "title": "EditMGT: Unleashing Potentials of Masked Generative Transformers in Image Editing", "comment": null, "summary": "Recent advances in diffusion models (DMs) have achieved exceptional visual quality in image editing tasks. However, the global denoising dynamics of DMs inherently conflate local editing targets with the full-image context, leading to unintended modifications in non-target regions. In this paper, we shift our attention beyond DMs and turn to Masked Generative Transformers (MGTs) as an alternative approach to tackle this challenge. By predicting multiple masked tokens rather than holistic refinement, MGTs exhibit a localized decoding paradigm that endows them with the inherent capacity to explicitly preserve non-relevant regions during the editing process. Building upon this insight, we introduce the first MGT-based image editing framework, termed EditMGT. We first demonstrate that MGT's cross-attention maps provide informative localization signals for localizing edit-relevant regions and devise a multi-layer attention consolidation scheme that refines these maps to achieve fine-grained and precise localization. On top of these adaptive localization results, we introduce region-hold sampling, which restricts token flipping within low-attention areas to suppress spurious edits, thereby confining modifications to the intended target regions and preserving the integrity of surrounding non-target areas. To train EditMGT, we construct CrispEdit-2M, a high-resolution dataset spanning seven diverse editing categories. Without introducing additional parameters, we adapt a pre-trained text-to-image MGT into an image editing model through attention injection. Extensive experiments across four standard benchmarks demonstrate that, with fewer than 1B parameters, our model achieves similarity performance while enabling 6 times faster editing. Moreover, it delivers comparable or superior editing quality, with improvements of 3.6% and 17.6% on style change and style transfer tasks, respectively.", "AI": {"tldr": "扩散模型在局部图像编辑中存在非目标区域修改问题。本文提出EditMGT，一个基于掩码生成Transformer（MGT）的图像编辑框架，利用MGT的局部解码和注意力机制，实现了更快、更精确的编辑，有效避免了对非目标区域的意外修改。", "motivation": "扩散模型在图像编辑中表现出色，但其全局去噪机制导致在局部编辑时容易波及非目标区域，产生不必要的修改。研究旨在寻找一种能更精确控制局部编辑，同时保留非相关区域的方法。", "method": "本文提出了EditMGT，首个基于MGT的图像编辑框架。它利用MGT的局部解码范式来固有地保留非相关区域。具体方法包括：1) 利用MGT的交叉注意力图提供局部编辑信号，并设计多层注意力整合方案以实现精细定位；2) 引入区域保持采样，限制在低注意力区域的标记翻转，从而将修改限制在目标区域；3) 构建了高分辨率数据集CrispEdit-2M用于训练；4) 通过注意力注入，将预训练的文本到图像MGT模型适应为图像编辑模型，无需额外参数。", "result": "EditMGT在四个标准基准测试中表现出色：1) 参数量少于1B，实现了与现有方法相似的性能；2) 编辑速度快6倍；3) 提供了可比或更优的编辑质量；4) 在风格改变任务上提升3.6%，在风格迁移任务上提升17.6%。", "conclusion": "EditMGT通过利用MGT的局部解码特性和注意力机制，成功克服了扩散模型在局部图像编辑中对非目标区域的意外修改问题。它提供了一个更快、更精确的图像编辑解决方案，并在编辑质量和效率上均表现出显著优势。"}}
{"id": "2512.11607", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.11607", "abs": "https://arxiv.org/abs/2512.11607", "authors": ["Zenghao Hou", "Ludovic Leclercq"], "title": "A Modeling and Optimization Framework for Fostering Modal Shift through the Integration of Tradable Credits and Demand-Responsive Autonomous Shuttles", "comment": null, "summary": "Tradable Credit Schemes (TCS) promote the use of public and shared transport by capping private car usage while maintaining fair welfare outcomes by allowing credit trading. However, most existing studies assume unlimited public transit capacity or a fixed occupancy of shared modes, often neglecting waiting time and oversimplifying time-based costs by depending solely on in-vehicle travel time. These assumptions can overstate the system's performance with TCS regulation, especially when there are insufficient public or shared transport supplies.\n  To address this, we develop a dynamic multimodal equilibrium model to capture operation constraints and induced waiting times under TCS regulation. The model integrates travelers' mode choices, credit trading, traffic dynamics, and waiting time, which depend on key operational features of service vehicles such as fleet size and capacity.\n  Besides, most TCS studies assume fixed transport supply, overlooking supply-side responses triggered by demand shifts. Therefore, we further propose integrating adaptive supply management through the deployment of Demand-Responsive Autonomous Shuttles (DRAS) and developing a bi-level optimization framework that incorporates the equilibrium model to jointly optimize TCS design and operational strategies for the DRAS.\n  We apply the framework to a section of the A10 highway near Paris, France, to examine demand-supply interactions and assess the potential benefits of jointly implementing TCS and DRAS. Numerical results demonstrate the importance of modeling operational features within multimodal equilibrium and incorporating flexible supply in TCS policies for mitigating overall generalized cost.", "AI": {"tldr": "本文提出一个动态多模式均衡模型，考虑运营约束和等待时间，以评估可交易信用方案（TCS）的性能。此外，还整合了按需响应式自动穿梭巴士（DRAS）的自适应供应管理，并通过双层优化框架共同优化TCS设计和DRAS运营策略，以缓解总广义成本。", "motivation": "现有TCS研究常假设公共交通运力无限或共享模式固定载客量，忽略等待时间，并过度简化基于时间的成本，导致在公共或共享交通供应不足时高估系统性能。此外，大多数研究假设交通供应固定，忽视了需求变化引发的供应侧响应。", "method": "1. 建立一个动态多模式均衡模型，捕获TCS下的运营约束和诱导等待时间，该模型整合了出行者模式选择、信用交易、交通动态和等待时间，并考虑服务车辆（如车队规模、运力）的关键运营特征。2. 提出通过部署按需响应式自动穿梭巴士（DRAS）整合自适应供应管理。3. 开发一个双层优化框架，将均衡模型纳入其中，以共同优化TCS设计和DRAS运营策略。4. 将该框架应用于法国巴黎A10高速公路的一个路段进行案例研究。", "result": "数值结果表明，在多模式均衡中建模运营特征以及在TCS政策中纳入灵活供应对于降低整体广义成本至关重要。研究展示了联合实施TCS和DRAS的潜在益处。", "conclusion": "在TCS政策中，准确建模运营特征和整合灵活的交通供应（如DRAS）对于缓解整体广义成本具有重要意义。联合实施TCS和DRAS，并通过双层优化框架进行设计和运营，能够有效提升系统性能。"}}
{"id": "2512.11249", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.11249", "abs": "https://arxiv.org/abs/2512.11249", "authors": ["Chandra Raskoti", "Weizi Li"], "title": "Elevation Aware 2D/3D Co-simulation Framework for Large-scale Traffic Flow and High-fidelity Vehicle Dynamics", "comment": null, "summary": "Reliable testing of autonomous driving systems requires simulation environments that combine large-scale traffic modeling with realistic 3D perception and terrain. Existing tools rarely capture real-world elevation, limiting their usefulness in cities with complex topography. This paper presents an automated, elevation-aware co-simulation framework that integrates SUMO with CARLA using a pipeline that fuses OpenStreetMap road networks and USGS elevation data into physically consistent 3D environments. The system generates smooth elevation profiles, validates geometric accuracy, and enables synchronized 2D-3D simulation across platforms. Demonstrations on multiple regions of San Francisco show the framework's scalability and ability to reproduce steep and irregular terrain. The result is a practical foundation for high-fidelity autonomous vehicle testing in realistic, elevation-rich urban settings.", "AI": {"tldr": "本文提出一个自动化的、考虑高程的协同仿真框架，将SUMO与CARLA集成，利用OpenStreetMap和USGS高程数据生成逼真的3D城市环境，以支持自动驾驶系统在复杂地形下的测试。", "motivation": "现有自动驾驶仿真工具很少捕获真实世界的高程信息，这限制了它们在具有复杂地形的城市中的实用性，而真实的高程对于可靠的自动驾驶系统测试至关重要。", "method": "研究开发了一个自动化的、高程感知的协同仿真框架。该框架通过一个管道将OpenStreetMap道路网络和USGS高程数据融合到物理一致的3D环境中，从而集成SUMO和CARLA。它能够生成平滑的高程剖面，验证几何精度，并实现跨平台的同步2D-3D仿真。", "result": "在旧金山多个区域的演示表明，该框架具有可扩展性，并能成功复现陡峭和不规则的地形。", "conclusion": "该框架为在真实、高程丰富的城市环境中进行高保真自动驾驶车辆测试提供了实用的基础。"}}
{"id": "2512.11098", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.11098", "abs": "https://arxiv.org/abs/2512.11098", "authors": ["Nazanin Mahjourian", "Vinh Nguyen"], "title": "Vision-Language Models for Infrared Industrial Sensing in Additive Manufacturing Scene Description", "comment": null, "summary": "Many manufacturing environments operate in low-light conditions or within enclosed machines where conventional vision systems struggle. Infrared cameras provide complementary advantages in such environments. Simultaneously, supervised AI systems require large labeled datasets, which makes zero-shot learning frameworks more practical for applications including infrared cameras. Recent advances in vision-language foundation models (VLMs) offer a new path in zero-shot predictions from paired image-text representations. However, current VLMs cannot understand infrared camera data since they are trained on RGB data. This work introduces VLM-IRIS (Vision-Language Models for InfraRed Industrial Sensing), a zero-shot framework that adapts VLMs to infrared data by preprocessing infrared images captured by a FLIR Boson sensor into RGB-compatible inputs suitable for CLIP-based encoders. We demonstrate zero-shot workpiece presence detection on a 3D printer bed where temperature differences between the build plate and workpieces make the task well-suited for thermal imaging. VLM-IRIS converts the infrared images to magma representation and applies centroid prompt ensembling with a CLIP ViT-B/32 encoder to achieve high accuracy on infrared images without any model retraining. These findings demonstrate that the proposed improvements to VLMs can be effectively extended to thermal applications for label-free monitoring.", "AI": {"tldr": "VLM-IRIS是一个零样本框架，通过将红外图像转换为RGB兼容输入，使视觉语言模型（VLMs）能够应用于红外工业传感，无需模型再训练。", "motivation": "传统的视觉系统在低光或封闭环境中效果不佳，而红外摄像机具有互补优势。然而，监督式AI需要大量标记数据，且当前的视觉语言基础模型（VLMs）仅基于RGB数据训练，无法理解红外数据，限制了其在红外应用中的零样本能力。", "method": "本文提出了VLM-IRIS框架，通过预处理FLIR Boson传感器捕获的红外图像，将其转换为RGB兼容的输入。具体方法包括将红外图像转换为岩浆（magma）表示，并结合质心提示集成（centroid prompt ensembling），然后输入到基于CLIP的ViT-B/32编码器中，整个过程无需模型再训练。", "result": "在3D打印机平台上进行了零样本工件存在检测的演示，利用构建板和工件之间的温差，该任务非常适合热成像。VLM-IRIS在红外图像上实现了高精度，且无需任何模型再训练。", "conclusion": "研究结果表明，所提出的VLM改进方案可以有效扩展到热成像应用中，实现无需标签的监测。"}}
{"id": "2512.11323", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11323", "abs": "https://arxiv.org/abs/2512.11323", "authors": ["Jianyi Zhang", "Ziyin Zhou", "Xu Ji", "Shizhao Liu", "Zhangchi Zhao"], "title": "CAPTURE: A Benchmark and Evaluation for LVLMs in CAPTCHA Resolving", "comment": null, "summary": "Benefiting from strong and efficient multi-modal alignment strategies, Large Visual Language Models (LVLMs) are able to simulate human visual and reasoning capabilities, such as solving CAPTCHAs. However, existing benchmarks based on visual CAPTCHAs still face limitations. Previous studies, when designing benchmarks and datasets, customized them according to their research objectives. Consequently, these benchmarks cannot comprehensively cover all CAPTCHA types. Notably, there is a dearth of dedicated benchmarks for LVLMs. To address this problem, we introduce a novel CAPTCHA benchmark for the first time, named CAPTURE CAPTCHA for Testing Under Real-world Experiments, specifically for LVLMs. Our benchmark encompasses 4 main CAPTCHA types and 25 sub-types from 31 vendors. The diversity enables a multi-dimensional and thorough evaluation of LVLM performance. CAPTURE features extensive class variety, large-scale data, and unique LVLM-tailored labels, filling the gaps in previous research in terms of data comprehensiveness and labeling pertinence. When evaluated by this benchmark, current LVLMs demonstrate poor performance in solving CAPTCHAs.", "AI": {"tldr": "本文首次提出了一个名为CAPTURE的综合性CAPTCHA基准，用于评估大型视觉语言模型（LVLMs），并发现当前LVLMs在解决CAPTCHA方面表现不佳。", "motivation": "现有基于视觉CAPTCHA的基准存在局限性，通常是根据研究目标定制的，无法全面覆盖所有CAPTCHA类型，并且缺乏专门针对LVLMs的基准。", "method": "研究者引入了一个名为CAPTURE（CAPTCHA for Testing Under Real-world Experiments）的新型CAPTCHA基准，专门为LVLMs设计。该基准包含来自31个供应商的4种主要CAPTCHA类型和25种子类型，具有广泛的类别多样性、大规模数据和针对LVLM的独特标签。", "result": "通过使用CAPTURE基准进行评估，当前的大型视觉语言模型在解决CAPTCHA任务时表现出较差的性能。", "conclusion": "CAPTURE基准在数据全面性和标签相关性方面填补了先前研究的空白，并揭示了当前LVLMs在解决多样化CAPTCHA方面的不足。"}}
{"id": "2512.11099", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11099", "abs": "https://arxiv.org/abs/2512.11099", "authors": ["Weitai Kang", "Jason Kuen", "Mengwei Ren", "Zijun Wei", "Yan Yan", "Kangning Liu"], "title": "VGent: Visual Grounding via Modular Design for Disentangling Reasoning and Prediction", "comment": "8 pages", "summary": "Current visual grounding models are either based on a Multimodal Large Language Model (MLLM) that performs auto-regressive decoding, which is slow and risks hallucinations, or on re-aligning an LLM with vision features to learn new special or object tokens for grounding, which may undermine the LLM's pretrained reasoning ability. In contrast, we propose VGent, a modular encoder-decoder architecture that explicitly disentangles high-level reasoning and low-level bounding box prediction. Specifically, a frozen MLLM serves as the encoder to provide untouched powerful reasoning capabilities, while a decoder takes high-quality boxes proposed by detectors as queries and selects target box(es) via cross-attending on encoder's hidden states. This design fully leverages advances in both object detection and MLLM, avoids the pitfalls of auto-regressive decoding, and enables fast inference. Moreover, it supports modular upgrades of both the encoder and decoder to benefit the whole system: we introduce (i) QuadThinker, an RL-based training paradigm for enhancing multi-target reasoning ability of the encoder; (ii) mask-aware label for resolving detection-segmentation ambiguity; and (iii) global target recognition to improve the recognition of all the targets which benefits the selection among augmented proposals. Experiments on multi-target visual grounding benchmarks show that VGent achieves a new state-of-the-art with +20.6% F1 improvement over prior methods, and further boosts gIoU by +8.2% and cIoU by +5.8% under visual reference challenges, while maintaining constant, fast inference latency.", "AI": {"tldr": "VGent是一种模块化的视觉定位模型，它将高层推理和低层边界框预测解耦。它利用冻结的多模态大语言模型（MLLM）进行推理，并使用基于检测器的解码器来选择目标框，从而避免了自回归解码的缺点，实现了最先进的性能和快速推理。", "motivation": "当前的视觉定位模型存在缺陷：基于MLLM的模型采用自回归解码，速度慢且有幻觉风险；通过视觉特征重新对齐LLM可能损害其预训练的推理能力。因此，需要一种新方法来充分利用MLLM的强大推理能力和目标检测的进步，同时避免这些缺点。", "method": "VGent提出了一种模块化的编码器-解码器架构：\n1.  **编码器**：使用一个冻结的MLLM，提供未触及的强大推理能力。\n2.  **解码器**：将检测器生成的高质量边界框作为查询，并通过交叉注意力在编码器的隐藏状态上选择目标框。\n这种设计充分利用了目标检测和MLLM的优势，避免了自回归解码的陷阱，并实现了快速推理。\n此外，它还支持模块化升级，包括：\n(i) **QuadThinker**：一种基于强化学习的训练范式，用于增强编码器的多目标推理能力。\n(ii) **掩码感知标签**：解决检测-分割模糊性。\n(iii) **全局目标识别**：改进所有目标的识别，有利于在增强提案中进行选择。", "result": "VGent在多目标视觉定位基准测试中取得了新的最先进成果：\n-   F1分数比现有方法提高了+20.6%。\n-   在视觉参考挑战下，gIoU提高了+8.2%，cIoU提高了+5.8%。\n-   保持了恒定、快速的推理延迟。", "conclusion": "VGent通过其模块化编码器-解码器架构，成功地将MLLM的强大推理能力与目标检测的精度相结合，有效解决了现有视觉定位模型的速度和性能问题。它在多个指标上取得了显著的SOTA改进，同时保持了快速推理，并通过模块化设计为未来的升级提供了便利。"}}
{"id": "2512.11250", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.11250", "abs": "https://arxiv.org/abs/2512.11250", "authors": ["Brock Marcinczyk", "Logan E. Beaver"], "title": "Optimal Control and Structurally-Informed Gradient Optimization of a Custom 4-DOF Rigid-Body Manipulator", "comment": "6 pages + 18 page appendix", "summary": "This work develops a control-centric framework for a custom 4-DOF rigid-body manipulator by coupling a reduced-order Pontryagin's Maximum Principle (PMP) controller with a physics-informed Gradient Descent stage. The reduced PMP model provides a closed-form optimal control law for the joint accelerations, while the Gradient Descent module determines the corresponding time horizons by minimizing a cost functional built directly from the full Rigid-Body Dynamics. Structural-mechanics reaction analysis is used only to initialize feasible joint velocities-most critically the azimuthal component-ensuring that the optimizer begins in a physically admissible region. The resulting kinematic trajectories and dynamically consistent time horizons are then supplied to the symbolic Euler-Lagrange model to yield closed-form inverse-dynamics inputs. This pipeline preserves a strict control-theoretic structure while embedding the physical constraints and loading behavior of the manipulator in a computationally efficient way.", "AI": {"tldr": "本文为定制的四自由度刚体机械臂开发了一个以控制为中心的框架，结合了简化的庞特里亚金最大值原理（PMP）控制器和物理信息梯度下降阶段，以高效地生成运动学轨迹和逆动力学输入，同时嵌入物理约束。", "motivation": "研究动机在于开发一种计算高效的控制框架，该框架能为刚体机械臂提供严格的控制理论结构，并有效嵌入其物理约束和负载行为。", "method": "该方法将简化的庞特里亚金最大值原理（PMP）控制器与物理信息梯度下降阶段耦合。PMP模型提供关节加速度的闭式最优控制律，而梯度下降模块通过最小化基于完整刚体动力学的成本函数来确定时间范围。结构力学反应分析用于初始化可行的关节速度，确保优化器从物理允许区域开始。最终的运动学轨迹和动态一致的时间范围被输入到符号欧拉-拉格朗模型，以获得闭式逆动力学输入。", "result": "该管道成功地保留了严格的控制理论结构，同时以计算高效的方式嵌入了机械臂的物理约束和负载行为。它生成了运动学轨迹、动态一致的时间范围以及闭式逆动力学输入。", "conclusion": "该工作开发了一个计算高效且控制理论结构严谨的框架，能够有效地控制机械臂，并通过结合最优控制和物理约束，生成所需的运动学和动力学结果。"}}
{"id": "2512.11713", "categories": ["eess.SY", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.11713", "abs": "https://arxiv.org/abs/2512.11713", "authors": ["Amirreza Akbari", "Johan Thunberg"], "title": "Two-dimensional Decompositions of High-dimensional Configurations for Efficient Multi-vehicle Coordination at Intelligent Intersections", "comment": null, "summary": "For multi-vehicle complex traffic scenarios in shared spaces such as intelligent intersections, safe coordination and trajectory planning is challenging due to computational complexity. To meet this challenge, we introduce a computationally efficient method for generating collision-free trajectories along predefined vehicle paths. We reformulate a constrained minimum-time trajectory planning problem as a problem in a high-dimensional configuration space, where conflict zones are modeled by high-dimensional polyhedra constructed from two-dimensional rectangles. Still, in such a formulation, as the number of vehicles involved increases, the computational complexity increases significantly. To address this, we propose two algorithms for near-optimal local optimization that significantly reduce the computational complexity by decomposing the high-dimensional problem into a sequence of 2D graph search problems. The resulting trajectories are then incorporated into a Nonlinear Model Predictive Control (NMPC) framework to ensure safe and smooth vehicle motion. We furthermore show in numerical evaluation that this approach significantly outperforms existing MILP-based time-scheduling; both in terms of objective-value and computational time.", "AI": {"tldr": "该研究提出了一种计算高效的多车辆共享空间（如智能交叉口）无碰撞轨迹规划方法，通过高维问题分解为2D图搜索并结合NMPC，显著优于现有方法。", "motivation": "在智能交叉口等共享空间中，多车辆复杂交通场景的安全协调和轨迹规划因计算复杂性而具有挑战性。", "method": "将受限的最小时间轨迹规划问题重新表述为高维配置空间问题，其中冲突区域由高维多面体建模。为降低计算复杂性，提出了两种近最优局部优化算法，将高维问题分解为一系列2D图搜索问题。最终轨迹被整合到非线性模型预测控制（NMPC）框架中，以确保车辆安全平稳运动。", "result": "数值评估表明，该方法在目标值和计算时间方面均显著优于现有的基于MILP的时间调度方法。", "conclusion": "该研究为多车辆共享空间提供了一种计算高效且性能优越的无碰撞轨迹生成方法，解决了现有方法在计算复杂性方面的挑战。"}}
{"id": "2512.11108", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11108", "abs": "https://arxiv.org/abs/2512.11108", "authors": ["Jonathan Kamp", "Roos Bakker", "Dominique Blok"], "title": "Explanation Bias is a Product: Revealing the Hidden Lexical and Position Preferences in Post-Hoc Feature Attribution", "comment": null, "summary": "Good quality explanations strengthen the understanding of language models and data. Feature attribution methods, such as Integrated Gradient, are a type of post-hoc explainer that can provide token-level insights. However, explanations on the same input may vary greatly due to underlying biases of different methods. Users may be aware of this issue and mistrust their utility, while unaware users may trust them inadequately. In this work, we delve beyond the superficial inconsistencies between attribution methods, structuring their biases through a model- and method-agnostic framework of three evaluation metrics. We systematically assess both the lexical and position bias (what and where in the input) for two transformers; first, in a controlled, pseudo-random classification task on artificial data; then, in a semi-controlled causal relation detection task on natural data. We find that lexical and position biases are structurally unbalanced in our model comparison, with models that score high on one type score low on the other. We also find signs that methods producing anomalous explanations are more likely to be biased themselves.", "AI": {"tldr": "本文通过一个模型和方法无关的框架，系统评估了特征归因方法在语言模型解释中存在的词汇和位置偏差，并发现这些偏差在不同模型间结构性不平衡，且异常解释可能源于方法本身的偏差。", "motivation": "特征归因方法提供的解释可能因方法固有偏差而差异巨大，导致用户对其效用产生不信任或过度信任。研究旨在深入理解并结构化这些偏差，而非仅停留在表面不一致性。", "method": "研究构建了一个模型和方法无关的框架，包含三个评估指标来结构化偏差。通过在人工数据上的受控伪随机分类任务和自然数据上的半受控因果关系检测任务，系统评估了两种Transformer模型在词汇偏差（什么）和位置偏差（哪里）方面的表现。", "result": "研究发现，在模型比较中，词汇和位置偏差结构性不平衡，即在一个偏差类型上得分高的模型在另一个类型上得分低。此外，产生异常解释的方法本身更可能存在偏差。", "conclusion": "特征归因方法存在结构性不平衡的词汇和位置偏差，不同模型在这些偏差类型上的表现存在权衡。同时，异常解释可能是方法自身偏差的信号，这对于理解和选择解释方法具有重要意义。"}}
{"id": "2512.11275", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.11275", "abs": "https://arxiv.org/abs/2512.11275", "authors": ["Suchang Chen", "Daqiang Guo"], "title": "Towards Logic-Aware Manipulation: A Knowledge Primitive for VLM-Based Assistants in Smart Manufacturing", "comment": "8 pages, 2 figures, submitted to the 2026 IFAC World Congress", "summary": "Existing pipelines for vision-language models (VLMs) in robotic manipulation prioritize broad semantic generalization from images and language, but typically omit execution-critical parameters required for contact-rich actions in manufacturing cells. We formalize an object-centric manipulation-logic schema, serialized as an eight-field tuple τ, which exposes object, interface, trajectory, tolerance, and force/impedance information as a first-class knowledge signal between human operators, VLM-based assistants, and robot controllers. We instantiate τ and a small knowledge base (KB) on a 3D-printer spool-removal task in a collaborative cell, and analyze τ-conditioned VLM planning using plan-quality metrics adapted from recent VLM/LLM planning benchmarks, while demonstrating how the same schema supports taxonomy-tagged data augmentation at training time and logic-aware retrieval-augmented prompting at test time as a building block for assistant systems in smart manufacturing enterprises.", "AI": {"tldr": "本文提出了一种名为τ的八字段元组对象中心操作逻辑模式，旨在弥补现有视觉-语言模型（VLMs）在机器人操作中对接触密集型任务执行关键参数的缺失，并展示了其在规划、数据增强和检索增强提示中的应用。", "motivation": "现有用于机器人操作的视觉-语言模型（VLMs）虽然侧重于图像和语言的广泛语义泛化，但通常忽略了制造单元中接触密集型动作所需的执行关键参数，这限制了它们在实际应用中的有效性。", "method": "研究者将对象中心操作逻辑模式形式化为一个八字段元组τ，该元组将对象、接口、轨迹、容差以及力/阻抗信息作为一等知识信号。他们在一个协作单元中的3D打印机线轴移除任务上实例化了τ和小型知识库（KB），并使用改编自VLM/LLM规划基准的计划质量指标分析了τ条件下的VLM规划。此外，他们还展示了该模式如何支持训练时的分类标签数据增强和测试时的逻辑感知检索增强提示。", "result": "该模式τ成功地将执行关键参数（如对象、接口、轨迹、容差、力/阻抗信息）作为知识信号在人类操作员、VLM助手和机器人控制器之间传递。在3D打印机线轴移除任务中，τ条件下的VLM规划得到了分析，并且该模式被证明能够支持分类标签数据增强和逻辑感知检索增强提示，为智能制造企业中的辅助系统提供了基础构建模块。", "conclusion": "所提出的对象中心操作逻辑模式τ是智能制造企业中辅助系统的重要构建模块，它使得视觉-语言模型能够处理接触密集型任务所需的执行关键参数，从而提高了VLM在机器人操作中的实用性和有效性。"}}
{"id": "2512.11426", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11426", "abs": "https://arxiv.org/abs/2512.11426", "authors": ["Shuowei Cai", "Yansong Ning", "Hao Liu"], "title": "AgentBalance: Backbone-then-Topology Design for Cost-Effective Multi-Agent Systems under Budget Constraints", "comment": null, "summary": "Large Language Model (LLM)-based multi-agent systems (MAS) are becoming indispensable building blocks for web-scale applications such as web search, social network analytics, and online customer support, where cost-effectiveness is increasingly the primary constraint for large-scale deployment. While recent work improves MAS cost-effectiveness by shaping inter-agent communication topologies and selecting agent backbones, it rarely models and optimizes under explicit token-cost and latency budgets that reflect deployment constraints. This often leads to topology-first designs and suboptimal cost-effectiveness when budgets are binding. We present AgentBalance, a framework for constructing cost-effective MAS under explicit token-cost and latency budgets via a backbone-then-topology design. AgentBalance first performs backbone-oriented agent generation, constructing agents with heterogeneous backbones through LLM pool construction, pool selection, and role-backbone matching. It then performs adaptive MAS topology generation, guiding inter-agent communication via agent representation learning, gating, and latency-aware topology synthesis. Experiments on benchmarks with 14 candidate LLM backbones show that AgentBalance achieves up to 10% and 22% performance gains under matched token-cost and latency budgets, respectively, and yields strong AUC on performance-versus-budget curves across benchmarks. AgentBalance also functions as a plug-in for existing MAS, improving performance under the same token-cost and latency constraints, and it generalizes well to unseen LLMs for practical, budget-aware deployment. Code: https://github.com/usail-hkust/AgentBalance", "AI": {"tldr": "AgentBalance是一个框架，通过“骨干优先，拓扑其次”的设计，在明确的令牌成本和延迟预算下构建具有成本效益的大语言模型多智能体系统（MAS）。", "motivation": "当前的多智能体系统成本效益研究很少在明确的令牌成本和延迟预算下进行建模和优化，这在预算受限时往往导致次优设计。对于网络规模应用，成本效益是部署的关键约束。", "method": "AgentBalance采用“骨干优先，拓扑其次”的设计。首先进行面向骨干的智能体生成，通过LLM池构建、池选择和角色-骨干匹配来创建异构骨干的智能体。然后进行自适应MAS拓扑生成，通过智能体表示学习、门控和延迟感知拓扑合成来指导智能体间通信。", "result": "在具有14个候选LLM骨干的基准测试中，AgentBalance在匹配的令牌成本预算下实现了高达10%的性能提升，在匹配的延迟预算下实现了高达22%的性能提升。它在性能-预算曲线上表现出强大的AUC，并能作为现有MAS的插件使用，且对未见过的LLM具有良好的泛化能力。", "conclusion": "AgentBalance有效解决了在明确令牌成本和延迟预算下构建成本效益型多智能体系统的问题，显著提高了性能，并为实际的预算感知部署提供了可行方案。"}}
{"id": "2512.11734", "categories": ["eess.SY", "math.DS"], "pdf": "https://arxiv.org/pdf/2512.11734", "abs": "https://arxiv.org/abs/2512.11734", "authors": ["Yuntao Dai"], "title": "Model Error Resonance: The Geometric Nature of Error Dynamics", "comment": null, "summary": "This paper introduces a geometric theory of model error, treating true and model dynamics as geodesic flows generated by distinct affine connections on a smooth manifold. When these connections differ, the resulting trajectory discrepancy--termed the Latent Error Dynamic Response (LEDR)--acquires an intrinsic dynamical structure governed by curvature. We show that the LEDR satisfies a Jacobi-type equation, where curvature mismatch acts as an explicit forcing term. In the important case of a flat model connection, the LEDR reduces to a classical Jacobi field on the true manifold, causing Model Error Resonance (MER) to emerge under positive sectional curvature. The theory is extended to a discrete-time analogue, establishing that this geometric structure and its resonant behavior persist in sampled systems. A closed-form analysis of a sphere--plane example demonstrates that curvature can be inferred directly from the LEDR evolution. This framework provides a unified geometric interpretation of structured error dynamics and offers foundational tools for curvature-informed model validation.", "AI": {"tldr": "本文提出了一种模型误差的几何理论，将真实和模型动力学视为流形上由不同仿射联络生成的测地流。误差（LEDR）被发现具有由曲率控制的内在动力学结构，并满足一个雅可比型方程，曲率不匹配作为强迫项。在模型联络为平坦的情况下，LEDR在正截面曲率下会导致模型误差共振（MER）。该理论也适用于离散时间系统，并通过一个球体-平面示例展示了曲率可从LEDR演化中推断。", "motivation": "理解和量化当真实和模型动力学不同时产生的模型误差，并揭示其内在的动力学结构。", "method": "将真实和模型动力学建模为光滑流形上由不同仿射联络生成的测地流。将轨迹差异定义为潜在误差动力学响应（LEDR）。推导LEDR满足的雅可比型方程，其中曲率不匹配是显式强迫项。分析了模型联络为平坦的情况。将理论扩展到离散时间系统。通过球体-平面示例进行闭式分析。", "result": "LEDR具有由曲率控制的内在动力学结构，并满足一个以曲率不匹配为强迫项的雅可比型方程。在模型联络为平坦的情况下，LEDR在正截面曲率下会导致模型误差共振（MER）。这种几何结构和共振行为在采样系统中依然存在。曲率可以直接从LEDR演化中推断出来。", "conclusion": "该框架为结构化误差动力学提供了一个统一的几何解释，并为基于曲率的模型验证提供了基础工具。"}}
{"id": "2512.11104", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11104", "abs": "https://arxiv.org/abs/2512.11104", "authors": ["Brennan Flannery", "Thomas DeSilvio", "Jane Nguyen", "Satish E. Viswanath"], "title": "Information-driven Fusion of Pathology Foundation Models for Enhanced Disease Characterization", "comment": "29 Pages, 10 figures", "summary": "Foundation models (FMs) have demonstrated strong performance across diverse pathology tasks. While there are similarities in the pre-training objectives of FMs, there is still limited understanding of their complementarity, redundancy in embedding spaces, or biological interpretation of features. In this study, we propose an information-driven, intelligent fusion strategy for integrating multiple pathology FMs into a unified representation and systematically evaluate its performance for cancer grading and staging across three distinct diseases. Diagnostic H&E whole-slide images from kidney (519 slides), prostate (490 slides), and rectal (200 slides) cancers were dichotomized into low versus high grade or stage. Both tile-level FMs (Conch v1.5, MUSK, Virchow2, H-Optimus1, Prov-Gigapath) and slide-level FMs (TITAN, CHIEF, MADELEINE) were considered to train downstream classifiers. We then evaluated three FM fusion schemes at both tile and slide levels: majority-vote ensembling, naive feature concatenation, and intelligent fusion based on correlation-guided pruning of redundant features. Under patient-stratified cross-validation with hold-out testing, intelligent fusion of tile-level embeddings yielded consistent gains in classification performance across all three cancers compared with the best single FMs and naive fusion. Global similarity metrics revealed substantial alignment of FM embedding spaces, contrasted by lower local neighborhood agreement, indicating complementary fine-grained information across FMs. Attention maps showed that intelligent fusion yielded concentrated attention on tumor regions while reducing spurious focus on benign regions. Our findings suggest that intelligent, correlation-guided fusion of pathology FMs can yield compact, task-tailored representations that enhance both predictive performance and interpretability in downstream computational pathology tasks.", "AI": {"tldr": "本研究提出了一种信息驱动的智能融合策略，用于整合多个病理基础模型（FMs），以创建统一的表示。该策略在癌症分级和分期任务中显示出优于单一模型和朴素融合方法的性能提升和可解释性。", "motivation": "基础模型在各种病理任务中表现出色，但对其互补性、嵌入空间中的冗余以及特征的生物学解释仍知之甚少。研究旨在探索如何有效地整合多个病理基础模型，以获得更强大、更具解释性的表示。", "method": "研究使用了来自肾脏、前列腺和直肠癌的H&E全玻片图像（共1209张），并将其二分为低级别与高级别或低分期与高分期。考虑了瓦片级FMs（如Conch v1.5, MUSK）和玻片级FMs（如TITAN, CHIEF）。评估了三种FM融合方案：多数投票集成、朴素特征拼接，以及基于相关性引导冗余特征剪枝的智能融合。采用患者分层交叉验证和保留测试集进行评估，并分析了全局相似性、局部邻域一致性和注意力图。", "result": "智能融合瓦片级嵌入在所有三种癌症的分类性能上，相对于最佳单一FM和朴素融合，均取得了持续提升。全局相似性指标显示FM嵌入空间具有显著对齐性，但局部邻域一致性较低，表明FMs之间存在互补的细粒度信息。注意力图显示，智能融合能将注意力集中在肿瘤区域，同时减少对良性区域的错误关注。", "conclusion": "病理基础模型的智能、相关性引导融合可以产生紧凑、任务定制的表示，从而提高下游计算病理任务的预测性能和可解释性。"}}
{"id": "2512.11433", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11433", "abs": "https://arxiv.org/abs/2512.11433", "authors": ["Agustin Martin Picard", "Thibaut Boissin", "Varshini Subhash", "Rémi Cadène", "Thomas Fel"], "title": "Back to the Baseline: Examining Baseline Effects on Explainability Metrics", "comment": null, "summary": "Attribution methods are among the most prevalent techniques in Explainable Artificial Intelligence (XAI) and are usually evaluated and compared using Fidelity metrics, with Insertion and Deletion being the most popular. These metrics rely on a baseline function to alter the pixels of the input image that the attribution map deems most important. In this work, we highlight a critical problem with these metrics: the choice of a given baseline will inevitably favour certain attribution methods over others. More concerningly, even a simple linear model with commonly used baselines contradicts itself by designating different optimal methods. A question then arises: which baseline should we use? We propose to study this problem through two desirable properties of a baseline: (i) that it removes information and (ii) that it does not produce overly out-of-distribution (OOD) images. We first show that none of the tested baselines satisfy both criteria, and there appears to be a trade-off among current baselines: either they remove information or they produce a sequence of OOD images. Finally, we introduce a novel baseline by leveraging recent work in feature visualisation to artificially produce a model-dependent baseline that removes information without being overly OOD, thus improving on the trade-off when compared to other existing baselines. Our code is available at https://github.com/deel-ai-papers/Back-to-the-Baseline", "AI": {"tldr": "归因方法评估中的保真度指标存在基线选择问题，现有基线无法同时满足信息移除和避免生成过多分布外图像的需求，本文提出了一种基于特征可视化的新型基线来改善这一权衡。", "motivation": "可解释人工智能（XAI）中的归因方法通常通过保真度指标（如插入和删除）进行评估和比较。这些指标依赖于基线函数来修改输入图像中被认为最重要的像素。然而，研究发现基线的选择会不可避免地偏袒某些归因方法，甚至简单的线性模型在使用常用基线时也会自相矛盾地指定不同的最优方法，这促使研究者思考应该使用哪种基线。", "method": "本文通过基线的两个理想属性来研究这个问题：(i) 移除信息，以及 (ii) 不产生过于分布外（OOD）的图像。首先，测试了现有基线是否满足这两个标准。最后，利用特征可视化领域的最新工作，引入了一种新颖的基线，通过人工生成一种模型依赖的基线，该基线在移除信息的同时避免过度OOD。", "result": "研究发现，所有测试的现有基线都无法同时满足信息移除和避免生成过多OOD图像这两个标准，现有基线似乎存在一种权衡：要么移除信息，要么生成一系列OOD图像。本文提出的新型基线通过利用特征可视化，在移除信息且不过度OOD方面，改善了与现有基线相比的权衡。", "conclusion": "基线选择对归因方法的评估至关重要。本文提出了一种基于特征可视化的新型模型依赖基线，它能在移除信息和避免生成过多分布外图像之间提供更好的权衡，从而改进了归因方法的评估方式。"}}
{"id": "2512.11351", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.11351", "abs": "https://arxiv.org/abs/2512.11351", "authors": ["Steffen Schäfer", "Martin Cichon"], "title": "Incremental Validation of Automated Driving Functions using Generic Volumes in Micro- Operational Design Domains", "comment": null, "summary": "The validation of highly automated, perception-based driving systems must ensure that they function correctly under the full range of real-world conditions. Scenario-based testing is a prominent approach to addressing this challenge, as it involves the systematic simulation of objects and environments. Operational Design Domains (ODDs) are usually described using a taxonomy of qualitative designations for individual objects. However, the process of transitioning from taxonomy to concrete test cases remains unstructured, and completeness is theoretical. This paper introduces a structured method of subdividing the ODD into manageable sections, termed micro-ODDs (mODDs), and deriving test cases with abstract object representations. This concept is demonstrated using a one-dimensional, laterally guided manoeuvre involving a shunting locomotive within a constrained ODD. In this example, mODDs are defined and refined into narrow taxonomies that enable test case generation. Obstacles are represented as generic cubes of varying sizes, providing a simplified yet robust means of evaluating perception performance. A series of tests were conducted in a closed-loop, co-simulated virtual environment featuring photorealistic rendering and simulated LiDAR, GNSS and camera sensors. The results demonstrate how edge cases in obstacle detection can be systematically explored and how perception quality can be evaluated based on observed vehicle behaviour, using crash versus safe stop as the outcome metrics. These findings support the development of a standardised framework for safety argumentation and offer a practical step towards the validation and authorisation of automated driving functions.", "AI": {"tldr": "本文提出了一种名为微操作设计域（mODD）的结构化方法，用于将操作设计域（ODD）细分为可管理的部分，并从抽象对象表示中导出测试用例，从而系统地验证基于感知的自动驾驶系统，以确保其在各种真实世界条件下的功能正确性。", "motivation": "自动驾驶系统（特别是基于感知的系统）的验证面临巨大挑战，需要确保其在所有真实世界条件下都能正确运行。当前的场景测试方法中，从定性的操作设计域（ODD）分类法到具体的测试用例的转换缺乏结构化，且完整性仅是理论上的，因此需要一种更系统、更具体的方法来生成测试用例并探索边缘情况。", "method": "本文引入了微操作设计域（mODD）的概念，将操作设计域（ODD）细分为更小的、可管理的区域。该方法通过一个一维横向引导机动（涉及调车机车）的受限ODD示例进行演示。mODD被定义并细化为狭窄的分类法，用于生成测试用例。障碍物被表示为不同尺寸的通用立方体。测试在一个闭环、协同仿真的虚拟环境中进行，该环境具有逼真的渲染和模拟的LiDAR、GNSS和摄像头传感器。", "result": "研究结果表明，该方法能够系统地探索障碍物检测中的边缘情况，并且可以根据观察到的车辆行为（如碰撞与安全停车）来评估感知质量。碰撞与安全停车被用作评估感知性能的输出指标。", "conclusion": "这些发现支持开发一个用于安全论证的标准化框架，并为自动驾驶功能的验证和授权提供了切实可行的步骤。该方法有助于解决从ODD分类法到具体测试用例的转换问题，提升了测试的系统性和完整性。"}}
{"id": "2512.11130", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.11130", "abs": "https://arxiv.org/abs/2512.11130", "authors": ["Bowen Wen", "Shaurya Dewan", "Stan Birchfield"], "title": "Fast-FoundationStereo: Real-Time Zero-Shot Stereo Matching", "comment": null, "summary": "Stereo foundation models achieve strong zero-shot generalization but remain computationally prohibitive for real-time applications. Efficient stereo architectures, on the other hand, sacrifice robustness for speed and require costly per-domain fine-tuning. To bridge this gap, we present Fast-FoundationStereo, a family of architectures that achieve, for the first time, strong zero-shot generalization at real-time frame rate. We employ a divide-and-conquer acceleration strategy with three components: (1) knowledge distillation to compress the hybrid backbone into a single efficient student; (2) blockwise neural architecture search for automatically discovering optimal cost filtering designs under latency budgets, reducing search complexity exponentially; and (3) structured pruning for eliminating redundancy in the iterative refinement module. Furthermore, we introduce an automatic pseudo-labeling pipeline used to curate 1.4M in-the-wild stereo pairs to supplement synthetic training data and facilitate knowledge distillation. The resulting model can run over 10x faster than FoundationStereo while closely matching its zero-shot accuracy, thus establishing a new state-of-the-art among real-time methods. Project page: https://nvlabs.github.io/Fast-FoundationStereo/", "AI": {"tldr": "Fast-FoundationStereo通过结合知识蒸馏、块级神经架构搜索和结构化剪枝，首次实现了立体匹配基础模型的实时运行速度和强大的零样本泛化能力。", "motivation": "现有的立体匹配基础模型虽然泛化能力强，但计算成本高昂，无法用于实时应用；而高效模型虽然速度快，但鲁棒性差，且需要昂贵的领域特定微调。研究旨在弥合这一差距，开发一种既能实时运行又具有强零样本泛化能力的立体匹配架构。", "method": "该研究采用分而治之的加速策略，包含三个核心组件：1) 知识蒸馏，将混合骨干网络压缩成一个高效的学生模型；2) 块级神经架构搜索，在延迟预算下自动发现最优的成本滤波设计；3) 结构化剪枝，消除迭代细化模块中的冗余。此外，还引入了自动伪标签流水线，生成1.4M野外立体图像对以补充合成训练数据并促进知识蒸馏。", "result": "Fast-FoundationStereo模型的运行速度比FoundationStereo快10倍以上，同时其零样本精度与其非常接近。这使得该模型在实时立体匹配方法中达到了新的最先进水平。", "conclusion": "该研究成功地弥合了立体匹配基础模型的零样本泛化能力与实时应用之间的鸿沟，首次实现了在实时帧率下具备强大零样本泛化能力的立体匹配模型。"}}
{"id": "2512.11110", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11110", "abs": "https://arxiv.org/abs/2512.11110", "authors": ["Evren Ayberk Munis", "Deniz Yılmaz", "Arianna Muti", "Çağrı Toraman"], "title": "FIBER: A Multilingual Evaluation Resource for Factual Inference Bias", "comment": null, "summary": "Large language models are widely used across domains, yet there are concerns about their factual reliability and biases. Factual knowledge probing offers a systematic means to evaluate these aspects. Most existing benchmarks focus on single-entity facts and monolingual data. We therefore present FIBER, a multilingual benchmark for evaluating factual knowledge in single- and multi-entity settings. The dataset includes sentence completion, question-answering, and object-count prediction tasks in English, Italian, and Turkish. Using FIBER, we examine whether the prompt language induces inference bias in entity selection and how large language models perform on multi-entity versus single-entity questions. The results indicate that the language of the prompt can influence the model's generated output, particularly for entities associated with the country corresponding to that language. However, this effect varies across different topics such that 31% of the topics exhibit factual inference bias score greater than 0.5. Moreover, the level of bias differs across languages such that Turkish prompts show higher bias compared to Italian in 83% of the topics, suggesting a language-dependent pattern. Our findings also show that models face greater difficulty when handling multi-entity questions than the single-entity questions. Model performance differs across both languages and model sizes. The highest mean average precision is achieved in English, while Turkish and Italian lead to noticeably lower scores. Larger models, including Llama-3.1-8B and Qwen-2.5-7B, show consistently better performance than smaller 3B-4B models.", "AI": {"tldr": "该研究引入了FIBER，一个多语言基准测试，用于评估大型语言模型在单实体和多实体事实知识方面的表现。结果表明，提示语言会引发推理偏差，多实体问题更具挑战性，且模型性能因语言和模型大小而异。", "motivation": "大型语言模型（LLMs）在事实可靠性和偏见方面存在担忧。现有的知识探测基准主要关注单实体事实和单语言数据，缺乏对多语言和多实体场景的系统评估。", "method": "研究构建了FIBER，一个多语言基准测试数据集，包含英语、意大利语和土耳其语的句子补全、问答和对象计数预测任务。利用FIBER，研究评估了提示语言是否在实体选择中引起推理偏差，以及LLMs在多实体和单实体问题上的表现。测试了Llama-3.1-8B、Qwen-2.5-7B等大型模型以及较小的3B-4B模型。", "result": "结果显示，提示语言会影响模型生成输出，特别是对于与该语言对应国家相关的实体（31%的主题表现出大于0.5的事实推理偏差）。偏差程度因语言而异，土耳其语提示在83%的主题中显示出比意大利语更高的偏差。模型在处理多实体问题时比单实体问题更困难。模型性能因语言和模型大小而异，英语的平均精度最高，而土耳其语和意大利语得分较低。更大的模型（如Llama-3.1-8B和Qwen-2.5-7B）表现优于较小的3B-4B模型。", "conclusion": "提示语言可以诱导LLMs产生事实推理偏差，尤其与实体所属国家相关。LLMs在处理多实体问题时面临更大挑战。模型在事实知识方面的表现依赖于语言和模型规模。这些发现强调了在多语言和多实体环境中评估和改进LLMs事实知识的重要性。"}}
{"id": "2512.11362", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.11362", "abs": "https://arxiv.org/abs/2512.11362", "authors": ["Chao Xu", "Suyu Zhang", "Yang Liu", "Baigui Sun", "Weihong Chen", "Bo Xu", "Qi Liu", "Juncheng Wang", "Shujun Wang", "Shan Luo", "Jan Peters", "Athanasios V. Vasilakos", "Stefanos Zafeiriou", "Jiankang Deng"], "title": "An Anatomy of Vision-Language-Action Models: From Modules to Milestones and Challenges", "comment": null, "summary": "Vision-Language-Action (VLA) models are driving a revolution in robotics, enabling machines to understand instructions and interact with the physical world. This field is exploding with new models and datasets, making it both exciting and challenging to keep pace with. This survey offers a clear and structured guide to the VLA landscape. We design it to follow the natural learning path of a researcher: we start with the basic Modules of any VLA model, trace the history through key Milestones, and then dive deep into the core Challenges that define recent research frontier. Our main contribution is a detailed breakdown of the five biggest challenges in: (1) Representation, (2) Execution, (3) Generalization, (4) Safety, and (5) Dataset and Evaluation. This structure mirrors the developmental roadmap of a generalist agent: establishing the fundamental perception-action loop, scaling capabilities across diverse embodiments and environments, and finally ensuring trustworthy deployment-all supported by the essential data infrastructure. For each of them, we review existing approaches and highlight future opportunities. We position this paper as both a foundational guide for newcomers and a strategic roadmap for experienced researchers, with the dual aim of accelerating learning and inspiring new ideas in embodied intelligence. A live version of this survey, with continuous updates, is maintained on our \\href{https://suyuz1.github.io/Survery/}{project page}.", "AI": {"tldr": "这篇综述旨在为视觉-语言-动作（VLA）模型在机器人领域的快速发展提供一个清晰的结构化指南，涵盖了其基本模块、发展里程碑及五大核心挑战。", "motivation": "VLA模型在机器人领域发展迅猛，新模型和数据集层出不穷，使得研究人员难以跟上步伐。因此，需要一个清晰且结构化的指南来帮助理解和导航这一复杂领域。", "method": "本综述采用结构化的方法，遵循研究者的学习路径：首先介绍VLA模型的基本模块，接着追溯其关键里程碑，然后深入探讨定义当前研究前沿的五大核心挑战：表征、执行、泛化、安全以及数据集与评估。对于每个挑战，综述都回顾了现有方法并指出了未来的机遇。", "result": "本综述的主要贡献是详细分解了VLA模型面临的五大挑战，并为每个挑战审查了现有方法并突出了未来的研究机会。这反映了通用智能体从感知-行动循环到能力扩展再到可信部署的发展路线图。", "conclusion": "这篇综述旨在作为新手的入门指南和经验丰富的研究人员的战略路线图，以加速学习并激发具身智能领域的新想法。其在线版本将持续更新。"}}
{"id": "2512.11750", "categories": ["eess.SY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.11750", "abs": "https://arxiv.org/abs/2512.11750", "authors": ["Ernesto Casablanca", "Oliver Schön", "Paolo Zuliani", "Sadegh Soudjani"], "title": "LUCID: Learning-Enabled Uncertainty-Aware Certification of Stochastic Dynamical Systems", "comment": "The manuscript has been accepted for publication in the main track of AAAI 2026", "summary": "Ensuring the safety of AI-enabled systems, particularly in high-stakes domains such as autonomous driving and healthcare, has become increasingly critical. Traditional formal verification tools fall short when faced with systems that embed both opaque, black-box AI components and complex stochastic dynamics. To address these challenges, we introduce LUCID (Learning-enabled Uncertainty-aware Certification of stochastIc Dynamical systems), a verification engine for certifying safety of black-box stochastic dynamical systems from a finite dataset of random state transitions. As such, LUCID is the first known tool capable of establishing quantified safety guarantees for such systems. Thanks to its modular architecture and extensive documentation, LUCID is designed for easy extensibility. LUCID employs a data-driven methodology rooted in control barrier certificates, which are learned directly from system transition data, to ensure formal safety guarantees. We use conditional mean embeddings to embed data into a reproducing kernel Hilbert space (RKHS), where an RKHS ambiguity set is constructed that can be inflated to robustify the result to out-of-distribution behavior. A key innovation within LUCID is its use of a finite Fourier kernel expansion to reformulate a semi-infinite non-convex optimization problem into a tractable linear program. The resulting spectral barrier allows us to leverage the fast Fourier transform to generate the relaxed problem efficiently, offering a scalable yet distributionally robust framework for verifying safety. LUCID thus offers a robust and efficient verification framework, able to handle the complexities of modern black-box systems while providing formal guarantees of safety. These unique capabilities are demonstrated on challenging benchmarks.", "AI": {"tldr": "LUCID是一个验证引擎，用于从有限数据集中认证黑盒随机动态系统的安全性，它利用数据驱动的控制障碍证书和创新的傅里叶核展开，提供可量化的安全保证。", "motivation": "在自动驾驶和医疗保健等高风险领域，确保AI系统安全至关重要。传统的形式验证工具无法处理包含不透明黑盒AI组件和复杂随机动力学的系统。", "method": "LUCID采用数据驱动的方法，基于从系统转换数据中学习的控制障碍证书。它使用条件均值嵌入将数据映射到再生核希尔伯特空间（RKHS），并构建一个RKHS模糊集以增强对分布外行为的鲁棒性。其关键创新在于使用有限傅里叶核展开，将半无限非凸优化问题转化为可处理的线性规划，并通过快速傅里叶变换高效生成松弛问题，实现可扩展且对分布鲁棒的安全验证。", "result": "LUCID是第一个能够从有限随机状态转换数据集中，为黑盒随机动态系统建立量化安全保证的已知工具。它提供了一个可扩展且对分布鲁棒的框架，能够进行稳健高效的验证，并在挑战性基准测试中展现了其独特能力。", "conclusion": "LUCID提供了一个强大而高效的验证框架，能够处理现代黑盒系统的复杂性，同时提供形式化的安全保证。"}}
{"id": "2512.11786", "categories": ["eess.SY", "cs.HC", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.11786", "abs": "https://arxiv.org/abs/2512.11786", "authors": ["Hannes Homburger", "Bastian Jäckl", "Stefan Wirtensohn", "Christian Stopp", "Maximilian T. Fischer", "Moritz Diehl", "Daniel A. Keim", "Johannes Reuter"], "title": "Toward a Decision Support System for Energy-Efficient Ferry Operation on Lake Constance based on Optimal Control", "comment": "6 pages, 8 figures", "summary": "The maritime sector is undergoing a disruptive technological change driven by three main factors: autonomy, decarbonization, and digital transformation. Addressing these factors necessitates a reassessment of inland vessel operations. This paper presents the design and development of a decision support system for ferry operations based on a shrinking-horizon optimal control framework. The problem formulation incorporates a mathematical model of the ferry's dynamics and environmental disturbances, specifically water currents and wind, which can significantly influence the dynamics. Real-world data and illustrative scenarios demonstrate the potential of the proposed system to effectively support ferry crews by providing real-time guidance. This enables enhanced operational efficiency while maintaining predefined maneuver durations. The findings suggest that optimal control applications hold substantial promise for advancing future ferry operations on inland waters. A video of the real-world ferry MS Insel Mainau operating on Lake Constance is available at: https://youtu.be/i1MjCdbEQyE", "AI": {"tldr": "本文提出一个基于收缩视界最优控制的决策支持系统，旨在为内陆渡轮操作提供实时指导，以提高效率并应对海事部门的技术变革。", "motivation": "海事部门正经历由自主化、脱碳和数字化转型驱动的颠覆性技术变革，这要求重新评估内陆船舶运营，尤其是渡轮操作。", "method": "设计并开发了一个基于收缩视界最优控制框架的渡轮操作决策支持系统。该系统整合了渡轮动力学数学模型以及水流和风等环境干扰因素。", "result": "通过真实世界数据和示例场景证明，该系统能有效支持渡轮船员，提供实时指导，从而提高运营效率并保持预定义的操作持续时间。", "conclusion": "研究结果表明，最优控制应用在推进未来内陆渡轮操作方面具有巨大潜力。"}}
{"id": "2512.11469", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11469", "abs": "https://arxiv.org/abs/2512.11469", "authors": ["Pranav Ramanathan", "Thomas Prellberg", "Matthew Lewis", "Prathamesh Dinesh Joshi", "Raj Abhijit Dandekar", "Rajat Dandekar", "Sreedath Panat"], "title": "Three methods, one problem: Classical and AI approaches to no-three-in-line", "comment": null, "summary": "The No-Three-In-Line problem asks for the maximum number of points that can be placed on an n by n grid with no three collinear, representing a famous problem in combinatorial geometry. While classical methods like Integer Linear Programming (ILP) guarantee optimal solutions, they face exponential scaling with grid size, and recent advances in machine learning offer promising alternatives for pattern-based approximation. This paper presents the first systematic comparison of classical optimization and AI approaches to this problem, evaluating their performance against traditional algorithms. We apply PatternBoost transformer learning and reinforcement learning (PPO) to this problem for the first time, comparing them against ILP. ILP achieves provably optimal solutions up to 19 by 19 grids, while PatternBoost matches optimal performance up to 14 by 14 grids with 96% test loss reduction. PPO achieves perfect solutions on 10 by 10 grids but fails at 11 by 11 grids, where constraint violations prevent valid configurations. These results demonstrate that classical optimization remains essential for exact solutions while AI methods offer competitive performance on smaller instances, with hybrid approaches presenting the most promising direction for scaling to larger problem sizes.", "AI": {"tldr": "本文首次系统比较了经典优化（ILP）和AI方法（PatternBoost, PPO）在“三点一线”问题上的表现，发现ILP提供精确解，而AI方法在较小规模问题上表现良好，混合方法有望扩展到更大规模。", "motivation": "“三点一线”问题是一个著名的组合几何问题，经典优化方法（如ILP）在解决该问题时面临计算复杂度随网格尺寸呈指数级增长的问题。机器学习的最新进展为基于模式的近似解提供了潜力，但缺乏对经典优化和AI方法之间性能的系统比较。", "method": "研究将PatternBoost（一种Transformer学习方法）和PPO（一种强化学习方法）首次应用于“三点一线”问题，并将其性能与传统的整数线性规划（ILP）算法进行比较。评估了这些方法在不同网格尺寸下的解决方案质量和效率。", "result": "ILP在19x19及以下网格上获得了可证明的最优解。PatternBoost在14x14及以下网格上达到了与最优解相同的性能，并实现了96%的测试损失降低。PPO在10x10网格上获得了完美解，但在11x11网格上因约束违反而失败。", "conclusion": "研究表明，对于“三点一线”问题，经典优化方法对于获得精确解仍然至关重要。AI方法在较小规模问题上表现出有竞争力的性能。未来的研究方向应着眼于结合两者的混合方法，以期能够扩展到更大规模的问题。"}}
{"id": "2512.11571", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.11571", "abs": "https://arxiv.org/abs/2512.11571", "authors": ["Andreu Matoses Gimenez", "Nils Wilde", "Chris Pek", "Javier Alonso-Mora"], "title": "Cross-Entropy Optimization of Physically Grounded Task and Motion Plans", "comment": "Preprint", "summary": "Autonomously performing tasks often requires robots to plan high-level discrete actions and continuous low-level motions to realize them. Previous TAMP algorithms have focused mainly on computational performance, completeness, or optimality by making the problem tractable through simplifications and abstractions. However, this comes at the cost of the resulting plans potentially failing to account for the dynamics or complex contacts necessary to reliably perform the task when object manipulation is required. Additionally, approaches that ignore effects of the low-level controllers may not obtain optimal or feasible plan realizations for the real system. We investigate the use of a GPU-parallelized physics simulator to compute realizations of plans with motion controllers, explicitly accounting for dynamics, and considering contacts with the environment. Using cross-entropy optimization, we sample the parameters of the controllers, or actions, to obtain low-cost solutions. Since our approach uses the same controllers as the real system, the robot can directly execute the computed plans. We demonstrate our approach for a set of tasks where the robot is able to exploit the environment's geometry to move an object. Website and code: https://andreumatoses.github.io/research/parallel-realization", "AI": {"tldr": "该研究提出了一种利用GPU并行物理模拟器和交叉熵优化来生成可靠机器人任务计划的方法，该方法明确考虑了动力学和接触，并使用与真实系统相同的控制器，从而实现了在复杂操作任务中的直接执行。", "motivation": "以往的TAMP算法通过简化和抽象来提高计算性能、完整性或最优性，但这可能导致生成的计划在需要物体操作时无法可靠地处理动力学或复杂接触。此外，忽略低级控制器影响的方法可能无法为真实系统获得最优或可行的计划实现。", "method": "研究利用GPU并行物理模拟器计算带有运动控制器的计划实现，明确考虑了动力学和环境接触。通过交叉熵优化(CEO)来采样控制器参数或动作，以获得低成本解决方案。由于使用与真实系统相同的控制器，机器人可以直接执行计算出的计划。", "result": "该方法使机器人能够直接执行计算出的计划。研究通过一系列任务进行了演示，在这些任务中，机器人能够利用环境几何形状来移动物体，从而验证了其有效性。", "conclusion": "通过结合GPU并行物理模拟和交叉熵优化，该方法能够为机器人生成考虑动力学和复杂接触的可靠、可直接执行的任务计划，解决了传统TAMP算法在复杂操作任务中遇到的挑战。"}}
{"id": "2512.11463", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11463", "abs": "https://arxiv.org/abs/2512.11463", "authors": ["Junghwan Lim", "Sungmin Lee", "Dongseok Kim", "Taehyun Kim", "Eunhwan Park", "Jeesoo Lee", "Jeongdoo Lee", "Junhyeok Lee", "Wai Ting Cheung", "Dahye Choi", "Minsu Ha", "Jaeheui Her", "Jaeyeon Huh", "Hanbin Jung", "Changjin Kang", "Beomgyu Kim", "Minjae Kim", "Taewhan Kim", "Youngrok Kim", "Hyukjin Kweon", "Haesol Lee", "Kungyu Lee", "Dongpin Oh", "Yeongjae Park", "Bokki Ryu", "Dongjoo Weon"], "title": "Motif-2-12.7B-Reasoning: A Practitioner's Guide to RL Training Recipes", "comment": null, "summary": "We introduce Motif-2-12.7B-Reasoning, a 12.7B parameter language model designed to bridge the gap between open-weight systems and proprietary frontier models in complex reasoning and long-context understanding. Addressing the common challenges of model collapse and training instability in reasoning adaptation, we propose a comprehensive, reproducible training recipe spanning system, data, and algorithmic optimizations. Our approach combines memory-efficient infrastructure for 64K-token contexts using hybrid parallelism and kernel-level optimizations with a two-stage Supervised Fine-Tuning (SFT) curriculum that mitigates distribution mismatch through verified, aligned synthetic data. Furthermore, we detail a robust Reinforcement Learning Fine-Tuning (RLFT) pipeline that stabilizes training via difficulty-aware data filtering and mixed-policy trajectory reuse. Empirical results demonstrate that Motif-2-12.7B-Reasoning achieves performance comparable to models with significantly larger parameter counts across mathematics, coding, and agentic benchmarks, offering the community a competitive open model and a practical blueprint for scaling reasoning capabilities under realistic compute constraints.", "AI": {"tldr": "本文介绍了Motif-2-12.7B-Reasoning，一个12.7B参数的语言模型，旨在弥合开源系统与专有前沿模型在复杂推理和长上下文理解方面的差距。通过系统、数据和算法优化，该模型在数学、编码和智能体基准测试中表现出与参数量更大的模型相当的性能，并提供了一个实用的推理能力扩展方案。", "motivation": "当前开源语言模型在复杂推理和长上下文理解方面与专有前沿模型存在差距。同时，在推理适应性训练中，模型崩溃和训练不稳定性是常见挑战，亟需一个综合、可复现的训练方案来解决这些问题。", "method": "研究提出了一套全面的、可复现的训练方案：\n1.  **系统优化**：采用混合并行和内核级优化的内存高效基础设施，支持64K token上下文。\n2.  **数据与算法优化**：设计了两阶段的监督微调（SFT）课程，通过验证和对齐的合成数据来缓解分布不匹配问题。\n3.  **强化学习微调（RLFT）**：构建了一个鲁棒的RLFT管道，通过难度感知数据过滤和混合策略轨迹重用来稳定训练。", "result": "经验结果表明，Motif-2-12.7B-Reasoning在数学、编码和智能体基准测试中，取得了与参数量显著更大的模型相当的性能。这为社区提供了一个具有竞争力的开源模型。", "conclusion": "Motif-2-12.7B-Reasoning模型成功弥合了开源系统与专有模型在推理能力上的差距，并为在实际计算约束下扩展推理能力提供了一个实用的蓝图。该模型及其训练方法对开源社区具有重要价值。"}}
{"id": "2512.11620", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.11620", "abs": "https://arxiv.org/abs/2512.11620", "authors": ["Kanisorn Sangchai", "Methasit Boonpun", "Withawin Kraipetchara", "Paulo Garcia"], "title": "Architecting Large Action Models for Human-in-the-Loop Intelligent Robots", "comment": null, "summary": "The realization of intelligent robots, operating autonomously and interacting with other intelligent agents, human or artificial, requires the integration of environment perception, reasoning, and action. Classic Artificial Intelligence techniques for this purpose, focusing on symbolic approaches, have long-ago hit the scalability wall on compute and memory costs. Advances in Large Language Models in the past decade (neural approaches) have resulted in unprecedented displays of capability, at the cost of control, explainability, and interpretability. Large Action Models aim at extending Large Language Models to encompass the full perception, reasoning, and action cycle; however, they typically require substantially more comprehensive training and suffer from the same deficiencies in reliability. Here, we show it is possible to build competent Large Action Models by composing off-the-shelf foundation models, and that their control, interpretability, and explainability can be effected by incorporating symbolic wrappers and associated verification on their outputs, achieving verifiable neuro-symbolic solutions for intelligent robots. Our experiments on a multi-modal robot demonstrate that Large Action Model intelligence does not require massive end-to-end training, but can be achieved by integrating efficient perception models with a logic-driven core. We find that driving action execution through the generation of Planning Domain Definition Language (PDDL) code enables a human-in-the-loop verification stage that effectively mitigates action hallucinations. These results can support practitioners in the design and development of robotic Large Action Models across novel industries, and shed light on the ongoing challenges that must be addressed to ensure safety in the field.", "AI": {"tldr": "本文提出一种可验证的神经符号解决方案，通过组合现成的基础模型和符号封装器，实现对智能机器人大型动作模型（LAM）的控制、可解释性和可靠性，无需大规模端到端训练，并能有效缓解动作幻觉。", "motivation": "传统符号AI在可扩展性上遇到瓶颈，而大型语言模型（LLM）及其扩展型大型动作模型（LAM）虽然能力强大，但在控制、可解释性和可靠性方面存在不足，且通常需要大量训练。研究旨在为智能机器人实现完整的感知、推理和行动循环，同时解决这些挑战。", "method": "通过组合现成的基础模型来构建大型动作模型。通过引入符号封装器并对其输出进行验证，以实现对LAM的控制、可解释性和可靠性。通过生成规划领域定义语言（PDDL）代码来驱动动作执行，从而实现人机回路验证。", "result": "实验证明，智能大型动作模型无需进行大规模端到端训练，通过整合高效感知模型和逻辑驱动核心即可实现。通过生成PDDL代码驱动动作执行，可以实现人机回路验证阶段，有效缓解动作幻觉。", "conclusion": "这些结果为新行业中机器人大型动作模型的设计和开发提供了支持，并为确保该领域安全所需解决的持续挑战提供了启示。该方法提供了一种无需大量训练即可构建安全、有能力的机器人LAM的途径。"}}
{"id": "2512.11141", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11141", "abs": "https://arxiv.org/abs/2512.11141", "authors": ["Yiwei Lyu", "Chenhui Zhao", "Soumyanil Banerjee", "Shixuan Liu", "Akshay Rao", "Akhil Kondepudi", "Honglak Lee", "Todd C. Hollon"], "title": "Learning complete and explainable visual representations from itemized text supervision", "comment": null, "summary": "Training vision models with language supervision enables general and transferable representations. However, many visual domains, especially non-object-centric domains such as medical imaging and remote sensing, contain itemized text annotations: multiple text items describing distinct and semantically independent findings within a single image. Such supervision differs from standard multi-caption supervision, where captions are redundant or highly overlapping. Here, we introduce ItemizedCLIP, a framework for learning complete and explainable visual representations from itemized text supervision. ItemizedCLIP employs a cross-attention module to produce text item-conditioned visual embeddings and a set of tailored objectives that jointly enforce item independence (distinct regions for distinct items) and representation completeness (coverage of all items). Across four domains with naturally itemized text supervision (brain MRI, head CT, chest CT, remote sensing) and one additional synthetically itemized dataset, ItemizedCLIP achieves substantial improvements in zero-shot performance and fine-grained interpretability over baselines. The resulting ItemizedCLIP representations are semantically grounded, item-differentiable, complete, and visually interpretable. Our code is available at https://github.com/MLNeurosurg/ItemizedCLIP.", "AI": {"tldr": "ItemizedCLIP是一个框架，旨在利用项目化文本监督训练视觉模型，通过交叉注意力模块和定制目标，在医学影像和遥感等非以对象为中心的领域中学习完整且可解释的视觉表示，显著提升了零样本性能和细粒度可解释性。", "motivation": "现有的语言监督（如多描述）通常假设描述是冗余或高度重叠的。然而，在医学影像和遥感等非以对象为中心的视觉领域中，存在项目化文本注释，即单个图像包含描述不同且语义独立的发现的多个文本项。标准方法难以处理这种监督形式，因此需要一种新的框架来学习完整且可解释的视觉表示。", "method": "ItemizedCLIP框架通过以下方式实现目标：1. 采用交叉注意力模块，生成文本项条件下的视觉嵌入。2. 引入一套定制的损失函数，共同强制执行“项目独立性”（不同项目对应不同区域）和“表示完整性”（覆盖所有项目）。", "result": "在脑部MRI、头部CT、胸部CT、遥感等四个具有自然项目化文本监督的领域，以及一个额外的人工合成项目化数据集上，ItemizedCLIP在零样本性能和细粒度可解释性方面均显著优于基线模型。", "conclusion": "ItemizedCLIP生成的表示是语义扎根的、项目可区分的、完整的且视觉可解释的，为处理项目化文本监督的视觉模型训练提供了一种有效且创新的解决方案。"}}
{"id": "2512.11167", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11167", "abs": "https://arxiv.org/abs/2512.11167", "authors": ["Anatole Jacquin de Margerie", "Alexis Roger", "Irina Rish"], "title": "Image Tiling for High-Resolution Reasoning: Balancing Local Detail with Global Context", "comment": "Accepted in AAAI 2025 Workshop on Reproducible AI", "summary": "Reproducibility remains a cornerstone of scientific progress, yet complex multimodal models often lack transparent implementation details and accessible training infrastructure. In this work, we present a detailed reproduction and critical analysis of the Monkey Vision-Language Model (VLM) (Li et al. 2023b) published in CVPR24, a recent approach to high-resolution image understanding via image tiling. The original paper proposed splitting large images into tiles to recover fine-grained visual details while maintaining computational efficiency. Our study replicates this strategy using open checkpoints and reimplements the training pipeline. We confirm the key finding of the original Monkey VLM work, namely that tiling effectively recovers local details. We then extend this work further, by investigating the effect of the inclusion of the global context, which provide practical insights for future high-resolution multimodal modeling. However, we also report deviations in the results, with the magnitude of these effects depending heavily on task type and tile granularity.", "AI": {"tldr": "本文对CVPR24的Monkey VLM进行了复现和批判性分析，确认了图像分块能有效恢复局部细节，但发现结果存在偏差，并进一步探讨了全局上下文的影响。", "motivation": "复杂的跨模态模型缺乏透明的实现细节和可访问的训练基础设施，阻碍了科学的可复现性。特别是，Monkey VLM提出通过图像分块实现高分辨率图像理解，需要详细的复现和批判性分析。", "method": "使用开放检查点复现了Monkey VLM的图像分块策略，并重新实现了训练流程。在此基础上，进一步研究了全局上下文对高分辨率跨模态建模的影响。", "result": "确认了原始Monkey VLM的关键发现，即分块能有效恢复局部细节。然而，也报告了结果上的偏差，其程度严重依赖于任务类型和分块粒度。研究全局上下文提供了未来高分辨率跨模态建模的实用见解。", "conclusion": "图像分块是恢复高分辨率跨模态模型局部细节的有效策略，但其效果受任务类型和分块粒度影响。未来高分辨率跨模态建模应考虑纳入全局上下文，以获得更全面的理解和性能。"}}
{"id": "2512.11258", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11258", "abs": "https://arxiv.org/abs/2512.11258", "authors": ["Di Wu", "Ruiyu Fang", "Liting Jiang", "Shuangyong Song", "Xiaomeng Huang", "Shiquan Wang", "Zhongqiu Li", "Lingling Shi", "Mengjiao Bao", "Yongxiang Li", "Hao Huang"], "title": "Multi-Intent Spoken Language Understanding: Methods, Trends, and Challenges", "comment": null, "summary": "Multi-intent spoken language understanding (SLU) involves two tasks: multiple intent detection and slot filling, which jointly handle utterances containing more than one intent. Owing to this characteristic, which closely reflects real-world applications, the task has attracted increasing research attention, and substantial progress has been achieved. However, there remains a lack of a comprehensive and systematic review of existing studies on multi-intent SLU. To this end, this paper presents a survey of recent advances in multi-intent SLU. We provide an in-depth overview of previous research from two perspectives: decoding paradigms and modeling approaches. On this basis, we further compare the performance of representative models and analyze their strengths and limitations. Finally, we discuss the current challenges and outline promising directions for future research. We hope this survey will offer valuable insights and serve as a useful reference for advancing research in multi-intent SLU.", "AI": {"tldr": "这篇综述论文系统地回顾了多意图口语理解（SLU）的最新进展，分析了解码范式和建模方法，并指出了未来的研究方向。", "motivation": "多意图SLU在现实世界应用中具有重要意义，但目前缺乏对现有研究的全面系统综述，这促使作者对该领域进行深入分析。", "method": "本文从解码范式和建模方法两个角度，对多意图SLU的现有研究进行了深入概述。在此基础上，比较了代表性模型的性能，并分析了它们的优缺点。", "result": "论文提供了多意图SLU研究的深入概述，比较并分析了代表性模型的性能、优势和局限性。同时，讨论了当前面临的挑战。", "conclusion": "总结了多意图SLU的当前挑战，并为未来的研究提出了有前景的方向，旨在为该领域的进步提供有价值的见解和参考。"}}
{"id": "2512.11261", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.11261", "abs": "https://arxiv.org/abs/2512.11261", "authors": ["Yun-Chung Liu", "Rui Yang", "Jonathan Chong Kai Liew", "Ziran Yin", "Henry Foote", "Christopher J. Lindsell", "Chuan Hong"], "title": "Leveraging LLMs for Title and Abstract Screening for Systematic Review: A Cost-Effective Dynamic Few-Shot Learning Approach", "comment": "22 pages, 3 figures", "summary": "Systematic reviews are a key component of evidence-based medicine, playing a critical role in synthesizing existing research evidence and guiding clinical decisions. However, with the rapid growth of research publications, conducting systematic reviews has become increasingly burdensome, with title and abstract screening being one of the most time-consuming and resource-intensive steps. To mitigate this issue, we designed a two-stage dynamic few-shot learning (DFSL) approach aimed at improving the efficiency and performance of large language models (LLMs) in the title and abstract screening task. Specifically, this approach first uses a low-cost LLM for initial screening, then re-evaluates low-confidence instances using a high-performance LLM, thereby enhancing screening performance while controlling computational costs. We evaluated this approach across 10 systematic reviews, and the results demonstrate its strong generalizability and cost-effectiveness, with potential to reduce manual screening burden and accelerate the systematic review process in practical applications.", "AI": {"tldr": "本研究提出了一种两阶段动态小样本学习（DFSL）方法，旨在提高大型语言模型（LLMs）在系统评价标题和摘要筛选任务中的效率和性能，同时控制计算成本。", "motivation": "系统评价是循证医学的关键组成部分，但随着研究出版物的快速增长，其标题和摘要筛选变得越来越耗时和资源密集，急需提高效率。", "method": "该方法采用两阶段动态小样本学习（DFSL）。第一阶段使用低成本LLM进行初步筛选；第二阶段，对置信度低的实例使用高性能LLM进行重新评估，以提高筛选性能并控制计算成本。", "result": "该方法在10个系统评价中进行了评估，结果表明其具有强大的泛化性和成本效益，有望减轻人工筛选负担并加速系统评价过程。", "conclusion": "所提出的两阶段DFSL方法能有效提升LLMs在系统评价筛选任务中的效率和性能，为实际应用中加速系统评价流程提供了可行的解决方案。"}}
{"id": "2512.11474", "categories": ["cs.AI", "cs.CY", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.11474", "abs": "https://arxiv.org/abs/2512.11474", "authors": ["Kris A. G. Wyckhuys"], "title": "General-purpose AI models can generate actionable knowledge on agroecological crop protection", "comment": "33 pages, 3 figures, 3 tables, 1 supplementary table", "summary": "Generative artificial intelligence (AI) offers potential for democratizing scientific knowledge and converting this to clear, actionable information, yet its application in agri-food science remains unexplored. Here, we verify the scientific knowledge on agroecological crop protection that is generated by either web-grounded or non-grounded large language models (LLMs), i.e., DeepSeek versus the free-tier version of ChatGPT. For nine globally limiting pests, weeds, and plant diseases, we assessed the factual accuracy, data consistency, and breadth of knowledge or data completeness of each LLM. Overall, DeepSeek consistently screened a 4.8-49.7-fold larger literature corpus and reported 1.6-2.4-fold more biological control agents or management solutions than ChatGPT. As a result, DeepSeek reported 21.6% higher efficacy estimates, exhibited greater laboratory-to-field data consistency, and showed more realistic effects of pest identity and management tactics. However, both models hallucinated, i.e., fabricated fictitious agents or references, reported on implausible ecological interactions or outcomes, confused old and new scientific nomenclatures, and omitted data on key agents or solutions. Despite these shortcomings, both LLMs correctly reported low-resolution efficacy trends. Overall, when paired with rigorous human oversight, LLMs may pose a powerful tool to support farm-level decision-making and unleash scientific creativity.", "AI": {"tldr": "本研究评估了生成式AI（大型语言模型，LLMs）在农业生态作物保护领域的应用潜力，比较了DeepSeek和ChatGPT在事实准确性、数据一致性和知识广度方面的表现。DeepSeek整体表现更优，但两者均存在幻觉问题。结论是，在严格的人工监督下，LLMs可成为支持农业决策的强大工具。", "motivation": "生成式AI有望使科学知识民主化并转化为可操作的信息，但其在农业食品科学领域的应用尚未被探索。", "method": "本研究比较了基于网络的（DeepSeek）和非基于网络的（免费版ChatGPT）大型语言模型。针对全球九种限制性病虫害和杂草，评估了每个LLM在农业生态作物保护知识方面的事实准确性、数据一致性和知识广度（数据完整性）。", "result": "DeepSeek检索的文献语料库比ChatGPT大4.8-49.7倍，报告的生物防治剂或管理方案多1.6-2.4倍。因此，DeepSeek报告的功效估算高21.6%，实验室到田间的数据一致性更高，并且显示出更真实的病虫害特性和管理策略效果。然而，两种模型都存在幻觉（捏造虚构的代理或参考文献、报告不合逻辑的生态相互作用或结果、混淆新旧科学命名、遗漏关键代理或解决方案的数据）。尽管有这些缺点，两种LLM都正确报告了低分辨率的功效趋势。", "conclusion": "尽管存在不足，但当与严格的人工监督结合时，大型语言模型可能成为支持农场层面决策和激发科学创造力的强大工具。"}}
{"id": "2512.11609", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.11609", "abs": "https://arxiv.org/abs/2512.11609", "authors": ["Tingyu Yuan", "Biaoliang Guan", "Wen Ye", "Ziyan Tian", "Yi Yang", "Weijie Zhou", "Yan Huang", "Peng Wang", "Chaoyang Zhao", "Jinqiao Wang"], "title": "UniBYD: A Unified Framework for Learning Robotic Manipulation Across Embodiments Beyond Imitation of Human Demonstrations", "comment": null, "summary": "In embodied intelligence, the embodiment gap between robotic and human hands brings significant challenges for learning from human demonstrations. Although some studies have attempted to bridge this gap using reinforcement learning, they remain confined to merely reproducing human manipulation, resulting in limited task performance. In this paper, we propose UniBYD, a unified framework that uses a dynamic reinforcement learning algorithm to discover manipulation policies aligned with the robot's physical characteristics. To enable consistent modeling across diverse robotic hand morphologies, UniBYD incorporates a unified morphological representation (UMR). Building on UMR, we design a dynamic PPO with an annealed reward schedule, enabling reinforcement learning to transition from imitation of human demonstrations to explore policies adapted to diverse robotic morphologies better, thereby going beyond mere imitation of human hands. To address the frequent failures of learning human priors in the early training stage, we design a hybrid Markov-based shadow engine that enables reinforcement learning to imitate human manipulations in a fine-grained manner. To evaluate UniBYD comprehensively, we propose UniManip, the first benchmark encompassing robotic manipulation tasks spanning multiple hand morphologies. Experiments demonstrate a 67.90% improvement in success rate over the current state-of-the-art. Upon acceptance of the paper, we will release our code and benchmark at https://github.com/zhanheng-creator/UniBYD.", "AI": {"tldr": "UniBYD是一个统一框架，通过动态强化学习和统一形态表示（UMR）来弥合机器人与人类手之间的具身差距，使机器人能够学习适应其自身物理特性的操纵策略，并在多个手部形态的基准测试中显著超越现有技术。", "motivation": "机器人与人类手之间的具身差距给从人类演示中学习带来了巨大挑战。现有方法仅限于简单地复现人类操作，导致任务性能有限，未能发现与机器人自身物理特性相符的操纵策略。", "method": "本文提出了UniBYD，一个统一框架：1) 采用动态强化学习算法发现与机器人物理特性对齐的操纵策略。2) 引入统一形态表示（UMR），实现跨多样机器人手部形态的一致建模。3) 设计了带有退火奖励机制的动态PPO，使强化学习能够从模仿人类演示过渡到探索适应机器人形态的策略。4) 设计了混合马尔可夫基的影子引擎，以解决早期训练阶段学习人类先验时常出现的失败，并实现对人类操纵的细粒度模仿。5) 提出了UniManip，一个涵盖多种手部形态机器人操纵任务的综合基准。", "result": "实验结果表明，UniBYD在成功率方面比现有最先进技术提高了67.90%。", "conclusion": "UniBYD框架成功弥合了机器人与人类手之间的具身差距，通过动态强化学习和形态适应性策略，使机器人能够超越单纯模仿，学习到更高效、更适应自身特性的操纵技能，显著提升了任务性能。"}}
{"id": "2512.11551", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.11551", "abs": "https://arxiv.org/abs/2512.11551", "authors": ["Jörg Gamerdinger", "Sven Teufel", "Simon Roller", "Oliver Bringmann"], "title": "CarlaNCAP: A Framework for Quantifying the Safety of Vulnerable Road Users in Infrastructure-Assisted Collective Perception Using EuroNCAP Scenarios", "comment": null, "summary": "The growing number of road users has significantly increased the risk of accidents in recent years. Vulnerable Road Users (VRUs) are particularly at risk, especially in urban environments where they are often occluded by parked vehicles or buildings. Autonomous Driving (AD) and Collective Perception (CP) are promising solutions to mitigate these risks. In particular, infrastructure-assisted CP, where sensor units are mounted on infrastructure elements such as traffic lights or lamp posts, can help overcome perceptual limitations by providing enhanced points of view, which significantly reduces occlusions. To encourage decision makers to adopt this technology, comprehensive studies and datasets demonstrating safety improvements for VRUs are essential. In this paper, we propose a framework for evaluating the safety improvement by infrastructure-based CP specifically targeted at VRUs including a dataset with safety-critical EuroNCAP scenarios (CarlaNCAP) with 11k frames. Using this dataset, we conduct an in-depth simulation study and demonstrate that infrastructure-assisted CP can significantly reduce accident rates in safety-critical scenarios, achieving up to 100% accident avoidance compared to a vehicle equipped with sensors with only 33%. Code is available at https://github.com/ekut-es/carla_ncap", "AI": {"tldr": "本文提出一个评估框架和数据集（CarlaNCAP），用于量化基础设施辅助集体感知（CP）对弱势道路使用者（VRU）安全的提升，模拟结果显示其能显著降低事故率。", "motivation": "近年来道路使用者数量增加导致事故风险上升，弱势道路使用者（VRU）尤其面临被遮挡的风险。自动驾驶（AD）和集体感知（CP）是潜在解决方案，其中基础设施辅助CP能通过提供更佳视角显著减少遮挡。为鼓励决策者采纳此技术，需要有力的研究和数据集来证明其对VRU安全性的改进。", "method": "本文提出一个评估基础设施辅助CP对VRU安全改进的框架，并构建了一个包含11k帧安全关键EuroNCAP场景的数据集（CarlaNCAP）。利用该数据集，进行了一项深入的模拟研究。", "result": "模拟研究表明，基础设施辅助CP能显著降低安全关键场景中的事故率，与仅配备车载传感器的车辆（33%事故避免率）相比，基础设施辅助CP实现了高达100%的事故避免。", "conclusion": "基础设施辅助集体感知技术能有效提升弱势道路使用者在安全关键场景中的安全性，通过提供增强的感知能力，显著降低事故风险，从而证明了其在未来道路安全中的巨大潜力。"}}
{"id": "2512.11277", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.11277", "abs": "https://arxiv.org/abs/2512.11277", "authors": ["Mrinal Rawat", "Arkajyoti Chakraborty", "Neha Gupta", "Roberto Pieraccini"], "title": "When Actions Teach You to Think: Reasoning-Action Synergy via Reinforcement Learning in Conversational Agents", "comment": null, "summary": "Supervised fine-tuning (SFT) has emerged as one of the most effective ways to improve the performance of large language models (LLMs) in downstream tasks. However, SFT can have difficulty generalizing when the underlying data distribution changes, even when the new data does not fall completely outside the training domain. Recent reasoning-focused models such as o1 and R1 have demonstrated consistent gains over their non-reasoning counterparts, highlighting the importance of reasoning for improved generalization and reliability. However, collecting high-quality reasoning traces for SFT remains challenging -- annotations are costly, subjective, and difficult to scale. To address this limitation, we leverage Reinforcement Learning (RL) to enable models to learn reasoning strategies directly from task outcomes. We propose a pipeline in which LLMs generate reasoning steps that guide both the invocation of tools (e.g., function calls) and the final answer generation for conversational agents. Our method employs Group Relative Policy Optimization (GRPO) with rewards designed around tool accuracy and answer correctness, allowing the model to iteratively refine its reasoning and actions. Experimental results demonstrate that our approach improves both the quality of reasoning and the precision of tool invocations, achieving a 1.5% relative improvement over the SFT model (trained without explicit thinking) and a 40% gain compared to the base of the vanilla Qwen3-1.7B model. These findings demonstrate the promise of unifying reasoning and action learning through RL to build more capable and generalizable conversational agents.", "AI": {"tldr": "本文提出了一种利用强化学习（RL）使大型语言模型（LLMs）直接从任务结果中学习推理策略的方法，以提高对话代理的泛化能力和工具调用精度，优于传统的监督微调（SFT）方法。", "motivation": "监督微调（SFT）在数据分布变化时泛化能力不足，而高质量的推理轨迹标注成本高昂、主观且难以扩展。尽管推理模型（如o1和R1）已显示出更好的泛化性和可靠性，但获取其训练数据仍是挑战。", "method": "我们提出一个RL流程，让LLMs生成推理步骤来指导工具调用和最终答案生成。该方法采用群组相对策略优化（GRPO），奖励设计围绕工具准确性和答案正确性，使模型能够迭代地完善其推理和行动。", "result": "实验结果表明，我们的方法显著提升了推理质量和工具调用的精度。与未进行显式思考训练的SFT模型相比，相对提升了1.5%；与基础的Qwen3-1.7B模型相比，性能提升了40%。", "conclusion": "这些发现证明了通过强化学习统一推理和行动学习的潜力，能够构建更强大、更具泛化能力的对话代理。"}}
{"id": "2512.11192", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.11192", "abs": "https://arxiv.org/abs/2512.11192", "authors": ["Luca Foppiano", "Sotaro Takeshita", "Pedro Ortiz Suarez", "Ekaterina Borisova", "Raia Abu Ahmad", "Malte Ostendorff", "Fabio Barth", "Julian Moreno-Schneider", "Georg Rehm"], "title": "SciLaD: A Large-Scale, Transparent, Reproducible Dataset for Natural Scientific Language Processing", "comment": "12 pages, 2 figures, 3 tables", "summary": "SciLaD is a novel, large-scale dataset of scientific language constructed entirely using open-source frameworks and publicly available data sources. It comprises a curated English split containing over 10 million scientific publications and a multilingual, unfiltered TEI XML split including more than 35 million publications. We also publish the extensible pipeline for generating SciLaD. The dataset construction and processing workflow demonstrates how open-source tools can enable large-scale, scientific data curation while maintaining high data quality. Finally, we pre-train a RoBERTa model on our dataset and evaluate it across a comprehensive set of benchmarks, achieving performance comparable to other scientific language models of similar size, validating the quality and utility of SciLaD. We publish the dataset and evaluation pipeline to promote reproducibility, transparency, and further research in natural scientific language processing and understanding including scholarly document processing.", "AI": {"tldr": "SciLaD是一个大规模科学语言数据集，使用开源框架和公开数据构建，包含英文和多语言出版物。该研究发布了数据集、生成管道和基于其预训练的RoBERTa模型，验证了数据集的质量和实用性。", "motivation": "推动科学自然语言处理和理解（包括学术文档处理）的研究，通过提供高质量、大规模的开放科学语言数据集来促进可复现性、透明度和进一步研究。", "method": "使用开源框架和公开数据源构建SciLaD数据集，包含一个超过1000万篇英文出版物的精选子集和一个超过3500万篇出版物的多语言TEI XML子集。研究者发布了可扩展的生成管道，并在此数据集上预训练了一个RoBERTa模型，然后通过全面的基准测试对其进行评估。", "result": "成功构建了大规模的SciLaD数据集，展示了开源工具在保持高质量的同时进行大规模科学数据整理的能力。预训练的RoBERTa模型在综合基准测试中表现出与同等规模其他科学语言模型相当的性能，验证了SciLaD的质量和实用性。", "conclusion": "SciLaD是一个高质量、大规模的科学语言数据集，通过开放源代码方法构建，并发布了其生成管道和预训练模型。该数据集有望促进科学自然语言处理和理解领域的研究，提高可复现性和透明度。"}}
{"id": "2512.11506", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11506", "abs": "https://arxiv.org/abs/2512.11506", "authors": ["Georgios Kaoukis", "Ioannis Aris Koufopoulos", "Psaroudaki Eleni", "Danae Pla Karidi", "Evaggelia Pitoura", "George Papastefanatos", "Panayiotis Tsaparas"], "title": "EmeraldMind: A Knowledge Graph-Augmented Framework for Greenwashing Detection", "comment": null, "summary": "As AI and web agents become pervasive in decision-making, it is critical to design intelligent systems that not only support sustainability efforts but also guard against misinformation. Greenwashing, i.e., misleading corporate sustainability claims, poses a major challenge to environmental progress. To address this challenge, we introduce EmeraldMind, a fact-centric framework integrating a domain-specific knowledge graph with retrieval-augmented generation to automate greenwashing detection. EmeraldMind builds the EmeraldGraph from diverse corporate ESG (environmental, social, and governance) reports, surfacing verifiable evidence, often missing in generic knowledge bases, and supporting large language models in claim assessment. The framework delivers justification-centric classifications, presenting transparent, evidence-backed verdicts and abstaining responsibly when claims cannot be verified. Experiments on a new greenwashing claims dataset demonstrate that EmeraldMind achieves competitive accuracy, greater coverage, and superior explanation quality compared to generic LLMs, without the need for fine-tuning or retraining.", "AI": {"tldr": "EmeraldMind是一个基于知识图谱和检索增强生成（RAG）的框架，旨在自动化检测企业可持续发展声明中的“漂绿”行为，提供透明且有证据支持的分类。", "motivation": "随着人工智能和网络代理在决策中日益普及，设计能够支持可持续发展并防范虚假信息的智能系统至关重要。漂绿（即误导性的企业可持续发展声明）对环境保护构成重大挑战。", "method": "本文提出了EmeraldMind框架，它将一个领域特定知识图谱（EmeraldGraph，从企业ESG报告构建，以获取可验证的证据）与检索增强生成（RAG）相结合，以实现漂绿检测自动化。该框架提供以理由为中心的分类，呈现透明、有证据支持的判断，并在声明无法验证时负责任地弃权。", "result": "在新的漂绿声明数据集上的实验表明，与通用大型语言模型相比，EmeraldMind在无需微调或重新训练的情况下，实现了具有竞争力的准确性、更广的覆盖范围和卓越的解释质量。", "conclusion": "EmeraldMind框架通过集成领域知识图谱和RAG，有效解决了漂绿检测的挑战，提供了透明且有证据支持的判断，并能在信息不足时负责任地弃权，其性能优于通用大型语言模型。"}}
{"id": "2512.11746", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.11746", "abs": "https://arxiv.org/abs/2512.11746", "authors": ["Hana Kopecka", "Jose Such"], "title": "The Influence of Human-like Appearance on Expected Robot Explanations", "comment": null, "summary": "A robot's appearance is a known factor influencing user's mental model and human-robot interaction, that has not been studied in the context of its influence in expected robot explanations. In this study, we investigate whether and to what extent the human-like appearance of robots elicits anthropomorphism, which is conceptualised as an attribution of mental capacities, and how the level of anthropomorphism is revealed in explanations that people expect to receive. We designed a between-subject study comprising conditions with visual stimuli of three domestic service robots with varying human-like appearance, and we prompted respondents to provide explanations they would expect to receive from the robot for the same robot actions. We found that most explanations were anthropomorphic across all conditions. However, there is a positive correlation between the anthropomorphic explanations and human-like appearance. We also report on more nuanced trends observed in non-anthropomorphic explanations and trends in robot descriptions.", "AI": {"tldr": "本研究发现机器人的类人外观会诱导拟人化，并影响用户对机器人解释的期望，类人外观越强，用户期望的解释越拟人化。", "motivation": "机器人的外观已知会影响用户的心理模型和人机交互，但其对用户期望机器人解释的影响尚未被研究。本研究旨在探讨机器人的类人外观是否以及在多大程度上引发拟人化（即心理能力的归因），以及这种拟人化程度如何体现在人们期望机器人提供的解释中。", "method": "采用一项组间实验设计，包含三种不同类人外观的家用服务机器人视觉刺激条件。研究提示受访者为相同的机器人动作提供他们期望从机器人那里获得的解释。", "result": "研究发现，在所有条件下，大多数解释都是拟人化的。然而，拟人化解释与机器人的类人外观之间存在正相关关系。研究还报告了在非拟人化解释和机器人描述中观察到的更细微的趋势。", "conclusion": "机器人的类人外观会引发拟人化，并且与用户期望的拟人化解释呈正相关。这意味着机器人外观设计对用户对机器人行为解释的期望有显著影响。"}}
{"id": "2512.11186", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11186", "abs": "https://arxiv.org/abs/2512.11186", "authors": ["Qi Yang", "Geert Van Der Auwera", "Zhu Li"], "title": "Lightweight 3D Gaussian Splatting Compression via Video Codec", "comment": "Accepted by DCC2026 Oral", "summary": "Current video-based GS compression methods rely on using Parallel Linear Assignment Sorting (PLAS) to convert 3D GS into smooth 2D maps, which are computationally expensive and time-consuming, limiting the application of GS on lightweight devices. In this paper, we propose a Lightweight 3D Gaussian Splatting (GS) Compression method based on Video codec (LGSCV). First, a two-stage Morton scan is proposed to generate blockwise 2D maps that are friendly for canonical video codecs in which the coding units (CU) are square blocks. A 3D Morton scan is used to permute GS primitives, followed by a 2D Morton scan to map the ordered GS primitives to 2D maps in a blockwise style. However, although the blockwise 2D maps report close performance to the PLAS map in high-bitrate regions, they show a quality collapse at medium-to-low bitrates. Therefore, a principal component analysis (PCA) is used to reduce the dimensionality of spherical harmonics (SH), and a MiniPLAS, which is flexible and fast, is designed to permute the primitives within certain block sizes. Incorporating SH PCA and MiniPLAS leads to a significant gain in rate-distortion (RD) performance, especially at medium and low bitrates. MiniPLAS can also guide the setting of the codec CU size configuration and significantly reduce encoding time. Experimental results on the MPEG dataset demonstrate that the proposed LGSCV achieves over 20% RD gain compared with state-of-the-art methods, while reducing 2D map generation time to approximately 1 second and cutting encoding time by 50%. The code is available at https://github.com/Qi-Yangsjtu/LGSCV .", "AI": {"tldr": "本文提出了一种轻量级3D高斯泼溅(GS)视频编码压缩方法(LGSCV)，通过两阶段Morton扫描、SH PCA和MiniPLAS，显著提升了压缩效率，降低了计算成本和编码时间，特别适用于轻量级设备。", "motivation": "现有的基于视频的GS压缩方法依赖于并行线性分配排序(PLAS)将3D GS转换为平滑的2D映射，但这种方法计算成本高且耗时，限制了GS在轻量级设备上的应用。", "method": "本文提出了LGSCV方法。首先，引入两阶段Morton扫描生成块状2D映射，包括3D Morton扫描重排GS图元，再通过2D Morton扫描将有序图元映射到2D块状映射。为解决中低码率下的质量下降问题，使用主成分分析(PCA)降低球谐函数(SH)的维度，并设计了灵活快速的MiniPLAS来重排特定块大小内的图元。MiniPLAS还能指导编解码器CU大小配置，并显著减少编码时间。", "result": "实验结果表明，LGSCV相比现有最先进方法实现了超过20%的率失真(RD)增益，同时将2D映射生成时间缩短至约1秒，并将编码时间减少了50%。", "conclusion": "LGSCV通过创新的两阶段Morton扫描、SH PCA和MiniPLAS，有效解决了现有GS压缩方法的计算开销问题，显著提高了压缩性能和效率，尤其在中低码率下表现优异，使其更适用于轻量级设备。"}}
{"id": "2512.11199", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.11199", "abs": "https://arxiv.org/abs/2512.11199", "authors": ["Tri Le", "Khang Nguyen", "Baoru Huang", "Tung D. Ta", "Anh Nguyen"], "title": "CADKnitter: Compositional CAD Generation from Text and Geometry Guidance", "comment": null, "summary": "Crafting computer-aided design (CAD) models has long been a painstaking and time-intensive task, demanding both precision and expertise from designers. With the emergence of 3D generation, this task has undergone a transformative impact, shifting not only from visual fidelity to functional utility but also enabling editable CAD designs. Prior works have achieved early success in single-part CAD generation, which is not well-suited for real-world applications, as multiple parts need to be assembled under semantic and geometric constraints. In this paper, we propose CADKnitter, a compositional CAD generation framework with a geometry-guided diffusion sampling strategy. CADKnitter is able to generate a complementary CAD part that follows both the geometric constraints of the given CAD model and the semantic constraints of the desired design text prompt. We also curate a dataset, so-called KnitCAD, containing over 310,000 samples of CAD models, along with textual prompts and assembly metadata that provide semantic and geometric constraints. Intensive experiments demonstrate that our proposed method outperforms other state-of-the-art baselines by a clear margin.", "AI": {"tldr": "本文提出CADKnitter，一个组合式CAD生成框架，利用几何引导扩散采样策略，能够根据给定CAD模型的几何约束和文本提示的语义约束生成互补的CAD零件。同时，还构建了一个包含31万多个样本的大型数据集KnitCAD。", "motivation": "传统的CAD模型创建耗时且需要专业知识。现有的3D生成技术主要集中于单一零件的CAD生成，不适用于需要多零件组装并遵循语义和几何约束的实际应用场景。", "method": "本文提出CADKnitter，一个组合式CAD生成框架，核心是几何引导的扩散采样策略。该方法能够生成与现有CAD模型几何匹配且符合文本提示语义的互补CAD零件。此外，作者还整理并发布了一个名为KnitCAD的数据集，包含超过31万个CAD模型样本，以及相应的文本提示和装配元数据，提供语义和几何约束。", "result": "CADKnitter能够生成遵循给定CAD模型几何约束和所需设计文本提示语义约束的互补CAD零件。通过大量实验证明，所提出的方法在性能上明显优于其他最先进的基线方法。", "conclusion": "CADKnitter成功地解决了多零件CAD模型的组合式生成问题，能够高效地根据几何和语义约束创建互补零件，并通过新颖的框架和大规模数据集推动了该领域的发展。"}}
{"id": "2512.11280", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.11280", "abs": "https://arxiv.org/abs/2512.11280", "authors": ["Kuan-Wei Lu", "Ding-Yong Hong", "Pangfeng Liu"], "title": "AdaSD: Adaptive Speculative Decoding for Efficient Language Model Inference", "comment": null, "summary": "Large language models (LLMs) have achieved remarkable performance across a wide range of tasks, but their increasing parameter sizes significantly slow down inference. Speculative decoding mitigates this issue by leveraging a smaller draft model to predict candidate tokens, which are then verified by a larger target model. However, existing approaches often require additional training, extensive hyperparameter tuning, or prior analysis of models and tasks before deployment. In this paper, we propose Adaptive Speculative Decoding (AdaSD), a hyperparameter-free decoding scheme that dynamically adjusts generation length and acceptance criteria during inference. AdaSD introduces two adaptive thresholds: one to determine when to stop candidate token generation and another to decide token acceptance, both updated in real time based on token entropy and Jensen-Shannon distance. This approach eliminates the need for pre-analysis or fine-tuning and is compatible with off-the-shelf models. Experiments on benchmark datasets demonstrate that AdaSD achieves up to 49\\% speedup over standard speculative decoding while limiting accuracy degradation to under 2\\%, making it a practical solution for efficient and adaptive LLM inference.", "AI": {"tldr": "本文提出自适应推测解码（AdaSD），一种无需超参数调整的解码方案，通过动态调整生成长度和接受标准，显著加速大型语言模型（LLM）的推理，同时保持高准确性。", "motivation": "大型语言模型（LLM）参数量大导致推理速度慢。现有推测解码方法虽能缓解此问题，但通常需要额外训练、大量超参数调优或预先模型/任务分析，部署复杂。", "method": "AdaSD引入了两个自适应阈值：一个用于决定何时停止候选token生成，另一个用于决定token接受。这两个阈值在推理过程中实时更新，依据token熵和Jensen-Shannon距离。这种方法无需预分析或微调，兼容现有模型。", "result": "在基准数据集上的实验表明，AdaSD相对于标准推测解码实现了高达49%的加速，同时将准确性下降限制在2%以内。", "conclusion": "AdaSD是一种实用、高效且自适应的LLM推理解决方案，能够显著提升推理速度，且无需超参数调优或预分析，使其成为一种实用的部署方案。"}}
{"id": "2512.11282", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11282", "abs": "https://arxiv.org/abs/2512.11282", "authors": ["Qingsen Ma", "Dianyun Wang", "Ran Jing", "Yujun Sun", "Zhenbo Xu"], "title": "CIP: A Plug-and-Play Causal Prompting Framework for Mitigating Hallucinations under Long-Context Noise", "comment": null, "summary": "Large language models often hallucinate when processing long and noisy retrieval contexts because they rely on spurious correlations rather than genuine causal relationships. We propose CIP, a lightweight and plug-and-play causal prompting framework that mitigates hallucinations at the input stage. CIP constructs a causal relation sequence among entities, actions, and events and injects it into the prompt to guide reasoning toward causally relevant evidence. Through causal intervention and counterfactual reasoning, CIP suppresses non causal reasoning paths, improving factual grounding and interpretability. Experiments across seven mainstream language models, including GPT-4o, Gemini 2.0 Flash, and Llama 3.1, show that CIP consistently enhances reasoning quality and reliability, achieving 2.6 points improvement in Attributable Rate, 0.38 improvement in Causal Consistency Score, and a fourfold increase in effective information density. API level profiling further shows that CIP accelerates contextual understanding and reduces end to end response latency by up to 55.1 percent. These results suggest that causal reasoning may serve as a promising paradigm for improving the explainability, stability, and efficiency of large language models.", "AI": {"tldr": "本文提出CIP，一个轻量级即插即用的因果提示框架，通过构建并注入实体、动作和事件之间的因果关系序列，引导大型语言模型（LLMs）在处理长而嘈杂的检索上下文时进行因果推理，从而减轻幻觉、提高事实依据和可解释性，并显著提升推理质量、可靠性和效率。", "motivation": "大型语言模型在处理长且嘈杂的检索上下文时，由于依赖虚假关联而非真正的因果关系，经常产生幻觉。", "method": "本文提出了CIP（Causal Prompting）框架，它是一个轻量级、即插即用的因果提示框架。CIP通过构建实体、动作和事件之间的因果关系序列，并将其注入到提示中，以引导模型进行因果相关证据的推理。通过因果干预和反事实推理，CIP抑制了非因果推理路径，从而改善了事实依据和可解释性。", "result": "在GPT-4o、Gemini 2.0 Flash和Llama 3.1等七个主流语言模型上的实验表明，CIP持续提升了推理质量和可靠性。具体表现为：可归因率（Attributable Rate）提高了2.6个百分点，因果一致性得分（Causal Consistency Score）提高了0.38，有效信息密度增加了四倍。API层面分析显示，CIP加速了上下文理解，并将端到端响应延迟降低了高达55.1%。", "conclusion": "这些结果表明，因果推理可能是一个有前景的范式，可以提高大型语言模型的可解释性、稳定性和效率。"}}
{"id": "2512.11189", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11189", "abs": "https://arxiv.org/abs/2512.11189", "authors": ["Anh-Kiet Duong", "Petra Gomez-Krämer"], "title": "Multi-task Learning with Extended Temporal Shift Module for Temporal Action Localization", "comment": "BinEgo360@ICCV25", "summary": "We present our solution to the BinEgo-360 Challenge at ICCV 2025, which focuses on temporal action localization (TAL) in multi-perspective and multi-modal video settings. The challenge provides a dataset containing panoramic, third-person, and egocentric recordings, annotated with fine-grained action classes. Our approach is built on the Temporal Shift Module (TSM), which we extend to handle TAL by introducing a background class and classifying fixed-length non-overlapping intervals. We employ a multi-task learning framework that jointly optimizes for scene classification and TAL, leveraging contextual cues between actions and environments. Finally, we integrate multiple models through a weighted ensemble strategy, which improves robustness and consistency of predictions. Our method is ranked first in both the initial and extended rounds of the competition, demonstrating the effectiveness of combining multi-task learning, an efficient backbone, and ensemble learning for TAL.", "AI": {"tldr": "本文介绍了在ICCV 2025 BinEgo-360挑战赛中获得第一名的解决方案，该方案通过扩展时间位移模块（TSM）、多任务学习和加权集成策略，有效解决了多视角多模态视频中的时间动作定位（TAL）问题。", "motivation": "BinEgo-360挑战赛专注于在包含全景、第三人称和自我中心记录的多视角多模态视频设置中进行时间动作定位（TAL），并标注了细粒度动作类别。", "method": "该方法基于时间位移模块（TSM），并对其进行扩展以处理TAL，具体包括引入背景类别和对固定长度非重叠时间间隔进行分类。同时，采用多任务学习框架，联合优化场景分类和TAL，利用动作与环境之间的上下文线索。最后，通过加权集成策略整合多个模型，提高预测的鲁棒性和一致性。", "result": "该方法在BinEgo-360挑战赛的初始轮和扩展轮中均排名第一，展示了其在时间动作定位方面的有效性。", "conclusion": "结合多任务学习、高效骨干网络（TSM）和集成学习是解决时间动作定位问题的有效策略。"}}
{"id": "2512.11505", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.11505", "abs": "https://arxiv.org/abs/2512.11505", "authors": ["Priyam Basu", "Yunfeng Zhang", "Vipul Raheja"], "title": "BAID: A Benchmark for Bias Assessment of AI Detectors", "comment": "Accepted at the workshop on Agentic AI Benchmarks and Applications for Enterprise Tasks at AAAI 2026", "summary": "AI-generated text detectors have recently gained adoption in educational and professional contexts. Prior research has uncovered isolated cases of bias, particularly against English Language Learners (ELLs) however, there is a lack of systematic evaluation of such systems across broader sociolinguistic factors. In this work, we propose BAID, a comprehensive evaluation framework for AI detectors across various types of biases. As a part of the framework, we introduce over 200k samples spanning 7 major categories: demographics, age, educational grade level, dialect, formality, political leaning, and topic. We also generated synthetic versions of each sample with carefully crafted prompts to preserve the original content while reflecting subgroup-specific writing styles. Using this, we evaluate four open-source state-of-the-art AI text detectors and find consistent disparities in detection performance, particularly low recall rates for texts from underrepresented groups. Our contributions provide a scalable, transparent approach for auditing AI detectors and emphasize the need for bias-aware evaluation before these tools are deployed for public use.", "AI": {"tldr": "本文提出了BAID框架和20万样本，系统评估了AI文本检测器在多种社会语言因素下的偏见，发现对弱势群体文本的召回率较低，强调了部署前进行偏见感知评估的重要性。", "motivation": "AI生成文本检测器在教育和专业领域日益普及，但现有研究仅揭示了孤立的偏见案例（如针对英语学习者），缺乏对更广泛社会语言因素的系统性评估。", "method": "提出了BAID（Bias-Aware AI Detector evaluation）综合评估框架。构建了包含20多万样本的数据集，涵盖人口统计学、年龄、教育年级、方言、正式程度、政治倾向和主题7大类别。通过精心设计的提示词生成了每个样本的合成版本，以保留原始内容并反映亚群体写作风格。使用该框架评估了四种最先进的开源AI文本检测器。", "result": "评估发现AI文本检测器在检测性能上存在持续的差异，特别是对来自弱势群体的文本，其召回率显著偏低。", "conclusion": "本文为AI检测器提供了一种可扩展、透明的审计方法，并强调在这些工具投入公共使用之前，必须进行偏见感知评估。"}}
{"id": "2512.11769", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.11769", "abs": "https://arxiv.org/abs/2512.11769", "authors": ["Xiaoyu Ma", "Zhengqing Yuan", "Zheyuan Zhang", "Kaiwen Shi", "Lichao Sun", "Yanfang Ye"], "title": "BLURR: A Boosted Low-Resource Inference for Vision-Language-Action Models", "comment": "10 pages, 3 figures. Code and integration scripts will be released at this http URL: https://github.com/JijiKing-Sam/BLURR-A-Boosted-Low-Resource-Inference-for-Vision-Language-Action-Model", "summary": "Vision-language-action (VLA) models enable impressive zero shot manipulation, but their inference stacks are often too heavy for responsive web demos or high frequency robot control on commodity GPUs. We present BLURR, a lightweight inference wrapper that can be plugged into existing VLA controllers without retraining or changing model checkpoints. Instantiated on the pi-zero VLA controller, BLURR keeps the original observation interfaces and accelerates control by combining an instruction prefix key value cache, mixed precision execution, and a single step rollout schedule that reduces per step computation. In our SimplerEnv based evaluation, BLURR maintains task success rates comparable to the original controller while significantly lowering effective FLOPs and wall clock latency. We also build an interactive web demo that allows users to switch between controllers and toggle inference options in real time while watching manipulation episodes. This highlights BLURR as a practical approach for deploying modern VLA policies under tight compute budgets.", "AI": {"tldr": "BLURR是一个轻量级推理封装，用于加速现有VLA模型，通过结合缓存、混合精度和单步调度，显著降低计算成本和延迟，同时保持任务成功率，适用于资源受限的部署。", "motivation": "当前的视觉-语言-动作（VLA）模型推理栈过于庞大，难以在商品GPU上实现响应式网络演示或高频机器人控制。", "method": "BLURR是一个轻量级推理封装，可即插即用于现有VLA控制器，无需重新训练或更改模型检查点。它通过结合指令前缀键值缓存、混合精度执行和单步推出调度来减少每步计算。", "result": "在SimplerEnv评估中，BLURR保持了与原始控制器相当的任务成功率，同时显著降低了有效FLOPs和实际延迟。还构建了一个交互式网络演示。", "conclusion": "BLURR为在计算预算紧张的情况下部署现代VLA策略提供了一种实用的方法。"}}
{"id": "2512.11203", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11203", "abs": "https://arxiv.org/abs/2512.11203", "authors": ["Zhengyang Yu", "Akio Hayakawa", "Masato Ishii", "Qingtao Yu", "Takashi Shibuya", "Jing Zhang", "Yuki Mitsufuji"], "title": "AutoRefiner: Improving Autoregressive Video Diffusion Models via Reflective Refinement Over the Stochastic Sampling Path", "comment": null, "summary": "Autoregressive video diffusion models (AR-VDMs) show strong promise as scalable alternatives to bidirectional VDMs, enabling real-time and interactive applications. Yet there remains room for improvement in their sample fidelity. A promising solution is inference-time alignment, which optimizes the noise space to improve sample fidelity without updating model parameters. Yet, optimization- or search-based methods are computationally impractical for AR-VDMs. Recent text-to-image (T2I) works address this via feedforward noise refiners that modulate sampled noises in a single forward pass. Can such noise refiners be extended to AR-VDMs? We identify the failure of naively extending T2I noise refiners to AR-VDMs and propose AutoRefiner-a noise refiner tailored for AR-VDMs, with two key designs: pathwise noise refinement and a reflective KV-cache. Experiments demonstrate that AutoRefiner serves as an efficient plug-in for AR-VDMs, effectively enhancing sample fidelity by refining noise along stochastic denoising paths.", "AI": {"tldr": "自回归视频扩散模型(AR-VDMs)在可扩展性方面表现出色，但样本保真度有待提高。传统的推理时对齐计算成本高昂，而文本到图像(T2I)的噪声精炼器无法直接应用于AR-VDMs。本文提出了AutoRefiner，通过路径式噪声精炼和反射式KV-缓存，高效提升AR-VDMs的样本保真度。", "motivation": "自回归视频扩散模型(AR-VDMs)在实时和交互式应用中具有巨大潜力，但其样本保真度仍有提升空间。现有的推理时对齐方法对AR-VDMs而言计算成本过高。尽管文本到图像(T2I)领域有高效的噪声精炼器，但它们无法直接应用于AR-VDMs。", "method": "本文提出了AutoRefiner，一个专为AR-VDMs设计的噪声精炼器。它包含两个关键设计：路径式噪声精炼（pathwise noise refinement）和反射式KV-缓存（reflective KV-cache）。", "result": "实验证明，AutoRefiner可以作为AR-VDMs的高效即插即用组件，通过沿随机去噪路径精炼噪声，有效提高了样本保真度。", "conclusion": "AutoRefiner成功地为AR-VDMs提供了一种高效的噪声精炼机制，有效解决了其样本保真度不足的问题，使其更适用于实时和交互式应用。"}}
{"id": "2512.11215", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11215", "abs": "https://arxiv.org/abs/2512.11215", "authors": ["Tianye Qi", "Weihao Li", "Nick Barnes"], "title": "SmokeBench: Evaluating Multimodal Large Language Models for Wildfire Smoke Detection", "comment": "Accepted to WACV 2026", "summary": "Wildfire smoke is transparent, amorphous, and often visually confounded with clouds, making early-stage detection particularly challenging. In this work, we introduce a benchmark, called SmokeBench, to evaluate the ability of multimodal large language models (MLLMs) to recognize and localize wildfire smoke in images. The benchmark consists of four tasks: (1) smoke classification, (2) tile-based smoke localization, (3) grid-based smoke localization, and (4) smoke detection. We evaluate several MLLMs, including Idefics2, Qwen2.5-VL, InternVL3, Unified-IO 2, Grounding DINO, GPT-4o, and Gemini-2.5 Pro. Our results show that while some models can classify the presence of smoke when it covers a large area, all models struggle with accurate localization, especially in the early stages. Further analysis reveals that smoke volume is strongly correlated with model performance, whereas contrast plays a comparatively minor role. These findings highlight critical limitations of current MLLMs for safety-critical wildfire monitoring and underscore the need for methods that improve early-stage smoke localization.", "AI": {"tldr": "本文引入了SmokeBench基准来评估多模态大语言模型（MLLMs）识别和定位野火烟雾的能力。结果显示，MLLMs在早期烟雾定位方面存在显著困难，其性能与烟雾量强相关。", "motivation": "野火烟雾透明、无定形且常与云混淆，导致早期检测极具挑战性。研究旨在评估当前多模态大语言模型在这一安全关键应用中的表现。", "method": "研究引入了名为SmokeBench的基准，包含四项任务：烟雾分类、基于瓦片的烟雾定位、基于网格的烟雾定位和烟雾检测。评估了Idefics2、Qwen2.5-VL、InternVL3、Unified-IO 2、Grounding DINO、GPT-4o和Gemini-2.5 Pro等多种MLLMs。", "result": "结果表明，虽然一些模型能够分类大面积烟雾的存在，但所有模型在准确的定位方面都表现不佳，尤其是在早期阶段。进一步分析显示，烟雾量与模型性能强相关，而对比度作用相对较小。", "conclusion": "当前MLLMs在安全关键的野火监测方面存在严重局限性，特别是在早期烟雾定位方面。这强调了需要改进早期烟雾定位方法的重要性。"}}
{"id": "2512.11544", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11544", "abs": "https://arxiv.org/abs/2512.11544", "authors": ["Yuan Shen", "Xiaojun Wu", "Linghua Yu"], "title": "AI-MASLD Metabolic Dysfunction and Information Steatosis of Large Language Models in Unstructured Clinical Narratives", "comment": "47 pages, 2 figures", "summary": "This study aims to simulate real-world clinical scenarios to systematically evaluate the ability of Large Language Models (LLMs) to extract core medical information from patient chief complaints laden with noise and redundancy, and to verify whether they exhibit a functional decline analogous to Metabolic Dysfunction-Associated Steatotic Liver Disease (MASLD). We employed a cross-sectional analysis design based on standardized medical probes, selecting four mainstream LLMs as research subjects: GPT-4o, Gemini 2.5, DeepSeek 3.1, and Qwen3-Max. An evaluation system comprising twenty medical probes across five core dimensions was used to simulate a genuine clinical communication environment. All probes had gold-standard answers defined by clinical experts and were assessed via a double-blind, inverse rating scale by two independent clinicians. The results show that all tested models exhibited functional defects to varying degrees, with Qwen3-Max demonstrating the best overall performance and Gemini 2.5 the worst. Under conditions of extreme noise, most models experienced a functional collapse. Notably, GPT-4o made a severe misjudgment in the risk assessment for pulmonary embolism (PE) secondary to deep vein thrombosis (DVT). This research is the first to empirically confirm that LLMs exhibit features resembling metabolic dysfunction when processing clinical information, proposing the innovative concept of \"AI-Metabolic Dysfunction-Associated Steatotic Liver Disease (AI-MASLD)\". These findings offer a crucial safety warning for the application of Artificial Intelligence (AI) in healthcare, emphasizing that current LLMs must be used as auxiliary tools under human expert supervision, as there remains a significant gap between their theoretical knowledge and practical clinical application.", "AI": {"tldr": "本研究评估了大型语言模型（LLMs）从嘈杂主诉中提取核心医疗信息的能力，发现它们存在不同程度的功能缺陷，尤其在极端噪音下会崩溃，并首次提出了“AI-代谢功能障碍相关脂肪性肝病（AI-MASLD）”概念。", "motivation": "模拟真实临床场景，系统评估LLMs从含噪音和冗余的患者主诉中提取核心医疗信息的能力，并验证它们是否表现出类似于代谢功能障碍相关脂肪性肝病（MASLD）的功能下降。", "method": "采用基于标准化医疗探针的横断面分析设计，选取GPT-4o、Gemini 2.5、DeepSeek 3.1和Qwen3-Max四种主流LLMs。使用包含20个医疗探针和5个核心维度的评估系统模拟真实临床沟通环境。所有探针均有临床专家定义的金标准答案，并由两名独立临床医生通过双盲、逆向评分量表进行评估。", "result": "所有测试模型都表现出不同程度的功能缺陷，其中Qwen3-Max表现最佳，Gemini 2.5最差。在极端噪音条件下，大多数模型出现功能崩溃。值得注意的是，GPT-4o在深静脉血栓（DVT）继发肺栓塞（PE）的风险评估中出现严重误判。本研究首次经验性证实LLMs在处理临床信息时表现出类似代谢功能障碍的特征，并提出了“AI-MASLD”的创新概念。", "conclusion": "LLMs在处理临床信息时存在功能缺陷，当前的LLMs必须在人类专家监督下作为辅助工具使用，因为它们的理论知识与实际临床应用之间仍存在显著差距，这为AI在医疗领域的应用提供了重要的安全警示。"}}
{"id": "2512.11588", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11588", "abs": "https://arxiv.org/abs/2512.11588", "authors": ["Gregor von Laszewski", "Wesley Brewer", "Jeyan Thiyagalingam", "Juri Papay", "Armstrong Foundjem", "Piotr Luszczek", "Murali Emani", "Shirley V. Moore", "Vijay Janapa Reddi", "Matthew D. Sinclair", "Sebastian Lobentanzer", "Sujata Goswami", "Benjamin Hawks", "Marco Colombo", "Nhan Tran", "Christine R. Kirkpatrick", "Abdulkareem Alsudais", "Gregg Barrett", "Tianhao Li", "Kirsten Morehouse", "Shivaram Venkataraman", "Rutwik Jain", "Kartik Mathur", "Victor Lu", "Tejinder Singh", "Khojasteh Z. Mirza", "Kongtao Chen", "Sasidhar Kunapuli", "Gavin Farrell", "Renato Umeton", "Geoffrey C. Fox"], "title": "AI Benchmark Democratization and Carpentry", "comment": "43 pages, 2 figures, 7 tables", "summary": "Benchmarks are a cornerstone of modern machine learning, enabling reproducibility, comparison, and scientific progress. However, AI benchmarks are increasingly complex, requiring dynamic, AI-focused workflows. Rapid evolution in model architectures, scale, datasets, and deployment contexts makes evaluation a moving target. Large language models often memorize static benchmarks, causing a gap between benchmark results and real-world performance.\n  Beyond traditional static benchmarks, continuous adaptive benchmarking frameworks are needed to align scientific assessment with deployment risks. This calls for skills and education in AI Benchmark Carpentry. From our experience with MLCommons, educational initiatives, and programs like the DOE's Trillion Parameter Consortium, key barriers include high resource demands, limited access to specialized hardware, lack of benchmark design expertise, and uncertainty in relating results to application domains. Current benchmarks often emphasize peak performance on top-tier hardware, offering limited guidance for diverse, real-world scenarios.\n  Benchmarking must become dynamic, incorporating evolving models, updated data, and heterogeneous platforms while maintaining transparency, reproducibility, and interpretability. Democratization requires both technical innovation and systematic education across levels, building sustained expertise in benchmark design and use. Benchmarks should support application-relevant comparisons, enabling informed, context-sensitive decisions. Dynamic, inclusive benchmarking will ensure evaluation keeps pace with AI evolution and supports responsible, reproducible, and accessible AI deployment. Community efforts can provide a foundation for AI Benchmark Carpentry.", "AI": {"tldr": "随着AI的快速发展，传统静态基准测试已不足以评估模型性能，需要动态、自适应的基准测试框架和AI基准测试技能培训，以实现负责任的AI部署。", "motivation": "现代AI基准测试日益复杂，模型架构、规模、数据集和部署环境快速演变，使得评估成为一个移动目标。大型语言模型常会记忆静态基准，导致基准结果与实际性能之间存在差距。此外，现有基准测试侧重于顶尖硬件上的峰值性能，对多样化的实际场景指导有限。", "method": "本文呼吁建立持续自适应的基准测试框架，并倡导“AI基准测试木工”（AI Benchmark Carpentry）的技能和教育。这包括技术创新和系统化的多层次教育，以培养基准设计和使用的专业知识。基准测试应支持与应用相关的比较，并在动态、包容的环境中进行，以保持透明度、可复现性和可解释性。", "result": "通过MLCommons、教育计划和DOE的万亿参数联盟的经验，主要障碍包括高资源需求、专业硬件获取受限、基准设计专业知识缺乏以及结果与应用领域关联性不确定。当前的基准测试未能为多样化的实际场景提供足够的指导。", "conclusion": "基准测试必须变得动态化，整合不断演进的模型、更新的数据和异构平台，同时保持透明、可复现和可解释性。通过技术创新和系统教育实现民主化，建立持久的基准设计和使用专业知识，将确保评估与AI发展同步，并支持负责任、可复现和可访问的AI部署。社区协作是AI基准测试技能培训的基础。"}}
{"id": "2512.11736", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.11736", "abs": "https://arxiv.org/abs/2512.11736", "authors": ["Ninghan Zhong", "Steven Caro", "Megnath Ramesh", "Rishi Bhatnagar", "Avraiem Iskandar", "Stephen L. Smith"], "title": "Bench-Push: Benchmarking Pushing-based Navigation and Manipulation Tasks for Mobile Robots", "comment": "Under review for ICRA 2026", "summary": "Mobile robots are increasingly deployed in cluttered environments with movable objects, posing challenges for traditional methods that prohibit interaction. In such settings, the mobile robot must go beyond traditional obstacle avoidance, leveraging pushing or nudging strategies to accomplish its goals. While research in pushing-based robotics is growing, evaluations rely on ad hoc setups, limiting reproducibility and cross-comparison. To address this, we present Bench-Push, the first unified benchmark for pushing-based mobile robot navigation and manipulation tasks. Bench-Push includes multiple components: 1) a comprehensive range of simulated environments that capture the fundamental challenges in pushing-based tasks, including navigating a maze with movable obstacles, autonomous ship navigation in ice-covered waters, box delivery, and area clearing, each with varying levels of complexity; 2) novel evaluation metrics to capture efficiency, interaction effort, and partial task completion; and 3) demonstrations using Bench-Push to evaluate example implementations of established baselines across environments. Bench-Push is open-sourced as a Python library with a modular design. The code, documentation, and trained models can be found at https://github.com/IvanIZ/BenchNPIN.", "AI": {"tldr": "Bench-Push是一个统一的基准测试平台，用于评估基于推动的移动机器人导航和操作任务，解决了现有评估方法缺乏可复现性和可比较性的问题。", "motivation": "传统移动机器人方法在有可移动障碍物的杂乱环境中表现不佳，因为它们禁止交互。当前基于推动的机器人研究评估依赖于临时设置，限制了可复现性和跨比较。", "method": "本文提出了Bench-Push，一个统一的基准测试平台，包括：1) 模拟环境，涵盖迷宫导航、破冰船导航、箱子递送和区域清理等任务，复杂度各异；2) 新颖的评估指标，衡量效率、交互努力和部分任务完成度；3) 使用Bench-Push评估现有基线实现的示例演示。该平台作为一个模块化的Python库开源。", "result": "Bench-Push提供了一个全面的平台，用于在各种复杂环境中评估基于推动的移动机器人任务，从而实现可复现和可比较的研究，并促进了该领域的发展。", "conclusion": "Bench-Push通过提供一个统一、可复现的基准，解决了基于推动的移动机器人导航和操作任务评估的挑战，为未来研究和开发奠定了基础。"}}
{"id": "2512.11297", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.11297", "abs": "https://arxiv.org/abs/2512.11297", "authors": ["Shogo Fujita", "Yuji Naraki", "Yiqing Zhu", "Shinsuke Mori"], "title": "LegalRikai: Open Benchmark -- A Benchmark for Complex Japanese Corporate Legal Tasks", "comment": null, "summary": "This paper introduces LegalRikai: Open Benchmark, a new benchmark comprising four complex tasks that emulate Japanese corporate legal practices. The benchmark was created by legal professionals under the supervision of an attorney. This benchmark has 100 samples that require long-form, structured outputs, and we evaluated them against multiple practical criteria. We conducted both human and automated evaluations using leading LLMs, including GPT-5, Gemini 2.5 Pro, and Claude Opus 4.1. Our human evaluation revealed that abstract instructions prompted unnecessary modifications, highlighting model weaknesses in document-level editing that were missed by conventional short-text tasks. Furthermore, our analysis reveals that automated evaluation aligns well with human judgment on criteria with clear linguistic grounding, and assessing structural consistency remains a challenge. The result demonstrates the utility of automated evaluation as a screening tool when expert availability is limited. We propose a dataset evaluation framework to promote more practice-oriented research in the legal domain.", "AI": {"tldr": "本文介绍了LegalRikai：开放基准，一个模拟日本企业法律实践的复杂任务基准，包含100个样本，需要长篇结构化输出。通过人工和自动化评估主流大型语言模型（LLM），揭示了LLM在文档级编辑方面的弱点，并提出了一个数据集评估框架以促进法律领域的实践导向研究。", "motivation": "现有基准可能未能充分模拟复杂的日本企业法律实践，也未能有效评估大型语言模型在需要长篇、结构化输出的任务上的表现，特别是在文档级编辑方面。", "method": "研究方法包括：1) 创建LegalRikai：开放基准，由法律专业人士在律师监督下开发，包含四项模拟日本企业法律实践的复杂任务，共100个需要长篇、结构化输出的样本。2) 针对多项实用标准进行评估。3) 使用GPT-5、Gemini 2.5 Pro和Claude Opus 4.1等领先的LLM进行人工和自动化评估。", "result": "主要结果是：1) 人工评估发现抽象指令导致不必要的修改，揭示了LLM在文档级编辑方面的弱点，这是传统短文本任务未能发现的。2) 自动化评估与人工判断在具有清晰语言基础的标准上表现出良好的一致性。3) 评估结构一致性对自动化评估而言仍是一项挑战。4) 自动化评估在专家资源有限时可作为有效的筛选工具。", "conclusion": "结论是：1) LegalRikai基准有效揭示了LLM在复杂法律任务中，尤其是在文档级编辑方面的局限性。2) 自动化评估在特定条件下具有实用性，但仍需改进以处理结构一致性评估。3) 提出了一个数据集评估框架，旨在推动法律领域更具实践导向的研究。"}}
{"id": "2512.11773", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.11773", "abs": "https://arxiv.org/abs/2512.11773", "authors": ["Britton Jordan", "Jordan Thompson", "Jesse F. d'Almeida", "Hao Li", "Nithesh Kumar", "Susheela Sharma Stern", "Ipek Oguz", "Robert J. Webster", "Daniel Brown", "Alan Kuntz", "James Ferguson"], "title": "ProbeMDE: Uncertainty-Guided Active Proprioception for Monocular Depth Estimation in Surgical Robotics", "comment": "9 pages, 5 figures", "summary": "Monocular depth estimation (MDE) provides a useful tool for robotic perception, but its predictions are often uncertain and inaccurate in challenging environments such as surgical scenes where textureless surfaces, specular reflections, and occlusions are common. To address this, we propose ProbeMDE, a cost-aware active sensing framework that combines RGB images with sparse proprioceptive measurements for MDE. Our approach utilizes an ensemble of MDE models to predict dense depth maps conditioned on both RGB images and on a sparse set of known depth measurements obtained via proprioception, where the robot has touched the environment in a known configuration. We quantify predictive uncertainty via the ensemble's variance and measure the gradient of the uncertainty with respect to candidate measurement locations. To prevent mode collapse while selecting maximally informative locations to propriocept (touch), we leverage Stein Variational Gradient Descent (SVGD) over this gradient map. We validate our method in both simulated and physical experiments on central airway obstruction surgical phantoms. Our results demonstrate that our approach outperforms baseline methods across standard depth estimation metrics, achieving higher accuracy while minimizing the number of required proprioceptive measurements.", "AI": {"tldr": "该研究提出ProbeMDE，一个成本感知的主动感知框架，通过结合RGB图像和稀疏本体感受测量来改进在外科手术等挑战性环境中的单目深度估计（MDE）。", "motivation": "单目深度估计在纹理缺失、镜面反射和遮挡常见的外科场景等挑战性环境中，其预测结果往往不确定且不准确，限制了其在机器人感知中的应用。", "method": "该方法利用MDE模型集成，根据RGB图像和机器人触碰环境获得的稀疏已知深度测量（本体感受）来预测密集深度图。它通过集成模型的方差量化预测不确定性，并测量不确定性相对于候选测量位置的梯度。为选择信息量最大的触碰位置并防止模式崩溃，该方法在梯度图上利用Stein变分梯度下降（SVGD）。", "result": "在气道阻塞手术模型上的模拟和物理实验表明，该方法在标准深度估计指标上优于基线方法，在最小化所需本体感受测量次数的同时，实现了更高的准确性。", "conclusion": "ProbeMDE通过智能地结合RGB信息与主动本体感受测量，有效提高了在挑战性环境下（如外科场景）的单目深度估计精度和效率。"}}
{"id": "2512.11303", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.11303", "abs": "https://arxiv.org/abs/2512.11303", "authors": ["Jiarun Liu", "Shiyue Xu", "Yang Li", "Shangkun Liu", "Yongli Yu", "Peng Cao"], "title": "Unifying Dynamic Tool Creation and Cross-Task Experience Sharing through Cognitive Memory Architecture", "comment": null, "summary": "Large Language Model agents face fundamental challenges in adapting to novel tasks due to limitations in tool availability and experience reuse. Existing approaches either rely on predefined tools with limited coverage or build tools from scratch without leveraging past experiences, leading to inefficient exploration and suboptimal performance. We introduce SMITH (Shared Memory Integrated Tool Hub), a unified cognitive architecture that seamlessly integrates dynamic tool creation with cross-task experience sharing through hierarchical memory organization. SMITH organizes agent memory into procedural, semantic, and episodic components, enabling systematic capability expansion while preserving successful execution patterns. Our approach formalizes tool creation as iterative code generation within controlled sandbox environments and experience sharing through episodic memory retrieval with semantic similarity matching. We further propose a curriculum learning strategy based on agent-ensemble difficulty re-estimation. Extensive experiments on the GAIA benchmark demonstrate SMITH's effectiveness, achieving 81.8% Pass@1 accuracy and outperforming state-of-the-art baselines including Alita (75.2%) and Memento (70.9%). Our work establishes a foundation for building truly adaptive agents that continuously evolve their capabilities through principled integration of tool creation and experience accumulation.", "AI": {"tldr": "SMITH是一个统一的认知架构，通过分层记忆组织，无缝集成动态工具创建和跨任务经验共享，旨在解决大型语言模型代理在适应新任务时面临的工具可用性和经验复用挑战。", "motivation": "现有的大语言模型代理在适应新任务时面临挑战，主要体现在预定义工具覆盖范围有限，或从零开始构建工具而未能利用过往经验，导致探索效率低下和性能不佳。", "method": "SMITH采用分层记忆（程序性、语义性、情景性）来系统地扩展能力并保留成功的执行模式。它将工具创建形式化为受控沙盒环境中的迭代代码生成，并通过情景记忆检索和语义相似性匹配实现经验共享。此外，还提出了一种基于代理集成难度重新评估的课程学习策略。", "result": "在GAIA基准测试中，SMITH实现了81.8%的Pass@1准确率，优于现有最先进的基线方法，包括Alita（75.2%）和Memento（70.9%）。", "conclusion": "该研究为构建真正自适应的代理奠定了基础，这些代理能够通过工具创建和经验积累的原则性整合，持续进化自身能力。"}}
{"id": "2512.11225", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.11225", "abs": "https://arxiv.org/abs/2512.11225", "authors": ["Gabrijel Boduljak", "Yushi Lan", "Christian Rupprecht", "Andrea Vedaldi"], "title": "VFMF: World Modeling by Forecasting Vision Foundation Model Features", "comment": null, "summary": "Forecasting from partial observations is central to world modeling. Many recent methods represent the world through images, and reduce forecasting to stochastic video generation. Although such methods excel at realism and visual fidelity, predicting pixels is computationally intensive and not directly useful in many applications, as it requires translating RGB into signals useful for decision making. An alternative approach uses features from vision foundation models (VFMs) as world representations, performing deterministic regression to predict future world states. These features can be directly translated into actionable signals such as semantic segmentation and depth, while remaining computationally efficient. However, deterministic regression averages over multiple plausible futures, undermining forecast accuracy by failing to capture uncertainty. To address this crucial limitation, we introduce a generative forecaster that performs autoregressive flow matching in VFM feature space. Our key insight is that generative modeling in this space requires encoding VFM features into a compact latent space suitable for diffusion. We show that this latent space preserves information more effectively than previously used PCA-based alternatives, both for forecasting and other applications, such as image generation. Our latent predictions can be easily decoded into multiple useful and interpretable output modalities: semantic segmentation, depth, surface normals, and even RGB. With matched architecture and compute, our method produces sharper and more accurate predictions than regression across all modalities. Our results suggest that stochastic conditional generation of VFM features offers a promising and scalable foundation for future world models.", "AI": {"tldr": "本文提出了一种生成式预测器，通过在视觉基础模型（VFM）特征空间中进行自回归流匹配，解决了现有方法在预测世界状态时无法捕捉不确定性的问题。该方法利用紧凑的潜在空间进行扩散建模，生成更清晰、更准确的多模态预测，为未来的世界模型提供了有前景的基础。", "motivation": "现有的世界建模方法存在局限性：基于图像的预测计算量大且难以直接用于决策；而基于VFM特征的确定性回归预测虽然高效，但会平均化多种可能的未来，导致预测不准确，无法捕捉不确定性。", "method": "研究引入了一个生成式预测器，在VFM特征空间中执行自回归流匹配。关键在于将VFM特征编码到一个紧凑的潜在空间（优于传统的PCA方法），该空间更适合扩散模型并能有效保留信息。预测结果可以解码为多种可解释的输出模态。", "result": "与确定性回归方法相比，在相同的架构和计算资源下，该方法在所有模态（语义分割、深度、表面法线甚至RGB）上都产生了更清晰、更准确的预测。此外，该方法使用的潜在空间比以往基于PCA的替代方案更有效地保留了信息，不仅适用于预测，也适用于图像生成等其他应用。", "conclusion": "VFM特征的随机条件生成为未来的世界模型提供了一个有前景且可扩展的基础，能够有效捕捉不确定性并提供多模态的、可操作的预测。"}}
{"id": "2512.11653", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11653", "abs": "https://arxiv.org/abs/2512.11653", "authors": ["Chutian Ma", "Grigorii Pomazkin", "Giacinto Paolo Saggese", "Paul Smith"], "title": "Causal Inference in Energy Demand Prediction", "comment": null, "summary": "Energy demand prediction is critical for grid operators, industrial energy\n  consumers, and service providers. Energy demand is influenced by multiple\n  factors, including weather conditions (e.g. temperature, humidity, wind\n  speed, solar radiation), and calendar information (e.g. hour of day and\n  month of year), which further affect daily work and life schedules. These\n  factors are causally interdependent, making the problem more complex than\n  simple correlation-based learning techniques satisfactorily allow for. We\n  propose a structural causal model that explains the causal relationship\n  between these variables. A full analysis is performed to validate our causal\n  beliefs, also revealing important insights consistent with prior studies.\n  For example, our causal model reveals that energy demand responds to\n  temperature fluctuations with season-dependent sensitivity. Additionally, we\n  find that energy demand exhibits lower variance in winter due to the\n  decoupling effect between temperature changes and daily activity patterns.\n  We then build a Bayesian model, which takes advantage of the causal insights\n  we learned as prior knowledge. The model is trained and tested on unseen\n  data and yields state-of-the-art performance in the form of a 3.84 percent MAPE on\n  the test set. The model also demonstrates strong robustness, as the\n  cross-validation across two years of data yields an average MAPE of 3.88 percent.", "AI": {"tldr": "该研究提出了一种结合结构因果模型和贝叶斯模型的方法，用于能源需求预测。通过揭示变量间的因果关系并将其作为先验知识融入贝叶斯模型，实现了最先进的预测精度和鲁棒性。", "motivation": "能源需求预测对电网运营商、工业消费者和服务提供商至关重要。能源需求受天气和日历信息等多种因素影响，这些因素之间存在因果相互依赖关系，使得传统基于相关性的学习技术难以满意地解决问题。", "method": "研究首先提出了一个结构因果模型来解释各变量（如天气、日历信息和能源需求）之间的因果关系，并进行了全面分析以验证这些因果信念。随后，构建了一个贝叶斯模型，将从因果模型中获得的洞察作为先验知识融入其中，并使用未见过的数据进行训练和测试。", "result": "因果模型揭示了重要见解，例如能源需求对温度波动的响应具有季节依赖性，并且由于温度变化与日常活动模式的解耦效应，冬季能源需求方差较低。结合因果洞察的贝叶斯模型在测试集上实现了3.84%的平均绝对百分比误差（MAPE），达到了最先进的性能，并通过两年数据的交叉验证显示出3.88%的平均MAPE，证明了其强大的鲁棒性。", "conclusion": "通过建立变量间的结构因果模型并将其因果洞察作为先验知识整合到贝叶斯预测模型中，该方法能够有效且鲁棒地预测能源需求，并取得了显著优于现有技术的表现。"}}
{"id": "2512.11781", "categories": ["cs.RO", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.11781", "abs": "https://arxiv.org/abs/2512.11781", "authors": ["Vineet Pasumarti", "Lorenzo Bianchi", "Antonio Loquercio"], "title": "Agile Flight Emerges from Multi-Agent Competitive Racing", "comment": null, "summary": "Through multi-agent competition and the sparse high-level objective of winning a race, we find that both agile flight (e.g., high-speed motion pushing the platform to its physical limits) and strategy (e.g., overtaking or blocking) emerge from agents trained with reinforcement learning. We provide evidence in both simulation and the real world that this approach outperforms the common paradigm of training agents in isolation with rewards that prescribe behavior, e.g., progress on the raceline, in particular when the complexity of the environment increases, e.g., in the presence of obstacles. Moreover, we find that multi-agent competition yields policies that transfer more reliably to the real world than policies trained with a single-agent progress-based reward, despite the two methods using the same simulation environment, randomization strategy, and hardware. In addition to improved sim-to-real transfer, the multi-agent policies also exhibit some degree of generalization to opponents unseen at training time. Overall, our work, following in the tradition of multi-agent competitive game-play in digital domains, shows that sparse task-level rewards are sufficient for training agents capable of advanced low-level control in the physical world.\n  Code: https://github.com/Jirl-upenn/AgileFlight_MultiAgent", "AI": {"tldr": "通过多智能体竞争和稀疏的获胜目标，强化学习智能体能习得敏捷飞行和策略，并在复杂环境和仿真到真实世界迁移方面优于单一智能体预设奖励方法。", "motivation": "研究多智能体竞争结合稀疏高层目标（如赢得比赛）是否能让智能体自然习得敏捷飞行和策略，并克服传统单一智能体基于预设行为奖励方法的局限性，特别是在复杂环境和实物迁移方面。", "method": "采用强化学习训练多个智能体进行竞争，并以赢得比赛作为稀疏的高层目标。该方法在仿真和真实世界中与单一智能体基于赛道进度等预设奖励的方法进行对比验证。", "result": "智能体习得了敏捷飞行（如高速运动）和策略（如超车、阻挡）。该方法在复杂环境（如存在障碍物）中表现优于单一智能体预设奖励方法。多智能体策略的仿真到真实世界迁移更可靠，且对训练时未见的对手展现出一定泛化能力。", "conclusion": "在多智能体竞争性博弈中，稀疏的任务级奖励足以训练出能够在物理世界中实现高级低级控制的智能体。"}}
{"id": "2512.11229", "categories": ["cs.CV", "cs.SD"], "pdf": "https://arxiv.org/pdf/2512.11229", "abs": "https://arxiv.org/abs/2512.11229", "authors": ["Haotian Wang", "Yuzhe Weng", "Xinyi Yu", "Jun Du", "Haoran Xu", "Xiaoyan Wu", "Shan He", "Bing Yin", "Cong Liu", "Qingfeng Liu"], "title": "REST: Diffusion-based Real-time End-to-end Streaming Talking Head Generation via ID-Context Caching and Asynchronous Streaming Distillation", "comment": "10pages, 4 figures", "summary": "Diffusion models have significantly advanced the field of talking head generation. However, the slow inference speeds and non-autoregressive paradigms severely constrain the application of diffusion-based THG models. In this study, we propose REST, the first diffusion-based, real-time, end-to-end streaming audio-driven talking head generation framework. To support real-time end-to-end generation, a compact video latent space is first learned through high spatiotemporal VAE compression. Additionally, to enable autoregressive streaming within the compact video latent space, we introduce an ID-Context Cache mechanism, which integrates ID-Sink and Context-Cache principles to key-value caching for maintaining temporal consistency and identity coherence during long-time streaming generation. Furthermore, an Asynchronous Streaming Distillation (ASD) training strategy is proposed to mitigate error accumulation in autoregressive generation and enhance temporal consistency, which leverages a non-streaming teacher with an asynchronous noise schedule to supervise the training of the streaming student model. REST bridges the gap between autoregressive and diffusion-based approaches, demonstrating substantial value for applications requiring real-time talking head generation. Experimental results demonstrate that REST outperforms state-of-the-art methods in both generation speed and overall performance.", "AI": {"tldr": "REST是首个基于扩散模型、实时、端到端流式音频驱动的说话人头像生成框架，通过紧凑视频潜在空间、ID-Context Cache机制和异步流式蒸馏策略，解决了扩散模型推理慢和非自回归的问题。", "motivation": "尽管扩散模型在说话人头像生成方面取得了显著进展，但其缓慢的推理速度和非自回归范式严重限制了其在实时应用中的潜力。", "method": "1. 通过高时空VAE压缩学习紧凑的视频潜在空间。2. 引入ID-Context Cache机制（结合ID-Sink和Context-Cache原理），实现潜在空间内的自回归流式生成，并维护长时间流式生成中的时间一致性和身份连贯性。3. 提出异步流式蒸馏（ASD）训练策略，利用非流式教师模型以异步噪声调度监督流式学生模型的训练，以减轻自回归生成中的误差累积并增强时间一致性。", "result": "REST在生成速度和整体性能上均优于现有最先进的方法。", "conclusion": "REST成功弥合了自回归方法和扩散模型之间的差距，对于需要实时说话人头像生成应用具有重要价值。"}}
{"id": "2512.11388", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.11388", "abs": "https://arxiv.org/abs/2512.11388", "authors": ["Felipe Ribeiro Fujita de Mello", "Hideyuki Takada"], "title": "Improving Translation Quality by Selecting Better Data for LLM Fine-Tuning: A Comparative Analysis", "comment": "To appear at IEEE Big Data 2025", "summary": "We investigated the impact of data selection on machine translation fine-tuning for open LLMs. Using Japanese-English corpora, we compare five selectors: TF-IDF, COMET Kiwi, QuRate, FD-Score, and random selection, under controlled training conditions. We observed that semantic selectors consistently outperform lexical and geometry-based heuristics, and that even when the selected data differ by less than 3%, the impact on model performance is substantial, underscoring the sensitivity of fine-tuning to data quality.", "AI": {"tldr": "研究发现，对于开放大型语言模型（LLM）的机器翻译微调，数据选择至关重要，其中语义选择器表现最佳，即使数据差异微小也能显著影响模型性能。", "motivation": "该研究旨在探究数据选择对开放LLM机器翻译微调的影响。", "method": "研究使用了日英语料库，在受控训练条件下比较了五种数据选择器：TF-IDF、COMET Kiwi、QuRate、FD-Score和随机选择。", "result": "结果表明，语义选择器始终优于基于词汇和几何启发式的方法。此外，即使所选数据差异不到3%，模型性能也会受到显著影响。", "conclusion": "微调过程对数据质量高度敏感，强调了有效数据选择的重要性，尤其是语义选择器在提升模型性能方面的优势。"}}
{"id": "2512.11366", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.11366", "abs": "https://arxiv.org/abs/2512.11366", "authors": ["Shreya Shukla", "Aditya Sriram", "Milinda Kuppur Narayanaswamy", "Hiteshi Jain"], "title": "qa-FLoRA: Data-free query-adaptive Fusion of LoRAs for LLMs", "comment": "Accepted at AAAI 2026 (Main Technical Track)", "summary": "The deployment of large language models for specialized tasks often requires domain-specific parameter-efficient finetuning through Low-Rank Adaptation (LoRA) modules. However, effectively fusing these adapters to handle complex, multi-domain composite queries remains a critical challenge. Existing LoRA fusion approaches either use static weights, which assign equal relevance to each participating LoRA, or require data-intensive supervised training for every possible LoRA combination to obtain respective optimal fusion weights. We propose qa-FLoRA, a novel query-adaptive data-and-training-free method for LoRA fusion that dynamically computes layer-level fusion weights by measuring distributional divergence between the base model and respective adapters. Our approach eliminates the need for composite training data or domain-representative samples, making it readily applicable to existing adapter collections. Extensive experiments across nine multilingual composite tasks spanning mathematics, coding, and medical domains, show that qa-FLoRA outperforms static fusion by ~5% with LLaMA-2 and ~6% with LLaMA-3, and the training-free baselines by ~7% with LLaMA-2 and ~10% with LLaMA-3, while significantly closing the gap with supervised baselines. Further, layer-level analysis of our fusion weights reveals interpretable fusion patterns, demonstrating the effectiveness of our approach for robust multi-domain adaptation.", "AI": {"tldr": "本文提出qa-FLoRA，一种新颖的、无需训练和数据的查询自适应方法，通过测量基础模型与LoRA适配器之间的分布差异，动态计算层级融合权重，以有效处理复杂的多领域复合查询。", "motivation": "将大型语言模型部署到专业任务时，常需通过LoRA模块进行领域特定参数高效微调。然而，如何有效融合这些适配器以处理复杂的多领域复合查询是一个关键挑战。现有LoRA融合方法要么使用静态权重，要么需要为每种可能的LoRA组合进行数据密集型监督训练来获取最优融合权重。", "method": "本文提出qa-FLoRA，一种新颖的查询自适应、无需数据和训练的LoRA融合方法。它通过测量基础模型与各个适配器之间的分布差异，动态计算层级融合权重。这种方法无需复合训练数据或领域代表性样本。", "result": "在涵盖数学、编码和医疗领域的九项多语言复合任务上进行的广泛实验表明，qa-FLoRA在LLaMA-2上比静态融合高出约5%，在LLaMA-3上高出约6%；比无需训练的基线在LLaMA-2上高出约7%，在LLaMA-3上高出约10%。同时，它显著缩小了与监督基线之间的差距。此外，对融合权重的层级分析揭示了可解释的融合模式。", "conclusion": "qa-FLoRA为LoRA融合提供了一种鲁棒的、无需数据和训练的解决方案，能够有效处理多领域适应问题，并展现出优越的性能和可解释的融合模式。"}}
{"id": "2512.11296", "categories": ["cs.CV", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.11296", "abs": "https://arxiv.org/abs/2512.11296", "authors": ["Yasaman Hashem Pour", "Nazanin Mahjourian", "Vinh Nguyen"], "title": "Few-Shot VLM-Based G-Code and HMI Verification in CNC Machining", "comment": null, "summary": "Manual generation of G-code is important for learning the operation of CNC machines. Prior work in G-code verification uses Large-Language Models (LLMs), which primarily examine errors in the written programming. However, CNC machining requires extensive use and knowledge of the Human-Machine Interface (HMI), which displays machine status and errors. LLMs currently lack the capability to leverage knowledge of HMIs due to their inability to access the vision modality. This paper proposes a few-shot VLM-based verification approach that simultaneously evaluates the G-code and the HMI display for errors and safety status. The input dataset includes paired G-code text and associated HMI screenshots from a 15-slant-PRO lathe, including both correct and error-prone cases. To enable few-shot learning, the VLM is provided with a structured JSON schema based on prior heuristic knowledge. After determining the prompts, instances of G-code and HMI that either contain errors or are error free are used as few-shot examples to guide the VLM. The model was then evaluated in comparison to a zero-shot VLM through multiple scenarios of incorrect G-code and HMI errors with respect to per-slot accuracy. The VLM showed that few-shot prompting led to overall enhancement of detecting HMI errors and discrepancies with the G-code for more comprehensive debugging. Therefore, the proposed framework was demonstrated to be suitable for verification of manually generated G-code that is typically developed in CNC training.", "AI": {"tldr": "本文提出了一种基于少量样本的视觉语言模型（VLM）验证方法，用于同时评估数控机床中手动生成的G代码及其人机界面（HMI）显示，以检测错误和安全状态，从而实现更全面的调试。", "motivation": "现有的G代码验证方法主要使用大型语言模型（LLM）检查代码本身的错误，但它们无法利用人机界面（HMI）的视觉信息，而HMI在数控机床操作中至关重要。因此，需要一种能够同时考虑G代码和HMI状态的验证方法。", "method": "研究人员提出了一种基于少量样本的VLM验证方法。输入数据集包含配对的G代码文本和相关的HMI截图（包括正确和错误案例）。通过提供基于启发式知识的结构化JSON schema，并使用包含错误或无错误的G代码和HMI实例作为少量样本示例来指导VLM。该模型通过与零样本VLM在多种错误场景下的每槽准确性进行比较评估。", "result": "结果显示，少量样本提示（few-shot prompting）显著增强了VLM检测HMI错误以及G代码与HMI之间不一致的能力，从而实现了更全面的调试。", "conclusion": "所提出的框架被证明适用于验证数控培训中通常手动生成的G代码，能够有效识别G代码和HMI显示中的错误及不一致性。"}}
{"id": "2512.11797", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11797", "abs": "https://arxiv.org/abs/2512.11797", "authors": ["Junjie Ye", "Rong Xue", "Basile Van Hoorick", "Pavel Tokmakov", "Muhammad Zubair Irshad", "Yue Wang", "Vitor Guizilini"], "title": "AnchorDream: Repurposing Video Diffusion for Embodiment-Aware Robot Data Synthesis", "comment": "Project page: https://jay-ye.github.io/AnchorDream/", "summary": "The collection of large-scale and diverse robot demonstrations remains a major bottleneck for imitation learning, as real-world data acquisition is costly and simulators offer limited diversity and fidelity with pronounced sim-to-real gaps. While generative models present an attractive solution, existing methods often alter only visual appearances without creating new behaviors, or suffer from embodiment inconsistencies that yield implausible motions. To address these limitations, we introduce AnchorDream, an embodiment-aware world model that repurposes pretrained video diffusion models for robot data synthesis. AnchorDream conditions the diffusion process on robot motion renderings, anchoring the embodiment to prevent hallucination while synthesizing objects and environments consistent with the robot's kinematics. Starting from only a handful of human teleoperation demonstrations, our method scales them into large, diverse, high-quality datasets without requiring explicit environment modeling. Experiments show that the generated data leads to consistent improvements in downstream policy learning, with relative gains of 36.4% in simulator benchmarks and nearly double performance in real-world studies. These results suggest that grounding generative world models in robot motion provides a practical path toward scaling imitation learning.", "AI": {"tldr": "AnchorDream是一个具身感知的世界模型，它利用预训练视频扩散模型生成大规模、多样化的机器人演示数据，有效解决了模仿学习中的数据瓶颈，并在下游策略学习中取得了显著提升。", "motivation": "模仿学习面临大规模、多样化机器人演示数据收集的瓶颈，因为真实世界数据获取成本高昂，模拟器多样性和保真度有限且存在明显的“模拟到现实”鸿沟。现有生成模型通常只改变视觉外观而不创造新行为，或存在具身不一致性导致不合理的动作。", "method": "本文提出了AnchorDream，一个具身感知的世界模型，它将预训练的视频扩散模型重新用于机器人数据合成。AnchorDream通过机器人运动渲染来条件化扩散过程，锚定具身以防止幻觉，同时合成与机器人运动学一致的物体和环境。该方法仅需少量人工遥操作演示即可扩展生成大规模、多样化、高质量的数据集，无需显式环境建模。", "result": "实验表明，生成的数据显著改善了下游策略学习，在模拟器基准测试中相对增益达到36.4%，在真实世界研究中性能几乎翻倍。这些结果证明了该方法能够有效扩展模仿学习。", "conclusion": "将生成式世界模型与机器人运动相结合，为扩展模仿学习提供了一条实用的途径。"}}
{"id": "2512.11234", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11234", "abs": "https://arxiv.org/abs/2512.11234", "authors": ["Wentang Chen", "Shougao Zhang", "Yiman Zhang", "Tianhao Zhou", "Ruihui Li"], "title": "RoomPilot: Controllable Synthesis of Interactive Indoor Environments via Multimodal Semantic Parsing", "comment": "20 pages, 6 figures", "summary": "Generating controllable and interactive indoor scenes is fundamental to applications in game development, architectural visualization, and embodied AI training. Yet existing approaches either handle a narrow range of input modalities or rely on stochastic processes that hinder controllability. To overcome these limitations, we introduce RoomPilot, a unified framework that parses diverse multi-modal inputs--textual descriptions or CAD floor plans--into an Indoor Domain-Specific Language (IDSL) for indoor structured scene generation. The key insight is that a well-designed IDSL can act as a shared semantic representation, enabling coherent, high-quality scene synthesis from any single modality while maintaining interaction semantics. In contrast to conventional procedural methods that produce visually plausible but functionally inert layouts, RoomPilot leverages a curated dataset of interaction-annotated assets to synthesize environments exhibiting realistic object behaviors. Extensive experiments further validate its strong multi-modal understanding, fine-grained controllability in scene generation, and superior physical consistency and visual fidelity, marking a significant step toward general-purpose controllable 3D indoor scene generation.", "AI": {"tldr": "RoomPilot是一个统一框架，通过将文本描述或CAD平面图等多模态输入解析为室内领域特定语言（IDSL），实现可控、交互式的高质量室内场景生成。", "motivation": "现有方法在生成可控和交互式室内场景时，要么只能处理狭窄的输入模态，要么依赖随机过程，导致可控性受限。克服这些限制是游戏开发、建筑可视化和具身AI训练等应用的基础。", "method": "RoomPilot引入了一个室内领域特定语言（IDSL）作为共享语义表示。它能解析多模态输入（文本描述或CAD平面图），并利用IDSL进行室内结构化场景生成。与传统程序化方法不同，RoomPilot利用一个经过整理的、带有交互注释的资产数据集来合成具有真实物体行为的环境。", "result": "RoomPilot在多模态理解、场景生成的细粒度可控性、物理一致性和视觉保真度方面表现出色，并通过大量实验得到验证。", "conclusion": "RoomPilot在通用可控3D室内场景生成方面迈出了重要一步，能够从任意单模态输入生成连贯、高质量的场景，并保持交互语义。"}}
{"id": "2512.11226", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11226", "abs": "https://arxiv.org/abs/2512.11226", "authors": ["Hongbin Lin", "Yiming Yang", "Yifan Zhang", "Chaoda Zheng", "Jie Feng", "Sheng Wang", "Zhennan Wang", "Shijia Chen", "Boyang Wang", "Yu Zhang", "Xianming Liu", "Shuguang Cui", "Zhen Li"], "title": "FutureX: Enhance End-to-End Autonomous Driving via Latent Chain-of-Thought World Model", "comment": null, "summary": "In autonomous driving, end-to-end planners learn scene representations from raw sensor data and utilize them to generate a motion plan or control actions. However, exclusive reliance on the current scene for motion planning may result in suboptimal responses in highly dynamic traffic environments where ego actions further alter the future scene. To model the evolution of future scenes, we leverage the World Model to represent how the ego vehicle and its environment interact and change over time, which entails complex reasoning. The Chain of Thought (CoT) offers a promising solution by forecasting a sequence of future thoughts that subsequently guide trajectory refinement. In this paper, we propose FutureX, a CoT-driven pipeline that enhances end-to-end planners to perform complex motion planning via future scene latent reasoning and trajectory refinement. Specifically, the Auto-think Switch examines the current scene and decides whether additional reasoning is required to yield a higher-quality motion plan. Once FutureX enters the Thinking mode, the Latent World Model conducts a CoT-guided rollout to predict future scene representation, enabling the Summarizer Module to further refine the motion plan. Otherwise, FutureX operates in an Instant mode to generate motion plans in a forward pass for relatively simple scenes. Extensive experiments demonstrate that FutureX enhances existing methods by producing more rational motion plans and fewer collisions without compromising efficiency, thereby achieving substantial overall performance gains, e.g., 6.2 PDMS improvement for TransFuser on NAVSIM. Code will be released.", "AI": {"tldr": "FutureX 是一种基于思维链（CoT）的端到端自动驾驶规划器增强方案，它通过潜在世界模型预测未来场景，并根据场景复杂性决定是否进行深入推理和轨迹优化，从而在动态环境中生成更合理、更安全的运动规划，同时保持效率。", "motivation": "自动驾驶中的端到端规划器仅依赖当前场景进行运动规划，在高度动态的交通环境中可能表现不佳，因为自我车辆的行动会进一步改变未来场景。因此，需要一种方法来建模未来场景的演变并进行复杂推理。", "method": "FutureX 提出了一种 CoT 驱动的管道。它包含一个“自动思考开关”，用于判断是否需要额外推理。在“思考模式”下，潜在世界模型通过 CoT 指导的推演来预测未来场景表示，再由“总结模块”优化运动规划。在“即时模式”下，对于相对简单的场景，它直接生成运动规划。", "result": "FutureX 显著增强了现有方法，生成了更合理的运动规划，减少了碰撞，并且在不影响效率的情况下实现了显著的整体性能提升，例如在 NAVSIM 上 TransFuser 的 PDMS 提升了 6.2。", "conclusion": "FutureX 通过整合未来场景潜在推理和轨迹优化，有效地增强了端到端自动驾驶规划器，通过 CoT 驱动的世界模型在动态环境中实现了更高的安全性和规划合理性。"}}
{"id": "2512.11399", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11399", "abs": "https://arxiv.org/abs/2512.11399", "authors": ["Galann Pennec", "Zhengyuan Liu", "Nicholas Asher", "Philippe Muller", "Nancy F. Chen"], "title": "Minimal Clips, Maximum Salience: Long Video Summarization via Key Moment Extraction", "comment": null, "summary": "Vision-Language Models (VLMs) are able to process increasingly longer videos. Yet, important visual information is easily lost throughout the entire context and missed by VLMs. Also, it is important to design tools that enable cost-effective analysis of lengthy video content. In this paper, we propose a clip selection method that targets key video moments to be included in a multimodal summary. We divide the video into short clips and generate compact visual descriptions of each using a lightweight video captioning model. These are then passed to a large language model (LLM), which selects the K clips containing the most relevant visual information for a multimodal summary. We evaluate our approach on reference clips for the task, automatically derived from full human-annotated screenplays and summaries in the MovieSum dataset. We further show that these reference clips (less than 6% of the movie) are sufficient to build a complete multimodal summary of the movies in MovieSum. Using our clip selection method, we achieve a summarization performance close to that of these reference clips while capturing substantially more relevant video information than random clip selection. Importantly, we maintain low computational cost by relying on a lightweight captioning model.", "AI": {"tldr": "本文提出了一种经济高效的视频片段选择方法，通过轻量级视频字幕模型和大型语言模型，从长视频中识别并选择关键时刻，以创建多模态摘要。", "motivation": "尽管视觉语言模型（VLMs）能处理长视频，但关键视觉信息容易丢失，且需要设计工具来经济有效地分析冗长视频内容。", "method": "该方法将视频分割成短片段，使用轻量级视频字幕模型生成每个片段的简洁视觉描述。然后，这些描述被输入到大型语言模型（LLM），由LLM选择包含最相关视觉信息的K个片段，用于多模态摘要。", "result": "研究表明，选定的参考片段（少于电影的6%）足以构建完整的电影多模态摘要。本文提出的片段选择方法实现了接近这些参考片段的摘要性能，并比随机选择捕获了更多相关视频信息，同时保持了较低的计算成本。", "conclusion": "通过结合轻量级字幕模型和LLM，该方法提供了一种有效且计算成本低廉的视频片段选择方案，能够从长视频中提取关键信息以生成高质量的多模态摘要，解决了VLM在处理长视频时信息丢失和成本效益问题。"}}
{"id": "2512.11682", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.11682", "abs": "https://arxiv.org/abs/2512.11682", "authors": ["Tim Cofala", "Christian Kalfar", "Jingge Xiao", "Johanna Schrader", "Michelle Tang", "Wolfgang Nejdl"], "title": "MedAI: Evaluating TxAgent's Therapeutic Agentic Reasoning in the NeurIPS CURE-Bench Competition", "comment": "7 pages, 3 figures", "summary": "Therapeutic decision-making in clinical medicine constitutes a high-stakes domain in which AI guidance interacts with complex interactions among patient characteristics, disease processes, and pharmacological agents. Tasks such as drug recommendation, treatment planning, and adverse-effect prediction demand robust, multi-step reasoning grounded in reliable biomedical knowledge. Agentic AI methods, exemplified by TxAgent, address these challenges through iterative retrieval-augmented generation (RAG). TxAgent employs a fine-tuned Llama-3.1-8B model that dynamically generates and executes function calls to a unified biomedical tool suite (ToolUniverse), integrating FDA Drug API, OpenTargets, and Monarch resources to ensure access to current therapeutic information. In contrast to general-purpose RAG systems, medical applications impose stringent safety constraints, rendering the accuracy of both the reasoning trace and the sequence of tool invocations critical. These considerations motivate evaluation protocols treating token-level reasoning and tool-usage behaviors as explicit supervision signals. This work presents insights derived from our participation in the CURE-Bench NeurIPS 2025 Challenge, which benchmarks therapeutic-reasoning systems using metrics that assess correctness, tool utilization, and reasoning quality. We analyze how retrieval quality for function (tool) calls influences overall model performance and demonstrate performance gains achieved through improved tool-retrieval strategies. Our work was awarded the Excellence Award in Open Science. Complete information can be found at https://curebench.ai/.", "AI": {"tldr": "TxAgent是一个基于Llama-3.1-8B模型和迭代RAG的智能体AI，用于临床治疗决策，通过调用生物医学工具套件提供药物推荐和治疗计划。该研究分析了工具检索质量对模型性能的影响，并展示了通过改进工具检索策略实现的性能提升，在CURE-Bench挑战赛中获得卓越奖。", "motivation": "临床治疗决策是一个高风险领域，涉及患者特征、疾病过程和药物之间复杂的相互作用。药物推荐、治疗计划和副作用预测等任务需要可靠的生物医学知识支持的强大、多步骤推理。现有AI系统在医学应用中面临严格的安全限制，要求推理过程和工具调用序列的准确性至关重要。", "method": "本研究采用名为TxAgent的智能体AI方法，其核心是一个微调的Llama-3.1-8B模型。TxAgent通过迭代的检索增强生成（RAG）动态生成并执行对统一生物医学工具套件（ToolUniverse，整合了FDA Drug API、OpenTargets和Monarch）的函数调用。评估协议将令牌级推理和工具使用行为视为明确的监督信号，并在CURE-Bench NeurIPS 2025挑战赛中，使用评估正确性、工具利用率和推理质量的指标进行基准测试。", "result": "研究分析了函数（工具）调用的检索质量如何影响整体模型性能，并证明了通过改进工具检索策略可以实现性能提升。该工作在开放科学领域荣获卓越奖。", "conclusion": "改进的工具检索策略对于提升像TxAgent这样的治疗推理系统至关重要，能够显著提高模型在复杂临床决策任务中的性能和安全性。未来的研究应继续关注优化AI在生物医学领域的工具集成和推理能力。"}}
{"id": "2512.11374", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.11374", "abs": "https://arxiv.org/abs/2512.11374", "authors": ["Tomáš Koref", "Lena Held", "Mahammad Namazov", "Harun Kumru", "Yassine Thlija", "Christoph Burchard", "Ivan Habernal"], "title": "Mining Legal Arguments to Study Judicial Formalism", "comment": "pre-print under review", "summary": "Courts must justify their decisions, but systematically analyzing judicial reasoning at scale remains difficult. This study refutes claims about formalistic judging in Central and Eastern Europe (CEE) by developing automated methods to detect and classify judicial reasoning in Czech Supreme Courts' decisions using state-of-the-art natural language processing methods. We create the MADON dataset of 272 decisions from two Czech Supreme Courts with expert annotations of 9,183 paragraphs with eight argument types and holistic formalism labels for supervised training and evaluation. Using a corpus of 300k Czech court decisions, we adapt transformer LLMs for Czech legal domain by continued pretraining and experiment with methods to address dataset imbalance including asymmetric loss and class weighting. The best models successfully detect argumentative paragraphs (82.6\\% macro-F1), classify traditional types of legal argument (77.5\\% macro-F1), and classify decisions as formalistic/non-formalistic (83.2\\% macro-F1). Our three-stage pipeline combining ModernBERT, Llama 3.1, and traditional feature-based machine learning achieves promising results for decision classification while reducing computational costs and increasing explainability. Empirically, we challenge prevailing narratives about CEE formalism. This work shows that legal argument mining enables reliable judicial philosophy classification and shows the potential of legal argument mining for other important tasks in computational legal studies. Our methodology is easily replicable across jurisdictions, and our entire pipeline, datasets, guidelines, models, and source codes are available at https://github.com/trusthlt/madon.", "AI": {"tldr": "本研究开发了自动化方法，利用先进的自然语言处理技术，大规模检测和分类捷克最高法院判决中的司法推理，并挑战了关于中东欧形式主义司法的普遍观点。", "motivation": "大规模系统分析司法推理仍然困难；同时，研究旨在反驳关于中东欧（CEE）司法中存在形式主义的说法。", "method": "创建了MADON数据集，包含272份捷克最高法院判决，专家标注了9,183个段落的八种论证类型和整体形式主义标签。使用30万份捷克法院判决语料库，通过持续预训练调整Transformer大型语言模型（LLMs）以适应捷克法律领域。实验了包括非对称损失和类别加权在内的方法来解决数据集不平衡问题。开发了一个结合ModernBERT、Llama 3.1和传统基于特征的机器学习的三阶段流水线。", "result": "模型成功检测论证段落（82.6% macro-F1），分类传统法律论证类型（77.5% macro-F1），并将判决分类为形式主义/非形式主义（83.2% macro-F1）。三阶段流水线在降低计算成本和提高可解释性的同时，实现了有前景的判决分类结果。经验性地挑战了关于中东欧形式主义的普遍叙述。", "conclusion": "法律论证挖掘能够实现可靠的司法哲学分类，并展示了其在计算法律研究中其他重要任务的潜力。该方法易于在不同司法管辖区复制。"}}
{"id": "2512.11237", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2512.11237", "abs": "https://arxiv.org/abs/2512.11237", "authors": ["Yuxuan Han", "Xin Ming", "Tianxiao Li", "Zhuofan Shen", "Qixuan Zhang", "Lan Xu", "Feng Xu"], "title": "WildCap: Facial Appearance Capture in the Wild via Hybrid Inverse Rendering", "comment": "Technical report. project page: https://yxuhan.github.io/WildCap/index.html; code: https://github.com/yxuhan/WildCap", "summary": "Existing methods achieve high-quality facial appearance capture under controllable lighting, which increases capture cost and limits usability. We propose WildCap, a novel method for high-quality facial appearance capture from a smartphone video recorded in the wild. To disentangle high-quality reflectance from complex lighting effects in in-the-wild captures, we propose a novel hybrid inverse rendering framework. Specifically, we first apply a data-driven method, i.e., SwitchLight, to convert the captured images into more constrained conditions and then adopt model-based inverse rendering. However, unavoidable local artifacts in network predictions, such as shadow-baking, are non-physical and thus hinder accurate inverse rendering of lighting and material. To address this, we propose a novel texel grid lighting model to explain non-physical effects as clean albedo illuminated by local physical lighting. During optimization, we jointly sample a diffusion prior for reflectance maps and optimize the lighting, effectively resolving scale ambiguity between local lights and albedo. Our method achieves significantly better results than prior arts in the same capture setup, closing the quality gap between in-the-wild and controllable recordings by a large margin. Our code will be released \\href{https://yxuhan.github.io/WildCap/index.html}{\\textcolor{magenta}{here}}.", "AI": {"tldr": "WildCap是一种新颖的方法，可以在野外智能手机视频中实现高质量的面部外观捕捉，它结合了混合逆渲染框架和新颖的纹素网格光照模型，以处理复杂的野外光照和网络预测中的非物理伪影。", "motivation": "现有的面部外观捕捉方法需要在可控光照下进行，这增加了捕捉成本并限制了可用性。研究的动机是开发一种方法，能够在不受控制的野外智能手机视频中实现高质量的面部外观捕捉。", "method": "该方法提出了一种混合逆渲染框架：首先，使用数据驱动方法（SwitchLight）将捕捉到的图像转换为更受约束的条件；然后，采用基于模型的逆渲染。为解决网络预测中不可避免的局部非物理伪影（如阴影烘焙），提出了一种新颖的纹素网格光照模型，将这些非物理效应解释为由局部物理光照照亮的干净反照率。在优化过程中，联合采样反射图的扩散先验并优化光照，有效解决了局部光照和反照率之间的尺度模糊性。", "result": "WildCap在相同的捕捉设置下取得了比现有技术显著更好的结果，大大缩小了野外捕捉与可控记录之间的质量差距。", "conclusion": "该研究成功地提出了一种在野外智能手机视频中实现高质量面部外观捕捉的方法，通过其混合逆渲染框架和创新的纹素网格光照模型，有效解决了复杂光照和网络预测伪影带来的挑战，显著提升了捕捉质量。"}}
{"id": "2512.11325", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11325", "abs": "https://arxiv.org/abs/2512.11325", "authors": ["Yuhang Wang", "Zhenxing Niu", "Haoxuan Ji", "Guangyu He", "Haichang Gao", "Gang Hua"], "title": "MLLM Machine Unlearning via Visual Knowledge Distillation", "comment": null, "summary": "Recently, machine unlearning approaches have been proposed to remove sensitive information from well-trained large models. However, most existing methods are tailored for LLMs, while MLLM-oriented unlearning remains at its early stage. Inspired by recent studies exploring the internal mechanisms of MLLMs, we propose to disentangle the visual and textual knowledge embedded within MLLMs and introduce a dedicated approach to selectively erase target visual knowledge while preserving textual knowledge. Unlike previous unlearning methods that rely on output-level supervision, our approach introduces a Visual Knowledge Distillation (VKD) scheme, which leverages intermediate visual representations within the MLLM as supervision signals. This design substantially enhances both unlearning effectiveness and model utility. Moreover, since our method only fine-tunes the visual components of the MLLM, it offers significant efficiency advantages. Extensive experiments demonstrate that our approach outperforms state-of-the-art unlearning methods in terms of both effectiveness and efficiency. Moreover, we are the first to evaluate the robustness of MLLM unlearning against relearning attacks.", "AI": {"tldr": "本文提出了一种针对多模态大模型（MLLMs）的视觉知识遗忘方法，通过解耦视觉和文本知识，并利用视觉知识蒸馏（VKD）在中间视觉表示上进行监督，有效且高效地擦除目标视觉知识，同时保持文本知识，并首次评估了其抗再学习攻击的鲁棒性。", "motivation": "现有的机器遗忘方法主要针对大型语言模型（LLMs），而面向多模态大模型（MLLMs）的遗忘研究仍处于早期阶段。特别地，需要一种方法来选择性地擦除MLLMs中的目标视觉知识，同时保留文本知识。", "method": "该方法受MLLM内部机制研究的启发，提出解耦MLLM中嵌入的视觉和文本知识。引入了一种专门的方法来选择性地擦除目标视觉知识，同时保留文本知识。不同于以往依赖输出级监督的方法，本文引入了视觉知识蒸馏（VKD）方案，利用MLLM内部的中间视觉表示作为监督信号。此外，该方法仅微调MLLM的视觉组件。", "result": "实验结果表明，该方法在有效性和效率方面均优于最先进的遗忘方法。它显著增强了遗忘效果和模型实用性，并提供了显著的效率优势。此外，该研究首次评估了MLLM遗忘方法对抗再学习攻击的鲁棒性。", "conclusion": "本文提出了一种有效、高效且鲁棒的MLLM视觉知识遗忘方法，通过解耦知识并利用中间视觉表示进行监督，实现了选择性擦除，并为MLLM遗忘的鲁棒性评估开辟了新方向。"}}
{"id": "2512.11437", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.11437", "abs": "https://arxiv.org/abs/2512.11437", "authors": ["Akash Ghosh", "Srivarshinee Sridhar", "Raghav Kaushik Ravi", "Muhsin Muhsin", "Sriparna Saha", "Chirag Agarwal"], "title": "CLINIC: Evaluating Multilingual Trustworthiness in Language Models for Healthcare", "comment": "49 pages, 31 figures", "summary": "Integrating language models (LMs) in healthcare systems holds great promise for improving medical workflows and decision-making. However, a critical barrier to their real-world adoption is the lack of reliable evaluation of their trustworthiness, especially in multilingual healthcare settings. Existing LMs are predominantly trained in high-resource languages, making them ill-equipped to handle the complexity and diversity of healthcare queries in mid- and low-resource languages, posing significant challenges for deploying them in global healthcare contexts where linguistic diversity is key. In this work, we present CLINIC, a Comprehensive Multilingual Benchmark to evaluate the trustworthiness of language models in healthcare. CLINIC systematically benchmarks LMs across five key dimensions of trustworthiness: truthfulness, fairness, safety, robustness, and privacy, operationalized through 18 diverse tasks, spanning 15 languages (covering all the major continents), and encompassing a wide array of critical healthcare topics like disease conditions, preventive actions, diagnostic tests, treatments, surgeries, and medications. Our extensive evaluation reveals that LMs struggle with factual correctness, demonstrate bias across demographic and linguistic groups, and are susceptible to privacy breaches and adversarial attacks. By highlighting these shortcomings, CLINIC lays the foundation for enhancing the global reach and safety of LMs in healthcare across diverse languages.", "AI": {"tldr": "本文提出了CLINIC，一个全面的多语言基准，用于评估语言模型在医疗保健领域的可信度，涵盖真实性、公平性、安全性、鲁棒性和隐私等维度，并揭示了现有模型的多项不足。", "motivation": "语言模型在医疗保健中应用前景广阔，但缺乏对其可信度的可靠评估，尤其是在多语言环境中。现有模型主要在资源丰富的语言上训练，难以应对中低资源语言的复杂性与多样性，阻碍了其在全球医疗保健领域的部署。", "method": "研究者开发了CLINIC，一个综合性的多语言基准，通过18项任务，涵盖15种语言（遍布各大洲），并涉及疾病、预防、诊断、治疗、手术和药物等关键医疗主题。CLINIC系统地从真实性、公平性、安全性、鲁棒性和隐私五个关键维度评估语言模型的可信度。", "result": "广泛评估结果显示，语言模型在事实准确性方面表现不佳，在人口统计和语言群体之间存在偏见，并且容易受到隐私泄露和对抗性攻击。", "conclusion": "CLINIC通过揭示这些不足，为增强语言模型在全球医疗保健领域中跨语言的覆盖范围和安全性奠定了基础。"}}
{"id": "2512.11350", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11350", "abs": "https://arxiv.org/abs/2512.11350", "authors": ["Tanu Singh", "Pranamesh Chakraborty", "Long T. Truong"], "title": "Surveillance Video-Based Traffic Accident Detection Using Transformer Architecture", "comment": null, "summary": "Road traffic accidents represent a leading cause of mortality globally, with incidence rates rising due to increasing population, urbanization, and motorization. Rising accident rates raise concerns about traffic surveillance effectiveness. Traditional computer vision methods for accident detection struggle with limited spatiotemporal understanding and poor cross-domain generalization. Recent advances in transformer architectures excel at modeling global spatial-temporal dependencies and parallel computation. However, applying these models to automated traffic accident detection is limited by small, non-diverse datasets, hindering the development of robust, generalizable systems. To address this gap, we curated a comprehensive and balanced dataset that captures a wide spectrum of traffic environments, accident types, and contextual variations. Utilizing the curated dataset, we propose an accident detection model based on a transformer architecture using pre-extracted spatial video features. The architecture employs convolutional layers to extract local correlations across diverse patterns within a frame, while leveraging transformers to capture sequential-temporal dependencies among the retrieved features. Moreover, most existing studies neglect the integration of motion cues, which are essential for understanding dynamic scenes, especially during accidents. These approaches typically rely on static features or coarse temporal information. In this study, multiple methods for incorporating motion cues were evaluated to identify the most effective strategy. Among the tested input approaches, concatenating RGB features with optical flow achieved the highest accuracy at 88.3%. The results were further compared with vision language models (VLM) such as GPT, Gemini, and LLaVA-NeXT-Video to assess the effectiveness of the proposed method.", "AI": {"tldr": "该研究针对交通事故检测中数据不足和传统方法局限性问题，构建了一个综合平衡的数据集，并提出了一种结合卷积层和Transformer架构的事故检测模型，通过整合光流等运动线索，显著提高了检测准确率，并与视觉语言模型进行了比较。", "motivation": "交通事故是全球主要死亡原因，发生率随人口增长和城市化而上升，但现有计算机视觉方法在时空理解和跨域泛化方面存在不足。此外，缺乏多样化数据集阻碍了鲁棒系统的开发，且大多数现有研究忽略了对理解动态场景至关重要的运动线索。", "method": "研究首先策划了一个全面且平衡的数据集，涵盖多种交通环境和事故类型。然后，提出了一种基于Transformer架构的事故检测模型，该模型利用预提取的空间视频特征，通过卷积层提取帧内局部关联，并利用Transformer捕获特征间的时序依赖性。此外，研究评估了多种整合运动线索的方法，以找到最有效的策略，并与GPT、Gemini、LLaVA-NeXT-Video等视觉语言模型进行了比较。", "result": "在测试的输入方法中，将RGB特征与光流拼接融合实现了最高的准确率，达到88.3%。研究结果还与视觉语言模型进行了比较，以评估所提出方法的有效性。", "conclusion": "该研究通过构建综合数据集和提出结合卷积与Transformer架构的模型，并有效整合光流等运动线索，显著提升了交通事故检测的准确性，为开发更通用、鲁棒的交通监控系统提供了有效途径。"}}
{"id": "2512.11438", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11438", "abs": "https://arxiv.org/abs/2512.11438", "authors": ["Tariq Berrada Ifriqi", "John Nguyen", "Karteek Alahari", "Jakob Verbeek", "Ricky T. Q. Chen"], "title": "Flowception: Temporally Expansive Flow Matching for Video Generation", "comment": null, "summary": "We present Flowception, a novel non-autoregressive and variable-length video generation framework. Flowception learns a probability path that interleaves discrete frame insertions with continuous frame denoising. Compared to autoregressive methods, Flowception alleviates error accumulation/drift as the frame insertion mechanism during sampling serves as an efficient compression mechanism to handle long-term context. Compared to full-sequence flows, our method reduces FLOPs for training three-fold, while also being more amenable to local attention variants, and allowing to learn the length of videos jointly with their content. Quantitative experimental results show improved FVD and VBench metrics over autoregressive and full-sequence baselines, which is further validated with qualitative results. Finally, by learning to insert and denoise frames in a sequence, Flowception seamlessly integrates different tasks such as image-to-video generation and video interpolation.", "AI": {"tldr": "Flowception是一种新颖的非自回归、可变长度视频生成框架，通过交错的帧插入和去噪机制，解决了自回归方法的误差累积和全序列流的计算效率问题，并在性能上超越了现有基线。", "motivation": "现有视频生成方法存在局限：自回归方法容易出现误差累积和漂移，尤其在生成长视频时；全序列流方法计算成本高（FLOPs），且通常难以处理可变长度视频和局部注意力变体。", "method": "Flowception学习一个概率路径，该路径将离散的帧插入与连续的帧去噪操作交错进行。这种非自回归框架允许生成可变长度视频，并在采样过程中通过帧插入机制有效压缩处理长期上下文。", "result": "实验结果显示，Flowception在FVD和VBench指标上优于自回归和全序列基线，并通过定性结果得到进一步验证。与全序列流相比，其训练FLOPs减少了三倍。此外，Flowception能无缝集成图像到视频生成和视频插值等不同任务。", "conclusion": "Flowception成功提出了一个高效且性能优越的非自回归、可变长度视频生成框架，有效解决了传统方法的缺点，并展现了在多种视频生成任务中的强大通用性。"}}
{"id": "2512.11239", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11239", "abs": "https://arxiv.org/abs/2512.11239", "authors": ["Wen-Jue He", "Xiaofeng Zhu", "Zheng Zhang"], "title": "Cross-modal Prompting for Balanced Incomplete Multi-modal Emotion Recognition", "comment": "Accepted by AAAI 2026", "summary": "Incomplete multi-modal emotion recognition (IMER) aims at understanding human intentions and sentiments by comprehensively exploring the partially observed multi-source data. Although the multi-modal data is expected to provide more abundant information, the performance gap and modality under-optimization problem hinder effective multi-modal learning in practice, and are exacerbated in the confrontation of the missing data. To address this issue, we devise a novel Cross-modal Prompting (ComP) method, which emphasizes coherent information by enhancing modality-specific features and improves the overall recognition accuracy by boosting each modality's performance. Specifically, a progressive prompt generation module with a dynamic gradient modulator is proposed to produce concise and consistent modality semantic cues. Meanwhile, cross-modal knowledge propagation selectively amplifies the consistent information in modality features with the delivered prompts to enhance the discrimination of the modality-specific output. Additionally, a coordinator is designed to dynamically re-weight the modality outputs as a complement to the balance strategy to improve the model's efficacy. Extensive experiments on 4 datasets with 7 SOTA methods under different missing rates validate the effectiveness of our proposed method.", "AI": {"tldr": "本文提出了一种名为跨模态提示（ComP）的新方法，用于解决不完整多模态情感识别（IMER）中因数据缺失导致的性能差距和模态优化不足问题，通过增强模态特定特征和动态平衡模态输出，有效提升识别准确性。", "motivation": "多模态数据虽然信息丰富，但在不完整多模态情感识别（IMER）中，模态间的性能差距和模态优化不足问题，尤其是在面对数据缺失时，严重阻碍了有效的多模态学习。", "method": "本文提出跨模态提示（ComP）方法。具体包括：1) 一个带有动态梯度调制器的渐进式提示生成模块，用于生成简洁一致的模态语义提示；2) 跨模态知识传播，利用生成的提示选择性地放大模态特征中的一致信息，以增强模态特定输出的判别力；3) 一个协调器，动态重新加权模态输出，作为平衡策略的补充，以提高模型效率。", "result": "在4个数据集上，与7种最先进的方法在不同缺失率下进行了广泛实验，验证了所提出方法的有效性。", "conclusion": "所提出的ComP方法通过增强模态特定特征和动态平衡模态输出，有效解决了不完整多模态情感识别中的挑战，显著提高了识别准确性。"}}
{"id": "2512.11253", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11253", "abs": "https://arxiv.org/abs/2512.11253", "authors": ["Zhiyuan Li", "Chi-Man Pun", "Chen Fang", "Jue Wang", "Xiaodong Cun"], "title": "PersonaLive! Expressive Portrait Image Animation for Live Streaming", "comment": null, "summary": "Current diffusion-based portrait animation models predominantly focus on enhancing visual quality and expression realism, while overlooking generation latency and real-time performance, which restricts their application range in the live streaming scenario. We propose PersonaLive, a novel diffusion-based framework towards streaming real-time portrait animation with multi-stage training recipes. Specifically, we first adopt hybrid implicit signals, namely implicit facial representations and 3D implicit keypoints, to achieve expressive image-level motion control. Then, a fewer-step appearance distillation strategy is proposed to eliminate appearance redundancy in the denoising process, greatly improving inference efficiency. Finally, we introduce an autoregressive micro-chunk streaming generation paradigm equipped with a sliding training strategy and a historical keyframe mechanism to enable low-latency and stable long-term video generation. Extensive experiments demonstrate that PersonaLive achieves state-of-the-art performance with up to 7-22x speedup over prior diffusion-based portrait animation models.", "AI": {"tldr": "PersonaLive是一种新型的基于扩散的肖像动画框架，通过混合隐式信号、外观蒸馏和自回归流式生成范式，实现了流媒体实时肖像动画，显著提升了推理速度和长期视频的稳定性。", "motivation": "当前的基于扩散的肖像动画模型主要关注视觉质量和表达真实感，但忽略了生成延迟和实时性能，这限制了它们在直播场景中的应用。", "method": "该研究首先采用混合隐式信号（隐式面部表示和3D隐式关键点）来实现富有表现力的图像级运动控制。其次，提出了一种更少步骤的外观蒸馏策略，以消除去噪过程中的外观冗余，从而大幅提高推理效率。最后，引入了一种配备滑动训练策略和历史关键帧机制的自回归微块流式生成范式，以实现低延迟和稳定的长期视频生成。", "result": "PersonaLive在性能上达到了最先进水平，与之前的基于扩散的肖像动画模型相比，速度提升了7-22倍。", "conclusion": "PersonaLive通过多阶段训练方法，成功解决了现有扩散模型在实时性能和延迟方面的不足，使其能够应用于直播等对实时性要求高的场景，并实现了高质量、低延迟的肖像动画。"}}
{"id": "2512.11485", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.11485", "abs": "https://arxiv.org/abs/2512.11485", "authors": ["Xuanbo Su", "Yingfang Zhang", "Hao Luo", "Xiaoteng Liu", "Leo Huang"], "title": "Mistake Notebook Learning: Selective Batch-Wise Context Optimization for In-Context Learning", "comment": null, "summary": "Large language models (LLMs) adapt to tasks via gradient fine-tuning (heavy computation, catastrophic forgetting) or In-Context Learning (ICL: low robustness, poor mistake learning). To fix this, we introduce Mistake Notebook Learning (MNL), a training-free framework with a persistent knowledge base of abstracted error patterns. Unlike prior instance/single-trajectory memory methods, MNL uses batch-wise error abstraction: it extracts generalizable guidance from multiple failures, stores insights in a dynamic notebook, and retains only baseline-outperforming guidance via hold-out validation (ensuring monotonic improvement). We show MNL nearly matches Supervised Fine-Tuning (93.9% vs 94.3% on GSM8K) and outperforms training-free alternatives on GSM8K, Spider, AIME, and KaggleDBQA. On KaggleDBQA (Qwen3-8B), MNL hits 28% accuracy (47% relative gain), outperforming Memento (15.1%) and Training-Free GRPO (22.1) - proving it's a strong training-free alternative for complex reasoning.", "AI": {"tldr": "Mistake Notebook Learning (MNL) 是一种无需训练的框架，它通过抽象错误模式并存储在动态知识库中来提高大型语言模型（LLMs）的性能，解决了传统微调和上下文学习的缺点，并在多个任务上表现出色，接近监督微调。", "motivation": "现有的大型语言模型适应任务的方法存在问题：梯度微调计算量大且容易灾难性遗忘；上下文学习（ICL）鲁棒性差且学习错误的能力不足。本研究旨在提供一种更有效、更鲁棒的替代方案。", "method": "MNL 引入了一个持久的、包含抽象错误模式的知识库。它采用批处理错误抽象，从多个失败中提取可泛化的指导，将这些见解存储在一个动态的“错误笔记本”中。通过留出验证，MNL 仅保留那些性能优于基线的指导，从而确保模型性能的单调提升。该框架是无需训练的。", "result": "MNL 在 GSM8K 上的性能（93.9%）几乎与监督微调（94.3%）持平。它在 GSM8K、Spider、AIME 和 KaggleDBQA 等任务上均优于其他无需训练的方法。特别是在 KaggleDBQA (Qwen3-8B) 上，MNL 达到了 28% 的准确率（相对提升 47%），超过了 Memento (15.1%) 和 Training-Free GRPO (22.1%)。", "conclusion": "MNL 是一种强大的无需训练的复杂推理替代方案，它能够显著提升大型语言模型的性能，并且在效率和鲁棒性方面优于现有的无需训练方法，甚至可以与监督微调的性能相媲美。"}}
{"id": "2512.11502", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.11502", "abs": "https://arxiv.org/abs/2512.11502", "authors": ["Kai Golan Hashiloni", "Brenda Kasabe Nokai", "Michal Shevach", "Esthy Shemesh", "Ronit Bartin", "Anna Bergrin", "Liran Harel", "Nachum Dershowitz", "Liat Nadai Arad", "Kfir Bar"], "title": "Building Patient Journeys in Hebrew: A Language Model for Clinical Timeline Extraction", "comment": "In Proceedings of the Workshop on Large Language Models and Generative AI for Health Informatics 2025, IJCAI 2025, Montreal, Canada", "summary": "We present a new Hebrew medical language model designed to extract structured clinical timelines from electronic health records, enabling the construction of patient journeys. Our model is based on DictaBERT 2.0 and continually pre-trained on over five million de-identified hospital records. To evaluate its effectiveness, we introduce two new datasets -- one from internal medicine and emergency departments, and another from oncology -- annotated for event temporal relations. Our results show that our model achieves strong performance on both datasets. We also find that vocabulary adaptation improves token efficiency and that de-identification does not compromise downstream performance, supporting privacy-conscious model development. The model is made available for research use under ethical restrictions.", "AI": {"tldr": "本文介绍了一种新的希伯来语医学语言模型，该模型基于DictaBERT 2.0，并利用去识别化的医院记录进行持续预训练，旨在从电子健康记录中提取结构化的临床时间线，以构建患者旅程。该模型在两个新数据集上表现出色，并支持隐私保护的开发。", "motivation": "研究动机是为了能够从电子健康记录中提取结构化的临床时间线，从而构建完整的患者旅程，这对于理解和分析患者的医疗历史至关重要。", "method": "该模型基于DictaBERT 2.0，并使用超过五百万份去识别化的医院记录进行持续预训练。为评估其有效性，研究引入了两个新的、带有事件时间关系标注的数据集，分别来自内科、急诊科和肿瘤科。", "result": "研究结果显示，该模型在两个数据集上均取得了强大的性能。此外，词汇适应性提高了标记效率，并且去识别化处理并未损害下游任务的性能，这支持了在保护隐私的前提下进行模型开发。", "conclusion": "该研究成功开发了一个用于从希伯来语电子健康记录中提取临床时间线的有效医学语言模型。该模型在性能和隐私保护方面均表现出色，并已开放供研究使用。"}}
{"id": "2512.11260", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11260", "abs": "https://arxiv.org/abs/2512.11260", "authors": ["Ali El Bellaj", "Mohammed-Amine Cheddadi", "Rhassan Berber"], "title": "Do We Need Reformer for Vision? An Experimental Comparison with Vision Transformers", "comment": null, "summary": "Transformers have recently demonstrated strong performance in computer vision, with Vision Transformers (ViTs) leveraging self-attention to capture both low-level and high-level image features. However, standard ViTs remain computationally expensive, since global self-attention scales quadratically with the number of tokens, which limits their practicality for high-resolution inputs and resource-constrained settings.\n  In this work, we investigate the Reformer architecture as an alternative vision backbone. By combining patch-based tokenization with locality-sensitive hashing (LSH) attention, our model approximates global self-attention while reducing its theoretical time complexity from $\\mathcal{O}(n^2)$ to $\\mathcal{O}(n \\log n)$ in the sequence length $n$. We evaluate the proposed Reformer-based vision model on CIFAR-10 to assess its behavior on small-scale datasets, on ImageNet-100 to study its accuracy--efficiency trade-off in a more realistic setting, and on a high-resolution medical imaging dataset to evaluate the model under longer token sequences.\n  While the Reformer achieves higher accuracy on CIFAR-10 compared to our ViT-style baseline, the ViT model consistently outperforms the Reformer in our experiments in terms of practical efficiency and end-to-end computation time across the larger and higher-resolution settings. These results suggest that, despite the theoretical advantages of LSH-based attention, meaningful computation gains require sequence lengths substantially longer than those produced by typical high-resolution images.", "AI": {"tldr": "本文研究了Reformer作为视觉骨干网络，旨在降低ViT的计算成本，但结果显示，在实际应用中，其理论上的效率优势并未在典型图像分辨率下转化为显著的计算增益。", "motivation": "标准Vision Transformers (ViTs) 的全局自注意力机制计算成本高昂，其复杂度随token数量呈二次方增长，限制了其在高分辨率输入和资源受限环境下的应用。", "method": "本文将Reformer架构作为替代视觉骨干网络进行研究，通过结合基于patch的token化和局部敏感哈希 (LSH) 注意力机制，近似全局自注意力，并将理论时间复杂度从O(n²)降低到O(n log n)。模型在CIFAR-10、ImageNet-100和高分辨率医学影像数据集上进行了评估。", "result": "Reformer在CIFAR-10上取得了比ViT基线更高的准确率。然而，在更大规模和更高分辨率的设置中，ViT模型在实际效率和端到端计算时间方面始终优于Reformer。", "conclusion": "尽管基于LSH的注意力机制具有理论上的优势，但要实现有意义的计算增益，所需的序列长度必须远超典型高分辨率图像所产生的序列长度。"}}
{"id": "2512.11458", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11458", "abs": "https://arxiv.org/abs/2512.11458", "authors": ["Jingmin Zhu", "Anqi Zhu", "Hossein Rahmani", "Jun Liu", "Mohammed Bennamoun", "Qiuhong Ke"], "title": "Boosting Skeleton-based Zero-Shot Action Recognition with Training-Free Test-Time Adaptation", "comment": null, "summary": "We introduce Skeleton-Cache, the first training-free test-time adaptation framework for skeleton-based zero-shot action recognition (SZAR), aimed at improving model generalization to unseen actions during inference. Skeleton-Cache reformulates inference as a lightweight retrieval process over a non-parametric cache that stores structured skeleton representations, combining both global and fine-grained local descriptors. To guide the fusion of descriptor-wise predictions, we leverage the semantic reasoning capabilities of large language models (LLMs) to assign class-specific importance weights. By integrating these structured descriptors with LLM-guided semantic priors, Skeleton-Cache dynamically adapts to unseen actions without any additional training or access to training data. Extensive experiments on NTU RGB+D 60/120 and PKU-MMD II demonstrate that Skeleton-Cache consistently boosts the performance of various SZAR backbones under both zero-shot and generalized zero-shot settings. The code is publicly available at https://github.com/Alchemist0754/Skeleton-Cache.", "AI": {"tldr": "Skeleton-Cache是一种无需训练的测试时自适应框架，通过非参数缓存和LLM引导的语义先验，提升骨架零样本动作识别对未见动作的泛化能力。", "motivation": "旨在提高骨架零样本动作识别（SZAR）模型在推理时对未见动作的泛化能力。", "method": "将推理重构为基于非参数缓存的轻量级检索过程，该缓存存储结合全局和细粒度局部描述符的结构化骨架表示。利用大型语言模型（LLM）的语义推理能力分配类别特定的重要性权重，以指导描述符预测的融合。通过整合这些结构化描述符与LLM引导的语义先验，实现无需额外训练或访问训练数据即可动态适应未见动作。", "result": "在NTU RGB+D 60/120和PKU-MMD II数据集上，Skeleton-Cache在零样本和广义零样本设置下，持续提升了各种SZAR骨干网络的性能。", "conclusion": "Skeleton-Cache是一个有效的无需训练的测试时自适应框架，通过结合结构化骨架描述符和LLM引导的语义先验，显著增强了骨架零样本动作识别模型对未见动作的泛化能力。"}}
{"id": "2512.11509", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11509", "abs": "https://arxiv.org/abs/2512.11509", "authors": ["Mohor Banerjee", "Nadya Yuki Wangsajaya", "Syed Ali Redha Alsagoff", "Min Sen Tan", "Zachary Choy Kit Chun", "Alvin Chan Guo Wei"], "title": "Does Less Hallucination Mean Less Creativity? An Empirical Investigation in LLMs", "comment": null, "summary": "Large Language Models (LLMs) exhibit remarkable capabilities in natural language understanding and reasoning, but suffer from hallucination: the generation of factually incorrect content. While numerous methods have been developed to reduce hallucinations, their impact on creative generations remains unexplored. This gap is particularly critical for AI-assisted scientific discovery, which requires both factual accuracy and creative hypothesis generation. We investigate how three hallucination-reduction techniques: Chain of Verification (CoVe), Decoding by Contrasting Layers (DoLa), and Retrieval-Augmented Generation (RAG), affect creativity in LLMs. Evaluating multiple model families (LLaMA, Qwen, Mistral) at varying scales (1B - 70B parameters) on two creativity benchmarks (NeoCoder and CS4), we find that these methods have opposing effects on divergent creativity. CoVe enhances divergent thinking, DoLa suppresses it, and RAG shows minimal impact. Our findings provide guidance for selecting appropriate hallucination-reduction methods in scientific applications, where the balance between factual accuracy and creative exploration is crucial.", "AI": {"tldr": "该研究调查了三种幻觉减少技术（CoVe、DoLa、RAG）对大型语言模型（LLM）发散性创造力的影响。结果发现CoVe能增强创造力，DoLa会抑制创造力，而RAG影响甚微。", "motivation": "大型语言模型虽能力强大但存在幻觉问题。现有减少幻觉的方法对创造性生成的影响尚不明确，这对于需要兼顾事实准确性和创造性假设的AI辅助科学发现至关重要。", "method": "研究人员考察了三种幻觉减少技术：CoVe（验证链）、DoLa（通过对比层解码）和RAG（检索增强生成）。他们使用多个模型家族（LLaMA、Qwen、Mistral）及其不同规模（1B - 70B参数），并在两个创造力基准（NeoCoder和CS4）上评估了这些方法对LLM发散性创造力的影响。", "result": "研究发现这些幻觉减少方法对发散性创造力有相反的影响。具体而言，CoVe增强了发散性思维，DoLa抑制了发散性思维，而RAG的影响最小。", "conclusion": "这些发现为在科学应用中选择合适的幻觉减少方法提供了指导，尤其是在事实准确性和创造性探索之间需要权衡的情况下。"}}
{"id": "2512.11267", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11267", "abs": "https://arxiv.org/abs/2512.11267", "authors": ["Rezwana Sultana", "Manzur Murshed", "Kathryn Sheffield", "Singarayer Florentine", "Tsz-Kwan Lee", "Shyh Wei Teng"], "title": "Evaluating the Efficacy of Sentinel-2 versus Aerial Imagery in Serrated Tussock Classification", "comment": "Accepted in Earthsense 2025 (IEEE INTERNATIONAL CONFERENCE ON NEXT-GEN TECHNOLOGIES OF ARTIFICIAL INTELLIGENCE AND GEOSCIENCE REMOTE SENSING)", "summary": "Invasive species pose major global threats to ecosystems and agriculture. Serrated tussock (\\textit{Nassella trichotoma}) is a highly competitive invasive grass species that disrupts native grasslands, reduces pasture productivity, and increases land management costs. In Victoria, Australia, it presents a major challenge due to its aggressive spread and ecological impact. While current ground surveys and subsequent management practices are effective at small scales, they are not feasible for landscape-scale monitoring. Although aerial imagery offers high spatial resolution suitable for detailed classification, its high cost limits scalability. Satellite-based remote sensing provides a more cost-effective and scalable alternative, though often with lower spatial resolution. This study evaluates whether multi-temporal Sentinel-2 imagery, despite its lower spatial resolution, can provide a comparable and cost-effective alternative for landscape-scale monitoring of serrated tussock by leveraging its higher spectral resolution and seasonal phenological information. A total of eleven models have been developed using various combinations of spectral bands, texture features, vegetation indices, and seasonal data. Using a random forest classifier, the best-performing Sentinel-2 model (M76*) has achieved an Overall Accuracy (OA) of 68\\% and an Overall Kappa (OK) of 0.55, slightly outperforming the best-performing aerial imaging model's OA of 67\\% and OK of 0.52 on the same dataset. These findings highlight the potential of multi-seasonal feature-enhanced satellite-based models for scalable invasive species classification.", "AI": {"tldr": "本研究评估了多时相Sentinel-2卫星影像在景观尺度上监测入侵物种锯齿草的可能性，发现其性能略优于航空影像，为可扩展的入侵物种分类提供了经济有效的替代方案。", "motivation": "入侵物种对生态系统和农业构成全球威胁。锯齿草在澳大利亚维多利亚州广泛传播，破坏本地草原并降低牧场生产力。现有地面调查和航空影像成本高昂，难以进行景观尺度的监测，因此需要一种经济有效且可扩展的替代方案。", "method": "研究利用多时相Sentinel-2影像，结合光谱波段、纹理特征、植被指数和季节性数据，开发了11个模型。采用随机森林分类器对这些模型进行评估，并与航空影像模型的性能进行比较。", "result": "表现最佳的Sentinel-2模型（M76*）实现了68%的总体准确率（OA）和0.55的Kappa系数（OK），略优于在相同数据集上表现最佳的航空影像模型（67% OA和0.52 OK）。", "conclusion": "研究结果表明，结合多季节特征的卫星模型在可扩展的入侵物种分类方面具有巨大潜力，为景观尺度的监测提供了经济有效的解决方案。"}}
{"id": "2512.11573", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.11573", "abs": "https://arxiv.org/abs/2512.11573", "authors": ["Paulius Rauba", "Qiyao Wei", "Mihaela van der Schaar"], "title": "Visualizing token importance for black-box language models", "comment": null, "summary": "We consider the problem of auditing black-box large language models (LLMs) to ensure they behave reliably when deployed in production settings, particularly in high-stakes domains such as legal, medical, and regulatory compliance. Existing approaches for LLM auditing often focus on isolated aspects of model behavior, such as detecting specific biases or evaluating fairness. We are interested in a more general question -- can we understand how the outputs of black-box LLMs depend on each input token? There is a critical need to have such tools in real-world applications that rely on inaccessible API endpoints to language models. However, this is a highly non-trivial problem, as LLMs are stochastic functions (i.e. two outputs will be different by chance), while computing prompt-level gradients to approximate input sensitivity is infeasible. To address this, we propose Distribution-Based Sensitivity Analysis (DBSA), a lightweight model-agnostic procedure to evaluate the sensitivity of the output of a language model for each input token, without making any distributional assumptions about the LLM. DBSA is developed as a practical tool for practitioners, enabling quick, plug-and-play visual exploration of LLMs reliance on specific input tokens. Through illustrative examples, we demonstrate how DBSA can enable users to inspect LLM inputs and find sensitivities that may be overlooked by existing LLM interpretability methods.", "AI": {"tldr": "本文提出了一种名为DBSA的轻量级、模型无关方法，用于审计黑盒大型语言模型（LLM），以评估其输出对每个输入token的敏感性，从而帮助用户识别现有方法可能忽略的依赖性。", "motivation": "在法律、医疗等高风险领域，确保部署的黑盒LLM行为可靠至关重要。现有LLM审计方法通常侧重于特定方面（如偏见），而缺乏一种通用工具来理解黑盒LLM的输出如何依赖于每个输入token，尤其是在无法访问API端点和LLM具有随机性、梯度计算不可行的情况下。", "method": "本文提出了一种名为“基于分布的敏感性分析”（Distribution-Based Sensitivity Analysis, DBSA）的方法。这是一种轻量级、模型无关的程序，用于评估语言模型输出对每个输入token的敏感性，且不作任何关于LLM的分布假设。DBSA被设计为一种实用的工具，支持即插即用的可视化探索。", "result": "通过示例，研究展示了DBSA如何帮助用户检查LLM输入并发现现有LLM可解释性方法可能忽略的敏感性。", "conclusion": "DBSA提供了一个实用的、可视化工具，使从业者能够快速、便捷地探索黑盒LLM对特定输入token的依赖性，从而提高LLM在生产环境中的可靠性审计能力。"}}
{"id": "2512.11635", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.11635", "abs": "https://arxiv.org/abs/2512.11635", "authors": ["Keerthana Murugaraj", "Salima Lamsiyah", "Marten During", "Martin Theobald"], "title": "Automating Historical Insight Extraction from Large-Scale Newspaper Archives via Neural Topic Modeling", "comment": "This is a preprint of a manuscript submitted to Digital Scholarship in the Humanities (Oxford University Press). The paper is currently under peer review", "summary": "Extracting coherent and human-understandable themes from large collections of unstructured historical newspaper archives presents significant challenges due to topic evolution, Optical Character Recognition (OCR) noise, and the sheer volume of text. Traditional topic-modeling methods, such as Latent Dirichlet Allocation (LDA), often fall short in capturing the complexity and dynamic nature of discourse in historical texts. To address these limitations, we employ BERTopic. This neural topic-modeling approach leverages transformerbased embeddings to extract and classify topics, which, despite its growing popularity, still remains underused in historical research. Our study focuses on articles published between 1955 and 2018, specifically examining discourse on nuclear power and nuclear safety. We analyze various topic distributions across the corpus and trace their temporal evolution to uncover long-term trends and shifts in public discourse. This enables us to more accurately explore patterns in public discourse, including the co-occurrence of themes related to nuclear power and nuclear weapons and their shifts in topic importance over time. Our study demonstrates the scalability and contextual sensitivity of BERTopic as an alternative to traditional approaches, offering richer insights into historical discourses extracted from newspaper archives. These findings contribute to historical, nuclear, and social-science research while reflecting on current limitations and proposing potential directions for future work.", "AI": {"tldr": "本研究利用BERTopic模型分析1955-2018年间报纸档案中关于核能和核安全的论述，揭示了主题演变、长期趋势和公共论述的转变，并证明了BERTopic在历史文本分析中的优越性。", "motivation": "从大量非结构化历史报纸档案中提取连贯且易于理解的主题面临巨大挑战，包括主题演变、OCR噪声和文本量庞大。传统的LDA等主题建模方法难以捕捉历史文本的复杂性和动态性，尤其在历史研究中，BERTopic等先进方法仍未得到充分利用。", "method": "研究采用BERTopic这一神经主题建模方法，该方法利用基于Transformer的嵌入来提取和分类主题。分析对象是1955年至2018年间发表的，关于核能和核安全的报纸文章。通过分析语料库中的各种主题分布及其时间演变，以揭示公共论述中的长期趋势和转变。", "result": "研究揭示了语料库中各种主题分布及其时间演变，发现了公共论述中的长期趋势和转变，包括核能与核武器相关主题的共现及其重要性随时间的推移而变化。结果表明BERTopic作为传统方法的替代方案，具有良好的可扩展性和上下文敏感性，能为从报纸档案中提取的历史论述提供更丰富的见解。", "conclusion": "BERTopic为历史文本分析提供了比传统方法更深入的洞察，为历史学、核研究和社会科学研究做出了贡献。研究同时反思了当前局限性并提出了未来工作的潜在方向。"}}
{"id": "2512.11560", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11560", "abs": "https://arxiv.org/abs/2512.11560", "authors": ["Marcel Dreier", "Nora Gourmelon", "Dakota Pyles", "Fei Wu", "Matthias Braun", "Thorsten Seehaus", "Andreas Maier", "Vincent Christlein"], "title": "Multi-temporal Calving Front Segmentation", "comment": null, "summary": "The calving fronts of marine-terminating glaciers undergo constant changes. These changes significantly affect the glacier's mass and dynamics, demanding continuous monitoring. To address this need, deep learning models were developed that can automatically delineate the calving front in Synthetic Aperture Radar imagery. However, these models often struggle to correctly classify areas affected by seasonal conditions such as ice melange or snow-covered surfaces. To address this issue, we propose to process multiple frames from a satellite image time series of the same glacier in parallel and exchange temporal information between the corresponding feature maps to stabilize each prediction. We integrate our approach into the current state-of-the-art architecture Tyrion and accomplish a new state-of-the-art performance on the CaFFe benchmark dataset. In particular, we achieve a Mean Distance Error of 184.4 m and a mean Intersection over Union of 83.6.", "AI": {"tldr": "该研究提出一种深度学习方法，通过并行处理合成孔径雷达（SAR）图像时间序列中的多帧并交换时间信息，以更准确地勾勒海洋终结冰川的崩解锋线，克服了季节性条件带来的挑战，并在CaFFe数据集上实现了最先进的性能。", "motivation": "海洋终结冰川的崩解锋线持续变化，显著影响冰川质量和动力学，因此需要持续监测。现有深度学习模型在处理冰混杂物或积雪覆盖表面等季节性条件时，难以准确分类相关区域。", "method": "提出并行处理同一冰川卫星图像时间序列中的多帧，并在相应的特征图之间交换时间信息以稳定每次预测。该方法被整合到当前最先进的Tyrion架构中。", "result": "在CaFFe基准数据集上实现了新的最先进性能，平均距离误差（MDE）为184.4米，平均交并比（IoU）为83.6。", "conclusion": "通过在深度学习模型中引入多帧并行处理和时间信息交换，有效解决了季节性条件对冰川崩解锋线自动识别的影响，显著提升了模型的性能和稳定性，达到了该领域的最新水平。"}}
{"id": "2512.11558", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.11558", "abs": "https://arxiv.org/abs/2512.11558", "authors": ["Zhenyang Cai", "Jiaming Zhang", "Junjie Zhao", "Ziyi Zeng", "Yanchao Li", "Jingyi Liang", "Junying Chen", "Yunjin Yang", "Jiajun You", "Shuzhi Deng", "Tongfei Wang", "Wanting Chen", "Chunxiu Hao", "Ruiqi Xie", "Zhenwei Wen", "Xiangyi Feng", "Zou Ting", "Jin Zou Lin", "Jianquan Li", "Guangjun Yu", "Liangyi Chen", "Junwen Wang", "Shan Jiang", "Benyou Wang"], "title": "DentalGPT: Incentivizing Multimodal Complex Reasoning in Dentistry", "comment": null, "summary": "Reliable interpretation of multimodal data in dentistry is essential for automated oral healthcare, yet current multimodal large language models (MLLMs) struggle to capture fine-grained dental visual details and lack sufficient reasoning ability for precise diagnosis. To address these limitations, we present DentalGPT, a specialized dental MLLM developed through high-quality domain knowledge injection and reinforcement learning. Specifically, the largest annotated multimodal dataset for dentistry to date was constructed by aggregating over 120k dental images paired with detailed descriptions that highlight diagnostically relevant visual features, making it the multimodal dataset with the most extensive collection of dental images to date. Training on this dataset significantly enhances the MLLM's visual understanding of dental conditions, while the subsequent reinforcement learning stage further strengthens its capability for multimodal complex reasoning. Comprehensive evaluations on intraoral and panoramic benchmarks, along with dental subsets of medical VQA benchmarks, show that DentalGPT achieves superior performance in disease classification and dental VQA tasks, outperforming many state-of-the-art MLLMs despite having only 7B parameters. These results demonstrate that high-quality dental data combined with staged adaptation provides an effective pathway for building capable and domain-specialized dental MLLMs.", "AI": {"tldr": "DentalGPT是一种专门的牙科多模态大语言模型（MLLM），通过高质量的牙科数据注入和强化学习，解决了现有MLLM在牙科细节理解和精确诊断方面的不足，并在牙科任务中取得了卓越表现。", "motivation": "当前的通用多模态大语言模型（MLLM）难以捕捉牙科图像中精细的视觉细节，并且缺乏进行精确诊断所需的充分推理能力，这限制了它们在自动化口腔医疗中的应用。", "method": "研究人员构建了迄今为止最大的牙科多模态标注数据集（超过12万张牙科图像，配有详细描述和诊断相关视觉特征），并通过该数据集训练增强了MLLM的视觉理解能力。随后，通过强化学习阶段进一步提升了其多模态复杂推理能力，从而开发出DentalGPT。", "result": "DentalGPT在口内和全景基准测试以及医学VQA牙科子集上进行了全面评估，结果显示其在疾病分类和牙科VQA任务中均取得了优越性能，尽管只有70亿参数，但仍超越了许多最先进的MLLM。", "conclusion": "高质量的牙科数据与分阶段适应相结合，为构建强大且领域专业的牙科多模态大语言模型提供了一条有效途径。"}}
{"id": "2512.11274", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11274", "abs": "https://arxiv.org/abs/2512.11274", "authors": ["Xiangyang Luo", "Qingyu Li", "Xiaokun Liu", "Wenyu Qin", "Miao Yang", "Meng Wang", "Pengfei Wan", "Di Zhang", "Kun Gai", "Shao-Lun Huang"], "title": "FilmWeaver: Weaving Consistent Multi-Shot Videos with Cache-Guided Autoregressive Diffusion", "comment": "AAAI-2026", "summary": "Current video generation models perform well at single-shot synthesis but struggle with multi-shot videos, facing critical challenges in maintaining character and background consistency across shots and flexibly generating videos of arbitrary length and shot count. To address these limitations, we introduce \\textbf{FilmWeaver}, a novel framework designed to generate consistent, multi-shot videos of arbitrary length. First, it employs an autoregressive diffusion paradigm to achieve arbitrary-length video generation. To address the challenge of consistency, our key insight is to decouple the problem into inter-shot consistency and intra-shot coherence. We achieve this through a dual-level cache mechanism: a shot memory caches keyframes from preceding shots to maintain character and scene identity, while a temporal memory retains a history of frames from the current shot to ensure smooth, continuous motion. The proposed framework allows for flexible, multi-round user interaction to create multi-shot videos. Furthermore, due to this decoupled design, our method demonstrates high versatility by supporting downstream tasks such as multi-concept injection and video extension. To facilitate the training of our consistency-aware method, we also developed a comprehensive pipeline to construct a high-quality multi-shot video dataset. Extensive experimental results demonstrate that our method surpasses existing approaches on metrics for both consistency and aesthetic quality, opening up new possibilities for creating more consistent, controllable, and narrative-driven video content. Project Page: https://filmweaver.github.io", "AI": {"tldr": "FilmWeaver是一个新颖的框架，通过自回归扩散和双层缓存机制，解决了现有视频生成模型在生成任意长度多镜头视频时，角色和背景一致性差的问题。", "motivation": "现有视频生成模型在单镜头合成方面表现良好，但在多镜头视频中，难以保持角色和背景在镜头间的一致性，且无法灵活生成任意长度和镜头数的视频。", "method": "该方法引入了FilmWeaver框架：1. 采用自回归扩散范式实现任意长度视频生成。2. 将一致性问题解耦为镜头间一致性（inter-shot consistency）和镜头内连贯性（intra-shot coherence）。3. 通过双层缓存机制实现：镜头记忆（shot memory）缓存前一镜头的关键帧以保持角色和场景身份，时间记忆（temporal memory）保留当前镜头的历史帧以确保平滑连续的运动。4. 支持灵活的多轮用户交互。5. 其解耦设计使其能支持多概念注入和视频扩展等下游任务。6. 开发了构建高质量多镜头视频数据集的流水线。", "result": "实验结果表明，FilmWeaver在一致性和美学质量指标上均超越了现有方法。", "conclusion": "该方法为创建更一致、可控和叙事驱动的视频内容开辟了新的可能性。"}}
{"id": "2512.11614", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.11614", "abs": "https://arxiv.org/abs/2512.11614", "authors": ["Björn Deiseroth", "Max Henning Höth", "Kristian Kersting", "Letitia Parcalabescu"], "title": "Bounding Hallucinations: Information-Theoretic Guarantees for RAG Systems via Merlin-Arthur Protocols", "comment": "34 pages, 19 figures", "summary": "Retrieval-augmented generation (RAG) models rely on retrieved evidence to guide large language model (LLM) generators, yet current systems treat retrieval as a weak heuristic rather than verifiable evidence. As a result, LLMs answer without support, hallucinate under incomplete or misleading context, and rely on spurious evidence. We introduce a training framework that treats the entire RAG pipeline -- both the retriever and the generator -- as an interactive proof system via an adaptation of the Merlin-Arthur (M/A) protocol. Arthur (the generator LLM) trains on questions of unkown provenance: Merlin provides helpful evidence, while Morgana injects adversarial, misleading context. Both use a linear-time XAI method to identify and modify the evidence most influential to Arthur. Consequently, Arthur learns to (i) answer when the context support the answer, (ii) reject when evidence is insufficient, and (iii) rely on the specific context spans that truly ground the answer. We further introduce a rigorous evaluation framework to disentangle explanation fidelity from baseline predictive errors. This allows us to introduce and measure the Explained Information Fraction (EIF), which normalizes M/A certified mutual-information guarantees relative to model capacity and imperfect benchmarks. Across three RAG datasets and two model families of varying sizes, M/A-trained LLMs show improved groundedness, completeness, soundness, and reject behavior, as well as reduced hallucinations -- without needing manually annotated unanswerable questions. The retriever likewise improves recall and MRR through automatically generated M/A hard positives and negatives. Our results demonstrate that autonomous interactive-proof-style supervision provides a principled and practical path toward reliable RAG systems that treat retrieved documents not as suggestions, but as verifiable evidence.", "AI": {"tldr": "该研究提出了一种基于梅林-亚瑟（M/A）协议的训练框架，将检索增强生成（RAG）视为交互式证明系统。通过让生成器（亚瑟）在有益和对抗性证据下学习，该框架旨在提高LLM的扎根性、完整性和可靠性，减少幻觉，并使检索器将检索到的文档视为可验证的证据而非启发式建议。", "motivation": "当前的RAG模型将检索到的证据视为弱启发式，导致大型语言模型（LLM）在没有支持的情况下回答、在不完整或误导性上下文中产生幻觉，并依赖虚假证据。研究动机在于开发一个将检索视为可验证证据的框架，以解决这些问题。", "method": "引入了一个将整个RAG管道（检索器和生成器）视为交互式证明系统的训练框架，该系统改编自梅林-亚瑟（M/A）协议。亚瑟（生成器LLM）在未知来源的问题上进行训练：梅林提供有益证据，而摩根娜注入对抗性、误导性上下文。两者都使用线性时间的可解释人工智能（XAI）方法来识别和修改对亚瑟最具影响力的证据。此外，引入了一个严格的评估框架，以区分解释保真度与基线预测错误，并引入了“解释信息分数”（EIF）来衡量M/A认证的互信息保证。", "result": "在三个RAG数据集和两种不同规模的模型家族上，M/A训练的LLM在扎根性、完整性、可靠性和拒绝行为方面均有所改善，并减少了幻觉，而无需手动标注不可回答的问题。检索器也通过自动生成的M/A硬正例和负例提高了召回率和MRR。亚瑟学会了在上下文支持答案时回答，在证据不足时拒绝，并依赖真正支持答案的特定上下文片段。", "conclusion": "自主交互式证明风格的监督（M/A框架）为构建可靠的RAG系统提供了一条原则性且实用的路径，使检索到的文档被视为可验证的证据，而非仅仅是建议。"}}
{"id": "2512.11567", "categories": ["cs.CL", "cs.MM"], "pdf": "https://arxiv.org/pdf/2512.11567", "abs": "https://arxiv.org/abs/2512.11567", "authors": ["Mevlüt Bagci", "Ali Abusaleh", "Daniel Baumartz", "Giueseppe Abrami", "Maxim Konca", "Alexander Mehler"], "title": "Extending a Parliamentary Corpus with MPs' Tweets: Automatic Annotation and Evaluation Using MultiParTweet", "comment": "Submitted to LREC 2026", "summary": "Social media serves as a critical medium in modern politics because it both reflects politicians' ideologies and facilitates communication with younger generations. We present MultiParTweet, a multilingual tweet corpus from X that connects politicians' social media discourse with German political corpus GerParCor, thereby enabling comparative analyses between online communication and parliamentary debates. MultiParTweet contains 39 546 tweets, including 19 056 media items. Furthermore, we enriched the annotation with nine text-based models and one vision-language model (VLM) to annotate MultiParTweet with emotion, sentiment, and topic annotations. Moreover, the automated annotations are evaluated against a manually annotated subset. MultiParTweet can be reconstructed using our tool, TTLABTweetCrawler, which provides a framework for collecting data from X. To demonstrate a methodological demonstration, we examine whether the models can predict each other using the outputs of the remaining models. In summary, we provide MultiParTweet, a resource integrating automatic text and media-based annotations validated with human annotations, and TTLABTweetCrawler, a general-purpose X data collection tool. Our analysis shows that the models are mutually predictable. In addition, VLM-based annotation were preferred by human annotators, suggesting that multimodal representations align more with human interpretation.", "AI": {"tldr": "本文介绍了MultiParTweet，一个多语言推文语料库，包含政客在X上的推文及其自动文本和多模态（视觉-语言模型）标注，并开发了数据收集工具TTLABTweetCrawler。研究发现，不同模型间具有相互可预测性，且人类标注者偏好VLM生成的标注。", "motivation": "社交媒体在现代政治中扮演着关键角色，它既能反映政治家的意识形态，又能促进与年轻一代的沟通。研究旨在将政客的社交媒体言论与议会辩论语料库联系起来，以实现线上和线下政治沟通的比较分析，并提供一个带有丰富标注的资源。", "method": "研究构建了MultiParTweet，一个包含39,546条推文（含19,056个媒体项）的多语言推文语料库，并将其与德国政治语料库GerParCor关联。语料库使用九个基于文本的模型和一个视觉-语言模型（VLM）进行情感、情绪和主题标注。自动化标注通过人工标注子集进行评估。此外，还开发了数据收集工具TTLABTweetCrawler。通过分析模型输出的相互预测性来展示方法论。", "result": "研究成功构建了MultiParTweet语料库和TTLABTweetCrawler工具。分析表明，模型之间具有相互可预测性。此外，人类标注者更偏爱基于VLM的标注，这表明多模态表示与人类解释更为一致。", "conclusion": "MultiParTweet是一个整合了自动文本和媒体标注并经过人工验证的重要资源，TTLABTweetCrawler是一个通用的X数据收集工具。研究结果强调了多模态方法（如VLM）在政治社交媒体分析中与人类解释的高度契合性。"}}
{"id": "2512.11464", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.11464", "abs": "https://arxiv.org/abs/2512.11464", "authors": ["Han Lin", "Xichen Pan", "Ziqi Huang", "Ji Hou", "Jialiang Wang", "Weifeng Chen", "Zecheng He", "Felix Juefei-Xu", "Junzhe Sun", "Zhipeng Fan", "Ali Thabet", "Mohit Bansal", "Chu Wang"], "title": "Exploring MLLM-Diffusion Information Transfer with MetaCanvas", "comment": "Project page: https://metacanvas.github.io", "summary": "Multimodal learning has rapidly advanced visual understanding, largely via multimodal large language models (MLLMs) that use powerful LLMs as cognitive cores. In visual generation, however, these powerful core models are typically reduced to global text encoders for diffusion models, leaving most of their reasoning and planning ability unused. This creates a gap: current multimodal LLMs can parse complex layouts, attributes, and knowledge-intensive scenes, yet struggle to generate images or videos with equally precise and structured control. We propose MetaCanvas, a lightweight framework that lets MLLMs reason and plan directly in spatial and spatiotemporal latent spaces and interface tightly with diffusion generators. We empirically implement MetaCanvas on three different diffusion backbones and evaluate it across six tasks, including text-to-image generation, text/image-to-video generation, image/video editing, and in-context video generation, each requiring precise layouts, robust attribute binding, and reasoning-intensive control. MetaCanvas consistently outperforms global-conditioning baselines, suggesting that treating MLLMs as latent-space planners is a promising direction for narrowing the gap between multimodal understanding and generation.", "AI": {"tldr": "MetaCanvas是一个轻量级框架，允许多模态大语言模型（MLLMs）直接在潜在空间中进行推理和规划，以实现精确的视觉生成，弥合了多模态理解与生成之间的鸿沟。", "motivation": "当前的多模态大语言模型（MLLMs）在视觉理解方面表现出色，但其强大的推理和规划能力在视觉生成中未被充分利用，通常仅作为扩散模型的全局文本编码器。这导致MLLMs能够解析复杂的布局和知识密集型场景，但在生成具有同等精确和结构化控制的图像或视频时面临困难。", "method": "本文提出了MetaCanvas，一个轻量级框架，使MLLMs能够直接在空间和时空潜在空间中进行推理和规划，并与扩散生成器紧密结合。该方法在三种不同的扩散骨干网络上进行了实现，并在文本到图像生成、文本/图像到视频生成、图像/视频编辑以及上下文视频生成等六项需要精确布局、鲁棒属性绑定和推理密集型控制的任务上进行了评估。", "result": "MetaCanvas在所有评估任务中始终优于全局条件基线模型，证明了其在需要精确布局、鲁棒属性绑定和推理密集型控制的视觉生成任务中的有效性。", "conclusion": "将多模态大语言模型（MLLMs）视为潜在空间规划器是缩小多模态理解与生成之间差距的一个有前景的方向。"}}
{"id": "2512.11284", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11284", "abs": "https://arxiv.org/abs/2512.11284", "authors": ["Rongcheng Wu", "Hao Zhu", "Shiying Zhang", "Mingzhe Wang", "Zhidong Li", "Hui Li", "Jianlong Zhou", "Jiangtao Cui", "Fang Chen", "Pingyang Sun", "Qiyu Liao", "Ye Lin"], "title": "RcAE: Recursive Reconstruction Framework for Unsupervised Industrial Anomaly Detection", "comment": "19 pages, 7 figures, to be published in AAAI-26", "summary": "Unsupervised industrial anomaly detection requires accurately identifying defects without labeled data. Traditional autoencoder-based methods often struggle with incomplete anomaly suppression and loss of fine details, as their single-pass decoding fails to effectively handle anomalies with varying severity and scale. We propose a recursive architecture for autoencoder (RcAE), which performs reconstruction iteratively to progressively suppress anomalies while refining normal structures. Unlike traditional single-pass models, this recursive design naturally produces a sequence of reconstructions, progressively exposing suppressed abnormal patterns. To leverage this reconstruction dynamics, we introduce a Cross Recursion Detection (CRD) module that tracks inconsistencies across recursion steps, enhancing detection of both subtle and large-scale anomalies. Additionally, we incorporate a Detail Preservation Network (DPN) to recover high-frequency textures typically lost during reconstruction. Extensive experiments demonstrate that our method significantly outperforms existing non-diffusion methods, and achieves performance on par with recent diffusion models with only 10% of their parameters and offering substantially faster inference. These results highlight the practicality and efficiency of our approach for real-world applications.", "AI": {"tldr": "本文提出了一种名为递归自编码器（RcAE）的新型架构，通过迭代重建和引入交叉递归检测（CRD）与细节保留网络（DPN）模块，显著提升了无监督工业异常检测的性能，超越了现有非扩散方法，并以更低的参数和更快的速度达到了与扩散模型相当的水平。", "motivation": "传统的自编码器在无监督工业异常检测中面临挑战，即难以有效抑制不同严重程度和尺度的异常，并且在重建过程中容易丢失精细细节，因为它们的单次解码无法有效处理这些问题。", "method": "研究者提出了一种递归自编码器（RcAE），通过迭代重建逐步抑制异常并细化正常结构。为利用重建动态，引入了交叉递归检测（CRD）模块，通过跟踪递归步骤间的不一致性来增强对细微和大规模异常的检测。此外，还集成了细节保留网络（DPN），以恢复重建过程中通常丢失的高频纹理。", "result": "实验结果表明，该方法显著优于现有非扩散方法，并且在性能上与最新的扩散模型相当，但其参数量仅为扩散模型的10%，并提供了更快的推理速度。", "conclusion": "这些结果突显了该方法在实际应用中的实用性和高效性。"}}
{"id": "2512.11718", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.11718", "abs": "https://arxiv.org/abs/2512.11718", "authors": ["Sergey Pankratov", "Dan Alistarh"], "title": "Speculative Decoding Speed-of-Light: Optimal Lower Bounds via Branching Random Walks", "comment": null, "summary": "Speculative generation has emerged as a promising technique to accelerate inference in large language models (LLMs) by leveraging parallelism to verify multiple draft tokens simultaneously. However, the fundamental limits on the achievable speedup remain poorly understood. In this work, we establish the first ``tight'' lower bounds on the runtime of any deterministic speculative generation algorithm. This is achieved by drawing a parallel between the token generation process and branching random walks, which allows us to analyze the optimal draft tree selection problem. We prove, under basic assumptions, that the expected number of tokens successfully predicted per speculative iteration is bounded as $\\mathbb{E}[X] \\leq (μ+ μ_{(2)})\\log(P )/μ^2 + O(1)$, where $P$ is the verifier's capacity, $μ$ is the expected entropy of the verifier's output distribution, and $μ_{(2)}$ is the expected second log-moment. This result provides new insights into the limits of parallel token generation, and could guide the design of future speculative decoding systems. Empirical evaluations on Llama models validate our theoretical predictions, confirming the tightness of our bounds in practical settings.", "AI": {"tldr": "本研究首次为投机生成（Speculative Generation）算法的运行时建立了严格的下界，揭示了大型语言模型推理加速的理论限制。", "motivation": "投机生成技术通过并行验证多个草稿标记来加速大型语言模型（LLMs）的推理，但其可实现加速的根本限制尚不清楚。", "method": "通过将标记生成过程与分支随机游走（branching random walks）进行类比，并分析最优草稿树选择问题，从而建立了任何确定性投机生成算法运行时的“严格”下界。", "result": "在基本假设下，证明了每个投机迭代成功预测的期望标记数受限于 $\\mathbb{E}[X] \\leq (μ+ μ_{(2)})\\log(P )/μ^2 + O(1)$，其中 $P$ 是验证器容量，$μ$ 是验证器输出分布的期望熵，$μ_{(2)}$ 是期望的第二对数矩。Llama模型上的实证评估验证了理论预测的准确性。", "conclusion": "该结果为并行标记生成的极限提供了新见解，并能指导未来投机解码系统的设计。"}}
{"id": "2512.11293", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11293", "abs": "https://arxiv.org/abs/2512.11293", "authors": ["Cuifeng Shen", "Lumin Xu", "Xingguo Zhu", "Gengdai Liu"], "title": "Autoregressive Video Autoencoder with Decoupled Temporal and Spatial Context", "comment": null, "summary": "Video autoencoders compress videos into compact latent representations for efficient reconstruction, playing a vital role in enhancing the quality and efficiency of video generation. However, existing video autoencoders often entangle spatial and temporal information, limiting their ability to capture temporal consistency and leading to suboptimal performance. To address this, we propose Autoregressive Video Autoencoder (ARVAE), which compresses and reconstructs each frame conditioned on its predecessor in an autoregressive manner, allowing flexible processing of videos with arbitrary lengths. ARVAE introduces a temporal-spatial decoupled representation that combines downsampled flow field for temporal coherence with spatial relative compensation for newly emerged content, achieving high compression efficiency without information loss. Specifically, the encoder compresses the current and previous frames into the temporal motion and spatial supplement, while the decoder reconstructs the original frame from the latent representations given the preceding frame. A multi-stage training strategy is employed to progressively optimize the model. Extensive experiments demonstrate that ARVAE achieves superior reconstruction quality with extremely lightweight models and small-scale training data. Moreover, evaluations on video generation tasks highlight its strong potential for downstream applications.", "AI": {"tldr": "本文提出自回归视频自编码器（ARVAE），通过自回归方式处理视频帧，并引入时间-空间解耦表示，显著提升了视频压缩效率和重建质量，尤其在处理任意长度视频和下游生成任务方面表现出色。", "motivation": "现有视频自编码器常将空间和时间信息纠缠在一起，限制了其捕捉时间一致性的能力，导致性能不佳。研究旨在解决这一问题，提升视频自编码器的性能和效率。", "method": "ARVAE采用自回归方式，每一帧的压缩和重建都以前一帧为条件。它引入了时间-空间解耦表示，结合下采样的光流场以保持时间连贯性，并利用空间相对补偿处理新出现的内容。编码器将当前帧和前一帧压缩为时间运动和空间补充，解码器则根据潜在表示和前一帧重建原始帧。模型采用多阶段训练策略进行优化。", "result": "ARVAE在极其轻量级的模型和少量训练数据下，实现了卓越的重建质量。此外，在视频生成任务上的评估也突显了其在下游应用中的强大潜力。", "conclusion": "ARVAE通过自回归处理和时间-空间解耦表示，有效解决了现有视频自编码器中空间和时间信息纠缠的问题，显著提高了视频压缩和重建的性能，并展现出在视频生成等任务中的广阔应用前景。"}}
{"id": "2512.11301", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11301", "abs": "https://arxiv.org/abs/2512.11301", "authors": ["Bate Li", "Houqiang Zhong", "Zhengxue Cheng", "Qiang Hu", "Qiang Wang", "Li Song", "Wenjun Zhang"], "title": "MultiEgo: A Multi-View Egocentric Video Dataset for 4D Scene Reconstruction", "comment": "ACM MM 2025 Dataset Track", "summary": "Multi-view egocentric dynamic scene reconstruction holds significant research value for applications in holographic documentation of social interactions. However, existing reconstruction datasets focus on static multi-view or single-egocentric view setups, lacking multi-view egocentric datasets for dynamic scene reconstruction. Therefore, we present MultiEgo, the first multi-view egocentric dataset for 4D dynamic scene reconstruction. The dataset comprises five canonical social interaction scenes: meetings, performances, and a presentation. Each scene provides five authentic egocentric videos captured by participants wearing AR glasses. We design a hardware-based data acquisition system and processing pipeline, achieving sub-millisecond temporal synchronization across views, coupled with accurate pose annotations. Experiment validation demonstrates the practical utility and effectiveness of our dataset for free-viewpoint video (FVV) applications, establishing MultiEgo as a foundational resource for advancing multi-view egocentric dynamic scene reconstruction research.", "AI": {"tldr": "本文提出了MultiEgo，首个用于4D动态场景重建的多视角以自我为中心的数据集，填补了现有数据集在多视角以自我为中心动态场景方面的空白，并验证了其在自由视角视频应用中的有效性。", "motivation": "多视角以自我为中心的动态场景重建对社交互动全息记录具有重要研究价值，但现有数据集主要关注静态多视角或单以自我为中心视角，缺乏用于动态场景重建的多视角以自我为中心数据集。", "method": "本文设计并构建了MultiEgo数据集，包含会议、表演和演示等五种典型社交互动场景，每个场景提供五段由参与者佩戴AR眼镜捕获的真实以自我为中心视频。通过硬件数据采集系统和处理流程，实现了亚毫秒级跨视角时间同步和精确姿态标注。", "result": "实验验证表明，MultiEgo数据集在自由视角视频（FVV）应用中具有实际效用和有效性。", "conclusion": "MultiEgo数据集为推动多视角以自我为中心动态场景重建研究奠定了基础资源。"}}
{"id": "2512.11771", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11771", "abs": "https://arxiv.org/abs/2512.11771", "authors": ["Kai Yao", "Marc Juarez"], "title": "Smudged Fingerprints: A Systematic Evaluation of the Robustness of AI Image Fingerprints", "comment": "This work has been accepted for publication in the 4th IEEE Conference on Secure and Trustworthy Machine Learning (IEEE SaTML 2026). The final version will be available on IEEE Xplore", "summary": "Model fingerprint detection techniques have emerged as a promising approach for attributing AI-generated images to their source models, but their robustness under adversarial conditions remains largely unexplored. We present the first systematic security evaluation of these techniques, formalizing threat models that encompass both white- and black-box access and two attack goals: fingerprint removal, which erases identifying traces to evade attribution, and fingerprint forgery, which seeks to cause misattribution to a target model. We implement five attack strategies and evaluate 14 representative fingerprinting methods across RGB, frequency, and learned-feature domains on 12 state-of-the-art image generators. Our experiments reveal a pronounced gap between clean and adversarial performance. Removal attacks are highly effective, often achieving success rates above 80% in white-box settings and over 50% under constrained black-box access. While forgery is more challenging than removal, its success significantly varies across targeted models. We also identify a utility-robustness trade-off: methods with the highest attribution accuracy are often vulnerable to attacks. Although some techniques exhibit robustness in specific settings, none achieves high robustness and accuracy across all evaluated threat models. These findings highlight the need for techniques balancing robustness and accuracy, and identify the most promising approaches for advancing this goal.", "AI": {"tldr": "本研究首次系统评估了AI生成图像模型指纹检测技术在对抗条件下的安全性，发现这些技术普遍脆弱，移除攻击高度有效，且存在实用性与鲁棒性的权衡，凸显了开发兼顾鲁棒性和准确性技术的需求。", "motivation": "模型指纹检测技术被视为识别AI生成图像来源的有前途方法，但其在对抗条件下的鲁棒性尚未得到充分探索。本研究旨在系统评估这些技术的安全性。", "method": "研究形式化了包含白盒和黑盒访问的威胁模型，以及两种攻击目标：指纹移除（擦除识别痕迹以逃避归因）和指纹伪造（导致错误归因到目标模型）。研究实现了五种攻击策略，并在12个最先进的图像生成器上，评估了RGB、频率和学习特征域中的14种代表性指纹识别方法。", "result": "实验揭示了在干净和对抗性能之间存在显著差距。移除攻击非常有效，在白盒设置中成功率常超过80%，在受限黑盒访问下超过50%。伪造攻击比移除更具挑战性，但其成功率在不同目标模型之间差异显著。研究还发现了一个实用性-鲁棒性权衡：归因准确性最高的方法往往容易受到攻击。尽管某些技术在特定设置中表现出鲁棒性，但没有一种技术能在所有评估的威胁模型中同时实现高鲁棒性和准确性。", "conclusion": "这些发现强调了开发平衡鲁棒性和准确性的技术的需求，并指出了实现这一目标最有前途的方法。模型指纹检测技术在对抗环境下普遍脆弱，需要进一步研究以提高其安全性。"}}
{"id": "2512.11755", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.11755", "abs": "https://arxiv.org/abs/2512.11755", "authors": ["Yuming Feng", "Xinrui Jiang"], "title": "SUMFORU: An LLM-Based Review Summarization Framework for Personalized Purchase Decision Support", "comment": "Code available at https://github.com/Harry20030331/SumForU", "summary": "Online product reviews contain rich but noisy signals that overwhelm users and hinder effective decision-making. Existing LLM-based summarizers remain generic and fail to account for individual preferences, limiting their practical utility. We propose SUMFORU, a steerable review summarization framework that aligns outputs with explicit user personas to support personalized purchase decisions. Our approach integrates a high-quality data pipeline built from the Amazon 2023 Review Dataset with a two-stage alignment procedure: (1) persona-aware Supervised Fine-Tuning (SFT) via asymmetric knowledge distillation, and (2) Reinforcement Learning with AI Feedback (RLAIF) using a preference estimator to capture fine-grained, persona-relevant signals. We evaluate the model across rule-based, LLM-based, and human-centered metrics, demonstrating consistent improvements in consistency, grounding, and preference alignment. Our framework achieves the highest performance across all evaluation settings and generalizes effectively to unseen product categories. Our results highlight the promise of steerable pluralistic alignment for building next-generation personalized decision-support systems.", "AI": {"tldr": "SUMFORU是一个可控的评论摘要框架，它通过两阶段对齐过程（包括基于人物角色的SFT和RLAIF）将摘要与用户人物角色对齐，从而实现个性化购买决策支持。", "motivation": "在线产品评论信息丰富但嘈杂，使消费者难以有效决策。现有基于LLM的摘要器过于通用，未能考虑个体偏好，限制了其实用性。", "method": "提出了SUMFORU框架，整合了来自Amazon 2023评论数据集的高质量数据管道和两阶段对齐程序：1) 通过非对称知识蒸馏进行人物角色感知的监督微调（SFT）；2) 使用偏好估计器通过AI反馈强化学习（RLAIF）捕捉细粒度、与人物角色相关的信号。", "result": "模型在基于规则、基于LLM和以人为中心的指标上均表现出一致的改进，包括一致性、接地性和偏好对齐。该框架在所有评估设置中均达到最高性能，并能有效泛化到未见过的产品类别。", "conclusion": "研究结果强调了可控多元对齐在构建下一代个性化决策支持系统方面的巨大潜力。"}}
{"id": "2512.11319", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11319", "abs": "https://arxiv.org/abs/2512.11319", "authors": ["Bingyuan Huang", "Guanyi Zhao", "Qian Xu", "Yang Lou", "Yung-Hui Li", "Jianping Wang"], "title": "SATMapTR: Satellite Image Enhanced Online HD Map Construction", "comment": "9 pages (+ 3 pages of Appendix)", "summary": "High-definition (HD) maps are evolving from pre-annotated to real-time construction to better support autonomous driving in diverse scenarios. However, this process is hindered by low-quality input data caused by onboard sensors limited capability and frequent occlusions, leading to incomplete, noisy, or missing data, and thus reduced mapping accuracy and robustness. Recent efforts have introduced satellite images as auxiliary input, offering a stable, wide-area view to complement the limited ego perspective. However, satellite images in Bird's Eye View are often degraded by shadows and occlusions from vegetation and buildings. Prior methods using basic feature extraction and fusion remain ineffective. To address these challenges, we propose SATMapTR, a novel online map construction model that effectively fuses satellite image through two key components: (1) a gated feature refinement module that adaptively filters satellite image features by integrating high-level semantics with low-level structural cues to extract high signal-to-noise ratio map-relevant representations; and (2) a geometry-aware fusion module that consistently fuse satellite and BEV features at a grid-to-grid level, minimizing interference from irrelevant regions and low-quality inputs. Experimental results on the nuScenes dataset show that SATMapTR achieves the highest mean average precision (mAP) of 73.8, outperforming state-of-the-art satellite-enhanced models by up to 14.2 mAP. It also shows lower mAP degradation under adverse weather and sensor failures, and achieves nearly 3 times higher mAP at extended perception ranges.", "AI": {"tldr": "SATMapTR是一种新型在线地图构建模型，通过门控特征细化和几何感知融合模块，有效整合卫星图像和车载传感器数据，显著提升了高精地图的构建精度和鲁棒性，尤其在恶劣条件和远距离感知下表现出色。", "motivation": "自动驾驶中的高精地图正从预标注转向实时构建，以适应多样化场景。然而，车载传感器数据质量低（不完整、噪声大、缺失）导致地图精度和鲁棒性下降。引入卫星图像作为辅助输入虽有潜力，但其自身也受阴影和遮挡影响。现有融合方法效率低下，无法有效解决这些挑战。", "method": "本文提出了SATMapTR模型，包含两个关键组件：1) 门控特征细化模块，通过整合高层语义和低层结构线索，自适应地过滤卫星图像特征，提取高信噪比的地图相关表示；2) 几何感知融合模块，以网格到网格的方式一致地融合卫星和鸟瞰图(BEV)特征，最大限度地减少无关区域和低质量输入的干扰。", "result": "在nuScenes数据集上的实验结果表明，SATMapTR实现了73.8的最高平均精度（mAP），比最先进的卫星增强模型高出14.2 mAP。它在恶劣天气和传感器故障下的mAP下降也更小，并在扩展感知范围内实现了近3倍的mAP提升。", "conclusion": "SATMapTR通过创新的门控特征细化和几何感知融合模块，成功解决了在线高精地图构建中数据质量和融合效率的挑战，显著提高了地图构建的精度和在复杂场景下的鲁棒性，尤其在远距离感知和恶劣条件下表现卓越。"}}
{"id": "2512.11798", "categories": ["cs.CV", "cs.AI", "cs.GR"], "pdf": "https://arxiv.org/pdf/2512.11798", "abs": "https://arxiv.org/abs/2512.11798", "authors": ["Ruining Li", "Yuxin Yao", "Chuanxia Zheng", "Christian Rupprecht", "Joan Lasenby", "Shangzhe Wu", "Andrea Vedaldi"], "title": "Particulate: Feed-Forward 3D Object Articulation", "comment": "Project page: https://ruiningli.com/particulate", "summary": "We present Particulate, a feed-forward approach that, given a single static 3D mesh of an everyday object, directly infers all attributes of the underlying articulated structure, including its 3D parts, kinematic structure, and motion constraints. At its core is a transformer network, Part Articulation Transformer, which processes a point cloud of the input mesh using a flexible and scalable architecture to predict all the aforementioned attributes with native multi-joint support. We train the network end-to-end on a diverse collection of articulated 3D assets from public datasets. During inference, Particulate lifts the network's feed-forward prediction to the input mesh, yielding a fully articulated 3D model in seconds, much faster than prior approaches that require per-object optimization. Particulate can also accurately infer the articulated structure of AI-generated 3D assets, enabling full-fledged extraction of articulated 3D objects from a single (real or synthetic) image when combined with an off-the-shelf image-to-3D generator. We further introduce a new challenging benchmark for 3D articulation estimation curated from high-quality public 3D assets, and redesign the evaluation protocol to be more consistent with human preferences. Quantitative and qualitative results show that Particulate significantly outperforms state-of-the-art approaches.", "AI": {"tldr": "Particulate 是一种前馈方法，它能从单个静态 3D 网格中直接推断出可动结构的所有属性（包括部件、运动学和运动约束），速度快于现有方法，并支持多关节和 AI 生成的资产。", "motivation": "现有方法通常需要对每个对象进行优化，耗时较长，并且不能直接从单个静态 3D 网格高效地推断出完整的可动结构属性。", "method": "该研究提出了 Particulate，一个基于 Transformer 网络的端到端前馈方法，名为 Part Articulation Transformer。它处理输入网格的点云，以预测所有关节属性，并支持多关节。该网络在多样化的可动 3D 资产数据集上进行端到端训练。推断时，Particulate 将网络预测结果映射到输入网格上。此外，研究还引入了一个新的 3D 关节估计基准和重新设计的评估协议。", "result": "Particulate 能够在数秒内生成完整的可动 3D 模型，比现有方法快得多。它还能准确推断 AI 生成 3D 资产的可动结构，结合现成的图像到 3D 生成器，可以实现从单张图像中提取完整的可动 3D 对象。定量和定性结果表明，Particulate 显著优于现有最先进的方法。", "conclusion": "Particulate 是一种高效、准确且鲁棒的前馈方法，能够从单个 3D 网格中推断出复杂的关节结构，在速度和性能上均超越了现有技术，并为从图像中提取可动 3D 对象提供了新的可能性。"}}
{"id": "2512.11534", "categories": ["cs.CV", "cs.CL", "cs.MM"], "pdf": "https://arxiv.org/pdf/2512.11534", "abs": "https://arxiv.org/abs/2512.11534", "authors": ["Yiqing Yang", "Kin-Man Lam"], "title": "HFS: Holistic Query-Aware Frame Selection for Efficient Video Reasoning", "comment": "18 pages, 8 figures", "summary": "Key frame selection in video understanding presents significant challenges. Traditional top-K selection methods, which score frames independently, often fail to optimize the selection as a whole. This independent scoring frequently results in selecting frames that are temporally clustered and visually redundant. Additionally, training lightweight selectors using pseudo labels generated offline by Multimodal Large Language Models (MLLMs) prevents the supervisory signal from dynamically adapting to task objectives. To address these limitations, we propose an end-to-end trainable, task-adaptive framework for frame selection. A Chain-of-Thought approach guides a Small Language Model (SLM) to generate task-specific implicit query vectors, which are combined with multimodal features to enable dynamic frame scoring. We further define a continuous set-level objective function that incorporates relevance, coverage, and redundancy, enabling differentiable optimization via Gumbel-Softmax to select optimal frame combinations at the set level. Finally, student-teacher mutual learning is employed, where the student selector (SLM) and teacher reasoner (MLLM) are trained to align their frame importance distributions via KL divergence. Combined with cross-entropy loss, this enables end-to-end optimization, eliminating reliance on static pseudo labels. Experiments across various benchmarks, including Video-MME, LongVideoBench, MLVU, and NExT-QA, demonstrate that our method significantly outperforms existing approaches.", "AI": {"tldr": "本文提出了一种端到端可训练、任务自适应的视频关键帧选择框架，通过小语言模型（SLM）生成任务特定查询，结合多模态特征动态评分，并利用连续集合级目标函数和师生互学习机制，解决了传统方法的冗余和静态伪标签问题。", "motivation": "传统关键帧选择方法独立评分，常导致帧在时序上聚集且视觉冗余；此外，使用多模态大语言模型（MLLM）离线生成的伪标签训练轻量级选择器，导致监督信号无法动态适应任务目标。", "method": "1. 提出端到端可训练、任务自适应的帧选择框架。2. 采用思维链（Chain-of-Thought）方法引导小语言模型（SLM）生成任务特定的隐式查询向量，并与多模态特征结合实现动态帧评分。3. 定义包含相关性、覆盖率和冗余的连续集合级目标函数，通过Gumbel-Softmax实现可微分优化以选择最优帧组合。4. 采用师生互学习（学生选择器SLM，教师推理器MLLM），通过KL散度对齐帧重要性分布，并结合交叉熵损失，实现端到端优化，摆脱对静态伪标签的依赖。", "result": "在Video-MME、LongVideoBench、MLVU和NExT-QA等多个基准测试中，所提出的方法显著优于现有方法。", "conclusion": "该框架成功解决了视频理解中关键帧选择的挑战，通过动态、任务自适应的机制，有效避免了传统方法的冗余和对静态伪标签的依赖，实现了优越的性能。"}}
{"id": "2512.11327", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11327", "abs": "https://arxiv.org/abs/2512.11327", "authors": ["Junqiao Wang", "Yuanfei Huang", "Hua Huang"], "title": "Physics-Informed Video Flare Synthesis and Removal Leveraging Motion Independence between Flare and Scene", "comment": null, "summary": "Lens flare is a degradation phenomenon caused by strong light sources. Existing researches on flare removal have mainly focused on images, while the spatiotemporal characteristics of video flare remain largely unexplored. Video flare synthesis and removal pose significantly greater challenges than in image, owing to the complex and mutually independent motion of flare, light sources, and scene content. This motion independence further affects restoration performance, often resulting in flicker and artifacts. To address this issue, we propose a physics-informed dynamic flare synthesis pipeline, which simulates light source motion using optical flow and models the temporal behaviors of both scattering and reflective flares. Meanwhile, we design a video flare removal network that employs an attention module to spatially suppress flare regions and incorporates a Mamba-based temporal modeling component to capture long range spatio-temporal dependencies. This motion-independent spatiotemporal representation effectively eliminates the need for multi-frame alignment, alleviating temporal aliasing between flares and scene content and thereby improving video flare removal performance. Building upon this, we construct the first video flare dataset to comprehensively evaluate our method, which includes a large set of synthetic paired videos and additional real-world videos collected from the Internet to assess generalization capability. Extensive experiments demonstrate that our method consistently outperforms existing video-based restoration and image-based flare removal methods on both real and synthetic videos, effectively removing dynamic flares while preserving light source integrity and maintaining spatiotemporal consistency of scene.", "AI": {"tldr": "本文提出了一种针对视频眩光的物理信息动态眩光合成管线和基于Mamba的视频眩光去除网络，解决了眩光、光源和场景内容独立运动导致的视频眩光去除挑战，并构建了首个视频眩光数据集。", "motivation": "现有眩光去除研究主要集中于图像，而视频眩光的时空特性尚未充分探索。视频眩光合成和去除更具挑战性，因为眩光、光源和场景内容运动复杂且相互独立，这会影响修复性能，导致闪烁和伪影。", "method": "本文提出：1) 一个物理信息动态眩光合成管线，利用光流模拟光源运动，并建模散射和反射眩光的时序行为。2) 一个视频眩光去除网络，采用注意力模块在空间上抑制眩光区域，并结合基于Mamba的时间建模组件来捕捉长程时空依赖。这种运动无关的时空表示避免了多帧对齐，减轻了眩光和场景内容之间的时间混叠。3) 构建了首个视频眩光数据集，包含大量合成配对视频和额外的真实世界视频。", "result": "广泛实验表明，本文方法在真实和合成视频上均持续优于现有视频修复和图像眩光去除方法。它能有效去除动态眩光，同时保留光源完整性并保持场景的时空一致性。", "conclusion": "本文成功开发了一种新的视频眩光合成与去除方法，解决了视频中眩光、光源和场景内容独立运动带来的挑战，并通过构建专用数据集，显著提升了视频眩光去除的性能和时空一致性。"}}
{"id": "2512.11321", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11321", "abs": "https://arxiv.org/abs/2512.11321", "authors": ["Jingchao Wu", "Zejian Kang", "Haibo Liu", "Yuanchen Fei", "Xiangru Huang"], "title": "KeyframeFace: From Text to Expressive Facial Keyframes", "comment": null, "summary": "Generating dynamic 3D facial animation from natural language requires understanding both temporally structured semantics and fine-grained expression changes. Existing datasets and methods mainly focus on speech-driven animation or unstructured expression sequences and therefore lack the semantic grounding and temporal structures needed for expressive human performance generation. In this work, we introduce KeyframeFace, a large-scale multimodal dataset designed for text-to-animation research through keyframe-level supervision. KeyframeFace provides 2,100 expressive scripts paired with monocular videos, per-frame ARKit coefficients, contextual backgrounds, complex emotions, manually defined keyframes, and multi-perspective annotations based on ARKit coefficients and images via Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs). Beyond the dataset, we propose the first text-to-animation framework that explicitly leverages LLM priors for interpretable facial motion synthesis. This design aligns the semantic understanding capabilities of LLMs with the interpretable structure of ARKit's coefficients, enabling high-fidelity expressive animation. KeyframeFace and our LLM-based framework together establish a new foundation for interpretable, keyframe-guided, and context-aware text-to-animation. Code and data are available at https://github.com/wjc12345123/KeyframeFace.", "AI": {"tldr": "本文介绍了KeyframeFace数据集和基于LLM的框架，旨在解决从自然语言生成动态3D面部动画时缺乏语义理解和时间结构的问题，实现了可解释、关键帧引导和上下文感知的文本到动画生成。", "motivation": "现有的数据集和方法主要侧重于语音驱动动画或非结构化表情序列，缺乏生成富有表现力的人类表演所需的语义基础和时间结构，难以从自然语言生成动态3D面部动画。", "method": "本文引入了KeyframeFace，一个大规模多模态数据集，通过关键帧级监督设计用于文本到动画研究。该数据集包含2,100个富有表现力的脚本，配对单目视频、逐帧ARKit系数、上下文背景、复杂情感、手动定义的关键帧以及基于ARKit系数和图像通过LLMs和MLLMs生成的多视角注释。此外，本文提出了首个明确利用LLM先验进行可解释面部运动合成的文本到动画框架。", "result": "KeyframeFace数据集提供了丰富的多模态数据和关键帧级监督，为文本到动画研究奠定了基础。所提出的基于LLM的框架将LLM的语义理解能力与ARKit系数的可解释结构相结合，实现了高保真、富有表现力的动画生成。", "conclusion": "KeyframeFace数据集和LLM驱动的框架共同为可解释、关键帧引导和上下文感知的文本到动画生成建立了新的基础。"}}
{"id": "2512.11335", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11335", "abs": "https://arxiv.org/abs/2512.11335", "authors": ["Yixuan Zhang", "Qing Xu", "Yue Li", "Xiangjian He", "Qian Zhang", "Mainul Haque", "Rong Qu", "Wenting Duan", "Zhen Chen"], "title": "FreqDINO: Frequency-Guided Adaptation for Generalized Boundary-Aware Ultrasound Image Segmentation", "comment": null, "summary": "Ultrasound image segmentation is pivotal for clinical diagnosis, yet challenged by speckle noise and imaging artifacts. Recently, DINOv3 has shown remarkable promise in medical image segmentation with its powerful representation capabilities. However, DINOv3, pre-trained on natural images, lacks sensitivity to ultrasound-specific boundary degradation. To address this limitation, we propose FreqDINO, a frequency-guided segmentation framework that enhances boundary perception and structural consistency. Specifically, we devise a Multi-scale Frequency Extraction and Alignment (MFEA) strategy to separate low-frequency structures and multi-scale high-frequency boundary details, and align them via learnable attention. We also introduce a Frequency-Guided Boundary Refinement (FGBR) module that extracts boundary prototypes from high-frequency components and refines spatial features. Furthermore, we design a Multi-task Boundary-Guided Decoder (MBGD) to ensure spatial coherence between boundary and semantic predictions. Extensive experiments demonstrate that FreqDINO surpasses state-of-the-art methods with superior achieves remarkable generalization capability. The code is at https://github.com/MingLang-FD/FreqDINO.", "AI": {"tldr": "FreqDINO是一种频率引导的超声图像分割框架，通过多尺度频率提取与对齐、频率引导边界细化和多任务边界引导解码器，解决了DINOv3在超声图像中对边界感知不足的问题，显著提升了分割性能和泛化能力。", "motivation": "超声图像分割对临床诊断至关重要，但受散斑噪声和伪影挑战。DINOv3在医学图像分割中表现出色，但其在自然图像上的预训练使其对超声特有的边界退化不敏感。本研究旨在解决DINOv3在超声图像中边界感知能力不足的问题。", "method": "本文提出了FreqDINO框架，包含以下策略：1. 多尺度频率提取与对齐（MFEA）策略：分离低频结构和多尺度高频边界细节，并通过可学习注意力进行对齐。2. 频率引导边界细化（FGBR）模块：从高频分量中提取边界原型并细化空间特征。3. 多任务边界引导解码器（MBGD）：确保边界和语义预测之间的空间一致性。", "result": "大量实验表明，FreqDINO超越了现有最先进的方法，取得了卓越的性能，并展现出显著的泛化能力。", "conclusion": "FreqDINO通过频率引导策略有效增强了超声图像分割中的边界感知和结构一致性，成功解决了DINOv3在超声特有边界退化问题上的局限性，并实现了卓越的分割性能和泛化能力。"}}
{"id": "2512.11373", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.11373", "abs": "https://arxiv.org/abs/2512.11373", "authors": ["Arnold Brosch", "Abdelrahman Eldesokey", "Michael Felsberg", "Kira Maag"], "title": "Out-of-Distribution Segmentation via Wasserstein-Based Evidential Uncertainty", "comment": null, "summary": "Deep neural networks achieve superior performance in semantic segmentation, but are limited to a predefined set of classes, which leads to failures when they encounter unknown objects in open-world scenarios. Recognizing and segmenting these out-of-distribution (OOD) objects is crucial for safety-critical applications such as automated driving. In this work, we present an evidence segmentation framework using a Wasserstein loss, which captures distributional distances while respecting the probability simplex geometry. Combined with Kullback-Leibler regularization and Dice structural consistency terms, our approach leads to improved OOD segmentation performance compared to uncertainty-based approaches.", "AI": {"tldr": "本文提出了一种基于Wasserstein损失的证据分割框架，结合KL正则化和Dice结构一致性项，以解决深度神经网络在语义分割中对未知（OOD）物体的识别和分割限制，并显著提高了OOD分割性能。", "motivation": "深度神经网络在语义分割中表现出色，但仅限于预定义类别，在开放世界场景中遇到未知物体时会失效。识别和分割这些分布外（OOD）物体对于自动驾驶等安全关键应用至关重要。", "method": "本研究提出了一个证据分割框架，该框架采用：1) Wasserstein损失，用于捕获分布距离并尊重概率单纯形几何；2) Kullback-Leibler (KL) 正则化；3) Dice结构一致性项。", "result": "与基于不确定性的方法相比，本文提出的方法显著提高了OOD分割性能。", "conclusion": "结合Wasserstein损失、KL正则化和Dice结构一致性项的证据分割框架，能够有效提升深度神经网络在开放世界场景中对未知物体的识别和分割能力，优于现有基于不确定性的方法。"}}
{"id": "2512.11401", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11401", "abs": "https://arxiv.org/abs/2512.11401", "authors": ["Qishan Wang", "Haofeng Wang", "Shuyong Gao", "Jia Guo", "Li Xiong", "Jiaqi Li", "Dengxuan Bai", "Wenqiang Zhang"], "title": "Collaborative Reconstruction and Repair for Multi-class Industrial Anomaly Detection", "comment": "Accepted to Data Intelligence 2025", "summary": "Industrial anomaly detection is a challenging open-set task that aims to identify unknown anomalous patterns deviating from normal data distribution. To avoid the significant memory consumption and limited generalizability brought by building separate models per class, we focus on developing a unified framework for multi-class anomaly detection. However, under this challenging setting, conventional reconstruction-based networks often suffer from an identity mapping problem, where they directly replicate input features regardless of whether they are normal or anomalous, resulting in detection failures. To address this issue, this study proposes a novel framework termed Collaborative Reconstruction and Repair (CRR), which transforms the reconstruction to repairation. First, we optimize the decoder to reconstruct normal samples while repairing synthesized anomalies. Consequently, it generates distinct representations for anomalous regions and similar representations for normal areas compared to the encoder's output. Second, we implement feature-level random masking to ensure that the representations from decoder contain sufficient local information. Finally, to minimize detection errors arising from the discrepancies between feature representations from the encoder and decoder, we train a segmentation network supervised by synthetic anomaly masks, thereby enhancing localization performance. Extensive experiments on industrial datasets that CRR effectively mitigates the identity mapping issue and achieves state-of-the-art performance in multi-class industrial anomaly detection.", "AI": {"tldr": "本文提出了一种名为协同重建与修复（CRR）的新框架，用于多类别工业异常检测。它通过将重建转换为修复，并结合特征级掩码和分割网络，有效解决了传统重建网络中的身份映射问题，实现了最先进的性能。", "motivation": "工业异常检测是一个具有挑战性的开放集任务，传统方法为每个类别构建模型会导致内存消耗大且泛化能力有限。在多类别设置下，常规的基于重建的网络常遇到“身份映射问题”，即无论输入正常或异常，网络都直接复制输入特征，导致检测失败。", "method": "本文提出了协同重建与修复（CRR）框架。首先，优化解码器以重建正常样本并修复合成异常，从而为异常区域生成独特的表示，为正常区域生成与编码器输出相似的表示。其次，实施特征级随机掩码以确保解码器表示包含足够的局部信息。最后，训练一个由合成异常掩码监督的分割网络，以最小化编码器和解码器特征表示之间的差异，提高定位性能。", "result": "在工业数据集上的大量实验表明，CRR有效缓解了身份映射问题，并在多类别工业异常检测中取得了最先进的性能。", "conclusion": "CRR是一个新颖且有效的框架，通过将重建转换为修复，并结合特征级掩码和分割网络，成功解决了多类别工业异常检测中的身份映射问题，并达到了最先进的检测和定位性能。"}}
{"id": "2512.11395", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11395", "abs": "https://arxiv.org/abs/2512.11395", "authors": ["Yilei Jiang", "Zhen Wang", "Yanghao Wang", "Jun Yu", "Yueting Zhuang", "Jun Xiao", "Long Chen"], "title": "FlowDC: Flow-Based Decoupling-Decay for Complex Image Editing", "comment": null, "summary": "With the surge of pre-trained text-to-image flow matching models, text-based image editing performance has gained remarkable improvement, especially for \\underline{simple editing} that only contains a single editing target. To satisfy the exploding editing requirements, the \\underline{complex editing} which contains multiple editing targets has posed as a more challenging task. However, current complex editing solutions: single-round and multi-round editing are limited by long text following and cumulative inconsistency, respectively. Thus, they struggle to strike a balance between semantic alignment and source consistency. In this paper, we propose \\textbf{FlowDC}, which decouples the complex editing into multiple sub-editing effects and superposes them in parallel during the editing process. Meanwhile, we observed that the velocity quantity that is orthogonal to the editing displacement harms the source structure preserving. Thus, we decompose the velocity and decay the orthogonal part for better source consistency. To evaluate the effectiveness of complex editing settings, we construct a complex editing benchmark: Complex-PIE-Bench. On two benchmarks, FlowDC shows superior results compared with existing methods. We also detail the ablations of our module designs.", "AI": {"tldr": "FlowDC提出了一种解耦和并行叠加的复杂文本到图像编辑方法，并通过分解速度来提高源图像一致性，解决了现有方法在语义对齐和源一致性之间的平衡问题。", "motivation": "预训练文本到图像流匹配模型在简单图像编辑方面表现出色，但对于包含多个编辑目标的“复杂编辑”仍面临挑战。现有复杂编辑解决方案（单轮和多轮编辑）分别受限于长文本跟随和累积不一致性，难以在语义对齐和源图像一致性之间取得平衡。", "method": "本文提出了FlowDC方法，它将复杂编辑解耦为多个子编辑效果，并在编辑过程中并行叠加这些效果。同时，该方法观察到与编辑位移正交的速度分量会损害源结构保留，因此分解了速度并衰减了正交部分以更好地保持源一致性。为评估复杂编辑设置的有效性，作者还构建了一个复杂编辑基准：Complex-PIE-Bench。", "result": "在两个基准测试（包括Complex-PIE-Bench）上，FlowDC显示出优于现有方法的性能。论文还详细介绍了其模块设计的消融研究。", "conclusion": "FlowDC通过解耦和并行叠加编辑效果以及分解速度以增强源一致性的方法，有效地解决了复杂文本到图像编辑的挑战，并在相关基准上取得了卓越表现。"}}
{"id": "2512.11336", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11336", "abs": "https://arxiv.org/abs/2512.11336", "authors": ["Hewen Pan", "Cong Wei", "Dashuang Liang", "Zepeng Huang", "Pengfei Gao", "Ziqi Zhou", "Lulu Xue", "Pengfei Yan", "Xiaoming Wei", "Minghui Li", "Shengshan Hu"], "title": "UFVideo: Towards Unified Fine-Grained Video Cooperative Understanding with Large Language Models", "comment": "22 pages, 13 figures, technical report", "summary": "With the advancement of multi-modal Large Language Models (LLMs), Video LLMs have been further developed to perform on holistic and specialized video understanding. However, existing works are limited to specialized video understanding tasks, failing to achieve a comprehensive and multi-grained video perception. To bridge this gap, we introduce UFVideo, the first Video LLM with unified multi-grained cooperative understanding capabilities. Specifically, we design unified visual-language guided alignment to flexibly handle video understanding across global, pixel and temporal scales within a single model. UFVideo dynamically encodes the visual and text inputs of different tasks and generates the textual response, temporal localization, or grounded mask. Additionally, to evaluate challenging multi-grained video understanding tasks, we construct the UFVideo-Bench consisting of three distinct collaborative tasks within the scales, which demonstrates UFVideo's flexibility and advantages over GPT-4o. Furthermore, we validate the effectiveness of our model across 9 public benchmarks covering various common video understanding tasks, providing valuable insights for future Video LLMs.", "AI": {"tldr": "UFVideo是首个统一多粒度协同理解的视频大语言模型，能在一个模型中处理全局、像素和时间尺度的视频理解任务，并在多项基准测试中表现出色。", "motivation": "现有的视频大语言模型（Video LLMs）主要局限于专门的视频理解任务，未能实现全面且多粒度的视频感知。", "method": "本文提出了UFVideo，通过设计统一的视觉-语言引导对齐机制，使其能够在单一模型中灵活处理全局、像素和时间尺度的视频理解。UFVideo能动态编码不同任务的视觉和文本输入，并生成文本响应、时间定位或接地掩码。此外，为评估多粒度视频理解任务，构建了UFVideo-Bench基准。", "result": "UFVideo在UFVideo-Bench上展示了优于GPT-4o的灵活性和优势，并在9个公共视频理解基准测试中验证了其有效性。", "conclusion": "UFVideo通过实现统一的多粒度协同理解能力，为未来的视频大语言模型提供了有价值的见解。"}}
{"id": "2512.11340", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11340", "abs": "https://arxiv.org/abs/2512.11340", "authors": ["Fei Long", "Yao Zhang", "Jiaming Lv", "Jiangtao Xie", "Peihua Li"], "title": "Task-Specific Distance Correlation Matching for Few-Shot Action Recognition", "comment": "9 pages. 4 figures, conference", "summary": "Few-shot action recognition (FSAR) has recently made notable progress through set matching and efficient adaptation of large-scale pre-trained models. However, two key limitations persist. First, existing set matching metrics typically rely on cosine similarity to measure inter-frame linear dependencies and then perform matching with only instance-level information, thus failing to capture more complex patterns such as nonlinear relationships and overlooking task-specific cues. Second, for efficient adaptation of CLIP to FSAR, recent work performing fine-tuning via skip-fusion layers (which we refer to as side layers) has significantly reduced memory cost. However, the newly introduced side layers are often difficult to optimize under limited data conditions. To address these limitations, we propose TS-FSAR, a framework comprising three components: (1) a visual Ladder Side Network (LSN) for efficient CLIP fine-tuning; (2) a metric called Task-Specific Distance Correlation Matching (TS-DCM), which uses $α$-distance correlation to model both linear and nonlinear inter-frame dependencies and leverages a task prototype to enable task-specific matching; and (3) a Guiding LSN with Adapted CLIP (GLAC) module, which regularizes LSN using the adapted frozen CLIP to improve training for better $α$-distance correlation estimation under limited supervision. Extensive experiments on five widely-used benchmarks demonstrate that our TS-FSAR yields superior performance compared to prior state-of-the-arts.", "AI": {"tldr": "本文提出TS-FSAR框架，通过引入视觉阶梯侧网络（LSN）、任务特定距离相关匹配（TS-DCM）和引导LSN模块（GLAC），解决了少样本动作识别中现有集合匹配和CLIP高效微调的局限性，实现了卓越的性能。", "motivation": "现有少样本动作识别方法存在两个主要局限性：1) 集合匹配指标通常只依赖余弦相似度测量帧间线性依赖，忽略非线性关系和任务特定线索。2) 用于CLIP高效适应的跳跃融合层（侧层）在有限数据条件下难以优化。", "method": "本文提出TS-FSAR框架，包含三个组件：1) 视觉阶梯侧网络（LSN），用于高效微调CLIP。2) 任务特定距离相关匹配（TS-DCM），利用α-距离相关性建模帧间线性和非线性依赖，并结合任务原型进行任务特定匹配。3) 引导LSN模块（GLAC），通过适应的冻结CLIP对LSN进行正则化，以在有限监督下改进训练并更好地估计α-距离相关性。", "result": "在五个广泛使用的基准测试中，TS-FSAR的性能优于现有最先进的方法。", "conclusion": "TS-FSAR框架有效解决了少样本动作识别中集合匹配的局限性和CLIP高效微调的优化难题，通过创新的网络结构和匹配度量显著提升了性能。"}}
{"id": "2512.11360", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11360", "abs": "https://arxiv.org/abs/2512.11360", "authors": ["Mohammad Sadegh Gholizadeh", "Amir Arsalan Rezapour", "Hamidreza Shayegh", "Ehsan Pazouki"], "title": "Reliable Detection of Minute Targets in High-Resolution Aerial Imagery across Temporal Shifts", "comment": null, "summary": "Efficient crop detection via Unmanned Aerial Vehicles is critical for scaling precision agriculture, yet it remains challenging due to the small scale of targets and environmental variability. This paper addresses the detection of rice seedlings in paddy fields by leveraging a Faster R-CNN architecture initialized via transfer learning. To overcome the specific difficulties of detecting minute objects in high-resolution aerial imagery, we curate a significant UAV dataset for training and rigorously evaluate the model's generalization capabilities. Specifically, we validate performance across three distinct test sets acquired at different temporal intervals, thereby assessing robustness against varying imaging conditions. Our empirical results demonstrate that transfer learning not only facilitates the rapid convergence of object detection models in agricultural contexts but also yields consistent performance despite domain shifts in image acquisition.", "AI": {"tldr": "本文提出一种基于Faster R-CNN和迁移学习的无人机稻苗检测方法，通过构建大型数据集并在不同时间间隔的测试集上验证，证明了其在环境变化下仍能实现高效且鲁棒的性能。", "motivation": "无人机高效作物检测对于规模化精准农业至关重要，但小目标和环境多变性使其面临挑战。本文旨在解决水稻田中稻苗的精确检测问题。", "method": "采用Faster R-CNN架构，通过迁移学习进行初始化。构建了一个大型无人机数据集用于训练。通过在不同时间间隔采集的三个独立测试集上验证模型性能，以评估其对不同成像条件的鲁棒性。", "result": "实证结果表明，迁移学习不仅能促进目标检测模型在农业场景中的快速收敛，而且即使在图像采集存在领域漂移的情况下，也能产生一致的性能。", "conclusion": "迁移学习能够有效帮助目标检测模型在农业环境中快速收敛，并能在成像条件变化时保持稳定的性能，从而实现对稻苗的鲁棒检测。"}}
{"id": "2512.11393", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11393", "abs": "https://arxiv.org/abs/2512.11393", "authors": ["Zhifan Zhu", "Yifei Huang", "Yoichi Sato", "Dima Damen"], "title": "The N-Body Problem: Parallel Execution from Single-Person Egocentric Video", "comment": "project webpage: https://zhifanzhu.github.io/ego-nbody", "summary": "Humans can intuitively parallelise complex activities, but can a model learn this from observing a single person? Given one egocentric video, we introduce the N-Body Problem: how N individuals, can hypothetically perform the same set of tasks observed in this video. The goal is to maximise speed-up, but naive assignment of video segments to individuals often violates real-world constraints, leading to physically impossible scenarios like two people using the same object or occupying the same space. To address this, we formalise the N-Body Problem and propose a suite of metrics to evaluate both performance (speed-up, task coverage) and feasibility (spatial collisions, object conflicts and causal constraints). We then introduce a structured prompting strategy that guides a Vision-Language Model (VLM) to reason about the 3D environment, object usage, and temporal dependencies to produce a viable parallel execution. On 100 videos from EPIC-Kitchens and HD-EPIC, our method for N = 2 boosts action coverage by 45% over a baseline prompt for Gemini 2.5 Pro, while simultaneously slashing collision rates, object and causal conflicts by 55%, 45% and 55% respectively.", "AI": {"tldr": "该研究引入“N体问题”，旨在从单个第一视角视频中学习如何让N个虚拟个体并行执行任务以最大化提速，同时避免现实世界中的冲突，并提出一种基于视觉语言模型（VLM）的结构化提示策略来解决此问题。", "motivation": "人类能直观地并行化复杂活动，但模型能否仅通过观察一个人的视频来学习这一点？挑战在于如何在最大化提速的同时，避免如两人使用同一物体或占用同一空间等物理上不可能的场景，这些是朴素任务分配会遇到的问题。", "method": "研究首先形式化了“N体问题”，即N个个体如何假设性地执行视频中观察到的同一组任务。接着，提出了一套评估性能（提速、任务覆盖率）和可行性（空间碰撞、物体冲突和因果约束）的指标。最后，引入了一种结构化提示策略，指导视觉语言模型（VLM）推理3D环境、物体使用和时间依赖性，以生成可行的并行执行方案。", "result": "在EPIC-Kitchens和HD-EPIC的100个视频上，当N=2时，该方法将动作覆盖率比Gemini 2.5 Pro的基线提示提高了45%，同时将碰撞率、物体冲突和因果冲突分别降低了55%、45%和55%。", "conclusion": "该研究提出的N体问题框架和基于VLM的结构化提示策略，能够有效实现从单个视频中学习任务的并行执行，显著提升了任务覆盖率和速度，同时大幅降低了现实世界中的冲突和不可行性。"}}
{"id": "2512.11354", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11354", "abs": "https://arxiv.org/abs/2512.11354", "authors": ["Qinghan Hu", "Haijiang Zhu", "Na Sun", "Lei Chen", "Zhengqiang Fan", "Zhiqing Li"], "title": "A Multi-Mode Structured Light 3D Imaging System with Multi-Source Information Fusion for Underwater Pipeline Detection", "comment": null, "summary": "Underwater pipelines are highly susceptible to corrosion, which not only shorten their service life but also pose significant safety risks. Compared with manual inspection, the intelligent real-time imaging system for underwater pipeline detection has become a more reliable and practical solution. Among various underwater imaging techniques, structured light 3D imaging can restore the sufficient spatial detail for precise defect characterization. Therefore, this paper develops a multi-mode underwater structured light 3D imaging system for pipeline detection (UW-SLD system) based on multi-source information fusion. First, a rapid distortion correction (FDC) method is employed for efficient underwater image rectification. To overcome the challenges of extrinsic calibration among underwater sensors, a factor graph-based parameter optimization method is proposed to estimate the transformation matrix between the structured light and acoustic sensors. Furthermore, a multi-mode 3D imaging strategy is introduced to adapt to the geometric variability of underwater pipelines. Given the presence of numerous disturbances in underwater environments, a multi-source information fusion strategy and an adaptive extended Kalman filter (AEKF) are designed to ensure stable pose estimation and high-accuracy measurements. In particular, an edge detection-based ICP (ED-ICP) algorithm is proposed. This algorithm integrates pipeline edge detection network with enhanced point cloud registration to achieve robust and high-fidelity reconstruction of defect structures even under variable motion conditions. Extensive experiments are conducted under different operation modes, velocities, and depths. The results demonstrate that the developed system achieves superior accuracy, adaptability and robustness, providing a solid foundation for autonomous underwater pipeline detection.", "AI": {"tldr": "本文开发了一种基于多源信息融合的多模式水下结构光三维成像系统（UW-SLD），用于水下管道缺陷检测。该系统通过快速畸变校正、因子图参数优化、多模式成像策略、自适应扩展卡尔曼滤波和基于边缘检测的ICP算法，实现了高精度、适应性和鲁棒性的管道缺陷三维重建。", "motivation": "水下管道极易腐蚀，不仅缩短寿命还带来安全隐患。相较于人工检测，智能实时成像系统更为可靠实用。其中，结构光三维成像能提供足够的空间细节进行精确缺陷表征，因此需要开发一种先进的水下结构光三维成像系统。", "method": "1. 开发了基于多源信息融合的多模式水下结构光三维成像系统（UW-SLD）。2. 采用快速畸变校正（FDC）方法进行高效图像校正。3. 提出基于因子图的参数优化方法，估计结构光与声学传感器间的外部变换矩阵。4. 引入多模式三维成像策略以适应管道几何变化。5. 设计多源信息融合策略和自适应扩展卡尔曼滤波（AEKF）以确保姿态估计和测量的高精度。6. 提出基于边缘检测的ICP（ED-ICP）算法，结合管道边缘检测网络和增强点云配准，实现缺陷结构的鲁棒高保真重建。", "result": "在不同操作模式、速度和深度下进行的广泛实验表明，所开发的系统具有卓越的精度、适应性和鲁棒性。它为水下管道的自主检测奠定了坚实的基础。", "conclusion": "本文开发的多模式水下结构光三维成像系统（UW-SLD）通过集成多种先进技术，成功克服了水下环境的挑战，实现了高精度、适应性强且鲁棒的水下管道缺陷检测，为自主水下管道检测提供了可靠方案。"}}
{"id": "2512.11356", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11356", "abs": "https://arxiv.org/abs/2512.11356", "authors": ["Meng-Li Shih", "Ying-Huan Chen", "Yu-Lun Liu", "Brian Curless"], "title": "Prior-Enhanced Gaussian Splatting for Dynamic Scene Reconstruction from Casual Video", "comment": null, "summary": "We introduce a fully automatic pipeline for dynamic scene reconstruction from casually captured monocular RGB videos. Rather than designing a new scene representation, we enhance the priors that drive Dynamic Gaussian Splatting. Video segmentation combined with epipolar-error maps yields object-level masks that closely follow thin structures; these masks (i) guide an object-depth loss that sharpens the consistent video depth, and (ii) support skeleton-based sampling plus mask-guided re-identification to produce reliable, comprehensive 2-D tracks. Two additional objectives embed the refined priors in the reconstruction stage: a virtual-view depth loss removes floaters, and a scaffold-projection loss ties motion nodes to the tracks, preserving fine geometry and coherent motion. The resulting system surpasses previous monocular dynamic scene reconstruction methods and delivers visibly superior renderings", "AI": {"tldr": "本文提出了一种从随意捕获的单目RGB视频重建动态场景的全自动流水线，通过增强Dynamic Gaussian Splatting的先验条件，实现了显著优于现有方法的重建和渲染效果。", "motivation": "从随意捕获的单目RGB视频中进行动态场景重建是一个具有挑战性的任务，现有方法可能在精细结构和运动连贯性方面存在不足。研究旨在通过增强先验知识来改进动态高斯泼溅法。", "method": "该方法通过以下步骤增强了Dynamic Gaussian Splatting的先验：1. 结合视频分割和对极误差图生成对象级掩码，以捕捉细微结构。2. 这些掩码用于指导对象深度损失，以锐化视频深度，并支持基于骨架的采样和掩码引导的重新识别，以生成可靠的2D跟踪。3. 在重建阶段引入两个额外目标：虚拟视图深度损失以消除浮点，以及脚手架投影损失以将运动节点与轨迹绑定，从而保留精细几何和连贯运动。", "result": "所提出的系统超越了以往的单目动态场景重建方法，并提供了明显更优质的渲染效果。", "conclusion": "通过引入增强的先验条件（包括精细对象掩码、可靠2D跟踪、虚拟视图深度损失和脚手架投影损失），本研究成功开发了一个全自动流水线，显著提升了从单目RGB视频重建动态场景的质量和渲染效果。"}}
{"id": "2512.11369", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11369", "abs": "https://arxiv.org/abs/2512.11369", "authors": ["Kuan Wang", "Yanjun Qin", "Mengge Lu", "Liejun Wang", "Xiaoming Tao"], "title": "Assisted Refinement Network Based on Channel Information Interaction for Camouflaged and Salient Object Detection", "comment": "15 pages, 9 figures", "summary": "Camouflaged Object Detection (COD) stands as a significant challenge in computer vision, dedicated to identifying and segmenting objects visually highly integrated with their backgrounds. Current mainstream methods have made progress in cross-layer feature fusion, but two critical issues persist during the decoding stage. The first is insufficient cross-channel information interaction within the same-layer features, limiting feature expressiveness. The second is the inability to effectively co-model boundary and region information, making it difficult to accurately reconstruct complete regions and sharp boundaries of objects. To address the first issue, we propose the Channel Information Interaction Module (CIIM), which introduces a horizontal-vertical integration mechanism in the channel dimension. This module performs feature reorganization and interaction across channels to effectively capture complementary cross-channel information. To address the second issue, we construct a collaborative decoding architecture guided by prior knowledge. This architecture generates boundary priors and object localization maps through Boundary Extraction (BE) and Region Extraction (RE) modules, then employs hybrid attention to collaboratively calibrate decoded features, effectively overcoming semantic ambiguity and imprecise boundaries. Additionally, the Multi-scale Enhancement (MSE) module enriches contextual feature representations. Extensive experiments on four COD benchmark datasets validate the effectiveness and state-of-the-art performance of the proposed model. We further transferred our model to the Salient Object Detection (SOD) task and demonstrated its adaptability across downstream tasks, including polyp segmentation, transparent object detection, and industrial and road defect detection. Code and experimental results are publicly available at: https://github.com/akuan1234/ARNet-v2.", "AI": {"tldr": "本文提出一种新的伪装目标检测（COD）模型，通过通道信息交互模块（CIIM）增强跨通道信息交互，并通过先验知识引导的协同解码架构（包含边界提取和区域提取模块及混合注意力）有效协同建模边界和区域信息，解决了现有方法在解码阶段的不足，实现了最先进的性能，并展现了在多项下游任务中的适应性。", "motivation": "现有伪装目标检测方法在解码阶段存在两个关键问题：一是同层特征内跨通道信息交互不足，限制了特征表达能力；二是无法有效协同建模边界和区域信息，导致难以准确重建完整的物体区域和清晰的边界。", "method": "为解决上述问题，本文提出：1) **通道信息交互模块（CIIM）**，引入水平-垂直集成机制在通道维度进行特征重组和交互，以捕获互补的跨通道信息。2) **先验知识引导的协同解码架构**，通过**边界提取（BE）**和**区域提取（RE）**模块生成边界先验和物体定位图，并利用**混合注意力**协同校准解码特征。此外，还引入了**多尺度增强（MSE）模块**以丰富上下文特征表示。", "result": "该模型在四个COD基准数据集上进行了广泛实验，验证了其有效性和最先进的性能。模型还成功迁移到显著目标检测（SOD）、息肉分割、透明物体检测以及工业和道路缺陷检测等下游任务中，展示了其良好的适应性。", "conclusion": "通过提出的通道信息交互模块和先验知识引导的协同解码架构，本研究成功解决了伪装目标检测中解码阶段的跨通道信息交互不足和边界区域协同建模困难的问题，显著提升了模型性能和泛化能力，并在多个相关视觉任务中展现出强大的适应性。"}}
{"id": "2512.11465", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.11465", "abs": "https://arxiv.org/abs/2512.11465", "authors": ["Mohamed Abdelsamad", "Michael Ulrich", "Bin Yang", "Miao Zhang", "Yakov Miron", "Abhinav Valada"], "title": "DOS: Distilling Observable Softmaps of Zipfian Prototypes for Self-Supervised Point Representation", "comment": "AAAI-26", "summary": "Recent advances in self-supervised learning (SSL) have shown tremendous potential for learning 3D point cloud representations without human annotations. However, SSL for 3D point clouds still faces critical challenges due to irregular geometry, shortcut-prone reconstruction, and unbalanced semantics distribution. In this work, we propose DOS (Distilling Observable Softmaps), a novel SSL framework that self-distills semantic relevance softmaps only at observable (unmasked) points. This strategy prevents information leakage from masked regions and provides richer supervision than discrete token-to-prototype assignments. To address the challenge of unbalanced semantics in an unsupervised setting, we introduce Zipfian prototypes and incorporate them using a modified Sinkhorn-Knopp algorithm, Zipf-Sinkhorn, which enforces a power-law prior over prototype usage and modulates the sharpness of the target softmap during training. DOS outperforms current state-of-the-art methods on semantic segmentation and 3D object detection across multiple benchmarks, including nuScenes, Waymo, SemanticKITTI, ScanNet, and ScanNet200, without relying on extra data or annotations. Our results demonstrate that observable-point softmaps distillation offers a scalable and effective paradigm for learning robust 3D representations.", "AI": {"tldr": "DOS是一种新颖的自监督学习框架，通过在可观察点蒸馏语义相关性软图，并结合Zipfian原型和Zipf-Sinkhorn算法处理语义不平衡问题，显著提升了3D点云表示学习的性能。", "motivation": "3D点云的自监督学习面临不规则几何、易产生捷径的重建以及语义分布不平衡等关键挑战，现有方法难以有效解决。", "method": "本文提出了DOS框架，其核心策略是在可观察（未遮蔽）点上自蒸馏语义相关性软图，以防止信息泄露并提供更丰富的监督。为解决无监督设置下的语义不平衡问题，引入了Zipfian原型，并通过改进的Sinkhorn-Knopp算法（Zipf-Sinkhorn）将其融入，该算法强制原型使用遵循幂律先验并调节目标软图的锐度。", "result": "DOS在多个基准测试（包括nuScenes、Waymo、SemanticKITTI、ScanNet和ScanNet200）的语义分割和3D目标检测任务上，无需额外数据或标注，均优于当前最先进的方法。", "conclusion": "可观察点软图蒸馏提供了一种可扩展且有效的范式，用于学习鲁棒的3D表示。"}}
{"id": "2512.11503", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11503", "abs": "https://arxiv.org/abs/2512.11503", "authors": ["Yanan Liu", "Jun Liu", "Hao Zhang", "Dan Xu", "Hossein Rahmani", "Mohammed Bennamoun", "Qiuhong Ke"], "title": "TSkel-Mamba: Temporal Dynamic Modeling via State Space Model for Human Skeleton-based Action Recognition", "comment": null, "summary": "Skeleton-based action recognition has garnered significant attention in the computer vision community. Inspired by the recent success of the selective state-space model (SSM) Mamba in modeling 1D temporal sequences, we propose TSkel-Mamba, a hybrid Transformer-Mamba framework that effectively captures both spatial and temporal dynamics. In particular, our approach leverages Spatial Transformer for spatial feature learning while utilizing Mamba for temporal modeling. Mamba, however, employs separate SSM blocks for individual channels, which inherently limits its ability to model inter-channel dependencies. To better adapt Mamba for skeleton data and enhance Mamba`s ability to model temporal dependencies, we introduce a Temporal Dynamic Modeling (TDM) block, which is a versatile plug-and-play component that integrates a novel Multi-scale Temporal Interaction (MTI) module. The MTI module employs multi-scale Cycle operators to capture cross-channel temporal interactions, a critical factor in action recognition. Extensive experiments on NTU-RGB+D 60, NTU-RGB+D 120, NW-UCLA and UAV-Human datasets demonstrate that TSkel-Mamba achieves state-of-the-art performance while maintaining low inference time, making it both efficient and highly effective.", "AI": {"tldr": "TSkel-Mamba是一种混合Transformer-Mamba框架，用于骨架行为识别，通过引入TDM模块和MTI模块有效捕捉空间和跨通道时间动态，实现了最先进的性能和低推理时间。", "motivation": "受Mamba在1D时间序列建模方面成功的启发，但Mamba在处理骨架数据时，其独立通道的SSM块限制了建模通道间依赖的能力。", "method": "提出了TSkel-Mamba，一个混合Transformer-Mamba框架。利用Spatial Transformer学习空间特征，Mamba进行时间建模。为解决Mamba的通道间依赖限制，引入了时间动态建模（TDM）块，其中包含多尺度时间交互（MTI）模块，该模块使用多尺度循环操作捕捉跨通道时间交互。", "result": "在NTU-RGB+D 60、NTU-RGB+D 120、NW-UCLA和UAV-Human数据集上取得了最先进的性能，并保持了较低的推理时间。", "conclusion": "TSkel-Mamba在骨架行为识别方面既高效又非常有效。"}}
{"id": "2512.11446", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11446", "abs": "https://arxiv.org/abs/2512.11446", "authors": ["Ahmed Mujtaba", "Gleb Radchenko", "Marc Masana", "Radu Prodan"], "title": "YawDD+: Frame-level Annotations for Accurate Yawn Prediction", "comment": "This paper is submitted at European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2026", "summary": "Driver fatigue remains a leading cause of road accidents, with 24\\% of crashes involving drowsy drivers. While yawning serves as an early behavioral indicator of fatigue, existing machine learning approaches face significant challenges due to video-annotated datasets that introduce systematic noise from coarse temporal annotations. We develop a semi-automated labeling pipeline with human-in-the-loop verification, which we apply to YawDD, enabling more accurate model training. Training the established MNasNet classifier and YOLOv11 detector architectures on YawDD+ improves frame accuracy by up to 6\\% and mAP by 5\\% over video-level supervision, achieving 99.34\\% classification accuracy and 95.69\\% detection mAP. The resulting approach deliver up to 59.8 FPS on edge AI hardware (NVIDIA Jetson Nano), confirming that enhanced data quality alone supports on-device yawning monitoring without server-side computation.", "AI": {"tldr": "为解决现有打哈欠检测模型因粗糙视频标注而产生的噪声问题，本文开发了一种半自动化标注流程，并将其应用于YawDD数据集，显著提升了分类和检测模型的准确性，实现了在边缘设备上的高效实时打哈欠监测。", "motivation": "驾驶员疲劳是导致交通事故的主要原因（占24%），而打哈欠是疲劳的早期行为指标。现有的机器学习方法在处理打哈欠检测时面临挑战，因为视频标注数据集存在系统性噪声，源于粗糙的时间标注。", "method": "本文开发了一个带有“人在回路”验证的半自动化标注流程，并将其应用于YawDD数据集，生成了更精确的YawDD+数据集。在此基础上，训练了已有的MNasNet分类器和YOLOv11检测器架构。", "result": "在YawDD+数据集上训练的模型，与视频级监督相比，帧准确率提高了6%，mAP提高了5%。最终分类准确率达到99.34%，检测mAP达到95.69%。该方法在边缘AI硬件（NVIDIA Jetson Nano）上可达到59.8 FPS的帧率。", "conclusion": "仅通过提高数据质量，就能显著支持在边缘设备上进行打哈欠监测，无需服务器端计算，从而实现了高效的实时疲劳预警。"}}
{"id": "2512.11507", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11507", "abs": "https://arxiv.org/abs/2512.11507", "authors": ["Mianjie Zheng", "Xinquan Yang", "Along He", "Xuguang Li", "Feilie Zhong", "Xuefen Liu", "Kun Tang", "Zhicheng Zhang", "Linlin Shen"], "title": "SSA3D: Text-Conditioned Assisted Self-Supervised Framework for Automatic Dental Abutment Design", "comment": null, "summary": "Abutment design is a critical step in dental implant restoration. However, manual design involves tedious measurement and fitting, and research on automating this process with AI is limited, due to the unavailability of large annotated datasets. Although self-supervised learning (SSL) can alleviate data scarcity, its need for pre-training and fine-tuning results in high computational costs and long training times. In this paper, we propose a Self-supervised assisted automatic abutment design framework (SS$A^3$D), which employs a dual-branch architecture with a reconstruction branch and a regression branch. The reconstruction branch learns to restore masked intraoral scan data and transfers the learned structural information to the regression branch. The regression branch then predicts the abutment parameters under supervised learning, which eliminates the separate pre-training and fine-tuning process. We also design a Text-Conditioned Prompt (TCP) module to incorporate clinical information (such as implant location, system, and series) into SS$A^3$D. This guides the network to focus on relevant regions and constrains the parameter predictions. Extensive experiments on a collected dataset show that SS$A^3$D saves half of the training time and achieves higher accuracy than traditional SSL methods. It also achieves state-of-the-art performance compared to other methods, significantly improving the accuracy and efficiency of automated abutment design.", "AI": {"tldr": "本文提出了一种名为SS$A^3$D的自监督辅助自动基台设计框架，通过双分支架构和文本条件提示模块，解决了牙科基台设计中数据稀缺和传统自监督学习计算成本高的问题，显著提高了自动化设计的准确性和效率。", "motivation": "牙科种植体修复中的基台设计手动过程繁琐且耗时。尽管AI自动化潜力巨大，但由于缺乏大型标注数据集，相关研究受限。传统的自监督学习（SSL）虽能缓解数据稀缺，但其预训练和微调过程导致计算成本高昂和训练时间长。", "method": "本文提出了SS$A^3$D框架，采用双分支架构：一个重建分支学习恢复被遮蔽的口内扫描数据，并将结构信息传递给回归分支；一个回归分支在监督学习下预测基台参数，从而消除了单独的预训练和微调过程。此外，设计了一个文本条件提示（TCP）模块，将临床信息（如种植体位置、系统、系列）融入SS$A^3$D，以引导网络关注相关区域并约束参数预测。", "result": "在收集的数据集上进行的广泛实验表明，SS$A^3$D相比传统自监督学习方法节省了一半的训练时间，并实现了更高的准确性。与现有其他方法相比，它也达到了最先进的性能，显著提升了自动化基台设计的准确性和效率。", "conclusion": "SS$A^3$D框架通过其独特的双分支架构和文本条件提示模块，有效地解决了牙科基台自动化设计中的数据稀缺和计算效率问题，实现了比传统方法更快的训练速度和更高的设计准确性，为自动化基台设计提供了最先进的解决方案。"}}
{"id": "2512.11508", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11508", "abs": "https://arxiv.org/abs/2512.11508", "authors": ["Jelena Bratulić", "Sudhanshu Mittal", "Thomas Brox", "Christian Rupprecht"], "title": "On Geometric Understanding and Learned Data Priors in VGGT", "comment": null, "summary": "The Visual Geometry Grounded Transformer (VGGT) is a 3D foundation model that infers camera geometry and scene structure in a single feed-forward pass. Trained in a supervised, single-step fashion on large datasets, VGGT raises a key question: does it build upon geometric concepts like traditional multi-view methods, or does it rely primarily on learned appearance-based data-driven priors? In this work, we conduct a systematic analysis of VGGT's internal mechanisms to uncover whether geometric understanding emerges within its representations. By probing intermediate features, analyzing attention patterns, and performing interventions, we examine how the model implements its functionality. Our findings reveal that VGGT implicitly performs correspondence matching within its global attention layers and encodes epipolar geometry, despite being trained without explicit geometric constraints. We further investigate VGGT's dependence on its learned data priors. Using spatial input masking and perturbation experiments, we assess its robustness to occlusions, appearance variations, and camera configurations, comparing it with classical multi-stage pipelines. Together, these insights highlight how VGGT internalizes geometric structure while using learned data-driven priors.", "AI": {"tldr": "本文系统分析了3D基础模型VGGT的内部机制，发现它在没有显式几何约束的情况下，能够隐式执行对应匹配并编码对极几何，同时依赖学习到的数据驱动先验。", "motivation": "VGGT作为一个单步、监督训练的3D基础模型，引发了一个关键问题：它是否像传统多视图方法那样基于几何概念，还是主要依赖学习到的外观数据驱动先验？本文旨在通过系统分析揭示其内部几何理解的形成。", "method": "通过探查中间特征、分析注意力模式和执行干预来检查模型功能。使用空间输入遮罩和扰动实验评估其对遮挡、外观变化和相机配置的鲁棒性，并与经典多阶段管道进行比较。", "result": "研究发现VGGT在其全局注意力层中隐式执行对应匹配，并编码对极几何，尽管训练时没有明确的几何约束。实验还评估了VGGT对其学习到的数据先验的依赖性及其在不同条件下的鲁棒性。", "conclusion": "这些见解突出表明，VGGT在内部整合了几何结构，同时有效利用了学习到的数据驱动先验。"}}
{"id": "2512.11490", "categories": ["cs.CV", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.11490", "abs": "https://arxiv.org/abs/2512.11490", "authors": ["Emanuel Sánchez Aimar", "Gulnaz Zhambulova", "Fahad Shahbaz Khan", "Yonghao Xu", "Michael Felsberg"], "title": "VLM2GeoVec: Toward Universal Multimodal Embeddings for Remote Sensing", "comment": "21 pages, 7 figures, under review", "summary": "Satellite imagery differs fundamentally from natural images: its aerial viewpoint, very high resolution, diverse scale variations, and abundance of small objects demand both region-level spatial reasoning and holistic scene understanding. Current remote-sensing approaches remain fragmented between dual-encoder retrieval models, which excel at large-scale cross-modal search but cannot interleave modalities, and generative assistants, which support region-level interpretation but lack scalable retrieval capabilities. We propose $\\textbf{VLM2GeoVec}$, an instruction-following, single-encoder vision-language model trained contrastively to embed interleaved inputs (images, text, bounding boxes, and geographic coordinates) in a unified vector space. Our single encoder interleaves all inputs into one joint embedding trained with a contrastive loss, eliminating multi-stage pipelines and task-specific modules. To evaluate its versatility, we introduce $\\textbf{RSMEB}$, a novel benchmark covering key remote-sensing embedding applications: scene classification; cross-modal search; compositional retrieval; visual-question answering; visual grounding and region-level reasoning; and semantic geospatial retrieval. On RSMEB, it achieves $\\textbf{26.6%}$ P@1 on region-caption retrieval (+25 pp vs. dual-encoder baselines), $\\textbf{32.5%}$ P@1 on referring-expression retrieval (+19 pp), and $\\textbf{17.8%}$ P@1 on semantic geo-localization retrieval (over $3\\times$ prior best), while matching or exceeding specialized baselines on conventional tasks such as scene classification and cross-modal retrieval. VLM2GeoVec unifies scalable retrieval with region-level spatial reasoning, enabling cohesive multimodal analysis in remote sensing. We will publicly release the code, checkpoints, and data upon acceptance.", "AI": {"tldr": "本文提出了VLM2GeoVec，一个单编码器视觉语言模型，通过对比学习将交错输入（图像、文本、边界框、地理坐标）嵌入统一向量空间，旨在解决遥感领域中可扩展检索与区域级空间推理的碎片化问题，并在新基准RSMEB上显著优于现有方法。", "motivation": "卫星图像与自然图像存在根本差异，需要区域级空间推理和整体场景理解。现有遥感方法碎片化：双编码器检索模型擅长大规模跨模态搜索但无法交错模态，而生成式助手支持区域级解释但缺乏可扩展的检索能力。因此，需要一个能统一可扩展检索和区域级空间推理的连贯多模态分析模型。", "method": "本文提出了VLM2GeoVec，一个遵循指令的单编码器视觉语言模型。该模型通过对比损失进行训练，将所有输入（图像、文本、边界框和地理坐标）交错编码到一个统一的联合嵌入空间中，从而消除了多阶段管道和特定任务模块。为评估其通用性，还引入了RSMEB，一个涵盖场景分类、跨模态搜索、组合检索、视觉问答、视觉定位和区域级推理以及语义地理空间检索等关键遥感嵌入应用的新基准。", "result": "在RSMEB基准测试中，VLM2GeoVec在区域-字幕检索上实现了26.6%的P@1（比双编码器基线提高25个百分点），在指代表达检索上实现了32.5%的P@1（提高19个百分点），在语义地理定位检索上实现了17.8%的P@1（是先前最佳水平的3倍多），同时在场景分类和跨模态检索等传统任务上匹配或超越了专业基线。", "conclusion": "VLM2GeoVec成功地将可扩展检索与区域级空间推理统一起来，从而在遥感领域实现了连贯的多模态分析。它提供了一个统一的解决方案，解决了当前方法在处理卫星图像复杂性方面的不足。"}}
{"id": "2512.11480", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11480", "abs": "https://arxiv.org/abs/2512.11480", "authors": ["Weijian Ma", "Shizhao Sun", "Ruiyu Wang", "Jiang Bian"], "title": "CADMorph: Geometry-Driven Parametric CAD Editing via a Plan-Generate-Verify Loop", "comment": "NeurIPS 2025", "summary": "A Computer-Aided Design (CAD) model encodes an object in two coupled forms: a parametric construction sequence and its resulting visible geometric shape. During iterative design, adjustments to the geometric shape inevitably require synchronized edits to the underlying parametric sequence, called geometry-driven parametric CAD editing. The task calls for 1) preserving the original sequence's structure, 2) ensuring each edit's semantic validity, and 3) maintaining high shape fidelity to the target shape, all under scarce editing data triplets. We present CADMorph, an iterative plan-generate-verify framework that orchestrates pretrained domain-specific foundation models during inference: a parameter-to-shape (P2S) latent diffusion model and a masked-parameter-prediction (MPP) model. In the planning stage, cross-attention maps from the P2S model pinpoint the segments that need modification and offer editing masks. The MPP model then infills these masks with semantically valid edits in the generation stage. During verification, the P2S model embeds each candidate sequence in shape-latent space, measures its distance to the target shape, and selects the closest one. The three stages leverage the inherent geometric consciousness and design knowledge in pretrained priors, and thus tackle structure preservation, semantic validity, and shape fidelity respectively. Besides, both P2S and MPP models are trained without triplet data, bypassing the data-scarcity bottleneck. CADMorph surpasses GPT-4o and specialized CAD baselines, and supports downstream applications such as iterative editing and reverse-engineering enhancement.", "AI": {"tldr": "CADMorph是一个迭代的“规划-生成-验证”框架，利用预训练的领域特定基础模型（P2S和MPP）解决几何驱动的参数化CAD编辑中的结构保持、语义有效性和形状保真度问题，且无需三元组编辑数据。", "motivation": "在迭代设计中，几何形状的调整需要同步修改底层的参数序列（即几何驱动的参数化CAD编辑）。这项任务面临三大挑战：1）保持原始序列结构；2）确保每次编辑的语义有效性；3）在高形状保真度下匹配目标形状。此外，这类编辑的三元组数据（原始序列、编辑操作、目标序列）非常稀缺。", "method": "本文提出了CADMorph，一个迭代的“规划-生成-验证”框架，协同使用预训练的领域特定基础模型：一个参数到形状（P2S）的潜在扩散模型和一个掩码参数预测（MPP）模型。在规划阶段，P2S模型的交叉注意力图识别需要修改的片段并提供编辑掩码。在生成阶段，MPP模型利用这些掩码填充语义有效的编辑。在验证阶段，P2S模型将每个候选序列嵌入到形状潜在空间，测量其与目标形状的距离，并选择最接近的一个。这三个阶段都利用了预训练先验中固有的几何意识和设计知识。值得注意的是，P2S和MPP模型均在没有三元组数据的情况下进行训练，绕过了数据稀缺的瓶颈。", "result": "CADMorph在几何驱动的参数化CAD编辑任务中超越了GPT-4o和专门的CAD基线。它支持迭代编辑和逆向工程增强等下游应用。", "conclusion": "CADMorph通过其创新的“规划-生成-验证”框架，并有效利用预训练的基础模型，成功解决了几何驱动的参数化CAD编辑中的关键挑战，包括结构保持、语义有效性和形状保真度，同时克服了数据稀缺问题，展现了卓越的性能和广泛的应用潜力。"}}
{"id": "2512.11423", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11423", "abs": "https://arxiv.org/abs/2512.11423", "authors": ["Chaochao Li", "Ruikui Wang", "Liangbo Zhou", "Jinheng Feng", "Huaishao Luo", "Huan Zhang", "Youzheng Wu", "Xiaodong He"], "title": "JoyAvatar: Real-time and Infinite Audio-Driven Avatar Generation with Autoregressive Diffusion", "comment": null, "summary": "Existing DiT-based audio-driven avatar generation methods have achieved considerable progress, yet their broader application is constrained by limitations such as high computational overhead and the inability to synthesize long-duration videos. Autoregressive methods address this problem by applying block-wise autoregressive diffusion methods. However, these methods suffer from the problem of error accumulation and quality degradation. To address this, we propose JoyAvatar, an audio-driven autoregressive model capable of real-time inference and infinite-length video generation with the following contributions: (1) Progressive Step Bootstrapping (PSB), which allocates more denoising steps to initial frames to stabilize generation and reduce error accumulation; (2) Motion Condition Injection (MCI), enhancing temporal coherence by injecting noise-corrupted previous frames as motion condition; and (3) Unbounded RoPE via Cache-Resetting (URCR), enabling infinite-length generation through dynamic positional encoding. Our 1.3B-parameter causal model achieves 16 FPS on a single GPU and achieves competitive results in visual quality, temporal consistency, and lip synchronization.", "AI": {"tldr": "JoyAvatar是一种音频驱动的自回归模型，能够实时生成无限长视频的数字人，通过渐进式步长引导、运动条件注入和无界RoPE实现稳定、连贯且高效的生成。", "motivation": "现有基于DiT的音频驱动数字人生成方法存在计算开销大和无法合成长视频的局限性。自回归方法虽能解决长视频问题，但存在误差累积和质量下降的问题。", "method": "本文提出了JoyAvatar模型，包含三个主要贡献：1) 渐进式步长引导（PSB），为初始帧分配更多去噪步长以稳定生成并减少误差累积；2) 运动条件注入（MCI），通过注入带噪声的先前帧作为运动条件来增强时间连贯性；3) 通过缓存重置实现无界RoPE（URCR），通过动态位置编码实现无限长视频生成。", "result": "所提出的1.3B参数因果模型在单GPU上达到16 FPS的实时推理速度，并在视觉质量、时间一致性和唇语同步方面取得了有竞争力的结果。", "conclusion": "JoyAvatar成功解决了现有方法的局限性，实现了实时、无限长视频的音频驱动数字人生成，同时保持了高质量、时间连贯性和唇语同步。"}}
{"id": "2512.11510", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11510", "abs": "https://arxiv.org/abs/2512.11510", "authors": ["Hanyue Lou", "Jiayi Zhou", "Yang Zhang", "Boyu Li", "Yi Wang", "Guangnan Ye", "Boxin Shi"], "title": "Reconstruction as a Bridge for Event-Based Visual Question Answering", "comment": null, "summary": "Integrating event cameras with Multimodal Large Language Models (MLLMs) promises general scene understanding in challenging visual conditions, yet requires navigating a trade-off between preserving the unique advantages of event data and ensuring compatibility with frame-based models. We address this challenge by using reconstruction as a bridge, proposing a straightforward Frame-based Reconstruction and Tokenization (FRT) method and designing an efficient Adaptive Reconstruction and Tokenization (ART) method that leverages event sparsity. For robust evaluation, we introduce EvQA, the first objective, real-world benchmark for event-based MLLMs, comprising 1,000 event-Q&A pairs from 22 public datasets. Our experiments demonstrate that our methods achieve state-of-the-art performance on EvQA, highlighting the significant potential of MLLMs in event-based vision.", "AI": {"tldr": "本文提出FRT和ART两种方法，通过重建将事件相机数据与多模态大语言模型（MLLM）结合，并引入首个事件基MLLM客观基准EvQA，在其中实现了最先进的性能。", "motivation": "将事件相机与多模态大语言模型（MLLM）结合有望在挑战性视觉条件下实现通用场景理解。然而，这需要在保留事件数据独特优势的同时，确保与基于帧的模型兼容，这是一个关键挑战。", "method": "1. 提出直接的基于帧的重建和标记（FRT）方法作为桥梁。2. 设计高效的自适应重建和标记（ART）方法，该方法利用事件数据的稀疏性。3. 引入EvQA，这是第一个用于事件基MLLM的客观、真实世界基准，包含来自22个公开数据集的1,000个事件-问答对。", "result": "实验证明，所提出的FRT和ART方法在EvQA基准测试上取得了最先进的性能。", "conclusion": "研究结果突显了多模态大语言模型在事件基视觉领域的巨大潜力，并通过提出的方法在EvQA基准上的优异表现得到了验证。"}}
{"id": "2512.11542", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11542", "abs": "https://arxiv.org/abs/2512.11542", "authors": ["Hossein Shahabadi", "Niki Sepasian", "Arash Marioriyad", "Ali Sharifi-Zarchi", "Mahdieh Soleymani Baghshah"], "title": "Infinity and Beyond: Compositional Alignment in VAR and Diffusion T2I Models", "comment": null, "summary": "Achieving compositional alignment between textual descriptions and generated images - covering objects, attributes, and spatial relationships - remains a core challenge for modern text-to-image (T2I) models. Although diffusion-based architectures have been widely studied, the compositional behavior of emerging Visual Autoregressive (VAR) models is still largely unexamined. We benchmark six diverse T2I systems - SDXL, PixArt-$α$, Flux-Dev, Flux-Schnell, Infinity-2B, and Infinity-8B - across the full T2I-CompBench++ and GenEval suites, evaluating alignment in color and attribute binding, spatial relations, numeracy, and complex multi-object prompts. Across both benchmarks, Infinity-8B achieves the strongest overall compositional alignment, while Infinity-2B also matches or exceeds larger diffusion models in several categories, highlighting favorable efficiency-performance trade-offs. In contrast, SDXL and PixArt-$α$ show persistent weaknesses in attribute-sensitive and spatial tasks. These results provide the first systematic comparison of VAR and diffusion approaches to compositional alignment and establish unified baselines for the future development of the T2I model.", "AI": {"tldr": "本文系统性比较了视觉自回归（VAR）和扩散模型在文本到图像（T2I）合成中的组合对齐能力。研究发现，Infinity-8B在整体组合对齐方面表现最佳，而Infinity-2B也展现出良好的效率-性能权衡。", "motivation": "实现文本描述与生成图像之间的组合对齐（涵盖物体、属性和空间关系）是现代T2I模型面临的核心挑战。尽管扩散架构已被广泛研究，但新兴的视觉自回归（VAR）模型的组合行为仍未得到充分检验。", "method": "本文对六种不同的T2I系统（SDXL、PixArt-α、Flux-Dev、Flux-Schnell、Infinity-2B和Infinity-8B）进行了基准测试。评估使用了T2I-CompBench++和GenEval套件，涵盖了颜色和属性绑定、空间关系、数字能力以及复杂多对象提示的对齐。", "result": "Infinity-8B在整体组合对齐方面表现最强，而Infinity-2B在多个类别中也达到或超越了更大的扩散模型，展现了有利的效率-性能权衡。相比之下，SDXL和PixArt-α在属性敏感和空间任务中持续表现出弱点。", "conclusion": "这些结果首次系统地比较了VAR和扩散方法在组合对齐方面的表现，并为未来T2I模型的发展建立了统一的基线。"}}
{"id": "2512.11548", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11548", "abs": "https://arxiv.org/abs/2512.11548", "authors": ["Zhendi Gong", "Xin Chen"], "title": "SSL-MedSAM2: A Semi-supervised Medical Image Segmentation Framework Powered by Few-shot Learning of SAM2", "comment": "Accepted by MICCAI 2025 CARE Challenge, waiting for publication", "summary": "Despite the success of deep learning based models in medical image segmentation, most state-of-the-art (SOTA) methods perform fully-supervised learning, which commonly rely on large scale annotated training datasets. However, medical image annotation is highly time-consuming, hindering its clinical applications. Semi-supervised learning (SSL) has been emerged as an appealing strategy in training with limited annotations, largely reducing the labelling cost. We propose a novel SSL framework SSL-MedSAM2, which contains a training-free few-shot learning branch TFFS-MedSAM2 based on the pretrained large foundation model Segment Anything Model 2 (SAM2) for pseudo label generation, and an iterative fully-supervised learning branch FSL-nnUNet based on nnUNet for pseudo label refinement. The results on MICCAI2025 challenge CARE-LiSeg (Liver Segmentation) demonstrate an outstanding performance of SSL-MedSAM2 among other methods. The average dice scores on the test set in GED4 and T1 MRI are 0.9710 and 0.9648 respectively, and the Hausdorff distances are 20.07 and 21.97 respectively. The code is available via https://github.com/naisops/SSL-MedSAM2/tree/main.", "AI": {"tldr": "本文提出了一种名为SSL-MedSAM2的新型半监督学习框架，用于解决医学图像分割中注释数据稀缺的问题，通过结合预训练的大型模型SAM2和nnUNet实现了卓越的分割性能。", "motivation": "尽管深度学习在医学图像分割方面取得了成功，但大多数最先进的方法依赖于大规模标注数据集。医学图像标注耗时费力，阻碍了其临床应用。半监督学习（SSL）作为一种在有限标注下进行训练的策略，可以大幅降低标注成本。", "method": "SSL-MedSAM2框架包含两个分支：1) 基于预训练大型基础模型Segment Anything Model 2 (SAM2) 的无训练少样本学习分支TFFS-MedSAM2，用于生成伪标签。2) 基于nnUNet的迭代全监督学习分支FSL-nnUNet，用于伪标签的精炼。", "result": "SSL-MedSAM2在MICCAI2025 CARE-LiSeg（肝脏分割）挑战中表现出色。在GED4和T1 MRI测试集上的平均Dice分数分别为0.9710和0.9648，Hausdorff距离分别为20.07和21.97。", "conclusion": "SSL-MedSAM2在医学图像分割任务中，尤其是在标注数据有限的情况下，展现了卓越的性能，证明了其作为一种有效半监督学习策略的潜力。"}}
{"id": "2512.11524", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.11524", "abs": "https://arxiv.org/abs/2512.11524", "authors": ["Ekaterina Kalinicheva", "Florian Helen", "Stéphane Mermoz", "Florian Mouret", "Milena Planells"], "title": "Super-Resolved Canopy Height Mapping from Sentinel-2 Time Series Using LiDAR HD Reference Data across Metropolitan France", "comment": null, "summary": "Fine-scale forest monitoring is essential for understanding canopy structure and its dynamics, which are key indicators of carbon stocks, biodiversity, and forest health. Deep learning is particularly effective for this task, as it integrates spectral, temporal, and spatial signals that jointly reflect the canopy structure. To address this need, we introduce THREASURE-Net, a novel end-to-end framework for Tree Height Regression And Super-Resolution. The model is trained on Sentinel-2 time series using reference height metrics derived from LiDAR HD data at multiple spatial resolutions over Metropolitan France to produce annual height maps. We evaluate three model variants, producing tree-height predictions at 2.5 m, 5 m, and 10 m resolution. THREASURE-Net does not rely on any pretrained model nor on reference very high resolution optical imagery to train its super-resolution module; instead, it learns solely from LiDAR-derived height information. Our approach outperforms existing state-of-the-art methods based on Sentinel data and is competitive with methods based on very high resolution imagery. It can be deployed to generate high-precision annual canopy-height maps, achieving mean absolute errors of 2.62 m, 2.72 m, and 2.88 m at 2.5 m, 5 m, and 10 m resolution, respectively. These results highlight the potential of THREASURE-Net for scalable and cost-effective structural monitoring of temperate forests using only freely available satellite data. The source code for THREASURE-Net is available at: https://github.com/Global-Earth-Observation/threasure-net.", "AI": {"tldr": "本文提出了THREASURE-Net，一个端到端的深度学习框架，利用Sentinel-2时间序列和LiDAR数据生成高分辨率年度树高图，其性能优于现有方法，且仅使用免费卫星数据。", "motivation": "精细尺度的森林监测对于理解冠层结构及其动态至关重要，因为它们是碳储量、生物多样性和森林健康的关键指标。深度学习在整合光谱、时间、空间信号方面非常有效，能有效反映冠层结构，因此需要一个有效的方法来满足这一需求。", "method": "研究引入了THREASURE-Net，一个用于树高回归和超分辨率的端到端框架。该模型使用Sentinel-2时间序列数据，并以来自法国大都市地区多空间分辨率的LiDAR高清数据作为参考高度指标进行训练，以生成年度高度图。模型评估了三个变体，分别在2.5米、5米和10米分辨率下预测树高。THREASURE-Net的超分辨率模块不依赖任何预训练模型或超高分辨率光学图像，而是仅从LiDAR派生的高度信息中学习。", "result": "THREASURE-Net的性能优于基于Sentinel数据的现有最先进方法，并与基于超高分辨率图像的方法具有竞争力。它能够生成高精度的年度冠层高度图，在2.5米、5米和10米分辨率下，平均绝对误差（MAE）分别为2.62米、2.72米和2.88米。", "conclusion": "这些结果突显了THREASURE-Net在仅使用免费卫星数据的情况下，对温带森林进行可扩展且经济高效的结构监测的潜力。"}}
{"id": "2512.11654", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11654", "abs": "https://arxiv.org/abs/2512.11654", "authors": ["Luca Cazzola", "Ahed Alboody"], "title": "Kinetic Mining in Context: Few-Shot Action Synthesis via Text-to-Motion Distillation", "comment": null, "summary": "The acquisition cost for large, annotated motion datasets remains a critical bottleneck for skeletal-based Human Activity Recognition (HAR). Although Text-to-Motion (T2M) generative models offer a compelling, scalable source of synthetic data, their training objectives, which emphasize general artistic motion, and dataset structures fundamentally differ from HAR's requirements for kinematically precise, class-discriminative actions. This disparity creates a significant domain gap, making generalist T2M models ill-equipped for generating motions suitable for HAR classifiers. To address this challenge, we propose KineMIC (Kinetic Mining In Context), a transfer learning framework for few-shot action synthesis. KineMIC adapts a T2M diffusion model to an HAR domain by hypothesizing that semantic correspondences in the text encoding space can provide soft supervision for kinematic distillation. We operationalize this via a kinetic mining strategy that leverages CLIP text embeddings to establish correspondences between sparse HAR labels and T2M source data. This process guides fine-tuning, transforming the generalist T2M backbone into a specialized few-shot Action-to-Motion generator. We validate KineMIC using HumanML3D as the source T2M dataset and a subset of NTU RGB+D 120 as the target HAR domain, randomly selecting just 10 samples per action class. Our approach generates significantly more coherent motions, providing a robust data augmentation source that delivers a +23.1% accuracy points improvement. Animated illustrations and supplementary materials are available at (https://lucazzola.github.io/publications/kinemic).", "AI": {"tldr": "KineMIC是一个少样本动作合成的迁移学习框架，它通过利用CLIP文本嵌入进行运动挖掘，将通用的文本到运动（T2M）扩散模型适应到人体活动识别（HAR）领域，显著提高了数据增强的性能。", "motivation": "大规模标注运动数据集的获取成本高昂是骨骼基HAR的关键瓶颈。尽管文本到运动（T2M）生成模型提供了可扩展的合成数据来源，但其训练目标和数据集结构与HAR对运动精确性和类别区分性的要求存在显著差异，导致通用T2M模型不适用于HAR分类器。", "method": "本文提出了KineMIC（Kinetic Mining In Context）框架，通过假设文本编码空间中的语义对应关系可以为运动蒸馏提供软监督，将T2M扩散模型适应到HAR领域。具体而言，KineMIC利用CLIP文本嵌入来建立稀疏HAR标签与T2M源数据之间的对应关系，指导微调过程，将通用T2M骨干网络转化为专门的少样本动作到运动（A2M）生成器。", "result": "使用HumanML3D作为T2M源数据集，NTU RGB+D 120的子集作为目标HAR域（每类仅随机选择10个样本），KineMIC生成了显著更连贯的运动。作为一种强大的数据增强来源，它带来了+23.1%的准确率提升。", "conclusion": "KineMIC成功解决了T2M模型与HAR需求之间的领域差距问题，有效地将通用T2M模型转化为专门的少样本动作到运动生成器，为HAR提供了强大的数据增强来源。"}}
{"id": "2512.11575", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.11575", "abs": "https://arxiv.org/abs/2512.11575", "authors": ["Fabian Fuchs", "Mario Ruben Fernandez", "Norman Ettrich", "Janis Keuper"], "title": "In-Context Learning for Seismic Data Processing", "comment": "Source code available under https://codeberg.org/fuchsfa/in-context-learning-seismic", "summary": "Seismic processing transforms raw data into subsurface images essential for geophysical applications. Traditional methods face challenges, such as noisy data, and manual parameter tuning, among others. Recently deep learning approaches have proposed alternative solutions to some of these problems. However, important challenges of existing deep learning approaches are spatially inconsistent results across neighboring seismic gathers and lack of user-control. We address these limitations by introducing ContextSeisNet, an in-context learning model, to seismic demultiple processing. Our approach conditions predictions on a support set of spatially related example pairs: neighboring common-depth point gathers from the same seismic line and their corresponding labels. This allows the model to learn task-specific processing behavior at inference time by observing how similar gathers should be processed, without any retraining. This method provides both flexibility through user-defined examples and improved lateral consistency across seismic lines. On synthetic data, ContextSeisNet outperforms a U-Net baseline quantitatively and demonstrates enhanced spatial coherence between neighboring gathers. On field data, our model achieves superior lateral consistency compared to both traditional Radon demultiple and the U-Net baseline. Relative to the U-Net, ContextSeisNet also delivers improved near-offset performance and more complete multiple removal. Notably, ContextSeisNet achieves comparable field data performance despite being trained on 90% less data, demonstrating substantial data efficiency. These results establish ContextSeisNet as a practical approach for spatially consistent seismic demultiple with potential applicability to other seismic processing tasks.", "AI": {"tldr": "ContextSeisNet是一种基于上下文学习的地震去多次波模型，通过利用相邻地震道集作为示例进行推理，解决了现有深度学习方法在空间不一致性和用户控制方面的挑战，并在合成和实际数据上均表现出卓越的性能和数据效率。", "motivation": "传统的地震数据处理方法面临数据噪声和手动参数调整等挑战。现有的深度学习方法虽然提供了一些解决方案，但仍存在空间不一致性（相邻地震道集之间）和缺乏用户控制的问题。", "method": "本文提出了ContextSeisNet，一个用于地震去多次波的上下文学习模型。该方法在推理时，通过观察一组空间相关的示例对（即来自同一地震线的相邻共深度点道集及其对应标签）来调整预测。这种方式使得模型无需重新训练即可学习任务特定的处理行为，从而提供灵活性和改进的横向一致性。", "result": "在合成数据上，ContextSeisNet在量化指标上优于U-Net基线，并增强了相邻道集间的空间相干性。在实际数据上，与传统Radon去多次波和U-Net基线相比，该模型实现了卓越的横向一致性、改进的近偏移距性能和更彻底的多次波去除。值得注意的是，ContextSeisNet在训练数据量减少90%的情况下，仍能达到可媲美的实际数据性能，展现出显著的数据效率。", "conclusion": "ContextSeisNet被确立为一种实用的、能实现空间一致性地震去多次波的方法，并具有应用于其他地震处理任务的潜力。"}}
{"id": "2512.11645", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11645", "abs": "https://arxiv.org/abs/2512.11645", "authors": ["Jiapeng Tang", "Kai Li", "Chengxiang Yin", "Liuhao Ge", "Fei Jiang", "Jiu Xu", "Matthias Nießner", "Christian Häne", "Timur Bagautdinov", "Egor Zakharov", "Peihong Guo"], "title": "FactorPortrait: Controllable Portrait Animation via Disentangled Expression, Pose, and Viewpoint", "comment": "Project page: https://tangjiapeng.github.io/FactorPortrait/", "summary": "We introduce FactorPortrait, a video diffusion method for controllable portrait animation that enables lifelike synthesis from disentangled control signals of facial expressions, head movement, and camera viewpoints. Given a single portrait image, a driving video, and camera trajectories, our method animates the portrait by transferring facial expressions and head movements from the driving video while simultaneously enabling novel view synthesis from arbitrary viewpoints. We utilize a pre-trained image encoder to extract facial expression latents from the driving video as control signals for animation generation. Such latents implicitly capture nuanced facial expression dynamics with identity and pose information disentangled, and they are efficiently injected into the video diffusion transformer through our proposed expression controller. For camera and head pose control, we employ Plücker ray maps and normal maps rendered from 3D body mesh tracking. To train our model, we curate a large-scale synthetic dataset containing diverse combinations of camera viewpoints, head poses, and facial expression dynamics. Extensive experiments demonstrate that our method outperforms existing approaches in realism, expressiveness, control accuracy, and view consistency.", "AI": {"tldr": "FactorPortrait是一种视频扩散方法，通过解耦面部表情、头部运动和摄像机视角的控制信号，实现可控的肖像动画，并能从任意视角进行新颖视图合成。", "motivation": "现有方法难以实现从解耦控制信号（面部表情、头部运动和摄像机视角）进行逼真的肖像动画合成，需要一种能够同时从驱动视频中迁移表情和头部运动，并支持任意视角合成的方法。", "method": "该方法引入FactorPortrait，一个视频扩散模型。它使用预训练图像编码器从驱动视频中提取面部表情潜在向量作为控制信号，并通过提出的表情控制器注入到视频扩散Transformer中。对于摄像机和头部姿态控制，利用从3D身体网格跟踪渲染的Plücker射线图和法线图。模型在一个包含多样化摄像机视角、头部姿态和面部表情动态的大规模合成数据集上进行训练。", "result": "广泛的实验表明，该方法在真实感、表现力、控制精度和视图一致性方面优于现有方法。", "conclusion": "FactorPortrait通过其解耦控制信号和视频扩散架构，实现了卓越的可控肖像动画，能够生成逼真、富有表现力且视图一致的动画，并支持新颖视角合成。"}}
{"id": "2512.11683", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11683", "abs": "https://arxiv.org/abs/2512.11683", "authors": ["Qiushi Guo"], "title": "Depth-Copy-Paste: Multimodal and Depth-Aware Compositing for Robust Face Detection", "comment": null, "summary": "Data augmentation is crucial for improving the robustness of face detection systems, especially under challenging conditions such as occlusion, illumination variation, and complex environments. Traditional copy paste augmentation often produces unrealistic composites due to inaccurate foreground extraction, inconsistent scene geometry, and mismatched background semantics. To address these limitations, we propose Depth Copy Paste, a multimodal and depth aware augmentation framework that generates diverse and physically consistent face detection training samples by copying full body person instances and pasting them into semantically compatible scenes. Our approach first employs BLIP and CLIP to jointly assess semantic and visual coherence, enabling automatic retrieval of the most suitable background images for the given foreground person. To ensure high quality foreground masks that preserve facial details, we integrate SAM3 for precise segmentation and Depth-Anything to extract only the non occluded visible person regions, preventing corrupted facial textures from being used in augmentation. For geometric realism, we introduce a depth guided sliding window placement mechanism that searches over the background depth map to identify paste locations with optimal depth continuity and scale alignment. The resulting composites exhibit natural depth relationships and improved visual plausibility. Extensive experiments show that Depth Copy Paste provides more diverse and realistic training data, leading to significant performance improvements in downstream face detection tasks compared with traditional copy paste and depth free augmentation methods.", "AI": {"tldr": "本文提出了一种名为“深度复制粘贴”（Depth Copy Paste）的多模态深度感知数据增强框架，通过复制完整人物实例并粘贴到语义兼容的场景中，生成多样且物理一致的人脸检测训练样本，显著提升了下游人脸检测任务的性能。", "motivation": "传统复制粘贴数据增强方法在人脸检测中存在局限性，由于前景提取不准确、场景几何不一致和背景语义不匹配，常产生不真实的合成图像。这限制了人脸检测系统在遮挡、光照变化和复杂环境下的鲁棒性。", "method": "该方法首先利用BLIP和CLIP联合评估语义和视觉一致性，自动检索最合适的背景图像。为确保高质量前景掩码，集成了SAM3进行精确分割，并结合Depth-Anything提取未被遮挡的可见人物区域，避免使用损坏的面部纹理。为了实现几何真实性，引入了深度引导的滑动窗口放置机制，在背景深度图上搜索具有最佳深度连续性和尺度对齐的粘贴位置。", "result": "实验表明，深度复制粘贴生成的合成图像展现出自然的深度关系和更高的视觉真实性。与传统复制粘贴和无深度增强方法相比，该方法提供了更多样化、更真实的训练数据，从而显著提升了下游人脸检测任务的性能。", "conclusion": "深度复制粘贴框架通过解决传统复制粘贴的局限性，生成了物理一致且视觉真实的人脸检测训练样本，为人脸检测系统的鲁棒性提升提供了有效的数据增强方案。"}}
{"id": "2512.11557", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11557", "abs": "https://arxiv.org/abs/2512.11557", "authors": ["Zhiguo Lu", "Jianwen Lou", "Mingjun Ma", "Hairong Jin", "Youyi Zheng", "Kun Zhou"], "title": "3DTeethSAM: Taming SAM2 for 3D Teeth Segmentation", "comment": "Accepted by AAAI 2026", "summary": "3D teeth segmentation, involving the localization of tooth instances and their semantic categorization in 3D dental models, is a critical yet challenging task in digital dentistry due to the complexity of real-world dentition. In this paper, we propose 3DTeethSAM, an adaptation of the Segment Anything Model 2 (SAM2) for 3D teeth segmentation. SAM2 is a pretrained foundation model for image and video segmentation, demonstrating a strong backbone in various downstream scenarios. To adapt SAM2 for 3D teeth data, we render images of 3D teeth models from predefined views, apply SAM2 for 2D segmentation, and reconstruct 3D results using 2D-3D projections. Since SAM2's performance depends on input prompts and its initial outputs often have deficiencies, and given its class-agnostic nature, we introduce three light-weight learnable modules: (1) a prompt embedding generator to derive prompt embeddings from image embeddings for accurate mask decoding, (2) a mask refiner to enhance SAM2's initial segmentation results, and (3) a mask classifier to categorize the generated masks. Additionally, we incorporate Deformable Global Attention Plugins (DGAP) into SAM2's image encoder. The DGAP enhances both the segmentation accuracy and the speed of the training process. Our method has been validated on the 3DTeethSeg benchmark, achieving an IoU of 91.90% on high-resolution 3D teeth meshes, establishing a new state-of-the-art in the field.", "AI": {"tldr": "本文提出3DTeethSAM，一种将SAM2模型应用于3D牙齿分割的方法，通过2D渲染、SAM2分割、3D重建，并结合轻量级可学习模块和DGAP，实现了3D牙齿分割的新SOTA。", "motivation": "在数字牙科中，3D牙齿分割（包括牙齿实例定位和语义分类）是一项关键但具有挑战性的任务，原因在于真实牙列的复杂性。", "method": "该方法将预训练的SAM2模型应用于3D牙齿数据。具体步骤包括：从预定义视角渲染3D牙齿模型生成2D图像；应用SAM2进行2D分割；利用2D-3D投影重建3D结果。为解决SAM2对提示的依赖、初始输出缺陷及类别无关性，引入了三个轻量级可学习模块：1) 提示嵌入生成器（从图像嵌入生成提示嵌入以进行精确掩码解码）；2) 掩码优化器（增强SAM2的初始分割结果）；3) 掩码分类器（对生成掩码进行分类）。此外，将可变形全局注意力插件（DGAP）整合到SAM2的图像编码器中，以提高分割精度和训练速度。", "result": "该方法在3DTeethSeg基准测试中，对高分辨率3D牙齿网格实现了91.90%的IoU，创造了该领域的新SOTA。", "conclusion": "3DTeethSAM通过对SAM2的创新性适应和增强，显著提升了3D牙齿分割的性能，并在3DTeethSeg基准上确立了新的领先地位。"}}
{"id": "2512.11611", "categories": ["cs.CV", "cs.AR"], "pdf": "https://arxiv.org/pdf/2512.11611", "abs": "https://arxiv.org/abs/2512.11611", "authors": ["Chunyi Li", "Longfei Li", "Zicheng Zhang", "Xiaohong Liu", "Min Tang", "Weisi Lin", "Guangtao Zhai"], "title": "Using GUI Agent for Electronic Design Automation", "comment": "17 pages, 15 figures, 8 tables", "summary": "Graphical User Interface (GUI) agents adopt an end-to-end paradigm that maps a screenshot to an action sequence, thereby automating repetitive tasks in virtual environments. However, existing GUI agents are evaluated almost exclusively on commodity software such as Microsoft Word and Excel. Professional Computer-Aided Design (CAD) suites promise an order-of-magnitude higher economic return, yet remain the weakest performance domain for existing agents and are still far from replacing expert Electronic-Design-Automation (EDA) engineers. We therefore present the first systematic study that deploys GUI agents for EDA workflows. Our contributions are: (1) a large-scale dataset named GUI-EDA, including 5 CAD tools and 5 physical domains, comprising 2,000+ high-quality screenshot-answer-action pairs recorded by EDA scientists and engineers during real-world component design; (2) a comprehensive benchmark that evaluates 30+ mainstream GUI agents, demonstrating that EDA tasks constitute a major, unsolved challenge; and (3) an EDA-specialized metric named EDAgent, equipped with a reflection mechanism that achieves reliable performance on industrial CAD software and, for the first time, outperforms Ph.D. students majored in Electrical Engineering. This work extends GUI agents from generic office automation to specialized, high-value engineering domains and offers a new avenue for advancing EDA productivity. The dataset will be released at: https://github.com/aiben-ch/GUI-EDA.", "AI": {"tldr": "本文首次系统性地将图形用户界面（GUI）代理部署到电子设计自动化（EDA）工作流中，提出了一个大规模数据集、一个综合基准测试，并开发了一个名为EDAgent的专用代理，该代理在工业CAD软件上表现可靠，并首次超越了电气工程博士生。", "motivation": "现有GUI代理主要在通用办公软件上进行评估，而在经济回报更高的专业计算机辅助设计（CAD）套件（如EDA工具）上的性能非常弱，远未能取代专家。EDA任务对现有代理构成重大挑战。", "method": "研究方法包括：1) 构建了一个名为GUI-EDA的大规模数据集，包含2000多对由EDA科学家和工程师记录的真实世界组件设计中的高质量截图-答案-动作对，涵盖5种CAD工具和5个物理领域。2) 建立了一个综合基准测试，评估了30多个主流GUI代理。3) 开发了一个名为EDAgent的EDA专用度量标准，配备了反射机制。", "result": "主要结果表明，EDA任务对现有GUI代理来说是一个尚未解决的重大挑战。EDAgent在工业CAD软件上实现了可靠的性能，并且首次在性能上超越了电气工程博士生。", "conclusion": "这项工作将GUI代理从通用办公自动化扩展到专业的、高价值工程领域，为提高EDA生产力提供了一条新途径。"}}
{"id": "2512.11624", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11624", "abs": "https://arxiv.org/abs/2512.11624", "authors": ["Maik Dannecker", "Steven Jia", "Nil Stolt-Ansó", "Nadine Girard", "Guillaume Auzias", "François Rousseau", "Daniel Rueckert"], "title": "Fast and Explicit: Slice-to-Volume Reconstruction via 3D Gaussian Primitives with Analytic Point Spread Function Modeling", "comment": "Under Review for MIDL 2026", "summary": "Recovering high-fidelity 3D images from sparse or degraded 2D images is a fundamental challenge in medical imaging, with broad applications ranging from 3D ultrasound reconstruction to MRI super-resolution. In the context of fetal MRI, high-resolution 3D reconstruction of the brain from motion-corrupted low-resolution 2D acquisitions is a prerequisite for accurate neurodevelopmental diagnosis. While implicit neural representations (INRs) have recently established state-of-the-art performance in self-supervised slice-to-volume reconstruction (SVR), they suffer from a critical computational bottleneck: accurately modeling the image acquisition physics requires expensive stochastic Monte Carlo sampling to approximate the point spread function (PSF). In this work, we propose a shift from neural network based implicit representations to Gaussian based explicit representations. By parameterizing the HR 3D image volume as a field of anisotropic Gaussian primitives, we leverage the property of Gaussians being closed under convolution and thus derive a \\textit{closed-form analytical solution} for the forward model. This formulation reduces the previously intractable acquisition integral to an exact covariance addition ($\\mathbfΣ_{obs} = \\mathbfΣ_{HR} + \\mathbfΣ_{PSF}$), effectively bypassing the need for compute-intensive stochastic sampling while ensuring exact gradient propagation. We demonstrate that our approach matches the reconstruction quality of self-supervised state-of-the-art SVR frameworks while delivering a 5$\\times$--10$\\times$ speed-up on neonatal and fetal data. With convergence often reached in under 30 seconds, our framework paves the way towards translation into clinical routine of real-time fetal 3D MRI. Code will be public at {https://github.com/m-dannecker/Gaussian-Primitives-for-Fast-SVR}.", "AI": {"tldr": "本文提出了一种基于高斯显式表示的新方法，通过利用高斯卷积的封闭性，为医学图像（特别是胎儿MRI）的3D重建提供了一个闭式解析解，从而在保持重建质量的同时，实现了5-10倍的速度提升。", "motivation": "从稀疏或降级的2D图像中恢复高保真3D图像是医学成像中的一个基本挑战，对胎儿MRI等应用至关重要，以实现准确的神经发育诊断。现有的自监督切片到体积重建（SVR）方法，特别是基于隐式神经表示（INR）的方法，在建模图像采集物理时，需要昂贵的蒙特卡洛采样来近似点扩散函数（PSF），导致计算瓶颈。", "method": "研究者提出从基于神经网络的隐式表示转向基于高斯的显式表示。通过将高分辨率3D图像体积参数化为各向异性高斯原语的场，利用高斯在卷积下的封闭性，推导出了正向模型的闭式解析解。这种公式将之前难以处理的采集积分简化为精确的协方差加法（Σ_obs = Σ_HR + Σ_PSF），从而避免了计算密集型随机采样，并确保了精确的梯度传播。", "result": "该方法在重建质量上与最先进的自监督SVR框架相当，同时在新生儿和胎儿数据上实现了5-10倍的速度提升。重建通常在30秒内收敛。", "conclusion": "该框架显著提高了3D重建的速度和效率，为实时胎儿3D MRI在临床常规中的应用铺平了道路。"}}
{"id": "2512.11680", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11680", "abs": "https://arxiv.org/abs/2512.11680", "authors": ["Xu Zhang", "Jiabin Fang", "Zhuoming Ding", "Jin Yuan", "Xuan Liu", "Qianjun Zhang", "Zhiyong Li"], "title": "Cross-modal Context-aware Learning for Visual Prompt Guided Multimodal Image Understanding in Remote Sensing", "comment": "12 pages, 5 figures", "summary": "Recent advances in image understanding have enabled methods that leverage large language models for multimodal reasoning in remote sensing. However, existing approaches still struggle to steer models to the user-relevant regions when only simple, generic text prompts are available. Moreover, in large-scale aerial imagery many objects exhibit highly similar visual appearances and carry rich inter-object relationships, which further complicates accurate recognition. To address these challenges, we propose Cross-modal Context-aware Learning for Visual Prompt-Guided Multimodal Image Understanding (CLV-Net). CLV-Net lets users supply a simple visual cue, a bounding box, to indicate a region of interest, and uses that cue to guide the model to generate correlated segmentation masks and captions that faithfully reflect user intent. Central to our design is a Context-Aware Mask Decoder that models and integrates inter-object relationships to strengthen target representations and improve mask quality. In addition, we introduce a Semantic and Relationship Alignment module: a Cross-modal Semantic Consistency Loss enhances fine-grained discrimination among visually similar targets, while a Relationship Consistency Loss enforces alignment between textual relations and visual interactions. Comprehensive experiments on two benchmark datasets show that CLV-Net outperforms existing methods and establishes new state-of-the-art results. The model effectively captures user intent and produces precise, intention-aligned multimodal outputs.", "AI": {"tldr": "CLV-Net通过视觉提示（边界框）引导多模态理解，利用上下文感知解码器和语义关系对齐模块，有效解决遥感图像中目标识别和用户意图对齐的挑战，实现SOTA性能。", "motivation": "现有遥感图像多模态推理方法在仅有简单文本提示时，难以将模型引导至用户相关区域。此外，大规模航空影像中目标视觉外观高度相似且具有丰富的对象间关系，进一步复杂化了准确识别。", "method": "提出CLV-Net，允许用户提供边界框作为视觉提示。核心设计包括：1) 上下文感知掩码解码器，用于建模和整合对象间关系以增强目标表示和掩码质量。2) 语义和关系对齐模块，包含跨模态语义一致性损失以增强视觉相似目标间的细粒度区分，以及关系一致性损失以强制文本关系与视觉交互对齐。", "result": "在两个基准数据集上的实验表明，CLV-Net超越现有方法并达到SOTA性能。该模型有效捕获用户意图，并生成精确、与意图对齐的多模态输出。", "conclusion": "CLV-Net通过结合视觉提示和上下文感知学习，成功解决了遥感图像中多模态理解的挑战，尤其是在处理相似目标和复杂关系方面，实现了用户意图的精确捕获和高质量的多模态输出。"}}
{"id": "2512.11574", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11574", "abs": "https://arxiv.org/abs/2512.11574", "authors": ["Valentina Lilova", "Toyesh Chakravorty", "Julian I. Bibo", "Emma Boccaletti", "Brandon Li", "Lívia Baxová", "Cees G. M. Snoek", "Mohammadreza Salehi"], "title": "Evaluating Foundation Models' 3D Understanding Through Multi-View Correspondence Analysis", "comment": "NeurIPS 2025 UniReps workshop", "summary": "Benchmarking 3D spatial understanding of foundation models is essential for real-world applications such as robotics and autonomous driving. Existing evaluations often rely on downstream finetuning with linear heads or task-specific decoders, making it difficult to isolate the intrinsic 3D reasoning ability of pretrained encoders. In this work, we introduce a novel benchmark for in-context 3D scene understanding that requires no finetuning and directly probes the quality of dense visual features. Building on the Hummingbird framework, which evaluates in-context 2D scene understanding, we extend the setup to the 3D Multi-View ImageNet (MVImgNet) dataset. Given a set of images from objects in specific angles (keys), we benchmark the performance of segmenting novel views (queries) and report the scores in 4 categories of easy, medium, hard, and extreme based on the key-query view contrast. We benchmark 8 state-of-the-art foundation models and show DINO-based encoders remain competitive across large viewpoint shifts, while 3D-aware models like VGGT require dedicated multi-view adjustments. Our code is publicly available at https://github.com/ToyeshC/open-hummingbird-3d-eval .", "AI": {"tldr": "该研究引入了一个无需微调的基准测试，用于评估基础模型在3D场景下的上下文理解能力。通过将2D Hummingbird框架扩展到3D多视角ImageNet (MVImgNet)数据集，它直接探测密集视觉特征的质量，并在不同视角对比度下对新视角分割进行基准测试，发现DINO类编码器在视角变化大时仍具竞争力。", "motivation": "现有的基础模型3D空间理解评估通常依赖下游任务的微调，这使得难以独立评估预训练编码器固有的3D推理能力。因此，需要一个无需微调、能直接探测密集视觉特征质量的基准。", "method": "研究引入了一个新的、无需微调的上下文3D场景理解基准。它基于Hummingbird框架，并将其扩展到3D多视角ImageNet (MVImgNet)数据集。方法是通过给定特定角度的图像（键），来评估分割新视角（查询）的性能，并根据键-查询视角的对比度将分数分为简单、中等、困难和极端四类。该研究对8个最先进的基础模型进行了基准测试。", "result": "基准测试结果显示，基于DINO的编码器在较大的视角偏移下仍保持竞争力。而像VGGT这样的3D感知模型则需要专门的多视角调整才能表现良好。", "conclusion": "该研究提出的无需微调的基准测试有效地评估了基础模型的固有3D空间理解能力。它揭示了不同模型架构（如DINO的鲁棒性，VGGT对调整的需求）在上下文3D场景理解方面的优势和局限性。"}}
{"id": "2512.11691", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2512.11691", "abs": "https://arxiv.org/abs/2512.11691", "authors": ["Aya Kaysan Bahjat"], "title": "Text images processing system using artificial intelligence models", "comment": "8 pages, 12 figures, article", "summary": "This is to present a text image classifier device that identifies textual content in images and then categorizes each image into one of four predefined categories, including Invoice, Form, Letter, or Report. The device supports a gallery mode, in which users browse files on flash disks, hard disk drives, or microSD cards, and a live mode which renders feeds of cameras connected to it. Its design is specifically aimed at addressing pragmatic challenges, such as changing light, random orientation, curvature or partial coverage of text, low resolution, and slightly visible text. The steps of the processing process are divided into four steps: image acquisition and preprocessing, textual elements detection with the help of DBNet++ (Differentiable Binarization Network Plus) model, BART (Bidirectional Auto-Regressive Transformers) model that classifies detected textual elements, and the presentation of the results through a user interface written in Python and PyQt5. All the stages are connected in such a way that they form a smooth workflow. The system achieved a text recognition rate of about 94.62% when tested over ten hours on the mentioned Total-Text dataset, that includes high resolution images, created so as to represent a wide range of problematic conditions. These experimental results support the effectiveness of the suggested methodology to practice, mixed-source text categorization, even in uncontrolled imaging conditions.", "AI": {"tldr": "本文介绍了一种文本图像分类设备，能将图像中的文本内容识别并归类为发票、表格、信件或报告四种预定义类别之一。该设备旨在解决实际挑战，并利用DBNet++进行文本检测，BART模型进行分类，在Total-Text数据集上实现了94.62%的文本识别率。", "motivation": "研究动机是为了解决文本图像分类在实际应用中面临的挑战，例如光照变化、随机方向、文本弯曲或部分遮挡、低分辨率以及文本模糊等问题。", "method": "该设备支持图库模式和实时模式。其处理流程分为四个步骤：图像采集与预处理、使用DBNet++模型检测文本元素、使用BART模型对检测到的文本元素进行分类，以及通过Python和PyQt5编写的用户界面展示结果。所有阶段无缝衔接，形成流畅的工作流程。", "result": "该系统在Total-Text数据集上进行了十小时测试，实现了约94.62%的文本识别率。该数据集包含高分辨率图像，旨在模拟各种复杂条件。", "conclusion": "实验结果支持了所提出方法在实际应用中的有效性，即使在不受控制的成像条件下，也能对混合来源的文本进行有效分类。"}}
{"id": "2512.11749", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11749", "abs": "https://arxiv.org/abs/2512.11749", "authors": ["Minglei Shi", "Haolin Wang", "Borui Zhang", "Wenzhao Zheng", "Bohan Zeng", "Ziyang Yuan", "Xiaoshi Wu", "Yuanxing Zhang", "Huan Yang", "Xintao Wang", "Pengfei Wan", "Kun Gai", "Jie Zhou", "Jiwen Lu"], "title": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder", "comment": "Code Repository: https://github.com/KlingTeam/SVG-T2I; Model Weights: https://huggingface.co/KlingTeam/SVG-T2I", "summary": "Visual generation grounded in Visual Foundation Model (VFM) representations offers a highly promising unified pathway for integrating visual understanding, perception, and generation. Despite this potential, training large-scale text-to-image diffusion models entirely within the VFM representation space remains largely unexplored. To bridge this gap, we scale the SVG (Self-supervised representations for Visual Generation) framework, proposing SVG-T2I to support high-quality text-to-image synthesis directly in the VFM feature domain. By leveraging a standard text-to-image diffusion pipeline, SVG-T2I achieves competitive performance, reaching 0.75 on GenEval and 85.78 on DPG-Bench. This performance validates the intrinsic representational power of VFMs for generative tasks. We fully open-source the project, including the autoencoder and generation model, together with their training, inference, evaluation pipelines, and pre-trained weights, to facilitate further research in representation-driven visual generation.", "AI": {"tldr": "本文提出SVG-T2I框架，首次在大规模视觉基础模型（VFM）表示空间中训练文本到图像扩散模型，实现了有竞争力的生成性能，并开源了所有资源。", "motivation": "尽管视觉基础模型（VFM）表示在整合视觉理解、感知和生成方面具有巨大潜力，但在VFM表示空间中训练大规模文本到图像扩散模型的研究仍未被充分探索。", "method": "通过扩展SVG（Self-supervised representations for Visual Generation）框架，提出了SVG-T2I。该方法利用标准的文本到图像扩散流程，但直接在VFM特征域中进行操作，以实现高质量的文本到图像合成。", "result": "SVG-T2I取得了有竞争力的性能，在GenEval上达到0.75分，在DPG-Bench上达到85.78分。这些结果验证了VFM在生成任务中固有的表示能力。", "conclusion": "视觉基础模型（VFM）具有强大的内在表示能力，适用于生成任务。该项目已完全开源，包括自编码器、生成模型、训练/推理/评估流程及预训练权重，以促进表示驱动的视觉生成领域的进一步研究。"}}
{"id": "2512.11719", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11719", "abs": "https://arxiv.org/abs/2512.11719", "authors": ["Yilmaz Korkmaz", "Jay N. Paranjape", "Celso M. de Melo", "Vishal M. Patel"], "title": "Referring Change Detection in Remote Sensing Imagery", "comment": "2026 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)", "summary": "Change detection in remote sensing imagery is essential for applications such as urban planning, environmental monitoring, and disaster management. Traditional change detection methods typically identify all changes between two temporal images without distinguishing the types of transitions, which can lead to results that may not align with specific user needs. Although semantic change detection methods have attempted to address this by categorizing changes into predefined classes, these methods rely on rigid class definitions and fixed model architectures, making it difficult to mix datasets with different label sets or reuse models across tasks, as the output channels are tightly coupled with the number and type of semantic classes. To overcome these limitations, we introduce Referring Change Detection (RCD), which leverages natural language prompts to detect specific classes of changes in remote sensing images. By integrating language understanding with visual analysis, our approach allows users to specify the exact type of change they are interested in. However, training models for RCD is challenging due to the limited availability of annotated data and severe class imbalance in existing datasets. To address this, we propose a two-stage framework consisting of (I) \\textbf{RCDNet}, a cross-modal fusion network designed for referring change detection, and (II) \\textbf{RCDGen}, a diffusion-based synthetic data generation pipeline that produces realistic post-change images and change maps for a specified category using only pre-change image, without relying on semantic segmentation masks and thereby significantly lowering the barrier to scalable data creation. Experiments across multiple datasets show that our framework enables scalable and targeted change detection. Project website is here: https://yilmazkorkmaz1.github.io/RCD.", "AI": {"tldr": "本文提出参照变化检测 (RCD) 方法，利用自然语言提示检测遥感图像中特定类型的变化。为解决数据稀缺问题，构建了一个两阶段框架，包括用于RCD的跨模态融合网络RCDNet和基于扩散的合成数据生成管道RCDGen，实现了可扩展和目标导向的变化检测。", "motivation": "传统变化检测方法无法区分变化类型，语义变化检测方法则依赖僵化的类别定义和模型架构，难以混合数据集或跨任务重用模型。现有方法输出通道与语义类别数量和类型紧密耦合，导致无法满足用户对特定类型变化的检测需求。", "method": "引入参照变化检测 (RCD)，通过整合语言理解和视觉分析，允许用户通过自然语言提示指定感兴趣的变化类型。为解决训练数据稀缺和类别不平衡问题，提出了一个两阶段框架：(I) RCDNet，一个用于参照变化检测的跨模态融合网络；(II) RCDGen，一个基于扩散的合成数据生成管道，仅使用变化前图像即可生成指定类别的逼真变化后图像和变化图，无需语义分割掩码，大大降低了数据创建门槛。", "result": "在多个数据集上进行的实验表明，所提出的框架能够实现可扩展和目标导向的变化检测。", "conclusion": "通过RCDNet和RCDGen构成的两阶段框架，成功克服了传统和语义变化检测方法的局限性，实现了基于自然语言提示的特定类型变化检测，并有效解决了数据稀缺问题，为遥感图像变化检测提供了更灵活、更具针对性的解决方案。"}}
{"id": "2512.11720", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11720", "abs": "https://arxiv.org/abs/2512.11720", "authors": ["Yan Zhang", "Han Zou", "Lincong Feng", "Cong Xie", "Ruiqi Yu", "Zhenpeng Zhan"], "title": "Reframing Music-Driven 2D Dance Pose Generation as Multi-Channel Image Generation", "comment": null, "summary": "Recent pose-to-video models can translate 2D pose sequences into photorealistic, identity-preserving dance videos, so the key challenge is to generate temporally coherent, rhythm-aligned 2D poses from music, especially under complex, high-variance in-the-wild distributions. We address this by reframing music-to-dance generation as a music-token-conditioned multi-channel image synthesis problem: 2D pose sequences are encoded as one-hot images, compressed by a pretrained image VAE, and modeled with a DiT-style backbone, allowing us to inherit architectural and training advances from modern text-to-image models and better capture high-variance 2D pose distributions. On top of this formulation, we introduce (i) a time-shared temporal indexing scheme that explicitly synchronizes music tokens and pose latents over time and (ii) a reference-pose conditioning strategy that preserves subject-specific body proportions and on-screen scale while enabling long-horizon segment-and-stitch generation. Experiments on a large in-the-wild 2D dance corpus and the calibrated AIST++2D benchmark show consistent improvements over representative music-to-dance methods in pose- and video-space metrics and human preference, and ablations validate the contributions of the representation, temporal indexing, and reference conditioning. See supplementary videos at https://hot-dance.github.io", "AI": {"tldr": "该研究将音乐到舞蹈生成重新定义为音乐标记条件下的多通道图像合成问题，通过将2D姿态序列编码为独热图像并使用DiT风格模型进行建模，同时引入时间共享时间索引和参考姿态条件策略，以生成与音乐同步、保持身份的舞蹈视频。", "motivation": "尽管最近的姿态到视频模型能够生成逼真的、保留身份的舞蹈视频，但主要挑战在于如何从音乐中生成时间连贯、节奏对齐的2D姿态序列，尤其是在复杂、高变化的真实世界分布中。", "method": "研究将音乐到舞蹈生成重新构建为音乐标记条件下的多通道图像合成问题。具体方法包括：将2D姿态序列编码为独热图像，使用预训练的图像VAE进行压缩，并用DiT风格的主干网络进行建模。在此基础上，引入了(i)一种时间共享的时间索引方案，明确同步音乐标记和姿态潜在变量，以及(ii)一种参考姿态条件策略，用于保留主体特定的身体比例和屏幕比例，并支持长距离分段拼接生成。", "result": "在大型真实世界2D舞蹈语料库和AIST++2D基准测试上的实验表明，与代表性的音乐到舞蹈方法相比，该方法在姿态空间和视频空间指标以及人类偏好方面均显示出持续的改进。消融实验验证了表示、时间索引和参考条件策略的贡献。", "conclusion": "通过将2D姿态序列视为图像并利用DiT风格模型，结合创新的时间索引和参考姿态条件策略，该方法有效解决了从音乐生成时间连贯、高保真2D舞蹈姿态的挑战，并在多项指标上取得了显著优于现有方法的性能。"}}
{"id": "2512.11722", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11722", "abs": "https://arxiv.org/abs/2512.11722", "authors": ["Lin Bai", "Xiaoyang Li", "Liqiang Huang", "Quynh Nguyen", "Hien Van Nguyen", "Saurabh Prasad", "Dragan Maric", "John Redell", "Pramod Dash", "Badrinath Roysam"], "title": "Weak-to-Strong Generalization Enables Fully Automated De Novo Training of Multi-head Mask-RCNN Model for Segmenting Densely Overlapping Cell Nuclei in Multiplex Whole-slice Brain Images", "comment": null, "summary": "We present a weak to strong generalization methodology for fully automated training of a multi-head extension of the Mask-RCNN method with efficient channel attention for reliable segmentation of overlapping cell nuclei in multiplex cyclic immunofluorescent (IF) whole-slide images (WSI), and present evidence for pseudo-label correction and coverage expansion, the key phenomena underlying weak to strong generalization. This method can learn to segment de novo a new class of images from a new instrument and/or a new imaging protocol without the need for human annotations. We also present metrics for automated self-diagnosis of segmentation quality in production environments, where human visual proofreading of massive WSI images is unaffordable. Our method was benchmarked against five current widely used methods and showed a significant improvement. The code, sample WSI images, and high-resolution segmentation results are provided in open form for community adoption and adaptation.", "AI": {"tldr": "该研究提出了一种基于弱到强泛化策略的自动化多头Mask-RCNN方法，用于可靠分割多重循环免疫荧光全玻片图像中重叠的细胞核，无需人工标注，并优于现有方法。", "motivation": "在多重循环免疫荧光全玻片图像中，可靠分割重叠细胞核是一个挑战，且在生产环境中，人工标注和视觉校对大量图像的成本过高，因此需要一种无需人工标注的自动化分割方法。", "method": "该方法是Mask-RCNN的多头扩展，集成了高效通道注意力机制，并采用弱到强泛化策略进行训练，涉及伪标签校正和覆盖范围扩展。此外，还提出了用于生产环境中分割质量的自动化自诊断指标。", "result": "该方法能够从新的仪器和/或成像协议中学习并从头开始分割新的图像类别，无需人工标注。与五种广泛使用的方法相比，该方法显示出显著的改进。研究还提供了弱到强泛化背后的关键现象——伪标签校正和覆盖范围扩展的证据。", "conclusion": "该研究成功开发了一种高度自动化、无需人工标注的细胞核分割方法，该方法在处理多重循环免疫荧光全玻片图像中的重叠细胞核方面表现出色，并能通过自诊断功能确保生产环境中的质量。代码和样本已开源供社区采用。"}}
{"id": "2512.11763", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11763", "abs": "https://arxiv.org/abs/2512.11763", "authors": ["Mohammad Dehghanmanshadi", "Wallapak Tavanapong"], "title": "Reducing Domain Gap with Diffusion-Based Domain Adaptation for Cell Counting", "comment": "Accepted at ICMLA 2025", "summary": "Generating realistic synthetic microscopy images is critical for training deep learning models in label-scarce environments, such as cell counting with many cells per image. However, traditional domain adaptation methods often struggle to bridge the domain gap when synthetic images lack the complex textures and visual patterns of real samples. In this work, we adapt the Inversion-Based Style Transfer (InST) framework originally designed for artistic style transfer to biomedical microscopy images. Our method combines latent-space Adaptive Instance Normalization with stochastic inversion in a diffusion model to transfer the style from real fluorescence microscopy images to synthetic ones, while weakly preserving content structure.\n  We evaluate the effectiveness of our InST-based synthetic dataset for downstream cell counting by pre-training and fine-tuning EfficientNet-B0 models on various data sources, including real data, hard-coded synthetic data, and the public Cell200-s dataset. Models trained with our InST-synthesized images achieve up to 37\\% lower Mean Absolute Error (MAE) compared to models trained on hard-coded synthetic data, and a 52\\% reduction in MAE compared to models trained on Cell200-s (from 53.70 to 25.95 MAE). Notably, our approach also outperforms models trained on real data alone (25.95 vs. 27.74 MAE). Further improvements are achieved when combining InST-synthesized data with lightweight domain adaptation techniques such as DACS with CutMix. These findings demonstrate that InST-based style transfer most effectively reduces the domain gap between synthetic and real microscopy data. Our approach offers a scalable path for enhancing cell counting performance while minimizing manual labeling effort. The source code and resources are publicly available at: https://github.com/MohammadDehghan/InST-Microscopy.", "AI": {"tldr": "该研究将基于反演的风格迁移（InST）框架应用于生物医学显微图像，通过将真实荧光显微图像的风格迁移到合成图像，显著缩小了域差距，并提高了细胞计数模型的性能，减少了对人工标注的需求。", "motivation": "在细胞计数等标签稀缺的环境中，生成逼真的合成显微图像对训练深度学习模型至关重要。然而，传统域适应方法在合成图像缺乏真实样本复杂纹理和视觉模式时，难以弥合域差距。", "method": "本研究将最初用于艺术风格迁移的基于反演的风格迁移（InST）框架改编用于生物医学显微图像。该方法结合了潜在空间自适应实例归一化（Adaptive Instance Normalization）和扩散模型中的随机反演，将真实荧光显微图像的风格迁移到合成图像，同时弱保留内容结构。", "result": "使用InST合成图像训练的模型，在细胞计数任务中，与硬编码合成数据训练的模型相比，平均绝对误差（MAE）降低了高达37%；与公共Cell200-s数据集训练的模型相比，MAE降低了52%（从53.70降至25.95）。值得注意的是，该方法还优于仅使用真实数据训练的模型（25.95 vs 27.74 MAE）。结合InST合成数据和轻量级域适应技术（如DACS与CutMix）可进一步提升性能。", "conclusion": "这些发现表明，基于InST的风格迁移最有效地减少了合成显微数据与真实数据之间的域差距。该方法为提高细胞计数性能提供了一条可扩展的途径，同时最大限度地减少了手动标注工作。"}}
{"id": "2512.11791", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11791", "abs": "https://arxiv.org/abs/2512.11791", "authors": ["Wentao Jiang", "Vamsi Varra", "Caitlin Perez-Stable", "Harrison Zhu", "Meredith Apicella", "Nicole Nyamongo"], "title": "Uncertainty-Aware Domain Adaptation for Vitiligo Segmentation in Clinical Photographs", "comment": null, "summary": "Accurately quantifying vitiligo extent in routine clinical photographs is crucial for longitudinal monitoring of treatment response. We propose a trustworthy, frequency-aware segmentation framework built on three synergistic pillars: (1) a data-efficient training strategy combining domain-adaptive pre-training on the ISIC 2019 dataset with an ROI-constrained dual-task loss to suppress background noise; (2) an architectural refinement via a ConvNeXt V2-based encoder enhanced with a novel High-Frequency Spectral Gating (HFSG) module and stem-skip connections to capture subtle textures; and (3) a clinical trust mechanism employing K-fold ensemble and Test-Time Augmentation (TTA) to generate pixel-wise uncertainty maps. Extensive validation on an expert-annotated clinical cohort demonstrates superior performance, achieving a Dice score of 85.05% and significantly reducing boundary error (95% Hausdorff Distance improved from 44.79 px to 29.95 px), consistently outperforming strong CNN (ResNet-50 and UNet++) and Transformer (MiT-B5) baselines. Notably, our framework demonstrates high reliability with zero catastrophic failures and provides interpretable entropy maps to identify ambiguous regions for clinician review. Our approach suggests that the proposed framework establishes a robust and reliable standard for automated vitiligo assessment.", "AI": {"tldr": "本文提出了一种可信赖的、频率感知的白癜风分割框架，通过数据高效训练、架构改进和临床信任机制，实现了对临床照片中白癜风范围的精确量化，并提供了可解释的不确定性图。", "motivation": "在常规临床照片中准确量化白癜风范围对于长期监测治疗反应至关重要。", "method": "该框架建立在三个协同支柱之上：\n1.  数据高效训练策略：结合ISIC 2019数据集的域适应预训练和ROI约束的双任务损失，以抑制背景噪声。\n2.  架构改进：基于ConvNeXt V2的编码器，通过新型高频谱门控（HFSG）模块和干跳连接增强，以捕获细微纹理。\n3.  临床信任机制：采用K折集成和测试时增强（TTA）生成像素级不确定性图。", "result": "在专家标注的临床队列上验证，该框架表现出卓越性能，Dice分数达到85.05%，显著降低了边界误差（95% Hausdorff距离从44.79像素改进到29.95像素），持续优于CNN和Transformer基线。该框架展现出高可靠性，零灾难性故障，并提供可解释的熵图以识别模糊区域供临床医生审查。", "conclusion": "所提出的框架为自动化白癜风评估建立了稳健可靠的标准。"}}
{"id": "2512.11782", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11782", "abs": "https://arxiv.org/abs/2512.11782", "authors": ["Peiqing Yang", "Shangchen Zhou", "Kai Hao", "Qingyi Tao"], "title": "MatAnyone 2: Scaling Video Matting via a Learned Quality Evaluator", "comment": "Project page: https://pq-yang.github.io/projects/MatAnyone2/", "summary": "Video matting remains limited by the scale and realism of existing datasets. While leveraging segmentation data can enhance semantic stability, the lack of effective boundary supervision often leads to segmentation-like mattes lacking fine details. To this end, we introduce a learned Matting Quality Evaluator (MQE) that assesses semantic and boundary quality of alpha mattes without ground truth. It produces a pixel-wise evaluation map that identifies reliable and erroneous regions, enabling fine-grained quality assessment. The MQE scales up video matting in two ways: (1) as an online matting-quality feedback during training to suppress erroneous regions, providing comprehensive supervision, and (2) as an offline selection module for data curation, improving annotation quality by combining the strengths of leading video and image matting models. This process allows us to build a large-scale real-world video matting dataset, VMReal, containing 28K clips and 2.4M frames. To handle large appearance variations in long videos, we introduce a reference-frame training strategy that incorporates long-range frames beyond the local window for effective training. Our MatAnyone 2 achieves state-of-the-art performance on both synthetic and real-world benchmarks, surpassing prior methods across all metrics.", "AI": {"tldr": "本文提出了一种学习型抠图质量评估器（MQE），用于在无真值情况下评估Alpha抠图的语义和边界质量。MQE既可作为训练中的在线反馈抑制错误区域，也可作为离线数据选择模块，从而构建了大规模真实世界视频抠图数据集VMReal。结合参考帧训练策略，其模型MatAnyone 2在合成和真实世界基准上均达到了最先进的性能。", "motivation": "现有的视频抠图数据集在规模和真实性方面存在局限性。虽然利用分割数据可以增强语义稳定性，但缺乏有效的边界监督往往导致抠图缺乏精细细节，使其更像分割结果而非高质量抠图。", "method": "1. 引入了学习型抠图质量评估器（MQE），无需真值即可评估Alpha抠图的语义和边界质量，并生成像素级评估图。2. MQE以两种方式扩展视频抠图：作为训练中的在线抠图质量反馈以抑制错误区域；作为数据整理的离线选择模块，通过结合领先的视频和图像抠图模型的优势来提高标注质量。3. 基于此过程构建了大规模真实世界视频抠图数据集VMReal（包含28K片段和2.4M帧）。4. 提出了一种参考帧训练策略，将局部窗口之外的远距离帧纳入训练，以处理长视频中大的外观变化。5. 开发了MatAnyone 2模型。", "result": "1. 成功构建了一个大规模真实世界视频抠图数据集VMReal。2. MatAnyone 2模型在合成和真实世界基准测试中均实现了最先进的性能，在所有指标上都超越了现有方法。", "conclusion": "所提出的MQE及其在训练和数据整理中的应用，结合新的参考帧训练策略，有效地扩展了视频抠图的规模和质量，从而在各种基准测试中实现了最先进的性能，并提供了宝贵的VMReal数据集。"}}
{"id": "2512.11800", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2512.11800", "abs": "https://arxiv.org/abs/2512.11800", "authors": ["Jan U. Müller", "Robin Tim Landsgesell", "Leif Van Holland", "Patrick Stotko", "Reinhard Klein"], "title": "Moment-Based 3D Gaussian Splatting: Resolving Volumetric Occlusion with Order-Independent Transmittance", "comment": null, "summary": "The recent success of 3D Gaussian Splatting (3DGS) has reshaped novel view synthesis by enabling fast optimization and real-time rendering of high-quality radiance fields. However, it relies on simplified, order-dependent alpha blending and coarse approximations of the density integral within the rasterizer, thereby limiting its ability to render complex, overlapping semi-transparent objects. In this paper, we extend rasterization-based rendering of 3D Gaussian representations with a novel method for high-fidelity transmittance computation, entirely avoiding the need for ray tracing or per-pixel sample sorting. Building on prior work in moment-based order-independent transparency, our key idea is to characterize the density distribution along each camera ray with a compact and continuous representation based on statistical moments. To this end, we analytically derive and compute a set of per-pixel moments from all contributing 3D Gaussians. From these moments, a continuous transmittance function is reconstructed for each ray, which is then independently sampled within each Gaussian. As a result, our method bridges the gap between rasterization and physical accuracy by modeling light attenuation in complex translucent media, significantly improving overall reconstruction and rendering quality.", "AI": {"tldr": "本文提出一种基于矩（moment-based）的顺序无关透明度方法，用于3D高斯溅射（3DGS），以实现高保真透射率计算，从而在不依赖光线追踪或像素排序的情况下，显著提升复杂半透明物体的渲染质量。", "motivation": "3DGS在实时渲染高质量辐射场方面取得了成功，但其依赖简化的、顺序相关的alpha混合和粗糙的密度积分近似，限制了其渲染复杂、重叠半透明物体的能力。", "method": "该方法通过扩展基于光栅化的3D高斯表示渲染，引入了一种高保真透射率计算新方法。它避免了光线追踪或逐像素样本排序，并借鉴了基于矩的顺序无关透明度工作。核心思想是利用统计矩来表征沿每个摄像机射线的密度分布，形成紧凑且连续的表示。具体而言，从所有贡献的3D高斯中解析推导并计算一组逐像素矩，然后从中重建每个射线的连续透射率函数，并在每个高斯内部独立采样。", "result": "该方法弥合了光栅化与物理精度之间的鸿沟，能够模拟复杂半透明介质中的光衰减，显著改善了整体重建和渲染质量。", "conclusion": "通过引入基于统计矩的高保真透射率计算方法，本研究成功提升了3DGS在处理复杂半透明物体时的渲染能力和物理准确性，从而显著改善了图像质量。"}}
{"id": "2512.11792", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11792", "abs": "https://arxiv.org/abs/2512.11792", "authors": ["Yang Fei", "George Stoica", "Jingyuan Liu", "Qifeng Chen", "Ranjay Krishna", "Xiaojuan Wang", "Benlin Liu"], "title": "Structure From Tracking: Distilling Structure-Preserving Motion for Video Generation", "comment": "Project Website: https://sam2videox.github.io/", "summary": "Reality is a dance between rigid constraints and deformable structures. For video models, that means generating motion that preserves fidelity as well as structure. Despite progress in diffusion models, producing realistic structure-preserving motion remains challenging, especially for articulated and deformable objects such as humans and animals. Scaling training data alone, so far, has failed to resolve physically implausible transitions. Existing approaches rely on conditioning with noisy motion representations, such as optical flow or skeletons extracted using an external imperfect model. To address these challenges, we introduce an algorithm to distill structure-preserving motion priors from an autoregressive video tracking model (SAM2) into a bidirectional video diffusion model (CogVideoX). With our method, we train SAM2VideoX, which contains two innovations: (1) a bidirectional feature fusion module that extracts global structure-preserving motion priors from a recurrent model like SAM2; (2) a Local Gram Flow loss that aligns how local features move together. Experiments on VBench and in human studies show that SAM2VideoX delivers consistent gains (+2.60\\% on VBench, 21-22\\% lower FVD, and 71.4\\% human preference) over prior baselines. Specifically, on VBench, we achieve 95.51\\%, surpassing REPA (92.91\\%) by 2.60\\%, and reduce FVD to 360.57, a 21.20\\% and 22.46\\% improvement over REPA- and LoRA-finetuning, respectively. The project website can be found at https://sam2videox.github.io/ .", "AI": {"tldr": "本文提出SAM2VideoX，一种通过将自回归视频跟踪模型（SAM2）的结构保持运动先验蒸馏到双向视频扩散模型（CogVideoX）中，并结合双向特征融合模块和局部Gram流损失，显著提升了视频生成中对铰接和可变形物体结构保持运动的真实感。", "motivation": "尽管扩散模型取得了进展，但生成逼真且能保持结构的运动（尤其是对于人体和动物等铰接和可变形物体）仍然具有挑战性。仅靠扩大训练数据未能解决物理上不合理的过渡问题。现有方法依赖于使用外部不完美模型提取的嘈杂运动表示（如光流或骨架）进行条件化。", "method": "引入一种算法，将自回归视频跟踪模型（SAM2）中的结构保持运动先验蒸馏到双向视频扩散模型（CogVideoX）中。该方法训练的SAM2VideoX包含两项创新：(1) 一个双向特征融合模块，用于从SAM2等循环模型中提取全局结构保持运动先验；(2) 一个局部Gram流损失，用于对齐局部特征的共同运动方式。", "result": "在VBench和人类研究中，SAM2VideoX均表现出持续的性能提升。在VBench上，得分达到95.51%（比REPA高出2.60%），FVD降低至360.57（比REPA和LoRA微调分别降低21.20%和22.46%），并获得了71.4%的人类偏好。", "conclusion": "SAM2VideoX通过有效地从视频跟踪模型中蒸馏结构保持运动先验并引入创新的特征融合和局部对齐损失，显著提高了扩散模型生成复杂物体结构保持运动的真实感和物理合理性。"}}
{"id": "2512.11799", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11799", "abs": "https://arxiv.org/abs/2512.11799", "authors": ["Ye Fang", "Tong Wu", "Valentin Deschaintre", "Duygu Ceylan", "Iliyan Georgiev", "Chun-Hao Paul Huang", "Yiwei Hu", "Xuelin Chen", "Tuanfeng Yang Wang"], "title": "V-RGBX: Video Editing with Accurate Controls over Intrinsic Properties", "comment": "Project Page: https://aleafy.github.io/vrgbx", "summary": "Large-scale video generation models have shown remarkable potential in modeling photorealistic appearance and lighting interactions in real-world scenes. However, a closed-loop framework that jointly understands intrinsic scene properties (e.g., albedo, normal, material, and irradiance), leverages them for video synthesis, and supports editable intrinsic representations remains unexplored. We present V-RGBX, the first end-to-end framework for intrinsic-aware video editing. V-RGBX unifies three key capabilities: (1) video inverse rendering into intrinsic channels, (2) photorealistic video synthesis from these intrinsic representations, and (3) keyframe-based video editing conditioned on intrinsic channels. At the core of V-RGBX is an interleaved conditioning mechanism that enables intuitive, physically grounded video editing through user-selected keyframes, supporting flexible manipulation of any intrinsic modality. Extensive qualitative and quantitative results show that V-RGBX produces temporally consistent, photorealistic videos while propagating keyframe edits across sequences in a physically plausible manner. We demonstrate its effectiveness in diverse applications, including object appearance editing and scene-level relighting, surpassing the performance of prior methods.", "AI": {"tldr": "V-RGBX是首个端到端的内在感知视频编辑框架，它将视频逆渲染、基于内在属性的视频合成和关键帧编辑结合起来，实现了时间一致且物理真实的视频操作。", "motivation": "大规模视频生成模型在真实感方面表现出色，但目前缺乏一个闭环框架，能够联合理解场景的内在属性（如反照率、法线、材质、辐照度），利用它们进行视频合成，并支持可编辑的内在表示。", "method": "V-RGBX是一个端到端框架，整合了三项关键能力：1) 将视频逆渲染为内在通道；2) 基于这些内在表示进行真实感视频合成；3) 支持以内在通道为条件的关键帧视频编辑。其核心是一个交错条件机制，通过用户选择的关键帧实现直观、物理基础的视频编辑，支持灵活操作任何内在模态。", "result": "定性和定量结果表明，V-RGBX能生成时间一致、真实感的视频，并以物理合理的方式将关键帧编辑传播到整个序列。它在对象外观编辑和场景级重新照明等多种应用中表现出有效性，超越了现有方法的性能。", "conclusion": "V-RGBX是首个端到端实现内在感知视频编辑的框架，通过统一逆渲染、合成和编辑，实现了对视频内容物理真实的、可控的编辑能力，并在多种应用中展现出优越性。"}}
