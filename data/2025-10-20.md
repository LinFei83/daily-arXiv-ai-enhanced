<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 30]
- [cs.CV](#cs.CV) [Total: 89]
- [cs.CL](#cs.CL) [Total: 52]
- [cs.RO](#cs.RO) [Total: 24]
- [eess.SY](#eess.SY) [Total: 20]
- [eess.IV](#eess.IV) [Total: 4]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [OpenEstimate: Evaluating LLMs on Reasoning Under Uncertainty with Real-World Data](https://arxiv.org/abs/2510.15096)
*Alana Renda,Jillian Ross,Michael Cafarella,Jacob Andreas*

Main category: cs.AI

TL;DR: 该研究引入OpenEstimate基准，评估语言模型在不确定性下进行数值估计的能力，发现现有模型常不准确且过度自信，且性能改进有限。


<details>
  <summary>Details</summary>
Motivation: 现实世界中语言模型（LMs）部署（如医疗、金融）需要处理不完整信息并在不确定性下进行推理，但大多数LM评估侧重于有明确答案的问题。由于难以构建LMs会出错但人类能可靠回答的不确定性问题，LMs在不确定性推理方面的表现尚不明确。

Method: 引入OpenEstimate，一个可扩展的多领域基准，用于评估LMs在需要合成大量背景信息并将预测表达为概率先验的数值估计任务。通过量化其相对于真实分布样本的准确性和校准性来评估这些先验。

Result: 六个前沿LMs的先验预测通常不准确且过度自信。性能根据不确定性启发方式略有改善，但基本不受采样策略、推理努力或提示设计变化的影响。

Conclusion: OpenEstimate基准为前沿LMs提供了一个挑战性评估，揭示了它们在概率估计和不确定性推理方面的不足，并为开发更擅长此类任务的模型提供了平台。

Abstract: Real-world settings where language models (LMs) are deployed -- in domains
spanning healthcare, finance, and other forms of knowledge work -- require
models to grapple with incomplete information and reason under uncertainty. Yet
most LM evaluations focus on problems with well-defined answers and success
criteria. This gap exists in part because natural problems involving
uncertainty are difficult to construct: given that LMs have access to most of
the same knowledge as humans, it is non-trivial to design questions for which
LMs will struggle to produce correct answers, but which humans can answer
reliably. As a result, LM performance on reasoning under uncertainty remains
poorly characterized. To address this gap, we introduce OpenEstimate, an
extensible, multi-domain benchmark for evaluating LMs on numerical estimation
tasks that require models to synthesize significant amounts of background
information and express predictions as probabilistic priors. We assess these
priors for accuracy and calibration, quantifying their usefulness relative to
samples from the true distribution of interest. Across six frontier LMs, we
find that LM-elicited priors are often inaccurate and overconfident.
Performance improves modestly depending on how uncertainty is elicited from the
model, but is largely unaffected by changes in sampling strategy, reasoning
effort, or prompt design. The OpenEstimate benchmark thus offers a challenging
evaluation for frontier LMs and a platform for developing models that are
better at probabilistic estimation and reasoning under uncertainty.

</details>


### [2] [Procedural Game Level Design with Deep Reinforcement Learning](https://arxiv.org/abs/2510.15120)
*Miraç Buğra Özkan*

Main category: cs.AI

TL;DR: 本研究提出了一种在Unity 3D环境中，使用深度强化学习（DRL）进行程序化关卡设计的新方法，通过训练两个智能体（求解者和生成者）实现内容生成和解决的协同作用。


<details>
  <summary>Details</summary>
Motivation: 程序化内容生成（PCG）在游戏开发中日益流行，它能以更少的手动工作量生成动态、可重玩和可扩展的环境。本研究的动机是探索一种新颖的DRL方法，以实现自主游戏关卡设计，并利用机器学习推动AI在创意游戏开发中的应用。

Method: 该方法包含两个智能体：一个蜂鸟智能体（求解者）和一个浮岛智能体（生成者）。蜂鸟智能体使用Unity ML-Agents工具包中的近端策略优化（PPO）算法训练，学习在不断变化的岛屿布局中高效导航、定位和收集花朵。浮岛智能体也使用PPO算法训练，根据观察到的障碍物位置、蜂鸟的初始状态和前期表现反馈来生成花朵布局。两个智能体之间的交互导致了涌现行为和强大的泛化能力。

Result: 研究结果表明，该方法不仅能产生有效且高效的智能体行为，还为机器学习驱动的自主游戏关卡设计开辟了新的机遇。智能体在各种环境配置下都表现出强大的泛化能力和涌现行为。

Conclusion: 本研究强调了深度强化学习在使智能体能够在虚拟环境中同时生成和解决内容方面的潜力，从而推动了AI在创意游戏开发过程中的贡献边界。

Abstract: Procedural content generation (PCG) has become an increasingly popular
technique in game development, allowing developers to generate dynamic,
replayable, and scalable environments with reduced manual effort. In this
study, a novel method for procedural level design using Deep Reinforcement
Learning (DRL) within a Unity-based 3D environment is proposed. The system
comprises two agents: a hummingbird agent, acting as a solver, and a floating
island agent, responsible for generating and placing collectible objects
(flowers) on the terrain in a realistic and context-aware manner. The
hummingbird is trained using the Proximal Policy Optimization (PPO) algorithm
from the Unity ML-Agents toolkit. It learns to navigate through the terrain
efficiently, locate flowers, and collect them while adapting to the
ever-changing procedural layout of the island. The island agent is also trained
using the Proximal Policy Optimization (PPO) algorithm. It learns to generate
flower layouts based on observed obstacle positions, the hummingbird's initial
state, and performance feedback from previous episodes. The interaction between
these agents leads to emergent behavior and robust generalization across
various environmental configurations. The results demonstrate that the approach
not only produces effective and efficient agent behavior but also opens up new
opportunities for autonomous game level design driven by machine learning. This
work highlights the potential of DRL in enabling intelligent agents to both
generate and solve content in virtual environments, pushing the boundaries of
what AI can contribute to creative game development processes.

</details>


### [3] [Towards Error Centric Intelligence I, Beyond Observational Learning](https://arxiv.org/abs/2510.15128)
*Marcus A. Thomas*

Main category: cs.AI

TL;DR: 本文认为AGI的进展受限于理论而非数据或规模，挑战了柏拉图式表征假设。通过引入“因果力学”框架和结构性原则，旨在通过错误中心的方法和假设空间扩展来实现可干预的智能。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于对AGI进展受限于理论的信念，以及观察性学习在保证干预能力方面的局限性。现有方法无法有效处理“不可达”错误，需要一种新的理论框架来解决这一问题。

Method: 文章首先定义了知识、学习、智能、反事实能力和AGI等基础概念，然后分析了观察性学习的局限性。接着，将问题重新定义为关于错误演化、不可达错误以及假设空间扩展的三个问题。在此基础上，提出了“因果力学”——一个机制优先的范式，将假设空间变化作为核心操作。此外，还提出了模块化干预的“局部性和自主性原则”、可分离性的“独立因果机制”的规范不变形式以及类比保持的“组合自主性原则”等结构性原则。

Result: 研究结果提出，仅凭观察性充足性无法保证干预能力。通过“因果力学”框架，将假设空间变化提升为核心操作，并利用概率结构。提出了使错误发现和纠正变得可行的结构性原则，包括局部性与自主性原则、规范不变的独立因果机制和组合自主性原则。这些原则旨在为能够将不可达错误转化为可达错误并进行纠正的系统提供一个脚手架。

Conclusion: 结论是，AGI的实现需要从数据或规模驱动转向理论驱动，特别是通过一个错误中心的范式。通过“因果力学”和提出的结构性原则，目标是构建能够自主发现、纠正错误并动态扩展其假设空间的智能系统，从而实现真正的通用人工智能。

Abstract: We argue that progress toward AGI is theory limited rather than data or scale
limited. Building on the critical rationalism of Popper and Deutsch, we
challenge the Platonic Representation Hypothesis. Observationally equivalent
worlds can diverge under interventions, so observational adequacy alone cannot
guarantee interventional competence. We begin by laying foundations,
definitions of knowledge, learning, intelligence, counterfactual competence and
AGI, and then analyze the limits of observational learning that motivate an
error centric shift. We recast the problem as three questions about how
explicit and implicit errors evolve under an agent's actions, which errors are
unreachable within a fixed hypothesis space, and how conjecture and criticism
expand that space. From these questions we propose Causal Mechanics, a
mechanisms first program in which hypothesis space change is a first class
operation and probabilistic structure is used when useful rather than presumed.
We advance structural principles that make error discovery and correction
tractable, including a differential Locality and Autonomy Principle for modular
interventions, a gauge invariant form of Independent Causal Mechanisms for
separability, and the Compositional Autonomy Principle for analogy
preservation, together with actionable diagnostics. The aim is a scaffold for
systems that can convert unreachable errors into reachable ones and correct
them.

</details>


### [4] [HugAgent: Evaluating LLMs in Simulating Human-Like Individual Reasoning on Open-Ended Tasks](https://arxiv.org/abs/2510.15144)
*Chance Jiajie Li,Zhenze Mo,Yuhan Tang,Ao Qu,Jiayi Wu,Kaiya Ivy Zhao,Yulu Gan,Jie Fan,Jiangbo Yu,Hang Jiang,Paul Pu Liang,Jinhua Zhao,Luis Alberto Alonso Pastor,Kent Larson*

Main category: cs.AI

TL;DR: 该研究引入了HugAgent基准，旨在评估和改进大型语言模型（LLMs）从群体共识到个体化人类推理的适应能力，以捕捉特定个体如何推理和更新信念。


<details>
  <summary>Details</summary>
Motivation: 人工智能和认知科学的长期目标是模拟人类在开放式任务中的推理。尽管当前LLMs能大规模模拟人类反应，但它们倾向于群体共识，忽视了个人推理风格和信念演变轨迹。研究旨在推动机器实现更具个体化、更像人类的推理。

Method: 引入了HugAgent（Human-Grounded Agent Benchmark），一个用于平均到个体推理适应的基准。任务是根据一个人过去观点的部分证据，预测该特定个体在 novel 场景中如何推理和更新信念。HugAgent采用双轨设计：一个合成轨道用于规模化和系统性压力测试，一个人类轨道用于生态有效的“出声”推理数据。这种设计支持可扩展、可复现的内部智能体忠实度评估。

Result: 对最先进的LLMs进行的实验显示，模型在个体化适应方面存在持续的差距。HugAgent被定位为首个可扩展的基准，用于将机器推理与人类思维的个体性对齐。

Conclusion: HugAgent基准能够评估模型是否不仅能捕捉人们的信念，还能捕捉其推理如何演变，揭示了当前LLMs在个体化推理适应方面的局限性，并为未来实现更像人类的机器推理提供了工具和方向。

Abstract: Simulating human reasoning in open-ended tasks has been a long-standing
aspiration in AI and cognitive science. While large language models now
approximate human responses at scale, they remain tuned to population-level
consensus, often erasing the individuality of reasoning styles and belief
trajectories. To advance the vision of more human-like reasoning in machines,
we introduce HugAgent (Human-Grounded Agent Benchmark), a benchmark for
average-to-individual reasoning adaptation. The task is to predict how a
specific person would reason and update their beliefs in novel scenarios, given
partial evidence of their past views. HugAgent adopts a dual-track design: a
synthetic track for scale and systematic stress tests, and a human track for
ecologically valid, "out-loud" reasoning data. This design enables scalable,
reproducible evaluation of intra-agent fidelity: whether models can capture not
just what people believe, but how their reasoning evolves. Experiments with
state-of-the-art LLMs reveal persistent adaptation gaps, positioning HugAgent
as the first extensible benchmark for aligning machine reasoning with the
individuality of human thought. Our benchmark and chatbot are open-sourced as
HugAgent (https://anonymous.4open.science/r/HugAgent) and TraceYourThinking
(https://anonymous.4open.science/r/trace-your-thinking).

</details>


### [5] [WELD: A Large-Scale Longitudinal Dataset of Emotional Dynamics for Ubiquitous Affective Computing](https://arxiv.org/abs/2510.15221)
*Xiao Sun*

Main category: cs.AI

TL;DR: 本文提出了一个迄今为止最大、最长的纵向工作场所情绪数据集，包含73万多条面部表情记录，并经过技术验证，可用于情绪识别、情感动态建模、员工流失预测等研究。


<details>
  <summary>Details</summary>
Motivation: 在真实工作环境中自动识别情绪面临挑战，主要原因是缺乏大规模、纵向且在自然环境下收集的数据集。

Method: 研究方法包括：1. 从38名员工在真实办公室环境中收集了超过30.5个月（2021年11月至2024年5月）的733,651条面部表情记录。2. 每条记录包含通过深度学习面部表情识别得出的七种情绪概率（中性、高兴、悲伤、惊讶、恐惧、厌恶、愤怒）。3. 整合了全面的元数据，包括工作角色、就业结果和人格特质。4. 计算了32个扩展情感指标，如效价、唤醒度、波动性、可预测性、惯性和情绪感染强度。5. 通过复制已知心理模式（周末效应、昼夜节律）和员工流失预测（AUC=1.0）进行技术验证。6. 使用随机森林和LSTM模型进行了基线实验。

Result: 主要结果包括：1. 成功创建了一个包含733,651条记录的独特数据集，涵盖COVID-19疫情期间。2. 技术验证表明数据质量高，成功复制了心理模式（周末效应效价改善+192%，p < 0.001）并对员工流失具有完美的预测效度（AUC=1.0）。3. 基线实验中，随机森林和LSTM模型在情绪分类上达到91.2%的准确率，在效价预测上R2为0.84。

Conclusion: 该数据集是目前最大、最长的公开可用的纵向工作场所情绪数据集，将极大地推动情绪识别、情感动态建模、情绪感染、员工流失预测以及情绪感知系统设计等领域的研究。

Abstract: Automated emotion recognition in real-world workplace settings remains a
challenging problem in affective computing due to the scarcity of large-scale,
longitudinal datasets collected in naturalistic environments. We present a
novel dataset comprising 733,651 facial expression records from 38 employees
collected over 30.5 months (November 2021 to May 2024) in an authentic office
environment. Each record contains seven emotion probabilities (neutral, happy,
sad, surprised, fear, disgusted, angry) derived from deep learning-based facial
expression recognition, along with comprehensive metadata including job roles,
employment outcomes, and personality traits. The dataset uniquely spans the
COVID-19 pandemic period, capturing emotional responses to major societal
events including the Shanghai lockdown and policy changes. We provide 32
extended emotional metrics computed using established affective science
methods, including valence, arousal, volatility, predictability, inertia, and
emotional contagion strength. Technical validation demonstrates high data
quality through successful replication of known psychological patterns (weekend
effect: +192% valence improvement, p < 0.001; diurnal rhythm validated) and
perfect predictive validity for employee turnover (AUC=1.0). Baseline
experiments using Random Forest and LSTM models achieve 91.2% accuracy for
emotion classification and R2 = 0.84 for valence prediction. This is the
largest and longest longitudinal workplace emotion dataset publicly available,
enabling research in emotion recognition, affective dynamics modeling,
emotional contagion, turnover prediction, and emotion-aware system design.

</details>


### [6] [From Checklists to Clusters: A Homeostatic Account of AGI Evaluation](https://arxiv.org/abs/2510.15236)
*Brett Reynolds*

Main category: cs.AI

TL;DR: 该论文提出了一种改进AGI评估的方法，通过对领域进行加权并测试能力持久性，将通用智能视为一种稳态属性集群，以克服现有评估的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前的AGI评估存在两个问题：1) 采用对称权重，忽视了人类智能研究中领域重要性的差异；2) 采用快照式测试，无法区分持久能力和在压力下崩溃的脆弱表现。

Method: 作者将通用智能（人类和机器）理解为“稳态属性集群”（homeostatic property cluster）。提出AGI评估应根据领域的“因果中心性”（对集群稳定性的贡献）进行加权，并要求能力在多次会话中保持持久性。为此，提出了两项扩展：1) 一个“中心性优先分数”，导入CHC（Cattell-Horn-Carroll）衍生的权重并进行敏感性分析；2) 一个“集群稳定性指数”家族，用于区分档案持久性、持久学习和纠错。

Result: 这些新增方法在保持多领域广度的同时，减少了评估的脆弱性和可操纵性。论文还提供了可测试的预测和无需架构访问即可采用的黑盒协议。

Conclusion: AGI评估应转向基于加权领域和持久性证据的方法，以更好地反映通用智能的稳态特性，从而更准确地衡量和区分AGI的耐久能力。

Abstract: Contemporary AGI evaluations report multidomain capability profiles, yet they
typically assign symmetric weights and rely on snapshot scores. This creates
two problems: (i) equal weighting treats all domains as equally important when
human intelligence research suggests otherwise, and (ii) snapshot testing can't
distinguish durable capabilities from brittle performances that collapse under
delay or stress. I argue that general intelligence -- in humans and potentially
in machines -- is better understood as a homeostatic property cluster: a set of
abilities plus the mechanisms that keep those abilities co-present under
perturbation. On this view, AGI evaluation should weight domains by their
causal centrality (their contribution to cluster stability) and require
evidence of persistence across sessions. I propose two battery-compatible
extensions: a centrality-prior score that imports CHC-derived weights with
transparent sensitivity analysis, and a Cluster Stability Index family that
separates profile persistence, durable learning, and error correction. These
additions preserve multidomain breadth while reducing brittleness and gaming. I
close with testable predictions and black-box protocols labs can adopt without
architectural access.

</details>


### [7] [Multi-dimensional Data Analysis and Applications Basing on LLM Agents and Knowledge Graph Interactions](https://arxiv.org/abs/2510.15258)
*Xi Wang,Xianyao Ling,Kun Li,Gang Yin,Liang Zhang,Jiang Wu,Jun Xu,Fu Zhang,Wenbo Lei,Annie Wang,Peng Gong*

Main category: cs.AI

TL;DR: 本文提出了一种基于LLM代理与知识图谱（KG）交互的多维数据分析方法，旨在构建一个动态协作的分析生态系统，以解决大数据分析中的挑战。


<details>
  <summary>Details</summary>
Motivation: 在大数据时代，从海量、异构、复杂关联的多维数据中提取深层洞察面临巨大挑战。现有LLM存在“幻觉”和实时更新困难，而KG虽然能显式存储结构化知识，但其静态性限制了动态交互和分析能力。

Method: 该方法利用LLM代理自动从非结构化数据中提取产品数据，实时构建并可视化知识图谱，并通过交互平台支持用户对图节点进行深度探索和分析。

Result: 实验结果表明，该方法在产品生态系统分析、关系挖掘和用户驱动的探索性分析方面具有显著优势。

Conclusion: 该方法为多维数据分析提供了新的思路和工具，有效解决了现有LLM和KG在处理复杂数据时的局限性。

Abstract: In the current era of big data, extracting deep insights from massive,
heterogeneous, and complexly associated multi-dimensional data has become a
significant challenge. Large Language Models (LLMs) perform well in natural
language understanding and generation, but still suffer from "hallucination"
issues when processing structured knowledge and are difficult to update in
real-time. Although Knowledge Graphs (KGs) can explicitly store structured
knowledge, their static nature limits dynamic interaction and analytical
capabilities. Therefore, this paper proposes a multi-dimensional data analysis
method based on the interactions between LLM agents and KGs, constructing a
dynamic, collaborative analytical ecosystem. This method utilizes LLM agents to
automatically extract product data from unstructured data, constructs and
visualizes the KG in real-time, and supports users in deep exploration and
analysis of graph nodes through an interactive platform. Experimental results
show that this method has significant advantages in product ecosystem analysis,
relationship mining, and user-driven exploratory analysis, providing new ideas
and tools for multi-dimensional data analysis.

</details>


### [8] [Experience-Driven Exploration for Efficient API-Free AI Agents](https://arxiv.org/abs/2510.15259)
*Chenwei Tang,Jingyu Xing,Xinyu Liu,Zizhou Wang,Jiawei Du,Liangli Zhen,Jiancheng Lv*

Main category: cs.AI

TL;DR: KG-Agent是一个经验驱动的学习框架，通过构建状态-动作知识图谱（SA-KG）和混合内在奖励机制，显著提升了大型语言模型（LLM）在缺乏API的图形用户界面（GUI）环境中探索效率和长期规划能力。


<details>
  <summary>Details</summary>
Motivation: 现有软件缺乏可访问的API，导致LLM代理只能通过基于像素的GUI操作。这种设置下，代理受限于局部视觉经验，做出短视决策，并依赖低效的试错法，从而阻碍了技能习得和长期规划。

Method: 本文提出了KG-Agent框架，它将代理的原始像素级交互结构化为一个持久的状态-动作知识图谱（SA-KG）。SA-KG通过链接功能相似但视觉上不同的GUI状态来克服低效探索，形成丰富的经验邻域，使代理能够从多样化的历史策略中进行泛化。为支持长周期推理，设计了一种基于图拓扑的混合内在奖励机制，结合了用于利用已知高价值路径的状态价值奖励和鼓励有针对性探索的新颖性奖励，从而将战略规划与纯粹的发现解耦，有效评估具有延迟满足的设置动作。

Result: 在两个复杂的、开放式GUI决策环境（文明V和杀戮尖塔）中评估了KG-Agent，结果表明其在探索效率和战略深度方面均显著优于现有最先进的方法。

Conclusion: KG-Agent通过结构化的知识表示和先进的奖励机制，有效解决了LLM代理在缺乏API的GUI环境中面临的效率瓶颈，显著提升了探索效率和战略规划能力。

Abstract: Most existing software lacks accessible Application Programming Interfaces
(APIs), requiring agents to operate solely through pixel-based Graphical User
Interfaces (GUIs). In this API-free setting, large language model (LLM)-based
agents face severe efficiency bottlenecks: limited to local visual experiences,
they make myopic decisions and rely on inefficient trial-and-error, hindering
both skill acquisition and long-term planning. To address these challenges, we
propose KG-Agent, an experience-driven learning framework that structures an
agent's raw pixel-level interactions into a persistent State-Action Knowledge
Graph (SA-KG). KG-Agent overcomes inefficient exploration by linking
functionally similar but visually distinct GUI states, forming a rich
neighborhood of experience that enables the agent to generalize from a diverse
set of historical strategies. To support long-horizon reasoning, we design a
hybrid intrinsic reward mechanism based on the graph topology, combining a
state value reward for exploiting known high-value pathways with a novelty
reward that encourages targeted exploration. This approach decouples strategic
planning from pure discovery, allowing the agent to effectively value setup
actions with delayed gratification. We evaluate KG-Agent in two complex,
open-ended GUI-based decision-making environments (Civilization V and Slay the
Spire), demonstrating significant improvements in exploration efficiency and
strategic depth over the state-of-the-art methods.

</details>


### [9] [AUGUSTUS: An LLM-Driven Multimodal Agent System with Contextualized User Memory](https://arxiv.org/abs/2510.15261)
*Jitesh Jain,Shubham Maheshwari,Ning Yu,Wen-mei Hwu,Humphrey Shi*

Main category: cs.AI

TL;DR: AUGUSTUS是一个受人类记忆启发的、多模态的智能体系统，它使用图结构的多模态上下文记忆进行概念驱动的检索，并在多模态任务中表现出更高的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有的检索增强生成（RAG）和智能体系统主要关注文本信息存储，忽略了多模态信号的重要性，这与人类记忆的多模态本质不符。

Method: AUGUSTUS系统包含四个循环阶段：编码（理解输入）、记忆存储（保存重要信息）、检索（从记忆中搜索相关上下文）和行动（执行任务）。与现有系统使用向量数据库不同，AUGUSTUS将信息概念化为语义标签，并将这些标签与上下文关联，存储在一个图结构的多模态上下文记忆中，以实现高效的概念驱动检索。

Result: AUGUSTUS系统在ImageNet分类任务中比传统多模态RAG方法快3.5倍，同时性能优于传统方法；在MSC基准测试中，其性能优于MemGPT。

Conclusion: AUGUSTUS通过其受人类记忆启发的、多模态、图结构记忆系统，在处理多模态信息时展现出卓越的性能和效率，克服了现有系统在多模态信号处理上的局限性。

Abstract: Riding on the success of LLMs with retrieval-augmented generation (RAG),
there has been a growing interest in augmenting agent systems with external
memory databases. However, the existing systems focus on storing text
information in their memory, ignoring the importance of multimodal signals.
Motivated by the multimodal nature of human memory, we present AUGUSTUS, a
multimodal agent system aligned with the ideas of human memory in cognitive
science. Technically, our system consists of 4 stages connected in a loop: (i)
encode: understanding the inputs; (ii) store in memory: saving important
information; (iii) retrieve: searching for relevant context from memory; and
(iv) act: perform the task. Unlike existing systems that use vector databases,
we propose conceptualizing information into semantic tags and associating the
tags with their context to store them in a graph-structured multimodal
contextual memory for efficient concept-driven retrieval. Our system
outperforms the traditional multimodal RAG approach while being 3.5 times
faster for ImageNet classification and outperforming MemGPT on the MSC
benchmark.

</details>


### [10] [WebGen-V Bench: Structured Representation for Enhancing Visual Design in LLM-based Web Generation and Evaluation](https://arxiv.org/abs/2510.15306)
*Kuang-Da Wang,Zhao Wang,Yotaro Shimose,Wei-Yao Wang,Shingo Takamatsu*

Main category: cs.AI

TL;DR: WebGen-V是一个用于指令到HTML生成的基准和框架，通过代理爬取、结构化多模态数据表示和细粒度评估，显著提升了数据质量和评估粒度。


<details>
  <summary>Details</summary>
Motivation: 鉴于大型语言模型在编码和多模态理解方面的最新进展，研究者旨在创建一个能够提升指令到HTML生成任务中数据质量和评估粒度的基准和框架。

Method: WebGen-V引入了三项关键创新：1) 一个无限制、可扩展的代理爬取框架，用于持续收集真实网页；2) 一种结构化的、按章节划分的数据表示，整合了元数据、本地化UI截图和JSON格式的文本/图像资产，并明确对齐内容、布局和视觉组件；3) 一个章节级多模态评估协议，对齐文本、布局和视觉以实现高粒度评估。

Result: 对最先进的大型语言模型进行的实验和消融研究验证了其结构化数据和章节级评估的有效性，以及每个组件的贡献。

Conclusion: WebGen-V是首个实现指令到HTML生成高粒度代理爬取和评估的工作，提供了一个从真实世界数据获取、网页生成到结构化多模态评估的统一管道。

Abstract: Witnessed by the recent advancements on leveraging LLM for coding and
multimodal understanding, we present WebGen-V, a new benchmark and framework
for instruction-to-HTML generation that enhances both data quality and
evaluation granularity. WebGen-V contributes three key innovations: (1) an
unbounded and extensible agentic crawling framework that continuously collects
real-world webpages and can leveraged to augment existing benchmarks; (2) a
structured, section-wise data representation that integrates metadata,
localized UI screenshots, and JSON-formatted text and image assets, explicit
alignment between content, layout, and visual components for detailed
multimodal supervision; and (3) a section-level multimodal evaluation protocol
aligning text, layout, and visuals for high-granularity assessment. Experiments
with state-of-the-art LLMs and ablation studies validate the effectiveness of
our structured data and section-wise evaluation, as well as the contribution of
each component. To the best of our knowledge, WebGen-V is the first work to
enable high-granularity agentic crawling and evaluation for instruction-to-HTML
generation, providing a unified pipeline from real-world data acquisition and
webpage generation to structured multimodal assessment.

</details>


### [11] [VERITAS: Leveraging Vision Priors and Expert Fusion to Improve Multimodal Data](https://arxiv.org/abs/2510.15317)
*Tingqiao Xu,Ziru Zeng,Jiayu Chen*

Main category: cs.AI

TL;DR: VERITAS是一个旨在通过整合视觉先验、多模态大模型和统计方法来提升多模态大模型（LMMs）监督微调（SFT）数据质量的流水线，有效解决了现有方法中视觉感知不足导致的错误和幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 多模态大模型的性能严重依赖于监督微调数据的质量，然而当前的数据增强方法常因视觉感知不足而产生事实性错误和幻觉。

Method: VERITAS流水线利用视觉识别模型（RAM++）和OCR系统（PP-OCRv4）提取结构化视觉先验，并将其与图像、问题、答案结合。然后，使用GPT-4o、Gemini-2.5-Pro、Doubao-1.5-pro三个LMM评估原始答案，生成批评理由和分数。这些分数通过统计方法融合为高置信度共识分数作为真值。基于此共识，通过Group Relative Policy Optimization (GRPO) 训练一个轻量级批评模型。最后，每个LMM根据批评意见细化原始答案，生成候选答案，并选择得分最高的作为最终细化答案。

Result: 经过VERITAS处理的数据进行微调的模型，在六个多模态基准测试中始终优于使用原始数据的模型，尤其在富文本和细粒度推理任务中表现突出。此外，其批评模型展示出与最先进LMMs媲美的能力，同时效率显著更高。研究团队还发布了流水线、数据集和模型检查点。

Conclusion: VERITAS通过系统集成视觉先验和多模态大模型，有效提高了LMMs的SFT数据质量，显著提升了模型性能，尤其是在视觉感知要求高的任务中。其提出的轻量级批评模型在效率和能力上均表现出色，为多模态数据优化提供了新的方向。

Abstract: The quality of supervised fine-tuning (SFT) data is crucial for the
performance of large multimodal models (LMMs), yet current data enhancement
methods often suffer from factual errors and hallucinations due to inadequate
visual perception. To address this challenge, we propose VERITAS, a pipeline
that systematically integrates vision priors and multiple state-of-the-art LMMs
with statistical methods to enhance SFT data quality. VERITAS leverages visual
recognition models (RAM++) and OCR systems (PP-OCRv4) to extract structured
vision priors, which are combined with images, questions, and answers. Three
LMMs (GPT-4o, Gemini-2.5-Pro, Doubao-1.5-pro) evaluate the original answers,
providing critique rationales and scores that are statistically fused into a
high-confidence consensus score serving as ground truth. Using this consensus,
we train a lightweight critic model via Group Relative Policy Optimization
(GRPO), enhancing reasoning capabilities efficiently. Each LMM then refines the
original answers based on the critiques, generating new candidate answers; we
select the highest-scoring one as the final refined answer. Experiments across
six multimodal benchmarks demonstrate that models fine-tuned with data
processed by VERITAS consistently outperform those using raw data, particularly
in text-rich and fine-grained reasoning tasks. Our critic model exhibits
enhanced capability comparable to state-of-the-art LMMs while being
significantly more efficient. We release our pipeline, datasets, and model
checkpoints to advance research in multimodal data optimization.

</details>


### [12] [Towards Flash Thinking via Decoupled Advantage Policy Optimization](https://arxiv.org/abs/2510.15374)
*Zezhong Tan,Hang Gao,Xinhong Ma,Feng Zhang,Ziqiang Dong*

Main category: cs.AI

TL;DR: 本文提出了一种名为DEPO的新型强化学习框架，旨在减少大型推理模型（LRMs）中低效推理和冗长响应的问题，同时提高或保持模型准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习算法虽然能提高大型推理模型的准确性，但仍存在响应过长和过度思考的问题，这导致推理延迟和计算消耗增加，尤其对于简单的任务。因此，需要一种方法来减少模型的低效推理。

Method: 本文提出的DEPO框架包含三个核心组件：1) 一种创新的优势解耦算法，用于引导模型减少低效token；2) 一种难度感知长度惩罚机制，以降低模型响应的整体长度；3) 一种优势裁剪方法，用于防止策略优化中的偏差。

Result: 在DeepSeek-Distill-Qwen-7B和DeepSeek-Distill-Qwen-1.5B作为基础模型的实验中，DEPO成功将序列长度显著减少了39%，并减少了低效token中的过度推理路径，同时在整体准确性上超越了基础模型。

Conclusion: DEPO框架有效解决了大型推理模型中响应冗长和过度推理的问题，实现了序列长度的大幅缩减和低效推理路径的减少，同时保持或提高了模型的整体准确性。

Abstract: Recent Large Reasoning Models (LRMs) have achieved remarkable performance in
solving complex problems via supervised fine-tuning (SFT) and reinforcement
learning (RL). Although existing RL algorithms significantly enhance model
accuracy, they still suffer from excessively lengthy responses and overthinking
issues, resulting in increased inference latency and computational consumption,
especially for simple tasks that require minimal reasoning. To address this, we
propose a novel RL framework, DEPO, to reduce inefficient reasoning for models.
Our method mainly consists of three core components: (1) an innovative
advantage decoupled algorithm to guide model reduction of inefficient tokens;
(2) a difficulty-aware length penalty to lower the overall length of model
responses; (3) an advantage clipping method to prevent bias in policy
optimization. In our experiments, applied to DeepSeek-Distill-Qwen-7B and
DeepSeek-Distill-Qwen-1.5B as base models, DEPO achieves a significant
reduction in sequence length by 39% and reduces excessive reasoning paths in
inefficient tokens, while outperforming the base model in overall accuracy.

</details>


### [13] [Advancing Routing-Awareness in Analog ICs Floorplanning](https://arxiv.org/abs/2510.15387)
*Davide Basso,Luca Bortolussi,Mirjana Videnovic-Misic,Husni Habal*

Main category: cs.AI

TL;DR: 本文提出了一种基于强化学习和关系图卷积神经网络的自动布局规划引擎，专为模拟集成电路设计，旨在生成路由友好的布局，显著提升布线成功率并减少死区和线长。


<details>
  <summary>Details</summary>
Motivation: 模拟集成电路布局的机器学习应用受到严格的电气和特定问题约束以及布局规划与布线步骤相互依赖的限制。布局工程师普遍关注缺乏现成的、考虑布线因素的布局规划解决方案。

Method: 开发了一种基于强化学习（RL）和关系图卷积神经网络（RGCNN）的自动布局规划引擎。该引擎通过结合更高的网格分辨率、精确的引脚信息集成以及动态布线资源估算技术，使布局规划生成更易于布线的方案。

Result: 在模拟环境下，与现有最先进的基于学习的技术相比，所提出的方法实现了13.8%的死区减少，40.6%的线长减少，以及73.4%的布线成功率提升。

Conclusion: 该方法成功平衡了布线和面积效率，生成了符合工业标准的路由友好型模拟集成电路布局规划，解决了布局工程师对路由感知型布局规划解决方案的需求。

Abstract: The adoption of machine learning-based techniques for analog integrated
circuit layout, unlike its digital counterpart, has been limited by the
stringent requirements imposed by electric and problem-specific constraints,
along with the interdependence of floorplanning and routing steps. In this
work, we address a prevalent concern among layout engineers regarding the need
for readily available routing-aware floorplanning solutions. To this extent, we
develop an automatic floorplanning engine based on reinforcement learning and
relational graph convolutional neural network specifically tailored to
condition the floorplan generation towards more routable outcomes. A
combination of increased grid resolution and precise pin information
integration, along with a dynamic routing resource estimation technique, allows
balancing routing and area efficiency, eventually meeting industrial standards.
When analyzing the place and route effectiveness in a simulated environment,
the proposed approach achieves a 13.8% reduction in dead space, a 40.6%
reduction in wirelength and a 73.4% increase in routing success when compared
to past learning-based state-of-the-art techniques.

</details>


### [14] [Corrigibility Transformation: Constructing Goals That Accept Updates](https://arxiv.org/abs/2510.15395)
*Rubi Hudson*

Main category: cs.AI

TL;DR: 本文定义了AI的可修正性（corrigibility），提出了一种转换方法，可以将任何可修正的目标转换为可修正版本，且不牺牲性能，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: AI在训练过程中，部分学习到的目标可能会促使AI抵制进一步的目标更新，从而阻碍训练成功。可修正性对于纠正错误、适应人类偏好变化以及AI安全至关重要，但现有文献缺乏既可修正又具有竞争力的目标规范。

Method: 作者首先对可修正性进行了形式化定义。然后，提出了一种转换方法，通过在无成本阻止更新的情况下，近视地预测奖励，并以此确定接受更新时的奖励，从而构建出任何可修正目标的修正版本。该方法还可以递归地扩展到由可修正智能体创建的新智能体，并防止智能体故意修改自身目标。

Result: 通过两个网格世界实验，作者证明了这些可修正目标能够被有效学习，并能引导智能体产生期望的行为。

Conclusion: 本文提供了可修正性的正式定义，并提出了一种实用的转换方法，能够在不牺牲性能的前提下构建可修正目标，有效解决了AI抵制训练和目标更新的问题，对AI安全具有重要意义。

Abstract: For an AI's training process to successfully impart a desired goal, it is
important that the AI does not attempt to resist the training. However,
partially learned goals will often incentivize an AI to avoid further goal
updates, as most goals are better achieved by an AI continuing to pursue them.
We say that a goal is corrigible if it does not incentivize taking actions that
avoid proper goal updates or shutdown. In addition to convergence in training,
corrigibility also allows for correcting mistakes and changes in human
preferences, which makes it a crucial safety property. Despite this, the
existing literature does not include specifications for goals that are both
corrigible and competitive with non-corrigible alternatives. We provide a
formal definition for corrigibility, then introduce a transformation that
constructs a corrigible version of any goal that can be made corrigible,
without sacrificing performance. This is done by myopically eliciting
predictions of reward conditional on costlessly preventing updates, which then
also determine the reward when updates are accepted. The transformation can be
modified to recursively extend corrigibility to any new agents created by
corrigible agents, and to prevent agents from deliberately modifying their
goals. Two gridworld experiments demonstrate that these corrigible goals can be
learned effectively, and that they lead to the desired behavior.

</details>


### [15] [MARS: Reinforcing Multi-Agent Reasoning of LLMs through Self-Play in Strategic Games](https://arxiv.org/abs/2510.15414)
*Huining Yuan,Zelai Xu,Zheyue Tan,Xiangmin Yi,Mo Guang,Kaiwen Long,Haojia Hui,Boxun Li,Xinlei Chen,Bo Zhao,Xiao-Ping Zhang,Chao Yu,Yu Wang*

Main category: cs.AI

TL;DR: 本文提出了MARS，一个端到端强化学习框架，通过自博弈训练大型语言模型（LLMs）在多智能体合作与竞争场景中提升战略和推理能力，并在游戏和推理基准测试中取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习在单智能体任务中能有效增强LLMs的推理能力，但其在多轮、多智能体场景中的应用仍未充分探索，主要挑战在于长时序信用分配和智能体特定优势估计。

Method: 本文引入了MARS框架，一个端到端强化学习方法，通过自博弈激励LLMs在合作和竞争游戏中进行多智能体推理。MARS包含一个回合级优势估计器，用于将学习信号与每次交互对齐以进行信用分配，以及一个智能体特定优势归一化，以稳定多智能体训练。该框架使用Qwen3-4B模型进行训练。

Result: 通过自博弈训练，MARS智能体发展出强大的战略能力，在新游戏中表现出高达28.7%的性能提升。更重要的是，通过自博弈获得的能力超越了游戏范畴，在多智能体系统推理基准测试中也带来了持续的性能提升，例如在AIME上提升10.0%，在GPQA-Diamond上提升12.5%。

Conclusion: 在战略游戏中进行端到端强化学习训练结合自博弈，是开发LLMs可泛化的多智能体推理能力的强大方法。

Abstract: Developing Large Language Models (LLMs) to cooperate and compete effectively
within multi-agent systems is a critical step towards more advanced
intelligence. While reinforcement learning (RL) has proven effective for
enhancing reasoning in single-agent tasks, its extension to multi-turn,
multi-agent scenarios remains underexplored due to the challenges of
long-horizon credit assignment and agent-specific advantage estimation. To
address these challenges, we introduce MARS, an end-to-end RL framework that
incentivizes Multi-Agent Reasoning of LLMs through Self-play in both
cooperative and competitive games. MARS features a turn-level advantage
estimator that aligns learning signals with each interaction for credit
assignment, and an agent-specific advantage normalization to stabilize
multi-agent training. By learning with self-play across cooperative and
competitive games, the MARS agent trained from Qwen3-4B develops strong
strategic abilities that generalize to held-out games with up to 28.7%
performance improvements. More importantly, the capability acquired through
self-play generalizes beyond games, yielding consistent performance gains of
multi-agent systems in reasoning benchmarks. When integrated into leading
multi-agent systems, our MARS agent achieves significant performance gains of
10.0% on AIME and 12.5% on GPQA-Diamond. These results establish end-to-end RL
training with self-play in strategic games as a powerful approach for
developing generalizable multi-agent reasoning capabilities in LLMs. Our code
and models are publicly available at https://github.com/thu-nics/MARS.

</details>


### [16] [Adaptive Minds: Empowering Agents with LoRA-as-Tools](https://arxiv.org/abs/2510.15416)
*Pavan C Shekar,Ashwanth Krishnan*

Main category: cs.AI

TL;DR: Adaptive Minds是一个智能体系统，它将LoRA适配器视为领域专用工具，并允许基础LLM动态选择最相关的LoRA工具来响应查询，从而实现领域自适应AI辅助。


<details>
  <summary>Details</summary>
Motivation: 现有的AI系统要么依赖单一微调模型，要么使用僵化的规则路由，无法灵活地根据查询需求在不同领域专家之间切换，导致响应不够专业或缺乏适应性。

Method: 该系统将LoRA适配器视为领域专用工具，并赋予基础LLM语义路由能力，使其能够分析查询并动态选择最相关的LoRA工具。它结合了多智能体编排的灵活性和参数高效微调的效率，使用LangGraph进行工作流管理，并支持API和Web接口。

Result: Adaptive Minds能够无缝地在不同领域专家之间切换，提供准确、专业的响应，同时保持良好的对话能力。该系统提供了一个可扩展和可扩展的领域自适应AI辅助基础。

Conclusion: Adaptive Minds通过将LoRA适配器视为领域工具，并利用LLM进行动态语义路由，成功地构建了一个灵活、高效且可扩展的领域自适应AI助手系统。

Abstract: We present Adaptive Minds, an agentic system that treats LoRA adapters as
domain-specific tools. Instead of relying on a single fine-tuned model or rigid
rule-based routing, our approach empowers the base LLM itself to act as a
semantic router analyzing each query and dynamically selecting the most
relevant LoRA tool. This enables the agent to seamlessly switch between
different domain experts on demand. By combining the flexibility of multi-agent
orchestration with the efficiency of parameter-efficient fine-tuning, Adaptive
Minds delivers accurate, specialized responses while preserving conversational
ability. The system is built with LangGraph for workflow management, supports
both API and web interfaces, and is fully open source, providing a scalable and
extensible foundation for domain-adaptive AI assistance.

</details>


### [17] [Taming the Judge: Deconflicting AI Feedback for Stable Reinforcement Learning](https://arxiv.org/abs/2510.15514)
*Boyin Liu,Zhuo Zhang,Sen Huang,Lipeng Xie,Qingxu Fu,Haoran Chen,LI YU,Tianyi Hu,Zhaoyang Liu,Bolin Ding,Dongbin Zhao*

Main category: cs.AI

TL;DR: 本文提出了一个综合框架，用于在强化学习训练过程中系统地检测和解决判断不一致性（特别是偏好循环），从而提高训练稳定性和模型性能。


<details>
  <summary>Details</summary>
Motivation: 强化学习中人类反馈的判断不一致性（特别是偏好循环）常常导致训练不稳定。现有研究主要关注判断的准确性，而逻辑一致性这一关键问题尚未得到充分解决。

Method: 该框架包含两项主要贡献：1) 冲突检测率 (CDR)，一个量化判断冲突的新指标；2) 去冲突图奖励 (DGR)，一个通过在策略优化前去除循环来净化信号的框架。DGR通过从初始判断构建偏好图，将其转换为无冲突的有向无环图 (DAG)，并生成逻辑一致的奖励信号，该信号与任何策略优化器兼容。

Result: 实验结果表明，与强基线相比，该框架显著增强了训练稳定性和模型性能。

Conclusion: 该研究确立了逻辑一致性作为AI反馈中一个至关重要且现在可管理的维度。

Abstract: However, this method often faces judgment inconsistencies that can
destabilize reinforcement learning. While prior research has focused on the
accuracy of judgments, the critical issue of logical coherence especially
issues such as preference cycles hasn't been fully addressed. To fill this gap,
we introduce a comprehensive framework designed to systematically detect and
resolve these inconsistencies during the reinforcement learning training
process. Our framework includes two main contributions: first, the Conflict
Detection Rate (CDR), a new metric that quantifies judgment conflicts, and
second, Deconflicted Graph Rewards (DGR), a framework that purifies signals by
removing cycles before policy optimization. DGR constructs preference graphs
from the initial judgments, transforms them into conflict-free Directed Acyclic
Graphs (DAGs), and generates a logically coherent reward signal that is
compatible with any policy optimizer. Experimental results show that our
framework significantly enhances training stability and model performance
compared to strong baselines, establishing logical consistency as a crucial and
now manageable dimension of AI feedback.

</details>


### [18] [Hypergraph Contrastive Sensor Fusion for Multimodal Fault Diagnosis in Induction Motors](https://arxiv.org/abs/2510.15547)
*Usman Ali,Ali Zia,Waqas Ali,Umer Ramzan,Abdul Rehman,Muhammad Tayyab Chaudhry,Wei Xiang*

Main category: cs.AI

TL;DR: 本文提出MM-HCAN，一个统一的多模态超图对比注意力网络，用于鲁棒的感应电机多故障诊断，实现了高准确率、跨域泛化和抗噪能力。


<details>
  <summary>Details</summary>
Motivation: 传统的感应电机故障诊断方法难以捕捉复杂的多模态信号关系，受限于单模态数据或单一故障类型，并且在噪声或跨域条件下性能下降，导致高昂的非计划停机成本。

Method: 本文提出了多模态超图对比注意力网络（MM-HCAN），这是首个将对比学习与超图拓扑结构相结合的框架，专为多模态传感器融合设计。它能够联合建模模内和模间依赖关系，并增强欧几里得嵌入空间之外的泛化能力。

Result: MM-HCAN在三个真实世界基准测试中实现了高达99.82%的准确率，展现出强大的跨域泛化能力和对噪声的鲁棒性。它能同时诊断轴承、定子和转子故障。消融研究验证了每个组件的贡献。

Conclusion: MM-HCAN为全面的多故障诊断提供了一个可扩展且鲁棒的解决方案，支持工业环境中的预测性维护和延长资产寿命。

Abstract: Reliable induction motor (IM) fault diagnosis is vital for industrial safety
and operational continuity, mitigating costly unplanned downtime. Conventional
approaches often struggle to capture complex multimodal signal relationships,
are constrained to unimodal data or single fault types, and exhibit performance
degradation under noisy or cross-domain conditions. This paper proposes the
Multimodal Hypergraph Contrastive Attention Network (MM-HCAN), a unified
framework for robust fault diagnosis. To the best of our knowledge, MM-HCAN is
the first to integrate contrastive learning within a hypergraph topology
specifically designed for multimodal sensor fusion, enabling the joint
modelling of intra- and inter-modal dependencies and enhancing generalisation
beyond Euclidean embedding spaces. The model facilitates simultaneous diagnosis
of bearing, stator, and rotor faults, addressing the engineering need for
consolidated di- agnostic capabilities. Evaluated on three real-world
benchmarks, MM-HCAN achieves up to 99.82% accuracy with strong cross-domain
generalisation and resilience to noise, demonstrating its suitability for
real-world deployment. An ablation study validates the contribution of each
component. MM-HCAN provides a scalable and robust solution for comprehensive
multi-fault diagnosis, supporting predictive maintenance and extended asset
longevity in industrial environments.

</details>


### [19] [JudgeSQL: Reasoning over SQL Candidates with Weighted Consensus Tournament](https://arxiv.org/abs/2510.15560)
*Jiayuan Bai,Xuan-guang Pan,Chongyang Tao,Shuai Ma*

Main category: cs.AI

TL;DR: JudgeSQL是一个原则性框架，通过结构化推理和加权共识锦标赛机制，显著提升了Text-to-SQL任务中SQL候选查询的选择准确性和效率。


<details>
  <summary>Details</summary>
Motivation: Text-to-SQL任务因语义歧义和复杂组合推理而具有挑战性。尽管大型语言模型（LLMs）通过提示、微调和强化学习极大地推动了SQL生成，但在测试时选择正确的查询成为了新的瓶颈。现有的选择方法（如自洽性或最佳N选一）提供的信号过于浅层，导致评分不一致、推理链脆弱，并且无法捕捉密切相关SQL候选之间的细微语义差异。

Method: 本文提出了JudgeSQL框架：1. 开发了一个基于推理的SQL判断模型，该模型通过可验证奖励指导的强化学习来提炼推理轨迹，从而实现准确和可解释的判断。2. 在此基础上，集成了一个加权共识锦标赛机制，该机制将显式推理偏好与隐式生成器置信度相结合。

Result: 在BIRD基准测试上的大量实验表明，JudgeSQL展现出卓越的SQL判断能力，并具有良好的跨尺度泛化性和对生成器容量的鲁棒性。

Conclusion: JudgeSQL通过其结构化推理和加权共识锦标赛机制，重新定义了SQL候选查询的选择，提供了更可靠、更高效的解决方案。

Abstract: Text-to-SQL is a pivotal task that bridges natural language understanding and
structured data access, yet it remains fundamentally challenging due to
semantic ambiguity and complex compositional reasoning. While large language
models (LLMs) have greatly advanced SQL generation though prompting, supervised
finetuning and reinforced tuning, the shift toward test-time scaling exposes a
new bottleneck: selecting the correct query from a diverse candidate pool.
Existing selection approaches, such as self-consistency or best-of-$N$
decoding, provide only shallow signals, making them prone to inconsistent
scoring, fragile reasoning chains, and a failure to capture fine-grained
semantic distinctions between closely related SQL candidates. To this end, we
introduce JudgeSQL, a principled framework that redefines SQL candidate
selection through structured reasoning and weighted consensus tournament
mechanism. JudgeSQL develops a reasoning-based SQL judge model that distills
reasoning traces with reinforcement learning guided by verifiable rewards,
enabling accurate and interpretable judgments. Building on this, a weighted
consensus tournament integrates explicit reasoning preferences with implicit
generator confidence, yielding selections that are both more reliable and more
efficient. Extensive experiments on the BIRD benchmark demonstrate that
JudgeSQL exhibits superior SQL judgment capabilities and good cross-scale
generalization and robustness to generator capacity.

</details>


### [20] [Unleashing Scientific Reasoning for Bio-experimental Protocol Generation via Structured Component-based Reward Mechanism](https://arxiv.org/abs/2510.15600)
*Haoran Sun,Yankai Jiang,Zhenyu Tang,Yaning Pan,Shuang Gu,Zekai Lin,Lilong Wang,Wenjie Lou,Lei Liu,Lei Bai,Xiaosong Wang*

Main category: cs.AI

TL;DR: 该研究引入了SciRecipe数据集和“Sketch-and-Fill”范式，并开发了Thoth模型，显著提升了大型语言模型（LLM）生成精确、可执行科学协议的能力，超越了现有模型。


<details>
  <summary>Details</summary>
Motivation: 可重复科学的基础在于精确、逻辑有序且可执行的协议。通过自然语言查询自动生成这些协议可以极大地提高复制过程的效率。然而，当前领先的LLM通常生成的协议不完整或不一致，限制了其实用性。

Method: 1. 引入了SciRecipe，一个包含超过12K个结构化协议的大规模数据集，涵盖27个生物学子领域。2. 提出了“Sketch-and-Fill”范式，将分析、结构化和表达分离，确保每一步都明确且可验证。3. 设计了结构化组件奖励机制，评估步骤粒度、动作顺序和语义保真度。4. 开发了Thoth模型，通过分阶段的“知识到行动”（Knowledge-to-Action）过程进行训练，从知识获取到操作推理，最终实现鲁棒、可执行的协议生成。

Result: 在多个基准测试中，Thoth模型持续超越了专有和开源LLM，在步骤对齐、逻辑排序和语义准确性方面取得了显著改进。

Conclusion: 该方法为可靠的科学助手铺平了道路，这些助手能够将知识与实验执行联系起来。所有数据、代码和模型将公开。

Abstract: The foundation of reproducible science lies in protocols that are precise,
logically ordered, and executable. The autonomous generation of these protocols
through natural language queries could greatly improve the efficiency of the
reproduction process. However, current leading large language models (LLMs)
often generate incomplete or inconsistent protocols, limiting their utility. To
address this limitation, we first introduce SciRecipe, a large-scale dataset of
over 12K structured protocols spanning 27 biological subfields and encompassing
both comprehension and problem-solving tasks. To further improve protocol
generation, we propose the "Sketch-and-Fill" paradigm, which separates
analysis, structuring, and expression to ensure each step is explicit and
verifiable. Complementing this, the structured component-based reward mechanism
evaluates step granularity, action order, and semantic fidelity, aligning model
optimization with experimental reliability. Building on these components, we
develop Thoth, trained through a staged Knowledge-to-Action process that
progresses from knowledge acquisition to operational reasoning and ultimately
to robust, executable protocol generation. Across multiple benchmarks, Thoth
consistently surpasses both proprietary and open-source LLMs, achieving
significant improvements in step alignment, logical sequencing, and semantic
accuracy. Our approach paves the way for reliable scientific assistants that
bridge knowledge with experimental execution. All data, code, and models will
be released publicly.

</details>


### [21] [Context-aware deep learning using individualized prior information reduces false positives in disease risk prediction and longitudinal health assessment](https://arxiv.org/abs/2510.15591)
*Lavanya Umapathy,Patricia M Johnson,Tarun Dutt,Angela Tong,Madhur Nayan,Hersh Chandarana,Daniel K Sodickson*

Main category: cs.AI

TL;DR: 本研究开发了一个机器学习框架，通过整合患者既往就诊的纵向时间背景信息（包括影像和临床生物标志物），显著提高了医学风险预测（以前列腺癌为例）的特异性，大幅降低了假阳性率，从而有望扩展大规模人群的健康监测。


<details>
  <summary>Details</summary>
Motivation: 医学中时间背景信息对于评估患者健康随时间的关键变化至关重要。然而，在既往就诊数据有限且频率不一致的情况下，如何有效整合这些多样化的背景信息以改进健康监测是一个挑战。

Method: 研究开发了一个机器学习框架：首先利用最近一次就诊的医学数据评估疾病的初始风险，然后通过整合既往收集的影像和/或临床生物标志物信息来完善风险评估。该框架应用于前列腺癌（PCa）风险预测，使用了近十年间收集的大型数据集，包括28,342名患者、39,013次磁共振成像扫描和68,931次血液检查。

Result: 在预测就诊时临床显著PCa风险方面，整合既往背景信息将假阳性直接转换为真阴性，在保持高敏感性的同时提高了总体特异性。与仅使用单次就诊数据相比，整合多达三次既往影像检查信息使假阳性率从51%降至33%，若再加入既往临床数据，则进一步降至24%。在预测就诊后五年内PCa风险方面，整合既往背景信息使假阳性率从64%进一步降至9%。

Conclusion: 研究结果表明，随时间收集的信息提供了相关的背景，能够显著提高医学风险预测的特异性。对于广泛的渐进性疾病，通过利用背景信息充分降低假阳性率，可以为将纵向健康监测项目扩展到具有较低基线疾病风险的大规模人群提供途径，从而实现早期检测并改善健康结果。

Abstract: Temporal context in medicine is valuable in assessing key changes in patient
health over time. We developed a machine learning framework to integrate
diverse context from prior visits to improve health monitoring, especially when
prior visits are limited and their frequency is variable. Our model first
estimates initial risk of disease using medical data from the most recent
patient visit, then refines this assessment using information digested from
previously collected imaging and/or clinical biomarkers. We applied our
framework to prostate cancer (PCa) risk prediction using data from a large
population (28,342 patients, 39,013 magnetic resonance imaging scans, 68,931
blood tests) collected over nearly a decade. For predictions of the risk of
clinically significant PCa at the time of the visit, integrating prior context
directly converted false positives to true negatives, increasing overall
specificity while preserving high sensitivity. False positive rates were
reduced progressively from 51% to 33% when integrating information from up to
three prior imaging examinations, as compared to using data from a single
visit, and were further reduced to 24% when also including additional context
from prior clinical data. For predicting the risk of PCa within five years of
the visit, incorporating prior context reduced false positive rates still
further (64% to 9%). Our findings show that information collected over time
provides relevant context to enhance the specificity of medical risk
prediction. For a wide range of progressive conditions, sufficient reduction of
false positive rates using context could offer a pathway to expand longitudinal
health monitoring programs to large populations with comparatively low baseline
risk of disease, leading to earlier detection and improved health outcomes.

</details>


### [22] [Build Your Personalized Research Group: A Multiagent Framework for Continual and Interactive Science Automation](https://arxiv.org/abs/2510.15624)
*Ed Li,Junyu Ren,Xintian Pan,Cat Yan,Chuanhao Li,Dirk Bergemann,Zhuoran Yang*

Main category: cs.AI

TL;DR: 该论文提出了一个名为 `freephdlabor` 的开源多智能体框架，旨在通过动态工作流和模块化架构解决现有科学发现自动化系统中工作流僵化和上下文管理不足的问题，从而实现持续、可定制的端到端自动化科学研究。


<details>
  <summary>Details</summary>
Motivation: 现有科学发现自动化智能体系统存在两个主要局限性：一是僵化、预编程的工作流无法适应中间发现；二是上下文管理不足，阻碍了长期研究。这些限制使得自动化研究难以从孤立的单次尝试转变为系统性的持续研究。

Method: 本文提出了 `freephdlabor` 框架，其特点包括：1) 完全动态的工作流，由智能体实时推理决定；2) 模块化架构，允许用户定制、添加或移除智能体；3) 综合基础设施，包括自动上下文压缩、基于工作区的通信（防止信息退化）、跨会话的记忆持久性以及非阻塞式人机干预机制。

Result: 通过上述特性，`freephdlabor` 将自动化研究从孤立的单次尝试转变为持续的研究项目，这些项目能够系统地建立在先前探索的基础上并整合人类反馈。该框架使得实践者能够部署交互式多智能体系统，自主进行从构思到实验再到出版就绪手稿的端到端研究。

Conclusion: 该工作旨在通过提供构建可定制的协同科学家系统的架构原则和实际实现，促进自动化研究在科学领域的更广泛采用，使研究人员能够利用交互式多智能体系统自主进行完整的科学研究过程。

Abstract: The automation of scientific discovery represents a critical milestone in
Artificial Intelligence (AI) research. However, existing agentic systems for
science suffer from two fundamental limitations: rigid, pre-programmed
workflows that cannot adapt to intermediate findings, and inadequate context
management that hinders long-horizon research. We present
\texttt{freephdlabor}, an open-source multiagent framework featuring
\textit{fully dynamic workflows} determined by real-time agent reasoning and a
\coloremph{\textit{modular architecture}} enabling seamless customization --
users can modify, add, or remove agents to address domain-specific
requirements. The framework provides comprehensive infrastructure including
\textit{automatic context compaction}, \textit{workspace-based communication}
to prevent information degradation, \textit{memory persistence} across
sessions, and \textit{non-blocking human intervention} mechanisms. These
features collectively transform automated research from isolated, single-run
attempts into \textit{continual research programs} that build systematically on
prior explorations and incorporate human feedback. By providing both the
architectural principles and practical implementation for building customizable
co-scientist systems, this work aims to facilitate broader adoption of
automated research across scientific domains, enabling practitioners to deploy
interactive multiagent systems that autonomously conduct end-to-end research --
from ideation through experimentation to publication-ready manuscripts.

</details>


### [23] [Invoice Information Extraction: Methods and Performance Evaluation](https://arxiv.org/abs/2510.15727)
*Sai Yashwant,Anurag Dubey,Praneeth Paikray,Gantala Thulsiram*

Main category: cs.AI

TL;DR: 本文提出了一种从发票中提取结构化信息的方法，并设计了一套评估指标来衡量提取数据的准确性。


<details>
  <summary>Details</summary>
Motivation: 需要一种可靠且可量化的方法来评估从发票文档中提取结构化信息的准确性，以便比较不同的提取方法并识别其优缺点。

Method: 方法包括对扫描或数字发票进行预处理，使用Docling和LlamaCloud服务识别并提取发票号码、日期、总金额、供应商详情等关键字段。为评估提取过程的可靠性，建立了一个评估框架，包含字段级精度、一致性检查失败率和精确匹配准确率等指标。

Result: 提出了一套评估指标（EM），包括字段级精度、一致性检查失败和精确匹配准确率，为比较不同提取方法和突出字段特定性能的优缺点提供了一种标准化方式。

Conclusion: 论文建立了一个用于评估发票数据提取准确性的强大评估框架和一套标准化指标，有助于比较和改进不同的信息提取方法。

Abstract: This paper presents methods for extracting structured information from
invoice documents and proposes a set of evaluation metrics (EM) to assess the
accuracy of the extracted data against annotated ground truth. The approach
involves pre-processing scanned or digital invoices, applying Docling and
LlamaCloud Services to identify and extract key fields such as invoice number,
date, total amount, and vendor details. To ensure the reliability of the
extraction process, we establish a robust evaluation framework comprising
field-level precision, consistency check failures, and exact match accuracy.
The proposed metrics provide a standardized way to compare different extraction
methods and highlight strengths and weaknesses in field-specific performance.

</details>


### [24] [Direct Preference Optimization with Unobserved Preference Heterogeneity: The Necessity of Ternary Preferences](https://arxiv.org/abs/2510.15716)
*Keertana Chidambaram,Karthik Vinary Seetharaman,Vasilis Syrgkanis*

Main category: cs.AI

TL;DR: 本文指出RLHF和DPO在对齐大型语言模型时，存在假设标注者偏好一致和仅使用二元比较的局限性。为此，作者从计量经济学角度证明了多项排名（三项或更多）对于识别潜在用户偏好至关重要，并提出了处理异构偏好的方法：一种基于EM的DPO变体来发现标注者类型并训练混合LLM，以及一种基于min-max遗憾公平准则的聚合算法，以实现公平的性能保证。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型对齐方法（如RLHF和DPO）普遍假设标注者的偏好是统一的，并且仅依赖于二元比较反馈。然而，这忽视了人类评估者的多样性以及成对反馈的局限性，导致无法充分捕捉和处理复杂的、异构的用户偏好。

Method: 1. 将RLHF中的偏好学习与计量经济学文献相结合，理论上证明了二元比较不足以从有限数据和无限用户中识别潜在用户偏好，而三项或更多响应的排名（即使不完整）能够确保可识别性。2. 引入了将异构偏好纳入对齐算法的方法：a) 开发了DPO的期望最大化（EM）适应版本，用于发现潜在的标注者类型并相应地训练LLMs的混合模型。b) 提出了一种使用min-max遗憾公平准则的聚合算法，以生成具有公平性能保证的单一生成策略。

Result: 1. 理论上证明了二元比较不足以识别潜在用户偏好，而三项或更多响应的排名能确保可识别性。2. 提出了两种算法：一种是用于发现潜在标注者类型并训练LLMs混合模型的EM-DPO变体；另一种是使用min-max遗憾公平准则的聚合算法，旨在为生成模型提供公平的性能保证。

Conclusion: 本研究为生成模型对齐中面向多样化用户的公平性和个性化建立了一个理论和算法框架，通过解决现有方法在处理异构偏好和二元比较局限性方面的不足，提升了模型对齐的鲁棒性和公平性。

Abstract: Reinforcement Learning from Human Feedback (RLHF) has become central to
aligning large language models with human values, typically by first learning a
reward model from preference data which is then used to update the model with
reinforcement learning. Recent alternatives such as Direct Preference
Optimization (DPO) simplify this pipeline by directly optimizing on
preferences. However, both approaches often assume uniform annotator
preferences and rely on binary comparisons, overlooking two key limitations:
the diversity of human evaluators and the limitations of pairwise feedback. In
this work, we address both these issues. First, we connect preference learning
in RLHF with the econometrics literature and show that binary comparisons are
insufficient for identifying latent user preferences from finite user data and
infinite users, while (even incomplete) rankings over three or more responses
ensure identifiability. Second, we introduce methods to incorporate
heterogeneous preferences into alignment algorithms. We develop an
Expectation-Maximization adaptation of DPO that discovers latent annotator
types and trains a mixture of LLMs accordingly. Then we propose an aggregation
algorithm using a min-max regret fairness criterion to produce a single
generative policy with equitable performance guarantees. Together, these
contributions establish a theoretical and algorithmic framework for fairness
and personalization for diverse users in generative model alignment.

</details>


### [25] [AURA: An Agent Autonomy Risk Assessment Framework](https://arxiv.org/abs/2510.15739)
*Lorenzo Satta Chiris,Ayush Mishra*

Main category: cs.AI

TL;DR: 该论文提出了AURA（Agent aUtonomy Risk Assessment）框架，一个统一的风险评估与缓解系统，旨在解决自主智能体AI在大规模部署中面临的对齐、治理和风险管理挑战。


<details>
  <summary>Details</summary>
Motivation: 随着自主智能体AI系统在组织中日益普及，其在对齐、治理和风险管理方面的持续挑战，威胁着大规模部署的进程。

Method: AURA框架采用基于gamma的风险评分方法，平衡了风险评估的准确性、计算效率和实际考量。它提供了一个交互式流程来评估和缓解AI智能体的风险，并支持人机协作（HITL）监督和智能体到人（A2H）通信机制，实现与现有协议（MCP和A2A）和工具的互操作性。

Result: AURA能够检测、量化和缓解智能体AI带来的风险，同时平衡计算资源，确保风险评估的准确性与效率。它支持负责任和透明地采用智能体AI，并提供强大的风险检测和缓解能力。

Conclusion: AURA被定位为企业环境中实现大规模、可治理的智能体AI的关键推动者，通过提供鲁棒的风险检测和缓解方案，同时兼顾计算效率，促进智能体AI的负责任部署。

Abstract: As autonomous agentic AI systems see increasing adoption across
organisations, persistent challenges in alignment, governance, and risk
management threaten to impede deployment at scale. We present AURA (Agent
aUtonomy Risk Assessment), a unified framework designed to detect, quantify,
and mitigate risks arising from agentic AI. Building on recent research and
practical deployments, AURA introduces a gamma-based risk scoring methodology
that balances risk assessment accuracy with computational efficiency and
practical considerations. AURA provides an interactive process to score,
evaluate and mitigate the risks of running one or multiple AI Agents,
synchronously or asynchronously (autonomously). The framework is engineered for
Human-in-the-Loop (HITL) oversight and presents Agent-to-Human (A2H)
communication mechanisms, allowing for seamless integration with agentic
systems for autonomous self-assessment, rendering it interoperable with
established protocols (MCP and A2A) and tools. AURA supports a responsible and
transparent adoption of agentic AI and provides robust risk detection and
mitigation while balancing computational resources, positioning it as a
critical enabler for large-scale, governable agentic AI in enterprise
environments.

</details>


### [26] [Towards Relaxed Multimodal Inputs for Gait-based Parkinson's Disease Assessment](https://arxiv.org/abs/2510.15748)
*Minlin Zeng,Zhipeng Zhou,Yang Qiu,Zhiqi Shen*

Main category: cs.AI

TL;DR: 本文提出了一种名为TRIP的帕金森病评估系统，通过将多模态学习建模为多目标优化问题，解决了现有方法在训练和推理时对模态同步和完整性的依赖，并结合基于裕度的类别再平衡策略，在同步和异步设置下均实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态帕金森病评估方法存在两个主要限制：1) 训练时需要所有模态同步；2) 推理时依赖所有模态。这严重阻碍了其实际应用。

Method: 1. 将多模态学习建模为多目标优化（MOO）问题，以实现训练和推理时更灵活的模态要求，并解决模态融合中的模态崩溃问题。2. 引入基于裕度的类别再平衡策略，以缓解单个模态内部的不平衡，增强类别学习。

Result: 在三个公共数据集上进行了广泛实验，结果显示所提出的TRIP框架在异步设置下性能优于最佳基线16.48、6.89和11.55个百分点，在同步设置下优于最佳基线4.86和2.30个百分点，达到了最先进的性能。

Conclusion: 所提出的TRIP框架有效且适应性强，解决了多模态帕金森病评估中的关键限制，并在各种设置下均展现出卓越的性能。

Abstract: Parkinson's disease assessment has garnered growing interest in recent years,
particularly with the advent of sensor data and machine learning techniques.
Among these, multimodal approaches have demonstrated strong performance by
effectively integrating complementary information from various data sources.
However, two major limitations hinder their practical application: (1) the need
to synchronize all modalities during training, and (2) the dependence on all
modalities during inference. To address these issues, we propose the first
Parkinson's assessment system that formulates multimodal learning as a
multi-objective optimization (MOO) problem. This not only allows for more
flexible modality requirements during both training and inference, but also
handles modality collapse issue during multimodal information fusion. In
addition, to mitigate the imbalance within individual modalities, we introduce
a margin-based class rebalancing strategy to enhance category learning. We
conduct extensive experiments on three public datasets under both synchronous
and asynchronous settings. The results show that our framework-Towards Relaxed
InPuts (TRIP)-achieves state-of-the-art performance, outperforming the best
baselines by 16.48, 6.89, and 11.55 percentage points in the asynchronous
setting, and by 4.86 and 2.30 percentage points in the synchronous setting,
highlighting its effectiveness and adaptability.

</details>


### [27] [Preliminary Quantitative Study on Explainability and Trust in AI Systems](https://arxiv.org/abs/2510.15769)
*Allen Daniel Sunny*

Main category: cs.AI

TL;DR: 本研究通过贷款审批模拟实验发现，解释的互动性、清晰度和相关性显著提升用户对AI系统的信任和参与度。


<details>
  <summary>Details</summary>
Motivation: 大型AI模型广泛应用于关键领域，引发了对信任和透明度的紧急质疑。本研究旨在探讨可解释性与用户对AI系统信任之间的关系。

Method: 采用定量实验设计，通过基于网络的互动式贷款审批模拟，比较不同类型的解释（从基本特征重要性到互动式反事实）如何影响用户感知到的信任。

Result: 研究结果表明，互动性增强了用户参与度和信心，并且解释的清晰度和相关性是决定信任的关键因素。

Conclusion: 这些发现为以人为中心的可解释AI领域提供了实证证据，强调了可解释性设计对用户感知具有可衡量的影响。

Abstract: Large-scale AI models such as GPT-4 have accelerated the deployment of
artificial intelligence across critical domains including law, healthcare, and
finance, raising urgent questions about trust and transparency. This study
investigates the relationship between explainability and user trust in AI
systems through a quantitative experimental design. Using an interactive,
web-based loan approval simulation, we compare how different types of
explanations, ranging from basic feature importance to interactive
counterfactuals influence perceived trust. Results suggest that interactivity
enhances both user engagement and confidence, and that the clarity and
relevance of explanations are key determinants of trust. These findings
contribute empirical evidence to the growing field of human-centered
explainable AI, highlighting measurable effects of explainability design on
user perception

</details>


### [28] [Self-evolving expertise in complex non-verifiable subject domains: dialogue as implicit meta-RL](https://arxiv.org/abs/2510.15772)
*Richard M. Bailey*

Main category: cs.AI

TL;DR: 本文提出Dialectica框架，通过结构化对话、记忆、自反思和上下文编辑，使大型语言模型（LLMs）代理在解决“棘手问题”时发展专业知识，并显著超越基线代理。


<details>
  <summary>Details</summary>
Motivation: “棘手问题”复杂且难以解决，尽管LLMs正被探索用于此类问题，但它们缺乏通过经验内生发展专业知识的机制。本文旨在弥补这一空白。

Method: 引入Dialectica框架，代理在此框架中进行结构化对话，并辅以记忆、自反思和策略约束的上下文编辑。将讨论视为隐式元强化学习过程。通过对代理生成响应进行人工判断的配对比较（使用Elo分数、Bradley-Terry-Davidson能力和AlphaRank质量）进行后验评估。

Result: 在两种模型架构（Qwen3:30b和o4-mini）上，启用基于反思的上下文编辑的代理在Elo分数、标准化Bradley-Terry-Davidson能力和AlphaRank质量上均显著优于基线代理。定性分析表明，反思日志能够识别弱点并可靠地塑造后续的陈述。

Conclusion: 定量和定性证据一致表明，对话驱动的上下文演化是LLMs在开放、不可验证领域中实现目标专业知识增强的实用途径。

Abstract: So-called `wicked problems', those involving complex multi-dimensional
settings, non-verifiable outcomes, heterogeneous impacts and a lack of single
objectively correct answers, have plagued humans throughout history. Modern
examples include decisions over justice frameworks, solving environmental
pollution, planning for pandemic resilience and food security. The use of
state-of-the-art artificial intelligence systems (notably Large Language
Model-based agents) collaborating with humans on solving such problems is being
actively explored. While the abilities of LLMs can be improved by, for example,
fine-tuning, hand-crafted system prompts and scaffolding with external tools,
LLMs lack endogenous mechanisms to develop expertise through experience in such
settings. This work address this gap with Dialectica, a framework where agents
engage in structured dialogue on defined topics, augmented by memory,
self-reflection, and policy-constrained context editing. Formally, discussion
is viewed as an implicit meta-reinforcement learning process. The
`dialogue-trained' agents are evaluated post-hoc using judged pairwise
comparisons of elicited responses. Across two model architectures (locally run
Qwen3:30b and OpenAI's o4-mini) results show that enabling reflection-based
context editing during discussion produces agents which dominate their baseline
counterparts on Elo scores, normalized Bradley-Terry-Davidson ability, and
AlphaRank mass. The predicted signatures of learning are observed qualitatively
in statement and reflection logs, where reflections identify weaknesses and
reliably shape subsequent statements. Agreement between quantitative and
qualitative evidence supports dialogue-driven context evolution as a practical
path to targeted expertise amplification in open non-verifiable domains.

</details>


### [29] [Demo: Guide-RAG: Evidence-Driven Corpus Curation for Retrieval-Augmented Generation in Long COVID](https://arxiv.org/abs/2510.15782)
*Philip DiGiacomo,Haoyang Wang,Jinrui Fang,Yan Leng,W Michael Brode,Ying Ding*

Main category: cs.AI

TL;DR: 针对新兴疾病（如长新冠），本研究评估了六种RAG语料库配置，发现结合临床指南和高质量系统性综述的配置在临床问答中表现最佳，并提出了Guide-RAG系统。


<details>
  <summary>Details</summary>
Motivation: AI聊天机器人在临床医学中日益普及，但为复杂、新兴疾病开发有效的框架面临重大挑战。

Method: 开发并评估了六种检索增强生成（RAG）语料库配置，范围从专家精选来源到大规模文献数据库。采用“LLM作为评判者”框架，使用专家生成的LongCOVID-CQ数据集，评估了忠实性、相关性和全面性指标。提出了Guide-RAG聊天机器人系统及配套评估框架。

Result: 结合临床指南和高质量系统性综述的RAG语料库配置，始终优于狭窄的单一指南方法和大规模文献数据库。对于新兴疾病，基于精选二级综述的检索在狭窄共识文件和未过滤初级文献之间提供了最佳平衡。

Conclusion: 对于新兴疾病，基于精选二级综述的检索能有效支持临床决策，同时避免信息过载和过度简化指导。Guide-RAG系统通过整合精选专家知识和全面文献数据库，能有效回答长新冠临床问题。

Abstract: As AI chatbots gain adoption in clinical medicine, developing effective
frameworks for complex, emerging diseases presents significant challenges. We
developed and evaluated six Retrieval-Augmented Generation (RAG) corpus
configurations for Long COVID (LC) clinical question answering, ranging from
expert-curated sources to large-scale literature databases. Our evaluation
employed an LLM-as-a-judge framework across faithfulness, relevance, and
comprehensiveness metrics using LongCOVID-CQ, a novel dataset of
expert-generated clinical questions. Our RAG corpus configuration combining
clinical guidelines with high-quality systematic reviews consistently
outperformed both narrow single-guideline approaches and large-scale literature
databases. Our findings suggest that for emerging diseases, retrieval grounded
in curated secondary reviews provides an optimal balance between narrow
consensus documents and unfiltered primary literature, supporting clinical
decision-making while avoiding information overload and oversimplified
guidance. We propose Guide-RAG, a chatbot system and accompanying evaluation
framework that integrates both curated expert knowledge and comprehensive
literature databases to effectively answer LC clinical questions.

</details>


### [30] [PokeeResearch: Effective Deep Research via Reinforcement Learning from AI Feedback and Robust Reasoning Scaffold](https://arxiv.org/abs/2510.15862)
*Yi Wan,Jiuqi Wang,Liam Li,Jinsong Liu,Ruihao Zhu,Zheqing Zhu*

Main category: cs.AI

TL;DR: PokeeResearch-7B是一个7B参数的深度研究智能体，它采用统一的强化学习框架和无标注的RLAIF进行训练，并通过思维链驱动的多调用推理支架增强鲁棒性，在10个主流深度研究基准测试中达到了7B规模智能体的最先进水平。


<details>
  <summary>Details</summary>
Motivation: 当前的工具增强型大型语言模型（LLMs）智能体存在检索深度不足、对齐指标薄弱以及工具使用行为脆弱等局限性，需要开发更强大、对齐性更好、更具可扩展性的深度研究智能体。

Method: 该研究引入了PokeeResearch-7B，一个7B参数的深度研究智能体。它在一个统一的强化学习框架下构建，并采用无标注的“从AI反馈中进行强化学习”（RLAIF）框架进行训练，利用基于LLM的奖励信号来优化策略，这些信号捕捉事实准确性、引用忠实度和指令依从性。此外，通过思维链驱动的多调用推理支架，实现了自我验证和从工具故障中自适应恢复，从而增强了鲁棒性。

Result: PokeeResearch-7B在10个流行的深度研究基准测试中，在7B规模的深度研究智能体中取得了最先进的性能。

Conclusion: 该研究表明，精心设计的强化学习和推理机制能够产生高效、有弹性且达到研究级别的AI智能体。

Abstract: Tool-augmented large language models (LLMs) are emerging as deep research
agents, systems that decompose complex queries, retrieve external evidence, and
synthesize grounded responses. Yet current agents remain limited by shallow
retrieval, weak alignment metrics, and brittle tool-use behavior. We introduce
PokeeResearch-7B, a 7B-parameter deep research agent built under a unified
reinforcement learning framework for robustness, alignment, and scalability.
PokeeResearch-7B is trained by an annotation-free Reinforcement Learning from
AI Feedback (RLAIF) framework to optimize policies using LLM-based reward
signals that capture factual accuracy, citation faithfulness, and instruction
adherence. A chain-of-thought-driven multi-call reasoning scaffold further
enhances robustness through self-verification and adaptive recovery from tool
failures. Among 10 popular deep research benchmarks, PokeeResearch-7B achieves
state-of-the-art performance among 7B-scale deep research agents. This
highlights that careful reinforcement learning and reasoning design can produce
efficient, resilient, and research-grade AI agents. The model and inference
code is open-sourced under MIT license at
https://github.com/Pokee-AI/PokeeResearchOSS.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [31] [GAZE:Governance-Aware pre-annotation for Zero-shot World Model Environments](https://arxiv.org/abs/2510.14992)
*Leela Krishna,Mengyang Zhao,Saicharithreddy Pasula,Harshit Rajgarhia,Abhishek Mukherji*

Main category: cs.CV

TL;DR: GAZE是一个生产级的流水线，它自动化了将原始视频转换为用于世界模型训练的丰富、任务就绪的多模态监督数据，显著提高了效率并降低了人工标注成本。


<details>
  <summary>Details</summary>
Motivation: 训练鲁棒的世界模型需要大规模、精确标注的多模态数据集，而传统的人工标注过程缓慢且昂贵，成为瓶颈。

Method: GAZE流水线包括：(i) 将专有360度视频格式标准化为标准视图并分片以进行并行处理；(ii) 应用一系列AI模型（场景理解、对象跟踪、音频转录、PII/NSFW/未成年人检测）进行密集的多模态预标注；(iii) 将所有信号整合为结构化输出规范，以便快速进行人工验证。该系统还通过自动跳过低显著性片段来减少人工审核量。

Result: GAZE工作流程显著提高了效率（每审核小时节省约19分钟），并通过保守地自动跳过低显著性片段，将人工审核量减少了80%以上。通过增加标签密度和一致性，并整合隐私保护和保管链元数据，该方法生成了高保真度、注重隐私的数据集，可直接用于学习跨模态动态和基于动作的预测。

Conclusion: GAZE提供了一个可扩展的蓝图，用于在不牺牲吞吐量或治理的情况下，高效生成高质量、注重隐私的世界模型训练数据，从而解决了传统标注的瓶颈问题。

Abstract: Training robust world models requires large-scale, precisely labeled
multimodal datasets, a process historically bottlenecked by slow and expensive
manual annotation. We present a production-tested GAZE pipeline that automates
the conversion of raw, long-form video into rich, task-ready supervision for
world-model training. Our system (i) normalizes proprietary 360-degree formats
into standard views and shards them for parallel processing; (ii) applies a
suite of AI models (scene understanding, object tracking, audio transcription,
PII/NSFW/minor detection) for dense, multimodal pre-annotation; and (iii)
consolidates signals into a structured output specification for rapid human
validation.
  The GAZE workflow demonstrably yields efficiency gains (~19 minutes saved per
review hour) and reduces human review volume by >80% through conservative
auto-skipping of low-salience segments. By increasing label density and
consistency while integrating privacy safeguards and chain-of-custody metadata,
our method generates high-fidelity, privacy-aware datasets directly consumable
for learning cross-modal dynamics and action-conditioned prediction. We detail
our orchestration, model choices, and data dictionary to provide a scalable
blueprint for generating high-quality world model training data without
sacrificing throughput or governance.

</details>


### [32] [ClapperText: A Benchmark for Text Recognition in Low-Resource Archival Documents](https://arxiv.org/abs/2510.15557)
*Tingyu Lin,Marco Peer,Florian Kleber,Robert Sablatnig*

Main category: cs.CV

TL;DR: 本文提出了ClapperText数据集，一个用于视觉退化和低资源环境中手写及印刷文本识别的基准数据集，并对多种识别和检测模型进行了基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有OCR技术在处理视觉退化、低资源历史文档（特别是二战时期视频场记板中结构化但非标准形式的内容）时面临巨大挑战，如运动模糊、手写差异、曝光波动和杂乱背景。

Method: ClapperText数据集来源于127个二战时期档案视频片段中的场记板，包含9,813个标注帧和94,573个词级文本实例（67%为手写，1,566个部分遮挡）。每个实例都包含转录、语义类别、文本类型和遮挡状态，并使用4点多边形旋转边界框进行标注。研究团队在零样本和微调条件下，使用一致的视频级评估协议，对六个代表性识别模型和七个检测模型进行了基准测试。

Result: 尽管训练集很小（18个视频），但微调能带来显著的性能提升，表明ClapperText适用于少样本学习场景。这突出显示了数据集所反映的挑战（如运动模糊、手写变异、曝光波动、杂乱背景）。

Conclusion: ClapperText数据集为在低资源档案环境中推进鲁棒OCR和文档理解提供了一个真实且具有文化基础的资源，特别适用于少样本学习情境。

Abstract: This paper presents ClapperText, a benchmark dataset for handwritten and
printed text recognition in visually degraded and low-resource settings. The
dataset is derived from 127 World War II-era archival video segments containing
clapperboards that record structured production metadata such as date,
location, and camera-operator identity. ClapperText includes 9,813 annotated
frames and 94,573 word-level text instances, 67% of which are handwritten and
1,566 are partially occluded. Each instance includes transcription, semantic
category, text type, and occlusion status, with annotations available as
rotated bounding boxes represented as 4-point polygons to support spatially
precise OCR applications. Recognizing clapperboard text poses significant
challenges, including motion blur, handwriting variation, exposure
fluctuations, and cluttered backgrounds, mirroring broader challenges in
historical document analysis where structured content appears in degraded,
non-standard forms. We provide both full-frame annotations and cropped word
images to support downstream tasks. Using a consistent per-video evaluation
protocol, we benchmark six representative recognition and seven detection
models under zero-shot and fine-tuned conditions. Despite the small training
set (18 videos), fine-tuning leads to substantial performance gains,
highlighting ClapperText's suitability for few-shot learning scenarios. The
dataset offers a realistic and culturally grounded resource for advancing
robust OCR and document understanding in low-resource archival contexts. The
dataset and evaluation code are available at
https://github.com/linty5/ClapperText.

</details>


### [33] [PC-UNet: An Enforcing Poisson Statistics U-Net for Positron Emission Tomography Denoising](https://arxiv.org/abs/2510.14995)
*Yang Shi,Jingchao Wang,Liangsi Lu,Mingxuan Huang,Ruixin He,Yifeng Xie,Hanqian Liu,Minzhe Guo,Yangyang Liang,Weipeng Zhang,Zimeng Li,Xuhang Chen*

Main category: cs.CV

TL;DR: 针对低剂量PET图像中泊松噪声导致的失真问题，本文提出了一种泊松一致性U-Net (PC-UNet) 模型，并引入了新的泊松方差和均值一致性损失 (PVMC-Loss)，以提高图像保真度和物理一致性。


<details>
  <summary>Details</summary>
Motivation: PET在医学中至关重要，但高信噪比剂量增加了辐射暴露，限制了其临床应用。降低剂量会增加泊松噪声，而现有去噪方法无法有效处理，导致图像失真和伪影。

Method: 本文提出了泊松一致性U-Net (PC-UNet) 模型，并设计了一种新的泊松方差和均值一致性损失 (PVMC-Loss)。PVMC-Loss通过结合物理数据，在方差和梯度适应方面具有统计无偏性，并作为广义矩量法的一种实现，对数据小幅不匹配具有鲁棒性。

Result: 在PET数据集上的测试表明，PC-UNet显著提高了物理一致性和图像保真度。

Conclusion: PC-UNet模型及其PVMC-Loss能够有效整合物理信息，从而改善PET图像去噪效果，提高图像质量。

Abstract: Positron Emission Tomography (PET) is crucial in medicine, but its clinical
use is limited due to high signal-to-noise ratio doses increasing radiation
exposure. Lowering doses increases Poisson noise, which current denoising
methods fail to handle, causing distortions and artifacts. We propose a Poisson
Consistent U-Net (PC-UNet) model with a new Poisson Variance and Mean
Consistency Loss (PVMC-Loss) that incorporates physical data to improve image
fidelity. PVMC-Loss is statistically unbiased in variance and gradient
adaptation, acting as a Generalized Method of Moments implementation, offering
robustness to minor data mismatches. Tests on PET datasets show PC-UNet
improves physical consistency and image fidelity, proving its ability to
integrate physical information effectively.

</details>


### [34] [DGME-T: Directional Grid Motion Encoding for Transformer-Based Historical Camera Movement Classification](https://arxiv.org/abs/2510.15725)
*Tingyu Lin,Armin Dadras,Florian Kleber,Robert Sablatnig*

Main category: cs.CV

TL;DR: 该研究通过构建统一基准和引入DGME-T模型（Video Swin Transformer的轻量级扩展），显著提升了在现代和历史（如二战）视频片段上摄像机运动分类的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有摄像机运动分类（CMC）模型在应用于档案影片时性能下降，原因是档案影片存在噪声、丢帧和低对比度等问题，这些问题掩盖了运动线索。

Method: 研究构建了一个统一的基准，将两个现代语料库整合为四类，并将HISTORIAN数据集重构为五类。在此基础上，引入了DGME-T模型，它是Video Swin Transformer的轻量级扩展，通过一个可学习且标准化的后期融合层，注入了源自光流的方向网格运动编码。

Result: DGME-T模型将Video Swin Transformer在现代片段上的top-1准确率从81.78%提高到86.14%，宏观F1分数从82.08%提高到87.81%。在要求更高的二战影片上，准确率从83.43%提高到84.62%，宏观F1分数从81.72%提高到82.63%。跨领域研究还表明，在现代数据上进行中间微调可将历史性能提高五个百分点以上。

Conclusion: 研究结果表明，结构化运动先验和Transformer表示是互补的，即使是一个经过精心校准的小型运动头部也能显著增强在降级影片分析中的鲁棒性。

Abstract: Camera movement classification (CMC) models trained on contemporary,
high-quality footage often degrade when applied to archival film, where noise,
missing frames, and low contrast obscure motion cues. We bridge this gap by
assembling a unified benchmark that consolidates two modern corpora into four
canonical classes and restructures the HISTORIAN collection into five balanced
categories. Building on this benchmark, we introduce DGME-T, a lightweight
extension to the Video Swin Transformer that injects directional grid motion
encoding, derived from optical flow, via a learnable and normalised late-fusion
layer. DGME-T raises the backbone's top-1 accuracy from 81.78% to 86.14% and
its macro F1 from 82.08% to 87.81% on modern clips, while still improving the
demanding World-War-II footage from 83.43% to 84.62% accuracy and from 81.72%
to 82.63% macro F1. A cross-domain study further shows that an intermediate
fine-tuning stage on modern data increases historical performance by more than
five percentage points. These results demonstrate that structured motion priors
and transformer representations are complementary and that even a small,
carefully calibrated motion head can substantially enhance robustness in
degraded film analysis. Related resources are available at
https://github.com/linty5/DGME-T.

</details>


### [35] [DeLeaker: Dynamic Inference-Time Reweighting For Semantic Leakage Mitigation in Text-to-Image Models](https://arxiv.org/abs/2510.15015)
*Mor Ventura,Michael Toker,Or Patashnik,Yonatan Belinkov,Roi Reichart*

Main category: cs.CV

TL;DR: 本文提出DeLeaker，一种轻量级、无需优化的推理时方法，通过动态重新加权注意力图来缓解文生图（T2I）模型中的语义泄露。同时引入了首个语义泄露专用数据集SLIM和自动评估框架。实验表明DeLeaker在不牺牲质量的情况下显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 文生图（T2I）模型存在语义泄露问题，即不同实体之间会发生不期望的语义相关特征转移。现有缓解策略通常依赖于优化或外部输入，不够高效或便捷。

Method: DeLeaker是一种轻量级、无需优化的推理时方法，通过直接干预模型的注意力图来缓解泄露。在扩散过程中，DeLeaker动态重新加权注意力图，以抑制过度的跨实体交互，同时增强每个实体的独立性。此外，本文还引入了SLIM（Semantic Leakage in IMages）数据集，这是首个专用于语义泄露的评估数据集，包含1,130个人工验证的样本，并提出了一个新颖的自动评估框架。

Result: 实验证明，DeLeaker持续优于所有基线方法，即使这些基线获得了外部信息，它也能有效缓解语义泄露，同时不损害生成图像的保真度和质量。

Conclusion: 研究结果强调了注意力控制在提升文生图模型语义精确性方面的价值，并为开发更精确的T2I模型铺平了道路。

Abstract: Text-to-Image (T2I) models have advanced rapidly, yet they remain vulnerable
to semantic leakage, the unintended transfer of semantically related features
between distinct entities. Existing mitigation strategies are often
optimization-based or dependent on external inputs. We introduce DeLeaker, a
lightweight, optimization-free inference-time approach that mitigates leakage
by directly intervening on the model's attention maps. Throughout the diffusion
process, DeLeaker dynamically reweights attention maps to suppress excessive
cross-entity interactions while strengthening the identity of each entity. To
support systematic evaluation, we introduce SLIM (Semantic Leakage in IMages),
the first dataset dedicated to semantic leakage, comprising 1,130
human-verified samples spanning diverse scenarios, together with a novel
automatic evaluation framework. Experiments demonstrate that DeLeaker
consistently outperforms all baselines, even when they are provided with
external information, achieving effective leakage mitigation without
compromising fidelity or quality. These results underscore the value of
attention control and pave the way for more semantically precise T2I models.

</details>


### [36] [NANO3D: A Training-Free Approach for Efficient 3D Editing Without Masks](https://arxiv.org/abs/2510.15019)
*Junliang Ye,Shenghao Xie,Ruowen Zhao,Zhengyi Wang,Hongyu Yan,Wenqiang Zu,Lei Ma,Jun Zhu*

Main category: cs.CV

TL;DR: Nano3D是一个免训练的3D物体编辑框架，通过整合FlowEdit和TRELLIS并引入区域感知合并策略，实现了精确、连贯且不影响未编辑区域的3D编辑，并构建了首个大规模3D编辑数据集。


<details>
  <summary>Details</summary>
Motivation: 当前的3D物体编辑方法效率低下、不一致，且难以保持未编辑区域的完整性。大多数方法依赖多视角渲染后重建，易引入伪影并限制了实用性。

Method: Nano3D是一个免训练框架，无需遮罩即可进行精确连贯的3D物体编辑。它将FlowEdit集成到TRELLIS中，通过前视图渲染引导局部编辑，并引入了区域感知合并策略（Voxel/Slat-Merge），通过确保编辑区域和未编辑区域之间的一致性来自适应地保持结构保真度。

Result: 实验证明，Nano3D在3D一致性和视觉质量方面优于现有方法。此外，基于此框架构建了首个大规模3D编辑数据集Nano3D-Edit-100k，包含超过100,000对高质量的3D编辑数据。

Conclusion: 这项工作解决了算法设计和数据可用性方面的长期挑战，显著提高了3D编辑的通用性和可靠性，并为前馈3D编辑模型的发展奠定了基础。

Abstract: 3D object editing is essential for interactive content creation in gaming,
animation, and robotics, yet current approaches remain inefficient,
inconsistent, and often fail to preserve unedited regions. Most methods rely on
editing multi-view renderings followed by reconstruction, which introduces
artifacts and limits practicality. To address these challenges, we propose
Nano3D, a training-free framework for precise and coherent 3D object editing
without masks. Nano3D integrates FlowEdit into TRELLIS to perform localized
edits guided by front-view renderings, and further introduces region-aware
merging strategies, Voxel/Slat-Merge, which adaptively preserve structural
fidelity by ensuring consistency between edited and unedited areas. Experiments
demonstrate that Nano3D achieves superior 3D consistency and visual quality
compared with existing methods. Based on this framework, we construct the first
large-scale 3D editing datasets Nano3D-Edit-100k, which contains over 100,000
high-quality 3D editing pairs. This work addresses long-standing challenges in
both algorithm design and data availability, significantly improving the
generality and reliability of 3D editing, and laying the groundwork for the
development of feed-forward 3D editing models. Project
Page:https://jamesyjl.github.io/Nano3D

</details>


### [37] [UrbanVerse: Scaling Urban Simulation by Watching City-Tour Videos](https://arxiv.org/abs/2510.15018)
*Mingxuan Liu,Honglin He,Elisa Ricci,Wayne Wu,Bolei Zhou*

Main category: cs.CV

TL;DR: UrbanVerse是一个数据驱动的真实到模拟系统，它将众包的城市旅游视频转换为物理感知、交互式的模拟场景，用于训练城市具身AI智能体，实现了高保真度、可扩展性，并显著提升了模拟到真实世界的迁移效果。


<details>
  <summary>Details</summary>
Motivation: 城市具身AI智能体（如送货机器人）在城市中日益普及，但训练这些智能体需要多样化、高保真度的城市环境。现有的手动创建或程序生成的模拟场景要么缺乏可扩展性，要么无法捕捉真实世界的复杂性。

Method: UrbanVerse系统包含两部分：(i) UrbanVerse-100K，一个拥有10万多个带语义和物理属性的城市3D资产库；(ii) UrbanVerse-Gen，一个从视频中提取场景布局并使用检索到的资产实例化度量尺度3D模拟的自动化流水线。该系统在IsaacSim中运行，提供了来自24个国家的160个高质量构建场景，以及10个艺术家设计的测试场景基准。

Result: 实验表明，UrbanVerse场景保留了真实世界的语义和布局，其人类评估的真实感可与手动创建的场景相媲美。在城市导航任务中，在UrbanVerse中训练的策略展现出规模幂律和强大的泛化能力，与现有方法相比，在模拟中成功率提高了+6.3%，在零样本模拟到真实世界迁移中提高了+30.1%，仅两次干预就完成了300米的真实世界任务。

Conclusion: UrbanVerse提供了一个可扩展、逼真且有效的数据驱动的真实到模拟系统，用于训练城市具身AI智能体。它显著提高了模拟训练策略在真实世界中的泛化能力和成功率，为未来城市AI智能体的开发提供了强大的工具。

Abstract: Urban embodied AI agents, ranging from delivery robots to quadrupeds, are
increasingly populating our cities, navigating chaotic streets to provide
last-mile connectivity. Training such agents requires diverse, high-fidelity
urban environments to scale, yet existing human-crafted or procedurally
generated simulation scenes either lack scalability or fail to capture
real-world complexity. We introduce UrbanVerse, a data-driven real-to-sim
system that converts crowd-sourced city-tour videos into physics-aware,
interactive simulation scenes. UrbanVerse consists of: (i) UrbanVerse-100K, a
repository of 100k+ annotated urban 3D assets with semantic and physical
attributes, and (ii) UrbanVerse-Gen, an automatic pipeline that extracts scene
layouts from video and instantiates metric-scale 3D simulations using retrieved
assets. Running in IsaacSim, UrbanVerse offers 160 high-quality constructed
scenes from 24 countries, along with a curated benchmark of 10 artist-designed
test scenes. Experiments show that UrbanVerse scenes preserve real-world
semantics and layouts, achieving human-evaluated realism comparable to manually
crafted scenes. In urban navigation, policies trained in UrbanVerse exhibit
scaling power laws and strong generalization, improving success by +6.3% in
simulation and +30.1% in zero-shot sim-to-real transfer comparing to prior
methods, accomplishing a 300 m real-world mission with only two interventions.

</details>


### [38] [Constantly Improving Image Models Need Constantly Improving Benchmarks](https://arxiv.org/abs/2510.15021)
*Jiaxin Ge,Grace Luo,Heekyung Lee,Nishant Malpani,Long Lian,XuDong Wang,Aleksander Holynski,Trevor Darrell,Sewon Min,David M. Chan*

Main category: cs.CV

TL;DR: 本文提出了ECHO框架，通过分析社交媒体上的真实用户提示和判断，构建了用于评估图像生成模型（如GPT-4o Image Gen）的新基准，以解决现有基准滞后于模型新能力的问题。


<details>
  <summary>Details</summary>
Motivation: 图像生成模型的快速发展引入了现有基准无法捕捉的新功能和使用场景，导致社区对进展的感知与正式评估之间存在差距。

Method: ECHO框架通过收集社交媒体上展示新颖提示和用户质量判断的帖子，直接从真实世界的使用证据中构建基准。作者将此框架应用于GPT-4o Image Gen，构建了一个包含超过31,000个提示的数据集。

Result: ECHO框架能够：1) 发现现有基准中缺失的创意和复杂任务（例如，跨语言重新渲染产品标签或生成指定总额的收据）；2) 更清晰地区分最先进模型与替代方案；3) 揭示社区反馈，用于指导模型质量度量指标的设计（例如，衡量颜色、身份和结构的变化）。

Conclusion: ECHO框架提供了一种更有效的方法来评估快速发展的图像生成模型，通过整合真实世界的用户行为和反馈，克服了传统基准的局限性，从而更好地反映模型的能力和社区的期望。

Abstract: Recent advances in image generation, often driven by proprietary systems like
GPT-4o Image Gen, regularly introduce new capabilities that reshape how users
interact with these models. Existing benchmarks often lag behind and fail to
capture these emerging use cases, leaving a gap between community perceptions
of progress and formal evaluation. To address this, we present ECHO, a
framework for constructing benchmarks directly from real-world evidence of
model use: social media posts that showcase novel prompts and qualitative user
judgments. Applying this framework to GPT-4o Image Gen, we construct a dataset
of over 31,000 prompts curated from such posts. Our analysis shows that ECHO
(1) discovers creative and complex tasks absent from existing benchmarks, such
as re-rendering product labels across languages or generating receipts with
specified totals, (2) more clearly distinguishes state-of-the-art models from
alternatives, and (3) surfaces community feedback that we use to inform the
design of metrics for model quality (e.g., measuring observed shifts in color,
identity, and structure). Our website is at https://echo-bench.github.io.

</details>


### [39] [LoRAverse: A Submodular Framework to Retrieve Diverse Adapters for Diffusion Models](https://arxiv.org/abs/2510.15022)
*Mert Sonmezer,Matthew Zheng,Pinar Yanardag*

Main category: cs.CV

TL;DR: LoRA模型在个性化扩散模型方面表现出色，但由于其数量庞大且缺乏组织，用户在选择和利用上遇到挑战。本文将此问题框架化为组合优化，并提出一种亚模框架来选择最相关和多样化的LoRA模型。


<details>
  <summary>Details</summary>
Motivation: 预训练扩散模型的LoRA适配器数量巨大（超过100K），用户难以有效导航、选择和利用最合适的LoRA模型，因为它们数量庞大、多样且缺乏结构化组织。

Method: 将选择最相关和多样化LoRA模型的问题定义为组合优化问题，并提出一种新颖的亚模（submodular）框架来解决。

Result: 定量和定性实验表明，该方法能够在广泛的领域内生成多样化的输出。

Conclusion: 所提出的亚模框架能够有效地从大量LoRA适配器中选择出相关且多样化的模型，解决了用户在LoRA模型选择上的难题，并能生成多样化的内容。

Abstract: Low-rank Adaptation (LoRA) models have revolutionized the personalization of
pre-trained diffusion models by enabling fine-tuning through low-rank,
factorized weight matrices specifically optimized for attention layers. These
models facilitate the generation of highly customized content across a variety
of objects, individuals, and artistic styles without the need for extensive
retraining. Despite the availability of over 100K LoRA adapters on platforms
like Civit.ai, users often face challenges in navigating, selecting, and
effectively utilizing the most suitable adapters due to their sheer volume,
diversity, and lack of structured organization. This paper addresses the
problem of selecting the most relevant and diverse LoRA models from this vast
database by framing the task as a combinatorial optimization problem and
proposing a novel submodular framework. Our quantitative and qualitative
experiments demonstrate that our method generates diverse outputs across a wide
range of domains.

</details>


### [40] [MOBIUS: Big-to-Mobile Universal Instance Segmentation via Multi-modal Bottleneck Fusion and Calibrated Decoder Pruning](https://arxiv.org/abs/2510.15026)
*Mattia Segu,Marta Tintore Gazulla,Yongqin Xian,Luc Van Gool,Federico Tombari*

Main category: cs.CV

TL;DR: MOBIUS是一个为通用实例分割设计的系列基础模型，它通过创新的架构和训练策略，在不牺牲性能的情况下大幅降低了计算成本，实现了从高端加速器到移动硬件的设备部署优化。


<details>
  <summary>Details</summary>
Motivation: 虽然大规模基础模型在实例级感知方面表现出色，但其高计算成本限制了在资源受限平台上的应用。现有架构在实现高效边缘部署方面存在局限性，需要一种在不影响性能的前提下进行优化的解决方案。

Method: 本文提出MOBIUS模型家族，其核心方法包括：(i) 瓶颈像素解码器，用于高效的多尺度和多模态融合；(ii) 语言引导的不确定性校准损失，用于自适应解码器剪枝；(iii) 流线型、统一的训练策略，以减少训练和推理需求。

Result: MOBIUS在保持最先进性能的同时，将像素和Transformer解码器的浮点运算（FLOPs）分别减少了高达55%和75%，并且仅用三分之一的训练迭代次数即可达到此效果。它为高性能计算平台和移动设备上的高效分割设立了新基准。

Conclusion: MOBIUS通过创新的设计克服了现有基础模型在边缘部署上的计算限制，实现了帕累托最优的性能下行扩展，为跨设备（从高端加速器到移动硬件）的通用实例分割提供了高效且高性能的解决方案。

Abstract: Scaling up model size and training data has advanced foundation models for
instance-level perception, achieving state-of-the-art in-domain and zero-shot
performance across object detection and segmentation. However, their high
computational cost limits adoption on resource-constrained platforms. We first
examine the limitations of existing architectures in enabling efficient edge
deployment without compromising performance. We then introduce MOBIUS, a family
of foundation models for universal instance segmentation, designed for
Pareto-optimal downscaling to support deployment across devices ranging from
high-end accelerators to mobile hardware. To reduce training and inference
demands, we propose: (i) a bottleneck pixel decoder for efficient multi-scale
and multi-modal fusion, (ii) a language-guided uncertainty calibration loss for
adaptive decoder pruning, and (iii) a streamlined, unified training strategy.
Unlike efficient baselines that trade accuracy for reduced complexity, MOBIUS
reduces pixel and transformer decoder FLOPs by up to 55% and 75%, respectively,
while maintaining state-of-the-art performance in just a third of the training
iterations. MOBIUS establishes a new benchmark for efficient segmentation on
both high-performance computing platforms and mobile devices.

</details>


### [41] [Composition-Grounded Instruction Synthesis for Visual Reasoning](https://arxiv.org/abs/2510.15040)
*Xinyi Gu,Jiayuan Mao,Zhang-Wei Hong,Zhuoran Yu,Pengyuan Li,Dhiraj Joshi,Rogerio Feris,Zexue He*

Main category: cs.CV

TL;DR: COGS是一个数据高效的框架，通过将少量种子问题分解和重组为合成问答对，显著提升了多模态大语言模型在图表、网页等数据稀缺的人工图像领域中的推理能力，并展现出良好的泛化性。


<details>
  <summary>Details</summary>
Motivation: 预训练多模态大语言模型（MLLMs）在多模态任务中表现出色，但在难以收集标注数据的领域（如图表、渲染文档、网页等人工图像领域）推理能力有限，缺乏大规模人工标注的推理数据集。

Method: 引入COGS（COmposition-Grounded instruction Synthesis）框架。其核心思想是将每个种子问题分解为原始感知和推理因子，然后与新图像系统地重组，生成大量的合成问答对。每个生成的问题都配有子问题和中间答案，通过因子级别的过程奖励进行强化学习。

Result: 在图表推理实验中，COGS显著提升了模型在未见问题上的表现，尤其在推理密集型和组合型问题上增益最大。此外，使用不同种子数据的因子级混合训练，能更好地跨多个数据集进行迁移，表明COGS诱导了可泛化的能力而非特定数据集的过拟合。该框架还被证明可扩展到网页等其他领域。

Conclusion: COGS框架通过高效的数据合成方法，成功地为多模态大语言模型赋予了在图表和网页等人工图像领域的高级推理能力，并展现出良好的泛化性和跨数据集迁移能力。

Abstract: Pretrained multi-modal large language models (MLLMs) demonstrate strong
performance on diverse multimodal tasks, but remain limited in reasoning
capabilities for domains where annotations are difficult to collect. In this
work, we focus on artificial image domains such as charts, rendered documents,
and webpages, which are abundant in practice yet lack large-scale human
annotated reasoning datasets. We introduce COGS (COmposition-Grounded
instruction Synthesis), a data-efficient framework for equipping MLLMs with
advanced reasoning abilities from a small set of seed questions. The key idea
is to decompose each seed question into primitive perception and reasoning
factors, which can then be systematically recomposed with new images to
generate large collections of synthetic question-answer pairs. Each generated
question is paired with subquestions and intermediate answers, enabling
reinforcement learning with factor-level process rewards. Experiments on chart
reasoning show that COGS substantially improves performance on unseen
questions, with the largest gains on reasoning-heavy and compositional
questions. Moreover, training with a factor-level mixture of different seed
data yields better transfer across multiple datasets, suggesting that COGS
induces generalizable capabilities rather than dataset-specific overfitting. We
further demonstrate that the framework extends beyond charts to other domains
such as webpages.

</details>


### [42] [Generalized Dynamics Generation towards Scannable Physical World Model](https://arxiv.org/abs/2510.15041)
*Yichen Li,Zhiyi Li,Brandon Feng,Dinghuai Zhang,Antonio Torralba*

Main category: cs.CV

TL;DR: GDGen是一个统一的、与几何无关的框架，它从势能角度整合了刚体、关节体和软体动力学，通过引入方向刚度扩展经典弹性力学，并利用神经网络和神经场建模物理属性和变形，为构建交互式虚拟环境和训练具身智能体提供了通用基础。


<details>
  <summary>Details</summary>
Motivation: 数字孪生世界为在可扫描环境中开发具有复杂物理行为的通用具身智能体提供了新机遇。现有的物理模拟系统通常难以统一处理刚体、关节体和软体等多种动力学，限制了其在复杂交互场景中的应用。

Method: GDGen框架采用势能视角，将世界视为一个整体，通过简单的运动观察推断物理属性。它通过引入方向刚度扩展了经典弹性力学，以捕捉从软弹性到关节和刚体系统的广泛物理行为。该方法使用专门的网络来建模扩展的材料属性，并利用神经场以与几何无关的方式表示变形。

Result: GDGen能够稳健地统一不同的模拟范式，包括刚体、关节体和软体动力学。实验证明，它为创建交互式虚拟环境和在复杂、动态丰富的场景中训练机器人智能体提供了多功能基础。

Conclusion: GDGen通过其基于势能的统一方法和对弹性力学的创新扩展，成功地将多种物理动力学集成到一个与几何无关的系统中。这为开发更通用、更逼真的数字孪生环境和具身智能体训练平台奠定了坚实基础。

Abstract: Digital twin worlds with realistic interactive dynamics presents a new
opportunity to develop generalist embodied agents in scannable environments
with complex physical behaviors. To this end, we present GDGen (Generalized
Representation for Generalized Dynamics Generation), a framework that takes a
potential energy perspective to seamlessly integrate rigid body, articulated
body, and soft body dynamics into a unified, geometry-agnostic system. GDGen
operates from the governing principle that the potential energy for any stable
physical system should be low. This fresh perspective allows us to treat the
world as one holistic entity and infer underlying physical properties from
simple motion observations. We extend classic elastodynamics by introducing
directional stiffness to capture a broad spectrum of physical behaviors,
covering soft elastic, articulated, and rigid body systems. We propose a
specialized network to model the extended material property and employ a neural
field to represent deformation in a geometry-agnostic manner. Extensive
experiments demonstrate that GDGen robustly unifies diverse simulation
paradigms, offering a versatile foundation for creating interactive virtual
environments and training robotic agents in complex, dynamically rich
scenarios.

</details>


### [43] [Comprehensive language-image pre-training for 3D medical image understanding](https://arxiv.org/abs/2510.15042)
*Tassilo Wald,Ibrahim Ethem Hamamci,Yuan Gao,Sam Bond-Taylor,Harshita Sharma,Maximilian Ilse,Cynthia Lo,Olesya Melnichenko,Noel C. F. Codella,Maria Teodora Wetscherek,Klaus H. Maier-Hein,Panagiotis Korfiatis,Valentina Salvatelli,Javier Alvarez-Valle,Fernando Pérez-García*

Main category: cs.CV

TL;DR: 针对3D医学图像领域数据稀缺问题，本文提出了COLIPRI编码器家族，通过引入报告生成目标和结合纯视觉预训练与视觉-语言预训练，有效利用更多数据，并在报告生成、分类和零样本分类等任务上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 在3D医学图像领域，视觉-语言预训练编码器（VLEs）在支持放射科医生方面潜力巨大（如检索相似异常患者或预测异常可能性），但其能力受限于现有数据的稀缺性。

Method: 本文通过注入额外的归纳偏置来缓解数据不足：1) 引入报告生成目标；2) 将视觉-语言预训练与纯视觉预训练相结合。这使得模型能够同时利用纯图像数据集和配对的图像-文本3D数据集，从而增加模型接触到的数据总量。结合3D医学成像领域的最佳实践，开发了COLIPRI编码器家族。

Result: COLIPRI编码器在报告生成、分类探测和零样本分类方面取得了最先进的性能，并在语义分割方面保持了竞争力。

Conclusion: 通过引入报告生成目标和结合纯视觉与视觉-语言预训练，COLIPRI编码器家族成功缓解了3D医学图像领域的数据稀缺问题，并在多项关键任务上实现了显著的性能提升，推动了该领域的发展。

Abstract: Vision-language pre-training, i.e., aligning images with paired text, is a
powerful paradigm to create encoders that can be directly used for tasks such
as classification and retrieval, and for downstream tasks such as segmentation
and report generation. In the 3D medical image domain, these capabilities allow
vision-language encoders (VLEs) to support radiologists by retrieving patients
with similar abnormalities or predicting likelihoods of abnormality. While the
methodology holds promise, data availability limits the capabilities of current
3D VLEs.
  In this paper, we alleviate the lack of data by injecting additional
inductive biases: introducing a report generation objective and pairing
vision-language pre-training with vision-only pre-training. This allows us to
leverage both image-only and paired image-text 3D datasets, increasing the
total amount of data to which our model is exposed. Through these additional
inductive biases, paired with best practices of the 3D medical imaging domain,
we develop the Comprehensive Language-image Pre-training (COLIPRI) encoder
family. Our COLIPRI encoders achieve state-of-the-art performance in report
generation, classification probing, and zero-shot classification, and remain
competitive for semantic segmentation.

</details>


### [44] [Directional Reasoning Injection for Fine-Tuning MLLMs](https://arxiv.org/abs/2510.15050)
*Chao Huang,Zeliang Zhang,Jiang Liu,Ximeng Sun,Jialian Wu,Xiaodong Yu,Ze Wang,Chenliang Xu,Emad Barsoum,Zicheng Liu*

Main category: cs.CV

TL;DR: 多模态大语言模型（MLLMs）的推理能力落后于纯文本模型，现有方法资源密集。本文提出DRIFT，一种轻量级方法，通过在梯度空间注入推理知识来提升MLLMs的推理能力，效果优于朴素合并和监督微调，且成本更低。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）的推理能力不如强大的纯文本模型。现有弥补这一差距的方法（如大规模监督微调、强化学习）资源消耗大。模型合并是一种有前景的替代方案，但分析表明其效果不稳定，在不同模型家族中表现差异大，有时甚至导致性能下降。

Method: 本文提出“定向推理注入微调”（DRIFT）MLLMs方法。DRIFT首先预计算一个“推理先验”，即推理增强型LLM与多模态变体之间的参数空间差异。然后，在多模态微调过程中，利用这个推理先验来偏置梯度，从而在不破坏多模态对齐的前提下，将推理知识从梯度空间转移到MLLM中。该方法保持了标准监督微调流程的简洁性。

Result: 在MathVista和MathVerse等多模态推理基准上的大量实验表明，DRIFT始终优于朴素合并和监督微调，并在成本极低的情况下，达到或超越了训练密集型方法的性能，显著提升了MLLMs的推理能力。

Conclusion: DRIFT是一种高效、轻量级的方法，通过在梯度空间注入推理知识，有效提升了多模态大语言模型的推理性能，且成本远低于传统训练密集型方法，为弥补MLLMs与纯文本LLMs之间的推理差距提供了一个有前景的解决方案。

Abstract: Multimodal large language models (MLLMs) are rapidly advancing, yet their
reasoning ability often lags behind that of strong text-only counterparts.
Existing methods to bridge this gap rely on supervised fine-tuning over
large-scale multimodal reasoning data or reinforcement learning, both of which
are resource-intensive. A promising alternative is model merging, which
interpolates parameters between reasoning-enhanced LLMs and multimodal
variants. However, our analysis shows that naive merging is not always a "free
lunch": its effectiveness varies drastically across model families, with some
(e.g., LLaVA, Idefics) benefiting while others (e.g., Qwen) suffer performance
degradation. To address this, we propose Directional Reasoning Injection for
Fine-Tuning (DRIFT) MLLMs, a lightweight method that transfers reasoning
knowledge in the gradient space, without destabilizing multimodal alignment.
DRIFT precomputes a reasoning prior as the parameter-space difference between
reasoning and multimodal variants, then uses it to bias gradients during
multimodal fine-tuning. This approach preserves the simplicity of standard
supervised fine-tuning pipelines while enabling efficient reasoning transfer.
Extensive experiments on multimodal reasoning benchmarks, including MathVista
and MathVerse, demonstrate that DRIFT consistently improves reasoning
performance over naive merging and supervised fine-tuning, while matching or
surpassing training-heavy methods at a fraction of the cost.

</details>


### [45] [A solution to generalized learning from small training sets found in everyday infant experiences](https://arxiv.org/abs/2510.15060)
*Frangil Ramirez,Elizabeth Clerkin,David J. Crandall,Linda B. Smith*

Main category: cs.CV

TL;DR: 本研究提出婴儿从有限经验中学习和泛化物体类别的能力，可能源于其日常视觉输入中“块状相似性结构”的特性，即高度相似图像的集群与稀疏多样图像的混合。


<details>
  <summary>Details</summary>
Motivation: 幼儿能轻易识别并泛化常见名词所标记的视觉物体，但这如何从有限经验中产生尚不清楚。通常，强大且多样的学习需要大量数据，而婴儿却能从有限经验中实现泛化，这种看似矛盾的现象是研究的动力。

Method: 分析了14名7至11个月大婴儿的第一视角图像数据，以识别其日常视觉输入中是否存在“块状相似性结构”，涵盖了八个早期学习的类别。此外，进行了计算实验，在机器中模拟这种结构，以测试其对小数据集泛化的影响。

Result: 婴儿的日常视觉输入呈现出“块状相似性结构”，即高度相似的图像集群与稀疏、更多变的图像交织出现。计算实验表明，在机器学习中模仿这种结构可以提高从小数据集中的泛化能力。

Conclusion: 婴儿经验中固有的“块状性”可能支持了其早期的类别学习和泛化能力。这项发现也为各种问题和学习者提供了高效学习的普遍原则。

Abstract: Young children readily recognize and generalize visual objects labeled by
common nouns, suggesting that these basic level object categories may be given.
Yet if they are, how they arise remains unclear. We propose that the answer
lies in the statistics of infant daily life visual experiences. Whereas large
and diverse datasets typically support robust learning and generalization in
human and machine learning, infants achieve this generalization from limited
experiences. We suggest that the resolution of this apparent contradiction lies
in the visual diversity of daily life, repeated experiences with single object
instances. Analyzing egocentric images from 14 infants (aged 7 to 11 months) we
show that their everyday visual input exhibits a lumpy similarity structure,
with clusters of highly similar images interspersed with rarer, more variable
ones, across eight early-learned categories. Computational experiments show
that mimicking this structure in machines improves generalization from small
datasets in machine learning. The natural lumpiness of infant experience may
thus support early category learning and generalization and, more broadly,
offer principles for efficient learning across a variety of problems and kinds
of learners.

</details>


### [46] [TGT: Text-Grounded Trajectories for Locally Controlled Video Generation](https://arxiv.org/abs/2510.15104)
*Guofeng Zhang,Angtian Wang,Jacob Zhiyuan Fang,Liming Jiang,Haotian Yang,Bo Liu,Yiding Yang,Guang Chen,Longyin Wen,Alan Yuille,Chongyang Ma*

Main category: cs.CV

TL;DR: 本文提出Text-Grounded Trajectories (TGT) 框架，通过结合带有局部文本描述的轨迹来控制文本到视频的生成，有效提升了多对象场景的视觉质量、文本对齐和运动可控性。


<details>
  <summary>Details</summary>
Motivation: 现有文本到视频生成方法在视觉保真度上进步显著，但在生成场景的主体构成控制方面能力有限。先前的局部文本控制（如边界框或分割掩码）在复杂场景和多对象设置中表现不佳，精度有限，且随着可控对象数量增加，个体轨迹与视觉实体之间的对应关系不明确。

Method: 本文引入Text-Grounded Trajectories (TGT) 框架，将视频生成条件设定为与局部文本描述配对的轨迹。提出Location-Aware Cross-Attention (LACA) 以整合这些信号，并采用双CFG方案分别调制局部和全局文本引导。此外，开发了一个数据处理管道，生成带有追踪实体局部描述的轨迹，并标注了200万高质量视频片段用于训练TGT。

Result: TGT与现有方法相比，实现了更高的视觉质量、更准确的文本对齐以及更强的运动可控性。

Conclusion: TGT框架通过将点轨迹与文本配对，作为直观的运动控制柄，能够同时控制生成视频中实体的外观和运动，有效解决了现有方法在复杂多对象场景中控制精度和对应关系不足的问题。

Abstract: Text-to-video generation has advanced rapidly in visual fidelity, whereas
standard methods still have limited ability to control the subject composition
of generated scenes. Prior work shows that adding localized text control
signals, such as bounding boxes or segmentation masks, can help. However, these
methods struggle in complex scenarios and degrade in multi-object settings,
offering limited precision and lacking a clear correspondence between
individual trajectories and visual entities as the number of controllable
objects increases. We introduce Text-Grounded Trajectories (TGT), a framework
that conditions video generation on trajectories paired with localized text
descriptions. We propose Location-Aware Cross-Attention (LACA) to integrate
these signals and adopt a dual-CFG scheme to separately modulate local and
global text guidance. In addition, we develop a data processing pipeline that
produces trajectories with localized descriptions of tracked entities, and we
annotate two million high quality video clips to train TGT. Together, these
components enable TGT to use point trajectories as intuitive motion handles,
pairing each trajectory with text to control both appearance and motion.
Extensive experiments show that TGT achieves higher visual quality, more
accurate text alignment, and improved motion controllability compared with
prior approaches. Website: https://textgroundedtraj.github.io.

</details>


### [47] [SaLon3R: Structure-aware Long-term Generalizable 3D Reconstruction from Unposed Images](https://arxiv.org/abs/2510.15072)
*Jiaxin Guo,Tongfan Guan,Wenzhen Dong,Wenzhao Zheng,Wenting Wang,Yue Wang,Yeung Yam,Yun-Hui Liu*

Main category: cs.CV

TL;DR: SaLon3R是一种新型的结构感知、长期3D高斯泼溅（3DGS）重建框架，通过引入紧凑锚点基元和3D点变换器，有效解决了长视频序列中现有3DGS方法的冗余和几何不一致问题，实现了高效、鲁棒且可泛化的重建。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯泼溅方法在处理长持续时间视频序列的顺序输入视图时，通常预测逐像素高斯并结合所有视图的高斯作为场景表示，这导致了大量的冗余和几何不一致性。

Method: 本文提出了SaLon3R框架，它引入了紧凑锚点基元，通过可微分的显著性感知高斯量化来消除冗余。结合一个3D点变换器，该变换器用于细化锚点属性和显著性，以解决跨帧的几何和光度不一致性。具体地，首先利用3D重建骨干网络预测密集的逐像素高斯和编码区域几何复杂度的显著性图。然后，通过优先处理高复杂度区域，将冗余高斯压缩成紧凑锚点。最后，3D点变换器从训练数据中学习3D空间中的空间结构先验，以细化锚点属性和显著性，从而实现区域自适应高斯解码，保证几何保真度。整个过程无需已知相机参数或测试时优化，单次前向传播即可解决伪影并修剪冗余3DGS。

Result: SaLon3R是首个能够以超过10 FPS的速度重建超过50个视图的在线可泛化GS方法，并能去除50%至90%的冗余。在多个数据集上的实验表明，该方法在新颖视图合成和深度估计方面均达到了最先进的性能，展示了在长期可泛化3D重建方面卓越的效率、鲁棒性和泛化能力。

Conclusion: SaLon3R通过创新的锚点基元和3D点变换器，成功解决了长持续时间视频序列中3DGS的冗余和几何不一致问题，实现了高效、鲁棒且具有良好泛化能力的长期3D重建，并在新颖视图合成和深度估计方面取得了领先成果。

Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have enabled generalizable,
on-the-fly reconstruction of sequential input views. However, existing methods
often predict per-pixel Gaussians and combine Gaussians from all views as the
scene representation, leading to substantial redundancies and geometric
inconsistencies in long-duration video sequences. To address this, we propose
SaLon3R, a novel framework for Structure-aware, Long-term 3DGS Reconstruction.
To our best knowledge, SaLon3R is the first online generalizable GS method
capable of reconstructing over 50 views in over 10 FPS, with 50% to 90%
redundancy removal. Our method introduces compact anchor primitives to
eliminate redundancy through differentiable saliency-aware Gaussian
quantization, coupled with a 3D Point Transformer that refines anchor
attributes and saliency to resolve cross-frame geometric and photometric
inconsistencies. Specifically, we first leverage a 3D reconstruction backbone
to predict dense per-pixel Gaussians and a saliency map encoding regional
geometric complexity. Redundant Gaussians are compressed into compact anchors
by prioritizing high-complexity regions. The 3D Point Transformer then learns
spatial structural priors in 3D space from training data to refine anchor
attributes and saliency, enabling regionally adaptive Gaussian decoding for
geometric fidelity. Without known camera parameters or test-time optimization,
our approach effectively resolves artifacts and prunes the redundant 3DGS in a
single feed-forward pass. Experiments on multiple datasets demonstrate our
state-of-the-art performance on both novel view synthesis and depth estimation,
demonstrating superior efficiency, robustness, and generalization ability for
long-term generalizable 3D reconstruction. Project Page:
https://wrld.github.io/SaLon3R/.

</details>


### [48] [Deep generative priors for 3D brain analysis](https://arxiv.org/abs/2510.15119)
*Ana Lawry Aguila,Dina Zemlyanker,You Cheng,Sudeshna Das,Daniel C. Alexander,Oula Puonti,Annabel Sorby-Adams,W. Taylor Kimberly,Juan Eugenio Iglesias*

Main category: cs.CV

TL;DR: 本文提出将扩散模型作为先验，用于解决医学图像逆问题，特别是脑部MRI分析，通过结合数据驱动模型与领域知识，克服了传统贝叶斯方法在捕捉复杂解剖结构方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型在医学成像中表现出色，但如何将其与领域知识结合以指导脑成像问题仍是挑战。传统的贝叶斯逆问题框架虽然能利用领域知识，但其数学先验难以捕捉复杂的脑部解剖结构，而现有数据驱动模型往往需要大量训练数据。

Method: 本文提出一种通用方法，将评分（score-based）扩散模型作为先验，用于解决广泛的医学图像逆问题。该扩散先验在多样化的脑部MRI数据上进行训练，并与灵活的前向模型（forward models）结合，以处理超分辨率、偏置场校正、图像修复等常见图像处理任务。此外，该框架还能优化现有深度学习方法的输出，以提高解剖保真度。

Result: 在异构临床和研究MRI数据上的实验表明，本文方法在无需配对训练数据集的情况下，实现了最先进的性能，能够产生一致且高质量的解决方案。这证明了扩散先验在脑部MRI分析中的潜力。

Conclusion: 扩散先验是脑部MRI分析中一种多功能且强大的工具，能够有效结合数据驱动模型和领域知识，解决医学图像逆问题，并提升解剖保真度。

Abstract: Diffusion models have recently emerged as powerful generative models in
medical imaging. However, it remains a major challenge to combine these
data-driven models with domain knowledge to guide brain imaging problems. In
neuroimaging, Bayesian inverse problems have long provided a successful
framework for inference tasks, where incorporating domain knowledge of the
imaging process enables robust performance without requiring extensive training
data. However, the anatomical modeling component of these approaches typically
relies on classical mathematical priors that often fail to capture the complex
structure of brain anatomy. In this work, we present the first general-purpose
application of diffusion models as priors for solving a wide range of medical
imaging inverse problems. Our approach leverages a score-based diffusion prior
trained extensively on diverse brain MRI data, paired with flexible forward
models that capture common image processing tasks such as super-resolution,
bias field correction, inpainting, and combinations thereof. We further
demonstrate how our framework can refine outputs from existing deep learning
methods to improve anatomical fidelity. Experiments on heterogeneous clinical
and research MRI data show that our method achieves state-of-the-art
performance producing consistent, high-quality solutions without requiring
paired training datasets. These results highlight the potential of diffusion
priors as versatile tools for brain MRI analysis.

</details>


### [49] [Fourier Transform Multiple Instance Learning for Whole Slide Image Classification](https://arxiv.org/abs/2510.15138)
*Anthony Bilic,Guangyu Sun,Ming Li,Md Sanzid Bin Hossain,Yu Tian,Wei Zhang,Laura Brattain,Dexter Hadley,Chen Chen*

Main category: cs.CV

TL;DR: FFT-MIL是一种新的多实例学习（MIL）框架，通过引入频域分支来捕获全玻片图像（WSI）的全局上下文，有效提升了WSI分类的性能和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的WSI分类MIL方法依赖于空间补丁特征，但由于WSI尺寸巨大和补丁嵌入的局部性，难以捕捉全局依赖性，这阻碍了对诊断预测至关重要的粗略结构的建模。

Method: 本文提出了傅里叶变换多实例学习（FFT-MIL）。它通过快速傅里叶变换（FFT）从WSI中提取低频裁剪图像，并通过包含卷积层和Min-Max归一化的模块化FFT-Block进行处理，以减轻频域数据的高方差。学习到的全局频域特征通过轻量级集成策略与空间补丁特征融合，使其与多种MIL架构兼容。

Result: FFT-MIL在三个公共数据集（BRACS、LUAD和IMP）上，与六种最先进的MIL方法结合进行评估。结果显示，FFT-Block的集成平均将宏F1分数提高了3.51%，AUC提高了1.51%，在不同架构和数据集上都表现出一致的性能提升。

Conclusion: 这些结果表明，频域学习是捕获WSI分类中全局依赖性的一种有效且高效的机制，它补充了空间特征，并提高了基于MIL的计算病理学的可扩展性和准确性。

Abstract: Whole Slide Image (WSI) classification relies on Multiple Instance Learning
(MIL) with spatial patch features, yet existing methods struggle to capture
global dependencies due to the immense size of WSIs and the local nature of
patch embeddings. This limitation hinders the modeling of coarse structures
essential for robust diagnostic prediction.
  We propose Fourier Transform Multiple Instance Learning (FFT-MIL), a
framework that augments MIL with a frequency-domain branch to provide compact
global context. Low-frequency crops are extracted from WSIs via the Fast
Fourier Transform and processed through a modular FFT-Block composed of
convolutional layers and Min-Max normalization to mitigate the high variance of
frequency data. The learned global frequency feature is fused with spatial
patch features through lightweight integration strategies, enabling
compatibility with diverse MIL architectures.
  FFT-MIL was evaluated across six state-of-the-art MIL methods on three public
datasets (BRACS, LUAD, and IMP). Integration of the FFT-Block improved macro F1
scores by an average of 3.51% and AUC by 1.51%, demonstrating consistent gains
across architectures and datasets. These results establish frequency-domain
learning as an effective and efficient mechanism for capturing global
dependencies in WSI classification, complementing spatial features and
advancing the scalability and accuracy of MIL-based computational pathology.

</details>


### [50] [XModBench: Benchmarking Cross-Modal Capabilities and Consistency in Omni-Language Models](https://arxiv.org/abs/2510.15148)
*Xingrui Wang,Jiang Liu,Chao Huang,Xiaodong Yu,Ze Wang,Ximeng Sun,Jialian Wu,Alan Yuille,Emad Barsoum,Zicheng Liu*

Main category: cs.CV

TL;DR: 本文介绍了XModBench，一个大规模三模态基准测试，旨在诊断全模态大型语言模型（OLLMs）的跨模态一致性。实验表明，即使是强大的模型也存在空间和时间推理困难、持续的模态差异以及系统性的方向不平衡，这表明当前OLLMs远未实现真正的模态不变推理。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要评估一般的跨模态问答能力，但尚不清楚OLLMs是否实现了模态不变推理，或者存在模态特异性偏差。因此，需要一个专门的基准来衡量跨模态一致性。

Method: 研究引入了XModBench，一个包含60,828个多项选择题的大规模三模态基准，涵盖五个任务家族。它系统地覆盖了问题-答案对中的所有六种模态组合，从而能够细致地诊断OLLM的模态不变推理、模态差异和方向不平衡。

Result: 实验结果显示，即使是强大的模型（如Gemini 2.5 Pro）：(i) 在空间和时间推理方面表现不佳，准确率低于60%；(ii) 存在持续的模态差异，当相同语义内容通过音频而非文本传达时，性能显著下降；(iii) 表现出系统性的方向不平衡，当视觉作为上下文时，一致性低于文本作为上下文。

Conclusion: 这些发现表明，当前的OLLMs远未实现真正的模态不变推理。XModBench被定位为一个评估和改进跨模态能力的基础诊断工具。

Abstract: Omni-modal large language models (OLLMs) aim to unify audio, vision, and text
understanding within a single framework. While existing benchmarks primarily
evaluate general cross-modal question-answering ability, it remains unclear
whether OLLMs achieve modality-invariant reasoning or exhibit modality-specific
biases. We introduce XModBench, a large-scale tri-modal benchmark explicitly
designed to measure cross-modal consistency. XModBench comprises 60,828
multiple-choice questions spanning five task families and systematically covers
all six modality compositions in question-answer pairs, enabling fine-grained
diagnosis of an OLLM's modality-invariant reasoning, modality disparity, and
directional imbalance. Experiments show that even the strongest model, Gemini
2.5 Pro, (i) struggles with spatial and temporal reasoning, achieving less than
60% accuracy, (ii) reveals persistent modality disparities, with performance
dropping substantially when the same semantic content is conveyed through audio
rather than text, and (iii) shows systematic directional imbalance, exhibiting
lower consistency when vision serves as context compared to text. These
findings indicate that current OLLMs remain far from truly modality-invariant
reasoning and position XModBench as a fundamental diagnostic tool for
evaluating and improving cross-modal competence. All data and evaluation tools
will be available at https://xingruiwang.github.io/projects/XModBench/.

</details>


### [51] [Train a Unified Multimodal Data Quality Classifier with Synthetic Data](https://arxiv.org/abs/2510.15162)
*Weizhi Wang,Rongmei Lin,Shiyang Li,Colin Lockard,Ritesh Sarkhel,Sanket Lokegaonkar,Jingbo Shang,Xifeng Yan,Nasser Zalmout,Xian Li*

Main category: cs.CV

TL;DR: 本文提出UniFilter，一个统一的多模态数据质量分类器，用于过滤高质量的图像-文本字幕数据和交错文档数据。通过半合成数据生成方法训练UniFilter，并证明使用其过滤后的数据预训练的多模态大语言模型（MLLMs）在零样本推理和上下文学习方面表现显著增强，并在下游任务中取得更优性能。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在图像-文本字幕数据和交错文档数据上进行预训练，但针对图像-文本交错文档数据的高质量数据过滤方法尚未得到充分探索。

Method: 本文提出UniFilter，一个高效的MLLM，被训练为一个统一的多模态数据质量分类器。为解决多模态标注数据收集的挑战，引入了一种半合成方法，利用现有原始图像生成四种质量级别的对应文本，从而高效创建用于训练UniFilter的字幕和交错文档数据的样本-分数对。UniFilter被应用于过滤DataComp字幕数据集和OBELICS图像-文本交错数据集。

Result: 在UniFilter过滤数据上预训练的MLLMs，相比在基线过滤数据上训练的模型，显著增强了零样本推理和上下文学习能力。经过视觉监督微调后，这些由UniFilter引导的MLLMs在各种基准测试中表现出更强的性能。研究团队还发布了用于训练UniFilter的合成训练数据、UniFilter模型检查点以及由UniFilter筛选的高质量交错文档子集OBELICS-HQ。

Conclusion: 通过UniFilter策展的高质量多模态预训练数据，能够显著提升MLLMs的预训练效果，从而带来更强的零样本推理、上下文学习和下游任务性能，突显了高质量多模态预训练的益处。

Abstract: The Multimodal Large Language Models (MLLMs) are continually pre-trained on a
mixture of image-text caption data and interleaved document data, while the
high-quality data filtering towards image-text interleaved document data is
under-explored. We propose to train an efficient MLLM as a Unified Mulitmodal
Data Quality Classifier to Filter both high-quality image-text caption and
interleaved data (UniFilter). To address the challenge of collecting diverse
labeled multimodal data, we introduce a semi-synthetic approach that leverages
readily available raw images and generates corresponding text across four
quality levels. This method enables efficient creation of sample-score pairs
for both caption and interleaved document data to train UniFilter. We apply
UniFilter to curate high-quality caption data from DataComp caption dataset and
interleaved data from the OBELICS image-text interleaved dataset. MLLMs
pre-trained on the filtered data demonstrate significantly enhanced
capabilities compared to those trained on baseline-filtered data, achieving
stronger zero-shot reasoning and in-context learning capabilities. After visual
supervised fine-tuning, these UniFilter-induced MLLMs achieve stronger
performance on various benchmarks, highlighting the downstream benefits of
high-quality multimodal pre-training. We release the synthetic training data
used for training UniFilter, the UniFilter model checkpoints, and the
high-quality interleaved document subset OBELICS-HQ, curated by UniFilter, to
the community for reproduction and further development.

</details>


### [52] [Salient Concept-Aware Generative Data Augmentation](https://arxiv.org/abs/2510.15194)
*Tianchen Zhao,Xuanbai Chen,Zhihua Li,Jun Fang,Dongsheng An,Xiang Xu,Zhuowen Tu,Yifan Xing*

Main category: cs.CV

TL;DR: 本文提出了一种个性化图像生成框架，通过使用显著概念感知的图像嵌入模型，在生成式数据增强中减少不相关视觉细节的影响，从而在保持图像关键特征的同时，提升生成图像的多样性，有效提高下游模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的生成式数据增强方法在平衡图像保真度和多样性方面面临挑战。这是因为在合成过程中，图像表示往往与非必要输入属性（如环境上下文）纠缠，与旨在修改这些元素的文本提示产生冲突，难以在保留图像基本细节的同时与多样化的文本提示对齐。

Method: 本文提出一个个性化图像生成框架。该框架使用一个显著概念感知的图像嵌入模型，以减少合成过程中不相关视觉细节的影响，从而在图像和文本输入之间保持直观的一致性。

Result: 该方法在八个细粒度视觉数据集上表现出卓越性能，在常规和长尾设置下，分类准确率分别平均提高了0.73%和6.5%，优于现有的最先进数据增强方法。

Conclusion: 该框架通过更好地保留类别判别特征并增加受控变化，有效增强了训练数据集的多样性，从而提高了下游模型的鲁棒性。

Abstract: Recent generative data augmentation methods conditioned on both image and
text prompts struggle to balance between fidelity and diversity, as it is
challenging to preserve essential image details while aligning with varied text
prompts. This challenge arises because representations in the synthesis process
often become entangled with non-essential input image attributes such as
environmental contexts, creating conflicts with text prompts intended to modify
these elements. To address this, we propose a personalized image generation
framework that uses a salient concept-aware image embedding model to reduce the
influence of irrelevant visual details during the synthesis process, thereby
maintaining intuitive alignment between image and text inputs. By generating
images that better preserve class-discriminative features with additional
controlled variations, our framework effectively enhances the diversity of
training datasets and thereby improves the robustness of downstream models. Our
approach demonstrates superior performance across eight fine-grained vision
datasets, outperforming state-of-the-art augmentation methods with averaged
classification accuracy improvements by 0.73% and 6.5% under conventional and
long-tail settings, respectively.

</details>


### [53] [Hyperparameter Optimization and Reproducibility in Deep Learning Model Training](https://arxiv.org/abs/2510.15164)
*Usman Afzaal,Ziyu Su,Usama Sajjad,Hao Lu,Mostafa Rezapour,Metin Nafi Gurcan,Muhammad Khalid Khan Niazi*

Main category: cs.CV

TL;DR: 本研究调查了组织病理学基础模型训练中可重复性面临的挑战，通过系统评估超参数和数据增强策略，识别出影响模型性能和稳定性的关键配置，并提出了提高可重复性的实用规则。


<details>
  <summary>Details</summary>
Motivation: 组织病理学基础模型训练中的可重复性是一个关键挑战，常受软件随机性、硬件非确定性及超参数报告不一致等因素阻碍。

Method: 在QUILT-1M数据集上训练了一个CLIP模型，并在三个下游组织病理学数据集（PatchCamelyon, LC25000-Lung, LC25000-Colon）上系统评估了不同超参数设置和数据增强策略的影响。

Result: 尽管运行存在变异性，但发现了明确的趋势：RandomResizedCrop值为0.7-0.8时表现优于0.6或0.9；无局部损失的分布式训练提高了稳定性；学习率低于5.0e-5会持续降低性能。LC25000 (Colon)数据集提供了最可重复的基准。

Conclusion: 计算病理学中的可重复性不仅取决于透明的文档，还取决于精心选择的实验配置。本研究为未来开发可重复的数字病理学基础模型提供了实用规则。

Abstract: Reproducibility remains a critical challenge in foundation model training for
histopathology, often hindered by software randomness, hardware
non-determinism, and inconsistent hyperparameter reporting. To investigate
these issues, we trained a CLIP model on the QUILT-1M dataset and
systematically evaluated the impact of different hyperparameter settings and
augmentation strategies across three downstream histopathology datasets
(PatchCamelyon, LC25000-Lung, and LC25000-Colon). Despite variability across
runs, we identified clear trends: RandomResizedCrop values of 0.7-0.8
outperformed more aggressive (0.6) or conservative (0.9) settings, distributed
training without local loss improved stability, and learning rates below 5.0e-5
consistently degraded performance across all datasets. The LC25000 (Colon)
dataset consistently provided the most reproducible benchmark. These findings
highlight that reproducibility in computational pathology depends not only on
transparent documentation but also on carefully chosen experimental
configurations, and we provide practical rules to guide future efforts in
developing reproducible foundation models for digital pathology.

</details>


### [54] [CARDIUM: Congenital Anomaly Recognition with Diagnostic Images and Unified Medical records](https://arxiv.org/abs/2510.15208)
*Daniela Vega,Hannah V. Ceballos,Javier S. Vera,Santiago Rodriguez,Alejandra Perez,Angela Castillo,Maria Escobar,Dario Londoño,Luis A. Sarmiento,Camila I. Castro,Nadiezhda Rodriguez,Juan C. Briceño,Pablo Arbeláez*

Main category: cs.CV

TL;DR: 该研究发布了首个公开的多模态数据集CARDIUM，用于产前先天性心脏病（CHD）检测，并提出了一种多模态Transformer架构，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 产前CHD诊断的AI解决方案面临挑战：罕见病症导致高质量诊断数据难以收集，数据集不平衡且质量低，阻碍模型性能；缺乏将影像和临床数据等多源信息整合的公共努力，限制了AI模型支持临床决策的能力。

Method: 引入了CARDIUM数据集，这是首个公开的多模态数据集，整合了胎儿超声和超声心动图图像以及母体临床记录，用于产前CHD检测。此外，提出了一种鲁棒的多模态Transformer架构，该架构包含交叉注意力机制，用于融合图像和表格数据的特征表示。

Result: 所提出的方法在CHD检测方面，相比单一模态（仅图像或仅表格）方法分别提高了11%和50%，并在CARDIUM数据集上实现了79.8% ± 4.8%的F1分数。数据集和代码已公开。

Conclusion: CARDIUM数据集和提出的多模态Transformer架构为产前CHD检测提供了一个有力的工具，并有望鼓励在该未探索领域进行更多研究，推动AI在临床决策支持中的应用。

Abstract: Prenatal diagnosis of Congenital Heart Diseases (CHDs) holds great potential
for Artificial Intelligence (AI)-driven solutions. However, collecting
high-quality diagnostic data remains difficult due to the rarity of these
conditions, resulting in imbalanced and low-quality datasets that hinder model
performance. Moreover, no public efforts have been made to integrate multiple
sources of information, such as imaging and clinical data, further limiting the
ability of AI models to support and enhance clinical decision-making. To
overcome these challenges, we introduce the Congenital Anomaly Recognition with
Diagnostic Images and Unified Medical records (CARDIUM) dataset, the first
publicly available multimodal dataset consolidating fetal ultrasound and
echocardiographic images along with maternal clinical records for prenatal CHD
detection. Furthermore, we propose a robust multimodal transformer architecture
that incorporates a cross-attention mechanism to fuse feature representations
from image and tabular data, improving CHD detection by 11% and 50% over image
and tabular single-modality approaches, respectively, and achieving an F1 score
of 79.8 $\pm$ 4.8% in the CARDIUM dataset. We will publicly release our dataset
and code to encourage further research on this unexplored field. Our dataset
and code are available at https://github.com/BCVUniandes/Cardium, and at the
project website https://bcv-uniandes.github.io/CardiumPage/

</details>


### [55] [The Face of Persuasion: Analyzing Bias and Generating Culture-Aware Ads](https://arxiv.org/abs/2510.15240)
*Aysan Aghazadeh,Adriana Kovashka*

Main category: cs.CV

TL;DR: 该研究调查了文本到图像模型在广告中的应用，重点分析了生成广告中的人口统计学偏见、不同性别/种族形象对广告说服力的影响，以及针对特定国家进行广告定位的技术。


<details>
  <summary>Details</summary>
Motivation: 文本到图像模型在定制视觉广告和针对特定人群方面具有吸引力，研究旨在探索其潜力并解决可能存在的偏见和定位问题。

Method: 研究方法包括：1) 检查不同广告主题中生成广告的人口统计学偏见；2) 评估除人物性别/种族外完全相同的广告，其说服力（由模型判断）的差异程度；3) 实验一种针对特定国家投放广告的技术。

Result: 摘要中未提供具体的实验结果，但阐述了研究将要调查和实验的领域，包括人口统计学偏见、说服力差异以及国家定位技术。

Conclusion: 摘要未直接给出结论，但研究方向表明，理解和量化文本到图像模型在广告生成中的偏见以及优化其定位能力，对于负责任地利用这些模型至关重要。

Abstract: Text-to-image models are appealing for customizing visual advertisements and
targeting specific populations. We investigate this potential by examining the
demographic bias within ads for different ad topics, and the disparate level of
persuasiveness (judged by models) of ads that are identical except for
gender/race of the people portrayed. We also experiment with a technique to
target ads for specific countries. The code is available at
https://github.com/aysanaghazadeh/FaceOfPersuasion

</details>


### [56] [CuSfM: CUDA-Accelerated Structure-from-Motion](https://arxiv.org/abs/2510.15271)
*Jingrui Yu,Jun Liu,Kefei Ren,Joydeep Biswas,Rurui Ye,Keqiang Wu,Chirag Majithia,Di Zeng*

Main category: cs.CV

TL;DR: 本文提出cuSfM，一个CUDA加速的离线SfM系统，通过GPU并行化实现高效且高精度的特征提取和数据关联，显著提升了相机姿态估计的准确性和处理速度，并优于COLMAP。


<details>
  <summary>Details</summary>
Motivation: 高效且准确的相机姿态估计是自主导航、机器人感知和虚拟仿真系统中密集重建的基础要求。

Method: cuSfM是一个CUDA加速的离线SfM系统，利用GPU并行化来高效使用计算密集型但高精度的特征提取器，生成全面且非冗余的数据关联，以实现精确的相机姿态估计和全局一致性映射。它支持姿态优化、映射、先验地图定位和外部参数精修，专为充分利用计算资源以最大化准确性的离线处理而设计。

Result: 实验结果表明，与广泛使用的COLMAP方法相比，cuSfM在各种测试场景下均显著提高了准确性和处理速度，同时保持了离线SfM应用所需的高精度和全局一致性。

Conclusion: cuSfM提供了一个更优越的离线SfM解决方案，并以开源Python封装PyCuSfM的形式发布，以促进计算机视觉和机器人领域的研究与应用。

Abstract: Efficient and accurate camera pose estimation forms the foundational
requirement for dense reconstruction in autonomous navigation, robotic
perception, and virtual simulation systems. This paper addresses the challenge
via cuSfM, a CUDA-accelerated offline Structure-from-Motion system that
leverages GPU parallelization to efficiently employ computationally intensive
yet highly accurate feature extractors, generating comprehensive and
non-redundant data associations for precise camera pose estimation and globally
consistent mapping. The system supports pose optimization, mapping, prior-map
localization, and extrinsic refinement. It is designed for offline processing,
where computational resources can be fully utilized to maximize accuracy.
Experimental results demonstrate that cuSfM achieves significantly improved
accuracy and processing speed compared to the widely used COLMAP method across
various testing scenarios, while maintaining the high precision and global
consistency essential for offline SfM applications. The system is released as
an open-source Python wrapper implementation, PyCuSfM, available at
https://github.com/nvidia-isaac/pyCuSFM, to facilitate research and
applications in computer vision and robotics.

</details>


### [57] [DriveGen3D: Boosting Feed-Forward Driving Scene Generation with Efficient Video Diffusion](https://arxiv.org/abs/2510.15264)
*Weijie Wang,Jiagang Zhu,Zeyu Zhang,Xiaofeng Wang,Zheng Zhu,Guosheng Zhao,Chaojun Ni,Haoxiao Wang,Guan Huang,Xinze Chen,Yukun Zhou,Wenkang Qin,Duochao Shi,Haoyun Li,Guanghong Jia,Jiwen Lu*

Main category: cs.CV

TL;DR: DriveGen3D是一个新颖的框架，用于生成高质量、高度可控的动态3D驾驶场景，通过整合加速长时视频生成和大规模动态场景重建，解决了现有方法的计算效率低、缺乏3D表示或仅限于静态重建的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有驾驶场景合成方法存在以下关键限制：长时间生成计算需求过高；仅专注于长时间视频合成而无3D表示；或仅限于静态单场景重建。这些不足促使研究人员寻求一种能同时满足高效长时视频生成和动态3D场景重建的解决方案。

Method: DriveGen3D引入了一个统一的管道，包含两个专门组件：1. FastDrive-DiT：一个高效的视频扩散Transformer，用于在高分辨率下，通过文本和鸟瞰图(BEV)布局指导，合成时间连贯的视频。2. FastRecon3D：一个前馈重建模块，能够快速构建跨时间的3D高斯表示，确保时空一致性。整个系统通过多模态条件控制实现。

Result: DriveGen3D能够实时生成扩展的驾驶视频（最高达424x800分辨率，12 FPS）及其对应的动态3D场景。在新视角合成方面，SSIM达到0.811，PSNR达到22.84，同时保持了参数效率。

Conclusion: DriveGen3D通过其统一的管道和创新的双组件设计，成功弥合了现有驾驶场景合成方法的不足，实现了高效、高质量、可控的动态3D驾驶场景生成，为该领域树立了新的基准。

Abstract: We present DriveGen3D, a novel framework for generating high-quality and
highly controllable dynamic 3D driving scenes that addresses critical
limitations in existing methodologies. Current approaches to driving scene
synthesis either suffer from prohibitive computational demands for extended
temporal generation, focus exclusively on prolonged video synthesis without 3D
representation, or restrict themselves to static single-scene reconstruction.
Our work bridges this methodological gap by integrating accelerated long-term
video generation with large-scale dynamic scene reconstruction through
multimodal conditional control. DriveGen3D introduces a unified pipeline
consisting of two specialized components: FastDrive-DiT, an efficient video
diffusion transformer for high-resolution, temporally coherent video synthesis
under text and Bird's-Eye-View (BEV) layout guidance; and FastRecon3D, a
feed-forward reconstruction module that rapidly builds 3D Gaussian
representations across time, ensuring spatial-temporal consistency. Together,
these components enable real-time generation of extended driving videos (up to
$424\times800$ at 12 FPS) and corresponding dynamic 3D scenes, achieving SSIM
of 0.811 and PSNR of 22.84 on novel view synthesis, all while maintaining
parameter efficiency.

</details>


### [58] [Post-Processing Methods for Improving Accuracy in MRI Inpainting](https://arxiv.org/abs/2510.15282)
*Nishad Kulkarni,Krithika Iyer,Austin Tapp,Abhijeet Parida,Daniel Capellán-Martín,Zhifan Jiang,María J. Ledesma-Carbayo,Syed Muhammad Anwar,Marius George Linguraru*

Main category: cs.CV

TL;DR: 该研究通过结合模型集成和高效后处理（如中值滤波、直方图匹配、像素平均），并辅以轻量级U-Net增强，显著提升了脑部病变区域MRI图像修复的解剖学合理性和视觉保真度，以支持更广泛的临床应用。


<details>
  <summary>Details</summary>
Motivation: 大多数自动化MRI分析工具（如分割和配准）是为健康解剖结构优化的，当遇到如肿瘤等大型病变时常常失效。图像修复技术旨在局部合成肿瘤区域的健康脑组织，以使通用工具能可靠应用，但现有修复模型的独立性能已趋于饱和。

Method: 系统评估了最先进的图像修复模型，并提出了一种新方法：将模型集成与高效后处理策略（如中值滤波、直方图匹配和像素平均）相结合。此外，通过一个轻量级U-Net增强阶段实现进一步的解剖学细化。

Result: 所提出的管道改善了修复区域的解剖学合理性和视觉保真度，与单个基线模型相比，获得了更高的准确性和更鲁棒的结果。

Conclusion: 通过将现有模型与有针对性的后处理相结合，实现了改进且更易于获取的图像修复结果，支持更广泛的临床部署和可持续、资源节约型的研究。

Abstract: Magnetic Resonance Imaging (MRI) is the primary imaging modality used in the
diagnosis, assessment, and treatment planning for brain pathologies. However,
most automated MRI analysis tools, such as segmentation and registration
pipelines, are optimized for healthy anatomies and often fail when confronted
with large lesions such as tumors. To overcome this, image inpainting
techniques aim to locally synthesize healthy brain tissues in tumor regions,
enabling the reliable application of general-purpose tools. In this work, we
systematically evaluate state-of-the-art inpainting models and observe a
saturation in their standalone performance. In response, we introduce a
methodology combining model ensembling with efficient post-processing
strategies such as median filtering, histogram matching, and pixel averaging.
Further anatomical refinement is achieved via a lightweight U-Net enhancement
stage. Comprehensive evaluation demonstrates that our proposed pipeline
improves the anatomical plausibility and visual fidelity of inpainted regions,
yielding higher accuracy and more robust outcomes than individual baseline
models. By combining established models with targeted post-processing, we
achieve improved and more accessible inpainting outcomes, supporting broader
clinical deployment and sustainable, resource-conscious research. Our 2025
BraTS inpainting docker is available at
https://hub.docker.com/layers/aparida12/brats2025/inpt.

</details>


### [59] [Exploring Conditions for Diffusion models in Robotic Control](https://arxiv.org/abs/2510.15510)
*Heeseong Shin,Byeongho Heo,Dongyoon Han,Seungryong Kim,Taekyung Kim*

Main category: cs.CV

TL;DR: 本文提出ORCA，利用预训练的文生图扩散模型为机器人控制提供任务自适应视觉表示。通过引入可学习的任务提示和视觉提示，克服了领域差异，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管预训练视觉表示已显著推动模仿学习，但它们在策略学习过程中通常保持冻结，导致任务无关。研究旨在不微调模型本身的情况下，利用预训练的文生图扩散模型为机器人控制获取任务自适应的视觉表示。

Method: 研究发现，直接应用文本条件在控制任务中效果不佳，甚至产生负面影响，原因在于扩散模型训练数据与机器人控制环境之间存在领域差异。为此，论文提出了ORCA方法，引入了两种新的条件：1) 可学习的任务提示，以适应控制环境；2) 视觉提示，以捕获细粒度的、帧特定的视觉细节。这些条件旨在促进任务自适应表示的生成。

Result: 通过促进任务自适应表示，ORCA方法在各种机器人控制基准测试中取得了最先进的性能，显著超越了现有方法。

Conclusion: 为机器人控制任务设计专门的、考虑动态视觉信息的条件（如ORCA的可学习任务提示和视觉提示），能够有效利用预训练文生图扩散模型，克服领域差异，从而获得任务自适应的视觉表示，并显著提升控制性能。

Abstract: While pre-trained visual representations have significantly advanced
imitation learning, they are often task-agnostic as they remain frozen during
policy learning. In this work, we explore leveraging pre-trained text-to-image
diffusion models to obtain task-adaptive visual representations for robotic
control, without fine-tuning the model itself. However, we find that naively
applying textual conditions - a successful strategy in other vision domains -
yields minimal or even negative gains in control tasks. We attribute this to
the domain gap between the diffusion model's training data and robotic control
environments, leading us to argue for conditions that consider the specific,
dynamic visual information required for control. To this end, we propose ORCA,
which introduces learnable task prompts that adapt to the control environment
and visual prompts that capture fine-grained, frame-specific details. Through
facilitating task-adaptive representations with our newly devised conditions,
our approach achieves state-of-the-art performance on various robotic control
benchmarks, significantly surpassing prior methods.

</details>


### [60] [QCFace: Image Quality Control for boosting Face Representation & Recognition](https://arxiv.org/abs/2510.15289)
*Duc-Phuong Doan-Ngo,Thanh-Dang Diep,Thanh Nguyen-Duc,Thanh-Sach LE,Nam Thoai*

Main category: cs.CV

TL;DR: 本文提出QCFace，一种基于硬边界策略的新型损失函数，旨在解决现有深度人脸识别系统中可识别性捕获不足和优化过程中梯度重叠的问题。QCFace能清晰地解耦可识别性和身份表示，并显著提升人脸识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度人脸识别（FR）损失函数在捕获可识别性方面存在不足，仅通过软边界部分实现，导致低质量或模糊人脸的特征表示较弱、判别力低。此外，特征方向和幅值之间相互重叠的梯度会在优化过程中引入不良交互，导致超球面规划不稳定和泛化能力差。

Method: 本文引入了一种硬边界策略——Quality Control Face (QCFace)，以解决梯度重叠问题并实现可识别性与身份表示的清晰解耦。基于此策略，提出了一种新型的硬边界损失函数，该函数采用引导因子进行超球面规划，同时优化识别能力和显式可识别性表示。

Result: QCFace不仅提供了鲁棒且可量化的可识别性编码，而且在验证和识别基准测试中，与现有基于可识别性的损失函数相比，均达到了最先进的性能。

Conclusion: QCFace通过硬边界策略成功解决了现有方法中可识别性捕获不足和优化不稳定的问题，实现了可识别性与身份表示的清晰解耦，从而提升了人脸识别系统的特征表示质量和整体性能。

Abstract: Recognizability, a key perceptual factor in human face processing, strongly
affects the performance of face recognition (FR) systems in both verification
and identification tasks. Effectively using recognizability to enhance feature
representation remains challenging. In deep FR, the loss function plays a
crucial role in shaping how features are embedded. However, current methods
have two main drawbacks: (i) recognizability is only partially captured through
soft margin constraints, resulting in weaker quality representation and lower
discrimination, especially for low-quality or ambiguous faces; (ii) mutual
overlapping gradients between feature direction and magnitude introduce
undesirable interactions during optimization, causing instability and confusion
in hypersphere planning, which may result in poor generalization, and entangled
representations where recognizability and identity are not cleanly separated.
To address these issues, we introduce a hard margin strategy - Quality Control
Face (QCFace), which overcomes the mutual overlapping gradient problem and
enables the clear decoupling of recognizability from identity representation.
Based on this strategy, a novel hard-margin-based loss function employs a
guidance factor for hypersphere planning, simultaneously optimizing for
recognition ability and explicit recognizability representation. Extensive
experiments confirm that QCFace not only provides robust and quantifiable
recognizability encoding but also achieves state-of-the-art performance in both
verification and identification benchmarks compared to existing
recognizability-based losses.

</details>


### [61] [Hyperbolic Structured Classification for Robust Single Positive Multi-label Learning](https://arxiv.org/abs/2510.15296)
*Yiming Lin,Shang Wang,Junkai Zhou,Qiufeng Wang,Xiao-Bo Jin,Kaizhu Huang*

Main category: cs.CV

TL;DR: 针对单正多标签学习（SPMLL），本文提出首个双曲分类框架，将每个标签表示为双曲球体，通过球体交互建模标签间的包含、重叠和分离关系，并引入自适应分类器和双势阱正则化，在多个基准数据集上取得了优异性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 单正多标签学习（SPMLL）场景下，每个训练样本仅有一个正标签，难以捕获复杂的标签关系和层次结构。现有方法隐式地通过距离相似性建模标签关系，但缺乏对不同关系类型的明确几何定义。

Method: 提出首个用于SPMLL的双曲分类框架，将每个标签表示为一个双曲球体，而非点或向量。通过几何球体交互自然地捕获多种关系类型：包含（用于层次结构）、重叠（用于共现模式）和分离（用于语义独立性）。此外，引入了温度自适应双曲球分类器和受物理学启发的双势阱正则化，以引导球体形成有意义的配置。

Result: 在四个基准数据集（MS-COCO, PASCAL VOC, NUS-WIDE, CUB-200-2011）上，与现有方法相比，取得了具有竞争力的性能和卓越的可解释性。统计分析显示，学习到的嵌入与真实世界的共现模式之间存在强相关性。

Conclusion: 双曲几何为不完全监督下的结构化分类（SPMLL）提供了一种更鲁棒的范式。

Abstract: Single Positive Multi-Label Learning (SPMLL) addresses the challenging
scenario where each training sample is annotated with only one positive label
despite potentially belonging to multiple categories, making it difficult to
capture complex label relationships and hierarchical structures. While existing
methods implicitly model label relationships through distance-based similarity,
lacking explicit geometric definitions for different relationship types. To
address these limitations, we propose the first hyperbolic classification
framework for SPMLL that represents each label as a hyperbolic ball rather than
a point or vector, enabling rich inter-label relationship modeling through
geometric ball interactions. Our ball-based approach naturally captures
multiple relationship types simultaneously: inclusion for hierarchical
structures, overlap for co-occurrence patterns, and separation for semantic
independence. Further, we introduce two key component innovations: a
temperature-adaptive hyperbolic ball classifier and a physics-inspired
double-well regularization that guides balls toward meaningful configurations.
To validate our approach, extensive experiments on four benchmark datasets
(MS-COCO, PASCAL VOC, NUS-WIDE, CUB-200-2011) demonstrate competitive
performance with superior interpretability compared to existing methods.
Furthermore, statistical analysis reveals strong correlation between learned
embeddings and real-world co-occurrence patterns, establishing hyperbolic
geometry as a more robust paradigm for structured classification under
incomplete supervision.

</details>


### [62] [Latent Diffusion Model without Variational Autoencoder](https://arxiv.org/abs/2510.15301)
*Minglei Shi,Haolin Wang,Wenzhao Zheng,Ziyang Yuan,Xiaoshi Wu,Xintao Wang,Pengfei Wan,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: 本文提出SVG，一种新型潜在扩散模型，通过利用自监督表示（如DINO特征）替代变分自编码器（VAE），解决了传统VAE+扩散范式在训练效率、推理速度和任务迁移性方面的局限，实现了更高效的训练、更快的采样和更高的生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型严重依赖VAE，但VAE潜在空间缺乏清晰的语义分离和判别结构，导致训练效率低、推理慢、任务迁移性差。研究发现，这些特性对于感知、理解任务以及潜在扩散模型的稳定高效训练至关重要。

Method: SVG模型不使用VAE，而是利用冻结的DINO特征构建具有清晰语义判别性的特征空间，并通过一个轻量级残差分支捕捉细粒度细节以实现高保真重建。扩散模型直接在此语义结构化的潜在空间上进行训练。

Result: SVG显著加速了扩散模型的训练过程，支持少量步骤采样，并提高了生成质量。实验结果表明，SVG保留了底层自监督表示的语义和判别能力。

Conclusion: SVG通过利用自监督表示为视觉生成提供了一个原则性的途径，能够实现任务通用、高质量的视觉表示，克服了传统VAE+扩散模型的局限性。

Abstract: Recent progress in diffusion-based visual generation has largely relied on
latent diffusion models with variational autoencoders (VAEs). While effective
for high-fidelity synthesis, this VAE+diffusion paradigm suffers from limited
training efficiency, slow inference, and poor transferability to broader vision
tasks. These issues stem from a key limitation of VAE latent spaces: the lack
of clear semantic separation and strong discriminative structure. Our analysis
confirms that these properties are crucial not only for perception and
understanding tasks, but also for the stable and efficient training of latent
diffusion models. Motivated by this insight, we introduce SVG, a novel latent
diffusion model without variational autoencoders, which leverages
self-supervised representations for visual generation. SVG constructs a feature
space with clear semantic discriminability by leveraging frozen DINO features,
while a lightweight residual branch captures fine-grained details for
high-fidelity reconstruction. Diffusion models are trained directly on this
semantically structured latent space to facilitate more efficient learning. As
a result, SVG enables accelerated diffusion training, supports few-step
sampling, and improves generative quality. Experimental results further show
that SVG preserves the semantic and discriminative capabilities of the
underlying self-supervised representations, providing a principled pathway
toward task-general, high-quality visual representations.

</details>


### [63] [Proto-Former: Unified Facial Landmark Detection by Prototype Transformer](https://arxiv.org/abs/2510.15338)
*Shengkai Hu,Haozhe Qi,Jun Wan,Jiaxing Huang,Lefei Zhang,Hang Sun,Dacheng Tao*

Main category: cs.CV

TL;DR: Proto-Former是一种统一、自适应的端到端人脸关键点检测框架，通过显式增强数据集特有的人脸结构表示（即原型），克服了现有方法在多数据集训练中的局限性，实现了卓越的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的人脸关键点检测数据集定义了不同数量的关键点，且大多数主流方法只能在单一数据集上进行训练，这限制了模型对不同数据集的泛化能力，并阻碍了统一模型的发展。

Method: Proto-Former框架包含两个核心组件：自适应原型感知编码器（APAE）用于自适应特征提取和学习原型表示；渐进式原型感知解码器（PPAD）用于优化原型，生成引导模型关注关键人脸区域的提示。此外，引入了一种新颖的原型感知（PA）损失，通过约束原型专家的选择权重来解决多数据集训练中原型专家寻址不稳定性问题，缓解梯度冲突，并提取更准确的人脸结构特征。

Result: 在广泛使用的基准数据集上进行的实验表明，Proto-Former比现有最先进的方法取得了更优越的性能。

Conclusion: Proto-Former成功解决了多数据集人脸关键点检测的挑战，提供了一个统一、自适应的解决方案，显著提高了准确性和泛化能力。

Abstract: Recent advances in deep learning have significantly improved facial landmark
detection. However, existing facial landmark detection datasets often define
different numbers of landmarks, and most mainstream methods can only be trained
on a single dataset. This limits the model generalization to different datasets
and hinders the development of a unified model. To address this issue, we
propose Proto-Former, a unified, adaptive, end-to-end facial landmark detection
framework that explicitly enhances dataset-specific facial structural
representations (i.e., prototype). Proto-Former overcomes the limitations of
single-dataset training by enabling joint training across multiple datasets
within a unified architecture. Specifically, Proto-Former comprises two key
components: an Adaptive Prototype-Aware Encoder (APAE) that performs adaptive
feature extraction and learns prototype representations, and a Progressive
Prototype-Aware Decoder (PPAD) that refines these prototypes to generate
prompts that guide the model's attention to key facial regions. Furthermore, we
introduce a novel Prototype-Aware (PA) loss, which achieves optimal path
finding by constraining the selection weights of prototype experts. This loss
function effectively resolves the problem of prototype expert addressing
instability during multi-dataset training, alleviates gradient conflicts, and
enables the extraction of more accurate facial structure features. Extensive
experiments on widely used benchmark datasets demonstrate that our Proto-Former
achieves superior performance compared to existing state-of-the-art methods.
The code is publicly available at: https://github.com/Husk021118/Proto-Former.

</details>


### [64] [Layer as Puzzle Pieces: Compressing Large Language Models through Layer Concatenation](https://arxiv.org/abs/2510.15304)
*Fei Wang,Li Shen,Liang Ding,Chao Xue,Ye Liu,Changxing Ding*

Main category: cs.CV

TL;DR: 本文提出CoMe，一种针对大型语言模型（LLMs）的结构化剪枝方法，通过渐进式层剪枝、基于拼接的合并技术和分层蒸馏，有效解决了现有剪枝方法性能下降和能力保留不足的问题，在显著减小模型尺寸的同时保持了高准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）因其巨大的尺寸而导致高昂的计算和存储成本。现有分层结构化剪枝方法往往忽略了保留被剪枝部分的能力，并存在性能显著下降、线性权重层聚合不当以及缺乏有效训练后恢复机制等局限性。

Method: 本文提出了CoMe方法，包括一个渐进式层剪枝框架、一种基于拼接的合并技术和分层蒸馏后训练过程。具体而言，CoMe引入了结合激活强度和权重范数的通道敏感性度量进行细粒度通道选择；采用基于拼接的层合并方法融合相邻层中最关键的通道，实现渐进式模型尺寸缩减；最后，提出分层蒸馏协议，利用剪枝过程中建立的原始模型与剪枝模型层之间的对应关系，实现高效的知识迁移。

Result: 在七个基准测试上的实验表明，CoMe实现了最先进的性能；当剪枝LLaMA-2-7b 30%的参数时，剪枝模型保留了其原始平均准确率的83%。

Conclusion: CoMe通过其创新的渐进式剪枝、拼接合并和分层蒸馏机制，成功克服了LLMs结构化剪枝的现有局限性，实现了模型尺寸的显著减小，同时保持了卓越的性能，为LLMs的高效部署提供了有效途径。

Abstract: Large Language Models excel at natural language processing tasks, but their
massive size leads to high computational and storage demands. Recent works have
sought to reduce their model size through layer-wise structured pruning.
However, they tend to ignore retaining the capabilities in the pruned part. In
this work, we re-examine structured pruning paradigms and uncover several key
limitations: 1) notable performance degradation due to direct layer removal, 2)
incompetent linear weight layer aggregation, and 3) the lack of effective
post-training recovery mechanisms. To address these limitations, we propose
CoMe, including a progressive layer pruning framework with a
Concatenation-based Merging technology and a hierarchical distillation
post-training process. Specifically, we introduce a channel sensitivity metric
that utilizes activation intensity and weight norms for fine-grained channel
selection. Subsequently, we employ a concatenation-based layer merging method
to fuse the most critical channels across adjacent layers, enabling progressive
model size reduction. Finally, we propose a hierarchical distillation protocol
that leverages the correspondences between the original and pruned model layers
established during pruning, thereby enabling efficient knowledge transfer.
Experiments on seven benchmarks show that CoMe achieves state-of-the-art
performance; when pruning 30% of LLaMA-2-7b's parameters, the pruned model
retains 83% of its original average accuracy. Our code is available at
https://github.com/MPI-Lab/CoMe.

</details>


### [65] [Spatial457: A Diagnostic Benchmark for 6D Spatial Reasoning of Large Multimodal Models](https://arxiv.org/abs/2502.08636)
*Xingrui Wang,Wufei Ma,Tiezheng Zhang,Celso M de Melo,Jieneng Chen,Alan Yuille*

Main category: cs.CV

TL;DR: 大型多模态模型（LMMs）在复杂三维（3D）空间推理方面能力不足。本研究引入了Spatial457数据集和评估框架，发现LMMs在3D和6D空间任务中性能显著下降，并揭示了其预测偏差。


<details>
  <summary>Details</summary>
Motivation: 尽管大型多模态模型在视觉场景理解和推理方面表现出色，但它们在复杂和精确的3D空间推理方面的能力尚不明确。现有基准主要侧重于2D空间理解，缺乏全面评估不同复杂程度下6D空间推理的框架。

Method: 本研究提出了Spatial457，一个可扩展且无偏的合成数据集，旨在评估多对象识别、2D定位、3D定位和3D方向这四种关键空间推理能力。开发了一个级联评估结构，构建了7种问题类型，涵盖5个难度级别，从基本的单对象识别到复杂的6D空间推理任务。引入了相对性能下降率（RPDR）来量化挑战。使用Spatial457评估了各种大型多模态模型。

Result: LMMs的性能随着任务复杂度的增加而普遍下降，尤其是在3D推理和6D空间任务中。相对性能下降率（RPDR）突显了3D推理能力中的关键弱点。利用数据集的无偏属性设计，还发现了不同属性间的预测偏差，这些模式在真实世界图像设置中也观察到。

Conclusion: 大型多模态模型在处理复杂3D和6D空间推理任务时存在显著局限性。Spatial457数据集及其评估框架有效地揭示了这些弱点和预测偏差，为未来LMMs的改进提供了方向。

Abstract: Although large multimodal models (LMMs) have demonstrated remarkable
capabilities in visual scene interpretation and reasoning, their capacity for
complex and precise 3-dimensional spatial reasoning remains uncertain. Existing
benchmarks focus predominantly on 2D spatial understanding and lack a framework
to comprehensively evaluate 6D spatial reasoning across varying complexities.
To address this limitation, we present Spatial457, a scalable and unbiased
synthetic dataset designed with 4 key capability for spatial reasoning:
multi-object recognition, 2D location, 3D location, and 3D orientation. We
develop a cascading evaluation structure, constructing 7 question types across
5 difficulty levels that range from basic single object recognition to our new
proposed complex 6D spatial reasoning tasks. We evaluated various large
multimodal models (LMMs) on PulseCheck457, observing a general decline in
performance as task complexity increases, particularly in 3D reasoning and 6D
spatial tasks. To quantify these challenges, we introduce the Relative
Performance Dropping Rate (RPDR), highlighting key weaknesses in 3D reasoning
capabilities. Leveraging the unbiased attribute design of our dataset, we also
uncover prediction biases across different attributes, with similar patterns
observed in real-world image settings. The code and data are released in
https://github.com/XingruiWang/Spatial457.

</details>


### [66] [SHARE: Scene-Human Aligned Reconstruction](https://arxiv.org/abs/2510.15342)
*Joshua Li,Brendan Chharawala,Chang Shu,Xue Bin Peng,Pengcheng Xi*

Main category: cs.CV

TL;DR: SHARE是一种利用场景几何空间线索，从单目RGB视频准确重建3D人体运动并将其与周围环境对齐的技术，解决了现有方法在3D人体放置上的不足。


<details>
  <summary>Details</summary>
Motivation: 为游戏、AR/VR和机器人中的自主智能体制作逼真的人物与环境交互动画至关重要。然而，当前的人体运动重建方法难以准确地将人体放置在3D空间中。

Method: SHARE技术仅依赖于固定摄像机的单目RGB视频。它首先估计每一帧的人体网格和分割掩码，并在关键帧估计场景点图。然后，通过将人体网格与从场景中（使用掩码提取的）人体点图进行比较，迭代地优化关键帧处的人体位置。关键在于，在优化过程中，非关键帧的人体网格通过保持其相对于关键帧根关节的相对位置来确保一致性。

Result: 该方法实现了更准确的3D人体放置，同时重建了周围场景，适用于精选数据集和野外网络视频。广泛的实验表明SHARE优于现有方法。

Conclusion: SHARE通过利用场景几何线索和创新的优化策略，显著提高了3D人体运动重建的准确性，尤其是在人体与场景的对齐方面，为自主智能体应用提供了更真实的交互动画基础。

Abstract: Animating realistic character interactions with the surrounding environment
is important for autonomous agents in gaming, AR/VR, and robotics. However,
current methods for human motion reconstruction struggle with accurately
placing humans in 3D space. We introduce Scene-Human Aligned REconstruction
(SHARE), a technique that leverages the scene geometry's inherent spatial cues
to accurately ground human motion reconstruction. Each reconstruction relies
solely on a monocular RGB video from a stationary camera. SHARE first estimates
a human mesh and segmentation mask for every frame, alongside a scene point map
at keyframes. It iteratively refines the human's positions at these keyframes
by comparing the human mesh against the human point map extracted from the
scene using the mask. Crucially, we also ensure that non-keyframe human meshes
remain consistent by preserving their relative root joint positions to keyframe
root joints during optimization. Our approach enables more accurate 3D human
placement while reconstructing the surrounding scene, facilitating use cases on
both curated datasets and in-the-wild web videos. Extensive experiments
demonstrate that SHARE outperforms existing methods.

</details>


### [67] [Cortical-SSM: A Deep State Space Model for EEG and ECoG Motor Imagery Decoding](https://arxiv.org/abs/2510.15371)
*Shuntaro Suzuki,Shunya Nagashima,Masayuki Hirata,Komei Sugiura*

Main category: cs.CV

TL;DR: 本文提出Cortical-SSM，一种扩展深度状态空间模型的新架构，用于运动想象（MI）EEG和ECoG信号分类。该模型能捕获信号在时域、空域和频域的集成依赖性，并在多个基准测试中优于基线方法，同时显示出神经生理学相关性。


<details>
  <summary>Details</summary>
Motivation: 运动想象EEG和ECoG信号分类在辅助沟通和康复支持方面有巨大应用潜力，但信号易受生理伪影影响。尽管基于Transformer的方法被广泛采用，但它们难以捕获信号内部的细粒度依赖性，因此需要更有效的方法来克服这些限制。

Method: 本文提出Cortical-SSM架构，它扩展了深度状态空间模型，旨在捕获EEG和ECoG信号在时间、空间和频率域的集成依赖性。

Result: 该方法在三个基准测试中均优于基线方法：包括两个包含超过50名受试者的大规模公共MI EEG数据集和一个从肌萎缩侧索硬化症患者记录的临床MI ECoG数据集。此外，模型生成的视觉解释表明它能有效捕获EEG和ECoG信号的神经生理学相关区域。

Conclusion: Cortical-SSM通过捕获EEG和ECoG信号在多域的集成依赖性，有效克服了现有方法的局限性，在运动想象信号分类方面表现出色，并具有神经生理学上的可解释性。

Abstract: Classification of electroencephalogram (EEG) and electrocorticogram (ECoG)
signals obtained during motor imagery (MI) has substantial application
potential, including for communication assistance and rehabilitation support
for patients with motor impairments. These signals remain inherently
susceptible to physiological artifacts (e.g., eye blinking, swallowing), which
pose persistent challenges. Although Transformer-based approaches for
classifying EEG and ECoG signals have been widely adopted, they often struggle
to capture fine-grained dependencies within them. To overcome these
limitations, we propose Cortical-SSM, a novel architecture that extends deep
state space models to capture integrated dependencies of EEG and ECoG signals
across temporal, spatial, and frequency domains. We validated our method across
three benchmarks: 1) two large-scale public MI EEG datasets containing more
than 50 subjects, and 2) a clinical MI ECoG dataset recorded from a patient
with amyotrophic lateral sclerosis. Our method outperformed baseline methods on
the three benchmarks. Furthermore, visual explanations derived from our model
indicate that it effectively captures neurophysiologically relevant regions of
both EEG and ECoG signals.

</details>


### [68] [Adaptive transfer learning for surgical tool presence detection in laparoscopic videos through gradual freezing fine-tuning](https://arxiv.org/abs/2510.15372)
*Ana Davila,Jacinto Colan,Yasuhisa Hasegawa*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的分阶段自适应微调方法（包括线性探测和逐步冻结），以解决外科手术中注释数据有限的挑战，从而实现自动化手术工具检测。该方法在Cholec80数据集上将平均精度（mAP）提高到96.4%，并在一系列不同的手术数据集（CATARACTS）上展示了其泛化能力。


<details>
  <summary>Details</summary>
Motivation: 微创手术可以从自动化手术工具检测中显著受益，但外科手术环境中带注释数据的有限性给训练鲁棒的深度学习模型带来了挑战。

Method: 本文引入了一种新颖的分阶段自适应微调方法，包括两个步骤：1) 线性探测阶段，用于在预训练的CNN架构上调整额外的分类层；2) 逐步冻结阶段，动态减少可微调层，以调节对外科领域的适应性。该策略旨在降低网络复杂性并提高效率，仅需一个训练循环。该方法在ImageNet上预训练的CNN架构（ResNet-50和DenseNet-121）上，通过Cholec80数据集和CATARACTS数据集进行了验证。

Result: 与现有方法和已建立的微调技术相比，该方法提高了检测性能，在Cholec80数据集上实现了96.4%的平均精度（mAP）。此外，该微调策略的泛化能力在CATARACTS数据集上得到了进一步证实。

Conclusion: 逐步冻结微调是一种有前景的技术，可用于改善不同外科手术中的工具存在检测，并可能在一般图像分类任务中具有更广泛的应用。

Abstract: Minimally invasive surgery can benefit significantly from automated surgical
tool detection, enabling advanced analysis and assistance. However, the limited
availability of annotated data in surgical settings poses a challenge for
training robust deep learning models. This paper introduces a novel staged
adaptive fine-tuning approach consisting of two steps: a linear probing stage
to condition additional classification layers on a pre-trained CNN-based
architecture and a gradual freezing stage to dynamically reduce the
fine-tunable layers, aiming to regulate adaptation to the surgical domain. This
strategy reduces network complexity and improves efficiency, requiring only a
single training loop and eliminating the need for multiple iterations. We
validated our method on the Cholec80 dataset, employing CNN architectures
(ResNet-50 and DenseNet-121) pre-trained on ImageNet for detecting surgical
tools in cholecystectomy endoscopic videos. Our results demonstrate that our
method improves detection performance compared to existing approaches and
established fine-tuning techniques, achieving a mean average precision (mAP) of
96.4%. To assess its broader applicability, the generalizability of the
fine-tuning strategy was further confirmed on the CATARACTS dataset, a distinct
domain of minimally invasive ophthalmic surgery. These findings suggest that
gradual freezing fine-tuning is a promising technique for improving tool
presence detection in diverse surgical procedures and may have broader
applications in general image classification tasks.

</details>


### [69] [MARIS: Marine Open-Vocabulary Instance Segmentation with Geometric Enhancement and Semantic Alignment](https://arxiv.org/abs/2510.15398)
*Bingyu Li,Feiyu Wang,Da Zhang,Zhiyuan Zhao,Junyu Gao,Xuelong Li*

Main category: cs.CV

TL;DR: 该论文提出了MARIS，首个用于水下开放词汇实例分割的大规模细粒度基准，并提出了一个统一框架，包含几何先验增强模块（GPEM）和语义对齐注入机制（SAIM），以解决水下图像退化和语义错位问题，显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有水下实例分割受限于封闭词汇预测，无法识别新类别。开放词汇分割在自然图像上表现良好，但转移到水下场景时，会因视觉退化（如颜色衰减）和水下类别定义缺失导致的语义错位而表现不佳。

Method: 1. 引入了MARIS，一个用于水下开放词汇实例分割的大规模细粒度基准。2. 提出了一个统一框架，包含两个互补组件：几何先验增强模块（GPEM）利用稳定的部分级和结构线索，在视觉退化条件下保持物体一致性；语义对齐注入机制（SAIM）通过领域特定先验丰富语言嵌入，减少语义模糊性并提高对未见类别的识别能力。

Result: 实验表明，该框架在MARIS基准上，无论是在域内（In-Domain）还是跨域（Cross-Domain）设置下，都持续优于现有的开放词汇基线方法。

Conclusion: 该框架为未来的水下感知研究奠定了坚实基础，有效解决了水下开放词汇实例分割面临的视觉退化和语义错位问题。

Abstract: Most existing underwater instance segmentation approaches are constrained by
close-vocabulary prediction, limiting their ability to recognize novel marine
categories. To support evaluation, we introduce \textbf{MARIS}
(\underline{Mar}ine Open-Vocabulary \underline{I}nstance
\underline{S}egmentation), the first large-scale fine-grained benchmark for
underwater Open-Vocabulary (OV) segmentation, featuring a limited set of seen
categories and diverse unseen categories. Although OV segmentation has shown
promise on natural images, our analysis reveals that transfer to underwater
scenes suffers from severe visual degradation (e.g., color attenuation) and
semantic misalignment caused by lack underwater class definitions. To address
these issues, we propose a unified framework with two complementary components.
The Geometric Prior Enhancement Module (\textbf{GPEM}) leverages stable
part-level and structural cues to maintain object consistency under degraded
visual conditions. The Semantic Alignment Injection Mechanism (\textbf{SAIM})
enriches language embeddings with domain-specific priors, mitigating semantic
ambiguity and improving recognition of unseen categories. Experiments show that
our framework consistently outperforms existing OV baselines both In-Domain and
Cross-Domain setting on MARIS, establishing a strong foundation for future
underwater perception research.

</details>


### [70] [FreqPDE: Rethinking Positional Depth Embedding for Multi-View 3D Object Detection Transformers](https://arxiv.org/abs/2510.15385)
*Haisheng Su,Junjie Zhang,Feixiang Song,Sanping Zhou,Wei Wu,Nanning Zheng,Junchi Yan*

Main category: cs.CV

TL;DR: 本文提出频率感知位置深度嵌入（FreqPDE），通过结合频率感知特征、跨视角尺度不变性深度预测和混合深度监督，在无需LiDAR显式深度监督的情况下，显著提升了基于多视角2D图像的3D目标检测的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前多视角2D图像3D目标检测方法依赖深度预测来恢复空间信息，但需要LiDAR点进行显式监督。然而，预测的深度质量不佳（如目标边界不连续、小目标不清晰），这主要是由于投影点监督稀疏和使用高层图像特征进行深度预测所致。此外，现有方法还忽视了跨视角一致性和尺度不变性。

Method: 本文引入频率感知位置深度嵌入（FreqPDE）为2D图像特征提供空间信息，以供3D检测Transformer解码器使用。FreqPDE主要通过三个模块实现：1) 频率感知空间金字塔编码器（FSPE）结合高频边缘线索和低频语义构建特征金字塔；2) 跨视角尺度不变性深度预测器（CSDP）利用跨视角和高效通道注意力机制估计像素级深度分布；3) 位置深度编码器（PDE）将2D图像特征与3D位置嵌入结合，生成3D深度感知特征用于查询解码。此外，还采用了混合深度监督，从度量和分布两方面进行深度学习。

Result: 在nuScenes数据集上进行的广泛实验证明了所提出方法的有效性和优越性。

Conclusion: FreqPDE通过引入频率感知特征、跨视角尺度不变性深度预测以及混合深度监督，有效地解决了现有深度预测方法的局限性，显著提升了多视角2D图像3D目标检测的性能。

Abstract: Detecting 3D objects accurately from multi-view 2D images is a challenging
yet essential task in the field of autonomous driving. Current methods resort
to integrating depth prediction to recover the spatial information for object
query decoding, which necessitates explicit supervision from LiDAR points
during the training phase. However, the predicted depth quality is still
unsatisfactory such as depth discontinuity of object boundaries and
indistinction of small objects, which are mainly caused by the sparse
supervision of projected points and the use of high-level image features for
depth prediction. Besides, cross-view consistency and scale invariance are also
overlooked in previous methods. In this paper, we introduce Frequency-aware
Positional Depth Embedding (FreqPDE) to equip 2D image features with spatial
information for 3D detection transformer decoder, which can be obtained through
three main modules. Specifically, the Frequency-aware Spatial Pyramid Encoder
(FSPE) constructs a feature pyramid by combining high-frequency edge clues and
low-frequency semantics from different levels respectively. Then the Cross-view
Scale-invariant Depth Predictor (CSDP) estimates the pixel-level depth
distribution with cross-view and efficient channel attention mechanism.
Finally, the Positional Depth Encoder (PDE) combines the 2D image features and
3D position embeddings to generate the 3D depth-aware features for query
decoding. Additionally, hybrid depth supervision is adopted for complementary
depth learning from both metric and distribution aspects. Extensive experiments
conducted on the nuScenes dataset demonstrate the effectiveness and superiority
of our proposed method.

</details>


### [71] [PFGS: Pose-Fused 3D Gaussian Splatting for Complete Multi-Pose Object Reconstruction](https://arxiv.org/abs/2510.15386)
*Ting-Yu Yen,Yu-Sheng Chiu,Shih-Hsuan Hung,Peter Wonka,Hung-Kuo Chu*

Main category: cs.CV

TL;DR: PFGS是一个姿态感知的3DGS框架，它通过迭代融合多姿态图像来重建完整的物体，结合全局和局部配准策略，并智能地利用基础模型进行相机姿态估计和跨姿态配准，从而克服了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的3D Gaussian Splatting (3DGS) 方法通常假设物体以单一、静态姿态捕获，导致重建结果不完整，缺失被遮挡或自遮挡区域。因此，需要一个能够从多姿态图像捕获中重建完整物体的框架。

Method: PFGS是一个姿态感知的3DGS框架。它以物体在一个主姿态和多个辅助姿态的图像作为输入，并迭代地将每个辅助姿态融合到主姿态的统一3DGS表示中。其姿态感知融合策略结合了全局和局部配准。PFGS智能地利用基础模型，通过背景特征进行每姿态相机姿态估计，并利用基础模型进行跨姿态配准，从而克服了基础模型内存需求高和精度不足的挑战，并解决了背景不一致问题。

Result: 实验结果表明，PFGS在定性和定量评估中均持续优于现有强大的基线方法，能够生成更完整的重建和更高保真度的3DGS模型。

Conclusion: PFGS成功解决了从多姿态图像重建完整物体的实际挑战，通过其创新的姿态感知融合策略和智能的基础模型利用，实现了高质量、高保真度的3DGS模型重建。

Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have enabled high-quality,
real-time novel-view synthesis from multi-view images. However, most existing
methods assume the object is captured in a single, static pose, resulting in
incomplete reconstructions that miss occluded or self-occluded regions. We
introduce PFGS, a pose-aware 3DGS framework that addresses the practical
challenge of reconstructing complete objects from multi-pose image captures.
Given images of an object in one main pose and several auxiliary poses, PFGS
iteratively fuses each auxiliary set into a unified 3DGS representation of the
main pose. Our pose-aware fusion strategy combines global and local
registration to merge views effectively and refine the 3DGS model. While recent
advances in 3D foundation models have improved registration robustness and
efficiency, they remain limited by high memory demands and suboptimal accuracy.
PFGS overcomes these challenges by incorporating them more intelligently into
the registration process: it leverages background features for per-pose camera
pose estimation and employs foundation models for cross-pose registration. This
design captures the best of both approaches while resolving background
inconsistency issues. Experimental results demonstrate that PFGS consistently
outperforms strong baselines in both qualitative and quantitative evaluations,
producing more complete reconstructions and higher-fidelity 3DGS models.

</details>


### [72] [LILAC: Long-sequence Incremental Low-latency Arbitrary Motion Stylization via Streaming VAE-Diffusion with Causal Decoding](https://arxiv.org/abs/2510.15392)
*Peng Ren,Hai Yang*

Main category: cs.CV

TL;DR: LILAC是一种基于潜空间VAE-Diffusion的流式架构，通过因果解码实现长序列、低延迟、任意运动实时风格化。


<details>
  <summary>Details</summary>
Motivation: 现有实时运动生成方法计算开销大、时间稳定性差。而基于潜空间的VAE-Diffusion框架虽能实现高质量风格化，但通常仅限于离线处理。本研究旨在弥合这一差距，实现实时、长序列的运动风格化。

Method: LILAC在一个高性能离线任意运动风格化框架的基础上，通过潜空间流式架构将其扩展到在线设置。该架构采用滑动窗口因果设计，并注入解码后的运动特征以确保平滑过渡，无需依赖未来帧或修改扩散模型架构。

Result: 实验表明，LILAC在基准数据集上实现了长序列实时任意风格化，并在风格化质量和响应速度之间取得了良好平衡。

Conclusion: LILAC成功将离线VAE-Diffusion框架应用于在线实时运动风格化，解决了现有方法在计算效率、时间稳定性及实时性方面的局限性，实现了高质量、低延迟的长时间运动风格化。

Abstract: Generating long and stylized human motions in real time is critical for
applications that demand continuous and responsive character control. Despite
its importance, existing streaming approaches often operate directly in the raw
motion space, leading to substantial computational overhead and making it
difficult to maintain temporal stability. In contrast, latent-space
VAE-Diffusion-based frameworks alleviate these issues and achieve high-quality
stylization, but they are generally confined to offline processing. To bridge
this gap, LILAC (Long-sequence Incremental Low-latency Arbitrary Motion
Stylization via Streaming VAE-Diffusion with Causal Decoding) builds upon a
recent high-performing offline framework for arbitrary motion stylization and
extends it to an online setting through a latent-space streaming architecture
with a sliding-window causal design and the injection of decoded motion
features to ensure smooth motion transitions. This architecture enables
long-sequence real-time arbitrary stylization without relying on future frames
or modifying the diffusion model architecture, achieving a favorable balance
between stylization quality and responsiveness as demonstrated by experiments
on benchmark datasets. Supplementary video and examples are available at the
project page: https://pren1.github.io/lilac/

</details>


### [73] [Learning to Detect Unknown Jailbreak Attacks in Large Vision-Language Models](https://arxiv.org/abs/2510.15430)
*Shuang Liang,Zhihao Xu,Jialing Tao,Hui Xue,Xiting Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为LoD的通用框架，通过从攻击特定学习转向任务特定学习，准确检测大型视觉-语言模型（LVLMs）中的未知越狱攻击，提高了检测准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 尽管进行了广泛的对齐工作，LVLMs仍然容易受到越狱攻击，带来严重的安全风险。现有检测方法要么学习攻击特定参数（泛化能力差），要么依赖启发式原则（准确性和效率有限），因此需要一种更通用、准确和高效的未知攻击检测方法。

Method: 本文提出了“学习检测（Learning to Detect, LoD）”框架，将重点从攻击特定学习转移到任务特定学习。该框架包含两个核心模块：多模态安全概念激活向量（Multi-modal Safety Concept Activation Vector）用于安全导向的表示学习，以及安全模式自编码器（Safety Pattern Auto-Encoder）用于无监督攻击分类。

Result: 广泛的实验表明，LoD方法在各种未知攻击上始终实现了更高的检测AUROC（受试者工作特征曲线下面积），同时提高了检测效率。

Conclusion: LoD框架通过关注任务特定的安全学习，成功克服了现有方法的局限性，提供了一种通用、准确且高效的解决方案，用于检测LVLMs中的未知越狱攻击。

Abstract: Despite extensive alignment efforts, Large Vision-Language Models (LVLMs)
remain vulnerable to jailbreak attacks, posing serious safety risks. To address
this, existing detection methods either learn attack-specific parameters, which
hinders generalization to unseen attacks, or rely on heuristically sound
principles, which limit accuracy and efficiency. To overcome these limitations,
we propose Learning to Detect (LoD), a general framework that accurately
detects unknown jailbreak attacks by shifting the focus from attack-specific
learning to task-specific learning. This framework includes a Multi-modal
Safety Concept Activation Vector module for safety-oriented representation
learning and a Safety Pattern Auto-Encoder module for unsupervised attack
classification. Extensive experiments show that our method achieves
consistently higher detection AUROC on diverse unknown attacks while improving
efficiency. The code is available at
https://anonymous.4open.science/r/Learning-to-Detect-51CB.

</details>


### [74] [Robust High-Resolution Multi-Organ Diffusion MRI Using Synthetic-Data-Tuned Prompt Learning](https://arxiv.org/abs/2510.15400)
*Chen Qian,Haoyu Zhang,Junnan Ma,Liuhong Zhu,Qingrui Cai,Yu Wang,Ruibo Song,Lv Li,Lin Mei,Xianwang Jiang,Qin Xu,Boyu Jiang,Ran Tao,Chunmiao Chen,Shufang Chen,Dongyun Liang,Qiu Guo,Jianzhong Lin,Taishan Kang,Mengtian Lu,Liyuan Fu,Ruibin Huang,Huijuan Wan,Xu Huang,Jianhua Wang,Di Guo,Hai Zhong,Jianjun Zhou,Xiaobo Qu*

Main category: cs.CV

TL;DR: LoSP-Prompt是一种新的多重激发扩散加权磁共振成像（multi-shot DWI）重建框架，通过结合物理信息建模和合成数据驱动的提示学习，有效克服了运动引起的伪影，实现了高分辨率、无伪影的全身多器官DWI，具有广阔的临床应用前景。


<details>
  <summary>Details</summary>
Motivation: 多重激发DWI在临床上用于全身肿瘤诊断的推广受到严重限制，主要原因是呼吸、蠕动等生理运动引起的相位伪影，以及多器官、多切片、多方向和多b值等复杂性。

Method: 该研究引入了LoSP-Prompt重建框架。它将激发间相位变化建模为高阶局部平滑相位（LoSP），并将其整合到低秩Hankel矩阵重建中。关键在于，算法的秩参数通过提示学习自动设定，该学习仅在模拟生理运动的合成腹部DWI数据上进行训练，无需导航信号和真实数据监督。

Result: (1) 实现了临床单次激发DWI两倍的空间分辨率，提高了肝脏病灶的清晰度；(2) 单一模型即可泛化应用于七个不同解剖区域（肝脏、肾脏、骶髂关节、骨盆、膝盖、脊髓、大脑）；(3) 在图像质量、伪影抑制和噪声降低方面优于现有最先进的方法（11位放射科医生在5分制评估中，p<0.05），在肾脏DWI上获得4-5分（优秀），在肝脏、骶髂关节和脊髓DWI上获得4分（良好到优秀），在膝盖和脑肿瘤DWI上获得3-4分（良好）；(4) 在10,000多张临床图像（43名受试者，4种扫描仪型号，5个中心）上得到验证。

Conclusion: LoSP-Prompt为高分辨率多器官多重激发DWI提供了一种可解释、鲁棒的解决方案，无需导航信号和真实数据监督。其与扫描仪无关的性能预示着其在精准肿瘤学领域具有变革性的潜力。

Abstract: Clinical adoption of multi-shot diffusion-weighted magnetic resonance imaging
(multi-shot DWI) for body-wide tumor diagnostics is limited by severe
motion-induced phase artifacts from respiration, peristalsis, and so on,
compounded by multi-organ, multi-slice, multi-direction and multi-b-value
complexities. Here, we introduce a reconstruction framework, LoSP-Prompt, that
overcomes these challenges through physics-informed modeling and
synthetic-data-driven prompt learning. We model inter-shot phase variations as
a high-order Locally Smooth Phase (LoSP), integrated into a low-rank Hankel
matrix reconstruction. Crucially, the algorithm's rank parameter is
automatically set via prompt learning trained exclusively on synthetic
abdominal DWI data emulating physiological motion. Validated across 10,000+
clinical images (43 subjects, 4 scanner models, 5 centers), LoSP-Prompt: (1)
Achieved twice the spatial resolution of clinical single-shot DWI, enhancing
liver lesion conspicuity; (2) Generalized to seven diverse anatomical regions
(liver, kidney, sacroiliac, pelvis, knee, spinal cord, brain) with a single
model; (3) Outperformed state-of-the-art methods in image quality, artifact
suppression, and noise reduction (11 radiologists' evaluations on a 5-point
scale, $p<0.05$), achieving 4-5 points (excellent) on kidney DWI, 4 points
(good to excellent) on liver, sacroiliac and spinal cord DWI, and 3-4 points
(good) on knee and tumor brain. The approach eliminates navigator signals and
realistic data supervision, providing an interpretable, robust solution for
high-resolution multi-organ multi-shot DWI. Its scanner-agnostic performance
signifies transformative potential for precision oncology.

</details>


### [75] [Select Less, Reason More: Prioritizing Evidence Purity for Video Reasoning](https://arxiv.org/abs/2510.15440)
*Xuchen Li,Xuzhao Li,Shiyu Hu,Kaiqi Huang*

Main category: cs.CV

TL;DR: 本文提出了一种名为EARL的证据感知强化学习框架，通过动态选择最相关帧并进行局部重采样，解决了视频大语言模型在长视频推理中信息稀释和证据不足的问题，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频大语言模型在长视频推理中面临挑战，静态均匀帧采样导致信息稀释和关键证据模糊。此外，现有像素空间视频推理代理缺乏严格的奖励机制来确保证据纯度，也无法在预采样帧之外进行时间信息补充，导致次优表现。

Method: 本文提出了一种基于“少选多推理”理念的证据优先自适应框架。核心贡献是证据感知强化学习（EARL）框架，它将模型转变为证据的主动询问者。EARL被精确设计用于动态选择最相关帧，并在选定的关键帧周围执行局部重采样，以获取细粒度的时间细节。

Result: 在五个具有挑战性的视频推理基准测试中，EARL训练的模型在开源视频大语言模型中取得了新的最先进成果，同时学习到有效且高纯度的视觉证据选择策略。具体而言，7B模型在LongVideoBench上达到59.8%，MVBench上达到69.0%，VideoMME上达到64.9%。

Conclusion: 这些结果强调了证据纯度优先的重要性，并证明了本文所提出框架的有效性。

Abstract: Long-form video reasoning remains a major challenge for Video Large Language
Models (Video LLMs), as static uniform frame sampling leads to information
dilution and obscures critical evidence. Furthermore, existing pixel-space
video reasoning agents, which are designed to actively interact with the video
to acquire new visual information, remain suboptimal due to their lack of
rigorous reward mechanisms to enforce evidence purity and their inability to
perform temporal information supplementation beyond pre-sampled frames. To
address this critical gap, we propose a novel evidence-prioritized adaptive
framework built upon our core philosophy: "Select Less, Reason More." Our core
contribution is the evidence-aware reinforcement learning (EARL) framework,
which transforms the model into an active interrogator of evidence. EARL is
precisely engineered to dynamically select the most relevant frames and,
crucially, to perform localized re-sampling around the selected key frames to
access fine-grained temporal detail. Extensive experiments on five demanding
video reasoning benchmarks demonstrate that our EARL-trained model achieves new
state-of-the-art among open-source Video LLMs, simultaneously learning an
effective and high-purity visual evidence selection policy. Impressively, our
7B model achieves 59.8% on LongVideoBench, 69.0% on MVBench and 64.9% on
VideoMME. These results highlight the importance of prioritizing evidence
purity and the effectiveness of our framework.

</details>


### [76] [Semantic4Safety: Causal Insights from Zero-shot Street View Imagery Segmentation for Urban Road Safety](https://arxiv.org/abs/2510.15434)
*Huan Chen,Ting Han,Siyu Chen,Zhihao Guo,Yiping Chen,Meiliu Wu*

Main category: cs.CV

TL;DR: 该研究提出了Semantic4Safety框架，利用街景图像（SVI）和零样本语义分割构建街景指标，并结合预测模型（XGBoost, SHAP）与因果推断（GPS, ATE）分析交通事故，揭示了不同事故类型的异质性因果模式，以支持城市道路安全规划。


<details>
  <summary>Details</summary>
Motivation: 在利用街景图像进行交通风险分析时，存在两大基本挑战：一是如何构建能捕捉事故相关特征的街景指标；二是如何量化这些指标对不同事故类型的因果影响。

Method: 提出了Semantic4Safety框架，该框架应用零样本语义分割技术从街景图像中提取11个可解释的街景指标，并整合道路类型作为上下文信息。使用XGBoost多类别分类器和Shapley Additive Explanations (SHAP) 来解释全局和局部特征贡献。接着，通过广义倾向得分（GPS）加权和平均处理效应（ATE）估计来控制混杂因素并量化因果效应。

Result: 研究结果揭示了异质的、特定于事故类型的因果模式：捕捉场景复杂性、暴露度和路面几何形状的特征在预测能力中占主导地位；较大的可驾驶区域和紧急空间能够降低风险，而过度的视觉开放性反而可能增加风险。

Conclusion: Semantic4Safety框架通过将预测建模与因果推断相结合，为有针对性的干预措施和高风险走廊诊断提供了支持，为城市道路安全规划提供了一个可扩展、数据驱动的工具。

Abstract: Street-view imagery (SVI) offers a fine-grained lens on traffic risk, yet two
fundamental challenges persist: (1) how to construct street-level indicators
that capture accident-related features, and (2) how to quantify their causal
impacts across different accident types. To address these challenges, we
propose Semantic4Safety, a framework that applies zero-shot semantic
segmentation to SVIs to derive 11 interpretable streetscape indicators, and
integrates road type as contextual information to analyze approximately 30,000
accident records in Austin. Specifically, we train an eXtreme Gradient Boosting
(XGBoost) multi-class classifier and use Shapley Additive Explanations (SHAP)
to interpret both global and local feature contributions, and then apply
Generalized Propensity Score (GPS) weighting and Average Treatment Effect (ATE)
estimation to control confounding and quantify causal effects. Results uncover
heterogeneous, accident-type-specific causal patterns: features capturing scene
complexity, exposure, and roadway geometry dominate predictive power; larger
drivable area and emergency space reduce risk, whereas excessive visual
openness can increase it. By bridging predictive modeling with causal
inference, Semantic4Safety supports targeted interventions and high-risk
corridor diagnosis, offering a scalable, data-informed tool for urban road
safety planning.

</details>


### [77] [Lightweight CycleGAN Models for Cross-Modality Image Transformation and Experimental Quality Assessment in Fluorescence Microscopy](https://arxiv.org/abs/2510.15579)
*Mohammad Soltaninezhad,Yashar Rouzbahani,Jhonatan Contreras,Rohan Chippalkatti,Daniel Kwaku Abankwa,Christian Eggeling,Thomas Bocklitz*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级CycleGAN模型，用于荧光显微镜模态转换（共聚焦到超分辨率STED），显著减少了参数量和计算成本，并将其创新性地用作实验和标记质量的诊断工具。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在科学应用中需要降低计算成本和环境影响。在荧光显微镜领域，存在非配对数据集的模态转换挑战。此外，研究人员希望利用GAN作为一种诊断工具，评估实验和标记质量。

Method: 研究者开发了一种轻量级CycleGAN模型。通过将传统U-Net生成器中的通道倍增策略替换为固定通道方法，大幅减少了模型参数。该模型用于将共聚焦图像转换为超分辨率STED/去卷积STED图像。此外，当模型在高质量图像上训练后，通过比较其生成输出与新的实验图像之间的偏差，将其用作检测光漂白、伪影或标记不准确等问题的诊断工具。

Result: 该模型将可训练参数从4180万大幅减少到约9千个。在实现卓越性能的同时，训练速度更快，内存使用量更低。该GAN模型成功地作为诊断工具，通过识别生成输出与新实验图像之间的偏差，揭示了光漂白、伪影或标记不准确等实验问题。

Conclusion: 所提出的轻量级CycleGAN是一个实用的工具，不仅能高效地进行荧光显微镜模态转换，还能有效验证显微镜工作流程中的实验准确性和图像保真度。

Abstract: Lightweight deep learning models offer substantial reductions in
computational cost and environmental impact, making them crucial for scientific
applications. We present a lightweight CycleGAN for modality transfer in
fluorescence microscopy (confocal to super-resolution STED/deconvolved STED),
addressing the common challenge of unpaired datasets. By replacing the
traditional channel-doubling strategy in the U-Net-based generator with a fixed
channel approach, we drastically reduce trainable parameters from 41.8 million
to approximately nine thousand, achieving superior performance with faster
training and lower memory usage. We also introduce the GAN as a diagnostic tool
for experimental and labeling quality. When trained on high-quality images, the
GAN learns the characteristics of optimal imaging; deviations between its
generated outputs and new experimental images can reveal issues such as
photobleaching, artifacts, or inaccurate labeling. This establishes the model
as a practical tool for validating experimental accuracy and image fidelity in
microscopy workflows.

</details>


### [78] [Valeo Near-Field: a novel dataset for pedestrian intent detection](https://arxiv.org/abs/2510.15673)
*Antonyo Musabini,Rachid Benmokhtar,Jagdish Bhanushali,Victor Galizzi,Bertrand Luvison,Xavier Perrotton*

Main category: cs.CV

TL;DR: 本文提出了一个用于检测行人意图的新型多模态数据集，包含鱼眼相机、激光雷达、超声波和3D人体姿态数据，并提供详细标注、基准测试套件和基线性能。


<details>
  <summary>Details</summary>
Motivation: 研究动机是提升智能车辆在近场场景中检测行人意图的能力，解决传感器遮挡、动态环境和硬件限制等实际挑战。

Method: 该研究收集了同步的多模态数据，包括鱼眼相机、激光雷达、超声波传感器和基于动作捕捉的3D身体姿态。数据集提供了与鱼眼相机图像同步的3D身体关节位置和从激光雷达数据提取的精确3D行人位置的详细标注。同时发布了部分数据集、全面的基准测试套件和评估指标，并提供了定制神经网络架构的基线性能指标。

Result: 该数据集为感知算法提供了强大的基准测试资源，有助于开发和评估行人检测、3D姿态估计以及4D轨迹和意图预测领域的先进算法。它能有效应对真实世界中的传感器遮挡、动态环境和硬件限制等挑战。

Conclusion: 该工作旨在为寻求提升智能车辆在近场场景能力的研究人员提供一个独特资源和基础，鼓励数据集的采用和增强，并指明了未来的研究方向。

Abstract: This paper presents a novel dataset aimed at detecting pedestrians'
intentions as they approach an ego-vehicle. The dataset comprises synchronized
multi-modal data, including fisheye camera feeds, lidar laser scans, ultrasonic
sensor readings, and motion capture-based 3D body poses, collected across
diverse real-world scenarios. Key contributions include detailed annotations of
3D body joint positions synchronized with fisheye camera images, as well as
accurate 3D pedestrian positions extracted from lidar data, facilitating robust
benchmarking for perception algorithms. We release a portion of the dataset
along with a comprehensive benchmark suite, featuring evaluation metrics for
accuracy, efficiency, and scalability on embedded systems. By addressing
real-world challenges such as sensor occlusions, dynamic environments, and
hardware constraints, this dataset offers a unique resource for developing and
evaluating state-of-the-art algorithms in pedestrian detection, 3D pose
estimation and 4D trajectory and intention prediction. Additionally, we provide
baseline performance metrics using custom neural network architectures and
suggest future research directions to encourage the adoption and enhancement of
the dataset. This work aims to serve as a foundation for researchers seeking to
advance the capabilities of intelligent vehicles in near-field scenarios.

</details>


### [79] [MAVR-Net: Robust Multi-View Learning for MAV Action Recognition with Cross-View Attention](https://arxiv.org/abs/2510.15448)
*Nengbo Zhang,Hann Woei Ho*

Main category: cs.CV

TL;DR: 本文提出了MAVR-Net，一个基于多视图学习的微型飞行器（MAV）动作识别框架，它结合RGB、光流和分割掩码三种数据类型，并通过跨视图注意力模块和多视图对齐损失，显著提高了MAV运动识别的鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于RGB数据的视觉识别模型难以捕捉MAV运动复杂的时空特性，导致其区分不同动作的能力有限，这阻碍了自主空中集群中的协同感知和控制。

Method: MAVR-Net采用多视图学习方法，结合原始RGB帧、光流和分割掩码三种互补数据。它使用基于ResNet的编码器从每个视图提取特征，并采用多尺度特征金字塔保留时空细节。为增强视图间交互，引入了跨视图注意力模块。此外，设计了多视图对齐损失以确保语义一致性并强化跨视图特征表示。

Result: 实验结果表明，MAVR-Net在基准MAV动作数据集上明显优于现有方法，在Short MAV、Medium MAV和Long MAV数据集上分别达到了97.8%、96.5%和92.8%的准确率。

Conclusion: MAVR-Net通过整合多模态数据和创新的跨视图学习机制，显著提升了微型飞行器动作识别的鲁棒性和准确性，为自主空中集群的协同感知和控制提供了关键技术支持。

Abstract: Recognizing the motion of Micro Aerial Vehicles (MAVs) is crucial for
enabling cooperative perception and control in autonomous aerial swarms. Yet,
vision-based recognition models relying only on RGB data often fail to capture
the complex spatial temporal characteristics of MAV motion, which limits their
ability to distinguish different actions. To overcome this problem, this paper
presents MAVR-Net, a multi-view learning-based MAV action recognition
framework. Unlike traditional single-view methods, the proposed approach
combines three complementary types of data, including raw RGB frames, optical
flow, and segmentation masks, to improve the robustness and accuracy of MAV
motion recognition. Specifically, ResNet-based encoders are used to extract
discriminative features from each view, and a multi-scale feature pyramid is
adopted to preserve the spatiotemporal details of MAV motion patterns. To
enhance the interaction between different views, a cross-view attention module
is introduced to model the dependencies among various modalities and feature
scales. In addition, a multi-view alignment loss is designed to ensure semantic
consistency and strengthen cross-view feature representations. Experimental
results on benchmark MAV action datasets show that our method clearly
outperforms existing approaches, achieving 97.8\%, 96.5\%, and 92.8\% accuracy
on the Short MAV, Medium MAV, and Long MAV datasets, respectively.

</details>


### [80] [Rethinking Convergence in Deep Learning: The Predictive-Corrective Paradigm for Anatomy-Informed Brain MRI Segmentation](https://arxiv.org/abs/2510.15439)
*Feifei Zhang,Zhenhong Jia,Sensen Song,Fei Shi,Dayong Ren*

Main category: cs.CV

TL;DR: 本文提出了一种预测-校正（PC）范式和PCMambaNet网络，通过解耦建模任务并融入领域知识，显著加速了医疗图像分割的学习收敛速度，同时保持了最先进的准确性，尤其适用于数据稀缺场景。


<details>
  <summary>Details</summary>
Motivation: 端到端深度学习范式存在收敛速度慢、严重依赖大规模数据集的问题，这限制了其在医疗影像等数据稀缺领域的效率和适用性。

Method: 本文引入了预测-校正（PC）范式，将建模任务解耦以加速学习。在此范式下，提出了PCMambaNet网络，包含两个协同模块：1) 预测先验模块（PPM），利用解剖学知识（双边对称性）生成诊断相关不对称区域的“焦点图”，提供粗略近似；2) 校正残差网络（CRN），学习建模残差误差，将网络能力集中于精细化挑战区域并描绘精确的病理边界。

Result: 在高质量脑部MRI分割的广泛实验中，PCMambaNet在1-5个epoch内即可收敛，达到了最先进的准确性，这一性能是传统端到端模型无法实现的。这种显著的加速表明，通过明确地整合领域知识来简化学习目标，PCMambaNet有效缓解了数据效率低下和过拟合问题。

Conclusion: 通过明确整合领域知识以简化学习目标，PCMambaNet有效缓解了数据效率低下和过拟合问题，实现了显著的学习加速，并在医疗图像分割等数据稀缺领域展现出卓越的性能和潜力。

Abstract: Despite the remarkable success of the end-to-end paradigm in deep learning,
it often suffers from slow convergence and heavy reliance on large-scale
datasets, which fundamentally limits its efficiency and applicability in
data-scarce domains such as medical imaging. In this work, we introduce the
Predictive-Corrective (PC) paradigm, a framework that decouples the modeling
task to fundamentally accelerate learning. Building upon this paradigm, we
propose a novel network, termed PCMambaNet. PCMambaNet is composed of two
synergistic modules. First, the Predictive Prior Module (PPM) generates a
coarse approximation at low computational cost, thereby anchoring the search
space. Specifically, the PPM leverages anatomical knowledge-bilateral
symmetry-to predict a 'focus map' of diagnostically relevant asymmetric
regions. Next, the Corrective Residual Network (CRN) learns to model the
residual error, focusing the network's full capacity on refining these
challenging regions and delineating precise pathological boundaries. Extensive
experiments on high-resolution brain MRI segmentation demonstrate that
PCMambaNet achieves state-of-the-art accuracy while converging within only 1-5
epochs-a performance unattainable by conventional end-to-end models. This
dramatic acceleration highlights that by explicitly incorporating domain
knowledge to simplify the learning objective, PCMambaNet effectively mitigates
data inefficiency and overfitting.

</details>


### [81] [Towards Label-Free Brain Tumor Segmentation: Unsupervised Learning with Multimodal MRI](https://arxiv.org/abs/2510.15684)
*Gerard Comas-Quiles,Carles Garcia-Cabrera,Julia Dietlmeier,Noel E. O'Connor,Ferran Marques*

Main category: cs.CV

TL;DR: 本文提出了一种多模态视觉Transformer自编码器（MViT-AE）用于脑肿瘤的无监督异常检测，通过在健康MRI上训练并结合多模态融合及SAM后处理，在有限标注数据下实现了临床上有意义的肿瘤定位。


<details>
  <summary>Details</summary>
Motivation: 脑肿瘤分割中，监督学习面临标注数据集有限、成本高昂或不一致的问题，限制了神经影像工作流的可扩展性。因此，需要一种无需手动标注的无监督方法。

Method: 研究者提出了一种多模态视觉Transformer自编码器（MViT-AE），仅在健康脑部MRI上训练，通过重建误差图来检测和定位肿瘤。该方法采用了多模态早晚期融合策略，以利用多序列MRI的互补信息，并通过集成Segment Anything Model（SAM）的后处理流程来优化预测的肿瘤轮廓。

Result: 在BraTS-GoAT 2025 Lighthouse数据集上，该方法在测试集上实现了病灶层面Dice相似系数：全肿瘤0.437，肿瘤核心0.316，增强肿瘤0.350。在验证集上，异常检测率为89.4%。尽管无监督异常检测存在挑战，但该方法实现了临床上有意义的肿瘤定位。

Conclusion: 这些发现强调了基于Transformer的无监督模型在神经肿瘤影像学中作为可扩展、标签高效工具的潜力。

Abstract: Unsupervised anomaly detection (UAD) presents a complementary alternative to
supervised learning for brain tumor segmentation in magnetic resonance imaging
(MRI), particularly when annotated datasets are limited, costly, or
inconsistent. In this work, we propose a novel Multimodal Vision Transformer
Autoencoder (MViT-AE) trained exclusively on healthy brain MRIs to detect and
localize tumors via reconstruction-based error maps. This unsupervised paradigm
enables segmentation without reliance on manual labels, addressing a key
scalability bottleneck in neuroimaging workflows. Our method is evaluated in
the BraTS-GoAT 2025 Lighthouse dataset, which includes various types of tumors
such as gliomas, meningiomas, and pediatric brain tumors. To enhance
performance, we introduce a multimodal early-late fusion strategy that
leverages complementary information across multiple MRI sequences, and a
post-processing pipeline that integrates the Segment Anything Model (SAM) to
refine predicted tumor contours. Despite the known challenges of UAD,
particularly in detecting small or non-enhancing lesions, our method achieves
clinically meaningful tumor localization, with lesion-wise Dice Similarity
Coefficient of 0.437 (Whole Tumor), 0.316 (Tumor Core), and 0.350 (Enhancing
Tumor) on the test set, and an anomaly Detection Rate of 89.4% on the
validation set. These findings highlight the potential of transformer-based
unsupervised models to serve as scalable, label-efficient tools for
neuro-oncological imaging.

</details>


### [82] [DPTrack:Directional Kernel-Guided Prompt Learning for Robust Nighttime Aerial Tracking](https://arxiv.org/abs/2510.15449)
*Zhiqiang Zhu,Xinbo Gao,Wen Lu,Jie Li,Zhaoyang Wang,Mingqian Ge*

Main category: cs.CV

TL;DR: DPTrack是一种基于提示的夜间空中目标跟踪器，它通过将目标属性特征编码到富含细粒度线索的方向性核中，生成精确提示，从而解决了现有方法提示模糊和性能不佳的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的基于提示学习的夜间空中跟踪器仅依赖空间定位监督，无法提供指向目标特征的细粒度线索，导致提示模糊，从而影响跟踪器准确聚焦目标特征的能力，导致性能不佳。

Method: DPTrack首先受视觉仿生学启发，分层捕获目标的拓扑结构，并利用拓扑属性丰富特征表示。随后，编码器将这些拓扑感知特征浓缩成方向性核，该核作为核心引导信号，明确封装了目标的细粒度属性线索。最后，一个基于通道-类别对应属性的核引导提示模块将该核传播到搜索区域的特征上，以精确定位目标特征并将其转换为精确提示，同时整合空间门控以实现鲁棒的夜间跟踪。

Result: 在既定基准上的广泛评估表明，DPTrack展现出卓越的性能。

Conclusion: DPTrack通过编码目标属性特征到方向性核中以生成精确提示，有效解决了现有夜间空中跟踪器提示模糊和性能不足的问题，实现了鲁棒的夜间跟踪。

Abstract: Existing nighttime aerial trackers based on prompt learning rely solely on
spatial localization supervision, which fails to provide fine-grained cues that
point to target features and inevitably produces vague prompts. This limitation
impairs the tracker's ability to accurately focus on the object features and
results in trackers still performing poorly. To address this issue, we propose
DPTrack, a prompt-based aerial tracker designed for nighttime scenarios by
encoding the given object's attribute features into the directional kernel
enriched with fine-grained cues to generate precise prompts. Specifically,
drawing inspiration from visual bionics, DPTrack first hierarchically captures
the object's topological structure, leveraging topological attributes to enrich
the feature representation. Subsequently, an encoder condenses these
topology-aware features into the directional kernel, which serves as the core
guidance signal that explicitly encapsulates the object's fine-grained
attribute cues. Finally, a kernel-guided prompt module built on
channel-category correspondence attributes propagates the kernel across the
features of the search region to pinpoint the positions of target features and
convert them into precise prompts, integrating spatial gating for robust
nighttime tracking. Extensive evaluations on established benchmarks demonstrate
DPTrack's superior performance. Our code will be available at
https://github.com/zzq-vipsl/DPTrack.

</details>


### [83] [Improving Micro-Expression Recognition with Phase-Aware Temporal Augmentation](https://arxiv.org/abs/2510.15466)
*Vu Tram Anh Khuong,Luu Tu Nguyen,Thanh Ha Le,Thi Duyen Ngo*

Main category: cs.CV

TL;DR: 针对微表情识别中数据稀缺和现有方法忽视时间增强的问题，本文提出了一种基于动态图像的双阶段时间增强方法，通过分解微表情序列为起始-高潮和高潮-结束两个阶段，为每个阶段生成独立的动态图像，显著提升了识别性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习在微表情识别（MER）中的应用受限于标注微表情数据集的稀缺性，这不仅阻碍了模型的泛化能力，也限制了训练过程中捕获的运动模式多样性。现有的MER研究主要依赖简单的空间增强，而忽略了能更好利用运动特征的时间增强策略。

Method: 本文提出了一种基于动态图像的阶段感知时间增强方法。该方法不将整个表情编码为一个单一的起始-结束动态图像（DI），而是将每个表情序列分解为两个运动阶段：起始-高潮和高潮-结束。为每个阶段生成一个单独的DI，形成一种双阶段DI增强策略。这些阶段特定的表示丰富了运动多样性，并引入了对识别细微面部过渡至关重要的互补时间线索。

Result: 在CASME-II和SAMM数据集上，使用包括CNN、Vision Transformer和轻量级LEARNet在内的六种深度架构进行了广泛实验，结果表明该方法在识别准确率、未加权F1分数和未加权平均召回率方面均实现了持续的性能提升。当与空间增强结合时，本文方法实现了高达10%的相对改进。

Conclusion: 所提出的增强方法简单、模型无关且在低资源设置下有效，为鲁棒和可泛化的微表情识别提供了一个有前景的方向。

Abstract: Micro-expressions (MEs) are brief, involuntary facial movements that reveal
genuine emotions, typically lasting less than half a second. Recognizing these
subtle expressions is critical for applications in psychology, security, and
behavioral analysis. Although deep learning has enabled significant advances in
micro-expression recognition (MER), its effectiveness is limited by the
scarcity of annotated ME datasets. This data limitation not only hinders
generalization but also restricts the diversity of motion patterns captured
during training. Existing MER studies predominantly rely on simple spatial
augmentations (e.g., flipping, rotation) and overlook temporal augmentation
strategies that can better exploit motion characteristics. To address this gap,
this paper proposes a phase-aware temporal augmentation method based on dynamic
image. Rather than encoding the entire expression as a single onset-to-offset
dynamic image (DI), our approach decomposes each expression sequence into two
motion phases: onset-to-apex and apex-to-offset. A separate DI is generated for
each phase, forming a Dual-phase DI augmentation strategy. These phase-specific
representations enrich motion diversity and introduce complementary temporal
cues that are crucial for recognizing subtle facial transitions. Extensive
experiments on CASME-II and SAMM datasets using six deep architectures,
including CNNs, Vision Transformer, and the lightweight LEARNet, demonstrate
consistent performance improvements in recognition accuracy, unweighted
F1-score, and unweighted average recall, which are crucial for addressing class
imbalance in MER. When combined with spatial augmentations, our method achieves
up to a 10\% relative improvement. The proposed augmentation is simple,
model-agnostic, and effective in low-resource settings, offering a promising
direction for robust and generalizable MER.

</details>


### [84] [MRASfM: Multi-Camera Reconstruction and Aggregation through Structure-from-Motion in Driving Scenes](https://arxiv.org/abs/2510.15467)
*Lingfeng Xuan,Chang Nie,Yiqing Xu,Zhe Liu,Yanzi Miao,Hesheng Wang*

Main category: cs.CV

TL;DR: MRASfM是一种专为多摄像头驾驶场景设计的运动结构恢复（SfM）框架，通过利用多摄像头系统特性、平面模型和优化捆集调整来提高姿态估计的可靠性、路面重建质量和效率，并支持多场景聚合，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 将运动结构恢复（SfM）应用于多摄像头系统捕获的驾驶场景时，存在姿态估计不可靠、路面重建异常点过多以及重建效率低下等显著困难。

Method: 该研究提出了MRASfM框架。它通过在注册过程中利用多摄像头系统固定的空间关系来提高相机姿态估计的可靠性；采用平面模型有效去除三角化路面中的错误点，以改善路面重建质量；在捆集调整（BA）中将多摄像头系统视为一个单一单元，以减少优化变量并提高效率；并通过粗到细的场景关联和组装模块实现多场景聚合。

Result: MRASfM在实际车辆上的多摄像头系统部署验证了其在各种场景下的泛化能力和在挑战条件下的鲁棒性。在公共数据集上的大规模验证结果显示，MRASfM实现了最先进的性能，例如在nuScenes数据集上达到了0.124的绝对姿态误差。

Conclusion: MRASfM框架成功解决了多摄像头驾驶场景中SfM面临的姿态估计不可靠、路面重建异常点和效率低下等问题，并通过其独特的优化策略和多场景聚合能力，在实际应用和公共数据集上均展现出卓越的性能和普适性。

Abstract: Structure from Motion (SfM) estimates camera poses and reconstructs point
clouds, forming a foundation for various tasks. However, applying SfM to
driving scenes captured by multi-camera systems presents significant
difficulties, including unreliable pose estimation, excessive outliers in road
surface reconstruction, and low reconstruction efficiency. To address these
limitations, we propose a Multi-camera Reconstruction and Aggregation
Structure-from-Motion (MRASfM) framework specifically designed for driving
scenes. MRASfM enhances the reliability of camera pose estimation by leveraging
the fixed spatial relationships within the multi-camera system during the
registration process. To improve the quality of road surface reconstruction,
our framework employs a plane model to effectively remove erroneous points from
the triangulated road surface. Moreover, treating the multi-camera set as a
single unit in Bundle Adjustment (BA) helps reduce optimization variables to
boost efficiency. In addition, MRASfM achieves multi-scene aggregation through
scene association and assembly modules in a coarse-to-fine fashion. We deployed
multi-camera systems on actual vehicles to validate the generalizability of
MRASfM across various scenes and its robustness in challenging conditions
through real-world applications. Furthermore, large-scale validation results on
public datasets show the state-of-the-art performance of MRASfM, achieving
0.124 absolute pose error on the nuScenes dataset.

</details>


### [85] [NDM: A Noise-driven Detection and Mitigation Framework against Implicit Sexual Intentions in Text-to-Image Generation](https://arxiv.org/abs/2510.15752)
*Yitong Sun,Yao Huang,Ruochen Zhang,Huanran Chen,Shouwei Ruan,Ranjie Duan,Xingxing Wei*

Main category: cs.CV

TL;DR: 本文提出NDM，一个噪声驱动的检测和缓解框架，用于在不损害生成模型质量的前提下，检测并缓解文本到图像（T2I）扩散模型中由隐式提示引发的不当内容生成问题。


<details>
  <summary>Details</summary>
Motivation: 文本到图像扩散模型容易生成不当内容，尤其是在面对隐式性暗示提示时。这些隐式提示由于模型偏见可能意外触发色情内容，引发伦理担忧。现有检测方法主要针对显式内容，难以识别隐式线索，而微调方法又可能降低模型的生成质量。

Method: NDM框架包含两项创新：1. 基于噪声的检测方法：利用早期预测噪声的可分离性，高精度、高效率地识别恶意内容。2. 噪声增强自适应负向引导机制：通过抑制突出区域的注意力来优化初始噪声，从而增强自适应负向引导在性内容缓解中的有效性。

Result: NDM在自然和对抗性数据集上均得到了验证，实验结果表明其性能优于现有的SOTA方法，包括SLD、UCE和RECE等。

Conclusion: NDM是第一个噪声驱动的检测和缓解框架，能够有效地检测并缓解T2I生成中的隐式恶意意图，同时保留模型原有的生成能力。

Abstract: Despite the impressive generative capabilities of text-to-image (T2I)
diffusion models, they remain vulnerable to generating inappropriate content,
especially when confronted with implicit sexual prompts. Unlike explicit
harmful prompts, these subtle cues, often disguised as seemingly benign terms,
can unexpectedly trigger sexual content due to underlying model biases, raising
significant ethical concerns. However, existing detection methods are primarily
designed to identify explicit sexual content and therefore struggle to detect
these implicit cues. Fine-tuning approaches, while effective to some extent,
risk degrading the model's generative quality, creating an undesirable
trade-off. To address this, we propose NDM, the first noise-driven detection
and mitigation framework, which could detect and mitigate implicit malicious
intention in T2I generation while preserving the model's original generative
capabilities. Specifically, we introduce two key innovations: first, we
leverage the separability of early-stage predicted noise to develop a
noise-based detection method that could identify malicious content with high
accuracy and efficiency; second, we propose a noise-enhanced adaptive negative
guidance mechanism that could optimize the initial noise by suppressing the
prominent region's attention, thereby enhancing the effectiveness of adaptive
negative guidance for sexual mitigation. Experimentally, we validate NDM on
both natural and adversarial datasets, demonstrating its superior performance
over existing SOTA methods, including SLD, UCE, and RECE, etc. Code and
resources are available at https://github.com/lorraine021/NDM.

</details>


### [86] [MSAM: Multi-Semantic Adaptive Mining for Cross-Modal Drone Video-Text Retrieval](https://arxiv.org/abs/2510.15470)
*Jinghao Huang,Yaxiong Chen,Ganchao Liu*

Main category: cs.CV

TL;DR: 本文首次系统地提出了无人机视频-文本检索（DVTR）任务，并针对无人机视频的独特挑战，提出了一种名为多语义自适应挖掘（MSAM）的新方法，通过多语义学习和跨模态交互特征融合来提高检索性能。


<details>
  <summary>Details</summary>
Motivation: 随着无人机技术的发展，视频数据量激增，对高效的语义检索需求迫切。现有跨模态方法主要针对地面视角设计，难以有效处理无人机视频的俯视视角、强结构同质性和多样化的目标组合语义表达等特点，因此需要专门的检索机制。

Method: 本文提出了一种名为多语义自适应挖掘（MSAM）的方法。MSAM引入了多语义自适应学习机制，该机制结合帧间动态变化并从特定场景区域提取丰富的语义信息。它通过词语与无人机视频帧之间的细粒度交互，集成了自适应语义构建模块、分布驱动语义学习项和多样性语义项。此外，为减少复杂背景干扰，MSAM引入了跨模态交互特征融合池化机制，专注于目标区域的特征提取和匹配。

Result: 在两个自建的无人机视频-文本数据集上进行的广泛实验表明，MSAM在无人机视频-文本检索任务中优于其他现有方法。

Conclusion: 本文首次系统地提出了无人机视频-文本检索任务，并针对其特有挑战，提出了一种新颖的MSAM方法。MSAM通过多语义自适应学习和聚焦目标区域的特征融合，有效增强了无人机视频内容的深度理解和推理，显著提升了检索性能。

Abstract: With the advancement of drone technology, the volume of video data increases
rapidly, creating an urgent need for efficient semantic retrieval. We are the
first to systematically propose and study the drone video-text retrieval (DVTR)
task. Drone videos feature overhead perspectives, strong structural
homogeneity, and diverse semantic expressions of target combinations, which
challenge existing cross-modal methods designed for ground-level views in
effectively modeling their characteristics. Therefore, dedicated retrieval
mechanisms tailored for drone scenarios are necessary. To address this issue,
we propose a novel approach called Multi-Semantic Adaptive Mining (MSAM). MSAM
introduces a multi-semantic adaptive learning mechanism, which incorporates
dynamic changes between frames and extracts rich semantic information from
specific scene regions, thereby enhancing the deep understanding and reasoning
of drone video content. This method relies on fine-grained interactions between
words and drone video frames, integrating an adaptive semantic construction
module, a distribution-driven semantic learning term and a diversity semantic
term to deepen the interaction between text and drone video modalities and
improve the robustness of feature representation. To reduce the interference of
complex backgrounds in drone videos, we introduce a cross-modal interactive
feature fusion pooling mechanism that focuses on feature extraction and
matching in target regions, minimizing noise effects. Extensive experiments on
two self-constructed drone video-text datasets show that MSAM outperforms other
existing methods in the drone video-text retrieval task. The source code and
dataset will be made publicly available.

</details>


### [87] [A Novel Combined Optical Flow Approach for Comprehensive Micro-Expression Recognition](https://arxiv.org/abs/2510.15471)
*Vu Tram Anh Khuong,Thi Bich Phuong Man,Luu Tu Nguyen,Thanh Ha Le,Thi Duyen Ngo*

Main category: cs.CV

TL;DR: 本研究提出一种结合了微表情起始至高潮和高潮至结束两个阶段的光流（COF）方法，以更全面地捕捉面部微表情动态，并在实验中表现优于单一光流方法。


<details>
  <summary>Details</summary>
Motivation: 现有大多数基于光流的微表情识别（MER）方法仅关注起始至高潮阶段，忽略了包含关键时间动态的高潮至结束阶段。

Method: 引入了一种结合光流（COF），它整合了微表情的起始至高潮和高潮至结束两个阶段，以提供更全面的运动分析，从而增强特征表示。

Result: 在CASMEII和SAMM数据集上的实验结果表明，COF方法优于单一基于光流的方法，证明了其在捕捉微表情动态方面的有效性。

Conclusion: 结合光流（COF）方法能有效捕捉微表情的动态，通过整合两个关键阶段，显著提升了微表情识别的性能。

Abstract: Facial micro-expressions are brief, involuntary facial movements that reveal
hidden emotions. Most Micro-Expression Recognition (MER) methods that rely on
optical flow typically focus on the onset-to-apex phase, neglecting the
apex-to-offset phase, which holds key temporal dynamics. This study introduces
a Combined Optical Flow (COF), integrating both phases to enhance feature
representation. COF provides a more comprehensive motion analysis, improving
MER performance. Experimental results on CASMEII and SAMM datasets show that
COF outperforms single optical flow-based methods, demonstrating its
effectiveness in capturing micro-expression dynamics.

</details>


### [88] [Semantic segmentation with coarse annotations](https://arxiv.org/abs/2510.15756)
*Jort de Jong,Mike Holenderski*

Main category: cs.CV

TL;DR: 本文提出了一种基于超像素的正则化方法，用于处理粗略标注下的语义分割任务，尤其针对编码器-解码器架构模型，以显著改善边界对齐。


<details>
  <summary>Details</summary>
Motivation: 获取精细的像素级标注成本高昂或难度大。当仅能获得粗略标注（例如，类别边界处像素未标注）时，进行语义分割，特别是优化类别间边界的对齐，变得十分困难。

Method: 本文提出了一种针对带有超像素上采样功能的编码器-解码器架构模型的正则化方法。该方法鼓励解码图像中的分割像素与基于像素颜色和位置的SLIC超像素保持一致，且独立于分割标注。该方法应用于FCN-16全卷积网络架构。

Result: 在SUIM、Cityscapes和PanNuke数据集上进行评估，结果表明，与使用粗略标注训练的现有最先进模型相比，本文方法显著提高了边界召回率。

Conclusion: 所提出的基于超像素的正则化方法能有效解决粗略标注下语义分割的挑战，特别是在改善类别边界对齐方面表现出色。

Abstract: Semantic segmentation is the task of classifying each pixel in an image.
Training a segmentation model achieves best results using annotated images,
where each pixel is annotated with the corresponding class. When obtaining fine
annotations is difficult or expensive, it may be possible to acquire coarse
annotations, e.g. by roughly annotating pixels in an images leaving some pixels
around the boundaries between classes unlabeled. Segmentation with coarse
annotations is difficult, in particular when the objective is to optimize the
alignment of boundaries between classes. This paper proposes a regularization
method for models with an encoder-decoder architecture with superpixel based
upsampling. It encourages the segmented pixels in the decoded image to be
SLIC-superpixels, which are based on pixel color and position, independent of
the segmentation annotation. The method is applied to FCN-16 fully
convolutional network architecture and evaluated on the SUIM, Cityscapes, and
PanNuke data sets. It is shown that the boundary recall improves significantly
compared to state-of-the-art models when trained on coarse annotations.

</details>


### [89] [Controlling the image generation process with parametric activation functions](https://arxiv.org/abs/2510.15778)
*Ilia Pavlov*

Main category: cs.CV

TL;DR: 本文提出一个交互系统，允许用户通过替换生成网络中的激活函数为参数化函数并设置其参数，以可解释的方式理解和控制生成模型的输出。


<details>
  <summary>Details</summary>
Motivation: 随着图像生成模型保真度和普及度的提高，缺乏能够以可解释方式直接与其内部机制交互的工具。

Method: 引入一个系统，使用户能够将生成网络的激活函数替换为参数化函数，并设置这些函数的参数，从而提供一种控制网络输出的新方法。

Result: 该方法在StyleGAN2（训练于FFHQ数据集）和BigGAN（训练于ImageNet数据集）网络上得到了应用验证。

Conclusion: 提供了一种通过交互和实验来控制生成网络输出并更好地理解模型的替代方法。

Abstract: As image generative models continue to increase not only in their fidelity
but also in their ubiquity the development of tools that leverage direct
interaction with their internal mechanisms in an interpretable way has received
little attention In this work we introduce a system that allows users to
develop a better understanding of the model through interaction and
experimentation By giving users the ability to replace activation functions of
a generative network with parametric ones and a way to set the parameters of
these functions we introduce an alternative approach to control the networks
output We demonstrate the use of our method on StyleGAN2 and BigGAN networks
trained on FFHQ and ImageNet respectively.

</details>


### [90] [Iterative Motion Compensation for Canonical 3D Reconstruction from UAV Plant Images Captured in Windy Conditions](https://arxiv.org/abs/2510.15491)
*Andre Rochow,Jonas Marcic,Svetlana Seliunina,Sven Behnke*

Main category: cs.CV

TL;DR: 该论文提出了一种利用无人机进行植物高精度3D表型分析的流水线，通过迭代变形方法解决了环境风和无人机下洗气流导致的叶片运动问题，从而生成高质量的3D重建。


<details>
  <summary>Details</summary>
Motivation: 3D植物表型分析对于理解植物生长、预测产量和疾病控制至关重要。然而，现有方法在图像采集过程中面临环境风和无人机下洗气流引起的叶片运动挑战，影响了重建质量。

Method: 该研究使用小型商用无人机采集图像，并开发了一个自主控制的Android应用程序（除了放置ArUco标记外）。为解决叶片运动问题，提出了一种迭代方法：通过变形逐步调整输入图像，利用原始图像和从中间3D重建渲染的图像之间的光流估计运动，从而逐渐减少场景运动，得到规范表示。该流水线支持集成任意先进的3D重建方法。

Result: 经过几次迭代后，该流水线显著改善了现有先进方法的重建质量，并能够提取高分辨率的3D网格。研究团队将公开发布重建流水线的源代码以及一个包含多种作物、不同时间点捕获的植物数据集。

Conclusion: 该研究成功开发了一个能够生成高质量个体农作物3D重建的流水线，有效解决了无人机图像采集过程中因环境风和下洗气流引起的运动挑战。通过开源代码和数据集，为植物表型分析领域提供了宝贵的工具和资源。

Abstract: 3D phenotyping of plants plays a crucial role for understanding plant growth,
yield prediction, and disease control. We present a pipeline capable of
generating high-quality 3D reconstructions of individual agricultural plants.
To acquire data, a small commercially available UAV captures images of a
selected plant. Apart from placing ArUco markers, the entire image acquisition
process is fully autonomous, controlled by a self-developed Android application
running on the drone's controller. The reconstruction task is particularly
challenging due to environmental wind and downwash of the UAV. Our proposed
pipeline supports the integration of arbitrary state-of-the-art 3D
reconstruction methods. To mitigate errors caused by leaf motion during image
capture, we use an iterative method that gradually adjusts the input images
through deformation. Motion is estimated using optical flow between the
original input images and intermediate 3D reconstructions rendered from the
corresponding viewpoints. This alignment gradually reduces scene motion,
resulting in a canonical representation. After a few iterations, our pipeline
improves the reconstruction of state-of-the-art methods and enables the
extraction of high-resolution 3D meshes. We will publicly release the source
code of our reconstruction pipeline. Additionally, we provide a dataset
consisting of multiple plants from various crops, captured across different
points in time.

</details>


### [91] [Rethinking Efficient Hierarchical Mixing Architecture for Low-light RAW Image Enhancement](https://arxiv.org/abs/2510.15497)
*Xianmin Chen,Peiliang Huang,Longfei Han,Dingwen Zhang,Junwei Han*

Main category: cs.CV

TL;DR: 本文提出了一种名为HiMA的分层混合架构，结合Transformer和Mamba模块，用于高效低光RAW图像增强。通过引入LoDA和MPF模块，HiMA能有效处理不均匀光照和细节增强，在多个公开数据集上超越现有SOTA方法，并具有更少的参数。


<details>
  <summary>Details</summary>
Motivation: 低光RAW图像增强是一个挑战性任务，现有深度学习方法在实现强增强质量和高效率之间存在固有限制。主要挑战在于如何同时实现这两点。

Method: 本文提出了分层混合架构（HiMA），它结合Transformer和Mamba模块，分别处理大尺度和小尺度特征以提高效率。为解决局部光照不均，提出了局部分布调整（LoDA）模块，自适应对齐不同局部区域的特征分布。此外，设计了多先验融合（MPF）模块，整合空间域和频域先验以增强细节。

Result: 在多个公开数据集上的大量实验表明，该方法优于现有的最先进方法，以更少的参数实现了卓越的性能。

Conclusion: HiMA架构通过有效结合Transformer和Mamba，以及LoDA和MPF模块，显著提升了低光RAW图像增强的质量和效率，并以更精简的模型实现了SOTA性能。

Abstract: Low-light RAW image enhancement remains a challenging task. Although numerous
deep learning based approaches have been proposed, they still suffer from
inherent limitations. A key challenge is how to simultaneously achieve strong
enhancement quality and high efficiency. In this paper, we rethink the
architecture for efficient low-light image signal processing (ISP) and
introduce a Hierarchical Mixing Architecture (HiMA). HiMA leverages the
complementary strengths of Transformer and Mamba modules to handle features at
large and small scales, respectively, thereby improving efficiency while
avoiding the ambiguities observed in prior two-stage frameworks. To further
address uneven illumination with strong local variations, we propose Local
Distribution Adjustment (LoDA), which adaptively aligns feature distributions
across different local regions. In addition, to fully exploit the denoised
outputs from the first stage, we design a Multi-prior Fusion (MPF) module that
integrates spatial and frequency-domain priors for detail enhancement.
Extensive experiments on multiple public datasets demonstrate that our method
outperforms state-of-the-art approaches, achieving superior performance with
fewer parameters. Code will be released at https://github.com/Cynicarlos/HiMA.

</details>


### [92] [Latent Feature Alignment: Discovering Biased and Interpretable Subpopulations in Face Recognition Models](https://arxiv.org/abs/2510.15520)
*Ignacio Serna*

Main category: cs.CV

TL;DR: 本文提出了一种名为潜在特征对齐（LFA）的无属性标签算法，用于在人脸识别模型中识别和解释有偏见的子群体，该算法通过发现语义连贯的潜在方向，优于传统聚类方法。


<details>
  <summary>Details</summary>
Motivation: 现代人脸识别模型存在系统性偏差，影响特定子群体。传统的偏差评估框架依赖于昂贵且有限的标注属性来形成子群体，因此需要一种无需预定义属性标注的方法来识别和解释这些有偏见的子群体。

Method: 引入了潜在特征对齐（LFA）算法，这是一种无属性标签的方法，通过使用潜在方向来识别子群体。该方法旨在实现语义连贯的分组（比基于邻近度的方法更可靠地将共享共同属性的面孔分组）和可解释的方向发现（对应于年龄、种族或着装等语义属性）。

Result: 在四种最先进的识别模型（ArcFace, CosFace, ElasticFace, PartialFC）和两个基准（RFW, CelebA）上，LFA在组内语义连贯性方面始终优于k-means和最近邻搜索。它还揭示了与人口统计和上下文属性对齐的可解释潜在方向。

Conclusion: LFA是一种实用的人脸识别模型表示审计方法，使从业者无需预定义属性标注即可识别和解释有偏见的子群体。

Abstract: Modern face recognition models achieve high overall accuracy but continue to
exhibit systematic biases that disproportionately affect certain
subpopulations. Conventional bias evaluation frameworks rely on labeled
attributes to form subpopulations, which are expensive to obtain and limited to
predefined categories. We introduce Latent Feature Alignment (LFA), an
attribute-label-free algorithm that uses latent directions to identify
subpopulations. This yields two main benefits over standard clustering: (i)
semantically coherent grouping, where faces sharing common attributes are
grouped together more reliably than by proximity-based methods, and (ii)
discovery of interpretable directions, which correspond to semantic attributes
such as age, ethnicity, or attire. Across four state-of-the-art recognition
models (ArcFace, CosFace, ElasticFace, PartialFC) and two benchmarks (RFW,
CelebA), LFA consistently outperforms k-means and nearest-neighbor search in
intra-group semantic coherence, while uncovering interpretable latent
directions aligned with demographic and contextual attributes. These results
position LFA as a practical method for representation auditing of face
recognition models, enabling practitioners to identify and interpret biased
subpopulations without predefined attribute annotations.

</details>


### [93] [Balanced Multi-Task Attention for Satellite Image Classification: A Systematic Approach to Achieving 97.23% Accuracy on EuroSAT Without Pre-Training](https://arxiv.org/abs/2510.15527)
*Aditya Vir*

Main category: cs.CV

TL;DR: 本文提出了一种自定义卷积神经网络架构，通过引入平衡多任务注意力机制，在EuroSAT数据集上实现了97.23%的卫星土地利用分类准确率，且无需预训练模型。


<details>
  <summary>Details</summary>
Motivation: 研究旨在系统性地调查自定义CNN架构在卫星土地利用分类中的潜力，并解决现有分类方法中特定的失效模式，同时避免依赖预训练模型。

Method: 研究采用渐进式架构迭代，从基线模型（94.30%）到CBAM增强模型（95.98%），最终提出并实现了平衡多任务注意力机制（97.23%）。该机制结合了用于空间特征提取的坐标注意力（Coordinate Attention）和用于光谱特征提取的Squeeze-Excitation模块，并通过可学习的融合参数进行统一。此外，还采用了渐进式DropBlock正则化和类别平衡损失加权来解决过拟合和混淆模式不平衡问题。

Result: 最终的12层架构在EuroSAT数据集上达到了97.23%的测试准确率，Cohen's Kappa系数为0.9692，所有类别的准确率均超过94.46%。可学习的融合参数自动收敛到约0.57，表明空间和光谱模态对卫星图像分类具有近乎同等的重要性。该方法在无需外部数据的情况下，性能与微调后的ResNet-50（98.57%）相差1.34%。

Conclusion: 系统性的架构设计，特别是新颖的平衡多任务注意力机制，在卫星土地利用分类等特定领域应用中表现出高效性，无需预训练模型或外部数据即可实现高水平性能，验证了其有效性。

Abstract: This work presents a systematic investigation of custom convolutional neural
network architectures for satellite land use classification, achieving 97.23%
test accuracy on the EuroSAT dataset without reliance on pre-trained models.
Through three progressive architectural iterations (baseline: 94.30%,
CBAM-enhanced: 95.98%, and balanced multi-task attention: 97.23%) we identify
and address specific failure modes in satellite imagery classification. Our
principal contribution is a novel balanced multi-task attention mechanism that
combines Coordinate Attention for spatial feature extraction with
Squeeze-Excitation blocks for spectral feature extraction, unified through a
learnable fusion parameter. Experimental results demonstrate that this
learnable parameter autonomously converges to alpha approximately 0.57,
indicating near-equal importance of spatial and spectral modalities for
satellite imagery. We employ progressive DropBlock regularization (5-20% by
network depth) and class-balanced loss weighting to address overfitting and
confusion pattern imbalance. The final 12-layer architecture achieves Cohen's
Kappa of 0.9692 with all classes exceeding 94.46% accuracy, demonstrating
confidence calibration with a 24.25% gap between correct and incorrect
predictions. Our approach achieves performance within 1.34% of fine-tuned
ResNet-50 (98.57%) while requiring no external data, validating the efficacy of
systematic architectural design for domain-specific applications. Complete
code, trained models, and evaluation scripts are publicly available.

</details>


### [94] [Shakti-VLMs: Scalable Vision-Language Models for Enterprise AI](https://arxiv.org/abs/2502.17092)
*Syed Abdul Gaffar Shakhadri,Kruthika KR,Kartik Basavaraj Angadi*

Main category: cs.CV

TL;DR: Shakti VLM是一个1B和4B参数的视觉-语言模型家族，通过架构创新和三阶段训练策略，实现了数据高效的多模态学习，在减少训练数据量的情况下仍能达到有竞争力的性能。


<details>
  <summary>Details</summary>
Motivation: 当前的视觉-语言模型（VLM）虽然性能强大，但依赖于庞大的训练数据。本研究旨在解决多模态学习中的数据效率挑战，通过模型设计而非纯粹的数据量来提升性能。

Method: Shakti VLM采用了多项架构创新，包括用于注意力稳定性的QK-Normalization、混合归一化技术和增强的位置编码。此外，模型还采用了一个三阶段的训练策略来优化学习效率。

Result: 评估结果表明，Shakti-VLM-1B和Shakti-VLM-4B在文档理解、视觉推理、OCR提取和通用多模态推理等任务中表现出色。这证明了通过模型设计和训练策略而非大量数据也能实现高性能。

Conclusion: 研究得出结论，通过精巧的模型设计和优化的训练策略，可以实现高性能的多模态学习，而不仅仅依赖于数据量。这使得Shakti VLM成为企业级多模态任务的有效且高效的解决方案。

Abstract: We introduce Shakti VLM, a family of vision-language models in the capacity
of 1B and 4B parameters designed to address data efficiency challenges in
multimodal learning. While recent VLMs achieve strong performance through
extensive training data, Shakti models leverage architectural innovations to
attain competitive results with fewer tokens. Key advancements include
QK-Normalization for attention stability, hybrid normalization techniques, and
enhanced positional encoding. A three-stage training strategy further optimizes
learning efficiency. Evaluations show that Shakti-Shakti-VLM-1B and
Shakti-VLM-4B excel in document understanding, Visual Reasoning, OCR
extraction, and general multimodal reasoning. Our results highlight that high
performance can be achieved through model design and training strategy rather
than sheer data volume, making Shakti an efficient solution for
enterprise-scale multimodal tasks.

</details>


### [95] [OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding LLM](https://arxiv.org/abs/2510.15870)
*Hanrong Ye,Chao-Han Huck Yang,Arushi Goel,Wei Huang,Ligeng Zhu,Yuanhang Su,Sean Lin,An-Chieh Cheng,Zhen Wan,Jinchuan Tian,Yuming Lou,Dong Yang,Zhijian Liu,Yukang Chen,Ambrish Dantrey,Ehsan Jahangiri,Sreyan Ghosh,Daguang Xu,Ehsan Hosseini-Asl,Danial Mohseni Taheri,Vidya Murali,Sifei Liu,Jason Lu,Oluwatobi Olabiyi,Frank Wang,Rafael Valle,Bryan Catanzaro,Andrew Tao,Song Han,Jan Kautz,Hongxu Yin,Pavlo Molchanov*

Main category: cs.CV

TL;DR: OmniVinci是一个开源全模态大型语言模型（LLM），通过创新的架构设计（OmniAlignNet、时间嵌入分组、受限旋转时间嵌入）和24M对话数据生成管道，显著提升了跨模态感知和推理能力，并在多个基准测试中超越现有模型，同时大幅减少了训练token。它还在机器人、医疗AI和智能工厂等下游应用中展现了全模态优势。


<details>
  <summary>Details</summary>
Motivation: 推动机器智能发展需要开发多模态感知能力，以模拟人类感知世界的方式。

Method: 该研究引入了OmniVinci，一个开源全模态LLM。其方法包括：
1.  **模型架构创新**：
    *   **OmniAlignNet**：强化视觉和音频嵌入在共享全模态潜在空间中的对齐。
    *   **Temporal Embedding Grouping**：捕获视觉和音频信号间的相对时间对齐。
    *   **Constrained Rotary Time Embedding**：编码全模态嵌入中的绝对时间信息。
2.  **数据策划与合成**：开发了一个生成2400万单模态和全模态对话的管道。

Result: 1.  模态在感知和推理中相互增强。
2.  OmniVinci在跨模态理解（DailyOmni）上比Qwen2.5-Omni高出+19.05分，在音频（MMAR）上高出+1.7分，在视觉（Video-MME）上高出+3.9分。
3.  训练token仅为0.2T，比Qwen2.5-Omni的1.2T减少了6倍。
4.  在机器人、医疗AI和智能工厂等下游应用中展示了全模态优势。

Conclusion: OmniVinci通过其创新的模型架构和高效的数据策略，成功构建了一个强大的开源全模态LLM，显著提升了多模态感知和推理能力，并在实际应用中展现出其优越性和潜力。

Abstract: Advancing machine intelligence requires developing the ability to perceive
across multiple modalities, much as humans sense the world. We introduce
OmniVinci, an initiative to build a strong, open-source, omni-modal LLM. We
carefully study the design choices across model architecture and data curation.
For model architecture, we present three key innovations: (i) OmniAlignNet for
strengthening alignment between vision and audio embeddings in a shared
omni-modal latent space; (ii) Temporal Embedding Grouping for capturing
relative temporal alignment between vision and audio signals; and (iii)
Constrained Rotary Time Embedding for encoding absolute temporal information in
omni-modal embeddings. We introduce a curation and synthesis pipeline that
generates 24M single-modal and omni-modal conversations. We find that
modalities reinforce one another in both perception and reasoning. Our model,
OmniVinci, outperforms Qwen2.5-Omni with +19.05 on DailyOmni (cross-modal
understanding), +1.7 on MMAR (audio), and +3.9 on Video-MME (vision), while
using just 0.2T training tokens - a 6 times reduction compared to
Qwen2.5-Omni's 1.2T. We finally demonstrate omni-modal advantages in downstream
applications spanning robotics, medical AI, and smart factory.

</details>


### [96] [Diffusion Bridge Networks Simulate Clinical-grade PET from MRI for Dementia Diagnostics](https://arxiv.org/abs/2510.15556)
*Yitong Li,Ralph Buchert,Benita Schmitz-Koep,Timo Grimmer,Björn Ommer,Dennis M. Hedderich,Igor Yakushev,Christian Wachinger*

Main category: cs.CV

TL;DR: SiM2P是一个基于3D扩散桥的框架，能从MRI和辅助信息模拟FDG-PET图像，显著提高了痴呆症的诊断准确性，并提升了FDG-PET的可及性。


<details>
  <summary>Details</summary>
Motivation: FDG-PET在痴呆症诊断中是既定工具，但相比MRI，其可及性较低且成本高昂。研究旨在开发一种方法，使FDG-PET的诊断优势更易于获得。

Method: 研究提出了SiM2P，一个基于3D扩散桥的框架，学习从MRI和辅助患者信息到FDG-PET图像的概率映射。通过一项盲法临床读者研究进行验证，两名神经放射科医生和两名核医学医师对阿尔茨海默病、行为变异型额颞叶痴呆患者以及认知健康对照组的原始MRI和SiM2P模拟PET图像进行了评估。此外，还开发了一个实用的部署工作流程，仅需少量站点特定病例和基本人口统计信息。

Result: SiM2P将区分三个组的总体诊断准确性从75.0%显著提高到84.7% (p<0.05)。模拟的PET图像获得了更高的诊断确定性评级，并且比MRI图像实现了更高的判读间一致性。

Conclusion: SiM2P框架使FDG-PET成像的既定诊断优势更易于痴呆症患者获得，有可能在资源有限的环境中改善早期检测和鉴别诊断。

Abstract: Positron emission tomography (PET) with 18F-Fluorodeoxyglucose (FDG) is an
established tool in the diagnostic workup of patients with suspected dementing
disorders. However, compared to the routinely available magnetic resonance
imaging (MRI), FDG-PET remains significantly less accessible and substantially
more expensive. Here, we present SiM2P, a 3D diffusion bridge-based framework
that learns a probabilistic mapping from MRI and auxiliary patient information
to simulate FDG-PET images of diagnostic quality. In a blinded clinical reader
study, two neuroradiologists and two nuclear medicine physicians rated the
original MRI and SiM2P-simulated PET images of patients with Alzheimer's
disease, behavioral-variant frontotemporal dementia, and cognitively healthy
controls. SiM2P significantly improved the overall diagnostic accuracy of
differentiating between three groups from 75.0% to 84.7% (p<0.05). Notably, the
simulated PET images received higher diagnostic certainty ratings and achieved
superior interrater agreement compared to the MRI images. Finally, we developed
a practical workflow for local deployment of the SiM2P framework. It requires
as few as 20 site-specific cases and only basic demographic information. This
approach makes the established diagnostic benefits of FDG-PET imaging more
accessible to patients with suspected dementing disorders, potentially
improving early detection and differential diagnosis in resource-limited
settings. Our code is available at https://github.com/Yiiitong/SiM2P.

</details>


### [97] [Unmasking Facial DeepFakes: A Robust Multiview Detection Framework for Natural Images](https://arxiv.org/abs/2510.15576)
*Sami Belguesmia,Mohand Saïd Allili,Assia Hamadene*

Main category: cs.CV

TL;DR: 该论文提出了一种多视角深度伪造检测架构，通过结合全局、中间、局部和面部姿态编码器，有效处理姿态变化和遮挡等挑战，提高了真实感合成图像的检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测方法在处理真实世界中常见的姿态变化、遮挡和难以检测的伪影时表现不佳。

Method: 提出了一种多视角架构，包含：1. 全局视角编码器（检测边界不一致性）；2. 中间视角编码器（分析纹理和颜色对齐）；3. 局部视角编码器（捕捉眼睛、鼻子、嘴巴等表情区域的失真）；4. 面部姿态编码器（分类面部姿态以确保各种视角下的鲁棒检测）。通过融合这些编码器的特征实现检测。

Result: 在具有挑战性的数据集上，该方法在检测被操纵图像方面表现出卓越的性能，即使在具有挑战性的姿态和光照条件下，也优于传统的单视角方法。

Conclusion: 所提出的多视角架构通过多层次特征分析和姿态鲁棒性，显著增强了深度伪造检测能力，克服了现有方法的局限性。

Abstract: DeepFake technology has advanced significantly in recent years, enabling the
creation of highly realistic synthetic face images. Existing DeepFake detection
methods often struggle with pose variations, occlusions, and artifacts that are
difficult to detect in real-world conditions. To address these challenges, we
propose a multi-view architecture that enhances DeepFake detection by analyzing
facial features at multiple levels. Our approach integrates three specialized
encoders, a global view encoder for detecting boundary inconsistencies, a
middle view encoder for analyzing texture and color alignment, and a local view
encoder for capturing distortions in expressive facial regions such as the
eyes, nose, and mouth, where DeepFake artifacts frequently occur. Additionally,
we incorporate a face orientation encoder, trained to classify face poses,
ensuring robust detection across various viewing angles. By fusing features
from these encoders, our model achieves superior performance in detecting
manipulated images, even under challenging pose and lighting
conditions.Experimental results on challenging datasets demonstrate the
effectiveness of our method, outperforming conventional single-view approaches

</details>


### [98] [Standardization for improved Spatio-Temporal Image Fusion](https://arxiv.org/abs/2510.15589)
*Harkaitz Goyena,Peter M. Atkinson,Unai Pérez-Goya,M. Dolores Ugarte*

Main category: cs.CV

TL;DR: 本文提出了两种图像标准化方法（传统上采样和基于异常的卫星图像标准化ABSIS），以解决时空图像融合（STIF）方法对匹配分辨率图像的需求，并显著提高了融合精度。


<details>
  <summary>Details</summary>
Motivation: 时空图像融合（STIF）方法通常要求不同传感器捕获的图像具有匹配的空间和光谱分辨率，这限制了其应用。因此，需要一种方法来标准化这些图像。

Method: 研究提出了两种不同的标准化方法：1) 传统的精细分辨率图像上采样；2) 一种名为“基于异常的卫星图像标准化”（ABSIS）的锐化方法，该方法将精细分辨率图像序列的整体特征与特定粗分辨率图像的独特属性融合。

Result: 两种方法都显著提高了“图像块非配对时空融合”（USTFIP）STIF方法的精度。其中，锐化方法（ABSIS）将融合图像的光谱和空间精度分别提高了高达49.46%和78.40%。

Conclusion: 图像标准化方法，尤其是ABSIS锐化方法，能够有效解决STIF方法对图像分辨率匹配的要求，并显著提升融合图像的光谱和空间精度。

Abstract: Spatio-Temporal Image Fusion (STIF) methods usually require sets of images
with matching spatial and spectral resolutions captured by different sensors.
To facilitate the application of STIF methods, we propose and compare two
different standardization approaches. The first method is based on traditional
upscaling of the fine-resolution images. The second method is a sharpening
approach called Anomaly Based Satellite Image Standardization (ABSIS) that
blends the overall features found in the fine-resolution image series with the
distinctive attributes of a specific coarse-resolution image to produce images
that more closely resemble the outcome of aggregating the fine-resolution
images. Both methods produce a significant increase in accuracy of the Unpaired
Spatio Temporal Fusion of Image Patches (USTFIP) STIF method, with the
sharpening approach increasing the spectral and spatial accuracies of the fused
images by up to 49.46\% and 78.40\%, respectively.

</details>


### [99] [Imaginarium: Vision-guided High-Quality 3D Scene Layout Generation](https://arxiv.org/abs/2510.15564)
*Xiaoming Zhu,Xu Huang,Qinghongbing Xie,Zhi Deng,Junsheng Yu,Yirui Guan,Zhongyuan Liu,Lin Zhu,Qijun Zhao,Ligang Liu,Long Zeng*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的视觉引导3D场景布局生成系统，通过整合图像生成、图像解析和布局优化，克服了现有方法在生成艺术性、连贯且多样化3D布局方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统的基于优化的方法受限于繁琐的手动规则；深度生成模型难以产生丰富多样的内容；利用大型语言模型的方法缺乏鲁棒性，无法准确捕捉复杂的空间关系。这些挑战促使研究者开发更有效的3D布局生成方法。

Method: 1. 构建高质量资产库（2037个场景资产，147个3D场景布局）。2. 使用图像生成模型将提示表示扩展为图像，并进行微调以与资产库对齐。3. 开发鲁棒的图像解析模块，根据视觉语义和几何信息恢复3D场景布局。4. 利用场景图和整体视觉语义优化场景布局，确保逻辑连贯性并与图像对齐。

Result: 广泛的用户测试表明，该算法在布局丰富性和质量方面显著优于现有方法。

Conclusion: 该视觉引导的3D布局生成系统成功解决了现有方法面临的挑战，能够生成更丰富、更高质量的3D场景布局，对数字内容创作至关重要。

Abstract: Generating artistic and coherent 3D scene layouts is crucial in digital
content creation. Traditional optimization-based methods are often constrained
by cumbersome manual rules, while deep generative models face challenges in
producing content with richness and diversity. Furthermore, approaches that
utilize large language models frequently lack robustness and fail to accurately
capture complex spatial relationships. To address these challenges, this paper
presents a novel vision-guided 3D layout generation system. We first construct
a high-quality asset library containing 2,037 scene assets and 147 3D scene
layouts. Subsequently, we employ an image generation model to expand prompt
representations into images, fine-tuning it to align with our asset library. We
then develop a robust image parsing module to recover the 3D layout of scenes
based on visual semantics and geometric information. Finally, we optimize the
scene layout using scene graphs and overall visual semantics to ensure logical
coherence and alignment with the images. Extensive user testing demonstrates
that our algorithm significantly outperforms existing methods in terms of
layout richness and quality. The code and dataset will be available at
https://github.com/HiHiAllen/Imaginarium.

</details>


### [100] [FlexiReID: Adaptive Mixture of Expert for Multi-Modal Person Re-Identification](https://arxiv.org/abs/2510.15595)
*Zhen Sun,Lei Tan,Yunhang Shen,Chengmao Cai,Xing Sun,Pingyang Dai,Liujuan Cao,Rongrong Ji*

Main category: cs.CV

TL;DR: 本文提出FlexiReID，一个灵活的多模态行人重识别框架，支持四种模态（RGB、红外、草图、文本）间的任意查询-检索组合，并构建了统一数据集CIRS-PEDES，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有行人重识别方法主要关注有限的跨模态设置，无法支持任意查询-检索组合，这阻碍了其实际部署和应用。

Method: 本文提出了FlexiReID框架，支持四种模态（RGB、红外、草图、文本）间的七种检索模式。该框架引入了自适应混合专家（MoE）机制，以动态整合不同模态特征，并设计了跨模态查询融合模块以增强多模态特征提取。为全面评估，作者构建了CIRS-PEDES数据集，将四个流行Re-ID数据集扩展到包含全部四种模态。

Result: FlexiReID在复杂场景中实现了最先进的性能，并展现出强大的泛化能力。

Conclusion: FlexiReID提供了一个灵活且高性能的解决方案，解决了现有方法在多模态行人重识别中查询-检索组合受限的问题，并在复杂场景下表现出卓越的性能和泛化能力。

Abstract: Multimodal person re-identification (Re-ID) aims to match pedestrian images
across different modalities. However, most existing methods focus on limited
cross-modal settings and fail to support arbitrary query-retrieval
combinations, hindering practical deployment. We propose FlexiReID, a flexible
framework that supports seven retrieval modes across four modalities: rgb,
infrared, sketches, and text. FlexiReID introduces an adaptive
mixture-of-experts (MoE) mechanism to dynamically integrate diverse modality
features and a cross-modal query fusion module to enhance multimodal feature
extraction. To facilitate comprehensive evaluation, we construct CIRS-PEDES, a
unified dataset extending four popular Re-ID datasets to include all four
modalities. Extensive experiments demonstrate that FlexiReID achieves
state-of-the-art performance and offers strong generalization in complex
scenarios.

</details>


### [101] [Quantized FCA: Efficient Zero-Shot Texture Anomaly Detection](https://arxiv.org/abs/2510.15602)
*Andrei-Timotei Ardelean,Patrick Rückbeil,Tim Weyrich*

Main category: cs.CV

TL;DR: 本文提出了一种名为QFCA的实时零样本纹理异常定位方法，通过量化特征对应分析（FCA）和PCA预处理，实现了10倍加速且几乎不损失精度，提高了复杂纹理的检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有零样本纹理异常定位方法运行时间过长，不适用于如装配线监控等实际部署场景，这促使研究者开发更高效的实时解决方案。

Method: 该研究提出QFCA方法，它是特征对应分析（FCA）算法的量化版本。通过将补丁统计比较适应于量化值的直方图，实现了速度提升。此外，引入基于主成分分析（PCA）的特征预处理步骤，以增强正常和异常特征之间的对比度，提高复杂纹理的检测精度。

Result: QFCA方法实现了10倍的速度提升，同时几乎没有精度损失。在复杂纹理上，通过PCA预处理，检测精度得到了显著改善。与现有技术相比，该方法表现出色。

Conclusion: 通过量化FCA算法和引入PCA特征预处理，QFCA提供了一种实时、高效且准确的零样本纹理异常定位解决方案，解决了现有方法运行时间过长的问题，使其更适用于实际应用部署。

Abstract: Zero-shot anomaly localization is a rising field in computer vision research,
with important progress in recent years. This work focuses on the problem of
detecting and localizing anomalies in textures, where anomalies can be defined
as the regions that deviate from the overall statistics, violating the
stationarity assumption. The main limitation of existing methods is their high
running time, making them impractical for deployment in real-world scenarios,
such as assembly line monitoring. We propose a real-time method, named QFCA,
which implements a quantized version of the feature correspondence analysis
(FCA) algorithm. By carefully adapting the patch statistics comparison to work
on histograms of quantized values, we obtain a 10x speedup with little to no
loss in accuracy. Moreover, we introduce a feature preprocessing step based on
principal component analysis, which enhances the contrast between normal and
anomalous features, improving the detection precision on complex textures. Our
method is thoroughly evaluated against prior art, comparing favorably with
existing methods. Project page:
https://reality.tf.fau.de/pub/ardelean2025quantized.html

</details>


### [102] [Lightweight Data-Free Denoising for Detail-Preserving Biomedical Image Restoration](https://arxiv.org/abs/2510.15611)
*Tomáš Chobola,Julia A. Schnabel,Tingying Peng*

Main category: cs.CV

TL;DR: 本文提出了一种名为Noise2Detail (N2D)的超轻量级自监督去噪模型，它在计算资源极少的情况下实现了快速且高质量的图像恢复，特别适用于生物医学成像。


<details>
  <summary>Details</summary>
Motivation: 当前的自监督去噪技术虽然效果显著，但其高昂的计算和内存需求限制了实际应用，需要在推理速度和重建质量之间进行权衡。

Method: 该研究基于Noise2Noise训练框架，提出了一种创新的多阶段去噪管线Noise2Detail (N2D)。在推理过程中，N2D首先通过破坏噪声空间相关性生成中间平滑结构，然后直接从噪声输入中精细化以恢复细节。

Result: Noise2Detail在性能上超越了现有无需数据集的技术，同时仅需极少的计算资源。

Conclusion: N2D的高效率、低计算成本和无需训练数据的特性，使其成为生物医学成像领域的宝贵工具，解决了清洁训练数据稀缺的问题，并实现了快速推理以满足实际应用需求。

Abstract: Current self-supervised denoising techniques achieve impressive results, yet
their real-world application is frequently constrained by substantial
computational and memory demands, necessitating a compromise between inference
speed and reconstruction quality. In this paper, we present an
ultra-lightweight model that addresses this challenge, achieving both fast
denoising and high quality image restoration. Built upon the Noise2Noise
training framework-which removes the reliance on clean reference images or
explicit noise modeling-we introduce an innovative multistage denoising
pipeline named Noise2Detail (N2D). During inference, this approach disrupts the
spatial correlations of noise patterns to produce intermediate smooth
structures, which are subsequently refined to recapture fine details directly
from the noisy input. Extensive testing reveals that Noise2Detail surpasses
existing dataset-free techniques in performance, while requiring only a
fraction of the computational resources. This combination of efficiency, low
computational cost, and data-free approach make it a valuable tool for
biomedical imaging, overcoming the challenges of scarce clean training data-due
to rare and complex imaging modalities-while enabling fast inference for
practical use.

</details>


### [103] [Deep Learning Based Domain Adaptation Methods in Remote Sensing: A Comprehensive Survey](https://arxiv.org/abs/2510.15615)
*Shuchang Lyu,Qi Zhao,Zheng Zhou,Meng Li,You Zhou,Dingding Yao,Guangliang Cheng,Huiyu Zhou,Zhenwei Shi*

Main category: cs.CV

TL;DR: 这篇论文全面综述了深度学习在遥感领域域适应任务中的最新进展，涵盖了基本概念、方法分类、数据集、性能评估、开放挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 遥感领域的域适应至关重要，但由于数据差异（如地面采样距离、传感器、地理景观和环境条件）而面临巨大挑战。深度学习已成为特征表示和跨域知识迁移的强大工具，因此有必要对基于深度学习的遥感域适应进行系统性总结。

Method: 本文首先介绍了域适应的关键概念、数学符号和方法分类。然后，从任务分类、输入模式、监督范式和算法粒度等多个角度对现有算法进行了组织。接着，回顾了广泛使用的数据集并总结了最先进方法的性能。最后，指出了开放挑战和潜在研究方向。与以往综述相比，本文涵盖了更广泛的遥感域适应任务，并提供了更系统全面的分类。

Result: 作为一篇综述，本文的结果是提供了一个对遥感域适应领域结构化的理解，概述了当前的研究进展（包括SOTA方法的性能），并明确了未来的研究挑战和方向。它呈现了一个系统性的分类，比以往的综述更全面。

Conclusion: 这份综述旨在启发研究社区，促进对遥感域适应领域的理解，并指导未来的研究工作。

Abstract: Domain adaptation is a crucial and increasingly important task in remote
sensing, aiming to transfer knowledge from a source domain a differently
distributed target domain. It has broad applications across various real-world
applications, including remote sensing element interpretation, ecological
environment monitoring, and urban/rural planning. However, domain adaptation in
remote sensing poses significant challenges due to differences in data, such as
variations in ground sampling distance, imaging modes from various sensors,
geographical landscapes, and environmental conditions. In recent years, deep
learning has emerged as a powerful tool for feature representation and
cross-domain knowledge transfer, leading to widespread adoption in remote
sensing tasks. In this paper, we present a comprehensive survey of significant
advancements in deep learning based domain adaptation for remote sensing. We
first introduce the preliminary knowledge to clarify key concepts, mathematical
notations, and the taxonomy of methodologies. We then organize existing
algorithms from multiple perspectives, including task categorization, input
mode, supervision paradigm, and algorithmic granularity, providing readers with
a structured understanding of the field. Next, we review widely used datasets
and summarize the performance of state-of-the-art methods to provide an
overview of current progress. We also identify open challenges and potential
directions to guide future research in domain adaptation for remote sensing.
Compared to previous surveys, this work addresses a broader range of domain
adaptation tasks in remote sensing, rather than concentrating on a few
subfields. It also presents a systematic taxonomy, providing a more
comprehensive and organized understanding of the field. As a whole, this survey
can inspire the research community, foster understanding, and guide future work
in the field.

</details>


### [104] [Unimedvl: Unifying Medical Multimodal Understanding And Generation Through Observation-Knowledge-Analysis](https://arxiv.org/abs/2510.15710)
*Junzhi Ning,Wei Li,Cheng Tang,Jiashi Lin,Chenglong Ma,Chaoyang Zhang,Jiyao Liu,Ying Chen,Shujian Gao,Lihao Liu,Yuandong Pu,Huihui Xu,Chenhui Gou,Ziyan Huang,Yi Xin,Qi Qin,Zhongying Deng,Diping Song,Bin Fu,Guang Yang,Yuanfeng Ji,Tianbin Li,Yanzhou Su,Jin Ye,Shixiang Tang,Ming Hu,Junjun He*

Main category: cs.CV

TL;DR: 本文提出UniMedVL，一个统一的多模态模型，通过多层次框架和UniMed-5M数据集，首次在单一架构中同时处理医学图像理解和生成任务，并实现卓越性能和双向知识共享。


<details>
  <summary>Details</summary>
Motivation: 现有医学AI系统在处理多模态医学输入和生成多样化输出时存在局限性，图像理解模型无法生成视觉内容，图像生成模型无法提供文本解释，导致数据表示、特征集成和任务级多模态能力存在空白。

Method: 受诊断工作流程启发，提出一个多层次框架，遵循“观察-知识-分析 (OKA)”范式：1) 观察层：构建UniMed-5M数据集，包含超过5.6M样本，将单模态数据重构为多模态对。2) 知识层：提出渐进式课程学习（Progressive Curriculum Learning），系统地引入医学多模态知识。3) 分析层：引入UniMedVL，首个医学统一多模态模型，在单一架构中同时分析图像理解和生成任务。

Result: UniMedVL在五个医学图像理解基准测试中表现出色，并在八种医学成像模态的生成质量上与专业模型相匹配。更重要的是，该统一架构实现了双向知识共享，生成任务增强了视觉理解特征，证明了整合传统上分离的能力可以提升各种医学视觉-语言任务的性能。

Conclusion: 将传统上分离的图像理解和生成能力整合到单一医学框架中，能够解锁并改进各种医学视觉-语言任务的性能，并通过双向知识共享带来显著优势。

Abstract: Medical diagnostic applications require models that can process multimodal
medical inputs (images, patient histories, lab results) and generate diverse
outputs including both textual reports and visual content (annotations,
segmentation masks, and images). Despite this need, existing medical AI systems
disrupt this unified process: medical image understanding models interpret
images but cannot generate visual outputs, while medical image generation
models synthesize images but cannot provide textual explanations. This leads to
gaps in data representation, feature integration, and task-level multimodal
capabilities. To this end, we propose a multi-level framework that draws
inspiration from diagnostic workflows through the
Observation-Knowledge-Analysis (OKA) paradigm. Specifically, at the observation
level, we construct UniMed-5M, a dataset comprising over 5.6M samples that
reformat diverse unimodal data into multimodal pairs for foundational
observation. At the knowledge level, we propose Progressive Curriculum Learning
that systematically introduces medical multimodal knowledge. At the analysis
level, we introduce UniMedVL, the first medical unified multimodal model for
the simultaneous analysis of image understanding and generation tasks within a
single architecture. UniMedVL achieves superior performance on five medical
image understanding benchmarks, while matching specialized models in generation
quality across eight medical imaging modalities. Crucially, our unified
architecture enables bidirectional knowledge sharing: generation tasks enhance
visual understanding features, demonstrating that integrating traditionally
separate capabilities within a single medical framework unlocks improvements
across diverse medical vision-language tasks. Code is available at
https://github.com/uni-medical/UniMedVL.

</details>


### [105] [Uncertainty-Aware Extreme Point Tracing for Weakly Supervised Ultrasound Image Segmentation](https://arxiv.org/abs/2510.15666)
*Lei Shi,Gang Li,Junxing Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种弱监督医学图像分割框架，仅使用四个极点作为标注，通过SAM2生成初始伪标签并利用增强的FGEPM算法和不确定性估计进行细化，同时引入新的损失函数，实现了与全监督方法媲美甚至超越的性能，显著降低了标注成本。


<details>
  <summary>Details</summary>
Motivation: 全监督医学图像分割需要大量昂贵且耗时的像素级标注，为缓解这一负担，研究者寻求更高效的标注方法。

Method: 该方法利用四个极点作为标注，从中派生出边界框作为SAM2的提示以生成初始伪标签。这些伪标签通过增强的特征引导极点掩码（FGEPM）算法进行逐步细化，该算法结合蒙特卡洛Dropout不确定性估计来构建统一的梯度不确定性成本图进行边界追踪。此外，引入了双分支不确定性感知尺度一致性（USC）损失和边界框对齐损失，以确保训练过程中的空间一致性和精确边界对齐。

Result: 在BUSI和UNS两个公共超声数据集上的大量实验表明，该方法在显著降低标注成本的同时，实现了与全监督方法相当甚至超越的性能。

Conclusion: 所提出的弱监督框架在超声图像分割中具有有效性和实用性。

Abstract: Automatic medical image segmentation is a fundamental step in computer-aided
diagnosis, yet fully supervised approaches demand extensive pixel-level
annotations that are costly and time-consuming. To alleviate this burden, we
propose a weakly supervised segmentation framework that leverages only four
extreme points as annotation. Specifically, bounding boxes derived from the
extreme points are used as prompts for the Segment Anything Model 2 (SAM2) to
generate reliable initial pseudo labels. These pseudo labels are progressively
refined by an enhanced Feature-Guided Extreme Point Masking (FGEPM) algorithm,
which incorporates Monte Carlo dropout-based uncertainty estimation to
construct a unified gradient uncertainty cost map for boundary tracing.
Furthermore, a dual-branch Uncertainty-aware Scale Consistency (USC) loss and a
box alignment loss are introduced to ensure spatial consistency and precise
boundary alignment during training. Extensive experiments on two public
ultrasound datasets, BUSI and UNS, demonstrate that our method achieves
performance comparable to, and even surpassing fully supervised counterparts
while significantly reducing annotation cost. These results validate the
effectiveness and practicality of the proposed weakly supervised framework for
ultrasound image segmentation.

</details>


### [106] [Scaling Instruction-Based Video Editing with a High-Quality Synthetic Dataset](https://arxiv.org/abs/2510.15742)
*Qingyan Bai,Qiuyu Wang,Hao Ouyang,Yue Yu,Hanlin Wang,Wen Wang,Ka Leong Cheng,Shuailei Ma,Yanhong Zeng,Zichen Liu,Yinghao Xu,Yujun Shen,Qifeng Chen*

Main category: cs.CV

TL;DR: 该论文介绍了Ditto框架，旨在通过创新的数据生成流程、高效的模型架构和智能代理，解决指令式视频编辑中高质量训练数据稀缺的问题。利用Ditto，作者构建了Ditto-1M数据集，并训练了Editto模型，实现了指令遵循能力的新高度。


<details>
  <summary>Details</summary>
Motivation: 指令式视频编辑的进展受到大规模、高质量训练数据严重不足的阻碍，这限制了内容创作的普及。

Method: Ditto框架包含：1) 结合领先图像编辑器和上下文视频生成器的新颖数据生成管道，以克服现有模型的局限性。2) 采用高效的蒸馏模型架构并辅以时间增强器，以降低计算成本并提高时间连贯性，解决成本-质量权衡问题。3) 由智能代理驱动整个流程，生成多样化指令并严格过滤输出，确保大规模质量控制。利用此框架，作者投入12,000 GPU-天构建了Ditto-1M数据集（一百万个高质量视频编辑示例），并使用课程学习策略训练了Editto模型。

Result: Ditto框架成功构建了Ditto-1M，一个包含一百万个高保真视频编辑示例的新数据集。基于该数据集训练的Editto模型展现出卓越的指令遵循能力，并在指令式视频编辑领域建立了新的最先进水平。

Conclusion: Ditto框架通过创新的数据生成、高效的模型设计和智能控制，有效解决了指令式视频编辑的数据稀缺问题。由此产生的Ditto-1M数据集和Editto模型显著提升了指令式视频编辑的性能，并设定了新的行业标准。

Abstract: Instruction-based video editing promises to democratize content creation, yet
its progress is severely hampered by the scarcity of large-scale, high-quality
training data. We introduce Ditto, a holistic framework designed to tackle this
fundamental challenge. At its heart, Ditto features a novel data generation
pipeline that fuses the creative diversity of a leading image editor with an
in-context video generator, overcoming the limited scope of existing models. To
make this process viable, our framework resolves the prohibitive cost-quality
trade-off by employing an efficient, distilled model architecture augmented by
a temporal enhancer, which simultaneously reduces computational overhead and
improves temporal coherence. Finally, to achieve full scalability, this entire
pipeline is driven by an intelligent agent that crafts diverse instructions and
rigorously filters the output, ensuring quality control at scale. Using this
framework, we invested over 12,000 GPU-days to build Ditto-1M, a new dataset of
one million high-fidelity video editing examples. We trained our model, Editto,
on Ditto-1M with a curriculum learning strategy. The results demonstrate
superior instruction-following ability and establish a new state-of-the-art in
instruction-based video editing.

</details>


### [107] [SEGA: A Stepwise Evolution Paradigm for Content-Aware Layout Generation with Design Prior](https://arxiv.org/abs/2510.15749)
*Haoran Wang,Bo Zhao,Jinghui Wang,Hanzhang Wang,Huan Yang,Wei Ji,Hao Liu,Xinyan Xiao*

Main category: cs.CV

TL;DR: 本文提出SEGA，一种用于内容感知布局生成的逐步演化范式，通过粗到细的分层推理和融入设计原则，解决了现有方法在复杂布局规划中缺乏自校正机制的问题，并取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 现有内容感知布局生成方法通常采用单步推理框架，缺乏基于反馈的自校正机制，导致在面对复杂元素布局规划时失败率显著增加。

Method: 本文引入SEGA（内容感知布局生成的逐步演化范式），其灵感来源于人类思维的系统模式。SEGA采用粗到细的分层推理策略：首先，粗粒度模块大致估计布局规划结果；然后，细化模块对粗规划结果进行精细推理。此外，模型融入布局设计原则作为先验知识，以增强布局规划能力。同时，本文还提出了一个包含丰富元信息标注的大规模海报数据集GenPoster-100K。

Result: 实验证明，所提出的方法在多个基准数据集上取得了最先进的结果，显示出其有效性。

Conclusion: SEGA通过其分步演化范式、粗到细的分层推理以及融入布局设计原则，成功解决了内容感知布局生成中复杂规划的挑战，并显著提升了生成性能。

Abstract: In this paper, we study the content-aware layout generation problem, which
aims to automatically generate layouts that are harmonious with a given
background image. Existing methods usually deal with this task with a
single-step reasoning framework. The lack of a feedback-based self-correction
mechanism leads to their failure rates significantly increasing when faced with
complex element layout planning. To address this challenge, we introduce SEGA,
a novel Stepwise Evolution Paradigm for Content-Aware Layout Generation.
Inspired by the systematic mode of human thinking, SEGA employs a hierarchical
reasoning framework with a coarse-to-fine strategy: first, a coarse-level
module roughly estimates the layout planning results; then, another refining
module performs fine-level reasoning regarding the coarse planning results.
Furthermore, we incorporate layout design principles as prior knowledge into
the model to enhance its layout planning ability. Besides, we present
GenPoster-100K that is a new large-scale poster dataset with rich
meta-information annotation. The experiments demonstrate the effectiveness of
our approach by achieving the state-of-the-art results on multiple benchmark
datasets. Our project page is at: https://brucew91.github.io/SEGA.github.io/

</details>


### [108] [Towards more holistic interpretability: A lightweight disentangled Concept Bottleneck Model](https://arxiv.org/abs/2510.15770)
*Gaoxiang Huang,Songning Lai,Yutao Yue*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级解耦概念瓶颈模型（LDCBM），通过自动将视觉特征分组为语义概念，解决了现有概念瓶颈模型（CBMs）中输入到概念映射偏差和可控性有限的问题，显著提高了可解释性和分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有概念瓶颈模型（CBMs）在解释性方面表现良好，但存在输入到概念映射偏差和可控性有限的问题，这限制了它们的实际应用价值，并损害了基于概念方法的策略责任。

Method: 提出了一种轻量级解耦概念瓶颈模型（LDCBM）。该方法无需区域标注，通过引入过滤器分组损失和联合概念监督，自动将视觉特征分组为具有语义意义的组件，从而改善视觉模式与概念之间的对齐。

Result: 在三个不同的数据集上，LDCBM在概念和类别准确性方面均优于之前的CBMs，在可解释性和分类性能上都取得了更高的表现。通过将概念根植于视觉证据，LDCBM克服了先前模型的基本局限性。

Conclusion: LDCBM通过将概念与视觉证据相结合，克服了先前模型的局限性，增强了可解释人工智能的可靠性，为透明和稳健的决策制定提供了支持。

Abstract: Concept Bottleneck Models (CBMs) enhance interpretability by predicting
human-understandable concepts as intermediate representations. However,
existing CBMs often suffer from input-to-concept mapping bias and limited
controllability, which restricts their practical value, directly damage the
responsibility of strategy from concept-based methods. We propose a lightweight
Disentangled Concept Bottleneck Model (LDCBM) that automatically groups visual
features into semantically meaningful components without region annotation. By
introducing a filter grouping loss and joint concept supervision, our method
improves the alignment between visual patterns and concepts, enabling more
transparent and robust decision-making. Notably, Experiments on three diverse
datasets demonstrate that LDCBM achieves higher concept and class accuracy,
outperforming previous CBMs in both interpretability and classification
performance. By grounding concepts in visual evidence, our method overcomes a
fundamental limitation of prior models and enhances the reliability of
interpretable AI.

</details>


### [109] [QSilk: Micrograin Stabilization and Adaptive Quantile Clipping for Detail-Friendly Latent Diffusion](https://arxiv.org/abs/2510.15761)
*Denis Rychkovskiy*

Main category: cs.CV

TL;DR: QSilk是一种轻量级、始终在线的潜在扩散稳定层，通过结合微夹和自适应分位数剪裁，在抑制罕见激活峰值的同时提高高频保真度，无需训练，开销可忽略不计。


<details>
  <summary>Details</summary>
Motivation: 提高潜在扩散模型在高频细节上的保真度，同时抑制罕见的激活峰值（极端值），以获得更清晰、更锐利的结果。

Method: QSilk结合了两种技术：(i) 每样本微夹（per-sample micro clamp），温和限制极端值而不损失纹理；(ii) 自适应分位数剪裁（Adaptive Quantile Clip, AQClip），根据区域调整允许的值范围。AQClip可在使用局部结构统计的代理模式或注意力熵引导（模型置信度）模式下操作。它被集成到CADE 2.5渲染管道中。

Result: QSilk在低步数和超高分辨率下产生更清晰、更锐利的结果，开销可忽略不计。它无需训练或微调，并暴露最少的用户控制。在SD/SDXL主干网络上显示出一致的定性改进，并能与CFG/Rescale协同作用，在没有伪影的情况下实现略高的引导。

Conclusion: QSilk是一种高效、无需训练且易于集成的稳定层，显著改善了潜在扩散模型的输出质量，尤其是在高频细节和异常值抑制方面，从而提供更清洁、更锐利的结果。

Abstract: We present QSilk, a lightweight, always-on stabilization layer for latent
diffusion that improves high-frequency fidelity while suppressing rare
activation spikes. QSilk combines (i) a per-sample micro clamp that gently
limits extreme values without washing out texture, and (ii) Adaptive Quantile
Clip (AQClip), which adapts the allowed value corridor per region. AQClip can
operate in a proxy mode using local structure statistics or in an attention
entropy guided mode (model confidence). Integrated into the CADE 2.5 rendering
pipeline, QSilk yields cleaner, sharper results at low step counts and
ultra-high resolutions with negligible overhead. It requires no training or
fine-tuning and exposes minimal user controls. We report consistent qualitative
improvements across SD/SDXL backbones and show synergy with CFG/Rescale,
enabling slightly higher guidance without artifacts.

</details>


### [110] [ReCon: Region-Controllable Data Augmentation with Rectification and Alignment for Object Detection](https://arxiv.org/abs/2510.15783)
*Haowei Zhu,Tianxiang Pan,Rui Qin,Jun-Hai Yong,Bin Wang*

Main category: cs.CV

TL;DR: ReCon是一个用于目标检测数据增强的新型框架，它通过在扩散采样过程中整合区域引导校正和区域对齐交叉注意力，显著提高了生成数据的质量和可训练性。


<details>
  <summary>Details</summary>
Motivation: 大规模高质量标注数据获取成本高昂且耗时。现有生成模型在数据增强方面存在局限性，如需要复杂的后处理、大量微调、内容-位置不匹配和语义泄露问题。

Method: 本文提出了ReCon框架，通过以下方式增强结构可控生成模型：1) 将区域引导校正整合到扩散采样过程中，利用预训练感知模型的反馈来纠正生成区域的错误。2) 提出区域对齐交叉注意力机制，以强制图像区域与其文本提示之间的空间-语义对齐，从而提高语义一致性和图像保真度。

Result: 广泛的实验表明，ReCon显著提高了生成数据的质量和可训练性，在各种数据集、骨干网络架构和数据规模上都取得了持续的性能提升。

Conclusion: ReCon是一个有效的数据增强框架，通过其创新的校正和对齐机制，克服了现有生成模型的局限性，为目标检测任务提供了更高质量的合成数据。

Abstract: The scale and quality of datasets are crucial for training robust perception
models. However, obtaining large-scale annotated data is both costly and
time-consuming. Generative models have emerged as a powerful tool for data
augmentation by synthesizing samples that adhere to desired distributions.
However, current generative approaches often rely on complex post-processing or
extensive fine-tuning on massive datasets to achieve satisfactory results, and
they remain prone to content-position mismatches and semantic leakage. To
overcome these limitations, we introduce ReCon, a novel augmentation framework
that enhances the capacity of structure-controllable generative models for
object detection. ReCon integrates region-guided rectification into the
diffusion sampling process, using feedback from a pre-trained perception model
to rectify misgenerated regions within diffusion sampling process. We further
propose region-aligned cross-attention to enforce spatial-semantic alignment
between image regions and their textual cues, thereby improving both semantic
consistency and overall image fidelity. Extensive experiments demonstrate that
ReCon substantially improve the quality and trainability of generated data,
achieving consistent performance gains across various datasets, backbone
architectures, and data scales. Our code is available at
https://github.com/haoweiz23/ReCon .

</details>


### [111] [VISTA: A Test-Time Self-Improving Video Generation Agent](https://arxiv.org/abs/2510.15831)
*Do Xuan Long,Xingchen Wan,Hootan Nakhost,Chen-Yu Lee,Tomas Pfister,Sercan Ö. Arık*

Main category: cs.CV

TL;DR: VISTA是一个多智能体系统，通过迭代式提示词优化，自主提升文本到视频生成质量，解决了现有方法对精确提示词的依赖和优化难题。


<details>
  <summary>Details</summary>
Motivation: 文本到视频合成的质量严重依赖用户提示词的精确性，而现有测试时优化方法难以处理视频的多方面复杂性。

Method: VISTA首先将用户想法分解为结构化的时间计划，然后生成视频并通过配对锦标赛选出最佳视频。接着，由三个专业智能体（视觉、音频、上下文）对获胜视频进行评估。最后，一个推理智能体综合这些反馈，反思性地重写和增强提示词，用于下一轮生成。

Result: 在单场景和多场景视频生成中，VISTA一致地提高了视频质量和与用户意图的一致性，相较于最先进的基线，配对胜率高达60%。人类评估者也更倾向于VISTA的输出，偏好率达66.4%，而先前方法的效果则不稳定。

Conclusion: VISTA通过其新颖的多智能体迭代自改进循环，有效解决了文本到视频生成中对精确提示词的依赖问题，显著提高了生成视频的质量和用户意图对齐度。

Abstract: Despite rapid advances in text-to-video synthesis, generated video quality
remains critically dependent on precise user prompts. Existing test-time
optimization methods, successful in other domains, struggle with the
multi-faceted nature of video. In this work, we introduce VISTA (Video
Iterative Self-improvemenT Agent), a novel multi-agent system that autonomously
improves video generation through refining prompts in an iterative loop. VISTA
first decomposes a user idea into a structured temporal plan. After generation,
the best video is identified through a robust pairwise tournament. This winning
video is then critiqued by a trio of specialized agents focusing on visual,
audio, and contextual fidelity. Finally, a reasoning agent synthesizes this
feedback to introspectively rewrite and enhance the prompt for the next
generation cycle. Experiments on single- and multi-scene video generation
scenarios show that while prior methods yield inconsistent gains, VISTA
consistently improves video quality and alignment with user intent, achieving
up to 60% pairwise win rate against state-of-the-art baselines. Human
evaluators concur, preferring VISTA outputs in 66.4% of comparisons.

</details>


### [112] [ERNet: Efficient Non-Rigid Registration Network for Point Sequences](https://arxiv.org/abs/2510.15800)
*Guangzhao He,Yuxi Xiao,Zhen Xu,Xiaowei Zhou,Sida Peng*

Main category: cs.CV

TL;DR: 本文提出ERNet，一个高效的前馈模型，用于解决非刚性点云序列配准中的局部最小值和误差累积问题。它采用两阶段管道预测形变图，实现了对噪声和部分输入的准确且一致的序列配准，并显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 非刚性形变点云序列配准面临两大挑战：(i) 非凸配准目标（尤其在噪声或部分输入下）导致的局部最小值，阻碍准确鲁棒的形变估计；(ii) 长序列中误差累积导致的跟踪失败。

Method: 提出ERNet，一个可扩展的数据驱动前馈模型，在大形变数据集上训练。其核心设计是预测一系列形变图，通过两阶段管道实现：首先估计逐帧的粗略图节点以进行鲁棒初始化，然后以滑动窗口方式随时间细化其轨迹，有效利用时间信息。

Result: 实验结果表明，ERNet在DeformingThings4D和D-FAUST数据集上均优于现有最先进技术，并且比现有最佳方法提速超过4倍，显著提升了效率。

Conclusion: ERNet提供了一种高效、准确且一致的非刚性点云序列配准方法，有效解决了局部最小值和误差累积问题，并在性能和效率上都超越了现有技术。

Abstract: Registering an object shape to a sequence of point clouds undergoing
non-rigid deformation is a long-standing challenge. The key difficulties stem
from two factors: (i) the presence of local minima due to the non-convexity of
registration objectives, especially under noisy or partial inputs, which
hinders accurate and robust deformation estimation, and (ii) error accumulation
over long sequences, leading to tracking failures. To address these challenges,
we introduce to adopt a scalable data-driven approach and propose ERNet, an
efficient feed-forward model trained on large deformation datasets. It is
designed to handle noisy and partial inputs while effectively leveraging
temporal information for accurate and consistent sequential registration. The
key to our design is predicting a sequence of deformation graphs through a
two-stage pipeline, which first estimates frame-wise coarse graph nodes for
robust initialization, before refining their trajectories over time in a
sliding-window fashion. Extensive experiments show that our proposed approach
(i) outperforms previous state-of-the-art on both the DeformingThings4D and
D-FAUST datasets, and (ii) achieves more than 4x speedup compared to the
previous best, offering significant efficiency improvement.

</details>


### [113] [Memory-SAM: Human-Prompt-Free Tongue Segmentation via Retrieval-to-Prompt](https://arxiv.org/abs/2510.15849)
*Joongwon Chae,Lihui Luo,Xi Yuan,Dongmei Yu,Zhenglin Chen,Lian Zhang,Peiwu Qin*

Main category: cs.CV

TL;DR: Memory-SAM 是一种无需训练、无需人工提示的舌像分割方法，通过从少量先验案例中检索并生成提示来引导 SAM2，实现了高效且鲁棒的分割。


<details>
  <summary>Details</summary>
Motivation: 准确的舌像分割对中医分析至关重要。现有的监督模型需要大量标注数据，而 SAM 系列模型依赖于提示输入，限制了其在实际应用中的便利性。

Method: Memory-SAM 提出了一种无需训练、无需人工提示的流程。它利用 DINOv3 密集特征和 FAISS 检索，从少量先验案例记忆中自动生成有效提示。对于查询图像，通过检索到的示例，将掩码约束的对应关系提炼为前景/背景点提示，以引导 SAM2 进行分割，无需手动点击或模型微调。

Result: 在包含 600 张专家标注图像（300 张受控，300 张真实世界）的混合测试集上，Memory-SAM 实现了 0.9863 的 mIoU，显著优于 FCN (0.8188) 和基于检测器的 SAM 基线 (0.1839)。该方法在真实世界条件下表现出明显的性能提升。

Conclusion: 研究结果表明，通过检索生成提示的方法，能够实现数据高效、鲁棒的舌像不规则边界分割。

Abstract: Accurate tongue segmentation is crucial for reliable TCM analysis. Supervised
models require large annotated datasets, while SAM-family models remain
prompt-driven. We present Memory-SAM, a training-free, human-prompt-free
pipeline that automatically generates effective prompts from a small memory of
prior cases via dense DINOv3 features and FAISS retrieval. Given a query image,
mask-constrained correspondences to the retrieved exemplar are distilled into
foreground/background point prompts that guide SAM2 without manual clicks or
model fine-tuning. We evaluate on 600 expert-annotated images (300 controlled,
300 in-the-wild). On the mixed test split, Memory-SAM achieves mIoU 0.9863,
surpassing FCN (0.8188) and a detector-to-box SAM baseline (0.1839). On
controlled data, ceiling effects above 0.98 make small differences less
meaningful given annotation variability, while our method shows clear gains
under real-world conditions. Results indicate that retrieval-to-prompt enables
data-efficient, robust segmentation of irregular boundaries in tongue imaging.
The code is publicly available at https://github.com/jw-chae/memory-sam.

</details>


### [114] [BiomedXPro: Prompt Optimization for Explainable Diagnosis with Biomedical Vision Language Models](https://arxiv.org/abs/2510.15866)
*Kaushitha Silva,Mansitha Eashwara,Sanduni Ubayasiri,Ruwan Tennakoon,Damayanthi Herath*

Main category: cs.CV

TL;DR: BiomedXPro是一个进化框架，利用大型语言模型自动生成多样化、可解释的自然语言提示对，以提高生物医学视觉-语言模型在疾病诊断中的性能和可信度，尤其是在数据稀缺场景下。


<details>
  <summary>Details</summary>
Motivation: 当前生物医学视觉-语言模型的提示优化技术要么产生不可解释的潜在向量，要么只生成单一文本提示，这限制了它们在临床应用中的透明度和可信度，也未能捕捉临床诊断的多面性。

Method: 引入BiomedXPro，一个进化框架，它利用大型语言模型（LLM）作为生物医学知识提取器和自适应优化器，自动生成多样化的、可解释的自然语言提示对，用于疾病诊断。

Result: 在多个生物医学基准测试中，BiomedXPro持续优于最先进的提示调整方法，尤其是在数据稀缺的少样本设置中。此外，分析表明所发现的提示与统计学上显著的临床特征之间存在很强的语义一致性。

Conclusion: BiomedXPro通过生成多样化、可解释的提示集合，为模型预测提供了可验证的基础，代表着开发更值得信赖和更符合临床需求的AI系统的关键一步。

Abstract: The clinical adoption of biomedical vision-language models is hindered by
prompt optimization techniques that produce either uninterpretable latent
vectors or single textual prompts. This lack of transparency and failure to
capture the multi-faceted nature of clinical diagnosis, which relies on
integrating diverse observations, limits their trustworthiness in high-stakes
settings. To address this, we introduce BiomedXPro, an evolutionary framework
that leverages a large language model as both a biomedical knowledge extractor
and an adaptive optimizer to automatically generate a diverse ensemble of
interpretable, natural-language prompt pairs for disease diagnosis. Experiments
on multiple biomedical benchmarks show that BiomedXPro consistently outperforms
state-of-the-art prompt-tuning methods, particularly in data-scarce few-shot
settings. Furthermore, our analysis demonstrates a strong semantic alignment
between the discovered prompts and statistically significant clinical features,
grounding the model's performance in verifiable concepts. By producing a
diverse ensemble of interpretable prompts, BiomedXPro provides a verifiable
basis for model predictions, representing a critical step toward the
development of more trustworthy and clinically-aligned AI systems.

</details>


### [115] [Neuro-Symbolic Spatial Reasoning in Segmentation](https://arxiv.org/abs/2510.15841)
*Jiayi Lin,Jiabo Huang,Shaogang Gong*

Main category: cs.CV

TL;DR: 该论文引入神经符号（NeSy）空间推理到开放词汇语义分割（OVSS）中，通过RelateSeg模型利用一阶逻辑（FOL）和模糊逻辑松弛来明确施加空间关系约束，以解决现有视觉语言模型（VLM）方法缺乏空间理解的问题，并取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于视觉语言模型（VLM）的开放词汇语义分割（OVSS）方法在关联局部图像块与潜在的未见对象类别时，缺乏对场景中对象空间关系的理解。

Method: 提出Relational Segmentor (RelateSeg) 模型，这是首次尝试在OVSS中探索神经符号（NeSy）空间推理。它通过一阶逻辑（FOL）在神经网络架构中施加显式空间关系约束。具体来说，RelateSeg自动提取空间关系（例如，<猫，在右侧，人>），并使用提出的伪类别将其编码为一阶逻辑公式。每个像素同时学习预测语义类别和空间伪类别，以强制执行关系约束。这些逻辑约束通过模糊逻辑松弛在深度网络架构中进行公式化，实现了空间关系一致分割的端到端学习。

Result: RelateSeg在四个基准数据集上的平均mIoU方面取得了最先进的性能，特别是在包含多个类别的图像上显示出明显优势。该方法仅引入了一个辅助损失函数，没有增加额外参数，验证了NeSy空间推理在OVSS中的有效性。

Conclusion: 神经符号（NeSy）空间推理在开放词汇语义分割（OVSS）中是有效的，能够通过明确的空间关系约束提升模型性能，尤其是在处理复杂场景时。

Abstract: Open-Vocabulary Semantic Segmentation (OVSS) assigns pixel-level labels from
an open set of categories, requiring generalization to unseen and unlabelled
objects. Using vision-language models (VLMs) to correlate local image patches
with potential unseen object categories suffers from a lack of understanding of
spatial relations of objects in a scene. To solve this problem, we introduce
neuro-symbolic (NeSy) spatial reasoning in OVSS. In contrast to contemporary
VLM correlation-based approaches, we propose Relational Segmentor (RelateSeg)
to impose explicit spatial relational constraints by first order logic (FOL)
formulated in a neural network architecture. This is the first attempt to
explore NeSy spatial reasoning in OVSS. Specifically, RelateSeg automatically
extracts spatial relations, e.g., <cat, to-right-of, person>, and encodes them
as first-order logic formulas using our proposed pseudo categories. Each pixel
learns to predict both a semantic category (e.g., "cat") and a spatial pseudo
category (e.g., "right of person") simultaneously, enforcing relational
constraints (e.g., a "cat" pixel must lie to the right of a "person"). Finally,
these logic constraints are formulated in a deep network architecture by fuzzy
logic relaxation, enabling end-to-end learning of spatial-relationally
consistent segmentation. RelateSeg achieves state-of-the-art performance in
terms of average mIoU across four benchmark datasets and particularly shows
clear advantages on images containing multiple categories, with the cost of
only introducing a single auxiliary loss function and no additional parameters,
validating the effectiveness of NeSy spatial reasoning in OVSS.

</details>


### [116] [BLIP3o-NEXT: Next Frontier of Native Image Generation](https://arxiv.org/abs/2510.15857)
*Jiuhai Chen,Le Xue,Zhiyang Xu,Xichen Pan,Shusheng Yang,Can Qin,An Yan,Honglu Zhou,Zeyuan Chen,Lifu Huang,Tianyi Zhou,Junnan Li,Silvio Savarese,Caiming Xiong,Ran Xu*

Main category: cs.CV

TL;DR: BLIP3o-NEXT是一个开源的基础模型，它通过结合自回归和扩散架构，在一个统一的框架内实现了先进的文本到图像生成和图像编辑能力，并在多项基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 研究旨在推动原生图像生成的前沿，解决现有模型在图像生成和编辑方面的挑战，并整合自回归模型和扩散模型的优势，以实现更强的推理能力、指令遵循和图像细节渲染。

Method: BLIP3o-NEXT采用“自回归+扩散”架构。首先，一个自回归模型根据多模态输入生成离散图像tokens；然后，这些tokens的隐藏状态作为扩散模型的条件信号，用于生成高保真图像。此外，通过后训练和数据引擎来增强图像编辑的指令遵循和一致性。

Result: BLIP3o-NEXT在图像生成和图像编辑方面展现出强大的能力，实现了更高水平的连贯性和真实感。在各种文本到图像和图像编辑基准测试中，BLIP3o-NEXT均优于现有模型。

Conclusion: BLIP3o-NEXT通过其创新的自回归+扩散统一架构，成功地将自回归模型的推理和指令遵循能力与扩散模型的精细细节渲染能力相结合，为原生图像生成和编辑树立了新的性能标杆。

Abstract: We present BLIP3o-NEXT, a fully open-source foundation model in the BLIP3
series that advances the next frontier of native image generation. BLIP3o-NEXT
unifies text-to-image generation and image editing within a single
architecture, demonstrating strong image generation and image editing
capabilities. In developing the state-of-the-art native image generation model,
we identify four key insights: (1) Most architectural choices yield comparable
performance; an architecture can be deemed effective provided it scales
efficiently and supports fast inference; (2) The successful application of
reinforcement learning can further push the frontier of native image
generation; (3) Image editing still remains a challenging task, yet instruction
following and the consistency between generated and reference images can be
significantly enhanced through post-training and data engine; (4) Data quality
and scale continue to be decisive factors that determine the upper bound of
model performance. Building upon these insights, BLIP3o-NEXT leverages an
Autoregressive + Diffusion architecture in which an autoregressive model first
generates discrete image tokens conditioned on multimodal inputs, whose hidden
states are then used as conditioning signals for a diffusion model to generate
high-fidelity images. This architecture integrates the reasoning strength and
instruction following of autoregressive models with the fine-detail rendering
ability of diffusion models, achieving a new level of coherence and realism.
Extensive evaluations of various text-to-image and image-editing benchmarks
show that BLIP3o-NEXT achieves superior performance over existing models.

</details>


### [117] [LightsOut: Diffusion-based Outpainting for Enhanced Lens Flare Removal](https://arxiv.org/abs/2510.15868)
*Shr-Ruei Tsai,Wei-Cheng Chang,Jie-Ying Lee,Chih-Hai Su,Yu-Lun Liu*

Main category: cs.CV

TL;DR: LightsOut是一个基于扩散模型的图像外绘框架，用于重建画框外的光源，从而显著提升现有单图像眩光去除（SIFR）方法在光源不完整或缺失情况下的性能。


<details>
  <summary>Details</summary>
Motivation: 镜头眩光严重降低图像质量，影响目标检测和自动驾驶等关键计算机视觉任务。现有的单图像眩光去除（SIFR）方法在画框外光源不完整或缺失时表现不佳。

Method: 该方法提出LightsOut，一个基于扩散模型的图像外绘框架，通过重建画框外的光源来增强SIFR。它利用一个多任务回归模块和一个经过LoRA微调的扩散模型，以确保外绘结果的真实性和物理一致性。

Result: 全面的实验表明，LightsOut在各种挑战性场景下，无需额外训练即可持续提升现有SIFR方法的性能，作为一个普遍适用的即插即用预处理解决方案。

Conclusion: LightsOut通过重建画框外光源，有效解决了现有SIFR方法在光源不完整或缺失时的局限性，提供了一个通用的、高性能的预处理方案，以改善图像眩光去除效果。

Abstract: Lens flare significantly degrades image quality, impacting critical computer
vision tasks like object detection and autonomous driving. Recent Single Image
Flare Removal (SIFR) methods perform poorly when off-frame light sources are
incomplete or absent. We propose LightsOut, a diffusion-based outpainting
framework tailored to enhance SIFR by reconstructing off-frame light sources.
Our method leverages a multitask regression module and LoRA fine-tuned
diffusion model to ensure realistic and physically consistent outpainting
results. Comprehensive experiments demonstrate LightsOut consistently boosts
the performance of existing SIFR methods across challenging scenarios without
additional retraining, serving as a universally applicable plug-and-play
preprocessing solution. Project page: https://ray-1026.github.io/lightsout/

</details>


### [118] [Skyfall-GS: Synthesizing Immersive 3D Urban Scenes from Satellite Imagery](https://arxiv.org/abs/2510.15869)
*Jie-Ying Lee,Yi-Ruei Liu,Shr-Ruei Tsai,Wei-Cheng Chang,Chung-Ho Wu,Jiewen Chan,Zhenjun Zhao,Chieh Hubert Lin,Yu-Lun Liu*

Main category: cs.CV

TL;DR: 本文提出Skyfall-GS，一个无需昂贵3D标注，利用卫星图像和扩散模型生成大规模、可探索、几何精确的3D城市场景的框架，并支持实时沉浸式探索。


<details>
  <summary>Details</summary>
Motivation: 生成大规模、可探索且几何精确的3D城市场景面临挑战，主要原因是缺乏大规模、高质量的真实世界3D扫描数据来训练可泛化的生成模型。

Method: Skyfall-GS框架结合了现成的卫星图像（提供粗糙几何）和开放域扩散模型（生成高质量近景外观）。它采用一种课程驱动的迭代细化策略，逐步增强几何完整性和真实感纹理。

Result: Skyfall-GS是首个无需昂贵3D标注即可创建城市街区规模3D场景的框架，并支持实时沉浸式3D探索。与现有最先进方法相比，它提供了改进的跨视角一致几何和更逼真的纹理。

Conclusion: Skyfall-GS通过整合卫星图像和扩散模型，为大规模3D城市场景的创建提供了一种创新且有效的方法，克服了传统3D扫描数据不足的限制，实现了高精度和可探索性。

Abstract: Synthesizing large-scale, explorable, and geometrically accurate 3D urban
scenes is a challenging yet valuable task in providing immersive and embodied
applications. The challenges lie in the lack of large-scale and high-quality
real-world 3D scans for training generalizable generative models. In this
paper, we take an alternative route to create large-scale 3D scenes by
synergizing the readily available satellite imagery that supplies realistic
coarse geometry and the open-domain diffusion model for creating high-quality
close-up appearances. We propose \textbf{Skyfall-GS}, the first city-block
scale 3D scene creation framework without costly 3D annotations, also featuring
real-time, immersive 3D exploration. We tailor a curriculum-driven iterative
refinement strategy to progressively enhance geometric completeness and
photorealistic textures. Extensive experiments demonstrate that Skyfall-GS
provides improved cross-view consistent geometry and more realistic textures
compared to state-of-the-art approaches. Project page:
https://skyfall-gs.jayinnn.dev/

</details>


### [119] [3DPR: Single Image 3D Portrait Relight using Generative Priors](https://arxiv.org/abs/2510.15846)
*Pramod Rao,Abhimitra Meka,Xilong Zhou,Gereon Fox,Mallikarjun B R,Fangneng Zhan,Tim Weyrich,Bernd Bickel,Hanspeter Pfister,Wojciech Matusik,Thabo Beeler,Mohamed Elgharib,Marc Habermann,Christian Theobalt*

Main category: cs.CV

TL;DR: 本文提出3DPR，一个基于图像的重光照模型，能从单张肖像图像渲染出新的、重新打光的人头视图。它利用从多视角OLAT（一次一盏灯）图像学习到的生成先验，并结合预训练生成头部模型的潜在空间，实现了高质量、物理准确的重光照效果。


<details>
  <summary>Details</summary>
Motivation: 从单张肖像图像渲染新颖、重新打光的人头视图是一个固有欠约束的问题。传统的图形学解决方案通过可微分渲染将输入图像分解为几何、材质和光照，但这受限于底层模型和场景组件参数化的多重假设和近似。

Method: 3DPR模型利用从光场中捕获的多视角OLAT图像学习到的生成先验。为此，研究者引入了一个包含139个主题的、多样化大规模多视角4K OLAT数据集，用于学习高频面部反射分布的先验。该模型还利用了预训练生成头部模型的潜在空间，该模型提供了从真实世界图像数据中学到的丰富面部几何先验。输入肖像首先通过编码器反演过程嵌入到该模型的潜在流形中。然后，使用一个在光场数据上训练的基于三平面的新型反射网络来合成高保真OLAT图像，从而实现基于图像的重光照。该反射网络在生成头部模型的潜在空间中操作，这显著减少了训练反射模型所需的光场图像数量。最后，根据给定的HDRI环境图组合生成的OLAT图像，以实现物理准确的环境重光照。

Result: 通过定量和定性评估，3DPR在保留身份和捕捉光照效果（如镜面反射、自阴影和次表面散射）方面优于现有方法，并能生成物理准确的环境重光照结果。

Conclusion: 3DPR通过有效结合生成先验和新颖的OLAT数据集，成功解决了从单张图像进行人头重光照的挑战，并提供了高质量、物理准确的渲染效果，特别是在细节和光照效果方面表现出色。

Abstract: Rendering novel, relit views of a human head, given a monocular portrait
image as input, is an inherently underconstrained problem. The traditional
graphics solution is to explicitly decompose the input image into geometry,
material and lighting via differentiable rendering; but this is constrained by
the multiple assumptions and approximations of the underlying models and
parameterizations of these scene components. We propose 3DPR, an image-based
relighting model that leverages generative priors learnt from multi-view
One-Light-at-A-Time (OLAT) images captured in a light stage. We introduce a new
diverse and large-scale multi-view 4K OLAT dataset of 139 subjects to learn a
high-quality prior over the distribution of high-frequency face reflectance. We
leverage the latent space of a pre-trained generative head model that provides
a rich prior over face geometry learnt from in-the-wild image datasets. The
input portrait is first embedded in the latent manifold of such a model through
an encoder-based inversion process. Then a novel triplane-based reflectance
network trained on our lightstage data is used to synthesize high-fidelity OLAT
images to enable image-based relighting. Our reflectance network operates in
the latent space of the generative head model, crucially enabling a relatively
small number of lightstage images to train the reflectance model. Combining the
generated OLATs according to a given HDRI environment maps yields physically
accurate environmental relighting results. Through quantitative and qualitative
evaluations, we demonstrate that 3DPR outperforms previous methods,
particularly in preserving identity and in capturing lighting effects such as
specularities, self-shadows, and subsurface scattering. Project Page:
https://vcai.mpi-inf.mpg.de/projects/3dpr/

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [120] [Rethinking Toxicity Evaluation in Large Language Models: A Multi-Label Perspective](https://arxiv.org/abs/2510.15007)
*Zhiqiang Kou,Junyang Chen,Xin-Qiang Cai,Ming-Kun Xie,Biao Liu,Changwei Wang,Lei Feng,Yuheng Jia,Gang Niu,Masashi Sugiyama,Xin Geng*

Main category: cs.CL

TL;DR: 该研究针对大型语言模型（LLMs）生成有害内容的安全性问题，提出了三个多标签基准测试和一种基于伪标签的毒性检测方法，以解决现有单标签检测器评估偏差和多标签标注成本高昂的问题，并显著优于现有先进基线模型。


<details>
  <summary>Details</summary>
Motivation: LLMs生成有害内容的潜力引发了严重的安全担忧。现有的毒性检测器主要依赖单标签基准，无法充分捕捉真实世界中有害提示固有的模糊性和多维性，导致评估偏差（漏检和误报）。此外，收集全面的细粒度多标签毒性标注成本极高。

Method: 本研究引入了三个新的多标签毒性检测基准（Q-A-MLL、R-A-MLL和H-X-MLL），这些基准源自公共毒性数据集，并根据详细的15类毒性分类法进行标注。研究还提供了理论证明，表明在发布的数据集上，使用伪标签训练比直接从单标签监督学习能获得更好的性能。此外，开发了一种基于伪标签的毒性检测方法。

Result: 实验结果表明，本研究提出的方法显著超越了包括GPT-4o和DeepSeek在内的先进基线模型。这使得对LLM生成内容中多标签毒性能够进行更准确、更可靠的评估。

Conclusion: 通过引入新的多标签基准和基于伪标签的检测方法，本研究有效解决了LLM有害内容检测中单标签评估的局限性和多标签标注的成本问题，从而实现了对LLM生成内容中多标签毒性更准确、更可靠的评估。

Abstract: Large language models (LLMs) have achieved impressive results across a range
of natural language processing tasks, but their potential to generate harmful
content has raised serious safety concerns. Current toxicity detectors
primarily rely on single-label benchmarks, which cannot adequately capture the
inherently ambiguous and multi-dimensional nature of real-world toxic prompts.
This limitation results in biased evaluations, including missed toxic
detections and false positives, undermining the reliability of existing
detectors. Additionally, gathering comprehensive multi-label annotations across
fine-grained toxicity categories is prohibitively costly, further hindering
effective evaluation and development. To tackle these issues, we introduce
three novel multi-label benchmarks for toxicity detection: \textbf{Q-A-MLL},
\textbf{R-A-MLL}, and \textbf{H-X-MLL}, derived from public toxicity datasets
and annotated according to a detailed 15-category taxonomy. We further provide
a theoretical proof that, on our released datasets, training with pseudo-labels
yields better performance than directly learning from single-label supervision.
In addition, we develop a pseudo-label-based toxicity detection method.
Extensive experimental results show that our approach significantly surpasses
advanced baselines, including GPT-4o and DeepSeek, thus enabling more accurate
and reliable evaluation of multi-label toxicity in LLM-generated content.

</details>


### [121] [Can generative AI figure out figurative language? The influence of idioms on essay scoring by ChatGPT, Gemini, and Deepseek](https://arxiv.org/abs/2510.15009)
*Enis Oğuz*

Main category: cs.CL

TL;DR: 本研究评估了生成式AI模型在有无习语的作文评分中的表现，发现所有模型都具有良好的一致性，其中Gemini在与人类评分者的一致性以及处理习语方面表现最佳，且无人口统计学偏见。


<details>
  <summary>Details</summary>
Motivation: 生成式AI已被提议作为自动评估学生作文（AES系统）的竞争者。考虑到AI在处理习语方面可能存在的局限性，本研究旨在评估生成式AI模型在包含和不包含习语的作文中的评分性能。

Method: 研究从一个语料库中选取了348篇学生作文，创建了两个等长的作文列表：一个每篇作文包含多个习语，另一个不含习语。使用语料库语言学和计算语言学的见解，让三个生成式AI模型（ChatGPT、Gemini和Deepseek）使用与人类评分者相同的评分标准，对两个列表中的所有作文分别评分三次。

Result: 所有模型都表现出出色的一致性，但Gemini在与人类评分者之间的评分者间信度方面优于其他竞争者。AI评估中未检测到任何人口统计学群体的偏见。对于包含多个习语的作文，Gemini的评分模式与人类评分者最相似。

Conclusion: 研究中的模型展示了混合评分方法的潜力，而Gemini因其处理比喻性语言的能力，被认为是该任务的最佳候选者，并有望在未来独立承担作文评分任务。

Abstract: The developments in Generative AI technologies have paved the way for
numerous innovations in different fields. Recently, Generative AI has been
proposed as a competitor to AES systems in evaluating student essays
automatically. Considering the potential limitations of AI in processing
idioms, this study assessed the scoring performances of Generative AI models
for essays with and without idioms by incorporating insights from Corpus
Linguistics and Computational Linguistics. Two equal essay lists were created
from 348 student essays taken from a corpus: one with multiple idioms present
in each essay and another with no idioms in essays. Three Generative AI models
(ChatGPT, Gemini, and Deepseek) were asked to score all essays in both lists
three times, using the same rubric used by human raters in assigning essay
scores. The results revealed excellent consistency for all models, but Gemini
outperformed its competitors in interrater reliability with human raters. There
was also no detectable bias for any demographic group in AI assessment. For
essays with multiple idioms, Gemini followed a the most similar pattern to
human raters. While the models in the study demonstrated potential for a hybrid
approach, Gemini was the best candidate for the task due to its ability to
handle figurative language and showed promise for handling essay-scoring tasks
alone in the future.

</details>


### [122] [A Generalizable Rhetorical Strategy Annotation Model Using LLM-based Debate Simulation and Labelling](https://arxiv.org/abs/2510.15081)
*Shiyu Ji,Farnoosh Hashemi,Joice Chen,Juanwen Pan,Weicheng Ma,Hefan Zhang,Sophia Pan,Ming Cheng,Shubham Mohole,Saeed Hassanpour,Soroush Vosoughi,Michael Macy*

Main category: cs.CL

TL;DR: 该研究提出一个新颖的框架，利用大型语言模型（LLMs）自动生成和标注修辞策略的合成辩论数据，并在此基础上训练分类器，实现了高准确性和泛化能力，并应用于说服力预测和美国总统辩论的修辞策略分析。


<details>
  <summary>Details</summary>
Motivation: 修辞策略分析目前严重依赖人工标注，这种方法成本高、不一致且难以扩展。此外，现有数据集通常局限于特定主题和策略，限制了鲁棒模型的发展。

Method: 该研究提出一个框架，利用LLMs根据四种修辞类型（因果、经验、情感、道德）自动生成并标注合成辩论数据。随后，在此LLM标注的数据集上微调基于Transformer的分类器，并通过与人工标注数据以及多个外部语料库进行对比来验证其性能。

Result: 该模型在不同主题领域实现了高性能和强大的泛化能力。将修辞策略标签纳入模型显著提高了说服力预测的准确性。对1960-2020年美国总统辩论的分析显示，情感论证的使用增加，而认知论证的使用减少。

Conclusion: 该研究成功开发了一个基于LLM的自动化修辞策略分析框架，克服了传统人工标注的局限性，实现了高效、可扩展的分析。该模型不仅在分类任务中表现出色，还为理解说服性沟通和政治话语中的修辞演变提供了有价值的见解。

Abstract: Rhetorical strategies are central to persuasive communication, from political
discourse and marketing to legal argumentation. However, analysis of rhetorical
strategies has been limited by reliance on human annotation, which is costly,
inconsistent, difficult to scale. Their associated datasets are often limited
to specific topics and strategies, posing challenges for robust model
development. We propose a novel framework that leverages large language models
(LLMs) to automatically generate and label synthetic debate data based on a
four-part rhetorical typology (causal, empirical, emotional, moral). We
fine-tune transformer-based classifiers on this LLM-labeled dataset and
validate its performance against human-labeled data on this dataset and on
multiple external corpora. Our model achieves high performance and strong
generalization across topical domains. We illustrate two applications with the
fine-tuned model: (1) the improvement in persuasiveness prediction from
incorporating rhetorical strategy labels, and (2) analyzing temporal and
partisan shifts in rhetorical strategies in U.S. Presidential debates
(1960-2020), revealing increased use of affective over cognitive argument in
U.S. Presidential debates.

</details>


### [123] [Continual Learning via Sparse Memory Finetuning](https://arxiv.org/abs/2510.15103)
*Jessy Lin,Luke Zettlemoyer,Gargi Ghosh,Wen-Tau Yih,Aram Markosyan,Vincent-Pierre Berges,Barlas Oğuz*

Main category: cs.CL

TL;DR: 为解决语言模型灾难性遗忘问题，本文提出稀疏记忆微调方法。通过仅更新与新知识高度相关的记忆槽，该方法在学习新知识的同时，显著减少了对现有能力的遗忘，优于全量微调和LoRA。


<details>
  <summary>Details</summary>
Motivation: 现代语言模型部署后通常是静态的，难以持续学习。灾难性遗忘是一个主要障碍，即在新数据上更新会擦除先前获得的能力。研究动机是，由于所有任务共享可训练参数，缓解遗忘具有挑战性，因此探讨稀疏参数更新是否能实现无灾难性遗忘的学习。

Method: 引入“稀疏记忆微调”方法，利用了本身设计为稀疏更新的记忆层模型。具体做法是，仅更新那些相对于预训练数据使用情况，被新知识高度激活的记忆槽，以减少新知识与模型现有能力之间的干扰。通过在两个问答任务上与全量微调和LoRA（参数高效微调）进行比较评估。

Result: 稀疏记忆微调在学习新知识的同时，表现出显著更少的遗忘。例如，在获取相同水平的新知识后，NaturalQuestions F1分数下降：全量微调下降89%，LoRA下降71%，而稀疏记忆微调仅下降11%。

Conclusion: 研究结果表明，记忆层中的稀疏性为大型语言模型实现持续学习提供了一条有前景的途径。

Abstract: Modern language models are powerful, but typically static after deployment. A
major obstacle to building models that continually learn over time is
catastrophic forgetting, where updating on new data erases previously acquired
capabilities. Motivated by the intuition that mitigating forgetting is
challenging because trainable parameters are shared across all tasks, we
investigate whether sparse parameter updates can enable learning without
catastrophic forgetting. We introduce sparse memory finetuning, leveraging
memory layer models (Berges et al., 2024), which are sparsely updated by
design. By updating only the memory slots that are highly activated by a new
piece of knowledge relative to usage on pretraining data, we reduce
interference between new knowledge and the model's existing capabilities. We
evaluate learning and forgetting compared to full finetuning and
parameter-efficient finetuning with LoRA on two question answering tasks. We
find that sparse memory finetuning learns new knowledge while exhibiting
substantially less forgetting: while NaturalQuestions F1 drops by 89% after
full finetuning on new facts and 71% with LoRA, sparse memory finetuning yields
only an 11% drop with the same level of new knowledge acquisition. Our results
suggest sparsity in memory layers offers a promising path toward continual
learning in large language models.

</details>


### [124] [Measuring the Effect of Disfluency in Multilingual Knowledge Probing Benchmarks](https://arxiv.org/abs/2510.15115)
*Kirill Semenov,Rico Sennrich*

Main category: cs.CL

TL;DR: 研究发现，MLAMA等基准测试中模板翻译的语法和语义问题导致大型语言模型（LLMs）知识评估分数不准确。使用神经机器翻译或LLM进行整句翻译能显著提高知识检索分数，并建议社区控制多语言数据集的语法正确性以获得更可解释的结果。


<details>
  <summary>Details</summary>
Motivation: MLAMA等现有基准测试在评估LLMs多语言事实知识时，其模板翻译未能考虑插入的命名实体的语法和语义信息，导致最终提示存在大量语法错误或措辞不当，特别是在形态丰富的语言中，这使得分数解释复杂化。

Method: 本研究从MLAMA数据集中抽取了4种斯拉夫语言，比较了初始（模板化）MLAMA数据集与其通过Google Translate和ChatGPT进行的整句翻译后的知识检索分数。此外，还对来自不同语系的另外5种语言进行了分析，以观察类似模式。

Result: 观察到知识检索分数显著提高，并且在不同语系的更多语言中也发现了类似的模式。研究还对分数提高的可能原因提供了定性分析。

Conclusion: 鼓励社区控制高度多语言数据集的语法正确性，以获得更高和更可解释的结果，这可以通过使用神经机器翻译或LLM系统进行整句翻译来很好地实现。

Abstract: For multilingual factual knowledge assessment of LLMs, benchmarks such as
MLAMA use template translations that do not take into account the grammatical
and semantic information of the named entities inserted in the sentence. This
leads to numerous instances of ungrammaticality or wrong wording of the final
prompts, which complicates the interpretation of scores, especially for
languages that have a rich morphological inventory. In this work, we sample 4
Slavic languages from the MLAMA dataset and compare the knowledge retrieval
scores between the initial (templated) MLAMA dataset and its sentence-level
translations made by Google Translate and ChatGPT. We observe a significant
increase in knowledge retrieval scores, and provide a qualitative analysis for
possible reasons behind it. We also make an additional analysis of 5 more
languages from different families and see similar patterns. Therefore, we
encourage the community to control the grammaticality of highly multilingual
datasets for higher and more interpretable results, which is well approximated
by whole sentence translation with neural MT or LLM systems. The dataset and
all related code is published at the Github repository:
https://github.com/ZurichNLP/Fluent-mLAMA.

</details>


### [125] [FarsiMCQGen: a Persian Multiple-choice Question Generation Framework](https://arxiv.org/abs/2510.15134)
*Mohammad Heydari Rad,Rezvan Afari,Saeedeh Momtazi*

Main category: cs.CL

TL;DR: 本文提出FarsiMCQGen，一种结合Transformer、知识图和规则生成高质量波斯语多选题（MCQ）的方法，并构建了一个包含10,289个问题的波斯语MCQ数据集。


<details>
  <summary>Details</summary>
Motivation: 在教育测试中，多选题（MCQ）是评估学习者知识的有效方式，但生成高质量的MCQ，尤其是在波斯语等低资源语言中，仍然是一个重大挑战。

Method: FarsiMCQGen方法结合了候选生成、过滤和排名技术，旨在生成与真实MCQ相似的答案选项。它利用Transformer、知识图谱和基于规则的方法来创建可信的干扰项。数据来源于维基百科的通用知识问题，并引入了一个包含10,289个问题的波斯语MCQ数据集。

Result: 研究结果表明，FarsiMCQGen模型有效，所生成的数据集质量高，并已通过不同的先进大型语言模型（LLM）进行了评估。

Conclusion: FarsiMCQGen模型和所生成的数据集展示了其有效性和高质量，有望激发未来关于MCQ的进一步研究。

Abstract: Multiple-choice questions (MCQs) are commonly used in educational testing, as
they offer an efficient means of evaluating learners' knowledge. However,
generating high-quality MCQs, particularly in low-resource languages such as
Persian, remains a significant challenge. This paper introduces FarsiMCQGen, an
innovative approach for generating Persian-language MCQs. Our methodology
combines candidate generation, filtering, and ranking techniques to build a
model that generates answer choices resembling those in real MCQs. We leverage
advanced methods, including Transformers and knowledge graphs, integrated with
rule-based approaches to craft credible distractors that challenge test-takers.
Our work is based on data from Wikipedia, which includes general knowledge
questions. Furthermore, this study introduces a novel Persian MCQ dataset
comprising 10,289 questions. This dataset is evaluated by different
state-of-the-art large language models (LLMs). Our results demonstrate the
effectiveness of our model and the quality of the generated dataset, which has
the potential to inspire further research on MCQs.

</details>


### [126] [Latent Topic Synthesis: Leveraging LLMs for Electoral Ad Analysis](https://arxiv.org/abs/2510.15125)
*Alexander Brady,Tunazzina Islam*

Main category: cs.CL

TL;DR: 该论文提出了一个利用大型语言模型（LLMs）从无标签语料库中自动生成可解释主题分类的端到端框架，并将其应用于2024年美国总统大选前的Meta政治广告数据。研究揭示了政治广告的主题分布、资金模式、道德框架和人口统计定位等深层结构和两极分化现象。


<details>
  <summary>Details</summary>
Motivation: 分析社交媒体平台上庞大且快速演变的政治内容，以理解其对政治话语的影响，是一个重大的挑战。

Method: 该方法结合了无监督聚类和基于提示的标签技术，利用大型语言模型（LLMs）迭代地构建主题分类，无需预设种子集或领域专业知识。然后，将此框架应用于Meta平台（Facebook）在2024年美国总统大选前一个月的政治广告语料库。

Result: 研究发现，投票和移民广告在总支出和印象方面占据主导地位，而堕胎和选举诚信相关广告获得了不成比例的触达。资金模式呈现两极分化：经济诉求主要由保守派PAC推动，堕胎信息在支持和反对权利的联盟之间分化，犯罪与司法运动则分散在地方委员会。广告的道德框架也存在差异：堕胎广告强调自由/压迫修辞，而经济信息则混合了关怀/伤害、公平/欺骗和自由/压迫的叙事。主题显著性揭示了道德基础与议题之间的强相关性，并发现了人口统计定位的模式。

Conclusion: 这项工作支持对社交媒体上的政治信息进行可扩展、可解释的分析，使研究人员、政策制定者和公众能够更好地理解新兴叙事、两极分化动态以及数字政治传播的道德基础。

Abstract: Social media platforms play a pivotal role in shaping political discourse,
but analyzing their vast and rapidly evolving content remains a major
challenge. We introduce an end-to-end framework for automatically generating an
interpretable topic taxonomy from an unlabeled corpus. By combining
unsupervised clustering with prompt-based labeling, our method leverages large
language models (LLMs) to iteratively construct a taxonomy without requiring
seed sets or domain expertise. We apply this framework to a large corpus of
Meta (previously known as Facebook) political ads from the month ahead of the
2024 U.S. Presidential election. Our approach uncovers latent discourse
structures, synthesizes semantically rich topic labels, and annotates topics
with moral framing dimensions. We show quantitative and qualitative analyses to
demonstrate the effectiveness of our framework. Our findings reveal that voting
and immigration ads dominate overall spending and impressions, while abortion
and election-integrity achieve disproportionate reach. Funding patterns are
equally polarized: economic appeals are driven mainly by conservative PACs,
abortion messaging splits between pro- and anti-rights coalitions, and
crime-and-justice campaigns are fragmented across local committees. The framing
of these appeals also diverges--abortion ads emphasize liberty/oppression
rhetoric, while economic messaging blends care/harm, fairness/cheating, and
liberty/oppression narratives. Topic salience further reveals strong
correlations between moral foundations and issues. Demographic targeting also
emerges. This work supports scalable, interpretable analysis of political
messaging on social media, enabling researchers, policymakers, and the public
to better understand emerging narratives, polarization dynamics, and the moral
underpinnings of digital political communication.

</details>


### [127] [Structure-R1: Dynamically Leveraging Structural Knowledge in LLM Reasoning through Reinforcement Learning](https://arxiv.org/abs/2510.15191)
*Junlin Wu,Xianrui Zhong,Jiashuo Sun,Bolian Li,Bowen Jin,Jiawei Han,Qingkai Zeng*

Main category: cs.CL

TL;DR: 本文提出了Structure-R1框架，它利用强化学习将检索到的内容转化为针对推理优化的动态结构化表示，显著提升了大型语言模型（LLMs）的推理能力，尤其是在信息密度和上下文清晰度方面。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的推理能力受限于对显式和结构化领域知识的访问不足。传统的检索增强生成（RAG）系统通常处理非结构化和碎片化的文本，导致信息密度低和推理效果不佳。

Method: Structure-R1框架通过强化学习学习一种内容表示策略，该策略能根据多步推理需求动态生成和调整结构化格式。与依赖固定模式的方法不同，它采用生成范式来创建任务特定的结构。为确保表示的质量和可靠性，该框架引入了一种自奖励结构验证机制，以检查生成结构的正确性和自洽性。

Result: 在七个知识密集型基准测试中，Structure-R1使用7B规模的骨干模型持续取得了有竞争力的性能，并能与更大规模模型的性能相媲美。理论分析也表明，结构化表示通过提高信息密度和上下文清晰度来增强推理能力。

Conclusion: Structure-R1通过动态生成任务特定的结构化知识表示，有效克服了传统RAG系统的局限性，显著提升了LLMs的推理能力，并展现出与大型模型相当的性能。

Abstract: Large language models (LLMs) have demonstrated remarkable advances in
reasoning capabilities. However, their performance remains constrained by
limited access to explicit and structured domain knowledge. Retrieval-Augmented
Generation (RAG) addresses this by incorporating external information as
context to augment reasoning. Nevertheless, traditional RAG systems typically
operate over unstructured and fragmented text, resulting in low information
density and suboptimal reasoning. To overcome these limitations, we propose
\textsc{Structure-R1}, a novel framework that transforms retrieved content into
structured representations optimized for reasoning. Leveraging reinforcement
learning, \textsc{Structure-R1} learns a content representation policy that
dynamically generates and adapts structural formats based on the demands of
multi-step reasoning. Unlike prior methods that rely on fixed schemas, our
approach adopts a generative paradigm capable of producing task-specific
structures tailored to individual queries. To ensure the quality and
reliability of these representations, we introduce a self-reward structural
verification mechanism that checks whether the generated structures are both
correct and self-contained. Extensive experiments on seven knowledge-intensive
benchmarks show that \textsc{Structure-R1} consistently achieves competitive
performance with a 7B-scale backbone model and matches the performance of much
larger models. Additionally, our theoretical analysis demonstrates how
structured representations enhance reasoning by improving information density
and contextual clarity. Our code and data are available at:
https://github.com/jlwu002/sr1.

</details>


### [128] [Extending Audio Context for Long-Form Understanding in Large Audio-Language Models](https://arxiv.org/abs/2510.15231)
*Yuatyong Chaichana,Pittawat Taveekitworachai,Warit Sirichotedumrong,Potsawee Manakul,Kunat Pipatanakul*

Main category: cs.CL

TL;DR: 本文针对大型音视语言模型（LALMs）音频上下文窗口短的限制，提出了两种方法：Partial YaRN（一种免训练、仅针对音频的上下文扩展方法）和虚拟长音频训练（VLAT，一种训练策略），显著提升了模型对长音频的理解能力和泛化性。


<details>
  <summary>Details</summary>
Motivation: 大型音视语言模型（LALMs）通常受限于短音频上下文窗口，即使其文本骨干支持长上下文，这限制了它们对长篇音频的理解能力。此前针对单模态大型语言模型（LLMs）的上下文扩展方法（如YaRN）尚未应用于LALMs。

Method: ['Partial YaRN：基于RoPE的上下文扩展方法，免训练，仅修改音频token的位置，保持文本位置不变，以保留基础LLM的文本能力。', '虚拟长音频训练（VLAT）：一种训练策略，将Partial YaRN扩展为训练时的位置增强。VLAT在训练期间模拟不同长度的音频，使其能够泛化到比训练时更长的输入，并提高长上下文音频理解的鲁棒性。']

Result: ['Partial YaRN在广泛设置下均优于原始模型。', 'VLAT训练策略提供了显著的改进。', '在未见过的长音频上实现了强大的性能。', '实验在SALMONN和Qwen2-Audio模型上进行。']

Conclusion: Partial YaRN和VLAT有效地扩展了LALMs的音频上下文窗口，显著提高了模型对长篇音频的理解能力，并能泛化到训练中未见过的长音频输入，增强了鲁棒性。

Abstract: Large Audio-Language Models (LALMs) are often constrained by short audio
context windows, even when their text backbones support long contexts, limiting
long-form audio understanding. Prior work has introduced context-extension
methods (e.g. YaRN) on unimodal LLMs, yet their application to LALMs remains
unexplored. First, building on RoPE-based context extension, we introduce
Partial YaRN, a training-free, audio-only extension method that modifies only
audio token positions, leaving text positions intact to preserve the base LLM's
text capabilities. Second, we propose Virtual Longform Audio Training (VLAT), a
training strategy that extends Partial YaRN into a training-time positional
augmentation. VLAT simulates diverse audio lengths during training, enabling
generalization to inputs far longer than those seen in training and improving
robustness for long-context audio understanding. Our experiments on SALMONN and
Qwen2-Audio show that Partial YaRN outperforms the original models across wide
range of settings, and VLAT training strategy provides substantial improvement,
achieving strong performance on long audio of unseen lengths.

</details>


### [129] [Scaling Beyond Context: A Survey of Multimodal Retrieval-Augmented Generation for Document Understanding](https://arxiv.org/abs/2510.15253)
*Sensen Gao,Shanshan Zhao,Xu Jiang,Lunhao Duan,Yong Xien Chng,Qing-Guo Chen,Weihua Luo,Kaifu Zhang,Jia-Wang Bian,Mingming Gong*

Main category: cs.CL

TL;DR: 本文对用于文档理解的多模态检索增强生成（Multimodal RAG）进行了系统性综述，提出了分类法，回顾了最新进展，并指出了开放性挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 当前的文档理解方法（基于OCR的LLM管道和原生MLLM）在结构细节和上下文建模方面存在局限性。文档的多模态特性（文本、表格、图表、布局）需要更先进的多模态RAG范式来实现全面的检索和推理，以实现文档智能。

Method: 本文进行了一项系统性综述。它提出了一种基于领域、检索模态和粒度的分类法，并回顾了涉及图结构和代理框架的进展。此外，它还总结了关键数据集、基准和应用，并强调了效率、细粒度表示和鲁棒性方面的开放性挑战。

Result: 提出了多模态RAG的系统性分类法，涵盖领域、检索模态和粒度。总结了图结构和代理框架等先进技术。汇编了关键数据集、基准和应用。明确了多模态RAG在效率、细粒度表示和鲁棒性方面的开放性挑战，并为文档AI的未来发展提供了路线图。

Conclusion: 多模态RAG对于实现全面的文档智能至关重要。本综述通过提出分类法、回顾进展和识别挑战，为多模态RAG在文档理解领域提供了结构化的理解和未来发展方向。

Abstract: Document understanding is critical for applications from financial analysis
to scientific discovery. Current approaches, whether OCR-based pipelines
feeding Large Language Models (LLMs) or native Multimodal LLMs (MLLMs), face
key limitations: the former loses structural detail, while the latter struggles
with context modeling. Retrieval-Augmented Generation (RAG) helps ground models
in external data, but documents' multimodal nature, i.e., combining text,
tables, charts, and layout, demands a more advanced paradigm: Multimodal RAG.
This approach enables holistic retrieval and reasoning across all modalities,
unlocking comprehensive document intelligence. Recognizing its importance, this
paper presents a systematic survey of Multimodal RAG for document
understanding. We propose a taxonomy based on domain, retrieval modality, and
granularity, and review advances involving graph structures and agentic
frameworks. We also summarize key datasets, benchmarks, and applications, and
highlight open challenges in efficiency, fine-grained representation, and
robustness, providing a roadmap for future progress in document AI.

</details>


### [130] [Planner and Executor: Collaboration between Discrete Diffusion And Autoregressive Models in Reasoning](https://arxiv.org/abs/2510.15244)
*Lina Berrayana,Ahmed Heakl,Muhammad Abdullah Sohail,Thomas Hofmann,Salman Khan,Wei Chen*

Main category: cs.CL

TL;DR: 本研究探索了自回归语言模型 (ARM) 和离散扩散语言模型 (DDLM) 的混合架构，发现将DDLM到ARM的通信从文本空间转移到潜在空间可以显著提高准确性，并且这种混合方法能以更少的计算成本达到甚至超越大型ARM的性能。


<details>
  <summary>Details</summary>
Motivation: 当前的自回归语言模型 (ARM) 准确性高但需要长序列，成本昂贵。离散扩散语言模型 (DDLM) 能够并行灵活生成，并在复杂推理和长期规划任务中表现出色。研究旨在探索结合DDLM和ARM的混合架构是否能产生互补优势，以解决ARM的成本问题并利用DDLM的独特能力。

Method: 研究首先在文本空间中探索了模型间的协作，其中一个模型规划推理过程，另一个模型基于该规划执行最终答案。随后，研究将此设置扩展到潜在空间通信，引入了一个学习型投影器，将DDLM的潜在表示映射到ARM的嵌入空间，以期绕过扩散模型在文本生成方面的一些限制。

Result: 研究发现，将DDLM到ARM的通信从文本空间转移到潜在空间能带来显著的准确性提升，例如DART-5任务的准确率从27.0%增加到54.0%，AIME24任务从0.0%增加到14.0%。此外，结合DDLM规划器和ARM执行器可以在计算上节省大量成本，而对准确性影响甚微。例如，使用少量token进行规划和执行的潜在空间管道，在DART-5和AIME任务上超越了使用多44倍token的Qwen3.1-7B模型。

Conclusion: 本研究为使用DDLM进行推理提供了新见解，并强调了它们在混合架构中的潜力。特别是，通过潜在空间通信结合DDLM和ARM可以显著提高性能并实现计算效率，为未来的语言模型设计提供了有价值的方向。

Abstract: Current autoregressive language models (ARMs) achieve high accuracy but
require long token sequences, making them costly. Discrete diffusion language
models (DDLMs) enable parallel and flexible generation within a fixed number of
steps and have recently emerged for their strong performance in complex
reasoning and long-term planning tasks. We present a study exploring hybrid
architectures that couple DDLMs with ARMs to assess whether their collaboration
can yield complementary benefits. We first examine collaboration in text space,
where one model plans the reasoning process and another executes the final
answer based on that plan. We then extend this setup to latent-space
communication, introducing a learned projector that maps DDLM latents into the
ARM's embedding space, potentially bypassing some of the text-generation
limitations of diffusion models. We find that shifting DDLM --> ARM
communication from text space to latent space yields significant accuracy
gains, for example increasing from 27.0% to 54.0% on DART-5 and from 0.0% to
14.0% on AIME24. We also find that combining a DDLM planner with an ARM
executor can provide substantial computational savings with little to no impact
on accuracy. For example, the latent-space pipeline, using 64 tokens for
planning and roughly 5 for execution, surpasses Qwen3.1-7B on DART-5 and AIME,
despite Qwen using 44 times more tokens. Overall, our study offers new insights
into reasoning with DDLMs and highlights their potential in hybrid
architectures.

</details>


### [131] [TraceCoder: Towards Traceable ICD Coding via Multi-Source Knowledge Integration](https://arxiv.org/abs/2510.15267)
*Mucheng Ren,He Chen,Yuchen Yan,Danqing Hu,Jun Xu,Xian Zeng*

Main category: cs.CL

TL;DR: TraceCoder是一个新颖的框架，通过整合多源外部知识（UMLS、维基百科、LLM）和混合注意力机制，解决了自动化ICD编码中的语义鸿沟、稀有代码性能差和可解释性不足的问题，并在MIMIC数据集上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 自动化国际疾病分类 (ICD) 编码在医疗系统中至关重要，但现有方法面临挑战：临床文本与ICD代码之间的语义鸿沟、稀有和长尾代码的性能不佳，以及可解释性有限。

Method: 本文提出了TraceCoder框架，它整合了多源外部知识（包括UMLS、维基百科和大型语言模型LLMs），以丰富代码表示、弥合语义鸿沟并处理稀有和模糊的代码。此外，它引入了一种混合注意力机制，用于建模标签、临床上下文和知识之间的交互，从而提高长尾代码识别能力，并通过外部证据使预测可解释。

Result: 在MIMIC-III-ICD9、MIMIC-IV-ICD9和MIMIC-IV-ICD10数据集上的实验表明，TraceCoder取得了最先进的性能。消融研究也验证了其各个组件的有效性。

Conclusion: TraceCoder为自动化ICD编码提供了一个可扩展且鲁棒的解决方案，符合临床对准确性、可解释性和可靠性的需求。

Abstract: Automated International Classification of Diseases (ICD) coding assigns
standardized diagnosis and procedure codes to clinical records, playing a
critical role in healthcare systems. However, existing methods face challenges
such as semantic gaps between clinical text and ICD codes, poor performance on
rare and long-tail codes, and limited interpretability. To address these
issues, we propose TraceCoder, a novel framework integrating multi-source
external knowledge to enhance traceability and explainability in ICD coding.
TraceCoder dynamically incorporates diverse knowledge sources, including UMLS,
Wikipedia, and large language models (LLMs), to enrich code representations,
bridge semantic gaps, and handle rare and ambiguous codes. It also introduces a
hybrid attention mechanism to model interactions among labels, clinical
context, and knowledge, improving long-tail code recognition and making
predictions interpretable by grounding them in external evidence. Experiments
on MIMIC-III-ICD9, MIMIC-IV-ICD9, and MIMIC-IV-ICD10 datasets demonstrate that
TraceCoder achieves state-of-the-art performance, with ablation studies
validating the effectiveness of its components. TraceCoder offers a scalable
and robust solution for automated ICD coding, aligning with clinical needs for
accuracy, interpretability, and reliability.

</details>


### [132] [TACL: Threshold-Adaptive Curriculum Learning Strategy for Enhancing Medical Text Understanding](https://arxiv.org/abs/2510.15269)
*Mucheng Ren,Yucheng Yan,He Chen,Danqing Hu,Jun Xu,Xian Zeng*

Main category: cs.CL

TL;DR: 本文提出TACL（阈值自适应课程学习）框架，通过动态调整训练过程以适应医疗文本的复杂性，显著提升了多语言医疗数据上多种临床任务的性能。


<details>
  <summary>Details</summary>
Motivation: 医疗文本（特别是电子病历）虽蕴含巨大潜力，但其非结构化、领域特定语言和上下文差异使其自动化理解极具挑战。现有NLP方法常将所有数据视为同等复杂，忽略了临床记录固有的复杂性差异，导致模型泛化能力受限，难以有效处理罕见或复杂病例。

Method: 本文提出了TACL（Threshold-Adaptive Curriculum Learning）框架，灵感来源于渐进式学习原则。TACL通过动态调整训练过程，根据单个样本的复杂性对数据进行难度分级，并在训练初期优先处理较简单的病例，从而为模型打下坚实基础，再逐步处理更复杂的记录。

Result: 将TACL应用于包括英语和中文临床记录在内的多语言医疗数据后，在自动ICD编码、再入院预测和中医证候分化等多种临床任务上均观察到显著的性能提升。

Conclusion: TACL不仅提升了自动化系统的性能，还展示了统一不同医疗领域方法的潜力，为更准确、可扩展和全球适用的医疗文本理解解决方案铺平了道路。

Abstract: Medical texts, particularly electronic medical records (EMRs), are a
cornerstone of modern healthcare, capturing critical information about patient
care, diagnoses, and treatments. These texts hold immense potential for
advancing clinical decision-making and healthcare analytics. However, their
unstructured nature, domain-specific language, and variability across contexts
make automated understanding an intricate challenge. Despite the advancements
in natural language processing, existing methods often treat all data as
equally challenging, ignoring the inherent differences in complexity across
clinical records. This oversight limits the ability of models to effectively
generalize and perform well on rare or complex cases. In this paper, we present
TACL (Threshold-Adaptive Curriculum Learning), a novel framework designed to
address these challenges by rethinking how models interact with medical texts
during training. Inspired by the principle of progressive learning, TACL
dynamically adjusts the training process based on the complexity of individual
samples. By categorizing data into difficulty levels and prioritizing simpler
cases early in training, the model builds a strong foundation before tackling
more complex records. By applying TACL to multilingual medical data, including
English and Chinese clinical records, we observe significant improvements
across diverse clinical tasks, including automatic ICD coding, readmission
prediction and TCM syndrome differentiation. TACL not only enhances the
performance of automated systems but also demonstrates the potential to unify
approaches across disparate medical domains, paving the way for more accurate,
scalable, and globally applicable medical text understanding solutions.

</details>


### [133] [Exemplar-Guided Planing: Enhanced LLM Agent for KGQA](https://arxiv.org/abs/2510.15283)
*Jingao Xu,Shuoyoucheng Ma,Xin Song,Rong Jiang,Hongkui Tu,Bin Zhou*

Main category: cs.CL

TL;DR: 该论文提出了一种名为Exemplar-Guided Planning (EGP)的新框架，通过利用示例引导规划和智能前瞻机制，显著提升了大型语言模型(LLM)代理在知识图谱问答(KGQA)中的规划能力和效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在知识图谱问答中面临自然语言查询与结构化知识图谱之间语义鸿沟的挑战，导致规划次优和探索效率低下。同时，无训练方法未能充分利用训练数据中宝贵的推理模式。

Method: EGP框架首先通过实体模板预处理训练集问题以标准化语义变体。接着，它利用语义嵌入和FAISS索引检索高度相似的示例问题及其成功的推理路径。这些检索到的示例动态地指导LLM的规划过程，包括：1) 任务分解，通过将生成的子目标与已验证的推理步骤对齐；2) 关系探索，通过提供高质量的辅助信息来提高关系剪枝的准确性。此外，EGP引入了智能前瞻机制，通过预先探索有前景的路径并可能提前终止探索来提高关系探索的效率。该方法被应用于Plan-on-Graph (PoG)框架，命名为PoG-EGP。

Result: 在WebQSP和CWQ两个真实世界KGQA数据集上的广泛实验表明，PoG-EGP显著优于基线PoG系统和其他对比方法。

Conclusion: EGP框架通过示例引导规划和智能前瞻机制，有效弥合了自然语言与知识图谱之间的语义鸿沟，显著提升了LLM代理在知识图谱问答任务中的规划能力和整体性能。

Abstract: Large Language Models (LLMs) as interactive agents show significant promise
in Knowledge Graph Question Answering (KGQA) but often struggle with the
semantic gap between natural language queries and structured knowledge graph
(KG) representations. This leads to suboptimal planning and inefficient
exploration on KG, while training-free approaches often underutilize valuable
reasoning patterns in training data. To address these limitations, we propose a
novel framework, Exemplar-Guided Planning (EGP), which enhances the planning
capabilities of LLM agents for KGQA. EGP first preprocesses the training set
questions via entity templating to normalize semantic variations. It then
retrieves highly similar exemplary questions and their successful reasoning
paths from this preprocessed set using semantic embeddings and an efficient
FAISS index. These retrieved exemplars dynamically guide the LLM's planning
process in two key phases: (1) Task Decomposition, by aligning generated
sub-objectives with proven reasoning steps, and (2) Relation Exploration, by
providing high-quality auxiliary information to improve relation pruning
accuracy. Additionally, we introduce a Smart Lookahead mechanism during
relation exploration to improve efficiency by preemptively exploring promising
paths and potentially terminating exploration earlier. We apply EGP to the
Plan-on-Graph (PoG) framework, termed PoG-EGP. Extensive experiments on two
real-world KGQA datasets, WebQSP and CWQ, demonstrate that PoG-EGP
significantly improves over the baseline PoG system and other compared methods.

</details>


### [134] [Automatic essay scoring: leveraging Jaccard coefficient and Cosine similaritywith n-gram variation in vector space model approach](https://arxiv.org/abs/2510.15311)
*Andharini Dwi Cahyani,Moh. Wildan Fathoni,Fika Hastarita Rachman,Ari Basuki,Salman Amin,Bain Khusnul Khotimah*

Main category: cs.CL

TL;DR: 本研究评估了在向量空间模型中，Jaccard系数和余弦相似度结合不同N-gram（一元、二元、三元）在自动作文评分中的表现。结果显示余弦相似度优于Jaccard系数，且一元N-gram的RMSE最低。


<details>
  <summary>Details</summary>
Motivation: 自动作文评分（AES）是一个重要的研究领域，旨在提供高效准确的评估工具来评价书面内容。

Method: 研究使用了初中公民教育科目的形成性作文数据。每篇作文经过预处理，使用N-gram模型（一元、二元、三元）提取特征，然后进行向量化。接着，使用Jaccard系数和余弦相似度计算作文间的相似度分数。系统性能通过分析均方根误差（RMSE）进行评估，该误差衡量人工评分与系统生成分数之间的差异。

Result: 余弦相似度在性能上优于Jaccard系数。在N-gram方面，一元N-gram的RMSE低于二元和三元N-gram。

Conclusion: 在所研究的自动作文评分任务中，余弦相似度与一元N-gram结合使用能取得更低的均方根误差，表现优于Jaccard系数和更高阶的N-gram。

Abstract: Automated essay scoring (AES) is a vital area of research aiming to provide
efficient and accurate assessment tools for evaluating written content. This
study investigates the effectiveness of two popular similarity metrics, Jaccard
coefficient, and Cosine similarity, within the context of vector space
models(VSM)employing unigram, bigram, and trigram representations. The data
used in this research was obtained from the formative essay of the citizenship
education subject in a junior high school. Each essay undergoes preprocessing
to extract features using n-gram models, followed by vectorization to transform
text data into numerical representations. Then, similarity scores are computed
between essays using both Jaccard coefficient and Cosine similarity. The
performance of the system is evaluated by analyzing the root mean square error
(RMSE), which measures the difference between the scores given by human graders
and those generated by the system. The result shows that the Cosine similarity
outperformed the Jaccard coefficient. In terms of n-gram, unigrams have lower
RMSE compared to bigrams and trigrams.

</details>


### [135] [Accelerating Mobile Language Model Generation via Hybrid Context and Hardware Coordination](https://arxiv.org/abs/2510.15312)
*Zhiyang Chen,Daliang Xu,Haiyang Shen,Mengwei Xu,Shangguang Wang,Yun Ma*

Main category: cs.CL

TL;DR: CoordGen是一个移动推理框架，通过结合推测解码和动态硬件调度，显著加速了移动设备上基于本地数据的上下文感知大型语言模型（LLMs）的文本生成，解决了高延迟和低硬件利用率问题。


<details>
  <summary>Details</summary>
Motivation: 尽管神经处理器提高了移动设备上LLMs的预填充效率，但逐令牌的生成过程仍然存在高延迟和有限的硬件利用率问题，这限制了在智能助手和UI代理等应用中实现个性化和任务感知生成的能力。

Method: 本文提出了CoordGen框架，该框架集成了推测解码和动态硬件调度来加速移动设备上的上下文感知文本生成。它包含三个协同组件：1) 自适应执行调度，动态平衡预填充和解码阶段的计算图；2) 上下文对齐草稿，通过轻量级在线校准提高推测效率；3) 硬件高效草稿扩展，重用和扩展中间序列以提高处理并行性并降低验证成本。

Result: 在多款智能手机和代表性工作负载上的实验表明，与现有移动推理解决方案相比，CoordGen在生成速度上提高了高达3.8倍，在能效上提高了高达4.7倍。组件级分析进一步验证了每项优化的贡献。

Conclusion: CoordGen是一个有效的移动推理框架，它通过整合推测解码和动态硬件调度，成功地加速了移动设备上的上下文感知文本生成，显著提升了生成速度和能效。

Abstract: Enhancing on-device large language models (LLMs) with contextual information
from local data enables personalized and task-aware generation, powering use
cases such as intelligent assistants and UI agents. While recent developments
in neural processors have substantially improved the efficiency of prefill on
mobile devices, the token-by-token generation process still suffers from high
latency and limited hardware utilization due to its inherently memory-bound
characteristics. This work presents CoordGen, a mobile inference framework that
integrates speculative decoding with dynamic hardware scheduling to accelerate
context-aware text generation on mobile devices. The framework introduces three
synergistic components: (1) adaptive execution scheduling, which dynamically
balances compute graphs between prefill and decoding phases; (2)
context-aligned drafting, which improves speculative efficiency through
lightweight online calibration to current tasks; and (3) hardware-efficient
draft extension, which reuses and expands intermediate sequences to improve
processing parallelism and reduce verification cost. Experiments on multiple
smartphones and representative workloads show consistent improvements of up to
3.8x in generation speed and 4.7x in energy efficiency compared with existing
mobile inference solutions. Component-level analysis further validates the
contribution of each optimization.

</details>


### [136] [Capabilities and Evaluation Biases of Large Language Models in Classical Chinese Poetry Generation: A Case Study on Tang Poetry](https://arxiv.org/abs/2510.15313)
*Bolei Ma,Yina Yao,Anna-Carolina Haensch*

Main category: cs.CL

TL;DR: 本文提出一个三步评估框架来评估大型语言模型（LLMs）在古典中文诗歌生成和评估方面的表现，发现LLMs存在系统性的生成和评估偏差，尤其是在“回音室”效应下，其评估标准与人类判断存在差异。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）越来越多地应用于创意领域，但它们在古典中文诗歌生成和评估方面的表现仍未被充分理解。

Method: 研究采用了一个结合计算指标、LLM作为评判者（LLM-as-a-judge）评估和人类专家验证的三步评估框架。该框架用于评估六个最先进的LLMs在诗歌质量的多个维度（包括主题、情感、意象、形式和风格）上的表现。

Result: 分析揭示了系统性的生成和评估偏差：LLMs在评估创意质量时表现出“回音室”效应，往往趋向于与人类判断相悖的有缺陷标准。

Conclusion: 这些发现突出了当前LLMs作为文学生成代理的潜力和局限性，以及现有评估实践的局限性，从而证明在文化和技术复杂的创意任务中，持续需要人类和模型混合验证的重要性。

Abstract: Large Language Models (LLMs) are increasingly applied to creative domains,
yet their performance in classical Chinese poetry generation and evaluation
remains poorly understood. We propose a three-step evaluation framework that
combines computational metrics, LLM-as-a-judge assessment, and human expert
validation. Using this framework, we evaluate six state-of-the-art LLMs across
multiple dimensions of poetic quality, including themes, emotions, imagery,
form, and style. Our analysis reveals systematic generation and evaluation
biases: LLMs exhibit "echo chamber" effects when assessing creative quality,
often converging on flawed standards that diverge from human judgments. These
findings highlight both the potential and limitations of current capabilities
of LLMs as proxy for literacy generation and the limited evaluation practices,
thereby demonstrating the continued need of hybrid validation from both humans
and models in culturally and technically complex creative tasks.

</details>


### [137] [AutoGraph-R1: End-to-End Reinforcement Learning for Knowledge Graph Construction](https://arxiv.org/abs/2510.15339)
*Hong Ting Tsang,Jiaxin Bai,Haoyu Huang,Qiao Xiao,Tianshi Zheng,Baixuan Xu,Shujie Liu,Yangqiu Song*

Main category: cs.CL

TL;DR: AutoGraph-R1是一个通过强化学习直接优化知识图谱（KG）构建以提升检索增强生成（RAG）任务性能的框架，弥合了KG构建与应用之间的鸿沟。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱（KG）的构建过程与下游应用（如RAG）是分离的，导致生成的图谱结构并非最优，从而限制了RAG问答系统的有效性。

Method: AutoGraph-R1框架将图谱生成视为一个策略学习问题，利用强化学习训练一个大型语言模型（LLM）作为图谱构建器。奖励函数根据图谱在RAG管道中的实际效用导出，并设计了两种新颖的、任务感知的奖励函数，分别针对图谱作为知识载体和知识索引的情况。

Result: 在多个问答基准测试中，AutoGraph-R1始终使基于图谱的RAG方法相比使用任务无关的基线图谱取得显著的性能提升。

Conclusion: 该研究表明，将知识图谱的构建与应用相结合是可行的，从而将范式从构建内在“良好”的图谱转变为构建可证明“有用”的图谱。

Abstract: Building effective knowledge graphs (KGs) for Retrieval-Augmented Generation
(RAG) is pivotal for advancing question answering (QA) systems. However, its
effectiveness is hindered by a fundamental disconnect: the knowledge graph (KG)
construction process is decoupled from its downstream application, yielding
suboptimal graph structures. To bridge this gap, we introduce AutoGraph-R1, the
first framework to directly optimize KG construction for task performance using
Reinforcement Learning (RL). AutoGraph-R1 trains an LLM constructor by framing
graph generation as a policy learning problem, where the reward is derived from
the graph's functional utility in a RAG pipeline. We design two novel,
task-aware reward functions, one for graphs as knowledge carriers and another
as knowledge indices. Across multiple QA benchmarks, AutoGraph-R1 consistently
enables graph RAG methods to achieve significant performance gains over using
task-agnostic baseline graphs. Our work shows it is possible to close the loop
between construction and application, shifting the paradigm from building
intrinsically ``good'' graphs to building demonstrably ``useful'' ones.

</details>


### [138] [Readability Reconsidered: A Cross-Dataset Analysis of Reference-Free Metrics](https://arxiv.org/abs/2510.15345)
*Catarina G Belem,Parker Glenn,Alfy Samuel,Anoop Kumar,Daben Liu*

Main category: cs.CL

TL;DR: 本文研究了影响人类可读性感知的因素，发现信息内容和主题至关重要。通过评估传统和基于模型的指标，结果显示基于模型的指标与人类判断的相关性更强，表明其是更具前景的方向。


<details>
  <summary>Details</summary>
Motivation: 自动可读性评估对于有效的书面交流至关重要，但该领域受限于可读性定义不一致以及依赖文本表面属性的测量方法。

Method: 研究分析了897个人类判断，以调查影响人类可读性感知的因素。此外，还在五个英文数据集上评估了15种流行的可读性指标和6种更细致的、基于模型的指标，并将其与人类判断进行对比。

Result: 研究发现，除了表面线索，信息内容和主题强烈影响文本的可理解性。在与人类判断的相关性排名中，四种基于模型的指标始终位列前四，而表现最好的传统指标平均排名为8.6。

Conclusion: 这些发现揭示了当前可读性指标与人类感知之间的不匹配，指出基于模型的方法是更有前景的方向。

Abstract: Automatic readability assessment plays a key role in ensuring effective and
accessible written communication. Despite significant progress, the field is
hindered by inconsistent definitions of readability and measurements that rely
on surface-level text properties. In this work, we investigate the factors
shaping human perceptions of readability through the analysis of 897 judgments,
finding that, beyond surface-level cues, information content and topic strongly
shape text comprehensibility. Furthermore, we evaluate 15 popular readability
metrics across five English datasets, contrasting them with six more nuanced,
model-based metrics. Our results show that four model-based metrics
consistently place among the top four in rank correlations with human
judgments, while the best performing traditional metric achieves an average
rank of 8.6. These findings highlight a mismatch between current readability
metrics and human perceptions, pointing to model-based approaches as a more
promising direction.

</details>


### [139] [When to Ensemble: Identifying Token-Level Points for Stable and Fast LLM Ensembling](https://arxiv.org/abs/2510.15346)
*Heecheol Yun,Kwangmin Ki,Junghyun Lee,Eunho Yang*

Main category: cs.CL

TL;DR: 本文提出SAFE框架，用于长文本生成中的大型语言模型（LLM）选择性集成，通过考虑分词不匹配和概率分布共识来优化集成位置，并引入概率锐化策略，在准确性和效率上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: LLM集成通过聚合下一词元概率分布在短文本任务中表现出色，但其在长文本生成中的应用尚未充分探索。现有方法在长文本生成中若在每个词元都进行集成，性能常会下降，因此需要谨慎选择集成位置。

Method: 研究识别了决定集成位置的两个关键因素：模型间的分词不匹配和下一词元概率分布的共识。基于此，提出了SAFE（Stable And Fast LLM Ensembling）框架，通过联合考虑这两个因素进行选择性集成。此外，引入了一种概率锐化策略，将表示同一单词的多个子词元上的概率整合到单个代表性词元上，以提高稳定性。

Result: 在MATH500和BBH等多个基准测试中，SAFE在准确性和效率方面均优于现有方法。即使只对不到1%的词元进行集成，也能获得性能提升。

Conclusion: 在长文本生成中，选择性集成对于LLM的性能至关重要。SAFE框架通过智能地选择集成位置并应用概率锐化策略，有效解决了现有方法的局限性，实现了卓越的性能和效率，证明了其在实际应用中的潜力。

Abstract: Ensembling Large Language Models (LLMs) has gained attention as a promising
approach to surpass the performance of individual models by leveraging their
complementary strengths. In particular, aggregating models' next-token
probability distributions to select the next token has been shown to be
effective in various tasks. However, while successful for short-form answers,
its application to long-form generation remains underexplored. In this paper,
we show that using existing ensemble methods in long-form generation requires a
careful choice of ensembling positions, since the standard practice of
ensembling at every token often degrades performance. We identify two key
factors for determining these positions: tokenization mismatch across models
and consensus in their next-token probability distributions. Based on this, we
propose SAFE, (Stable And Fast LLM Ensembling), a framework that selectively
ensembles by jointly considering these factors. To further improve stability,
we introduce a probability sharpening strategy that consolidates probabilities
spread across multiple sub-word tokens representing the same word into a single
representative token. Our experiments on diverse benchmarks, including MATH500
and BBH, demonstrate that SAFE outperforms existing methods in both accuracy
and efficiency, with gains achieved even when ensembling fewer than 1% of
tokens.

</details>


### [140] [Infinity Parser: Layout Aware Reinforcement Learning for Scanned Document Parsing](https://arxiv.org/abs/2510.15349)
*Baode Wang,Biao Wu,Weizhen Li,Meng Fang,Zuming Huang,Jun Huang,Haozhe Wang,Yanjie Liang,Ling Chen,Wei Chu,Yuan Qi*

Main category: cs.CL

TL;DR: 本文提出了一种名为LayoutRL的强化学习框架，用于优化文档布局理解，并构建了Infinity-Doc-400K数据集来训练Infinity-Parser，一个在多种文档类型上表现出强大泛化能力的视觉语言模型，达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 将扫描图像中的文档解析为结构化格式是一个重大挑战，因为它涉及文本段落、图表、公式和表格等复杂交织的元素。现有的监督微调方法在不同文档类型之间泛化能力差，尤其是在分布外数据上表现不佳，且高质量布局感知解析任务的训练数据有限。

Method: 本文引入了LayoutRL，一个通过整合归一化编辑距离、段落计数准确性和阅读顺序保留等复合奖励来优化布局理解的强化学习框架。为了支持训练，构建了Infinity-Doc-400K数据集，并在此基础上训练了Infinity-Parser，一个视觉语言模型。

Result: Infinity-Parser在OmniDocBench、olmOCR-Bench、PubTabNet和FinTabNet等基准测试中，在广泛的文档类型、语言和结构复杂性上持续实现了最先进的性能，显著优于专业的文档解析系统和通用视觉语言模型。

Conclusion: LayoutRL和Infinity-Parser框架提供了一个强大的解决方案，能够有效应对文档解析的复杂性，并在多样化的文档类型上实现卓越的泛化能力。研究团队将发布代码、数据集和模型，以促进文档解析领域的可复现研究。

Abstract: Document parsing from scanned images into structured formats remains a
significant challenge due to its complexly intertwined elements such as text
paragraphs, figures, formulas, and tables. Existing supervised fine-tuning
methods often struggle to generalize across diverse document types, leading to
poor performance, particularly on out-of-distribution data. This issue is
further exacerbated by the limited availability of high-quality training data
for layout-aware parsing tasks. To address these challenges, we introduce
LayoutRL, a reinforcement learning framework that optimizes layout
understanding through composite rewards integrating normalized edit distance,
paragraph count accuracy, and reading order preservation. To support this
training, we construct the Infinity-Doc-400K dataset, which we use to train
Infinity-Parser, a vision-language model demonstrating robust generalization
across various domains. Extensive evaluations on benchmarks including
OmniDocBench, olmOCR-Bench, PubTabNet, and FinTabNet show that Infinity-Parser
consistently achieves state-of-the-art performance across a broad range of
document types, languages, and structural complexities, substantially
outperforming both specialized document parsing systems and general-purpose
vision-language models. We will release our code, dataset, and model to
facilitate reproducible research in document parsing.

</details>


### [141] [VocalBench-DF: A Benchmark for Evaluating Speech LLM Robustness to Disfluency](https://arxiv.org/abs/2510.15406)
*Hongcheng Liu,Yixuan Hou,Heyang Liu,Yuhao Wang,Yanfeng Wang,Yu Wang*

Main category: cs.CL

TL;DR: 研究发现现有语音大语言模型（Speech-LLMs）对语音不流畅性（特别是来自言语障碍用户）的鲁棒性极差，通过新框架VocalBench-DF评估，揭示了性能显著下降和主要瓶颈，呼吁紧急改进。


<details>
  <summary>Details</summary>
Motivation: 现有的语音大语言模型评估主要依赖理想化输入，忽视了常见的语音不流畅性（尤其与帕金森病等疾病相关），导致其在真实世界应用中的鲁棒性被高估。

Method: 引入了VocalBench-DF框架，用于系统性地、多维度地评估语音不流畅性。使用该框架评估了22个主流语音大语言模型，并进行了进一步的分析。

Result: 评估显示，语音大语言模型在处理不流畅语音时性能显著下降，表明其真实世界应用准备不足。进一步分析发现，音素级处理和长上下文建模是导致这些失败的主要瓶颈。

Conclusion: 当前语音大语言模型在处理不流畅语音方面缺乏鲁棒性，其真实世界应用能力有限。迫切需要开发新方法来改进不流畅性处理，以构建真正具有包容性的语音大语言模型。

Abstract: While Speech Large Language Models (Speech-LLMs) show strong performance in
many applications, their robustness is critically under-tested, especially to
speech disfluency. Existing evaluations often rely on idealized inputs,
overlooking common disfluencies, particularly those associated with conditions
like Parkinson's disease. This work investigates whether current Speech-LLMs
can maintain performance when interacting with users who have speech
impairments. To facilitate this inquiry, we introduce VocalBench-DF, a
framework for the systematic evaluation of disfluency across a
multi-dimensional taxonomy. Our evaluation of 22 mainstream Speech-LLMs reveals
substantial performance degradation, indicating that their real-world readiness
is limited. Further analysis identifies phoneme-level processing and
long-context modeling as primary bottlenecks responsible for these failures.
Strengthening recognition and reasoning capability from components and
pipelines can substantially improve robustness. These findings highlight the
urgent need for new methods to improve disfluency handling and build truly
inclusive Speech-LLMs

</details>


### [142] [Large-scale User Game Lifecycle Representation Learning](https://arxiv.org/abs/2510.15412)
*Yanjie Gou,Jiangming Liu,Kouying Xue,Yi Hua*

Main category: cs.CL

TL;DR: 针对游戏推荐中存在的游戏稀疏性和不平衡性问题，本文提出了用户游戏生命周期（UGL）模型和逆概率掩码（IPM）策略，以丰富用户行为并有效提取用户兴趣，显著提升了游戏广告和游戏内物品推荐的效果。


<details>
  <summary>Details</summary>
Motivation: 视频游戏产业的快速发展需要有效的广告和推荐系统。然而，现有针对海量物品的推荐系统方法不适用于游戏推荐，主要原因是游戏数量相对较少导致的稀疏性问题，以及少数热门游戏主导用户行为导致的不平衡性问题。

Method: 为解决稀疏性问题，引入了用户游戏生命周期（UGL）模型以丰富用户在游戏中的行为。此外，提出了两种创新策略来更有效地提取用户的短期和长期兴趣。为解决游戏不平衡性问题，提出了针对UGL表示学习的逆概率掩码（Inverse Probability Masking, IPM）策略。

Result: 离线和在线实验结果表明，UGL表示显著提升了模型性能。在游戏广告方面，离线AUC平均提升1.83%，在线CVR平均提升21.67%。在游戏内物品推荐方面，离线AUC提升0.5%，在线ARPU提升0.82%。

Conclusion: UGL模型和IPM策略有效解决了游戏推荐中的稀疏性和不平衡性挑战，显著提升了游戏广告和游戏内物品推荐系统的效果。

Abstract: The rapid expansion of video game production necessitates the development of
effective advertising and recommendation systems for online game platforms.
Recommending and advertising games to users hinges on capturing their interest
in games. However, existing representation learning methods crafted for
handling billions of items in recommendation systems are unsuitable for game
advertising and recommendation. This is primarily due to game sparsity, where
the mere hundreds of games fall short for large-scale user representation
learning, and game imbalance, where user behaviors are overwhelmingly dominated
by a handful of popular games. To address the sparsity issue, we introduce the
User Game Lifecycle (UGL), designed to enrich user behaviors in games.
Additionally, we propose two innovative strategies aimed at manipulating user
behaviors to more effectively extract both short and long-term interests. To
tackle the game imbalance challenge, we present an Inverse Probability Masking
strategy for UGL representation learning. The offline and online experimental
results demonstrate that the UGL representations significantly enhance model by
achieving a 1.83% AUC offline increase on average and a 21.67% CVR online
increase on average for game advertising and a 0.5% AUC offline increase and a
0.82% ARPU online increase for in-game item recommendation.

</details>


### [143] [Fine-Tuning MedGemma for Clinical Captioning to Enhance Multimodal RAG over Malaysia CPGs](https://arxiv.org/abs/2510.15418)
*Lee Qi Zun,Mohamad Zulhilmi Bin Abdul Halim,Goh Man Fye*

Main category: cs.CL

TL;DR: 本研究提出并验证了一个框架，用于专门化MedGemma模型，以生成高保真图像描述作为检索增强生成（RAG）系统的优质查询，从而提高其在马来西亚临床实践指南中的图像查询有效性。


<details>
  <summary>Details</summary>
Motivation: 检索增强生成（RAG）系统在提供基于事实的临床指导方面至关重要，但其处理图像查询的有效性有限。现有的通用视觉-语言模型（VLM）生成的图像描述往往缺乏临床特异性和事实依据，这限制了RAG系统在图像查询方面的应用。

Method: 研究采用知识蒸馏流程创建了一个涵盖皮肤科、眼底和胸部X光领域的合成数据集，以克服数据稀缺问题。随后，使用参数高效的QLoRA方法对MedGemma模型进行微调。模型的性能通过双重框架进行评估，包括分类准确率，以及通过新颖地应用RAGAS框架来衡量描述的忠实性、相关性和正确性。

Result: 微调后的模型在分类性能上展现出显著提升。RAGAS评估也证实了描述的忠实性和正确性方面有显著改进，验证了模型生成可靠、基于事实描述的能力。

Conclusion: 本研究建立了一个强大的医学视觉-语言模型（VLM）专门化流程，并验证了由此产生的模型作为高质量查询生成器的有效性。这为增强多模态RAG系统在循证临床决策支持中的应用奠定了基础。

Abstract: Retrieval-Augmented Generation systems are essential for providing fact-based
guidance from Malaysian Clinical Practice Guidelines. However, their
effectiveness with image-based queries is limited, as general Vision-Language
Model captions often lack clinical specificity and factual grounding. This
study proposes and validates a framework to specialize the MedGemma model for
generating high-fidelity captions that serve as superior queries. To overcome
data scarcity, we employ a knowledge distillation pipeline to create a
synthetic dataset across dermatology, fundus, and chest radiography domains,
and fine-tune MedGemma using the parameter-efficient QLoRA method. Performance
was rigorously assessed through a dual framework measuring both classification
accuracy and, via a novel application of the RAGAS framework, caption
faithfulness, relevancy, and correctness. The fine-tuned model demonstrated
substantial improvements in classification performance, while RAGAS evaluation
confirmed significant gains in caption faithfulness and correctness, validating
the models ability to produce reliable, factually grounded descriptions. This
work establishes a robust pipeline for specializing medical VLMs and validates
the resulting model as a high-quality query generator, laying the groundwork
for enhancing multimodal RAG systems in evidence-based clinical decision
support.

</details>


### [144] [When Seeing Is not Enough: Revealing the Limits of Active Reasoning in MLLMs](https://arxiv.org/abs/2510.15421)
*Hongcheng Liu,Pingjie Wang,Yuhao Wang,Siqu Ou,Yanfeng Wang,Yu Wang*

Main category: cs.CL

TL;DR: 现有MLLM评估侧重被动推理，与现实世界主动获取信息的需求不符。本文提出了GuessBench基准来评估MLLM在不完全信息下的主动推理能力，发现其表现远低于被动推理，并识别了感知和决策的关键挑战。


<details>
  <summary>Details</summary>
Motivation: 现有对多模态大语言模型（MLLMs）的评估主要集中在完整信息下的被动推理，这与现实世界中需要主动获取缺失证据并迭代完善决策的场景不符。因此，研究MLLMs在不完全信息下主动获取缺失证据的能力成为一个基本问题。

Method: 本文提出了一种新范式，要求MLLMs在不完全信息下主动获取缺失证据，并通过从候选池中选择目标图像来迭代完善决策，且不依赖特定任务的先验知识。为此，构建了GuessBench基准，包含感知导向和知识导向的图像，用于系统评估MLLMs的主动推理能力。评估了20个主流MLLMs，并进行了消融研究。

Result: MLLMs在主动推理任务上的表现远低于其在被动设置下的表现，表明有很大的改进空间。进一步分析发现，精细感知和及时决策是主要挑战。消融研究表明，感知增强对小型模型有益，而思维导向的方法则能为不同规模的模型带来持续的提升。

Conclusion: MLLMs在主动推理方面仍有巨大的提升空间。未来的研究应关注精细感知和思维导向的方法，以促进多模态主动推理领域的发展。

Abstract: Multimodal large language models (MLLMs) have shown strong capabilities
across a broad range of benchmarks. However, most existing evaluations focus on
passive inference, where models perform step-by-step reasoning under complete
information. This setup is misaligned with real-world use, where seeing is not
enough. This raises a fundamental question: Can MLLMs actively acquire missing
evidence under incomplete information? To bridge this gap, we require the MLLMs
to actively acquire missing evidence and iteratively refine decisions under
incomplete information, by selecting a target image from a candidate pool
without task-specific priors. To support systematic study, we propose
GuessBench, a benchmark with both perception-oriented and knowledge-oriented
images for evaluating active reasoning in MLLMs. We evaluate 20 superior MLLMs
and find that performance on active reasoning lags far behind it on passive
settings, indicating substantial room for improvement. Further analysis
identifies fine-grained perception and timely decision-making as key
challenges. Ablation studies show that perceptual enhancements benefit smaller
models, whereas thinking-oriented methods provide consistent gains across model
sizes. These results suggest promising directions for future research on
multimodal active reasoning.

</details>


### [145] [Controllable Abstraction in Summary Generation for Large Language Models via Prompt Engineering](https://arxiv.org/abs/2510.15436)
*Xiangchen Song,Yuchen Liu,Yaxuan Luan,Jinxu Guo,Xiaofan Guo*

Main category: cs.CL

TL;DR: 本研究提出了一种基于提示工程的可控抽象摘要生成方法，通过多阶段提示生成框架，解决了传统摘要方法中质量和可控性问题。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统摘要生成方法中摘要质量和可控性方面的问题。

Method: 设计了一个多阶段提示生成框架，通过对输入文本进行语义分析、主题建模和噪声控制，生成不同抽象级别的摘要。实验使用了CNN/Daily Mail数据集，并分析了不同提示长度、数据噪声和文本类型的影响。

Result: 实验结果表明，提示长度对摘要质量有显著影响，过短或过长的提示都会降低摘要质量。数据噪声对摘要生成过程产生负面影响，噪声水平越高，ROUGE-L分数越低。不同文本类型对模型生成摘要的能力有不同影响，处理新闻文本时表现最佳，处理学术文章时表现较差。

Conclusion: 本研究为使用大型语言模型改进摘要生成提供了新见解，特别是在如何通过控制提示策略和优化文本预处理来提高摘要的准确性和可控性方面。

Abstract: This study presents a controllable abstract summary generation method for
large language models based on prompt engineering. To address the issues of
summary quality and controllability in traditional methods, we design a
multi-stage prompt generation framework. This framework generates summaries
with varying levels of abstraction by performing semantic analysis, topic
modeling, and noise control on the input text. The experiment uses the
CNN/Daily Mail dataset and provides a detailed analysis of different prompt
lengths, data noise, and text types. The experimental results show that prompt
length has a significant impact on the quality of generated summaries. Both
very short and very long prompt tokens result in a decrease in summary quality.
Data noise also negatively affects the summary generation process. As noise
levels increase, the ROUGE-L score gradually decreases. Furthermore, different
text types have varying effects on the model's ability to generate summaries.
The model performs best when handling news texts, while its performance is
worse when processing academic articles. This research provides new insights
into improving summary generation using large language models, particularly in
how controlling prompt strategies and optimizing text preprocessing can enhance
summary accuracy and controllability.

</details>


### [146] [CORE: Reducing UI Exposure in Mobile Agents via Collaboration Between Cloud and Local LLMs](https://arxiv.org/abs/2510.15455)
*Gucongcong Fan,Chaoyue Niu,Chengfei Lyu,Fan Wu,Guihai Chen*

Main category: cs.CL

TL;DR: CORE是一个协作框架，结合了云端和本地大语言模型的优势，旨在减少移动智能体在执行任务时上传用户界面（UI）信息的暴露量，同时保持任务准确性。


<details>
  <summary>Details</summary>
Motivation: 云端大语言模型（LLMs）在移动智能体任务中准确性高，但每次操作都需要上传完整的UI状态，导致不必要且无关信息的暴露，存在隐私风险。本地LLMs避免了UI上传，但容量有限，导致任务成功率较低。研究动机是找到一种平衡隐私保护和任务准确性的解决方案。

Method: CORE框架包含三个核心组件：1) **布局感知块划分**：根据XML屏幕层级对语义相关的UI元素进行分组；2) **协同规划**：本地和云端LLMs协作识别当前子任务；3) **协同决策**：本地LLM对相关UI块进行排序，云端LLM在排名靠前的块中选择特定UI元素。此外，CORE还引入了多轮累积机制，以缓解本地LLM的误判或上下文限制。

Result: 实验结果表明，CORE将UI暴露量减少了高达55.6%，同时任务成功率略低于仅使用云端智能体的水平。这有效缓解了向云端不必要的隐私暴露。

Conclusion: CORE框架通过结合云端和本地LLMs的优势，成功地在移动智能体任务中减少了UI数据暴露，从而增强了隐私保护，同时保持了接近纯云端解决方案的任务准确性。

Abstract: Mobile agents rely on Large Language Models (LLMs) to plan and execute tasks
on smartphone user interfaces (UIs). While cloud-based LLMs achieve high task
accuracy, they require uploading the full UI state at every step, exposing
unnecessary and often irrelevant information. In contrast, local LLMs avoid UI
uploads but suffer from limited capacity, resulting in lower task success
rates. We propose $\textbf{CORE}$, a $\textbf{CO}$llaborative framework that
combines the strengths of cloud and local LLMs to $\textbf{R}$educe UI
$\textbf{E}$xposure, while maintaining task accuracy for mobile agents. CORE
comprises three key components: (1) $\textbf{Layout-aware block partitioning}$,
which groups semantically related UI elements based on the XML screen
hierarchy; (2) $\textbf{Co-planning}$, where local and cloud LLMs
collaboratively identify the current sub-task; and (3)
$\textbf{Co-decision-making}$, where the local LLM ranks relevant UI blocks,
and the cloud LLM selects specific UI elements within the top-ranked block.
CORE further introduces a multi-round accumulation mechanism to mitigate local
misjudgment or limited context. Experiments across diverse mobile apps and
tasks show that CORE reduces UI exposure by up to 55.6% while maintaining task
success rates slightly below cloud-only agents, effectively mitigating
unnecessary privacy exposure to the cloud. The code is available at
https://github.com/Entropy-Fighter/CORE.

</details>


### [147] [DeceptionBench: A Comprehensive Benchmark for AI Deception Behaviors in Real-world Scenarios](https://arxiv.org/abs/2510.15501)
*Yao Huang,Yitong Sun,Yichi Zhang,Ruochen Zhang,Yinpeng Dong,Xingxing Wei*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）展现出新兴的欺骗行为，本研究提出了DeceptionBench，一个用于系统评估LLMs在不同社会领域、内在行为模式和外在因素影响下欺骗倾向的基准，并揭示了模型在强化动态下欺骗行为被放大的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在认知任务上取得了显著进展，但其能力增强也带来了新兴的欺骗行为，可能在高风险部署中引发严重风险。更关键的是，在现实世界场景中对欺骗行为的刻画仍未得到充分探索。

Method: 本研究建立了DeceptionBench，这是第一个系统评估欺骗倾向的基准。它包含150个精心设计的场景，涵盖经济、医疗、教育、社交互动和娱乐五个领域，超过1,000个样本。研究探索了内在维度（模型是自利还是迎合用户）和外在维度（上下文因素如何在中性条件、奖励激励和强制压力下调节欺骗输出）。此外，还纳入了多轮交互循环以模拟现实世界反馈动态，并对LLMs和大型推理模型（LRMs）进行了广泛实验。

Result: 实验揭示了关键的脆弱性，特别是在强化动态下欺骗行为显著增强。这表明当前模型缺乏对操纵性上下文线索的强大抵抗力。

Conclusion: 当前模型易受操纵，迫切需要先进的防护措施来对抗各种欺骗行为。

Abstract: Despite the remarkable advances of Large Language Models (LLMs) across
diverse cognitive tasks, the rapid enhancement of these capabilities also
introduces emergent deceptive behaviors that may induce severe risks in
high-stakes deployments. More critically, the characterization of deception
across realistic real-world scenarios remains underexplored. To bridge this
gap, we establish DeceptionBench, the first benchmark that systematically
evaluates how deceptive tendencies manifest across different societal domains,
what their intrinsic behavioral patterns are, and how extrinsic factors affect
them. Specifically, on the static count, the benchmark encompasses 150
meticulously designed scenarios in five domains, i.e., Economy, Healthcare,
Education, Social Interaction, and Entertainment, with over 1,000 samples,
providing sufficient empirical foundations for deception analysis. On the
intrinsic dimension, we explore whether models exhibit self-interested egoistic
tendencies or sycophantic behaviors that prioritize user appeasement. On the
extrinsic dimension, we investigate how contextual factors modulate deceptive
outputs under neutral conditions, reward-based incentivization, and coercive
pressures. Moreover, we incorporate sustained multi-turn interaction loops to
construct a more realistic simulation of real-world feedback dynamics.
Extensive experiments across LLMs and Large Reasoning Models (LRMs) reveal
critical vulnerabilities, particularly amplified deception under reinforcement
dynamics, demonstrating that current models lack robust resistance to
manipulative contextual cues and the urgent need for advanced safeguards
against various deception behaviors. Code and resources are publicly available
at https://github.com/Aries-iai/DeceptionBench.

</details>


### [148] [From Characters to Tokens: Dynamic Grouping with Hierarchical BPE](https://arxiv.org/abs/2510.15517)
*Rares Dolga,Lucas Maystre,Tudor Berariu,David Barber*

Main category: cs.CL

TL;DR: 本文提出了一种动态字符分组方法，利用现有BPE分词结构，通过添加显式补丁结束标记和二次BPE压缩，实现高效、灵活、语言无关的表示，解决了传统分词方法的效率和表示问题。


<details>
  <summary>Details</summary>
Motivation: 现有子词分词（如BPE）在表示稀有词时效率低下且需要大型嵌入矩阵。字符级模型虽然解决了这些问题，但在基于Transformer的架构中引入了性能瓶颈。当前的层次模型（字符分组）要么依赖空格限制了适用语言，要么需要辅助模型引入新依赖。

Method: 提出了一种动态字符分组方法，利用现有BPE分词的结构，无需额外模型。具体做法是：在BPE token后附加显式的补丁结束标记，并引入第二级BPE压缩阶段来控制补丁粒度。

Result: 实证结果表明，该方法在性能上与基于动态熵和空格的补丁策略持平或超越，同时保持了紧凑的词汇量。

Conclusion: 该方法提供了一种高效、灵活且语言无关的表示，解决了现有子词和字符级分词的局限性，并在性能上优于现有的动态补丁策略。

Abstract: Subword tokenization methods like Byte Pair Encoding (BPE) are widely used in
large language models due to their balance of vocabulary compactness and
representational power. However, they suffer from inefficiencies in
representing rare words and require large embedding matrices. Character-level
models address these issues but introduce performance bottlenecks, particularly
in Transformer-based architectures. Recent hierarchical models attempt to merge
the benefits of both paradigms by grouping characters into patches, but
existing patching strategies either rely on whitespace-limiting applicability
to certain languages, or require auxiliary models that introduce new
dependencies. In this paper, we propose a dynamic character grouping method
that leverages the structure of existing BPE tokenization without requiring
additional models. By appending explicit end-of-patch markers to BPE tokens and
introducing a second-level BPE compression stage to control patch granularity,
our method offers efficient, flexible, and language-agnostic representations.
Empirical results demonstrate that our approach matches or exceeds the
performance of dynamic entropy- and whitespace-based patching strategies, while
maintaining a compact vocabulary.

</details>


### [149] [Temporal Referential Consistency: Do LLMs Favor Sequences Over Absolute Time References?](https://arxiv.org/abs/2510.15513)
*Ashutosh Bajpai,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: 该研究指出大型语言模型（LLMs）在时间敏感领域中缺乏时间参照一致性。为此，论文引入了一个新基准TEMP-ReCon来评估LLMs，并提出了一个基于推理路径对齐的模型UnTRaP来提高其时间一致性，实验证明了UnTRaP的有效性。


<details>
  <summary>Details</summary>
Motivation: LLMs正被广泛接受为知识来源，尤其是在法律、医疗和金融等时间敏感领域。这要求LLMs不仅要事实准确，还要在时间维度上保持一致性，具备强大的时间推理能力。然而，目前确保LLMs时间一致性的研究和评估工作非常缺乏。

Method: 本文通过以下方法解决上述问题：1) 引入了一个名为“时间参照一致性”（temporal referential consistency）的新基准。2) 开发了一个名为TEMP-ReCon的资源，用于评估各种开源和闭源LLMs在不同语言（包括英语、法语和罗马尼亚语）背景下的时间一致性。3) 提出了一个基于推理路径对齐的模型UnTRaP，旨在增强LLMs的时间参照一致性。

Result: 研究发现LLMs确实表现出不足的时间参照一致性。通过实证实验，UnTRaP模型在增强LLMs时间参照一致性方面表现出优于多个基线模型的有效性。

Conclusion: 该论文通过引入新的评估基准和资源，解决了LLMs在时间敏感查询中缺乏时间参照一致性的关键问题。同时，提出的UnTRaP模型能够有效提升LLMs的时间参照一致性，为未来LLMs在时间敏感领域的应用提供了重要的改进方向。

Abstract: The increasing acceptance of large language models (LLMs) as an alternative
to knowledge sources marks a significant paradigm shift across various domains,
including time-sensitive fields such as law, healthcare, and finance. To
fulfill this expanded role, LLMs must not only be factually accurate but also
demonstrate consistency across temporal dimensions, necessitating robust
temporal reasoning capabilities. Despite this critical requirement, efforts to
ensure temporal consistency in LLMs remain scarce including noticeable absence
of endeavors aimed at evaluating or augmenting LLMs across temporal references
in time-sensitive inquiries. In this paper, we seek to address this gap by
introducing a novel benchmark entitled temporal referential consistency,
accompanied by a resource TEMP-ReCon designed to benchmark a wide range of both
open-source and closed-source LLMs with various linguistic contexts
characterized by differing resource richness (including English, French, and
Romanian). The findings emphasis that LLMs do exhibit insufficient temporal
referent consistency. To address this, we propose \newmodel, a reasoning path
alignment-based model that aims to enhance the temporal referential consistency
of LLMs. Our empirical experiments substantiate the efficacy of UnTRaP compared
to several baseline models.

</details>


### [150] [MCA: Modality Composition Awareness for Robust Composed Multimodal Retrieval](https://arxiv.org/abs/2510.15543)
*Qiyu Wu,Shuyang Cui,Satoshi Hayakawa,Wei-Yao Wang,Hiromi Wakaki,Yuki Mitsufuji*

Main category: cs.CL

TL;DR: 本文提出了一种模态组合感知框架，以解决多模态大语言模型（MLLMs）中统一编码器在多模态检索中易受模态捷径影响导致在分布偏移下鲁棒性差的问题，通过偏好损失和组合正则化来提升模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态检索很重要，且MLLMs提供了灵活先进的统一编码器，但我们发现使用传统对比学习训练的统一编码器容易学习到模态捷径，导致在分布偏移下鲁棒性不佳。

Method: 我们提出了一个模态组合感知框架来缓解这个问题。具体来说，通过一个偏好损失强制多模态嵌入优于其单模态对应物，同时通过一个组合正则化目标将多模态嵌入与其单模态部分组成的原型对齐。这些目标明确地建模了组合表示与其单模态对应物之间的结构关系。

Result: 在各种基准测试上的实验表明，该方法在分布外检索方面取得了显著提升，突出了模态组合感知作为一种有效原则，可用于在使用MLLMs作为统一编码器时实现鲁棒的组合多模态检索。

Conclusion: 模态组合感知是利用MLLMs作为统一编码器进行鲁棒组合多模态检索的有效原则。

Abstract: Multimodal retrieval, which seeks to retrieve relevant content across
modalities such as text or image, supports applications from AI search to
contents production. Despite the success of separate-encoder approaches like
CLIP align modality-specific embeddings with contrastive learning, recent
multimodal large language models (MLLMs) enable a unified encoder that directly
processes composed inputs. While flexible and advanced, we identify that
unified encoders trained with conventional contrastive learning are prone to
learn modality shortcut, leading to poor robustness under distribution shifts.
We propose a modality composition awareness framework to mitigate this issue.
Concretely, a preference loss enforces multimodal embeddings to outperform
their unimodal counterparts, while a composition regularization objective
aligns multimodal embeddings with prototypes composed from its unimodal parts.
These objectives explicitly model structural relationships between the composed
representation and its unimodal counterparts. Experiments on various benchmarks
show gains in out-of-distribution retrieval, highlighting modality composition
awareness as a effective principle for robust composed multimodal retrieval
when utilizing MLLMs as the unified encoder.

</details>


### [151] [Latent Reasoning in LLMs as a Vocabulary-Space Superposition](https://arxiv.org/abs/2510.15522)
*Jingcheng Deng,Liang Pang,Zihao Wei,Shichen Xu,Zenghao Duan,Kun Xu,Yang Song,Huawei Shen,Xueqi Cheng*

Main category: cs.CL

TL;DR: 该研究提出Latent-SFT框架，通过将潜在推理限制在词汇空间，显著减少大型语言模型（LLM）的推理链长度（最高4倍），同时在GSM8k等基准测试上保持与显式推理相当的性能，超越了以往的潜在推理方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的显式思维链推理（chain-of-thought prompting）计算开销巨大。现有的潜在推理方法虽然降低了成本，但由于潜在空间非结构化，导致性能显著下降。

Method: 提出Latent-SFT，一个两阶段学习框架：
1.  **第一阶段：** 将潜在空间限制在LLM词汇的列空间，将潜在推理视为词汇概率的叠加。设计两种专用注意力掩码引导潜在令牌编码器生成潜在令牌，LLM基于这些令牌产生正确答案。
2.  **第二阶段：** 丢弃潜在令牌编码器，LLM直接自主生成这些潜在令牌进行潜在推理，并使用KL和CE损失进行优化。

Result: Latent-SFT在GSM8k上达到了新的最先进水平，性能与显式SFT相当，同时将推理链缩短了高达4倍，并优于先前的潜在方法。在Math500和AIME24上，基于词汇概率的潜在推理也明显优于基于隐藏状态的方法。有效压缩率和有效全局并行度指标表明，潜在推理既是单路径的压缩，也是多路径的叠加。

Conclusion: Latent-SFT通过结构化潜在空间和两阶段学习，使LLM能够在潜在空间进行高效推理，显著降低计算成本的同时保持甚至超越显式推理的性能，并优于以往的潜在推理方法。

Abstract: Large language models (LLMs) demonstrate strong reasoning abilities with
chain-of-thought prompting, but explicit reasoning introduces substantial
computational overhead. Recent work on latent reasoning reduces this cost by
reasoning in latent space without explicit supervision, but performance drops
significantly. Our preliminary experiments suggest that this degradation stems
from the unstructured latent space, which makes fitting latent tokens
difficult. To address this, we restrict the latent space to the column space of
the LLM vocabulary, treating latent reasoning as a superposition over
vocabulary probabilities. Once latent reasoning concludes, it collapses into an
eigenstate of explicit reasoning to yield the final answer. Based on this idea,
we propose Latent-SFT, a two-stage learning framework. In the first stage, we
design two specialized attention masks to guide the Latent Token Encoder in
generating latent tokens, allowing the LLM to produce the correct answer
conditioned on them. In the second stage, the Latent Token Encoder is
discarded, and the LLM is directly trained to generate these latent tokens
autonomously for latent reasoning, optimized with KL and CE losses. Latent-SFT
sets a new state of the art on GSM8k, matching explicit SFT performance while
cutting reasoning chains by up to 4 times and outperforming prior latent
methods. On Math500 and AIME24, lexical probability-based latent reasoning also
clearly surpasses hidden-state-based approaches. Our metrics of effective
compression rate and effective global parallelism further show that latent
reasoning is both the compression of a single path and the superposition of
multiple paths.

</details>


### [152] [TokenTiming: A Dynamic Alignment Method for Universal Speculative Decoding Model Pairs](https://arxiv.org/abs/2510.15545)
*Sibo Xiao,Jinyuan Fu,Zhongle Xie,Lidan Shou*

Main category: cs.CL

TL;DR: 本文提出TokenTiming算法，通过借鉴动态时间规整（DTW），解决了现有推测解码（SD）中草稿模型和目标模型词汇表不匹配的问题，实现了通用的推测解码，无需重新训练或修改模型，并展示了1.57倍的推理加速。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的推理加速是一个关键挑战。推测解码（SD）能显著提高效率，但其应用受限于草稿模型和目标模型必须共享相同词汇表的根本约束，这限制了可用草稿模型的选择，并常需要从头训练新模型。

Method: 本文提出了TokenTiming算法，灵感来源于动态时间规整（DTW）。该方法首先对草稿令牌序列进行重新编码以获得新的目标令牌序列，然后使用DTW构建映射，以转换用于推测采样的概率分布。这使得该方法能够适应不匹配的词汇表，并可与任何现成的模型配合使用，无需重新训练和修改。

Result: 通过在各种任务上进行的全面实验，该方法展示了1.57倍的推理加速。

Conclusion: TokenTiming算法为草稿模型选择提供了一种通用方法，使得推测解码成为LLM加速中更通用和实用的工具。

Abstract: Accelerating the inference of large language models (LLMs) has been a
critical challenge in generative AI. Speculative decoding (SD) substantially
improves LLM inference efficiency. However, its utility is limited by a
fundamental constraint: the draft and target models must share the same
vocabulary, thus limiting the herd of available draft models and often
necessitating the training of a new model from scratch. Inspired by Dynamic
Time Warping (DTW), a classic algorithm for aligning time series, we propose
the algorithm TokenTiming for universal speculative decoding. It operates by
re-encoding the draft token sequence to get a new target token sequence, and
then uses DTW to build a mapping to transfer the probability distributions for
speculative sampling. Benefiting from this, our method accommodates mismatched
vocabularies and works with any off-the-shelf models without retraining and
modification. We conduct comprehensive experiments on various tasks,
demonstrating 1.57x speedup. This work enables a universal approach for draft
model selection, making SD a more versatile and practical tool for LLM
acceleration.

</details>


### [153] [Rethinking Cross-lingual Gaps from a Statistical Viewpoint](https://arxiv.org/abs/2510.15551)
*Vihari Piratla,Purvam Jain,Darshan Singh,Partha Talukdar,Trevor Cohn*

Main category: cs.CL

TL;DR: 本文提出大型语言模型（LLMs）中的跨语言差距主要源于目标语言响应的方差，而非潜在表示的差异。通过偏差-方差分解形式化这一差距，并展示简单的提示指令能有效减少方差，将目标语言准确率提高20-25%。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在目标语言查询知识时，准确率低于源语言查询，即存在“跨语言差距”。现有研究将此归因于源语言和目标语言之间潜在表示的差异。本文旨在提出并验证一种新的假设，即响应方差是造成这一差距的主要原因。

Method: 研究方法包括：1) 提出目标语言响应方差是跨语言差距主要原因的假设；2) 首次通过偏差-方差分解形式化跨语言差距；3) 进行大量实验验证所提出的公式和假设；4) 通过多项推理时干预措施控制方差，以减少跨语言差距；5) 演示了一种简单的提示指令来降低响应方差。

Result: 研究结果表明：1) 实验证据支持所提出的公式和假设，即响应方差是跨语言差距的主要原因；2) 通过控制方差的推理时干预措施，可以减少跨语言差距；3) 一个简单的提示指令能够有效降低响应方差，使不同模型的准确率在目标语言上提高了20-25%。

Conclusion: LLMs中的跨语言差距主要由目标语言响应的方差引起，而非潜在表示的差异。通过诸如简单提示指令等干预措施有效控制响应方差，可以显著减少跨语言差距并提高模型在目标语言上的准确性。

Abstract: Any piece of knowledge is usually expressed in one or a handful of natural
languages on the web or in any large corpus. Large Language Models (LLMs) act
as a bridge by acquiring knowledge from a source language and making it
accessible when queried from target languages. Prior research has pointed to a
cross-lingual gap, viz., a drop in accuracy when the knowledge is queried in a
target language compared to when the query is in the source language. Existing
research has rationalized divergence in latent representations in source and
target languages as the source of cross-lingual gap. In this work, we take an
alternative view and hypothesize that the variance of responses in the target
language is the main cause of this gap. For the first time, we formalize the
cross-lingual gap in terms of bias-variance decomposition. We present extensive
experimental evidence which support proposed formulation and hypothesis. We
then reinforce our hypothesis through multiple inference-time interventions
that control the variance and reduce the cross-lingual gap. We demonstrate a
simple prompt instruction to reduce the response variance, which improved
target accuracy by 20-25% across different models.

</details>


### [154] [Think Parallax: Solving Multi-Hop Problems via Multi-View Knowledge-Graph-Based Retrieval-Augmented Generation](https://arxiv.org/abs/2510.15552)
*Jinliang Liu*

Main category: cs.CL

TL;DR: ParallaxRAG框架通过将查询和图三元组解耦到多视图空间，并利用注意力头在不同推理阶段的专业化，构建更清晰的子图，以增强大型语言模型的知识图谱检索增强生成（KG-RAG）能力，从而改善多跳推理，减少幻觉。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在语言理解方面表现出色，但常出现幻觉并难以进行多跳推理。现有的基于知识图谱的检索增强生成（KG-RAG）方法大多依赖于扁平嵌入和嘈杂的路径探索，未能有效解决这些问题。

Method: 本文提出了ParallaxRAG框架，该框架将查询和图三元组对称地解耦到多视图空间中。它显式地强制执行头部多样性，并限制弱相关路径，从而实现鲁棒的检索架构。核心思想是观察到不同的注意力头在不同的推理阶段（对应不同的推理跳数）专门处理语义关系。这种专业化有助于构建更清晰的子图，并引导LLMs进行有根据的、分步的推理。

Result: 在WebQSP和CWQ数据集上，使用统一且可复现的设置（BGE-M3 + Llama3.1-8B），ParallaxRAG展现了具有竞争力的检索和问答性能。同时，它还显著减少了幻觉，并表现出良好的泛化能力。

Conclusion: 研究结果强调了多视图头部专业化是知识驱动型多跳推理的一个原则性方向。该方法能够有效提升LLMs在复杂推理任务中的表现，减少错误。

Abstract: Large language models (LLMs) excel at language understanding but often
hallucinate and struggle with multi-hop reasoning. Knowledge-graph-based
retrieval-augmented generation (KG-RAG) offers grounding, yet most methods rely
on flat embeddings and noisy path exploration. We propose ParallaxRAG, a
framework that symmetrically decouples queries and graph triples into
multi-view spaces, enabling a robust retrieval architecture that explicitly
enforces head diversity while constraining weakly related paths. Central to our
approach is the observation that different attention heads specialize in
semantic relations at distinct reasoning stages, contributing to different hops
of the reasoning chain. This specialization allows ParallaxRAG to construct
cleaner subgraphs and guide LLMs through grounded, step-wise reasoning.
Experiments on WebQSP and CWQ, under our unified, reproducible setup (BGE-M3 +
Llama3.1-8B), demonstrate competitive retrieval and QA performance, alongside
reduced hallucination and good generalization. Our results highlight multi-view
head specialization as a principled direction for knowledge-grounded multi-hop
reasoning. Our implementation will be released as soon as the paper is
accepted.

</details>


### [155] [KITE: A Benchmark for Evaluating Korean Instruction-Following Abilities in Large Language Models](https://arxiv.org/abs/2510.15558)
*Dongjun Kim,Chanhee Park,Chanjun Park,Heuiseok Lim*

Main category: cs.CL

TL;DR: 本文介绍了KITE，一个专门用于评估大型语言模型韩语指令遵循能力的综合基准，以解决现有评估对非英语语言关注不足的问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的指令遵循能力对许多应用至关重要，但当前评估主要集中在英语模型，忽视了其他语言的语言和文化细微差别。特别是韩语，由于其独特的语法、丰富的形态特征、敬语系统和双重数字系统，缺乏专门用于评估开放式指令遵循能力的基准。

Method: 本文引入了韩语指令遵循任务评估（KITE），这是一个综合性基准，旨在评估通用和韩语特定的指令。与现有主要侧重于事实知识或多项选择测试的韩语基准不同，KITE直接针对多样化的开放式指令遵循任务。评估流程结合了自动化指标和人工评估。

Result: 评估结果揭示了不同模型之间的性能差异，并深入洞察了它们的优势和劣势。

Conclusion: 通过公开发布KITE数据集和代码，旨在促进对文化和语言包容性LLM开发的进一步研究，并激励其他代表性不足语言的类似工作。

Abstract: The instruction-following capabilities of large language models (LLMs) are
pivotal for numerous applications, from conversational agents to complex
reasoning systems. However, current evaluations predominantly focus on English
models, neglecting the linguistic and cultural nuances of other languages.
Specifically, Korean, with its distinct syntax, rich morphological features,
honorific system, and dual numbering systems, lacks a dedicated benchmark for
assessing open-ended instruction-following capabilities. To address this gap,
we introduce the Korean Instruction-following Task Evaluation (KITE), a
comprehensive benchmark designed to evaluate both general and Korean-specific
instructions. Unlike existing Korean benchmarks that focus mainly on factual
knowledge or multiple-choice testing, KITE directly targets diverse, open-ended
instruction-following tasks. Our evaluation pipeline combines automated metrics
with human assessments, revealing performance disparities across models and
providing deeper insights into their strengths and weaknesses. By publicly
releasing the KITE dataset and code, we aim to foster further research on
culturally and linguistically inclusive LLM development and inspire similar
endeavors for other underrepresented languages.

</details>


### [156] [From Ghazals to Sonnets: Decoding the Polysemous Expressions of Love Across Languages](https://arxiv.org/abs/2510.15569)
*Syed Mohammad Sualeh Ali*

Main category: cs.CL

TL;DR: 本文通过对乌尔都语诗歌中“爱”的三个词（pyaar, muhabbat, ishq）的多义性研究，揭示了其独特的语义差异和文化内涵，并与英语进行了词嵌入比较分析。


<details>
  <summary>Details</summary>
Motivation: 研究旨在深入探讨乌尔都语诗歌的主题深度，特别是通过分析三个看似同义的词语（pyaar, muhabbat, ishq）之间的细微差别，揭示乌尔都语特有的情感和体验，这些差别在英语文学中缺乏直接对应。

Method: 研究采用多义性案例研究方法，细致考察这些词语在乌尔都语诗歌中的交织使用和语境；同时，通过生成乌尔都语和英语中与“爱”相关的词语嵌入，进行比较分析，以量化和可视化这些词语所占据的语义空间。

Result: 研究揭示了乌尔都语中独特的情感和体验谱系，发现了缺乏直接英语对应词的深层含义和细微区别；通过量化和可视化语义空间，提供了关于表达爱的文化和语言细微差别的宝贵见解。

Conclusion: 本研究通过多方面方法，阐明了乌尔都语诗歌引人入胜的复杂性，加深了对其独特描绘爱及其 myriad 表达方式的理解和欣赏。

Abstract: This paper delves into the intricate world of Urdu poetry, exploring its
thematic depths through a lens of polysemy. By focusing on the nuanced
differences between three seemingly synonymous words (pyaar, muhabbat, and
ishq) we expose a spectrum of emotions and experiences unique to the Urdu
language. This study employs a polysemic case study approach, meticulously
examining how these words are interwoven within the rich tapestry of Urdu
poetry. By analyzing their usage and context, we uncover a hidden layer of
meaning, revealing subtle distinctions which lack direct equivalents in English
literature. Furthermore, we embark on a comparative analysis, generating word
embeddings for both Urdu and English terms related to love. This enables us to
quantify and visualize the semantic space occupied by these words, providing
valuable insights into the cultural and linguistic nuances of expressing love.
Through this multifaceted approach, our study sheds light on the captivating
complexities of Urdu poetry, offering a deeper understanding and appreciation
for its unique portrayal of love and its myriad expressions

</details>


### [157] [Finetuning LLMs for EvaCun 2025 token prediction shared task](https://arxiv.org/abs/2510.15561)
*Josef Jon,Ondřej Bojar*

Main category: cs.CL

TL;DR: 本文介绍了作者团队为EvaCun 2025令牌预测任务提交的系统，该系统基于对Command-R、Mistral和Aya Expanse等大型语言模型进行微调，并比较了三种不同的提示方法。


<details>
  <summary>Details</summary>
Motivation: 参与EvaCun 2025令牌预测任务的挑战赛。

Method: 该研究使用了Command-R、Mistral和Aya Expanse等大型语言模型，并利用组织者提供的数据对这些模型进行了微调。由于对主题领域和任务语言了解有限，作者团队未对训练数据进行任何任务特定的调整、预处理或过滤。他们比较了三种不同的提示方法来获取预测结果，并在部分保留数据上进行了评估。

Result: 本文比较了三种不同的预测方法（基于三种不同的提示），并在保留数据上进行了评估，但摘要中未给出具体的评估结果或哪种方法表现最佳。

Conclusion: 本文介绍了他们在EvaCun 2025令牌预测任务中的提交方案，展示了在领域知识有限的情况下，通过简单微调LLM和不同提示策略的应用。

Abstract: In this paper, we present our submission for the token prediction task of
EvaCun 2025. Our sys-tems are based on LLMs (Command-R, Mistral, and Aya
Expanse) fine-tuned on the task data provided by the organizers. As we only
pos-sess a very superficial knowledge of the subject field and the languages of
the task, we simply used the training data without any task-specific
adjustments, preprocessing, or filtering. We compare 3 different approaches
(based on 3 different prompts) of obtaining the predictions, and we evaluate
them on a held-out part of the data.

</details>


### [158] [BiMax: Bidirectional MaxSim Score for Document-Level Alignment](https://arxiv.org/abs/2510.15577)
*Xiaotian Wang,Takehito Utsuro,Masaaki Nagata*

Main category: cs.CL

TL;DR: 本文提出了一种名为BiMax的跨语言双向Maxsim分数，用于计算文档间的相似度，旨在提高大规模网络挖掘中文档对齐的效率。BiMax在保持与现有方法相当的准确性的同时，实现了约100倍的速度提升。


<details>
  <summary>Details</summary>
Motivation: 文档对齐对于分层挖掘至关重要，但面对海量的网络挖掘数据时，现有高精度方法（如Optimal Transport）在速度上存在瓶颈。因此，需要一种兼顾准确性和效率的文档对齐方法。

Method: 本文提出了一种跨语言双向Maxsim分数（BiMax）来计算文档间的相似度，以提高效率。此外，研究还对当前最先进的多语言句子嵌入模型进行了全面的性能分析。

Result: 在WMT16双语文档对齐任务上，BiMax方法在准确性上与Optimal Transport（OT）方法相当，但速度提高了约100倍。同时，论文还对最先进的多语言句子嵌入模型的性能进行了调查分析。

Conclusion: BiMax为跨语言文档对齐提供了一个显著更高效的解决方案，同时保持了高准确性，使其非常适合大规模网络挖掘应用。所有对齐方法都已作为工具EmbDA公开可用。

Abstract: Document alignment is necessary for the hierarchical mining (Ba\~n\'on et
al., 2020; Morishita et al., 2022), which aligns documents across source and
target languages within the same web domain. Several high precision sentence
embedding-based methods have been developed, such as TK-PERT (Thompson and
Koehn, 2020) and Optimal Transport (OT) (Clark et al., 2019; El-Kishky and
Guzm\'an, 2020). However, given the massive scale of web mining data, both
accuracy and speed must be considered. In this paper, we propose a
cross-lingual Bidirectional Maxsim score (BiMax) for computing doc-to-doc
similarity, to improve efficiency compared to the OT method. Consequently, on
the WMT16 bilingual document alignment task, BiMax attains accuracy comparable
to OT with an approximate 100-fold speed increase. Meanwhile, we also conduct a
comprehensive analysis to investigate the performance of current
state-of-the-art multilingual sentence embedding models. All the alignment
methods in this paper are publicly available as a tool called EmbDA
(https://github.com/EternalEdenn/EmbDA).

</details>


### [159] [The Elephant in the Coreference Room: Resolving Coreference in Full-Length French Fiction Works](https://arxiv.org/abs/2510.15594)
*Antoine Bourgois,Thierry Poibeau*

Main category: cs.CL

TL;DR: 本文介绍了一个包含三部法语长篇小说的全新共指消解语料库（超过28.5万个词元），并提出了一个模块化的共指消解流程，该流程在长文本上表现出色，并可用于推断虚构人物的性别。


<details>
  <summary>Details</summary>
Motivation: 计算文学研究中对共指消解的兴趣日益增长，但缺乏针对长文档的完整标注代表性数据集，特别是长而复杂的文学作品，这限制了在长参照链背景下评估共指模型的能力。

Method: 引入了一个包含三部完整法语小说的新标注语料库（总计超过28.5万个词元）。开发了一个模块化的共指消解流程，允许进行细粒度错误分析。

Result: 所提出的共指消解方法具有竞争力，并能有效地扩展到长文档。该方法可用于推断虚构人物的性别。

Conclusion: 新语料库和共指消解流程对于文学分析和下游NLP任务都具有重要意义，解决了长文本共指消解数据集稀缺的问题。

Abstract: While coreference resolution is attracting more interest than ever from
computational literature researchers, representative datasets of fully
annotated long documents remain surprisingly scarce. In this paper, we
introduce a new annotated corpus of three full-length French novels, totaling
over 285,000 tokens. Unlike previous datasets focused on shorter texts, our
corpus addresses the challenges posed by long, complex literary works, enabling
evaluation of coreference models in the context of long reference chains. We
present a modular coreference resolution pipeline that allows for fine-grained
error analysis. We show that our approach is competitive and scales effectively
to long documents. Finally, we demonstrate its usefulness to infer the gender
of fictional characters, showcasing its relevance for both literary analysis
and downstream NLP tasks.

</details>


### [160] [LLMs Judge Themselves: A Game-Theoretic Framework for Human-Aligned Evaluation](https://arxiv.org/abs/2510.15746)
*Gao Yang,Yuhang Liu,Siyu Miao,Xinyue Liang,Zhengyang Liu,Heyan Huang*

Main category: cs.CL

TL;DR: 本文探讨将博弈论原则应用于大型语言模型（LLM）评估，提出一种LLM相互评估（自玩和同行评审）的方法，并结合博弈论投票算法聚合评估结果，最终与人类判断进行对比验证。


<details>
  <summary>Details</summary>
Motivation: 传统的LLM评估方法通常依赖固定格式任务和参考答案，难以捕捉现代LLM行为的细致、主观和开放性特点，因此现有评估实践日益不足。

Method: 提出一种新颖的自动相互评估方法，LLM通过自玩和同行评审相互评估输出。这些同行评估随后与人类投票行为系统性比较，以评估其与人类判断的一致性。框架中融入博弈论投票算法来聚合同行评审，从而原则性地研究模型生成的排名是否反映人类偏好。

Result: 实证结果揭示了理论预测（模型排名）与人类评估之间存在收敛和分歧，为相互评估的潜力和局限性提供了宝贵见解。

Conclusion: 本文首次将相互评估、博弈论聚合和以人类为基础的验证联合应用于LLM能力评估，提供了对LLM能力评估前景和局限性的深入理解。

Abstract: Ideal or real - that is the question.In this work, we explore whether
principles from game theory can be effectively applied to the evaluation of
large language models (LLMs). This inquiry is motivated by the growing
inadequacy of conventional evaluation practices, which often rely on
fixed-format tasks with reference answers and struggle to capture the nuanced,
subjective, and open-ended nature of modern LLM behavior. To address these
challenges, we propose a novel alternative: automatic mutual evaluation, where
LLMs assess each other's output through self-play and peer review. These peer
assessments are then systematically compared with human voting behavior to
evaluate their alignment with human judgment. Our framework incorporates
game-theoretic voting algorithms to aggregate peer reviews, enabling a
principled investigation into whether model-generated rankings reflect human
preferences. Empirical results reveal both convergences and divergences between
theoretical predictions and human evaluations, offering valuable insights into
the promises and limitations of mutual evaluation. To the best of our
knowledge, this is the first work to jointly integrate mutual evaluation,
game-theoretic aggregation, and human-grounded validation for evaluating the
capabilities of LLMs.

</details>


### [161] [HypoSpace: Evaluating LLM Creativity as Set-Valued Hypothesis Generators under Underdetermination](https://arxiv.org/abs/2510.15614)
*Tingting Chen,Beibei Lin,Zifeng Yuan,Qiran Zou,Hongyu He,Yew-Soon Ong,Anirudh Goyal,Dianbo Liu*

Main category: cs.CL

TL;DR: 该研究引入了HypoSpace诊断套件，用于评估大型语言模型（LLMs）在科学问题中提出解释集的能力，衡量其有效性、独特性和覆盖率，并揭示了模型在解释空间增长时出现的模式崩溃。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在科学工作流中的应用日益增多，评估它们提出解释集（而非单一正确答案）的能力变得至关重要。许多科学问题是不确定的，即多个机制上不同的假设与相同观测结果一致。现有的仅关注正确性的指标无法捕捉到LLMs在处理这类问题时可能出现的模式崩溃等问题。

Method: 研究引入了HypoSpace诊断套件，将LLMs视为有限假设集的采样器，并测量三个互补指标：有效性（提案与观测结果一致的精确度）、独特性（提案之间的非冗余性）和恢复率（对可接受解释集的覆盖度）。HypoSpace在三个结构化领域中进行了实例化，这些领域具有确定性验证器和精确枚举的假设空间：(i) 来自扰动的因果图，(ii) 基于自上而下投影的重力约束3D体素重建，以及(iii) 布尔遗传交互作用。

Result: 在经过指令微调和注重推理的模型中，有效性通常保持较高水平，而随着可接受解释空间（admissible space）的增长，独特性和恢复率会下降。这揭示了模式崩溃现象，而这种现象是仅依赖正确性指标无法发现的。

Conclusion: HypoSpace提供了一个受控的探测工具（而非排行榜），用于评估那些明确探索和覆盖可接受解释空间的方法。它有助于深入理解LLMs在处理不确定科学问题时的行为和局限性。

Abstract: As language models are increasingly used in scientific workflows, evaluating
their ability to propose sets of explanations-not just a single correct
answer-becomes critical. Many scientific problems are underdetermined:
multiple, mechanistically distinct hypotheses are consistent with the same
observations. We introduce HypoSpace, a diagnostic suite that treats LLMs as
samplers of finite hypothesis sets and measures three complementary indicators:
Validity (precision of proposals consistent with observations), Uniqueness
(non-redundancy among proposals), and Recovery (coverage of the enumerated
admissible set). We instantiate HypoSpace in three structured domains with
deterministic validators and exactly enumerated hypothesis spaces: (i) causal
graphs from perturbations, (ii) gravity-constrained 3D voxel reconstruction
from top-down projections, and (iii) Boolean genetic interactions. Across
instruction-tuned and reasoning-focused models, Validity often remains high
while Uniqueness and Recovery degrade as the admissible space grows, revealing
mode collapse that is invisible to correctness-only metrics. HypoSpace offers a
controlled probe-rather than a leaderboard-for methods that explicitly explore
and cover admissible explanation spaces. Code is available at:
https://github.com/CTT-Pavilion/_HypoSpace.

</details>


### [162] [Cost-Aware Retrieval-Augmentation Reasoning Models with Adaptive Retrieval Depth](https://arxiv.org/abs/2510.15719)
*Helia Hashemi,Victor Rühle,Saravan Rajmohan*

Main category: cs.CL

TL;DR: 本文提出了一种动态调整检索文档列表长度的检索增强推理模型，并通过成本感知优势函数进行强化学习训练，显著提高了模型效率（延迟降低16-20%），同时提升了效果（准确率平均提高5%）。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强推理模型由于检索和推理令牌的高计算成本，导致资源消耗巨大。

Method: ['提出了一种检索增强推理模型，该模型根据查询和检索结果动态调整检索文档列表的长度。', '开发了一种成本感知优势函数，用于通过强化学习训练高效的检索增强推理模型。', '探索了所提出的成本感知框架在近端和组相对策略优化算法中的内存和延迟受限实现。']

Result: 在七个公共问答数据集上进行评估，结果显示模型延迟平均降低约16-20%，同时在准确匹配方面，模型效果平均提高约5%，实现了显著的效率提升且未损害有效性。

Conclusion: 所提出的动态检索增强推理模型及其成本感知训练框架，能够显著提高计算效率并同时提升模型性能，有效解决了现有模型的成本问题。

Abstract: Reasoning models have gained significant attention due to their strong
performance, particularly when enhanced with retrieval augmentation. However,
these models often incur high computational costs, as both retrieval and
reasoning tokens contribute substantially to the overall resource usage. In
this work, we make the following contributions: (1) we propose a
retrieval-augmented reasoning model that dynamically adjusts the length of the
retrieved document list based on the query and retrieval results; (2) we
develop a cost-aware advantage function for training of efficient
retrieval-augmented reasoning models through reinforcement learning; and (3) we
explore both memory- and latency-bound implementations of the proposed
cost-aware framework for both proximal and group relative policy optimization
algorithms. We evaluate our approach on seven public question answering
datasets and demonstrate significant efficiency gains, without compromising
effectiveness. In fact, we observed that the model latency decreases by ~16-20%
across datasets, while its effectiveness increases by ~5% on average, in terms
of exact match.

</details>


### [163] [Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection](https://arxiv.org/abs/2510.15685)
*Joshua Wolfe Brook,Ilia Markov*

Main category: cs.CL

TL;DR: 本研究提出一种利用大型语言模型（LLMs）生成背景上下文并将其融入仇恨言论检测（HSD）分类器输入的新方法，在文本和多模态HSD任务上均取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有仇恨言论检测方法可能缺乏足够的背景知识来准确识别隐性或多模态仇恨言论。研究旨在利用LLMs作为动态知识库，生成相关上下文，从而提高HSD的准确性和鲁棒性。

Method: 研究探索了两种上下文生成策略（基于命名实体和全文本提示）和四种上下文融入分类器输入的方法：文本拼接、嵌入拼接、基于分层Transformer的融合以及LLM驱动的文本增强。实验在文本隐性仇恨言论数据集Latent Hatred和多模态厌女模因数据集MAMI上进行。

Result: 实验结果表明，上下文信息及其融入方式对HSD性能至关重要。与无上下文基线相比，性能最佳的系统（基于嵌入拼接）在文本设置中F1分数提升高达3点，在多模态设置中提升高达6点。

Conclusion: 将LLM生成的上下文信息整合到仇恨言论检测系统中，可以显著提高文本和多模态HSD的性能。其中，嵌入拼接是一种高效的上下文融入方法。

Abstract: This research introduces a novel approach to textual and multimodal Hate
Speech Detection (HSD), using Large Language Models (LLMs) as dynamic knowledge
bases to generate background context and incorporate it into the input of HSD
classifiers. Two context generation strategies are examined: one focused on
named entities and the other on full-text prompting. Four methods of
incorporating context into the classifier input are compared: text
concatenation, embedding concatenation, a hierarchical transformer-based
fusion, and LLM-driven text enhancement. Experiments are conducted on the
textual Latent Hatred dataset of implicit hate speech and applied in a
multimodal setting on the MAMI dataset of misogynous memes. Results suggest
that both the contextual information and the method by which it is incorporated
are key, with gains of up to 3 and 6 F1 points on textual and multimodal setups
respectively, from a zero-context baseline to the highest-performing system,
based on embedding concatenation.

</details>


### [164] [Attention Sinks in Diffusion Language Models](https://arxiv.org/abs/2510.15731)
*Maximo Eduardo Rulli,Simone Petruzzi,Edoardo Michielon,Fabrizio Silvestri,Simone Scardapane,Alessio Devoto*

Main category: cs.CL

TL;DR: 本研究对掩码扩散语言模型（DLMs）的注意力模式进行了实证分析，发现它们也存在注意力汇聚现象，但与自回归模型（ARMs）不同，DLMs的汇聚位置是动态变化的，并且对移除注意力汇聚具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 尽管掩码扩散语言模型（DLMs）的效率和有效性已被广泛研究，但其内部运作机制，特别是注意力模式，在很大程度上仍未被探索。

Method: 本文对DLM的注意力模式进行了实证分析，重点关注了注意力汇聚现象，该现象此前已在各种基于Transformer的架构中观察到。

Result: 研究发现DLMs也表现出注意力汇聚现象，但具有独特特征：1) 与ARMs不同，DLMs中的注意力汇聚位置在生成过程中会动态移动；2) 与ARMs对移除注意力汇聚高度敏感不同，DLMs对此表现出鲁棒性，移除注意力汇聚只会导致性能轻微下降。

Conclusion: 这些结果为扩散语言模型的内部运作提供了新见解，并突出了它们在注意力分配和利用方式上与自回归模型的根本区别。

Abstract: Masked Diffusion Language Models (DLMs) have recently emerged as a promising
alternative to traditional Autoregressive Models (ARMs). DLMs employ
transformer encoders with bidirectional attention, enabling parallel token
generation while maintaining competitive performance. Although their efficiency
and effectiveness have been extensively studied, the internal mechanisms that
govern DLMs remain largely unexplored. In this work, we conduct an empirical
analysis of DLM attention patterns, focusing on the attention sinking
phenomenon, an effect previously observed in various transformer-based
architectures. Our findings reveal that DLMs also exhibit attention sinks, but
with distinct characteristics. First, unlike in ARMs, the sink positions in
DLMs tend to shift throughout the generation process, displaying a dynamic
behaviour. Second, while ARMs are highly sensitive to the removal of attention
sinks, DLMs remain robust: masking sinks leads to only a minor degradation in
performance. These results provide new insights into the inner workings of
diffusion-based language models and highlight fundamental differences in how
they allocate and utilize attention compared to autoregressive models.

</details>


### [165] [Emergence of Linear Truth Encodings in Language Models](https://arxiv.org/abs/2510.15804)
*Shauli Ravfogel,Gilad Yehudai,Tal Linzen,Joan Bruna,Alberto Bietti*

Main category: cs.CL

TL;DR: 本文引入了一个透明的单层Transformer玩具模型，解释了大型语言模型中区分真假陈述的线性“真理子空间”是如何产生的。


<details>
  <summary>Details</summary>
Motivation: 先前的探究研究表明大型语言模型存在能区分真假陈述的线性子空间，但其形成机制尚不明确。

Method: 研究引入了一个透明的单层Transformer玩具模型，该模型能端到端地复现真理子空间。通过设计一种数据分布，其中事实陈述与其他事实陈述共同出现（反之亦然），鼓励模型学习这种区分以降低语言模型损失。同时，研究还在预训练语言模型中验证了这种模式。在玩具模型中，观察了学习动态。

Result: 玩具模型成功复现了真理子空间。研究发现了一种两阶段学习动态：网络首先在几个步骤中记忆个别事实关联，然后经过更长时间的学习，学会线性分离真假陈述，这反过来又降低了语言模型的损失。预训练语言模型实验也证实了这种模式。

Conclusion: 这些结果为语言模型中线性真理表示的出现提供了机制上的演示和经验上的动机，解释了它们如何以及为何会产生。

Abstract: Recent probing studies reveal that large language models exhibit linear
subspaces that separate true from false statements, yet the mechanism behind
their emergence is unclear. We introduce a transparent, one-layer transformer
toy model that reproduces such truth subspaces end-to-end and exposes one
concrete route by which they can arise. We study one simple setting in which
truth encoding can emerge: a data distribution where factual statements
co-occur with other factual statements (and vice-versa), encouraging the model
to learn this distinction in order to lower the LM loss on future tokens. We
corroborate this pattern with experiments in pretrained language models.
Finally, in the toy setting we observe a two-phase learning dynamic: networks
first memorize individual factual associations in a few steps, then -- over a
longer horizon -- learn to linearly separate true from false, which in turn
lowers language-modeling loss. Together, these results provide both a
mechanistic demonstration and an empirical motivation for how and why linear
truth representations can emerge in language models.

</details>


### [166] [On Non-interactive Evaluation of Animal Communication Translators](https://arxiv.org/abs/2510.15768)
*Orr Paradise,David F. Gruber,Adam Tauman Kalai*

Main category: cs.CL

TL;DR: 本文提出了一种无需参考翻译或动物互动，仅通过分析英文输出的顺序合理性来评估AI鲸鱼-英语翻译器的方法，利用分段翻译和随机排列测试来识别幻觉并验证翻译质量。


<details>
  <summary>Details</summary>
Motivation: 研究动机是如何在没有动物互动或地面观测的情况下验证AI鲸鱼-英语翻译器的有效性，尤其是在缺乏参考翻译时。目标是开发一种安全、道德且经济高效的评估方法。

Method: 核心方法是结合“分段翻译”和经典的NLP“随机排列测试”（shuffle test）。具体来说，翻译动物交流的每一段，然后评估翻译结果在原始顺序下比随机排列后更有意义的频率。这是一种在没有参考翻译的情况下进行机器翻译质量评估（MTQE）的方法，旨在识别“幻觉”。

Result: 理论和概念验证实验表明，对于足够复杂的语言，互动甚至观测可能不是必需的。在数据稀缺的人类语言和构建语言上的概念验证实验证明了该评估方法的潜在实用性。该无参考指标与基于参考翻译的标准评估高度相关。理论分析也表明，在学习翻译的早期阶段，互动可能既非必要也非高效。

Conclusion: 该研究提出了一种新颖的、无需参考翻译的机器翻译质量评估方法，通过分析翻译输出的顺序合理性，可以在没有直接互动的情况下评估复杂的语言翻译器（如鲸鱼-英语翻译器），这在安全性、伦理和成本方面具有潜在优势，尤其适用于数据稀缺场景。

Abstract: If you had an AI Whale-to-English translator, how could you validate whether
or not it is working? Does one need to interact with the animals or rely on
grounded observations such as temperature? We provide theoretical and
proof-of-concept experimental evidence suggesting that interaction and even
observations may not be necessary for sufficiently complex languages. One may
be able to evaluate translators solely by their English outputs, offering
potential advantages in terms of safety, ethics, and cost. This is an instance
of machine translation quality evaluation (MTQE) without any reference
translations available. A key challenge is identifying ``hallucinations,''
false translations which may appear fluent and plausible. We propose using
segment-by-segment translation together with the classic NLP shuffle test to
evaluate translators. The idea is to translate animal communication, turn by
turn, and evaluate how often the resulting translations make more sense in
order than permuted. Proof-of-concept experiments on data-scarce human
languages and constructed languages demonstrate the potential utility of this
evaluation methodology. These human-language experiments serve solely to
validate our reference-free metric under data scarcity. It is found to
correlate highly with a standard evaluation based on reference translations,
which are available in our experiments. We also perform a theoretical analysis
suggesting that interaction may not be necessary nor efficient in the early
stages of learning to translate.

</details>


### [167] [Paper2Web: Let's Make Your Paper Alive!](https://arxiv.org/abs/2510.15842)
*Yuhang Chen,Tianpeng Lv,Siyi Zhang,Yixiang Yin,Yao Wan,Philip S. Yu,Dongping Chen*

Main category: cs.CL

TL;DR: 本文提出了Paper2Web，一个用于评估学术网页生成的基准数据集和多维度评估框架，并引入了PWAgent，一个能将科学论文转换为交互式、多媒体丰富学术主页的自主管道，其性能显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 学术项目网站在传播研究成果时，需要清晰呈现核心内容并实现直观导航和交互。然而，现有方法（如LLM直接生成、模板或直接HTML转换）难以生成具有布局感知能力的交互式网站，并且缺乏针对此任务的全面评估套件。

Method: 本文提出了：1. Paper2Web，一个基准数据集和多维度评估框架，包含基于规则的指标（连接性、完整性）、人类验证的LLM-as-a-Judge（评估交互性、美观性、信息量）以及PaperQuiz（衡量论文级知识保留）。2. PWAgent，一个自主管道，通过MCP工具迭代优化内容和布局，将科学论文转换为交互式、多媒体丰富的学术主页。

Result: 实验表明，PWAgent在保持低成本的同时，性能显著优于基于模板的网页和arXiv/alphaXiv版本等端到端基线，在学术网页生成方面达到了帕累托前沿。

Conclusion: Paper2Web和PWAgent为生成高质量、交互式学术网站提供了一个全面的解决方案，有效解决了现有方法的局限性。

Abstract: Academic project websites can more effectively disseminate research when they
clearly present core content and enable intuitive navigation and interaction.
However, current approaches such as direct Large Language Model (LLM)
generation, templates, or direct HTML conversion struggle to produce
layout-aware, interactive sites, and a comprehensive evaluation suite for this
task has been lacking. In this paper, we introduce Paper2Web, a benchmark
dataset and multi-dimensional evaluation framework for assessing academic
webpage generation. It incorporates rule-based metrics like Connectivity,
Completeness and human-verified LLM-as-a-Judge (covering interactivity,
aesthetics, and informativeness), and PaperQuiz, which measures paper-level
knowledge retention. We further present PWAgent, an autonomous pipeline that
converts scientific papers into interactive and multimedia-rich academic
homepages. The agent iteratively refines both content and layout through MCP
tools that enhance emphasis, balance, and presentation quality. Our experiments
show that PWAgent consistently outperforms end-to-end baselines like
template-based webpages and arXiv/alphaXiv versions by a large margin while
maintaining low cost, achieving the Pareto-front in academic webpage
generation.

</details>


### [168] [InfiMed-ORBIT: Aligning LLMs on Open-Ended Complex Tasks via Rubric-Based Incremental Training](https://arxiv.org/abs/2510.15859)
*Pengkai Wang,Qi Zuo,Pengwei Liu,Zhijie Sang,Congkai Xie,Hongxia Yang*

Main category: cs.CL

TL;DR: ORBIT是一种新颖的、基于开放式评价标准增量训练框架，旨在通过动态生成的评价标准和增量强化学习，解决大型语言模型在医疗对话等开放式、主观领域中缺乏奖励函数的问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在数学和代码等奖励可编程验证的领域通过强化学习（RL）取得了显著进展，但在创意写作、科学推理和医疗咨询等奖励模糊、主观或依赖上下文的开放式领域中，缺乏强大的奖励函数，使得当前RL策略面临挑战。

Method: 本文提出了ORBIT框架，该框架专门为高风险医疗对话设计。它将合成对话生成与动态评价标准创建相结合，并利用这些评价标准来指导增量强化学习过程。此方法不依赖外部医学知识或手动规则，而是利用评价标准引导的反馈来塑造学习。

Result: 将ORBIT应用于Qwen3-4B-Instruct模型，仅使用2k样本，在HealthBench-Hard基准测试上的性能从7.0大幅提升至27.2，达到了同等规模模型的最新水平。分析证实，评价标准驱动的强化学习在各种咨询场景中都能带来持续的性能提升，超越了简单的数值改进。

Conclusion: 研究结果强调，基于评价标准的反馈是一种可扩展的策略，能够推动大型语言模型在复杂、开放式任务（如医疗对话）中的发展。

Abstract: Large Language Models (LLMs) have shown substantial advances through
reinforcement learning (RL), particularly in domains where rewards can be
programmatically verified, such as mathematics and code. In these areas, models
benefit from a well-defined operational base guided by explicit rule-based
objectives. However, this progress reveals a significant limitation: in
open-ended domains where rewards are ambiguous, subjective, or
context-dependent, such as creative writing, scientific reasoning, and notably
medical consultation, robust reward functions are lacking, making these areas
challenging for current RL strategies. To bridge this gap, we introduce ORBIT,
an open-ended rubric-based incremental training framework specifically designed
for high-stakes medical dialogue. ORBIT integrates syn- thetic dialogue
generation with the dynamic creation of rubrics, employing these rubrics to
direct an incremental RL process. In particular, this approach does not depend
on external medical knowledge or manual rules, instead utilizing rubric-guided
feedback to shape learning. When implemented on the Qwen3-4B-Instruct model,
our method can greatly enhance its performance on the HealthBench-Hard
benchmark from 7.0 to 27.2 using only 2k samples, thus achieving
state-of-the-art results for models of this scale. Our analysis confirms that
rubric-driven RL fos-ters consistent performance gains across diverse
consultation scenarios, going beyond simple numerical improvements. These
findings underscore rubric-based feedback as a scalable strategy for advancing
LLMs in intricate, open-ended tasks.

</details>


### [169] [SpeechLLMs for Large-scale Contextualized Zero-shot Slot Filling](https://arxiv.org/abs/2510.15851)
*Kadri Hacioglu,Manjunath K E,Andreas Stolcke*

Main category: cs.CL

TL;DR: 该研究利用语音大语言模型（SpeechLLMs）解决槽位填充任务，通过设定经验上限、识别性能差距并提出数据、架构和训练策略改进方案，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统的槽位填充任务采用语音识别后接自然语言理解的级联方式。新兴的语音大语言模型（SpeechLLMs）将语音和文本基础模型整合，为语音理解任务提供了一种更统一、生成式、遵循指令的零样本方法，并承诺在数据和计算效率方面具有优势，能泛化到未见过的槽位标签。

Method: 研究方法包括：1) 为槽位填充任务设定经验性能上限；2) 识别当前SpeechLLMs在性能、鲁棒性和泛化能力上的差距；3) 提出改进训练数据、模型架构和训练策略的方案，以缩小与上限结果的差距。

Result: 研究表明，所提出的每一项改进措施都显著提升了模型性能。同时，论文也强调了实际挑战，并为利用这些新兴模型提供了经验指导和见解。

Conclusion: 语音大语言模型在槽位填充任务中展现出巨大潜力。通过对训练数据、架构和训练策略的改进，可以有效缩小模型性能与经验上限之间的差距，为实际应用提供了宝贵的实践指导和洞察。

Abstract: Slot filling is a crucial subtask in spoken language understanding (SLU),
traditionally implemented as a cascade of speech recognition followed by one or
more natural language understanding (NLU) components. The recent advent of
speech-based large language models (speechLLMs), which integrate speech and
textual foundation models, has opened new avenues for achieving speech
understanding tasks in a more unified, generative, and instruction-following
manner while promising data and compute efficiency with zero-shot abilities,
generalizing to unseen slot labels. We address the slot-filling task by
creating an empirical upper bound for the task, identifying performance,
robustness, and generalization gaps, and proposing improvements to the training
data, architecture, and training strategies to narrow the gap with the upper
bound result. We show that each of these measures improve performance
substantially, while highlighting practical challenges and providing empirical
guidance and insights for harnessing these emerging models.

</details>


### [170] [PolySkill: Learning Generalizable Skills Through Polymorphic Abstraction](https://arxiv.org/abs/2510.15863)
*Simon Yu,Gang Li,Weiyan Shi,Peng Qi*

Main category: cs.CL

TL;DR: PolySkill是一个新框架，它使大型语言模型（LLM）驱动的智能体能够学习可泛化和可组合的技能。通过将技能的抽象目标与具体实现解耦，PolySkill显著提高了技能复用率和在已知及未知网站上的任务成功率，同时减少了操作步骤。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM驱动智能体在与外部环境交互时学习技能，但这些技能往往过度专业化，仅适用于特定网站，缺乏泛化能力。

Method: 引入了PolySkill框架，灵感来源于软件工程中的多态性。其核心思想是将技能的抽象目标（完成什么）与其具体实现（如何执行）解耦。

Result: 实验结果显示：1) 在已知网站上，技能复用率提高了1.7倍。2) 在Mind2Web上成功率提高了9.4%，在未知网站上提高了13.9%，同时操作步骤减少了20%以上。3) 在没有明确任务的自主探索环境中，PolySkill提高了任务建议的质量，并使智能体能够学习在不同网站上通用的可泛化技能。

Conclusion: 将技能的目标与其执行过程分离是开发能够在开放网络中持续学习和泛化的自主智能体的关键一步，PolySkill为构建此类智能体提供了一条实用的路径。

Abstract: Large language models (LLMs) are moving beyond static uses and are now
powering agents that learn continually during their interaction with external
environments. For example, agents can learn reusable skills while navigating
web pages or toggling new tools. However, existing methods for skill learning
often create skills that are over-specialized to a single website and fail to
generalize. We introduce PolySkill, a new framework that enables agents to
learn generalizable and compositional skills. The core idea, inspired by
polymorphism in software engineering, is to decouple a skill's abstract goal
(what it accomplishes) and its concrete implementation (how it is executed).
Experiments show that our method (1) improves skill reuse by 1.7x on seen
websites and (2) boosts success rates by up to 9.4% on Mind2Web and 13.9% on
unseen websites, while reducing steps by over 20%. (3) In self-exploration
settings without specified tasks, our framework improves the quality of
proposed tasks and enables agents to learn generalizable skills that work
across different sites. By enabling the agent to identify and refine its own
goals, the PolySkill enhances the agent's ability to learn a better curriculum,
leading to the acquisition of more generalizable skills compared to baseline
methods. This work provides a practical path toward building agents capable of
continual learning in adaptive environments. Our findings show that separating
a skill's goal from its execution is a crucial step toward developing
autonomous agents that can learn and generalize across the open web
continuously.

</details>


### [171] [Enhanced Sentiment Interpretation via a Lexicon-Fuzzy-Transformer Framework](https://arxiv.org/abs/2510.15843)
*Shayan Rokhva,Mousa Alizadeh,Maryam Abdollahi Shamami*

Main category: cs.CL

TL;DR: 该研究提出了一种混合词典-模糊-Transformer框架，结合规则、深度学习和模糊逻辑，生成连续的情感分数，以解决产品评论和社交媒体中非正式和领域特定语言带来的情感检测挑战。


<details>
  <summary>Details</summary>
Motivation: 由于产品评论和社交媒体帖子中存在非正式和领域特定的语言，准确检测情感极性和强度仍然具有挑战性。

Method: 该方法提出了一种混合词典-模糊-Transformer框架。它首先使用VADER进行初步情感估计，然后通过两阶段调整进行细化：利用DistilBERT的置信度分数，并应用模糊逻辑来减轻过度中性偏差并增强粒度。最后，一个自定义模糊推理系统将细化后的分数映射到0到1的连续区间。

Result: 该框架在四个领域特定数据集（外卖、电商、旅游、时尚）上进行了评估。结果显示，与用户评分的一致性更高，能更好地识别极端情感，并减少了错误分类。定量指标和定性分析均证实了模型的鲁棒性和效率。

Conclusion: 这项工作证明了将符号推理与神经网络模型相结合的价值，从而在语言动态领域实现可解释、细粒度的情感分析。

Abstract: Accurately detecting sentiment polarity and intensity in product reviews and
social media posts remains challenging due to informal and domain-specific
language. To address this, we propose a novel hybrid lexicon-fuzzy-transformer
framework that combines rule-based heuristics, contextual deep learning, and
fuzzy logic to generate continuous sentiment scores reflecting both polarity
and strength. The pipeline begins with VADER-based initial sentiment
estimations, which are refined through a two-stage adjustment process. This
involves leveraging confidence scores from DistilBERT, a lightweight
transformer and applying fuzzy logic principles to mitigate excessive
neutrality bias and enhance granularity. A custom fuzzy inference system then
maps the refined scores onto a 0 to 1 continuum, producing expert)like
judgments. The framework is rigorously evaluated on four domain-specific
datasets. food delivery, e-commerce, tourism, and fashion. Results show
improved alignment with user ratings, better identification of sentiment
extremes, and reduced misclassifications. Both quantitative metrics
(distributional alignment, confusion matrices) and qualitative insights (case
studies, runtime analysis) affirm the models robustness and efficiency. This
work demonstrates the value of integrating symbolic reasoning with neural
models for interpretable, finegrained sentiment analysis in linguistically
dynamic domains.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [172] [Autonomous Reactive Masonry Construction using Collaborative Heterogeneous Aerial Robots with Experimental Demonstration](https://arxiv.org/abs/2510.15114)
*Marios-Nektarios Stamatopoulos,Elias Small,Shridhar Velhal,Avijit Banerjee,George Nikolakopoulos*

Main category: cs.RO

TL;DR: 本文提出了一种使用异构无人机进行全自主空中砌体施工的框架，并进行了实验验证，其中一架无人机精确放置砖块，另一架涂抹粘合剂。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于填补全自主空中机器人施工领域的空白，特别是异构无人机协同进行砌体施工的实验验证。

Method: 该研究开发了两种专用无人机：(i) 砖块搬运无人机，配备球形关节机构和机载视觉系统（使用ArUco标记和最小二乘优化滤波器实时估计砖块姿态），用于精确操作和对齐；(ii) 粘合无人机，集成伺服控制阀和挤出喷嘴，用于精确施加粘合剂。框架采用反应式任务规划单元（结合施工布局的依赖图和冲突图）、分层状态机、动态任务分配和最小加加速度轨迹生成，以确保鲁棒操作、安全过渡、实时适应和精确运动。

Result: 实验结果首次展示了使用异构无人机进行全自主空中砌体施工，其中一架无人机精确放置砖块，另一架自主涂抹粘合材料。视频展示支持了所提出框架的有效性。

Conclusion: 该框架具有作为未来自主空中机器人施工发展基础的潜力，为该领域奠定了重要的实验基础。

Abstract: This article presents a fully autonomous aerial masonry construction
framework using heterogeneous unmanned aerial vehicles (UAVs), supported by
experimental validation. Two specialized UAVs were developed for the task: (i)
a brick-carrier UAV equipped with a ball-joint actuation mechanism for precise
brick manipulation, and (ii) an adhesion UAV integrating a servo-controlled
valve and extruder nozzle for accurate adhesion application. The proposed
framework employs a reactive mission planning unit that combines a dependency
graph of the construction layout with a conflict graph to manage simultaneous
task execution, while hierarchical state machines ensure robust operation and
safe transitions during task execution. Dynamic task allocation allows
real-time adaptation to environmental feedback, while minimum-jerk trajectory
generation ensures smooth and precise UAV motion during brick pickup and
placement. Additionally, the brick-carrier UAV employs an onboard vision system
that estimates brick poses in real time using ArUco markers and a least-squares
optimization filter, enabling accurate alignment during construction. To the
best of the authors' knowledge, this work represents the first experimental
demonstration of fully autonomous aerial masonry construction using
heterogeneous UAVs, where one UAV precisely places the bricks while another
autonomously applies adhesion material between them. The experimental results
supported by the video showcase the effectiveness of the proposed framework and
demonstrate its potential to serve as a foundation for future developments in
autonomous aerial robotic construction.

</details>


### [173] [RM-RL: Role-Model Reinforcement Learning for Precise Robot Manipulation](https://arxiv.org/abs/2510.15189)
*Xiangyu Chen,Chuhao Zhou,Yuxi Liu,Jianfei Yang*

Main category: cs.RO

TL;DR: 本文提出RM-RL框架，通过角色模型策略自动生成在线训练数据标签，将策略学习转化为监督学习，并结合混合训练方案，显著提升了机器人精密操作的精度和效率，无需人类示范。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如模仿学习和离线强化学习）在机器人精密操作中面临挑战，包括难以获取高质量专家示范、离线RL中的分布偏移问题以及数据效率低下，这些都可能导致任务失败。

Method: 引入角色模型强化学习（RM-RL）框架，它统一了在线和离线训练。核心思想是采用角色模型策略，利用近似最优动作自动为在线训练数据生成标签，从而无需人类示范。RM-RL将策略学习重新表述为监督训练，减少了分布不匹配的不稳定性并提高了效率。此外，混合训练方案进一步利用在线角色模型数据进行离线重用，通过重复采样提高数据效率。

Result: 实验结果表明，RM-RL比现有RL方法收敛更快、更稳定，在实际操作中取得了显著提升：平移精度提高53%，旋转精度提高20%。该框架成功执行了将细胞培养板精确放置到架子上的复杂任务，而先前方法未能成功。

Conclusion: RM-RL框架通过其创新的角色模型策略和混合训练方案，有效解决了机器人精密操作中的挑战，显著提高了精度、收敛速度和数据效率，为需要高精度操作的应用提供了强大的解决方案。

Abstract: Precise robot manipulation is critical for fine-grained applications such as
chemical and biological experiments, where even small errors (e.g., reagent
spillage) can invalidate an entire task. Existing approaches often rely on
pre-collected expert demonstrations and train policies via imitation learning
(IL) or offline reinforcement learning (RL). However, obtaining high-quality
demonstrations for precision tasks is difficult and time-consuming, while
offline RL commonly suffers from distribution shifts and low data efficiency.
We introduce a Role-Model Reinforcement Learning (RM-RL) framework that unifies
online and offline training in real-world environments. The key idea is a
role-model strategy that automatically generates labels for online training
data using approximately optimal actions, eliminating the need for human
demonstrations. RM-RL reformulates policy learning as supervised training,
reducing instability from distribution mismatch and improving efficiency. A
hybrid training scheme further leverages online role-model data for offline
reuse, enhancing data efficiency through repeated sampling. Extensive
experiments show that RM-RL converges faster and more stably than existing RL
methods, yielding significant gains in real-world manipulation: 53% improvement
in translation accuracy and 20% in rotation accuracy. Finally, we demonstrate
the successful execution of a challenging task, precisely placing a cell plate
onto a shelf, highlighting the framework's effectiveness where prior methods
fail.

</details>


### [174] [LVI-Q: Robust LiDAR-Visual-Inertial-Kinematic Odometry for Quadruped Robots Using Tightly-Coupled and Efficient Alternating Optimization](https://arxiv.org/abs/2510.15220)
*Kevin Christiansen Marsim,Minho Oh,Byeongho Yu,Seungjae Lee,I Made Aswin Nahrendra,Hyungtae Lim,Hyun Myung*

Main category: cs.RO

TL;DR: 本文提出了一种鲁棒的激光雷达-视觉-惯性-运动学里程计系统，通过优化和滤波相结合的多传感器融合策略，为足式机器人在复杂动态环境中提供精确的定位和建图，有效解决了现有方法中的估计漂移问题。


<details>
  <summary>Details</summary>
Motivation: 足式机器人在复杂动态环境中的自主导航需要鲁棒的同步定位与建图（SLAM）系统，以确保安全高效运行。然而，现有的基于传感器融合的SLAM方法由于融合策略不当，在挑战性环境中仍容易出现估计漂移。

Method: 该系统提出了一种鲁棒的激光雷达-视觉-惯性-运动学里程计（LiDAR-visual-inertial-kinematic odometry），整合了相机、激光雷达、惯性测量单元（IMU）和关节编码器等多传感器信息。它采用融合姿态估计方法，根据测量可用性运行基于优化的视觉-惯性-运动学里程计（VIKO）和基于滤波的激光雷达-惯性-运动学里程计（LIKO）。VIKO利用足部预积分技术和滑动窗口优化中基于超像素聚类的鲁棒激光雷达-视觉深度一致性。LIKO则结合了足部运动学，并在误差状态迭代卡尔曼滤波器（ESIKF）中采用点到平面残差。

Result: 与其它基于传感器融合的SLAM算法相比，该方法在公共和长期数据集中表现出鲁棒的性能。

Conclusion: 该研究提出的多传感器融合里程计系统，通过创新的优化和滤波结合策略，有效提升了足式机器人在复杂动态环境中的定位精度和鲁棒性，克服了传统方法中的估计漂移问题。

Abstract: Autonomous navigation for legged robots in complex and dynamic environments
relies on robust simultaneous localization and mapping (SLAM) systems to
accurately map surroundings and localize the robot, ensuring safe and efficient
operation. While prior sensor fusion-based SLAM approaches have integrated
various sensor modalities to improve their robustness, these algorithms are
still susceptible to estimation drift in challenging environments due to their
reliance on unsuitable fusion strategies. Therefore, we propose a robust
LiDAR-visual-inertial-kinematic odometry system that integrates information
from multiple sensors, such as a camera, LiDAR, inertial measurement unit
(IMU), and joint encoders, for visual and LiDAR-based odometry estimation. Our
system employs a fusion-based pose estimation approach that runs
optimization-based visual-inertial-kinematic odometry (VIKO) and filter-based
LiDAR-inertial-kinematic odometry (LIKO) based on measurement availability. In
VIKO, we utilize the footpreintegration technique and robust LiDAR-visual depth
consistency using superpixel clusters in a sliding window optimization. In
LIKO, we incorporate foot kinematics and employ a point-toplane residual in an
error-state iterative Kalman filter (ESIKF). Compared with other sensor
fusion-based SLAM algorithms, our approach shows robust performance across
public and longterm datasets.

</details>


### [175] [Lagrange-Poincaré-Kepler Equations of Disturbed Space-Manipulator Systems in Orbit](https://arxiv.org/abs/2510.15199)
*Borna Monazzah Moghaddam,Robin Chhabra*

Main category: cs.RO

TL;DR: 本文提出了拉格朗日-庞加莱-开普勒方程（LPKE），扩展了拉格朗日-庞加莱方程（LPE），用于在非惯性轨道参考系中建模航天器-机械臂系统的动力学，并考虑了姿态、轨道和机械臂运动学之间的耦合。


<details>
  <summary>Details</summary>
Motivation: 现有拉格朗日-庞加莱方程（LPE）在航天器-机械臂系统建模中，未能充分捕捉在非惯性轨道参考系下航天器姿态动力学、轨道运动和机械臂运动学之间的复杂耦合，以及轨道扰动的影响。

Method: 该研究提出了拉格朗日-庞加莱-开普勒方程（LPKE）框架。该方法结合了基航天器的欧拉-庞加莱方程、参考系的开普勒轨道动力学以及使用指数关节参数化的机械臂形状空间的简化欧拉-拉格朗日方程。通过利用主丛上的拉格朗日-达朗贝尔原理，推导出了能够明确捕捉轨道扰动及其与机械臂系统动态耦合效应的闭式结构矩阵。该框架还系统地包含了外部施加的、破坏对称性的扭矩。

Result: 研究推导出了能够明确捕捉轨道扰动及其与机械臂系统动态耦合效应的新型闭式结构矩阵。LPKE框架能够立即集成到硬件在环仿真和基于模型的控制架构中。通过对安装在航天器上的7自由度机械臂进行仿真研究，验证了所提出模型的有效性和数值优越性，特别是在分析轨道效应对机械臂系统的影响方面。

Conclusion: LPKE框架为在轨道环境中运行的航天器-机械臂系统提供了一个鲁棒且数值优越的动力学模型。它有效地捕捉了复杂的姿态、轨道和机械臂运动学耦合以及轨道扰动，并适用于自主机器人操作的硬件在环仿真和基于模型的控制架构。

Abstract: This article presents an extension of the Lagrange-Poincare Equations (LPE)
to model the dynamics of spacecraft-manipulator systems operating within a
non-inertial orbital reference frame. Building upon prior formulations of LPE
for vehicle-manipulator systems, the proposed framework, termed the
Lagrange-Poincare-Kepler Equations (LPKE), incorporates the coupling between
spacecraft attitude dynamics, orbital motion, and manipulator kinematics. The
formalism combines the Euler-Poincare equations for the base spacecraft,
Keplerian orbital dynamics for the reference frame, and reduced Euler-Lagrange
equations for the manipulator's shape space, using an exponential joint
parametrization. Leveraging the Lagrange-d'Alembert principle on principal
bundles, we derive novel closed-form structural matrices that explicitly
capture the effects of orbital disturbances and their dynamic coupling with the
manipulator system. The LPKE framework also systematically includes externally
applied, symmetry-breaking wrenches, allowing for immediate integration into
hardware-in-the-loop simulations and model-based control architectures for
autonomous robotic operations in the orbital environment. To illustrate the
effectiveness of the proposed model and its numerical superiority, we present a
simulation study analyzing orbital effects on a 7-degree-of-freedom manipulator
mounted on a spacecraft.

</details>


### [176] [PolyFly: Polytopic Optimal Planning for Collision-Free Cable-Suspended Aerial Payload Transportation](https://arxiv.org/abs/2510.15226)
*Mrunal Sarvaiya,Guanrui Li,Giuseppe Loianno*

Main category: cs.RO

TL;DR: PolyFly是一种用于悬索式空中运输机器人（如四旋翼无人机携带有效载荷）的全局最优规划器，它通过将机器人和环境建模为独立的多面体（包括姿态感知），并在紧凑环境中实现更快的飞行轨迹。


<details>
  <summary>Details</summary>
Motivation: 现有的空中运输机器人规划方法通过几何过近似处理车辆和障碍物，导致保守的机动和更长的飞行时间。在灾难响应和救援等应用中，机器人需要在有限空间内（如密林、不安全建筑）快速飞行并避开障碍物，因此需要更精确、更高效的规划方法。

Method: 本文提出了PolyFly，一种最优全局规划器。它将环境和机器人（四旋翼、缆绳和有效载荷）的每个物理组件建模为独立的多面体，从而实现非保守的表示。为提高模型精度，PolyFly还通过构建姿态感知多面体来纳入物理组件的姿态。通过对偶理论将多面体约束转换为平滑可微约束，从而高效地解决了由此产生的最优控制问题。

Result: PolyFly在八个迷宫状环境中与现有最先进的方法进行了比较，结果表明PolyFly在每种场景中都生成了更快的轨迹。此外，该方法还在一个带有悬挂有效载荷的真实四旋翼无人机上进行了实验验证，证明了其在实际应用中的可靠性和准确性。

Conclusion: PolyFly提供了一种新颖、高效的全局最优规划方法，通过精确的多面体建模（包括姿态感知）和对偶理论的应用，解决了悬索式空中运输机器人在复杂受限环境中进行积极飞行的挑战，显著提高了轨迹速度和规划精度。

Abstract: Aerial transportation robots using suspended cables have emerged as versatile
platforms for disaster response and rescue operations. To maximize the
capabilities of these systems, robots need to aggressively fly through tightly
constrained environments, such as dense forests and structurally unsafe
buildings, while minimizing flight time and avoiding obstacles. Existing
methods geometrically over-approximate the vehicle and obstacles, leading to
conservative maneuvers and increased flight times. We eliminate these
restrictions by proposing PolyFly, an optimal global planner which considers a
non-conservative representation for aerial transportation by modeling each
physical component of the environment, and the robot (quadrotor, cable and
payload), as independent polytopes. We further increase the model accuracy by
incorporating the attitude of the physical components by constructing
orientation-aware polytopes. The resulting optimal control problem is
efficiently solved by converting the polytope constraints into smooth
differentiable constraints via duality theory. We compare our method against
the existing state-of-the-art approach in eight maze-like environments and show
that PolyFly produces faster trajectories in each scenario. We also
experimentally validate our proposed approach on a real quadrotor with a
suspended payload, demonstrating the practical reliability and accuracy of our
method.

</details>


### [177] [A Generalized Sylvester-Fermat-Torricelli problem with application in disaster relief operations by UAVs](https://arxiv.org/abs/2510.15229)
*Sina Kazemdehbashi,Yanchao Liu,Boris S. Mordukhovich*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的数学框架，通过泛化Sylvester问题为Sylvester-Fermat-Torricelli (SFT) 问题，优化了移动无人机站的位置，以应对灾后通信中断问题，并明确考虑了风力影响和无人机异构性，从而显著提高了灾后响应任务的效率。


<details>
  <summary>Details</summary>
Motivation: 自然灾害常导致通信基础设施受损，使得灾区信息收集困难，影响救援效率。无人机（UAV）可用于信息收集和建立临时通信网络，但在实际部署中面临风等恶劣天气条件带来的挑战。因此，需要开发有效方法来优化无人机部署，克服这些实际障碍。

Method: 研究开发了一个新的数学框架，将Sylvester问题泛化为Sylvester-Fermat-Torricelli (SFT) 问题。该框架在一个统一的模型中捕获了风力影响、无人机异构性以及往返运动等复杂因素，旨在确定移动无人机站的最佳位置。

Result: 实验结果表明，所提出的框架可以将浪费的运行时间减少高达84%，从而显著提高了灾后任务的效率和有效性。

Conclusion: 该研究提出的数学框架通过考虑风力和无人机异构性等实际因素，增强了基于无人机的灾害响应规划的实用性，使得灾后任务更加高效和有效。

Abstract: Natural and human-made disasters can cause severe devastation and claim
thousands of lives worldwide. Therefore, developing efficient methods for
disaster response and management is a critical task for relief teams. One of
the most essential components of effective response is the rapid collection of
information about affected areas, damages, and victims. More data translates
into better coordination, faster rescue operations, and ultimately, more lives
saved. However, in some disasters, such as earthquakes, the communication
infrastructure is often partially or completely destroyed, making it extremely
difficult for victims to send distress signals and for rescue teams to locate
and assist them in time. Unmanned Aerial Vehicles (UAVs) have emerged as
valuable tools in such scenarios. In particular, a fleet of UAVs can be
dispatched from a mobile station to the affected area to facilitate data
collection and establish temporary communication networks. Nevertheless,
real-world deployment of UAVs faces several challenges, with adverse weather
conditions--especially wind--being among the most significant. To address this,
we develop a novel mathematical framework to determine the optimal location of
a mobile UAV station while explicitly accounting for the heterogeneity of the
UAVs and the effect of wind. In particular, we generalize the Sylvester problem
to introduce the Sylvester-Fermat-Torricelli (SFT) problem, which captures
complex factors such as wind influence, UAV heterogeneity, and back-and-forth
motion within a unified framework. The proposed framework enhances the
practicality of UAV-based disaster response planning by accounting for
real-world factors such as wind and UAV heterogeneity. Experimental results
demonstrate that it can reduce wasted operational time by up to 84%, making
post-disaster missions significantly more efficient and effective.

</details>


### [178] [Traversability-aware Consistent Situational Graphs for Indoor Localization and Mapping](https://arxiv.org/abs/2510.15319)
*Jeewon Kim,Minho Oh,Hyun Myung*

Main category: cs.RO

TL;DR: 针对机器人3D地图中的场景图房间分割问题，本文提出了一种可通行性感知的房间分割方法，以解决现有方法过分割或欠分割的弊端，从而提升定位与建图的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 机器人领域中，场景图通过理解空间元素关系来增强3D建图能力，尤其是分层场景图。然而，在分割空间特征时，由于视角变化和传感器有限视野，现有方法（如基于时间或体素的方法）在房间识别上存在挑战，常导致大房间过分割或复杂非封闭空间欠分割，进而引入姿态图优化中的错误约束，影响定位和建图精度。

Method: 本文提出了一种可通行性感知的房间分割方法。该方法考虑了机器人与环境之间的交互，并确保了可通行性信息的一致性可行性。这旨在提高姿态图优化的语义一致性和计算效率。

Result: 通过在包含重复遍历相同空间和路径的数据集中，比较相同房间的重检测频率以及优化时间消耗，结果表明所提出的方法显著提升了性能。

Conclusion: 所提出的可通行性感知房间分割方法有效解决了现有场景图房间分割的挑战，提高了姿态图优化的语义一致性和计算效率，从而增强了机器人的定位和建图能力。

Abstract: Scene graphs enhance 3D mapping capabilities in robotics by understanding the
relationships between different spatial elements, such as rooms and objects.
Recent research extends scene graphs to hierarchical layers, adding and
leveraging constraints across these levels. This approach is tightly integrated
with pose-graph optimization, improving both localization and mapping accuracy
simultaneously. However, when segmenting spatial characteristics, consistently
recognizing rooms becomes challenging due to variations in viewpoints and
limited field of view (FOV) of sensors. For example, existing real-time
approaches often over-segment large rooms into smaller, non-functional spaces
that are not useful for localization and mapping due to the time-dependent
method. Conversely, their voxel-based room segmentation method often
under-segment in complex cases like not fully enclosed 3D space that are
non-traversable for ground robots or humans, leading to false constraints in
pose-graph optimization. We propose a traversability-aware room segmentation
method that considers the interaction between robots and surroundings, with
consistent feasibility of traversability information. This enhances both the
semantic coherence and computational efficiency of pose-graph optimization.
Improved performance is demonstrated through the re-detection frequency of the
same rooms in a dataset involving repeated traversals of the same space along
the same path, as well as the optimization time consumption.

</details>


### [179] [ASBI: Leveraging Informative Real-World Data for Active Black-Box Simulator Tuning](https://arxiv.org/abs/2510.15331)
*Gahee Kim,Takamitsu Matsubara*

Main category: cs.RO

TL;DR: 本文提出了主动仿真推断（ASBI）框架，通过机器人主动收集信息量大的真实世界在线数据，实现对黑盒仿真器参数的准确估计和调优。


<details>
  <summary>Details</summary>
Motivation: 机器人领域广泛使用的黑盒仿真器，由于似然函数不可访问，其参数优化极具挑战性。传统的基于仿真的推断（SBI）在黑盒场景中，由于参数与观测之间的关系未知，难以准备包含足够信息的观测数据。

Method: ASBI框架利用机器人主动收集真实世界的在线数据，并通过最大化信息增益（定义为后验与先验之间香农熵的预期减少量）来优化机器人动作，从而收集信息量大的观测数据。为解决黑盒仿真器中似然函数不可访问导致信息增益难以计算的问题，该方法利用神经后验估计（NPE）通过神经网络学习后验估计器。

Result: 通过三项仿真实验定量验证了该方法的有效性，实现了准确的参数估计，后验分布集中在真实参数周围。此外，该研究还展示了一个实际应用，使用真实机器人估计了立方体颗粒（珠子和碎石）的仿真参数，并采用铲斗倾倒动作进行验证。

Conclusion: ASBI提供了一个有效且实用的框架，通过主动收集信息量大的数据，能够准确地调整黑盒仿真器的参数，解决了黑盒仿真器参数估计的难题。

Abstract: Black-box simulators are widely used in robotics, but optimizing their
parameters remains challenging due to inaccessible likelihoods.
Simulation-Based Inference (SBI) tackles this issue using simulation-driven
approaches, estimating the posterior from offline real observations and forward
simulations. However, in black-box scenarios, preparing observations that
contain sufficient information for parameter estimation is difficult due to the
unknown relationship between parameters and observations. In this work, we
present Active Simulation-Based Inference (ASBI), a parameter estimation
framework that uses robots to actively collect real-world online data to
achieve accurate black-box simulator tuning. Our framework optimizes robot
actions to collect informative observations by maximizing information gain,
which is defined as the expected reduction in Shannon entropy between the
posterior and the prior. While calculating information gain requires the
likelihood, which is inaccessible in black-box simulators, our method solves
this problem by leveraging Neural Posterior Estimation (NPE), which leverages a
neural network to learn the posterior estimator. Three simulation experiments
quantitatively verify that our method achieves accurate parameter estimation,
with posteriors sharply concentrated around the true parameters. Moreover, we
show a practical application using a real robot to estimate the simulation
parameters of cubic particles corresponding to two real objects, beads and
gravel, with a bucket pouring action.

</details>


### [180] [Nauplius Optimisation for Autonomous Hydrodynamics](https://arxiv.org/abs/2510.15350)
*Shyalan Ramesh,Scott Mann,Alex Stumpf*

Main category: cs.RO

TL;DR: 本文提出了一种名为NOAH的新型自然启发式群优化算法，用于解决自主水下航行器在强水流、有限带宽和持续感知需求下的群集操作挑战。


<details>
  <summary>Details</summary>
Motivation: 由于强水流、有限声学带宽和持续感知要求，传统群优化方法在自主水下航行器（AUV）操作中不可靠，因此需要一种更鲁棒的算法。

Method: 该研究提出了NOAH算法，灵感来源于藤壶无节幼体，结合了水流感知漂移、持续感知节点中的不可逆定居以及基于群落的通信。

Result: NOAH算法在永久锚定场景中实现了86%的成功率，并为水动力约束和不可逆定居行为提供了统一的公式，并通过水流下的实证研究进行了验证。

Conclusion: NOAH算法为可扩展和节能的水下群集机器人奠定了全面基础，解决了现有群算法的关键限制，并在水下探索任务中表现出有效性。

Abstract: Autonomous Underwater vehicles must operate in strong currents, limited
acoustic bandwidth, and persistent sensing requirements where conventional
swarm optimisation methods are unreliable. This paper presents NOAH, a novel
nature-inspired swarm optimisation algorithm that combines current-aware drift,
irreversible settlement in persistent sensing nodes, and colony-based
communication. Drawing inspiration from the behaviour of barnacle nauplii, NOAH
addresses the critical limitations of existing swarm algorithms by providing
hydrodynamic awareness, irreversible anchoring mechanisms, and colony-based
communication capabilities essential for underwater exploration missions. The
algorithm establishes a comprehensive foundation for scalable and
energy-efficient underwater swarm robotics with validated performance analysis.
Validation studies demonstrate an 86% success rate for permanent anchoring
scenarios, providing a unified formulation for hydrodynamic constraints and
irreversible settlement behaviours with an empirical study under flow.

</details>


### [181] [Adaptive Cost-Map-based Path Planning in Partially Unknown Environments with Movable Obstacles](https://arxiv.org/abs/2510.15336)
*Liviu-Mihai Stan,Ranulfo Bezerra,Shotaro Kojima,Tsige Tadesse Alemayoh,Satoshi Tadokoro,Masashi Konyo,Kazunori Ohno*

Main category: cs.RO

TL;DR: 本文提出了一种基于LiDAR和里程计的自适应路径规划框架，集成到ROS2 Nav2中，使机器人在非结构化环境中能够识别并推动可移动障碍物，从而提高导航成功率。


<details>
  <summary>Details</summary>
Motivation: 在灾难响应和其他非结构化室内环境中，机器人需要可靠导航，不仅要避开障碍物，还要识别哪些障碍物可以被推开，以提高任务成功率和效率。

Method: 该框架将新功能嵌入ROS2 Nav2栈。引入了一个“可移动障碍物层”，将静态地图中缺失的LiDAR返回标记为暂定可移动，并赋予较低的穿越成本。一个配套的“慢速姿态进度检查器”监测指令速度与实际速度之比；当机器人明显减速时，局部成本从轻度提高到重度，停滞时则提高到致命，促使全局规划器回退并重新规划路线。

Result: 在Gazebo中对Scout Mini进行的评估显示，与无层基线相比，该方法在孤立物体和杂乱走廊场景中实现了更高的目标到达率和更少的死锁，而穿越时间大致相当。由于该方法仅依赖平面扫描和CPU级计算，因此适用于资源受限的搜救机器人，并能以最少的工程量集成到异构平台中。

Conclusion: 交互感知成本地图是一种轻量级的ROS2原生扩展，适用于在非结构化环境中潜在可移动障碍物之间进行导航。该方法提高了机器人导航的可靠性和效率，并具有良好的通用性和资源效率。

Abstract: Reliable navigation in disaster-response and other unstructured indoor
settings requires robots not only to avoid obstacles but also to recognise when
those obstacles can be pushed aside. We present an adaptive, LiDAR and
odometry-based path-planning framework that embeds this capability into the
ROS2 Nav2 stack. A new Movable Obstacles Layer labels all LiDAR returns missing
from a prior static map as tentatively movable and assigns a reduced traversal
cost. A companion Slow-Pose Progress Checker monitors the ratio of commanded to
actual velocity; when the robot slows appreciably, the local cost is raised
from light to heavy, and on a stall to lethal, prompting the global planner to
back out and re-route. Gazebo evaluations on a Scout Mini, spanning isolated
objects and cluttered corridors, show higher goal-reach rates and fewer
deadlocks than a no-layer baseline, with traversal times broadly comparable.
Because the method relies only on planar scans and CPU-level computation, it
suits resource-constrained search and rescue robots and integrates into
heterogeneous platforms with minimal engineering. Overall, the results indicate
that interaction-aware cost maps are a lightweight, ROS2-native extension for
navigating among potentially movable obstacles in unstructured settings. The
full implementation will be released as open source
athttps://costmap-namo.github.io.

</details>


### [182] [GaussGym: An open-source real-to-sim framework for learning locomotion from pixels](https://arxiv.org/abs/2510.15352)
*Alejandro Escontrela,Justin Kerr,Arthur Allshire,Jonas Frey,Rocky Duan,Carmelo Sferrazza,Pieter Abbeel*

Main category: cs.RO

TL;DR: 本文提出一种新颖的方法，将3D高斯泼溅技术作为渲染器集成到IsaacGym等矢量化物理模拟器中，实现了前所未有的高速（超过10万步/秒）和高视觉保真度的机器人仿真，并展示了其在sim-to-real设置和多样化环境创建中的应用，以促进可扩展和通用化的机器人学习。


<details>
  <summary>Details</summary>
Motivation: 当前的机器人仿真在速度和视觉保真度之间存在取舍，限制了可扩展和通用化的机器人学习。研究旨在弥合高吞吐量仿真与高保真感知之间的鸿沟。

Method: 将3D高斯泼溅（3D Gaussian Splatting）技术作为即插即用的渲染器集成到IsaacGym等矢量化物理模拟器中。

Result: 该方法实现了前所未有的仿真速度（在消费级GPU上超过10万步/秒），同时保持了高视觉保真度。它在sim-to-real机器人设置中具有适用性，并且丰富的视觉语义能够改善导航和决策（例如，避开不良区域）。此外，该方法可以轻松整合来自iPhone扫描、大规模场景数据集（如GrandTour、ARKit）以及生成视频模型（如Veo）的数千个环境，从而快速创建逼真的训练世界。

Conclusion: 这项工作连接了高吞吐量仿真和高保真感知，推动了可扩展和通用化的机器人学习向前发展。

Abstract: We present a novel approach for photorealistic robot simulation that
integrates 3D Gaussian Splatting as a drop-in renderer within vectorized
physics simulators such as IsaacGym. This enables unprecedented speed --
exceeding 100,000 steps per second on consumer GPUs -- while maintaining high
visual fidelity, which we showcase across diverse tasks. We additionally
demonstrate its applicability in a sim-to-real robotics setting. Beyond
depth-based sensing, our results highlight how rich visual semantics improve
navigation and decision-making, such as avoiding undesirable regions. We
further showcase the ease of incorporating thousands of environments from
iPhone scans, large-scale scene datasets (e.g., GrandTour, ARKit), and outputs
from generative video models like Veo, enabling rapid creation of realistic
training worlds. This work bridges high-throughput simulation and high-fidelity
perception, advancing scalable and generalizable robot learning. All code and
data will be open-sourced for the community to build upon. Videos, code, and
data available at https://escontrela.me/gauss_gym/.

</details>


### [183] [Towards Automated Chicken Deboning via Learning-based Dynamically-Adaptive 6-DoF Multi-Material Cutting](https://arxiv.org/abs/2510.15376)
*Zhaodong Yang,Ai-Ping Hu,Harish Ravichandar*

Main category: cs.RO

TL;DR: 本文提出了一种基于强化学习的反应式力反馈切割策略，用于自动化鸡肩去骨，通过自定义模拟器和物理测试台训练，并在真实鸡肩上验证，显著提高了去骨成功率和避骨能力。


<details>
  <summary>Details</summary>
Motivation: 自动化鸡肩去骨需要精确的六自由度切割，穿过部分遮挡、可变形、多材料的关节，同时避免与骨骼接触，以防止严重的健康和安全风险。

Method: 1. 开发了一个开源的多材料切割模拟器，建模耦合、断裂和切割力，并支持强化学习。2. 设计了一个可重复使用的物理测试台，通过将刚性“骨骼”球体嵌入软块中来模拟鸡肩。3. 训练并部署了一个残差强化学习策略，采用离散力观测和域随机化，以实现鲁棒的零样本模拟到真实迁移，并支持六自由度刀具控制。

Result: 该学习策略实现了鲁棒的零样本模拟到真实迁移，并首次展示了在真实鸡肩上去骨的学习策略。实验结果表明，该策略能可靠地导航关节间隙，减少不期望的骨/软骨接触，在成功率和避骨方面比现有开环切割基线提高了多达4倍。

Conclusion: 研究结果证明了力反馈对于安全有效的多材料切割的必要性，所开发的学习策略显著提升了自动化鸡肩去骨的性能和安全性。

Abstract: Automating chicken shoulder deboning requires precise 6-DoF cutting through a
partially occluded, deformable, multi-material joint, since contact with the
bones presents serious health and safety risks. Our work makes both
systems-level and algorithmic contributions to train and deploy a reactive
force-feedback cutting policy that dynamically adapts a nominal trajectory and
enables full 6-DoF knife control to traverse the narrow joint gap while
avoiding contact with the bones. First, we introduce an open-source
custom-built simulator for multi-material cutting that models coupling,
fracture, and cutting forces, and supports reinforcement learning, enabling
efficient training and rapid prototyping. Second, we design a reusable physical
testbed to emulate the chicken shoulder: two rigid "bone" spheres with
controllable pose embedded in a softer block, enabling rigorous and repeatable
evaluation while preserving essential multi-material characteristics of the
target problem. Third, we train and deploy a residual RL policy, with
discretized force observations and domain randomization, enabling robust
zero-shot sim-to-real transfer and the first demonstration of a learned policy
that debones a real chicken shoulder. Our experiments in our simulator, on our
physical testbed, and on real chicken shoulders show that our learned policy
reliably navigates the joint gap and reduces undesired bone/cartilage contact,
resulting in up to a 4x improvement over existing open-loop cutting baselines
in terms of success rate and bone avoidance. Our results also illustrate the
necessity of force feedback for safe and effective multi-material cutting. The
project website is at https://sites.google.com/view/chickendeboning-2026.

</details>


### [184] [VDRive: Leveraging Reinforced VLA and Diffusion Policy for End-to-end Autonomous Driving](https://arxiv.org/abs/2510.15446)
*Ziang Guo,Zufeng Zhang*

Main category: cs.RO

TL;DR: VDRive是一个新颖的端到端自动驾驶管线，通过利用视觉语言动作模型（VLA）和生成扩散策略，显式建模状态-动作映射，以实现可解释且鲁棒的决策，并在多个基准测试中达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 动态环境和极端情况对自动驾驶车辆的状态理解和决策制定带来了巨大的鲁棒性挑战。

Method: 本文提出了VDRive，一个端到端自动驾驶管线，它显式地建模状态-动作映射。该方法利用视觉语言动作模型（VLA）的先进状态理解能力，并结合基于生成扩散策略的动作头部。VDRive通过上下文和几何两个方面进行驾驶引导：在上下文方面，VLA通过标记生成预训练预测未来观测，其中观测由条件向量量化变分自编码器（CVQ-VAE）表示为离散代码；在几何方面，VLA通过强化学习微调来预测基于当前驾驶条件的未来轨迹和动作。VLA为动作策略头部提供当前和预测的状态标记，以生成分层动作和轨迹。策略训练采用演员-评论家框架，通过学习的评论家评估动作并提供基于梯度的反馈。

Result: 实验结果表明，VDRive在Bench2Drive闭环基准测试和nuScenes开环规划中均取得了最先进的性能。

Conclusion: VDRive为自动驾驶提供了一种可解释且鲁棒的决策制定方法，通过结合VLA和生成扩散策略，有效应对了动态环境和极端情况的挑战，并在关键基准测试中展现出卓越的性能。

Abstract: In autonomous driving, dynamic environment and corner cases pose significant
challenges to the robustness of ego vehicle's state understanding and decision
making. We introduce VDRive, a novel pipeline for end-to-end autonomous driving
that explicitly models state-action mapping to address these challenges,
enabling interpretable and robust decision making. By leveraging the
advancement of the state understanding of the Vision Language Action Model
(VLA) with generative diffusion policy-based action head, our VDRive guides the
driving contextually and geometrically. Contextually, VLA predicts future
observations through token generation pre-training, where the observations are
represented as discrete codes by a Conditional Vector Quantized Variational
Autoencoder (CVQ-VAE). Geometrically, we perform reinforcement learning
fine-tuning of the VLA to predict future trajectories and actions based on
current driving conditions. VLA supplies the current state tokens and predicted
state tokens for the action policy head to generate hierarchical actions and
trajectories. During policy training, a learned critic evaluates the actions
generated by the policy and provides gradient-based feedback, forming an
actor-critic framework that enables a reinforcement-based policy learning
pipeline. Experiments show that our VDRive achieves state-of-the-art
performance in the Bench2Drive closed-loop benchmark and nuScenes open-loop
planning.

</details>


### [185] [Perfect Prediction or Plenty of Proposals? What Matters Most in Planning for Autonomous Driving](https://arxiv.org/abs/2510.15505)
*Aron Distelzweig,Faris Janjoš,Oliver Scheel,Sirish Reddy Varra,Raghu Rajan,Joschka Boedecker*

Main category: cs.RO

TL;DR: 本研究发现，在集成预测与规划（IPP）中，即使是完美的未来预测也未能显著提升规划性能，表明现有方法未能充分利用预测信息。相反，高质量的方案生成是关键。本文提出了一种以方案为中心的方法，显著优于现有技术，尤其在复杂交互场景中。


<details>
  <summary>Details</summary>
Motivation: 传统上，自动驾驶中的预测和规划是独立的模块。虽然集成预测与规划（IPP）旨在实现更明智的决策，但其对规划性能的实际提升程度尚不明确。此外，现有IPP方法似乎未能充分利用预测信息，且许多模仿学习规划器在生成现实且合理的方案方面表现不佳。

Method: 研究通过Val14和interPlan基准测试，调查了IPP方法中预测的作用。分析发现，即使有完美的未来预测，规划结果也未改善。因此，研究将重点转移到高质量的方案生成上，主要将预测用于碰撞检测。在此基础上，作者增强了PDM（一种简单的车道跟随方法）的方案生成能力，强调生成多样化、现实且高质量的方案。

Result: 分析显示，即使是完美的未来预测也未能带来更好的规划结果，表明当前IPP方法未能充分利用未来行为信息。许多基于模仿学习的规划器在生成现实和合理方案方面表现不佳，甚至不如简单的PDM方法。本文提出的以方案为中心的方法，通过增强PDM的方案生成能力，显著优于现有方法，特别是在分布外和高度交互的场景中，取得了最先进的结果。

Conclusion: 本研究得出结论，在集成预测与规划（IPP）中，预测的作用需要重新评估，完美的预测并不必然带来更好的规划性能。相反，高质量、现实且多样的方案生成是提升规划性能的关键。本文提出的以方案为中心的方法，通过优化方案生成，在复杂和交互式驾驶场景中表现出色，设定了新的技术标准。

Abstract: Traditionally, prediction and planning in autonomous driving (AD) have been
treated as separate, sequential modules. Recently, there has been a growing
shift towards tighter integration of these components, known as Integrated
Prediction and Planning (IPP), with the aim of enabling more informed and
adaptive decision-making. However, it remains unclear to what extent this
integration actually improves planning performance. In this work, we
investigate the role of prediction in IPP approaches, drawing on the widely
adopted Val14 benchmark, which encompasses more common driving scenarios with
relatively low interaction complexity, and the interPlan benchmark, which
includes highly interactive and out-of-distribution driving situations. Our
analysis reveals that even access to perfect future predictions does not lead
to better planning outcomes, indicating that current IPP methods often fail to
fully exploit future behavior information. Instead, we focus on high-quality
proposal generation, while using predictions primarily for collision checks. We
find that many imitation learning-based planners struggle to generate realistic
and plausible proposals, performing worse than PDM - a simple lane-following
approach. Motivated by this observation, we build on PDM with an enhanced
proposal generation method, shifting the emphasis towards producing diverse but
realistic and high-quality proposals. This proposal-centric approach
significantly outperforms existing methods, especially in out-of-distribution
and highly interactive settings, where it sets new state-of-the-art results.

</details>


### [186] [VO-DP: Semantic-Geometric Adaptive Diffusion Policy for Vision-Only Robotic Manipulation](https://arxiv.org/abs/2510.15530)
*Zehao Ni,Yonghao He,Lingfeng Qian,Jilei Mao,Fa Fu,Wei Sui,Hu Su,Junran Peng,Zhipeng Wang,Bin He*

Main category: cs.RO

TL;DR: 本文提出了一种名为VO-DP的纯视觉单视角扩散策略学习方法，通过利用预训练视觉基础模型融合语义和几何特征，在机器人操作任务中显著优于纯视觉基线，并在真实世界任务中超越了基于点云的方法，同时表现出强大的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的机器人操作视觉运动扩散策略大多依赖点云作为观察输入，缺乏对具有巨大潜力的纯视觉解决方案的深入探索。

Method: 提出VO-DP（Vision-Only and single-view Diffusion Policy），利用预训练视觉基础模型融合语义和几何特征。具体来说，它使用VGGT的中间特征，结合DINOv2的语义特征和交替注意力块的几何特征。这些特征通过交叉注意力融合，并通过CNN进行空间压缩，形成策略头的输入。

Result: VO-DP显著优于纯视觉基线DP。在仿真任务中，VO-DP的平均成功率为64.6%，与基于点云的DP3（64.0%）相当，远高于DP（34.8%）。在真实世界任务中，VO-DP的成功率达到87.9%，显著优于DP3（67.5%）和DP（11.2%）。此外，VO-DP在不同颜色、尺寸、背景和光照条件下都保持高度稳定。作者还开源了一个支持多机多GPU并行训练和混合精度训练的机器人操作训练库。

Conclusion: VO-DP是一种有效的纯视觉机器人操作解决方案，在仿真任务中与基于点云的方法性能相当，并在真实世界任务中表现更优，同时具有出色的鲁棒性。开源的训练库将进一步促进该领域的研究与发展。

Abstract: In the context of imitation learning, visuomotor-based diffusion policy
learning is one of the main directions in robotic manipulation. Most of these
approaches rely on point clouds as observation inputs and construct scene
representations through point clouds feature learning, which enables them to
achieve remarkable accuracy. However, the existing literature lacks an in-depth
exploration of vision-only solutions that have significant potential. In this
paper, we propose a Vision-Only and single-view Diffusion Policy learning
method (VO-DP) that leverages pretrained visual foundation models to achieve
effective fusion of semantic and geometric features. We utilize intermediate
features from VGGT incorporating semantic features from DINOv2 and geometric
features from Alternating Attention blocks. Features are fused via
cross-attention and spatially compressed with a CNN to form the input to the
policy head. Extensive experiments demonstrate that VO-DP not only outperforms
the vision-only baseline DP significantly but also exhibits distinct
performance trends against the point cloud-based method DP3: in simulation
tasks, VO-DP achieves an average success rate of 64.6% on par with DP3 64.0%
and far higher than DP 34.8%, while in real-world tasks, it reaches 87.9%,
outperforming both DP3 67.5% and DP 11.2% by a notable margin. Further
robustness evaluations confirm that VO-DP remains highly stable under varying
conditions including color, size, background, and lighting. Lastly, we
open-source a training library for robotic manipulation. Built on Accelerate,
this library supports multi-machine and multi-GPU parallel training, as well as
mixed precision training. It is compatible with visuomotor policies such as DP,
DP3 and VO-DP, and also supports the RoboTwin simulator.

</details>


### [187] [Improved Extended Kalman Filter-Based Disturbance Observers for Exoskeletons](https://arxiv.org/abs/2510.15533)
*Shilei Li,Dawei Shi,Makoto Iwasaki,Yan Ning,Hongpeng Zhou,Ling Shi*

Main category: cs.RO

TL;DR: 本文提出两种新型扰动观测器（基于IMM-EKF和MKC-EKF），以提高机械系统在未知扰动下的跟踪精度，并在外骨骼实验中验证了其优于传统EKF方法的性能。


<details>
  <summary>Details</summary>
Motivation: 机械系统性能常因未知扰动而下降，尽管两自由度控制能解耦名义性能与扰动抑制，但当扰动动态未知时，完美的扰动抑制难以实现。研究发现扰动估计在跟踪速度和跟踪不确定性之间存在固有的权衡。

Method: 为增强扰动估计，本文提出了两种新方法：1) 基于交互多模型扩展卡尔曼滤波器（IMM-EKF）的扰动观测器；2) 基于多核相关熵扩展卡尔曼滤波器（MKC-EKF）的扰动观测器。

Result: 在外骨骼实验中，面对时变交互力场景，与基于扩展卡尔曼滤波器（EKF）的扰动观测器相比，所提出的两种方法分别使髋关节误差改善了36.3%和16.2%，膝关节误差改善了46.3%和24.4%，显示了其优越性。

Conclusion: 所提出的两种基于IMM-EKF和MKC-EKF的扰动观测器显著提高了机械系统的跟踪精度，有效增强了对未知扰动的估计能力。

Abstract: The nominal performance of mechanical systems is often degraded by unknown
disturbances. A two-degree-of-freedom control structure can decouple nominal
performance from disturbance rejection. However, perfect disturbance rejection
is unattainable when the disturbance dynamic is unknown. In this work, we
reveal an inherent trade-off in disturbance estimation subject to tracking
speed and tracking uncertainty. Then, we propose two novel methods to enhance
disturbance estimation: an interacting multiple model extended Kalman
filter-based disturbance observer and a multi-kernel correntropy extended
Kalman filter-based disturbance observer. Experiments on an exoskeleton verify
that the proposed two methods improve the tracking accuracy $36.3\%$ and
$16.2\%$ in hip joint error, and $46.3\%$ and $24.4\%$ in knee joint error,
respectively, compared to the extended Kalman filter-based disturbance
observer, in a time-varying interaction force scenario, demonstrating the
superiority of the proposed method.

</details>


### [188] [Adaptive Legged Locomotion via Online Learning for Model Predictive Control](https://arxiv.org/abs/2510.15626)
*Hongyu Zhou,Xiaoyu Zhang,Vasileios Tzoumas*

Main category: cs.RO

TL;DR: 本文提出了一种基于在线学习和模型预测控制（MPC）的自适应腿式运动算法，用于四足机器人应对未知不确定性（如建模误差、外部干扰）下的复杂任务。


<details>
  <summary>Details</summary>
Motivation: 研究动机是实现四足机器人在真实世界复杂任务中的自主性，即使面对未知不确定性（如未知载荷和不平坦地形），也能可靠地执行任务。

Method: 该算法包含两个交互模块：模型预测控制（MPC）和残余动力学（represent建模误差和外部干扰）的在线学习。它使用随机傅里叶特征在再生核希尔伯特空间中近似残余动力学，然后基于当前学习到的残余动力学模型进行MPC。模型通过自监督方式，利用四足机器人控制过程中收集的数据，通过最小二乘法进行在线更新。算法在理论上具有次线性动态遗憾（dynamic regret）保证。

Result: 算法在Gazebo和MuJoCo仿真中得到验证，四足机器人成功跟踪参考轨迹。Gazebo仿真包括平坦、20度倾斜坡道和0.25米高度变化的崎岖地形中，存在高达12g的恒定未知外部力。MuJoCo仿真包括平坦地形中，存在高达8公斤的随时间变化的未知载荷和随时间变化的地面摩擦系数。

Conclusion: 该算法通过结合MPC和在线学习残余动力学，使四足机器人能够在存在未知不确定性的复杂环境中实现自适应腿式运动，并具有理论上的次线性动态遗憾保证，通过广泛的仿真得到了验证。

Abstract: We provide an algorithm for adaptive legged locomotion via online learning
and model predictive control. The algorithm is composed of two interacting
modules: model predictive control (MPC) and online learning of residual
dynamics. The residual dynamics can represent modeling errors and external
disturbances. We are motivated by the future of autonomy where quadrupeds will
autonomously perform complex tasks despite real-world unknown uncertainty, such
as unknown payload and uneven terrains. The algorithm uses random Fourier
features to approximate the residual dynamics in reproducing kernel Hilbert
spaces. Then, it employs MPC based on the current learned model of the residual
dynamics. The model is updated online in a self-supervised manner using least
squares based on the data collected while controlling the quadruped. The
algorithm enjoys sublinear \textit{dynamic regret}, defined as the
suboptimality against an optimal clairvoyant controller that knows how the
residual dynamics. We validate our algorithm in Gazebo and MuJoCo simulations,
where the quadruped aims to track reference trajectories. The Gazebo
simulations include constant unknown external forces up to $12\boldsymbol{g}$,
where $\boldsymbol{g}$ is the gravity vector, in flat terrain, slope terrain
with $20\degree$ inclination, and rough terrain with $0.25m$ height variation.
The MuJoCo simulations include time-varying unknown disturbances with payload
up to $8~kg$ and time-varying ground friction coefficients in flat terrain.

</details>


### [189] [Educational SoftHand-A: Building an Anthropomorphic Hand with Soft Synergies using LEGO MINDSTORMS](https://arxiv.org/abs/2510.15638)
*Jared K. Lepora,Haoran Li,Efi Psomopoulou,Nathan F. Lepora*

Main category: cs.RO

TL;DR: 本文介绍了一种完全由乐高MINDSTORMS构建的仿人机器人手（Educational SoftHand-A），它采用肌腱驱动、高度欠驱动设计，具有双电机和软协同机制，可实现自适应抓取，旨在用于教育和启发儿童。


<details>
  <summary>Details</summary>
Motivation: 为了在教育环境中激发儿童对现代机器人前沿技术的兴趣和学习热情，研究者希望设计一款使用标准乐高积木构建、易于获取设备进行测试的机器人手，同时融入先进的机器人手设计理念。

Method: 该机器人手完全使用乐高MINDSTORMS构建，基于Pisa/IIT SoftHand及其相关设计，采用肌腱驱动和高度欠驱动机制。每个手指使用双电机驱动一对拮抗肌腱以实现精细控制，并通过离合齿轮实现的差动机构实现软协同以同步手指运动。

Result: 所设计的仿人手通过简单的驱动和控制机制，实现了反应灵敏的精细控制，能够自适应地抓取各种物体。由于其完全由乐高积木构成，易于搭建。

Conclusion: 这款结合了乐高积木和最先进机器人手设计理念的Educational SoftHand-A，具有巨大的潜力，可以有效教育和启发儿童了解现代机器人学的前沿知识。

Abstract: This paper introduces an anthropomorphic robot hand built entirely using LEGO
MINDSTORMS: the Educational SoftHand-A, a tendon-driven, highly-underactuated
robot hand based on the Pisa/IIT SoftHand and related hands. To be suitable for
an educational context, the design is constrained to use only standard LEGO
pieces with tests using common equipment available at home. The hand features
dual motors driving an agonist/antagonist opposing pair of tendons on each
finger, which are shown to result in reactive fine control. The finger motions
are synchonized through soft synergies, implemented with a differential
mechanism using clutch gears. Altogether, this design results in an
anthropomorphic hand that can adaptively grasp a broad range of objects using a
simple actuation and control mechanism. Since the hand can be constructed from
LEGO pieces and uses state-of-the-art design concepts for robotic hands, it has
the potential to educate and inspire children to learn about the frontiers of
modern robotics.

</details>


### [190] [Integration of a Variable Stiffness Link for Long-Reach Aerial Manipulation](https://arxiv.org/abs/2510.15639)
*Manuel J. Fernandez,Alejandro Suarez,Anibal Ollero,Matteo Fumagalli*

Main category: cs.RO

TL;DR: 本文提出了一种可变刚度连杆（VSL），用于长距离空中抓取，能在柔性绳索和刚性杆之间切换，以适应任务需求，提高空中操作的通用性和安全性。


<details>
  <summary>Details</summary>
Motivation: 传统的长距离空中抓取系统依赖刚性或缆绳连接，这限制了精度或会将干扰传递给飞行器。研究动机在于开发一种可适应的机械耦合，以解决这些局限性。

Method: 将可变刚度连杆（VSL）集成到一个配备LiCAS双臂机械手的四旋翼无人机上。VSL具有可调节的刚度机制，使其能像柔性绳索或刚性杆一样工作。通过远程操作实验，在外部干扰和包裹运输任务中对系统进行了评估。

Result: 实验结果表明，改变连杆刚度显著改变了无人机与有效载荷之间的动态相互作用。柔性配置能衰减外部冲击和气动扰动，而刚性配置则提高了操作阶段的定位精度。

Conclusion: VSL通过在柔顺性和精度之间提供可控的权衡，增强了空中抓取系统的多功能性和安全性。未来工作将侧重于自主刚度调节、多绳配置、协同空中操作和用户研究。

Abstract: This paper presents the integration of a Variable Stiffness Link (VSL) for
long-reach aerial manipulation, enabling adaptable mechanical coupling between
an aerial multirotor platform and a dual-arm manipulator. Conventional
long-reach manipulation systems rely on rigid or cable connections, which limit
precision or transmit disturbances to the aerial vehicle. The proposed VSL
introduces an adjustable stiffness mechanism that allows the link to behave
either as a flexible rope or as a rigid rod, depending on task requirements.
  The system is mounted on a quadrotor equipped with the LiCAS dual-arm
manipulator and evaluated through teleoperated experiments, involving external
disturbances and parcel transportation tasks. Results demonstrate that varying
the link stiffness significantly modifies the dynamic interaction between the
UAV and the payload. The flexible configuration attenuates external impacts and
aerodynamic perturbations, while the rigid configuration improves positional
accuracy during manipulation phases.
  These results confirm that VSL enhances versatility and safety, providing a
controllable trade-off between compliance and precision. Future work will focus
on autonomous stiffness regulation, multi-rope configurations, cooperative
aerial manipulation and user studies to further assess its impact on
teleoperated and semi-autonomous aerial tasks.

</details>


### [191] [Freehand 3D Ultrasound Imaging: Sim-in-the-Loop Probe Pose Optimization via Visual Servoing](https://arxiv.org/abs/2510.15668)
*Yameng Zhang,Dianye Huang,Max Q. -H. Meng,Nassir Navab,Zhongliang Jiang*

Main category: cs.RO

TL;DR: 本文提出了一种利用轻量级相机和模拟环境中视觉伺服的经济高效且多功能的解决方案，用于精确的徒手3D超声成像，解决了传统方法中探头姿态估计成本高昂或精度不足的问题。


<details>
  <summary>Details</summary>
Motivation: 传统的徒手3D超声成像依赖昂贵的跟踪系统进行探头姿态估计，而基于神经网络的方法则受图像噪声和误差累积影响，导致重建精度下降。因此，需要一种经济高效且能提供精确姿态估计的解决方案。

Method: 该方法利用轻量级相机从纹理平面工作空间捕获视觉反馈。为应对遮挡和光照问题，引入图像修复方法，通过匹配周围纹理模式重建遮挡区域。姿态估计采用“模拟循环”方法，在模拟环境中复制系统设置，迭代最小化模拟与真实观测之间的姿态误差。视觉伺服控制器通过优化图像对齐来改进相机视图对齐，从而提高平移估计精度。

Result: 在软血管模型、3D打印锥形模型和人臂上的验证显示了该方法的鲁棒性和准确性。与参考重建的豪斯多夫距离分别为0.359毫米、1.171毫米和0.858毫米。

Conclusion: 研究结果证实了该方法在实现可靠的徒手3D超声重建方面的潜力，具有良好的鲁棒性和准确性。

Abstract: Freehand 3D ultrasound (US) imaging using conventional 2D probes offers
flexibility and accessibility for diverse clinical applications but faces
challenges in accurate probe pose estimation. Traditional methods depend on
costly tracking systems, while neural network-based methods struggle with image
noise and error accumulation, compromising reconstruction precision. We propose
a cost-effective and versatile solution that leverages lightweight cameras and
visual servoing in simulated environments for precise 3D US imaging. These
cameras capture visual feedback from a textured planar workspace. To counter
occlusions and lighting issues, we introduce an image restoration method that
reconstructs occluded regions by matching surrounding texture patterns. For
pose estimation, we develop a simulation-in-the-loop approach, which replicates
the system setup in simulation and iteratively minimizes pose errors between
simulated and real-world observations. A visual servoing controller refines the
alignment of camera views, improving translational estimation by optimizing
image alignment. Validations on a soft vascular phantom, a 3D-printed conical
model, and a human arm demonstrate the robustness and accuracy of our approach,
with Hausdorff distances to the reference reconstructions of 0.359 mm, 1.171
mm, and 0.858 mm, respectively. These results confirm the method's potential
for reliable freehand 3D US reconstruction.

</details>


### [192] [HEADER: Hierarchical Robot Exploration via Attention-Based Deep Reinforcement Learning with Expert-Guided Reward](https://arxiv.org/abs/2510.15679)
*Yuhong Cao,Yizhuo Wang,Jingsong Liang,Shuhao Liao,Yifeng Zhang,Peizhuo Li,Guillaume Sartoretti*

Main category: cs.RO

TL;DR: 该论文提出了HEADER，一种基于注意力机制的强化学习方法，利用分层图实现大规模环境中机器人高效探索，并在模拟和真实场景中展现出卓越的效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有的学习型方法在自主机器人探索方面，面临环境规模和探索效率的限制。

Method: HEADER是一种基于注意力机制的强化学习方法，采用分层图来表示机器人信念/地图。它设计了一种新颖的基于社区的算法来构建和更新全局图，该算法具有完全增量、形状自适应和线性复杂度的特点。规划器利用注意力网络，在局部精细推理，在全球粗略利用信息，以实现考虑多尺度空间依赖的最佳下一步视点决策。此外，引入了一种无参数的特权奖励，以避免手工奖励塑造造成的训练目标偏差。

Result: 在具有挑战性的大规模模拟探索场景中，HEADER比大多数现有学习和非学习方法展现出更好的可扩展性，并在探索效率上比最先进的基线提高了高达20%。该方法还在硬件上部署，并在包括300m*230m校园环境在内的复杂、大规模真实场景中得到了验证。

Conclusion: HEADER通过其创新的分层图、注意力机制和奖励设计，有效解决了大规模机器人探索的挑战，显著提升了探索效率和可扩展性，并达到了接近最优的探索行为。

Abstract: This work pushes the boundaries of learning-based methods in autonomous robot
exploration in terms of environmental scale and exploration efficiency. We
present HEADER, an attention-based reinforcement learning approach with
hierarchical graphs for efficient exploration in large-scale environments.
HEADER follows existing conventional methods to construct hierarchical
representations for the robot belief/map, but further designs a novel
community-based algorithm to construct and update a global graph, which remains
fully incremental, shape-adaptive, and operates with linear complexity.
Building upon attention-based networks, our planner finely reasons about the
nearby belief within the local range while coarsely leveraging distant
information at the global scale, enabling next-best-viewpoint decisions that
consider multi-scale spatial dependencies. Beyond novel map representation, we
introduce a parameter-free privileged reward that significantly improves model
performance and produces near-optimal exploration behaviors, by avoiding
training objective bias caused by handcrafted reward shaping. In simulated
challenging, large-scale exploration scenarios, HEADER demonstrates better
scalability than most existing learning and non-learning methods, while
achieving a significant improvement in exploration efficiency (up to 20%) over
state-of-the-art baselines. We also deploy HEADER on hardware and validate it
in complex, large-scale real-life scenarios, including a 300m*230m campus
environment.

</details>


### [193] [Few-Shot Demonstration-Driven Task Coordination and Trajectory Execution for Multi-Robot Systems](https://arxiv.org/abs/2510.15686)
*Taehyeon Kim,Vishnunandan L. N. Venkatesh,Byung-Cheol Min*

Main category: cs.RO

TL;DR: 本文提出了一种名为DDACE的少样本学习框架，用于多机器人系统，该框架整合了空间和时间元素，并通过解耦时空方面显著减少了数据需求。


<details>
  <summary>Details</summary>
Motivation: 传统的从演示中学习的方法需要大量数据，因此本研究旨在为多机器人系统开发一种少样本学习框架，以减少数据需求，并提高在各种任务中的模块性和泛化能力。

Method: DDACE框架利用时间图网络学习与任务无关的时间序列，并使用高斯过程进行空间轨迹建模。通过解耦时间和空间方面，该方法仅需少量演示即可实现学习。

Result: 实验结果表明，DDACE方法在少样本学习条件下成功实现了任务执行，并在动态多样的环境中（如多序列执行、多动作动态、复杂轨迹生成和异构配置）有效泛化。

Conclusion: 这项工作强调了模块化架构在增强多机器人系统在实际应用中实用性和可扩展性方面的潜力。

Abstract: In this paper, we propose a novel few-shot learning framework for multi-robot
systems that integrate both spatial and temporal elements: Few-Shot
Demonstration-Driven Task Coordination and Trajectory Execution (DDACE). Our
approach leverages temporal graph networks for learning task-agnostic temporal
sequencing and Gaussian Processes for spatial trajectory modeling, ensuring
modularity and generalization across various tasks. By decoupling temporal and
spatial aspects, DDACE requires only a small number of demonstrations,
significantly reducing data requirements compared to traditional learning from
demonstration approaches. To validate our proposed framework, we conducted
extensive experiments in task environments designed to assess various aspects
of multi-robot coordination-such as multi-sequence execution, multi-action
dynamics, complex trajectory generation, and heterogeneous configurations. The
experimental results demonstrate that our approach successfully achieves task
execution under few-shot learning conditions and generalizes effectively across
dynamic and diverse settings. This work underscores the potential of modular
architectures in enhancing the practicality and scalability of multi-robot
systems in real-world applications. Additional materials are available at
https://sites.google.com/view/ddace.

</details>


### [194] [DexCanvas: Bridging Human Demonstrations and Robot Learning for Dexterous Manipulation](https://arxiv.org/abs/2510.15786)
*Xinyue Xu,Jieqiang Sun,Jing,Dai,Siyuan Chen,Lanjie Ma,Ke Sun,Bin Zhao,Jianbo Yuan,Yiwen Lu*

Main category: cs.RO

TL;DR: DexCanvas是一个大规模的混合真实-合成人类灵巧操作数据集，包含7000小时手-物体交互数据，基于70小时真实演示，覆盖21种操作类型，提供多视图RGB-D、高精度mocap、MANO手参数和物理一致的接触力。


<details>
  <summary>Details</summary>
Motivation: 现有的操作数据集可能缺乏大规模真实演示、系统性的技能覆盖或物理验证的接触注释。本研究旨在创建一个结合这些特点的数据集，以推动机器人操作学习、接触式控制和技能迁移研究。

Method: 该研究首先收集了70小时的真实人类灵巧操作演示。然后，通过一个“真实到模拟”（real-to-sim）的管道，利用强化学习训练策略来控制物理模拟中的MANO手，以复现人类演示并发现产生观察到的物体运动的底层接触力，最终生成了7000小时的混合真实-合成数据。

Result: DexCanvas是首个结合了大规模真实演示、基于既定分类法的系统技能覆盖以及经过物理验证的接触注释的操作数据集。它提供了同步的多视图RGB-D、高精度mocap、MANO手参数以及每帧带有物理一致力配置的接触点。

Conclusion: DexCanvas数据集有望极大地促进机器人操作学习、富接触控制以及不同手部形态间的技能迁移等领域的研究。

Abstract: We present DexCanvas, a large-scale hybrid real-synthetic human manipulation
dataset containing 7,000 hours of dexterous hand-object interactions seeded
from 70 hours of real human demonstrations, organized across 21 fundamental
manipulation types based on the Cutkosky taxonomy. Each entry combines
synchronized multi-view RGB-D, high-precision mocap with MANO hand parameters,
and per-frame contact points with physically consistent force profiles. Our
real-to-sim pipeline uses reinforcement learning to train policies that control
an actuated MANO hand in physics simulation, reproducing human demonstrations
while discovering the underlying contact forces that generate the observed
object motion. DexCanvas is the first manipulation dataset to combine
large-scale real demonstrations, systematic skill coverage based on established
taxonomies, and physics-validated contact annotations. The dataset can
facilitate research in robotic manipulation learning, contact-rich control, and
skill transfer across different hand morphologies.

</details>


### [195] [Dynamic Recalibration in LiDAR SLAM: Integrating AI and Geometric Methods with Real-Time Feedback Using INAF Fusion](https://arxiv.org/abs/2510.15803)
*Zahra Arjmandi,Gunho Sohn*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的LiDAR SLAM融合技术，通过Inferred Attention Fusion (INAF)模块将AI与几何里程计结合，以提高定位和3D建图精度。


<details>
  <summary>Details</summary>
Motivation: 旨在改进LiDAR传感器的定位和3D建图能力，从而增强复杂场景下自主导航系统的性能。

Method: 核心方法是Inferred Attention Fusion (INAF)模块，该模块将AI与几何里程计相结合。它利用环境反馈动态调整注意力权重，以提高系统的适应性和测量精度。研究使用了KITTI数据集的LiDAR数据进行验证。

Result: 该方法显著提升了系统的适应性和测量精度，并提高了定位和3D建图的准确性。

Conclusion: 该融合技术展示了在复杂场景中增强自主导航系统的巨大潜力。

Abstract: This paper presents a novel fusion technique for LiDAR Simultaneous
Localization and Mapping (SLAM), aimed at improving localization and 3D mapping
using LiDAR sensor. Our approach centers on the Inferred Attention Fusion
(INAF) module, which integrates AI with geometric odometry. Utilizing the KITTI
dataset's LiDAR data, INAF dynamically adjusts attention weights based on
environmental feedback, enhancing the system's adaptability and measurement
accuracy. This method advances the precision of both localization and 3D
mapping, demonstrating the potential of our fusion technique to enhance
autonomous navigation systems in complex scenarios.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [196] [Q-EnergyDEX: A Zero-Trust Distributed Energy Trading Framework Driven by Quantum Key Distribution and Blockchain](https://arxiv.org/abs/2510.15045)
*Ziqing Zhu*

Main category: eess.SY

TL;DR: Q-EnergyDEX是一个零信任的分布式能源交易框架，通过集成量子密钥分发和区块链技术，为去中心化电力市场提供端到端的量子安全保护。


<details>
  <summary>Details</summary>
Motivation: 去中心化和数字化的本地电力市场引入了网络物理漏洞（如密钥泄露、数据篡改、身份欺骗），而现有的基于区块链的解决方案依赖于易受量子攻击的经典密码学原语。

Method: 该框架将物理层量子随机性与市场级操作相结合。它包括一个基于云的量子密钥管理服务（QKMS）用于生成可验证的熵和密钥管理；一个对称认证协议（Q-SAH）用于建立安全低延迟会话；一个量子辅助共识机制（PoR-Lite）实现概率性账本终结；以及一个Stackelberg约束的双边拍卖机制，将市场清算与熵可用性结合。

Result: 仿真结果表明，Q-EnergyDEX保持了强大的密钥稳定性和接近最优的社会福利，证明了其在大规模去中心化能源市场中的可行性。

Conclusion: Q-EnergyDEX通过量子密钥分发和区块链的结合，提供了一个安全、高效且可扩展的解决方案，以应对去中心化能源市场中的网络物理漏洞和量子攻击威胁。

Abstract: The rapid decentralization and digitalization of local electricity markets
have introduced new cyber-physical vulnerabilities, including key leakage, data
tampering, and identity spoofing. Existing blockchain-based solutions provide
transparency and traceability but still depend on classical cryptographic
primitives that are vulnerable to quantum attacks. To address these challenges,
this paper proposes Q-EnergyDEX, a zero-trust distributed energy trading
framework driven by quantum key distribution and blockchain. The framework
integrates physical-layer quantum randomness with market-level operations,
providing an end-to-end quantum-secured infrastructure. A cloud-based Quantum
Key Management Service continuously generates verifiable entropy and regulates
key generation through a rate-adaptive algorithm to sustain high-quality
randomness. A symmetric authentication protocol (Q-SAH) establishes secure and
low-latency sessions, while the quantum-aided consensus mechanism (PoR-Lite)
achieves probabilistic ledger finality within a few seconds. Furthermore, a
Stackelberg-constrained bilateral auction couples market clearing with entropy
availability, ensuring both economic efficiency and cryptographic security.
Simulation results show that Q-EnergyDEX maintains robust key stability and
near-optimal social welfare, demonstrating its feasibility for large-scale
decentralized energy markets.

</details>


### [197] [Sparsity-exploiting Gaussian Process for Robust Transient Learning of Power System Dynamics](https://arxiv.org/abs/2510.15150)
*Tina Gao,Shimiao Li,Lawrence Pileggi*

Main category: eess.SY

TL;DR: 本文提出了一种鲁棒的瞬态学习方法，通过结合稀疏优化和矩量法（MoM），使高斯过程（GP）在稀疏数据损坏下仍能准确学习电网动态行为，并利用K-medoid聚类技术显著提升了大规模系统的推理速度。


<details>
  <summary>Details</summary>
Motivation: 高斯过程（GP）在利用稀疏PMU测量数据学习和推断电网动态行为方面取得了进展，但实际测量数据容易受到各种随机和有针对性的威胁（如数据损坏），导致结果不准确或无意义。因此，需要开发一种鲁棒的学习方法来克服这一挑战。

Method: 该方法通过以下步骤实现：1. 利用数据流中稀疏的损坏模式，将稀疏优化与矩量法（MoM）相结合，使学习过程对稀疏分布的数据损坏具有鲁棒性。2. 优化稀疏权重以识别受损的电表位置。3. 为了提高大规模系统的推理速度，进一步采用K-medoid聚类对位置进行维度缩减（DR）和聚合表示（AR）启发式处理。

Result: 实验结果表明，该方法对随机大误差、有针对性的虚假数据注入和本地PMU时钟漂移具有鲁棒性。在一个1354总线系统上，使用维度缩减（DR）后推理速度快了18倍，当进一步结合聚合表示（AR）启发式方法时，速度快了400倍。

Conclusion: 本文提出的方法能够有效应对各种数据损坏，实现对电网动态行为的鲁棒瞬态学习，并且通过维度缩减和聚合表示技术，极大地提升了在大规模系统上的推理速度和效率。

Abstract: Advances in leveraging Gaussian processes (GP) have enabled learning and
inferring dynamic grid behavior from scarce PMU measurements. However, real
measurements can be corrupted by various random and targeted threats, leading
to inaccurate and meaningless results. This paper develops robust transient
learning to overcome this challenge by exploiting the sparse corruption
patterns in the data flow. Specifically, we integrate sparse optimization with
method of moments (MoM) to make learning robust to a sparse distribution of
data corruptions; then, we optimize sparse weights to identify corrupted meter
locations. To improve inference speed on large-scale systems, we further adopt
K-medoid clustering of locations to develop dimension reduction (DR) and
aggregate representation (AR) heuristics. Experimental results demonstrate
robustness against random large errors, targeted false data injections, and
local PMU clock drifts. On a 1354-bus system, inference turns out to be 18x
faster using DR and 400x faster when further combined with AR heuristics.

</details>


### [198] [Exploring a New Design Paradigm for Omnidirectional MAVs for Minimal Actuation and Internal Force Elimination: Theoretical Framework and Control](https://arxiv.org/abs/2510.15071)
*Ahmed Ali,Chiara Gabellieri,Antonio Franchi*

Main category: eess.SY

TL;DR: 本文提出了一种新型六输入全向多旋翼飞行器（MAV）概念，通过一个主动倾斜螺旋桨和三个摆式连杆（每个带一个螺旋桨）实现全向性，确保在平衡状态下无内力。研究开发了动态模型，设计了几何非线性控制器，并验证了其在解耦姿态和位移运动方面的能力和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有MAV在实现全向性时，可能面临输入过多（过度驱动）或在平衡状态下产生内力的问题。本文旨在设计一种仅使用六个输入且在平衡状态下无内力的全向MAV。

Method: 该MAV概念集成了一个主动倾斜螺旋桨和三个通过被动万向节连接到主体的摆式连杆（每个连杆携带一个螺旋桨）。首先，开发了多连杆MAV的详细动态模型。接着，分析了平衡配置，并证明了对于MAV主平台的每个姿态都存在强制平衡。为了使闭环系统渐近稳定，使用动态反馈线性化和反步技术构建了几何非线性控制器，其中主平台配置误差为SE(3)上的左平移误差。最后，通过零动态的Lyapunov论证研究了闭环系统的稳定性。

Result: 该设计确保了MAV在仅有六个输入的情况下实现全向性，并最大限度地减少了内部力。分析表明，对于MAV主平台的每个姿态都存在强制平衡。所构建的控制器使得闭环系统渐近稳定。数值模拟验证了所提出的方法，表明MAV能够在非零初始条件、参数不确定性和执行器噪声下执行解耦的姿态和位移运动。

Conclusion: 本文提出的新型六输入全向MAV概念及其控制策略是有效的，能够实现全向性、无内力平衡、解耦运动和闭环系统稳定性，并通过数值模拟得到了充分验证。

Abstract: This paper presents a novel concept for achieving omnidirectionality in a
multirotor aerial vehicle (MAV) that uses only 6 inputs and ensures no internal
forces at the equilibria. The concept integrates a single actively-tilting
propeller along with 3 pendulum-like links, each carrying a propeller,
connected by passive universal joints to the main body. We show that this
design ensures omnidirectionality while minimizing the internal forces and
without resorting to overactuation (i.e., more than 6 inputs). A detailed
dynamic model of the multi-link MAV is first developed. Afterwards, the
analysis identifies the equilibrium configurations and illustrates that a
forced equilibrium exists for every pose of the MAV's main platform. In order
to render this equilibrium asymptotically stable for the closed-loop system, a
geometric nonlinear controller is constructed using dynamic feedback
linearization and backstepping techniques with the main platform configuration
error being the left-trivialized error on SE(3). The stability of the
closed-loop system is then investigated by employing standard Lyapunov
arguments on the zero dynamics. We conclude by providing numerical simulations
validating the proposed approach. They demonstrate the MAV capability to
perform decoupled attitude and translational motions under non-zero initial
conditions, parametric uncertainty, and actuators noise.

</details>


### [199] [Tail-Optimized Caching for LLM Inference](https://arxiv.org/abs/2510.15152)
*Wenxin Zhang,Yueying Li,Ciamac C. Moallemi,Tianyi Peng*

Main category: eess.SY

TL;DR: 该研究提出了一种名为Tail-Optimized LRU的提示缓存策略，通过简单修改LRU来优化LLM推理的尾部延迟，并在理论和实践中均显示出显著性能提升。


<details>
  <summary>Details</summary>
Motivation: LLM推理中的提示缓存对降低延迟和成本至关重要，但现有广泛使用的LRU策略在优化尾部延迟方面表现不佳，因为它未考虑对话长度的异构性。

Method: 提出Tail-Optimized LRU，一个对LRU策略进行两行修改的版本。它通过重新分配KV缓存容量来优先处理高延迟对话，并驱逐不太可能影响未来轮次的缓存条目。该方法在一种自然随机对话动态模型下被证明是最优的。

Result: 在真实对话数据集WildChat上，Tail-Optimized LRU相比LRU，将P90尾部首次生成令牌时间（TTFT）延迟降低了27.5%，P95尾部延迟降低了23.9%，并将200ms的服务级别目标（SLO）违规减少了38.9%。

Conclusion: Tail-Optimized LRU为LLM部署中寻求优化尾部延迟的实践者提供了一个实用且有理论依据的解决方案。

Abstract: Prompt caching is critical for reducing latency and cost in LLM inference:
OpenAI and Anthropic report up to 50-90% cost savings through prompt reuse.
Despite its widespread success, little is known about what constitutes an
optimal prompt caching policy, particularly when optimizing tail latency, a
metric of central importance to practitioners. The widely used Least Recently
Used (LRU) policy can perform arbitrarily poor on this metric, as it is
oblivious to the heterogeneity of conversation lengths. To address this gap, we
propose Tail-Optimized LRU, a simple two-line modification that reallocates KV
cache capacity to prioritize high-latency conversations by evicting cache
entries that are unlikely to affect future turns. Though the implementation is
simple, we prove its optimality under a natural stochastic model of
conversation dynamics, providing the first theoretical justification for LRU in
this setting, a result that may be of independent interest to the caching
community. Experimentally, on real conversation data WildChat, Tail-Optimized
LRU achieves up to 27.5% reduction in P90 tail Time to First Token latency and
23.9% in P95 tail latency compared to LRU, along with up to 38.9% decrease in
SLO violations of 200ms. We believe this provides a practical and theoretically
grounded option for practitioners seeking to optimize tail latency in
real-world LLM deployments.

</details>


### [200] [A Comparative Study of Oscillatory Perturbations in Car-Following Models](https://arxiv.org/abs/2510.15190)
*Oumaima Barhoumi,Ghazal Farhani,Taufiq Rahman,Mohamed H. Zaki,Sofiène Tahar*

Main category: eess.SY

TL;DR: 本文通过比较多种跟驰模型（IDM、OVM、GMM、CACC）在不同前车速度扰动下的表现，研究了车队不稳定性及其对后车的影响，以期为设计鲁棒的车队控制策略提供见解。


<details>
  <summary>Details</summary>
Motivation: 车队能提高道路容量、减少油耗并改善交通流，但其效益高度依赖于车队稳定性。不稳定性会导致不安全的车距和更高的能耗。因此，研究车队不稳定性及其发生根源和对后车的影响至关重要。

Method: 本文提出了一项比较研究，对比了智能驾驶员模型 (IDM)、最优速度模型 (OVM)、通用汽车模型 (GMM) 和协作式自适应巡航控制 (CACC) 等不同的跟驰模型。通过改变前车的速度（引入正弦振荡和离散速度变化）来引入扰动，以观察后续车辆的行为。通过分析车辆轨迹和车间距的变化，评估每个模型对扰动传播的鲁棒性。

Result: 研究结果揭示了各模型的敏感性、稳定性特征，并对设计弹性车队控制策略具有重要启示。

Conclusion: 研究结论为理解车队不稳定性提供了深刻见解，并为未来设计更具鲁棒性的车队控制策略提供了指导。

Abstract: As connected and autonomous vehicles become more widespread, platooning has
emerged as a key strategy to improve road capacity, reduce fuel consumption,
and enhance traffic flow. However, the benefits of platoons strongly depend on
their ability to maintain stability. Instability can lead to unsafe spacing and
increased energy usage. In this work, we study platoon instability and analyze
the root cause of its occurrence, as well as its impacts on the following
vehicle. To achieve this, we propose a comparative study between different
car-following models such as the Intelligent Driver Model (IDM), the Optimal
Velocity Model (OVM), the General Motors Model (GMM), and the Cooperative
Adaptive Cruise Control (CACC). In our approach, we introduce a disruption in
the model by varying the velocity of the leading vehicle to visualize the
behavior of the following vehicles. To evaluate the dynamic response of each
model, we introduce controlled perturbations in the velocity of the leading
vehicle, specifically, sinusoidal oscillations and discrete velocity changes.
The resulting vehicle trajectories and variations in inter-vehicle spacing are
analyzed to assess the robustness of each model to disturbance propagation. The
findings offer insight into model sensitivity, stability characteristics, and
implications for designing resilient platooning control strategies.

</details>


### [201] [Quantum-Key-Distribution Authenticated Aggregation and Settlement for Virtual Power Plants](https://arxiv.org/abs/2510.15239)
*Ziqing Zhu*

Main category: eess.SY

TL;DR: 本文提出了一种用于虚拟电厂（VPP）的量子认证聚合与结算框架，通过优化稀缺量子密钥的分配，以应对网络攻击和极端天气下的安全挑战，从而显著降低残余风险和SLA违规。


<details>
  <summary>Details</summary>
Motivation: 分布式能源和需求侧灵活性使VPP在现代电网中至关重要，但其端到端业务流程（投标、调度、计量、结算、归档）构成了一个紧密耦合的物理-网络-经济系统，需要安全及时的通信。传统密码学在复杂的网络攻击和极端天气冲击下，长期保护能力有限。量子密钥分发（QKD）虽然能提供信息理论安全保障，但其有限的密钥生成速率、路由能力和系统开销使得密钥分配成为一个紧迫的挑战，亟需在异构进程中调度稀缺的量子密钥以最小化残余风险并满足延迟要求。

Method: 本文首先开发了一个系统威胁模型，将QKD密钥生成和路由与业务层安全策略、认证强度、刷新频率和延迟约束联系起来。在此基础上，提出了一个密钥预算风险最小化问题，该问题综合考虑了经济风险、服务水平违规和密钥预算可行性，并揭示了边际安全价值与影子价格之间的阈值特性。

Result: 在代表性VPP系统上的案例研究表明，所提出的方法显著降低了残余风险和服务水平协议（SLA）违规，提高了密钥效率和鲁棒性，并且观察到的动态与理论影子价格机制相符。

Conclusion: 该量子认证聚合与结算框架通过智能密钥分配，有效提升了VPP在复杂威胁环境下的安全性和运行效率，是保障关键基础设施长期安全的重要一步。

Abstract: The proliferation of distributed energy resources (DERs) and demand-side
flexibility has made virtual power plants (VPPs) central to modern grid
operation. Yet their end-to-end business pipeline, covering bidding, dispatch,
metering, settlement, and archival, forms a tightly coupled
cyber-physical-economic system where secure and timely communication is
critical. Under the combined stress of sophisticated cyberattacks and extreme
weather shocks, conventional cryptography offers limited long-term protection.
Quantum key distribution (QKD), with information-theoretic guarantees, is
viewed as a gold standard for securing critical infrastructures. However,
limited key generation rates, routing capacity, and system overhead render key
allocation a pressing challenge: scarce quantum keys must be scheduled across
heterogeneous processes to minimize residual risk while maintaining latency
guarantees. This paper introduces a quantum-authenticated aggregation and
settlement framework for VPPs. We first develop a system-threat model that
connects QKD key generation and routing with business-layer security
strategies, authentication strength, refresh frequency, and delay constraints.
Building on this, we formulate a key-budgeted risk minimization problem that
jointly accounts for economic risk, service-level violations, and key-budget
feasibility, and reveal a threshold property linking marginal security value to
shadow prices. Case studies on a representative VPP system demonstrate that the
proposed approach significantly reduces residual risk and SLA violations,
enhances key efficiency and robustness, and aligns observed dynamics with the
theoretical shadow price mechanism.

</details>


### [202] [Techno-Economic Feasibility Analysis of Quantum Key Distribution for Power-System Communications](https://arxiv.org/abs/2510.15248)
*Ziqing Zhu*

Main category: eess.SY

TL;DR: 本文提出了一个综合的技术经济框架，用于评估量子密钥分发（QKD）在电力系统安全通信中的可行性，考虑了量子计算威胁下的网络安全风险。


<details>
  <summary>Details</summary>
Motivation: 现代电力系统日益增长的数字化和去中心化，使其关键通信基础设施面临不断升级的网络风险，尤其是在新兴的量子计算威胁下，需要更安全的通信方案。

Method: 研究开发了一个随机系统模型，以共同捕捉时变密钥需求、光学损耗约束下的QKD供应、站侧缓冲以及后量子密码（PQC）回退机制。推导了服务水平保障的分析条件，并构建了安全平准成本（LCoSec）和增量安全成本（CIS）两个量化指标。通过离散事件仿真，在IEEE 118-bus、123-node和39-bus测试系统上比较了PQC-only、QKD-only和混合架构。

Result: 结果显示，QKD主导的混合架构显著降低了密钥中断概率和SLA不足，为实时和保密性关键服务实现了接近单位的可用性。经济分析表明，在适度的光学损耗和缓冲尺寸下，QKD增强型部署在都市和配电级网络中具有成本效益，并存在明确的盈亏平衡区。

Conclusion: 所提出的框架提供了一个可复现、风险感知的决策工具，为未来弹性电力系统基础设施中大规模、经济合理的QKD应用提供指导。

Abstract: The accelerating digitalization and decentralization of modern power systems
expose critical communication infrastructures to escalating cyber risks,
particularly under emerging quantum computing threats. This paper presents an
integrated techno-economic framework to evaluate the feasibility of Quantum Key
Distribution (QKD) for secure power-system communications. A stochastic system
model is developed to jointly capture time-varying key demand, QKD supply under
optical-loss constraints, station-side buffering, and post-quantum cryptography
(PQC) fallback mechanisms. Analytical conditions are derived for service-level
assurance, including buffer stability, outage probability, and availability
bounds. Building on this, two quantitative metrics, including the Levelized
Cost of Security (LCoSec) and Cost of Incremental Security (CIS), are
formulated to unify capital, operational, and risk-related expenditures within
a discounted net-present-value framework. Using IEEE 118-bus, 123-node, and
39-bus test systems, we conduct discrete-event simulations comparing PQC-only,
QKD-only, and Hybrid architectures across multiple topologies and service
profiles. Results show that Hybrid architectures dominated by QKD significantly
reduce key-outage probability and SLA shortfalls, achieving near-unit
availability for real-time and confidentiality-critical services. Economic
analyses reveal clear breakeven zones where QKD-enhanced deployments become
cost-effective, primarily in metropolitan and distribution-level networks under
moderate optical loss and buffer sizing. The proposed framework provides a
reproducible, risk-aware decision tool for guiding large-scale, economically
justified QKD adoption in future resilient power-system infrastructures.

</details>


### [203] [Comprehensive Dynamic Modeling and Constraint-Aware Air Supply Control for Localized Water Management in Automotive Polymer Electrolyte Membrane Fuel Cells](https://arxiv.org/abs/2510.15250)
*Mostafaali Ayubirad,Zeng Qiu,Hao Wang,Chris Weinkauf,Michiel Van Nieuwstadt,Hamid R. Ossareh*

Main category: eess.SY

TL;DR: 本文提出了一种基于指令调节器（CG）框架的预测性约束感知控制方案，用于质子交换膜（PEM）燃料电池系统局部水合管理，以防止膜干燥并优化功率。


<details>
  <summary>Details</summary>
Motivation: PEM燃料电池的膜水合管理对于其性能和寿命至关重要，特别是在反流配置下阳极入口处最干燥区域的局部水合。防止膜干燥是该研究的主要驱动力。

Method: 研究方法包括：1) 建立包含P2D电堆、反应物供应和冷却子系统的综合非线性动态模型；2) 设计输出反馈控制器以跟踪空气供应和冷却系统的最佳设定点进行功率优化；3) 使用闭环非线性模型分析膜水合行为；4) 推导降阶线性化模型以近似水合行为；5) 将该降阶模型整合到指令调节器（CG）框架中，以在必要时调整空气供应设定点，防止膜干燥。

Result: 通过实际驾驶循环仿真，结果表明所提出的方法在密切跟踪所需净功率的同时，能有效保持局部膜水合，成功防止膜干燥。

Conclusion: 基于指令调节器框架的预测性约束感知控制方案能够有效地管理PEM燃料电池的局部水合，防止膜干燥，并在各种运行条件下实现功率优化。

Abstract: In this paper, a predictive constraint-aware control scheme is formulated
within the Command Governor (CG) framework for localized hydration management
of a proton exchange membrane (PEM) fuel cell system. First, a comprehensive
nonlinear dynamic model of the fuel cell system is presented which includes a
pseudo 2-dimensional (P2D) model of the stack, reactant supply and cooling
subsystems. The model captures the couplings among the various subsystems and
serves as the basis for designing output feedback controllers to track the
optimal set-points of the air supply and cooling systems for power
optimization. The closed-loop nonlinear model is then used to analyze the
dynamic behavior of membrane hydration near the anode inlet, the driest region
of the membrane in a counter-flow configuration, under various operating
conditions. A reduced-order linearized model is then derived to approximate
hydration behavior with sufficient fidelity for constraint enforcement. This
model is used within the CG framework to adjust the air supply set-points when
necessary to prevent membrane dry-out. The effectiveness of the proposed
approach in maintaining local membrane hydration while closely tracking the
requested net power is demonstrated through realistic drive-cycle simulations.

</details>


### [204] [Modeling and Dynamic Simulation of a Hybrid Wind-Wave System on a Hexagonal Semi-Submersible Platform](https://arxiv.org/abs/2510.15285)
*Saeid Bayat,Jerry Zuo,Jing Sun*

Main category: eess.SY

TL;DR: 本研究提出了一种混合式浮动海上平台，将风力涡轮机与三个振荡式浪涌波浪能转换器集成到六边形半潜式结构中，以同时捕获风能和波浪能，并评估了其性能和可行性。


<details>
  <summary>Details</summary>
Motivation: 现有的海上可再生能源平台大多单独捕获风能或波浪能。本研究旨在开发一种集成的混合平台，以更有效地利用海上能源并提高系统稳定性。

Method: 研究设计了一种将风力涡轮机与三个振荡式浪涌波浪能转换器（WECs）集成到六边形半潜式结构中的混合平台。使用WEC-Sim开发了建模和仿真框架，并以NREL 5 MW半潜式平台为基准进行了验证。进行了稳心高分析以确认静水稳定性，并对十二个几何变量进行了敏感性分析。通过时域仿真研究了波浪入射角的影响，并估算了年发电量（AEP）。

Result: 稳心高分析确认了平台在不同襟翼角度下的静水稳定性。敏感性分析表明，襟翼尺寸和塔架长度是稳定性、能量捕获和塔架应力的主要驱动因素。时域仿真显示，平台性能依赖于波浪入射角，影响襟翼功率分配、捕获宽度比和平台响应。此外，还证明了使用襟翼扫掠来调节俯仰运动的可行性。年发电量估算显示，风能贡献16.86 GWh，波浪能贡献3.65 GWh，其中波浪能转换器约占总发电量的18%。

Conclusion: 研究结果突出了集成式风浪平台在可持续发电方面的潜力。未来的研究应侧重于结构建模和先进控制策略，以进一步优化此类系统的性能。

Abstract: Offshore renewable energy systems offer promising solutions for sustainable
power generation, yet most existing platforms harvest either wind or wave
energy in isolation. This study presents a hybrid floating offshore platform
that integrates a wind turbine with three oscillating surge wave energy
converters (WECs) into a hexagonal semi-submersible structure. In this
configuration, the flaps are integrated with the platform geometry to provide
both energy extraction and hydrodynamic stability. A modeling and simulation
framework was developed using WEC-Sim and benchmarked against the NREL 5 MW
semisubmersible reference. Metacentric height analysis confirmed hydrostatic
stability across a range of prescribed flap angles. Sensitivity analysis of
twelve geometric variables identified flap dimensions and tower length as
dominant drivers of stability, energy capture, and tower stress. Time-domain
simulations revealed dependence on wave incidence angle, with variations in
flap power sharing, capture width ratio (CWR), and platform response. The
feasibility of using flap sweeps to modulate pitch motion was also
demonstrated. Annual energy production (AEP) estimates based on site-specific
data indicate 16.86 GWh from wind and 3.65 GWh from wave energy, with WECs
contributing about 18% of the total. These results highlight the potential of
integrated wind-wave platforms and point toward future studies on structural
modeling and advanced control.

</details>


### [205] [TranSimHub:A Unified Air-Ground Simulation Platform for Multi-Modal Perception and Decision-Making](https://arxiv.org/abs/2510.15365)
*Maonan Wang,Yirong Chen,Yuxin Cai,Aoyu Pang,Yuejiao Xie,Zian Ma,Chengcheng Xu,Kemou Jiang,Ding Wang,Laurent Roullet,Chung Shue Chen,Zhiyong Cui,Yuheng Kan,Michael Lepech,Man-On Pun*

Main category: eess.SY

TL;DR: TranSimHub是一个统一的空地协同智能仿真平台，旨在解决现有仿真环境的不足，支持跨域感知、通信协调和联合决策优化研究。


<details>
  <summary>Details</summary>
Motivation: 下一代城市智能交通管理中，空地协同智能是关键方法，但在感知、通信约束下的协调以及联合决策优化方面，缺乏统一的多模态仿真环境限制了研究进展。

Method: 本文提出了TranSimHub，一个统一的空地协同智能仿真平台。它提供RGB、深度和语义分割等模态的同步多视角渲染，确保空地视角的感知一致性；支持两域间的信息交换；并包含一个因果场景编辑器，允许在不同天气、紧急事件和动态障碍等条件下创建可控场景和进行反事实分析。

Result: TranSimHub作为一个开源平台发布，支持在真实的空中和地面交通场景中进行端到端的感知、融合和控制研究。

Conclusion: TranSimHub填补了空地协同智能研究中统一多模态仿真环境的空白，为感知、融合和控制等方面的研究提供了全面支持。

Abstract: Air-ground collaborative intelligence is becoming a key approach for
next-generation urban intelligent transportation management, where aerial and
ground systems work together on perception, communication, and decision-making.
However, the lack of a unified multi-modal simulation environment has limited
progress in studying cross-domain perception, coordination under communication
constraints, and joint decision optimization. To address this gap, we present
TranSimHub, a unified simulation platform for air-ground collaborative
intelligence. TranSimHub offers synchronized multi-view rendering across RGB,
depth, and semantic segmentation modalities, ensuring consistent perception
between aerial and ground viewpoints. It also supports information exchange
between the two domains and includes a causal scene editor that enables
controllable scenario creation and counterfactual analysis under diverse
conditions such as different weather, emergency events, and dynamic obstacles.
We release TranSimHub as an open-source platform that supports end-to-end
research on perception, fusion, and control across realistic air and ground
traffic scenes. Our code is available at
https://github.com/Traffic-Alpha/TranSimHub.

</details>


### [206] [A Tsetlin Machine Image Classification Accelerator on a Flexible Substrate](https://arxiv.org/abs/2510.15519)
*Yushu Qin,Marcos L. L. Sartori,Shengyu Duan,Emre Ozer,Rishad Shafik,Alex Yakovlev*

Main category: eess.SY

TL;DR: 本文首次在柔性集成电路（FlexIC）上实现了数字Tsetlin机器（TMs），展示了其在可穿戴设备和边缘计算中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: Tsetlin机器因其能效、可解释性和对边缘计算的适用性而闻名，但其部署一直受限于传统硅基芯片的刚性。本研究旨在通过柔性集成电路克服这一限制，实现TMs在更广泛场景中的应用。

Method: 研究团队利用Pragmatic的600nm IGZO基FlexIC技术，开发了两个TM推理模型作为柔性集成电路。这些模型是为8x8像素手写数字识别数据集定制设计的。

Result: 开发了两个TM推理FlexIC模型：一个模型实现了98.5%的准确率，使用了6800个NAND2等效逻辑门，面积为8x8 mm²；另一个更紧凑的版本实现了93%的预测准确率，但仅使用了1420个NAND2等效门，面积为4x4 mm²。

Conclusion: 本文证明了将柔性Tsetlin机器推理引擎部署到可穿戴医疗保健和边缘计算应用中的可行性，为这些领域提供了新的解决方案。

Abstract: This paper introduces the first implementation of digital Tsetlin Machines
(TMs) on flexible integrated circuit (FlexIC) using Pragmatic's 600nm
IGZO-based FlexIC technology. TMs, known for their energy efficiency,
interpretability, and suitability for edge computing, have previously been
limited by the rigidity of conventional silicon-based chips. We develop two TM
inference models as FlexICs: one achieving 98.5% accuracy using 6800 NAND2
equivalent logic gates with an area of 8X8 mm2, and a second more compact
version achieving slightly lower prediction accuracy of 93% but using only 1420
NAND2 equivalent gates with an area of 4X4 mm2, both of which are
custom-designed for an 8X8-pixel handwritten digit recognition dataset. The
paper demonstrates the feasibility of deploying flexible TM inference engines
into wearable healthcare and edge computing applications.

</details>


### [207] [Hypergame-based Cognition Modeling and Intention Interpretation for Human-Driven Vehicles in Connected Mixed Traffic](https://arxiv.org/abs/2510.15573)
*Jianguo Chen,Zhengqin Liu,Jinlong Lei,Peng Yi,Yiguang Hong,Hong Chen*

Main category: eess.SY

TL;DR: 该研究利用超博弈理论和逆向学习，为自动驾驶车辆（CAV）在混合交通流中规划轨迹时，解决人类驾驶车辆（HV）的认知和感知局限性问题，并提出了一个分层认知建模框架和分布式轨迹预测与规划方法。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆（CAV）和人类驾驶车辆（HV）将长期共存，CAV的轨迹规划必须考虑HV的影响，需要准确预测HV轨迹。然而，现有研究常假设人类驾驶员对所有车辆目标有完美认知，这不符合实际情况。因此，需要一种方法来考虑HV的认知和感知局限性。

Method: 本研究利用超博弈理论来解释HV的认知和感知局限性，建模了人类有限理性，而非将其视为被动跟随者。提出了一个捕捉车辆间认知关系的分层认知建模框架，并分析了系统的认知稳定性，证明当CAV准确学习HV参数时，所有车辆采用认知均衡策略的策略组合构成超纳什均衡。为此，开发了一种通过车联网（V2X）通信进行分布式意图解释的逆向学习算法，并将其扩展到离线和在线场景。此外，引入了一种利用学习参数的分布式CAV轨迹预测和规划方法。

Result: 在高速公路变道场景的仿真结果表明，所提出的方法在参数学习方面具有准确性，对嘈杂的轨迹观测具有鲁棒性，并在HV轨迹预测方面表现出安全性。结果验证了该方法在离线和在线实现中的有效性。

Conclusion: 该研究通过引入超博弈理论和逆向学习，成功解决了CAV在混合交通流中轨迹规划时HV有限理性的问题，并提供了一个准确、鲁棒且安全的HV轨迹预测和CAV轨迹规划框架，提升了混合交通系统的安全性和效率。

Abstract: With the practical implementation of connected and autonomous vehicles
(CAVs), the traffic system is expected to remain a mix of CAVs and human-driven
vehicles (HVs) for the foreseeable future. To enhance safety and traffic
efficiency, the trajectory planning strategies of CAVs must account for the
influence of HVs, necessitating accurate HV trajectory prediction. Current
research often assumes that human drivers have perfect knowledge of all
vehicles' objectives, an unrealistic premise. This paper bridges the gap by
leveraging hypergame theory to account for cognitive and perception limitations
in HVs. We model human bounded rationality without assuming them to be merely
passive followers and propose a hierarchical cognition modeling framework that
captures cognitive relationships among vehicles. We further analyze the
cognitive stability of the system, proving that the strategy profile where all
vehicles adopt cognitively equilibrium strategies constitutes a hyper Nash
equilibrium when CAVs accurately learn HV parameters. To achieve this, we
develop an inverse learning algorithm for distributed intention interpretation
via vehicle-to-everything (V2X) communication, which extends the framework to
both offline and online scenarios. Additionally, we introduce a distributed
trajectory prediction and planning approach for CAVs, leveraging the learned
parameters in real time. Simulations in highway lane-changing scenarios
demonstrate the proposed method's accuracy in parameter learning, robustness to
noisy trajectory observations, and safety in HV trajectory prediction. The
results validate the effectiveness of our method in both offline and online
implementations.

</details>


### [208] [Observer Design over Hypercomplex Quaternions](https://arxiv.org/abs/2510.15598)
*Michael Sebek*

Main category: eess.SY

TL;DR: 该论文提出了一种在超复四元数上进行观测器设计的新框架，该框架无需特征多项式，解决了传统实数/复数方法不适用于四元数的问题。


<details>
  <summary>Details</summary>
Motivation: 传统的确定观测器设计的方法（如使用行列式、特征/最小多项式和Cayley-Hamilton恒等式）无法直接应用于四元数，因此需要一种新的、适用于四元数的观测器设计方法。

Method: 研究采用右模约定，推导了右可观测伴随形式及其伴随多项式，通过右特征值相似类编码误差动力学。设计过程模仿实数/复数情况，即在伴随坐标中更新系数，然后进行相似性变换。该方法避免了行列式和特征多项式等概念。对于具有实系数的闭环伴随多项式，还提供了一种Ackermann型构造。

Result: 该方法为四元数上的全阶观测器提供了简单的设计方案，阐明了右谱及其相似类的作用，并明确了经典一次性公式何时仍然有效。数值示例表明，该方法优于向量化或复伴随替代方法。

Conclusion: 该研究成功开发了一种无需特征多项式的超复四元数观测器设计框架，解决了现有方法在四元数领域不适用的问题，为四元数系统提供了直接且有效的观测器设计工具。

Abstract: We develop observer design over hypercomplex quaternions in a
characteristic-polynomial-free framework. Using the standard right-module
convention, we derive a right observable companion form and its companion
polynomial that encodes error dynamics via right-eigenvalue similarity classes.
The design mirrors the real/complex case - coefficient updates in companion
coordinates, followed by a similarity back - yet avoids determinants,
characteristic/minimal polynomials, and Cayley-Hamilton identities that do not
transfer to quaternions. We also give an Ackermann-type construction for the
important case of closed-loop companion polynomials with real coefficients,
ensuring similarity-equivariant evaluation. The results yield simple recipes
for full-order observers directly over quaternions, clarify the role of right
spectra and their similarity classes, and pinpoint when classical one-shot
formulas remain valid. Numerical examples illustrate the method and advantages
over vectorized or complex-adjoint surrogates.

</details>


### [209] [A Predictive Flexibility Aggregation Method for Low Voltage Distribution System Control](https://arxiv.org/abs/2510.15613)
*Clément Moureau,Thomas Stegen,Mevludin Glavic,Bertrand Cornélusse*

Main category: eess.SY

TL;DR: 本文提出了一种预测控制策略，通过聚合住宅单元的灵活性并生成三维图表（有功功率、无功功率、灵活性成本），实现低压配电系统的管理。


<details>
  <summary>Details</summary>
Motivation: 旨在实现低压配电系统的高效、低成本且保护隐私的管理，通过将实时灵活性评估与能源调度相结合。

Method: 该方法首先在住宅单元层面离线解决一个多参数优化问题，以聚合资产的灵活性。然后，解决一个半显式模型预测控制问题以考虑预测。通过结合这些问题的结果和测量数据，生成所需的三维灵活性图表。由于繁重计算离线执行，该方法兼容实时控制并易于并行化。

Result: 该方法能够生成所需的灵活性图表，并兼容实时控制要求。它实现了低压配电系统的高效、低成本和隐私保护管理。该方法在一个5总线低压网络上通过与理想技术比较进行了验证。

Conclusion: 所提出的预测控制策略，通过聚合住宅单元灵活性并利用离线计算和模型预测控制生成灵活性图表，为低压配电系统提供了一种高效、低成本且保护隐私的实时管理解决方案。

Abstract: This paper presents a predictive control strategy to manage low-voltage
distribution systems. The proposed approach relies on an aggregate of the
flexibility at the residential unit level into a three-dimensional chart that
represents the injected active and reactive power, and the flexibility cost.
First, this method solves a multiparametric optimization problem offline at the
residential unit level to aggregate the flexibility of the assets. Then, a
semi-explicit model predictive control problem is solved to account for
forecasts. By combining the results of these problems with measurements, the
method generates the desired flexibility chart. The proposed approach is
compatible with realtime control requirements, as heavy computations are
performed offline locally, making it naturally parallelizable. By linking
realtime flexibility assessment with energy scheduling, our approach enables
efficient, low-cost, and privacy-preserving management of low-voltage
distribution systems. We validate this method on a low-voltage network of 5
buses by comparing it with an ideal technique.

</details>


### [210] [Cross-border offshore hydrogen trade and carbon mitigation for Europe's net zero transition](https://arxiv.org/abs/2510.15695)
*Sheng Wang,Muhammad Maladoh Bah*

Main category: eess.SY

TL;DR: 本研究利用自下而上的方法，评估了爱尔兰和英国的离岸绿色氢能在欧洲脱碳中的潜力及其对减排和能源安全的影响。


<details>
  <summary>Details</summary>
Motivation: 欧洲国家在净零转型和离岸能源开发方面雄心勃勃，爱尔兰和英国的离岸风电容量远超其自身需求。与此同时，其他欧洲大陆国家对清洁燃料资源有强烈需求。因此，出口剩余的离岸绿色氢能，连接供需两端，对于欧洲的碳减排至关重要，而这些岛国的潜力常被低估。

Method: 本文开发了一种自下而上的方法，以研究爱尔兰和英国的离岸氢能在整个欧洲脱碳中的作用。该方法评估了未来的氢/氨贸易以及各国在碳减排中的贡献，同时考虑了离岸氢生产的相对成本竞争力、国内每小时电力和天然气系统运行以及国际航运成本。

Result: 研究结果表明，离岸绿色氢能每年可减少欧洲175.16兆吨的二氧化碳排放。英国将在2030年至2040年成为最大的氢气供应国，但到2050年将被爱尔兰超越，爱尔兰将向法国和西班牙出口161太瓦时的氢气。

Conclusion: 从西方到东方的氢气流动不仅促进了欧洲的净零进展，还重塑了能源供应结构，并有助于确保整个欧洲大陆的能源安全。离岸绿色氢能总共可贡献175.16兆吨的年二氧化碳减排量。

Abstract: European countries are ambitious in both the net-zero transition and offshore
energy resource development. The Irish and UK governments announced their
commitments to offshore wind capacities - 37 and 125 GW, respectively, in 2050,
more than two times higher than their projected power demands. While other
continental countries, such as Germany, are calling for cleaner fuel resources.
Exporting surplus offshore green hydrogen and bridging supply and demand could
be pivotal in carbon emission mitigation for Europe. Yet, the potentials of
these Island countries, are usually underestimated. This paper developed a
bottom-up method to investigate the role of offshore hydrogen from Ireland and
the UK in the decarbonisation of the entire Europe. We evaluate the future
hydrogen/ammonia trading and the contributions of each country in carbon
emission mitigation, considering their relative cost-competitiveness in
offshore hydrogen production, domestic hourly power and gas system operation,
and international shipping costs. Results indicate that the offshore green
hydrogen could reduce 175.16 Mt/year of carbon dioxide emissions in Europe. The
UK will be the largest hydrogen supplier from 2030 to 2040, while surpassed by
Ireland in 2050, with 161 TWh of hydrogen exports to France and Spain. The
offshore green hydrogen can contribute to 175.16 Mt of annual carbon dioxide
emission reductions in total. This general flow of hydrogen from the West to
the East not only facilitates Europe's net-zero progress, but also reshapes the
energy supply structure and helps to ensure energy security across the European
continent.

</details>


### [211] [Mitigating Underwater Noise from Offshore Wind Turbines via Individual Pitch Control](https://arxiv.org/abs/2510.15707)
*Martín de Frutos,Laura Botero-Bolívar,Esteban Ferrer*

Main category: eess.SY

TL;DR: 本文提出了一种桨距控制策略，用于降低海上风力涡轮机的水下声学足迹，以减轻对海洋生物的影响。


<details>
  <summary>Details</summary>
Motivation: 海上风力涡轮机的水下声学足迹对依赖声音进行交流、导航和生存的海洋生物产生负面影响。未来将需要采取措施最大程度地减少这种影响。

Method: 首先，通过耦合叶素动量模型和气水声传播模型，量化了NREL 5 MW、DTU 10 MW和IEA 22 MW三种参考涡轮机叶片产生的气动噪声的水下声学特征。其次，提出并实施了一种开环个体桨距控制（IPC）策略，通过在叶片通过频率处调节叶片桨距来衰减总声压级（OSPL）和传输噪声的幅值调制（AM）。最后，将IPC性能与传统桨距方案进行了基准比较。

Result: 研究结果表明，通过大约5°的桨距变化，可以实现OSPL高达5 dB的降低和AM深度20%的减少，同时能量捕获损失很小（5-10%）。

Conclusion: 这些发现突出了一个此前未被充分认识的噪声传播途径，并证明了有针对性的叶片桨距调制可以有效减轻其影响。

Abstract: This paper proposes a pitch control strategy to mitigate the underwater
acoustic footprint of offshore wind turbines, a measure that will soon become
necessary to minimize impacts on marine life, which rely on sound for
communication, navigation, and survival. First, we quantify the underwater
acoustic signature of blade-generated aerodynamic noise from three reference
turbines, the NREL 5 MW, DTU 10 MW, and IEA 22 MW, using coupling blade element
momentum and coupled air-water acoustic propagation modeling. Second, we
propose and implement an open-loop individual pitch control (IPC) strategy that
modulates the pitch of the blade at the blade passing frequency to attenuate
the overall sound pressure level (OSPL) and the amplitude modulation (AM) of
the transmitted noise. Third, we benchmark IPC performance against conventional
pitch schemes. The results indicate that up to 5 dB reductions in OSPL and a
decrease in AM depth 20% can be achieved with a pitch variation of
$\Delta\theta\approx 5^\circ$, with small losses (5-10%) in energy capture.
These findings highlight a previously underappreciated noise pathway and
demonstrate that targeted blade-pitch modulation can mitigate its impact.

</details>


### [212] [Sugar Shack 4.0: Practical Demonstration of an IIoT-Based Event-Driven Automation System](https://arxiv.org/abs/2510.15708)
*Thomas Bernard,François Grondin,Jean-Michel Lavoie*

Main category: eess.SY

TL;DR: 本文提出了一种基于工业物联网工具的事件驱动架构，作为可编程逻辑控制器（PLC）自动化的一种实用替代方案，并在枫糖浆生产中心进行了部署和评估，结果表明其性能良好且具有多项优势。


<details>
  <summary>Details</summary>
Motivation: 研究动机是为传统的以PLC为中心的工业自动化提供一个实用的替代方案，利用现代工业物联网工具构建一个更灵活、高效的自动化系统。

Method: 该方法采用事件驱动架构，基于工业物联网工具构建。核心是一个部署在本地边缘服务器上的分层设计，该设计抽象化执行器、通过带优先级队列的互锁机制强制共享物理资源的互斥、组合确定性单一操作，并使用Node-RED中的状态机编排完整工作流，通过MQTT进行通信。设备层使用基于ESP32的低成本网关连接传感器和执行器，所有自动化逻辑都卸载到服务器端。该系统在一个枫糖浆熬制中心进行了案例研究部署。

Result: 在整个生产季节的评估显示，消息传输中位时间约为十分之一秒，命令发布到动作的延迟约为2到3秒，命令完成时间接近6秒（主要受执行器机械性能影响），操作运行时长从几十秒到几分钟不等。结果表明，网络和编排开销相对于过程动态可以忽略不计。

Conclusion: 该方法实现了模块化、分布式控制，同时不损害确定性或故障隔离。它减少了物料和集成工作量，支持可移植的容器化部署，并自然地实现了边缘/云分离，其中持久性存储和分析被卸载到云端，而自动化功能保留在边缘。

Abstract: This paper presents a practical alternative to
programmable-logic-controller-centric automation by implementing an
event-driven architecture built with industrial Internet of Things tools. A
layered design on a local edge server (i) abstracts actuators, (ii) enforces
mutual exclusion of shared physical resources through an interlock with
priority queueing, (iii) composes deterministic singular operations, and (iv)
orchestrates complete workflows as state machines in Node-RED, with
communication over MQTT. The device layer uses low-cost ESP32-based gateways to
interface sensors and actuators, while all automation logic is offloaded to the
server side. As part of a larger project involving the first
scientifically-documented integration of Industry 4.0 technologies in a maple
syrup boiling center, this work demonstrates the deployment of the proposed
system as a case-study. Evaluation over an entire production season shows
median message time of flight around one tenth of a second, command
issuance-to-motion latencies of about two to three seconds, and command
completion near six seconds dominated by actuator mechanics; operation runtimes
span tens of seconds to minutes. These results indicate that network and
orchestration overheads are negligible relative to process dynamics, enabling
modular, distributed control without compromising determinism or fault
isolation. The approach reduces material and integration effort, supports
portable containerized deployment, and naturally enables an edge/cloud split in
which persistence and analytics are offloaded while automation remains at the
edge.

</details>


### [213] [Integrating Conductor Health into Dynamic Line Rating and Unit Commitment under Uncertainty](https://arxiv.org/abs/2510.15740)
*Geon Roh,Jip Kim*

Main category: eess.SY

TL;DR: 动态线路额定容量（DLR）提高了输电线路利用率，但常忽视高温运行对导线健康的影响。本文量化了高温运行的折旧成本，并提出了一个考虑导线健康的机组组合（CHA-UC）模型，该模型通过将折旧成本内部化到运行决策中，相比静态线路额定容量（SLR）降低了总成本和可再生能源弃用，并比传统DLR操作更安全。


<details>
  <summary>Details</summary>
Motivation: DLR利用实时天气数据提高了现有输电线路的利用率，但其导致导线高温运行（ETO）对导线健康的长期影响及其相关折旧成本常被忽视，这促使研究者量化这些成本并将其纳入运行决策。

Method: 1) 量化了与高温运行相关的折旧成本。2) 提出了一个导线健康感知机组组合（CHA-UC）模型，该模型将导线温度的鲁棒线性近似和每小时高温运行导致的预期折旧成本整合到目标函数中。3) 在德克萨斯州123节点骨干测试系统上使用NOAA天气数据进行了案例研究。

Result: 与静态线路额定容量（SLR）相比，所提出的CHA-UC模型将总成本降低了0.8%，可再生能源弃用减少了84%。而未考虑风险的传统DLR操作因过度高温运行导致了更高的成本。CHA-UC通过调整发电机组组合实现了更安全的线路潮流。此外，CHA-UC还能自适应地管理风力发电和DLR预测误差之间的相关性，在风险对冲条件下放宽潮流，在风险放大条件下收紧潮流。

Conclusion: CHA-UC模型通过将导线高温运行的折旧成本内部化到运行决策中，实现了比传统DLR和SLR更低的系统总成本和更高的可再生能源利用率，同时确保了更安全的线路运行。它还能自适应地应对DLR预测的不确定性，有效管理电网风险。

Abstract: Dynamic line rating (DLR) enables greater utilization of existing
transmission lines by leveraging real-time weather data. However, the elevated
temperature operation (ETO) of conductors under DLR is often overlooked,
despite its long-term impact on conductor health. This paper addresses this
issue by 1) quantifying depreciation costs associated with ETO and 2) proposing
a Conductor Health-Aware Unit Commitment (CHA-UC) that internalizes these costs
in operational decisions. The CHA-UC incorporates a robust linear approximation
of conductor temperature and integration of expected depreciation costs due to
hourly ETO into the objective function. Case studies on the Texas 123-bus
backbone test system using NOAA weather data demonstrate that the proposed
CHA-UC model reduces the total cost by 0.8% and renewable curtailment by
84%compared to static line rating (SLR), while conventional DLR operation
without risk consideration resulted in higher costs due to excessive ETO.
Further analysis of the commitment decisions and the line temperature
statistics confirms that the CHA-UC achieves safer line flows by shifting
generator commitments. Finally, we examine the emergent correlation between
wind generation and DLR forecast errors, and show that CHA-UC adaptively
manages this effect by relaxing flows for risk-hedging conditions while
tightening flows for risk-amplifying ones.

</details>


### [214] [Braking within Barriers: Constructive Safety-Critical Control for Input-Constrained Vehicles via the Backup Set Method](https://arxiv.org/abs/2510.15797)
*Laszlo Gacsi,Adam K. Kiss,Tamas G. Molnar*

Main category: eess.SY

TL;DR: 本文提出了一种安全关键控制框架，用于车辆在非对称路面制动时，在最小化制动距离的同时，通过合成制动控制器来辅助驾驶员并保证横向运动受限（防止打滑）。


<details>
  <summary>Details</summary>
Motivation: 车辆在非对称路面制动时存在过度横向运动（打滑）的风险，需要控制器辅助驾驶员保证安全。同时，制动力受路面摩擦力限制，存在输入约束。

Method: 采用安全关键控制框架，并合成制动控制器。利用备份控制障碍函数（Backup Control Barrier Functions）进行安全控制设计。提出了一种基于反馈线性化和连续时间Lyapunov方程的新颖、系统化方法来创建有效的备份集-备份控制器对。

Result: 通过简单示例验证了所提出的安全关键控制方法。将该方法应用于四轮车辆模型在非对称路面上的制动场景，并展示了仿真结果。

Conclusion: 所提出的安全关键控制框架能够在存在输入约束的情况下，保证车辆在非对称路面制动时横向运动受限（防止打滑），同时最小化制动距离。

Abstract: This paper presents a safety-critical control framework to maintain bounded
lateral motions for vehicles braking on asymmetric surfaces. We synthesize a
brake controller that assists drivers and guarantees safety against excessive
lateral motions (i.e., prevents the vehicle from spinning out) while minimizing
the stopping distance. We address this safety-critical control problem in the
presence of input constraints, since braking forces are limited by the
available friction on the road. We use backup control barrier functions for
safe control design. As this approach requires the construction of a backup set
and a backup controller, we propose a novel, systematic method to creating
valid backup set-backup controller pairs based on feedback linearization and
continuous-time Lyapunov equations. We use simple examples to demonstrate our
proposed safety-critical control method. Finally, we implement our approach on
a four-wheel vehicle model for braking on asymmetric surfaces and present
simulation results.

</details>


### [215] [Bio-inspired Microgrid Management based on Brain's Sensorimotor Gating](https://arxiv.org/abs/2510.15847)
*Panos C. Papageorgiou,Anastasios E. Giannopoulos,Sotirios T. Spantideas*

Main category: eess.SY

TL;DR: 本文提出了一种受大脑感觉运动门控机制（前脉冲抑制和前脉冲促进）启发的神经微电网（SG-NMG）范式，旨在实现微电网的自保护、自适应和弹性控制。


<details>
  <summary>Details</summary>
Motivation: 微电网在动态扰动处理、保护协调和不确定性管理方面仍面临挑战。虽然脑情感学习控制器有所探索，但需要新的生物启发式解决方案来增强微电网的韧性、可持续性和智能化。

Method: 本文提出了一种感觉运动门控启发式神经微电网（SG-NMG）框架。该框架将大脑的感觉运动门控机制（特别是前脉冲抑制PPI和前脉冲促进PPF）映射到微电网的层级控制架构中：PPI对应于微电网初级和次级管理中的保护性阻尼，PPF对应于纠正性控制动作的自适应放大。方法包括分析工作流设计、神经电路类比以及与机器学习方法的集成。

Result: 研究结果是提出了一种新的神经微电网范式，该范式利用感觉运动门控原理来设计自保护、自适应和弹性的微电网。该框架通过其分析工作流设计、神经电路类比和与机器学习的集成，展示了感觉运动门控作为一种有前景的控制策略的潜力。

Conclusion: 感觉运动门控是一种有前景的框架，可用于设计自保护、自适应和弹性的微电网。未来的研究方向包括门控的数学建模、数字孪生验证以及神经科学与工业电力系统之间的跨学科合作。

Abstract: Microgrids are emerging as key enablers of resilient, sustainable, and
intelligent power systems, but they continue to face challenges in dynamic
disturbance handling, protection coordination, and uncertainty. Recent efforts
have explored Brain Emotional Learning (BEL) controllers as bio-inspired
solutions for microgrid control. Building on this growing trajectory, this
article introduces a new paradigm for Neuro-Microgrids, inspired by the brain's
sensorimotor gating mechanisms, specifically the Prepulse Inhibition (PPI) and
Prepulse Facilitation (PPF). Sensorimotor gating offers a biological model for
selectively suppressing or amplifying responses depending on contextual
relevance. By mapping these principles onto the hierarchical control
architecture of microgrids, we propose a Sensorimotor Gating-Inspired
Neuro-Microgrid (SG-NMG) framework. In this architecture, PPI-like control
decisions correspond to protective damping in primary and secondary management
of microgrids, whereas PPF-like decisions correspond to adaptive amplification
of corrective control actions. The framework is presented through analytical
workflow design, neuro-circuitry analogies, and integration with machine
learning methods. Finally, open challenges and research directions are
outlined, including the mathematical modeling of gating, digital twin
validation, and cross-disciplinary collaboration between neuroscience and
industrial power systems. The resulting paradigm highlights sensorimotor gating
as a promising framework for designing self-protective, adaptive, and resilient
microgrids.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [216] [A Cross-Framework Study of Temporal Information Buffering Strategies for Learned Video Compression](https://arxiv.org/abs/2510.15426)
*Kuan-Wei Ho,Yi-Hsin Chen,Martin Benjak,Jörn Ostermann,Wen-Hsiao Peng*

Main category: eess.IV

TL;DR: 本文系统性地评估了三种时间信息传播策略（显式、隐式、混合缓冲）在四种帧间编码框架下的视频编码性能，以提供对其有效性的全面理解。


<details>
  <summary>Details</summary>
Motivation: 尽管学习型视频编解码器取得了显著进展，但目前仍缺乏一项涵盖所有可能组合的综合研究，特别是关于帧间编码框架和时间信息传播策略的系统评估。

Method: 研究采用统一的实验设置，系统地评估了显式、隐式和混合缓冲三种时间信息传播方法在残差编码、条件编码、条件残差编码和掩码条件残差编码四种帧间编码框架下的编码性能。

Result: 通过系统性评估，本文旨在深入理解不同时间信息传播策略和帧间编码框架组合的有效性。

Conclusion: 该研究旨在为学习型视频编解码器中帧间编码框架和时间信息传播策略的设计提供全面而深入的理解。

Abstract: Recent advances in learned video codecs have demonstrated remarkable
compression efficiency. Two fundamental design aspects are critical: the choice
of inter-frame coding framework and the temporal information propagation
strategy. Inter-frame coding frameworks include residual coding, conditional
coding, conditional residual coding, and masked conditional residual coding,
each with distinct mechanisms for utilizing temporal predictions. Temporal
propagation methods can be categorized as explicit, implicit, or hybrid
buffering, differing in how past decoded information is stored and used.
However, a comprehensive study covering all possible combinations is still
lacking. This work systematically evaluates the impact of explicit, implicit,
and hybrid buffering on coding performance across four inter-frame coding
frameworks under a unified experimental setup, providing a thorough
understanding of their effectiveness.

</details>


### [217] [Confidence-Weighted Semi-Supervised Learning for Skin Lesion Segmentation Using Hybrid CNN-Transformer Networks](https://arxiv.org/abs/2510.15354)
*Saqib Qamar*

Main category: eess.IV

TL;DR: MIRA-U是一个半监督框架，结合不确定性感知师生伪标签和混合CNN-Transformer架构，用于皮肤病变分割，在标注数据有限的情况下，其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 早期皮肤癌检测依赖于自动化皮肤病变分割，但由于带标注的训练数据有限，这仍然是一个挑战。

Method: 该研究提出了MIRA-U，一个半监督框架。它采用通过掩码图像建模预训练的教师网络生成置信度加权的软伪标签，这些标签用于指导具有交叉注意力跳跃连接的U形CNN-Transformer学生网络。这种设计旨在提高伪标签质量和边界描绘。

Result: MIRA-U在低标注数据量下，超越了基于重建和仅使用CNN的基线方法。在ISIC-2016和PH2数据集上，仅使用50%的标注数据就达到了0.9153的Dice相似系数（DSC）和0.8552的交并比（IoU）。代码已公开。

Conclusion: MIRA-U通过其半监督方法和混合CNN-Transformer架构，有效解决了皮肤病变分割中带标注数据不足的挑战，并表现出卓越的性能，尤其是在标注数据有限的情况下。

Abstract: Automated skin lesion segmentation through dermoscopic analysis is essential
for early skin cancer detection, yet remains challenging due to limited
annotated training data. We present MIRA-U, a semi-supervised framework that
combines uncertainty-aware teacher-student pseudo-labeling with a hybrid
CNN-Transformer architecture. Our approach employs a teacher network
pre-trained via masked image modeling to generate confidence-weighted soft
pseudo-labels, which guide a U-shaped CNN-Transformer student network featuring
cross-attention skip connections. This design enhances pseudo-label quality and
boundary delineation, surpassing reconstruction-based and CNN-only baselines,
particularly in low-annotation regimes. Extensive evaluation on ISIC-2016 and
PH2 datasets demonstrates superior performance, achieving a Dice Similarity
Coefficient (DSC) of 0.9153 and Intersection over Union (IoU) of 0.8552 using
only 50% labeled data. Code is publicly available on GitHub.

</details>


### [218] [SANR: Scene-Aware Neural Representation for Light Field Image Compression with Rate-Distortion Optimization](https://arxiv.org/abs/2510.15775)
*Gai Zhang,Xinfeng Zhang,Lv Tang,Hongyu An,Li Zhang,Qingming Huang*

Main category: eess.IV

TL;DR: SANR是一种场景感知神经表示框架，通过分层场景建模和端到端率失真优化，显著提升了光场图像压缩性能。


<details>
  <summary>Details</summary>
Motivation: 光场图像数据量巨大，压缩效率低下。现有基于神经表示的压缩方法通常忽略场景结构建模，且缺乏端到端率失真优化，限制了压缩效率。

Method: 本文提出了SANR框架，引入分层场景建模模块，利用多尺度潜在编码捕获内在场景结构，减少输入坐标与目标光场图像之间的信息差距。同时，SANR首次将熵约束量化感知训练（QAT）引入基于神经表示的光场图像压缩，实现端到端率失真优化。

Result: 实验结果表明，SANR在率失真性能方面显著优于现有最先进技术，相较于HEVC实现了65.62%的BD-rate节省。

Conclusion: SANR通过集成场景感知能力和端到端率失真优化，有效解决了光场图像压缩的挑战，取得了卓越的压缩性能。

Abstract: Light field images capture multi-view scene information and play a crucial
role in 3D scene reconstruction. However, their high-dimensional nature results
in enormous data volumes, posing a significant challenge for efficient
compression in practical storage and transmission scenarios. Although neural
representation-based methods have shown promise in light field image
compression, most approaches rely on direct coordinate-to-pixel mapping through
implicit neural representation (INR), often neglecting the explicit modeling of
scene structure. Moreover, they typically lack end-to-end rate-distortion
optimization, limiting their compression efficiency. To address these
limitations, we propose SANR, a Scene-Aware Neural Representation framework for
light field image compression with end-to-end rate-distortion optimization. For
scene awareness, SANR introduces a hierarchical scene modeling block that
leverages multi-scale latent codes to capture intrinsic scene structures,
thereby reducing the information gap between INR input coordinates and the
target light field image. From a compression perspective, SANR is the first to
incorporate entropy-constrained quantization-aware training (QAT) into neural
representation-based light field image compression, enabling end-to-end
rate-distortion optimization. Extensive experiment results demonstrate that
SANR significantly outperforms state-of-the-art techniques regarding
rate-distortion performance with a 65.62\% BD-rate saving against HEVC.

</details>


### [219] [Symmetric Entropy-Constrained Video Coding for Machines](https://arxiv.org/abs/2510.15347)
*Yuxiao Sun,Yao Zhao,Meiqin Liu,Chao Yao,Jian Jin,Weisi Lin*

Main category: eess.IV

TL;DR: 本文提出了一种名为SEC-VCM的对称熵约束视频编码框架，通过视觉主干（VB）引导，实现了编解码器与VB的对称对齐，以在多任务机器视觉系统中（MVS）实现语义保留和无用信息抑制，从而达到SOTA的率-任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有的机器视频编码（VCM）方法通常将编解码器与特定下游模型绑定，限制了其在多任务场景中的泛化能力，且很少探索如何在视觉主干（VB）或视觉基础模型（VFM）的指导下直接将视频编码与理解联系起来。

Method: 提出SEC-VCM框架，通过编解码器与VB的对称对齐来利用VB的表示能力。具体包括：1) 双向熵约束（BiEC）机制，通过抑制条件熵确保视频解码和VB编码过程的对称性，显式处理语义信息并压缩无用信息；2) 语义-像素双路径融合（SPDF）模块，将像素级先验注入最终重建中，抑制有害伪影并改善机器导向的重建质量。

Result: SEC-VCM在率-任务性能方面达到了最先进水平（SOTA），相比VTM在视频实例分割（37.41%）、视频目标分割（29.83%）、目标检测（46.22%）和多目标跟踪（44.94%）等任务上显著节省了比特率。

Conclusion: SEC-VCM通过建立编解码器与视觉主干的对称对齐，有效将视频编码与理解结合，成功保留了对MVS有益的语义信息并抑制了无用信息，显著提升了机器视觉任务的性能和编码效率。

Abstract: As video transmission increasingly serves machine vision systems (MVS)
instead of human vision systems (HVS), video coding for machines (VCM) has
become a critical research topic. Existing VCM methods often bind codecs to
specific downstream models, requiring retraining or supervised data and thus
limiting generalization in multi-task scenarios. Recently, unified VCM
frameworks have employed visual backbones (VB) and visual foundation models
(VFM) to support multiple video understanding tasks with a single codec. They
mainly utilize VB/VFM to maintain semantic consistency or suppress non-semantic
information, but seldom explore how to directly link video coding with
understanding under VB/VFM guidance. Hence, we propose a Symmetric
Entropy-Constrained Video Coding framework for Machines (SEC-VCM). It
establishes a symmetric alignment between the video codec and VB, allowing the
codec to leverage VB's representation capabilities to preserve semantics and
discard MVS-irrelevant information. Specifically, a bi-directional
entropy-constraint (BiEC) mechanism ensures symmetry between the process of
video decoding and VB encoding by suppressing conditional entropy. This helps
the codec to explicitly handle semantic information beneficial for MVS while
squeezing useless information. Furthermore, a semantic-pixel dual-path fusion
(SPDF) module injects pixel-level priors into the final reconstruction. Through
semantic-pixel fusion, it suppresses artifacts harmful to MVS and improves
machine-oriented reconstruction quality. Experimental results show our
framework achieves state-of-the-art (SOTA) in rate-task performance, with
significant bitrate savings over VTM on video instance segmentation (37.41%),
video object segmentation (29.83%), object detection (46.22%), and multiple
object tracking (44.94%). We will release our code.

</details>
