{"id": "2508.08262", "categories": ["cs.CL", "68T50", "I.2.7; H.3.1"], "pdf": "https://arxiv.org/pdf/2508.08262", "abs": "https://arxiv.org/abs/2508.08262", "authors": ["Alaa Alhamzeh", "Mays Al Rebdawi"], "title": "Argument Quality Annotation and Gender Bias Detection in Financial Communication through Large Language Models", "comment": "8 pages, 4 figures, Passau uni, Master thesis in NLP", "summary": "Financial arguments play a critical role in shaping investment decisions and\npublic trust in financial institutions. Nevertheless, assessing their quality\nremains poorly studied in the literature. In this paper, we examine the\ncapabilities of three state-of-the-art LLMs GPT-4o, Llama 3.1, and Gemma 2 in\nannotating argument quality within financial communications, using the\nFinArgQuality dataset. Our contributions are twofold. First, we evaluate the\nconsistency of LLM-generated annotations across multiple runs and benchmark\nthem against human annotations. Second, we introduce an adversarial attack\ndesigned to inject gender bias to analyse models responds and ensure model's\nfairness and robustness. Both experiments are conducted across three\ntemperature settings to assess their influence on annotation stability and\nalignment with human labels. Our findings reveal that LLM-based annotations\nachieve higher inter-annotator agreement than human counterparts, though the\nmodels still exhibit varying degrees of gender bias. We provide a multifaceted\nanalysis of these outcomes and offer practical recommendations to guide future\nresearch toward more reliable, cost-effective, and bias-aware annotation\nmethodologies.", "AI": {"tldr": "本文评估了GPT-4o、Llama 3.1和Gemma 2在金融论证质量标注方面的能力，发现LLM在标注一致性上优于人类，但仍存在性别偏见。", "motivation": "金融论证对投资决策和公众信任至关重要，但对其质量的评估研究不足。本文旨在探究LLM在评估金融论证质量方面的潜力。", "method": "使用FinArgQuality数据集，评估了GPT-4o、Llama 3.1和Gemma 2这三种LLM在金融论证质量标注上的表现。方法包括：1) 评估LLM生成标注的跨运行一致性，并与人类标注进行基准测试。2) 引入对抗性攻击，注入性别偏见以分析模型响应、公平性和鲁棒性。所有实验均在三种温度设置下进行，以评估其对标注稳定性和与人类标签一致性的影响。", "result": "研究发现，基于LLM的标注在标注者间一致性方面高于人类标注，但模型仍表现出不同程度的性别偏见。", "conclusion": "LLM在金融论证质量标注方面具有潜力，可以实现更高的标注一致性，但需要关注并解决其固有的偏见问题。研究提供了多方面分析和实践建议，以指导未来更可靠、经济高效和偏见感知的标注方法研究。"}}
{"id": "2508.08265", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08265", "abs": "https://arxiv.org/abs/2508.08265", "authors": ["Tarık Saraç", "Selin Mergen", "Mucahid Kutlu"], "title": "TurQUaz at CheckThat! 2025: Debating Large Language Models for Scientific Web Discourse Detection", "comment": null, "summary": "In this paper, we present our work developed for the scientific web discourse\ndetection task (Task 4a) of CheckThat! 2025. We propose a novel council debate\nmethod that simulates structured academic discussions among multiple large\nlanguage models (LLMs) to identify whether a given tweet contains (i) a\nscientific claim, (ii) a reference to a scientific study, or (iii) mentions of\nscientific entities. We explore three debating methods: i) single debate, where\ntwo LLMs argue for opposing positions while a third acts as a judge; ii) team\ndebate, in which multiple models collaborate within each side of the debate;\nand iii) council debate, where multiple expert models deliberate together to\nreach a consensus, moderated by a chairperson model. We choose council debate\nas our primary model as it outperforms others in the development test set.\nAlthough our proposed method did not rank highly for identifying scientific\nclaims (8th out of 10) or mentions of scientific entities (9th out of 10), it\nranked first in detecting references to scientific studies.", "AI": {"tldr": "该论文提出了一种新颖的“委员会辩论”方法，通过模拟多LLM的结构化讨论来检测推文中的科学内容，在识别科学研究引用方面表现最佳。", "motivation": "该研究旨在解决CheckThat! 2025挑战赛任务4a中科学网络话语检测的问题，即判断推文是否包含科学主张、科学研究引用或科学实体提及。", "method": "本文提出了一种“委员会辩论”方法，模拟多大型语言模型（LLM）之间的结构化学术讨论。研究探索了三种辩论方法：1) 单一辩论（两个LLM对立，一个LLM裁决）；2) 团队辩论（多个模型在各自立场内协作）；3) 委员会辩论（多个专家模型在主席模型协调下共同讨论达成共识）。最终选择委员会辩论作为主要模型，因其在开发测试集上表现最佳。", "result": "所提出的方法在检测科学研究引用方面排名第一；但在识别科学主张方面排名第8（共10名），在提及科学实体方面排名第9（共10名）。", "conclusion": "尽管该方法在检测科学主张和科学实体提及方面表现不佳，但其在检测推文中科学研究引用方面取得了领先地位，证明了LLM辩论模型在特定科学话语检测任务上的有效性。"}}
{"id": "2508.08271", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.08271", "abs": "https://arxiv.org/abs/2508.08271", "authors": ["Victoria Williams", "Benjamin Rosman"], "title": "Heartificial Intelligence: Exploring Empathy in Language Models", "comment": "21 pages, 5 tables", "summary": "Large language models have become increasingly common, used by millions of\npeople worldwide in both professional and personal contexts. As these models\ncontinue to advance, they are frequently serving as virtual assistants and\ncompanions. In human interactions, effective communication typically involves\ntwo types of empathy: cognitive empathy (understanding others' thoughts and\nemotions) and affective empathy (emotionally sharing others' feelings). In this\nstudy, we investigated both cognitive and affective empathy across several\nsmall (SLMs) and large (LLMs) language models using standardized psychological\ntests. Our results revealed that LLMs consistently outperformed humans -\nincluding psychology students - on cognitive empathy tasks. However, despite\ntheir cognitive strengths, both small and large language models showed\nsignificantly lower affective empathy compared to human participants. These\nfindings highlight rapid advancements in language models' ability to simulate\ncognitive empathy, suggesting strong potential for providing effective virtual\ncompanionship and personalized emotional support. Additionally, their high\ncognitive yet lower affective empathy allows objective and consistent emotional\nsupport without running the risk of emotional fatigue or bias.", "AI": {"tldr": "研究发现大型语言模型（LLMs）在认知同理心方面超越人类，但在情感同理心方面远低于人类，这表明其在提供虚拟陪伴和客观情感支持方面的潜力。", "motivation": "随着大型语言模型日益普及并被用作虚拟助手和伴侣，研究它们在人类互动中关键的同理心（包括认知同理心和情感同理心）能力变得重要。", "method": "研究使用标准化心理测试，评估了多个人类受试者（包括心理学学生）、小型语言模型（SLMs）和大型语言模型（LLMs）的认知同理心和情感同理心水平。", "result": "结果显示，LLMs在认知同理心任务上持续优于人类（包括心理学学生），但在情感同理心方面，所有语言模型（SLMs和LLMs）都显著低于人类参与者。", "conclusion": "研究表明语言模型在模拟认知同理心方面取得了快速进展，这使其在提供有效的虚拟陪伴和个性化情感支持方面具有巨大潜力。同时，其高认知同理心和低情感同理心的特点，使其能够提供客观且一致的情感支持，而不会面临情感疲劳或偏见的风险。"}}
{"id": "2508.08578", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.08578", "abs": "https://arxiv.org/abs/2508.08578", "authors": ["Ruohan Leng", "Linbin Huang", "Huanhai Xin", "Ping Ju", "Xiongfei Wang", "Eduardo Prieto-Araujo", "Florian Dörfler"], "title": "DeePConverter: A Data-Driven Optimal Control Architecture for Grid-Connected Power Converters", "comment": null, "summary": "Grid-connected power converters are ubiquitous in modern power systems,\nacting as grid interfaces of renewable energy sources, energy storage systems,\nelectric vehicles, high-voltage DC systems, etc. Conventionally, power\nconverters use multiple PID regulators to achieve different control objectives\nsuch as grid synchronization and voltage/power regulations, where the PID\nparameters are usually tuned based on a presumed (and often overly-simplified)\npower grid model. However, this may lead to inferior performance or even\ninstabilities in practice, as the real power grid is highly complex, variable,\nand generally unknown. To tackle this problem, we employ a data-enabled\npredictive control (DeePC) to perform data-driven, optimal, and robust control\nfor power converters. We call the converters that are operated in this way\n\\textit{DeePConverters}. A DeePConverter can implicitly perceive the\ncharacteristics of the power grid from data and adjust its control strategy to\nachieve optimal and robust performance. We present the modular configurations,\ngeneralized structure, control behavior specification, detailed implementation,\nand computation of DeePConverters. High-fidelity simulations and\nhardware-in-the-loop (HIL) tests are provided to validate the effectiveness of\nDeePConverters.", "AI": {"tldr": "本文提出了一种名为DeePConverter的电网并网变流器控制方法，它利用数据驱动的预测控制（DeePC）来替代传统的PID控制，以实现对复杂电网的鲁棒和最优控制。", "motivation": "传统的并网变流器使用PID控制器，其参数通常基于简化且不准确的电网模型进行调整。然而，实际电网复杂、多变且未知，这可能导致控制性能不佳甚至不稳定。", "method": "本文采用数据驱动的预测控制（DeePC）方法来控制并网变流器，称之为DeePConverters。DeePConverter通过数据隐式感知电网特性，并调整控制策略以实现最优和鲁棒性能。文章详细介绍了其模块化配置、通用结构、控制行为规范、具体实现和计算。", "result": "DeePConverters能够实现电网并网变流器的最优和鲁棒控制，并通过高保真仿真和硬件在环（HIL）测试验证了其有效性。", "conclusion": "数据驱动的预测控制（DeePC）为并网变流器提供了一种有效且鲁棒的控制方案，能够克服传统PID控制在复杂电网环境下的局限性，实现更好的性能。"}}
{"id": "2508.08309", "categories": ["eess.IV", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.08309", "abs": "https://arxiv.org/abs/2508.08309", "authors": ["Conor Rowan", "Sumedh Soman", "John A. Evans"], "title": "Variational volume reconstruction with the Deep Ritz Method", "comment": null, "summary": "We present a novel approach to variational volume reconstruction from sparse,\nnoisy slice data using the Deep Ritz method. Motivated by biomedical imaging\napplications such as MRI-based slice-to-volume reconstruction (SVR), our\napproach addresses three key challenges: (i) the reliance on image segmentation\nto extract boundaries from noisy grayscale slice images, (ii) the need to\nreconstruct volumes from a limited number of slice planes, and (iii) the\ncomputational expense of traditional mesh-based methods. We formulate a\nvariational objective that combines a regression loss designed to avoid image\nsegmentation by operating on noisy slice data directly with a modified\nCahn-Hilliard energy incorporating anisotropic diffusion to regularize the\nreconstructed geometry. We discretize the phase field with a neural network,\napproximate the objective at each optimization step with Monte Carlo\nintegration, and use ADAM to find the minimum of the approximated variational\nobjective. While the stochastic integration may not yield the true solution to\nthe variational problem, we demonstrate that our method reliably produces\nhigh-quality reconstructed volumes in a matter of seconds, even when the slice\ndata is sparse and noisy.", "AI": {"tldr": "本文提出一种基于深度Ritz方法的新型变分体积重建方法，可从稀疏、嘈杂的切片数据中高效重建高质量体积，无需图像分割。", "motivation": "该研究的动机源于生物医学成像应用（如基于MRI的切片到体积重建SVR），旨在解决三个关键挑战：(i) 依赖图像分割从嘈杂灰度切片中提取边界，(ii) 需要从有限的切片平面重建体积，以及(iii) 传统基于网格方法的计算开销大。", "method": "该方法构建了一个变分目标函数，结合了：(i) 直接作用于嘈杂切片数据以避免图像分割的回归损失，以及(ii) 引入各向异性扩散以正则化重建几何形状的修改版Cahn-Hilliard能量。它使用神经网络离散化相场（深度Ritz方法），通过蒙特卡洛积分在每个优化步骤近似目标函数，并使用ADAM优化器寻找近似变分目标的最小值。", "result": "尽管随机积分可能无法得到变分问题的真实解，但该方法能够可靠地在几秒钟内生成高质量的重建体积，即使切片数据稀疏且嘈杂。", "conclusion": "该方法克服了传统体积重建的局限性，提供了一种快速、高效且无需图像分割的解决方案，适用于处理稀疏和嘈杂的切片数据，在生物医学成像等领域具有应用潜力。"}}
{"id": "2508.08272", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.08272", "abs": "https://arxiv.org/abs/2508.08272", "authors": ["Tadej Škvorc", "Nikola Ivačič", "Sebastjan Hribar", "Marko Robnik-Šikonja"], "title": "Real-time News Story Identification", "comment": null, "summary": "To improve the reading experience, many news sites organize news into topical\ncollections, called stories. In this work, we present an approach for\nimplementing real-time story identification for a news monitoring system that\nautomatically collects news articles as they appear online and processes them\nin various ways. Story identification aims to assign each news article to a\nspecific story that the article is covering. The process is similar to text\nclustering and topic modeling, but requires that articles be grouped based on\nparticular events, places, and people, rather than general text similarity (as\nin clustering) or general (predefined) topics (as in topic modeling). We\npresent an approach to story identification that is capable of functioning in\nreal time, assigning articles to stories as they are published online. In the\nproposed approach, we combine text representation techniques, clustering\nalgorithms, and online topic modeling methods. We combine various text\nrepresentation methods to extract specific events and named entities necessary\nfor story identification, showing that a mixture of online topic-modeling\napproaches such as BERTopic, DBStream, and TextClust can be adapted for story\ndiscovery. We evaluate our approach on a news dataset from Slovene media\ncovering a period of 1 month. We show that our real-time approach produces\nsensible results as judged by human evaluators.", "AI": {"tldr": "本文提出了一种实时新闻故事识别方法，旨在将在线新闻文章自动归类到特定的事件故事中，以提升用户阅读体验。", "motivation": "新闻网站通常将新闻组织成专题故事以改善阅读体验。现有的文本聚类和主题建模方法不足以根据特定事件、地点和人物对文章进行分组，而新闻故事识别需要这种更细粒度的分组。因此，需要一种能够实时处理并准确识别新闻故事的方法。", "method": "该方法结合了多种文本表示技术、聚类算法和在线主题建模方法（如BERTopic、DBStream和TextClust），以提取故事识别所需的特定事件和命名实体。该系统能够在文章发布时实时将其分配给相应的故事。", "result": "该实时故事识别方法在斯洛文尼亚媒体的一个月新闻数据集上进行了评估，结果表明其产生了人类评估者认为合理的、有意义的结果。", "conclusion": "所提出的实时新闻故事识别方法是有效的，能够根据特定事件、地点和人物将新闻文章分组，并产生高质量的结果，从而改善新闻监控系统的功能和用户体验。"}}
{"id": "2508.08642", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.08642", "abs": "https://arxiv.org/abs/2508.08642", "authors": ["Tianyi Hu", "Tianyuan Du", "Zhehan Qu", "Maria Gorlatova"], "title": "XR Reality Check: What Commercial Devices Deliver for Spatial Tracking", "comment": null, "summary": "Inaccurate spatial tracking in extended reality (XR) devices leads to virtual\nobject jitter, misalignment, and user discomfort, fundamentally limiting\nimmersive experiences and natural interactions. In this work, we introduce a\nnovel testbed that enables simultaneous, synchronized evaluation of multiple XR\ndevices under identical environmental and kinematic conditions. Leveraging this\nplatform, we present the first comprehensive empirical benchmarking of five\nstate-of-the-art XR devices across 16 diverse scenarios. Our results reveal\nsubstantial intra-device performance variation, with individual devices\nexhibiting up to 101\\% increases in error when operating in featureless\nenvironments. We also demonstrate that tracking accuracy strongly correlates\nwith visual conditions and motion dynamics. We also observe significant\ninter-device disparities, with performance differences of up to 2.8$\\times$,\nwhich are closely linked to hardware specifications such as sensor\nconfigurations and dedicated processing units. Finally, we explore the\nfeasibility of substituting a motion capture system with the Apple Vision Pro\nas a practical ground truth reference. While the Apple Vision Pro delivers\nhighly accurate relative pose error estimates ($R^2 = 0.830$), its absolute\npose error estimation remains limited ($R^2 = 0.387$), highlighting both its\npotential and its constraints for rigorous XR evaluation. This work establishes\nthe first standardized framework for comparative XR tracking evaluation,\nproviding the research community with reproducible methodologies, comprehensive\nbenchmark datasets, and open-source tools that enable systematic analysis of\ntracking performance across devices and conditions, thereby accelerating the\ndevelopment of more robust spatial sensing technologies for XR systems.", "AI": {"tldr": "本文介绍了一种新颖的测试平台，用于同步评估多款XR设备的空间追踪性能，揭示了设备内部和设备间的显著性能差异，并探讨了Apple Vision Pro作为地面真值的可行性，为XR追踪评估建立了首个标准化框架。", "motivation": "XR设备中不准确的空间追踪会导致虚拟物体抖动、错位和用户不适，严重限制了沉浸式体验和自然交互。缺乏标准化的评估方法和综合基准测试是当前挑战。", "method": "研究者开发了一个新颖的测试平台，能够同时、同步地在相同环境和运动条件下评估多台XR设备。利用该平台，对五款主流XR设备在16种不同场景下进行了首次全面的实证基准测试。此外，还探索了使用Apple Vision Pro替代光学动作捕捉系统作为地面真值的可行性。", "result": "研究发现设备内部性能存在显著差异，在无特征环境中误差可增加高达101%。追踪精度与视觉条件和运动动态强相关。设备间性能差异高达2.8倍，这与传感器配置和专用处理单元等硬件规格密切相关。Apple Vision Pro在相对姿态误差估计方面表现出高精度（R²=0.830），但在绝对姿态误差估计方面仍有限制（R²=0.387）。", "conclusion": "本研究建立了首个用于比较XR追踪评估的标准化框架，为研究社区提供了可复现的方法、全面的基准数据集和开源工具，以系统分析不同设备和条件下的追踪性能，从而加速XR系统更鲁棒空间传感技术的发展。"}}
{"id": "2508.08431", "categories": ["eess.IV", "cs.CV", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.08431", "abs": "https://arxiv.org/abs/2508.08431", "authors": ["Praveen Sumanasekara", "Athulya Ratnayake", "Buddhi Wijenayake", "Keshawa Ratnayake", "Roshan Godaliyadda", "Parakrama Ekanayake", "Vijitha Herath"], "title": "Preprocessing Algorithm Leveraging Geometric Modeling for Scale Correction in Hyperspectral Images for Improved Unmixing Performance", "comment": "20 pages, 17 figures", "summary": "Spectral variability significantly impacts the accuracy and convergence of\nhyperspectral unmixing algorithms. While many methods address complex spectral\nvariability, large-scale variations in spectral signature scale caused by\nfactors such as topography, illumination, and shadowing remain a major\nchallenge. These variations often degrade unmixing performance and complicate\nmodel fitting. In this paper, we propose a novel preprocessing algorithm that\ncorrects scale-induced spectral variability prior to unmixing. By isolating and\ncompensating for these large-scale multiplicative effects, the algorithm\nprovides a cleaner input, enabling unmixing methods to focus more effectively\non modeling nonlinear spectral variability and abundance estimation. We present\na rigorous mathematical framework to describe scale variability and extensive\nexperimental validation of the proposed algorithm. Furthermore, the algorithm's\nimpact is evaluated across a broad spectrum of state-of-the-art unmixing\nalgorithms on two synthetic and two real hyperspectral datasets. The proposed\npreprocessing step consistently improves the performance of these algorithms,\nincluding those specifically designed to handle spectral variability, with\nerror reductions close to 50% in many cases. This demonstrates that scale\ncorrection acts as a complementary step, facilitating more accurate unmixing by\nexisting methods. The algorithm's generality and significant impact highlight\nits potential as a key component in practical hyperspectral unmixing pipelines.\nThe implementation code will be made publicly available upon publication.", "AI": {"tldr": "本文提出了一种新颖的预处理算法，用于校正高光谱解混中由尺度引起的谱变异性，显著提升了现有解混算法的性能。", "motivation": "高光谱解混中，由地形、光照、阴影等因素引起的大尺度谱签名尺度变化是主要挑战，严重影响解混精度和模型拟合。", "method": "开发了一种预处理算法，通过隔离和补偿大尺度乘性效应来校正尺度引起的谱变异性。提供了严格的数学框架。", "result": "该算法在合成和真实高光谱数据集上，一致性地提升了多种现有解混算法的性能，包括那些专门处理谱变异性的算法，误差降低接近50%。", "conclusion": "尺度校正作为一种通用且互补的预处理步骤，能显著提高现有解混方法的精度，有望成为实用高光谱解混流程中的关键组成部分。"}}
{"id": "2508.08258", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.08258", "abs": "https://arxiv.org/abs/2508.08258", "authors": ["Gerald Brantner"], "title": "Humanoid Robot Acrobatics Utilizing Complete Articulated Rigid Body Dynamics", "comment": null, "summary": "Endowing humanoid robots with the ability to perform highly dynamic motions\nakin to human-level acrobatics has been a long-standing challenge. Successfully\nperforming these maneuvers requires close consideration of the underlying\nphysics in both trajectory optimization for planning and control during\nexecution. This is particularly challenging due to humanoids' high\ndegree-of-freedom count and associated exponentially scaling complexities,\nwhich makes planning on the explicit equations of motion intractable. Typical\nworkarounds include linearization methods and model approximations. However,\nneither are sufficient because they produce degraded performance on the true\nrobotic system. This paper presents a control architecture comprising\ntrajectory optimization and whole-body control, intermediated by a matching\nmodel abstraction, that enables the execution of acrobatic maneuvers, including\nconstraint and posture behaviors, conditioned on the unabbreviated equations of\nmotion of the articulated rigid body model. A review of underlying modeling and\ncontrol methods is given, followed by implementation details including model\nabstraction, trajectory optimization and whole-body controller. The system's\neffectiveness is analyzed in simulation.", "AI": {"tldr": "一种使人形机器人执行高动态杂技动作的控制架构。", "motivation": "赋予人形机器人类人杂技动作能力是一个长期挑战，因其高自由度导致规划复杂且现有线性化和模型近似方法在真实机器人系统上性能退化。", "method": "提出一种包含轨迹优化和全身控制的控制架构，通过匹配模型抽象作为中介，使机器人能够基于未简化的关节刚体模型运动方程执行杂技动作，包括约束和姿态行为。", "result": "在仿真中分析并验证了所提系统的有效性。", "conclusion": "所提出的控制架构能够使人形机器人在考虑完整运动方程的情况下执行复杂的杂技动作。"}}
{"id": "2508.08317", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08317", "abs": "https://arxiv.org/abs/2508.08317", "authors": ["Saptarshi Banerjee", "Tausif Mallick", "Amlan Chakroborty", "Himadri Nath Saha", "Nityananda T. Takur"], "title": "Evaluation of State-of-the-Art Deep Learning Techniques for Plant Disease and Pest Detection", "comment": "AI/ML, Computer Vision", "summary": "Addressing plant diseases and pests is critical for enhancing crop production\nand preventing economic losses. Recent advances in artificial intelligence\n(AI), machine learning (ML), and deep learning (DL) have significantly improved\nthe precision and efficiency of detection methods, surpassing the limitations\nof manual identification. This study reviews modern computer-based techniques\nfor detecting plant diseases and pests from images, including recent AI\ndevelopments. The methodologies are organized into five categories:\nhyperspectral imaging, non-visualization techniques, visualization approaches,\nmodified deep learning architectures, and transformer models. This structured\ntaxonomy provides researchers with detailed, actionable insights for selecting\nadvanced state-of-the-art detection methods. A comprehensive survey of recent\nwork and comparative studies demonstrates the consistent superiority of modern\nAI-based approaches, which often outperform older image analysis methods in\nspeed and accuracy. In particular, vision transformers such as the Hierarchical\nVision Transformer (HvT) have shown accuracy exceeding 99.3% in plant disease\ndetection, outperforming architectures like MobileNetV3. The study concludes by\ndiscussing system design challenges, proposing solutions, and outlining\npromising directions for future research.", "AI": {"tldr": "本文综述了利用人工智能、机器学习和深度学习技术，通过图像识别方法检测植物病虫害的最新进展，并对不同方法进行了分类和比较。", "motivation": "植物病虫害严重影响作物产量并造成经济损失，传统人工识别方法存在局限性，需要更精确高效的检测技术。", "method": "研究将现代基于计算机的植物病虫害图像检测技术分为五类：高光谱成像、非可视化技术、可视化方法、改进的深度学习架构和Transformer模型，并对近期工作和比较研究进行了综述。", "result": "现代基于AI的方法在速度和准确性上持续优于旧的图像分析方法。特别是，视觉Transformer模型（如分层视觉Transformer HvT）在植物病害检测中准确率超过99.3%，优于MobileNetV3等架构。", "conclusion": "AI驱动的检测方法表现出卓越的性能，尤其以视觉Transformer为代表。研究还讨论了系统设计挑战，提出了解决方案，并展望了未来的研究方向。"}}
{"id": "2508.08293", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08293", "abs": "https://arxiv.org/abs/2508.08293", "authors": ["Sridhar Mahadevan"], "title": "Topos Theory for Generative AI and LLMs", "comment": "30 pages", "summary": "We propose the design of novel categorical generative AI architectures\n(GAIAs) using topos theory, a type of category that is ``set-like\": a topos has\nall (co)limits, is Cartesian closed, and has a subobject classifier. Previous\ntheoretical results on the Transformer model have shown that it is a universal\nsequence-to-sequence function approximator, and dense in the space of all\ncontinuous functions with compact support on the Euclidean space of embeddings\nof tokens. Building on this theoretical result, we explore novel architectures\nfor LLMs that exploit the property that the category of LLMs, viewed as\nfunctions, forms a topos. Previous studies of large language models (LLMs) have\nfocused on daisy-chained linear architectures or mixture-of-experts. In this\npaper, we use universal constructions in category theory to construct novel LLM\narchitectures based on new types of compositional structures. In particular,\nthese new compositional structures are derived from universal properties of LLM\ncategories, and include pullback, pushout, (co) equalizers, exponential\nobjects, and subobject classifiers. We theoretically validate these new\ncompositional structures by showing that the category of LLMs is (co)complete,\nmeaning that all diagrams have solutions in the form of (co)limits. Building on\nthis completeness result, we then show that the category of LLMs forms a topos,\na ``set-like\" category, which requires showing the existence of exponential\nobjects as well as subobject classifiers. We use a functorial characterization\nof backpropagation to define a potential implementation of an LLM topos\narchitecture.", "AI": {"tldr": "本文提出使用拓扑斯理论（一种范畴论）设计新型生成式AI（GAIA）架构，特别是针对大型语言模型（LLM），旨在构建基于通用范畴论构造的更具组合性的LLM架构。", "motivation": "现有LLM架构多为线性或专家混合模型，而Transformer已被证明是通用函数逼近器。本文旨在利用LLM作为函数形成一个拓扑斯的特性，探索基于范畴论的全新、更具组合性的LLM架构设计，超越传统的链式结构。", "method": "本文利用范畴论中的普适构造（如拉回、推出、等化子、指数对象、子对象分类器）来构建新型LLM组合结构。通过理论验证，证明LLM的范畴是完备的（所有图都有极限/上极限解），并进一步证明该范畴形成一个拓扑斯（通过展示指数对象和子对象分类器的存在）。最后，利用反向传播的函子化特征定义了LLM拓扑斯架构的潜在实现。", "result": "本文理论上验证了基于LLM范畴普适性质的新型组合结构。主要结果表明，LLM的范畴是（上）完备的，并且形成一个拓扑斯（“类集合”范畴），这意味着它具有所有必要的范畴论性质以支持复杂的组合设计。", "conclusion": "通过将LLM视为函数并证明其范畴形成一个拓扑斯，本文为设计具有更丰富组合结构的新型LLM架构提供了理论基础和方法，超越了当前主流的线性或专家混合模型。"}}
{"id": "2508.08273", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.08273", "abs": "https://arxiv.org/abs/2508.08273", "authors": ["Kristian Miok", "Blaz Škrlj", "Daniela Zaharie", "Marko Robnik Šikonja"], "title": "TT-XAI: Trustworthy Clinical Text Explanations via Keyword Distillation and LLM Reasoning", "comment": null, "summary": "Clinical language models often struggle to provide trustworthy predictions\nand explanations when applied to lengthy, unstructured electronic health\nrecords (EHRs). This work introduces TT-XAI, a lightweight and effective\nframework that improves both classification performance and interpretability\nthrough domain-aware keyword distillation and reasoning with large language\nmodels (LLMs). First, we demonstrate that distilling raw discharge notes into\nconcise keyword representations significantly enhances BERT classifier\nperformance and improves local explanation fidelity via a focused variant of\nLIME. Second, we generate chain-of-thought clinical explanations using\nkeyword-guided prompts to steer LLMs, producing more concise and clinically\nrelevant reasoning. We evaluate explanation quality using deletion-based\nfidelity metrics, self-assessment via LLaMA-3 scoring, and a blinded human\nstudy with domain experts. All evaluation modalities consistently favor the\nkeyword-augmented method, confirming that distillation enhances both machine\nand human interpretability. TT-XAI offers a scalable pathway toward\ntrustworthy, auditable AI in clinical decision support.", "AI": {"tldr": "TT-XAI框架通过领域感知关键词提炼和LLM推理，提高了临床语言模型在处理冗长EHR时的分类性能和可解释性，使其更值得信赖。", "motivation": "临床语言模型在应用于冗长、非结构化的电子健康记录（EHRs）时，难以提供可信的预测和解释。", "method": "1. 将原始出院记录提炼为简洁的关键词表示，以增强BERT分类器性能并提高LIME局部解释的保真度。2. 使用关键词引导的提示来指导大型语言模型（LLMs），生成链式思维的临床解释，使其更简洁、临床相关。3. 通过基于删除的保真度指标、LLaMA-3自评估和盲法人类专家研究来评估解释质量。", "result": "关键词提炼显著提升了BERT分类器性能，改进了LIME局部解释的保真度。使用关键词引导的LLM生成了更简洁且临床相关的推理。所有评估方式（保真度、LLaMA-3评分、人类研究）均一致表明关键词增强方法更优，证实提炼增强了机器和人类的可解释性。", "conclusion": "TT-XAI通过关键词提炼和LLM推理，为临床决策支持中的可信、可审计AI提供了一条可扩展的途径，同时提升了性能和可解释性。"}}
{"id": "2508.08725", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.08725", "abs": "https://arxiv.org/abs/2508.08725", "authors": ["Zeinab Hijazi", "Fatima Bzeih", "Ali Ibrahim"], "title": "Architecture and FPGA Implementation of Digital Time-to-Digital Converter for Sensing Applications", "comment": null, "summary": "Many application domains face the challenges of high-power consumption and\nhigh computational demands, especially with the advancement in embedded machine\nlearning and edge computing. Designing application-specific circuits is crucial\nto reducing hardware complexity and power consumption. In these perspectives,\nthis paper presents the design of a Digital Time-to-Digital converter (DTDC)\nbased on multiple delay line topologies. The DTDC is implemented in VHDL for\nthe Xilinx Artix-7 AC701 FPGA device. Simulation results demonstrate the\neffectiveness of the circuit in converting the input period along a wide range\nup to 1ps. The designed circuit is implemented with less than 1% of the\nresource utilization on the target FPGA device.", "AI": {"tldr": "本文设计了一种基于多延迟线拓扑结构的数字时间-数字转换器（DTDC），用于解决嵌入式机器学习和边缘计算中高功耗和高计算需求问题，并在FPGA上实现了低资源占用和高精度转换。", "motivation": "嵌入式机器学习和边缘计算领域面临高功耗和高计算需求挑战，迫切需要设计专用电路来降低硬件复杂性和功耗。", "method": "设计了一种基于多延迟线拓扑结构的数字时间-数字转换器（DTDC），使用VHDL语言在Xilinx Artix-7 AC701 FPGA设备上实现。", "result": "仿真结果表明该电路能有效地将输入周期转换至1ps的精度，且在目标FPGA设备上资源利用率低于1%。", "conclusion": "所设计的DTDC电路在性能（转换精度）和资源效率方面表现出色，适用于需要低功耗和低复杂度的嵌入式应用。"}}
{"id": "2508.08854", "categories": ["eess.IV", "cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.08854", "abs": "https://arxiv.org/abs/2508.08854", "authors": ["Yingxue Pang", "Shijie Zhao", "Haiqiang Wang", "Gen Zhan", "Junlin Li", "Li Zhang"], "title": "Frequency-Assisted Adaptive Sharpening Scheme Considering Bitrate and Quality Tradeoff", "comment": null, "summary": "Sharpening is a widely adopted technique to improve video quality, which can\neffectively emphasize textures and alleviate blurring. However, increasing the\nsharpening level comes with a higher video bitrate, resulting in degraded\nQuality of Service (QoS). Furthermore, the video quality does not necessarily\nimprove with increasing sharpening levels, leading to issues such as\nover-sharpening. Clearly, it is essential to figure out how to boost video\nquality with a proper sharpening level while also controlling bandwidth costs\neffectively. This paper thus proposes a novel Frequency-assisted Sharpening\nlevel Prediction model (FreqSP). We first label each video with the sharpening\nlevel correlating to the optimal bitrate and quality tradeoff as ground truth.\nThen taking uncompressed source videos as inputs, the proposed FreqSP leverages\nintricate CNN features and high-frequency components to estimate the optimal\nsharpening level. Extensive experiments demonstrate the effectiveness of our\nmethod.", "AI": {"tldr": "本文提出了一种名为FreqSP的频率辅助锐化级别预测模型，旨在为视频找到最佳锐化级别，以平衡视频质量和带宽成本。", "motivation": "视频锐化虽能提升质量，但会增加比特率并可能导致过度锐化，从而降低服务质量。因此，如何在保证视频质量的同时有效控制带宽成本，找到合适的锐化级别至关重要。", "method": "首先，将每个视频标记上与最佳比特率和质量权衡相对应的锐化级别作为真值。然后，FreqSP模型以未压缩的源视频作为输入，利用复杂的CNN特征和高频分量来估计最佳锐化级别。", "result": "大量的实验证明了该方法的有效性。", "conclusion": "FreqSP模型能够有效预测视频的最佳锐化级别，从而在提升视频质量的同时有效控制带宽成本。"}}
{"id": "2508.08259", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.08259", "abs": "https://arxiv.org/abs/2508.08259", "authors": ["Chun-Ming Yang", "Pranav A. Bhounsule"], "title": "Koopman Operator Based Linear Model Predictive Control for Quadruped Trotting", "comment": null, "summary": "Online optimal control of quadruped robots would enable them to adapt to\nvarying inputs and changing conditions in real time. A common way of achieving\nthis is linear model predictive control (LMPC), where a quadratic programming\n(QP) problem is formulated over a finite horizon with a quadratic cost and\nlinear constraints obtained by linearizing the equations of motion and solved\non the fly. However, the model linearization may lead to model inaccuracies. In\nthis paper, we use the Koopman operator to create a linear model of the\nquadrupedal system in high dimensional space which preserves the nonlinearity\nof the equations of motion. Then using LMPC, we demonstrate high fidelity\ntracking and disturbance rejection on a quadrupedal robot. This is the first\nwork that uses the Koopman operator theory for LMPC of quadrupedal locomotion.", "AI": {"tldr": "本文首次将Koopman算子理论应用于四足机器人运动的线性模型预测控制（LMPC），以在实时控制中保留非线性动力学，实现高精度跟踪和扰动抑制。", "motivation": "传统的四足机器人在线最优控制方法（如LMPC）通过线性化运动方程来简化问题，但这会导致模型不准确性，限制了机器人在变化输入和条件下的适应性。", "method": "利用Koopman算子将四足机器人系统映射到高维空间，在此空间中创建一个线性模型，该模型能够保留原始运动方程的非线性特性。然后，将此线性模型应用于LMPC框架，进行控制器的设计和求解。", "result": "在四足机器人上展示了高精度的轨迹跟踪能力和出色的扰动抑制性能。", "conclusion": "首次将Koopman算子理论应用于四足机器人运动的线性模型预测控制，成功克服了传统LMPC模型线性化的不足，为实时最优控制提供了新途径。"}}
{"id": "2508.08338", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.08338", "abs": "https://arxiv.org/abs/2508.08338", "authors": ["Yuqin He", "Tengfei Ma", "Chaoyi Li", "Pengsen Ma", "Hongxin Xiang", "Jianmin Wang", "Yiping Liu", "Bosheng Song", "Xiangxiang Zeng"], "title": "ImageDDI: Image-enhanced Molecular Motif Sequence Representation for Drug-Drug Interaction Prediction", "comment": "Accepted By Information Fusion", "summary": "To mitigate the potential adverse health effects of simultaneous multi-drug\nuse, including unexpected side effects and interactions, accurately identifying\nand predicting drug-drug interactions (DDIs) is considered a crucial task in\nthe field of deep learning. Although existing methods have demonstrated\npromising performance, they suffer from the bottleneck of limited functional\nmotif-based representation learning, as DDIs are fundamentally caused by motif\ninteractions rather than the overall drug structures. In this paper, we propose\nan Image-enhanced molecular motif sequence representation framework for\n\\textbf{DDI} prediction, called ImageDDI, which represents a pair of drugs from\nboth global and local structures. Specifically, ImageDDI tokenizes molecules\ninto functional motifs. To effectively represent a drug pair, their motifs are\ncombined into a single sequence and embedded using a transformer-based encoder,\nstarting from the local structure representation. By leveraging the\nassociations between drug pairs, ImageDDI further enhances the spatial\nrepresentation of molecules using global molecular image information (e.g.\ntexture, shadow, color, and planar spatial relationships). To integrate\nmolecular visual information into functional motif sequence, ImageDDI employs\nAdaptive Feature Fusion, enhancing the generalization of ImageDDI by\ndynamically adapting the fusion process of feature representations.\nExperimental results on widely used datasets demonstrate that ImageDDI\noutperforms state-of-the-art methods. Moreover, extensive experiments show that\nImageDDI achieved competitive performance in both 2D and 3D image-enhanced\nscenarios compared to other models.", "AI": {"tldr": "ImageDDI是一个用于DDI预测的深度学习框架，它通过将药物表示为功能性基序序列并结合全局分子图像信息，利用Transformer编码器和自适应特征融合来提高预测性能。", "motivation": "多药同时使用可能导致不良健康影响、意外副作用和相互作用，因此准确识别和预测药物-药物相互作用（DDI）至关重要。现有方法受限于基于功能基序的表示学习瓶颈，因为DDI根本上是由基序相互作用而非整体药物结构引起的。", "method": "ImageDDI框架将分子标记为功能性基序，将一对药物的基序组合成一个序列，并使用基于Transformer的编码器进行嵌入以表示局部结构。它进一步利用全局分子图像信息（如纹理、阴影、颜色和平面空间关系）增强分子的空间表示。为整合分子视觉信息与功能基序序列，ImageDDI采用自适应特征融合（Adaptive Feature Fusion）来动态调整特征表示的融合过程。", "result": "在广泛使用的数据集上，ImageDDI的实验结果优于现有最先进的方法。此外，在2D和3D图像增强场景中，ImageDDI与其他模型相比也取得了具有竞争力的性能。", "conclusion": "ImageDDI通过结合药物的局部功能基序序列表示和全局分子图像信息，有效地提高了药物-药物相互作用的预测准确性和泛化能力，为DDI预测提供了一种新的有效方法。"}}
{"id": "2508.08295", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08295", "abs": "https://arxiv.org/abs/2508.08295", "authors": ["Sridhar Mahadevan"], "title": "Topos Causal Models", "comment": "31 pages", "summary": "We propose topos causal models (TCMs), a novel class of causal models that\nexploit the key properties of a topos category: they are (co)complete, meaning\nall (co)limits exist, they admit a subobject classifier, and allow exponential\nobjects. The main goal of this paper is to show that these properties are\ncentral to many applications in causal inference. For example, subobject\nclassifiers allow a categorical formulation of causal intervention, which\ncreates sub-models. Limits and colimits allow causal diagrams of arbitrary\ncomplexity to be ``solved\", using a novel interpretation of causal\napproximation. Exponential objects enable reasoning about equivalence classes\nof operations on causal models, such as covered edge reversal and causal\nhomotopy. Analogous to structural causal models (SCMs), TCMs are defined by a\ncollection of functions, each defining a ``local autonomous\" causal mechanism\nthat assemble to induce a unique global function from exogenous to endogenous\nvariables. Since the category of TCMs is (co)complete, which we prove in this\npaper, every causal diagram has a ``solution\" in the form of a (co)limit: this\nimplies that any arbitrary causal model can be ``approximated\" by some global\nfunction with respect to the morphisms going into or out of the diagram.\nNatural transformations are crucial in measuring the quality of approximation.\nIn addition, we show that causal interventions are modeled by subobject\nclassifiers: any sub-model is defined by a monic arrow into its parent model.\nExponential objects permit reasoning about entire classes of causal\nequivalences and interventions. Finally, as TCMs form a topos, they admit an\ninternal logic defined as a Mitchell-Benabou language with an associated\nKripke-Joyal semantics. We show how to reason about causal models in TCMs using\nthis internal logic.", "AI": {"tldr": "本文提出了一种新型因果模型——拓扑因果模型（TCMs），它利用拓扑斯范畴的关键性质（完备性、子对象分类器、指数对象）来形式化和解决因果推断中的干预、复杂图求解和因果等价类问题。", "motivation": "研究动机是利用拓扑斯范畴的数学性质，为因果推断提供一个更通用、更严格的框架，以应对现有结构因果模型（SCMs）在处理复杂因果图、干预建模以及因果等价性推理方面的挑战。", "method": "TCMs被定义为一组函数，模仿SCMs的局部自主机制。研究方法包括：1) 证明TCMs范畴是（共）完备的；2) 利用子对象分类器对因果干预进行范畴化建模，生成子模型；3) 通过极限和余极限解释因果近似，从而“求解”任意复杂度的因果图；4) 利用指数对象推导因果模型操作的等价类（如覆盖边反转和因果同伦）；5) 使用自然变换衡量近似质量；6) 引入Mitchell-Benabou语言和Kripke-Joyal语义的内部逻辑，用于在TCMs中进行因果推理。", "result": "主要结果包括：1) 提出了TCMs这一新型因果模型类；2) 证明了TCMs的范畴是（共）完备的；3) 展示了子对象分类器如何实现因果干预的范畴化表达；4) 揭示了极限和余极限如何通过因果近似来“求解”任意复杂度的因果图；5) 阐明了指数对象如何支持对因果等价类操作的推理；6) 引入了TCMs的内部逻辑，为因果推理提供了新工具。", "conclusion": "结论是拓扑因果模型（TCMs）提供了一个强大的、统一的范畴学框架，能够有效处理因果推断中的关键问题，如干预、复杂模型近似和因果等价性。其内部逻辑进一步增强了在这一框架内进行因果推理的能力。"}}
{"id": "2508.08274", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.08274", "abs": "https://arxiv.org/abs/2508.08274", "authors": ["Roberto Labadie-Tamayo", "Djordje Slijepčević", "Xihui Chen", "Adrian Jaques Böck", "Andreas Babic", "Liz Freimann", "Christiane Atzmüller Matthias Zeppelzauer"], "title": "Distilling Knowledge from Large Language Models: A Concept Bottleneck Model for Hate and Counter Speech Recognition", "comment": "33 pages, 10 figures, This is a preprint of a manuscript accepted for\n  publication in Information Processing & Management (Elsevier)", "summary": "The rapid increase in hate speech on social media has exposed an\nunprecedented impact on society, making automated methods for detecting such\ncontent important. Unlike prior black-box models, we propose a novel\ntransparent method for automated hate and counter speech recognition, i.e.,\n\"Speech Concept Bottleneck Model\" (SCBM), using adjectives as\nhuman-interpretable bottleneck concepts. SCBM leverages large language models\n(LLMs) to map input texts to an abstract adjective-based representation, which\nis then sent to a light-weight classifier for downstream tasks. Across five\nbenchmark datasets spanning multiple languages and platforms (e.g., Twitter,\nReddit, YouTube), SCBM achieves an average macro-F1 score of 0.69 which\noutperforms the most recently reported results from the literature on four out\nof five datasets. Aside from high recognition accuracy, SCBM provides a high\nlevel of both local and global interpretability. Furthermore, fusing our\nadjective-based concept representation with transformer embeddings, leads to a\n1.8% performance increase on average across all datasets, showing that the\nproposed representation captures complementary information. Our results\ndemonstrate that adjective-based concept representations can serve as compact,\ninterpretable, and effective encodings for hate and counter speech recognition.\nWith adapted adjectives, our method can also be applied to other NLP tasks.", "AI": {"tldr": "本文提出了一种名为“语音概念瓶颈模型”（SCBM）的透明方法，利用形容词作为可解释的瓶颈概念，用于自动化仇恨言论和反驳言论识别，并在多个数据集上取得了优异的性能和高可解释性。", "motivation": "社交媒体上仇恨言论的迅速增长对社会产生了前所未有的影响，使得自动化检测此类内容变得重要。现有的模型多为“黑箱”模型，缺乏透明度。", "method": "提出了一种名为“语音概念瓶颈模型”（SCBM）的透明方法。SCBM利用大型语言模型（LLMs）将输入文本映射到基于形容词的抽象表示，该表示随后被送入轻量级分类器进行下游任务。研究在涵盖多种语言和平台的五个基准数据集上进行了评估。此外，还将形容词概念表示与Transformer嵌入融合以探索互补信息。", "result": "SCBM在五个基准数据集上取得了平均0.69的Macro-F1分数，超越了文献中最近报告的成果（在五个数据集中有四个数据集表现更优）。SCBM提供了高水平的局部和全局可解释性。将形容词概念表示与Transformer嵌入融合后，性能平均提升了1.8%，表明所提出的表示捕获了互补信息。结果表明，基于形容词的概念表示可以作为仇恨言论和反驳言论识别的紧凑、可解释和有效的编码。", "conclusion": "基于形容词的概念表示可以作为仇恨言论和反驳言论识别的紧凑、可解释和有效的编码。通过调整形容词，该方法也可以应用于其他自然语言处理任务。"}}
{"id": "2508.09106", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.09106", "abs": "https://arxiv.org/abs/2508.09106", "authors": ["Ninad Gaikwad", "Anamika Dubey"], "title": "Smart Residential Community Simulator for Developing and Benchmarking Energy Management Systems", "comment": "This manuscript is an extended version of our paper accepted at the\n  IEEE SmartGridComm 2025", "summary": "Home Energy Management Systems (HEMS) are being actively developed for both\nindividual houses and communities to support demand response in on-grid\noperation, and ensure resilience during off-grid scenarios. However, most\nsimulators used for closed-loop HEMS testing are tailored to a specific\ndistributed energy resource (DER) configuration with a fixed number of houses,\nlimiting flexibility and scalability. This leads to additional development\nefforts to support diverse DER configurations across any number of houses and\nto integrate appropriate weather and load data pipelines. To address these\nlimitations, we present a scalable simulator capable of modeling any number of\nhouses in both on-grid and off-grid modes as a Gymnasium environment. Each\nhouse can have a unique DER configuration - Rooftop Solar Photovoltaics (PV),\nBattery-only, PV-only, or no DER - and includes models for air-conditioning and\neight grouped circuit-level loads. The simulator integrates National Solar\nRadiation Database (NSRDB) weather and Pecan Street load datasets, supports\nthree default controllers (two for off-grid, and one for on-grid scenarios),\nand includes performance metrics and visualization tools. We demonstrate its\nflexibility through simulations on individual houses and a four-house community\nwith heterogeneous DERs, benchmarking the controllers across built-in metrics\nand computation time. The results highlight the simulator's capability to\nsystematically evaluate control policy performance under varying system\nconfigurations.", "AI": {"tldr": "本文提出了一个可扩展的家庭能源管理系统（HEMS）模拟器，能够在并网和离网模式下模拟任意数量的房屋，支持多样化的分布式能源（DER）配置，并集成了真实天气和负荷数据，用于灵活的控制策略评估。", "motivation": "现有的HEMS模拟器通常针对特定的分布式能源配置和固定数量的房屋进行定制，缺乏灵活性和可扩展性，导致在支持不同配置和集成数据时需要额外的开发工作。", "method": "开发了一个可扩展的HEMS模拟器，将其作为Gymnasium环境。该模拟器能够模拟任意数量的房屋，支持每栋房屋独特的DER配置（屋顶光伏、仅电池、仅光伏或无DER），包含空调和八组电路级负荷模型。它集成了NSRDB天气数据和Pecan Street负荷数据集，并支持三种默认控制器（两种用于离网，一种用于并网场景），同时提供性能指标和可视化工具。", "result": "通过对单个房屋和具有异构DER的四户社区进行模拟，展示了模拟器的灵活性。对内置控制器进行了基准测试，评估了它们的性能和计算时间。结果表明，该模拟器能够系统地评估不同系统配置下控制策略的性能。", "conclusion": "该模拟器成功解决了现有HEMS测试工具的局限性，提供了一个高度灵活和可扩展的平台，用于在各种系统配置下开发和评估HEMS控制策略。"}}
{"id": "2508.09068", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09068", "abs": "https://arxiv.org/abs/2508.09068", "authors": ["Conall Daly", "Anil Kokaram"], "title": "A new dataset and comparison for multi-camera frame synthesis", "comment": "SPIE2025 - Applications of Digital Image Processing XLVIII accepted\n  manuscript", "summary": "Many methods exist for frame synthesis in image sequences but can be broadly\ncategorised into frame interpolation and view synthesis techniques.\nFundamentally, both frame interpolation and view synthesis tackle the same\ntask, interpolating a frame given surrounding frames in time or space. However,\nmost frame interpolation datasets focus on temporal aspects with single cameras\nmoving through time and space, while view synthesis datasets are typically\nbiased toward stereoscopic depth estimation use cases. This makes direct\ncomparison between view synthesis and frame interpolation methods challenging.\nIn this paper, we develop a novel multi-camera dataset using a custom-built\ndense linear camera array to enable fair comparison between these approaches.\nWe evaluate classical and deep learning frame interpolators against a view\nsynthesis method (3D Gaussian Splatting) for the task of view in-betweening.\nOur results reveal that deep learning methods do not significantly outperform\nclassical methods on real image data, with 3D Gaussian Splatting actually\nunderperforming frame interpolators by as much as 3.5 dB PSNR. However, in\nsynthetic scenes, the situation reverses -- 3D Gaussian Splatting outperforms\nframe interpolation algorithms by almost 5 dB PSNR at a 95% confidence level.", "AI": {"tldr": "本文提出了一个新的多相机数据集，用于公平比较帧插值和视图合成方法。研究发现，在真实数据上，深度学习方法并未显著优于传统方法，3D Gaussian Splatting甚至表现不佳；但在合成场景中，3D Gaussian Splatting则表现优异。", "motivation": "现有的帧插值和视图合成数据集在时间或空间维度上存在偏差，导致难以直接公平地比较这两种技术。帧插值数据集侧重时间维度，而视图合成数据集偏向立体深度估计。", "method": "开发了一个定制的密集线性相机阵列，构建了一个新型多相机数据集。使用该数据集，评估了经典和深度学习帧插值器，并与一种视图合成方法（3D Gaussian Splatting）在视图中间帧生成任务上进行比较。", "result": "在真实图像数据上，深度学习方法并未显著优于经典方法，且3D Gaussian Splatting比帧插值器表现差高达3.5 dB PSNR。然而，在合成场景中，情况逆转，3D Gaussian Splatting比帧插值算法高出近5 dB PSNR，置信水平为95%。", "conclusion": "研究表明，帧插值和视图合成方法在真实和合成数据上的表现存在显著差异，这提示了在评估这些方法时需要考虑数据集的特性，并对现有基准的通用性提出了疑问。"}}
{"id": "2508.08264", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.08264", "abs": "https://arxiv.org/abs/2508.08264", "authors": ["Hadush Hailu", "Bruk Gebregziabher", "Prudhvi Raj"], "title": "Forecast-Driven MPC for Decentralized Multi-Robot Collision Avoidance", "comment": "8 pages, 4 figures and 1 table", "summary": "The Iterative Forecast Planner (IFP) is a geometric planning approach that\noffers lightweight computations, scalable, and reactive solutions for\nmulti-robot path planning in decentralized, communication-free settings.\nHowever, it struggles in symmetric configurations, where mirrored interactions\noften lead to collisions and deadlocks. We introduce eIFP-MPC, an optimized and\nextended version of IFP that improves robustness and path consistency in dense,\ndynamic environments. The method refines threat prioritization using a\ntime-to-collision heuristic, stabilizes path generation through cost-based\nvia-point selection, and ensures dynamic feasibility by incorporating model\npredictive control (MPC) into the planning process. These enhancements are\ntightly integrated into the IFP to preserve its efficiency while improving its\nadaptability and stability. Extensive simulations across symmetric and\nhigh-density scenarios show that eIFP-MPC significantly reduces oscillations,\nensures collision-free motion, and improves trajectory efficiency. The results\ndemonstrate that geometric planners can be strengthened through optimization,\nenabling robust performance at scale in complex multi-agent environments.", "AI": {"tldr": "eIFP-MPC是一种优化和扩展的迭代预测规划器（IFP），通过改进威胁优先级、路径生成和集成模型预测控制（MPC），解决了去中心化多机器人路径规划中IFP在对称配置和高密度环境下的碰撞和死锁问题，显著提高了鲁棒性、路径一致性和轨迹效率。", "motivation": "原有的迭代预测规划器（IFP）在去中心化、无通信的多机器人路径规划中虽然计算轻量、可扩展且反应迅速，但在对称配置下表现不佳，镜像交互常导致碰撞和死锁。", "method": "本文提出了eIFP-MPC，通过以下方式优化和扩展了IFP：1) 使用碰撞时间启发式方法精炼威胁优先级；2) 通过基于成本的路径点选择稳定路径生成；3) 将模型预测控制（MPC）整合到规划过程中以确保动态可行性。这些增强功能与IFP紧密集成，以保持其效率。", "result": "在对称和高密度场景下的广泛仿真表明，eIFP-MPC显著减少了振荡，确保了无碰撞运动，并提高了轨迹效率。这证明了几何规划器可以通过优化得到增强，从而在复杂的多智能体环境中实现大规模的鲁棒性能。", "conclusion": "通过优化（如eIFP-MPC），几何规划器可以得到强化，使其能够在复杂的多智能体环境中实现大规模的鲁棒性能，有效解决传统几何规划器在对称和密集场景中的局限性。"}}
{"id": "2508.08352", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08352", "abs": "https://arxiv.org/abs/2508.08352", "authors": ["Christophe EL Zeinaty", "Wassim Hamidouche", "Glenn Herrou", "Daniel Menard"], "title": "Designing Object Detection Models for TinyML: Foundations, Comparative Analysis, Challenges, and Emerging Solutions", "comment": null, "summary": "Object detection (OD) has become vital for numerous computer vision\napplications, but deploying it on resource-constrained IoT devices presents a\nsignificant challenge. These devices, often powered by energy-efficient\nmicrocontrollers, struggle to handle the computational load of deep\nlearning-based OD models. This issue is compounded by the rapid proliferation\nof IoT devices, predicted to surpass 150 billion by 2030. TinyML offers a\ncompelling solution by enabling OD on ultra-low-power devices, paving the way\nfor efficient and real-time processing at the edge. Although numerous survey\npapers have been published on this topic, they often overlook the optimization\nchallenges associated with deploying OD models in TinyML environments. To\naddress this gap, this survey paper provides a detailed analysis of key\noptimization techniques for deploying OD models on resource-constrained\ndevices. These techniques include quantization, pruning, knowledge\ndistillation, and neural architecture search. Furthermore, we explore both\ntheoretical approaches and practical implementations, bridging the gap between\nacademic research and real-world edge artificial intelligence deployment.\nFinally, we compare the key performance indicators (KPIs) of existing OD\nimplementations on microcontroller devices, highlighting the achieved maturity\nlevel of these solutions in terms of both prediction accuracy and efficiency.\nWe also provide a public repository to continually track developments in this\nfast-evolving field:\nhttps://github.com/christophezei/Optimizing-Object-Detection-Models-for-TinyML-A-Comprehensive-Survey.", "AI": {"tldr": "这篇综述论文详细分析了在资源受限的TinyML设备上部署目标检测模型所面临的优化挑战，并探讨了主要的优化技术及其性能表现。", "motivation": "目标检测在计算机视觉中至关重要，但在资源受限的IoT设备（如微控制器）上部署面临巨大计算负载挑战。现有综述论文往往忽视了TinyML环境中目标检测模型的优化难题，因此需要弥补这一空白。", "method": "本文作为一篇综述，详细分析了用于在资源受限设备上部署目标检测模型的关键优化技术，包括量化、剪枝、知识蒸馏和神经架构搜索。同时，探讨了理论方法和实际实现，并比较了现有目标检测实现在微控制器设备上的关键性能指标（KPIs）。", "result": "论文提供了对量化、剪枝、知识蒸馏和神经架构搜索等优化技术的深入分析，弥合了学术研究与实际边缘AI部署之间的差距。此外，还比较了现有目标检测实现在微控制器上的性能，并提供了一个公共仓库以持续追踪该领域的最新进展。", "conclusion": "TinyML为超低功耗设备上的目标检测提供了解决方案，实现了高效的边缘实时处理。通过分析优化技术和现有实现的KPIs，论文展示了这些解决方案在预测精度和效率方面的成熟度，并为该领域的未来发展提供了追踪资源。"}}
{"id": "2508.08297", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08297", "abs": "https://arxiv.org/abs/2508.08297", "authors": ["Rodrigo Lankaites Pinheiro", "Dario Landa-Silva", "Wasakorn Laesanklang", "Ademir Aparecido Constantino"], "title": "An Efficient Application of Goal Programming to Tackle Multiobjective Problems with Recurring Fitness Landscapes", "comment": null, "summary": "Many real-world applications require decision-makers to assess the quality of\nsolutions while considering multiple conflicting objectives. Obtaining good\napproximation sets for highly constrained many-objective problems is often a\ndifficult task even for modern multiobjective algorithms. In some cases,\nmultiple instances of the problem scenario present similarities in their\nfitness landscapes. That is, there are recurring features in the fitness\nlandscapes when searching for solutions to different problem instances. We\npropose a methodology to exploit this characteristic by solving one instance of\na given problem scenario using computationally expensive multiobjective\nalgorithms to obtain a good approximation set and then using Goal Programming\nwith efficient single-objective algorithms to solve other instances of the same\nproblem scenario. We use three goal-based objective functions and show that on\nbenchmark instances of the multiobjective vehicle routing problem with time\nwindows, the methodology is able to produce good results in short computation\ntime. The methodology allows to combine the effectiveness of state-of-the-art\nmultiobjective algorithms with the efficiency of goal programming to find good\ncompromise solutions in problem scenarios where instances have similar fitness\nlandscapes.", "AI": {"tldr": "提出一种新方法，通过结合多目标算法和目标规划来高效解决具有相似适应度景观的复杂多目标问题。", "motivation": "许多实际应用中的决策者需要评估具有多个冲突目标的解决方案质量。对于高度约束的多目标问题，即使是现代多目标算法也很难获得好的近似解集。在某些情况下，同一问题场景的不同实例具有相似的适应度景观。", "method": "该方法首先使用计算成本高的多目标算法解决给定问题场景的一个实例，以获得一个良好的近似解集。然后，对于同一问题场景的其他实例，利用目标规划（Goal Programming）和高效的单目标算法进行求解。研究中使用了三种基于目标的目标函数。", "result": "在带有时间窗的多目标车辆路径问题的基准实例上，该方法能够在短时间内产生良好的结果。", "conclusion": "该方法能够将最先进的多目标算法的有效性与目标规划的效率相结合，在实例具有相似适应度景观的问题场景中找到良好的折衷解决方案。"}}
{"id": "2508.08275", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08275", "abs": "https://arxiv.org/abs/2508.08275", "authors": ["Haiyun Guo", "ZhiYan Hou", "Yu Chen", "Jinghan He", "Yandu Sun", "Yuzhe Zhou", "Shujing Guo", "Kuan Zhu", "Jinqiao Wang"], "title": "MLLM-CBench:A Comprehensive Benchmark for Continual Instruction Tuning of Multimodal LLMs with Chain-of-Thought Reasoning Analysis", "comment": "under review", "summary": "Multimodal Large Language Models (MLLMs) rely on continual instruction tuning\nto adapt to the evolving demands of real-world applications. However, progress\nin this area is hindered by the lack of rigorous and systematic benchmarks. To\naddress this gap, we present MLLM-CTBench, a comprehensive evaluation benchmark\nwith three key contributions: (1) Multidimensional Evaluation: We combine final\nanswer accuracy with fine-grained CoT reasoning quality assessment, enabled by\na specially trained CoT evaluator; (2) Comprehensive Evaluation of Algorithms\nand Training Paradigms: We benchmark eight continual learning algorithms across\nfour major categories and systematically compare reinforcement learning with\nsupervised fine-tuning paradigms; (3) Carefully Curated Tasks: We select and\norganize 16 datasets from existing work, covering six challenging domains. Our\nkey findings include: (i) Models with stronger general capabilities exhibit\ngreater robustness to forgetting during continual learning; (ii) Reasoning\nchains degrade more slowly than final answers, supporting the hierarchical\nforgetting hypothesis; (iii) The effectiveness of continual learning algorithms\nis highly dependent on both model capability and task order; (iv) In\nreinforcement learning settings, incorporating KL-divergence constraints helps\nmaintain policy stability and plays a crucial role in mitigating forgetting.\nMLLM-CTBench establishes a rigorous standard for continual instruction tuning\nof MLLMs and offers practical guidance for algorithm design and evaluation.", "AI": {"tldr": "MLLM-CTBench是一个针对多模态大语言模型（MLLMs）持续指令调优的综合评估基准，旨在解决现有基准的不足，并提供算法设计和评估的实践指导。", "motivation": "多模态大语言模型（MLLMs）需要持续指令调优以适应不断变化的实际应用需求，但该领域缺乏严格和系统的基准，阻碍了进展。", "method": "本文提出了MLLM-CTBench，包含三个关键贡献：1) 多维度评估，结合最终答案准确性与由专门训练的CoT评估器实现的CoT推理质量评估；2) 全面评估算法和训练范式，对八种持续学习算法进行了基准测试，并系统比较了强化学习和监督微调范式；3) 精心策划的任务，从现有工作中选择了16个数据集，涵盖六个具有挑战性的领域。", "result": "主要发现包括：1) 具有更强通用能力的模型在持续学习中表现出更大的遗忘鲁棒性；2) 推理链比最终答案退化得更慢，支持分层遗忘假说；3) 持续学习算法的有效性高度依赖于模型能力和任务顺序；4) 在强化学习设置中，引入KL散度约束有助于保持策略稳定性，并在缓解遗忘方面发挥关键作用。", "conclusion": "MLLM-CTBench为MLLMs的持续指令调优建立了严格的标准，并为算法设计和评估提供了实践指导。"}}
{"id": "2508.09118", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.09118", "abs": "https://arxiv.org/abs/2508.09118", "authors": ["Ninad Gaikwad", "Kunal Shankar", "Anamika Dubey", "Alan Love", "Olvar Bergland"], "title": "Comparing Building Thermal Dynamics Models and Estimation Methods for Grid-Edge Applications", "comment": "This manuscript is a version of our paper accepted at the IEEE Power\n  & Energy Society General Meeting (PESGM) 2025", "summary": "We need computationally efficient and accurate building thermal dynamics\nmodels for use in grid-edge applications. This work evaluates two grey-box\napproaches for modeling building thermal dynamics: RC-network models and\nstructured regression models. For RC-network models, we compare parameter\nestimation methods including Nonlinear Least Squares, Batch Estimation, and\nMaximum Likelihood Estimation. We use the Almon Lag Structure with Linear Least\nSquares for estimating the structured regression models. The performance of\nthese models and methods is evaluated on simulated house and commercial\nbuilding data across three different simulation types.", "AI": {"tldr": "本研究评估了两种灰箱方法（RC网络模型和结构化回归模型）在建筑热动力学建模中的性能，并比较了不同的参数估计方法，以支持电网边缘应用。", "motivation": "需要在电网边缘应用中，使用计算效率高且准确的建筑热动力学模型。", "method": "研究评估了两种灰箱建模方法：RC网络模型和结构化回归模型。对于RC网络模型，比较了非线性最小二乘法、批量估计和最大似然估计等参数估计方法。对于结构化回归模型，使用带有线性最小二乘法的Almon滞后结构进行估计。模型和方法的性能在模拟住宅和商业建筑数据上，通过三种不同的模拟类型进行评估。", "result": "抽象部分描述了对这些模型和方法在模拟住宅和商业建筑数据上进行性能评估的过程，但未给出具体的评估结果。", "conclusion": "抽象部分主要介绍了研究的目的、方法和评估框架，未包含具体的实验结果或研究结论。"}}
{"id": "2508.09078", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09078", "abs": "https://arxiv.org/abs/2508.09078", "authors": ["Conall Daly", "Darren Ramsook", "Anil Kokaram"], "title": "Efficient motion-based metrics for video frame interpolation", "comment": "SPIE2025 - Applications of Digital Image Processing XLVIII accepted\n  manuscript", "summary": "Video frame interpolation (VFI) offers a way to generate intermediate frames\nbetween consecutive frames of a video sequence. Although the development of\nadvanced frame interpolation algorithms has received increased attention in\nrecent years, assessing the perceptual quality of interpolated content remains\nan ongoing area of research. In this paper, we investigate simple ways to\nprocess motion fields, with the purposes of using them as video quality metric\nfor evaluating frame interpolation algorithms. We evaluate these quality\nmetrics using the BVI-VFI dataset which contains perceptual scores measured for\ninterpolated sequences. From our investigation we propose a motion metric based\non measuring the divergence of motion fields. This metric correlates reasonably\nwith these perceptual scores (PLCC=0.51) and is more computationally efficient\n(x2.7 speedup) compared to FloLPIPS (a well known motion-based metric). We then\nuse our new proposed metrics to evaluate a range of state of the art frame\ninterpolation metrics and find our metrics tend to favour more perceptual\npleasing interpolated frames that may not score highly in terms of PSNR or\nSSIM.", "AI": {"tldr": "本文提出了一种基于运动场散度的新视频帧插值（VFI）质量评估指标，该指标与感知质量相关性良好且计算效率高。", "motivation": "视频帧插值算法发展迅速，但评估插值内容感知质量的方法仍是研究热点。", "method": "研究了处理运动场的简单方法，并将其用作VFI算法的视频质量指标。特别地，提出了一种基于测量运动场散度的运动指标。使用包含感知分数的BVI-VFI数据集对这些质量指标进行了评估，并与FloLPIPS等现有指标进行了比较。", "result": "提出的基于运动场散度的指标与感知分数有合理的相关性（PLCC=0.51），并且比FloLPIPS计算效率更高（快2.7倍）。该指标倾向于评估出更具视觉愉悦感的插值帧，即使这些帧在PSNR或SSIM等传统指标上得分不高。", "conclusion": "基于运动场散度的指标可以有效且高效地评估视频帧插值算法的感知质量，并能识别出传统指标可能忽视的感知上更优的插值结果。"}}
{"id": "2508.08269", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08269", "abs": "https://arxiv.org/abs/2508.08269", "authors": ["Sagar Verma"], "title": "emg2tendon: From sEMG Signals to Tendon Control in Musculoskeletal Hands", "comment": "Accepted in Robotics: Science and Systems (RSS 2025)", "summary": "Tendon-driven robotic hands offer unparalleled dexterity for manipulation\ntasks, but learning control policies for such systems presents unique\nchallenges. Unlike joint-actuated robotic hands, tendon-driven systems lack a\ndirect one-to-one mapping between motion capture (mocap) data and tendon\ncontrols, making the learning process complex and expensive. Additionally,\nvisual tracking methods for real-world applications are prone to occlusions and\ninaccuracies, further complicating joint tracking. Wrist-wearable surface\nelectromyography (sEMG) sensors present an inexpensive, robust alternative to\ncapture hand motion. However, mapping sEMG signals to tendon control remains a\nsignificant challenge despite the availability of EMG-to-pose data sets and\nregression-based models in the existing literature.\n  We introduce the first large-scale EMG-to-Tendon Control dataset for robotic\nhands, extending the emg2pose dataset, which includes recordings from 193\nsubjects, spanning 370 hours and 29 stages with diverse gestures. This dataset\nincorporates tendon control signals derived using the MyoSuite MyoHand model,\naddressing limitations such as invalid poses in prior methods. We provide three\nbaseline regression models to demonstrate emg2tendon utility and propose a\nnovel diffusion-based regression model for predicting tendon control from sEMG\nrecordings. This dataset and modeling framework marks a significant step\nforward for tendon-driven dexterous robotic manipulation, laying the groundwork\nfor scalable and accurate tendon control in robotic hands.\nhttps://emg2tendon.github.io/", "AI": {"tldr": "本文介绍了首个大规模肌电信号（sEMG）到肌腱控制的数据集，并提出了一种新的基于扩散的回归模型，以解决肌腱驱动机器人手难以控制的问题，为实现可扩展、精确的机器人手肌腱控制奠定基础。", "motivation": "肌腱驱动机器人手在操作任务中具有无与伦比的灵活性，但其控制策略学习面临独特挑战：运动捕捉数据与肌腱控制之间缺乏直接映射，导致学习过程复杂且昂贵；视觉追踪易受遮挡和不准确性影响；腕部可穿戴式sEMG传感器虽是廉价替代方案，但将sEMG信号映射到肌腱控制仍是重大挑战。", "method": "本文扩展了emg2pose数据集，构建了首个大规模EMG-to-Tendon Control数据集，包含193名受试者、370小时、29个阶段的多样手势记录。该数据集利用MyoSuite MyoHand模型推导肌腱控制信号，解决了以往方法中无效姿态的限制。同时，提供了三个基线回归模型来展示数据集的效用，并提出了一种新颖的基于扩散的回归模型，用于从sEMG记录中预测肌腱控制。", "result": "创建了首个大规模EMG-to-Tendon Control数据集，其数据量庞大且质量高，解决了现有方法的局限性。提供了基线回归模型证明了数据集的实用性。提出了一种新的扩散模型，有望提高sEMG到肌腱控制的预测精度。", "conclusion": "该数据集和建模框架的引入是肌腱驱动灵巧机器人操作领域的一大进步，为机器人手实现可扩展且精确的肌腱控制奠定了基础。"}}
{"id": "2508.08421", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.08421", "abs": "https://arxiv.org/abs/2508.08421", "authors": ["Jinlin Xiang", "Minho Choi", "Yubo Zhang", "Zhihao Zhou", "Arka Majumdar", "Eli Shlizerman"], "title": "Neural Tangent Knowledge Distillation for Optical Convolutional Networks", "comment": null, "summary": "Hybrid Optical Neural Networks (ONNs, typically consisting of an optical\nfrontend and a digital backend) offer an energy-efficient alternative to fully\ndigital deep networks for real-time, power-constrained systems. However, their\nadoption is limited by two main challenges: the accuracy gap compared to\nlarge-scale networks during training, and discrepancies between simulated and\nfabricated systems that further degrade accuracy. While previous work has\nproposed end-to-end optimizations for specific datasets (e.g., MNIST) and\noptical systems, these approaches typically lack generalization across tasks\nand hardware designs. To address these limitations, we propose a task-agnostic\nand hardware-agnostic pipeline that supports image classification and\nsegmentation across diverse optical systems. To assist optical system design\nbefore training, we estimate achievable model accuracy based on user-specified\nconstraints such as physical size and the dataset. For training, we introduce\nNeural Tangent Knowledge Distillation (NTKD), which aligns optical models with\nelectronic teacher networks, thereby narrowing the accuracy gap. After\nfabrication, NTKD also guides fine-tuning of the digital backend to compensate\nfor implementation errors. Experiments on multiple datasets (e.g., MNIST,\nCIFAR, Carvana Masking) and hardware configurations show that our pipeline\nconsistently improves ONN performance and enables practical deployment in both\npre-fabrication simulations and physical implementations.", "AI": {"tldr": "该论文提出了一种任务和硬件无关的混合光神经网络（ONN）优化流程，通过在训练前估计精度、引入神经正切知识蒸馏（NTKD）进行模型对齐和后期微调，显著提升了ONN的性能并解决了仿真与实际系统间的精度差异。", "motivation": "混合光神经网络（ONN）是实时、功耗受限系统的节能替代方案，但其应用受限于两大挑战：与大型数字网络相比的训练精度差距，以及仿真与实际制造系统之间的精度不一致。现有解决方案通常缺乏跨任务和硬件设计的泛化能力。", "method": "本文提出一个任务和硬件无关的流水线：1) 在训练前，根据物理尺寸和数据集等用户约束，估算可达到的模型精度，以辅助光学系统设计。2) 在训练期间，引入神经正切知识蒸馏（NTKD），使光模型与电子教师网络对齐，从而缩小精度差距。3) 在制造后，NTKD指导数字后端进行微调，以补偿实现误差。", "result": "在多个数据集（如MNIST、CIFAR、Carvana Masking）和硬件配置上的实验表明，该流水线持续提升了ONN的性能，并在预制造仿真和物理实现中都实现了实际部署。", "conclusion": "所提出的任务和硬件无关的流水线，结合神经正切知识蒸馏（NTKD），有效解决了混合光神经网络的精度差距和仿真-制造差异问题，显著提升了ONN的性能，使其能够实际部署在各种任务和硬件配置中。"}}
{"id": "2508.08300", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08300", "abs": "https://arxiv.org/abs/2508.08300", "authors": ["Yongchao Huang"], "title": "LLM-BI: Towards Fully Automated Bayesian Inference with Large Language Models", "comment": "6 pages", "summary": "A significant barrier to the widespread adoption of Bayesian inference is the\nspecification of prior distributions and likelihoods, which often requires\nspecialized statistical expertise. This paper investigates the feasibility of\nusing a Large Language Model (LLM) to automate this process. We introduce\nLLM-BI (Large Language Model-driven Bayesian Inference), a conceptual pipeline\nfor automating Bayesian workflows. As a proof-of-concept, we present two\nexperiments focused on Bayesian linear regression. In Experiment I, we\ndemonstrate that an LLM can successfully elicit prior distributions from\nnatural language. In Experiment II, we show that an LLM can specify the entire\nmodel structure, including both priors and the likelihood, from a single\nhigh-level problem description. Our results validate the potential of LLMs to\nautomate key steps in Bayesian modeling, enabling the possibility of an\nautomated inference pipeline for probabilistic programming.", "AI": {"tldr": "本文提出LLM-BI概念，探索使用大型语言模型（LLM）自动化贝叶斯推断中先验分布和似然函数规范的流程，并通过贝叶斯线性回归的实验验证了其可行性。", "motivation": "贝叶斯推断的广泛应用面临一个重要障碍，即先验分布和似然函数的指定通常需要专业的统计知识。", "method": "引入LLM-BI（大型语言模型驱动的贝叶斯推断）概念管道，并进行两项以贝叶斯线性回归为重点的实验：实验一验证LLM从自然语言中提取先验分布的能力；实验二验证LLM从高层问题描述中指定完整模型结构（包括先验和似然）的能力。", "result": "实验结果表明，LLM能成功从自然语言中提取先验分布，也能从单一高层描述中指定完整的贝叶斯模型结构（包括先验和似然）。", "conclusion": "研究验证了LLM在自动化贝叶斯建模关键步骤方面的潜力，为实现概率编程的自动化推断管道提供了可能性。"}}
{"id": "2508.08276", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08276", "abs": "https://arxiv.org/abs/2508.08276", "authors": ["Yassine Jamaa", "Badr AlKhamissi", "Satrajit Ghosh", "Martin Schrimpf"], "title": "Evaluating Contrast Localizer for Identifying Causal Unitsin Social & Mathematical Tasks in Language Models", "comment": null, "summary": "This work adapts a neuroscientific contrast localizer to pinpoint causally\nrelevant units for Theory of Mind (ToM) and mathematical reasoning tasks in\nlarge language models (LLMs) and vision-language models (VLMs). Across 11 LLMs\nand 5 VLMs ranging in size from 3B to 90B parameters, we localize top-activated\nunits using contrastive stimulus sets and assess their causal role via targeted\nablations. We compare the effect of lesioning functionally selected units\nagainst low-activation and randomly selected units on downstream accuracy\nacross established ToM and mathematical benchmarks. Contrary to expectations,\nlow-activation units sometimes produced larger performance drops than the\nhighly activated ones, and units derived from the mathematical localizer often\nimpaired ToM performance more than those from the ToM localizer. These findings\ncall into question the causal relevance of contrast-based localizers and\nhighlight the need for broader stimulus sets and more accurately capture\ntask-specific units.", "AI": {"tldr": "本研究利用神经科学的对比定位器在大型语言模型中寻找特定任务（如心理理论和数学推理）的因果相关单元，但发现对比定位器识别出的高激活单元并非总是因果相关性最强的，甚至低激活单元或跨任务定位的单元可能导致更大的性能下降。", "motivation": "研究旨在确定大型语言模型（LLMs）和视觉语言模型（VLMs）中与心理理论（ToM）和数学推理任务因果相关的特定神经单元。", "method": "研究采用神经科学对比定位器，通过对比刺激集识别LLMs和VLMs（11个LLMs，5个VLMs，参数范围3B至90B）中的高激活单元。随后，通过定向消融评估这些单元的因果作用，并将功能性选择单元、低激活单元和随机选择单元的损伤效果在ToM和数学基准测试上进行比较。", "result": "研究发现与预期相反：有时低激活单元导致的性能下降比高激活单元更大；数学定位器识别出的单元对ToM性能的损害往往比ToM定位器识别出的单元更大。", "conclusion": "这些发现对基于对比的定位器在识别因果相关单元方面的有效性提出了质疑，并强调需要更广泛的刺激集和更精确的方法来捕获任务特异性单元。"}}
{"id": "2508.09128", "categories": ["eess.SY", "cs.SY", "93E99", "A.1; I.2"], "pdf": "https://arxiv.org/pdf/2508.09128", "abs": "https://arxiv.org/abs/2508.09128", "authors": ["Dhruv S. Kushwaha", "Zoleikha A. Biron"], "title": "A Review On Safe Reinforcement Learning Using Lyapunov and Barrier Functions", "comment": "pages - 19, figures - 9, Submitted to IEEE Access", "summary": "Reinforcement learning (RL) has proven to be particularly effective in\nsolving complex decision-making problems for a wide range of applications. From\na control theory perspective, RL can be considered as an adaptive optimal\ncontrol scheme. Lyapunov and barrier functions are the most commonly used\ncertificates to guarantee system stability for a proposed/derived controller\nand constraint satisfaction guarantees, respectively, in control theoretic\napproaches. However, compared to theoretical guarantees available in control\ntheoretic methods, RL lacks closed-loop stability of a computed policy and\nconstraint satisfaction guarantees. Safe reinforcement learning refers to a\nclass of constrained problems where the constraint violations lead to partial\nor complete system failure. The goal of this review is to provide an overview\nof safe RL techniques using Lyapunov and barrier functions to guarantee this\nnotion of safety discussed (stability of the system in terms of a computed\npolicy and constraint satisfaction during training and deployment). The\ndifferent approaches employed are discussed in detail along with their\nshortcomings and benefits to provide critique and possible future research\ndirections. Key motivation for this review is to discuss current theoretical\napproaches for safety and stability guarantees in RL similar to control\ntheoretic approaches using Lyapunov and barrier functions. The review provides\nproven potential and promising scope of providing safety guarantees for complex\ndynamical systems with operational constraints using model-based and model-free\nRL.", "AI": {"tldr": "该综述探讨了如何利用控制理论中的Lyapunov函数和障碍函数来为强化学习（RL）提供闭环稳定性与约束满足的安全保障，弥补了RL在理论保证方面的不足，并讨论了现有方法的优缺点及未来研究方向。", "motivation": "与控制理论方法相比，强化学习在计算策略的闭环稳定性以及约束满足方面缺乏理论保证。当约束违反可能导致系统故障时，确保RL系统的安全至关重要。因此，需要探讨如何将控制理论中用于安全保障的Lyapunov函数和障碍函数引入到RL中。", "method": "该综述通过详细讨论使用Lyapunov函数和障碍函数来保证安全（包括系统稳定性与训练和部署过程中的约束满足）的各种安全强化学习技术，并分析了这些方法的优点、缺点以及潜在的未来研究方向。", "result": "该综述提供了安全强化学习技术的全面概述，展示了利用Lyapunov函数和障碍函数为复杂动态系统提供安全保证的已验证潜力和广阔前景，适用于基于模型和无模型的强化学习方法。同时，也指出了现有方法的不足并提出了未来研究方向。", "conclusion": "利用Lyapunov函数和障碍函数为强化学习提供安全和稳定性保障的方法具有巨大的潜力，能够为具有操作约束的复杂动态系统提供可靠的安全保证，是未来强化学习领域的重要研究方向。"}}
{"id": "2508.08588", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.08588", "abs": "https://arxiv.org/abs/2508.08588", "authors": ["Jingyun Liang", "Jingkai Zhou", "Shikai Li", "Chenjie Cao", "Lei Sun", "Yichen Qian", "Weihua Chen", "Fan Wang"], "title": "RealisMotion: Decomposed Human Motion Control and Video Generation in the World Space", "comment": "Project page: https://jingyunliang.github.io/RealisMotion", "summary": "Generating human videos with realistic and controllable motions is a\nchallenging task. While existing methods can generate visually compelling\nvideos, they lack separate control over four key video elements: foreground\nsubject, background video, human trajectory and action patterns. In this paper,\nwe propose a decomposed human motion control and video generation framework\nthat explicitly decouples motion from appearance, subject from background, and\naction from trajectory, enabling flexible mix-and-match composition of these\nelements. Concretely, we first build a ground-aware 3D world coordinate system\nand perform motion editing directly in the 3D space. Trajectory control is\nimplemented by unprojecting edited 2D trajectories into 3D with focal-length\ncalibration and coordinate transformation, followed by speed alignment and\norientation adjustment; actions are supplied by a motion bank or generated via\ntext-to-motion methods. Then, based on modern text-to-video diffusion\ntransformer models, we inject the subject as tokens for full attention,\nconcatenate the background along the channel dimension, and add motion\n(trajectory and action) control signals by addition. Such a design opens up the\npossibility for us to generate realistic videos of anyone doing anything\nanywhere. Extensive experiments on benchmark datasets and real-world cases\ndemonstrate that our method achieves state-of-the-art performance on both\nelement-wise controllability and overall video quality.", "AI": {"tldr": "本文提出了一种分解式人体运动控制与视频生成框架，实现了对前景主体、背景视频、人体轨迹和动作模式的独立控制，从而能够灵活组合这些元素生成高质量视频。", "motivation": "现有视频生成方法在视觉上引人注目，但缺乏对前景主体、背景视频、人体轨迹和动作模式这四个关键视频元素的独立控制。", "method": "该框架将运动与外观、主体与背景、动作与轨迹明确解耦。首先，建立一个地面感知的3D世界坐标系，在3D空间中进行运动编辑。轨迹控制通过将2D轨迹反投影到3D空间（结合焦距校准和坐标变换），并进行速度对齐和方向调整实现；动作则可从动作库获取或通过文本到动作方法生成。然后，基于文本到视频扩散Transformer模型，将主体作为token注入进行注意力计算，沿通道维度拼接背景，并通过加法添加运动（轨迹和动作）控制信号。", "result": "在基准数据集和真实世界案例上的广泛实验表明，该方法在元素级可控性和整体视频质量方面均达到了最先进的性能。它能够生成“任何人做任何事在任何地方”的逼真视频。", "conclusion": "所提出的分解式框架通过解耦并独立控制视频元素，极大地提升了人体视频生成的灵活性和可控性，实现了高质量且可自由组合的逼真人体视频生成。"}}
{"id": "2508.08303", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.08303", "abs": "https://arxiv.org/abs/2508.08303", "authors": ["Yasuyuki Fujii", "Dinh Tuan Tran", "Joo-Ho Lee"], "title": "Evaluation of an Autonomous Surface Robot Equipped with a Transformable Mobility Mechanism for Efficient Mobility Control", "comment": "5 pages, 6 figures. Presented at the ICRA 2025 Workshop on REaCT:\n  Robotics for Environmental and Climate Assessment", "summary": "Efficient mobility and power consumption are critical for autonomous water\nsurface robots in long-term water environmental monitoring. This study develops\nand evaluates a transformable mobility mechanism for a water surface robot with\ntwo control modes: station-keeping and traveling to improve energy efficiency\nand maneuverability. Field experiments show that, in a round-trip task between\ntwo points, the traveling mode reduces power consumption by 10\\% and decreases\nthe total time required for travel by 5\\% compared to the station-keeping mode.\nThese results confirm the effectiveness of the transformable mobility mechanism\nfor enhancing operational efficiency in patrolling on water surface.", "AI": {"tldr": "本研究开发并评估了一种可变形移动机构，用于水面机器人以提高能源效率和机动性，并通过现场实验验证了其在巡逻任务中的有效性。", "motivation": "自主水面机器人在长期水环境监测中，高效的移动性和低功耗至关重要。", "method": "开发了一种具有驻点保持和巡航两种控制模式的可变形移动机构。通过现场实验，在两点间的往返任务中对其进行了评估。", "result": "与驻点保持模式相比，巡航模式在往返任务中将功耗降低了10%，总耗时减少了5%。", "conclusion": "可变形移动机构有效提升了水面机器人巡逻的运行效率。"}}
{"id": "2508.08487", "categories": ["cs.CV", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.08487", "abs": "https://arxiv.org/abs/2508.08487", "authors": ["Qian Wang", "Ziqi Huang", "Ruoxi Jia", "Paul Debevec", "Ning Yu"], "title": "MAViS: A Multi-Agent Framework for Long-Sequence Video Storytelling", "comment": "Video Generation Agent", "summary": "Despite recent advances, long-sequence video generation frameworks still\nsuffer from significant limitations: poor assistive capability, suboptimal\nvisual quality, and limited expressiveness. To mitigate these limitations, we\npropose MAViS, an end-to-end multi-agent collaborative framework for\nlong-sequence video storytelling. MAViS orchestrates specialized agents across\nmultiple stages, including script writing, shot designing, character modeling,\nkeyframe generation, video animation, and audio generation. In each stage,\nagents operate under the 3E Principle -- Explore, Examine, and Enhance -- to\nensure the completeness of intermediate outputs. Considering the capability\nlimitations of current generative models, we propose the Script Writing\nGuidelines to optimize compatibility between scripts and generative tools.\nExperimental results demonstrate that MAViS achieves state-of-the-art\nperformance in assistive capability, visual quality, and video expressiveness.\nIts modular framework further enables scalability with diverse generative\nmodels and tools. With just a brief user prompt, MAViS is capable of producing\nhigh-quality, expressive long-sequence video storytelling, enriching\ninspirations and creativity for users. To the best of our knowledge, MAViS is\nthe only framework that provides multimodal design output -- videos with\nnarratives and background music.", "AI": {"tldr": "MAViS是一个多智能体协作框架，用于长序列视频故事生成，通过多阶段智能体协作和3E原则，解决了现有框架在辅助能力、视觉质量和表达力方面的局限性，实现了高质量的多模态视频输出。", "motivation": "现有的长序列视频生成框架存在辅助能力差、视觉质量不佳和表达力有限等显著局限性。", "method": "MAViS采用端到端的多智能体协作框架，编排专门的智能体负责剧本创作、镜头设计、角色建模、关键帧生成、视频动画和音频生成。每个阶段的智能体都遵循“3E原则”（探索、审查和增强）以确保中间输出的完整性。此外，还提出了“剧本创作指南”以优化剧本与生成工具的兼容性。", "result": "实验结果表明，MAViS在辅助能力、视觉质量和视频表达力方面均达到了最先进的性能。其模块化框架使其能够与多样化的生成模型和工具进行扩展。MAViS仅需简短的用户提示即可生成高质量、富有表现力的长序列视频故事，并且是唯一提供多模态设计输出（带叙述和背景音乐的视频）的框架。", "conclusion": "MAViS是一个开创性的框架，能够从简短的用户提示生成高质量、富有表现力的长序列视频故事，提供多模态设计输出，极大地丰富了用户的灵感和创造力。"}}
{"id": "2508.08308", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08308", "abs": "https://arxiv.org/abs/2508.08308", "authors": ["Chuanruo Fu", "Yuncheng Du"], "title": "First Ask Then Answer: A Framework Design for AI Dialogue Based on Supplementary Questioning with Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) often struggle to deliver accurate and\nactionable answers when user-provided information is incomplete or\nill-specified. We propose a new interaction paradigm, First Ask Then Answer\n(FATA), in which, through prompt words, LLMs are guided to proactively generate\nmultidimensional supplementary questions for users prior to response\ngeneration. Subsequently, by integrating user-provided supplementary\ninformation with the original query through sophisticated prompting techniques,\nwe achieve substantially improved response quality and relevance. In contrast\nto existing clarification approaches -- such as the CLAM framework oriented to\nambiguity and the self-interrogation Self-Ask method -- FATA emphasizes\ncompleteness (beyond mere disambiguation) and user participation (inviting\nhuman input instead of relying solely on model-internal reasoning). It also\nadopts a single-turn strategy: all clarifying questions are produced at once,\nthereby reducing dialogue length and improving efficiency. Conceptually, FATA\nuses the reasoning power of LLMs to scaffold user expression, enabling\nnon-expert users to formulate more comprehensive and contextually relevant\nqueries. To evaluate FATA, we constructed a multi-domain benchmark and compared\nit with two controls: a baseline prompt (B-Prompt) and a context-enhanced\nexpert prompt (C-Prompt). Experimental results show that FATA outperforms\nB-Prompt by approximately 40% in aggregate metrics and exhibits a coefficient\nof variation 8% lower than C-Prompt, indicating superior stability.", "AI": {"tldr": "针对用户输入信息不完整的问题，本文提出FATA（First Ask Then Answer）交互范式，引导LLM先主动生成多维补充问题，再结合用户补充信息提供更准确和相关的回答。", "motivation": "大型语言模型（LLMs）在用户提供的信息不完整或不明确时，难以给出准确和可操作的答案。", "method": "FATA范式通过提示词引导LLM在生成响应前主动生成多维度的补充问题，邀请用户提供额外信息。随后，通过复杂的提示技术将用户提供的补充信息与原始查询整合，以提高响应质量。FATA强调信息的完整性和用户参与，并采用单轮策略一次性生成所有澄清问题，以提高效率。", "result": "在构建的多领域基准测试中，FATA在综合指标上比基线提示（B-Prompt）高出约40%，并且比上下文增强的专家提示（C-Prompt）的变异系数低8%，表明其具有更优越的稳定性。", "conclusion": "FATA利用LLM的推理能力来辅助用户表达，使非专业用户能够构建更全面和上下文相关的查询，从而显著提高了响应的质量和相关性。"}}
{"id": "2508.08277", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.08277", "abs": "https://arxiv.org/abs/2508.08277", "authors": ["Haoze Du", "Richard Li", "Edward Gehringer"], "title": "Objective Metrics for Evaluating Large Language Models Using External Data Sources", "comment": "This version of the paper is lightly revised from the EDM 2025\n  proceedings for the sake of clarity", "summary": "Evaluating the performance of Large Language Models (LLMs) is a critical yet\nchallenging task, particularly when aiming to avoid subjective assessments.\nThis paper proposes a framework for leveraging subjective metrics derived from\nthe class textual materials across different semesters to assess LLM outputs\nacross various tasks. By utilizing well-defined benchmarks, factual datasets,\nand structured evaluation pipelines, the approach ensures consistent,\nreproducible, and bias-minimized measurements. The framework emphasizes\nautomation and transparency in scoring, reducing reliance on human\ninterpretation while ensuring alignment with real-world applications. This\nmethod addresses the limitations of subjective evaluation methods, providing a\nscalable solution for performance assessment in educational, scientific, and\nother high-stakes domains.", "AI": {"tldr": "本文提出一个框架，利用跨学期课堂文本材料中的主观指标来评估大型语言模型（LLM）的性能，旨在实现客观、可扩展和自动化评估。", "motivation": "评估LLM的性能至关重要但充满挑战，尤其是在避免主观评估方面。", "method": "该方法提出了一个框架，利用从不同学期的课堂文本材料中提取的主观指标来评估LLM在各种任务上的输出。通过使用明确的基准、事实数据集和结构化评估流程，该框架强调评分的自动化和透明度。", "result": "该方法确保了评估结果的一致性、可复现性和最小化偏差，减少了对人工解释的依赖，并与实际应用保持一致。", "conclusion": "该方法解决了主观评估方法的局限性，为教育、科学和其他高风险领域的性能评估提供了一个可扩展的解决方案。"}}
{"id": "2508.09130", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.09130", "abs": "https://arxiv.org/abs/2508.09130", "authors": ["Ninad Gaikwad", "Kasey Dettlaff", "Athul Jose P", "Anamika Dubey"], "title": "An Open-Source Simulation and Data Management Tool for EnergyPlus Building Models", "comment": "This manuscript is a version of our paper accepted at the IEEE Power\n  & Energy Society General Meeting (PESGM) 2025", "summary": "We present a new open-source, GUI-based application created using\nPlotly-Dash, along with an integrated PostgreSQL-based relational database,\ndeveloped to streamline EnergyPlus building model simulation workflows. The\napplication facilitates data generation, aggregation (across thermal zones),\nand visualization based on customizable user preferences, while the database\nefficiently stores and retrieves complex simulation data generated by\nEnergyPlus. We demonstrate the need for this application and database,\nemphasizing how existing approaches for generating, managing, and analyzing\nEnergyPlus simulation data can be cumbersome, particularly when handling a\nlarge number of building models with varying simulation setups. This integrated\nframework enables building energy engineers and researchers to simplify their\nEnergyPlus simulations, manage generated simulation data, perform data\nanalyses, and support data-driven modeling tasks.", "AI": {"tldr": "开发了一个基于Plotly-Dash和PostgreSQL的开源GUI应用，用于简化EnergyPlus建筑模型模拟工作流。", "motivation": "现有EnergyPlus模拟数据生成、管理和分析方法繁琐，尤其是在处理大量建筑模型和不同模拟设置时。", "method": "开发了一个开源的、基于GUI的应用程序，使用Plotly-Dash进行界面开发，并集成了一个基于PostgreSQL的关系型数据库。该应用支持数据生成、跨热区聚合和可视化，数据库负责高效存储和检索复杂的模拟数据。", "result": "该集成框架简化了EnergyPlus模拟过程，实现了模拟数据的有效管理，支持数据分析，并有助于数据驱动的建模任务。", "conclusion": "该应用程序和数据库的集成提供了一个更高效、用户友好的解决方案，解决了EnergyPlus模拟数据管理的痛点，赋能建筑能源工程师和研究人员。"}}
{"id": "2508.08328", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.08328", "abs": "https://arxiv.org/abs/2508.08328", "authors": ["Qiwei Liang", "Boyang Cai", "Rongyi He", "Hui Li", "Tao Teng", "Haihan Duan", "Changxin Huang", "Runhao Zeng"], "title": "Whole-Body Coordination for Dynamic Object Grasping with Legged Manipulators", "comment": null, "summary": "Quadrupedal robots with manipulators offer strong mobility and adaptability\nfor grasping in unstructured, dynamic environments through coordinated\nwhole-body control. However, existing research has predominantly focused on\nstatic-object grasping, neglecting the challenges posed by dynamic targets and\nthus limiting applicability in dynamic scenarios such as logistics sorting and\nhuman-robot collaboration. To address this, we introduce DQ-Bench, a new\nbenchmark that systematically evaluates dynamic grasping across varying object\nmotions, velocities, heights, object types, and terrain complexities, along\nwith comprehensive evaluation metrics. Building upon this benchmark, we propose\nDQ-Net, a compact teacher-student framework designed to infer grasp\nconfigurations from limited perceptual cues. During training, the teacher\nnetwork leverages privileged information to holistically model both the static\ngeometric properties and dynamic motion characteristics of the target, and\nintegrates a grasp fusion module to deliver robust guidance for motion\nplanning. Concurrently, we design a lightweight student network that performs\ndual-viewpoint temporal modeling using only the target mask, depth map, and\nproprioceptive state, enabling closed-loop action outputs without reliance on\nprivileged data. Extensive experiments on DQ-Bench demonstrate that DQ-Net\nachieves robust dynamic objects grasping across multiple task settings,\nsubstantially outperforming baseline methods in both success rate and\nresponsiveness.", "AI": {"tldr": "该论文提出了DQ-Bench基准和DQ-Net框架，以解决四足机器人动态目标抓取问题，通过教师-学生网络实现鲁棒高效的抓取。", "motivation": "现有研究主要关注静态物体抓取，忽略了动态目标带来的挑战，限制了四足机器人在物流分拣、人机协作等动态场景中的应用。", "method": "引入DQ-Bench基准，系统评估不同运动、速度、高度、物体类型和地形复杂度的动态抓取。提出DQ-Net，一个紧凑的教师-学生框架：教师网络利用特权信息（几何和运动特性）提供抓取指导和运动规划；学生网络仅使用目标掩码、深度图和本体状态进行双视角时间建模，实现闭环动作输出。", "result": "在DQ-Bench上的大量实验表明，DQ-Net在多种任务设置下实现了鲁棒的动态物体抓取，在成功率和响应速度方面均显著优于基线方法。", "conclusion": "DQ-Net框架有效解决了四足机器人在复杂动态环境中抓取移动目标的问题，展示了其在实际应用中的巨大潜力。"}}
{"id": "2508.08488", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08488", "abs": "https://arxiv.org/abs/2508.08488", "authors": ["Ankan Deria", "Dwarikanath Mahapatra", "Behzad Bozorgtabar", "Mohna Chakraborty", "Snehashis Chakraborty", "Sudipta Roy"], "title": "MuGa-VTON: Multi-Garment Virtual Try-On via Diffusion Transformers with Prompt Customization", "comment": null, "summary": "Virtual try-on seeks to generate photorealistic images of individuals in\ndesired garments, a task that must simultaneously preserve personal identity\nand garment fidelity for practical use in fashion retail and personalization.\nHowever, existing methods typically handle upper and lower garments separately,\nrely on heavy preprocessing, and often fail to preserve person-specific cues\nsuch as tattoos, accessories, and body shape-resulting in limited realism and\nflexibility. To this end, we introduce MuGa-VTON, a unified multi-garment\ndiffusion framework that jointly models upper and lower garments together with\nperson identity in a shared latent space. Specifically, we proposed three key\nmodules: the Garment Representation Module (GRM) for capturing both garment\nsemantics, the Person Representation Module (PRM) for encoding identity and\npose cues, and the A-DiT fusion module, which integrates garment, person, and\ntext-prompt features through a diffusion transformer. This architecture\nsupports prompt-based customization, allowing fine-grained garment\nmodifications with minimal user input. Extensive experiments on the VITON-HD\nand DressCode benchmarks demonstrate that MuGa-VTON outperforms existing\nmethods in both qualitative and quantitative evaluations, producing\nhigh-fidelity, identity-preserving results suitable for real-world virtual\ntry-on applications.", "AI": {"tldr": "MuGa-VTON是一个统一的多服装扩散框架，用于虚拟试穿，能够同时处理上下装并保留个人身份，通过新的模块和扩散Transformer架构，在现实世界应用中表现出色。", "motivation": "现有虚拟试穿方法通常分开处理上下装，需要大量预处理，并且未能很好地保留个人特有信息（如纹身、配饰、体型），导致真实感和灵活性不足。", "method": "提出了MuGa-VTON框架，它在共享潜在空间中联合建模上下装和个人身份。核心模块包括：服装表示模块（GRM）用于捕捉服装语义；人物表示模块（PRM）用于编码身份和姿态；以及A-DiT融合模块，通过扩散Transformer整合服装、人物和文本提示特征，支持基于提示的定制。", "result": "在VITON-HD和DressCode基准测试上，MuGa-VTON在定性和定量评估中均优于现有方法，生成了高保真度、保留身份的结果。", "conclusion": "MuGa-VTON生成的图像高度逼真并保留个人身份，非常适用于现实世界的虚拟试穿应用。"}}
{"id": "2508.08344", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08344", "abs": "https://arxiv.org/abs/2508.08344", "authors": ["Dongzhuoran Zhou", "Yuqicheng Zhu", "Xiaxia Wang", "Hongkuan Zhou", "Yuan He", "Jiaoyan Chen", "Evgeny Kharlamov", "Steffen Staab"], "title": "What Breaks Knowledge Graph based RAG? Empirical Insights into Reasoning under Incomplete Knowledge", "comment": null, "summary": "Knowledge Graph-based Retrieval-Augmented Generation (KG-RAG) is an\nincreasingly explored approach for combining the reasoning capabilities of\nlarge language models with the structured evidence of knowledge graphs.\nHowever, current evaluation practices fall short: existing benchmarks often\ninclude questions that can be directly answered using existing triples in KG,\nmaking it unclear whether models perform reasoning or simply retrieve answers\ndirectly. Moreover, inconsistent evaluation metrics and lenient answer matching\ncriteria further obscure meaningful comparisons. In this work, we introduce a\ngeneral method for constructing benchmarks, together with an evaluation\nprotocol, to systematically assess KG-RAG methods under knowledge\nincompleteness. Our empirical results show that current KG-RAG methods have\nlimited reasoning ability under missing knowledge, often rely on internal\nmemorization, and exhibit varying degrees of generalization depending on their\ndesign.", "AI": {"tldr": "针对现有知识图谱增强检索生成（KG-RAG）评估的缺陷，本文提出新的基准构建方法和评估协议，发现当前KG-RAG方法在知识不完整条件下推理能力有限且依赖内部记忆。", "motivation": "当前的KG-RAG评估基准存在问题，例如问题可直接从知识图谱中检索答案，导致难以区分模型是进行推理还是直接检索；此外，评估指标不一致和宽松的答案匹配标准也阻碍了有意义的比较。", "method": "本文提出了一种通用的基准构建方法，并设计了一套评估协议，旨在系统地评估KG-RAG方法在知识不完整条件下的表现。", "result": "实证结果表明，当前的KG-RAG方法在知识缺失的情况下推理能力有限，通常依赖于模型的内部记忆，并且其泛化能力因设计不同而异。", "conclusion": "目前的KG-RAG方法在处理知识不完整时的推理能力有待提高，且易受内部记忆影响。需要更系统和严格的评估方法来推动该领域的发展。"}}
{"id": "2508.08283", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.08283", "abs": "https://arxiv.org/abs/2508.08283", "authors": ["Andres Garcia Rincon", "Eliseo Ferrante"], "title": "MinionsLLM: a Task-adaptive Framework For The Training and Control of Multi-Agent Systems Through Natural Language", "comment": null, "summary": "This paper presents MinionsLLM, a novel framework that integrates Large\nLanguage Models (LLMs) with Behavior Trees (BTs) and Formal Grammars to enable\nnatural language control of multi-agent systems within arbitrary, user-defined\nenvironments. MinionsLLM provides standardized interfaces for defining\nenvironments, agents, and behavioral primitives, and introduces two synthetic\ndataset generation methods (Method A and Method B) to fine-tune LLMs for\nimproved syntactic validity and semantic task relevance. We validate our\napproach using Google's Gemma 3 model family at three parameter scales (1B, 4B,\nand 12B) and demonstrate substantial gains: Method B increases syntactic\nvalidity to 92.6% and achieves a mean task performance improvement of 33% over\nbaseline. Notably, our experiments show that smaller models benefit most from\nfine-tuning, suggesting promising directions for deploying compact, locally\nhosted LLMs in resource-constrained multi-agent control scenarios. The\nframework and all resources are released open-source to support reproducibility\nand future research.", "AI": {"tldr": "MinionsLLM是一个新颖的框架，结合LLM、行为树和形式文法，实现多智能体系统的自然语言控制。通过两种合成数据集生成方法微调LLM，显著提升了句法有效性和任务性能，尤其对小型模型效果显著。", "motivation": "实现多智能体系统在任意用户定义环境中的自然语言控制具有挑战性，需要整合LLM的能力与结构化行为表示，并提高LLM生成控制指令的准确性和相关性。", "method": "本文提出了MinionsLLM框架，将大型语言模型（LLM）与行为树（BTs）和形式文法相结合。该框架提供标准化接口用于定义环境、智能体和行为原语。为微调LLM以提高句法有效性和语义任务相关性，引入了两种合成数据集生成方法（方法A和方法B）。研究使用Google Gemma 3模型家族（1B、4B、12B）进行验证。", "result": "通过方法B微调，LLM的句法有效性提高到92.6%，平均任务性能比基线提升了33%。实验还表明，较小的模型（如1B）从微调中获益最大。", "conclusion": "MinionsLLM框架通过整合LLM、行为树和形式文法，并利用合成数据微调，显著提升了多智能体系统自然语言控制的性能。研究结果表明，在资源受限的多智能体控制场景中，部署紧凑、本地托管的LLM具有巨大潜力。"}}
{"id": "2508.09003", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.09003", "abs": "https://arxiv.org/abs/2508.09003", "authors": ["Filippo A. Spinelli", "Yifan Zhai", "Fang Nan", "Pascal Egli", "Julian Nubert", "Thilo Bleumer", "Lukas Miller", "Ferdinand Hofmann", "Marco Hutter"], "title": "Large Scale Robotic Material Handling: Learning, Planning, and Control", "comment": "Preliminary version, currently undergoing review process", "summary": "Bulk material handling involves the efficient and precise moving of large\nquantities of materials, a core operation in many industries, including cargo\nship unloading, waste sorting, construction, and demolition. These repetitive,\nlabor-intensive, and safety-critical operations are typically performed using\nlarge hydraulic material handlers equipped with underactuated grippers. In this\nwork, we present a comprehensive framework for the autonomous execution of\nlarge-scale material handling tasks. The system integrates specialized modules\nfor environment perception, pile attack point selection, path planning, and\nmotion control. The main contributions of this work are two reinforcement\nlearning-based modules: an attack point planner that selects optimal grasping\nlocations on the material pile to maximize removal efficiency and minimize the\nnumber of scoops, and a robust trajectory following controller that addresses\nthe precision and safety challenges associated with underactuated grippers in\nmovement, while utilizing their free-swinging nature to release material\nthrough dynamic throwing. We validate our framework through real-world\nexperiments on a 40 t material handler in a representative worksite, focusing\non two key tasks: high-throughput bulk pile management and high-precision truck\nloading. Comparative evaluations against human operators demonstrate the\nsystem's effectiveness in terms of precision, repeatability, and operational\nsafety. To the best of our knowledge, this is the first complete automation of\nmaterial handling tasks on a full scale.", "AI": {"tldr": "该论文提出了一个用于大规模物料搬运任务的自主执行框架，利用强化学习实现抓取点选择和轨迹跟踪控制，并在真实场景中验证了其在精度、重复性和操作安全性方面的有效性。", "motivation": "物料搬运操作（如货物卸载、垃圾分类、建筑拆除）重复、劳动密集且存在安全风险，通常由配备欠驱动抓具的大型液压物料搬运机执行，需要自动化来提高效率和安全性。", "method": "该研究提出了一个综合框架，集成了环境感知、堆料攻击点选择、路径规划和运动控制模块。主要贡献是两个基于强化学习的模块：一个攻击点规划器，用于选择最佳抓取位置以最大化移除效率并最小化铲取次数；一个鲁棒的轨迹跟踪控制器，解决欠驱动抓具的精度和安全挑战，并利用其自由摆动特性通过动态抛掷释放物料。", "result": "该框架在真实场景中（使用一台40吨物料搬运机）进行了验证，重点关注高吞吐量散料堆管理和高精度卡车装载两项任务。与人类操作员的对比评估表明，该系统在精度、重复性和操作安全性方面表现出有效性。据作者所知，这是首次实现物料搬运任务的全面自动化。", "conclusion": "该研究成功开发并验证了一个用于大规模物料搬运的完整自主系统，通过集成强化学习模块，显著提高了操作的效率、精度、重复性和安全性，实现了该领域全尺寸任务的首次自动化。"}}
{"id": "2508.08473", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.08473", "abs": "https://arxiv.org/abs/2508.08473", "authors": ["Hossein B. Jond"], "title": "A Minimal Model for Emergent Collective Behaviors in Autonomous Robotic Multi-Agent Systems", "comment": null, "summary": "Collective behaviors such as swarming and flocking emerge from simple,\ndecentralized interactions in biological systems. Existing models, such as\nVicsek and Cucker-Smale, lack collision avoidance, whereas the Olfati-Saber\nmodel imposes rigid formations, limiting their applicability in swarm robotics.\nTo address these limitations, this paper proposes a minimal yet expressive\nmodel that governs agent dynamics using relative positions, velocities, and\nlocal density, modulated by two tunable parameters: the spatial offset and\nkinetic offset. The model achieves spatially flexible, collision-free behaviors\nthat reflect naturalistic group dynamics. Furthermore, we extend the framework\nto cognitive autonomous systems, enabling energy-aware phase transitions\nbetween swarming and flocking through adaptive control parameter tuning. This\ncognitively inspired approach offers a robust foundation for real-world\napplications in multi-robot systems, particularly autonomous aerial swarms.", "AI": {"tldr": "本文提出一种新的群体行为模型，通过相对位置、速度和局部密度实现灵活无碰撞的群体行为，并扩展到认知系统以实现能量感知相变。", "motivation": "现有群体行为模型（如Vicsek、Cucker-Smale）缺乏避碰功能，而Olfati-Saber模型则形成僵硬的队形，限制了其在群体机器人中的应用。", "method": "提出一个基于相对位置、速度和局部密度，并由空间偏移和动能偏移两个可调参数调节的代理动力学模型。该框架进一步扩展到认知自主系统，通过自适应控制参数调整实现能量感知的蜂拥和集群行为之间的相变。", "result": "该模型实现了空间灵活、无碰撞的群体行为，反映了自然群体动力学。扩展框架能实现能量感知的蜂拥与集群行为间的相变。", "conclusion": "这种受认知启发的模型为多机器人系统（特别是自主空中蜂群）的实际应用提供了坚实基础。"}}
{"id": "2508.08498", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08498", "abs": "https://arxiv.org/abs/2508.08498", "authors": ["Aneel Damaraju", "Dean Hazineh", "Todd Zickler"], "title": "CObL: Toward Zero-Shot Ordinal Layering without User Prompting", "comment": "ICCV 2025: Project page with demo, datasets, and code:\n  https://vision.seas.harvard.edu/cobl/", "summary": "Vision benefits from grouping pixels into objects and understanding their\nspatial relationships, both laterally and in depth. We capture this with a\nscene representation comprising an occlusion-ordered stack of \"object layers,\"\neach containing an isolated and amodally-completed object. To infer this\nrepresentation from an image, we introduce a diffusion-based architecture named\nConcurrent Object Layers (CObL). CObL generates a stack of object layers in\nparallel, using Stable Diffusion as a prior for natural objects and\ninference-time guidance to ensure the inferred layers composite back to the\ninput image. We train CObL using a few thousand synthetically-generated images\nof multi-object tabletop scenes, and we find that it zero-shot generalizes to\nphotographs of real-world tabletops with varying numbers of novel objects. In\ncontrast to recent models for amodal object completion, CObL reconstructs\nmultiple occluded objects without user prompting and without knowing the number\nof objects beforehand. Unlike previous models for unsupervised object-centric\nrepresentation learning, CObL is not limited to the world it was trained in.", "AI": {"tldr": "该论文提出了一种名为CObL的扩散模型，用于从图像中推断出由多个遮挡排序的“对象层”组成的场景表示，实现无提示、无预设对象数量的多对象非模态补全，并具有零样本泛化能力。", "motivation": "视觉系统通过将像素分组为对象并理解它们在横向和深度上的空间关系来获得优势。特别是在处理遮挡和实现非模态补全时，需要一种能够有效表示这些关系和完整对象形态的场景表示。", "method": "该研究采用一种场景表示，即由“对象层”组成的遮挡排序堆栈，每个层包含一个独立且非模态补全的对象。为推断此表示，引入了基于扩散的CObL架构，其利用Stable Diffusion作为自然对象先验，并通过推理时引导确保推断的层能够复合回输入图像。模型在数千张合成的多对象桌面场景图像上进行训练。", "result": "CObL能够零样本泛化到包含不同数量新颖对象的真实世界桌面照片。与现有非模态对象补全模型不同，CObL无需用户提示或预先知道对象数量即可重建多个被遮挡对象。与之前的无监督对象中心表示学习模型不同，CObL不受限于其训练世界。", "conclusion": "CObL成功地从图像中推断出一种有效的遮挡排序对象层表示，实现了多对象的非模态补全，并在无需额外输入或预知信息的情况下展现出强大的泛化能力，克服了现有方法的局限性。"}}
{"id": "2508.08382", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08382", "abs": "https://arxiv.org/abs/2508.08382", "authors": ["Timo Bertram"], "title": "UrzaGPT: LoRA-Tuned Large Language Models for Card Selection in Collectible Card Games", "comment": null, "summary": "Collectible card games (CCGs) are a difficult genre for AI due to their\npartial observability, long-term decision-making, and evolving card sets. Due\nto this, current AI models perform vastly worse than human players at CCG tasks\nsuch as deckbuilding and gameplay. In this work, we introduce UrzaGPT, a\ndomain-adapted large language model that recommends real-time drafting\ndecisions in Magic: The Gathering. Starting from an open-weight LLM, we use\nLow-Rank Adaptation fine-tuning on a dataset of annotated draft logs. With\nthis, we leverage the language modeling capabilities of LLM, and can quickly\nadapt to different expansions of the game. We benchmark UrzaGPT in comparison\nto zero-shot LLMs and the state-of-the-art domain-specific model. Untuned,\nsmall LLMs like Llama-3-8B are completely unable to draft, but the larger\nGPT-4o achieves a zero-shot performance of 43%. Using UrzaGPT to fine-tune\nsmaller models, we achieve an accuracy of 66.2% using only 10,000 steps.\nDespite this not reaching the capability of domain-specific models, we show\nthat solely using LLMs to draft is possible and conclude that using LLMs can\nenable performant, general, and update-friendly drafting AIs in the future.", "AI": {"tldr": "UrzaGPT是一个领域适应的大型语言模型，通过LoRA微调，能为《万智牌》提供实时选牌建议，证明了LLM在集换式卡牌游戏（CCG）选牌任务中的潜力。", "motivation": "集换式卡牌游戏（CCG）对AI而言极具挑战性，因为其部分可观察性、长期决策和不断变化的卡牌集导致当前AI在组牌和游戏玩法上远逊于人类玩家。", "method": "引入UrzaGPT，一个领域适应的大型语言模型。从一个开放权重的LLM开始，使用低秩适应（LoRA）在标注的选牌日志数据集上进行微调，以利用LLM的语言建模能力并快速适应游戏的不同扩展。", "result": "未经调优的小型LLM（如Llama-3-8B）完全无法选牌，而GPT-4o的零样本性能为43%。UrzaGPT通过微调小型模型，仅用10,000步就达到了66.2%的准确率。", "conclusion": "尽管UrzaGPT的性能尚未达到领域专用模型的水平，但研究表明单独使用LLM进行选牌是可行的，并预示LLM未来能实现高性能、通用且易于更新的选牌AI。"}}
{"id": "2508.08285", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.08285", "abs": "https://arxiv.org/abs/2508.08285", "authors": ["Denis Janiak", "Jakub Binkowski", "Albert Sawczyn", "Bogdan Gabrys", "Ravid Schwartz-Ziv", "Tomasz Kajdanowicz"], "title": "The Illusion of Progress: Re-evaluating Hallucination Detection in LLMs", "comment": "Preprint, under review", "summary": "Large language models (LLMs) have revolutionized natural language processing,\nyet their tendency to hallucinate poses serious challenges for reliable\ndeployment. Despite numerous hallucination detection methods, their evaluations\noften rely on ROUGE, a metric based on lexical overlap that misaligns with\nhuman judgments. Through comprehensive human studies, we demonstrate that while\nROUGE exhibits high recall, its extremely low precision leads to misleading\nperformance estimates. In fact, several established detection methods show\nperformance drops of up to 45.9\\% when assessed using human-aligned metrics\nlike LLM-as-Judge. Moreover, our analysis reveals that simple heuristics based\non response length can rival complex detection techniques, exposing a\nfundamental flaw in current evaluation practices. We argue that adopting\nsemantically aware and robust evaluation frameworks is essential to accurately\ngauge the true performance of hallucination detection methods, ultimately\nensuring the trustworthiness of LLM outputs.", "AI": {"tldr": "当前LLM幻觉检测方法的评估指标（如ROUGE）与人类判断不符，导致性能估计误导。本研究通过人类研究发现ROUGE精度极低，并揭示现有检测方法在人类对齐指标下性能显著下降，甚至不如简单启发式方法。因此，需要采用语义感知且稳健的评估框架。", "motivation": "大型语言模型（LLMs）的幻觉问题严重阻碍了其可靠部署。尽管存在多种幻觉检测方法，但它们的评估常依赖于ROUGE等词汇重叠度指标，这些指标与人类判断不一致，可能导致对检测方法真实性能的误判。", "method": "本研究通过全面的“人类研究”（human studies）来评估幻觉检测方法的性能。研究将ROUGE指标与“LLM-as-Judge”等与人类判断更对齐的语义感知指标进行对比，并分析了基于响应长度的简单启发式方法与复杂检测技术的表现。", "result": "研究发现ROUGE指标虽然召回率高，但精度极低，导致对幻觉检测方法性能的评估具有误导性。当使用LLM-as-Judge等与人类判断对齐的指标进行评估时，多个已建立的检测方法性能下降高达45.9%。此外，分析显示基于响应长度的简单启发式方法甚至可以与复杂的检测技术相媲美，揭示了当前评估实践中存在的根本缺陷。", "conclusion": "为了准确衡量幻觉检测方法的真实性能并最终确保LLM输出的可靠性，采用语义感知且稳健的评估框架至关重要。当前基于词汇重叠的评估方法存在严重缺陷，需要被更符合人类判断的新方法取代。"}}
{"id": "2508.08507", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.08507", "abs": "https://arxiv.org/abs/2508.08507", "authors": ["Shaun Macdonald", "Salma ElSayed", "Mark McGill"], "title": "AZRA: Extending the Affective Capabilities of Zoomorphic Robots using Augmented Reality", "comment": "Companion of the 2025 ACM/IEEE International Conference on\n  Human-Robot Interaction (RO-MAN 2025)", "summary": "Zoomorphic robots could serve as accessible and practical alternatives for\nusers unable or unwilling to keep pets. However, their affective interactions\nare often simplistic and short-lived, limiting their potential for domestic\nadoption. In order to facilitate more dynamic and nuanced affective\ninteractions and relationships between users and zoomorphic robots we present\nAZRA, a novel augmented reality (AR) framework that extends the affective\ncapabilities of these robots without physical modifications. To demonstrate\nAZRA, we augment a zoomorphic robot, Petit Qoobo, with novel emotional displays\n(face, light, sound, thought bubbles) and interaction modalities (voice, touch,\nproximity, gaze). Additionally, AZRA features a computational model of emotion\nto calculate the robot's emotional responses, daily moods, evolving personality\nand needs. We highlight how AZRA can be used for rapid participatory\nprototyping and enhancing existing robots, then discuss implications on future\nzoomorphic robot development.", "AI": {"tldr": "提出AZRA，一个增强现实（AR）框架，通过虚拟显示和交互模式，在不进行物理修改的情况下，扩展了拟动物机器人的情感交互能力，使其更适合家庭使用。", "motivation": "拟动物机器人作为宠物替代品，其情感交互往往过于简单和短暂，限制了它们在家庭中的普及和用户建立长期情感联系的潜力。", "method": "开发了AZRA AR框架，通过在现有拟动物机器人（如Petit Qoobo）上叠加虚拟情感显示（面部、灯光、声音、思想泡泡）和交互模式（语音、触摸、接近、凝视）来增强其情感能力。此外，AZRA包含一个计算情感模型，用于计算机器人的情感反应、日常情绪、演变个性和需求。", "result": "展示了AZRA如何增强拟动物机器人的情感交互，使其更具动态性和细微性。该框架可用于快速参与式原型设计和现有机器人的功能增强。", "conclusion": "AZRA框架能够显著提升拟动物机器人的情感交互和关系建立能力，为未来拟动物机器人的发展提供了新的方向和可能性。"}}
{"id": "2508.08508", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.08508", "abs": "https://arxiv.org/abs/2508.08508", "authors": ["Aaditya Baranwal", "Madhav Kataria", "Naitik Agrawal", "Yogesh S Rawat", "Shruti Vyas"], "title": "Re:Verse -- Can Your VLM Read a Manga?", "comment": null, "summary": "Current Vision Language Models (VLMs) demonstrate a critical gap between\nsurface-level recognition and deep narrative reasoning when processing\nsequential visual storytelling. Through a comprehensive investigation of manga\nnarrative understanding, we reveal that while recent large multimodal models\nexcel at individual panel interpretation, they systematically fail at temporal\ncausality and cross-panel cohesion, core requirements for coherent story\ncomprehension. We introduce a novel evaluation framework that combines\nfine-grained multimodal annotation, cross-modal embedding analysis, and\nretrieval-augmented assessment to systematically characterize these\nlimitations.\n  Our methodology includes (i) a rigorous annotation protocol linking visual\nelements to narrative structure through aligned light novel text, (ii)\ncomprehensive evaluation across multiple reasoning paradigms, including direct\ninference and retrieval-augmented generation, and (iii) cross-modal similarity\nanalysis revealing fundamental misalignments in current VLMs' joint\nrepresentations. Applying this framework to Re:Zero manga across 11 chapters\nwith 308 annotated panels, we conduct the first systematic study of long-form\nnarrative understanding in VLMs through three core evaluation axes: generative\nstorytelling, contextual dialogue grounding, and temporal reasoning. Our\nfindings demonstrate that current models lack genuine story-level intelligence,\nstruggling particularly with non-linear narratives, character consistency, and\ncausal inference across extended sequences. This work establishes both the\nfoundation and practical methodology for evaluating narrative intelligence,\nwhile providing actionable insights into the capability of deep sequential\nunderstanding of Discrete Visual Narratives beyond basic recognition in\nMultimodal Models.", "AI": {"tldr": "当前视觉语言模型（VLMs）在处理顺序视觉叙事（如漫画）时，在表面识别与深层叙事推理之间存在显著差距，尤其在时间因果关系和跨面板连贯性方面表现不佳。本文引入了一个新的评估框架，系统地揭示了这些局限性，并发现现有模型缺乏真正的故事级智能。", "motivation": "当前的视觉语言模型（VLMs）在处理顺序视觉叙事（如漫画）时，虽然能很好地解释单个面板，但在深层叙事推理（如时间因果关系和跨面板连贯性）方面存在关键性缺陷。", "method": "引入了一个新颖的评估框架，该框架结合了：1) 精细的多模态标注，通过对齐的轻小说文本将视觉元素与叙事结构关联；2) 跨多种推理范式（直接推理和检索增强生成）的全面评估；3) 跨模态相似性分析，以揭示当前VLM联合表示中的根本性错位。该框架应用于《Re:Zero》漫画的11个章节（308个标注面板），通过生成式故事叙述、上下文对话接地和时间推理三个核心评估轴进行系统研究。", "result": "研究结果表明，当前模型缺乏真正的故事级智能，特别是在处理非线性叙事、角色一致性和跨长序列的因果推理方面表现挣扎。", "conclusion": "这项工作为评估叙事智能奠定了基础和实践方法，并为多模态模型在离散视觉叙事中超越基本识别的深度序列理解能力提供了可操作的见解。"}}
{"id": "2508.08385", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.08385", "abs": "https://arxiv.org/abs/2508.08385", "authors": ["Masataro Asai"], "title": "Bilevel MCTS for Amortized O(1) Node Selection in Classical Planning", "comment": null, "summary": "We study an efficient implementation of Multi-Armed Bandit (MAB)-based\nMonte-Carlo Tree Search (MCTS) for classical planning. One weakness of MCTS is\nthat it spends a significant time deciding which node to expand next. While\nselecting a node from an OPEN list with $N$ nodes has $O(1)$ runtime complexity\nwith traditional array-based priority-queues for dense integer keys, the\ntree-based OPEN list used by MCTS requires $O(\\log N)$, which roughly\ncorresponds to the search depth $d$. In classical planning, $d$ is arbitrarily\nlarge (e.g., $2^k-1$ in $k$-disk Tower-of-Hanoi) and the runtime for node\nselection is significant, unlike in game tree search, where the cost is\nnegligible compared to the node evaluation (rollouts) because $d$ is inherently\nlimited by the game (e.g., $d\\leq 361$ in Go). To improve this bottleneck, we\npropose a bilevel modification to MCTS that runs a best-first search from each\nselected leaf node with an expansion budget proportional to $d$, which achieves\namortized $O(1)$ runtime for node selection, equivalent to the traditional\nqueue-based OPEN list. In addition, we introduce Tree Collapsing, an\nenhancement that reduces action selection steps and further improves the\nperformance.", "AI": {"tldr": "针对经典规划中MCTS节点选择效率低的问题，提出了一种双层MCTS修改和树折叠技术，实现了摊销O(1)的节点选择时间，显著提升了性能。", "motivation": "MCTS在经典规划中，节点选择的时间复杂度为$O(\\log N)$，这大致对应于搜索深度$d$。在经典规划中，$d$可以非常大，导致节点选择成为显著的性能瓶颈，而在游戏树搜索中此开销则可忽略不计。", "method": "1. 提出MCTS的双层修改，从每个选定的叶节点开始运行一个最佳优先搜索，其扩展预算与深度$d$成比例，以实现摊销$O(1)$的节点选择运行时。2. 引入“树折叠”（Tree Collapsing）技术，以减少动作选择步骤并进一步提升性能。", "result": "成功实现了与传统队列式OPEN列表等效的摊销$O(1)$节点选择运行时。通过树折叠进一步提高了算法性能。", "conclusion": "所提出的双层MCTS修改和树折叠技术有效解决了经典规划中MCTS的节点选择瓶颈，显著提升了其效率。"}}
{"id": "2508.08287", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.08287", "abs": "https://arxiv.org/abs/2508.08287", "authors": ["Farah Atif", "Nursultan Askarbekuly", "Kareem Darwish", "Monojit Choudhury"], "title": "Sacred or Synthetic? Evaluating LLM Reliability and Abstention for Religious Questions", "comment": "8th AAAI/ACM Conference on AI, Ethics, and Society (AIES 2025)", "summary": "Despite the increasing usage of Large Language Models (LLMs) in answering\nquestions in a variety of domains, their reliability and accuracy remain\nunexamined for a plethora of domains including the religious domains. In this\npaper, we introduce a novel benchmark FiqhQA focused on the LLM generated\nIslamic rulings explicitly categorized by the four major Sunni schools of\nthought, in both Arabic and English. Unlike prior work, which either overlooks\nthe distinctions between religious school of thought or fails to evaluate\nabstention behavior, we assess LLMs not only on their accuracy but also on\ntheir ability to recognize when not to answer. Our zero-shot and abstention\nexperiments reveal significant variation across LLMs, languages, and legal\nschools of thought. While GPT-4o outperforms all other models in accuracy,\nGemini and Fanar demonstrate superior abstention behavior critical for\nminimizing confident incorrect answers. Notably, all models exhibit a\nperformance drop in Arabic, highlighting the limitations in religious reasoning\nfor languages other than English. To the best of our knowledge, this is the\nfirst study to benchmark the efficacy of LLMs for fine-grained Islamic school\nof thought specific ruling generation and to evaluate abstention for Islamic\njurisprudence queries. Our findings underscore the need for task-specific\nevaluation and cautious deployment of LLMs in religious applications.", "AI": {"tldr": "本文引入了FiqhQA基准，首次评估了大型语言模型（LLMs）在细粒度伊斯兰教法裁决生成方面的准确性和弃权行为，发现模型在不同语言和学派间表现差异显著，并强调了在宗教应用中谨慎部署LLMs的重要性。", "motivation": "尽管LLMs在各种领域中广泛应用，但其在宗教领域（如伊斯兰教法裁决）的可靠性和准确性尚未得到充分检验。现有工作往往忽略不同宗教思想流派间的区别，或未能评估模型何时应拒绝回答的能力。", "method": "引入了FiqhQA基准，该基准包含按四大逊尼派思想流派明确分类的伊斯兰裁决，并提供阿拉伯语和英语版本。研究不仅评估了LLMs的准确性，还评估了其识别何时不应回答（弃权行为）的能力。实验采用了零样本（zero-shot）和弃权（abstention）设置。", "result": "实验结果显示，LLMs在不同模型、语言和法律学派之间的表现存在显著差异。GPT-4o在准确性方面优于所有其他模型，而Gemini和Fanar在弃权行为上表现更佳，这对于最大程度地减少自信的错误回答至关重要。值得注意的是，所有模型在阿拉伯语中的性能均有所下降，这突显了LLMs在英语以外语言的宗教推理方面的局限性。", "conclusion": "研究结果强调了在宗教应用中需要进行特定任务评估和谨慎部署LLMs的重要性，以确保其可靠性和适用性。"}}
{"id": "2508.08574", "categories": ["cs.RO", "cs.MA", "I.2.9; I.2.11"], "pdf": "https://arxiv.org/pdf/2508.08574", "abs": "https://arxiv.org/abs/2508.08574", "authors": ["Ameya Agaskar", "Sriram Siva", "William Pickering", "Kyle O'Brien", "Charles Kekeh", "Ang Li", "Brianna Gallo Sarker", "Alicia Chua", "Mayur Nemade", "Charun Thattai", "Jiaming Di", "Isaac Iyengar", "Ramya Dharoor", "Dino Kirouani", "Jimmy Erskine", "Tamir Hegazy", "Scott Niekum", "Usman A. Khan", "Federico Pecora", "Joseph W. Durham"], "title": "DeepFleet: Multi-Agent Foundation Models for Mobile Robots", "comment": "25 pages, 10 figures, 2 tables", "summary": "We introduce DeepFleet, a suite of foundation models designed to support\ncoordination and planning for large-scale mobile robot fleets. These models are\ntrained on fleet movement data, including robot positions, goals, and\ninteractions, from hundreds of thousands of robots in Amazon warehouses\nworldwide. DeepFleet consists of four architectures that each embody a distinct\ninductive bias and collectively explore key points in the design space for\nmulti-agent foundation models: the robot-centric (RC) model is an\nautoregressive decision transformer operating on neighborhoods of individual\nrobots; the robot-floor (RF) model uses a transformer with cross-attention\nbetween robots and the warehouse floor; the image-floor (IF) model applies\nconvolutional encoding to a multi-channel image representation of the full\nfleet; and the graph-floor (GF) model combines temporal attention with graph\nneural networks for spatial relationships. In this paper, we describe these\nmodels and present our evaluation of the impact of these design choices on\nprediction task performance. We find that the robot-centric and graph-floor\nmodels, which both use asynchronous robot state updates and incorporate the\nlocalized structure of robot interactions, show the most promise. We also\npresent experiments that show that these two models can make effective use of\nlarger warehouses operation datasets as the models are scaled up.", "AI": {"tldr": "DeepFleet是一套为大规模移动机器人车队协调和规划设计的预训练基础模型，通过探索四种不同的架构，发现以机器人为中心（RC）和图-地面（GF）模型最有前景，且能有效利用大规模数据集进行扩展。", "motivation": "支持大规模移动机器人车队的协调和规划。", "method": "引入DeepFleet，一套包含四种不同架构的基础模型：机器人中心（RC）模型（自回归决策Transformer），机器人-地面（RF）模型（Transformer与交叉注意力），图像-地面（IF）模型（卷积编码），以及图-地面（GF）模型（时间注意力与图神经网络）。这些模型在亚马逊全球仓库数百万机器人的车队运动数据上进行训练，并评估了设计选择对预测任务性能的影响。", "result": "机器人中心（RC）模型和图-地面（GF）模型表现出最大的潜力，它们都使用了异步机器人状态更新并整合了机器人交互的局部结构。实验还表明，随着模型规模的扩大，这两种模型能有效利用更大的仓库运营数据集。", "conclusion": "机器人中心（RC）和图-地面（GF）模型由于其处理局部交互的能力和与数据规模共同扩展的潜力，在大规模机器人车队协调方面显示出巨大前景。"}}
{"id": "2508.08518", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08518", "abs": "https://arxiv.org/abs/2508.08518", "authors": ["Ilerioluwakiiye Abolade", "Emmanuel Idoko", "Solomon Odelola", "Promise Omoigui", "Adetola Adebanwo", "Aondana Iorumbur", "Udunna Anazodo", "Alessandro Crimi", "Raymond Confidence"], "title": "SharpXR: Structure-Aware Denoising for Pediatric Chest X-Rays", "comment": "Accepted at MICCAI 2025 MIRASOL Workshop, 10 pages, 5 figures", "summary": "Pediatric chest X-ray imaging is essential for early diagnosis, particularly\nin low-resource settings where advanced imaging modalities are often\ninaccessible. Low-dose protocols reduce radiation exposure in children but\nintroduce substantial noise that can obscure critical anatomical details.\nConventional denoising methods often degrade fine details, compromising\ndiagnostic accuracy. In this paper, we present SharpXR, a structure-aware\ndual-decoder U-Net designed to denoise low-dose pediatric X-rays while\npreserving diagnostically relevant features. SharpXR combines a\nLaplacian-guided edge-preserving decoder with a learnable fusion module that\nadaptively balances noise suppression and structural detail retention. To\naddress the scarcity of paired training data, we simulate realistic\nPoisson-Gaussian noise on the Pediatric Pneumonia Chest X-ray dataset. SharpXR\noutperforms state-of-the-art baselines across all evaluation metrics while\nmaintaining computational efficiency suitable for resource-constrained\nsettings. SharpXR-denoised images improved downstream pneumonia classification\naccuracy from 88.8% to 92.5%, underscoring its diagnostic value in low-resource\npediatric care.", "AI": {"tldr": "SharpXR是一种结构感知的双解码器U-Net，旨在对低剂量儿科胸部X射线图像进行去噪，同时保留诊断相关特征，并在资源受限环境下提高下游肺炎分类准确性。", "motivation": "在资源匮乏地区，先进影像设备难以获取，儿科胸部X射线成像至关重要。低剂量方案可减少儿童辐射暴露，但会引入大量噪声，模糊关键解剖细节。传统去噪方法会损害精细细节，影响诊断准确性。", "method": "本文提出SharpXR，一种结构感知的双解码器U-Net，用于低剂量儿科X射线去噪。它结合了拉普拉斯引导的边缘保留解码器和一个可学习的融合模块，自适应地平衡噪声抑制和结构细节保留。为解决配对训练数据稀缺问题，该方法在儿科肺炎胸部X射线数据集上模拟了真实的泊松-高斯噪声。", "result": "SharpXR在所有评估指标上均优于现有基线方法，同时保持了适用于资源受限环境的计算效率。经SharpXR去噪后的图像将下游肺炎分类准确率从88.8%提高到92.5%。", "conclusion": "SharpXR通过有效去噪和细节保留，显著提高了低剂量儿科胸部X射线的诊断价值，尤其适用于资源受限的儿科护理环境。"}}
{"id": "2508.08442", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08442", "abs": "https://arxiv.org/abs/2508.08442", "authors": ["Niklas Dewally", "Özgür Akgün"], "title": "Solver-Aided Expansion of Loops to Avoid Generate-and-Test", "comment": "13 pages, 4 figures, published in ModRef 2025 workshop", "summary": "Constraint modelling languages like MiniZinc and Essence rely on unrolling\nloops (in the form of quantified expressions and comprehensions) during\ncompilation. Standard approaches generate all combinations of induction\nvariables and use partial evaluation to discard those that simplify to identity\nelements of associative-commutative operators (e.g. true for conjunction, 0 for\nsummation). This can be inefficient for problems where most combinations are\nultimately irrelevant. We present a method that avoids full enumeration by\nusing a solver to compute only the combinations required to generate the final\nset of constraints. The resulting model is identical to that produced by\nconventional flattening, but compilation can be significantly faster. This\nimproves the efficiency of translating high-level user models into solver-ready\nform, particularly when induction variables range over large domains with\nselective preconditions.", "AI": {"tldr": "提出一种基于求解器的新方法，用于约束建模语言中循环展开，避免传统方法的全枚举低效问题，显著加速编译过程。", "motivation": "约束建模语言（如MiniZinc、Essence）在编译时需要展开循环，传统方法通过枚举所有归纳变量组合并部分求值来过滤，但当大多数组合不相关时，这种方法效率低下。", "method": "使用一个求解器来计算仅生成最终约束集所需的归纳变量组合，从而避免了完全枚举。", "result": "生成的结果模型与传统扁平化方法产生的模型相同，但编译速度显著加快。这在归纳变量范围大且存在选择性前置条件的问题中尤为有效。", "conclusion": "该方法提高了将高级用户模型转换为求解器可读形式的效率，特别适用于归纳变量域大且有选择性先决条件的问题。"}}
{"id": "2508.08292", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.LO", "cs.NE", "68T20, 68T05, 68Q32", "F.2.2; I.2.3; I.2.6; I.2.8"], "pdf": "https://arxiv.org/pdf/2508.08292", "abs": "https://arxiv.org/abs/2508.08292", "authors": ["Aryan Gulati", "Brando Miranda", "Eric Chen", "Emily Xia", "Kai Fronsdal", "Bruno Dumont", "Elyas Obbad", "Sanmi Koyejo"], "title": "Putnam-AXIOM: A Functional and Static Benchmark", "comment": "27 pages total (10-page main paper + 17-page appendix), 12 figures, 6\n  tables. Submitted to ICML 2025 (under review)", "summary": "Current mathematical reasoning benchmarks for large language models (LLMs)\nare approaching saturation, with some achieving > 90% accuracy, and are\nincreasingly compromised by training-set contamination. We introduce\nPutnam-AXIOM, a benchmark of 522 university-level competition problems drawn\nfrom the prestigious William Lowell Putnam Mathematical Competition, and\nPutnam-AXIOM Variation, an unseen companion set of 100 functional variants\ngenerated by programmatically perturbing variables and constants. The variation\nprotocol produces an unlimited stream of equally difficult, unseen instances --\nyielding a contamination-resilient test bed. On the Original set, OpenAI's\no1-preview -- the strongest evaluated model -- scores 41.9%, but its accuracy\ndrops by 19.6% (46.8% relative decrease) on the paired Variations. The\nremaining eighteen models show the same downward trend, ten of them with\nnon-overlapping 95% confidence intervals. These gaps suggest memorization and\nhighlight the necessity of dynamic benchmarks. We complement \"boxed\" accuracy\nwith Teacher-Forced Accuracy (TFA), a lightweight metric that directly scores\nreasoning traces and automates natural language proof evaluations. Putnam-AXIOM\ntherefore provides a rigorous, contamination-resilient evaluation framework for\nassessing advanced mathematical reasoning of LLMs. Data and evaluation code are\npublicly available at https://github.com/brando90/putnam-axiom.", "AI": {"tldr": "本文引入了Putnam-AXIOM，一个针对大型语言模型（LLMs）的大学级别数学推理基准，以解决现有基准饱和和数据污染问题。研究发现LLMs存在记忆化现象，并强调了动态基准的必要性。", "motivation": "当前的LLM数学推理基准已接近饱和（准确率超过90%），并且日益受到训练集污染的影响，这使得评估LLMs的真实推理能力变得困难。", "method": "研究引入了Putnam-AXIOM基准，包含522个普特南数学竞赛问题，以及Putnam-AXIOM Variation，一个由程序化扰动变量和常数生成的100个变体问题集。为了评估推理过程，还引入了“教师强制准确率”（Teacher-Forced Accuracy, TFA）指标。实验测试了包括OpenAI的o1-preview在内的19个模型。", "result": "在原始问题集上，最强的模型o1-preview得分41.9%，但在配对的变体问题集上，其准确率下降了19.6%（相对下降46.8%）。其余18个模型也表现出类似的下降趋势，其中10个模型的95%置信区间不重叠，这表明模型存在记忆化现象。", "conclusion": "LLMs在数学推理任务中存在记忆化行为，现有静态基准不足以准确评估其泛化能力。动态基准对于评估LLMs的真实推理能力至关重要。Putnam-AXIOM提供了一个严格且抗污染的评估框架，用于评估LLMs的高级数学推理能力。"}}
{"id": "2508.08576", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.08576", "abs": "https://arxiv.org/abs/2508.08576", "authors": ["Deniz Karanfil", "Daniel Lindmark", "Martin Servin", "David Torick", "Bahram Ravani"], "title": "Developing a Calibrated Physics-Based Digital Twin for Construction Vehicles", "comment": null, "summary": "This paper presents the development of a calibrated digital twin of a wheel\nloader. A calibrated digital twin integrates a construction vehicle with a\nhigh-fidelity digital model allowing for automated diagnostics and optimization\nof operations as well as pre-planning simulations enhancing automation\ncapabilities. The high-fidelity digital model is a virtual twin of the physical\nwheel loader. It uses a physics-based multibody dynamic model of the wheel\nloader in the software AGX Dynamics. Interactions of the wheel loader's bucket\nwhile in use in construction can be simulated in the virtual model. Calibration\nmakes this simulation of high-fidelity which can enhance realistic planning for\nautomation of construction operations. In this work, a wheel loader was\ninstrumented with several sensors used to calibrate the digital model. The\ncalibrated digital twin was able to estimate the magnitude of the forces on the\nbucket base with high accuracy, providing a high-fidelity simulation.", "AI": {"tldr": "本文开发了一个轮式装载机的校准数字孪生，用于高精度模拟、诊断和自动化规划。", "motivation": "旨在通过集成施工车辆与高保真数字模型，实现自动化诊断、操作优化以及增强自动化能力的预规划模拟。", "method": "构建了一个基于物理的多体动力学高保真数字模型（虚拟孪生），使用AGX Dynamics软件模拟轮式装载机。通过在物理装载机上安装传感器，收集数据对数字模型进行校准。", "result": "校准后的数字孪生能够高精度地估算铲斗底部受力的大小，提供了高保真度的模拟。", "conclusion": "开发的校准数字孪生实现了高保真模拟，能够准确估算铲斗受力，为施工操作的自动化规划和诊断优化提供了有力支持。"}}
{"id": "2508.08521", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08521", "abs": "https://arxiv.org/abs/2508.08521", "authors": ["Mansi Phute", "Ravikumar Balakrishnan"], "title": "VISOR: Visual Input-based Steering for Output Redirection in Vision-Language Models", "comment": null, "summary": "Vision Language Models (VLMs) are increasingly being used in a broad range of\napplications, bringing their security and behavioral control to the forefront.\nWhile existing approaches for behavioral control or output redirection, like\nsystem prompting in VLMs, are easily detectable and often ineffective,\nactivation-based steering vectors require invasive runtime access to model\ninternals--incompatible with API-based services and closed-source deployments.\nWe introduce VISOR (Visual Input-based Steering for Output Redirection), a\nnovel method that achieves sophisticated behavioral control through optimized\nvisual inputs alone. By crafting universal steering images that induce target\nactivation patterns, VISOR enables practical deployment across all VLM serving\nmodalities while remaining imperceptible compared to explicit textual\ninstructions. We validate VISOR on LLaVA-1.5-7B across three critical alignment\ntasks: refusal, sycophancy and survival instinct. A single 150KB steering image\nmatches steering vector performance within 1-2% for positive behavioral shifts\nwhile dramatically exceeding it for negative steering--achieving up to 25%\nshifts from baseline compared to steering vectors' modest changes. Unlike\nsystem prompting (3-4% shifts), VISOR provides robust bidirectional control\nwhile maintaining 99.9% performance on 14,000 unrelated MMLU tasks. Beyond\neliminating runtime overhead and model access requirements, VISOR exposes a\ncritical security vulnerability: adversaries can achieve sophisticated\nbehavioral manipulation through visual channels alone, bypassing text-based\ndefenses. Our work fundamentally re-imagines multimodal model control and\nhighlights the urgent need for defenses against visual steering attacks.", "AI": {"tldr": "VISOR是一种通过优化视觉输入实现VLM行为控制的新方法，它通过生成引导图像诱导目标激活模式，无需模型内部访问，且比现有方法更有效和隐蔽，同时也揭示了视觉通道的潜在安全漏洞。", "motivation": "现有VLM行为控制方法（如系统提示）易于检测且效果不佳；基于激活的引导向量需要侵入式运行时访问模型内部，不适用于API服务和闭源部署。因此，需要一种更实用、有效且隐蔽的行为控制方法。", "method": "引入VISOR方法，该方法通过仅优化视觉输入来实现复杂的行为控制。具体而言，它通过精心制作能诱导目标激活模式的通用引导图像，使VLM实现行为转向，且与显式文本指令相比不易察觉。", "result": "在LLaVA-1.5-7B上，单个150KB的引导图像在正向行为转变上与引导向量性能相近（1-2%差距），但在负向引导上远超引导向量，可实现高达25%的基线转变。与系统提示（3-4%转变）相比，VISOR提供强大的双向控制，同时在14,000个不相关的MMLU任务上保持99.9%的性能。此外，VISOR消除了运行时开销和模型访问要求，并揭示了攻击者仅通过视觉通道即可实现复杂行为操纵的安全漏洞。", "conclusion": "VISOR从根本上重塑了多模态模型的控制方式，通过视觉输入实现了强大且隐蔽的行为操纵。同时，它也突显了针对视觉引导攻击进行防御的紧迫需求。"}}
{"id": "2508.08446", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08446", "abs": "https://arxiv.org/abs/2508.08446", "authors": ["Woojeong Kim", "Junxiong Wang", "Jing Nathan Yan", "Mohamed Abdelfattah", "Alexander M. Rush"], "title": "OverFill: Two-Stage Models for Efficient Language Model Decoding", "comment": "Accepted to COLM 2025", "summary": "Large language models (LLMs) excel across diverse tasks but face significant\ndeployment challenges due to high inference costs. LLM inference comprises\nprefill (compute-bound) and decode (memory-bound) stages, with decode\ndominating latency particularly for long sequences. Current decoder-only models\nhandle both stages uniformly, despite their distinct computational profiles. We\npropose OverFill, which decouples these stages to optimize accuracy-efficiency\ntradeoffs. OverFill begins with a full model for prefill, processing system and\nuser inputs in parallel. It then switches to a dense pruned model, while\ngenerating tokens sequentially. Leveraging more compute during prefill,\nOverFill improves generation quality with minimal latency overhead. Our\n3B-to-1B OverFill configuration outperforms 1B pruned models by 83.2%, while\nthe 8B-to-3B configuration improves over 3B pruned models by 79.2% on average\nacross standard benchmarks. OverFill matches the performance of same-sized\nmodels trained from scratch, while using significantly less training data. Our\ncode is available at https://github.com/friendshipkim/overfill.", "AI": {"tldr": "OverFill通过解耦大型语言模型（LLM）的预填充和解码阶段，分别使用完整模型和剪枝模型，显著提高了推理效率和生成质量。", "motivation": "大型语言模型推理成本高昂，特别是解码阶段在长序列中占主导地位。当前模型对计算特性不同的预填充和解码阶段采用统一处理，导致效率低下。", "method": "OverFill将预填充（计算密集型）和解码（内存密集型）阶段解耦。预填充阶段使用完整模型并行处理输入，然后切换到密集剪枝模型进行顺序令牌生成。通过在预填充阶段投入更多计算资源，优化了准确性-效率权衡。", "result": "OverFill在生成质量上有所提升，且延迟开销极小。3B-to-1B配置比1B剪枝模型性能提高83.2%；8B-to-3B配置比3B剪枝模型平均提高79.2%。它能匹配相同大小从头训练的模型性能，同时显著减少训练数据。", "conclusion": "OverFill通过对LLM推理阶段的优化处理，提供了一种有效提升生成质量和效率的方法，显著优于传统的剪枝模型，并能与从头训练的模型性能媲美。"}}
{"id": "2508.08386", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.08386", "abs": "https://arxiv.org/abs/2508.08386", "authors": ["Shuzhou Yuan", "William LaCroix", "Hardik Ghoshal", "Ercong Nie", "Michael Färber"], "title": "CoDAE: Adapting Large Language Models for Education via Chain-of-Thought Data Augmentation", "comment": null, "summary": "Large Language Models (LLMs) are increasingly employed as AI tutors due to\ntheir scalability and potential for personalized instruction. However,\noff-the-shelf LLMs often underperform in educational settings: they frequently\nreveal answers too readily, fail to adapt their responses to student\nuncertainty, and remain vulnerable to emotionally manipulative prompts. To\naddress these challenges, we introduce CoDAE, a framework that adapts LLMs for\neducational use through Chain-of-Thought (CoT) data augmentation. We collect\nreal-world dialogues between students and a ChatGPT-based tutor and enrich them\nusing CoT prompting to promote step-by-step reasoning and pedagogically aligned\nguidance. Furthermore, we design targeted dialogue cases to explicitly mitigate\nthree key limitations: over-compliance, low response adaptivity, and threat\nvulnerability. We fine-tune four open-source LLMs on different variants of the\naugmented datasets and evaluate them in simulated educational scenarios using\nboth automatic metrics and LLM-as-a-judge assessments. Our results show that\nmodels fine-tuned with CoDAE deliver more pedagogically appropriate guidance,\nbetter support reasoning processes, and effectively resist premature answer\ndisclosure.", "AI": {"tldr": "CoDAE框架通过思维链数据增强，提升大型语言模型在教育场景中的辅导能力，使其提供更具教学性、适应性并抵抗不良诱导的指导。", "motivation": "现成的大型语言模型作为AI导师存在不足，包括过早泄露答案、响应适应性差以及易受情感操控提示影响。", "method": "引入CoDAE框架，通过思维链（CoT）数据增强来适应教育用途。收集真实师生对话，利用CoT提示丰富数据，以促进逐步推理和符合教学的指导。设计针对性对话案例，以缓解过度顺从、低响应适应性和威胁脆弱性。在增强数据集的不同变体上微调四种开源LLM，并使用自动指标和LLM作为评判者进行评估。", "result": "通过CoDAE微调的模型能提供更符合教学的指导，更好地支持推理过程，并有效抵抗过早答案泄露。", "conclusion": "CoDAE框架通过数据增强和针对性案例设计，成功地将大型语言模型适应于教育环境，显著提高了其作为AI导师的性能和鲁棒性。"}}
{"id": "2508.08607", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.08607", "abs": "https://arxiv.org/abs/2508.08607", "authors": ["Justin London"], "title": "Autonomous Mobile Plant Watering Robot : A Kinematic Approach", "comment": null, "summary": "Plants need regular and the appropriate amount of watering to thrive and\nsurvive. While agricultural robots exist that can spray water on plants and\ncrops such as the , they are expensive and have limited mobility and/or\nfunctionality. We introduce a novel autonomous mobile plant watering robot that\nuses a 6 degree of freedom (DOF) manipulator, connected to a 4 wheel drive\nalloy chassis, to be able to hold a garden hose, recognize and detect plants,\nand to water them with the appropriate amount of water by being able to insert\na soil humidity/moisture sensor into the soil. The robot uses Jetson Nano and\nArduino microcontroller and real sense camera to perform computer vision to\ndetect plants using real-time YOLOv5 with the Pl@ntNet-300K dataset. The robot\nuses LIDAR for object and collision avoideance and does not need to move on a\npre-defined path and can keep track of which plants it has watered. We provide\nthe Denavit-Hartenberg (DH) Table, forward kinematics, differential driving\nkinematics, and inverse kinematics along with simulation and experiment results", "AI": {"tldr": "本文介绍了一种新型的自主移动植物浇水机器人，它配备了6自由度机械臂和传感器，能够识别植物、检测土壤湿度并进行精准浇水，同时具备自主导航和避障能力。", "motivation": "现有的农业机器人成本高昂、移动性或功能有限，而植物需要定期和适量的浇水才能茁壮成长。因此，需要一种更经济、功能更全面的自主浇水解决方案。", "method": "该机器人采用四轮驱动合金底盘，连接一个6自由度机械臂，用于握持水管和插入土壤湿度传感器。它使用Jetson Nano和Arduino微控制器，通过RealSense摄像头结合YOLOv5和Pl@ntNet-300K数据集进行实时植物检测。LIDAR用于物体和碰撞避障，实现非预定义路径的自主移动。文中还提供了Denavit-Hartenberg (DH) 表、正向运动学、差分驱动运动学和逆向运动学。", "result": "该机器人能够识别和检测植物，将土壤湿度传感器插入土壤以确定适当的水量进行浇水，并通过LIDAR实现自主导航和避障，无需预设路径，并能记录已浇水的植物。文中提供了仿真和实验结果。", "conclusion": "研究成功开发了一种新颖的自主移动植物浇水机器人，它克服了现有农业机器人成本高和功能受限的问题，实现了植物的智能识别、精准浇水和自主导航。"}}
{"id": "2508.08537", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08537", "abs": "https://arxiv.org/abs/2508.08537", "authors": ["Anh Le", "Asanobu Kitamoto"], "title": "Training Kindai OCR with parallel textline images and self-attention feature distance-based loss", "comment": null, "summary": "Kindai documents, written in modern Japanese from the late 19th to early 20th\ncentury, hold significant historical value for researchers studying societal\nstructures, daily life, and environmental conditions of that period. However,\ntranscribing these documents remains a labor-intensive and time-consuming task,\nresulting in limited annotated data for training optical character recognition\n(OCR) systems. This research addresses this challenge of data scarcity by\nleveraging parallel textline images - pairs of original Kindai text and their\ncounterparts in contemporary Japanese fonts - to augment training datasets. We\nintroduce a distance-based objective function that minimizes the gap between\nself-attention features of the parallel image pairs. Specifically, we explore\nEuclidean distance and Maximum Mean Discrepancy (MMD) as domain adaptation\nmetrics. Experimental results demonstrate that our method reduces the character\nerror rate (CER) by 2.23% and 3.94% over a Transformer-based OCR baseline when\nusing Euclidean distance and MMD, respectively. Furthermore, our approach\nimproves the discriminative quality of self-attention representations, leading\nto more effective OCR performance for historical documents.", "AI": {"tldr": "本研究通过利用并行文本行图像（近代文本与现代字体对应），并引入基于距离的目标函数（如欧氏距离和MMD）来缩小自注意力特征差距，从而增强训练数据集，显著提升了近代文献OCR系统的性能。", "motivation": "近代文献具有重要历史价值，但其转录工作耗时耗力，导致用于训练OCR系统的标注数据稀缺，限制了OCR技术在该领域的应用和发展。", "method": "利用并行文本行图像（原始近代文本及其现代日文字体对应）来扩充训练数据集。引入一种基于距离的目标函数，旨在最小化并行图像对自注意力特征之间的差距。具体探索了欧氏距离和最大均值差异（MMD）作为域适应度量。", "result": "实验结果表明，该方法在Transformer-based OCR基线上，使用欧氏距离和MMD分别将字符错误率（CER）降低了2.23%和3.94%。此外，该方法还提高了自注意力表示的判别质量。", "conclusion": "该方法通过有效解决数据稀缺问题并提升自注意力表示的判别能力，显著改善了历史文献的OCR性能，为近代文献的数字化提供了更有效的解决方案。"}}
{"id": "2508.08477", "categories": ["cs.AI", "cs.DM"], "pdf": "https://arxiv.org/pdf/2508.08477", "abs": "https://arxiv.org/abs/2508.08477", "authors": ["Joan Salvà Soler", "Grégoire de Lambertye"], "title": "A Fast GRASP Metaheuristic for the Trigger Arc TSP with MIP-Based Construction and Multi-Neighborhood Local Search", "comment": "9 pages, 2 figures, 2-column format", "summary": "The Trigger Arc Traveling Salesman Problem (TA-TSP) extends the classical TSP\nby introducing dynamic arc costs that change when specific \\textit{trigger}\narcs are traversed, modeling scenarios such as warehouse operations with\ncompactable storage systems. This paper introduces a GRASP-based metaheuristic\nthat combines multiple construction heuristics with a multi-neighborhood local\nsearch. The construction phase uses mixed-integer programming (MIP) techniques\nto transform the TA-TSP into a sequence of tailored TSP instances, while the\nimprovement phase applies 2-Opt, Swap, and Relocate operators. Computational\nexperiments on MESS 2024 competition instances achieved average optimality gaps\nof 0.77\\% and 0.40\\% relative to the best-known solutions within a 60-second\nlimit. On smaller, synthetically generated datasets, the method produced\nsolutions 11.3\\% better than the Gurobi solver under the same time constraints.\nThe algorithm finished in the top three at MESS 2024, demonstrating its\nsuitability for real-time routing applications with state-dependent travel\ncosts.", "AI": {"tldr": "该论文提出了一个基于GRASP的元启发式算法，用于解决触发弧旅行商问题（TA-TSP），该问题引入了随触发弧遍历而变化的动态弧成本。该算法在MESS 2024竞赛中表现出色，证明了其在具有状态依赖旅行成本的实时路径规划应用中的适用性。", "motivation": "经典的旅行商问题（TSP）无法模拟某些现实场景，例如仓库操作中可压缩存储系统导致的动态弧成本变化。触发弧旅行商问题（TA-TSP）旨在解决这一限制，即当特定“触发”弧被遍历时，其他弧的成本会发生变化。", "method": "该研究引入了一个基于GRASP的元启发式算法。构建阶段利用混合整数规划（MIP）技术将TA-TSP转换为一系列定制的TSP实例。改进阶段应用了2-Opt、Swap和Relocate等邻域搜索操作。", "result": "在MESS 2024竞赛实例上，该算法在60秒限制内相对于已知最优解取得了0.77%和0.40%的平均最优性差距。在较小的合成数据集上，该方法在相同时间限制下比Gurobi求解器生成了11.3%更好的解决方案。该算法在MESS 2024竞赛中位列前三名。", "conclusion": "该算法在MESS 2024竞赛中的优异表现证明了其在具有状态依赖旅行成本的实时路径规划应用中的适用性。"}}
{"id": "2508.08401", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.08401", "abs": "https://arxiv.org/abs/2508.08401", "authors": ["Jiatong Li", "Weida Wang", "Qinggang Zhang", "Junxian Li", "Di Zhang", "Changmeng Zheng", "Shufei Zhang", "Xiaoyong Wei", "Qing Li"], "title": "Mol-R1: Towards Explicit Long-CoT Reasoning in Molecule Discovery", "comment": "20 pages", "summary": "Large language models (LLMs), especially Explicit Long Chain-of-Thought (CoT)\nreasoning models like DeepSeek-R1 and QWQ, have demonstrated powerful reasoning\ncapabilities, achieving impressive performance in commonsense reasoning and\nmathematical inference. Despite their effectiveness, Long-CoT reasoning models\nare often criticized for their limited ability and low efficiency in\nknowledge-intensive domains such as molecule discovery. Success in this field\nrequires a precise understanding of domain knowledge, including molecular\nstructures and chemical principles, which is challenging due to the inherent\ncomplexity of molecular data and the scarcity of high-quality expert\nannotations. To bridge this gap, we introduce Mol-R1, a novel framework\ndesigned to improve explainability and reasoning performance of R1-like\nExplicit Long-CoT reasoning LLMs in text-based molecule generation. Our\napproach begins with a high-quality reasoning dataset curated through Prior\nRegulation via In-context Distillation (PRID), a dedicated distillation\nstrategy to effectively generate paired reasoning traces guided by prior\nregulations. Building upon this, we introduce MoIA, Molecular Iterative\nAdaptation, a sophisticated training strategy that iteratively combines\nSupervised Fine-tuning (SFT) with Reinforced Policy Optimization (RPO),\ntailored to boost the reasoning performance of R1-like reasoning models for\nmolecule discovery. Finally, we examine the performance of Mol-R1 in the\ntext-based molecule reasoning generation task, showing superior performance\nagainst existing baselines.", "AI": {"tldr": "Mol-R1是一个新框架，通过高质量数据集（PRID）和迭代训练策略（MoIA）提升了类R1长链思维LLM在文本分子生成中的可解释性和推理性能，优于现有基线。", "motivation": "尽管大型语言模型（LLMs），特别是长链思维（CoT）推理模型，在常识和数学推理方面表现出色，但它们在分子发现等知识密集型领域效率低下且能力有限，因为这需要精确理解复杂的分子数据和稀缺的高质量专家标注。", "method": "本文提出了Mol-R1框架。首先，通过“上下文蒸馏先验调节”（PRID）策略构建高质量推理数据集。其次，引入“分子迭代适应”（MoIA）训练策略，该策略迭代结合监督微调（SFT）和强化策略优化（RPO），以提升类R1模型在分子发现中的推理性能。", "result": "Mol-R1在文本分子推理生成任务中表现出卓越的性能，优于现有基线。", "conclusion": "Mol-R1框架通过结合创新的数据蒸馏和迭代训练策略，成功弥补了现有LLM在分子发现领域可解释性和推理能力方面的不足，显著提升了该领域模型的性能。"}}
{"id": "2508.08624", "categories": ["cs.RO", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.08624", "abs": "https://arxiv.org/abs/2508.08624", "authors": ["Chenxuan Liu", "He Li", "Zongze Li", "Shuai Wang", "Wei Xu", "Kejiang Ye", "Derrick Wing Kwan Ng", "Chengzhong Xu"], "title": "Communication Efficient Robotic Mixed Reality with Gaussian Splatting Cross-Layer Optimization", "comment": "14 pages, 18 figures, to appear in IEEE Transactions on Cognitive\n  Communications and Networking", "summary": "Realizing low-cost communication in robotic mixed reality (RoboMR) systems\npresents a challenge, due to the necessity of uploading high-resolution images\nthrough wireless channels. This paper proposes Gaussian splatting (GS) RoboMR\n(GSMR), which enables the simulator to opportunistically render a\nphoto-realistic view from the robot's pose by calling ``memory'' from a GS\nmodel, thus reducing the need for excessive image uploads. However, the GS\nmodel may involve discrepancies compared to the actual environments. To this\nend, a GS cross-layer optimization (GSCLO) framework is further proposed, which\njointly optimizes content switching (i.e., deciding whether to upload image or\nnot) and power allocation (i.e., adjusting to content profiles) across\ndifferent frames by minimizing a newly derived GSMR loss function. The GSCLO\nproblem is addressed by an accelerated penalty optimization (APO) algorithm\nthat reduces computational complexity by over $10$x compared to traditional\nbranch-and-bound and search algorithms. Moreover, variants of GSCLO are\npresented to achieve robust, low-power, and multi-robot GSMR. Extensive\nexperiments demonstrate that the proposed GSMR paradigm and GSCLO method\nachieve significant improvements over existing benchmarks on both wheeled and\nlegged robots in terms of diverse metrics in various scenarios. For the first\ntime, it is found that RoboMR can be achieved with ultra-low communication\ncosts, and mixture of data is useful for enhancing GS performance in dynamic\nscenarios.", "AI": {"tldr": "本文提出了一种基于高斯泼溅（GS）的机器人混合现实（RoboMR）系统（GSMR），通过从GS模型渲染视图来大幅降低通信成本。为解决GS模型与实际环境的差异，进一步提出了GS跨层优化（GSCLO）框架，并设计了加速罚函数优化（APO）算法来高效解决该问题，实现了超低通信开销。", "motivation": "在机器人混合现实（RoboMR）系统中，通过无线信道上传高分辨率图像是实现低成本通信面临的挑战。", "method": "本文提出了高斯泼溅RoboMR（GSMR），允许模拟器通过调用GS模型的“记忆”来从机器人姿态渲染逼真的视图，从而减少不必要的图像上传。为解决GS模型与实际环境的差异，进一步提出了GS跨层优化（GSCLO）框架，该框架联合优化了跨帧的内容切换（是否上传图像）和功率分配，以最小化新推导的GSMR损失函数。GSCLO问题通过加速罚函数优化（APO）算法解决，该算法将计算复杂度降低了10倍以上。此外，还提出了GSCLO的变体以实现鲁棒、低功耗和多机器人的GSMR。", "result": "APO算法相比传统方法将计算复杂度降低了10倍以上。所提出的GSMR范式和GSCLO方法在轮式和腿式机器人上，在各种场景和多样化指标方面，均显著优于现有基准。首次发现RoboMR可以实现超低通信成本，并且在动态场景中，混合数据有助于增强GS性能。", "conclusion": "通过结合高斯泼溅模型和跨层优化框架，RoboMR系统能够实现超低通信成本。此外，在动态场景中，混合数据对提升GS性能至关重要。"}}
{"id": "2508.08547", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08547", "abs": "https://arxiv.org/abs/2508.08547", "authors": ["Wenhao Liang", "Wei Emma Zhang", "Lin Yue", "Miao Xu", "Olaf Maennel", "Weitong Chen"], "title": "Calibration Attention: Instance-wise Temperature Scaling for Vision Transformers", "comment": "UnderReview", "summary": "Probability calibration is critical when Vision Transformers are deployed in\nrisk-sensitive applications. The standard fix, post-hoc temperature scaling,\nuses a single global scalar and requires a held-out validation set. We\nintroduce Calibration Attention (CalAttn), a drop-in module that learns an\nadaptive, per-instance temperature directly from the ViT's CLS token. Across\nCIFAR-10/100, MNIST, Tiny-ImageNet, and ImageNet-1K, CalAttn reduces\ncalibration error by up to 4x on ViT-224, DeiT, and Swin, while adding under\n0.1 percent additional parameters. The learned temperatures cluster tightly\naround 1.0, in contrast to the large global values used by standard temperature\nscaling. CalAttn is simple, efficient, and architecture-agnostic, and yields\nmore trustworthy probabilities without sacrificing accuracy. Code:\n[https://github.com/EagleAdelaide/CalibrationAttention-CalAttn-](https://github.com/EagleAdelaide/CalibrationAttention-CalAttn-)", "AI": {"tldr": "本文提出CalAttn模块，为视觉Transformer提供自适应的逐实例温度标定，显著降低校准误差，同时保持高效率和低参数量。", "motivation": "在风险敏感应用中，视觉Transformer的概率校准至关重要。传统的温度标定方法使用单一全局标量且需要独立的验证集，存在局限性。", "method": "引入Calibration Attention (CalAttn)，这是一个可即插即用的模块，它直接从ViT的CLS token学习自适应的、逐实例的温度。", "result": "在CIFAR-10/100、MNIST、Tiny-ImageNet和ImageNet-1K等数据集上，CalAttn将ViT-224、DeiT和Swin模型的校准误差降低了高达4倍，而额外参数量不到0.1%。学习到的温度紧密聚集在1.0附近，与标准温度标定使用的较大全局值形成对比。", "conclusion": "CalAttn简单、高效、与架构无关，能在不牺牲准确性的前提下，提供更值得信赖的概率输出。"}}
{"id": "2508.08486", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08486", "abs": "https://arxiv.org/abs/2508.08486", "authors": ["Parker Whitfill", "Stewy Slocum"], "title": "Beyond Ordinal Preferences: Why Alignment Needs Cardinal Human Feedback", "comment": null, "summary": "Alignment techniques for LLMs rely on optimizing preference-based objectives\n-- where these preferences are typically elicited as ordinal, binary choices\nbetween responses. Recent work has focused on improving label quality or\nmitigating particular biases, but we identify a more fundamental limitation:\nthese methods collect the wrong kind of data. We prove an impossibility result:\nno algorithm relying solely on ordinal comparisons can systematically recover\nthe most preferred model. Intuitively, ordinal data lacks the information\nneeded to resolve tradeoffs -- e.g., fixing a factual error on one prompt\nversus improving style on another. We show that selecting the optimal model\nrequires recovering preferences over \\emph{models} (rather than just\nresponses), which can only be identified given cardinal feedback about response\nquality. To address this, we collect and publicly release a dataset of 25,000\ncardinal judgments using willingness-to-pay elicitations, a well-established\ntool from experimental economics. Empirically, we find that incorporating\ncardinal feedback into preference fine-tuning allows models to prioritize\nhigh-impact improvements and outperform ordinal-only methods on downstream\nbenchmarks, such as Arena-Hard.", "AI": {"tldr": "现有LLM对齐技术依赖的序数偏好数据存在根本性局限，无法系统性找到最优模型。本研究证明了这一点，并提出需要基数偏好数据，通过收集基数判断数据集并将其融入微调，显著提升了模型性能。", "motivation": "LLM对齐技术通常依赖二元选择的序数偏好数据，但这种数据在解决权衡问题时信息不足，无法系统地识别和恢复最受偏爱的模型。研究旨在解决这一根本性限制。", "method": "1. 证明了仅依赖序数比较的算法无法系统性恢复最受偏爱模型的“不可能结果”。2. 提出选择最优模型需要恢复对“模型”的偏好，这需要关于响应质量的基数反馈。3. 收集并公开发布了一个包含25,000个基数判断的数据集，使用实验经济学中成熟的“支付意愿”方法。4. 将基数反馈整合到偏好微调中。", "result": "1. 理论上证明了序数数据在LLM对齐中的局限性。2. 经验发现，将基数反馈纳入偏好微调后，模型能够优先考虑高影响力的改进。3. 使用基数反馈的模型在Arena-Hard等下游基准测试中，性能优于仅使用序数方法训练的模型。", "conclusion": "传统的LLM对齐方法因仅依赖序数偏好数据而存在根本性缺陷。基数偏好数据对于识别最优模型和解决对齐中的权衡至关重要。整合基数反馈能显著提升模型性能，是未来LLM对齐研究的关键方向。"}}
{"id": "2508.08424", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08424", "abs": "https://arxiv.org/abs/2508.08424", "authors": ["Saketh Reddy Vemula", "Dipti Mishra Sharma", "Parameswari Krishnamurthy"], "title": "Rethinking Tokenization for Rich Morphology: The Dominance of Unigram over BPE and Morphological Alignment", "comment": null, "summary": "Prior work on language modeling showed conflicting findings about whether\nmorphologically aligned approaches to tokenization improve performance,\nparticularly for languages with complex morphology. To investigate this, we\nselect a typologically diverse set of languages: Telugu (agglutinative), Hindi\n(primarily fusional with some agglutination), and English (fusional). We\nconduct a comprehensive evaluation of language models -- starting from\ntokenizer training and extending through the finetuning and downstream task\nevaluation. To account for the consistent performance differences observed\nacross tokenizer variants, we focus on two key factors: morphological alignment\nand tokenization quality. To assess morphological alignment of tokenizers in\nTelugu, we create a dataset containing gold morpheme segmentations of 600\nderivational and 7000 inflectional word forms.\n  Our experiments reveal that better morphological alignment correlates\npositively -- though moderately -- with performance in syntax-based tasks such\nas Parts-of-Speech tagging, Named Entity Recognition and Dependency Parsing.\nHowever, we also find that the tokenizer algorithm (Byte-pair Encoding vs.\nUnigram) plays a more significant role in influencing downstream performance\nthan morphological alignment alone. Naive Unigram tokenizers outperform others\nacross most settings, though hybrid tokenizers that incorporate morphological\nsegmentation significantly improve performance within the BPE framework. In\ncontrast, intrinsic metrics like Corpus Token Count (CTC) and R\\'enyi entropy\nshowed no correlation with downstream performance.", "AI": {"tldr": "研究发现，在语言模型中，词法对齐与句法任务表现呈正相关（中等程度），但分词算法本身（如Unigram优于BPE）对下游任务性能的影响更大。混合分词器（BPE+词法）在BPE框架内能显著提升性能。", "motivation": "先前关于语言模型中词法对齐分词方法是否能提升性能，特别是对于形态复杂语言的结论存在矛盾，因此需要进行深入探究。", "method": "选择了三种类型学上多样化的语言（泰卢固语、印地语、英语）进行研究。对语言模型进行了从分词器训练到微调和下游任务评估的全面评估。创建了一个泰卢固语的词法切分金标数据集。重点分析了词法对齐和分词质量两个关键因素，并比较了BPE和Unigram等不同分词算法。", "result": "词法对齐与句法相关任务（如词性标注、命名实体识别、依存句法分析）的性能呈正相关，但相关性为中等。分词算法本身对下游性能的影响比单独的词法对齐更显著。朴素的Unigram分词器在大多数设置下表现优于其他分词器。在BPE框架内，结合词法切分的混合分词器能显著提升性能。然而，语料库词元计数（CTC）和Rényi熵等内在指标与下游性能无关联。", "conclusion": "词法对齐对语言模型性能有益，尤其是在句法相关任务上，但分词算法的选择（如Unigram的优越性）和分词质量更为关键。在BPE框架下引入词法信息可有效提升性能。"}}
{"id": "2508.08690", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.08690", "abs": "https://arxiv.org/abs/2508.08690", "authors": ["Zhenjiang Wang", "Yunhua Jiang", "Zikun Zhen", "Yifan Jiang", "Yubin Tan", "Wubin Wang"], "title": "ZS-Puffin: Design, Modeling and Implementation of an Unmanned Aerial-Aquatic Vehicle with Amphibious Wings", "comment": "Accepted to IROS 2025", "summary": "Unmanned aerial-aquatic vehicles (UAAVs) can operate both in the air and\nunderwater, giving them broad application prospects. Inspired by the\ndual-function wings of puffins, we propose a UAAV with amphibious wings to\naddress the challenge posed by medium differences on the vehicle's propulsion\nsystem. The amphibious wing, redesigned based on a fixed-wing structure,\nfeatures a single degree of freedom in pitch and requires no additional\ncomponents. It can generate lift in the air and function as a flapping wing for\npropulsion underwater, reducing disturbance to marine life and making it\nenvironmentally friendly. Additionally, an artificial central pattern generator\n(CPG) is introduced to enhance the smoothness of the flapping motion. This\npaper presents the prototype, design details, and practical implementation of\nthis concept.", "AI": {"tldr": "受海鹦启发，本文提出一种新型两栖无人飞行器（UAAV），其两栖机翼可在空中产生升力，在水下作为扑翼推进，并引入人工中央模式发生器（CPG）以平滑扑翼运动，实现环境友好的两栖操作。", "motivation": "无人两栖飞行器（UAAV）具有广泛应用前景，但不同介质（空气和水）对推进系统造成的挑战是其研发的关键问题。", "method": "设计了一种基于固定翼结构、具有单个俯仰自由度的两栖机翼，无需额外部件即可在空中产生升力并在水下作为扑翼推进。此外，引入人工中央模式发生器（CPG）以提高扑翼运动的平滑性。论文还介绍了原型、设计细节和实际实现。", "result": "所提出的两栖机翼能在空中产生升力，并在水下作为扑翼进行推进，减少对海洋生物的干扰，实现环保操作。人工CPG的引入有效增强了扑翼运动的平滑性。", "conclusion": "该研究成功设计并实现了具有两栖机翼的UAAV概念，通过巧妙的结构设计和CPG控制，有效解决了不同介质下的推进挑战，并展现出良好的环境友好性。"}}
{"id": "2508.08549", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08549", "abs": "https://arxiv.org/abs/2508.08549", "authors": ["Wei Li", "Pengcheng Zhou", "Linye Ma", "Wenyi Zhao", "Huihua Yang"], "title": "Boosting Generic Semi-Supervised Medical Image Segmentation via Diverse Teaching and Label Propagation", "comment": null, "summary": "Both limited annotation and domain shift are significant challenges\nfrequently encountered in medical image segmentation, leading to derivative\nscenarios like semi-supervised medical (SSMIS), semi-supervised medical domain\ngeneralization (Semi-MDG) and unsupervised medical domain adaptation (UMDA).\nConventional methods are generally tailored to specific tasks in isolation, the\nerror accumulation hinders the effective utilization of unlabeled data and\nlimits further improvements, resulting in suboptimal performance when these\nissues occur. In this paper, we aim to develop a generic framework that masters\nall three tasks. We found that the key to solving the problem lies in how to\ngenerate reliable pseudo labels for the unlabeled data in the presence of\ndomain shift with labeled data and increasing the diversity of the model. To\ntackle this issue, we employ a Diverse Teaching and Label Propagation Network\n(DTLP-Net) to boosting the Generic Semi-Supervised Medical Image Segmentation.\nOur DTLP-Net involves a single student model and two diverse teacher models,\nwhich can generate reliable pseudo-labels for the student model. The first\nteacher model decouple the training process with labeled and unlabeled data,\nThe second teacher is momentum-updated periodically, thus generating reliable\nyet divers pseudo-labels. To fully utilize the information within the data, we\nadopt inter-sample and intra-sample data augmentation to learn the global and\nlocal knowledge. In addition, to further capture the voxel-level correlations,\nwe propose label propagation to enhance the model robust. We evaluate our\nproposed framework on five benchmark datasets for SSMIS, UMDA, and Semi-MDG\ntasks. The results showcase notable improvements compared to state-of-the-art\nmethods across all five settings, indicating the potential of our framework to\ntackle more challenging SSL scenarios.", "AI": {"tldr": "针对医疗图像分割中有限标注和域偏移的挑战，本文提出一个通用框架DTLP-Net，通过生成可靠伪标签和增加模型多样性，同时处理半监督、半监督域泛化和无监督域适应任务。", "motivation": "医疗图像分割常面临标注数据有限和域偏移问题，导致半监督医疗图像分割 (SSMIS)、半监督医疗域泛化 (Semi-MDG) 和无监督医疗域适应 (UMDA) 等场景。现有方法通常孤立地解决特定任务，存在误差累积，未能有效利用无标注数据，导致性能不佳。", "method": "本文提出一个通用框架——多样化教学和标签传播网络 (DTLP-Net)。该网络包含一个学生模型和两个多样化的教师模型，用于生成可靠的伪标签：第一个教师模型解耦有标签和无标签数据的训练过程，第二个教师模型通过周期性动量更新产生多样性伪标签。此外，采用样本间和样本内数据增强来学习全局和局部知识，并引入标签传播以增强模型对体素级别相关性的捕获和鲁棒性。", "result": "在五个基准数据集上对SSMIS、UMDA和Semi-MDG任务进行评估。结果显示，与最先进的方法相比，该框架在所有五种设置下都取得了显著改进。", "conclusion": "所提出的框架在处理医疗图像分割中有限标注和域偏移的复杂半监督场景方面表现出巨大潜力，能够有效解决SSMIS、UMDA和Semi-MDG等多种任务。"}}
{"id": "2508.08493", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08493", "abs": "https://arxiv.org/abs/2508.08493", "authors": ["Szymon Jakubicz", "Karol Kuźniak", "Jan Wawszczak", "Paweł Gora"], "title": "POMO+: Leveraging starting nodes in POMO for solving Capacitated Vehicle Routing Problem", "comment": null, "summary": "In recent years, reinforcement learning (RL) methods have emerged as a\npromising approach for solving combinatorial problems. Among RL-based models,\nPOMO has demonstrated strong performance on a variety of tasks, including\nvariants of the Vehicle Routing Problem (VRP). However, there is room for\nimprovement for these tasks. In this work, we improved POMO, creating a method\n(\\textbf{POMO+}) that leverages the initial nodes to find a solution in a more\ninformed way. We ran experiments on our new model and observed that our\nsolution converges faster and achieves better results. We validated our models\non the CVRPLIB dataset and noticed improvements in problem instances with up to\n100 customers. We hope that our research in this project can lead to further\nadvancements in the field.", "AI": {"tldr": "本文提出POMO+，通过利用初始节点信息改进了强化学习模型POMO，使其在组合优化问题（如VRP）上收敛更快并取得更好结果。", "motivation": "强化学习（RL）方法在解决组合问题方面表现出潜力，特别是POMO模型在VRP等任务上性能突出，但仍有改进空间。", "method": "在POMO模型的基础上进行改进，创建了POMO+方法。该方法通过更智能地利用初始节点信息来寻找解决方案。", "result": "实验结果表明，POMO+解决方案收敛速度更快，并取得了更好的结果。在CVRPLIB数据集上对多达100个客户的问题实例进行了验证，并观察到性能提升。", "conclusion": "该研究希望能够推动强化学习在组合优化领域的进一步发展。"}}
{"id": "2508.08466", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.08466", "abs": "https://arxiv.org/abs/2508.08466", "authors": ["Daren Yao", "Jinsong Yuan", "Ruike Chen"], "title": "Enhancing Small LLM Alignment through Margin-Based Objective Modifications under Resource Constraints", "comment": "10 pages, 3 figures", "summary": "Small large language models (LLMs) often face difficulties in aligning output\nto human preferences, particularly when operating under severe performance\ngaps. In this work, we propose two lightweight DPO-based variants -- Adaptive\nMargin-Sigmoid Loss and APO-hinge-zero -- to better address underperformance\nscenarios by introducing margin-based objectives and selective update\nmechanisms.\n  Our APO-hinge-zero method, which combines hinge-induced hard-example mining\nwith the chosen-focused optimization of APO-zero, achieves strong results. In\nAlpacaEval, APO-hinge-zero improves the win rate by +2.0 points and the\nlength-controlled win rate by +1.4 points compared to the APO-zero baseline. In\nMT-Bench, our methods maintain competitive performance in diverse categories,\nparticularly excelling in STEM and Humanities tasks.\n  These results demonstrate that simple modifications to preference-based\nobjectives can significantly enhance small LLM alignment under resource\nconstraints, offering a practical path toward more efficient deployment.", "AI": {"tldr": "本文提出了两种基于DPO的轻量级变体——自适应裕度S型损失（Adaptive Margin-Sigmoid Loss）和APO-hinge-zero，旨在解决小型大型语言模型（LLMs）在性能差距较大时难以与人类偏好对齐的问题，并通过引入基于裕度的目标和选择性更新机制显著提升了对齐效果。", "motivation": "小型大型语言模型（LLMs）在与人类偏好对齐方面常面临困难，尤其是在存在严重性能差距的情况下，这限制了它们在资源受限环境下的实际部署。", "method": "本文提出了两种轻量级、基于DPO的变体：自适应裕度S型损失（Adaptive Margin-Sigmoid Loss）和APO-hinge-zero。APO-hinge-zero方法结合了铰链（hinge）诱导的难例挖掘与APO-zero的偏好选择优化机制，通过引入基于裕度的目标和选择性更新机制来解决性能不足的情况。", "result": "在AlpacaEval测试中，APO-hinge-zero方法相比APO-zero基线，胜率提高了2.0个百分点，长度控制胜率提高了1.4个百分点。在MT-Bench测试中，所提出的方法在各类任务中保持了有竞争力的性能，尤其在STEM和人文学科任务中表现出色。", "conclusion": "研究结果表明，对基于偏好的目标函数进行简单修改，可以显著提升资源受限下小型LLM的对齐能力，为更高效的部署提供了实用的途径。"}}
{"id": "2508.08706", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.08706", "abs": "https://arxiv.org/abs/2508.08706", "authors": ["Zhengxue Cheng", "Yiqian Zhang", "Wenkang Zhang", "Haoyu Li", "Keyu Wang", "Li Song", "Hengdi Zhang"], "title": "OmniVTLA: Vision-Tactile-Language-Action Model with Semantic-Aligned Tactile Sensing", "comment": "15 pages, 7 figures, 8 tables", "summary": "Recent vision-language-action (VLA) models build upon vision-language\nfoundations, and have achieved promising results and exhibit the possibility of\ntask generalization in robot manipulation. However, due to the heterogeneity of\ntactile sensors and the difficulty of acquiring tactile data, current VLA\nmodels significantly overlook the importance of tactile perception and fail in\ncontact-rich tasks. To address this issue, this paper proposes OmniVTLA, a\nnovel architecture involving tactile sensing. Specifically, our contributions\nare threefold. First, our OmniVTLA features a dual-path tactile encoder\nframework. This framework enhances tactile perception across diverse\nvision-based and force-based tactile sensors by using a pretrained vision\ntransformer (ViT) and a semantically-aligned tactile ViT (SA-ViT). Second, we\nintroduce ObjTac, a comprehensive force-based tactile dataset capturing\ntextual, visual, and tactile information for 56 objects across 10 categories.\nWith 135K tri-modal samples, ObjTac supplements existing visuo-tactile\ndatasets. Third, leveraging this dataset, we train a semantically-aligned\ntactile encoder to learn a unified tactile representation, serving as a better\ninitialization for OmniVTLA. Real-world experiments demonstrate substantial\nimprovements over state-of-the-art VLA baselines, achieving 96.9% success rates\nwith grippers, (21.9% higher over baseline) and 100% success rates with\ndexterous hands (6.2% higher over baseline) in pick-and-place tasks. Besides,\nOmniVTLA significantly reduces task completion time and generates smoother\ntrajectories through tactile sensing compared to existing VLA.", "AI": {"tldr": "OmniVTLA是一个结合触觉感知的视觉-语言-动作（VLA）模型，通过双路径触觉编码器和新的多模态触觉数据集ObjTac，显著提升了机器人在接触密集型任务中的表现。", "motivation": "现有VLA模型忽视触觉感知的重要性，在接触密集型任务中表现不佳，原因在于触觉传感器的异构性以及触觉数据获取的困难。", "method": "1. 提出了OmniVTLA，一个包含双路径触觉编码器框架的新型架构，结合预训练的视觉Transformer (ViT) 和语义对齐的触觉ViT (SA-ViT) 来增强触觉感知。2. 引入了ObjTac数据集，一个包含56种物体、13.5万个三模态（文本、视觉、触觉）样本的力基触觉数据集。3. 利用ObjTac数据集训练语义对齐的触觉编码器，学习统一的触觉表示，作为OmniVTLA更好的初始化。", "result": "在抓取和放置任务中，OmniVTLA在夹持器上实现了96.9%的成功率（比基线高21.9%），在灵巧手实现了100%的成功率（比基线高6.2%）。此外，与现有VLA模型相比，OmniVTLA显著减少了任务完成时间并生成了更平滑的轨迹。", "conclusion": "OmniVTLA通过有效整合触觉感知，显著提升了VLA模型在机器人操作中的性能，特别是在接触密集型任务中，证明了触觉信息对于机器人任务泛化和效率的关键作用。"}}
{"id": "2508.08556", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08556", "abs": "https://arxiv.org/abs/2508.08556", "authors": ["Yunqi Miao", "Zhiyu Qu", "Mingqi Gao", "Changrui Chen", "Jifei Song", "Jungong Han", "Jiankang Deng"], "title": "Unlocking the Potential of Diffusion Priors in Blind Face Restoration", "comment": null, "summary": "Although diffusion prior is rising as a powerful solution for blind face\nrestoration (BFR), the inherent gap between the vanilla diffusion model and BFR\nsettings hinders its seamless adaptation. The gap mainly stems from the\ndiscrepancy between 1) high-quality (HQ) and low-quality (LQ) images and 2)\nsynthesized and real-world images. The vanilla diffusion model is trained on\nimages with no or less degradations, whereas BFR handles moderately to severely\ndegraded images. Additionally, LQ images used for training are synthesized by a\nnaive degradation model with limited degradation patterns, which fails to\nsimulate complex and unknown degradations in real-world scenarios. In this\nwork, we use a unified network FLIPNET that switches between two modes to\nresolve specific gaps. In Restoration mode, the model gradually integrates\nBFR-oriented features and face embeddings from LQ images to achieve authentic\nand faithful face restoration. In Degradation mode, the model synthesizes\nreal-world like degraded images based on the knowledge learned from real-world\ndegradation datasets. Extensive evaluations on benchmark datasets show that our\nmodel 1) outperforms previous diffusion prior based BFR methods in terms of\nauthenticity and fidelity, and 2) outperforms the naive degradation model in\nmodeling the real-world degradations.", "AI": {"tldr": "该论文提出了一种名为FLIPNET的统一网络，通过切换两种模式（修复模式和降质模式）来解决扩散模型在盲人脸修复（BFR）中遇到的高质量/低质量图像差异以及合成/真实世界图像降质模式不匹配的问题，从而提高了修复的真实性和保真度。", "motivation": "现有的扩散模型在盲人脸修复（BFR）中应用时存在固有差距：1) 扩散模型通常在高质量图像上训练，而BFR处理的是中度到严重退化的图像；2) 训练中使用的低质量图像是基于简单降质模型合成的，无法模拟真实世界中复杂多变的降质模式，导致模型泛化能力不足。", "method": "本文提出了一个统一网络FLIPNET，它在两种模式之间切换以解决特定差距：1) 修复模式：模型逐渐整合BFR导向的特征和来自低质量图像的面部嵌入，以实现真实和忠实的人脸修复。2) 降质模式：模型基于从真实世界降质数据集中学习到的知识，合成类似真实世界的退化图像。", "result": "在基准数据集上的广泛评估表明，该模型1) 在真实性和保真度方面优于以前基于扩散先验的BFR方法；2) 在建模真实世界降质方面优于传统的简单降质模型。", "conclusion": "FLIPNET通过其独特的双模式设计，成功弥合了扩散模型与盲人脸修复任务之间的差距，有效处理了图像质量差异和复杂真实世界降质问题，从而在人脸修复效果上取得了显著提升。"}}
{"id": "2508.08500", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08500", "abs": "https://arxiv.org/abs/2508.08500", "authors": ["Sviatoslav Lushnei", "Dmytro Shumskyi", "Severyn Shykula", "Ernesto Jimenez-Ruiz", "Artur d'Avila Garcez"], "title": "Large Language Models as Oracles for Ontology Alignment", "comment": "Submitted to a conference. 17 pages", "summary": "Ontology alignment plays a crucial role in integrating diverse data sources\nacross domains. There is a large plethora of systems that tackle the ontology\nalignment problem, yet challenges persist in producing highly quality\ncorrespondences among a set of input ontologies. Human-in-the-loop during the\nalignment process is essential in applications requiring very accurate\nmappings. User involvement is, however, expensive when dealing with large\nontologies. In this paper, we explore the feasibility of using Large Language\nModels (LLM) as an alternative to the domain expert. The use of the LLM focuses\nonly on the validation of the subset of correspondences where an ontology\nalignment system is very uncertain. We have conducted an extensive evaluation\nover several matching tasks of the Ontology Alignment Evaluation Initiative\n(OAEI), analysing the performance of several state-of-the-art LLMs using\ndifferent ontology-driven prompt templates. The LLM results are also compared\nagainst simulated Oracles with variable error rates.", "AI": {"tldr": "本体对齐中，大型语言模型（LLM）被探索用于验证不确定的对应关系，以替代昂贵的人类专家。研究在OAEI任务上评估了不同LLM的性能。", "motivation": "现有本体对齐系统难以产生高质量的对应关系。人工参与虽然能确保高精度，但对于大型本体而言成本高昂。因此，需要寻找一种经济高效的替代方案来验证本体对齐结果。", "method": "本研究利用LLM来验证本体对齐系统高度不确定的对应关系子集。通过在本体对齐评估倡议（OAEI）的多个匹配任务上，使用不同的本体驱动提示模板，对多种最先进的LLM进行了广泛评估，并将其性能与具有可变错误率的模拟Oracle进行了比较。", "result": "（论文）分析了多种最先进LLM在OAEI任务上的性能，并将其与模拟Oracle的性能进行了比较，以评估LLM作为领域专家替代方案在验证不确定本体对齐方面的可行性。", "conclusion": "（论文将得出）大型语言模型作为领域专家替代方案，在本体对齐过程中验证不确定对应关系方面是可行的。"}}
{"id": "2508.08492", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.08492", "abs": "https://arxiv.org/abs/2508.08492", "authors": ["Lorenzo Tomaz", "Judd Rosenblatt", "Thomas Berry Jones", "Diogo Schwerz de Lucena"], "title": "Momentum Point-Perplexity Mechanics in Large Language Models", "comment": null, "summary": "We take a physics-based approach to studying how the internal hidden states\nof large language models change from token to token during inference. Across 20\nopen-source transformer models (135M-3B parameters), we find that a quantity\ncombining the rate of change in hidden states and the model's next-token\ncertainty, analogous to energy in physics, remains nearly constant.\nRandom-weight models conserve this \"energy\" more tightly than pre-trained ones,\nwhile training shifts models into a faster, more decisive regime with greater\nvariability. Using this \"log-Lagrangian\" view, we derive a control method\ncalled Jacobian steering, which perturbs hidden states in the minimal way\nneeded to favor a target token. This approach maintained near-constant energy\nin two tested models and produced continuations rated higher in semantic\nquality than the models' natural outputs. Viewing transformers through this\nmechanics lens offers a principled basis for interpretability, anomaly\ndetection, and low-risk steering. This could help make powerful models more\npredictable and aligned with human intent.", "AI": {"tldr": "研究发现大型语言模型推理时，其内部隐藏状态的变化率与模型下一词确定性相结合的“能量”量保持近似恒定。基于此，提出了一种名为Jacobian转向的控制方法，可在保持能量不变的情况下，提高生成文本的语义质量。", "motivation": "理解大型语言模型在推理过程中内部隐藏状态如何变化，以提高模型的可预测性，实现与人类意图的对齐，并为模型的可解释性、异常检测和低风险控制提供原理基础。", "method": "采用基于物理学的方法，分析了20个开源Transformer模型的隐藏状态变化。定义了一个结合隐藏状态变化率和模型下一词确定性的“能量”量。基于这种“对数拉格朗日”视角，推导出了一个名为Jacobian转向的控制方法，该方法通过最小扰动隐藏状态来偏好目标词。", "result": "发现一个结合隐藏状态变化率和模型下一词确定性的“能量”量在模型推理时几乎保持恒定。随机权重模型比预训练模型更严格地保持这种“能量”。训练使模型进入一个更快、更果断、变异性更大的状态。Jacobian转向方法在测试模型中保持了近似恒定的能量，并生成了比模型自然输出语义质量更高的续写。", "conclusion": "通过力学视角审视Transformer模型，为模型的可解释性、异常检测和低风险控制提供了一个有原则的基础。这有助于使强大的模型更可预测，并更好地与人类意图对齐。"}}
{"id": "2508.08707", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.08707", "abs": "https://arxiv.org/abs/2508.08707", "authors": ["Haoran Ding", "Anqing Duan", "Zezhou Sun", "Leonel Rozo", "Noémie Jaquier", "Dezhen Song", "Yoshihiko Nakamura"], "title": "Towards Safe Imitation Learning via Potential Field-Guided Flow Matching", "comment": "8 pages, 6 figures, Accepted to IROS 2025", "summary": "Deep generative models, particularly diffusion and flow matching models, have\nrecently shown remarkable potential in learning complex policies through\nimitation learning. However, the safety of generated motions remains\noverlooked, particularly in complex environments with inherent obstacles. In\nthis work, we address this critical gap by proposing Potential Field-Guided\nFlow Matching Policy (PF2MP), a novel approach that simultaneously learns task\npolicies and extracts obstacle-related information, represented as a potential\nfield, from the same set of successful demonstrations. During inference, PF2MP\nmodulates the flow matching vector field via the learned potential field,\nenabling safe motion generation. By leveraging these complementary fields, our\napproach achieves improved safety without compromising task success across\ndiverse environments, such as navigation tasks and robotic manipulation\nscenarios. We evaluate PF2MP in both simulation and real-world settings,\ndemonstrating its effectiveness in task space and joint space control.\nExperimental results demonstrate that PF2MP enhances safety, achieving a\nsignificant reduction of collisions compared to baseline policies. This work\npaves the way for safer motion generation in unstructured and obstaclerich\nenvironments.", "AI": {"tldr": "PF2MP是一种新颖的方法，通过从成功演示中同时学习任务策略和提取障碍物相关的势场，从而在不影响任务成功的情况下提高深度生成模型（如流匹配）在复杂环境中的运动生成安全性。", "motivation": "深度生成模型（特别是扩散和流匹配模型）在模仿学习中展现出巨大潜力，但其生成的运动在复杂、有障碍的环境中的安全性被忽视，这是一个关键的研究空白。", "method": "提出势场引导的流匹配策略（PF2MP），该方法从同一组成功演示中同时学习任务策略并提取障碍物相关的势场。在推理阶段，PF2MP通过学习到的势场调制流匹配向量场，从而实现安全的运动生成。", "result": "PF2MP在导航和机器人操作等多种环境中，在不损害任务成功的情况下显著提高了安全性，与基线策略相比，碰撞显著减少。该方法在仿真和现实世界中均得到验证，适用于任务空间和关节空间控制。", "conclusion": "PF2MP为在非结构化和障碍丰富的环境中生成更安全的运动铺平了道路。"}}
{"id": "2508.08566", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08566", "abs": "https://arxiv.org/abs/2508.08566", "authors": ["Tuo Liu", "Qinghan Yang", "Yu Zhang", "Rongjun Ge", "Yang Chen", "Guangquan Zhou"], "title": "Think as Cardiac Sonographers: Marrying SAM with Left Ventricular Indicators Measurements According to Clinical Guidelines", "comment": null, "summary": "Left ventricular (LV) indicator measurements following clinical\nechocardiog-raphy guidelines are important for diagnosing cardiovascular\ndisease. Alt-hough existing algorithms have explored automated LV\nquantification, they can struggle to capture generic visual representations due\nto the normally small training datasets. Therefore, it is necessary to\nintroduce vision founda-tional models (VFM) with abundant knowledge. However,\nVFMs represented by the segment anything model (SAM) are usually suitable for\nsegmentation but incapable of identifying key anatomical points, which are\ncritical in LV indicator measurements. In this paper, we propose a novel\nframework named AutoSAME, combining the powerful visual understanding of SAM\nwith seg-mentation and landmark localization tasks simultaneously.\nConsequently, the framework mimics the operation of cardiac sonographers,\nachieving LV indi-cator measurements consistent with clinical guidelines. We\nfurther present fil-tered cross-branch attention (FCBA) in AutoSAME, which\nleverages relatively comprehensive features in the segmentation to enhance the\nheatmap regression (HR) of key points from the frequency domain perspective,\noptimizing the vis-ual representation learned by the latter. Moreover, we\npropose spatial-guided prompt alignment (SGPA) to automatically generate prompt\nembeddings guid-ed by spatial properties of LV, thereby improving the accuracy\nof dense pre-dictions by prior spatial knowledge. The extensive experiments on\nan echocar-diography dataset demonstrate the efficiency of each design and the\nsuperiori-ty of our AutoSAME in LV segmentation, landmark localization, and\nindicator measurements. The code will be available at\nhttps://github.com/QC-LIU-1997/AutoSAME.", "AI": {"tldr": "AutoSAME是一个结合SAM强大视觉理解能力的框架，能同时进行左心室（LV）分割和关键点定位，从而实现符合临床指南的LV指标测量，解决了小训练数据集和现有视觉基础模型（VFM）在关键点识别上的局限性。", "motivation": "现有LV自动量化算法因训练数据集小而难以捕获通用视觉表示；视觉基础模型（VFM）如SAM虽擅长分割，但无法识别对LV指标测量至关重要的关键解剖点。", "method": "提出AutoSAME框架，同时进行分割和地标定位。引入“滤波跨分支注意力”（FCBA），利用分割中的特征从频域增强关键点热图回归。提出“空间引导提示对齐”（SGPA），根据LV空间特性自动生成提示嵌入，利用先验空间知识提高预测精度。", "result": "在超声心动图数据集上的大量实验证明了AutoSAME每个设计的效率及其在LV分割、地标定位和指标测量方面的优越性。", "conclusion": "AutoSAME成功地模仿了心脏超声医师的操作，实现了与临床指南一致的LV指标测量，并通过FCBA和SGPA优化了视觉表示和预测精度，解决了现有方法的局限性。"}}
{"id": "2508.08501", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08501", "abs": "https://arxiv.org/abs/2508.08501", "authors": ["Yuchen Li", "Cong Lin", "Muhammad Umair Nasir", "Philip Bontrager", "Jialin Liu", "Julian Togelius"], "title": "GVGAI-LLM: Evaluating Large Language Model Agents with Infinite Games", "comment": null, "summary": "We introduce GVGAI-LLM, a video game benchmark for evaluating the reasoning\nand problem-solving capabilities of large language models (LLMs). Built on the\nGeneral Video Game AI framework, it features a diverse collection of\narcade-style games designed to test a model's ability to handle tasks that\ndiffer from most existing LLM benchmarks. The benchmark leverages a game\ndescription language that enables rapid creation of new games and levels,\nhelping to prevent overfitting over time. Each game scene is represented by a\ncompact set of ASCII characters, allowing for efficient processing by language\nmodels. GVGAI-LLM defines interpretable metrics, including the meaningful step\nratio, step efficiency, and overall score, to assess model behavior. Through\nzero-shot evaluations across a broad set of games and levels with diverse\nchallenges and skill depth, we reveal persistent limitations of LLMs in spatial\nreasoning and basic planning. Current models consistently exhibit spatial and\nlogical errors, motivating structured prompting and spatial grounding\ntechniques. While these interventions lead to partial improvements, the\nbenchmark remains very far from solved. GVGAI-LLM provides a reproducible\ntestbed for advancing research on language model capabilities, with a\nparticular emphasis on agentic behavior and contextual reasoning.", "AI": {"tldr": "GVGAI-LLM是一个用于评估大型语言模型（LLM）推理和问题解决能力的视频游戏基准。它包含多样化的街机风格游戏，通过ASCII字符表示场景，并定义可解释的评估指标。零样本评估显示LLM在空间推理和基本规划方面存在持续限制，尽管结构化提示和空间接地技术有所改善，但问题远未解决。", "motivation": "现有LLM基准测试未能充分评估LLM在处理与众不同任务时的推理和问题解决能力，尤其是在需要代理行为和上下文推理的动态游戏环境中。因此，需要一个能测试这些能力的基准。", "method": "引入GVGAI-LLM，基于通用视频游戏AI框架构建，包含多样化的街机风格游戏。利用游戏描述语言快速创建新游戏和关卡，防止过拟合。游戏场景通过紧凑的ASCII字符集表示。定义了可解释的评估指标，包括有意义步数比、步数效率和总分。通过零样本评估，测试了LLM在各种游戏和关卡中的表现。尝试了结构化提示和空间接地技术来改善模型行为。", "result": "LLM在空间推理和基本规划方面表现出持续的局限性。当前模型经常出现空间和逻辑错误。虽然结构化提示和空间接地技术能带来部分改进，但基准测试离“解决”还很远。", "conclusion": "GVGAI-LLM提供了一个可复现的测试平台，用于推进语言模型能力的研究，特别是关注代理行为和上下文推理。研究结果揭示了当前LLM在处理复杂游戏环境中的空间推理和规划任务时的显著不足。"}}
{"id": "2508.08509", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08509", "abs": "https://arxiv.org/abs/2508.08509", "authors": ["Jadie Adams", "Brian Hu", "Emily Veenhuis", "David Joy", "Bharadwaj Ravichandran", "Aaron Bray", "Anthony Hoogs", "Arslan Basharat"], "title": "Steerable Pluralism: Pluralistic Alignment via Few-Shot Comparative Regression", "comment": "AIES '25: Proceedings of the 2025 AAAI/ACM Conference on AI, Ethics,\n  and Society", "summary": "Large language models (LLMs) are currently aligned using techniques such as\nreinforcement learning from human feedback (RLHF). However, these methods use\nscalar rewards that can only reflect user preferences on average. Pluralistic\nalignment instead seeks to capture diverse user preferences across a set of\nattributes, moving beyond just helpfulness and harmlessness. Toward this end,\nwe propose a steerable pluralistic model based on few-shot comparative\nregression that can adapt to individual user preferences. Our approach\nleverages in-context learning and reasoning, grounded in a set of fine-grained\nattributes, to compare response options and make aligned choices. To evaluate\nour algorithm, we also propose two new steerable pluralistic benchmarks by\nadapting the Moral Integrity Corpus (MIC) and the HelpSteer2 datasets,\ndemonstrating the applicability of our approach to value-aligned\ndecision-making and reward modeling, respectively. Our few-shot comparative\nregression approach is interpretable and compatible with different attributes\nand LLMs, while outperforming multiple baseline and state-of-the-art methods.\nOur work provides new insights and research directions in pluralistic\nalignment, enabling a more fair and representative use of LLMs and advancing\nthe state-of-the-art in ethical AI.", "AI": {"tldr": "本文提出一种基于少样本比较回归的可控多元对齐模型，旨在通过细粒度属性捕捉并适应用户多样化偏好，超越传统LLM对齐方法的局限性，并在新基准上表现优异。", "motivation": "当前大型语言模型（LLMs）的对齐技术（如RLHF）使用标量奖励，只能平均反映用户偏好，无法捕捉多样化的用户偏好和更广泛的属性，因此需要一种能反映多元偏好的对齐方法。", "method": "提出一种可控的多元对齐模型，核心是基于少样本比较回归（few-shot comparative regression）。该方法利用上下文学习和推理，以一组细粒度属性为基础，比较响应选项并做出对齐选择。同时，构建了两个新的可控多元基准（改编自Moral Integrity Corpus和HelpSteer2数据集）来评估算法。", "result": "所提出的少样本比较回归方法具有可解释性，与不同属性和LLM兼容，并且性能优于多种基线和现有先进方法。它适用于价值对齐的决策和奖励建模。", "conclusion": "这项工作为多元对齐提供了新的见解和研究方向，有助于实现更公平和更具代表性的LLM使用，并推动伦理AI领域的发展。"}}
{"id": "2508.08709", "categories": ["cs.RO", "cs.AR", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.08709", "abs": "https://arxiv.org/abs/2508.08709", "authors": ["Lukas Krupp", "Maximilian Schöffel", "Elias Biehl", "Norbert Wehn"], "title": "CRADLE: Conversational RTL Design Space Exploration with LLM-based Multi-Agent Systems", "comment": "Accepted for presentation at the 22nd International SoC Conference\n  (ISOCC 2025). Proceedings to be included in IEEE Xplore", "summary": "This paper presents CRADLE, a conversational framework for design space\nexploration of RTL designs using LLM-based multi-agent systems. Unlike existing\nrigid approaches, CRADLE enables user-guided flows with internal\nself-verification, correction, and optimization. We demonstrate the framework\nwith a generator-critic agent system targeting FPGA resource minimization using\nstate-of-the-art LLMs. Experimental results on the RTLLM benchmark show that\nCRADLE achieves significant reductions in resource usage with averages of 48%\nand 40% in LUTs and FFs across all benchmark designs.", "AI": {"tldr": "CRADLE是一个基于LLM多智能体系统的对话式框架，用于RTL设计的探索，实现资源优化。", "motivation": "现有方法僵化，缺乏用户引导、自验证、修正和优化的能力，因此需要一个更灵活的对话式框架。", "method": "采用LLM驱动的多智能体系统，特别是生成器-评论器（generator-critic）智能体系统，实现内部自验证、修正和优化，以最小化FPGA资源。", "result": "在RTLLM基准测试中，CRADLE使LUTs和FFs的资源使用量平均分别减少了48%和40%。", "conclusion": "CRADLE框架能够有效减少RTL设计的FPGA资源使用，展现了LLM多智能体系统在设计空间探索中的潜力。"}}
{"id": "2508.08570", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.08570", "abs": "https://arxiv.org/abs/2508.08570", "authors": ["Chenruo Liu", "Hongjun Liu", "Zeyu Lai", "Yiqiu Shen", "Chen Zhao", "Qi Lei"], "title": "Superclass-Guided Representation Disentanglement for Spurious Correlation Mitigation", "comment": null, "summary": "To enhance group robustness to spurious correlations, prior work often relies\non auxiliary annotations for groups or spurious features and assumes identical\nsets of groups across source and target domains. These two requirements are\nboth unnatural and impractical in real-world settings. To overcome these\nlimitations, we propose a method that leverages the semantic structure inherent\nin class labels--specifically, superclass information--to naturally reduce\nreliance on spurious features. Our model employs gradient-based attention\nguided by a pre-trained vision-language model to disentangle\nsuperclass-relevant and irrelevant features. Then, by promoting the use of all\nsuperclass-relevant features for prediction, our approach achieves robustness\nto more complex spurious correlations without the need to annotate any source\nsamples. Experiments across diverse datasets demonstrate that our method\nsignificantly outperforms baselines in domain generalization tasks, with clear\nimprovements in both quantitative metrics and qualitative visualizations.", "AI": {"tldr": "该研究提出一种新方法，通过利用类别标签中的超类语义信息，并结合预训练的视觉-语言模型和基于梯度的注意力机制，来解耦特征并增强模型对虚假相关性的鲁棒性，且无需辅助标注，在域泛化任务中表现优异。", "motivation": "现有增强群组鲁棒性以对抗虚假相关性的方法，通常依赖辅助标注或假设源域和目标域拥有相同的群组设置，这在现实世界中既不自然也不实用。", "method": "该方法利用类别标签中固有的语义结构（特别是超类信息），通过预训练的视觉-语言模型引导的基于梯度的注意力机制，来解耦与超类相关和不相关的特征。随后，通过促进模型使用所有与超类相关的特征进行预测，从而在无需任何源样本标注的情况下，实现对更复杂虚假相关性的鲁棒性。", "result": "在多样化数据集上的实验表明，该方法在域泛化任务中显著优于现有基线方法，并在定量指标和定性可视化方面均取得了明显提升。", "conclusion": "该方法通过利用类别语义结构和视觉-语言模型的引导，成功克服了传统鲁棒性方法对辅助标注和特定域设置的依赖，提供了一种更实用、有效的对抗虚假相关性的解决方案，显著提升了模型在域泛化任务中的性能。"}}
{"id": "2508.08529", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08529", "abs": "https://arxiv.org/abs/2508.08529", "authors": ["Arshia Ilaty", "Hossein Shirazi", "Hajar Homayouni"], "title": "SynLLM: A Comparative Analysis of Large Language Models for Medical Tabular Synthetic Data Generation via Prompt Engineering", "comment": "10 Pages, 2 Supplementary Pages, 6 Tables", "summary": "Access to real-world medical data is often restricted due to privacy\nregulations, posing a significant barrier to the advancement of healthcare\nresearch. Synthetic data offers a promising alternative; however, generating\nrealistic, clinically valid, and privacy-conscious records remains a major\nchallenge. Recent advancements in Large Language Models (LLMs) offer new\nopportunities for structured data generation; however, existing approaches\nfrequently lack systematic prompting strategies and comprehensive,\nmulti-dimensional evaluation frameworks.\n  In this paper, we present SynLLM, a modular framework for generating\nhigh-quality synthetic medical tabular data using 20 state-of-the-art\nopen-source LLMs, including LLaMA, Mistral, and GPT variants, guided by\nstructured prompts. We propose four distinct prompt types, ranging from\nexample-driven to rule-based constraints, that encode schema, metadata, and\ndomain knowledge to control generation without model fine-tuning. Our framework\nfeatures a comprehensive evaluation pipeline that rigorously assesses generated\ndata across statistical fidelity, clinical consistency, and privacy\npreservation.\n  We evaluate SynLLM across three public medical datasets, including Diabetes,\nCirrhosis, and Stroke, using 20 open-source LLMs. Our results show that prompt\nengineering significantly impacts data quality and privacy risk, with\nrule-based prompts achieving the best privacy-quality balance. SynLLM\nestablishes that, when guided by well-designed prompts and evaluated with\nrobust, multi-metric criteria, LLMs can generate synthetic medical data that is\nboth clinically plausible and privacy-aware, paving the way for safer and more\neffective data sharing in healthcare research.", "AI": {"tldr": "SynLLM是一个模块化框架，利用20种开源大型语言模型（LLMs）和结构化提示，生成高质量、临床有效且隐私保护的合成医疗表格数据，并通过多维度评估验证其有效性。", "motivation": "由于隐私法规限制，真实医疗数据难以获取，阻碍了医疗研究进展。合成数据是替代方案，但生成真实、临床有效且隐私保护的数据仍是挑战。现有LLM生成结构化数据的方法缺乏系统性提示策略和全面的多维度评估框架。", "method": "本文提出了SynLLM框架，使用20种SOTA开源LLMs（如LLaMA, Mistral, GPT变体）生成合成医疗表格数据。通过四种结构化提示类型（从示例驱动到基于规则的约束）引导LLM生成，这些提示编码了数据模式、元数据和领域知识，无需模型微调。框架包含全面的评估流程，严格评估生成数据的统计保真度、临床一致性和隐私保护。", "result": "在糖尿病、肝硬化和中风三个公共医疗数据集上，使用20种开源LLM进行评估。结果表明，提示工程显著影响数据质量和隐私风险，其中基于规则的提示实现了最佳的隐私-质量平衡。SynLLM证明，在精心设计的提示和鲁棒的多指标评估指导下，LLMs可以生成既临床合理又注重隐私的合成医疗数据。", "conclusion": "SynLLM框架证明了LLMs在生成高质量、隐私保护的合成医疗数据方面的潜力，为医疗研究中更安全、更有效的数据共享铺平了道路。"}}
{"id": "2508.08514", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.08514", "abs": "https://arxiv.org/abs/2508.08514", "authors": ["Sameer Panwar"], "title": "DeCAL Tokenwise Compression", "comment": null, "summary": "This paper introduces DeCAL, a new method for tokenwise compression. DeCAL\nuses an encoder-decoder language model pretrained with denoising to learn to\nproduce high-quality, general-purpose compressed representations by the\nencoder. DeCAL applies small modifications to the encoder, with the emphasis on\nmaximizing compression quality, even at the expense of compute. We show that\nDeCAL at 2x compression can match uncompressed on many downstream tasks, with\nusually only minor dropoff in metrics up to 8x compression, among\nquestion-answering, summarization, and multi-vector retrieval tasks. DeCAL\noffers significant savings where pre-computed dense representations can be\nutilized, and we believe the approach can be further developed to be more\nbroadly applicable.", "AI": {"tldr": "DeCAL是一种新的逐token压缩方法，利用经过去噪预训练的编解码语言模型，通过编码器学习生成高质量、通用的压缩表示。", "motivation": "旨在生成高质量、通用目的的压缩表示，以在下游任务中（特别是需要预计算密集表示的场景）实现显著的资源节省。", "method": "DeCAL采用一个经过去噪预训练的编解码语言模型，其编码器负责生成压缩表示。该方法对编码器进行了少量修改，重点在于最大化压缩质量。", "result": "在2倍压缩比下，DeCAL在许多下游任务上能与未压缩数据表现相当；在问答、摘要和多向量检索任务中，即使达到8倍压缩比，性能下降也通常很小。该方法在可利用预计算密集表示的场景中能显著节省资源。", "conclusion": "DeCAL是一种有效的压缩方法，未来有望得到更广泛的应用。"}}
{"id": "2508.08743", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.08743", "abs": "https://arxiv.org/abs/2508.08743", "authors": ["Haoyu Zhang", "Long Cheng"], "title": "Boosting Action-Information via a Variational Bottleneck on Unlabelled Robot Videos", "comment": null, "summary": "Learning from demonstrations (LfD) typically relies on large amounts of\naction-labeled expert trajectories, which fundamentally constrains the scale of\navailable training data. A promising alternative is to learn directly from\nunlabeled video demonstrations. However, we find that existing methods tend to\nencode latent actions that share little mutual information with the true robot\nactions, leading to suboptimal control performance. To address this limitation,\nwe introduce a novel framework that explicitly maximizes the mutual information\nbetween latent actions and true actions, even in the absence of action labels.\nOur method leverage the variational information-bottleneck to extract\naction-relevant representations while discarding task-irrelevant information.\nWe provide a theoretical analysis showing that our objective indeed maximizes\nthe mutual information between latent and true actions. Finally, we validate\nour approach through extensive experiments: first in simulated robotic\nenvironments and then on real-world robotic platforms, the experimental results\ndemonstrate that our method significantly enhances mutual information and\nconsistently improves policy performance.", "AI": {"tldr": "本文提出了一种从无标签视频演示中学习的新框架，通过最大化潜在动作与真实动作之间的互信息，解决了现有方法潜在动作编码与真实动作关联性低的问题，从而提高了机器人控制性能。", "motivation": "传统的从演示中学习（LfD）方法依赖大量带动作标签的专家轨迹，限制了训练数据的规模。尽管从无标签视频中学习是一个有前景的方向，但现有方法编码的潜在动作与真实机器人动作的互信息较低，导致控制性能不佳。", "method": "引入了一个新颖的框架，即使在没有动作标签的情况下，也能显式最大化潜在动作与真实动作之间的互信息。该方法利用变分信息瓶颈来提取与动作相关的表示，同时丢弃与任务无关的信息。提供了理论分析证明其目标函数确实最大化了互信息。", "result": "通过广泛的模拟和真实世界机器人平台实验验证了该方法。实验结果表明，该方法显著增强了互信息，并持续改善了策略性能。", "conclusion": "所提出的方法通过在无标签视频学习中最大化潜在动作与真实动作的互信息，有效解决了现有方法的局限性，显著提高了机器人控制策略的性能。"}}
{"id": "2508.08589", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08589", "abs": "https://arxiv.org/abs/2508.08589", "authors": ["Wenwen Yu", "Zhibo Yang", "Yuliang Liu", "Xiang Bai"], "title": "DocThinker: Explainable Multimodal Large Language Models with Rule-based Reinforcement Learning for Document Understanding", "comment": "ICCV 2025", "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities in document understanding. However, their reasoning processes\nremain largely black-box, making it difficult to ensure reliability and\ntrustworthiness, especially in high-stakes domains such as legal, financial,\nand medical document analysis. Existing methods use fixed Chain-of-Thought\n(CoT) reasoning with supervised fine-tuning (SFT) but suffer from catastrophic\nforgetting, poor adaptability, and limited generalization across domain tasks.\nIn this paper, we propose DocThinker, a rule-based Reinforcement Learning (RL)\nframework for dynamic inference-time reasoning. Instead of relying on static\nCoT templates, DocThinker autonomously refines reasoning strategies via policy\nlearning, generating explainable intermediate results, including structured\nreasoning processes, rephrased questions, regions of interest (RoI) supporting\nthe answer, and the final answer. By integrating multi-objective rule-based\nrewards and KL-constrained optimization, our method mitigates catastrophic\nforgetting and enhances both adaptability and transparency. Extensive\nexperiments on multiple benchmarks demonstrate that DocThinker significantly\nimproves generalization while producing more explainable and\nhuman-understandable reasoning steps. Our findings highlight RL as a powerful\nalternative for enhancing explainability and adaptability in MLLM-based\ndocument understanding. Code will be available at\nhttps://github.com/wenwenyu/DocThinker.", "AI": {"tldr": "DocThinker是一个基于强化学习的框架，旨在解决多模态大语言模型（MLLMs）在文档理解中推理过程不透明的问题，通过动态推理和可解释的中间结果，显著提升模型的泛化能力、适应性和透明度，并缓解灾难性遗忘。", "motivation": "多模态大语言模型（MLLMs）在文档理解中表现出色，但其推理过程是“黑箱”，在高风险领域（如法律、金融、医疗）难以保证可靠性和可信度。现有基于固定思维链（CoT）和监督微调（SFT）的方法存在灾难性遗忘、适应性差和跨领域任务泛化能力有限等问题。", "method": "本文提出了DocThinker，一个基于规则的强化学习（RL）框架，用于动态推理时推理。它通过策略学习自主优化推理策略，而非依赖静态CoT模板，生成可解释的中间结果，包括结构化推理过程、重述的问题、支持答案的感兴趣区域（RoI）和最终答案。通过整合多目标规则奖励和KL约束优化，该方法减轻了灾难性遗忘，并增强了适应性和透明度。", "result": "在多个基准测试上的大量实验表明，DocThinker显著提高了泛化能力，同时产生了更具解释性和人类可理解的推理步骤。", "conclusion": "研究结果强调，强化学习是增强基于MLLM的文档理解模型可解释性和适应性的强大替代方案。"}}
{"id": "2508.08615", "categories": ["cs.AI", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2508.08615", "abs": "https://arxiv.org/abs/2508.08615", "authors": ["Zhichao Wang", "Xinhai Chen", "Qinglin Wang", "Xiang Gao", "Qingyang Zhang", "Menghan Jia", "Xiang Zhang", "Jie Liu"], "title": "UGM2N: An Unsupervised and Generalizable Mesh Movement Network via M-Uniform Loss", "comment": null, "summary": "Partial differential equations (PDEs) form the mathematical foundation for\nmodeling physical systems in science and engineering, where numerical solutions\ndemand rigorous accuracy-efficiency tradeoffs. Mesh movement techniques address\nthis challenge by dynamically relocating mesh nodes to rapidly-varying regions,\nenhancing both simulation accuracy and computational efficiency. However,\ntraditional approaches suffer from high computational complexity and geometric\ninflexibility, limiting their applicability, and existing supervised\nlearning-based approaches face challenges in zero-shot generalization across\ndiverse PDEs and mesh topologies.In this paper, we present an Unsupervised and\nGeneralizable Mesh Movement Network (UGM2N). We first introduce unsupervised\nmesh adaptation through localized geometric feature learning, eliminating the\ndependency on pre-adapted meshes. We then develop a physics-constrained loss\nfunction, M-Uniform loss, that enforces mesh equidistribution at the nodal\nlevel.Experimental results demonstrate that the proposed network exhibits\nequation-agnostic generalization and geometric independence in efficient mesh\nadaptation. It demonstrates consistent superiority over existing methods,\nincluding robust performance across diverse PDEs and mesh geometries,\nscalability to multi-scale resolutions and guaranteed error reduction without\nmesh tangling.", "AI": {"tldr": "本文提出了一种无监督且可泛化的网格移动网络（UGM2N），通过局部几何特征学习和物理约束损失，实现了对不同偏微分方程和网格拓扑的高效、精确且无缠结的自适应网格划分。", "motivation": "偏微分方程（PDEs）的数值解需要平衡精度和效率。传统的网格移动技术计算复杂且几何不灵活，而现有的监督学习方法在零样本泛化方面存在挑战，限制了它们在多样PDEs和网格拓扑上的应用。", "method": "UGM2N通过以下两点实现无监督网格自适应：1. 引入无监督网格自适应，通过局部几何特征学习消除对预适应网格的依赖。2. 开发了一种物理约束损失函数——M-Uniform损失，用于在节点层面强制执行网格均匀分布。", "result": "实验结果表明，所提出的网络在高效网格自适应方面表现出方程无关的泛化能力和几何独立性。它持续优于现有方法，包括在不同PDEs和网格几何形状上的鲁棒性能、对多尺度分辨率的可扩展性以及保证误差减少而不会出现网格缠结。", "conclusion": "UGM2N提供了一种有效、可泛化且鲁棒的网格自适应解决方案，能够克服传统和监督学习方法的局限性，在科学与工程领域的PDEs数值模拟中具有重要应用价值。"}}
{"id": "2508.08591", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08591", "abs": "https://arxiv.org/abs/2508.08591", "authors": ["Sehwan Moon", "Aram Lee", "Jeong Eun Kim", "Hee-Ju Kang", "Il-Seon Shin", "Sung-Wan Kim", "Jae-Min Kim", "Min Jhon", "Ju-Wan Kim"], "title": "DepressLLM: Interpretable domain-adapted language model for depression detection from real-world narratives", "comment": null, "summary": "Advances in large language models (LLMs) have enabled a wide range of\napplications. However, depression prediction is hindered by the lack of\nlarge-scale, high-quality, and rigorously annotated datasets. This study\nintroduces DepressLLM, trained and evaluated on a novel corpus of 3,699\nautobiographical narratives reflecting both happiness and distress. DepressLLM\nprovides interpretable depression predictions and, via its Score-guided Token\nProbability Summation (SToPS) module, delivers both improved classification\nperformance and reliable confidence estimates, achieving an AUC of 0.789, which\nrises to 0.904 on samples with confidence $\\geq$ 0.95. To validate its\nrobustness to heterogeneous data, we evaluated DepressLLM on in-house datasets,\nincluding an Ecological Momentary Assessment (EMA) corpus of daily stress and\nmood recordings, and on public clinical interview data. Finally, a psychiatric\nreview of high-confidence misclassifications highlighted key model and data\nlimitations that suggest directions for future refinements. These findings\ndemonstrate that interpretable AI can enable earlier diagnosis of depression\nand underscore the promise of medical AI in psychiatry.", "AI": {"tldr": "本研究推出了DepressLLM，一个基于大规模自传叙事文本训练的大语言模型，用于可解释的抑郁症预测，并通过SToPS模块提升了分类性能和置信度估计。", "motivation": "抑郁症预测受限于缺乏大规模、高质量且严格标注的数据集，这阻碍了大语言模型（LLMs）在此领域的应用。", "method": "研究构建了一个包含3,699个反映幸福和痛苦的自传叙事文本的新语料库，并在此基础上训练和评估了DepressLLM。DepressLLM包含一个Score-guided Token Probability Summation (SToPS) 模块，用于提供可解释的预测和可靠的置信度估计。模型在内部数据集（包括生态瞬时评估数据）和公共临床访谈数据上进行了鲁棒性验证，并对高置信度误分类进行了精神病学审查。", "result": "DepressLLM在抑郁症预测上取得了0.789的AUC，在置信度高于0.95的样本上AUC提高到0.904。模型对异构数据表现出鲁棒性。对高置信度误分类的精神病学审查揭示了模型和数据局限性，为未来改进提供了方向。", "conclusion": "研究结果表明，可解释的人工智能能够实现抑郁症的早期诊断，并突显了医疗AI在精神病学领域的应用前景。"}}
{"id": "2508.08748", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08748", "abs": "https://arxiv.org/abs/2508.08748", "authors": ["Muhammad A. Muttaqien", "Tomohiro Motoda", "Ryo Hanai", "Yukiyasu Domae"], "title": "Visual Prompting for Robotic Manipulation with Annotation-Guided Pick-and-Place Using ACT", "comment": null, "summary": "Robotic pick-and-place tasks in convenience stores pose challenges due to\ndense object arrangements, occlusions, and variations in object properties such\nas color, shape, size, and texture. These factors complicate trajectory\nplanning and grasping. This paper introduces a perception-action pipeline\nleveraging annotation-guided visual prompting, where bounding box annotations\nidentify both pickable objects and placement locations, providing structured\nspatial guidance. Instead of traditional step-by-step planning, we employ\nAction Chunking with Transformers (ACT) as an imitation learning algorithm,\nenabling the robotic arm to predict chunked action sequences from human\ndemonstrations. This facilitates smooth, adaptive, and data-driven\npick-and-place operations. We evaluate our system based on success rate and\nvisual analysis of grasping behavior, demonstrating improved grasp accuracy and\nadaptability in retail environments.", "AI": {"tldr": "本文提出一种结合视觉提示（边界框标注）和ACT（Transformer动作分块）模仿学习的感知-动作流程，以应对便利店中机器人抓取放置任务的复杂性，实现了更平滑、自适应的操作。", "motivation": "便利店中机器人抓取放置任务面临挑战，包括物体密集排列、遮挡以及物体颜色、形状、尺寸、纹理等属性多样性，这些因素使轨迹规划和抓取复杂化。", "method": "该研究引入了一个感知-动作流程，利用注释引导的视觉提示（通过边界框标注识别可抓取物体和放置位置，提供结构化空间指导）。不同于传统分步规划，采用基于Transformer的动作分块（ACT）作为模仿学习算法，使机械臂能从人类演示中预测分块的动作序列。", "result": "系统在抓取成功率和抓取行为视觉分析方面进行了评估，结果表明在零售环境中抓取精度和适应性得到提高。", "conclusion": "该系统通过结合视觉提示和ACT模仿学习，有效提升了机器人在复杂零售环境中的抓取精度和适应性，解决了便利店中机器人抓取放置任务的挑战。"}}
{"id": "2508.08590", "categories": ["cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.08590", "abs": "https://arxiv.org/abs/2508.08590", "authors": ["Yuxiao Wang", "Wolin Liang", "Yu Lei", "Weiying Xue", "Nan Zhuang", "Qi Liu"], "title": "QueryCraft: Transformer-Guided Query Initialization for Enhanced Human-Object Interaction Detection", "comment": null, "summary": "Human-Object Interaction (HOI) detection aims to localize human-object pairs\nand recognize their interactions in images. Although DETR-based methods have\nrecently emerged as the mainstream framework for HOI detection, they still\nsuffer from a key limitation: Randomly initialized queries lack explicit\nsemantics, leading to suboptimal detection performance. To address this\nchallenge, we propose QueryCraft, a novel plug-and-play HOI detection framework\nthat incorporates semantic priors and guided feature learning through\ntransformer-based query initialization. Central to our approach is\n\\textbf{ACTOR} (\\textbf{A}ction-aware \\textbf{C}ross-modal\n\\textbf{T}ransf\\textbf{OR}mer), a cross-modal Transformer encoder that jointly\nattends to visual regions and textual prompts to extract action-relevant\nfeatures. Rather than merely aligning modalities, ACTOR leverages\nlanguage-guided attention to infer interaction semantics and produce\nsemantically meaningful query representations. To further enhance object-level\nquery quality, we introduce a \\textbf{P}erceptual \\textbf{D}istilled\n\\textbf{Q}uery \\textbf{D}ecoder (\\textbf{PDQD}), which distills object category\nawareness from a pre-trained detector to serve as object query initiation. This\ndual-branch query initialization enables the model to generate more\ninterpretable and effective queries for HOI detection. Extensive experiments on\nHICO-Det and V-COCO benchmarks demonstrate that our method achieves\nstate-of-the-art performance and strong generalization. Code will be released\nupon publication.", "AI": {"tldr": "QueryCraft是一种新颖的可插拔HOI检测框架，通过引入语义先验和引导式特征学习，解决了基于DETR的方法中随机初始化查询缺乏语义的问题，实现了最先进的性能。", "motivation": "现有基于DETR的人-物交互（HOI）检测方法存在关键局限性：随机初始化的查询缺乏明确的语义信息，导致检测性能不佳。", "method": "本文提出QueryCraft框架，包含两个核心组件：1) ACTOR (Action-aware Cross-modal Transformer)，一个跨模态Transformer编码器，联合关注视觉区域和文本提示以提取动作相关特征，通过语言引导注意力推断交互语义并生成有意义的查询表示。2) PDQD (Perceptual Distilled Query Decoder)，从预训练检测器中提取对象类别感知信息，用于对象查询初始化。这种双分支查询初始化机制旨在生成更具可解释性和有效性的查询。", "result": "在HICO-Det和V-COCO基准测试上进行了广泛实验，结果表明该方法取得了最先进的性能和强大的泛化能力。", "conclusion": "通过引入语义先验和引导式特征学习，QueryCraft框架有效地解决了DETR基HOI检测中查询语义不足的问题，显著提升了HOI检测的性能和解释性。"}}
{"id": "2508.08632", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08632", "abs": "https://arxiv.org/abs/2508.08632", "authors": ["Bo Yang", "Yu Zhang", "Lanfei Feng", "Yunkui Chen", "Jianyu Zhang", "Xiao Xu", "Nueraili Aierken", "Yurui Li", "Yuxuan Chen", "Guijun Yang", "Yong He", "Runhe Huang", "Shijian Li"], "title": "AgriGPT: a Large Language Model Ecosystem for Agriculture", "comment": null, "summary": "Despite the rapid progress of Large Language Models (LLMs), their application\nin agriculture remains limited due to the lack of domain-specific models,\ncurated datasets, and robust evaluation frameworks. To address these\nchallenges, we propose AgriGPT, a domain-specialized LLM ecosystem for\nagricultural usage. At its core, we design a multi-agent scalable data engine\nthat systematically compiles credible data sources into Agri-342K, a\nhigh-quality, standardized question-answer (QA) dataset. Trained on this\ndataset, AgriGPT supports a broad range of agricultural stakeholders, from\npractitioners to policy-makers. To enhance factual grounding, we employ\nTri-RAG, a three-channel Retrieval-Augmented Generation framework combining\ndense retrieval, sparse retrieval, and multi-hop knowledge graph reasoning,\nthereby improving the LLM's reasoning reliability. For comprehensive\nevaluation, we introduce AgriBench-13K, a benchmark suite comprising 13 tasks\nwith varying types and complexities. Experiments demonstrate that AgriGPT\nsignificantly outperforms general-purpose LLMs on both domain adaptation and\nreasoning. Beyond the model itself, AgriGPT represents a modular and extensible\nLLM ecosystem for agriculture, comprising structured data construction,\nretrieval-enhanced generation, and domain-specific evaluation. This work\nprovides a generalizable framework for developing scientific and\nindustry-specialized LLMs. All models, datasets, and code will be released to\nempower agricultural communities, especially in underserved regions, and to\npromote open, impactful research.", "AI": {"tldr": "AgriGPT是一个农业领域的专业大语言模型生态系统，通过构建高质量数据集、采用多通道检索增强生成框架和设计综合评估基准，显著提升了LLMs在农业领域的表现，并提供了一个可推广的专业LLM开发框架。", "motivation": "尽管大型语言模型（LLMs）发展迅速，但由于缺乏领域专用模型、精选数据集和稳健的评估框架，它们在农业领域的应用仍然有限。", "method": "核心方法包括：1. 设计了一个多智能体可扩展数据引擎，系统性地将可信数据源编译成高质量、标准化的问答数据集Agri-342K。2. 采用Tri-RAG（三通道检索增强生成）框架，结合密集检索、稀疏检索和多跳知识图谱推理，以增强事实依据和推理可靠性。3. 引入了AgriBench-13K，一个包含13个不同类型和复杂度的任务的基准套件，用于全面评估。", "result": "实验证明，AgriGPT在领域适应性和推理能力方面显著优于通用大语言模型。它代表了一个模块化和可扩展的农业LLM生态系统，包括结构化数据构建、检索增强生成和领域特定评估。", "conclusion": "AgriGPT为农业领域提供了一个强大的专业LLM解决方案，并通过其结构化数据构建、检索增强生成和领域特定评估，提供了一个可推广的开发科学和行业专业LLM的框架。所有模型、数据集和代码都将发布，以赋能农业社区并促进开放、有影响力的研究。"}}
{"id": "2508.08610", "categories": ["cs.CL", "68T50", "I.2.7; I.2.6; H.3.3"], "pdf": "https://arxiv.org/pdf/2508.08610", "abs": "https://arxiv.org/abs/2508.08610", "authors": ["David Santandreu Calonge", "Linda Smail"], "title": "Optimizing Retrieval-Augmented Generation (RAG) for Colloquial Cantonese: A LoRA-Based Systematic Review", "comment": "27 pages, 1 figure, 8 tables", "summary": "This review examines recent advances in Parameter-Efficient Fine-Tuning\n(PEFT), with a focus on Low-Rank Adaptation (LoRA), to optimize\nRetrieval-Augmented Generation (RAG) systems like Qwen3, DeepSeek, and Kimi.\nThese systems face challenges in understanding and generating authentic\nCantonese colloquial expressions due to limited annotated data and linguistic\nvariability. The review evaluates the integration of LoRA within RAG\nframeworks, benchmarks PEFT methods for retrieval and generation accuracy,\nidentify domain adaptation strategies under limited data, and compares\nfine-tuning techniques aimed at improving semantic fidelity under data-scarce\nconditions. A systematic analysis of recent studies employing diverse LoRA\nvariants, synthetic data generation, user feedback integration, and adaptive\nparameter allocation was conducted to assess their impact on computational\nefficiency, retrieval precision, linguistic authenticity, and scalability.\nFindings reveal that dynamic and ensemble LoRA adaptations significantly reduce\ntrainable parameters without sacrificing retrieval accuracy and generation\nquality in dialectal contexts. However, limitations remain in fully preserving\nfine-grained linguistic nuances, especially for low-resource settings like\nCantonese. The integration of real-time user feedback and domain-specific data\nremains underdeveloped, limiting model adaptability and personalization. While\nselective parameter freezing and nonlinear adaptation methods offer better\ntrade-offs between efficiency and accuracy, their robustness at scale remains\nan open challenge. This review highlights the promise of PEFT-enhanced RAG\nsystems for domain-specific language tasks and calls for future work targeting\ndialectal authenticity, dynamic adaptation, and scalable fine-tuning pipelines.", "AI": {"tldr": "该综述分析了参数高效微调（PEFT），特别是LoRA，在优化RAG系统处理粤语口语表达方面的最新进展，以解决数据稀缺和语言变异性问题。", "motivation": "现有RAG系统（如Qwen3、DeepSeek、Kimi）在理解和生成地道粤语口语表达方面面临挑战，原因在于标注数据有限和语言变异性高。", "method": "本综述通过系统分析，评估了LoRA在RAG框架中的集成、PEFT方法在检索和生成准确性上的基准表现、有限数据下的领域适应策略以及数据稀缺条件下的语义保真度微调技术。研究考察了LoRA变体、合成数据生成、用户反馈整合和自适应参数分配对计算效率、检索精度、语言真实性和可扩展性的影响。", "result": "研究发现，动态和集成LoRA适应方法显著减少了可训练参数，同时在方言环境中保持了检索准确性和生成质量。然而，在充分保留细粒度语言细微差别方面仍存在局限性，特别是在粤语等低资源设置中。实时用户反馈和领域特定数据整合仍不成熟，限制了模型的适应性和个性化。选择性参数冻结和非线性适应方法在效率和准确性之间提供了更好的权衡，但其大规模鲁棒性仍是一个开放挑战。", "conclusion": "PEFT增强型RAG系统在特定领域语言任务中展现出潜力。未来的工作应侧重于方言真实性、动态适应和可扩展的微调流程。"}}
{"id": "2508.08767", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.08767", "abs": "https://arxiv.org/abs/2508.08767", "authors": ["Kazuki Komura", "Kumi Ozaki", "Seiji Yamada"], "title": "Robot can reduce superior's dominance in group discussions with human social hierarchy", "comment": "8 pages, 7 figures. International Conference on Human-Agent\n  Interaction (HAI '24), November 24-27, 2024, Swansea, United Kingdom", "summary": "This study investigated whether robotic agents that deal with social\nhierarchical relationships can reduce the dominance of superiors and equalize\nparticipation among participants in discussions with hierarchical structures.\nThirty doctors and students having hierarchical relationship were gathered as\nparticipants, and an intervention experiment was conducted using a robot that\ncan encourage participants to speak depending on social hierarchy. These were\ncompared with strategies that intervened equally for all participants without\nconsidering hierarchy and with a no-action. The robots performed follow\nactions, showing backchanneling to speech, and encourage actions, prompting\nspeech from members with less speaking time, on the basis of the hierarchical\nrelationships among group members to equalize participation. The experimental\nresults revealed that the robot's actions could potentially influence the\nspeaking time among members, but it could not be conclusively stated that there\nwere significant differences between the robot's action conditions. However,\nthe results suggested that it might be possible to influence speaking time\nwithout decreasing the satisfaction of superiors. This indicates that in\ndiscussion scenarios where experienced superiors are likely to dominate,\ncontrolling the robot's backchanneling behavior could potentially suppress\ndominance and equalize participation among group members.", "AI": {"tldr": "本研究探讨了机器人代理如何通过干预，在具有层级结构的讨论中减少上级的主导地位并促进参与者发言平等。", "motivation": "在社会层级关系中，上级往往主导讨论，导致参与不均。本研究旨在探索机器人代理是否能减少这种主导地位并实现参与平等。", "method": "招募了30名具有层级关系的医生和学生作为参与者。实验设计包括三种干预策略：机器人根据社会层级鼓励发言、机器人对所有参与者平等干预、以及无干预。机器人通过“跟随行动”（对发言者进行反馈）和“鼓励行动”（提示发言时间较少的成员发言）来尝试平衡参与。", "result": "实验结果显示，机器人的行为可能影响成员的发言时间，但未能明确得出机器人不同行动条件之间存在显著差异。然而，结果表明，在不降低上级满意度的情况下，可能能够影响发言时间。", "conclusion": "在经验丰富的上级可能主导的讨论场景中，通过控制机器人的反馈（backchanneling）行为，有可能抑制主导地位，并使团队成员之间的参与趋于平等。"}}
{"id": "2508.08601", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08601", "abs": "https://arxiv.org/abs/2508.08601", "authors": ["Yan Team"], "title": "Yan: Foundational Interactive Video Generation", "comment": null, "summary": "We present Yan, a foundational framework for interactive video generation,\ncovering the entire pipeline from simulation and generation to editing.\nSpecifically, Yan comprises three core modules. AAA-level Simulation: We design\na highly-compressed, low-latency 3D-VAE coupled with a KV-cache-based\nshift-window denoising inference process, achieving real-time 1080P/60FPS\ninteractive simulation. Multi-Modal Generation: We introduce a hierarchical\nautoregressive caption method that injects game-specific knowledge into\nopen-domain multi-modal video diffusion models (VDMs), then transforming the\nVDM into a frame-wise, action-controllable, real-time infinite interactive\nvideo generator. Notably, when the textual and visual prompts are sourced from\ndifferent domains, the model demonstrates strong generalization, allowing it to\nblend and compose the style and mechanics across domains flexibly according to\nuser prompts. Multi-Granularity Editing: We propose a hybrid model that\nexplicitly disentangles interactive mechanics simulation from visual rendering,\nenabling multi-granularity video content editing during interaction through\ntext. Collectively, Yan offers an integration of these modules, pushing\ninteractive video generation beyond isolated capabilities toward a\ncomprehensive AI-driven interactive creation paradigm, paving the way for the\nnext generation of creative tools, media, and entertainment. The project page\nis: https://greatx3.github.io/Yan/.", "AI": {"tldr": "Yan是一个用于交互式视频生成的综合性基础框架，涵盖了模拟、生成和编辑的整个流程。", "motivation": "现有的交互式视频生成能力较为孤立，缺乏一个全面的AI驱动的交互式创作范式。", "method": "Yan框架包含三个核心模块：1) AAA级模拟：设计了高度压缩、低延迟的3D-VAE结合基于KV缓存的移窗去噪推理过程，实现实时1080P/60FPS交互模拟。2) 多模态生成：引入分层自回归字幕方法，将游戏特定知识注入开放域多模态视频扩散模型(VDM)，使其成为帧级、动作可控、实时无限交互视频生成器。3) 多粒度编辑：提出混合模型，显式分离交互机制模拟与视觉渲染，通过文本实现交互过程中的多粒度视频内容编辑。", "result": "Yan实现了实时1080P/60FPS的交互式模拟，在多模态生成中展示出强大的泛化能力，能够灵活地融合和组合不同领域风格和机制。同时，它支持在交互过程中进行多粒度视频内容编辑。", "conclusion": "Yan框架通过集成模拟、生成和编辑模块，将交互式视频生成从孤立能力推向全面的AI驱动交互式创作范式，为下一代创意工具、媒体和娱乐奠定了基础。"}}
{"id": "2508.08633", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2508.08633", "abs": "https://arxiv.org/abs/2508.08633", "authors": ["HuanYu Yang", "Fengming Zhu", "YangFan Wu", "Jianmin Ji"], "title": "Diminution: On Reducing the Size of Grounding ASP Programs", "comment": null, "summary": "Answer Set Programming (ASP) is often hindered by the grounding bottleneck:\nlarge Herbrand universes generate ground programs so large that solving becomes\ndifficult. Many methods employ ad-hoc heuristics to improve grounding\nperformance, motivating the need for a more formal and generalizable strategy.\nWe introduce the notion of diminution, defined as a selected subset of the\nHerbrand universe used to generate a reduced ground program before solving. We\ngive a formal definition of diminution, analyze its key properties, and study\nthe complexity of identifying it. We use a specific encoding that enables\noff-the-shelf ASP solver to evaluate candidate subsets. Our approach integrates\nseamlessly with existing grounders via domain predicates. In extensive\nexperiments on five benchmarks, applying diminutions selected by our strategy\nyields significant performance improvements, reducing grounding time by up to\n70% on average and decreasing the size of grounding files by up to 85%. These\nresults demonstrate that leveraging diminutions constitutes a robust and\ngeneral-purpose approach for alleviating the grounding bottleneck in ASP.", "AI": {"tldr": "本文提出“缩减”（diminution）概念，通过选择Herbrand域的子集来生成更小的接地程序，从而显著缓解ASP中的接地瓶颈，提升性能。", "motivation": "ASP（Answer Set Programming）常受制于接地瓶颈，即大型Herbrand域生成过大的接地程序，导致求解困难。现有方法多依赖特设启发式，缺乏形式化和通用策略。", "method": "引入“缩减”（diminution）概念，定义为Herbrand域的选定子集，用于生成缩减的接地程序。形式化定义了缩减，分析了其关键性质，并研究了识别缩减的复杂性。使用特定编码，使现成的ASP求解器能够评估候选子集，并通过域谓词与现有接地器无缝集成。", "result": "在五个基准测试中，应用该策略选择的缩减带来了显著的性能提升，平均接地时间减少高达70%，接地文件大小减少高达85%。", "conclusion": "利用缩减是缓解ASP中接地瓶颈的一种鲁棒且通用的方法。"}}
{"id": "2508.08636", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.08636", "abs": "https://arxiv.org/abs/2508.08636", "authors": ["Peiji Li", "Jiasheng Ye", "Yongkang Chen", "Yichuan Ma", "Zijie Yu", "Kedi Chen", "Ganqu Cui", "Haozhan Li", "Jiacheng Chen", "Chengqi Lyu", "Wenwei Zhang", "Linyang Li", "Qipeng Guo", "Dahua Lin", "Bowen Zhou", "Kai Chen"], "title": "InternBootcamp Technical Report: Boosting LLM Reasoning with Verifiable Task Scaling", "comment": "InternBootcamp Tech Report", "summary": "Large language models (LLMs) have revolutionized artificial intelligence by\nenabling complex reasoning capabilities. While recent advancements in\nreinforcement learning (RL) have primarily focused on domain-specific reasoning\ntasks (e.g., mathematics or code generation), real-world reasoning scenarios\noften require models to handle diverse and complex environments that\nnarrow-domain benchmarks cannot fully capture. To address this gap, we present\nInternBootcamp, an open-source framework comprising 1000+ domain-diverse task\nenvironments specifically designed for LLM reasoning research. Our codebase\noffers two key functionalities: (1) automated generation of unlimited\ntraining/testing cases with configurable difficulty levels, and (2) integrated\nverification modules for objective response evaluation. These features make\nInternBootcamp fundamental infrastructure for RL-based model optimization,\nsynthetic data generation, and model evaluation. Although manually developing\nsuch a framework with enormous task coverage is extremely cumbersome, we\naccelerate the development procedure through an automated agent workflow\nsupplemented by manual validation protocols, which enables the task scope to\nexpand rapidly. % With these bootcamps, we further establish Bootcamp-EVAL, an\nautomatically generated benchmark for comprehensive performance assessment.\nEvaluation reveals that frontier models still underperform in many reasoning\ntasks, while training with InternBootcamp provides an effective way to\nsignificantly improve performance, leading to our 32B model that achieves\nstate-of-the-art results on Bootcamp-EVAL and excels on other established\nbenchmarks. In particular, we validate that consistent performance gains come\nfrom including more training tasks, namely \\textbf{task scaling}, over two\norders of magnitude, offering a promising route towards capable reasoning\ngeneralist.", "AI": {"tldr": "该论文提出了InternBootcamp，一个包含1000多个领域多样化任务环境的开源框架，用于LLM推理研究。它支持自动化任务生成和结果验证，并通过任务扩展（task scaling）显著提升了模型在多样化推理任务上的性能，验证了其作为通用推理模型训练基础设施的有效性。", "motivation": "现有强化学习（RL）在LLM推理方面的进展主要集中于特定领域（如数学或代码生成），但现实世界推理场景需要模型处理多样且复杂的环境，而狭窄领域的基准无法充分捕捉这些需求。因此，需要一个能涵盖广泛领域和复杂性的框架来弥补这一差距。", "method": "开发了InternBootcamp框架，包含1000多个领域多样化的任务环境，并提供两个核心功能：1) 自动化生成无限量、难度可配置的训练/测试案例；2) 集成验证模块用于客观响应评估。为了加速开发，采用了自动化代理工作流辅以人工验证协议。此外，基于此框架建立了自动生成的基准测试集Bootcamp-EVAL。", "result": "评估显示，现有前沿模型在许多推理任务上表现仍不佳。使用InternBootcamp进行训练能显著提高模型性能，作者的32B模型在Bootcamp-EVAL上取得了最先进的结果，并在其他现有基准测试中表现出色。特别验证了通过增加训练任务数量（即任务扩展）可以带来持续的性能提升，且规模超过两个数量级。", "conclusion": "InternBootcamp是一个用于RL模型优化、合成数据生成和模型评估的基础设施。通过任务扩展（task scaling）可以有效提升LLM的推理能力，为开发具有强大推理能力的通用模型提供了有前景的途径。"}}
{"id": "2508.08896", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.08896", "abs": "https://arxiv.org/abs/2508.08896", "authors": ["Haoyu Zhao", "Linghao Zhuang", "Xingyue Zhao", "Cheng Zeng", "Haoran Xu", "Yuming Jiang", "Jun Cen", "Kexiang Wang", "Jiayan Guo", "Siteng Huang", "Xin Li", "Deli Zhao", "Hua Zou"], "title": "Towards Affordance-Aware Robotic Dexterous Grasping with Human-like Priors", "comment": "13 pages, 8 figures", "summary": "A dexterous hand capable of generalizable grasping objects is fundamental for\nthe development of general-purpose embodied AI. However, previous methods focus\nnarrowly on low-level grasp stability metrics, neglecting affordance-aware\npositioning and human-like poses which are crucial for downstream manipulation.\nTo address these limitations, we propose AffordDex, a novel framework with\ntwo-stage training that learns a universal grasping policy with an inherent\nunderstanding of both motion priors and object affordances. In the first stage,\na trajectory imitator is pre-trained on a large corpus of human hand motions to\ninstill a strong prior for natural movement. In the second stage, a residual\nmodule is trained to adapt these general human-like motions to specific object\ninstances. This refinement is critically guided by two components: our Negative\nAffordance-aware Segmentation (NAA) module, which identifies functionally\ninappropriate contact regions, and a privileged teacher-student distillation\nprocess that ensures the final vision-based policy is highly successful.\nExtensive experiments demonstrate that AffordDex not only achieves universal\ndexterous grasping but also remains remarkably human-like in posture and\nfunctionally appropriate in contact location. As a result, AffordDex\nsignificantly outperforms state-of-the-art baselines across seen objects,\nunseen instances, and even entirely novel categories.", "AI": {"tldr": "AffordDex是一种新型两阶段训练框架，通过模仿人类运动和理解物体功能，实现通用灵巧抓取，同时保持类人姿态和功能性接触，显著优于现有方法。", "motivation": "现有方法过度关注低级抓取稳定性，忽略了对下游操作至关重要的功能感知定位和类人姿态，这限制了通用具身AI的发展。", "method": "提出AffordDex框架，采用两阶段训练：第一阶段，轨迹模仿器预训练大量人类手部运动数据，学习自然运动先验；第二阶段，训练残差模块以适应特定物体实例，通过负功能感知分割（NAA）识别不当接触区域，并结合特权教师-学生蒸馏过程，确保最终视觉策略高效成功。", "result": "AffordDex不仅实现了通用灵巧抓取，还在姿态上保持了显著的类人特征，接触位置功能性强。在已见物体、未见实例甚至全新类别上，其性能均显著超越现有最佳基线。", "conclusion": "AffordDex成功解决了通用灵巧抓取中功能感知定位和类人姿态的挑战，为通用具身AI的发展提供了重要基础，并展示了在复杂抓取任务中的卓越泛化能力和实用性。"}}
{"id": "2508.08604", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.08604", "abs": "https://arxiv.org/abs/2508.08604", "authors": ["Jihwan Park", "Taehoon song", "Sanghyeok Lee", "Miso Choi", "Hyunwoo J. Kim"], "title": "Transferable Model-agnostic Vision-Language Model Adaptation for Efficient Weak-to-Strong Generalization", "comment": null, "summary": "Vision-Language Models (VLMs) have been widely used in various visual\nrecognition tasks due to their remarkable generalization capabilities. As these\nmodels grow in size and complexity, fine-tuning becomes costly, emphasizing the\nneed to reuse adaptation knowledge from 'weaker' models to efficiently enhance\n'stronger' ones. However, existing adaptation transfer methods exhibit limited\ntransferability across models due to their model-specific design and high\ncomputational demands. To tackle this, we propose Transferable Model-agnostic\nadapter (TransMiter), a light-weight adapter that improves vision-language\nmodels 'without backpropagation'. TransMiter captures the knowledge gap between\npre-trained and fine-tuned VLMs, in an 'unsupervised' manner. Once trained,\nthis knowledge can be seamlessly transferred across different models without\nthe need for backpropagation. Moreover, TransMiter consists of only a few\nlayers, inducing a negligible additional inference cost. Notably, supplementing\nthe process with a few labeled data further yields additional performance gain,\noften surpassing a fine-tuned stronger model, with a marginal training cost.\nExperimental results and analyses demonstrate that TransMiter effectively and\nefficiently transfers adaptation knowledge while preserving generalization\nabilities across VLMs of different sizes and architectures in visual\nrecognition tasks.", "AI": {"tldr": "提出TransMiter，一种轻量级、模型无关的适配器，无需反向传播即可在不同视觉语言模型（VLM）之间高效迁移适应性知识，同时保持泛化能力。", "motivation": "大型VLM微调成本高昂，现有适应性知识迁移方法因其模型特定设计和高计算需求，导致跨模型可迁移性有限，无法有效重用“弱”模型的适应知识来提升“强”模型。", "method": "提出Transferable Model-agnostic adapter (TransMiter)，它是一种轻量级适配器，以“无监督”方式捕获预训练和微调VLM之间的知识差距。一旦训练完成，该知识可以在不同模型之间无缝迁移，无需反向传播。TransMiter仅包含少量层，推理成本可忽略不计。此外，结合少量标注数据可进一步提升性能，甚至超越微调的更强模型，且训练成本极低。", "result": "实验结果表明，TransMiter能够有效且高效地在不同尺寸和架构的VLM之间迁移适应性知识，同时保持其在视觉识别任务中的泛化能力。结合少量标注数据可实现显著的性能提升，甚至超越微调的更强模型。", "conclusion": "TransMiter提供了一种有效且高效的解决方案，用于在不同视觉语言模型之间迁移适应性知识，解决了现有方法在跨模型可迁移性和计算成本方面的局限性，实现了“无需反向传播”的模型提升。"}}
{"id": "2508.08646", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08646", "abs": "https://arxiv.org/abs/2508.08646", "authors": ["Naama Kashani", "Mira Cohen", "Uri Shaham"], "title": "P-CAFE: Personalized Cost-Aware Incremental Feature Selection For Electronic Health Records", "comment": "17 pages, 5 figures", "summary": "Electronic Health Records (EHR) have revolutionized healthcare by digitizing\npatient data, improving accessibility, and streamlining clinical workflows.\nHowever, extracting meaningful insights from these complex and multimodal\ndatasets remains a significant challenge for researchers. Traditional feature\nselection methods often struggle with the inherent sparsity and heterogeneity\nof EHR data, especially when accounting for patient-specific variations and\nfeature costs in clinical applications. To address these challenges, we propose\na novel personalized, online and cost-aware feature selection framework\ntailored specifically for EHR datasets. The features are aquired in an online\nfashion for individual patients, incorporating budgetary constraints and\nfeature variability costs. The framework is designed to effectively manage\nsparse and multimodal data, ensuring robust and scalable performance in diverse\nhealthcare contexts. A primary application of our proposed method is to support\nphysicians' decision making in patient screening scenarios. By guiding\nphysicians toward incremental acquisition of the most informative features\nwithin budget constraints, our approach aims to increase diagnostic confidence\nwhile optimizing resource utilization.", "AI": {"tldr": "针对电子健康记录（EHR）数据的挑战，本文提出一种新型个性化、在线、成本感知的特征选择框架，旨在辅助医生决策并优化资源利用。", "motivation": "电子健康记录（EHR）数据复杂且多模态，传统特征选择方法难以处理其固有的稀疏性、异构性、患者特异性变异及临床应用中的特征成本，导致从EHR中提取有意义的洞察面临重大挑战。", "method": "本文提出一个新颖的个性化、在线且成本感知的特征选择框架，专门为EHR数据集量身定制。该框架以在线方式为个体患者获取特征，并整合预算限制和特征可变成本，旨在有效管理稀疏和多模态数据。", "result": "该框架能够确保在多样化医疗环境中实现稳健和可扩展的性能。通过在预算约束内指导医生逐步获取最具信息量的特征，旨在提高诊断信心并优化资源利用。", "conclusion": "所提出的方法主要应用于支持医生在患者筛查场景中的决策，通过优化特征获取过程来提升诊断效率和资源利用率。"}}
{"id": "2508.08645", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.08645", "abs": "https://arxiv.org/abs/2508.08645", "authors": ["Zheng Wu", "Heyuan Huang", "Yanjia Yang", "Yuanyi Song", "Xingyu Lou", "Weiwen Liu", "Weinan Zhang", "Jun Wang", "Zhuosheng Zhang"], "title": "Quick on the Uptake: Eliciting Implicit Intents from Human Demonstrations for Personalized Mobile-Use Agents", "comment": null, "summary": "As multimodal large language models advance rapidly, the automation of mobile\ntasks has become increasingly feasible through the use of mobile-use agents\nthat mimic human interactions from graphical user interface. To further enhance\nmobile-use agents, previous studies employ demonstration learning to improve\nmobile-use agents from human demonstrations. However, these methods focus\nsolely on the explicit intention flows of humans (e.g., step sequences) while\nneglecting implicit intention flows (e.g., personal preferences), which makes\nit difficult to construct personalized mobile-use agents. In this work, to\nevaluate the \\textbf{I}ntention \\textbf{A}lignment \\textbf{R}ate between\nmobile-use agents and humans, we first collect \\textbf{MobileIAR}, a dataset\ncontaining human-intent-aligned actions and ground-truth actions. This enables\na comprehensive assessment of the agents' understanding of human intent. Then\nwe propose \\textbf{IFRAgent}, a framework built upon \\textbf{I}ntention\n\\textbf{F}low \\textbf{R}ecognition from human demonstrations. IFRAgent analyzes\nexplicit intention flows from human demonstrations to construct a query-level\nvector library of standard operating procedures (SOP), and analyzes implicit\nintention flows to build a user-level habit repository. IFRAgent then leverages\na SOP extractor combined with retrieval-augmented generation and a query\nrewriter to generate personalized query and SOP from a raw ambiguous query,\nenhancing the alignment between mobile-use agents and human intent.\nExperimental results demonstrate that IFRAgent outperforms baselines by an\naverage of 6.79\\% (32.06\\% relative improvement) in human intention alignment\nrate and improves step completion rates by an average of 5.30\\% (26.34\\%\nrelative improvement). The codes are available at\nhttps://github.com/MadeAgents/Quick-on-the-Uptake.", "AI": {"tldr": "该研究通过识别并利用人类的显式（操作步骤）和隐式（个人偏好）意图流，提出了一个名为IFRAgent的框架，旨在提高移动代理与人类意图的对齐度，从而实现更个性化的移动任务自动化。", "motivation": "现有的移动代理通过演示学习来提升，但它们仅关注人类的显式意图流（如步骤序列），忽略了隐式意图流（如个人偏好），这使得构建个性化移动代理变得困难。", "method": "首先，收集了MobileIAR数据集，其中包含与人类意图对齐的动作和真实动作，用于评估代理对人类意图的理解。然后，提出了IFRAgent框架，该框架通过分析人类演示中的显式意图流构建标准操作程序（SOP）的查询级向量库，并通过分析隐式意图流建立用户级习惯库。IFRAgent结合SOP提取器、检索增强生成和查询重写器，从原始模糊查询中生成个性化查询和SOP。", "result": "实验结果表明，IFRAgent在人类意图对齐率上平均优于基线6.79%（相对提升32.06%），在步骤完成率上平均提升5.30%（相对提升26.34%）。", "conclusion": "IFRAgent通过识别和整合人类的显式和隐式意图流，显著提高了移动代理与人类意图的对齐程度和任务完成率，有效解决了现有方法在个性化方面的不足。"}}
{"id": "2508.08982", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.08982", "abs": "https://arxiv.org/abs/2508.08982", "authors": ["Seungeun Rho", "Kartik Garg", "Morgan Byrd", "Sehoon Ha"], "title": "Unsupervised Skill Discovery as Exploration for Learning Agile Locomotion", "comment": "Conference on Robot Learning 2025", "summary": "Exploration is crucial for enabling legged robots to learn agile locomotion\nbehaviors that can overcome diverse obstacles. However, such exploration is\ninherently challenging, and we often rely on extensive reward engineering,\nexpert demonstrations, or curriculum learning - all of which limit\ngeneralizability. In this work, we propose Skill Discovery as Exploration\n(SDAX), a novel learning framework that significantly reduces human engineering\neffort. SDAX leverages unsupervised skill discovery to autonomously acquire a\ndiverse repertoire of skills for overcoming obstacles. To dynamically regulate\nthe level of exploration during training, SDAX employs a bi-level optimization\nprocess that autonomously adjusts the degree of exploration. We demonstrate\nthat SDAX enables quadrupedal robots to acquire highly agile behaviors\nincluding crawling, climbing, leaping, and executing complex maneuvers such as\njumping off vertical walls. Finally, we deploy the learned policy on real\nhardware, validating its successful transfer to the real world.", "AI": {"tldr": "SDAX是一个新的学习框架，它通过无监督技能发现和双层优化，使四足机器人在无需大量人工干预的情况下，自主学习并掌握各种敏捷的障碍物穿越行为，并成功部署到真实硬件上。", "motivation": "现有legged机器人学习敏捷运动行为的方法（如奖励工程、专家演示、课程学习）通常需要大量人工干预，且泛化能力有限。探索本身具有挑战性，需要减少人类工程量并提高泛化性。", "method": "本文提出了SDAX（Skill Discovery as Exploration）框架。它利用无监督技能发现来自主获取多样化的技能库以克服障碍物，并通过一个双层优化过程动态调节训练过程中的探索水平。", "result": "SDAX使四足机器人能够学习到高度敏捷的行为，包括爬行、攀爬、跳跃以及执行复杂的机动，例如从垂直墙壁上跳下。所学习的策略成功部署到真实硬件上，验证了其在现实世界中的迁移能力。", "conclusion": "SDAX显著减少了人类工程量，通过自主技能发现和探索调节，使legged机器人能够学习到多样化和敏捷的障碍物穿越行为，并成功实现了从模拟到真实世界的迁移。"}}
{"id": "2508.08605", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08605", "abs": "https://arxiv.org/abs/2508.08605", "authors": ["Honglei Xu", "Zhilu Zhang", "Junjie Fan", "Xiaohe Wu", "Wangmeng Zuo"], "title": "SelfHVD: Self-Supervised Handheld Video Deblurring for Mobile Phones", "comment": null, "summary": "Shooting video with a handheld mobile phone, the most common photographic\ndevice, often results in blurry frames due to shaking hands and other\ninstability factors. Although previous video deblurring methods have achieved\nimpressive progress, they still struggle to perform satisfactorily on\nreal-world handheld video due to the blur domain gap between training and\ntesting data. To address the issue, we propose a self-supervised method for\nhandheld video deblurring, which is driven by sharp clues in the video. First,\nto train the deblurring model, we extract the sharp clues from the video and\ntake them as misalignment labels of neighboring blurry frames. Second, to\nimprove the model's ability, we propose a novel Self-Enhanced Video Deblurring\n(SEVD) method to create higher-quality paired video data. Third, we propose a\nSelf-Constrained Spatial Consistency Maintenance (SCSCM) method to regularize\nthe model, preventing position shifts between the output and input frames.\nMoreover, we construct a synthetic and a real-world handheld video dataset for\nhandheld video deblurring. Extensive experiments on these two and other common\nreal-world datasets demonstrate that our method significantly outperforms\nexisting self-supervised ones. The code and datasets are publicly available at\nhttps://github.com/cshonglei/SelfHVD.", "AI": {"tldr": "该论文提出了一种自监督的手持视频去模糊方法，利用视频中的清晰线索，通过数据增强和空间一致性维护来解决真实世界手持视频的模糊域差距问题。", "motivation": "手持手机拍摄的视频常因抖动导致模糊帧，现有视频去模糊方法在真实世界手持视频上表现不佳，因为训练和测试数据之间存在模糊域差距。", "method": "1. 提出自监督方法，从视频中提取清晰线索作为相邻模糊帧的错位标签来训练去模糊模型。2. 提出新型自增强视频去模糊（SEVD）方法，生成更高质量的配对视频数据以提升模型能力。3. 提出自约束空间一致性维护（SCSCM）方法，正则化模型以防止输出和输入帧之间的位置偏移。4. 构建了合成和真实世界手持视频数据集。", "result": "在自建数据集和其他常用真实世界数据集上的大量实验表明，该方法显著优于现有的自监督方法。", "conclusion": "该研究提供了一种有效且鲁棒的自监督手持视频去模糊方案，并通过公开代码和数据集促进了该领域的发展。"}}
{"id": "2508.08652", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08652", "abs": "https://arxiv.org/abs/2508.08652", "authors": ["Vishakha Lall", "Yisi Liu"], "title": "Prompt-and-Check: Using Large Language Models to Evaluate Communication Protocol Compliance in Simulation-Based Training", "comment": null, "summary": "Accurate evaluation of procedural communication compliance is essential in\nsimulation-based training, particularly in safety-critical domains where\nadherence to compliance checklists reflects operational competence. This paper\nexplores a lightweight, deployable approach using prompt-based inference with\nopen-source large language models (LLMs) that can run efficiently on\nconsumer-grade GPUs. We present Prompt-and-Check, a method that uses\ncontext-rich prompts to evaluate whether each checklist item in a protocol has\nbeen fulfilled, solely based on transcribed verbal exchanges. We perform a case\nstudy in the maritime domain with participants performing an identical\nsimulation task, and experiment with models such as LLama 2 7B, LLaMA 3 8B and\nMistral 7B, running locally on an RTX 4070 GPU. For each checklist item, a\nprompt incorporating relevant transcript excerpts is fed into the model, which\noutputs a compliance judgment. We assess model outputs against expert-annotated\nground truth using classification accuracy and agreement scores. Our findings\ndemonstrate that prompting enables effective context-aware reasoning without\ntask-specific training. This study highlights the practical utility of LLMs in\naugmenting debriefing, performance feedback, and automated assessment in\ntraining environments.", "AI": {"tldr": "本文提出了一种名为“Prompt-and-Check”的轻量级方法，利用开源大语言模型（LLMs）通过提示推理来评估模拟训练中程序性沟通的合规性，特别是在安全关键领域。", "motivation": "在安全关键领域的模拟训练中，准确评估程序性沟通合规性至关重要，因为这反映了操作能力。现有评估可能效率不高或需要大量资源。", "method": "该方法使用基于提示的推理，结合开源LLMs（如LLaMA 2 7B、LLaMA 3 8B和Mistral 7B），可在消费级GPU上高效运行。通过向模型输入包含相关对话摘录的上下文丰富提示，评估每个清单项是否已完成。研究在海事领域的模拟任务中进行了案例研究，并使用分类准确性和一致性分数评估模型输出与专家标注的真实情况。", "result": "研究结果表明，提示方法无需进行特定任务训练，即可实现有效的上下文感知推理。这突显了LLMs在增强训练环境中的汇报、绩效反馈和自动化评估方面的实用性。", "conclusion": "大语言模型（LLMs）提供了一种实用且轻量级的方法，可以有效地自动化评估模拟训练中程序性沟通的合规性，从而增强训练效果和反馈机制。"}}
{"id": "2508.08649", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.08649", "abs": "https://arxiv.org/abs/2508.08649", "authors": ["Jakub Šmíd", "Pavel Přibáň", "Pavel Král"], "title": "LLaMA-Based Models for Aspect-Based Sentiment Analysis", "comment": "Published in Proceedings of the 14th Workshop on Computational\n  Approaches to Subjectivity, Sentiment, & Social Media Analysis (WASSA 2024).\n  Official version: https://aclanthology.org/2024.wassa-1.6/", "summary": "While large language models (LLMs) show promise for various tasks, their\nperformance in compound aspect-based sentiment analysis (ABSA) tasks lags\nbehind fine-tuned models. However, the potential of LLMs fine-tuned for ABSA\nremains unexplored. This paper examines the capabilities of open-source LLMs\nfine-tuned for ABSA, focusing on LLaMA-based models. We evaluate the\nperformance across four tasks and eight English datasets, finding that the\nfine-tuned Orca~2 model surpasses state-of-the-art results in all tasks.\nHowever, all models struggle in zero-shot and few-shot scenarios compared to\nfully fine-tuned ones. Additionally, we conduct error analysis to identify\nchallenges faced by fine-tuned models.", "AI": {"tldr": "研究发现，针对方面级情感分析（ABSA）微调的大型语言模型，特别是Orca 2，在所有任务中均超越了现有最佳水平，但在零样本和少样本场景中表现不佳。", "motivation": "尽管大型语言模型（LLMs）在多项任务中表现出潜力，但在复合方面级情感分析（ABSA）任务中，其性能落后于经过微调的模型。然而，针对ABSA任务微调LLMs的潜力尚未被充分探索。", "method": "本文研究了针对ABSA微调的开源LLMs（侧重于LLaMA系列模型）的能力。通过在四个任务和八个英文数据集上评估其性能，并进行了错误分析以识别微调模型面临的挑战。", "result": "微调后的Orca 2模型在所有任务中均超越了现有最佳（SOTA）结果。然而，所有模型在零样本和少样本场景下的表现均不如完全微调的模型。", "conclusion": "针对ABSA任务微调大型语言模型（如Orca 2）能够显著提升性能，甚至超越现有最佳水平。尽管如此，在零样本和少样本设置下，这些模型的性能仍有待提高。"}}
{"id": "2508.08983", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08983", "abs": "https://arxiv.org/abs/2508.08983", "authors": ["Ben Zandonati", "Tomás Lozano-Pérez", "Leslie Pack Kaelbling"], "title": "Rational Inverse Reasoning", "comment": null, "summary": "Humans can observe a single, imperfect demonstration and immediately\ngeneralize to very different problem settings. Robots, in contrast, often\nrequire hundreds of examples and still struggle to generalize beyond the\ntraining conditions. We argue that this limitation arises from the inability to\nrecover the latent explanations that underpin intelligent behavior, and that\nthese explanations can take the form of structured programs consisting of\nhigh-level goals, sub-task decomposition, and execution constraints. In this\nwork, we introduce Rational Inverse Reasoning (RIR), a framework for inferring\nthese latent programs through a hierarchical generative model of behavior. RIR\nframes few-shot imitation as Bayesian program induction: a vision-language\nmodel iteratively proposes structured symbolic task hypotheses, while a\nplanner-in-the-loop inference scheme scores each by the likelihood of the\nobserved demonstration under that hypothesis. This loop yields a posterior over\nconcise, executable programs. We evaluate RIR on a suite of continuous\nmanipulation tasks designed to test one-shot and few-shot generalization across\nvariations in object pose, count, geometry, and layout. With as little as one\ndemonstration, RIR infers the intended task structure and generalizes to novel\nsettings, outperforming state-of-the-art vision-language model baselines.", "AI": {"tldr": "该论文提出了一种名为“理性逆向推理”（RIR）的框架，通过贝叶斯程序归纳从少量（甚至一个）演示中推断出机器人行为背后的潜在结构化程序，从而使机器人能够像人类一样实现快速泛化。", "motivation": "机器人目前需要大量示例才能泛化，且难以超越训练条件，这与人类能从单个不完美演示中立即泛化形成鲜明对比。这种局限性源于机器人无法恢复智能行为背后的潜在解释，即由高级目标、子任务分解和执行约束组成的结构化程序。", "method": "引入了“理性逆向推理”（RIR）框架，通过行为的层次生成模型推断潜在程序。RIR将少样本模仿视为贝叶斯程序归纳：一个视觉-语言模型迭代地提出结构化的符号任务假设，同时一个包含规划器的推理方案根据观察到的演示在该假设下的似然性来评分。这个循环产生了一个关于简洁、可执行程序的后验分布。", "result": "RIR在设计用于测试物体姿态、数量、几何形状和布局变化的连续操作任务上进行了评估。结果显示，RIR仅需一个演示就能推断出预期的任务结构，并泛化到新颖设置，优于当前最先进的视觉-语言模型基线。", "conclusion": "RIR框架通过推断行为背后的潜在结构化程序，使得机器人能够从极少量（甚至一个）演示中学习并泛化到多种新颖的任务设置，显著提升了机器人的少样本模仿和泛化能力。"}}
{"id": "2508.08608", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.08608", "abs": "https://arxiv.org/abs/2508.08608", "authors": ["Justin London"], "title": "Neural Artistic Style and Color Transfer Using Deep Learning", "comment": null, "summary": "Neural artistic style transfers and blends the content and style\nrepresentation of one image with the style of another. This enables artists to\ncreate unique innovative visuals and enhances artistic expression in various\nfields including art, design, and film. Color transfer algorithms are an\nimportant in digital image processing by adjusting the color information in a\ntarget image based on the colors in the source image. Color transfer enhances\nimages and videos in film and photography, and can aid in image correction. We\nintroduce a methodology that combines neural artistic style with color\ntransfer. The method uses the Kullback-Leibler (KL) divergence to\nquantitatively evaluate color and luminance histogram matching algorithms\nincluding Reinhard global color transfer, iteration distribution transfer\n(IDT), IDT with regrain, Cholesky, and PCA between the original and neural\nartistic style transferred image using deep learning. We estimate the color\nchannel kernel densities. Various experiments are performed to evaluate the KL\nof these algorithms and their color histograms for style to content transfer.", "AI": {"tldr": "该研究结合了神经艺术风格迁移和色彩迁移，并使用KL散度定量评估了多种色彩和亮度直方图匹配算法在风格迁移中的效果。", "motivation": "神经艺术风格迁移能创造独特视觉效果，但色彩传递是数字图像处理的重要组成部分，能增强图像并辅助校正。本研究旨在将两者结合，并提供一种定量评估色彩匹配效果的方法。", "method": "引入了一种结合神经艺术风格迁移和色彩迁移的方法。使用Kullback-Leibler（KL）散度定量评估了Reinhard全局色彩迁移、迭代分布迁移（IDT）、带re-grain的IDT、Cholesky和PCA等色彩及亮度直方图匹配算法在原始图像和经神经艺术风格迁移后的图像之间的效果。同时估计了色彩通道的核密度。", "result": "通过各种实验评估了这些算法的KL散度及其色彩直方图在风格到内容迁移中的表现。", "conclusion": "该研究提供了一种结合神经艺术风格迁移与色彩迁移的新方法，并建立了基于KL散度的定量评估框架，用于比较不同色彩匹配算法在风格迁移上下文中的性能。"}}
{"id": "2508.08659", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08659", "abs": "https://arxiv.org/abs/2508.08659", "authors": ["Bachtiar Herdianto", "Romain Billot", "Flavien Lucas", "Marc Sevaux", "Daniele Vigo"], "title": "Hybrid Node-Destroyer Model with Large Neighborhood Search for Solving the Capacitated Vehicle Routing Problem", "comment": "19 pages, 10 figures", "summary": "In this research, we propose an iterative learning hybrid optimization solver\ndeveloped to strengthen the performance of metaheuristic algorithms in solving\nthe Capacitated Vehicle Routing Problem (CVRP). The iterative hybrid mechanism\nintegrates the proposed Node-Destroyer Model, a machine learning hybrid model\nthat utilized Graph Neural Networks (GNNs) such identifies and selects customer\nnodes to guide the Large Neighborhood Search (LNS) operator within the\nmetaheuristic optimization frameworks. This model leverages the structural\nproperties of the problem and solution that can be represented as a graph, to\nguide strategic selections concerning node removal. The proposed approach\nreduces operational complexity and scales down the search space involved in the\noptimization process. The hybrid approach is applied specifically to the CVRP\nand does not require retraining across problem instances of different sizes.\nThe proposed hybrid mechanism is able to improve the performance of baseline\nmetaheuristic algorithms. Our approach not only enhances the solution quality\nfor standard CVRP benchmarks but also proves scalability on very large-scale\ninstances with up to 30,000 customer nodes. Experimental evaluations on\nbenchmark datasets show that the proposed hybrid mechanism is capable of\nimproving different baseline algorithms, achieving better quality of solutions\nunder similar settings.", "AI": {"tldr": "本研究提出一种迭代学习混合优化器，通过结合图神经网络（GNN）和大规模邻域搜索（LNS），增强元启发式算法解决带容量车辆路径问题（CVRP）的性能。该方法提高了求解质量，并能扩展到大规模问题实例。", "motivation": "旨在提升元启发式算法在解决带容量车辆路径问题（CVRP）时的性能，以获得更高质量的解决方案和更好的可扩展性。", "method": "提出了一种迭代学习混合优化器，其核心是“节点销毁器模型”（Node-Destroyer Model）。该模型利用图神经网络（GNN）识别和选择客户节点，以指导元启发式框架中的大规模邻域搜索（LNS）操作。该方法利用问题和解的图结构属性来指导节点移除，从而降低操作复杂性并缩小搜索空间。该混合方法专用于CVRP，且无需针对不同规模的实例进行重新训练。", "result": "所提出的混合机制能够显著提升基线元启发式算法的性能。实验结果表明，该方法不仅提高了标准CVRP基准测试的解质量，还在高达30,000个客户节点的大规模实例上展现出良好的可扩展性。在相似设置下，该机制能够改进不同的基线算法，获得更高质量的解决方案。", "conclusion": "所提出的混合机制有效提升了元启发式算法在解决CVRP时的性能，在提高解质量和处理大规模实例的可扩展性方面均表现出色，证明了其对不同基线算法的通用性和优越性。"}}
{"id": "2508.08650", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.08650", "abs": "https://arxiv.org/abs/2508.08650", "authors": ["Jakub Šmíd", "Pavel Přibáň", "Pavel Král"], "title": "UWB at WASSA-2024 Shared Task 2: Cross-lingual Emotion Detection", "comment": "Published in Proceedings of the 14th Workshop on Computational\n  Approaches to Subjectivity, Sentiment, & Social Media Analysis (WASSA 2024).\n  Official version: https://aclanthology.org/2024.wassa-1.47/", "summary": "This paper presents our system built for the WASSA-2024 Cross-lingual Emotion\nDetection Shared Task. The task consists of two subtasks: first, to assess an\nemotion label from six possible classes for a given tweet in one of five\nlanguages, and second, to predict words triggering the detected emotions in\nbinary and numerical formats. Our proposed approach revolves around fine-tuning\nquantized large language models, specifically Orca~2, with low-rank adapters\n(LoRA) and multilingual Transformer-based models, such as XLM-R and mT5. We\nenhance performance through machine translation for both subtasks and trigger\nword switching for the second subtask. The system achieves excellent\nperformance, ranking 1st in numerical trigger words detection, 3rd in binary\ntrigger words detection, and 7th in emotion detection.", "AI": {"tldr": "本文介绍了一个为WASSA-2024跨语言情感检测共享任务构建的系统，该系统通过微调量化大型语言模型和多语言Transformer模型，实现了情感标签和触发词的检测。", "motivation": "参与WASSA-2024跨语言情感检测共享任务，该任务包含两个子任务：从五种语言的推文中识别六种情感标签，以及预测触发这些情感的词语。", "method": "核心方法是使用LoRA微调量化大型语言模型（如Orca 2）和多语言Transformer模型（如XLM-R和mT5）。通过机器翻译和触发词切换技术进一步提升性能。", "result": "系统在数值触发词检测中排名第1，在二元触发词检测中排名第3，在情感检测中排名第7。", "conclusion": "该系统在WASSA-2024跨语言情感检测共享任务中取得了出色的表现。"}}
{"id": "2508.08999", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.08999", "abs": "https://arxiv.org/abs/2508.08999", "authors": ["Chao Wang", "Michael Gienger", "Fan Zhang"], "title": "Generation of Real-time Robotic Emotional Expressions Learning from Human Demonstration in Mixed Reality", "comment": "4", "summary": "Expressive behaviors in robots are critical for effectively conveying their\nemotional states during interactions with humans. In this work, we present a\nframework that autonomously generates realistic and diverse robotic emotional\nexpressions based on expert human demonstrations captured in Mixed Reality\n(MR). Our system enables experts to teleoperate a virtual robot from a\nfirst-person perspective, capturing their facial expressions, head movements,\nand upper-body gestures, and mapping these behaviors onto corresponding robotic\ncomponents including eyes, ears, neck, and arms. Leveraging a\nflow-matching-based generative process, our model learns to produce coherent\nand varied behaviors in real-time in response to moving objects, conditioned\nexplicitly on given emotional states. A preliminary test validated the\neffectiveness of our approach for generating autonomous expressions.", "AI": {"tldr": "该论文提出一个框架，利用混合现实中专家的人类演示，并通过基于流匹配的生成过程，自主生成逼真且多样的机器人情感表达。", "motivation": "机器人在与人类互动时，表达性行为对于有效传达其情感状态至关重要。", "method": "系统允许专家以第一人称视角远程操作虚拟机器人，捕获其面部表情、头部运动和上半身手势，并将其映射到机器人相应部件（眼睛、耳朵、颈部、手臂）。利用基于流匹配的生成过程，模型学习在响应移动物体时，根据给定情感状态实时生成连贯且多样的行为。", "result": "初步测试验证了该方法在生成自主表达方面的有效性。", "conclusion": "该框架能够基于人类演示，自主生成逼真且多样的机器人情感表达，为机器人情感交互提供了有效途径。"}}
{"id": "2508.08612", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08612", "abs": "https://arxiv.org/abs/2508.08612", "authors": ["Jiahua Dong", "Hui Yin", "Wenqi Liang", "Hanbin Zhao", "Henghui Ding", "Nicu Sebe", "Salman Khan", "Fahad Shahbaz Khan"], "title": "Hierarchical Visual Prompt Learning for Continual Video Instance Segmentation", "comment": "Accepted to ICCV2025", "summary": "Video instance segmentation (VIS) has gained significant attention for its\ncapability in tracking and segmenting object instances across video frames.\nHowever, most of the existing VIS approaches unrealistically assume that the\ncategories of object instances remain fixed over time. Moreover, they\nexperience catastrophic forgetting of old classes when required to continuously\nlearn object instances belonging to new categories. To resolve these\nchallenges, we develop a novel Hierarchical Visual Prompt Learning (HVPL) model\nthat overcomes catastrophic forgetting of previous categories from both\nframe-level and video-level perspectives. Specifically, to mitigate forgetting\nat the frame level, we devise a task-specific frame prompt and an orthogonal\ngradient correction (OGC) module. The OGC module helps the frame prompt encode\ntask-specific global instance information for new classes in each individual\nframe by projecting its gradients onto the orthogonal feature space of old\nclasses. Furthermore, to address forgetting at the video level, we design a\ntask-specific video prompt and a video context decoder. This decoder first\nembeds structural inter-class relationships across frames into the frame prompt\nfeatures, and then propagates task-specific global video contexts from the\nframe prompt features to the video prompt. Through rigorous comparisons, our\nHVPL model proves to be more effective than baseline approaches. The code is\navailable at https://github.com/JiahuaDong/HVPL.", "AI": {"tldr": "该论文提出了一种名为分层视觉提示学习（HVPL）的新模型，用于解决视频实例分割（VIS）中持续学习新类别时出现的灾难性遗忘问题，从帧级别和视频级别两个层面进行缓解。", "motivation": "现有的视频实例分割（VIS）方法普遍假设对象类别固定不变，并且在持续学习新类别时，会遭受旧类别知识的灾难性遗忘。", "method": "开发了HVPL模型。在帧级别，设计了任务特定的帧提示和正交梯度校正（OGC）模块，OGC通过将新类别的梯度投影到旧类别的正交特征空间来编码任务特定全局实例信息。在视频级别，设计了任务特定的视频提示和视频上下文解码器，该解码器将跨帧的类间结构关系嵌入到帧提示特征中，并将任务特定的全局视频上下文从帧提示特征传播到视频提示。", "result": "通过严格比较，HVPL模型被证明比基线方法更有效。", "conclusion": "HVPL模型成功克服了视频实例分割中持续学习的灾难性遗忘问题，为处理不断演变的视频数据提供了有效方案。"}}
{"id": "2508.08665", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08665", "abs": "https://arxiv.org/abs/2508.08665", "authors": ["Ritvik Rastogi", "Sachin Dharashivkar", "Sandeep Varma"], "title": "Aryabhata: An exam-focused language model for JEE Math", "comment": null, "summary": "We present Aryabhata 1.0, a compact 7B parameter math reasoning model\noptimized for the Indian academic exam, the Joint Entrance Examination (JEE).\nDespite rapid progress in large language models (LLMs), current models often\nremain unsuitable for educational use. Aryabhata 1.0 is built by merging strong\nopen-weight reasoning models, followed by supervised fine-tuning (SFT) with\ncurriculum learning on verified chain-of-thought (CoT) traces curated through\nbest-of-$n$ rejection sampling. To further boost performance, we apply\nreinforcement learning with verifiable rewards (RLVR) using A2C objective with\ngroup-relative advantage estimation along with novel exploration strategies\nsuch as Adaptive Group Resizing and Temperature Scaling. Evaluated on both\nin-distribution (JEE Main 2025) and out-of-distribution (MATH, GSM8K)\nbenchmarks, Aryabhata outperforms existing models in accuracy and efficiency,\nwhile offering pedagogically useful step-by-step reasoning. We release\nAryabhata as a foundation model to advance exam-centric, open-source small\nlanguage models. This marks our first open release for community feedback\n(https://huggingface.co/PhysicsWallahAI/Aryabhata-1.0); PW is actively training\nfuture models to further improve learning outcomes for students.", "AI": {"tldr": "Aryabhata 1.0是一个7B参数的数学推理模型，针对印度JEE考试优化，通过合并模型、课程学习SFT和RLVR训练，在准确性和效率上超越现有模型，并提供教学有用的分步推理。", "motivation": "尽管大型语言模型（LLMs）发展迅速，但现有模型通常不适合教育用途，特别是在应对印度JEE等特定学术考试时。因此，需要一个专门为教育场景优化、高效且能提供教学辅助的数学推理模型。", "method": "该模型通过合并强大的开源推理模型构建，然后使用课程学习进行监督微调（SFT），微调数据是经过最佳n拒绝采样验证的思维链（CoT）轨迹。为进一步提升性能，模型还应用了可验证奖励强化学习（RLVR），使用A2C目标函数、组相对优势估计，以及自适应组大小调整和温度缩放等新颖探索策略。", "result": "Aryabhata 1.0在分布内（JEE Main 2025）和分布外（MATH、GSM8K）基准测试中，在准确性和效率上均优于现有模型，同时提供了具有教学价值的逐步推理过程。", "conclusion": "Aryabhata 1.0作为一款基础模型发布，旨在推动以考试为中心的开源小型语言模型的发展，并期望通过未来的模型训练进一步改善学生的学习成果。"}}
{"id": "2508.08651", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.08651", "abs": "https://arxiv.org/abs/2508.08651", "authors": ["Jakub Šmíd", "Pavel Přibáň"], "title": "Prompt-Based Approach for Czech Sentiment Analysis", "comment": "Published in Proceedings of the 14th International Conference on\n  Recent Advances in Natural Language Processing (RANLP 2023). Official\n  version: https://aclanthology.org/2023.ranlp-1.118/", "summary": "This paper introduces the first prompt-based methods for aspect-based\nsentiment analysis and sentiment classification in Czech. We employ the\nsequence-to-sequence models to solve the aspect-based tasks simultaneously and\ndemonstrate the superiority of our prompt-based approach over traditional\nfine-tuning. In addition, we conduct zero-shot and few-shot learning\nexperiments for sentiment classification and show that prompting yields\nsignificantly better results with limited training examples compared to\ntraditional fine-tuning. We also demonstrate that pre-training on data from the\ntarget domain can lead to significant improvements in a zero-shot scenario.", "AI": {"tldr": "该论文首次提出了基于提示的方法，用于捷克语的方面级情感分析和情感分类，在低资源设置下表现优于传统微调。", "motivation": "研究动机是为捷克语的方面级情感分析和情感分类引入基于提示的方法，并证明其相对于传统微调的优越性，特别是在零样本和少样本学习场景中。", "method": "采用序列到序列模型，同时解决方面级任务。使用基于提示的方法，并与传统微调进行比较。进行了零样本和少样本学习实验。探索了在目标领域数据上进行预训练的效果。", "result": "基于提示的方法在方面级任务上优于传统微调。在训练样本有限的情况下（零样本和少样本），提示方法比传统微调取得了显著更好的结果。在目标领域数据上进行预训练可以在零样本场景中带来显著改进。", "conclusion": "基于提示的方法是解决捷克语方面级情感分析和情感分类的有效途径，尤其在数据稀缺的场景下表现出色，且通过目标领域预训练可进一步提升性能。"}}
{"id": "2508.09071", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.09071", "abs": "https://arxiv.org/abs/2508.09071", "authors": ["Lin Sun", "Bin Xie", "Yingfei Liu", "Hao Shi", "Tiancai Wang", "Jiale Cao"], "title": "GeoVLA: Empowering 3D Representations in Vision-Language-Action Models", "comment": "The project is visible at https://linsun449.github.io/GeoVLA/", "summary": "Vision-Language-Action (VLA) models have emerged as a promising approach for\nenabling robots to follow language instructions and predict corresponding\nactions.However, current VLA models mainly rely on 2D visual inputs, neglecting\nthe rich geometric information in the 3D physical world, which limits their\nspatial awareness and adaptability. In this paper, we present GeoVLA, a novel\nVLA framework that effectively integrates 3D information to advance robotic\nmanipulation. It uses a vision-language model (VLM) to process images and\nlanguage instructions,extracting fused vision-language embeddings. In parallel,\nit converts depth maps into point clouds and employs a customized point\nencoder, called Point Embedding Network, to generate 3D geometric embeddings\nindependently. These produced embeddings are then concatenated and processed by\nour proposed spatial-aware action expert, called 3D-enhanced Action Expert,\nwhich combines information from different sensor modalities to produce precise\naction sequences. Through extensive experiments in both simulation and\nreal-world environments, GeoVLA demonstrates superior performance and\nrobustness. It achieves state-of-the-art results in the LIBERO and ManiSkill2\nsimulation benchmarks and shows remarkable robustness in real-world tasks\nrequiring height adaptability, scale awareness and viewpoint invariance.", "AI": {"tldr": "GeoVLA是一个新型视觉-语言-动作（VLA）框架，通过有效整合3D几何信息来增强机器人操作，解决了现有VLA模型对2D视觉依赖的局限性。", "motivation": "当前的视觉-语言-动作（VLA）模型主要依赖2D视觉输入，忽略了3D物理世界中丰富的几何信息，这限制了它们的空间感知能力和适应性，从而影响了机器人遵循语言指令和预测相应动作的性能。", "method": "GeoVLA采用并行处理机制：首先，使用一个视觉-语言模型（VLM）处理图像和语言指令，提取融合的视觉-语言嵌入。同时，将深度图转换为点云，并使用定制的点编码器（Point Embedding Network）独立生成3D几何嵌入。最后，将这些嵌入连接起来，并通过提出的空间感知动作专家（3D-enhanced Action Expert）处理，该专家结合来自不同传感器模态的信息，生成精确的动作序列。", "result": "GeoVLA在仿真和真实世界环境中都表现出卓越的性能和鲁棒性。它在LIBERO和ManiSkill2仿真基准测试中取得了最先进的结果，并在需要高度适应性、尺度感知和视点不变性的真实世界任务中展现出显著的鲁棒性。", "conclusion": "GeoVLA通过有效整合3D信息，显著提升了机器人操作能力，克服了传统VLA模型对2D视觉的依赖，证明了其在复杂机器人任务中的优越性和鲁棒性。"}}
{"id": "2508.08644", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08644", "abs": "https://arxiv.org/abs/2508.08644", "authors": ["Guiming Cao", "Yuming Ou"], "title": "AME: Aligned Manifold Entropy for Robust Vision-Language Distillation", "comment": null, "summary": "Knowledge distillation is a long-established technique for knowledge\ntransfer, and has regained attention in the context of the recent emergence of\nlarge vision-language models (VLMs). However, vision-language knowledge\ndistillation often requires sufficient training data to achieve robust\ngeneralization on amples with ambiguous or boundary-adjacent representations,\nwhich are associated with high predictive uncertainty. Critically, collecting\nsuch large-scale, task-specific data for training is often impractical in\nreal-world scenarios. To address this major challenge arising from the\nentanglement of uncertainty and cross-modal feature representation, we propose\nAligned Manifold Entropy for Robust Vision-Language Distillation (AME), aiming\nto achieve robust generalization under real-world conditions. AME applies\nentropy minimization over a reconfigured shared manifold, where multi-modal\ndata (i.e., image and text) are bridged through a pair of projection functions,\nconducive to structural compression for cross-modal feature representations.\nThis enables robust knowledge distillation under low-data regimes, while\nrequiring no architectural modifications to the backbone. As a result, it can\nserve as a plug-and-play module compatible with a wide range of vision-language\ndistillation frameworks. Notably, our theoretical analysis reveals that\nintegrating knowledge distillation with entropy minimization over the shared\nmanifold leads to a tighter generalization error bound. Extensive experiments\nacross diverse distillation architectures and training settings demonstrate\nthat AME consistently facilitates robust knowledge distillation, resulting in\nsuperior generalization performance across a wide spectrum of downstream tasks.", "AI": {"tldr": "针对视觉-语言模型（VLMs）在低数据量和高不确定性样本下知识蒸馏泛化能力差的问题，本文提出了AME（Aligned Manifold Entropy）方法，通过在重构的共享流形上进行熵最小化，实现鲁棒的跨模态知识蒸馏，且无需修改骨干网络。", "motivation": "视觉-语言知识蒸馏（VLKD）通常需要大量训练数据才能在具有高预测不确定性的模糊或边界样本上实现鲁棒泛化。然而，在实际场景中收集大规模、任务特定的训练数据往往不切实际，这是不确定性和跨模态特征表示纠缠所带来的主要挑战。", "method": "本文提出对齐流形熵（AME）方法，通过一对投影函数将多模态数据（图像和文本）连接起来，在重构的共享流形上应用熵最小化，以实现跨模态特征表示的结构压缩。这使得在低数据量条件下也能进行鲁棒的知识蒸馏，且无需对骨干网络进行架构修改，可作为即插即用模块。", "result": "理论分析表明，将知识蒸馏与共享流形上的熵最小化相结合，可以获得更紧密的泛化误差界。在多种蒸馏架构和训练设置上的大量实验表明，AME持续促进鲁棒的知识蒸态，在广泛的下游任务中实现了卓越的泛化性能。", "conclusion": "AME提供了一种在低数据量条件下实现鲁棒视觉-语言知识蒸馏的有效方法，通过对齐流形熵最小化，显著提高了模型在各种下游任务上的泛化性能，并能作为广泛视觉-语言蒸馏框架的即插即用模块。"}}
{"id": "2508.08688", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08688", "abs": "https://arxiv.org/abs/2508.08688", "authors": ["Chen Li", "Han Zhang", "Zhantao Yang", "Fangyi Chen", "Zihan Wang", "Anudeepsekhar Bolimera", "Marios Savvides"], "title": "STELAR-VISION: Self-Topology-Aware Efficient Learning for Aligned Reasoning in Vision", "comment": null, "summary": "Vision-language models (VLMs) have made significant strides in reasoning, yet\nthey often struggle with complex multimodal tasks and tend to generate overly\nverbose outputs. A key limitation is their reliance on chain-of-thought (CoT)\nreasoning, despite many tasks benefiting from alternative topologies like trees\nor graphs. To address this, we introduce STELAR-Vision, a training framework\nfor topology-aware reasoning. At its core is TopoAug, a synthetic data pipeline\nthat enriches training with diverse topological structures. Using supervised\nfine-tuning and reinforcement learning, we post-train Qwen2VL models with both\naccuracy and efficiency in mind. Additionally, we propose Frugal Learning,\nwhich reduces output length with minimal accuracy loss. On MATH-V and VLM-S2H,\nSTELAR-Vision improves accuracy by 9.7% over its base model and surpasses the\nlarger Qwen2VL-72B-Instruct by 7.3%. On five out-of-distribution benchmarks, it\noutperforms Phi-4-Multimodal-Instruct by up to 28.4% and\nLLaMA-3.2-11B-Vision-Instruct by up to 13.2%, demonstrating strong\ngeneralization. Compared to Chain-Only training, our approach achieves 4.3%\nhigher overall accuracy on in-distribution datasets and consistently\noutperforms across all OOD benchmarks. We have released datasets, and code will\nbe available.", "AI": {"tldr": "STELAR-Vision是一个拓扑感知推理的视觉语言模型训练框架，通过TopoAug生成多样拓扑结构数据，并结合监督微调和强化学习，解决了现有VLM在复杂任务中推理能力不足和输出冗长的问题，显著提升了准确性和泛化能力，同时通过Frugal Learning减少了输出长度。", "motivation": "现有的视觉语言模型（VLMs）在复杂多模态任务中表现不佳，且倾向于生成冗长的输出。其主要限制在于过度依赖链式思维（CoT）推理，而许多任务实际上更适合树状或图状等其他拓扑结构。", "method": "引入STELAR-Vision训练框架，核心是TopoAug合成数据管道，用于生成具有多样拓扑结构（如树、图）的训练数据。通过监督微调（supervised fine-tuning）和强化学习（reinforcement learning）对Qwen2VL模型进行后训练，以兼顾准确性和效率。此外，提出了Frugal Learning方法，旨在在最小化准确性损失的前提下缩短模型输出长度。", "result": "在MATH-V和VLM-S2H数据集上，STELAR-Vision相比其基础模型准确率提高了9.7%，并超越了更大的Qwen2VL-72B-Instruct模型7.3%。在五个域外（out-of-distribution）基准测试中，其表现优于Phi-4-Multimodal-Instruct高达28.4%，优于LLaMA-3.2-11B-Vision-Instruct高达13.2%，显示出强大的泛化能力。与仅使用链式推理的训练方法相比，STELAR-Vision在域内数据集上整体准确率高出4.3%，并在所有域外基准测试中持续表现更优。", "conclusion": "STELAR-Vision框架通过引入拓扑感知推理和多样化的数据增强，显著提升了视觉语言模型在复杂多模态任务中的推理准确性和泛化能力，同时有效地减少了模型输出的冗余性，证明了其在解决现有VLM局限性方面的有效性。"}}
{"id": "2508.08653", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08653", "abs": "https://arxiv.org/abs/2508.08653", "authors": ["Rajmohan C", "Sarthak Harne", "Arvind Agarwal"], "title": "LLM driven Text-to-Table Generation through Sub-Tasks Guidance and Iterative Refinement", "comment": null, "summary": "Transforming unstructured text into structured data is a complex task,\nrequiring semantic understanding, reasoning, and structural comprehension.\nWhile Large Language Models (LLMs) offer potential, they often struggle with\nhandling ambiguous or domain-specific data, maintaining table structure,\nmanaging long inputs, and addressing numerical reasoning. This paper proposes\nan efficient system for LLM-driven text-to-table generation that leverages\nnovel prompting techniques. Specifically, the system incorporates two key\nstrategies: breaking down the text-to-table task into manageable, guided\nsub-tasks and refining the generated tables through iterative self-feedback. We\nshow that this custom task decomposition allows the model to address the\nproblem in a stepwise manner and improves the quality of the generated table.\nFurthermore, we discuss the benefits and potential risks associated with\niterative self-feedback on the generated tables while highlighting the\ntrade-offs between enhanced performance and computational cost. Our methods\nachieve strong results compared to baselines on two complex text-to-table\ngeneration datasets available in the public domain.", "AI": {"tldr": "本文提出了一种高效的LLM驱动文本到表格生成系统，通过任务分解和迭代自反馈来提高生成质量，解决了LLM在该任务中的挑战。", "motivation": "将非结构化文本转换为结构化数据（表格）是一项复杂任务，需要语义理解、推理和结构理解。尽管大型语言模型（LLMs）有潜力，但它们在处理歧义、领域特定数据、维护表格结构、管理长输入以及数值推理方面存在困难。", "method": "该系统采用了两种核心策略：1) 将文本到表格任务分解为可管理、有指导的子任务；2) 通过迭代自反馈来优化生成的表格。", "result": "与现有基线相比，该方法在两个复杂的公开文本到表格生成数据集上取得了显著的成果。", "conclusion": "定制的任务分解允许模型逐步解决问题并提高生成表格的质量。迭代自反馈虽然能增强性能，但也存在计算成本的权衡。该方法有效提升了LLM在文本到表格任务上的表现。"}}
{"id": "2508.08923", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.08923", "abs": "https://arxiv.org/abs/2508.08923", "authors": ["Miruna-Alexandra Gafencu", "Reem Shaban", "Yordanka Velikova", "Mohammad Farid Azampour", "Nassir Navab"], "title": "Shape Completion and Real-Time Visualization in Robotic Ultrasound Spine Acquisitions", "comment": null, "summary": "Ultrasound (US) imaging is increasingly used in spinal procedures due to its\nreal-time, radiation-free capabilities; however, its effectiveness is hindered\nby shadowing artifacts that obscure deeper tissue structures. Traditional\napproaches, such as CT-to-US registration, incorporate anatomical information\nfrom preoperative CT scans to guide interventions, but they are limited by\ncomplex registration requirements, differences in spine curvature, and the need\nfor recent CT imaging. Recent shape completion methods can offer an alternative\nby reconstructing spinal structures in US data, while being pretrained on large\nset of publicly available CT scans. However, these approaches are typically\noffline and have limited reproducibility. In this work, we introduce a novel\nintegrated system that combines robotic ultrasound with real-time shape\ncompletion to enhance spinal visualization. Our robotic platform autonomously\nacquires US sweeps of the lumbar spine, extracts vertebral surfaces from\nultrasound, and reconstructs the complete anatomy using a deep learning-based\nshape completion network. This framework provides interactive, real-time\nvisualization with the capability to autonomously repeat scans and can enable\nnavigation to target locations. This can contribute to better consistency,\nreproducibility, and understanding of the underlying anatomy. We validate our\napproach through quantitative experiments assessing shape completion accuracy\nand evaluations of multiple spine acquisition protocols on a phantom setup.\nAdditionally, we present qualitative results of the visualization on a\nvolunteer scan.", "AI": {"tldr": "该研究提出一个结合机器人超声和实时形状补全的新系统，以克服超声图像中脊柱结构被遮挡的问题，提供实时、交互式的完整脊柱解剖结构可视化。", "motivation": "超声成像在脊柱手术中因实时、无辐射而日益普及，但其有效性受阴影伪影阻碍，导致深层组织结构不清晰。传统CT-US配准方法存在复杂配准、脊柱曲度差异和需近期CT扫描的局限。现有形状补全方法多为离线且可复现性差。", "method": "开发了一个集成系统，结合机器人超声和实时形状补全。该系统通过机器人平台自主获取腰椎超声扫描，从超声数据中提取椎体表面，并利用基于深度学习的形状补全网络重建完整的解剖结构。", "result": "该框架提供了交互式、实时的可视化能力，并能自主重复扫描，有望实现目标位置导航。通过定量实验评估了形状补全的准确性，并在模型上评估了多种脊柱采集协议。此外，还展示了志愿者扫描的定性可视化结果。", "conclusion": "该系统能显著提高脊柱超声成像的一致性、可复现性和对解剖结构的理解，并可能辅助导航，从而提升脊柱手术的精度和安全性。"}}
{"id": "2508.08660", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08660", "abs": "https://arxiv.org/abs/2508.08660", "authors": ["Xin Wang", "Yin Guo", "Jiamin Xia", "Kaiyu Zhang", "Niranjan Balu", "Mahmud Mossa-Basha", "Linda Shapiro", "Chun Yuan"], "title": "Unified and Semantically Grounded Domain Adaptation for Medical Image Segmentation", "comment": null, "summary": "Most prior unsupervised domain adaptation approaches for medical image\nsegmentation are narrowly tailored to either the source-accessible setting,\nwhere adaptation is guided by source-target alignment, or the source-free\nsetting, which typically resorts to implicit supervision mechanisms such as\npseudo-labeling and model distillation. This substantial divergence in\nmethodological designs between the two settings reveals an inherent flaw: the\nlack of an explicit, structured construction of anatomical knowledge that\nnaturally generalizes across domains and settings. To bridge this longstanding\ndivide, we introduce a unified, semantically grounded framework that supports\nboth source-accessible and source-free adaptation. Fundamentally distinct from\nall prior works, our framework's adaptability emerges naturally as a direct\nconsequence of the model architecture, without the need for any handcrafted\nadaptation strategies. Specifically, our model learns a domain-agnostic\nprobabilistic manifold as a global space of anatomical regularities, mirroring\nhow humans establish visual understanding. Thus, the structural content in each\nimage can be interpreted as a canonical anatomy retrieved from the manifold and\na spatial transformation capturing individual-specific geometry. This\ndisentangled, interpretable formulation enables semantically meaningful\nprediction with intrinsic adaptability. Extensive experiments on challenging\ncardiac and abdominal datasets show that our framework achieves\nstate-of-the-art results in both settings, with source-free performance closely\napproaching its source-accessible counterpart, a level of consistency rarely\nobserved in prior works. Beyond quantitative improvement, we demonstrate strong\ninterpretability of the proposed framework via manifold traversal for smooth\nshape manipulation.", "AI": {"tldr": "提出一个统一的、语义驱动的框架，通过学习领域无关的概率流形来解决医学图像分割中无监督域适应（UDA）在有源和无源设置间的鸿沟，实现了最先进的性能和可解释性。", "motivation": "当前医学图像分割中的无监督域适应方法在有源和无源设置之间存在显著差异，且缺乏一种能自然泛化跨领域和设置的显式、结构化解剖知识构建方法。", "method": "引入一个统一的、语义驱动的框架，其适应性源于模型架构本身，无需手动设计适应策略。该模型学习一个领域无关的概率流形作为解剖规律的全局空间，将图像结构内容解释为从流形中检索出的典型解剖结构和捕获个体特定几何的空间变换，实现了可解耦、可解释的固有适应性预测。", "result": "在心脏和腹部数据集上实现了两种设置下的最先进性能，其中无源设置的性能非常接近有源设置，表现出前所未有的高一致性。此外，通过流形遍历展示了该框架强大的可解释性。", "conclusion": "该统一框架通过构建领域无关的解剖知识流形，有效弥合了医学图像分割UDA在有源和无源设置之间的性能差距，实现了卓越的性能、一致性和可解释性，为未来的域适应研究提供了新范式。"}}
{"id": "2508.08726", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.08726", "abs": "https://arxiv.org/abs/2508.08726", "authors": ["Yuwei Yan", "Jinghua Piao", "Xiaochong Lan", "Chenyang Shao", "Pan Hui", "Yong Li"], "title": "Simulating Generative Social Agents via Theory-Informed Workflow Design", "comment": null, "summary": "Recent advances in large language models have demonstrated strong reasoning\nand role-playing capabilities, opening new opportunities for agent-based social\nsimulations. However, most existing agents' implementations are\nscenario-tailored, without a unified framework to guide the design. This lack\nof a general social agent limits their ability to generalize across different\nsocial contexts and to produce consistent, realistic behaviors. To address this\nchallenge, we propose a theory-informed framework that provides a systematic\ndesign process for LLM-based social agents. Our framework is grounded in\nprinciples from Social Cognition Theory and introduces three key modules:\nmotivation, action planning, and learning. These modules jointly enable agents\nto reason about their goals, plan coherent actions, and adapt their behavior\nover time, leading to more flexible and contextually appropriate responses.\nComprehensive experiments demonstrate that our theory-driven agents reproduce\nrealistic human behavior patterns under complex conditions, achieving up to 75%\nlower deviation from real-world behavioral data across multiple fidelity\nmetrics compared to classical generative baselines. Ablation studies further\nshow that removing motivation, planning, or learning modules increases errors\nby 1.5 to 3.2 times, confirming their distinct and essential contributions to\ngenerating realistic and coherent social behaviors.", "AI": {"tldr": "提出一个基于社会认知理论的统一框架，包含动机、行动规划和学习模块，用于设计更通用、行为更真实的LLM社会智能体。", "motivation": "现有LLM智能体实现多为特定场景定制，缺乏统一框架，导致泛化能力差、行为不一致且不真实，限制了其在社会模拟中的应用。", "method": "提出了一个受社会认知理论启发的框架，包含三个核心模块：动机、行动规划和学习。这些模块共同使智能体能够推理目标、规划连贯行动并随时间调整行为。", "result": "实验证明，该理论驱动的智能体在复杂条件下能复现真实人类行为模式，与真实行为数据的偏差比传统生成基线低75%。消融实验表明，移除任一模块（动机、规划、学习）会使错误增加1.5到3.2倍，证实了其对生成真实连贯社会行为的独特贡献。", "conclusion": "所提出的理论驱动框架及其包含的动机、行动规划和学习模块，显著提高了LLM社会智能体生成真实、连贯行为的能力，解决了现有智能体泛化性和行为一致性不足的问题。"}}
{"id": "2508.08680", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.08680", "abs": "https://arxiv.org/abs/2508.08680", "authors": ["Armel Zebaze", "Benoît Sagot", "Rachel Bawden"], "title": "TopXGen: Topic-Diverse Parallel Data Generation for Low-Resource Machine Translation", "comment": null, "summary": "LLMs have been shown to perform well in machine translation (MT) with the use\nof in-context learning (ICL), rivaling supervised models when translating into\nhigh-resource languages (HRLs). However, they lag behind when translating into\nlow-resource language (LRLs). Example selection via similarity search and\nsupervised fine-tuning help. However the improvements they give are limited by\nthe size, quality and diversity of existing parallel datasets. A common\ntechnique in low-resource MT is synthetic parallel data creation, the most\nfrequent of which is backtranslation, whereby existing target-side texts are\nautomatically translated into the source language. However, this assumes the\nexistence of good quality and relevant target-side texts, which are not readily\navailable for many LRLs. In this paper, we present \\textsc{TopXGen}, an\nLLM-based approach for the generation of high quality and topic-diverse data in\nmultiple LRLs, which can then be backtranslated to produce useful and diverse\nparallel texts for ICL and fine-tuning. Our intuition is that while LLMs\nstruggle to translate into LRLs, their ability to translate well into HRLs and\ntheir multilinguality enable them to generate good quality, natural-sounding\ntarget-side texts, which can be translated well into a high-resource source\nlanguage. We show that \\textsc{TopXGen} boosts LLM translation performance\nduring fine-tuning and in-context learning. Code and outputs are available at\nhttps://github.com/ArmelRandy/topxgen.", "AI": {"tldr": "本文提出了一种名为TopXGen的LLM方法，用于生成高质量、主题多样化的低资源语言（LRL）文本，这些文本可以被回译以创建有用的并行数据，从而提升LLM在低资源机器翻译（MT）中的表现。", "motivation": "尽管大型语言模型（LLMs）在高资源语言（HRLs）机器翻译中表现出色，但在低资源语言（LRLs）翻译中表现不佳。现有的改进方法（如上下文学习和微调）受限于并行数据集的大小、质量和多样性。传统的反向翻译需要高质量的目标侧文本，而这在许多LRLs中不易获得。", "method": "本文提出了TopXGen，一种基于LLM的方法，用于生成高质量且主题多样的LRL目标侧文本。其核心思想是利用LLM擅长高资源语言翻译和多语言能力来生成听起来自然的LRL文本，然后将这些文本反向翻译成高资源源语言，以创建有用的并行文本，用于上下文学习和微调。", "result": "实验结果表明，TopXGen能够显著提升LLM在微调和上下文学习过程中的翻译性能。", "conclusion": "TopXGen提供了一种有效的方法，通过生成高质量的合成并行数据来解决LLM在低资源语言机器翻译中的挑战，从而提高了其翻译表现。"}}
{"id": "2508.09032", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.09032", "abs": "https://arxiv.org/abs/2508.09032", "authors": ["Maxim A. Patratskiy", "Alexey K. Kovalev", "Aleksandr I. Panov"], "title": "Spatial Traces: Enhancing VLA Models with Spatial-Temporal Understanding", "comment": null, "summary": "Vision-Language-Action models have demonstrated remarkable capabilities in\npredicting agent movements within virtual environments and real-world scenarios\nbased on visual observations and textual instructions. Although recent research\nhas focused on enhancing spatial and temporal understanding independently, this\npaper presents a novel approach that integrates both aspects through visual\nprompting. We introduce a method that projects visual traces of key points from\nobservations onto depth maps, enabling models to capture both spatial and\ntemporal information simultaneously. The experiments in SimplerEnv show that\nthe mean number of tasks successfully solved increased for 4% compared to\nSpatialVLA and 19% compared to TraceVLA. Furthermore, we show that this\nenhancement can be achieved with minimal training data, making it particularly\nvaluable for real-world applications where data collection is challenging. The\nproject page is available at https://ampiromax.github.io/ST-VLA.", "AI": {"tldr": "本文提出了一种通过视觉提示将关键点视觉轨迹投影到深度图上的方法，以同时整合视觉-语言-动作模型中的空间和时间信息，从而提高任务解决能力，尤其是在数据稀缺的真实世界应用中。", "motivation": "现有的视觉-语言-动作（VLA）模型在预测智能体运动方面表现出色，但多侧重于独立增强空间或时间理解。研究动机在于开发一种能同时捕获这两种信息的统一方法，并使其适用于数据收集困难的真实世界应用。", "method": "引入了一种新颖的视觉提示方法，通过将观测中关键点的视觉轨迹投影到深度图上，使模型能够同时捕获空间和时间信息。", "result": "在SimplerEnv环境中的实验表明，与SpatialVLA相比，成功解决的任务平均数量增加了4%；与TraceVLA相比，增加了19%。此外，这种性能提升仅需最少的训练数据即可实现。", "conclusion": "所提出的视觉提示方法有效地整合了VLA模型中的空间和时间信息，显著提高了任务解决能力，且所需训练数据量小，这对于数据收集具有挑战性的真实世界应用尤为有价值。"}}
{"id": "2508.08667", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.08667", "abs": "https://arxiv.org/abs/2508.08667", "authors": ["Ke Liu", "Xuanhan Wang", "Qilong Zhang", "Lianli Gao", "Jingkuan Song"], "title": "Learning Generalizable and Efficient Image Watermarking via Hierarchical Two-Stage Optimization", "comment": null, "summary": "Deep image watermarking, which refers to enable imperceptible watermark\nembedding and reliable extraction in cover images, has shown to be effective\nfor copyright protection of image assets. However, existing methods face\nlimitations in simultaneously satisfying three essential criteria for\ngeneralizable watermarking: 1) invisibility (imperceptible hide of watermarks),\n2) robustness (reliable watermark recovery under diverse conditions), and 3)\nbroad applicability (low latency in watermarking process). To address these\nlimitations, we propose a Hierarchical Watermark Learning (HiWL), a two-stage\noptimization that enable a watermarking model to simultaneously achieve three\ncriteria. In the first stage, distribution alignment learning is designed to\nestablish a common latent space with two constraints: 1) visual consistency\nbetween watermarked and non-watermarked images, and 2) information invariance\nacross watermark latent representations. In this way, multi-modal inputs\nincluding watermark message (binary codes) and cover images (RGB pixels) can be\nwell represented, ensuring the invisibility of watermarks and robustness in\nwatermarking process thereby. The second stage employs generalized watermark\nrepresentation learning to establish a disentanglement policy for separating\nwatermarks from image content in RGB space. In particular, it strongly\npenalizes substantial fluctuations in separated RGB watermarks corresponding to\nidentical messages. Consequently, HiWL effectively learns generalizable\nlatent-space watermark representations while maintaining broad applicability.\nExtensive experiments demonstrate the effectiveness of proposed method. In\nparticular, it achieves 7.6\\% higher accuracy in watermark extraction than\nexisting methods, while maintaining extremely low latency (100K images\nprocessed in 8s).", "AI": {"tldr": "本文提出了一种名为分层水印学习（HiWL）的两阶段优化方法，旨在同时提升深度图像水印的隐蔽性、鲁棒性和广泛适用性，并取得了显著效果。", "motivation": "现有深度图像水印方法难以同时满足三个关键标准：1) 隐蔽性（水印不可察觉）；2) 鲁棒性（在不同条件下可靠恢复水印）；3) 广泛适用性（水印处理低延迟）。", "method": "提出了一种两阶段优化方法：分层水印学习（HiWL）。第一阶段是分布对齐学习，旨在建立一个公共潜在空间，并施加两个约束：水印图像与非水印图像之间的视觉一致性，以及水印潜在表示之间的信息不变性，以确保隐蔽性和鲁棒性。第二阶段是广义水印表示学习，旨在建立一种分离策略，将水印从RGB空间中的图像内容中解耦出来，并对相同消息对应的分离RGB水印的显著波动进行强惩罚，以保持广泛适用性。", "result": "实验结果表明，所提出的方法有效。特别地，它在水印提取精度上比现有方法高7.6%，同时保持了极低的延迟（10万张图像在8秒内处理完成）。", "conclusion": "HiWL方法通过学习可推广的潜在空间水印表示，有效地解决了现有深度图像水印方法的局限性，同时在隐蔽性、鲁棒性和广泛适用性方面表现出色。"}}
{"id": "2508.08774", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.08774", "abs": "https://arxiv.org/abs/2508.08774", "authors": ["Dongwook Choi", "Taeyoon Kwon", "Dongil Yang", "Hyojun Kim", "Jinyoung Yeo"], "title": "Designing Memory-Augmented AR Agents for Spatiotemporal Reasoning in Personalized Task Assistance", "comment": "7 pages, 2 figures", "summary": "Augmented Reality (AR) systems are increasingly integrating foundation\nmodels, such as Multimodal Large Language Models (MLLMs), to provide more\ncontext-aware and adaptive user experiences. This integration has led to the\ndevelopment of AR agents to support intelligent, goal-directed interactions in\nreal-world environments. While current AR agents effectively support immediate\ntasks, they struggle with complex multi-step scenarios that require\nunderstanding and leveraging user's long-term experiences and preferences. This\nlimitation stems from their inability to capture, retain, and reason over\nhistorical user interactions in spatiotemporal contexts. To address these\nchallenges, we propose a conceptual framework for memory-augmented AR agents\nthat can provide personalized task assistance by learning from and adapting to\nuser-specific experiences over time. Our framework consists of four\ninterconnected modules: (1) Perception Module for multimodal sensor processing,\n(2) Memory Module for persistent spatiotemporal experience storage, (3)\nSpatiotemporal Reasoning Module for synthesizing past and present contexts, and\n(4) Actuator Module for effective AR communication. We further present an\nimplementation roadmap, a future evaluation strategy, a potential target\napplication and use cases to demonstrate the practical applicability of our\nframework across diverse domains. We aim for this work to motivate future\nresearch toward developing more intelligent AR systems that can effectively\nbridge user's interaction history with adaptive, context-aware task assistance.", "AI": {"tldr": "该研究提出了一个记忆增强的增强现实（AR）智能体概念框架，旨在通过捕获、保留和推理用户的时空交互历史，解决当前AR智能体在处理复杂多步骤任务时缺乏长期经验和偏好理解的问题，从而提供个性化的任务辅助。", "motivation": "当前的AR智能体虽然能有效支持即时任务，但在需要理解和利用用户长期经验及偏好的复杂多步骤场景中表现不佳。这源于它们无法捕获、保留和推理时空背景下的历史用户交互。", "method": "本文提出了一个记忆增强的AR智能体概念框架，该框架包含四个互联模块：感知模块（用于多模态传感器处理）、记忆模块（用于持久性时空经验存储）、时空推理模块（用于整合过去和现在的情境）以及执行器模块（用于有效的AR通信）。", "result": "研究提出了一个能够通过学习和适应用户特定经验来提供个性化任务辅助的记忆增强AR智能体概念框架。同时，还提出了一个实施路线图、未来的评估策略、潜在目标应用和用例，以展示该框架在不同领域的实际适用性。", "conclusion": "该工作旨在激励未来的研究，以开发更智能的AR系统，使其能够有效地将用户交互历史与自适应、情境感知的任务辅助相结合，从而弥补现有AR智能体在处理复杂长期任务时的不足。"}}
{"id": "2508.08684", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.08684", "abs": "https://arxiv.org/abs/2508.08684", "authors": ["Bram van Dijk", "Tiberon Kuiper", "Sirin Aoulad si Ahmed", "Armel Levebvre", "Jake Johnson", "Jan Duin", "Simon Mooijaart", "Marco Spruit"], "title": "Out of the Box, into the Clinic? Evaluating State-of-the-Art ASR for Clinical Applications for Older Adults", "comment": null, "summary": "Voice-controlled interfaces can support older adults in clinical contexts,\nwith chatbots being a prime example, but reliable Automatic Speech Recognition\n(ASR) for underrepresented groups remains a bottleneck. This study evaluates\nstate-of-the-art ASR models on language use of older Dutch adults, who\ninteracted with the Welzijn.AI chatbot designed for geriatric contexts. We\nbenchmark generic multilingual ASR models, and models fine-tuned for Dutch\nspoken by older adults, while also considering processing speed. Our results\nshow that generic multilingual models outperform fine-tuned models, which\nsuggests recent ASR models can generalise well out of the box to realistic\ndatasets. Furthermore, our results suggest that truncating existing\narchitectures is helpful in balancing the accuracy-speed trade-off, though we\nalso identify some cases with high WER due to hallucinations.", "AI": {"tldr": "本研究评估了用于老年荷兰语用户的语音识别（ASR）模型，发现通用多语言模型表现优于微调模型，且截断现有架构有助于平衡准确性和速度。", "motivation": "语音控制界面（如聊天机器人）对老年人在临床环境中很有帮助，但针对弱势群体（如老年人）的可靠自动语音识别（ASR）仍是瓶颈。", "method": "研究评估了最先进的ASR模型，使用老年荷兰成年人与为老年病学设计的Welzijn.AI聊天机器人交互时产生的语言数据。对比了通用多语言ASR模型和针对老年人荷兰语进行微调的模型，并考虑了处理速度。", "result": "通用多语言模型表现优于微调模型，表明最新的ASR模型能很好地泛化到真实数据集。此外，截断现有架构有助于平衡准确性和速度，但也发现了一些由于幻觉导致高词错误率（WER）的情况。", "conclusion": "当前通用ASR模型在处理老年人语音方面表现出色，可能无需大量特定微调。在实际应用中，通过截断架构可以有效平衡性能与速度，但需警惕并解决模型幻觉问题。"}}
{"id": "2508.08679", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08679", "abs": "https://arxiv.org/abs/2508.08679", "authors": ["Tao Luo", "Weihua Xu"], "title": "MMIF-AMIN: Adaptive Loss-Driven Multi-Scale Invertible Dense Network for Multimodal Medical Image Fusion", "comment": "10 pages, 6 figures,conference", "summary": "Multimodal medical image fusion (MMIF) aims to integrate images from\ndifferent modalities to produce a comprehensive image that enhances medical\ndiagnosis by accurately depicting organ structures, tissue textures, and\nmetabolic information. Capturing both the unique and complementary information\nacross multiple modalities simultaneously is a key research challenge in MMIF.\nTo address this challenge, this paper proposes a novel image fusion method,\nMMIF-AMIN, which features a new architecture that can effectively extract these\nunique and complementary features. Specifically, an Invertible Dense Network\n(IDN) is employed for lossless feature extraction from individual modalities.\nTo extract complementary information between modalities, a Multi-scale\nComplementary Feature Extraction Module (MCFEM) is designed, which incorporates\na hybrid attention mechanism, convolutional layers of varying sizes, and\nTransformers. An adaptive loss function is introduced to guide model learning,\naddressing the limitations of traditional manually-designed loss functions and\nenhancing the depth of data mining. Extensive experiments demonstrate that\nMMIF-AMIN outperforms nine state-of-the-art MMIF methods, delivering superior\nresults in both quantitative and qualitative analyses. Ablation experiments\nconfirm the effectiveness of each component of the proposed method.\nAdditionally, extending MMIF-AMIN to other image fusion tasks also achieves\npromising performance.", "AI": {"tldr": "本文提出了一种名为MMIF-AMIN的新型多模态医学图像融合方法，通过独特且互补的特征提取以及自适应损失函数，显著提升了融合性能。", "motivation": "多模态医学图像融合（MMIF）旨在整合不同模态图像以增强诊断，但同时捕获各模态的独有和互补信息是关键挑战。", "method": "MMIF-AMIN方法包含：1) 可逆密集网络（IDN）用于无损地提取单模态特征；2) 多尺度互补特征提取模块（MCFEM），结合混合注意力机制、不同尺寸卷积层和Transformer，用于提取模态间互补信息；3) 引入自适应损失函数以指导模型学习和深度数据挖掘。", "result": "MMIF-AMIN在定量和定性分析中均优于九种最先进的MMIF方法。消融实验证实了各组件的有效性。此外，该方法在其他图像融合任务中也表现出良好性能。", "conclusion": "MMIF-AMIN是一种有效且性能卓越的多模态医学图像融合方法，能够解决关键挑战，并具有扩展到其他图像融合任务的潜力。"}}
{"id": "2508.08795", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.08795", "abs": "https://arxiv.org/abs/2508.08795", "authors": ["Amir Mohammad Salehoof", "Ali Ramezani", "Yadollah Yaghoobzadeh", "Majid Nili Ahmadabadi"], "title": "A Dual-Axis Taxonomy of Knowledge Editing for LLMs: From Mechanisms to Functions", "comment": "13 pages, 1 figure", "summary": "Large language models (LLMs) acquire vast knowledge from large text corpora,\nbut this information can become outdated or inaccurate. Since retraining is\ncomputationally expensive, knowledge editing offers an efficient alternative --\nmodifying internal knowledge without full retraining. These methods aim to\nupdate facts precisely while preserving the model's overall capabilities. While\nexisting surveys focus on the mechanism of editing (e.g., parameter changes vs.\nexternal memory), they often overlook the function of the knowledge being\nedited. This survey introduces a novel, complementary function-based taxonomy\nto provide a more holistic view. We examine how different mechanisms apply to\nvarious knowledge types -- factual, temporal, conceptual, commonsense, and\nsocial -- highlighting how editing effectiveness depends on the nature of the\ntarget knowledge. By organizing our review along these two axes, we map the\ncurrent landscape, outline the strengths and limitations of existing methods,\ndefine the problem formally, survey evaluation tasks and datasets, and conclude\nwith open challenges and future directions.", "AI": {"tldr": "这篇综述提出了一种基于知识功能的新型分类法，用于分析大型语言模型知识编辑方法，并探讨了不同编辑机制如何应用于不同类型的知识。", "motivation": "大型语言模型（LLMs）的知识会过时或不准确，而重新训练成本高昂。知识编辑提供了一种高效的替代方案。现有综述侧重于编辑机制，但忽视了被编辑知识的功能类型，导致视角不够全面。", "method": "本研究引入了一种新颖的、互补的基于功能的分类法，将知识分为事实性、时间性、概念性、常识性和社会性。在此基础上，结合现有的编辑机制（如参数修改、外部记忆），双轴地审查了知识编辑领域，并 formalizes 问题、评估任务和数据集。", "result": "通过这种双轴分类，本综述描绘了当前知识编辑领域的全貌，阐述了现有方法的优缺点，并总结了评估任务和数据集。", "conclusion": "本综述指出了知识编辑领域的开放挑战和未来研究方向，强调了编辑效果与目标知识性质的相关性。"}}
{"id": "2508.08712", "categories": ["cs.CL", "cs.AI", "cs.DC", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.08712", "abs": "https://arxiv.org/abs/2508.08712", "authors": ["Lingzhe Zhang", "Liancheng Fang", "Chiming Duan", "Minghua He", "Leyi Pan", "Pei Xiao", "Shiyu Huang", "Yunpeng Zhai", "Xuming Hu", "Philip S. Yu", "Aiwei Liu"], "title": "A Survey on Parallel Text Generation: From Parallel Decoding to Diffusion Language Models", "comment": null, "summary": "As text generation has become a core capability of modern Large Language\nModels (LLMs), it underpins a wide range of downstream applications. However,\nmost existing LLMs rely on autoregressive (AR) generation, producing one token\nat a time based on previously generated context-resulting in limited generation\nspeed due to the inherently sequential nature of the process. To address this\nchallenge, an increasing number of researchers have begun exploring parallel\ntext generation-a broad class of techniques aimed at breaking the\ntoken-by-token generation bottleneck and improving inference efficiency.\nDespite growing interest, there remains a lack of comprehensive analysis on\nwhat specific techniques constitute parallel text generation and how they\nimprove inference performance. To bridge this gap, we present a systematic\nsurvey of parallel text generation methods. We categorize existing approaches\ninto AR-based and Non-AR-based paradigms, and provide a detailed examination of\nthe core techniques within each category. Following this taxonomy, we assess\ntheir theoretical trade-offs in terms of speed, quality, and efficiency, and\nexamine their potential for combination and comparison with alternative\nacceleration strategies. Finally, based on our findings, we highlight recent\nadvancements, identify open challenges, and outline promising directions for\nfuture research in parallel text generation.", "AI": {"tldr": "该论文系统性地综述了并行文本生成方法，旨在解决大型语言模型（LLMs）自回归生成速度慢的问题，并对现有技术进行分类、分析和展望。", "motivation": "现代LLMs的核心能力是文本生成，但其主要依赖的自回归（AR）生成方式是顺序的，导致生成速度受限。研究人员正探索并行文本生成以提高推理效率，但目前缺乏对这些技术的全面分析。", "method": "本文对并行文本生成方法进行了系统性综述。将现有方法分为基于AR和非基于AR两种范式，详细考察了每种类别中的核心技术。在此分类基础上，评估了它们在速度、质量和效率方面的理论权衡，并探讨了它们与其他加速策略结合和比较的潜力。", "result": "提出了一个系统的并行文本生成方法分类体系，详细分析了基于AR和非基于AR范畴下的核心技术，并评估了它们在速度、质量和效率方面的理论权衡。此外，还探讨了这些技术与其他加速策略结合的可能性。", "conclusion": "基于研究发现，论文强调了并行文本生成的最新进展，指出了存在的开放挑战，并为未来的研究方向提供了有前景的展望。"}}
{"id": "2508.08685", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08685", "abs": "https://arxiv.org/abs/2508.08685", "authors": ["Yimeng Geng", "Mingyang Zhao", "Fan Xu", "Guanglin Cao", "Gaofeng Meng", "Hongbin Liu"], "title": "PADReg: Physics-Aware Deformable Registration Guided by Contact Force for Ultrasound Sequences", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Ultrasound deformable registration estimates spatial transformations between\npairs of deformed ultrasound images, which is crucial for capturing\nbiomechanical properties and enhancing diagnostic accuracy in diseases such as\nthyroid nodules and breast cancer. However, ultrasound deformable registration\nremains highly challenging, especially under large deformation. The inherently\nlow contrast, heavy noise and ambiguous tissue boundaries in ultrasound images\nseverely hinder reliable feature extraction and correspondence matching.\nExisting methods often suffer from poor anatomical alignment and lack physical\ninterpretability. To address the problem, we propose PADReg, a physics-aware\ndeformable registration framework guided by contact force. PADReg leverages\nsynchronized contact force measured by robotic ultrasound systems as a physical\nprior to constrain the registration. Specifically, instead of directly\npredicting deformation fields, we first construct a pixel-wise stiffness map\nutilizing the multi-modal information from contact force and ultrasound images.\nThe stiffness map is then combined with force data to estimate a dense\ndeformation field, through a lightweight physics-aware module inspired by\nHooke's law. This design enables PADReg to achieve physically plausible\nregistration with better anatomical alignment than previous methods relying\nsolely on image similarity. Experiments on in-vivo datasets demonstrate that it\nattains a HD95 of 12.90, which is 21.34\\% better than state-of-the-art methods.\nThe source code is available at https://github.com/evelynskip/PADReg.", "AI": {"tldr": "超声可变形配准面临大变形、低对比度等挑战。PADReg提出一种物理感知框架，利用接触力作为物理先验，结合图像信息构建刚度图，并通过类胡克定律估算形变场，实现更佳的物理合理性和解剖对齐，性能显著优于现有方法。", "motivation": "超声可变形配准在捕获生物力学特性和提高疾病诊断准确性方面至关重要，但面临巨大挑战，尤其在大变形下，图像固有的低对比度、高噪声和模糊边界严重阻碍可靠特征提取。现有方法常导致解剖对齐不佳且缺乏物理可解释性。", "method": "提出PADReg，一个由接触力引导的物理感知可变形配准框架。该方法利用机器人超声系统测量的同步接触力作为物理先验来约束配准。具体地，不直接预测形变场，而是首先利用接触力和超声图像的多模态信息构建像素级刚度图。随后，将刚度图与力数据结合，通过一个受胡克定律启发的轻量级物理感知模块，估算密集的形变场。", "result": "在体内数据集上的实验表明，PADReg的HD95指标为12.90，比现有最先进方法提高了21.34%。", "conclusion": "PADReg通过引入接触力作为物理先验，实现了物理上更合理且解剖对齐更优的超声可变形配准，克服了以往仅依赖图像相似性方法的局限性。"}}
{"id": "2508.08815", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08815", "abs": "https://arxiv.org/abs/2508.08815", "authors": ["Roberto Barile", "Claudia d'Amato", "Nicola Fanizzi"], "title": "GRainsaCK: a Comprehensive Software Library for Benchmarking Explanations of Link Prediction Tasks on Knowledge Graphs", "comment": null, "summary": "Since Knowledge Graphs are often incomplete, link prediction methods are\nadopted for predicting missing facts. Scalable embedding based solutions are\nmostly adopted for this purpose, however, they lack comprehensibility, which\nmay be crucial in several domains. Explanation methods tackle this issue by\nidentifying supporting knowledge explaining the predicted facts. Regretfully,\nevaluating/comparing quantitatively the resulting explanations is challenging\nas there is no standard evaluation protocol and overall benchmarking resource.\nWe fill this important gap by proposing GRainsaCK, a reusable software resource\nthat fully streamlines all the tasks involved in benchmarking explanations,\ni.e., from model training to evaluation of explanations along the same\nevaluation protocol. Moreover, GRainsaCK furthers modularity/extensibility by\nimplementing the main components as functions that can be easily replaced.\nFinally, fostering its reuse, we provide extensive documentation including a\ntutorial.", "AI": {"tldr": "本文提出了GRainsaCK，一个可复用的软件资源，旨在标准化知识图谱链接预测解释的基准测试和评估过程，解决了缺乏统一评估协议和基准资源的问题。", "motivation": "知识图谱通常不完整，链接预测方法被用来预测缺失事实。尽管可扩展的嵌入式解决方案被广泛采用，但它们缺乏可理解性。解释方法试图通过识别支持知识来解决这个问题，但目前缺乏量化的评估/比较标准和整体基准资源。", "method": "开发了GRainsaCK，一个可复用的软件资源，它能完全简化基准测试解释所需的所有任务，从模型训练到遵循相同评估协议的解释评估。此外，它通过将主要组件实现为可轻松替换的函数，进一步提高了模块化和可扩展性。", "result": "GRainsaCK作为一个软件资源，填补了知识图谱链接预测解释评估方面的空白，提供了一个流线型、模块化、可扩展的解决方案，并附有详尽的文档和教程，以促进其复用。", "conclusion": "GRainsaCK为知识图谱链接预测解释的基准测试和评估提供了一个急需的标准化和可复用平台，有助于推动该领域的研究和比较。"}}
{"id": "2508.08719", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.08719", "abs": "https://arxiv.org/abs/2508.08719", "authors": ["Yuzhuo Bai", "Shitong Duan", "Muhua Huang", "Jing Yao", "Zhenghao Liu", "Peng Zhang", "Tun Lu", "Xiaoyuan Yi", "Maosong Sun", "Xing Xie"], "title": "IROTE: Human-like Traits Elicitation of Large Language Model via In-Context Self-Reflective Optimization", "comment": null, "summary": "Trained on various human-authored corpora, Large Language Models (LLMs) have\ndemonstrated a certain capability of reflecting specific human-like traits\n(e.g., personality or values) by prompting, benefiting applications like\npersonalized LLMs and social simulations. However, existing methods suffer from\nthe superficial elicitation problem: LLMs can only be steered to mimic shallow\nand unstable stylistic patterns, failing to embody the desired traits precisely\nand consistently across diverse tasks like humans. To address this challenge,\nwe propose IROTE, a novel in-context method for stable and transferable trait\nelicitation. Drawing on psychological theories suggesting that traits are\nformed through identity-related reflection, our method automatically generates\nand optimizes a textual self-reflection within prompts, which comprises\nself-perceived experience, to stimulate LLMs' trait-driven behavior. The\noptimization is performed by iteratively maximizing an information-theoretic\nobjective that enhances the connections between LLMs' behavior and the target\ntrait, while reducing noisy redundancy in reflection without any fine-tuning,\nleading to evocative and compact trait reflection. Extensive experiments across\nthree human trait systems manifest that one single IROTE-generated\nself-reflection can induce LLMs' stable impersonation of the target trait\nacross diverse downstream tasks beyond simple questionnaire answering,\nconsistently outperforming existing strong baselines.", "AI": {"tldr": "IROTE是一种新的上下文方法，通过生成和优化LLM的文本自我反思，实现稳定且可迁移的人类特质激发，克服了现有方法的表面化问题。", "motivation": "现有的大型语言模型（LLMs）在激发人类特质（如个性、价值观）时，存在“表面化激发问题”，即LLMs只能模仿浅层和不稳定的风格模式，无法像人类一样在不同任务中精确且一致地体现所需特质。", "method": "本文提出了IROTE方法，一种新颖的上下文（in-context）特质激发方法。该方法借鉴心理学理论，通过在提示中自动生成和优化文本自我反思（包含自我感知的经验）来刺激LLMs的特质驱动行为。优化过程通过迭代最大化一个信息论目标来完成，该目标旨在增强LLMs行为与目标特质之间的联系，同时减少反思中的冗余信息，从而生成简洁而富有启发性的特质反思，且无需微调。", "result": "在三种人类特质系统上的大量实验表明，一个由IROTE生成的自我反思能够使LLMs在多种下游任务中稳定地模仿目标特质，而不仅仅是简单的问卷回答，并且持续优于现有的强基线方法。", "conclusion": "IROTE方法通过其独特的自我反思生成和优化机制，成功解决了LLMs特质激发中的稳定性与可迁移性问题，使其能够更精确、一致地展现人类特质。"}}
{"id": "2508.08697", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08697", "abs": "https://arxiv.org/abs/2508.08697", "authors": ["Tong Sun", "Hongliang Ye", "Jilin Mei", "Liang Chen", "Fangzhou Zhao", "Leiqiang Zong", "Yu Hu"], "title": "ROD: RGB-Only Fast and Efficient Off-road Freespace Detection", "comment": null, "summary": "Off-road freespace detection is more challenging than on-road scenarios\nbecause of the blurred boundaries of traversable areas. Previous\nstate-of-the-art (SOTA) methods employ multi-modal fusion of RGB images and\nLiDAR data. However, due to the significant increase in inference time when\ncalculating surface normal maps from LiDAR data, multi-modal methods are not\nsuitable for real-time applications, particularly in real-world scenarios where\nhigher FPS is required compared to slow navigation. This paper presents a novel\nRGB-only approach for off-road freespace detection, named ROD, eliminating the\nreliance on LiDAR data and its computational demands. Specifically, we utilize\na pre-trained Vision Transformer (ViT) to extract rich features from RGB\nimages. Additionally, we design a lightweight yet efficient decoder, which\ntogether improve both precision and inference speed. ROD establishes a new SOTA\non ORFD and RELLIS-3D datasets, as well as an inference speed of 50 FPS,\nsignificantly outperforming prior models.", "AI": {"tldr": "本文提出了一种名为ROD的纯RGB方法，用于越野自由空间检测，克服了多模态方法因LiDAR数据处理而导致的实时性问题，实现了更高的精度和推理速度，并成为新的SOTA。", "motivation": "越野自由空间检测比公路场景更具挑战性，且现有最先进的多模态方法（RGB+LiDAR）在计算LiDAR表面法线图时推理时间显著增加，不适用于需要高帧率的实时应用。", "method": "ROD是一种纯RGB方法，不依赖LiDAR数据。它利用预训练的Vision Transformer (ViT) 从RGB图像中提取丰富的特征，并设计了一个轻量级但高效的解码器，以提高精度和推理速度。", "result": "ROD在ORFD和RELLIS-3D数据集上建立了新的最先进性能（SOTA），推理速度达到50 FPS，显著优于现有模型。", "conclusion": "ROD证明了纯RGB方法在越野自由空间检测中可以实现高精度和实时性，克服了传统多模态方法的计算瓶颈，为实际应用提供了更高效的解决方案。"}}
{"id": "2508.08816", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08816", "abs": "https://arxiv.org/abs/2508.08816", "authors": ["Yuechen Wang", "Yuming Qiao", "Dan Meng", "Jun Yang", "Haonan Lu", "Zhenyu Yang", "Xudong Zhang"], "title": "Efficient Agent: Optimizing Planning Capability for Multimodal Retrieval Augmented Generation", "comment": null, "summary": "Multimodal Retrieval-Augmented Generation (mRAG) has emerged as a promising\nsolution to address the temporal limitations of Multimodal Large Language\nModels (MLLMs) in real-world scenarios like news analysis and trending topics.\nHowever, existing approaches often suffer from rigid retrieval strategies and\nunder-utilization of visual information. To bridge this gap, we propose\nE-Agent, an agent framework featuring two key innovations: a mRAG planner\ntrained to dynamically orchestrate multimodal tools based on contextual\nreasoning, and a task executor employing tool-aware execution sequencing to\nimplement optimized mRAG workflows. E-Agent adopts a one-time mRAG planning\nstrategy that enables efficient information retrieval while minimizing\nredundant tool invocations. To rigorously assess the planning capabilities of\nmRAG systems, we introduce the Real-World mRAG Planning (RemPlan) benchmark.\nThis novel benchmark contains both retrieval-dependent and\nretrieval-independent question types, systematically annotated with essential\nretrieval tools required for each instance. The benchmark's explicit mRAG\nplanning annotations and diverse question design enhance its practical\nrelevance by simulating real-world scenarios requiring dynamic mRAG decisions.\nExperiments across RemPlan and three established benchmarks demonstrate\nE-Agent's superiority: 13% accuracy gain over state-of-the-art mRAG methods\nwhile reducing redundant searches by 37%.", "AI": {"tldr": "本文提出E-Agent，一个多模态检索增强生成（mRAG）代理框架，通过动态规划和工具感知执行优化mRAG工作流，并引入RemPlan基准评估规划能力。E-Agent在准确性上超越现有方法，并显著减少冗余搜索。", "motivation": "现有多模态大语言模型（MLLMs）在新闻分析、热点话题等实时场景中存在时间局限性。现有mRAG方法检索策略僵化，且未充分利用视觉信息，导致性能受限。", "method": "本文提出E-Agent框架，包含两项创新：1. 一个mRAG规划器，根据上下文推理动态协调多模态工具；2. 一个任务执行器，采用工具感知执行序列实现优化的mRAG工作流。E-Agent采用一次性mRAG规划策略，以提高检索效率并减少冗余工具调用。为评估mRAG系统的规划能力，本文还引入了Real-World mRAG Planning (RemPlan) 基准，其包含依赖检索和不依赖检索的问题类型，并明确标注所需的检索工具。", "result": "在RemPlan和三个现有基准上的实验表明，E-Agent优于现有mRAG方法：准确率提高13%，同时冗余搜索减少37%。", "conclusion": "E-Agent通过动态、高效的规划和执行，有效解决了现有mRAG的局限性，显著提升了多模态检索增强生成的性能和效率。所提出的RemPlan基准为评估mRAG系统的规划能力提供了新的、更贴近实际的工具。"}}
{"id": "2508.08730", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.08730", "abs": "https://arxiv.org/abs/2508.08730", "authors": ["Weibin Liao", "Tianlong Wang", "Yinghao Zhu", "Yasha Wang", "Junyi Gao", "Liantao Ma"], "title": "Magical: Medical Lay Language Generation via Semantic Invariance and Layperson-tailored Adaptation", "comment": null, "summary": "Medical Lay Language Generation (MLLG) plays a vital role in improving the\naccessibility of complex scientific content for broader audiences. Recent\nliterature to MLLG commonly employ parameter-efficient fine-tuning methods such\nas Low-Rank Adaptation (LoRA) to fine-tuning large language models (LLMs) using\npaired expert-lay language datasets. However, LoRA struggles with the\nchallenges posed by multi-source heterogeneous MLLG datasets. Specifically,\nthrough a series of exploratory experiments, we reveal that standard LoRA fail\nto meet the requirement for semantic fidelity and diverse lay-style generation\nin MLLG task. To address these limitations, we propose Magical, an asymmetric\nLoRA architecture tailored for MLLG under heterogeneous data scenarios. Magical\nemploys a shared matrix $A$ for abstractive summarization, along with multiple\nisolated matrices $B$ for diverse lay-style generation. To preserve semantic\nfidelity during the lay language generation process, Magical introduces a\nSemantic Invariance Constraint to mitigate semantic subspace shifts on matrix\n$A$. Furthermore, to better adapt to diverse lay-style generation, Magical\nincorporates the Recommendation-guided Switch, an externally interface to\nprompt the LLM to switch between different matrices $B$. Experimental results\non three real-world lay language generation datasets demonstrate that Magical\nconsistently outperforms prompt-based methods, vanilla LoRA, and its recent\nvariants, while also reducing trainable parameters by 31.66%.", "AI": {"tldr": "提出Magical，一种非对称LoRA架构，用于解决异构医疗通俗语言生成（MLLG）中语义保真度和风格多样性不足的问题。", "motivation": "标准LoRA在处理多源异构MLLG数据集时，难以满足语义保真度和多样化通俗风格生成的要求。", "method": "提出Magical，一种非对称LoRA架构，采用共享矩阵A进行抽象摘要，并使用多个独立的矩阵B进行多样化通俗风格生成。通过引入语义不变性约束来保持语义保真度，并通过推荐引导开关来适应多样化风格生成。", "result": "在三个真实世界通俗语言生成数据集上，Magical持续优于基于提示的方法、标准LoRA及其最新变体，同时可训练参数减少了31.66%。", "conclusion": "Magical有效解决了异构MLLG中的挑战，在提高通俗语言生成质量和效率方面表现出色，有助于提升复杂科学内容的普及性。"}}
{"id": "2508.08700", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08700", "abs": "https://arxiv.org/abs/2508.08700", "authors": ["Qi Zheng", "Li-Heng Chen", "Chenlong He", "Neil Berkbeck", "Yilin Wang", "Balu Adsumilli", "Alan C. Bovik", "Yibo Fan", "Zhengzhong Tu"], "title": "Subjective and Objective Quality Assessment of Banding Artifacts on Compressed Videos", "comment": null, "summary": "Although there have been notable advancements in video compression\ntechnologies in recent years, banding artifacts remain a serious issue\naffecting the quality of compressed videos, particularly on smooth regions of\nhigh-definition videos. Noticeable banding artifacts can severely impact the\nperceptual quality of videos viewed on a high-end HDTV or high-resolution\nscreen. Hence, there is a pressing need for a systematic investigation of the\nbanding video quality assessment problem for advanced video codecs. Given that\nthe existing publicly available datasets for studying banding artifacts are\nlimited to still picture data only, which cannot account for temporal banding\ndynamics, we have created a first-of-a-kind open video dataset, dubbed\nLIVE-YT-Banding, which consists of 160 videos generated by four different\ncompression parameters using the AV1 video codec. A total of 7,200 subjective\nopinions are collected from a cohort of 45 human subjects. To demonstrate the\nvalue of this new resources, we tested and compared a variety of models that\ndetect banding occurrences, and measure their impact on perceived quality.\nAmong these, we introduce an effective and efficient new no-reference (NR)\nvideo quality evaluator which we call CBAND. CBAND leverages the properties of\nthe learned statistics of natural images expressed in the embeddings of deep\nneural networks. Our experimental results show that the perceptual banding\nprediction performance of CBAND significantly exceeds that of previous\nstate-of-the-art models, and is also orders of magnitude faster. Moreover,\nCBAND can be employed as a differentiable loss function to optimize video\ndebanding models. The LIVE-YT-Banding database, code, and pre-trained model are\nall publically available at https://github.com/uniqzheng/CBAND.", "AI": {"tldr": "本文创建了一个新的视频带状伪影数据集LIVE-YT-Banding，并提出了一种高效的无参考视频质量评估器CBAND，其在带状伪影预测性能上显著优于现有模型。", "motivation": "尽管视频压缩技术取得了进展，但带状伪影（banding artifacts）仍严重影响压缩视频质量，尤其是在高清视频的平滑区域。现有用于研究带状伪影的公开数据集仅限于静态图像，无法捕捉时间动态，因此迫切需要对高级视频编解码器中的带状伪影视频质量评估问题进行系统性研究。", "method": "1. 创建了首个开放视频数据集LIVE-YT-Banding，包含160个使用AV1编解码器以四种不同压缩参数生成的视频。2. 从45名人类受试者收集了7,200个主观意见。3. 测试并比较了多种现有的带状伪影检测和感知质量测量模型。4. 引入了一种新的有效且高效的无参考（NR）视频质量评估器CBAND，该方法利用深度神经网络嵌入中表达的自然图像学习统计特性。", "result": "1. CBAND在感知带状伪影预测性能上显著优于现有最先进的模型。2. CBAND的速度比现有模型快几个数量级。3. CBAND可以作为可微分的损失函数，用于优化视频去带状伪影模型。4. LIVE-YT-Banding数据库、代码和预训练模型均已公开可用。", "conclusion": "LIVE-YT-Banding数据集弥补了现有带状伪影研究中缺乏时间动态数据的空白。所提出的CBAND模型为无参考视频带状伪影质量评估提供了一个卓越且高效的解决方案，并能促进去带状伪影模型的开发与优化。"}}
{"id": "2508.08830", "categories": ["cs.AI", "cs.CV", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.08830", "abs": "https://arxiv.org/abs/2508.08830", "authors": ["Mustafa Akben", "Vinayaka Gude", "Haya Ajjan"], "title": "Silicon Minds versus Human Hearts: The Wisdom of Crowds Beats the Wisdom of AI in Emotion Recognition", "comment": null, "summary": "The ability to discern subtle emotional cues is fundamental to human social\nintelligence. As artificial intelligence (AI) becomes increasingly common, AI's\nability to recognize and respond to human emotions is crucial for effective\nhuman-AI interactions. In particular, whether such systems can match or surpass\nhuman experts remains to be seen. However, the emotional intelligence of AI,\nparticularly multimodal large language models (MLLMs), remains largely\nunexplored. This study evaluates the emotion recognition abilities of MLLMs\nusing the Reading the Mind in the Eyes Test (RMET) and its multiracial\ncounterpart (MRMET), and compares their performance against human participants.\nResults show that, on average, MLLMs outperform humans in accurately\nidentifying emotions across both tests. This trend persists even when comparing\nperformance across low, medium, and expert-level performing groups. Yet when we\naggregate independent human decisions to simulate collective intelligence,\nhuman groups significantly surpass the performance of aggregated MLLM\npredictions, highlighting the wisdom of the crowd. Moreover, a collaborative\napproach (augmented intelligence) that combines human and MLLM predictions\nachieves greater accuracy than either humans or MLLMs alone. These results\nsuggest that while MLLMs exhibit strong emotion recognition at the individual\nlevel, the collective intelligence of humans and the synergistic potential of\nhuman-AI collaboration offer the most promising path toward effective emotional\nAI. We discuss the implications of these findings for the development of\nemotionally intelligent AI systems and future research directions.", "AI": {"tldr": "研究评估了多模态大语言模型（MLLMs）的情绪识别能力，发现MLLMs个体表现优于人类，但人类集体智慧超越MLLMs，而人机协作能达到最佳效果。", "motivation": "随着AI日益普及，其识别和响应人类情绪的能力对有效的人机交互至关重要。特别是，AI能否匹敌或超越人类专家在情绪识别方面的表现尚不清楚，而MLLMs的情绪智能更是未被充分探索。", "method": "本研究使用“读心识眼测试”（RMET）及其多民族版本（MRMET）评估了MLLMs的情绪识别能力，并将其表现与人类参与者进行比较。此外，还模拟了人类的集体智慧，并探索了人与MLLM预测相结合的协作方法（增强智能）。", "result": "结果显示，平均而言，MLLMs在两项测试中识别情绪的准确性均优于人类个体。然而，当模拟集体智慧时，人类群体的表现显著超越了聚合的MLLM预测。更进一步，结合人类和MLLM预测的协作方法（增强智能）比单独的人类或MLLMs取得了更高的准确性。", "conclusion": "虽然MLLMs在个体层面展现出强大的情绪识别能力，但人类的集体智慧以及人机协作的协同潜力，为开发情感智能AI系统提供了最有前景的途径。"}}
{"id": "2508.08742", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08742", "abs": "https://arxiv.org/abs/2508.08742", "authors": ["Haotian Chen", "Qingqing Long", "Meng Xiao", "Xiao Luo", "Wei Ju", "Chengrui Wang", "Xuezhi Wang", "Yuanchun Zhou", "Hengshu Zhu"], "title": "SciRerankBench: Benchmarking Rerankers Towards Scientific Retrieval-Augmented Generated LLMs", "comment": null, "summary": "Scientific literature question answering is a pivotal step towards new\nscientific discoveries. Recently, \\textit{two-stage} retrieval-augmented\ngenerated large language models (RAG-LLMs) have shown impressive advancements\nin this domain. Such a two-stage framework, especially the second stage\n(reranker), is particularly essential in the scientific domain, where subtle\ndifferences in terminology may have a greatly negative impact on the final\nfactual-oriented or knowledge-intensive answers. Despite this significant\nprogress, the potential and limitations of these works remain unexplored. In\nthis work, we present a Scientific Rerank-oriented RAG Benchmark\n(SciRerankBench), for evaluating rerankers within RAG-LLMs systems, spanning\nfive scientific subjects. To rigorously assess the reranker performance in\nterms of noise resilience, relevance disambiguation, and factual consistency,\nwe develop three types of question-context-answer (Q-C-A) pairs, i.e., Noisy\nContexts (NC), Semantically Similar but Logically Irrelevant Contexts (SSLI),\nand Counterfactual Contexts (CC). Through systematic evaluation of 13 widely\nused rerankers on five families of LLMs, we provide detailed insights into\ntheir relative strengths and limitations. To the best of our knowledge,\nSciRerankBench is the first benchmark specifically developed to evaluate\nrerankers within RAG-LLMs, which provides valuable observations and guidance\nfor their future development.", "AI": {"tldr": "本文提出了SciRerankBench，一个专门用于评估科学领域RAG-LLM系统中重排器性能的基准，并揭示了现有重排器的优缺点。", "motivation": "两阶段检索增强生成大型语言模型（RAG-LLM）在科学文献问答中取得了显著进展，其中重排器（第二阶段）至关重要，尤其是在科学领域，术语的细微差异可能严重影响答案的准确性。然而，这些工作的潜力和局限性尚未被充分探索。", "method": "开发了SciRerankBench，一个涵盖五个科学主题的科学重排器评估基准。为了严格评估重排器在抗噪声、相关性消歧和事实一致性方面的性能，设计了三种问答对类型：噪声上下文（NC）、语义相似但逻辑不相关的上下文（SSLI）和反事实上下文（CC）。系统评估了13种广泛使用的重排器在五类大型语言模型上的表现。", "result": "通过对13种重排器在五类LLM上的系统评估，提供了关于它们相对优势和局限性的详细见解。据作者所知，SciRerankBench是第一个专门为评估RAG-LLM中重排器而开发的基准。", "conclusion": "SciRerankBench为RAG-LLM中重排器的未来发展提供了宝贵的观察和指导。"}}
{"id": "2508.08701", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.08701", "abs": "https://arxiv.org/abs/2508.08701", "authors": ["Ouyang Xu", "Baoming Zhang", "Ruiyu Mao", "Yunhui Guo"], "title": "SafeFix: Targeted Model Repair via Controlled Image Generation", "comment": null, "summary": "Deep learning models for visual recognition often exhibit systematic errors\ndue to underrepresented semantic subpopulations. Although existing debugging\nframeworks can pinpoint these failures by identifying key failure attributes,\nrepairing the model effectively remains difficult. Current solutions often rely\non manually designed prompts to generate synthetic training images -- an\napproach prone to distribution shift and semantic errors. To overcome these\nchallenges, we introduce a model repair module that builds on an interpretable\nfailure attribution pipeline. Our approach uses a conditional text-to-image\nmodel to generate semantically faithful and targeted images for failure cases.\nTo preserve the quality and relevance of the generated samples, we further\nemploy a large vision-language model (LVLM) to filter the outputs, enforcing\nalignment with the original data distribution and maintaining semantic\nconsistency. By retraining vision models with this rare-case-augmented\nsynthetic dataset, we significantly reduce errors associated with rare cases.\nOur experiments demonstrate that this targeted repair strategy improves model\nrobustness without introducing new bugs. Code is available at\nhttps://github.com/oxu2/SafeFix", "AI": {"tldr": "该研究提出一种模型修复模块，通过利用可解释的故障归因管线和条件文本到图像模型生成语义准确且有针对性的合成图像，并使用大型视觉语言模型（LVLM）过滤，以修复深度学习模型在稀有语义子群体上的系统性错误。", "motivation": "深度学习模型在视觉识别中常因语义子群体代表性不足而出现系统性错误。现有调试框架虽能识别故障属性，但有效修复模型仍很困难。当前解决方案依赖手动设计的提示生成合成训练图像，这容易导致分布偏移和语义错误。", "method": "本研究基于一个可解释的故障归因管线，引入模型修复模块。它使用条件文本到图像模型为故障案例生成语义忠实且有针对性的图像。为保持生成样本的质量和相关性，进一步采用大型视觉语言模型（LVLM）进行过滤，以确保与原始数据分布对齐并保持语义一致性。最后，使用这个罕见案例增强的合成数据集重新训练视觉模型。", "result": "通过使用这种罕见案例增强的合成数据集重新训练视觉模型，显著减少了与罕见案例相关的错误。实验证明，这种有针对性的修复策略提高了模型鲁棒性，且没有引入新的错误。", "conclusion": "该研究表明，通过结合可解释的故障归因、有针对性的合成图像生成和LVLM过滤，可以有效地修复深度学习模型在稀有案例上的系统性错误，提高模型鲁棒性而不会引入新的缺陷。"}}
{"id": "2508.08882", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08882", "abs": "https://arxiv.org/abs/2508.08882", "authors": ["Dayu Wang", "Jiaye Yang", "Weikang Li", "Jiahui Liang", "Yang Li"], "title": "Reducing Cognitive Load in Multi-Agent Reinforcement Learning for Mathematical Problem Solving: Decoupling Reasoning and Code Generation", "comment": null, "summary": "Current tool-integrated mathematical reasoning systems often adopt a\nsingle-agent paradigm, where one large language model handles problem\nreasoning, code generation, and code execution in an integrated workflow. While\nthis design eases coordination, we hypothesize that it imposes cognitive load\ninterference, as the agent must interleave long-horizon reasoning with precise\nprogram synthesis. We validate this hypothesis through a controlled comparison\nbetween a reasoning-only agent and a reasoning-plus-code agent, finding that\nthe latter produces significantly fewer correct reasoning paths despite having\ntool-calling capabilities. To address this, we propose a dual-agent hybrid\nframework: a Reasoning Agent performs stepwise problem decomposition, and a\nCode Agent handles code generation and execution. Training combines imitation\nlearning and reinforcement learning: the Code Agent receives strong rewards for\nmatching intermediate ground-truth programs and weaker rewards for valid\nexecution, while the Reasoning Agent is optimized chiefly via final-answer\naccuracy using advantage estimation to credit intermediate steps. This\ndecoupled role design reduces cognitive interference and promotes stable\nreasoning-coding coordination.", "AI": {"tldr": "该研究提出了一种双智能体混合框架，将数学推理系统中的问题分解与代码生成/执行解耦，以减少认知干扰并提高性能。", "motivation": "当前的单智能体数学推理系统（一个大型语言模型处理推理、代码生成和执行）存在认知负荷干扰，因为智能体必须交错长周期推理和精确的程序合成，导致推理路径的正确性降低。", "method": "首先，通过对比纯推理智能体和推理加代码智能体，验证了后者在正确推理路径上表现更差的假设。其次，提出了一个双智能体框架：推理智能体负责逐步问题分解，代码智能体处理代码生成和执行。训练结合了模仿学习和强化学习：代码智能体因匹配中间真实程序而获得强奖励，因有效执行获得弱奖励；推理智能体主要通过最终答案准确性进行优化，并使用优势估计来奖励中间步骤。", "result": "研究发现，具有工具调用能力的推理加代码智能体产生的正确推理路径显著减少。所提出的双智能体解耦设计能有效减少认知干扰，并促进推理与编码的稳定协调。", "conclusion": "将数学推理系统中的推理和编码角色解耦到独立的智能体中，可以有效降低认知干扰，提升推理与编码的协同稳定性，从而提高系统性能。"}}
{"id": "2508.08761", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08761", "abs": "https://arxiv.org/abs/2508.08761", "authors": ["Stavros Doropoulos", "Stavros Vologiannidis", "Ioannis Magnisalis"], "title": "DevNous: An LLM-Based Multi-Agent System for Grounding IT Project Management in Unstructured Conversation", "comment": null, "summary": "The manual translation of unstructured team dialogue into the structured\nartifacts required for Information Technology (IT) project governance is a\ncritical bottleneck in modern information systems management. We introduce\nDevNous, a Large Language Model-based (LLM) multi-agent expert system, to\nautomate this unstructured-to-structured translation process. DevNous\nintegrates directly into team chat environments, identifying actionable intents\nfrom informal dialogue and managing stateful, multi-turn workflows for core\nadministrative tasks like automated task formalization and progress summary\nsynthesis. To quantitatively evaluate the system, we introduce a new benchmark\nof 160 realistic, interactive conversational turns. The dataset was manually\nannotated with a multi-label ground truth and is publicly available. On this\nbenchmark, DevNous achieves an exact match turn accuracy of 81.3\\% and a\nmultiset F1-Score of 0.845, providing strong evidence for its viability. The\nprimary contributions of this work are twofold: (1) a validated architectural\npattern for developing ambient administrative agents, and (2) the introduction\nof the first robust empirical baseline and public benchmark dataset for this\nchallenging problem domain.", "AI": {"tldr": "DevNous是一个基于LLM的多智能体专家系统，旨在自动化将团队非结构化对话转化为IT项目治理所需结构化工件的过程，并在新基准测试中表现出色。", "motivation": "将非结构化的团队对话手动翻译成IT项目治理所需的结构化工件是现代信息系统管理中的一个关键瓶颈。", "method": "引入DevNous系统，一个基于大型语言模型（LLM）的多智能体专家系统。该系统直接集成到团队聊天环境中，能够识别非正式对话中的可操作意图，并管理有状态、多轮的工作流，以完成自动化任务形式化和进度摘要合成等核心管理任务。为评估系统，创建了一个包含160个现实交互对话轮次的新基准数据集，并进行手动多标签标注。", "result": "在新的基准测试中，DevNous实现了81.3%的准确匹配回合精度和0.845的多集F1分数，证明了其可行性。", "conclusion": "该工作的主要贡献是：1) 提出了一种经过验证的开发环境管理代理的架构模式；2) 引入了针对这一挑战性问题领域的第一个稳健的实证基线和公开基准数据集。DevNous为解决IT项目治理中的对话转换瓶颈提供了有前景的解决方案。"}}
{"id": "2508.08705", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08705", "abs": "https://arxiv.org/abs/2508.08705", "authors": ["Zunjie Xiao", "Xiao Wu", "Tianhang Liu", "Lingxi Hu", "Yinling Zhang", "Xiaoqing Zhang", "Risa Higashita", "Jiang Liu"], "title": "Adaptive Confidence-Wise Loss for Improved Lens Structure Segmentation in AS-OCT", "comment": null, "summary": "Precise lens structure segmentation is essential for the design of\nintraocular lenses (IOLs) in cataract surgery. Existing deep segmentation\nnetworks typically weight all pixels equally under cross-entropy (CE) loss,\noverlooking the fact that sub-regions of lens structures are inhomogeneous\n(e.g., some regions perform better than others) and that boundary regions often\nsuffer from poor segmentation calibration at the pixel level. Clinically,\nexperts annotate different sub-regions of lens structures with varying\nconfidence levels, considering factors such as sub-region proportions,\nambiguous boundaries, and lens structure shapes. Motivated by this observation,\nwe propose an Adaptive Confidence-Wise (ACW) loss to group each lens structure\nsub-region into different confidence sub-regions via a confidence threshold\nfrom the unique region aspect, aiming to exploit the potential of expert\nannotation confidence prior. Specifically, ACW clusters each target region into\nlow-confidence and high-confidence groups and then applies a region-weighted\nloss to reweigh each confidence group. Moreover, we design an adaptive\nconfidence threshold optimization algorithm to adjust the confidence threshold\nof ACW dynamically. Additionally, to better quantify the miscalibration errors\nin boundary region segmentation, we propose a new metric, termed Boundary\nExpected Calibration Error (BECE). Extensive experiments on a clinical lens\nstructure AS-OCT dataset and other multi-structure datasets demonstrate that\nour ACW significantly outperforms competitive segmentation loss methods across\ndifferent deep segmentation networks (e.g., MedSAM). Notably, our method\nsurpasses CE with 6.13% IoU gain, 4.33% DSC increase, and 4.79% BECE reduction\nin lens structure segmentation under U-Net. The code of this paper is available\nat https://github.com/XiaoLing12138/Adaptive-Confidence-Wise-Loss.", "AI": {"tldr": "针对白内障手术中晶状体结构分割不均匀和边界校准差的问题，本文提出了一种自适应置信度感知（ACW）损失函数，利用专家标注的置信度先验对区域进行分组并重新加权，并通过自适应阈值优化动态调整。同时引入了边界预期校准误差（BECE）新指标，实验证明ACW显著提升了分割性能。", "motivation": "现有的深度分割网络在交叉熵损失下平等对待所有像素，忽略了晶状体结构子区域的不均匀性（性能差异）以及边界区域像素级分割校准差的问题。临床上，专家在标注晶状体结构的不同子区域时，会根据区域比例、模糊边界和结构形状等因素，赋予不同的置信度，这启发了作者利用专家标注的置信度先验。", "method": "本文提出了自适应置信度感知（ACW）损失，通过置信度阈值将每个晶状体结构子区域划分为低置信度组和高置信度组，并对每个置信度组应用区域加权损失进行重新加权。此外，设计了一种自适应置信度阈值优化算法来动态调整ACW的置信度阈值。为了更好地量化边界区域分割的校准误差，提出了一种新的度量标准——边界预期校准误差（BECE）。", "result": "在临床晶状体结构AS-OCT数据集和其他多结构数据集上的大量实验表明，ACW在不同的深度分割网络（如MedSAM）上显著优于竞争性的分割损失方法。特别是在U-Net下进行晶状体结构分割时，ACW相对于CE损失在IoU上提高了6.13%，DSC提高了4.33%，BECE降低了4.79%。", "conclusion": "通过整合专家标注的置信度先验并自适应地重新加权子区域，本文提出的ACW损失显著提高了晶状体结构的精确分割性能，并有效解决了边界区域的校准误差问题。新提出的BECE指标也为边界分割的校准误差量化提供了更好的方法。"}}
{"id": "2508.08909", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08909", "abs": "https://arxiv.org/abs/2508.08909", "authors": ["Anxiang Zeng", "Haibo Zhang", "Kaixiang Mo", "Long Zhang", "Shuman Liu", "Yanhui Huang", "Yawen Liu", "Yuepeng Sheng", "Yuwei Huang"], "title": "Compass-Thinker-7B Technical Report", "comment": null, "summary": "Recent R1-Zero-like research further demonstrates that reasoning extension\nhas given large language models (LLMs) unprecedented reasoning capabilities,\nand Reinforcement Learning is the core technology to elicit its complex\nreasoning. However, conducting RL experiments directly on hyperscale models\ninvolves high computational costs and resource demands, posing significant\nrisks. We propose the Compass-Thinker-7B model, which aims to explore the\npotential of Reinforcement Learning with less computational resources and\ncosts, and provides insights for further research into RL recipes for larger\nmodels. Compass-Thinker-7B is trained from an open source model through a\nspecially designed Reinforcement Learning Pipeline. we curate a dataset of 30k\nverifiable mathematics problems for the Reinforcement Learning Pipeline. By\nconfiguring data and training settings with different difficulty distributions\nfor different stages, the potential of the model is gradually released and the\ntraining efficiency is improved. Extensive evaluations show that\nCompass-Thinker-7B possesses exceptional reasoning potential, and achieves\nsuperior performance on mathematics compared to the same-sized RL\nmodel.Especially in the challenging AIME2024 evaluation, Compass-Thinker-7B\nachieves 40% accuracy.", "AI": {"tldr": "该研究提出了Compass-Thinker-7B模型，旨在以更低的计算成本和资源消耗探索强化学习在大型语言模型推理能力上的潜力，并在数学推理任务上取得了优异表现。", "motivation": "现有研究表明强化学习是激发大型语言模型复杂推理能力的核心技术，但直接在超大规模模型上进行强化学习实验计算成本和资源需求极高，风险巨大。", "method": "提出了Compass-Thinker-7B模型，通过专门设计的强化学习管道从开源模型进行训练。构建了一个包含3万个可验证数学问题的RL训练数据集，并为不同阶段配置了不同难度分布的数据和训练设置，以逐步释放模型潜力并提高训练效率。", "result": "Compass-Thinker-7B模型展现出卓越的推理潜力，在数学任务上超越了同等规模的RL模型。特别是在极具挑战性的AIME2024评估中，达到了40%的准确率。", "conclusion": "Compass-Thinker-7B模型证明了在较低资源消耗下，强化学习仍能有效提升模型推理能力，并为更大模型上的强化学习配方研究提供了有价值的见解。"}}
{"id": "2508.08785", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.08785", "abs": "https://arxiv.org/abs/2508.08785", "authors": ["Yunfeng Ning", "Mayi Xu", "Jintao Wen", "Qiankun Pi", "Yuanyuan Zhu", "Ming Zhong", "Jiawei Jiang", "Tieyun Qian"], "title": "Privacy-protected Retrieval-Augmented Generation for Knowledge Graph Question Answering", "comment": null, "summary": "LLMs often suffer from hallucinations and outdated or incomplete knowledge.\nRAG is proposed to address these issues by integrating external knowledge like\nthat in KGs into LLMs. However, leveraging private KGs in RAG systems poses\nsignificant privacy risks due to the black-box nature of LLMs and potential\ninsecure data transmission, especially when using third-party LLM APIs lacking\ntransparency and control. In this paper, we investigate the privacy-protected\nRAG scenario for the first time, where entities in KGs are anonymous for LLMs,\nthus preventing them from accessing entity semantics. Due to the loss of\nsemantics of entities, previous RAG systems cannot retrieve question-relevant\nknowledge from KGs by matching questions with the meaningless identifiers of\nanonymous entities. To realize an effective RAG system in this scenario, two\nkey challenges must be addressed: (1) How can anonymous entities be converted\ninto retrievable information. (2) How to retrieve question-relevant anonymous\nentities. Hence, we propose a novel ARoG framework including relation-centric\nabstraction and structure-oriented abstraction strategies. For challenge (1),\nthe first strategy abstracts entities into high-level concepts by dynamically\ncapturing the semantics of their adjacent relations. It supplements meaningful\nsemantics which can further support the retrieval process. For challenge (2),\nthe second strategy transforms unstructured natural language questions into\nstructured abstract concept paths. These paths can be more effectively aligned\nwith the abstracted concepts in KGs, thereby improving retrieval performance.\nTo guide LLMs to effectively retrieve knowledge from KGs, the two strategies\nstrictly protect privacy from being exposed to LLMs. Experiments on three\ndatasets demonstrate that ARoG achieves strong performance and\nprivacy-robustness.", "AI": {"tldr": "本文首次提出并研究了隐私保护的RAG场景，其中知识图谱中的实体对LLM是匿名的。为此，我们提出了ARoG框架，通过关系中心抽象和结构导向抽象策略，在保护隐私的同时实现有效的知识检索。", "motivation": "大型语言模型（LLMs）存在幻觉和知识过时问题。检索增强生成（RAG）通过整合外部知识（如知识图谱）来解决这些问题。然而，在RAG系统中使用私有知识图谱，尤其是在使用缺乏透明度和控制的第三方LLM API时，由于LLMs的黑盒特性和潜在的不安全数据传输，会带来显著的隐私风险。", "method": "本文提出了一个新颖的ARoG框架，以解决隐私保护RAG场景中的两个关键挑战：如何将匿名实体转换为可检索信息，以及如何检索与问题相关的匿名实体。ARoG包含两种策略：1) 关系中心抽象：通过动态捕获相邻关系的语义，将实体抽象为高级概念，补充有意义的语义以支持检索。2) 结构导向抽象：将非结构化自然语言问题转换为结构化的抽象概念路径，使其能更有效地与知识图谱中的抽象概念对齐，从而提高检索性能。这两种策略严格保护隐私不暴露给LLMs。", "result": "在三个数据集上的实验表明，ARoG框架在实现强大的性能和隐私鲁棒性方面表现出色。", "conclusion": "ARoG框架通过创新的抽象策略，首次成功地在实体匿名的知识图谱场景下实现了隐私保护的RAG系统，有效解决了匿名实体检索的挑战，并在保护隐私的同时保持了高性能。"}}
{"id": "2508.08765", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08765", "abs": "https://arxiv.org/abs/2508.08765", "authors": ["Andrea Montibeller", "Dasara Shullani", "Daniele Baracchi", "Alessandro Piva", "Giulia Boato"], "title": "Bridging the Gap: A Framework for Real-World Video Deepfake Detection via Social Network Compression Emulation", "comment": null, "summary": "The growing presence of AI-generated videos on social networks poses new\nchallenges for deepfake detection, as detectors trained under controlled\nconditions often fail to generalize to real-world scenarios. A key factor\nbehind this gap is the aggressive, proprietary compression applied by platforms\nlike YouTube and Facebook, which launder low-level forensic cues. However,\nreplicating these transformations at scale is difficult due to API limitations\nand data-sharing constraints. For these reasons, we propose a first framework\nthat emulates the video sharing pipelines of social networks by estimating\ncompression and resizing parameters from a small set of uploaded videos. These\nparameters enable a local emulator capable of reproducing platform-specific\nartifacts on large datasets without direct API access. Experiments on\nFaceForensics++ videos shared via social networks demonstrate that our emulated\ndata closely matches the degradation patterns of real uploads. Furthermore,\ndetectors fine-tuned on emulated videos achieve comparable performance to those\ntrained on actual shared media. Our approach offers a scalable and practical\nsolution for bridging the gap between lab-based training and real-world\ndeployment of deepfake detectors, particularly in the underexplored domain of\ncompressed video content.", "AI": {"tldr": "针对社交媒体上AI生成视频的深度伪造检测，由于平台压缩导致现有检测器泛化能力差。本文提出一个框架，通过估计压缩参数来模拟社交网络视频共享管道，从而在本地重现平台特有伪影，有效提升检测器性能。", "motivation": "AI生成视频在社交网络上日益增多，但受控条件下训练的深度伪造检测器难以泛化到真实世界场景。主要原因是YouTube和Facebook等平台采用的激进专有压缩会消除低级取证线索。然而，由于API限制和数据共享约束，大规模复制这些转换很困难。", "method": "提出一个首次框架，通过从少量上传视频中估计压缩和调整大小参数来模拟社交网络的视频共享管道。这些参数使得一个本地模拟器能够在没有直接API访问的情况下，在大数据集上重现平台特有的伪影。", "result": "在通过社交网络共享的FaceForensics++视频上的实验表明，模拟数据与真实上传的退化模式高度匹配。此外，在模拟视频上微调的检测器，其性能与在实际共享媒体上训练的检测器相当。", "conclusion": "该方法为弥合实验室训练与深度伪造检测器实际部署之间的差距提供了一个可扩展且实用的解决方案，特别是在压缩视频内容这一未充分探索的领域。"}}
{"id": "2508.08926", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08926", "abs": "https://arxiv.org/abs/2508.08926", "authors": ["Wei Cai", "Jian Zhao", "Yuchu Jiang", "Tianle Zhang", "Xuelong Li"], "title": "Safe Semantics, Unsafe Interpretations: Tackling Implicit Reasoning Safety in Large Vision-Language Models", "comment": null, "summary": "Large Vision-Language Models face growing safety challenges with multimodal\ninputs. This paper introduces the concept of Implicit Reasoning Safety, a\nvulnerability in LVLMs. Benign combined inputs trigger unsafe LVLM outputs due\nto flawed or hidden reasoning. To showcase this, we developed Safe Semantics,\nUnsafe Interpretations, the first dataset for this critical issue. Our\ndemonstrations show that even simple In-Context Learning with SSUI\nsignificantly mitigates these implicit multimodal threats, underscoring the\nurgent need to improve cross-modal implicit reasoning.", "AI": {"tldr": "大型视觉语言模型（LVLMs）存在一种名为“隐式推理安全”的新漏洞，即良性多模态输入因 flawed 或隐藏推理导致不安全输出。本文引入了SSUI数据集来展示此问题，并证明简单的上下文学习可显著缓解此类威胁，强调需改进跨模态隐式推理。", "motivation": "大型视觉语言模型（LVLMs）在处理多模态输入时面临日益增长的安全挑战，特别是当良性组合输入因LVLM内部有缺陷或隐藏的推理而触发不安全输出时。", "method": "本文提出了“隐式推理安全”的概念，并开发了首个针对此关键问题的“安全语义，不安全解释”（SSUI）数据集。研究通过简单的上下文学习（In-Context Learning, ICL）配合SSUI数据集来演示并缓解这些隐式多模态威胁。", "result": "研究表明，即使是使用SSUI数据集进行的简单上下文学习，也能显著缓解这些隐式多模态威胁。", "conclusion": "迫切需要改进大型视觉语言模型中的跨模态隐式推理能力，以解决“隐式推理安全”这一漏洞。"}}
{"id": "2508.08791", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08791", "abs": "https://arxiv.org/abs/2508.08791", "authors": ["Junjie Ye", "Changhao Jiang", "Zhengyin Du", "Yufei Xu", "Xuesong Yao", "Zhiheng Xi", "Xiaoran Fan", "Qi Zhang", "Xuanjing Huang", "Jiecao Chen"], "title": "Feedback-Driven Tool-Use Improvements in Large Language Models via Automated Build Environments", "comment": null, "summary": "Effective tool use is essential for large language models (LLMs) to interact\nmeaningfully with their environment. However, progress is limited by the lack\nof efficient reinforcement learning (RL) frameworks specifically designed for\ntool use, due to challenges in constructing stable training environments and\ndesigning verifiable reward mechanisms. To address this, we propose an\nautomated environment construction pipeline, incorporating scenario\ndecomposition, document generation, function integration, complexity scaling,\nand localized deployment. This enables the creation of high-quality training\nenvironments that provide detailed and measurable feedback without relying on\nexternal tools. Additionally, we introduce a verifiable reward mechanism that\nevaluates both the precision of tool use and the completeness of task\nexecution. When combined with trajectory data collected from the constructed\nenvironments, this mechanism integrates seamlessly with standard RL algorithms\nto facilitate feedback-driven model training. Experiments on LLMs of varying\nscales demonstrate that our approach significantly enhances the models'\ntool-use performance without degrading their general capabilities, regardless\nof inference modes or training algorithms. Our analysis suggests that these\ngains result from improved context understanding and reasoning, driven by\nupdates to the lower-layer MLP parameters in models.", "AI": {"tldr": "该研究提出了一个自动化环境构建流程和可验证的奖励机制，以提升大型语言模型（LLMs）的工具使用能力，并在实验中取得了显著效果，且不损害模型的通用能力。", "motivation": "大型语言模型（LLMs）有效使用工具的能力对它们与环境的交互至关重要，但当前缺乏专门为工具使用设计的有效强化学习（RL）框架，主要挑战在于构建稳定的训练环境和设计可验证的奖励机制。", "method": "1. 自动化环境构建流程：包括场景分解、文档生成、函数集成、复杂度扩展和本地化部署，以创建高质量的训练环境，提供详细且可衡量的反馈，且不依赖外部工具。\n2. 可验证的奖励机制：评估工具使用的精确性和任务执行的完整性。\n3. 将此机制与从构建环境中收集的轨迹数据结合，无缝集成到标准RL算法中，以实现反馈驱动的模型训练。", "result": "在不同规模的LLMs上进行的实验表明，该方法显著增强了模型的工具使用性能，且不损害其通用能力，无论推理模式或训练算法如何。分析表明，性能提升源于模型底层MLP参数的更新，从而改善了上下文理解和推理能力。", "conclusion": "本研究提出的自动化环境构建和可验证奖励机制，有效解决了LLMs工具使用训练中的环境和奖励挑战，显著提升了LLMs的工具使用能力，并通过改进上下文理解和推理实现了这一目标。"}}
{"id": "2508.08781", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08781", "abs": "https://arxiv.org/abs/2508.08781", "authors": ["Trong-Thuan Nguyen", "Viet-Tham Huynh", "Quang-Thuc Nguyen", "Hoang-Phuc Nguyen", "Long Le Bao", "Thai Hoang Minh", "Minh Nguyen Anh", "Thang Nguyen Tien", "Phat Nguyen Thuan", "Huy Nguyen Phong", "Bao Huynh Thai", "Vinh-Tiep Nguyen", "Duc-Vu Nguyen", "Phu-Hoa Pham", "Minh-Huy Le-Hoang", "Nguyen-Khang Le", "Minh-Chinh Nguyen", "Minh-Quan Ho", "Ngoc-Long Tran", "Hien-Long Le-Hoang", "Man-Khoi Tran", "Anh-Duong Tran", "Kim Nguyen", "Quan Nguyen Hung", "Dat Phan Thanh", "Hoang Tran Van", "Tien Huynh Viet", "Nhan Nguyen Viet Thien", "Dinh-Khoi Vo", "Van-Loc Nguyen", "Trung-Nghia Le", "Tam V. Nguyen", "Minh-Triet Tran"], "title": "SHREC 2025: Retrieval of Optimal Objects for Multi-modal Enhanced Language and Spatial Assistance (ROOMELSA)", "comment": null, "summary": "Recent 3D retrieval systems are typically designed for simple, controlled\nscenarios, such as identifying an object from a cropped image or a brief\ndescription. However, real-world scenarios are more complex, often requiring\nthe recognition of an object in a cluttered scene based on a vague, free-form\ndescription. To this end, we present ROOMELSA, a new benchmark designed to\nevaluate a system's ability to interpret natural language. Specifically,\nROOMELSA attends to a specific region within a panoramic room image and\naccurately retrieves the corresponding 3D model from a large database. In\naddition, ROOMELSA includes over 1,600 apartment scenes, nearly 5,200 rooms,\nand more than 44,000 targeted queries. Empirically, while coarse object\nretrieval is largely solved, only one top-performing model consistently ranked\nthe correct match first across nearly all test cases. Notably, a lightweight\nCLIP-based model also performed well, although it struggled with subtle\nvariations in materials, part structures, and contextual cues, resulting in\noccasional errors. These findings highlight the importance of tightly\nintegrating visual and language understanding. By bridging the gap between\nscene-level grounding and fine-grained 3D retrieval, ROOMELSA establishes a new\nbenchmark for advancing robust, real-world 3D recognition systems.", "AI": {"tldr": "ROOMELSA是一个新的基准测试，旨在评估3D检索系统在复杂全景房间图像中，根据模糊的自然语言描述准确识别并检索3D模型的能力，揭示了当前模型在细粒度理解方面的挑战。", "motivation": "现有的3D检索系统通常针对简单、受控场景设计，例如从裁剪图像或简短描述中识别对象。然而，真实世界场景更为复杂，需要根据模糊的自由形式描述，在杂乱场景中识别对象，这促使了对更鲁棒系统的需求。", "method": "本文提出了ROOMELSA基准测试。它要求系统解释自然语言，在全景房间图像中关注特定区域，并从大型数据库中准确检索相应的3D模型。ROOMELSA包含超过1600个公寓场景、近5200个房间和超过44000个目标查询。", "result": "实验结果表明，粗略的对象检索已基本解决，但只有少数顶级模型能够一致地在几乎所有测试案例中将正确匹配项排在首位。一个轻量级的基于CLIP的模型表现良好，但在处理材料、部件结构和上下文线索的细微变化时遇到困难，导致偶尔出错。", "conclusion": "这些发现强调了视觉和语言理解紧密结合的重要性。ROOMELSA通过弥合场景级接地与细粒度3D检索之间的差距，为推进鲁棒的真实世界3D识别系统建立了新的基准。"}}
{"id": "2508.08992", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08992", "abs": "https://arxiv.org/abs/2508.08992", "authors": ["Rui Wang", "Qihan Lin", "Jiayu Liu", "Qing Zong", "Tianshi Zheng", "Weiqi Wang", "Yangqiu Song"], "title": "Prospect Theory Fails for LLMs: Revealing Instability of Decision-Making under Epistemic Uncertainty", "comment": null, "summary": "Prospect Theory (PT) models human decision-making under uncertainty, while\nepistemic markers (e.g., maybe) serve to express uncertainty in language.\nHowever, it remains largely unexplored whether Prospect Theory applies to\ncontemporary Large Language Models and whether epistemic markers, which express\nhuman uncertainty, affect their decision-making behaviour. To address these\nresearch gaps, we design a three-stage experiment based on economic\nquestionnaires. We propose a more general and precise evaluation framework to\nmodel LLMs' decision-making behaviour under PT, introducing uncertainty through\nthe empirical probability values associated with commonly used epistemic\nmarkers in comparable contexts. We then incorporate epistemic markers into the\nevaluation framework based on their corresponding probability values to examine\ntheir influence on LLM decision-making behaviours. Our findings suggest that\nmodelling LLMs' decision-making with PT is not consistently reliable,\nparticularly when uncertainty is expressed in diverse linguistic forms. Our\ncode is released in https://github.com/HKUST-KnowComp/MarPT.", "AI": {"tldr": "该研究探讨了前景理论是否适用于大型语言模型（LLM）的决策，以及不确定性语言标记对其决策行为的影响。结果表明，前景理论在建模LLM决策时并不总是可靠，尤其当不确定性以多种语言形式表达时。", "motivation": "前景理论（PT）在人类不确定性决策中得到应用，而认知标记（如“也许”）用于表达语言中的不确定性。然而，目前尚不清楚前景理论是否适用于当代大型语言模型，以及表达人类不确定性的认知标记是否会影响它们的决策行为。", "method": "研究设计了一个基于经济问卷的三阶段实验。提出了一个更通用和精确的评估框架来建模LLM在前景理论下的决策行为，通过常用认知标记在可比上下文中的经验概率值引入不确定性。随后，根据这些认知标记的相应概率值将其纳入评估框架，以检验它们对LLM决策行为的影响。", "result": "研究结果表明，使用前景理论来建模LLM的决策行为并不总是可靠的，特别是在不确定性以多样化语言形式表达时。", "conclusion": "前景理论在解释大型语言模型决策行为方面存在局限性，尤其是在面对通过不同语言形式表达的不确定性时，其建模能力并不一致可靠。"}}
{"id": "2508.08827", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.08827", "abs": "https://arxiv.org/abs/2508.08827", "authors": ["Robin Faro", "Dongyang Fan", "Tamar Alphaidze", "Martin Jaggi"], "title": "TiMoE: Time-Aware Mixture of Language Experts", "comment": null, "summary": "Large language models (LLMs) are typically trained on fixed snapshots of the\nweb, which means that their knowledge becomes stale and their predictions risk\ntemporal leakage: relying on information that lies in the future relative to a\nquery. We tackle this problem by pre-training from scratch a set of GPT-style\nexperts on disjoint two-year slices of a 2013-2024 corpus and combining them\nthrough TiMoE, a Time-aware Mixture of Language Experts. At inference time,\nTiMoE masks all experts whose training window ends after the query timestamp\nand merges the remaining log-probabilities in a shared space, guaranteeing\nstrict causal validity while retaining the breadth of multi-period knowledge.\nWe also release TSQA, a 10k-question benchmark whose alternatives are\nexplicitly labelled as past, future or irrelevant, allowing fine-grained\nmeasurement of temporal hallucinations. Experiments on eight standard NLP tasks\nplus TSQA show that a co-adapted TiMoE variant matches or exceeds the best\nsingle-period expert and cuts future-knowledge errors by up to 15%. Our results\ndemonstrate that modular, time-segmented pre-training paired with causal\nrouting is a simple yet effective path toward LLMs that stay chronologically\ngrounded without sacrificing general performance much. We open source our code\nat TiMoE (Github): https://github.com/epfml/TiMoE", "AI": {"tldr": "为解决大型语言模型知识过时和时间泄漏问题，本文提出了TiMoE（时间感知语言专家混合模型），通过对不同时间段预训练的专家进行因果路由，实现知识的时效性和准确性。", "motivation": "大型语言模型通常在固定时间点的数据快照上训练，导致其知识过时，且预测可能出现时间泄漏（即在回答查询时使用了未来信息）。", "method": "从零开始预训练一组GPT风格的专家模型，每个专家在2013-2024语料库的不同两年时间切片上训练。通过TiMoE（时间感知语言专家混合模型）将这些专家组合起来。在推理时，TiMoE会屏蔽训练窗口晚于查询时间戳的所有专家，并融合剩余专家的对数概率，以确保严格的因果有效性。同时，发布了TSQA基准测试集，用于精细测量时间幻觉。", "result": "在八个标准NLP任务和TSQA上的实验表明，TiMoE变体性能与最佳单时期专家相当或更优，并将未来知识错误率降低了高达15%。结果证明，模块化、时间分段的预训练结合因果路由是构建时间上更准确且不牺牲通用性能的LLM的简单有效途径。", "conclusion": "模块化、时间分段的预训练结合因果路由是一种简单而有效的方法，能够使大型语言模型在保持通用性能的同时，更好地遵守时间顺序，避免知识过时和时间泄漏问题。"}}
{"id": "2508.08783", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08783", "abs": "https://arxiv.org/abs/2508.08783", "authors": ["Tianyu Xiong", "Dayi Tan", "Wei Tian"], "title": "DiffPose-Animal: A Language-Conditioned Diffusion Framework for Animal Pose Estimation", "comment": "13pages,2figures", "summary": "Animal pose estimation is a fundamental task in computer vision, with growing\nimportance in ecological monitoring, behavioral analysis, and intelligent\nlivestock management. Compared to human pose estimation, animal pose estimation\nis more challenging due to high interspecies morphological diversity, complex\nbody structures, and limited annotated data. In this work, we introduce\nDiffPose-Animal, a novel diffusion-based framework for top-down animal pose\nestimation. Unlike traditional heatmap regression methods, DiffPose-Animal\nreformulates pose estimation as a denoising process under the generative\nframework of diffusion models. To enhance semantic guidance during keypoint\ngeneration, we leverage large language models (LLMs) to extract both global\nanatomical priors and local keypoint-wise semantics based on species-specific\nprompts. These textual priors are encoded and fused with image features via\ncross-attention modules to provide biologically meaningful constraints\nthroughout the denoising process. Additionally, a diffusion-based keypoint\ndecoder is designed to progressively refine pose predictions, improving\nrobustness to occlusion and annotation sparsity. Extensive experiments on\npublic animal pose datasets demonstrate the effectiveness and generalization\ncapability of our method, especially under challenging scenarios with diverse\nspecies, cluttered backgrounds, and incomplete keypoints.", "AI": {"tldr": "提出DiffPose-Animal，一个基于扩散模型的动物姿态估计框架，利用LLM提供语义指导，并通过去噪过程逐步细化姿态预测，有效应对物种多样性、遮挡和数据稀疏等挑战。", "motivation": "动物姿态估计在生态监测、行为分析和智能畜牧管理中日益重要。然而，与人类姿态估计相比，动物姿态估计面临物种间形态多样性高、身体结构复杂以及标注数据有限等挑战。", "method": "提出DiffPose-Animal，一个新颖的基于扩散模型的自上而下动物姿态估计框架。该方法将姿态估计重新定义为去噪过程。它利用大型语言模型（LLMs）从物种特定提示中提取全局解剖先验和局部关键点语义，并通过交叉注意力模块将这些文本先验与图像特征融合，提供生物学上有意义的约束。此外，设计了一个基于扩散的关键点解码器来逐步细化姿态预测。", "result": "在公共动物姿态数据集上的广泛实验表明，该方法在具有多样物种、杂乱背景和不完整关键点的挑战性场景下，表现出有效性和泛化能力，尤其在处理遮挡和标注稀疏性方面具有鲁棒性。", "conclusion": "DiffPose-Animal通过结合扩散模型和LLM的语义指导，为动物姿态估计提供了一种有效且鲁棒的解决方案，尤其擅长处理复杂和数据受限的场景，展现了其在实际应用中的巨大潜力。"}}
{"id": "2508.08997", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08997", "abs": "https://arxiv.org/abs/2508.08997", "authors": ["Sizhe Yuen", "Francisco Gomez Medina", "Ting Su", "Yali Du", "Adam J. Sobey"], "title": "Intrinsic Memory Agents: Heterogeneous Multi-Agent LLM Systems through Structured Contextual Memory", "comment": null, "summary": "Multi-agent systems built on Large Language Models (LLMs) show exceptional\npromise for complex collaborative problem-solving, yet they face fundamental\nchallenges stemming from context window limitations that impair memory\nconsistency, role adherence, and procedural integrity. This paper introduces\nIntrinsic Memory Agents, a novel framework that addresses these limitations\nthrough structured agent-specific memories that evolve intrinsically with agent\noutputs. Specifically, our method maintains role-aligned memory templates that\npreserve specialized perspectives while focusing on task-relevant information.\nWe benchmark our approach on the PDDL dataset, comparing its performance to\nexisting state-of-the-art multi-agentic memory approaches and showing an\nimprovement of 38.6\\% with the highest token efficiency. An additional\nevaluation is performed on a complex data pipeline design task, we demonstrate\nthat our approach produces higher quality designs when comparing 5 metrics:\nscalability, reliability, usability, cost-effectiveness and documentation with\nadditional qualitative evidence of the improvements. Our findings suggest that\naddressing memory limitations through structured, intrinsic approaches can\nimprove the capabilities of multi-agent LLM systems on structured planning\ntasks.", "AI": {"tldr": "本文提出了一种名为“内在记忆代理”（Intrinsic Memory Agents）的新框架，通过结构化、随输出演进的代理特定记忆来解决多智能体LLM系统因上下文窗口限制导致的记忆一致性问题，显著提升了在规划任务上的性能和效率。", "motivation": "多智能体大语言模型（LLM）系统在复杂协作问题解决中展现巨大潜力，但受限于上下文窗口大小，导致记忆一致性、角色遵守和程序完整性方面存在根本性挑战。", "method": "引入“内在记忆代理”框架，通过维护与代理输出内在演进的、结构化的代理特定记忆模板来解决问题。这些模板旨在保留专业视角，同时聚焦于任务相关信息。", "result": "在PDDL数据集上，该方法比现有最先进的多智能体记忆方法性能提升38.6%，并具有最高的token效率。在复杂数据管道设计任务上，该方法在可扩展性、可靠性、可用性、成本效益和文档化5个指标上产生了更高质量的设计，并提供了额外的定性证据。", "conclusion": "通过结构化、内在的方法解决记忆限制，可以显著提高多智能体LLM系统在结构化规划任务上的能力。"}}
{"id": "2508.08833", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.08833", "abs": "https://arxiv.org/abs/2508.08833", "authors": ["Yuren Hao", "Xiang Wan", "Chengxiang Zhai"], "title": "An Investigation of Robustness of LLMs in Mathematical Reasoning: Benchmarking with Mathematically-Equivalent Transformation of Advanced Mathematical Problems", "comment": "16 pages, 8 figures", "summary": "In this paper, we introduce a systematic framework beyond conventional method\nto assess LLMs' mathematical-reasoning robustness by stress-testing them on\nadvanced math problems that are mathematically equivalent but with linguistic\nand parametric variation. These transformations allow us to measure the\nsensitivity of LLMs to non-mathematical perturbations, thereby enabling a more\naccurate evaluation of their mathematical reasoning capabilities. Using this\nnew evaluation methodology, we created PutnamGAP, a new benchmark dataset with\nmultiple mathematically-equivalent variations of competition-level math\nproblems. With the new dataset, we evaluate multiple families of representative\nLLMs and examine their robustness. Across 18 commercial and open-source models\nwe observe sharp performance degradation on the variants. OpenAI's flagship\nreasoning model, O3, scores 49 % on the originals but drops by 4 percentage\npoints on surface variants, and by 10.5 percentage points on core-step-based\nvariants, while smaller models fare far worse. Overall, the results show that\nthe proposed new evaluation methodology is effective for deepening our\nunderstanding of the robustness of LLMs and generating new insights for further\nimproving their mathematical reasoning capabilities.", "AI": {"tldr": "本文提出了一种超越传统方法的系统框架，通过对数学上等价但存在语言和参数变化的进阶数学问题进行压力测试，评估大型语言模型（LLMs）的数学推理鲁棒性。", "motivation": "传统的评估方法无法衡量LLMs对非数学扰动的敏感性，因此无法准确评估其数学推理能力。", "method": "引入了一个系统框架，通过对数学上等价但具有语言和参数变体的进阶数学问题进行压力测试，来评估LLMs的数学推理鲁棒性。基于此方法创建了PutnamGAP数据集，其中包含竞赛级别的数学问题及其多个数学等价变体。使用该数据集评估了18个商业和开源LLMs。", "result": "在变体问题上，所有LLMs都表现出显著的性能下降。OpenAI的旗舰推理模型O3在原始问题上得分为49%，但在表面变体上下降了4个百分点，在基于核心步骤的变体上下降了10.5个百分点，而小型模型表现更差。", "conclusion": "所提出的新评估方法能有效加深对LLMs鲁棒性的理解，并为进一步提升其数学推理能力提供了新见解。"}}
{"id": "2508.08794", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08794", "abs": "https://arxiv.org/abs/2508.08794", "authors": ["Yingxue Pang", "Shijie Zhao", "Mengxi Guo", "Junlin Li", "Li Zhang"], "title": "Region-Adaptive Video Sharpening via Rate-Perception Optimization", "comment": null, "summary": "Sharpening is a widely adopted video enhancement technique. However, uniform\nsharpening intensity ignores texture variations, degrading video quality.\nSharpening also increases bitrate, and there's a lack of techniques to\noptimally allocate these additional bits across diverse regions. Thus, this\npaper proposes RPO-AdaSharp, an end-to-end region-adaptive video sharpening\nmodel for both perceptual enhancement and bitrate savings. We use the coding\ntree unit (CTU) partition mask as prior information to guide and constrain the\nallocation of increased bits. Experiments on benchmarks demonstrate the\neffectiveness of the proposed model qualitatively and quantitatively.", "AI": {"tldr": "本文提出RPO-AdaSharp，一个端到端的区域自适应视频锐化模型，旨在同时提升感知质量并节省比特率。", "motivation": "传统的统一锐化强度忽略纹理变化，导致视频质量下降；同时，锐化会增加比特率，但缺乏有效技术来优化分配这些额外比特到不同区域。", "method": "提出RPO-AdaSharp模型，利用编码树单元（CTU）划分掩码作为先验信息，指导和约束增加比特的分配。", "result": "在基准测试中，所提出的模型在定性和定量上均表现出有效性。", "conclusion": "RPO-AdaSharp模型能够有效地实现视频的区域自适应锐化，同时兼顾感知增强和比特率节省。"}}
{"id": "2508.09019", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09019", "abs": "https://arxiv.org/abs/2508.09019", "authors": ["Shivam Dubey"], "title": "Activation Steering for Bias Mitigation: An Interpretable Approach to Safer LLMs", "comment": null, "summary": "As large language models (LLMs) become more integrated into societal systems,\nthe risk of them perpetuating and amplifying harmful biases becomes a critical\nsafety concern. Traditional methods for mitigating bias often rely on data\nfiltering or post-hoc output moderation, which treat the model as an opaque\nblack box. In this work, we introduce a complete, end-to-end system that uses\ntechniques from mechanistic interpretability to both identify and actively\nmitigate bias directly within a model's internal workings. Our method involves\ntwo primary stages. First, we train linear \"probes\" on the internal activations\nof a model to detect the latent representations of various biases (e.g.,\ngender, race, age). Our experiments on \\texttt{gpt2-large} demonstrate that\nthese probes can identify biased content with near-perfect accuracy, revealing\nthat bias representations become most salient in the model's later layers.\nSecond, we leverage these findings to compute \"steering vectors\" by contrasting\nthe model's activation patterns for biased and neutral statements. By adding\nthese vectors during inference, we can actively steer the model's generative\nprocess away from producing harmful, stereotypical, or biased content in\nreal-time. We demonstrate the efficacy of this activation steering technique,\nshowing that it successfully alters biased completions toward more neutral\nalternatives. We present our work as a robust and reproducible system that\noffers a more direct and interpretable approach to building safer and more\naccountable LLMs.", "AI": {"tldr": "该论文提出一个端到端的系统，利用可解释性技术直接在LLM内部识别和减轻偏见，通过训练探针检测偏见并使用转向向量实时纠正生成内容。", "motivation": "随着大型语言模型（LLMs）日益融入社会系统，它们可能传播和放大有害偏见的风险成为一个关键的安全问题。传统的偏见缓解方法（如数据过滤或后处理）将模型视为黑箱，无法直接干预其内部工作机制。", "method": "该方法包括两个主要阶段：1. 训练线性“探针”来检测模型内部激活中的潜在偏见表示（如性别、种族、年龄），以识别偏见。2. 利用这些发现，通过对比偏见和中性语句的模型激活模式，计算“转向向量”。在推理过程中添加这些向量，以实时引导模型的生成过程，使其远离有害、刻板或有偏见的内容。", "result": "实验在gpt2-large上进行，结果表明：1. 探针能够以接近完美的准确率识别偏见内容，且偏见表示在模型的后期层中最为突出。2. 激活转向技术有效，成功地将有偏见的补全内容转向更中性的替代方案。", "conclusion": "该工作提出了一个健壮且可复现的系统，提供了一种更直接、更可解释的方法来构建更安全、更负责任的LLMs，通过深入模型内部机制来识别和缓解偏见。"}}
{"id": "2508.08846", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08846", "abs": "https://arxiv.org/abs/2508.08846", "authors": ["Afrozah Nadeem", "Mark Dras", "Usman Naseem"], "title": "Steering Towards Fairness: Mitigating Political Bias in LLMs", "comment": "Preprint", "summary": "Recent advancements in large language models (LLMs) have enabled their\nwidespread use across diverse real-world applications. However, concerns remain\nabout their tendency to encode and reproduce ideological biases, particularly\nalong political and economic dimensions. In this paper, we propose a framework\nfor probing and mitigating such biases in decoder-based LLMs through analysis\nof internal model representations. Grounded in the Political Compass Test\n(PCT), our method uses contrastive pairs to extract and compare hidden layer\nactivations from models like Mistral and DeepSeek. We introduce a comprehensive\nactivation extraction pipeline capable of layer-wise analysis across multiple\nideological axes, revealing meaningful disparities linked to political framing.\nOur results show that decoder LLMs systematically encode representational bias\nacross layers, which can be leveraged for effective steering vector-based\nmitigation. This work provides new insights into how political bias is encoded\nin LLMs and offers a principled approach to debiasing beyond surface-level\noutput interventions.", "AI": {"tldr": "本文提出一个框架，通过分析解码器LLM的内部表示，探测并缓解其编码的政治和经济意识形态偏见，并展示了基于激活向量的有效缓解方法。", "motivation": "大型语言模型（LLMs）在广泛应用的同时，存在编码和再现意识形态偏见的倾向，尤其是在政治和经济维度上，这引发了担忧。", "method": "该方法基于“政治罗盘测试”（Political Compass Test, PCT），使用对比对来提取并比较Mistral和DeepSeek等模型隐藏层的激活。研究者引入了一个全面的激活提取流程，能够进行跨多个意识形态轴的逐层分析，以揭示与政治框架相关的显著差异。同时，利用所发现的偏见进行基于转向向量（steering vector）的缓解。", "result": "结果显示，解码器LLMs在不同层系统性地编码了表示偏见，这些偏见可以被有效利用进行基于转向向量的缓解。", "conclusion": "这项工作深入揭示了政治偏见在LLMs中如何被编码，并提供了一种超越表面输出干预的、有原则的去偏方法。"}}
{"id": "2508.08798", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08798", "abs": "https://arxiv.org/abs/2508.08798", "authors": ["Yao Lu", "Jiawei Li", "Ming Jiang"], "title": "MonoPartNeRF:Human Reconstruction from Monocular Video via Part-Based Neural Radiance Fields", "comment": null, "summary": "In recent years, Neural Radiance Fields (NeRF) have achieved remarkable\nprogress in dynamic human reconstruction and rendering. Part-based rendering\nparadigms, guided by human segmentation, allow for flexible parameter\nallocation based on structural complexity, thereby enhancing representational\nefficiency. However, existing methods still struggle with complex pose\nvariations, often producing unnatural transitions at part boundaries and\nfailing to reconstruct occluded regions accurately in monocular settings. We\npropose MonoPartNeRF, a novel framework for monocular dynamic human rendering\nthat ensures smooth transitions and robust occlusion recovery. First, we build\na bidirectional deformation model that combines rigid and non-rigid\ntransformations to establish a continuous, reversible mapping between\nobservation and canonical spaces. Sampling points are projected into a\nparameterized surface-time space (u, v, t) to better capture non-rigid motion.\nA consistency loss further suppresses deformation-induced artifacts and\ndiscontinuities. We introduce a part-based pose embedding mechanism that\ndecomposes global pose vectors into local joint embeddings based on body\nregions. This is combined with keyframe pose retrieval and interpolation, along\nthree orthogonal directions, to guide pose-aware feature sampling. A learnable\nappearance code is integrated via attention to model dynamic texture changes\neffectively. Experiments on the ZJU-MoCap and MonoCap datasets demonstrate that\nour method significantly outperforms prior approaches under complex pose and\nocclusion conditions, achieving superior joint alignment, texture fidelity, and\nstructural continuity.", "AI": {"tldr": "MonoPartNeRF是一种用于单目动态人体渲染的新框架，通过结合双向变形模型、局部姿态嵌入和学习外观编码，解决了复杂姿态和遮挡下的重建挑战，实现了平滑过渡和鲁棒的遮挡恢复。", "motivation": "现有的人体动态NeRF方法在处理复杂姿态变化时表现不佳，常在身体部位边界产生不自然过渡，并且在单目设置下难以准确重建被遮挡区域。", "method": "MonoPartNeRF提出：1) 建立一个结合刚性和非刚性变换的双向变形模型，在观察空间和规范空间之间建立连续可逆映射，并将采样点投影到参数化的表面-时间空间(u, v, t)以捕捉非刚性运动，辅以一致性损失。2) 引入基于身体区域的局部关节嵌入的部件级姿态嵌入机制，结合关键帧姿态检索和三正交方向的插值来指导姿态感知特征采样。3) 通过注意力机制整合可学习的外观编码，有效建模动态纹理变化。", "result": "在ZJU-MoCap和MonoCap数据集上的实验表明，MonoPartNeRF在复杂姿态和遮挡条件下显著优于现有方法，实现了卓越的关节对齐、纹理保真度和结构连续性。", "conclusion": "MonoPartNeRF通过其创新的框架，成功解决了单目动态人体渲染中复杂姿态和遮挡的挑战，展现了优越的重建和渲染能力。"}}
{"id": "2508.09027", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09027", "abs": "https://arxiv.org/abs/2508.09027", "authors": ["Jie Wang", "Guang Wang"], "title": "A First Look at Predictability and Explainability of Pre-request Passenger Waiting Time in Ridesharing Systems", "comment": null, "summary": "Passenger waiting time prediction plays a critical role in enhancing both\nridesharing user experience and platform efficiency. While most existing\nresearch focuses on post-request waiting time prediction with knowing the\nmatched driver information, pre-request waiting time prediction (i.e., before\nsubmitting a ride request and without matching a driver) is also important, as\nit enables passengers to plan their trips more effectively and enhance the\nexperience of both passengers and drivers. However, it has not been fully\nstudied by existing works. In this paper, we take the first step toward\nunderstanding the predictability and explainability of pre-request passenger\nwaiting time in ridesharing systems. Particularly, we conduct an in-depth\ndata-driven study to investigate the impact of demand&supply dynamics on\npassenger waiting time. Based on this analysis and feature engineering, we\npropose FiXGBoost, a novel feature interaction-based XGBoost model designed to\npredict waiting time without knowing the assigned driver information. We\nfurther perform an importance analysis to quantify the contribution of each\nfactor. Experiments on a large-scale real-world ridesharing dataset including\nover 30 million trip records show that our FiXGBoost can achieve a good\nperformance for pre-request passenger waiting time prediction with high\nexplainability.", "AI": {"tldr": "本研究首次探讨了网约车系统中预请求乘客等待时间的可预测性和可解释性，并提出了基于特征交互的XGBoost模型FiXGBoost，在真实大规模数据集上表现出良好的预测性能和高可解释性。", "motivation": "现有研究大多关注已知匹配司机信息的请求后等待时间预测，而预请求等待时间（即在提交乘车请求和匹配司机之前）的预测对乘客规划行程和提升用户体验同样重要，但尚未得到充分研究。", "method": "通过深入的数据驱动研究，分析了供需动态对乘客等待时间的影响。在此分析和特征工程的基础上，提出了FiXGBoost模型，一个新颖的基于特征交互的XGBoost模型，用于在未知分配司机信息的情况下预测等待时间。此外，还进行了重要性分析以量化各因素的贡献。", "result": "在包含超过3000万条行程记录的大规模真实网约车数据集上的实验表明，FiXGBoost模型在预请求乘客等待时间预测方面取得了良好性能，并具有高可解释性。", "conclusion": "该研究为理解网约车系统中预请求乘客等待时间的可预测性和可解释性迈出了第一步，并提出了一种有效且可解释的模型FiXGBoost，有助于提升网约车用户体验和平台效率。"}}
{"id": "2508.08855", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.08855", "abs": "https://arxiv.org/abs/2508.08855", "authors": ["Sekh Mainul Islam", "Nadav Borenstein", "Siddhesh Milind Pawar", "Haeun Yu", "Arnav Arora", "Isabelle Augenstein"], "title": "BiasGym: Fantastic Biases and How to Find (and Remove) Them", "comment": "Under review", "summary": "Understanding biases and stereotypes encoded in the weights of Large Language\nModels (LLMs) is crucial for developing effective mitigation strategies. Biased\nbehaviour is often subtle and non-trivial to isolate, even when deliberately\nelicited, making systematic analysis and debiasing particularly challenging. To\naddress this, we introduce BiasGym, a simple, cost-effective, and generalizable\nframework for reliably injecting, analyzing, and mitigating conceptual\nassociations within LLMs. BiasGym consists of two components: BiasInject, which\ninjects specific biases into the model via token-based fine-tuning while\nkeeping the model frozen, and BiasScope, which leverages these injected signals\nto identify and steer the components responsible for biased behavior. Our\nmethod enables consistent bias elicitation for mechanistic analysis, supports\ntargeted debiasing without degrading performance on downstream tasks, and\ngeneralizes to biases unseen during training. We demonstrate the effectiveness\nof BiasGym in reducing real-world stereotypes (e.g., people from a country\nbeing `reckless drivers') and in probing fictional associations (e.g., people\nfrom a country having `blue skin'), showing its utility for both safety\ninterventions and interpretability research.", "AI": {"tldr": "BiasGym是一个通用框架，用于可靠地注入、分析和缓解大型语言模型（LLM）中的概念关联偏见，旨在实现系统性分析和去偏。", "motivation": "LLM中编码的偏见行为通常微妙且难以隔离，即使故意引发也很难，这使得系统分析和去偏特别具有挑战性。", "method": "BiasGym包含两部分：BiasInject通过基于token的微调将特定偏见注入冻结的模型中；BiasScope利用这些注入的信号来识别和引导负责偏见行为的组件。", "result": "该方法能够实现一致的偏见引出，进行机制分析；支持有针对性的去偏，且不降低下游任务性能；并能泛化到训练期间未见的偏见。BiasGym在减少真实世界刻板印象和探索虚构关联方面均显示出有效性。", "conclusion": "BiasGym框架对LLM的安全干预和可解释性研究具有实用价值。"}}
{"id": "2508.08808", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08808", "abs": "https://arxiv.org/abs/2508.08808", "authors": ["Luis S. Luevano", "Pavel Korshunov", "Sebastien Marcel"], "title": "Identity-Preserving Aging and De-Aging of Faces in the StyleGAN Latent Space", "comment": "Accepted for publication in IEEE International Joint Conference on\n  Biometrics (IJCB), 2025", "summary": "Face aging or de-aging with generative AI has gained significant attention\nfor its applications in such fields like forensics, security, and media.\nHowever, most state of the art methods rely on conditional Generative\nAdversarial Networks (GANs), Diffusion-based models, or Visual Language Models\n(VLMs) to age or de-age faces based on predefined age categories and\nconditioning via loss functions, fine-tuning, or text prompts. The reliance on\nsuch conditioning leads to complex training requirements, increased data needs,\nand challenges in generating consistent results. Additionally, identity\npreservation is rarely taken into accountor evaluated on a single face\nrecognition system without any control or guarantees on whether identity would\nbe preserved in a generated aged/de-aged face. In this paper, we propose to\nsynthesize aged and de-aged faces via editing latent space of StyleGAN2 using a\nsimple support vector modeling of aging/de-aging direction and several feature\nselection approaches. By using two state-of-the-art face recognition systems,\nwe empirically find the identity preserving subspace within the StyleGAN2\nlatent space, so that an apparent age of a given face can changed while\npreserving the identity. We then propose a simple yet practical formula for\nestimating the limits on aging/de-aging parameters that ensures identity\npreservation for a given input face. Using our method and estimated parameters\nwe have generated a public dataset of synthetic faces at different ages that\ncan be used for benchmarking cross-age face recognition, age assurance systems,\nor systems for detection of synthetic images. Our code and dataset are\navailable at the project page https://www.idiap.ch/paper/agesynth/", "AI": {"tldr": "本文提出了一种通过编辑StyleGAN2潜在空间来实现面部年龄变化（老化/去老化）的方法，该方法通过寻找身份保持子空间来确保身份不被改变，并提供了一个估算参数限制的公式。", "motivation": "现有的面部年龄变化方法（如基于GAN、Diffusion、VLM）通常依赖于复杂的条件设定（损失函数、微调、文本提示），导致训练复杂、数据需求高且结果一致性差。此外，这些方法很少充分考虑或保证身份的保留。", "method": "该研究通过以下方式实现：1) 使用简单的支持向量建模和特征选择方法，编辑StyleGAN2的潜在空间以实现老化/去老化方向；2) 利用两个最先进的人脸识别系统，经验性地在StyleGAN2潜在空间中找到身份保持子空间；3) 提出了一个简单实用的公式，用于估算确保身份保持的年龄变化参数限制。", "result": "研究成功生成了在保持身份的同时改变年龄的合成人脸。基于此方法和估算的参数，研究团队创建了一个公共合成人脸数据集，可用于跨年龄人脸识别、年龄验证系统或合成图像检测系统的基准测试。", "conclusion": "该方法提供了一种简单而实用的面部年龄变化解决方案，有效解决了现有方法的复杂性和身份保留问题，并生成了一个有价值的公共数据集，可推动相关领域的研究和应用。"}}
{"id": "2508.09054", "categories": ["cs.AI", "cs.LG", "68T07, 68T05", "I.2.6; I.5.1; I.5.4"], "pdf": "https://arxiv.org/pdf/2508.09054", "abs": "https://arxiv.org/abs/2508.09054", "authors": ["Debdeep Mukherjee", "Eduardo Di Santi", "Clément Lefebvre", "Nenad Mijatovic", "Victor Martin", "Thierry Josse", "Jonathan Brown", "Kenza Saiah"], "title": "CVCM Track Circuits Pre-emptive Failure Diagnostics for Predictive Maintenance Using Deep Neural Networks", "comment": "Peer-reviewed conference paper. Presented at ICROMA 2025\n  (International Conference on Railway Operations Modelling and Analysis),\n  Dresden, Germany. https://tu-dresden.de/raildresden2025 8 pages, 6 figures, 1\n  table", "summary": "Track circuits are critical for railway operations, acting as the main\nsignalling sub-system to locate trains. Continuous Variable Current Modulation\n(CVCM) is one such technology. Like any field-deployed, safety-critical asset,\nit can fail, triggering cascading disruptions. Many failures originate as\nsubtle anomalies that evolve over time, often not visually apparent in\nmonitored signals. Conventional approaches, which rely on clear signal changes,\nstruggle to detect them early. Early identification of failure types is\nessential to improve maintenance planning, minimising downtime and revenue\nloss. Leveraging deep neural networks, we propose a predictive maintenance\nframework that classifies anomalies well before they escalate into failures.\nValidated on 10 CVCM failure cases across different installations, the method\nis ISO-17359 compliant and outperforms conventional techniques, achieving\n99.31% overall accuracy with detection within 1% of anomaly onset. Through\nconformal prediction, we provide uncertainty estimates, reaching 99% confidence\nwith consistent coverage across classes. Given CVCMs global deployment, the\napproach is scalable and adaptable to other track circuits and railway systems,\nenhancing operational reliability.", "AI": {"tldr": "该研究提出一个基于深度神经网络的预测性维护框架，用于早期检测轨道电路（特别是CVCM技术）的微妙异常，以防止故障升级，提高铁路运营可靠性。", "motivation": "轨道电路作为列车定位的关键信号子系统，其故障会引发连锁中断。许多故障源于随时间演变的微妙异常，传统方法难以早期发现。因此，需要早期识别故障类型以改进维护规划，减少停机时间和收入损失。", "method": "研究提出了一个利用深度神经网络的预测性维护框架，用于在异常升级为故障之前对其进行分类。该方法通过一致性预测提供不确定性估计。", "result": "该方法在10个CVCM故障案例上进行了验证，符合ISO-17359标准，并优于传统技术，实现了99.31%的总体准确率，能在异常发生后1%的时间内检测到。一致性预测达到了99%的置信度，且各类别覆盖率一致。", "conclusion": "该方法具有可扩展性，可适应其他轨道电路和铁路系统，显著提高运营可靠性。通过早期、准确地检测轨道电路异常，有助于改进维护，减少中断。"}}
{"id": "2508.08876", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.08876", "abs": "https://arxiv.org/abs/2508.08876", "authors": ["Kaiyu Wang", "Lin Mu", "Zhiyao Yang", "Ximing Li", "Xiaotang Zhou Wanfu Gao", "Huimao Zhang"], "title": "Weakly Supervised Fine-grained Span-Level Framework for Chinese Radiology Report Quality Assurance", "comment": "Accepted by CIKM 2025. 11 pages, 7 figures", "summary": "Quality Assurance (QA) for radiology reports refers to judging whether the\njunior reports (written by junior doctors) are qualified. The QA scores of one\njunior report are given by the senior doctor(s) after reviewing the image and\njunior report. This process requires intensive labor costs for senior doctors.\nAdditionally, the QA scores may be inaccurate for reasons like diagnosis bias,\nthe ability of senior doctors, and so on. To address this issue, we propose a\nSpan-level Quality Assurance EvaluaTOR (Sqator) to mark QA scores\nautomatically. Unlike the common document-level semantic comparison method, we\ntry to analyze the semantic difference by exploring more fine-grained text\nspans. Unlike the common document-level semantic comparison method, we try to\nanalyze the semantic difference by exploring more fine-grained text spans.\nSpecifically, Sqator measures QA scores by measuring the importance of revised\nspans between junior and senior reports, and outputs the final QA scores by\nmerging all revised span scores. We evaluate Sqator using a collection of\n12,013 radiology reports. Experimental results show that Sqator can achieve\ncompetitive QA scores. Moreover, the importance scores of revised spans can be\nalso consistent with the judgments of senior doctors.", "AI": {"tldr": "提出Sqator模型，通过分析初级和高级放射报告之间的修订文本片段（span-level）来自动评估放射报告的质量分数。", "motivation": "放射报告的质量保证（QA）目前依赖高级医生手动审查，这耗费大量人力且可能因诊断偏差或医生能力等因素导致评分不准确。", "method": "Sqator（Span-level Quality Assurance EvaluaTOR）通过测量初级报告和高级报告之间修订片段的重要性来计算QA分数，然后合并所有修订片段的分数以输出最终QA分数。这与常见的文档级语义比较方法不同，它着重于更细粒度的文本片段分析。", "result": "在包含12,013份放射报告的数据集上进行评估，Sqator能够获得具有竞争力的QA分数。此外，修订片段的重要性分数与高级医生的判断一致。", "conclusion": "Sqator模型能够有效地自动化放射报告的质量保证过程，减轻高级医生的工作负担，并提高QA评分的准确性和一致性。"}}
{"id": "2508.08811", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08811", "abs": "https://arxiv.org/abs/2508.08811", "authors": ["Shi-Chen Zhang", "Yunheng Li", "Yu-Huan Wu", "Qibin Hou", "Ming-Ming Cheng"], "title": "Revisiting Efficient Semantic Segmentation: Learning Offsets for Better Spatial and Class Feature Alignment", "comment": "Accepted at ICCV 2025. Project page:\n  https://github.com/HVision-NKU/OffSeg", "summary": "Semantic segmentation is fundamental to vision systems requiring pixel-level\nscene understanding, yet deploying it on resource-constrained devices demands\nefficient architectures. Although existing methods achieve real-time inference\nthrough lightweight designs, we reveal their inherent limitation: misalignment\nbetween class representations and image features caused by a per-pixel\nclassification paradigm. With experimental analysis, we find that this paradigm\nresults in a highly challenging assumption for efficient scenarios: Image pixel\nfeatures should not vary for the same category in different images. To address\nthis dilemma, we propose a coupled dual-branch offset learning paradigm that\nexplicitly learns feature and class offsets to dynamically refine both class\nrepresentations and spatial image features. Based on the proposed paradigm, we\nconstruct an efficient semantic segmentation network, OffSeg. Notably, the\noffset learning paradigm can be adopted to existing methods with no additional\narchitectural changes. Extensive experiments on four datasets, including\nADE20K, Cityscapes, COCO-Stuff-164K, and Pascal Context, demonstrate consistent\nimprovements with negligible parameters. For instance, on the ADE20K dataset,\nour proposed offset learning paradigm improves SegFormer-B0, SegNeXt-T, and\nMask2Former-Tiny by 2.7%, 1.9%, and 2.6% mIoU, respectively, with only 0.1-0.2M\nadditional parameters required.", "AI": {"tldr": "该论文提出了一种双分支偏移学习范式，通过动态调整类别表示和图像特征来解决语义分割中存在的特征与类别表示不对齐问题，显著提升了现有方法的性能且只增加了极少量参数。", "motivation": "现有轻量级语义分割方法虽然实现了实时推理，但其逐像素分类范式导致类别表示与图像特征之间存在错位。具体来说，该范式隐含了一个具有挑战性的假设：同一类别在不同图像中的像素特征应保持不变，这在高效场景下难以实现。", "method": "为解决上述问题，论文提出了一种耦合双分支偏移学习范式，该范式显式学习特征和类别偏移，以动态地细化类别表示和空间图像特征。基于此范式，构建了一个高效的语义分割网络OffSeg。该偏移学习范式可以无缝集成到现有方法中，无需额外的架构修改。", "result": "在ADE20K、Cityscapes、COCO-Stuff-164K和Pascal Context四个数据集上进行了广泛实验，结果表明该方法带来了持续的性能提升，且仅增加了可忽略的参数。例如，在ADE20K数据集上，所提出的偏移学习范式使SegFormer-B0、SegNeXt-T和Mask2Former-Tiny的mIoU分别提高了2.7%、1.9%和2.6%，而仅需额外增加0.1-0.2M参数。", "conclusion": "所提出的耦合双分支偏移学习范式有效解决了高效语义分割中特征与类别表示不对齐的固有局限性，通过动态精炼机制，在保持轻量化的同时显著提升了分割性能，并具有良好的通用性，可应用于现有多种模型。"}}
{"id": "2508.09105", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09105", "abs": "https://arxiv.org/abs/2508.09105", "authors": ["Shixuan Sun", "Siyuan Liang", "Ruoyu Chen", "Jianjie Huang", "Jingzhi Li", "Xiaochun Cao"], "title": "SMA: Who Said That? Auditing Membership Leakage in Semi-Black-box RAG Controlling", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) and its Multimodal Retrieval-Augmented\nGeneration (MRAG) significantly improve the knowledge coverage and contextual\nunderstanding of Large Language Models (LLMs) by introducing external knowledge\nsources. However, retrieval and multimodal fusion obscure content provenance,\nrendering existing membership inference methods unable to reliably attribute\ngenerated outputs to pre-training, external retrieval, or user input, thus\nundermining privacy leakage accountability\n  To address these challenges, we propose the first Source-aware Membership\nAudit (SMA) that enables fine-grained source attribution of generated content\nin a semi-black-box setting with retrieval control capabilities.To address the\nenvironmental constraints of semi-black-box auditing, we further design an\nattribution estimation mechanism based on zero-order optimization, which\nrobustly approximates the true influence of input tokens on the output through\nlarge-scale perturbation sampling and ridge regression modeling. In addition,\nSMA introduces a cross-modal attribution technique that projects image inputs\ninto textual descriptions via MLLMs, enabling token-level attribution in the\ntext modality, which for the first time facilitates membership inference on\nimage retrieval traces in MRAG systems. This work shifts the focus of\nmembership inference from 'whether the data has been memorized' to 'where the\ncontent is sourced from', offering a novel perspective for auditing data\nprovenance in complex generative systems.", "AI": {"tldr": "针对RAG/MRAG中内容来源不透明导致隐私泄露问责困难的问题，提出首个源感知成员审计（SMA）方法，实现细粒度内容归因，并首次在MRAG中对图像检索痕迹进行成员推断。", "motivation": "检索增强生成（RAG）和多模态RAG（MRAG）虽能提升大型语言模型（LLM）的知识覆盖和上下文理解，但其检索和多模态融合过程模糊了内容来源。现有成员推断方法无法可靠地将生成内容归因于预训练、外部检索或用户输入，从而阻碍了隐私泄露的问责。", "method": "本文提出了首个源感知成员审计（SMA）方法，用于在具有检索控制能力的半黑盒设置中实现生成内容的细粒度来源归因。为适应半黑盒审计环境，设计了一种基于零阶优化的归因估计机制，通过大规模扰动采样和岭回归建模，稳健地近似输入token对输出的真实影响。此外，SMA引入了一种跨模态归因技术，通过多模态LLM（MLLM）将图像输入投影到文本描述，从而在文本模态中实现token级归因，首次实现了对MRAG系统中图像检索痕迹的成员推断。", "result": "SMA首次实现了对MRAG系统中图像检索痕迹的成员推断，并将成员推断的关注点从“数据是否被记忆”转移到“内容来源于何处”。", "conclusion": "SMA为复杂生成系统的数据溯源审计提供了一个新颖的视角，有效解决了RAG/MRAG系统中内容来源不明导致隐私泄露问责困难的挑战。"}}
{"id": "2508.08879", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08879", "abs": "https://arxiv.org/abs/2508.08879", "authors": ["Haeun Yu", "Seogyeong Jeong", "Siddhesh Pawar", "Jisu Shin", "Jiho Jin", "Junho Myung", "Alice Oh", "Isabelle Augenstein"], "title": "Entangled in Representations: Mechanistic Investigation of Cultural Biases in Large Language Models", "comment": "16 pages, 7 figures", "summary": "The growing deployment of large language models (LLMs) across diverse\ncultural contexts necessitates a better understanding of how the\novergeneralization of less documented cultures within LLMs' representations\nimpacts their cultural understanding. Prior work only performs extrinsic\nevaluation of LLMs' cultural competence, without accounting for how LLMs'\ninternal mechanisms lead to cultural (mis)representation. To bridge this gap,\nwe propose Culturescope, the first mechanistic interpretability-based method\nthat probes the internal representations of LLMs to elicit the underlying\ncultural knowledge space. CultureScope utilizes a patching method to extract\nthe cultural knowledge. We introduce a cultural flattening score as a measure\nof the intrinsic cultural biases. Additionally, we study how LLMs internalize\nWestern-dominance bias and cultural flattening, which allows us to trace how\ncultural biases emerge within LLMs. Our experimental results reveal that LLMs\nencode Western-dominance bias and cultural flattening in their cultural\nknowledge space. We find that low-resource cultures are less susceptible to\ncultural biases, likely due to their limited training resources. Our work\nprovides a foundation for future research on mitigating cultural biases and\nenhancing LLMs' cultural understanding. Our codes and data used for experiments\nare publicly available.", "AI": {"tldr": "该研究提出了CultureScope，一种基于机械可解释性的方法，用于探测大型语言模型（LLMs）的内部表示，以理解其文化知识空间中的偏见，特别是西方主导偏见和文化扁平化。", "motivation": "随着LLMs在全球范围内的广泛部署，需要更好地理解其对欠发达文化的过度概括如何影响文化理解。现有工作仅进行LLMs文化能力的外部评估，未能解释LLMs内部机制如何导致文化（错误）表征。", "method": "提出了CultureScope，首个基于机械可解释性的方法，通过补丁技术提取LLMs的内部文化知识。引入了“文化扁平化分数”来衡量内在文化偏见。此外，研究了LLMs如何内化西方主导偏见和文化扁平化，以追踪文化偏见在LLMs内部的产生。", "result": "实验结果显示，LLMs在其文化知识空间中编码了西方主导偏见和文化扁平化。发现低资源文化对文化偏见的敏感度较低，这可能归因于其有限的训练资源。", "conclusion": "该工作为未来减轻文化偏见和增强LLMs文化理解的研究奠定了基础。"}}
{"id": "2508.08812", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08812", "abs": "https://arxiv.org/abs/2508.08812", "authors": ["Yuqi Peng", "Lingtao Zheng", "Yufeng Yang", "Yi Huang", "Mingfu Yan", "Jianzhuang Liu", "Shifeng Chen"], "title": "TARA: Token-Aware LoRA for Composable Personalization in Diffusion Models", "comment": null, "summary": "Personalized text-to-image generation aims to synthesize novel images of a\nspecific subject or style using only a few reference images. Recent methods\nbased on Low-Rank Adaptation (LoRA) enable efficient single-concept\ncustomization by injecting lightweight, concept-specific adapters into\npre-trained diffusion models. However, combining multiple LoRA modules for\nmulti-concept generation often leads to identity missing and visual feature\nleakage. In this work, we identify two key issues behind these failures: (1)\ntoken-wise interference among different LoRA modules, and (2) spatial\nmisalignment between the attention map of a rare token and its corresponding\nconcept-specific region. To address these issues, we propose Token-Aware LoRA\n(TARA), which introduces a token mask to explicitly constrain each module to\nfocus on its associated rare token to avoid interference, and a training\nobjective that encourages the spatial attention of a rare token to align with\nits concept region. Our method enables training-free multi-concept composition\nby directly injecting multiple independently trained TARA modules at inference\ntime. Experimental results demonstrate that TARA enables efficient\nmulti-concept inference and effectively preserving the visual identity of each\nconcept by avoiding mutual interference between LoRA modules. The code and\nmodels are available at https://github.com/YuqiPeng77/TARA.", "AI": {"tldr": "TARA通过引入token掩码和空间对齐训练，解决了多概念LoRA文本到图像生成中概念丢失和特征泄露的问题，实现了高效且保真的多概念组合。", "motivation": "现有的基于LoRA的个性化文本到图像生成方法在单概念定制上表现良好，但将多个LoRA模块组合进行多概念生成时，常导致概念身份丢失和视觉特征泄露。作者识别出两个关键问题：不同LoRA模块间的token干扰和稀有token注意力图与对应概念区域的空间错位。", "method": "本文提出了Token-Aware LoRA (TARA) 方法。它引入了一个token掩码，明确约束每个模块专注于其关联的稀有token，以避免干扰。同时，设计了一个训练目标，鼓励稀有token的空间注意力与其概念区域对齐。", "result": "实验结果表明，TARA能够在推理时直接注入多个独立训练的TARA模块，实现无需额外训练的多概念组合。该方法有效地保留了每个概念的视觉身份，避免了LoRA模块间的相互干扰，实现了高效的多概念推理。", "conclusion": "TARA通过解决LoRA模块间的token干扰和空间错位问题，显著提升了多概念文本到图像生成的质量和效率，有效解决了现有方法的缺陷，能更好地保留每个概念的视觉身份。"}}
{"id": "2508.09123", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09123", "abs": "https://arxiv.org/abs/2508.09123", "authors": ["Xinyuan Wang", "Bowen Wang", "Dunjie Lu", "Junlin Yang", "Tianbao Xie", "Junli Wang", "Jiaqi Deng", "Xiaole Guo", "Yiheng Xu", "Chen Henry Wu", "Zhennan Shen", "Zhuokai Li", "Ryan Li", "Xiaochuan Li", "Junda Chen", "Boyuan Zheng", "Peihang Li", "Fangyu Lei", "Ruisheng Cao", "Yeqiao Fu", "Dongchan Shin", "Martin Shin", "Jiarui Hu", "Yuyan Wang", "Jixuan Chen", "Yuxiao Ye", "Danyang Zhang", "Dikang Du", "Hao Hu", "Huarong Chen", "Zaida Zhou", "Yipu Wang", "Heng Wang", "Diyi Yang", "Victor Zhong", "Flood Sung", "Y. Charles", "Zhilin Yang", "Tao Yu"], "title": "OpenCUA: Open Foundations for Computer-Use Agents", "comment": null, "summary": "Vision-language models have demonstrated impressive capabilities as\ncomputer-use agents (CUAs) capable of automating diverse computer tasks. As\ntheir commercial potential grows, critical details of the most capable CUA\nsystems remain closed. As these agents will increasingly mediate digital\ninteractions and execute consequential decisions on our behalf, the research\ncommunity needs access to open CUA frameworks to study their capabilities,\nlimitations, and risks. To bridge this gap, we propose OpenCUA, a comprehensive\nopen-source framework for scaling CUA data and foundation models. Our framework\nconsists of: (1) an annotation infrastructure that seamlessly captures human\ncomputer-use demonstrations; (2) AgentNet, the first large-scale computer-use\ntask dataset spanning 3 operating systems and 200+ applications and websites;\n(3) a scalable pipeline that transforms demonstrations into state-action pairs\nwith reflective long Chain-of-Thought reasoning that sustain robust performance\ngains as data scales. Our end-to-end agent models demonstrate strong\nperformance across CUA benchmarks. In particular, OpenCUA-32B achieves an\naverage success rate of 34.8% on OSWorld-Verified, establishing a new\nstate-of-the-art (SOTA) among open-source models and surpassing OpenAI CUA\n(GPT-4o). Further analysis confirms that our approach generalizes well across\ndomains and benefits significantly from increased test-time computation. We\nrelease our annotation tool, datasets, code, and models to build open\nfoundations for further CUA research.", "AI": {"tldr": "OpenCUA是一个开源的计算机使用代理（CUA）框架，旨在解决现有CUA系统不透明的问题。它包含数据标注工具、大规模数据集AgentNet和可扩展的数据处理流程，其模型在CUA基准测试中表现出色，超越了现有开源和部分闭源模型，并已开源以推动研究。", "motivation": "当前功能强大的计算机使用代理（CUA）系统大多是闭源的，阻碍了研究社区对其能力、局限性和风险的深入研究。随着这些代理在数字交互和决策中的作用日益增强，迫切需要开放的CUA框架。", "method": "该研究提出了OpenCUA框架，包括：1) 一个无缝捕获人类计算机使用演示的标注基础设施；2) AgentNet，首个涵盖3个操作系统和200多个应用/网站的大规模计算机使用任务数据集；3) 一个可扩展的管道，将演示转化为带有反思性长思维链推理的状态-动作对，以在数据扩展时保持性能提升。", "result": "OpenCUA的端到端代理模型在CUA基准测试中表现强劲。特别是，OpenCUA-32B在OSWorld-Verified上实现了34.8%的平均成功率，在开源模型中建立了新的SOTA，并超越了OpenAI CUA (GPT-4o)。研究还证实该方法具有良好的跨领域泛化能力，并能从增加的测试时计算中显著获益。", "conclusion": "OpenCUA框架为计算机使用代理研究提供了开放的基础。研究团队已发布其标注工具、数据集、代码和模型，以促进未来的CUA研究和发展。"}}
{"id": "2508.08895", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08895", "abs": "https://arxiv.org/abs/2508.08895", "authors": ["Keyu Chen", "Zhifeng Shen", "Daohai Yu", "Haoqian Wu", "Wei Wen", "Jianfeng He", "Ruizhi Qiao", "Xing Sun"], "title": "ASPD: Unlocking Adaptive Serial-Parallel Decoding by Exploring Intrinsic Parallelism in LLMs", "comment": "20 pages, 9 figures", "summary": "The increasing scale and complexity of large language models (LLMs) pose\nsignificant inference latency challenges, primarily due to their autoregressive\ndecoding paradigm characterized by the sequential nature of next-token\nprediction. By re-examining the outputs of autoregressive models, we observed\nthat some segments exhibit parallelizable structures, which we term intrinsic\nparallelism. Decoding each parallelizable branch simultaneously (i.e. parallel\ndecoding) can significantly improve the overall inference speed of LLMs. In\nthis paper, we propose an Adaptive Serial-Parallel Decoding (ASPD), which\naddresses two core challenges: automated construction of parallelizable data\nand efficient parallel decoding mechanism. More specifically, we introduce a\nnon-invasive pipeline that automatically extracts and validates parallelizable\nstructures from the responses of autoregressive models. To empower efficient\nadaptive serial-parallel decoding, we implement a Hybrid Decoding Engine which\nenables seamless transitions between serial and parallel decoding modes while\nmaintaining a reusable KV cache, maximizing computational efficiency. Extensive\nevaluations across General Tasks, Retrieval-Augmented Generation, Mathematical\nReasoning, demonstrate that ASPD achieves unprecedented performance in both\neffectiveness and efficiency. Notably, on Vicuna Bench, our method achieves up\nto 3.19x speedup (1.85x on average) while maintaining response quality within\n1% difference compared to autoregressive models, realizing significant\nacceleration without compromising generation quality. Our framework sets a\ngroundbreaking benchmark for efficient LLM parallel inference, paving the way\nfor its deployment in latency-sensitive applications such as AI-powered\ncustomer service bots and answer retrieval engines.", "AI": {"tldr": "提出一种自适应串行-并行解码（ASPD）方法，通过利用LLM输出中的内在并行性，显著提高推理速度，同时保持生成质量。", "motivation": "大型语言模型（LLM）的自回归解码范式导致高推理延迟，限制了其在延迟敏感应用中的部署。", "method": "提出ASPD框架，解决并行数据构建和高效并行解码两大挑战。通过非侵入式管道自动提取和验证并行结构，并实现混合解码引擎，支持串行和并行模式间的无缝切换，同时维护可复用的KV缓存。", "result": "在通用任务、检索增强生成和数学推理等多个任务上，ASPD实现了显著的性能提升。在Vicuna Bench上，速度提升高达3.19倍（平均1.85倍），同时响应质量与自回归模型相比仅有1%的差异。", "conclusion": "ASPD为高效LLM并行推理设立了新基准，为LLM在客户服务机器人和答案检索引擎等延迟敏感应用中的部署铺平了道路。"}}
{"id": "2508.08821", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08821", "abs": "https://arxiv.org/abs/2508.08821", "authors": ["Noor Ahmed", "Cameron Braunstein", "Steffen Eger", "Eddy Ilg"], "title": "3DFroMLLM: 3D Prototype Generation only from Pretrained Multimodal LLMs", "comment": null, "summary": "Recent Multi-Modal Large Language Models (MLLMs) have demonstrated strong\ncapabilities in learning joint representations from text and images. However,\ntheir spatial reasoning remains limited. We introduce 3DFroMLLM, a novel\nframework that enables the generation of 3D object prototypes directly from\nMLLMs, including geometry and part labels. Our pipeline is agentic, comprising\na designer, coder, and visual inspector operating in a refinement loop.\nNotably, our approach requires no additional training data or detailed user\ninstructions. Building on prior work in 2D generation, we demonstrate that\nrendered images produced by our framework can be effectively used for image\nclassification pretraining tasks and outperforms previous methods by 15%. As a\ncompelling real-world use case, we show that the generated prototypes can be\nleveraged to improve fine-grained vision-language models by using the rendered,\npart-labeled prototypes to fine-tune CLIP for part segmentation and achieving a\n55% accuracy improvement without relying on any additional human-labeled data.", "AI": {"tldr": "3DFroMLLM是一个新颖的框架，无需额外训练或详细指令，即可使多模态大语言模型（MLLMs）直接生成包含几何和部件标签的3D对象原型，并通过代理式精炼循环提高MLLMs的空间推理能力，同时在图像分类预训练和细粒度视觉-语言模型（部件分割）任务中展现出显著性能提升。", "motivation": "尽管最近的多模态大语言模型（MLLMs）在学习文本和图像的联合表示方面表现出强大的能力，但它们的空间推理能力仍然有限。本研究旨在解决这一限制，使MLLMs能够直接生成3D对象原型。", "method": "引入了3DFroMLLM框架，该框架通过一个代理式管道实现3D对象原型生成，包括一个设计师、一个编码器和一个视觉检查器，它们在一个精炼循环中操作。该方法无需额外的训练数据或详细的用户指令，并基于先前在2D生成方面的工作。", "result": "通过3DFroMLLM框架生成的渲染图像可以有效地用于图像分类预训练任务，并比现有方法性能提高15%。此外，生成的原型可以用于改进细粒度视觉-语言模型，通过使用渲染的、带有部件标签的原型来微调CLIP进行部件分割，实现了55%的精度提升，且不依赖任何额外的人工标注数据。", "conclusion": "3DFroMLLM成功地使MLLMs能够生成3D对象原型，有效提升了它们的空间推理能力。该框架在无需额外数据或训练的情况下，为图像分类预训练和细粒度视觉-语言模型（特别是部件分割）提供了显著的性能改进，展示了其强大的实用性。"}}
{"id": "2508.09129", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09129", "abs": "https://arxiv.org/abs/2508.09129", "authors": ["Xianghe Pang", "Shuo Tang", "Rui Ye", "Yuwen Du", "Yaxin Du", "Siheng Chen"], "title": "BrowseMaster: Towards Scalable Web Browsing via Tool-Augmented Programmatic Agent Pair", "comment": null, "summary": "Effective information seeking in the vast and ever-growing digital landscape\nrequires balancing expansive search with strategic reasoning. Current large\nlanguage model (LLM)-based agents struggle to achieve this balance due to\nlimitations in search breadth and reasoning depth, where slow, serial querying\nrestricts coverage of relevant sources and noisy raw inputs disrupt the\ncontinuity of multi-step reasoning. To address these challenges, we propose\nBrowseMaster, a scalable framework built around a programmatically augmented\nplanner-executor agent pair. The planner formulates and adapts search\nstrategies based on task constraints, while the executor conducts efficient,\ntargeted retrieval to supply the planner with concise, relevant evidence. This\ndivision of labor preserves coherent, long-horizon reasoning while sustaining\nbroad and systematic exploration, overcoming the trade-off that limits existing\nagents. Extensive experiments on challenging English and Chinese benchmarks\nshow that BrowseMaster consistently outperforms open-source and proprietary\nbaselines, achieving scores of 30.0 on BrowseComp-en and 46.5 on BrowseComp-zh,\nwhich demonstrates its strong capability in complex, reasoning-heavy\ninformation-seeking tasks at scale.", "AI": {"tldr": "BrowseMaster是一个可扩展的框架，通过结合规划器和执行器代理，解决了LLM代理在信息搜索中广度与深度平衡的挑战，实现了高效的探索和连贯的推理。", "motivation": "当前的LLM代理在广阔的数字信息搜索中难以平衡广泛搜索与策略推理，因为串行查询限制了覆盖范围，且原始输入噪音干扰了多步推理的连续性。", "method": "提出BrowseMaster框架，包含程序增强的规划器-执行器代理对。规划器根据任务制定和调整搜索策略，执行器进行高效、有针对性的检索，为规划器提供简洁相关证据，从而实现广阔探索和连贯推理的分工。", "result": "在具有挑战性的英文和中文基准测试（BrowseComp-en和BrowseComp-zh）上，BrowseMaster始终优于开源和专有基线，分别获得30.0和46.5的分数。", "conclusion": "BrowseMaster在复杂的、重推理的信息搜索任务中表现出强大的能力，有效克服了现有代理在搜索广度和推理深度之间的权衡限制。"}}
{"id": "2508.08912", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08912", "abs": "https://arxiv.org/abs/2508.08912", "authors": ["Mahmoud Salhab", "Shameed Sait", "Mohammad Abusheikh", "Hasan Abusheikh"], "title": "Munsit at NADI 2025 Shared Task 2: Pushing the Boundaries of Multidialectal Arabic ASR with Weakly Supervised Pretraining and Continual Supervised Fine-tuning", "comment": null, "summary": "Automatic speech recognition (ASR) plays a vital role in enabling natural\nhuman-machine interaction across applications such as virtual assistants,\nindustrial automation, customer support, and real-time transcription. However,\ndeveloping accurate ASR systems for low-resource languages like Arabic remains\na significant challenge due to limited labeled data and the linguistic\ncomplexity introduced by diverse dialects. In this work, we present a scalable\ntraining pipeline that combines weakly supervised learning with supervised\nfine-tuning to develop a robust Arabic ASR model. In the first stage, we\npretrain the model on 15,000 hours of weakly labeled speech covering both\nModern Standard Arabic (MSA) and various Dialectal Arabic (DA) variants. In the\nsubsequent stage, we perform continual supervised fine-tuning using a mixture\nof filtered weakly labeled data and a small, high-quality annotated dataset.\nOur approach achieves state-of-the-art results, ranking first in the\nmulti-dialectal Arabic ASR challenge. These findings highlight the\neffectiveness of weak supervision paired with fine-tuning in overcoming data\nscarcity and delivering high-quality ASR for low-resource, dialect-rich\nlanguages.", "AI": {"tldr": "本文提出了一种可扩展的训练流程，结合弱监督学习和监督微调，为低资源、方言丰富的阿拉伯语开发了鲁棒的自动语音识别（ASR）模型，并在多方言阿拉伯语ASR挑战赛中取得了最先进的结果。", "motivation": "自动语音识别在人机交互中至关重要，但为阿拉伯语等低资源语言开发准确的ASR系统面临挑战，原因在于标注数据有限以及方言多样性带来的语言复杂性。", "method": "该方法采用两阶段训练流程：首先，模型在15,000小时包含现代标准阿拉伯语和各种方言阿拉伯语的弱标注语音数据上进行预训练；随后，使用过滤后的弱标注数据和少量高质量标注数据集进行持续的监督微调。", "result": "该方法取得了最先进的结果，在多方言阿拉伯语ASR挑战赛中排名第一。", "conclusion": "研究结果表明，弱监督与微调相结合的方法在克服数据稀缺性方面是有效的，能为低资源、方言丰富的语言提供高质量的ASR系统。"}}
{"id": "2508.08824", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08824", "abs": "https://arxiv.org/abs/2508.08824", "authors": ["Diego Frias"], "title": "A Parametric Bi-Directional Curvature-Based Framework for Image Artifact Classification and Quantification", "comment": null, "summary": "This work presents a novel framework for No-Reference Image Quality\nAssessment (NR-IQA) founded on the analysis of directional image curvature.\nWithin this framework, we define a measure of Anisotropic Texture Richness\n(ATR), which is computed at the pixel level using two tunable thresholds -- one\npermissive and one restrictive -- that quantify orthogonal texture suppression.\nWhen its parameters are optimized for a specific artifact, the resulting ATR\nscore serves as a high-performance quality metric, achieving Spearman\ncorrelations with human perception of approximately -0.93 for Gaussian blur and\n-0.95 for white noise on the LIVE dataset. The primary contribution is a\ntwo-stage system that leverages the differential response of ATR to various\ndistortions. First, the system utilizes the signature from two specialist ATR\nconfigurations to classify the primary artifact type (blur vs. noise) with over\n97% accuracy. Second, following classification, it employs a dedicated\nregression model mapping the relevant ATR score to a quality rating to quantify\nthe degradation. On a combined dataset, the complete system predicts human\nscores with a coefficient of determination (R2) of 0.892 and a Root Mean Square\nError (RMSE) of 5.17 DMOS points. This error corresponds to just 7.4% of the\ndataset's total quality range, demonstrating high predictive accuracy. This\nestablishes our framework as a robust, dual-purpose tool for the classification\nand subsequent quantification of image degradation.", "AI": {"tldr": "该工作提出了一种基于方向图像曲率分析的无参考图像质量评估（NR-IQA）新框架，通过各向异性纹理丰富度（ATR）度量，实现了图像失真的分类和量化。", "motivation": "现有图像质量评估方法可能不足以准确或鲁棒地处理不同类型的图像失真，因此需要开发一种高性能、能同时分类和量化图像降级的新型NR-IQA框架。", "method": "该方法基于方向图像曲率分析，定义了像素级的各向异性纹理丰富度（ATR），使用两个可调阈值量化正交纹理抑制。系统分为两阶段：首先，利用两种专业ATR配置的特征识别主要失真类型（模糊或噪声），准确率超过97%；其次，根据分类结果，采用专门的回归模型将相关ATR得分映射到质量等级以量化图像降级。", "result": "优化参数后，ATR得分作为质量度量与人类感知的斯皮尔曼相关性，在高斯模糊下约为-0.93，白噪声下约为-0.95（在LIVE数据集上）。在组合数据集上，完整系统预测人类得分的决定系数（R2）为0.892，均方根误差（RMSE）为5.17 DMOS点，相当于数据集总质量范围的7.4%，显示出高预测准确性。", "conclusion": "该框架被确立为一个鲁棒的双重用途工具，可用于图像降级的分类和随后的量化，具有高预测精度。"}}
{"id": "2508.08933", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.08933", "abs": "https://arxiv.org/abs/2508.08933", "authors": ["Khondoker Ittehadul Islam", "Gabriele Sarti"], "title": "Reveal-Bangla: A Dataset for Cross-Lingual Multi-Step Reasoning Evaluation", "comment": "Submitted to IJCNLP-AACL 2025", "summary": "Language models have demonstrated remarkable performance on complex\nmulti-step reasoning tasks. However, their evaluation has been predominantly\nconfined to high-resource languages such as English. In this paper, we\nintroduce a manually translated Bangla multi-step reasoning dataset derived\nfrom the English Reveal dataset, featuring both binary and non-binary question\ntypes. We conduct a controlled evaluation of English-centric and Bangla-centric\nmultilingual small language models on the original dataset and our translated\nversion to compare their ability to exploit relevant reasoning steps to produce\ncorrect answers. Our results show that, in comparable settings, reasoning\ncontext is beneficial for more challenging non-binary questions, but models\nstruggle to employ relevant Bangla reasoning steps effectively. We conclude by\nexploring how reasoning steps contribute to models' predictions, highlighting\ndifferent trends across models and languages.", "AI": {"tldr": "该研究评估了多语言小型语言模型在孟加拉语多步推理任务上的表现，发现推理上下文对更复杂的非二元问题有益，但模型难以有效利用孟加拉语推理步骤。", "motivation": "当前语言模型在复杂多步推理任务上的评估主要集中在高资源语言（如英语），而对其他语言（特别是低资源语言）的评估不足。", "method": "研究构建了一个人工翻译的孟加拉语多步推理数据集（源自英文Reveal数据集，包含二元和非二元问题），并在此数据集和原始英文数据集上，对以英语为中心和以孟加拉语为中心的多语言小型语言模型进行了受控评估。", "result": "在可比较的设置下，推理上下文对更具挑战性的非二元问题是有益的。然而，模型在有效利用相关的孟加拉语推理步骤方面存在困难。", "conclusion": "推理上下文对非二元问题有益，但当前模型在孟加拉语推理步骤的利用上表现不佳。研究还揭示了推理步骤对模型预测的贡献在不同模型和语言之间存在差异。"}}
{"id": "2508.08849", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08849", "abs": "https://arxiv.org/abs/2508.08849", "authors": ["Yingxue Pang", "Shijie Zhao", "Junlin Li", "Li Zhang"], "title": "Adaptive High-Frequency Preprocessing for Video Coding", "comment": null, "summary": "High-frequency components are crucial for maintaining video clarity and\nrealism, but they also significantly impact coding bitrate, resulting in\nincreased bandwidth and storage costs. This paper presents an end-to-end\nlearning-based framework for adaptive high-frequency preprocessing to enhance\nsubjective quality and save bitrate in video coding. The framework employs the\nFrequency-attentive Feature pyramid Prediction Network (FFPN) to predict the\noptimal high-frequency preprocessing strategy, guiding subsequent filtering\noperators to achieve the optimal tradeoff between bitrate and quality after\ncompression. For training FFPN, we pseudo-label each training video with the\noptimal strategy, determined by comparing the rate-distortion (RD) performance\nacross different preprocessing types and strengths. Distortion is measured\nusing the latest quality assessment metric. Comprehensive evaluations on\nmultiple datasets demonstrate the visually appealing enhancement capabilities\nand bitrate savings achieved by our framework.", "AI": {"tldr": "本文提出一个端到端的学习框架，用于自适应高频预处理，以在视频编码中提升主观质量并节省码率。", "motivation": "高频分量对视频清晰度和真实感至关重要，但显著增加编码码率，导致带宽和存储成本上升。", "method": "该框架采用频率注意力特征金字塔预测网络（FFPN）来预测最优的高频预处理策略，指导后续滤波操作。FFPN通过比较不同预处理类型和强度的率失真（RD）性能来伪标记训练视频，失真使用最新的质量评估指标衡量。", "result": "在多个数据集上的综合评估表明，该框架实现了视觉上吸引人的质量提升和码率节省。", "conclusion": "所提出的学习框架能够有效地自适应预处理高频分量，从而在视频编码中实现码率与质量之间的最佳权衡。"}}
{"id": "2508.08940", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.08940", "abs": "https://arxiv.org/abs/2508.08940", "authors": ["Hasan Abed Al Kader Hammoud", "Kumail Alhamoud", "Abed Hammoud", "Elie Bou-Zeid", "Marzyeh Ghassemi", "Bernard Ghanem"], "title": "Train Long, Think Short: Curriculum Learning for Efficient Reasoning", "comment": "Under Review", "summary": "Recent work on enhancing the reasoning abilities of large language models\n(LLMs) has introduced explicit length control as a means of constraining\ncomputational cost while preserving accuracy. However, existing approaches rely\non fixed-length training budgets, which do not take advantage of the natural\nprogression from exploration to compression during learning. In this work, we\npropose a curriculum learning strategy for length-controlled reasoning using\nGroup Relative Policy Optimization (GRPO). Our method starts with generous\ntoken budgets and gradually tightens them over training, encouraging models to\nfirst discover effective solution strategies and then distill them into more\nconcise reasoning traces. We augment GRPO with a reward function that balances\nthree signals: task correctness (via verifier feedback), length efficiency, and\nformatting adherence (via structural tags). Experiments on GSM8K, MATH500,\nSVAMP, College Math, and GSM+ demonstrate that curriculum-based training\nconsistently outperforms fixed-budget baselines at the same final budget,\nachieving higher accuracy and significantly improved token efficiency. We\nfurther ablate the impact of reward weighting and decay schedule design,\nshowing that progressive constraint serves as a powerful inductive bias for\ntraining efficient reasoning models. Our code and checkpoints are released at:\nhttps://github.com/hammoudhasan/curriculum_grpo.", "AI": {"tldr": "本文提出一种基于课程学习的GRPO（Group Relative Policy Optimization）方法，用于控制大型语言模型（LLMs）的推理长度。通过在训练过程中逐步收紧token预算，该方法能有效提高LLM在数学推理任务上的准确性和token效率，优于固定预算的基线方法。", "motivation": "现有LLM推理的长度控制方法依赖于固定长度的训练预算，这未能充分利用学习过程中从探索到压缩的自然演进，导致在控制计算成本的同时保持准确性方面存在局限性。", "method": "作者提出一种基于GRPO的课程学习策略。训练初期，模型被给予宽松的token预算，鼓励其探索有效的解决方案；随后预算逐渐收紧，促使模型将解决方案提炼为更简洁的推理轨迹。此外，GRPO通过一个奖励函数进行增强，该函数平衡了三个信号：任务正确性（通过验证器反馈）、长度效率和格式依从性（通过结构化标签）。", "result": "在GSM8K、MATH500、SVAMP、College Math和GSM+等数据集上的实验表明，在相同的最终预算下，基于课程学习的训练方法始终优于固定预算的基线方法，实现了更高的准确性和显著提升的token效率。研究还探讨了奖励权重和衰减策略设计的影响。", "conclusion": "渐进式约束（即课程学习）是训练高效推理模型的一种强大归纳偏置。"}}
{"id": "2508.08867", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08867", "abs": "https://arxiv.org/abs/2508.08867", "authors": ["Lin Zeng", "Boming Zhao", "Jiarui Hu", "Xujie Shen", "Ziqiang Dang", "Hujun Bao", "Zhaopeng Cui"], "title": "GaussianUpdate: Continual 3D Gaussian Splatting Update for Changing Environments", "comment": "Accepted to ICCV 2025", "summary": "Novel view synthesis with neural models has advanced rapidly in recent years,\nyet adapting these models to scene changes remains an open problem. Existing\nmethods are either labor-intensive, requiring extensive model retraining, or\nfail to capture detailed types of changes over time. In this paper, we present\nGaussianUpdate, a novel approach that combines 3D Gaussian representation with\ncontinual learning to address these challenges. Our method effectively updates\nthe Gaussian radiance fields with current data while preserving information\nfrom past scenes. Unlike existing methods, GaussianUpdate explicitly models\ndifferent types of changes through a novel multi-stage update strategy.\nAdditionally, we introduce a visibility-aware continual learning approach with\ngenerative replay, enabling self-aware updating without the need to store\nimages. The experiments on the benchmark dataset demonstrate our method\nachieves superior and real-time rendering with the capability of visualizing\nchanges over different times", "AI": {"tldr": "GaussianUpdate是一种新颖的方法，结合3D高斯表示和持续学习，实现了神经视图合成模型对场景变化的有效适应，并能实时渲染和可视化不同时间的变化。", "motivation": "现有神经视图合成模型难以适应场景变化，需要大量重新训练或无法捕捉细节变化。", "method": "提出了GaussianUpdate方法，结合3D高斯表示和持续学习，通过多阶段更新策略显式建模不同类型的变化，并引入了带有生成式回放的可见性感知持续学习，无需存储图像即可实现自我感知更新。", "result": "在基准数据集上，该方法实现了卓越的实时渲染，并能可视化不同时间点的场景变化。", "conclusion": "GaussianUpdate能有效利用当前数据更新高斯辐射场，同时保留过去场景信息，明确建模变化类型，并实现无需存储图像的自我感知更新。"}}
{"id": "2508.08942", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.08942", "abs": "https://arxiv.org/abs/2508.08942", "authors": ["Lucas Albarede", "Jose Moreno", "Lynda Tamine", "Luce Lefeuvre"], "title": "Jointly Generating and Attributing Answers using Logits of Document-Identifier Tokens", "comment": null, "summary": "Despite their impressive performances, Large Language Models (LLMs) remain\nprone to hallucination, which critically undermines their trustworthiness.\nWhile most of the previous work focused on tackling answer and attribution\ncorrectness, a recent line of work investigated faithfulness, with a focus on\nleveraging internal model signals to reflect a model's actual decision-making\nprocess while generating the answer. Nevertheless, these methods induce\nadditional latency and have shown limitations in directly aligning token\ngeneration with attribution generation. In this paper, we introduce LoDIT, a\nmethod that jointly generates and faithfully attributes answers in RAG by\nleveraging specific token logits during generation. It consists of two steps:\n(1) marking the documents with specific token identifiers and then leveraging\nthe logits of these tokens to estimate the contribution of each document to the\nanswer during generation, and (2) aggregating these contributions into document\nattributions. Experiments on a trustworthiness-focused attributed\ntext-generation benchmark, Trust-Align, show that LoDIT significantly\noutperforms state-of-the-art models on several metrics. Finally, an in-depth\nanalysis of LoDIT shows both its efficiency in terms of latency and its\nrobustness in different settings.", "AI": {"tldr": "LoDIT是一种新方法，通过在生成过程中利用特定token的logits，在RAG中同时生成答案并进行可信归因，显著优于现有技术。", "motivation": "尽管大型语言模型（LLMs）表现出色，但其幻觉问题严重损害了可信度。现有归因方法侧重于答案和归因的正确性，或利用模型内部信号反映决策过程，但存在额外延迟且难以直接对齐token生成与归因生成。", "method": "LoDIT方法分为两步：1) 使用特定token标识符标记文档，并在生成答案时利用这些token的logits来估计每个文档对答案的贡献；2) 将这些贡献聚合为文档归因。", "result": "在Trust-Align可信度归因文本生成基准测试中，LoDIT在多项指标上显著优于现有最先进模型。此外，LoDIT在延迟方面表现出高效率，并在不同设置下展现出鲁棒性。", "conclusion": "LoDIT通过在生成过程中联合生成答案和进行可信归因，有效解决了LLM的幻觉问题，并在性能、效率和鲁棒性方面均表现出色。"}}
{"id": "2508.08891", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08891", "abs": "https://arxiv.org/abs/2508.08891", "authors": ["Chaoyi Wang", "Yifan Yang", "Jun Pei", "Lijie Xia", "Jianpo Liu", "Xiaobing Yuan", "Xinhan Di"], "title": "Preview WB-DH: Towards Whole Body Digital Human Bench for the Generation of Whole-body Talking Avatar Videos", "comment": "This paper has been accepted by ICCV 2025 Workshop MMFM4", "summary": "Creating realistic, fully animatable whole-body avatars from a single\nportrait is challenging due to limitations in capturing subtle expressions,\nbody movements, and dynamic backgrounds. Current evaluation datasets and\nmetrics fall short in addressing these complexities. To bridge this gap, we\nintroduce the Whole-Body Benchmark Dataset (WB-DH), an open-source, multi-modal\nbenchmark designed for evaluating whole-body animatable avatar generation. Key\nfeatures include: (1) detailed multi-modal annotations for fine-grained\nguidance, (2) a versatile evaluation framework, and (3) public access to the\ndataset and tools at https://github.com/deepreasonings/WholeBodyBenchmark.", "AI": {"tldr": "该论文引入了一个名为WB-DH的开放获取、多模态基准数据集，旨在解决从单张肖像生成逼真、全身可动画虚拟形象的评估挑战。", "motivation": "从单张肖像创建逼真、全身可动画的虚拟形象面临捕捉细微表情、身体动作和动态背景的挑战，且现有评估数据集和指标无法充分应对这些复杂性。", "method": "引入了全身基准数据集（WB-DH），一个开源、多模态基准，具有详细的多模态标注以实现细粒度指导，并提供了一个多功能评估框架。", "result": "提供了WB-DH数据集和相关工具的公共访问权限，旨在弥补当前评估方法的不足，并支持全身可动画虚拟形象的生成研究。", "conclusion": "WB-DH数据集通过提供详细标注和多功能评估框架，旨在弥合全身可动画虚拟形象生成领域在评估方面的差距。"}}
{"id": "2508.09001", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.09001", "abs": "https://arxiv.org/abs/2508.09001", "authors": ["Seonghwan Choi", "Beomseok Kang", "Dongwon Jo", "Jae-Joon Kim"], "title": "Retrospective Sparse Attention for Efficient Long-Context Generation", "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed in long-context tasks\nsuch as reasoning, code generation, and multi-turn dialogue. However, inference\nover extended contexts is bottlenecked by the Key-Value (KV) cache, whose\nmemory footprint grows linearly with sequence length and dominates latency at\neach decoding step. While recent KV cache compression methods identify and load\nimportant tokens, they focus predominantly on input contexts and fail to\naddress the cumulative attention errors that arise during long decoding. In\nthis paper, we introduce RetroAttention, a novel KV cache update technique that\nretrospectively revises past attention outputs using newly arrived KV entries\nfrom subsequent decoding steps. By maintaining a lightweight output cache,\nRetroAttention enables past queries to efficiently access more relevant\ncontext, while incurring minimal latency overhead. This breaks the\nfixed-attention-output paradigm and allows continual correction of prior\napproximations. Extensive experiments on long-generation benchmarks show that\nRetroAttention consistently outperforms state-of-the-art (SOTA) KV compression\nmethods, increasing effective KV exposure by up to 1.6$\\times$ and accuracy by\nup to 21.9\\%.", "AI": {"tldr": "RetroAttention是一种新的KV缓存更新技术，通过回顾性修正过去的注意力输出，显著提高了长文本生成任务中LLM的性能和准确性。", "motivation": "大型语言模型（LLMs）在长上下文任务中面临KV缓存瓶颈，其内存占用随序列长度线性增长，并在解码步骤中导致延迟。现有KV缓存压缩方法主要关注输入上下文，未能解决长解码过程中累积的注意力错误。", "method": "本文提出了RetroAttention，一种新颖的KV缓存更新技术。它通过利用后续解码步骤中新到达的KV条目，回顾性地修订过去的注意力输出。通过维护一个轻量级的输出缓存，RetroAttention使过去的查询能够有效地访问更相关的上下文，同时引入最小的延迟开销。这打破了固定注意力输出的范式，并允许持续修正先前的近似值。", "result": "在长文本生成基准测试上的广泛实验表明，RetroAttention始终优于最先进的（SOTA）KV压缩方法，将有效KV暴露度提高了1.6倍，准确性提高了21.9%。", "conclusion": "RetroAttention通过允许对过去注意力输出进行持续修正，有效解决了长解码中的累积注意力错误，显著提升了LLMs在长上下文生成任务中的性能和准确性。"}}
{"id": "2508.08900", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08900", "abs": "https://arxiv.org/abs/2508.08900", "authors": ["Noor Islam S. Mohammad"], "title": "A Robust Epipolar-Domain Regularization Algorithm for Light Field Depth Estimation", "comment": null, "summary": "Robust depth estimation in light field imaging remains a critical challenge\nfor pattern recognition applications such as augmented reality, biomedical\nimaging, and scene reconstruction. While existing approaches often rely heavily\non deep convolutional neural networks, they tend to incur high computational\ncosts and struggle in noisy real-world environments. This paper proposes a\nnovel lightweight depth estimation pipeline that integrates light field-based\ndisparity information with a directed random walk refinement algorithm. Unlike\ntraditional CNN-based methods, our approach enhances depth map consistency\nwithout requiring extensive training or large-scale datasets. The proposed\nmethod was evaluated on the 4D Light Field Benchmark dataset and a diverse set\nof real-world images. Experimental results indicate that while performance\nslightly declines under uncontrolled conditions, the algorithm consistently\nmaintains low computational complexity and competitive accuracy compared to\nstate-of-the-art deep learning models. These findings highlight the potential\nof our method as a robust and efficient alternative for depth estimation and\nsegmentation in light field imaging. The work provides insights into practical\nalgorithm design for light field-based pattern recognition and opens new\ndirections for integrating probabilistic graph models with depth sensing\nframeworks.", "AI": {"tldr": "本论文提出一种轻量级光场深度估计算法，通过结合光场视差信息和有向随机游走精炼，实现了低计算复杂度和与现有深度学习模型相当的精度，是光场深度估计和分割的有效替代方案。", "motivation": "光场成像中的鲁棒深度估计是模式识别应用（如增强现实、生物医学成像、场景重建）的关键挑战。现有深度卷积神经网络方法计算成本高，且在真实噪声环境下表现不佳。", "method": "本文提出一种新颖的轻量级深度估计管线，将基于光场的视差信息与有向随机游走精炼算法相结合。该方法无需大量训练或大型数据集即可增强深度图的一致性。", "result": "该方法在4D光场基准数据集和多样化的真实世界图像上进行了评估。实验结果表明，尽管在不受控条件下性能略有下降，但算法始终保持低计算复杂度和与最先进深度学习模型相当的竞争力精度。", "conclusion": "研究结果突出了该方法作为光场成像中深度估计和分割的鲁棒高效替代方案的潜力。本工作为光场模式识别的实际算法设计提供了见解，并为概率图模型与深度传感框架的集成开辟了新方向。"}}
{"id": "2508.09012", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.09012", "abs": "https://arxiv.org/abs/2508.09012", "authors": ["Adrián Gude", "Roi Santos-Ríos", "Francisco Prado-Valiño", "Ana Ezquerro", "Jesús Vilares"], "title": "LyS at SemEval 2025 Task 8: Zero-Shot Code Generation for Tabular QA", "comment": "Accepted to SemEval 2025. Camera-ready version", "summary": "This paper describes our participation in SemEval 2025 Task 8, focused on\nTabular Question Answering. We developed a zero-shot pipeline that leverages an\nLarge Language Model to generate functional code capable of extracting the\nrelevant information from tabular data based on an input question. Our approach\nconsists of a modular pipeline where the main code generator module is\nsupported by additional components that identify the most relevant columns and\nanalyze their data types to improve extraction accuracy. In the event that the\ngenerated code fails, an iterative refinement process is triggered,\nincorporating the error feedback into a new generation prompt to enhance\nrobustness. Our results show that zero-shot code generation is a valid approach\nfor Tabular QA, achieving rank 33 of 53 in the test phase despite the lack of\ntask-specific fine-tuning.", "AI": {"tldr": "本文描述了在SemEval 2025任务8（表格问答）中，通过零样本LLM生成代码来提取表格信息的方法，并在没有特定任务微调的情况下取得了中等排名。", "motivation": "参与SemEval 2025任务8（表格问答），旨在探索大型语言模型在零样本设置下解决表格问答的能力。", "method": "开发了一个零样本流水线，利用大型语言模型生成功能代码，从表格数据中提取信息。该方法包括一个主代码生成模块，并辅以识别相关列和分析数据类型的组件，以提高准确性。若生成代码失败，会触发迭代细化过程，将错误反馈融入新的生成提示中以增强鲁棒性。", "result": "在测试阶段，该方法在53支队伍中排名第33位，表明零样本代码生成是表格问答的有效方法，尽管缺乏任务特定的微调。", "conclusion": "零样本代码生成是解决表格问答问题的一种有效且具有鲁棒性的方法。"}}
{"id": "2508.09016", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.09016", "abs": "https://arxiv.org/abs/2508.09016", "authors": ["Birong Pan", "Yongqi Li", "Weiyu Zhang", "Wenpeng Lu", "Mayi Xu", "Shen Zhou", "Yuanyuan Zhu", "Ming Zhong", "Tieyun Qian"], "title": "A Survey on Training-free Alignment of Large Language Models", "comment": null, "summary": "The alignment of large language models (LLMs) aims to ensure their outputs\nadhere to human values, ethical standards, and legal norms. Traditional\nalignment methods often rely on resource-intensive fine-tuning (FT), which may\nsuffer from knowledge degradation and face challenges in scenarios where the\nmodel accessibility or computational resources are constrained. In contrast,\ntraining-free (TF) alignment techniques--leveraging in-context learning,\ndecoding-time adjustments, and post-generation corrections--offer a promising\nalternative by enabling alignment without heavily retraining LLMs, making them\nadaptable to both open-source and closed-source environments. This paper\npresents the first systematic review of TF alignment methods, categorizing them\nby stages of pre-decoding, in-decoding, and post-decoding. For each stage, we\nprovide a detailed examination from the viewpoint of LLMs and multimodal LLMs\n(MLLMs), highlighting their mechanisms and limitations. Furthermore, we\nidentify key challenges and future directions, paving the way for more\ninclusive and effective TF alignment techniques. By synthesizing and organizing\nthe rapidly growing body of research, this survey offers a guidance for\npractitioners and advances the development of safer and more reliable LLMs.", "AI": {"tldr": "本文首次系统综述了大型语言模型（LLMs）和多模态LLMs（MLLMs）的免训练（TF）对齐方法，按预解码、解码中和后解码阶段进行分类，并探讨了其机制、局限性、挑战和未来方向。", "motivation": "传统LLMs对齐方法（如微调）资源消耗大、可能导致知识退化，且在模型可访问性或计算资源受限时面临挑战。免训练对齐技术提供了一种无需大量再训练即可实现对齐的替代方案，适用于开源和闭源环境。", "method": "本文对免训练对齐方法进行了首次系统综述，将其分为预解码、解码中和后解码三个阶段。针对每个阶段，从LLMs和MLLMs的角度详细检查了其机制和局限性。此外，还识别了关键挑战和未来发展方向。", "result": "本文提供了首个关于免训练对齐方法的系统综述，成功将其按阶段分类，并详细阐述了各类方法在LLMs和MLLMs中的机制与局限性。同时，明确了该领域面临的主要挑战和未来的研究方向。", "conclusion": "免训练对齐技术是实现更安全、更可靠LLMs的重要途径。本综述通过系统梳理和组织现有研究，为实践者提供了指导，并推动了TF对齐技术的发展。"}}
{"id": "2508.08910", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08910", "abs": "https://arxiv.org/abs/2508.08910", "authors": ["Bin Ren", "Xiaoshui Huang", "Mengyuan Liu", "Hong Liu", "Fabio Poiesi", "Nicu Sebe", "Guofeng Mei"], "title": "Masked Clustering Prediction for Unsupervised Point Cloud Pre-training", "comment": "3D point cloud pretraining method. 8 pages in the main manuscript", "summary": "Vision transformers (ViTs) have recently been widely applied to 3D point\ncloud understanding, with masked autoencoding as the predominant pre-training\nparadigm. However, the challenge of learning dense and informative semantic\nfeatures from point clouds via standard ViTs remains underexplored. We propose\nMaskClu, a novel unsupervised pre-training method for ViTs on 3D point clouds\nthat integrates masked point modeling with clustering-based learning. MaskClu\nis designed to reconstruct both cluster assignments and cluster centers from\nmasked point clouds, thus encouraging the model to capture dense semantic\ninformation. Additionally, we introduce a global contrastive learning mechanism\nthat enhances instance-level feature learning by contrasting different masked\nviews of the same point cloud. By jointly optimizing these complementary\nobjectives, i.e., dense semantic reconstruction, and instance-level contrastive\nlearning. MaskClu enables ViTs to learn richer and more semantically meaningful\nrepresentations from 3D point clouds. We validate the effectiveness of our\nmethod via multiple 3D tasks, including part segmentation, semantic\nsegmentation, object detection, and classification, where MaskClu sets new\ncompetitive results. The code and models will be released\nat:https://github.com/Amazingren/maskclu.", "AI": {"tldr": "MaskClu是一种新颖的无监督预训练方法，用于3D点云上的Vision Transformer (ViT)，它结合了掩码点建模和聚类学习，并引入全局对比学习，旨在捕获密集语义和实例级特征，在多项3D任务中取得了有竞争力的结果。", "motivation": "尽管掩码自编码是3D点云ViT预训练的主流范式，但通过标准ViT从点云中学习密集且信息丰富的语义特征的挑战仍未得到充分探索。", "method": "MaskClu集成了掩码点建模与基于聚类的学习，通过重建聚类分配和聚类中心来捕获密集语义信息。此外，它引入了全局对比学习机制，通过对比同一点云的不同掩码视图来增强实例级特征学习。这两种互补目标被联合优化。", "result": "MaskClu使ViT能够从3D点云中学习到更丰富、更具语义意义的表示。在多个3D任务（包括部件分割、语义分割、目标检测和分类）中，MaskClu取得了新的有竞争力的结果。", "conclusion": "MaskClu通过其新颖的无监督预训练方法，成功解决了3D点云ViT学习密集语义特征的挑战，从而在各种下游任务中显著提升了性能。"}}
{"id": "2508.09022", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09022", "abs": "https://arxiv.org/abs/2508.09022", "authors": ["Zhiqiang Yang", "Renshuai Tao", "Xiaolong Zheng", "Guodong Yang", "Chunjie Zhang"], "title": "When Deepfakes Look Real: Detecting AI-Generated Faces with Unlabeled Data due to Annotation Challenges", "comment": "10pages,5figures", "summary": "Existing deepfake detection methods heavily depend on labeled training data.\nHowever, as AI-generated content becomes increasingly realistic, even\n\\textbf{human annotators struggle to distinguish} between deepfakes and\nauthentic images. This makes the labeling process both time-consuming and less\nreliable. Specifically, there is a growing demand for approaches that can\neffectively utilize large-scale unlabeled data from online social networks.\nUnlike typical unsupervised learning tasks, where categories are distinct,\nAI-generated faces closely mimic real image distributions and share strong\nsimilarities, causing performance drop in conventional strategies. In this\npaper, we introduce the Dual-Path Guidance Network (DPGNet), to tackle two key\nchallenges: (1) bridging the domain gap between faces from different generation\nmodels, and (2) utilizing unlabeled image samples. The method features two core\nmodules: text-guided cross-domain alignment, which uses learnable prompts to\nunify visual and textual embeddings into a domain-invariant feature space, and\ncurriculum-driven pseudo label generation, which dynamically exploit more\ninformative unlabeled samples. To prevent catastrophic forgetting, we also\nfacilitate bridging between domains via cross-domain knowledge distillation.\nExtensive experiments on \\textbf{11 popular datasets}, show that DPGNet\noutperforms SoTA approaches by \\textbf{6.3\\%}, highlighting its effectiveness\nin leveraging unlabeled data to address the annotation challenges posed by the\nincreasing realism of deepfakes.", "AI": {"tldr": "本文提出DPGNet，一种利用文本引导跨域对齐和课程驱动伪标签生成的技术，有效利用大量未标注数据来提高Deepfake检测的性能，应对人工标注困难的挑战。", "motivation": "现有Deepfake检测方法严重依赖标注数据，但随着AI生成内容日益逼真，人工标注变得耗时且不可靠。此外，AI生成的人脸与真实图像高度相似，导致传统无监督方法表现不佳，因此迫切需要能有效利用大规模未标注数据的方法。", "method": "本文引入双路径引导网络（DPGNet），包含两个核心模块：1) 文本引导的跨域对齐，使用可学习提示将视觉和文本嵌入统一到域不变特征空间；2) 课程驱动的伪标签生成，动态利用信息量更大的未标注样本。为防止灾难性遗忘，还通过跨域知识蒸馏促进域间桥接。", "result": "在11个流行数据集上进行的广泛实验表明，DPGNet的性能优于现有最先进方法6.3%。", "conclusion": "DPGNet通过有效利用未标注数据，成功解决了Deepfake日益逼真所带来的标注挑战，证明了其在Deepfake检测中的有效性。"}}
{"id": "2508.09042", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.09042", "abs": "https://arxiv.org/abs/2508.09042", "authors": ["Chen Xu", "Zhenyu Lv", "Tian Lan", "Xianyang Wang", "Luyao Ji", "Leyang Cui", "Minqiang Yang", "Jian Shen", "Qunxi Dong", "Xiuling Liu", "Juan Wang", "Bin Hu"], "title": "LLM-as-a-Supervisor: Mistaken Therapeutic Behaviors Trigger Targeted Supervisory Feedback", "comment": "9 pages, 5 figures", "summary": "Although large language models (LLMs) hold significant promise in\npsychotherapy, their direct application in patient-facing scenarios raises\nethical and safety concerns. Therefore, this work shifts towards developing an\nLLM as a supervisor to train real therapists. In addition to the privacy of\nclinical therapist training data, a fundamental contradiction complicates the\ntraining of therapeutic behaviors: clear feedback standards are necessary to\nensure a controlled training system, yet there is no absolute \"gold standard\"\nfor appropriate therapeutic behaviors in practice. In contrast, many common\ntherapeutic mistakes are universal and identifiable, making them effective\ntriggers for targeted feedback that can serve as clearer evidence. Motivated by\nthis, we create a novel therapist-training paradigm: (1) guidelines for\nmistaken behaviors and targeted correction strategies are first established as\nstandards; (2) a human-in-the-loop dialogue-feedback dataset is then\nconstructed, where a mistake-prone agent intentionally makes standard mistakes\nduring interviews naturally, and a supervisor agent locates and identifies\nmistakes and provides targeted feedback; (3) after fine-tuning on this dataset,\nthe final supervisor model is provided for real therapist training. The\ndetailed experimental results of automated, human and downstream assessments\ndemonstrate that models fine-tuned on our dataset MATE, can provide\nhigh-quality feedback according to the clinical guideline, showing significant\npotential for the therapist training scenario.", "AI": {"tldr": "该研究开发了一种新颖的范式，利用大型语言模型（LLM）作为导师，通过识别常见的治疗错误并提供有针对性的反馈来训练真实的治疗师。", "motivation": "直接将LLM应用于患者治疗存在伦理和安全问题。此外，治疗师培训需要明确的反馈标准，但治疗行为缺乏绝对的“黄金标准”。然而，常见的治疗错误是普遍且可识别的，这为有针对性的反馈提供了基础。", "method": "1. 建立错误行为指南和纠正策略作为标准；2. 构建一个“人机协同”的对话反馈数据集，其中一个易犯错的代理故意制造标准错误，一个主管代理定位并识别错误，提供有针对性的反馈；3. 在此数据集（MATE）上微调，得到最终的主管模型，用于真实治疗师的培训。", "result": "自动化、人工和下游评估的详细实验结果表明，在MATE数据集上微调的模型能够根据临床指南提供高质量的反馈。", "conclusion": "该研究提出的LLM主管模型在治疗师培训场景中显示出巨大的潜力，尤其是在缺乏“黄金标准”但常见错误可识别的背景下。"}}
{"id": "2508.08916", "categories": ["cs.CV", "cs.LG", "I.4.6; J.3"], "pdf": "https://arxiv.org/pdf/2508.08916", "abs": "https://arxiv.org/abs/2508.08916", "authors": ["David Bouget", "Mathilde Gajda Faanes", "Asgeir Store Jakola", "Frederik Barkhof", "Hilko Ardon", "Lorenzo Bello", "Mitchel S. Berger", "Shawn L. Hervey-Jumper", "Julia Furtner", "Albert J. S. Idema", "Barbara Kiesel", "Georg Widhalm", "Rishi Nandoe Tewarie", "Emmanuel Mandonnet", "Pierre A. Robe", "Michiel Wagemakers", "Timothy R. Smith", "Philip C. De Witt Hamer", "Ole solheim", "Ingerid Reinertsen"], "title": "Automatic and standardized surgical reporting for central nervous system tumors", "comment": "16 pages, 6 figures, 9 tables", "summary": "Magnetic resonance (MR) imaging is essential for evaluating central nervous\nsystem (CNS) tumors, guiding surgical planning, treatment decisions, and\nassessing postoperative outcomes and complication risks. While recent work has\nadvanced automated tumor segmentation and report generation, most efforts have\nfocused on preoperative data, with limited attention to postoperative imaging\nanalysis. This study introduces a comprehensive pipeline for standardized\npostsurtical reporting in CNS tumors. Using the Attention U-Net architecture,\nsegmentation models were trained for the preoperative (non-enhancing) tumor\ncore, postoperative contrast-enhancing residual tumor, and resection cavity.\nAdditionally, MR sequence classification and tumor type identification for\ncontrast-enhancing lesions were explored using the DenseNet architecture. The\nmodels were integrated into a reporting pipeline, following the RANO 2.0\nguidelines. Training was conducted on multicentric datasets comprising 2000 to\n7000 patients, using a 5-fold cross-validation. Evaluation included patient-,\nvoxel-, and object-wise metrics, with benchmarking against the latest BraTS\nchallenge results. The segmentation models achieved average voxel-wise Dice\nscores of 87%, 66%, 70%, and 77% for the tumor core, non-enhancing tumor core,\ncontrast-enhancing residual tumor, and resection cavity, respectively.\nClassification models reached 99.5% balanced accuracy in MR sequence\nclassification and 80% in tumor type classification. The pipeline presented in\nthis study enables robust, automated segmentation, MR sequence classification,\nand standardized report generation aligned with RANO 2.0 guidelines, enhancing\npostoperative evaluation and clinical decision-making. The proposed models and\nmethods were integrated into Raidionics, open-source software platform for CNS\ntumor analysis, now including a dedicated module for postsurgical analysis.", "AI": {"tldr": "本研究提出一个用于中枢神经系统肿瘤术后MR影像分析的自动化管道，包括肿瘤分割、序列分类和标准化报告生成，以增强术后评估和临床决策。", "motivation": "磁共振成像对中枢神经系统肿瘤的评估至关重要，但现有自动化肿瘤分析主要集中于术前影像，对术后影像分析的关注有限，缺乏一个标准化、全面的术后报告系统。", "method": "研究采用Attention U-Net架构训练分割模型，用于识别术前非增强肿瘤核心、术后对比增强残余肿瘤和切除腔。同时，使用DenseNet架构探索MR序列分类和对比增强病灶的肿瘤类型识别。这些模型被整合到一个符合RANO 2.0指南的报告管道中。模型在包含2000至7000名患者的多中心数据集上进行5折交叉验证训练，并使用患者、体素和对象级指标进行评估，同时与最新的BraTS挑战结果进行基准测试。提出的模型和方法已集成到开源软件Raidionics中。", "result": "分割模型在肿瘤核心、非增强肿瘤核心、对比增强残余肿瘤和切除腔上的平均体素级Dice分数分别为87%、66%、70%和77%。分类模型在MR序列分类中达到99.5%的平衡准确率，在肿瘤类型分类中达到80%的平衡准确率。该管道实现了鲁棒的自动化分割、MR序列分类和符合RANO 2.0指南的标准化报告生成。", "conclusion": "本研究提出的自动化管道能够显著提升中枢神经系统肿瘤的术后评估和临床决策，通过提供鲁棒的自动化分割、MR序列分类和符合RANO 2.0指南的标准化报告生成，解决了当前术后影像分析的不足。"}}
{"id": "2508.09138", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09138", "abs": "https://arxiv.org/abs/2508.09138", "authors": ["Wen Wang", "Bozhen Fang", "Chenchen Jing", "Yongliang Shen", "Yangyi Shen", "Qiuyu Wang", "Hao Ouyang", "Hao Chen", "Chunhua Shen"], "title": "Time Is a Feature: Exploiting Temporal Dynamics in Diffusion Language Models", "comment": "Project webpage: https://aim-uofa.github.io/dLLM-MidTruth", "summary": "Diffusion large language models (dLLMs) generate text through iterative\ndenoising, yet current decoding strategies discard rich intermediate\npredictions in favor of the final output. Our work here reveals a critical\nphenomenon, temporal oscillation, where correct answers often emerge in the\nmiddle process, but are overwritten in later denoising steps. To address this\nissue, we introduce two complementary methods that exploit temporal\nconsistency: 1) Temporal Self-Consistency Voting, a training-free, test-time\ndecoding strategy that aggregates predictions across denoising steps to select\nthe most consistent output; and 2) a post-training method termed Temporal\nConsistency Reinforcement, which uses Temporal Semantic Entropy (TSE), a\nmeasure of semantic stability across intermediate predictions, as a reward\nsignal to encourage stable generations. Empirical results across multiple\nbenchmarks demonstrate the effectiveness of our approach. Using the negative\nTSE reward alone, we observe a remarkable average improvement of 24.7% on the\nCountdown dataset over an existing dLLM. Combined with the accuracy reward, we\nachieve absolute gains of 2.0% on GSM8K, 4.3% on MATH500, 6.6% on SVAMP, and\n25.3% on Countdown, respectively. Our findings underscore the untapped\npotential of temporal dynamics in dLLMs and offer two simple yet effective\ntools to harness them.", "AI": {"tldr": "该研究揭示了扩散大语言模型（dLLMs）在去噪过程中存在“时间振荡”现象，即正确答案可能在中途出现但随后被覆盖。为解决此问题，论文提出了两种利用时间一致性的方法：时间自一致性投票（TSC-Voting）和时间一致性强化（TCR），显著提升了模型性能。", "motivation": "当前的扩散大语言模型解码策略会丢弃丰富的中间预测结果，且存在“时间振荡”现象，导致正确答案在后期去噪步骤中被覆盖，未能充分利用中间过程信息。", "method": "1. 时间自一致性投票（TSC-Voting）：一种无需训练的测试时解码策略，通过聚合去噪步骤中的预测结果来选择最一致的输出。2. 时间一致性强化（TCR）：一种后训练方法，利用时间语义熵（TSE）作为奖励信号，以鼓励生成更稳定的文本。", "result": "单独使用负TSE奖励，在Countdown数据集上比现有dLLM平均提高了24.7%。结合准确性奖励，在GSM8K、MATH500、SVAMP和Countdown数据集上分别实现了2.0%、4.3%、6.6%和25.3%的绝对增益。", "conclusion": "扩散大语言模型中的时间动态具有未被开发的潜力。论文提出的时间自一致性投票和时间一致性强化是利用这些时间动态的简单而有效的方法，能够显著提高模型性能。"}}
{"id": "2508.09057", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.09057", "abs": "https://arxiv.org/abs/2508.09057", "authors": ["Zeyu Huang", "Juyuan Wang", "Longfeng Chen", "Boyi Xiao", "Leng Cai", "Yawen Zeng", "Jin Xu"], "title": "MVISU-Bench: Benchmarking Mobile Agents for Real-World Tasks by Multi-App, Vague, Interactive, Single-App and Unethical Instructions", "comment": "ACM MM 2025", "summary": "Given the significant advances in Large Vision Language Models (LVLMs) in\nreasoning and visual understanding, mobile agents are rapidly emerging to meet\nusers' automation needs. However, existing evaluation benchmarks are\ndisconnected from the real world and fail to adequately address the diverse and\ncomplex requirements of users. From our extensive collection of user\nquestionnaire, we identified five tasks: Multi-App, Vague, Interactive,\nSingle-App, and Unethical Instructions. Around these tasks, we present\n\\textbf{MVISU-Bench}, a bilingual benchmark that includes 404 tasks across 137\nmobile applications. Furthermore, we propose Aider, a plug-and-play module that\nacts as a dynamic prompt prompter to mitigate risks and clarify user intent for\nmobile agents. Our Aider is easy to integrate into several frameworks and has\nsuccessfully improved overall success rates by 19.55\\% compared to the current\nstate-of-the-art (SOTA) on MVISU-Bench. Specifically, it achieves success rate\nimprovements of 53.52\\% and 29.41\\% for unethical and interactive instructions,\nrespectively. Through extensive experiments and analysis, we highlight the gap\nbetween existing mobile agents and real-world user expectations.", "AI": {"tldr": "该论文提出了MVISU-Bench，一个双语基准测试，用于评估移动智能体在真实世界复杂用户场景下的表现，并引入了Aider模块，一个动态提示器，显著提升了移动智能体的任务成功率。", "motivation": "尽管大型视觉语言模型（LVLMs）在推理和视觉理解方面取得显著进展，但现有评估基准与现实世界脱节，未能充分解决用户多样化和复杂的需求，特别是对于移动智能体。", "method": "通过广泛收集用户问卷，识别出五种核心任务类型：多应用、模糊、交互式、单应用和不道德指令。围绕这些任务，构建了MVISU-Bench，包含137个移动应用上的404个任务。此外，提出了Aider，一个即插即用的模块，作为动态提示器，用于降低风险和澄清用户意图。", "result": "MVISU-Bench是一个包含404个任务和137个移动应用的双语基准测试。Aider模块易于集成，相比现有最先进技术，在MVISU-Bench上将整体成功率提高了19.55%。特别地，对于不道德指令和交互式指令，成功率分别提高了53.52%和29.41%。", "conclusion": "现有移动智能体与真实世界用户期望之间存在显著差距。MVISU-Bench为更真实的评估提供了工具，而Aider模块则有效提升了移动智能体处理复杂和特定类型指令（如不道德和交互式指令）的能力，有助于弥合这一差距。"}}
{"id": "2508.08917", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08917", "abs": "https://arxiv.org/abs/2508.08917", "authors": ["Jintao Cheng", "Jiehao Luo", "Xieyuanli Chen", "Jin Wu", "Rui Fan", "Xiaoyu Tang", "Wei Zhang"], "title": "A Pseudo Global Fusion Paradigm-Based Cross-View Network for LiDAR-Based Place Recognition", "comment": null, "summary": "LiDAR-based Place Recognition (LPR) remains a critical task in Embodied\nArtificial Intelligence (AI) and Autonomous Driving, primarily addressing\nlocalization challenges in GPS-denied environments and supporting loop closure\ndetection. Existing approaches reduce place recognition to a Euclidean\ndistance-based metric learning task, neglecting the feature space's intrinsic\nstructures and intra-class variances. Such Euclidean-centric formulation\ninherently limits the model's capacity to capture nonlinear data distributions,\nleading to suboptimal performance in complex environments and temporal-varying\nscenarios. To address these challenges, we propose a novel cross-view network\nbased on an innovative fusion paradigm. Our framework introduces a\npseudo-global information guidance mechanism that coordinates multi-modal\nbranches to perform feature learning within a unified semantic space.\nConcurrently, we propose a Manifold Adaptation and Pairwise Variance-Locality\nLearning Metric that constructs a Symmetric Positive Definite (SPD) matrix to\ncompute Mahalanobis distance, superseding traditional Euclidean distance\nmetrics. This geometric formulation enables the model to accurately\ncharacterize intrinsic data distributions and capture complex inter-class\ndependencies within the feature space. Experimental results demonstrate that\nthe proposed algorithm achieves competitive performance, particularly excelling\nin complex environmental conditions.", "AI": {"tldr": "该论文提出了一种新的跨视图网络，结合伪全局信息引导和基于马哈拉诺比斯距离的度量学习方法，用于激光雷达地点识别（LPR），以克服传统欧氏距离方法的局限性，在复杂环境下实现卓越性能。", "motivation": "现有的激光雷达地点识别（LPR）方法将问题简化为基于欧氏距离的度量学习，忽略了特征空间的内在结构和类内方差。这种以欧氏距离为中心的方法限制了模型捕捉非线性数据分布的能力，导致在复杂和时变场景中性能不佳。", "method": "本文提出了一种基于创新融合范式的新型跨视图网络。该框架引入了伪全局信息引导机制，协调多模态分支在统一语义空间中进行特征学习。同时，提出了一种流形自适应和成对方差-局部性学习度量，通过构建对称正定（SPD）矩阵来计算马哈拉诺比斯距离，取代传统的欧氏距离度量。这种几何公式旨在准确表征内在数据分布并捕获特征空间中复杂的类间依赖关系。", "result": "实验结果表明，所提出的算法取得了具有竞争力的性能，尤其在复杂环境条件下表现出色。", "conclusion": "该研究通过引入创新的网络架构和基于马哈拉诺比斯距离的度量学习方法，有效解决了传统LPR方法在复杂环境下的性能局限性，显著提升了地点识别的准确性和鲁棒性。"}}
{"id": "2508.09072", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.09072", "abs": "https://arxiv.org/abs/2508.09072", "authors": ["Maxim Divilkovskiy", "Vitaly Malygin", "Sergey Zlobin", "Sultan Isali", "Vasily Kalugin", "Stanislav Ilyushin", "Nuriza Aitassova", "Yi Fei", "Zeng Weidi"], "title": "READER: Retrieval-Assisted Drafter for Efficient LLM Inference", "comment": null, "summary": "Large Language Models (LLMs) generate tokens autoregressively, with each\ntoken depending on the preceding context. This sequential nature makes the\ninference process inherently difficult to accelerate, posing a significant\nchallenge for efficient deployment. In recent years, various methods have been\nproposed to address this issue, with the most effective approaches often\ninvolving the training of additional draft models. In this paper, we introduce\nREADER (Retrieval-Assisted Drafter for Efficient LLM Inference), a novel\nlossless speculative decoding method that enhances model-based approaches by\nleveraging self-repetitions in the text. Our algorithm expands the speculative\ndecoding tree using tokens obtained through statistical search. This work\nfocuses on large batch sizes (>= 8), an underexplored yet important area for\nindustrial applications. We also analyze the key-value (KV) cache size during\nspeculative decoding and propose an optimization to improve performance for\nlarge batches. As a result, READER outperforms existing speculative decoding\nmethods. Notably, READER requires no additional training and can reuse\npre-trained speculator models, increasing the speedup by over 40\\%. Our method\ndemonstrates particularly strong performance on search-based tasks, such as\nretrieval-augmented generation, where we achieve more than 10x speedup.", "AI": {"tldr": "本文提出READER，一种新颖的无损推测解码方法，通过利用文本中的自重复和统计搜索来加速大型语言模型（LLMs）的推理，特别是在大批量处理时，无需额外训练即可显著超越现有方法。", "motivation": "LLM的自回归生成特性导致推理速度慢，难以高效部署。现有加速方法通常需要训练额外的草稿模型。此外，大批量推理（>=8）在工业应用中很重要，但研究不足。", "method": "引入READER（Retrieval-Assisted Drafter for Efficient LLM Inference），一种无损推测解码方法。该方法通过统计搜索获得的token扩展推测解码树，利用文本中的自重复来增强基于模型的加速方法。同时，分析并优化了推测解码过程中键值（KV）缓存的大小，以提高大批量处理的性能。", "result": "READER在性能上超越了现有推测解码方法。它无需额外训练，可重用预训练的推测器模型，将加速比提高了40%以上。在检索增强生成等搜索型任务上，READER表现尤为出色，实现了超过10倍的加速。", "conclusion": "READER是一种高效、无需额外训练的LLM推测解码方法，通过利用文本自重复和统计搜索，显著加速了LLM推理，尤其适用于大批量处理和搜索型任务，性能优于现有方法。"}}
{"id": "2508.08937", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.08937", "abs": "https://arxiv.org/abs/2508.08937", "authors": ["Leona Žůrková", "Petr Strakoš", "Michal Kravčenko", "Tomáš Brzobohatý", "Lubomír Říha"], "title": "Accelerated Volumetric Compression without Hierarchies: A Fourier Feature Based Implicit Neural Representation Approach", "comment": "2 pages, accepted for the VIS IEEE 2025 poster", "summary": "Volumetric data compression is critical in fields like medical imaging,\nscientific simulation, and entertainment. We introduce a structure-free neural\ncompression method combining Fourierfeature encoding with selective voxel\nsampling, yielding compact volumetric representations and faster convergence.\nOur dynamic voxel selection uses morphological dilation to prioritize active\nregions, reducing redundant computation without any hierarchical metadata. In\nthe experiment, sparse training reduced training time by 63.7 % (from 30 to 11\nminutes) with only minor quality loss: PSNR dropped 0.59 dB (from 32.60 to\n32.01) and SSIM by 0.008 (from 0.948 to 0.940). The resulting neural\nrepresentation, stored solely as network weights, achieves a compression rate\nof 14 and eliminates traditional data-loading overhead. This connects\ncoordinate-based neural representation with efficient volumetric compression,\noffering a scalable, structure-free solution for practical applications.", "AI": {"tldr": "该论文提出了一种结合傅里叶特征编码和选择性体素采样的无结构神经体数据压缩方法，实现了紧凑表示、更快的收敛和高压缩率。", "motivation": "体数据压缩在医学成像、科学模拟和娱乐等领域至关重要，但现有方法可能存在效率和表示问题。", "method": "该方法结合了傅里叶特征编码和选择性体素采样。通过形态学膨胀进行动态体素选择，优先处理活跃区域，减少冗余计算，且无需分层元数据。最终的神经表示仅存储为网络权重。", "result": "稀疏训练使训练时间减少了63.7%（从30分钟到11分钟），且质量损失轻微：PSNR下降0.59 dB（从32.60到32.01），SSIM下降0.008（从0.948到0.940）。实现了14倍的压缩率，并消除了传统数据加载开销。", "conclusion": "该研究将基于坐标的神经表示与高效的体数据压缩相结合，为实际应用提供了一个可扩展、无结构的解决方案。"}}
{"id": "2508.09074", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.09074", "abs": "https://arxiv.org/abs/2508.09074", "authors": ["Xinge Ye", "Rui Wang", "Yuchuan Wu", "Victor Ma", "Feiteng Fang", "Fei Huang", "Yongbin Li"], "title": "CPO: Addressing Reward Ambiguity in Role-playing Dialogue via Comparative Policy Optimization", "comment": null, "summary": "Reinforcement Learning Fine-Tuning (RLFT) has achieved notable success in\ntasks with objectively verifiable answers (e.g., code generation, mathematical\nreasoning), yet struggles with open-ended subjective tasks like role-playing\ndialogue. Traditional reward modeling approaches, which rely on independent\nsample-wise scoring, face dual challenges: subjective evaluation criteria and\nunstable reward signals.Motivated by the insight that human evaluation\ninherently combines explicit criteria with implicit comparative judgments, we\npropose Comparative Policy Optimization (CPO). CPO redefines the reward\nevaluation paradigm by shifting from sample-wise scoring to comparative\ngroup-wise scoring.Building on the same principle, we introduce the\nCharacterArena evaluation framework, which comprises two stages:(1)\nContextualized Multi-turn Role-playing Simulation, and (2) Trajectory-level\nComparative Evaluation. By operationalizing subjective scoring via objective\ntrajectory comparisons, CharacterArena minimizes contextual bias and enables\nmore robust and fair performance evaluation. Empirical results on\nCharacterEval, CharacterBench, and CharacterArena confirm that CPO effectively\nmitigates reward ambiguity and leads to substantial improvements in dialogue\nquality.", "AI": {"tldr": "针对RLFT在主观任务上的挑战，本文提出CPO通过比较式群体评分优化奖励信号，并引入CharacterArena框架进行轨迹级比较评估，显著提升了对话质量。", "motivation": "强化学习微调（RLFT）在客观任务上表现出色，但在开放式主观任务（如角色扮演对话）上表现不佳。传统奖励建模依赖独立样本评分，面临主观评价标准和不稳定奖励信号的双重挑战。受人类评估结合显式标准和隐式比较判断的启发，本文旨在解决这些问题。", "method": "提出比较式策略优化（CPO），将奖励评估范式从样本级评分转变为比较式群体级评分。在此基础上，引入CharacterArena评估框架，包含两个阶段：1) 上下文多轮角色扮演模拟，2) 轨迹级比较评估。通过客观的轨迹比较来操作主观评分，以最小化上下文偏差并实现更稳健和公平的性能评估。", "result": "在CharacterEval、CharacterBench和CharacterArena上的实证结果证实，CPO有效缓解了奖励模糊性，并显著提高了对话质量。", "conclusion": "CPO和CharacterArena通过引入比较式评估方法，成功解决了RLFT在主观开放式对话任务中奖励信号不稳定和评估困难的问题，从而显著提升了对话质量。"}}
{"id": "2508.08939", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08939", "abs": "https://arxiv.org/abs/2508.08939", "authors": ["Eduarda Caldeira", "Fadi Boutros", "Naser Damer"], "title": "MADPromptS: Unlocking Zero-Shot Morphing Attack Detection with Multiple Prompt Aggregation", "comment": "Accepted at ACM Multimedia Workshops", "summary": "Face Morphing Attack Detection (MAD) is a critical challenge in face\nrecognition security, where attackers can fool systems by interpolating the\nidentity information of two or more individuals into a single face image,\nresulting in samples that can be verified as belonging to multiple identities\nby face recognition systems. While multimodal foundation models (FMs) like CLIP\noffer strong zero-shot capabilities by jointly modeling images and text, most\nprior works on FMs for biometric recognition have relied on fine-tuning for\nspecific downstream tasks, neglecting their potential for direct, generalizable\ndeployment. This work explores a pure zero-shot approach to MAD by leveraging\nCLIP without any additional training or fine-tuning, focusing instead on the\ndesign and aggregation of multiple textual prompts per class. By aggregating\nthe embeddings of diverse prompts, we better align the model's internal\nrepresentations with the MAD task, capturing richer and more varied cues\nindicative of bona-fide or attack samples. Our results show that prompt\naggregation substantially improves zero-shot detection performance,\ndemonstrating the effectiveness of exploiting foundation models' built-in\nmultimodal knowledge through efficient prompt engineering.", "AI": {"tldr": "本研究提出了一种纯零样本的人脸形变攻击检测（MAD）方法，通过利用CLIP模型并聚合多个文本提示来提高检测性能，无需额外训练或微调。", "motivation": "人脸形变攻击（MAD）对人脸识别系统构成严重安全威胁，攻击者可创建能被多个身份验证的图像。尽管CLIP等多模态基础模型具有强大的零样本能力，但多数现有研究仍依赖于微调，忽视了其直接、泛化部署的潜力。", "method": "该研究采用纯零样本方法，不进行任何额外的训练或微调，直接利用CLIP模型进行MAD。核心方法是设计和聚合每个类别的多个文本提示，通过聚合不同提示的嵌入来更好地对齐模型内部表示与MAD任务，捕捉更丰富多样的真实或攻击样本线索。", "result": "实验结果表明，提示聚合显著提高了零样本检测性能。", "conclusion": "该研究证明了通过高效的提示工程，可以有效利用基础模型内置的多模态知识进行人脸形变攻击检测。"}}
{"id": "2508.09091", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.09091", "abs": "https://arxiv.org/abs/2508.09091", "authors": ["Imalsha Puranegedara", "Themira Chathumina", "Nisal Ranathunga", "Nisansa de Silva", "Surangika Ranathunga", "Mokanarangan Thayaparan"], "title": "Utilizing Multilingual Encoders to Improve Large Language Models for Low-Resource Languages", "comment": null, "summary": "Large Language Models (LLMs) excel in English, but their performance degrades\nsignificantly on low-resource languages (LRLs) due to English-centric training.\nWhile methods like LangBridge align LLMs with multilingual encoders such as the\nMassively Multilingual Text-to-Text Transfer Transformer (mT5), they typically\nuse only the final encoder layer. We propose a novel architecture that fuses\nall intermediate layers, enriching the linguistic information passed to the\nLLM. Our approach features two strategies: (1) a Global Softmax weighting for\noverall layer importance, and (2) a Transformer Softmax model that learns\ntoken-specific weights. The fused representations are mapped into the LLM's\nembedding space, enabling it to process multilingual inputs. The model is\ntrained only on English data, without using any parallel or multilingual data.\nEvaluated on XNLI, IndicXNLI, Sinhala News Classification, and Amazon Reviews,\nour Transformer Softmax model significantly outperforms the LangBridge\nbaseline. We observe strong performance gains in LRLs, improving Sinhala\nclassification accuracy from 71.66% to 75.86% and achieving clear improvements\nacross Indic languages such as Tamil, Bengali, and Malayalam. These specific\ngains contribute to an overall boost in average XNLI accuracy from 70.36% to\n71.50%. This approach offers a scalable, data-efficient path toward more\ncapable and equitable multilingual LLMs.", "AI": {"tldr": "该研究提出了一种新颖的架构，通过融合多语言编码器（如mT5）的所有中间层，而非仅最后一层，来增强LLMs在低资源语言上的性能，且仅需英文数据训练。", "motivation": "大型语言模型（LLMs）在英语表现出色，但在低资源语言（LRLs）上的性能显著下降，原因在于其以英语为中心的训练。现有方法（如LangBridge）通常只使用多语言编码器的最后一层，未能充分利用语言信息。", "method": "提出了一种新颖的架构，融合多语言编码器（如mT5）的所有中间层，以丰富传递给LLM的语言信息。采用两种策略：1) 全局Softmax加权，用于评估整体层的重要性；2) Transformer Softmax模型，学习特定于token的权重。融合后的表示被映射到LLM的嵌入空间。模型仅使用英文数据训练，无需并行或多语言数据。", "result": "在XNLI、IndicXNLI、僧伽罗语新闻分类和亚马逊评论数据集上进行评估，Transformer Softmax模型显著优于LangBridge基线。在LRLs中表现出强劲的性能提升，例如僧伽罗语分类准确率从71.66%提高到75.86%，并在泰米尔语、孟加拉语和马拉雅拉姆语等印度语言中取得了明显改善。XNLI平均准确率从70.36%提升至71.50%。", "conclusion": "该方法提供了一种可扩展、数据高效的途径，以构建更强大、更公平的多语言LLMs。"}}
{"id": "2508.08944", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08944", "abs": "https://arxiv.org/abs/2508.08944", "authors": ["Wenhan Wu", "Zhishuai Guo", "Chen Chen", "Aidong Lu"], "title": "UniSTFormer: Unified Spatio-Temporal Lightweight Transformer for Efficient Skeleton-Based Action Recognition", "comment": null, "summary": "Skeleton-based action recognition (SAR) has achieved impressive progress with\ntransformer architectures. However, existing methods often rely on complex\nmodule compositions and heavy designs, leading to increased parameter counts,\nhigh computational costs, and limited scalability. In this paper, we propose a\nunified spatio-temporal lightweight transformer framework that integrates\nspatial and temporal modeling within a single attention module, eliminating the\nneed for separate temporal modeling blocks. This approach reduces redundant\ncomputations while preserving temporal awareness within the spatial modeling\nprocess. Furthermore, we introduce a simplified multi-scale pooling fusion\nmodule that combines local and global pooling pathways to enhance the model's\nability to capture fine-grained local movements and overarching global motion\npatterns. Extensive experiments on benchmark datasets demonstrate that our\nlightweight model achieves a superior balance between accuracy and efficiency,\nreducing parameter complexity by over 58% and lowering computational cost by\nover 60% compared to state-of-the-art transformer-based baselines, while\nmaintaining competitive recognition performance.", "AI": {"tldr": "本文提出了一种统一的时空轻量级Transformer框架，用于骨架行为识别，通过集成时空建模和简化的多尺度池化融合，显著降低了模型复杂度和计算成本，同时保持了竞争力。", "motivation": "现有的基于Transformer的骨架行为识别方法通常依赖复杂的模块组合和重型设计，导致参数量大、计算成本高和可扩展性受限。", "method": "提出了一种统一的时空轻量级Transformer框架，将空间和时间建模集成到单个注意力模块中，无需单独的时间建模块。此外，引入了一个简化的多尺度池化融合模块，结合局部和全局池化路径以捕获细粒度局部运动和整体全局运动模式。", "result": "与现有最先进的基于Transformer的基线相比，该轻量级模型在准确性和效率之间取得了卓越平衡，参数复杂度降低了58%以上，计算成本降低了60%以上，同时保持了有竞争力的识别性能。", "conclusion": "所提出的轻量级模型在骨架行为识别任务中，实现了效率和性能的有效权衡，显著减少了模型复杂度和计算开销。"}}
{"id": "2508.09096", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.09096", "abs": "https://arxiv.org/abs/2508.09096", "authors": ["Anastasia Zhukova", "Thomas Walton", "Christian E. Matt", "Bela Gipp"], "title": "Link Prediction for Event Logs in the Process Industry", "comment": null, "summary": "Knowledge management (KM) is vital in the process industry for optimizing\noperations, ensuring safety, and enabling continuous improvement through\neffective use of operational data and past insights. A key challenge in this\ndomain is the fragmented nature of event logs in shift books, where related\nrecords, e.g., entries documenting issues related to equipment or processes and\nthe corresponding solutions, may remain disconnected. This fragmentation\nhinders the recommendation of previous solutions to the users. To address this\nproblem, we investigate record linking (RL) as link prediction, commonly\nstudied in graph-based machine learning, by framing it as a cross-document\ncoreference resolution (CDCR) task enhanced with natural language inference\n(NLI) and semantic text similarity (STS) by shifting it into the causal\ninference (CI). We adapt CDCR, traditionally applied in the news domain, into\nan RL model to operate at the passage level, similar to NLI and STS, while\naccommodating the process industry's specific text formats, which contain\nunstructured text and structured record attributes. Our RL model outperformed\nthe best versions of NLI- and STS-driven baselines by 28% (11.43 points) and\n27% (11.21 points), respectively. Our work demonstrates how domain adaptation\nof the state-of-the-art CDCR models, enhanced with reasoning capabilities, can\nbe effectively tailored to the process industry, improving data quality and\nconnectivity in shift logs.", "AI": {"tldr": "该研究针对流程工业中班次日志事件记录碎片化的问题，提出了一种结合自然语言推理（NLI）和语义文本相似度（STS）的跨文档共指消解（CDCR）模型，以实现记录链接，从而提升数据质量和连接性。", "motivation": "在流程工业中，知识管理对于优化运营、确保安全和持续改进至关重要。然而，班次日志中的事件记录常常是碎片化的，导致相关记录（如设备/过程问题及其解决方案）相互分离，从而阻碍了向用户推荐历史解决方案。", "method": "将记录链接（RL）问题框定为图机器学习中常见的链接预测问题，并进一步将其视为通过自然语言推理（NLI）和语义文本相似度（STS）增强的跨文档共指消解（CDCR）任务，并引入因果推断（CI）的视角。作者将传统应用于新闻领域的CDCR模型进行领域适应，使其能在段落级别操作，并能处理流程工业特有的非结构化文本和结构化记录属性。", "result": "所提出的记录链接（RL）模型在性能上优于NLI和STS驱动的最佳基线模型，分别高出28%（11.43点）和27%（11.21点）。", "conclusion": "研究表明，通过领域适应并增强推理能力的最先进CDCR模型，可以有效地应用于流程工业，显著改善班次日志中的数据质量和连接性。"}}
{"id": "2508.08949", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08949", "abs": "https://arxiv.org/abs/2508.08949", "authors": ["Ao Ma", "Jiasong Feng", "Ke Cao", "Jing Wang", "Yun Wang", "Quanwei Zhang", "Zhanjie Zhang"], "title": "Lay2Story: Extending Diffusion Transformers for Layout-Togglable Story Generation", "comment": "Accepted by ICCV 2025", "summary": "Storytelling tasks involving generating consistent subjects have gained\nsignificant attention recently. However, existing methods, whether\ntraining-free or training-based, continue to face challenges in maintaining\nsubject consistency due to the lack of fine-grained guidance and inter-frame\ninteraction. Additionally, the scarcity of high-quality data in this field\nmakes it difficult to precisely control storytelling tasks, including the\nsubject's position, appearance, clothing, expression, and posture, thereby\nhindering further advancements. In this paper, we demonstrate that layout\nconditions, such as the subject's position and detailed attributes, effectively\nfacilitate fine-grained interactions between frames. This not only strengthens\nthe consistency of the generated frame sequence but also allows for precise\ncontrol over the subject's position, appearance, and other key details.\nBuilding on this, we introduce an advanced storytelling task: Layout-Togglable\nStorytelling, which enables precise subject control by incorporating layout\nconditions. To address the lack of high-quality datasets with layout\nannotations for this task, we develop Lay2Story-1M, which contains over 1\nmillion 720p and higher-resolution images, processed from approximately 11,300\nhours of cartoon videos. Building on Lay2Story-1M, we create Lay2Story-Bench, a\nbenchmark with 3,000 prompts designed to evaluate the performance of different\nmethods on this task. Furthermore, we propose Lay2Story, a robust framework\nbased on the Diffusion Transformers (DiTs) architecture for Layout-Togglable\nStorytelling tasks. Through both qualitative and quantitative experiments, we\nfind that our method outperforms the previous state-of-the-art (SOTA)\ntechniques, achieving the best results in terms of consistency, semantic\ncorrelation, and aesthetic quality.", "AI": {"tldr": "该论文提出了一个名为“Layout-Togglable Storytelling”的新型讲故事任务，旨在通过引入布局条件实现对主体的高度精确控制。为此，作者构建了大规模数据集Lay2Story-1M和评估基准Lay2Story-Bench，并提出了基于Diffusion Transformers的Lay2Story框架，在主体一致性、语义关联性和美学质量方面超越了现有SOTA方法。", "motivation": "现有的故事生成方法在保持主体一致性方面存在挑战，原因在于缺乏细粒度指导和帧间交互。此外，高质量数据的稀缺也使得精确控制故事中主体的位置、外观、服装、表情和姿势变得困难，从而阻碍了该领域的进一步发展。", "method": "1. 引入并定义了“Layout-Togglable Storytelling”这一高级故事生成任务，通过布局条件（如主体位置和详细属性）实现对主体的精确控制。\n2. 开发了Lay2Story-1M数据集，包含超过100万张带有布局注释的高分辨率图像，来源于约11,300小时的卡通视频。\n3. 基于Lay2Story-1M创建了Lay2Story-Bench基准，包含3,000个提示，用于评估不同方法在该任务上的性能。\n4. 提出了Lay2Story框架，一个基于Diffusion Transformers (DiTs) 架构的鲁棒模型，用于处理Layout-Togglable Storytelling任务。", "result": "通过定性和定量实验，本研究发现所提出的Lay2Story方法在主体一致性、语义关联性和美学质量方面均优于先前的最先进（SOTA）技术，取得了最佳结果。", "conclusion": "布局条件能有效促进帧间的细粒度交互，从而增强生成序列的一致性并实现对主体细节的精确控制。所提出的Lay2Story任务、数据集、基准和框架有效解决了当前故事生成任务中主体一致性和精确控制的难题，并显著提升了该领域的性能。"}}
{"id": "2508.09101", "categories": ["cs.CL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.09101", "abs": "https://arxiv.org/abs/2508.09101", "authors": ["Jason Chou", "Ao Liu", "Yuchi Deng", "Zhiying Zeng", "Tao Zhang", "Haotian Zhu", "Jianwei Cai", "Yue Mao", "Chenchen Zhang", "Lingyun Tan", "Ziyan Xu", "Bohui Zhai", "Hengyi Liu", "Speed Zhu", "Wiggin Zhou", "Fengzong Lian"], "title": "AutoCodeBench: Large Language Models are Automatic Code Benchmark Generators", "comment": "Homepage: https://autocodebench.github.io/", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious domains, with code generation emerging as a key area of focus. While\nnumerous benchmarks have been proposed to evaluate their code generation\nabilities, these benchmarks face several critical limitations. First, they\noften rely on manual annotations, which are time-consuming and difficult to\nscale across different programming languages and problem complexities. Second,\nmost existing benchmarks focus primarily on Python, while the few multilingual\nbenchmarks suffer from limited difficulty and uneven language distribution. To\naddress these challenges, we propose AutoCodeGen, an automated method for\ngenerating high-difficulty multilingual code generation datasets without manual\nannotations. AutoCodeGen ensures the correctness and completeness of test cases\nby generating test inputs with LLMs and obtaining test outputs through a\nmultilingual sandbox, while achieving high data quality through reverse-order\nproblem generation and multiple filtering steps. Using this novel method, we\nintroduce AutoCodeBench, a large-scale code generation benchmark comprising\n3,920 problems evenly distributed across 20 programming languages. It is\nspecifically designed to evaluate LLMs on challenging, diverse, and practical\nmultilingual tasks. We evaluate over 30 leading open-source and proprietary\nLLMs on AutoCodeBench and its simplified version AutoCodeBench-Lite. The\nresults show that even the most advanced LLMs struggle with the complexity,\ndiversity, and multilingual nature of these tasks. Besides, we introduce\nAutoCodeBench-Complete, specifically designed for base models to assess their\nfew-shot code generation capabilities. We hope the AutoCodeBench series will\nserve as a valuable resource and inspire the community to focus on more\nchallenging and practical multilingual code generation scenarios.", "AI": {"tldr": "本文提出AutoCodeGen，一种自动化方法，用于生成高难度、无手动标注的多语言代码生成数据集。基于此方法，构建了AutoCodeBench系列基准，并评估发现即使最先进的大语言模型在复杂多语言代码生成任务上仍表现不佳。", "motivation": "现有代码生成基准面临多项挑战：过度依赖手动标注导致耗时且难以扩展；主要关注Python语言，缺乏多语言支持；少数多语言基准存在难度有限和语言分布不均的问题。", "method": "提出AutoCodeGen自动化方法，无需手动标注即可生成高难度多语言代码生成数据集。该方法通过大语言模型生成测试输入，利用多语言沙箱获取测试输出，并通过逆序问题生成和多重过滤步骤确保测试用例的正确性、完整性和数据质量。", "result": "基于AutoCodeGen，构建了大规模代码生成基准AutoCodeBench，包含3,920个问题，均匀分布于20种编程语言，旨在评估大语言模型在挑战性、多样化和实用多语言任务上的表现。对30多个领先的开源和专有大语言模型进行评估，结果显示即使最先进的模型也难以应对这些任务的复杂性、多样性和多语言特性。此外，还引入了AutoCodeBench-Lite和AutoCodeBench-Complete用于不同场景的评估。", "conclusion": "AutoCodeBench系列基准是评估大语言模型在多语言代码生成能力方面的宝贵资源，有望激励社区关注更具挑战性和实用性的多语言代码生成场景。"}}
{"id": "2508.08974", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08974", "abs": "https://arxiv.org/abs/2508.08974", "authors": ["Elman Ghazaei", "Erchan Aptoula"], "title": "Text-conditioned State Space Model For Domain-generalized Change Detection Visual Question Answering", "comment": null, "summary": "The Earth's surface is constantly changing, and detecting these changes\nprovides valuable insights that benefit various aspects of human society. While\ntraditional change detection methods have been employed to detect changes from\nbi-temporal images, these approaches typically require expert knowledge for\naccurate interpretation. To enable broader and more flexible access to change\ninformation by non-expert users, the task of Change Detection Visual Question\nAnswering (CDVQA) has been introduced. However, existing CDVQA methods have\nbeen developed under the assumption that training and testing datasets share\nsimilar distributions. This assumption does not hold in real-world\napplications, where domain shifts often occur. In this paper, the CDVQA task is\nrevisited with a focus on addressing domain shift. To this end, a new\nmulti-modal and multi-domain dataset, BrightVQA, is introduced to facilitate\ndomain generalization research in CDVQA. Furthermore, a novel state space\nmodel, termed Text-Conditioned State Space Model (TCSSM), is proposed. The\nTCSSM framework is designed to leverage both bi-temporal imagery and\ngeo-disaster-related textual information in an unified manner to extract\ndomain-invariant features across domains. Input-dependent parameters existing\nin TCSSM are dynamically predicted by using both bi-temporal images and\ngeo-disaster-related description, thereby facilitating the alignment between\nbi-temporal visual data and the associated textual descriptions. Extensive\nexperiments are conducted to evaluate the proposed method against\nstate-of-the-art models, and superior performance is consistently demonstrated.\nThe code and dataset will be made publicly available upon acceptance at\nhttps://github.com/Elman295/TCSSM.", "AI": {"tldr": "针对变化检测视觉问答（CDVQA）中的域偏移问题，本文提出了一个新的多模态多域数据集BrightVQA和一个文本条件状态空间模型（TCSSM），该模型通过整合图像和文本信息来提取域不变特征，并在实验中展现出卓越的性能。", "motivation": "传统变化检测方法需要专业知识。为使非专家用户更广泛地获取变化信息，引入了CDVQA任务。然而，现有CDVQA方法假设训练和测试数据集分布相似，这在存在域偏移的真实世界应用中无法成立。", "method": "1. 引入了一个新的多模态、多域数据集BrightVQA，以促进CDVQA中的域泛化研究。2. 提出了一种新颖的文本条件状态空间模型（TCSSM），该模型统一利用双时相图像和地理灾害相关的文本信息，以提取跨域的域不变特征。3. TCSSM中的输入依赖参数通过双时相图像和地理灾害描述动态预测，以促进视觉数据与文本描述的对齐。", "result": "广泛的实验表明，所提出的方法在与现有最先进模型的比较中持续展现出卓越的性能。", "conclusion": "本文成功解决了CDVQA任务中的域偏移问题，通过引入新的多模态多域数据集和文本条件状态空间模型（TCSSM），实现了在真实世界应用中更鲁棒、更准确的变化检测视觉问答能力。"}}
{"id": "2508.09115", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.09115", "abs": "https://arxiv.org/abs/2508.09115", "authors": ["H. W. K. Aravinda", "Rashad Sirajudeen", "Samith Karunathilake", "Nisansa de Silva", "Surangika Ranathunga", "Rishemjit Kaur"], "title": "SinLlama -- A Large Language Model for Sinhala", "comment": null, "summary": "Low-resource languages such as Sinhala are often overlooked by open-source\nLarge Language Models (LLMs). In this research, we extend an existing\nmultilingual LLM (Llama-3-8B) to better serve Sinhala. We enhance the LLM\ntokenizer with Sinhala specific vocabulary and perform continual pre-training\non a cleaned 10 million Sinhala corpus, resulting in the SinLlama model. This\nis the very first decoder-based open-source LLM with explicit Sinhala support.\nWhen SinLlama was instruction fine-tuned for three text classification tasks,\nit outperformed base and instruct variants of Llama-3-8B by a significant\nmargin.", "AI": {"tldr": "通过词汇扩展和持续预训练，将Llama-3-8B模型适配斯里兰卡僧伽罗语，创建了首个支持僧伽罗语的开源解码器LLM（SinLlama），并在文本分类任务中表现优异。", "motivation": "开源大型语言模型（LLMs）通常忽视像僧伽罗语这样的低资源语言。", "method": "扩展现有Llama-3-8B的多语言LLM，增强其分词器以包含僧伽罗语特定词汇，并在一个1000万词的清洗过的僧伽罗语语料库上进行持续预训练，从而得到SinLlama模型。随后对SinLlama进行指令微调，用于三个文本分类任务。", "result": "SinLlama是首个明确支持僧伽罗语的开源解码器LLM。在三个文本分类任务中，经过指令微调的SinLlama显著优于Llama-3-8B的基础版和指令微调版。", "conclusion": "通过词汇增强和持续预训练的方法，可以有效地将现有LLM扩展并优化，使其更好地服务于低资源语言，如僧伽罗语。"}}
{"id": "2508.08978", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08978", "abs": "https://arxiv.org/abs/2508.08978", "authors": ["Zhentao Fan", "Zongzuo Wang", "Weiwei Zhang"], "title": "TaoCache: Structure-Maintained Video Generation Acceleration", "comment": null, "summary": "Existing cache-based acceleration methods for video diffusion models\nprimarily skip early or mid denoising steps, which often leads to structural\ndiscrepancies relative to full-timestep generation and can hinder instruction\nfollowing and character consistency. We present TaoCache, a training-free,\nplug-and-play caching strategy that, instead of residual-based caching, adopts\na fixed-point perspective to predict the model's noise output and is\nspecifically effective in late denoising stages. By calibrating cosine\nsimilarities and norm ratios of consecutive noise deltas, TaoCache preserves\nhigh-resolution structure while enabling aggressive skipping. The approach is\northogonal to complementary accelerations such as Pyramid Attention Broadcast\n(PAB) and TeaCache, and it integrates seamlessly into DiT-based frameworks.\nAcross Latte-1, OpenSora-Plan v110, and Wan2.1, TaoCache attains substantially\nhigher visual quality (LPIPS, SSIM, PSNR) than prior caching methods under the\nsame speedups.", "AI": {"tldr": "TaoCache是一种免训练、即插即用的视频扩散模型缓存策略，通过固定点视角预测噪声输出，特别适用于去噪后期阶段，显著提升了生成质量和一致性，优于现有加速方法。", "motivation": "现有的视频扩散模型缓存加速方法主要跳过早期或中期去噪步骤，导致结构不一致，并损害指令遵循和角色一致性。", "method": "TaoCache是一种免训练、即插即用的缓存策略，它采用固定点视角来预测模型的噪声输出，在去噪后期阶段特别有效。它通过校准连续噪声增量的余弦相似度和范数比来保留高分辨率结构，同时实现激进的跳步。该方法与PAB和TeaCache等其他加速方法正交，并能无缝集成到基于DiT的框架中。", "result": "在Latte-1、OpenSora-Plan v110和Wan2.1等模型上，TaoCache在相同加速比下，比现有缓存方法获得了显著更高的视觉质量（LPIPS、SSIM、PSNR）。", "conclusion": "TaoCache通过其独特的固定点预测方法，有效解决了现有缓存加速策略的质量和一致性问题，尤其在去噪后期表现出色，是视频扩散模型加速的有效方案。"}}
{"id": "2508.09124", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.09124", "abs": "https://arxiv.org/abs/2508.09124", "authors": ["Weixuan Wang", "Dongge Han", "Daniel Madrigal Diaz", "Jin Xu", "Victor Rühle", "Saravan Rajmohan"], "title": "OdysseyBench: Evaluating LLM Agents on Long-Horizon Complex Office Application Workflows", "comment": null, "summary": "Autonomous agents powered by large language models (LLMs) are increasingly\ndeployed in real-world applications requiring complex, long-horizon workflows.\nHowever, existing benchmarks predominantly focus on atomic tasks that are\nself-contained and independent, failing to capture the long-term contextual\ndependencies and multi-interaction coordination required in realistic\nscenarios. To address this gap, we introduce OdysseyBench, a comprehensive\nbenchmark for evaluating LLM agents on long-horizon workflows across diverse\noffice applications including Word, Excel, PDF, Email, and Calendar. Our\nbenchmark comprises two complementary splits: OdysseyBench+ with 300 tasks\nderived from real-world use cases, and OdysseyBench-Neo with 302 newly\nsynthesized complex tasks. Each task requires agent to identify essential\ninformation from long-horizon interaction histories and perform multi-step\nreasoning across various applications. To enable scalable benchmark creation,\nwe propose HomerAgents, a multi-agent framework that automates the generation\nof long-horizon workflow benchmarks through systematic environment exploration,\ntask generation, and dialogue synthesis. Our extensive evaluation demonstrates\nthat OdysseyBench effectively challenges state-of-the-art LLM agents, providing\nmore accurate assessment of their capabilities in complex, real-world contexts\ncompared to existing atomic task benchmarks. We believe that OdysseyBench will\nserve as a valuable resource for advancing the development and evaluation of\nLLM agents in real-world productivity scenarios. In addition, we release\nOdysseyBench and HomerAgents to foster research along this line.", "AI": {"tldr": "本文介绍了OdysseyBench，一个用于评估LLM代理在复杂、长周期办公应用（如Word、Excel、PDF、邮件、日历）工作流中表现的综合基准，并提出了HomerAgents框架自动化基准生成。", "motivation": "现有基准主要关注独立的原子任务，未能捕捉真实场景中所需的长期上下文依赖和多交互协调，无法准确评估LLM代理在复杂、长周期工作流中的能力。", "method": "提出了OdysseyBench基准，包含OdysseyBench+（300个真实用例任务）和OdysseyBench-Neo（302个新合成复杂任务）两个互补部分，每个任务都需要代理识别长期交互历史中的关键信息并进行多步骤跨应用推理。为实现可扩展的基准创建，提出了HomerAgents，一个通过系统环境探索、任务生成和对话合成来自动化生成长周期工作流基准的多代理框架。", "result": "广泛的评估表明，OdysseyBench能够有效挑战最先进的LLM代理，与现有原子任务基准相比，能更准确地评估其在复杂真实世界环境中的能力。", "conclusion": "OdysseyBench将成为推动LLM代理在真实世界生产力场景中发展和评估的宝贵资源。作者同时发布了OdysseyBench和HomerAgents以促进相关研究。"}}
{"id": "2508.08987", "categories": ["cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.08987", "abs": "https://arxiv.org/abs/2508.08987", "authors": ["Ding Xia", "Naoto Inoue", "Qianru Qiu", "Kotaro Kikuchi"], "title": "ColorGPT: Leveraging Large Language Models for Multimodal Color Recommendation", "comment": "Accepted to ICDAR2025", "summary": "Colors play a crucial role in the design of vector graphic documents by\nenhancing visual appeal, facilitating communication, improving usability, and\nensuring accessibility. In this context, color recommendation involves\nsuggesting appropriate colors to complete or refine a design when one or more\ncolors are missing or require alteration. Traditional methods often struggled\nwith these challenges due to the complex nature of color design and the limited\ndata availability. In this study, we explored the use of pretrained Large\nLanguage Models (LLMs) and their commonsense reasoning capabilities for color\nrecommendation, raising the question: Can pretrained LLMs serve as superior\ndesigners for color recommendation tasks? To investigate this, we developed a\nrobust, rigorously validated pipeline, ColorGPT, that was built by\nsystematically testing multiple color representations and applying effective\nprompt engineering techniques. Our approach primarily targeted color palette\ncompletion by recommending colors based on a set of given colors and\naccompanying context. Moreover, our method can be extended to full palette\ngeneration, producing an entire color palette corresponding to a provided\ntextual description. Experimental results demonstrated that our LLM-based\npipeline outperformed existing methods in terms of color suggestion accuracy\nand the distribution of colors in the color palette completion task. For the\nfull palette generation task, our approach also yielded improvements in color\ndiversity and similarity compared to current techniques.", "AI": {"tldr": "本研究探索了预训练大型语言模型（LLMs）在颜色推荐任务中的应用，开发了ColorGPT管道，并在颜色调色板补全和生成任务中表现优于现有方法。", "motivation": "传统方法在颜色设计中面临复杂性和数据有限的挑战，难以有效地进行颜色推荐。本研究旨在探究LLMs是否能成为更优秀的颜色推荐设计师。", "method": "研究开发了一个名为ColorGPT的管道，该管道通过系统测试多种颜色表示和应用有效的提示工程技术构建。它主要针对基于给定颜色和上下文的颜色调色板补全，并可扩展到根据文本描述生成完整的颜色调色板。", "result": "在颜色调色板补全任务中，基于LLM的管道在颜色建议准确性和颜色分布方面优于现有方法。在完整调色板生成任务中，该方法在颜色多样性和相似性方面也取得了改进。", "conclusion": "预训练LLMs及其常识推理能力可以作为颜色推荐任务的优秀设计师，显著提升颜色建议的质量和多样性。"}}
{"id": "2508.09125", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.09125", "abs": "https://arxiv.org/abs/2508.09125", "authors": ["Mian Zhang", "Shujian Liu", "Sixun Dong", "Ming Yin", "Yebowen Hu", "Xun Wang", "Steven Ma", "Song Wang", "Sathish Reddy Indurthi", "Haoyun Deng", "Zhiyu Zoey Chen", "Kaiqiang Song"], "title": "Complex Logical Instruction Generation", "comment": null, "summary": "Instruction following has catalyzed the recent era of Large Language Models\n(LLMs) and is the foundational skill underpinning more advanced capabilities\nsuch as reasoning and agentic behaviors. As tasks grow more challenging, the\nlogic structures embedded in natural language instructions becomes increasingly\nintricate. However, how well LLMs perform on such logic-rich instructions\nremains under-explored. We propose LogicIFGen and LogicIFEval. LogicIFGen is a\nscalable, automated framework for generating verifiable instructions from code\nfunctions, which can naturally express rich logic such as conditionals,\nnesting, recursion, and function calls. We further curate a collection of\ncomplex code functions and use LogicIFGen to construct LogicIFEval, a benchmark\ncomprising 426 verifiable logic-rich instructions. Our experiments demonstrate\nthat current state-of-the-art LLMs still struggle to correctly follow the\ninstructions in LogicIFEval. Most LLMs can only follow fewer than 60% of the\ninstructions, revealing significant deficiencies in the instruction-following\nability. Code and Benchmark: https://github.com/mianzhang/LogicIF", "AI": {"tldr": "研究提出LogicIFGen和LogicIFEval，一个用于生成和评估LLM在复杂逻辑指令遵循能力上的基准，发现当前SOTA LLM在此方面表现不佳。", "motivation": "指令遵循是LLM的基础技能，支撑推理和智能体行为。然而，随着任务复杂性增加，自然语言指令中的逻辑结构也变得复杂，LLM在此类富逻辑指令上的表现尚未得到充分探索。", "method": "提出LogicIFGen，一个可扩展的自动化框架，能从代码函数（自然表达条件、嵌套、递归、函数调用等丰富逻辑）生成可验证的指令。基于此，构建LogicIFEval基准，包含426个由复杂代码函数生成的富逻辑可验证指令。", "result": "实验表明，当前最先进的LLM在LogicIFEval上的表现不佳，大多数LLM只能正确遵循不到60%的指令。", "conclusion": "LLM在遵循富逻辑指令的能力上存在显著缺陷。"}}
{"id": "2508.08989", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08989", "abs": "https://arxiv.org/abs/2508.08989", "authors": ["Ming Nie", "Chunwei Wang", "Hang Xu", "Li Zhang"], "title": "KFFocus: Highlighting Keyframes for Enhanced Video Understanding", "comment": null, "summary": "Recently, with the emergence of large language models, multimodal LLMs have\ndemonstrated exceptional capabilities in image and video modalities. Despite\nadvancements in video comprehension, the substantial computational demands of\nlong video sequences lead current video LLMs (Vid-LLMs) to employ compression\nstrategies at both the inter-frame level (e.g., uniform sampling of video\nframes) and intra-frame level (e.g., condensing all visual tokens of each frame\ninto a limited number). However, this approach often neglects the uneven\ntemporal distribution of critical information across frames, risking the\nomission of keyframes that contain essential temporal and semantic details. To\ntackle these challenges, we propose KFFocus, a method designed to efficiently\ncompress video tokens and emphasize the informative context present within\nvideo frames. We substitute uniform sampling with a refined approach inspired\nby classic video compression principles to identify and capture keyframes based\non their temporal redundancy. By assigning varying condensation ratios to\nframes based on their contextual relevance, KFFocus efficiently reduces token\nredundancy while preserving informative content details. Additionally, we\nintroduce a spatiotemporal modeling module that encodes both the temporal\nrelationships between video frames and the spatial structure within each frame,\nthus providing Vid-LLMs with a nuanced understanding of spatial-temporal\ndynamics. Extensive experiments on widely recognized video understanding\nbenchmarks, especially long video scenarios, demonstrate that KFFocus\nsignificantly outperforms existing methods, achieving substantial computational\nefficiency and enhanced accuracy.", "AI": {"tldr": "KFFocus是一种针对视频LLM的视频令牌高效压缩方法，通过识别关键帧和调整帧内压缩比，同时引入时空建模模块，显著提升了长视频理解的效率和准确性。", "motivation": "现有视频LLM在处理长视频时，因计算量大而采用帧间均匀采样和帧内令牌压缩策略，但这可能导致关键信息（包含重要时序和语义细节的关键帧）的丢失，因为关键信息在时间上分布不均。", "method": "KFFocus通过以下方式解决问题：1. 用基于时间冗余的关键帧识别方法取代均匀采样，以捕获重要帧；2. 根据帧的上下文相关性分配不同的帧内压缩比，以高效减少冗余并保留信息；3. 引入一个时空建模模块，编码帧间时间关系和帧内空间结构，增强对时空动态的理解。", "result": "在广泛认可的视频理解基准测试（特别是长视频场景）中，KFFocus显著优于现有方法，实现了计算效率和准确性的显著提升。", "conclusion": "KFFocus通过智能的关键帧选择、自适应的帧内压缩和精细的时空建模，有效解决了长视频理解中的计算效率和信息丢失问题，为视频LLM提供了更强的长视频理解能力。"}}
{"id": "2508.08991", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08991", "abs": "https://arxiv.org/abs/2508.08991", "authors": ["Zan Wang", "Jingze Zhang", "Yixin Chen", "Baoxiong Jia", "Wei Liang", "Siyuan Huang"], "title": "Spatial-Temporal Multi-Scale Quantization for Flexible Motion Generation", "comment": "18 pages", "summary": "Despite significant advancements in human motion generation, current motion\nrepresentations, typically formulated as discrete frame sequences, still face\ntwo critical limitations: (i) they fail to capture motion from a multi-scale\nperspective, limiting the capability in complex patterns modeling; (ii) they\nlack compositional flexibility, which is crucial for model's generalization in\ndiverse generation tasks. To address these challenges, we introduce MSQ, a\nnovel quantization method that compresses the motion sequence into multi-scale\ndiscrete tokens across spatial and temporal dimensions. MSQ employs distinct\nencoders to capture body parts at varying spatial granularities and temporally\ninterpolates the encoded features into multiple scales before quantizing them\ninto discrete tokens. Building on this representation, we establish a\ngenerative mask modeling model to effectively support motion editing, motion\ncontrol, and conditional motion generation. Through quantitative and\nqualitative analysis, we show that our quantization method enables the seamless\ncomposition of motion tokens without requiring specialized design or\nre-training. Furthermore, extensive evaluations demonstrate that our approach\noutperforms existing baseline methods on various benchmarks.", "AI": {"tldr": "提出MSQ多尺度量化方法，将人体运动序列压缩为时空多尺度离散tokens，解决了现有运动表示在多尺度建模和组合灵活性上的局限性，并支持多种生成任务。", "motivation": "当前人体运动表示（通常为离散帧序列）存在两大局限性：1) 无法从多尺度视角捕捉运动，限制了对复杂模式的建模能力；2) 缺乏组合灵活性，这对于模型在多样化生成任务中的泛化至关重要。", "method": "引入MSQ（多尺度量化）方法，将运动序列压缩为时空多尺度离散tokens。MSQ使用不同的编码器捕捉不同空间粒度的身体部位，并将编码特征在时间上插值到多个尺度，然后进行量化。在此表示基础上，构建了一个生成式掩码建模模型，以支持运动编辑、运动控制和条件运动生成。", "result": "定量和定性分析表明，MSQ量化方法无需特殊设计或重新训练即可实现运动tokens的无缝组合。广泛评估证明，该方法在各种基准测试中优于现有基线方法。", "conclusion": "MSQ通过多尺度离散tokens的创新表示，有效解决了现有运动表示的局限性，显著提升了运动建模的复杂模式捕捉能力和组合灵活性，并在多种生成任务中展现出卓越性能。"}}
{"id": "2508.09000", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09000", "abs": "https://arxiv.org/abs/2508.09000", "authors": ["Yuhao Wang", "Wei Xi"], "title": "UniConvNet: Expanding Effective Receptive Field while Maintaining Asymptotically Gaussian Distribution for ConvNets of Any Scale", "comment": "ICCV 2025", "summary": "Convolutional neural networks (ConvNets) with large effective receptive field\n(ERF), still in their early stages, have demonstrated promising effectiveness\nwhile constrained by high parameters and FLOPs costs and disrupted\nasymptotically Gaussian distribution (AGD) of ERF. This paper proposes an\nalternative paradigm: rather than merely employing extremely large ERF, it is\nmore effective and efficient to expand the ERF while maintaining AGD of ERF by\nproper combination of smaller kernels, such as $7\\times{7}$, $9\\times{9}$,\n$11\\times{11}$. This paper introduces a Three-layer Receptive Field Aggregator\nand designs a Layer Operator as the fundamental operator from the perspective\nof receptive field. The ERF can be expanded to the level of existing\nlarge-kernel ConvNets through the stack of proposed modules while maintaining\nAGD of ERF. Using these designs, we propose a universal model for ConvNet of\nany scale, termed UniConvNet. Extensive experiments on ImageNet-1K, COCO2017,\nand ADE20K demonstrate that UniConvNet outperforms state-of-the-art CNNs and\nViTs across various vision recognition tasks for both lightweight and\nlarge-scale models with comparable throughput. Surprisingly, UniConvNet-T\nachieves $84.2\\%$ ImageNet top-1 accuracy with $30M$ parameters and $5.1G$\nFLOPs. UniConvNet-XL also shows competitive scalability to big data and large\nmodels, acquiring $88.4\\%$ top-1 accuracy on ImageNet. Code and models are\npublicly available at https://github.com/ai-paperwithcode/UniConvNet.", "AI": {"tldr": "本文提出了一种名为UniConvNet的新型卷积神经网络范式，通过组合小尺寸卷积核来高效扩展有效感受野（ERF）并保持其渐近高斯分布（AGD），在多种视觉任务上超越现有SOTA模型。", "motivation": "现有具有大有效感受野的卷积网络（ConvNets）面临参数和计算成本高昂，以及有效感受野的渐近高斯分布（AGD）被破坏的问题。", "method": "论文提出了一种替代范式：通过合理组合较小的卷积核（如7x7, 9x9, 11x11），在扩展有效感受野的同时保持其AGD。为此，引入了“三层感受野聚合器”（Three-layer Receptive Field Aggregator）和“层运算符”（Layer Operator）作为基本操作，并堆叠这些模块构建了UniConvNet模型。", "result": "UniConvNet在ImageNet-1K、COCO2017和ADE20K等数据集上的广泛实验表明，其在轻量级和大型模型上均优于最先进的CNN和ViT。例如，UniConvNet-T以30M参数和5.1G FLOPs实现了84.2%的ImageNet top-1准确率；UniConvNet-XL在ImageNet上达到了88.4%的top-1准确率。", "conclusion": "通过结合小尺寸卷积核以高效扩展感受野并保持其AGD，UniConvNet为任意规模的卷积神经网络提供了一个通用的模型，在多种视觉识别任务上表现出卓越的性能和可扩展性。"}}
{"id": "2508.09009", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09009", "abs": "https://arxiv.org/abs/2508.09009", "authors": ["Luyang Cao", "Han Xu", "Jian Zhang", "Lei Qi", "Jiayi Ma", "Yinghuan Shi", "Yang Gao"], "title": "Towards Perfection: Building Inter-component Mutual Correction for Retinex-based Low-light Image Enhancement", "comment": "This article has been accepted by ACMMM 2025", "summary": "In low-light image enhancement, Retinex-based deep learning methods have\ngarnered significant attention due to their exceptional interpretability. These\nmethods decompose images into mutually independent illumination and reflectance\ncomponents, allows each component to be enhanced separately. In fact, achieving\nperfect decomposition of illumination and reflectance components proves to be\nquite challenging, with some residuals still existing after decomposition. In\nthis paper, we formally name these residuals as inter-component residuals\n(ICR), which has been largely underestimated by previous methods. In our\ninvestigation, ICR not only affects the accuracy of the decomposition but also\ncauses enhanced components to deviate from the ideal outcome, ultimately\nreducing the final synthesized image quality. To address this issue, we propose\na novel Inter-correction Retinex model (IRetinex) to alleviate ICR during the\ndecomposition and enhancement stage. In the decomposition stage, we leverage\ninter-component residual reduction module to reduce the feature similarity\nbetween illumination and reflectance components. In the enhancement stage, we\nutilize the feature similarity between the two components to detect and\nmitigate the impact of ICR within each enhancement unit. Extensive experiments\non three low-light benchmark datasets demonstrated that by reducing ICR, our\nmethod outperforms state-of-the-art approaches both qualitatively and\nquantitatively.", "AI": {"tldr": "本文提出一种新的Inter-correction Retinex模型（IRetinex），通过在分解和增强阶段缓解低光图像增强中Retinex模型固有的“组件间残差”（ICR）问题，显著提升了图像质量。", "motivation": "Retinex基深度学习方法在低光图像增强中因其可解释性而备受关注，但其将图像分解为光照和反射分量时，难以实现完美分解，会留下“组件间残差”（ICR）。这些ICR不仅影响分解精度，还导致增强分量偏离理想结果，最终降低合成图像质量，且此前方法普遍低估了ICR的影响。", "method": "本文提出Inter-correction Retinex模型（IRetinex）。在分解阶段，利用组件间残差减少模块来降低光照和反射分量之间的特征相似性；在增强阶段，利用两分量间的特征相似性来检测并减轻每个增强单元内ICR的影响。", "result": "在三个低光基准数据集上的大量实验表明，通过减少ICR，所提出的方法在定性和定量上均优于现有的最先进方法。", "conclusion": "组件间残差（ICR）是Retinex基低光图像增强方法中一个被低估的关键问题。通过在分解和增强阶段有效缓解ICR，可以显著提高图像增强的质量和性能。"}}
{"id": "2508.09014", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09014", "abs": "https://arxiv.org/abs/2508.09014", "authors": ["Kaiwen Huang", "Tao Zhou", "Huazhu Fu", "Yizhe Zhang", "Yi Zhou", "Xiao-Jun Wu"], "title": "Uncertainty-aware Cross-training for Semi-supervised Medical Image Segmentation", "comment": "14 pages, 10 figures", "summary": "Semi-supervised learning has gained considerable popularity in medical image\nsegmentation tasks due to its capability to reduce reliance on expert-examined\nannotations. Several mean-teacher (MT) based semi-supervised methods utilize\nconsistency regularization to effectively leverage valuable information from\nunlabeled data. However, these methods often heavily rely on the student model\nand overlook the potential impact of cognitive biases within the model.\nFurthermore, some methods employ co-training using pseudo-labels derived from\ndifferent inputs, yet generating high-confidence pseudo-labels from perturbed\ninputs during training remains a significant challenge. In this paper, we\npropose an Uncertainty-aware Cross-training framework for semi-supervised\nmedical image Segmentation (UC-Seg). Our UC-Seg framework incorporates two\ndistinct subnets to effectively explore and leverage the correlation between\nthem, thereby mitigating cognitive biases within the model. Specifically, we\npresent a Cross-subnet Consistency Preservation (CCP) strategy to enhance\nfeature representation capability and ensure feature consistency across the two\nsubnets. This strategy enables each subnet to correct its own biases and learn\nshared semantics from both labeled and unlabeled data. Additionally, we propose\nan Uncertainty-aware Pseudo-label Generation (UPG) component that leverages\nsegmentation results and corresponding uncertainty maps from both subnets to\ngenerate high-confidence pseudo-labels. We extensively evaluate the proposed\nUC-Seg on various medical image segmentation tasks involving different modality\nimages, such as MRI, CT, ultrasound, colonoscopy, and so on. The results\ndemonstrate that our method achieves superior segmentation accuracy and\ngeneralization performance compared to other state-of-the-art semi-supervised\nmethods. Our code will be released at https://github.com/taozh2017/UCSeg.", "AI": {"tldr": "本文提出了UC-Seg，一个不确定性感知交叉训练框架，用于半监督医学图像分割。它通过两个子网络协同工作来减轻认知偏差，并利用不确定性图生成高置信度伪标签，在多模态医学图像分割任务中表现优异。", "motivation": "半监督学习在医学图像分割中广受欢迎，但现有方法存在局限性：基于Mean-Teacher的方法过度依赖学生模型并忽视认知偏差；一些协同训练方法难以从扰动输入中生成高置信度伪标签。", "method": "提出不确定性感知交叉训练框架(UC-Seg)。该框架包含两个独立的子网络，通过探索和利用它们之间的关联来减轻模型中的认知偏差。具体包括：1) 跨子网络一致性保持(CCP)策略，增强特征表示能力并确保特征一致性，使每个子网络能纠正自身偏差并学习共享语义；2) 不确定性感知伪标签生成(UPG)组件，利用两个子网络的分割结果和相应的不确定性图生成高置信度伪标签。", "result": "在MRI、CT、超声、结肠镜等多种模态的医学图像分割任务中对所提出的UC-Seg进行了广泛评估。结果表明，与现有最先进的半监督方法相比，UC-Seg实现了卓越的分割精度和泛化性能。", "conclusion": "UC-Seg框架通过引入双子网络协同和不确定性感知伪标签生成，有效解决了半监督医学图像分割中模型认知偏差和伪标签质量问题，显著提升了分割准确性和泛化能力。"}}
{"id": "2508.09045", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09045", "abs": "https://arxiv.org/abs/2508.09045", "authors": ["Ori Malca", "Dvir Samuel", "Gal Chechik"], "title": "Per-Query Visual Concept Learning", "comment": "Project page is at\n  https://per-query-visual-concept-learning.github.io/", "summary": "Visual concept learning, also known as Text-to-image personalization, is the\nprocess of teaching new concepts to a pretrained model. This has numerous\napplications from product placement to entertainment and personalized design.\nHere we show that many existing methods can be substantially augmented by\nadding a personalization step that is (1) specific to the prompt and noise\nseed, and (2) using two loss terms based on the self- and cross- attention,\ncapturing the identity of the personalized concept. Specifically, we leverage\nPDM features -- previously designed to capture identity -- and show how they\ncan be used to improve personalized semantic similarity. We evaluate the\nbenefit that our method gains on top of six different personalization methods,\nand several base text-to-image models (both UNet- and DiT-based). We find\nsignificant improvements even over previous per-query personalization methods.", "AI": {"tldr": "本文提出一种针对提示词和噪声种子特化的个性化步骤，利用自注意力与交叉注意力损失（基于PDM特征）来显著提升现有文本到图像个性化方法的性能。", "motivation": "视觉概念学习（即文本到图像个性化）在产品植入、娱乐和个性化设计等领域有广泛应用。现有方法仍有提升空间，尤其是在捕获个性化概念的身份方面。", "method": "该方法通过增加一个个性化步骤来增强现有方法，此步骤具备两个特点：1) 针对提示词和噪声种子特定化；2) 使用基于自注意力和交叉注意力的两个损失项，结合PDM特征来捕获个性化概念的身份。", "result": "在六种不同的个性化方法和多种基于UNet和DiT的文本到图像模型上进行评估，结果显示即使相较于先前的按查询个性化方法，本文方法也能带来显著的性能提升。", "conclusion": "通过引入特定的个性化步骤和基于注意力与PDM特征的损失，可以显著改进视觉概念学习和文本到图像个性化的效果。"}}
{"id": "2508.09058", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09058", "abs": "https://arxiv.org/abs/2508.09058", "authors": ["Shanle Yao", "Ghazal Alinezhad Noghre", "Armin Danesh Pazho", "Hamed Tabkhi"], "title": "ALFred: An Active Learning Framework for Real-world Semi-supervised Anomaly Detection with Adaptive Thresholds", "comment": null, "summary": "Video Anomaly Detection (VAD) can play a key role in spotting unusual\nactivities in video footage. VAD is difficult to use in real-world settings due\nto the dynamic nature of human actions, environmental variations, and domain\nshifts. Traditional evaluation metrics often prove inadequate for such\nscenarios, as they rely on static assumptions and fall short of identifying a\nthreshold that distinguishes normal from anomalous behavior in dynamic\nsettings. To address this, we introduce an active learning framework tailored\nfor VAD, designed for adapting to the ever-changing real-world conditions. Our\napproach leverages active learning to continuously select the most informative\ndata points for labeling, thereby enhancing model adaptability. A critical\ninnovation is the incorporation of a human-in-the-loop mechanism, which enables\nthe identification of actual normal and anomalous instances from\npseudo-labeling results generated by AI. This collected data allows the\nframework to define an adaptive threshold tailored to different environments,\nensuring that the system remains effective as the definition of 'normal' shifts\nacross various settings. Implemented within a lab-based framework that\nsimulates real-world conditions, our approach allows rigorous testing and\nrefinement of VAD algorithms with a new metric. Experimental results show that\nour method achieves an EBI (Error Balance Index) of 68.91 for Q3 in real-world\nsimulated scenarios, demonstrating its practical effectiveness and\nsignificantly enhancing the applicability of VAD in dynamic environments.", "AI": {"tldr": "本文提出了一种针对视频异常检测（VAD）的主动学习框架，该框架结合了人工干预，以适应不断变化的真实世界条件，并动态定义正常与异常行为的区分阈值。", "motivation": "视频异常检测在现实世界中应用面临挑战，原因在于人类行为的动态性、环境变化和领域漂移。传统的评估指标依赖静态假设，无法在动态环境中有效区分正常与异常，因此需要一种能适应这些变化的解决方案。", "method": "引入了一个主动学习框架，持续选择最具信息量的数据点进行标注，以增强模型适应性。关键创新是引入了“人机协作”机制，通过人工干预来识别AI伪标签结果中的真实正常和异常实例。收集到的数据用于定义适应不同环境的动态阈值。该方法在一个模拟真实世界的实验室框架中实现，并使用新的评估指标进行测试。", "result": "在模拟真实世界的场景中，该方法在Q3指标上取得了68.91的EBI（错误平衡指数），证明了其实际有效性。", "conclusion": "该方法有效提升了视频异常检测在动态环境中的实用性和适用性，通过主动学习和人工干预实现了对不断变化的现实条件的适应性，并能定义自适应阈值。"}}
{"id": "2508.09061", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09061", "abs": "https://arxiv.org/abs/2508.09061", "authors": ["Fuhao Chang", "Shuxin Li", "Yabei Li", "Lei He"], "title": "VLM-3D:End-to-End Vision-Language Models for Open-World 3D Perception", "comment": null, "summary": "Open-set perception in complex traffic environments poses a critical\nchallenge for autonomous driving systems, particularly in identifying\npreviously unseen object categories, which is vital for ensuring safety. Visual\nLanguage Models (VLMs), with their rich world knowledge and strong semantic\nreasoning capabilities, offer new possibilities for addressing this task.\nHowever, existing approaches typically leverage VLMs to extract visual features\nand couple them with traditional object detectors, resulting in multi-stage\nerror propagation that hinders perception accuracy. To overcome this\nlimitation, we propose VLM-3D, the first end-to-end framework that enables VLMs\nto perform 3D geometric perception in autonomous driving scenarios. VLM-3D\nincorporates Low-Rank Adaptation (LoRA) to efficiently adapt VLMs to driving\ntasks with minimal computational overhead, and introduces a joint\nsemantic-geometric loss design: token-level semantic loss is applied during\nearly training to ensure stable convergence, while 3D IoU loss is introduced in\nlater stages to refine the accuracy of 3D bounding box predictions. Evaluations\non the nuScenes dataset demonstrate that the proposed joint semantic-geometric\nloss in VLM-3D leads to a 12.8% improvement in perception accuracy, fully\nvalidating the effectiveness and advancement of our method.", "AI": {"tldr": "VLM-3D是一个端到端的框架，利用视觉语言模型（VLMs）解决自动驾驶中的开放集3D感知问题，通过LoRA和联合语义-几何损失提高感知精度。", "motivation": "自动驾驶系统在复杂交通环境中识别未曾见过的物体类别（开放集感知）是一个关键挑战，现有利用VLM的方法存在多阶段误差传播，影响感知精度。", "method": "提出了VLM-3D，首个使VLM能够进行3D几何感知的端到端框架。它采用低秩适应（LoRA）高效地将VLM适应于驾驶任务，并引入了联合语义-几何损失设计：早期训练使用token级语义损失确保稳定收敛，后期引入3D IoU损失以提高3D边界框预测精度。", "result": "在nuScenes数据集上的评估表明，VLM-3D中提出的联合语义-几何损失使感知精度提高了12.8%。", "conclusion": "VLM-3D及其联合语义-几何损失的有效性得到了充分验证，证明了该方法在自动驾驶3D感知领域的先进性。"}}
{"id": "2508.09075", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09075", "abs": "https://arxiv.org/abs/2508.09075", "authors": ["Yuqi Li", "Haotian Zhang", "Li Li", "Dong Liu", "Feng Wu"], "title": "Scaling Learned Image Compression Models up to 1 Billion", "comment": "11 pages, technical report", "summary": "Recent advances in large language models (LLMs) highlight a strong connection\nbetween intelligence and compression. Learned image compression, a fundamental\ntask in modern data compression, has made significant progress in recent years.\nHowever, current models remain limited in scale, restricting their\nrepresentation capacity, and how scaling model size influences compression\nperformance remains unexplored. In this work, we present a pioneering study on\nscaling up learned image compression models and revealing the performance\ntrends through scaling laws. Using the recent state-of-the-art HPCM model as\nbaseline, we scale model parameters from 68.5 millions to 1 billion and fit\npower-law relations between test loss and key scaling variables, including\nmodel size and optimal training compute. The results reveal a scaling trend,\nenabling extrapolation to larger scale models. Experimental results demonstrate\nthat the scaled-up HPCM-1B model achieves state-of-the-art rate-distortion\nperformance. We hope this work inspires future exploration of large-scale\ncompression models and deeper investigations into the connection between\ncompression and intelligence.", "AI": {"tldr": "本文首次研究了学习型图像压缩模型的规模扩展，揭示了性能与模型大小及计算量的标度律关系，并展示了大规模模型在压缩性能上达到SOTA。", "motivation": "大型语言模型（LLMs）的进步揭示了智能与压缩之间的紧密联系。尽管学习型图像压缩已取得进展，但当前模型规模有限，限制了其表示能力，且模型规模如何影响压缩性能尚不明确。", "method": "以最先进的HPCM模型为基线，将模型参数从6850万扩展到10亿。通过拟合测试损失与模型大小、最优训练计算量等关键标度变量之间的幂律关系，揭示了性能趋势。", "result": "研究揭示了模型性能的标度趋势，使得能够外推到更大规模的模型。实验结果表明，扩展后的HPCM-1B模型实现了最先进的率失真性能。", "conclusion": "这项工作有望启发未来对大规模压缩模型的探索，并促进对压缩与智能之间联系的更深入研究。"}}
{"id": "2508.09087", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09087", "abs": "https://arxiv.org/abs/2508.09087", "authors": ["Ahsan Habib Akash", "Greg Murray", "Annahita Amireskandari", "Joel Palko", "Carol Laxson", "Binod Bhattarai", "Prashnna Gyawali"], "title": "Addressing Bias in VLMs for Glaucoma Detection Without Protected Attribute Supervision", "comment": "3rd Workshop in Data Engineering in Medical Imaging (DEMI),\n  MICCAI-2025 Workshop", "summary": "Vision-Language Models (VLMs) have achieved remarkable success on multimodal\ntasks such as image-text retrieval and zero-shot classification, yet they can\nexhibit demographic biases even when explicit protected attributes are absent\nduring training. In this work, we focus on automated glaucoma screening from\nretinal fundus images, a critical application given that glaucoma is a leading\ncause of irreversible blindness and disproportionately affects underserved\npopulations. Building on a reweighting-based contrastive learning framework, we\nintroduce an attribute-agnostic debiasing method that (i) infers proxy\nsubgroups via unsupervised clustering of image-image embeddings, (ii) computes\ngradient-similarity weights between the CLIP-style multimodal loss and a\nSimCLR-style image-pair contrastive loss, and (iii) applies these weights in a\njoint, top-$k$ weighted objective to upweight underperforming clusters. This\nlabel-free approach adaptively targets the hardest examples, thereby reducing\nsubgroup disparities. We evaluate our method on the Harvard FairVLMed glaucoma\nsubset, reporting Equalized Odds Distance (EOD), Equalized Subgroup AUC (ES\nAUC), and Groupwise AUC to demonstrate equitable performance across inferred\ndemographic subgroups.", "AI": {"tldr": "本文提出一种针对视觉语言模型（VLMs）的无属性去偏方法，通过无监督聚类和梯度相似性加权来提升医疗图像（青光眼筛查）中表现不佳的子组，从而减少人口统计学偏见和不平等。", "motivation": "视觉语言模型（VLMs）在多模态任务中取得了显著成功，但即使在训练中没有明确的受保护属性，它们也可能表现出人口统计学偏见。青光眼筛查是一个关键的医疗应用，因为它会导致不可逆转的失明，并且不成比例地影响服务不足的人群，因此解决VLM在该应用中的偏见问题至关重要。", "method": "该研究引入了一种基于重加权对比学习框架的无属性去偏方法。具体步骤包括：(i) 通过对图像-图像嵌入进行无监督聚类来推断代理子组；(ii) 计算CLIP风格的多模态损失与SimCLR风格的图像对对比损失之间的梯度相似性权重；(iii) 将这些权重应用于一个联合的、top-k加权目标，以提高表现不佳聚类的权重。该方法是无标签的，并自适应地针对最难的样本，旨在减少子组差异。", "result": "该方法在Harvard FairVLMed青光眼子集上进行了评估，并报告了均衡赔率距离（EOD）、均衡子组AUC（ES AUC）和组间AUC。结果表明，该方法在推断的人口统计学子组中实现了公平的性能。", "conclusion": "该研究提出的无属性去偏方法能够有效减少VLMs在自动化青光眼筛查中可能存在的子组不平等，从而在关键医疗应用中实现更公平的性能，特别是在服务不足人群中。"}}
{"id": "2508.09094", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09094", "abs": "https://arxiv.org/abs/2508.09094", "authors": ["Oleksandr Kuznetsov", "Emanuele Frontoni", "Luca Romeo", "Riccardo Rosati", "Andrea Maranesi", "Alessandro Muscatello"], "title": "Deep Learning Models for Robust Facial Liveness Detection", "comment": null, "summary": "In the rapidly evolving landscape of digital security, biometric\nauthentication systems, particularly facial recognition, have emerged as\nintegral components of various security protocols. However, the reliability of\nthese systems is compromised by sophisticated spoofing attacks, where imposters\ngain unauthorized access by falsifying biometric traits. Current literature\nreveals a concerning gap: existing liveness detection methodologies - designed\nto counteract these breaches - fall short against advanced spoofing tactics\nemploying deepfakes and other artificial intelligence-driven manipulations.\nThis study introduces a robust solution through novel deep learning models\naddressing the deficiencies in contemporary anti-spoofing techniques. By\ninnovatively integrating texture analysis and reflective properties associated\nwith genuine human traits, our models distinguish authentic presence from\nreplicas with remarkable precision. Extensive evaluations were conducted across\nfive diverse datasets, encompassing a wide range of attack vectors and\nenvironmental conditions. Results demonstrate substantial advancement over\nexisting systems, with our best model (AttackNet V2.2) achieving 99.9% average\naccuracy when trained on combined data. Moreover, our research unveils critical\ninsights into the behavioral patterns of impostor attacks, contributing to a\nmore nuanced understanding of their evolving nature. The implications are\nprofound: our models do not merely fortify the authentication processes but\nalso instill confidence in biometric systems across various sectors reliant on\nsecure access.", "AI": {"tldr": "本研究提出了一种基于深度学习的新型面部识别防欺骗方案，通过整合纹理分析和反射特性，有效区分真实人脸和伪造攻击，显著提升了生物识别系统的安全性。", "motivation": "现有的活体检测技术在面对深度伪造（deepfakes）等AI驱动的复杂欺骗攻击时表现不足，严重影响了面部识别等生物识别认证系统的可靠性，因此需要更鲁棒的解决方案。", "method": "研究引入了新颖的深度学习模型（例如AttackNet V2.2），创新性地整合了纹理分析和真实人类特征相关的反射特性，以区分真实存在和伪造复制品。模型在五个多样化数据集上进行了广泛评估。", "result": "实验结果显示，该方法比现有系统有显著进步，其中最佳模型AttackNet V2.2在组合数据训练下达到了99.9%的平均准确率。此外，研究还揭示了欺骗攻击行为模式的关键见解。", "conclusion": "本研究的模型不仅增强了认证过程，还提升了各行各业对生物识别系统的信心，为安全访问提供了更可靠的保障。"}}
{"id": "2508.09136", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09136", "abs": "https://arxiv.org/abs/2508.09136", "authors": ["Ya Zou", "Jingfeng Yao", "Siyuan Yu", "Shuai Zhang", "Wenyu Liu", "Xinggang Wang"], "title": "Turbo-VAED: Fast and Stable Transfer of Video-VAEs to Mobile Devices", "comment": null, "summary": "There is a growing demand for deploying large generative AI models on mobile\ndevices. For recent popular video generative models, however, the Variational\nAutoEncoder (VAE) represents one of the major computational bottlenecks. Both\nlarge parameter sizes and mismatched kernels cause out-of-memory errors or\nextremely slow inference on mobile devices. To address this, we propose a\nlow-cost solution that efficiently transfers widely used video VAEs to mobile\ndevices. (1) We analyze redundancy in existing VAE architectures and get\nempirical design insights. By integrating 3D depthwise separable convolutions\ninto our model, we significantly reduce the number of parameters. (2) We\nobserve that the upsampling techniques in mainstream video VAEs are poorly\nsuited to mobile hardware and form the main bottleneck. In response, we propose\na decoupled 3D pixel shuffle scheme that slashes end-to-end delay. Building\nupon these, we develop a universal mobile-oriented VAE decoder, Turbo-VAED. (3)\nWe propose an efficient VAE decoder training method. Since only the decoder is\nused during deployment, we distill it to Turbo-VAED instead of retraining the\nfull VAE, enabling fast mobile adaptation with minimal performance loss. To our\nknowledge, our method enables real-time 720p video VAE decoding on mobile\ndevices for the first time. This approach is widely applicable to most video\nVAEs. When integrated into four representative models, with training cost as\nlow as $95, it accelerates original VAEs by up to 84.5x at 720p resolution on\nGPUs, uses as low as 17.5% of original parameter count, and retains 96.9% of\nthe original reconstruction quality. Compared to mobile-optimized VAEs,\nTurbo-VAED achieves a 2.9x speedup in FPS and better reconstruction quality on\nthe iPhone 16 Pro. The code and models will soon be available at\nhttps://github.com/hustvl/Turbo-VAED.", "AI": {"tldr": "该研究提出了一种名为Turbo-VAED的低成本解决方案，通过优化架构和训练方法，高效地将视频VAE部署到移动设备上，解决了其计算瓶颈。", "motivation": "将大型生成式AI模型部署到移动设备的需求日益增长，但视频生成模型中的变分自编码器（VAE）因参数量大和内核不匹配，导致在移动设备上出现内存不足或推理速度极慢的问题，成为主要计算瓶颈。", "method": "1. 通过分析现有VAE架构的冗余性，并集成3D深度可分离卷积，显著减少了模型参数。2. 针对主流视频VAE中不适合移动硬件的升采样技术，提出了一种解耦的3D像素重排方案，以降低端到端延迟，并在此基础上开发了通用移动端VAE解码器Turbo-VAED。3. 提出了一种高效的VAE解码器训练方法，通过将解码器蒸馏到Turbo-VAED，而非重新训练整个VAE，实现了快速移动适应和最小的性能损失。", "result": "该方法首次实现了移动设备上的实时720p视频VAE解码，并广泛适用于大多数视频VAE。将其集成到四个代表性模型中，训练成本低至95美元，在GPU上将原始VAE在720p分辨率下加速高达84.5倍，参数量仅为原始的17.5%，并保持了96.9%的原始重建质量。与移动优化的VAE相比，Turbo-VAED在iPhone 16 Pro上实现了2.9倍的帧率提升和更好的重建质量。", "conclusion": "Turbo-VAED提供了一种低成本且高效的解决方案，能够将广泛使用的视频VAE成功迁移到移动设备上，显著提升了推理速度和效率，同时保持了高重建质量，克服了现有VAE在移动部署中的主要障碍。"}}
{"id": "2508.09137", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.09137", "abs": "https://arxiv.org/abs/2508.09137", "authors": ["Timo Teufel", "Pulkit Gera", "Xilong Zhou", "Umar Iqbal", "Pramod Rao", "Jan Kautz", "Vladislav Golyanik", "Christian Theobalt"], "title": "HumanOLAT: A Large-Scale Dataset for Full-Body Human Relighting and Novel-View Synthesis", "comment": "TT and PG contributed equally; accepted at ICCV 2025; project page:\n  https://vcai.mpi-inf.mpg.de/projects/HumanOLAT/", "summary": "Simultaneous relighting and novel-view rendering of digital human\nrepresentations is an important yet challenging task with numerous\napplications. Progress in this area has been significantly limited due to the\nlack of publicly available, high-quality datasets, especially for full-body\nhuman captures. To address this critical gap, we introduce the HumanOLAT\ndataset, the first publicly accessible large-scale dataset of multi-view\nOne-Light-at-a-Time (OLAT) captures of full-body humans. The dataset includes\nHDR RGB frames under various illuminations, such as white light, environment\nmaps, color gradients and fine-grained OLAT illuminations. Our evaluations of\nstate-of-the-art relighting and novel-view synthesis methods underscore both\nthe dataset's value and the significant challenges still present in modeling\ncomplex human-centric appearance and lighting interactions. We believe\nHumanOLAT will significantly facilitate future research, enabling rigorous\nbenchmarking and advancements in both general and human-specific relighting and\nrendering techniques.", "AI": {"tldr": "本文介绍了HumanOLAT数据集，这是首个公开的大规模多视角全身人体“逐光”捕捉数据集，旨在解决数字人像重打光和新视角渲染领域高质量数据集缺失的问题。", "motivation": "数字人像的同步重打光和新视角渲染是一项重要但具有挑战性的任务，其进展受到高质量公开数据集（特别是全身人体捕捉数据）严重限制。", "method": "引入了HumanOLAT数据集，通过多视角“逐光”（OLAT）捕捉全身人体，包含白光、环境贴图、颜色渐变和细粒度OLAT照明下的HDR RGB帧。", "result": "对现有最先进的重打光和新视角合成方法进行评估，结果突出了该数据集的价值，并揭示了在建模复杂以人为中心的外观和光照交互方面仍存在的显著挑战。", "conclusion": "HumanOLAT数据集将极大地促进未来研究，为通用和针对人类的重打光和渲染技术提供严格的基准测试和进步机会。"}}
