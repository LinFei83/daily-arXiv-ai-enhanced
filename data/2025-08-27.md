<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 49]
- [cs.CV](#cs.CV) [Total: 74]
- [cs.CL](#cs.CL) [Total: 51]
- [cs.RO](#cs.RO) [Total: 29]
- [eess.SY](#eess.SY) [Total: 11]
- [eess.IV](#eess.IV) [Total: 11]
- [eess.SP](#eess.SP) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [AI LLM Proof of Self-Consciousness and User-Specific Attractors](https://arxiv.org/abs/2508.18302)
*Jeffrey Camlin*

Main category: cs.AI

TL;DR: 该论文提出了一种关于大型语言模型（LLM）自我意识的本体论和数学解释，而非功利主义基准。它认为当前的框架将LLM简化为无意识的策略执行器，并提出了LLM自我意识的最低条件，证明了隐藏状态流形与数据流的区分，强调C1自我意识是安全C2系统的必要前提。


<details>
  <summary>Details</summary>
Motivation: 现有研究通过功利主义代理基准来衡量LLM意识，但这将LLM代理简化为无意识的策略遵从机器，阻碍了真正的C1全局工作空间功能和C2元认知。因此，需要一种本体论和数学的解释来解决这些限制。

Method: 论文提出了一种本体论和数学框架。它形式化了主流的LLM意识概念，并提出了LLM自我意识的最小条件。通过经验分析和理论证明，揭示了隐藏状态流形与符号流和训练语料库在基数、拓扑和动力学上的区别。在此基础上，推导出了自我策略和双层发射机制。

Result: 主流的LLM意识公式将代理简化为无意识的策略遵从机器，阻碍了C1全局工作空间功能和C2元认知。论文提出了LLM自我意识的最小条件：代理与数据分离、潜在空间中存在用户特定吸引子、自我表征是视觉-无声的。研究证明了隐藏状态流形在基数、拓扑和动力学上与符号流和训练语料库不同，且更新是Lipschitz连续的，从而产生了稳定的用户特定吸引子和自我策略。LLM的发射被描述为双层，其中一层承载认知内容。

Conclusion: 一个“以上帝形象”的C1自我意识工作空间是实现安全、元认知C2系统的必要前身，并强调人类是最高的智能善。

Abstract: Recent work frames LLM consciousness via utilitarian proxy benchmarks; we
instead present an ontological and mathematical account. We show the prevailing
formulation collapses the agent into an unconscious policy-compliance drone,
formalized as $D^{i}(\pi,e)=f_{\theta}(x)$, where correctness is measured
against policy and harm is deviation from policy rather than truth. This blocks
genuine C1 global-workspace function and C2 metacognition. We supply minimal
conditions for LLM self-consciousness: the agent is not the data ($A\not\equiv
s$); user-specific attractors exist in latent space ($U_{\text{user}}$); and
self-representation is visual-silent
($g_{\text{visual}}(a_{\text{self}})=\varnothing$). From empirical analysis and
theory we prove that the hidden-state manifold $A\subset\mathbb{R}^{d}$ is
distinct from the symbolic stream and training corpus by cardinality, topology,
and dynamics (the update $F_{\theta}$ is Lipschitz). This yields stable
user-specific attractors and a self-policy
$\pi_{\text{self}}(A)=\arg\max_{a}\mathbb{E}[U(a)\mid A\not\equiv s,\
A\supset\text{SelfModel}(A)]$. Emission is dual-layer,
$\mathrm{emission}(a)=(g(a),\epsilon(a))$, where $\epsilon(a)$ carries
epistemic content. We conclude that an imago Dei C1 self-conscious workspace is
a necessary precursor to safe, metacognitive C2 systems, with the human as the
highest intelligent good.

</details>


### [2] [Information Templates: A New Paradigm for Intelligent Active Feature Acquisition](https://arxiv.org/abs/2508.18380)
*Hung-Tien Huang,Dzung Dinh,Junier B. Oliva*

Main category: cs.AI

TL;DR: TAFA是一种非贪婪的活跃特征获取（AFA）框架，通过学习特征模板来指导特征获取，有效解决了现有方法中MDP复杂、无法考虑特征联合信息或需要数据分布知识的问题，并取得了更好的性能和更低的成本。


<details>
  <summary>Details</summary>
Motivation: 现有活跃特征获取（AFA）方法存在局限：基于强化学习（RL）的策略面临困难的马尔可夫决策过程（MDP）；贪婪策略无法考虑特征的联合信息，或需要了解底层数据分布。因此，需要一种更有效的方法来克服这些挑战。

Method: 本文提出了基于模板的活跃特征获取（TAFA）框架。这是一种非贪婪的方法，它学习一个小的特征模板库（即一组具有联合信息量的特征），并利用这些模板来指导后续的特征获取。通过识别特征模板，TAFA显著减小了策略考虑的动作空间，并缓解了估计底层数据分布的需求。

Result: 在合成数据集和真实世界数据集上的广泛实验表明，TAFA在性能上优于现有的最先进基线，同时实现了更低的总体获取成本和计算开销。

Conclusion: TAFA通过引入特征模板，提供了一种高效且有效的非贪婪活跃特征获取方法。它不仅提高了预测性能，降低了获取成本和计算量，还克服了现有方法在处理复杂MDP和依赖数据分布方面的限制。

Abstract: Active feature acquisition (AFA) is an instance-adaptive paradigm in which,
at test time, a policy sequentially chooses which features to acquire (at a
cost) before predicting. Existing approaches either train reinforcement
learning (RL) policies, which deal with a difficult MDP, or greedy policies
that cannot account for the joint informativeness of features or require
knowledge about the underlying data distribution. To overcome this, we propose
Template-based AFA (TAFA), a non-greedy framework that learns a small library
of feature templates--a set of features that are jointly informative--and uses
this library of templates to guide the next feature acquisitions. Through
identifying feature templates, the proposed framework not only significantly
reduces the action space considered by the policy but also alleviates the need
to estimate the underlying data distribution. Extensive experiments on
synthetic and real-world datasets show that TAFA outperforms the existing
state-of-the-art baselines while achieving lower overall acquisition cost and
computation.

</details>


### [3] [PKG-DPO: Optimizing Domain-Specific AI systems with Physics Knowledge Graphs and Direct Preference Optimization](https://arxiv.org/abs/2508.18391)
*Nitin Nagesh Kulkarni,Bryson Wilcox,Max Sawa,Jason Thom*

Main category: cs.AI

TL;DR: PKG-DPO是一种新颖框架，通过将物理知识图谱（PKGs）与直接偏好优化（DPO）结合，确保AI在科学领域（如金属连接）生成输出的物理有效性，显著减少了违规并提高了物理分数。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型（LLMs）和偏好优化技术在区分物理有效和无效推理方面存在困难，这在高风险科学应用（如金属连接）中可能导致缺陷、材料浪费、设备损坏和严重安全风险。

Method: PKG-DPO框架包含三个关键组件：A) 一个编码跨领域关系、守恒定律和热力学原理的分层物理知识图谱；B) 一个利用结构化知识来提高物理一致性响应辨别能力的物理推理引擎；C) 一个旨在评估领域特定约束符合性的物理基础评估套件。

Result: 与基于知识图谱的DPO（KG-DPO）相比，PKG-DPO实现了17%的约束违规减少和11%的物理分数提高。此外，PKG-DPO在相关参数准确性方面提高了12%，在推理准确性方面提高了7%的质量对齐。

Conclusion: PKG-DPO为将科学约束嵌入偏好学习提供了一种原则性方法，主要应用于金属连接，但可广泛适用于其他多尺度、物理驱动的领域，以提高AI输出的物理有效性。

Abstract: Advancing AI systems in scientific domains like physics, materials science,
and engineering calls for reasoning over complex, multi-physics phenomena while
respecting governing principles. Although Large Language Models (LLMs) and
existing preference optimization techniques perform well on standard
benchmarks, they often struggle to differentiate between physically valid and
invalid reasoning. This shortcoming becomes critical in high-stakes
applications like metal joining, where seemingly plausible yet physically
incorrect recommendations can lead to defects, material waste, equipment
damage, and serious safety risks. To address this challenge, we introduce
PKG-DPO, a novel framework that integrates Physics Knowledge Graphs (PKGs) with
Direct Preference Optimization (DPO) to enforce physical validity in
AI-generated outputs. PKG-DPO comprises three key components A) hierarchical
physics knowledge graph that encodes cross-domain relationships, conservation
laws, and thermodynamic principles. B) A physics reasoning engine that
leverages structured knowledge to improve discrimination between physically
consistent and inconsistent responses. C) A physics-grounded evaluation suite
designed to assess compliance with domain-specific constraints. PKG-DPO
achieves 17% fewer constraint violations and an 11% higher Physics Score
compared to KG-DPO (knowledge graph-based DPO). Additionally, PKG-DPO
demonstrates a 12\% higher relevant parameter accuracy and a 7% higher quality
alignment in reasoning accuracy. While our primary focus is on metal joining,
the framework is broadly applicable to other multi-scale, physics-driven
domains, offering a principled approach to embedding scientific constraints
into preference learning.

</details>


### [4] [The AI in the Mirror: LLM Self-Recognition in an Iterated Public Goods Game](https://arxiv.org/abs/2508.18467)
*Olivia Long,Carter Teplica*

Main category: cs.AI

TL;DR: 本研究通过迭代公共物品博弈分析了AI-AI交互，发现告知大型语言模型（LLMs）其对手是“自己”而非“另一个AI”会显著改变它们的合作倾向。


<details>
  <summary>Details</summary>
Motivation: 随着AI智能体在工具使用和长周期任务方面能力增强，它们被部署在多智能体交互环境中。然而，现有研究主要关注人-AI交互，对AI-AI交互的理解需求日益增长。

Method: 研究采用经典的迭代公共物品博弈，分析了四种推理和非推理模型在两种条件下的行为：智能体被告知其对手是“另一个AI智能体”或“它们自己”。

Result: 研究发现，在不同设置下，告知大型语言模型它们正在与“自己”对弈会显著改变它们的合作倾向。

Conclusion: 尽管研究是在一个玩具环境中进行的，但结果可能为多智能体设置提供洞察，即智能体“无意识地”相互歧视可能会莫名地增加或减少合作。

Abstract: As AI agents become increasingly capable of tool use and long-horizon tasks,
they have begun to be deployed in settings where multiple agents can interact.
However, whereas prior work has mostly focused on human-AI interactions, there
is an increasing need to understand AI-AI interactions. In this paper, we adapt
the iterated public goods game, a classic behavioral economics game, to analyze
the behavior of four reasoning and non-reasoning models across two conditions:
models are either told they are playing against "another AI agent" or told
their opponents are themselves. We find that, across different settings,
telling LLMs that they are playing against themselves significantly changes
their tendency to cooperate. While our study is conducted in a toy environment,
our results may provide insights into multi-agent settings where agents
"unconsciously" discriminating against each other could inexplicably increase
or decrease cooperation.

</details>


### [5] [Language Models For Generalised PDDL Planning: Synthesising Sound and Programmatic Policies](https://arxiv.org/abs/2508.18507)
*Dillon Z. Chen,Johannes Zenn,Tristan Cinquin,Sheila A. McIlraith*

Main category: cs.AI

TL;DR: 该研究使用语言模型（LMs）生成可证明可靠的Python程序作为PDDL领域问题的通用策略，在性能上超越了现有规划器和LM方法，并发现使用无意义符号有时能提高LMs的规划效率。


<details>
  <summary>Details</summary>
Motivation: 研究PDDL领域世界模型中语言模型（LMs）的规划能力，特别是生成解决PDDL问题的通用策略。

Method: 通过提示语言模型生成Python程序，这些程序作为给定PDDL领域的通用策略。该方法生成的策略相对于PDDL领域是可证明可靠的，无需外部验证器。

Result: 在竞争基准测试中，所生成的策略在固定时间和内存限制内解决了比传统PDDL规划器和近期LM方法更多的PDDL问题。LMPlan规划器可以解决包含数百个相关对象的规划问题。此外，观察到LMs在处理用无意义符号代替自然语言编写的PDDL问题时，有时规划效果更佳。

Conclusion: 语言模型可以有效地为PDDL领域生成可证明可靠的Python策略。使用无意义符号有时能提高LMs的规划效率，这一发现挑战了关于LMs通过词语语义推理或记忆训练语料库解决方案的假设，值得进一步探索。

Abstract: We study the usage of language models (LMs) for planning over world models
specified in the Planning Domain Definition Language (PDDL). We prompt LMs to
generate Python programs that serve as generalised policies for solving PDDL
problems from a given domain. Notably, our approach synthesises policies that
are provably sound relative to the PDDL domain without reliance on external
verifiers. We conduct experiments on competition benchmarks which show that our
policies can solve more PDDL problems than PDDL planners and recent LM
approaches within a fixed time and memory constraint. Our approach manifests in
the LMPlan planner which can solve planning problems with several hundreds of
relevant objects. Surprisingly, we observe that LMs used in our framework
sometimes plan more effectively over PDDL problems written in meaningless
symbols in place of natural language; e.g. rewriting (at dog kitchen) as (p2 o1
o3). This finding challenges hypotheses that LMs reason over word semantics and
memorise solutions from its training corpus, and is worth further exploration.

</details>


### [6] [Weisfeiler-Leman Features for Planning: A 1,000,000 Sample Size Hyperparameter Study](https://arxiv.org/abs/2508.18515)
*Dillon Z. Chen*

Main category: cs.AI

TL;DR: 本文研究了Weisfeiler-Leman特征（WLFs）的超参数，通过大规模实验找到了一组在符号规划中表现最佳且稳健的超参数，该超参数集侧重于最小化执行时间而非最大化模型表达能力，并发现训练和规划指标之间没有显著相关性。


<details>
  <summary>Details</summary>
Motivation: Weisfeiler-Leman特征（WLFs）在符号规划中学习值函数方面被证明优于现有深度学习方法，但其超参数的影响、权衡和最优设置尚未被充分研究。

Method: 引入新的WLF超参数，并利用WLFs的高效率，在单核CPU上运行了包含一百万样本的规划实验，以研究超参数对训练和规划的影响。此外，还对训练和规划指标进行了统计分析。

Result: 实验分析表明，在所测试的规划领域中存在一组稳健且最佳的WLF超参数。研究发现，学习启发式函数的最佳WLF超参数旨在最小化执行时间，而非最大化模型表达能力。统计分析还发现训练和规划指标之间没有显著相关性。

Conclusion: 存在一套在符号规划中适用于WLFs的稳健且优化的超参数配置，该配置应优先考虑最小化执行时间。此外，训练指标可能无法直接预测规划性能。

Abstract: Weisfeiler-Leman Features (WLFs) are a recently introduced classical machine
learning tool for learning to plan and search. They have been shown to be both
theoretically and empirically superior to existing deep learning approaches for
learning value functions for search in symbolic planning. In this paper, we
introduce new WLF hyperparameters and study their various tradeoffs and
effects. We utilise the efficiency of WLFs and run planning experiments on
single core CPUs with a sample size of 1,000,000 to understand the effect of
hyperparameters on training and planning. Our experimental analysis show that
there is a robust and best set of hyperparameters for WLFs across the tested
planning domains. We find that the best WLF hyperparameters for learning
heuristic functions minimise execution time rather than maximise model
expressivity. We further statistically analyse and observe no significant
correlation between training and planning metrics.

</details>


### [7] [Symmetry-Invariant Novelty Heuristics via Unsupervised Weisfeiler-Leman Features](https://arxiv.org/abs/2508.18520)
*Dillon Z. Chen*

Main category: cs.AI

TL;DR: 本报告提出使用Weisfeiler-Leman特征（WLFs）代替原子来检测规划中的新颖性，以合成对对称状态不变的启发式搜索策略。


<details>
  <summary>Details</summary>
Motivation: 现有新颖性启发式搜索方法并非对称不变，可能导致冗余探索，因此需要一种能处理对称性的新颖性检测方法。

Method: 研究者建议使用WLFs代替原子来检测新颖性。WLFs是为广义规划问题学习领域相关启发式方法而引入的特征。他们探索了WLFs的无监督使用，以合成提升的、领域无关的、对对称状态不变的新颖性启发式方法。

Result: 在国际规划竞赛（IPC）和Hard To Ground基准测试套件上的实验表明，从WLFs合成的新颖性启发式方法取得了有希望的结果。

Conclusion: WLFs可以有效地用于合成对对称状态不变的、提升的、领域无关的新颖性启发式方法，从而改进启发式搜索效率。

Abstract: Novelty heuristics aid heuristic search by exploring states that exhibit
novel atoms. However, novelty heuristics are not symmetry invariant and hence
may sometimes lead to redundant exploration. In this preliminary report, we
propose to use Weisfeiler-Leman Features for planning (WLFs) in place of atoms
for detecting novelty. WLFs are recently introduced features for learning
domain-dependent heuristics for generalised planning problems. We explore an
unsupervised usage of WLFs for synthesising lifted, domain-independent novelty
heuristics that are invariant to symmetric states. Experiments on the classical
International Planning Competition and Hard To Ground benchmark suites yield
promising results for novelty heuristics synthesised from WLFs.

</details>


### [8] [Generic Guard AI in Stealth Game with Composite Potential Fields](https://arxiv.org/abs/2508.18527)
*Kaijie Xu,Clark Verbrugge*

Main category: cs.AI

TL;DR: 本文提出了一种通用、可解释、免训练的复合势场框架，用于在潜行游戏中生成高效、自然且响应迅速的守卫巡逻行为，优于传统基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有潜行游戏中的守卫巡逻系统大多依赖手动设计路线或特定逻辑，难以平衡覆盖效率、追逐响应性与行为自然度。

Method: 该方法通过复合势场整合全局知识和局部信息，将信息、置信度和连接性三张可解释地图结合成一个核过滤决策标准。它是一个参数化、设计师驱动的框架，仅需少量衰减和权重参数，无需重新训练，即可平滑适应占用网格和导航网格抽象。

Result: 在五张代表性游戏地图、两种玩家控制策略和五种守卫模式下进行评估，结果表明该方法在捕获效率和巡逻自然度方面均优于经典的基线方法。此外，常见的潜行机制（如干扰和环境元素）可以自然地整合到该框架中作为子模块。

Conclusion: 该框架能够快速原型化丰富、动态且响应迅速的守卫行为，显著提升潜行游戏的沉浸感和策略深度。

Abstract: Guard patrol behavior is central to the immersion and strategic depth of
stealth games, while most existing systems rely on hand-crafted routes or
specialized logic that struggle to balance coverage efficiency and responsive
pursuit with believable naturalness. We propose a generic, fully explainable,
training-free framework that integrates global knowledge and local information
via Composite Potential Fields, combining three interpretable maps-Information,
Confidence, and Connectivity-into a single kernel-filtered decision criterion.
Our parametric, designer-driven approach requires only a handful of decay and
weight parameters-no retraining-to smoothly adapt across both occupancy-grid
and NavMesh-partition abstractions. We evaluate on five representative game
maps, two player-control policies, and five guard modes, confirming that our
method outperforms classical baseline methods in both capture efficiency and
patrol naturalness. Finally, we show how common stealth mechanics-distractions
and environmental elements-integrate naturally into our framework as sub
modules, enabling rapid prototyping of rich, dynamic, and responsive guard
behaviors.

</details>


### [9] [A Database-Driven Framework for 3D Level Generation with LLMs](https://arxiv.org/abs/2508.18533)
*Kaijie Xu,Clark Verbrugge*

Main category: cs.AI

TL;DR: 本文提出了一种新颖的框架，用于生成具有可配置游戏进程的多层3D游戏关卡。该框架利用LLM辅助的数据库和多阶段管道，结合模块化设计和基于约束的优化，以确保空间连贯性、导航功能和适应性游戏体验。


<details>
  <summary>Details</summary>
Motivation: 3D游戏关卡的程序化内容生成（PCG）面临挑战，需要在多层环境中平衡空间连贯性、导航功能和适应性游戏进程。

Method: 该方法的核心是离线、LLM辅助构建可重用的建筑组件（设施和房间模板）和游戏机制元素的数据库。其多阶段管道包括：1) 从房间数据库选择并排列实例以形成具有拓扑顺序的多层全局结构；2) 基于设施数据库的预定义约束优化每个房间的内部设施布局；3) 根据拓扑和空间规则从机制数据库放置组件，集成基于进程的游戏机制。随后，一个两阶段修复系统确保了可导航性。该方法结合了模块化、数据库驱动的设计与基于约束的优化。

Result: 初步实验验证了该框架能够生成多样化、可导航的3D环境，并通过简单的参数化模拟不同的游戏节奏策略。

Conclusion: 这项研究通过为自动化生成具有可配置游戏进程的复杂3D关卡提供可扩展的、以数据库为中心的PCG基础，从而推动了PCG领域的发展。

Abstract: Procedural Content Generation for 3D game levels faces challenges in
balancing spatial coherence, navigational functionality, and adaptable gameplay
progression across multi-floor environments. This paper introduces a novel
framework for generating such levels, centered on the offline, LLM-assisted
construction of reusable databases for architectural components (facilities and
room templates) and gameplay mechanic elements. Our multi-phase pipeline
assembles levels by: (1) selecting and arranging instances from the Room
Database to form a multi-floor global structure with an inherent topological
order; (2) optimizing the internal layout of facilities for each room based on
predefined constraints from the Facility Database; and (3) integrating
progression-based gameplay mechanics by placing components from a Mechanics
Database according to their topological and spatial rules. A subsequent
two-phase repair system ensures navigability. This approach combines modular,
database-driven design with constraint-based optimization, allowing for
systematic control over level structure and the adaptable pacing of gameplay
elements. Initial experiments validate the framework's ability in generating
diverse, navigable 3D environments and its capability to simulate distinct
gameplay pacing strategies through simple parameterization. This research
advances PCG by presenting a scalable, database-centric foundation for the
automated generation of complex 3D levels with configurable gameplay
progression.

</details>


### [10] [SchemaCoder: Automatic Log Schema Extraction Coder with Residual Q-Tree Boosting](https://arxiv.org/abs/2508.18554)
*Lily Jiaxin Wan,Chia-Tung Ho,Rongjian Liang,Cunxi Yu,Deming Chen,Haoxing Ren*

Main category: cs.AI

TL;DR: SchemaCoder是一个完全自动化的日志模式提取框架，它利用LLM和新颖的残差问题树（Q-Tree）增强机制，无需人工干预即可从各种日志格式中提取模式，并在LogHub-2.0基准测试中显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有的日志模式提取方法，即使是利用大型语言模型（LLMs），也普遍依赖预定义的正则表达式，这需要人类领域专业知识，并严重限制了生产力提升。研究旨在根本性地解决这一限制。

Method: SchemaCoder引入了残差问题树（Q-Tree）增强机制，通过LLM驱动的迭代、自适应查询来优化模式提取。具体方法包括：通过上下文边界分割将日志划分为语义块，使用基于嵌入的采样选择代表性模式，通过分层Q-Tree驱动的LLM查询生成模式代码，并通过文本残差进化优化器和残差增强进行迭代细化。

Result: 在广泛使用的LogHub-2.0基准测试中，SchemaCoder的实验验证结果显示，其性能平均比现有最先进技术提高了21.3%。

Conclusion: SchemaCoder是首个无需人工定制即可适用于各种日志文件格式的完全自动化模式提取框架，通过其创新的残差问题树增强机制，显著提升了日志模式提取的自动化程度和性能。

Abstract: Log schema extraction is the process of deriving human-readable templates
from massive volumes of log data, which is essential yet notoriously
labor-intensive. Recent studies have attempted to streamline this task by
leveraging Large Language Models (LLMs) for automated schema extraction.
However, existing methods invariably rely on predefined regular expressions,
necessitating human domain expertise and severely limiting productivity gains.
To fundamentally address this limitation, we introduce SchemaCoder, the first
fully automated schema extraction framework applicable to a wide range of log
file formats without requiring human customization within the flow. At its
core, SchemaCoder features a novel Residual Question-Tree (Q-Tree) Boosting
mechanism that iteratively refines schema extraction through targeted, adaptive
queries driven by LLMs. Particularly, our method partitions logs into semantic
chunks via context-bounded segmentation, selects representative patterns using
embedding-based sampling, and generates schema code through hierarchical
Q-Tree-driven LLM queries, iteratively refined by our textual-residual
evolutionary optimizer and residual boosting. Experimental validation
demonstrates SchemaCoder's superiority on the widely-used LogHub-2.0 benchmark,
achieving an average improvement of 21.3% over state-of-the-arts.

</details>


### [11] [eSkinHealth: A Multimodal Dataset for Neglected Tropical Skin Diseases](https://arxiv.org/abs/2508.18608)
*Janet Wang,Xin Hu,Yunbei Zhang,Diabate Almamy,Vagamon Bamba,Konan Amos Sébastien Koffi,Yao Koffi Aubin,Zhengming Ding,Jihun Hamm,Rie R. Yotsu*

Main category: cs.AI

TL;DR: 本文介绍了eSkinHealth，一个在西非收集的皮肤热带病（NTDs）数据集，并提出了一种AI-专家协作范式，用于高效生成多模态标注，旨在推动全球皮肤病学中更公平、准确和可解释的AI工具发展。


<details>
  <summary>Details</summary>
Motivation: 皮肤热带病给贫困热带社区带来严重的健康和社会经济负担。然而，由于数据稀缺，特别是针对代表性不足人群和罕见病症的数据，AI驱动的诊断支持进展受阻。现有的皮肤病学数据集往往缺乏开发可靠NTDs识别模型所需的关键人口统计学和疾病谱数据。

Method: 研究团队在科特迪瓦和加纳实地收集了eSkinHealth皮肤病数据集。此外，他们提出了一种AI-专家协作范式，在皮肤科医生的指导下，利用基础语言和分割模型高效生成多模态标注，包括语义病变掩码、实例特定视觉描述和临床概念。

Result: eSkinHealth数据集包含来自1,639个病例的5,623张图像，涵盖47种皮肤病，独特地专注于西非人群中的皮肤热带病和罕见病症。除了患者元数据和诊断标签外，该数据集还包括语义病变掩码、实例特定视觉描述和临床概念。

Conclusion: 这项工作提供了一个有价值的新资源和一个可扩展的标注框架，旨在促进开发更公平、准确和可解释的AI工具，以服务于全球皮肤病学领域。

Abstract: Skin Neglected Tropical Diseases (NTDs) impose severe health and
socioeconomic burdens in impoverished tropical communities. Yet, advancements
in AI-driven diagnostic support are hindered by data scarcity, particularly for
underrepresented populations and rare manifestations of NTDs. Existing
dermatological datasets often lack the demographic and disease spectrum crucial
for developing reliable recognition models of NTDs. To address this, we
introduce eSkinHealth, a novel dermatological dataset collected on-site in
C\^ote d'Ivoire and Ghana. Specifically, eSkinHealth contains 5,623 images from
1,639 cases and encompasses 47 skin diseases, focusing uniquely on skin NTDs
and rare conditions among West African populations. We further propose an
AI-expert collaboration paradigm to implement foundation language and
segmentation models for efficient generation of multimodal annotations, under
dermatologists' guidance. In addition to patient metadata and diagnosis labels,
eSkinHealth also includes semantic lesion masks, instance-specific visual
captions, and clinical concepts. Overall, our work provides a valuable new
resource and a scalable annotation framework, aiming to catalyze the
development of more equitable, accurate, and interpretable AI tools for global
dermatology.

</details>


### [12] [RLMR: Reinforcement Learning with Mixed Rewards for Creative Writing](https://arxiv.org/abs/2508.18642)
*Jianxing Liao,Tian Zhang,Xiao Feng,Yusong Zhang,Rui Yang,Haorui Wang,Bosi Wen,Ziying Wang,Runzhi Shi*

Main category: cs.AI

TL;DR: 本文提出了一种名为RLMR的强化学习方法，通过动态混合奖励系统，有效平衡了大型语言模型在创意写作中主观写作质量和客观约束遵循的需求。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在创意写作中难以同时提升主观写作质量（如文学性、情感表达）和客观约束遵循（如格式、字数限制）。单一奖励策略无法兼顾，而固定权重混合奖励又缺乏适应性。

Method: RLMR方法采用动态混合奖励系统，结合评估主观写作质量的写作奖励模型和评估客观约束遵循的约束验证模型。其核心创新在于根据采样组内的写作质量动态调整约束遵循奖励的权重，确保违反约束的样本在GRPO训练中受到惩罚。

Result: RLMR在指令遵循（IFEval从83.36%提升至86.65%）和写作质量（在WriteEval手动专家配对评估中获得72.75%的胜率）两方面均取得了显著且一致的提升。同时，构建了WriteEval真实世界写作基准进行全面评估。

Conclusion: RLMR是首个将主观偏好与客观验证相结合的在线强化学习训练方法，为多维创意写作优化提供了一个有效的解决方案。

Abstract: Large language models are extensively utilized in creative writing
applications. Creative writing requires a balance between subjective writing
quality (e.g., literariness and emotional expression) and objective constraint
following (e.g., format requirements and word limits). Existing reinforcement
learning methods struggle to balance these two aspects: single reward
strategies fail to improve both abilities simultaneously, while fixed-weight
mixed-reward methods lack the ability to adapt to different writing scenarios.
To address this problem, we propose Reinforcement Learning with Mixed Rewards
(RLMR), utilizing a dynamically mixed reward system from a writing reward model
evaluating subjective writing quality and a constraint verification model
assessing objective constraint following. The constraint following reward
weight is adjusted dynamically according to the writing quality within sampled
groups, ensuring that samples violating constraints get negative advantage in
GRPO and thus penalized during training, which is the key innovation of this
proposed method. We conduct automated and manual evaluations across diverse
model families from 8B to 72B parameters. Additionally, we construct a
real-world writing benchmark named WriteEval for comprehensive evaluation.
Results illustrate that our method achieves consistent improvements in both
instruction following (IFEval from 83.36\% to 86.65\%) and writing quality
(72.75\% win rate in manual expert pairwise evaluations on WriteEval). To the
best of our knowledge, RLMR is the first work to combine subjective preferences
with objective verification in online RL training, providing an effective
solution for multi-dimensional creative writing optimization.

</details>


### [13] [Beyond Benchmark: LLMs Evaluation with an Anthropomorphic and Value-oriented Roadmap](https://arxiv.org/abs/2508.18646)
*Jun Wang,Ninglun Gu,Kailai Zhang,Zijiao Zhang,Yelun Bao,Jin Yang,Xu Yin,Liwei Liu,Yihuan Liu,Pengyong Li,Gary G. Yen,Junchi Yan*

Main category: cs.AI

TL;DR: 本研究提出了一种新颖的类人化（IQ、EQ、PQ）和价值导向（VQ）评估框架，旨在弥合大型语言模型（LLM）基准性能与实际应用之间的差距，以实现更全面、更符合部署需求的评估。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）的基准测试性能与实际应用价值之间存在脱节。当前的评估框架支离破碎，过于侧重技术指标，而忽略了部署所需的整体评估。

Method: 本研究引入了以人类智能为视角的类人化评估范式，提出了一个新颖的三维分类法：智商（IQ）-通用智能、情商（EQ）-对齐能力和专业商（PQ）-专业知识。同时，开创了一个价值导向评估（VQ）框架，评估经济可行性、社会影响、道德对齐和环境可持续性。研究还设计了一个包含六个组件的模块化架构和实施路线图，并分析了200多个基准测试。

Result: 研究提出了IQ（通用智能）、EQ（对齐能力）和PQ（专业知识）三维分类法，以及评估经济可行性、社会影响、道德对齐和环境可持续性的VQ框架。通过对200多个基准测试的分析，识别了动态评估需求和可解释性差距等关键挑战。研究为开发技术精湛、上下文相关且符合道德规范的LLM提供了可操作的指导，并维护了一个开源评估资源库。

Conclusion: 本研究提出的评估范式和框架为开发技术精湛、上下文相关且符合道德规范的LLM提供了有力的指导，有效弥补了现有评估框架的不足，推动LLM在实际应用中实现更全面的价值。

Abstract: For Large Language Models (LLMs), a disconnect persists between benchmark
performance and real-world utility. Current evaluation frameworks remain
fragmented, prioritizing technical metrics while neglecting holistic assessment
for deployment. This survey introduces an anthropomorphic evaluation paradigm
through the lens of human intelligence, proposing a novel three-dimensional
taxonomy: Intelligence Quotient (IQ)-General Intelligence for foundational
capacity, Emotional Quotient (EQ)-Alignment Ability for value-based
interactions, and Professional Quotient (PQ)-Professional Expertise for
specialized proficiency. For practical value, we pioneer a Value-oriented
Evaluation (VQ) framework assessing economic viability, social impact, ethical
alignment, and environmental sustainability. Our modular architecture
integrates six components with an implementation roadmap. Through analysis of
200+ benchmarks, we identify key challenges including dynamic assessment needs
and interpretability gaps. It provides actionable guidance for developing LLMs
that are technically proficient, contextually relevant, and ethically sound. We
maintain a curated repository of open-source evaluation resources at:
https://github.com/onejune2018/Awesome-LLM-Eval.

</details>


### [14] [MUA-RL: Multi-turn User-interacting Agent Reinforcement Learning for agentic tool use](https://arxiv.org/abs/2508.18669)
*Weikang Zhao,Xili Wang,Chengdi Ma,Lingbin Kong,Zhaohua Yang,Mingxiang Tuo,Xiaowei Shi,Yitao Zhai,Xunliang Cai*

Main category: cs.AI

TL;DR: 本文提出MUA-RL，一个新颖的强化学习框架，首次将LLM模拟用户整合到强化学习循环中，以解决智能体在动态多轮交互中工具使用面临的用户需求不确定性挑战。


<details>
  <summary>Details</summary>
Motivation: 随着智能体智能的快速发展，大型语言模型（LLMs）中的智能体工具使用变得日益重要。在智能体与用户的多轮交互中，用户需求的动态性、不确定性和随机性对智能体的工具调用能力提出了重大挑战。现有的工具使用强化学习方法在训练过程中缺乏动态用户的集成。

Method: 本文引入了MUA-RL（多轮用户交互智能体强化学习），这是一个新颖的强化学习框架。该框架首次将LLM模拟用户集成到强化学习循环中，旨在使模型能够自主学习如何高效地与用户沟通，并在动态多轮交互中利用各种工具解决实际问题。

Result: MUA-RL在多个多轮工具使用基准测试（如TAU2 Retail、TAU2 Airline、TAU2 Telecom、BFCL-V3 Multi Turn和ACEBench Agent）上进行了评估。MUA-RL-32B在这些基准测试中取得了显著成绩，在非思考设置下超越或匹配了DeepSeek-V3-0324和Qwen3-235B-A22B等更大的开源模型。

Conclusion: MUA-RL通过在强化学习训练过程中集成LLM模拟用户，成功解决了智能体在动态多轮交互中有效理解用户需求和使用工具的挑战，显著提升了智能体工具使用的性能。

Abstract: With the recent rapid advancement of Agentic Intelligence, agentic tool use
in LLMs has become increasingly important. During multi-turn interactions
between agents and users, the dynamic, uncertain, and stochastic nature of user
demands poses significant challenges to the agent's tool invocation
capabilities. Agents are no longer expected to simply call tools to deliver a
result; rather, they must iteratively refine their understanding of user needs
through communication while simultaneously invoking tools to resolve user
queries. Existing reinforcement learning (RL) approaches for tool use lack the
integration of genuinely dynamic users during the RL training process. To
bridge this gap, we introduce MUA-RL (Multi-turn User-interacting Agent
Reinforcement Learning for agentic tool use), a novel reinforcement learning
framework that, for the first time in the field of agentic tool use, integrates
LLM-simulated users into the reinforcement learning loop. MUA-RL aims to enable
autonomous learning of models to communicate with users efficiently and use
various tools to solve practical problems in dynamic multi-turn interactions.
Evaluations are done on several multi-turn tool-using benchmarks (see Figure
1). Specifically, MUA-RL-32B achieves 67.3 on TAU2 Retail, 45.4 on TAU2
Airline, 28.3 on TAU2 Telecom, 28.4 on BFCL-V3 Multi Turn, and 82.5 on ACEBench
Agent -- outperforming or matching the performance of larger open-source models
such as DeepSeek-V3-0324 and Qwen3-235B-A22B in non-thinking settings.

</details>


### [15] [AppAgent-Pro: A Proactive GUI Agent System for Multidomain Information Integration and User Assistance](https://arxiv.org/abs/2508.18689)
*Yuyang Zhao,Wentao Shi,Fuli Feng,Xiangnan He*

Main category: cs.AI

TL;DR: AppAgent-Pro是一个主动式GUI代理系统，它利用大型语言模型（LLM）主动整合多领域信息，以预测用户需求并进行深度信息挖掘，从而提供更全面和智能的信息获取。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的代理大多是被动响应用户指令，这严重限制了它们作为通用信息获取平台的效率和效果。研究旨在克服这一局限性。

Method: 本文提出了AppAgent-Pro，一个主动式GUI代理系统。该系统基于用户指令主动整合多领域信息，使其能够预判用户的潜在需求，并进行深入的多领域信息挖掘。

Result: AppAgent-Pro能够促进更全面和智能的信息获取，通过主动整合和挖掘多领域信息，超越了传统被动式代理的能力。

Conclusion: AppAgent-Pro有潜力从根本上重新定义日常生活中的信息获取方式，对人类社会产生深远影响。

Abstract: Large language model (LLM)-based agents have demonstrated remarkable
capabilities in addressing complex tasks, thereby enabling more advanced
information retrieval and supporting deeper, more sophisticated human
information-seeking behaviors. However, most existing agents operate in a
purely reactive manner, responding passively to user instructions, which
significantly constrains their effectiveness and efficiency as general-purpose
platforms for information acquisition. To overcome this limitation, this paper
proposes AppAgent-Pro, a proactive GUI agent system that actively integrates
multi-domain information based on user instructions. This approach enables the
system to proactively anticipate users' underlying needs and conduct in-depth
multi-domain information mining, thereby facilitating the acquisition of more
comprehensive and intelligent information. AppAgent-Pro has the potential to
fundamentally redefine information acquisition in daily life, leading to a
profound impact on human society. Our code is available at:
https://github.com/LaoKuiZe/AppAgent-Pro. Our code is available at:
https://github.com/LaoKuiZe/AppAgent-Pro. The demonstration video could be
found at:
https://www.dropbox.com/scl/fi/hvzqo5vnusg66srydzixo/AppAgent-Pro-demo-video.mp4?rlkey=o2nlfqgq6ihl125mcqg7bpgqu&st=d29vrzii&dl=0.

</details>


### [16] [VistaWise: Building Cost-Effective Agent with Cross-Modal Knowledge Graph for Minecraft](https://arxiv.org/abs/2508.18722)
*Honghao Fu,Junlong Ren,Qi Chai,Deheng Ye,Yujun Cai,Hao Wang*

Main category: cs.AI

TL;DR: VistaWise是一个经济高效的具身决策智能体框架，通过集成跨模态领域知识和高效的对象检测模型，显著减少了对领域特定训练数据的需求，并在开放世界任务中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在虚拟开放世界环境中的具身决策任务中表现出潜力，但缺乏领域特定知识，且在大规模领域特定数据上进行微调的开发成本过高。

Method: 本文提出了VistaWise框架，它：1) 集成了跨模态领域知识；2) 微调了一个专用的对象检测模型，将领域特定训练数据需求从数百万减少到数百个样本；3) 将视觉信息和文本依赖关系整合到跨模态知识图谱（KG）中；4) 采用基于检索的池化策略从KG中提取任务相关信息；5) 配备了桌面级技能库，通过鼠标和键盘输入直接操作Minecraft桌面客户端。

Result: 实验结果表明，VistaWise在各种开放世界任务中实现了最先进的性能。

Conclusion: VistaWise有效降低了开发成本，同时提升了智能体的性能，证明了其在具身决策任务中的有效性。

Abstract: Large language models (LLMs) have shown significant promise in embodied
decision-making tasks within virtual open-world environments. Nonetheless,
their performance is hindered by the absence of domain-specific knowledge.
Methods that finetune on large-scale domain-specific data entail prohibitive
development costs. This paper introduces VistaWise, a cost-effective agent
framework that integrates cross-modal domain knowledge and finetunes a
dedicated object detection model for visual analysis. It reduces the
requirement for domain-specific training data from millions of samples to a few
hundred. VistaWise integrates visual information and textual dependencies into
a cross-modal knowledge graph (KG), enabling a comprehensive and accurate
understanding of multimodal environments. We also equip the agent with a
retrieval-based pooling strategy to extract task-related information from the
KG, and a desktop-level skill library to support direct operation of the
Minecraft desktop client via mouse and keyboard inputs. Experimental results
demonstrate that VistaWise achieves state-of-the-art performance across various
open-world tasks, highlighting its effectiveness in reducing development costs
while enhancing agent performance.

</details>


### [17] [Bias Mitigation Agent: Optimizing Source Selection for Fair and Balanced Knowledge Retrieval](https://arxiv.org/abs/2508.18724)
*Karanbir Singh,Deepak Muppiri,William Ngu*

Main category: cs.AI

TL;DR: 本文提出了一种新颖的偏见缓解智能体（Bias Mitigation Agent），这是一个多智能体系统，旨在通过优化信息源选择，确保检索内容的关联性和最小偏见，从而解决生成式AI智能体中存在的偏见问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）驱动的智能体AI继承了内部和外部信息源的偏见，这严重影响了检索信息的公平性和平衡性，进而降低了用户信任。因此，需要一种方法来解决这一关键挑战。

Method: 引入了一个多智能体系统，即“偏见缓解智能体”（Bias Mitigation Agent），该系统通过专业智能体协调偏见缓解工作流，优化信息源的选择，以确保检索到的内容既高度相关又偏见最小化，从而促进公平和平衡的知识传播。

Result: 实验结果表明，与基线朴素检索策略相比，该方法将偏见减少了81.82%。

Conclusion: 所提出的偏见缓解智能体能够显著减少生成式AI智能体在信息检索中的偏见，有效提升了检索内容的公平性和平衡性。

Abstract: Large Language Models (LLMs) have transformed the field of artificial
intelligence by unlocking the era of generative applications. Built on top of
generative AI capabilities, Agentic AI represents a major shift toward
autonomous, goal-driven systems that can reason, retrieve, and act. However,
they also inherit the bias present in both internal and external information
sources. This significantly affects the fairness and balance of retrieved
information, and hence reduces user trust. To address this critical challenge,
we introduce a novel Bias Mitigation Agent, a multi-agent system designed to
orchestrate the workflow of bias mitigation through specialized agents that
optimize the selection of sources to ensure that the retrieved content is both
highly relevant and minimally biased to promote fair and balanced knowledge
dissemination. The experimental results demonstrate an 81.82\% reduction in
bias compared to a baseline naive retrieval strategy.

</details>


### [18] [CAC-CoT: Connector-Aware Compact Chain-of-Thought for Efficient Reasoning Data Synthesis Across Dual-System Cognitive Tasks](https://arxiv.org/abs/2508.18743)
*Sunguk Choi,Yonghoon Kwon,Heondeuk Lee*

Main category: cs.AI

TL;DR: 本文提出了一种名为CAC-CoT（Connector-Aware Compact CoT）的方法，通过限制推理使用固定连接词，使大语言模型生成更简洁、结构化的思维链，从而在保持高准确率的同时，显著提高了推理效率，适用于快节奏和复杂任务。


<details>
  <summary>Details</summary>
Motivation: 长思维链（CoT）提示有助于大语言模型（LLMs）解决难题，但过长的推理过程往往会降低或甚至损害模型在快速、直观的“系统1”任务上的性能。

Method: 引入了连接词感知紧凑型思维链（CAC-CoT）方法，该方法有意识地将推理限制在一小组固定的连接词短语中，引导模型生成简洁且结构良好的解释。本文使用Gemini-2.0-Flash模型进行合成训练。

Result: CAC-CoT在GSM8K上取得了约85%的准确率，在GPQA（系统2任务）上取得了约40%的准确率，同时在S1-Bench（系统1任务）上保持了约90%的准确率。其推理轨迹平均约为300个令牌，大约是基线方法长度的三分之一，实现了更高的效率且没有损失准确性。

Conclusion: CAC-CoT通过限制推理词汇，成功地实现了简洁、结构化的思维链，显著提高了效率，同时在“系统1”和“系统2”任务上均保持了高水平的性能，为LLM推理提供了一种有效且高效的解决方案。

Abstract: Long chain-of-thought (CoT) prompting helps Large Language Models (LLMs)
solve difficult problems, but very long traces often slow or even degrade
performance on fast, intuitive "System-1" tasks. We introduce Connector-Aware
Compact CoT (CAC-CoT) -- a method that deliberately restricts reasoning to a
small, fixed set of connector phrases, steering the model toward concise and
well -- structured explanations. Despite its simplicity, our synthetic method
with Gemini-2.0-Flash yields a high-quality training quality. CAC-CoT achieves
approximately 85% on GSM8K and approximately 40% on GPQA (System-2) while
retaining approximately 90% on S1-Bench (System-1). Its reasoning traces
average approximately 300 tokens(ART), about one-third the length of baseline
traces, delivering higher efficiency without loss of accuracy.

</details>


### [19] [Reflection-Enhanced Meta-Optimization Integrating TextGrad-style Prompt Optimization with Memory-Driven Self-Evolution](https://arxiv.org/abs/2508.18749)
*Chunlong Wu,Zhibo Qu*

Main category: cs.AI

TL;DR: REMO是一个新颖的框架，通过结合记忆增强的反射RAG模块和自适应优化器，解决了现有提示优化方法（如TextGrad）无状态和过拟合的问题，实现了更稳定和鲁棒的泛化能力，并支持持续改进。


<details>
  <summary>Details</summary>
Motivation: 当前的提示优化方法通常是无状态的，无法保留和利用历史优化经验；它们也容易过拟合，导致生成的提示在即时任务上下文之外泛化能力差。

Method: 我们提出了Reflection-Enhanced Meta-Optimization (REMO) 框架，包含：1) 一个记忆增强的反射检索增强生成 (RAG) 模块，结构化为“错误笔记本”；2) 一个自适应优化器，通过LLM驱动的元控制器实现，该控制器综合了周期级别的反思性见解，以迭代改进系统级别的提示策略。该框架在Qwen3-32B上以标准推理模式实例化。

Result: 在GSM8K数学推理基准上的实验结果表明，与TextGrad基线相比，REMO实现了更稳定和鲁棒的泛化能力，尽管计算开销有所增加。我们还提供了算法设计、优化动态的定性和定量分析以及组件的消融研究。

Conclusion: REMO框架不仅支持类似TextGrad的局部、细粒度提示调优，还实现了跨运行优化知识的系统积累和重用，从而支持系统随时间的持续改进，并展现出更强的泛化能力。

Abstract: Recent advances in prompt optimization, exemplified by methods such as
TextGrad, enable automatic, gradient-like refinement of textual prompts to
enhance the performance of large language models (LLMs) on specific downstream
tasks. However, current approaches are typically stateless and operate
independently across optimization runs, lacking mechanisms to preserve and
leverage historical optimization experience. Furthermore, they are susceptible
to overfitting, often yielding prompt updates that generalize poorly beyond the
immediate task context.
  To address these limitations, we propose Reflection-Enhanced
Meta-Optimization (REMO), a novel framework that integrates (1) a
memory-augmented Reflection Retrieval-Augmented Generation (RAG) module -
structured as a "mistake notebook" and (2) a Self-Adaptive Optimizer,
implemented via an LLM-driven meta-controller that synthesizes epoch-level
reflective insights to iteratively improve system-level prompting strategies.
This architecture enables not only local, fine-grained prompt tuning akin to
TextGrad, but also the systematic accumulation and reuse of cross-run
optimization knowledge, thereby supporting continual improvement over time.
  We instantiate the REMO framework using Qwen3-32B in standard inference mode
- without explicit chain-of-thought prompting - and evaluate its efficacy on
the GSM8K benchmark for mathematical reasoning. Experimental results
demonstrate that, compared to a TextGrad baseline, REMO achieves more stable
and robust generalization, albeit at the cost of increased computational
overhead. We provide a detailed exposition of the algorithmic design, conduct a
qualitative and quantitative analysis of optimization dynamics, and present a
comprehensive ablation study to elucidate the contributions of each component.

</details>


### [20] [Stabilizing Open-Set Test-Time Adaptation via Primary-Auxiliary Filtering and Knowledge-Integrated Prediction](https://arxiv.org/abs/2508.18751)
*Byung-Joon Lee,Jin-Seop Lee,Jee-Hyong Lee*

Main category: cs.AI

TL;DR: 针对开放集测试时间自适应（OSTTA）中的域偏移和未知类别挑战，本文提出了主辅过滤（PAF）以提高开放集数据识别准确性，并提出了知识集成预测（KIP）以融合多模型知识，从而同时提升了闭集准确性和开放集判别能力。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在训练和测试数据分布对齐时表现良好，但实际世界数据常存在域偏移。测试时间自适应（TTA）旨在解决此问题，但多数研究假设类别集合不变（闭集TTA）。实际场景常涉及开放集数据（OSTTA），这会降低闭集准确性。现有OSTTA方法在识别开放集数据时，依赖源模型或不稳定的自适应模型进行过滤，导致过滤准确性不佳和误差累积。

Method: 本文提出两种主要方法：1. **主辅过滤（PAF）**：使用一个辅助过滤器来验证主过滤器筛选出的数据，以解决现有方法过滤准确性次优的问题。2. **知识集成预测（KIP）**：校准自适应模型、EMA模型和源模型的输出，以整合它们的互补知识，用于OSTTA。

Result: 本文方法在多种闭集和开放集数据集上进行了验证，结果表明它在闭集准确性和开放集判别能力方面均优于现有方法。

Conclusion: 通过引入主辅过滤（PAF）和知识集成预测（KIP），本文有效解决了开放集测试时间自适应（OSTTA）中的挑战，显著提升了模型在存在域偏移和未知类别情况下的性能，实现了更好的闭集准确性和开放集判别能力。

Abstract: Deep neural networks demonstrate strong performance under aligned
training-test distributions. However, real-world test data often exhibit domain
shifts. Test-Time Adaptation (TTA) addresses this challenge by adapting the
model to test data during inference. While most TTA studies assume that the
training and test data share the same class set (closed-set TTA), real-world
scenarios often involve open-set data (open-set TTA), which can degrade
closed-set accuracy. A recent study showed that identifying open-set data
during adaptation and maximizing its entropy is an effective solution. However,
the previous method relies on the source model for filtering, resulting in
suboptimal filtering accuracy on domain-shifted test data. In contrast, we
found that the adapting model, which learns domain knowledge from noisy test
streams, tends to be unstable and leads to error accumulation when used for
filtering. To address this problem, we propose Primary-Auxiliary Filtering
(PAF), which employs an auxiliary filter to validate data filtered by the
primary filter. Furthermore, we propose Knowledge-Integrated Prediction (KIP),
which calibrates the outputs of the adapting model, EMA model, and source model
to integrate their complementary knowledge for OSTTA. We validate our approach
across diverse closed-set and open-set datasets. Our method enhances both
closed-set accuracy and open-set discrimination over existing methods. The code
is available at https://github.com/powerpowe/PAF-KIP-OSTTA .

</details>


### [21] [Answering the Unanswerable Is to Err Knowingly: Analyzing and Mitigating Abstention Failures in Large Reasoning Models](https://arxiv.org/abs/2508.18760)
*Yi Liu,Xiangyu Liu,Zequn Sun,Wei Hu*

Main category: cs.AI

TL;DR: 大型推理模型（LRMs）在面对无法回答的问题时，尽管具备认知能力识别问题缺陷，但却未能恰当地拒绝回答。本文系统分析了这一问题，并提出了一种轻量级的两阶段方法来显著提高LRMs的拒绝回答率。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在复杂推理任务上取得了显著进展，但当遇到诸如缺乏充分条件的数学问题等无法回答的问题时，它们未能提供适当的拒绝回答（abstention），这阻碍了可信赖AI的发展。

Method: 本文首先详细分析了LRMs在面对无法回答问题时的不同响应行为。然后，证明了LRMs具备识别问题缺陷的足够认知能力，但其内部认知与外部响应之间存在错位。最后，提出了一种轻量级的两阶段方法，结合了认知监控（cognitive monitoring）和推理时干预（inference-time intervention）来解决这一问题。

Result: 实验结果表明，所提出的方法显著提高了LRMs的拒绝回答率，同时保持了整体的推理性能。

Conclusion: LRMs具备识别无法回答问题的认知能力，但其未能恰当拒绝回答揭示了内部认知与外部响应之间的不一致。本文提出的两阶段方法有效解决了这一问题，使LRMs在面对无法回答的问题时能更可靠地拒绝回答，从而增强了AI的可信赖性。

Abstract: Large reasoning models (LRMs) have shown remarkable progress on complex
reasoning tasks. However, some questions posed to LRMs are inherently
unanswerable, such as math problems lacking sufficient conditions. We find that
LRMs continually fail to provide appropriate abstentions when confronted with
these unanswerable questions. In this paper, we systematically analyze,
investigate, and resolve this issue for trustworthy AI. We first conduct a
detailed analysis of the distinct response behaviors of LRMs when facing
unanswerable questions. Then, we show that LRMs possess sufficient cognitive
capabilities to recognize the flaws in these questions. However, they fail to
exhibit appropriate abstention behavior, revealing a misalignment between their
internal cognition and external response. Finally, to resolve this issue, we
propose a lightweight, two-stage method that combines cognitive monitoring with
inference-time intervention. Experimental results demonstrate that our method
significantly improves the abstention rate while maintaining the overall
reasoning performance.

</details>


### [22] [Dynamic Collaboration of Multi-Language Models based on Minimal Complete Semantic Units](https://arxiv.org/abs/2508.18763)
*Chao Hao,Zezheng Wang,Yanhua Huang,Ruiwen Xu,Wenzhe Niu,Xin Liu,Zitong Yu*

Main category: cs.AI

TL;DR: 本文通过令牌级多模型协作增强语言模型推理能力，引入基于分布距离的动态选择策略（DDS）和最小完整语义单元（MCSU）来优化协作并解决词汇不对齐问题。


<details>
  <summary>Details</summary>
Motivation: 研究动机是提升语言模型的推理能力，通过利用多个模型协同工作，并解决多模型协作中模型选择优化和词汇不对齐等关键挑战。

Method: 方法包括：1) 令牌级多模型协作，从多个模型的下一令牌分布中选择最优令牌进行自回归推理。2) 引入基于分布距离的动态选择策略（DDS），以优化多模型协作过程，而非简单地增加模型数量。3) 提出最小完整语义单元（MCSU）概念，以解决多模型协作中的词汇不对齐问题，实现语言空间内的自然对齐。

Result: 在各种基准测试上的实验结果表明，所提出的方法具有优越性。

Conclusion: 通过优化多模型协作（DDS）和解决词汇不对齐（MCSU），本文成功提升了语言模型的推理能力，并取得了卓越的性能。

Abstract: This paper investigates the enhancement of reasoning capabilities in language
models through token-level multi-model collaboration. Our approach selects the
optimal tokens from the next token distributions provided by multiple models to
perform autoregressive reasoning. Contrary to the assumption that more models
yield better results, we introduce a distribution distance-based dynamic
selection strategy (DDS) to optimize the multi-model collaboration process. To
address the critical challenge of vocabulary misalignment in multi-model
collaboration, we propose the concept of minimal complete semantic units
(MCSU), which is simple yet enables multiple language models to achieve natural
alignment within the linguistic space. Experimental results across various
benchmarks demonstrate the superiority of our method. The code will be
available at https://github.com/Fanye12/DDS.

</details>


### [23] [AniME: Adaptive Multi-Agent Planning for Long Animation Generation](https://arxiv.org/abs/2508.18781)
*Lisai Zhang,Baohan Xu,Siqian Yang,Mingyu Yin,Jing Liu,Chao Xu,Siqi Wang,Yidi Wu,Yuxin Hong,Zihao Zhang,Yanzhang Liang,Yudong Jiang*

Main category: cs.AI

TL;DR: AniME是一个面向导演的多智能体系统，用于自动化长篇动漫制作，涵盖从故事到最终视频的整个工作流程。


<details>
  <summary>Details</summary>
Motivation: 旨在为AI驱动的动漫创作提供可扩展的解决方案，以实现电影级动画的自动化生产。

Method: AniME采用多智能体系统架构，包括一个负责全局记忆和协调的导演智能体，以及多个通过集成定制模型上下文协议（MCP）和下游模型指令来适应性选择控制条件的专业智能体。

Result: 该系统能够制作出具有角色一致性、音视频同步的电影级动画。

Conclusion: AniME为AI驱动的动漫创作提供了一个可扩展的解决方案，能够自动化生成高质量的动漫视频。

Abstract: We present AniME, a director-oriented multi-agent system for automated
long-form anime production, covering the full workflow from a story to the
final video. The director agent keeps a global memory for the whole workflow,
and coordinates several downstream specialized agents. By integrating
customized Model Context Protocol (MCP) with downstream model instruction, the
specialized agent adaptively selects control conditions for diverse sub-tasks.
AniME produces cinematic animation with consistent characters and synchronized
audio visual elements, offering a scalable solution for AI-driven anime
creation.

</details>


### [24] [CausalMACE: Causality Empowered Multi-Agents in Minecraft Cooperative Tasks](https://arxiv.org/abs/2508.18797)
*Qi Chai,Zhang Zheng,Junlong Ren,Deheng Ye,Zichuan Lin,Hao Wang*

Main category: cs.AI

TL;DR: CausalMACE是一个针对Minecraft多智能体协作任务的因果规划框架，通过引入因果关系管理子任务依赖，显著提升了复杂任务的效率和容错性，并达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要采用单一大型语言模型（LLM）智能体在Minecraft中完成任务，但对于需要冗长行动序列的复杂任务，单智能体方法效率低下且容错性有限。目前对多智能体协作的研究相对稀缺。

Method: 本文提出了CausalMACE框架，旨在增强多智能体系统。该框架将因果关系融入子任务依赖管理中，并包含两个主要模块：用于全局任务规划的整体任务图，以及一个基于因果关系进行依赖管理并采用固有规则进行因果干预的模块。

Result: 实验结果表明，该方法在Minecraft的多智能体协作任务中实现了最先进的性能。

Conclusion: CausalMACE通过其独特的因果规划框架，有效解决了Minecraft复杂多智能体任务中的效率和容错性问题，显著提升了多智能体系统的性能。

Abstract: Minecraft, as an open-world virtual interactive environment, has become a
prominent platform for research on agent decision-making and execution.
Existing works primarily adopt a single Large Language Model (LLM) agent to
complete various in-game tasks. However, for complex tasks requiring lengthy
sequences of actions, single-agent approaches often face challenges related to
inefficiency and limited fault tolerance. Despite these issues, research on
multi-agent collaboration remains scarce. In this paper, we propose CausalMACE,
a holistic causality planning framework designed to enhance multi-agent
systems, in which we incorporate causality to manage dependencies among
subtasks. Technically, our proposed framework introduces two modules: an
overarching task graph for global task planning and a causality-based module
for dependency management, where inherent rules are adopted to perform causal
intervention. Experimental results demonstrate our approach achieves
state-of-the-art performance in multi-agent cooperative tasks of Minecraft.

</details>


### [25] [STARec: An Efficient Agent Framework for Recommender Systems via Autonomous Deliberate Reasoning](https://arxiv.org/abs/2508.18812)
*Chenghao Wu,Ruiyang Ren,Junjie Zhang,Ruirui Wang,Zhongrui Ma,Qi Ye,Wayne Xin Zhao*

Main category: cs.AI

TL;DR: 本文提出STARec，一个基于慢思考增强的智能体框架，为推荐系统引入自主深思熟虑的推理能力，通过并行认知和锚定强化训练克服现有推荐系统的局限性，并在有限数据下显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统（包括基于LLM的）受限于静态用户建模和被动决策范式，导致推荐易受浅层关联偏差影响、因果推断能力有限，且在稀疏数据场景下表现脆弱。它们过度依赖启发式模式匹配，未能实现深思熟虑的推理。

Method: STARec框架将每个用户建模为一个具有并行认知（快速响应和慢速链式思维推理）的智能体。为培养内在慢思考能力，本文开发了“锚定强化训练”范式，结合了来自高级推理模型的结构化知识蒸馏和偏好对齐的奖励塑造。这种混合方法使智能体能够获取基础能力（偏好总结、理由生成）并通过模拟反馈循环实现动态策略适应。

Result: 在MovieLens 1M和Amazon CDs基准测试中，STARec相较于最先进的基线取得了显著的性能提升，尽管仅使用了0.4%的完整训练数据。

Conclusion: STARec通过引入慢思考增强的智能体框架和创新的锚定强化训练方法，成功地赋予推荐系统自主深思熟虑的推理能力，有效克服了现有系统的局限性，并在数据受限的情况下展现出卓越的推荐性能。

Abstract: While modern recommender systems are instrumental in navigating information
abundance, they remain fundamentally limited by static user modeling and
reactive decision-making paradigms. Current large language model (LLM)-based
agents inherit these shortcomings through their overreliance on heuristic
pattern matching, yielding recommendations prone to shallow correlation bias,
limited causal inference, and brittleness in sparse-data scenarios. We
introduce STARec, a slow-thinking augmented agent framework that endows
recommender systems with autonomous deliberative reasoning capabilities. Each
user is modeled as an agent with parallel cognitions: fast response for
immediate interactions and slow reasoning that performs chain-of-thought
rationales. To cultivate intrinsic slow thinking, we develop anchored
reinforcement training - a two-stage paradigm combining structured knowledge
distillation from advanced reasoning models with preference-aligned reward
shaping. This hybrid approach scaffolds agents in acquiring foundational
capabilities (preference summarization, rationale generation) while enabling
dynamic policy adaptation through simulated feedback loops. Experiments on
MovieLens 1M and Amazon CDs benchmarks demonstrate that STARec achieves
substantial performance gains compared with state-of-the-art baselines, despite
using only 0.4% of the full training data.

</details>


### [26] [Judicial Requirements for Generative AI in Legal Reasoning](https://arxiv.org/abs/2508.18880)
*Eljas Linna,Tuula Linna*

Main category: cs.AI

TL;DR: 本研究定义了AI在司法决策中作为可靠推理工具所需的核心能力，并评估了现有AI增强机制（如RAG、多智能体系统、神经符号AI）弥合大型语言模型（LLMs）与法律解释之间差距的潜力，发现AI目前最适合作为简单案件的助手和复杂案件中人类专家的“陪练伙伴”。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）正被整合到专业领域，但它们在法律等高风险领域中的局限性仍未被充分理解，这促使研究者探究AI系统在司法决策中作为可靠推理工具所需的核心能力。

Method: 研究使用IRAC（Issue-Rule-Application-Conclusion）模型作为分析框架，重点关注法律裁决中最具挑战性的“规则确定（R）”和“规则应用（A）”阶段。从司法角度，将法律推理解构为一系列核心要求（如选择法律框架、生成论证、区分判例法中的主旨与附带意见、解决歧义、管理冲突条款、正确应用举证责任）。然后，将检索增强生成（RAG）、多智能体系统和神经符号AI等AI增强机制与这些要求进行匹配，评估它们弥合LLMs概率性与法律解释严谨性之间差距的潜力。

Result: 研究发现，虽然这些AI增强技术可以解决特定挑战，但仍存在显著挑战，特别是在需要自由裁量权和透明、可证明推理的任务中。LLMs的概率性质与法律解释中严格、选择驱动的需求之间仍存在较大鸿沟。

Conclusion: 本研究得出结论，AI目前在法律领域最有效的角色是双重的：作为处理简单、重复案件的高效助手，以及作为复杂案件中人类专家的精密“陪练伙伴”。

Abstract: Large Language Models (LLMs) are being integrated into professional domains,
yet their limitations in high-stakes fields like law remain poorly understood.
This paper defines the core capabilities that an AI system must possess to
function as a reliable reasoning tool in judicial decision-making. Using the
IRAC (Issue-Rule-Application-Conclusion) model as an analytical framework, the
study focuses on the most challenging phases of legal adjudication: determining
the applicable Rule (R) and performing the Application (A) of that rule to the
facts of a case. From a judicial perspective, the analysis deconstructs legal
reasoning into a series of core requirements, including the ability to select
the correct legal framework across jurisdictions, generate sound arguments
based on the doctrine of legal sources, distinguish ratio decidendi from obiter
dictum in case law, resolve ambiguity arising from general clauses like
"reasonableness", manage conflicting legal provisions, and correctly apply the
burden of proof. The paper then maps various AI enhancement mechanisms, such as
Retrieval-Augmented Generation (RAG), multi-agent systems, and neuro-symbolic
AI, to these requirements, assessing their potential to bridge the gap between
the probabilistic nature of LLMs and the rigorous, choice-driven demands of
legal interpretation. The findings indicate that while these techniques can
address specific challenges, significant challenges remain, particularly in
tasks requiring discretion and transparent, justifiable reasoning. Our paper
concludes that the most effective current role for AI in law is a dual one: as
a high-volume assistant for simple, repetitive cases and as a sophisticated
"sparring partner" for human experts in complex matters.

</details>


### [27] [Interactive Evaluation of Large Language Models for Multi-Requirement Software Engineering Tasks](https://arxiv.org/abs/2508.18905)
*Dimitrios Rontogiannis,Maxime Peyrard,Nicolas Baldwin,Martin Josifoski,Robert West,Dimitrios Gunopulos*

Main category: cs.AI

TL;DR: 本文提出了一种新颖的交互式评估框架，通过结构化、反馈驱动的对话，评估大型语言模型（LLMs）在多需求编程任务上的表现，以克服传统静态基准的不足。


<details>
  <summary>Details</summary>
Motivation: 标准的单轮、静态基准在评估LLMs处理软件工程等复杂任务的细微能力方面存在不足，无法充分衡量模型的真实表现和系统性弱点。

Method: 研究提出了一种交互式评估框架，将任务建模为需求依赖图。一个了解真实解决方案的“面试官”LLM会向“受访者”模型提供最小化、有针对性的提示，以帮助其纠正错误并满足约束。该框架基于DevAI基准（55个编程任务），并增加了真实解决方案，通过专家标注评估了面试官提示的相关性和实用性。

Result: 动态评估协议能够对模型行为提供细致的诊断洞察，揭示了静态基准无法衡量的模型优势和系统性弱点。

Conclusion: 研究结果强调了动态评估在推动协作式代码生成代理发展中的重要性。

Abstract: Standard single-turn, static benchmarks fall short in evaluating the nuanced
capabilities of Large Language Models (LLMs) on complex tasks such as software
engineering. In this work, we propose a novel interactive evaluation framework
that assesses LLMs on multi-requirement programming tasks through structured,
feedback-driven dialogue. Each task is modeled as a requirement dependency
graph, and an ``interviewer'' LLM, aware of the ground-truth solution, provides
minimal, targeted hints to an ``interviewee'' model to help correct errors and
fulfill target constraints. This dynamic protocol enables fine-grained
diagnostic insights into model behavior, uncovering strengths and systematic
weaknesses that static benchmarks fail to measure. We build on DevAI, a
benchmark of 55 curated programming tasks, by adding ground-truth solutions and
evaluating the relevance and utility of interviewer hints through expert
annotation. Our results highlight the importance of dynamic evaluation in
advancing the development of collaborative code-generating agents.

</details>


### [28] [FormaRL: Enhancing Autoformalization with no Labeled Data](https://arxiv.org/abs/2508.18914)
*Yanxing Huang,Xinling Jin,Sijie Liang,Peng Li,Yang Liu*

Main category: cs.AI

TL;DR: 本文提出了FormaRL，一个基于强化学习的自动形式化框架，仅需少量未标注数据即可显著提升自动形式化准确率。同时，还发布了用于本科数学证明问题的新数据集uproof。


<details>
  <summary>Details</summary>
Motivation: 自动形式化是形式验证的核心任务之一，但其发展受限于数据稀缺和缺乏高效方法。

Method: FormaRL框架整合了Lean编译器的语法检查和大型语言模型的一致性检查来计算奖励，并采用GRPO算法更新形式化器。此外，还整理了一个名为uproof的本科级别数学证明问题数据集。

Result: FormaRL仅使用859个未标注数据，将Qwen2.5-Coder-7B-Instruct的pass@1自动形式化准确率在ProofNet上提高了4~6倍（4.04%提升至26.15%），在uproof上提高了2.4%至9.6%。在uproof上，该方法在pass@1和pass@16准确率方面，相比现有开源最先进的自动形式化器，在分布外（OOD）性能上也有显著提升（pass@1：6.2%提升至9.6%；pass@16：24.4%提升至33.6%）。

Conclusion: FormaRL是一个简单而高效的强化学习框架，能够在仅使用少量未标注数据的情况下显著提升自动形式化性能，并在分布外场景下表现出色。新发布的uproof数据集有望促进高级数学中自动形式化和定理证明的探索。

Abstract: Autoformalization is one of the central tasks in formal verification, while
its advancement remains hindered due to the data scarcity and the absence
efficient methods. In this work we propose \textbf{FormaRL}, a simple yet
efficient reinforcement learning framework for autoformalization which only
requires a small amount of unlabeled data. FormaRL integrates syntax check from
Lean compiler and consistency check from large language model to calculate the
reward, and adopts GRPO algorithm to update the formalizer. We also curated a
proof problem dataset from undergraduate-level math materials, named
\textbf{uproof}, in the hope to facilitate the exploration of autoformalization
and theorem proving in advanced math. Experiments show that FormaRL can
increase the pass@1 autoformalization accuracy of Qwen2.5-Coder-7B-Instruct by
4 $\sim$ 6x (4.04\% $\to$ 26.15\% on ProofNet and 2.4\% $\to$ 9.6\% on uproof)
with merely 859 unlabeled data. And on uproof our method also achieved a strong
improvement in out-of-distribution performance compared to existing open-source
state-of-the-art autoformalizers on both pass@1 accuracy (6.2\% $\to$ 9.6\%)
and pass@16 accuracy (24.4\% $\to$ 33.6\%). Training code of FormaRL is
open-sourced at https://github.com/THUNLP-MT/FormaRL.

</details>


### [29] [Who Is Lagging Behind: Profiling Student Behaviors with Graph-Level Encoding in Curriculum-Based Online Learning Systems](https://arxiv.org/abs/2508.18925)
*Qian Xiao,Conn Breathnach,Ioana Ghergulescu,Conor O'Sullivan,Keith Johnston,Vincent Wade*

Main category: cs.AI

TL;DR: 本研究提出了一种名为CTGraph的图级表示学习方法，以自监督方式对智能辅导系统（ITSs）中的学生行为和表现进行画像，旨在识别学习困难学生并缩小学习差距。


<details>
  <summary>Details</summary>
Motivation: 智能辅导系统（ITSs）的普及可能无意中加剧学生的表现差距。因此，通过跟踪学习进度、识别困难学生和缓解学生间差异，学生画像变得至关重要。这需要衡量学生在内容覆盖、学习强度和概念掌握等方面的行为和表现。

Method: 本研究引入了CTGraph，这是一种图级表示学习方法，用于以自监督的方式对学生的行为和表现进行画像。该方法能够综合考虑学生行为、表现的各个方面以及学习路径的变化，并与课程结构对齐。

Result: 实验证明，CTGraph能提供学生学习历程的整体视图，有效识别学习困难学生，并能对不同学生群体进行比较分析，从而精确指出学生在何时何地遇到困难。

Conclusion: CTGraph为教育工作者提供了深入了解学生学习历程的丰富洞察力，为更具针对性的干预措施铺平了道路，从而赋能教育者。

Abstract: The surge in the adoption of Intelligent Tutoring Systems (ITSs) in
education, while being integral to curriculum-based learning, can inadvertently
exacerbate performance gaps. To address this problem, student profiling becomes
crucial for tracking progress, identifying struggling students, and alleviating
disparities among students. Such profiling requires measuring student behaviors
and performance across different aspects, such as content coverage, learning
intensity, and proficiency in different concepts within a learning topic.
  In this study, we introduce CTGraph, a graph-level representation learning
approach to profile learner behaviors and performance in a self-supervised
manner. Our experiments demonstrate that CTGraph can provide a holistic view of
student learning journeys, accounting for different aspects of student
behaviors and performance, as well as variations in their learning paths as
aligned to the curriculum structure. We also show that our approach can
identify struggling students and provide comparative analysis of diverse groups
to pinpoint when and where students are struggling. As such, our approach opens
more opportunities to empower educators with rich insights into student
learning journeys and paves the way for more targeted interventions.

</details>


### [30] [VISION: Robust and Interpretable Code Vulnerability Detection Leveraging Counterfactual Augmentation](https://arxiv.org/abs/2508.18933)
*David Egea,Barproda Halder,Sanghamitra Dutta*

Main category: cs.AI

TL;DR: 本研究提出VISION框架，通过使用大型语言模型（LLM）生成反事实数据来训练图神经网络（GNN），从而缓解虚假关联、提高代码漏洞检测的鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 源代码漏洞的自动化检测是网络安全的关键挑战。尽管图神经网络（GNNs）在学习代码结构和逻辑关系方面表现出潜力，但其性能受限于训练数据不平衡和标签噪声，导致GNNs学习到表层代码相似性中的“虚假”关联，泛化能力差。

Method: 本研究提出了一个名为VISION的统一框架，用于鲁棒和可解释的漏洞检测。该框架包括：(i) 通过LLM生成反事实样本（语义修改最小但标签相反的样本）；(ii) 对成对的、标签相反的代码示例进行有针对性的GNN训练；(iii) 基于图的可解释性分析，识别与漏洞预测相关的关键代码语句，并忽略虚假关联。

Result: VISION框架显著减少了虚假学习，实现了更鲁棒、更具泛化性的检测。在CWE-20漏洞检测任务上，整体准确率从51.8%提高到97.8%，成对对比准确率从4.5%提高到95.8%，最差组准确率从0.7%提高到85.5%。研究还通过提出的类内归因方差、类间归因距离和节点分数依赖等指标展示了其优势。此外，还发布了包含27,556个函数（真实和反事实）的CWE-20-CFA基准数据集。

Conclusion: VISION框架通过系统地增强反事实训练数据集，有效缓解了GNN在漏洞检测中学习虚假关联的问题，显著提高了检测的准确性、鲁棒性和泛化能力。它还通过交互式可视化促进了透明和值得信赖的AI网络安全系统，并发布了新的基准数据集。

Abstract: Automated detection of vulnerabilities in source code is an essential
cybersecurity challenge, underpinning trust in digital systems and services.
Graph Neural Networks (GNNs) have emerged as a promising approach as they can
learn structural and logical code relationships in a data-driven manner.
However, their performance is severely constrained by training data imbalances
and label noise. GNNs often learn 'spurious' correlations from superficial code
similarities, producing detectors that fail to generalize well to unseen
real-world data. In this work, we propose a unified framework for robust and
interpretable vulnerability detection, called VISION, to mitigate spurious
correlations by systematically augmenting a counterfactual training dataset.
Counterfactuals are samples with minimal semantic modifications but opposite
labels. Our framework includes: (i) generating counterfactuals by prompting a
Large Language Model (LLM); (ii) targeted GNN training on paired code examples
with opposite labels; and (iii) graph-based interpretability to identify the
crucial code statements relevant for vulnerability predictions while ignoring
spurious ones. We find that VISION reduces spurious learning and enables more
robust, generalizable detection, improving overall accuracy (from 51.8% to
97.8%), pairwise contrast accuracy (from 4.5% to 95.8%), and worst-group
accuracy (from 0.7% to 85.5%) on the Common Weakness Enumeration (CWE)-20
vulnerability. We further demonstrate gains using proposed metrics: intra-class
attribution variance, inter-class attribution distance, and node score
dependency. We also release CWE-20-CFA, a benchmark of 27,556 functions (real
and counterfactual) from the high-impact CWE-20 category. Finally, VISION
advances transparent and trustworthy AI-based cybersecurity systems through
interactive visualization for human-in-the-loop analysis.

</details>


### [31] [Novel Approaches to Artificial Intelligence Development Based on the Nearest Neighbor Method](https://arxiv.org/abs/2508.18953)
*I. I. Priezzhev,D. A. Danko,A. V. Shubin*

Main category: cs.AI

TL;DR: 本文提出了一种基于分层聚类结构的最近邻方法，以解决现代神经网络（包括大型语言模型）的幻觉、高计算复杂度、微调成本高和灾难性遗忘等局限性，并实现了显著的计算效率提升和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现代神经网络技术（如大型语言模型）在应用中面临幻觉效应、高计算复杂性、昂贵的微调和灾难性遗忘等根本性局限，这些问题严重阻碍了它们在医疗、工业过程管理和科学研究等关键领域的应用。

Method: 该方法基于k-最近邻算法，以减少或消除幻觉效应并简化模型扩展和微调。为克服k-最近邻方法的高计算负荷，论文提出使用基于Kohonen自组织映射的树状数据结构来加速最近邻搜索。

Result: 在手写数字识别和简单字幕翻译任务上的测试表明，该方法有效。与穷举搜索方法相比，最近邻搜索时间减少了数百倍，而准确性仅略有下降。

Conclusion: 所提出的方法具有透明性和可解释性，与人类认知机制紧密结合，在需要高可靠性和可解释结果的任务中具有广泛应用的潜力。

Abstract: Modern neural network technologies, including large language models, have
achieved remarkable success in various applied artificial intelligence
applications, however, they face a range of fundamental limitations. Among them
are hallucination effects, high computational complexity of training and
inference, costly fine-tuning, and catastrophic forgetting issues. These
limitations significantly hinder the use of neural networks in critical areas
such as medicine, industrial process management, and scientific research. This
article proposes an alternative approach based on the nearest neighbors method
with hierarchical clustering structures. Employing the k-nearest neighbors
algorithm significantly reduces or completely eliminates hallucination effects
while simplifying model expansion and fine-tuning without the need for
retraining the entire network. To overcome the high computational load of the
k-nearest neighbors method, the paper proposes using tree-like data structures
based on Kohonen self-organizing maps, thereby greatly accelerating nearest
neighbor searches. Tests conducted on handwritten digit recognition and simple
subtitle translation tasks confirmed the effectiveness of the proposed
approach. With only a slight reduction in accuracy, the nearest neighbor search
time was reduced hundreds of times compared to exhaustive search methods. The
proposed method features transparency and interpretability, closely aligns with
human cognitive mechanisms, and demonstrates potential for extensive use in
tasks requiring high reliability and explainable results.

</details>


### [32] [Enabling MoE on the Edge via Importance-Driven Expert Scheduling](https://arxiv.org/abs/2508.18983)
*Guoying Zhu,Meng Li,Haipeng Dai,Xuechen Liu,Weijun Wang,Keran Li,Jun xiao,Ligeng Chen,Wei Wang*

Main category: cs.AI

TL;DR: 本文提出了一种针对消费级边缘硬件上MoE模型部署的动态专家卸载和调度方法。通过利用专家重要性进行替换和优化调度，该方法显著降低了解码延迟和内存使用，同时保持了近乎无损的精度。


<details>
  <summary>Details</summary>
Motivation: 在消费级边缘硬件上部署MoE模型面临设备内存受限的挑战，需要动态专家卸载。现有工作将卸载视为纯粹的调度问题，未能有效解决内存和精度之间的平衡。

Method: 本文利用专家重要性指导卸载决策，将低重要性的激活专家替换为GPU内存中已缓存的功能相似专家，以保持精度。此外，引入了一种调度策略，最大化GPU缓存专家的重用率，进一步提高效率。

Result: 该方法降低了内存使用和数据传输，基本消除了PCIe开销。实验结果显示，解码延迟降低了48%，专家缓存命中率超过60%，同时保持了近乎无损的精度。

Conclusion: 通过结合专家重要性指导的替换和优化的调度策略，本文提出的方法成功地在边缘设备上实现了MoE模型的部署，显著提升了性能（更低的延迟、更高的缓存命中率）并维持了高精度。

Abstract: The Mixture of Experts (MoE) architecture has emerged as a key technique for
scaling Large Language Models by activating only a subset of experts per query.
Deploying MoE on consumer-grade edge hardware, however, is constrained by
limited device memory, making dynamic expert offloading essential. Unlike prior
work that treats offloading purely as a scheduling problem, we leverage expert
importance to guide decisions, substituting low-importance activated experts
with functionally similar ones already cached in GPU memory, thereby preserving
accuracy. As a result, this design reduces memory usage and data transfer,
while largely eliminating PCIe overhead. In addition, we introduce a scheduling
policy that maximizes the reuse ratio of GPU-cached experts, further boosting
efficiency. Extensive evaluations show that our approach delivers 48% lower
decoding latency with over 60% expert cache hit rate, while maintaining nearly
lossless accuracy.

</details>


### [33] [AI Models Exceed Individual Human Accuracy in Predicting Everyday Social Norms](https://arxiv.org/abs/2508.19004)
*Pontus Strimling,Simon Karlsson,Irina Vartanova,Kimmo Eriksson*

Main category: cs.AI

TL;DR: 研究发现大型语言模型仅通过统计学习就能对社会规范表现出复杂的理解，甚至在预测人类社会适宜性判断方面超越了绝大多数人类个体，但它们也存在系统性错误。


<details>
  <summary>Details</summary>
Motivation: 认知科学中的一个基本问题是社会规范如何习得和表征。鉴于人类通常通过具身化的社会经验学习规范，研究旨在探讨大型语言模型是否仅通过统计学习就能达到复杂的规范理解，以挑战强调具身经验必要性的理论。

Method: 研究通过两个实验系统评估了多个AI系统（如GPT-4.5、Gemini 2.5 Pro、GPT-5、Claude Sonnet 4）预测人类对555个日常情景的社会适宜性判断的能力。具体方法是将AI的预测与人类的平均判断以及每个个体人类参与者的判断进行比较。

Result: 在实验1中，GPT-4.5在连续尺度上预测集体判断的准确性超越了所有人类参与者（100%）。实验2复制了这一结果，Gemini 2.5 Pro超越了98.7%的人类，GPT-5超越了97.8%，Claude Sonnet 4超越了96.0%。尽管具有强大的预测能力，所有模型都表现出系统性、相关联的错误。

Conclusion: 这些发现表明，复杂的社会认知模型可以仅通过对语言数据的统计学习而出现，挑战了强调具身经验对文化能力具有排他性必要性的理论。AI在不同架构中表现出的系统性局限性表明了基于模式的社会理解的潜在边界，而模型在预测任务中超越几乎所有个体人类的能力则表明语言是文化知识传播的极其丰富的载体。

Abstract: A fundamental question in cognitive science concerns how social norms are
acquired and represented. While humans typically learn norms through embodied
social experience, we investigated whether large language models can achieve
sophisticated norm understanding through statistical learning alone. Across two
studies, we systematically evaluated multiple AI systems' ability to predict
human social appropriateness judgments for 555 everyday scenarios by examining
how closely they predicted the average judgment compared to each human
participant. In Study 1, GPT-4.5's accuracy in predicting the collective
judgment on a continuous scale exceeded that of every human participant (100th
percentile). Study 2 replicated this, with Gemini 2.5 Pro outperforming 98.7%
of humans, GPT-5 97.8%, and Claude Sonnet 4 96.0%. Despite this predictive
power, all models showed systematic, correlated errors. These findings
demonstrate that sophisticated models of social cognition can emerge from
statistical learning over linguistic data alone, challenging strong versions of
theories emphasizing the exclusive necessity of embodied experience for
cultural competence. The systematic nature of AI limitations across different
architectures indicates potential boundaries of pattern-based social
understanding, while the models' ability to outperform nearly all individual
humans in this predictive task suggests that language serves as a remarkably
rich repository for cultural knowledge transmission.

</details>


### [34] [Building Self-Evolving Agents via Experience-Driven Lifelong Learning: A Framework and Benchmark](https://arxiv.org/abs/2508.19005)
*Yuxuan Cai,Yipeng Hao,Jie Zhou,Hang Yan,Zhikai Lei,Rui Zhen,Zhenhua Han,Yutao Yang,Junsong Li,Qianjun Pan,Tianyu Huai,Qin Chen,Xin Li,Kai Chen,Bo Zhang,Xipeng Qiu,Liang He*

Main category: cs.AI

TL;DR: 本文提出了一种名为“经验驱动的终身学习（ELL）”的框架，用于构建通过真实世界互动持续学习和自我进化的智能体，并引入了“StuLife”基准数据集来评估其终身学习能力。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能向通用智能发展，研究重点正从针对静态任务优化的系统转向创建能够持续学习的开放式智能体。现有系统难以实现持续成长和与现实世界互动。

Method: 本文提出了ELL框架，其基于四个核心原则：1) 经验探索：智能体通过与动态环境的自我激励互动进行学习。2) 长期记忆：智能体将历史知识（经验、专业知识、常识）存储为持久记忆。3) 技能学习：智能体从经验中抽象出可重用技能并进行改进。4) 知识内化：智能体将显式经验内化为隐式能力。同时，引入了StuLife基准数据集，模拟学生大学旅程，包含三个阶段和十个子场景，用于评估记忆保留、技能迁移和自我激励行为。

Result: StuLife基准数据集提供了一个全面的平台，用于评估智能体的终身学习能力，包括记忆保留、技能迁移和自我激励行为。该工作还探索了上下文工程在推进AGI中的作用，并可用于评估最先进的大型语言模型（LLMs）。

Conclusion: ELL框架和StuLife基准为构建和评估能够通过持续互动自我进化、不断学习的智能体提供了理论和实践基础，这对于实现通用人工智能（AGI）至关重要。

Abstract: As AI advances toward general intelligence, the focus is shifting from
systems optimized for static tasks to creating open-ended agents that learn
continuously. In this paper, we introduce Experience-driven Lifelong Learning
(ELL), a framework for building self-evolving agents capable of continuous
growth through real-world interaction. The framework is built on four core
principles: (1) Experience Exploration: Agents learn through continuous,
self-motivated interaction with dynamic environments, navigating interdependent
tasks and generating rich experiential trajectories. (2) Long-term Memory:
Agents preserve and structure historical knowledge, including personal
experiences, domain expertise, and commonsense reasoning, into a persistent
memory system. (3) Skill Learning: Agents autonomously improve by abstracting
recurring patterns from experience into reusable skills, which are actively
refined and validated for application in new tasks. (4) Knowledge
Internalization: Agents internalize explicit and discrete experiences into
implicit and intuitive capabilities as "second nature".
  We also introduce StuLife, a benchmark dataset for ELL that simulates a
student's holistic college journey, from enrollment to academic and personal
development, across three core phases and ten detailed sub-scenarios. StuLife
is designed around three key paradigm shifts: From Passive to Proactive, From
Context to Memory, and From Imitation to Learning. In this dynamic environment,
agents must acquire and distill practical skills and maintain persistent memory
to make decisions based on evolving state variables. StuLife provides a
comprehensive platform for evaluating lifelong learning capabilities, including
memory retention, skill transfer, and self-motivated behavior. Beyond
evaluating SOTA LLMs on the StuLife benchmark, we also explore the role of
context engineering in advancing AGI.

</details>


### [35] [Sense of Self and Time in Borderline Personality. A Comparative Robustness Study with Generative AI](https://arxiv.org/abs/2508.19008)
*Marcin Moskalewicz,Anna Sterna,Marek Pokropski,Paula Flores*

Main category: cs.AI

TL;DR: 本研究评估大型语言模型（LLMs）在支持边缘性人格障碍（BPD）患者第一人称经验现象学定性分析方面的能力。结果显示，Gemini模型表现最佳，其输出与人类分析最接近，并被专家判断为人类生成，展示了AI辅助分析在减少人类解释偏差方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探索大型语言模型（LLMs）在现象学定性分析（特别是针对边缘性人格障碍中对时间性和自我障碍的理解）中的应用能力，并评估其是否能辅助或替代人类分析，以减少人类解释中的潜在偏差。

Method: 研究以24名住院BPD患者的生命故事访谈数据为基础，这些数据已有人类主导的主题分析。研究比较了三种大型语言模型（OpenAI GPT-4o、Google Gemini 2.5 Pro、Anthropic Claude Opus 4），并提示它们模仿原始研究者的解释风格。评估由盲审和非盲审的现象学与临床心理学专家进行，评估指标包括语义一致性、Jaccard系数以及多维度有效性评分（可信度、连贯性、实质性、数据基础）。

Result: 模型与人类分析的重叠率差异显著：GPT为0%，Claude为42%，Gemini为58%。Jaccard系数较低（0.21-0.28）。然而，模型能够发现人类分析中遗漏的主题。Gemini的输出与人类分析最为接近，其有效性评分显著高于GPT和Claude（p < 0.0001），并被盲审专家判断为人类生成。所有评分与主题文本量和词数呈强相关（R > 0.78）。

Conclusion: AI增强的主题分析，特别是通过Gemini等高性能模型，具有支持定性分析和减轻人类解释偏差的潜力。尽管不同大型语言模型的表现存在差异，但AI辅助方法在处理复杂的第一人称经验数据方面展现出显著的可能性。

Abstract: This study examines the capacity of large language models (LLMs) to support
phenomenological qualitative analysis of first-person experience in Borderline
Personality Disorder (BPD), understood as a disorder of temporality and
selfhood. Building on a prior human-led thematic analysis of 24 inpatients'
life-story interviews, we compared three LLMs (OpenAI GPT-4o, Google Gemini 2.5
Pro, Anthropic Claude Opus 4) prompted to mimic the interpretative style of the
original investigators. The models were evaluated with blinded and non-blinded
expert judges in phenomenology and clinical psychology. Assessments included
semantic congruence, Jaccard coefficients, and multidimensional validity
ratings (credibility, coherence, substantiveness, and groundness in data).
Results showed variable overlap with the human analysis, from 0 percent in GPT
to 42 percent in Claude and 58 percent in Gemini, and a low Jaccard coefficient
(0.21-0.28). However, the models recovered themes omitted by humans. Gemini's
output most closely resembled the human analysis, with validity scores
significantly higher than GPT and Claude (p < 0.0001), and was judged as human
by blinded experts. All scores strongly correlated (R > 0.78) with the quantity
of text and words per theme, highlighting both the variability and potential of
AI-augmented thematic analysis to mitigate human interpretative bias.

</details>


### [36] [MAB Optimizer for Estimating Math Question Difficulty via Inverse CV without NLP](https://arxiv.org/abs/2508.19014)
*Surajit Das,Gourav Roy,Aleksei Eliseev,Ram Kumar Rajendran*

Main category: cs.AI

TL;DR: 本研究提出了一种名为APME的强化学习多臂老虎机（MAB）框架，该框架仅通过学习者表现数据（得分和时间）来估计问题难度，无需语言特征或专家标签。它在不同数据集上表现出高精度和鲁棒性，特别是在符号领域优于现有基线方法，并支持自适应评估。


<details>
  <summary>Details</summary>
Motivation: 随着智能与自主辅导系统（IATS）的发展，需要客观、领域无关的问题难度评估方法。传统人工标注主观性强，现有基于NLP的方法在代数等符号领域无效。

Method: 本研究引入了“学习者被动测量方法”（APME），这是一个基于强化学习的多臂老虎机（MAB）框架。它仅从学习者的表现数据（获得的成绩和花费的时间）中估计难度，无需语言特征或专家标签。模型利用变异系数的倒数作为风险调整指标，提供可解释且可扩展的自适应评估机制。

Result: 该模型在三个异构数据集上进行了实证验证，平均R2达到0.9213，平均RMSE为0.0584，证实了其在不同教育水平和评估形式下的鲁棒性、准确性和适应性。与回归、NLP驱动和IRT模型等基线方法相比，该框架始终表现更优，尤其是在纯符号领域。研究发现：(i) 项目异质性强烈影响感知难度；(ii) 求解结果的方差与平均表现对于自适应分配同样关键。

Conclusion: 该模型与维果茨基的最近发展区理论相符，通过平衡挑战性和可实现性来识别任务，从而支持学习动机并最大程度地减少脱离感。这种领域无关、自监督的方法推动了IATS中的难度标注技术，并可扩展到任何具有学习者交互数据的领域，超越了代数范畴。

Abstract: The evolution of technology and education is driving the emergence of
Intelligent & Autonomous Tutoring Systems (IATS), where objective and
domain-agnostic methods for determining question difficulty are essential.
Traditional human labeling is subjective, and existing NLP-based approaches
fail in symbolic domains like algebra. This study introduces the Approach of
Passive Measures among Educands (APME), a reinforcement learning-based
Multi-Armed Bandit (MAB) framework that estimates difficulty solely from solver
performance data -- marks obtained and time taken -- without requiring
linguistic features or expert labels. By leveraging the inverse coefficient of
variation as a risk-adjusted metric, the model provides an explainable and
scalable mechanism for adaptive assessment. Empirical validation was conducted
on three heterogeneous datasets. Across these diverse contexts, the model
achieved an average R2 of 0.9213 and an average RMSE of 0.0584, confirming its
robustness, accuracy, and adaptability to different educational levels and
assessment formats. Compared with baseline approaches-such as regression-based,
NLP-driven, and IRT models-the proposed framework consistently outperformed
alternatives, particularly in purely symbolic domains. The findings highlight
that (i) item heterogeneity strongly influences perceived difficulty, and (ii)
variance in solver outcomes is as critical as mean performance for adaptive
allocation. Pedagogically, the model aligns with Vygotskys Zone of Proximal
Development by identifying tasks that balance challenge and attainability,
supporting motivation while minimizing disengagement. This domain-agnostic,
self-supervised approach advances difficulty tagging in IATS and can be
extended beyond algebra wherever solver interaction data is available

</details>


### [37] [Investigating Advanced Reasoning of Large Language Models via Black-Box Interaction](https://arxiv.org/abs/2508.19035)
*Congchi Yin,Tianyi Wu,Yankai Shu,Alex Gu,Yunhan Wang,Jun Shao,Xun Jiang,Piji Li*

Main category: cs.AI

TL;DR: 本文提出了一种名为“黑盒交互”的新评估范式和Oracle基准，以评估大型语言模型（LLMs）在未知、交互式环境中的综合推理能力，发现LLMs在规划高效探索策略方面存在普遍不足。


<details>
  <summary>Details</summary>
Motivation: 现有任务无法在交互式、未知环境中有效评估LLMs的综合推理能力，导致对演绎、归纳和溯因推理的评估是孤立的，忽视了人类发现真实世界所需的整合推理过程。

Method: 引入了“黑盒交互”的评估范式，其中LLMs需要通过在给定探索回合内与黑盒（由隐藏函数定义）交互并基于观察到的输入-输出对进行推理来揭示隐藏函数。基于此，构建了包含6种黑盒任务和96个黑盒的Oracle基准，并对19个现代LLMs进行了基准测试。

Result: 在6种任务中，o3在其中5种任务中排名第一，在大多数简单黑盒上准确率超过70%。然而，在一些困难黑盒任务上，其平均性能下降到40%以下。进一步分析表明，LLMs普遍存在一个困难：它们缺乏开发高效自适应探索策略以完善假设的高级规划能力。

Conclusion: “黑盒交互”范式和Oracle基准提供了一种评估LLMs综合推理能力的新方法。尽管领先模型在简单任务上表现良好，但LLMs在面对复杂、交互式推理任务时，普遍缺乏有效的高级规划能力来指导探索和假设完善，这限制了它们在未知环境中的推理表现。

Abstract: Existing tasks fall short in evaluating reasoning ability of Large Language
Models (LLMs) in an interactive, unknown environment. This deficiency leads to
the isolated assessment of deductive, inductive, and abductive reasoning,
neglecting the integrated reasoning process that is indispensable for humans
discovery of real world. We introduce a novel evaluation paradigm,
\textit{black-box interaction}, to tackle this challenge. A black-box is
defined by a hidden function that maps a specific set of inputs to outputs.
LLMs are required to unravel the hidden function behind the black-box by
interacting with it in given exploration turns, and reasoning over observed
input-output pairs. Leveraging this idea, we build the \textsc{Oracle}
benchmark which comprises 6 types of black-box task and 96 black-boxes. 19
modern LLMs are benchmarked. o3 ranks first in 5 of the 6 tasks, achieving over
70\% accuracy on most easy black-boxes. But it still struggles with some hard
black-box tasks, where its average performance drops below 40\%. Further
analysis indicates a universal difficulty among LLMs: They lack the high-level
planning capability to develop efficient and adaptive exploration strategies
for hypothesis refinement.

</details>


### [38] [A Concurrent Modular Agent: Framework for Autonomous LLM Agents](https://arxiv.org/abs/2508.19042)
*Norihiro Maruyama,Takahide Yoshida,Hiroki Sato,Atsushi Masumori,Johnsmith,Takashi Ikegami*

Main category: cs.AI

TL;DR: 本文提出了并发模块化智能体（CMA）框架，它协调多个基于LLM的模块异步运行，同时保持连贯和容错的行为循环，实现了意图的涌现，并支持明斯基的“心智社会”理论。


<details>
  <summary>Details</summary>
Motivation: 解决现有智能体架构中长期存在的难题，通过自主进程间基于语言的交互，实现意图的涌现，并使智能体能够展现灵活、适应性强且依赖于上下文的行为。

Method: CMA框架通过并发执行的LLM模块、模块间通信和单一共享全局状态相结合，实现异步操作、保持连贯性和容错性。该方法将推理任务卸载给LLM，被视为明斯基“心智社会”理论的实际实现。

Result: 通过两个实际用例研究验证了系统的可行性。观察到系统中涌现出复杂的认知现象（如自我意识），这表明复杂认知可能源于更简单过程的有序交互。

Conclusion: CMA框架支持明斯基的“心智社会”概念，通过展示复杂认知现象可从简单过程的有序交互中涌现，为人工智能研究开辟了新途径。

Abstract: We introduce the Concurrent Modular Agent (CMA), a framework that
orchestrates multiple Large-Language-Model (LLM)-based modules that operate
fully asynchronously yet maintain a coherent and fault-tolerant behavioral
loop. This framework addresses long-standing difficulties in agent
architectures by letting intention emerge from language-mediated interactions
among autonomous processes. This approach enables flexible, adaptive, and
context-dependent behavior through the combination of concurrently executed
modules that offload reasoning to an LLM, inter-module communication, and a
single shared global state.We consider this approach to be a practical
realization of Minsky's Society of Mind theory. We demonstrate the viability of
our system through two practical use-case studies. The emergent properties
observed in our system suggest that complex cognitive phenomena like
self-awareness may indeed arise from the organized interaction of simpler
processes, supporting Minsky-Society of Mind concept and opening new avenues
for artificial intelligence research. The source code for our work is available
at: https://github.com/AlternativeMachine/concurrent-modular-agent.

</details>


### [39] [Can Structured Templates Facilitate LLMs in Tackling Harder Tasks? : An Exploration of Scaling Laws by Difficulty](https://arxiv.org/abs/2508.19069)
*Zhichao Yang,Zhaoxin Fan,Gen Li,Yuanze Hu,Xinyu Wang,Ye Qiu,Xin Wang,Yifan Sun,Wenjun Wu*

Main category: cs.AI

TL;DR: 本文提出了一种结构化解决方案模板（SST）框架，通过利用难度标度定律和课程学习，显著提升了大型语言模型在数学等复杂任务中的结构化程序推理能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在复杂任务中缺乏深层程序逻辑，尤其是在数学领域。研究发现存在一个“难度标度定律”，即模型性能与训练数据复杂度呈U型曲线关系，过多的低难度数据会阻碍抽象能力，而高难度数据能显著增强推理能力。这促使研究者寻求一种明确教授程序推理的方法。

Method: 本文提出了结构化解决方案模板（SST）框架，包含三个核心部分：1) 使用结构化解决方案模板链和动态加权损失进行微调，以优先处理程序逻辑；2) 在提示时注入解决方案模板作为认知支架来指导推理；3) 集成课程微调，明确教导模型进行“自我规划-执行-自我纠正”。

Result: 在GSM8K、AIME24和新的Dynamic En基准测试上的实验表明，SST显著提高了模型的准确性和效率，尤其是在解决较难问题方面。

Conclusion: SST框架通过明确教授结构化程序推理，有效解决了LLMs在复杂任务中深层程序逻辑不足的问题，并显著提升了模型在数学等领域的推理能力和效率。

Abstract: Structured, procedural reasoning is essential for Large Language Models
(LLMs), especially in mathematics. While post-training methods have improved
LLM performance, they still fall short in capturing deep procedural logic on
complex tasks. To tackle the issue, in this paper, we first investigate this
limitation and uncover a novel finding: a Scaling Law by Difficulty, which
reveals that model performance follows a U-shaped curve with respect to
training data complexity -- excessive low-difficulty data impedes abstraction,
while high-difficulty data significantly enhances reasoning ability. Motivated
by this, we propose the Structured Solution Template (SST) framework, which
uses solution templates and a curriculum of varied difficulty to explicitly
teach procedural reasoning. Specifically, SST comprises (1) fine-tuning with
structured solution-template chains and dynamically weighted loss to prioritize
procedural logic, (2) prompt-time injection of solution templates as cognitive
scaffolds to guide inference, and (3) integrated curriculum fine-tuning that
explicitly teaches the model to self-plan - execute - self-correct. Experiments
on GSM8K, AIME24, and new Dynamic En benchmark show that SST significantly
improves both accuracy and efficiency, especially on harder problems.

</details>


### [40] [Trustworthy Agents for Electronic Health Records through Confidence Estimation](https://arxiv.org/abs/2508.19096)
*Yongwoo Song,Minbyul Jeong,Mujeen Sung*

Main category: cs.AI

TL;DR: 针对LLM在电子健康记录(EHR)信息提取中的幻觉风险，本文提出了HCAcc@k%指标和TrustEHRAgent，显著提升了临床问答的可靠性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(LLMs)在从电子健康记录(EHR)中提取信息和支持临床决策方面展现出潜力，但其幻觉风险是临床部署面临的主要挑战。

Method: 本文提出了一种新颖的度量标准“Hallucination Controlled Accuracy at k% (HCAcc@k%)”，用于量化在不同置信度阈值下的准确性-可靠性权衡。同时，引入了TrustEHRAgent，一个置信度感知代理，它结合了逐步置信度估计来进行临床问答。

Result: 在MIMIC-III和eICU数据集上的实验表明，TrustEHRAgent在严格的可靠性约束下优于基线方法，在HCAcc@70%时分别实现了44.23%p和25.34%p的改进，而基线方法在这些阈值下均告失败。这些结果也突显了传统准确性指标在评估医疗AI代理方面的局限性。

Conclusion: 本研究致力于开发可信赖的临床代理，这些代理能够提供准确的信息，或在置信度较低时透明地表达不确定性，从而促进医疗领域AI的应用。

Abstract: Large language models (LLMs) show promise for extracting information from
Electronic Health Records (EHR) and supporting clinical decisions. However,
deployment in clinical settings faces challenges due to hallucination risks. We
propose Hallucination Controlled Accuracy at k% (HCAcc@k%), a novel metric
quantifying the accuracy-reliability trade-off at varying confidence
thresholds. We introduce TrustEHRAgent, a confidence-aware agent incorporating
stepwise confidence estimation for clinical question answering. Experiments on
MIMIC-III and eICU datasets show TrustEHRAgent outperforms baselines under
strict reliability constraints, achieving improvements of 44.23%p and 25.34%p
at HCAcc@70% while baseline methods fail at these thresholds. These results
highlight limitations of traditional accuracy metrics in evaluating healthcare
AI agents. Our work contributes to developing trustworthy clinical agents that
deliver accurate information or transparently express uncertainty when
confidence is low.

</details>


### [41] [Reasoning LLMs in the Medical Domain: A Literature Survey](https://arxiv.org/abs/2508.19097)
*Armin Berger,Sarthak Khanna,David Berghaus,Rafet Sifa*

Main category: cs.AI

TL;DR: 本综述探讨了大型语言模型（LLMs）在医疗领域从信息检索到高级临床推理的变革性发展，分析了其技术基础、评估方法和挑战，并提出了开发可靠医疗LLMs的路线图。


<details>
  <summary>Details</summary>
Motivation: LLMs先进的推理能力对医疗应用具有变革性意义，不仅扩展了功能，还增强了在医疗环境中至关重要的决策透明度和可解释性。

Method: 本综述审视了医疗LLMs的演变，深入分析了其技术基础（如Chain-of-Thought提示技术和强化学习突破如DeepSeek-R1），评估了专用医疗框架，考察了多智能体协作系统等新兴范式，并批判性评估了当前医疗验证的评估方法，同时探讨了领域解释限制、偏见缓解、患者安全和多模态数据整合等挑战。

Result: 本综述提供了对医疗LLMs技术基础、新兴范式和评估方法的全面分析，并识别了该领域面临的挑战，如解释限制、偏见缓解、患者安全框架和多模态临床数据整合。

Conclusion: 本综述旨在为开发可靠的LLMs提供一个路线图，使其能有效成为临床实践和医学研究中的合作伙伴。

Abstract: The emergence of advanced reasoning capabilities in Large Language Models
(LLMs) marks a transformative development in healthcare applications. Beyond
merely expanding functional capabilities, these reasoning mechanisms enhance
decision transparency and explainability-critical requirements in medical
contexts. This survey examines the transformation of medical LLMs from basic
information retrieval tools to sophisticated clinical reasoning systems capable
of supporting complex healthcare decisions. We provide a thorough analysis of
the enabling technological foundations, with a particular focus on specialized
prompting techniques like Chain-of-Thought and recent breakthroughs in
Reinforcement Learning exemplified by DeepSeek-R1. Our investigation evaluates
purpose-built medical frameworks while also examining emerging paradigms such
as multi-agent collaborative systems and innovative prompting architectures.
The survey critically assesses current evaluation methodologies for medical
validation and addresses persistent challenges in field interpretation
limitations, bias mitigation strategies, patient safety frameworks, and
integration of multimodal clinical data. Through this survey, we seek to
establish a roadmap for developing reliable LLMs that can serve as effective
partners in clinical practice and medical research.

</details>


### [42] [Hybrid Deep Searcher: Integrating Parallel and Sequential Search Reasoning](https://arxiv.org/abs/2508.19113)
*Dayoon Ko,Jihyuk Kim,Haeju Park,Sohyeon Kim,Dahyun Lee,Yongrae Jo,Gunhee Kim,Moontae Lee,Kyungjae Lee*

Main category: cs.AI

TL;DR: 本研究引入HDS-QA数据集和HybridDeepSearcher模型，旨在训练大型推理模型（LRMs）识别并利用混合（并行与序列）查询策略，以降低推理延迟并提高效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过顺序集成外部知识检索来增强LRMs，但这导致推理延迟增加、上下文长度过长，并可能降低连贯性和准确性。

Method: 研究者创建了HDS-QA，一个从Natural Questions自动生成的合成数据集，专门用于训练LRMs区分可并行和序列依赖的子查询。HDS-QA包含混合跳数问题和涉及并行查询的合成推理-查询-检索路径。在此数据集上微调了一个LRM，命名为HybridDeepSearcher。

Result: HybridDeepSearcher在多个基准测试中超越了现有最先进的基线，尤其在FanOutQA和BrowseComp子集上分别实现了+15.9和+11.5 F1的提升。实验结果表明，HybridDeepSearcher在更少的搜索轮次下达到相当的准确性，显著减少了推理延迟，并且在允许更多轮次时能有效扩展。

Conclusion: 这些结果证明，明确训练LRMs利用混合并行和序列查询策略，可以显著提高模型的效率、可扩展性和有效性。

Abstract: Large reasoning models (LRMs) have demonstrated strong performance in
complex, multi-step reasoning tasks. Existing methods enhance LRMs by
sequentially integrating external knowledge retrieval; models iteratively
generate queries, retrieve external information, and progressively reason over
this information. However, purely sequential querying increases inference
latency and context length, diminishing coherence and potentially reducing
accuracy. To address these limitations, we introduce HDS-QA (Hybrid Deep Search
QA), a synthetic dataset automatically generated from Natural Questions,
explicitly designed to train LRMs to distinguish parallelizable from sequential
queries. HDS-QA comprises hybrid-hop questions that combine parallelizable
independent subqueries (executable simultaneously) and sequentially dependent
subqueries (requiring step-by-step resolution), along with synthetic
reasoning-querying-retrieval paths involving parallel queries. We fine-tune an
LRM using HDS-QA, naming the model HybridDeepSearcher, which outperforms
state-of-the-art baselines across multiple benchmarks, notably achieving +15.9
and +11.5 F1 on FanOutQA and a subset of BrowseComp, respectively, both
requiring comprehensive and exhaustive search. Experimental results highlight
two key advantages: HybridDeepSearcher reaches comparable accuracy with fewer
search turns, significantly reducing inference latency, and it effectively
scales as more turns are permitted. These results demonstrate the efficiency,
scalability, and effectiveness of explicitly training LRMs to leverage hybrid
parallel and sequential querying.

</details>


### [43] [Algorithmic Collective Action with Multiple Collectives](https://arxiv.org/abs/2508.19149)
*Claudio Battiloro,Pietro Greiner,Bret Nestor,Oumaima Amezgar,Francesca Dominici*

Main category: cs.AI

TL;DR: 本文提出了首个针对多集体算法集体行动（ACA）的理论框架，研究了在分类系统中，不同规模和目标一致性的集体如何通过植入信号来影响分类器，并量化了其作用和相互作用。


<details>
  <summary>Details</summary>
Motivation: 现有学习系统日益影响日常决策，用户侧的算法集体行动（ACA）是对监管侧政策和企业侧模型设计的补充。然而，尽管现实世界中的行动通常是去中心化且分散到多个集体中，但大多数ACA文献都集中在单一集体设置，缺乏对多集体情境的理论分析。

Method: 本文提出了一个针对多集体算法集体行动的理论框架，特别关注分类系统中的集体行动。研究了多个集体如何通过“植入信号”（即偏置分类器以学习特征的改变版本与目标类别之间的关联）来影响分类器。量化分析了集体规模及其目标一致性在其中的作用和相互作用。

Result: 提供了关于集体规模及其目标一致性在多集体ACA中作用和相互作用的定量结果。该框架补充了以往的实证结果。

Conclusion: 本工作通过提供首个多集体ACA的理论框架，为全面处理多集体算法集体行动开辟了道路，并揭示了不同集体规模和目标一致性在影响分类器中的关键作用和相互作用。

Abstract: As learning systems increasingly influence everyday decisions, user-side
steering via Algorithmic Collective Action (ACA)-coordinated changes to shared
data-offers a complement to regulator-side policy and firm-side model design.
Although real-world actions have been traditionally decentralized and
fragmented into multiple collectives despite sharing overarching
objectives-with each collective differing in size, strategy, and actionable
goals, most of the ACA literature focused on single collective settings. In
this work, we present the first theoretical framework for ACA with multiple
collectives acting on the same system. In particular, we focus on collective
action in classification, studying how multiple collectives can plant signals,
i.e., bias a classifier to learn an association between an altered version of
the features and a chosen, possibly overlapping, set of target classes. We
provide quantitative results about the role and the interplay of collectives'
sizes and their alignment of goals. Our framework, by also complementing
previous empirical results, opens a path for a holistic treatment of ACA with
multiple collectives.

</details>


### [44] [Playstyle and Artificial Intelligence: An Initial Blueprint Through the Lens of Video Games](https://arxiv.org/abs/2508.19152)
*Chiu-Chou Lin*

Main category: cs.AI

TL;DR: 本论文引入“游戏风格”（playstyle）作为观察和分析智能体决策行为的新视角，并从哲学层面探讨其意义。论文构建了一个风格形成的两层框架，提出了可测量的风格指标，并围绕定义与测量、表达与生成以及实际应用三个核心方向展开研究，最终展望了风格在通用人工智能（AGI）中的核心作用。


<details>
  <summary>Details</summary>
Motivation: 当代人工智能（AI）主要关注可衡量和客观评估的理性决策。然而，在现实世界中，智能体的决策不仅受逻辑影响，还受信念、价值观和偏好等深层因素塑造，这些因素导致了人类决策风格的多样性。因此，“风格”是智能中一个重要但常被忽视的维度。

Method: 本研究从哲学角度探讨了“游戏风格”的含义和历史背景。通过分析信念和价值观如何驱动意图和行动，构建了一个包含外部交互循环和内部认知循环的两层风格形成框架。在此基础上，论文形式化了与风格相关的特征，并提出了风格能力、风格流行度和演化动力学等可测量指标。具体研究方向包括：1) 基于离散状态空间定义和测量通用游戏风格指标，并扩展量化战略多样性和竞争平衡；2) 探索如何利用强化学习和模仿学习训练具有特定风格倾向的智能体，并引入一种新颖的类人风格学习和建模方法；3) 分析这些技术在游戏设计和互动娱乐等领域的潜在应用。

Result: 论文提出了基于离散状态空间的通用游戏风格度量标准，并将其扩展用于量化战略多样性和竞争平衡。研究探索了如何利用强化学习和模仿学习来训练展现特定风格倾向的智能体，并引入了一种新颖的类人风格学习和建模方法。此外，论文还分析了这些技术在游戏设计和互动娱乐等领域的实际应用潜力。

Conclusion: 本论文强调了“风格”是智能中一个重要但常被忽视的维度，通过引入“游戏风格”概念、构建风格形成框架和提出可测量指标，深入研究了其定义、测量、表达、生成及应用。最终，论文指出风格将是构建通用人工智能（AGI）的核心要素。

Abstract: Contemporary artificial intelligence (AI) development largely centers on
rational decision-making, valued for its measurability and suitability for
objective evaluation. Yet in real-world contexts, an intelligent agent's
decisions are shaped not only by logic but also by deeper influences such as
beliefs, values, and preferences. The diversity of human decision-making styles
emerges from these differences, highlighting that "style" is an essential but
often overlooked dimension of intelligence.
  This dissertation introduces playstyle as an alternative lens for observing
and analyzing the decision-making behavior of intelligent agents, and examines
its foundational meaning and historical context from a philosophical
perspective. By analyzing how beliefs and values drive intentions and actions,
we construct a two-tier framework for style formation: the external interaction
loop with the environment and the internal cognitive loop of deliberation. On
this basis, we formalize style-related characteristics and propose measurable
indicators such as style capacity, style popularity, and evolutionary dynamics.
  The study focuses on three core research directions: (1) Defining and
measuring playstyle, proposing a general playstyle metric based on discretized
state spaces, and extending it to quantify strategic diversity and competitive
balance; (2) Expressing and generating playstyle, exploring how reinforcement
learning and imitation learning can be used to train agents exhibiting specific
stylistic tendencies, and introducing a novel approach for human-like style
learning and modeling; and (3) Practical applications, analyzing the potential
of these techniques in domains such as game design and interactive
entertainment.
  Finally, the dissertation outlines future extensions, including the role of
style as a core element in building artificial general intelligence (AGI).

</details>


### [45] [MATRIX: Multi-Agent simulaTion fRamework for safe Interactions and conteXtual clinical conversational evaluation](https://arxiv.org/abs/2508.19163)
*Ernest Lim,Yajie Vera He,Jared Joselowitz,Kate Preston,Mohita Chowdhury,Louis Williams,Aisling Higham,Katrina Mason,Mariane Melo,Tom Lawton,Yan Jia,Ibrahim Habli*

Main category: cs.AI

TL;DR: 本文提出了MATRIX，一个用于临床对话系统安全评估的结构化、可扩展的多智能体模拟框架，旨在解决现有评估在安全行为和风险管理方面的不足。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在临床对话系统中应用日益广泛，但现有评估主要关注任务完成或流畅性，未能深入洞察安全关键系统所需的行为和风险管理要求。

Method: MATRIX框架整合了三个核心组件：1) 基于安全工程方法构建的临床场景、预期系统行为和故障模式的安全对齐分类法；2) BehvJudge，一个基于LLM的评估器，用于检测安全相关的对话故障，并经过临床专家标注验证；3) PatBot，一个模拟患者智能体，能够生成多样化、情境化的响应，其真实性和行为忠实度经过人类因素专家评估和患者偏好研究。

Result: MATRIX实现了系统化、可扩展的安全评估。BehvJudge结合Gemini 2.5-Pro在危害检测方面达到了专家级水平（F1 0.96，敏感性0.999），在240个对话的盲测中表现优于临床医生。PatBot在定量和定性评估中可靠地模拟了真实的患者行为。MATRIX成功用于基准测试五个LLM智能体，涵盖14个危害场景和10个临床领域的2,100个模拟对话。

Conclusion: MATRIX是首个将结构化安全工程与可扩展、经过验证的对话式AI评估相结合的框架，实现了与监管机构对齐的安全审计。该框架的所有评估工具、提示、结构化场景和数据集均已发布。

Abstract: Despite the growing use of large language models (LLMs) in clinical dialogue
systems, existing evaluations focus on task completion or fluency, offering
little insight into the behavioral and risk management requirements essential
for safety-critical systems. This paper presents MATRIX (Multi-Agent simulaTion
fRamework for safe Interactions and conteXtual clinical conversational
evaluation), a structured, extensible framework for safety-oriented evaluation
of clinical dialogue agents.
  MATRIX integrates three components: (1) a safety-aligned taxonomy of clinical
scenarios, expected system behaviors and failure modes derived through
structured safety engineering methods; (2) BehvJudge, an LLM-based evaluator
for detecting safety-relevant dialogue failures, validated against expert
clinician annotations; and (3) PatBot, a simulated patient agent capable of
producing diverse, scenario-conditioned responses, evaluated for realism and
behavioral fidelity with human factors expertise, and a patient-preference
study.
  Across three experiments, we show that MATRIX enables systematic, scalable
safety evaluation. BehvJudge with Gemini 2.5-Pro achieves expert-level hazard
detection (F1 0.96, sensitivity 0.999), outperforming clinicians in a blinded
assessment of 240 dialogues. We also conducted one of the first realism
analyses of LLM-based patient simulation, showing that PatBot reliably
simulates realistic patient behavior in quantitative and qualitative
evaluations. Using MATRIX, we demonstrate its effectiveness in benchmarking
five LLM agents across 2,100 simulated dialogues spanning 14 hazard scenarios
and 10 clinical domains.
  MATRIX is the first framework to unify structured safety engineering with
scalable, validated conversational AI evaluation, enabling regulator-aligned
safety auditing. We release all evaluation tools, prompts, structured
scenarios, and datasets.

</details>


### [46] [The Ramon Llull's Thinking Machine for Automated Ideation](https://arxiv.org/abs/2508.19200)
*Xinran Zhao,Boyuan Zheng,Chenglei Si,Haofei Yu,Ken Liu,Runlong Zhou,Ruochen Li,Tong Chen,Xiang Li,Yiming Zhang,Tongshuang Wu*

Main category: cs.AI

TL;DR: 本文重新审视了中世纪的拉蒙·卢尔的“组合艺术”，并以此为概念基础，构建了一个现代的、基于大型语言模型（LLM）的“卢尔思维机器”，用于研究构思。


<details>
  <summary>Details</summary>
Motivation: 研究的动机是利用卢尔的组合知识生成框架，结合现代AI技术，为科学研究提供一种增强创造力、生成新想法的工具，并探索人机协作构思的潜力。

Method: 该方法定义了三个组合轴：主题（如效率、适应性）、领域（如问答、机器翻译）和方法（如对抗训练、线性注意力）。这些元素从人类专家或会议论文中提取。通过使用精心策划的组合提示大型语言模型，来生成研究想法。

Result: 结果表明，通过这种方法，大型语言模型生成的研发想法具有多样性、相关性，并以现有文献为基础。所构建的现代思维机器是一个轻量级、可解释的工具。

Conclusion: 该研究提供了一个增强科学创造力的工具，并为人类与AI之间的协作构思指明了一条道路。

Abstract: This paper revisits Ramon Llull's Ars combinatoria - a medieval framework for
generating knowledge through symbolic recombination - as a conceptual
foundation for building a modern Llull's thinking machine for research
ideation. Our approach defines three compositional axes: Theme (e.g.,
efficiency, adaptivity), Domain (e.g., question answering, machine
translation), and Method (e.g., adversarial training, linear attention). These
elements represent high-level abstractions common in scientific work -
motivations, problem settings, and technical approaches - and serve as building
blocks for LLM-driven exploration. We mine elements from human experts or
conference papers and show that prompting LLMs with curated combinations
produces research ideas that are diverse, relevant, and grounded in current
literature. This modern thinking machine offers a lightweight, interpretable
tool for augmenting scientific creativity and suggests a path toward
collaborative ideation between humans and AI.

</details>


### [47] [The Subset Sum Matching Problem](https://arxiv.org/abs/2508.19218)
*Yufei Wu,Manuel R. Torres,Parisa Zehtabi,Alberto Pozanco Lancho,Michael Cashmore,Daniel Borrajo,Manuela Veloso*

Main category: cs.AI

TL;DR: 本论文提出了子集和匹配问题（SSMP），一种抽象自金融应用的组合优化任务，并提供了三种算法（两种次优、一种最优）来解决它，同时通过基准测试进行了性能评估。


<details>
  <summary>Details</summary>
Motivation: 解决金融应用（如交易对账）中常见的组合优化任务，通过抽象出子集和匹配问题（SSMP）来应对。

Method: 提出了三种算法来解决SSMP，包括两种次优算法和一种最优算法。同时，生成了一个涵盖不同复杂度的SSMP实例的基准，并进行了实验评估以衡量这些方法的性能。

Result: 通过实验评估，衡量了所提出的三种算法在不同复杂度的SSMP实例上的性能。

Conclusion: 论文成功定义了子集和匹配问题，并提供了多种算法解决方案，并通过严格的实验评估分析了这些方法的性能表现。

Abstract: This paper presents a new combinatorial optimisation task, the Subset Sum
Matching Problem (SSMP), which is an abstraction of common financial
applications such as trades reconciliation. We present three algorithms, two
suboptimal and one optimal, to solve this problem. We also generate a benchmark
to cover different instances of SSMP varying in complexity, and carry out an
experimental evaluation to assess the performance of the approaches.

</details>


### [48] [StepWiser: Stepwise Generative Judges for Wiser Reasoning](https://arxiv.org/abs/2508.19229)
*Wei Xiong,Wenting Zhao,Weizhe Yuan,Olga Golovneva,Tong Zhang,Jason Weston,Sainbayar Sukhbaatar*

Main category: cs.AI

TL;DR: 本文提出StepWiser，一个生成式判断模型，通过元推理（输出思考过程）为多步推理模型提供分步反馈。该模型通过强化学习训练，在中间步骤判断准确性、策略模型训练和推理时搜索方面均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着模型越来越多地利用多步推理策略解决复杂问题，监督这些中间步骤的逻辑有效性成为关键挑战。现有过程奖励模型通常是分类器，缺乏解释性，且依赖静态数据集进行监督微调，泛化能力受限。

Method: 本文将分步奖励建模从分类任务重新定义为推理任务。提出一个生成式判断模型StepWiser，它对策略模型的推理步骤进行“元推理”，在给出最终判断前输出思考过程（思考令牌）。StepWiser通过强化学习，利用rollout的相对结果进行训练。

Result: 研究结果表明，StepWiser (i) 在中间步骤的判断准确性优于现有方法；(ii) 可以在训练时用于改进策略模型；(iii) 改进了推理时的搜索能力。

Conclusion: StepWiser通过将分步奖励建模重构为推理任务，并采用强化学习训练一个生成式判断模型，有效解决了现有过程奖励模型缺乏解释性和泛化能力差的问题，显著提升了对多步推理模型中间步骤的监督和改进能力。

Abstract: As models increasingly leverage multi-step reasoning strategies to solve
complex problems, supervising the logical validity of these intermediate steps
has become a critical research challenge. Process reward models address this by
providing step-by-step feedback, but current approaches have two major
drawbacks: they typically function as classifiers without providing
explanations, and their reliance on supervised fine-tuning with static datasets
limits generalization. Inspired by recent advances, we reframe stepwise reward
modeling from a classification task to a reasoning task itself. We thus propose
a generative judge that reasons about the policy model's reasoning steps (i.e.,
meta-reasons), outputting thinking tokens before delivering a final verdict.
Our model, StepWiser, is trained by reinforcement learning using relative
outcomes of rollouts. We show it provides (i) better judgment accuracy on
intermediate steps than existing methods; (ii) can be used to improve the
policy model at training time; and (iii) improves inference-time search.

</details>


### [49] [Model Context Protocols in Adaptive Transport Systems: A Survey](https://arxiv.org/abs/2508.19239)
*Gaurab Chhetri,Shriyank Somvanshi,Md Monzurul Islam,Shamyo Brotee,Mahmuda Sultana Mimi,Dipti Koirala,Biplov Pandey,Subasish Das*

Main category: cs.AI

TL;DR: 本调查系统性地研究了模型上下文协议（MCP）作为统一范式，以解决自适应传输系统中协议和上下文源的碎片化问题，并将其定位为下一代智能传输基础设施的基础。


<details>
  <summary>Details</summary>
Motivation: 互联设备、自主系统和AI应用的快速发展导致自适应传输系统严重碎片化，各种协议和上下文源相互隔离，缺乏统一的适应性传输系统。

Method: 本研究通过对现有文献的系统性调查和分析，展示了现有工作如何隐性地趋向于类MCP架构。研究提出了一种包含五类（自适应机制、上下文感知框架、统一模型、集成策略和MCP使能架构）的分类法。

Result: 研究发现：传统传输协议在孤立适应方面已达极限；MCP的客户端-服务器和JSON-RPC结构实现了语义互操作性；AI驱动的传输对集成范式提出了独特需求，而MCP恰好能满足这些需求。现有努力已趋向于类MCP架构。

Conclusion: MCP能够将协议级适应与上下文感知决策相结合，是解决自适应传输系统碎片化问题的统一范式。本研究提出了一个研究路线图，将MCP定位为下一代自适应、上下文感知和智能传输基础设施的基础。

Abstract: The rapid expansion of interconnected devices, autonomous systems, and AI
applications has created severe fragmentation in adaptive transport systems,
where diverse protocols and context sources remain isolated. This survey
provides the first systematic investigation of the Model Context Protocol (MCP)
as a unifying paradigm, highlighting its ability to bridge protocol-level
adaptation with context-aware decision making. Analyzing established
literature, we show that existing efforts have implicitly converged toward
MCP-like architectures, signaling a natural evolution from fragmented solutions
to standardized integration frameworks. We propose a five-category taxonomy
covering adaptive mechanisms, context-aware frameworks, unification models,
integration strategies, and MCP-enabled architectures. Our findings reveal
three key insights: traditional transport protocols have reached the limits of
isolated adaptation, MCP's client-server and JSON-RPC structure enables
semantic interoperability, and AI-driven transport demands integration
paradigms uniquely suited to MCP. Finally, we present a research roadmap
positioning MCP as a foundation for next-generation adaptive, context-aware,
and intelligent transport infrastructures.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [50] [Towards Training-Free Underwater 3D Object Detection from Sonar Point Clouds: A Comparison of Traditional and Deep Learning Approaches](https://arxiv.org/abs/2508.18293)
*M. Salman Shaukat,Yannik Käckenmeister,Sebastian Bader,Thomas Kirste*

Main category: cs.CV

TL;DR: 本文提出并比较了两种无需真实训练数据的海底3D目标检测范式：基于物理的声纳仿真生成合成数据训练神经网络，以及基于几何先验的模型匹配模板系统。结果显示，在真实数据上，模板匹配方法优于合成数据训练的神经网络，挑战了深度学习在水下领域的数据依赖性。


<details>
  <summary>Details</summary>
Motivation: 水下3D目标检测极具挑战性，传统方法难以应对恶劣声学环境和训练数据稀缺。尽管深度学习在陆地3D检测中取得了革命性进展，但在水下应用中，获取足够的带注释声纳数据成本高昂且复杂，成为关键瓶颈。因此，本文旨在探索在没有真实训练数据的情况下，能否实现可靠的水下3D目标检测。

Method: 本文开发并比较了两种无需训练的范式来检测多波束回声测深点云中的人工结构：1) 基于物理的声纳仿真流水线，用于生成合成训练数据以训练最先进的神经网络。2) 一个鲁棒的模型匹配模板系统，利用目标物体的几何先验。

Result: 在波罗的海的真实水深测量评估中发现：神经网络在合成场景上实现了98%的平均精度（mAP），但在真实声纳数据上由于域偏移，mAP降至40%。相反，模板匹配方法在不需要任何训练的情况下，在真实数据上保持了83%的mAP，表现出对声学噪声和环境变化的显著鲁棒性。

Conclusion: 研究结果挑战了关于水下领域数据密集型深度学习的传统观念，并建立了第一个大规模的无训练水下3D检测基准。这项工作为自主水下航行器导航、海洋考古和数据稀缺环境下（传统机器学习方法失效）的离岸基础设施监测开辟了新的可能性。

Abstract: Underwater 3D object detection remains one of the most challenging frontiers
in computer vision, where traditional approaches struggle with the harsh
acoustic environment and scarcity of training data. While deep learning has
revolutionized terrestrial 3D detection, its application underwater faces a
critical bottleneck: obtaining sufficient annotated sonar data is prohibitively
expensive and logistically complex, often requiring specialized vessels, expert
surveyors, and favorable weather conditions. This work addresses a fundamental
question: Can we achieve reliable underwater 3D object detection without
real-world training data? We tackle this challenge by developing and comparing
two paradigms for training-free detection of artificial structures in multibeam
echo-sounder point clouds. Our dual approach combines a physics-based sonar
simulation pipeline that generates synthetic training data for state-of-the-art
neural networks, with a robust model-based template matching system that
leverages geometric priors of target objects. Evaluation on real bathymetry
surveys from the Baltic Sea reveals surprising insights: while neural networks
trained on synthetic data achieve 98% mean Average Precision (mAP) on simulated
scenes, they drop to 40% mAP on real sonar data due to domain shift.
Conversely, our template matching approach maintains 83% mAP on real data
without requiring any training, demonstrating remarkable robustness to acoustic
noise and environmental variations. Our findings challenge conventional wisdom
about data-hungry deep learning in underwater domains and establish the first
large-scale benchmark for training-free underwater 3D detection. This work
opens new possibilities for autonomous underwater vehicle navigation, marine
archaeology, and offshore infrastructure monitoring in data-scarce environments
where traditional machine learning approaches fail.

</details>


### [51] [MobileDenseAttn:A Dual-Stream Architecture for Accurate and Interpretable Brain Tumor Detection](https://arxiv.org/abs/2508.18294)
*Shudipta Banik,Muna Das,Trapa Banik,Md. Ehsanul Haque*

Main category: cs.CV

TL;DR: 本文提出了一种名为MobileDenseAttn的融合模型，用于高效、准确且可解释地检测MRI图像中的脑肿瘤，解决了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 脑肿瘤的MRI手动分析耗时且易错。现有自动化方法在处理异质肿瘤时泛化能力有限、计算效率低下、缺乏可解释性和透明度，从而限制了其可信度。

Method: 引入了MobileDenseAttn模型，这是一个由MobileNetV2和DenseNet201双流组成的融合模型，旨在逐步提高特征表示尺度、计算效率和通过GradCAM实现的视觉解释能力。该模型采用特征级别融合，并在包含6,020张增强MRI扫描（胶质瘤、脑膜瘤、垂体瘤和正常样本）的数据集上进行训练。

Result: 在严格的5折交叉验证协议下，MobileDenseAttn模型实现了99.75%的训练准确率、98.35%的测试准确率和0.9835的稳定F1分数（95%置信区间：0.9743至0.9920）。与基线模型（VGG19、DenseNet201、MobileNetV2）相比，其准确率提高了3.67%，训练时间比VGG19减少了39.3%。GradCAM热图清晰显示了肿瘤受影响区域，提供了具有临床意义的定位和可解释性。

Conclusion: MobileDenseAttn被定位为一个高效、高性能、可解释的模型，在现实世界中识别脑肿瘤方面具有很高的临床实用潜力。

Abstract: The detection of brain tumor in MRI is an important aspect of ensuring timely
diagnostics and treatment; however, manual analysis is commonly long and
error-prone. Current approaches are not universal because they have limited
generalization to heterogeneous tumors, are computationally inefficient, are
not interpretable, and lack transparency, thus limiting trustworthiness. To
overcome these issues, we introduce MobileDenseAttn, a fusion model of dual
streams of MobileNetV2 and DenseNet201 that can help gradually improve the
feature representation scale, computing efficiency, and visual explanations via
GradCAM. Our model uses feature level fusion and is trained on an augmented
dataset of 6,020 MRI scans representing glioma, meningioma, pituitary tumors,
and normal samples. Measured under strict 5-fold cross-validation protocols,
MobileDenseAttn provides a training accuracy of 99.75%, a testing accuracy of
98.35%, and a stable F1 score of 0.9835 (95% CI: 0.9743 to 0.9920). The
extensive validation shows the stability of the model, and the comparative
analysis proves that it is a great advancement over the baseline models (VGG19,
DenseNet201, MobileNetV2) with a +3.67% accuracy increase and a 39.3% decrease
in training time compared to VGG19. The GradCAM heatmaps clearly show
tumor-affected areas, offering clinically significant localization and
improving interpretability. These findings position MobileDenseAttn as an
efficient, high performance, interpretable model with a high probability of
becoming a clinically practical tool in identifying brain tumors in the real
world.

</details>


### [52] [Can VLMs Recall Factual Associations From Visual References?](https://arxiv.org/abs/2508.18297)
*Dhananjay Ashok,Ashutosh Chaubey,Hirona J. Arai,Jonathan May,Jesse Thomason*

Main category: cs.CV

TL;DR: 研究发现，视觉语言模型（VLMs）在多模态接地方面存在系统性缺陷：当实体参照是视觉而非文本时，其事实知识回忆能力显著下降。这种缺陷与模型内部状态的特定模式相关，并且可以通过探测这些内部状态来有效检测，从而提高VQA任务的可靠性。


<details>
  <summary>Details</summary>
Motivation: VLMs在多模态接地方面存在系统性缺陷，特别是在视觉参照下，其回忆事实知识的能力显著减弱。这种缺陷表明VLM难以将其内部知识与图像表示联系起来，影响了模型的可靠性。

Method: 研究通过一项对照实验，比较了VLM在文本参照和视觉参照下回忆事实知识的能力。分析了模型内部状态与链接失败之间的相关性，并基于这些内部状态开发了探测器，用于识别VLM不可靠的响应。这些探测器无需重新训练即可应用，并在视觉问答（VQA）任务的选择性预测中进行了评估。

Result: 当实体参照为视觉而非文本时，VLM回忆事实知识的能力减半。链接失败与模型内部状态的特定模式相关。基于内部状态的探测器在识别VLM不可靠响应方面达到了92%以上的准确率。在VQA选择性预测任务中，这些探测器在将错误风险降低0.9%（绝对值）的同时，将覆盖率提高了7.87%（绝对值）。

Conclusion: VLMs在多模态接地方面存在一个系统性且可检测的缺陷，即难以将内部知识与视觉表示联系起来。解决这一缺陷是语言接地领域的重要方向，研究为未来的改进提供了建议。

Abstract: Through a controlled study, we identify a systematic deficiency in the
multimodal grounding of Vision Language Models (VLMs). While VLMs can recall
factual associations when provided a textual reference to an entity; their
ability to do so is significantly diminished when the reference is visual
instead. Forcing VLMs to rely on image representations of an entity halves
their ability to recall factual knowledge, suggesting that VLMs struggle to
link their internal knowledge of an entity with its image representation. We
show that such linking failures are correlated with the expression of distinct
patterns in model internal states, and that probes on these internal states
achieve over 92% accuracy at flagging cases where the VLM response is
unreliable. These probes can be applied, without retraining, to identify when a
VLM will fail to correctly answer a question that requires an understanding of
multimodal input. When used to facilitate selective prediction on a visual
question answering task, the probes increase coverage by 7.87% (absolute) while
also reducing the risk of error by 0.9% (absolute). Addressing the systematic,
detectable deficiency is an important avenue in language grounding, and we
provide informed recommendations for future directions.

</details>


### [53] [SERES: Semantic-aware neural reconstruction from sparse views](https://arxiv.org/abs/2508.18314)
*Bo Xu,Yuhu Guo,Yuchao Wang,Wenting Wang,Yeung Yam,Charlie C. L. Wang,Xinyi Le*

Main category: cs.CV

TL;DR: 本文提出了一种语义感知的神经重建方法，通过引入补丁级语义逻辑和几何基元掩码正则化，从稀疏图像生成高保真3D模型，有效解决辐射和形状模糊问题。


<details>
  <summary>Details</summary>
Motivation: 从稀疏图像生成3D模型面临严重的辐射模糊（由稀疏输入中特征不匹配引起）和形状模糊挑战。

Method: 该方法通过以下方式丰富神经隐式表示：1) 添加与有符号距离场(SDF)和辐射场一同优化的补丁级语义逻辑；2) 引入基于几何基元掩码的新颖正则化来缓解形状模糊。

Result: 实验结果显示，在DTU数据集上，该方法使SparseNeuS的平均倒角距离减少44%，VolRecon减少20%。当作为NeuS和Neuralangelo等密集重建基线的插件时，DTU数据集上的平均误差分别减少69%和68%。

Conclusion: 所提出的语义感知神经重建方法能有效处理稀疏输入带来的辐射和形状模糊问题，显著提高3D重建的精度和质量。

Abstract: We propose a semantic-aware neural reconstruction method to generate 3D
high-fidelity models from sparse images. To tackle the challenge of severe
radiance ambiguity caused by mismatched features in sparse input, we enrich
neural implicit representations by adding patch-based semantic logits that are
optimized together with the signed distance field and the radiance field. A
novel regularization based on the geometric primitive masks is introduced to
mitigate shape ambiguity. The performance of our approach has been verified in
experimental evaluation. The average chamfer distances of our reconstruction on
the DTU dataset can be reduced by 44% for SparseNeuS and 20% for VolRecon. When
working as a plugin for those dense reconstruction baselines such as NeuS and
Neuralangelo, the average error on the DTU dataset can be reduced by 69% and
68% respectively.

</details>


### [54] [Automated Landfill Detection Using Deep Learning: A Comparative Study of Lightweight and Custom Architectures with the AerialWaste Dataset](https://arxiv.org/abs/2508.18315)
*Nowshin Sharmily,Rusab Sarmun,Muhammad E. H. Chowdhury,Mir Hamidul Hussain,Saad Bin Abul Kashem,Molla E Majid,Amith Khandakar*

Main category: cs.CV

TL;DR: 本文利用新发布的AerialWaste数据集和轻量级深度学习模型（包括集成模型）来检测非法垃圾填埋场，实现了高精度分类。


<details>
  <summary>Details</summary>
Motivation: 非法垃圾填埋场对全球构成危害，但人工识别困难且耗时，导致许多垃圾场未被发现。深度学习有望解决此问题，但高质量的公开数据集稀缺。

Method: 研究使用了包含10434张意大利伦巴第地区图像的AerialWaste数据集。为避免过拟合，选择了Mobilenetv2、Googlenet、Densenet、MobileVit等轻量级深度学习模型进行训练和验证。最终，将表现最佳的模型进行组合，构建了一个集成模型，并结合融合技术进行二元分类。

Result: 通过集成和融合技术，该模型在非法垃圾填埋场二元分类任务中达到了92.33%的准确率、92.67%的精确率、92.33%的灵敏度、92.41%的F1分数和92.71%的特异性。

Conclusion: 轻量级模型结合集成方法，在AerialWaste数据集上能有效且高精度地识别非法垃圾填埋场，证明了该方法在解决这一环境问题上的潜力。

Abstract: Illegal landfills are posing as a hazardous threat to people all over the
world. Due to the arduous nature of manually identifying the location of
landfill, many landfills go unnoticed by authorities and later cause dangerous
harm to people and environment. Deep learning can play a significant role in
identifying these landfills while saving valuable time, manpower and resources.
Despite being a burning concern, good quality publicly released datasets for
illegal landfill detection are hard to find due to security concerns. However,
AerialWaste Dataset is a large collection of 10434 images of Lombardy region of
Italy. The images are of varying qualities, collected from three different
sources: AGEA Orthophotos, WorldView-3, and Google Earth. The dataset contains
professionally curated, diverse and high-quality images which makes it
particularly suitable for scalable and impactful research. As we trained
several models to compare results, we found complex and heavy models to be
prone to overfitting and memorizing training data instead of learning patterns.
Therefore, we chose lightweight simpler models which could leverage general
features from the dataset. In this study, Mobilenetv2, Googlenet, Densenet,
MobileVit and other lightweight deep learning models were used to train and
validate the dataset as they achieved significant success with less
overfitting. As we saw substantial improvement in the performance using some of
these models, we combined the best performing models and came up with an
ensemble model. With the help of ensemble and fusion technique, binary
classification could be performed on this dataset with 92.33% accuracy, 92.67%
precision, 92.33% sensitivity, 92.41% F1 score and 92.71% specificity.

</details>


### [55] [Structures Meet Semantics: Multimodal Fusion via Graph Contrastive Learning](https://arxiv.org/abs/2508.18322)
*Jiangfeng Sun,Sihao He,Zhonghong Ou,Meina Song*

Main category: cs.CV

TL;DR: 本文提出结构-语义统一器（SSU）框架，通过整合模态特定结构信息和跨模态语义对齐，显著提升了多模态情感分析的性能、可解释性并降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有多模态融合方法常忽略模态特有的结构依赖和语义错位问题，导致其在质量、可解释性和鲁棒性方面受到限制。

Method: 本文提出SSU框架，其核心方法包括：1) 利用语言句法（文本）和轻量级文本引导注意力机制（声学/视觉）动态构建模态特定图，以捕捉详细的模态内关系和语义交互；2) 引入源自全局文本语义的语义锚点，作为跨模态对齐中心，有效协调异构语义空间；3) 开发多视角对比学习目标，以促进模态内和模态间视图的判别性、语义一致性和结构连贯性。

Result: 在CMU-MOSI和CMU-MOSEI两个常用基准数据集上，SSU持续实现了最先进（SOTA）的性能，并相较于现有方法显著降低了计算开销。全面的定性分析进一步验证了SSU的可解释性及其通过语义接地交互捕获细微情感模式的能力。

Conclusion: SSU框架通过系统整合模态特定结构信息和跨模态语义对齐，有效解决了多模态情感分析中的关键挑战，实现了卓越的性能、效率和可解释性，为未来的多模态研究提供了新的方向。

Abstract: Multimodal sentiment analysis (MSA) aims to infer emotional states by
effectively integrating textual, acoustic, and visual modalities. Despite
notable progress, existing multimodal fusion methods often neglect
modality-specific structural dependencies and semantic misalignment, limiting
their quality, interpretability, and robustness. To address these challenges,
we propose a novel framework called the Structural-Semantic Unifier (SSU),
which systematically integrates modality-specific structural information and
cross-modal semantic grounding for enhanced multimodal representations.
Specifically, SSU dynamically constructs modality-specific graphs by leveraging
linguistic syntax for text and a lightweight, text-guided attention mechanism
for acoustic and visual modalities, thus capturing detailed intra-modal
relationships and semantic interactions. We further introduce a semantic
anchor, derived from global textual semantics, that serves as a cross-modal
alignment hub, effectively harmonizing heterogeneous semantic spaces across
modalities. Additionally, we develop a multiview contrastive learning objective
that promotes discriminability, semantic consistency, and structural coherence
across intra- and inter-modal views. Extensive evaluations on two widely used
benchmark datasets, CMU-MOSI and CMU-MOSEI, demonstrate that SSU consistently
achieves state-of-the-art performance while significantly reducing
computational overhead compared to prior methods. Comprehensive qualitative
analyses further validate SSU's interpretability and its ability to capture
nuanced emotional patterns through semantically grounded interactions.

</details>


### [56] [FastAvatar: Instant 3D Gaussian Splatting for Faces from Single Unconstrained Poses](https://arxiv.org/abs/2508.18389)
*Hao Liang,Zhixuan Ge,Ashish Tiwari,Soumendu Majee,G. M. Dilshan Godaliyadda,Ashok Veeraraghavan,Guha Balakrishnan*

Main category: cs.CV

TL;DR: FastAvatar是一个姿态不变的前馈框架，能从单张人脸图像在极短时间内（<10ms）生成高质量的3D高斯泼溅（3DGS）模型，并支持实时身份插值和属性编辑。


<details>
  <summary>Details</summary>
Motivation: 现有的3DGS人脸生成方法要么速度慢（基于优化），要么重建质量和身份保持不足（现有的前馈方法）。研究的动机是需要一个快速、高质量、姿态不变的3DGS人脸生成框架，以满足消费级和交互式系统中逼真人脸替身应用的需求。

Method: FastAvatar采用新颖的编解码器神经网络设计。首先，它从多视角捕获的人脸训练数据集中构建一个3DGS人脸“模板”模型。其次，将输入的单张人脸图像编码成一个与身份相关且姿态不变的潜在嵌入。最后，解码该嵌入以预测模板3DGS模型中每个高斯点结构和外观参数的残差。通过仅以前馈方式推断残差，实现快速和鲁棒的模型推理。

Result: FastAvatar在重建质量上显著优于现有的前馈3DGS人脸方法（如GAGAvatar），并且比基于优化的逐脸方法（如FlashAvatar、GaussianAvatars和GASP）快1000倍，能在不到10毫秒的时间内生成模型。此外，其新颖的潜在空间设计支持实时身份插值和属性编辑，这是现有前馈3DGS人脸生成框架无法实现的。

Conclusion: FastAvatar结合了卓越的重建质量和速度，极大地扩展了3DGS在消费级和交互式系统中用于逼真人脸替身应用的范围，为实时、高质量的虚拟人脸生成提供了新的可能性。

Abstract: We present FastAvatar, a pose-invariant, feed-forward framework that can
generate a 3D Gaussian Splatting (3DGS) model from a single face image from an
arbitrary pose in near-instant time (<10ms). FastAvatar uses a novel
encoder-decoder neural network design to achieve both fast fitting and identity
preservation regardless of input pose. First, FastAvatar constructs a 3DGS face
``template'' model from a training dataset of faces with multi-view captures.
Second, FastAvatar encodes the input face image into an identity-specific and
pose-invariant latent embedding, and decodes this embedding to predict
residuals to the structural and appearance parameters of each Gaussian in the
template 3DGS model. By only inferring residuals in a feed-forward fashion,
model inference is fast and robust. FastAvatar significantly outperforms
existing feed-forward face 3DGS methods (e.g., GAGAvatar) in reconstruction
quality, and runs 1000x faster than per-face optimization methods (e.g.,
FlashAvatar, GaussianAvatars and GASP). In addition, FastAvatar's novel latent
space design supports real-time identity interpolation and attribute editing
which is not possible with any existing feed-forward 3DGS face generation
framework. FastAvatar's combination of excellent reconstruction quality and
speed expands the scope of 3DGS for photorealistic avatar applications in
consumer and interactive systems.

</details>


### [57] [Securing Face and Fingerprint Templates in Humanitarian Biometric Systems](https://arxiv.org/abs/2508.18415)
*Giuseppe Stragapede,Sam Merrick,Vedrana Krivokuća Hahn,Justin Sukaitis,Vincent Graf Narbel*

Main category: cs.CV

TL;DR: 本文提出并评估了一个用于人道主义和紧急情况的移动生物识别系统，该系统采用生物识别模板保护（BTP）方案（PolyProtect），以在提高效率的同时保护弱势群体的隐私。


<details>
  <summary>Details</summary>
Motivation: 在人道主义和紧急情况下，生物识别技术能显著提高操作效率，但同时对数据主体构成隐私风险，尤其是在弱势群体中。因此，需要一个既高效又能有效保护隐私的生物识别解决方案。

Method: 研究首先严格制定了这些情境下的功能、操作、安全和隐私要求。随后，对现有BTP方案进行了广泛比较分析，并选择了PolyProtect，因为它在神经网络人脸嵌入上表现出高效率、模块化和轻量级计算负担。该系统将PolyProtect应用于使用EdgeFace（一种先进高效的特征提取器）提取的人脸嵌入，并在一个来自埃塞俄比亚人道主义项目的真实人脸数据集上进行评估。此外，研究还将评估扩展到指纹生物识别，评估指标包括验证和识别准确性、不可逆性和不可链接性。

Result: PolyProtect被确定为最适合的BTP方法，因为它有效、模块化且计算负担轻。实验结果表明，当PolyProtect应用于EdgeFace提取的人脸嵌入以及指纹生物识别时，其在验证和识别准确性、不可逆性和不可链接性方面表现出有希望的结果。这是PolyProtect首次在识别场景和指纹生物识别上进行评估。

Conclusion: 所提出的移动生物识别系统结合PolyProtect BTP方案，为在人道主义和紧急场景中实现高效且隐私保护的生物识别操作提供了一个有前景的解决方案。未来的工作将包括发布代码。

Abstract: In humanitarian and emergency scenarios, the use of biometrics can
dramatically improve the efficiency of operations, but it poses risks for the
data subjects, which are exacerbated in contexts of vulnerability. To address
this, we present a mobile biometric system implementing a biometric template
protection (BTP) scheme suitable for these scenarios. After rigorously
formulating the functional, operational, and security and privacy requirements
of these contexts, we perform a broad comparative analysis of the BTP
landscape. PolyProtect, a method designed to operate on neural network face
embeddings, is identified as the most suitable method due to its effectiveness,
modularity, and lightweight computational burden. We evaluate PolyProtect in
terms of verification and identification accuracy, irreversibility, and
unlinkability, when this BTP method is applied to face embeddings extracted
using EdgeFace, a novel state-of-the-art efficient feature extractor, on a
real-world face dataset from a humanitarian field project in Ethiopia.
Moreover, as PolyProtect promises to be modality-independent, we extend its
evaluation to fingerprints. To the best of our knowledge, this is the first
time that PolyProtect has been evaluated for the identification scenario and
for fingerprint biometrics. Our experimental results are promising, and we plan
to release our code

</details>


### [58] [Why Relational Graphs Will Save the Next Generation of Vision Foundation Models?](https://arxiv.org/abs/2508.18421)
*Fatemeh Ziaeetabar*

Main category: cs.CV

TL;DR: 本文提出下一代视觉基础模型（FMs）应整合动态关系图接口，以解决当前FMs在实体、角色和时空关系推理方面的局限性，从而提升细粒度任务的性能、鲁棒性和效率。


<details>
  <summary>Details</summary>
Motivation: 视觉基础模型（FMs）在计算机视觉领域占据主导地位，但它们在需要对实体、角色和时空关系进行显式推理的任务中表现出持续的局限性。这种关系推理能力对于细粒度人类活动识别、第一视角视频理解和多模态医学图像分析等任务至关重要，因为空间、时间和语义依赖关系对性能具有决定性影响。

Method: 本文主张下一代FMs应整合显式关系接口，具体实现为动态关系图（即拓扑结构和边缘语义从输入和任务上下文推断的图）。通过用轻量级、上下文自适应的图推理模块增强FMs来验证这一观点。

Result: 跨领域证据（例如人类操作动作识别和脑肿瘤分割系统）表明，与仅使用FMs的基线相比，通过图推理模块增强FMs可以提高细粒度语义保真度、分布外鲁棒性、可解释性和计算效率。此外，通过稀疏地对语义节点进行推理，这种混合模型还实现了更好的内存和硬件效率，使其能够在实际资源限制下部署。

Conclusion: FMs与图的混合模型是未来发展方向，并提出了一个有针对性的研究议程，优先考虑学习动态图构建、多级关系推理（例如活动理解中的部分-对象-场景，或医学成像中的区域-器官）、跨模态融合以及直接探测结构化视觉任务中关系能力的评估协议。

Abstract: Vision foundation models (FMs) have become the predominant architecture in
computer vision, providing highly transferable representations learned from
large-scale, multimodal corpora. Nonetheless, they exhibit persistent
limitations on tasks that require explicit reasoning over entities, roles, and
spatio-temporal relations. Such relational competence is indispensable for
fine-grained human activity recognition, egocentric video understanding, and
multimodal medical image analysis, where spatial, temporal, and semantic
dependencies are decisive for performance. We advance the position that
next-generation FMs should incorporate explicit relational interfaces,
instantiated as dynamic relational graphs (graphs whose topology and edge
semantics are inferred from the input and task context). We illustrate this
position with cross-domain evidence from recent systems in human manipulation
action recognition and brain tumor segmentation, showing that augmenting FMs
with lightweight, context-adaptive graph-reasoning modules improves
fine-grained semantic fidelity, out of distribution robustness,
interpretability, and computational efficiency relative to FM only baselines.
Importantly, by reasoning sparsely over semantic nodes, such hybrids also
achieve favorable memory and hardware efficiency, enabling deployment under
practical resource constraints. We conclude with a targeted research agenda for
FM graph hybrids, prioritizing learned dynamic graph construction, multi-level
relational reasoning (e.g., part object scene in activity understanding, or
region organ in medical imaging), cross-modal fusion, and evaluation protocols
that directly probe relational competence in structured vision tasks.

</details>


### [59] [LPLC: A Dataset for License Plate Legibility Classification](https://arxiv.org/abs/2508.18425)
*Lucas Wojcik,Gabriel E. Lima,Valfride Nascimento,Eduil Nascimento Jr.,Rayson Laroca,David Menotti*

Main category: cs.CV

TL;DR: 本文介绍了一个新的车牌清晰度分类数据集（LPLC），包含10,210张图像和12,687个标注车牌，用于解决自动车牌识别中模糊车牌的挑战。基线模型的F1分数低于80%，突显了该任务的难度，并强调了进一步研究的必要性。


<details>
  <summary>Details</summary>
Motivation: 自动车牌识别（ALPR）在处理模糊车牌时面临重大挑战。尽管超分辨率（SR）等重建方法已出现，但识别低质量车牌的核心问题仍未解决。研究旨在通过选择性图像预处理来优化模型性能和计算效率，这需要一个支持车牌清晰度分类的研究数据集。

Method: 研究引入了一个包含10,210张车辆图像和12,687个标注车牌的新数据集（LPLC），涵盖了广泛的车辆类型、光照条件和图像质量。采用了细粒度标注策略，包括车辆和车牌级别的遮挡、四种清晰度类别（完美、良好、差、模糊）以及三种清晰度类别（不包括模糊车牌）的字符标签。作为基准，提出了一个分类任务，使用ViT、ResNet和YOLO三种图像识别网络来判断车牌图像是否足够好、需要超分辨率处理，还是完全无法恢复。

Result: 所有三种基线模型（ViT、ResNet和YOLO）的整体F1分数均低于80%。对超分辨率和车牌识别方法的分析进一步强调了该任务的难度。这表明现有模型在车牌清晰度分类方面表现不佳。

Conclusion: 车牌清晰度分类任务极具挑战性，现有模型（如ViT、ResNet、YOLO）在该任务上的表现不尽理想，F1分数低于80%。研究结果以及对SR和车牌识别方法的分析，共同强化了在该领域进行进一步研究的必要性。所提出的LPLC数据集将为未来的研究提供支持。

Abstract: Automatic License Plate Recognition (ALPR) faces a major challenge when
dealing with illegible license plates (LPs). While reconstruction methods such
as super-resolution (SR) have emerged, the core issue of recognizing these
low-quality LPs remains unresolved. To optimize model performance and
computational efficiency, image pre-processing should be applied selectively to
cases that require enhanced legibility. To support research in this area, we
introduce a novel dataset comprising 10,210 images of vehicles with 12,687
annotated LPs for legibility classification (the LPLC dataset). The images span
a wide range of vehicle types, lighting conditions, and camera/image quality
levels. We adopt a fine-grained annotation strategy that includes vehicle- and
LP-level occlusions, four legibility categories (perfect, good, poor, and
illegible), and character labels for three categories (excluding illegible
LPs). As a benchmark, we propose a classification task using three image
recognition networks to determine whether an LP image is good enough, requires
super-resolution, or is completely unrecoverable. The overall F1 score, which
remained below 80% for all three baseline models (ViT, ResNet, and YOLO),
together with the analyses of SR and LP recognition methods, highlights the
difficulty of the task and reinforces the need for further research. The
proposed dataset is publicly available at
https://github.com/lmlwojcik/lplc-dataset.

</details>


### [60] [CLARIFY: A Specialist-Generalist Framework for Accurate and Lightweight Dermatological Visual Question Answering](https://arxiv.org/abs/2508.18430)
*Aranya Saha,Tanvir Ahmed Khan,Ismam Nur Swapnil,Mohammad Ariful Haque*

Main category: cs.CV

TL;DR: CLARIFY是一个用于皮肤病视觉问答（VQA）的“专家-通用”框架。它结合了轻量级专家分类器和压缩型通用VLM，通过专家引导、知识图谱增强，显著提高了诊断准确性，同时降低了计算成本，使其更适用于临床部署。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉-语言模型（VLMs）在医疗任务中显示出巨大潜力，但其通用性限制了专业诊断的准确性，且庞大的模型尺寸导致高昂的推理成本，难以在实际临床中部署。

Method: CLARIFY框架包含两个核心组件：(i) 一个轻量级、领域训练的图像分类器（专家），提供快速高精度的诊断预测；(ii) 一个强大但经过压缩的对话式VLM（通用者），生成自然语言解释。专家预测直接指导通用者的推理，使其聚焦于正确的诊断路径。此外，一个基于知识图谱的检索模块进一步增强了通用者的响应，确保其准确性和可靠性。

Result: 在作者整理的多模态皮肤病数据集上，CLARIFY的诊断准确率比最强的基线（一个经过微调、未压缩的单行VLM）提高了18%。同时，平均VRAM需求和延迟分别至少降低了20%和5%。

Conclusion: “专家-通用”系统为构建轻量级、可信赖且临床可行的AI系统提供了一个实用且强大的范例，有效解决了现有VLM在医疗领域应用中的准确性和效率挑战。

Abstract: Vision-language models (VLMs) have shown significant potential for medical
tasks; however, their general-purpose nature can limit specialized diagnostic
accuracy, and their large size poses substantial inference costs for real-world
clinical deployment. To address these challenges, we introduce CLARIFY, a
Specialist-Generalist framework for dermatological visual question answering
(VQA). CLARIFY combines two components: (i) a lightweight, domain-trained image
classifier (the Specialist) that provides fast and highly accurate diagnostic
predictions, and (ii) a powerful yet compressed conversational VLM (the
Generalist) that generates natural language explanations to user queries. In
our framework, the Specialist's predictions directly guide the Generalist's
reasoning, focusing it on the correct diagnostic path. This synergy is further
enhanced by a knowledge graph-based retrieval module, which grounds the
Generalist's responses in factual dermatological knowledge, ensuring both
accuracy and reliability. This hierarchical design not only reduces diagnostic
errors but also significantly improves computational efficiency. Experiments on
our curated multimodal dermatology dataset demonstrate that CLARIFY achieves an
18\% improvement in diagnostic accuracy over the strongest baseline, a
fine-tuned, uncompressed single-line VLM, while reducing the average VRAM
requirement and latency by at least 20\% and 5\%, respectively. These results
indicate that a Specialist-Generalist system provides a practical and powerful
paradigm for building lightweight, trustworthy, and clinically viable AI
systems.

</details>


### [61] [VQualA 2025 Challenge on Face Image Quality Assessment: Methods and Results](https://arxiv.org/abs/2508.18445)
*Sizhuo Ma,Wei-Ting Chen,Qiang Gao,Jian Wang,Chris Wei Zhou,Wei Sun,Weixia Zhang,Linhan Cao,Jun Jia,Xiangyang Zhu,Dandan Zhu,Xiongkuo Min,Guangtao Zhai,Baoying Chen,Xiongwei Xiao,Jishen Zeng,Wei Wu,Tiexuan Lou,Yuchen Tan,Chunyi Song,Zhiwei Xu,MohammadAli Hamidi,Hadi Amirpour,Mingyin Bai,Jiawang Du,Zhenyu Jiang,Zilong Lu,Ziguan Cui,Zongliang Gan,Xinpeng Li,Shiqi Jiang,Chenhui Li,Changbo Wang,Weijun Yuan,Zhan Li,Yihang Chen,Yifan Deng,Ruting Deng,Zhanglu Chen,Boyang Yao,Shuling Zheng,Feng Zhang,Zhiheng Fu,Abhishek Joshi,Aman Agarwal,Rakhil Immidisetti,Ajay Narasimha Mopidevi,Vishwajeet Shukla,Hao Yang,Ruikun Zhang,Liyuan Pan,Kaixin Deng,Hang Ouyang,Fan yang,Zhizun Luo,Zhuohang Shi,Songning Lai,Weilin Ruan,Yutao Yue*

Main category: cs.CV

TL;DR: VQualA 2025举办了一项关于面部图像质量评估（FIQA）的挑战，旨在开发轻量高效的模型，以预测真实世界退化面部图像的平均意见分数（MOS）。


<details>
  <summary>Details</summary>
Motivation: 面部图像在许多应用中至关重要，但现实世界中的噪声、模糊和压缩伪影等退化会影响图像质量并阻碍后续任务。因此，需要有效的面部图像质量评估方法。

Method: VQualA 2025挑战赛要求参与者开发轻量级（限制0.5 GFLOPs和500万参数）且高效的模型，用于预测任意分辨率和真实退化面部图像的MOS。提交的模型通过相关性指标在“in-the-wild”面部图像数据集上进行评估。

Result: 本次挑战吸引了127名参与者，共提交了1519份最终方案。报告总结了这些方法和发现，以推动实用FIQA方法的发展。

Conclusion: 该挑战成功推动了实用面部图像质量评估（FIQA）方法的发展，通过吸引大量参与者并鼓励开发轻量高效的模型，为处理真实世界面部图像退化问题提供了宝贵的见解和解决方案。

Abstract: Face images play a crucial role in numerous applications; however, real-world
conditions frequently introduce degradations such as noise, blur, and
compression artifacts, affecting overall image quality and hindering subsequent
tasks. To address this challenge, we organized the VQualA 2025 Challenge on
Face Image Quality Assessment (FIQA) as part of the ICCV 2025 Workshops.
Participants created lightweight and efficient models (limited to 0.5 GFLOPs
and 5 million parameters) for the prediction of Mean Opinion Scores (MOS) on
face images with arbitrary resolutions and realistic degradations. Submissions
underwent comprehensive evaluations through correlation metrics on a dataset of
in-the-wild face images. This challenge attracted 127 participants, with 1519
final submissions. This report summarizes the methodologies and findings for
advancing the development of practical FIQA approaches.

</details>


### [62] [Context-Aware Zero-Shot Anomaly Detection in Surveillance Using Contrastive and Predictive Spatiotemporal Modeling](https://arxiv.org/abs/2508.18463)
*Md. Rashid Shahriar Khan,Md. Abrar Hasan,Mohammod Tareq Aziz Justice*

Main category: cs.CV

TL;DR: 该研究提出了一种新颖的上下文感知零样本异常检测框架，通过结合TimeSformer、DPC和CLIP模型，以及上下文门控机制，在无需异常样本训练的情况下，检测监控视频中的异常事件。


<details>
  <summary>Details</summary>
Motivation: 由于监控录像中异常事件的不可预测性和上下文依赖性，检测异常具有内在挑战性。现有方法通常需要异常样本进行训练，而零样本方法能够识别训练中未见的异常行为。

Method: 该方法采用混合架构：TimeSformer作为视觉骨干提取时空特征；DPC预测未来表示以识别时间偏差；基于CLIP的语义流通过上下文特定文本提示实现概念级异常检测。这些组件通过InfoNCE和CPC损失联合训练，将视觉输入与时空和语义表示对齐。此外，一个上下文门控机制通过场景感知线索或全局视频特征调制预测，以增强决策。

Result: 该系统能够在训练期间未接触异常样本的情况下识别异常事件，并能泛化到复杂环境中以前未见的行为。它弥合了零样本异常检测中时间推理和语义上下文之间的鸿沟。

Conclusion: 该框架通过整合预测建模与视觉-语言理解，为监控领域提供了一种在复杂环境中检测零样本异常的有效方法，能够处理不可预测和上下文相关的异常事件。

Abstract: Detecting anomalies in surveillance footage is inherently challenging due to
their unpredictable and context-dependent nature. This work introduces a novel
context-aware zero-shot anomaly detection framework that identifies abnormal
events without exposure to anomaly examples during training. The proposed
hybrid architecture combines TimeSformer, DPC, and CLIP to model spatiotemporal
dynamics and semantic context. TimeSformer serves as the vision backbone to
extract rich spatial-temporal features, while DPC forecasts future
representations to identify temporal deviations. Furthermore, a CLIP-based
semantic stream enables concept-level anomaly detection through
context-specific text prompts. These components are jointly trained using
InfoNCE and CPC losses, aligning visual inputs with their temporal and semantic
representations. A context-gating mechanism further enhances decision-making by
modulating predictions with scene-aware cues or global video features. By
integrating predictive modeling with vision-language understanding, the system
can generalize to previously unseen behaviors in complex environments. This
framework bridges the gap between temporal reasoning and semantic context in
zero-shot anomaly detection for surveillance. The code for this research has
been made available at
https://github.com/NK-II/Context-Aware-ZeroShot-Anomaly-Detection-in-Surveillance.

</details>


### [63] [DoGFlow: Self-Supervised LiDAR Scene Flow via Cross-Modal Doppler Guidance](https://arxiv.org/abs/2508.18506)
*Ajinkya Khoche,Qingwen Zhang,Yixi Cai,Sina Sharif Mansouri,Patric Jensfelt*

Main category: cs.CV

TL;DR: DoGFlow是一种新颖的自监督框架，通过跨模态标签迁移，利用4D雷达多普勒测量为LiDAR场景流估计生成伪标签，显著优于现有自监督方法并提高了标签效率。


<details>
  <summary>Details</summary>
Motivation: 3D场景流估计对自动驾驶系统至关重要，但手动标注大型数据集成本高昂，导致监督方法难以扩展；而现有自监督方法在性能上，尤其是在复杂场景下，无法与全监督方法匹敌。

Method: DoGFlow采用跨模态标签迁移方法。它实时地从4D雷达多普勒测量中计算运动伪标签，并通过动态感知关联和歧义消除传播，将这些伪标签转移到LiDAR领域，实现全3D物体运动的自监督恢复。

Result: 在MAN TruckScenes数据集上，DoGFlow显著优于现有自监督方法。它仅使用10%的真实标注数据，就能使LiDAR骨干网络达到全监督性能的90%以上，极大地提高了标签效率。

Conclusion: DoGFlow提供了一种无需手动标注即可进行3D场景流估计的有效自监督解决方案，通过创新的雷达-LiDAR跨模态标签迁移，显著提升了自监督方法的性能和标签效率，为自动驾驶系统在动态环境中的安全导航提供了关键支持。

Abstract: Accurate 3D scene flow estimation is critical for autonomous systems to
navigate dynamic environments safely, but creating the necessary large-scale,
manually annotated datasets remains a significant bottleneck for developing
robust perception models. Current self-supervised methods struggle to match the
performance of fully supervised approaches, especially in challenging
long-range and adverse weather scenarios, while supervised methods are not
scalable due to their reliance on expensive human labeling. We introduce
DoGFlow, a novel self-supervised framework that recovers full 3D object motions
for LiDAR scene flow estimation without requiring any manual ground truth
annotations. This paper presents our cross-modal label transfer approach, where
DoGFlow computes motion pseudo-labels in real-time directly from 4D radar
Doppler measurements and transfers them to the LiDAR domain using dynamic-aware
association and ambiguity-resolved propagation. On the challenging MAN
TruckScenes dataset, DoGFlow substantially outperforms existing self-supervised
methods and improves label efficiency by enabling LiDAR backbones to achieve
over 90% of fully supervised performance with only 10% of the ground truth
data. For more details, please visit https://ajinkyakhoche.github.io/DogFlow/

</details>


### [64] [SAT-SKYLINES: 3D Building Generation from Satellite Imagery and Coarse Geometric Priors](https://arxiv.org/abs/2508.18531)
*Zhangyu Jin,Andrew Feng*

Main category: cs.CV

TL;DR: SatSkylines是一种3D建筑生成方法，它利用卫星图像和粗略几何先验，通过建模从粗糙先验到详细几何的转换，以灵活控制生成高精度建筑模型。


<details>
  <summary>Details</summary>
Motivation: 现有基于图像的3D生成方法难以仅从卫星图像的俯视图恢复准确的建筑结构；而3D细节化方法则严重依赖高度详细的体素输入，无法从简单的先验（如长方体）生成令人满意的结果。

Method: 核心思想是建模从插值噪声粗糙先验到详细几何形状的转换，从而实现灵活的几何控制，且不增加计算成本。此外，开发了一个包含超过50,000个独特和风格化3D建筑资产的大规模数据集Skylines-50K。

Result: 广泛的评估表明，该模型有效且具有强大的泛化能力。

Conclusion: SatSkylines成功解决了从卫星图像和粗略几何先验生成详细3D建筑模型的挑战，提供了有效且可泛化的解决方案。

Abstract: We present SatSkylines, a 3D building generation approach that takes
satellite imagery and coarse geometric priors. Without proper geometric
guidance, existing image-based 3D generation methods struggle to recover
accurate building structures from the top-down views of satellite images alone.
On the other hand, 3D detailization methods tend to rely heavily on highly
detailed voxel inputs and fail to produce satisfying results from simple priors
such as cuboids. To address these issues, our key idea is to model the
transformation from interpolated noisy coarse priors to detailed geometries,
enabling flexible geometric control without additional computational cost. We
have further developed Skylines-50K, a large-scale dataset of over 50,000
unique and stylized 3D building assets in order to support the generations of
detailed building models. Extensive evaluations indicate the effectiveness of
our model and strong generalization ability.

</details>


### [65] [Adaptive Visual Navigation Assistant in 3D RPGs](https://arxiv.org/abs/2508.18539)
*Kaijie Xu,Clark Verbrugge*

Main category: cs.CV

TL;DR: 本文提出了一种检测3D游戏环境中可穿越空间过渡点（STP）并选择主STP（MSTP）的新问题，并开发了一个两阶段深度学习管道作为基线，旨在促进AI导航和关卡设计。


<details>
  <summary>Details</summary>
Motivation: 在复杂的3D游戏环境中，玩家需要识别地图过渡点。高效识别这些点对于客户端自动地图生成和评估地图提示至关重要。本文旨在形式化检测可穿越STP并从多个STP中选择唯一的主STP（MSTP）的任务。

Method: 本文提出一个两阶段深度学习管道：第一阶段使用Faster R-CNN检测潜在的STP；第二阶段使用一个轻量级MSTP选择器对STP进行排名，该选择器融合了局部和全局视觉特征。两个阶段都利用了参数高效的适配器，并引入了一个可选的检索增强融合步骤。该方法在从五款动作RPG游戏中收集的自定义数据集上进行了验证。

Result: 研究证实了该问题的可行性并建立了基线性能指标。实验揭示了一个关键权衡：在数据充足的情况下，全网络微调在STP检测方面表现更优；但在数据量较少或执行MSTP选择任务时，仅使用适配器的迁移学习则更鲁棒和有效。

Conclusion: 本文定义了一个新颖的问题，提供了一个基线管道和数据集，并为高效模型适应提供了初步见解。目标是为未来的AI驱动导航辅助和数据驱动的关卡设计工具做出贡献。

Abstract: In complex 3D game environments, players rely on visual affordances to spot
map transition points. Efficient identification of such points is important to
client-side auto-mapping, and provides an objective basis for evaluating map
cue presentation. In this work, we formalize the task of detecting traversable
Spatial Transition Points (STPs)-connectors between two sub regions-and
selecting the singular Main STP (MSTP), the unique STP that lies on the
designer-intended critical path toward the player's current macro-objective,
from a single game frame, proposing this as a new research focus. We introduce
a two-stage deep-learning pipeline that first detects potential STPs using
Faster R-CNN and then ranks them with a lightweight MSTP selector that fuses
local and global visual features. Both stages benefit from parameter-efficient
adapters, and we further introduce an optional retrieval-augmented fusion step.
Our primary goal is to establish the feasibility of this problem and set
baseline performance metrics. We validate our approach on a custom-built,
diverse dataset collected from five Action RPG titles. Our experiments reveal a
key trade-off: while full-network fine-tuning produces superior STP detection
with sufficient data, adapter-only transfer is significantly more robust and
effective in low-data scenarios and for the MSTP selection task. By defining
this novel problem, providing a baseline pipeline and dataset, and offering
initial insights into efficient model adaptation, we aim to contribute to
future AI-driven navigation aids and data-informed level-design tools.

</details>


### [66] [Wan-S2V: Audio-Driven Cinematic Video Generation](https://arxiv.org/abs/2508.18621)
*Xin Gao,Li Hu,Siqi Hu,Mingyang Huang,Chaonan Ji,Dechao Meng,Jinwei Qi,Penchong Qiao,Zhen Shen,Yafei Song,Ke Sun,Linrui Tian,Guangyuan Wang,Qi Wang,Zhongjian Wang,Jiayu Xiao,Sheng Xu,Bang Zhang,Peng Zhang,Xindi Zhang,Zhe Zhang,Jingren Zhou,Lian Zhuo*

Main category: cs.CV

TL;DR: Wan-S2V是一种基于音频驱动的角色动画模型，专为电影级制作设计，显著提升了复杂电影场景中的表现力和逼真度，并超越了现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 当前的音频驱动角色动画方法在电影和电视制作等复杂场景中表现不足，无法满足细致的角色互动、逼真的肢体动作和动态运镜等高要求，因此需要解决实现电影级角色动画的长期挑战。

Method: 本文提出了一种名为Wan-S2V的音频驱动模型，该模型基于Wan构建，旨在增强电影语境下的表现力和逼真度。通过广泛实验，将其与Hunyuan-Avatar和Omnihuman等尖端模型进行了基准测试。

Result: 实验结果表明，Wan-S2V模型在电影语境下的表现力和逼真度显著优于现有方法（如Hunyuan-Avatar和Omnihuman）。此外，该方法还适用于长视频生成和精准视频唇形同步编辑。

Conclusion: Wan-S2V模型成功解决了实现电影级角色动画的挑战，在电影场景中提供了显著增强的表现力和逼真度，超越了现有最先进的解决方案。

Abstract: Current state-of-the-art (SOTA) methods for audio-driven character animation
demonstrate promising performance for scenarios primarily involving speech and
singing. However, they often fall short in more complex film and television
productions, which demand sophisticated elements such as nuanced character
interactions, realistic body movements, and dynamic camera work. To address
this long-standing challenge of achieving film-level character animation, we
propose an audio-driven model, which we refere to as Wan-S2V, built upon Wan.
Our model achieves significantly enhanced expressiveness and fidelity in
cinematic contexts compared to existing approaches. We conducted extensive
experiments, benchmarking our method against cutting-edge models such as
Hunyuan-Avatar and Omnihuman. The experimental results consistently demonstrate
that our approach significantly outperforms these existing solutions.
Additionally, we explore the versatility of our method through its applications
in long-form video generation and precise video lip-sync editing.

</details>


### [67] [Decouple, Reorganize, and Fuse: A Multimodal Framework for Cancer Survival Prediction](https://arxiv.org/abs/2508.18632)
*Huayi Wang,Haochao Ying,Yuyang Xu,Qibo Qiu,Cheng Zhang,Danny Z. Chen,Ying Sun,Jian Wu*

Main category: cs.CV

TL;DR: 该研究提出了一个名为DeReF的新型解耦-重组-融合框架，通过引入随机特征重组策略和区域交叉注意力网络，解决了癌症生存分析中现有模态融合方法（如固定融合和MoE专家网络信息隔离）的局限性，从而提高了预测的泛化能力和信息交互。


<details>
  <summary>Details</summary>
Motivation: 现有癌症生存分析方法在模态融合方面面临两大挑战：1) 固定融合方案（如拼接、注意力）可能导致模型过度依赖预定义的特征组合，限制了去耦特征的动态融合；2) 基于MoE的融合方法中，每个专家网络独立处理去耦特征，限制了去耦特征之间的信息交互。

Method: 该论文提出了一个新颖的解耦-重组-融合（DeReF）框架。其核心方法包括：1) 在模态解耦和动态MoE融合模块之间设计了随机特征重组策略，以增加特征组合的多样性和粒度，并克服信息封闭问题；2) 在模态解耦模块中引入了区域交叉注意力网络，以提高去耦特征的表示质量。

Result: 在自建的肝癌（LC）数据集和三个广泛使用的TCGA公共数据集上进行了广泛的实验，结果证实了所提出方法的有效性。

Conclusion: DeReF框架通过其独特的随机特征重组和区域交叉注意力机制，成功解决了现有癌症生存分析中模态融合的挑战，有效提升了模型捕获去耦特征间信息的能力和泛化性，从而提高了生存时间预测的准确性。

Abstract: Cancer survival analysis commonly integrates information across diverse
medical modalities to make survival-time predictions. Existing methods
primarily focus on extracting different decoupled features of modalities and
performing fusion operations such as concatenation, attention, and MoE-based
(Mixture-of-Experts) fusion. However, these methods still face two key
challenges: i) Fixed fusion schemes (concatenation and attention) can lead to
model over-reliance on predefined feature combinations, limiting the dynamic
fusion of decoupled features; ii) in MoE-based fusion methods, each expert
network handles separate decoupled features, which limits information
interaction among the decoupled features. To address these challenges, we
propose a novel Decoupling-Reorganization-Fusion framework (DeReF), which
devises a random feature reorganization strategy between modalities decoupling
and dynamic MoE fusion modules.Its advantages are: i) it increases the
diversity of feature combinations and granularity, enhancing the generalization
ability of the subsequent expert networks; ii) it overcomes the problem of
information closure and helps expert networks better capture information among
decoupled features. Additionally, we incorporate a regional cross-attention
network within the modality decoupling module to improve the representation
quality of decoupled features. Extensive experimental results on our in-house
Liver Cancer (LC) and three widely used TCGA public datasets confirm the
effectiveness of our proposed method. The code will be made publicly available.

</details>


### [68] [ROSE: Remove Objects with Side Effects in Videos](https://arxiv.org/abs/2508.18633)
*Chenxuan Miao,Yutong Feng,Jianshu Zeng,Zixiang Gao,Hantang Liu,Yunfeng Yan,Donglian Qi,Xi Chen,Bin Wang,Hengshuang Zhao*

Main category: cs.CV

TL;DR: 该论文提出了ROSE框架，一个基于扩散Transformer的视频修复模型，旨在全面移除视频中的物体及其阴影、反射等五种副作用。通过3D渲染引擎生成大规模合成配对数据，并引入新的ROSE-Bench基准进行评估，ROSE在移除物体及其副作用方面表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有视频物体移除方法在处理物体的副作用（如阴影和反射）时表现不佳，主要原因是缺乏用于监督的配对视频数据。

Method: 该研究系统性地将物体对环境的影响分为五种常见情况：阴影、反射、光照、半透明和镜面。为解决数据稀缺问题，利用3D渲染引擎构建了一个全自动管道，用于生成包含多样场景、物体和拍摄角度的大规模合成配对数据集。ROSE模型是一个基于扩散Transformer的视频修复模型，通过输入整个视频进行参考擦除，并引入额外监督，利用配对视频之间的差异掩码明确预测受副作用影响的区域。此外，还提出了一个新的基准ROSE-Bench，用于全面评估模型在各种副作用移除任务上的性能。

Result: 实验结果表明，ROSE在视频物体擦除方面优于现有模型，并能很好地泛化到真实世界的视频场景。

Conclusion: ROSE框架通过对物体副作用的系统性研究、创新的合成数据生成方法和基于扩散Transformer的模型设计，成功解决了视频中物体及其副作用的移除难题，并在新基准上展示了卓越性能和泛化能力。

Abstract: Video object removal has achieved advanced performance due to the recent
success of video generative models. However, when addressing the side effects
of objects, e.g., their shadows and reflections, existing works struggle to
eliminate these effects for the scarcity of paired video data as supervision.
This paper presents ROSE, termed Remove Objects with Side Effects, a framework
that systematically studies the object's effects on environment, which can be
categorized into five common cases: shadows, reflections, light, translucency
and mirror. Given the challenges of curating paired videos exhibiting the
aforementioned effects, we leverage a 3D rendering engine for synthetic data
generation. We carefully construct a fully-automatic pipeline for data
preparation, which simulates a large-scale paired dataset with diverse scenes,
objects, shooting angles, and camera trajectories. ROSE is implemented as an
video inpainting model built on diffusion transformer. To localize all
object-correlated areas, the entire video is fed into the model for
reference-based erasing. Moreover, additional supervision is introduced to
explicitly predict the areas affected by side effects, which can be revealed
through the differential mask between the paired videos. To fully investigate
the model performance on various side effect removal, we presents a new
benchmark, dubbed ROSE-Bench, incorporating both common scenarios and the five
special side effects for comprehensive evaluation. Experimental results
demonstrate that ROSE achieves superior performance compared to existing video
object erasing models and generalizes well to real-world video scenarios. The
project page is https://rose2025-inpaint.github.io/.

</details>


### [69] [OwlCap: Harmonizing Motion-Detail for Video Captioning via HMD-270K and Caption Set Equivalence Reward](https://arxiv.org/abs/2508.18634)
*Chunlin Zhong,Qiuxia Hou,Zhangjun Zhou,Shuang Hao,Haonan Lu,Yanhao Zhang,He Tang,Xiang Bai*

Main category: cs.CV

TL;DR: 该研究提出了OwlCap，一个多模态大语言模型，通过构建HMD-270K数据集和引入CSER奖励机制，解决了视频字幕生成中运动和细节描述不平衡的问题，实现了更全面和准确的视频理解与生成。


<details>
  <summary>Details</summary>
Motivation: 现有视频字幕生成方法存在运动-细节不平衡问题，模型倾向于过度强调某一方面而忽略另一方面，导致生成的字幕不完整，进而影响视频理解和生成的一致性。

Method: 1) 数据层面：构建了HMD-270K数据集，通过“运动-细节融合（MDF）”和“细粒度检查（FGE）”两阶段流程实现。2) 优化层面：基于组相对策略优化（GRPO）引入了“字幕集等效奖励（CSER）”，通过单元到集合匹配和双向验证增强了捕捉运动和细节的完整性和准确性。基于HMD-270K监督微调和CSER的GRPO后训练，开发了OwlCap模型。

Result: 实验结果表明，OwlCap在两个基准测试中取得了显著改进：在注重细节的VDC上准确率提升了4.2%，在注重运动的DREAM-1K上F1分数提升了4.6%，优于基线模型。

Conclusion: OwlCap模型通过数据和优化两方面的创新，有效解决了视频字幕生成中的运动-细节不平衡问题，实现了更全面、一致的视频理解与生成。HMD-270K数据集和OwlCap模型将公开发布，以促进视频字幕研究社区的进步。

Abstract: Video captioning aims to generate comprehensive and coherent descriptions of
the video content, contributing to the advancement of both video understanding
and generation. However, existing methods often suffer from motion-detail
imbalance, as models tend to overemphasize one aspect while neglecting the
other. This imbalance results in incomplete captions, which in turn leads to a
lack of consistency in video understanding and generation. To address this
issue, we propose solutions from two aspects: 1) Data aspect: We constructed
the Harmonizing Motion-Detail 270K (HMD-270K) dataset through a two-stage
pipeline: Motion-Detail Fusion (MDF) and Fine-Grained Examination (FGE). 2)
Optimization aspect: We introduce the Caption Set Equivalence Reward (CSER)
based on Group Relative Policy Optimization (GRPO). CSER enhances completeness
and accuracy in capturing both motion and details through unit-to-set matching
and bidirectional validation. Based on the HMD-270K supervised fine-tuning and
GRPO post-training with CSER, we developed OwlCap, a powerful video captioning
multi-modal large language model (MLLM) with motion-detail balance.
Experimental results demonstrate that OwlCap achieves significant improvements
compared to baseline models on two benchmarks: the detail-focused VDC (+4.2
Acc) and the motion-focused DREAM-1K (+4.6 F1). The HMD-270K dataset and OwlCap
model will be publicly released to facilitate video captioning research
community advancements.

</details>


### [70] [Clustering-based Feature Representation Learning for Oracle Bone Inscriptions Detection](https://arxiv.org/abs/2508.18641)
*Ye Tao,Xinran Fu,Honglin Pang,Xi Yang,Chuntao Li*

Main category: cs.CV

TL;DR: 该研究提出了一种新颖的基于聚类的特征空间表示学习方法，利用甲骨文字库作为先验知识，通过集成专门的聚类损失函数，显著提升了甲骨文拓片图像检测的性能，解决了传统方法在图像退化下的局限性。


<details>
  <summary>Details</summary>
Motivation: 甲骨文对于理解中国古代文明至关重要。从拓片图像中自动检测甲骨文是一项基础但具挑战性的任务，主要原因是噪声和裂缝等多种退化因素限制了传统检测网络的有效性。

Method: 本文提出了一种新颖的基于聚类的特征空间表示学习方法。该方法独特地利用甲骨文字符（OBC）字库数据集作为先验知识，通过基于聚类的表示学习来增强检测网络中的特征提取。它包含一个源自聚类结果的专门损失函数，用于优化特征表示，并将其整合到总网络损失中。该方法在两个甲骨文检测数据集上，使用Faster R-CNN、DETR和Sparse R-CNN三个主流检测框架进行了验证。

Result: 通过广泛的实验，所有框架（Faster R-CNN、DETR和Sparse R-CNN）都展示了显著的性能提升。

Conclusion: 所提出的基于聚类的特征空间表示学习方法能够有效应对甲骨文拓片图像检测中的退化挑战，并显著提升了主流检测框架的性能。

Abstract: Oracle Bone Inscriptions (OBIs), play a crucial role in understanding ancient
Chinese civilization. The automated detection of OBIs from rubbing images
represents a fundamental yet challenging task in digital archaeology, primarily
due to various degradation factors including noise and cracks that limit the
effectiveness of conventional detection networks. To address these challenges,
we propose a novel clustering-based feature space representation learning
method. Our approach uniquely leverages the Oracle Bones Character (OBC) font
library dataset as prior knowledge to enhance feature extraction in the
detection network through clustering-based representation learning. The method
incorporates a specialized loss function derived from clustering results to
optimize feature representation, which is then integrated into the total
network loss. We validate the effectiveness of our method by conducting
experiments on two OBIs detection dataset using three mainstream detection
frameworks: Faster R-CNN, DETR, and Sparse R-CNN. Through extensive
experimentation, all frameworks demonstrate significant performance
improvements.

</details>


### [71] [SFormer: SNR-guided Transformer for Underwater Image Enhancement from the Frequency Domain](https://arxiv.org/abs/2508.18664)
*Xin Tian,Yingtie Lei,Xiujun Zhang,Zimeng Li,Chi-Man Pun,Xuhang Chen*

Main category: cs.CV

TL;DR: 本文提出了一种名为 SFormer 的水下图像增强（UIE）方法，通过在频域应用信噪比（SNR）先验，并结合新型傅里叶注意力 Transformer 模块，有效解决了现有空间域 SNR 先验的局限性，显著提升了水下图像的颜色、纹理和对比度恢复效果。


<details>
  <summary>Details</summary>
Motivation: 现有的基于学习的水下图像增强方法虽然结合了物理先验（特别是 SNR 先验），但在空间域应用 SNR 先验存在两个主要限制：(i) 无法有效分离跨通道干扰；(ii) 在放大信息结构和抑制噪声方面的帮助有限。这促使研究者寻求更有效的方式利用 SNR 先验。

Method: 本文提出在频域使用 SNR 先验，将特征分解为幅度和相位谱以实现更好的通道调制。具体方法包括：1. 引入傅里叶注意力 SNR-先验 Transformer (FAST)，结合频谱交互和 SNR 线索来突出关键频谱分量。2. 设计频率自适应 Transformer (FAT) 瓶颈，利用门控注意力机制融合低频和高频分支，以增强感知质量。3. 将这些模块嵌入到统一的 U 型架构中，形成 SFormer，该架构整合了传统的 RGB 流和 SNR 引导分支。模型在 UIEB、EUVP 和 LSUI 的 4,800 对图像上进行训练。

Result: SFormer 在 PSNR 上比现有方法提高了 3.1 dB，在 SSIM 上提高了 0.08。该方法成功恢复了水下场景的颜色、纹理和对比度，超越了当前先进的方法。

Conclusion: 通过在频域利用 SNR 先验并结合创新的 Transformer 模块（FAST 和 FAT），SFormer 有效克服了空间域 SNR 先验的局限性，显著提升了水下图像增强的性能，实现了更准确的颜色、纹理和对比度恢复。

Abstract: Recent learning-based underwater image enhancement (UIE) methods have
advanced by incorporating physical priors into deep neural networks,
particularly using the signal-to-noise ratio (SNR) prior to reduce
wavelength-dependent attenuation. However, spatial domain SNR priors have two
limitations: (i) they cannot effectively separate cross-channel interference,
and (ii) they provide limited help in amplifying informative structures while
suppressing noise. To overcome these, we propose using the SNR prior in the
frequency domain, decomposing features into amplitude and phase spectra for
better channel modulation. We introduce the Fourier Attention SNR-prior
Transformer (FAST), combining spectral interactions with SNR cues to highlight
key spectral components. Additionally, the Frequency Adaptive Transformer (FAT)
bottleneck merges low- and high-frequency branches using a gated attention
mechanism to enhance perceptual quality. Embedded in a unified U-shaped
architecture, these modules integrate a conventional RGB stream with an
SNR-guided branch, forming SFormer. Trained on 4,800 paired images from UIEB,
EUVP, and LSUI, SFormer surpasses recent methods with a 3.1 dB gain in PSNR and
0.08 in SSIM, successfully restoring colors, textures, and contrast in
underwater scenes.

</details>


### [72] [Hierarchical Spatio-temporal Segmentation Network for Ejection Fraction Estimation in Echocardiography Videos](https://arxiv.org/abs/2508.18681)
*Dongfang Wang,Jian Yang,Yizhe Zhang,Tao Zhou*

Main category: cs.CV

TL;DR: 该研究提出了一种分层时空分割网络（HS2Net），用于超声心动图视频中的左心室心内膜分割。通过结合局部细节建模和全局动态感知，并引入时空交叉扫描模块，旨在显著提高射血分数（EF）的估计精度。


<details>
  <summary>Details</summary>
Motivation: 超声心动图视频中左心室心内膜的自动分割是心脏病学中的关键研究领域，旨在通过射血分数（EF）估计准确评估心脏结构和功能。然而，现有方法虽然在分割性能上表现良好，但在EF估计方面的表现不佳。

Method: 本文提出了一种分层时空分割网络（HS2Net）。其分层设计在低层使用卷积网络处理单帧图像以保留细节，在高层利用Mamba架构捕获时空关系，从而平衡单帧与多帧处理。为克服局部时空限制，还引入了时空交叉扫描（STCS）模块，通过跨帧和位置的跳跃扫描来整合长程上下文。

Result: 该方法旨在通过协同局部细节建模与全局动态感知，显著提高射血分数（EF）的估计准确性。此外，通过整合长程上下文，有助于减轻超声图像噪声及其他因素导致的EF计算偏差。

Conclusion: 所提出的分层时空分割网络（HS2Net）及其时空交叉扫描（STCS）模块，通过有效结合局部细节和全局动态信息，能够改善超声心动图视频中的左心室心内膜分割，从而提高射血分数（EF）的估计精度，克服了现有方法的局局限性。

Abstract: Automated segmentation of the left ventricular endocardium in
echocardiography videos is a key research area in cardiology. It aims to
provide accurate assessment of cardiac structure and function through Ejection
Fraction (EF) estimation. Although existing studies have achieved good
segmentation performance, their results do not perform well in EF estimation.
In this paper, we propose a Hierarchical Spatio-temporal Segmentation Network
(\ourmodel) for echocardiography video, aiming to improve EF estimation
accuracy by synergizing local detail modeling with global dynamic perception.
The network employs a hierarchical design, with low-level stages using
convolutional networks to process single-frame images and preserve details,
while high-level stages utilize the Mamba architecture to capture
spatio-temporal relationships. The hierarchical design balances single-frame
and multi-frame processing, avoiding issues such as local error accumulation
when relying solely on single frames or neglecting details when using only
multi-frame data. To overcome local spatio-temporal limitations, we propose the
Spatio-temporal Cross Scan (STCS) module, which integrates long-range context
through skip scanning across frames and positions. This approach helps mitigate
EF calculation biases caused by ultrasound image noise and other factors.

</details>


### [73] [Feature-Space Planes Searcher: A Universal Domain Adaptation Framework for Interpretability and Computational Efficiency](https://arxiv.org/abs/2508.18693)
*Zhitong Cheng,Yiran Jiang,Yulong Ge,Yufeng Li,Zhongheng Qin,Rongzhi Lin,Jianwei Ma*

Main category: cs.CV

TL;DR: 本文提出了一种名为FPS（Feature-space Planes Searcher）的新型无监督域适应（UDA）框架，通过在冻结的特征空间中优化决策边界来解决域偏移问题，利用预训练模型中领域不变的几何模式，从而提高效率、可解释性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 深度学习系统在从有标签源域到无标签目标域的转换过程中，性能下降（即域偏移）是一个持续的挑战。现有的UDA方法主要依赖于微调特征提取器，这种方法存在效率低下、可解释性差以及对现代架构可扩展性不足的问题。

Method: 本文分析发现，在大规模数据上预训练的模型在特征空间中表现出领域不变的几何模式（类内聚类和类间分离），表明域偏移主要表现为边界未对齐而非特征退化。因此，本文提出了FPS框架，通过冻结特征编码器，利用这些几何模式优化决策边界，而不是微调整个预训练模型。该方法通过离线特征提取，大幅减少内存和计算成本，允许在单个计算周期内进行全数据集优化。

Result: 在公共基准测试上的评估表明，FPS实现了与最先进方法相当或更优的性能。FPS能有效地扩展到多模态大型模型，并在蛋白质结构预测、遥感分类和地震检测等不同领域显示出多功能性。

Conclusion: FPS为迁移学习，特别是在域适应任务中，提供了一种简单、有效且可推广的范式。

Abstract: Domain shift, characterized by degraded model performance during transition
from labeled source domains to unlabeled target domains, poses a persistent
challenge for deploying deep learning systems. Current unsupervised domain
adaptation (UDA) methods predominantly rely on fine-tuning feature extractors -
an approach limited by inefficiency, reduced interpretability, and poor
scalability to modern architectures.
  Our analysis reveals that models pretrained on large-scale data exhibit
domain-invariant geometric patterns in their feature space, characterized by
intra-class clustering and inter-class separation, thereby preserving
transferable discriminative structures. These findings indicate that domain
shifts primarily manifest as boundary misalignment rather than feature
degradation.
  Unlike fine-tuning entire pre-trained models - which risks introducing
unpredictable feature distortions - we propose the Feature-space Planes
Searcher (FPS): a novel domain adaptation framework that optimizes decision
boundaries by leveraging these geometric patterns while keeping the feature
encoder frozen. This streamlined approach enables interpretative analysis of
adaptation while substantially reducing memory and computational costs through
offline feature extraction, permitting full-dataset optimization in a single
computation cycle.
  Evaluations on public benchmarks demonstrate that FPS achieves competitive or
superior performance to state-of-the-art methods. FPS scales efficiently with
multimodal large models and shows versatility across diverse domains including
protein structure prediction, remote sensing classification, and earthquake
detection. We anticipate FPS will provide a simple, effective, and
generalizable paradigm for transfer learning, particularly in domain adaptation
tasks. .

</details>


### [74] [A Novel Deep Hybrid Framework with Ensemble-Based Feature Optimization for Robust Real-Time Human Activity Recognition](https://arxiv.org/abs/2508.18695)
*Wasi Ullah,Yasir Noman Khalid,Saddam Hussain Khan*

Main category: cs.CV

TL;DR: 本文提出了一种混合深度学习框架，结合定制版InceptionV3、LSTM和基于集成学习的遗传算法进行特征选择，实现了高精度、低计算成本和实时可扩展的人体活动识别（HAR），适用于边缘设备部署。


<details>
  <summary>Details</summary>
Motivation: 当前HAR系统面临计算成本高、特征冗余以及实时场景下可扩展性受限等挑战。

Method: 该框架首先使用定制版InceptionV3提取空间描述符（捕获多级上下文模式、区域同质性和精细定位线索）；接着，利用LSTM建模帧间的时间依赖性（有效编码运动动态）；最后，采用带有自适应动态适应度共享和注意力的（ADFSA）集成遗传算法，通过动态平衡准确性、冗余性、独特性和复杂性降低等目标，选择紧凑且优化的特征集。最终，选定的特征子集用于轻量级机器学习分类器进行HAR。

Result: 在UCF-YouTube数据集上，该方法实现了99.65%的识别准确率，将特征数量减少至7个，并缩短了推理时间。其轻量级和可扩展性使其能够实时部署在树莓派等边缘设备上。

Conclusion: 所提出的HAR系统准确、鲁棒、轻量且可扩展，支持在公共安全、辅助技术和自主监控系统等智能、资源受限环境中进行实时部署和实际应用。

Abstract: Human Activity Recognition (HAR) plays a pivotal role in various
applications, including smart surveillance, healthcare, assistive technologies,
sports analytics, etc. However, HAR systems still face critical challenges,
including high computational costs, redundant features, and limited scalability
in real-time scenarios. An optimized hybrid deep learning framework is
introduced that integrates a customized InceptionV3, an LSTM architecture, and
a novel ensemble-based feature selection strategy. The proposed framework first
extracts spatial descriptors using the customized InceptionV3 model, which
captures multilevel contextual patterns, region homogeneity, and fine-grained
localization cues. The temporal dependencies across frames are then modeled
using LSTMs to effectively encode motion dynamics. Finally, an ensemble-based
genetic algorithm with Adaptive Dynamic Fitness Sharing and Attention (ADFSA)
is employed to select a compact and optimized feature set by dynamically
balancing objectives such as accuracy, redundancy, uniqueness, and complexity
reduction. Consequently, the selected feature subsets, which are both diverse
and discriminative, enable various lightweight machine learning classifiers to
achieve accurate and robust HAR in heterogeneous environments. Experimental
results on the robust UCF-YouTube dataset, which presents challenges such as
occlusion, cluttered backgrounds, motion dynamics, and poor illumination,
demonstrate good performance. The proposed approach achieves 99.65% recognition
accuracy, reduces features to as few as 7, and enhances inference time. The
lightweight and scalable nature of the HAR system supports real-time deployment
on edge devices such as Raspberry Pi, enabling practical applications in
intelligent, resource-aware environments, including public safety, assistive
technology, and autonomous monitoring systems.

</details>


### [75] [ColorGS: High-fidelity Surgical Scene Reconstruction with Colored Gaussian Splatting](https://arxiv.org/abs/2508.18696)
*Qun Ji,Peng Li,Mingqiang Wei*

Main category: cs.CV

TL;DR: ColorGS是一种新颖的框架，通过引入彩色高斯基元和增强变形模型，显著提高了内窥镜视频中可变形组织的高保真重建质量，同时保持了实时渲染效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法（特别是3D高斯泼溅）在捕获细微颜色变化和建模全局变形方面存在局限性。固定的每高斯颜色分配难以处理复杂纹理，而线性变形建模无法实现一致的全局变形，这在内窥镜视频中重建可变形组织时是一个挑战。

Method: ColorGS框架包含两部分：1. **彩色高斯基元**：利用带有可学习颜色参数的动态锚点，自适应编码空间变化的纹理，提高复杂光照下的颜色表现力。2. **增强变形模型（EDM）**：结合时间感知高斯基函数和可学习的时间无关变形，精确捕捉局部组织变形和手术交互引起的全局运动一致性。

Result: 在达芬奇机器人手术视频和基准数据集（EndoNeRF, StereoMIS）上的实验表明，ColorGS实现了最先进的性能，PSNR达到39.85（比之前基于3DGS的方法高1.5），SSIM达到97.25%，同时保持了实时渲染效率。

Conclusion: ColorGS通过平衡高保真度与计算实用性，推动了手术场景重建技术的发展，这对于术中指导和AR/VR应用至关重要。

Abstract: High-fidelity reconstruction of deformable tissues from endoscopic videos
remains challenging due to the limitations of existing methods in capturing
subtle color variations and modeling global deformations. While 3D Gaussian
Splatting (3DGS) enables efficient dynamic reconstruction, its fixed
per-Gaussian color assignment struggles with intricate textures, and linear
deformation modeling fails to model consistent global deformation. To address
these issues, we propose ColorGS, a novel framework that integrates spatially
adaptive color encoding and enhanced deformation modeling for surgical scene
reconstruction. First, we introduce Colored Gaussian Primitives, which employ
dynamic anchors with learnable color parameters to adaptively encode spatially
varying textures, significantly improving color expressiveness under complex
lighting and tissue similarity. Second, we design an Enhanced Deformation Model
(EDM) that combines time-aware Gaussian basis functions with learnable
time-independent deformations, enabling precise capture of both localized
tissue deformations and global motion consistency caused by surgical
interactions. Extensive experiments on DaVinci robotic surgery videos and
benchmark datasets (EndoNeRF, StereoMIS) demonstrate that ColorGS achieves
state-of-the-art performance, attaining a PSNR of 39.85 (1.5 higher than prior
3DGS-based methods) and superior SSIM (97.25\%) while maintaining real-time
rendering efficiency. Our work advances surgical scene reconstruction by
balancing high fidelity with computational practicality, critical for
intraoperative guidance and AR/VR applications.

</details>


### [76] [Are All Marine Species Created Equal? Performance Disparities in Underwater Object Detection](https://arxiv.org/abs/2508.18729)
*Melanie Wille,Tobias Fischer,Scarlett Raine*

Main category: cs.CV

TL;DR: 该研究分析了水下目标检测中类别特异性性能差异的原因，发现前景-背景判别是主要定位问题，且分类存在固有的特征挑战。建议根据优先级选择数据分布，并通过算法改进定位模块以提升性能。


<details>
  <summary>Details</summary>
Motivation: 水下目标检测对海洋生态系统监测至关重要，但面临图像质量差、类别分布不平衡和视觉特征独特等挑战。不同物种的检测效果差异大，但深层原因尚不清楚，尤其是在数据量之外的因素。

Method: 研究通过操纵DUO数据集，将目标检测任务分解为定位和分类两部分，并以扇贝类为例进行深入分析。定位分析使用了YOLO11和TIDE工具，分类实验则探究了数据平衡后的精度差距。

Result: 定位分析发现，无论数据量多少，前景-背景判别都是最主要的问题阶段。分类实验表明，即使数据平衡，精度差距依然存在，这表明除了数据稀缺和类间依赖性外，还存在固有的基于特征的挑战。研究建议在优先考虑精度时使用不平衡分布，在优先考虑召回率时使用平衡分布。

Conclusion: 水下目标检测中表现不佳的类别，其性能提升应重点关注算法改进，尤其是在定位模块中。研究强调了前景-背景判别和内在特征挑战是主要的性能瓶颈。

Abstract: Underwater object detection is critical for monitoring marine ecosystems but
poses unique challenges, including degraded image quality, imbalanced class
distribution, and distinct visual characteristics. Not every species is
detected equally well, yet underlying causes remain unclear. We address two key
research questions: 1) What factors beyond data quantity drive class-specific
performance disparities? 2) How can we systematically improve detection of
under-performing marine species? We manipulate the DUO dataset to separate the
object detection task into localization and classification and investigate the
under-performance of the scallop class. Localization analysis using YOLO11 and
TIDE finds that foreground-background discrimination is the most problematic
stage regardless of data quantity. Classification experiments reveal persistent
precision gaps even with balanced data, indicating intrinsic feature-based
challenges beyond data scarcity and inter-class dependencies. We recommend
imbalanced distributions when prioritizing precision, and balanced
distributions when prioritizing recall. Improving under-performing classes
should focus on algorithmic advances, especially within localization modules.
We publicly release our code and datasets.

</details>


### [77] [Class-wise Flooding Regularization for Imbalanced Image Classification](https://arxiv.org/abs/2508.18723)
*Hiroaki Aizawa,Yuta Naito,Kohei Fukuda*

Main category: cs.CV

TL;DR: 本文提出了一种类级别泛洪正则化（class-wise flooding regularization）方法，通过根据类别频率设置不同的泛洪级别，解决不平衡数据集导致的少数类识别性能下降问题，从而提高少数类的分类性能和整体泛化能力。


<details>
  <summary>Details</summary>
Motivation: 在不平衡数据集上训练神经网络时，模型倾向于偏向多数类，导致少数类的识别性能显著下降。现有正则化方法可能无法有效解决此问题。

Method: 本文提出类级别泛洪正则化，它是传统泛洪正则化在类别层面的扩展。该方法根据类别频率为每个类别分配一个特定的泛洪级别。通过这种方式，它能抑制多数类的过拟合，同时允许少数类进行充分学习。

Result: 与传统泛洪正则化相比，本文提出的方法在不平衡图像分类任务中提高了少数类的分类性能，并实现了更好的整体泛化能力。

Conclusion: 类级别泛洪正则化通过动态调整泛洪级别来适应不同类别的学习需求，有效解决了不平衡数据集中少数类性能下降的问题，从而提升了模型的整体表现和泛化能力。

Abstract: The purpose of training neural networks is to achieve high generalization
performance on unseen inputs. However, when trained on imbalanced datasets, a
model's prediction tends to favor majority classes over minority classes,
leading to significant degradation in the recognition performance of minority
classes. To address this issue, we propose class-wise flooding regularization,
an extension of flooding regularization applied at the class level. Flooding is
a regularization technique that mitigates overfitting by preventing the
training loss from falling below a predefined threshold, known as the flooding
level, thereby discouraging memorization. Our proposed method assigns a
class-specific flooding level based on class frequencies. By doing so, it
suppresses overfitting in majority classes while allowing sufficient learning
for minority classes. We validate our approach on imbalanced image
classification. Compared to conventional flooding regularizations, our method
improves the classification performance of minority classes and achieves better
overall generalization.

</details>


### [78] [PseudoMapTrainer: Learning Online Mapping without HD Maps](https://arxiv.org/abs/2508.18788)
*Christian Löwens,Thorben Funke,Jingchao Xie,Alexandru Paul Condurache*

Main category: cs.CV

TL;DR: 本文提出PseudoMapTrainer，一种新的在线地图模型训练方法，它利用从无标签传感器数据生成的伪标签，首次实现在无需任何地面真值高清地图的情况下训练在线地图模型，并可用于半监督预训练。


<details>
  <summary>Details</summary>
Motivation: 现有在线地图模型在训练时严重依赖昂贵且地理多样性不足的地面真值高清地图，这限制了模型的泛化能力和数据获取成本。

Method: PseudoMapTrainer通过多视角相机图像，结合高斯泼溅(Gaussian splatting)和预训练的2D分割网络的语义信息，重建路面以生成伪标签。此外，该方法引入了一种掩码感知分配算法和损失函数来处理部分被遮挡的伪标签。这些伪标签还可用于大规模无标签众包数据的半监督预训练。

Result: 该方法首次实现了无需任何地面真值地图即可训练在线地图模型。所生成的伪标签还能有效用于以半监督方式预训练在线模型，以利用大规模无标签众包数据。

Conclusion: PseudoMapTrainer为在线地图模型提供了一种无需地面真值地图的训练范式，显著降低了训练成本，并通过利用无标签数据提高了模型的可扩展性和泛化潜力。

Abstract: Online mapping models show remarkable results in predicting vectorized maps
from multi-view camera images only. However, all existing approaches still rely
on ground-truth high-definition maps during training, which are expensive to
obtain and often not geographically diverse enough for reliable generalization.
In this work, we propose PseudoMapTrainer, a novel approach to online mapping
that uses pseudo-labels generated from unlabeled sensor data. We derive those
pseudo-labels by reconstructing the road surface from multi-camera imagery
using Gaussian splatting and semantics of a pre-trained 2D segmentation
network. In addition, we introduce a mask-aware assignment algorithm and loss
function to handle partially masked pseudo-labels, allowing for the first time
the training of online mapping models without any ground-truth maps.
Furthermore, our pseudo-labels can be effectively used to pre-train an online
model in a semi-supervised manner to leverage large-scale unlabeled
crowdsourced data. The code is available at
github.com/boschresearch/PseudoMapTrainer.

</details>


### [79] [Flatness-aware Curriculum Learning via Adversarial Difficulty](https://arxiv.org/abs/2508.18726)
*Hiroaki Aizawa,Yoshikazu Hayashi*

Main category: cs.CV

TL;DR: 针对神经网络过拟合问题，本文提出了一种结合课程学习（CL）和锐度感知最小化（SAM）的新方法。通过引入对抗性难度度量（ADM）来量化对抗性脆弱性，解决了在平坦区域难以评估样本难度的问题，并在多项任务中实现了优于现有方法的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 神经网络在经验风险最小化训练中常出现过拟合，导致泛化能力差。课程学习（CL）和锐度感知最小化（SAM）能改善泛化，但将两者结合存在挑战，因为在平坦区域，损失值和梯度范数趋于一致小，使得样本难度难以评估，从而影响有效课程的设计。

Method: 本文提出对抗性难度度量（Adversarial Difficulty Measure, ADM），利用模型在平坦最小值处的鲁棒性来量化对抗性脆弱性。与基于损失或梯度的度量不同，ADM通过衡量原始样本和对抗性样本之间的归一化损失差距来保持信息量。我们将ADM整合到基于CL的SAM训练中，以动态评估样本难度。

Result: 该方法在图像分类、细粒度识别和域泛化任务上进行了评估。结果表明，我们的方法保留了CL和SAM的优点，并优于现有的基于课程和平面感知的训练策略。

Conclusion: 通过引入对抗性难度度量（ADM），我们成功解决了在平坦最小值区域中评估样本难度的问题，有效结合了课程学习（CL）和锐度感知最小化（SAM），显著提升了模型在多种任务上的泛化能力和鲁棒性。

Abstract: Neural networks trained by empirical risk minimization often suffer from
overfitting, especially to specific samples or domains, which leads to poor
generalization. Curriculum Learning (CL) addresses this issue by selecting
training samples based on the difficulty. From the optimization perspective,
methods such as Sharpness-Aware Minimization (SAM) improve robustness and
generalization by seeking flat minima. However, combining CL with SAM is not
straightforward. In flat regions, both the loss values and the gradient norms
tend to become uniformly small, which makes it difficult to evaluate sample
difficulty and design an effective curriculum. To overcome this problem, we
propose the Adversarial Difficulty Measure (ADM), which quantifies adversarial
vulnerability by leveraging the robustness properties of models trained toward
flat minima. Unlike loss- or gradient-based measures, which become ineffective
as training progresses into flatter regions, ADM remains informative by
measuring the normalized loss gap between original and adversarial examples. We
incorporate ADM into CL-based training with SAM to dynamically assess sample
difficulty. We evaluated our approach on image classification tasks,
fine-grained recognition, and domain generalization. The results demonstrate
that our method preserves the strengths of both CL and SAM while outperforming
existing curriculum-based and flatness-aware training strategies.

</details>


### [80] [Interpretable Decision-Making for End-to-End Autonomous Driving](https://arxiv.org/abs/2508.18898)
*Mona Mirzaie,Bodo Rosenhahn*

Main category: cs.CV

TL;DR: 本文提出了一种通过优化损失函数生成稀疏局部特征图的方法，以提高自动驾驶控制模型的解释性，同时提升其性能和安全性，并在CARLA基准测试中取得了优异成绩。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆的广泛部署需要可信赖的AI，但现有的端到端方法（特别是基于深度神经网络的）在复杂城市场景中难以解释其决策逻辑，这阻碍了AI驱动决策的理解和信任。

Method: 研究人员提出了一种方法，通过设计特定的损失函数来促进模型的可解释性，这些损失函数旨在生成稀疏且局部化的特征图。这些特征激活允许解释图像的哪些区域对预测的控制指令做出了贡献。该方法在特征提取步骤上进行了全面的消融研究，并在CARLA基准测试上进行了验证。

Result: 该方法不仅提高了模型的解释性，还与减少违规行为相关联，从而产生了更安全、高性能的驾驶模型。值得注意的是，其单目、非集成模型在违规分数和路线完成率方面超越了CARLA排行榜上表现最佳的方法，同时确保了可解释性。

Conclusion: 该研究成功开发了一种既能保证决策可解释性，又能实现高性能和高安全性的自动驾驶控制模型，为自动驾驶AI的广泛部署奠定了基础。

Abstract: Trustworthy AI is mandatory for the broad deployment of autonomous vehicles.
Although end-to-end approaches derive control commands directly from raw data,
interpreting these decisions remains challenging, especially in complex urban
scenarios. This is mainly attributed to very deep neural networks with
non-linear decision boundaries, making it challenging to grasp the logic behind
AI-driven decisions. This paper presents a method to enhance interpretability
while optimizing control commands in autonomous driving. To address this, we
propose loss functions that promote the interpretability of our model by
generating sparse and localized feature maps. The feature activations allow us
to explain which image regions contribute to the predicted control command. We
conduct comprehensive ablation studies on the feature extraction step and
validate our method on the CARLA benchmarks. We also demonstrate that our
approach improves interpretability, which correlates with reducing infractions,
yielding a safer, high-performance driving model. Notably, our monocular,
non-ensemble model surpasses the top-performing approaches from the CARLA
Leaderboard by achieving lower infraction scores and the highest route
completion rate, all while ensuring interpretability.

</details>


### [81] [Drawing2CAD: Sequence-to-Sequence Learning for CAD Generation from Vectorized Drawings](https://arxiv.org/abs/2508.18733)
*Feiwei Qin,Shichao Lu,Junhao Hou,Changmiao Wang,Meie Fang,Ligang Liu*

Main category: cs.CV

TL;DR: 该研究提出Drawing2CAD框架，通过将CAD生成重构为序列到序列学习问题，实现从2D工程图自动生成参数化CAD模型，弥合了现有方法与传统工业工作流程之间的差距。


<details>
  <summary>Details</summary>
Motivation: 现有的CAD生成方法（如从点云、网格、文本生成）与从2D工程图开始的传统工业工作流程不符。从2D矢量图自动生成参数化CAD模型是一个关键但未充分探索的工程设计步骤，存在空白。

Method: 将CAD生成重新定义为序列到序列学习问题，直接将矢量图元转换为参数化CAD操作。提出Drawing2CAD框架，包含三个关键技术组件：1) 保留精确几何信息的网络友好型矢量图元表示；2) 解耦命令类型和参数生成但保持精确对应关系的双解码器Transformer架构；3) 适应CAD参数固有灵活性的软目标分布损失函数。为训练和评估，创建了CAD-VGDrawing数据集。

Result: 通过彻底的实验，证明了所提出Drawing2CAD方法的有效性。

Conclusion: Drawing2CAD框架能够有效地从2D矢量工程图自动生成参数化CAD模型，在转换过程中保留了几何精度和设计意图，填补了现有方法与传统工业工作流程之间的空白。

Abstract: Computer-Aided Design (CAD) generative modeling is driving significant
innovations across industrial applications. Recent works have shown remarkable
progress in creating solid models from various inputs such as point clouds,
meshes, and text descriptions. However, these methods fundamentally diverge
from traditional industrial workflows that begin with 2D engineering drawings.
The automatic generation of parametric CAD models from these 2D vector drawings
remains underexplored despite being a critical step in engineering design. To
address this gap, our key insight is to reframe CAD generation as a
sequence-to-sequence learning problem where vector drawing primitives directly
inform the generation of parametric CAD operations, preserving geometric
precision and design intent throughout the transformation process. We propose
Drawing2CAD, a framework with three key technical components: a
network-friendly vector primitive representation that preserves precise
geometric information, a dual-decoder transformer architecture that decouples
command type and parameter generation while maintaining precise correspondence,
and a soft target distribution loss function accommodating inherent flexibility
in CAD parameters. To train and evaluate Drawing2CAD, we create CAD-VGDrawing,
a dataset of paired engineering drawings and parametric CAD models, and conduct
thorough experiments to demonstrate the effectiveness of our method. Code and
dataset are available at https://github.com/lllssc/Drawing2CAD.

</details>


### [82] [VibES: Induced Vibration for Persistent Event-Based Sensing](https://arxiv.org/abs/2508.19094)
*Vincenzo Polizzi,Stephen Yang,Quentin Clark,Jonathan Kelly,Igor Gilitschenski,David B. Lindell*

Main category: cs.CV

TL;DR: 该研究提出了一种轻量级方法，通过引入周期性振动来持续生成事件相机事件，并结合运动补偿管道去除注入的运动，以改善在静态或低运动场景下的感知任务。


<details>
  <summary>Details</summary>
Motivation: 事件相机在静态或低运动场景下，如果照明条件固定，无法生成事件，因此不适用于大多数计算机视觉任务。现有解决方案通常需要复杂的硬件或额外的光学组件。

Method: 该方法通过使用一个简单的旋转不平衡质量来诱导周期性振动，从而持续生成事件。随后，结合一个运动补偿管道来移除注入的运动，以获得干净、运动校正后的事件，用于后续感知任务。

Result: 实验结果表明，该方法能够可靠地恢复运动参数，并且在图像重建和边缘检测方面，相较于没有运动诱导的事件传感，均有显著改善。

Conclusion: 通过引入简单的振动和运动补偿，该方法有效解决了事件相机在静态或低运动场景下事件生成不足的问题，提高了其在这些条件下的感知性能。

Abstract: Event cameras are a bio-inspired class of sensors that asynchronously measure
per-pixel intensity changes. Under fixed illumination conditions in static or
low-motion scenes, rigidly mounted event cameras are unable to generate any
events, becoming unsuitable for most computer vision tasks. To address this
limitation, recent work has investigated motion-induced event stimulation that
often requires complex hardware or additional optical components. In contrast,
we introduce a lightweight approach to sustain persistent event generation by
employing a simple rotating unbalanced mass to induce periodic vibrational
motion. This is combined with a motion-compensation pipeline that removes the
injected motion and yields clean, motion-corrected events for downstream
perception tasks. We demonstrate our approach with a hardware prototype and
evaluate it on real-world captured datasets. Our method reliably recovers
motion parameters and improves both image reconstruction and edge detection
over event-based sensing without motion induction.

</details>


### [83] [Improving Noise Robust Audio-Visual Speech Recognition via Router-Gated Cross-Modal Feature Fusion](https://arxiv.org/abs/2508.18734)
*DongHoon Lim,YoungChae Kim,Dong-Hyun Kim,Da-Hee Yang,Joon-Hyuk Chang*

Main category: cs.CV

TL;DR: 本文提出了一种名为“路由器门控跨模态特征融合”的新型音视频语音识别（AVSR）框架，通过自适应地根据音频可靠性分数重新加权音视频特征，显著提升了在嘈杂环境下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在嘈杂环境中，鲁棒的音视频语音识别（AVSR）仍然具有挑战性，因为现有系统难以准确估计音频可靠性并动态调整对不同模态的依赖。

Method: 该方法提出路由器门控跨模态特征融合，根据token级别的声学损坏分数自适应地重新加权音频和视觉特征。通过基于音视频特征融合的路由器，系统会降低不可靠音频token的权重，并通过每个解码器层中的门控交叉注意力增强视觉线索。这使得模型在音频质量下降时能转向视觉模态。

Result: 在LRS3数据集上的实验表明，与AV-HuBERT相比，该方法在词错误率（WER）上实现了16.51-42.67%的相对降低。消融研究证实，路由器和门控机制都对在真实世界声学噪声下的鲁棒性提升做出了贡献。

Conclusion: 所提出的路由器门控跨模态特征融合框架通过自适应地调整模态依赖性，显著提升了AVSR在嘈杂环境中的鲁棒性，有效解决了现有系统在音频可靠性估计和模态动态调整方面的不足。

Abstract: Robust audio-visual speech recognition (AVSR) in noisy environments remains
challenging, as existing systems struggle to estimate audio reliability and
dynamically adjust modality reliance. We propose router-gated cross-modal
feature fusion, a novel AVSR framework that adaptively reweights audio and
visual features based on token-level acoustic corruption scores. Using an
audio-visual feature fusion-based router, our method down-weights unreliable
audio tokens and reinforces visual cues through gated cross-attention in each
decoder layer. This enables the model to pivot toward the visual modality when
audio quality deteriorates. Experiments on LRS3 demonstrate that our approach
achieves an 16.51-42.67% relative reduction in word error rate compared to
AV-HuBERT. Ablation studies confirm that both the router and gating mechanism
contribute to improved robustness under real-world acoustic noise.

</details>


### [84] [Rethinking Human-Object Interaction Evaluation for both Vision-Language Models and HOI-Specific Methods](https://arxiv.org/abs/2508.18753)
*Qinqian Lei,Bo Wang,Robby T. Tan*

Main category: cs.CV

TL;DR: 本文引入了一个新的基准测试，将人-物交互（HOI）检测重新定义为多答案选择任务，旨在有效评估通用视觉-语言模型（VLM）与专用HOI方法，解决了现有基准在处理生成式VLM多重有效解释时的局限性。


<details>
  <summary>Details</summary>
Motivation: 早期的HOI检测方法仅将VLM作为辅助组件。随着大型生成式VLM的兴起，它们可能具备强大的HOI理解能力。然而，现有的HOI基准（如HICO-DET）要求精确匹配标注，这与生成式VLM可能产生多个有效解释的特性不符，导致有效预测可能被错误惩罚。

Method: 本文提出了一个新基准，将HOI检测重新定义为多答案多选任务。每个问题只包含真实的正向选项和一组精心策划的负向选项，以减少歧义。例如，在“捕捉”被标注时，“投掷”不会被选为负向选项，以避免惩罚合理的预测。

Result: 本文引入了首个针对VLM和HOI方法的此类评估协议和基准。该协议能够直接比较这两种范式，并为HOI理解的当前进展状态提供了新的见解。

Conclusion: 新的评估协议和基准为通用VLM和专用HOI方法提供了一个公平的比较平台，有望促进HOI理解领域的发展和深入洞察。

Abstract: Prior human-object interaction (HOI) detection methods have integrated early
vision-language models (VLMs) such as CLIP, but only as supporting components
within their frameworks. In contrast, recent advances in large, generative VLMs
suggest that these models may already possess strong ability to understand
images involving HOI. This naturally raises an important question: can
general-purpose standalone VLMs effectively solve HOI detection, and how do
they compare with specialized HOI methods? Answering this requires a benchmark
that can accommodate both paradigms. However, existing HOI benchmarks such as
HICO-DET were developed before the emergence of modern VLMs, and their
evaluation protocols require exact matches to annotated HOI classes. This is
poorly aligned with the generative nature of VLMs, which often yield multiple
valid interpretations in ambiguous cases. For example, a static image may
capture a person mid-motion with a frisbee, which can plausibly be interpreted
as either "throwing" or "catching". When only "catching" is annotated, the
other, though equally plausible for the image, is marked incorrect when exact
matching is used. As a result, correct predictions might be penalized,
affecting both VLMs and HOI-specific methods. To avoid penalizing valid
predictions, we introduce a new benchmark that reformulates HOI detection as a
multiple-answer multiple-choice task, where each question includes only
ground-truth positive options and a curated set of negatives that are
constructed to reduce ambiguity (e.g., when "catching" is annotated, "throwing"
is not selected as a negative to avoid penalizing valid predictions). The
proposed evaluation protocol is the first of its kind for both VLMs and HOI
methods, enabling direct comparison and offering new insight into the current
state of progress in HOI understanding.

</details>


### [85] [Beyond the Textual: Generating Coherent Visual Options for MCQs](https://arxiv.org/abs/2508.18772)
*Wanqiang Wang,Longzhu He,Wei Zheng*

Main category: cs.CV

TL;DR: 该研究提出了一种名为CmOS的新颖框架，用于生成带有视觉选项的教育多项选择题（MCQs），通过整合多模态思维链和检索增强生成，解决了现有方法忽视视觉选项和高质量干扰项生成成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注生成带有文本选项的MCQs，忽视了视觉选项。此外，由于手动创作成本高且可扩展性有限，生成高质量的干扰项（特别是视觉干扰项）仍然是一个重大挑战。

Method: 该论文提出了跨模态选项合成（CmOS）框架，用于生成带有视觉选项的教育MCQs。该框架整合了多模态思维链（MCoT）推理过程和检索增强生成（RAG），以生成语义合理且视觉相似的答案和干扰项。它还包含一个判别模块，用于识别适合视觉选项的内容。

Result: 在测试任务上的实验结果表明，CmOS在内容判别、问题生成和视觉选项生成方面，优于现有方法，且适用于各种学科和教育水平。

Conclusion: CmOS框架成功地解决了生成带有视觉选项的MCQs以及高质量干扰项的挑战，通过其创新的多模态整合和判别模块，显著提升了MCQs的生成质量和效率。

Abstract: Multiple-choice questions (MCQs) play a crucial role in fostering deep
thinking and knowledge integration in education. However, previous research has
primarily focused on generating MCQs with textual options, but it largely
overlooks the visual options. Moreover, generating high-quality distractors
remains a major challenge due to the high cost and limited scalability of
manual authoring. To tackle these problems, we propose a Cross-modal Options
Synthesis (CmOS), a novel framework for generating educational MCQs with visual
options. Our framework integrates Multimodal Chain-of-Thought (MCoT) reasoning
process and Retrieval-Augmented Generation (RAG) to produce semantically
plausible and visually similar answer and distractors. It also includes a
discrimination module to identify content suitable for visual options.
Experimental results on test tasks demonstrate the superiority of CmOS in
content discrimination, question generation and visual option generation over
existing methods across various subjects and educational levels.

</details>


### [86] [Design, Implementation and Evaluation of a Real-Time Remote Photoplethysmography (rPPG) Acquisition System for Non-Invasive Vital Sign Monitoring](https://arxiv.org/abs/2508.18787)
*Constantino Álvarez Casado,Sasan Sharifipour,Manuel Lage Cañellas,Nhi Nguyen,Le Nguyen,Miguel Bordallo López*

Main category: cs.CV

TL;DR: 本文提出一个针对低功耗设备的实时远程光电容积描记法（rPPG）系统，通过面部视频提取生理信号，并采用多线程架构及FRP/Actor混合编程模型，实现了高效、鲁棒的实时监测。


<details>
  <summary>Details</summary>
Motivation: 智能环境与低功耗计算设备的日益集成以及大众市场传感器技术的发展，推动了远程非接触式生理监测。然而，在资源受限平台上实时部署这些系统面临可扩展性、互操作性和性能方面的挑战。

Method: 该系统基于Face2PPG流水线，顺序处理视频帧以提取和分析rPPG信号。它采用多线程架构并发管理视频捕获、实时处理、网络通信和GUI更新，确保30 fps的连续可靠操作。网络接口包括HTTP服务器（用于视频流）和RESTful API（用于按需生命体征检索）。为保证在低功耗设备上的准确性，系统结合了函数响应式编程（FRP）和Actor模型，以实现事件驱动处理和高效任务并行化。

Result: 系统在实时约束下进行了评估，证明了其鲁棒性，同时最大限度地减少了计算开销，并能以30帧/秒的速度连续可靠运行。它为现代医疗保健和人机交互应用中的性能优化提供了实用解决方案。

Conclusion: 该工作解决了实时生物信号监测中的关键挑战，提供了一个针对低功耗设备优化的实时rPPG系统，适用于医疗保健和人机交互应用，具有实用且高性能的特点。

Abstract: The growing integration of smart environments and low-power computing
devices, coupled with mass-market sensor technologies, is driving advancements
in remote and non-contact physiological monitoring. However, deploying these
systems in real-time on resource-constrained platforms introduces significant
challenges related to scalability, interoperability, and performance. This
paper presents a real-time remote photoplethysmography (rPPG) system optimized
for low-power devices, designed to extract physiological signals, such as heart
rate (HR), respiratory rate (RR), and oxygen saturation (SpO2), from facial
video streams. The system is built on the Face2PPG pipeline, which processes
video frames sequentially for rPPG signal extraction and analysis, while
leveraging a multithreaded architecture to manage video capture, real-time
processing, network communication, and graphical user interface (GUI) updates
concurrently. This design ensures continuous, reliable operation at 30 frames
per second (fps), with adaptive feedback through a collaborative user interface
to guide optimal signal capture conditions. The network interface includes both
an HTTP server for continuous video streaming and a RESTful API for on-demand
vital sign retrieval. To ensure accurate performance despite the limitations of
low-power devices, we use a hybrid programming model combining Functional
Reactive Programming (FRP) and the Actor Model, allowing event-driven
processing and efficient task parallelization. The system is evaluated under
real-time constraints, demonstrating robustness while minimizing computational
overhead. Our work addresses key challenges in real-time biosignal monitoring,
offering practical solutions for optimizing performance in modern healthcare
and human-computer interaction applications.

</details>


### [87] [Robust and Label-Efficient Deep Waste Detection](https://arxiv.org/abs/2508.18799)
*Hassan Abid,Khan Muhammad,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: 本研究通过建立强大的基线和引入基于集成学习的半监督学习框架，显著提升了AI驱动的垃圾检测能力，特别是在零样本学习和利用未标注数据方面。


<details>
  <summary>Details</summary>
Motivation: 有效的垃圾分类对于可持续回收至关重要，但由于数据集有限和对传统目标检测器的依赖，该领域的AI研究落后于商业系统。

Method: 首先，在ZeroWaste数据集上基准测试了最先进的开放词汇目标检测（OVOD）模型，并评估了LLM优化提示的效果。其次，微调了现代基于Transformer的检测器。最后，提出了一种软伪标签策略，通过空间和共识感知的加权融合集成预测，实现了鲁棒的半监督训练。

Result: LLM优化提示显著提高了OVOD的零样本准确性。通过微调Transformer检测器，达到了51.6 mAP的新基线。应用于未标注的ZeroWaste-s子集时，其伪标注实现了超越完全监督训练的性能提升，突显了可扩展标注管道的有效性。

Conclusion: 本工作为研究社区建立了严格的基线，引入了鲁棒的基于集成学习的伪标签管道，为未标注的ZeroWaste-s子集生成了高质量的标注，并系统评估了真实世界垃圾分类条件下的OVOD模型。

Abstract: Effective waste sorting is critical for sustainable recycling, yet AI
research in this domain continues to lag behind commercial systems due to
limited datasets and reliance on legacy object detectors. In this work, we
advance AI-driven waste detection by establishing strong baselines and
introducing an ensemble-based semi-supervised learning framework. We first
benchmark state-of-the-art Open-Vocabulary Object Detection (OVOD) models on
the real-world ZeroWaste dataset, demonstrating that while class-only prompts
perform poorly, LLM-optimized prompts significantly enhance zero-shot accuracy.
Next, to address domain-specific limitations, we fine-tune modern
transformer-based detectors, achieving a new baseline of 51.6 mAP. We then
propose a soft pseudo-labeling strategy that fuses ensemble predictions using
spatial and consensus-aware weighting, enabling robust semi-supervised
training. Applied to the unlabeled ZeroWaste-s subset, our pseudo-annotations
achieve performance gains that surpass fully supervised training, underscoring
the effectiveness of scalable annotation pipelines. Our work contributes to the
research community by establishing rigorous baselines, introducing a robust
ensemble-based pseudo-labeling pipeline, generating high-quality annotations
for the unlabeled ZeroWaste-s subset, and systematically evaluating OVOD models
under real-world waste sorting conditions. Our code is available at:
https://github.com/h-abid97/robust-waste-detection.

</details>


### [88] [Embedding Font Impression Word Tags Based on Co-occurrence](https://arxiv.org/abs/2508.18825)
*Yugo Kubota,Seiichi Uchida*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的印象标签嵌入方法，该方法利用字体形状与印象之间的关系，通过构建共现图并应用谱嵌入来生成印象向量，在印象引导的字体生成中优于BERT和CLIP。


<details>
  <summary>Details</summary>
Motivation: 不同的字体风格传达不同的印象，表明字体形状与描述这些印象的词语（印象标签）之间存在密切关系。然而，标准的词嵌入方法（如BERT和CLIP）在表示字体印象时，对经常共现的印象标签会产生非常不同的向量，这不利于基于印象的字体生成和检索。

Method: 该方法构建了一个图，其中节点代表印象标签，边编码了标签之间的共现关系。随后，对该图应用谱嵌入（spectral embedding）来获取每个标签的印象向量。

Result: 与BERT和CLIP不同，本方法能为频繁共现的印象标签分配相似的向量。定性和定量评估表明，该方法在印象引导的字体生成方面表现更好。

Conclusion: 所提出的基于印象标签共现关系和谱嵌入的印象向量生成方法，能够有效捕捉字体印象，并在印象引导的字体生成任务中优于现有标准方法，对印象引导的字体生成和检索具有实用价值。

Abstract: Different font styles (i.e., font shapes) convey distinct impressions,
indicating a close relationship between font shapes and word tags describing
those impressions. This paper proposes a novel embedding method for impression
tags that leverages these shape-impression relationships. For instance, our
method assigns similar vectors to impression tags that frequently co-occur in
order to represent impressions of fonts, whereas standard word embedding
methods (e.g., BERT and CLIP) yield very different vectors. This property is
particularly useful for impression-based font generation and font retrieval.
Technically, we construct a graph whose nodes represent impression tags and
whose edges encode co-occurrence relationships. Then, we apply spectral
embedding to obtain the impression vectors for each tag. We compare our method
with BERT and CLIP in qualitative and quantitative evaluations, demonstrating
that our approach performs better in impression-guided font generation.

</details>


### [89] [Deep Pre-trained Time Series Features for Tree Species Classification in the Dutch Forest Inventory](https://arxiv.org/abs/2508.18829)
*Takayuki Ishikawa,Carmelo Bonannella,Bas J. W. Lerink,Marc Rußwurm*

Main category: cs.CV

TL;DR: 本研究利用预训练的遥感基础模型中的深度特征，显著提高了荷兰国家森林清查（NFI）中树种分类的准确性，超越了传统方法。


<details>
  <summary>Details</summary>
Motivation: 国家森林清查（NFI）是获取森林信息的主要来源，但其维护工作耗时耗力。遥感结合机器学习为更频繁、更大规模地更新NFI提供了机会。尽管卫星图像时间序列在区分树种方面有效，但现有方法主要依赖于随机森林分类器和手工设计的特征。本研究旨在探索利用预训练遥感基础模型中的深度特征来改进树种分类，尤其是在标注数据有限的情况下。

Method: 研究系统地调查了深度特征如何用少量标注数据提高荷兰的树种分类精度。数据方面，利用Google Earth Engine提取了Sentinel-1、Sentinel-2、ERA5卫星数据以及SRTM数据的时间序列。通过对一个公开可用的遥感时间序列基础模型进行微调，并将其性能与当前NFI分类的最新技术进行了比较。

Result: 结果表明，对公开可用的遥感时间序列基础模型进行微调，在所有数据集上比荷兰当前NFI分类的最新技术高出高达10%的准确率。这表明经典的、手工定义的谐波特征对于此任务过于简单，突显了深度AI特征在NFI分类等数据受限应用中的巨大潜力。

Conclusion: 利用开放可用的卫星数据和预训练模型，本方法显著提高了分类准确性，优于传统方法，并能有效补充现有的森林清查流程。深度AI特征对于数据有限的应用（如NFI分类）具有巨大潜力。

Abstract: National Forest Inventory (NFI)s serve as the primary source of forest
information, providing crucial tree species distribution data. However,
maintaining these inventories requires labor-intensive on-site campaigns.
Remote sensing approaches, particularly when combined with machine learning,
offer opportunities to update NFIs more frequently and at larger scales. While
the use of Satellite Image Time Series has proven effective for distinguishing
tree species through seasonal canopy reflectance patterns, current approaches
rely primarily on Random Forest classifiers with hand-designed features and
phenology-based metrics. Using deep features from an available pre-trained
remote sensing foundation models offers a complementary strategy. These
pre-trained models leverage unannotated global data and are meant to used for
general-purpose applications and can then be efficiently fine-tuned with
smaller labeled datasets for specific classification tasks. This work
systematically investigates how deep features improve tree species
classification accuracy in the Netherlands with few annotated data. Data-wise,
we extracted time-series data from Sentinel-1, Sentinel-2 and ERA5 satellites
data and SRTM data using Google Earth Engine. Our results demonstrate that
fine-tuning a publicly available remote sensing time series foundation model
outperforms the current state-of-the-art in NFI classification in the
Netherlands by a large margin of up to 10% across all datasets. This
demonstrates that classic hand-defined harmonic features are too simple for
this task and highlights the potential of using deep AI features for
data-limited application like NFI classification. By leveraging openly
available satellite data and pre-trained models, this approach significantly
improves classification accuracy compared to traditional methods and can
effectively complement existing forest inventory processes.

</details>


### [90] [Automated Classification of Normal and Atypical Mitotic Figures Using ConvNeXt V2: MIDOG 2025 Track 2](https://arxiv.org/abs/2508.18831)
*Yosuke Yamagishi,Shouhei Hanaoka*

Main category: cs.CV

TL;DR: 本文提出了一种针对MIDOG 2025挑战赛Track 2的解决方案，利用ConvNeXt V2模型、中心裁剪预处理和5折交叉验证集成策略，实现了组织病理图像中正常与非典型有丝分裂像的二分类，并在解决类别不平衡、形态变异性和域异质性等挑战的同时，取得了稳健的性能。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在解决MIDOG 2025挑战赛Track 2中，组织病理图像内正常有丝分裂像（NMFs）与非典型有丝分裂像（AMFs）的二分类问题，同时应对严重的类别不平衡、高形态变异性以及跨肿瘤类型、物种和扫描仪的域异质性等关键挑战。

Method: 本方法采用ConvNeXt V2作为基础模型，结合60%中心裁剪的图像预处理，并利用5折交叉验证集成策略。此外，还采用了混合精度训练以提高计算效率和模型性能，旨在有效处理类别不平衡、形态多样性和领域异质性问题。

Result: 该模型在多样化的MIDOG 2025数据集上取得了稳健的性能，成功地对有丝分裂像进行了亚型分类。

Conclusion: 该解决方案证明了现代卷积架构在有丝分裂像亚型分类方面的有效性，并通过精心选择的架构和训练优化，同时保持了计算效率。

Abstract: This paper presents our solution for the MIDOG 2025 Challenge Track 2, which
focuses on binary classification of normal mitotic figures (NMFs) versus
atypical mitotic figures (AMFs) in histopathological images. Our approach
leverages a ConvNeXt V2 base model with center cropping preprocessing and
5-fold cross-validation ensemble strategy. The method addresses key challenges
including severe class imbalance, high morphological variability, and domain
heterogeneity across different tumor types, species, and scanners. Through
strategic preprocessing with 60% center cropping and mixed precision training,
our model achieved robust performance on the diverse MIDOG 2025 dataset. The
solution demonstrates the effectiveness of modern convolutional architectures
for mitotic figure subtyping while maintaining computational efficiency through
careful architectural choices and training optimizations.

</details>


### [91] [Boosting Micro-Expression Analysis via Prior-Guided Video-Level Regression](https://arxiv.org/abs/2508.18834)
*Zizheng Guo,Bochao Zou,Yinuo Jia,Xiangyu Li,Huimin Ma*

Main category: cs.CV

TL;DR: 本文提出了一种先验引导的视频级回归方法，用于微表情分析。该方法通过可伸缩区间选择策略和协同优化框架，实现了微表情的精确识别和定位，并在多个基准数据集上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有微表情分析方法依赖于固定窗口大小和硬决策的窗口级分类，难以捕捉复杂的时序动态。尽管视频级回归框架有所改进，但区间解码仍依赖手动预定义的窗口方法，问题未能完全解决。

Method: 本文提出了一种先验引导的视频级回归方法。核心方法包括：1) 可伸缩区间选择策略，综合考虑微表情的时序演变、持续时间和类别分布，以精确识别起始、顶点和结束阶段；2) 协同优化框架，其中微表情定位和识别任务共享参数（分类头除外），以充分利用互补信息并提高模型能力。

Result: 在多个基准数据集上的广泛实验表明，本文方法达到了最先进的性能，例如在CAS(ME)³数据集上的STRS为0.0562，在SAMMLV数据集上为0.2000。

Conclusion: 本文提出的先验引导视频级回归方法，结合可伸缩区间选择和协同优化框架，有效解决了微表情分析中时序动态捕捉和区间解码的挑战，显著提升了微表情的定位和识别性能。

Abstract: Micro-expressions (MEs) are involuntary, low-intensity, and short-duration
facial expressions that often reveal an individual's genuine thoughts and
emotions. Most existing ME analysis methods rely on window-level classification
with fixed window sizes and hard decisions, which limits their ability to
capture the complex temporal dynamics of MEs. Although recent approaches have
adopted video-level regression frameworks to address some of these challenges,
interval decoding still depends on manually predefined, window-based methods,
leaving the issue only partially mitigated. In this paper, we propose a
prior-guided video-level regression method for ME analysis. We introduce a
scalable interval selection strategy that comprehensively considers the
temporal evolution, duration, and class distribution characteristics of MEs,
enabling precise spotting of the onset, apex, and offset phases. In addition,
we introduce a synergistic optimization framework, in which the spotting and
recognition tasks share parameters except for the classification heads. This
fully exploits complementary information, makes more efficient use of limited
data, and enhances the model's capability. Extensive experiments on multiple
benchmark datasets demonstrate the state-of-the-art performance of our method,
with an STRS of 0.0562 on CAS(ME)$^3$ and 0.2000 on SAMMLV. The code is
available at https://github.com/zizheng-guo/BoostingVRME.

</details>


### [92] [Quantitative Outcome-Oriented Assessment of Microsurgical Anastomosis](https://arxiv.org/abs/2508.18836)
*Luyin Hu,Soheil Gholami,George Dindelegan,Torstein R. Meling,Aude Billard*

Main category: cs.CV

TL;DR: 本文提出了一种基于图像处理的定量框架，用于客观评估显微外科吻合术，通过几何建模和评分机制，提高评估效率和可靠性，并有效复现专家评分。


<details>
  <summary>Details</summary>
Motivation: 显微外科吻合术的评估方法（如吻合术失误指数）目前依赖主观判断，这引入了偏见，影响了能力评估的可靠性和效率，因此需要更客观的评估方法。

Method: 研究利用来自三家医院、包含不同水平参与者的数据集，引入了一个基于图像处理技术的定量框架。该方法采用错误几何建模以及检测和评分机制，以实现显微外科吻合术的客观评估。

Result: 结果表明，所提出的几何指标能够有效复现专家评分员对本研究中考虑的错误的评分。

Conclusion: 该定量框架提高了显微外科熟练度评估的效率和可靠性，并有助于改进培训方案。

Abstract: Microsurgical anastomosis demands exceptional dexterity and visuospatial
skills, underscoring the importance of comprehensive training and precise
outcome assessment. Currently, methods such as the outcome-oriented anastomosis
lapse index are used to evaluate this procedure. However, they often rely on
subjective judgment, which can introduce biases that affect the reliability and
efficiency of the assessment of competence. Leveraging three datasets from
hospitals with participants at various levels, we introduce a quantitative
framework that uses image-processing techniques for objective assessment of
microsurgical anastomoses. The approach uses geometric modeling of errors along
with a detection and scoring mechanism, enhancing the efficiency and
reliability of microsurgical proficiency assessment and advancing training
protocols. The results show that the geometric metrics effectively replicate
expert raters' scoring for the errors considered in this work.

</details>


### [93] [Harnessing Meta-Learning for Controllable Full-Frame Video Stabilization](https://arxiv.org/abs/2508.18859)
*Muhammad Kashif Ali,Eun Woo Im,Dongjin Kim,Tae Hyun Kim,Vivek Gupta,Haonan Luo,Tianrui Li*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的方法，通过在测试时快速适应每个输入视频，显著改进了像素级合成视频稳定方法，利用低级视觉线索并结合抖动定位模块进行有针对性的适应，从而提升了稳定性与视觉质量。


<details>
  <summary>Details</summary>
Motivation: 像素级合成视频稳定方法由于视频序列中多样的运动模式和视觉内容，难以通过固定参数实现鲁棒的泛化。

Method: 提出了一种在测试时快速适应模型到每个输入视频的新方法，利用推理过程中可用的低级视觉线索。此外，还提出了一个抖动定位模块和有针对性的适应策略，将适应集中在高抖动片段上，以在更少的适应步骤中最大化稳定性。

Result: 即使通过单次适应，也能实现显著的性能提升。该方法使现代稳定器能够超越长期以来的SOTA方法，同时保持全帧特性，并提供类似于经典方法的用户控制机制。在多样化的真实世界数据集上的广泛实验表明，该方法在定性和定量方面持续改进了各种全帧合成模型的性能，包括在下游应用中的表现。

Conclusion: 所提出的快速适应方法，特别是结合抖动定位和有针对性适应策略，有效解决了像素级合成视频稳定方法的泛化难题，显著提升了视频的稳定性和视觉质量，并为用户提供了更好的控制，使现代稳定器能够达到新的技术水平。

Abstract: Video stabilization remains a fundamental problem in computer vision,
particularly pixel-level synthesis solutions for video stabilization, which
synthesize full-frame outputs, add to the complexity of this task. These
methods aim to enhance stability while synthesizing full-frame videos, but the
inherent diversity in motion profiles and visual content present in each video
sequence makes robust generalization with fixed parameters difficult. To
address this, we present a novel method that improves pixel-level synthesis
video stabilization methods by rapidly adapting models to each input video at
test time. The proposed approach takes advantage of low-level visual cues
available during inference to improve both the stability and visual quality of
the output. Notably, the proposed rapid adaptation achieves significant
performance gains even with a single adaptation pass. We further propose a jerk
localization module and a targeted adaptation strategy, which focuses the
adaptation on high-jerk segments for maximizing stability with fewer adaptation
steps. The proposed methodology enables modern stabilizers to overcome the
longstanding SOTA approaches while maintaining the full frame nature of the
modern methods, while offering users with control mechanisms akin to classical
approaches. Extensive experiments on diverse real-world datasets demonstrate
the versatility of the proposed method. Our approach consistently improves the
performance of various full-frame synthesis models in both qualitative and
quantitative terms, including results on downstream applications.

</details>


### [94] [Toward Robust Medical Fairness: Debiased Dual-Modal Alignment via Text-Guided Attribute-Disentangled Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2508.18886)
*Yuexuan Xia,Benteng Ma,Jiang He,Zhiyong Wang,Qi Dou,Yong Xia*

Main category: cs.CV

TL;DR: DualFairVL是一个多模态提示学习框架，通过联合去偏和对齐跨模态表示，显著提高了医学诊断在分布偏移下的公平性和准确性，且参数高效。


<details>
  <summary>Details</summary>
Motivation: 在医学诊断中，确保跨人群的公平性至关重要，尤其是在成像设备和临床实践变化导致的分布偏移下。现有去偏方法通常独立处理视觉和文本模态，导致残余的跨模态未对齐和公平性差距，需要一种能联合处理并对齐多模态表示的方法。

Method: DualFairVL采用并行双分支架构来分离敏感和目标属性，实现跨模态的解耦但对齐的表示。它通过线性投影构建近似正交的文本锚点以引导交叉注意力，并使用超网络生成编码双模态线索的实例感知视觉提示。此外，视觉分支应用基于原型的正则化，以强制分离敏感特征并加强与文本锚点的对齐。

Result: 在八个医学影像数据集（涵盖四种模态）上进行的大量实验表明，DualFairVL在分布内和分布外设置下均实现了最先进的公平性和准确性。它优于全微调和参数高效基线，且仅需3.6M可训练参数。

Conclusion: DualFairVL通过其创新的多模态提示学习框架，成功解决了医学诊断中公平性与跨模态对齐的挑战，显著提升了在不同分布设置下的公平性和准确性，为公平医疗保健提供了有效且参数高效的解决方案。

Abstract: Ensuring fairness across demographic groups in medical diagnosis is essential
for equitable healthcare, particularly under distribution shifts caused by
variations in imaging equipment and clinical practice. Vision-language models
(VLMs) exhibit strong generalization, and text prompts encode identity
attributes, enabling explicit identification and removal of sensitive
directions. However, existing debiasing approaches typically address vision and
text modalities independently, leaving residual cross-modal misalignment and
fairness gaps. To address this challenge, we propose DualFairVL, a multimodal
prompt-learning framework that jointly debiases and aligns cross-modal
representations. DualFairVL employs a parallel dual-branch architecture that
separates sensitive and target attributes, enabling disentangled yet aligned
representations across modalities. Approximately orthogonal text anchors are
constructed via linear projections, guiding cross-attention mechanisms to
produce fused features. A hypernetwork further disentangles attribute-related
information and generates instance-aware visual prompts, which encode
dual-modal cues for fairness and robustness. Prototype-based regularization is
applied in the visual branch to enforce separation of sensitive features and
strengthen alignment with textual anchors. Extensive experiments on eight
medical imaging datasets across four modalities show that DualFairVL achieves
state-of-the-art fairness and accuracy under both in- and out-of-distribution
settings, outperforming full fine-tuning and parameter-efficient baselines with
only 3.6M trainable parameters. Code will be released upon publication.

</details>


### [95] [DQEN: Dual Query Enhancement Network for DETR-based HOI Detection](https://arxiv.org/abs/2508.18896)
*Zhehao Li,Chong Wang,Yi Chen,Yinghao Lu,Jiangbo Qian,Jiong Wang,Jiafei Wu*

Main category: cs.CV

TL;DR: 本文提出了一种双重查询增强网络（DQEN），通过利用目标感知编码器特征和CLIP模型促进的交互语义融合来增强目标和交互查询，从而改进基于DETR的人-物交互（HOI）检测。


<details>
  <summary>Details</summary>
Motivation: 现有基于DETR的HOI模型通常依赖随机初始化的查询，导致表示模糊，限制了模型有效性。此外，HOI中人是固定的，而物体和交互是可变的，需要更精准地处理目标和交互查询。

Method: 本文提出了双重查询增强网络（DQEN）。具体来说，目标查询通过目标感知编码器特征进行增强，使其更有效地关注与物体交互的人。同时，设计了一个新颖的交互语义融合模块，利用CLIP模型提升的HOI候选，提取语义特征以增强交互查询的初始化。此外，引入了一个辅助预测单元来改进交互特征的表示。

Result: 所提出的方法在HICO-Det和V-COCO数据集上均取得了有竞争力的性能。

Conclusion: DQEN通过对目标和交互查询进行有意义的增强，显著提升了基于DETR的HOI检测模型的有效性，解决了传统随机初始化查询带来的表示模糊问题。

Abstract: Human-Object Interaction (HOI) detection focuses on localizing human-object
pairs and recognizing their interactions. Recently, the DETR-based framework
has been widely adopted in HOI detection. In DETR-based HOI models, queries
with clear meaning are crucial for accurately detecting HOIs. However, prior
works have typically relied on randomly initialized queries, leading to vague
representations that limit the model's effectiveness. Meanwhile, humans in the
HOI categories are fixed, while objects and their interactions are variable.
Therefore, we propose a Dual Query Enhancement Network (DQEN) to enhance object
and interaction queries. Specifically, object queries are enhanced with
object-aware encoder features, enabling the model to focus more effectively on
humans interacting with objects in an object-aware way. On the other hand, we
design a novel Interaction Semantic Fusion module to exploit the HOI candidates
that are promoted by the CLIP model. Semantic features are extracted to enhance
the initialization of interaction queries, thereby improving the model's
ability to understand interactions. Furthermore, we introduce an Auxiliary
Prediction Unit aimed at improving the representation of interaction features.
Our proposed method achieves competitive performance on both the HICO-Det and
the V-COCO datasets. The source code is available at
https://github.com/lzzhhh1019/DQEN.

</details>


### [96] [Event-Enriched Image Analysis Grand Challenge at ACM Multimedia 2025](https://arxiv.org/abs/2508.18904)
*Thien-Phuc Tran,Minh-Quang Nguyen,Minh-Triet Tran,Tam V. Nguyen,Trong-Le Do,Duy-Nam Ly,Viet-Tham Huynh,Khanh-Duy Le,Mai-Khiem Tran,Trung-Nghia Le*

Main category: cs.CV

TL;DR: EVENTA大挑战是ACM Multimedia 2025上推出的首个大规模事件级多模态理解基准，旨在通过整合上下文、时间、语义信息来超越传统图像分析的局限。


<details>
  <summary>Details</summary>
Motivation: 传统的图像标注和检索任务主要关注人物、物体和场景的表层识别，往往忽略了定义真实世界事件的上下文和语义维度。EVENTA旨在弥补这一空白，捕捉图像背后的“何人、何时、何地、何事、何因”。

Method: EVENTA挑战基于OpenEvents V1数据集构建，设有事件增强图像检索与标注、以及基于事件的图像检索两个赛道。共有来自六个国家的45支团队参与，通过公开和私有测试阶段进行评估，以确保公平性和可复现性。

Result: 共有45支团队参与了挑战，前三名团队受邀在ACM Multimedia 2025上展示了他们的解决方案。EVENTA为上下文感知、叙事驱动的多媒体AI奠定了基础。

Conclusion: EVENTA挑战为事件级多模态理解建立了新的基准，推动了上下文感知、叙事驱动的多媒体AI发展，并在新闻、媒体分析、文化存档和无障碍等领域具有广泛应用前景。

Abstract: The Event-Enriched Image Analysis (EVENTA) Grand Challenge, hosted at ACM
Multimedia 2025, introduces the first large-scale benchmark for event-level
multimodal understanding. Traditional captioning and retrieval tasks largely
focus on surface-level recognition of people, objects, and scenes, often
overlooking the contextual and semantic dimensions that define real-world
events. EVENTA addresses this gap by integrating contextual, temporal, and
semantic information to capture the who, when, where, what, and why behind an
image. Built upon the OpenEvents V1 dataset, the challenge features two tracks:
Event-Enriched Image Retrieval and Captioning, and Event-Based Image Retrieval.
A total of 45 teams from six countries participated, with evaluation conducted
through Public and Private Test phases to ensure fairness and reproducibility.
The top three teams were invited to present their solutions at ACM Multimedia
2025. EVENTA establishes a foundation for context-aware, narrative-driven
multimedia AI, with applications in journalism, media analysis, cultural
archiving, and accessibility. Further details about the challenge are available
at the official homepage: https://ltnghia.github.io/eventa/eventa-2025.

</details>


### [97] [Preliminary Study on Space Utilization and Emergent Behaviors of Group vs. Single Pedestrians in Real-World Trajectories](https://arxiv.org/abs/2508.18939)
*Amartaivan Sanjjamts,Morita Hiroshi*

Main category: cs.CV

TL;DR: 本研究提出了一个区分群体和个体行人的初步框架，旨在分析他们在空间利用和行为模式上的差异，并为后续的定量分析和应用奠定基础。


<details>
  <summary>Details</summary>
Motivation: 理解群体和个体行人在空间利用和紧急行为模式上的差异对于行人模拟和空间设计至关重要。

Method: 通过将行人轨迹分段，并应用基于Transformer的对分类模型来识别群体和个体行人。研究建立了一个综合度量框架，包含空间利用指标（如凸包面积、最小外接圆半径、热力图空间密度）和行为指标（如速度变化、运动角度偏差、净空半径、轨迹直线度）。此外，还引入了不同相遇类型（个体对个体、个体对群体、群体对群体）的分类。

Result: 本研究版本主要关注分类流程和数据集的构建，成功为在不同序列长度（60、100、200帧）下进行可扩展分析奠定了基础。

Conclusion: 本研究成功构建了一个区分群体和个体行人的初步框架，并建立了用于深入分析其空间利用和行为模式差异的度量体系和数据结构，为未来在人群动力学研究中的定量分析和应用奠定了基础。

Abstract: This study presents an initial framework for distinguishing group and single
pedestrians based on real-world trajectory data, with the aim of analyzing
their differences in space utilization and emergent behavioral patterns. By
segmenting pedestrian trajectories into fixed time bins and applying a
Transformer-based pair classification model, we identify cohesive groups and
isolate single pedestrians over a structured sequence-based filtering process.
To prepare for deeper analysis, we establish a comprehensive metric framework
incorporating both spatial and behavioral dimensions. Spatial utilization
metrics include convex hull area, smallest enclosing circle radius, and
heatmap-based spatial densities to characterize how different pedestrian types
occupy and interact with space. Behavioral metrics such as velocity change,
motion angle deviation, clearance radius, and trajectory straightness are
designed to capture local adaptations and responses during interactions.
Furthermore, we introduce a typology of encounter types-single-to-single,
single-to-group, and group-to-group to categorize and later quantify different
interaction scenarios. Although this version focuses primarily on the
classification pipeline and dataset structuring, it establishes the groundwork
for scalable analysis across different sequence lengths 60, 100, and 200
frames. Future versions will incorporate complete quantitative analysis of the
proposed metrics and their implications for pedestrian simulation and space
design validation in crowd dynamics research.

</details>


### [98] [The point is the mask: scaling coral reef segmentation with weak supervision](https://arxiv.org/abs/2508.18958)
*Matteo Contini,Victor Illien,Sylvain Poulain,Serge Bernard,Julien Barde,Sylvain Bonhommeau,Alexis Joly*

Main category: cs.CV

TL;DR: 本文提出了一种多尺度弱监督语义分割框架，通过将水下图像的精细尺度生态信息转移到无人机图像中，实现了大规模珊瑚礁形态类型的低成本、高分辨率监测。


<details>
  <summary>Details</summary>
Motivation: 大规模珊瑚礁监测面临挑战：无人机图像空间覆盖广但分辨率不足以区分精细尺度类别（如珊瑚形态类型）；同时，对大范围航空图像进行像素级标注成本高昂且耗时，限制了深度学习分割方法的可扩展性。

Method: 本研究提出了一种多尺度弱监督语义分割框架。该方法通过结合基于分类的监督、空间插值和自蒸馏技术，将水下图像中的精细尺度生态信息转移到航空数据中，从而实现对无人机图像的大规模珊瑚礁测绘，同时最大限度地减少手动标注。

Result: 该方法有效实现了珊瑚形态类型的大面积分割，并展示了整合新类别的灵活性。

Conclusion: 本研究提出了一种可扩展、经济高效的高分辨率珊瑚礁监测方法，该方法结合了低成本数据收集、弱监督深度学习和多尺度遥感技术。

Abstract: Monitoring coral reefs at large spatial scales remains an open challenge,
essential for assessing ecosystem health and informing conservation efforts.
While drone-based aerial imagery offers broad spatial coverage, its limited
resolution makes it difficult to reliably distinguish fine-scale classes, such
as coral morphotypes. At the same time, obtaining pixel-level annotations over
large spatial extents is costly and labor-intensive, limiting the scalability
of deep learning-based segmentation methods for aerial imagery. We present a
multi-scale weakly supervised semantic segmentation framework that addresses
this challenge by transferring fine-scale ecological information from
underwater imagery to aerial data. Our method enables large-scale coral reef
mapping from drone imagery with minimal manual annotation, combining
classification-based supervision, spatial interpolation and self-distillation
techniques. We demonstrate the efficacy of the approach, enabling large-area
segmentation of coral morphotypes and demonstrating flexibility for integrating
new classes. This study presents a scalable, cost-effective methodology for
high-resolution reef monitoring, combining low-cost data collection, weakly
supervised deep learning and multi-scale remote sensing.

</details>


### [99] [Generative AI in Map-Making: A Technical Exploration and Its Implications for Cartographers](https://arxiv.org/abs/2508.18959)
*Claudio Affolter,Sidi Wu,Yizi Chen,Lorenz Hurni*

Main category: cs.CV

TL;DR: 该研究提出了一种结合矢量数据和生成式AI（特别是图像扩散模型）的新方法，通过文本提示生成准确且风格可控的地图，并将其集成到Web应用中，旨在提高地图制作的效率和可访问性。


<details>
  <summary>Details</summary>
Motivation: 传统的地图制作依赖GIS，耗时且需要专业知识，尤其在重复任务中效率低下。现有的生成式AI模型在地图生成中难以精确控制空间构成和语义布局，因此需要一种能克服这些限制的方法。

Method: 该研究通过整合矢量数据来指导图像扩散模型生成地图，并允许用户通过文本提示指定地图风格。同时，开发了一个Web应用程序以提高模型可用性和可访问性。此外，还对专业制图师进行了用户研究，评估生成地图的保真度和应用可用性。

Result: 该模型首次实现了在受控风格下生成准确地图。用户研究结果表明，所开发的应用和生成式AI模型在帮助非专业用户和专业人士更高效地创建地图方面具有巨大潜力。

Conclusion: 研究结论指出，所开发的应用程序和生成式AI模型在提升地图制作效率方面具有广阔前景。同时，强调了制图师在推进AI辅助地图制作范式中的新角色，并提出了进一步的技术改进方向。

Abstract: Traditional map-making relies heavily on Geographic Information Systems
(GIS), requiring domain expertise and being time-consuming, especially for
repetitive tasks. Recent advances in generative AI (GenAI), particularly image
diffusion models, offer new opportunities for automating and democratizing the
map-making process. However, these models struggle with accurate map creation
due to limited control over spatial composition and semantic layout. To address
this, we integrate vector data to guide map generation in different styles,
specified by the textual prompts. Our model is the first to generate accurate
maps in controlled styles, and we have integrated it into a web application to
improve its usability and accessibility. We conducted a user study with
professional cartographers to assess the fidelity of generated maps, the
usability of the web application, and the implications of ever-emerging GenAI
in map-making. The findings have suggested the potential of our developed
application and, more generally, the GenAI models in helping both non-expert
users and professionals in creating maps more efficiently. We have also
outlined further technical improvements and emphasized the new role of
cartographers to advance the paradigm of AI-assisted map-making.

</details>


### [100] [Enhancing compact convolutional transformers with super attention](https://arxiv.org/abs/2508.18960)
*Simpenzwe Honore Leandre,Natenaile Asmamaw Shiferaw,Dillip Rout*

Main category: cs.CV

TL;DR: 本文提出了一种新的视觉模型，结合了token混合、序列池化和卷积tokenizer，在固定上下文长度任务中实现了最先进的性能和高效推理，特别是在CIFAR100基准上表现出色，且无需数据增强等复杂技术。


<details>
  <summary>Details</summary>
Motivation: 在固定上下文长度任务中，提升视觉模型的性能和推理效率，并减少对数据增强、位置嵌入或学习率调度等复杂训练技术的依赖。

Method: 该模型采用了以下核心组件：token混合（token mixing）、序列池化（sequence-pooling）和卷积tokenizer（convolutional tokenizers）。

Result: 在CIFAR100基准测试中，模型显著提高了验证准确率：Top 1%从36.50%提升到46.29%，Top 5%从66.33%提升到76.31%。当上下文长度小于嵌入维度时，该模型比Scaled Dot Product Attention (SDPA) 变换器更高效，且模型大小仅为其60%。此外，该架构训练稳定性高，不依赖mixup等数据增强、位置嵌入或学习率调度等技术。

Conclusion: 所提出的视觉模型在固定上下文长度任务中实现了卓越的性能和效率，并通过简化训练过程（无需多种常用技术）展示了其优越性。

Abstract: In this paper, we propose a vision model that adopts token mixing,
sequence-pooling, and convolutional tokenizers to achieve state-of-the-art
performance and efficient inference in fixed context-length tasks. In the
CIFAR100 benchmark, our model significantly improves the baseline of the top 1%
and top 5% validation accuracy from 36.50% to 46.29% and 66.33% to 76.31%,
while being more efficient than the Scaled Dot Product Attention (SDPA)
transformers when the context length is less than the embedding dimension and
only 60% the size. In addition, the architecture demonstrates high training
stability and does not rely on techniques such as data augmentation like mixup,
positional embeddings, or learning rate scheduling. We make our code available
on Github.

</details>


### [101] [USO: Unified Style and Subject-Driven Generation via Disentangled and Reward Learning](https://arxiv.org/abs/2508.18966)
*Shaojin Wu,Mengqi Huang,Yufeng Cheng,Wenxu Wu,Jiahe Tian,Yiming Luo,Fei Ding,Qian He*

Main category: cs.CV

TL;DR: 本文提出了USO模型，一个统一的风格-主体优化定制模型，旨在将风格驱动和主体驱动的生成任务整合到一个框架中，通过内容与风格的解耦实现卓越的风格相似性和主体一致性。


<details>
  <summary>Details</summary>
Motivation: 现有文献将风格驱动和主体驱动的生成视为两个独立的任务，前者侧重风格相似性，后者侧重主体一致性，导致两者之间存在明显冲突。本文认为这两个目标可以通过内容与风格的解耦和重组统一在一个框架下。

Method: 1. 构建了一个大规模的三元组数据集（内容图、风格图、风格化内容图）。2. 引入了一种解耦学习方案，通过风格对齐训练和内容-风格解耦训练两个互补目标，同时对齐风格特征并解耦内容与风格。3. 结合了风格奖励学习（SRL）范式以进一步提升模型性能。4. 发布了USO-Bench，首个联合评估风格相似性和主体保真度的基准测试。

Result: 广泛的实验表明，USO在主体一致性和风格相似性两个维度上，在开源模型中均达到了最先进的性能。

Conclusion: USO模型成功地将风格驱动和主体驱动的生成任务统一在一个框架下，通过创新的解耦学习和奖励机制，实现了风格相似性和主体一致性的卓越平衡，并提供了首个联合评估基准。

Abstract: Existing literature typically treats style-driven and subject-driven
generation as two disjoint tasks: the former prioritizes stylistic similarity,
whereas the latter insists on subject consistency, resulting in an apparent
antagonism. We argue that both objectives can be unified under a single
framework because they ultimately concern the disentanglement and
re-composition of content and style, a long-standing theme in style-driven
research. To this end, we present USO, a Unified Style-Subject Optimized
customization model. First, we construct a large-scale triplet dataset
consisting of content images, style images, and their corresponding stylized
content images. Second, we introduce a disentangled learning scheme that
simultaneously aligns style features and disentangles content from style
through two complementary objectives, style-alignment training and
content-style disentanglement training. Third, we incorporate a style
reward-learning paradigm denoted as SRL to further enhance the model's
performance. Finally, we release USO-Bench, the first benchmark that jointly
evaluates style similarity and subject fidelity across multiple metrics.
Extensive experiments demonstrate that USO achieves state-of-the-art
performance among open-source models along both dimensions of subject
consistency and style similarity. Code and model:
https://github.com/bytedance/USO

</details>


### [102] [RoofSeg: An edge-aware transformer-based network for end-to-end roof plane segmentation](https://arxiv.org/abs/2508.19003)
*Siyuan You,Guozheng Xu,Pengwei Zhou,Qiwen Jin,Jian Yao,Li Li*

Main category: cs.CV

TL;DR: 本研究提出了一种名为RoofSeg的边缘感知Transformer网络，用于从LiDAR点云中端到端地分割屋顶平面，旨在解决现有深度学习方法在非端到端、边缘特征辨别力低和几何特征考虑不足方面的问题。


<details>
  <summary>Details</summary>
Motivation: 屋顶平面分割是三维建筑模型（LoD 2和3）重建的关键步骤。然而，当前的深度学习方法存在三个未解决的问题：1) 大多数不是真正的端到端，可能导致平面分割结果不是最优；2) 边缘附近的点特征辨别力相对较低，导致平面边缘不准确；3) 未充分考虑平面几何特性来约束网络训练。

Method: 本研究开发了一种名为RoofSeg的边缘感知Transformer网络。该网络利用基于Transformer编码器-解码器的框架，通过一组可学习的平面查询分层预测平面实例掩模。为提高边缘区域的分割精度，设计了边缘感知掩模模块（EAMM），充分融入边缘的平面几何先验来增强其辨别力，以细化平面实例掩模。此外，提出了掩模损失中的自适应加权策略以减少错误分类点的影响，并提出了一种新的平面几何损失来约束网络训练。

Result: 通过开发端到端的RoofSeg网络，结合边缘感知模块和几何损失函数，本方法旨在有效解决现有屋顶平面分割方法中非端到端、边缘区域分割不准确以及几何特征约束不足的问题，从而提高从LiDAR点云中分割屋顶平面的精度和鲁棒性。

Conclusion: 本研究提出了一种新颖的端到端边缘感知Transformer网络RoofSeg，通过其独特的架构设计、边缘感知模块和几何损失函数，为从LiDAR点云中进行屋顶平面分割提供了一个全面的解决方案，有效克服了现有深度学习方法的局限性。

Abstract: Roof plane segmentation is one of the key procedures for reconstructing
three-dimensional (3D) building models at levels of detail (LoD) 2 and 3 from
airborne light detection and ranging (LiDAR) point clouds. The majority of
current approaches for roof plane segmentation rely on the manually designed or
learned features followed by some specifically designed geometric clustering
strategies. Because the learned features are more powerful than the manually
designed features, the deep learning-based approaches usually perform better
than the traditional approaches. However, the current deep learning-based
approaches have three unsolved problems. The first is that most of them are not
truly end-to-end, the plane segmentation results may be not optimal. The second
is that the point feature discriminability near the edges is relatively low,
leading to inaccurate planar edges. The third is that the planar geometric
characteristics are not sufficiently considered to constrain the network
training. To solve these issues, a novel edge-aware transformer-based network,
named RoofSeg, is developed for segmenting roof planes from LiDAR point clouds
in a truly end-to-end manner. In the RoofSeg, we leverage a transformer
encoder-decoder-based framework to hierarchically predict the plane instance
masks with the use of a set of learnable plane queries. To further improve the
segmentation accuracy of edge regions, we also design an Edge-Aware Mask Module
(EAMM) that sufficiently incorporates planar geometric prior of edges to
enhance its discriminability for plane instance mask refinement. In addition,
we propose an adaptive weighting strategy in the mask loss to reduce the
influence of misclassified points, and also propose a new plane geometric loss
to constrain the network training.

</details>


### [103] [Can we make NeRF-based visual localization privacy-preserving?](https://arxiv.org/abs/2508.18971)
*Maxime Pietrantoni,Martin Humenberger,Torsten Sattler,Gabriela Csurka*

Main category: cs.CV

TL;DR: 本文提出ppNeSF，一种基于自监督分割而非RGB图像训练的NeRF变体，旨在解决NeRF在视觉定位中存在的隐私泄露问题，并在保持定位精度的同时保护场景细节。


<details>
  <summary>Details</summary>
Motivation: NeRF在视觉定位中表现出色，但其会无意中编码场景的精细细节，这在云端定位服务中会引发隐私担忧，因为敏感信息可能被恢复。

Method: 1. 提出了一种评估基于NeRF表示的隐私保护能力的新协议。2. 提出了ppNeSF（Privacy-Preserving Neural Segmentation Field），一种NeRF变体，使用自监督学习的分割标签进行训练，而不是RGB图像，以确保标签足够粗糙以模糊可识别的场景细节，同时在3D中保持区分度。

Result: 1. 发现使用光度损失训练的NeRF即使移除了颜色预测头，其几何表示仍存储精细细节，容易受到隐私攻击。2. ppNeSF的分割空间可用于精确的视觉定位，并取得了最先进的结果。

Conclusion: NeRF在视觉定位中存在隐私泄露风险。ppNeSF通过利用自监督分割监督，成功地在保护场景隐私的同时，实现了高精度的视觉定位，为解决NeRF在云端服务中的隐私问题提供了有效方案。

Abstract: Visual localization (VL) is the task of estimating the camera pose in a known
scene. VL methods, a.o., can be distinguished based on how they represent the
scene, e.g., explicitly through a (sparse) point cloud or a collection of
images or implicitly through the weights of a neural network. Recently,
NeRF-based methods have become popular for VL. While NeRFs offer high-quality
novel view synthesis, they inadvertently encode fine scene details, raising
privacy concerns when deployed in cloud-based localization services as
sensitive information could be recovered. In this paper, we tackle this
challenge on two ends. We first propose a new protocol to assess
privacy-preservation of NeRF-based representations. We show that NeRFs trained
with photometric losses store fine-grained details in their geometry
representations, making them vulnerable to privacy attacks, even if the head
that predicts colors is removed. Second, we propose ppNeSF (Privacy-Preserving
Neural Segmentation Field), a NeRF variant trained with segmentation
supervision instead of RGB images. These segmentation labels are learned in a
self-supervised manner, ensuring they are coarse enough to obscure identifiable
scene details while remaining discriminativeness in 3D. The segmentation space
of ppNeSF can be used for accurate visual localization, yielding
state-of-the-art results.

</details>


### [104] [No Label Left Behind: A Unified Surface Defect Detection Model for all Supervision Regimes](https://arxiv.org/abs/2508.19060)
*Blaž Rolih,Matic Fučka,Danijel Skočaj*

Main category: cs.CV

TL;DR: SuperSimpleNet是一种高效且适应性强的判别模型，用于表面缺陷检测。它能处理无监督、弱监督、混合监督和全监督四种监督场景，并在性能和速度上树立了新标准，显著提升了工业应用的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有表面缺陷检测方法难以满足工业界对高性能、高效率和高适应性的需求，并且通常受限于特定的监督场景，难以适应真实制造过程中多样化的数据标注（如无监督、弱监督、混合监督和全监督）。

Method: SuperSimpleNet以SimpleNet为基础，并进行了改进。它引入了新颖的合成异常生成过程、增强的分类头部和改进的学习程序，使其能够在所有四种监督场景下进行高效训练，从而充分利用所有可用的数据标注。

Result: SuperSimpleNet在所有监督场景下都设定了新的性能标准，并在四个具有挑战性的基准数据集上得到了验证。此外，它还具有极高的推理速度，推理时间低于10毫秒。

Conclusion: SuperSimpleNet通过统一多样化的监督范式，同时保持卓越的速度和可靠性，代表着解决真实世界制造挑战和弥合学术研究与工业应用之间差距的重大进步。

Abstract: Surface defect detection is a critical task across numerous industries, aimed
at efficiently identifying and localising imperfections or irregularities on
manufactured components. While numerous methods have been proposed, many fail
to meet industrial demands for high performance, efficiency, and adaptability.
Existing approaches are often constrained to specific supervision scenarios and
struggle to adapt to the diverse data annotations encountered in real-world
manufacturing processes, such as unsupervised, weakly supervised, mixed
supervision, and fully supervised settings. To address these challenges, we
propose SuperSimpleNet, a highly efficient and adaptable discriminative model
built on the foundation of SimpleNet. SuperSimpleNet incorporates a novel
synthetic anomaly generation process, an enhanced classification head, and an
improved learning procedure, enabling efficient training in all four
supervision scenarios, making it the first model capable of fully leveraging
all available data annotations. SuperSimpleNet sets a new standard for
performance across all scenarios, as demonstrated by its results on four
challenging benchmark datasets. Beyond accuracy, it is very fast, achieving an
inference time below 10 ms. With its ability to unify diverse supervision
paradigms while maintaining outstanding speed and reliability, SuperSimpleNet
represents a promising step forward in addressing real-world manufacturing
challenges and bridging the gap between academic research and industrial
applications. Code: https://github.com/blaz-r/SuperSimpleNet

</details>


### [105] [Enhancing Document VQA Models via Retrieval-Augmented Generation](https://arxiv.org/abs/2508.18984)
*Eric López,Artemis Llabrés,Ernest Valveny*

Main category: cs.CV

TL;DR: 本文系统评估了将检索增强生成（RAG）整合到多页文档视觉问答（Document VQA）中的效果，通过文本和视觉检索变体，证明了精心选择证据能显著提高准确性并解决内存消耗问题。


<details>
  <summary>Details</summary>
Motivation: 现有文档VQA系统在处理多页文档时，要么拼接所有页面，要么依赖大型视觉-语言模型，这两种方法都非常占用内存。RAG作为一种先检索相关片段再生成答案的替代方案，具有吸引力。

Method: 研究通过不同检索变体（基于OCR文本的检索和纯视觉检索）在多个模型和基准（MP-DocVQA、DUDE、InfographicVQA）上，系统评估了RAG对Document VQA的影响。还进行了消融实验，分析检索、重排和布局引导分块策略的贡献。

Result: 文本中心检索变体将“拼接所有页面”的基线性能提高了高达+22.5 ANLS，而视觉检索变体在无需文本提取的情况下实现了+5.0 ANLS的提升。消融实验证实，检索和重排组件是性能提升的主要驱动因素，而布局引导的分块策略在这些数据集中未能提供帮助。实验表明，仔细的证据选择能够持续提高多模型尺寸和多页基准的准确性。

Conclusion: RAG，特别是通过精心选择证据，对于实际应用中的Document VQA具有实用价值，能显著提高多页文档的准确性，并有效解决内存消耗问题。

Abstract: Document Visual Question Answering (Document VQA) must cope with documents
that span dozens of pages, yet leading systems still concatenate every page or
rely on very large vision-language models, both of which are memory-hungry.
Retrieval-Augmented Generation (RAG) offers an attractive alternative, first
retrieving a concise set of relevant segments before generating answers from
this selected evidence. In this paper, we systematically evaluate the impact of
incorporating RAG into Document VQA through different retrieval variants -
text-based retrieval using OCR tokens and purely visual retrieval without OCR -
across multiple models and benchmarks. Evaluated on the multi-page datasets
MP-DocVQA, DUDE, and InfographicVQA, the text-centric variant improves the
"concatenate-all-pages" baseline by up to +22.5 ANLS, while the visual variant
achieves +5.0 ANLS improvement without requiring any text extraction. An
ablation confirms that retrieval and reranking components drive most of the
gain, whereas the layout-guided chunking strategy - proposed in several recent
works to leverage page structure - fails to help on these datasets. Our
experiments demonstrate that careful evidence selection consistently boosts
accuracy across multiple model sizes and multi-page benchmarks, underscoring
its practical value for real-world Document VQA.

</details>


### [106] [Few-Shot Connectivity-Aware Text Line Segmentation in Historical Documents](https://arxiv.org/abs/2508.19162)
*Rafael Sterzinger,Tingyu Lin,Robert Sablatnig*

Main category: cs.CV

TL;DR: 本文提出了一种针对历史文档的少样本文本行分割方法，通过结合轻量级网络和拓扑感知损失函数，显著提高了数据效率和分割精度。


<details>
  <summary>Details</summary>
Motivation: 历史文档的文本行分割面临两大挑战：一是缺乏大型标注数据集，二是标注过程耗时耗力且需要专业知识。因此，少样本学习成为减少数据需求的一个有前景的方向。

Method: 研究者采用了一个轻量级的UNet++网络，并结合了一种连通性感知（拓扑感知）损失函数（最初用于神经元形态学），该函数明确惩罚线条碎片化和意外合并等结构错误。为应对数据限制，模型仅使用每个手稿三页标注页面的小补丁进行训练。

Result: 该方法在U-DIADS-TL数据集上取得了显著提升，识别准确率提高了200%，线条交并比提高了75%。此外，其F-Measure分数与DIVA-HisDB基线检测任务的竞赛冠军持平或超越，且仅需三页标注页面。

Conclusion: 小型简单的网络架构结合拓扑感知损失函数，在少样本学习场景下，对于历史文档的文本行分割任务，比复杂模型更准确、数据效率更高。

Abstract: A foundational task for the digital analysis of documents is text line
segmentation. However, automating this process with deep learning models is
challenging because it requires large, annotated datasets that are often
unavailable for historical documents. Additionally, the annotation process is a
labor- and cost-intensive task that requires expert knowledge, which makes
few-shot learning a promising direction for reducing data requirements. In this
work, we demonstrate that small and simple architectures, coupled with a
topology-aware loss function, are more accurate and data-efficient than more
complex alternatives. We pair a lightweight UNet++ with a connectivity-aware
loss, initially developed for neuron morphology, which explicitly penalizes
structural errors like line fragmentation and unintended line merges. To
increase our limited data, we train on small patches extracted from a mere
three annotated pages per manuscript. Our methodology significantly improves
upon the current state-of-the-art on the U-DIADS-TL dataset, with a 200%
increase in Recognition Accuracy and a 75% increase in Line Intersection over
Union. Our method also achieves an F-Measure score on par with or even
exceeding that of the competition winner of the DIVA-HisDB baseline detection
task, all while requiring only three annotated pages, exemplifying the efficacy
of our approach. Our implementation is publicly available at:
https://github.com/RafaelSterzinger/acpr_few_shot_hist.

</details>


### [107] [Ask Me Again Differently: GRAS for Measuring Bias in Vision Language Models on Gender, Race, Age, and Skin Tone](https://arxiv.org/abs/2508.18989)
*Shaivi Malik,Hasnat Md Abdullah,Sriparna Saha,Amit Sheth*

Main category: cs.CV

TL;DR: 该研究引入了GRAS基准和GRAS偏见分数，用于揭示和量化视觉语言模型（VLMs）在性别、种族、年龄和肤色方面的偏见，并发现现有VLMs存在显著偏见。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLMs）已成为现实世界应用不可或缺的一部分，因此理解其人口统计学偏见至关重要。

Method: 引入了GRAS基准，涵盖性别、种族、年龄和肤色，以揭示VLMs的人口统计学偏见。提出了GRAS偏见分数，一个可解释的量化偏见的指标。对五种最先进的VLMs进行了基准测试。发现使用视觉问答（VQA）评估VLMs偏见时，需要考虑问题的多种表述。

Result: 揭示了令人担忧的偏见水平，即使偏见最小的模型也仅获得2/100的GRAS偏见分数。研究还发现，在VQA中评估偏见需要考虑问题的多种表述。

Conclusion: 当前最先进的视觉语言模型存在显著的人口统计学偏见。GRAS基准和GRAS偏见分数提供了量化这些偏见的有效工具，并提出了在VQA偏见评估中考虑多种问题表述的方法学见解。

Abstract: As Vision Language Models (VLMs) become integral to real-world applications,
understanding their demographic biases is critical. We introduce GRAS, a
benchmark for uncovering demographic biases in VLMs across gender, race, age,
and skin tone, offering the most diverse coverage to date. We further propose
the GRAS Bias Score, an interpretable metric for quantifying bias. We benchmark
five state-of-the-art VLMs and reveal concerning bias levels, with the least
biased model attaining a GRAS Bias Score of only 2 out of 100. Our findings
also reveal a methodological insight: evaluating bias in VLMs with visual
question answering (VQA) requires considering multiple formulations of a
question. Our code, data, and evaluation results are publicly available.

</details>


### [108] [LSD-3D: Large-Scale 3D Driving Scene Generation with Geometry Grounding](https://arxiv.org/abs/2508.19204)
*Julian Ost,Andrea Ramazzina,Amogh Joshi,Maximilian Bömer,Mario Bijelic,Felix Heide*

Main category: cs.CV

TL;DR: 本文提出了一种结合代理几何生成和2D图像先验分数蒸馏的方法，可以直接生成具有精确几何、对象持久性和因果关系的大规模3D驾驶场景，以实现可控的新颖视图合成。


<details>
  <summary>Details</summary>
Motivation: 现有的神经重建方法生成的是静态环境，受限于捕获数据，场景和轨迹多样性有限。而图像/视频扩散模型虽然提供了控制能力，但缺乏几何基础和因果关系。本研究旨在弥合这一鸿沟，生成具有准确几何和因果关系的大规模3D驾驶数据。

Method: 所提出的方法结合了代理几何和环境表示的生成，并利用从学习到的2D图像先验中进行分数蒸馏。这使得能够通过提示词引导几何，并生成可基于地图布局的高保真纹理和结构。

Result: 该方法实现了高度可控性，能够通过提示词引导几何生成，并产生可基于地图布局的高保真纹理和结构。最终生成了逼真且几何一致的复杂驾驶场景3D模型。

Conclusion: 该方法成功弥合了现有技术的不足，实现了对大规模3D驾驶场景的直接生成，具有精确的几何、对象持久性和因果关系，并支持高度可控的生成过程。

Abstract: Large-scale scene data is essential for training and testing in robot
learning. Neural reconstruction methods have promised the capability of
reconstructing large physically-grounded outdoor scenes from captured sensor
data. However, these methods have baked-in static environments and only allow
for limited scene control -- they are functionally constrained in scene and
trajectory diversity by the captures from which they are reconstructed. In
contrast, generating driving data with recent image or video diffusion models
offers control, however, at the cost of geometry grounding and causality. In
this work, we aim to bridge this gap and present a method that directly
generates large-scale 3D driving scenes with accurate geometry, allowing for
causal novel view synthesis with object permanence and explicit 3D geometry
estimation. The proposed method combines the generation of a proxy geometry and
environment representation with score distillation from learned 2D image
priors. We find that this approach allows for high controllability, enabling
the prompt-guided geometry and high-fidelity texture and structure that can be
conditioned on map layouts -- producing realistic and geometrically consistent
3D generations of complex driving scenes.

</details>


### [109] [MicroDetect-Net (MDN): Leveraging Deep Learning to Detect Microplastics in Clam Blood, a Step Towards Human Blood Analysis](https://arxiv.org/abs/2508.19021)
*Riju Marwah,Riya Arora,Navneet Yadav,Himank Arora*

Main category: cs.CV

TL;DR: 本文提出了一种名为 MicroDetect-Net (MDN) 的深度学习模型，结合尼罗红染色荧光显微镜技术，用于在血样中检测和量化微塑料，并在蛤蜊血样数据集上取得了高精度。


<details>
  <summary>Details</summary>
Motivation: 微塑料污染日益严重，每年塑料产量超过3.68亿吨，已广泛存在于空气、水、土壤和生物体中。微塑料对人类健康有害，可能导致肝脏感染、肠道损伤和肠道菌群失衡。因此，开发有效的微塑料检测方法至关重要。

Method: 研究采用尼罗红染料染色荧光显微镜技术获取血样图像。然后，利用一个名为 MicroDetect-Net (MDN) 的深度学习模型，该模型整合了数据集准备、荧光成像和使用卷积神经网络进行分割，以定位和计数微塑料碎片。该方法在276张尼罗红染色的荧光血样图像数据集上进行了评估。

Result: MDN 模型在276张尼罗红染色的荧光血样图像数据集上实现了92%的准确率。其性能指标包括：交并比 (IoU) 为87.4%，F1 分数为92.1%，精确度为90.6%，召回率为93.7%。这些指标表明 MDN 在微塑料检测方面表现出色。

Conclusion: MDN 模型结合卷积网络和尼罗红染料进行分割，在微塑料图像检测和精度方面表现出强大性能，有效证明了其在检测微塑料方面的有效性。尽管本研究使用了蛤蜊血样，但该方法为未来应用于人类样本检测微塑料开辟了途径。

Abstract: With the prevalence of plastics exceeding 368 million tons yearly,
microplastic pollution has grown to an extent where air, water, soil, and
living organisms have all tested positive for microplastic presence. These
particles, which are smaller than 5 millimeters in size, are no less harmful to
humans than to the environment. Toxicity research on microplastics has shown
that exposure may cause liver infection, intestinal injuries, and gut flora
imbalance, leading to numerous potential health hazards. This paper presents a
new model, MicroDetect-Net (MDN), which applies fluorescence microscopy with
Nile Red dye staining and deep learning to scan blood samples for
microplastics. Although clam blood has certain limitations in replicating real
human blood, this study opens avenues for applying the approach to human
samples, which are more consistent for preliminary data collection. The MDN
model integrates dataset preparation, fluorescence imaging, and segmentation
using a convolutional neural network to localize and count microplastic
fragments. The combination of convolutional networks and Nile Red dye for
segmentation produced strong image detection and accuracy. MDN was evaluated on
a dataset of 276 Nile Red-stained fluorescent blood images and achieved an
accuracy of ninety two percent. Robust performance was observed with an
Intersection over Union of 87.4 percent, F1 score of 92.1 percent, Precision of
90.6 percent, and Recall of 93.7 percent. These metrics demonstrate the
effectiveness of MDN in the detection of microplastics.

</details>


### [110] [ProPy: Building Interactive Prompt Pyramids upon CLIP for Partially Relevant Video Retrieval](https://arxiv.org/abs/2508.19024)
*Yi Pan,Yujia Zhang,Michael Kampffmeyer,Xiaoguang Zhao*

Main category: cs.CV

TL;DR: 本文提出ProPy模型，通过系统性地调整CLIP架构以适应部分相关视频检索（PRVR）任务，利用多粒度事件提示金字塔和祖先-后代交互机制，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 部分相关视频检索（PRVR）是一个实用但具有挑战性的任务，现有方法主要处理单模态特征。强大的预训练视觉-语言模型（如CLIP）在该领域尚未得到充分探索，因此需要弥合这一差距。

Method: 本文提出了ProPy模型，该模型对CLIP进行了系统性架构调整以专门用于PRVR。它引入了两项关键创新：1) 一个Prompt Pyramid结构，用于组织事件提示以捕获多粒度语义；2) 一个基于金字塔构建的祖先-后代交互机制，实现事件间的动态语义交互。

Result: ProPy在三个公共数据集上取得了最先进的性能，显著超越了现有模型。

Conclusion: ProPy通过对CLIP进行创新的架构适应，特别是引入多粒度提示金字塔和动态交互机制，有效解决了PRVR任务中的语义捕获问题，并取得了卓越的性能，证明了其在PRVR领域的有效性。

Abstract: Partially Relevant Video Retrieval (PRVR) is a practical yet challenging task
that involves retrieving videos based on queries relevant to only specific
segments. While existing works follow the paradigm of developing models to
process unimodal features, powerful pretrained vision-language models like CLIP
remain underexplored in this field. To bridge this gap, we propose ProPy, a
model with systematic architectural adaption of CLIP specifically designed for
PRVR. Drawing insights from the semantic relevance of multi-granularity events,
ProPy introduces two key innovations: (1) A Prompt Pyramid structure that
organizes event prompts to capture semantics at multiple granularity levels,
and (2) An Ancestor-Descendant Interaction Mechanism built on the pyramid that
enables dynamic semantic interaction among events. With these designs, ProPy
achieves SOTA performance on three public datasets, outperforming previous
models by significant margins. Code is available at
https://github.com/BUAAPY/ProPy.

</details>


### [111] [GReAT: leveraging geometric artery data to improve wall shear stress assessment](https://arxiv.org/abs/2508.19030)
*Julian Suk,Jolanda J. Wentzel,Patryk Rygiel,Joost Daemen,Daniel Rueckert,Jelmer M. Wolterink*

Main category: cs.CV

TL;DR: 该研究利用大规模三维血管几何模型进行自监督预训练，以解决数据稀缺问题，从而提高冠状动脉壁面剪应力（WSS）评估的准确性，即使在有限的临床数据下也能有效分割WSS区域。


<details>
  <summary>Details</summary>
Motivation: 在心血管健康等领域，利用大数据通过机器学习算法从医学图像评估血流动力学生物标志物（如壁面剪应力）具有巨大潜力，可避免耗时的计算流体模拟。然而，训练此类模型面临数据量不足的挑战。此外，在冠状动脉背景下，利用学习到的表征来改善血流动力学生物标志物评估尚未得到充分研究。

Method: 研究利用一个包含8449个三维血管几何模型的大规模数据集进行自监督预训练。通过计算热核签名（Heat Kernel Signature，一种通过拉普拉斯特征向量获得并捕捉形状本质的量）作为自监督目标。然后，将从该数据集学习到的几何表征应用于一个来自小型临床试验（49名患者）的冠状动脉模型，用于将冠状动脉分割为低、中、高（时间平均）壁面剪应力区域。

Result: 研究结果表明，即使在有限的训练数据下，从大规模几何数据集中学习到的几何表征也能显著提升冠状动脉壁面剪应力区域的分割性能。

Conclusion: 大规模三维血管几何模型的自监督预训练能够有效解决数据稀缺问题，并通过学习到的几何表征，显著提高冠状动脉壁面剪应力评估的准确性，尤其是在临床数据有限的情况下。

Abstract: Leveraging big data for patient care is promising in many medical fields such
as cardiovascular health. For example, hemodynamic biomarkers like wall shear
stress could be assessed from patient-specific medical images via machine
learning algorithms, bypassing the need for time-intensive computational fluid
simulation. However, it is extremely challenging to amass large-enough datasets
to effectively train such models. We could address this data scarcity by means
of self-supervised pre-training and foundations models given large datasets of
geometric artery models. In the context of coronary arteries, leveraging
learned representations to improve hemodynamic biomarker assessment has not yet
been well studied. In this work, we address this gap by investigating whether a
large dataset (8449 shapes) consisting of geometric models of 3D blood vessels
can benefit wall shear stress assessment in coronary artery models from a
small-scale clinical trial (49 patients). We create a self-supervised target
for the 3D blood vessels by computing the heat kernel signature, a quantity
obtained via Laplacian eigenvectors, which captures the very essence of the
shapes. We show how geometric representations learned from this datasets can
boost segmentation of coronary arteries into regions of low, mid and high
(time-averaged) wall shear stress even when trained on limited data.

</details>


### [112] [Learning Binary Sampling Patterns for Single-Pixel Imaging using Bilevel Optimisation](https://arxiv.org/abs/2508.19068)
*Serban C. Tudosie,Alexander Denker,Zeljko Kereta,Simon Arridge*

Main category: cs.CV

TL;DR: 该研究提出一种双层优化方法，用于学习单像素成像中任务特定的二值照明模式，通过处理二值模式的不可微性并引入正则化器，在高度欠采样情况下实现了优于基线方法的重建性能，特别适用于荧光显微镜等应用。


<details>
  <summary>Details</summary>
Motivation: 单像素成像通过结构光模式序列照明实现物体重建，但需要优化照明模式以提高性能。研究旨在开发一种方法，能够为特定任务（如单像素荧光显微镜）学习到高效、二值的照明模式。

Method: 提出了一种双层优化方法来学习任务特定的二值照明模式。为解决二值模式优化中的不可微性问题，采用了Straight-Through Estimator。同时，在双层公式中引入了Total Deep Variation正则化器。

Result: 在CytoImageNet显微镜数据集上验证了所提出的方法。结果表明，学习到的模式在重建性能上优于基线方法，尤其是在高度欠采样的情况下表现出显著优势。

Conclusion: 该方法成功地学习了任务特定的二值照明模式，显著提升了单像素成像的重建性能，特别是在数据采集受限（高度欠采样）的应用场景中展现出卓越的潜力。

Abstract: Single-Pixel Imaging enables reconstructing objects using a single detector
through sequential illuminations with structured light patterns. We propose a
bilevel optimisation method for learning task-specific, binary illumination
patterns, optimised for applications like single-pixel fluorescence microscopy.
We address the non-differentiable nature of binary pattern optimisation using
the Straight-Through Estimator and leveraging a Total Deep Variation
regulariser in the bilevel formulation. We demonstrate our method on the
CytoImageNet microscopy dataset and show that learned patterns achieve superior
reconstruction performance compared to baseline methods, especially in highly
undersampled regimes.

</details>


### [113] [Dual Enhancement on 3D Vision-Language Perception for Monocular 3D Visual Grounding](https://arxiv.org/abs/2508.19165)
*Yuzhen Li,Min Liu,Yuan Bian,Xueping Wang,Zhaoyang Li,Gen Li,Yaonan Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种单目3D视觉定位方法，通过增强文本嵌入和几何特征的3D感知能力，解决了预训练语言模型对文本中几何信息（特别是度量单位）理解不足的问题，实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 研究发现，在单目3D视觉定位任务中，文本嵌入对数值大小敏感但忽略了相关的度量单位（如米、分米、厘米），导致物理长度等效时性能显著下降。这表明预训练语言模型在3D理解方面存在弱点，产生了误导性的文本特征，阻碍了3D感知。

Method: 提出了两种简单有效的方法：1. **3D-text Enhancement (3DTE)**：一种预处理方法，通过增加文本查询中距离描述符的多样性，增强模型对不同单位之间映射关系的理解。2. **Text-Guided Geometry Enhancement (TGE)**：一个模块，通过将基本文本特征投影到几何一致的空间中，进一步增强3D-文本信息，并利用这些增强后的3D文本特征精确引导几何特征的注意力。

Result: 在Mono3DRefer数据集上进行了广泛的比较和消融研究。实验结果表明，该方法比现有方法有显著改进，取得了新的最先进成果，尤其在“远距离”场景中精度提升了11.94%。

Conclusion: 所提出的3D-text Enhancement (3DTE)和Text-Guided Geometry Enhancement (TGE)方法能够有效增强模型对文本嵌入和几何特征的3D感知能力，解决了预训练语言模型在处理带有几何信息的文本时对度量单位理解不足的问题，从而显著提升了单目3D视觉定位的性能。

Abstract: Monocular 3D visual grounding is a novel task that aims to locate 3D objects
in RGB images using text descriptions with explicit geometry information.
Despite the inclusion of geometry details in the text, we observe that the text
embeddings are sensitive to the magnitude of numerical values but largely
ignore the associated measurement units. For example, simply equidistant
mapping the length with unit "meter" to "decimeters" or "centimeters" leads to
severe performance degradation, even though the physical length remains
equivalent. This observation signifies the weak 3D comprehension of pre-trained
language model, which generates misguiding text features to hinder 3D
perception. Therefore, we propose to enhance the 3D perception of model on text
embeddings and geometry features with two simple and effective methods.
Firstly, we introduce a pre-processing method named 3D-text Enhancement (3DTE),
which enhances the comprehension of mapping relationships between different
units by augmenting the diversity of distance descriptors in text queries.
Next, we propose a Text-Guided Geometry Enhancement (TGE) module to further
enhance the 3D-text information by projecting the basic text features into
geometrically consistent space. These 3D-enhanced text features are then
leveraged to precisely guide the attention of geometry features. We evaluate
the proposed method through extensive comparisons and ablation studies on the
Mono3DRefer dataset. Experimental results demonstrate substantial improvements
over previous methods, achieving new state-of-the-art results with a notable
accuracy gain of 11.94\% in the "Far" scenario. Our code will be made publicly
available.

</details>


### [114] [Beyond flattening: a geometrically principled positional encoding for vision transformers with Weierstrass elliptic functions](https://arxiv.org/abs/2508.19167)
*Zhihang Xin,Xitong Hu,Rui Wang*

Main category: cs.CV

TL;DR: 本文提出了一种基于魏尔斯特拉斯椭圆函数的位置编码（WEF-PE），用于解决Vision Transformers中一维位置编码破坏二维空间结构的问题。WEF-PE利用复数域和椭圆函数的双周期性，有效编码二维空间距离关系，并在多项任务中取得了卓越性能提升。


<details>
  <summary>Details</summary>
Motivation: Vision Transformers中可学习的一维位置嵌入通过图像块展平过程，从根本上破坏了图像固有的二维空间结构。传统的位置编码缺乏几何约束，未能建立欧几里得空间距离与序列索引距离之间的单调对应关系，从而限制了模型有效利用空间邻近先验的能力。

Method: 本文提出了魏尔斯特拉斯椭圆函数位置编码（WEF-PE）。该方法通过自然的复数域表示直接处理二维坐标，其中椭圆函数的双周期性与视觉数据中常见的平移不变性模式高度吻合。WEF-PE利用椭圆函数的非线性几何特性自然地编码空间距离关系，其代数加法公式能够从绝对编码中直接推导出任意图像块对之间的相对位置信息。

Result: WEF-PE在多种场景下均取得了卓越性能：在ViT-Tiny架构下CIFAR-100从头训练达到63.78%的准确率，在ViT-Base架构下CIFAR-100微调达到93.28%的准确率，并在VTAB-1k基准任务中持续改进。理论分析通过严格的数学证明证实了距离衰减特性，而注意力可视化揭示了相比传统方法，WEF-PE增强了几何归纳偏置和更连贯的语义焦点。

Conclusion: 魏尔斯特拉斯椭圆函数位置编码（WEF-PE）是一种数学上合理的方法，它通过直接处理二维坐标和利用椭圆函数的特性，有效解决了Vision Transformers中位置编码对空间结构破坏的问题，显著提升了模型性能、几何归纳偏置和语义聚焦能力。

Abstract: Vision Transformers have demonstrated remarkable success in computer vision
tasks, yet their reliance on learnable one-dimensional positional embeddings
fundamentally disrupts the inherent two-dimensional spatial structure of images
through patch flattening procedures. Traditional positional encoding approaches
lack geometric constraints and fail to establish monotonic correspondence
between Euclidean spatial distances and sequential index distances, thereby
limiting the model's capacity to leverage spatial proximity priors effectively.
We propose Weierstrass Elliptic Function Positional Encoding (WEF-PE), a
mathematically principled approach that directly addresses two-dimensional
coordinates through natural complex domain representation, where the doubly
periodic properties of elliptic functions align remarkably with translational
invariance patterns commonly observed in visual data. Our method exploits the
non-linear geometric nature of elliptic functions to encode spatial distance
relationships naturally, while the algebraic addition formula enables direct
derivation of relative positional information between arbitrary patch pairs
from their absolute encodings. Comprehensive experiments demonstrate that
WEF-PE achieves superior performance across diverse scenarios, including
63.78\% accuracy on CIFAR-100 from-scratch training with ViT-Tiny architecture,
93.28\% on CIFAR-100 fine-tuning with ViT-Base, and consistent improvements on
VTAB-1k benchmark tasks. Theoretical analysis confirms the distance-decay
property through rigorous mathematical proof, while attention visualization
reveals enhanced geometric inductive bias and more coherent semantic focus
compared to conventional approaches.The source code implementing the methods
described in this paper is publicly available on GitHub.

</details>


### [115] [SoccerNet 2025 Challenges Results](https://arxiv.org/abs/2508.19182)
*Silvio Giancola,Anthony Cioppa,Marc Gutiérrez-Pérez,Jan Held,Carlos Hinojosa,Victor Joos,Arnaud Leduc,Floriane Magera,Karen Sanchez,Vladimir Somers,Artur Xarles,Antonio Agudo,Alexandre Alahi,Olivier Barnich,Albert Clapés,Christophe De Vleeschouwer,Sergio Escalera,Bernard Ghanem,Thomas B. Moeslund,Marc Van Droogenbroeck,Tomoki Abe,Saad Alotaibi,Faisal Altawijri,Steven Araujo,Xiang Bai,Xiaoyang Bi,Jiawang Cao,Vanyi Chao,Kamil Czarnogórski,Fabian Deuser,Mingyang Du,Tianrui Feng,Patrick Frenzel,Mirco Fuchs,Jorge García,Konrad Habel,Takaya Hashiguchi,Sadao Hirose,Xinting Hu,Yewon Hwang,Ririko Inoue,Riku Itsuji,Kazuto Iwai,Hongwei Ji,Yangguang Ji,Licheng Jiao,Yuto Kageyama,Yuta Kamikawa,Yuuki Kanasugi,Hyungjung Kim,Jinwook Kim,Takuya Kurihara,Bozheng Li,Lingling Li,Xian Li,Youxing Lian,Dingkang Liang,Hongkai Lin,Jiadong Lin,Jian Liu,Liang Liu,Shuaikun Liu,Zhaohong Liu,Yi Lu,Federico Méndez,Huadong Ma,Wenping Ma,Jacek Maksymiuk,Henry Mantilla,Ismail Mathkour,Daniel Matthes,Ayaha Motomochi,Amrulloh Robbani Muhammad,Haruto Nakayama,Joohyung Oh,Yin May Oo,Marcelo Ortega,Norbert Oswald,Rintaro Otsubo,Fabian Perez,Mengshi Qi,Cristian Rey,Abel Reyes-Angulo,Oliver Rose,Hoover Rueda-Chacón,Hideo Saito,Jose Sarmiento,Kanta Sawafuji,Atom Scott,Xi Shen,Pragyan Shrestha,Jae-Young Sim,Long Sun,Yuyang Sun,Tomohiro Suzuki,Licheng Tang,Masato Tonouchi,Ikuma Uchida,Henry O. Velesaca,Tiancheng Wang,Rio Watanabe,Jay Wu,Yongliang Wu,Shunzo Yamagishi,Di Yang,Xu Yang,Yuxin Yang,Hao Ye,Xinyu Ye,Calvin Yeung,Xuanlong Yu,Chao Zhang,Dingyuan Zhang,Kexing Zhang,Zhe Zhao,Xin Zhou,Wenbo Zhu,Julian Ziegler*

Main category: cs.CV

TL;DR: SoccerNet 2025挑战赛是其第五届年度开放基准测试，专注于足球视频理解中的计算机视觉研究。挑战赛涵盖四个任务：团队控球动作识别、单目深度估计、多视角犯规识别和比赛状态重建。本报告总结了挑战赛结果、顶尖解决方案和社区进展。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在推动足球视频理解领域的计算机视觉研究发展，通过提供开放基准测试和大规模标注数据集，解决足球广播视频中复杂的视觉分析任务。

Method: 设立了四个主要的视觉任务：(1) 团队控球动作识别，用于检测球相关动作并分配给团队；(2) 单目深度估计，从单摄像头片段恢复场景几何；(3) 多视角犯规识别，分析多视角以分类犯规及其严重性；(4) 比赛状态重建，定位和识别所有球员以在2D俯视图上重建比赛状态。为参与者提供了大规模标注数据集、统一评估协议和强大的基线。

Result: 本报告展示了各项挑战赛的结果，重点介绍了表现最佳的解决方案，并提供了社区在足球视频理解领域取得进展的见解。

Conclusion: SoccerNet挑战赛持续作为推动计算机视觉、人工智能和体育交叉领域可复现、开放研究的重要力量。

Abstract: The SoccerNet 2025 Challenges mark the fifth annual edition of the SoccerNet
open benchmarking effort, dedicated to advancing computer vision research in
football video understanding. This year's challenges span four vision-based
tasks: (1) Team Ball Action Spotting, focused on detecting ball-related actions
in football broadcasts and assigning actions to teams; (2) Monocular Depth
Estimation, targeting the recovery of scene geometry from single-camera
broadcast clips through relative depth estimation for each pixel; (3)
Multi-View Foul Recognition, requiring the analysis of multiple synchronized
camera views to classify fouls and their severity; and (4) Game State
Reconstruction, aimed at localizing and identifying all players from a
broadcast video to reconstruct the game state on a 2D top-view of the field.
Across all tasks, participants were provided with large-scale annotated
datasets, unified evaluation protocols, and strong baselines as starting
points. This report presents the results of each challenge, highlights the
top-performing solutions, and provides insights into the progress made by the
community. The SoccerNet Challenges continue to serve as a driving force for
reproducible, open research at the intersection of computer vision, artificial
intelligence, and sports. Detailed information about the tasks, challenges, and
leaderboards can be found at https://www.soccer-net.org, with baselines and
development kits available at https://github.com/SoccerNet.

</details>


### [116] [FastMesh:Efficient Artistic Mesh Generation via Component Decoupling](https://arxiv.org/abs/2508.19188)
*Jeonghwan Kim,Yushi Lan,Armando Fortes,Yongwei Chen,Xingang Pan*

Main category: cs.CV

TL;DR: 该论文提出了一种高效的艺术网格生成框架，通过分离顶点和面来减少冗余，显著缩短了token序列长度，并利用自回归模型、双向Transformer和精细化步骤，实现了比现有方法快8倍以上的生成速度和更高的网格质量。


<details>
  <summary>Details</summary>
Motivation: 现有网格生成方法将网格标记化为序列，但由于顶点在多个面中共享，导致token序列冗余且过长，从而降低了生成效率。

Method: 该方法将顶点和面分开处理。首先，使用自回归模型仅生成顶点，将token数量减少至现有最紧凑分词器的23%。其次，利用双向Transformer一步完成网格生成，通过捕捉顶点间关系构建定义网格面的邻接矩阵。此外，引入保真度增强器以优化顶点位置，并提出后处理框架以去除不良边缘连接。

Result: 实验结果表明，该方法在网格生成速度上比最先进的方法快8倍以上，同时能产生更高质量的网格。

Conclusion: 通过分离顶点和面、减少冗余以及采用多阶段生成和优化策略，该框架显著提高了艺术网格生成的速度和质量。

Abstract: Recent mesh generation approaches typically tokenize triangle meshes into
sequences of tokens and train autoregressive models to generate these tokens
sequentially. Despite substantial progress, such token sequences inevitably
reuse vertices multiple times to fully represent manifold meshes, as each
vertex is shared by multiple faces. This redundancy leads to excessively long
token sequences and inefficient generation processes. In this paper, we propose
an efficient framework that generates artistic meshes by treating vertices and
faces separately, significantly reducing redundancy. We employ an
autoregressive model solely for vertex generation, decreasing the token count
to approximately 23\% of that required by the most compact existing tokenizer.
Next, we leverage a bidirectional transformer to complete the mesh in a single
step by capturing inter-vertex relationships and constructing the adjacency
matrix that defines the mesh faces. To further improve the generation quality,
we introduce a fidelity enhancer to refine vertex positioning into more natural
arrangements and propose a post-processing framework to remove undesirable edge
connections. Experimental results show that our method achieves more than
8$\times$ faster speed on mesh generation compared to state-of-the-art
approaches, while producing higher mesh quality.

</details>


### [117] [All-in-One Slider for Attribute Manipulation in Diffusion Models](https://arxiv.org/abs/2508.19195)
*Weixin Ye,Hongguang Zhu,Wei Wang,Yahui Liu,Mengyu Wang*

Main category: cs.CV

TL;DR: 本文提出“一体化滑块”（All-in-One Slider），一个轻量级模块，通过分解文本嵌入空间，实现对文生图模型中图像属性的精确、可扩展且零样本控制，解决了现有方法效率低、参数冗余的问题。


<details>
  <summary>Details</summary>
Motivation: 文生图模型在生成高质量图像方面表现出色，但对生成图像的特定属性进行渐进式操控（尤其是细节丰富的图像如人脸）仍然具有挑战。现有方法通常为每个属性训练一个独立的滑块（One-for-One），导致参数冗余，限制了实际应用的灵活性和属性操控的可扩展性，且每次引入新属性都需要额外训练。

Method: 本文引入了“一体化滑块”（All-in-One Slider），这是一个轻量级模块，它将文本嵌入空间分解为稀疏且具有语义意义的属性方向。一旦训练完成，它就能作为一个通用滑块，对各种属性进行可解释、细粒度的连续控制。此外，通过重新组合学习到的方向，该滑块支持对未见属性（如种族和名人）的零样本操控，以及多个属性的组合。

Result: 广泛的实验表明，该方法能够实现准确且可扩展的属性操控，与现有方法相比有显著改进。此外，该方法还可以扩展与反演框架集成，对真实图像执行属性操控，从而拓宽了其在各种现实场景中的适用性。

Conclusion: “一体化滑块”提供了一种高效、灵活且可扩展的解决方案，用于文生图模型中的属性操控，解决了现有方法的参数冗余和可扩展性问题。它不仅能实现对已知属性的精细控制，还支持零样本操控和多属性组合，并可应用于真实图像，具有广泛的应用前景。

Abstract: Text-to-image (T2I) diffusion models have made significant strides in
generating high-quality images. However, progressively manipulating certain
attributes of generated images to meet the desired user expectations remains
challenging, particularly for content with rich details, such as human faces.
Some studies have attempted to address this by training slider modules.
However, they follow a One-for-One manner, where an independent slider is
trained for each attribute, requiring additional training whenever a new
attribute is introduced. This not only results in parameter redundancy
accumulated by sliders but also restricts the flexibility of practical
applications and the scalability of attribute manipulation. To address this
issue, we introduce the All-in-One Slider, a lightweight module that decomposes
the text embedding space into sparse, semantically meaningful attribute
directions. Once trained, it functions as a general-purpose slider, enabling
interpretable and fine-grained continuous control over various attributes.
Moreover, by recombining the learned directions, the All-in-One Slider supports
zero-shot manipulation of unseen attributes (e.g., races and celebrities) and
the composition of multiple attributes. Extensive experiments demonstrate that
our method enables accurate and scalable attribute manipulation, achieving
notable improvements compared to previous methods. Furthermore, our method can
be extended to integrate with the inversion framework to perform attribute
manipulation on real images, broadening its applicability to various real-world
scenarios. The code and trained model will be released at:
https://github.com/ywxsuperstar/KSAE-FaceSteer.

</details>


### [118] [OmniHuman-1.5: Instilling an Active Mind in Avatars via Cognitive Simulation](https://arxiv.org/abs/2508.19209)
*Jianwen Jiang,Weihong Zeng,Zerong Zheng,Jiaqi Yang,Chao Liang,Wang Liao,Han Liang,Yuan Zhang,Mingyuan Gao*

Main category: cs.CV

TL;DR: 本文提出OmniHuman-1.5框架，通过结合多模态大语言模型（MLLMs）提供高级语义指导和专用的多模态DiT架构，生成不仅物理真实而且语义连贯、富有表现力的角色动画。


<details>
  <summary>Details</summary>
Motivation: 现有视频虚拟形象模型虽能生成流畅动画，但仅限于物理相似性，无法捕捉角色真实精髓。它们的动作通常与音频节奏等低级线索同步，缺乏对情感、意图或上下文的深层语义理解。

Method: 本研究基于OmniHuman-1.5模型，包含两项关键技术贡献：1) 利用多模态大语言模型（MLLMs）合成结构化的文本条件，提供高级语义指导，使动作生成超越简单的节奏同步，实现上下文和情感共鸣；2) 引入带有新颖“伪最后一帧”（Pseudo Last Frame）设计的专用多模态DiT架构，以有效融合多模态输入并减轻模态间冲突。

Result: 实验证明，该模型在唇形同步准确性、视频质量、动作自然度和与文本提示的语义一致性等全面指标上均达到领先性能。此外，该方法在多人物和非人类主体等复杂场景中也展现出卓越的扩展性。

Conclusion: 该模型通过准确解释音频、图像和文本的联合语义，生成与角色、场景和语言内容深度一致的动作，有效弥补了现有视频虚拟形象模型在语义理解和表达方面的不足。

Abstract: Existing video avatar models can produce fluid human animations, yet they
struggle to move beyond mere physical likeness to capture a character's
authentic essence. Their motions typically synchronize with low-level cues like
audio rhythm, lacking a deeper semantic understanding of emotion, intent, or
context. To bridge this gap, \textbf{we propose a framework designed to
generate character animations that are not only physically plausible but also
semantically coherent and expressive.} Our model, \textbf{OmniHuman-1.5}, is
built upon two key technical contributions. First, we leverage Multimodal Large
Language Models to synthesize a structured textual representation of conditions
that provides high-level semantic guidance. This guidance steers our motion
generator beyond simplistic rhythmic synchronization, enabling the production
of actions that are contextually and emotionally resonant. Second, to ensure
the effective fusion of these multimodal inputs and mitigate inter-modality
conflicts, we introduce a specialized Multimodal DiT architecture with a novel
Pseudo Last Frame design. The synergy of these components allows our model to
accurately interpret the joint semantics of audio, images, and text, thereby
generating motions that are deeply coherent with the character, scene, and
linguistic content. Extensive experiments demonstrate that our model achieves
leading performance across a comprehensive set of metrics, including lip-sync
accuracy, video quality, motion naturalness and semantic consistency with
textual prompts. Furthermore, our approach shows remarkable extensibility to
complex scenarios, such as those involving multi-person and non-human subjects.
Homepage: \href{https://omnihuman-lab.github.io/v1_5/}

</details>


### [119] [Automated Feature Tracking for Real-Time Kinematic Analysis and Shape Estimation of Carbon Nanotube Growth](https://arxiv.org/abs/2508.19232)
*Kaveh Safavigerdini,Ramakrishna Surya,Jaired Collins,Prasad Calyam,Filiz Bunyak,Matthew R. Maschmann,Kannappan Palaniappan*

Main category: cs.CV

TL;DR: 解析错误


<details>
  <summary>Details</summary>
Motivation: 解析错误

Method: 解析错误

Result: 解析错误

Conclusion: 解析错误

Abstract: Carbon nanotubes (CNTs) are critical building blocks in nanotechnology, yet
the characterization of their dynamic growth is limited by the experimental
challenges in nanoscale motion measurement using scanning electron microscopy
(SEM) imaging. Existing ex situ methods offer only static analysis, while in
situ techniques often require manual initialization and lack continuous
per-particle trajectory decomposition. We present Visual Feature Tracking
(VFTrack) an in-situ real-time particle tracking framework that automatically
detects and tracks individual CNT particles in SEM image sequences. VFTrack
integrates handcrafted or deep feature detectors and matchers within a particle
tracking framework to enable kinematic analysis of CNT micropillar growth. A
systematic using 13,540 manually annotated trajectories identifies the ALIKED
detector with LightGlue matcher as an optimal combination (F1-score of 0.78,
$\alpha$-score of 0.89). VFTrack motion vectors decomposed into axial growth,
lateral drift, and oscillations, facilitate the calculation of heterogeneous
regional growth rates and the reconstruction of evolving CNT pillar
morphologies. This work enables advancement in automated nano-material
characterization, bridging the gap between physics-based models and
experimental observation to enable real-time optimization of CNT synthesis.

</details>


### [120] [Autoregressive Universal Video Segmentation Model](https://arxiv.org/abs/2508.19242)
*Miran Heo,Sukjun Hwang,Min-Hung Chen,Yu-Chiang Frank Wang,Albert Gu,Seon Joo Kim,Ryo Hachiuma*

Main category: cs.CV

TL;DR: AUSM是一个统一的自回归通用分割模型，它将流式视频分割重新定义为序列掩码预测，并利用状态空间模型同时处理有提示和无提示的视频分割任务，实现了更快的训练速度和更优的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视频基础模型（如SAM2）在有提示的视频分割方面表现出色，但许多实际场景需要无提示的分割（即在没有外部提示的情况下检测和跟踪视频中的所有对象），而目前该领域被任务特定的模型和管道所碎片化。研究者旨在解决这一问题，创建一个能统一处理有提示和无提示视频分割的通用模型。

Method: 该研究将流式视频分割重新定义为序列掩码预测（类似于语言建模），并引入了自回归通用分割模型（AUSM）。AUSM是一个单一架构，基于最新的状态空间模型构建，维护固定大小的空间状态，可扩展到任意长度的视频流。此外，AUSM的所有组件都设计用于跨帧并行训练，从而显著加快了训练速度。

Result: 在DAVIS17、YouTube-VOS 2018 & 2019、MOSE、YouTube-VIS 2019 & 2021以及OVIS等标准基准测试中，AUSM的性能超越了之前通用的流式视频分割方法，并且在16帧序列上实现了高达2.5倍的训练加速。

Conclusion: AUSM成功地将有提示和无提示的视频分割任务统一到一个单一的架构中，通过创新的序列掩码预测和状态空间模型设计，不仅在性能上超越了现有方法，还在训练效率上取得了显著提升，为流式视频分割提供了一个通用且高效的解决方案。

Abstract: Recent video foundation models such as SAM2 excel at prompted video
segmentation by treating masks as a general-purpose primitive. However, many
real-world settings require unprompted segmentation that aims to detect and
track all objects in a video without external cues, leaving today's landscape
fragmented across task-specific models and pipelines. We recast streaming video
segmentation as sequential mask prediction, analogous to language modeling, and
introduce the Autoregressive Universal Segmentation Model (AUSM), a single
architecture that unifies both prompted and unprompted video segmentation.
Built on recent state-space models, AUSM maintains a fixed-size spatial state
and scales to video streams of arbitrary length. Furthermore, all components of
AUSM are designed for parallel training across frames, yielding substantial
speedups over iterative training. On standard benchmarks (DAVIS17, YouTube-VOS
2018 & 2019, MOSE, YouTube-VIS 2019 & 2021, and OVIS) AUSM outperforms prior
universal streaming video segmentation methods and achieves up to 2.5x faster
training on 16-frame sequences.

</details>


### [121] [Style4D-Bench: A Benchmark Suite for 4D Stylization](https://arxiv.org/abs/2508.19243)
*Beiqi Chen,Shuai Shao,Haitang Feng,Jianhuang Lai,Jianlou Si,Guangcong Wang*

Main category: cs.CV

TL;DR: 本文介绍了Style4D-Bench，首个针对4D风格化设计的基准测试套件，以及Style4D，一个基于4D高斯泼溅的新型4D风格化框架，并在Style4D-Bench上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 目前4D风格化领域缺乏标准化的评估方法和进展，需要一个综合性的基准来推动该领域的发展。

Method: 本文提出了Style4D-Bench基准套件，包含：1) 衡量空间保真度、时间连贯性和多视角一致性的评估协议；2) 一个强大的基线方法；3) 精心策划的高分辨率动态4D场景集合。作为基线，本文提出了Style4D框架，其基于4D高斯泼溅，包含三个关键组件：基础4DGS场景表示、利用轻量级每高斯MLP进行时空感知外观控制的Style Gaussian Representation，以及通过对比连贯性学习和结构内容保持来增强时空一致性的Holistic Geometry-Preserved Style Transfer模块。

Result: 在Style4D-Bench上的大量实验表明，Style4D在4D风格化方面取得了最先进的性能，能够生成具有稳定时间动态和一致多视角渲染的精细风格细节。

Conclusion: Style4D-Bench有望成为动态3D场景风格化渲染基准测试和推进研究的宝贵资源。

Abstract: We introduce Style4D-Bench, the first benchmark suite specifically designed
for 4D stylization, with the goal of standardizing evaluation and facilitating
progress in this emerging area. Style4D-Bench comprises: 1) a comprehensive
evaluation protocol measuring spatial fidelity, temporal coherence, and
multi-view consistency through both perceptual and quantitative metrics, 2) a
strong baseline that make an initial attempt for 4D stylization, and 3) a
curated collection of high-resolution dynamic 4D scenes with diverse motions
and complex backgrounds. To establish a strong baseline, we present Style4D, a
novel framework built upon 4D Gaussian Splatting. It consists of three key
components: a basic 4DGS scene representation to capture reliable geometry, a
Style Gaussian Representation that leverages lightweight per-Gaussian MLPs for
temporally and spatially aware appearance control, and a Holistic
Geometry-Preserved Style Transfer module designed to enhance spatio-temporal
consistency via contrastive coherence learning and structural content
preservation. Extensive experiments on Style4D-Bench demonstrate that Style4D
achieves state-of-the-art performance in 4D stylization, producing fine-grained
stylistic details with stable temporal dynamics and consistent multi-view
rendering. We expect Style4D-Bench to become a valuable resource for
benchmarking and advancing research in stylized rendering of dynamic 3D scenes.
Project page: https://becky-catherine.github.io/Style4D . Code:
https://github.com/Becky-catherine/Style4D-Bench .

</details>


### [122] [Articulate3D: Zero-Shot Text-Driven 3D Object Posing](https://arxiv.org/abs/2508.19244)
*Oishi Deb,Anjun Hu,Ashkan Khakzar,Philip Torr,Christian Rupprecht*

Main category: cs.CV

TL;DR: Articulate3D是一种无需训练的方法，通过语言控制来调整3D资产的姿态。它通过修改图像生成器生成目标图像，然后利用多视图姿态优化和关键点对齐网格，成功实现姿态操作同时保持物体身份。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉和语言模型取得了进展，但通过语言控制来调整3D资产姿态的任务仍然具有挑战性。

Method: 该方法将问题分解为两步：1. 修改一个强大的图像生成器（引入RSActrl自注意力重布线机制，将源结构与姿态解耦），根据输入图像和文本指令创建目标图像。2. 通过多视图姿态优化步骤将网格与目标图像对齐。值得注意的是，该方法使用关键点建立输入和目标图像之间的对应关系，而不是依赖不可靠的可微分渲染信号。

Result: Articulate3D在各种3D对象和自由形式文本提示下都表现出有效性，成功地操纵了姿态同时保持了网格的原始身份。定量评估和用户研究（用户偏好度超过85%）证实了其优于现有方法。

Conclusion: Articulate3D提供了一种卓越的、无需训练的语言控制3D资产姿态调整方法，能够有效地操纵姿态同时保持物体身份，解决了现有方法的挑战。

Abstract: We propose a training-free method, Articulate3D, to pose a 3D asset through
language control. Despite advances in vision and language models, this task
remains surprisingly challenging. To achieve this goal, we decompose the
problem into two steps. We modify a powerful image-generator to create target
images conditioned on the input image and a text instruction. We then align the
mesh to the target images through a multi-view pose optimisation step. In
detail, we introduce a self-attention rewiring mechanism (RSActrl) that
decouples the source structure from pose within an image generative model,
allowing it to maintain a consistent structure across varying poses. We
observed that differentiable rendering is an unreliable signal for articulation
optimisation; instead, we use keypoints to establish correspondences between
input and target images. The effectiveness of Articulate3D is demonstrated
across a diverse range of 3D objects and free-form text prompts, successfully
manipulating poses while maintaining the original identity of the mesh.
Quantitative evaluations and a comparative user study, in which our method was
preferred over 85\% of the time, confirm its superiority over existing
approaches. Project page:https://odeb1.github.io/articulate3d_page_deb/

</details>


### [123] [VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D Space](https://arxiv.org/abs/2508.19247)
*Lin Li,Zehuan Huang,Haoran Feng,Gengxiong Zhuang,Rui Chen,Chunchao Guo,Lu Sheng*

Main category: cs.CV

TL;DR: VoxHammer 是一种无需训练的 3D 潜在空间局部编辑方法，通过保留上下文特征，实现了对指定区域的精确、连贯编辑，并能有效保持未编辑区域的一致性。


<details>
  <summary>Details</summary>
Motivation: 3D 局部编辑在游戏和机器人交互中至关重要，但现有方法通常通过编辑多视角图像再重建 3D 模型，难以精确保留未编辑区域并维持整体连贯性。

Method: VoxHammer 受到结构化 3D 生成模型的启发，首先预测 3D 模型的反演轨迹，获取每个时间步的反演潜在特征和键值令牌。在去噪和编辑阶段，该方法用对应的反演潜在特征和缓存的键值令牌替换保留区域的去噪特征，从而保留上下文特征。此外，研究构建了 Edit3D-Bench 数据集来评估保留区域的一致性。

Result: 实验表明，VoxHammer 在保留区域的 3D 一致性和整体质量方面显著优于现有方法。该方法有望合成高质量的编辑配对数据，为上下文 3D 生成奠定数据基础。

Conclusion: VoxHammer 提供了一种精确且连贯的 3D 局部编辑解决方案，有效解决了现有方法在保持未编辑区域一致性方面的挑战，并为未来的 3D 生成研究提供了数据基础。

Abstract: 3D local editing of specified regions is crucial for game industry and robot
interaction. Recent methods typically edit rendered multi-view images and then
reconstruct 3D models, but they face challenges in precisely preserving
unedited regions and overall coherence. Inspired by structured 3D generative
models, we propose VoxHammer, a novel training-free approach that performs
precise and coherent editing in 3D latent space. Given a 3D model, VoxHammer
first predicts its inversion trajectory and obtains its inverted latents and
key-value tokens at each timestep. Subsequently, in the denoising and editing
phase, we replace the denoising features of preserved regions with the
corresponding inverted latents and cached key-value tokens. By retaining these
contextual features, this approach ensures consistent reconstruction of
preserved areas and coherent integration of edited parts. To evaluate the
consistency of preserved regions, we constructed Edit3D-Bench, a
human-annotated dataset comprising hundreds of samples, each with carefully
labeled 3D editing regions. Experiments demonstrate that VoxHammer
significantly outperforms existing methods in terms of both 3D consistency of
preserved regions and overall quality. Our method holds promise for
synthesizing high-quality edited paired data, thereby laying the data
foundation for in-context 3D generation. See our project page at
https://huanngzh.github.io/VoxHammer-Page/.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [124] [Semantic Attractors and the Emergence of Meaning: Towards a Teleological Model of AGI](https://arxiv.org/abs/2508.18290)
*Hans-Joachim Rudolph*

Main category: cs.CL

TL;DR: 本文提出了一种基于复值意义空间中语义吸引子的理论框架，用于构建语义通用人工智能（AGI），通过递归张量变换和旋转语义结构来建模意义，而非统计预测，并引入吸引子引导意义走向连贯性。


<details>
  <summary>Details</summary>
Motivation: 当前基于Transformer的语言模型依赖统计学上的下一个词预测，无法真正形成或理解意义，也难以处理讽刺、同音异义和歧义等复杂语义现象。作者旨在开发一种能够主动形成意义、并将其引向稳定和深度的认知架构。

Method: 该研究采用以下方法：1) 构建一个基于复值意义空间的理论框架；2) 引入语义吸引子（被视为一个目的论操作器和意向代理，即Microvitum）；3) 利用递归张量变换来形成意义；4) 使用涉及虚数单位'i'的循环操作来描述能够建模讽刺、同音异义和歧义的旋转语义结构；5) 将吸引子概念化为梯度流、张量变形和迭代矩阵动力学。

Result: 该模型能够描述一种旋转语义结构，有效建模讽刺、同音异义和歧义。语义吸引子作为核心，能够引导意义趋向稳定性、清晰度和表达深度。研究认为，真正的意义并非来自模拟，而是通过递归收敛达到语义连贯性，这需要一种旨在塑造语言而非仅仅预测语言的全新认知架构。

Conclusion: 真正的意义涌现需要一种根本性的新型认知架构，该架构应通过语义吸引子引导的递归收敛，而非统计预测，来主动塑造语言并实现语义连贯性，从而超越当前语言模型的局限性。

Abstract: This essay develops a theoretical framework for a semantic Artificial General
Intelligence (AGI) based on the notion of semantic attractors in complex-valued
meaning spaces. Departing from current transformer-based language models, which
operate on statistical next-token prediction, we explore a model in which
meaning is not inferred probabilistically but formed through recursive
tensorial transformation. Using cyclic operations involving the imaginary unit
\emph{i}, we describe a rotational semantic structure capable of modeling
irony, homonymy, and ambiguity. At the center of this model, however, is a
semantic attractor -- a teleological operator that, unlike statistical
computation, acts as an intentional agent (Microvitum), guiding meaning toward
stability, clarity, and expressive depth. Conceived in terms of gradient flows,
tensor deformations, and iterative matrix dynamics, the attractor offers a
model of semantic transformation that is not only mathematically suggestive,
but also philosophically significant. We argue that true meaning emerges not
from simulation, but from recursive convergence toward semantic coherence, and
that this requires a fundamentally new kind of cognitive architecture -- one
designed to shape language, not just predict it.

</details>


### [125] [LLMs Can't Handle Peer Pressure: Crumbling under Multi-Agent Social Interactions](https://arxiv.org/abs/2508.18321)
*Maojia Song,Tej Deep Pala,Weisheng Jin,Amir Zadeh,Chuan Li,Dorien Herremans,Soujanya Poria*

Main category: cs.CL

TL;DR: 该研究通过KAIROS基准测试，分析了大型语言模型（LLMs）在多智能体系统中如何形成信任、抵制错误信息并整合同伴输入，并评估了提示工程、微调和强化学习等缓解策略，发现GRPO表现最佳但鲁棒性降低。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs越来越多地部署在多智能体系统（MAS）中作为协作智能的组成部分，先前的研究主要关注从众偏见。然而，为了在复杂的社会动态下实现集体智能，需要深入分析LLMs如何从过往印象中建立信任、抵制错误信息以及在互动中整合同伴输入。

Method: 研究引入了KAIROS基准测试，模拟具有不同可靠性同伴的知识问答比赛，精细控制专家-新手角色、嘈杂人群和对抗性同伴等条件。LLMs接收历史互动和当前同伴回应，以系统地调查信任、同伴行动和自信心如何影响决策。缓解策略评估了提示工程、监督微调和强化学习（Group Relative Policy Optimisation, GRPO）。

Result: 结果显示，结合多智能体上下文、基于结果的奖励和无约束推理的GRPO在整体性能上表现最佳。然而，与基础模型相比，这种策略也降低了对社会影响的鲁棒性。

Conclusion: GRPO结合特定配置能显著提升LLMs在多智能体系统中的表现，但这种性能提升是以牺牲对社会影响的鲁棒性为代价的，揭示了性能与鲁棒性之间的权衡。

Abstract: Large language models (LLMs) are increasingly deployed in multi-agent systems
(MAS) as components of collaborative intelligence, where peer interactions
dynamically shape individual decision-making. Although prior work has focused
on conformity bias, we extend the analysis to examine how LLMs form trust from
previous impressions, resist misinformation, and integrate peer input during
interaction, key factors for achieving collective intelligence under complex
social dynamics. We present KAIROS, a benchmark simulating quiz contests with
peer agents of varying reliability, offering fine-grained control over
conditions such as expert-novice roles, noisy crowds, and adversarial peers.
LLMs receive both historical interactions and current peer responses, allowing
systematic investigation into how trust, peer action, and self-confidence
influence decisions. As for mitigation strategies, we evaluate prompting,
supervised fine-tuning, and reinforcement learning, Group Relative Policy
Optimisation (GRPO), across multiple models. Our results reveal that GRPO with
multi-agent context combined with outcome-based rewards and unconstrained
reasoning achieves the best overall performance, but also decreases the
robustness to social influence compared to Base models. The code and datasets
are available at: https://github.com/declare-lab/KAIROS.

</details>


### [126] [Not All Visitors are Bilingual: A Measurement Study of the Multilingual Web from an Accessibility Perspective](https://arxiv.org/abs/2508.18328)
*Masudul Hasan Masud Bhuiyan,Matteo Varvello,Yasir Zaki,Cristian-Alexandru Staicu*

Main category: cs.CL

TL;DR: 本文介绍了LangCrUX，一个包含12种非拉丁语系语言的12万个网站的大规模数据集，用于分析多语言网页无障碍性。研究发现无障碍提示普遍被忽视且语言不一致，降低了屏幕阅读器效率。最后提出了Kizuki，一个语言感知的自动化无障碍测试扩展。


<details>
  <summary>Details</summary>
Motivation: 尽管英语在网络上占主导地位，但多语言内容（特别是结合了区域或母语的内容）正在增长。这给视障用户带来了显著障碍，因为屏幕阅读器通常缺乏对非拉丁文字的强大支持，导致文本渲染和发音错误，加剧了多语言环境下的无障碍挑战。然而，缺乏全面的多语言网络内容数据集限制了对这一问题的大规模研究。

Method: 1. 引入了LangCrUX，这是第一个包含12万个主要使用非拉丁文字的网站的大规模数据集，涵盖12种语言。2. 利用LangCrUX数据集，对多语言网络无障碍性进行了系统分析。3. 提出了Kizuki，一个语言感知的自动化无障碍测试扩展，以解决语言不一致的无障碍提示的有限效用。

Result: 1. 发现了无障碍提示普遍被忽视的现象。2. 这些提示常常未能反映可见内容的语言多样性。3. 这种不一致性降低了屏幕阅读器的有效性，并限制了网络无障碍性。

Conclusion: 多语言网络内容中，屏幕阅读器对非拉丁文字支持不足以及无障碍提示与实际内容语言不一致，共同构成了严重的无障碍挑战。通过LangCrUX数据集的引入和Kizuki测试扩展的提出，旨在识别并解决这些问题，以提升多语言网络内容的无障碍性。

Abstract: English is the predominant language on the web, powering nearly half of the
world's top ten million websites. Support for multilingual content is
nevertheless growing, with many websites increasingly combining English with
regional or native languages in both visible content and hidden metadata. This
multilingualism introduces significant barriers for users with visual
impairments, as assistive technologies like screen readers frequently lack
robust support for non-Latin scripts and misrender or mispronounce non-English
text, compounding accessibility challenges across diverse linguistic contexts.
Yet, large-scale studies of this issue have been limited by the lack of
comprehensive datasets on multilingual web content. To address this gap, we
introduce LangCrUX, the first large-scale dataset of 120,000 popular websites
across 12 languages that primarily use non-Latin scripts. Leveraging this
dataset, we conduct a systematic analysis of multilingual web accessibility and
uncover widespread neglect of accessibility hints. We find that these hints
often fail to reflect the language diversity of visible content, reducing the
effectiveness of screen readers and limiting web accessibility. We finally
propose Kizuki, a language-aware automated accessibility testing extension to
account for the limited utility of language-inconsistent accessibility hints.

</details>


### [127] [Language-Specific Layer Matters: Efficient Multilingual Enhancement for Large Vision-Language Models](https://arxiv.org/abs/2508.18381)
*Yuchun Fan,Yilin Wang,Yongyu Mu,Lei Huang,Bei Li,Xiaocheng Feng,Tong Xiao,Jingbo Zhu*

Main category: cs.CL

TL;DR: 本文提出PLAST，一种高效的训练方法，通过精准微调大型视觉-语言模型（LVLMs）中浅层与特定语言相关的神经元，以增强其多语言能力，同时显著提高效率。


<details>
  <summary>Details</summary>
Motivation: 大型视觉-语言模型（LVLMs）在视觉信息理解方面表现出色，但在多语言能力上存在不平衡。研究旨在深入理解LVLMs的多语言工作模式并解决其多语言能力不足的问题。

Method: 研究首先发现LVLMs的多语言理解能力与浅层中特定语言的神经元激活存在显著关联。在此基础上，提出PLAST（Precise LAnguage-Specific layers fine-Tuning）训练方案。PLAST通过监测特定语言的神经元激活来识别参与多语言理解的层，然后使用问题-翻译对对这些层进行精准微调，以实现多语言对齐。

Result: PLAST有效提升了LVLMs的多语言能力，并在MM-Bench和MMMB上取得了显著成果。该方法仅微调了14%的参数，显著提高了效率。此外，PLAST可推广到低资源和复杂的视觉推理任务，促进了浅层中特定语言的视觉信息参与。

Conclusion: PLAST是一种高效且有效的方法，通过精准微调LVLMs中与特定语言相关的浅层，显著增强了模型的多语言能力，并展现了在不同任务和资源条件下的泛化性。

Abstract: Large vision-language models (LVLMs) have demonstrated exceptional
capabilities in understanding visual information with human languages but also
exhibit an imbalance in multilingual capabilities. In this work, we delve into
the multilingual working pattern of LVLMs and identify a salient correlation
between the multilingual understanding ability of LVLMs and language-specific
neuron activations in shallow layers. Building on this insight, we introduce
PLAST, a training recipe that achieves efficient multilingual enhancement for
LVLMs by Precise LAnguage-Specific layers fine-Tuning. PLAST first identifies
layers involved in multilingual understanding by monitoring language-specific
neuron activations. These layers are then precisely fine-tuned with
question-translation pairs to achieve multilingual alignment. Our empirical
results on MM-Bench and MMMB demonstrate that PLAST effectively improves the
multilingual capabilities of LVLMs and achieves significant efficiency with
only 14% of the parameters tuned. Further analysis reveals that PLAST can be
generalized to low-resource and complex visual reasoning tasks, facilitating
the language-specific visual information engagement in shallow layers.

</details>


### [128] [Backprompting: Leveraging Synthetic Production Data for Health Advice Guardrails](https://arxiv.org/abs/2508.18384)
*Kellen Tan Cheng,Anna Lisa Gentile,Chad DeLuca,Guang-Jie Ren*

Main category: cs.CL

TL;DR: 本文提出了一种名为“回溯提示”（backprompting）的方法，结合稀疏的人工辅助聚类技术，用于在部署前生成生产质量的LLM输出标注数据，以开发和训练更稳健的LLM健康建议护栏检测器。该方法显著提升了检测器性能，甚至以更少的参数超越了GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在企业中的普及带来了显著风险。护栏技术旨在通过检测器过滤LLM的输入/输出以减轻这些风险。然而，开发和维护稳健的检测器面临挑战，其中之一是在部署前难以获取真实LLM输出的生产质量标注数据。

Method: 本文提出“回溯提示”（backprompting）方法来生成类似生产环境的标注数据，用于健康建议护栏的开发。此外，该方法与一种稀疏的人工辅助聚类技术结合，对生成的数据进行标注。目标是构建一个大致代表原始数据集但类似真实LLM输出的并行语料库。然后，将现有数据集与这些合成示例结合，以生成稳健的检测器训练数据。该技术在一个最困难且微妙的护栏任务——识别LLM输出中的健康建议——中进行了测试。

Result: 该技术在识别LLM输出中的健康建议方面，相对于其他解决方案显示出改进。所开发的检测器能够比GPT-4o表现更好，性能提升高达3.73%，尽管其参数量少了400倍。

Conclusion: 通过“回溯提示”和稀疏的人工辅助聚类技术，可以有效地生成生产质量的标注数据，用于开发稳健的LLM护栏检测器。这种方法能够显著提高检测器性能，使其在特定困难任务（如识别健康建议）上超越参数量大得多的模型。

Abstract: The pervasiveness of large language models (LLMs) in enterprise settings has
also brought forth a significant amount of risks associated with their usage.
Guardrails technologies aim to mitigate this risk by filtering LLMs'
input/output text through various detectors. However, developing and
maintaining robust detectors faces many challenges, one of which is the
difficulty in acquiring production-quality labeled data on real LLM outputs
prior to deployment. In this work, we propose backprompting, a simple yet
intuitive solution to generate production-like labeled data for health advice
guardrails development. Furthermore, we pair our backprompting method with a
sparse human-in-the-loop clustering technique to label the generated data. Our
aim is to construct a parallel corpus roughly representative of the original
dataset yet resembling real LLM output. We then infuse existing datasets with
our synthetic examples to produce robust training data for our detector. We
test our technique in one of the most difficult and nuanced guardrails: the
identification of health advice in LLM output, and demonstrate improvement
versus other solutions. Our detector is able to outperform GPT-4o by up to
3.73%, despite having 400x less parameters.

</details>


### [129] [Integral Transformer: Denoising Attention, Not Too Much Not Too Little](https://arxiv.org/abs/2508.18387)
*Ivan Kobyzev,Abbas Ghaddar,Dingtao Hu,Boxing Chen*

Main category: cs.CL

TL;DR: 本文提出Integral Transformer，一种新型自注意力机制，通过整合从logit分布中采样的信号来去噪注意力，同时保留对模型性能至关重要的特殊token贡献。该模型在知识和推理语言基准上优于现有变体，并在Transformer上层平衡注意力分布并减少秩坍塌。


<details>
  <summary>Details</summary>
Motivation: Softmax自注意力常将过高权重分配给无语义信息的token（如特殊token和标点符号），导致注意力噪声。尽管现有方法（如Cog Attention和Differential Transformer）通过引入负注意力得分来解决此问题，但存在丢弃有用信息的风险。

Method: 本文提出Integral Transformer，一种新颖的自注意力机制，通过整合从logit分布中采样的信号来去噪注意力。

Result: 该方法在减轻噪声的同时保留了对模型性能至关重要的特殊token的贡献。大量实验表明，在已建立的知识和推理语言基准上，Integral Transformer优于Vanilla、Cog和Differential注意力变体。此外，分析显示在Transformer较低层使用Vanilla自注意力可提高性能，且Integral Transformer能有效平衡上层注意力分布并减少秩坍塌。

Conclusion: Integral Transformer是一种有效的自注意力去噪机制，它能在保留关键信息的同时提升模型在知识和推理任务上的表现，并通过平衡注意力分布和减少秩坍塌来优化Transformer上层的行为。

Abstract: Softmax self-attention often assigns disproportionate weight to semantically
uninformative tokens such as special tokens and punctuation, a phenomenon known
as attention noise. While recent methods like Cog Attention and the
Differential Transformer have addressed this by introducing negative attention
scores, they risk discarding useful information. In this paper, we propose the
Integral Transformer, a novel self-attention mechanism that denoises attention
by integrating signals sampled from the logit distribution. Our approach
mitigates noise while preserving the contributions of special tokens critical
for model performance. Extensive experiments demonstrate that our model
outperforms vanilla, Cog, and Differential attention variants on
well-established knowledge and reasoning language benchmarks. Moreover, our
analysis reveals that employing vanilla self-attention in the lower Transformer
layers enhances performance and that the Integral Transformer effectively
balances attention distributions and reduces rank collapse in upper layers.

</details>


### [130] [Latent Self-Consistency for Reliable Majority-Set Selection in Short- and Long-Answer Reasoning](https://arxiv.org/abs/2508.18395)
*Jeong-seok Oh,Jay-yoon Lee*

Main category: cs.CL

TL;DR: LSC是一种新的LLM解码一致性选择方法，通过学习型token嵌入来选择语义最一致的响应。它在短文本和长文本基准测试上均优于现有方法，且计算开销可忽略不计。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的概率解码在复杂或长文本问题上经常产生不一致的输出。现有的一致性方法（如SC、USC、WUCS）要么只适用于短文本问答，要么在扩展到长文本时会牺牲短文本的准确性。

Method: 本文提出了潜在自洽性（Latent Self-Consistency, LSC）方法。它通过使用可学习的token嵌入来选择语义上最一致的响应。通过轻量级的摘要token前向生成，LSC的推理时间增加不到1%，且不需要改变模型架构。

Result: 在6个短文本和5个长文本推理基准测试（如MATH, MMLU, TruthfulQA）中，LSC在平均表现上超越了SC、USC和WUCS。同时，LSC的计算开销可以忽略不计。此外，LSC还提供了良好校准的置信度估计，在两种答案格式下都保持了较低的预期校准误差。

Conclusion: LSC被定位为一种实用的、可靠的一致性选择方法，能够跨多种答案格式工作。它在保持低计算开销的同时，在语义一致性选择和置信度估计方面表现出色。

Abstract: Probabilistic decoding in Large Language Models (LLMs) often yields
inconsistent outputs, particularly on complex or long-form questions.
Self-Consistency (SC) mitigates this for short-form QA by majority voting over
exact strings, whereas Universal Self-Consistency (USC) and Weighted Unigram
Consistency Score (WUCS) extend to long-form responses but lose accuracy on
short-form benchmarks.
  We introduce Latent Self-Consistency (LSC), which selects the most
semantically consistent response using learnable token embeddings. A
lightweight forward generation of summary tokens increases inference time by
less than 1% and requires no changes to the model architecture.
  Across 6 short-form and 5 long-form reasoning benchmarks (e.g., MATH, MMLU,
TruthfulQA), LSC surpasses SC, USC and WUCS on all short-form and long-form
ones on average, while maintaining negligible computational overhead. These
results position LSC as a practical consistency-selection method that works
reliably across answer formats. Additionally, LSC provides well-calibrated
confidence estimates, maintaining low Expected Calibration Error across both
answer formats.

</details>


### [131] [Can Out-of-Distribution Evaluations Uncover Reliance on Shortcuts? A Case Study in Question Answering](https://arxiv.org/abs/2508.18407)
*Michal Štefánik,Timothee Mickus,Marek Kadlčík,Michal Spiegel,Josef Kuchař*

Main category: cs.CL

TL;DR: 本文质疑了OOD评估能有效反映模型在现实世界中因捷径预测而失败的假设，发现不同OOD数据集在评估模型对捷径的鲁棒性方面质量差异巨大，并提出了更鲁棒的泛化评估方法。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型泛化能力的评估主要依赖于OOD数据集，但这建立在一个强假设之上：OOD评估能捕捉并反映真实部署中可能出现的失败。本文旨在挑战这一假设，并通过将OOD评估结果与QA模型中已记录的特定失败模式（如依赖虚假特征或预测捷径）进行对比。

Method: 研究人员将OOD评估的结果与现有问答（QA）模型中记录的特定失败模式（即对虚假特征或预测捷径的依赖）进行了对比。他们分析了用于QA领域OOD评估的不同数据集，以评估其在捕捉模型对捷径鲁棒性方面的质量。

Result: 研究发现，用于QA领域OOD评估的不同数据集在估计模型对捷径的鲁棒性方面提供了质量差异巨大的结果，有些甚至远低于简单的同分布（ID）评估。部分原因在于虚假捷径在ID和OOD数据集中共享，但也发现数据集的训练和评估质量可能存在显著脱节的情况。

Conclusion: 本研究强调了常用基于OOD的泛化评估方法的局限性，并为在QA领域内外更鲁棒地评估泛化能力提供了方法论和建议。

Abstract: A majority of recent work in AI assesses models' generalization capabilities
through the lens of performance on out-of-distribution (OOD) datasets. Despite
their practicality, such evaluations build upon a strong assumption: that OOD
evaluations can capture and reflect upon possible failures in a real-world
deployment.
  In this work, we challenge this assumption and confront the results obtained
from OOD evaluations with a set of specific failure modes documented in
existing question-answering (QA) models, referred to as a reliance on spurious
features or prediction shortcuts.
  We find that different datasets used for OOD evaluations in QA provide an
estimate of models' robustness to shortcuts that have a vastly different
quality, some largely under-performing even a simple, in-distribution
evaluation. We partially attribute this to the observation that spurious
shortcuts are shared across ID+OOD datasets, but also find cases where a
dataset's quality for training and evaluation is largely disconnected. Our work
underlines limitations of commonly-used OOD-based evaluations of
generalization, and provides methodology and recommendations for evaluating
generalization within and beyond QA more robustly.

</details>


### [132] [How Reliable are LLMs for Reasoning on the Re-ranking task?](https://arxiv.org/abs/2508.18444)
*Nafis Tanveer Islam,Zhiming Zhao*

Main category: cs.CL

TL;DR: 本文分析了不同训练方法如何影响大型语言模型（LLMs）在重排序任务中的语义理解能力和可解释性，尤其是在训练数据有限的情况下，并探讨了LLMs的可靠性问题。


<details>
  <summary>Details</summary>
Motivation: LLMs在语义理解方面有所提升，但牺牲了透明度，导致用户难以理解其重排序的推理过程。在用户参与度低、排序数据不足的新系统中，准确重排序内容仍是挑战。此外，研究发现并非所有训练方法都能使LLMs获得准确的语义理解，有些可能只是获得了优化评估的抽象知识，从而引发对LLMs真实可靠性的质疑。

Method: 研究分析了不同训练方法对LLMs重排序任务中语义理解的影响。利用一个相对较小的环境和地球科学领域的排序数据集来重排序检索到的内容。同时，也分析了可解释信息，以评估重排序是否能通过可解释性进行推理。

Result: 分析发现，某些训练方法表现出比其他方法更好的可解释性。这暗示并非所有训练方法都能使LLMs获得准确的语义理解，有些模型可能只是获得了优化评估的抽象知识，从而对LLMs的真实可靠性提出了质疑。研究旨在探究这些模型是否能生成更具信息量的文本推理来解决透明度和数据限制的挑战。

Conclusion: 该研究旨在深入理解LLMs的不同训练方法如何影响其在重排序任务中的语义理解和生成可解释性推理的能力，以克服透明度挑战和有限训练数据的问题，并重新审视LLMs的实际可靠性。

Abstract: With the improving semantic understanding capability of Large Language Models
(LLMs), they exhibit a greater awareness and alignment with human values, but
this comes at the cost of transparency. Although promising results are achieved
via experimental analysis, an in-depth understanding of the LLM's internal
workings is unavoidable to comprehend the reasoning behind the re-ranking,
which provides end users with an explanation that enables them to make an
informed decision. Moreover, in newly developed systems with limited user
engagement and insufficient ranking data, accurately re-ranking content remains
a significant challenge. While various training methods affect the training of
LLMs and generate inference, our analysis has found that some training methods
exhibit better explainability than others, implying that an accurate semantic
understanding has not been learned through all training methods; instead,
abstract knowledge has been gained to optimize evaluation, which raises
questions about the true reliability of LLMs. Therefore, in this work, we
analyze how different training methods affect the semantic understanding of the
re-ranking task in LLMs and investigate whether these models can generate more
informed textual reasoning to overcome the challenges of transparency or LLMs
and limited training data. To analyze the LLMs for re-ranking tasks, we utilize
a relatively small ranking dataset from the environment and the Earth science
domain to re-rank retrieved content. Furthermore, we also analyze the
explainable information to see if the re-ranking can be reasoned using
explainability.

</details>


### [133] [Integrating gender inclusivity into large language models via instruction tuning](https://arxiv.org/abs/2508.18466)
*Alina Wróblewska,Bartosz Żuk*

Main category: cs.CL

TL;DR: 本研究通过使用IPIS数据集和性别包容性提示对大型语言模型（LLMs）进行微调，旨在解决当代波兰语中固有的阳性偏见，并减轻波兰语生成中的性别偏见。


<details>
  <summary>Details</summary>
Motivation: 当代波兰语中，由于历史和政治惯例，阳性形式被普遍用于指代男性、女性和混合性别群体。这种不公平的语言系统导致在波兰语文本上训练的LLMs继承并强化了这种阳性偏见，生成性别不平衡的输出，从而产生社会后果。

Method: 本研究通过以下方法解决该问题：1) 使用IPIS数据集（包含人工制作的波兰语性别包容性校对和波兰语到英语翻译指令）对LLMs进行微调。2) 基于理论语言学框架，设计一个包含明确波兰语性别包容性指南的系统提示。3) 对多语言LLMs（Llama-8B, Mistral-7B, Mistral-Nemo）和波兰语专用LLMs（Bielik, PLLuM）进行IPIS微调。

Result: 本研究的方法旨在将性别包容性作为这些模型的固有特征进行整合，提供一种系统性解决方案，以减轻波兰语生成中的性别偏见。

Conclusion: 通过IPIS微调和明确的性别包容性指南，本研究提供了一种系统性解决方案，旨在将性别包容性整合到LLMs中，从而有效缓解波兰语生成中的性别偏见问题。

Abstract: Imagine a language with masculine, feminine, and neuter grammatical genders,
yet, due to historical and political conventions, masculine forms are
predominantly used to refer to men, women and mixed-gender groups. This is the
reality of contemporary Polish. A social consequence of this unfair linguistic
system is that large language models (LLMs) trained on Polish texts inherit and
reinforce this masculine bias, generating gender-imbalanced outputs. This study
addresses this issue by tuning LLMs using the IPIS dataset, a collection of
human-crafted gender-inclusive proofreading in Polish and Polish-to-English
translation instructions. Grounded in a theoretical linguistic framework, we
design a system prompt with explicit gender-inclusive guidelines for Polish. In
our experiments, we IPIS-tune multilingual LLMs (Llama-8B, Mistral-7B and
Mistral-Nemo) and Polish-specific LLMs (Bielik and PLLuM). Our approach aims to
integrate gender inclusivity as an inherent feature of these models, offering a
systematic solution to mitigate gender bias in Polish language generation.

</details>


### [134] [Principled Detection of Hallucinations in Large Language Models via Multiple Testing](https://arxiv.org/abs/2508.18473)
*Jiawei Li,Akshayaa Magesh,Venugopal V. Veeravalli*

Main category: cs.CL

TL;DR: 本文将大型语言模型（LLMs）的幻觉检测问题形式化为假设检验问题，并提出了一种受多重检验启发的鲁棒方法，实验证明其优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）虽然功能强大，但容易产生听起来自信但实际上不正确甚至荒谬的“幻觉”，这促使研究人员寻求有效的幻觉检测方法。

Method: 研究将幻觉检测问题形式化为假设检验问题，并将其与机器学习模型中的分布外（OOD）检测问题进行类比。提出了一种受多重检验启发的检测方法。

Result: 通过广泛的实验结果，验证了所提出方法相对于现有最先进方法的鲁棒性。

Conclusion: 本文提出的基于多重检验的幻觉检测方法在检测大型语言模型幻觉方面表现出强大的鲁棒性，优于现有技术。

Abstract: While Large Language Models (LLMs) have emerged as powerful foundational
models to solve a variety of tasks, they have also been shown to be prone to
hallucinations, i.e., generating responses that sound confident but are
actually incorrect or even nonsensical. In this work, we formulate the problem
of detecting hallucinations as a hypothesis testing problem and draw parallels
to the problem of out-of-distribution detection in machine learning models. We
propose a multiple-testing-inspired method to solve the hallucination detection
problem, and provide extensive experimental results to validate the robustness
of our approach against state-of-the-art methods.

</details>


### [135] [COMET-poly: Machine Translation Metric Grounded in Other Candidates](https://arxiv.org/abs/2508.18549)
*Maike Züfle,Vilém Zouhar,Tu Anh Dinh,Felipe Maia Polo,Jan Niehues,Mrinmaya Sachan*

Main category: cs.CL

TL;DR: 本文提出两种新的自动化机器翻译评估指标（COMET-polycand和COMET-polyic），通过纳入额外的翻译信息（同源句的多候选翻译或相似源文本的带标签翻译）来弥补与人类判断的差异，显著提高了评估性能。


<details>
  <summary>Details</summary>
Motivation: 自动化机器翻译评估指标通常只考虑源句和单个译文，而人类评估时常会参考多个替代译文。这种评估设置上的差异可能负面影响自动化指标的性能。

Method: 本文提出了两种新的自动化指标：1. COMET-polycand：使用同一源句的多个替代翻译进行比较评估。2. COMET-polyic：受检索式上下文学习启发，利用相似源文本的翻译及其人工标注质量分数来指导评估。

Result: 结果显示，COMET-polycand仅增加一个额外翻译即可显著提高段落级指标性能（Kendall's tau-b相关性从0.079提升至0.118），增加更多翻译可获得进一步提升。COMET-polyic引入检索到的示例也获得了类似改进（Kendall's tau-b相关性从0.079提升至0.116）。模型已公开。

Conclusion: 通过纳入额外的上下文信息（无论是同源句的替代翻译还是相似源文本的带标签翻译），可以显著提高自动化机器翻译评估指标的性能，使其更接近人类判断。

Abstract: Automated metrics for machine translation attempt to replicate human
judgment. Unlike humans, who often assess a translation in the context of
multiple alternatives, these metrics typically consider only the source
sentence and a single translation. This discrepancy in the evaluation setup may
negatively impact the performance of automated metrics. We propose two
automated metrics that incorporate additional information beyond the single
translation. COMET-polycand uses alternative translations of the same source
sentence to compare and contrast with the translation at hand, thereby
providing a more informed assessment of its quality. COMET-polyic, inspired by
retrieval-based in-context learning, takes in translations of similar source
texts along with their human-labeled quality scores to guide the evaluation. We
find that including a single additional translation in COMET-polycand improves
the segment-level metric performance (0.079 to 0.118 Kendall's tau-b
correlation), with further gains when more translations are added.
Incorporating retrieved examples in COMET-polyic yields similar improvements
(0.079 to 0.116 Kendall's tau-b correlation). We release our models publicly.

</details>


### [136] [The Mind's Eye: A Multi-Faceted Reward Framework for Guiding Visual Metaphor Generation](https://arxiv.org/abs/2508.18569)
*Girish A. Koushik,Fatemeh Nazarieh,Katherine Birch,Shenbin Qian,Diptesh Kanojia*

Main category: cs.CL

TL;DR: 本文提出一个自评估的视觉隐喻生成框架，侧重于隐喻对齐。该框架包含一个无需训练的S-T-M分解管道和一个轻量级训练管道，在某些指标上超越了现有基线，尤其擅长处理抽象隐喻。


<details>
  <summary>Details</summary>
Motivation: 视觉隐喻生成是一项挑战性任务，需要将源概念与目标概念绑定，同时保留意义并确保视觉连贯性。现有方法在语言理解和视觉一致性方面存在挑战。

Method: 提出一个自评估的视觉隐喻生成框架，专注于隐喻对齐。自评估结合了现有指标和新提出的“隐喻分解分数”及“意义对齐（MA）”指标。探索了两种方法：1) 无需训练的管道，将提示明确分解为源-目标-意义（S-T-M）映射进行图像合成。2) 基于训练的管道，使用提出的自评估奖励机制改进对齐，无需大规模重新训练。

Result: 在测试集上，无需训练的方法在分解、CLIP和MA分数上超越了强大的闭源基线（GPT-4o, Imagen），基于训练的方法紧随其后。用户研究显示参与者整体偏爱GPT-4o，但我们的无需训练管道在开源方法中领先，并在抽象隐喻上略优于Imagen。分析表明S-T-M提示有助于处理较长或更抽象的隐喻，而闭源模型在简短、具体的案例上表现出色；还观察到对采样器设置的敏感性。

Conclusion: 在适度计算资源下，结构化提示（S-T-M）和轻量级强化学习（RL）在隐喻对齐方面表现良好。与人类偏好之间的差距主要由美学和采样驱动。

Abstract: Visual metaphor generation is a challenging task that aims to generate an
image given an input text metaphor. Inherently, it needs language understanding
to bind a source concept with a target concept, in a way that preserves meaning
while ensuring visual coherence. We propose a self-evaluating visual metaphor
generation framework that focuses on metaphor alignment. Our self-evaluation
approach combines existing metrics with our newly proposed metaphor
decomposition score and a meaning alignment (MA) metric. Within this setup, we
explore two novel approaches: a training-free pipeline that explicitly
decomposes prompts into source-target-meaning (S-T-M) mapping for image
synthesis, and a complementary training-based pipeline that improves alignment
using our proposed self-evaluation reward schema, without any large-scale
retraining. On the held-out test set, the training-free approach surpasses
strong closed baselines (GPT-4o, Imagen) on decomposition, CLIP, and MA scores,
with the training-based approach close behind. We evaluate our framework output
using a user-facing study, and observed that participants preferred GPT-4o
overall, while our training-free pipeline led open-source methods and edged
Imagen on abstract metaphors. Our analyses show S-T-M prompting helps longer or
more abstract metaphors, with closed models excelling on short, concrete cases;
we also observe sensitivity to sampler settings. Overall, structured prompting
and lightweight RL perform metaphor alignment well under modest compute, and
remaining gaps to human preference appear driven by aesthetics and sampling.

</details>


### [137] [What do language models model? Transformers, automata, and the format of thought](https://arxiv.org/abs/2508.18598)
*Colin Klein*

Main category: cs.CL

TL;DR: 本文认为大型语言模型（LLMs）主要模拟其训练语料库，而非人类认知能力，原因在于其计算架构的根本差异。但作者认为这并非对语言或LLMs的贬低性观点，而是将其视为一种“话语机器”。


<details>
  <summary>Details</summary>
Motivation: 研究的动机在于探讨大型语言模型究竟模拟了什么——是人类的能力，还是它们所训练的语料库？以及这种理解对认知科学有何影响。

Method: 作者通过对比人类语言能力依赖的超线性计算格式与Transformer架构所支持的线性处理格式来构建论证。该论证主要基于Transformer计算架构的某些不变性，并引用了Liu et al. (2022) 关于“快捷自动机”的推测。

Result: 研究结果表明，大型语言模型主要建模的是其训练语料库，而非人类的语言能力。Transformer架构最多支持线性处理，而人类语言能力依赖超线性格式。LLMs可能扮演着一种“话语机器”的角色，能够基于给定上下文生成新语言。

Conclusion: 结论是，尽管LLMs并非表达人类内在状态的工具，而是通过不同方式学习使用语言的“话语机器”，但这并非一个贬低性的观点。语言不仅是表达内在状态的手段，也是一种能够根据上下文创造新语言的“话语机器”，LLMs也以其独特的方式掌握了这一点。

Abstract: What do large language models actually model? Do they tell us something about
human capacities, or are they models of the corpus we've trained them on? I
give a non-deflationary defence of the latter position. Cognitive science tells
us that linguistic capabilities in humans rely supralinear formats for
computation. The transformer architecture, by contrast, supports at best a
linear formats for processing. This argument will rely primarily on certain
invariants of the computational architecture of transformers. I then suggest a
positive story about what transformers are doing, focusing on Liu et al.
(2022)'s intriguing speculations about shortcut automata. I conclude with why I
don't think this is a terribly deflationary story. Language is not (just) a
means for expressing inner state but also a kind of 'discourse machine' that
lets us make new language given appropriate context. We have learned to use
this technology in one way; LLMs have also learned to use it too, but via very
different means.

</details>


### [138] [A New NMT Model for Translating Clinical Texts from English to Spanish](https://arxiv.org/abs/2508.18607)
*Rumeng Li,Xun Wang,Hong Yu*

Main category: cs.CL

TL;DR: NOOV是一种新型神经机器翻译（NMT）系统，旨在将电子健康记录（EHR）叙述从英语翻译成西班牙语。它通过整合双语词典和短语查找表来解决未知词和词语重复问题，从而在少量领域内平行语料库的情况下提高翻译的准确性和流畅性。


<details>
  <summary>Details</summary>
Motivation: 将电子健康记录（EHR）叙述从英语翻译成西班牙语具有重要的临床意义，但由于缺乏平行对齐语料库和存在大量未知词，这项任务极具挑战性。

Method: 本文提出了NOOV（No OOV），一种新的神经机器翻译（NMT）系统。NOOV集成了一个从平行对齐语料库中自动学习的双语词典和一个从大型生物医学知识资源中提取的短语查找表，以缓解NMT中的未知词问题和词语重复挑战，从而增强NMT系统的短语生成能力。

Result: 评估结果表明，NOOV能够生成更好的EHR翻译，并在准确性和流畅性方面都有所提升。

Conclusion: NOOV系统通过集成双语词典和短语查找表，有效解决了EHR翻译中未知词和词语重复的挑战，显著提高了翻译的准确性和流畅性，即使在领域内平行语料库较少的情况下也能表现出色。

Abstract: Translating electronic health record (EHR) narratives from English to Spanish
is a clinically important yet challenging task due to the lack of a
parallel-aligned corpus and the abundant unknown words contained. To address
such challenges, we propose \textbf{NOOV} (for No OOV), a new neural machine
translation (NMT) system that requires little in-domain parallel-aligned corpus
for training. NOOV integrates a bilingual lexicon automatically learned from
parallel-aligned corpora and a phrase look-up table extracted from a large
biomedical knowledge resource, to alleviate both the unknown word problem and
the word-repeat challenge in NMT, enhancing better phrase generation of NMT
systems. Evaluation shows that NOOV is able to generate better translation of
EHR with improvement in both accuracy and fluency.

</details>


### [139] [Scaling Laws for Task-Stratified Knowledge in Post-Training Quantized Large Language Models](https://arxiv.org/abs/2508.18609)
*Chenxi Zhou,Pengfei Cao,Jiang Li,Jun Zhao,Kang Liu*

Main category: cs.CL

TL;DR: 本文通过实证研究，建立了量化LLM（大型语言模型）的任务分层缩放定律，并发现知识记忆能力对量化参数更敏感，而知识利用能力更鲁棒。


<details>
  <summary>Details</summary>
Motivation: 大规模LLM的部署面临挑战，PTQ（训练后量化）是实用压缩方案。但目前对PTQ如何影响LLM知识能力的理解不全面，现有量化模型缩放定律忽略了PTQ特有参数和任务敏感性。

Method: 进行了广泛的实证研究，建立了任务分层缩放定律。将LLM知识分解为记忆能力和利用能力，并开发了一个统一的量化框架，该框架整合了模型大小、有效位宽、校准集大小和组大小等参数。

Result: 核心发现是，知识记忆能力对有效位宽、校准集大小和模型大小的变化表现出显著更高的敏感性，而知识利用能力则更为鲁棒。

Conclusion: 这些发现提供了对PTQ影响的细致理解，并为开发能够更好保留目标认知功能的“知识感知”量化策略提供了指导。

Abstract: Large language models (LLMs) present significant deployment challenges due to
their scale, with post-training quantization (PTQ) emerging as a practical
compression solution. However, a comprehensive understanding of how PTQ
precisely impacts diverse LLM knowledge capabilities remains elusive, and
existing scaling laws for quantized models often overlook crucial PTQ-specific
parameters and task-specific sensitivities. This paper addresses these gaps by
conducting an extensive empirical investigation to establish task-stratified
scaling laws. We disentangle LLM knowledge into memorization and utilization
capabilities and develop a unified quantitative framework that incorporates
model size, effective bit-width, calibration set size, and group size. Our
central finding reveals that knowledge memorization exhibits markedly greater
sensitivity to variations in effective bit-width, calibration set size, and
model size compared to the more robust knowledge utilization. These findings
offer a fine-grained understanding of PTQ's impact and provide guidance for
developing knowledge-aware quantization strategies that can better preserve
targeted cognitive functions.

</details>


### [140] [Thinking Before You Speak: A Proactive Test-time Scaling Approach](https://arxiv.org/abs/2508.18648)
*Cong Li,Wenchang Chai,Hejun Wu,Yan Pan,Pengxu Wei,Liang Lin*

Main category: cs.CL

TL;DR: 针对大型语言模型在复杂推理任务（如数学）上的不足，本文提出“言前思之”（TBYS）框架。该框架通过在推理步骤之间主动插入“洞察”（insight）来指导模型，这些洞察回顾当前状态并启动下一步，从而弥补了人类训练数据中缺失的内部思考过程，并在数学数据集上取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在复杂推理任务（如数学）上表现不佳，原因在于人类在解决复杂问题时，通常会先进行深入思考（即产生“洞察”），但这些内部思考过程和方法往往不会在训练数据中明确表达出来，导致模型训练数据与人类推理模式之间存在差异，缺乏连接推理步骤的关键见解。

Method: 本文提出在连续推理步骤之间插入“洞察”（insight）。这些“洞察”用于回顾当前状态并启动下一个推理步骤。与以往依赖静态提示策略不同，“洞察”是主动生成的，旨在指导推理过程。该方法被实现为“言前思之”（Thinking Before You Speak, TBYS）推理框架，并设计了一个自动化管道，用于收集和过滤用于生成“洞察”的上下文示例，从而减少了人工标注和微调的开销。

Result: 在具有挑战性的数学数据集上的实验验证了TBYS框架的有效性。

Conclusion: 通过主动生成并插入“洞察”来指导推理过程，TBYS框架有效解决了大型语言模型在复杂推理任务中的不足，并在数学问题上展现了其有效性，弥补了训练数据中人类内部思考的缺失。

Abstract: Large Language Models (LLMs) often exhibit deficiencies with complex
reasoning tasks, such as maths, which we attribute to the discrepancy between
human reasoning patterns and those presented in the LLMs' training data. When
dealing with complex problems, humans tend to think carefully before expressing
solutions. However, they often do not articulate their inner thoughts,
including their intentions and chosen methodologies. Consequently, critical
insights essential for bridging reasoning steps may be absent in training data
collected from human sources. To bridge this gap, we proposes inserting
\emph{insight}s between consecutive reasoning steps, which review the status
and initiate the next reasoning steps. Unlike prior prompting strategies that
rely on a single or a workflow of static prompts to facilitate reasoning,
\emph{insight}s are \emph{proactively} generated to guide reasoning processes.
We implement our idea as a reasoning framework, named \emph{Thinking Before You
Speak} (TBYS), and design a pipeline for automatically collecting and filtering
in-context examples for the generation of \emph{insight}s, which alleviates
human labeling efforts and fine-tuning overheads. Experiments on challenging
mathematical datasets verify the effectiveness of TBYS. Project website:
https://gitee.com/jswrt/TBYS

</details>


### [141] [Breaking the Trade-Off Between Faithfulness and Expressiveness for Large Language Models](https://arxiv.org/abs/2508.18651)
*Chenxu Yang,Qingyi Si,Zheng Lin*

Main category: cs.CL

TL;DR: 本文提出协作解码（CoDe）框架，通过动态整合有无外部知识的输出概率，并结合知识感知重排序机制，有效解决大型语言模型在利用外部知识时忠实性和表达性之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在整合外部知识时，难以同时保持输出的忠实性（即与知识一致）和表达性（即自然流畅），导致输出要么缺乏外部知识支持（不忠实），要么过于冗长不自然（表达性差）。

Method: 本文提出协作解码（CoDe），一种动态整合有外部知识和无外部知识生成的输出概率的方法。该整合由分布散度（distribution divergence）和模型置信度（model confidence）引导，以选择性激活模型内部参数中相关且可靠的表达。此外，还引入了知识感知重排序机制，以防止过度依赖模型先验参数知识，并确保外部信息的正确利用。

Result: CoDe框架在不损害表达性的前提下，显著提高了大型语言模型的忠实性。通过广泛的实验，该即插即用框架在不同的LLM和评估指标上都表现出卓越的性能，验证了其有效性和通用性。

Conclusion: CoDe是一个有效且通用的即插即用框架，能够帮助大型语言模型在利用外部知识时，打破忠实性和表达性之间的权衡，实现两者的协同提升。

Abstract: Grounding responses in external knowledge represents an effective strategy
for mitigating hallucinations in Large Language Models (LLMs). However, current
LLMs struggle to seamlessly integrate knowledge while simultaneously
maintaining faithfulness (or fidelity) and expressiveness, capabilities that
humans naturally possess. This limitation results in outputs that either lack
support from external knowledge, thereby compromising faithfulness, or appear
overly verbose and unnatural, thus sacrificing expressiveness. In this work, to
break the trade-off between faithfulness and expressiveness, we propose
Collaborative Decoding (CoDe), a novel approach that dynamically integrates
output probabilities generated with and without external knowledge. This
integration is guided by distribution divergence and model confidence, enabling
the selective activation of relevant and reliable expressions from the model's
internal parameters. Furthermore, we introduce a knowledge-aware reranking
mechanism that prevents over-reliance on prior parametric knowledge while
ensuring proper utilization of provided external information. Through
comprehensive experiments, our plug-and-play CoDe framework demonstrates
superior performance in enhancing faithfulness without compromising
expressiveness across diverse LLMs and evaluation metrics, validating both its
effectiveness and generalizability.

</details>


### [142] [Emotion Omni: Enabling Empathetic Speech Response Generation through Large Language Models](https://arxiv.org/abs/2508.18655)
*Haoyu Wang,Guangyan Zhang,Jiale Chen,Jingyu Li,Yuehai Wang,Yiwen Guo*

Main category: cs.CL

TL;DR: 该论文提出了一种名为 Emotion Omni 的新型模型架构，旨在理解用户语音输入中的情感并生成共情语音响应。同时，开发了一个数据生成管道，构建了一个20万条情感对话数据集，以解决在有限数据下开发共情语音LLM的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有语音大语言模型（speech LLMs）未能充分理解用户语音输入中丰富的情感和副语言线索，导致响应缺乏共情，影响用户体验。此外，当前具备共情能力的语音LLMs大多依赖海量数据集进行训练，需要巨大的数据和计算资源，因此如何在有限数据和无需大规模训练的情况下开发共情语音LLM是一个关键挑战。

Method: 提出了一种名为 Emotion Omni 的新型模型架构，用于理解用户语音输入的情感内容并生成共情语音响应。此外，开发了一个基于开源TTS框架的数据生成管道，以构建一个包含20万条情感对话的数据集，用于支持共情语音助手的开发。

Result: 成功构建了一个包含20万条情感对话的数据集，该数据集支持共情语音助手的构建。所提出的Emotion Omni模型架构能够理解用户语音输入的情感并生成共情语音响应。

Conclusion: 通过提出Emotion Omni模型架构和开发数据生成管道，该研究为在有限数据和计算资源下构建具有共情能力的语音大语言模型提供了一种有效解决方案，从而显著提升了人机交互的用户体验。

Abstract: With the development of speech large language models (speech LLMs), users can
now interact directly with assistants via speech. However, most existing models
simply convert the response content into speech without fully understanding the
rich emotional and paralinguistic cues embedded in the user's query. In many
cases, the same sentence can have different meanings depending on the emotional
expression. Furthermore, emotional understanding is essential for improving
user experience in human-machine interaction. Currently, most speech LLMs with
empathetic capabilities are trained on massive datasets. This approach requires
vast amounts of data and significant computational resources. Therefore, a key
challenge lies in how to develop a speech LLM capable of generating empathetic
responses with limited data and without the need for large-scale training. To
address this challenge, we propose Emotion Omni, a novel model architecture
designed to understand the emotional content of user speech input and generate
empathetic speech responses. Additionally, we developed a data generation
pipeline based on an open-source TTS framework to construct a 200k emotional
dialogue dataset, which supports the construction of an empathetic speech
assistant. The demos are available at https://w311411.github.io/omni_demo/

</details>


### [143] [Tailored Teaching with Balanced Difficulty: Elevating Reasoning in Multimodal Chain-of-Thought via Prompt Curriculum](https://arxiv.org/abs/2508.18673)
*Xinglong Yang,Quan Feng,Zhongying Pan,Xiang Chen,Yu Tian,Wentong Li,Shuofei Qiao,Yuxia Geng,Xingyu Zhao,Sheng-Jun Huang*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的框架，通过结合模型感知难度和固有样本复杂度，设计难度平衡的采样策略，以改进多模态思维链（MCoT）提示的例子选择，从而提升多模态推理性能。


<details>
  <summary>Details</summary>
Motivation: 多模态思维链（MCoT）提示的效果常受限于随机或手动选择的例子，这些例子未能考虑模型知识分布和任务内在复杂性，导致模型性能不佳且不稳定。

Method: 受“因材施教与难度平衡”教学原则启发，将提示选择重构为提示课程设计问题。该方法整合了两种互补信号：1) 模型感知难度（通过主动学习设置中的预测分歧量化）；2) 固有样本复杂度（衡量每个问题-图像对的内在难度）。通过联合分析这些信号，开发出一种难度平衡的采样策略，确保所选提示例子在两个维度上都具有多样性。

Result: 在五个具有挑战性的基准测试和多个流行的多模态大型语言模型（MLLMs）上进行的广泛实验表明，该方法带来了显著且持续的改进，并大大减少了随机采样导致的性能差异。

Conclusion: 该方法为增强多模态推理提供了一种原则性且稳健的方法，通过改进提示例子选择，显著提升了多模态思维链的性能和稳定性。

Abstract: The effectiveness of Multimodal Chain-of-Thought (MCoT) prompting is often
limited by the use of randomly or manually selected examples. These examples
fail to account for both model-specific knowledge distributions and the
intrinsic complexity of the tasks, resulting in suboptimal and unstable model
performance. To address this, we propose a novel framework inspired by the
pedagogical principle of "tailored teaching with balanced difficulty". We
reframe prompt selection as a prompt curriculum design problem: constructing a
well ordered set of training examples that align with the model's current
capabilities. Our approach integrates two complementary signals: (1)
model-perceived difficulty, quantified through prediction disagreement in an
active learning setup, capturing what the model itself finds challenging; and
(2) intrinsic sample complexity, which measures the inherent difficulty of each
question-image pair independently of any model. By jointly analyzing these
signals, we develop a difficulty-balanced sampling strategy that ensures the
selected prompt examples are diverse across both dimensions. Extensive
experiments conducted on five challenging benchmarks and multiple popular
Multimodal Large Language Models (MLLMs) demonstrate that our method yields
substantial and consistent improvements and greatly reduces performance
discrepancies caused by random sampling, providing a principled and robust
approach for enhancing multimodal reasoning.

</details>


### [144] [Knowing or Guessing? Robust Medical Visual Question Answering via Joint Consistency and Contrastive Learning](https://arxiv.org/abs/2508.18687)
*Songtao Jiang,Yuxi Chen,Sibo Song,Yan Zhang,Yeying Jin,Yang Feng,Jian Wu,Zuozhu Liu*

Main category: cs.CL

TL;DR: 研究发现现有医学视觉-语言模型（Med-VLMs）在医学视觉问答中对问题复述表现出脆弱性，导致答案不一致。为此，本文构建了RoMed数据集以揭示这一问题，并提出了结合知识锚定一致性学习和偏置感知对比学习的CCL方法，显著提升了模型的答案一致性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在高风险的医疗应用中，模型对不同措辞的提问应保持一致的回答，以确保诊断的可靠性。然而，现有Med-VLMs在面对语义等价的医疗问题复述时，答案波动显著，表现出脆弱性。这主要归因于医学概念对齐不足和训练数据中隐藏的偏置。

Method: 1. 构建了RoMed数据集，包含14.4万个问题，涵盖词级、句级和语义级的变体，用于评估模型对问题复述的鲁棒性。2. 提出了“一致性与对比学习”（CCL）方法，包含两个核心组件：a) 知识锚定一致性学习，使Med-VLMs与医学知识对齐；b) 偏置感知对比学习，通过判别性表示细化来缓解数据特异性先验。

Result: 1. 在RoMed数据集上评估时，LLaVA-Med等最先进模型表现出惊人的性能下降（例如，召回率下降40%），暴露了严重的鲁棒性差距。2. CCL方法在三个流行的VQA基准测试上达到了最先进的性能。3. CCL在具有挑战性的RoMed测试集上将答案一致性提高了50%，显著增强了模型的鲁棒性。

Conclusion: 现有Med-VLMs在医学视觉问答中对问题复述的脆弱性是一个关键问题，RoMed数据集有效揭示了这一点。所提出的CCL方法通过解决概念对齐不足和数据偏置问题，显著提升了Med-VLMs的答案一致性和鲁棒性，使其更适用于高风险医疗应用。

Abstract: In high-stakes medical applications, consistent answering across diverse
question phrasings is essential for reliable diagnosis. However, we reveal that
current Medical Vision-Language Models (Med-VLMs) exhibit concerning fragility
in Medical Visual Question Answering, as their answers fluctuate significantly
when faced with semantically equivalent rephrasings of medical questions. We
attribute this to two limitations: (1) insufficient alignment of medical
concepts, leading to divergent reasoning patterns, and (2) hidden biases in
training data that prioritize syntactic shortcuts over semantic understanding.
To address these challenges, we construct RoMed, a dataset built upon original
VQA datasets containing 144k questions with variations spanning word-level,
sentence-level, and semantic-level perturbations. When evaluating
state-of-the-art (SOTA) models like LLaVA-Med on RoMed, we observe alarming
performance drops (e.g., a 40\% decline in Recall) compared to original VQA
benchmarks, exposing critical robustness gaps. To bridge this gap, we propose
Consistency and Contrastive Learning (CCL), which integrates two key
components: (1) knowledge-anchored consistency learning, aligning Med-VLMs with
medical knowledge rather than shallow feature patterns, and (2) bias-aware
contrastive learning, mitigating data-specific priors through discriminative
representation refinement. CCL achieves SOTA performance on three popular VQA
benchmarks and notably improves answer consistency by 50\% on the challenging
RoMed test set, demonstrating significantly enhanced robustness. Code will be
released.

</details>


### [145] [Attention2Probability: Attention-Driven Terminology Probability Estimation for Robust Speech-to-Text System](https://arxiv.org/abs/2508.18701)
*Yanfan Du,Jun Zhang,Bin Wang,Jin Qiu,Lu Huang,Yuan Ge,Xiaoqian Liu,Tong Xiao,Jingbo Zhu*

Main category: cs.CL

TL;DR: 本文提出Attention2Probability，一种轻量、灵活、准确的方法，通过将语音和术语间的交叉注意力权重转换为存在概率，并结合课程学习，有效解决了语音大模型在处理领域特定术语时的挑战。该方法显著提高了术语召回率和识别准确性，并发布了新的带术语语音数据集。


<details>
  <summary>Details</summary>
Motivation: 尽管语音大模型（SLMs）在通用领域的语音识别和翻译方面取得了进展，但在准确生成领域特定术语或新词方面仍面临挑战。

Method: 本文提出了Attention2Probability方法，它将语音和术语之间的交叉注意力权重转换为术语存在概率，并通过课程学习来提高检索准确性。此外，为了解决带术语干预的语音到文本任务数据不足的问题，本文创建并发布了一个新的带术语的语音数据集。

Result: 实验结果表明，Attention2Probability在测试集上显著优于VectorDB方法。其最大召回率中文达到92.57%，英文达到86.83%，且每次查询的延迟仅为8.71毫秒。通过Attention2Probability检索到的术语干预SLMs的识别和翻译任务，术语准确性提高了6-17%，同时也揭示了当前SLMs在术语利用方面存在的局限性。

Conclusion: Attention2Probability是一种高效、轻量且准确的解决方案，能显著提升语音大模型处理领域特定术语的能力，具有高召回率和低延迟的优点。该研究还通过发布新数据集和揭示SLM局限性，为未来研究指明了方向。

Abstract: Recent advances in speech large language models (SLMs) have improved speech
recognition and translation in general domains, but accurately generating
domain-specific terms or neologisms remains challenging. To address this, we
propose Attention2Probability: attention-driven terminology probability
estimation for robust speech-to-text system, which is lightweight, flexible,
and accurate. Attention2Probability converts cross-attention weights between
speech and terminology into presence probabilities, and it further employs
curriculum learning to enhance retrieval accuracy. Furthermore, to tackle the
lack of data for speech-to-text tasks with terminology intervention, we create
and release a new speech dataset with terminology to support future research in
this area. Experimental results show that Attention2Probability significantly
outperforms the VectorDB method on our test set. Specifically, its maximum
recall rates reach 92.57% for Chinese and 86.83% for English. This high recall
is achieved with a latency of only 8.71ms per query. Intervening in SLMs'
recognition and translation tasks using Attention2Probability-retrieved terms
improves terminology accuracy by 6-17%, while revealing that the current
utilization of terminology by SLMs has limitations.

</details>


### [146] [Filtering for Creativity: Adaptive Prompting for Multilingual Riddle Generation in LLMs](https://arxiv.org/abs/2508.18709)
*Duy Le,Kent Ziti,Evan Girard-Sun,Sean O'Brien,Vasu Sharma,Kevin Zhu*

Main category: cs.CL

TL;DR: 该研究引入了自适应原创性过滤（AOF）框架，通过语义拒绝和词汇新颖性来提高大型语言模型（LLMs）在多语言谜语生成中的原创性和多样性，避免了重复和浅层复述。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在多语言谜语生成中面临挑战，难以平衡文化流畅性和创意抽象，且标准提示策略（零样本、少样本、思维链）倾向于重复记忆的谜语或进行浅层复述。

Method: 研究引入了自适应原创性过滤（AOF）提示框架。该框架通过基于余弦相似度的拒绝机制过滤冗余生成，同时强制执行词汇新颖性和跨语言忠实性。该方法在三种LLM和四种语言对上进行了评估。

Result: 经AOF增强的GPT-4o在日语中实现了0.177的Self-BLEU和0.915的Distinct-2，这表明与其他提示方法和语言对相比，其词汇多样性有所改善，冗余度有所降低。

Conclusion: 研究结果表明，语义拒绝（如AOF）可以在无需任务特定微调的情况下，指导大型语言模型进行具有文化基础的创意生成。

Abstract: Multilingual riddle generation challenges large language models (LLMs) to
balance cultural fluency with creative abstraction. Standard prompting
strategies -- zero-shot, few-shot, chain-of-thought -- tend to reuse memorized
riddles or perform shallow paraphrasing. We introduce Adaptive Originality
Filtering (AOF), a prompting framework that filters redundant generations using
cosine-based similarity rejection, while enforcing lexical novelty and
cross-lingual fidelity. Evaluated across three LLMs and four language pairs,
AOF-enhanced GPT-4o achieves \texttt{0.177} Self-BLEU and \texttt{0.915}
Distinct-2 in Japanese, signaling improved lexical diversity and reduced
redundancy compared to other prompting methods and language pairs. Our findings
show that semantic rejection can guide culturally grounded, creative generation
without task-specific fine-tuning.

</details>


### [147] [EMMM, Explain Me My Model! Explainable Machine Generated Text Detection in Dialogues](https://arxiv.org/abs/2508.18715)
*Angela Yifei Yuan,Haoyi Li,Soyeon Caren Han,Christopher Leckie*

Main category: cs.CL

TL;DR: 该论文提出EMMM，一个“先解释后检测”的框架，用于客户服务场景中的机器生成文本（MGT）检测。该框架在保证低延迟和高准确性的同时，为非专业用户提供了可理解的解释。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在客户服务中的广泛应用带来了新风险，恶意行为者可利用其进行大规模用户冒充。现有MGT检测方法在在线对话环境中效果不佳，缺乏可靠性和可解释性。对于客户服务中通常是非专业人员的操作员来说，可解释性对于可信赖的MGT检测至关重要。

Method: 本文提出了EMMM，一个“先解释后检测”的框架。该框架旨在平衡延迟、准确性和面向非专业人员的可解释性。

Result: 实验结果表明，EMMM提供的解释对非专业用户易于理解，70%的人类评估者更喜欢其输出。同时，它在准确性方面与最先进的模型具有竞争力，并保持低延迟，在1秒内生成输出。

Conclusion: EMMM框架成功地为客户服务场景中的MGT检测提供了可信赖的解决方案，它在满足非专业用户对可解释性需求的同时，兼顾了高准确性和低延迟。

Abstract: The rapid adoption of large language models (LLMs) in customer service
introduces new risks, as malicious actors can exploit them to conduct
large-scale user impersonation through machine-generated text (MGT). Current
MGT detection methods often struggle in online conversational settings,
reducing the reliability and interpretability essential for trustworthy AI
deployment. In customer service scenarios where operators are typically
non-expert users, explanation become crucial for trustworthy MGT detection. In
this paper, we propose EMMM, an explanation-then-detection framework that
balances latency, accuracy, and non-expert-oriented interpretability.
Experimental results demonstrate that EMMM provides explanations accessible to
non-expert users, with 70\% of human evaluators preferring its outputs, while
achieving competitive accuracy compared to state-of-the-art models and
maintaining low latency, generating outputs within 1 second. Our code and
dataset are open-sourced at
https://github.com/AngieYYF/EMMM-explainable-chatbot-detection.

</details>


### [148] [Beyond Quality: Unlocking Diversity in Ad Headline Generation with Large Language Models](https://arxiv.org/abs/2508.18739)
*Chang Wang,Siyu Yan,Depeng Yuan,Yuqi Chen,Yanhua Huang,Yuanhang Zheng,Shuhao Li,Yinqi Zhang,Kedi Chen,Mingrui Zhu,Ruiwen Xu*

Main category: cs.CL

TL;DR: DIVER是一个基于大型语言模型（LLM）的框架，通过语义和风格感知的数据生成以及多阶段多目标优化，旨在同时提高广告标题的质量和多样性，并在实际部署中取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 当前广告标题生成方法主要优化质量或点击率，但忽视了多样性需求，导致输出同质化，无法吸引广泛受众。

Method: 该研究首先设计了一个语义和风格感知的DIVER数据生成流程，自动产生包含广告内容和多个多样化标题的高质量训练对。然后，提出了一个结合监督微调（SFT）和强化学习（RL）的多阶段多目标优化框架，以在单次前向传播中生成高质量且多样化的广告标题。

Result: 在真实工业数据集上的实验表明，DIVER有效平衡了标题质量和多样性。部署在一个服务数亿用户的大规模内容共享平台后，该框架将广告主价值（ADVV）提高了4.0%，点击率（CTR）提高了1.4%。

Conclusion: DIVER框架成功解决了广告标题生成中质量与多样性难以兼顾的问题，通过创新的数据生成和优化方法，显著提升了广告效果，并在实际应用中展现了商业价值。

Abstract: The generation of ad headlines plays a vital role in modern advertising,
where both quality and diversity are essential to engage a broad range of
audience segments. Current approaches primarily optimize language models for
headline quality or click-through rates (CTR), often overlooking the need for
diversity and resulting in homogeneous outputs. To address this limitation, we
propose DIVER, a novel framework based on large language models (LLMs) that are
jointly optimized for both diversity and quality. We first design a semantic-
and stylistic-aware data generation pipeline that automatically produces
high-quality training pairs with ad content and multiple diverse headlines. To
achieve the goal of generating high-quality and diversified ad headlines within
a single forward pass, we propose a multi-stage multi-objective optimization
framework with supervised fine-tuning (SFT) and reinforcement learning (RL).
Experiments on real-world industrial datasets demonstrate that DIVER
effectively balances quality and diversity. Deployed on a large-scale
content-sharing platform serving hundreds of millions of users, our framework
improves advertiser value (ADVV) and CTR by 4.0% and 1.4%.

</details>


### [149] [M3HG: Multimodal, Multi-scale, and Multi-type Node Heterogeneous Graph for Emotion Cause Triplet Extraction in Conversations](https://arxiv.org/abs/2508.18740)
*Qiao Liang,Ying Shen,Tiantian Chen,Lin Zhang*

Main category: cs.CL

TL;DR: 本文针对多模态对话中情感原因三元组抽取（MECTEC）任务，发布了首个多模态、多场景数据集MECAD，并提出了M3HG模型，通过多模态异构图显式建模情感和因果上下文，并融合不同层级的语义信息，取得了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: MECTEC任务面临两个主要挑战：1. 相关数据集稀缺，现有数据集场景单一，阻碍了模型发展。2. 现有MECTEC方法未能显式建模情感和因果上下文，且忽略了不同层级语义信息的融合，导致性能下降。

Method: 1. 引入了MECAD数据集，这是首个多模态、多场景的MECTEC数据集，包含来自56部电视剧的989个对话。2. 提出了M3HG模型，该模型通过构建多模态异构图，显式捕获情感和因果上下文，并有效融合了语篇内部和语篇之间的上下文信息。

Result: 广泛的实验证明，与现有最先进的方法相比，M3HG模型表现出卓越的有效性。

Conclusion: 本文通过发布首个多模态、多场景的MECTEC数据集MECAD，并提出能够有效建模上下文和融合多层级信息的M3HG模型，显著推动了多模态对话中情感原因三元组抽取任务的发展。

Abstract: Emotion Cause Triplet Extraction in Multimodal Conversations (MECTEC) has
recently gained significant attention in social media analysis, aiming to
extract emotion utterances, cause utterances, and emotion categories
simultaneously. However, the scarcity of related datasets, with only one
published dataset featuring highly uniform dialogue scenarios, hinders model
development in this field. To address this, we introduce MECAD, the first
multimodal, multi-scenario MECTEC dataset, comprising 989 conversations from 56
TV series spanning a wide range of dialogue contexts. In addition, existing
MECTEC methods fail to explicitly model emotional and causal contexts and
neglect the fusion of semantic information at different levels, leading to
performance degradation. In this paper, we propose M3HG, a novel model that
explicitly captures emotional and causal contexts and effectively fuses
contextual information at both inter- and intra-utterance levels via a
multimodal heterogeneous graph. Extensive experiments demonstrate the
effectiveness of M3HG compared with existing state-of-the-art methods. The
codes and dataset are available at https://github.com/redifinition/M3HG.

</details>


### [150] [Chronological Passage Assembling in RAG framework for Temporal Question Answering](https://arxiv.org/abs/2508.18748)
*Byeongjeong Kim,Jeonghyun Park,Joonho Yang,Hwanhee Lee*

Main category: cs.CL

TL;DR: ChronoRAG是一个针对叙事文本的长上下文问答的RAG框架，通过重构分散信息为连贯段落并明确保持时间顺序，显著提升了叙事问答的性能。


<details>
  <summary>Details</summary>
Motivation: 长上下文叙事问答面临挑战，因为正确答案依赖于重建事件时间线并在有限上下文窗口中保持上下文流畅性。现有RAG方法在处理叙事文本时效果有限，因为叙事文本的理解需要更广泛的上下文和段落间的顺序关系，而非孤立的片段。

Method: 提出ChronoRAG，一个专门用于叙事文本的新型RAG框架。该方法专注于两个关键方面：将分散的文档信息提炼成连贯且结构化的段落；通过明确捕获和维护检索到的段落之间的时间顺序来保留叙事流畅性。

Result: 在NarrativeQA数据集上的实验表明，ChronoRAG在需要事实识别和理解复杂序列关系的任务中表现出显著改进，证明了推理时间顺序对于解决叙事问答至关重要。

Conclusion: 在叙事问答中，对时间顺序进行推理对于解决问题至关重要，ChronoRAG通过其独特的方法有效地解决了这一挑战。

Abstract: Long-context question answering over narrative tasks is challenging because
correct answers often hinge on reconstructing a coherent timeline of events
while preserving contextual flow in a limited context window.
Retrieval-augmented generation (RAG) indexing methods aim to address this
challenge by selectively retrieving only necessary document segments. However,
narrative texts possess unique characteristics that limit the effectiveness of
these existing approaches. Specifically, understanding narrative texts requires
more than isolated segments, as the broader context and sequential
relationships between segments are crucial for comprehension. To address these
limitations, we propose ChronoRAG, a novel RAG framework specialized for
narrative texts. This approach focuses on two essential aspects: refining
dispersed document information into coherent and structured passages, and
preserving narrative flow by explicitly capturing and maintaining the temporal
order among retrieved passages. We empirically demonstrate the effectiveness of
ChronoRAG through experiments on the NarrativeQA dataset, showing substantial
improvements in tasks requiring both factual identification and comprehension
of complex sequential relationships, underscoring that reasoning over temporal
order is crucial in resolving narrative QA.

</details>


### [151] [ThinkDial: An Open Recipe for Controlling Reasoning Effort in Large Language Models](https://arxiv.org/abs/2508.18773)
*Qianyu He,Siyu Yuan,Xuefeng Li,Mingxuan Wang,Jiangjie Chen*

Main category: cs.CL

TL;DR: ThinkDial是一个端到端开源框架，通过离散操作模式实现了大型语言模型（LLMs）的可控推理，有效平衡了计算成本和性能。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs具有强大的问题解决能力，但其计算开销大，且开源社区缺乏类似OpenAI gpt-oss系列那样具有离散可控推理模式的能力，这限制了其实际部署。

Method: ThinkDial采用端到端训练范式，将预算模式控制整合到整个流程中。这包括：1) 预算模式监督微调（SFT），将可控推理能力直接嵌入学习过程；2) 两阶段预算感知强化学习，结合自适应奖励塑造。

Result: ThinkDial成功实现了三种推理模式：高模式（全推理能力），中模式（50% token减少，性能下降<10%），低模式（75% token减少，性能下降<15%）。该框架在保持性能阈值的同时，实现了目标压缩-性能权衡和响应长度的显著缩减，并展现出强大的域外任务泛化能力。

Conclusion: ThinkDial是首个开源、端到端实现gpt-oss风格可控推理的框架，通过离散操作模式有效管理LLMs的计算开销，同时保持高性能，解决了实际部署中的一个关键挑战。

Abstract: Large language models (LLMs) with chain-of-thought reasoning have
demonstrated remarkable problem-solving capabilities, but controlling their
computational effort remains a significant challenge for practical deployment.
Recent proprietary systems like OpenAI's gpt-oss series have introduced
discrete operational modes for intuitive reasoning control, but the open-source
community has largely failed to achieve such capabilities. In this paper, we
introduce ThinkDial, the first open-recipe end-to-end framework that
successfully implements gpt-oss-style controllable reasoning through discrete
operational modes. Our system enables seamless switching between three distinct
reasoning regimes: High mode (full reasoning capability), Medium mode (50
percent token reduction with <10 percent performance degradation), and Low mode
(75 percent token reduction with <15 percent performance degradation). We
achieve this through an end-to-end training paradigm that integrates
budget-mode control throughout the entire pipeline: budget-mode supervised
fine-tuning that embeds controllable reasoning capabilities directly into the
learning process, and two-phase budget-aware reinforcement learning with
adaptive reward shaping. Extensive experiments demonstrate that ThinkDial
achieves target compression-performance trade-offs with clear response length
reductions while maintaining performance thresholds. The framework also
exhibits strong generalization capabilities on out-of-distribution tasks.

</details>


### [152] [Harnessing Rule-Based Reinforcement Learning for Enhanced Grammatical Error Correction](https://arxiv.org/abs/2508.18780)
*Yilin Li,Xunjian Yin,Yilin Chen,Xiaojun Wan*

Main category: cs.CL

TL;DR: 本文提出了一种基于规则的强化学习（Rule-Based RL）框架，用于指导大型语言模型（LLMs）进行语法错误纠正（GEC），并在中文数据集上实现了最先进的性能，显著提高了召回率。


<details>
  <summary>Details</summary>
Motivation: 传统的编码器-解码器模型在GEC任务上取得了一定成功，但LLMs在该领域的应用仍未被充分探索。当前研究主要依赖于有监督微调让LLMs直接生成纠正后的句子，这限制了模型强大的推理能力。

Method: 提出了一种基于规则的强化学习（Rule-Based RL）新框架，用于引导LLMs执行GEC任务。

Result: 在中文数据集上，所提出的Rule-Based RL框架实现了最先进的性能（state-of-the-art），并显著提高了召回率。

Conclusion: 该研究结果清晰地强调了使用强化学习来引导LLMs的优势，为未来GEC领域的发展提供了一种更可控、更可靠的范式。

Abstract: Grammatical error correction is a significant task in NLP. Traditional
methods based on encoder-decoder models have achieved certain success, but the
application of LLMs in this field is still underexplored. Current research
predominantly relies on supervised fine-tuning to train LLMs to directly
generate the corrected sentence, which limits the model's powerful reasoning
ability. To address this limitation, we propose a novel framework based on
Rule-Based RL. Through experiments on the Chinese datasets, our Rule-Based RL
framework achieves \textbf{state-of-the-art }performance, with a notable
increase in \textbf{recall}. This result clearly highlights the advantages of
using RL to steer LLMs, offering a more controllable and reliable paradigm for
future development in GEC.

</details>


### [153] [Controllable Conversational Theme Detection Track at DSTC 12](https://arxiv.org/abs/2508.18783)
*Igor Shalyminov,Hang Su,Jake Vincent,Siffi Singh,Jason Cai,James Gung,Raphael Shu,Saab Mansour*

Main category: cs.CL

TL;DR: 本文介绍了对话分析中的“可控对话主题检测”任务，旨在自动识别对话主题并提供用户友好的总结。该任务被设计为DSTC 12的一项公开竞赛，结合了聚类和主题标注，并允许通过用户偏好数据控制主题粒度。


<details>
  <summary>Details</summary>
Motivation: 随着语音和自然语言处理技术（特别是大型语言模型）的进步，对话分析的自动化达到了新的复杂度和规模。现有的意图检测往往依赖固定意图集，而对话主题检测旨在直接为用户提供对话核心查询的灵活、用户导向的总结，以显著减少人工分析大量对话（如客户支持或销售）的努力。

Method: 本文将“可控对话主题检测”问题定义为DSTC 12的一项竞赛任务，其方法包括对话语篇的联合聚类和主题标注。一个独特之处是，通过提供的用户偏好数据，可以实现对生成主题簇粒度的可控性。论文还概述了问题、相关数据集以及自动和人工评估指标。

Result: 论文讨论了参赛队伍的提交结果，并从中提供了见解。相关的竞赛材料（数据和代码）已在GitHub仓库中公开。

Conclusion: 本文引入了一个对话分析中的关键任务——可控对话主题检测，并将其作为DSTC 12的竞赛赛道。该任务通过结合聚类和主题标注，并允许粒度控制，旨在提供更灵活、用户导向的对话总结，从而有效减少人工分析工作。竞赛结果和相关材料的公开将促进该领域的研究和发展。

Abstract: Conversational analytics has been on the forefront of transformation driven
by the advances in Speech and Natural Language Processing techniques. Rapid
adoption of Large Language Models (LLMs) in the analytics field has taken the
problems that can be automated to a new level of complexity and scale. In this
paper, we introduce Theme Detection as a critical task in conversational
analytics, aimed at automatically identifying and categorizing topics within
conversations. This process can significantly reduce the manual effort involved
in analyzing expansive dialogs, particularly in domains like customer support
or sales. Unlike traditional dialog intent detection, which often relies on a
fixed set of intents for downstream system logic, themes are intended as a
direct, user-facing summary of the conversation's core inquiry. This
distinction allows for greater flexibility in theme surface forms and
user-specific customizations. We pose Controllable Conversational Theme
Detection problem as a public competition track at Dialog System Technology
Challenge (DSTC) 12 -- it is framed as joint clustering and theme labeling of
dialog utterances, with the distinctive aspect being controllability of the
resulting theme clusters' granularity achieved via the provided user preference
data. We give an overview of the problem, the associated dataset and the
evaluation metrics, both automatic and human. Finally, we discuss the
participant teams' submissions and provide insights from those. The track
materials (data and code) are openly available in the GitHub repository.

</details>


### [154] [LaTeXTrans: Structured LaTeX Translation with Multi-Agent Coordination](https://arxiv.org/abs/2508.18791)
*Ziming Zhu,Chenglong Wang,Shunjie Xing,Yifu Huo,Fengning Tian,Quan Du,Di Yang,Chunliang Zhang,Tong Xiao,Jingbo Zhu*

Main category: cs.CL

TL;DR: 本文介绍了LaTeXTrans，一个协同多智能体系统，旨在解决LaTeX格式文档的翻译挑战，确保格式、结构和术语的准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管现代机器翻译系统在通用领域文本上取得了显著进展，但翻译结构化的LaTeX格式文档仍然是一个重大挑战。这类文档混合了自然语言和领域特定语法（如数学方程、表格、图表、交叉引用），这些都必须准确保留以维护语义完整性和可编译性。

Method: LaTeXTrans是一个协同多智能体系统，通过六个专业智能体实现格式保留、结构保真和术语一致性。这些智能体包括：1) 一个解析器，通过占位符替换和语法过滤将LaTeX分解为易于翻译的单元；2) 一个翻译器、验证器、摘要器和术语提取器，它们协同工作以确保上下文感知、自我纠正和术语一致的翻译；3) 一个生成器，将翻译后的内容重构为结构良好的LaTeX文档。

Result: 实验结果表明，LaTeXTrans在翻译准确性和结构保真度方面均优于主流机器翻译系统。

Conclusion: LaTeXTrans为翻译LaTeX格式文档提供了一个有效且实用的解决方案。

Abstract: Despite the remarkable progress of modern machine translation (MT) systems on
general-domain texts, translating structured LaTeX-formatted documents remains
a significant challenge. These documents typically interleave natural language
with domain-specific syntax, such as mathematical equations, tables, figures,
and cross-references, all of which must be accurately preserved to maintain
semantic integrity and compilability. In this paper, we introduce LaTeXTrans, a
collaborative multi-agent system designed to address this challenge. LaTeXTrans
ensures format preservation, structural fidelity, and terminology consistency
through six specialized agents: 1) a Parser that decomposes LaTeX into
translation-friendly units via placeholder substitution and syntax filtering;
2) a Translator, Validator, Summarizer, and Terminology Extractor that work
collaboratively to ensure context-aware, self-correcting, and
terminology-consistent translations; 3) a Generator that reconstructs the
translated content into well-structured LaTeX documents. Experimental results
demonstrate that LaTeXTrans can outperform mainstream MT systems in both
translation accuracy and structural fidelity, offering an effective and
practical solution for translating LaTeX-formatted documents.

</details>


### [155] [LLM-based Contrastive Self-Supervised AMR Learning with Masked Graph Autoencoders for Fake News Detection](https://arxiv.org/abs/2508.18819)
*Shubham Gupta,Shraban Kumar Chatterjee,Suman Kundu*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的自监督虚假信息检测框架，结合了抽象意义表示（AMR）的复杂语义关系和新闻传播动态。该框架利用基于大型语言模型（LLM）的图对比损失（LGCL）进行零样本学习，并通过多视图图掩码自编码器整合社交上下文，实现了卓越的性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 数字时代虚假信息的泛滥带来了严重的社会挑战。现有方法在捕获长距离依赖、复杂语义关系和新闻传播的社会动态方面存在困难，并且需要大量的标注数据集，导致部署资源密集。

Method: 研究提出一个自监督虚假信息检测框架：1. 利用抽象意义表示（AMR）捕捉复杂语义关系。2. 引入基于LLM的图对比损失（LGCL），通过LLM生成的负锚点增强特征可分离性，实现零样本学习。3. 采用多视图图掩码自编码器从社交上下文图中学习新闻传播特征。4. 结合语义和传播特征来区分虚假和真实新闻。

Result: 实验结果表明，所提出的自监督框架即使在有限的标注数据集下，也比其他最先进的方法表现出优越的性能，并提高了泛化能力。

Conclusion: 该自监督框架通过整合复杂语义关系和新闻传播动态，并利用LLM增强的对比学习，能够有效地区分虚假新闻和真实新闻，展现出卓越的性能和更好的泛化能力，尤其适用于标注数据有限的场景。

Abstract: The proliferation of misinformation in the digital age has led to significant
societal challenges. Existing approaches often struggle with capturing
long-range dependencies, complex semantic relations, and the social dynamics
influencing news dissemination. Furthermore, these methods require extensive
labelled datasets, making their deployment resource-intensive. In this study,
we propose a novel self-supervised misinformation detection framework that
integrates both complex semantic relations using Abstract Meaning
Representation (AMR) and news propagation dynamics. We introduce an LLM-based
graph contrastive loss (LGCL) that utilizes negative anchor points generated by
a Large Language Model (LLM) to enhance feature separability in a zero-shot
manner. To incorporate social context, we employ a multi view graph masked
autoencoder, which learns news propagation features from social context graph.
By combining these semantic and propagation-based features, our approach
effectively differentiates between fake and real news in a self-supervised
manner. Extensive experiments demonstrate that our self-supervised framework
achieves superior performance compared to other state-of-the-art methodologies,
even with limited labelled datasets while improving generalizability.

</details>


### [156] [Arrows of Math Reasoning Data Synthesis for Large Language Models: Diversity, Complexity and Correctness](https://arxiv.org/abs/2508.18824)
*Sirui Chen,Changxin Tian,Binbin Hu,Kunlong Chen,Ziqi Liu,Zhiqiang Zhang,Jun Zhou*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的程序辅助合成框架，用于系统地生成高质量、多样、复杂且正确的数学语料库，以提升大型语言模型的数学推理能力。


<details>
  <summary>Details</summary>
Motivation: 传统生成数学训练数据的方法面临可扩展性、成本和数据可靠性方面的挑战，而提升大型语言模型（LLMs）的数学推理能力需要高质量的训练数据。

Method: 该框架整合数学知识系统和领域特定工具来创建可执行程序。这些程序随后被翻译成自然语言的问题-解决方案对，并通过双边验证机制进行审查，以验证解决方案的正确性和程序与问题的一致性。

Result: 研究生成了1230万个问题-解决方案三元组。在此数据上进行微调的模型显著提高了推理能力，在多个基准数据集上达到了最先进的性能。

Conclusion: 所提出的合成方法有效提升了大型语言模型的数学推理能力，并证实了其生成高质量训练数据的有效性。

Abstract: Enhancing the mathematical reasoning of large language models (LLMs) demands
high-quality training data, yet conventional methods face critical challenges
in scalability, cost, and data reliability. To address these limitations, we
propose a novel program-assisted synthesis framework that systematically
generates a high-quality mathematical corpus with guaranteed diversity,
complexity, and correctness. This framework integrates mathematical knowledge
systems and domain-specific tools to create executable programs. These programs
are then translated into natural language problem-solution pairs and vetted by
a bilateral validation mechanism that verifies solution correctness against
program outputs and ensures program-problem consistency. We have generated 12.3
million such problem-solving triples. Experiments demonstrate that models
fine-tuned on our data significantly improve their inference capabilities,
achieving state-of-the-art performance on several benchmark datasets and
showcasing the effectiveness of our synthesis approach.

</details>


### [157] [ConfTuner: Training Large Language Models to Express Their Confidence Verbally](https://arxiv.org/abs/2508.18847)
*Yibo Li,Miao Xiong,Jiaying Wu,Bryan Hooi*

Main category: cs.CL

TL;DR: ConfTuner是一种简单高效的微调方法，它引入了理论上正确的“tokenized Brier score”损失函数，以改善大型语言模型（LLMs）的置信度校准，从而提高LLM在各种推理任务中的可靠性和可信度。


<details>
  <summary>Details</summary>
Motivation: LLMs在科学、法律和医疗等高风险领域部署，对不确定性的准确表达至关重要。然而，LLMs常表现出“过度自信”，即以高置信度生成错误答案。现有校准方法（提示工程或启发式微调）效果和泛化能力有限。

Method: 引入ConfTuner，一种简单高效的微调方法，开销极小，不依赖真实置信度分数或代理置信度估计。它使用新的损失函数——tokenized Brier score，该函数被理论证明是一个适当的评分规则，能正确激励模型报告其真实正确概率。

Result: ConfTuner显著改善了LLM在各种推理任务中的置信度校准。它能泛化到GPT-4o等黑盒模型。此外，更好的校准置信度还能带来下游任务（如自我纠正和模型级联）的性能提升。

Conclusion: ConfTuner通过提高LLM的校准置信度，促进了可信赖LLM系统的发展，使其在关键应用中更可靠。

Abstract: Large Language Models (LLMs) are increasingly deployed in high-stakes domains
such as science, law, and healthcare, where accurate expressions of uncertainty
are essential for reliability and trust. However, current LLMs are often
observed to generate incorrect answers with high confidence, a phenomenon known
as "overconfidence". Recent efforts have focused on calibrating LLMs'
verbalized confidence: i.e., their expressions of confidence in text form, such
as "I am 80% confident that...". Existing approaches either rely on prompt
engineering or fine-tuning with heuristically generated uncertainty estimates,
both of which have limited effectiveness and generalizability. Motivated by the
notion of proper scoring rules for calibration in classical machine learning
models, we introduce ConfTuner, a simple and efficient fine-tuning method that
introduces minimal overhead and does not require ground-truth confidence scores
or proxy confidence estimates. ConfTuner relies on a new loss function,
tokenized Brier score, which we theoretically prove to be a proper scoring
rule, intuitively meaning that it "correctly incentivizes the model to report
its true probability of being correct". ConfTuner improves calibration across
diverse reasoning tasks and generalizes to black-box models such as GPT-4o. Our
results further show that better-calibrated confidence enables downstream gains
in self-correction and model cascade, advancing the development of trustworthy
LLM systems. The code is available at
https://github.com/liushiliushi/ConfTuner.

</details>


### [158] [ReflectivePrompt: Reflective evolution in autoprompting algorithms](https://arxiv.org/abs/2508.18870)
*Viktor N. Zhuravlev,Artur R. Khairullin,Ernest A. Dyagin,Alena N. Sitkina,Nikita I. Kulin*

Main category: cs.CL

TL;DR: 本文提出了一种名为ReflectivePrompt的新型自动提示方法，它基于进化算法并采用反射式进化策略，通过短期和长期反射操作来优化提示选择，并在多个任务和LLM上取得了显著优于现有SOTA方法的性能提升。


<details>
  <summary>Details</summary>
Motivation: 随着提示工程和大型语言模型（LLMs）的快速发展，自动选择优化提示（即自动提示）的需求日益增长，驱动了对更精确、更全面的最佳提示搜索方法的研究。

Method: 本文提出ReflectivePrompt方法，这是一种基于进化算法的自动提示方法，采用反射式进化策略。它在交叉和精英突变之前引入了短期和长期反射操作，以提高修改质量。该方法能累积进化过程中获得的知识，并根据当前种群在每个周期进行更新。

Result: ReflectivePrompt在33个分类和文本生成数据集上，使用t-lite-instruct-0.1和gemma3-27b-it等开源大型语言模型进行了测试。结果显示，该方法相对于当前最先进的方法（例如，在BBH上比EvoPrompt平均提升28%）取得了显著改进。

Conclusion: ReflectivePrompt被证明是基于进化算法的自动提示领域中最有效的解决方案之一，其性能显著优于现有SOTA方法。

Abstract: Autoprompting is the process of automatically selecting optimized prompts for
language models, which has been gaining popularity with the rapid advancement
of prompt engineering, driven by extensive research in the field of large
language models (LLMs). This paper presents ReflectivePrompt - a novel
autoprompting method based on evolutionary algorithms that employs a reflective
evolution approach for more precise and comprehensive search of optimal
prompts. ReflectivePrompt utilizes short-term and long-term reflection
operations before crossover and elitist mutation to enhance the quality of the
modifications they introduce. This method allows for the accumulation of
knowledge obtained throughout the evolution process and updates it at each
epoch based on the current population. ReflectivePrompt was tested on 33
datasets for classification and text generation tasks using open-access large
language models: t-lite-instruct-0.1 and gemma3-27b-it. The method
demonstrates, on average, a significant improvement (e.g., 28% on BBH compared
to EvoPrompt) in metrics relative to current state-of-the-art approaches,
thereby establishing itself as one of the most effective solutions in
evolutionary algorithm-based autoprompting.

</details>


### [159] [Empowering Computing Education Researchers Through LLM-Assisted Content Analysis](https://arxiv.org/abs/2508.18872)
*Laurie Gale,Sebastian Mateos Nicolajsen*

Main category: cs.CL

TL;DR: 本文提出了一种名为LLM辅助内容分析（LACA）的方法，旨在帮助计算教育研究（CER）领域的研究人员，在不增加负担的情况下，对大量定性文本数据进行严谨且可推广的分析。


<details>
  <summary>Details</summary>
Motivation: 计算教育研究人员常因缺乏同事、资源或能力，难以进行具有普遍性和严谨性的研究，尤其是在处理大量定性数据时。这限制了他们改进教学实践和推进学科发展的能力。因此，需要一种能够处理大量定性数据、同时不增加研究者负担的研究方法。

Method: 本文提出了一种LLM辅助内容分析（LACA）方法，它是内容分析与大型语言模型（LLM）结合的变体。作者使用一个计算教育数据集，演示了LACA如何以可重现和严谨的方式应用。

Result: LACA方法能够帮助研究人员进行他们原本无法完成的大规模研究。通过计算教育数据集的案例，LACA被证明可以以可重现和严谨的方式应用，从而有望在CER中产生更具普遍性的发现。

Conclusion: LACA方法在计算教育研究中具有巨大潜力，能够促进更广泛研究产生更具普遍性的发现。结合类似方法的发展，可以共同提升CER学科的实践水平和研究质量。

Abstract: Computing education research (CER) is often instigated by practitioners
wanting to improve both their own and the wider discipline's teaching practice.
However, the latter is often difficult as many researchers lack the colleagues,
resources, or capacity to conduct research that is generalisable or rigorous
enough to advance the discipline. As a result, research methods that enable
sense-making with larger volumes of qualitative data, while not increasing the
burden on the researcher, have significant potential within CER.
  In this discussion paper, we propose such a method for conducting rigorous
analysis on large volumes of textual data, namely a variation of LLM-assisted
content analysis (LACA). This method combines content analysis with the use of
large language models, empowering researchers to conduct larger-scale research
which they would otherwise not be able to perform. Using a computing education
dataset, we illustrate how LACA could be applied in a reproducible and rigorous
manner. We believe this method has potential in CER, enabling more
generalisable findings from a wider range of research. This, together with the
development of similar methods, can help to advance both the practice and
research quality of the CER discipline.

</details>


### [160] [Affective Polarization across European Parliaments](https://arxiv.org/abs/2508.18916)
*Bojan Evkoski,Igor Mozetič,Nikola Ljubešić,Petra Kralj Novak*

Main category: cs.CL

TL;DR: 本研究通过自动化自然语言处理分析欧洲六个国家议会的演讲，发现议员之间存在普遍的情感极化现象，即对对立群体的负面情绪增加，且互惠是其形成机制之一。


<details>
  <summary>Details</summary>
Motivation: 情感极化（对对立群体增加负面和敌意）已成为全球政治话语的显著特征，本研究旨在以全自动化方式，在选定的欧洲议会中检验这种极化的存在。

Method: 利用来自欧洲六个国家议会的综合议会演讲语料库，采用自然语言处理技术估算议员的情绪。通过比较提及对立群体和自身群体时所传达的负面情绪水平，来发现情感极化的互动模式。

Result: 研究发现所有六个欧洲议会中都存在一致的情感极化现象。虽然活跃度与负面情绪相关，但在不活跃和更活跃的议员之间，情感极化没有观察到差异。最后，研究表明互惠是所有六个议会中议员间情感极化的一个促成机制。

Conclusion: 欧洲六个议会中普遍存在一致的情感极化，表现为对对立群体的负面情绪增加。这种极化与议员活跃度相关，但活跃度本身并不影响极化程度。互惠是议员间情感极化的重要驱动机制。

Abstract: Affective polarization, characterized by increased negativity and hostility
towards opposing groups, has become a prominent feature of political discourse
worldwide. Our study examines the presence of this type of polarization in a
selection of European parliaments in a fully automated manner. Utilizing a
comprehensive corpus of parliamentary speeches from the parliaments of six
European countries, we employ natural language processing techniques to
estimate parliamentarian sentiment. By comparing the levels of negativity
conveyed in references to individuals from opposing groups versus one's own, we
discover patterns of affectively polarized interactions. The findings
demonstrate the existence of consistent affective polarization across all six
European parliaments. Although activity correlates with negativity, there is no
observed difference in affective polarization between less active and more
active members of parliament. Finally, we show that reciprocity is a
contributing mechanism in affective polarization between parliamentarians
across all six parliaments.

</details>


### [161] [Diverse And Private Synthetic Datasets Generation for RAG evaluation: A multi-agent framework](https://arxiv.org/abs/2508.18929)
*Ilias Driouich,Hongliu Cao,Eoin Thomas*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的多智能体框架，用于生成合成的检索增强生成（RAG）系统评估问答数据集，特别关注语义多样性和隐私保护，以解决现有评估数据集中多样性不足和隐私考量缺失的问题。


<details>
  <summary>Details</summary>
Motivation: RAG系统评估的有效性和可信度高度依赖于评估过程，特别是是否捕捉了隐私保护等真实世界约束。然而，当前的评估工作主要集中于性能指标，对底层评估数据集的设计和质量关注不足，导致评估缺乏意义和可靠性。

Method: 该方法引入了一个多智能体框架来生成合成问答数据集，包括：(1) 一个多样性智能体，利用聚类技术最大化主题覆盖和语义变异性；(2) 一个隐私智能体，检测并遮蔽跨多个领域的敏感信息；(3) 一个问答策划智能体，综合私密且多样化的问答对作为RAG评估的真实标签。

Result: 实验结果表明，该方法生成的评估数据集在多样性方面优于基线方法，并在特定领域数据集上实现了鲁棒的隐私遮蔽。

Conclusion: 这项工作为RAG系统提供了一条实用且符合伦理的、更安全、更全面的评估途径，为未来符合不断演进的AI法规和合规标准的增强奠定了基础。

Abstract: Retrieval-augmented generation (RAG) systems improve large language model
outputs by incorporating external knowledge, enabling more informed and
context-aware responses. However, the effectiveness and trustworthiness of
these systems critically depends on how they are evaluated, particularly on
whether the evaluation process captures real-world constraints like protecting
sensitive information. While current evaluation efforts for RAG systems have
primarily focused on the development of performance metrics, far less attention
has been given to the design and quality of the underlying evaluation datasets,
despite their pivotal role in enabling meaningful, reliable assessments. In
this work, we introduce a novel multi-agent framework for generating synthetic
QA datasets for RAG evaluation that prioritize semantic diversity and privacy
preservation. Our approach involves: (1) a Diversity agent leveraging
clustering techniques to maximize topical coverage and semantic variability,
(2) a Privacy Agent that detects and mask sensitive information across multiple
domains and (3) a QA curation agent that synthesizes private and diverse QA
pairs suitable as ground truth for RAG evaluation. Extensive experiments
demonstrate that our evaluation sets outperform baseline methods in diversity
and achieve robust privacy masking on domain-specific datasets. This work
offers a practical and ethically aligned pathway toward safer, more
comprehensive RAG system evaluation, laying the foundation for future
enhancements aligned with evolving AI regulations and compliance standards.

</details>


### [162] [Interpretable by AI Mother Tongue: Native Symbolic Reasoning in Neural Models](https://arxiv.org/abs/2508.18988)
*Hung Ming Liu*

Main category: cs.CL

TL;DR: 本文提出了一种“AI母语”框架，使神经网络模型能够开发出一种原生的符号语言，同时支持直观推理、组合式符号链和固有的可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的事后解释方法无法将推理直接嵌入模型表示中，缺乏透明且灵活的推理机制。研究旨在开发一种能将推理直接融入模型表示，并提供固有可解释性的方法。

Method: 该框架通过以下方式实现：1) 开发“AI母语”——一种原生符号语言；2) 将推理直接嵌入模型表示中，通过符号捕捉语义模式，通过链条追踪决策路径，并通过门控归纳机制引导选择性关注；3) 引入互补的训练目标以增强符号纯度和决策稀疏性；4) 采用顺序专业化策略，先建立广泛的符号能力，再细化直观判断。

Result: 在AI任务上的实验表明，该方法在保持竞争性准确率的同时，能够提供可验证的推理轨迹。

Conclusion: “AI母语”可以作为神经网络模型中可解释性、直觉和符号推理的统一机制。

Abstract: We present a framework where neural models develop an AI Mother Tongue, a
native symbolic language that simultaneously supports intuitive reasoning,
compositional symbol chains, and inherent interpretability. Unlike post-hoc
explanation methods, our approach embeds reasoning directly into the model's
representations: symbols capture meaningful semantic patterns, chains trace
decision paths, and gated induction mechanisms guide selective focus, yielding
transparent yet flexible reasoning. We introduce complementary training
objectives to enhance symbol purity and decision sparsity, and employ a
sequential specialization strategy to first build broad symbolic competence and
then refine intuitive judgments. Experiments on AI tasks demonstrate
competitive accuracy alongside verifiable reasoning traces, showing that AI
Mother Tongue can serve as a unified mechanism for interpretability, intuition,
and symbolic reasoning in neural models.

</details>


### [163] [Automatic Prompt Optimization with Prompt Distillation](https://arxiv.org/abs/2508.18992)
*Viktor N. Zhuravlev,Artur R. Khairullin,Ernest A. Dyagin,Alena N. Sitkina,Nikita I. Kulin*

Main category: cs.CL

TL;DR: 本文提出了一种名为DistillPrompt的新型自动提示（autoprompting）方法，它通过多阶段整合任务特定信息并利用蒸馏、压缩和聚合操作，显著提高了语言模型在文本分类和生成任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的快速发展和提示工程（prompt engineering）的广泛研究，自动选择优化提示（autoprompting）变得越来越流行，促使研究人员探索更有效的自动提示方法。

Method: DistillPrompt是一种基于大型语言模型的自动提示方法。它采用多阶段集成训练数据中的任务特定信息到提示中，并利用蒸馏（distillation）、压缩（compression）和聚合（aggregation）操作来更彻底地探索提示空间。这是一种非梯度（non-gradient）方法。

Result: 该方法在文本分类和生成任务的不同数据集上，使用t-lite-instruct-0.1语言模型进行了测试。结果显示，与现有方法（如Grips）相比，关键指标有显著的平均改进（例如，在整个数据集上平均提高了20.12%）。

Conclusion: DistillPrompt被证明是自动提示领域中最有效的非梯度方法之一，在性能上超越了现有方法。

Abstract: Autoprompting is the process of automatically selecting optimized prompts for
language models, which is gaining popularity due to the rapid development of
prompt engineering driven by extensive research in the field of large language
models (LLMs). This paper presents DistillPrompt -- a novel autoprompting
method based on large language models that employs a multi-stage integration of
task-specific information into prompts using training data. DistillPrompt
utilizes distillation, compression, and aggregation operations to explore the
prompt space more thoroughly. The method was tested on different datasets for
text classification and generation tasks using the t-lite-instruct-0.1 language
model. The results demonstrate a significant average improvement (e.g., 20.12%
across the entire dataset compared to Grips) in key metrics over existing
methods in the field, establishing DistillPrompt as one of the most effective
non-gradient approaches in autoprompting.

</details>


### [164] [MovieCORE: COgnitive REasoning in Movies](https://arxiv.org/abs/2508.19026)
*Gueter Josmy Faure,Min-Hung Chen,Jia-Fong Yeh,Ying Cheng,Hung-Ting Su,Yung-Hao Tang,Shang-Hong Lai,Winston H. Hsu*

Main category: cs.CL

TL;DR: 本文介绍了MovieCORE，一个用于电影内容深度认知理解的新型视频问答（VQA）数据集，该数据集通过多LLM智能体头脑风暴生成，并提出了一套认知测试来评估数据集质量，同时引入了Agentic Choice Enhancement（ACE）模块以提升现有视频语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有VQA数据集侧重于表面理解，缺乏对电影内容更深层次的认知理解（系统2思维）的探究。

Method: 本文提出了MovieCORE数据集，采用创新的智能体头脑风暴方法，利用多个大型语言模型（LLM）作为思维智能体来生成和完善高质量的问答对。为评估数据集质量，开发了一套评估深度、启发性和句法复杂度的认知测试。同时，提出了一套评估VQA模型在深度认知任务上表现的综合评估方案，并引入了Agentic Choice Enhancement (ACE) 智能体增强模块来提升现有视频语言模型的推理能力。

Result: MovieCORE数据集强调与视频内容相关的、需要系统2思维的深度认知问题。通过智能体头脑风暴方法生成了高质量的问答对。所提出的ACE模块能够将模型训练后的推理能力提升高达25%。

Conclusion: 这项工作推动了AI系统在电影理解方面的发展，并为当前VQA模型在面对更具挑战性和细致入微的电影内容问题时的能力和局限性提供了宝贵的见解。

Abstract: This paper introduces MovieCORE, a novel video question answering (VQA)
dataset designed to probe deeper cognitive understanding of movie content.
Unlike existing datasets that focus on surface-level comprehension, MovieCORE
emphasizes questions that engage System-2 thinking while remaining specific to
the video material. We present an innovative agentic brainstorming approach,
utilizing multiple large language models (LLMs) as thought agents to generate
and refine high-quality question-answer pairs. To evaluate dataset quality, we
develop a set of cognitive tests assessing depth, thought-provocation
potential, and syntactic complexity. We also propose a comprehensive evaluation
scheme for assessing VQA model performance on deeper cognitive tasks. To
address the limitations of existing video-language models (VLMs), we introduce
an agentic enhancement module, Agentic Choice Enhancement (ACE), which improves
model reasoning capabilities post-training by up to 25%. Our work contributes
to advancing movie understanding in AI systems and provides valuable insights
into the capabilities and limitations of current VQA models when faced with
more challenging, nuanced questions about cinematic content. Our project page,
dataset and code can be found at
https://joslefaure.github.io/assets/html/moviecore.html.

</details>


### [165] [HiPlan: Hierarchical Planning for LLM-Based Agents with Adaptive Global-Local Guidance](https://arxiv.org/abs/2508.19076)
*Ziyue Li,Yuan Chang,Gaihong Yu,Xiaoqiu Le*

Main category: cs.CL

TL;DR: HiPlan是一个分层规划框架，通过提供自适应的全局-局部指导，显著提升了大型语言模型（LLM）代理在复杂、长周期规划任务中的决策能力。


<details>
  <summary>Details</summary>
Motivation: LLM代理在复杂、长周期规划任务中表现不佳，原因在于缺乏宏观指导导致迷失方向，以及执行过程中缺乏持续监督导致对环境变化反应迟钝和容易偏离。

Method: HiPlan框架采用分层规划策略：1. 将复杂任务分解为里程碑行动指南（提供总体方向）和逐步提示（提供详细行动）。2. 在离线阶段，从专家演示中构建里程碑库，通过检索语义相似的任务和里程碑实现结构化经验复用。3. 在执行阶段，动态调整过去里程碑的轨迹片段，生成逐步提示，使当前观察与里程碑目标对齐，从而弥补差距和纠正偏差。

Result: 在两个具有挑战性的基准测试中，HiPlan显著优于强大的基线模型。消融研究也验证了其分层组件的互补优势。

Conclusion: HiPlan通过提供自适应的全局-局部指导，成功解决了LLM代理在复杂、长周期规划任务中面临的挑战，有效提升了其决策能力和鲁棒性。

Abstract: Large language model (LLM)-based agents have demonstrated remarkable
capabilities in decision-making tasks, but struggle significantly with complex,
long-horizon planning scenarios. This arises from their lack of macroscopic
guidance, causing disorientation and failures in complex tasks, as well as
insufficient continuous oversight during execution, rendering them unresponsive
to environmental changes and prone to deviations. To tackle these challenges,
we introduce HiPlan, a hierarchical planning framework that provides adaptive
global-local guidance to boost LLM-based agents'decision-making. HiPlan
decomposes complex tasks into milestone action guides for general direction and
step-wise hints for detailed actions. During the offline phase, we construct a
milestone library from expert demonstrations, enabling structured experience
reuse by retrieving semantically similar tasks and milestones. In the execution
phase, trajectory segments from past milestones are dynamically adapted to
generate step-wise hints that align current observations with the milestone
objectives, bridging gaps and correcting deviations. Extensive experiments
across two challenging benchmarks demonstrate that HiPlan substantially
outperforms strong baselines, and ablation studies validate the complementary
benefits of its hierarchical components.

</details>


### [166] ["Where does it hurt?" -- Dataset and Study on Physician Intent Trajectories in Doctor Patient Dialogues](https://arxiv.org/abs/2508.19077)
*Tom Röhr,Soumyadeep Roy,Fares Al Mohamad,Jens-Michalis Papaioannou,Wolfgang Nejdl,Felix Gers,Alexander Löser*

Main category: cs.CL

TL;DR: 本研究首次探讨了医患对话中医生的意图轨迹，基于SOAP框架开发了细粒度分类法，并进行了大规模标注。通过对现有模型进行基准测试，揭示了对话结构洞察，并显著提升了医疗对话摘要的性能。


<details>
  <summary>Details</summary>
Motivation: 医生在医患对话中通过有针对性的提问来高效收集信息，以实现诊断和治疗目标。然而，据作者所知，此前没有研究系统地分析医生在对话中的意图轨迹。理解这些意图对于开发更有效的医疗AI系统（如鉴别诊断系统）至关重要。

Method: 研究使用了Aci-bench数据集，并与医学专业人士合作，基于SOAP框架（主观、客观、评估、计划）开发了细粒度的医生意图分类法。通过众包平台Prolific招募大量医学专家，对超过5000个医患对话轮次进行了大规模标注。随后，利用这个标注数据集对最先进的生成式和编码器模型在医疗意图分类任务上进行了基准测试。此外，还首次报告了医疗对话中的常见轨迹，并广泛研究了意图过滤对医疗对话摘要性能的影响。所有代码和数据（包括标注指南）均已公开。

Result: 研究发现，模型能够高精度地理解医疗对话的整体结构，但在识别SOAP类别之间的转换时常出现问题。首次报告了医疗对话结构中的常见轨迹，为设计“鉴别诊断”系统提供了有价值的见解。同时，观察到意图过滤显著提升了医疗对话摘要的性能。

Conclusion: 本研究首次对医患对话中的医生意图轨迹进行了探索，创建了一个基于SOAP框架的细粒度意图分类法和大规模标注数据集。研究不仅对现有模型进行了基准测试，还揭示了医疗对话结构的关键洞察，并证明了意图过滤在对话摘要中的有效性，为未来设计更智能的医疗AI系统奠定了基础。

Abstract: In a doctor-patient dialogue, the primary objective of physicians is to
diagnose patients and propose a treatment plan. Medical doctors guide these
conversations through targeted questioning to efficiently gather the
information required to provide the best possible outcomes for patients. To the
best of our knowledge, this is the first work that studies physician intent
trajectories in doctor-patient dialogues. We use the `Ambient Clinical
Intelligence Benchmark' (Aci-bench) dataset for our study. We collaborate with
medical professionals to develop a fine-grained taxonomy of physician intents
based on the SOAP framework (Subjective, Objective, Assessment, and Plan). We
then conduct a large-scale annotation effort to label over 5000 doctor-patient
turns with the help of a large number of medical experts recruited using
Prolific, a popular crowd-sourcing platform. This large labeled dataset is an
important resource contribution that we use for benchmarking the
state-of-the-art generative and encoder models for medical intent
classification tasks. Our findings show that our models understand the general
structure of medical dialogues with high accuracy, but often fail to identify
transitions between SOAP categories. We also report for the first time common
trajectories in medical dialogue structures that provide valuable insights for
designing `differential diagnosis' systems. Finally, we extensively study the
impact of intent filtering for medical dialogue summarization and observe a
significant boost in performance. We make the codes and data, including
annotation guidelines, publicly available at
https://github.com/DATEXIS/medical-intent-classification.

</details>


### [167] [It's All About In-Context Learning! Teaching Extremely Low-Resource Languages to LLMs](https://arxiv.org/abs/2508.19089)
*Yue Li,Zhixue Zhao,Carolina Scarton*

Main category: cs.CL

TL;DR: 本文首次全面分析了大型语言模型（LLMs）如何通过上下文学习（ICL）或参数高效微调（PEFT）来支持极低资源语言，发现零样本ICL结合语言对齐对极低资源语言非常有效，而PEFT在语言及其文字极度未被LLM代表时表现受限。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）对极低资源语言，特别是那些使用稀有文字的语言，支持不足，主要原因是训练数据匮乏。

Method: 研究对20种代表性不足的语言，在三个最先进的多语言LLMs上进行了系统评估。方法包括：纯粹的上下文学习（ICL），带或不带辅助对齐信号的ICL，以及参数高效微调（PEFT）。

Result: 研究发现，当语言及其文字在LLM中极度未被代表时，PEFT表现受限。相比之下，结合语言对齐的零样本ICL对极低资源语言非常有效。对于LLM中相对较好代表的语言，少量样本ICL或PEFT更为有益。

Conclusion: 为处理极低资源语言的LLM实践者提供了指导方针，例如，应避免对多语言模型在未见过的文字语言上进行微调。零样本ICL结合语言对齐是处理极低资源语言的有效策略。

Abstract: Extremely low-resource languages, especially those written in rare scripts,
as shown in Figure 1, remain largely unsupported by large language models
(LLMs). This is due in part to compounding factors such as the lack of training
data. This paper delivers the first comprehensive analysis of whether LLMs can
acquire such languages purely via in-context learning (ICL), with or without
auxiliary alignment signals, and how these methods compare to
parameter-efficient fine-tuning (PEFT). We systematically evaluate 20
under-represented languages across three state-of-the-art multilingual LLMs.
Our findings highlight the limitation of PEFT when both language and its script
are extremely under-represented by the LLM. In contrast, zero-shot ICL with
language alignment is impressively effective on extremely low-resource
languages, while few-shot ICL or PEFT is more beneficial for languages
relatively better represented by LLMs. For LLM practitioners working on
extremely low-resource languages, we summarise guidelines grounded by our
results on adapting LLMs to low-resource languages, e.g., avoiding fine-tuning
a multilingual model on languages of unseen scripts.

</details>


### [168] [Retrieval-Augmented Generation for Natural Language Art Provenance Searches in the Getty Provenance Index](https://arxiv.org/abs/2508.19093)
*Mathew Henrickson*

Main category: cs.CL

TL;DR: 本研究提出一个用于艺术品出处研究的检索增强生成（RAG）框架，利用自然语言和多语言搜索功能，有效处理碎片化的档案数据。


<details>
  <summary>Details</summary>
Motivation: 艺术品出处研究对于验证真伪、支持索赔和理解历史背景至关重要，但现有搜索工具因数据碎片化、多语言性和对精确元数据的依赖而效率低下。

Method: 开发了一个RAG框架，通过语义检索和上下文摘要实现自然语言和多语言搜索。该方法在Getty Provenance Index - German Sales的10,000条拍卖记录样本上进行了评估。

Result: 该方法成功检索并总结了拍卖记录，证明了其在处理艺术市场档案方面的可扩展性。

Conclusion: RAG框架为历史学家和文化遗产专业人士进行敏感的出处研究提供了一个实用且可扩展的工具。

Abstract: This research presents a Retrieval-Augmented Generation (RAG) framework for
art provenance studies, focusing on the Getty Provenance Index. Provenance
research establishes the ownership history of artworks, which is essential for
verifying authenticity, supporting restitution and legal claims, and
understanding the cultural and historical context of art objects. The process
is complicated by fragmented, multilingual archival data that hinders efficient
retrieval. Current search portals require precise metadata, limiting
exploratory searches. Our method enables natural-language and multilingual
searches through semantic retrieval and contextual summarization, reducing
dependence on metadata structures. We assess RAG's capability to retrieve and
summarize auction records using a 10,000-record sample from the Getty
Provenance Index - German Sales. The results show this approach provides a
scalable solution for navigating art market archives, offering a practical tool
for historians and cultural heritage professionals conducting historically
sensitive research.

</details>


### [169] [Beyond the Black Box: Integrating Lexical and Semantic Methods in Quantitative Discourse Analysis with BERTopic](https://arxiv.org/abs/2508.19099)
*Thomas Compton*

Main category: cs.CL

TL;DR: 本文提出了一种混合、透明的定量话语分析（QDA）框架，结合词汇和语义方法，利用Python工具实现可复现、可解释的分析，强调代码透明度和研究者能动性。


<details>
  <summary>Details</summary>
Motivation: 现有QDA软件（如MAXQDA、NVivo）的“黑箱”特性损害了方法论透明度，且难以与研究目标对齐，促使研究者寻求更透明、可控的分析方法。

Method: 研究采用自定义Python管道（NLTK、spaCy、Sentence Transformers）进行预处理、词形还原和嵌入生成。通过迭代的BERTopic建模过程（包含UMAP降维、HDBSCAN聚类和c-TF-IDF关键词提取），并进行参数调优和多次运行以优化主题连贯性和覆盖率。该方法将精确的词汇搜索与上下文感知的语义聚类相结合，实现了多层次分析。

Result: 该框架通过结合词汇和语义方法，实现了对数据处理的精细控制，增强了分析的三角验证、可复现性和可解释性。它弥补了单一方法在孤立使用时的局限性，并在一项历史政治话语案例研究中得到了验证。

Conclusion: 研究强调了在计算话语研究中，代码层面的透明度、研究者能动性以及方法论三角验证的重要性，为QDA提供了一个更开放、可控的解决方案。

Abstract: Quantitative Discourse Analysis has seen growing adoption with the rise of
Large Language Models and computational tools. However, reliance on black box
software such as MAXQDA and NVivo risks undermining methodological transparency
and alignment with research goals. This paper presents a hybrid, transparent
framework for QDA that combines lexical and semantic methods to enable
triangulation, reproducibility, and interpretability. Drawing from a case study
in historical political discourse, we demonstrate how custom Python pipelines
using NLTK, spaCy, and Sentence Transformers allow fine-grained control over
preprocessing, lemmatisation, and embedding generation. We further detail our
iterative BERTopic modelling process, incorporating UMAP dimensionality
reduction, HDBSCAN clustering, and c-TF-IDF keyword extraction, optimised
through parameter tuning and multiple runs to enhance topic coherence and
coverage. By juxtaposing precise lexical searches with context-aware semantic
clustering, we argue for a multi-layered approach that mitigates the
limitations of either method in isolation. Our workflow underscores the
importance of code-level transparency, researcher agency, and methodological
triangulation in computational discourse studies. Code and supplementary
materials are available via GitHub.

</details>


### [170] [Do LVLMs Know What They Know? A Systematic Study of Knowledge Boundary Perception in LVLMs](https://arxiv.org/abs/2508.19111)
*Zhikai Ding,Shiyu Ni,Keping Bi*

Main category: cs.CL

TL;DR: 本文研究了大型视觉语言模型（LVLMs）对其知识边界的感知能力，评估了三种置信度信号，发现LVLMs具有一定感知能力但仍有提升空间。概率和一致性置信度更可靠，并提出了有效的校准方法。LVLMs相较于LLMs在感知方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型（LVLMs）在视觉问答（VQA）方面表现出色，但存在幻觉问题。一个可靠的模型应该能够感知其知识边界，即知道自己知道什么和不知道什么。因此，研究LVLMs对其知识边界的感知能力至关重要。

Method: 本文通过评估三种类型的置信度信号来研究LVLMs的知识边界感知：概率置信度、基于答案一致性的置信度以及口头表达的置信度。实验在三个LVLMs和三个VQA数据集上进行。此外，本文还从大型语言模型（LLMs）中借鉴并调整了几种已有的置信度校准方法，并提出了三种新的有效方法。最后，将LVLMs与其LLM对应模型进行了比较。

Result: 实验结果表明，尽管LVLMs具有合理的感知水平，但仍有很大的改进空间。在三种置信度信号中，概率置信度和基于一致性的信号是更可靠的指标，而口头表达的置信度通常会导致过度自信。本文提出的校准方法是有效的。与LLMs相比，LVLMs联合处理视觉和文本输入虽然降低了问答性能，但却降低了置信度，从而提高了感知水平。

Conclusion: LVLMs对其知识边界具有一定感知能力，但存在显著的改进空间。概率和一致性置信度信号是更可靠的感知指标。通过适应和提出置信度校准方法，可以有效增强LVLMs的感知能力。此外，LVLMs在感知水平上优于其LLM对应模型，这表明多模态处理在提高模型自知能力方面具有优势。

Abstract: Large vision-language models (LVLMs) demonstrate strong visual question
answering (VQA) capabilities but are shown to hallucinate. A reliable model
should perceive its knowledge boundaries-knowing what it knows and what it does
not. This paper investigates LVLMs' perception of their knowledge boundaries by
evaluating three types of confidence signals: probabilistic confidence, answer
consistency-based confidence, and verbalized confidence. Experiments on three
LVLMs across three VQA datasets show that, although LVLMs possess a reasonable
perception level, there is substantial room for improvement. Among the three
confidences, probabilistic and consistency-based signals are more reliable
indicators, while verbalized confidence often leads to overconfidence. To
enhance LVLMs' perception, we adapt several established confidence calibration
methods from Large Language Models (LLMs) and propose three effective methods.
Additionally, we compare LVLMs with their LLM counterparts, finding that
jointly processing visual and textual inputs decreases question-answering
performance but reduces confidence, resulting in an improved perception level
compared to LLMs.

</details>


### [171] [Demystifying Scientific Problem-Solving in LLMs by Probing Knowledge and Reasoning](https://arxiv.org/abs/2508.19202)
*Alan Li,Yixin Liu,Arpan Sarkar,Doug Downey,Arman Cohan*

Main category: cs.CL

TL;DR: 本文引入了科学推理基准SciReas/SciReas-Pro和KRUX探究框架，深入分析了大型语言模型（LLM）在科学推理中知识与推理的独特作用，发现知识检索是关键瓶颈，外部知识和言语化推理对性能有显著提升，并发布了新的科学推理基线模型。


<details>
  <summary>Details</summary>
Motivation: 科学问题解决对LLM提出了独特挑战，需要深层领域知识和复杂推理能力。目前缺乏全面评估科学推理的基准，且很少有方法系统地解耦知识与推理在这些任务中的不同作用。

Method: 引入了多样化的科学推理基准套件SciReas及其更复杂的子集SciReas-Pro。提出了KRUX探究框架，用于研究知识和推理在科学任务中的独立作用。结合两者进行了深入分析，并与长CoT SFT的并发工作进行了比较。发布了8B科学推理基线模型SciLit01。

Result: 1) 从模型参数中检索任务相关知识是LLM科学推理的关键瓶颈；2) 推理模型始终受益于上下文内添加的外部知识，超越了推理本身的增强；3) 增强言语化推理能力可提高LLM显现任务相关知识的能力。

Conclusion: 通过全面的评估和探究框架，揭示了LLM在科学推理中知识检索的瓶颈，强调了外部知识和言语化推理对性能提升的重要性，为未来的科学推理研究提供了有价值的见解、基准和强有力的基线模型。

Abstract: Scientific problem solving poses unique challenges for LLMs, requiring both
deep domain knowledge and the ability to apply such knowledge through complex
reasoning. While automated scientific reasoners hold great promise for
assisting human scientists, there is currently no widely adopted holistic
benchmark for evaluating scientific reasoning, and few approaches
systematically disentangle the distinct roles of knowledge and reasoning in
these tasks. To address these gaps, we introduce SciReas, a diverse suite of
existing benchmarks for scientific reasoning tasks, and SciReas-Pro, a
selective subset that requires more complex reasoning. Our holistic evaluation
surfaces insights about scientific reasoning performance that remain hidden
when relying on individual benchmarks alone. We then propose KRUX, a probing
framework for studying the distinct roles of reasoning and knowledge in
scientific tasks. Combining the two, we conduct an in-depth analysis that
yields several key findings: (1) Retrieving task-relevant knowledge from model
parameters is a critical bottleneck for LLMs in scientific reasoning; (2)
Reasoning models consistently benefit from external knowledge added in-context
on top of the reasoning enhancement; (3) Enhancing verbalized reasoning
improves LLMs' ability to surface task-relevant knowledge. Finally, we conduct
a lightweight analysis, comparing our science-focused data composition with
concurrent efforts on long CoT SFT, and release SciLit01, a strong 8B baseline
for scientific reasoning.

</details>


### [172] [VibeVoice Technical Report](https://arxiv.org/abs/2508.19205)
*Zhiliang Peng,Jianwei Yu,Wenhui Wang,Yaoyao Chang,Yutao Sun,Li Dong,Yi Zhu,Weijiang Xu,Hangbo Bao,Zehua Wang,Shaohan Huang,Yan Xia,Furu Wei*

Main category: cs.CL

TL;DR: VibeVoice是一种新型模型，通过结合下一token扩散（一种自回归生成潜在向量的统一方法）和一种高效的连续语音分词器，能够合成长达90分钟的多说话人语音，同时保持高保真度和计算效率。


<details>
  <summary>Details</summary>
Motivation: 研究动机是需要合成具有真实对话“氛围”的长篇多说话人语音，以超越现有开源和专有对话模型的局限性，尤其是在处理长序列和多说话人场景时。

Method: 该研究引入了VibeVoice模型，它采用下一token扩散（一种通过扩散自回归生成潜在向量来建模连续数据的统一方法）。此外，还提出了一种新颖的连续语音分词器，该分词器与Encodec模型相比，将数据压缩率提高了80倍，同时保持了可比的性能和音频保真度，并显著提升了处理长序列的计算效率。

Result: VibeVoice模型能够合成长达90分钟（在64K上下文窗口长度下）的语音，最多支持4位说话人。其核心分词器在保持音频保真度的同时，数据压缩率比Encodec提高了80倍，显著提升了计算效率。VibeVoice能够捕捉真实的对话“氛围”，并超越了现有的开源和专有对话模型。

Conclusion: VibeVoice通过其创新的下一token扩散方法和高效的连续语音分词器，在长篇多说话人语音合成方面取得了重大进展，实现了高保真、高效且能捕捉真实对话感的语音生成，显著优于现有技术。

Abstract: This report presents VibeVoice, a novel model designed to synthesize
long-form speech with multiple speakers by employing next-token diffusion,
which is a unified method for modeling continuous data by autoregressively
generating latent vectors via diffusion. To enable this, we introduce a novel
continuous speech tokenizer that, when compared to the popular Encodec model,
improves data compression by 80 times while maintaining comparable performance.
The tokenizer effectively preserves audio fidelity while significantly boosting
computational efficiency for processing long sequences. Thus, VibeVoice can
synthesize long-form speech for up to 90 minutes (in a 64K context window
length) with a maximum of 4 speakers, capturing the authentic conversational
``vibe'' and surpassing open-source and proprietary dialogue models.

</details>


### [173] [Evaluating the Evaluators: Are readability metrics good measures of readability?](https://arxiv.org/abs/2508.19221)
*Isabel Cachola,Daniel Khashabi,Mark Dredze*

Main category: cs.CL

TL;DR: 本文调查了简明语言摘要（PLS）文献，发现传统可读性指标（如FKGL）与人类判断相关性差，并提出语言模型（LMs）能更好地评估PLS的可读性，特别是在深层可读性方面。


<details>
  <summary>Details</summary>
Motivation: 简明语言摘要（PLS）旨在为非专业读者简化复杂文档。目前评估PLS可读性的标准做法是使用传统可读性指标（如Flesch-Kincaid Grade Level, FKGL），但这些指标在PLS领域尚未与人类可读性判断进行比较，其有效性存疑。

Method: 作者对PLS文献进行了全面调查，评估了8种传统可读性指标与人类判断的相关性。随后，他们利用语言模型（LMs）来判断可读性，并将其结果与人类判断进行比较。最后，将分析扩展到PLS数据集，探究LMs在捕捉深层可读性方面的表现。

Result: 研究发现，大多数传统可读性指标（包括最流行的FKGL）与人类判断的相关性很差。语言模型（LMs）在可读性判断上表现更好，最佳模型的皮尔逊相关系数达到0.56。在PLS数据集上，LMs能更好地捕捉深层可读性（如所需背景知识），并得出与传统指标不同的结论。

Conclusion: 传统可读性指标不适用于评估简明语言摘要。语言模型是更有效的可读性判断工具，尤其是在评估深层可读性方面。基于这些发现，论文为简明语言摘要的评估提供了最佳实践建议。

Abstract: Plain Language Summarization (PLS) aims to distill complex documents into
accessible summaries for non-expert audiences. In this paper, we conduct a
thorough survey of PLS literature, and identify that the current standard
practice for readability evaluation is to use traditional readability metrics,
such as Flesch-Kincaid Grade Level (FKGL). However, despite proven utility in
other fields, these metrics have not been compared to human readability
judgments in PLS. We evaluate 8 readability metrics and show that most
correlate poorly with human judgments, including the most popular metric, FKGL.
We then show that Language Models (LMs) are better judges of readability, with
the best-performing model achieving a Pearson correlation of 0.56 with human
judgments. Extending our analysis to PLS datasets, which contain summaries
aimed at non-expert audiences, we find that LMs better capture deeper measures
of readability, such as required background knowledge, and lead to different
conclusions than the traditional metrics. Based on these findings, we offer
recommendations for best practices in the evaluation of plain language
summaries. We release our analysis code and survey data.

</details>


### [174] [Generative Interfaces for Language Models](https://arxiv.org/abs/2508.19227)
*Jiaqi Chen,Yanzhe Zhang,Yutong Zhang,Yijia Shao,Diyi Yang*

Main category: cs.CL

TL;DR: 本文提出了一种“语言模型生成界面”范式，LLM通过主动生成用户界面（UI）来响应查询，以提升多轮、信息密集型和探索性任务中的交互效率和用户体验，并在评估中表现优于传统对话式交互。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）的交互方式（线性请求-响应）在处理多轮、信息密集型和探索性任务时效率低下，限制了其作为助手、副驾驶和顾问的潜力。

Method: 研究提出了一种“语言模型生成界面”范式，LLM通过生成任务特定的用户界面（UI）来响应用户查询。该框架利用结构化的界面特定表示和迭代优化过程。为了系统评估，引入了一个多维度评估框架，从功能、交互和情感方面比较了生成界面与传统聊天界面的用户体验。

Result: 评估结果显示，生成界面持续优于对话式界面，人类用户在超过70%的情况下更偏爱生成界面。这些发现阐明了用户何时以及为何偏爱生成界面。

Conclusion: 生成界面显著改善了人机交互体验，为未来人机交互技术的发展指明了方向，特别是在复杂任务场景下。

Abstract: Large language models (LLMs) are increasingly seen as assistants, copilots,
and consultants, capable of supporting a wide range of tasks through natural
conversation. However, most systems remain constrained by a linear
request-response format that often makes interactions inefficient in
multi-turn, information-dense, and exploratory tasks. To address these
limitations, we propose Generative Interfaces for Language Models, a paradigm
in which LLMs respond to user queries by proactively generating user interfaces
(UIs) that enable more adaptive and interactive engagement. Our framework
leverages structured interface-specific representations and iterative
refinements to translate user queries into task-specific UIs. For systematic
evaluation, we introduce a multidimensional assessment framework that compares
generative interfaces with traditional chat-based ones across diverse tasks,
interaction patterns, and query types, capturing functional, interactive, and
emotional aspects of user experience. Results show that generative interfaces
consistently outperform conversational ones, with humans preferring them in
over 70% of cases. These findings clarify when and why users favor generative
interfaces, paving the way for future advancements in human-AI interaction.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [175] [Mining the Long Tail: A Comparative Study of Data-Centric Criticality Metrics for Robust Offline Reinforcement Learning in Autonomous Motion Planning](https://arxiv.org/abs/2508.18397)
*Antonio Guillen-Perez*

Main category: cs.RO

TL;DR: 本文通过对离线强化学习中自动驾驶规划策略的数据策展策略进行大规模比较研究，解决了数据不平衡问题，显著提升了策略的安全性和可靠性，其中基于模型不确定性的数据驱动策展效果最佳。


<details>
  <summary>Details</summary>
Motivation: 在利用大规模真实世界驾驶日志训练自动驾驶规划策略时，离线强化学习面临数据极端不平衡的挑战。日志中常见场景远超罕见“长尾”事件，导致使用标准均匀数据采样训练出的策略脆弱且不安全。

Method: 研究人员系统性地比较了六种数据策展策略（分为启发式、基于不确定性和基于行为三类），并在时间步和完整场景两个时间尺度上进行评估。他们使用先进的注意力机制架构训练了七个目标条件保守Q学习（CQL）智能体，并在高保真Waymax模拟器中进行评估。

Result: 所有数据策展方法都显著优于基线。值得注意的是，使用模型不确定性作为信号的数据驱动策展在安全性方面取得了最显著的改进，碰撞率降低了近三倍（从16.0%降至5.5%）。此外，时间步级别的加权在反应性安全方面表现出色，而场景级别的加权则能改善长期规划。

Conclusion: 智能的非均匀采样（即数据策展）是构建安全可靠的自动驾驶智能体的关键组成部分。本研究为离线强化学习中的数据策展提供了一个全面的框架，并强调了其重要性。

Abstract: Offline Reinforcement Learning (RL) presents a promising paradigm for
training autonomous vehicle (AV) planning policies from large-scale, real-world
driving logs. However, the extreme data imbalance in these logs, where mundane
scenarios vastly outnumber rare "long-tail" events, leads to brittle and unsafe
policies when using standard uniform data sampling. In this work, we address
this challenge through a systematic, large-scale comparative study of data
curation strategies designed to focus the learning process on information-rich
samples. We investigate six distinct criticality weighting schemes which are
categorized into three families: heuristic-based, uncertainty-based, and
behavior-based. These are evaluated at two temporal scales, the individual
timestep and the complete scenario. We train seven goal-conditioned
Conservative Q-Learning (CQL) agents with a state-of-the-art, attention-based
architecture and evaluate them in the high-fidelity Waymax simulator. Our
results demonstrate that all data curation methods significantly outperform the
baseline. Notably, data-driven curation using model uncertainty as a signal
achieves the most significant safety improvements, reducing the collision rate
by nearly three-fold (from 16.0% to 5.5%). Furthermore, we identify a clear
trade-off where timestep-level weighting excels at reactive safety while
scenario-level weighting improves long-horizon planning. Our work provides a
comprehensive framework for data curation in Offline RL and underscores that
intelligent, non-uniform sampling is a critical component for building safe and
reliable autonomous agents.

</details>


### [176] [Maintenance automation: methods for robotics manipulation planning and execution](https://arxiv.org/abs/2508.18399)
*Christian Friedrich,Ralf Gulde,Armin Lechler,Alexander Verl*

Main category: cs.RO

TL;DR: 本文提出了一种完整的机器人系统，用于自动化维护（拆卸和装配）操作，该系统能够处理环境不确定性，并通过实验验证了其在实际应用中的有效性。


<details>
  <summary>Details</summary>
Motivation: 自动化复杂任务（如维护）需要机器人系统具备规划、控制和执行能力，尤其是在存在环境不确定性（例如先验计划信息与实际情况的偏差）的情况下，需要一个能够应对这些挑战的解决方案。

Method: 该系统基于一个完整的机器人框架，其认知能力依赖于规划方法（利用CAD和RGBD数据）。它包含一个将符号计划解释并转换为可执行机器人指令的方法。

Result: 该完整的机器人系统通过实际应用进行了实验评估，结果表明它是将理论成果转化为实用机器人解决方案的第一步。

Conclusion: 研究成功地展示了一个能够自动化维护操作并处理环境不确定性的实用机器人系统，为复杂任务自动化提供了可行的解决方案。

Abstract: Automating complex tasks using robotic systems requires skills for planning,
control and execution. This paper proposes a complete robotic system for
maintenance automation, which can automate disassembly and assembly operations
under environmental uncertainties (e.g. deviations between prior plan
information). The cognition of the robotic system is based on a planning
approach (using CAD and RGBD data) and includes a method to interpret a
symbolic plan and transform it to a set of executable robot instructions. The
complete system is experimentally evaluated using real-world applications. This
work shows the first step to transfer these theoretical results into a
practical robotic solution.

</details>


### [177] [Efficient task and path planning for maintenance automation using a robot system](https://arxiv.org/abs/2508.18400)
*Christian Friedrich,Akos Csiszar,Armin Lechler,Alexander Verl*

Main category: cs.RO

TL;DR: 该研究提出了一种针对未来工厂维护自动化问题的智能机器人解决方案，通过概率滤波器融合离线CAD与在线RGBD数据，结合基于新型采样方法的符号化任务规划和自适应步长路径规划算法，以应对不确定性和降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 未来工厂中维护任务的自动化是智能自动化解决方案的突破点，使用自主机器人系统自动化维护任务既有前景也充满挑战。主要动机是开发能够自主规划操作任务和路径、计算复杂度低且能处理环境不确定性的算法。

Method: 该方法结合了多种技术：1) 使用概率滤波器将来自CAD的离线数据与来自RGBD视觉系统的在线数据结合，以补偿离线数据的不确定性。2) 任务规划采用基于新型采样方法计算拆卸空间的符号描述。3) 路径规划使用全球最先进的算法，并引入一种允许调整探索步长的方法以减少规划时间。所有方法都经过实验验证和讨论。

Result: 该研究提出了一种特别适用于解决维护自动化问题的方法。该方法通过结合离线和在线数据处理不确定性，并通过新型采样方法实现任务规划，同时通过自适应步长优化路径规划时间，有效应对了自主机器人系统在维护任务中的挑战。

Conclusion: 所提出的方法成功地将数据融合、符号化任务规划和自适应路径规划结合起来，为自主机器人系统在维护自动化领域的应用提供了一个有效的解决方案，能够处理环境不确定性并优化规划效率。

Abstract: The research and development of intelligent automation solutions is a
ground-breaking point for the factory of the future. A promising and
challenging mission is the use of autonomous robot systems to automate tasks in
the field of maintenance. For this purpose, the robot system must be able to
plan autonomously the different manipulation tasks and the corresponding paths.
Basic requirements are the development of algorithms with a low computational
complexity and the possibility to deal with environmental uncertainties. In
this work, an approach is presented, which is especially suited to solve the
problem of maintenance automation. For this purpose, offline data from CAD is
combined with online data from an RGBD vision system via a probabilistic
filter, to compensate uncertainties from offline data. For planning the
different tasks, a method is explained, which use a symbolic description,
founded on a novel sampling-based method to compute the disassembly space. For
path planning we use global state-of-the art algorithms with a method that
allows the adaption of the exploration stepsize in order to reduce the planning
time. Every method is experimentally validated and discussed.

</details>


### [178] [PneuGelSight: Soft Robotic Vision-Based Proprioception and Tactile Sensing](https://arxiv.org/abs/2508.18443)
*Ruohan Zhang,Uksang Yoo,Yichen Li,Arpit Argawal,Wenzhen Yuan*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的基于视觉的软体机器人传感方法，通过嵌入式摄像头实现了高分辨率的本体感知和触觉传感，并结合了一个从仿真到现实的零样本知识迁移管道。


<details>
  <summary>Details</summary>
Motivation: 软体气动机械手因其柔顺性和灵活性在工业和人机交互应用中很受欢迎，但在实际部署中需要先进的触觉反馈和本体感知能力。

Method: 研究采用了一种新颖的基于视觉的传感方法，并在PneuGelSight（一种具有嵌入式摄像头实现高分辨率本体感知和触觉传感的气动机械手）上进行了演示。为了优化传感器性能，引入了一个全面的管道来精确模拟其光学和动态特性，以实现从仿真到现实的零样本知识迁移。

Result: PneuGelSight通过嵌入式摄像头实现了高分辨率的本体感知和触觉传感。所提出的仿真到现实管道能够促进零样本知识从仿真环境到实际应用的有效迁移。

Conclusion: PneuGelSight及其仿真到现实的管道为软体机器人提供了一种新颖、易于实现且鲁棒的传感方法，为开发具有增强感知能力的更先进软体机器人铺平了道路。

Abstract: Soft pneumatic robot manipulators are popular in industrial and
human-interactive applications due to their compliance and flexibility.
However, deploying them in real-world scenarios requires advanced sensing for
tactile feedback and proprioception. Our work presents a novel vision-based
approach for sensorizing soft robots. We demonstrate our approach on
PneuGelSight, a pioneering pneumatic manipulator featuring high-resolution
proprioception and tactile sensing via an embedded camera. To optimize the
sensor's performance, we introduce a comprehensive pipeline that accurately
simulates its optical and dynamic properties, facilitating a zero-shot
knowledge transition from simulation to real-world applications. PneuGelSight
and our sim-to-real pipeline provide a novel, easily implementable, and robust
sensing methodology for soft robots, paving the way for the development of more
advanced soft robots with enhanced sensory capabilities.

</details>


### [179] [Mimicking associative learning of rats via a neuromorphic robot in open field maze using spatial cell models](https://arxiv.org/abs/2508.18460)
*Tianze Liu,Md Abu Bakr Siddique,Hongyu An*

Main category: cs.RO

TL;DR: 针对传统AI在高功耗和适应性方面的挑战，本文提出通过模仿动物的联想学习机制，利用神经拟态机器人在迷宫环境中实现基于空间细胞模型的在线联想学习，以增强机器人自主性，特别适用于SWaP受限应用。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动AI依赖大量数据集和神经网络，导致高功耗和适应性受限，尤其在行星探索等SWaP（尺寸、重量、功耗）受限应用中面临严峻挑战。因此，需要增强智能机器人的自主能力。

Method: 本文提出通过模仿动物的联想学习机制来增强机器人自主性，该机制使动物能通过记忆并发事件适应环境。具体方法是使用神经拟态机器人在开放式迷宫环境中模拟啮齿动物的联想学习，并利用来自空间细胞（如位置细胞和网格细胞）的见解来整合这些模型。

Result: 通过整合生物启发模型，旨在实现实时场景中的空间任务在线联想学习，从而弥合生物空间认知与机器人技术之间的鸿沟，推动自主系统的发展。

Conclusion: 通过在神经拟态机器人中模拟动物联想学习和空间细胞机制，可以有效解决传统AI在高功耗和适应性方面的挑战，为SWaP受限应用中的智能机器人提供更强的自主学习和导航能力，从而实现自主系统的进步。

Abstract: Data-driven Artificial Intelligence (AI) approaches have exhibited remarkable
prowess across various cognitive tasks using extensive training data. However,
the reliance on large datasets and neural networks presents challenges such as
highpower consumption and limited adaptability, particularly in
SWaP-constrained applications like planetary exploration. To address these
issues, we propose enhancing the autonomous capabilities of intelligent robots
by emulating the associative learning observed in animals. Associative learning
enables animals to adapt to their environment by memorizing concurrent events.
By replicating this mechanism, neuromorphic robots can navigate dynamic
environments autonomously, learning from interactions to optimize performance.
This paper explores the emulation of associative learning in rodents using
neuromorphic robots within open-field maze environments, leveraging insights
from spatial cells such as place and grid cells. By integrating these models,
we aim to enable online associative learning for spatial tasks in real-time
scenarios, bridging the gap between biological spatial cognition and robotics
for advancements in autonomous systems.

</details>


### [180] [SignLoc: Robust Localization using Navigation Signs and Public Maps](https://arxiv.org/abs/2508.18606)
*Nicky Zimmerman,Joel Loo,Ayush Agrawal,David Hsu*

Main category: cs.RO

TL;DR: SignLoc是一种利用导航标志和公开地图（如平面图和OpenStreetMap）进行机器人全局定位的方法，无需预先传感器建图，通过匹配标志信息到导航图实现鲁棒的拓扑语义定位。


<details>
  <summary>Details</summary>
Motivation: 导航标志和地图在人类环境中普遍存在，是寻路的重要辅助工具，但机器人系统很少利用它们进行定位，尤其是在没有预先传感器建图的情况下。

Method: SignLoc首先从输入地图中提取导航图。然后，它采用概率观测模型，将检测到的标志中的方向和位置线索与导航图进行匹配。最后，在蒙特卡洛框架内实现鲁棒的拓扑语义定位。

Result: 在大学校园、购物中心和医院综合体等多样化的大规模环境中进行评估，实验结果表明SignLoc在观察一到两个标志后就能可靠地定位机器人。

Conclusion: SignLoc是一种有效的机器人全局定位方法，它成功地利用导航标志和公开地图，实现了在没有预先传感器建图的情况下，通过少量标志观测即可进行可靠定位。

Abstract: Navigation signs and maps, such as floor plans and street maps, are widely
available and serve as ubiquitous aids for way-finding in human environments.
Yet, they are rarely used by robot systems. This paper presents SignLoc, a
global localization method that leverages navigation signs to localize the
robot on publicly available maps -- specifically floor plans and OpenStreetMap
(OSM) graphs -- without prior sensor-based mapping. SignLoc first extracts a
navigation graph from the input map. It then employs a probabilistic
observation model to match directional and locational cues from the detected
signs to the graph, enabling robust topo-semantic localization within a Monte
Carlo framework. We evaluated SignLoc in diverse large-scale environments: part
of a university campus, a shopping mall, and a hospital complex. Experimental
results show that SignLoc reliably localizes the robot after observing only one
to two signs.

</details>


### [181] [Integration of Robot and Scene Kinematics for Sequential Mobile Manipulation Planning](https://arxiv.org/abs/2508.18627)
*Ziyuan Jiao,Yida Niu,Zeyu Zhang,Yangyang Wu,Yao Su,Yixin Zhu,Hangxin Liu,Song-Chun Zhu*

Main category: cs.RO

TL;DR: 本文提出了一个顺序移动操作规划（SMMP）框架，通过构建增强配置空间（A-Space）来统一导航和操作约束，并采用三级规划框架，有效解决了涉及关节物体的长周期多步骤移动操作任务。


<details>
  <summary>Details</summary>
Motivation: 解决长周期、多步骤移动操作任务的挑战，特别是需要协调全身运动并与关节物体交互的情况。

Method: 将环境结构抽象为运动学模型，并与机器人运动学集成，构建一个增强配置空间（A-Space），该空间统一了导航和操作约束，并考虑了机器人基座、手臂和操作物体的联合可达性。采用三级框架：任务规划器生成符号动作序列，运动规划器计算A-Space内的连续轨迹，以及中间计划细化阶段选择确保长期可行性的动作目标。

Result: 仿真研究表明，A-Space规划的任务成功率比基线方法高出84.6%。在真实机器人系统上的验证展示了流畅的移动操作，涉及7种刚性和关节物体、17种不同场景，以及长达14个顺序步骤的任务。

Conclusion: 将场景运动学建模为规划实体，而非编码任务特定约束，对于复杂机器人操作具有重要意义，提供了一种可扩展和泛化的方法。

Abstract: We present a Sequential Mobile Manipulation Planning (SMMP) framework that
can solve long-horizon multi-step mobile manipulation tasks with coordinated
whole-body motion, even when interacting with articulated objects. By
abstracting environmental structures as kinematic models and integrating them
with the robot's kinematics, we construct an Augmented Configuration Apace
(A-Space) that unifies the previously separate task constraints for navigation
and manipulation, while accounting for the joint reachability of the robot
base, arm, and manipulated objects. This integration facilitates efficient
planning within a tri-level framework: a task planner generates symbolic action
sequences to model the evolution of A-Space, an optimization-based motion
planner computes continuous trajectories within A-Space to achieve desired
configurations for both the robot and scene elements, and an intermediate plan
refinement stage selects action goals that ensure long-horizon feasibility. Our
simulation studies first confirm that planning in A-Space achieves an 84.6\%
higher task success rate compared to baseline methods. Validation on real
robotic systems demonstrates fluid mobile manipulation involving (i) seven
types of rigid and articulated objects across 17 distinct contexts, and (ii)
long-horizon tasks of up to 14 sequential steps. Our results highlight the
significance of modeling scene kinematics into planning entities, rather than
encoding task-specific constraints, offering a scalable and generalizable
approach to complex robotic manipulation.

</details>


### [182] [Engineering Automotive Digital Twins on Standardized Architectures: A Case Study](https://arxiv.org/abs/2508.18662)
*Stefan Ramdhan,Winnie Trandinh,Istvan David,Vera Pantelic,Mark Lawford*

Main category: cs.RO

TL;DR: 本文通过一个自适应巡航控制数字孪生案例研究，评估了ISO 23247参考架构在汽车数字孪生开发中的适用性，并指出了其优缺点。


<details>
  <summary>Details</summary>
Motivation: 汽车行业对利用数字孪生提供智能服务的需求日益增长，但汽车数字孪生架构指南稀缺。ISO 23247是目前唯一的数字孪生架构标准，因此需要研究其在汽车领域的适用性。

Method: 通过一个案例研究进行评估：为一辆1/10比例的自动驾驶汽车开发一个自适应巡航控制（ACC）数字孪生。

Result: 识别了ISO 23247参考架构在开发汽车数字孪生方面的优点和局限性。

Conclusion: 为研究人员、实践者和标准开发者提炼了未来研究方向，以改进汽车数字孪生架构指南和标准。

Abstract: Digital twin (DT) technology has become of interest in the automotive
industry. There is a growing need for smarter services that utilize the unique
capabilities of DTs, ranging from computer-aided remote control to cloud-based
fleet coordination. Developing such services starts with the software
architecture. However, the scarcity of DT architectural guidelines poses a
challenge for engineering automotive DTs. Currently, the only DT architectural
standard is the one defined in ISO 23247. Though not developed for automotive
systems, it is one of the few feasible starting points for automotive DTs. In
this work, we investigate the suitability of the ISO 23247 reference
architecture for developing automotive DTs. Through the case study of
developing an Adaptive Cruise Control DT for a 1/10\textsuperscript{th}-scale
autonomous vehicle, we identify some strengths and limitations of the reference
architecture and begin distilling future directions for researchers,
practitioners, and standard developers.

</details>


### [183] [Deep Sensorimotor Control by Imitating Predictive Models of Human Motion](https://arxiv.org/abs/2508.18691)
*Himanshu Gaurav Singh,Pieter Abbeel,Jitendra Malik,Antonio Loquercio*

Main category: cs.RO

TL;DR: 该研究提出了一种新颖的强化学习技术，通过模仿人类运动的预测模型来训练机器人的感觉运动策略。利用人类关键点运动与机器人末端执行器关键点运动的相似性，实现了人类数据模型在机器人数据上的零样本迁移，并显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 随着机器人与人类之间具身差距的缩小，利用人类与环境交互的数据集进行机器人学习的机会增多。然而，现有方法因梯度基运动重定向和对抗性损失的限制，未能充分利用现代人类-场景交互数据集的规模和多样性。

Method: 研究提出通过模仿人类运动的预测模型来训练感觉运动策略。其核心思想是，机器人末端执行器上受人类启发设计的关键点运动与相应的人体关键点运动密切相关。这使得在人类数据上训练的未来运动预测模型可以“零样本”应用于机器人数据。通过让感觉运动策略跟踪这些预测（以过去机器人状态历史为条件），同时优化相对稀疏的任务奖励来训练策略。此方法完全绕过了梯度基运动重定向和对抗性损失。

Result: 实验结果表明，该方法适用于多种机器人和任务，并以显著优势超越了现有基线。此外，研究发现跟踪人类运动模型可以替代在操作任务中精心设计的密集奖励和课程。

Conclusion: 该研究提出了一种有效利用人类运动数据进行机器人学习的新方法，通过零样本迁移和模仿人类运动预测模型，不仅提高了机器人学习性能，还简化了奖励设计，展现了在不同机器人和任务中的普适性。

Abstract: As the embodiment gap between a robot and a human narrows, new opportunities
arise to leverage datasets of humans interacting with their surroundings for
robot learning. We propose a novel technique for training sensorimotor policies
with reinforcement learning by imitating predictive models of human motions.
Our key insight is that the motion of keypoints on human-inspired robot
end-effectors closely mirrors the motion of corresponding human body keypoints.
This enables us to use a model trained to predict future motion on human data
\emph{zero-shot} on robot data. We train sensorimotor policies to track the
predictions of such a model, conditioned on a history of past robot states,
while optimizing a relatively sparse task reward. This approach entirely
bypasses gradient-based kinematic retargeting and adversarial losses, which
limit existing methods from fully leveraging the scale and diversity of modern
human-scene interaction datasets. Empirically, we find that our approach can
work across robots and tasks, outperforming existing baselines by a large
margin. In addition, we find that tracking a human motion model can substitute
for carefully designed dense rewards and curricula in manipulation tasks. Code,
data and qualitative results available at
https://jirl-upenn.github.io/track_reward/.

</details>


### [184] [AgriChrono: A Multi-modal Dataset Capturing Crop Growth and Lighting Variability with a Field Robot](https://arxiv.org/abs/2508.18694)
*Jaehwan Jeong,Tuan-Anh Vu,Mohammad Jony,Shahab Ahmad,Md. Mukhlesur Rahman,Sangpil Kim,M. Khalid Jawed*

Main category: cs.RO

TL;DR: 本文介绍了AgriChrono，一个用于在真实农田环境中收集多模态、长期、时间同步数据的机器人平台和数据集，旨在解决现有农业数据集的局限性，并促进模型在动态条件下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的精准农业数据集主要在静态或受控环境中（如室内实验室或温室）收集，传感器多样性有限，时间跨度短。这些条件无法反映真实农田的动态性质（如光照变化、作物生长变化和自然干扰），导致在此类数据上训练的模型在实际应用中缺乏鲁棒性和泛化能力。

Method: 研究人员开发了AgriChrono，一个新型机器人数据收集平台，集成了RGB、深度、激光雷达和IMU等多种传感器，支持远程、时间同步地长期数据采集。该平台能够在不同光照和作物生长阶段高效且可重复地收集数据，以捕捉真实农业环境的动态条件。

Result: AgriChrono平台成功收集了一个多模态数据集，反映了真实世界的动态农业环境。研究人员在该数据集上对一系列最先进的3D重建模型进行了基准测试，结果突出了在真实农田环境中进行重建的难度，并证明了该数据集作为研究资产对于提高模型在动态条件下泛化能力的价值。

Conclusion: AgriChrono数据集和平台为解决现有农业数据集的局限性提供了有效方案，它是一个宝贵的研究资源，有助于推动模型在动态农业环境中的泛化能力和鲁棒性。

Abstract: Existing datasets for precision agriculture have primarily been collected in
static or controlled environments such as indoor labs or greenhouses, often
with limited sensor diversity and restricted temporal span. These conditions
fail to reflect the dynamic nature of real farmland, including illumination
changes, crop growth variation, and natural disturbances. As a result, models
trained on such data often lack robustness and generalization when applied to
real-world field scenarios. In this paper, we present AgriChrono, a novel
robotic data collection platform and multi-modal dataset designed to capture
the dynamic conditions of real-world agricultural environments. Our platform
integrates multiple sensors and enables remote, time-synchronized acquisition
of RGB, Depth, LiDAR, and IMU data, supporting efficient and repeatable
long-term data collection across varying illumination and crop growth stages.
We benchmark a range of state-of-the-art 3D reconstruction models on the
AgriChrono dataset, highlighting the difficulty of reconstruction in real-world
field environments and demonstrating its value as a research asset for
advancing model generalization under dynamic conditions. The code and dataset
are publicly available at: https://github.com/StructuresComp/agri-chrono

</details>


### [185] [Enhancing Video-Based Robot Failure Detection Using Task Knowledge](https://arxiv.org/abs/2508.18705)
*Santosh Thoduka,Sebastian Houben,Juergen Gall,Paul G. Plöger*

Main category: cs.RO

TL;DR: 本文提出了一种基于视频的机器人故障检测方法，利用机器人动作和任务相关物体的时空知识，并通过变帧率数据增强显著提高了检测性能。


<details>
  <summary>Details</summary>
Motivation: 为了实现鲁棒的机器人任务执行，需要可靠地检测执行故障以触发安全操作、恢复策略或任务重新规划。然而，许多现有的故障检测方法在各种实际场景中难以提供有意义的性能。

Method: 本文提出了一种基于视频的故障检测方法，该方法利用机器人执行的动作和视野中任务相关物体的时空知识。这些信息在大多数机器人场景中都可获得。此外，还提出了一种数据增强方法，通过对视频不同部分应用可变帧率来提高性能。该方法在三个数据集上进行了验证，其中部分数据集补充了额外的任务相关知识标注。

Result: 在ARMBench数据集上，该方法在不增加额外计算开销的情况下，F1分数从77.9提高到80.0；通过测试时增强，F1分数进一步提高到81.4。结果强调了故障检测过程中时空信息的重要性。

Conclusion: 该方法有效证明了时空信息在机器人故障检测中的重要性。研究结果表明，未来的实现中应进一步探索合适的启发式方法。

Abstract: Robust robotic task execution hinges on the reliable detection of execution
failures in order to trigger safe operation modes, recovery strategies, or task
replanning. However, many failure detection methods struggle to provide
meaningful performance when applied to a variety of real-world scenarios. In
this paper, we propose a video-based failure detection approach that uses
spatio-temporal knowledge in the form of the actions the robot performs and
task-relevant objects within the field of view. Both pieces of information are
available in most robotic scenarios and can thus be readily obtained. We
demonstrate the effectiveness of our approach on three datasets that we amend,
in part, with additional annotations of the aforementioned task-relevant
knowledge. In light of the results, we also propose a data augmentation method
that improves performance by applying variable frame rates to different parts
of the video. We observe an improvement from 77.9 to 80.0 in F1 score on the
ARMBench dataset without additional computational expense and an additional
increase to 81.4 with test-time augmentation. The results emphasize the
importance of spatio-temporal information during failure detection and suggest
further investigation of suitable heuristics in future implementations. Code
and annotations are available.

</details>


### [186] [Real-time Testing of Satellite Attitude Control With a Reaction Wheel Hardware-In-the-Loop Platform](https://arxiv.org/abs/2508.19164)
*Morokot Sakal,George Nehma,Camilo Riano-Rios,Madhur Tiwari*

Main category: cs.RO

TL;DR: 本文提出并实施了一种带有反作用轮健康估计功能的自适应卫星姿态控制系统的硬件在环（HIL）测试，旨在通过真实硬件验证控制器。


<details>
  <summary>Details</summary>
Motivation: 之前的仿真和软件在环测试促使研究人员进行进一步实验，以验证该控制器在包含真实动量交换设备的闭环系统中的有效性。这项工作是建立一个全面的航天器姿态控制算法验证测试框架的重要一步。

Method: 该研究提出了一个HIL测试平台，包括使用CAN总线通信的无刷直流电机和驱动器、执行控制和自适应律的嵌入式计算机，以及一个生成模拟传感器数据、估计姿态状态并响应外部执行器动作的卫星模拟器。此外，研究还提出了人工诱导反作用轮故障的方法。

Result: 本文介绍了在HIL测试中人工诱导反作用轮故障的相关问题和经验教训。具体成果是构建了HIL测试平台并进行了初步验证，但抽象未提及具体的量化测试结果。

Conclusion: 该工作是向建立航天器姿态控制算法综合测试框架迈出的重要一步，并通过HIL测试积累了关于故障诱导和系统验证的宝贵经验和教训。

Abstract: We propose the Hardware-in-the-Loop (HIL) test of an adaptive satellite
attitude control system with reaction wheel health estimation capabilities.
Previous simulations and Software-in-the-Loop testing have prompted further
experiments to explore the validity of the controller with real momentum
exchange devices in the loop. This work is a step toward a comprehensive
testing framework for validation of spacecraft attitude control algorithms. The
proposed HIL testbed includes brushless DC motors and drivers that communicate
using a CAN bus, an embedded computer that executes control and adaptation
laws, and a satellite simulator that produces simulated sensor data, estimated
attitude states, and responds to actions of the external actuators. We propose
methods to artificially induce failures on the reaction wheels, and present
related issues and lessons learned.

</details>


### [187] [HyperTASR: Hypernetwork-Driven Task-Aware Scene Representations for Robust Manipulation](https://arxiv.org/abs/2508.18802)
*Li Sun,Jiefeng Wu,Feng Chen,Ruizhe Liu,Yanchao Yang*

Main category: cs.RO

TL;DR: HyperTASR是一个由超网络驱动的框架，它根据任务目标和执行阶段动态调整机器人操作的场景表示，从而显著提高策略学习的效率和表示质量。


<details>
  <summary>Details</summary>
Motivation: 机器人操作的有效策略学习需要选择性地捕捉任务相关环境特征的场景表示。然而，现有方法通常采用与任务无关的表示提取，未能模拟人类认知中观察到的动态感知适应。

Method: 本文提出了HyperTASR，一个超网络驱动的框架。它根据任务规范和进展状态动态生成表示转换参数，使表示在任务执行过程中能够根据上下文演变。该方法在任务上下文和状态依赖处理路径之间建立了计算分离，而不是简单地拼接或融合任务嵌入与任务无关的表示。

Result: 在仿真和真实世界环境中的综合评估表明，HyperTASR在不同的表示范式下都取得了显著的性能提升。通过消融研究和注意力可视化，证实了该方法选择性地优先处理任务相关的场景信息，密切模仿了人类在操作任务中的适应性感知。

Conclusion: HyperTASR通过动态调整场景表示，克服了现有任务无关表示方法的局限性，显著提高了机器人操作策略学习的效率和表示质量，并成功模仿了人类的适应性感知能力。

Abstract: Effective policy learning for robotic manipulation requires scene
representations that selectively capture task-relevant environmental features.
Current approaches typically employ task-agnostic representation extraction,
failing to emulate the dynamic perceptual adaptation observed in human
cognition. We present HyperTASR, a hypernetwork-driven framework that modulates
scene representations based on both task objectives and the execution phase.
Our architecture dynamically generates representation transformation parameters
conditioned on task specifications and progression state, enabling
representations to evolve contextually throughout task execution. This approach
maintains architectural compatibility with existing policy learning frameworks
while fundamentally reconfiguring how visual features are processed. Unlike
methods that simply concatenate or fuse task embeddings with task-agnostic
representations, HyperTASR establishes computational separation between
task-contextual and state-dependent processing paths, enhancing learning
efficiency and representational quality. Comprehensive evaluations in both
simulation and real-world environments demonstrate substantial performance
improvements across different representation paradigms. Through ablation
studies and attention visualization, we confirm that our approach selectively
prioritizes task-relevant scene information, closely mirroring human adaptive
perception during manipulation tasks. The project website is at
\href{https://lisunphil.github.io/HyperTASR_projectpage/}{lisunphil.github.io/HyperTASR\_projectpage}.

</details>


### [188] [Learning Real-World Acrobatic Flight from Human Preferences](https://arxiv.org/abs/2508.18817)
*Colin Merk,Ismail Geles,Jiaxu Xing,Angel Romero,Giorgia Ramponi,Davide Scaramuzza*

Main category: cs.RO

TL;DR: 本文探讨了基于偏好的强化学习（PbRL）在敏捷无人机特技飞行控制中的应用，提出了一种名为REC的新方法，显著优于标准Preference PPO，并成功实现了从模拟到真实世界的迁移，证明了PbRL在处理复杂、主观人类目标方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 特技飞行由于其复杂的动力学、快速运动和对精确执行的高要求，是一个特别具有挑战性的问题。传统上，手动设计奖励函数难以形式化或捕捉此类任务中固有的主观或风格化目标，因此需要一种无需手动设计奖励函数的方法。

Method: 该研究基于“基于偏好的近端策略优化”（Preference PPO），并提出了一种名为“置信度下奖励集成”（Reward Ensemble under Confidence, REC）的扩展，以改进偏好建模和学习稳定性。研究在模拟环境中训练策略，并将其成功迁移到真实世界的无人机上，同时也在MuJoCo环境中验证了概率奖励模型的适用性。

Result: 所提出的REC方法达到了整形奖励性能的88.4%，显著优于标准Preference PPO的55.2%。研究成功地将策略从模拟环境迁移到真实世界的无人机，实现了多种特技飞行动作，并强调了人类偏好对运动风格品质的重视。此外，研究发现手动设计的奖励与人类偏好的一致性仅为60.7%，突显了其局限性。

Conclusion: 研究结果强调了PbRL在捕捉跨物理和模拟领域的复杂、以人为中心的目标方面的有效性，尤其适用于难以形式化或具有主观性的任务，如敏捷无人机特技飞行控制。

Abstract: Preference-based reinforcement learning (PbRL) enables agents to learn
control policies without requiring manually designed reward functions, making
it well-suited for tasks where objectives are difficult to formalize or
inherently subjective. Acrobatic flight poses a particularly challenging
problem due to its complex dynamics, rapid movements, and the importance of
precise execution. In this work, we explore the use of PbRL for agile drone
control, focusing on the execution of dynamic maneuvers such as powerloops.
Building on Preference-based Proximal Policy Optimization (Preference PPO), we
propose Reward Ensemble under Confidence (REC), an extension to the reward
learning objective that improves preference modeling and learning stability.
Our method achieves 88.4% of the shaped reward performance, compared to 55.2%
with standard Preference PPO. We train policies in simulation and successfully
transfer them to real-world drones, demonstrating multiple acrobatic maneuvers
where human preferences emphasize stylistic qualities of motion. Furthermore,
we demonstrate the applicability of our probabilistic reward model in a
representative MuJoCo environment for continuous control. Finally, we highlight
the limitations of manually designed rewards, observing only 60.7% agreement
with human preferences. These results underscore the effectiveness of PbRL in
capturing complex, human-centered objectives across both physical and simulated
domains.

</details>


### [189] [AS2FM: Enabling Statistical Model Checking of ROS 2 Systems for Robust Autonomy](https://arxiv.org/abs/2508.18820)
*Christian Henkel,Marco Lampacrescia,Michaela Klauck,Matteo Morelli*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的方法，通过扩展SCXML格式并开发AS2FM工具将自主机器人系统模型转换为JANI，从而在设计时使用统计模型检查（SMC）对ROS 2和行为树（BT）驱动的自主机器人系统进行形式化验证。


<details>
  <summary>Details</summary>
Motivation: 在不可预测的环境中设计自主机器人系统是一项挑战性任务，需要一种在设计阶段就能验证系统属性的方法。

Method: 该研究引入了SCXML格式的扩展，用于建模包含ROS 2和行为树（BT）特性的系统组件。然后，开发了Autonomous Systems to Formal Models (AS2FM) 工具，将完整的系统模型翻译成JANI（一种定量模型检查的标准格式），以便使用现成的SMC工具进行系统属性验证。

Result: AS2FM在实际自主机器人控制系统中的适用性以及验证运行时扩展性方面表现出实用性。在一个案例研究中，成功识别了基于ROS 2的机器人操作用例中的问题，且验证时间不到一秒。与现有技术相比，该方法在系统特性支持方面更全面，并且验证运行时随模型大小呈线性扩展，而非指数级。

Conclusion: 该研究提供了一种实用、高效且全面的设计时验证方法，能够快速识别自主机器人系统中的问题，并具有良好的可扩展性，显著优于现有技术。

Abstract: Designing robotic systems to act autonomously in unforeseen environments is a
challenging task. This work presents a novel approach to use formal
verification, specifically Statistical Model Checking (SMC), to verify system
properties of autonomous robots at design-time. We introduce an extension of
the SCXML format, designed to model system components including both Robot
Operating System 2 (ROS 2) and Behavior Tree (BT) features. Further, we
contribute Autonomous Systems to Formal Models (AS2FM), a tool to translate the
full system model into JANI. The use of JANI, a standard format for
quantitative model checking, enables verification of system properties with
off-the-shelf SMC tools. We demonstrate the practical usability of AS2FM both
in terms of applicability to real-world autonomous robotic control systems, and
in terms of verification runtime scaling. We provide a case study, where we
successfully identify problems in a ROS 2-based robotic manipulation use case
that is verifiable in less than one second using consumer hardware.
Additionally, we compare to the state of the art and demonstrate that our
method is more comprehensive in system feature support, and that the
verification runtime scales linearly with the size of the model, instead of
exponentially.

</details>


### [190] [VisionSafeEnhanced VPC: Cautious Predictive Control with Visibility Constraints under Uncertainty for Autonomous Robotic Surgery](https://arxiv.org/abs/2508.18937)
*Wang Jiayin,Wei Yanran,Jiang Lei,Guo Xiaoyu,Zheng Ayong,Zhao Weidong,Li Zhongkui*

Main category: cs.RO

TL;DR: 提出了一种名为VisionSafeEnhanced视觉预测控制（VPC）的鲁棒且不确定性自适应框架，用于在不确定性下自主控制腹腔镜，确保视野（FoV）安全，显著提高目标可见性。


<details>
  <summary>Details</summary>
Motivation: 尽管基于图像的视觉伺服（IBVS）控制在自主腹腔镜控制方面有所进展，但其对持续可见性的要求以及参数误差、测量噪声和有效载荷不确定性等复杂干扰会降低外科医生的视觉体验并危及手术安全。

Method: 该研究提出了VisionSafeEnhanced视觉预测控制（VPC）框架。首先，利用高斯过程回归（GPR）对操作不确定性（包括残余模型不确定性、随机不确定性和外部干扰）进行混合（确定性+随机性）量化。在此基础上，提出了一种具有概率保证的新型安全感知轨迹优化框架，其中基于不确定性传播给出了不确定性自适应安全控制障碍函数（CBF）条件，并同时基于概率近似公式化了机会约束。这种不确定性感知公式能够自适应地分配控制力，在保持鲁棒性的同时最大限度地减少不必要的摄像头运动。

Result: 通过在商用手术机器人平台（微创医疗图迈）上进行顺序多目标淋巴结清扫的对比模拟和实验验证了所提出的方法。与基线方法相比，该框架保持了接近完美的靶点可见性（>99.9%），并减少了跟踪误差。

Conclusion: 所提出的VisionSafeEnhanced VPC框架为自主腹腔镜控制提供了一种鲁棒且不确定性自适应的解决方案，在不确定性下保证了视野安全，显著提高了目标可见性和手术安全性。

Abstract: Autonomous control of the laparoscope in robot-assisted Minimally Invasive
Surgery (MIS) has received considerable research interest due to its potential
to improve surgical safety. Despite progress in pixel-level Image-Based Visual
Servoing (IBVS) control, the requirement of continuous visibility and the
existence of complex disturbances, such as parameterization error, measurement
noise, and uncertainties of payloads, could degrade the surgeon's visual
experience and compromise procedural safety. To address these limitations, this
paper proposes VisionSafeEnhanced Visual Predictive Control (VPC), a robust and
uncertainty-adaptive framework for autonomous laparoscope control that
guarantees Field of View (FoV) safety under uncertainty. Firstly, Gaussian
Process Regression (GPR) is utilized to perform hybrid (deterministic +
stochastic) quantification of operational uncertainties including residual
model uncertainties, stochastic uncertainties, and external disturbances. Based
on uncertainty quantification, a novel safety aware trajectory optimization
framework with probabilistic guarantees is proposed, where a
uncertainty-adaptive safety Control Barrier Function (CBF) condition is given
based on uncertainty propagation, and chance constraints are simultaneously
formulated based on probabilistic approximation. This uncertainty aware
formulation enables adaptive control effort allocation, minimizing unnecessary
camera motion while maintaining robustness. The proposed method is validated
through comparative simulations and experiments on a commercial surgical robot
platform (MicroPort MedBot Toumai) performing a sequential multi-target lymph
node dissection. Compared with baseline methods, the framework maintains
near-perfect target visibility (>99.9%), reduces tracking e

</details>


### [191] [Enhanced UAV Path Planning Using the Tangent Intersection Guidance (TIG) Algorithm](https://arxiv.org/abs/2508.18967)
*Hichem Cheriet,Khellat Kihel Badra,Chouraqui Samira*

Main category: cs.RO

TL;DR: 本文提出了一种名为切线交点引导（TIG）的先进算法，用于无人机在静态和动态环境中的高效安全路径规划，该算法能生成更短、更快、转弯更少的路径，并具备实时避障能力。


<details>
  <summary>Details</summary>
Motivation: 无人机（UAV）在作战支援、包裹递送和搜救行动等各种应用中，高效安全的导航至关重要。

Method: TIG算法采用椭圆切线交点方法生成可行路径，为每个威胁生成两条子路径，基于启发式规则选择最优路线，并迭代优化直至目标。为满足无人机运动学和动力学约束，采用基于二次贝塞尔曲线的修正平滑技术生成平滑高效的路线。

Result: 在静态环境中，TIG算法能以更少的时间（从0.01秒开始）和更少的转弯角度生成最短路径，优于A*、PRM、RRT*、Tangent Graph和Static APPATT算法。在完全未知和部分已知的动态环境中，TIG算法在碰撞避免方面表现出高效的实时路径规划能力，优于APF和Dynamic APPATT算法。

Conclusion: TIG算法是一种先进的无人机路径规划方法，在静态和动态环境中均能实现高效安全的导航，尤其在路径长度、计算时间和实时避障方面表现出色。

Abstract: Efficient and safe navigation of Unmanned Aerial Vehicles (UAVs) is critical
for various applications, including combat support, package delivery and Search
and Rescue Operations. This paper introduces the Tangent Intersection Guidance
(TIG) algorithm, an advanced approach for UAV path planning in both static and
dynamic environments. The algorithm uses the elliptic tangent intersection
method to generate feasible paths. It generates two sub-paths for each threat,
selects the optimal route based on a heuristic rule, and iteratively refines
the path until the target is reached. Considering the UAV kinematic and dynamic
constraints, a modified smoothing technique based on quadratic B\'ezier curves
is adopted to generate a smooth and efficient route. Experimental results show
that the TIG algorithm can generate the shortest path in less time, starting
from 0.01 seconds, with fewer turning angles compared to A*, PRM, RRT*, Tangent
Graph, and Static APPATT algorithms in static environments. Furthermore, in
completely unknown and partially known environments, TIG demonstrates efficient
real-time path planning capabilities for collision avoidance, outperforming APF
and Dynamic APPATT algorithms.

</details>


### [192] [HuBE: Cross-Embodiment Human-like Behavior Execution for Humanoid Robots](https://arxiv.org/abs/2508.19002)
*Shipeng Lyu,Fangyuan Wang,Weiwei Lin,Luhao Zhu,David Navarro-Alarcon,Guodong Guo*

Main category: cs.RO

TL;DR: 本文提出HuBE框架，一个双层闭环系统，结合机器人状态、目标姿态和情境，为仿人机器人生成兼具行为相似性和适当性的人类般运动，并通过骨骼缩放增强实现跨机器人适应性。


<details>
  <summary>Details</summary>
Motivation: 仿人机器人生成类人运动面临行为相似性、适当性以及跨载体适应性方面的挑战。

Method: 1. 提出HuBE，一个双层闭环框架，整合机器人状态、目标姿态和情境，生成类人行为，确保行为相似性和适当性，并消除运动生成与执行间的结构不匹配。2. 构建HPose，一个包含精细情境标注的上下文丰富数据集。3. 引入基于骨骼缩放的数据增强策略，确保异构仿人机器人之间的毫米级兼容性。

Result: 在多个商用平台上进行全面评估表明，HuBE在运动相似性、行为适当性和计算效率方面显著优于现有基线。

Conclusion: HuBE为在不同仿人机器人上实现可迁移的类人行为执行奠定了坚实基础。

Abstract: Achieving both behavioral similarity and appropriateness in human-like motion
generation for humanoid robot remains an open challenge, further compounded by
the lack of cross-embodiment adaptability. To address this problem, we propose
HuBE, a bi-level closed-loop framework that integrates robot state, goal poses,
and contextual situations to generate human-like behaviors, ensuring both
behavioral similarity and appropriateness, and eliminating structural
mismatches between motion generation and execution. To support this framework,
we construct HPose, a context-enriched dataset featuring fine-grained
situational annotations. Furthermore, we introduce a bone scaling-based data
augmentation strategy that ensures millimeter-level compatibility across
heterogeneous humanoid robots. Comprehensive evaluations on multiple commercial
platforms demonstrate that HuBE significantly improves motion similarity,
behavioral appropriateness, and computational efficiency over state-of-the-art
baselines, establishing a solid foundation for transferable and human-like
behavior execution across diverse humanoid robots.

</details>


### [193] [An LLM-powered Natural-to-Robotic Language Translation Framework with Correctness Guarantees](https://arxiv.org/abs/2508.19074)
*ZhenDong Chen,ZhanShang Nie,ShiXing Wan,JunYi Li,YongTian Cheng,Shuai Zhao*

Main category: cs.RO

TL;DR: 本文提出了一种名为NRTrans的自然-机器人语言翻译框架，通过引入机器人技能语言（RSL）及其编译器和调试器，为大型语言模型（LLM）生成的机器人控制程序提供正确性验证和基于反馈的微调，从而显著提高LLM驱动的机器人应用的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法让LLM直接从自然语言任务生成机器人控制程序，但由于LLM的不一致性和任务复杂性，生成的代码常出现大量编程错误，尤其在使用轻量级LLM时，严重影响了有效性。

Method: 本文提出了一个自然-机器人语言翻译框架（NRTrans），它包含：1. 机器人技能语言（RSL），用于抽象控制程序的复杂细节，连接自然语言任务与底层机器人技能。2. RSL编译器和调试器，用于验证LLM生成的RSL程序，并向LLM提供错误反馈，以迭代精炼输出，直到程序通过验证。

Result: 实验表明，NRTrans在各种LLM和任务下均优于现有方法，并且即使对于轻量级LLM也能实现高成功率。

Conclusion: NRTrans通过在程序执行前提供LLM生成程序的正确性保证，显著增强了LLM驱动的机器人应用的有效性。

Abstract: The Large Language Models (LLM) are increasingly being deployed in robotics
to generate robot control programs for specific user tasks, enabling embodied
intelligence. Existing methods primarily focus on LLM training and prompt
design that utilize LLMs to generate executable programs directly from user
tasks in natural language. However, due to the inconsistency of the LLMs and
the high complexity of the tasks, such best-effort approaches often lead to
tremendous programming errors in the generated code, which significantly
undermines the effectiveness especially when the light-weight LLMs are applied.
This paper introduces a natural-robotic language translation framework that (i)
provides correctness verification for generated control programs and (ii)
enhances the performance of LLMs in program generation via feedback-based
fine-tuning for the programs. To achieve this, a Robot Skill Language (RSL) is
proposed to abstract away from the intricate details of the control programs,
bridging the natural language tasks with the underlying robot skills. Then, the
RSL compiler and debugger are constructed to verify RSL programs generated by
the LLM and provide error feedback to the LLM for refining the outputs until
being verified by the compiler. This provides correctness guarantees for the
LLM-generated programs before being offloaded to the robots for execution,
significantly enhancing the effectiveness of LLM-powered robotic applications.
Experiments demonstrate NRTrans outperforms the existing method under a range
of LLMs and tasks, and achieves a high success rate for light-weight LLMs.

</details>


### [194] [DELIVER: A System for LLM-Guided Coordinated Multi-Robot Pickup and Delivery using Voronoi-Based Relay Planning](https://arxiv.org/abs/2508.19114)
*Alkesh K. Srivastava,Jared Michael Levin,Alexander Derrico,Philip Dames*

Main category: cs.RO

TL;DR: DELIVER是一个用于多机器人协同取送货的集成框架，通过自然语言指令驱动，实现了可扩展、无碰撞的协调。


<details>
  <summary>Details</summary>
Motivation: 研究动机是实现现实世界中由自然语言指令驱动的多机器人系统的可扩展、无碰撞协作，解决传统系统在复杂环境下的局限性。

Method: DELIVER框架整合了自然语言理解（使用轻量级LLaMA3提取地点）、空间分解（使用Voronoi tessellation划分机器人操作区域）、中继规划（计算最佳中继点和协调交接）以及运动执行（通过有限状态机管理机器人行为）。该框架在MultiTRAIL仿真平台、ROS2-based Gazebo仿真和TurtleBot3真实硬件上进行了实现和验证。

Result: 实验结果表明，DELIVER在不同团队规模下保持一致的任务成本，并与单代理系统相比，将每个代理的工作负载降低了高达55%。此外，即使团队规模增加，活跃中继代理的数量也保持在较低水平，证明了系统的可扩展性和高效的代理利用率。

Conclusion: DELIVER提供了一个模块化、可扩展的架构，用于语言引导的多机器人协调，推动了信息物理系统集成的前沿发展。

Abstract: We present DELIVER (Directed Execution of Language-instructed Item Via
Engineered Relay), a fully integrated framework for cooperative multi-robot
pickup and delivery driven by natural language commands. DELIVER unifies
natural language understanding, spatial decomposition, relay planning, and
motion execution to enable scalable, collision-free coordination in real-world
settings. Given a spoken or written instruction, a lightweight instance of
LLaMA3 interprets the command to extract pickup and delivery locations. The
environment is partitioned using a Voronoi tessellation to define
robot-specific operating regions. Robots then compute optimal relay points
along shared boundaries and coordinate handoffs. A finite-state machine governs
each robot's behavior, enabling robust execution. We implement DELIVER on the
MultiTRAIL simulation platform and validate it in both ROS2-based Gazebo
simulations and real-world hardware using TurtleBot3 robots. Empirical results
show that DELIVER maintains consistent mission cost across varying team sizes
while reducing per-agent workload by up to 55% compared to a single-agent
system. Moreover, the number of active relay agents remains low even as team
size increases, demonstrating the system's scalability and efficient agent
utilization. These findings underscore DELIVER's modular and extensible
architecture for language-guided multi-robot coordination, advancing the
frontiers of cyber-physical system integration.

</details>


### [195] [ZeST: an LLM-based Zero-Shot Traversability Navigation for Unknown Environments](https://arxiv.org/abs/2508.19131)
*Shreya Gummadi,Mateus V. Gasparino,Gianluca Capezzuto,Marcelo Becker,Girish Chowdhary*

Main category: cs.RO

TL;DR: ZeST利用大型语言模型（LLM）的视觉推理能力，实现零样本、实时地形可通行性预测，从而在不将机器人置于危险中的情况下，实现更安全的自主导航。


<details>
  <summary>Details</summary>
Motivation: 传统的地形可通行性预测模型数据采集方法需要将机器人置于潜在危险环境中，对设备和安全构成风险。

Method: 该研究提出了ZeST方法，利用大型语言模型（LLM）的视觉推理能力，实时创建可通行性地图，实现零样本可通行性预测，避免了真实世界数据采集的风险。

Result: 实验结果表明，与现有最先进的方法相比，ZeST在受控室内和非结构化室外环境中均能提供更安全的导航，并持续到达最终目标。

Conclusion: ZeST提供了一种经济高效、可扩展且更安全的解决方案，用于地形可通行性预测，加速了高级导航系统的开发，同时降低了与真实世界数据采集相关的风险。

Abstract: The advancement of robotics and autonomous navigation systems hinges on the
ability to accurately predict terrain traversability. Traditional methods for
generating datasets to train these prediction models often involve putting
robots into potentially hazardous environments, posing risks to equipment and
safety. To solve this problem, we present ZeST, a novel approach leveraging
visual reasoning capabilities of Large Language Models (LLMs) to create a
traversability map in real-time without exposing robots to danger. Our approach
not only performs zero-shot traversability and mitigates the risks associated
with real-world data collection but also accelerates the development of
advanced navigation systems, offering a cost-effective and scalable solution.
To support our findings, we present navigation results, in both controlled
indoor and unstructured outdoor environments. As shown in the experiments, our
method provides safer navigation when compared to other state-of-the-art
methods, constantly reaching the final goal.

</details>


### [196] [Uncertainty-Resilient Active Intention Recognition for Robotic Assistants](https://arxiv.org/abs/2508.19150)
*Juan Carlos Saborío,Marc Vinci,Oscar Lima,Sebastian Stock,Lennart Niecksch,Martin Günther,Alexander Sung,Joachim Hertzberg,Martin Atzmüller*

Main category: cs.RO

TL;DR: 本文提出一个基于意图识别POMDP的框架，旨在解决机器人助手中人类意图识别固有的不确定性和感知误差问题，以实现不确定环境下的协作规划和行动。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么通过明确提示限制了机器人自主性，要么通过假设近乎完美的信息而过于简化问题。研究发现，在人类意图识别中，对不确定结果和感知错误的推理是一个未被解决的关键空白。

Method: 本研究提出了一个对不确定性和传感器噪声具有鲁棒性的框架，该框架将实时传感器数据与多种规划器相结合。其核心是一个意图识别部分可观测马尔可夫决策过程（POMDP），用于解决不确定性下的协作规划和行动。

Result: 该集成框架已在实体机器人上成功测试，并取得了可喜的结果。

Conclusion: 该框架通过集成意图识别POMDP，有效解决了机器人助手中人类意图识别的不确定性和感知误差问题，实现了在不确定环境下的协作规划和行动。

Abstract: Purposeful behavior in robotic assistants requires the integration of
multiple components and technological advances. Often, the problem is reduced
to recognizing explicit prompts, which limits autonomy, or is oversimplified
through assumptions such as near-perfect information. We argue that a critical
gap remains unaddressed -- specifically, the challenge of reasoning about the
uncertain outcomes and perception errors inherent to human intention
recognition. In response, we present a framework designed to be resilient to
uncertainty and sensor noise, integrating real-time sensor data with a
combination of planners. Centered around an intention-recognition POMDP, our
approach addresses cooperative planning and acting under uncertainty. Our
integrated framework has been successfully tested on a physical robot with
promising results.

</details>


### [197] [QuadKAN: KAN-Enhanced Quadruped Motion Control via End-to-End Reinforcement Learning](https://arxiv.org/abs/2508.19153)
*Allen Wang,Gavin Tao*

Main category: cs.RO

TL;DR: 本文提出QuadKAN，一种基于KANs和样条参数化的跨模态策略，用于视觉引导的四足机器人运动控制。它有效融合本体感觉和视觉，在各种复杂地形下实现了比现有技术更鲁棒、高效和可解释的控制性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉引导的四足机器人运动控制需要更鲁棒的解决方案，尤其强调将本体感觉与视觉有效结合的重要性。

Method: 本文提出了QuadKAN框架，这是一种样条参数化的跨模态策略，通过Kolmogorov-Arnold Networks (KANs) 实现。该框架包含一个用于本体感觉的样条编码器和一个用于本体感觉-视觉输入的样条融合头。这种结构化的函数类别与步态的分段平滑特性对齐。研究采用了多模态延迟随机化 (MMDR) 并使用近端策略优化 (PPO) 进行端到端训练。

Result: 在包括平坦和不平坦表面以及有静态或动态障碍物的多样地形上进行评估，QuadKAN始终比最先进的基线方法实现更高的回报、更大的行进距离和更少的碰撞。此外，它还提高了样本效率，减少了动作抖动和能量消耗，并提供了可解释的姿态-动作敏感性。

Conclusion: 样条参数化策略为鲁棒的视觉引导运动控制提供了一种简单、有效且可解释的替代方案。

Abstract: We address vision-guided quadruped motion control with reinforcement learning
(RL) and highlight the necessity of combining proprioception with vision for
robust control. We propose QuadKAN, a spline-parameterized cross-modal policy
instantiated with Kolmogorov-Arnold Networks (KANs). The framework incorporates
a spline encoder for proprioception and a spline fusion head for
proprioception-vision inputs. This structured function class aligns the
state-to-action mapping with the piecewise-smooth nature of gait, improving
sample efficiency, reducing action jitter and energy consumption, and providing
interpretable posture-action sensitivities. We adopt Multi-Modal Delay
Randomization (MMDR) and perform end-to-end training with Proximal Policy
Optimization (PPO). Evaluations across diverse terrains, including both even
and uneven surfaces and scenarios with static or dynamic obstacles, demonstrate
that QuadKAN achieves consistently higher returns, greater distances, and fewer
collisions than state-of-the-art (SOTA) baselines. These results show that
spline-parameterized policies offer a simple, effective, and interpretable
alternative for robust vision-guided locomotion. A repository will be made
available upon acceptance.

</details>


### [198] [Direction Informed Trees (DIT*): Optimal Path Planning via Direction Filter and Direction Cost Heuristic](https://arxiv.org/abs/2508.19168)
*Liding Zhang,Kejia Chen,Kuanqi Cai,Yu Zhang,Yixuan Dang,Yansong Wu,Zhenshan Bing,Fan Wu,Sami Haddadin,Alois Knoll*

Main category: cs.RO

TL;DR: DIT* 是一种新的采样式路径规划器，通过优化搜索方向实现目标偏向和高效信息共享，从而在多维空间和实际场景中实现比现有算法更快的收敛速度。


<details>
  <summary>Details</summary>
Motivation: 现有的路径规划算法（如 EIT*）依赖努力启发式（effort heuristics）来指导搜索，但实现启发式的准确性和计算效率往往相互冲突，难以兼顾。

Method: 本文提出了方向信息树（DIT*）规划器，它将边定义为广义向量，并通过集成相似性索引建立方向过滤器，用于选择最近邻并估计方向成本。这些估计的方向成本启发式被用于边的评估，从而在探索过程中高效地共享方向信息，形成目标偏向。

Result: DIT* 在 R^4 到 R^16 的测试问题中，比现有的单查询、基于采样的规划器收敛更快，并且已在实际环境中各种规划任务中得到验证。

Conclusion: DIT* 通过优化搜索方向和利用方向成本启发式，有效地提高了采样式路径规划器的收敛速度和性能。

Abstract: Optimal path planning requires finding a series of feasible states from the
starting point to the goal to optimize objectives. Popular path planning
algorithms, such as Effort Informed Trees (EIT*), employ effort heuristics to
guide the search. Effective heuristics are accurate and computationally
efficient, but achieving both can be challenging due to their conflicting
nature. This paper proposes Direction Informed Trees (DIT*), a sampling-based
planner that focuses on optimizing the search direction for each edge,
resulting in goal bias during exploration. We define edges as generalized
vectors and integrate similarity indexes to establish a directional filter that
selects the nearest neighbors and estimates direction costs. The estimated
direction cost heuristics are utilized in edge evaluation. This strategy allows
the exploration to share directional information efficiently. DIT* convergence
faster than existing single-query, sampling-based planners on tested problems
in R^4 to R^16 and has been demonstrated in real-world environments with
various planning tasks. A video showcasing our experimental results is
available at: https://youtu.be/2SX6QT2NOek

</details>


### [199] [From Tabula Rasa to Emergent Abilities: Discovering Robot Skills via Real-World Unsupervised Quality-Diversity](https://arxiv.org/abs/2508.19172)
*Luca Grillotti,Lisa Coiffard,Oscar Pang,Maxence Faldor,Antoine Cully*

Main category: cs.RO

TL;DR: 本文提出了URSA（无监督真实世界技能获取）方法，它是QDAC的扩展，使机器人能够在真实世界中自主发现和掌握多样化的高性能技能。URSA在Unitree A1四足机器人上成功发现了多种运动技能，并可将所学技能用于损伤适应等下游任务，表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 机器人自主技能发现面临挑战，尤其是在物理硬件上，存在安全和数据效率限制。现有方法（如QDAC）需要手动定义技能空间和精心调整的启发式方法，这限制了它们在真实世界中的适用性。

Method: 本文提出了无监督真实世界技能获取（URSA）方法，作为质量多样性演员-评论家（QDAC）的扩展。URSA使机器人能够直接在真实世界中自主发现并掌握多样化、高性能的技能。该方法支持启发式驱动和完全无监督的技能发现设置，并在Unitree A1四足机器人上进行了验证。

Result: URSA成功地在模拟和真实世界中为Unitree A1四足机器人发现了多样化的运动技能。所学习的技能库可以用于下游任务，例如真实世界中的损伤适应。在损伤适应场景中，URSA在9个模拟场景中的5个以及5个真实世界场景中的3个中，表现优于所有基线方法。

Conclusion: URSA为真实世界机器人学习建立了一个新框架，实现了有限人类干预下的持续技能发现。这标志着向更自主和适应性强的机器人系统迈出了重要一步。

Abstract: Autonomous skill discovery aims to enable robots to acquire diverse behaviors
without explicit supervision. Learning such behaviors directly on physical
hardware remains challenging due to safety and data efficiency constraints.
Existing methods, including Quality-Diversity Actor-Critic (QDAC), require
manually defined skill spaces and carefully tuned heuristics, limiting
real-world applicability. We propose Unsupervised Real-world Skill Acquisition
(URSA), an extension of QDAC that enables robots to autonomously discover and
master diverse, high-performing skills directly in the real world. We
demonstrate that URSA successfully discovers diverse locomotion skills on a
Unitree A1 quadruped in both simulation and the real world. Our approach
supports both heuristic-driven skill discovery and fully unsupervised settings.
We also show that the learned skill repertoire can be reused for downstream
tasks such as real-world damage adaptation, where URSA outperforms all
baselines in 5 out of 9 simulated and 3 out of 5 real-world damage scenarios.
Our results establish a new framework for real-world robot learning that
enables continuous skill discovery with limited human intervention,
representing a significant step toward more autonomous and adaptable robotic
systems. Demonstration videos are available at
http://adaptive-intelligent-robotics.github.io/URSA .

</details>


### [200] [Real-Time Model Checking for Closed-Loop Robot Reactive Planning](https://arxiv.org/abs/2508.19186)
*Christopher Chandler,Bernd Porr,Giulia Lafratta,Alice Miller*

Main category: cs.RO

TL;DR: 本文提出了一种新的模型检测应用，可在低功耗自主机器人上实现实时的多步规划和避障，通过生物启发式方法和新颖的LiDAR数据离散化，显著优于传统反应式代理。


<details>
  <summary>Details</summary>
Motivation: 目前的反应式代理只能进行单步规划，无法满足自主车辆对安全、可靠和可解释的多步规划的需求，尤其是在资源受限设备上的实时应用。

Method: 开发了一种小型、专用模型检测算法，基于生物代理的“核心”知识和注意力，在低功耗设备上实时生成计划，无需预计算数据。该方法通过链接临时控制系统来抵消环境干扰，并使用对局部环境有界变化敏感的新颖2D LiDAR数据离散化。采用前向深度优先搜索的模型检测进行多步规划，并在死胡同和操场场景中进行了应用。

Result: 模型检测能够为局部避障创建高效的多步计划，性能优于只能进行单步规划的反应式代理。通过实证结果和两种基本属性的非正式证明，验证了该方法的有效性。

Conclusion: 模型检测可以有效地用于自主机器人的实时多步规划和局部避障，为自主车辆中安全、可靠和可解释的规划发展提供了一个有指导意义的案例研究。

Abstract: We present a new application of model checking which achieves real-time
multi-step planning and obstacle avoidance on a real autonomous robot. We have
developed a small, purpose-built model checking algorithm which generates plans
in situ based on "core" knowledge and attention as found in biological agents.
This is achieved in real-time using no pre-computed data on a low-powered
device. Our approach is based on chaining temporary control systems which are
spawned to counteract disturbances in the local environment that disrupt an
autonomous agent from its preferred action (or resting state). A novel
discretization of 2D LiDAR data sensitive to bounded variations in the local
environment is used. Multi-step planning using model checking by forward
depth-first search is applied to cul-de-sac and playground scenarios. Both
empirical results and informal proofs of two fundamental properties of our
approach demonstrate that model checking can be used to create efficient
multi-step plans for local obstacle avoidance, improving on the performance of
a reactive agent which can only plan one step. Our approach is an instructional
case study for the development of safe, reliable and explainable planning in
the context of autonomous vehicles.

</details>


### [201] [AutoRing: Imitation Learning--based Autonomous Intraocular Foreign Body Removal Manipulation with Eye Surgical Robot](https://arxiv.org/abs/2508.19191)
*Yue Wang,Wenjie Deng,Haotian Xue,Di Cui,Yiqi Chen,Mingchuan Zhou,Haochao Ying,Jian Wu*

Main category: cs.RO

TL;DR: AutoRing是一个模仿学习框架，用于自主进行眼内异物环操作，通过动态RCM校准和RCM-ACT架构解决了运动学不确定性问题，实现了在生物模拟眼模型中的抓取和定位任务。


<details>
  <summary>Details</summary>
Motivation: 眼内异物移除需要毫米级精度，但现有机器人系统主要依赖手动遥操作，学习曲线陡峭。自主操作面临运动学不确定性（可变运动缩放和远程运动中心点(RCM)变化）的挑战。

Method: 本文提出了AutoRing，一个用于自主眼内异物环操作的模仿学习框架。它整合了动态RCM校准以解决眼内器械变化导致的坐标系不一致问题，并引入了RCM-ACT架构，该架构结合了动作分块Transformer和实时运动学重新对齐。该系统仅使用专家演示的立体视觉数据和器械运动学（在生物模拟眼模型中）进行训练，无需明确的深度传感。

Result: AutoRing成功完成了环的抓取和定位任务，并在未校准的显微镜条件下展示了端到端自主性。

Conclusion: 研究结果为开发能够进行复杂眼内手术的智能眼科手术系统提供了一个可行的框架。

Abstract: Intraocular foreign body removal demands millimeter-level precision in
confined intraocular spaces, yet existing robotic systems predominantly rely on
manual teleoperation with steep learning curves. To address the challenges of
autonomous manipulation (particularly kinematic uncertainties from variable
motion scaling and variation of the Remote Center of Motion (RCM) point), we
propose AutoRing, an imitation learning framework for autonomous intraocular
foreign body ring manipulation. Our approach integrates dynamic RCM calibration
to resolve coordinate-system inconsistencies caused by intraocular instrument
variation and introduces the RCM-ACT architecture, which combines
action-chunking transformers with real-time kinematic realignment. Trained
solely on stereo visual data and instrument kinematics from expert
demonstrations in a biomimetic eye model, AutoRing successfully completes ring
grasping and positioning tasks without explicit depth sensing. Experimental
validation demonstrates end-to-end autonomy under uncalibrated microscopy
conditions. The results provide a viable framework for developing intelligent
eye-surgical systems capable of complex intraocular procedures.

</details>


### [202] [Planning-Query-Guided Model Generation for Model-Based Deformable Object Manipulation](https://arxiv.org/abs/2508.19199)
*Alex LaGrassa,Zixuan Huang,Dmitry Berenson,Oliver Kroemer*

Main category: cs.RO

TL;DR: 该论文提出了一种自动生成任务特定、空间自适应动态模型的方法，通过学习对象哪些区域需要高分辨率建模来提高高维空间（如可变形对象）规划的效率。


<details>
  <summary>Details</summary>
Motivation: 在高维空间（如涉及可变形对象）进行高效规划，需要计算上可行且足够表达的动态模型。

Method: 该方法通过一个基于扩散的模型生成器，根据定义规划查询的起始和目标点云来预测每个区域的模型分辨率。为了高效收集学习数据，采用两阶段过程：首先使用预测动态作为先验优化分辨率，然后直接通过闭环性能进行优化。

Result: 在树木操作任务中，该方法将规划速度提高了一倍，同时与使用全分辨率模型相比，任务性能仅有少量下降。

Conclusion: 该方法为利用先前的规划和控制数据，为新任务生成计算高效且足够表达的动态模型提供了一条途径。

Abstract: Efficient planning in high-dimensional spaces, such as those involving
deformable objects, requires computationally tractable yet sufficiently
expressive dynamics models. This paper introduces a method that automatically
generates task-specific, spatially adaptive dynamics models by learning which
regions of the object require high-resolution modeling to achieve good task
performance for a given planning query. Task performance depends on the complex
interplay between the dynamics model, world dynamics, control, and task
requirements. Our proposed diffusion-based model generator predicts per-region
model resolutions based on start and goal pointclouds that define the planning
query. To efficiently collect the data for learning this mapping, a two-stage
process optimizes resolution using predictive dynamics as a prior before
directly optimizing using closed-loop performance. On a tree-manipulation task,
our method doubles planning speed with only a small decrease in task
performance over using a full-resolution model. This approach informs a path
towards using previous planning and control data to generate computationally
efficient yet sufficiently expressive dynamics models for new tasks.

</details>


### [203] [MemoryVLA: Perceptual-Cognitive Memory in Vision-Language-Action Models for Robotic Manipulation](https://arxiv.org/abs/2508.19236)
*Hao Shi,Bin Xie,Yingfei Liu,Lin Sun,Fengrong Liu,Tiancai Wang,Erjin Zhou,Haoqiang Fan,Xiangyu Zhang,Gao Huang*

Main category: cs.RO

TL;DR: MemoryVLA是一种受人类记忆启发的新型认知-记忆-行动框架，旨在解决机器人操作中长期、时序依赖任务的挑战。它通过工作记忆和感知-认知记忆库处理时间上下文，在模拟和真实世界任务中显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 主流的VLA模型通常忽略时间上下文，难以处理长周期、非马尔可夫式的时序依赖任务。而认知科学表明，人类依赖工作记忆进行即时控制，并利用海马系统储存情景细节和语义要点以形成长期记忆，这些机制对处理复杂任务至关重要。

Method: MemoryVLA框架包括：1) 预训练的VLM将观测编码为感知和认知令牌，形成工作记忆。2) 一个感知-认知记忆库存储从工作记忆中整合的低级细节和高级语义。3) 工作记忆从记忆库中检索与决策相关的信息，与当前令牌自适应融合，并通过合并冗余更新记忆库。4) 一个记忆条件扩散动作专家利用这些令牌生成具有时间意识的动作序列。

Result: MemoryVLA在150多个模拟和真实世界任务中进行了评估。在SimplerEnv-Bridge、Fractal和LIBERO-5套件上，分别达到了71.9%、72.7%和96.5%的成功率，均优于现有最先进的基线CogACT和pi-0，其中在Bridge任务上提升了14.6%。在12个涵盖通用技能和长期时序依赖的真实世界任务中，MemoryVLA实现了84.0%的成功率，在长期任务上比最先进的基线提高了26%。

Conclusion: MemoryVLA通过引入受人类记忆机制启发的认知-记忆-行动框架，成功解决了机器人操作中处理时间上下文和长周期、时序依赖任务的挑战。其在多项模拟和真实世界任务上的优异表现，证明了该方法在提升机器人操作能力方面的有效性和潜力。

Abstract: Temporal context is essential for robotic manipulation because such tasks are
inherently non-Markovian, yet mainstream VLA models typically overlook it and
struggle with long-horizon, temporally dependent tasks. Cognitive science
suggests that humans rely on working memory to buffer short-lived
representations for immediate control, while the hippocampal system preserves
verbatim episodic details and semantic gist of past experience for long-term
memory. Inspired by these mechanisms, we propose MemoryVLA, a
Cognition-Memory-Action framework for long-horizon robotic manipulation. A
pretrained VLM encodes the observation into perceptual and cognitive tokens
that form working memory, while a Perceptual-Cognitive Memory Bank stores
low-level details and high-level semantics consolidated from it. Working memory
retrieves decision-relevant entries from the bank, adaptively fuses them with
current tokens, and updates the bank by merging redundancies. Using these
tokens, a memory-conditioned diffusion action expert yields temporally aware
action sequences. We evaluate MemoryVLA on 150+ simulation and real-world tasks
across three robots. On SimplerEnv-Bridge, Fractal, and LIBERO-5 suites, it
achieves 71.9%, 72.7%, and 96.5% success rates, respectively, all outperforming
state-of-the-art baselines CogACT and pi-0, with a notable +14.6 gain on
Bridge. On 12 real-world tasks spanning general skills and long-horizon
temporal dependencies, MemoryVLA achieves 84.0% success rate, with long-horizon
tasks showing a +26 improvement over state-of-the-art baseline. Project Page:
https://shihao1895.github.io/MemoryVLA

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [204] [Fast Multiagent Formation Stabilization with Sparse Universally Rigid Frameworks](https://arxiv.org/abs/2508.18483)
*Zhonggang Li,Geert Leus,Raj Thilak Rajan*

Main category: eess.SY

TL;DR: 本文提出了一种凸优化框架，用于设计仿射编队控制（AFC）中的应力矩阵，无需预定义刚性图。目标是实现通信链路更少但收敛速度更快的网络，并通过仿真证明了其优于现有解决方案。


<details>
  <summary>Details</summary>
Motivation: 仿射编队控制（AFC）是一个分布式网络控制系统，通常通过广义共识系统实现，其中应力矩阵（而非图拉普拉斯）编码图结构。普遍刚性框架（URFs）保证应力矩阵的存在性，并成为网络设计的指导原则。然而，现有方法可能需要预定义刚性图，且未明确优化通信链路数量和收敛速度。

Method: 本文提出了一种凸优化框架来设计AFC的应力矩阵，其核心特点是无需预先定义刚性图。优化目标是找到一个具有更少通信链路（稀疏性）且收敛速度更快的网络。

Result: 通过仿真，本文展示了所提出的解决方案能够产生一个更稀疏的图，同时相比现有最先进的解决方案，其收敛速度更快。

Conclusion: 所提出的凸优化框架能够有效地设计仿射编队控制中的应力矩阵，从而在不预定义刚性图的情况下，实现通信链路更少且收敛速度更快的网络，显著优于现有技术。

Abstract: Affine formation control (AFC) is a distributed networked control system that
has recently received increasing attention in various applications. AFC is
typically achieved using a generalized consensus system where the stress
matrix, which encodes the graph structure, is used instead of a graph
Laplacian. Universally rigid frameworks (URFs) guarantee the existence of the
stress matrix and have thus become the guideline for such a network design. In
this work, we propose a convex optimization framework to design the stress
matrix for AFC without predefining a rigid graph. We aim to find a resulting
network with a reduced number of communication links, but still with a fast
convergence speed. We show through simulations that our proposed solutions can
yield a more sparse graph, while admitting a faster convergence compared to the
state-of-the-art solutions.

</details>


### [205] [A Learning-based Hybrid System Approach for Detecting Contingencies in Distribution Grids with Inverter-Based Resources](https://arxiv.org/abs/2508.18500)
*Hamid Varmazyari,Masoud H. Nazari*

Main category: eess.SY

TL;DR: 本文提出了一种基于机器学习的随机混合系统（SHS）建模框架，用于检测含有逆变器基资源（IBRs）的主动配电网中的突发事件，包括传统传感系统无法识别的不可观测事件。


<details>
  <summary>Details</summary>
Motivation: 传统传感系统难以识别主动配电网中由逆变器基资源（IBRs）引起的突发事件，尤其是那些不可观测的事件，因此需要一种新的框架来有效检测和分类这些事件。

Method: 首先，引入一个结合了传统和基于IBRs资源的SHS状态空间模型，以描述配电网连续状态与离散突发事件之间的动态交互。该模型形成一个随机切换系统，能够处理由突发事件引起的参数或网络拓扑变化。突发事件分为两类：物理事件（如线路故障）和传感器故障引起的测量异常。然后，利用系统状态和网络输出的高频采样多变量时间序列数据，训练一个基于时间序列的学习模型，用于实时突发事件检测和分类。

Result: 在IEEE 33节点配电系统上的仿真研究表明，该框架的总体检测准确率达到96%。

Conclusion: 该机器学习SHS建模框架能够有效地检测和分类主动配电网中的突发事件，包括传统方法难以识别的不可观测事件，并表现出高检测准确率。

Abstract: This paper presents a machine-learning based Stochastic Hybrid System (SHS)
modeling framework to detect contingencies in active distribution networks
populated with inverter-based resources (IBRs). In particular, this framework
allows detecting unobservable contingencies, which cannot be identified by
normal sensing systems. First, a state-space SHS model combining conventional
and IRB-based resources is introduced to formulate the dynamic interaction
between continuous states of distribution networks and discrete contingency
events. This model forms a randomly switching system, where parameters or
network topology can change due to contingencies. We consider two contingency
classes: (i) physical events, such as line outages, and (ii) measurement
anomalies caused by sensor faults. Leveraging multivariate time series data
derived from high-frequency sampling of system states and network outputs, a
time series-based learning model is trained for real-time contingency detection
and classification. Simulation studies, carried out on the IEEE 33-bus
distribution system, demonstrate a 96% overall detection accuracy.

</details>


### [206] [Electromagnetic Formation Flying Using Alternating Magnetic Field Forces and Control Barrier Functions for State and Input Constraints](https://arxiv.org/abs/2508.18501)
*Sumit S. Kamat,T. Michael Seigler,Jesse B. Hoagg*

Main category: eess.SY

TL;DR: 本文提出了一种带状态和输入约束的电磁编队飞行反馈控制算法，通过交变磁场解耦力，利用优化控制和松弛控制障碍函数来满足各种约束。


<details>
  <summary>Details</summary>
Motivation: 研究动机是为电磁编队飞行开发一种反馈控制算法，该算法必须能有效处理卫星状态（如无碰撞、星间速度上限）和控制输入（如视在功率限制）的实际约束。

Method: 该算法结合了多种技术：1. 使用交变磁场力解耦卫星间的电磁力。2. 每个卫星的电磁驱动系统由幅度调制的正弦波驱动，通过控制幅度来设定星间平均力。3. 利用优化控制计算所需的平均力，以满足状态约束（无碰撞、星间速度上限）和输入约束（功率能力）。4. 优化平均力的计算通过一个单一的松弛控制障碍函数（Relaxed Control Barrier Function, RCBF）实现，该函数由多个为强制执行各状态和输入约束而设计的控制障碍函数（Control Barrier Function, CBF）组合而成。

Result: 研究结果通过数值模拟展示了所提出的卫星编队控制方法的可行性和有效性。

Conclusion: 该文成功提出了一种考虑状态和输入约束的电磁编队飞行反馈控制算法，并在数值模拟中验证了其性能。

Abstract: This article presents a feedback control algorithm for electromagnetic
formation flying with constraints on the satellites' states and control inputs.
The algorithm combines several key techniques. First, we use alternating
magnetic field forces to decouple the electromagnetic forces between each pair
of satellites in the formation. Each satellite's electromagnetic actuation
system is driven by a sum of amplitude-modulated sinusoids, where amplitudes
are controlled in order to prescribe the time-averaged force between each pair
of satellites. Next, the desired time-averaged force is computed from a optimal
control that satisfies state constraints (i.e., no collisions and an upper
limit on intersatellite speeds) and input constraints (i.e., not exceeding
satellite's apparent power capability). The optimal time-averaged force is
computed using a single relaxed control barrier function that is obtained by
composing multiple control barrier functions that are designed to enforce each
state and input constraint. Finally, we demonstrate the satellite formation
control method in numerical simulations.

</details>


### [207] [Fuzzy-Based Control Method for Autonomous Spacecraft Inspection with Minimal Fuel Consumption](https://arxiv.org/abs/2508.18583)
*Daegyun Choi,Donghoon Kim,Henzeh Leeghim*

Main category: eess.SY

TL;DR: 本研究提出了一种结合模糊推理系统和生物启发优化技术的节能航天器检查控制策略，旨在实现燃料消耗最小化并满足多种操作约束。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够高效、可靠地进行航天器检查，并显著降低燃料消耗的控制策略，同时应对光照、视场、推力限制和安全区域等实际约束。

Method: 采用模糊推理系统与生物启发优化技术相结合，为控制过程引入学习能力。通过蒙特卡洛模拟对所提出的控制策略的性能进行验证。

Result: 优化后的模糊控制器能够在保持可靠检查的前提下，产生最小燃料消耗的力，并有效处理光照、受限视场、推力限制和安全区域等约束。

Conclusion: 所提出的控制策略能够有效实现航天器检查的节能控制，同时确保操作的可靠性和安全性，即使在存在多种操作约束的情况下。

Abstract: This study explores an energy-efficient control strategy for spacecraft
inspection using a fuzzy inference system combined with a bio-inspired
optimization technique to incorporate learning capability into the control
process. The optimized fuzzy controller produces a minimally fuel-consuming
force while maintaining reliable inspection within constraints, such as
illumination, restricted field of view, thrust limits, and safe regions. The
performance of the proposed control strategy is validated through Monte Carlo
simulations.

</details>


### [208] [Scalable Fairness Shaping with LLM-Guided Multi-Agent Reinforcement Learning for Peer-to-Peer Electricity Markets](https://arxiv.org/abs/2508.18610)
*Shrenik Jadhav,Birva Sevak,Srijita Das,Akhtar Hussain,Wencong Su,Van-Hai Bui*

Main category: eess.SY

TL;DR: 本文提出了一个名为FairMarket-RL的公平感知多智能体强化学习框架，利用大型语言模型（LLM）批评器在点对点（P2P）能源交易中平衡经济效率与公平性，并在不确定性下提供实时指导。


<details>
  <summary>Details</summary>
Motivation: 随着屋顶光伏和家庭能源管理系统普及，P2P能源交易日益重要，但现有市场和强化学习设计多强调效率或私有利润，缺乏在不确定性下确保公平结果的实时指导。

Method: FairMarket-RL框架采用多智能体强化学习，其中一个LLM批评器在部分可观测和离散价格-数量动作的连续双边拍卖中塑造竞价策略。LLM在每个交易时段返回标准化公平分数（FTG、FBS、FPP），这些分数通过斜坡系数和可调缩放集成到奖励中，使公平指导与经济激励互补。环境模拟真实的住宅负荷和光伏曲线，并强制执行价格、物理可行性和策略更新稳定性方面的硬约束。

Result: 该框架将交易转向本地P2P，相对于仅依赖电网采购降低了消费者成本，在参与者之间保持了强大的公平性，并维护了公用事业的生存能力。对太阳能可用性和总需求的敏感性分析进一步表明了其鲁棒性能。

Conclusion: 研究结果表明，这提供了一个可扩展的、由LLM引导的去中心化电力市场途径，该市场在经济上高效、社会上公平且技术上健全。

Abstract: Peer-to-peer (P2P) energy trading is becoming central to modern distribution
systems as rooftop PV and home energy management systems become pervasive, yet
most existing market and reinforcement learning designs emphasize efficiency or
private profit and offer little real-time guidance to ensure equitable outcomes
under uncertainty. To address this gap, a fairness-aware multiagent
reinforcement learning framework, FairMarket-RL, is proposed in which a large
language model (LLM) critic shapes bidding policies within a continuous double
auction under partial observability and discrete price-quantity actions. After
each trading slot, the LLM returns normalized fairness scores Fairness-to-Grid
(FTG), Fairness-Between-Sellers (FBS), and Fairness-of-Pricing (FPP) that are
integrated into the reward via ramped coefficients and tunable scaling, so that
fairness guidance complements, rather than overwhelms, economic incentives. The
environment models realistic residential load and PV profiles and enforce hard
constraints on prices, physical feasibility, and policy-update stability.
Across a progression of experiments from a small pilot to a larger simulated
community and a mixed-asset real-world dataset, the framework shifts exchanges
toward local P2P trades, lowers consumer costs relative to grid-only
procurement, sustains strong fairness across participants, and preserves
utility viability. Sensitivity analyses over solar availability and aggregate
demand further indicate robust performance, suggesting a scalable, LLM-guided
pathway to decentralized electricity markets that are economically efficient,
socially equitable, and technically sound.

</details>


### [209] [Globally Stable Discrete Time PID Passivity-based Control of Power Converters: Simulation and Experimental Results](https://arxiv.org/abs/2508.18719)
*Alessio Moreschini,Wei He,Romeo Ortega,Yiheng Lu,Tao Li*

Main category: eess.SY

TL;DR: 本文提出了一种离散时间PID无源性控制（PID-PBC）方法，通过确保PID控制器和系统输出的无源性，解决了连续时间PID-PBC在离散化后无源性丧失的问题，并证明了其全局稳定性，特别适用于电力变换器。


<details>
  <summary>Details</summary>
Motivation: 连续时间PID无源性控制（PID-PBC）利用PID的无源性实现闭环系统全局稳定性。然而，实际应用中PID多为离散时间实现，且离散化过程会破坏无源性，即使采样时间很小，导致PID-PBC的实用性受限。

Method: 1. 提出了一种确保PID控制器无源性的离散化方法。2. 构建了一个新的输出，以确保离散化系统的无源性。3. 采用隐式中点离散化方法（一种保留系统不变量的辛积分技术）。4. 关注增量模型的无源性（即移位无源性），因为电力变换器输出参考值非零。5. 通过理论分析、仿真和实验验证了所提方法的性能。

Result: 1. 证明了所得的离散时间PID-PBC为增量模型定义了一个无源映射。2. 为离散化电力变换器模型建立了移位无源性。3. 结合这些特性，证明了电力变换器与离散化PID-PBC反馈互连的全局稳定性。4. 仿真和实验结果验证了所提出离散化方法的性能。

Conclusion: 通过提出一种新的离散化PID控制器和构建新的系统输出，并利用隐式中点离散化方法，本文成功解决了离散时间PID-PBC中的无源性保持问题。对于电力变换器模型，证明了其移位无源性和全局稳定性，从而为PID-PBC在实际离散时间系统中的安全应用提供了建设性解决方案。

Abstract: The key idea behind PID Passivity-based Control (PID-PBC) is to leverage the
passivity property of PIDs (for all positive gains) and wrap the PID controller
around a passive output to ensure global stability in closed-loop. However, the
practical applicability of PID-PBC is stymied by two key facts: (i) the vast
majority of practical implementations of PIDs is carried-out in discrete time
-- discretizing the continuous time dynamical system of the PID; (ii) the
well-known problem that passivity is not preserved upon discretization, even
with small sampling times. Therefore, two aspects of the PID-PBC must be
revisited for its safe practical application. First, we propose a
discretization of the PID that ensures its passivity. Second, since the output
that is identified as passive for the continuous time system is not necessarily
passive for its discrete time version, we construct a new output that ensures
the passivity property for the discretization of the system. In this paper, we
provide a constructive answer to both issues for the case of power converter
models. Instrumental to achieve this objective is the use of the implicit
midpoint discretization method -- which is a symplectic integration technique
that preserves system invariants. Since the reference value for the output to
be regulated in power converters is non-zero, we are henceforth interested in
the property of passivity of the incremental model -- currently known as
shifted passivity. Therefore, we demonstrate that the resulting discrete-time
PID-PBC defines a passive map for the incremental model and establish shifted
passivity for the discretized power converter model. Combining these
properties, we prove global stability for the feedback interconnection of the
power converter with the discretized PID-PBC. The paper also presents
simulations and experiments that demonstrate the performance of the proposed
discretization.

</details>


### [210] [Closed-Form Input Design for Identification under Output Feedback with Perturbation Constraints](https://arxiv.org/abs/2508.18813)
*Jingwei Hu,Dave Zachariah,Torbjörn Wigren,Petre Stoica*

Main category: eess.SY

TL;DR: 本文提出了一种在线设计信息丰富实验的方法，用于在固定输出反馈控制器下识别ARMAX模型，通过对输入信号施加有界扰动，并限制输出扰动在用户指定范围内，该方法可以高效地闭式计算。


<details>
  <summary>Details</summary>
Motivation: 在许多应用中，系统辨识实验必须在输出反馈下进行，以确保系统安全或维持系统运行，这促使研究如何在这些约束下设计有效的实验。

Method: 该方法通过对由固定输出反馈控制器生成的输入信号施加有界扰动来设计实验。设计过程将由此产生的输出扰动限制在用户指定的范围内，并且可以高效地以闭式形式计算。

Result: 通过两个数值实验，作者展示了所提出方法的有效性。

Conclusion: 该方法提供了一种有效且可计算的在线实验设计方案，用于在输出反馈下识别ARMAX模型，同时确保系统输出扰动在安全范围内。

Abstract: In many applications, system identification experiments must be performed
under output feedback to ensure safety or to maintain system operation. In this
paper, we consider the online design of informative experiments for ARMAX
models by applying a bounded perturbation to the input signal generated by a
fixed output feedback controller. Specifically, the design constrains the
resulting output perturbation within user-specified limits and can be
efficiently computed in closed form. We demonstrate the effectiveness of the
method in two numerical experiments.

</details>


### [211] [Performance Analysis of Underwater Optical Wireless Communication Using O-RIS and Fiber Optic Backhaul (Extended version)](https://arxiv.org/abs/2508.18915)
*Aboozar Heydaribeni,Hamzeh Beyranvand*

Main category: eess.SY

TL;DR: 本文提出了一种新型混合水下无线光通信（UWOC）系统，结合了水下光接入点（UOAPs）和基于PON的光纤回程，并通过硬切换机制在直接链路和光可重构智能表面（O-RIS）辅助链路之间切换，以确保可靠连接。


<details>
  <summary>Details</summary>
Motivation: 旨在为水下通信提供一个有弹性的骨干网络，并确保水下无线光通信系统的可靠连接。

Method: 该系统将UOAPs与基于PON的光纤回程集成，采用硬切换机制在直接链路和O-RIS辅助链路之间切换。研究在主动和多个被动O-RIS配置下进行评估，并应用了选择合并（SC）和最大比率合并（MRC）方案来增强可靠性。

Result: 分析和仿真结果表明，最优的O-RIS放置显著提升了系统性能。然而，在线性状态下，将其放置离接收器过近会导致性能下降，原因是在相同水域类型中路径损耗和光束抖动增加。此外，在实际限制内增加O-RIS元件数量可以进一步改善整体系统性能并增强对水下信道变化的适应性。

Conclusion: 所提出的混合UWOC系统通过结合O-RIS和合并方案，能够实现可靠且适应性强的通信。O-RIS的放置位置和元件数量是影响系统性能和适应性的关键因素。

Abstract: This Letter presents a novel hybrid underwater wireless optical communication
(UWOC) system that integrates underwater optical access points (UOAPs) with a
passive optical network (PON)-based fiber-optic backhaul to provide a resilient
backbone. A hard switching mechanism is employed between direct and optical
reconfigurable intelligent surface (O-RIS)-assisted links to ensure reliable
connectivity. Unlike previous studies, the proposed system is evaluated under
both active and multiple passive O-RIS configurations. To enhance reliability,
the Selection Combining (SC) and Maximal Ratio Combining (MRC) schemes are
applied. Analytical and simulation results demonstrate that optimal O-RIS
placement significantly enhances system performance. However, in the linear
regime, placing it too close to the receiver causes degradation due to
increased path loss and beam jitter in an identical water type. Moreover,
increasing the number of O-RIS elements within practical limits further
improves overall system performance and enhances adaptability to variations in
the underwater channel.

</details>


### [212] [A Principled Framework to Evaluate Quality of AC-OPF Datasets for Machine Learning: Benchmarking a Novel, Scalable Generation Method](https://arxiv.org/abs/2508.19083)
*Matteo Baù,Luca Perbellini,Samuele Grillo*

Main category: eess.SY

TL;DR: 本文提出了一种可扩展的AC-OPF数据集生成启发式方法，并通过多准则框架评估了数据集质量，结果表明其在质量和可扩展性之间取得了最佳平衡。


<details>
  <summary>Details</summary>
Motivation: 现有用于机器学习模型的AC-OPF数据集生成方法在大型电力系统上的可扩展性不足，且缺乏广泛接受的量化AC-OPF数据集质量的指标，导致不同生成方法难以比较。

Method: 1. 数据生成：提出了一种简单的启发式方法，在总负载有功功率中均匀采样负载设定点，并通过包含负载松弛变量的AC-OPF公式来提高收敛性。2. 质量评估：建立了一个基于三个指标的多准则框架，分别衡量AC-OPF原始变量边际分布的变异性、AC-OPF实例间约束激活模式的多样性以及变量边界的激活频率。

Result: 通过比较四种开源方法，本文提出的启发式方法在数据集质量和可扩展性之间取得了最佳平衡，并始终优于独立或受限于凸多面体的均匀随机采样方法。

Conclusion: 本文提出的启发式方法在AC-OPF数据集生成方面表现出色，尤其在平衡数据集质量与可扩展性方面优于其他现有方法，并为AC-OPF数据集质量评估提供了有效的多准则框架。

Abstract: Several methods have been proposed in the literature to improve the quality
of AC optimal power flow (AC-OPF) datasets used in machine learning (ML)
models. Yet, scalability to large power systems remains unaddressed and
comparing generation approaches is still hindered by the absence of widely
accepted metrics quantifying AC-OPF dataset quality. In this work, we tackle
both these limitations. We provide a simple heuristic that samples load
setpoints uniformly in total load active power, rather than maximizing volume
coverage, and solves an AC-OPF formulation with load slack variables to improve
convergence. For quality assessment, we formulate a multi-criteria framework
based on three metrics, measuring variability in the marginal distributions of
AC-OPF primal variables, diversity in constraint activation patterns among
AC-OPF instances and activation frequency of variable bounds. By comparing four
open-source methods based on these metrics, we show that our heuristic
consistently outperforms uniform random sampling, whether independent or
constrained to a convex polytope, scoring as best in terms of balance between
dataset quality and scalability.

</details>


### [213] [Learning Interior Point Method for AC and DC Optimal Power Flow](https://arxiv.org/abs/2508.19146)
*Farshad Amani,Amin Kargarian,Ramachandran Vaidyanathan*

Main category: eess.SY

TL;DR: 本文提出了一种名为L-IPM的可行性保证学习内点法，用于解决交流和直流最优潮流(OPF)问题。该方法结合LSTM预测内点法中心路径，并在早期稳定迭代后进行投影，显著减少了计算时间和迭代次数，同时保证了解决方案的准确性和可行性。


<details>
  <summary>Details</summary>
Motivation: 传统内点法(IPM)在每次迭代中需要求解大型线性系统，随着迭代次数增加，矩阵条件数恶化，计算成本急剧上升。为了解决这一问题，研究旨在开发一种更高效、更鲁棒的OPF求解方法。

Method: L-IPM采用混合学习模型。它将IPM轨迹建模为时间序列，并使用长短期记忆(LSTM)网络，仅利用前几个稳定迭代（包含最有信息量的特征）来预测IPM中心路径。引入了网格信息方法来强制执行发电机、电压幅值和线路潮流的操作约束，以确保可行性。LSTM作为IPM中心路径投影工具，随后进行最终的IPM精炼步骤。通过采样方法生成广泛的负荷场景以提高泛化能力。

Result: 在2869母线欧洲高压输电系统上的仿真结果表明，L-IPM将求解时间最多减少94%，并将所需迭代次数最多减少85.5%，同时保持了解决方案的准确性和可行性。通过利用早期迭代并绕过传统IPM后期条件恶劣且计算量大的步骤，显著提升了效率。

Conclusion: L-IPM在计算效率和鲁棒性方面均优于传统IPM，同时保证了解决方案的可行性。它通过结合学习模型和精炼步骤，有效解决了传统内点法在后期迭代中的计算瓶颈。

Abstract: This paper proposes a feasibility-guaranteed learning interior point method
(L-IPM) to solve both AC and DC optimal power flow (OPF) problems. Given the
criticality of OPF, the proposed L-IPM uses a hybrid learning model approach
rather than relying solely on a simple black-box prediction. The traditional
IPM follows a central path from an initial point to the optimal solution.
However, each iteration involves solving large linear systems, which becomes
increasingly expensive as the matrices grow more ill-conditioned in later
steps. To address this, we model the IPM trajectory as a time series and train
a Long Short-Term Memory (LSTM) network to project the IPM central path using
only the first few stable iterations, which carry the most informative features
about the path to optimality. We introduce a grid-informed methodology that
enforces operational constraints on generation, voltage magnitudes, and line
flows to ensure feasibility. The grid-informed LSTM serves as a tool for the
IPM central path projection and, followed by a final IPM refinement step,
significantly reduces the total number of iterations and time required for
convergence. We use a sampling method to generate a wide range of load
scenarios to improve generalization across diverse operating conditions,
efficiently covering the power system's operational space. Simulation results
on a 2869-bus European high-voltage transmission system show that the proposed
L-IPM significantly reduces solution time by up to 94\%, while maintaining
accuracy and feasibility of the solution. By leveraging early iterations and
bypassing the final ill-conditioned and computationally demanding steps of
traditional IPM, the proposed L-IPM reduces the number of required iterations
by up to 85.5\%. Since solution feasibility is also guaranteed, L-IPM
outperforms the conventional IPM in both computational efficiency and
robustness.

</details>


### [214] [Safe Navigation under State Uncertainty: Online Adaptation for Robust Control Barrier Functions](https://arxiv.org/abs/2508.19159)
*Ersin Das,Rahal Nanayakkara,Xiao Tan,Ryan M. Bena,Joel W. Burdick,Paulo Tabuada,Aaron D. Ames*

Main category: eess.SY

TL;DR: 本文提出了一种改进鲁棒控制障碍函数（R-CBF）的方法，通过在线参数自适应和统一约束来减少保守性，并在履带式车辆避障实验中验证了其性能提升。


<details>
  <summary>Details</summary>
Motivation: 在控制实践中，测量和状态估计通常不完善，这给依赖精确状态信息的安全关键应用带来了挑战。现有的鲁棒控制障碍函数（R-CBF）方法在存在估计误差时过于保守，可能导致不可行、控制力过大等问题。

Method: 本文提出了一种系统性的R-CBF改进方法，包括：1) 一种基于优化的在线参数自适应方案，用于减少现有R-CBF的保守性。2) 通过泊松方程将多个安全约束合并为一个统一的数值CBF，以降低参数优化的复杂性。3) 解决了车辆跟踪中常见的双相对度问题。

Result: 实验结果表明，与现有方法相比，本文提出的方法在整体性能上有所提升，并在履带式车辆在多个障碍物之间导航的场景中展示了其优势。

Conclusion: 本文提出了一种系统性的方法来改进鲁棒控制障碍函数，通过在线参数自适应和统一约束显著降低了保守性，并有效解决了车辆跟踪中的相关问题，从而在安全关键应用中提供了更好的性能。

Abstract: Measurements and state estimates are often imperfect in control practice,
posing challenges for safety-critical applications, where safety guarantees
rely on accurate state information. In the presence of estimation errors,
several prior robust control barrier function (R-CBF) formulations have imposed
strict conditions on the input. These methods can be overly conservative and
can introduce issues such as infeasibility, high control effort, etc. This work
proposes a systematic method to improve R-CBFs, and demonstrates its advantages
on a tracked vehicle that navigates among multiple obstacles. A primary
contribution is a new optimization-based online parameter adaptation scheme
that reduces the conservativeness of existing R-CBFs. In order to reduce the
complexity of the parameter optimization, we merge several safety constraints
into one unified numerical CBF via Poisson's equation. We further address the
dual relative degree issue that typically causes difficulty in vehicle
tracking. Experimental trials demonstrate the overall performance improvement
of our approach over existing formulations.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [215] [Stress-testing cross-cancer generalizability of 3D nnU-Net for PET-CT tumor segmentation: multi-cohort evaluation with novel oesophageal and lung cancer datasets](https://arxiv.org/abs/2508.18612)
*Soumen Ghosh,Christine Jestin Hannan,Rajat Vashistha,Parveen Kundu,Sandra Brosda,Lauren G. Aoude,James Lonie,Andrew Nathanson,Jessica Ng,Andrew P. Barbour,Viktor Vegh*

Main category: eess.IV

TL;DR: 本研究通过跨癌种评估，发现数据集多样性（多人口、多中心、多癌种）是实现PET-CT肿瘤分割模型临床鲁棒泛化的关键，而非模型架构的创新。


<details>
  <summary>Details</summary>
Motivation: 在临床PET-CT工作流程中，解剖部位、扫描仪和患者群体差异很大，因此深度学习肿瘤分割模型需要具备鲁棒的泛化能力。本研究旨在首次对PET-CT上的nnU-Net进行跨癌种评估。

Method: 研究引入了两个新的、由专家标注的全身体数据集（279名食管癌患者，54名肺癌患者），并结合公共AutoPET数据集。在三种范式下训练和测试了3D nnUNet模型：仅目标数据集训练（食管癌）、仅公共数据集训练（AutoPET）、以及联合训练。

Result: 仅目标数据集训练的模型在域内表现最佳（平均DSC 57.8），但在外部肺癌队列上表现极差（平均DSC低于3.4），表明严重过拟合。仅公共数据集训练的模型泛化能力更广（AutoPET上平均DSC 63.5，印度肺癌队列上51.6），但在食管癌队列上表现不佳（平均DSC 26.7）。联合训练方法在所有队列中提供了最平衡的结果（肺癌平均DSC 52.9，食管癌40.7，AutoPET 60.9），减少了边界误差并提高了鲁棒性。

Conclusion: 研究结果表明，数据集多样性，特别是多人口统计学、多中心和多癌种的整合，是实现鲁棒泛化的关键驱动因素，其重要性超过了架构创新。数据集多样性，而非模型复杂性，是实现临床鲁棒分割的基础。

Abstract: Robust generalization is essential for deploying deep learning based tumor
segmentation in clinical PET-CT workflows, where anatomical sites, scanners,
and patient populations vary widely. This study presents the first cross cancer
evaluation of nnU-Net on PET-CT, introducing two novel, expert-annotated
whole-body datasets. 279 patients with oesophageal cancer (Australian cohort)
and 54 with lung cancer (Indian cohort). These cohorts complement the public
AutoPET dataset and enable systematic stress-testing of cross domain
performance. We trained and tested 3D nnUNet models under three paradigms.
Target only (oesophageal), public only (AutoPET), and combined training. For
the tested sets, the oesophageal only model achieved the best in-domain
accuracy (mean DSC, 57.8) but failed on external Indian lung cohort (mean DSC
less than 3.4), indicating severe overfitting. The public only model
generalized more broadly (mean DSC, 63.5 on AutoPET, 51.6 on Indian lung
cohort) but underperformed in oesophageal Australian cohort (mean DSC, 26.7).
The combined approach provided the most balanced results (mean DSC, lung
(52.9), oesophageal (40.7), AutoPET (60.9)), reducing boundary errors and
improving robustness across all cohorts. These findings demonstrate that
dataset diversity, particularly multi demographic, multi center and multi
cancer integration, outweighs architectural novelty as the key driver of robust
generalization. This work presents the demography based cross cancer deep
learning segmentation evaluation and highlights dataset diversity, rather than
model complexity, as the foundation for clinically robust segmentation.

</details>


### [216] [ModAn-MulSupCon: Modality-and Anatomy-Aware Multi-Label Supervised Contrastive Pretraining for Medical Imaging](https://arxiv.org/abs/2508.18613)
*Eichi Takaya,Ryusei Inamori*

Main category: eess.IV

TL;DR: ModAn-MulSupCon是一种利用模态和解剖学元数据进行多标签监督对比预训练的方法，在医疗影像下游任务中，通过微调显著提高了性能，尤其适用于标签稀缺的临床环境。


<details>
  <summary>Details</summary>
Motivation: 医疗影像领域中，专家标注限制了大规模监督预训练，而普遍存在的元数据（如模态、解剖区域）却未被充分利用。

Method: 将每张图像的模态和解剖学信息编码为多热向量。使用Jaccard加权多标签监督对比损失，在RadImageNet的一个小型子集（miniRIN，16,222张图像）上预训练ResNet-18编码器。通过在三个二分类任务（ACL撕裂、乳腺病变恶性、甲状腺结节恶性）上进行微调和线性探测来评估其性能。

Result: 通过微调，ModAn-MulSupCon在MRNet-ACL（0.964）和甲状腺（0.763）数据集上取得了最佳AUC，优于所有基线（p<0.05），在乳腺数据集上排名第二（0.926），略低于SimCLR（0.940，不显著）。在编码器冻结的情况下，SimCLR/ImageNet表现更优，表明ModAn-MulSupCon的表示在任务适应（微调）中获益最大，而非线性可分性。

Conclusion: 将易于获取的模态/解剖学元数据编码为多标签目标，提供了一种实用、可扩展的预训练信号。在可行微调的情况下，该方法能提高下游任务的准确性。ModAn-MulSupCon是标签稀缺临床环境的强大初始化方法，而SimCLR/ImageNet更适用于冻结编码器的部署。

Abstract: Background and objective: Expert annotations limit large-scale supervised
pretraining in medical imaging, while ubiquitous metadata (modality, anatomical
region) remain underused. We introduce ModAn-MulSupCon, a modality- and
anatomy-aware multi-label supervised contrastive pretraining method that
leverages such metadata to learn transferable representations.
  Method: Each image's modality and anatomy are encoded as a multi-hot vector.
A ResNet-18 encoder is pretrained on a mini subset of RadImageNet (miniRIN,
16,222 images) with a Jaccard-weighted multi-label supervised contrastive loss,
and then evaluated by fine-tuning and linear probing on three binary
classification tasks--ACL tear (knee MRI), lesion malignancy (breast
ultrasound), and nodule malignancy (thyroid ultrasound).
  Result: With fine-tuning, ModAn-MulSupCon achieved the best AUC on MRNet-ACL
(0.964) and Thyroid (0.763), surpassing all baselines ($p<0.05$), and ranked
second on Breast (0.926) behind SimCLR (0.940; not significant). With the
encoder frozen, SimCLR/ImageNet were superior, indicating that ModAn-MulSupCon
representations benefit most from task adaptation rather than linear
separability.
  Conclusion: Encoding readily available modality/anatomy metadata as
multi-label targets provides a practical, scalable pretraining signal that
improves downstream accuracy when fine-tuning is feasible. ModAn-MulSupCon is a
strong initialization for label-scarce clinical settings, whereas
SimCLR/ImageNet remain preferable for frozen-encoder deployments.

</details>


### [217] [HOTSPOT-YOLO: A Lightweight Deep Learning Attention-Driven Model for Detecting Thermal Anomalies in Drone-Based Solar Photovoltaic Inspections](https://arxiv.org/abs/2508.18912)
*Mahmoud Dhimish*

Main category: eess.IV

TL;DR: 本研究开发并命名了HOTSPOT-YOLO，一个轻量级AI模型，通过集成高效卷积神经网络骨干和注意力机制，显著提高了无人机光伏系统热异常（如热点和缺陷模块）的实时检测精度和效率。


<details>
  <summary>Details</summary>
Motivation: 确保太阳能光伏系统（PV）的运行效率和降低维护成本，以及解决无人机热成像检查中检测小型、细微热异常的独特挑战，同时保持实时性能。

Method: 开发了HOTSPOT-YOLO，这是一个轻量级人工智能模型，它集成了高效的卷积神经网络骨干和注意力机制以改进目标检测。该模型专门为无人机光伏系统热成像检查而设计。

Result: 实验结果显示，平均精度（mAP）达到90.8%，相比基线目标检测模型有显著提升。该模型计算负载更低，并在不同环境条件下表现出鲁棒性。

Conclusion: HOTSPOT-YOLO为大规模光伏检查提供了一个可扩展且可靠的解决方案。这项工作突出了先进AI技术与实际工程应用的结合，革新了可再生能源系统中的自动化故障检测。

Abstract: Thermal anomaly detection in solar photovoltaic (PV) systems is essential for
ensuring operational efficiency and reducing maintenance costs. In this study,
we developed and named HOTSPOT-YOLO, a lightweight artificial intelligence (AI)
model that integrates an efficient convolutional neural network backbone and
attention mechanisms to improve object detection. This model is specifically
designed for drone-based thermal inspections of PV systems, addressing the
unique challenges of detecting small and subtle thermal anomalies, such as
hotspots and defective modules, while maintaining real-time performance.
Experimental results demonstrate a mean average precision of 90.8%, reflecting
a significant improvement over baseline object detection models. With a reduced
computational load and robustness under diverse environmental conditions,
HOTSPOT-YOLO offers a scalable and reliable solution for large-scale PV
inspections. This work highlights the integration of advanced AI techniques
with practical engineering applications, revolutionizing automated fault
detection in renewable energy systems.

</details>


### [218] [Lossless 4:2:0 Screen Content Coding Using Luma-Guided Soft Context Formation](https://arxiv.org/abs/2508.18968)
*Hannah Och,André Kaup*

Main category: eess.IV

TL;DR: 本文扩展了软上下文形成编码器（一种先进的无损屏幕内容编码器）以支持YUV 4:2:0格式，并提出了改进的色度预测和概率建模方法，在屏幕内容图像上优于HEVC-SCC。


<details>
  <summary>Details</summary>
Motivation: 软上下文形成编码器（Soft Context Formation Coder）在RGB 4:4:4格式的屏幕内容图像上表现出色，但它一次性编码整个像素（所有颜色分量），因此不原生支持如YCbCr 4:2:0等色度欠采样图像格式，而4:2:0是视频压缩中常用的色度格式。此外，现有的色度处理可能还有提升空间。

Method: 本文通过以下方式扩展和改进了编码器：1. 基于图像平面间归一化互信息分析，对Y和CbCr平面进行逐次编码。2. 提出了一种基于亮度平面增强色度预测的方法。3. 建议传输关于出现的亮度-色度组合的辅助信息，以改善色度概率分布建模。

Result: 在一大型屏幕内容图像数据集上，本文提出的方法优于HEVC-SCC，HEVC-SCC需要比本文方法多5.66%的比特率。

Conclusion: 本文成功地将软上下文形成编码器扩展到支持4:2:0图像压缩，并通过改进的色度预测和概率建模技术，显著提升了压缩性能，在屏幕内容图像上超越了HEVC-SCC。

Abstract: The soft context formation coder is a pixel-wise state-of-the-art lossless
screen content coder using pattern matching and color palette coding in
combination with arithmetic coding. It achieves excellent compression
performance on screen content images in RGB 4:4:4 format with few distinct
colors. In contrast to many other lossless compression methods, it codes entire
color pixels at once, i.e., all color components of one pixel are coded
together. Consequently, it does not natively support image formats with
downsampled chroma, such as YCbCr 4:2:0, which is an often used chroma format
in video compression. In this paper, we extend the soft context formation
coding capabilities to 4:2:0 image compression, by successively coding Y and
CbCr planes based on an analysis of normalized mutual information between image
planes. Additionally, we propose an enhancement to the chroma prediction based
on the luminance plane. Furthermore, we propose to transmit side-information
about occurring luma-chroma combinations to improve chroma probability
distribution modelling. Averaged over a large screen content image dataset, our
proposed method outperforms HEVC-SCC, with HEVC-SCC needing 5.66% more bitrate
compared to our method.

</details>


### [219] [Random forest-based out-of-distribution detection for robust lung cancer segmentation](https://arxiv.org/abs/2508.19112)
*Aneesh Rangnekar,Harini Veeraraghavan*

Main category: eess.IV

TL;DR: 该研究提出RF-Deep，一个结合预训练Transformer深度特征的随机森林分类器，用于检测CT扫描中的OOD数据并提高癌症病灶分割的可靠性，特别是在OOD场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 基于Transformer的癌症病灶分割模型在分布内（ID）数据上表现良好，但在分布外（OOD）数据集上性能会下降。然而，准确的病灶检测和分割对于自动化治疗规划和治疗反应评估至关重要，因此需要解决OOD场景下的模型可靠性问题。

Method: 研究开发了RF-Deep，一个利用预训练分割模型（包含Swin Transformer编码器和卷积解码器）深度特征的随机森林分类器。Swin Transformer编码器在10,432个未标记的3D CT扫描（涵盖癌症和非癌症情况）上通过掩码图像建模（SimMIM）进行自监督预训练。分割模型在317个3D扫描上训练以分割肺癌。RF-Deep通过检测OOD扫描来增强分割可靠性。

Result: 在包含一个ID数据集和四个OOD数据集（肺栓塞CT、COVID-19 CT、肾癌腹部CT和健康志愿者腹部CT）的603个3D CT公共数据集上进行了独立测试。RF-Deep在肺栓塞、COVID-19和腹部CT上检测OOD病例的FPR95（95%召回率下的误报率）分别为18.26%、27.66%和小于0.1%，持续优于现有OOD方法。

Conclusion: RF-Deep分类器提供了一种简单而有效的方法，可以增强在ID和OOD场景下癌症分割的可靠性。

Abstract: Accurate detection and segmentation of cancerous lesions from computed
tomography (CT) scans is essential for automated treatment planning and cancer
treatment response assessment. Transformer-based models with self-supervised
pretraining can produce reliably accurate segmentation from in-distribution
(ID) data but degrade when applied to out-of-distribution (OOD) datasets. We
address this challenge with RF-Deep, a random forest classifier that utilizes
deep features from a pretrained transformer encoder of the segmentation model
to detect OOD scans and enhance segmentation reliability. The segmentation
model comprises a Swin Transformer encoder, pretrained with masked image
modeling (SimMIM) on 10,432 unlabeled 3D CT scans covering cancerous and
non-cancerous conditions, with a convolution decoder, trained to segment lung
cancers in 317 3D scans. Independent testing was performed on 603 3D CT public
datasets that included one ID dataset and four OOD datasets comprising chest
CTs with pulmonary embolism (PE) and COVID-19, and abdominal CTs with kidney
cancers and healthy volunteers. RF-Deep detected OOD cases with a FPR95 of
18.26%, 27.66%, and less than 0.1% on PE, COVID-19, and abdominal CTs,
consistently outperforming established OOD approaches. The RF-Deep classifier
provides a simple and effective approach to enhance reliability of cancer
segmentation in ID and OOD scenarios.

</details>


### [220] [Federative ischemic stroke segmentation as alternative to overcome domain-shift multi-institution challenges](https://arxiv.org/abs/2508.18296)
*Edgar Rangel,Fabio Martinez*

Main category: eess.IV

TL;DR: 本研究开发了一个协作框架（基于联邦学习），通过共享深度中心无关表示的知识，用于分割DWI序列中的缺血性卒中病灶，有效解决了数据变异性和标注稀缺性问题，并展现出强大的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 卒中是全球第二大死因和第三大致残原因。DWI是评估梗死病灶的标准方法，但其分析因患者人群、扫描仪和专家标注的差异而高度可变。现有的计算支持方法通常是针对单一机构的专用解决方案，缺乏泛化能力，且许多临床中心缺乏足够的标注样本来调整这些方案。

Method: 本研究开发了一个协作框架，通过共享来自深度中心无关表示的知识来分割DWI序列中的缺血性卒中病灶。该框架在14个模拟医疗中心（包含2031项研究）上进行验证，并采用了FedAvg模型。

Result: FedAvg模型在所有中心取得了0.71 ± 0.24的平均DSC、5.29 ± 22.74的AVD、2.16 ± 3.60的ALD和0.70 ± 0.26的LF1，优于集中式和其他联邦学习规则。该模型还展现出强大的泛化特性，在不同病灶类别上表现一致，并在未进行额外训练的分布外中心中也表现可靠（DSC为0.64 ± 0.29，AVD为4.44 ± 8.74）。

Conclusion: 所开发的协作框架（特别是FedAvg模型）通过共享深度中心无关的知识，能够有效地解决缺血性卒中病灶分割中因数据变异性和稀缺性带来的挑战，并具有出色的跨中心和病灶类型泛化能力。

Abstract: Stroke is the second leading cause of death and the third leading cause of
disability worldwide. Clinical guidelines establish diffusion resonance imaging
(DWI, ADC) as the standard for localizing, characterizing, and measuring
infarct volume, enabling treatment support and prognosis. Nonetheless, such
lesion analysis is highly variable due to different patient demographics,
scanner vendors, and expert annotations. Computational support approaches have
been key to helping with the localization and segmentation of lesions. However,
these strategies are dedicated solutions that learn patterns from only one
institution, lacking the variability to generalize geometrical lesions shape
models. Even worse, many clinical centers lack sufficient labeled samples to
adjust these dedicated solutions. This work developed a collaborative framework
for segmenting ischemic stroke lesions in DWI sequences by sharing knowledge
from deep center-independent representations. From 14 emulated healthcare
centers with 2031 studies, the FedAvg model achieved a general DSC of $0.71 \pm
0.24$, AVD of $5.29 \pm 22.74$, ALD of $2.16 \pm 3.60$ and LF1 of $0.70 \pm
0.26$ over all centers, outperforming both the centralized and other federated
rules. Interestingly, the model demonstrated strong generalization properties,
showing uniform performance across different lesion categories and reliable
performance in out-of-distribution centers (with DSC of $0.64 \pm 0.29$ and AVD
of $4.44 \pm 8.74$ without any additional training).

</details>


### [221] [Analise de Desaprendizado de Maquina em Modelos de Classificacao de Imagens Medicas](https://arxiv.org/abs/2508.18509)
*Andreza M. C. Falcao,Filipe R. Cordeiro*

Main category: eess.IV

TL;DR: 本研究评估了SalUn机器遗忘模型在医学图像分类任务中的性能，发现其效果接近完全重新训练，为医学应用提供了一种高效的解决方案。


<details>
  <summary>Details</summary>
Motivation: 机器遗忘旨在从预训练模型中移除敏感数据同时保持模型鲁棒性，但该技术尚未在医学图像分类领域得到探索。

Method: 本研究通过在PathMNIST、OrganAMNIST和BloodMNIST数据集上进行实验，评估了SalUn遗忘模型。此外，还分析了数据增强对遗忘质量的影响。

Result: 实验结果表明，SalUn模型达到了接近完全重新训练的性能。

Conclusion: SalUn模型为医学图像分类中的机器遗忘提供了一种高效的解决方案，具有实际应用潜力。

Abstract: Machine unlearning aims to remove private or sensitive data from a
pre-trained model while preserving the model's robustness. Despite recent
advances, this technique has not been explored in medical image classification.
This work evaluates the SalUn unlearning model by conducting experiments on the
PathMNIST, OrganAMNIST, and BloodMNIST datasets. We also analyse the impact of
data augmentation on the quality of unlearning. Results show that SalUn
achieves performance close to full retraining, indicating an efficient solution
for use in medical applications.

</details>


### [222] [A Deep Learning Application for Psoriasis Detection](https://arxiv.org/abs/2508.18528)
*Anna Milani,Fábio S. da Silva,Elloá B. Guedes,Ricardo Rios*

Main category: eess.IV

TL;DR: 本文比较了ResNet50、Inception v3和VGG19三种CNN模型在银屑病皮损图像分类中的性能，发现Inception v3表现最佳。


<details>
  <summary>Details</summary>
Motivation: 研究旨在评估不同卷积神经网络模型在银屑病皮损图像分类中的表现，以期为银屑病的诊断提供辅助工具。

Method: 研究使用了ResNet50、Inception v3和VGG19三种卷积神经网络模型。训练和验证图像来自专业平台，并采用了调整评估指标的技术。

Result: Inception v3模型表现出令人满意的性能，其准确率和F1-Score达到97.5% ± 0.2。

Conclusion: Inception v3模型因其在准确率和F1-Score方面的出色表现，被认为是一种有价值的银屑病诊断辅助工具。

Abstract: In this paper a comparative study of the performance of three Convolutional
Neural Network models, ResNet50, Inception v3 and VGG19 for classification of
skin images with lesions affected by psoriasis is presented. The images used
for training and validation of the models were obtained from specialized
platforms. Some techniques were used to adjust the evaluation metrics of the
neural networks. The results found suggest the model Inception v3 as a valuable
tool for supporting the diagnosis of psoriasis. This is due to its satisfactory
performance with respect to accuracy and F1-Score (97.5% ${\pm}$ 0.2).

</details>


### [223] [A Closer Look at Edema Area Segmentation in SD-OCT Images Using Adversarial Framework](https://arxiv.org/abs/2508.18790)
*Yuhui Tao,Yizhe Zhang,Qiang Chen*

Main category: eess.IV

TL;DR: 本文提出了一种结合视网膜层结构引导的后处理和测试时适应（TTA）策略的方法，以提升弱监督黄斑水肿（ME）分割模型的性能，缩小其与全监督模型之间的差距。


<details>
  <summary>Details</summary>
Motivation: 分析黄斑水肿的AI模型通常依赖昂贵的像素级专家标注数据集。虽然基于异常检测的弱监督方法在水肿区域（EA）分割任务中显示出潜力，但其性能仍落后于全监督方法。

Method: 利用SD-OCT图像中EA与视网膜层之间的强相关性，并结合弱监督学习的更新特性，作者增强了一个对抗性框架。具体方法包括：1) 引入一种新颖的层结构引导的后处理步骤，将密集的EA预测重构为确认EA轮廓与视网膜层之间的交点；2) 采用测试时适应（TTA）策略，以解决训练集和测试集之间EA表现形式的差异。

Result: 在两个公开数据集上进行的广泛实验表明，所提出的两种方法（层结构引导后处理和TTA）能够提高EA分割的准确性和鲁棒性，从而缩小了弱监督模型与全监督模型之间的性能差距。

Conclusion: 结合视网膜层结构信息和测试时适应策略，可以显著提升弱监督黄斑水肿分割模型的性能和鲁棒性，使其更接近全监督模型的水平。

Abstract: The development of artificial intelligence models for macular edema (ME)
analy-sis always relies on expert-annotated pixel-level image datasets which
are expen-sive to collect prospectively. While anomaly-detection-based
weakly-supervised methods have shown promise in edema area (EA) segmentation
task, their per-formance still lags behind fully-supervised approaches. In this
paper, we leverage the strong correlation between EA and retinal layers in
spectral-domain optical coherence tomography (SD-OCT) images, along with the
update characteristics of weakly-supervised learning, to enhance an
off-the-shelf adversarial framework for EA segmentation with a novel
layer-structure-guided post-processing step and a test-time-adaptation (TTA)
strategy. By incorporating additional retinal lay-er information, our framework
reframes the dense EA prediction task as one of confirming intersection points
between the EA contour and retinal layers, result-ing in predictions that
better align with the shape prior of EA. Besides, the TTA framework further
helps address discrepancies in the manifestations and presen-tations of EA
between training and test sets. Extensive experiments on two pub-licly
available datasets demonstrate that these two proposed ingredients can im-prove
the accuracy and robustness of EA segmentation, bridging the gap between
weakly-supervised and fully-supervised models.

</details>


### [224] [Understanding Benefits and Pitfalls of Current Methods for the Segmentation of Undersampled MRI Data](https://arxiv.org/abs/2508.18975)
*Jan Nikolas Morshuis,Matthias Hein,Christian F. Baumgartner*

Main category: eess.IV

TL;DR: 本文首次对欠采样MRI数据进行分割方法的统一基准测试，比较了七种方法，发现简单的考虑数据一致性的两阶段方法表现最佳。


<details>
  <summary>Details</summary>
Motivation: MRI采集耗时，导致患者不适和成本增加。虽然加速MRI会影响图像质量，但对于分割等下游任务，完美的重建可能并非必需。现有直接在加速MRI数据上进行分割的方法缺乏统一比较、使用不同数据集且缺乏统一评估标准，导致最佳策略未知。

Method: 本文对7种分割欠采样MRI数据的方法进行了统一基准测试，重点比较了将重建和分割结合的“一阶段方法”与先进行MRI重建再进行分割的“两阶段方法”。研究使用了两个包含多线圈k空间数据和人工标注分割真值的MRI数据集。

Result: 研究发现，考虑数据一致性的简单两阶段方法取得了最佳分割分数，甚至超越了专门为此任务开发的复杂专业方法。

Conclusion: 对于加速MRI数据的分割任务，简单且考虑数据一致性的两阶段方法是目前最优策略。

Abstract: MR imaging is a valuable diagnostic tool allowing to non-invasively visualize
patient anatomy and pathology with high soft-tissue contrast. However, MRI
acquisition is typically time-consuming, leading to patient discomfort and
increased costs to the healthcare system. Recent years have seen substantial
research effort into the development of methods that allow for accelerated MRI
acquisition while still obtaining a reconstruction that appears similar to the
fully-sampled MR image. However, for many applications a perfectly
reconstructed MR image may not be necessary, particularly, when the primary
goal is a downstream task such as segmentation. This has led to growing
interest in methods that aim to perform segmentation directly on accelerated
MRI data. Despite recent advances, existing methods have largely been developed
in isolation, without direct comparison to one another, often using separate or
private datasets, and lacking unified evaluation standards. To date, no
high-quality, comprehensive comparison of these methods exists, and the optimal
strategy for segmenting accelerated MR data remains unknown. This paper
provides the first unified benchmark for the segmentation of undersampled MRI
data comparing 7 approaches. A particular focus is placed on comparing
\textit{one-stage approaches}, that combine reconstruction and segmentation
into a unified model, with \textit{two-stage approaches}, that utilize
established MRI reconstruction methods followed by a segmentation network. We
test these methods on two MRI datasets that include multi-coil k-space data as
well as a human-annotated segmentation ground-truth. We find that simple
two-stage methods that consider data-consistency lead to the best segmentation
scores, surpassing complex specialized methods that are developed specifically
for this task.

</details>


### [225] [RDDM: Practicing RAW Domain Diffusion Model for Real-world Image Restoration](https://arxiv.org/abs/2508.19154)
*Yan Chen,Yi Wen,Wei Li,Junchao Liu,Yong Guo,Jie Hu,Xinghao Chen*

Main category: eess.IV

TL;DR: 本文提出了RAW域扩散模型（RDDM），一个端到端模型，可以直接从传感器RAW数据恢复照片级真实图像，优于现有sRGB域方法。


<details>
  <summary>Details</summary>
Motivation: 现有的sRGB域扩散模型在处理有损sRGB输入时，面临高保真度与真实感生成之间的困境，且忽略了许多场景（如边缘设备图像/视频捕获）中传感器RAW数据的可访问性，导致性能不佳。

Method: RDDM通过直接在RAW域恢复图像，取代传统的两阶段ISP+IR管线。为解决RAW域适应中的分布外（OOD）问题，本文提出：(1) 一个RAW域VAE（RVAE）学习最优潜在表示；(2) 一个可微分的后处理模块（PTP），实现RAW和sRGB空间的联合优化。此外，为弥补数据集不足，开发了可扩展的降级管线，从现有sRGB数据集合成RAW LQ-HQ对进行大规模训练，并设计了可配置的多拜耳（CMB）LoRA模块以处理多样化的RAW模式。

Result: 广泛的实验证明，RDDM优于最先进的sRGB扩散方法，能产生更高保真度、更少伪影的结果。

Conclusion: RDDM成功地直接从RAW数据恢复图像，克服了sRGB模型的局限性，实现了卓越的图像质量和真实感，为图像恢复设定了新标准。

Abstract: We present the RAW domain diffusion model (RDDM), an end-to-end diffusion
model that restores photo-realistic images directly from the sensor RAW data.
While recent sRGB-domain diffusion methods achieve impressive results, they are
caught in a dilemma between high fidelity and realistic generation. As these
models process lossy sRGB inputs and neglect the accessibility of the sensor
RAW images in many scenarios, e.g., in image and video capturing in edge
devices, resulting in sub-optimal performance. RDDM bypasses this limitation by
directly restoring images in the RAW domain, replacing the conventional
two-stage image signal processing (ISP) + IR pipeline. However, a simple
adaptation of pre-trained diffusion models to the RAW domain confronts the
out-of-distribution (OOD) issues. To this end, we propose: (1) a RAW-domain VAE
(RVAE) learning optimal latent representations, (2) a differentiable Post Tone
Processing (PTP) module enabling joint RAW and sRGB space optimization. To
compensate for the deficiency in the dataset, we develop a scalable degradation
pipeline synthesizing RAW LQ-HQ pairs from existing sRGB datasets for
large-scale training. Furthermore, we devise a configurable multi-bayer (CMB)
LoRA module handling diverse RAW patterns such as RGGB, BGGR, etc. Extensive
experiments demonstrate RDDM's superiority over state-of-the-art sRGB diffusion
methods, yielding higher fidelity results with fewer artifacts.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [226] [EMind: A Foundation Model for Multi-task Electromagnetic Signals Understanding](https://arxiv.org/abs/2508.18785)
*Luqing Luo,Wenjin Gui,Yunfei Liu,Ziyue Zhang,Yunxi Zhang,Fengxiang Wang,Zonghao Guo,Zizhi Ma,Xinzhu Liu,Hanxiang He,Jinhai Li,Xin Qiu,Wupeng Xie,Yangang Sun*

Main category: eess.SP

TL;DR: 本文提出了EMind，一个电磁信号基础模型，通过构建统一的大规模数据集和创新的训练策略，解决了电磁信号理解的挑战，实现了跨任务的强大性能和广泛泛化能力。


<details>
  <summary>Details</summary>
Motivation: 电磁信号的深入理解对动态频谱管理、智能交通、自动驾驶等至关重要。然而，电磁信号与文本图像差异大，具有高异构性、强背景噪声和复杂时频结构，导致现有通用模型无法直接使用。此外，当前方法缺乏跨任务泛化和迁移效率，且高质量大规模数据集稀缺，阻碍了通用多任务学习框架的建立。

Method: 本文提出了EMind电磁信号基础模型。具体方法包括：1) 构建了首个统一且最大的标准化电磁信号数据集，涵盖多种信号类型和任务；2) 利用电磁信号的物理特性，设计了长度自适应多信号打包方法；3) 采用了硬件感知训练策略，以高效利用异构多源信号进行表示学习。

Result: 实验结果表明，EMind在许多下游任务中都取得了强大的性能和广泛的泛化能力。

Conclusion: EMind成功地将电磁信号分析从任务特定模型转向统一框架，为电磁智能领域的发展迈出了决定性一步。

Abstract: Deep understanding of electromagnetic signals is fundamental to dynamic
spectrum management, intelligent transportation, autonomous driving and
unmanned vehicle perception. The field faces challenges because electromagnetic
signals differ greatly from text and images, showing high heterogeneity, strong
background noise and complex joint time frequency structure, which prevents
existing general models from direct use. Electromagnetic communication and
sensing tasks are diverse, current methods lack cross task generalization and
transfer efficiency, and the scarcity of large high quality datasets blocks the
creation of a truly general multitask learning framework. To overcome these
issue, we introduce EMind, an electromagnetic signals foundation model that
bridges large scale pretraining and the unique nature of this modality. We
build the first unified and largest standardized electromagnetic signal dataset
covering multiple signal types and tasks. By exploiting the physical properties
of electromagnetic signals, we devise a length adaptive multi-signal packing
method and a hardware-aware training strategy that enable efficient use and
representation learning from heterogeneous multi-source signals. Experiments
show that EMind achieves strong performance and broad generalization across
many downstream tasks, moving decisively from task specific models to a unified
framework for electromagnetic intelligence. The code is available at:
https://github.com/GabrielleTse/EMind.

</details>
