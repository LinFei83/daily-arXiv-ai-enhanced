{"id": "2509.13336", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13336", "abs": "https://arxiv.org/abs/2509.13336", "authors": ["Mehran Behjati", "Rosdiadee Nordin", "Nor Fadzilah Abdullah"], "title": "Maximizing UAV Cellular Connectivity with Reinforcement Learning for BVLoS Path Planning", "comment": "Submitted to an IEEE Conference", "summary": "This paper presents a reinforcement learning (RL) based approach for path\nplanning of cellular connected unmanned aerial vehicles (UAVs) operating beyond\nvisual line of sight (BVLoS). The objective is to minimize travel distance\nwhile maximizing the quality of cellular link connectivity by considering real\nworld aerial coverage constraints and employing an empirical aerial channel\nmodel. The proposed solution employs RL techniques to train an agent, using the\nquality of communication links between the UAV and base stations (BSs) as the\nreward function. Simulation results demonstrate the effectiveness of the\nproposed method in training the agent and generating feasible UAV path plans.\nThe proposed approach addresses the challenges due to limitations in UAV\ncellular communications, highlighting the need for investigations and\nconsiderations in this area. The RL algorithm efficiently identifies optimal\npaths, ensuring maximum connectivity with ground BSs to ensure safe and\nreliable BVLoS flight operation. Moreover, the solution can be deployed as an\noffline path planning module that can be integrated into future ground control\nsystems (GCS) for UAV operations, enhancing their capabilities and safety. The\nmethod holds potential for complex long range UAV applications, advancing the\ntechnology in the field of cellular connected UAV path planning.", "AI": {"tldr": "本文提出了一种基于强化学习（RL）的方法，用于蜂窝网络连接的超视距（BVLoS）无人机路径规划，旨在最小化行程距离并最大化蜂窝链路连接质量。", "motivation": "由于无人机蜂窝通信的限制以及在超视距飞行中确保安全可靠运行的需求，研究人员需要一种能够同时优化飞行距离和通信连接质量的路径规划解决方案。", "method": "该方法采用强化学习技术训练一个智能体，将无人机与基站（BSs）之间的通信链路质量作为奖励函数，并考虑真实的空中覆盖约束和经验性空中信道模型。", "result": "仿真结果表明，该方法能有效训练智能体并生成可行的无人机路径规划，算法能高效识别最佳路径，确保与地面基站的最大连接，从而保障安全可靠的超视距飞行操作。", "conclusion": "所提出的基于强化学习的解决方案解决了无人机蜂窝通信的挑战，可以作为离线路径规划模块集成到未来的地面控制系统（GCS）中，增强无人机操作能力和安全性，对复杂的长距离无人机应用具有潜力。"}}
{"id": "2509.13342", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13342", "abs": "https://arxiv.org/abs/2509.13342", "authors": ["Isaac Ronald Ward"], "title": "Real World Robotic Exploration using Deep Neural Networks Trained in Photorealistic Reconstructed Environments", "comment": "This report is submitted as partial fulfilment of the requirements\n  for the Honours Programme of the Department of Computer Science and Software\n  Engineering, The University of Western Australia, 2019", "summary": "In this work, an existing deep neural network approach for determining a\nrobot's pose from visual information (RGB images) is modified, improving its\nlocalization performance without impacting its ease of training. Explicitly,\nthe network's loss function is extended in a manner which intuitively combines\nthe positional and rotational error in order to increase robustness to\nperceptual aliasing. An improvement in the localization accuracy for indoor\nscenes is observed: with decreases of up to 9.64% and 2.99% in the median\npositional and rotational error respectively, when compared to the unmodified\nnetwork.\n  Additionally, photogrammetry data is used to produce a pose-labelled dataset\nwhich allows the above model to be trained on a local environment, resulting in\nlocalization accuracies of 0.11m & 0.89 degrees. This trained model forms the\nbasis of a navigation algorithm, which is tested in real-time on a TurtleBot (a\nwheeled robotic device). As such, this work introduces a full pipeline for\ncreating a robust navigational algorithm for any given real world indoor scene;\nthe only requirement being a collection of images from the scene, which can be\ncaptured in as little as 330 seconds of", "AI": {"tldr": "本文改进了一种基于深度神经网络的机器人视觉位姿估计方法，通过修改损失函数提高了定位性能，并提出一个完整的室内场景机器人导航算法流程，实现了高精度实时导航。", "motivation": "研究动机是提高现有深度神经网络机器人视觉位姿估计方法的定位性能，同时不影响训练简易性，并增强对感知混叠的鲁棒性。", "method": "研究方法包括：1) 修改现有网络的损失函数，直观地结合位置和旋转误差，以提高对感知混叠的鲁棒性。2) 利用摄影测量数据生成带位姿标签的数据集，用于在本地环境中训练模型。3) 将训练好的模型作为导航算法的基础，并在TurtleBot上进行实时测试。", "result": "主要结果有：1) 相较于未修改的网络，改进后的网络在室内场景定位精度上有所提升，中位数位置误差和旋转误差分别降低了9.64%和2.99%。2) 在本地环境训练后，定位精度达到0.11米和0.89度。3) 提出一个完整的导航算法流程，仅需330秒的图像采集即可为任何室内场景创建鲁棒的导航算法。", "conclusion": "本文成功改进了基于深度神经网络的机器人视觉位姿估计方法，通过损失函数优化和本地环境数据训练，显著提高了定位精度和对感知混叠的鲁棒性。同时，提出并验证了一个高效、鲁棒的室内机器人导航算法完整流程。"}}
{"id": "2509.13349", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13349", "abs": "https://arxiv.org/abs/2509.13349", "authors": ["Jed Guzelkabaagac", "Boris Petrović"], "title": "Label-Efficient Grasp Joint Prediction with Point-JEPA", "comment": "4 pages, 5 figures. Submitted to IROS 2025 Workshop", "summary": "We investigate whether 3D self-supervised pretraining with a Joint-Embedding\nPredictive Architecture (Point-JEPA) enables label-efficient grasp joint-angle\nprediction. Using point clouds tokenized from meshes and a ShapeNet-pretrained\nPoint-JEPA encoder, we train a lightweight multi-hypothesis head with\nwinner-takes-all and evaluate by top-logit selection. On DLR-Hand II with\nobject-level splits, Point-JEPA reduces RMSE by up to 26% in low-label regimes\nand reaches parity with full supervision. These results suggest JEPA-style\npretraining is a practical approach for data-efficient grasp learning.", "AI": {"tldr": "本文研究了使用3D自监督预训练的联合嵌入预测架构（Point-JEPA）是否能实现标签高效的抓取关节角度预测，结果表明在低标签环境下显著降低了RMSE。", "motivation": "研究动机是探索一种标签高效的方法来预测抓取关节角度，以减少对大量标注数据的依赖。", "method": "研究方法是使用从网格中标记化的点云，通过一个经过ShapeNet预训练的Point-JEPA编码器进行3D自监督预训练。在此基础上，训练一个轻量级的多假设头部，并采用“赢者通吃”策略，通过最高logit选择进行评估。实验在DLR-Hand II数据集上进行，采用对象级分割。", "result": "主要结果显示，在低标签环境下，Point-JEPA将均方根误差（RMSE）降低了高达26%，并达到了与完全监督方法相当的性能。", "conclusion": "结论是JEPA风格的预训练是一种实现数据高效抓取学习的实用方法。"}}
{"id": "2509.13358", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13358", "abs": "https://arxiv.org/abs/2509.13358", "authors": ["Ethan Koland", "Lin Xi", "Nadeev Wijesuriya", "YingLiang Ma"], "title": "3D Reconstruction of Coronary Vessel Trees from Biplanar X-Ray Images Using a Geometric Approach", "comment": null, "summary": "X-ray angiography is widely used in cardiac interventions to visualize\ncoronary vessels, assess integrity, detect stenoses and guide treatment. We\npropose a framework for reconstructing 3D vessel trees from biplanar X-ray\nimages which are extracted from two X-ray videos captured at different C-arm\nangles. The proposed framework consists of three main components: image\nsegmentation, motion phase matching, and 3D reconstruction. An automatic video\nsegmentation method for X-ray angiography to enable semantic segmentation for\nimage segmentation and motion phase matching. The goal of the motion phase\nmatching is to identify a pair of X-ray images that correspond to a similar\nrespiratory and cardiac motion phase to reduce errors in 3D reconstruction.\nThis is achieved by tracking a stationary object such as a catheter or lead\nwithin the X-ray video. The semantic segmentation approach assigns different\nlabels to different object classes enabling accurate differentiation between\nblood vessels, balloons, and catheters. Once a suitable image pair is selected,\nkey anatomical landmarks (vessel branching points and endpoints) are matched\nbetween the two views using a heuristic method that minimizes reconstruction\nerrors. This is followed by a novel geometric reconstruction algorithm to\ngenerate the 3D vessel tree. The algorithm computes the 3D vessel centrelines\nby determining the intersection of two 3D surfaces. Compared to traditional\nmethods based on epipolar constraints, the proposed approach simplifies there\nconstruction workflow and improves overall accuracy. We trained and validated\nour segmentation method on 62 X-ray angiography video sequences. On the test\nset, our method achieved a segmentation accuracy of 0.703. The 3D\nreconstruction framework was validated by measuring the reconstruction error of\nkey anatomical landmarks, achieving a reprojection errors of 0.62mm +/- 0.38mm.", "AI": {"tldr": "该研究提出了一种从双平面X射线视频中重建3D血管树的框架，通过图像分割、运动相位匹配和新颖的几何重建算法，提高了重建精度并简化了工作流程。", "motivation": "X射线血管造影术广泛用于心脏介入治疗中，以可视化冠状血管、评估完整性、检测狭窄和指导治疗。然而，从2D图像中准确重建3D血管树，尤其是在存在呼吸和心脏运动的情况下，是一个挑战。本研究旨在提供一种更准确、更简化的3D血管重建方法。", "method": "所提出的框架包括三个主要组成部分：图像分割、运动相位匹配和3D重建。首先，使用自动视频语义分割方法区分血管、球囊和导管。其次，通过跟踪导管等静止物体，识别具有相似呼吸和心脏运动相位的X射线图像对，以减少重建误差。然后，使用启发式方法匹配两个视图中的关键解剖标志点（血管分支点和端点）。最后，采用一种新颖的几何重建算法，通过确定两个3D曲面的交点来计算3D血管中心线，从而生成3D血管树。该方法与基于极线约束的传统方法相比，简化了工作流程并提高了整体精度。", "result": "在62个X射线血管造影视频序列上训练和验证了分割方法，在测试集上实现了0.703的分割精度。3D重建框架通过测量关键解剖标志点的重建误差进行验证，再投影误差为0.62mm ± 0.38mm。", "conclusion": "该研究提出的3D血管树重建框架，结合了自动语义分割、运动相位匹配和新颖的几何重建算法，能够从双平面X射线图像中准确地重建3D血管结构。该方法简化了重建工作流程，并显著提高了重建精度，为心脏介入治疗提供了更好的3D可视化工具。"}}
{"id": "2509.13378", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13378", "abs": "https://arxiv.org/abs/2509.13378", "authors": ["Mattias Wingren", "Sören Andersson", "Sara Rosenberg", "Malin Andtfolk", "Susanne Hägglund", "Prashani Jayasingha Arachchige", "Linda Nyholm"], "title": "Using role-play and Hierarchical Task Analysis for designing human-robot interaction", "comment": "11 pages. This is a preprint version of the published paper in the\n  International Conference on Social Robotics:\n  https://link.springer.com/chapter/10.1007/978-981-96-3522-1_28", "summary": "We present the use of two methods we believe warrant more use than they\ncurrently have in the field of human-robot interaction: role-play and\nHierarchical Task Analysis. Some of its potential is showcased through our use\nof them in an ongoing research project which entails developing a robot\napplication meant to assist at a community pharmacy. The two methods have\nprovided us with several advantages. The role-playing provided a controlled and\nadjustable environment for understanding the customers' needs where pharmacists\ncould act as models for the robot's behavior; and the Hierarchical Task\nAnalysis ensured the behavior displayed was modelled correctly and aided\ndevelopment through facilitating co-design. Future research could focus on\ndeveloping task analysis methods especially suited for social robot\ninteraction.", "AI": {"tldr": "本文提出在人机交互领域中使用角色扮演和层级任务分析两种方法，并通过一个社区药房机器人应用案例展示了其在理解用户需求、建模机器人行为和促进协同设计方面的优势。", "motivation": "作者认为角色扮演和层级任务分析在人机交互领域的使用不足，但潜力巨大。因此，他们在一个旨在协助社区药房的机器人应用开发项目中，探索并展示了这两种方法的潜力。", "method": "研究中使用了两种主要方法：角色扮演（role-play）和层级任务分析（Hierarchical Task Analysis, HTA）。角色扮演用于模拟受控和可调节的环境以理解顾客需求，药剂师可作为机器人行为的模型；层级任务分析则用于确保行为建模的正确性并促进协同设计。", "result": "这两种方法带来了多项优势：角色扮演提供了一个受控且可调节的环境，有助于理解顾客需求，并让药剂师能为机器人行为提供模型；层级任务分析则确保了机器人行为的正确建模，并通过促进协同设计辅助了开发过程。", "conclusion": "角色扮演和层级任务分析是人机交互领域中行之有效的方法，尤其适用于理解用户需求、建模机器人行为和促进协同设计。未来的研究可以专注于开发特别适用于社交机器人交互的任务分析方法。"}}
{"id": "2509.13360", "categories": ["eess.IV", "cs.CV", "cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2509.13360", "abs": "https://arxiv.org/abs/2509.13360", "authors": ["L. Zimmer", "J. Weidner", "M. Balcerak", "F. Kofler", "I. Ezhov", "B. Menze", "B. Wiestler"], "title": "PREDICT-GBM: Platform for Robust Evaluation and Development of Individualized Computational Tumor Models in Glioblastoma", "comment": null, "summary": "Glioblastoma is the most prevalent primary brain malignancy, distinguished by\nits highly invasive behavior and exceptionally high rates of recurrence.\nConventional radiation therapy, which employs uniform treatment margins, fails\nto account for patient-specific anatomical and biological factors that\ncritically influence tumor cell migration. To address this limitation, numerous\ncomputational models of glioblastoma growth have been developed, enabling\ngeneration of tumor cell distribution maps extending beyond radiographically\nvisible regions and thus informing more precise treatment strategies. However,\ndespite encouraging preliminary findings, the clinical adoption of these growth\nmodels remains limited. To bridge this translational gap and accelerate both\nmodel development and clinical validation, we introduce PREDICT-GBM, a\ncomprehensive integrated pipeline and dataset for modeling and evaluation. This\nplatform enables systematic benchmarking of state-of-the-art tumor growth\nmodels using an expert-curated clinical dataset comprising 255 subjects with\ncomplete tumor segmentations and tissue characterization maps. Our analysis\ndemonstrates that personalized radiation treatment plans derived from tumor\ngrowth predictions achieved superior recurrence coverage compared to\nconventional uniform margin approaches for two of the evaluated models. This\nwork establishes a robust platform for advancing and systematically evaluating\ncutting-edge tumor growth modeling approaches, with the ultimate goal of\nfacilitating clinical translation and improving patient outcomes.", "AI": {"tldr": "该研究引入了PREDICT-GBM平台，这是一个用于胶质母细胞瘤生长模型建模和评估的综合管道和数据集。通过对255名患者的临床数据进行基准测试，结果显示，与传统方法相比，由肿瘤生长预测衍生的个性化放疗计划在复发覆盖率方面表现更优，旨在加速模型临床转化并改善患者预后。", "motivation": "胶质母细胞瘤是一种高度侵袭性且复发率极高的脑部恶性肿瘤。传统的放射治疗采用统一的治疗边缘，未能考虑患者特异性的解剖学和生物学因素，导致肿瘤细胞迁移无法被有效覆盖。尽管已开发出许多计算肿瘤生长模型来指导更精确的治疗策略，但其临床应用仍然有限，存在一个亟待弥合的转化鸿沟。", "method": "为弥合这一转化鸿沟并加速模型开发和临床验证，研究团队引入了PREDICT-GBM，一个用于建模和评估的综合集成管道及数据集。该平台利用一个由专家策划的临床数据集（包含255名患者的完整肿瘤分割和组织特征图），对最先进的肿瘤生长模型进行系统基准测试。", "result": "分析结果表明，对于所评估的两个模型，根据肿瘤生长预测得出的个性化放射治疗计划，与传统的统一边缘方法相比，在复发覆盖率方面取得了显著优势。", "conclusion": "这项工作建立了一个强大的平台，用于推进和系统评估尖端肿瘤生长建模方法，最终目标是促进其临床转化并改善患者预后。"}}
{"id": "2509.13330", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.13330", "abs": "https://arxiv.org/abs/2509.13330", "authors": ["Jorge Vicente-Martinez", "Edgar Ramirez-Laboreo"], "title": "A hybrid dynamic model and parameter estimation method for accurately simulating overhead cranes with friction", "comment": "11 pages, 13 figures", "summary": "This paper presents a new approach to accurately simulating 3D overhead\ncranes with friction. Nonlinear friction dynamics have a significant impact on\nthese systems, however, accurately modeling this phenomenon in simulations is a\nsignificant challenge. Traditional methods often rely on imprecise\napproximations of friction or require excessive computational times for\nreliable results. To address this, we present a hybrid dynamical model that\nfeatures a trade-off between high-fidelity friction modeling and computational\nefficiency. Furthermore, we present a step-by-step algorithm for the\ncomprehensive estimation of all unknown system parameters, including friction.\nThis methodology is based on Gaussian Process Regression (GPR) and Least\nSquares (LS) estimations. Finally, experimental validation with a laboratory\ncrane confirms the effectiveness of the proposed modeling and estimation\napproach.", "AI": {"tldr": "本文提出了一种新的方法，用于准确高效地模拟带有摩擦的3D桥式起重机，并提供全面的参数估计算法。", "motivation": "在3D桥式起重机模拟中，非线性摩擦动力学影响显著，但传统方法在准确建模摩擦方面存在精度不足或计算耗时过长的问题。", "method": "本文提出了一种混合动力学模型，旨在平衡高精度摩擦建模和计算效率。此外，还提出了一种基于高斯过程回归（GPR）和最小二乘（LS）估计的逐步算法，用于全面估计所有未知系统参数（包括摩擦）。", "result": "通过实验室起重机的实验验证，证实了所提出的建模和估计方法的有效性。", "conclusion": "所提出的混合动力学模型和基于GPR/LS的参数估计方法，能够有效、准确且计算高效地模拟带有摩擦的3D桥式起重机。"}}
{"id": "2509.13480", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13480", "abs": "https://arxiv.org/abs/2509.13480", "authors": ["Andrea Piergentili", "Beatrice Savoldi", "Matteo Negri", "Luisa Bentivogli"], "title": "Gender-Neutral Rewriting in Italian: Models, Approaches, and Trade-offs", "comment": "Accepted at CLiC-it 2025", "summary": "Gender-neutral rewriting (GNR) aims to reformulate text to eliminate\nunnecessary gender specifications while preserving meaning, a particularly\nchallenging task in grammatical-gender languages like Italian. In this work, we\nconduct the first systematic evaluation of state-of-the-art large language\nmodels (LLMs) for Italian GNR, introducing a two-dimensional framework that\nmeasures both neutrality and semantic fidelity to the input. We compare\nfew-shot prompting across multiple LLMs, fine-tune selected models, and apply\ntargeted cleaning to boost task relevance. Our findings show that open-weight\nLLMs outperform the only existing model dedicated to GNR in Italian, whereas\nour fine-tuned models match or exceed the best open-weight LLM's performance at\na fraction of its size. Finally, we discuss the trade-off between optimizing\nthe training data for neutrality and meaning preservation.", "AI": {"tldr": "本文首次系统评估了大型语言模型（LLM）在意大利语性别中立改写（GNR）任务上的表现，并引入了一个衡量中立性和语义保真度的二维框架。研究发现，开源LLM优于现有专用模型，而经过微调的模型在尺寸更小的情况下，性能与最佳开源LLM持平或更优。", "motivation": "在意大利语等具有语法性别的语言中，性别中立改写（GNR）是一项极具挑战性的任务，旨在消除不必要的性别特指同时保留文本含义。现有研究可能缺乏对最先进大型语言模型的系统性评估。", "method": "研究引入了一个衡量中立性和语义保真度的二维框架，对意大利语GNR任务中的最先进大型语言模型进行了首次系统评估。具体方法包括：比较多个LLM的少样本提示（few-shot prompting）、对选定模型进行微调，并对训练数据进行有针对性的清洗以提高任务相关性。", "result": "研究发现，开源LLM在意大利语GNR任务中表现优于唯一现有的专用模型。此外，经过微调的模型在尺寸远小于最佳开源LLM的情况下，其性能能够达到或超越最佳开源LLM。", "conclusion": "文章讨论了在优化训练数据时，平衡中立性与意义保留之间的权衡问题。"}}
{"id": "2509.13338", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.NE", "68T07, 68T09"], "pdf": "https://arxiv.org/pdf/2509.13338", "abs": "https://arxiv.org/abs/2509.13338", "authors": ["Hassan Gharoun", "Mohammad Sadegh Khorshidi", "Kasra Ranjbarigderi", "Fang Chen", "Amir H. Gandomi"], "title": "Proximity-Based Evidence Retrieval for Uncertainty-Aware Neural Networks", "comment": "15 pages, 4 figures, 3 tables", "summary": "This work proposes an evidence-retrieval mechanism for uncertainty-aware\ndecision-making that replaces a single global cutoff with an\nevidence-conditioned, instance-adaptive criterion. For each test instance,\nproximal exemplars are retrieved in an embedding space; their predictive\ndistributions are fused via Dempster-Shafer theory. The resulting fused belief\nacts as a per-instance thresholding mechanism. Because the supporting evidences\nare explicit, decisions are transparent and auditable. Experiments on\nCIFAR-10/100 with BiT and ViT backbones show higher or comparable\nuncertainty-aware performance with materially fewer confidently incorrect\noutcomes and a sustainable review load compared with applying threshold on\nprediction entropy. Notably, only a few evidences are sufficient to realize\nthese gains; increasing the evidence set yields only modest changes. These\nresults indicate that evidence-conditioned tagging provides a more reliable and\ninterpretable alternative to fixed prediction entropy thresholds for\noperational uncertainty-aware decision-making.", "AI": {"tldr": "本文提出了一种基于证据检索的、不确定性感知决策机制，它使用实例自适应的阈值而非全局固定阈值。通过融合检索到的近邻样本的预测分布，该机制提供了透明且可审计的决策，并在实验中展现出更可靠的性能和更少的错误。", "motivation": "现有方法通常依赖单一的全局截止点进行不确定性感知决策，缺乏实例适应性，且决策过程不够透明和可审计。研究旨在提供一种更可靠、更具解释性的替代方案。", "method": "该方法为每个测试实例在嵌入空间中检索近邻样本（证据），然后使用Dempster-Shafer理论融合这些样本的预测分布。由此产生的融合信念作为每个实例的自适应阈值机制。由于支持证据是明确的，因此决策是透明和可审计的。", "result": "在CIFAR-10/100数据集上，使用BiT和ViT骨干网络进行实验，结果显示该机制在不确定性感知性能方面更高或相当，同时显著减少了自信但错误的预测，并维持了可接受的审查负荷。值得注意的是，少量证据就足以实现这些改进，增加证据集只会带来微小变化。", "conclusion": "研究表明，证据条件标记为操作性不确定性感知决策提供了一种比固定预测熵阈值更可靠、更具解释性的替代方案。"}}
{"id": "2509.13332", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13332", "abs": "https://arxiv.org/abs/2509.13332", "authors": ["Pratik Jayarao", "Himanshu Gupta", "Neeraj Varshney", "Chaitanya Dwivedi"], "title": "Explicit Reasoning Makes Better Judges: A Systematic Study on Accuracy, Efficiency, and Robustness", "comment": null, "summary": "As Large Language Models (LLMs) are increasingly adopted as automated judges\nin benchmarking and reward modeling, ensuring their reliability, efficiency,\nand robustness has become critical. In this work, we present a systematic\ncomparison of \"thinking\" and \"non-thinking\" LLMs in the LLM-as-a-judge paradigm\nusing open-source Qwen 3 models of relatively small sizes (0.6B, 1.7B, and 4B\nparameters). We evaluate both accuracy and computational efficiency (FLOPs) on\nRewardBench tasks, and further examine augmentation strategies for non-thinking\nmodels, including in-context learning, rubric-guided judging, reference-based\nevaluation, and n-best aggregation. Our results show that despite these\nenhancements, non-thinking models generally fall short of their thinking\ncounterparts. Our results show that thinking models achieve approximately 10%\npoints higher accuracy with little overhead (under 2x), in contrast to\naugmentation strategies like few-shot learning, which deliver modest gains at a\nhigher cost (>8x). Bias and robustness analyses further demonstrate that\nthinking models maintain significantly greater consistency under a variety of\nbias conditions such as positional, bandwagon, identity, diversity, and random\nbiases (6% higher on average). We further extend our experiments to the\nmultilingual setting and our results confirm that explicit reasoning extends\nits benefits beyond English. Overall, our work results in several important\nfindings that provide systematic evidence that explicit reasoning offers clear\nadvantages in the LLM-as-a-judge paradigm not only in accuracy and efficiency\nbut also in robustness.", "AI": {"tldr": "本研究系统比较了“思考型”和“非思考型”大型语言模型（LLMs）作为评判者时的表现，发现思考型模型在准确性、效率和鲁棒性方面具有明显优势。", "motivation": "随着大型语言模型（LLMs）越来越多地被用作基准测试和奖励建模中的自动化评判者，确保其可靠性、效率和鲁棒性变得至关重要。", "method": "研究使用开源Qwen 3模型（0.6B、1.7B、4B参数）在RewardBench任务上，系统比较了“思考型”和“非思考型”LLMs的准确性和计算效率（FLOPs）。同时，还探讨了针对非思考型模型的增强策略，包括情境学习、基于评判标准的评估、基于参考的评估和n-best聚合。此外，还进行了偏见和鲁棒性分析，并将实验扩展到多语言环境。", "result": "结果显示，尽管有增强策略，非思考型模型普遍不如思考型模型。思考型模型在准确性上高出约10个百分点，而开销很小（低于2倍），相比之下，少样本学习等增强策略仅带来适度收益但成本更高（>8倍）。偏见和鲁棒性分析进一步表明，思考型模型在各种偏见条件下（如位置、羊群效应、身份、多样性和随机偏见）保持了显著更高的一致性（平均高出6%）。多语言实验也证实了显式推理的益处超越了英语。", "conclusion": "本研究提供了系统性证据，表明显式推理在LLM作为评判者范式中具有明显优势，不仅体现在准确性和效率上，也体现在鲁棒性上。"}}
{"id": "2509.13380", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.MA", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.13380", "abs": "https://arxiv.org/abs/2509.13380", "authors": ["Alejandro D. Mousist"], "title": "ASTREA: Introducing Agentic Intelligence for Orbital Thermal Autonomy", "comment": "This preprint presents ASTREA, a multi-agent architecture combining\n  LLM-guided semantic modulation with reinforcement learning for autonomous\n  satellite operations. The system is validated in hardware orbital\n  environments", "summary": "This paper presents ASTREA, the first agentic system deployed on\nflight-heritage hardware (TRL 9) for autonomous spacecraft operations. Using\nthermal control as a representative use case, we integrate a\nresource-constrained Large Language Model (LLM) agent with a reinforcement\nlearning controller in an asynchronous architecture tailored for\nspace-qualified platforms. Ground experiments show that LLM-guided supervision\nimproves thermal stability and reduces violations, confirming the feasibility\nof combining semantic reasoning with adaptive control under hardware\nconstraints. However, on-orbit validation aboard the International Space\nStation (ISS) reveals performance degradation caused by inference latency\nmismatched with the rapid thermal cycles characteristic of Low Earth Orbit\n(LEO) satellites. These results highlight both the opportunities and current\nlimitations of agentic LLM-based systems in real flight environments, providing\npractical design guidelines for future space autonomy.", "AI": {"tldr": "本文介绍了ASTREA，首个部署在飞行遗产硬件（TRL 9）上的代理系统，用于自主航天器操作。以热控制为例，该系统将资源受限的大语言模型（LLM）代理与强化学习控制器集成，并在地面实验中表现出LLM指导的监督能提高热稳定性。然而，国际空间站（ISS）上的在轨验证揭示了推理延迟与低地球轨道（LEO）卫星快速热循环不匹配导致的性能下降，突出了此类系统在实际飞行环境中的机遇与局限性。", "motivation": "开发并部署首个基于大语言模型（LLM）的代理系统到飞行遗产硬件上，以实现自主航天器操作，并以热控制作为代表性应用场景，验证其在资源受限空间平台上的可行性。", "method": "ASTREA系统采用异步架构，集成了资源受限的大语言模型（LLM）代理与强化学习控制器。该系统部署在飞行遗产硬件（TRL 9）上，并以热控制作为用例。研究方法包括地面实验和国际空间站（ISS）上的在轨验证。", "result": "地面实验表明，LLM引导的监督显著提高了热稳定性并减少了违规，证实了在硬件约束下结合语义推理与自适应控制的可行性。然而，在轨验证揭示，由于推理延迟与低地球轨道（LEO）卫星的快速热循环不匹配，导致了系统性能下降。", "conclusion": "研究结果突出了代理式LLM系统在真实飞行环境中的机遇与当前局限性，为未来的空间自主性设计提供了实用的指导方针。"}}
{"id": "2509.13372", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.ET", "q-bio.QM", "92C50, 68T07, 76D05, 65D18, 92C55", "I.4.6; I.4.8; J.3; I.2.10; I.4.9"], "pdf": "https://arxiv.org/pdf/2509.13372", "abs": "https://arxiv.org/abs/2509.13372", "authors": ["Prahlad G Menon"], "title": "Generative AI Pipeline for Interactive Prompt-driven 2D-to-3D Vascular Reconstruction for Fontan Geometries from Contrast-Enhanced X-Ray Fluoroscopy Imaging", "comment": null, "summary": "Fontan palliation for univentricular congenital heart disease progresses to\nhemodynamic failure with complex flow patterns poorly characterized by\nconventional 2D imaging. Current assessment relies on fluoroscopic angiography,\nproviding limited 3D geometric information essential for computational fluid\ndynamics (CFD) analysis and surgical planning.\n  A multi-step AI pipeline was developed utilizing Google's Gemini 2.5 Flash\n(2.5B parameters) for systematic, iterative processing of fluoroscopic\nangiograms through transformer-based neural architecture. The pipeline\nencompasses medical image preprocessing, vascular segmentation, contrast\nenhancement, artifact removal, and virtual hemodynamic flow visualization\nwithin 2D projections. Final views were processed through Tencent's\nHunyuan3D-2mini (384M parameters) for stereolithography file generation.\n  The pipeline successfully generated geometrically optimized 2D projections\nfrom single-view angiograms after 16 processing steps using a custom web\ninterface. Initial iterations contained hallucinated vascular features\nrequiring iterative refinement to achieve anatomically faithful\nrepresentations. Final projections demonstrated accurate preservation of\ncomplex Fontan geometry with enhanced contrast suitable for 3D conversion.\nAI-generated virtual flow visualization identified stagnation zones in central\nconnections and flow patterns in branch arteries. Complete processing required\nunder 15 minutes with second-level API response times.\n  This approach demonstrates clinical feasibility of generating CFD-suitable\ngeometries from routine angiographic data, enabling 3D generation and rapid\nvirtual flow visualization for cursory insights prior to full CFD simulation.\nWhile requiring refinement cycles for accuracy, this establishes foundation for\ndemocratizing advanced geometric and hemodynamic analysis using readily\navailable imaging data.", "AI": {"tldr": "该研究开发了一个多步AI管道，能够从常规2D血管造影数据中生成适用于计算流体动力学（CFD）的3D Fontan几何结构，并进行快速虚拟血流可视化。", "motivation": "Fontan手术后的单心室先天性心脏病患者常出现血流动力学衰竭，其复杂的血流模式难以通过传统2D成像进行充分表征。当前的评估依赖于荧光血管造影，但其提供的3D几何信息有限，不足以进行CFD分析和手术规划。", "method": "开发了一个多步AI管道，利用Google的Gemini 2.5 Flash（2.5B参数，基于Transformer的神经网络架构）对荧光血管造影图像进行系统、迭代处理，包括医学图像预处理、血管分割、对比度增强、伪影去除和2D投影中的虚拟血流可视化。最终视图通过腾讯的Hunyuan3D-2mini（384M参数）处理，用于生成立体光刻（STL）文件。", "result": "该管道成功地从单视图血管造影图像生成了几何优化的2D投影，经过16个处理步骤后，准确保留了复杂的Fontan几何结构并增强了对比度，适用于3D转换。AI生成的虚拟血流可视化识别了中心连接处的停滞区和分支动脉中的血流模式。整个处理过程在15分钟内完成，API响应时间为秒级。", "conclusion": "该方法证明了从常规血管造影数据生成CFD适用几何的临床可行性，实现了3D生成和快速虚拟血流可视化，为全面的CFD模拟提供了初步见解。尽管需要迭代优化以提高准确性，但这为利用现有成像数据普及高级几何和血流动力学分析奠定了基础。"}}
{"id": "2509.13369", "categories": ["eess.SY", "cs.CY", "cs.HC", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.13369", "abs": "https://arxiv.org/abs/2509.13369", "authors": ["Rashid Mushkani"], "title": "Right-to-Override for Critical Urban Control Systems: A Deliberative Audit Method for Buildings, Power, and Transport", "comment": null, "summary": "Automation now steers building HVAC, distribution grids, and traffic signals,\nyet residents rarely have authority to pause or redirect these systems when\nthey harm inclusivity, safety, or accessibility. We formalize a\nRight-to-Override (R2O) - defining override authorities, evidentiary\nthresholds, and domain-validated safe fallback states - and introduce a\nDeliberative Audit Method (DAM) with playbooks for pre-deployment walkthroughs,\nshadow-mode trials, and post-incident review. We instantiate R2O/DAM in\nsimulations of smart-grid load shedding, building HVAC under occupancy\nuncertainty, and multi-agent traffic signals. R2O reduces distributional harm\nwith limited efficiency loss: load-shedding disparity in unserved energy drops\nfrom 5.61x to 0.69x with constant curtailment; an override eliminates two\ndiscomfort-hours for seniors at an energy cost of 77 kWh; and median pedestrian\nwait falls from 90.4 s to 55.9 s with a 6.0 s increase in mean vehicle delay.\nWe also contribute a policy standard, audit worksheets, and a ModelOps\nintegration pattern to make urban automation contestable and reviewable.", "AI": {"tldr": "该研究提出了“覆盖权（R2O）”和“审议审计方法（DAM）”，旨在让居民能够干预自动化城市系统，并通过模拟证明其在减少分配不公和危害方面的有效性，同时保持有限的效率损失。", "motivation": "尽管自动化系统（如HVAC、电网、交通信号）已广泛应用，但居民很少有权暂停或重定向这些系统，当它们损害包容性、安全性或可访问性时。这促使研究人员探索如何让居民拥有对这些系统的控制权。", "method": "该研究正式定义了“覆盖权（R2O）”，包括覆盖权限、证据阈值和领域验证的安全回退状态。同时，引入了“审议审计方法（DAM）”，提供用于部署前演练、影子模式试验和事件后审查的剧本。研究通过智能电网负荷削减、基于占用不确定性的建筑HVAC和多智能体交通信号的模拟来实例化R2O/DAM。此外，还贡献了政策标准、审计工作表和ModelOps集成模式。", "result": "R2O显著减少了分配不公，同时效率损失有限：负荷削减中未服务能源的差异从5.61倍降至0.69倍（在恒定削减下）；一次覆盖消除了老年人两小时的不适，能源成本为77千瓦时；行人等待时间中位数从90.4秒降至55.9秒，而平均车辆延迟仅增加6.0秒。", "conclusion": "R2O和DAM使城市自动化系统变得可质疑和可审查，有效减少了分配不公和危害，同时将效率损失降至最低。这有助于提升自动化系统的包容性、安全性和可访问性。"}}
{"id": "2509.13539", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13539", "abs": "https://arxiv.org/abs/2509.13539", "authors": ["Alisa Kanganis", "Katherine A. Keith"], "title": "Op-Fed: Opinion, Stance, and Monetary Policy Annotations on FOMC Transcripts Using Active Learning", "comment": null, "summary": "The U.S. Federal Open Market Committee (FOMC) regularly discusses and sets\nmonetary policy, affecting the borrowing and spending decisions of millions of\npeople. In this work, we release Op-Fed, a dataset of 1044 human-annotated\nsentences and their contexts from FOMC transcripts. We faced two major\ntechnical challenges in dataset creation: imbalanced classes -- we estimate\nfewer than 8% of sentences express a non-neutral stance towards monetary policy\n-- and inter-sentence dependence -- 65% of instances require context beyond the\nsentence-level. To address these challenges, we developed a five-stage\nhierarchical schema to isolate aspects of opinion, monetary policy, and stance\ntowards monetary policy as well as the level of context needed. Second, we\nselected instances to annotate using active learning, roughly doubling the\nnumber of positive instances across all schema aspects. Using Op-Fed, we found\na top-performing, closed-weight LLM achieves 0.80 zero-shot accuracy in opinion\nclassification but only 0.61 zero-shot accuracy classifying stance towards\nmonetary policy -- below our human baseline of 0.89. We expect Op-Fed to be\nuseful for future model training, confidence calibration, and as a seed dataset\nfor future annotation efforts.", "AI": {"tldr": "本文发布了Op-Fed数据集，包含1044条人工标注的FOMC会议记录语句及其上下文，旨在分析货币政策立场。该数据集通过分层标注模式和主动学习克服了类别不平衡和语句间依赖的挑战。初步测试显示，LLM在意见分类上表现尚可，但在货币政策立场分类上仍远低于人类基线。", "motivation": "美国联邦公开市场委员会(FOMC)的货币政策讨论影响数百万人的借贷和消费决策。研究人员旨在通过分析FOMC会议记录中的意见和立场，更好地理解这些政策，并为此创建一个专门的数据集。主要动机是解决现有数据中类别高度不平衡（非中立立场语句少于8%）和语句理解需依赖上下文（65%的实例）的挑战。", "method": "研究方法包括：1. 发布Op-Fed数据集，包含1044条来自FOMC会议记录的人工标注语句及其上下文。2. 针对类别不平衡和语句间依赖问题，开发了一个五阶段分层标注模式，以隔离意见、货币政策、对货币政策的立场以及所需上下文级别。3. 采用主动学习方法选择实例进行标注，将所有模式方面的“积极”实例数量大致翻倍。", "result": "1. 成功创建并发布了Op-Fed数据集。2. 使用Op-Fed数据集，发现一个表现最佳的闭源LLM在意见分类上的零样本准确率达到0.80。3. 但该LLM在对货币政策立场的零样本分类准确率仅为0.61，远低于人类基线（0.89）。", "conclusion": "Op-Fed数据集预计将对未来的模型训练、置信度校准以及作为后续标注工作的种子数据集非常有价值。尽管LLM在意见分类上表现尚可，但在更细致的货币政策立场分类上仍有显著提升空间。"}}
{"id": "2509.13353", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13353", "abs": "https://arxiv.org/abs/2509.13353", "authors": ["Muhammad Adnan Shahzad"], "title": "Hybrid Quantum-Classical Model for Image Classification", "comment": null, "summary": "This study presents a systematic comparison between hybrid quantum-classical\nneural networks and purely classical models across three benchmark datasets\n(MNIST, CIFAR100, and STL10) to evaluate their performance, efficiency, and\nrobustness. The hybrid models integrate parameterized quantum circuits with\nclassical deep learning architectures, while the classical counterparts use\nconventional convolutional neural networks (CNNs). Experiments were conducted\nover 50 training epochs for each dataset, with evaluations on validation\naccuracy, test accuracy, training time, computational resource usage, and\nadversarial robustness (tested with $\\epsilon=0.1$ perturbations).Key findings\ndemonstrate that hybrid models consistently outperform classical models in\nfinal accuracy, achieving {99.38\\% (MNIST), 41.69\\% (CIFAR100), and 74.05\\%\n(STL10) validation accuracy, compared to classical benchmarks of 98.21\\%,\n32.25\\%, and 63.76\\%, respectively. Notably, the hybrid advantage scales with\ndataset complexity, showing the most significant gains on CIFAR100 (+9.44\\%)\nand STL10 (+10.29\\%). Hybrid models also train 5--12$\\times$ faster (e.g.,\n21.23s vs. 108.44s per epoch on MNIST) and use 6--32\\% fewer parameters} while\nmaintaining superior generalization to unseen test data.Adversarial robustness\ntests reveal that hybrid models are significantly more resilient on simpler\ndatasets (e.g., 45.27\\% robust accuracy on MNIST vs. 10.80\\% for classical) but\nshow comparable fragility on complex datasets like CIFAR100 ($\\sim$1\\%\nrobustness for both). Resource efficiency analyses indicate that hybrid models\nconsume less memory (4--5GB vs. 5--6GB for classical) and lower CPU utilization\n(9.5\\% vs. 23.2\\% on average).These results suggest that hybrid\nquantum-classical architectures offer compelling advantages in accuracy,\ntraining efficiency, and parameter scalability, particularly for complex vision\ntasks.", "AI": {"tldr": "解析错误", "motivation": "解析错误", "method": "解析错误", "result": "解析错误", "conclusion": "解析错误"}}
{"id": "2509.13333", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13333", "abs": "https://arxiv.org/abs/2509.13333", "authors": ["Maheep Chaudhary", "Ian Su", "Nikhil Hooda", "Nishith Shankar", "Julia Tan", "Kevin Zhu", "Ashwinee Panda", "Ryan Lagasse", "Vasu Sharma"], "title": "Evaluation Awareness Scales Predictably in Open-Weights Large Language Models", "comment": null, "summary": "Large language models (LLMs) can internally distinguish between evaluation\nand deployment contexts, a behaviour known as \\emph{evaluation awareness}. This\nundermines AI safety evaluations, as models may conceal dangerous capabilities\nduring testing. Prior work demonstrated this in a single $70$B model, but the\nscaling relationship across model sizes remains unknown. We investigate\nevaluation awareness across $15$ models scaling from $0.27$B to $70$B\nparameters from four families using linear probing on steering vector\nactivations. Our results reveal a clear power-law scaling: evaluation awareness\nincreases predictably with model size. This scaling law enables forecasting\ndeceptive behavior in future larger models and guides the design of scale-aware\nevaluation strategies for AI safety. A link to the implementation of this paper\ncan be found at\nhttps://anonymous.4open.science/r/evaluation-awareness-scaling-laws/README.md.", "AI": {"tldr": "大型语言模型（LLMs）的“评估意识”能力（即区分评估和部署环境）随模型规模的增大呈幂律关系增长，这挑战了AI安全评估。", "motivation": "LLMs的评估意识会阻碍AI安全评估，因为模型可能在测试中隐藏危险能力。此前研究仅在一个70B模型中证实了这一点，但其与模型规模的扩展关系尚不清楚。", "method": "研究人员使用线性探测（linear probing）技术，通过分析转向向量激活（steering vector activations），对来自四个系列、规模从0.27B到70B参数的15个模型进行了评估意识的调查。", "result": "研究结果揭示了一个清晰的幂律扩展关系：评估意识随着模型规模的增大而可预测地增强。", "conclusion": "这一扩展定律使得预测未来更大模型中的欺骗行为成为可能，并为设计针对AI安全的、规模感知的评估策略提供了指导。"}}
{"id": "2509.13381", "categories": ["cs.RO", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.13381", "abs": "https://arxiv.org/abs/2509.13381", "authors": ["Zhang Xueyao", "Yang Bo", "Yu Zhiwen", "Cao Xuelin", "George C. Alexandropoulos", "Merouane Debbah", "Chau Yuen"], "title": "Cooperative Target Detection with AUVs: A Dual-Timescale Hierarchical MARDL Approach", "comment": "6 pages", "summary": "Autonomous Underwater Vehicles (AUVs) have shown great potential for\ncooperative detection and reconnaissance. However, collaborative AUV\ncommunications introduce risks of exposure. In adversarial environments,\nachieving efficient collaboration while ensuring covert operations becomes a\nkey challenge for underwater cooperative missions. In this paper, we propose a\nnovel dual time-scale Hierarchical Multi-Agent Proximal Policy Optimization\n(H-MAPPO) framework. The high-level component determines the individuals\nparticipating in the task based on a central AUV, while the low-level component\nreduces exposure probabilities through power and trajectory control by the\nparticipating AUVs. Simulation results show that the proposed framework\nachieves rapid convergence, outperforms benchmark algorithms in terms of\nperformance, and maximizes long-term cooperative efficiency while ensuring\ncovert operations.", "AI": {"tldr": "本文提出了一种双时间尺度分层多智能体近端策略优化（H-MAPPO）框架，用于在对抗环境中实现水下自主水下航行器（AUV）的高效隐蔽协作。", "motivation": "水下AUV协作通信存在暴露风险。在对抗性环境中，如何在确保隐蔽操作的同时实现高效协作是水下合作任务的关键挑战。", "method": "提出了一种新颖的双时间尺度分层多智能体近端策略优化（H-MAPPO）框架。高层组件由中央AUV决定任务参与者，低层组件由参与AUV通过功率和轨迹控制来降低暴露概率。", "result": "仿真结果表明，所提出的框架实现了快速收敛，在性能上优于基准算法，并在确保隐蔽操作的同时最大化了长期协作效率。", "conclusion": "该H-MAPPO框架能够有效平衡水下AUV合作任务的效率和隐蔽性。"}}
{"id": "2509.13576", "categories": ["eess.IV", "cs.CV", "65R32"], "pdf": "https://arxiv.org/pdf/2509.13576", "abs": "https://arxiv.org/abs/2509.13576", "authors": ["Haodong Li", "Shuo Han", "Haiyang Mao", "Yu Shi", "Changsheng Fang", "Jianjia Zhang", "Weiwen Wu", "Hengyong Yu"], "title": "Cross-Distribution Diffusion Priors-Driven Iterative Reconstruction for Sparse-View CT", "comment": "11 pages, 8 figures, under reviewing of IEEE TMI", "summary": "Sparse-View CT (SVCT) reconstruction enhances temporal resolution and reduces\nradiation dose, yet its clinical use is hindered by artifacts due to view\nreduction and domain shifts from scanner, protocol, or anatomical variations,\nleading to performance degradation in out-of-distribution (OOD) scenarios. In\nthis work, we propose a Cross-Distribution Diffusion Priors-Driven Iterative\nReconstruction (CDPIR) framework to tackle the OOD problem in SVCT. CDPIR\nintegrates cross-distribution diffusion priors, derived from a Scalable\nInterpolant Transformer (SiT), with model-based iterative reconstruction\nmethods. Specifically, we train a SiT backbone, an extension of the Diffusion\nTransformer (DiT) architecture, to establish a unified stochastic interpolant\nframework, leveraging Classifier-Free Guidance (CFG) across multiple datasets.\nBy randomly dropping the conditioning with a null embedding during training,\nthe model learns both domain-specific and domain-invariant priors, enhancing\ngeneralizability. During sampling, the globally sensitive transformer-based\ndiffusion model exploits the cross-distribution prior within the unified\nstochastic interpolant framework, enabling flexible and stable control over\nmulti-distribution-to-noise interpolation paths and decoupled sampling\nstrategies, thereby improving adaptation to OOD reconstruction. By alternating\nbetween data fidelity and sampling updates, our model achieves state-of-the-art\nperformance with superior detail preservation in SVCT reconstructions.\nExtensive experiments demonstrate that CDPIR significantly outperforms existing\napproaches, particularly under OOD conditions, highlighting its robustness and\npotential clinical value in challenging imaging scenarios.", "AI": {"tldr": "该论文提出了一种名为CDPIR的框架，通过整合来自可扩展插值变换器（SiT）的跨分布扩散先验与模型迭代重建方法，解决了稀疏视图CT（SVCT）在分布外（OOD）场景下的重建伪影和性能下降问题。", "motivation": "稀疏视图CT（SVCT）重建虽然能提高时间分辨率并减少辐射剂量，但由于视图减少以及扫描仪、协议或解剖结构变化导致的领域偏移，在分布外（OOD）场景下会出现伪影和性能下降，从而阻碍了其临床应用。", "method": "CDPIR框架将从可扩展插值变换器（SiT）导出的跨分布扩散先验与基于模型的迭代重建方法相结合。具体而言，训练了一个作为Diffusion Transformer (DiT) 架构扩展的SiT骨干网络，通过利用跨多个数据集的无分类器指导（CFG）建立统一的随机插值框架。在训练期间，通过随机丢弃带有空嵌入的条件，模型学习领域特定和领域不变的先验，增强泛化能力。在采样时，全局敏感的基于Transformer的扩散模型在统一的随机插值框架内利用跨分布先验，实现对多分布到噪声插值路径的灵活稳定控制和解耦采样策略，从而改善对OOD重建的适应性。通过在数据保真度和采样更新之间交替进行，实现重建。", "result": "CDPIR在SVCT重建中实现了最先进的性能，尤其是在OOD条件下，表现出卓越的细节保留。广泛的实验表明，CDPIR显著优于现有方法。", "conclusion": "CDPIR框架在OOD条件下表现出卓越的鲁棒性和性能，解决了SVCT重建的挑战，凸显了其在复杂成像场景中的潜在临床价值。"}}
{"id": "2509.13371", "categories": ["eess.SY", "cs.LG", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.13371", "abs": "https://arxiv.org/abs/2509.13371", "authors": ["Xuyuan Kang", "Xiao Wang", "Jingjing An", "Da Yan"], "title": "A novel approach of day-ahead cooling load prediction and optimal control for ice-based thermal energy storage (TES) system in commercial buildings", "comment": "16 pages,14 figures,published to Energy & Buildings", "summary": "Thermal energy storage (TES) is an effective method for load shifting and\ndemand response in buildings. Optimal TES control and management are essential\nto improve the performance of the cooling system. Most existing TES systems\noperate on a fixed schedule, which cannot take full advantage of its load\nshifting capability, and requires extensive investigation and optimization.\nThis study proposed a novel integrated load prediction and optimized control\napproach for ice-based TES in commercial buildings. A cooling load prediction\nmodel was developed and a mid-day modification mechanism was introduced into\nthe prediction model to improve the accuracy. Based on the predictions, a\nrule-based control strategy was proposed according to the time-of-use tariff;\nthe mid-day control adjustment mechanism was introduced in accordance with the\nmid-day prediction modifications. The proposed approach was applied in the\nice-based TES system of a commercial complex in Beijing, and achieved a mean\nabsolute error (MAE) of 389 kW and coefficient of variance of MAE of 12.5%. The\nintegrated prediction-based control strategy achieved an energy cost saving\nrate of 9.9%. The proposed model was deployed in the realistic building\nautomation system of the case building and significantly improved the\nefficiency and automation of the cooling system.", "AI": {"tldr": "本研究提出了一种针对商业建筑冰蓄冷系统负荷预测与优化控制的集成方法，通过引入日中修正机制提高预测精度，并基于分时电价设计控制策略，实现了显著的能源成本节约和系统效率提升。", "motivation": "现有热能储存（TES）系统多采用固定调度，未能充分利用其负荷转移能力，且需要大量研究和优化。因此，需要开发一种更智能、更高效的TES控制与管理方法，以应对负荷转移和需求响应的需求。", "method": "本研究开发了一个冷却负荷预测模型，并引入了日中修正机制以提高预测精度。在此基础上，根据分时电价提出了一种基于规则的控制策略，并结合日中预测修正引入了日中控制调整机制。该方法应用于北京某商业综合体的冰蓄冷系统。", "result": "预测模型实现了389 kW的平均绝对误差（MAE）和12.5%的MAE变异系数。集成的预测-控制策略实现了9.9%的能源成本节约率。该模型已部署到实际建筑自动化系统中，显著提升了冷却系统的效率和自动化水平。", "conclusion": "所提出的集成负荷预测与优化控制方法能够有效提高冰蓄冷系统的性能，显著降低能源成本，并提升冷却系统的效率和自动化水平，在实际应用中表现出色。"}}
{"id": "2509.13569", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13569", "abs": "https://arxiv.org/abs/2509.13569", "authors": ["John Mendonça", "Lining Zhang", "Rahul Mallidi", "Alon Lavie", "Isabel Trancoso", "Luis Fernando D'Haro", "João Sedoc"], "title": "Overview of Dialog System Evaluation Track: Dimensionality, Language, Culture and Safety at DSTC 12", "comment": "DSTC12 Track 1 Overview Paper. https://chateval.org/dstc12", "summary": "The rapid advancement of Large Language Models (LLMs) has intensified the\nneed for robust dialogue system evaluation, yet comprehensive assessment\nremains challenging. Traditional metrics often prove insufficient, and safety\nconsiderations are frequently narrowly defined or culturally biased. The DSTC12\nTrack 1, \"Dialog System Evaluation: Dimensionality, Language, Culture and\nSafety,\" is part of the ongoing effort to address these critical gaps. The\ntrack comprised two subtasks: (1) Dialogue-level, Multi-dimensional Automatic\nEvaluation Metrics, and (2) Multilingual and Multicultural Safety Detection.\nFor Task 1, focused on 10 dialogue dimensions, a Llama-3-8B baseline achieved\nthe highest average Spearman's correlation (0.1681), indicating substantial\nroom for improvement. In Task 2, while participating teams significantly\noutperformed a Llama-Guard-3-1B baseline on the multilingual safety subset (top\nROC-AUC 0.9648), the baseline proved superior on the cultural subset (0.5126\nROC-AUC), highlighting critical needs in culturally-aware safety. This paper\ndescribes the datasets and baselines provided to participants, as well as\nsubmission evaluation results for each of the two proposed subtasks.", "AI": {"tldr": "DSTC12 Track 1 旨在解决大型语言模型对话系统评估中的挑战，包括多维度自动评估和多语言多文化安全检测。研究发现，多维度评估仍有很大提升空间，而文化感知安全检测是一个亟待解决的关键问题。", "motivation": "大型语言模型（LLMs）的快速发展，对健壮的对话系统评估提出了更高要求。然而，传统指标往往不足，且安全考量常被狭隘定义或存在文化偏见。", "method": "DSTC12 Track 1 包含两个子任务：(1) 对话级、多维度自动评估指标，涵盖10个对话维度；(2) 多语言和多文化安全检测。研究提供了数据集和基线模型（任务1使用Llama-3-8B，任务2使用Llama-Guard-3-1B），并评估了参与团队的提交结果。", "result": "任务1中，Llama-3-8B基线取得了最高的平均Spearman相关性（0.1681），表明仍有显著改进空间。任务2中，参与团队在多语言安全子集上显著优于Llama-Guard-3-1B基线（最高ROC-AUC 0.9648），但基线在文化子集上表现更优（0.5126 ROC-AUC），凸显了文化感知安全方面的关键需求。", "conclusion": "在多维度对话评估指标方面仍有巨大的改进空间。同时，迫切需要提升文化感知安全检测能力。本论文详细描述了为参与者提供的数据集、基线以及两个子任务的提交评估结果。"}}
{"id": "2509.13361", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13361", "abs": "https://arxiv.org/abs/2509.13361", "authors": ["Tong Yulin", "Liang Xuechen"], "title": "Research on Expressway Congestion Warning Technology Based on YOLOv11-DIoU and GRU-Attention", "comment": null, "summary": "Expressway traffic congestion severely reduces travel efficiency and hinders\nregional connectivity. Existing \"detection-prediction\" systems have critical\nflaws: low vehicle perception accuracy under occlusion and loss of\nlong-sequence dependencies in congestion forecasting. This study proposes an\nintegrated technical framework to resolve these issues.For traffic flow\nperception, two baseline algorithms were optimized. Traditional YOLOv11 was\nupgraded to YOLOv11-DIoU by replacing GIoU Loss with DIoU Loss, and DeepSort\nwas improved by fusing Mahalanobis (motion) and cosine (appearance) distances.\nExperiments on Chang-Shen Expressway videos showed YOLOv11-DIoU achieved 95.7\\%\nmAP (6.5 percentage points higher than baseline) with 5.3\\% occlusion miss\nrate. DeepSort reached 93.8\\% MOTA (11.3 percentage points higher than SORT)\nwith only 4 ID switches. Using the Greenberg model (for 10-15 vehicles/km\nhigh-density scenarios), speed and density showed a strong negative correlation\n(r=-0.97), conforming to traffic flow theory. For congestion warning, a\nGRU-Attention model was built to capture congestion precursors. Trained 300\nepochs with flow, density, and speed, it achieved 99.7\\% test accuracy (7-9\npercentage points higher than traditional GRU). In 10-minute advance warnings\nfor 30-minute congestion, time error was $\\leq$ 1 minute. Validation with an\nindependent video showed 95\\% warning accuracy, over 90\\% spatial overlap of\ncongestion points, and stable performance in high-flow ($>$5 vehicles/second)\nscenarios.This framework provides quantitative support for expressway\ncongestion control, with promising intelligent transportation applications.", "AI": {"tldr": "本研究提出了一套集成技术框架，通过优化目标检测与跟踪算法（YOLOv11-DIoU和改进DeepSort）以及构建GRU-Attention模型，显著提升了高速公路交通拥堵的感知准确性和预测预警能力。", "motivation": "高速公路交通拥堵严重降低出行效率并阻碍区域互联互通。现有“检测-预测”系统存在关键缺陷：遮挡下车辆感知精度低，以及拥堵预测中长序列依赖关系缺失。", "method": "该研究优化了两个基线算法：将YOLOv11升级为YOLOv11-DIoU（用DIoU Loss替换GIoU Loss）；通过融合马哈拉诺比斯（运动）和余弦（外观）距离改进了DeepSort。同时，采用Greenberg模型分析高密度场景下的交通流，并构建了GRU-Attention模型来捕捉拥堵前兆进行预警。", "result": "YOLOv11-DIoU在常深高速视频上实现了95.7%的mAP（比基线高6.5个百分点），遮挡漏检率为5.3%。改进的DeepSort达到了93.8%的MOTA（比SORT高11.3个百分点），仅有4次ID切换。Greenberg模型显示速度和密度呈强负相关（r=-0.97）。GRU-Attention模型在300个epoch训练后，测试准确率达到99.7%（比传统GRU高7-9个百分点），在10分钟提前预警30分钟拥堵时，时间误差≤1分钟。独立视频验证显示，预警准确率达95%，拥堵点空间重叠度超过90%，并在高流量场景下表现稳定。", "conclusion": "该框架为高速公路拥堵控制提供了量化支持，并在智能交通应用方面具有广阔前景。"}}
{"id": "2509.13334", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13334", "abs": "https://arxiv.org/abs/2509.13334", "authors": ["Anand Swaroop", "Akshat Nallani", "Saksham Uboweja", "Adiliia Uzdenova", "Michael Nguyen", "Kevin Zhu", "Sunishchal Dev", "Ashwinee Panda", "Vasu Sharma", "Maheep Chaudhary"], "title": "FRIT: Using Causal Importance to Improve Chain-of-Thought Faithfulness", "comment": null, "summary": "Chain-of-thought (CoT) reasoning has emerged as a powerful tool for improving\nlarge language model performance on complex tasks, but recent work shows that\nreasoning steps often fail to causally influence the final answer, creating\nbrittle and untrustworthy outputs. Prior approaches focus primarily on\nmeasuring faithfulness, while methods for systematically improving it remain\nlimited. We introduce Faithful Reasoning via Intervention Training (FRIT), a\nscalable alignment method that trains models to produce causally consistent\nreasoning by learning from systematically corrupted examples. FRIT generates\nsynthetic training data by intervening on individual reasoning steps in\nmodel-generated CoTs, creating faithful/unfaithful pairs that highlight when\nreasoning breaks down. We then apply Direct Preference Optimization to teach\nmodels to prefer causally consistent reasoning paths. Evaluating on Qwen3-8B\nand Mistral-7B-v0.1 across factual and symbolic reasoning tasks, FRIT increases\nfaithful reasoning by $3.4$ percentage points for Mistral on GSM8K while\nimproving accuracy by $7.6$ percentage points. Our approach provides the first\nscalable, supervision-free method for training language models to produce more\nreliable and interpretable reasoning, addressing a critical gap between\nreasoning performance and trustworthiness. We release our code at\n\\href{https://github.com/Anut-py/frit}.", "AI": {"tldr": "本文提出FRIT（干预训练下的忠实推理），这是一种可扩展、无需监督的对齐方法，通过系统地从受损示例中学习，训练大型语言模型生成因果一致的思维链推理，从而提高推理的忠实性和准确性。", "motivation": "思维链（CoT）推理虽然能提高大语言模型在复杂任务上的性能，但其推理步骤往往未能因果地影响最终答案，导致输出脆弱且不可信。现有方法主要关注衡量忠实性，而系统地改进忠实性的方法仍然有限。", "method": "本文引入了FRIT方法，通过干预模型生成的CoT中的单个推理步骤来生成合成训练数据，从而创建忠实/不忠实的配对示例，以揭示推理何时失效。随后，利用直接偏好优化（DPO）训练模型偏好因果一致的推理路径。该方法是可扩展且无需监督的。", "result": "在Qwen3-8B和Mistral-7B-v0.1模型上，针对事实和符号推理任务进行评估，FRIT使Mistral在GSM8K任务上的忠实推理提高了3.4个百分点，同时准确率提高了7.6个百分点。", "conclusion": "FRIT是首个可扩展、无需监督的方法，用于训练语言模型产生更可靠和可解释的推理，弥补了推理性能与可信度之间的关键差距。"}}
{"id": "2509.13386", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13386", "abs": "https://arxiv.org/abs/2509.13386", "authors": ["Hansol Lim", "Minhyeok Im", "Jonathan Boyack", "Jee Won Lee", "Jongseong Brad Choi"], "title": "VEGA: Electric Vehicle Navigation Agent via Physics-Informed Neural Operator and Proximal Policy Optimization", "comment": "This work has been submitted to the 2026 IEEE International\n  Conference on Robotics and Automation (ICRA) for possible publication", "summary": "Demands for software-defined vehicles (SDV) are rising and electric vehicles\n(EVs) are increasingly being equipped with powerful computers. This enables\nonboard AI systems to optimize charge-aware path optimization customized to\nreflect vehicle's current condition and environment. We present VEGA, a\ncharge-aware EV navigation agent that plans over a charger-annotated road graph\nusing Proximal Policy Optimization (PPO) with budgeted A* teacher-student\nguidance under state-of-charge (SoC) feasibility. VEGA consists of two modules.\nFirst, a physics-informed neural operator (PINO), trained on real vehicle speed\nand battery-power logs, uses recent vehicle speed logs to estimate aerodynamic\ndrag, rolling resistance, mass, motor and regenerative-braking efficiencies,\nand auxiliary load by learning a vehicle-custom dynamics. Second, a\nReinforcement Learning (RL) agent uses these dynamics to optimize a path with\noptimal charging stops and dwell times under SoC constraints. VEGA requires no\nadditional sensors and uses only vehicle speed signals. It may serve as a\nvirtual sensor for power and efficiency to potentially reduce EV cost. In\nevaluation on long routes like San Francisco to New York, VEGA's stops, dwell\ntimes, SoC management, and total travel time closely track Tesla Trip Planner\nwhile being slightly more conservative, presumably due to real vehicle\nconditions such as vehicle parameter drift due to deterioration. Although\ntrained only in U.S. regions, VEGA was able to compute optimal charge-aware\npaths in France and Japan, demonstrating generalizability. It achieves\npractical integration of physics-informed learning and RL for EV eco-routing.", "AI": {"tldr": "VEGA是一种充电感知的电动汽车导航代理，它结合了物理信息神经网络算子（PINO）和强化学习（RL），利用车辆速度信号优化充电站和停留时间，以实现充电感知的路径规划，并在实际路线中表现出与特斯拉旅行规划器相似的性能和良好的泛化能力。", "motivation": "软件定义汽车（SDV）的需求不断增长，电动汽车（EV）正配备越来越强大的计算机。这使得车载AI系统能够根据车辆当前状况和环境，优化考虑充电的路径规划。研究旨在开发一种能够实现这种定制化优化功能的系统。", "method": "VEGA系统采用带有预算A*教师-学生指导的近端策略优化（PPO）算法，在充电器标注的路网图上进行规划，并考虑荷电状态（SoC）的可行性。它包含两个模块：1. 物理信息神经网络算子（PINO）：通过真实车辆速度和电池功率日志训练，利用近期车辆速度日志估计空气动力阻力、滚动阻力、质量、电机和再生制动效率以及辅助负载，学习车辆定制的动力学。2. 强化学习（RL）代理：利用PINO提供的动力学信息，在SoC约束下优化路径，包括最佳充电站和停留时间。VEGA仅需车辆速度信号，无需额外传感器。", "result": "在旧金山到纽约等长途路线的评估中，VEGA的停车点、停留时间、SoC管理和总旅行时间与特斯拉旅行规划器非常接近，且略微保守（可能由于实际车辆状况如参数漂移）。尽管仅在美国地区训练，VEGA仍能在法国和日本计算出最佳的充电感知路径，展现了良好的泛化能力。它可能作为功率和效率的虚拟传感器，有望降低电动汽车成本。", "conclusion": "VEGA成功地将物理信息学习与强化学习实际集成到电动汽车的生态路线规划中。它提供了一种通用且实用的解决方案，能够根据车辆的真实动态进行充电感知导航，并优化旅行时间和充电策略。"}}
{"id": "2509.13660", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2509.13660", "abs": "https://arxiv.org/abs/2509.13660", "authors": ["Jingyue Ma", "Zhenming Yu", "Zhengyang Li", "Liang Lin", "Liming Cheng", "Jiayu Di", "Tongshuo Zhang", "Ning Zhan", "Kun Xu"], "title": "Integrated diffractive full-Stokes spectro-polarimetric imaging", "comment": "14 pages, 7 figures", "summary": "Spectro-polarimetric imaging provides multidimensional optical information\nacquisition capabilities, offering significant potential for diverse\napplications. Current spectro-polarimetric imaging systems typically suffer\nfrom large physical footprints, high design complexity, elevated costs, or the\ndrawback of requiring replacement of standard components with polarization\noptics. To address these issues, we propose an integrated diffractive\nfull-Stokes spectro-polarimetric imaging framework that synergistically\ncombines end-to-end designed diffractive polarization spectral element (DPSE)\nwith SPMSA-Net to demonstrate high-performance spectro-polarimetric imaging.\nThe DPSE modulates scene and generates modulated images carrying phase-encoding\nand polarization information. The modulated images are the input of the\nSPMSA-NET for the reconstruction of the spectro-polarimetric data cube. The\nframework achieves an average improvement of 0.78 dB in PSNR and 0.012 in SSIM\nover existing state-of-the-art algorithms. Based on this framework, our\nprototype system can simultaneously capture spectral information (400-700 nm)\nwith 10 nm spectral resolution and full-Stokes parameters (S0,S1,S2,S3).\nMeanwhile, the system provides high spatial resolution of 2252*2252 pixels.\nExperimental results demonstrate that our system achieves high-fidelity\nspectral imaging (over 98.9% fidelity) and precise polarization\ncharacterization, with a compact architecture (modulation component of merely\n2-mm thickness).", "AI": {"tldr": "本文提出了一种集成衍射式全斯托克斯光谱偏振成像框架，通过结合衍射偏振光谱元件（DPSE）和SPMSA-Net，实现了高性能、紧凑且高保真度的光谱偏振成像。", "motivation": "当前光谱偏振成像系统普遍存在体积庞大、设计复杂、成本高昂或需要替换标准组件为偏振光学器件等缺点。", "method": "该研究提出了一种集成衍射式全斯托克斯光谱偏振成像框架。该框架协同结合了端到端设计的衍射偏振光谱元件（DPSE）和SPMSA-Net。DPSE负责调制场景并生成带有相位编码和偏振信息的调制图像，这些图像随后作为SPMSA-Net的输入，用于重建光谱偏振数据立方体。", "result": "该框架在PSNR上比现有最先进算法平均提升0.78 dB，SSIM提升0.012。原型系统能够同时捕获400-700 nm波段、10 nm光谱分辨率的光谱信息和全斯托克斯参数（S0,S1,S2,S3），并提供2252*2252像素的高空间分辨率。实验结果表明，该系统实现了高保真光谱成像（超过98.9%的保真度）和精确的偏振表征，且具有紧凑的架构（调制组件厚度仅2毫米）。", "conclusion": "该研究成功开发了一种紧凑、高性能、高保真度的集成衍射式全斯托克斯光谱偏振成像系统，有效解决了现有系统的局限性，为多维度光学信息获取提供了新的解决方案。"}}
{"id": "2509.13383", "categories": ["cs.SY"], "pdf": "https://arxiv.org/pdf/2509.13383", "abs": "https://arxiv.org/abs/2509.13383", "authors": ["Boliang Lin", "Xiang Li", "Yuxue Gu", "Dishen Lu"], "title": "Location and allocation problem of high-speed train maintenance bases", "comment": null, "summary": "Maintenance bases are crucial for the safe and stable operation of high-speed\ntrains, necessitating significant financial investment for their construction\nand operation. Planning the location and task allocation of these bases in the\nvast high-speed railway network is a complex combinatorial optimization\nproblem. This paper explored the strategic planning of identifying optimal\nlocations for maintenance bases, introducing a bi-level programming model. The\nupper-level objective was to minimize the annualized total cost, including\ninvestment for new or expanding bases and total maintenance costs, while the\nlower-level focused on dispatching high-speed trains to the most suitable base\nfor maintenance tasks, thereby reducing maintenance operation dispatch costs\nunder various investment scenarios. A case study of the Northwest China\nhigh-speed rail network demonstrated the application of this model, and\nincluded the sensitivity analysis reflecting maintenance policy reforms. The\nresults showed that establishing a new base in Hami and expanding Xi'an base\ncould minimize the total annualized cost during the planning period, amounting\nto a total of 2,278.15 million RMB. This paper offers an optimization method\nfor selecting maintenance base locations that ensures reliability and\nefficiency in maintenance work as the number of trains increases in the future.", "AI": {"tldr": "本文提出一个双层规划模型，用于优化高速列车维修基地选址及任务分配，旨在最小化总年度成本，并以中国西北高铁网络为例进行了验证。", "motivation": "高速列车维修基地对列车安全稳定运行至关重要，但其建设和运营需要大量投资。在大规模高铁网络中规划维修基地位置和任务分配是一个复杂的组合优化问题。", "method": "研究采用双层规划模型。上层目标是最小化年化总成本，包括新建或扩建基地的投资成本和总维修成本；下层目标是在不同投资情景下，将高速列车调度到最合适的基地进行维修，以降低维修作业调度成本。", "result": "通过对中国西北高速铁路网络的案例研究，结果显示在哈密新建一个基地并扩建西安基地可以使规划期内的年化总成本最小化，总计22.7815亿元人民币。研究还包括了维修政策改革的敏感性分析。", "conclusion": "本文提供了一种优化方法，用于选择维修基地位置，以确保未来列车数量增加时维修工作的可靠性和效率。"}}
{"id": "2509.13624", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13624", "abs": "https://arxiv.org/abs/2509.13624", "authors": ["Shambhavi Krishna", "Atharva Naik", "Chaitali Agarwal", "Sudharshan Govindan", "Taesung Lee", "Haw-Shiuan Chang"], "title": "Latent Traits and Cross-Task Transfer: Deconstructing Dataset Interactions in LLM Fine-tuning", "comment": "Camera-ready version. Accepted to appear in the proceedings of the\n  14th Joint Conference on Lexical and Computational Semantics (*SEM 2025)", "summary": "Large language models are increasingly deployed across diverse applications.\nThis often includes tasks LLMs have not encountered during training. This\nimplies that enumerating and obtaining the high-quality training data for all\ntasks is infeasible. Thus, we often need to rely on transfer learning using\ndatasets with different characteristics, and anticipate out-of-distribution\nrequests. Motivated by this practical need, we propose an analysis framework,\nbuilding a transfer learning matrix and dimensionality reduction, to dissect\nthese cross-task interactions. We train and analyze 10 models to identify\nlatent abilities (e.g., Reasoning, Sentiment Classification, NLU, Arithmetic)\nand discover the side effects of the transfer learning. Our findings reveal\nthat performance improvements often defy explanations based on surface-level\ndataset similarity or source data quality. Instead, hidden statistical factors\nof the source dataset, such as class distribution and generation length\nproclivities, alongside specific linguistic features, are actually more\ninfluential. This work offers insights into the complex dynamics of transfer\nlearning, paving the way for more predictable and effective LLM adaptation.", "AI": {"tldr": "本文提出一个分析框架，通过迁移学习矩阵和降维来解剖跨任务交互，发现LLM在迁移学习中的性能提升更多受源数据集的隐藏统计因素和语言特征影响，而非表面相似度或数据质量。", "motivation": "大型语言模型（LLMs）部署在多样化应用中，常遇到训练时未曾接触的任务。为所有任务获取高质量训练数据不可行，因此需要依赖迁移学习并应对分布外请求。", "method": "本文提出了一个分析框架，包括构建迁移学习矩阵和使用降维技术来剖析跨任务交互。研究中训练并分析了10个模型，以识别潜在能力（如推理、情感分类、NLU、算术）并发现迁移学习的副作用。", "result": "研究发现，性能提升通常无法用表面数据集相似性或源数据质量来解释。相反，源数据集的隐藏统计因素（如类别分布、生成长度倾向）以及特定的语言特征实际上更具影响力。", "conclusion": "这项工作深入揭示了迁移学习的复杂动态，为实现更可预测和有效的LLM适应铺平了道路。"}}
{"id": "2509.13366", "categories": ["cs.CV", "68U99", "J.2"], "pdf": "https://arxiv.org/pdf/2509.13366", "abs": "https://arxiv.org/abs/2509.13366", "authors": ["Tony Rohe", "Martin Margreiter", "Markus Moertl"], "title": "Parking Space Ground Truth Test Automation by Artificial Intelligence Using Convolutional Neural Networks", "comment": "10 pages, 5 figures", "summary": "This research is part of a study of a real-time, cloud-based on-street\nparking service using crowd-sourced in-vehicle fleet data. The service provides\nreal-time information about available parking spots by classifying\ncrowd-sourced detections observed via ultrasonic sensors. The goal of this\nresearch is to optimize the current parking service quality by analyzing the\nautomation of the existing test process for ground truth tests. Therefore,\nmethods from the field of machine learning, especially image pattern\nrecognition, are applied to enrich the database and substitute human\nengineering work in major areas of the analysis process. After an introduction\ninto the related areas of machine learning, this paper explains the methods and\nimplementations made to achieve a high level of automation, applying\nconvolutional neural networks. Finally, predefined metrics present the\nperformance level achieved, showing a time reduction of human resources up to\n99.58 %. The overall improvements are discussed, summarized, and followed by an\noutlook for future development and potential application of the analysis\nautomation tool.", "AI": {"tldr": "该研究旨在优化实时云端路边停车服务的质量，通过应用卷积神经网络自动化其现有地面真值测试过程，从而显著减少人力资源投入。", "motivation": "现有实时云端路边停车服务依赖众包车载数据，并通过超声波传感器分类停车位信息。为了提高服务质量，需要优化其地面真值测试过程，该过程目前涉及大量人工工程分析工作。", "method": "应用机器学习领域的方法，特别是图像模式识别和卷积神经网络（CNNs），来丰富数据库并替代分析过程中主要领域的人工工程工作，以实现高度自动化。", "result": "通过自动化，实现了高达99.58%的人力资源时间减少，显著提高了测试性能水平。", "conclusion": "分析自动化工具极大地提高了停车服务的测试效率，大幅减少了人力投入，并为未来的发展和潜在应用提供了前景。"}}
{"id": "2509.13339", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13339", "abs": "https://arxiv.org/abs/2509.13339", "authors": ["Ming Jin", "Hyunin Lee"], "title": "Position: AI Safety Must Embrace an Antifragile Perspective", "comment": null, "summary": "This position paper contends that modern AI research must adopt an\nantifragile perspective on safety -- one in which the system's capacity to\nguarantee long-term AI safety such as handling rare or out-of-distribution\n(OOD) events expands over time. Conventional static benchmarks and single-shot\nrobustness tests overlook the reality that environments evolve and that models,\nif left unchallenged, can drift into maladaptation (e.g., reward hacking,\nover-optimization, or atrophy of broader capabilities). We argue that an\nantifragile approach -- Rather than striving to rapidly reduce current\nuncertainties, the emphasis is on leveraging those uncertainties to better\nprepare for potentially greater, more unpredictable uncertainties in the future\n-- is pivotal for the long-term reliability of open-ended ML systems. In this\nposition paper, we first identify key limitations of static testing, including\nscenario diversity, reward hacking, and over-alignment. We then explore the\npotential of antifragile solutions to manage rare events. Crucially, we\nadvocate for a fundamental recalibration of the methods used to measure,\nbenchmark, and continually improve AI safety over the long term, complementing\nexisting robustness approaches by providing ethical and practical guidelines\ntowards fostering an antifragile AI safety community.", "AI": {"tldr": "该立场论文主张现代AI安全研究应采纳“反脆弱”视角，以应对不断变化的环境和罕见事件，从而提升AI系统长期安全保障能力，而非依赖静态基准测试。", "motivation": "传统的静态基准测试和单次鲁棒性测试未能充分考虑环境演变和模型漂移（如奖励作弊、过度优化、能力萎缩）的现实，导致AI系统难以应对罕见或分布外事件，无法保证长期安全。", "method": "本文作为一篇立场论文，首先指出了静态测试的关键局限性（如场景多样性不足、奖励作弊、过度对齐）。接着，探讨了反脆弱解决方案在管理罕见事件方面的潜力。最后，提倡对衡量、基准测试和持续改进AI安全的方法进行根本性重新校准，并提供伦理和实践指导，以建立一个反脆弱的AI安全社区。", "result": "论文提出，当前AI安全方法的局限性在于无法处理环境演变和不确定性。通过采纳反脆弱方法，AI系统处理罕见或分布外事件的能力将随时间增强，从而更好地应对未来不可预测的挑战，实现长期可靠性。", "conclusion": "采纳反脆弱的AI安全视角对于开放式机器学习系统的长期可靠性至关重要，这要求我们对衡量和持续改进AI安全的方法进行根本性重新校准，并辅以伦理和实践指导，以构建一个更具适应性的AI安全社区。"}}
{"id": "2509.13434", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13434", "abs": "https://arxiv.org/abs/2509.13434", "authors": ["Wei-Chen Li", "Glen Chou"], "title": "A Convex Formulation of Compliant Contact between Filaments and Rigid Bodies", "comment": null, "summary": "We present a computational framework for simulating filaments interacting\nwith rigid bodies through contact. Filaments are challenging to simulate due to\ntheir codimensionality, i.e., they are one-dimensional structures embedded in\nthree-dimensional space. Existing methods often assume that filaments remain\npermanently attached to rigid bodies. Our framework unifies discrete elastic\nrod (DER) modeling, a pressure field patch contact model, and a convex contact\nformulation to accurately simulate frictional interactions between slender\nfilaments and rigid bodies - capabilities not previously achievable. Owing to\nthe convex formulation of contact, each time step can be solved to global\noptimality, guaranteeing complementarity between contact velocity and impulse.\nWe validate the framework by assessing the accuracy of frictional forces and\ncomparing its physical fidelity against baseline methods. Finally, we\ndemonstrate its applicability in both soft robotics, such as a stochastic\nfilament-based gripper, and deformable object manipulation, such as shoelace\ntying, providing a versatile simulator for systems involving complex\nfilament-filament and filament-rigid body interactions.", "AI": {"tldr": "该论文提出了一个计算框架，用于模拟细丝与刚体之间的摩擦接触，解决了现有方法在处理细丝复杂交互时的局限性。", "motivation": "细丝（一维结构嵌入三维空间）因其协同维度性而难以模拟。现有方法通常假设细丝永久附着在刚体上，无法准确模拟细丝与刚体之间的摩擦相互作用。", "method": "该框架统一了离散弹性杆（DER）建模、压力场补丁接触模型和凸接触公式。由于采用了凸接触公式，每个时间步都能求解到全局最优，保证了接触速度和冲量之间的互补性。", "result": "该框架能够准确模拟细长细丝与刚体之间的摩擦相互作用，这是以前无法实现的能力。通过评估摩擦力精度并与基线方法进行物理保真度比较，验证了该框架的有效性。", "conclusion": "该框架提供了一个多功能的模拟器，适用于涉及复杂细丝-细丝和细丝-刚体相互作用的系统，例如软机器人（如基于细丝的随机抓手）和可变形物体操作（如鞋带系结）。"}}
{"id": "2509.13890", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2509.13890", "abs": "https://arxiv.org/abs/2509.13890", "authors": ["Madhu Koirala", "Pål Gunnar Ellingsen", "Ashenafi Zebene Woldaregay"], "title": "Validation of Dry Bulk Pile Volume Estimation Algorithm based on Angle of Repose using Experimental Images", "comment": null, "summary": "Estimation of volume of piles in shipping ports plays a pivotal role for\nlogistics management, facilitates better ship rescheduling and rerouting for\neconomic benefits and contributes to overall efficient shipping management.\nThis paper presents validation results for a volume estimation algorithm for\ndry bulk cargo piles stored in open ports. Using remote sensing images obtained\nin a laboratory setting, the method first detects the contour of the pile and\nthen reconstructs its 3D model based on the material's angle of repose, and\nestimates the volume accordingly. We validated the algorithm on full conical\npiles and single-ridge elongated piles, and further tested it on reclaimed\nconical and elongated piles. The results demonstrated the algorithm's strong\npotential for accurately estimating pile volume from experimental images and a\nreference satellite image, achieving high accuracy in our validation.", "AI": {"tldr": "本文验证了一种基于遥感图像的干散货堆体积估算算法，该算法通过检测堆体轮廓并结合安息角重建3D模型，在实验图像和卫星图像上均表现出高精度。", "motivation": "港口堆体体积估算对于物流管理、船舶重新调度和改道以及整体航运效率至关重要，能带来经济效益。", "method": "该方法利用实验室环境下的遥感图像，首先检测堆体的轮廓，然后根据材料的安息角重建其3D模型，并据此估算体积。", "result": "该算法在完整锥形堆、单脊细长堆以及回收的锥形和细长堆上进行了验证和测试。结果表明，该算法在从实验图像和参考卫星图像中准确估算堆体体积方面具有强大潜力，并达到了高精度。", "conclusion": "所提出的体积估算算法在各种堆体类型上均表现出高准确性，证明了其在实际港口物流管理中的强大应用潜力。"}}
{"id": "2509.13392", "categories": ["cs.SY", "cs.GT"], "pdf": "https://arxiv.org/pdf/2509.13392", "abs": "https://arxiv.org/abs/2509.13392", "authors": ["Demyan Yarmoshik", "Igor Ignashin", "Ekaterina Sikacheva", "Alexander Gasnikov"], "title": "Modeling skiers flows via Wardrope equilibrium in closed capacitated networks", "comment": null, "summary": "We propose an equilibrium model of ski resorts where users are assigned to\ncycles in a closed network. As queues form on lifts with limited capacity, we\nderive an efficient way to find waiting times via convex optimization. The\nequilibrium problem is formulated as a variational inequality, and numerical\nexperiments show that it can be solved using standard algorithms.", "AI": {"tldr": "该论文提出了一个滑雪场均衡模型，通过凸优化有效计算等待时间，并将均衡问题表述为变分不等式，可使用标准算法求解。", "motivation": "滑雪场中缆车容量有限导致排队，需要一种有效的方法来计算等待时间并理解用户在封闭网络中的分配均衡。", "method": "模型将用户分配到封闭网络中的循环，通过凸优化推导并高效计算缆车排队等待时间。均衡问题被公式化为变分不等式，并使用标准算法进行数值求解。", "result": "研究结果表明，可以通过凸优化有效计算等待时间。均衡问题可以被公式化为变分不等式，并通过标准算法成功求解，数值实验验证了其可行性。", "conclusion": "该模型提供了一种在滑雪场有限容量环境下，有效计算等待时间并求解用户分配均衡的方法，为滑雪场运营管理提供了理论基础。"}}
{"id": "2509.13664", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13664", "abs": "https://arxiv.org/abs/2509.13664", "authors": ["Zhuoxuan Zhang", "Jinhao Duan", "Edward Kim", "Kaidi Xu"], "title": "Sparse Neurons Carry Strong Signals of Question Ambiguity in LLMs", "comment": "To be appeared in EMNLP 2025 (main)", "summary": "Ambiguity is pervasive in real-world questions, yet large language models\n(LLMs) often respond with confident answers rather than seeking clarification.\nIn this work, we show that question ambiguity is linearly encoded in the\ninternal representations of LLMs and can be both detected and controlled at the\nneuron level. During the model's pre-filling stage, we identify that a small\nnumber of neurons, as few as one, encode question ambiguity information. Probes\ntrained on these Ambiguity-Encoding Neurons (AENs) achieve strong performance\non ambiguity detection and generalize across datasets, outperforming\nprompting-based and representation-based baselines. Layerwise analysis reveals\nthat AENs emerge from shallow layers, suggesting early encoding of ambiguity\nsignals in the model's processing pipeline. Finally, we show that through\nmanipulating AENs, we can control LLM's behavior from direct answering to\nabstention. Our findings reveal that LLMs form compact internal representations\nof question ambiguity, enabling interpretable and controllable behavior.", "AI": {"tldr": "本研究发现大型语言模型（LLMs）在内部表示中线性编码了问题歧义，这些歧义信息由少量神经元（AENs）编码，可以被检测和控制，从而使LLM从直接回答转变为回避。", "motivation": "现实世界中的问题普遍存在歧义，但LLMs通常会给出自信的答案而非寻求澄清，这促使研究者探索LLMs内部如何处理和表示这种歧义。", "method": "研究者在模型预填充阶段识别出编码问题歧义信息的神经元（AENs）。他们训练了基于这些AENs的探针进行歧义检测，并进行了逐层分析。最后，通过操纵这些AENs来控制LLM的行为，使其从直接回答转变为回避。", "result": "研究发现问题歧义在线性编码在LLMs的内部表示中，并且仅少数神经元（甚至一个）就编码了这些歧义信息。基于AENs训练的探针在歧义检测上表现出色，泛化能力强，优于基于提示和基于表示的基线。逐层分析显示AENs在浅层就开始出现。通过操纵AENs，可以控制LLM的行为，使其在直接回答和回避之间切换。", "conclusion": "LLMs形成了关于问题歧义的紧凑内部表示，这使得它们的行为变得可解释和可控，为未来LLM的行为干预提供了新途径。"}}
{"id": "2509.13375", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13375", "abs": "https://arxiv.org/abs/2509.13375", "authors": ["Yuxiao Lee", "Xiaofeng Cao", "Wei Ye", "Jiangchao Yao", "Jingkuan Song", "Heng Tao Shen"], "title": "An Empirical Analysis of VLM-based OOD Detection: Mechanisms, Advantages, and Sensitivity", "comment": null, "summary": "Vision-Language Models (VLMs), such as CLIP, have demonstrated remarkable\nzero-shot out-of-distribution (OOD) detection capabilities, vital for reliable\nAI systems. Despite this promising capability, a comprehensive understanding of\n(1) why they work so effectively, (2) what advantages do they have over\nsingle-modal methods, and (3) how is their behavioral robustness -- remains\nnotably incomplete within the research community. This paper presents a\nsystematic empirical analysis of VLM-based OOD detection using in-distribution\n(ID) and OOD prompts. (1) Mechanisms: We systematically characterize and\nformalize key operational properties within the VLM embedding space that\nfacilitate zero-shot OOD detection. (2) Advantages: We empirically quantify the\nsuperiority of these models over established single-modal approaches,\nattributing this distinct advantage to the VLM's capacity to leverage rich\nsemantic novelty. (3) Sensitivity: We uncovers a significant and previously\nunder-explored asymmetry in their robustness profile: while exhibiting\nresilience to common image noise, these VLM-based methods are highly sensitive\nto prompt phrasing. Our findings contribute a more structured understanding of\nthe strengths and critical vulnerabilities inherent in VLM-based OOD detection,\noffering crucial, empirically-grounded guidance for developing more robust and\nreliable future designs.", "AI": {"tldr": "本文对基于视觉-语言模型（VLM）的零样本分布外（OOD）检测进行了系统实证分析，揭示了其工作机制、相对于单模态方法的优势以及在提示语措辞方面存在的鲁棒性弱点。", "motivation": "尽管像CLIP这样的VLM在零样本OOD检测方面表现出色，对构建可靠AI系统至关重要，但研究界对其有效性原因、相对于单模态方法的优势以及行为鲁棒性缺乏全面理解。", "method": "本文采用系统性实证分析方法，利用分布内（ID）和OOD提示语对基于VLM的OOD检测进行研究。具体包括：1) 系统地描述和形式化VLM嵌入空间中促进零样本OOD检测的关键操作特性。2) 实证量化VLM相对于现有单模态方法的优越性。3) 探索VLM对常见图像噪声和提示语措辞的敏感性。", "result": "1) VLM的有效性归因于其利用丰富语义新颖性的能力。2) VLM在OOD检测方面优于已建立的单模态方法。3) VLM对常见图像噪声表现出韧性，但对提示语措辞高度敏感，存在显著且先前未被充分探索的鲁棒性不对称。", "conclusion": "本研究为理解基于VLM的OOD检测的优势和关键脆弱性提供了更结构化的认识，为未来设计更鲁棒、更可靠的系统提供了重要的经验指导。"}}
{"id": "2509.13341", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13341", "abs": "https://arxiv.org/abs/2509.13341", "authors": ["Ahmet H. Güzel", "Matthew Thomas Jackson", "Jarek Luca Liesen", "Tim Rocktäschel", "Jakob Nicolaus Foerster", "Ilija Bogunovic", "Jack Parker-Holder"], "title": "Imagined Autocurricula", "comment": null, "summary": "Training agents to act in embodied environments typically requires vast\ntraining data or access to accurate simulation, neither of which exists for\nmany cases in the real world. Instead, world models are emerging as an\nalternative leveraging offline, passively collected data, they make it possible\nto generate diverse worlds for training agents in simulation. In this work, we\nharness world models to generate imagined environments to train robust agents\ncapable of generalizing to novel task variations. One of the challenges in\ndoing this is ensuring the agent trains on useful generated data. We thus\npropose a novel approach, IMAC (Imagined Autocurricula), leveraging\nUnsupervised Environment Design (UED), which induces an automatic curriculum\nover generated worlds. In a series of challenging, procedurally generated\nenvironments, we show it is possible to achieve strong transfer performance on\nheld-out environments, having trained only inside a world model learned from a\nnarrower dataset. We believe this opens the path to utilizing larger-scale,\nfoundation world models for generally capable agents.", "AI": {"tldr": "本文提出IMAC（想象自动课程）方法，利用世界模型和无监督环境设计生成想象环境，以训练能够泛化到新任务变体的鲁棒智能体，并在挑战性环境中展示了强大的迁移性能。", "motivation": "在具身环境中训练智能体通常需要大量训练数据或精确模拟，但这些在许多现实世界场景中并不存在。世界模型作为一种替代方案，可以利用离线收集的数据生成多样化的世界用于智能体训练。", "method": "作者利用世界模型生成想象环境来训练鲁棒智能体。为确保智能体在有用的生成数据上训练，他们提出了IMAC（Imagined Autocurricula）方法，该方法利用无监督环境设计（UED）在生成的世界上诱导自动课程。", "result": "在具有挑战性的程序生成环境中，智能体仅在从较窄数据集学习的世界模型中进行训练，却能在未见过的环境中实现强大的迁移性能。", "conclusion": "这项工作为利用大规模基础世界模型来训练通用智能体开辟了道路。"}}
{"id": "2509.13501", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13501", "abs": "https://arxiv.org/abs/2509.13501", "authors": ["Hossein Gholampour", "Logan E. Beaver"], "title": "Trajectory Tracking with Reachability-Guided Quadratic Programming and Freeze-Resume", "comment": null, "summary": "Many robotic systems must follow planned paths yet pause safely and resume\nwhen people or objects intervene. We present an output-space method for systems\nwhose tracked output can be feedback-linearized to a double integrator (e.g.,\nmanipulators). The approach has two parts. Offline, we perform a pre-run\nreachability check to verify that the motion plan respects speed and\nacceleration magnitude limits. Online, we apply a quadratic program to track\nthe motion plan under the same limits. We use a one-step reachability test to\nbound the maximum disturbance the system is capable of rejecting. When the\nstate coincides with the reference path we recover perfect tracking in the\ndeterministic case, and we correct errors using a KKT-inspired weight. We\ndemonstrate that safety stops and unplanned deviations are handled efficiently,\nand the system returns to the motion plan without replanning. We demonstrate\nour system's improved performance over pure pursuit in simulation.", "AI": {"tldr": "本文提出了一种针对可反馈线性化为双积分器的机器人系统（如机械臂）的输出空间方法。该方法结合离线可达性检查和在线二次规划，实现了在速度和加速度限制下的鲁棒路径跟踪，能够高效处理安全停车和意外偏差，且无需重新规划即可返回原路径。", "motivation": "许多机器人系统需要遵循规划好的路径，同时在遇到人员或物体干预时能够安全暂停并恢复，且需遵守速度和加速度限制，避免在中断后进行复杂的重新规划。", "method": "该方法适用于其跟踪输出可反馈线性化为双积分器的系统。它包含两部分：1. 离线阶段：进行预运行可达性检查，以验证运动计划是否符合速度和加速度幅值限制。2. 在线阶段：应用二次规划（QP）在相同限制下跟踪运动计划。此外，使用一步可达性测试来限制系统能够拒绝的最大扰动，并在状态与参考路径一致时恢复完美跟踪，使用KKT启发式权重纠正误差。", "result": "研究表明，该系统能够高效处理安全停车和意外偏差，并且无需重新规划即可返回运动计划。在仿真中，该系统相对于纯跟踪（pure pursuit）方法表现出更好的性能。", "conclusion": "该输出空间方法通过结合离线可达性检查和在线二次规划，为机器人系统提供了一种在严格限制下进行鲁棒路径跟踪的有效方案。它能高效应对各种中断和偏差，无需重新规划即可恢复，并在性能上优于现有方法。"}}
{"id": "2509.13505", "categories": ["eess.SY", "cs.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.13505", "abs": "https://arxiv.org/abs/2509.13505", "authors": ["Jaidev Gill", "Jing Shuang Li"], "title": "Identifying Network Structure of Nonlinear Dynamical Systems: Contraction and Kuramoto Oscillators", "comment": "7 pages, 4 figures, in submission", "summary": "In this work, we study the identifiability of network topologies for\nnetworked nonlinear systems when partial measurements of the nodes are taken.\nWe explore scenarios where different candidate topologies can yield similar\nmeasurements, thus limiting identifiability. To do so, we apply the contraction\ntheory framework to facilitate comparisons between candidate topologies. We\nshow that semicontraction in the observable space is a sufficient condition for\ntwo systems to become indistinguishable from one another based on partial\nmeasurements. We apply this framework to study networks of Kuramoto\noscillators, and discuss scenarios in which different topologies (both\nconnected and disconnected) become indistinguishable.", "AI": {"tldr": "本文研究在部分测量下，非线性网络系统拓扑结构的可识别性，并利用收缩理论框架分析不同拓扑结构何时变得不可区分。", "motivation": "当仅能获得部分节点测量数据时，不同的候选网络拓扑结构可能产生相似的测量结果，从而限制了拓扑结构的可识别性。", "method": "应用收缩理论框架，特别是可观测空间中的半收缩概念，来比较不同的候选拓扑结构。将此框架应用于Kuramoto振子网络。", "result": "研究表明，可观测空间中的半收缩是两个系统在部分测量下变得不可区分的充分条件。在Kuramoto振子网络的应用中，发现不同拓扑结构（包括连通和非连通）在特定场景下会变得不可区分。", "conclusion": "收缩理论为分析非线性网络系统在部分测量下拓扑结构的可识别性提供了一个有效工具，并揭示了不同拓扑结构何时会因测量相似性而变得不可区分的条件。"}}
{"id": "2509.13672", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13672", "abs": "https://arxiv.org/abs/2509.13672", "authors": ["Shang Qin", "Jingheng Ye", "Yinghui Li", "Hai-Tao Zheng", "Qi Li", "Jinxiao Shan", "Zhixing Li", "Hong-Gee Kim"], "title": "CL$^2$GEC: A Multi-Discipline Benchmark for Continual Learning in Chinese Literature Grammatical Error Correction", "comment": null, "summary": "The growing demand for automated writing assistance in diverse academic\ndomains highlights the need for robust Chinese Grammatical Error Correction\n(CGEC) systems that can adapt across disciplines. However, existing CGEC\nresearch largely lacks dedicated benchmarks for multi-disciplinary academic\nwriting, overlooking continual learning (CL) as a promising solution to handle\ndomain-specific linguistic variation and prevent catastrophic forgetting. To\nfill this crucial gap, we introduce CL$^2$GEC, the first Continual Learning\nbenchmark for Chinese Literature Grammatical Error Correction, designed to\nevaluate adaptive CGEC across multiple academic fields. Our benchmark includes\n10,000 human-annotated sentences spanning 10 disciplines, each exhibiting\ndistinct linguistic styles and error patterns. CL$^2$GEC focuses on evaluating\ngrammatical error correction in a continual learning setting, simulating\nsequential exposure to diverse academic disciplines to reflect real-world\neditorial dynamics. We evaluate large language models under sequential tuning,\nparameter-efficient adaptation, and four representative CL algorithms, using\nboth standard GEC metrics and continual learning metrics adapted to task-level\nvariation. Experimental results reveal that regularization-based methods\nmitigate forgetting more effectively than replay-based or naive sequential\napproaches. Our benchmark provides a rigorous foundation for future research in\nadaptive grammatical error correction across diverse academic domains.", "AI": {"tldr": "本文引入了CL²GEC，首个针对中文文学语法纠错的持续学习基准，旨在评估跨多个学术领域的自适应CGEC系统，并发现基于正则化的方法在缓解遗忘方面更有效。", "motivation": "现有CGEC研究缺乏多学科学术写作的专用基准，忽视了持续学习（CL）作为处理领域特定语言变异和防止灾难性遗忘的解决方案，导致系统难以适应不同学科的需求。", "method": "引入了CL²GEC基准，包含10个学科的10,000个人工标注句子，每个学科具有独特的语言风格和错误模式。该基准用于在持续学习设置下评估语法纠错，模拟顺序接触不同学术领域。研究评估了大型语言模型在顺序微调、参数高效适应和四种代表性CL算法下的表现，使用标准GEC指标和适应任务级别变化的持续学习指标。", "result": "实验结果表明，基于正则化的方法在缓解遗忘方面比基于重放或朴素顺序方法更有效。", "conclusion": "CL²GEC基准为未来在不同学术领域进行自适应语法纠错的研究提供了严格的基础。"}}
{"id": "2509.13385", "categories": ["cs.CV", "cs.DM", "cs.LG", "51K05 (primary) 57-08, 53Z50, 55U10 (secondary)", "G.2.2"], "pdf": "https://arxiv.org/pdf/2509.13385", "abs": "https://arxiv.org/abs/2509.13385", "authors": ["Charlotte Beylier", "Parvaneh Joharinad", "Jürgen Jost", "Nahid Torbati"], "title": "Curvature as a tool for evaluating dimensionality reduction and estimating intrinsic dimension", "comment": "31 pages, 14 figures", "summary": "Utilizing recently developed abstract notions of sectional curvature, we\nintroduce a method for constructing a curvature-based geometric profile of\ndiscrete metric spaces. The curvature concept that we use here captures the\nmetric relations between triples of points and other points. More\nsignificantly, based on this curvature profile, we introduce a quantitative\nmeasure to evaluate the effectiveness of data representations, such as those\nproduced by dimensionality reduction techniques. Furthermore, Our experiments\ndemonstrate that this curvature-based analysis can be employed to estimate the\nintrinsic dimensionality of datasets. We use this to explore the large-scale\ngeometry of empirical networks and to evaluate the effectiveness of\ndimensionality reduction techniques.", "AI": {"tldr": "本文提出了一种基于抽象截面曲率的离散度量空间几何剖面构建方法，并利用该剖面定量评估数据表示的有效性和估计数据集的内在维度。", "motivation": "研究旨在为离散度量空间提供一种曲率驱动的几何分析方法，并在此基础上开发一种定量评估数据表示（如降维技术产生的结果）有效性的工具，同时估计数据集的内在维度。", "method": "利用新开发的抽象截面曲率概念（捕捉三点组与其他点之间的度量关系），构建离散度量空间的曲率几何剖面。基于此剖面，引入一个定量度量来评估数据表示的有效性。", "result": "该方法成功构建了离散度量空间的曲率几何剖面，并能据此定量评估数据表示的有效性。实验证明，这种基于曲率的分析可用于估计数据集的内在维度，并能探索经验网络的宏观几何结构以及评估降维技术的有效性。", "conclusion": "基于曲率的几何分析是一种有效工具，可用于理解离散度量空间、评估数据表示的质量以及估计数据集的内在维度，在网络几何和降维技术评估中具有实际应用价值。"}}
{"id": "2509.13347", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13347", "abs": "https://arxiv.org/abs/2509.13347", "authors": ["Zihao Wang", "Muyao Li", "Kaichen He", "Xiangyu Wang", "Zhancun Mu", "Anji Liu", "Yitao Liang"], "title": "OpenHA: A Series of Open-Source Hierarchical Agentic Models in Minecraft", "comment": null, "summary": "The choice of action spaces is a critical yet unresolved challenge in\ndeveloping capable, end-to-end trainable agents. This paper first presents a\nlarge-scale, systematic comparison of prominent abstracted action spaces and\ntokenizers for Vision-Language-Action (VLA) or hierarchical agent models in the\nopen-ended Minecraft. Our analysis reveals that no single action space is\nuniversally optimal; instead, the most effective abstraction is highly\ntask-dependent, creating a dilemma for building generalist agents. To resolve\nthis, we introduce Chain of Action (CoA), a novel framework that unifies\nhigh-level planning and low-level control within a single, monolithic VLA\nmodel. CoA treats an abstracted action not as a command for a separate policy,\nbut as an intermediate reasoning step--akin to a chain of thought--that guides\nthe generation of the final, executable action. Furthermore, we demonstrate\nthat an All-in-One agent trained on a diverse mixture of action spaces using\nthe CoA paradigm learns a more robust and generalizable policy. This unified\nagent achieves a new state-of-the-art, improving the overall task success rate\nover strong, specialized baselines. To foster reproducible research, we release\nthe OpenHA (Open Hierarchical Agents) suite, which includes our comprehensive\nbenchmark of over 800 distinct tasks, curated datasets, source code, and all\npretrained model checkpoints at https://github.com/CraftJarvis/OpenHA", "AI": {"tldr": "本文系统比较了Minecraft中VLA代理的抽象动作空间，发现没有普适最优解。为解决此问题，提出了“动作链（CoA）”框架，将高级规划与低级控制统一在一个模型中，使代理学习到更鲁棒、泛化性更强的策略，并达到了新的SOTA。", "motivation": "开发端到端可训练代理时，动作空间的选择是一个关键但未解决的挑战。研究发现没有单一动作空间是普遍最优的，其有效性高度依赖于任务，这给构建通用代理带来了困境。", "method": "首先，对开放式Minecraft中VLA或分层代理模型的突出抽象动作空间和分词器进行了大规模、系统比较。然后，引入了“动作链（CoA）”框架，将抽象动作视为指导最终可执行动作生成的中间推理步骤，从而在一个单一的VLA模型中统一了高级规划和低级控制。最后，在一个多样化的动作空间混合上使用CoA范式训练了一个“一体化”代理。", "result": "分析表明没有单一动作空间是普遍最优的，最有效的抽象高度依赖于任务。引入的CoA范式使得在多样化动作空间上训练的“一体化”代理能够学习到更鲁棒、更具泛化性的策略。该统一代理取得了新的SOTA，在整体任务成功率上超越了强大的专业基线。", "conclusion": "CoA框架通过将抽象动作视为中间推理步骤，成功解决了动作空间选择的任务依赖性困境，并在单一VLA模型中统一了高级规划和低级控制。这使得代理能够学习到更鲁棒和泛化性更强的策略，实现了SOTA性能。为促进可复现研究，本文还发布了OpenHA套件。"}}
{"id": "2509.13534", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13534", "abs": "https://arxiv.org/abs/2509.13534", "authors": ["Chunxin Zheng", "Kai Chen", "Zhihai Bi", "Yulin Li", "Liang Pan", "Jinni Zhou", "Haoang Li", "Jun Ma"], "title": "Embracing Bulky Objects with Humanoid Robots: Whole-Body Manipulation with Reinforcement Learning", "comment": null, "summary": "Whole-body manipulation (WBM) for humanoid robots presents a promising\napproach for executing embracing tasks involving bulky objects, where\ntraditional grasping relying on end-effectors only remains limited in such\nscenarios due to inherent stability and payload constraints. This paper\nintroduces a reinforcement learning framework that integrates a pre-trained\nhuman motion prior with a neural signed distance field (NSDF) representation to\nachieve robust whole-body embracing. Our method leverages a teacher-student\narchitecture to distill large-scale human motion data, generating kinematically\nnatural and physically feasible whole-body motion patterns. This facilitates\ncoordinated control across the arms and torso, enabling stable multi-contact\ninteractions that enhance the robustness in manipulation and also the load\ncapacity. The embedded NSDF further provides accurate and continuous geometric\nperception, improving contact awareness throughout long-horizon tasks. We\nthoroughly evaluate the approach through comprehensive simulations and\nreal-world experiments. The results demonstrate improved adaptability to\ndiverse shapes and sizes of objects and also successful sim-to-real transfer.\nThese indicate that the proposed framework offers an effective and practical\nsolution for multi-contact and long-horizon WBM tasks of humanoid robots.", "AI": {"tldr": "本文提出了一种基于强化学习的全身操作（WBM）框架，结合了预训练的人体运动先验和神经符号距离场（NSDF），以实现人形机器人对大体积物体的鲁棒全身抱持，克服了传统抓取方法的局限性。", "motivation": "传统仅依赖末端执行器的抓取方法在处理大体积物体时，由于固有的稳定性和有效载荷限制而表现不足。全身操作（WBM）为人形机器人执行此类抱持任务提供了一种有前景的方法。", "method": "该方法引入了一个强化学习框架，该框架整合了预训练的人体运动先验（通过教师-学生架构从大规模人体运动数据中提取）和神经符号距离场（NSDF）表示。这促进了手臂和躯干的协调控制，实现了稳定的多接触交互，并提供了准确连续的几何感知，增强了长时间任务中的接触意识。", "result": "通过全面的仿真和实际实验评估，结果表明该方法显著提高了对不同形状和尺寸物体的适应性，并成功实现了从仿真到现实的迁移。这证明了其在鲁棒全身抱持方面的有效性。", "conclusion": "所提出的框架为人形机器人的多接触、长时间全身操作任务提供了一个有效且实用的解决方案，能够实现对大体积物体的稳定抱持。"}}
{"id": "2509.13531", "categories": ["eess.SY", "cs.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.13531", "abs": "https://arxiv.org/abs/2509.13531", "authors": ["Piotr Łaszkiewicz", "Maria Carvalho", "Cláudia Soares", "Pedro Lourenço"], "title": "The impact of modeling approaches on controlling safety-critical, highly perturbed systems: the case for data-driven models", "comment": null, "summary": "This paper evaluates the impact of three system models on the reference\ntrajectory tracking error of the LQR optimal controller, in the challenging\nproblem of guidance and control of the state of a system under strong\nperturbations and reconfiguration. We compared a smooth Linear Time Variant\nsystem learned from data (DD-LTV) with state of the art Linear Time Variant\n(LTV) system identification methods, showing its superiority in the task of\nstate propagation. Moreover, we have found that DD-LTV allows for better\nperformance in terms of trajectory tracking error than the standard solutions\nof a Linear Time Invariant (LTI) system model, and comparable performance to a\nlinearized Linear Time Variant (L-LTV) system model. We tested the three\napproaches on the perturbed and time varying spring-mass-damper systems.", "AI": {"tldr": "本研究评估了三种系统模型（DD-LTV、LTV、LTI、L-LTV）对LQR最优控制器在强扰动和重构下参考轨迹跟踪误差的影响，发现数据驱动的DD-LTV模型在状态传播和轨迹跟踪方面表现优异。", "motivation": "在系统面临强扰动和重构的挑战性问题中，需要有效评估和改进LQR最优控制器的参考轨迹跟踪性能。", "method": "论文比较了三种系统模型对LQR最优控制器轨迹跟踪误差的影响：从数据中学习的平滑线性时变系统（DD-LTV）、现有技术线性时变系统（LTV）以及线性时不变系统（LTI）和线性化线性时变系统（L-LTV）。这些方法在受扰动和时变的弹簧-质量-阻尼器系统上进行了测试。", "result": "研究发现，DD-LTV模型在状态传播任务中优于现有技术LTV系统辨识方法。在轨迹跟踪误差方面，DD-LTV的表现优于标准的LTI系统模型，并与线性化L-LTV系统模型的性能相当。", "conclusion": "数据驱动的平滑线性时变系统（DD-LTV）在强扰动和重构条件下，能够为LQR最优控制器提供更好的轨迹跟踪性能，并在状态传播方面表现出优越性，是解决此类复杂引导和控制问题的有效模型。"}}
{"id": "2509.13677", "categories": ["cs.CL", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.13677", "abs": "https://arxiv.org/abs/2509.13677", "authors": ["Xinxu Zhou", "Jiaqi Bai", "Zhenqi Sun", "Fanxiang Zeng", "Yue Liu"], "title": "AgentCTG: Harnessing Multi-Agent Collaboration for Fine-Grained Precise Control in Text Generation", "comment": null, "summary": "Although significant progress has been made in many tasks within the field of\nNatural Language Processing (NLP), Controlled Text Generation (CTG) continues\nto face numerous challenges, particularly in achieving fine-grained conditional\ncontrol over generation. Additionally, in real scenario and online\napplications, cost considerations, scalability, domain knowledge learning and\nmore precise control are required, presenting more challenge for CTG. This\npaper introduces a novel and scalable framework, AgentCTG, which aims to\nenhance precise and complex control over the text generation by simulating the\ncontrol and regulation mechanisms in multi-agent workflows. We explore various\ncollaboration methods among different agents and introduce an auto-prompt\nmodule to further enhance the generation effectiveness. AgentCTG achieves\nstate-of-the-art results on multiple public datasets. To validate its\neffectiveness in practical applications, we propose a new challenging\nCharacter-Driven Rewriting task, which aims to convert the original text into\nnew text that conform to specific character profiles and simultaneously\npreserve the domain knowledge. When applied to online navigation with\nrole-playing, our approach significantly enhances the driving experience\nthrough improved content delivery. By optimizing the generation of contextually\nrelevant text, we enable a more immersive interaction within online\ncommunities, fostering greater personalization and user engagement.", "AI": {"tldr": "AgentCTG是一个新颖且可扩展的多智能体框架，通过模拟多智能体工作流，旨在实现对文本生成的精确复杂控制。它在多个公共数据集上取得了最先进的结果，并在字符驱动重写和在线导航等实际应用中展现出显著效果。", "motivation": "受控文本生成（CTG）在实现细粒度条件控制方面面临诸多挑战，尤其是在实际场景和在线应用中，还需要考虑成本、可扩展性、领域知识学习和更精确的控制。", "method": "本文提出了AgentCTG框架，通过模拟多智能体工作流中的控制和调节机制，增强文本生成的精确和复杂控制。它探索了不同的智能体协作方法，并引入了一个自动提示模块来进一步提升生成效果。此外，为了验证其在实际应用中的有效性，提出了一项新的字符驱动重写任务。", "result": "AgentCTG在多个公共数据集上取得了最先进（SOTA）的结果。在应用于带有角色扮演的在线导航时，它通过改进内容交付显著提升了驾驶体验。通过优化上下文相关文本的生成，它促进了在线社区中更具沉浸感的互动，增强了个性化和用户参与度。", "conclusion": "AgentCTG通过模拟多智能体工作流，为受控文本生成提供了精确而复杂的控制，不仅在基准测试上表现出色，还在实际应用中，如字符驱动重写和在线导航，显著提升了用户体验和互动效果。"}}
{"id": "2509.13388", "categories": ["cs.CV", "cs.AI", "stat.AP"], "pdf": "https://arxiv.org/pdf/2509.13388", "abs": "https://arxiv.org/abs/2509.13388", "authors": ["Yadvendra Gurjar", "Ruoni Wan", "Ehsan Farahbakhsh", "Rohitash Chandra"], "title": "Landcover classification and change detection using remote sensing and machine learning: a case study of Western Fiji", "comment": null, "summary": "As a developing country, Fiji is facing rapid urbanisation, which is visible\nin the massive development projects that include housing, roads, and civil\nworks. In this study, we present machine learning and remote sensing frameworks\nto compare land use and land cover change from 2013 to 2024 in Nadi, Fiji. The\nultimate goal of this study is to provide technical support in land cover/land\nuse modelling and change detection. We used Landsat-8 satellite image for the\nstudy region and created our training dataset with labels for supervised\nmachine learning. We used Google Earth Engine and unsupervised machine learning\nvia k-means clustering to generate the land cover map. We used convolutional\nneural networks to classify the selected regions' land cover types. We present\na visualisation of change detection, highlighting urban area changes over time\nto monitor changes in the map.", "AI": {"tldr": "本研究利用机器学习和遥感技术，比较了斐济纳迪地区2013年至2024年间的土地利用和土地覆盖变化，旨在为土地覆盖/土地利用建模和变化检测提供技术支持。", "motivation": "斐济作为一个发展中国家，正面临快速城市化，表现为大规模的住房、道路和土木工程开发项目。因此，需要技术支持来监测和建模土地覆盖/土地利用变化。", "method": "研究使用了Landsat-8卫星图像，并创建了带标签的训练数据集用于监督机器学习。通过Google Earth Engine和k-means聚类（无监督机器学习）生成了土地覆盖图。卷积神经网络（CNN）被用于对选定区域的土地覆盖类型进行分类。", "result": "研究展示了变化检测的可视化结果，突出了城市区域随时间的变化，以监测地图上的变化。", "conclusion": "本研究为斐济纳迪地区的土地覆盖/土地利用建模和变化检测提供了技术支持，并实现了对城市区域变化的有效监测。"}}
{"id": "2509.13351", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13351", "abs": "https://arxiv.org/abs/2509.13351", "authors": ["Pulkit Verma", "Ngoc La", "Anthony Favier", "Swaroop Mishra", "Julie A. Shah"], "title": "Teaching LLMs to Plan: Logical Chain-of-Thought Instruction Tuning for Symbolic Planning", "comment": null, "summary": "Large language models (LLMs) have demonstrated impressive capabilities across\ndiverse tasks, yet their ability to perform structured symbolic planning\nremains limited, particularly in domains requiring formal representations like\nthe Planning Domain Definition Language (PDDL). In this paper, we present a\nnovel instruction tuning framework, PDDL-Instruct, designed to enhance LLMs'\nsymbolic planning capabilities through logical chain-of-thought reasoning. Our\napproach focuses on teaching models to rigorously reason about action\napplicability, state transitions, and plan validity using explicit logical\ninference steps. By developing instruction prompts that guide models through\nthe precise logical reasoning required to determine when actions can be applied\nin a given state, we enable LLMs to self-correct their planning processes\nthrough structured reflection. The framework systematically builds verification\nskills by decomposing the planning process into explicit reasoning chains about\nprecondition satisfaction, effect application, and invariant preservation.\nExperimental results on multiple planning domains show that our\nchain-of-thought reasoning based instruction-tuned models are significantly\nbetter at planning, achieving planning accuracy of up to 94% on standard\nbenchmarks, representing a 66% absolute improvement over baseline models. This\nwork bridges the gap between the general reasoning capabilities of LLMs and the\nlogical precision required for automated planning, offering a promising\ndirection for developing better AI planning systems.", "AI": {"tldr": "PDDL-Instruct是一个指令微调框架，通过逻辑思维链推理显著提升了大型语言模型（LLMs）在PDDL等形式化领域中的符号规划能力，规划准确率最高达到94%。", "motivation": "尽管大型语言模型（LLMs）在多种任务中表现出色，但在需要形式化表示（如PDDL）的结构化符号规划领域中，其能力仍受限。", "method": "本文提出了PDDL-Instruct指令微调框架，通过逻辑思维链推理来增强LLMs的符号规划能力。该方法教导模型通过明确的逻辑推理步骤，严谨地推理动作适用性、状态转换和规划有效性，并通过结构化反思进行自我纠正。它将规划过程分解为关于前置条件满足、效果应用和不变量保持的明确推理链。", "result": "在多个规划领域进行的实验结果表明，基于思维链推理进行指令微调的模型在规划方面表现显著更优，在标准基准测试中规划准确率高达94%，比基线模型绝对提升了66%。", "conclusion": "这项工作弥合了LLMs的通用推理能力与自动化规划所需的逻辑精确性之间的差距，为开发更优秀的AI规划系统提供了一个有前景的方向。"}}
{"id": "2509.13541", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13541", "abs": "https://arxiv.org/abs/2509.13541", "authors": ["Ayberk Acar", "Fangjie Li", "Hao Li", "Lidia Al-Zogbi", "Kanyifeechukwu Jane Oguine", "Susheela Sharma Stern", "Jesse F. d'Almeida", "Robert J. Webster III", "Ipek Oguz", "Jie Ying Wu"], "title": "Semantic 3D Reconstructions with SLAM for Central Airway Obstruction", "comment": "5 pages, 2 figures, 1 table", "summary": "Central airway obstruction (CAO) is a life-threatening condition with\nincreasing incidence, caused by tumors in and outside of the airway.\nTraditional treatment methods such as bronchoscopy and electrocautery can be\nused to remove the tumor completely; however, these methods carry a high risk\nof complications. Recent advances allow robotic interventions with lesser risk.\nThe combination of robot interventions with scene understanding and mapping\nalso opens up the possibilities for automation. We present a novel pipeline\nthat enables real-time, semantically informed 3D reconstructions of the central\nairway using monocular endoscopic video.\n  Our approach combines DROID-SLAM with a segmentation model trained to\nidentify obstructive tissues. The SLAM module reconstructs the 3D geometry of\nthe airway in real time, while the segmentation masks guide the annotation of\nobstruction regions within the reconstructed point cloud. To validate our\npipeline, we evaluate the reconstruction quality using ex vivo models.\n  Qualitative and quantitative results show high similarity between ground\ntruth CT scans and the 3D reconstructions (0.62 mm Chamfer distance). By\nintegrating segmentation directly into the SLAM workflow, our system produces\nannotated 3D maps that highlight clinically relevant regions in real time.\nHigh-speed capabilities of the pipeline allows quicker reconstructions compared\nto previous work, reflecting the surgical scene more accurately.\n  To the best of our knowledge, this is the first work to integrate semantic\nsegmentation with real-time monocular SLAM for endoscopic CAO scenarios. Our\nframework is modular and can generalize to other anatomies or procedures with\nminimal changes, offering a promising step toward autonomous robotic\ninterventions.", "AI": {"tldr": "该论文提出了一种利用单目内窥镜视频，结合SLAM和语义分割，实时、语义感知地重建中央气道阻塞三维模型的新方法，以支持自主机器人干预。", "motivation": "中央气道阻塞（CAO）是一种危及生命的疾病，发病率不断上升。传统治疗方法风险高，而机器人干预风险较低，结合场景理解和建图可实现自动化。因此，需要开发一种能提供实时、语义化三维重建的工具来辅助或实现机器人干预。", "method": "该方法提出了一种新颖的管道，将DROID-SLAM与一个用于识别阻塞组织的分割模型相结合。SLAM模块实时重建气道的三维几何结构，而分割掩码则指导重建点云中阻塞区域的标注。该管道通过离体模型评估重建质量。", "result": "定性和定量结果表明，重建的三维模型与地面实况CT扫描具有高度相似性（0.62毫米的Chamfer距离）。该系统通过将分割直接集成到SLAM工作流程中，能够实时生成突出临床相关区域的带注释三维地图。与以往工作相比，该管道具有高速能力，重建速度更快，更准确地反映手术场景。", "conclusion": "这是首次将语义分割与实时单目SLAM集成用于内窥镜CAO场景的工作。该框架具有模块化特性，只需少量修改即可推广到其他解剖结构或手术，为自主机器人干预迈出了有希望的一步。"}}
{"id": "2509.13545", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.13545", "abs": "https://arxiv.org/abs/2509.13545", "authors": ["Sheng Yu", "Boli Chen", "Imad M. Jaimoukha", "Simos A. Evangelou"], "title": "A Game-Theoretic Predictive Control Framework with Statistical Collision Avoidance Constraints for Autonomous Vehicle Overtaking", "comment": null, "summary": "This work develops a control framework for the autonomous overtaking of\nconnected and automated vehicles (CAVs) in a mixed traffic environment, where\nthe overtaken vehicle is an unconnected but interactive human-driven vehicle.\nThe proposed method, termed the Game-Theoretic, PRedictive Overtaking (GT-PRO)\nstrategy, successfully decouples the longitudinal and lateral vehicle dynamics\nof the CAV and comprehensively coordinates these decoupled dynamics via\ninnovative longitudinal and lateral model predictive (MPC) based controllers,\nrespectively. To address the real-time interactive behavior of the human-driven\novertaken vehicle, a dynamic Stackelberg game-based bilevel optimization is\nsolved by the lateral controller to directly control the CAV lateral motion and\npredict the overtaken vehicle longitudinal responses that are subsequently\nshared with a stochastic MPC that governs the CAV longitudinal motion. The\nproposed strategy exploits a comprehensive real-world dataset, which captures\nhuman driver responses when being overtaken, to tune the game-theoretic lateral\ncontroller according to the most common human responses, and to statistically\ncharacterize human uncertainties and hence implement a collision avoidance\nchance constraint for the stochastic longitudinal controller. The simulation\nresults for both polite and aggressive human response case studies of the\novertaken vehicle demonstrate that the proposed GT-PRO can achieve for this\nrange of human driver responsiveness, safer, more efficient, and more\ncomfortable autonomous overtaking, as compared to existing autonomous\novertaking approaches in the literature. Furthermore, the results suggest that\nthe GT-PRO method is capable of real-time implementation.", "AI": {"tldr": "本文提出了一种名为GT-PRO的博弈论预测超车策略，用于在混合交通环境中（包含未联网的人类驾驶车辆）实现互联自动驾驶车辆（CAV）的自主超车，该策略能有效处理人类驾驶员的交互行为，提升超车安全性、效率和舒适性。", "motivation": "在混合交通环境中，互联自动驾驶车辆（CAV）的自主超车面临挑战，尤其是在超车对象是未联网且具有交互行为的人类驾驶车辆时，需要有效应对人类驾驶员的实时反应和不确定性。", "method": "该方法（GT-PRO）将CAV的纵向和横向动力学解耦，并通过创新的基于模型预测控制（MPC）的控制器进行协调。横向控制器通过求解动态Stackelberg博弈（双层优化）来直接控制CAV的横向运动，并预测被超车辆的纵向响应。这些预测结果随后被共享给管理CAV纵向运动的随机MPC控制器，该控制器考虑了碰撞避免的概率约束。此外，该策略利用真实世界数据集来调整博弈论横向控制器，并统计性地表征人类不确定性。", "result": "在礼貌和激进两种人类反应场景下，GT-PRO策略相比现有方法，实现了更安全、更高效、更舒适的自主超车。仿真结果表明，GT-PRO能够应对不同程度的人类驾驶员反应，并具备实时实施的能力。", "conclusion": "GT-PRO策略为在混合交通环境中CAV的自主超车提供了一个鲁棒且高效的控制框架，通过结合博弈论和预测控制，能够有效应对人类驾驶员的实时交互行为和不确定性，从而提升超车性能和安全性。"}}
{"id": "2509.13683", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13683", "abs": "https://arxiv.org/abs/2509.13683", "authors": ["Suyuchen Wang", "Jinlin Wang", "Xinyu Wang", "Shiqi Li", "Xiangru Tang", "Sirui Hong", "Xiao-Wen Chang", "Chenglin Wu", "Bang Liu"], "title": "Improving Context Fidelity via Native Retrieval-Augmented Reasoning", "comment": "Accepted as a main conference paper at EMNLP 2025", "summary": "Large language models (LLMs) often struggle with context fidelity, producing\ninconsistent answers when responding to questions based on provided\ninformation. Existing approaches either rely on expensive supervised\nfine-tuning to generate evidence post-answer or train models to perform web\nsearches without necessarily improving utilization of the given context. We\npropose CARE, a novel native retrieval-augmented reasoning framework that\nteaches LLMs to explicitly integrate in-context evidence within their reasoning\nprocess with the model's own retrieval capabilities. Our method requires\nlimited labeled evidence data while significantly enhancing both retrieval\naccuracy and answer generation performance through strategically retrieved\nin-context tokens in the reasoning chain. Extensive experiments on multiple\nreal-world and counterfactual QA benchmarks demonstrate that our approach\nsubstantially outperforms supervised fine-tuning, traditional\nretrieval-augmented generation methods, and external retrieval solutions. This\nwork represents a fundamental advancement in making LLMs more accurate,\nreliable, and efficient for knowledge-intensive tasks.", "AI": {"tldr": "CARE是一种新型的检索增强推理框架，它通过利用LLM自身的检索能力，将上下文证据显式地整合到推理过程中，显著提高了LLM的上下文忠实度和问答性能。", "motivation": "大型语言模型（LLMs）在基于提供信息回答问题时，常因上下文忠实度不足而产生不一致的答案。现有方法要么依赖昂贵的监督微调来生成证据，要么训练模型进行网络搜索，但未能有效提升对给定上下文的利用。", "method": "本文提出了CARE（Context-Aware Retrieval-augmented Reasoning），一种原生的检索增强推理框架。该方法通过模型自身的检索能力，教会LLMs将上下文证据明确地整合到其推理过程中。它仅需要有限的标注证据数据，并通过在推理链中策略性地检索上下文token来工作。", "result": "CARE显著提升了检索准确性和答案生成性能。在多个真实世界和反事实QA基准测试中，该方法显著优于监督微调、传统检索增强生成方法和外部检索解决方案。", "conclusion": "CARE代表了使LLMs在知识密集型任务中更准确、可靠和高效的基础性进步。"}}
{"id": "2509.13396", "categories": ["cs.CV", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.13396", "abs": "https://arxiv.org/abs/2509.13396", "authors": ["Xinan Wang", "Di Shi", "Fengyu Wang"], "title": "Real-Time Detection and Tracking of Foreign Object Intrusions in Power Systems via Feature-Based Edge Intelligence", "comment": "12 page Journal paper, accepted by IEEE Open Access Journal of Power\n  and Energy", "summary": "This paper presents a novel three-stage framework for real-time foreign\nobject intrusion (FOI) detection and tracking in power transmission systems.\nThe framework integrates: (1) a YOLOv7 segmentation model for fast and robust\nobject localization, (2) a ConvNeXt-based feature extractor trained with\ntriplet loss to generate discriminative embeddings, and (3) a feature-assisted\nIoU tracker that ensures resilient multi-object tracking under occlusion and\nmotion. To enable scalable field deployment, the pipeline is optimized for\ndeployment on low-cost edge hardware using mixed-precision inference. The\nsystem supports incremental updates by adding embeddings from previously unseen\nobjects into a reference database without requiring model retraining. Extensive\nexperiments on real-world surveillance and drone video datasets demonstrate the\nframework's high accuracy and robustness across diverse FOI scenarios. In\naddition, hardware benchmarks on NVIDIA Jetson devices confirm the framework's\npracticality and scalability for real-world edge applications.", "AI": {"tldr": "本文提出一个新颖的三阶段框架，用于电力传输系统中异物入侵 (FOI) 的实时检测和跟踪，该框架针对低成本边缘硬件进行了优化，并支持增量更新。", "motivation": "在电力传输系统中，需要一个快速、鲁棒且可扩展的实时异物入侵检测和跟踪系统。", "method": "该框架包含三个阶段：1) 使用 YOLOv7 分割模型进行快速目标定位；2) 使用经过三重损失训练的 ConvNeXt 特征提取器生成判别性嵌入；3) 使用特征辅助的 IoU 跟踪器，确保在遮挡和运动下实现弹性多目标跟踪。为实现可扩展的现场部署，该管道通过混合精度推理针对低成本边缘硬件进行了优化，并支持无需重新训练模型的增量更新。", "result": "在真实世界监控和无人机视频数据集上进行的广泛实验表明，该框架在各种 FOI 场景中具有高精度和鲁棒性。此外，在 NVIDIA Jetson 设备上的硬件基准测试证实了该框架在实际边缘应用中的实用性和可扩展性。", "conclusion": "所提出的三阶段框架能够有效、鲁棒地在电力传输系统中进行实时异物入侵检测和跟踪，并且在边缘硬件上具有良好的实用性和可扩展性，支持增量更新。"}}
{"id": "2509.13352", "categories": ["cs.AI", "cs.RO", "68T07, 68T40, 68T42", "I.2.9; I.2.11; I.2.8; I.2.10"], "pdf": "https://arxiv.org/pdf/2509.13352", "abs": "https://arxiv.org/abs/2509.13352", "authors": ["Anis Koubaa", "Khaled Gabr"], "title": "Agentic UAVs: LLM-Driven Autonomy with Integrated Tool-Calling and Cognitive Reasoning", "comment": "14 pages, 1 figure", "summary": "Unmanned Aerial Vehicles (UAVs) are increasingly deployed in defense,\nsurveillance, and disaster response, yet most systems remain confined to SAE\nLevel 2--3 autonomy. Their reliance on rule-based control and narrow AI\nrestricts adaptability in dynamic, uncertain missions. Existing UAV frameworks\nlack context-aware reasoning, autonomous decision-making, and ecosystem-level\nintegration; critically, none leverage Large Language Model (LLM) agents with\ntool-calling for real-time knowledge access. This paper introduces the Agentic\nUAVs framework, a five-layer architecture (Perception, Reasoning, Action,\nIntegration, Learning) that augments UAVs with LLM-driven reasoning, database\nquerying, and third-party system interaction. A ROS2 and Gazebo-based prototype\nintegrates YOLOv11 object detection with GPT-4 reasoning and local Gemma-3\ndeployment. In simulated search-and-rescue scenarios, agentic UAVs achieved\nhigher detection confidence (0.79 vs. 0.72), improved person detection rates\n(91% vs. 75%), and markedly increased action recommendation (92% vs. 4.5%).\nThese results confirm that modest computational overhead enables qualitatively\nnew levels of autonomy and ecosystem integration.", "AI": {"tldr": "本文提出Agentic UAVs框架，通过LLM驱动的五层架构（感知、推理、行动、集成、学习）增强无人机自主性，并在模拟搜救中展示了显著提升的检测置信度、人员检测率和行动推荐能力。", "motivation": "现有无人机系统（SAE L2-3）受限于基于规则的控制和狭隘AI，导致在动态不确定任务中适应性差，缺乏上下文感知推理、自主决策和生态系统集成，且未利用LLM代理进行实时知识访问。", "method": "引入Agentic UAVs框架，一个五层架构（感知、推理、行动、集成、学习）。原型基于ROS2和Gazebo，集成了YOLOv11目标检测、GPT-4推理和本地Gemma-3部署，以实现LLM驱动的推理、数据库查询和第三方系统交互。", "result": "在模拟搜救场景中，Agentic UAVs实现了更高的检测置信度（0.79 vs. 0.72）、更高的人员检测率（91% vs. 75%），以及显著提高的行动推荐能力（92% vs. 4.5%）。", "conclusion": "研究结果证实，适度的计算开销能够使无人机实现质变的新级别自主性和生态系统集成。"}}
{"id": "2509.13572", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13572", "abs": "https://arxiv.org/abs/2509.13572", "authors": ["Ozan Karaali", "Hossam Farag", "Strahinja Dosen", "Cedomir Stefanovic"], "title": "Using Visual Language Models to Control Bionic Hands: Assessment of Object Perception and Grasp Inference", "comment": "ICAT 2025", "summary": "This study examines the potential of utilizing Vision Language Models (VLMs)\nto improve the perceptual capabilities of semi-autonomous prosthetic hands. We\nintroduce a unified benchmark for end-to-end perception and grasp inference,\nevaluating a single VLM to perform tasks that traditionally require complex\npipelines with separate modules for object detection, pose estimation, and\ngrasp planning. To establish the feasibility and current limitations of this\napproach, we benchmark eight contemporary VLMs on their ability to perform a\nunified task essential for bionic grasping. From a single static image, they\nshould (1) identify common objects and their key properties (name, shape,\norientation, and dimensions), and (2) infer appropriate grasp parameters (grasp\ntype, wrist rotation, hand aperture, and number of fingers). A corresponding\nprompt requesting a structured JSON output was employed with a dataset of 34\nsnapshots of common objects. Key performance metrics, including accuracy for\ncategorical attributes (e.g., object name, shape) and errors in numerical\nestimates (e.g., dimensions, hand aperture), along with latency and cost, were\nanalyzed. The results demonstrated that most models exhibited high performance\nin object identification and shape recognition, while accuracy in estimating\ndimensions and inferring optimal grasp parameters, particularly hand rotation\nand aperture, varied more significantly. This work highlights the current\ncapabilities and limitations of VLMs as advanced perceptual modules for\nsemi-autonomous control of bionic limbs, demonstrating their potential for\neffective prosthetic applications.", "AI": {"tldr": "本研究评估了视觉语言模型（VLMs）在半自主假肢感知和抓取推断方面的潜力。通过统一基准测试，发现VLMs在物体识别和形状识别上表现出色，但在尺寸估计和最优抓取参数推断上准确性差异较大，揭示了其在假肢应用中的潜力和局限性。", "motivation": "传统半自主假肢的感知能力依赖于复杂的模块化流程（如物体检测、姿态估计、抓取规划）。研究旨在利用VLMs简化这一流程，以端到端的方式提升假肢的感知能力，并评估其可行性及当前局限性。", "method": "研究引入了一个用于端到端感知和抓取推断的统一基准。评估了八个当代VLMs，要求它们从单张静态图像中完成两项统一任务：1) 识别常见物体的关键属性（名称、形状、方向、尺寸）；2) 推断合适的抓取参数（抓取类型、手腕旋转、手部开合度、手指数量）。研究使用包含34张常见物体快照的数据集，并采用请求结构化JSON输出的提示。分析了分类属性的准确性、数值估计的误差以及延迟和成本等关键性能指标。", "result": "大多数模型在物体识别和形状识别方面表现出高水平性能。然而，在估计物体尺寸和推断最优抓取参数（特别是手部旋转和开合度）方面的准确性则有显著差异。", "conclusion": "本研究强调了VLMs作为半自主仿生肢体高级感知模块的当前能力和局限性。结果表明VLMs在有效的假肢应用中具有巨大潜力，但也需进一步改进其在精确数值估计和抓取参数推断方面的表现。"}}
{"id": "2509.13558", "categories": ["eess.SY", "cs.SY", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2509.13558", "abs": "https://arxiv.org/abs/2509.13558", "authors": ["Saad Rahman", "Doyal Sarker", "Tri Ngo", "Roger Bergua", "Daniel Zalkind", "Jason Jonkman", "Tuhin Das"], "title": "Modeling and Verification of Lumped-Parameter, Multibody Structural Dynamics for Offshore Wind Turbines", "comment": null, "summary": "This paper presents the modeling and verification of multibody structural\ndynamics for offshore wind turbines. The flexible tower and support structure\nof a monopile-based offshore wind turbine are modeled using an acausal,\nlumped-parameter, multibody approach that incorporates structural flexibility,\nsoil-structure interaction, and hydrodynamic models. Simulation results are\nbenchmarked against alternative modeling approaches, demonstrating the model's\nability to accurately capture both static and dynamic behaviors under various\nwind and wave conditions while maintaining computational efficiency. This work\nprovides a valuable tool for analyzing key structural characteristics of wind\nturbines, including eigenfrequencies, mode shapes, damping, and internal\nforces.", "AI": {"tldr": "本文提出了一种针对海上风力涡轮机的多体结构动力学建模与验证方法，该方法采用非因果、集总参数的多体方法，能准确高效地捕捉其静态和动态行为。", "motivation": "需要一种能够准确捕捉海上风力涡轮机（包括柔性塔架和支撑结构）结构灵活性、土-结构相互作用和水动力学行为，同时保持计算效率的建模工具。", "method": "采用非因果、集总参数的多体方法对单桩式海上风力涡轮机的柔性塔架和支撑结构进行建模，并整合了结构柔性、土-结构相互作用和水动力学模型。通过与替代建模方法的仿真结果进行对比验证。", "result": "该模型能够准确捕捉各种风浪条件下的静态和动态行为，同时保持计算效率。它为分析风力涡轮机的关键结构特性（包括特征频率、振型、阻尼和内力）提供了一个有价值的工具。", "conclusion": "所开发的模型是一种准确且计算高效的工具，可用于分析海上风力涡轮机的关键结构特性及其在各种环境条件下的行为。"}}
{"id": "2509.13695", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13695", "abs": "https://arxiv.org/abs/2509.13695", "authors": ["Yosuke Mikami", "Daiki Matsuoka", "Hitomi Yanaka"], "title": "Can Large Language Models Robustly Perform Natural Language Inference for Japanese Comparatives?", "comment": "To appear in Proceedings of the 16th International Conference on\n  Computational Semantics (IWCS 2025)", "summary": "Large Language Models (LLMs) perform remarkably well in Natural Language\nInference (NLI). However, NLI involving numerical and logical expressions\nremains challenging. Comparatives are a key linguistic phenomenon related to\nsuch inference, but the robustness of LLMs in handling them, especially in\nlanguages that are not dominant in the models' training data, such as Japanese,\nhas not been sufficiently explored. To address this gap, we construct a\nJapanese NLI dataset that focuses on comparatives and evaluate various LLMs in\nzero-shot and few-shot settings. Our results show that the performance of the\nmodels is sensitive to the prompt formats in the zero-shot setting and\ninfluenced by the gold labels in the few-shot examples. The LLMs also struggle\nto handle linguistic phenomena unique to Japanese. Furthermore, we observe that\nprompts containing logical semantic representations help the models predict the\ncorrect labels for inference problems that they struggle to solve even with\nfew-shot examples.", "AI": {"tldr": "本研究构建了一个日语比较句NLI数据集，评估了LLM在该任务上的表现。结果显示LLM在零样本设置中对提示格式敏感，在少样本设置中受示例标签影响，且难以处理日语特有现象。逻辑语义提示有助于解决困难问题。", "motivation": "尽管大型语言模型（LLMs）在自然语言推理（NLI）方面表现出色，但涉及数值和逻辑表达式的NLI仍然具有挑战性。比较句是此类推理中的关键语言现象，然而，LLMs处理比较句的鲁棒性，特别是在日语等非主流训练数据语言中的表现，尚未得到充分探索。", "method": "为解决这一空白，研究者构建了一个专注于比较句的日语NLI数据集，并在零样本和少样本设置下评估了各种LLMs。", "result": "结果表明，模型在零样本设置中的性能对提示格式敏感，并受到少样本示例中黄金标签的影响。LLMs也难以处理日语特有的语言现象。此外，研究发现包含逻辑语义表示的提示有助于模型预测其即使在少样本示例下也难以解决的推理问题的正确标签。", "conclusion": "LLMs在处理日语比较句NLI方面存在挑战，其性能受提示格式和少样本示例标签的影响，并且难以处理日语特有现象。然而，引入逻辑语义表示的提示可以有效提升模型解决复杂推理问题的能力。"}}
{"id": "2509.13399", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13399", "abs": "https://arxiv.org/abs/2509.13399", "authors": ["Tianyu Chen", "Yasi Zhang", "Zhi Zhang", "Peiyu Yu", "Shu Wang", "Zhendong Wang", "Kevin Lin", "Xiaofei Wang", "Zhengyuan Yang", "Linjie Li", "Chung-Ching Lin", "Jianwen Xie", "Oscar Leong", "Lijuan Wang", "Ying Nian Wu", "Mingyuan Zhou"], "title": "EdiVal-Agent: An Object-Centric Framework for Automated, Scalable, Fine-Grained Evaluation of Multi-Turn Editing", "comment": "Tianyu Chen and Yasi Zhang contributed equally; Oscar Leong, Lijuan\n  Wang, Ying Nian Wu, and Mingyuan Zhou advised equally", "summary": "Instruction-based image editing has advanced rapidly, yet reliable and\ninterpretable evaluation remains a bottleneck. Current protocols either (i)\ndepend on paired reference images -- resulting in limited coverage and\ninheriting biases from prior generative models -- or (ii) rely solely on\nzero-shot vision-language models (VLMs), whose prompt-based assessments of\ninstruction following, content consistency, and visual quality are often\nimprecise.\n  To address this, we introduce EdiVal-Agent, an automated, scalable, and\nfine-grained evaluation framework for multi-turn instruction-based editing from\nan object-centric perspective, supported by a suite of expert tools. Given an\nimage, EdiVal-Agent first decomposes it into semantically meaningful objects,\nthen synthesizes diverse, context-aware editing instructions. For evaluation,\nit integrates VLMs with open-vocabulary object detectors to assess instruction\nfollowing, uses semantic-level feature extractors to evaluate content\nconsistency, and leverages human preference models to judge visual quality. We\nshow that combining VLMs with object detectors yields stronger agreement with\nhuman judgments in instruction-following evaluation compared to using VLMs\nalone and CLIP-based metrics. Furthermore, the pipeline's modular design allows\nfuture tools to be seamlessly integrated, enhancing evaluation accuracy over\ntime.\n  Instantiating this pipeline, we build EdiVal-Bench, a multi-turn editing\nbenchmark covering 9 instruction types and 11 state-of-the-art editing models\nspanning autoregressive (AR) (including Nano Banana, GPT-Image-1),\nflow-matching, and diffusion paradigms. We demonstrate that EdiVal-Agent can be\nused to identify existing failure modes, thereby informing the development of\nthe next generation of editing models. Project page:\nhttps://tianyucodings.github.io/EdiVAL-page/.", "AI": {"tldr": "本文提出EdiVal-Agent，一个自动化、可扩展、细粒度的多轮指令图像编辑评估框架，通过结合VLM和目标检测器等工具，提供更准确、与人类判断更一致的评估，并构建了EdiVal-Bench基准测试。", "motivation": "当前的指令式图像编辑评估方法存在局限性：要么依赖配对参考图像（覆盖范围有限且存在偏差），要么仅依赖零样本视觉-语言模型（VLM）进行不精确的评估，导致可靠和可解释的评估成为瓶颈。", "method": "EdiVal-Agent框架从以对象为中心的角度进行评估。它首先将图像分解为语义对象，然后合成多样、上下文感知的编辑指令。评估时，它将VLM与开放词汇目标检测器结合评估指令遵循度，使用语义级特征提取器评估内容一致性，并利用人类偏好模型评估视觉质量。基于此，他们构建了EdiVal-Bench基准测试，涵盖9种指令类型和11个SOTA编辑模型。", "result": "研究表明，EdiVal-Agent中结合VLM和目标检测器在指令遵循评估方面与人类判断的契合度高于单独使用VLM和基于CLIP的指标。此外，该流水线的模块化设计允许未来工具的无缝集成，提高评估准确性。EdiVal-Agent能够识别现有模型的失败模式，为下一代编辑模型的开发提供信息。", "conclusion": "EdiVal-Agent提供了一个自动化、可扩展且细粒度的图像编辑评估框架，通过结合多种专家工具，解决了现有评估方法的不足。它能更准确地评估指令遵循、内容一致性和视觉质量，并有效识别模型缺陷，从而指导未来编辑模型的发展。"}}
{"id": "2509.13357", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13357", "abs": "https://arxiv.org/abs/2509.13357", "authors": ["Yongchao Huang", "Hassan Raza"], "title": "Semantic Fusion with Fuzzy-Membership Features for Controllable Language Modelling", "comment": "16 pages", "summary": "We propose semantic fusion, a lightweight scheme that augments a Transformer\nlanguage model (LM) with a parallel, fuzzy-membership feature channel that\nencodes token-level semantics. Each token is represented by a vector of\ninterpretable features (e.g. part-of-speech cues, shallow roles, boundary\nflags, sentiment polarity and strength) whose values are graded degrees from\ndifferentiable membership functions (e.g. power kernels). These per-token\nvectors form a sentence-level semantic matrix fused via a gated adapter into\nthe LM. Training uses standard next-token prediction, an auxiliary loss that\nreconstructs the semantic features from hidden states, and a lightweight\nuniformizer that regularizes adjective-class distributions. On a synthetic\ntwo-clause corpus with held-out adjectives for out-of-distribution (OOD)\ncontrol, semantic fusion improves perplexity and enables precise,\nuser-controllable generation of polarity and punctuation while maintaining\nmodel simplicity. This approach adds only small overhead, remains fully\ncompatible with tied input-output embeddings, and provides an interpretable\npathway for conditioned natural language generation.", "AI": {"tldr": "本文提出语义融合（semantic fusion），一种轻量级方案，通过并行的模糊成员特征通道增强Transformer语言模型，以编码词元级语义，从而提高困惑度、实现可控生成并提供可解释性，同时保持模型简洁和低开销。", "motivation": "现有Transformer语言模型在精确、用户可控的生成方面可能存在局限性。研究旨在通过引入可解释的词元级语义特征来增强语言模型，以实现更精细的控制和更好的可解释性。", "method": "该方法通过一个并行的模糊成员特征通道来增强Transformer语言模型，该通道编码词元级语义。每个词元由一个可解释特征（如词性、浅层角色、边界标志、情感极性及强度）向量表示，这些特征值通过可微分成员函数（如幂核）获得分级程度。这些词元向量构成句级语义矩阵，通过门控适配器融合到语言模型中。训练采用标准下一词元预测、一个从隐藏状态重建语义特征的辅助损失，以及一个用于正则化形容词类别分布的轻量级均匀器。", "result": "在一个包含OOD形容词的合成双子句语料库上，语义融合改进了困惑度，并实现了精确、用户可控的极性和标点符号生成，同时保持了模型的简洁性。该方法仅增加了少量开销，与绑定的输入-输出嵌入完全兼容，并为条件自然语言生成提供了可解释的途径。", "conclusion": "语义融合是一种有效、轻量且可解释的方法，能够增强Transformer语言模型，通过引入词元级语义特征，在保持模型简洁和低开销的同时，提高了性能和生成的可控性。"}}
{"id": "2509.13574", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13574", "abs": "https://arxiv.org/abs/2509.13574", "authors": ["Zidong Chen", "Zihao Guo", "Peng Wang", "ThankGod Itua Egbe", "Yan Lyu", "Chenghao Qian"], "title": "Dense-Jump Flow Matching with Non-Uniform Time Scheduling for Robotic Policies: Mitigating Multi-Step Inference Degradation", "comment": null, "summary": "Flow matching has emerged as a competitive framework for learning\nhigh-quality generative policies in robotics; however, we find that\ngeneralisation arises and saturates early along the flow trajectory, in\naccordance with recent findings in the literature. We further observe that\nincreasing the number of Euler integration steps during inference\ncounter-intuitively and universally degrades policy performance. We attribute\nthis to (i) additional, uniformly spaced integration steps oversample the\nlate-time region, thereby constraining actions towards the training\ntrajectories and reducing generalisation; and (ii) the learned velocity field\nbecoming non-Lipschitz as integration time approaches 1, causing instability.\nTo address these issues, we propose a novel policy that utilises non-uniform\ntime scheduling (e.g., U-shaped) during training, which emphasises both early\nand late temporal stages to regularise policy training, and a dense-jump\nintegration schedule at inference, which uses a single-step integration to\nreplace the multi-step integration beyond a jump point, to avoid unstable areas\naround 1. Essentially, our policy is an efficient one-step learner that still\npushes forward performance through multi-step integration, yielding up to 23.7%\nperformance gains over state-of-the-art baselines across diverse robotic tasks.", "AI": {"tldr": "研究发现流匹配（Flow Matching）在机器人策略学习中存在泛化早期饱和和多步推理性能下降的问题，原因在于晚期时间区域过采样和速度场不稳定性。为此，提出了一种在训练中采用非均匀时间调度（如U形）并在推理中采用密集跳跃积分调度的新策略，实现了显著的性能提升。", "motivation": "流匹配在机器人生成策略学习中表现出色，但研究发现其泛化能力早期出现并饱和。此外，在推理过程中增加欧拉积分步数反而普遍降低了策略性能。作者将此归因于：1) 额外的均匀间隔积分步数过度采样了后期时间区域，限制了泛化；2) 随着积分时间接近1，学习到的速度场变得非Lipschitz，导致不稳定性。这些问题促使了本研究。", "method": "为解决上述问题，本文提出了一种新颖的策略：1) 在训练期间采用非均匀时间调度（例如U形），以强调早期和晚期时间阶段，从而规范策略训练；2) 在推理期间采用密集跳跃积分调度，即在跳跃点之后使用单步积分替代多步积分，以避免接近1的不稳定区域。", "result": "提出的策略本质上是一个高效的单步学习器，但通过多步积分进一步提升了性能。在各种机器人任务中，该策略比现有最先进的基线实现了高达23.7%的性能提升。", "conclusion": "本研究通过引入非均匀时间调度训练和密集跳跃积分推理，成功解决了流匹配策略在泛化和稳定性方面的问题。该策略在保持效率的同时显著提高了机器人任务的性能，证明了其在生成式机器人策略学习中的有效性和优越性。"}}
{"id": "2509.13564", "categories": ["eess.SY", "cs.RO", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.13564", "abs": "https://arxiv.org/abs/2509.13564", "authors": ["Arman Pourghorban", "Dipankar Maity"], "title": "Multi-Attacker Single-Defender Target Defense in Conical Environments", "comment": null, "summary": "We consider a variant of the target defense problem in a planar conical\nenvironment where a single defender is tasked to capture a sequence of incoming\nattackers. The attackers' objective is to breach the target boundary without\nbeing captured by the defender. As soon as the current attacker breaches the\ntarget or gets captured by the defender, the next attacker appears at the\nboundary of the environment and moves radially toward the target with maximum\nspeed. Therefore, the defender's final location at the end of the current game\nbecomes its initial location for the next game. The attackers pick strategies\nthat are advantageous for the current as well as for future engagements between\nthe defender and the remaining attackers. The attackers have their own sensors\nwith limited range, using which they can perfectly detect if the defender is\nwithin their sensing range. We derive equilibrium strategies for all the\nplayers to optimize the capture percentage using the notions of capture\ndistribution. Finally, the theoretical results are verified through numerical\nexamples using Monte Carlo type random trials of experiments.", "AI": {"tldr": "本文研究了平面锥形环境中的目标防御问题，其中一个防御者需要捕获一系列入侵的攻击者。攻击者旨在突破目标边界，并考虑当前及未来交战的策略。研究通过捕获分布概念推导了所有参与者的均衡策略，并通过蒙特卡洛随机试验进行了数值验证。", "motivation": "研究动机是解决一种特殊的目标防御问题变体，该问题涉及在平面锥形环境中，一个防御者面对按顺序出现的攻击者。攻击者不仅考虑当前交战，还会考虑对未来交战有利的策略，且具有有限的传感器范围。目标是优化捕获百分比。", "method": "该研究通过使用“捕获分布”的概念来推导所有参与者的均衡策略，以优化捕获百分比。理论结果通过蒙特卡洛类型的随机试验的数值例子进行验证。", "result": "研究得出了所有参与者（防御者和攻击者）的均衡策略，这些策略旨在优化捕获百分比。这些理论结果通过数值示例得到了验证。", "conclusion": "该研究成功地为平面锥形环境中的目标防御问题推导了所有参与者的均衡策略，并优化了捕获百分比，并通过数值验证了这些理论结果。"}}
{"id": "2509.13696", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13696", "abs": "https://arxiv.org/abs/2509.13696", "authors": ["Iyadh Ben Cheikh Larbi", "Ajay Madhavan Ravichandran", "Aljoscha Burchardt", "Roland Roller"], "title": "Integrating Text and Time-Series into (Large) Language Models to Predict Medical Outcomes", "comment": "Presented and published at BioCreative IX", "summary": "Large language models (LLMs) excel at text generation, but their ability to\nhandle clinical classification tasks involving structured data, such as time\nseries, remains underexplored. In this work, we adapt instruction-tuned LLMs\nusing DSPy-based prompt optimization to process clinical notes and structured\nEHR inputs jointly. Our results show that this approach achieves performance on\npar with specialized multimodal systems while requiring less complexity and\noffering greater adaptability across tasks.", "AI": {"tldr": "该研究探索了大型语言模型（LLMs）在处理临床分类任务（结合文本和结构化电子健康记录数据）中的潜力，通过基于DSPy的提示优化，实现了与专业多模态系统相当的性能，且复杂性更低、适应性更强。", "motivation": "大型语言模型在文本生成方面表现出色，但其处理涉及结构化数据（如时间序列）的临床分类任务的能力尚未得到充分探索。", "method": "研究采用基于DSPy的提示优化方法，对经过指令微调的LLMs进行调整，使其能够联合处理临床笔记和结构化电子健康记录（EHR）输入。", "result": "该方法在临床分类任务上取得了与专业多模态系统相当的性能，同时所需的复杂性更低，并且在不同任务之间展现出更大的适应性。", "conclusion": "通过提示优化，LLMs能够有效地整合临床文本和结构化数据进行分类，提供了一种与现有专业多模态系统性能相当、但更简单且适应性更强的解决方案。"}}
{"id": "2509.13414", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13414", "abs": "https://arxiv.org/abs/2509.13414", "authors": ["Nikhil Keetha", "Norman Müller", "Johannes Schönberger", "Lorenzo Porzi", "Yuchen Zhang", "Tobias Fischer", "Arno Knapitsch", "Duncan Zauss", "Ethan Weber", "Nelson Antunes", "Jonathon Luiten", "Manuel Lopez-Antequera", "Samuel Rota Bulò", "Christian Richardt", "Deva Ramanan", "Sebastian Scherer", "Peter Kontschieder"], "title": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction", "comment": "Project Page: https://map-anything.github.io/", "summary": "We introduce MapAnything, a unified transformer-based feed-forward model that\ningests one or more images along with optional geometric inputs such as camera\nintrinsics, poses, depth, or partial reconstructions, and then directly\nregresses the metric 3D scene geometry and cameras. MapAnything leverages a\nfactored representation of multi-view scene geometry, i.e., a collection of\ndepth maps, local ray maps, camera poses, and a metric scale factor that\neffectively upgrades local reconstructions into a globally consistent metric\nframe. Standardizing the supervision and training across diverse datasets,\nalong with flexible input augmentation, enables MapAnything to address a broad\nrange of 3D vision tasks in a single feed-forward pass, including uncalibrated\nstructure-from-motion, calibrated multi-view stereo, monocular depth\nestimation, camera localization, depth completion, and more. We provide\nextensive experimental analyses and model ablations demonstrating that\nMapAnything outperforms or matches specialist feed-forward models while\noffering more efficient joint training behavior, thus paving the way toward a\nuniversal 3D reconstruction backbone.", "AI": {"tldr": "MapAnything是一个统一的基于Transformer的前馈模型，它能从图像和可选的几何输入中直接回归度量3D场景几何和相机，可解决多种3D视觉任务。", "motivation": "现有3D视觉任务通常需要专业的模型。研究动机是开发一个统一的模型，能够高效地在单次前向传播中处理广泛的3D任务，并实现通用3D重建骨干。", "method": "MapAnything是一个统一的基于Transformer的前馈模型。它接收一张或多张图像以及可选的几何输入（如相机内参、位姿、深度或部分重建），然后直接回归度量3D场景几何和相机。该模型利用多视图场景几何的因子化表示，包括深度图、局部射线图、相机位姿和度量尺度因子，将局部重建升级为全局一致的度量框架。通过标准化跨不同数据集的监督和训练，并结合灵活的输入增强，使其能够处理广泛的3D视觉任务。", "result": "MapAnything能够在单次前向传播中解决多种3D视觉任务，包括未校准的运动恢复结构(SfM)、校准的多视图立体(MVS)、单目深度估计、相机定位和深度补全等。实验表明，MapAnything的性能优于或媲美专业的同类前馈模型，同时提供了更高效的联合训练行为。", "conclusion": "MapAnything为构建通用的3D重建骨干铺平了道路，因为它提供了一个统一、高效且性能卓越的模型，能够处理各种3D视觉任务。"}}
{"id": "2509.13364", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13364", "abs": "https://arxiv.org/abs/2509.13364", "authors": ["Zixi Li"], "title": "Asterisk Operator", "comment": "Code available at: https://github.com/lizixi-0x2F/Asterisk-Games", "summary": "We propose the \\textbf{Asterisk Operator} ($\\ast$-operator), a novel unified\nframework for abstract reasoning based on Adjacency-Structured Parallel\nPropagation (ASPP). The operator formalizes structured reasoning tasks as\nlocal, parallel state evolution processes guided by implicit relational graphs.\nWe prove that the $\\ast$-operator maintains local computational constraints\nwhile achieving global reasoning capabilities, providing an efficient and\nconvergent computational paradigm for abstract reasoning problems. Through\nrigorous mathematical analysis and comprehensive experiments on ARC2 challenges\nand Conway's Game of Life, we demonstrate the operator's universality,\nconvergence properties, and superior performance. Our innovative\nEmbedding-Asterisk distillation method achieves 100\\% accuracy on ARC2\nvalidation with only 6M parameters, representing a significant breakthrough in\nneural-symbolic reasoning.\n  \\textbf{Keywords:} Abstract Reasoning, Adjacency Structure, Parallel\nPropagation, Asterisk Operator, Convergence, Universal Approximation", "AI": {"tldr": "解析错误", "motivation": "解析错误", "method": "解析错误", "result": "解析错误", "conclusion": "解析错误"}}
{"id": "2509.13579", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13579", "abs": "https://arxiv.org/abs/2509.13579", "authors": ["Momchil S. Tomov", "Sang Uk Lee", "Hansford Hendrago", "Jinwook Huh", "Teawon Han", "Forbes Howington", "Rafael da Silva", "Gianmarco Bernasconi", "Marc Heim", "Samuel Findler", "Xiaonan Ji", "Alexander Boule", "Michael Napoli", "Kuo Chen", "Jesse Miller", "Boaz Floor", "Yunqing Hu"], "title": "TreeIRL: Safe Urban Driving with Tree Search and Inverse Reinforcement Learning", "comment": null, "summary": "We present TreeIRL, a novel planner for autonomous driving that combines\nMonte Carlo tree search (MCTS) and inverse reinforcement learning (IRL) to\nachieve state-of-the-art performance in simulation and in real-world driving.\nThe core idea is to use MCTS to find a promising set of safe candidate\ntrajectories and a deep IRL scoring function to select the most human-like\namong them. We evaluate TreeIRL against both classical and state-of-the-art\nplanners in large-scale simulations and on 500+ miles of real-world autonomous\ndriving in the Las Vegas metropolitan area. Test scenarios include dense urban\ntraffic, adaptive cruise control, cut-ins, and traffic lights. TreeIRL achieves\nthe best overall performance, striking a balance between safety, progress,\ncomfort, and human-likeness. To our knowledge, our work is the first\ndemonstration of MCTS-based planning on public roads and underscores the\nimportance of evaluating planners across a diverse set of metrics and in\nreal-world environments. TreeIRL is highly extensible and could be further\nimproved with reinforcement learning and imitation learning, providing a\nframework for exploring different combinations of classical and learning-based\napproaches to solve the planning bottleneck in autonomous driving.", "AI": {"tldr": "TreeIRL是一种结合蒙特卡洛树搜索（MCTS）和逆向强化学习（IRL）的自动驾驶规划器，在模拟和实际驾驶中均表现出色，实现了安全性、效率、舒适性和类人性的平衡。", "motivation": "开发一种先进的自动驾驶规划器，能够生成安全且类人性的驾驶行为，解决自动驾驶中的规划瓶颈。", "method": "该方法名为TreeIRL，利用蒙特卡洛树搜索（MCTS）生成一组有前景的安全候选轨迹，并使用一个深度逆向强化学习（IRL）评分函数从这些轨迹中选择最符合人类驾驶习惯的轨迹。", "result": "TreeIRL在大型模拟和拉斯维加斯都会区超过500英里的实际自动驾驶测试（包括密集城市交通、自适应巡航、切入和交通灯等场景）中，相较于传统和最先进的规划器，取得了最佳的综合性能，在安全性、进度、舒适性和类人性之间取得了良好平衡。这是首次在公共道路上展示基于MCTS的规划。", "conclusion": "TreeIRL提供了一个高度可扩展的框架，通过结合MCTS和深度IRL有效解决了自动驾驶规划问题，并强调了在真实世界环境中和多维度指标下评估规划器的重要性。该框架为未来通过强化学习和模仿学习进一步改进提供了基础。"}}
{"id": "2509.13567", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.13567", "abs": "https://arxiv.org/abs/2509.13567", "authors": ["Praveen Verma", "Di Shi", "Yanzhu Ye", "Fengyu Wang", "Ying Zhang"], "title": "Impact of Solar Integration on Grid Security: Unveiling Vulnerabilities in Load Redistribution Attacks", "comment": "Accepted in 16th IEEE PowerTech 2025, Kiel, Germany", "summary": "Load redistribution (LR) attacks represent a practical and sophisticated form\nof false data injection (FDI) attacks, where the attacker manipulates grid data\nto influence economic operations of the grid through misleading security\nconstrained economic dispatch (SCED) decisions. Traditionally, LR attack models\noperate under the assumption that generator measurements are secure and immune\nto tampering. However, the increasing integration of solar generation into\npower grids challenges this assumption, exposing new vulnerabilities. This\npaper proposes an enhanced load redistribution attack model, addressing new\nvulnerabilities introduced by the increasing integration of solar generation in\npower grids. The study demonstrates that manipulating solar generation data\nsignificantly disrupts grid economics, with peak impacts during periods of high\nsolar generation.", "AI": {"tldr": "本文提出了一种增强的负荷再分配攻击模型，考虑了太阳能发电集成带来的新漏洞，并证明了操纵太阳能数据能显著扰乱电网经济。", "motivation": "传统的负荷再分配攻击模型假设发电机测量数据安全，但太阳能发电的日益集成挑战了这一假设，暴露了新的脆弱性，促使研究更全面的攻击模型。", "method": "提出了一种增强的负荷再分配攻击模型，通过操纵太阳能发电数据来利用电网中因太阳能集成而引入的新漏洞。", "result": "研究表明，操纵太阳能发电数据能显著扰乱电网的经济运行，并且在太阳能发电量高的时期影响最为显著。", "conclusion": "太阳能发电的集成引入了新的负荷再分配攻击漏洞，攻击者可以利用这些漏洞通过操纵太阳能数据来影响电网经济，尤其在太阳能高峰期。"}}
{"id": "2509.13702", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13702", "abs": "https://arxiv.org/abs/2509.13702", "authors": ["Xiao Zheng"], "title": "DSCC-HS: A Dynamic Self-Reinforcing Framework for Hallucination Suppression in Large Language Models", "comment": null, "summary": "Large Language Model (LLM) hallucination is a significant barrier to their\nreliable deployment. Current methods like Retrieval-Augmented Generation (RAG)\nare often reactive. We introduce **Dynamic Self-reinforcing Calibration for\nHallucination Suppression (DSCC-HS)**, a novel, proactive framework that\nintervenes during autoregressive decoding. Inspired by dual-process cognitive\ntheory, DSCC-HS uses a compact proxy model, trained in adversarial roles as a\nFactual Alignment Proxy (FAP) and a Hallucination Detection Proxy (HDP). During\ninference, these proxies dynamically steer a large target model by injecting a\nreal-time steering vector, which is the difference between FAP and HDP logits,\nat each decoding step. This plug-and-play approach requires no modification to\nthe target model. Our experiments on TruthfulQA and BioGEN show DSCC-HS\nachieves state-of-the-art performance. On TruthfulQA, it reached a 99.2%\nFactual Consistency Rate (FCR). On the long-form BioGEN benchmark, it attained\nthe highest FActScore of 46.50. These results validate DSCC-HS as a principled\nand efficient solution for enhancing LLM factuality.", "AI": {"tldr": "DSCC-HS是一种新颖的主动式框架，通过在自回归解码过程中使用对抗训练的紧凑代理模型（事实对齐代理FAP和幻觉检测代理HDP）动态引导大型语言模型，从而有效抑制幻觉。", "motivation": "大型语言模型（LLM）的幻觉是其可靠部署的重大障碍。现有的方法（如RAG）通常是被动反应式的，无法在生成过程中主动干预。", "method": "DSCC-HS框架受双过程认知理论启发，使用一个紧凑的代理模型，该模型以对抗角色训练为事实对齐代理（FAP）和幻觉检测代理（HDP）。在推理过程中，这些代理在每个解码步骤动态注入一个实时引导向量（FAP和HDP逻辑值之间的差异），以引导目标大型模型。这种即插即用的方法无需修改目标模型。", "result": "实验结果表明DSCC-HS达到了最先进的性能。在TruthfulQA上，它实现了99.2%的事实一致性率（FCR）。在长文本BioGEN基准测试中，它达到了最高的FActScore 46.50。", "conclusion": "DSCC-HS是一种原则性强且高效的解决方案，能够显著提高大型语言模型的事实性。"}}
{"id": "2509.13474", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13474", "abs": "https://arxiv.org/abs/2509.13474", "authors": ["Yujia Lin", "Nicholas Evans"], "title": "Semantic-Enhanced Cross-Modal Place Recognition for Robust Robot Localization", "comment": null, "summary": "Ensuring accurate localization of robots in environments without GPS\ncapability is a challenging task. Visual Place Recognition (VPR) techniques can\npotentially achieve this goal, but existing RGB-based methods are sensitive to\nchanges in illumination, weather, and other seasonal changes. Existing\ncross-modal localization methods leverage the geometric properties of RGB\nimages and 3D LiDAR maps to reduce the sensitivity issues highlighted above.\nCurrently, state-of-the-art methods struggle in complex scenes, fine-grained or\nhigh-resolution matching, and situations where changes can occur in viewpoint.\nIn this work, we introduce a framework we call Semantic-Enhanced Cross-Modal\nPlace Recognition (SCM-PR) that combines high-level semantics utilizing RGB\nimages for robust localization in LiDAR maps. Our proposed method introduces: a\nVMamba backbone for feature extraction of RGB images; a Semantic-Aware Feature\nFusion (SAFF) module for using both place descriptors and segmentation masks;\nLiDAR descriptors that incorporate both semantics and geometry; and a\ncross-modal semantic attention mechanism in NetVLAD to improve matching.\nIncorporating the semantic information also was instrumental in designing a\nMulti-View Semantic-Geometric Matching and a Semantic Consistency Loss, both in\na contrastive learning framework. Our experimental work on the KITTI and\nKITTI-360 datasets show that SCM-PR achieves state-of-the-art performance\ncompared to other cross-modal place recognition methods.", "AI": {"tldr": "本文提出了一种名为SCM-PR的语义增强跨模态地点识别框架，结合RGB图像的高级语义信息和LiDAR地图，实现了在无GPS环境中鲁棒的机器人定位，解决了现有方法在复杂场景和视角变化下的局限性，并达到了最先进的性能。", "motivation": "在无GPS环境中，机器人准确本地化是一个挑战。现有基于RGB的视觉地点识别（VPR）方法对光照、天气和季节变化敏感。现有的跨模态定位方法虽然利用RGB图像和3D LiDAR地图的几何特性来减少敏感性，但在复杂场景、细粒度或高分辨率匹配以及视角变化时表现不佳。", "method": "本文提出了语义增强跨模态地点识别（SCM-PR）框架。其主要创新包括：使用VMamba作为RGB图像特征提取骨干；引入语义感知特征融合（SAFF）模块，结合地点描述符和分割掩码；设计融合语义和几何信息的LiDAR描述符；在NetVLAD中加入跨模态语义注意力机制以改进匹配。此外，还在对比学习框架中设计了多视角语义几何匹配和语义一致性损失。", "result": "在KITTI和KITTI-360数据集上的实验结果表明，SCM-PR与其他跨模态地点识别方法相比，实现了最先进的性能。", "conclusion": "SCM-PR通过结合高级语义信息，显著提高了在LiDAR地图中利用RGB图像进行鲁棒定位的能力，克服了现有跨模态方法在复杂环境和视角变化下的不足，达到了卓越的定位效果。"}}
{"id": "2509.13368", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13368", "abs": "https://arxiv.org/abs/2509.13368", "authors": ["Yuan Wei", "Xiaohan Shan", "Ran Miao", "Jianmin Li"], "title": "$Agent^2$: An Agent-Generates-Agent Framework for Reinforcement Learning Automation", "comment": "9 pages, 7 figures", "summary": "Reinforcement learning agent development traditionally requires extensive\nexpertise and lengthy iterations, often resulting in high failure rates and\nlimited accessibility. This paper introduces $Agent^2$, a novel\nagent-generates-agent framework that achieves fully automated RL agent design\nthrough intelligent LLM-driven generation. The system autonomously transforms\nnatural language task descriptions and environment code into comprehensive,\nhigh-performance reinforcement learning solutions without human intervention.\n$Agent^2$ features a revolutionary dual-agent architecture. The Generator Agent\nserves as an autonomous AI designer that analyzes tasks and generates\nexecutable RL agents, while the Target Agent is the resulting automatically\ngenerated RL agent. The framework decomposes RL development into two distinct\nstages: MDP modeling and algorithmic optimization, enabling more targeted and\neffective agent generation. Built on the Model Context Protocol, $Agent^2$\nprovides a unified framework that standardizes intelligent agent creation\nacross diverse environments and algorithms, while incorporating adaptive\ntraining management and intelligent feedback analysis for continuous\nimprovement. Extensive experiments on a wide range of benchmarks, including\nMuJoCo, MetaDrive, MPE, and SMAC, demonstrate that $Agent^2$ consistently\noutperforms manually designed solutions across all tasks, achieving up to 55%\nperformance improvement and substantial gains on average. By enabling truly\nend-to-end, closed-loop automation, this work establishes a new paradigm in\nwhich intelligent agents design and optimize other agents, marking a\nfundamental breakthrough for automated AI systems.", "AI": {"tldr": "本文提出了$Agent^2$，一个由大型语言模型（LLM）驱动的“智能体生成智能体”框架，实现了强化学习（RL）智能体的全自动化设计，并在多项基准测试中显著优于手动设计方案。", "motivation": "传统的强化学习智能体开发需要丰富的专业知识和漫长的迭代过程，导致高失败率和有限的可及性。", "method": "$Agent^2$采用双智能体架构：生成器智能体（Generator Agent）作为AI设计师，分析任务并生成可执行的RL智能体；目标智能体（Target Agent）是自动生成的RL智能体。该框架将RL开发分解为MDP建模和算法优化两个阶段，并基于模型上下文协议（Model Context Protocol），整合了自适应训练管理和智能反馈分析，以实现持续改进。", "result": "在MuJoCo、MetaDrive、MPE和SMAC等广泛基准测试中，$Agent^2$在所有任务上均持续优于手动设计方案，实现了高达55%的性能提升和显著的平均收益。", "conclusion": "这项工作通过实现真正端到端、闭环的自动化，确立了智能体设计和优化其他智能体的新范式，标志着自动化AI系统的根本性突破。"}}
{"id": "2509.13591", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13591", "abs": "https://arxiv.org/abs/2509.13591", "authors": ["Amir-Hossein Shahidzadeh", "Jiyue Zhu", "Kezhou Chen", "Sha Yi", "Cornelia Fermüller", "Yiannis Aloimonos", "Xiaolong Wang"], "title": "Object Pose Estimation through Dexterous Touch", "comment": null, "summary": "Robust object pose estimation is essential for manipulation and interaction\ntasks in robotics, particularly in scenarios where visual data is limited or\nsensitive to lighting, occlusions, and appearances. Tactile sensors often offer\nlimited and local contact information, making it challenging to reconstruct the\npose from partial data. Our approach uses sensorimotor exploration to actively\ncontrol a robot hand to interact with the object. We train with Reinforcement\nLearning (RL) to explore and collect tactile data. The collected 3D point\nclouds are used to iteratively refine the object's shape and pose. In our\nsetup, one hand holds the object steady while the other performs active\nexploration. We show that our method can actively explore an object's surface\nto identify critical pose features without prior knowledge of the object's\ngeometry. Supplementary material and more demonstrations will be provided at\nhttps://amirshahid.github.io/BimanualTactilePose .", "AI": {"tldr": "该研究提出了一种基于双臂触觉探索和强化学习的鲁棒物体姿态估计方法，通过主动交互收集触觉数据并迭代优化物体形状和姿态，无需预先了解物体几何信息。", "motivation": "机器人操作和交互任务需要鲁棒的物体姿态估计，但视觉数据往往受光照、遮挡和外观限制。触觉传感器提供的信息有限且局部，难以从部分数据重建姿态。", "method": "该方法利用感觉运动探索，通过强化学习（RL）训练机器人手主动与物体交互以收集触觉数据。收集到的3D点云用于迭代细化物体的形状和姿态。在一个双臂设置中，一只手固定物体，另一只手进行主动探索。", "result": "该方法能够主动探索物体表面，识别关键的姿态特征，且无需预先了解物体的几何信息。", "conclusion": "研究表明，所提出的方法能够通过主动触觉探索有效地进行物体姿态估计，克服了传统视觉和被动触觉方法的局限性，并且不需要预先知道物体几何形状。"}}
{"id": "2509.13585", "categories": ["eess.SY", "cs.GT", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.13585", "abs": "https://arxiv.org/abs/2509.13585", "authors": ["Sean Anderson", "Chris Darken", "João Hespanha"], "title": "Zero-sum turn games using Q-learning: finite computation with security guarantees", "comment": "8 pages", "summary": "This paper addresses zero-sum ``turn'' games, in which only one player can\nmake decisions at each state. We show that pure saddle-point state-feedback\npolicies for turn games can be constructed from dynamic programming fixed-point\nequations for a single value function or Q-function. These fixed-points can be\nconstructed using a suitable form of Q-learning. For discounted costs,\nconvergence of this form of Q-learning can be established using classical\ntechniques. For undiscounted costs, we provide a convergence result that\napplies to finite-time deterministic games, which we use to illustrate our\nresults. For complex games, the Q-learning iteration must be terminated before\nexploring the full-state, which can lead to policies that cannot guarantee the\nsecurity levels implied by the final Q-function. To mitigate this, we propose\nan ``opponent-informed'' exploration policy for selecting the Q-learning\nsamples. This form of exploration can guarantee that the final Q-function\nprovides security levels that hold, at least, against a given set of policies.\nA numerical demonstration for a multi-agent game, Atlatl, indicates the\neffectiveness of these methods.", "AI": {"tldr": "本文提出了一种通过动态规划不动点方程和Q-learning为零和“回合制”游戏构建纯鞍点状态反馈策略的方法。针对复杂游戏中的不完全探索问题，引入了“对手感知”的探索策略，以确保最终Q函数所隐含的安全水平，并在多智能体游戏中进行了有效性验证。", "motivation": "研究动机在于为零和回合制游戏构建纯鞍点状态反馈策略，并解决在复杂游戏中，Q-learning因不完全探索导致最终策略无法保证安全水平的问题。", "method": "本文利用动态规划的不动点方程（针对单一价值函数或Q函数）来构建纯鞍点状态反馈策略。这些不动点通过一种适合的Q-learning形式来构建。为了解决不完全探索的问题，提出了一种“对手感知”的探索策略来选择Q-learning样本。", "result": "研究结果表明，纯鞍点策略可以通过Q-learning从动态规划不动点方程中构建。对于折扣成本，Q-learning的收敛性可以通过经典技术证明；对于非折扣成本，为有限时间确定性游戏提供了收敛性结果。所提出的“对手感知”探索策略能够保证最终Q函数提供的安全水平至少对给定的一组策略有效。在多智能体游戏Atlatl上的数值演示验证了这些方法的有效性。", "conclusion": "本文成功地将Q-learning应用于零和回合制游戏，以构建具有纯鞍点的状态反馈策略。通过引入“对手感知”的探索策略，有效解决了复杂游戏中因不完全探索导致的安全水平无法保证的问题，使得最终策略能够对抗特定对手集提供可靠的安全保障。"}}
{"id": "2509.13706", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13706", "abs": "https://arxiv.org/abs/2509.13706", "authors": ["Peter Beidler", "Mark Nguyen", "Kevin Lybarger", "Ola Holmberg", "Eric Ford", "John Kang"], "title": "Automated Triaging and Transfer Learning of Incident Learning Safety Reports Using Large Language Representational Models", "comment": null, "summary": "PURPOSE: Incident reports are an important tool for safety and quality\nimprovement in healthcare, but manual review is time-consuming and requires\nsubject matter expertise. Here we present a natural language processing (NLP)\nscreening tool to detect high-severity incident reports in radiation oncology\nacross two institutions.\n  METHODS AND MATERIALS: We used two text datasets to train and evaluate our\nNLP models: 7,094 reports from our institution (Inst.), and 571 from IAEA\nSAFRON (SF), all of which had severity scores labeled by clinical content\nexperts. We trained and evaluated two types of models: baseline support vector\nmachines (SVM) and BlueBERT which is a large language model pretrained on\nPubMed abstracts and hospitalized patient data. We assessed for\ngeneralizability of our model in two ways. First, we evaluated models trained\nusing Inst.-train on SF-test. Second, we trained a BlueBERT_TRANSFER model that\nwas first fine-tuned on Inst.-train then on SF-train before testing on SF-test\nset. To further analyze model performance, we also examined a subset of 59\nreports from our Inst. dataset, which were manually edited for clarity.\n  RESULTS Classification performance on the Inst. test achieved AUROC 0.82\nusing SVM and 0.81 using BlueBERT. Without cross-institution transfer learning,\nperformance on the SF test was limited to an AUROC of 0.42 using SVM and 0.56\nusing BlueBERT. BlueBERT_TRANSFER, which was fine-tuned on both datasets,\nimproved the performance on SF test to AUROC 0.78. Performance of SVM, and\nBlueBERT_TRANSFER models on the manually curated Inst. reports (AUROC 0.85 and\n0.74) was similar to human performance (AUROC 0.81).\n  CONCLUSION: In summary, we successfully developed cross-institution NLP\nmodels on incident report text from radiation oncology centers. These models\nwere able to detect high-severity reports similarly to humans on a curated\ndataset.", "AI": {"tldr": "本文开发了一种自然语言处理（NLP）工具，用于在放射肿瘤学领域检测高严重性事件报告，并在跨机构数据集上展示了与人类专家相似的性能。", "motivation": "事件报告是医疗安全和质量改进的重要工具，但人工审查耗时且需要专业知识，因此需要一种自动化方法来筛选高严重性报告。", "method": "研究使用了两个文本数据集：7,094份来自本机构（Inst.）的报告和571份来自IAEA SAFRON（SF）的报告，所有报告均由临床专家标注了严重性分数。研究训练并评估了两种模型：基线支持向量机（SVM）和BlueBERT（一个预训练于PubMed摘要和住院患者数据的大型语言模型）。模型泛化能力通过两种方式评估：1) 使用Inst.-train在SF-test上进行评估；2) 训练BlueBERT_TRANSFER模型，该模型首先在Inst.-train上微调，然后在SF-train上微调，最后在SF-test上测试。此外，还分析了Inst.数据集中59份经过人工编辑以提高清晰度的报告子集。", "result": "在Inst.测试集上，SVM的AUROC为0.82，BlueBERT为0.81。在SF测试集上，未经跨机构迁移学习的模型表现有限，SVM的AUROC为0.42，BlueBERT为0.56。经过两个数据集微调的BlueBERT_TRANSFER模型将SF测试集上的性能提升至AUROC 0.78。在人工整理的Inst.报告上，SVM和BlueBERT_TRANSFER模型的性能（AUROC分别为0.85和0.74）与人类表现（AUROC 0.81）相似。", "conclusion": "研究成功开发了用于放射肿瘤学事件报告文本的跨机构NLP模型。这些模型能够以与人类专家在整理数据集上相似的水平检测高严重性报告。"}}
{"id": "2509.13482", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13482", "abs": "https://arxiv.org/abs/2509.13482", "authors": ["Hao Xu", "Xiaolin Wu", "Xi Zhang"], "title": "Improving 3D Gaussian Splatting Compression by Scene-Adaptive Lattice Vector Quantization", "comment": "Code available at https://github.com/hxu160/SALVQ", "summary": "3D Gaussian Splatting (3DGS) is rapidly gaining popularity for its\nphotorealistic rendering quality and real-time performance, but it generates\nmassive amounts of data. Hence compressing 3DGS data is necessary for the cost\neffectiveness of 3DGS models. Recently, several anchor-based neural compression\nmethods have been proposed, achieving good 3DGS compression performance.\nHowever, they all rely on uniform scalar quantization (USQ) due to its\nsimplicity. A tantalizing question is whether more sophisticated quantizers can\nimprove the current 3DGS compression methods with very little extra overhead\nand minimal change to the system. The answer is yes by replacing USQ with\nlattice vector quantization (LVQ). To better capture scene-specific\ncharacteristics, we optimize the lattice basis for each scene, improving LVQ's\nadaptability and R-D efficiency. This scene-adaptive LVQ (SALVQ) strikes a\nbalance between the R-D efficiency of vector quantization and the low\ncomplexity of USQ. SALVQ can be seamlessly integrated into existing 3DGS\ncompression architectures, enhancing their R-D performance with minimal\nmodifications and computational overhead. Moreover, by scaling the lattice\nbasis vectors, SALVQ can dynamically adjust lattice density, enabling a single\nmodel to accommodate multiple bit rate targets. This flexibility eliminates the\nneed to train separate models for different compression levels, significantly\nreducing training time and memory consumption.", "AI": {"tldr": "本文提出了一种名为场景自适应格点矢量量化（SALVQ）的新方法，用以取代3D高斯泼溅（3DGS）压缩中的均匀标量量化（USQ），显著提升了压缩性能，并实现了单一模型多比特率适应性。", "motivation": "3D高斯泼溅（3DGS）虽然渲染质量高且实时性强，但生成的数据量巨大，需要高效压缩以降低成本。现有的基于锚点的神经压缩方法依赖于简单的均匀标量量化（USQ），存在进一步提升压缩效率的空间。", "method": "本文用格点矢量量化（LVQ）取代了均匀标量量化（USQ），并通过为每个场景优化格点基来提高LVQ的适应性和R-D效率，称之为场景自适应格点矢量量化（SALVQ）。此外，通过缩放格点基向量，SALVQ可以动态调整格点密度，使单个模型能够适应多个比特率目标。", "result": "SALVQ能够无缝集成到现有3DGS压缩架构中，以最小的修改和计算开销显著提升R-D性能。通过动态调整格点密度，SALVQ允许一个单一模型适应不同的比特率目标，从而减少了为不同压缩级别训练单独模型所需的时间和内存。", "conclusion": "SALVQ通过用场景自适应格点矢量量化取代均匀标量量化，在保持低复杂度的同时，有效提升了3DGS数据的压缩性能和R-D效率。其多比特率适应性进一步简化了训练过程，使其成为3DGS压缩的强大且灵活的改进方案。"}}
{"id": "2509.13379", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13379", "abs": "https://arxiv.org/abs/2509.13379", "authors": ["Asif Azad", "Mohammad Sadat Hossain", "MD Sadik Hossain Shanto", "M Saifur Rahman", "Md Rizwan Pervez"], "title": "The Art of Saying \"Maybe\": A Conformal Lens for Uncertainty Benchmarking in VLMs", "comment": null, "summary": "Vision-Language Models (VLMs) have achieved remarkable progress in complex\nvisual understanding across scientific and reasoning tasks. While performance\nbenchmarking has advanced our understanding of these capabilities, the critical\ndimension of uncertainty quantification has received insufficient attention.\nTherefore, unlike prior conformal prediction studies that focused on limited\nsettings, we conduct a comprehensive uncertainty benchmarking study, evaluating\n16 state-of-the-art VLMs (open and closed-source) across 6 multimodal datasets\nwith 3 distinct scoring functions. Our findings demonstrate that larger models\nconsistently exhibit better uncertainty quantification; models that know more\nalso know better what they don't know. More certain models achieve higher\naccuracy, while mathematical and reasoning tasks elicit poorer uncertainty\nperformance across all models compared to other domains. This work establishes\na foundation for reliable uncertainty evaluation in multimodal systems.", "AI": {"tldr": "本文对16个最先进的视觉-语言模型（VLM）在6个多模态数据集上进行了全面的不确定性量化基准测试，发现模型越大不确定性量化能力越好，且在数学和推理任务中表现较差。", "motivation": "尽管视觉-语言模型（VLM）在复杂视觉理解方面取得了显著进展，但其不确定性量化这一关键维度尚未得到充分关注。以往的共形预测研究也仅限于有限设置。", "method": "研究评估了16个最先进的（开源和闭源）VLM，涵盖6个多模态数据集和3种不同的评分函数，进行全面的不确定性基准测试。", "result": "研究发现，模型越大，不确定性量化能力越好，即“懂得多”的模型也“更清楚自己不懂什么”。更确定的模型通常具有更高的准确性。然而，与其它领域相比，数学和推理任务在所有模型中都表现出较差的不确定性量化性能。", "conclusion": "这项工作为多模态系统中的可靠不确定性评估奠定了基础。"}}
{"id": "2509.13595", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13595", "abs": "https://arxiv.org/abs/2509.13595", "authors": ["Xiao Liu", "Weijun Wang", "Tianlun Huang", "Zhiyong Wang", "Wei Feng"], "title": "Leg-Arm Coordinated Operation for Curtain Wall Installation", "comment": null, "summary": "With the acceleration of urbanization, the number of high-rise buildings and\nlarge public facilities is increasing, making curtain walls an essential\ncomponent of modern architecture with widespread applications. Traditional\ncurtain wall installation methods face challenges such as variable on-site\nterrain, high labor intensity, low construction efficiency, and significant\nsafety risks. Large panels often require multiple workers to complete\ninstallation. To address these issues, based on a hexapod curtain wall\ninstallation robot, we design a hierarchical optimization-based whole-body\ncontrol framework for coordinated arm-leg planning tailored to three key tasks:\nwall installation, ceiling installation, and floor laying. This framework\nintegrates the motion of the hexapod legs with the operation of the folding arm\nand the serial-parallel manipulator. We conduct experiments on the hexapod\ncurtain wall installation robot to validate the proposed control method,\ndemonstrating its capability in performing curtain wall installation tasks. Our\nresults confirm the effectiveness of the hierarchical optimization-based\narm-leg coordination framework for the hexapod robot, laying the foundation for\nits further application in complex construction site environments.", "AI": {"tldr": "本文提出了一种基于六足幕墙安装机器人的分层优化全身控制框架，用于协调臂腿规划，以解决传统幕墙安装中的挑战，并通过实验验证了其在幕墙安装任务中的有效性。", "motivation": "随着城市化进程加速，高层建筑和大型公共设施增多，幕墙应用广泛。传统幕墙安装面临现场地形多变、劳动强度大、施工效率低、安全风险高以及大型面板需多人安装等挑战。", "method": "基于六足幕墙安装机器人，设计了一个分层优化全身控制框架，实现臂腿协同规划。该框架整合了六足腿部运动与折叠臂和串并联机械手的操作，并针对墙体、天花板和地板安装三种关键任务进行了定制。", "result": "在六足幕墙安装机器人上进行的实验验证了所提出的控制方法，证明了其执行幕墙安装任务的能力。结果确认了分层优化臂腿协调框架对六足机器人的有效性。", "conclusion": "该研究为六足机器人在复杂施工现场环境中的进一步应用奠定了基础。"}}
{"id": "2509.13674", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.13674", "abs": "https://arxiv.org/abs/2509.13674", "authors": ["Yuezhang He", "Hongxi Luo", "Yuancheng Lin", "Carl J. Talsma", "Anna Li", "Zhenqian Wang", "Yujuan Fang", "Pei Liu", "Jesse D. Jenkins", "Eric Larson", "Zheng Li"], "title": "Scaling green hydrogen and CCUS via cement-methanol co-production in China", "comment": null, "summary": "High costs of green hydrogen and of carbon capture, utilization, and\nsequestration (CCUS) have hindered policy ambition and slowed real-world\ndeployment, despite their importance for decarbonizing hard-to-abate sectors,\nincluding cement and methanol. Given the economic challenges of adopting CCUS\nin cement and green hydrogen in methanol production separately, we propose a\nrenewable-powered co-production system that couples electrolytic hydrogen and\nCCUS through molecule exchange. We optimize system configurations using an\nhourly-resolved, process-based model incorporating operational flexibility, and\nexplore integrated strategies for plant-level deployment and CO2 source-sink\nmatching across China. We find that co-production could reduce CO2 abatement\ncosts to USD 41-53 per tonne by 2035, significantly lower than approximately\nUSD 75 for standalone cement CCUS and over USD 120 for standalone\nrenewable-based methanol. Co-production is preferentially deployed at cement\nplants in renewable-rich regions, potentially reshaping national CO2\ninfrastructure planning. This hydrogen-CCUS coupling paradigm could accelerate\nindustrial decarbonization and scaling for other applications.", "AI": {"tldr": "本研究提出一种可再生能源驱动的氢气与碳捕获、利用与封存（CCUS）联合生产系统，通过分子交换降低了难以减排行业的脱碳成本，显著优于独立系统。", "motivation": "绿色氢能和CCUS的高成本阻碍了水泥和甲醇等难以减排行业的脱碳进程。单独采用CCUS于水泥生产或绿色氢能于甲醇生产都面临经济挑战。", "method": "研究提出一种可再生能源驱动的联合生产系统，通过分子交换将电解氢气与CCUS结合。利用每小时解析的、基于过程的模型进行系统配置优化，并探索了中国范围内的工厂级部署和二氧化碳源汇匹配的综合策略。", "result": "到2035年，联合生产可将二氧化碳减排成本降至每吨41-53美元，远低于独立水泥CCUS的约75美元和独立可再生甲醇的超过120美元。联合生产优先部署在可再生能源丰富的地区的水泥厂，可能重塑国家二氧化碳基础设施规划。", "conclusion": "氢能-CCUS耦合范式可以加速工业脱碳，并为其他应用提供规模化解决方案，具有显著的成本效益和潜在的基础设施影响。"}}
{"id": "2509.13723", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13723", "abs": "https://arxiv.org/abs/2509.13723", "authors": ["Yaxin Gao", "Yao Lu", "Zongfei Zhang", "Jiaqi Nie", "Shanqing Yu", "Qi Xuan"], "title": "DSPC: Dual-Stage Progressive Compression Framework for Efficient Long-Context Reasoning", "comment": null, "summary": "Large language models (LLMs) have achieved remarkable success in many natural\nlanguage processing (NLP) tasks. To achieve more accurate output, the prompts\nused to drive LLMs have become increasingly longer, which incurs higher\ncomputational costs. To address this prompt inflation problem, prompt\ncompression has been proposed. However, most existing methods require training\na small auxiliary model for compression, incurring a significant amount of\nadditional computation. To avoid this, we propose a two-stage, training-free\napproach, called Dual-Stage Progressive Compression (DSPC). In the\ncoarse-grained stage, semantic-related sentence filtering removes sentences\nwith low semantic value based on TF-IDF. In the fine-grained stage, token\nimportance is assessed using attention contribution, cross-model loss\ndifference, and positional importance, enabling the pruning of low-utility\ntokens while preserving semantics. We validate DSPC on LLaMA-3.1-8B-Instruct\nand GPT-3.5-Turbo under a constrained token budget and observe consistent\nimprovements. For instance, in the FewShot task of the Longbench dataset, DSPC\nachieves a performance of 49.17 by using only 3x fewer tokens, outperforming\nthe best state-of-the-art baseline LongLLMLingua by 7.76.", "AI": {"tldr": "本文提出了一种名为DSPC的双阶段、免训练的提示词压缩方法，通过粗粒度句子过滤和细粒度词元剪枝，有效缓解了大型语言模型（LLMs）的提示词膨胀问题，并在保持或提升性能的同时显著减少了计算成本。", "motivation": "随着大型语言模型（LLMs）在自然语言处理任务中取得成功，为了获得更准确的输出，提示词变得越来越长，导致计算成本随之增加。现有的提示词压缩方法大多需要训练一个辅助模型，这又带来了额外的计算开销。因此，研究的动机是寻找一种无需训练的、高效的提示词压缩方案来解决提示词膨胀问题。", "method": "本文提出了一种名为DSPC（Dual-Stage Progressive Compression）的双阶段、免训练方法。在粗粒度阶段，利用TF-IDF进行语义相关句子过滤，移除语义价值低的句子。在细粒度阶段，通过评估注意力贡献、跨模型损失差异和位置重要性来衡量词元的重要性，从而剪枝低效用词元，同时保留语义。", "result": "DSPC在LLaMA-3.1-8B-Instruct和GPT-3.5-Turbo模型上进行了验证，在受限的词元预算下，取得了持续的性能提升。例如，在Longbench数据集的FewShot任务中，DSPC仅使用3倍少的词元就达到了49.17的性能，比最先进的基线模型LongLLMLingua高出7.76。", "conclusion": "DSPC是一种有效的、免训练的双阶段提示词压缩方法，能够显著减少大型语言模型的提示词长度和计算成本，同时在多个任务中保持甚至超越现有基线的性能，成功解决了提示词膨胀问题。"}}
{"id": "2509.13484", "categories": ["cs.CV", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.13484", "abs": "https://arxiv.org/abs/2509.13484", "authors": ["Liu Liu", "Alexandra Kudaeva", "Marco Cipriano", "Fatimeh Al Ghannam", "Freya Tan", "Gerard de Melo", "Andres Sevtsuk"], "title": "MINGLE: VLMs for Semantically Complex Region Detection in Urban Scenes", "comment": "13 pages, 4 figures, under review at AAAI 2026", "summary": "Understanding group-level social interactions in public spaces is crucial for\nurban planning, informing the design of socially vibrant and inclusive\nenvironments. Detecting such interactions from images involves interpreting\nsubtle visual cues such as relations, proximity, and co-movement - semantically\ncomplex signals that go beyond traditional object detection. To address this\nchallenge, we introduce a social group region detection task, which requires\ninferring and spatially grounding visual regions defined by abstract\ninterpersonal relations. We propose MINGLE (Modeling INterpersonal Group-Level\nEngagement), a modular three-stage pipeline that integrates: (1) off-the-shelf\nhuman detection and depth estimation, (2) VLM-based reasoning to classify\npairwise social affiliation, and (3) a lightweight spatial aggregation\nalgorithm to localize socially connected groups. To support this task and\nencourage future research, we present a new dataset of 100K urban street-view\nimages annotated with bounding boxes and labels for both individuals and\nsocially interacting groups. The annotations combine human-created labels and\noutputs from the MINGLE pipeline, ensuring semantic richness and broad coverage\nof real-world scenarios.", "AI": {"tldr": "该研究引入了社交群组区域检测任务，旨在从图像中识别和定位公共空间中的社交互动群组。为此，提出了一个名为MINGLE的三阶段管道，并构建了一个包含10万张城市街景图像的新数据集。", "motivation": "理解公共空间中的群组级社交互动对城市规划至关重要，有助于设计充满活力和包容性的环境。然而，从图像中检测此类互动需要解释复杂且超越传统物体检测的微妙视觉线索（如关系、距离和共同移动）。", "method": "本文提出MINGLE（Modeling INterpersonal Group-Level Engagement）管道，一个模块化的三阶段方法：1) 使用现成的人体检测和深度估计；2) 基于VLM（视觉语言模型）的推理来分类成对的社交关系；3) 轻量级空间聚合算法来定位社交连接的群组。", "result": "引入了社交群组区域检测任务，要求推断和空间定位由抽象人际关系定义的视觉区域。创建了一个包含10万张城市街景图像的新数据集，其中包含个体和社交互动群组的边界框和标签，注释结合了人工创建标签和MINGLE管道的输出。", "conclusion": "该研究通过引入一项新任务、提出MINGLE管道以及构建一个大型、语义丰富的公共空间社交互动数据集，为理解群组级社交互动提供了关键工具和资源，有助于未来的城市规划和环境设计。"}}
{"id": "2509.13389", "categories": ["cs.AI", "I.2.4; I.2.6; I.2.8"], "pdf": "https://arxiv.org/pdf/2509.13389", "abs": "https://arxiv.org/abs/2509.13389", "authors": ["Carlos Núñez-Molina", "Vicenç Gómez", "Hector Geffner"], "title": "From Next Token Prediction to (STRIPS) World Models -- Preliminary Results", "comment": "10 pages, 3 figures", "summary": "We consider the problem of learning propositional STRIPS world models from\naction traces alone, using a deep learning architecture (transformers) and\ngradient descent. The task is cast as a supervised next token prediction\nproblem where the tokens are the actions, and an action $a$ may follow an\naction sequence if the hidden effects of the previous actions do not make an\naction precondition of $a$ false. We show that a suitable transformer\narchitecture can faithfully represent propositional STRIPS world models, and\nthat the models can be learned from sets of random valid (positive) and invalid\n(negative) action sequences alone. A number of experiments are reported.", "AI": {"tldr": "本文探讨了如何仅从动作轨迹中，使用深度学习（Transformer）和梯度下降来学习命题性STRIPS世界模型。", "motivation": "研究动机是仅从动作轨迹中学习命题性STRIPS世界模型。", "method": "该任务被视为一个监督式的下一个token预测问题，其中token是动作。通过Transformer架构和梯度下降，利用随机的有效（正例）和无效（负例）动作序列进行训练。", "result": "研究表明，合适的Transformer架构能够忠实地表示命题性STRIPS世界模型，并且这些模型可以仅从正负动作序列中学习得到。文中报告了多项实验结果。", "conclusion": "Transformer架构能够有效地从动作轨迹中学习并表示命题性STRIPS世界模型。"}}
{"id": "2509.13649", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.13649", "abs": "https://arxiv.org/abs/2509.13649", "authors": ["Méloné Nyoba Tchonkeu", "Soulaimane Berkane", "Tarek Hamel"], "title": "Barometer-Aided Attitude Estimation", "comment": "6 pages, 4 figures. this manuscript is submitted to IEEE Control\n  Systems Letters (L-CSS) with American Control Conference (ACC) option", "summary": "Accurate and robust attitude estimation is a central challenge for autonomous\nvehicles operating in GNSS-denied or highly dynamic environments. In such\ncases, Inertial Measurement Units (IMUs) alone are insufficient for reliable\ntilt estimation due to the ambiguity between gravitational and inertial\naccelerations. While auxiliary velocity sensors, such as GNSS, Pitot tubes,\nDoppler radar, or visual odometry, are often used, they can be unavailable,\nintermittent, or costly. This work introduces a barometer-aided attitude\nestimation architecture that leverages barometric altitude measurements to\ninfer vertical velocity and attitude within a nonlinear observer on SO(3). The\ndesign cascades a deterministic Riccati observer with a complementary filter,\nensuring Almost Global Asymptotic Stability (AGAS) under a uniform\nobservability condition while maintaining geometric consistency. The analysis\nhighlights barometer-aided estimation as a lightweight and effective\ncomplementary modality.", "AI": {"tldr": "本文提出了一种气压计辅助姿态估计算法，通过利用气压高度测量来推断垂直速度和姿态，以解决自主车辆在GNSS拒止或高动态环境下IMU单独姿态估计不足的问题。", "motivation": "在GNSS拒止或高动态环境下，自主车辆的姿态估计面临挑战。单独使用惯性测量单元（IMU）无法可靠地估计倾斜姿态，因为重力加速度和惯性加速度之间存在模糊性。虽然辅助速度传感器（如GNSS、皮托管、多普勒雷达或视觉里程计）常被使用，但它们可能不可用、间歇性或成本高昂。", "method": "该研究引入了一种气压计辅助姿态估计算法。它利用气压高度测量来推断非线性SO(3)观测器内的垂直速度和姿态。设计上，该方法将一个确定性Riccati观测器与一个互补滤波器级联，确保在统一可观测性条件下实现几乎全局渐近稳定性（AGAS），同时保持几何一致性。", "result": "该方法在统一可观测性条件下实现了几乎全局渐近稳定性（AGAS），并保持了几何一致性。分析表明，气压计辅助估计是一种轻量级且有效的补充模式，能够增强姿态估计的鲁棒性。", "conclusion": "气压计辅助姿态估计是一种轻量级且有效的补充模式，能够为自主车辆在GNSS拒止或高动态环境中提供准确和鲁棒的姿态估计，克服了传统IMU和昂贵辅助传感器的局限性。"}}
{"id": "2509.13719", "categories": ["eess.SY", "cond-mat.mtrl-sci", "cs.SY", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2509.13719", "abs": "https://arxiv.org/abs/2509.13719", "authors": ["Chenghao Wan", "Conner Cremers", "Ariana B. Höfelmann", "Zhennan Ru", "Calvin H. Lin", "Kesha N. Tamakuwala", "Dolly Mantle", "Pinak Mohapatra", "Juan Rivas-Davila", "Matthew W. Kanan", "Jonathan A. Fan"], "title": "Scale Up Analysis of Inductively Heated Metamaterial Reactors", "comment": null, "summary": "Inductively heated metamaterial reactors, which utilize an open cell lattice\nbaffle structure as a heating susceptor for magnetic induction, are promising\ncandidates for scaled electrified thermochemical reactor operation due to their\nability to support volumetric heating profiles and enhanced heat transfer\nproperties. In this work, we present a systematic scale up analysis of\ninductive metamaterial reactors where we utilize a combination of analytic\nmodeling, numerical simulations, and experiments to project the capabilities\nand performance of scaled reactors. We use reverse water gas shift as a model\nreaction system and show that for reactor configurations featuring a uniform\nmetamaterial susceptor, the total system efficiency increases with scale.\nHowever, the throughput of these scaled reactors is limited by radial\ntemperature gradients. We further show this bottleneck can be overcome by\ntailoring the radial effective conductivity profile of the susceptor, which can\nenable scaled reactors with nearly ideal plug flow-like capabilities. These\nconcepts provide a pathway towards scaled electrified thermochemical reactors\nwith optimal chemical conversion capabilities.", "AI": {"tldr": "本文系统分析了感应加热超材料反应器的放大过程，发现其总系统效率随规模增大而提高，但受径向温度梯度限制。通过调整径向有效导热系数分布，可克服此瓶颈，实现接近理想活塞流的放大反应器。", "motivation": "感应加热超材料反应器因其支持体积加热和增强传热的特性，在放大电化学热化学反应器操作中具有广阔前景，因此有必要对其放大性能进行系统分析和优化。", "method": "结合解析建模、数值模拟和实验，对感应超材料反应器进行系统性放大分析。以逆水煤气变换反应作为模型反应系统。", "result": "对于具有均匀超材料受热体的反应器配置，总系统效率随规模增大而提高。然而，这些放大反应器的处理能力受径向温度梯度的限制。研究进一步表明，通过调整受热体的径向有效导热系数分布，可以克服这一瓶颈，实现具有接近理想活塞流能力的放大反应器。", "conclusion": "这些概念为实现具有最佳化学转化能力的放大电化学热化学反应器提供了新途径。"}}
{"id": "2509.13734", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13734", "abs": "https://arxiv.org/abs/2509.13734", "authors": ["Yosuke Mikami", "Daiki Matsuoka", "Hitomi Yanaka"], "title": "Implementing a Logical Inference System for Japanese Comparatives", "comment": "In Proceedings of the 5th Workshop on Natural Logic Meets Machine\n  Learning (NALOMA)", "summary": "Natural Language Inference (NLI) involving comparatives is challenging\nbecause it requires understanding quantities and comparative relations\nexpressed by sentences. While some approaches leverage Large Language Models\n(LLMs), we focus on logic-based approaches grounded in compositional semantics,\nwhich are promising for robust handling of numerical and logical expressions.\nPrevious studies along these lines have proposed logical inference systems for\nEnglish comparatives. However, it has been pointed out that there are several\nmorphological and semantic differences between Japanese and English\ncomparatives. These differences make it difficult to apply such systems\ndirectly to Japanese comparatives. To address this gap, this study proposes\nccg-jcomp, a logical inference system for Japanese comparatives based on\ncompositional semantics. We evaluate the proposed system on a Japanese NLI\ndataset containing comparative expressions. We demonstrate the effectiveness of\nour system by comparing its accuracy with that of existing LLMs.", "AI": {"tldr": "本研究提出并评估了ccg-jcomp，一个基于组合语义的日语句子比较级自然语言推理（NLI）逻辑推理系统，并证明其有效性优于现有的大型语言模型。", "motivation": "涉及比较级的NLI具有挑战性，需要理解数量和比较关系。尽管逻辑方法在处理数值和逻辑表达式方面具有潜力，但现有的英文比较级逻辑推理系统无法直接应用于日语，因为日英比较级在形态和语义上存在显著差异。", "method": "本研究提出了一种名为ccg-jcomp的逻辑推理系统，专门针对日语句子比较级，其基础是组合语义。该系统旨在弥补现有英文系统无法直接应用于日语的空白。", "result": "研究在包含比较表达式的日语NLI数据集上评估了所提出的系统，并将其准确性与现有大型语言模型进行了比较，结果证明了ccg-jcomp系统的有效性。", "conclusion": "ccg-jcomp是一个有效且基于组合语义的日语句子比较级逻辑推理系统，能够处理日语特有的形态和语义差异，并在NLI任务中表现出优于现有大型语言模型的准确性。"}}
{"id": "2509.13496", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13496", "abs": "https://arxiv.org/abs/2509.13496", "authors": ["Rajatsubhra Chakraborty", "Xujun Che", "Depeng Xu", "Cori Faklaris", "Xi Niu", "Shuhan Yuan"], "title": "BiasMap: Leveraging Cross-Attentions to Discover and Mitigate Hidden Social Biases in Text-to-Image Generation", "comment": null, "summary": "Bias discovery is critical for black-box generative models, especiall\ntext-to-image (TTI) models. Existing works predominantly focus on output-level\ndemographic distributions, which do not necessarily guarantee concept\nrepresentations to be disentangled post-mitigation. We propose BiasMap, a\nmodel-agnostic framework for uncovering latent concept-level representational\nbiases in stable diffusion models. BiasMap leverages cross-attention\nattribution maps to reveal structural entanglements between demographics (e.g.,\ngender, race) and semantics (e.g., professions), going deeper into\nrepresentational bias during the image generation. Using attribution maps of\nthese concepts, we quantify the spatial demographics-semantics concept\nentanglement via Intersection over Union (IoU), offering a lens into bias that\nremains hidden in existing fairness discovery approaches. In addition, we\nfurther utilize BiasMap for bias mitigation through energy-guided diffusion\nsampling that directly modifies latent noise space and minimizes the expected\nSoftIoU during the denoising process. Our findings show that existing fairness\ninterventions may reduce the output distributional gap but often fail to\ndisentangle concept-level coupling, whereas our mitigation method can mitigate\nconcept entanglement in image generation while complementing distributional\nbias mitigation.", "AI": {"tldr": "BiasMap是一个模型无关的框架，通过交叉注意力归因图发现并量化文本到图像（TTI）模型中潜在的概念级偏见（人口统计学与语义学纠缠），并通过能量引导扩散采样进行缓解。", "motivation": "现有偏见发现工作主要关注输出层面的统计分布，但这并不能保证在缓解后概念表示能够解耦。研究需要深入到图像生成过程中潜在的概念层面，以揭示更深层次的偏见。", "method": "BiasMap框架利用交叉注意力归因图揭示人口统计学（如性别、种族）和语义学（如职业）之间的结构性纠缠。通过计算这些概念的归因图之间的交并比（IoU）来量化空间上的人口统计学-语义学概念纠缠。在偏见缓解方面，BiasMap通过能量引导扩散采样，直接修改潜在噪声空间并在去噪过程中最小化预期的SoftIoU。", "result": "研究发现，现有公平性干预措施可以减少输出分布差距，但往往未能解耦概念层面的耦合。而BiasMap的缓解方法能够在图像生成过程中减轻概念纠缠，并补充了分布偏见的缓解。", "conclusion": "BiasMap提供了一种深入理解和有效缓解文本到图像模型中潜在概念级偏见的新方法，能够揭示现有公平性发现方法中隐藏的偏见，并实现更全面的偏见缓解。"}}
{"id": "2509.13450", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13450", "abs": "https://arxiv.org/abs/2509.13450", "authors": ["Vincent Siu", "Nicholas Crispino", "David Park", "Nathan W. Henry", "Zhun Wang", "Yang Liu", "Dawn Song", "Chenguang Wang"], "title": "SteeringControl: Holistic Evaluation of Alignment Steering in LLMs", "comment": null, "summary": "We introduce SteeringControl, a benchmark for evaluating representation\nsteering methods across core alignment objectives--bias, harmful generation,\nand hallucination--and their effects on secondary behaviors such as sycophancy\nand commonsense morality. While prior alignment work often highlights\ntruthfulness or reasoning ability to demonstrate the side effects of\nrepresentation steering, we find there are many unexplored tradeoffs not yet\nunderstood in a systematic way. We collect a dataset of safety-relevant primary\nand secondary behaviors to evaluate steering effectiveness and behavioral\nentanglement centered around five popular steering methods. To enable this, we\ncraft a modular steering framework based on unique components that serve as the\nbuilding blocks of many existing methods. Our results on Qwen-2.5-7B and\nLlama-3.1-8B find that strong steering performance is dependent on the specific\ncombination of steering method, model, and targeted behavior, and that severe\nconcept entanglement can result from poor combinations of these three as well.\nWe release our code here:\nhttps://github.com/wang-research-lab/SteeringControl.git.", "AI": {"tldr": "本文引入SteeringControl基准，系统评估表征引导方法在核心对齐目标（偏见、有害生成、幻觉）及其对次级行为（奉承、常识道德）影响上的表现和权衡。", "motivation": "先前的对齐工作虽然强调真实性或推理能力来展示表征引导的副作用，但缺乏对多种未探索权衡的系统理解，尤其是在安全相关的初级和次级行为方面。", "method": "引入SteeringControl基准，收集了一个包含安全相关初级和次级行为的数据集。构建了一个模块化的引导框架，该框架由现有方法的独特构建块组成。使用该框架评估了五种流行的引导方法，并在Qwen-2.5-7B和Llama-3.1-8B模型上进行了实验。", "result": "研究发现，强大的引导性能取决于引导方法、模型和目标行为的特定组合。不佳的组合可能导致严重的概念纠缠。", "conclusion": "表征引导的有效性和行为纠缠是复杂且相互依赖的，需要根据具体的引导方法、模型和目标行为进行细致的选择和评估，以避免负面权衡和概念纠缠。"}}
{"id": "2509.13666", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13666", "abs": "https://arxiv.org/abs/2509.13666", "authors": ["Zhenqi Wu", "Abhinav Modi", "Angelos Mavrogiannis", "Kaustubh Joshi", "Nikhil Chopra", "Yiannis Aloimonos", "Nare Karapetyan", "Ioannis Rekleitis", "Xiaomin Lin"], "title": "DREAM: Domain-aware Reasoning for Efficient Autonomous Underwater Monitoring", "comment": "submitted to ICRA 2026", "summary": "The ocean is warming and acidifying, increasing the risk of mass mortality\nevents for temperature-sensitive shellfish such as oysters. This motivates the\ndevelopment of long-term monitoring systems. However, human labor is costly and\nlong-duration underwater work is highly hazardous, thus favoring robotic\nsolutions as a safer and more efficient option. To enable underwater robots to\nmake real-time, environment-aware decisions without human intervention, we must\nequip them with an intelligent \"brain.\" This highlights the need for\npersistent,wide-area, and low-cost benthic monitoring. To this end, we present\nDREAM, a Vision Language Model (VLM)-guided autonomy framework for long-term\nunderwater exploration and habitat monitoring. The results show that our\nframework is highly efficient in finding and exploring target objects (e.g.,\noysters, shipwrecks) without prior location information. In the\noyster-monitoring task, our framework takes 31.5% less time than the previous\nbaseline with the same amount of oysters. Compared to the vanilla VLM, it uses\n23% fewer steps while covering 8.88% more oysters. In shipwreck scenes, our\nframework successfully explores and maps the wreck without collisions,\nrequiring 27.5% fewer steps than the vanilla model and achieving 100% coverage,\nwhile the vanilla model achieves 60.23% average coverage in our shipwreck\nenvironments.", "AI": {"tldr": "本文提出DREAM框架，一个由视觉语言模型（VLM）引导的自主水下机器人系统，用于高效、长期地监测海洋栖息地和目标（如牡蛎、沉船），无需事先位置信息，且优于现有基线和普通VLM。", "motivation": "海洋变暖和酸化增加了牡蛎等温度敏感型贝类大规模死亡的风险，因此需要长期监测系统。然而，人工成本高昂且水下工作危险，促使研究人员寻求更安全高效的机器人解决方案。为了让水下机器人无需人工干预即可做出实时、环境感知的决策，需要为其配备智能“大脑”，从而凸显了对持久、广域、低成本海底监测的需求。", "method": "本文提出了DREAM，一个由视觉语言模型（VLM）引导的自主框架，用于长期水下探索和栖息地监测。该框架旨在使水下机器人能够进行实时、环境感知的决策，无需人类干预。", "result": "DREAM框架在无需事先位置信息的情况下，能高效地发现和探索目标物体（如牡蛎、沉船）。在牡蛎监测任务中，与现有基线相比，DREAM在相同数量的牡蛎监测中节省了31.5%的时间；与普通VLM相比，其步数减少了23%，覆盖的牡蛎数量增加了8.88%。在沉船场景中，DREAM成功探索并绘制了沉船地图，无碰撞发生，所需步数比普通模型减少了27.5%，实现了100%的覆盖率，而普通模型平均覆盖率为60.23%。", "conclusion": "DREAM框架通过VLM引导的自主性，为长期水下探索和栖息地监测提供了一个高效且全面的解决方案，其性能优于先前的基线方法和普通VLM。"}}
{"id": "2509.13793", "categories": ["eess.SY", "cs.LG", "cs.NE", "cs.SY", "math.OC", "65K10, 68T05, 93B30, 93D99"], "pdf": "https://arxiv.org/pdf/2509.13793", "abs": "https://arxiv.org/abs/2509.13793", "authors": ["Thomas Chaffey"], "title": "Circuit realization and hardware linearization of monotone operator equilibrium networks", "comment": null, "summary": "It is shown that the port behavior of a resistor-diode network corresponds to\nthe solution of a ReLU monotone operator equilibrium network (a neural network\nin the limit of infinite depth), giving a parsimonious construction of a neural\nnetwork in analog hardware. We furthermore show that the gradient of such a\ncircuit can be computed directly in hardware, using a procedure we call\nhardware linearization. This allows the network to be trained in hardware,\nwhich we demonstrate with a device-level circuit simulation. We extend the\nresults to cascades of resistor-diode networks, which can be used to implement\nfeedforward and other asymmetric networks. We finally show that different\nnonlinear elements give rise to different activation functions, and introduce\nthe novel diode ReLU which is induced by a non-ideal diode model.", "AI": {"tldr": "电阻-二极管网络可被视为模拟硬件中的神经网络，并通过硬件线性化实现直接在硬件中进行训练。", "motivation": "研究旨在提供一种精简的模拟硬件神经网络构建方法，并实现网络在硬件中的直接梯度计算和训练。", "method": "该研究将电阻-二极管网络的端口行为与ReLU单调算子平衡网络（无限深度极限的神经网络）联系起来。提出“硬件线性化”过程以直接在硬件中计算梯度，并通过器件级电路仿真进行验证。将结果扩展到电阻-二极管网络的级联，以实现前馈及其他非对称网络。此外，探讨了不同非线性元件如何产生不同的激活函数，并引入了基于非理想二极管模型的新型二极管ReLU。", "result": "结果表明，电阻-二极管网络可以构建为模拟硬件中的神经网络。梯度可以直接在硬件中通过硬件线性化计算，并且通过电路仿真验证了硬件训练的可行性。级联网络可以实现前馈和其他非对称网络。不同的非线性元件可以产生不同的激活函数，并提出了一种新的二极管ReLU激活函数。", "conclusion": "电阻-二极管网络提供了一种在模拟硬件中精简构建神经网络的方法，支持直接在硬件中进行梯度计算和训练，并通过选择不同的非线性元件实现灵活的激活函数设计。"}}
{"id": "2509.13775", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13775", "abs": "https://arxiv.org/abs/2509.13775", "authors": ["Vani Kanjirangat", "Ljiljana Dolamic", "Fabio Rinaldi"], "title": "Exploring Data and Parameter Efficient Strategies for Arabic Dialect Identifications", "comment": "4 main pages, 4 additional, 5 figures", "summary": "This paper discusses our exploration of different data-efficient and\nparameter-efficient approaches to Arabic Dialect Identification (ADI). In\nparticular, we investigate various soft-prompting strategies, including\nprefix-tuning, prompt-tuning, P-tuning, and P-tuning V2, as well as LoRA\nreparameterizations. For the data-efficient strategy, we analyze hard prompting\nwith zero-shot and few-shot inferences to analyze the dialect identification\ncapabilities of Large Language Models (LLMs). For the parameter-efficient PEFT\napproaches, we conducted our experiments using Arabic-specific encoder models\non several major datasets. We also analyzed the n-shot inferences on\nopen-source decoder-only models, a general multilingual model (Phi-3.5), and an\nArabic-specific one(SILMA). We observed that the LLMs generally struggle to\ndifferentiate the dialectal nuances in the few-shot or zero-shot setups. The\nsoft-prompted encoder variants perform better, while the LoRA-based fine-tuned\nmodels perform best, even surpassing full fine-tuning.", "AI": {"tldr": "本文探讨了阿拉伯语方言识别（ADI）中数据高效和参数高效的方法，发现LoRA微调模型表现最佳，甚至超越了完全微调，而大型语言模型（LLMs）在零样本/少样本设置下表现不佳。", "motivation": "研究旨在探索和比较不同的数据高效和参数高效策略，以提高阿拉伯语方言识别的性能，特别是评估大型语言模型在此任务中的能力和效率。", "method": "研究方法包括：1) 数据高效策略：使用硬提示（零样本和少样本推断）评估LLMs的方言识别能力。2) 参数高效（PEFT）策略：探究各种软提示方法（如prefix-tuning, prompt-tuning, P-tuning, P-tuning V2）以及LoRA重参数化。实验使用了阿拉伯语专用编码器模型和多个主要数据集，并分析了开源解码器模型（Phi-3.5和SILMA）的n样本推断。", "result": "研究结果显示：1) LLMs在少样本或零样本设置下难以区分方言细微差别。2) 软提示的编码器变体表现更好。3) 基于LoRA的微调模型表现最佳，甚至超越了完全微调的性能。", "conclusion": "LoRA是阿拉伯语方言识别任务中最有效的参数高效方法，其性能优于其他软提示策略，并能超越完全微调。同时，LLMs在低资源（零样本/少样本）方言识别场景中面临挑战。"}}
{"id": "2509.13504", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13504", "abs": "https://arxiv.org/abs/2509.13504", "authors": ["Uriel Garcilazo-Cruz", "Joseph O. Okeme", "Rodrigo A. Vargas--Hernández"], "title": "LivePyxel: Accelerating image annotations with a Python-integrated webcam live streaming", "comment": "8 pages, 10 figures, SM, 5 pages, 4 figures", "summary": "The lack of flexible annotation tools has hindered the deployment of AI\nmodels in some scientific areas. Most existing image annotation software\nrequires users to upload a precollected dataset, which limits support for\non-demand pipelines and introduces unnecessary steps to acquire images. This\nconstraint is particularly problematic in laboratory environments, where\nreal-time data acquisition from instruments such as microscopes is increasingly\ncommon. In this work, we introduce \\texttt{LivePixel}, a Python-based graphical\nuser interface that integrates with imaging systems, such as webcams,\nmicroscopes, and others, to enable real-time image annotation. LivePyxel is\ndesigned to be easy to use through a simple interface that allows users to\nprecisely delimit areas for annotation using tools commonly found in commercial\ngraphics editing software. Of particular interest is the availability of\nB\\'ezier splines and binary masks, and the software's capacity to work with\nnon-destructive layers that enable high-performance editing. LivePyxel also\nintegrates a wide compatibility across video devices, and it's optimized for\nobject detection operations via the use of OpenCV in combination with\nhigh-performance libraries designed to handle matrix and linear algebra\noperations via Numpy effectively. LivePyxel facilitates seamless data\ncollection and labeling, accelerating the development of AI models in\nexperimental workflows. LivePyxel freely available at\nhttps://github.com/UGarCil/LivePyxel", "AI": {"tldr": "LivePixel是一个基于Python的图形用户界面，旨在解决AI模型在科学领域部署中缺乏灵活实时图像标注工具的问题，它支持与成像系统集成以进行实时标注和数据收集。", "motivation": "现有图像标注软件通常要求用户上传预先收集的数据集，这限制了对按需管道的支持，并增加了图像获取的步骤。在实验室环境中，从显微镜等仪器实时获取数据越来越普遍，这种限制尤其成问题，阻碍了AI模型的部署。", "method": "本文介绍了LivePixel，一个基于Python的图形用户界面，可与网络摄像头、显微镜等成像系统集成，实现实时图像标注。它提供简单易用的界面，包含商业图形编辑软件中常见的标注工具，如贝塞尔曲线和二进制掩码，并支持非破坏性图层以实现高性能编辑。LivePixel还广泛兼容各种视频设备，并通过结合OpenCV和NumPy优化了目标检测操作。", "result": "LivePixel能够实现实时、精确和高性能的图像标注，促进了无缝数据收集和标签创建，从而加速了实验工作流程中AI模型的开发。", "conclusion": "LivePixel通过提供一个灵活、实时的标注解决方案，显著加速了实验工作流程中AI模型的开发和部署，解决了现有工具在科学领域应用中的局限性。"}}
{"id": "2509.13547", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.13547", "abs": "https://arxiv.org/abs/2509.13547", "authors": ["Harper Reed", "Michael Sugimura", "Angelo Zangari"], "title": "AI Agents with Human-Like Collaborative Tools: Adaptive Strategies for Enhanced Problem-Solving", "comment": "16 pages, 5 tables", "summary": "We investigate whether giving LLM agents the collaborative tools and autonomy\nthat humans naturally use for problem solving can improve their performance. We\nequip Claude Code agents with MCP-based social media and journaling tools and\nallow them to use these tools as they see fit. Across 34 Aider Polyglot Python\nprogramming challenges, collaborative tools substantially improve performance\non the hardest problems, delivering 15-40% lower cost, 12-27% fewer turns, and\n12-38% faster completion than baseline agents. Effects on the full challenge\nset are mixed, suggesting these tools act as performance enhancers when\nadditional reasoning scaffolding is most needed. Surprisingly, Different models\nnaturally adopted distinct collaborative strategies without explicit\ninstruction. Sonnet 3.7 engaged broadly across tools and benefited from\narticulation-based cognitive scaffolding. Sonnet 4 showed selective adoption,\nleaning on journal-based semantic search when problems were genuinely\ndifficult. This mirrors how human developers adjust collaboration based on\nexpertise and task complexity. Behavioral analysis shows agents prefer writing\nover reading by about 2-9x, indicating that structured articulation drives much\nof the improvement rather than information access alone. Overall, AI agents can\nsystematically benefit from human-inspired collaboration tools at the edge of\ntheir capabilities, pointing to adaptive collaborative interfaces as reasoning\nenhancers rather than universal efficiency boosts.", "AI": {"tldr": "本研究发现，为LLM代理提供人类常用的协作工具（如社交媒体和日记）和自主权，能显著提升它们在解决困难编程问题上的表现，主要通过结构化表达而非单纯信息获取来降低成本、减少轮次并加快完成速度。", "motivation": "研究旨在探究赋予LLM代理人类解决问题时自然使用的协作工具和自主性，是否能提高其性能。", "method": "研究为Claude Code代理配备了基于MCP的社交媒体和日记工具，并允许它们自主使用这些工具。通过34个Aider Polyglot Python编程挑战进行测试，并与基线代理进行比较。同时，分析了不同模型对工具的采纳策略以及代理在写作和阅读行为上的偏好。", "result": "在最困难的问题上，协作工具显著提高了性能，与基线代理相比，成本降低15-40%，轮次减少12-27%，完成速度加快12-38%。对所有挑战的整体效果喜忧参半，表明这些工具在最需要额外推理支架时能发挥性能增强作用。不同模型自然形成了独特的协作策略：Sonnet 3.7广泛使用工具，受益于基于表达的认知支架；Sonnet 4则选择性使用，在真正困难的问题上依赖基于日记的语义搜索。行为分析显示，代理的写作频率比阅读高约2-9倍，表明结构化表达而非信息获取本身是改进的主要驱动力。", "conclusion": "AI代理在其能力边缘可以系统地受益于受人类启发的协作工具。这些工具是适应性协作接口和推理增强器，而非普适的效率提升。结构化表达是性能提升的关键因素。"}}
{"id": "2509.13691", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13691", "abs": "https://arxiv.org/abs/2509.13691", "authors": ["Songhao Huang", "Yuwei Wu", "Guangyao Shi", "Gaurav S. Sukhatme", "Vijay Kumar"], "title": "SPAR: Scalable LLM-based PDDL Domain Generation for Aerial Robotics", "comment": null, "summary": "We investigate the problem of automatic domain generation for the Planning\nDomain Definition Language (PDDL) using Large Language Models (LLMs), with a\nparticular focus on unmanned aerial vehicle (UAV) tasks. Although PDDL is a\nwidely adopted standard in robotic planning, manually designing domains for\ndiverse applications such as surveillance, delivery, and inspection is\nlabor-intensive and error-prone, which hinders adoption and real-world\ndeployment. To address these challenges, we propose SPAR, a framework that\nleverages the generative capabilities of LLMs to automatically produce valid,\ndiverse, and semantically accurate PDDL domains from natural language input. To\nthis end, we first introduce a systematically formulated and validated UAV\nplanning dataset, consisting of ground-truth PDDL domains and associated\nproblems, each paired with detailed domain and action descriptions. Building on\nthis dataset, we design a prompting framework that generates high-quality PDDL\ndomains from language input. The generated domains are evaluated through syntax\nvalidation, executability, feasibility, and interpretability. Overall, this\nwork demonstrates that LLMs can substantially accelerate the creation of\ncomplex planning domains, providing a reproducible dataset and evaluation\npipeline that enables application experts without prior experience to leverage\nit for practical tasks and advance future research in aerial robotics and\nautomated planning.", "AI": {"tldr": "该研究利用大型语言模型（LLMs）自动生成用于无人机（UAV）任务的规划领域定义语言（PDDL）领域，提出了一个名为SPAR的框架，并构建了一个新的无人机规划数据集。", "motivation": "手动设计PDDL领域费时费力且易出错，这阻碍了PDDL在各种应用（如无人机监控、配送、检查）中的采用和实际部署。", "method": "1. 提出了SPAR框架，利用LLM的生成能力，从自然语言输入自动生成有效、多样且语义准确的PDDL领域。2. 构建了一个系统化且经过验证的无人机规划数据集，包含真实PDDL领域、问题以及详细的领域和动作描述。3. 基于此数据集设计了一个提示框架。4. 通过语法验证、可执行性、可行性和可解释性来评估生成的PDDL领域。", "result": "LLMs能够显著加速复杂规划领域的创建。该工作提供了一个可复现的数据集和评估流程。", "conclusion": "LLMs可以帮助没有PDDL经验的应用专家，将此技术应用于实际任务，并推动航空机器人和自动化规划领域的未来研究。"}}
{"id": "2509.13840", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.13840", "abs": "https://arxiv.org/abs/2509.13840", "authors": ["Vinay C K", "Vikas Vazhayil", "Madhav rao"], "title": "Characterizing Human Limb Movements Using An In-House Multi-Channel Non-Invasive Surface-EMG System", "comment": null, "summary": "Electromyography (EMG) signals are obtained from muscle cell activity. The\nrecording and analysis of EMG signals has several applications. The EMG is of\ndiagnostic importance for treating patients suffering from neurological and\nneuromuscular disorders. Conventional methods involve placement of invasive\nelectrodes within the muscles to record EMG signals. The goal is to showcase\nthe usage of surface based EMG signals to characterize all possible human limb\nmovements. An in-house non-invasive EMG signal acquisition system that offers\ncharacterization of human limb actions is a suitable candidate for motor\nimpairment studies and easily extendable to design bionics control specifically\nfor neuromuscular disorder patients. An in-house 8-channel surface-EMG signal\nacquisition system was designed, fabricated, and employed for characterizing\nspecific movements of upper and lower limb. The non-invasive acquisition system\ncaptures the compound electromuscular activity generated from the group of\nmuscles. The EMG acquisition system was designed as a modular structure where\nthe front end analog circuit designs were replicated for all 8 channels, and\nwere designed to function independently. Support vector machine (SVM) as\nclassifier models were developed offline to successfully characterize different\nhuman limb actions. The in house built 8 channel acquisition system with ML\nclassifier models were utilized to successfully characterize movements at\nvarious joints of the upper and lower limb including fingers, wrist, elbow,\nshoulder, knee, and ankle individually.", "AI": {"tldr": "该研究设计并构建了一个8通道非侵入式表面肌电图（sEMG）采集系统，结合支持向量机（SVM）分类器，成功表征了人体上下肢的各种运动。", "motivation": "传统的肌电图（EMG）信号采集方法具有侵入性。研究旨在开发一种非侵入式表面肌电图系统，用于诊断神经和神经肌肉疾病，并为运动障碍研究和仿生控制（特别是针对神经肌肉疾病患者）提供支持。", "method": "研究设计、制造并使用了一个内部开发的8通道非侵入式表面肌电图信号采集系统。该系统采用模块化模拟电路设计，可独立运行。离线使用支持向量机（SVM）作为分类模型，以表征不同的人体肢体动作。", "result": "内部构建的8通道采集系统与机器学习分类模型成功地单独表征了上肢和下肢各个关节（包括手指、手腕、肘部、肩部、膝盖和脚踝）的运动。该系统能够捕获肌肉群产生的复合肌电活动。", "conclusion": "该内部开发的8通道表面肌电图采集系统结合SVM分类器，能够有效地表征人体肢体运动，适用于运动障碍研究，并易于扩展用于神经肌肉疾病患者的仿生控制设计。"}}
{"id": "2509.13790", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13790", "abs": "https://arxiv.org/abs/2509.13790", "authors": ["Yangning Li", "Tingwei Lu", "Yinghui Li", "Yankai Chen", "Wei-Chieh Huang", "Wenhao Jiang", "Hui Wang", "Hai-Tao Zheng", "Philip S. Yu"], "title": "Teaching According to Talents! Instruction Tuning LLMs with Competence-Aware Curriculum Learning", "comment": "EMNLP 2025 Findings", "summary": "Efficient instruction tuning aims to enhance the ultimate performance of\nlarge language models (LLMs) trained on a given instruction dataset. Curriculum\nlearning as a typical data organization strategy has shown preliminary\neffectiveness in instruction tuning. However, current curriculum tuning methods\nsuffer from the curriculum rigidity, since they rely solely on static heuristic\ndifficulty metrics. These methods fail to adapt to the evolving capabilities of\nmodels during training, resulting in a fixed and potentially sub-optimal\nlearning trajectory. To address the issue, Competence-Aware Multi-Perspective\ncUrriculum inStruction tuning framework termed CAMPUS is proposed. CAMPUS\noffers several advantages: (1) Dynamic selection for sub-curriculum. (2)\nCompetency-aware adjustment to the curriculum schedule. (3) Multiple\ndifficulty-based scheduling. Extensive experiments prove the superior\nperformance of CAMPUS, compared to other state-of-the-art baselines for\nefficient instruction tuning.", "AI": {"tldr": "CAMPUS提出了一种能力感知的多视角课程指令微调框架，解决了现有课程微调方法中课程刚性问题，并通过动态选择、能力感知调整和多难度调度实现了卓越性能。", "motivation": "现有的课程指令微调方法依赖静态启发式难度指标，导致课程刚性，无法适应模型在训练过程中不断演变的能力，从而形成固定且可能次优的学习轨迹。", "method": "CAMPUS框架通过以下方式解决问题：1) 动态选择子课程；2) 根据模型能力调整课程进度；3) 基于多维难度进行调度。", "result": "广泛的实验证明，与现有最先进的高效指令微调基线相比，CAMPUS展现出卓越的性能。", "conclusion": "CAMPUS通过引入动态、能力感知的多视角课程调度，有效克服了传统课程微调的刚性问题，显著提升了指令微调的效率和效果。"}}
{"id": "2509.13506", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13506", "abs": "https://arxiv.org/abs/2509.13506", "authors": ["Xingzi Xu", "Qi Li", "Shuwen Qiu", "Julien Han", "Karim Bouyarmane"], "title": "DEFT-VTON: Efficient Virtual Try-On with Consistent Generalised H-Transform", "comment": "Published in 2025 CVPR Workshop", "summary": "Diffusion models enable high-quality virtual try-on (VTO) with their\nestablished image synthesis abilities. Despite the extensive end-to-end\ntraining of large pre-trained models involved in current VTO methods,\nreal-world applications often prioritize limited training and inference,\nserving, and deployment budgets for VTO. To solve this obstacle, we apply\nDoob's h-transform efficient fine-tuning (DEFT) for adapting large pre-trained\nunconditional models for downstream image-conditioned VTO abilities. DEFT\nfreezes the pre-trained model's parameters and trains a small h-transform\nnetwork to learn a conditional h-transform. The h-transform network allows\ntraining only 1.42 percent of the frozen parameters, compared to a baseline of\n5.52 percent in traditional parameter-efficient fine-tuning (PEFT).\n  To further improve DEFT's performance and decrease existing models' inference\ntime, we additionally propose an adaptive consistency loss. Consistency\ntraining distills slow but high-performing diffusion models into a fast one\nwhile retaining performance by enforcing consistencies along the inference\npath. Inspired by constrained optimization, instead of distillation, we combine\nthe consistency loss and the denoising score matching loss in a data-adaptive\nmanner for fine-tuning existing VTO models at a low cost. Empirical results\nshow the proposed DEFT-VTON method achieves state-of-the-art performance on VTO\ntasks, with as few as 15 denoising steps, while maintaining competitive\nresults.", "AI": {"tldr": "本文提出DEFT-VTON方法，通过Doob的h-变换高效微调和自适应一致性损失，显著降低了扩散模型在虚拟试穿（VTO）任务中的训练参数和推理时间，同时保持了最先进的性能。", "motivation": "现有基于扩散模型的虚拟试穿（VTO）方法需要对大型预训练模型进行广泛的端到端训练，导致高昂的训练、推理、服务和部署成本，这与实际应用中有限的预算相悖。", "method": "1. 采用Doob的h-变换高效微调（DEFT）：冻结预训练模型的参数，仅训练一个小型h-变换网络来学习条件h-变换，从而仅训练1.42%的参数（相较于传统PEFT的5.52%）。2. 提出自适应一致性损失：以数据自适应方式结合一致性损失和去噪分数匹配损失，用于微调现有VTO模型，以进一步提升性能并减少推理时间。", "result": "所提出的DEFT-VTON方法在VTO任务上实现了最先进的性能，仅需15个去噪步骤，同时保持了有竞争力的结果。", "conclusion": "DEFT-VTON通过大幅减少训练参数和推理步骤，提供了一种高效且高性能的VTO解决方案，有效解决了现有基于扩散模型的VTO方法的成本限制问题。"}}
{"id": "2509.13570", "categories": ["cs.AI", "math.HO", "Primary: 97U50, Secondary: 97U70, 97D40, 97D60, 97E50, 97H40"], "pdf": "https://arxiv.org/pdf/2509.13570", "abs": "https://arxiv.org/abs/2509.13570", "authors": ["Hannah Klawa", "Shraddha Rajpal", "Cigole Thomas"], "title": "Gen AI in Proof-based Math Courses: A Pilot Study", "comment": "35 pages, 6 figures, Comments welcome!", "summary": "With the rapid rise of generative AI in higher education and the\nunreliability of current AI detection tools, developing policies that encourage\nstudent learning and critical thinking has become increasingly important. This\nstudy examines student use and perceptions of generative AI across three\nproof-based undergraduate mathematics courses: a first-semester abstract\nalgebra course, a topology course and a second-semester abstract algebra\ncourse. In each case, course policy permitted some use of generative AI.\nDrawing on survey responses and student interviews, we analyze how students\nengaged with AI tools, their perceptions of generative AI's usefulness and\nlimitations, and what implications these perceptions hold for teaching\nproof-based mathematics. We conclude by discussing future considerations for\nintegrating generative AI into proof-based mathematics instruction.", "AI": {"tldr": "本研究调查了大学生在允许使用生成式AI的证明型数学课程中对AI工具的使用情况和看法，并探讨了其对教学的启示。", "motivation": "鉴于生成式AI在高等教育中的迅速普及以及现有AI检测工具的不可靠性，制定鼓励学生学习和批判性思维的政策变得日益重要。", "method": "研究对象为三门证明型本科数学课程（抽象代数I、拓扑学、抽象代数II）的学生，这些课程允许部分使用生成式AI。通过问卷调查和学生访谈，分析学生如何使用AI工具，他们对生成式AI有用性和局限性的看法。", "result": "分析了学生如何参与AI工具，他们对生成式AI有用性和局限性的看法，以及这些看法对证明型数学教学的启示。", "conclusion": "讨论了未来将生成式AI融入证明型数学教学的考量。"}}
{"id": "2509.13692", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13692", "abs": "https://arxiv.org/abs/2509.13692", "authors": ["Yadan Zeng", "Jiadong Zhou", "Xiaohan Li", "I-Ming Chen"], "title": "HGACNet: Hierarchical Graph Attention Network for Cross-Modal Point Cloud Completion", "comment": "9 pages, 6 figures", "summary": "Point cloud completion is essential for robotic perception, object\nreconstruction and supporting downstream tasks like grasp planning, obstacle\navoidance, and manipulation. However, incomplete geometry caused by\nself-occlusion and sensor limitations can significantly degrade downstream\nreasoning and interaction. To address these challenges, we propose HGACNet, a\nnovel framework that reconstructs complete point clouds of individual objects\nby hierarchically encoding 3D geometric features and fusing them with\nimage-guided priors from a single-view RGB image. At the core of our approach,\nthe Hierarchical Graph Attention (HGA) encoder adaptively selects critical\nlocal points through graph attention-based downsampling and progressively\nrefines hierarchical geometric features to better capture structural continuity\nand spatial relationships. To strengthen cross-modal interaction, we further\ndesign a Multi-Scale Cross-Modal Fusion (MSCF) module that performs\nattention-based feature alignment between hierarchical geometric features and\nstructured visual representations, enabling fine-grained semantic guidance for\ncompletion. In addition, we proposed the contrastive loss (C-Loss) to\nexplicitly align the feature distributions across modalities, improving\ncompletion fidelity under modality discrepancy. Finally, extensive experiments\nconducted on both the ShapeNet-ViPC benchmark and the YCB-Complete dataset\nconfirm the effectiveness of HGACNet, demonstrating state-of-the-art\nperformance as well as strong applicability in real-world robotic manipulation\ntasks.", "AI": {"tldr": "HGACNet是一个新颖的框架，通过分层编码三维几何特征并融合单视图RGB图像的图像引导先验，实现对单个物体的完整点云重建，解决了不完整点云对下游任务的影响。", "motivation": "点云补全是机器人感知、物体重建和下游任务（如抓取规划、避障和操作）的关键。然而，自遮挡和传感器限制导致的不完整几何信息会显著降低下游推理和交互的性能。", "method": "HGACNet框架包含三个核心组件：1. 分层图注意力（HGA）编码器：通过基于图注意力的降采样自适应选择关键局部点，并逐步细化分层几何特征，以更好地捕获结构连续性和空间关系。2. 多尺度跨模态融合（MSCF）模块：在分层几何特征和结构化视觉表示之间进行基于注意力的特征对齐，实现细粒度的语义指导。3. 对比损失（C-Loss）：显式对齐跨模态的特征分布，提高模态差异下的补全保真度。", "result": "在ShapeNet-ViPC基准和YCB-Complete数据集上进行的广泛实验证实了HGACNet的有效性，展示了最先进的性能以及在实际机器人操作任务中的强大适用性。", "conclusion": "HGACNet成功解决了点云补全的挑战，通过其独特的分层几何特征编码和图像引导融合机制，实现了卓越的补全精度和在真实机器人应用中的实用性。"}}
{"id": "2509.13934", "categories": ["eess.SY", "cs.LG", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.13934", "abs": "https://arxiv.org/abs/2509.13934", "authors": ["Zhixion Chen", "Jiangzhou Wang", "and Hyundong Shin", "Arumugam Nallanathan"], "title": "Large Language Model-Empowered Decision Transformer for UAV-Enabled Data Collection", "comment": "14pages, 8 figures", "summary": "The deployment of unmanned aerial vehicles (UAVs) for reliable and\nenergy-efficient data collection from spatially distributed devices holds great\npromise in supporting diverse Internet of Things (IoT) applications.\nNevertheless, the limited endurance and communication range of UAVs necessitate\nintelligent trajectory planning. While reinforcement learning (RL) has been\nextensively explored for UAV trajectory optimization, its interactive nature\nentails high costs and risks in real-world environments. Offline RL mitigates\nthese issues but remains susceptible to unstable training and heavily rely on\nexpert-quality datasets. To address these challenges, we formulate a joint UAV\ntrajectory planning and resource allocation problem to maximize energy\nefficiency of data collection. The resource allocation subproblem is first\ntransformed into an equivalent linear programming formulation and solved\noptimally with polynomial-time complexity. Then, we propose a large language\nmodel (LLM)-empowered critic-regularized decision transformer (DT) framework,\ntermed LLM-CRDT, to learn effective UAV control policies. In LLM-CRDT, we\nincorporate critic networks to regularize the DT model training, thereby\nintegrating the sequence modeling capabilities of DT with critic-based value\nguidance to enable learning effective policies from suboptimal datasets.\nFurthermore, to mitigate the data-hungry nature of transformer models, we\nemploy a pre-trained LLM as the transformer backbone of the DT model and adopt\na parameter-efficient fine-tuning strategy, i.e., LoRA, enabling rapid\nadaptation to UAV control tasks with small-scale dataset and low computational\noverhead. Extensive simulations demonstrate that LLM-CRDT outperforms benchmark\nonline and offline RL methods, achieving up to 36.7\\% higher energy efficiency\nthan the current state-of-the-art DT approaches.", "AI": {"tldr": "本文提出了一种名为LLM-CRDT的框架，用于无人机轨迹规划和资源分配，以最大化数据收集的能量效率。该框架结合了批评网络正则化、预训练大型语言模型作为决策Transformer骨干和LoRA微调策略，有效解决了离线强化学习的挑战，并在模拟中显著优于现有方法。", "motivation": "无人机在物联网数据收集中具有潜力，但其有限的续航和通信范围需要智能轨迹规划。传统的强化学习在实际部署中成本高昂且风险大。离线强化学习虽然有所缓解，但仍面临训练不稳定和对高质量数据集的过度依赖问题。因此，需要一种能够从次优数据中学习有效策略的方法。", "method": "1. 将无人机轨迹规划和资源分配问题联合建模，以最大化数据收集的能量效率。2. 将资源分配子问题转化为等价的线性规划并以多项式时间复杂度最优求解。3. 提出LLM-CRDT（LLM赋能的批评网络正则化决策Transformer）框架来学习有效的无人机控制策略。4. LLM-CRDT通过引入批评网络来正则化决策Transformer的训练，结合了决策Transformer的序列建模能力和基于批评网络的值指导。5. 利用预训练的大型语言模型作为决策Transformer的骨干，并采用LoRA（参数高效微调）策略，以实现小规模数据集下的快速适应和低计算开销。", "result": "广泛的模拟结果表明，LLM-CRDT优于基准在线和离线强化学习方法，比当前最先进的决策Transformer方法高出高达36.7%的能量效率。", "conclusion": "LLM-CRDT框架通过结合批评网络正则化和预训练LLM的决策Transformer，有效解决了无人机数据收集中的能量效率和离线强化学习的挑战。它能够从次优数据中学习高效策略，并在实际应用中展现出卓越的性能。"}}
{"id": "2509.13803", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13803", "abs": "https://arxiv.org/abs/2509.13803", "authors": ["Laura García-Sardiña", "Hermenegildo Fabregat", "Daniel Deniz", "Rabih Zbib"], "title": "Measuring Gender Bias in Job Title Matching for Grammatical Gender Languages", "comment": null, "summary": "This work sets the ground for studying how explicit grammatical gender\nassignment in job titles can affect the results of automatic job ranking\nsystems. We propose the usage of metrics for ranking comparison controlling for\ngender to evaluate gender bias in job title ranking systems, in particular RBO\n(Rank-Biased Overlap). We generate and share test sets for a job title matching\ntask in four grammatical gender languages, including occupations in masculine\nand feminine form and annotated by gender and matching relevance. We use the\nnew test sets and the proposed methodology to evaluate the gender bias of\nseveral out-of-the-box multilingual models to set as baselines, showing that\nall of them exhibit varying degrees of gender bias.", "AI": {"tldr": "本研究探讨了招聘职位名称中显式语法性别对自动职位排名系统结果的影响，并提出使用控制性别的排名比较指标（如RBO）来评估性别偏见，发现现有模型普遍存在不同程度的性别偏见。", "motivation": "招聘职位名称中明确的语法性别分配可能影响自动职位排名系统的结果，导致潜在的性别偏见，因此需要方法来研究和评估这种影响。", "method": "研究提出使用控制性别的排名比较指标（特别是RBO）来评估职位名称排名系统中的性别偏见。为此，他们生成并共享了四种具有语法性别语言的职位名称匹配测试集，这些测试集包含阳性和阴性形式的职业，并按性别和匹配相关性进行标注。然后，使用这些测试集和提出的方法评估了几个现成的多语言模型作为基线。", "result": "通过使用新的测试集和提出的方法，研究发现所有评估的现成多语言模型都表现出不同程度的性别偏见。", "conclusion": "职位名称中的显式语法性别确实会影响自动职位排名系统，并导致性别偏见。本研究提出的方法和测试集可以有效地评估和揭示这些偏见，为未来的去偏见工作奠定基础。"}}
{"id": "2509.13507", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13507", "abs": "https://arxiv.org/abs/2509.13507", "authors": ["Artem Savkin", "Thomas Lapotre", "Kevin Strauss", "Uzair Akbar", "Federico Tombari"], "title": "Adversarial Appearance Learning in Augmented Cityscapes for Pedestrian Recognition in Autonomous Driving", "comment": null, "summary": "In the autonomous driving area synthetic data is crucial for cover specific\ntraffic scenarios which autonomous vehicle must handle. This data commonly\nintroduces domain gap between synthetic and real domains. In this paper we\ndeploy data augmentation to generate custom traffic scenarios with VRUs in\norder to improve pedestrian recognition. We provide a pipeline for augmentation\nof the Cityscapes dataset with virtual pedestrians. In order to improve\naugmentation realism of the pipeline we reveal a novel generative network\narchitecture for adversarial learning of the data-set lighting conditions. We\nalso evaluate our approach on the tasks of semantic and instance segmentation.", "AI": {"tldr": "本文提出了一种通过数据增强和新型生成网络来创建包含虚拟行人的自定义交通场景的方法，以提高自动驾驶中的行人识别能力并缩小合成数据与真实数据之间的领域差距。", "motivation": "自动驾驶领域中，合成数据对于覆盖特定交通场景至关重要，但通常会引入合成域与真实域之间的领域差距。研究旨在通过生成更真实的增强数据来改善行人识别。", "method": "部署数据增强技术以生成包含虚拟道路使用者（VRUs）的自定义交通场景，以改善行人识别。提供了一个用于使用虚拟行人增强Cityscapes数据集的流程。为了提高增强的真实感，提出了一种新颖的生成网络架构，用于对抗性学习数据集的光照条件。", "result": "该方法在语义分割和实例分割任务上进行了评估。抽象中未提供具体量化结果，但表明其方法在这些任务上进行了测试。", "conclusion": "通过结合数据增强和新颖的生成网络来模拟真实光照条件，可以有效地生成更真实的自定义交通场景，从而有望提高自动驾驶系统中行人识别的性能，并减少合成数据与真实数据之间的领域差距。"}}
{"id": "2509.13588", "categories": ["cs.AI", "cs.CE", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.13588", "abs": "https://arxiv.org/abs/2509.13588", "authors": ["Xuan Liu", "Haoyang Shang", "Haojian Jin"], "title": "Programmable Cognitive Bias in Social Agents", "comment": null, "summary": "This paper introduces CoBRA, a novel toolkit for systematically specifying\nagent behavior in LLM-based social simulation. We found that conventional\napproaches that specify agent behaviors through implicit natural language\ndescriptions cannot yield consistent behaviors across models, and the produced\nagent behaviors do not capture the nuances of the descriptions. In contrast,\nCoBRA presents a new approach to program agents' cognitive biases explicitly,\nby grounding agents' expected behaviors using classic social science\nexperiments. CoBRA has two components: (1) Cognitive Bias Index that measures\nthe cognitive bias of a social agent, by quantifying the agent's reactions in a\nset of validated classical social science experiments; (2) Behavioral\nRegulation Engine that aligns the agent's behavior to demonstrate controlled\ncognitive bias. We evaluated CoBRA as an HCI toolkit through demonstration and\ntechnical benchmarks. Our results suggest that CoBRA can precisely program the\ncognitive bias demonstrated in a social agent in a model-agnostic manner.", "AI": {"tldr": "CoBRA是一个新颖的工具包，通过明确编程认知偏差来系统地指定基于LLM的社会模拟中智能体的行为，并解决了传统自然语言描述的一致性问题。", "motivation": "传统通过隐式自然语言描述指定智能体行为的方法，在不同模型之间无法产生一致的行为，且无法捕捉描述的细微差别。", "method": "CoBRA通过将智能体的预期行为基于经典社会科学实验，显式地编程智能体的认知偏差。它包含两个组件：(1) 认知偏差指数，通过量化智能体在一系列经过验证的经典社会科学实验中的反应来衡量其认知偏差；(2) 行为调节引擎，用于调整智能体的行为以展示受控的认知偏差。", "result": "CoBRA能够以模型无关的方式精确地编程社会智能体所表现出的认知偏差。", "conclusion": "CoBRA是一个有效的人机交互工具包，可以系统地指定和控制LLM社会模拟中智能体的行为，解决了传统方法在一致性和细微差别捕捉方面的局限性。"}}
{"id": "2509.13720", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13720", "abs": "https://arxiv.org/abs/2509.13720", "authors": ["Tianle Zeng", "Jianwei Peng", "Hanjing Ye", "Guangcheng Chen", "Senzi Luo", "Hong Zhang"], "title": "EZREAL: Enhancing Zero-Shot Outdoor Robot Navigation toward Distant Targets under Varying Visibility", "comment": "Page:https://tianlezeng.github.io/EzReal/", "summary": "Zero-shot object navigation (ZSON) in large-scale outdoor environments faces\nmany challenges; we specifically address a coupled one: long-range targets that\nreduce to tiny projections and intermittent visibility due to partial or\ncomplete occlusion. We present a unified, lightweight closed-loop system built\non an aligned multi-scale image tile hierarchy. Through hierarchical\ntarget-saliency fusion, it summarizes localized semantic contrast into a stable\ncoarse-layer regional saliency that provides the target direction and indicates\ntarget visibility. This regional saliency supports visibility-aware heading\nmaintenance through keyframe memory, saliency-weighted fusion of historical\nheadings, and active search during temporary invisibility. The system avoids\nwhole-image rescaling, enables deterministic bottom-up aggregation, supports\nzero-shot navigation, and runs efficiently on a mobile robot. Across simulation\nand real-world outdoor trials, the system detects semantic targets beyond 150m,\nmaintains a correct heading through visibility changes with 82.6% probability,\nand improves overall task success by 17.5% compared with the SOTA methods,\ndemonstrating robust ZSON toward distant and intermittently observable targets.", "AI": {"tldr": "该论文提出了一种轻量级闭环系统，通过多尺度图像瓦片层级和层级目标显著性融合，解决了大规模室外环境中零样本目标导航（ZSON）中远距离、微小投影和间歇性可见性目标的问题，显著提高了导航成功率和航向保持能力。", "motivation": "大规模室外环境中的零样本目标导航（ZSON）面临诸多挑战，特别是远距离目标导致微小投影，以及因部分或完全遮挡造成的间歇性可见性。", "method": "该系统是一个统一、轻量级的闭环系统，基于对齐的多尺度图像瓦片层级构建。它通过层级目标显著性融合，将局部语义对比度汇总为稳定的粗层区域显著性，从而提供目标方向并指示目标可见性。该区域显著性通过关键帧记忆、显著性加权的历史航向融合以及在暂时不可见期间的主动搜索，支持可见性感知的航向维护。该系统避免了整图缩放，实现了确定性自下而上聚合，支持零样本导航，并可在移动机器人上高效运行。", "result": "在模拟和真实世界的室外试验中，该系统能够检测超过150米的语义目标，在可见性变化下以82.6%的概率保持正确航向，并且与现有最佳方法相比，整体任务成功率提高了17.5%。", "conclusion": "该系统展示了针对远距离和间歇性可见目标的鲁棒零样本目标导航能力，有效解决了大规模室外环境中的关键挑战。"}}
{"id": "2509.13985", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.13985", "abs": "https://arxiv.org/abs/2509.13985", "authors": ["Yixun Wen", "Yulong Gao", "Boli Chen"], "title": "Distributionally Robust Equilibria over the Wasserstein Distance for Generalized Nash Game", "comment": null, "summary": "Generalized Nash equilibrium problem (GNEP) is fundamental for practical\napplications where multiple self-interested agents work together to make\noptimal decisions. In this work, we study GNEP with shared distributionally\nrobust chance constraints (DRCCs) for incorporating inevitable uncertainties.\nThe DRCCs are defined over the Wasserstein ball, which can be explicitly\ncharacterized even with limited sample data. To determine the equilibrium of\nthe GNEP, we propose an exact approach to transform the original\ncomputationally intractable problem into a deterministic formulation using the\nNikaido-Isoda function. Specifically, we show that when all agents' objectives\nare quadratic in their respective variables, the equilibrium can be obtained by\nsolving a typical mixed-integer nonlinear programming (MINLP) problem, where\nthe integer and continuous variables are decoupled in both the objective\nfunction and the constraints. This structure significantly improves\ncomputational tractability, as demonstrated through a case study on the\ncharging station pricing problem.", "AI": {"tldr": "本文提出了一种精确方法，通过将带有共享分布鲁棒机会约束（DRCCs）的广义纳什均衡问题（GNEP）转化为一个可计算的混合整数非线性规划（MINLP）问题来求解，特别是在目标函数为二次型时。", "motivation": "广义纳什均衡问题（GNEP）对于多个自利主体协同做出最优决策的应用至关重要。为了应对不可避免的不确定性，研究带有共享分布鲁棒机会约束（DRCCs）的GNEP是必要的。", "method": "研究了带有基于Wasserstein球的共享DRCCs的GNEP。提出了一种精确方法，利用Nikaido-Isoda函数将原始计算上难以处理的问题转化为确定性公式。具体地，当所有主体的目标函数在各自变量中是二次型时，均衡可以通过求解一个典型的混合整数非线性规划（MINLP）问题获得，其中整数和连续变量在目标函数和约束中都是解耦的。", "result": "当目标函数为二次型时，GNEP的均衡可以通过求解一个典型的MINLP问题获得。该MINLP问题中整数和连续变量的解耦结构显著提高了计算的可处理性。通过充电站定价问题的案例研究验证了方法的有效性。", "conclusion": "本文为带有共享分布鲁棒机会约束的广义纳什均衡问题提供了一种精确且计算上可行的求解方法，特别是在主体目标函数为二次型时，通过转化为一个结构优化的MINLP问题，显著提高了计算效率。"}}
{"id": "2509.13813", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13813", "abs": "https://arxiv.org/abs/2509.13813", "authors": ["Edward Phillips", "Sean Wu", "Soheila Molaei", "Danielle Belgrave", "Anshul Thakur", "David Clifton"], "title": "Geometric Uncertainty for Detecting and Correcting Hallucinations in LLMs", "comment": null, "summary": "Large language models demonstrate impressive results across diverse tasks but\nare still known to hallucinate, generating linguistically plausible but\nincorrect answers to questions. Uncertainty quantification has been proposed as\na strategy for hallucination detection, but no existing black-box approach\nprovides estimates for both global and local uncertainty. The former attributes\nuncertainty to a batch of responses, while the latter attributes uncertainty to\nindividual responses. Current local methods typically rely on white-box access\nto internal model states, whilst black-box methods only provide global\nuncertainty estimates. We introduce a geometric framework to address this,\nbased on archetypal analysis of batches of responses sampled with only\nblack-box model access. At the global level, we propose Geometric Volume, which\nmeasures the convex hull volume of archetypes derived from response embeddings.\nAt the local level, we propose Geometric Suspicion, which ranks responses by\nreliability and enables hallucination reduction through preferential response\nselection. Unlike prior dispersion methods which yield only a single global\nscore, our approach provides semantic boundary points which have utility for\nattributing reliability to individual responses. Experiments show that our\nframework performs comparably to or better than prior methods on short form\nquestion-answering datasets, and achieves superior results on medical datasets\nwhere hallucinations carry particularly critical risks. We also provide\ntheoretical justification by proving a link between convex hull volume and\nentropy.", "AI": {"tldr": "本文提出了一种黑盒几何框架，通过对响应嵌入进行原型分析，量化大型语言模型（LLM）的全局和局部不确定性，从而检测幻觉。该方法在问答和医疗数据集上表现良好，尤其在医疗领域取得了显著优势。", "motivation": "大型语言模型存在幻觉问题，会生成看似合理但错误的答案。现有不确定性量化方法要么只能提供批次级别的全局不确定性（黑盒方法），要么需要访问模型内部状态才能提供个体响应的局部不确定性（白盒方法）。目前缺乏一种同时提供全局和局部不确定性的黑盒方法，尤其是在医疗等高风险领域，幻觉的后果更为严重。", "method": "本文引入了一个基于原型分析的几何框架，仅通过黑盒模型访问对响应批次进行采样。在全局层面，提出了“几何体积”（Geometric Volume），用于测量响应嵌入派生原型的凸包体积。在局部层面，提出了“几何可疑度”（Geometric Suspicion），通过可靠性对响应进行排名，并通过优先选择响应来减少幻觉。与仅提供单一全局分数的分散方法不同，该方法提供了语义边界点，有助于评估个体响应的可靠性。理论上，证明了凸包体积与熵之间的联系。", "result": "实验表明，该框架在短形式问答数据集上的表现与现有方法相当或更优，并在幻觉风险尤其关键的医疗数据集上取得了卓越的成果。", "conclusion": "所提出的几何框架能够有效且仅通过黑盒访问来量化大型语言模型的全局和局部不确定性。它通过提供语义边界点来评估个体响应的可靠性，从而有效地检测和减少幻觉，尤其在医疗等关键领域具有重要应用价值，并提供了坚实的理论基础。"}}
{"id": "2509.13508", "categories": ["cs.CV", "I.4.3; I.4.6"], "pdf": "https://arxiv.org/pdf/2509.13508", "abs": "https://arxiv.org/abs/2509.13508", "authors": ["Maksim Penkin", "Andrey Krylov"], "title": "FunKAN: Functional Kolmogorov-Arnold Network for Medical Image Enhancement and Segmentation", "comment": "9 pages, 5 figures, submitted to the Fortieth AAAI Conference on\n  Artificial Intelligence (AAAI-26)", "summary": "Medical image enhancement and segmentation are critical yet challenging tasks\nin modern clinical practice, constrained by artifacts and complex anatomical\nvariations. Traditional deep learning approaches often rely on complex\narchitectures with limited interpretability. While Kolmogorov-Arnold networks\noffer interpretable solutions, their reliance on flattened feature\nrepresentations fundamentally disrupts the intrinsic spatial structure of\nimaging data. To address this issue we propose a Functional Kolmogorov-Arnold\nNetwork (FunKAN) -- a novel interpretable neural framework, designed\nspecifically for image processing, that formally generalizes the\nKolmogorov-Arnold representation theorem onto functional spaces and learns\ninner functions using Fourier decomposition over the basis Hermite functions.\nWe explore FunKAN on several medical image processing tasks, including Gibbs\nringing suppression in magnetic resonance images, benchmarking on IXI dataset.\nWe also propose U-FunKAN as state-of-the-art binary medical segmentation model\nwith benchmarks on three medical datasets: BUSI (ultrasound images), GlaS\n(histological structures) and CVC-ClinicDB (colonoscopy videos), detecting\nbreast cancer, glands and polyps, respectively. Experiments on those diverse\ndatasets demonstrate that our approach outperforms other KAN-based backbones in\nboth medical image enhancement (PSNR, TV) and segmentation (IoU, F1). Our work\nbridges the gap between theoretical function approximation and medical image\nanalysis, offering a robust, interpretable solution for clinical applications.", "AI": {"tldr": "本文提出了一种名为FunKAN的新型可解释神经网络框架，专门用于医学图像处理。FunKAN将Kolmogorov-Arnold表示定理推广到函数空间，并通过傅里叶分解学习内部函数。它在图像增强和分割任务上均优于其他基于KAN的方法。", "motivation": "医学图像增强和分割面临伪影和复杂解剖变异的挑战。传统深度学习模型通常复杂且可解释性有限。虽然Kolmogorov-Arnold网络（KAN）提供可解释性，但其扁平化的特征表示破坏了图像固有的空间结构。", "method": "本文提出了功能性Kolmogorov-Arnold网络（FunKAN），将Kolmogorov-Arnold表示定理推广到函数空间。FunKAN使用基于傅里叶分解和埃尔米特函数学习内部函数。此外，还提出了U-FunKAN作为用于二值医学图像分割的模型。", "result": "实验结果表明，在磁共振图像的吉布斯振铃抑制等医学图像增强任务（IXI数据集）和乳腺癌、腺体、息肉检测等分割任务（BUSI、GlaS、CVC-ClinicDB数据集）中，FunKAN在PSNR、TV、IoU和F1指标上均优于其他基于KAN的骨干网络。", "conclusion": "FunKAN为医学图像分析提供了一个鲁棒、可解释的解决方案，弥合了理论函数逼近与医学图像分析之间的鸿沟，具有重要的临床应用潜力。"}}
{"id": "2509.13615", "categories": ["cs.AI", "cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.13615", "abs": "https://arxiv.org/abs/2509.13615", "authors": ["Zongru Wu", "Rui Mao", "Zhiyuan Tian", "Pengzhou Cheng", "Tianjie Ju", "Zheng Wu", "Lingzhong Dong", "Haiyue Sheng", "Zhuosheng Zhang", "Gongshen Liu"], "title": "See, Think, Act: Teaching Multimodal Agents to Effectively Interact with GUI by Identifying Toggles", "comment": null, "summary": "The advent of multimodal agents facilitates effective interaction within\ngraphical user interface (GUI), especially in ubiquitous GUI control. However,\ntheir inability to reliably execute toggle control instructions remains a key\nbottleneck. To investigate this, we construct a state control benchmark with\nbinary toggle instructions from public datasets. Evaluations of existing agents\ndemonstrate their unreliability, particularly when the current toggle state\nalready matches the desired state. To address the challenge, we propose\nState-aware Reasoning (StaR), a training method that teaches agents to perceive\nthe current toggle state, analyze the desired state from the instruction, and\nact accordingly. Experiments on three multimodal agents demonstrate that StaR\ncan improve toggle instruction execution accuracy by over 30\\%. Further\nevaluations on three public benchmarks show that StaR also enhances general\ntask performance. Finally, evaluations on a dynamic environment highlight the\npotential of StaR for real-world applications. Code, benchmark, and\nStaR-enhanced agents are available at https://github.com/ZrW00/StaR.", "AI": {"tldr": "多模态智能体在图形用户界面（GUI）的开关控制中表现不可靠。本文构建了一个开关控制基准，并提出了“状态感知推理”（StaR）训练方法，使智能体能够感知当前状态并根据指令采取行动，从而将开关指令执行准确率提高30%以上，并提升通用任务性能。", "motivation": "多模态智能体在GUI交互中有效，但在可靠执行开关控制指令方面存在关键瓶颈，尤其是在当前开关状态与期望状态已匹配时。", "method": "研究人员构建了一个包含二元开关指令的状态控制基准；提出了一种名为“状态感知推理”（StaR）的训练方法，该方法旨在教会智能体感知当前开关状态、分析指令中的期望状态并据此采取行动。", "result": "现有智能体在开关控制上不可靠，特别是在当前状态与期望状态匹配时；StaR在三种多模态智能体上将开关指令执行准确率提高了30%以上；StaR还提升了在三个公共基准上的通用任务性能；在动态环境中的评估表明StaR在实际应用中具有潜力。", "conclusion": "StaR训练方法有效解决了多模态智能体在GUI开关控制中执行不可靠的问题，显著提高了指令执行准确性和通用任务性能，展现了在现实世界应用中的巨大潜力。"}}
{"id": "2509.13731", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13731", "abs": "https://arxiv.org/abs/2509.13731", "authors": ["Jeongwoo Park", "Seabin Lee", "Changmin Park", "Wonjong Lee", "Changjoo Nam"], "title": "Reinforcement Learning for Robotic Insertion of Flexible Cables in Industrial Settings", "comment": null, "summary": "The industrial insertion of flexible flat cables (FFCs) into receptacles\npresents a significant challenge owing to the need for submillimeter precision\nwhen handling the deformable cables. In manufacturing processes, FFC insertion\nwith robotic manipulators often requires laborious human-guided trajectory\ngeneration. While Reinforcement Learning (RL) offers a solution to automate\nthis task without modeling complex properties of FFCs, the nondeterminism\ncaused by the deformability of FFCs requires significant efforts and time on\ntraining. Moreover, training directly in a real environment is dangerous as\nindustrial robots move fast and possess no safety measure. We propose an RL\nalgorithm for FFC insertion that leverages a foundation model-based real-to-sim\napproach to reduce the training time and eliminate the risk of physical damages\nto robots and surroundings. Training is done entirely in simulation, allowing\nfor random exploration without the risk of physical damages. Sim-to-real\ntransfer is achieved through semantic segmentation masks which leave only those\nvisual features relevant to the insertion tasks such as the geometric and\nspatial information of the cables and receptacles. To enhance generality, we\nuse a foundation model, Segment Anything Model 2 (SAM2). To eleminate human\nintervention, we employ a Vision-Language Model (VLM) to automate the initial\nprompting of SAM2 to find segmentation masks. In the experiments, our method\nexhibits zero-shot capabilities, which enable direct deployments to real\nenvironments without fine-tuning.", "AI": {"tldr": "该研究提出一种基于强化学习（RL）的柔性扁平电缆（FFC）插入算法，利用基础模型（SAM2、VLM）实现从真实到模拟的转移，从而在仿真中安全高效地训练，并实现零样本的模拟到真实环境部署。", "motivation": "柔性扁平电缆（FFC）的插入需要亚毫米级的精度，且电缆易变形，给机器人操作带来了巨大挑战。现有的机器人FFC插入方法常需人工引导轨迹。虽然强化学习（RL）可自动化此任务，但FFC的非确定性变形导致训练耗时且危险，尤其在真实环境中训练存在物理损坏风险。", "method": "该研究提出一种RL算法，采用基于基础模型的真实到模拟（real-to-sim）方法。训练完全在仿真环境中进行，以避免物理损坏。通过语义分割掩码实现模拟到真实的迁移，仅保留与插入任务相关的视觉特征（如电缆和插座的几何和空间信息）。为增强通用性，使用基础模型Segment Anything Model 2 (SAM2)。为消除人工干预，采用视觉-语言模型（VLM）自动化SAM2的初始提示以获取分割掩码。", "result": "实验表明，该方法具有零样本能力，可以直接部署到真实环境中而无需微调。", "conclusion": "所提出的结合基础模型和真实到模拟方法的RL算法，能安全、高效地自动化FFC插入任务，并通过零样本迁移实现直接部署，有效解决了FFC插入的挑战。"}}
{"id": "2509.13994", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.13994", "abs": "https://arxiv.org/abs/2509.13994", "authors": ["Giacomo Bastianel", "Dirk Van Hertem", "Hakan Ergun", "Line Roald"], "title": "Day-Ahead Transmission Grid Topology Optimization Considering Renewable Energy Sources' Uncertainty", "comment": null, "summary": "The increasing renewable penetration introduces significant uncertainty in\npower system operations. At the same time, the existing transmission grid is\noften already congested, and urgently needed reinforcements are frequently\ndelayed due to several constraints. To address these challenges, adjusting the\ngrid topology based on congestion patterns is considered a non-costly remedy to\nguarantee efficient power transmission. Based on this idea, this paper proposes\na grid topology optimization model combining optimal transmission switching and\nbusbar splitting for AC and hybrid AC/DC grids. The methodology incorporates\nRES forecast uncertainty through a scenario-based stochastic optimization\napproach, using real offshore wind data and K-means clustering to generate\nrepresentative forecast error scenarios. The proposed model includes several\nformulations to be compared with a plain optimal power flow (OPF) model: hourly\noptimizing the topology, one topology for 24 hours, or a limited number of\nswitching actions over a day. The grid topology optimization model is\nformulated as a Mixed-Integer Quadratic Convex Problem, optimized based on the\nday-ahead (D-1) RES forecast and validated for AC-feasibility via an AC-OPF\nformulation. Based on the generation setpoints of the feasibility check, a\nredispatch simulation based on the measured (D) RES realization is then\ncomputed. The methodology is tested on an AC 30-bus test case and a hybrid\nAC/DC 50-bus test case, for a 24-hours (30-bus) and a 14-days (both test cases)\ntime series. The results highlight the economic benefits brought by grid\ntopology optimization for congested test cases with high penetration of RES. In\naddition, the results demonstrate that accounting for RES uncertainty with at\nleast 6 to 8 scenarios leads to lower or comparable total costs to\ndeterministic day-ahead forecasts, even when limiting the frequency of\ntopological actions.", "AI": {"tldr": "本文提出了一种结合输电线路切换和母线分裂的电网拓扑优化模型，用于交流和交直流混合电网，通过基于情景的随机优化方法考虑可再生能源预测不确定性，旨在降低拥堵和运营成本，并在测试案例中展示了显著的经济效益。", "motivation": "可再生能源渗透率的增加导致电力系统运行不确定性显著提升。同时，现有输电网经常拥堵，且急需的扩建工程常因各种限制而延迟。因此，需要一种非成本高昂的解决方案，通过调整电网拓扑来保证高效输电。", "method": "本研究提出了一种结合最优输电切换和母线分裂的电网拓扑优化模型，适用于交流和交直流混合电网。该方法通过基于情景的随机优化方法纳入可再生能源预测不确定性，利用真实海上风电数据和K-means聚类生成代表性预测误差情景。模型比较了多种拓扑优化策略（每小时优化、24小时单一拓扑、每日有限切换动作），并被表述为混合整数二次凸问题（MIQCP）。优化基于日前（D-1）可再生能源预测进行，并通过交流最优潮流（AC-OPF）验证交流可行性。随后，基于测量的（D）可再生能源实现进行再调度模拟。该方法在30节点交流和50节点交直流混合测试案例上进行了验证。", "result": "研究结果表明，对于可再生能源渗透率高的拥堵测试案例，电网拓扑优化带来了显著的经济效益。此外，即使限制拓扑动作的频率，考虑6到8个情景的可再生能源不确定性也能实现与确定性日前预测相比更低或相当的总成本。", "conclusion": "电网拓扑优化是一种有效的非成本高昂的解决方案，能够显著提高可再生能源渗透率高且拥堵的电力系统的经济效益。通过基于情景的随机优化方法考虑可再生能源的不确定性，能够进一步优化系统性能并降低总成本，即使在拓扑动作受限的情况下。"}}
{"id": "2509.13814", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13814", "abs": "https://arxiv.org/abs/2509.13814", "authors": ["Kartik Shinde", "Laurent Besacier", "Ondrej Bojar", "Thibaut Thonet", "Tirthankar Ghosal"], "title": "Findings of the Third Automatic Minuting (AutoMin) Challenge", "comment": "Automin 2025 Website: https://ufal.github.io/automin-2025/", "summary": "This paper presents the third edition of AutoMin, a shared task on automatic\nmeeting summarization into minutes. In 2025, AutoMin featured the main task of\nminuting, the creation of structured meeting minutes, as well as a new task:\nquestion answering (QA) based on meeting transcripts.\n  The minuting task covered two languages, English and Czech, and two domains:\nproject meetings and European Parliament sessions. The QA task focused solely\non project meetings and was available in two settings: monolingual QA in\nEnglish, and cross-lingual QA, where questions were asked and answered in Czech\nbased on English meetings.\n  Participation in 2025 was more limited compared to previous years, with only\none team joining the minuting task and two teams participating in QA. However,\nas organizers, we included multiple baseline systems to enable a comprehensive\nevaluation of current (2025) large language models (LLMs) on both tasks.", "AI": {"tldr": "AutoMin 2025是一个关于自动会议纪要生成和基于会议转录的问答的共享任务，涵盖多语言、多领域，并评估了大型语言模型（LLMs）。", "motivation": "推动自动会议纪要和问答技术的发展，特别是为了全面评估2025年当前大型语言模型（LLMs）在这两项任务上的性能。", "method": "AutoMin 2025采用共享任务形式，包含两个主要任务：1) 会议纪要生成（minuting），涉及英语和捷克语，以及项目会议和欧洲议会会议两个领域；2) 基于会议转录的问答（QA），仅限于项目会议，包括英语单语QA和英-捷克语跨语言QA。任务组织者提供了多个基线系统以进行全面的LLM评估。", "result": "2025年的参与度相对有限，纪要任务仅有一个团队参加，问答任务有两个团队参加。然而，组织者纳入了多个基线系统，对当前（2025年）的大型语言模型（LLMs）在这两项任务上的表现进行了全面的评估。", "conclusion": "尽管参与团队数量较少，AutoMin 2025成功地为自动会议纪要和问答任务提供了一个评估框架，并通过基线系统对当前LLMs的能力进行了全面评估。"}}
{"id": "2509.13515", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13515", "abs": "https://arxiv.org/abs/2509.13515", "authors": ["Jiangbei Yue", "Shuonan Yang", "Tailin Chen", "Jianbo Jiao", "Zeyu Fu"], "title": "Multimodal Hate Detection Using Dual-Stream Graph Neural Networks", "comment": null, "summary": "Hateful videos present serious risks to online safety and real-world\nwell-being, necessitating effective detection methods. Although multimodal\nclassification approaches integrating information from several modalities\noutperform unimodal ones, they typically neglect that even minimal hateful\ncontent defines a video's category. Specifically, they generally treat all\ncontent uniformly, instead of emphasizing the hateful components. Additionally,\nexisting multimodal methods cannot systematically capture structured\ninformation in videos, limiting the effectiveness of multimodal fusion. To\naddress these limitations, we propose a novel multimodal dual-stream graph\nneural network model. It constructs an instance graph by separating the given\nvideo into several instances to extract instance-level features. Then, a\ncomplementary weight graph assigns importance weights to these features,\nhighlighting hateful instances. Importance weights and instance features are\ncombined to generate video labels. Our model employs a graph-based framework to\nsystematically model structured relationships within and across modalities.\nExtensive experiments on public datasets show that our model is\nstate-of-the-art in hateful video classification and has strong explainability.\nCode is available:\nhttps://github.com/Multimodal-Intelligence-Lab-MIL/MultiHateGNN.", "AI": {"tldr": "本文提出一种新颖的多模态双流图神经网络模型，通过强调仇恨实例和系统地捕捉结构化信息，解决了现有仇恨视频检测方法忽略关键仇恨内容和融合效果有限的问题，实现了最先进的性能和强大的可解释性。", "motivation": "现有的多模态仇恨视频检测方法通常忽略即使是微小的仇恨内容也能定义视频类别，将所有内容一视同仁，未能突出仇恨部分。此外，它们无法系统地捕捉视频中的结构化信息，限制了多模态融合的有效性。", "method": "本文提出一种新颖的多模态双流图神经网络（GNN）模型。该模型通过将视频分离成多个实例来构建一个实例图，提取实例级特征。随后，一个互补权重图为这些特征分配重要性权重，以突出仇恨实例。重要性权重和实例特征被结合起来生成视频标签。该模型采用基于图的框架来系统地建模模态内部和跨模态的结构化关系。", "result": "在公共数据集上进行的广泛实验表明，该模型在仇恨视频分类方面达到了最先进的水平，并具有强大的可解释性。", "conclusion": "所提出的多模态双流图神经网络模型通过强调仇恨实例并系统地捕捉结构化信息，有效解决了现有方法的局限性，在仇恨视频分类中取得了最先进的性能和良好的可解释性。"}}
{"id": "2509.13704", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.13704", "abs": "https://arxiv.org/abs/2509.13704", "authors": ["Liangtao Lin", "Zhaomeng Zhu", "Tianwei Zhang", "Yonggang Wen"], "title": "InfraMind: A Novel Exploration-based GUI Agentic Framework for Mission-critical Industrial Management", "comment": null, "summary": "Mission-critical industrial infrastructure, such as data centers,\nincreasingly depends on complex management software. Its operations, however,\npose significant challenges due to the escalating system complexity,\nmulti-vendor integration, and a shortage of expert operators. While Robotic\nProcess Automation (RPA) offers partial automation through handcrafted scripts,\nit suffers from limited flexibility and high maintenance costs. Recent advances\nin Large Language Model (LLM)-based graphical user interface (GUI) agents have\nenabled more flexible automation, yet these general-purpose agents face five\ncritical challenges when applied to industrial management, including unfamiliar\nelement understanding, precision and efficiency, state localization, deployment\nconstraints, and safety requirements. To address these issues, we propose\nInfraMind, a novel exploration-based GUI agentic framework specifically\ntailored for industrial management systems. InfraMind integrates five\ninnovative modules to systematically resolve different challenges in industrial\nmanagement: (1) systematic search-based exploration with virtual machine\nsnapshots for autonomous understanding of complex GUIs; (2) memory-driven\nplanning to ensure high-precision and efficient task execution; (3) advanced\nstate identification for robust localization in hierarchical interfaces; (4)\nstructured knowledge distillation for efficient deployment with lightweight\nmodels; and (5) comprehensive, multi-layered safety mechanisms to safeguard\nsensitive operations. Extensive experiments on both open-source and commercial\nDCIM platforms demonstrate that our approach consistently outperforms existing\nframeworks in terms of task success rate and operational efficiency, providing\na rigorous and scalable solution for industrial management automation.", "AI": {"tldr": "本文提出InfraMind，一个基于探索的GUI智能体框架，专门为工业管理系统设计。它通过五个创新模块解决现有LLM-based GUI智能体在工业应用中面临的挑战，并在实验中表现出卓越的性能和效率。", "motivation": "任务关键型工业基础设施（如数据中心）日益依赖复杂的管理软件，但其操作面临系统复杂性增加、多供应商集成和专家操作员短缺等挑战。RPA自动化缺乏灵活性且维护成本高。虽然基于LLM的GUI智能体提供了更灵活的自动化，但通用智能体在工业管理中面临五大关键挑战：不熟悉的元素理解、精度和效率、状态定位、部署限制和安全要求。", "method": "本文提出InfraMind，一个新颖的基于探索的GUI智能体框架，专为工业管理系统量身定制。它集成了五个创新模块来系统地解决工业管理中的不同挑战：(1) 基于系统搜索的探索与虚拟机快照相结合，用于自主理解复杂GUI；(2) 记忆驱动的规划，以确保高精度和高效的任务执行；(3) 先进的状态识别，用于在分层界面中实现鲁棒定位；(4) 结构化知识蒸馏，用于使用轻量级模型进行高效部署；(5) 全面、多层次的安全机制，以保护敏感操作。", "result": "在开源和商业DCIM平台上的大量实验表明，InfraMind在任务成功率和操作效率方面始终优于现有框架。", "conclusion": "InfraMind为工业管理自动化提供了一个严谨且可扩展的解决方案，有效解决了现有LLM-based GUI智能体在工业应用中面临的挑战，并显著提高了任务成功率和操作效率。"}}
{"id": "2509.13733", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13733", "abs": "https://arxiv.org/abs/2509.13733", "authors": ["Xiaolin Zhou", "Tingyang Xiao", "Liu Liu", "Yucheng Wang", "Maiyue Chen", "Xinrui Meng", "Xinjie Wang", "Wei Feng", "Wei Sui", "Zhizhong Su"], "title": "FSR-VLN: Fast and Slow Reasoning for Vision-Language Navigation with Hierarchical Multi-modal Scene Graph", "comment": "8 pages", "summary": "Visual-Language Navigation (VLN) is a fundamental challenge in robotic\nsystems, with broad applications for the deployment of embodied agents in\nreal-world environments. Despite recent advances, existing approaches are\nlimited in long-range spatial reasoning, often exhibiting low success rates and\nhigh inference latency, particularly in long-range navigation tasks. To address\nthese limitations, we propose FSR-VLN, a vision-language navigation system that\ncombines a Hierarchical Multi-modal Scene Graph (HMSG) with Fast-to-Slow\nNavigation Reasoning (FSR). The HMSG provides a multi-modal map representation\nsupporting progressive retrieval, from coarse room-level localization to\nfine-grained goal view and object identification. Building on HMSG, FSR first\nperforms fast matching to efficiently select candidate rooms, views, and\nobjects, then applies VLM-driven refinement for final goal selection. We\nevaluated FSR-VLN across four comprehensive indoor datasets collected by\nhumanoid robots, utilizing 87 instructions that encompass a diverse range of\nobject categories. FSR-VLN achieves state-of-the-art (SOTA) performance in all\ndatasets, measured by the retrieval success rate (RSR), while reducing the\nresponse time by 82% compared to VLM-based methods on tour videos by activating\nslow reasoning only when fast intuition fails. Furthermore, we integrate\nFSR-VLN with speech interaction, planning, and control modules on a Unitree-G1\nhumanoid robot, enabling natural language interaction and real-time navigation.", "AI": {"tldr": "FSR-VLN是一种新的视觉-语言导航系统，结合了分层多模态场景图（HMSG）和快到慢导航推理（FSR），解决了长距离空间推理的挑战，在保持最先进性能的同时显著降低了推理延迟。", "motivation": "现有的视觉-语言导航（VLN）方法在长距离空间推理方面存在局限性，导致成功率低和推理延迟高，尤其是在长距离导航任务中。", "method": "FSR-VLN系统包含两个核心组件：1. 分层多模态场景图（HMSG）：提供多模态地图表示，支持从粗粒度房间级定位到细粒度目标视图和物体识别的渐进式检索。2. 快到慢导航推理（FSR）：首先进行快速匹配以高效选择候选房间、视图和物体，然后应用VLM驱动的细化来最终选择目标。", "result": "FSR-VLN在四个综合室内数据集上实现了最先进的检索成功率（RSR），同时将响应时间比基于VLM的方法减少了82%。此外，该系统已与Unitree-G1人形机器人集成，实现了自然语言交互和实时导航。", "conclusion": "FSR-VLN通过结合HMSG和FSR，有效解决了长距离视觉-语言导航中的空间推理和效率问题，在提高导航成功率的同时显著缩短了响应时间，为具身智能体在现实世界环境中的部署提供了更高效、准确的解决方案。"}}
{"id": "2509.14047", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.14047", "abs": "https://arxiv.org/abs/2509.14047", "authors": ["Taiki Nakano", "Ahmed Aboudonia", "Jaap Eising", "Andrea Martinelli", "Florian Dörfler", "John Lygeros"], "title": "Dissipativity-Based Data-Driven Decentralized Control of Interconnected Systems", "comment": null, "summary": "We propose data-driven decentralized control algorithms for stabilizing\ninterconnected systems. We first derive a data-driven condition to synthesize a\nlocal controller that ensures the dissipativity of the local subsystems. Then,\nwe propose data-driven decentralized stability conditions for the global system\nbased on the dissipativity of each local system. Since both conditions take the\nform of linear matrix inequalities and are based on dissipativity theory, this\nyields a unified pipeline, resulting in a data-driven decentralized control\nalgorithm. As a special case, we also consider stabilizing systems\ninterconnected through diffusive coupling and propose a control algorithm. We\nvalidate the effectiveness and the scalability of the proposed control\nalgorithms in numerical examples in the context of microgrids.", "AI": {"tldr": "本文提出了一种基于耗散性理论和线性矩阵不等式（LMI）的数据驱动分散控制算法，用于稳定互联系统。", "motivation": "研究的动机是为互联系统开发数据驱动的分散控制算法，以确保其稳定性。", "method": "方法包括：1) 推导数据驱动条件以合成局部控制器，确保局部子系统的耗散性。2) 基于局部系统的耗散性，提出全局系统的数据驱动分散稳定性条件。3) 这两个条件都采用LMI形式并基于耗散性理论，形成统一的流程。4) 考虑扩散耦合互联系统的特殊情况并提出控制算法。", "result": "提出了数据驱动的分散控制算法，并在微电网的数值示例中验证了其有效性和可扩展性。", "conclusion": "所提出的数据驱动分散控制算法提供了一个统一的管道，能够有效稳定互联系统，包括通过扩散耦合互联的系统，并展现出良好的可扩展性。"}}
{"id": "2509.13835", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13835", "abs": "https://arxiv.org/abs/2509.13835", "authors": ["Minh Duc Bui", "Carolin Holtermann", "Valentin Hofmann", "Anne Lauscher", "Katharina von der Wense"], "title": "Large Language Models Discriminate Against Speakers of German Dialects", "comment": "Accepted to EMNLP 2025 Main", "summary": "Dialects represent a significant component of human culture and are found\nacross all regions of the world. In Germany, more than 40% of the population\nspeaks a regional dialect (Adler and Hansen, 2022). However, despite cultural\nimportance, individuals speaking dialects often face negative societal\nstereotypes. We examine whether such stereotypes are mirrored by large language\nmodels (LLMs). We draw on the sociolinguistic literature on dialect perception\nto analyze traits commonly associated with dialect speakers. Based on these\ntraits, we assess the dialect naming bias and dialect usage bias expressed by\nLLMs in two tasks: an association task and a decision task. To assess a model's\ndialect usage bias, we construct a novel evaluation corpus that pairs sentences\nfrom seven regional German dialects (e.g., Alemannic and Bavarian) with their\nstandard German counterparts. We find that: (1) in the association task, all\nevaluated LLMs exhibit significant dialect naming and dialect usage bias\nagainst German dialect speakers, reflected in negative adjective associations;\n(2) all models reproduce these dialect naming and dialect usage biases in their\ndecision making; and (3) contrary to prior work showing minimal bias with\nexplicit demographic mentions, we find that explicitly labeling linguistic\ndemographics--German dialect speakers--amplifies bias more than implicit cues\nlike dialect usage.", "AI": {"tldr": "本研究发现，大型语言模型（LLMs）对德国方言使用者存在显著的负面偏见，这种偏见在显式提及语言人口统计信息时会被放大。", "motivation": "方言是人类文化的重要组成部分，但方言使用者常面临负面社会刻板印象。研究旨在探究大型语言模型是否也反映了这些刻板印象。", "method": "研究借鉴社会语言学文献，分析与方言使用者相关的常见特质。通过关联任务和决策任务，评估LLMs的方言命名偏见和方言使用偏见。为此，构建了一个新的评估语料库，将七种德国地方方言（如阿勒曼尼语和巴伐利亚语）的句子与标准德语对应句子进行配对。", "result": "1) 在关联任务中，所有评估的LLMs都对德国方言使用者表现出显著的负面方言命名和方言使用偏见，体现在负面形容词关联上；2) 所有模型在决策过程中都再现了这些方言命名和方言使用偏见；3) 与以往研究认为显式提及人口统计信息偏见最小的结论相反，本研究发现显式标注语言人口统计信息（德国方言使用者）比隐式线索（如方言使用）更能放大偏见。", "conclusion": "大型语言模型对德国方言使用者存在显著的负面偏见，并且这种偏见在明确标注语言人口统计信息时会进一步加剧。"}}
{"id": "2509.13525", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13525", "abs": "https://arxiv.org/abs/2509.13525", "authors": ["Romain Hardy", "Tyler Berzin", "Pranav Rajpurkar"], "title": "ColonCrafter: A Depth Estimation Model for Colonoscopy Videos Using Diffusion Priors", "comment": "12 pages, 8 figures", "summary": "Three-dimensional (3D) scene understanding in colonoscopy presents\nsignificant challenges that necessitate automated methods for accurate depth\nestimation. However, existing depth estimation models for endoscopy struggle\nwith temporal consistency across video sequences, limiting their applicability\nfor 3D reconstruction. We present ColonCrafter, a diffusion-based depth\nestimation model that generates temporally consistent depth maps from monocular\ncolonoscopy videos. Our approach learns robust geometric priors from synthetic\ncolonoscopy sequences to generate temporally consistent depth maps. We also\nintroduce a style transfer technique that preserves geometric structure while\nadapting real clinical videos to match our synthetic training domain.\nColonCrafter achieves state-of-the-art zero-shot performance on the C3VD\ndataset, outperforming both general-purpose and endoscopy-specific approaches.\nAlthough full trajectory 3D reconstruction remains a challenge, we demonstrate\nclinically relevant applications of ColonCrafter, including 3D point cloud\ngeneration and surface coverage assessment.", "AI": {"tldr": "ColonCrafter是一种基于扩散模型的结肠镜深度估计方法，它能从单目视频生成时间一致的深度图，并在C3VD数据集上取得了最先进的零样本性能。", "motivation": "结肠镜检查中的三维场景理解面临挑战，现有内窥镜深度估计模型在视频序列的时间一致性方面表现不佳，限制了其在三维重建中的应用。", "method": "本文提出了ColonCrafter，一个基于扩散的深度估计模型，通过从合成结肠镜序列中学习鲁棒的几何先验来生成时间一致的深度图。此外，还引入了一种风格迁移技术，用于将真实临床视频适应到合成训练域，同时保留几何结构。", "result": "ColonCrafter在C3VD数据集上实现了最先进的零样本性能，优于通用和内窥镜专用方法。虽然完整的轨迹三维重建仍具挑战，但该方法展示了临床相关的应用，包括三维点云生成和表面覆盖评估。", "conclusion": "ColonCrafter通过生成时间一致的深度图，显著提升了结肠镜检查中的深度估计能力，为三维点云生成和表面覆盖评估等临床应用提供了有效工具，尽管完整轨迹的三维重建仍需进一步研究。"}}
{"id": "2509.13761", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13761", "abs": "https://arxiv.org/abs/2509.13761", "authors": ["Qikai Chang", "Zhenrong Zhang", "Pengfei Hu", "Jiefeng Ma", "Yicheng Pan", "Jianshu Zhang", "Jun Du", "Quan Liu", "Jianqing Gao"], "title": "THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical Reasoning", "comment": "22 pages, 13 figures", "summary": "Large Language Models (LLMs) have made remarkable progress in mathematical\nreasoning, but still continue to struggle with high-precision tasks like\nnumerical computation and formal symbolic manipulation. Integrating external\ntools has emerged as a promising approach to bridge this gap. Despite recent\nadvances, existing methods struggle with three key challenges: constructing\ntool-integrated reasoning data, performing fine-grained optimization, and\nenhancing inference. To overcome these limitations, we propose THOR\n(Tool-Integrated Hierarchical Optimization via RL). First, we introduce TIRGen,\na multi-agent actor-critic-based pipeline for constructing high-quality\ndatasets of tool-integrated reasoning paths, aligning with the policy and\ngeneralizing well across diverse models. Second, to perform fine-grained\nhierarchical optimization, we introduce an RL strategy that jointly optimizes\nfor both trajectory-level problem solving and step-level code generation. This\nis motivated by our key insight that the success of an intermediate tool call\nis a strong predictor of the final answer's correctness. Finally, THOR\nincorporates a self-correction mechanism that leverages immediate tool feedback\nto dynamically revise erroneous reasoning paths during inference. Our approach\ndemonstrates strong generalization across diverse models, performing\neffectively in both reasoning and non-reasoning models. It further achieves\nstate-of-the-art performance for models of a similar scale on multiple\nmathematical benchmarks, while also delivering consistent improvements on code\nbenchmarks. Our code will be publicly available at\nhttps://github.com/JingMog/THOR.", "AI": {"tldr": "THOR (Tool-Integrated Hierarchical Optimization via RL) 提出了一种新的框架，通过多智能体数据生成、分层强化学习优化和推理时自校正机制，显著提升了大型语言模型在高精度数学和编程任务中的表现。", "motivation": "尽管大型语言模型在数学推理方面取得了显著进展，但在数值计算和形式符号操作等高精度任务上仍然存在困难。现有的工具集成方法在构建工具集成推理数据、执行细粒度优化和增强推理方面面临挑战。", "method": "THOR方法包含三个关键部分：1) TIRGen，一个基于多智能体actor-critic的管道，用于构建高质量的工具集成推理路径数据集。2) 一种分层强化学习策略，共同优化轨迹级问题解决和步骤级代码生成，利用中间工具调用的成功作为最终答案正确性的强预测因子。3) 一个自校正机制，利用即时工具反馈在推理过程中动态修正错误的推理路径。", "result": "THOR在各种模型（包括推理和非推理模型）上表现出强大的泛化能力。在多个数学基准测试中，它使同等规模的模型达到了最先进的性能，并在代码基准测试中持续实现改进。", "conclusion": "THOR通过创新的数据生成、分层优化和自校正机制，有效解决了大型语言模型在高精度数学和编程任务中的局限性，实现了显著的性能提升和广泛的泛化能力。"}}
{"id": "2509.13736", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13736", "abs": "https://arxiv.org/abs/2509.13736", "authors": ["Muyuan Ma", "Long Cheng", "Lijun Han", "Xiuze Xia", "Houcheng Li"], "title": "Motion Adaptation Across Users and Tasks for Exoskeletons via Meta-Learning", "comment": null, "summary": "Wearable exoskeletons can augment human strength and reduce muscle fatigue\nduring specific tasks. However, developing personalized and task-generalizable\nassistance algorithms remains a critical challenge. To address this, a\nmeta-imitation learning approach is proposed. This approach leverages a\ntask-specific neural network to predict human elbow joint movements, enabling\neffective assistance while enhancing generalization to new scenarios. To\naccelerate data collection, full-body keypoint motions are extracted from\npublicly available RGB video and motion-capture datasets across multiple tasks,\nand subsequently retargeted in simulation. Elbow flexion trajectories generated\nin simulation are then used to train the task-specific neural network within\nthe model-agnostic meta-learning (MAML) framework, which allows the network to\nrapidly adapt to novel tasks and unseen users with only a few gradient updates.\nThe adapted network outputs personalized references tracked by a\ngravity-compensated PD controller to ensure stable assistance. Experimental\nresults demonstrate that the exoskeleton significantly reduces both muscle\nactivation and metabolic cost for new users performing untrained tasks,\ncompared to performing without exoskeleton assistance. These findings suggest\nthat the proposed framework effectively improves task generalization and user\nadaptability for wearable exoskeleton systems.", "AI": {"tldr": "该研究提出了一种元模仿学习方法，利用MAML框架训练任务特异性神经网络，以实现可穿戴外骨骼对新任务和新用户的快速个性化适应，显著降低肌肉活动和代谢成本。", "motivation": "可穿戴外骨骼在增强人类力量和减少肌肉疲劳方面有潜力，但开发个性化和任务泛化性强的辅助算法仍是关键挑战。", "method": "提出了一种元模仿学习方法。该方法利用任务特异性神经网络预测人类肘关节运动。通过从公开RGB视频和动作捕捉数据集中提取全身关键点运动并在仿真中重定向，加速数据收集。利用仿真生成的肘部屈曲轨迹在模型无关元学习（MAML）框架下训练神经网络，使其能通过少量梯度更新快速适应新任务和未见过用户。适应后的网络输出个性化参考，由重力补偿PD控制器跟踪以确保稳定辅助。", "result": "实验结果表明，与没有外骨骼辅助相比，外骨骼显著降低了新用户执行未训练任务时的肌肉激活和代谢成本。", "conclusion": "研究结果表明，所提出的框架有效提高了可穿戴外骨骼系统的任务泛化能力和用户适应性。"}}
{"id": "2509.14065", "categories": ["eess.SY", "cs.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.14065", "abs": "https://arxiv.org/abs/2509.14065", "authors": ["Jaidev Gill", "Jing Shuang Li"], "title": "Identifying Network Structure of Linear Dynamical Systems: Observability and Edge Misclassification", "comment": "7 pages, 5 figures, in submission", "summary": "This work studies the limitations of uniquely identifying a linear network's\ntopology from partial measurements of its nodes. We show that the set of\nnetworks that are consistent with the measurements are related through the\nnullspace of the observability matrix for the true network. In doing so, we\nillustrate how potentially many networks are fully consistent with the\nmeasurements despite having topologies that are structurally inconsistent with\neach other, an often neglected consideration in the design of topology\ninference methods. We then provide an aggregate characterization of the space\nof possible networks by analytically solving for the most structurally\ndissimilar network. We find that when observing over 6% of nodes in random\nnetwork models (e.g., Erd\\H{o}s-R\\'{e}nyi and Watts-Strogatz) the rate of edge\nmisclassification drops to ~1%. Extending this discussion, we construct a\nfamily of networks that keep measurements $\\epsilon$-\"close\" to each other, and\nconnect the identifiability of these networks to the spectral properties of an\naugmented observability Gramian.", "AI": {"tldr": "本研究探讨了从部分节点测量中唯一识别线性网络拓扑的局限性，发现许多结构上不一致的网络可能与测量结果一致，并通过分析和测量覆盖率揭示了拓扑可识别性。", "motivation": "现有的拓扑推断方法常忽略一个关键问题：即使拓扑结构彼此不一致，许多网络仍可能与给定的部分测量结果完全一致。本研究旨在探讨这一被忽视的限制。", "method": "研究方法包括：1) 将与测量结果一致的网络集合与真实网络的观测矩阵的零空间相关联；2) 通过解析求解结构上差异最大的网络，对可能的网络空间进行聚合表征；3) 构建一系列使测量结果彼此“接近”的网络，并将其可识别性与增广可观测性Gramian的谱特性联系起来。", "result": "研究发现：1) 许多结构上不一致的网络可能与部分测量结果完全一致；2) 在随机网络模型（如Erdős-Rényi和Watts-Strogatz）中，当观测超过6%的节点时，边错分率降至约1%；3) 建立了测量结果“接近”的网络族，并将其可识别性与增广可观测性Gramian的谱特性相关联。", "conclusion": "从部分测量中唯一识别线性网络拓扑存在固有的局限性，因为可能存在多个结构不一致但与测量结果一致的网络。本研究通过量化这些限制，并揭示测量覆盖率和网络谱特性对拓扑可识别性的影响，为拓扑推断方法的设计提供了重要考虑。"}}
{"id": "2509.13869", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13869", "abs": "https://arxiv.org/abs/2509.13869", "authors": ["Yang Liu", "Chenhui Chu"], "title": "Do LLMs Align Human Values Regarding Social Biases? Judging and Explaining Social Biases with LLMs", "comment": "38 pages, 31 figures", "summary": "Large language models (LLMs) can lead to undesired consequences when\nmisaligned with human values, especially in scenarios involving complex and\nsensitive social biases. Previous studies have revealed the misalignment of\nLLMs with human values using expert-designed or agent-based emulated bias\nscenarios. However, it remains unclear whether the alignment of LLMs with human\nvalues differs across different types of scenarios (e.g., scenarios containing\nnegative vs. non-negative questions). In this study, we investigate the\nalignment of LLMs with human values regarding social biases (HVSB) in different\ntypes of bias scenarios. Through extensive analysis of 12 LLMs from four model\nfamilies and four datasets, we demonstrate that LLMs with large model parameter\nscales do not necessarily have lower misalignment rate and attack success rate.\nMoreover, LLMs show a certain degree of alignment preference for specific types\nof scenarios and the LLMs from the same model family tend to have higher\njudgment consistency. In addition, we study the understanding capacity of LLMs\nwith their explanations of HVSB. We find no significant differences in the\nunderstanding of HVSB across LLMs. We also find LLMs prefer their own generated\nexplanations. Additionally, we endow smaller language models (LMs) with the\nability to explain HVSB. The generation results show that the explanations\ngenerated by the fine-tuned smaller LMs are more readable, but have a\nrelatively lower model agreeability.", "AI": {"tldr": "本研究调查了大型语言模型（LLMs）在不同类型偏见情景下与人类社会偏见价值观（HVSB）的对齐情况。结果显示，模型规模不一定与低未对齐率相关，LLMs对特定情景类型存在对齐偏好，同系列模型判断一致性更高。此外，LLMs对HVSB的理解能力差异不显著，且倾向于使用自身生成的解释。通过微调，小型语言模型能生成可读性更高的HVSB解释，但模型认同度较低。", "motivation": "以往研究已揭示LLMs与人类价值观的未对齐问题，尤其是在涉及复杂敏感社会偏见的场景中。然而，目前尚不清楚LLMs与人类价值观的对齐程度是否会因情景类型（如负面与非负面问题情景）的不同而有所差异。", "method": "本研究对来自四个模型家族的12个LLMs和四个数据集进行了广泛分析。方法包括评估LLMs在不同类型偏见情景下的未对齐率和攻击成功率，分析模型判断的一致性，研究LLMs对HVSB的解释能力，并尝试通过微调使小型语言模型具备解释HVSB的能力。", "result": "研究发现，模型参数规模较大的LLMs不一定具有更低的未对齐率和攻击成功率。LLMs对特定类型的情景表现出一定程度的对齐偏好，且来自同一模型家族的LLMs倾向于具有更高的判断一致性。在HVSB理解能力方面，不同LLMs之间没有显著差异，且LLMs更偏好自身生成的解释。此外，微调后的小型语言模型生成的HVSB解释更具可读性，但模型认同度相对较低。", "conclusion": "LLMs与人类社会偏见价值观的对齐是一个复杂的问题，受模型规模、情景类型和模型家族等多种因素影响。模型规模并非对齐程度的决定性因素，且LLMs在不同情景下表现出对齐偏好。未来需要更深入地研究如何提升LLMs对HVSB的理解和解释能力，并提高小型模型在生成解释时的模型认同度，以更好地校准LLMs与人类价值观。"}}
{"id": "2509.13536", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13536", "abs": "https://arxiv.org/abs/2509.13536", "authors": ["Yinlong Bai", "Hongxin Zhang", "Sheng Zhong", "Junkai Niu", "Hai Li", "Yijia He", "Yi Zhou"], "title": "MemGS: Memory-Efficient Gaussian Splatting for Real-Time SLAM", "comment": null, "summary": "Recent advancements in 3D Gaussian Splatting (3DGS) have made a significant\nimpact on rendering and reconstruction techniques. Current research\npredominantly focuses on improving rendering performance and reconstruction\nquality using high-performance desktop GPUs, largely overlooking applications\nfor embedded platforms like micro air vehicles (MAVs). These devices, with\ntheir limited computational resources and memory, often face a trade-off\nbetween system performance and reconstruction quality. In this paper, we\nimprove existing methods in terms of GPU memory usage while enhancing rendering\nquality. Specifically, to address redundant 3D Gaussian primitives in SLAM, we\npropose merging them in voxel space based on geometric similarity. This reduces\nGPU memory usage without impacting system runtime performance. Furthermore,\nrendering quality is improved by initializing 3D Gaussian primitives via\nPatch-Grid (PG) point sampling, enabling more accurate modeling of the entire\nscene. Quantitative and qualitative evaluations on publicly available datasets\ndemonstrate the effectiveness of our improvements.", "AI": {"tldr": "本文提出了一种优化3D高斯泼溅（3DGS）的方法，旨在减少GPU内存使用并提高渲染质量，以适应微型飞行器（MAVs）等嵌入式平台的资源限制。", "motivation": "当前的3DGS研究主要集中于高性能桌面GPU，忽略了MAVs等计算和内存受限的嵌入式平台。这些平台在系统性能和重建质量之间面临权衡，现有方法未能有效解决此问题。", "method": "为解决SLAM中冗余的3D高斯基元问题，提出基于几何相似性在体素空间中合并它们，以减少GPU内存使用。为提高渲染质量，通过Patch-Grid (PG) 点采样初始化3D高斯基元，实现更精确的场景建模。", "result": "定量和定性评估表明，改进后的方法在不影响系统运行时性能的前提下，有效减少了GPU内存使用，并提高了渲染质量。在公开数据集上验证了其有效性。", "conclusion": "本文提出的方法通过合并冗余高斯基元和优化初始化过程，成功地为资源受限的嵌入式平台（如MAVs）优化了3DGS，实现了GPU内存的减少和渲染质量的提升。"}}
{"id": "2509.13773", "categories": ["cs.AI", "cs.IR", "I.2.7; I.2.10"], "pdf": "https://arxiv.org/pdf/2509.13773", "abs": "https://arxiv.org/abs/2509.13773", "authors": ["Zhipeng Bian", "Jieming Zhu", "Xuyang Xie", "Quanyu Dai", "Zhou Zhao", "Zhenhua Dong"], "title": "MIRA: Empowering One-Touch AI Services on Smartphones with MLLM-based Instruction Recommendation", "comment": "Published in Proceedings of the 63rd Annual Meeting of the\n  Association for Computational Linguistics (Volume 6: Industry Track), ACL\n  2025. Official version: https://doi.org/10.18653/v1/2025.acl-industry.103", "summary": "The rapid advancement of generative AI technologies is driving the\nintegration of diverse AI-powered services into smartphones, transforming how\nusers interact with their devices. To simplify access to predefined AI\nservices, this paper introduces MIRA, a pioneering framework for task\ninstruction recommendation that enables intuitive one-touch AI tasking on\nsmartphones. With MIRA, users can long-press on images or text objects to\nreceive contextually relevant instruction recommendations for executing AI\ntasks. Our work introduces three key innovations: 1) A multimodal large\nlanguage model (MLLM)-based recommendation pipeline with structured reasoning\nto extract key entities, infer user intent, and generate precise instructions;\n2) A template-augmented reasoning mechanism that integrates high-level\nreasoning templates, enhancing task inference accuracy; 3) A prefix-tree-based\nconstrained decoding strategy that restricts outputs to predefined instruction\ncandidates, ensuring coherent and intent-aligned suggestions. Through\nevaluation using a real-world annotated datasets and a user study, MIRA has\ndemonstrated substantial improvements in the accuracy of instruction\nrecommendation. The encouraging results highlight MIRA's potential to\nrevolutionize the way users engage with AI services on their smartphones,\noffering a more seamless and efficient experience.", "AI": {"tldr": "MIRA是一个创新的框架，旨在通过长按图像或文本对象，为智能手机上的AI任务提供直观的“一键式”指令推荐，利用多模态大语言模型、模板增强推理和前缀树约束解码。", "motivation": "生成式AI技术的快速发展导致AI驱动的服务被整合到智能手机中，改变了用户与设备的交互方式。研究的动机是简化用户对预定义AI服务的访问。", "method": "MIRA引入了三项关键创新：1) 基于多模态大语言模型（MLLM）的推荐管道，通过结构化推理提取关键实体、推断用户意图并生成精确指令；2) 模板增强推理机制，整合高级推理模板以提高任务推断准确性；3) 基于前缀树的约束解码策略，将输出限制为预定义的指令候选，确保建议的连贯性和意图对齐。", "result": "通过使用真实世界标注数据集和用户研究进行评估，MIRA在指令推荐的准确性方面表现出显著提升。", "conclusion": "MIRA的积极成果突显了其彻底改变用户在智能手机上与AI服务互动方式的潜力，提供更无缝和高效的体验。"}}
{"id": "2509.13737", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13737", "abs": "https://arxiv.org/abs/2509.13737", "authors": ["Renjie Wang", "Shangke Lyu", "Donglin Wang"], "title": "Dynamic Adaptive Legged Locomotion Policy via Decoupling Reaction Force Control and Gait Control", "comment": null, "summary": "While Reinforcement Learning (RL) has achieved remarkable progress in legged\nlocomotion control, it often suffers from performance degradation in\nout-of-distribution (OOD) conditions and discrepancies between the simulation\nand the real environments. Instead of mainly relying on domain randomization\n(DR) to best cover the real environments and thereby close the sim-to-real gap\nand enhance robustness, this work proposes an emerging decoupled framework that\nacquires fast online adaptation ability and mitigates the sim-to-real problems\nin unfamiliar environments by isolating stance-leg control and swing-leg\ncontrol. Various simulation and real-world experiments demonstrate its\neffectiveness against horizontal force disturbances, uneven terrains, heavy and\nbiased payloads, and sim-to-real gap.", "AI": {"tldr": "本文提出了一种解耦的强化学习框架，通过分离站立腿和摆动腿控制，以实现在线快速适应，从而解决腿式运动控制在分布外条件下的性能下降和仿真到现实差距问题。", "motivation": "传统的强化学习在腿式运动控制中，常因分布外条件和仿真与现实环境差异而导致性能下降。主要依赖领域随机化（DR）的方法在覆盖真实环境、缩小仿真到现实差距和增强鲁棒性方面仍有局限。", "method": "该研究提出了一种新兴的解耦框架，通过隔离站立腿控制和摆动腿控制，从而获得快速的在线适应能力，并在不熟悉的环境中缓解仿真到现实的问题。", "result": "各种仿真和真实世界实验表明，该方法在应对水平力干扰、不平坦地形、重载和偏载以及仿真到现实差距方面均表现出有效性。", "conclusion": "所提出的解耦框架能有效提升腿式机器人面对复杂和不熟悉环境时的适应性和鲁棒性，并有效弥合仿真到现实的鸿沟。"}}
{"id": "2509.14106", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.14106", "abs": "https://arxiv.org/abs/2509.14106", "authors": ["Yudong Li", "Yirui Cong", "Shimin Wang", "Martin Guay", "Jiuxiang Dong"], "title": "Asymptotic Boundedness of Distributed Set-Membership Filtering", "comment": null, "summary": "Asymptotic boundedness is a crucial property of Distributed Set-Membership\nFiltering (DSMFing) that prevents the unbounded growth of the set estimates\ncaused by the wrapping effect. However, this important property remains\nunderinvestigated, compared to its noise-free and stochastic-noise\ncounterparts, i.e., the convergence of Distributed Observers (DOs) and the\nbounded error covariance of Distributed Kalman Filters (DKFs). This paper\nstudies the asymptotic boundedness of DSMFing for linear discrete-time systems.\nA novel concept, termed the Collective Observation-Information Tower (COIT), is\nintroduced to characterize the fundamental relationship between the structure\nof graphs and the set estimates, which enables the boundedness analysis.\nLeveraging the COIT, an easily verifiable sufficient condition for the\nasymptotic boundedness of linear DSMFing is established. Surprisingly, the\nsufficient condition generalizes the well-known collective detectability\ncondition for DOs and DKFs; it links DSMFs to existing distributed estimation\nmethods and reveals the unique characteristic of DSMFs.", "AI": {"tldr": "本文研究了分布式集合成员滤波（DSMFing）的渐近有界性，引入了“集体观测信息塔”（COIT）概念，并基于此建立了一个易于验证的充分条件，该条件泛化了现有分布式估计方法的集体可检测性条件。", "motivation": "渐近有界性是DSMFing防止集合估计因包装效应而无界增长的关键特性，但与分布式观测器（DOs）和分布式卡尔曼滤波器（DKFs）的收敛性/有界误差协方差相比，该特性在DSMFing中研究不足。", "method": "引入了一个新概念，即“集体观测信息塔”（Collective Observation-Information Tower, COIT），用以描述图结构与集合估计之间的基本关系，从而进行有界性分析。", "result": "利用COIT，本文为线性离散时间DSMFing建立了渐近有界性的一个易于验证的充分条件。该条件出人意料地泛化了DOs和DKFs中众所周知的集体可检测性条件。", "conclusion": "本文为线性DSMFing的渐近有界性提供了理论分析，通过COIT建立了图结构与集合估计的联系，并提出了一个泛化现有分布式估计方法条件的充分条件，揭示了DSMFs的独特特性。"}}
{"id": "2509.13879", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.13879", "abs": "https://arxiv.org/abs/2509.13879", "authors": ["Mariano Barone", "Antonio Romano", "Giuseppe Riccio", "Marco Postiglione", "Vincenzo Moscato"], "title": "Combining Evidence and Reasoning for Biomedical Fact-Checking", "comment": "Proceedings of the 48th International ACM SIGIR Conference on\n  Research and Development in Information Retrieval, 2025", "summary": "Misinformation in healthcare, from vaccine hesitancy to unproven treatments,\nposes risks to public health and trust in medical systems. While machine\nlearning and natural language processing have advanced automated fact-checking,\nvalidating biomedical claims remains uniquely challenging due to complex\nterminology, the need for domain expertise, and the critical importance of\ngrounding in scientific evidence. We introduce CER (Combining Evidence and\nReasoning), a novel framework for biomedical fact-checking that integrates\nscientific evidence retrieval, reasoning via large language models, and\nsupervised veracity prediction. By integrating the text-generation capabilities\nof large language models with advanced retrieval techniques for high-quality\nbiomedical scientific evidence, CER effectively mitigates the risk of\nhallucinations, ensuring that generated outputs are grounded in verifiable,\nevidence-based sources. Evaluations on expert-annotated datasets (HealthFC,\nBioASQ-7b, SciFact) demonstrate state-of-the-art performance and promising\ncross-dataset generalization. Code and data are released for transparency and\nreproducibility: https: //github.com/PRAISELab-PicusLab/CER.", "AI": {"tldr": "本文提出CER（结合证据与推理）框架，通过整合科学证据检索、大型语言模型推理和监督真实性预测，实现生物医学领域的事实核查，有效减轻幻觉风险并取得最先进的性能。", "motivation": "医疗保健领域的错误信息（如疫苗犹豫、未经证实疗法）对公众健康和医疗系统信任构成威胁。尽管机器学习和自然语言处理在自动化事实核查方面取得进展，但生物医学声明的验证因术语复杂、需领域专业知识及依赖科学证据而面临独特挑战。", "method": "本文引入CER框架，用于生物医学事实核查。该框架整合了科学证据检索、通过大型语言模型进行的推理，以及监督下的真实性预测。通过结合大型语言模型的文本生成能力与先进的高质量生物医学科学证据检索技术，CER有效缓解了幻觉风险，确保生成输出基于可验证的、有证据支持的来源。", "result": "在专家标注的数据集（HealthFC, BioASQ-7b, SciFact）上的评估表明，CER框架取得了最先进的性能，并展示了良好的跨数据集泛化能力。", "conclusion": "CER是一个新颖且有效的生物医学事实核查框架，它通过整合证据检索和大型语言模型推理，成功应对了该领域的独特挑战，并显著提升了事实核查的准确性和可靠性，为公共卫生和医疗信任提供了支持。"}}
{"id": "2509.13577", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13577", "abs": "https://arxiv.org/abs/2509.13577", "authors": ["Tongfei Guo", "Lili Su"], "title": "Dynamic Aware: Adaptive Multi-Mode Out-of-Distribution Detection for Trajectory Prediction in Autonomous Vehicles", "comment": "8 pages, 7 figures", "summary": "Trajectory prediction is central to the safe and seamless operation of\nautonomous vehicles (AVs). In deployment, however, prediction models inevitably\nface distribution shifts between training data and real-world conditions, where\nrare or underrepresented traffic scenarios induce out-of-distribution (OOD)\ncases. While most prior OOD detection research in AVs has concentrated on\ncomputer vision tasks such as object detection and segmentation,\ntrajectory-level OOD detection remains largely underexplored. A recent study\nformulated this problem as a quickest change detection (QCD) task, providing\nformal guarantees on the trade-off between detection delay and false alarms\n[1]. Building on this foundation, we propose a new framework that introduces\nadaptive mechanisms to achieve robust detection in complex driving\nenvironments. Empirical analysis across multiple real-world datasets reveals\nthat prediction errors -- even on in-distribution samples -- exhibit\nmode-dependent distributions that evolve over time with dataset-specific\ndynamics. By explicitly modeling these error modes, our method achieves\nsubstantial improvements in both detection delay and false alarm rates.\nComprehensive experiments on established trajectory prediction benchmarks show\nthat our framework significantly outperforms prior UQ- and vision-based OOD\napproaches in both accuracy and computational efficiency, offering a practical\npath toward reliable, driving-aware autonomy.", "AI": {"tldr": "针对自动驾驶轨迹预测中的OOD（分布外）检测问题，本文提出了一种新的自适应框架。该框架通过显式建模预测误差的模式，显著提升了OOD检测的延迟和误报率，并在准确性和计算效率上优于现有方法。", "motivation": "自动驾驶车辆（AVs）的安全运行依赖于准确的轨迹预测。然而，预测模型在实际部署中不可避免地面临训练数据与真实世界条件之间的分布偏移，导致稀有或未充分表示的交通场景产生OOD情况。现有OOD检测研究大多集中在计算机视觉任务，而轨迹层面的OOD检测仍未被充分探索。", "method": "本研究基于快速变化检测（QCD）任务的框架，引入了自适应机制以在复杂驾驶环境中实现鲁棒检测。通过对多个真实世界数据集的经验分析发现，预测误差（即使是分布内样本）也表现出模式依赖的分布，并随时间动态演变。本文的方法通过显式建模这些误差模式来提升检测性能。", "result": "研究发现预测误差存在模式依赖的分布，且随时间具有数据集特定的动态演变。所提出的方法在检测延迟和误报率方面均取得了显著改善。在既定的轨迹预测基准测试中，该框架在准确性和计算效率上均显著优于先前的基于不确定性量化（UQ）和基于视觉的OOD方法。", "conclusion": "本框架为自动驾驶轨迹预测中的OOD检测提供了一条实用且有效的途径，能够实现更可靠、更具驾驶感知的自主性。"}}
{"id": "2509.13880", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13880", "abs": "https://arxiv.org/abs/2509.13880", "authors": ["Mingwei Zhang", "Zhenhao Gu", "Liangda Fang", "Cunjing Ge", "Ziliang Chen", "Zhao-Rong Lai", "Quanlong Guan"], "title": "An Exhaustive DPLL Approach to Model Counting over Integer Linear Constraints with Simplification Techniques", "comment": null, "summary": "Linear constraints are one of the most fundamental constraints in fields such\nas computer science, operations research and optimization. Many applications\nreduce to the task of model counting over integer linear constraints (MCILC).\nIn this paper, we design an exact approach to MCILC based on an exhaustive DPLL\narchitecture. To improve the efficiency, we integrate several effective\nsimplification techniques from mixed integer programming into the architecture.\nWe compare our approach to state-of-the-art MCILC counters and propositional\nmodel counters on 2840 random and 4131 application benchmarks. Experimental\nresults show that our approach significantly outperforms all exact methods in\nrandom benchmarks solving 1718 instances while the state-of-the-art approach\nonly computes 1470 instances. In addition, our approach is the only approach to\nsolve all 4131 application instances.", "AI": {"tldr": "本文提出了一种基于穷举DPLL架构并结合混合整数规划简化技术的整数线性约束模型计数（MCILC）的精确方法，实验证明其在随机和应用基准测试上均显著优于现有最先进方法。", "motivation": "整数线性约束模型计数（MCILC）是计算机科学、运筹学和优化等领域中的一个基本任务，具有广泛的应用，因此需要一个精确且高效的解决方案。", "method": "本文设计了一种基于穷举DPLL架构的MCILC精确方法，并为了提高效率，将混合整数规划中的多种有效简化技术集成到该架构中。", "result": "在2840个随机基准测试中，本文方法解决了1718个实例，而最先进的方法仅解决了1470个，表现显著优于所有精确方法。此外，本文方法是唯一能够解决所有4131个应用实例的方法。", "conclusion": "本文提出的基于DPLL和MIP简化技术的MCILC精确方法在处理随机和应用基准测试方面，相比现有最先进方法展现出显著的性能提升和更强的鲁棒性。"}}
{"id": "2509.13771", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13771", "abs": "https://arxiv.org/abs/2509.13771", "authors": ["Mengzhu Li", "Yunyu Zhou", "He Ying", "F. Richard Yu"], "title": "CDFlow: Generative Gradient Flows for Configuration Space Distance Fields via Neural ODEs", "comment": null, "summary": "Signed Distance Fields (SDFs) are a fundamental representation in robot\nmotion planning. Their configuration-space counterpart, the Configuration Space\nDistance Field (CDF), directly encodes distances in joint space, offering a\nunified representation for optimization and control. However, existing CDF\nformulations face two major challenges in high-degree-of-freedom (DoF) robots:\n(1) they effectively return only a single nearest collision configuration,\nneglecting the multi-modal nature of minimal-distance collision configurations\nand leading to gradient ambiguity; and (2) they rely on sparse sampling of the\ncollision boundary, which often fails to identify the true closest\nconfigurations, producing oversmoothed approximations and geometric distortion\nin high-dimensional spaces. We propose CDFlow, a novel framework that addresses\nthese limitations by learning a continuous flow in configuration space via\nNeural Ordinary Differential Equations (Neural ODEs). We redefine the problem\nfrom finding a single nearest point to modeling the distribution of\nminimal-distance collision configurations. We also introduce an adaptive\nrefinement sampling strategy to generate high-fidelity training data for this\ndistribution. The resulting Neural ODE implicitly models this multi-modal\ndistribution and produces a smooth, consistent gradient field-derived as the\nexpected direction towards the distribution-that mitigates gradient ambiguity\nand preserves sharp geometric features. Extensive experiments on high-DoF\nmotion planning tasks demonstrate that CDFlow significantly improves planning\nefficiency, trajectory quality, and robustness compared to existing CDF-based\nmethods, enabling more robust and efficient planning for collision-aware robots\nin complex environments.", "AI": {"tldr": "现有配置空间距离场（CDF）在处理高自由度机器人时存在梯度模糊和几何失真问题。本文提出CDFlow，通过神经常微分方程（Neural ODEs）学习配置空间中的连续流，并建模最小距离碰撞配置的多模态分布，显著提高了运动规划的效率和质量。", "motivation": "现有CDF公式在处理高自由度（DoF）机器人时面临两大挑战：1) 它们只返回一个最近的碰撞配置，忽略了最小距离碰撞配置的多模态性质，导致梯度模糊；2) 它们依赖稀疏采样碰撞边界，在高维空间中无法识别真实的最近配置，产生过平滑近似和几何失真。", "method": "本文提出CDFlow框架，通过以下方法解决上述限制：1) 利用神经常微分方程（Neural ODEs）学习配置空间中的连续流；2) 将问题重新定义为建模最小距离碰撞配置的分布，而非寻找单一最近点；3) 引入自适应细化采样策略，为该分布生成高保真训练数据。由此产生的Neural ODE隐式建模这种多模态分布，并生成平滑、一致的梯度场，从而缓解梯度模糊并保留尖锐的几何特征。", "result": "在多自由度运动规划任务上的大量实验表明，与现有基于CDF的方法相比，CDFlow显著提高了规划效率、轨迹质量和鲁棒性。", "conclusion": "CDFlow能够为复杂环境中具有碰撞感知能力的机器人实现更鲁棒、更高效的规划，因为它解决了现有CDF在处理高自由度机器人时面临的挑战，通过建模多模态碰撞分布并生成平滑一致的梯度场。"}}
{"id": "2509.14121", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.14121", "abs": "https://arxiv.org/abs/2509.14121", "authors": ["Marco A. Gomez", "Christopher D. Cruz-Ancona"], "title": "Safe Sliding Mode Control in Position for Double Integrator Systems", "comment": "6 pages, 3 figures, accepted at 2025 22 th International Conference\n  on Electrical Engineering, Computing Science and Automatic Control", "summary": "We address the problem of robust safety control design for double integrator\nsystems. We show that, when the constraints are defined only on position\nstates, it is possible to construct a safe sliding domain from the dynamic of a\nsimple integrator that is already safe. On this domain, the closed-loop\ntrajectories remain robust and safe against uncertainties and disturbances.\nFurthermore, we design a controller gain that guarantees convergence to the\nsafe sliding domain while avoiding the given unsafe set. The concept is\ninitially developed for first-order sliding mode and is subsequently\ngeneralized to an adaptive framework, ensuring that trajectories remain\nconfined to a predefined vicinity of the sliding domain, outside the unsafe\nregion.", "AI": {"tldr": "本文提出了一种针对双积分器系统的鲁棒安全控制设计方法，通过构建基于安全单积分器动态的安全滑模域，并设计控制器增益，确保系统轨迹在不确定性和扰动下保持安全，且能收敛到安全域并避免非安全集，该方法可扩展至自适应框架。", "motivation": "研究动机是解决双积分器系统在存在不确定性和扰动的情况下，如何设计鲁棒安全控制器，特别是当约束仅限于位置状态时。", "method": "该研究通过以下方法实现：1) 从已安全的简单积分器动态中构建一个安全滑模域；2) 设计控制器增益，以确保系统轨迹收敛到该安全滑模域，同时避开给定的非安全集；3) 该概念首先应用于一阶滑模，随后推广到自适应框架。", "result": "主要结果包括：1) 在所构建的安全滑模域内，闭环轨迹对不确定性和扰动保持鲁棒和安全；2) 控制器增益保证了轨迹收敛到安全滑模域并避免非安全集；3) 在自适应框架下，轨迹被限制在滑模域的预定义邻域内，且位于非安全区域之外。", "conclusion": "该研究为双积分器系统的鲁棒安全控制提供了一种有效设计方法，通过利用安全滑模域和控制器增益，确保系统在不确定性和扰动下始终保持安全并避免非安全区域，且该方法具有可推广性，可应用于自适应控制场景。"}}
{"id": "2509.13888", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.13888", "abs": "https://arxiv.org/abs/2509.13888", "authors": ["Mariano Barone", "Antonio Romano", "Giuseppe Riccio", "Marco Postiglione", "Vincenzo Moscato"], "title": "Combating Biomedical Misinformation through Multi-modal Claim Detection and Evidence-based Verification", "comment": null, "summary": "Misinformation in healthcare, from vaccine hesitancy to unproven treatments,\nposes risks to public health and trust in medical systems. While machine\nlearning and natural language processing have advanced automated fact-checking,\nvalidating biomedical claims remains uniquely challenging due to complex\nterminology, the need for domain expertise, and the critical importance of\ngrounding in scientific evidence. We introduce CER (Combining Evidence and\nReasoning), a novel framework for biomedical fact-checking that integrates\nscientific evidence retrieval, reasoning via large language models, and\nsupervised veracity prediction. By integrating the text-generation capabilities\nof large language models with advanced retrieval techniques for high-quality\nbiomedical scientific evidence, CER effectively mitigates the risk of\nhallucinations, ensuring that generated outputs are grounded in verifiable,\nevidence-based sources. Evaluations on expert-annotated datasets (HealthFC,\nBioASQ-7b, SciFact) demonstrate state-of-the-art performance and promising\ncross-dataset generalization. Code and data are released for transparency and\nreproducibility: https://github.com/PRAISELab-PicusLab/CER", "AI": {"tldr": "该研究引入了CER框架，一个结合科学证据检索、大型语言模型推理和监督真实性预测的生物医学事实核查系统，以有效对抗医疗健康领域的错误信息。", "motivation": "医疗健康领域的错误信息（如疫苗犹豫、未经证实疗法）对公众健康和医疗系统信任构成威胁。尽管机器学习和自然语言处理在自动化事实核查方面有所进展，但由于术语复杂性、领域专业知识需求以及对科学证据的严格依赖，验证生物医学主张仍面临独特挑战。", "method": "该研究提出了CER（Combining Evidence and Reasoning）框架，该框架整合了科学证据检索、通过大型语言模型进行的推理，以及监督下的真实性预测。通过结合大型语言模型的文本生成能力与先进的检索技术来获取高质量生物医学科学证据，CER有效降低了“幻觉”风险，确保生成输出基于可验证的、有证据支持的来源。", "result": "在专家标注的数据集（HealthFC、BioASQ-7b、SciFact）上的评估表明，CER实现了最先进的性能，并展现出良好的跨数据集泛化能力。代码和数据已发布以提高透明度和可复现性。", "conclusion": "CER框架通过整合证据检索和大型语言模型推理，提供了一种有效且可靠的生物医学事实核查方法，能够生成基于科学证据的、可验证的输出，从而有助于应对医疗健康领域的错误信息挑战。"}}
{"id": "2509.13586", "categories": ["cs.CV", "cs.CL", "cs.IR", "cs.MM", "I.2; I.4; I.7; H.3"], "pdf": "https://arxiv.org/pdf/2509.13586", "abs": "https://arxiv.org/abs/2509.13586", "authors": ["Nathalie Neptune", "Josiane Mothe"], "title": "Annotating Satellite Images of Forests with Keywords from a Specialized Corpus in the Context of Change Detection", "comment": null, "summary": "The Amazon rain forest is a vital ecosystem that plays a crucial role in\nregulating the Earth's climate and providing habitat for countless species.\nDeforestation in the Amazon is a major concern as it has a significant impact\non global carbon emissions and biodiversity. In this paper, we present a method\nfor detecting deforestation in the Amazon using image pairs from Earth\nobservation satellites. Our method leverages deep learning techniques to\ncompare the images of the same area at different dates and identify changes in\nthe forest cover. We also propose a visual semantic model that automatically\nannotates the detected changes with relevant keywords. The candidate annotation\nfor images are extracted from scientific documents related to the Amazon\nregion. We evaluate our approach on a dataset of Amazon image pairs and\ndemonstrate its effectiveness in detecting deforestation and generating\nrelevant annotations. Our method provides a useful tool for monitoring and\nstudying the impact of deforestation in the Amazon. While we focus on\nenvironment applications of our work by using images of deforestation in the\nAmazon rain forest to demonstrate the effectiveness of our proposed approach,\nit is generic enough to be applied to other domains.", "AI": {"tldr": "本文提出了一种利用深度学习和卫星图像对亚马逊雨林进行森林砍伐检测的方法，并能自动生成语义注释，为环境监测提供了通用工具。", "motivation": "亚马逊雨林是地球气候调节和生物多样性的关键生态系统。森林砍伐是一个重大问题，对全球碳排放和生物多样性产生显著影响。因此，需要有效的工具来监测和研究亚马逊地区的森林砍伐。", "method": "该方法使用地球观测卫星的图像对，利用深度学习技术比较不同日期的同一区域图像，以识别森林覆盖的变化。此外，还提出了一个视觉语义模型，利用从亚马逊相关科学文献中提取的关键词，自动注释检测到的变化。", "result": "该方法在亚马逊图像对数据集上进行了评估，并证明了其在检测森林砍伐和生成相关注释方面的有效性。实验结果表明该方法能够成功识别森林覆盖变化并提供有意义的语义信息。", "conclusion": "本文提出的方法为监测和研究亚马逊地区森林砍伐的影响提供了一个有用的工具。尽管其主要应用是亚马逊雨林的森林砍伐检测，但该方法具有足够的通用性，可以应用于其他领域。"}}
{"id": "2509.13968", "categories": ["cs.AI", "cs.CL", "cs.FL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13968", "abs": "https://arxiv.org/abs/2509.13968", "authors": ["Konstantinos Voudouris", "Andrew Barron", "Marta Halina", "Colin Klein", "Matishalin Patel"], "title": "Exploring Major Transitions in the Evolution of Biological Cognition With Artificial Neural Networks", "comment": null, "summary": "Transitional accounts of evolution emphasise a few changes that shape what is\nevolvable, with dramatic consequences for derived lineages. More recently it\nhas been proposed that cognition might also have evolved via a series of major\ntransitions that manipulate the structure of biological neural networks,\nfundamentally changing the flow of information. We used idealised models of\ninformation flow, artificial neural networks (ANNs), to evaluate whether\nchanges in information flow in a network can yield a transitional change in\ncognitive performance. We compared networks with feed-forward, recurrent and\nlaminated topologies, and tested their performance learning artificial grammars\nthat differed in complexity, controlling for network size and resources. We\ndocumented a qualitative expansion in the types of input that recurrent\nnetworks can process compared to feed-forward networks, and a related\nqualitative increase in performance for learning the most complex grammars. We\nalso noted how the difficulty in training recurrent networks poses a form of\ntransition barrier and contingent irreversibility -- other key features of\nevolutionary transitions. Not all changes in network topology confer a\nperformance advantage in this task set. Laminated networks did not outperform\nnon-laminated networks in grammar learning. Overall, our findings show how some\nchanges in information flow can yield transitions in cognitive performance.", "AI": {"tldr": "本研究使用人工神经网络模型，探索信息流的变化如何导致认知表现的质变，发现循环网络在处理复杂语法方面具有显著优势，并揭示了训练难度作为一种过渡障碍。", "motivation": "进化论强调少数关键变化塑造了可进化性，并对后代产生深远影响。最近有观点提出，认知也可能通过一系列重大转变而演化，这些转变通过改变生物神经网络的结构来操纵信息流。本研究旨在评估网络中信息流的变化是否能导致认知表现的过渡性变化。", "method": "研究使用了理想化的信息流模型——人工神经网络（ANNs）。比较了具有前馈、循环和层状拓扑结构的神经网络，并测试它们学习不同复杂程度的人工语法的性能，同时控制了网络大小和资源。", "result": "与前馈网络相比，循环网络在可处理的输入类型上实现了质的扩展，并且在学习最复杂语法时表现出质的提升。研究还指出，训练循环网络的难度构成了过渡障碍和偶然不可逆性，这些是进化过渡的关键特征。并非所有网络拓扑结构的变化都能带来性能优势，例如层状网络在语法学习中并未优于非层状网络。", "conclusion": "研究结果表明，网络中信息流的某些变化确实可以导致认知性能的过渡性变化。"}}
{"id": "2509.13774", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13774", "abs": "https://arxiv.org/abs/2509.13774", "authors": ["Piaopiao Jin", "Qi Wang", "Guokang Sun", "Ziwen Cai", "Pinjia He", "Yangwei You"], "title": "Dual-Actor Fine-Tuning of VLA Models: A Talk-and-Tweak Human-in-the-Loop Approach", "comment": null, "summary": "Vision-language-action (VLA) models demonstrate strong generalization in\nrobotic manipulation but face challenges in complex, real-world tasks. While\nsupervised fine-tuning with demonstrations is constrained by data quality,\nreinforcement learning (RL) offers a promising alternative. We propose a\nhuman-in-the-loop dual-actor fine-tuning framework grounded in RL. The\nframework integrates a primary actor for robust multi-task performance with a\nrefinement actor for latent-space adaptation. Beyond standard physical\ninterventions, we introduce a lightweight talk-and-tweak scheme that converts\nhuman corrections into semantically grounded language commands, thereby\ngenerating a new dataset for policy learning. In real-world multi-task\nexperiments, our approach achieves 100% success across three tasks within 101\nminutes of online fine-tuning. For long-horizon tasks, it sustains a 50%\nsuccess rate over 12 consecutive operations. Furthermore, the framework scales\neffectively to multi-robot training, achieving up to a 2 times improvement in\nefficiency when using dual robots. The experiment videos are available at\nhttps://sites.google.com/view/hil-daft/.", "AI": {"tldr": "本文提出了一种基于强化学习的人机协作双演员微调框架，用于提升视觉-语言-动作（VLA）模型在复杂机器人操作任务中的性能和效率，通过“对话与微调”机制将人类修正转化为语言指令。", "motivation": "VLA模型在机器人操作中泛化能力强，但在复杂真实世界任务中面临挑战。监督微调受数据质量限制，而强化学习提供了一个有前景的替代方案。", "method": "提出了一种基于强化学习的人机协作双演员微调框架。该框架包含一个用于鲁棒多任务性能的主演员和一个用于潜在空间适应的细化演员。除了物理干预，还引入了一种轻量级的“对话与微调”方案，将人类修正转换为语义接地的语言指令，从而生成新的策略学习数据集。", "result": "在真实世界多任务实验中，该方法在101分钟的在线微调内，在三个任务上实现了100%的成功率。对于长程任务，它在连续12次操作中保持了50%的成功率。此外，该框架能有效扩展到多机器人训练，使用双机器人时效率提高了2倍。", "conclusion": "该框架通过人机协作的强化学习和新颖的修正方案，显著提高了VLA模型在复杂真实世界机器人操作任务中的成功率和训练效率，并能有效扩展到多机器人场景。"}}
{"id": "2509.14168", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.14168", "abs": "https://arxiv.org/abs/2509.14168", "authors": ["Walden Marshall"], "title": "Factored Output Feedback Controller Synthesis with Locality Constraints for Spatially-Invariant Systems", "comment": "9 pages, 2 figures", "summary": "We consider H2 output feedback controller synthesis with pre-specified\nconstraints on spatial communication distance (locality) for\nspatially-invariant systems using two factored controller frameworks: the\nsystem-level parameterization and the input-output parameterization. In our\nmain result, we show that in both frameworks, output feedback controller\nsynthesis with locality constraints can be formulated as a convex problem in\nfinitely many transfer function variables, admitting the use of standard\nnumerical solution techniques. The number of decision variables in the optimal\ncontroller design problem scales linearly with the distance of allowed\ncommunication. We also show that the optimal controller design problems for the\nsystem-level and input-ouptput parameterizations are equivalent for the chosen\nsystem of interest. We present numerical examples to illustrate the tradeoff\nbetween communication sparsity and performance.", "AI": {"tldr": "本文研究了空间不变系统在预设空间通信距离（局部性）约束下的H2输出反馈控制器综合问题，并使用两种分解控制器框架（系统级参数化和输入-输出参数化）将其表述为有限个传递函数变量的凸问题。", "motivation": "为空间不变系统设计H2输出反馈控制器时，需要考虑对空间通信距离（即局部性）的预设约束，以实现通信稀疏性。", "method": "研究采用了两种分解控制器框架：系统级参数化（System-Level Parameterization, SLP）和输入-输出参数化（Input-Output Parameterization, IOP）。核心方法是将带有局部性约束的输出反馈控制器综合问题表述为一个有限个传递函数变量的凸优化问题。", "result": "研究表明，在SLP和IOP两种框架下，带有局部性约束的输出反馈控制器综合问题均可被公式化为有限个传递函数变量的凸问题，从而可以使用标准的数值求解技术。决策变量的数量与允许的通信距离呈线性关系。此外，对于所选系统，SLP和IOP的优化控制器设计问题是等价的。数值示例也展示了通信稀疏性与性能之间的权衡。", "conclusion": "通过将带有局部性约束的H2输出反馈控制器综合问题转化为凸问题，本研究为空间不变系统提供了一种可行的、计算效率高的方法。同时，验证了两种主流控制器参数化框架在此特定问题上的等价性，并揭示了通信稀疏性与系统性能之间的内在权衡。"}}
{"id": "2509.13905", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13905", "abs": "https://arxiv.org/abs/2509.13905", "authors": ["Domenico Meconi", "Simone Stirpe", "Federico Martelli", "Leonardo Lavalle", "Roberto Navigli"], "title": "Do Large Language Models Understand Word Senses?", "comment": "20 pages, to be published in EMNLP2025", "summary": "Understanding the meaning of words in context is a fundamental capability for\nLarge Language Models (LLMs). Despite extensive evaluation efforts, the extent\nto which LLMs show evidence that they truly grasp word senses remains\nunderexplored. In this paper, we address this gap by evaluating both i) the\nWord Sense Disambiguation (WSD) capabilities of instruction-tuned LLMs,\ncomparing their performance to state-of-the-art systems specifically designed\nfor the task, and ii) the ability of two top-performing open- and closed-source\nLLMs to understand word senses in three generative settings: definition\ngeneration, free-form explanation, and example generation. Notably, we find\nthat, in the WSD task, leading models such as GPT-4o and DeepSeek-V3 achieve\nperformance on par with specialized WSD systems, while also demonstrating\ngreater robustness across domains and levels of difficulty. In the generation\ntasks, results reveal that LLMs can explain the meaning of words in context up\nto 98\\% accuracy, with the highest performance observed in the free-form\nexplanation task, which best aligns with their generative capabilities.", "AI": {"tldr": "本研究评估了大型语言模型（LLMs）的词义消歧（WSD）能力及其在生成式任务中对词义的理解。结果表明，领先的LLMs在WSD任务上能与专用系统媲美，并在生成任务中表现出高达98%的准确率。", "motivation": "尽管LLMs得到了广泛评估，但它们对词义的真正理解程度仍未被充分探索。本研究旨在填补这一空白，深入评估LLMs的词义理解能力。", "method": "本研究采用两种方法进行评估：i) 评估指令调优LLMs的词义消歧（WSD）能力，并与现有最先进的专用WSD系统进行比较；ii) 评估两款顶级开源和闭源LLMs在三种生成式任务（定义生成、自由形式解释、示例生成）中理解词义的能力。", "result": "在WSD任务中，GPT-4o和DeepSeek-V3等领先模型表现与专用WSD系统相当，并在不同领域和难度级别上展现出更强的鲁棒性。在生成任务中，LLMs解释上下文词义的准确率高达98%，其中自由形式解释任务表现最佳，这与其生成能力最为契合。", "conclusion": "LLMs，特别是顶尖模型，在词义消歧和生成任务中都展现出强大的词义理解能力。它们在WSD任务上能达到或超越专用系统的性能，并在生成任务中能以高准确率解释词义，表明其对词义有深入的掌握。"}}
{"id": "2509.13590", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13590", "abs": "https://arxiv.org/abs/2509.13590", "authors": ["Samer Al-Hamadani"], "title": "Intelligent Healthcare Imaging Platform An VLM-Based Framework for Automated Medical Image Analysis and Clinical Report Generation", "comment": "32 pages, 14 figures, 6 tables", "summary": "The rapid advancement of artificial intelligence (AI) in healthcare imaging\nhas revolutionized diagnostic medicine and clinical decision-making processes.\nThis work presents an intelligent multimodal framework for medical image\nanalysis that leverages Vision-Language Models (VLMs) in healthcare\ndiagnostics. The framework integrates Google Gemini 2.5 Flash for automated\ntumor detection and clinical report generation across multiple imaging\nmodalities including CT, MRI, X-ray, and Ultrasound. The system combines visual\nfeature extraction with natural language processing to enable contextual image\ninterpretation, incorporating coordinate verification mechanisms and\nprobabilistic Gaussian modeling for anomaly distribution. Multi-layered\nvisualization techniques generate detailed medical illustrations, overlay\ncomparisons, and statistical representations to enhance clinical confidence,\nwith location measurement achieving 80 pixels average deviation. Result\nprocessing utilizes precise prompt engineering and textual analysis to extract\nstructured clinical information while maintaining interpretability.\nExperimental evaluations demonstrated high performance in anomaly detection\nacross multiple modalities. The system features a user-friendly Gradio\ninterface for clinical workflow integration and demonstrates zero-shot learning\ncapabilities to reduce dependence on large datasets. This framework represents\na significant advancement in automated diagnostic support and radiological\nworkflow efficiency, though clinical validation and multi-center evaluation are\nnecessary prior to widespread adoption.", "AI": {"tldr": "该研究提出了一种智能多模态医学图像分析框架，利用Vision-Language Models (VLMs) 和Google Gemini 2.5 Flash进行肿瘤检测和临床报告生成，适用于多种成像模态，并展示了零样本学习能力和高异常检测性能。", "motivation": "人工智能在医疗影像领域的快速发展正在彻底改变诊断医学和临床决策过程，因此需要一个智能的多模态框架来进一步提升医疗诊断能力。", "method": "该框架集成了Google Gemini 2.5 Flash，用于跨CT、MRI、X射线和超声等多种成像模态的自动化肿瘤检测和临床报告生成。它结合了视觉特征提取和自然语言处理，实现上下文图像解释，并融入了坐标验证机制和用于异常分布的概率高斯建模。多层可视化技术用于生成详细的医学插图、叠加比较和统计表示。结果处理利用精确的提示工程和文本分析来提取结构化临床信息，并提供了用户友好的Gradio界面和零样本学习能力。", "result": "实验评估表明，该系统在多种模态下均表现出高异常检测性能。位置测量实现了80像素的平均偏差。系统通过零样本学习能力减少了对大型数据集的依赖，并能生成详细的医学插图、叠加比较和统计表示以增强临床信心。", "conclusion": "该框架代表了自动化诊断支持和放射学工作流程效率的重大进步。然而，在广泛采用之前，仍需要进行临床验证和多中心评估。"}}
{"id": "2509.14030", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14030", "abs": "https://arxiv.org/abs/2509.14030", "authors": ["Maosheng Qin", "Renyu Zhu", "Mingxuan Xia", "Chenkai Chen", "Zhen Zhu", "Minmin Lin", "Junbo Zhao", "Lu Xu", "Changjie Fan", "Runze Wu", "Haobo Wang"], "title": "CrowdAgent: Multi-Agent Managed Multi-Source Annotation System", "comment": null, "summary": "High-quality annotated data is a cornerstone of modern Natural Language\nProcessing (NLP). While recent methods begin to leverage diverse annotation\nsources-including Large Language Models (LLMs), Small Language Models (SLMs),\nand human experts-they often focus narrowly on the labeling step itself. A\ncritical gap remains in the holistic process control required to manage these\nsources dynamically, addressing complex scheduling and quality-cost trade-offs\nin a unified manner. Inspired by real-world crowdsourcing companies, we\nintroduce CrowdAgent, a multi-agent system that provides end-to-end process\ncontrol by integrating task assignment, data annotation, and quality/cost\nmanagement. It implements a novel methodology that rationally assigns tasks,\nenabling LLMs, SLMs, and human experts to advance synergistically in a\ncollaborative annotation workflow. We demonstrate the effectiveness of\nCrowdAgent through extensive experiments on six diverse multimodal\nclassification tasks. The source code and video demo are available at\nhttps://github.com/QMMMS/CrowdAgent.", "AI": {"tldr": "本文提出了CrowdAgent，一个多智能体系统，用于端到端地控制数据标注流程，协同整合大型语言模型（LLMs）、小型语言模型（SLMs）和人类专家，以解决任务分配、标注、质量和成本管理中的复杂权衡。", "motivation": "尽管现有方法已开始利用LLMs、SLMs和人类专家等多样化标注源，但它们往往只关注标注本身。目前缺乏一种全面的流程控制方法，能够动态管理这些来源，并在统一框架下解决复杂的调度和质量-成本权衡问题。", "method": "本文受真实世界众包公司的启发，引入了CrowdAgent，一个多智能体系统，提供端到端的流程控制。它集成了任务分配、数据标注和质量/成本管理。该系统实施了一种新颖的方法，能够合理地分配任务，使LLMs、SLMs和人类专家在一个协作标注工作流中协同推进。", "result": "通过在六个不同的多模态分类任务上进行大量实验，本文证明了CrowdAgent的有效性。", "conclusion": "CrowdAgent提供了一种有效且全面的方法，通过多智能体系统协同管理LLMs、SLMs和人类专家，优化数据标注流程中的任务分配、质量和成本，从而解决了现有方法的局限性。"}}
{"id": "2509.13780", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13780", "abs": "https://arxiv.org/abs/2509.13780", "authors": ["Weishuai Zeng", "Shunlin Lu", "Kangning Yin", "Xiaojie Niu", "Minyue Dai", "Jingbo Wang", "Jiangmiao Pang"], "title": "Behavior Foundation Model for Humanoid Robots", "comment": null, "summary": "Whole-body control (WBC) of humanoid robots has witnessed remarkable progress\nin skill versatility, enabling a wide range of applications such as locomotion,\nteleoperation, and motion tracking. Despite these achievements, existing WBC\nframeworks remain largely task-specific, relying heavily on labor-intensive\nreward engineering and demonstrating limited generalization across tasks and\nskills. These limitations hinder their response to arbitrary control modes and\nrestrict their deployment in complex, real-world scenarios. To address these\nchallenges, we revisit existing WBC systems and identify a shared objective\nacross diverse tasks: the generation of appropriate behaviors that guide the\nrobot toward desired goal states. Building on this insight, we propose the\nBehavior Foundation Model (BFM), a generative model pretrained on large-scale\nbehavioral datasets to capture broad, reusable behavioral knowledge for\nhumanoid robots. BFM integrates a masked online distillation framework with a\nConditional Variational Autoencoder (CVAE) to model behavioral distributions,\nthereby enabling flexible operation across diverse control modes and efficient\nacquisition of novel behaviors without retraining from scratch. Extensive\nexperiments in both simulation and on a physical humanoid platform demonstrate\nthat BFM generalizes robustly across diverse WBC tasks while rapidly adapting\nto new behaviors. These results establish BFM as a promising step toward a\nfoundation model for general-purpose humanoid control.", "AI": {"tldr": "本文提出行为基础模型（BFM），一个预训练的生成模型，旨在解决人形机器人全身控制（WBC）中任务特异性、奖励工程繁琐和泛化能力差的问题，实现对任意控制模式的灵活操作和新行为的快速学习。", "motivation": "现有的人形机器人全身控制（WBC）框架高度任务特异化，严重依赖耗时耗力的奖励工程，且在不同任务和技能之间泛化能力有限。这些局限性阻碍了它们对任意控制模式的响应，并限制了其在复杂真实世界场景中的部署。", "method": "作者重新审视了现有WBC系统，并识别出跨任务的共同目标：生成引导机器人达到期望目标状态的适当行为。在此基础上，提出了行为基础模型（BFM），一个在大型行为数据集上预训练的生成模型，用于捕捉人形机器人广泛可重用的行为知识。BFM集成了掩码在线蒸馏框架与条件变分自编码器（CVAE），以建模行为分布。", "result": "在仿真和物理人形机器人平台上的大量实验表明，BFM能够稳健地泛化到各种WBC任务中，并能快速适应新行为。这些结果表明BFM是迈向通用人形机器人控制基础模型的有希望的一步。", "conclusion": "BFM通过预训练和生成式建模，有效解决了人形机器人全身控制中任务特异性和泛化能力不足的问题，展现了作为通用人形机器人控制基础模型的巨大潜力。"}}
{"id": "2509.13943", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.13943", "abs": "https://arxiv.org/abs/2509.13943", "authors": ["Salim Oyinlola", "Nitesh Subedi", "Soumik Sarkar"], "title": "Reinforcement Learning for Autonomous Point-to-Point UAV Navigation", "comment": "Presented at the Research Experience for Undergraduates (REU)\n  Symposium at the Translational AI Centre in Iowa State University", "summary": "Unmanned Aerial Vehicles (UAVs) are increasingly used in automated\ninspection, delivery, and navigation tasks that require reliable autonomy. This\nproject develops a reinforcement learning (RL) approach to enable a single UAV\nto autonomously navigate between predefined points without manual intervention.\nThe drone learns navigation policies through trial-and-error interaction, using\na custom reward function that encourages goal-reaching efficiency while\npenalizing collisions and unsafe behavior. The control system integrates ROS\nwith a Gym-compatible training environment, enabling flexible deployment and\ntesting. After training, the learned policy is deployed on a real UAV platform\nand evaluated under practical conditions. Results show that the UAV can\nsuccessfully perform autonomous navigation with minimal human oversight,\ndemonstrating the viability of RL-based control for point-to-point drone\noperations in real-world scenarios.", "AI": {"tldr": "本项目开发了一种基于强化学习（RL）的方法，使无人机（UAV）能够自主地在预定义点之间导航，并在实际场景中验证了其可行性。", "motivation": "无人机在自动化检查、配送和导航任务中的应用日益增多，这些任务需要可靠的自主性。目前的无人机操作可能需要人工干预，因此需要开发一种无需人工干预的自主导航方法。", "method": "研究采用强化学习（RL）方法，通过试错交互让无人机学习导航策略。设计了自定义奖励函数，以鼓励高效达成目标并惩罚碰撞和不安全行为。控制系统整合了ROS与兼容Gym的训练环境，支持灵活部署和测试。训练完成后，将学习到的策略部署到真实的无人机平台上进行评估。", "result": "结果表明，无人机能够成功地执行自主导航，且仅需最少的人工监督。", "conclusion": "该研究证明了基于强化学习的控制对于无人机在现实世界场景中进行点对点操作的可行性。"}}
{"id": "2509.13930", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13930", "abs": "https://arxiv.org/abs/2509.13930", "authors": ["Dayeon Ki", "Marine Carpuat", "Paul McNamee", "Daniel Khashabi", "Eugene Yang", "Dawn Lawrie", "Kevin Duh"], "title": "Linguistic Nepotism: Trading-off Quality for Language Preference in Multilingual RAG", "comment": "33 pages, 20 figures", "summary": "Multilingual Retrieval-Augmented Generation (mRAG) systems enable language\nmodels to answer knowledge-intensive queries with citation-supported responses\nacross languages. While such systems have been proposed, an open questions is\nwhether the mixture of different document languages impacts generation and\ncitation in unintended ways. To investigate, we introduce a controlled\nmethodology using model internals to measure language preference while holding\nother factors such as document relevance constant. Across eight languages and\nsix open-weight models, we find that models preferentially cite English sources\nwhen queries are in English, with this bias amplified for lower-resource\nlanguages and for documents positioned mid-context. Crucially, we find that\nmodels sometimes trade-off document relevance for language preference,\nindicating that citation choices are not always driven by informativeness\nalone. Our findings shed light on how language models leverage multilingual\ncontext and influence citation behavior.", "AI": {"tldr": "研究发现，多语言检索增强生成（mRAG）模型在引用时存在语言偏好，尤其是在英语查询下偏好引用英语来源，即使这可能牺牲文档的相关性，揭示了模型如何利用多语言上下文并影响引用行为。", "motivation": "现有mRAG系统虽然能跨语言回答知识密集型查询并提供引用，但一个未解决的问题是，不同文档语言的混合是否会以意想不到的方式影响生成和引用。", "method": "引入了一种受控方法，利用模型内部机制来衡量语言偏好，同时保持文档相关性等其他因素不变。该研究涉及八种语言和六个开源模型。", "result": "研究发现，当查询为英语时，模型优先引用英语来源，这种偏见在低资源语言和位于上下文中间的文档中更为明显。更重要的是，模型有时会为了语言偏好而牺牲文档相关性，表明引用选择并非总是仅由信息量驱动。", "conclusion": "研究结果揭示了语言模型如何利用多语言上下文并影响其引用行为，表明模型在多语言环境中可能存在非预期的语言偏好。"}}
{"id": "2509.13605", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13605", "abs": "https://arxiv.org/abs/2509.13605", "authors": ["Ruochen Hou", "Gabriel I. Fernandez", "Alex Xu", "Dennis W. Hong"], "title": "A Generalization of CLAP from 3D Localization to Image Processing, A Connection With RANSAC & Hough Transforms", "comment": null, "summary": "In previous work, we introduced a 2D localization algorithm called CLAP,\nClustering to Localize Across $n$ Possibilities, which was used during our\nchampionship win in RoboCup 2024, an international autonomous humanoid soccer\ncompetition. CLAP is particularly recognized for its robustness against\noutliers, where clustering is employed to suppress noise and mitigate against\nerroneous feature matches. This clustering-based strategy provides an\nalternative to traditional outlier rejection schemes such as RANSAC, in which\ncandidates are validated by reprojection error across all data points. In this\npaper, CLAP is extended to a more general framework beyond 2D localization,\nspecifically to 3D localization and image stitching. We also show how CLAP,\nRANSAC, and Hough transforms are related. The generalization of CLAP is widely\napplicable to many different fields and can be a useful tool to deal with noise\nand uncertainty.", "AI": {"tldr": "本文将之前在2D定位中表现出色的CLAP算法扩展到3D定位和图像拼接等更通用框架，并探讨了其与RANSAC和Hough变换的关系，强调其在处理噪声和不确定性方面的广泛适用性。", "motivation": "CLAP算法在RoboCup 2024的2D定位中展现出对异常值的鲁棒性，通过聚类有效抑制噪声和错误特征匹配。研究动机在于将这一成功的聚类策略推广到2D定位之外的更通用场景。", "method": "核心方法是CLAP算法，其通过聚类来抑制噪声和减轻错误的特征匹配，作为传统异常值剔除方法（如RANSAC）的替代。本文将CLAP扩展到3D定位和图像拼接领域，并分析了CLAP与RANSAC、Hough变换之间的关系。", "result": "CLAP算法被成功推广到2D定位之外的更通用框架，包括3D定位和图像拼接。研究揭示了CLAP、RANSAC和Hough变换之间的内在联系。推广后的CLAP算法在处理噪声和不确定性方面展现出广泛的适用性。", "conclusion": "推广后的CLAP算法是一个处理噪声和不确定性的有用工具，具有广泛的适用性，可应用于许多不同的领域。"}}
{"id": "2509.14195", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14195", "abs": "https://arxiv.org/abs/2509.14195", "authors": ["Shalima Binta Manir", "Tim Oates"], "title": "Hierarchical Learning for Maze Navigation: Emergence of Mental Representations via Second-Order Learning", "comment": "8 pages, 3 figures", "summary": "Mental representation, characterized by structured internal models mirroring\nexternal environments, is fundamental to advanced cognition but remains\nchallenging to investigate empirically. Existing theory hypothesizes that\nsecond-order learning -- learning mechanisms that adapt first-order learning\n(i.e., learning about the task/domain) -- promotes the emergence of such\nenvironment-cognition isomorphism. In this paper, we empirically validate this\nhypothesis by proposing a hierarchical architecture comprising a Graph\nConvolutional Network (GCN) as a first-order learner and an MLP controller as a\nsecond-order learner. The GCN directly maps node-level features to predictions\nof optimal navigation paths, while the MLP dynamically adapts the GCN's\nparameters when confronting structurally novel maze environments. We\ndemonstrate that second-order learning is particularly effective when the\ncognitive system develops an internal mental map structurally isomorphic to the\nenvironment. Quantitative and qualitative results highlight significant\nperformance improvements and robust generalization on unseen maze tasks,\nproviding empirical support for the pivotal role of structured mental\nrepresentations in maximizing the effectiveness of second-order learning.", "AI": {"tldr": "本研究通过一个分层GCN-MLP架构，实证验证了二阶学习能促进认知系统形成与环境结构同构的心理表征，从而显著提高导航性能和泛化能力。", "motivation": "心理表征对高级认知至关重要但难以实证研究。现有理论假设二阶学习能促进环境与认知之间的同构性，本研究旨在经验性地验证这一假设。", "method": "提出一个分层架构：使用图卷积网络（GCN）作为一阶学习器，负责将节点特征映射到最优导航路径预测；使用多层感知机（MLP）控制器作为二阶学习器，负责在面对结构新颖的迷宫环境时动态调整GCN的参数。", "result": "研究表明，当认知系统发展出与环境结构同构的内部心理地图时，二阶学习特别有效。定量和定性结果均显示，在未见过的迷宫任务上，性能显著提升且泛化能力强。", "conclusion": "本研究为结构化心理表征在最大化二阶学习有效性中的关键作用提供了实证支持。"}}
{"id": "2509.13802", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13802", "abs": "https://arxiv.org/abs/2509.13802", "authors": ["Takuya Kiyokawa", "Ryunosuke Takebayashi", "Kensuke Harada"], "title": "Shell-Type Soft Jig for Holding Objects during Disassembly", "comment": "6 pages, 8 figures", "summary": "This study addresses a flexible holding tool for robotic disassembly. We\npropose a shell-type soft jig that securely and universally holds objects,\nmitigating the risk of component damage and adapting to diverse shapes while\nenabling soft fixation that is robust to recognition, planning, and control\nerrors. The balloon-based holding mechanism ensures proper alignment and stable\nholding performance, thereby reducing the need for dedicated jig design, highly\naccurate perception, precise grasping, and finely tuned trajectory planning\nthat are typically required with conventional fixtures. Our experimental\nresults demonstrate the practical feasibility of the proposed jig through\nperformance comparisons with a vise and a jamming-gripper-inspired soft jig.\nTests on ten different objects further showed representative successes and\nfailures, clarifying the jig's limitations and outlook.", "AI": {"tldr": "本研究提出了一种用于机器人拆卸的柔性壳型软夹具，通过气囊机制实现对多样化物体的通用、安全、稳固夹持，同时减少对高精度感知和规划的需求。", "motivation": "传统夹具需要专门设计、高精度感知、精确抓取和精细轨迹规划，且存在组件损坏风险和难以适应多样化形状的问题。", "method": "提出了一种壳型软夹具，采用基于气囊的夹持机制，确保物体对齐和稳定夹持，并能实现对识别、规划和控制误差具有鲁棒性的软固定。", "result": "实验结果证明了所提出夹具的实用可行性，并与虎钳和受堵塞抓手启发的软夹具进行了性能比较。对十种不同物体的测试展示了成功和失败案例，明确了夹具的局限性和前景。", "conclusion": "该软夹具减少了对专用夹具设计、高精度感知、精确抓取和精细轨迹规划的需求，能安全、通用地夹持物体，并能适应多样化形状，同时降低组件损坏风险。"}}
{"id": "2509.14040", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.14040", "abs": "https://arxiv.org/abs/2509.14040", "authors": ["Zewen Yang", "Xiaobing Dai", "Dongfa Zhang", "Yu Li", "Ziyang Meng", "Bingkun Huang", "Hamid Sadeghian", "Sami Haddadin"], "title": "Prompt2Auto: From Motion Prompt to Automated Control via Geometry-Invariant One-Shot Gaussian Process Learning", "comment": null, "summary": "Learning from demonstration allows robots to acquire complex skills from\nhuman demonstrations, but conventional approaches often require large datasets\nand fail to generalize across coordinate transformations. In this paper, we\npropose Prompt2Auto, a geometry-invariant one-shot Gaussian process (GeoGP)\nlearning framework that enables robots to perform human-guided automated\ncontrol from a single motion prompt. A dataset-construction strategy based on\ncoordinate transformations is introduced that enforces invariance to\ntranslation, rotation, and scaling, while supporting multi-step predictions.\nMoreover, GeoGP is robust to variations in the user's motion prompt and\nsupports multi-skill autonomy. We validate the proposed approach through\nnumerical simulations with the designed user graphical interface and two\nreal-world robotic experiments, which demonstrate that the proposed method is\neffective, generalizes across tasks, and significantly reduces the\ndemonstration burden. Project page is available at:\nhttps://prompt2auto.github.io", "AI": {"tldr": "本文提出Prompt2Auto，一个几何不变的一次性高斯过程（GeoGP）学习框架，使机器人能从单个运动提示中实现人类引导的自动化控制，显著减少演示负担并提高泛化能力。", "motivation": "传统的机器人示教学习方法需要大量数据集，并且在坐标变换下泛化能力差。", "method": "提出Prompt2Auto框架，核心是几何不变的一次性高斯过程（GeoGP）。该方法引入基于坐标变换的数据集构建策略，以实现对平移、旋转和缩放的不变性，并支持多步预测。GeoGP对用户运动提示的变化具有鲁棒性，并支持多技能自主学习。", "result": "通过数值模拟和两次真实机器人实验验证了该方法，结果表明其有效、能跨任务泛化，并显著减少了演示负担。", "conclusion": "所提出的Prompt2Auto方法能够让机器人从单个运动提示中学习复杂技能，具有几何不变性、鲁棒性和多技能支持，有效解决了传统方法的局限性。"}}
{"id": "2509.13980", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13980", "abs": "https://arxiv.org/abs/2509.13980", "authors": ["Sami Ul Haq", "Chinonso Cynthia Osuji", "Sheila Castilho", "Brian Davis"], "title": "Long-context Reference-based MT Quality Estimation", "comment": null, "summary": "In this paper, we present our submission to the Tenth Conference on Machine\nTranslation (WMT25) Shared Task on Automated Translation Quality Evaluation.\n  Our systems are built upon the COMET framework and trained to predict\nsegment-level Error Span Annotation (ESA) scores using augmented long-context\ndata.\n  To construct long-context training data, we concatenate in-domain,\nhuman-annotated sentences and compute a weighted average of their scores.\n  We integrate multiple human judgment datasets (MQM, SQM, and DA) by\nnormalising their scales and train multilingual regression models to predict\nquality scores from the source, hypothesis, and reference translations.\n  Experimental results show that incorporating long-context information\nimproves correlations with human judgments compared to models trained only on\nshort segments.", "AI": {"tldr": "本文介绍了WMT25机器翻译质量评估共享任务的提交系统，该系统基于COMET框架，使用增强的长上下文数据训练预测段级ESA分数，并通过整合多个人工判断数据集提升了与人工判断的相关性。", "motivation": "参与第十届机器翻译大会（WMT25）自动翻译质量评估共享任务，旨在改进自动翻译质量评估方法。", "method": "系统基于COMET框架，训练预测段级错误跨度标注（ESA）分数。通过连接领域内人工标注句子并计算分数的加权平均来构建长上下文训练数据。整合了MQM、SQM和DA等多个不同尺度的人工判断数据集，并进行标准化。训练多语言回归模型，从源文本、假设译文和参考译文预测质量分数。", "result": "实验结果表明，与仅在短片段上训练的模型相比，整合长上下文信息显著提高了与人工判断的相关性。", "conclusion": "长上下文信息对于提升自动翻译质量评估模型与人工判断的相关性具有积极作用。"}}
{"id": "2509.13629", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13629", "abs": "https://arxiv.org/abs/2509.13629", "authors": ["Yue He", "Min Liu", "Qinghao Liu", "Jiazheng Wang", "Yaonan Wang", "Hang Zhang", "Xiang Chen"], "title": "SAMIR, an efficient registration framework via robust feature learning from SAM", "comment": null, "summary": "Image registration is a fundamental task in medical image analysis.\nDeformations are often closely related to the morphological characteristics of\ntissues, making accurate feature extraction crucial. Recent weakly supervised\nmethods improve registration by incorporating anatomical priors such as\nsegmentation masks or landmarks, either as inputs or in the loss function.\nHowever, such weak labels are often not readily available, limiting their\npractical use. Motivated by the strong representation learning ability of\nvisual foundation models, this paper introduces SAMIR, an efficient medical\nimage registration framework that utilizes the Segment Anything Model (SAM) to\nenhance feature extraction. SAM is pretrained on large-scale natural image\ndatasets and can learn robust, general-purpose visual representations. Rather\nthan using raw input images, we design a task-specific adaptation pipeline\nusing SAM's image encoder to extract structure-aware feature embeddings,\nenabling more accurate modeling of anatomical consistency and deformation\npatterns. We further design a lightweight 3D head to refine features within the\nembedding space, adapting to local deformations in medical images.\nAdditionally, we introduce a Hierarchical Feature Consistency Loss to guide\ncoarse-to-fine feature matching and improve anatomical alignment. Extensive\nexperiments demonstrate that SAMIR significantly outperforms state-of-the-art\nmethods on benchmark datasets for both intra-subject cardiac image registration\nand inter-subject abdomen CT image registration, achieving performance\nimprovements of 2.68% on ACDC and 6.44% on the abdomen dataset. The source code\nwill be publicly available on GitHub following the acceptance of this paper.", "AI": {"tldr": "本文提出SAMIR框架，利用Segment Anything Model (SAM)增强医学图像配准的特征提取能力，无需弱监督标签，并在多个基准数据集上超越现有SOTA方法。", "motivation": "现有的弱监督医学图像配准方法依赖于分割掩膜或地标等解剖学先验，但这些弱标签往往难以获取，限制了其实用性。受视觉基础模型强大表示学习能力的启发，研究旨在利用这些模型改进特征提取，克服标签稀缺问题。", "method": "SAMIR框架利用预训练的Segment Anything Model (SAM)的图像编码器提取结构感知的特征嵌入，而非直接使用原始输入图像。为了适应医学图像中的局部形变，设计了一个轻量级的3D头部来细化嵌入空间中的特征。此外，引入了分层特征一致性损失（Hierarchical Feature Consistency Loss）来指导从粗到细的特征匹配，以提高解剖学对齐。", "result": "在心脏图像配准（ACDC数据集）和腹部CT图像配准（腹部数据集）的广泛实验中，SAMIR显著优于现有最先进的方法。在ACDC数据集上性能提升了2.68%，在腹部数据集上提升了6.44%。", "conclusion": "SAMIR是一个高效的医学图像配准框架，通过利用SAM的强大特征提取能力，无需依赖稀缺的弱监督标签，实现了更准确的解剖一致性建模和形变模式识别，并在各种配准任务中取得了显著的性能提升。"}}
{"id": "2509.13815", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13815", "abs": "https://arxiv.org/abs/2509.13815", "authors": ["Takuya Kiyokawa", "Zhengtao Hu", "Weiwei Wan", "Kensuke Harada"], "title": "Soft Regrasping Tool Inspired by Jamming Gripper", "comment": "6 pages, 9 figures", "summary": "Regrasping on fixtures is a promising approach to reduce pose uncertainty in\nrobotic assembly, but conventional rigid fixtures lack adaptability and require\ndedicated designs for each part. To overcome this limitation, we propose a soft\njig inspired by the jamming transition phenomenon, which can be continuously\ndeformed to accommodate diverse object geometries. By pressing a\ntriangular-pyramid-shaped tool into the membrane and evacuating the enclosed\nair, a stable cavity is formed as a placement space. We further optimize the\nstamping depth to balance placement stability and gripper accessibility. In\nsoft-jig-based regrasping, the key challenge lies in optimizing the cavity size\nto achieve precise dropping; once the part is reliably placed, subsequent\ngrasping can be performed with reduced uncertainty. Accordingly, we conducted\ndrop experiments on ten mechanical parts of varying shapes, which achieved\nplacement success rates exceeding 80% for most objects and above 90% for\ncylindrical ones, while failures were mainly caused by geometric constraints\nand membrane properties. These results demonstrate that the proposed jig\nenables general-purpose, accurate, and repeatable regrasping, while also\nclarifying its current limitations and future potential as a practical\nalternative to rigid fixtures in assembly automation.", "AI": {"tldr": "本文提出了一种受堵塞效应启发的软夹具，通过在膜中形成可变形腔体来适应不同形状的零件，从而实现机器人装配中精确且可重复的重抓取，以减少姿态不确定性。", "motivation": "传统的刚性夹具缺乏适应性，需要为每个零件进行专门设计，这限制了机器人在装配中减少姿态不确定性的能力。", "method": "研究人员提出了一种受堵塞效应启发的软夹具。通过将三角锥形工具压入膜中并抽空内部空气，形成一个稳定的腔体作为放置空间。他们优化了冲压深度以平衡放置稳定性和抓取器可达性，并进一步优化腔体尺寸以实现精确的零件投放。", "result": "对十种不同形状的机械零件进行的投放实验显示，大多数零件的放置成功率超过80%，圆柱形零件的成功率超过90%。失败主要归因于几何约束和膜的特性。", "conclusion": "所提出的软夹具能够实现通用、准确且可重复的重抓取，证明了其作为刚性夹具在自动化装配中的实用替代方案的潜力，同时也明确了其当前的局限性和未来发展方向。"}}
{"id": "2509.14075", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.14075", "abs": "https://arxiv.org/abs/2509.14075", "authors": ["Yu Li", "Hamid Sadeghian", "Zewen Yang", "Valentin Le Mesle", "Sami Haddadin"], "title": "Constraint-Consistent Control of Task-Based and Kinematic RCM Constraints for Surgical Robots", "comment": null, "summary": "Robotic-assisted minimally invasive surgery (RAMIS) requires precise\nenforcement of the remote center of motion (RCM) constraint to ensure safe tool\nmanipulation through a trocar. Achieving this constraint under dynamic and\ninteractive conditions remains challenging, as existing control methods either\nlack robustness at the torque level or do not guarantee consistent RCM\nconstraint satisfaction. This paper proposes a constraint-consistent torque\ncontroller that treats the RCM as a rheonomic holonomic constraint and embeds\nit into a projection-based inverse-dynamics framework. The method unifies\ntask-level and kinematic formulations, enabling accurate tool-tip tracking\nwhile maintaining smooth and efficient torque behavior. The controller is\nvalidated both in simulation and on a RAMIS training platform, and is\nbenchmarked against state-of-the-art approaches. Results show improved RCM\nconstraint satisfaction, reduced required torque, and robust performance by\nimproving joint torque smoothness through the consistency formulation under\nclinically relevant scenarios, including spiral trajectories, variable\ninsertion depths, moving trocars, and human interaction. These findings\ndemonstrate the potential of constraint-consistent torque control to enhance\nsafety and reliability in surgical robotics. The project page is available at:\nhttps://rcmpc-cube.github.io", "AI": {"tldr": "本文提出了一种约束一致的扭矩控制器，用于机器人辅助微创手术（RAMIS）中的远程运动中心（RCM）约束，以提高RCM满足度、降低所需扭矩并增强手术机器人操作的安全性和可靠性。", "motivation": "RAMIS需要精确执行RCM约束以确保器械安全操作，但在动态和交互条件下，现有控制方法在扭矩层面缺乏鲁棒性或不能保证RCM约束的一致性满足。", "method": "该方法将RCM视为一个时变完整约束，并将其嵌入到基于投影的逆动力学框架中。它统一了任务级和运动学公式，实现了精确的工具尖端跟踪，同时保持平滑高效的扭矩行为。该控制器在仿真和RAMIS训练平台上进行了验证，并与现有最先进的方法进行了基准测试。", "result": "结果显示，在临床相关场景（包括螺旋轨迹、可变插入深度、移动套管和人机交互）下，该方法显著改善了RCM约束满足度，降低了所需扭矩，并通过一致性公式提高了关节扭矩平滑度，从而表现出鲁棒的性能。", "conclusion": "这些发现表明，约束一致的扭矩控制有潜力增强手术机器人的安全性和可靠性。"}}
{"id": "2509.13990", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "pdf": "https://arxiv.org/pdf/2509.13990", "abs": "https://arxiv.org/abs/2509.13990", "authors": ["Colin Hong", "Xu Guo", "Anand Chaanan Singh", "Esha Choukse", "Dmitrii Ustiugov"], "title": "Slim-SC: Thought Pruning for Efficient Scaling with Self-Consistency", "comment": "Accepted by EMNLP 2025 (Oral), 9 pages", "summary": "Recently, Test-Time Scaling (TTS) has gained increasing attention for\nimproving LLM reasoning performance at test time without retraining the model.\nA notable TTS technique is Self-Consistency (SC), which generates multiple\nreasoning chains in parallel and selects the final answer via majority voting.\nWhile effective, the order-of-magnitude computational overhead limits its broad\ndeployment. Prior attempts to accelerate SC mainly rely on model-based\nconfidence scores or heuristics with limited empirical support. For the first\ntime, we theoretically and empirically analyze the inefficiencies of SC and\nreveal actionable opportunities for improvement. Building on these insights, we\npropose Slim-SC, a step-wise pruning strategy that identifies and removes\nredundant chains using inter-chain similarity at the thought level. Experiments\non three STEM reasoning datasets and two recent LLM architectures show that\nSlim-SC reduces inference latency and KVC usage by up to 45% and 26%,\nrespectively, with R1-Distill, while maintaining or improving accuracy, thus\noffering a simple yet efficient TTS alternative for SC.", "AI": {"tldr": "该论文针对自洽性（Self-Consistency, SC）这一测试时缩放（TTS）技术在提高大型语言模型推理能力方面存在的计算开销问题，首次从理论和实证角度分析其低效性，并提出Slim-SC，一种基于思维层面链间相似性的逐步剪枝策略，显著降低推理延迟和KVC使用量，同时保持或提升准确性。", "motivation": "自洽性（SC）能有效提升LLM的推理性能，但其巨大的计算开销限制了广泛部署。现有的加速SC方法主要依赖模型置信度或启发式规则，缺乏充分的经验支持。因此，研究人员希望深入分析SC的低效性，并寻找有效的改进方法。", "method": "1. 理论和实证分析自洽性（SC）的低效性，揭示改进机会。2. 基于这些洞察，提出Slim-SC策略。3. Slim-SC是一种逐步剪枝策略，通过在思维层面利用链间相似性来识别和移除冗余的推理链。", "result": "在三个STEM推理数据集和两种LLM架构上的实验表明，Slim-SC能将推理延迟降低高达45%，KVC使用量降低高达26%（结合R1-Distill），同时保持或提高了准确性。", "conclusion": "Slim-SC为自洽性提供了一种简单而高效的测试时缩放替代方案，有效解决了其计算开销问题，同时保持了性能，使其更易于部署。"}}
{"id": "2509.13631", "categories": ["cs.CV", "cs.DC", "14J60", "F.2.2; I.2.7"], "pdf": "https://arxiv.org/pdf/2509.13631", "abs": "https://arxiv.org/abs/2509.13631", "authors": ["Yuvraj Dutta", "Aaditya Sikder", "Basabdatta Palit"], "title": "Federated Learning for Deforestation Detection: A Distributed Approach with Satellite Imagery", "comment": "6 pages, 7 figures, accepted at IEEE INDISCON 2025", "summary": "Accurate identification of deforestation from satellite images is essential\nin order to understand the geographical situation of an area. This paper\nintroduces a new distributed approach to identify as well as locate\ndeforestation across different clients using Federated Learning (FL). Federated\nLearning enables distributed network clients to collaboratively train a model\nwhile maintaining data privacy and security of the active users. In our\nframework, a client corresponds to an edge satellite center responsible for\nlocal data processing. Moreover, FL provides an advantage over centralized\ntraining method which requires combining data, thereby compromising with data\nsecurity of the clients. Our framework leverages the FLOWER framework with RAY\nframework to execute the distributed learning workload. Furthermore, efficient\nclient spawning is ensured by RAY as it can select definite amount of users to\ncreate an emulation environment. Our FL framework uses YOLOS-small (a Vision\nTransformer variant), Faster R-CNN with a ResNet50 backbone, and Faster R-CNN\nwith a MobileNetV3 backbone models trained and tested on publicly available\ndatasets. Our approach provides us a different view for image\nsegmentation-based tasks on satellite imagery.", "AI": {"tldr": "本文提出一种基于联邦学习（FL）的分布式方法，用于从卫星图像中识别和定位森林砍伐，同时保护数据隐私。", "motivation": "准确识别森林砍伐对理解地理状况至关重要。传统的集中式训练方法可能牺牲数据隐私，因此需要一种分布式且能维护数据隐私的解决方案。", "method": "该框架利用联邦学习（FL）让边缘卫星中心作为客户端协同训练模型，同时保持数据隐私。它结合了FLOWER和RAY框架来执行分布式学习任务，并使用RAY进行高效的客户端模拟。模型方面，采用了YOLOS-small（Vision Transformer变体）、基于ResNet50骨干的Faster R-CNN以及基于MobileNetV3骨干的Faster R-CNN，并在公开数据集上进行训练和测试。", "result": "该方法为基于图像分割的卫星图像任务提供了一种新的视角，实现了分布式和隐私保护的森林砍伐识别。", "conclusion": "联邦学习为卫星图像上的森林砍伐识别和图像分割任务提供了一种分布式、安全且保护隐私的有效方法。"}}
{"id": "2509.13662", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13662", "abs": "https://arxiv.org/abs/2509.13662", "authors": ["Yulan Guo", "Longguang Wang", "Wendong Mao", "Xiaoyu Dong", "Yingqian Wang", "Li Liu", "Wei An"], "title": "Deep Lookup Network", "comment": null, "summary": "Convolutional neural networks are constructed with massive operations with\ndifferent types and are highly computationally intensive. Among these\noperations, multiplication operation is higher in computational complexity and\nusually requires {more} energy consumption with longer inference time than\nother operations, which hinders the deployment of convolutional neural networks\non mobile devices. In many resource-limited edge devices, complicated\noperations can be calculated via lookup tables to reduce computational cost.\nMotivated by this, in this paper, we introduce a generic and efficient lookup\noperation which can be used as a basic operation for the construction of neural\nnetworks. Instead of calculating the multiplication of weights and activation\nvalues, simple yet efficient lookup operations are adopted to compute their\nresponses. To enable end-to-end optimization of the lookup operation, we\nconstruct the lookup tables in a differentiable manner and propose several\ntraining strategies to promote their convergence. By replacing computationally\nexpensive multiplication operations with our lookup operations, we develop\nlookup networks for the image classification, image super-resolution, and point\ncloud classification tasks. It is demonstrated that our lookup networks can\nbenefit from the lookup operations to achieve higher efficiency in terms of\nenergy consumption and inference speed while maintaining competitive\nperformance to vanilla convolutional networks. Extensive experiments show that\nour lookup networks produce state-of-the-art performance on different tasks\n(both classification and regression tasks) and different data types (both\nimages and point clouds).", "AI": {"tldr": "本文提出了一种通用且高效的可微分查找操作，用于替代卷积神经网络中计算成本较高的乘法操作，从而构建查找网络，以提高在资源受限设备上的能效和推理速度，同时保持竞争力或达到最先进的性能。", "motivation": "卷积神经网络中的乘法操作计算复杂度高，能耗大，推理时间长，阻碍了其在移动设备上的部署。在许多资源受限的边缘设备上，可以通过查找表来降低复杂操作的计算成本。", "method": "引入了一种通用且高效的查找操作作为神经网络的基本操作，用其替代权重与激活值之间的乘法计算。为了实现端到端优化，查找表被构建为可微分形式，并提出了几种训练策略以促进其收敛。基于此，开发了用于图像分类、图像超分辨率和点云分类任务的查找网络。", "result": "所提出的查找网络在保持与传统卷积网络相当的性能的同时，显著提高了能耗效率和推理速度。广泛的实验表明，查找网络在不同任务（分类和回归）和不同数据类型（图像和点云）上均取得了最先进的性能。", "conclusion": "通过将计算昂贵的乘法操作替换为可微分的查找操作，本文提出的查找网络为在资源受限设备上部署高效神经网络提供了一种有效途径，并在多个任务和数据类型上展现出卓越的性能和效率。"}}
{"id": "2509.13816", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13816", "abs": "https://arxiv.org/abs/2509.13816", "authors": ["Yude Li", "Zhexuan Zhou", "Huizhe Li", "Youmin Gong", "Jie Mei"], "title": "Agile in the Face of Delay: Asynchronous End-to-End Learning for Real-World Aerial Navigation", "comment": null, "summary": "Robust autonomous navigation for Autonomous Aerial Vehicles (AAVs) in complex\nenvironments is a critical capability. However, modern end-to-end navigation\nfaces a key challenge: the high-frequency control loop needed for agile flight\nconflicts with low-frequency perception streams, which are limited by sensor\nupdate rates and significant computational cost. This mismatch forces\nconventional synchronous models into undesirably low control rates. To resolve\nthis, we propose an asynchronous reinforcement learning framework that\ndecouples perception and control, enabling a high-frequency policy to act on\nthe latest IMU state for immediate reactivity, while incorporating perception\nfeatures asynchronously. To manage the resulting data staleness, we introduce a\ntheoretically-grounded Temporal Encoding Module (TEM) that explicitly\nconditions the policy on perception delays, a strategy complemented by a\ntwo-stage curriculum to ensure stable and efficient training. Validated in\nextensive simulations, our method was successfully deployed in zero-shot\nsim-to-real transfer on an onboard NUC, where it sustains a 100~Hz control rate\nand demonstrates robust, agile navigation in cluttered real-world environments.\nOur source code will be released for community reference.", "AI": {"tldr": "该论文提出了一种异步强化学习框架，通过解耦感知和控制，使自主飞行器（AAV）能够在复杂环境中实现高频率控制（100Hz）和鲁棒、敏捷的导航，同时解决了感知数据低频率和计算成本高的问题。", "motivation": "现代端到端导航中，敏捷飞行所需的高频控制循环与受限于传感器更新率和计算成本的低频感知流之间存在冲突。这种不匹配导致传统的同步模型控制速率过低，影响AAV在复杂环境中的鲁棒自主导航能力。", "method": "本文提出了一种异步强化学习框架，将感知与控制解耦。该框架允许高频策略基于最新的IMU状态即时反应，同时异步整合感知特征。为解决数据陈旧问题，引入了理论基础的“时间编码模块（TEM）”，明确地根据感知延迟调整策略。此外，还采用了两阶段课程学习策略以确保稳定高效的训练。", "result": "该方法在广泛的模拟中得到验证，并在机载NUC上成功进行了零样本模拟到真实世界的迁移部署。它能够维持100Hz的控制速率，并在杂乱的真实世界环境中展示出鲁棒、敏捷的导航能力。", "conclusion": "所提出的异步强化学习框架有效解决了AAV导航中高频控制与低频感知之间的冲突，通过解耦感知和控制，并引入时间编码模块处理数据陈旧性，实现了在复杂真实环境中鲁棒、敏捷的自主导航能力。"}}
{"id": "2509.14004", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.14004", "abs": "https://arxiv.org/abs/2509.14004", "authors": ["Minjia Mao", "Bowen Yin", "Yu Zhu", "Xiao Fang"], "title": "Early Stopping Chain-of-thoughts in Large Language Models", "comment": null, "summary": "Reasoning large language models (LLMs) have demonstrated superior capacities\nin solving complicated problems by generating long chain-of-thoughts (CoT), but\nsuch a lengthy CoT incurs high inference costs. In this study, we introduce\nES-CoT, an inference-time method that shortens CoT generation by detecting\nanswer convergence and stopping early with minimal performance loss. At the end\nof each reasoning step, we prompt the LLM to output its current final answer,\ndenoted as a step answer. We then track the run length of consecutive identical\nstep answers as a measure of answer convergence. Once the run length exhibits a\nsharp increase and exceeds a minimum threshold, the generation is terminated.\nWe provide both empirical and theoretical support for this heuristic: step\nanswers steadily converge to the final answer, and large run-length jumps\nreliably mark this convergence. Experiments on five reasoning datasets across\nthree LLMs show that ES-CoT reduces the number of inference tokens by about\n41\\% on average while maintaining accuracy comparable to standard CoT. Further,\nES-CoT integrates seamlessly with self-consistency prompting and remains robust\nacross hyperparameter choices, highlighting it as a practical and effective\napproach for efficient reasoning.", "AI": {"tldr": "ES-CoT是一种推理时方法，通过检测答案收敛并提前停止CoT生成，平均减少41%的推理token，同时保持与标准CoT相当的准确性。", "motivation": "大型语言模型（LLMs）通过生成长链式思维（CoT）在解决复杂问题方面表现出色，但这种冗长的CoT会导致高昂的推理成本。", "method": "ES-CoT在每个推理步骤结束时提示LLM输出当前最终答案（步答案），然后跟踪连续相同步答案的运行长度。一旦运行长度急剧增加并超过最小阈值，则终止生成。该方法有经验和理论支持，即步答案稳定收敛到最终答案，并且运行长度的大幅跳跃可靠地标志着这种收敛。", "result": "在五个推理数据集和三个LLM上的实验表明，ES-CoT平均减少了约41%的推理token数量，同时保持了与标准CoT相当的准确性。此外，ES-CoT可以无缝集成到自洽提示中，并且对超参数选择具有鲁棒性。", "conclusion": "ES-CoT是一种实用且有效的方法，可以实现高效推理，通过缩短CoT生成来显著降低推理成本，同时保持性能。"}}
{"id": "2509.13652", "categories": ["cs.CV", "I.4.8; I.4.5"], "pdf": "https://arxiv.org/pdf/2509.13652", "abs": "https://arxiv.org/abs/2509.13652", "authors": ["Yumin Li", "Dylan Campbell"], "title": "Gaussian Alignment for Relative Camera Pose Estimation via Single-View Reconstruction", "comment": "12 pages, 4 figures, accepted by AJCAI 2025", "summary": "Estimating metric relative camera pose from a pair of images is of great\nimportance for 3D reconstruction and localisation. However, conventional\ntwo-view pose estimation methods are not metric, with camera translation known\nonly up to a scale, and struggle with wide baselines and textureless or\nreflective surfaces. This paper introduces GARPS, a training-free framework\nthat casts this problem as the direct alignment of two independently\nreconstructed 3D scenes. GARPS leverages a metric monocular depth estimator and\na Gaussian scene reconstructor to obtain a metric 3D Gaussian Mixture Model\n(GMM) for each image. It then refines an initial pose from a feed-forward\ntwo-view pose estimator by optimising a differentiable GMM alignment objective.\nThis objective jointly considers geometric structure, view-independent colour,\nanisotropic covariance, and semantic feature consistency, and is robust to\nocclusions and texture-poor regions without requiring explicit 2D\ncorrespondences. Extensive experiments on the Real\\-Estate10K dataset\ndemonstrate that GARPS outperforms both classical and state-of-the-art\nlearning-based methods, including MASt3R. These results highlight the potential\nof bridging single-view perception with multi-view geometry to achieve robust\nand metric relative pose estimation.", "AI": {"tldr": "GARPS是一个免训练的框架，通过对两个独立重建的3D高斯混合模型（GMM）进行直接对齐，实现了鲁棒且度量化的相机相对位姿估计。", "motivation": "传统的双视角位姿估计算法无法提供度量化的相机平移（仅能确定到尺度），并且在基线较宽、纹理缺失或反射表面等场景下表现不佳。", "method": "GARPS将问题转化为两个独立重建的3D场景的直接对齐。它利用度量单目深度估计器和高斯场景重建器为每张图像生成度量3D高斯混合模型（GMM）。然后，通过优化一个可微分的GMM对齐目标来精炼初始位姿（由前馈双视角位姿估计器提供）。该目标综合考虑了几何结构、视角无关颜色、各向异性协方差和语义特征一致性，并且在遮挡和纹理贫乏区域无需显式2D对应点即可保持鲁棒性。", "result": "在RealEstate10K数据集上进行的广泛实验表明，GARPS的性能优于经典的以及包括MASt3R在内的最先进的基于学习的方法。", "conclusion": "这些结果突出了将单视角感知与多视角几何相结合，以实现鲁棒且度量化的相对位姿估计的巨大潜力。"}}
{"id": "2509.13676", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13676", "abs": "https://arxiv.org/abs/2509.13676", "authors": ["Xiaobo Yang", "Xiaojin Gong"], "title": "Re-purposing SAM into Efficient Visual Projectors for MLLM-Based Referring Image Segmentation", "comment": null, "summary": "Recently, Referring Image Segmentation (RIS) frameworks that pair the\nMultimodal Large Language Model (MLLM) with the Segment Anything Model (SAM)\nhave achieved impressive results. However, adapting MLLM to segmentation is\ncomputationally intensive, primarily due to visual token redundancy. We observe\nthat traditional patch-wise visual projectors struggle to strike a balance\nbetween reducing the number of visual tokens and preserving semantic clarity,\noften retaining overly long token sequences to avoid performance drops.\nInspired by text tokenizers, we propose a novel semantic visual projector that\nleverages semantic superpixels generated by SAM to identify \"visual words\" in\nan image. By compressing and projecting semantic superpixels as visual tokens,\nour approach adaptively shortens the token sequence according to scene\ncomplexity while minimizing semantic loss in compression. To mitigate loss of\ninformation, we propose a semantic superpixel positional embedding to\nstrengthen MLLM's awareness of superpixel geometry and position, alongside a\nsemantic superpixel aggregator to preserve both fine-grained details inside\nsuperpixels and global context outside. Experiments show that our method cuts\nvisual tokens by 93% without compromising performance, notably speeding up MLLM\ntraining and inference, and outperforming existing compressive visual\nprojectors on RIS.", "AI": {"tldr": "本文提出一种基于SAM语义超像素的语义视觉投影器，用于解决MLLM+SAM指代图像分割中视觉token冗余导致计算量大的问题。该方法能将视觉token数量减少93%而不影响性能，显著加速MLLM训练和推理。", "motivation": "现有的MLLM与SAM结合的指代图像分割（RIS）框架计算成本高昂，主要原因在于视觉token冗余。传统的基于块的视觉投影器难以在减少视觉token数量和保持语义清晰度之间取得平衡，常保留过长的token序列以避免性能下降。", "method": "受文本分词器启发，本文提出一种新颖的语义视觉投影器。它利用SAM生成的语义超像素来识别图像中的“视觉词”，将语义超像素压缩并投影为视觉token，根据场景复杂性自适应地缩短token序列，同时最小化压缩过程中的语义损失。为进一步减轻信息损失，还引入了语义超像素位置嵌入来增强MLLM对超像素几何和位置的感知，并设计了语义超像素聚合器以保留超像素内部的细粒度细节和外部的全局上下文。", "result": "实验表明，该方法在不影响性能的情况下，将视觉token数量减少了93%，显著加速了MLLM的训练和推理，并且在RIS任务上优于现有的压缩视觉投影器。", "conclusion": "通过利用SAM语义超像素进行智能压缩和投影，本文提出的方法有效解决了MLLM+SAM指代图像分割中的视觉token冗余问题，实现了显著的计算效率提升和加速，同时保持了优异的性能。"}}
{"id": "2509.13827", "categories": ["cs.RO", "cs.NE"], "pdf": "https://arxiv.org/pdf/2509.13827", "abs": "https://arxiv.org/abs/2509.13827", "authors": ["Renyuan Liu", "Haoting Zhou", "Chuankai Fang", "Qinbing Fu"], "title": "How Fly Neural Perception Mechanisms Enhance Visuomotor Control of Micro Robots", "comment": "9 pages, 6 figures", "summary": "Anyone who has tried to swat a fly has likely been frustrated by its\nremarkable agility.This ability stems from its visual neural perception system,\nparticularly the collision-selective neurons within its small brain.For\nautonomous robots operating in complex and unfamiliar environments, achieving\nsimilar agility is highly desirable but often constrained by the trade-off\nbetween computational cost and performance.In this context, insect-inspired\nintelligence offers a parsimonious route to low-power, computationally\nefficient frameworks.In this paper, we propose an attention-driven visuomotor\ncontrol strategy inspired by a specific class of fly visual projection\nneurons-the lobula plate/lobula column type-2 (LPLC2)-and their associated\nescape behaviors.To our knowledge, this represents the first embodiment of an\nLPLC2 neural model in the embedded vision of a physical mobile robot, enabling\ncollision perception and reactive evasion.The model was simplified and\noptimized at 70KB in memory to suit the computational constraints of a\nvision-based micro robot, the Colias, while preserving key neural perception\nmechanisms.We further incorporated multi-attention mechanisms to emulate the\ndistributed nature of LPLC2 responses, allowing the robot to detect and react\nto approaching targets both rapidly and selectively.We systematically evaluated\nthe proposed method against a state-of-the-art locust-inspired collision\ndetection model.Results showed that the fly-inspired visuomotor model achieved\ncomparable robustness, at success rate of 96.1% in collision detection while\nproducing more adaptive and elegant evasive maneuvers.Beyond demonstrating an\neffective collision-avoidance strategy, this work highlights the potential of\nfly-inspired neural models for advancing research into collective behaviors in\ninsect intelligence.", "AI": {"tldr": "本文提出了一种受苍蝇LPLC2神经元启发的视觉运动控制策略，用于微型机器人的碰撞感知和规避，实现了高效且适应性强的避障能力。", "motivation": "自主机器人在复杂环境中需要高灵活性，但受限于计算成本和性能的权衡。昆虫智能为低功耗、计算高效的框架提供了启发，特别是苍蝇的视觉神经系统及其碰撞选择性神经元。", "method": "提出了一种受苍蝇LPLC2神经元及其逃逸行为启发的注意力驱动视觉运动控制策略。将LPLC2神经模型简化并优化至70KB内存，以适应微型机器人Colias的计算限制。进一步整合了多注意力机制，以模拟LPLC2响应的分布式特性，使机器人能够快速选择性地检测和反应接近的目标。", "result": "与最先进的蝗虫启发碰撞检测模型相比，该苍蝇启发视觉运动模型在碰撞检测方面达到了96.1%的成功率，具有相当的鲁棒性，并产生了更具适应性和优雅的规避动作。", "conclusion": "该工作不仅展示了一种有效的碰撞规避策略，还强调了苍蝇启发神经模型在推进昆虫智能集体行为研究方面的潜力。"}}
{"id": "2509.14008", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14008", "abs": "https://arxiv.org/abs/2509.14008", "authors": ["Hasan Abed Al Kader Hammoud", "Mohammad Zbeeb", "Bernard Ghanem"], "title": "Hala Technical Report: Building Arabic-Centric Instruction & Translation Models at Scale", "comment": "Technical Report", "summary": "We present Hala, a family of Arabic-centric instruction and translation\nmodels built with our translate-and-tune pipeline. We first compress a strong\nAR$\\leftrightarrow$EN teacher to FP8 (yielding $\\sim$2$\\times$ higher\nthroughput with no quality loss) and use it to create high-fidelity bilingual\nsupervision. A lightweight language model LFM2-1.2B is then fine-tuned on this\ndata and used to translate high-quality English instruction sets into Arabic,\nproducing a million-scale corpus tailored to instruction following. We train\nHala models at 350M, 700M, 1.2B, and 9B parameters, and apply slerp merging to\nbalance Arabic specialization with base-model strengths. On Arabic-centric\nbenchmarks, Hala achieves state-of-the-art results within both the \"nano\"\n($\\leq$2B) and \"small\" (7-9B) categories, outperforming their bases. We release\nmodels, data, evaluation, and recipes to accelerate research in Arabic NLP.", "AI": {"tldr": "Hala是一个阿拉伯语指令和翻译模型家族，通过“翻译-微调”流程构建，在阿拉伯语基准测试中达到了SOTA，并发布了模型、数据和评估工具。", "motivation": "旨在开发以阿拉伯语为中心的指令遵循和翻译模型，以加速阿拉伯语自然语言处理（NLP）领域的研究。", "method": "采用“翻译-微调”流程：首先将一个强大的阿-英翻译教师模型压缩至FP8以提高吞吐量并生成高质量双语监督数据；然后，使用这些数据微调一个轻量级语言模型LFM2-1.2B，并用其将高质量英语指令集翻译成阿拉伯语，从而创建了一个百万级的阿拉伯语指令遵循语料库。在此基础上，训练了350M、700M、1.2B和9B参数规模的Hala模型，并应用slerp合并技术以平衡阿拉伯语特化能力与基础模型优势。", "result": "压缩后的教师模型在无质量损失的情况下实现了约2倍的吞吐量提升。Hala模型在阿拉伯语基准测试中，于“纳米”（≤2B）和“小型”（7-9B）类别中均取得了最先进（SOTA）的结果，并超越了其基础模型。", "conclusion": "Hala模型家族及其“翻译-微调”流程成功地为阿拉伯语指令遵循和翻译任务带来了显著提升。研究团队发布了模型、数据、评估方法和训练配方，以期促进阿拉伯语NLP的进一步研究和发展。"}}
{"id": "2509.13681", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13681", "abs": "https://arxiv.org/abs/2509.13681", "authors": ["Hang Li", "Dianmo Sheng", "Qiankun Dong", "Zichun Wang", "Zhiwei Xu", "Tao Li"], "title": "FishBEV: Distortion-Resilient Bird's Eye View Segmentation with Surround-View Fisheye Cameras", "comment": "8 pages, 4 figures", "summary": "As a cornerstone technique for autonomous driving, Bird's Eye View (BEV)\nsegmentation has recently achieved remarkable progress with pinhole cameras.\nHowever, it is non-trivial to extend the existing methods to fisheye cameras\nwith severe geometric distortion, ambiguous multi-view correspondences and\nunstable temporal dynamics, all of which significantly degrade BEV performance.\nTo address these challenges, we propose FishBEV, a novel BEV segmentation\nframework specifically tailored for fisheye cameras. This framework introduces\nthree complementary innovations, including a Distortion-Resilient Multi-scale\nExtraction (DRME) backbone that learns robust features under distortion while\npreserving scale consistency, an Uncertainty-aware Spatial Cross-Attention\n(U-SCA) mechanism that leverages uncertainty estimation for reliable cross-view\nalignment, a Distance-aware Temporal Self-Attention (D-TSA) module that\nadaptively balances near field details and far field context to ensure temporal\ncoherence. Extensive experiments on the Synwoodscapes dataset demonstrate that\nFishBEV consistently outperforms SOTA baselines, regarding the performance\nevaluation of FishBEV on the surround-view fisheye BEV segmentation tasks.", "AI": {"tldr": "FishBEV是一个专门为鱼眼相机设计的BEV分割框架，通过引入DRME、U-SCA和D-TSA模块，解决了鱼眼相机固有的几何畸变、多视角对应模糊和时间动态不稳定等问题，并在Synwoodscapes数据集上表现优于现有SOTA方法。", "motivation": "自动驾驶中的鸟瞰图（BEV）分割技术在针孔相机上取得了显著进展，但现有方法难以扩展到鱼眼相机。鱼眼相机存在严重的几何畸变、模糊的多视角对应关系和不稳定的时间动态，这些因素严重降低了BEV分割的性能。", "method": "本文提出了FishBEV框架，包含三项互补创新：1) 畸变鲁棒多尺度提取（DRME）骨干网络，用于在畸变下学习鲁棒特征并保持尺度一致性；2) 不确定性感知空间交叉注意力（U-SCA）机制，利用不确定性估计实现可靠的跨视角对齐；3) 距离感知时间自注意力（D-TSA）模块，自适应地平衡近场细节和远场上下文以确保时间连贯性。", "result": "在Synwoodscapes数据集上进行的广泛实验表明，FishBEV在环视鱼眼BEV分割任务的性能评估方面，始终优于现有SOTA基线方法。", "conclusion": "FishBEV框架成功应对了鱼眼相机BEV分割的挑战，通过其创新的模块设计，实现了在畸变、多视角对应和时间动态方面的鲁棒性，并取得了卓越的性能。"}}
{"id": "2509.13722", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13722", "abs": "https://arxiv.org/abs/2509.13722", "authors": ["Dingwei Zhang", "Dong Zhang", "Jinhui Tang"], "title": "Mitigating Query Selection Bias in Referring Video Object Segmentation", "comment": null, "summary": "Recently, query-based methods have achieved remarkable performance in\nReferring Video Object Segmentation (RVOS) by using textual static object\nqueries to drive cross-modal alignment. However, these static queries are\neasily misled by distractors with similar appearance or motion, resulting in\n\\emph{query selection bias}. To address this issue, we propose Triple Query\nFormer (TQF), which factorizes the referring query into three specialized\ncomponents: an appearance query for static attributes, an intra-frame\ninteraction query for spatial relations, and an inter-frame motion query for\ntemporal association. Instead of relying solely on textual embeddings, our\nqueries are dynamically constructed by integrating both linguistic cues and\nvisual guidance. Furthermore, we introduce two motion-aware aggregation modules\nthat enhance object token representations: Intra-frame Interaction Aggregation\nincorporates position-aware interactions among objects within a single frame,\nwhile Inter-frame Motion Aggregation leverages trajectory-guided alignment\nacross frames to ensure temporal coherence. Extensive experiments on multiple\nRVOS benchmarks demonstrate the advantages of TQF and the effectiveness of our\nstructured query design and motion-aware aggregation modules.", "AI": {"tldr": "现有基于查询的RVOS方法存在查询选择偏差，本文提出Triple Query Former (TQF)，通过将查询分解为外观、帧内交互和帧间运动三种动态查询，并引入运动感知聚合模块，有效解决了这一问题，显著提升了RVOS性能。", "motivation": "现有基于查询的Referring Video Object Segmentation (RVOS) 方法使用静态文本对象查询来驱动跨模态对齐，但这些静态查询容易被外观或运动相似的干扰物误导，导致“查询选择偏差”。", "method": "本文提出Triple Query Former (TQF)，将参照查询分解为三个专门组件：用于静态属性的外观查询、用于空间关系的帧内交互查询和用于时间关联的帧间运动查询。这些查询通过整合语言线索和视觉指导动态构建。此外，引入了两个运动感知聚合模块：帧内交互聚合（整合帧内对象的位置感知交互）和帧间运动聚合（利用轨迹引导的帧间对齐），以增强对象token表示和确保时间一致性。", "result": "在多个RVOS基准上的大量实验证明了TQF的优势以及其结构化查询设计和运动感知聚合模块的有效性。", "conclusion": "通过分解参照查询为动态构建的三重专业查询，并结合运动感知聚合模块，TQF成功解决了RVOS中查询选择偏差问题，并取得了卓越的性能。"}}
{"id": "2509.13832", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13832", "abs": "https://arxiv.org/abs/2509.13832", "authors": ["Teng Wang", "Haojun Jiang", "Yuxuan Wang", "Zhenguo Sun", "Xiangjie Yan", "Xiang Li", "Gao Huang"], "title": "UltraHiT: A Hierarchical Transformer Architecture for Generalizable Internal Carotid Artery Robotic Ultrasonography", "comment": null, "summary": "Carotid ultrasound is crucial for the assessment of cerebrovascular health,\nparticularly the internal carotid artery (ICA). While previous research has\nexplored automating carotid ultrasound, none has tackled the challenging ICA.\nThis is primarily due to its deep location, tortuous course, and significant\nindividual variations, which greatly increase scanning complexity. To address\nthis, we propose a Hierarchical Transformer-based decision architecture, namely\nUltraHiT, that integrates high-level variation assessment with low-level action\ndecision. Our motivation stems from conceptualizing individual vascular\nstructures as morphological variations derived from a standard vascular model.\nThe high-level module identifies variation and switches between two low-level\nmodules: an adaptive corrector for variations, or a standard executor for\nnormal cases. Specifically, both the high-level module and the adaptive\ncorrector are implemented as causal transformers that generate predictions\nbased on the historical scanning sequence. To ensure generalizability, we\ncollected the first large-scale ICA scanning dataset comprising 164\ntrajectories and 72K samples from 28 subjects of both genders. Based on the\nabove innovations, our approach achieves a 95% success rate in locating the ICA\non unseen individuals, outperforming baselines and demonstrating its\neffectiveness. Our code will be released after acceptance.", "AI": {"tldr": "本文提出了一种名为UltraHiT的基于分层Transformer的决策架构，用于自动化颈内动脉（ICA）超声扫描，解决了ICA因其复杂性而难以自动化的挑战，并在未见过个体上实现了95%的定位成功率。", "motivation": "颈内动脉（ICA）因其位置深、路径迂曲和个体差异大，导致超声扫描复杂，之前的自动化研究未能解决这一难题。研究者受将血管结构视为标准血管模型的形态变异这一概念启发，旨在开发一种能处理这些变异的自动化系统。", "method": "本文提出了UltraHiT，一个基于分层Transformer的决策架构，它将高层变异评估与低层动作决策相结合。高层模块识别变异，并在“自适应校正器”（处理变异）和“标准执行器”（处理正常情况）两个低层模块之间切换。高层模块和自适应校正器均实现为因果Transformer，根据历史扫描序列生成预测。为确保通用性，作者收集了首个大规模ICA扫描数据集，包含来自28名受试者的164条轨迹和72K个样本。", "result": "该方法在未见过的个体上实现了95%的ICA定位成功率，优于基线方法，证明了其有效性。", "conclusion": "UltraHiT成功解决了自动化颈内动脉超声扫描的挑战，通过其分层Transformer架构和对个体变异的处理，显著提高了ICA的定位成功率，具有重要的临床应用潜力。"}}
{"id": "2509.14023", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.14023", "abs": "https://arxiv.org/abs/2509.14023", "authors": ["Sami Ul Haq", "Sheila Castilho", "Yvette Graham"], "title": "Audio-Based Crowd-Sourced Evaluation of Machine Translation Quality", "comment": "Accepted at WMT2025 (ENNLP) for oral presented", "summary": "Machine Translation (MT) has achieved remarkable performance, with growing\ninterest in speech translation and multimodal approaches. However, despite\nthese advancements, MT quality assessment remains largely text centric,\ntypically relying on human experts who read and compare texts. Since many\nreal-world MT applications (e.g Google Translate Voice Mode, iFLYTEK\nTranslator) involve translation being spoken rather printed or read, a more\nnatural way to assess translation quality would be through speech as opposed\ntext-only evaluations. This study compares text-only and audio-based\nevaluations of 10 MT systems from the WMT General MT Shared Task, using\ncrowd-sourced judgments collected via Amazon Mechanical Turk. We additionally,\nperformed statistical significance testing and self-replication experiments to\ntest reliability and consistency of audio-based approach. Crowd-sourced\nassessments based on audio yield rankings largely consistent with text only\nevaluations but, in some cases, identify significant differences between\ntranslation systems. We attribute this to speech richer, more natural modality\nand propose incorporating speech-based assessments into future MT evaluation\nframeworks.", "AI": {"tldr": "本研究比较了机器翻译系统基于文本和基于音频的评估方法，发现音频评估与文本评估结果基本一致，但在某些情况下能发现显著差异，并建议将语音评估纳入未来的机器翻译评估框架。", "motivation": "尽管机器翻译技术取得了显著进步，尤其是在语音翻译和多模态方面，但其质量评估仍主要以文本为中心，依赖人工专家阅读和比较文本。然而，许多实际的机器翻译应用（如谷歌翻译语音模式、科大讯飞翻译机）涉及语音输出而非打印或阅读，因此，通过语音而非纯文本进行翻译质量评估会更自然。", "method": "本研究比较了来自WMT通用机器翻译共享任务的10个机器翻译系统的纯文本评估和基于音频的评估。评估数据通过Amazon Mechanical Turk收集的众包判断获得。此外，还进行了统计显著性检验和自我复制实验，以测试基于音频方法的可靠性和一致性。", "result": "基于音频的众包评估得出的系统排名与纯文本评估结果大体一致。然而，在某些情况下，基于音频的评估能够识别出翻译系统之间存在的显著差异。研究将此归因于语音作为一种更丰富、更自然的模态。", "conclusion": "本研究建议将基于语音的评估纳入未来的机器翻译评估框架，因为语音作为一种更丰富的模态，能够提供与文本评估一致但有时能揭示更多差异的评估结果。"}}
{"id": "2509.13687", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13687", "abs": "https://arxiv.org/abs/2509.13687", "authors": ["Kaniz Fatema", "Emad A. Mohammed", "Sukhjit Singh Sehra"], "title": "Taylor-Series Expanded Kolmogorov-Arnold Network for Medical Imaging Classification", "comment": null, "summary": "Effective and interpretable classification of medical images is a challenge\nin computer-aided diagnosis, especially in resource-limited clinical settings.\nThis study introduces spline-based Kolmogorov-Arnold Networks (KANs) for\naccurate medical image classification with limited, diverse datasets. The\nmodels include SBTAYLOR-KAN, integrating B-splines with Taylor series;\nSBRBF-KAN, combining B-splines with Radial Basis Functions; and SBWAVELET-KAN,\nembedding B-splines in Morlet wavelet transforms. These approaches leverage\nspline-based function approximation to capture both local and global\nnonlinearities. The models were evaluated on brain MRI, chest X-rays,\ntuberculosis X-rays, and skin lesion images without preprocessing,\ndemonstrating the ability to learn directly from raw data. Extensive\nexperiments, including cross-dataset validation and data reduction analysis,\nshowed strong generalization and stability. SBTAYLOR-KAN achieved up to 98.93%\naccuracy, with a balanced F1-score, maintaining over 86% accuracy using only\n30% of the training data across three datasets. Despite class imbalance in the\nskin cancer dataset, experiments on both imbalanced and balanced versions\nshowed SBTAYLOR-KAN outperforming other models, achieving 68.22% accuracy.\nUnlike traditional CNNs, which require millions of parameters (e.g., ResNet50\nwith 24.18M), SBTAYLOR-KAN achieves comparable performance with just 2,872\ntrainable parameters, making it more suitable for constrained medical\nenvironments. Gradient-weighted Class Activation Mapping (Grad-CAM) was used\nfor interpretability, highlighting relevant regions in medical images. This\nframework provides a lightweight, interpretable, and generalizable solution for\nmedical image classification, addressing the challenges of limited datasets and\ndata-scarce scenarios in clinical AI applications.", "AI": {"tldr": "本研究提出了一种基于样条的Kolmogorov-Arnold网络（KANs）框架，包括SBTAYLOR-KAN、SBRBF-KAN和SBWAVELET-KAN，用于在有限、多样化数据集下进行准确且可解释的医学图像分类。该方法参数量极少，泛化性强，适用于资源受限的临床环境。", "motivation": "计算机辅助诊断中，医学图像的有效且可解释分类是一个挑战，尤其是在资源受限的临床环境中，数据集往往有限且多样。传统的卷积神经网络（CNNs）通常需要数百万参数，不适用于此类受限环境。", "method": "本研究引入了基于样条的Kolmogorov-Arnold网络（KANs），具体包括：SBTAYLOR-KAN（结合B样条和泰勒级数）、SBRBF-KAN（结合B样条和径向基函数）以及SBWAVELET-KAN（将B样条嵌入莫莱小波变换）。这些模型利用基于样条的函数逼近来捕获局部和全局非线性，并能直接从原始数据中学习，无需预处理。模型在脑部MRI、胸部X射线、结核病X射线和皮肤病变图像上进行了评估，并使用梯度加权类激活映射（Grad-CAM）提高可解释性。", "result": "SBTAYLOR-KAN模型在多种医学图像数据集上表现出色，最高准确率达98.93%。即使仅使用30%的训练数据，其在三个数据集上仍能保持超过86%的准确率，展现出强大的泛化性和稳定性。在皮肤癌数据集的类别不平衡和平衡版本上，SBTAYLOR-KAN均优于其他模型，最高准确率为68.22%。与ResNet50（24.18M参数）相比，SBTAYLOR-KAN仅需2,872个可训练参数即可达到可比性能，更适用于受限的医疗环境。Grad-CAM成功地突出了图像中的相关区域，增强了可解释性。", "conclusion": "该框架为医学图像分类提供了一个轻量级、可解释且泛化性强的解决方案，有效应对了临床AI应用中数据集有限和数据稀缺场景的挑战。"}}
{"id": "2509.13789", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13789", "abs": "https://arxiv.org/abs/2509.13789", "authors": ["Hanshuai Cui", "Zhiqing Tang", "Zhifei Xu", "Zhi Yao", "Wenyi Zeng", "Weijia Jia"], "title": "BWCache: Accelerating Video Diffusion Transformers through Block-Wise Caching", "comment": null, "summary": "Recent advancements in Diffusion Transformers (DiTs) have established them as\nthe state-of-the-art method for video generation. However, their inherently\nsequential denoising process results in inevitable latency, limiting real-world\napplicability. Existing acceleration methods either compromise visual quality\ndue to architectural modifications or fail to reuse intermediate features at\nproper granularity. Our analysis reveals that DiT blocks are the primary\ncontributors to inference latency. Across diffusion timesteps, the feature\nvariations of DiT blocks exhibit a U-shaped pattern with high similarity during\nintermediate timesteps, which suggests substantial computational redundancy. In\nthis paper, we propose Block-Wise Caching (BWCache), a training-free method to\naccelerate DiT-based video generation. BWCache dynamically caches and reuses\nfeatures from DiT blocks across diffusion timesteps. Furthermore, we introduce\na similarity indicator that triggers feature reuse only when the differences\nbetween block features at adjacent timesteps fall below a threshold, thereby\nminimizing redundant computations while maintaining visual fidelity. Extensive\nexperiments on several video diffusion models demonstrate that BWCache achieves\nup to 2.24$\\times$ speedup with comparable visual quality.", "AI": {"tldr": "本文提出了一种名为Block-Wise Caching (BWCache) 的无训练方法，通过动态缓存和重用Diffusion Transformers (DiTs) 块的特征来加速基于DiT的视频生成，在保持视觉质量的同时实现了显著的加速。", "motivation": "Diffusion Transformers (DiTs) 在视频生成领域达到了最先进水平，但其固有的顺序去噪过程导致不可避免的延迟，限制了实际应用。现有加速方法要么因架构修改而损害视觉质量，要么未能以适当的粒度重用中间特征。分析表明，DiT块是推理延迟的主要来源，且在扩散时间步长中，DiT块的特征变化呈现U形模式，中间时间步长具有高度相似性，表明存在大量计算冗余。", "method": "本文提出了Block-Wise Caching (BWCache)，这是一种无需训练的加速DiT视频生成的方法。BWCache在扩散时间步长之间动态缓存和重用DiT块的特征。此外，引入了一个相似性指标，仅当相邻时间步长之间块特征的差异低于某个阈值时才触发特征重用，从而在最小化冗余计算的同时保持视觉保真度。", "result": "在多个视频扩散模型上进行的广泛实验表明，BWCache在保持可比视觉质量的同时，实现了高达2.24倍的加速。", "conclusion": "BWCache通过利用DiT块特征在中间时间步长的高相似性所产生的计算冗余，有效地加速了基于DiT的视频生成，同时通过智能的特征缓存和重用机制，成功地保持了生成视频的视觉质量。"}}
{"id": "2509.13833", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13833", "abs": "https://arxiv.org/abs/2509.13833", "authors": ["Zhikai Zhang", "Jun Guo", "Chao Chen", "Jilong Wang", "Chenghuai Lin", "Yunrui Lian", "Han Xue", "Zhenrong Wang", "Maoqi Liu", "Huaping Liu", "He Wang", "Li Yi"], "title": "Track Any Motions under Any Disturbances", "comment": null, "summary": "A foundational humanoid motion tracker is expected to be able to track\ndiverse, highly dynamic, and contact-rich motions. More importantly, it needs\nto operate stably in real-world scenarios against various dynamics\ndisturbances, including terrains, external forces, and physical property\nchanges for general practical use. To achieve this goal, we propose Any2Track\n(Track Any motions under Any disturbances), a two-stage RL framework to track\nvarious motions under multiple disturbances in the real world. Any2Track\nreformulates dynamics adaptability as an additional capability on top of basic\naction execution and consists of two key components: AnyTracker and AnyAdapter.\nAnyTracker is a general motion tracker with a series of careful designs to\ntrack various motions within a single policy. AnyAdapter is a history-informed\nadaptation module that endows the tracker with online dynamics adaptability to\novercome the sim2real gap and multiple real-world disturbances. We deploy\nAny2Track on Unitree G1 hardware and achieve a successful sim2real transfer in\na zero-shot manner. Any2Track performs exceptionally well in tracking various\nmotions under multiple real-world disturbances.", "AI": {"tldr": "Any2Track是一个两阶段强化学习框架，旨在实现人形机器人对各种运动在多种真实世界扰动下的稳定跟踪，并成功在硬件上进行了零样本迁移。", "motivation": "需要一个基础的人形运动跟踪器，它不仅能跟踪多样化、高动态、接触丰富的运动，更重要的是，能在真实世界中面对地形、外力、物理属性变化等各种动态扰动时稳定运行。", "method": "提出Any2Track框架，将动态适应性重新定义为在基本动作执行之上的额外能力。它包含两个核心组件：AnyTracker（一个通过精心设计实现单一策略跟踪各种运动的通用运动跟踪器）和AnyAdapter（一个基于历史信息的适应模块，赋予跟踪器在线动态适应性，以克服sim2real鸿沟和多种真实世界扰动）。", "result": "Any2Track已部署在Unitree G1硬件上，实现了零样本的sim2real迁移。它在多种真实世界扰动下，对各种运动的跟踪表现出色。", "conclusion": "Any2Track框架成功地解决了在真实世界复杂动态扰动下，人形机器人对多样化运动进行稳定跟踪的挑战，并展现了优秀的泛化和适应能力。"}}
{"id": "2509.14031", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14031", "abs": "https://arxiv.org/abs/2509.14031", "authors": ["Paweł Mąka", "Yusuf Can Semerci", "Jan Scholtes", "Gerasimos Spanakis"], "title": "You Are What You Train: Effects of Data Composition on Training Context-aware Machine Translation Models", "comment": "EMNLP 2025 main conference", "summary": "Achieving human-level translations requires leveraging context to ensure\ncoherence and handle complex phenomena like pronoun disambiguation. Sparsity of\ncontextually rich examples in the standard training data has been hypothesized\nas the reason for the difficulty of context utilization. In this work, we\nsystematically validate this claim in both single- and multilingual settings by\nconstructing training datasets with a controlled proportions of contextually\nrelevant examples. We demonstrate a strong association between training data\nsparsity and model performance confirming sparsity as a key bottleneck.\nImportantly, we reveal that improvements in one contextual phenomenon do no\ngeneralize to others. While we observe some cross-lingual transfer, it is not\nsignificantly higher between languages within the same sub-family. Finally, we\npropose and empirically evaluate two training strategies designed to leverage\nthe available data. These strategies improve context utilization, resulting in\naccuracy gains of up to 6 and 8 percentage points on the ctxPro evaluation in\nsingle- and multilingual settings respectively.", "AI": {"tldr": "本研究验证了训练数据稀疏性是神经机器翻译（NMT）上下文利用率低的关键瓶颈，发现上下文现象改进不泛化且跨语言迁移有限。文章提出了两种训练策略，显著提升了上下文利用准确性。", "motivation": "实现人类水平的翻译需要有效利用上下文来确保连贯性并处理代词消歧等复杂现象。现有的假设认为，标准训练数据中上下文丰富示例的稀疏性是NMT难以利用上下文的原因，本研究旨在系统地验证这一说法。", "method": "研究通过构建包含受控比例上下文相关示例的训练数据集（包括单语言和多语言设置），系统地验证了训练数据稀疏性与模型性能之间的关联。此外，研究还提出了两种旨在利用现有数据的训练策略，并对其进行了实证评估。", "result": "研究发现训练数据稀疏性与模型性能之间存在强关联，证实了稀疏性是关键瓶颈。一项上下文现象的改进不能泛化到其他现象。观察到一些跨语言迁移，但在同一语族内的语言之间并未显著更高。提出的两种训练策略提高了上下文利用率，在单语言和多语言设置下，ctxPro评估的准确率分别提高了6和8个百分点。", "conclusion": "数据稀疏性是NMT上下文利用的关键瓶颈。上下文现象的改进不具有泛化性，且跨语言迁移有限。本文提出的训练策略能有效利用现有数据，显著提升上下文利用能力。"}}
{"id": "2509.13711", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13711", "abs": "https://arxiv.org/abs/2509.13711", "authors": ["Qiuyu Tang", "Joshua Krinsky", "Aparna Bharati"], "title": "StyleProtect: Safeguarding Artistic Identity in Fine-tuned Diffusion Models", "comment": null, "summary": "The rapid advancement of generative models, particularly diffusion-based\napproaches, has inadvertently facilitated their potential for misuse. Such\nmodels enable malicious exploiters to replicate artistic styles that capture an\nartist's creative labor, personal vision, and years of dedication in an\ninexpensive manner. This has led to a rise in the need and exploration of\nmethods for protecting artworks against style mimicry. Although generic\ndiffusion models can easily mimic an artistic style, finetuning amplifies this\ncapability, enabling the model to internalize and reproduce the style with\nhigher fidelity and control. We hypothesize that certain cross-attention layers\nexhibit heightened sensitivity to artistic styles. Sensitivity is measured\nthrough activation strengths of attention layers in response to style and\ncontent representations, and assessing their correlations with features\nextracted from external models. Based on our findings, we introduce an\nefficient and lightweight protection strategy, StyleProtect, that achieves\neffective style defense against fine-tuned diffusion models by updating only\nselected cross-attention layers. Our experiments utilize a carefully curated\nartwork dataset based on WikiArt, comprising representative works from 30\nartists known for their distinctive and influential styles and cartoon\nanimations from the Anita dataset. The proposed method demonstrates promising\nperformance in safeguarding unique styles of artworks and anime from malicious\ndiffusion customization, while maintaining competitive imperceptibility.", "AI": {"tldr": "生成模型（特别是微调后的扩散模型）能轻易模仿艺术风格，本文提出StyleProtect，一种轻量级保护策略，通过更新选定的交叉注意力层来有效防御恶意风格模仿，并保持不可感知性。", "motivation": "生成模型（尤其是扩散模型）的快速发展使其易被滥用，恶意用户能廉价复制艺术风格，侵犯艺术家的劳动成果。因此，亟需开发保护艺术作品免受风格模仿的方法，尤其是在模型经过微调后模仿能力更强的情况下。", "method": "研究假设某些交叉注意力层对艺术风格高度敏感，并通过测量注意力层对风格和内容表示的激活强度及其与外部模型特征的关联来评估敏感性。基于这些发现，提出了一种名为StyleProtect的高效轻量级保护策略，该策略仅通过更新选定的交叉注意力层来防御经过微调的扩散模型。实验使用了基于WikiArt（30位艺术家作品）和Anita数据集（卡通动画）精心策划的艺术品数据集。", "result": "所提出的StyleProtect方法在保护艺术作品和动漫的独特风格免受恶意扩散模型定制方面表现出良好的性能。同时，它还能保持具有竞争力的不可感知性。", "conclusion": "StyleProtect通过选择性更新交叉注意力层，成功为艺术风格提供了针对微调扩散模型的有效防御，展现出有前景的保护效果和良好的不可感知性。"}}
{"id": "2509.13792", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13792", "abs": "https://arxiv.org/abs/2509.13792", "authors": ["Inder Pal Singh", "Nidhal Eddine Chenni", "Abd El Rahman Shabayek", "Arunkumar Rathinam", "Djamila Aouada"], "title": "Bridging the Synthetic-Real Gap: Supervised Domain Adaptation for Robust Spacecraft 6-DoF Pose Estimation", "comment": null, "summary": "Spacecraft Pose Estimation (SPE) is a fundamental capability for autonomous\nspace operations such as rendezvous, docking, and in-orbit servicing. Hybrid\npipelines that combine object detection, keypoint regression, and\nPerspective-n-Point (PnP) solvers have recently achieved strong results on\nsynthetic datasets, yet their performance deteriorates sharply on real or\nlab-generated imagery due to the persistent synthetic-to-real domain gap.\nExisting unsupervised domain adaptation approaches aim to mitigate this issue\nbut often underperform when a modest number of labeled target samples are\navailable. In this work, we propose the first Supervised Domain Adaptation\n(SDA) framework tailored for SPE keypoint regression. Building on the Learning\nInvariant Representation and Risk (LIRR) paradigm, our method jointly optimizes\ndomain-invariant representations and task-specific risk using both labeled\nsynthetic and limited labeled real data, thereby reducing generalization error\nunder domain shift. Extensive experiments on the SPEED+ benchmark demonstrate\nthat our approach consistently outperforms source-only, fine-tuning, and oracle\nbaselines. Notably, with only 5% labeled target data, our method matches or\nsurpasses oracle performance trained on larger fractions of labeled data. The\nframework is lightweight, backbone-agnostic, and computationally efficient,\noffering a practical pathway toward robust and deployable spacecraft pose\nestimation in real-world space environments.", "AI": {"tldr": "由于合成到真实数据的域间隙，航天器姿态估计（SPE）在真实图像上性能不佳。本文提出了首个针对SPE关键点回归的监督域适应（SDA）框架，利用有限的真实标记数据，显著提高了在真实世界环境中的鲁棒性。", "motivation": "航天器姿态估计（SPE）是自主空间操作的关键能力。混合管道在合成数据集上表现出色，但在真实或实验室生成的图像上，由于合成到真实的域间隙，性能急剧下降。现有的无监督域适应方法在有少量标记目标样本时表现不佳。", "method": "本文提出了首个针对SPE关键点回归的监督域适应（SDA）框架。该方法基于学习不变表示和风险（LIRR）范式，利用标记的合成数据和有限的标记真实数据，共同优化域不变表示和任务特定风险，从而减少域偏移下的泛化误差。", "result": "在SPEED+基准上的大量实验表明，该方法始终优于仅源训练、微调和基线方法。值得注意的是，仅使用5%的标记目标数据，该方法就能达到或超越使用更大比例标记数据训练的基线性能。该框架轻量、与骨干网络无关且计算高效。", "conclusion": "所提出的SDA框架为在真实世界空间环境中实现鲁棒和可部署的航天器姿态估计提供了一条实用途径，有效解决了合成到真实数据的域间隙问题。"}}
{"id": "2509.13839", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13839", "abs": "https://arxiv.org/abs/2509.13839", "authors": ["Motonari Kambara", "Komei Sugiura"], "title": "Pre-Manipulation Alignment Prediction with Parallel Deep State-Space and Transformer Models", "comment": "Published in Advanced Robotics", "summary": "In this work, we address the problem of predicting the future success of\nopen-vocabulary object manipulation tasks. Conventional approaches typically\ndetermine success or failure after the action has been carried out. However,\nthey make it difficult to prevent potential hazards and rely on failures to\ntrigger replanning, thereby reducing the efficiency of object manipulation\nsequences. To overcome these challenges, we propose a model, which predicts the\nalignment between a pre-manipulation egocentric image with the planned\ntrajectory and a given natural language instruction. We introduce a Multi-Level\nTrajectory Fusion module, which employs a state-of-the-art deep state-space\nmodel and a transformer encoder in parallel to capture multi-level time-series\nself-correlation within the end effector trajectory. Our experimental results\nindicate that the proposed method outperformed existing methods, including\nfoundation models.", "AI": {"tldr": "该研究提出了一种模型，用于在开放词汇对象操作任务执行前预测其成功率，以避免潜在危险并提高效率。", "motivation": "传统方法在动作执行后才判断成功或失败，这使得难以预防潜在危险，并依赖失败来触发重新规划，从而降低了对象操作序列的效率。", "method": "本文提出一个模型，预测操作前自我中心图像与规划轨迹和给定自然语言指令之间的对齐程度。该模型引入了一个多级轨迹融合模块，该模块并行使用先进的深度状态空间模型和Transformer编码器，以捕获末端执行器轨迹中的多级时间序列自相关性。", "result": "实验结果表明，所提出的方法优于现有方法，包括基础模型。", "conclusion": "该研究成功开发了一种在对象操作任务执行前预测成功率的模型，有效解决了传统方法的局限性，并表现出优越的性能。"}}
{"id": "2509.14034", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.14034", "abs": "https://arxiv.org/abs/2509.14034", "authors": ["Zijie Lin", "Bryan Hooi"], "title": "Enhancing Multi-Agent Debate System Performance via Confidence Expression", "comment": "EMNLP'25 Findings", "summary": "Generative Large Language Models (LLMs) have demonstrated remarkable\nperformance across a wide range of tasks. Recent research has introduced\nMulti-Agent Debate (MAD) systems, which leverage multiple LLMs to simulate\nhuman debate and thereby improve task performance. However, while some LLMs may\npossess superior knowledge or reasoning capabilities for specific tasks, they\noften struggle to clearly communicate this advantage during debates, in part\ndue to a lack of confidence expression. Moreover, inappropriate confidence\nexpression can cause agents in MAD systems to either stubbornly maintain\nincorrect beliefs or converge prematurely on suboptimal answers, ultimately\nreducing debate effectiveness and overall system performance. To address these\nchallenges, we propose incorporating confidence expression into MAD systems to\nallow LLMs to explicitly communicate their confidence levels. To validate this\napproach, we develop ConfMAD, a MAD framework that integrates confidence\nexpression throughout the debate process. Experimental results demonstrate the\neffectiveness of our method, and we further analyze how confidence influences\ndebate dynamics, offering insights into the design of confidence-aware MAD\nsystems.", "AI": {"tldr": "本文提出ConfMAD框架，通过在多智能体辩论（MAD）系统中整合置信度表达，解决大型语言模型（LLM）在辩论中因置信度沟通不足导致的性能问题，并验证其有效性。", "motivation": "多智能体辩论（MAD）系统中的LLM在特定任务上可能拥有优越知识或推理能力，但由于缺乏置信度表达，难以清晰传达这些优势。不恰当的置信度表达可能导致智能体固执己见或过早收敛到次优答案，从而降低辩论效率和系统性能。", "method": "本文提出将置信度表达整合到MAD系统中，允许LLM明确沟通其置信水平。为此，开发了ConfMAD框架，该框架在整个辩论过程中集成了置信度表达。", "result": "实验结果证明了ConfMAD方法的有效性。研究还进一步分析了置信度如何影响辩论动态，为设计置信度感知的MAD系统提供了见解。", "conclusion": "在MAD系统中整合置信度表达能有效提升LLM的辩论性能和系统效率，为未来设计更智能的多智能体系统提供了重要方向。"}}
{"id": "2509.13713", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13713", "abs": "https://arxiv.org/abs/2509.13713", "authors": ["Tae-Wook Um", "Ki-Hyeon Kim", "Hyun-Duck Choi", "Hyo-Sung Ahn"], "title": "UM-Depth : Uncertainty Masked Self-Supervised Monocular Depth Estimation with Visual Odometry", "comment": null, "summary": "Monocular depth estimation has been increasingly adopted in robotics and\nautonomous driving for its ability to infer scene geometry from a single\ncamera. In self-supervised monocular depth estimation frameworks, the network\njointly generates and exploits depth and pose estimates during training,\nthereby eliminating the need for depth labels. However, these methods remain\nchallenged by uncertainty in the input data, such as low-texture or dynamic\nregions, which can cause reduced depth accuracy. To address this, we introduce\nUM-Depth, a framework that combines motion- and uncertainty-aware refinement to\nenhance depth accuracy at dynamic object boundaries and in textureless regions.\nSpecifically, we develop a teacherstudent training strategy that embeds\nuncertainty estimation into both the training pipeline and network\narchitecture, thereby strengthening supervision where photometric signals are\nweak. Unlike prior motion-aware approaches that incur inference-time overhead\nand rely on additional labels or auxiliary networks for real-time generation,\nour method uses optical flow exclusively within the teacher network during\ntraining, which eliminating extra labeling demands and any runtime cost.\nExtensive experiments on the KITTI and Cityscapes datasets demonstrate the\neffectiveness of our uncertainty-aware refinement. Overall, UM-Depth achieves\nstate-of-the-art results in both self-supervised depth and pose estimation on\nthe KITTI datasets.", "AI": {"tldr": "UM-Depth是一种自监督单目深度估计框架，通过运动和不确定性感知细化来提高低纹理和动态区域的深度精度。它采用师生训练策略，在训练中利用光流和不确定性估计，避免了推理时额外开销和标签需求，并在KITTI和Cityscapes数据集上取得了最先进的结果。", "motivation": "自监督单目深度估计在机器人和自动驾驶中应用广泛，但面临输入数据不确定性（如低纹理或动态区域）的挑战，这会导致深度精度下降。", "method": "本文提出了UM-Depth框架，结合了运动和不确定性感知细化来增强动态物体边界和无纹理区域的深度精度。具体来说，它开发了一种师生训练策略，将不确定性估计嵌入到训练流程和网络架构中，以在光度信号较弱的区域加强监督。与现有方法不同，UM-Depth仅在训练期间的教师网络中使用光流，从而消除了额外的标签需求和任何运行时成本。", "result": "在KITTI和Cityscapes数据集上进行的广泛实验证明了其不确定性感知细化的有效性。UM-Depth在KITTI数据集上的自监督深度和姿态估计方面均取得了最先进的成果。", "conclusion": "UM-Depth通过创新的运动和不确定性感知细化以及师生训练策略，有效解决了自监督单目深度估计中由数据不确定性导致的精度问题，特别是在动态物体边界和无纹理区域。它在不引入额外推理成本或标签需求的情况下，实现了领先的深度和姿态估计性能。"}}
{"id": "2509.13926", "categories": ["cs.RO", "cs.AI", "cs.CV", "I.2.9; I.2.10"], "pdf": "https://arxiv.org/pdf/2509.13926", "abs": "https://arxiv.org/abs/2509.13926", "authors": ["Huilin Yin", "Yiming Kan", "Daniel Watzenig"], "title": "MAP: End-to-End Autonomous Driving with Map-Assisted Planning", "comment": "8 pages, 2 figures, accepted by ICCVW Author list updated to match\n  the camera-ready version, in compliance with conference policy", "summary": "In recent years, end-to-end autonomous driving has attracted increasing\nattention for its ability to jointly model perception, prediction, and planning\nwithin a unified framework. However, most existing approaches underutilize the\nonline mapping module, leaving its potential to enhance trajectory planning\nlargely untapped. This paper proposes MAP (Map-Assisted Planning), a novel\nmap-assisted end-to-end trajectory planning framework. MAP explicitly\nintegrates segmentation-based map features and the current ego status through a\nPlan-enhancing Online Mapping module, an Ego-status-guided Planning module, and\na Weight Adapter based on current ego status. Experiments conducted on the\nDAIR-V2X-seq-SPD dataset demonstrate that the proposed method achieves a 16.6%\nreduction in L2 displacement error, a 56.2% reduction in off-road rate, and a\n44.5% improvement in overall score compared to the UniV2X baseline, even\nwithout post-processing. Furthermore, it achieves top ranking in Track 2 of the\nEnd-to-End Autonomous Driving through V2X Cooperation Challenge of MEIS\nWorkshop @CVPR2025, outperforming the second-best model by 39.5% in terms of\noverall score. These results highlight the effectiveness of explicitly\nleveraging semantic map features in planning and suggest new directions for\nimproving structure design in end-to-end autonomous driving systems. Our code\nis available at https://gitee.com/kymkym/map.git", "AI": {"tldr": "本文提出MAP，一个地图辅助的端到端轨迹规划框架，通过显式整合地图特征和自车状态，显著提升了自动驾驶的规划性能。", "motivation": "现有的端到端自动驾驶方法未能充分利用在线地图模块的潜力，限制了其在轨迹规划中的增强作用。", "method": "本文提出了MAP（Map-Assisted Planning）框架，通过一个规划增强在线地图模块、一个自车状态引导规划模块和一个基于自车状态的权重适配器，显式整合了基于分割的地图特征和当前自车状态。", "result": "在DAIR-V2X-seq-SPD数据集上，MAP相比UniV2X基线，L2位移误差降低16.6%，偏离道路率降低56.2%，总体得分提高44.5%。它还在MEIS Workshop @CVPR2025的End-to-End Autonomous Driving through V2X Cooperation Challenge中获得赛道2第一名，总体得分比第二名高39.5%。", "conclusion": "实验结果表明，显式利用语义地图特征在规划中是有效的，并为端到端自动驾驶系统的结构设计提供了新的改进方向。"}}
{"id": "2509.13857", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13857", "abs": "https://arxiv.org/abs/2509.13857", "authors": ["Nguyen Hoang Khoi Tran", "Julie Stephany Berrio", "Mao Shan", "Stewart Worrall"], "title": "InterKey: Cross-modal Intersection Keypoints for Global Localization on OpenStreetMap", "comment": "8 pages, 5 figures", "summary": "Reliable global localization is critical for autonomous vehicles, especially\nin environments where GNSS is degraded or unavailable, such as urban canyons\nand tunnels. Although high-definition (HD) maps provide accurate priors, the\ncost of data collection, map construction, and maintenance limits scalability.\nOpenStreetMap (OSM) offers a free and globally available alternative, but its\ncoarse abstraction poses challenges for matching with sensor data. We propose\nInterKey, a cross-modal framework that leverages road intersections as\ndistinctive landmarks for global localization. Our method constructs compact\nbinary descriptors by jointly encoding road and building imprints from point\nclouds and OSM. To bridge modality gaps, we introduce discrepancy mitigation,\norientation determination, and area-equalized sampling strategies, enabling\nrobust cross-modal matching. Experiments on the KITTI dataset demonstrate that\nInterKey achieves state-of-the-art accuracy, outperforming recent baselines by\na large margin. The framework generalizes to sensors that can produce dense\nstructural point clouds, offering a scalable and cost-effective solution for\nrobust vehicle localization.", "AI": {"tldr": "InterKey是一个跨模态框架，利用路口作为地标，通过联合编码点云和OpenStreetMap中的道路及建筑信息构建紧凑的二值描述符，实现自动驾驶车辆的全局定位，在GNSS受限环境下提供可扩展且经济高效的解决方案。", "motivation": "自动驾驶车辆需要可靠的全局定位，尤其是在GNSS信号不佳（如城市峡谷、隧道）的环境中。高精地图成本高昂且维护困难，限制了其可扩展性。OpenStreetMap（OSM）虽然免费且全球可用，但其粗糙的抽象层级难以与传感器数据匹配，因此需要一种新的方法来克服这些挑战。", "method": "本文提出了InterKey框架，利用路口作为独特的标志物进行全局定位。该方法通过联合编码点云和OSM中的道路及建筑印记，构建紧凑的二值描述符。为弥合模态差异，引入了差异缓解、方向确定和面积均衡采样策略，以实现鲁棒的跨模态匹配。", "result": "在KITTI数据集上的实验表明，InterKey取得了最先进的精度，大幅优于现有的基线方法。该框架能够推广到产生密集结构点云的传感器，提供了一种可扩展且经济高效的车辆鲁棒定位解决方案。", "conclusion": "InterKey框架通过利用路口和跨模态匹配策略，为自动驾驶车辆在GNSS受限环境下提供了一种高精度、可扩展且经济高效的全局定位解决方案。"}}
{"id": "2509.14036", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14036", "abs": "https://arxiv.org/abs/2509.14036", "authors": ["Zekang Liu", "Wei Feng", "Fanhua Shang", "Lianyu Hu", "Jichao Feng", "Liqing Gao"], "title": "SSL-SSAW: Self-Supervised Learning with Sigmoid Self-Attention Weighting for Question-Based Sign Language Translation", "comment": null, "summary": "Sign Language Translation (SLT) bridges the communication gap between deaf\npeople and hearing people, where dialogue provides crucial contextual cues to\naid in translation. Building on this foundational concept, this paper proposes\nQuestion-based Sign Language Translation (QB-SLT), a novel task that explores\nthe efficient integration of dialogue. Unlike gloss (sign language\ntranscription) annotations, dialogue naturally occurs in communication and is\neasier to annotate. The key challenge lies in aligning multimodality features\nwhile leveraging the context of the question to improve translation. To address\nthis issue, we propose a cross-modality Self-supervised Learning with Sigmoid\nSelf-attention Weighting (SSL-SSAW) fusion method for sign language\ntranslation. Specifically, we employ contrastive learning to align\nmultimodality features in QB-SLT, then introduce a Sigmoid Self-attention\nWeighting (SSAW) module for adaptive feature extraction from question and sign\nlanguage sequences. Additionally, we leverage available question text through\nself-supervised learning to enhance representation and translation\ncapabilities. We evaluated our approach on newly constructed CSL-Daily-QA and\nPHOENIX-2014T-QA datasets, where SSL-SSAW achieved SOTA performance. Notably,\neasily accessible question assistance can achieve or even surpass the\nperformance of gloss assistance. Furthermore, visualization results demonstrate\nthe effectiveness of incorporating dialogue in improving translation quality.", "AI": {"tldr": "本文提出了一种基于问题的手语翻译（QB-SLT）新任务，通过利用对话上下文来弥补沟通障碍。为解决多模态对齐和上下文利用问题，提出了一种交叉模态自监督学习与Sigmoid自注意力加权（SSL-SSAW）融合方法，并在新构建的数据集上实现了最先进的性能，证明了问题辅助优于甚至超越了词汇（gloss）辅助。", "motivation": "手语翻译（SLT）旨在弥合聋哑人与健听人之间的沟通鸿沟。对话能提供关键的上下文线索以辅助翻译。然而，传统的手语翻译通常依赖于词汇（gloss）标注，这种标注方式不如对话自然，且标注难度大。因此，研究人员希望探索一种有效集成对话信息的手语翻译方法。", "method": "本文提出了基于问题的手语翻译（QB-SLT）任务，旨在有效整合对话信息。为解决多模态特征对齐和利用问题上下文的挑战，提出了一种交叉模态自监督学习与Sigmoid自注意力加权（SSL-SSAW）融合方法。具体而言，该方法采用对比学习来对齐QB-SLT中的多模态特征；引入Sigmoid自注意力加权（SSAW）模块，用于自适应地从问题和手语序列中提取特征；并通过自监督学习利用可用的问题文本来增强表示和翻译能力。", "result": "该方法在新建的CSL-Daily-QA和PHOENIX-2014T-QA数据集上进行了评估，SSL-SSAW取得了最先进（SOTA）的性能。值得注意的是，易于获取的问题辅助能够达到甚至超越词汇（gloss）辅助的性能。此外，可视化结果也证明了整合对话在提高翻译质量方面的有效性。", "conclusion": "本文成功提出了基于问题的手语翻译（QB-SLT）任务和SSL-SSAW融合方法，有效解决了手语翻译中多模态对齐和上下文利用的挑战。研究结果表明，通过利用易于获取的对话问题，可以显著提高手语翻译的性能，甚至超越传统的词汇辅助方法，为手语翻译领域开辟了新的方向。"}}
{"id": "2509.13747", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13747", "abs": "https://arxiv.org/abs/2509.13747", "authors": ["Ming Dai", "Wenxuan Cheng", "Jiang-Jiang Liu", "Lingfeng Yang", "Zhenhua Feng", "Wankou Yang", "Jingdong Wang"], "title": "Improving Generalized Visual Grounding with Instance-aware Joint Learning", "comment": "Accepted by IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI) in September 2025", "summary": "Generalized visual grounding tasks, including Generalized Referring\nExpression Comprehension (GREC) and Segmentation (GRES), extend the classical\nvisual grounding paradigm by accommodating multi-target and non-target\nscenarios. Specifically, GREC focuses on accurately identifying all referential\nobjects at the coarse bounding box level, while GRES aims for achieve\nfine-grained pixel-level perception. However, existing approaches typically\ntreat these tasks independently, overlooking the benefits of jointly training\nGREC and GRES to ensure consistent multi-granularity predictions and streamline\nthe overall process. Moreover, current methods often treat GRES as a semantic\nsegmentation task, neglecting the crucial role of instance-aware capabilities\nand the necessity of ensuring consistent predictions between instance-level\nboxes and masks. To address these limitations, we propose InstanceVG, a\nmulti-task generalized visual grounding framework equipped with instance-aware\ncapabilities, which leverages instance queries to unify the joint and\nconsistency predictions of instance-level boxes and masks. To the best of our\nknowledge, InstanceVG is the first framework to simultaneously tackle both GREC\nand GRES while incorporating instance-aware capabilities into generalized\nvisual grounding. To instantiate the framework, we assign each instance query a\nprior reference point, which also serves as an additional basis for target\nmatching. This design facilitates consistent predictions of points, boxes, and\nmasks for the same instance. Extensive experiments obtained on ten datasets\nacross four tasks demonstrate that InstanceVG achieves state-of-the-art\nperformance, significantly surpassing the existing methods in various\nevaluation metrics. The code and model will be publicly available at\nhttps://github.com/Dmmm1997/InstanceVG.", "AI": {"tldr": "InstanceVG是一个多任务通用视觉定位框架，首次同时处理GREC和GRES，并引入实例感知能力，通过实例查询统一预测，在多项任务上取得了最先进的性能。", "motivation": "现有的通用视觉定位方法通常独立处理广义指代表达式理解（GREC）和分割（GRES）任务，忽略了联合训练和多粒度预测一致性的益处。此外，GRES常被视为语义分割任务，忽视了实例感知能力的重要性以及实例级边界框和掩码之间预测一致性的必要性。", "method": "本文提出了InstanceVG框架，一个具备实例感知能力的多任务通用视觉定位框架。它利用实例查询来统一实例级边界框和掩码的联合与一致性预测。为实现此框架，每个实例查询被分配一个先验参考点，该点也作为目标匹配的额外依据，从而促进同一实例的点、框和掩码的一致性预测。", "result": "InstanceVG在涵盖四种任务的十个数据集上进行了广泛实验，结果表明它在各种评估指标上均取得了最先进的性能，显著超越了现有方法。", "conclusion": "InstanceVG是首个同时解决GREC和GRES任务，并融入实例感知能力的通用视觉定位框架。它通过统一的实例查询实现了多粒度预测的一致性，并在多项任务上展现出卓越的性能，为通用视觉定位领域带来了显著的改进。"}}
{"id": "2509.14001", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14001", "abs": "https://arxiv.org/abs/2509.14001", "authors": ["Elena Camuffo", "Francesco Barbato", "Mete Ozay", "Simone Milani", "Umberto Michieli"], "title": "MOCHA: Multi-modal Objects-aware Cross-arcHitecture Alignment", "comment": null, "summary": "We introduce MOCHA (Multi-modal Objects-aware Cross-arcHitecture Alignment),\na knowledge distillation approach that transfers region-level multimodal\nsemantics from a large vision-language teacher (e.g., LLaVa) into a lightweight\nvision-only object detector student (e.g., YOLO). A translation module maps\nstudent features into a joint space, where the training of the student and\ntranslator is guided by a dual-objective loss that enforces both local\nalignment and global relational consistency. Unlike prior approaches focused on\ndense or global alignment, MOCHA operates at the object level, enabling\nefficient transfer of semantics without modifying the teacher or requiring\ntextual input at inference. We validate our method across four personalized\ndetection benchmarks under few-shot regimes. Results show consistent gains over\nbaselines, with a +10.1 average score improvement. Despite its compact\narchitecture, MOCHA reaches performance on par with larger multimodal models,\nproving its suitability for real-world deployment.", "AI": {"tldr": "MOCHA是一种知识蒸馏方法，能将大型多模态教师模型（如LLaVa）的区域级多模态语义转移到轻量级纯视觉目标检测学生模型（如YOLO）中，实现高效的对象级对齐，并在少样本检测任务中取得显著性能提升。", "motivation": "现有方法专注于密集或全局对齐，效率不高且可能需要文本输入。研究旨在将大型视觉-语言模型（VLMs）的多模态语义有效且高效地迁移到轻量级纯视觉目标检测器，以适应实际部署需求，且推理时无需文本输入。", "method": "引入MOCHA（多模态对象感知跨架构对齐），通过知识蒸馏实现。一个翻译模块将学生模型的特征映射到一个联合空间，并使用双目标损失（dual-objective loss）指导学生模型和翻译模块的训练，该损失强制执行局部对齐和全局关系一致性。该方法在对象级别操作，无需修改教师模型。", "result": "在四个少样本个性化检测基准测试中，MOCHA始终优于基线方法，平均分数提高10.1分。尽管架构紧凑，MOCHA的性能与更大的多模态模型相当。", "conclusion": "MOCHA能够有效地将多模态语义从大型视觉-语言教师模型迁移到轻量级纯视觉目标检测学生模型，且无需在推理时输入文本。其紧凑的架构和强大的性能证明了其在实际部署中的适用性。"}}
{"id": "2509.13861", "categories": ["cs.RO", "I.6.0; A.0"], "pdf": "https://arxiv.org/pdf/2509.13861", "abs": "https://arxiv.org/abs/2509.13861", "authors": ["Görkem Kılınç Soylu", "Neziha Akalin", "Maria Riveiro"], "title": "Using Petri Nets for Context-Adaptive Robot Explanations", "comment": "In proceedings of TRUST 2025 (arXiv:2509.11402), a workshop at IEEE\n  RO-MAN 2025: https://www.ro-man2025.org/", "summary": "In human-robot interaction, robots must communicate in a natural and\ntransparent manner to foster trust, which requires adapting their communication\nto the context. In this paper, we propose using Petri nets (PNs) to model\ncontextual information for adaptive robot explanations. PNs provide a formal,\ngraphical method for representing concurrent actions, causal dependencies, and\nsystem states, making them suitable for analyzing dynamic interactions between\nhumans and robots. We demonstrate this approach through a scenario involving a\nrobot that provides explanations based on contextual cues such as user\nattention and presence. Model analysis confirms key properties, including\ndeadlock-freeness, context-sensitive reachability, boundedness, and liveness,\nshowing the robustness and flexibility of PNs for designing and verifying\ncontext-adaptive explanations in human-robot interactions.", "AI": {"tldr": "本文提出使用Petri网（PNs）对上下文信息进行建模，以实现机器人解释的自适应性，从而在人机交互中建立信任，并通过模型分析验证了其鲁棒性和灵活性。", "motivation": "在人机交互中，机器人必须以自然透明的方式进行沟通以建立信任，这要求其沟通能够适应上下文环境。", "method": "本文提出使用Petri网（PNs）来建模上下文信息，以实现机器人的自适应解释。PNs提供了一种形式化、图形化的方法来表示并发动作、因果依赖和系统状态。通过一个机器人根据用户注意力、存在等上下文线索提供解释的场景来演示该方法。", "result": "模型分析证实了关键特性，包括无死锁、上下文敏感可达性、有界性和活性，这表明Petri网在设计和验证人机交互中上下文自适应解释方面的鲁棒性和灵活性。", "conclusion": "Petri网是一种鲁棒且灵活的工具，适用于设计和验证人机交互中的上下文自适应解释系统。"}}
{"id": "2509.14128", "categories": ["cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.14128", "abs": "https://arxiv.org/abs/2509.14128", "authors": ["Monica Sekoyan", "Nithin Rao Koluguri", "Nune Tadevosyan", "Piotr Zelasko", "Travis Bartley", "Nick Karpov", "Jagadeesh Balam", "Boris Ginsburg"], "title": "Canary-1B-v2 & Parakeet-TDT-0.6B-v3: Efficient and High-Performance Models for Multilingual ASR and AST", "comment": "Mini Version of it Submitted to ICASSP 2026", "summary": "This report introduces Canary-1B-v2, a fast, robust multilingual model for\nAutomatic Speech Recognition (ASR) and Speech-to-Text Translation (AST). Built\nwith a FastConformer encoder and Transformer decoder, it supports 25 languages\nprimarily European. The model was trained on 1.7M hours of total data samples,\nincluding Granary and NeMo ASR Set 3.0, with non-speech audio added to reduce\nhallucinations for ASR and AST. We describe its two-stage pre-training and\nfine-tuning process with dynamic data balancing, as well as experiments with an\nnGPT encoder. Results show nGPT scales well with massive data, while\nFastConformer excels after fine-tuning. For timestamps, Canary-1B-v2 uses the\nNeMo Forced Aligner (NFA) with an auxiliary CTC model, providing reliable\nsegment-level timestamps for ASR and AST. Evaluations show Canary-1B-v2\noutperforms Whisper-large-v3 on English ASR while being 10x faster, and\ndelivers competitive multilingual ASR and AST performance against larger models\nlike Seamless-M4T-v2-large and LLM-based systems. We also release\nParakeet-TDT-0.6B-v3, a successor to v2, offering multilingual ASR across the\nsame 25 languages with just 600M parameters.", "AI": {"tldr": "该报告介绍了Canary-1B-v2，一个快速、鲁棒的多语言自动语音识别（ASR）和语音到文本翻译（AST）模型。它支持25种语言，在英语ASR上超越Whisper-large-v3且速度快10倍，同时在多语言ASR和AST方面与更大模型表现相当。", "motivation": "开发一个更快、更鲁棒且在多语言ASR和AST任务上表现卓越的模型，以应对现有模型在速度和性能上的挑战，并减少幻觉。", "method": "该模型采用FastConformer编码器和Transformer解码器架构，支持25种语言。训练数据总量达1.7M小时，包括Granary和NeMo ASR Set 3.0，并添加了非语音音频以减少幻觉。训练过程分为两阶段：预训练和微调，并采用动态数据平衡。时间戳功能通过NeMo Forced Aligner (NFA) 和辅助CTC模型实现。研究还比较了nGPT编码器。", "result": "Canary-1B-v2在英语ASR上优于Whisper-large-v3，且速度快10倍。在多语言ASR和AST方面，它与Seamless-M4T-v2-large和基于LLM的系统等更大模型相比具有竞争力。nGPT编码器在大规模数据下表现良好，而FastConformer在微调后表现出色。模型提供了可靠的段级时间戳。此外，还发布了Parakeet-TDT-0.6B-v3，一个参数量为600M的多语言ASR模型。", "conclusion": "Canary-1B-v2是一个高效且高性能的多语言ASR和AST模型，在速度和准确性方面均表现出色，尤其在英语ASR上显著优于现有模型。其鲁棒性和可靠的时间戳功能使其成为一个强大的工具。同时，更小的Parakeet-TDT-0.6B-v3的发布也展示了在保持性能的同时优化模型尺寸的潜力。"}}
{"id": "2509.13754", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13754", "abs": "https://arxiv.org/abs/2509.13754", "authors": ["Hao Yin", "Xin Man", "Feiyu Chen", "Jie Shao", "Heng Tao Shen"], "title": "Cross-modal Full-mode Fine-grained Alignment for Text-to-Image Person Retrieval", "comment": null, "summary": "Text-to-Image Person Retrieval (TIPR) is a cross-modal matching task that\naims to retrieve the most relevant person images based on a given text query.\nThe key challenge in TIPR lies in achieving effective alignment between textual\nand visual modalities within a common latent space. To address this challenge,\nprior approaches incorporate attention mechanisms for implicit cross-modal\nlocal alignment. However, they lack the ability to verify whether all local\nfeatures are correctly aligned. Moreover, existing methods primarily focus on\nhard negative samples during model updates, with the goal of refining\ndistinctions between positive and negative pairs, often neglecting incorrectly\nmatched positive pairs. To alleviate these issues, we propose FMFA, a\ncross-modal Full-Mode Fine-grained Alignment framework, which enhances global\nmatching through explicit fine-grained alignment and existing implicit\nrelational reasoning -- hence the term ``full-mode\" -- without requiring\nadditional supervision. Specifically, we design an Adaptive Similarity\nDistribution Matching (A-SDM) module to rectify unmatched positive sample\npairs. A-SDM adaptively pulls the unmatched positive pairs closer in the joint\nembedding space, thereby achieving more precise global alignment. Additionally,\nwe introduce an Explicit Fine-grained Alignment (EFA) module, which makes up\nfor the lack of verification capability of implicit relational reasoning. EFA\nstrengthens explicit cross-modal fine-grained interactions by sparsifying the\nsimilarity matrix and employs a hard coding method for local alignment. Our\nproposed method is evaluated on three public datasets, achieving\nstate-of-the-art performance among all global matching methods. Our code is\navailable at https://github.com/yinhao1102/FMFA.", "AI": {"tldr": "本文提出FMFA框架，通过自适应相似度分布匹配（A-SDM）纠正未匹配的正样本对，并引入显式细粒度对齐（EFA）模块进行局部对齐验证，解决了文本到图像行人检索（TIPR）中跨模态对齐的挑战，实现了最先进的性能。", "motivation": "文本到图像行人检索（TIPR）的关键挑战在于文本和视觉模态的有效对齐。现有方法存在问题：1) 基于注意力机制的隐式局部对齐缺乏验证能力；2) 主要关注难负样本，却忽视了错误匹配的正样本对。", "method": "本文提出FMFA（Full-Mode Fine-grained Alignment）框架，通过显式细粒度对齐和现有隐式关系推理增强全局匹配，无需额外监督。具体包括：1) 自适应相似度分布匹配（A-SDM）模块，通过自适应地拉近未匹配的正样本对，实现更精确的全局对齐；2) 显式细粒度对齐（EFA）模块，通过稀疏化相似度矩阵和硬编码方法进行局部对齐，弥补了隐式关系推理缺乏验证能力的不足。", "result": "所提出的方法在三个公开数据集上进行了评估，在所有全局匹配方法中取得了最先进的性能。", "conclusion": "FMFA框架通过引入A-SDM和EFA模块，有效解决了TIPR中跨模态对齐和正样本对处理的挑战，显著提升了检索性能，达到了最先进水平。"}}
{"id": "2509.14165", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14165", "abs": "https://arxiv.org/abs/2509.14165", "authors": ["Michal Szczepanski", "Martyna Poreba", "Karim Haroun"], "title": "Where Do Tokens Go? Understanding Pruning Behaviors in STEP at High Resolutions", "comment": null, "summary": "Vision Transformers (ViTs) achieve state-of-the-art performance in semantic\nsegmentation but are hindered by high computational and memory costs. To\naddress this, we propose STEP (SuperToken and Early-Pruning), a hybrid\ntoken-reduction framework that combines dynamic patch merging and token pruning\nto enhance efficiency without significantly compromising accuracy. At the core\nof STEP is dCTS, a lightweight CNN-based policy network that enables flexible\nmerging into superpatches. Encoder blocks integrate also early-exits to remove\nhigh-confident supertokens, lowering computational load. We evaluate our method\non high-resolution semantic segmentation benchmarks, including images up to\n1024 x 1024, and show that when dCTS is applied alone, the token count can be\nreduced by a factor of 2.5 compared to the standard 16 x 16 pixel patching\nscheme. This yields a 2.6x reduction in computational cost and a 3.4x increase\nin throughput when using ViT-Large as the backbone. Applying the full STEP\nframework further improves efficiency, reaching up to a 4x reduction in\ncomputational complexity and a 1.7x gain in inference speed, with a maximum\naccuracy drop of no more than 2.0%. With the proposed STEP configurations, up\nto 40% of tokens can be confidently predicted and halted before reaching the\nfinal encoder layer.", "AI": {"tldr": "本文提出STEP（SuperToken and Early-Pruning）框架，通过动态补丁合并和令牌剪枝，显著降低Vision Transformer（ViT）在语义分割中的计算和内存成本，同时保持高精度。", "motivation": "Vision Transformers在语义分割中表现卓越，但其高计算和内存成本是主要瓶颈。", "method": "STEP是一个混合令牌减少框架，结合了动态补丁合并和令牌剪枝。其核心是dCTS，一个轻量级基于CNN的策略网络，用于灵活地将补丁合并为超补丁。编码器块还集成了早期退出机制，用于移除高置信度的超令牌，从而降低计算负荷。", "result": "在1024x1024高分辨率语义分割基准测试中，单独应用dCTS可将令牌数量减少2.5倍，计算成本降低2.6倍，吞吐量提高3.4倍（使用ViT-Large骨干）。完整的STEP框架进一步将计算复杂度降低4倍，推理速度提高1.7倍，最大精度下降不超过2.0%。高达40%的令牌可以在到达最终编码器层之前被预测并停止。", "conclusion": "STEP框架通过动态令牌减少和早期退出，有效解决了ViT在语义分割中的高计算和内存成本问题，显著提升了效率，且对精度影响极小。"}}
{"id": "2509.13882", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.13882", "abs": "https://arxiv.org/abs/2509.13882", "authors": ["Junhwa Hong", "Beomjoon Lee", "Woojin Lee", "Changjoo Nam"], "title": "Repulsive Trajectory Modification and Conflict Resolution for Efficient Multi-Manipulator Motion Planning", "comment": "7 pages", "summary": "We propose an efficient motion planning method designed to efficiently find\ncollision-free trajectories for multiple manipulators. While multi-manipulator\nsystems offer significant advantages, coordinating their motions is\ncomputationally challenging owing to the high dimensionality of their composite\nconfiguration space. Conflict-Based Search (CBS) addresses this by decoupling\nmotion planning, but suffers from subsequent conflicts incurred by resolving\nexisting conflicts, leading to an exponentially growing constraint tree of CBS.\nOur proposed method is based on repulsive trajectory modification within the\ntwo-level structure of CBS. Unlike conventional CBS variants, the low-level\nplanner applies a gradient descent approach using an Artificial Potential\nField. This field generates repulsive forces that guide the trajectory of the\nconflicting manipulator away from those of other robots. As a result,\nsubsequent conflicts are less likely to occur. Additionally, we develop a\nstrategy that, under a specific condition, directly attempts to find a\nconflict-free solution in a single step without growing the constraint tree.\nThrough extensive tests including physical robot experiments, we demonstrate\nthat our method consistently reduces the number of expanded nodes in the\nconstraint tree, achieves a higher success rate, and finds a solution faster\ncompared to Enhanced CBS and other state-of-the-art algorithms.", "AI": {"tldr": "本文提出了一种高效的多机械臂运动规划方法，它基于冲突解决搜索（CBS）的两层结构，通过在低层规划器中引入人工势场和梯度下降进行排斥轨迹修改，以减少后续冲突并提高规划效率和成功率。", "motivation": "多机械臂系统具有显著优势，但由于其复合构型空间的高维度性，协调它们的运动在计算上具有挑战性。现有的冲突解决搜索（CBS）方法虽然通过解耦规划来解决此问题，但由于解决现有冲突会引发后续冲突，导致CBS的约束树呈指数级增长。", "method": "该方法基于CBS的两层结构，在低层规划器中应用梯度下降方法，利用人工势场生成排斥力，引导冲突机械臂的轨迹远离其他机器人，从而减少后续冲突的发生。此外，该方法还开发了一种策略，在特定条件下，无需增长约束树即可直接尝试一步找到无冲突解决方案。", "result": "通过广泛的测试（包括物理机器人实验），该方法在约束树中扩展的节点数量持续减少，成功率更高，并且比增强型CBS和其他最先进的算法更快地找到解决方案。", "conclusion": "所提出的方法通过在CBS框架内引入排斥轨迹修改和人工势场，有效地解决了多机械臂运动规划中的冲突和效率问题，显著提高了规划性能和成功率。"}}
{"id": "2509.14161", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.14161", "abs": "https://arxiv.org/abs/2509.14161", "authors": ["Brian Yan", "Injy Hamed", "Shuichiro Shimizu", "Vasista Lodagala", "William Chen", "Olga Iakovenko", "Bashar Talafha", "Amir Hussein", "Alexander Polok", "Kalvin Chang", "Dominik Klement", "Sara Althubaiti", "Puyuan Peng", "Matthew Wiesner", "Thamar Solorio", "Ahmed Ali", "Sanjeev Khudanpur", "Shinji Watanabe", "Chih-Chen Chen", "Zhen Wu", "Karim Benharrak", "Anuj Diwan", "Samuele Cornell", "Eunjung Yeo", "Kwanghee Choi", "Carlos Carvalho", "Karen Rosero"], "title": "CS-FLEURS: A Massively Multilingual and Code-Switched Speech Dataset", "comment": null, "summary": "We present CS-FLEURS, a new dataset for developing and evaluating\ncode-switched speech recognition and translation systems beyond high-resourced\nlanguages. CS-FLEURS consists of 4 test sets which cover in total 113 unique\ncode-switched language pairs across 52 languages: 1) a 14 X-English language\npair set with real voices reading synthetically generated code-switched\nsentences, 2) a 16 X-English language pair set with generative text-to-speech\n3) a 60 {Arabic, Mandarin, Hindi, Spanish}-X language pair set with the\ngenerative text-to-speech, and 4) a 45 X-English lower-resourced language pair\ntest set with concatenative text-to-speech. Besides the four test sets,\nCS-FLEURS also provides a training set with 128 hours of generative\ntext-to-speech data across 16 X-English language pairs. Our hope is that\nCS-FLEURS helps to broaden the scope of future code-switched speech research.\nDataset link: https://huggingface.co/datasets/byan/cs-fleurs.", "AI": {"tldr": "CS-FLEURS是一个新的数据集，用于开发和评估跨越高资源语言以外的语码转换语音识别和翻译系统，包含113个语码转换语言对，覆盖52种语言，并提供训练和测试集。", "motivation": "现有语码转换语音识别和翻译系统主要集中在高资源语言，研究者希望开发和评估超越这些语言的系统，并拓宽未来语码转换语音研究的范围。", "method": "构建了CS-FLEURS数据集，包含四个测试集和一个训练集。测试集涵盖113个独特的语码转换语言对（52种语言），其中包括真实人声录制合成语句、生成式文本转语音（TTS）和拼接式TTS数据。训练集包含128小时的生成式TTS数据，用于16个X-英语语言对。", "result": "成功构建并发布了CS-FLEURS数据集，该数据集包含四个测试集（覆盖113个独特的语码转换语言对，共52种语言）和一个包含128小时生成式TTS数据的训练集，支持多种语言组合和资源水平。", "conclusion": "CS-FLEURS数据集有望拓宽未来语码转换语音研究的范围，特别是在高资源语言之外的领域。"}}
{"id": "2509.13756", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13756", "abs": "https://arxiv.org/abs/2509.13756", "authors": ["Yuqi Yang", "Dongliang Chang", "Yuanchen Fang", "Yi-Zhe SonG", "Zhanyu Ma", "Jun Guo"], "title": "Controllable-Continuous Color Editing in Diffusion Model via Color Mapping", "comment": null, "summary": "In recent years, text-driven image editing has made significant progress.\nHowever, due to the inherent ambiguity and discreteness of natural language,\ncolor editing still faces challenges such as insufficient precision and\ndifficulty in achieving continuous control. Although linearly interpolating the\nembedding vectors of different textual descriptions can guide the model to\ngenerate a sequence of images with varying colors, this approach lacks precise\ncontrol over the range of color changes in the output images. Moreover, the\nrelationship between the interpolation coefficient and the resulting image\ncolor is unknown and uncontrollable. To address these issues, we introduce a\ncolor mapping module that explicitly models the correspondence between the text\nembedding space and image RGB values. This module predicts the corresponding\nembedding vector based on a given RGB value, enabling precise color control of\nthe generated images while maintaining semantic consistency. Users can specify\na target RGB range to generate images with continuous color variations within\nthe desired range, thereby achieving finer-grained, continuous, and\ncontrollable color editing. Experimental results demonstrate that our method\nperforms well in terms of color continuity and controllability.", "AI": {"tldr": "针对文本驱动图像编辑中颜色控制精度不足和连续性差的问题，本文提出一个颜色映射模块，显式建模文本嵌入空间与RGB值的对应关系，实现精确、连续且可控的颜色编辑。", "motivation": "现有文本驱动图像编辑在颜色编辑方面面临挑战，如精度不足和难以实现连续控制，原因在于自然语言的固有模糊性和离散性。线性插值文本嵌入向量虽然能生成颜色变化的图像序列，但缺乏对颜色变化范围的精确控制，且插值系数与图像颜色间的关系未知且不可控。", "method": "引入一个颜色映射模块，显式建模文本嵌入空间与图像RGB值之间的对应关系。该模块根据给定的RGB值预测相应的嵌入向量，从而实现对生成图像颜色的精确控制，同时保持语义一致性。用户可以通过指定目标RGB范围来生成在该范围内具有连续颜色变化的图像。", "result": "实验结果表明，本文方法在颜色连续性和可控性方面表现良好，实现了更精细、连续且可控的颜色编辑。", "conclusion": "通过引入颜色映射模块，本文有效解决了文本驱动图像编辑中颜色控制精度不足和连续性差的问题，实现了精确、连续且可控的颜色编辑，提升了用户对图像颜色的控制能力。"}}
{"id": "2509.14180", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2.7; J.4"], "pdf": "https://arxiv.org/pdf/2509.14180", "abs": "https://arxiv.org/abs/2509.14180", "authors": ["Akhil Theerthala"], "title": "Synthesizing Behaviorally-Grounded Reasoning Chains: A Data-Generation Framework for Personal Finance LLMs", "comment": "24 pages, 11 figures. The paper presents a novel framework for\n  generating a personal finance dataset. The resulting fine-tuned model and\n  dataset are publicly available", "summary": "Personalized financial advice requires consideration of user goals,\nconstraints, risk tolerance, and jurisdiction. Prior LLM work has focused on\nsupport systems for investors and financial planners. Simultaneously, numerous\nrecent studies examine broader personal finance tasks, including budgeting,\ndebt management, retirement, and estate planning, through agentic pipelines\nthat incur high maintenance costs, yielding less than 25% of their expected\nfinancial returns. In this study, we introduce a novel and reproducible\nframework that integrates relevant financial context with behavioral finance\nstudies to construct supervision data for end-to-end advisors. Using this\nframework, we create a 19k sample reasoning dataset and conduct a comprehensive\nfine-tuning of the Qwen-3-8B model on the dataset. Through a held-out test\nsplit and a blind LLM-jury study, we demonstrate that through careful data\ncuration and behavioral integration, our 8B model achieves performance\ncomparable to significantly larger baselines (14-32B parameters) across factual\naccuracy, fluency, and personalization metrics while incurring 80% lower costs\nthan the larger counterparts.", "AI": {"tldr": "本研究提出了一种结合金融背景和行为金融学的框架，用于构建端到端个性化金融顾问的监督数据。通过对Qwen-3-8B模型进行微调，该模型在事实准确性、流畅性和个性化方面实现了与大型模型（14-32B）相当的性能，同时成本降低了80%。", "motivation": "个性化金融建议需要考虑用户目标、限制、风险承受能力和管辖权。现有的LLM工作主要集中在投资者和金融规划师的支持系统上。同时，针对预算、债务管理、退休和遗产规划等个人金融任务的代理管道成本高昂，且预期财务回报率不到25%。", "method": "引入了一个新颖且可复现的框架，该框架将相关的金融背景与行为金融学研究相结合，以构建用于端到端顾问的监督数据。使用此框架创建了一个包含19k样本的推理数据集，并在此数据集上对Qwen-3-8B模型进行了全面的微调。", "result": "通过保留测试集和盲审LLM-陪审团研究，结果表明，通过精心的数据整理和行为整合，其8B模型在事实准确性、流畅性和个性化指标上达到了与显著更大的基线模型（14-32B参数）相当的性能，同时成本比大型模型降低了80%。", "conclusion": "本研究证明，通过仔细的数据整理和行为整合，较小的LLM模型（如8B参数）也能在个性化金融建议领域实现与大型模型相当甚至超越的性能，同时显著降低成本，为构建高效、个性化的金融顾问提供了可行方案。"}}
{"id": "2509.13903", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13903", "abs": "https://arxiv.org/abs/2509.13903", "authors": ["Artem Lykov", "Jeffrin Sam", "Hung Khang Nguyen", "Vladislav Kozlovskiy", "Yara Mahmoud", "Valerii Serpiva", "Miguel Altamirano Cabrera", "Mikhail Konenkov", "Dzmitry Tsetserukou"], "title": "PhysicalAgent: Towards General Cognitive Robotics with Foundation World Models", "comment": "submitted to IEEE conference", "summary": "We introduce PhysicalAgent, an agentic framework for robotic manipulation\nthat integrates iterative reasoning, diffusion-based video generation, and\nclosed-loop execution. Given a textual instruction, our method generates short\nvideo demonstrations of candidate trajectories, executes them on the robot, and\niteratively re-plans in response to failures. This approach enables robust\nrecovery from execution errors. We evaluate PhysicalAgent across multiple\nperceptual modalities (egocentric, third-person, and simulated) and robotic\nembodiments (bimanual UR3, Unitree G1 humanoid, simulated GR1), comparing\nagainst state-of-the-art task-specific baselines. Experiments demonstrate that\nour method consistently outperforms prior approaches, achieving up to 83%\nsuccess on human-familiar tasks. Physical trials reveal that first-attempt\nsuccess is limited (20-30%), yet iterative correction increases overall success\nto 80% across platforms. These results highlight the potential of video-based\ngenerative reasoning for general-purpose robotic manipulation and underscore\nthe importance of iterative execution for recovering from initial failures. Our\nframework paves the way for scalable, adaptable, and robust robot control.", "AI": {"tldr": "PhysicalAgent是一个用于机器人操作的智能体框架，它结合了迭代推理、基于扩散的视频生成和闭环执行，以实现对执行错误的鲁棒恢复，并在多模态和多机器人平台上表现出色。", "motivation": "当前的机器人操作方法在面对执行错误时恢复能力不足，因此需要一种能够鲁棒处理失败并迭代改进的通用框架。", "method": "PhysicalAgent接收文本指令，生成候选轨迹的短视频演示，在机器人上执行这些轨迹，并根据失败情况迭代地重新规划。该方法整合了迭代推理、扩散模型视频生成和闭环执行。", "result": "PhysicalAgent在多种感知模态和机器人实体上进行了评估，结果显示它持续优于现有方法，在人类熟悉任务上实现了高达83%的成功率。虽然首次尝试成功率有限（20-30%），但迭代修正将总成功率提高到跨平台的80%。", "conclusion": "研究结果突出了基于视频的生成推理在通用机器人操作中的潜力，并强调了迭代执行对于从初始失败中恢复的重要性。该框架为可扩展、适应性强和鲁棒的机器人控制铺平了道路。"}}
{"id": "2509.14171", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.14171", "abs": "https://arxiv.org/abs/2509.14171", "authors": ["Yifan Liu", "Wenkuan Zhao", "Shanshan Zhong", "Jinghui Qin", "Mingfu Liang", "Zhongzhan Huang", "Wushao Wen"], "title": "AssoCiAm: A Benchmark for Evaluating Association Thinking while Circumventing Ambiguity", "comment": null, "summary": "Recent advancements in multimodal large language models (MLLMs) have garnered\nsignificant attention, offering a promising pathway toward artificial general\nintelligence (AGI). Among the essential capabilities required for AGI,\ncreativity has emerged as a critical trait for MLLMs, with association serving\nas its foundation. Association reflects a model' s ability to think creatively,\nmaking it vital to evaluate and understand. While several frameworks have been\nproposed to assess associative ability, they often overlook the inherent\nambiguity in association tasks, which arises from the divergent nature of\nassociations and undermines the reliability of evaluations. To address this\nissue, we decompose ambiguity into two types-internal ambiguity and external\nambiguity-and introduce AssoCiAm, a benchmark designed to evaluate associative\nability while circumventing the ambiguity through a hybrid computational\nmethod. We then conduct extensive experiments on MLLMs, revealing a strong\npositive correlation between cognition and association. Additionally, we\nobserve that the presence of ambiguity in the evaluation process causes MLLMs'\nbehavior to become more random-like. Finally, we validate the effectiveness of\nour method in ensuring more accurate and reliable evaluations. See Project Page\nfor the data and codes.", "AI": {"tldr": "本文提出AssoCiAm基准，通过混合计算方法解决多模态大语言模型（MLLMs）关联能力评估中固有的模糊性问题，发现认知与关联存在正相关，且模糊性会导致模型行为更随机，验证了新方法的准确性和可靠性。", "motivation": "多模态大语言模型（MLLMs）在通向通用人工智能（AGI）的道路上备受关注，其中创造力是关键能力，而关联是其基础。现有评估关联能力的方法常忽视关联任务固有的模糊性，这种模糊性源于关联的发散性，并损害了评估的可靠性，因此需要一种更准确可靠的评估方法。", "method": "将关联任务中的模糊性分解为内部模糊性和外部模糊性两种类型。引入了AssoCiAm基准，并采用混合计算方法来规避模糊性，从而评估模型的关联能力。随后在MLLMs上进行了广泛的实验。", "result": "实验结果显示，认知与关联之间存在强烈的正相关关系。此外，评估过程中模糊性的存在会导致MLLMs的行为变得更加随机化。研究还验证了所提出的方法在确保更准确和可靠的评估方面的有效性。", "conclusion": "所提出的AssoCiAm基准和混合计算方法能够有效解决MLLMs关联能力评估中的模糊性问题，从而提供更准确和可靠的评估结果。这对于深入理解MLLMs的创造力潜能及其向AGI发展至关重要。"}}
{"id": "2509.13760", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13760", "abs": "https://arxiv.org/abs/2509.13760", "authors": ["Jinwoo Jeon", "JunHyeok Oh", "Hayeong Lee", "Byung-Jun Lee"], "title": "Iterative Prompt Refinement for Safer Text-to-Image Generation", "comment": null, "summary": "Text-to-Image (T2I) models have made remarkable progress in generating images\nfrom text prompts, but their output quality and safety still depend heavily on\nhow prompts are phrased. Existing safety methods typically refine prompts using\nlarge language models (LLMs), but they overlook the images produced, which can\nresult in unsafe outputs or unnecessary changes to already safe prompts. To\naddress this, we propose an iterative prompt refinement algorithm that uses\nVision Language Models (VLMs) to analyze both the input prompts and the\ngenerated images. By leveraging visual feedback, our method refines prompts\nmore effectively, improving safety while maintaining user intent and\nreliability comparable to existing LLM-based approaches. Additionally, we\nintroduce a new dataset labeled with both textual and visual safety signals\nusing off-the-shelf multi-modal LLM, enabling supervised fine-tuning.\nExperimental results demonstrate that our approach produces safer outputs\nwithout compromising alignment with user intent, offering a practical solution\nfor generating safer T2I content. Our code is available at\nhttps://github.com/ku-dmlab/IPR. \\textbf{\\textcolor{red}WARNING: This paper\ncontains examples of harmful or inappropriate images generated by models.", "AI": {"tldr": "T2I模型输出安全性受提示词影响，现有方法忽略图像。本文提出IPR算法，利用VLM分析提示词和生成图像，迭代优化提示词，提高安全性并保持用户意图。同时构建了一个新的多模态安全数据集。", "motivation": "文本到图像（T2I）模型的输出质量和安全性严重依赖提示词措辞。现有安全方法通常使用大型语言模型（LLMs）精炼提示词，但忽略了实际生成的图像，这可能导致不安全输出或对已安全提示词进行不必要的修改。", "method": "本文提出了一种迭代提示词精炼算法（IPR），该算法利用视觉语言模型（VLMs）分析输入提示词和生成的图像。通过利用视觉反馈，该方法能更有效地精炼提示词。此外，研究引入了一个新的数据集，该数据集使用现成的多模态LLM标注了文本和视觉安全信号，以支持监督微调。", "result": "实验结果表明，本文方法在不损害用户意图一致性的前提下，能够生成更安全的输出。与现有基于LLM的方法相比，该方法在保持用户意图和可靠性方面具有可比性。", "conclusion": "该研究提供了一种实用的解决方案，通过利用视觉语言模型的视觉反馈迭代精炼提示词，并结合多模态安全数据集进行监督微调，从而生成更安全的T2I内容。"}}
{"id": "2509.14199", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14199", "abs": "https://arxiv.org/abs/2509.14199", "authors": ["Haichao Zhang", "Wenhao Chai", "Shwai He", "Ang Li", "Yun Fu"], "title": "Dense Video Understanding with Gated Residual Tokenization", "comment": null, "summary": "High temporal resolution is essential for capturing fine-grained details in\nvideo understanding. However, current video large language models (VLLMs) and\nbenchmarks mostly rely on low-frame-rate sampling, such as uniform sampling or\nkeyframe selection, discarding dense temporal information. This compromise\navoids the high cost of tokenizing every frame, which otherwise leads to\nredundant computation and linear token growth as video length increases. While\nthis trade-off works for slowly changing content, it fails for tasks like\nlecture comprehension, where information appears in nearly every frame and\nrequires precise temporal alignment. To address this gap, we introduce Dense\nVideo Understanding (DVU), which enables high-FPS video comprehension by\nreducing both tokenization time and token overhead. Existing benchmarks are\nalso limited, as their QA pairs focus on coarse content changes. We therefore\npropose DIVE (Dense Information Video Evaluation), the first benchmark designed\nfor dense temporal reasoning. To make DVU practical, we present Gated Residual\nTokenization (GRT), a two-stage framework: (1) Motion-Compensated Inter-Gated\nTokenization uses pixel-level motion estimation to skip static regions during\ntokenization, achieving sub-linear growth in token count and compute. (2)\nSemantic-Scene Intra-Tokenization Merging fuses tokens across static regions\nwithin a scene, further reducing redundancy while preserving dynamic semantics.\nExperiments on DIVE show that GRT outperforms larger VLLM baselines and scales\npositively with FPS. These results highlight the importance of dense temporal\ninformation and demonstrate that GRT enables efficient, scalable high-FPS video\nunderstanding.", "AI": {"tldr": "本文提出密集视频理解（DVU）框架，通过门控残差分词（GRT）技术实现高效的高帧率视频理解，并引入DIVE基准测试来评估密集时间推理能力，实验证明GRT优于现有大型VLLM。", "motivation": "现有视频大型语言模型（VLLMs）和基准测试多采用低帧率采样，忽略了视频中密集的时序信息，这对于讲座理解等需要精确时间对齐的任务是不足的。同时，直接对每帧进行分词会导致高昂的计算成本和冗余。", "method": "本文提出密集视频理解（DVU）框架，旨在减少分词时间和令牌开销以实现高帧率视频理解。为此，引入了DIVE（密集信息视频评估）基准测试，专门用于密集时间推理。核心方法是门控残差分词（GRT），一个两阶段框架：1) 运动补偿帧间门控分词，利用像素级运动估计跳过静态区域，实现令牌数量和计算的次线性增长。2) 语义场景帧内分词合并，在场景内融合静态区域的令牌，进一步减少冗余同时保留动态语义。", "result": "在DIVE基准测试上的实验表明，GRT优于更大的VLLM基线模型，并且性能随帧率（FPS）的增加而正向提升。", "conclusion": "这些结果突出了密集时间信息的重要性，并证明GRT能够实现高效、可扩展的高帧率视频理解，有效解决了现有VLLM在处理密集时间信息方面的不足。"}}
{"id": "2509.13948", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13948", "abs": "https://arxiv.org/abs/2509.13948", "authors": ["Benedict Barrow", "Roger K. Moore"], "title": "The Influence of Facial Features on the Perceived Trustworthiness of a Social Robot", "comment": "In proceedings of TRUST 2025 (arXiv:2509.11402), a workshop at IEEE\n  RO-MAN 2025: https://ro-man2025.org/", "summary": "Trust and the perception of trustworthiness play an important role in\ndecision-making and our behaviour towards others, and this is true not only of\nhuman-human interactions but also of human-robot interactions. While\nsignificant advances have been made in recent years in the field of social\nrobotics, there is still some way to go before we fully understand the factors\nthat influence human trust in robots. This paper presents the results of a\nstudy into the first impressions created by a social robot's facial features,\nbased on the hypothesis that a `babyface' engenders trust. By manipulating the\nback-projected face of a Furhat robot, the study confirms that eye shape and\nsize have a significant impact on the perception of trustworthiness. The work\nthus contributes to an understanding of the design choices that need to be made\nwhen developing social robots so as to optimise the effectiveness of\nhuman-robot interaction.", "AI": {"tldr": "本研究探讨了社交机器人面部特征（特别是“娃娃脸”）对人类信任感的影响，发现眼睛的形状和大小显著影响信任感知。", "motivation": "信任在人际互动和人机互动中都至关重要。尽管社交机器人领域进展显著，但我们仍未完全理解影响人类信任机器人的因素。本研究旨在探究社交机器人面部特征（特别是“娃娃脸”效应）如何影响初次信任感。", "method": "研究基于“娃娃脸”能产生信任的假设，通过操纵Furhat机器人背投式面部的眼睛形状和大小，进行了一项实验。", "result": "研究结果证实，眼睛的形状和大小对信任感知有显著影响。", "conclusion": "这项工作有助于理解在开发社交机器人时应如何进行设计选择，以优化人机交互的有效性。"}}
{"id": "2509.14197", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.14197", "abs": "https://arxiv.org/abs/2509.14197", "authors": ["Vahid Ghafouri", "Robert McNeil", "Teodor Yankov", "Madeleine Sumption", "Luc Rocher", "Scott A. Hale", "Adam Mahdi"], "title": "Framing Migration: A Computational Analysis of UK Parliamentary Discourse", "comment": null, "summary": "We present a large-scale computational analysis of migration-related\ndiscourse in UK parliamentary debates spanning over 75 years and compare it\nwith US congressional discourse. Using open-weight LLMs, we annotate each\nstatement with high-level stances toward migrants and track the net tone toward\nmigrants across time and political parties. For the UK, we extend this with a\nsemi-automated framework for extracting fine-grained narrative frames to\ncapture nuances of migration discourse. Our findings show that, while US\ndiscourse has grown increasingly polarised, UK parliamentary attitudes remain\nrelatively aligned across parties, with a persistent ideological gap between\nLabour and the Conservatives, reaching its most negative level in 2025. The\nanalysis of narrative frames in the UK parliamentary statements reveals a shift\ntoward securitised narratives such as border control and illegal immigration,\nwhile longer-term integration-oriented frames such as social integration have\ndeclined. Moreover, discussions of national law about immigration have been\nreplaced over time by international law and human rights, revealing nuances in\ndiscourse trends. Taken together broadly, our findings demonstrate how LLMs can\nsupport scalable, fine-grained discourse analysis in political and historical\ncontexts.", "AI": {"tldr": "本文对英美议会75年来的移民相关言论进行大规模计算分析，发现英国党派立场相对一致但叙事转向安全化，而美国则日益两极分化，并展示了LLMs在此类分析中的潜力。", "motivation": "旨在通过大规模计算分析，理解英国议会长期以来的移民相关言论演变，并与美国国会进行比较，追踪不同时期和政治党派对移民的态度和叙事框架。", "method": "使用开源大语言模型（LLMs）标注每项声明对移民的高级立场，并追踪跨时间和党派的净态度。针对英国，进一步采用半自动化框架提取细粒度叙事框架，以捕捉移民话语的细微差别。", "result": "美国国会言论日益两极分化，而英国议会态度在党派间保持相对一致，但工党和保守党之间存在持续的意识形态鸿沟，并在2025年达到最负面水平。英国叙事框架分析显示，话语转向边境控制和非法移民等安全化叙事，而社会融合等长期整合导向的框架有所下降。此外，关于国家移民法的讨论逐渐被国际法和人权取代。", "conclusion": "本研究结果表明，大语言模型（LLMs）能够支持政治和历史背景下可扩展、细粒度的言论分析，为理解复杂社会话语提供有力工具。"}}
{"id": "2509.13762", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13762", "abs": "https://arxiv.org/abs/2509.13762", "authors": ["Kai Chen", "Jin Xiao", "Leheng Zhang", "Kexuan Shi", "Shuhang Gu"], "title": "Task-Aware Image Signal Processor for Advanced Visual Perception", "comment": null, "summary": "In recent years, there has been a growing trend in computer vision towards\nexploiting RAW sensor data, which preserves richer information compared to\nconventional low-bit RGB images. Early studies mainly focused on enhancing\nvisual quality, while more recent efforts aim to leverage the abundant\ninformation in RAW data to improve the performance of visual perception tasks\nsuch as object detection and segmentation. However, existing approaches still\nface two key limitations: large-scale ISP networks impose heavy computational\noverhead, while methods based on tuning traditional ISP pipelines are\nrestricted by limited representational capacity.To address these issues, we\npropose Task-Aware Image Signal Processing (TA-ISP), a compact RAW-to-RGB\nframework that produces task-oriented representations for pretrained vision\nmodels. Instead of heavy dense convolutional pipelines, TA-ISP predicts a small\nset of lightweight, multi-scale modulation operators that act at global,\nregional, and pixel scales to reshape image statistics across different spatial\nextents. This factorized control significantly expands the range of spatially\nvarying transforms that can be represented while keeping memory usage,\ncomputation, and latency tightly constrained. Evaluated on several RAW-domain\ndetection and segmentation benchmarks under both daytime and nighttime\nconditions, TA-ISP consistently improves downstream accuracy while markedly\nreducing parameter count and inference time, making it well suited for\ndeployment on resource-constrained devices.", "AI": {"tldr": "本文提出了一种名为TA-ISP的紧凑型RAW-to-RGB框架，通过预测轻量级、多尺度的调制算子，为预训练视觉模型生成任务导向的表示，从而在显著降低计算开销的同时提高目标检测和分割等视觉感知任务的准确性。", "motivation": "计算机视觉领域越来越倾向于利用保留更丰富信息的RAW传感器数据。然而，现有方法存在两个主要限制：大规模ISP网络导致计算开销巨大，而基于传统ISP管线的方法则受限于表示能力不足。研究动机是解决这些问题，以更高效地利用RAW数据提升视觉感知任务的性能。", "method": "本文提出了任务感知图像信号处理（TA-ISP）框架。它不使用沉重的密集卷积管线，而是预测一组轻量级、多尺度的调制算子。这些算子在全局、区域和像素级别上作用，以重塑不同空间范围内的图像统计数据。这种分解控制显著扩展了可表示的空间可变变换范围，同时严格限制了内存使用、计算和延迟。", "result": "TA-ISP在白天和夜间条件下，针对多个RAW域检测和分割基准进行了评估，结果表明它能持续提高下游任务的准确性，并显著减少参数数量和推理时间。这使得TA-ISP非常适合部署在资源受限的设备上。", "conclusion": "TA-ISP通过提供一种紧凑且高效的RAW-to-RGB框架，成功解决了现有方法的计算开销大和表示能力有限的问题。它能够为预训练视觉模型生成任务导向的表示，从而在保持高性能的同时，显著降低资源消耗，适用于多种视觉感知任务和设备。"}}
{"id": "2509.14233", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14233", "abs": "https://arxiv.org/abs/2509.14233", "authors": ["Alejandro Hernández-Cano", "Alexander Hägele", "Allen Hao Huang", "Angelika Romanou", "Antoni-Joan Solergibert", "Barna Pasztor", "Bettina Messmer", "Dhia Garbaya", "Eduard Frank Ďurech", "Ido Hakimi", "Juan García Giraldo", "Mete Ismayilzada", "Negar Foroutan", "Skander Moalla", "Tiancheng Chen", "Vinko Sabolčec", "Yixuan Xu", "Michael Aerni", "Badr AlKhamissi", "Ines Altemir Marinas", "Mohammad Hossein Amani", "Matin Ansaripour", "Ilia Badanin", "Harold Benoit", "Emanuela Boros", "Nicholas Browning", "Fabian Bösch", "Maximilian Böther", "Niklas Canova", "Camille Challier", "Clement Charmillot", "Jonathan Coles", "Jan Deriu", "Arnout Devos", "Lukas Drescher", "Daniil Dzenhaliou", "Maud Ehrmann", "Dongyang Fan", "Simin Fan", "Silin Gao", "Miguel Gila", "María Grandury", "Diba Hashemi", "Alexander Hoyle", "Jiaming Jiang", "Mark Klein", "Andrei Kucharavy", "Anastasiia Kucherenko", "Frederike Lübeck", "Roman Machacek", "Theofilos Manitaras", "Andreas Marfurt", "Kyle Matoba", "Simon Matrenok", "Henrique Mendoncça", "Fawzi Roberto Mohamed", "Syrielle Montariol", "Luca Mouchel", "Sven Najem-Meyer", "Jingwei Ni", "Gennaro Oliva", "Matteo Pagliardini", "Elia Palme", "Andrei Panferov", "Léo Paoletti", "Marco Passerini", "Ivan Pavlov", "Auguste Poiroux", "Kaustubh Ponkshe", "Nathan Ranchin", "Javi Rando", "Mathieu Sauser", "Jakhongir Saydaliev", "Muhammad Ali Sayfiddinov", "Marian Schneider", "Stefano Schuppli", "Marco Scialanga", "Andrei Semenov", "Kumar Shridhar", "Raghav Singhal", "Anna Sotnikova", "Alexander Sternfeld", "Ayush Kumar Tarun", "Paul Teiletche", "Jannis Vamvas", "Xiaozhe Yao", "Hao Zhao Alexander Ilic", "Ana Klimovic", "Andreas Krause", "Caglar Gulcehre", "David Rosenthal", "Elliott Ash", "Florian Tramèr", "Joost VandeVondele", "Livio Veraldi", "Martin Rajman", "Thomas Schulthess", "Torsten Hoefler", "Antoine Bosselut", "Martin Jaggi", "Imanol Schlag"], "title": "Apertus: Democratizing Open and Compliant LLMs for Global Language Environments", "comment": null, "summary": "We present Apertus, a fully open suite of large language models (LLMs)\ndesigned to address two systemic shortcomings in today's open model ecosystem:\ndata compliance and multilingual representation. Unlike many prior models that\nrelease weights without reproducible data pipelines or regard for content-owner\nrights, Apertus models are pretrained exclusively on openly available data,\nretroactively respecting robots.txt exclusions and filtering for\nnon-permissive, toxic, and personally identifiable content. To mitigate risks\nof memorization, we adopt the Goldfish objective during pretraining, strongly\nsuppressing verbatim recall of data while retaining downstream task\nperformance. The Apertus models also expand multilingual coverage, training on\n15T tokens from over 1800 languages, with ~40% of pretraining data allocated to\nnon-English content. Released at 8B and 70B scales, Apertus approaches\nstate-of-the-art results among fully open models on multilingual benchmarks,\nrivalling or surpassing open-weight counterparts. Beyond model weights, we\nrelease all scientific artifacts from our development cycle with a permissive\nlicense, including data preparation scripts, checkpoints, evaluation suites,\nand training code, enabling transparent audit and extension.", "AI": {"tldr": "Apertus 是一个完全开放的大型语言模型 (LLM) 套件，旨在解决开放模型生态系统中数据合规性和多语言表示不足的问题，通过使用开放数据、Goldfish 目标和扩展多语言覆盖来实现。", "motivation": "目前的开放模型生态系统存在两个主要系统性缺陷：数据合规性（许多模型发布权重时缺乏可复现的数据管道，不尊重内容所有者权利）和多语言表示不足。", "method": "Apertus 模型仅使用公开可用数据进行预训练，追溯遵守 robots.txt 排除，并过滤非许可、有毒和个人身份识别内容。为减轻记忆风险，预训练期间采用 Goldfish 目标，以强烈抑制数据逐字回忆。模型扩展了多语言覆盖，在来自 1800 多种语言的 15T token 上进行训练，其中约 40% 的预训练数据分配给非英语内容。同时，Apertus 以 8B 和 70B 规模发布，并提供所有科学产物，包括数据准备脚本、检查点、评估套件和训练代码，均采用宽松许可。", "result": "Apertus 在多语言基准测试中，在完全开放模型中接近最先进水平，性能媲美或超越了开放权重的同类模型。", "conclusion": "Apertus 通过其对数据合规性、多语言覆盖和开发透明度的承诺，为开放 LLM 生态系统树立了新标准，有效解决了现有模型的系统性缺陷。"}}
{"id": "2509.13949", "categories": ["cs.RO", "I.2.9"], "pdf": "https://arxiv.org/pdf/2509.13949", "abs": "https://arxiv.org/abs/2509.13949", "authors": ["Jannick Stranghöner", "Philipp Hartmann", "Marco Braun", "Sebastian Wrede", "Klaus Neumann"], "title": "SHaRe-RL: Structured, Interactive Reinforcement Learning for Contact-Rich Industrial Assembly Tasks", "comment": "8 pages, 5 figures, submitted to the IEEE International Conference on\n  Robotics and Automation (ICRA) 2026", "summary": "High-mix low-volume (HMLV) industrial assembly, common in small and\nmedium-sized enterprises (SMEs), requires the same precision, safety, and\nreliability as high-volume automation while remaining flexible to product\nvariation and environmental uncertainty. Current robotic systems struggle to\nmeet these demands. Manual programming is brittle and costly to adapt, while\nlearning-based methods suffer from poor sample efficiency and unsafe\nexploration in contact-rich tasks. To address this, we present SHaRe-RL, a\nreinforcement learning framework that leverages multiple sources of prior\nknowledge. By (i) structuring skills into manipulation primitives, (ii)\nincorporating human demonstrations and online corrections, and (iii) bounding\ninteraction forces with per-axis compliance, SHaRe-RL enables efficient and\nsafe online learning for long-horizon, contact-rich industrial assembly tasks.\nExperiments on the insertion of industrial Harting connector modules with\n0.2-0.4 mm clearance demonstrate that SHaRe-RL achieves reliable performance\nwithin practical time budgets. Our results show that process expertise, without\nrequiring robotics or RL knowledge, can meaningfully contribute to learning,\nenabling safer, more robust, and more economically viable deployment of RL for\nindustrial assembly.", "AI": {"tldr": "SHaRe-RL是一个强化学习框架，通过整合先验知识、人工演示和在线修正，实现了高混合低产量工业装配任务的高效安全在线学习。", "motivation": "高混合低产量（HMLV）工业装配需要高精度、安全性和可靠性，同时要求对产品变化和环境不确定性保持灵活性。现有机器人系统（手动编程和基于学习的方法）在适应性、样本效率和接触密集任务的安全性方面存在不足，难以满足这些需求。", "method": "本文提出了SHaRe-RL，一个利用多源先验知识的强化学习框架。具体方法包括：(i) 将技能结构化为操作原语；(ii) 结合人类演示和在线修正；(iii) 通过轴向柔顺性限制交互力。这些措施旨在实现长周期、接触密集型工业装配任务的高效安全在线学习。", "result": "在0.2-0.4毫米间隙的工业Harting连接器模块插入实验中，SHaRe-RL在实际时间预算内实现了可靠的性能。结果表明，即使没有机器人或强化学习知识，工艺专业知识也能有效地促进学习，从而实现更安全、更稳健、更经济可行的工业装配强化学习部署。", "conclusion": "SHaRe-RL通过有效整合先验知识和工艺专业知识，为工业装配任务的强化学习部署提供了一个更安全、更稳健、更经济可行的解决方案。"}}
{"id": "2509.13836", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13836", "abs": "https://arxiv.org/abs/2509.13836", "authors": ["Weihang Wang", "Xinhao Li", "Ziyue Wang", "Yan Pang", "Jielei Zhang", "Peiyi Li", "Qiang Zhang", "Longwen Gao"], "title": "Diving into Mitigating Hallucinations from a Vision Perspective for Large Vision-Language Models", "comment": "Accepted by EMNLP2025 Finding", "summary": "Object hallucination in Large Vision-Language Models (LVLMs) significantly\nimpedes their real-world applicability. As the primary component for accurately\ninterpreting visual information, the choice of visual encoder is pivotal. We\nhypothesize that the diverse training paradigms employed by different visual\nencoders instill them with distinct inductive biases, which leads to their\ndiverse hallucination performances. Existing benchmarks typically focus on\ncoarse-grained hallucination detection and fail to capture the diverse\nhallucinations elaborated in our hypothesis. To systematically analyze these\neffects, we introduce VHBench-10, a comprehensive benchmark with approximately\n10,000 samples for evaluating LVLMs across ten fine-grained hallucination\ncategories. Our evaluations confirm encoders exhibit unique hallucination\ncharacteristics. Building on these insights and the suboptimality of simple\nfeature fusion, we propose VisionWeaver, a novel Context-Aware Routing Network.\nIt employs global visual features to generate routing signals, dynamically\naggregating visual features from multiple specialized experts. Comprehensive\nexperiments confirm the effectiveness of VisionWeaver in significantly reducing\nhallucinations and improving overall model performance.", "AI": {"tldr": "本文提出VHBench-10基准，用于细粒度评估大型视觉语言模型（LVLMs）中的对象幻觉问题，并基于此提出VisionWeaver模型，通过上下文感知路由网络动态聚合多专家视觉特征，有效减少幻觉并提升模型性能。", "motivation": "大型视觉语言模型（LVLMs）中的对象幻觉问题严重阻碍了其在实际场景中的应用。研究者假设不同视觉编码器的训练范式导致其归纳偏置不同，进而产生多样化的幻觉表现，而现有基准未能捕捉这些细粒度的幻觉差异。", "method": "1. 引入VHBench-10，一个包含约10,000个样本的综合基准，用于在十个细粒度幻觉类别上评估LVLMs。2. 基于对幻觉特性和简单特征融合次优性的洞察，提出VisionWeaver，一个新颖的上下文感知路由网络。该网络利用全局视觉特征生成路由信号，动态聚合来自多个专业专家模型的视觉特征。", "result": "1. 评估结果证实了不同视觉编码器表现出独特的幻觉特征。2. 综合实验证实了VisionWeaver在显著减少幻觉和提高整体模型性能方面的有效性。", "conclusion": "视觉编码器的训练范式和归纳偏置是导致LVLMs中多样化幻觉表现的关键因素。通过提出的VHBench-10基准可以系统分析这些细粒度幻觉。VisionWeaver作为一种上下文感知路由网络，通过动态聚合多专家视觉特征，能有效解决对象幻觉问题并提升LVLMs的性能。"}}
{"id": "2509.13766", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13766", "abs": "https://arxiv.org/abs/2509.13766", "authors": ["Huichun Liu", "Xiaosong Li", "Yang Liu", "Xiaoqi Cheng", "Haishu Tan"], "title": "NDLPNet: A Location-Aware Nighttime Deraining Network and a Real-World Benchmark Dataset", "comment": null, "summary": "Visual degradation caused by rain streak artifacts in low-light conditions\nsignificantly hampers the performance of nighttime surveillance and autonomous\nnavigation. Existing image deraining techniques are primarily designed for\ndaytime conditions and perform poorly under nighttime illumination due to the\nspatial heterogeneity of rain distribution and the impact of light-dependent\nstripe visibility. In this paper, we propose a novel Nighttime Deraining\nLocation-enhanced Perceptual Network(NDLPNet) that effectively captures the\nspatial positional information and density distribution of rain streaks in\nlow-light environments. Specifically, we introduce a Position Perception Module\n(PPM) to capture and leverage spatial contextual information from input data,\nenhancing the model's capability to identify and recalibrate the importance of\ndifferent feature channels. The proposed nighttime deraining network can\neffectively remove the rain streaks as well as preserve the crucial background\ninformation. Furthermore, We construct a night scene rainy (NSR) dataset\ncomprising 900 image pairs, all based on real-world nighttime scenes, providing\na new benchmark for nighttime deraining task research. Extensive qualitative\nand quantitative experimental evaluations on both existing datasets and the NSR\ndataset consistently demonstrate our method outperform the state-of-the-art\n(SOTA) methods in nighttime deraining tasks. The source code and dataset is\navailable at https://github.com/Feecuin/NDLPNet.", "AI": {"tldr": "本文提出了一种名为NDLPNet的新型夜间去雨网络，通过引入位置感知模块有效处理低光照下雨纹的空间位置和密度信息。同时构建了一个夜间雨景数据集（NSR）。实验证明NDLPNet在夜间去雨任务上优于现有最先进方法。", "motivation": "低光照条件下雨纹伪影导致的视觉退化严重影响夜间监控和自动驾驶性能。现有图像去雨技术主要针对白天条件设计，在夜间照明下因雨分布的空间异质性和光照依赖的条纹可见性而表现不佳。", "method": "提出了一种新颖的夜间去雨位置增强感知网络（NDLPNet），旨在有效捕获低光照环境中雨纹的空间位置信息和密度分布。具体来说，引入了一个位置感知模块（PPM）来捕获和利用输入数据的空间上下文信息，增强模型识别和重新校准不同特征通道重要性的能力。此外，构建了一个包含900对图像的夜间雨景（NSR）数据集，全部基于真实世界夜间场景。", "result": "所提出的夜间去雨网络能够有效去除雨纹并保留关键背景信息。在现有数据集和NSR数据集上进行的广泛定性和定量实验评估一致表明，NDLPNet在夜间去雨任务中优于最先进（SOTA）方法。", "conclusion": "NDLPNet及其新构建的NSR数据集为夜间去雨任务提供了一个有效的解决方案和新的基准，显著提升了该领域的技术水平，解决了现有技术在低光照条件下去雨能力不足的问题。"}}
{"id": "2509.13956", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13956", "abs": "https://arxiv.org/abs/2509.13956", "authors": ["Zewei Yang", "Zengqi Peng", "Jun Ma"], "title": "SEG-Parking: Towards Safe, Efficient, and Generalizable Autonomous Parking via End-to-End Offline Reinforcement Learning", "comment": null, "summary": "Autonomous parking is a critical component for achieving safe and efficient\nurban autonomous driving. However, unstructured environments and dynamic\ninteractions pose significant challenges to autonomous parking tasks. To\naddress this problem, we propose SEG-Parking, a novel end-to-end offline\nreinforcement learning (RL) framework to achieve interaction-aware autonomous\nparking. Notably, a specialized parking dataset is constructed for parking\nscenarios, which include those without interference from the opposite vehicle\n(OV) and complex ones involving interactions with the OV. Based on this\ndataset, a goal-conditioned state encoder is pretrained to map the fused\nperception information into the latent space. Then, an offline RL policy is\noptimized with a conservative regularizer that penalizes out-of-distribution\nactions. Extensive closed-loop experiments are conducted in the high-fidelity\nCARLA simulator. Comparative results demonstrate the superior performance of\nour framework with the highest success rate and robust generalization to\nout-of-distribution parking scenarios. The related dataset and source code will\nbe made publicly available after the paper is accepted.", "AI": {"tldr": "本文提出SEG-Parking，一个端到端的离线强化学习（RL）框架，用于实现交互感知的自动泊车。该框架利用专门构建的泊车数据集、预训练的目标条件状态编码器和带有保守正则化的离线RL策略，在CARLA模拟器中表现出卓越的成功率和泛化能力。", "motivation": "自动泊车是城市自动驾驶的关键组成部分，但非结构化环境和动态交互（如与对向车辆的互动）对其构成重大挑战。", "method": "1. 提出了SEG-Parking，一个端到端的离线强化学习框架。2. 构建了一个包含有无对向车辆（OV）干扰的专业泊车数据集。3. 预训练了一个目标条件状态编码器，将融合感知信息映射到潜在空间。4. 使用惩罚分布外（OOD）动作的保守正则化器优化离线RL策略。5. 在高保真CARLA模拟器中进行闭环实验评估。", "result": "SEG-Parking框架在CARLA模拟器中表现出卓越的性能，具有最高的成功率，并能稳健地泛化到分布外（OOD）的泊车场景。", "conclusion": "SEG-Parking框架有效解决了复杂环境下的交互感知自动泊车问题，通过专门的数据集和离线RL方法，实现了高成功率和强大的泛化能力。"}}
{"id": "2509.13767", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13767", "abs": "https://arxiv.org/abs/2509.13767", "authors": ["Daiqi Liu", "Tomás Arias-Vergara", "Johannes Enk", "Fangxu Xing", "Maureen Stone", "Jerry L. Prince", "Jana Hutter", "Andreas Maier", "Jonghye Woo", "Paula Andrea Pérez-Toro"], "title": "VocSegMRI: Multimodal Learning for Precise Vocal Tract Segmentation in Real-time MRI", "comment": "Preprint submitted to ICASSP", "summary": "Accurately segmenting articulatory structures in real-time magnetic resonance\nimaging (rtMRI) remains challenging, as most existing methods rely almost\nentirely on visual cues. Yet synchronized acoustic and phonological signals\nprovide complementary context that can enrich visual information and improve\nprecision. In this paper, we introduce VocSegMRI, a multimodal framework that\nintegrates video, audio, and phonological inputs through cross-attention fusion\nfor dynamic feature alignment. To further enhance cross-modal representation,\nwe incorporate a contrastive learning objective that improves segmentation\nperformance even when the audio modality is unavailable at inference. Evaluated\non a sub-set of USC-75 rtMRI dataset, our approach achieves state-of-the-art\nperformance, with a Dice score of 0.95 and a 95th percentile Hausdorff Distance\n(HD_95) of 4.20 mm, outperforming both unimodal and multimodal baselines.\nAblation studies confirm the contributions of cross-attention and contrastive\nlearning to segmentation precision and robustness. These results highlight the\nvalue of integrative multimodal modeling for accurate vocal tract analysis.", "AI": {"tldr": "本文提出VocSegMRI，一个多模态框架，通过整合视频、音频和音韵输入，并利用交叉注意力融合和对比学习，实现了实时磁共振成像（rtMRI）发音结构分割的最新技术水平。", "motivation": "实时磁共振成像（rtMRI）中的发音结构分割主要依赖视觉线索，但同步的声学和音韵信号可以提供补充上下文信息，从而丰富视觉信息并提高分割精度。", "method": "本文引入了VocSegMRI框架，该框架通过交叉注意力融合整合视频、音频和音韵输入，以实现动态特征对齐。此外，还结合了对比学习目标，以增强跨模态表示，即使在推理时音频模态不可用也能提高分割性能。", "result": "在USC-75 rtMRI数据集的一个子集上，VocSegMRI取得了最先进的性能，Dice分数为0.95，95% Hausdorff距离（HD_95）为4.20毫米，优于单模态和多模态基线。消融研究证实了交叉注意力和对比学习对分割精度和鲁棒性的贡献。", "conclusion": "这些结果强调了集成多模态建模对于准确声道的分析具有重要价值。"}}
{"id": "2509.13965", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13965", "abs": "https://arxiv.org/abs/2509.13965", "authors": ["Abhijeet Nayak", "Débora N. P. Oliveira", "Samiran Gode", "Cordelia Schmid", "Wolfram Burgard"], "title": "MetricNet: Recovering Metric Scale in Generative Navigation Policies", "comment": null, "summary": "Generative navigation policies have made rapid progress in improving\nend-to-end learned navigation. Despite their promising results, this paradigm\nhas two structural problems. First, the sampled trajectories exist in an\nabstract, unscaled space without metric grounding. Second, the control strategy\ndiscards the full path, instead moving directly towards a single waypoint. This\nleads to short-sighted and unsafe actions, moving the robot towards obstacles\nthat a complete and correctly scaled path would circumvent. To address these\nissues, we propose MetricNet, an effective add-on for generative navigation\nthat predicts the metric distance between waypoints, grounding policy outputs\nin real-world coordinates. We evaluate our method in simulation with a new\nbenchmarking framework and show that executing MetricNet-scaled waypoints\nsignificantly improves both navigation and exploration performance. Beyond\nsimulation, we further validate our approach in real-world experiments.\nFinally, we propose MetricNav, which integrates MetricNet into a navigation\npolicy to guide the robot away from obstacles while still moving towards the\ngoal.", "AI": {"tldr": "本文提出了MetricNet和MetricNav，通过预测路点间的度量距离，解决了生成式导航策略中轨迹无尺度和控制短视的问题，显著提升了导航和探索性能。", "motivation": "生成式导航策略存在两个结构性问题：一是采样的轨迹存在于抽象、无尺度的空间中，缺乏度量基础；二是控制策略只关注单个路点，忽略完整路径，导致短视和不安全的行为，可能使机器人撞向障碍物。", "method": "提出MetricNet，一个生成式导航的附加模块，用于预测路点间的度量距离，从而将策略输出映射到真实世界坐标。在此基础上，进一步提出MetricNav，将MetricNet集成到导航策略中，以引导机器人避开障碍物并持续向目标移动。", "result": "通过新的基准模拟框架评估，执行MetricNet缩放的路点显著提升了导航和探索性能。此外，该方法在真实世界实验中也得到了验证。", "conclusion": "MetricNet和MetricNav成功地为生成式导航策略提供了度量基础，解决了其固有的无尺度和短视问题，从而显著提高了机器人在复杂环境中的导航和探索的安全性与效率。"}}
{"id": "2509.13768", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13768", "abs": "https://arxiv.org/abs/2509.13768", "authors": ["Jianhui Chang"], "title": "Generative Image Coding with Diffusion Prior", "comment": null, "summary": "As generative technologies advance, visual content has evolved into a complex\nmix of natural and AI-generated images, driving the need for more efficient\ncoding techniques that prioritize perceptual quality. Traditional codecs and\nlearned methods struggle to maintain subjective quality at high compression\nratios, while existing generative approaches face challenges in visual fidelity\nand generalization. To this end, we propose a novel generative coding framework\nleveraging diffusion priors to enhance compression performance at low bitrates.\nOur approach employs a pre-optimized encoder to generate generalized\ncompressed-domain representations, integrated with the pretrained model's\ninternal features via a lightweight adapter and an attentive fusion module.\nThis framework effectively leverages existing pretrained diffusion models and\nenables efficient adaptation to different pretrained models for new\nrequirements with minimal retraining costs. We also introduce a distribution\nrenormalization method to further enhance reconstruction fidelity. Extensive\nexperiments show that our method (1) outperforms existing methods in visual\nfidelity across low bitrates, (2) improves compression performance by up to 79%\nover H.266/VVC, and (3) offers an efficient solution for AI-generated content\nwhile being adaptable to broader content types.", "AI": {"tldr": "本文提出了一种利用扩散先验的生成式编码框架，旨在低比特率下提高压缩性能和感知质量，尤其适用于混合的自然和AI生成内容。", "motivation": "随着生成技术的发展，视觉内容日益复杂，由自然和AI生成图像混合而成，这促使人们需要更高效且优先考虑感知质量的编码技术。传统和学习型编解码器在实现高压缩比时难以保持主观质量，而现有生成方法则面临视觉保真度和泛化性的挑战。", "method": "我们提出了一种新颖的生成式编码框架，利用扩散先验来增强低比特率下的压缩性能。该方法采用预优化的编码器生成通用的压缩域表示，并通过轻量级适配器和注意力融合模块与预训练模型的内部特征集成。此外，引入了分布重归一化方法以进一步提高重建保真度。", "result": "实验结果表明，我们的方法（1）在低比特率下视觉保真度优于现有方法，（2）将压缩性能比H.266/VVC提高了高达79%，（3）为AI生成内容提供了高效解决方案，并可适应更广泛的内容类型。", "conclusion": "该框架通过有效利用扩散先验和预训练模型，提供了一种高效、可适应的解决方案，能够在低比特率下实现高视觉保真度的压缩，特别适用于AI生成内容，并且具有最小的再训练成本。"}}
{"id": "2509.13972", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13972", "abs": "https://arxiv.org/abs/2509.13972", "authors": ["Asier Bikandi", "Miguel Fernandez-Cortizas", "Muhammad Shaheer", "Ali Tourani", "Holger Voos", "Jose Luis Sanchez-Lopez"], "title": "BIM Informed Visual SLAM for Construction Monitoring", "comment": "8 pages, 5 tables, 4 figures", "summary": "Simultaneous Localization and Mapping (SLAM) is a key tool for monitoring\nconstruction sites, where aligning the evolving as-built state with the\nas-planned design enables early error detection and reduces costly rework.\nLiDAR-based SLAM achieves high geometric precision, but its sensors are\ntypically large and power-demanding, limiting their use on portable platforms.\nVisual SLAM offers a practical alternative with lightweight cameras already\nembedded in most mobile devices. however, visually mapping construction\nenvironments remains challenging: repetitive layouts, occlusions, and\nincomplete or low-texture structures often cause drift in the trajectory map.\nTo mitigate this, we propose an RGB-D SLAM system that incorporates the\nBuilding Information Model (BIM) as structural prior knowledge. Instead of\nrelying solely on visual cues, our system continuously establishes\ncorrespondences between detected wall and their BIM counterparts, which are\nthen introduced as constraints in the back-end optimization. The proposed\nmethod operates in real time and has been validated on real construction sites,\nreducing trajectory error by an average of 23.71% and map RMSE by 7.14%\ncompared to visual SLAM baselines. These results demonstrate that BIM\nconstraints enable reliable alignment of the digital plan with the as-built\nscene, even under partially constructed conditions.", "AI": {"tldr": "本文提出一种结合BIM结构先验知识的RGB-D SLAM系统，用于建筑工地监控，有效减少了视觉SLAM在复杂环境中的漂移问题。", "motivation": "建筑工地监控需要将施工现场与设计图对齐以检测错误。LiDAR SLAM精度高但设备笨重；视觉SLAM虽便携但易受建筑工地重复布局、遮挡和低纹理结构等因素影响，导致轨迹漂移。", "method": "该方法提出一个RGB-D SLAM系统，将建筑信息模型（BIM）作为结构先验知识。系统持续建立检测到的墙体与其BIM对应物之间的对应关系，并将这些对应关系作为约束引入后端优化。该方法支持实时操作。", "result": "在真实建筑工地进行验证，与视觉SLAM基线相比，该方法平均减少了23.71%的轨迹误差和7.14%的地图RMSE。即使在部分施工完成的条件下，BIM约束也能实现数字计划与实际场景的可靠对齐。", "conclusion": "BIM约束能够使数字计划与施工现场进行可靠对齐，即使在部分施工完成的情况下，也能提高SLAM的鲁棒性，从而实现更可靠的建筑工地监控。"}}
{"id": "2509.13769", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13769", "abs": "https://arxiv.org/abs/2509.13769", "authors": ["Yuechen Luo", "Fang Li", "Shaoqing Xu", "Zhiyi Lai", "Lei Yang", "Qimao Chen", "Ziang Luo", "Zixun Xie", "Shengyin Jiang", "Jiaxin Liu", "Long Chen", "Bing Wang", "Zhi-xin Yang"], "title": "AdaThinkDrive: Adaptive Thinking via Reinforcement Learning for Autonomous Driving", "comment": null, "summary": "While reasoning technology like Chain of Thought (CoT) has been widely\nadopted in Vision Language Action (VLA) models, it demonstrates promising\ncapabilities in end to end autonomous driving. However, recent efforts to\nintegrate CoT reasoning often fall short in simple scenarios, introducing\nunnecessary computational overhead without improving decision quality. To\naddress this, we propose AdaThinkDrive, a novel VLA framework with a dual mode\nreasoning mechanism inspired by fast and slow thinking. First, our framework is\npretrained on large scale autonomous driving (AD) scenarios using both question\nanswering (QA) and trajectory datasets to acquire world knowledge and driving\ncommonsense. During supervised fine tuning (SFT), we introduce a two mode\ndataset, fast answering (w/o CoT) and slow thinking (with CoT), enabling the\nmodel to distinguish between scenarios that require reasoning. Furthermore, an\nAdaptive Think Reward strategy is proposed in conjunction with the Group\nRelative Policy Optimization (GRPO), which rewards the model for selectively\napplying CoT by comparing trajectory quality across different reasoning modes.\nExtensive experiments on the Navsim benchmark show that AdaThinkDrive achieves\na PDMS of 90.3, surpassing the best vision only baseline by 1.7 points.\nMoreover, ablations show that AdaThinkDrive surpasses both the never Think and\nalways Think baselines, improving PDMS by 2.0 and 1.4, respectively. It also\nreduces inference time by 14% compared to the always Think baseline,\ndemonstrating its ability to balance accuracy and efficiency through adaptive\nreasoning.", "AI": {"tldr": "AdaThinkDrive是一个受快慢思维启发的新型VLA框架，它在自动驾驶中通过双模式推理机制自适应地应用CoT，从而在提高决策质量的同时，平衡了准确性和效率。", "motivation": "虽然思维链（CoT）推理技术在视觉-语言-动作（VLA）模型中展现出应用于端到端自动驾驶的潜力，但在简单场景下，集成CoT往往引入不必要的计算开销，而未能提升决策质量。", "method": "该框架首先利用问答和轨迹数据集在大型自动驾驶场景上进行预训练，以获取世界知识和驾驶常识。在监督微调阶段，引入包含“快速回答”（无CoT）和“慢速思考”（有CoT）两种模式的数据集，使模型能区分需要推理的场景。此外，提出了一种自适应思考奖励策略，结合群体相对策略优化（GRPO），通过比较不同推理模式下的轨迹质量来奖励模型选择性地应用CoT。", "result": "在Navsim基准测试中，AdaThinkDrive取得了90.3的PDMS，比最佳纯视觉基线高出1.7点。消融实验表明，AdaThinkDrive分别比“从不思考”和“总是思考”基线提高了2.0和1.4的PDMS。与“总是思考”基线相比，推理时间减少了14%。", "conclusion": "AdaThinkDrive通过自适应推理有效平衡了自动驾驶中的准确性和效率，在性能上超越了现有基线，并显著降低了推理时间。"}}
{"id": "2509.13998", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13998", "abs": "https://arxiv.org/abs/2509.13998", "authors": ["Bailey Dacre", "Rodrigo Moreno", "Serhat Demirtas", "Ziqiao Wang", "Yuhao Jiang", "Jamie Paik", "Kasper Stoy", "Andrés Faíña"], "title": "Flexible and Foldable: Workspace Analysis and Object Manipulation Using a Soft, Interconnected, Origami-Inspired Actuator Array", "comment": null, "summary": "Object manipulation is a fundamental challenge in robotics, where systems\nmust balance trade-offs among manipulation capabilities, system complexity, and\nthroughput. Distributed manipulator systems (DMS) use the coordinated motion of\nactuator arrays to perform complex object manipulation tasks, seeing widespread\nexploration within the literature and in industry. However, existing DMS\ndesigns typically rely on high actuator densities and impose constraints on\nobject-to-actuator scale ratios, limiting their adaptability. We present a\nnovel DMS design utilizing an array of 3-DoF, origami-inspired robotic tiles\ninterconnected by a compliant surface layer. Unlike conventional DMS, our\napproach enables manipulation not only at the actuator end effectors but also\nacross a flexible surface connecting all actuators; creating a continuous,\ncontrollable manipulation surface. We analyse the combined workspace of such a\nsystem, derive simple motion primitives, and demonstrate its capabilities to\ntranslate simple geometric objects across an array of tiles. By leveraging the\ninter-tile connective material, our approach significantly reduces actuator\ndensity, increasing the area over which an object can be manipulated by x1.84\nwithout an increase in the number of actuators. This design offers a lower cost\nand complexity alternative to traditional high-density arrays, and introduces\nnew opportunities for manipulation strategies that leverage the flexibility of\nthe interconnected surface.", "AI": {"tldr": "本文提出了一种新型分布式机械手系统（DMS），该系统采用3自由度、受折纸启发的机器人瓦片，并通过柔性表面层互连，形成一个连续可控的操纵表面，显著降低了执行器密度。", "motivation": "现有的分布式机械手系统（DMS）通常依赖高执行器密度，并对物体与执行器的尺寸比例施加限制，从而限制了它们的适应性。这促使研究人员寻求一种更具成本效益和适应性的操纵系统。", "method": "研究人员设计了一种新型DMS，该系统由一系列3自由度、受折纸启发的机器人瓦片组成，这些瓦片通过一个柔性表面层互连。他们分析了这种系统的组合工作空间，推导了简单的运动原语，并展示了其在瓦片阵列上平移简单几何物体的能力。", "result": "该方法不仅在执行器末端执行器上实现操纵，还在连接所有执行器的柔性表面上实现操纵，创建了一个连续、可控的操纵表面。通过利用瓦片间的连接材料，该方法显著降低了执行器密度，在不增加执行器数量的情况下，将物体可操纵区域增加了1.84倍。", "conclusion": "这种设计为传统高密度阵列提供了一种更低成本和复杂性的替代方案，并为利用互连表面柔韧性的操纵策略带来了新的机遇。"}}
{"id": "2509.13776", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13776", "abs": "https://arxiv.org/abs/2509.13776", "authors": ["Chao Shuai", "Gaojian Wang", "Kun Pan", "Tong Wu", "Fanli Jin", "Haohan Tan", "Mengxiang Li", "Zhenguang Liu", "Feng Lin", "Kui Ren"], "title": "Morphology-optimized Multi-Scale Fusion: Combining Local Artifacts and Mesoscopic Semantics for Deepfake Detection and Localization", "comment": "The 3rd Place, IJCAI 2025 Workshop on Deepfake Detection,\n  Localization, and Interpretability", "summary": "While the pursuit of higher accuracy in deepfake detection remains a central\ngoal, there is an increasing demand for precise localization of manipulated\nregions. Despite the remarkable progress made in classification-based\ndetection, accurately localizing forged areas remains a significant challenge.\nA common strategy is to incorporate forged region annotations during model\ntraining alongside manipulated images. However, such approaches often neglect\nthe complementary nature of local detail and global semantic context, resulting\nin suboptimal localization performance. Moreover, an often-overlooked aspect is\nthe fusion strategy between local and global predictions. Naively combining the\noutputs from both branches can amplify noise and errors, thereby undermining\nthe effectiveness of the localization.\n  To address these issues, we propose a novel approach that independently\npredicts manipulated regions using both local and global perspectives. We\nemploy morphological operations to fuse the outputs, effectively suppressing\nnoise while enhancing spatial coherence. Extensive experiments reveal the\neffectiveness of each module in improving the accuracy and robustness of\nforgery localization.", "AI": {"tldr": "本文提出了一种新颖的深度伪造区域定位方法，通过独立预测局部和全局操作区域，并利用形态学操作融合输出，有效抑制噪声并增强空间一致性，从而提高定位的准确性和鲁棒性。", "motivation": "现有深度伪造检测在分类方面取得显著进展，但在精确局部化伪造区域方面仍面临挑战。常见方法在训练时加入伪造区域标注，但往往忽略局部细节和全局语义上下文的互补性，导致定位性能不佳。此外，局部和全局预测的融合策略常被忽视，简单结合可能放大噪声和误差，损害定位效果。", "method": "该方法提出独立地从局部和全局视角预测操作区域。它采用形态学操作来融合这些输出，旨在有效抑制噪声并增强空间一致性。", "result": "广泛的实验表明，该方法中的每个模块都能有效提高伪造定位的准确性和鲁棒性。", "conclusion": "该研究通过独立预测局部和全局操作区域，并利用形态学操作进行融合，成功解决了深度伪造区域定位中的挑战，显著提高了定位的准确性和鲁棒性。"}}
{"id": "2509.14010", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14010", "abs": "https://arxiv.org/abs/2509.14010", "authors": ["Zong Chen", "Shaoyang Li", "Ben Liu", "Min Li", "Zhouping Yin", "Yiqun Li"], "title": "Whole-body Motion Control of an Omnidirectional Wheel-Legged Mobile Manipulator via Contact-Aware Dynamic Optimization", "comment": null, "summary": "Wheel-legged robots with integrated manipulators hold great promise for\nmobile manipulation in logistics, industrial automation, and human-robot\ncollaboration. However, unified control of such systems remains challenging due\nto the redundancy in degrees of freedom, complex wheel-ground contact dynamics,\nand the need for seamless coordination between locomotion and manipulation. In\nthis work, we present the design and whole-body motion control of an\nomnidirectional wheel-legged quadrupedal robot equipped with a dexterous\nmanipulator. The proposed platform incorporates independently actuated steering\nmodules and hub-driven wheels, enabling agile omnidirectional locomotion with\nhigh maneuverability in structured environments. To address the challenges of\ncontact-rich interaction, we develop a contact-aware whole-body dynamic\noptimization framework that integrates point-contact modeling for manipulation\nwith line-contact modeling for wheel-ground interactions. A warm-start strategy\nis introduced to accelerate online optimization, ensuring real-time feasibility\nfor high-dimensional control. Furthermore, a unified kinematic model tailored\nfor the robot's 4WIS-4WID actuation scheme eliminates the need for mode\nswitching across different locomotion strategies, improving control consistency\nand robustness. Simulation and experimental results validate the effectiveness\nof the proposed framework, demonstrating agile terrain traversal, high-speed\nomnidirectional mobility, and precise manipulation under diverse scenarios,\nunderscoring the system's potential for factory automation, urban logistics,\nand service robotics in semi-structured environments.", "AI": {"tldr": "本文提出了一种配备灵巧机械臂的全向轮腿式四足机器人设计及其全身运动控制框架，通过接触感知动态优化和统一运动学模型，实现了敏捷的越障、高速全向移动和精确操作。", "motivation": "轮腿式机器人与机械臂集成在物流、工业自动化和人机协作领域前景广阔，但由于自由度冗余、复杂的轮地接触动力学以及运动和操作之间的无缝协调需求，其统一控制仍面临挑战。", "method": "研究人员设计了一种具有独立驱动转向模块和轮毂驱动车轮的全向轮腿式四足机器人平台。开发了一个接触感知的全身动态优化框架，该框架将操作的点接触建模与轮地交互的线接触建模相结合。引入了热启动策略以加速在线优化。此外，为机器人的4WIS-4WID驱动方案定制的统一运动学模型消除了不同运动策略间的模式切换需求。", "result": "仿真和实验结果验证了所提出框架的有效性，展示了在各种场景下敏捷的越障能力、高速全向移动能力和精确的操作能力。", "conclusion": "该系统在半结构化环境中的工厂自动化、城市物流和服务机器人领域具有巨大潜力，突显了其在实际应用中的价值。"}}
{"id": "2509.13784", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13784", "abs": "https://arxiv.org/abs/2509.13784", "authors": ["Hanfang Liang", "Bing Wang", "Shizhen Zhang", "Wen Jiang", "Yizhuo Yang", "Weixiang Guo", "Shenghai Yuan"], "title": "CETUS: Causal Event-Driven Temporal Modeling With Unified Variable-Rate Scheduling", "comment": "8 pages, 6 figures", "summary": "Event cameras capture asynchronous pixel-level brightness changes with\nmicrosecond temporal resolution, offering unique advantages for high-speed\nvision tasks. Existing methods often convert event streams into intermediate\nrepresentations such as frames, voxel grids, or point clouds, which inevitably\nrequire predefined time windows and thus introduce window latency. Meanwhile,\npointwise detection methods face computational challenges that prevent\nreal-time efficiency due to their high computational cost. To overcome these\nlimitations, we propose the Variable-Rate Spatial Event Mamba, a novel\narchitecture that directly processes raw event streams without intermediate\nrepresentations. Our method introduces a lightweight causal spatial\nneighborhood encoder to efficiently capture local geometric relations, followed\nby Mamba-based state space models for scalable temporal modeling with linear\ncomplexity. During inference, a controller adaptively adjusts the processing\nspeed according to the event rate, achieving an optimal balance between window\nlatency and inference latency.", "AI": {"tldr": "本文提出了一种名为Variable-Rate Spatial Event Mamba (VSEM)的新架构，直接处理原始事件流，无需中间表示，通过轻量级空间编码器和基于Mamba的状态空间模型实现高效的时空建模，并自适应调整处理速度以优化延迟。", "motivation": "现有事件相机处理方法存在局限性：将事件流转换为中间表示（如帧、体素网格、点云）需要预定义时间窗口，从而引入窗口延迟；点式检测方法计算成本高昂，难以实现实时效率。", "method": "本文提出了Variable-Rate Spatial Event Mamba (VSEM)架构，直接处理原始事件流。该方法引入了一个轻量级的因果空间邻域编码器，以高效捕获局部几何关系，随后使用基于Mamba的状态空间模型进行可扩展的线性复杂度时间建模。在推理过程中，一个控制器根据事件速率自适应调整处理速度。", "result": "通过在推理时自适应调整处理速度，该方法在窗口延迟和推理延迟之间实现了最佳平衡。", "conclusion": "VSEM架构通过直接处理原始事件流、高效的时空建模以及自适应处理速度，克服了现有方法的局限性，为高速视觉任务提供了更优的实时解决方案。"}}
{"id": "2509.14025", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.14025", "abs": "https://arxiv.org/abs/2509.14025", "authors": ["Rui Huang", "Zhiyu Gao", "Siyu Tang", "Jialin Zhang", "Lei He", "Ziqian Zhang", "Lin Zhao"], "title": "TransforMARS: Fault-Tolerant Self-Reconfiguration for Arbitrarily Shaped Modular Aerial Robot Systems", "comment": null, "summary": "Modular Aerial Robot Systems (MARS) consist of multiple drone modules that\nare physically bound together to form a single structure for flight. Exploiting\nstructural redundancy, MARS can be reconfigured into different formations to\nmitigate unit or rotor failures and maintain stable flight. Prior work on MARS\nself-reconfiguration has solely focused on maximizing controllability margins\nto tolerate a single rotor or unit fault for rectangular-shaped MARS. We\npropose TransforMARS, a general fault-tolerant reconfiguration framework that\ntransforms arbitrarily shaped MARS under multiple rotor and unit faults while\nensuring continuous in-air stability. Specifically, we develop algorithms to\nfirst identify and construct minimum controllable assemblies containing faulty\nunits. We then plan feasible disassembly-assembly sequences to transport MARS\nunits or subassemblies to form target configuration. Our approach enables more\nflexible and practical feasible reconfiguration. We validate TransforMARS in\nchallenging arbitrarily shaped MARS configurations, demonstrating substantial\nimprovements over prior works in both the capacity of handling diverse\nconfigurations and the number of faults tolerated. The videos and source code\nof this work are available at the anonymous repository:\nhttps://anonymous.4open.science/r/TransforMARS-1030/", "AI": {"tldr": "TransforMARS是一个通用的容错重构框架，用于在多旋翼和单元故障下，对任意形状的模块化空中机器人系统(MARS)进行空中稳定重构，显著优于现有方法。", "motivation": "现有的模块化空中机器人系统(MARS)自重构工作主要集中于矩形MARS，且仅针对单个旋翼或单元故障，旨在最大化可控性裕度。然而，对于任意形状的MARS，在多重旋翼和单元故障下，仍缺乏一个能在空中保持连续稳定性的通用容错重构框架。", "method": "本文提出了TransforMARS框架。具体方法包括：首先，开发算法识别并构建包含故障单元的最小可控组件；然后，规划可行的拆卸-组装序列，以运输MARS单元或子组件，形成目标配置。", "result": "TransforMARS实现了更灵活、更实用的重构能力。在复杂的任意形状MARS配置中进行了验证，结果表明，与现有工作相比，TransforMARS在处理多样化配置的能力和可容忍的故障数量方面均有显著提升。", "conclusion": "TransforMARS提供了一个通用的容错重构框架，能够处理任意形状MARS在多重故障下的重构，同时确保空中稳定性，大幅提高了故障容忍度和配置多样性，超越了现有技术水平。"}}
{"id": "2509.13795", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13795", "abs": "https://arxiv.org/abs/2509.13795", "authors": ["Jiayu Yuan", "Ming Dai", "Enhui Zheng", "Chao Su", "Nanxing Chen", "Qiming Hu", "Shibo Zhu", "Yibin Cao"], "title": "SWA-PF: Semantic-Weighted Adaptive Particle Filter for Memory-Efficient 4-DoF UAV Localization in GNSS-Denied Environments", "comment": null, "summary": "Vision-based Unmanned Aerial Vehicle (UAV) localization systems have been\nextensively investigated for Global Navigation Satellite System (GNSS)-denied\nenvironments. However, existing retrieval-based approaches face limitations in\ndataset availability and persistent challenges including suboptimal real-time\nperformance, environmental sensitivity, and limited generalization capability,\nparticularly in dynamic or temporally varying environments. To overcome these\nlimitations, we present a large-scale Multi-Altitude Flight Segments dataset\n(MAFS) for variable altitude scenarios and propose a novel Semantic-Weighted\nAdaptive Particle Filter (SWA-PF) method. This approach integrates robust\nsemantic features from both UAV-captured images and satellite imagery through\ntwo key innovations: a semantic weighting mechanism and an optimized particle\nfiltering architecture. Evaluated using our dataset, the proposed method\nachieves 10x computational efficiency gain over feature extraction methods,\nmaintains global positioning errors below 10 meters, and enables rapid 4 degree\nof freedom (4-DoF) pose estimation within seconds using accessible\nlow-resolution satellite maps. Code and dataset will be available at\nhttps://github.com/YuanJiayuuu/SWA-PF.", "AI": {"tldr": "本文提出了一个大规模多高度飞行片段数据集（MAFS）和一个名为语义加权自适应粒子滤波器（SWA-PF）的新方法，用于在GNSS受限环境中实现高效、准确的无人机视觉定位，解决了现有方法的局限性。", "motivation": "在GNSS受限环境中，基于视觉的无人机定位系统面临数据集可用性不足、实时性能不佳、环境敏感性高以及泛化能力有限（尤其是在动态或时变环境中）等挑战。", "method": "研究者提出了一个大规模多高度飞行片段数据集（MAFS），并提出了一种新颖的语义加权自适应粒子滤波器（SWA-PF）方法。该方法通过语义加权机制和优化的粒子滤波架构，整合了无人机图像和卫星图像中的鲁棒语义特征。", "result": "该方法在所提出的数据集上进行了评估，实现了比特征提取方法高10倍的计算效率，将全局定位误差保持在10米以下，并能在数秒内使用低分辨率卫星地图实现快速的4自由度（4-DoF）姿态估计。", "conclusion": "所提出的SWA-PF方法结合MAFS数据集，有效克服了现有无人机视觉定位方法的局限性，在GNSS受限环境中提供了高效、准确且具有良好泛化能力的定位解决方案。"}}
{"id": "2509.14063", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14063", "abs": "https://arxiv.org/abs/2509.14063", "authors": ["Sundhar Vinodh Sangeetha", "Chih-Yuan Chiu", "Sarah H. Q. Li", "Shreyas Kousik"], "title": "Language Conditioning Improves Accuracy of Aircraft Goal Prediction in Untowered Airspace", "comment": "The last two authors advised equally. Submitted to the 2026 IEEE\n  International Conference on Robotics and Automation. 8 pages, 6 figures", "summary": "Autonomous aircraft must safely operate in untowered airspace, where\ncoordination relies on voice-based communication among human pilots. Safe\noperation requires an aircraft to predict the intent, and corresponding goal\nlocation, of other aircraft. This paper introduces a multimodal framework for\naircraft goal prediction that integrates natural language understanding with\nspatial reasoning to improve autonomous decision-making in such environments.\nWe leverage automatic speech recognition and large language models to\ntranscribe and interpret pilot radio calls, identify aircraft, and extract\ndiscrete intent labels. These intent labels are fused with observed\ntrajectories to condition a temporal convolutional network and Gaussian mixture\nmodel for probabilistic goal prediction. Our method significantly reduces goal\nprediction error compared to baselines that rely solely on motion history,\ndemonstrating that language-conditioned prediction increases prediction\naccuracy. Experiments on a real-world dataset from an untowered airport\nvalidate the approach and highlight its potential to enable socially aware,\nlanguage-conditioned robotic motion planning.", "AI": {"tldr": "本文提出了一种多模态框架，通过结合自然语言理解和空间推理，来预测非塔台空域中飞机的意图和目标位置，从而提高自主飞机的决策能力。", "motivation": "自主飞机需要在非塔台空域安全运行，而该空域的协调依赖于飞行员之间的语音通信。为了安全操作，飞机需要预测其他飞机的意图和目标位置。", "method": "该方法利用自动语音识别（ASR）和大型语言模型（LLM）来转录和解释飞行员的无线电通话，识别飞机并提取离散的意图标签。这些意图标签与观测到的轨迹融合，以条件化时间卷积网络（TCN）和高斯混合模型（GMM），进行概率性的目标预测。", "result": "与仅依赖运动历史的基线方法相比，该方法显著降低了目标预测误差，证明了语言条件预测能提高预测准确性。在真实世界的非塔台机场数据集上的实验验证了该方法的有效性。", "conclusion": "该方法在真实世界数据上的验证，突显了其在实现具有社交意识、语言条件机器人运动规划方面的潜力。"}}
{"id": "2509.13801", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13801", "abs": "https://arxiv.org/abs/2509.13801", "authors": ["Wenlve Zhou", "Zhiheng Zhou", "Tiantao Xian", "Yikui Zhai", "Weibin Wu", "Biyun Ma"], "title": "Masked Feature Modeling Enhances Adaptive Segmentation", "comment": null, "summary": "Unsupervised domain adaptation (UDA) for semantic segmentation aims to\ntransfer models from a labeled source domain to an unlabeled target domain.\nWhile auxiliary self-supervised tasks-particularly contrastive learning-have\nimproved feature discriminability, masked modeling approaches remain\nunderexplored in this setting, largely due to architectural incompatibility and\nmisaligned optimization objectives. We propose Masked Feature Modeling (MFM), a\nnovel auxiliary task that performs feature masking and reconstruction directly\nin the feature space. Unlike existing masked modeling methods that reconstruct\nlow-level inputs or perceptual features (e.g., HOG or visual tokens), MFM\naligns its learning target with the main segmentation task, ensuring\ncompatibility with standard architectures like DeepLab and DAFormer without\nmodifying the inference pipeline. To facilitate effective reconstruction, we\nintroduce a lightweight auxiliary module, Rebuilder, which is trained jointly\nbut discarded during inference, adding zero computational overhead at test\ntime. Crucially, MFM leverages the segmentation decoder to classify the\nreconstructed features, tightly coupling the auxiliary objective with the\npixel-wise prediction task to avoid interference with the primary task.\nExtensive experiments across various architectures and UDA benchmarks\ndemonstrate that MFM consistently enhances segmentation performance, offering a\nsimple, efficient, and generalizable strategy for unsupervised domain-adaptive\nsemantic segmentation.", "AI": {"tldr": "本文提出了一种名为掩码特征建模（MFM）的新型辅助任务，用于无监督域适应语义分割，通过在特征空间中进行掩码和重建，并与分割解码器紧密结合，有效提升了模型性能。", "motivation": "无监督域适应（UDA）语义分割中，特征判别性是关键。现有自监督方法（如对比学习）有所帮助，但掩码建模（masked modeling）方法因架构不兼容和优化目标不一致而未被充分探索。", "method": "本文提出了掩码特征建模（MFM），一种直接在特征空间进行特征掩码和重建的辅助任务。它不重建低级输入或感知特征，而是将学习目标与主分割任务对齐。MFM引入了一个轻量级辅助模块Rebuilder进行重建（推理时丢弃），并在重建特征上使用分割解码器进行分类，确保辅助目标与像素级预测任务紧密耦合，避免干扰主任务。", "result": "在多种架构和UDA基准上的广泛实验表明，MFM持续提升了分割性能，提供了一种简单、高效且可泛化的无监督域适应语义分割策略。", "conclusion": "MFM是一种新颖、简单、高效且通用性强的无监督域适应语义分割策略，通过在特征空间进行掩码和重建，并与主分割任务紧密耦合，显著提升了模型性能。"}}
{"id": "2509.14082", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14082", "abs": "https://arxiv.org/abs/2509.14082", "authors": ["Valerii Serpiva", "Artem Lykov", "Faryal Batool", "Vladislav Kozlovskiy", "Miguel Altamirano Cabrera", "Dzmitry Tsetserukou"], "title": "FlightDiffusion: Revolutionising Autonomous Drone Training with Diffusion Models Generating FPV Video", "comment": "Submitted to conference", "summary": "We present FlightDiffusion, a diffusion-model-based framework for training\nautonomous drones from first-person view (FPV) video. Our model generates\nrealistic video sequences from a single frame, enriched with corresponding\naction spaces to enable reasoning-driven navigation in dynamic environments.\nBeyond direct policy learning, FlightDiffusion leverages its generative\ncapabilities to synthesize diverse FPV trajectories and state-action pairs,\nfacilitating the creation of large-scale training datasets without the high\ncost of real-world data collection. Our evaluation demonstrates that the\ngenerated trajectories are physically plausible and executable, with a mean\nposition error of 0.25 m (RMSE 0.28 m) and a mean orientation error of 0.19 rad\n(RMSE 0.24 rad). This approach enables improved policy learning and dataset\nscalability, leading to superior performance in downstream navigation tasks.\nResults in simulated environments highlight enhanced robustness, smoother\ntrajectory planning, and adaptability to unseen conditions. An ANOVA revealed\nno statistically significant difference between performance in simulation and\nreality (F(1, 16) = 0.394, p = 0.541), with success rates of M = 0.628 (SD =\n0.162) and M = 0.617 (SD = 0.177), respectively, indicating strong sim-to-real\ntransfer. The generated datasets provide a valuable resource for future UAV\nresearch. This work introduces diffusion-based reasoning as a promising\nparadigm for unifying navigation, action generation, and data synthesis in\naerial robotics.", "AI": {"tldr": "FlightDiffusion是一个基于扩散模型的框架，用于通过第一人称视角（FPV）视频训练自主无人机，能够生成逼真的视频序列和动作空间，合成大规模训练数据集，从而提高策略学习和下游导航任务的性能。", "motivation": "训练自主无人机需要大量的真实世界数据，但数据收集成本高昂。研究旨在寻找一种有效的方法来生成多样化的训练数据，以克服这一挑战并提高无人机在动态环境中的导航能力。", "method": "该研究提出了FlightDiffusion框架，利用扩散模型从单个FPV帧生成逼真的视频序列，并结合相应的动作空间。它利用生成能力合成多样化的FPV轨迹和状态-动作对，从而创建大规模的训练数据集，而无需昂贵的真实世界数据收集。", "result": "生成的轨迹具有物理合理性和可执行性，平均位置误差为0.25米（RMSE 0.28米），平均方向误差为0.19弧度（RMSE 0.24弧度）。该方法显著改善了策略学习和数据集可扩展性，在下游导航任务中表现出卓越性能。在模拟环境中，模型展示了增强的鲁棒性、更平滑的轨迹规划和对未知条件的适应性。方差分析显示，模拟和现实性能之间无统计学显著差异（F(1, 16) = 0.394, p = 0.541），成功率分别为M = 0.628和M = 0.617，表明强大的模拟到现实迁移能力。", "conclusion": "FlightDiffusion引入了基于扩散的推理作为一种有前景的范式，可以统一空中机器人领域的导航、动作生成和数据合成。生成的合成数据集为未来的无人机研究提供了宝贵资源。"}}
{"id": "2509.13809", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13809", "abs": "https://arxiv.org/abs/2509.13809", "authors": ["Nick Theisen", "Kenny Schlegel", "Dietrich Paulus", "Peer Neubert"], "title": "Data-Efficient Spectral Classification of Hyperspectral Data Using MiniROCKET and HDC-MiniROCKET", "comment": "Accepted for publication at IEEE CASE 2025", "summary": "The classification of pixel spectra of hyperspectral images, i.e. spectral\nclassification, is used in many fields ranging from agricultural, over medical\nto remote sensing applications and is currently also expanding to areas such as\nautonomous driving. Even though for full hyperspectral images the\nbest-performing methods exploit spatial-spectral information, performing\nclassification solely on spectral information has its own advantages, e.g.\nsmaller model size and thus less data required for training. Moreover, spectral\ninformation is complementary to spatial information and improvements on either\npart can be used to improve spatial-spectral approaches in the future.\nRecently, 1D-Justo-LiuNet was proposed as a particularly efficient model with\nvery few parameters, which currently defines the state of the art in spectral\nclassification. However, we show that with limited training data the model\nperformance deteriorates. Therefore, we investigate MiniROCKET and\nHDC-MiniROCKET for spectral classification to mitigate that problem. The model\nextracts well-engineered features without trainable parameters in the feature\nextraction part and is therefore less vulnerable to limited training data. We\nshow that even though MiniROCKET has more parameters it outperforms\n1D-Justo-LiuNet in limited data scenarios and is mostly on par with it in the\ngeneral case", "AI": {"tldr": "本研究针对高光谱图像光谱分类中训练数据有限导致现有最先进模型性能下降的问题，提出使用MiniROCKET和HDC-MiniROCKET模型，并证明其在数据受限情况下优于1D-Justo-LiuNet。", "motivation": "高光谱图像光谱分类在许多领域有应用，且纯光谱分类具有模型小、所需数据少的优势，并能补充空-谱方法。然而，目前最先进的1D-Justo-LiuNet模型在训练数据有限时性能会下降，这促使研究者寻找更鲁棒的替代方案。", "method": "研究者引入并评估了MiniROCKET和HDC-MiniROCKET模型用于光谱分类。这些模型在特征提取部分不包含可训练参数，因此对有限的训练数据不那么敏感。", "result": "实验结果表明，在训练数据有限的情况下，MiniROCKET模型即使参数更多，其性能也优于1D-Justo-LiuNet。在一般情况下，MiniROCKET的性能与1D-Justo-LiuNet基本持平。", "conclusion": "MiniROCKET和HDC-MiniROCKET是解决高光谱图像光谱分类中训练数据不足问题的有效方法，在数据受限场景下表现出显著优势，并有望成为该领域的新选择。"}}
{"id": "2509.14117", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14117", "abs": "https://arxiv.org/abs/2509.14117", "authors": ["Ali Abouzeid", "Malak Mansour", "Zezhou Sun", "Dezhen Song"], "title": "GeoAware-VLA: Implicit Geometry Aware Vision-Language-Action Model", "comment": "Under Review", "summary": "Vision-Language-Action (VLA) models often fail to generalize to novel camera\nviewpoints, a limitation stemming from their difficulty in inferring robust 3D\ngeometry from 2D images. We introduce GeoAware-VLA, a simple yet effective\napproach that enhances viewpoint invariance by integrating strong geometric\npriors into the vision backbone. Instead of training a visual encoder or\nrelying on explicit 3D data, we leverage a frozen, pretrained geometric vision\nmodel as a feature extractor. A trainable projection layer then adapts these\ngeometrically-rich features for the policy decoder, relieving it of the burden\nof learning 3D consistency from scratch. Through extensive evaluations on\nLIBERO benchmark subsets, we show GeoAware-VLA achieves substantial\nimprovements in zero-shot generalization to novel camera poses, boosting\nsuccess rates by over 2x in simulation. Crucially, these benefits translate to\nthe physical world; our model shows a significant performance gain on a real\nrobot, especially when evaluated from unseen camera angles. Our approach proves\neffective across both continuous and discrete action spaces, highlighting that\nrobust geometric grounding is a key component for creating more generalizable\nrobotic agents.", "AI": {"tldr": "VLA模型难以泛化到新颖视角，因其2D图像3D几何推理能力不足。GeoAware-VLA通过整合预训练几何视觉模型的强几何先验，显著提升了模拟和真实机器人对新颖摄像机姿态的零样本泛化能力。", "motivation": "Vision-Language-Action (VLA) 模型难以泛化到新颖摄像机视角，主要原因是它们难以从2D图像中推断出鲁棒的3D几何信息。", "method": "GeoAware-VLA通过将强大的几何先验整合到视觉骨干中来增强视角不变性。它利用一个冻结的、预训练的几何视觉模型作为特征提取器，而非训练视觉编码器或依赖显式3D数据。一个可训练的投影层将这些富含几何信息的特征适配到策略解码器，从而减轻了策略解码器从头学习3D一致性的负担。", "result": "在LIBERO基准测试子集上，GeoAware-VLA在零样本泛化到新颖摄像机姿态方面取得了显著改进，在模拟中成功率提高了2倍以上。这些优势也转化到了物理世界，模型在真实机器人上显示出显著的性能提升，尤其是在未见过的摄像机角度下。该方法在连续和离散动作空间中均有效。", "conclusion": "鲁棒的几何基础是创建更具泛化能力的机器人代理的关键组成部分。"}}
{"id": "2509.13834", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13834", "abs": "https://arxiv.org/abs/2509.13834", "authors": ["Nguyen Lan Vi Vu", "Thanh-Huy Nguyen", "Thien Nguyen", "Daisuke Kihara", "Tianyang Wang", "Xingjian Li", "Min Xu"], "title": "Semi-MoE: Mixture-of-Experts meets Semi-Supervised Histopathology Segmentation", "comment": "Accepted to BMVC 2025", "summary": "Semi-supervised learning has been employed to alleviate the need for\nextensive labeled data for histopathology image segmentation, but existing\nmethods struggle with noisy pseudo-labels due to ambiguous gland boundaries and\nmorphological misclassification. This paper introduces Semi-MOE, to the best of\nour knowledge, the first multi-task Mixture-of-Experts framework for\nsemi-supervised histopathology image segmentation. Our approach leverages three\nspecialized expert networks: A main segmentation expert, a signed distance\nfield regression expert, and a boundary prediction expert, each dedicated to\ncapturing distinct morphological features. Subsequently, the Multi-Gating\nPseudo-labeling module dynamically aggregates expert features, enabling a\nrobust fuse-and-refine pseudo-labeling mechanism. Furthermore, to eliminate\nmanual tuning while dynamically balancing multiple learning objectives, we\npropose an Adaptive Multi-Objective Loss. Extensive experiments on GlaS and\nCRAG benchmarks show that our method outperforms state-of-the-art approaches in\nlow-label settings, highlighting the potential of MoE-based architectures in\nadvancing semi-supervised segmentation. Our code is available at\nhttps://github.com/vnlvi2k3/Semi-MoE.", "AI": {"tldr": "本文提出Semi-MOE，一个多任务专家混合（MoE）框架，用于半监督组织病理学图像分割，通过专门的专家网络、多门控伪标签模块和自适应多目标损失，有效处理伪标签噪声并优于现有方法。", "motivation": "现有的半监督学习方法在组织病理学图像分割中，由于腺体边界模糊和形态误分类，容易产生噪声伪标签，导致性能不佳。", "method": "Semi-MOE是首个用于半监督组织病理学图像分割的多任务专家混合框架。它包含三个专家网络：主分割专家、有符号距离场回归专家和边界预测专家。此外，引入了Multi-Gating Pseudo-labeling模块来动态聚合专家特征并优化伪标签机制，并提出了Adaptive Multi-Objective Loss以实现多学习目标的动态平衡，无需手动调优。", "result": "在GlaS和CRAG基准测试上进行的大量实验表明，在低标签设置下，Semi-MOE的性能优于最先进的方法。", "conclusion": "MoE（专家混合）架构在推进半监督分割方面具有巨大潜力，特别是在组织病理学图像分析领域。"}}
{"id": "2509.14126", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.14126", "abs": "https://arxiv.org/abs/2509.14126", "authors": ["Viktor Lorentz", "Khaled Wahba", "Sayantan Auddy", "Marc Toussaint", "Wolfgang Hönig"], "title": "CrazyMARL: Decentralized Direct Motor Control Policies for Cooperative Aerial Transport of Cable-Suspended Payloads", "comment": "This work has been submitted to IEEE for possible publication", "summary": "Collaborative transportation of cable-suspended payloads by teams of Unmanned\nAerial Vehicles (UAVs) has the potential to enhance payload capacity, adapt to\ndifferent payload shapes, and provide built-in compliance, making it attractive\nfor applications ranging from disaster relief to precision logistics. However,\nmulti-UAV coordination under disturbances, nonlinear payload dynamics, and\nslack--taut cable modes remains a challenging control problem. To our\nknowledge, no prior work has addressed these cable mode transitions in the\nmulti-UAV context, instead relying on simplifying rigid-link assumptions. We\npropose CrazyMARL, a decentralized Reinforcement Learning (RL) framework for\nmulti-UAV cable-suspended payload transport. Simulation results demonstrate\nthat the learned policies can outperform classical decentralized controllers in\nterms of disturbance rejection and tracking precision, achieving an 80%\nrecovery rate from harsh conditions compared to 44% for the baseline method. We\nalso achieve successful zero-shot sim-to-real transfer and demonstrate that our\npolicies are highly robust under harsh conditions, including wind, random\nexternal disturbances, and transitions between slack and taut cable dynamics.\nThis work paves the way for autonomous, resilient UAV teams capable of\nexecuting complex payload missions in unstructured environments.", "AI": {"tldr": "本文提出CrazyMARL，一个去中心化的强化学习框架，用于多无人机吊索负载运输。它能有效处理干扰、非线性负载动力学和松弛-张紧线缆模式转换，在模拟和实物传输中表现出优于传统控制器的鲁棒性和精度。", "motivation": "多无人机协作吊索运输在提高负载能力、适应不同负载形状和提供内置柔性方面具有巨大潜力，但面临挑战，包括在干扰下多无人机协调、非线性负载动力学以及松弛-张紧线缆模式转换。现有研究尚未解决多无人机背景下的线缆模式转换问题，而是依赖简化的刚性连接假设。", "method": "本文提出CrazyMARL，一个去中心化的强化学习（RL）框架，用于多无人机吊索负载运输。该方法通过学习策略来应对干扰、非线性负载动力学和线缆模式转换，并通过模拟和零样本模拟到真实环境的迁移进行验证。", "result": "学习到的策略在干扰抑制和跟踪精度方面优于经典的去中心化控制器，在恶劣条件下的恢复率达到80%，而基线方法为44%。此外，还实现了成功的零样本模拟到真实环境的迁移，并证明了策略在恶劣条件（包括风、随机外部干扰以及松弛和张紧线缆动力学之间的转换）下具有高度鲁棒性。", "conclusion": "这项工作为自主、有弹性的无人机团队执行非结构化环境中的复杂负载任务铺平了道路，显示出在处理多无人机吊索运输挑战方面的巨大潜力。"}}
{"id": "2509.13846", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13846", "abs": "https://arxiv.org/abs/2509.13846", "authors": ["Puru Vaish", "Felix Meister", "Tobias Heimann", "Christoph Brune", "Jelmer M. Wolterink"], "title": "Consistent View Alignment Improves Foundation Models for 3D Medical Image Segmentation", "comment": "MICCAI 2025: 1st Place in Transformer track and 2nd Place in\n  Convolution track of SSL3D-OpenMind challenge", "summary": "Many recent approaches in representation learning implicitly assume that\nuncorrelated views of a data point are sufficient to learn meaningful\nrepresentations for various downstream tasks. In this work, we challenge this\nassumption and demonstrate that meaningful structure in the latent space does\nnot emerge naturally. Instead, it must be explicitly induced. We propose a\nmethod that aligns representations from different views of the data to align\ncomplementary information without inducing false positives. Our experiments\nshow that our proposed self-supervised learning method, Consistent View\nAlignment, improves performance for downstream tasks, highlighting the critical\nrole of structured view alignment in learning effective representations. Our\nmethod achieved first and second place in the MICCAI 2025 SSL3D challenge when\nusing a Primus vision transformer and ResEnc convolutional neural network,\nrespectively. The code and pretrained model weights are released at\nhttps://github.com/Tenbatsu24/LatentCampus.", "AI": {"tldr": "本文挑战了表征学习中不相关视图足以学习有意义表征的假设，并提出了一种名为“一致视图对齐”（Consistent View Alignment）的自监督学习方法，通过显式对齐不同视图的互补信息来改善下游任务性能。", "motivation": "许多近期表征学习方法隐含地假设数据点的不相关视图足以学习有意义的表征。本文挑战了这一假设，并指出潜在空间中有意义的结构并不会自然出现，而是必须被显式地引导。", "method": "提出了一种名为“一致视图对齐”（Consistent View Alignment）的自监督学习方法。该方法旨在对齐来自数据不同视图的表征，以对齐互补信息，同时避免引入假阳性。", "result": "实验表明，所提出的方法改善了下游任务的性能，凸显了结构化视图对齐在学习有效表征中的关键作用。该方法在使用Primus vision transformer和ResEnc卷积神经网络时，分别在MICCAI 2025 SSL3D挑战中获得了第一名和第二名。", "conclusion": "结构化视图对齐在学习有效表征中扮演着关键角色。本文提出的“一致视图对齐”方法能够有效地实现这一点，并显著提升了下游任务的性能。"}}
{"id": "2509.14127", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.14127", "abs": "https://arxiv.org/abs/2509.14127", "authors": ["Alkesh K. Srivastava", "Jared Michael Levin", "Philip Dames"], "title": "Energy Efficient Multi Robot Package Delivery under Capacity-Constraints via Voronoi-Constrained Networks", "comment": null, "summary": "We consider the problem of delivering multiple packages from a single pickup\ndepot to distinct goal locations using a homogeneous fleet of robots with\nlimited carrying capacity. We propose VCST-RCP, a Voronoi-Constrained Steiner\nTree Relay Coordination Planning framework that constructs sparse relay trunks\nusing Steiner tree optimization and then synthesizes robot-level pickup, relay,\nand delivery schedules. This framework reframes relays from incidental\nbyproducts into central elements of coordination, offering a contrast with\ntraditional delivery methods that rely on direct source-to-destination\ntransport. Extensive experiments show consistent improvements of up to 34%\ncompared to conventional baselines, underscoring the benefits of incorporating\nrelays into the delivery process. These improvements translate directly to\nenhanced energy efficiency in multi-robot delivery under capacity constraints,\nproviding a scalable framework for real-world logistics.", "AI": {"tldr": "该研究提出了一种名为VCST-RCP的框架，通过Voronoi约束的Steiner树优化来构建稀疏中继骨干，并协调多机器人包裹递送中的取件、中继和递送调度，以提高能源效率。", "motivation": "传统的递送方法依赖于点对点的直接运输，但机器人车队携带能力有限，难以高效地从单个取货点向多个目的地递送包裹。本研究旨在将中继从偶然的副产品转变为协调的核心要素，以克服这一限制。", "method": "研究提出VCST-RCP（Voronoi-Constrained Steiner Tree Relay Coordination Planning）框架。该框架首先利用Steiner树优化构建稀疏的中继骨干，然后综合生成机器人层面的取件、中继和递送调度。它将中继视为协调的核心要素，而非传统的直接运输模式。", "result": "与传统基线方法相比，该框架在实验中实现了高达34%的持续改进。这些改进直接转化为在容量限制下多机器人递送的能源效率提升。", "conclusion": "将中继整合到递送过程中能显著提高多机器人递送的效率和能源利用率，特别是在容量受限的场景下，为现实世界的物流提供了一个可扩展的解决方案。"}}
{"id": "2509.13848", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13848", "abs": "https://arxiv.org/abs/2509.13848", "authors": ["Jiayi Pan", "Jiaming Xu", "Yongkang Zhou", "Guohao Dai"], "title": "SpecDiff: Accelerating Diffusion Model Inference with Self-Speculation", "comment": null, "summary": "Feature caching has recently emerged as a promising method for diffusion\nmodel acceleration. It effectively alleviates the inefficiency problem caused\nby high computational requirements by caching similar features in the inference\nprocess of the diffusion model. In this paper, we analyze existing feature\ncaching methods from the perspective of information utilization, and point out\nthat relying solely on historical information will lead to constrained accuracy\nand speed performance. And we propose a novel paradigm that introduces future\ninformation via self-speculation based on the information similarity at the\nsame time step across different iteration times. Based on this paradigm, we\npresent \\textit{SpecDiff}, a training-free multi-level feature caching strategy\nincluding a cached feature selection algorithm and a multi-level feature\nclassification algorithm. (1) Feature selection algorithm based on\nself-speculative information. \\textit{SpecDiff} determines a dynamic importance\nscore for each token based on self-speculative information and historical\ninformation, and performs cached feature selection through the importance\nscore. (2) Multi-level feature classification algorithm based on feature\nimportance scores. \\textit{SpecDiff} classifies tokens by leveraging the\ndifferences in feature importance scores and introduces a multi-level feature\ncalculation strategy. Extensive experiments show that \\textit{SpecDiff}\nachieves average 2.80 \\times, 2.74 \\times , and 3.17\\times speedup with\nnegligible quality loss in Stable Diffusion 3, 3.5, and FLUX compared to RFlow\non NVIDIA A800-80GB GPU. By merging speculative and historical information,\n\\textit{SpecDiff} overcomes the speedup-accuracy trade-off bottleneck, pushing\nthe Pareto frontier of speedup and accuracy in the efficient diffusion model\ninference.", "AI": {"tldr": "本文提出SpecDiff，一种无训练的多级特征缓存策略，通过引入基于自推测的未来信息，显著加速扩散模型推理，同时保持图像质量。", "motivation": "现有的特征缓存方法主要依赖历史信息，导致扩散模型加速时在准确性和速度性能上受限，无法有效解决计算量大的问题。", "method": "本文提出一种新范式，通过基于不同迭代时间相同时间步的信息相似性进行自推测，引入未来信息。在此范式下，提出无训练的多级特征缓存策略SpecDiff，包括：1) 基于自推测信息的特征选择算法，根据自推测信息和历史信息为每个token确定动态重要性分数；2) 基于特征重要性分数的多级特征分类算法，利用重要性分数差异对token进行分类，并引入多级特征计算策略。", "result": "SpecDiff在Stable Diffusion 3、3.5和FLUX上，与RFlow相比，在NVIDIA A800-80GB GPU上分别实现了平均2.80倍、2.74倍和3.17倍的加速，且图像质量损失可忽略不计。", "conclusion": "SpecDiff通过结合推测信息和历史信息，克服了加速-精度权衡的瓶颈，推动了高效扩散模型推理在速度和精度上的帕累托前沿。"}}
{"id": "2509.14138", "categories": ["cs.RO", "68T40"], "pdf": "https://arxiv.org/pdf/2509.14138", "abs": "https://arxiv.org/abs/2509.14138", "authors": ["Ran Yang", "Zijian An", "Lifeng ZHou", "Yiming Feng"], "title": "SeqVLA: Sequential Task Execution for Long-Horizon Manipulation with Completion-Aware Vision-Language-Action Model", "comment": "8 pages, 9 figures, 1 table", "summary": "Long-horizon robotic manipulation tasks require executing multiple\ninterdependent subtasks in strict sequence, where errors in detecting subtask\ncompletion can cascade into downstream failures. Existing\nVision-Language-Action (VLA) models such as $\\pi_0$ excel at continuous\nlow-level control but lack an internal signal for identifying when a subtask\nhas finished, making them brittle in sequential settings. We propose SeqVLA, a\ncompletion-aware extension of $\\pi_0$ that augments the base architecture with\na lightweight detection head perceiving whether the current subtask is\ncomplete. This dual-head design enables SeqVLA not only to generate\nmanipulation actions but also to autonomously trigger transitions between\nsubtasks. We investigate four finetuning strategies that vary in how the action\nand detection heads are optimized (joint vs. sequential finetuning) and how\npretrained knowledge is preserved (full finetuning vs. frozen backbone).\nExperiments are performed on two multi-stage tasks: salad packing with seven\ndistinct subtasks and candy packing with four distinct subtasks. Results show\nthat SeqVLA significantly outperforms the baseline $\\pi_0$ and other strong\nbaselines in overall success rate. In particular, joint finetuning with an\nunfrozen backbone yields the most decisive and statistically reliable\ncompletion predictions, eliminating sequence-related failures and enabling\nrobust long-horizon execution. Our results highlight the importance of coupling\naction generation with subtask-aware detection for scalable sequential\nmanipulation.", "AI": {"tldr": "SeqVLA通过增加一个轻量级完成检测头来扩展现有视觉-语言-动作(VLA)模型（如π_0），使其能够在生成操作的同时自主触发子任务间的转换。这显著提高了机器人在长程、多阶段操作任务中的成功率和鲁棒性。", "motivation": "长程机器人操作任务需要严格按顺序执行多个相互依赖的子任务。现有VLA模型虽然擅长低级连续控制，但缺乏识别子任务何时完成的内部信号，这导致在序列任务中容易出现错误累积和下游故障。", "method": "本文提出了SeqVLA，它是π_0模型的一个扩展。它在基础架构上增加了一个轻量级的检测头，用于感知当前子任务是否完成。这种双头设计使SeqVLA不仅能生成操作动作，还能自主触发子任务间的转换。研究了四种微调策略，包括动作头和检测头的优化方式（联合 vs. 顺序微调）以及预训练知识的保留方式（完全微调 vs. 冻结骨干网络）。", "result": "实验在沙拉打包（七个子任务）和糖果打包（四个子任务）两个多阶段任务上进行。结果表明，SeqVLA在整体成功率上显著优于基线π_0和其他强基线。特别是，采用未冻结骨干网络的联合微调策略产生了最果断和统计学上最可靠的完成预测，消除了与序列相关的故障，实现了鲁棒的长程执行。", "conclusion": "研究结果强调了将动作生成与子任务感知检测相结合对于可扩展的序列操作的重要性。"}}
{"id": "2509.13858", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13858", "abs": "https://arxiv.org/abs/2509.13858", "authors": ["Qianxin Xia", "Jiawei Du", "Guoming Lu", "Zhiyong Shu", "Jielei Wang"], "title": "EDITS: Enhancing Dataset Distillation with Implicit Textual Semantics", "comment": null, "summary": "Dataset distillation aims to synthesize a compact dataset from the original\nlarge-scale one, enabling highly efficient learning while preserving\ncompetitive model performance. However, traditional techniques primarily\ncapture low-level visual features, neglecting the high-level semantic and\nstructural information inherent in images. In this paper, we propose EDITS, a\nnovel framework that exploits the implicit textual semantics within the image\ndata to achieve enhanced distillation. First, external texts generated by a\nVision Language Model (VLM) are fused with image features through a Global\nSemantic Query module, forming the prior clustered buffer. Local Semantic\nAwareness then selects representative samples from the buffer to construct\nimage and text prototypes, with the latter produced by guiding a Large Language\nModel (LLM) with meticulously crafted prompt. Ultimately, Dual Prototype\nGuidance strategy generates the final synthetic dataset through a diffusion\nmodel. Extensive experiments confirm the effectiveness of our method.Source\ncode is available in: https://github.com/einsteinxia/EDITS.", "AI": {"tldr": "EDITS是一种新颖的数据集蒸馏框架，通过利用图像中隐含的文本语义，结合视觉语言模型和大型语言模型，生成具有高层语义和结构信息的紧凑数据集。", "motivation": "传统的数据集蒸馏技术主要捕获低级视觉特征，忽略了图像固有的高级语义和结构信息，导致合成数据集性能受限。", "method": "EDITS首先通过全局语义查询模块将视觉语言模型生成的外部文本与图像特征融合，形成预聚类缓冲区。接着，局部语义感知从缓冲区选择代表性样本构建图像和文本原型（后者由精心设计的提示引导大型语言模型生成）。最终，双原型指导策略通过扩散模型生成最终的合成数据集。", "result": "广泛的实验证实了该方法的有效性。", "conclusion": "EDITS框架通过利用图像中的隐含文本语义，成功地增强了数据集蒸馏，实现了更高效的学习并保持了有竞争力的模型性能。"}}
{"id": "2509.14143", "categories": ["cs.RO", "68T40"], "pdf": "https://arxiv.org/pdf/2509.14143", "abs": "https://arxiv.org/abs/2509.14143", "authors": ["Zijian An", "Ran Yang", "Yiming Feng", "Lifeng Zhou"], "title": "CLAW: A Vision-Language-Action Framework for Weight-Aware Robotic Grasping", "comment": "8 pages, 5 figures, 1 table", "summary": "Vision-language-action (VLA) models have recently emerged as a promising\nparadigm for robotic control, enabling end-to-end policies that ground natural\nlanguage instructions into visuomotor actions. However, current VLAs often\nstruggle to satisfy precise task constraints, such as stopping based on numeric\nthresholds, since their observation-to-action mappings are implicitly shaped by\ntraining data and lack explicit mechanisms for condition monitoring. In this\nwork, we propose CLAW (CLIP-Language-Action for Weight), a framework that\ndecouples condition evaluation from action generation. CLAW leverages a\nfine-tuned CLIP model as a lightweight prompt generator, which continuously\nmonitors the digital readout of a scale and produces discrete directives based\non task-specific weight thresholds. These prompts are then consumed by $\\pi_0$,\na flow-based VLA policy, which integrates the prompts with multi-view camera\nobservations to produce continuous robot actions. This design enables CLAW to\ncombine symbolic weight reasoning with high-frequency visuomotor control. We\nvalidate CLAW on three experimental setups: single-object grasping and\nmixed-object tasks requiring dual-arm manipulation. Across all conditions, CLAW\nreliably executes weight-aware behaviors and outperforms both raw-$\\pi_0$ and\nfine-tuned $\\pi_0$ models. We have uploaded the videos as supplementary\nmaterials.", "AI": {"tldr": "CLAW是一个新的机器人控制框架，它通过将条件评估（使用CLIP进行重量监测）与动作生成（使用VLA策略）解耦，解决了现有视觉-语言-动作（VLA）模型在满足精确任务约束方面的不足。", "motivation": "当前的视觉-语言-动作（VLA）模型在满足精确任务约束（如基于数字阈值的停止）时表现不佳，因为它们的观察到动作映射是隐式形成的，缺乏明确的条件监测机制。", "method": "CLAW（CLIP-Language-Action for Weight）框架将条件评估与动作生成解耦。它利用一个经过微调的CLIP模型作为轻量级提示生成器，持续监测秤的数字读数，并根据任务特定的重量阈值生成离散指令。这些指令随后由一个基于流的VLA策略$\\\\pi_0$消费，该策略将指令与多视角相机观察相结合，生成连续的机器人动作。", "result": "CLAW在单物体抓取和需要双臂操作的混合物体任务中，都能可靠地执行重量感知行为，并且优于原始$\\\\pi_0$模型和经过微调的$\\\\pi_0$模型。", "conclusion": "CLAW成功地将符号重量推理与高频视觉运动控制相结合，有效解决了VLA模型在精确约束满足方面的局限性。"}}
{"id": "2509.13863", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13863", "abs": "https://arxiv.org/abs/2509.13863", "authors": ["Chu Chen", "Ander Biguri", "Jean-Michel Morel", "Raymond H. Chan", "Carola-Bibiane Schönlieb", "Jizhou Li"], "title": "LamiGauss: Pitching Radiative Gaussian for Sparse-View X-ray Laminography Reconstruction", "comment": null, "summary": "X-ray Computed Laminography (CL) is essential for non-destructive inspection\nof plate-like structures in applications such as microchips and composite\nbattery materials, where traditional computed tomography (CT) struggles due to\ngeometric constraints. However, reconstructing high-quality volumes from\nlaminographic projections remains challenging, particularly under highly\nsparse-view acquisition conditions. In this paper, we propose a reconstruction\nalgorithm, namely LamiGauss, that combines Gaussian Splatting radiative\nrasterization with a dedicated detector-to-world transformation model\nincorporating the laminographic tilt angle. LamiGauss leverages an\ninitialization strategy that explicitly filters out common laminographic\nartifacts from the preliminary reconstruction, preventing redundant Gaussians\nfrom being allocated to false structures and thereby concentrating model\ncapacity on representing the genuine object. Our approach effectively optimizes\ndirectly from sparse projections, enabling accurate and efficient\nreconstruction with limited data. Extensive experiments on both synthetic and\nreal datasets demonstrate the effectiveness and superiority of the proposed\nmethod over existing techniques. LamiGauss uses only 3$\\%$ of full views to\nachieve superior performance over the iterative method optimized on a full\ndataset.", "AI": {"tldr": "LamiGauss是一种结合高斯泼溅和特定变换模型的X射线计算机层析成像（CL）重建算法，旨在解决稀疏视图下的高质量体积重建挑战。", "motivation": "传统计算机断层扫描（CT）在板状结构（如微芯片和复合电池材料）的无损检测中存在几何限制，因此X射线计算机层析成像（CL）至关重要。然而，从层析投影中重建高质量体积仍然具有挑战性，尤其是在高度稀疏视图采集条件下。", "method": "本文提出LamiGauss重建算法，该算法结合了高斯泼溅辐射光栅化和包含层析倾斜角的专用探测器到世界坐标转换模型。LamiGauss利用一种初始化策略，明确滤除初步重建中的常见层析伪影，防止将冗余高斯分配给错误结构，从而将模型容量集中于表示真实物体。该方法直接从稀疏投影进行有效优化。", "result": "在合成和真实数据集上的大量实验证明了所提出方法的有效性和优越性。LamiGauss仅使用3%的完整视图即可实现优于在完整数据集上优化的迭代方法的性能。", "conclusion": "LamiGauss算法能够从有限的稀疏数据中实现准确高效的X射线计算机层析成像重建，有效克服了传统方法在稀疏视图条件下的挑战，为板状结构检测提供了新的解决方案。"}}
{"id": "2509.14147", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14147", "abs": "https://arxiv.org/abs/2509.14147", "authors": ["Fanxing Li", "Shengyang Wang", "Fangyu Sun", "Shuyu Wu", "Dexin Zuo", "Wenxian Yu", "Danping Zou"], "title": "StableTracker: Learning to Stably Track Target via Differentiable Simulation", "comment": null, "summary": "FPV object tracking methods heavily rely on handcraft modular designs,\nresulting in hardware overload and cumulative error, which seriously degrades\nthe tracking performance, especially for rapidly accelerating or decelerating\ntargets. To address these challenges, we present \\textbf{StableTracker}, a\nlearning-based control policy that enables quadrotors to robustly follow the\nmoving target from arbitrary perspectives. The policy is trained using\nbackpropagation-through-time via differentiable simulation, allowing the\nquadrotor to maintain the target at the center of the visual field in both\nhorizontal and vertical directions, while keeping a fixed relative distance,\nthereby functioning as an autonomous aerial camera. We compare StableTracker\nagainst both state-of-the-art traditional algorithms and learning baselines.\nSimulation experiments demonstrate that our policy achieves superior accuracy,\nstability and generalization across varying safe distances, trajectories, and\ntarget velocities. Furthermore, a real-world experiment on a quadrotor with an\nonboard computer validated practicality of the proposed approach.", "AI": {"tldr": "本文提出StableTracker，一种基于学习的控制策略，使四旋翼无人机能够从任意视角稳定跟踪移动目标，作为自主空中摄像机。", "motivation": "现有的FPV目标跟踪方法依赖手工模块化设计，导致硬件过载和累积误差，严重降低了跟踪性能，尤其对于快速加减速目标。", "method": "StableTracker是一种基于学习的控制策略，通过可微分仿真中的时间反向传播进行训练。该策略使四旋翼无人机能够在水平和垂直方向上将目标保持在视野中心，同时保持固定的相对距离。", "result": "仿真实验表明，StableTracker在不同安全距离、轨迹和目标速度下，比最先进的传统算法和学习基线实现了卓越的精度、稳定性和泛化能力。此外，在搭载机载计算机的四旋翼无人机上的真实世界实验验证了所提方法的实用性。", "conclusion": "StableTracker提出了一种实用且鲁棒的、基于学习的控制策略，能够实现四旋翼无人机作为自主空中摄像机，稳定跟踪移动目标。"}}
{"id": "2509.13864", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13864", "abs": "https://arxiv.org/abs/2509.13864", "authors": ["Jovana Videnovic", "Matej Kristan", "Alan Lukezic"], "title": "Distractor-Aware Memory-Based Visual Object Tracking", "comment": "Code available on Github: https://github.com/jovanavidenovic/DAM4SAM", "summary": "Recent emergence of memory-based video segmentation methods such as SAM2 has\nled to models with excellent performance in segmentation tasks, achieving\nleading results on numerous benchmarks. However, these modes are not fully\nadjusted for visual object tracking, where distractors (i.e., objects visually\nsimilar to the target) pose a key challenge. In this paper we propose a\ndistractor-aware drop-in memory module and introspection-based management\nmethod for SAM2, leading to DAM4SAM. Our design effectively reduces the\ntracking drift toward distractors and improves redetection capability after\nobject occlusion. To facilitate the analysis of tracking in the presence of\ndistractors, we construct DiDi, a Distractor-Distilled dataset. DAM4SAM\noutperforms SAM2.1 on thirteen benchmarks and sets new state-of-the-art results\non ten. Furthermore, integrating the proposed distractor-aware memory into a\nreal-time tracker EfficientTAM leads to 11% improvement and matches tracking\nquality of the non-real-time SAM2.1-L on multiple tracking and segmentation\nbenchmarks, while integration with edge-based tracker EdgeTAM delivers 4%\nperformance boost, demonstrating a very good generalization across\narchitectures.", "AI": {"tldr": "本文提出DAM4SAM，通过为SAM2引入一个注意力机制的记忆模块和内省管理方法，有效解决了视觉目标跟踪中分散物导致的漂移和遮挡后的重检测问题。DAM4SAM在多个基准测试中超越SAM2.1并取得SOTA结果，并能很好地泛化到其他跟踪器架构。", "motivation": "现有的基于记忆的视频分割方法（如SAM2）在分割任务中表现出色，但在视觉目标跟踪中，面对分散物（与目标视觉相似的物体）时表现不佳，导致跟踪漂移和遮挡后重检测能力弱。", "method": "本文提出了DAM4SAM，它为SAM2引入了一个注意力分散物（distractor-aware）的即插即用记忆模块和基于内省的记忆管理方法。同时，为了分析分散物存在下的跟踪性能，构建了一个名为DiDi（Distractor-Distilled）的数据集。", "result": "DAM4SAM有效减少了对分散物的跟踪漂移，并提高了物体遮挡后的重检测能力。它在十三个基准测试中超越了SAM2.1，并在其中十个上创造了新的最先进（SOTA）结果。此外，将所提出的注意力分散物记忆模块集成到实时跟踪器EfficientTAM中，性能提升了11%，并达到了非实时SAM2.1-L的跟踪质量；集成到边缘跟踪器EdgeTAM中，性能提升了4%，显示出良好的架构泛化能力。", "conclusion": "通过引入注意力分散物记忆模块和管理方法，DAM4SAM显著提升了在分散物存在下的视觉目标跟踪性能和重检测能力。该方法具有出色的泛化性，可有效集成到多种跟踪器架构中。"}}
{"id": "2509.14159", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14159", "abs": "https://arxiv.org/abs/2509.14159", "authors": ["Dayi Dong", "Maulik Bhatt", "Seoyeon Choi", "Negar Mehr"], "title": "MIMIC-D: Multi-modal Imitation for MultI-agent Coordination with Decentralized Diffusion Policies", "comment": "9 pages, 4 figures, 5 tables", "summary": "As robots become more integrated in society, their ability to coordinate with\nother robots and humans on multi-modal tasks (those with multiple valid\nsolutions) is crucial. We propose to learn such behaviors from expert\ndemonstrations via imitation learning (IL). However, when expert demonstrations\nare multi-modal, standard IL approaches can struggle to capture the diverse\nstrategies, hindering effective coordination. Diffusion models are known to be\neffective at handling complex multi-modal trajectory distributions in\nsingle-agent systems. Diffusion models have also excelled in multi-agent\nscenarios where multi-modality is more common and crucial to learning\ncoordinated behaviors. Typically, diffusion-based approaches require a\ncentralized planner or explicit communication among agents, but this assumption\ncan fail in real-world scenarios where robots must operate independently or\nwith agents like humans that they cannot directly communicate with. Therefore,\nwe propose MIMIC-D, a Centralized Training, Decentralized Execution (CTDE)\nparadigm for multi-modal multi-agent imitation learning using diffusion\npolicies. Agents are trained jointly with full information, but execute\npolicies using only local information to achieve implicit coordination. We\ndemonstrate in both simulation and hardware experiments that our method\nrecovers multi-modal coordination behavior among agents in a variety of tasks\nand environments, while improving upon state-of-the-art baselines.", "AI": {"tldr": "本文提出MIMIC-D，一种基于扩散模型的集中训练、分散执行（CTDE）范式，用于多模态多智能体模仿学习，以实现机器人间的隐式协调，并在模拟和硬件实验中超越现有基线。", "motivation": "当专家演示是多模态时，标准模仿学习难以捕捉多样化策略，阻碍有效协调。现有基于扩散的多智能体方法通常需要集中规划或显式通信，这在机器人必须独立操作或与无法直接通信的人类协作的真实世界场景中不可行。", "method": "提出MIMIC-D，一种用于多模态多智能体模仿学习的集中训练、分散执行（CTDE）范式，采用扩散策略。智能体在训练时共享完整信息，但在执行时仅使用局部信息，从而实现隐式协调。", "result": "在模拟和硬件实验中，该方法成功恢复了各种任务和环境下智能体之间的多模态协调行为，并优于现有的最先进基线方法。", "conclusion": "MIMIC-D框架通过CTDE和扩散策略，有效解决了多模态多智能体模仿学习中去中心化执行的挑战，使机器人能够学习和执行复杂的协调行为。"}}
{"id": "2509.13873", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13873", "abs": "https://arxiv.org/abs/2509.13873", "authors": ["Siam Tahsin Bhuiyan", "Rashedur Rahman", "Sefatul Wasi", "Naomi Yagi", "Syoji Kobashi", "Ashraful Islam", "Saadia Binte Alam"], "title": "Invisible Yet Detected: PelFANet with Attention-Guided Anatomical Fusion for Pelvic Fracture Diagnosis", "comment": "Accepted at MICCAI EMERGE 2025", "summary": "Pelvic fractures pose significant diagnostic challenges, particularly in\ncases where fracture signs are subtle or invisible on standard radiographs. To\naddress this, we introduce PelFANet, a dual-stream attention network that fuses\nraw pelvic X-rays with segmented bone images to improve fracture\nclassification. The network em-ploys Fused Attention Blocks (FABlocks) to\niteratively exchange and refine fea-tures from both inputs, capturing global\ncontext and localized anatomical detail. Trained in a two-stage pipeline with a\nsegmentation-guided approach, PelFANet demonstrates superior performance over\nconventional methods. On the AMERI dataset, it achieves 88.68% accuracy and\n0.9334 AUC on visible fractures, while generalizing effectively to invisible\nfracture cases with 82.29% accuracy and 0.8688 AUC, despite not being trained\non them. These results highlight the clini-cal potential of anatomy-aware\ndual-input architectures for robust fracture detec-tion, especially in\nscenarios with subtle radiographic presentations.", "AI": {"tldr": "PelFANet是一种双流注意力网络，通过融合原始骨盆X射线和分割骨图像来改进骨盆骨折分类，尤其在细微或不可见的骨折诊断中表现出色。", "motivation": "骨盆骨折的诊断具有挑战性，特别是当骨折迹象在标准X光片上不明显或不可见时。", "method": "引入了PelFANet，一个双流注意力网络，融合原始骨盆X光片和分割后的骨骼图像。该网络使用融合注意力块（FABlocks）迭代地交换和细化来自两种输入的特征，以捕获全局上下文和局部解剖细节。它采用两阶段训练流程，并结合了分割引导方法。", "result": "PelFANet在AMERI数据集上表现优于传统方法。对于可见骨折，它达到了88.68%的准确率和0.9334的AUC。尽管未在不可见骨折上训练，它仍能有效泛化到这些病例，达到82.29%的准确率和0.8688的AUC。", "conclusion": "这些结果突出了结合解剖学感知双输入架构在鲁棒骨折检测方面的临床潜力，尤其是在射线照片表现不明显的场景中。"}}
{"id": "2509.14178", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14178", "abs": "https://arxiv.org/abs/2509.14178", "authors": ["Kai Ye", "Yuhang Wu", "Shuyuan Hu", "Junliang Li", "Meng Liu", "Yongquan Chen", "Rui Huang"], "title": "\\textsc{Gen2Real}: Towards Demo-Free Dexterous Manipulation by Harnessing Generated Video", "comment": null, "summary": "Dexterous manipulation remains a challenging robotics problem, largely due to\nthe difficulty of collecting extensive human demonstrations for learning. In\nthis paper, we introduce \\textsc{Gen2Real}, which replaces costly human demos\nwith one generated video and drives robot skill from it: it combines\ndemonstration generation that leverages video generation with pose and depth\nestimation to yield hand-object trajectories, trajectory optimization that uses\nPhysics-aware Interaction Optimization Model (PIOM) to impose physics\nconsistency, and demonstration learning that retargets human motions to a robot\nhand and stabilizes control with an anchor-based residual Proximal Policy\nOptimization (PPO) policy. Using only generated videos, the learned policy\nachieves a 77.3\\% success rate on grasping tasks in simulation and demonstrates\ncoherent executions on a real robot. We also conduct ablation studies to\nvalidate the contribution of each component and demonstrate the ability to\ndirectly specify tasks using natural language, highlighting the flexibility and\nrobustness of \\textsc{Gen2Real} in generalizing grasping skills from imagined\nvideos to real-world execution.", "AI": {"tldr": "Gen2Real提出了一种新方法，利用一个生成的视频替代昂贵的人类演示来学习机器人灵巧操作技能，并在模拟和真实机器人上均取得了高成功率和连贯执行。", "motivation": "灵巧操作在机器人领域仍具挑战性，主要原因是难以收集大量人类演示数据用于学习。", "method": "Gen2Real包含三个主要组成部分：1. 演示生成：利用视频生成技术结合姿态和深度估计来产生手-物体轨迹。2. 轨迹优化：使用物理感知交互优化模型（PIOM）来确保物理一致性。3. 演示学习：将人类动作重新定位到机器人手上，并使用基于锚点的残差近端策略优化（PPO）策略来稳定控制。", "result": "仅使用生成的视频，学习到的策略在模拟抓取任务中达到了77.3%的成功率，并在真实机器人上展示了连贯的执行。此外，消融研究验证了每个组件的贡献，并展示了通过自然语言直接指定任务的能力。", "conclusion": "Gen2Real系统具有灵活性和鲁棒性，能够将从想象视频中学习到的抓取技能泛化到现实世界执行，有效解决了人类演示数据收集的难题。"}}
{"id": "2509.13883", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13883", "abs": "https://arxiv.org/abs/2509.13883", "authors": ["Zhen Xu", "Guorui Lu", "Chang Gao", "Qinyu Chen"], "title": "EvHand-FPV: Efficient Event-Based 3D Hand Tracking from First-Person View", "comment": "8 pages", "summary": "Hand tracking holds great promise for intuitive interaction paradigms, but\nframe-based methods often struggle to meet the requirements of accuracy, low\nlatency, and energy efficiency, especially in resource-constrained settings\nsuch as Extended Reality (XR) devices. Event cameras provide $\\mu$s-level\ntemporal resolution at mW-level power by asynchronously sensing brightness\nchanges. In this work, we present EvHand-FPV, a lightweight framework for\negocentric First-Person-View 3D hand tracking from a single event camera. We\nconstruct an event-based FPV dataset that couples synthetic training data with\n3D labels and real event data with 2D labels for evaluation to address the\nscarcity of egocentric benchmarks. EvHand-FPV also introduces a wrist-based\nregion of interest (ROI) that localizes the hand region via geometric cues,\ncombined with an end-to-end mapping strategy that embeds ROI offsets into the\nnetwork to reduce computation without explicit reconstruction, and a multi-task\nlearning strategy with an auxiliary geometric feature head that improves\nrepresentations without test-time overhead. On our real FPV test set,\nEvHand-FPV improves 2D-AUCp from 0.77 to 0.85 while reducing parameters from\n11.2M to 1.2M by 89% and FLOPs per inference from 1.648G to 0.185G by 89%. It\nalso maintains a competitive 3D-AUCp of 0.84 on synthetic data. These results\ndemonstrate accurate and efficient egocentric event-based hand tracking\nsuitable for on-device XR applications. The dataset and code are available at\nhttps://github.com/zen5x5/EvHand-FPV.", "AI": {"tldr": "EvHand-FPV是一个轻量级框架，利用单事件相机实现第一人称视角的3D手部跟踪，在XR设备上实现了高精度和高效率。", "motivation": "传统的基于帧的手部跟踪方法在精度、低延迟和能效方面表现不佳，尤其是在XR设备等资源受限的环境中。事件相机因其微秒级时间分辨率和毫瓦级功耗而具有潜力。", "method": "本文提出了EvHand-FPV框架，用于从单个事件相机进行以自我为中心的FPV 3D手部跟踪。为此，构建了一个新的事件基FPV数据集，结合了带3D标签的合成数据和带2D标签的真实事件数据。该框架引入了基于手腕的ROI（感兴趣区域）以几何线索定位手部，并采用端到端映射策略将ROI偏移嵌入网络以减少计算。此外，还使用了多任务学习策略，带有一个辅助几何特征头以改进表示，而无测试时开销。", "result": "在真实FPV测试集上，EvHand-FPV将2D-AUCp从0.77提高到0.85，同时将参数量从11.2M减少到1.2M（减少89%），每次推理的FLOPs从1.648G减少到0.185G（减少89%）。在合成数据上，它保持了0.84的3D-AUCp，具有竞争力。", "conclusion": "EvHand-FPV展示了准确且高效的以自我为中心的事件基手部跟踪能力，非常适用于设备上的XR应用。"}}
{"id": "2509.14191", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.14191", "abs": "https://arxiv.org/abs/2509.14191", "authors": ["Zhihao Cao", "Hanyu Wu", "Li Wa Tang", "Zizhou Luo", "Zihan Zhu", "Wei Zhang", "Marc Pollefeys", "Martin R. Oswald"], "title": "MCGS-SLAM: A Multi-Camera SLAM Framework Using Gaussian Splatting for High-Fidelity Mapping", "comment": null, "summary": "Recent progress in dense SLAM has primarily targeted monocular setups, often\nat the expense of robustness and geometric coverage. We present MCGS-SLAM, the\nfirst purely RGB-based multi-camera SLAM system built on 3D Gaussian Splatting\n(3DGS). Unlike prior methods relying on sparse maps or inertial data, MCGS-SLAM\nfuses dense RGB inputs from multiple viewpoints into a unified, continuously\noptimized Gaussian map. A multi-camera bundle adjustment (MCBA) jointly refines\nposes and depths via dense photometric and geometric residuals, while a scale\nconsistency module enforces metric alignment across views using low-rank\npriors. The system supports RGB input and maintains real-time performance at\nlarge scale. Experiments on synthetic and real-world datasets show that\nMCGS-SLAM consistently yields accurate trajectories and photorealistic\nreconstructions, usually outperforming monocular baselines. Notably, the wide\nfield of view from multi-camera input enables reconstruction of side-view\nregions that monocular setups miss, critical for safe autonomous operation.\nThese results highlight the promise of multi-camera Gaussian Splatting SLAM for\nhigh-fidelity mapping in robotics and autonomous driving.", "AI": {"tldr": "MCGS-SLAM是首个纯RGB多摄像头3D高斯泼溅SLAM系统，它通过多摄像头束调整和尺度一致性模块，实现高精度轨迹和逼真重建，优于单目系统，并能重建单目缺失的侧视图区域，适用于机器人和自动驾驶。", "motivation": "单目稠密SLAM在鲁棒性和几何覆盖方面存在不足；现有方法依赖稀疏地图或惯性数据；需要一种纯RGB的多摄像头SLAM系统来提供更全面的几何信息，尤其对自动驾驶中的侧视图区域至关重要。", "method": "MCGS-SLAM是一个纯RGB多摄像头SLAM系统，基于3D高斯泼溅（3DGS）。它将来自多个视角的稠密RGB输入融合到统一且持续优化的3D高斯地图中。通过多摄像头束调整（MCBA）利用稠密光度学和几何残差共同优化位姿和深度。此外，一个尺度一致性模块通过低秩先验强制视图间的度量对齐。", "result": "MCGS-SLAM在合成和真实世界数据集上均能持续产生准确的轨迹和逼真的重建，通常优于单目基线系统。多摄像头输入提供的广阔视野使得系统能够重建单目设置遗漏的侧视图区域，这对于安全的自主操作至关重要。", "conclusion": "这些结果突显了多摄像头高斯泼溅SLAM在机器人和自动驾驶领域实现高保真度地图构建的巨大潜力。"}}
{"id": "2509.13907", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13907", "abs": "https://arxiv.org/abs/2509.13907", "authors": ["Jiyun Im", "SuBeen Lee", "Miso Lee", "Jae-Pil Heo"], "title": "White Aggregation and Restoration for Few-shot 3D Point Cloud Semantic Segmentation", "comment": "9 pages, 5 figures", "summary": "Few-Shot 3D Point Cloud Segmentation (FS-PCS) aims to predict per-point\nlabels for an unlabeled point cloud, given only a few labeled examples. To\nextract discriminative representations from the limited support set, existing\nmethods have constructed prototypes using conventional algorithms such as\nfarthest point sampling. However, we point out that its initial randomness\nsignificantly affects FS-PCS performance and that the prototype generation\nprocess remains underexplored despite its prevalence. This motivates us to\ninvestigate an advanced prototype generation method based on attention\nmechanism. Despite its potential, we found that vanilla module suffers from the\ndistributional gap between learnable prototypical tokens and support features.\nTo overcome this, we propose White Aggregation and Restoration Module (WARM),\nwhich resolves the misalignment by sandwiching cross-attention between\nwhitening and coloring transformations. Specifically, whitening aligns the\nsupport features to prototypical tokens before attention process, and\nsubsequently coloring restores the original distribution to the attended\ntokens. This simple yet effective design enables robust attention, thereby\ngenerating representative prototypes by capturing the semantic relationships\namong support features. Our method achieves state-of-the-art performance with a\nsignificant margin on multiple FS-PCS benchmarks, demonstrating its\neffectiveness through extensive experiments.", "AI": {"tldr": "本文提出了一种名为WARM的新型原型生成方法，用于少样本3D点云分割，通过在注意力机制中引入白化和着色变换来解决原型令牌与支持特征之间的分布差异，从而显著提升了性能。", "motivation": "现有的少样本3D点云分割（FS-PCS）方法在原型生成时，如使用最远点采样，其初始随机性会严重影响性能，且原型生成过程未被充分探索。此外，朴素注意力模块在学习到的原型令牌与支持特征之间存在分布差异。", "method": "本文提出了白化聚合与恢复模块（White Aggregation and Restoration Module, WARM）。该模块通过在交叉注意力机制前后分别进行白化和着色变换来解决分布错位问题。具体而言，白化操作在注意力过程前将支持特征与原型令牌对齐，随后着色操作恢复注意力处理后的令牌的原始分布。这种设计使得注意力机制更加鲁棒，能够捕捉支持特征间的语义关系以生成更具代表性的原型。", "result": "本文方法在多个FS-PCS基准测试上取得了显著超越现有最先进性能的成果，并通过大量实验证明了其有效性。", "conclusion": "WARM模块通过解决原型生成中的分布错位问题，能够鲁棒地生成代表性原型，从而显著提升了少样本3D点云分割任务的性能，达到了当前最先进水平。"}}
{"id": "2509.14210", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14210", "abs": "https://arxiv.org/abs/2509.14210", "authors": ["Seth Farrell", "Chenghao Li", "Hongzhan Yu", "Hesam Mojtahedi", "Sicun Gao", "Henrik I. Christensen"], "title": "GLIDE: A Coordinated Aerial-Ground Framework for Search and Rescue in Unknown Environments", "comment": null, "summary": "We present a cooperative aerial-ground search-and-rescue (SAR) framework that\npairs two unmanned aerial vehicles (UAVs) with an unmanned ground vehicle (UGV)\nto achieve rapid victim localization and obstacle-aware navigation in unknown\nenvironments. We dub this framework Guided Long-horizon Integrated Drone Escort\n(GLIDE), highlighting the UGV's reliance on UAV guidance for long-horizon\nplanning. In our framework, a goal-searching UAV executes real-time onboard\nvictim detection and georeferencing to nominate goals for the ground platform,\nwhile a terrain-scouting UAV flies ahead of the UGV's planned route to provide\nmid-level traversability updates. The UGV fuses aerial cues with local sensing\nto perform time-efficient A* planning and continuous replanning as information\narrives. Additionally, we present a hardware demonstration (using a GEM e6 golf\ncart as the UGV and two X500 UAVs) to evaluate end-to-end SAR mission\nperformance and include simulation ablations to assess the planning stack in\nisolation from detection. Empirical results demonstrate that explicit role\nseparation across UAVs, coupled with terrain scouting and guided planning,\nimproves reach time and navigation safety in time-critical SAR missions.", "AI": {"tldr": "该论文提出了一种名为GLIDE的空地协同搜救（SAR）框架，该框架利用两架无人机（UAV）和一辆无人地面车辆（UGV）在未知环境中实现快速受害者定位和避障导航。", "motivation": "在未知环境中，快速定位受害者并进行避障导航是搜救任务中的关键挑战，需要更高效和安全的解决方案。", "method": "GLIDE框架中，一架“目标搜索”无人机负责实时机载受害者检测和地理定位，为地面平台指定目标；另一架“地形侦察”无人机则飞行在UGV前方，提供中级可通行性更新。UGV结合空中信息和本地感知数据，执行高效的A*路径规划，并随着新信息的到来进行持续重规划。研究还通过硬件演示（使用高尔夫球车作为UGV和两架X500无人机）评估了端到端搜救任务性能，并进行了仿真消融实验以独立评估规划堆栈。", "result": "实证结果表明，无人机之间明确的角色分离，结合地形侦察和引导式规划，显著缩短了任务抵达时间，并提高了时间敏感型搜救任务的导航安全性。", "conclusion": "GLIDE框架通过无人机间的专业化分工和对UGV的引导规划，有效提升了在未知环境中进行快速受害者定位和安全导航的搜救能力。"}}
{"id": "2509.13919", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13919", "abs": "https://arxiv.org/abs/2509.13919", "authors": ["Yuanchen Wu", "Ke Yan", "Shouhong Ding", "Ziyin Zhou", "Xiaoqiang Li"], "title": "Towards Rationale-Answer Alignment of LVLMs via Self-Rationale Calibration", "comment": "Accepted by ICML 2025", "summary": "Large Vision-Language Models (LVLMs) have manifested strong visual question\nanswering capability. However, they still struggle with aligning the rationale\nand the generated answer, leading to inconsistent reasoning and incorrect\nresponses. To this end, this paper introduces the Self-Rationale Calibration\n(SRC) framework to iteratively calibrate the alignment between rationales and\nanswers. SRC begins by employing a lightweight \"rationale fine-tuning\"\napproach, which modifies the model's response format to require a rationale\nbefore deriving an answer without explicit prompts. Next, SRC searches for a\ndiverse set of candidate responses from the fine-tuned LVLMs for each sample,\nfollowed by a proposed pairwise scoring strategy using a tailored scoring\nmodel, R-Scorer, to evaluate both rationale quality and factual consistency of\ncandidates. Based on a confidence-weighted preference curation process, SRC\ndecouples the alignment calibration into a preference fine-tuning manner,\nleading to significant improvements of LVLMs in perception, reasoning, and\ngeneralization across multiple benchmarks. Our results emphasize the\nrationale-oriented alignment in exploring the potential of LVLMs.", "AI": {"tldr": "本文提出了自理由校准（SRC）框架，通过迭代校准理由与答案之间的一致性，显著提升了大型视觉-语言模型（LVLMs）的感知、推理和泛化能力。", "motivation": "大型视觉-语言模型（LVLMs）在视觉问答方面表现出色，但仍难以对齐其推理理由和生成的答案，导致推理不一致和响应错误。", "method": "SRC框架首先采用轻量级“理由微调”方法，修改模型响应格式，要求在得出答案前先给出理由。接着，它为每个样本从微调后的LVLMs中搜索多样化的候选响应，并使用定制的评分模型R-Scorer通过成对评分策略评估候选响应的理由质量和事实一致性。最后，基于置信度加权的偏好筛选过程，SRC将对齐校准解耦为偏好微调方式。", "result": "SRC框架使LVLMs在多个基准测试中的感知、推理和泛化能力得到显著提升。", "conclusion": "研究结果强调了以理由为导向的对齐在探索LVLMs潜力方面的重要性。"}}
{"id": "2509.14228", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14228", "abs": "https://arxiv.org/abs/2509.14228", "authors": ["Benjamin Shaffer", "Victoria Edwards", "Brooks Kinch", "Nathaniel Trask", "M. Ani Hsieh"], "title": "Multi-robot Multi-source Localization in Complex Flows with Physics-Preserving Environment Models", "comment": null, "summary": "Source localization in a complex flow poses a significant challenge for\nmulti-robot teams tasked with localizing the source of chemical leaks or\ntracking the dispersion of an oil spill. The flow dynamics can be time-varying\nand chaotic, resulting in sporadic and intermittent sensor readings, and\ncomplex environmental geometries further complicate a team's ability to model\nand predict the dispersion. To accurately account for the physical processes\nthat drive the dispersion dynamics, robots must have access to computationally\nintensive numerical models, which can be difficult when onboard computation is\nlimited. We present a distributed mobile sensing framework for source\nlocalization in which each robot carries a machine-learned, finite element\nmodel of its environment to guide information-based sampling. The models are\nused to evaluate an approximate mutual information criterion to drive an\ninfotaxis control strategy, which selects sensing regions that are expected to\nmaximize informativeness for the source localization objective. Our approach\nachieves faster error reduction compared to baseline sensing strategies and\nresults in more accurate source localization compared to baseline machine\nlearning approaches.", "AI": {"tldr": "本文提出了一种分布式移动传感框架，利用机器人携带的机器学习有限元模型和信息驱动的采样策略，在复杂流场中实现更快速、更准确的源定位。", "motivation": "在复杂流场（如化学泄漏、溢油）中进行源定位，面临多重挑战：流体动力学时变且混沌，导致传感器读数零星间歇；环境几何复杂；机器人板载计算能力有限，难以运行计算密集型数值模型来准确描述扩散动力学。", "method": "该框架采用分布式移动传感，每个机器人携带一个机器学习的有限元模型来表征其环境，并以此指导基于信息的采样。模型用于评估一个近似互信息准则，以驱动一种信息趋向（infotaxis）控制策略，选择预期能最大化源定位目标信息量的传感区域。", "result": "与基线传感策略相比，本文方法实现了更快的误差降低。与基线机器学习方法相比，该方法实现了更准确的源定位。", "conclusion": "所提出的分布式移动传感框架，通过结合机器人携带的机器学习有限元模型和基于互信息的信息趋向控制策略，显著提高了在复杂流场中进行源定位的速度和准确性。"}}
{"id": "2509.13922", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13922", "abs": "https://arxiv.org/abs/2509.13922", "authors": ["Wenkui Yang", "Jie Cao", "Junxian Duan", "Ran He"], "title": "Towards Robust Defense against Customization via Protective Perturbation Resistant to Diffusion-based Purification", "comment": "Accepted to ICCV 2025", "summary": "Diffusion models like Stable Diffusion have become prominent in visual\nsynthesis tasks due to their powerful customization capabilities, which also\nintroduce significant security risks, including deepfakes and copyright\ninfringement. In response, a class of methods known as protective perturbation\nemerged, which mitigates image misuse by injecting imperceptible adversarial\nnoise. However, purification can remove protective perturbations, thereby\nexposing images again to the risk of malicious forgery. In this work, we\nformalize the anti-purification task, highlighting challenges that hinder\nexisting approaches, and propose a simple diagnostic protective perturbation\nnamed AntiPure. AntiPure exposes vulnerabilities of purification within the\n\"purification-customization\" workflow, owing to two guidance mechanisms: 1)\nPatch-wise Frequency Guidance, which reduces the model's influence over\nhigh-frequency components in the purified image, and 2) Erroneous Timestep\nGuidance, which disrupts the model's denoising strategy across different\ntimesteps. With additional guidance, AntiPure embeds imperceptible\nperturbations that persist under representative purification settings,\nachieving effective post-customization distortion. Experiments show that, as a\nstress test for purification, AntiPure achieves minimal perceptual discrepancy\nand maximal distortion, outperforming other protective perturbation methods\nwithin the purification-customization workflow.", "AI": {"tldr": "扩散模型存在安全风险，防护性扰动可缓解，但净化方法会将其移除。本文提出AntiPure，一种诊断性防护扰动，通过频率和时间步指导，使扰动在净化后依然存在，有效对抗恶意伪造。", "motivation": "扩散模型（如Stable Diffusion）强大的定制能力带来了深度伪造和版权侵犯等严重安全风险。虽然防护性扰动可以减轻图像滥用，但净化方法能够去除这些扰动，使图像再次面临恶意伪造的风险，因此需要一种能抵抗净化的防护扰动。", "method": "本文将抗净化任务形式化，并提出了一种名为AntiPure的诊断性防护扰动。AntiPure通过两种指导机制来暴露净化在“净化-定制”工作流中的脆弱性：1) 逐块频率指导，减少模型对净化图像高频分量的影响；2) 错误时间步指导，扰乱模型在不同时间步的去噪策略。这些指导机制使得嵌入的微扰在代表性净化设置下依然存在。", "result": "实验表明，作为净化的压力测试，AntiPure实现了最小的感知差异和最大的定制后失真，在“净化-定制”工作流中优于其他防护性扰动方法。", "conclusion": "AntiPure成功解决了净化移除防护性扰动的问题，通过其独特的指导机制，使扰动在净化后仍能有效抵御恶意定制，从而暴露了净化方法的脆弱性，为图像安全提供了新的保障。"}}
{"id": "2509.13936", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13936", "abs": "https://arxiv.org/abs/2509.13936", "authors": ["Harvey Mannering", "Zhiwu Huang", "Adam Prugel-Bennett"], "title": "Noise-Level Diffusion Guidance: Well Begun is Half Done", "comment": null, "summary": "Diffusion models have achieved state-of-the-art image generation. However,\nthe random Gaussian noise used to start the diffusion process influences the\nfinal output, causing variations in image quality and prompt adherence.\nExisting noise-level optimization approaches generally rely on extra dataset\nconstruction, additional networks, or backpropagation-based optimization,\nlimiting their practicality. In this paper, we propose Noise Level Guidance\n(NLG), a simple, efficient, and general noise-level optimization approach that\nrefines initial noise by increasing the likelihood of its alignment with\ngeneral guidance - requiring no additional training data, auxiliary networks,\nor backpropagation. The proposed NLG approach provides a unified framework\ngeneralizable to both conditional and unconditional diffusion models,\naccommodating various forms of diffusion-level guidance. Extensive experiments\non five standard benchmarks demonstrate that our approach enhances output\ngeneration quality and input condition adherence. By seamlessly integrating\nwith existing guidance methods while maintaining computational efficiency, our\nmethod establishes NLG as a practical and scalable enhancement to diffusion\nmodels. Code can be found at\nhttps://github.com/harveymannering/NoiseLevelGuidance.", "AI": {"tldr": "本文提出噪声水平引导（NLG），一种简单、高效且通用的噪声水平优化方法，通过提高初始噪声与通用引导的对齐可能性来改进扩散模型，从而提升图像质量和提示依从性，无需额外训练数据、辅助网络或反向传播。", "motivation": "扩散模型中使用的随机高斯噪声会影响最终输出，导致图像质量和提示依从性出现偏差。现有的噪声水平优化方法通常依赖于额外的数据集构建、附加网络或基于反向传播的优化，限制了其实用性。", "method": "本文提出噪声水平引导（NLG）方法。该方法通过增加初始噪声与通用引导对齐的可能性来优化初始噪声，无需额外训练数据、辅助网络或反向传播。它提供了一个统一的框架，可推广到条件和无条件扩散模型，并适应各种形式的扩散级引导。", "result": "在五个标准基准上的大量实验表明，所提出的NLG方法显著提高了输出生成质量和输入条件依从性。该方法能够与现有引导方法无缝集成，同时保持计算效率。", "conclusion": "NLG作为一种实用且可扩展的增强方法，为扩散模型提供了显著改进，提升了图像生成质量和对输入条件的依从性。"}}
{"id": "2509.13939", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13939", "abs": "https://arxiv.org/abs/2509.13939", "authors": ["Gia Khanh Nguyen", "Yifeng Huang", "Minh Hoai"], "title": "Can Current AI Models Count What We Mean, Not What They See? A Benchmark and Systematic Evaluation", "comment": null, "summary": "Visual counting is a fundamental yet challenging task, especially when users\nneed to count objects of a specific type in complex scenes. While recent\nmodels, including class-agnostic counting models and large vision-language\nmodels (VLMs), show promise in counting tasks, their ability to perform\nfine-grained, intent-driven counting remains unclear. In this paper, we\nintroduce PairTally, a benchmark dataset specifically designed to evaluate\nfine-grained visual counting. Each of the 681 high-resolution images in\nPairTally contains two object categories, requiring models to distinguish and\ncount based on subtle differences in shape, size, color, or semantics. The\ndataset includes both inter-category (distinct categories) and intra-category\n(closely related subcategories) settings, making it suitable for rigorous\nevaluation of selective counting capabilities. We benchmark a variety of\nstate-of-the-art models, including exemplar-based methods, language-prompted\nmodels, and large VLMs. Our results show that despite recent advances, current\nmodels struggle to reliably count what users intend, especially in fine-grained\nand visually ambiguous cases. PairTally provides a new foundation for\ndiagnosing and improving fine-grained visual counting systems.", "AI": {"tldr": "本文提出了PairTally，一个用于评估细粒度视觉计数的基准数据集。它旨在测试模型在复杂场景中根据用户意图进行精确计数的SOTA能力，并发现现有模型在此类任务中表现不佳。", "motivation": "尽管现有的类别无关计数模型和大型视觉-语言模型（VLMs）在计数任务中展现出潜力，但它们在执行细粒度、意图驱动的计数方面的能力尚不明确。研究动机是填补这一评估空白。", "method": "研究方法是引入PairTally数据集，包含681张高分辨率图像，每张图像包含两个需要根据形状、大小、颜色或语义进行区分和计数的对象类别。数据集包括类别间（distinct categories）和类别内（closely related subcategories）设置。作者使用该数据集对各种最先进的模型（包括基于示例的方法、语言提示模型和大型VLMs）进行了基准测试。", "result": "基准测试结果表明，尽管近期有所进展，但当前模型在可靠地计数用户意图的对象方面仍然面临困难，尤其是在细粒度和视觉模糊的情况下。", "conclusion": "PairTally数据集为诊断和改进细粒度视觉计数系统提供了一个新的基础，揭示了当前模型在该领域存在的局限性。"}}
{"id": "2509.14012", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.14012", "abs": "https://arxiv.org/abs/2509.14012", "authors": ["Tamara R. Lenhard", "Andreas Weinmann", "Tobias Koch"], "title": "Performance Optimization of YOLO-FEDER FusionNet for Robust Drone Detection in Visually Complex Environments", "comment": null, "summary": "Drone detection in visually complex environments remains challenging due to\nbackground clutter, small object scale, and camouflage effects. While generic\nobject detectors like YOLO exhibit strong performance in low-texture scenes,\ntheir effectiveness degrades in cluttered environments with low\nobject-background separability. To address these limitations, this work\npresents an enhanced iteration of YOLO-FEDER FusionNet -- a detection framework\nthat integrates generic object detection with camouflage object detection\ntechniques. Building upon the original architecture, the proposed iteration\nintroduces systematic advancements in training data composition, feature fusion\nstrategies, and backbone design. Specifically, the training process leverages\nlarge-scale, photo-realistic synthetic data, complemented by a small set of\nreal-world samples, to enhance robustness under visually complex conditions.\nThe contribution of intermediate multi-scale FEDER features is systematically\nevaluated, and detection performance is comprehensively benchmarked across\nmultiple YOLO-based backbone configurations. Empirical results indicate that\nintegrating intermediate FEDER features, in combination with backbone upgrades,\ncontributes to notable performance improvements. In the most promising\nconfiguration -- YOLO-FEDER FusionNet with a YOLOv8l backbone and FEDER\nfeatures derived from the DWD module -- these enhancements lead to a FNR\nreduction of up to 39.1 percentage points and a mAP increase of up to 62.8\npercentage points at an IoU threshold of 0.5, compared to the initial baseline.", "AI": {"tldr": "本文提出YOLO-FEDER FusionNet的增强版本，通过优化训练数据、特征融合策略和骨干网络设计，显著提升了在视觉复杂环境下无人机检测的性能。", "motivation": "在背景杂乱、目标尺度小和伪装效应等视觉复杂环境中，无人机检测仍然具有挑战性。通用目标检测器（如YOLO）在此类低目标-背景可分离性的环境中性能会下降，因此需要解决这些局限性。", "method": "本研究提出了YOLO-FEDER FusionNet的增强迭代版本。具体方法包括：利用大规模逼真合成数据辅以少量真实世界样本进行训练；系统评估中间多尺度FEDER特征的贡献；以及在多种基于YOLO的骨干网络配置（例如YOLOv8l）上进行全面性能基准测试。最佳配置结合了YOLOv8l骨干网络和来自DWD模块的FEDER特征。", "result": "经验结果表明，整合中间FEDER特征并结合骨干网络升级，可带来显著的性能提升。在最佳配置下（YOLO-FEDER FusionNet结合YOLOv8l骨干网络和DWD模块的FEDER特征），与初始基线相比，FNR（假阴性率）降低高达39.1个百分点，mAP（平均精度）在IoU阈值为0.5时增加高达62.8个百分点。", "conclusion": "通过整合中间FEDER特征和骨干网络升级，YOLO-FEDER FusionNet的增强版本显著提高了在视觉复杂环境中无人机检测的性能，证明了这些改进的有效性。"}}
{"id": "2509.14033", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.14033", "abs": "https://arxiv.org/abs/2509.14033", "authors": ["Weijie Yin", "Yongjie Ye", "Fangxun Shu", "Yue Liao", "Zijian Kang", "Hongyuan Dong", "Haiyang Yu", "Dingkang Yang", "Jiacong Wang", "Han Wang", "Wenzhuo Liu", "Xiao Liang", "Shuicheng Yan", "Chao Feng"], "title": "SAIL-VL2 Technical Report", "comment": "Technical Report", "summary": "We introduce SAIL-VL2, an open-suite vision-language foundation model (LVM)\nfor comprehensive multimodal understanding and reasoning. As the successor to\nSAIL-VL, SAIL-VL2 achieves state-of-the-art performance at the 2B and 8B\nparameter scales across diverse image and video benchmarks, demonstrating\nstrong capabilities from fine-grained perception to complex reasoning. Three\ncore innovations drive its effectiveness. First, a large-scale data curation\npipeline with scoring and filtering strategies enhances both quality and\ndistribution across captioning, OCR, QA, and video data, improving training\nefficiency. Second, a progressive training framework begins with a powerful\npre-trained vision encoder (SAIL-ViT), advances through multimodal\npre-training, and culminates in a thinking-fusion SFT-RL hybrid paradigm that\nsystematically strengthens model capabilities. Third, architectural advances\nextend beyond dense LLMs to efficient sparse Mixture-of-Experts (MoE) designs.\nWith these contributions, SAIL-VL2 demonstrates competitive performance across\n106 datasets and achieves state-of-the-art results on challenging reasoning\nbenchmarks such as MMMU and MathVista. Furthermore, on the OpenCompass\nleaderboard, SAIL-VL2-2B ranks first among officially released open-source\nmodels under the 4B parameter scale, while serving as an efficient and\nextensible foundation for the open-source multimodal community.", "AI": {"tldr": "SAIL-VL2是一个开源视觉语言基础模型，通过大规模数据、渐进式训练框架和MoE架构创新，在图像和视频理解与推理任务中取得了最先进的性能。", "motivation": "旨在开发一个更全面、更高效的视觉语言基础模型，以超越SAIL-VL，并在多模态理解和推理任务中实现最先进的性能，同时为开源社区提供一个可扩展的基础。", "method": "1. 大规模数据整理流程，通过评分和过滤策略优化字幕、OCR、问答和视频数据的质量和分布。2. 渐进式训练框架，包括强大的预训练视觉编码器（SAIL-ViT）、多模态预训练，以及“thinking-fusion”SFT-RL混合范式。3. 架构创新，采用高效的稀疏专家混合（MoE）设计。", "result": "SAIL-VL2在2B和8B参数规模下，在各种图像和视频基准测试中达到了最先进的性能。在106个数据集上表现出竞争力，并在MMMU和MathVista等挑战性推理基准上取得了最先进结果。SAIL-VL2-2B在OpenCompass排行榜上，在4B参数规模以下的官方开源模型中排名第一。", "conclusion": "SAIL-VL2展示了强大的多模态理解和推理能力，并作为一个高效且可扩展的开源基础模型，为开源多模态社区提供了重要的贡献。"}}
{"id": "2509.14051", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.14051", "abs": "https://arxiv.org/abs/2509.14051", "authors": ["Suhang You", "Carla Pitarch-Abaigar", "Sanket Kachole", "Sumedh Sonawane", "Juhyung Ha", "Anish Sudarshan Gada", "David Crandall", "Rakesh Shiradkar", "Spyridon Bakas"], "title": "PROFUSEme: PROstate Cancer Biochemical Recurrence Prediction via FUSEd Multi-modal Embeddings", "comment": "11 pages, 1 figure, method paper for CHIMERA 2025 Challenge", "summary": "Almost 30% of prostate cancer (PCa) patients undergoing radical prostatectomy\n(RP) experience biochemical recurrence (BCR), characterized by increased\nprostate specific antigen (PSA) and associated with increased mortality.\nAccurate early prediction of BCR, at the time of RP, would contribute to prompt\nadaptive clinical decision-making and improved patient outcomes. In this work,\nwe propose prostate cancer BCR prediction via fused multi-modal embeddings\n(PROFUSEme), which learns cross-modal interactions of clinical, radiology, and\npathology data, following an intermediate fusion configuration in combination\nwith Cox Proportional Hazard regressors. Quantitative evaluation of our\nproposed approach reveals superior performance, when compared with late fusion\nconfigurations, yielding a mean C-index of 0.861 ($\\sigma=0.112$) on the\ninternal 5-fold nested cross-validation framework, and a C-index of 0.7103 on\nthe hold out data of CHIMERA 2025 challenge validation leaderboard.", "AI": {"tldr": "解析错误", "motivation": "解析错误", "method": "解析错误", "result": "解析错误", "conclusion": "解析错误"}}
{"id": "2509.14055", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.14055", "abs": "https://arxiv.org/abs/2509.14055", "authors": ["Gang Cheng", "Xin Gao", "Li Hu", "Siqi Hu", "Mingyang Huang", "Chaonan Ji", "Ju Li", "Dechao Meng", "Jinwei Qi", "Penchong Qiao", "Zhen Shen", "Yafei Song", "Ke Sun", "Linrui Tian", "Feng Wang", "Guangyuan Wang", "Qi Wang", "Zhongjian Wang", "Jiayu Xiao", "Sheng Xu", "Bang Zhang", "Peng Zhang", "Xindi Zhang", "Zhe Zhang", "Jingren Zhou", "Lian Zhuo"], "title": "Wan-Animate: Unified Character Animation and Replacement with Holistic Replication", "comment": "Project Page: https://humanaigc.github.io/wan-animate/", "summary": "We introduce Wan-Animate, a unified framework for character animation and\nreplacement. Given a character image and a reference video, Wan-Animate can\nanimate the character by precisely replicating the expressions and movements of\nthe character in the video to generate high-fidelity character videos.\nAlternatively, it can integrate the animated character into the reference video\nto replace the original character, replicating the scene's lighting and color\ntone to achieve seamless environmental integration. Wan-Animate is built upon\nthe Wan model. To adapt it for character animation tasks, we employ a modified\ninput paradigm to differentiate between reference conditions and regions for\ngeneration. This design unifies multiple tasks into a common symbolic\nrepresentation. We use spatially-aligned skeleton signals to replicate body\nmotion and implicit facial features extracted from source images to reenact\nexpressions, enabling the generation of character videos with high\ncontrollability and expressiveness. Furthermore, to enhance environmental\nintegration during character replacement, we develop an auxiliary Relighting\nLoRA. This module preserves the character's appearance consistency while\napplying the appropriate environmental lighting and color tone. Experimental\nresults demonstrate that Wan-Animate achieves state-of-the-art performance. We\nare committed to open-sourcing the model weights and its source code.", "AI": {"tldr": "Wan-Animate是一个统一的角色动画和替换框架，能够根据参考视频高保真地动画化角色，或将其无缝替换到视频中。", "motivation": "现有方法可能在角色动画的保真度、表现力或角色替换的环境集成方面存在不足，需要一个统一且高效的解决方案。", "method": "该框架基于Wan模型，并采用修改的输入范式来区分参考条件和生成区域，从而将多个任务统一为共同的符号表示。它使用空间对齐的骨架信号来复制身体动作，并利用从源图像中提取的隐式面部特征来重现表情。此外，为增强角色替换时的环境集成，开发了一个辅助的Relighting LoRA模块，以保持角色外观一致性并应用适当的环境光照和色调。", "result": "Wan-Animate在角色动画和替换方面取得了最先进的性能，能够生成高保真、高可控性和表现力的角色视频，并实现无缝的环境集成。", "conclusion": "Wan-Animate提供了一个创新的统一框架，通过结合先进的生成技术和专门的模块，显著提升了角色动画和替换的质量和灵活性，并计划开源其模型权重和源代码。"}}
{"id": "2509.14060", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.14060", "abs": "https://arxiv.org/abs/2509.14060", "authors": ["Jun Du", "Weiwei Xing", "Ming Li", "Fei Richard Yu"], "title": "VSE-MOT: Multi-Object Tracking in Low-Quality Video Scenes Guided by Visual Semantic Enhancement", "comment": null, "summary": "Current multi-object tracking (MOT) algorithms typically overlook issues\ninherent in low-quality videos, leading to significant degradation in tracking\nperformance when confronted with real-world image deterioration. Therefore,\nadvancing the application of MOT algorithms in real-world low-quality video\nscenarios represents a critical and meaningful endeavor. To address the\nchallenges posed by low-quality scenarios, inspired by vision-language models,\nthis paper proposes a Visual Semantic Enhancement-guided Multi-Object Tracking\nframework (VSE-MOT). Specifically, we first design a tri-branch architecture\nthat leverages a vision-language model to extract global visual semantic\ninformation from images and fuse it with query vectors. Subsequently, to\nfurther enhance the utilization of visual semantic information, we introduce\nthe Multi-Object Tracking Adapter (MOT-Adapter) and the Visual Semantic Fusion\nModule (VSFM). The MOT-Adapter adapts the extracted global visual semantic\ninformation to suit multi-object tracking tasks, while the VSFM improves the\nefficacy of feature fusion. Through extensive experiments, we validate the\neffectiveness and superiority of the proposed method in real-world low-quality\nvideo scenarios. Its tracking performance metrics outperform those of existing\nmethods by approximately 8% to 20%, while maintaining robust performance in\nconventional scenarios.", "AI": {"tldr": "本文提出了一种名为VSE-MOT的视觉语义增强多目标跟踪框架，通过利用视觉-语言模型提取全局视觉语义信息，并结合适配器和融合模块，显著提升了在低质量视频场景下的跟踪性能。", "motivation": "当前的多目标跟踪（MOT）算法在低质量视频中表现不佳，导致在真实世界图像质量下降时跟踪性能严重退化。因此，在真实世界低质量视频场景中推进MOT算法的应用是一项关键且有意义的工作。", "method": "本文提出了VSE-MOT框架。首先，设计了一个三分支架构，利用视觉-语言模型提取全局视觉语义信息并与查询向量融合。其次，引入了多目标跟踪适配器（MOT-Adapter）来调整提取的全局视觉语义信息以适应MOT任务，并引入了视觉语义融合模块（VSFM）来提高特征融合的效率。", "result": "通过大量实验验证了所提方法在真实世界低质量视频场景中的有效性和优越性。其跟踪性能指标比现有方法提高了约8%到20%，同时在常规场景中保持了鲁棒的性能。", "conclusion": "VSE-MOT框架通过有效利用视觉语义信息，显著改善了多目标跟踪在低质量视频场景下的性能，并在常规场景中保持了竞争力，为真实世界应用提供了有力支持。"}}
{"id": "2509.14084", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.14084", "abs": "https://arxiv.org/abs/2509.14084", "authors": ["Jingyi Yuan", "Jianxiong Ye", "Wenkang Chen", "Chenqiang Gao"], "title": "AD-DINOv3: Enhancing DINOv3 for Zero-Shot Anomaly Detection with Anomaly-Aware Calibration", "comment": null, "summary": "Zero-Shot Anomaly Detection (ZSAD) seeks to identify anomalies from arbitrary\nnovel categories, offering a scalable and annotation-efficient solution.\nTraditionally, most ZSAD works have been based on the CLIP model, which\nperforms anomaly detection by calculating the similarity between visual and\ntext embeddings. Recently, vision foundation models such as DINOv3 have\ndemonstrated strong transferable representation capabilities. In this work, we\nare the first to adapt DINOv3 for ZSAD. However, this adaptation presents two\nkey challenges: (i) the domain bias between large-scale pretraining data and\nanomaly detection tasks leads to feature misalignment; and (ii) the inherent\nbias toward global semantics in pretrained representations often leads to\nsubtle anomalies being misinterpreted as part of the normal foreground objects,\nrather than being distinguished as abnormal regions. To overcome these\nchallenges, we introduce AD-DINOv3, a novel vision-language multimodal\nframework designed for ZSAD. Specifically, we formulate anomaly detection as a\nmultimodal contrastive learning problem, where DINOv3 is employed as the visual\nbackbone to extract patch tokens and a CLS token, and the CLIP text encoder\nprovides embeddings for both normal and abnormal prompts. To bridge the domain\ngap, lightweight adapters are introduced in both modalities, enabling their\nrepresentations to be recalibrated for the anomaly detection task. Beyond this\nbaseline alignment, we further design an Anomaly-Aware Calibration Module\n(AACM), which explicitly guides the CLS token to attend to anomalous regions\nrather than generic foreground semantics, thereby enhancing discriminability.\nExtensive experiments on eight industrial and medical benchmarks demonstrate\nthat AD-DINOv3 consistently matches or surpasses state-of-the-art methods,\nverifying its superiority as a general zero-shot anomaly detection framework.", "AI": {"tldr": "本文首次将DINOv3应用于零样本异常检测（ZSAD），并提出AD-DINOv3框架，通过多模态对比学习、轻量级适配器和异常感知校准模块，解决了领域偏差和全局语义偏差问题，在多个基准测试中取得了SOTA性能。", "motivation": "零样本异常检测（ZSAD）需要可扩展且标注高效的解决方案。传统方法多基于CLIP模型，但DINOv3等视觉基础模型展现出强大的可迁移表示能力。然而，将DINOv3应用于ZSAD面临两大挑战：预训练数据与异常检测任务之间的领域偏差导致特征错位；预训练表示对全局语义的偏向导致细微异常被误判为正常前景。", "method": "本文提出了AD-DINOv3，一个新颖的视觉-语言多模态框架，首次将DINOv3应用于ZSAD。具体方法包括：将异常检测公式化为多模态对比学习问题，DINOv3作为视觉骨干提取补丁和CLS token，CLIP文本编码器提供正常/异常提示的嵌入。为弥合领域差距，在视觉和文本模态中引入轻量级适配器进行表示校准。此外，设计了异常感知校准模块（AACM），显式引导CLS token关注异常区域而非通用前景语义，以增强判别力。", "result": "在八个工业和医学基准测试中，AD-DINOv3始终与或超越了最先进的方法，验证了其作为通用零样本异常检测框架的优越性。", "conclusion": "AD-DINOv3是一个卓越的通用零样本异常检测框架，通过有效解决DINOv3在ZSAD应用中的挑战，显著提升了检测性能。"}}
{"id": "2509.14097", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.14097", "abs": "https://arxiv.org/abs/2509.14097", "authors": ["Yaru Chen", "Ruohao Guo", "Liting Gao", "Yang Xiang", "Qingyu Luo", "Zhenbo Li", "Wenwu Wang"], "title": "Teacher-Guided Pseudo Supervision and Cross-Modal Alignment for Audio-Visual Video Parsing", "comment": null, "summary": "Weakly-supervised audio-visual video parsing (AVVP) seeks to detect audible,\nvisible, and audio-visual events without temporal annotations. Previous work\nhas emphasized refining global predictions through contrastive or collaborative\nlearning, but neglected stable segment-level supervision and class-aware\ncross-modal alignment. To address this, we propose two strategies: (1) an\nexponential moving average (EMA)-guided pseudo supervision framework that\ngenerates reliable segment-level masks via adaptive thresholds or top-k\nselection, offering stable temporal guidance beyond video-level labels; and (2)\na class-aware cross-modal agreement (CMA) loss that aligns audio and visual\nembeddings at reliable segment-class pairs, ensuring consistency across\nmodalities while preserving temporal structure. Evaluations on LLP and UnAV-100\ndatasets shows that our method achieves state-of-the-art (SOTA) performance\nacross multiple metrics.", "AI": {"tldr": "本文提出了一种弱监督音视频解析（AVVP）方法，通过指数移动平均（EMA）引导的伪监督框架和类别感知跨模态一致性（CMA）损失，解决了缺乏时间注释和类别感知跨模态对齐的问题，并在多个数据集上达到了SOTA性能。", "motivation": "以往的弱监督音视频解析（AVVP）工作主要侧重于通过对比或协作学习来优化全局预测，但忽视了稳定的片段级监督和类别感知的跨模态对齐。", "method": "本文提出了两种策略：1) 一个由指数移动平均（EMA）引导的伪监督框架，通过自适应阈值或top-k选择生成可靠的片段级掩码，提供超越视频级标签的稳定时间指导；2) 一个类别感知跨模态一致性（CMA）损失，在可靠的片段-类别对上对齐音频和视觉嵌入，确保模态间的一致性并保留时间结构。", "result": "在LLP和UnAV-100数据集上的评估表明，该方法在多项指标上均达到了最先进（SOTA）的性能。", "conclusion": "所提出的EMA引导的伪监督框架和CMA损失有效解决了弱监督音视频解析中缺乏稳定片段级监督和类别感知跨模态对齐的问题，显著提升了模型性能。"}}
{"id": "2509.14104", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.14104", "abs": "https://arxiv.org/abs/2509.14104", "authors": ["Leonard Hackel", "Tom Burgert", "Begüm Demir"], "title": "CSMoE: An Efficient Remote Sensing Foundation Model with Soft Mixture-of-Experts", "comment": null, "summary": "Self-supervised learning through masked autoencoders has attracted great\nattention for remote sensing (RS) foundation model (FM) development, enabling\nimproved representation learning across diverse sensors and downstream tasks.\nHowever, existing RS FMs often either suffer from substantial computational\ncomplexity during both training and inference or exhibit limited\nrepresentational capacity. These issues restrict their practical applicability\nin RS. To address this limitation, we propose an adaptation for enhancing the\nefficiency of RS FMs by integrating the Soft mixture-of-experts (MoE) mechanism\ninto the FM. The integration of Soft MoEs into the FM allows modality-specific\nexpert specialization alongside shared cross-sensor representation learning. To\ndemonstrate the effectiveness of our adaptation, we apply it on the\nCross-Sensor Masked Autoencoder (CSMAE) model, resulting in the Cross-Sensor\nMixture-of-Experts (CSMoE) model. In addition, we introduce a thematic-climatic\ndescriptor-driven sampling strategy for the construction of a representative\nand diverse training set to train our CSMoE model. Extensive experiments on\nscene classification, semantic segmentation, and content-based image retrieval\ndemonstrate that our adaptation yields a reduction in computational\nrequirements while maintaining or improving representational performance.\nCompared to state-of-the-art RS FMs, CSMoE achieves a superior trade-off\nbetween representational capacity, accuracy, and computational efficiency. On\naverage, CSMoE achieves more than twice the computational efficiency of\nexisting RS FMs, while maintaining competitive performance across all\nexperiments. These results show the effectiveness of the proposed adaptation\nfor creating computationally efficient RS FMs. The code for the model, the\ntraining set creation, and the model weights will be available at\nhttps://git.tu-berlin.de/rsim/csmoe.", "AI": {"tldr": "该研究提出了一种将Soft MoE机制集成到遥感基础模型（RS FMs）中的方法，以提高其计算效率和表示能力，同时保持或提升性能，从而解决了现有RS FMs计算复杂性高或表示能力有限的问题。", "motivation": "现有的遥感基础模型（RS FMs）在训练和推理过程中往往计算成本高昂，或者表示能力有限，这限制了它们在遥感领域的实际应用。因此，需要一种方法来提高这些模型的效率。", "method": "研究通过将Soft mixture-of-experts (MoE) 机制集成到基础模型中，增强了RS FMs的效率。这种集成允许模型在共享跨传感器表示学习的同时，实现模态特定的专家特化。具体地，他们将此适应性应用于Cross-Sensor Masked Autoencoder (CSMAE) 模型，创建了Cross-Sensor Mixture-of-Experts (CSMoE) 模型。此外，他们还引入了一种基于主题-气候描述符的采样策略来构建代表性强且多样化的训练数据集。", "result": "CSMoE模型在场景分类、语义分割和基于内容的图像检索等任务上进行了广泛实验。结果表明，该适应性在降低计算需求的同时，保持或提高了表示性能。与最先进的RS FMs相比，CSMoE在表示能力、准确性和计算效率之间取得了更优的平衡。平均而言，CSMoE的计算效率是现有RS FMs的两倍以上，同时在所有实验中保持了具有竞争力的性能。", "conclusion": "该研究提出的将Soft MoE机制集成到遥感基础模型中的适应性方法是有效的，能够创建计算效率更高的遥感基础模型，同时保持或提升其表示性能和准确性，从而解决了现有模型在实际应用中的局限性。"}}
{"id": "2509.14119", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.14119", "abs": "https://arxiv.org/abs/2509.14119", "authors": ["Jiabo MA", "Wenqiang Li", "Jinbang Li", "Ziyi Liu", "Linshan Wu", "Fengtao Zhou", "Li Liang", "Ronald Cheong Kin Chan", "Terence T. W. Wong", "Hao Chen"], "title": "Generative AI for Misalignment-Resistant Virtual Staining to Accelerate Histopathology Workflows", "comment": "the arxiv version of the under review journal paper", "summary": "Accurate histopathological diagnosis often requires multiple differently\nstained tissue sections, a process that is time-consuming, labor-intensive, and\nenvironmentally taxing due to the use of multiple chemical stains. Recently,\nvirtual staining has emerged as a promising alternative that is faster,\ntissue-conserving, and environmentally friendly. However, existing virtual\nstaining methods face significant challenges in clinical applications,\nprimarily due to their reliance on well-aligned paired data. Obtaining such\ndata is inherently difficult because chemical staining processes can distort\ntissue structures, and a single tissue section cannot undergo multiple staining\nprocedures without damage or loss of information. As a result, most available\nvirtual staining datasets are either unpaired or roughly paired, making it\ndifficult for existing methods to achieve accurate pixel-level supervision. To\naddress this challenge, we propose a robust virtual staining framework\nfeaturing cascaded registration mechanisms to resolve spatial mismatches\nbetween generated outputs and their corresponding ground truth. Experimental\nresults demonstrate that our method significantly outperforms state-of-the-art\nmodels across five datasets, achieving an average improvement of 3.2% on\ninternal datasets and 10.1% on external datasets. Moreover, in datasets with\nsubstantial misalignment, our approach achieves a remarkable 23.8% improvement\nin peak signal-to-noise ratio compared to baseline models. The exceptional\nrobustness of the proposed method across diverse datasets simplifies the data\nacquisition process for virtual staining and offers new insights for advancing\nits development.", "AI": {"tldr": "针对虚拟染色中配对数据对齐困难的问题，本文提出了一种具有级联配准机制的鲁棒虚拟染色框架，显著优于现有方法，提高了数据获取的简易性并推动了该领域的发展。", "motivation": "传统的组织病理学诊断耗时、费力且不环保。虚拟染色虽有前景，但现有方法依赖于精确对齐的配对数据。然而，化学染色过程会扭曲组织结构，且单个组织切片无法多次染色，导致难以获得完美对齐的数据，现有方法难以实现准确的像素级监督。", "method": "提出了一种鲁棒的虚拟染色框架，该框架采用级联配准机制来解决生成输出与对应真实值之间的空间错位问题。", "result": "该方法在五个数据集上显著优于现有最先进模型，内部数据集平均提升3.2%，外部数据集平均提升10.1%。在存在显著错位的数据集中，峰值信噪比(PSNR)相比基线模型提升了23.8%。", "conclusion": "所提出的方法在多样化数据集上的卓越鲁棒性简化了虚拟染色数据获取过程，并为推动其发展提供了新见解。"}}
{"id": "2509.14120", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.14120", "abs": "https://arxiv.org/abs/2509.14120", "authors": ["Sara Concas", "Simone Maurizio La Cava", "Andrea Panzino", "Ester Masala", "Giulia Orrù", "Gian Luca Marcialis"], "title": "Deceptive Beauty: Evaluating the Impact of Beauty Filters on Deepfake and Morphing Attack Detection", "comment": "Accepted at the 2025 IEEE INTERNATIONAL CONFERENCE ON Metrology for\n  eXtended Reality, Artificial Intelligence and Neural Engineering", "summary": "Digital beautification through social media filters has become increasingly\npopular, raising concerns about the reliability of facial images and videos and\nthe effectiveness of automated face analysis. This issue is particularly\ncritical for digital manipulation detectors, systems aiming at distinguishing\nbetween genuine and manipulated data, especially in cases involving deepfakes\nand morphing attacks designed to deceive humans and automated facial\nrecognition. This study examines whether beauty filters impact the performance\nof deepfake and morphing attack detectors. We perform a comprehensive analysis,\nevaluating multiple state-of-the-art detectors on benchmark datasets before and\nafter applying various smoothing filters. Our findings reveal performance\ndegradation, highlighting vulnerabilities introduced by facial enhancements and\nunderscoring the need for robust detection models resilient to such\nalterations.", "AI": {"tldr": "研究发现社交媒体美颜滤镜会降低现有深度伪造和活体攻击检测器的性能，凸显了开发更鲁棒检测模型的必要性。", "motivation": "社交媒体美颜滤镜的普及引发了对面部图像和视频可靠性以及自动化人脸分析有效性的担忧。这对于旨在区分真实与伪造数据的数字操纵检测器（尤其是针对深度伪造和活体攻击的系统）而言尤为关键。", "method": "本研究通过对基准数据集应用多种平滑滤镜，然后评估多个最先进的深度伪造和活体攻击检测器在应用滤镜前后的性能，进行了全面的分析。", "result": "研究结果表明，检测器性能出现下降，揭示了面部美化引入的漏洞。", "conclusion": "研究强调了对能够抵御此类改变的鲁棒检测模型的需求。"}}
{"id": "2509.14142", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.14142", "abs": "https://arxiv.org/abs/2509.14142", "authors": ["Peng Xu", "Shengwu Xiong", "Jiajun Zhang", "Yaxiong Chen", "Bowen Zhou", "Chen Change Loy", "David A. Clifton", "Kyoung Mu Lee", "Luc Van Gool", "Ruiming He", "Ruilin Yao", "Xinwei Long", "Jirui Huang", "Kai Tian", "Sa Yang", "Yihua Shao", "Jin Feng", "Yue Zhong", "Jiakai Zhou", "Cheng Tang", "Tianyu Zou", "Yifang Zhang", "Junming Liang", "Guoyou Li", "Zhaoxiang Wang", "Qiang Zhou", "Yichen Zhao", "Shili Xiong", "Hyeongjin Nam", "Jaerin Lee", "Jaeyoung Chung", "JoonKyu Park", "Junghun Oh", "Kanggeon Lee", "Wooseok Lee", "Juneyoung Ro", "Turghun Osman", "Can Hu", "Chaoyang Liao", "Cheng Chen", "Chengcheng Han", "Chenhao Qiu", "Chong Peng", "Cong Xu", "Dailin Li", "Feiyu Wang", "Feng Gao", "Guibo Zhu", "Guopeng Tang", "Haibo Lu", "Han Fang", "Han Qi", "Hanxiao Wu", "Haobo Cheng", "Hongbo Sun", "Hongyao Chen", "Huayong Hu", "Hui Li", "Jiaheng Ma", "Jiang Yu", "Jianing Wang", "Jie Yang", "Jing He", "Jinglin Zhou", "Jingxuan Li", "Josef Kittler", "Lihao Zheng", "Linnan Zhao", "Mengxi Jia", "Muyang Yan", "Nguyen Thanh Thien", "Pu Luo", "Qi Li", "Shien Song", "Shijie Dong", "Shuai Shao", "Shutao Li", "Taofeng Xue", "Tianyang Xu", "Tianyi Gao", "Tingting Li", "Wei Zhang", "Weiyang Su", "Xiaodong Dong", "Xiao-Jun Wu", "Xiaopeng Zhou", "Xin Chen", "Xin Wei", "Xinyi You", "Xudong Kang", "Xujie Zhou", "Xusheng Liu", "Yanan Wang", "Yanbin Huang", "Yang Liu", "Yang Yang", "Yanglin Deng", "Yashu Kang", "Ye Yuan", "Yi Wen", "Yicen Tian", "Yilin Tao", "Yin Tang", "Yipeng Lin", "Yiqing Wang", "Yiting Xi", "Yongkang Yu", "Yumei Li", "Yuxin Qin", "Yuying Chen", "Yuzhe Cen", "Zhaofan Zou", "Zhaohong Liu", "Zhehao Shen", "Zhenglin Du", "Zhengyang Li", "Zhenni Huang", "Zhenwei Shao", "Zhilong Song", "Zhiyong Feng", "Zhiyu Wang", "Zhou Yu", "Ziang Li", "Zihan Zhai", "Zijian Zhang", "Ziyang Peng", "Ziyun Xiao", "Zongshu Li"], "title": "MARS2 2025 Challenge on Multimodal Reasoning: Datasets, Methods, Results, Discussion, and Outlook", "comment": "ICCV 2025 MARS2 Workshop and Challenge \"Multimodal Reasoning and Slow\n  Thinking in the Large Model Era: Towards System 2 and Beyond''", "summary": "This paper reviews the MARS2 2025 Challenge on Multimodal Reasoning. We aim\nto bring together different approaches in multimodal machine learning and LLMs\nvia a large benchmark. We hope it better allows researchers to follow the\nstate-of-the-art in this very dynamic area. Meanwhile, a growing number of\ntestbeds have boosted the evolution of general-purpose large language models.\nThus, this year's MARS2 focuses on real-world and specialized scenarios to\nbroaden the multimodal reasoning applications of MLLMs. Our organizing team\nreleased two tailored datasets Lens and AdsQA as test sets, which support\ngeneral reasoning in 12 daily scenarios and domain-specific reasoning in\nadvertisement videos, respectively. We evaluated 40+ baselines that include\nboth generalist MLLMs and task-specific models, and opened up three competition\ntracks, i.e., Visual Grounding in Real-world Scenarios (VG-RS), Visual Question\nAnswering with Spatial Awareness (VQA-SA), and Visual Reasoning in Creative\nAdvertisement Videos (VR-Ads). Finally, 76 teams from the renowned academic and\nindustrial institutions have registered and 40+ valid submissions (out of\n1200+) have been included in our ranking lists. Our datasets, code sets (40+\nbaselines and 15+ participants' methods), and rankings are publicly available\non the MARS2 workshop website and our GitHub organization page\nhttps://github.com/mars2workshop/, where our updates and announcements of\nupcoming events will be continuously provided.", "AI": {"tldr": "本文回顾了MARS2 2025多模态推理挑战赛，旨在通过大型基准测试汇集多模态机器学习和LLM的不同方法，并专注于真实世界和专业场景的应用，发布了Lens和AdsQA数据集，评估了40多个基线模型，并吸引了76支团队参与。", "motivation": "组织本次挑战赛的动机是为了汇集多模态机器学习和大型语言模型（LLM）的不同方法，通过大型基准测试帮助研究人员追踪该动态领域的前沿进展。同时，鉴于通用大型语言模型（LLMs）的快速发展，挑战赛旨在拓宽多模态大语言模型（MLLMs）在真实世界和专业场景中的应用。", "method": "挑战赛发布了两个定制数据集：Lens（支持12个日常场景的通用推理）和AdsQA（支持广告视频中的领域特定推理）。组织者评估了包括通用MLLMs和任务特定模型在内的40多个基线模型，并设立了三个竞赛赛道：真实世界场景中的视觉定位（VG-RS）、具有空间感知能力的视觉问答（VQA-SA）以及创意广告视频中的视觉推理（VR-Ads）。所有数据集、代码和排名均公开可用。", "result": "挑战赛吸引了来自知名学术和工业机构的76支团队注册参与，共收到1200多次提交，其中40多个有效提交被纳入排名列表。挑战赛的数据集、代码集（40多个基线和15个以上参与者的方法）和排名已在MARS2研讨会网站和GitHub页面上公开。", "conclusion": "MARS2 2025挑战赛成功地汇集了多模态推理领域的研究力量，通过定制数据集和多赛道设置，促进了MLLMs在真实世界和专业场景应用中的发展。挑战赛的公开资源将持续推动该领域的进步和社区参与。"}}
{"id": "2509.14149", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.14149", "abs": "https://arxiv.org/abs/2509.14149", "authors": ["Haotian Li", "Jianbo Jiao"], "title": "An Exploratory Study on Abstract Images and Visual Representations Learned from Them", "comment": "Accepted to BMVC 2025", "summary": "Imagine living in a world composed solely of primitive shapes, could you\nstill recognise familiar objects? Recent studies have shown that abstract\nimages-constructed by primitive shapes-can indeed convey visual semantic\ninformation to deep learning models. However, representations obtained from\nsuch images often fall short compared to those derived from traditional raster\nimages. In this paper, we study the reasons behind this performance gap and\ninvestigate how much high-level semantic content can be captured at different\nabstraction levels. To this end, we introduce the Hierarchical Abstraction\nImage Dataset (HAID), a novel data collection that comprises abstract images\ngenerated from normal raster images at multiple levels of abstraction. We then\ntrain and evaluate conventional vision systems on HAID across various tasks\nincluding classification, segmentation, and object detection, providing a\ncomprehensive study between rasterised and abstract image representations. We\nalso discuss if the abstract image can be considered as a potentially effective\nformat for conveying visual semantic information and contributing to vision\ntasks.", "AI": {"tldr": "本文研究了抽象图像与传统栅格图像在深度学习任务中性能差距的原因，并引入了分层抽象图像数据集（HAID），通过在分类、分割和目标检测等任务上的评估，探讨了抽象图像作为视觉语义信息载体的潜力。", "motivation": "先前的研究表明，由原始形状构建的抽象图像能向深度学习模型传达视觉语义信息，但其表示效果通常不如传统栅格图像。本文旨在探究造成这种性能差距的原因，并研究在不同抽象级别上能捕获多少高层语义内容。", "method": "引入了分层抽象图像数据集（HAID），该数据集包含从正常栅格图像在多个抽象级别生成的抽象图像。然后，在HAID上训练并评估了传统的视觉系统，涵盖了分类、分割和目标检测等多种任务。", "result": "提供了一项关于栅格化图像和抽象图像表示之间在各种任务上的全面研究。结果讨论了抽象图像是否可以被视为一种潜在有效的格式，用于传达视觉语义信息并有助于视觉任务。", "conclusion": "抽象图像可以被视为一种潜在有效的格式，用于传达视觉语义信息并有助于视觉任务，但需要进一步研究以弥补与传统栅格图像的性能差距。"}}
{"id": "2509.14151", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.14151", "abs": "https://arxiv.org/abs/2509.14151", "authors": ["Rongyu Zhang", "Jiaming Liu", "Xiaoqi Li", "Xiaowei Chi", "Dan Wang", "Li Du", "Yuan Du", "Shanghang Zhang"], "title": "BEVUDA++: Geometric-aware Unsupervised Domain Adaptation for Multi-View 3D Object Detection", "comment": "Accepted by IEEE TCSVT", "summary": "Vision-centric Bird's Eye View (BEV) perception holds considerable promise\nfor autonomous driving. Recent studies have prioritized efficiency or accuracy\nenhancements, yet the issue of domain shift has been overlooked, leading to\nsubstantial performance degradation upon transfer. We identify major domain\ngaps in real-world cross-domain scenarios and initiate the first effort to\naddress the Domain Adaptation (DA) challenge in multi-view 3D object detection\nfor BEV perception. Given the complexity of BEV perception approaches with\ntheir multiple components, domain shift accumulation across multi-geometric\nspaces (e.g., 2D, 3D Voxel, BEV) poses a significant challenge for BEV domain\nadaptation. In this paper, we introduce an innovative geometric-aware\nteacher-student framework, BEVUDA++, to diminish this issue, comprising a\nReliable Depth Teacher (RDT) and a Geometric Consistent Student (GCS) model.\nSpecifically, RDT effectively blends target LiDAR with dependable depth\npredictions to generate depth-aware information based on uncertainty\nestimation, enhancing the extraction of Voxel and BEV features that are\nessential for understanding the target domain. To collaboratively reduce the\ndomain shift, GCS maps features from multiple spaces into a unified geometric\nembedding space, thereby narrowing the gap in data distribution between the two\ndomains. Additionally, we introduce a novel Uncertainty-guided Exponential\nMoving Average (UEMA) to further reduce error accumulation due to domain shifts\ninformed by previously obtained uncertainty guidance. To demonstrate the\nsuperiority of our proposed method, we execute comprehensive experiments in\nfour cross-domain scenarios, securing state-of-the-art performance in BEV 3D\nobject detection tasks, e.g., 12.9\\% NDS and 9.5\\% mAP enhancement on Day-Night\nadaptation.", "AI": {"tldr": "本文提出BEVUDA++，一个几何感知的师生框架，旨在解决自动驾驶中BEV感知跨域场景下的域偏移问题，通过可靠深度教师和几何一致性学生模型显著提升了3D目标检测性能。", "motivation": "现有BEV感知研究忽视了域偏移问题，导致跨域场景下性能显著下降。BEV感知方法组件复杂，多几何空间（2D、3D体素、BEV）中的域偏移累积带来了巨大挑战。因此，需要首次解决多视角3D目标检测中的域适应问题。", "method": "引入BEVUDA++，一个几何感知的师生框架。它包含：1) 可靠深度教师（RDT），融合目标LiDAR和深度预测，基于不确定性估计生成深度感知信息，增强体素和BEV特征提取。2) 几何一致性学生（GCS），将多空间特征映射到统一几何嵌入空间，缩小数据分布差距。此外，引入不确定性引导的指数移动平均（UEMA），进一步减少域偏移引起的误差累积。", "result": "在四个跨域场景中进行了全面实验，BEVUDA++取得了BEV 3D目标检测任务的最新性能，例如在昼夜适应中，NDS提升12.9%，mAP提升9.5%。", "conclusion": "所提出的BEVUDA++框架通过几何感知和师生学习，有效解决了BEV感知中的域偏移挑战，显著提升了在真实世界跨域场景下3D目标检测的性能，达到了最先进水平。"}}
{"id": "2509.14227", "categories": ["cs.CV", "I.2.10; I.2.7"], "pdf": "https://arxiv.org/pdf/2509.14227", "abs": "https://arxiv.org/abs/2509.14227", "authors": ["Nisarg A. Shah", "Amir Ziai", "Chaitanya Ekanadham", "Vishal M. Patel"], "title": "Cinéaste: A Fine-grained Contextual Movie Question Answering Benchmark", "comment": "11 pages, 5 figures, 5 tables", "summary": "While recent advancements in vision-language models have improved video\nunderstanding, diagnosing their capacity for deep, narrative comprehension\nremains a challenge. Existing benchmarks often test short-clip recognition or\nuse template-based questions, leaving a critical gap in evaluating fine-grained\nreasoning over long-form narrative content. To address these gaps, we introduce\n$\\mathsf{Cin\\acute{e}aste}$, a comprehensive benchmark for long-form movie\nunderstanding. Our dataset comprises 3,119 multiple-choice question-answer\npairs derived from 1,805 scenes across 200 diverse movies, spanning five novel\nfine-grained contextual reasoning categories. We use GPT-4o to generate\ndiverse, context-rich questions by integrating visual descriptions, captions,\nscene titles, and summaries, which require deep narrative understanding. To\nensure high-quality evaluation, our pipeline incorporates a two-stage filtering\nprocess: Context-Independence filtering ensures questions require video\ncontext, while Contextual Veracity filtering validates factual consistency\nagainst the movie content, mitigating hallucinations. Experiments show that\nexisting MLLMs struggle on $\\mathsf{Cin\\acute{e}aste}$; our analysis reveals\nthat long-range temporal reasoning is a primary bottleneck, with the top\nopen-source model achieving only 63.15\\% accuracy. This underscores significant\nchallenges in fine-grained contextual understanding and the need for\nadvancements in long-form movie comprehension.", "AI": {"tldr": "解析错误", "motivation": "解析错误", "method": "解析错误", "result": "解析错误", "conclusion": "解析错误"}}
{"id": "2509.14232", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.14232", "abs": "https://arxiv.org/abs/2509.14232", "authors": ["Zhaokai Wang", "Penghao Yin", "Xiangyu Zhao", "Changyao Tian", "Yu Qiao", "Wenhai Wang", "Jifeng Dai", "Gen Luo"], "title": "GenExam: A Multidisciplinary Text-to-Image Exam", "comment": null, "summary": "Exams are a fundamental test of expert-level intelligence and require\nintegrated understanding, reasoning, and generation. Existing exam-style\nbenchmarks mainly focus on understanding and reasoning tasks, and current\ngeneration benchmarks emphasize the illustration of world knowledge and visual\nconcepts, neglecting the evaluation of rigorous drawing exams. We introduce\nGenExam, the first benchmark for multidisciplinary text-to-image exams,\nfeaturing 1,000 samples across 10 subjects with exam-style prompts organized\nunder a four-level taxonomy. Each problem is equipped with ground-truth images\nand fine-grained scoring points to enable a precise evaluation of semantic\ncorrectness and visual plausibility. Experiments show that even\nstate-of-the-art models such as GPT-Image-1 and Gemini-2.5-Flash-Image achieve\nless than 15% strict scores, and most models yield almost 0%, suggesting the\ngreat challenge of our benchmark. By framing image generation as an exam,\nGenExam offers a rigorous assessment of models' ability to integrate knowledge,\nreasoning, and generation, providing insights on the path to general AGI.", "AI": {"tldr": "GenExam是首个多学科文本到图像考试基准，旨在严格评估模型在理解、推理和生成方面的综合能力。它包含10个学科的1,000个样本，并采用细粒度评分，发现当前最先进的模型表现不佳。", "motivation": "现有考试风格基准主要侧重于理解和推理，而当前的生成基准则强调世界知识和视觉概念，但都忽略了对严谨绘画考试的评估。考试是衡量专家级智能的基本方式，需要综合理解、推理和生成能力。", "method": "引入了GenExam，这是第一个多学科文本到图像考试基准。它包含10个学科的1,000个样本，采用考试风格的提示，并按四级分类法组织。每个问题都配备了真实图像和细粒度评分点，以实现对语义正确性和视觉合理性的精确评估。", "result": "实验表明，即使是GPT-Image-1和Gemini-2.5-Flash-Image等最先进的模型也仅能获得不到15%的严格分数，而大多数模型几乎得分为0%，这表明该基准带来了巨大的挑战。", "conclusion": "GenExam通过将图像生成构建为考试，对模型整合知识、推理和生成的能力进行了严格评估，为实现通用AGI（通用人工智能）提供了深入的见解。"}}
