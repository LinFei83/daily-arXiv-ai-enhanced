<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 22]
- [cs.CV](#cs.CV) [Total: 99]
- [cs.CL](#cs.CL) [Total: 77]
- [cs.RO](#cs.RO) [Total: 30]
- [eess.SY](#eess.SY) [Total: 20]
- [eess.IV](#eess.IV) [Total: 6]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [From Literal to Liberal: A Meta-Prompting Framework for Eliciting Human-Aligned Exception Handling in Large Language Models](https://arxiv.org/abs/2510.12864)
*Imran Khan*

Main category: cs.AI

TL;DR: 大型语言模型（LLMs）存在“规则刚性”问题，导致决策与人类常识不符。本文提出Rule-Intent Distinction (RID) 框架，一种低成本元提示技术，通过提供结构化认知模式，显著提升LLMs在零样本场景下处理例外情况的能力，使其决策更符合人类意图。


<details>
  <summary>Details</summary>
Motivation: LLMs作为AI代理系统的推理引擎，其对明确规则的僵化遵守导致决策与人类常识和意图不符，即“规则刚性”，这严重阻碍了可信赖自主代理的构建。现有通过人类解释进行SFT的方法成本高昂且不易普及，因此需要一种低计算成本、易于访问的解决方案。

Method: 本文引入了Rule-Intent Distinction (RID) 框架，这是一种新颖、低计算成本的元提示技术，旨在零样本地引导LLMs进行与人类对齐的例外处理。RID框架为模型提供了一个结构化的认知模式，用于解构任务、分类规则、权衡冲突结果并证明其最终决策。研究人员在一个包含20个需要细致判断的定制基准场景上，将RID框架与基线提示和思维链（CoT）提示进行了评估。

Result: 人类验证的结果表明，RID框架显著提高了LLMs的性能，实现了95%的人类对齐分数（HAS），远高于基线提示的80%和CoT提示的75%。此外，RID框架持续产生更高质量、意图驱动的推理。

Conclusion: 这项工作提出了一种实用、易于访问且有效的方法，能够引导LLMs从字面上的指令遵循转向自由、以目标为导向的推理，为构建更可靠和实用的AI代理铺平了道路。

Abstract: Large Language Models (LLMs) are increasingly being deployed as the reasoning
engines for agentic AI systems, yet they exhibit a critical flaw: a rigid
adherence to explicit rules that leads to decisions misaligned with human
common sense and intent. This "rule-rigidity" is a significant barrier to
building trustworthy autonomous agents. While prior work has shown that
supervised fine-tuning (SFT) with human explanations can mitigate this issue,
SFT is computationally expensive and inaccessible to many practitioners. To
address this gap, we introduce the Rule-Intent Distinction (RID) Framework, a
novel, low-compute meta-prompting technique designed to elicit human-aligned
exception handling in LLMs in a zero-shot manner. The RID framework provides
the model with a structured cognitive schema for deconstructing tasks,
classifying rules, weighing conflicting outcomes, and justifying its final
decision. We evaluated the RID framework against baseline and Chain-of-Thought
(CoT) prompting on a custom benchmark of 20 scenarios requiring nuanced
judgment across diverse domains. Our human-verified results demonstrate that
the RID framework significantly improves performance, achieving a 95% Human
Alignment Score (HAS), compared to 80% for the baseline and 75% for CoT.
Furthermore, it consistently produces higher-quality, intent-driven reasoning.
This work presents a practical, accessible, and effective method for steering
LLMs from literal instruction-following to liberal, goal-oriented reasoning,
paving the way for more reliable and pragmatic AI agents.

</details>


### [2] [DeepPlanner: Scaling Planning Capability for Deep Research Agents via Advantage Shaping](https://arxiv.org/abs/2510.12979)
*Wei Fan,Wenlin Yao,Zheng Li,Feng Yao,Xin Liu,Liang Qiu,Qingyu Yin,Yangqiu Song,Bing Yin*

Main category: cs.AI

TL;DR: DeepPlanner是一个端到端的强化学习框架，通过基于熵的优势塑造和对规划密集型样本的加权，有效优化了大型语言模型（LLMs）的规划能力，在降低训练成本的同时实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM工具使用方法在规划阶段存在不足，要么依赖隐式规划，要么引入显式规划器但未系统优化。研究发现，在标准强化学习下，规划令牌的熵值显著高于其他动作令牌，表明决策点不确定且优化不足。

Method: 本文提出了DeepPlanner，一个端到端的强化学习框架。它通过以下方式增强深度研究代理的规划能力：1) 使用基于熵的项来塑造令牌级优势，为高熵令牌分配更大的更新；2) 有选择地提高规划密集型样本的样本级优势。

Result: 在七个深度研究基准测试中，DeepPlanner显著提高了规划质量，并在大幅降低训练预算的情况下实现了最先进（state-of-the-art）的结果。

Conclusion: DeepPlanner通过其创新的强化学习方法，有效地提升了深度研究代理的规划能力，解决了现有LLM工具使用中规划不足的问题，并带来了性能和效率的双重提升。

Abstract: Large language models (LLMs) augmented with multi-step reasoning and action
generation abilities have shown promise in leveraging external tools to tackle
complex tasks that require long-horizon planning. However, existing approaches
either rely on implicit planning in the reasoning stage or introduce explicit
planners without systematically addressing how to optimize the planning stage.
As evidence, we observe that under vanilla reinforcement learning (RL),
planning tokens exhibit significantly higher entropy than other action tokens,
revealing uncertain decision points that remain under-optimized. To address
this, we propose DeepPlanner, an end-to-end RL framework that effectively
enhances the planning capabilities of deep research agents. Our approach shapes
token-level advantage with an entropy-based term to allocate larger updates to
high entropy tokens, and selectively upweights sample-level advantages for
planning-intensive rollouts. Extensive experiments across seven deep research
benchmarks demonstrate that DeepPlanner improves planning quality and achieves
state-of-the-art results under a substantially lower training budget.

</details>


### [3] [SENTINEL: A Multi-Level Formal Framework for Safety Evaluation of LLM-based Embodied Agents](https://arxiv.org/abs/2510.12985)
*Simon Sinong Zhan,Yao Liu,Philip Wang,Zinan Wang,Qineng Wang,Zhian Ruan,Xiangyu Shi,Xinyu Cao,Frank Yang,Kangrui Wang,Huajie Shao,Manling Li,Qi Zhu*

Main category: cs.AI

TL;DR: Sentinel是一个用于正式评估基于大型语言模型(LLM)的具身智能体在语义、规划和轨迹层面物理安全性的框架，它通过时序逻辑(TL)将安全要求形式化，并采用多级验证流水线来检测不安全行为。


<details>
  <summary>Details</summary>
Motivation: 现有的具身智能体安全评估方法依赖启发式规则或主观的LLM判断，缺乏严谨性。研究动机在于需要一个能将实际安全要求基于形式化时序逻辑，并能系统地评估LLM具身智能体物理安全性的框架。

Method: Sentinel将实际安全要求基于形式化时序逻辑(TL)语义。它采用多级验证流水线：(i) 语义层面，将自然语言安全要求形式化为TL公式，并探测LLM智能体对这些要求的理解与TL公式的一致性；(ii) 规划层面，验证LLM智能体生成的高级行动计划和子目标是否符合TL公式，以在执行前检测不安全计划；(iii) 轨迹层面，将多个执行轨迹合并为计算树，并针对物理细节的TL规范进行高效验证，进行最终安全检查。

Result: 实验结果表明，通过将物理安全基于时序逻辑并在多级应用验证方法，Sentinel为系统评估LLM具身智能体在物理环境中的安全性提供了严格的基础。它揭示了先前方法忽视的安全违规，并提供了对其故障模式的深入见解。该框架已在VirtualHome和ALFRED中应用。

Conclusion: Sentinel通过将物理安全要求形式化为时序逻辑并在语义、规划和轨迹层面进行多级验证，为LLM具身智能体的物理安全性评估提供了一个严谨且系统化的新方法，能够更有效地发现安全问题并分析其原因，优于现有方法。

Abstract: We present Sentinel, the first framework for formally evaluating the physical
safety of Large Language Model(LLM-based) embodied agents across the semantic,
plan, and trajectory levels. Unlike prior methods that rely on heuristic rules
or subjective LLM judgments, Sentinel grounds practical safety requirements in
formal temporal logic (TL) semantics that can precisely specify state
invariants, temporal dependencies, and timing constraints. It then employs a
multi-level verification pipeline where (i) at the semantic level, intuitive
natural language safety requirements are formalized into TL formulas and the
LLM agent's understanding of these requirements is probed for alignment with
the TL formulas; (ii) at the plan level, high-level action plans and subgoals
generated by the LLM agent are verified against the TL formulas to detect
unsafe plans before execution; and (iii) at the trajectory level, multiple
execution trajectories are merged into a computation tree and efficiently
verified against physically-detailed TL specifications for a final safety
check. We apply Sentinel in VirtualHome and ALFRED, and formally evaluate
multiple LLM-based embodied agents against diverse safety requirements. Our
experiments show that by grounding physical safety in temporal logic and
applying verification methods across multiple levels, Sentinel provides a
rigorous foundation for systematically evaluating LLM-based embodied agents in
physical environments, exposing safety violations overlooked by previous
methods and offering insights into their failure modes.

</details>


### [4] [From Narratives to Probabilistic Reasoning: Predicting and Interpreting Drivers' Hazardous Actions in Crashes Using Large Language Model](https://arxiv.org/abs/2510.13002)
*Boyou Chen,Gerui Xu,Zifei Wang,Huizhong Guo,Ananna Ahmed,Zhaonan Sun,Zhen Hu,Kaihan Zhang,Shan Bao*

Main category: cs.AI

TL;DR: 本研究提出一个创新框架，利用微调的大型语言模型自动从交通事故文本叙述中推断驾驶员危险行为（DHA），显著提高了DHA分类的有效性和可解释性，并超越了传统机器学习模型。


<details>
  <summary>Details</summary>
Motivation: 交通事故中驾驶员危险行为（DHA）的识别对于理解事故原因至关重要。然而，现有大规模数据库中的DHA数据可靠性受限于不一致且劳动密集型的人工编码实践。双车事故占所有道路事故的约70%，对交通安全构成重大挑战，因此需要更可靠、自动化的DHA识别方法。

Method: 研究团队使用MTCF五年间的双车事故数据，对Llama 3.2 1B模型进行了微调，使其能够处理详细的事故文本叙述。该模型表现与包括随机森林、XGBoost、CatBoost和神经网络在内的传统机器学习分类器进行了基准测试。此外，为提高可解释性，开发了一种概率推理方法，通过分析模型在原始测试集和三种目标反事实情景（驾驶员分心和年龄变化）下的输出变化来解释结果。

Result: 微调后的LLM在DHA自动识别上取得了80%的整体准确率，超越了所有基线模型，并在数据不平衡场景中表现出显著改进。概率推理分析揭示，一名驾驶员分心会大幅增加“一般不安全驾驶”的可能性；两名驾驶员都分心会最大化“两名驾驶员都采取了危险行为”的概率；青少年驾驶员会显著提高“超速和停车违规”的概率。

Conclusion: 本框架和分析方法为大规模自动化DHA检测提供了一个鲁棒且可解释的解决方案，为交通安全分析和干预提供了新的机会。

Abstract: Vehicle crashes involve complex interactions between road users, split-second
decisions, and challenging environmental conditions. Among these, two-vehicle
crashes are the most prevalent, accounting for approximately 70% of roadway
crashes and posing a significant challenge to traffic safety. Identifying
Driver Hazardous Action (DHA) is essential for understanding crash causation,
yet the reliability of DHA data in large-scale databases is limited by
inconsistent and labor-intensive manual coding practices. Here, we present an
innovative framework that leverages a fine-tuned large language model to
automatically infer DHAs from textual crash narratives, thereby improving the
validity and interpretability of DHA classifications. Using five years of
two-vehicle crash data from MTCF, we fine-tuned the Llama 3.2 1B model on
detailed crash narratives and benchmarked its performance against conventional
machine learning classifiers, including Random Forest, XGBoost, CatBoost, and a
neural network. The fine-tuned LLM achieved an overall accuracy of 80%,
surpassing all baseline models and demonstrating pronounced improvements in
scenarios with imbalanced data. To increase interpretability, we developed a
probabilistic reasoning approach, analyzing model output shifts across original
test sets and three targeted counterfactual scenarios: variations in driver
distraction and age. Our analysis revealed that introducing distraction for one
driver substantially increased the likelihood of "General Unsafe Driving";
distraction for both drivers maximized the probability of "Both Drivers Took
Hazardous Actions"; and assigning a teen driver markedly elevated the
probability of "Speed and Stopping Violations." Our framework and analytical
methods provide a robust and interpretable solution for large-scale automated
DHA detection, offering new opportunities for traffic safety analysis and
intervention.

</details>


### [5] [Toward Reasoning-Centric Time-Series Analysis](https://arxiv.org/abs/2510.13029)
*Xinlei Wang,Mingtian Tan,Jing Qiu,Junhua Zhao,Jinjin Gu*

Main category: cs.AI

TL;DR: 本文提出利用大型语言模型（LLMs）将时间序列分析重新定义为一项推理任务，强调因果结构和可解释性，而非仅仅数值回归，以应对复杂现实世界的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列分析依赖静态基准和模式识别，难以应对政策变化、人类行为适应和突发事件等动态现实世界情境。现有基于LLMs的方法多停留在数值回归，忽视了LLMs深层的推理潜力。

Method: 本文主张将时间序列分析与LLMs结合，视为一项推理任务。这种方法优先考虑因果结构和可解释性，旨在超越表面趋势，揭示驱动事件的实际力量，并整合多模态输入。

Result: 通过将时间序列分析重新定义为推理任务，并强调因果结构和可解释性，可以使分析更贴近人类理解，在复杂的现实环境中提供透明且情境感知的洞察。

Conclusion: 利用LLMs进行时间序列分析应侧重于其推理能力，优先考虑因果结构和可解释性，以实现对复杂现实世界更深入、更具人类对齐的理解和洞察。

Abstract: Traditional time series analysis has long relied on pattern recognition,
trained on static and well-established benchmarks. However, in real-world
settings -- where policies shift, human behavior adapts, and unexpected events
unfold -- effective analysis must go beyond surface-level trends to uncover the
actual forces driving them. The recent rise of Large Language Models (LLMs)
presents new opportunities for rethinking time series analysis by integrating
multimodal inputs. However, as the use of LLMs becomes popular, we must remain
cautious, asking why we use LLMs and how to exploit them effectively. Most
existing LLM-based methods still employ their numerical regression ability and
ignore their deeper reasoning potential. This paper argues for rethinking time
series with LLMs as a reasoning task that prioritizes causal structure and
explainability. This shift brings time series analysis closer to human-aligned
understanding, enabling transparent and context-aware insights in complex
real-world environments.

</details>


### [6] [Repairing Reward Functions with Human Feedback to Mitigate Reward Hacking](https://arxiv.org/abs/2510.13036)
*Stephane Hatgis-Kessell,Logan Mondal Bhamidipaty,Emma Brunskill*

Main category: cs.AI

TL;DR: 本文提出了一种名为PBRR的自动化迭代框架，通过从人类偏好中学习一个附加的、依赖于转换的修正项，来修复人类指定的代理奖励函数，从而解决奖励函数未对齐和奖励学习成本高昂的问题。


<details>
  <summary>Details</summary>
Motivation: 强化学习中人类设计的奖励函数经常与人类的真实目标不一致，导致“奖励破解”和策略未对齐。虽然从人类反馈中学习奖励函数是一种替代方案，但这需要收集大量的轨迹偏好数据，成本高昂。

Method: 本文提出了偏好基奖励修复（PBRR）框架。它通过从人类偏好中学习一个附加的、依赖于转换的修正项来修复人类指定的代理奖励函数。PBRR采用了一种有针对性的探索策略和一种新的偏好学习目标，以识别和纠正那些导致次优性能的关键转换。

Result: 在表格域中，PBRR的累积遗憾与先前的基于偏好的强化学习方法相匹配。在奖励破解基准测试中，PBRR始终优于从头开始学习奖励函数或使用其他方法修改代理奖励函数的基线，并且需要显著更少的偏好来学习高性能策略。

Conclusion: PBRR框架有效地解决了奖励函数未对齐和从头学习奖励成本高的问题。它通过迭代修复现有奖励函数，用更少的偏好数据实现了与人类真实目标高度对齐的高性能策略。

Abstract: Human-designed reward functions for reinforcement learning (RL) agents are
frequently misaligned with the humans' true, unobservable objectives, and thus
act only as proxies. Optimizing for a misspecified proxy reward function often
induces reward hacking, resulting in a policy misaligned with the human's true
objectives. An alternative is to perform RL from human feedback, which involves
learning a reward function from scratch by collecting human preferences over
pairs of trajectories. However, building such datasets is costly. To address
the limitations of both approaches, we propose Preference-Based Reward Repair
(PBRR): an automated iterative framework that repairs a human-specified proxy
reward function by learning an additive, transition-dependent correction term
from preferences. A manually specified reward function can yield policies that
are highly suboptimal under the ground-truth objective, yet corrections on only
a few transitions may suffice to recover optimal performance. To identify and
correct for those transitions, PBRR uses a targeted exploration strategy and a
new preference-learning objective. We prove in tabular domains PBRR has a
cumulative regret that matches, up to constants, that of prior preference-based
RL methods. In addition, on a suite of reward-hacking benchmarks, PBRR
consistently outperforms baselines that learn a reward function from scratch
from preferences or modify the proxy reward function using other approaches,
requiring substantially fewer preferences to learn high performing policies.

</details>


### [7] [Emotional Cognitive Modeling Framework with Desire-Driven Objective Optimization for LLM-empowered Agent in Social Simulation](https://arxiv.org/abs/2510.13195)
*Qun Ma,Xiao Xue,Xuwen Zhang,Zihan Zhao,Yuwei Guo,Ming Zhang*

Main category: cs.AI

TL;DR: 本研究针对现有大语言模型（LLM）智能体在情感认知方面的局限性，提出了一种情感认知框架，通过整合欲望生成和目标管理，使LLM智能体能更好地模拟人类情感和决策过程，从而展现出更接近人类的行为模式。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的智能体在情感认知方面存在严重缺陷，无法模拟有限理性，并且缺乏经过实证验证的将情感融入智能体决策架构的机制。这限制了它们在社会模拟中连接虚拟和现实世界服务的能力。

Method: 构建了一个情感认知框架，该框架整合了欲望生成和目标管理，旨在实现LLM智能体与人类之间的情感对齐。此框架模拟了LLM智能体完整的决策过程，包括状态演化、欲望生成、目标优化、决策生成和行动执行。该框架在一个专有的多智能体交互环境中实现。

Result: 实验结果表明，受该框架控制的智能体不仅表现出与其情绪状态一致的行为，而且在与其他类型智能体的比较评估中，展现出卓越的生态有效性，并能生成更显著地接近人类行为模式的决策结果。

Conclusion: 所提出的情感认知框架成功地增强了LLM智能体模拟人类情感和决策过程的能力，使其行为更具人类特征和生态有效性，有望弥合虚拟与现实世界服务之间的鸿沟。

Abstract: The advent of large language models (LLMs) has enabled agents to represent
virtual humans in societal simulations, facilitating diverse interactions
within complex social systems. However, existing LLM-based agents exhibit
severe limitations in affective cognition: They fail to simulate the bounded
rationality essential for bridging virtual and real-world services; They lack
empirically validated integration mechanisms embedding emotions within agent
decision architectures. This paper constructs an emotional cognition framework
incorporating desire generation and objective management, designed to achieve
emotion alignment between LLM-based agents and humans, modeling the complete
decision-making process of LLM-based agents, encompassing state evolution,
desire generation, objective optimization, decision generation, and action
execution. This study implements the proposed framework within our proprietary
multi-agent interaction environment. Experimental results demonstrate that
agents governed by our framework not only exhibit behaviors congruent with
their emotional states but also, in comparative assessments against other agent
types, demonstrate superior ecological validity and generate decision outcomes
that significantly more closely approximate human behavioral patterns.

</details>


### [8] [Adaptive Reasoning Executor: A Collaborative Agent System for Efficient Reasoning](https://arxiv.org/abs/2510.13214)
*Zehui Ling,Deshu Chen,Yichi Zhang,Yuchen Liu,Xigui Li,Xin Guo,Yuan Cheng*

Main category: cs.AI

TL;DR: 该论文提出了一种结合小型和大型LLM的混合智能体系统，旨在降低复杂任务中深度推理的计算成本，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: LLM的链式思考和深度推理能显著提升复杂任务性能，多智能体系统通过辩论进一步提高准确性，但将深度推理应用于所有问题计算成本高昂。

Method: 该系统首先由小型LLM生成初步答案，然后由大型LLM进行验证。如果答案正确，则直接采纳；否则，大型LLM执行深度推理。

Result: 实验结果表明，对于简单问题，该方法将大型LLM的计算成本降低了50%以上，且准确性损失可忽略不计；同时，在复杂任务上持续保持稳健的性能。

Conclusion: 该互补智能体系统有效降低了大型LLM在深度推理中的计算成本，特别是在处理简单问题时，实现了成本效益与性能的平衡。

Abstract: Recent advances in Large Language Models (LLMs) demonstrate that
chain-of-thought prompting and deep reasoning substantially enhance performance
on complex tasks, and multi-agent systems can further improve accuracy by
enabling model debates. However, applying deep reasoning to all problems is
computationally expensive. To mitigate these costs, we propose a complementary
agent system integrating small and large LLMs. The small LLM first generates an
initial answer, which is then verified by the large LLM. If correct, the answer
is adopted directly; otherwise, the large LLM performs in-depth reasoning.
Experimental results show that, for simple problems, our approach reduces the
computational cost of the large LLM by more than 50% with negligible accuracy
loss, while consistently maintaining robust performance on complex tasks.

</details>


### [9] [Personalized Learning Path Planning with Goal-Driven Learner State Modeling](https://arxiv.org/abs/2510.13215)
*Joy Jia Yin Lim,Ye He,Jifan Yu,Xin Cong,Daniel Zhang-Li,Zhiyuan Liu,Huiqin Liu,Lei Hou,Juanzi Li,Bin Xu*

Main category: cs.AI

TL;DR: Pxplore是一个新颖的个性化学习路径规划框架，它结合了强化学习训练范式和大型语言模型驱动的教育架构，旨在生成连贯、个性化且目标导向的学习路径。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在个性化学习体验方面展现出潜力，但现有方法通常缺乏与学习目标对齐的规划机制，这是个性化学习路径规划（PLPP）中的一个关键挑战。

Method: 该研究引入了Pxplore框架，整合了基于强化学习的训练范式和LLM驱动的教育架构。具体方法包括：设计结构化的学习者状态模型和自动化奖励函数（将抽象目标转化为可计算信号）；结合监督微调（SFT）和组相对策略优化（GRPO）来训练策略；并将该策略部署到真实学习平台中。

Result: 广泛的实验验证了Pxplore在生成连贯、个性化和目标驱动的学习路径方面的有效性。

Conclusion: Pxplore框架能有效实现个性化和目标导向的学习路径规划。研究团队已发布代码和数据集，以促进未来的相关研究。

Abstract: Personalized Learning Path Planning (PLPP) aims to design adaptive learning
paths that align with individual goals. While large language models (LLMs) show
potential in personalizing learning experiences, existing approaches often lack
mechanisms for goal-aligned planning. We introduce Pxplore, a novel framework
for PLPP that integrates a reinforcement-based training paradigm and an
LLM-driven educational architecture. We design a structured learner state model
and an automated reward function that transforms abstract objectives into
computable signals. We train the policy combining supervised fine-tuning (SFT)
and Group Relative Policy Optimization (GRPO), and deploy it within a
real-world learning platform. Extensive experiments validate Pxplore's
effectiveness in producing coherent, personalized, and goal-driven learning
paths. We release our code and dataset to facilitate future research.

</details>


### [10] [An Analytical Framework to Enhance Autonomous Vehicle Perception for Smart Cities](https://arxiv.org/abs/2510.13230)
*Jalal Khan,Manzoor Khan,Sherzod Turaev,Sumbal Malik,Hesham El-Sayed,Farman Ullah*

Main category: cs.AI

TL;DR: 本文提出了一种新颖的基于效用的分析模型，以增强自动驾驶汽车（AVs）的环境感知能力。该模型结合了自定义数据集、YOLOv8s目标检测和感知服务效用评估模块，并通过实验验证了AdamW优化器在类级别性能上优于SGD和Adam。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶的环境感知至关重要，需要开发深度学习模型和AI解决方案来提高自动驾驶汽车的智能出行能力。当前缺乏能够准确感知道路上多个物体并预测驾驶员感知以控制车辆运动的模型。

Method: 本文提出了一个基于效用的分析模型，包括三个模块：1) 获取包含摩托车手、人力车等独特物体的自定义数据集；2) 使用基于深度学习的模型（YOLOv8s）进行目标检测；3) 从训练模型实例的性能值中衡量感知服务效用。该感知模型通过目标检测任务进行验证，并与nuScense数据集上最先进的深度学习模型性能指标进行基准测试。

Result: 实验结果显示，基于mAP@0.5值的三个最佳YOLOv8s实例分别是基于SGD（0.832）、基于Adam（0.810）和基于AdamW（0.822）。尽管mAP@0.5值略低，但基于AdamW的模型在类级别性能上（例如，汽车：0.921，摩托车手：0.899，卡车：0.793）优于基于SGD的模型（汽车：0.915，摩托车手：0.892，卡车：0.781），这得到了所提出的感知模型的证实。研究验证了所提出的效用函数能够为自动驾驶汽车找到正确的感知方案。

Conclusion: 所提出的感知模型能够有效评估学习模型的效用并确定自动驾驶汽车的适当感知。这些结果鼓励使用该模型来评估学习模型的效用，并为自动驾驶汽车选择合适的感知方案。

Abstract: The driving environment perception has a vital role for autonomous driving
and nowadays has been actively explored for its realization. The research
community and relevant stakeholders necessitate the development of Deep
Learning (DL) models and AI-enabled solutions to enhance autonomous vehicles
(AVs) for smart mobility. There is a need to develop a model that accurately
perceives multiple objects on the road and predicts the driver's perception to
control the car's movements. This article proposes a novel utility-based
analytical model that enables perception systems of AVs to understand the
driving environment. The article consists of modules: acquiring a custom
dataset having distinctive objects, i.e., motorcyclists, rickshaws, etc; a
DL-based model (YOLOv8s) for object detection; and a module to measure the
utility of perception service from the performance values of trained model
instances. The perception model is validated based on the object detection
task, and its process is benchmarked by state-of-the-art deep learning models'
performance metrics from the nuScense dataset. The experimental results show
three best-performing YOLOv8s instances based on mAP@0.5 values, i.e.,
SGD-based (0.832), Adam-based (0.810), and AdamW-based (0.822). However, the
AdamW-based model (i.e., car: 0.921, motorcyclist: 0.899, truck: 0.793, etc.)
still outperforms the SGD-based model (i.e., car: 0.915, motorcyclist: 0.892,
truck: 0.781, etc.) because it has better class-level performance values,
confirmed by the proposed perception model. We validate that the proposed
function is capable of finding the right perception for AVs. The results above
encourage using the proposed perception model to evaluate the utility of
learning models and determine the appropriate perception for AVs.

</details>


### [11] [SAJA: A State-Action Joint Attack Framework on Multi-Agent Deep Reinforcement Learning](https://arxiv.org/abs/2510.13262)
*Weiqi Guo,Guanjun Liu,Ziyuan Zhou*

Main category: cs.AI

TL;DR: 该论文提出了一种名为SAJA的联合状态-动作攻击框架，用于多智能体深度强化学习（MADRL）模型。SAJA通过协同组合状态和动作扰动，优于现有单一攻击方法，且更具隐蔽性，并能规避现有防御措施。


<details>
  <summary>Details</summary>
Motivation: MADRL模型在合作和竞争任务中表现出潜力，但易受状态和动作上的对抗性扰动攻击。现有研究仅关注单一的状态攻击或动作攻击，未能有效结合两者以利用其潜在的协同效应。

Method: 本文提出了SAJA（State-Action Joint Attack）框架，包含两个阶段：1) 在状态攻击阶段，利用多步梯度上升法结合actor和critic网络计算对抗性状态；2) 在动作攻击阶段，基于已扰动的状态，使用critic网络通过第二次梯度上升来生成最终的对抗性动作。此外，引入了一个启发式正则化项到损失函数中，以衡量扰动动作与原始动作之间的距离，从而增强critic的指导效果。

Result: 在多智能体粒子环境（MPE）中的评估表明：1) SAJA优于仅状态攻击或仅动作攻击，且更具隐蔽性；2) 现有的状态或动作防御方法无法有效防御SAJA的攻击。

Conclusion: SAJA框架是一种有效且隐蔽的联合状态-动作攻击方法，能够对MADRL模型造成显著影响，并揭示了当前防御机制的不足，强调了开发更强健防御策略的必要性。

Abstract: Multi-Agent Deep Reinforcement Learning (MADRL) has shown potential for
cooperative and competitive tasks such as autonomous driving and strategic
gaming. However, models trained by MADRL are vulnerable to adversarial
perturbations on states and actions. Therefore, it is essential to investigate
the robustness of MADRL models from an attack perspective. Existing studies
focus on either state-only attacks or action-only attacks, but do not consider
how to effectively joint them. Simply combining state and action perturbations
such as randomly perturbing states and actions does not exploit their potential
synergistic effects. In this paper, we propose the State-Action Joint Attack
(SAJA) framework that has a good synergistic effects. SAJA consists of two
important phases: (1) In the state attack phase, a multi-step gradient ascent
method utilizes both the actor network and the critic network to compute an
adversarial state, and (2) in the action attack phase, based on the perturbed
state, a second gradient ascent uses the critic network to craft the final
adversarial action. Additionally, a heuristic regularizer measuring the
distance between the perturbed actions and the original clean ones is added
into the loss function to enhance the effectiveness of the critic's guidance.
We evaluate SAJA in the Multi-Agent Particle Environment (MPE), demonstrating
that (1) it outperforms and is more stealthy than state-only or action-only
attacks, and (2) existing state or action defense methods cannot defend its
attacks.

</details>


### [12] [EvoTest: Evolutionary Test-Time Learning for Self-Improving Agentic Systems](https://arxiv.org/abs/2510.13220)
*Yufei He,Juncheng Liu,Yue Liu,Yibo Li,Tri Cao,Zhiyuan Hu,Xinxing Xu,Bryan Hooi*

Main category: cs.AI

TL;DR: 本文提出J-TTL基准来衡量AI代理在测试时学习新技能的能力，并引入EvoTest框架。EvoTest通过在每个回合后进化代理系统（而非微调或梯度），显著提升了代理在J-TTL上的表现，超越了现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理在测试时无法即时学习复杂技能，在陌生环境中表现不佳，限制了其实用性。研究旨在系统性地衡量并推动解决这一挑战。

Method: 首先，引入Jericho测试时学习（J-TTL）基准，代理需在连续回合中玩同一游戏并尝试提升表现。其次，提出EvoTest进化测试时学习框架，该框架无需微调或梯度，在每个回合后通过进化整个代理系统进行改进。EvoTest包含“行动代理”负责游戏，“进化代理”分析回合记录并提出修订配置，包括重写提示、更新记忆、调整超参数和学习工具使用例程。

Result: 在J-TTL基准上，EvoTest持续提升性能，不仅超越了基于反射和仅记忆的基线方法，也优于更复杂的在线微调方法。值得注意的是，EvoTest是唯一能够赢得两款游戏（Detective和Library）的方法，而所有基线方法均未能获胜。

Conclusion: EvoTest框架有效解决了AI代理在测试时学习复杂技能的难题，显著提升了代理在陌生环境中的适应和学习能力，展现出优于现有方法的性能。

Abstract: A fundamental limitation of current AI agents is their inability to learn
complex skills on the fly at test time, often behaving like "clever but
clueless interns" in novel environments. This severely limits their practical
utility. To systematically measure and drive progress on this challenge, we
first introduce the Jericho Test-Time Learning (J-TTL) benchmark. J-TTL is a
new evaluation setup where an agent must play the same game for several
consecutive episodes, attempting to improve its performance from one episode to
the next. On J-TTL, we find that existing adaptation methods like reflection,
memory, or reinforcement learning struggle. To address the challenges posed by
our benchmark, we present EvoTest, an evolutionary test-time learning framework
that improves an agent without any fine-tuning or gradients-by evolving the
entire agentic system after every episode. EvoTest has two roles: the Actor
Agent, which plays the game, and the Evolver Agent, which analyzes the episode
transcript to propose a revised configuration for the next run. This
configuration rewrites the prompt, updates memory by logging effective
state-action choices, tunes hyperparameters, and learns the tool-use routines.
On our J-TTL benchmark, EvoTest consistently increases performance,
outperforming not only reflection and memory-only baselines but also more
complex online fine-tuning methods. Notably, our method is the only one capable
of winning two games (Detective and Library), while all baselines fail to win
any.

</details>


### [13] [Learnable Game-theoretic Policy Optimization for Data-centric Self-explanation Rationalization](https://arxiv.org/abs/2510.13393)
*Yunxiao Zhao,Zhiqiang Wang,Xingtong Yu,Xiaoli Li,Jiye Liang,Ru Li*

Main category: cs.AI

TL;DR: 本文提出了一种名为PORAT的博弈论策略优化方法，通过引入策略干预来解决合作合理化模型中生成器模式崩溃问题，从而使模型收敛到更优的解决方案。


<details>
  <summary>Details</summary>
Motivation: 传统的合理化方法通过正则化项来约束生成，但存在模式崩溃问题，即预测器虽然预测正确，但生成器却输出模式单一的合理化结果。现有研究通常针对特定模式崩溃设计，缺乏统一考虑。本文认为根本原因是生成器不再探索新策略，导致系统收敛到次优的博弈均衡（正确预测但合理化结果模式崩溃）。

Method: 本文从博弈论角度重新审视合作合理化，并提出了一种名为“面向博弈论策略优化的合理化”（PORAT）的新方法。PORAT逐步引入策略干预来解决合作博弈过程中的博弈均衡问题，从而引导模型走向更优的解决方案状态。

Result: 本文理论分析了次优均衡的原因并证明了所提方法的可行性。在九个广泛使用的真实世界数据集和两个合成设置上验证了PORAT，结果显示其性能比现有最先进方法提高了高达8.1%。

Conclusion: PORAT通过博弈论策略优化和策略干预，有效解决了合作合理化中的模式崩溃问题，引导模型达到更优的博弈均衡，从而生成更具信息量的合理化结果，并显著提升了性能。

Abstract: Rationalization, a data-centric framework, aims to build self-explanatory
models to explain the prediction outcome by generating a subset of
human-intelligible pieces of the input data. It involves a cooperative game
model where a generator generates the most human-intelligible parts of the
input (i.e., rationales), followed by a predictor that makes predictions based
on these generated rationales. Conventional rationalization methods typically
impose constraints via regularization terms to calibrate or penalize undesired
generation. However, these methods are suffering from a problem called mode
collapse, in which the predictor produces correct predictions yet the generator
consistently outputs rationales with collapsed patterns. Moreover, existing
studies are typically designed separately for specific collapsed patterns,
lacking a unified consideration. In this paper, we systematically revisit
cooperative rationalization from a novel game-theoretic perspective and
identify the fundamental cause of this problem: the generator no longer tends
to explore new strategies to uncover informative rationales, ultimately leading
the system to converge to a suboptimal game equilibrium (correct predictions
v.s collapsed rationales). To solve this problem, we then propose a novel
approach, Game-theoretic Policy Optimization oriented RATionalization (PORAT),
which progressively introduces policy interventions to address the game
equilibrium in the cooperative game process, thereby guiding the model toward a
more optimal solution state. We theoretically analyse the cause of such a
suboptimal equilibrium and prove the feasibility of the proposed method.
Furthermore, we validate our method on nine widely used real-world datasets and
two synthetic settings, where PORAT achieves up to 8.1% performance
improvements over existing state-of-the-art methods.

</details>


### [14] [Assessing LLM Reasoning Through Implicit Causal Chain Discovery in Climate Discourse](https://arxiv.org/abs/2510.13417)
*Liesbeth Allein,Nataly Pineda-Castañeda,Andrea Rocci,Marie-Francine Moens*

Main category: cs.AI

TL;DR: 本研究评估了大型语言模型（LLMs）在发现隐含因果链方面的机制性因果推理能力。结果显示LLMs能生成连贯的因果链，但其判断主要基于联想模式匹配而非真正的因果推理，尽管人类评估认为其逻辑一致。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探究大型语言模型如何解释因果关系，以及它们识别连接因果对的中间因果步骤的能力，即其机制性因果推理能力。

Method: 采用诊断性评估框架，指示九个LLMs生成连接给定因果对（来自气候变化辩论资源）的所有可能中间因果步骤。分析了LLMs生成步骤的数量和粒度，以及其内部一致性和置信度，并进行了人工评估以验证生成链的逻辑连贯性。

Result: LLMs生成的因果步骤在数量和粒度上存在差异。它们对其生成的中间因果连接表现出自我一致性和信心，但其判断主要由联想模式匹配驱动，而非真正的因果推理。然而，人类评估证实了生成因果链的逻辑连贯性和完整性。

Conclusion: 本研究提出的基线因果链发现方法、诊断性评估的见解以及带有因果链的基准数据集，为未来在论证背景下进行隐含、机制性因果推理的研究奠定了坚实基础。

Abstract: How does a cause lead to an effect, and which intermediate causal steps
explain their connection? This work scrutinizes the mechanistic causal
reasoning capabilities of large language models (LLMs) to answer these
questions through the task of implicit causal chain discovery. In a diagnostic
evaluation framework, we instruct nine LLMs to generate all possible
intermediate causal steps linking given cause-effect pairs in causal chain
structures. These pairs are drawn from recent resources in argumentation
studies featuring polarized discussion on climate change. Our analysis reveals
that LLMs vary in the number and granularity of causal steps they produce.
Although they are generally self-consistent and confident about the
intermediate causal connections in the generated chains, their judgments are
mainly driven by associative pattern matching rather than genuine causal
reasoning. Nonetheless, human evaluations confirmed the logical coherence and
integrity of the generated chains. Our baseline causal chain discovery
approach, insights from our diagnostic evaluation, and benchmark dataset with
causal chains lay a solid foundation for advancing future work in implicit,
mechanistic causal reasoning in argumentation settings.

</details>


### [15] [Mobile Coverage Analysis using Crowdsourced Data](https://arxiv.org/abs/2510.13459)
*Timothy Wong,Tom Freeman,Joseph Feehily*

Main category: cs.AI

TL;DR: 本文提出了一种利用众包QoE数据和One-Class Support Vector Machine (OC-SVM)算法，对移动网络覆盖范围进行精确评估并识别服务弱点的新框架。


<details>
  <summary>Details</summary>
Motivation: 有效评估移动网络覆盖范围和精确识别服务弱点对于运营商提升用户体验质量（QoE）至关重要。

Method: 该方法的核心是利用经验地理位置数据，在单个小区（天线）级别进行覆盖分析，并汇总到基站级别。关键贡献在于应用OC-SVM算法计算移动网络覆盖，将决策超平面建模为有效覆盖等高线。同样的方法也被扩展用于分析众包服务丢失报告，以识别和量化地理位置上的弱点。

Result: 研究结果表明，该新型框架能够有效准确地绘制移动覆盖图，并突出显示信号不足的细粒度区域，尤其是在复杂的城市环境中。

Conclusion: 该新型框架能够有效评估移动网络覆盖并识别弱点，对提升用户体验和网络优化具有重要意义。

Abstract: Effective assessment of mobile network coverage and the precise
identification of service weak spots are paramount for network operators
striving to enhance user Quality of Experience (QoE). This paper presents a
novel framework for mobile coverage and weak spot analysis utilising
crowdsourced QoE data. The core of our methodology involves coverage analysis
at the individual cell (antenna) level, subsequently aggregated to the site
level, using empirical geolocation data. A key contribution of this research is
the application of One-Class Support Vector Machine (OC-SVM) algorithm for
calculating mobile network coverage. This approach models the decision
hyperplane as the effective coverage contour, facilitating robust calculation
of coverage areas for individual cells and entire sites. The same methodology
is extended to analyse crowdsourced service loss reports, thereby identifying
and quantifying geographically localised weak spots. Our findings demonstrate
the efficacy of this novel framework in accurately mapping mobile coverage and,
crucially, in highlighting granular areas of signal deficiency, particularly
within complex urban environments.

</details>


### [16] [Confidence as a Reward: Transforming LLMs into Reward Models](https://arxiv.org/abs/2510.13501)
*He Du,Bowen Li,Chengxing Xie,Chang Gao,Kai Chen,Dacheng Tao*

Main category: cs.AI

TL;DR: 本文提出CRew，一种无需训练的奖励方法，利用LLM对最终答案的token级置信度作为奖励，在数学推理任务上优于现有无训练和大多数有训练的奖励模型，并可用于数据过滤和DPO微调以进一步提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 奖励模型能显著提升大型语言模型（LLMs）的推理能力，但需要大量人工标注数据和昂贵的训练。现有的无训练方法（如LLM-as-a-Judge）利用LLM的内在推理能力进行评估，但模型置信度作为奖励的概念尚未被全面研究。

Method: 本文系统研究了“置信度即奖励”（CRew），这是一种简单而强大的无训练方法，利用模型最终答案的token级置信度作为奖励的替代指标，特别适用于封闭式任务。在此基础上，结合置信度分数和正确性信号，构建偏好数据，提出了CRew-DPO训练策略。

Result: CRew在MATH500和RewardMATH基准上超越了现有的无训练奖励方法，甚至优于大多数已训练的奖励模型。研究发现CRew分数与模型的实际推理性能之间存在很强的相关性。此外，CRew能有效过滤高质量的训练数据。通过CRew-DPO进行的微调进一步增强了模型的判断能力，并持续优于现有的自训练方法。

Conclusion: CRew是一种基于置信度的简单而强大的无训练奖励方法，能有效提升LLM在数学推理任务上的表现。它不仅可以直接作为奖励，还能用于高质量数据过滤，并通过CRew-DPO策略进一步优化模型，超越了现有自训练方法。

Abstract: Reward models can significantly enhance the reasoning capabilities of large
language models (LLMs), but they typically require extensive curated data and
costly training. To mitigate these challenges, training-free approaches such as
LLM-as-a-Judge leverage the intrinsic reasoning abilities of LLMs to evaluate
responses, achieving promising results. Recent works have also indicated that
model confidence can serve effectively as a reward metric, distinguishing
between chain-of-thought (CoT) and non-CoT paths. However, the concept of using
confidence as a reward has not been comprehensively studied. In this work, we
systematically investigate Confidence-as-a-Reward (CRew), a simple yet powerful
training-free method that utilizes token-level confidence in the model's final
answers as a proxy for reward, especially suitable for close-ended tasks.
Through extensive experiments on mathematical reasoning tasks, we demonstrate
that CRew outperforms existing training-free reward approaches on the MATH500
and RewardMATH benchmarks, and even surpasses most trained reward models. We
further identify a strong correlation between CRew scores and the actual
reasoning performance of the model. Additionally, we find that CRew can
effectively filter high-quality training data. Building upon these insights, we
propose CRew-DPO, a training strategy that constructs preference data from
confidence scores combined with correctness signals. Finetuning with CRew-DPO
further enhances the model's judging capabilities and consistently outperforms
existing self-training methods.

</details>


### [17] [A Methodology for Assessing the Risk of Metric Failure in LLMs Within the Financial Domain](https://arxiv.org/abs/2510.13524)
*William Flanagan,Mukunda Das,Rajitha Ramanyake,Swaunja Maslekar,Meghana Manipuri,Joong Ho Choi,Shruti Nair,Shambhavi Bhusan,Sanjana Dulam,Mouni Pendharkar,Nidhi Singh,Vashisth Doshi,Sachi Shah Paresh*

Main category: cs.AI

TL;DR: 随着生成式AI在金融服务业的应用，衡量模型性能是主要障碍。本文解释了现有机器学习指标和专家评估的不足，并提出了一个风险评估框架，以更好地应用这些指标。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在金融服务业的应用面临性能评估的显著障碍。历史机器学习指标往往无法泛化到生成式AI工作负载，且常需专家评估补充。即便如此，许多项目仍未能考虑到选择特定指标时存在的独特风险。此外，广泛使用的基准测试也难以适用于工业用途。

Method: 本文通过解释这些挑战，并提供一个“风险评估框架”来解决问题。

Result: 该风险评估框架能够更好地应用主题专家评估和机器学习指标，从而改善生成式AI的性能衡量。

Conclusion: 生成式AI在金融领域的性能衡量存在挑战，现有指标和基准不足以应对。本文提出了一个风险评估框架，旨在优化专家评估和机器学习指标的应用，以克服这些障碍。

Abstract: As Generative Artificial Intelligence is adopted across the financial
services industry, a significant barrier to adoption and usage is measuring
model performance. Historical machine learning metrics can oftentimes fail to
generalize to GenAI workloads and are often supplemented using Subject Matter
Expert (SME) Evaluation. Even in this combination, many projects fail to
account for various unique risks present in choosing specific metrics.
Additionally, many widespread benchmarks created by foundational research labs
and educational institutions fail to generalize to industrial use. This paper
explains these challenges and provides a Risk Assessment Framework to allow for
better application of SME and machine learning Metrics

</details>


### [18] [A Modal Logic for Temporal and Jurisdictional Classifier Models](https://arxiv.org/abs/2510.13691)
*Cecilia Di Florio,Huimin Dong,Antonino Rotolo*

Main category: cs.AI

TL;DR: 本文提出一种分类器模态逻辑，用于形式化捕捉法律领域的案例推理（CBR），并通过融入案件时间维度和法院层级来解决判例冲突。


<details>
  <summary>Details</summary>
Motivation: 机器学习分类器在法律领域预测案件结果，执行案例推理（CBR），因此需要逻辑模型来构建验证工具。现有的CBR模型可能缺乏对法律领域特有冲突解决原则的捕捉。

Method: 引入一种分类器模态逻辑，该逻辑旨在形式化捕捉法律CBR。通过将案件的时间维度和法律系统内的法院层级纳入逻辑，以解决判例之间的冲突。

Result: 设计了一种能够形式化捕捉法律案例推理的模态逻辑，并融入了解决判例冲突的原则，考虑了案件的时间性和法院的层级结构。

Conclusion: 所提出的分类器模态逻辑能够有效地形式化法律领域的案例推理，并通过考虑时间维度和法院层级来解决判例冲突，为机器学习分类器的验证工具提供了理论基础。

Abstract: Logic-based models can be used to build verification tools for machine
learning classifiers employed in the legal field. ML classifiers predict the
outcomes of new cases based on previous ones, thereby performing a form of
case-based reasoning (CBR). In this paper, we introduce a modal logic of
classifiers designed to formally capture legal CBR. We incorporate principles
for resolving conflicts between precedents, by introducing into the logic the
temporal dimension of cases and the hierarchy of courts within the legal
system.

</details>


### [19] [Tandem Training for Language Models](https://arxiv.org/abs/2510.13551)
*Robert West,Ashton Anderson,Ece Kamar,Eric Horvitz*

Main category: cs.AI

TL;DR: 随着语言模型能力的提升，其推理过程对弱代理和人类而言变得难以理解。本文提出“串联训练”方法，通过强化学习让强模型生成对弱模型可理解的解决方案，从而提升可解释性和可审计性。该方法在GSM8K任务上成功使模型适应弱伙伴的语言，同时保持高准确率。


<details>
  <summary>Details</summary>
Motivation: 语言模型快速发展，其行为和推理过程对较弱的代理和人类而言越来越难以理解或无法追踪，这损害了解释性和监督能力。研究旨在寻找鼓励模型生成对弱协作者保持可理解的解决方案的方法，以应对长期未来挑战。

Method: 将可理解性形式化为“交接鲁棒性”：如果沿解决方案路径随机将控制权交给弱模型不会导致失败，则强模型的解决方案对弱模型是可理解的。在此基础上，引入“串联训练”，这是一种强化学习范式，在训练过程中，轨迹中的令牌会间歇性地、随机地从一个固定的弱模型而非正在训练的强模型中采样。通过优化标准RL目标，当两个模型能共同构建成功解决方案时，隐式地激励了正确性和可理解性。

Result: 在GSM8K数学推理任务中，串联训练能可靠地教会模型放弃行话，使其语言适应较弱的伙伴，同时保持较高的任务准确性。

Conclusion: 研究结果展示了一条构建可被较弱代理审计的AI系统的有前景的途径，对人机协作和多代理通信具有重要意义。

Abstract: As language models continue to rapidly improve, we can expect their actions
and reasoning to become difficult or impossible for weaker agents and humans to
follow, undermining interpretability and oversight. With an eye on long-term
futures, we pursue methods that encourage models to produce solutions that
remain intelligible to weaker collaborators. We formalize intelligibility as
handoff robustness: a strong model's solution is intelligible to a weaker model
if randomly handing off control to the weaker model along the solution path
does not cause failure. Building on this criterion, we introduce tandem
training for language models, a reinforcement learning (RL) paradigm in which
rollout tokens are intermittently and randomly sampled from a frozen weak model
rather than the strong model being trained. Because rollouts succeed only when
the strong model's actions and reasoning process can be continued by the weak
model -- when the two can co-construct a successful solution -- optimizing
standard RL objectives with tandem training implicitly incentivizes both
correctness and intelligibility. In the GSM8K math reasoning task, tandem
training reliably teaches models to abandon jargon and adapt their language to
weaker partners while keeping task accuracy high. Our results demonstrate a
promising route to building AI systems that remain auditable by weaker agents,
with implications for human--AI collaboration and multi-agent communication.

</details>


### [20] [Training LLM Agents to Empower Humans](https://arxiv.org/abs/2510.13709)
*Evan Ellis,Vivek Myers,Jens Tuyls,Sergey Levine,Anca Dragan,Benjamin Eysenbach*

Main category: cs.AI

TL;DR: 本文提出了一种名为 Empower 的新型自监督方法，通过最大化人类的赋能（即其在环境中实现期望改变的能力）来微调辅助语言模型。该方法仅需离线文本数据，无需额外的人类反馈，并在用户研究和模拟环境中显示出显著优于基线的性能，提高了用户偏好和任务成功率。


<details>
  <summary>Details</summary>
Motivation: 目前的辅助智能体倾向于自主完成任务而非真正协助人类，且通常需要昂贵且明确的人类反馈进行训练。研究目标是开发一种能适时让出控制权、真正辅助人类实现目标，且无需显式人类反馈的训练方法。

Method: 提出了一种名为 Empower 的自监督方法，通过最大化人类的赋能（即人类在环境中实现期望改变的能力）来微调辅助语言模型。该方法仅利用离线文本数据进行训练，无需额外的人类反馈信号。

Result: 在18人用户研究中，参与者有78%的时间更偏爱 Empower 辅助智能体（p=0.015），其建议接受率高出31%，建议数量减少38%。在一个新的多轮代码辅助模拟环境中，Empower 训练的智能体使模拟人类程序员在困难编码问题上的成功率比 SFT 基线平均提高了192%。

Conclusion: 赋能目标为大规模开发有用且对齐的 AI 智能体提供了一个框架，该框架仅需离线数据，无需额外的人类反馈或可验证的奖励。

Abstract: Assistive agents should not only take actions on behalf of a human, but also
step out of the way and cede control when there are important decisions to be
made. However, current methods for building assistive agents, whether via
mimicking expert humans or via RL finetuning on an inferred reward, often
encourage agents to complete tasks on their own rather than truly assisting the
human attain their objectives. Additionally, these methods often require costly
explicit human feedback to provide a training signal. We propose a new approach
to tuning assistive language models based on maximizing the human's
empowerment, their ability to effect desired changes in the environment. Our
empowerment-maximizing method, Empower, only requires offline text data,
providing a self-supervised method for fine-tuning language models to better
assist humans. To study the efficacy of our approach, we conducted an 18-person
user study comparing our empowerment assistant with a strong baseline.
Participants preferred our assistant 78% of the time (p=0.015), with a 31%
higher acceptance rate and 38% fewer suggestions. Additionally, we introduce a
new environment for evaluating multi-turn code assistance using simulated
humans. Using this environment, we show that agents trained with Empower
increase the success rate of a simulated human programmer on challenging coding
questions by an average of 192% over an SFT baseline. With this empowerment
objective, we provide a framework for useful aligned AI agents at scale using
only offline data without the need for any additional human feedback or
verifiable rewards.

</details>


### [21] [From Refusal to Recovery: A Control-Theoretic Approach to Generative AI Guardrails](https://arxiv.org/abs/2510.13727)
*Ravi Pandya,Madison Bland,Duy P. Nguyen,Changliu Liu,Jaime Fernández Fisac,Andrea Bajcsy*

Main category: cs.AI

TL;DR: 本文提出了一种基于控制理论的预测性AI护栏，用于代理式AI系统。该护栏能实时监控并主动纠正AI的危险行为，以预防下游危害（如财务或物理损害），并将其视为一个序列决策问题，通过安全关键强化学习进行训练。


<details>
  <summary>Details</summary>
Motivation: 目前的AI安全护栏主要依赖于基于标签数据集和人工标准的输出分类，这使得它们在面对新的危险情境时显得脆弱。此外，当检测到不安全条件时，通常只采取拒绝行动的方式，但这并非总是最安全的选择。对于日益增长的代理式AI系统（如购物助手、自动驾驶汽车），安全问题已不再是简单地阻止有害内容，而是需要预防下游的金融或物理危害，这本质上是一个序列决策问题。

Method: 将代理式AI安全问题形式化为安全关键控制理论中的序列决策问题，但作用于AI模型的潜在表示空间。开发了预测性护栏，能够实时监控AI系统的输出（行动），并主动将危险输出纠正为安全输出。该方法具有模型无关性，可通过安全关键强化学习进行大规模训练。

Result: 在模拟驾驶和电子商务环境中的实验表明，这种基于控制理论的护栏能够可靠地引导大型语言模型（LLM）代理避免灾难性后果（从碰撞到破产），同时保持任务性能。这提供了一种有原则的动态替代方案，优于目前“标记即阻止”的护栏。

Conclusion: 代理式AI安全应被视为一个序列决策问题，通过在AI模型潜在表示中应用安全关键控制理论，可以构建预测性护栏。这种方法能够实时、主动地纠正AI的危险行为，有效预防下游危害，同时保持任务性能，为AI安全提供了一种动态且有原则的新范式。

Abstract: Generative AI systems are increasingly assisting and acting on behalf of end
users in practical settings, from digital shopping assistants to
next-generation autonomous cars. In this context, safety is no longer about
blocking harmful content, but about preempting downstream hazards like
financial or physical harm. Yet, most AI guardrails continue to rely on output
classification based on labeled datasets and human-specified criteria,making
them brittle to new hazardous situations. Even when unsafe conditions are
flagged, this detection offers no path to recovery: typically, the AI system
simply refuses to act--which is not always a safe choice. In this work, we
argue that agentic AI safety is fundamentally a sequential decision problem:
harmful outcomes arise from the AI system's continually evolving interactions
and their downstream consequences on the world. We formalize this through the
lens of safety-critical control theory, but within the AI model's latent
representation of the world. This enables us to build predictive guardrails
that (i) monitor an AI system's outputs (actions) in real time and (ii)
proactively correct risky outputs to safe ones, all in a model-agnostic manner
so the same guardrail can be wrapped around any AI model. We also offer a
practical training recipe for computing such guardrails at scale via
safety-critical reinforcement learning. Our experiments in simulated driving
and e-commerce settings demonstrate that control-theoretic guardrails can
reliably steer LLM agents clear of catastrophic outcomes (from collisions to
bankruptcy) while preserving task performance, offering a principled dynamic
alternative to today's flag-and-block guardrails.

</details>


### [22] [Hard2Verify: A Step-Level Verification Benchmark for Open-Ended Frontier Math](https://arxiv.org/abs/2510.13744)
*Shrey Pandit,Austin Xu,Xuan-Phi Nguyen,Yifei Ming,Caiming Xiong,Shafiq Joty*

Main category: cs.AI

TL;DR: 本文介绍了Hard2Verify，一个用于评估大型语言模型（LLM）数学推理系统步进级验证器的人工标注基准，并评估了多种验证器，发现开源模型普遍落后于闭源模型。


<details>
  <summary>Details</summary>
Motivation: LLM在IMO等竞赛中取得了金牌级表现，但其数学证明需要每个步骤不仅正确且有充分支持。在开放式设置中训练LLM推理器，需要强大的验证器来捕捉步进级错误。

Method: 引入了Hard2Verify，一个耗时超过500小时人工标注的步进级验证基准。该基准要求验证器对前沿LLM在挑战性数学问题上生成的响应提供步进级标注或识别第一个错误。评估了29个生成式评论器和过程奖励模型。

Result: 评估结果显示，除了少数几个表现突出的模型外，开源验证器普遍落后于闭源模型。论文还分析了导致步进级验证性能不佳的原因、扩展验证器计算资源的影响，以及自验证和验证-生成动态等基本问题。

Conclusion: Hard2Verify基准为严格评估步进级验证器提供了工具，并揭示了当前开源验证器与闭源模型之间的性能差距。研究还深入探讨了影响验证性能的关键因素和未来研究方向。

Abstract: Large language model (LLM)-based reasoning systems have recently achieved
gold medal-level performance in the IMO 2025 competition, writing mathematical
proofs where, to receive full credit, each step must be not only correct but
also sufficiently supported. To train LLM-based reasoners in such challenging,
open-ended settings, strong verifiers capable of catching step-level mistakes
are necessary prerequisites. We introduce Hard2Verify, a human-annotated,
step-level verification benchmark produced with over 500 hours of human labor.
Hard2Verify is designed to rigorously assess step-level verifiers at the
frontier: Verifiers must provide step-level annotations or identify the first
error in responses generated by frontier LLMs for very recent, challenging, and
open-ended math questions. We evaluate 29 generative critics and process reward
models, demonstrating that, beyond a few standouts, open-source verifiers lag
closed source models. We subsequently analyze what drives poor performance in
step-level verification, the impacts of scaling verifier compute, as well as
fundamental questions such as self-verification and verification-generation
dynamics.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [23] [Epistemic-aware Vision-Language Foundation Model for Fetal Ultrasound Interpretation](https://arxiv.org/abs/2510.12953)
*Xiao He,Huangxuan Zhao,Guojia Wan,Wei Zhou,Yanxing Liu,Juhua Liu,Yongchao Xu,Yong Luo,Dacheng Tao,Bo Du*

Main category: cs.CV

TL;DR: FetalMind是一个专为胎儿超声设计的医学AI系统，用于报告生成和诊断。它通过引入显著知识解耦（SED）方法和构建大规模FetalSigma-1M数据集，解决了现有模型在胎儿超声领域表现不佳的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的医学视觉-语言模型在结构化成人影像上表现良好，但在胎儿超声领域表现不佳，因为胎儿超声面临多视图图像推理、疾病种类繁多和图像多样性等挑战。此外，该领域缺乏专门的模型和大规模数据集。

Method: 该研究提出了FetalMind系统，并引入了显著知识解耦（SED）方法。SED通过将专家策划的二分图注入模型，解耦视图-疾病关联，并通过强化学习引导模型沿临床流程进行偏好选择。为训练FetalMind，研究团队还策划了首个大规模胎儿超声报告语料库FetalSigma-1M，包含来自12个医疗中心的2万份报告。

Result: FetalMind在所有孕期阶段均优于现有开源和闭源基线模型，平均性能提升14%，在关键疾病条件上的准确性提高61.2%，同时保持高效、稳定和可扩展性。

Conclusion: FetalMind是一个为胎儿超声量身定制的卓越AI系统，它通过创新的SED方法和大规模数据集解决了该领域的独特挑战，并在报告生成和诊断方面取得了显著的性能提升，与产科实践高度一致。

Abstract: Recent medical vision-language models have shown promise on tasks such as
VQA, report generation, and anomaly detection. However, most are adapted to
structured adult imaging and underperform in fetal ultrasound, which poses
challenges of multi-view image reasoning, numerous diseases, and image
diversity. To bridge this gap, we introduce FetalMind, a medical AI system
tailored to fetal ultrasound for both report generation and diagnosis. Guided
by clinical workflow, we propose Salient Epistemic Disentanglement (SED), which
injects an expert-curated bipartite graph into the model to decouple
view-disease associations and to steer preference selection along clinically
faithful steps via reinforcement learning. This design mitigates variability
across diseases and heterogeneity across views, reducing learning bottlenecks
while aligning the model's inference with obstetric practice. To train
FetalMind at scale, we curate FetalSigma-1M dataset, the first large-scale
fetal ultrasound report corpus, comprising 20K reports from twelve medical
centers, addressing the scarcity of domain data. Extensive experiments show
that FetalMind outperforms open- and closed-source baselines across all
gestational stages, achieving +14% average gains and +61.2% higher accuracy on
critical conditions while remaining efficient, stable, and scalable. Project
Page: https://hexiao0275.github.io/FetalMind.

</details>


### [24] [State-Change Learning for Prediction of Future Events in Endoscopic Videos](https://arxiv.org/abs/2510.12904)
*Saurav Sharma,Chinedu Innocent Nwoye,Didier Mutter,Nicolas Padoy*

Main category: cs.CV

TL;DR: 该研究提出SurgFUTR，一种基于状态变化学习的手术未来预测AI模型，通过教师-学生架构和Sinkhorn-Knopp聚类压缩视频状态，解决了现有方法在预测未来事件、统一短/长期任务和泛化能力方面的不足，并在多项任务和数据集上显示出显著改进和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 手术未来预测（如即将发生的事件、时间、风险）对于手术室安全和效率至关重要，能优化资源分配、器械准备和早期并发症预警。然而，当前手术AI研究主要集中在理解当前事件，而非预测未来，且现有方法任务孤立、缺乏统一的短/长期预测，依赖粗粒度监督，并且仅基于未来特征预测的方法难以泛化。

Method: 该研究将手术未来预测重新定义为“状态变化学习”，即分类当前和未来时间步之间的状态转换。为此，提出了SurgFUTR模型，其核心是一个教师-学生架构。视频片段通过Sinkhorn-Knopp聚类压缩为状态表示；教师网络从当前和未来片段中学习，而学生网络仅从当前视频预测未来状态，并由Action Dynamics (ActDyn) 模块引导。此外，还建立了SFPBench基准，包含短时（动作三元组、事件）和长时（剩余手术时长、阶段和步骤转换）的五项预测任务。

Result: 在四个数据集和三种手术类型上的实验表明，SurgFUTR模型实现了持续的性能改进。跨手术过程的迁移验证了其泛化能力。

Conclusion: 通过将手术未来预测重构为状态变化学习，并引入SurgFUTR模型及其教师-学生架构和ActDyn模块，该研究成功克服了现有方法的局限性，在短时和长时预测任务上均表现出显著的性能提升和跨手术过程的泛化能力，为手术室的安全和效率提供了有力的实时AI分析工具。

Abstract: Surgical future prediction, driven by real-time AI analysis of surgical
video, is critical for operating room safety and efficiency. It provides
actionable insights into upcoming events, their timing, and risks-enabling
better resource allocation, timely instrument readiness, and early warnings for
complications (e.g., bleeding, bile duct injury). Despite this need, current
surgical AI research focuses on understanding what is happening rather than
predicting future events. Existing methods target specific tasks in isolation,
lacking unified approaches that span both short-term (action triplets, events)
and long-term horizons (remaining surgery duration, phase transitions). These
methods rely on coarse-grained supervision while fine-grained surgical action
triplets and steps remain underexplored. Furthermore, methods based only on
future feature prediction struggle to generalize across different surgical
contexts and procedures. We address these limits by reframing surgical future
prediction as state-change learning. Rather than forecasting raw observations,
our approach classifies state transitions between current and future timesteps.
We introduce SurgFUTR, implementing this through a teacher-student
architecture. Video clips are compressed into state representations via
Sinkhorn-Knopp clustering; the teacher network learns from both current and
future clips, while the student network predicts future states from current
videos alone, guided by our Action Dynamics (ActDyn) module. We establish
SFPBench with five prediction tasks spanning short-term (triplets, events) and
long-term (remaining surgery duration, phase and step transitions) horizons.
Experiments across four datasets and three procedures show consistent
improvements. Cross-procedure transfer validates generalizability.

</details>


### [25] [Unifying Vision-Language Latents for Zero-label Image Caption Enhancement](https://arxiv.org/abs/2510.12931)
*Sanghyun Byun,Jung Ick Guack,Mohanad Odema,Baisub Lee,Jacob Song,Woo Seong Chung*

Main category: cs.CV

TL;DR: ViZer是一个零标签增强训练框架，通过在训练中对视觉和语言特征进行对齐，使现有视觉语言模型无需文本标签或完全重新训练即可生成更优质的图像描述，解决了标注数据限制和未标记数据利用不足的问题。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型(VLM)的性能依赖于大规模图像-文本预训练，但其对标注图像数据集的依赖限制了可扩展性，并导致大量未标注图像数据未被充分利用。

Method: 提出了统一视觉-语言对齐零标签增强(ViZer)框架。该框架在训练过程中主动对视觉和语言表示特征进行对齐，使现有VLM无需文本标签或完全重新训练即可生成改进的图像描述。

Result: 将ViZer应用于SmolVLM-Base和Qwen2-VL模型，观察到持续的定性改进，生成的图像描述比基线模型更具基础性和描述性。研究指出，CIDEr和BERTScore等自动化描述指标常会惩罚参考描述中缺失的细节，因此定性评估更为重要。

Conclusion: ViZer为图像描述中的零标签学习提供了一个实用的起点，并为视觉语言任务中更广泛的零标签适应提供了可能性，通过定性评估证明了其在生成更具基础性和描述性描述方面的优势。

Abstract: Vision-language models (VLMs) achieve remarkable performance through
large-scale image-text pretraining. However, their reliance on labeled image
datasets limits scalability and leaves vast amounts of unlabeled image data
underutilized. To address this, we propose Unified Vision-Language Alignment
for Zero-Label Enhancement (ViZer), an enhancement training framework that
enables zero-label learning in image captioning, providing a practical starting
point for broader zero-label adaptation in vision-language tasks. Unlike prior
approaches that rely on human or synthetically annotated datasets, ViZer
actively aligns vision and language representation features during training,
enabling existing VLMs to generate improved captions without requiring text
labels or full retraining. We demonstrate ViZer's advantage in qualitative
evaluation, as automated caption metrics such as CIDEr and BERTScore often
penalize details that are absent in reference captions. Applying ViZer on
SmolVLM-Base and Qwen2-VL, we observe consistent qualitative improvements,
producing captions that are more grounded and descriptive than their baseline.

</details>


### [26] [SimULi: Real-Time LiDAR and Camera Simulation with Unscented Transforms](https://arxiv.org/abs/2510.12901)
*Haithem Turki,Qi Wu,Xin Kang,Janick Martinez Esturo,Shengyu Huang,Ruilong Li,Zan Gojcic,Riccardo de Lutio*

Main category: cs.CV

TL;DR: SimULi是首个能实时渲染任意相机模型和激光雷达数据的多传感器模拟方法，通过扩展3DGUT和优化3D高斯表示，显著提升了自动驾驶模拟的速度和精度。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶机器人需要严格测试以确保安全，这要求高保真模拟器能测试真实世界难以收集的场景。现有神经渲染方法（如NeRF和3DGS）渲染速度慢，或仅支持针孔相机模型，无法满足高畸变镜头和激光雷达数据的需求。多传感器模拟还面临跨传感器不一致性问题，现有方法常以牺牲其他模态质量为代价。

Method: 本文提出了SimULi方法，它扩展了原生支持复杂相机模型的3DGUT，增加了对激光雷达数据的支持，包括针对任意旋转激光雷达模型的自动化平铺策略和基于光线的剔除。为解决跨传感器不一致性，设计了一种分解的3D高斯表示和锚定策略。

Result: SimULi的渲染速度比光线追踪方法快10-20倍，比现有的基于光栅化的工作快1.5-10倍（且支持更广泛的相机模型）。与现有方法相比，它将平均相机和深度误差降低了高达40%。在两个广泛使用的自动驾驶数据集上评估，SimULi在众多相机和激光雷达指标上达到或超越了现有最先进方法的保真度。

Conclusion: SimULi是第一个能够实时渲染任意相机模型和激光雷达数据的方法，有效解决了自动驾驶模拟中速度、多传感器支持和跨模态一致性的挑战，并展现出卓越的性能和精度。

Abstract: Rigorous testing of autonomous robots, such as self-driving vehicles, is
essential to ensure their safety in real-world deployments. This requires
building high-fidelity simulators to test scenarios beyond those that can be
safely or exhaustively collected in the real-world. Existing neural rendering
methods based on NeRF and 3DGS hold promise but suffer from low rendering
speeds or can only render pinhole camera models, hindering their suitability to
applications that commonly require high-distortion lenses and LiDAR data.
Multi-sensor simulation poses additional challenges as existing methods handle
cross-sensor inconsistencies by favoring the quality of one modality at the
expense of others. To overcome these limitations, we propose SimULi, the first
method capable of rendering arbitrary camera models and LiDAR data in
real-time. Our method extends 3DGUT, which natively supports complex camera
models, with LiDAR support, via an automated tiling strategy for arbitrary
spinning LiDAR models and ray-based culling. To address cross-sensor
inconsistencies, we design a factorized 3D Gaussian representation and
anchoring strategy that reduces mean camera and depth error by up to 40%
compared to existing methods. SimULi renders 10-20x faster than ray tracing
approaches and 1.5-10x faster than prior rasterization-based work (and handles
a wider range of camera models). When evaluated on two widely benchmarked
autonomous driving datasets, SimULi matches or exceeds the fidelity of existing
state-of-the-art methods across numerous camera and LiDAR metrics.

</details>


### [27] [Robust Plant Disease Diagnosis with Few Target-Domain Samples](https://arxiv.org/abs/2510.12909)
*Takafumi Nogami,Satoshi Kagiwada,Hitoshi Iyatomi*

Main category: cs.CV

TL;DR: 现有植物病害诊断深度学习系统在不同部署环境下泛化能力差。本文提出TMPS框架，通过度量学习和优先级采样，仅用少量目标域样本即可显著提升诊断鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 深度学习在植物病害诊断中表现出色，但其诊断准确性在训练环境之外的图像上往往无法保持，这主要是由于病害症状的细微变异和域间隙（图像背景和环境差异）导致。根本原因是训练数据多样性相对于任务复杂性有限，使得先进模型在未见领域中容易失效。

Method: 本文提出了一种名为目标感知度量学习与优先级采样（Target-Aware Metric Learning with Prioritized Sampling, TMPS）的简单且高度适应性的学习框架，其核心是度量学习。TMPS假设可以访问目标（部署）域中有限数量的带标签样本，并有效利用这些样本来提高诊断鲁棒性。

Result: TMPS在一个包含223,073张叶片图像的大规模植物病害诊断任务上进行了评估，这些图像来自23个农田，涵盖3种作物、21种病害和健康实例。通过为每种病害仅纳入10个目标域样本进行训练，TMPS超越了使用相同源域和目标域组合样本训练的模型，以及在源域数据上预训练后用这些目标样本进行微调的模型，平均宏F1分数分别提高了7.3和3.6个百分点。与基线和传统度量学习相比，TMPS分别实现了18.7和17.1个百分点的显著提升。

Conclusion: TMPS框架能够有效地利用少量目标域样本，显著提升植物病害诊断系统的鲁棒性，并在性能上超越了传统的训练和微调方法，为解决深度学习模型在实际部署中的泛化性问题提供了有效方案。

Abstract: Various deep learning-based systems have been proposed for accurate and
convenient plant disease diagnosis, achieving impressive performance. However,
recent studies show that these systems often fail to maintain diagnostic
accuracy on images captured under different conditions from the training
environment -- an essential criterion for model robustness. Many deep learning
methods have shown high accuracy in plant disease diagnosis. However, they
often struggle to generalize to images taken in conditions that differ from the
training setting. This drop in performance stems from the subtle variability of
disease symptoms and domain gaps -- differences in image context and
environment. The root cause is the limited diversity of training data relative
to task complexity, making even advanced models vulnerable in unseen domains.
To tackle this challenge, we propose a simple yet highly adaptable learning
framework called Target-Aware Metric Learning with Prioritized Sampling (TMPS),
grounded in metric learning. TMPS operates under the assumption of access to a
limited number of labeled samples from the target (deployment) domain and
leverages these samples effectively to improve diagnostic robustness. We assess
TMPS on a large-scale automated plant disease diagnostic task using a dataset
comprising 223,073 leaf images sourced from 23 agricultural fields, spanning 21
diseases and healthy instances across three crop species. By incorporating just
10 target domain samples per disease into training, TMPS surpasses models
trained using the same combined source and target samples, and those fine-tuned
with these target samples after pre-training on source data. It achieves
average macro F1 score improvements of 7.3 and 3.6 points, respectively, and a
remarkable 18.7 and 17.1 point improvement over the baseline and conventional
metric learning.

</details>


### [28] [CADE 2.5 - ZeResFDG: Frequency-Decoupled, Rescaled and Zero-Projected Guidance for SD/SDXL Latent Diffusion Models](https://arxiv.org/abs/2510.12954)
*Denis Rychkovskiy,GPT-5*

Main category: cs.CV

TL;DR: CADE 2.5引入了ZeResFDG和QSilk稳定器，显著提升了SD/SDXL潜在扩散模型的图像锐度、提示依从性和伪影控制，无需重新训练。


<details>
  <summary>Details</summary>
Motivation: 改善SD/SDXL潜在扩散模型在采样过程中图像的锐度、对提示词的依从性以及对伪影的控制，尤其是在适度的引导尺度下。

Method: 核心方法是CADE 2.5，一个采样器级别的引导堆栈。它包括：1. ZeResFDG模块，统一了频率解耦引导（重新加权低频和高频分量）、能量重缩放（使引导预测的样本幅度与正分支匹配）和零投影（移除与无条件方向平行的分量）。2. 轻量级频谱EMA，带有滞后切换机制，在采样过程中根据结构结晶程度在保守模式和细节寻求模式之间切换。3. QSilk微粒稳定器（分位数钳位+深度/边缘门控微细节注入），用于在推理时提高鲁棒性并生成自然的高频微纹理。

Result: ZeResFDG在SD/SDXL采样器上，以适度的引导尺度，无需任何重新训练，提高了图像锐度、提示依从性和伪影控制。QSilk微粒稳定器提高了鲁棒性，并在高分辨率下以可忽略的开销产生了自然的高频微纹理。

Conclusion: CADE 2.5通过其创新的ZeResFDG引导模块和QSilk稳定器，为SD/SDXL潜在扩散模型提供了一种有效的、无需重新训练的解决方案，以显著提升生成图像的质量、细节和稳定性。

Abstract: We introduce CADE 2.5 (Comfy Adaptive Detail Enhancer), a sampler-level
guidance stack for SD/SDXL latent diffusion models. The central module,
ZeResFDG, unifies (i) frequency-decoupled guidance that reweights low- and
high-frequency components of the guidance signal, (ii) energy rescaling that
matches the per-sample magnitude of the guided prediction to the positive
branch, and (iii) zero-projection that removes the component parallel to the
unconditional direction. A lightweight spectral EMA with hysteresis switches
between a conservative and a detail-seeking mode as structure crystallizes
during sampling. Across SD/SDXL samplers, ZeResFDG improves sharpness, prompt
adherence, and artifact control at moderate guidance scales without any
retraining. In addition, we employ a training-free inference-time stabilizer,
QSilk Micrograin Stabilizer (quantile clamp + depth/edge-gated micro-detail
injection), which improves robustness and yields natural high-frequency
micro-texture at high resolutions with negligible overhead. For completeness we
note that the same rule is compatible with alternative parameterizations (e.g.,
velocity), which we briefly discuss in the Appendix; however, this paper
focuses on SD/SDXL latent diffusion models.

</details>


### [29] [Scope: Selective Cross-modal Orchestration of Visual Perception Experts](https://arxiv.org/abs/2510.12974)
*Tianyu Zhang,Suyuchen Wang,Chao Wang,Juan Rodriguez,Ahmed Masry,Xiangru Jian,Yoshua Bengio,Perouz Taslakian*

Main category: cs.CV

TL;DR: SCOPE是一种混合编码器（MoEnc）框架，通过实例级路由为每个图像-文本对动态选择一个专业编码器，从而在提高视觉语言模型性能的同时显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 在视觉语言模型（VLMs）中，简单堆叠多个视觉编码器会导致收益递减，并大幅增加推理成本。

Method: SCOPE采用混合编码器（MoEnc）框架，通过实例级路由（而非传统的token级路由）为每个图像-文本对动态选择一个专业编码器。它包含一个共享编码器和一组路由编码器。一个轻量级路由器利用文本提示与共享视觉特征之间的交叉注意力来选择最佳编码器。为训练该路由器，引入了双熵正则化和辅助损失，以平衡数据集级别的负载分布和实例级别的路由置信度。

Result: SCOPE框架，仅使用一个共享编码器加一个路由编码器，其性能优于同时使用所有四个额外编码器的模型，同时计算量减少了24-49%。

Conclusion: 智能的编码器选择方法（如SCOPE）在多编码器视觉语言模型中优于蛮力聚合，挑战了当前的主流范式。

Abstract: Vision-language models (VLMs) benefit from multiple vision encoders, but
naively stacking them yields diminishing returns while multiplying inference
costs. We propose SCOPE, a Mixture-of-Encoders (MoEnc) framework that
dynamically selects one specialized encoder per image-text pair via
instance-level routing, unlike token-level routing in traditional MoE. SCOPE
maintains a shared encoder and a pool of routed encoders. A lightweight router
uses cross-attention between text prompts and shared visual features to select
the optimal encoder from the routed encoders. To train this router, we
introduce dual entropy regularization with auxiliary losses to balance
dataset-level load distribution with instance-level routing confidence.
Remarkably, SCOPE with one shared plus one routed encoder outperforms models
using all four extra encoders simultaneously, while reducing compute by
24-49\%. This demonstrates that intelligent encoder selection beats brute-force
aggregation, challenging the prevailing paradigm in multi-encoder VLMs.

</details>


### [30] [SVAG-Bench: A Large-Scale Benchmark for Multi-Instance Spatio-temporal Video Action Grounding](https://arxiv.org/abs/2510.13016)
*Tanveer Hannan,Shuaicong Wu,Mark Weber,Suprosanna Shit,Jindong Gu,Rajat Koner,Aljoša Ošep,Laura Leal-Taixé,Thomas Seidl*

Main category: cs.CV

TL;DR: 本文提出了时空视频动作定位（SVAG）这一新任务，要求模型根据自然语言描述同时检测、跟踪并时序定位视频中的所有相关对象。为此，构建了SVAG-Bench大型基准，并提出了SVAGFormer基线模型和SVAGEval评估工具。结果表明现有模型在该任务上表现不佳，尤其是在复杂场景中。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解方法主要关注粗粒度动作识别或通用对象跟踪，忽视了根据对象的细粒度动作，同时进行检测、跟踪和时序定位的挑战。这阻碍了下一代AI系统（如具身智能体、自主平台、人机交互框架）的发展。

Method: 1. 引入了“时空视频动作定位（SVAG）”新任务。2. 构建了SVAG-Bench，一个包含688个视频、19,590条标注记录和903个独特动词的大规模基准。3. 提出了SVAGFormer，一个将最先进视觉语言模型应用于联合空间和时间定位的基线框架。4. 引入了SVAGEval，一个标准化评估工具，用于公平和可复现的基准测试。

Result: 实证结果表明，现有模型在SVAG任务上表现不佳，特别是在密集或复杂的场景中。这凸显了在长视频中对细粒度对象-动作交互进行更高级推理的需求。

Conclusion: SVAG任务和相关基准、工具的引入，揭示了当前模型在联合检测、跟踪和时序定位细粒度动作方面的不足。未来研究需着重开发更先进的推理能力，以处理长视频中复杂的对象-动作交互。

Abstract: Understanding fine-grained actions and accurately localizing their
corresponding actors in space and time are fundamental capabilities for
advancing next-generation AI systems, including embodied agents, autonomous
platforms, and human-AI interaction frameworks. Despite recent progress in
video understanding, existing methods predominantly address either
coarse-grained action recognition or generic object tracking, thereby
overlooking the challenge of jointly detecting and tracking multiple objects
according to their actions while grounding them temporally. To address this
gap, we introduce Spatio-temporal Video Action Grounding (SVAG), a novel task
that requires models to simultaneously detect, track, and temporally localize
all referent objects in videos based on natural language descriptions of their
actions. To support this task, we construct SVAG-Bench, a large-scale benchmark
comprising 688 videos, 19,590 annotated records, and 903 unique verbs, covering
a diverse range of objects, actions, and real-world scenes. We further propose
SVAGFormer, a baseline framework that adapts state of the art vision language
models for joint spatial and temporal grounding, and introduce SVAGEval, a
standardized evaluation toolkit for fair and reproducible benchmarking.
Empirical results show that existing models perform poorly on SVAG,
particularly in dense or complex scenes, underscoring the need for more
advanced reasoning over fine-grained object-action interactions in long videos.

</details>


### [31] [SeqBench: Benchmarking Sequential Narrative Generation in Text-to-Video Models](https://arxiv.org/abs/2510.13042)
*Zhengxu Tang,Zizheng Wang,Luning Wang,Zitao Shuai,Chenhao Zhang,Siyu Qian,Yirui Wu,Bohao Wang,Haosong Rao,Zhenyu Yang,Chenwei Wu*

Main category: cs.CV

TL;DR: SeqBench是一个评估文本到视频（T2V）生成模型序列叙事连贯性的综合基准，包含数据集和动态时间图（DTG）自动评估指标，揭示了当前T2V模型在叙事推理方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 尽管文本到视频（T2V）生成模型在视觉效果上取得了显著进展，但它们在生成需要逻辑进展的多事件连贯序列叙事方面表现不佳。现有的T2V基准主要关注视觉质量，未能评估长序列的叙事连贯性，因此需要一个专门的评估框架来弥补这一空白。

Method: 本文提出了SeqBench，一个用于评估T2V生成中序列叙事连贯性的综合基准。它包含一个精心设计的包含320个提示词的数据集，涵盖不同叙事复杂性，并从8个最先进的T2V模型生成了2,560个人工标注视频。此外，还设计了一种基于动态时间图（DTG）的自动评估指标，该指标能够高效捕捉长程依赖和时间顺序，并保持计算效率。

Result: DTG自动评估指标与人工标注显示出很强的相关性。通过使用SeqBench进行的系统评估揭示了当前T2V模型的关键局限性：无法在多动作序列中保持对象状态一致性，在多对象场景中产生物理上不合理的结果，以及难以保持序列动作之间现实的时间和顺序关系。

Conclusion: SeqBench提供了第一个系统性的框架来评估T2V生成中的叙事连贯性，并为未来模型改进其序列推理能力提供了具体的见解。

Abstract: Text-to-video (T2V) generation models have made significant progress in
creating visually appealing videos. However, they struggle with generating
coherent sequential narratives that require logical progression through
multiple events. Existing T2V benchmarks primarily focus on visual quality
metrics but fail to evaluate narrative coherence over extended sequences. To
bridge this gap, we present SeqBench, a comprehensive benchmark for evaluating
sequential narrative coherence in T2V generation. SeqBench includes a carefully
designed dataset of 320 prompts spanning various narrative complexities, with
2,560 human-annotated videos generated from 8 state-of-the-art T2V models.
Additionally, we design a Dynamic Temporal Graphs (DTG)-based automatic
evaluation metric, which can efficiently capture long-range dependencies and
temporal ordering while maintaining computational efficiency. Our DTG-based
metric demonstrates a strong correlation with human annotations. Through
systematic evaluation using SeqBench, we reveal critical limitations in current
T2V models: failure to maintain consistent object states across multi-action
sequences, physically implausible results in multi-object scenarios, and
difficulties in preserving realistic timing and ordering relationships between
sequential actions. SeqBench provides the first systematic framework for
evaluating narrative coherence in T2V generation and offers concrete insights
for improving sequential reasoning capabilities in future models. Please refer
to https://videobench.github.io/SeqBench.github.io/ for more details.

</details>


### [32] [SceneAdapt: Scene-aware Adaptation of Human Motion Diffusion](https://arxiv.org/abs/2510.13044)
*Jungbin Cho,Minsu Kim,Jisoo Kim,Ce Zheng,Laszlo A. Jeni,Ming-Hsuan Yang,Youngjae Yu,Seonjoo Kim*

Main category: cs.CV

TL;DR: SceneAdapt是一个框架，它通过利用分离的场景-动作和文本-动作数据集，并采用两阶段适应（中间帧生成和场景感知中间帧生成），将场景感知能力注入到文本条件动作生成模型中。


<details>
  <summary>Details</summary>
Motivation: 人类动作具有多样性和语义丰富性，并受周围场景的影响。然而，现有的动作生成方法要么单独处理动作语义，要么单独处理场景感知，因为构建同时具有丰富文本-动作覆盖和精确场景交互的大规模数据集极具挑战性。

Method: 本文提出了SceneAdapt框架。其核心思想是使用无需文本即可学习的动作中间帧生成作为代理任务，以连接两个不同的数据集，从而将场景感知能力注入到文本到动作模型中。第一阶段引入关键帧层，调制动作潜在变量以进行中间帧生成，同时保留潜在流形。第二阶段添加一个场景条件层，通过交叉注意力自适应地查询局部上下文来注入场景几何信息。

Result: 实验结果表明，SceneAdapt能够有效地将场景感知能力注入到文本到动作模型中。研究还进一步分析了这种感知能力是如何产生的机制。

Conclusion: SceneAdapt成功地通过创新性的两阶段适应方法，解决了在文本条件动作生成中整合场景感知的挑战，有效利用了分离的数据集，实现了文本到动作模型中的场景感知能力注入。

Abstract: Human motion is inherently diverse and semantically rich, while also shaped
by the surrounding scene. However, existing motion generation approaches
address either motion semantics or scene-awareness in isolation, since
constructing large-scale datasets with both rich text--motion coverage and
precise scene interactions is extremely challenging. In this work, we introduce
SceneAdapt, a framework that injects scene awareness into text-conditioned
motion models by leveraging disjoint scene--motion and text--motion datasets
through two adaptation stages: inbetweening and scene-aware inbetweening. The
key idea is to use motion inbetweening, learnable without text, as a proxy task
to bridge two distinct datasets and thereby inject scene-awareness to
text-to-motion models. In the first stage, we introduce keyframing layers that
modulate motion latents for inbetweening while preserving the latent manifold.
In the second stage, we add a scene-conditioning layer that injects scene
geometry by adaptively querying local context through cross-attention.
Experimental results show that SceneAdapt effectively injects scene awareness
into text-to-motion models, and we further analyze the mechanisms through which
this awareness emerges. Code and models will be released.

</details>


### [33] [One Dimensional CNN ECG Mamba for Multilabel Abnormality Classification in 12 Lead ECG](https://arxiv.org/abs/2510.13046)
*Huawei Jiang,Husna Mutahira,Gan Huang,Mannan Saeed Muhammad*

Main category: cs.CV

TL;DR: 本研究提出了一种结合一维卷积神经网络和Mamba（一种选择性状态空间模型）的混合框架，用于心电图异常检测，并在长序列信号处理上表现出优于现有方法的卓越性能。


<details>
  <summary>Details</summary>
Motivation: 准确检测心电图（ECG）异常对临床诊断和决策支持至关重要。然而，传统的深度学习模型在处理长序列ECG信号时性能有限。状态空间模型（SSM）作为一种高效的替代方案被引入，激发了本研究。

Method: 本研究引入了一个名为“一维卷积神经网络心电图Mamba”（One Dimensional Convolutional Neural Network Electrocardiogram Mamba）的混合框架。该模型将卷积特征提取与Mamba（一种专为有效序列建模而设计的选择性状态空间模型）相结合。具体而言，该模型基于Vision Mamba（一种双向变体）构建，以增强心电图数据中时间依赖性的表示。

Result: 在PhysioNet Computing in Cardiology Challenges 2020和2021的综合实验中，所提出的模型取得了优于现有方法的性能。特别是在十二导联心电图上，该模型实现了比先前最佳算法显著更高的AUPRC和AUROC分数。

Conclusion: 这些结果证明了基于Mamba的架构在推进可靠ECG分类方面的潜力。这种能力支持早期诊断和个性化治疗，同时提高了远程医疗和资源受限医疗系统的可及性。

Abstract: Accurate detection of cardiac abnormalities from electrocardiogram recordings
is regarded as essential for clinical diagnostics and decision support.
Traditional deep learning models such as residual networks and transformer
architectures have been applied successfully to this task, but their
performance has been limited when long sequential signals are processed.
Recently, state space models have been introduced as an efficient alternative.
In this study, a hybrid framework named One Dimensional Convolutional Neural
Network Electrocardiogram Mamba is introduced, in which convolutional feature
extraction is combined with Mamba, a selective state space model designed for
effective sequence modeling. The model is built upon Vision Mamba, a
bidirectional variant through which the representation of temporal dependencies
in electrocardiogram data is enhanced. Comprehensive experiments on the
PhysioNet Computing in Cardiology Challenges of 2020 and 2021 were conducted,
and superior performance compared with existing methods was achieved.
Specifically, the proposed model achieved substantially higher AUPRC and AUROC
scores than those reported by the best previously published algorithms on
twelve lead electrocardiograms. These results demonstrate the potential of
Mamba-based architectures to advance reliable ECG classification. This
capability supports early diagnosis and personalized treatment, while enhancing
accessibility in telemedicine and resource-constrained healthcare systems.

</details>


### [34] [True Self-Supervised Novel View Synthesis is Transferable](https://arxiv.org/abs/2510.13063)
*Thomas W. Mitchel,Hyunwoo Ryu,Vincent Sitzmann*

Main category: cs.CV

TL;DR: 本文提出XFactor，首个无几何、自监督模型，通过可迁移的潜在姿态实现真正的Novel View Synthesis (NVS)，解决了以往NVS模型姿态不可迁移的问题。


<details>
  <summary>Details</summary>
Motivation: 研究发现，判断模型是否真正具备Novel View Synthesis (NVS) 能力的关键标准是“可迁移性”：即从一个视频序列中提取的姿态表示能否用于在另一个场景中重新渲染相同的相机轨迹。之前的自监督NVS工作预测的姿态不具备可迁移性，即相同的姿态在不同3D场景中会导致不同的相机轨迹。

Method: XFactor结合了成对姿态估计和简单的输入/输出增强方案，共同实现了相机姿态与场景内容的分离，并促进了几何推理。它是一个无几何、自监督的模型，使用无约束的潜在姿态变量，不依赖任何3D归纳偏置或多视图几何概念（如SE(3）的显式姿态参数化）。

Result: XFactor实现了姿态的可迁移性，并且显著优于先前的无姿态NVS Transformer模型。通过探究实验表明，其潜在姿态与真实世界姿态高度相关。本文还引入了一种新的指标来量化可迁移性。

Conclusion: XFactor是第一个无几何、自监督的模型，能够通过可迁移的潜在姿态实现真正的Novel View Synthesis，而无需任何3D归纳偏置或多视图几何概念。

Abstract: In this paper, we identify that the key criterion for determining whether a
model is truly capable of novel view synthesis (NVS) is transferability:
Whether any pose representation extracted from one video sequence can be used
to re-render the same camera trajectory in another. We analyze prior work on
self-supervised NVS and find that their predicted poses do not transfer: The
same set of poses lead to different camera trajectories in different 3D scenes.
Here, we present XFactor, the first geometry-free self-supervised model capable
of true NVS. XFactor combines pair-wise pose estimation with a simple
augmentation scheme of the inputs and outputs that jointly enables
disentangling camera pose from scene content and facilitates geometric
reasoning. Remarkably, we show that XFactor achieves transferability with
unconstrained latent pose variables, without any 3D inductive biases or
concepts from multi-view geometry -- such as an explicit parameterization of
poses as elements of SE(3). We introduce a new metric to quantify
transferability, and through large-scale experiments, we demonstrate that
XFactor significantly outperforms prior pose-free NVS transformers, and show
that latent poses are highly correlated with real-world poses through probing
experiments.

</details>


### [35] [Direction-aware multi-scale gradient loss for infrared and visible image fusion](https://arxiv.org/abs/2510.13067)
*Kaixuan Yang,Wei Xiang,Zhenshuai Chen,Tong Jin,Yunpeng Liu*

Main category: cs.CV

TL;DR: 本文提出了一种方向感知、多尺度的梯度损失函数，通过分别监督水平和垂直分量并保留其符号，显著提升了红外与可见光图像融合中边缘的清晰度和纹理保留。


<details>
  <summary>Details</summary>
Motivation: 大多数基于学习的图像融合方法在训练时使用梯度幅度损失，但这会丢失方向信息，导致监督模糊，边缘保真度不佳。

Method: 引入了一种方向感知、多尺度的梯度损失函数。该方法分别监督水平和垂直梯度分量，并在不同尺度上保留其符号。这种轴向、符号保留的目标在精细和粗糙分辨率上都提供了清晰的方向指导。

Result: 实验证明，在不改变模型架构或训练协议的情况下，所提出的方法能够促进生成更锐利、对齐更好的边缘，并保留更丰富的纹理。在开源模型和多个公共基准测试上均显示出有效性。

Conclusion: 方向感知、多尺度梯度损失能够有效解决现有梯度幅度损失的局限性，显著提升红外与可见光图像融合的质量，尤其是在边缘保真度和纹理保留方面。

Abstract: Infrared and visible image fusion aims to integrate complementary information
from co-registered source images to produce a single, informative result. Most
learning-based approaches train with a combination of structural similarity
loss, intensity reconstruction loss, and a gradient-magnitude term. However,
collapsing gradients to their magnitude removes directional information,
yielding ambiguous supervision and suboptimal edge fidelity. We introduce a
direction-aware, multi-scale gradient loss that supervises horizontal and
vertical components separately and preserves their sign across scales. This
axis-wise, sign-preserving objective provides clear directional guidance at
both fine and coarse resolutions, promoting sharper, better-aligned edges and
richer texture preservation without changing model architectures or training
protocols. Experiments on open-source model and multiple public benchmarks
demonstrate effectiveness of our approach.

</details>


### [36] [Counting Hallucinations in Diffusion Models](https://arxiv.org/abs/2510.13080)
*Shuai Fu,Jian Zhou,Qi Chen,Huang Jing,Huy Anh Nguyen,Xiaohan Liu,Zhixiong Zeng,Lin Ma,Quanshi Zhang,Qi Wu*

Main category: cs.CV

TL;DR: 该研究旨在系统量化扩散模型中的“计数幻觉”（即生成不正确数量的物体），通过构建新数据集和评估协议，发现常用指标（如FID）无法有效捕捉此类幻觉。


<details>
  <summary>Details</summary>
Motivation: 扩散概率模型（DPMs）在生成任务中常产生与现实世界知识冲突的“幻觉样本”（如生成多余的物体或不正确的结构）。目前缺乏系统量化这些幻觉的方法，阻碍了相关研究进展和下一代生成模型的设计。

Method: 该研究聚焦于一种特定的幻觉形式——“计数幻觉”（生成不正确数量的实例或结构化对象）。为此，他们构建了一个名为CountHalluSet的数据集套件（包含ToyShape、SimObject和RealHand），并定义了明确的计数标准。在此基础上，开发了一个标准化的评估协议来量化计数幻觉，并系统性地检查了DPMs中不同采样条件（如求解器类型、ODE求解器阶数、采样步数和初始噪声）如何影响计数幻觉水平。此外，还分析了计数幻觉与FID等常用评估指标的相关性。

Result: 该研究开发了一个量化计数幻觉的标准化评估协议，并系统性地考察了DPMs中不同采样条件对计数幻觉水平的影响。结果显示，FID等广泛使用的图像质量指标无法始终如一地捕捉计数幻觉。

Conclusion: 这项工作迈出了系统量化扩散模型中幻觉的第一步，为图像生成中幻觉现象的研究提供了新的见解。

Abstract: Diffusion probabilistic models (DPMs) have demonstrated remarkable progress
in generative tasks, such as image and video synthesis. However, they still
often produce hallucinated samples (hallucinations) that conflict with
real-world knowledge, such as generating an implausible duplicate cup floating
beside another cup. Despite their prevalence, the lack of feasible
methodologies for systematically quantifying such hallucinations hinders
progress in addressing this challenge and obscures potential pathways for
designing next-generation generative models under factual constraints. In this
work, we bridge this gap by focusing on a specific form of hallucination, which
we term counting hallucination, referring to the generation of an incorrect
number of instances or structured objects, such as a hand image with six
fingers, despite such patterns being absent from the training data. To this
end, we construct a dataset suite CountHalluSet, with well-defined counting
criteria, comprising ToyShape, SimObject, and RealHand. Using these datasets,
we develop a standardized evaluation protocol for quantifying counting
hallucinations, and systematically examine how different sampling conditions in
DPMs, including solver type, ODE solver order, sampling steps, and initial
noise, affect counting hallucination levels. Furthermore, we analyze their
correlation with common evaluation metrics such as FID, revealing that this
widely used image quality metric fails to capture counting hallucinations
consistently. This work aims to take the first step toward systematically
quantifying hallucinations in diffusion models and offer new insights into the
investigation of hallucination phenomena in image generation.

</details>


### [37] [Unsupervised Domain Adaptation via Content Alignment for Hippocampus Segmentation](https://arxiv.org/abs/2510.13075)
*Hoda Kalabizadeh,Ludovica Griffanti,Pak-Hei Yeung,Ana I. L. Namburete,Nicola K. Dinsdale,Konstantinos Kamnitsas*

Main category: cs.CV

TL;DR: 针对医学图像分割中跨域部署的域偏移问题，特别是内容变化，本文提出了一种无监督域适应框架。该框架结合z-标准化和双向可变形图像配准（DIR）策略，用于海马体分割，并在多项实验中显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在跨不同医学图像数据集（如MRI）部署时，由于图像外观（风格）和人群依赖的解剖特征（内容）的域偏移，其性能会显著下降。特别是内容变化，对海马体分割等任务构成了巨大挑战。

Method: 本文提出了一种新颖的无监督域适应框架，直接解决跨域海马体分割中的域偏移。该方法结合了通过z-标准化实现的有效风格协调，以及一种双向可变形图像配准（DIR）策略。DIR网络与分割网络和判别器网络联合训练，以指导感兴趣区域的配准，并生成解剖学上合理的变换，将源图像与目标域对齐。

Result: 该方法在合成数据集Morpho-MNIST和三个MRI海马体数据集上进行了全面评估，结果表明其性能优于所有现有基线。在从年轻健康人群到临床痴呆患者的海马体分割任务中，该框架与标准增强方法相比，Dice分数相对提高了高达15%，在内容偏移较大的场景中增益最为显著。

Conclusion: 本研究的结果强调了所提出方法在跨不同人群实现准确海马体分割方面的有效性，尤其在存在显著内容偏移的场景中表现出色。

Abstract: Deep learning models for medical image segmentation often struggle when
deployed across different datasets due to domain shifts - variations in both
image appearance, known as style, and population-dependent anatomical
characteristics, referred to as content. This paper presents a novel
unsupervised domain adaptation framework that directly addresses domain shifts
encountered in cross-domain hippocampus segmentation from MRI, with specific
emphasis on content variations. Our approach combines efficient style
harmonisation through z-normalisation with a bidirectional deformable image
registration (DIR) strategy. The DIR network is jointly trained with
segmentation and discriminator networks to guide the registration with respect
to a region of interest and generate anatomically plausible transformations
that align source images to the target domain. We validate our approach through
comprehensive evaluations on both a synthetic dataset using Morpho-MNIST (for
controlled validation of core principles) and three MRI hippocampus datasets
representing populations with varying degrees of atrophy. Across all
experiments, our method outperforms existing baselines. For hippocampus
segmentation, when transferring from young, healthy populations to clinical
dementia patients, our framework achieves up to 15% relative improvement in
Dice score compared to standard augmentation methods, with the largest gains
observed in scenarios with substantial content shift. These results highlight
the efficacy of our approach for accurate hippocampus segmentation across
diverse populations.

</details>


### [38] [Edit-Your-Interest: Efficient Video Editing via Feature Most-Similar Propagation](https://arxiv.org/abs/2510.13084)
*Yi Zuo,Zitao Wang,Lingling Li,Xu Liu,Fang Liu,Licheng Jiao*

Main category: cs.CV

TL;DR: 本文提出了一种名为Edit-Your-Interest的轻量级、文本驱动、零样本视频编辑方法，通过引入时空特征记忆库和特征最相似传播机制，显著降低了计算开销，同时提高了视频编辑的视觉保真度和时间一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像（T2I）扩散模型在视频编辑方面存在严重的局限性：计算开销和内存消耗高；牺牲视觉保真度，导致时间不一致性、模糊和明显的马赛克状伪影。

Method: 本文提出了Edit-Your-Interest方法：1. 引入时空特征记忆库（SFM）来缓存前一帧的关键图像特征，以减少计算开销。2. 提出特征最相似传播（FMP）方法，将最相关的特征从前一帧传播到后续帧，以保持时间一致性。3. 引入SFM更新算法，持续刷新缓存特征，确保其长期相关性。4. 利用交叉注意力图自动提取感兴趣实例的掩码，并将其集成到扩散去噪过程中，实现对目标对象的精细控制和背景完整性保护。

Result: Edit-Your-Interest在效率和视觉保真度方面均优于现有最先进的方法。

Conclusion: 所提出的Edit-Your-Interest方法在视频编辑方面表现出卓越的有效性和实用性。

Abstract: Text-to-image (T2I) diffusion models have recently demonstrated significant
progress in video editing.
  However, existing video editing methods are severely limited by their high
computational overhead and memory consumption.
  Furthermore, these approaches often sacrifice visual fidelity, leading to
undesirable temporal inconsistencies and artifacts such as blurring and
pronounced mosaic-like patterns.
  We propose Edit-Your-Interest, a lightweight, text-driven, zero-shot video
editing method.
  Edit-Your-Interest introduces a spatio-temporal feature memory to cache
features from previous frames, significantly reducing computational overhead
compared to full-sequence spatio-temporal modeling approaches.
  Specifically, we first introduce a Spatio-Temporal Feature Memory bank (SFM),
which is designed to efficiently cache and retain the crucial image tokens
processed by spatial attention.
  Second, we propose the Feature Most-Similar Propagation (FMP) method. FMP
propagates the most relevant tokens from previous frames to subsequent ones,
preserving temporal consistency.
  Finally, we introduce an SFM update algorithm that continuously refreshes the
cached features, ensuring their long-term relevance and effectiveness
throughout the video sequence.
  Furthermore, we leverage cross-attention maps to automatically extract masks
for the instances of interest.
  These masks are seamlessly integrated into the diffusion denoising process,
enabling fine-grained control over target objects and allowing
Edit-Your-Interest to perform highly accurate edits while robustly preserving
the background integrity.
  Extensive experiments decisively demonstrate that the proposed
Edit-Your-Interest outperforms state-of-the-art methods in both efficiency and
visual fidelity, validating its superior effectiveness and practicality.

</details>


### [39] [EgoSocial: Benchmarking Proactive Intervention Ability of Omnimodal LLMs via Egocentric Social Interaction Perception](https://arxiv.org/abs/2510.13105)
*Xijun Wang,Tanay Sharma,Achin Kulshrestha,Abhimitra Meka,Aveek Purohit,Dinesh Manocha*

Main category: cs.CV

TL;DR: 该研究针对AR/VR中AI助手缺乏社交意识导致的不当干预问题，提出了一个大规模的以自我为中心的社交视频问答数据集EgoSocial，并提出了一种端到端的方法EgoSoD，通过整合多模态上下文线索和社交思维图来准确判断AI干预时机和社交互动。


<details>
  <summary>Details</summary>
Motivation: 随着AR/VR技术日益融入日常生活，AI需要从以自我为中心的视角理解人类社交动态。然而，当前的LLM缺乏社交意识，无法判断何时作为AI助手进行干预，导致频繁且不合时宜的响应，打断自然对话并影响用户注意力。

Method: 1. 构建了大规模以自我为中心的社交视频问答数据集EgoSocial，包含13,500对视频-问题，用于基准测试社交互动感知中的干预问题。 2. 对现有全模态LLM (OLLMs) 进行了深入分析，评估它们检测社交上下文线索的有效性。 3. 提出了EgoSoD (EgoSocial Detection) 方法，该方法整合了多模态上下文线索（如音频和视觉），并将其融入社交思维图，动态建模参与者和互动，以主动检测干预时机和社交互动。

Result: 1. 实验表明，当前OLLMs在检测干预时机方面表现不佳（例如，Gemini 2.5 Pro 仅为14.4%）。 2. EgoSoD方法在干预时机检测性能上，相对于Phi-4提升了45.6%，相对于Gemini 2.5 Pro提升了9.9%。 3. 在整体社交互动性能上，EgoSoD相对于Phi-4提升了20.4%，相对于Gemini 2.5 Pro提升了6.9%。

Conclusion: 该研究成功解决了AI助手在AR/VR环境中缺乏社交意识的问题。通过引入EgoSocial数据集和EgoSoD方法，显著提高了AI在以自我为中心的视角下判断干预时机和理解社交互动的能力。数据集和代码将很快发布。

Abstract: As AR/VR technologies become integral to daily life, there's a growing need
for AI that understands human social dynamics from an egocentric perspective.
However, current LLMs often lack the social awareness to discern when to
intervene as AI assistant. This leads to constant, socially unaware responses
that may disrupt natural conversation and negatively impact user focus. To
address these limitations, we introduce EgoSocial, a large-scale egocentric
dataset with 13,500 social video-question pairs, specifically designed to
benchmark intervention in social interaction perception. We also present an
in-depth analysis of current omnimodal LLMs (OLLMs) to assess their
effectiveness in detecting diverse social contextual cues. Experiments show
that OLLMs still struggle to detect the intervention timing (14.4% for Gemini
2.5 Pro). We also propose EgoSoD (EgoSocial Detection), an end-to-end method
for robustly discerning social dynamics. Informed by our OLLM analysis, EgoSoD
integrates multimodal contextual cues (e.g., audio and visual cues) into a
social thinking graph, dynamically modeling participants and interactions. Our
method proactively detects intervention timing and social interactions,
precisely determining when to intervene. Our EgoSoD improves Phi-4 by 45.6% and
Gemini 2.5 Pro by 9.9% on Intervention Timing performance, and improves Phi-4
by 20.4% and Gemini 2.5 Pro by 6.9% on overall Social Interaction performance.
We will release the dataset and code soon.

</details>


### [40] [DriveCritic: Towards Context-Aware, Human-Aligned Evaluation for Autonomous Driving with Vision-Language Models](https://arxiv.org/abs/2510.13108)
*Jingyu Song,Zhenxin Li,Shiyi Lan,Xinglong Sun,Nadine Chang,Maying Shen,Joshua Chen,Katherine A. Skinner,Jose M. Alvarez*

Main category: cs.CV

TL;DR: 本文提出了DriveCritic框架，包含一个挑战性场景数据集（带人类偏好标注）和一个基于视觉-语言模型（VLM）的评估器。该框架旨在通过整合视觉和符号上下文，更准确地对自动驾驶规划器进行人类偏好对齐评估，并显著优于现有指标。


<details>
  <summary>Details</summary>
Motivation: 现有的自动驾驶规划器评估指标（如EPDMS）缺乏对细微场景的上下文感知能力，导致评估结果与人类判断存在偏差，这是自动驾驶领域面临的一个关键挑战。

Method: 研究引入了DriveCritic框架，包括：1) DriveCritic数据集，一个包含关键上下文的挑战性场景集合，并标注了人类的成对偏好；2) DriveCritic模型，一个基于视觉-语言模型（VLM）的评估器。该模型通过两阶段的监督学习和强化学习管道进行微调，以学习通过整合视觉和符号上下文来判断轨迹对。

Result: 实验结果表明，DriveCritic在匹配人类偏好方面显著优于现有指标和基线，并展示出强大的上下文感知能力。

Conclusion: 这项工作为评估自动驾驶系统提供了一个更可靠、更符合人类判断的基础。

Abstract: Benchmarking autonomous driving planners to align with human judgment remains
a critical challenge, as state-of-the-art metrics like the Extended Predictive
Driver Model Score (EPDMS) lack context awareness in nuanced scenarios. To
address this, we introduce DriveCritic, a novel framework featuring two key
contributions: the DriveCritic dataset, a curated collection of challenging
scenarios where context is critical for correct judgment and annotated with
pairwise human preferences, and the DriveCritic model, a Vision-Language Model
(VLM) based evaluator. Fine-tuned using a two-stage supervised and
reinforcement learning pipeline, the DriveCritic model learns to adjudicate
between trajectory pairs by integrating visual and symbolic context.
Experiments show DriveCritic significantly outperforms existing metrics and
baselines in matching human preferences and demonstrates strong context
awareness. Overall, our work provides a more reliable, human-aligned foundation
to evaluating autonomous driving systems.

</details>


### [41] [OS-HGAdapter: Open Semantic Hypergraph Adapter for Large Language Models Assisted Entropy-Enhanced Image-Text Alignment](https://arxiv.org/abs/2510.13131)
*Rongjun Chen,Chengsi Yao,Jinchang Ren,Xianxian Zeng,Peixian Wang,Jun Yuan,Jiawen Li,Huimin Zhao,Xu Lu*

Main category: cs.CV

TL;DR: 本文提出了一种开放语义超图适配器（OS-HGAdapter），通过利用大型语言模型（LLM）增强文本多义性以弥补信息熵差距，并结合超图适配器构建多边连接，显著提升了跨模态图文检索性能，达到了SOTA水平。


<details>
  <summary>Details</summary>
Motivation: 多媒体内容理解中的图文对齐面临挑战，传统方法因文本和图像间固有的信息熵差异，导致跨模态相互检索性能不平衡。

Method: 该方法通过两步实现熵增强对齐：1) 设计新的提示模板，利用LLM增强文本模态的多义性描述，增加其相对于视觉模态的信息熵；2) 使用超图适配器构建文本和图像模态之间的多边连接，以纠正同义语义的匹配错误，并通过维度映射减少开放语义熵引起的噪声。

Result: 在Flickr30K和MS-COCO基准测试中，OS-HGAdapter相比现有方法，文本到图像检索性能提升16.8%，图像到文本检索性能提升40.1%，并在语义对齐任务中建立了新的最先进性能。

Conclusion: 所提出的开放语义超图适配器（OS-HGAdapter）能有效弥补文本和图像之间的信息熵差距，显著提升跨模态检索性能，并在语义对齐任务中取得了领先成果。

Abstract: Text-image alignment constitutes a foundational challenge in multimedia
content understanding, where effective modeling of cross-modal semantic
correspondences critically enhances retrieval system performance through joint
embedding space optimization. Given the inherent difference in information
entropy between texts and images, conventional approaches often show an
imbalance in the mutual retrieval of these two modalities. To address this
particular challenge, we propose to use the open semantic knowledge of Large
Language Model (LLM) to fill for the entropy gap and reproduce the alignment
ability of humans in these tasks. Our entropy-enhancing alignment is achieved
through a two-step process: 1) a new prompt template that does not rely on
explicit knowledge in the task domain is designed to use LLM to enhance the
polysemy description of the text modality. By analogy, the information entropy
of the text modality relative to the visual modality is increased; 2) A
hypergraph adapter is used to construct multilateral connections between the
text and image modalities, which can correct the positive and negative matching
errors for synonymous semantics in the same fixed embedding space, whilst
reducing the noise caused by open semantic entropy by mapping the reduced
dimensions back to the original dimensions. Comprehensive evaluations on the
Flickr30K and MS-COCO benchmarks validate the superiority of our Open Semantic
Hypergraph Adapter (OS-HGAdapter), showcasing 16.8\% (text-to-image) and 40.1\%
(image-to-text) cross-modal retrieval gains over existing methods while
establishing new state-of-the-art performance in semantic alignment tasks.

</details>


### [42] [VPREG: An Optimal Control Formulation for Diffeomorphic Image Registration Based on the Variational Principle Grid Generation Method](https://arxiv.org/abs/2510.13109)
*Zicong Zhou,Baihan Zhao,Andreas Mang,Guojun Liao*

Main category: cs.CV

TL;DR: 本文介绍了一种名为VPreg的新型微分同胚图像配准方法，它在确保配准变换质量（如正雅可比行列式和准确的逆变换）的同时，显著提高了配准精度，并在脑部扫描数据上优于现有先进方法。


<details>
  <summary>Details</summary>
Motivation: 研究旨在改进以往在网格生成和微分同胚图像配准方面的工作，以实现卓越的配准精度，同时严格控制配准变换的质量，包括确保空间变换的正雅可比行列式，并提供一个准确的逆变换，这对于许多神经影像工作流程至关重要。

Method: VPreg是一种新型的微分同胚图像配准方法，其核心是“变分原理”（VP）网格生成方法。VP方法能够构建具有预设雅可比行列式和旋度的无折叠网格，从而保证微分同胚空间变换。它在微分同胚群内部生成逆变换，而非在图像空间操作。研究通过对OASIS-1数据集中150次脑部扫描配准进行性能分析，使用35个感兴趣区域的Dice分数以及对计算出的空间变换属性的实证分析来评估VPreg，并与ANTs-SyN、Freesurfer-Easyreg和FSL-Fnirt等方法进行比较。

Result: VPreg在Dice分数、计算变换的正则性以及所提供逆映射的准确性和一致性方面均优于现有先进方法。它确保了微分同胚空间变换，并提供了比现有方法更准确的逆变换。在对OASIS-1数据集的脑部扫描配准中，VPreg的表现超越了ANTs-SyN、Freesurfer-Easyreg和FSL-Fnirt。

Conclusion: VPreg是一种性能卓越的微分同胚图像配准方法，它不仅实现了高配准精度，而且有效地控制了变换的质量，确保了正雅可比行列式和准确的逆变换。其在神经影像学应用中具有重要价值，并优于当前最先进的方法。

Abstract: This paper introduces VPreg, a novel diffeomorphic image registration method.
This work provides several improvements to our past work on mesh generation and
diffeomorphic image registration. VPreg aims to achieve excellent registration
accuracy while controlling the quality of the registration transformations. It
ensures a positive Jacobian determinant of the spatial transformation and
provides an accurate approximation of the inverse of the registration, a
crucial property for many neuroimaging workflows. Unlike conventional methods,
VPreg generates this inverse transformation within the group of diffeomorphisms
rather than operating on the image space. The core of VPreg is a grid
generation approach, referred to as \emph{Variational Principle} (VP), which
constructs non-folding grids with prescribed Jacobian determinant and curl.
These VP-generated grids guarantee diffeomorphic spatial transformations
essential for computational anatomy and morphometry, and provide a more
accurate inverse than existing methods. To assess the potential of the proposed
approach, we conduct a performance analysis for 150 registrations of brain
scans from the OASIS-1 dataset. Performance evaluation based on Dice scores for
35 regions of interest, along with an empirical analysis of the properties of
the computed spatial transformations, demonstrates that VPreg outperforms
state-of-the-art methods in terms of Dice scores, regularity properties of the
computed transformation, and accuracy and consistency of the provided inverse
map. We compare our results to ANTs-SyN, Freesurfer-Easyreg, and FSL-Fnirt.

</details>


### [43] [Real-Time Sign Language to text Translation using Deep Learning: A Comparative study of LSTM and 3D CNN](https://arxiv.org/abs/2510.13137)
*Madhumati Pol,Anvay Anturkar,Anushka Khot,Ayush Andure,Aniruddha Ghosh,Anvit Magadum,Anvay Bahadur*

Main category: cs.CV

TL;DR: 本研究比较了3D CNN和LSTM在实时美国手语（ASL）识别中的性能，发现3D CNN准确率更高但计算成本大，而LSTM效率更高。混合模型表现尚可，强调了在实际应用中平衡精度和实时性的重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管3D CNN擅长从视频序列中提取时空特征，而LSTM擅长建模序列数据中的时间依赖性，但需要评估这两种架构在实时ASL识别中的表现，以了解它们在准确性、计算效率和延迟方面的权衡。

Method: 研究在包含50个类别、1,200个ASL手语的数据集上，评估了3D CNN和LSTM网络（以及混合3D CNN-LSTM模型）的性能。比较指标包括识别准确率、计算效率和延迟，所有评估均在相似的训练条件下进行。

Result: 实验结果显示，3D CNN实现了92.4%的识别准确率，但每帧处理时间比LSTM多3.2%。LSTM则保持86.7%的准确率，并显著降低了资源消耗。混合3D CNN-LSTM模型也表现出不错的性能。

Conclusion: 研究表明，在实际实现中，根据具体情境选择架构至关重要，因为识别精度与实时操作要求之间存在权衡。该项目为开发辅助技术提供了专业基准，特别强调了边缘计算环境中精度和实时性之间的取舍。

Abstract: This study investigates the performance of 3D Convolutional Neural Networks
(3D CNNs) and Long Short-Term Memory (LSTM) networks for real-time American
Sign Language (ASL) recognition. Though 3D CNNs are good at spatiotemporal
feature extraction from video sequences, LSTMs are optimized for modeling
temporal dependencies in sequential data. We evaluate both architectures on a
dataset containing 1,200 ASL signs across 50 classes, comparing their accuracy,
computational efficiency, and latency under similar training conditions.
Experimental results demonstrate that 3D CNNs achieve 92.4% recognition
accuracy but require 3.2% more processing time per frame compared to LSTMs,
which maintain 86.7% accuracy with significantly lower resource consumption.
The hybrid 3D CNNLSTM model shows decent performance, which suggests that
context-dependent architecture selection is crucial for practical
implementation.This project provides professional benchmarks for developing
assistive technologies, highlighting trade-offs between recognition precision
and real-time operational requirements in edge computing environments.

</details>


### [44] [Foveation Improves Payload Capacity in Steganography](https://arxiv.org/abs/2510.13151)
*Lifeng Qiu Lin,Henry Kam,Qi Sun,Kaan Akşit*

Main category: cs.CV

TL;DR: 该研究通过高效的潜在表示和凹视渲染技术，将隐写术的容量从100比特提升至500比特，同时显著提高了准确性并保持了良好的视觉质量。


<details>
  <summary>Details</summary>
Motivation: 隐写术在视觉介质中用于提供元数据和水印，现有容量存在限制，需要开发更高效、更准确的隐写方法。

Method: 研究人员训练了模型，利用高效的潜在表示（latent representations）和凹视渲染（foveated rendering）技术。此外，采用了新颖的感知设计（perceptual design）来创建多模态潜在表示。

Result: 隐写容量从100比特提升至500比特；在200K测试比特中，准确性达到2000比特中仅有1个错误比特；视觉质量方面，PSNR达到31.47 dB，LPIPS为0.13，与现有水平相当。

Conclusion: 新颖的感知设计结合多模态潜在表示和凹视渲染，有效提升了隐写术的容量和准确性，同时保持了可接受的视觉质量，证明了其在隐写领域应用的有效性。

Abstract: Steganography finds its use in visual medium such as providing metadata and
watermarking. With support of efficient latent representations and foveated
rendering, we trained models that improve existing capacity limits from 100 to
500 bits, while achieving better accuracy of up to 1 failure bit out of 2000,
at 200K test bits. Finally, we achieve a comparable visual quality of 31.47 dB
PSNR and 0.13 LPIPS, showing the effectiveness of novel perceptual design in
creating multi-modal latent representations in steganography.

</details>


### [45] [DP-TTA: Test-time Adaptation for Transient Electromagnetic Signal Denoising via Dictionary-driven Prior Regularization](https://arxiv.org/abs/2510.13160)
*Meng Yang,Kecheng Chen,Wei Luo,Xianjie Chen,Yong Jia,Mingyue Wang,Fanqiang Lin*

Main category: cs.CV

TL;DR: 针对瞬变电磁（TEM）信号去噪中深度学习模型在不同地理区域噪声特性差异大、泛化能力差的问题，本文提出了一种字典驱动先验正则化测试时自适应（DP-TTA）方法，利用TEM信号固有的物理特性作为先验知识，通过自监督损失指导模型动态调整参数，显著提升了在新场景下的去噪性能。


<details>
  <summary>Details</summary>
Motivation: 瞬变电磁（TEM）信号常被各种噪声淹没。现有基于深度学习的去噪模型大多在模拟或单一真实场景数据上训练，忽略了不同地理区域噪声特性的显著差异（如地质条件、设备、外部干扰），导致模型在新的环境中去噪性能下降。

Method: 本文提出了字典驱动先验正则化测试时自适应（DP-TTA）方法，并定制了DTEMDNet网络。核心思想是利用TEM信号固有的物理特性（如指数衰减和光滑性），这些特性在不同区域保持一致，作为指导TTA策略的理想先验知识。具体而言，在训练阶段，通过字典学习将这些固有特性编码为字典驱动先验，并将其集成到模型中。在测试阶段，该先验指导模型通过最小化源自字典驱动一致性和信号一阶变化的自监督损失，动态适应新环境。

Result: 广泛的实验结果表明，所提出的DP-TTA方法比现有TEM去噪方法和TTA方法取得了更好的性能。

Conclusion: DP-TTA通过利用TEM信号固有的物理特性作为字典驱动先验，并结合测试时自适应策略，有效解决了深度学习去噪模型在不同地理区域TEM噪声特性差异下的泛化性问题，显著提升了去噪效果。

Abstract: Transient Electromagnetic (TEM) method is widely used in various geophysical
applications, providing valuable insights into subsurface properties. However,
time-domain TEM signals are often submerged in various types of noise. While
recent deep learning-based denoising models have shown strong performance,
these models are mostly trained on simulated or single real-world scenario
data, overlooking the significant differences in noise characteristics from
different geographical regions. Intuitively, models trained in one environment
often struggle to perform well in new settings due to differences in geological
conditions, equipment, and external interference, leading to reduced denoising
performance. To this end, we propose the Dictionary-driven Prior Regularization
Test-time Adaptation (DP-TTA). Our key insight is that TEM signals possess
intrinsic physical characteristics, such as exponential decay and smoothness,
which remain consistent across different regions regardless of external
conditions. These intrinsic characteristics serve as ideal prior knowledge for
guiding the TTA strategy, which helps the pre-trained model dynamically adjust
parameters by utilizing self-supervised losses, improving denoising performance
in new scenarios. To implement this, we customized a network, named DTEMDNet.
Specifically, we first use dictionary learning to encode these intrinsic
characteristics as a dictionary-driven prior, which is integrated into the
model during training. At the testing stage, this prior guides the model to
adapt dynamically to new environments by minimizing self-supervised losses
derived from the dictionary-driven consistency and the signal one-order
variation. Extensive experimental results demonstrate that the proposed method
achieves much better performance than existing TEM denoising methods and TTA
methods.

</details>


### [46] [STT-GS: Sample-Then-Transmit Edge Gaussian Splatting with Joint Client Selection and Power Control](https://arxiv.org/abs/2510.13186)
*Zhen Li,Xibin Jin,Guoliang Li,Shuai Wang,Miaowen Wen,Huseyin Arslan,Derrick Wing Kwan Ng,Chengzhong Xu*

Main category: cs.CV

TL;DR: 本文提出了一种针对边缘高斯溅射（EGS）的资源管理策略，旨在最大化高斯溅射（GS）质量。通过“先采样后传输”（STT-GS）策略解决因果困境，并利用特征域聚类（FDC）和联合客户端选择与功率控制（JCSPC）框架，在通信资源限制下优化GS性能。


<details>
  <summary>Details</summary>
Motivation: 传统的边缘资源管理方法主要关注通信吞吐量或通用学习性能，不适用于以最大化GS质量为目标的EGS。为解决此问题，需要一个新颖的、区分客户端异构视图贡献的GS导向目标函数。然而，评估该函数需要客户端图像，导致因果困境。

Method: 1. 提出一个新颖的GS导向目标函数，考虑不同客户端的异构视图贡献。2. 提出“先采样后传输”（STT-GS）策略：首先从每个客户端采样少量图像作为先导数据进行损失预测；然后根据评估结果优先向更有价值的客户端分配通信资源。3. 为实现高效采样，提出特征域聚类（FDC）方案选择最具代表性的数据，并采用先导传输时间最小化（PTTM）减少先导开销。4. 开发联合客户端选择与功率控制（JCSPC）框架，在通信资源约束下最大化GS导向函数。5. 针对非凸问题，提出基于惩罚交替主次最小化（PAMM）算法的低复杂度高效解决方案。

Result: 实验表明，所提出的方案在真实世界数据集上显著优于现有基准。研究发现，GS导向目标可以通过低采样率（例如10%）准确预测，并且该方法在视图贡献和通信成本之间实现了出色的权衡。

Conclusion: 本文提出的STT-GS策略和JCSPC框架能够有效解决EGS中的因果困境和资源分配问题，通过优化GS导向目标函数，显著提升了边缘高斯溅射的重建质量，并在通信成本和视图贡献之间取得了良好平衡。

Abstract: Edge Gaussian splatting (EGS), which aggregates data from distributed clients
and trains a global GS model at the edge server, is an emerging paradigm for
scene reconstruction. Unlike traditional edge resource management methods that
emphasize communication throughput or general-purpose learning performance, EGS
explicitly aims to maximize the GS qualities, rendering existing approaches
inapplicable. To address this problem, this paper formulates a novel
GS-oriented objective function that distinguishes the heterogeneous view
contributions of different clients. However, evaluating this function in turn
requires clients' images, leading to a causality dilemma. To this end, this
paper further proposes a sample-then-transmit EGS (or STT-GS for short)
strategy, which first samples a subset of images as pilot data from each client
for loss prediction. Based on the first-stage evaluation, communication
resources are then prioritized towards more valuable clients. To achieve
efficient sampling, a feature-domain clustering (FDC) scheme is proposed to
select the most representative data and pilot transmission time minimization
(PTTM) is adopted to reduce the pilot overhead.Subsequently, we develop a joint
client selection and power control (JCSPC) framework to maximize the
GS-oriented function under communication resource constraints. Despite the
nonconvexity of the problem, we propose a low-complexity efficient solution
based on the penalty alternating majorization minimization (PAMM) algorithm.
Experiments unveil that the proposed scheme significantly outperforms existing
benchmarks on real-world datasets. It is found that the GS-oriented objective
can be accurately predicted with low sampling ratios (e.g.,10%), and our method
achieves an excellent tradeoff between view contributions and communication
costs.

</details>


### [47] [Complementary Information Guided Occupancy Prediction via Multi-Level Representation Fusion](https://arxiv.org/abs/2510.13198)
*Rongtao Xu,Jinzhou Lin,Jialei Zhou,Jiahua Dong,Changwei Wang,Ruisheng Wang,Li Guo,Shibiao Xu,Xiaodan Liang*

Main category: cs.CV

TL;DR: 本文提出了CIGOcc，一个基于多级表示融合的两阶段占用预测框架，通过融合分割、图形和深度特征并结合SAM知识蒸馏，在不增加训练成本的情况下，在SemanticKITTI基准上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的相机占用预测方法主要通过结构修改来提升性能，但效果有限。很少有研究从表示融合的角度进行探索，导致2D图像中丰富的特征多样性未被充分利用。

Method: 本文提出了CIGOcc框架，包括两个阶段：1) 从输入图像中提取分割、图形和深度特征；2) 引入可变形的多级融合机制来融合这三种多级特征。此外，CIGOcc还融入了从SAM中蒸馏的知识，以进一步提高预测精度。

Result: CIGOcc在不增加训练成本的情况下，在SemanticKITTI基准测试上实现了最先进的性能。

Conclusion: CIGOcc通过创新的多级特征融合和SAM知识蒸馏，有效提升了相机占用预测的准确性，达到了行业领先水平，为未来研究提供了新方向。

Abstract: Camera-based occupancy prediction is a mainstream approach for 3D perception
in autonomous driving, aiming to infer complete 3D scene geometry and semantics
from 2D images. Almost existing methods focus on improving performance through
structural modifications, such as lightweight backbones and complex cascaded
frameworks, with good yet limited performance. Few studies explore from the
perspective of representation fusion, leaving the rich diversity of features in
2D images underutilized. Motivated by this, we propose \textbf{CIGOcc, a
two-stage occupancy prediction framework based on multi-level representation
fusion. \textbf{CIGOcc extracts segmentation, graphics, and depth features from
an input image and introduces a deformable multi-level fusion mechanism to fuse
these three multi-level features. Additionally, CIGOcc incorporates knowledge
distilled from SAM to further enhance prediction accuracy. Without increasing
training costs, CIGOcc achieves state-of-the-art performance on the
SemanticKITTI benchmark. The code is provided in the supplementary material and
will be released https://github.com/VitaLemonTea1/CIGOcc

</details>


### [48] [Paper Copilot: Tracking the Evolution of Peer Review in AI Conferences](https://arxiv.org/abs/2510.13201)
*Jing Yang,Qiyao Wei,Jiaxin Pei*

Main category: cs.CV

TL;DR: 本文介绍了Paper Copilot系统，旨在创建计算机科学领域同行评审的持久数字档案和开放数据集，并对ICLR评审进行了大规模实证分析，以支持对同行评审演变的可重复研究。


<details>
  <summary>Details</summary>
Motivation: AI会议的快速增长给同行评审系统带来了巨大压力，导致审稿人工作量过大、专业不匹配、评估标准不一致、审稿质量下降以及问责制不足。会议组织者采取的临时性政策往往造成更多混乱，使得评审过程及其年度演变不透明。

Method: 研究团队开发了Paper Copilot系统，用于创建广泛计算机科学会议的同行评审持久数字档案。他们构建了一个开放数据集，并对跨多年份的ICLR评审进行了大规模实证分析。同时，他们发布了基础设施和数据集。

Result: Paper Copilot成功创建了同行评审的持久数字档案和开放数据集，使得研究人员能够大规模研究同行评审。通过对ICLR评审的分析，该系统支持了同行评审演变的可重复研究。

Conclusion: Paper Copilot提供的基础设施和数据集将帮助社区追踪同行评审的变化、诊断失败模式，并为基于证据的改进提供信息，以期建立一个更稳健、透明和可靠的同行评审系统。

Abstract: The rapid growth of AI conferences is straining an already fragile
peer-review system, leading to heavy reviewer workloads, expertise mismatches,
inconsistent evaluation standards, superficial or templated reviews, and
limited accountability under compressed timelines. In response, conference
organizers have introduced new policies and interventions to preserve review
standards. Yet these ad-hoc changes often create further concerns and confusion
about the review process, leaving how papers are ultimately accepted - and how
practices evolve across years - largely opaque. We present Paper Copilot, a
system that creates durable digital archives of peer reviews across a wide
range of computer-science venues, an open dataset that enables researchers to
study peer review at scale, and a large-scale empirical analysis of ICLR
reviews spanning multiple years. By releasing both the infrastructure and the
dataset, Paper Copilot supports reproducible research on the evolution of peer
review. We hope these resources help the community track changes, diagnose
failure modes, and inform evidence-based improvements toward a more robust,
transparent, and reliable peer-review system.

</details>


### [49] [MimicParts: Part-aware Style Injection for Speech-Driven 3D Motion Generation](https://arxiv.org/abs/2510.13208)
*Lianlian Liu,YongKang He,Zhaojie Chu,Xiaofen Xing,Xiangmin Xu*

Main category: cs.CV

TL;DR: MimicParts是一种新颖的框架，通过部分感知风格注入和去噪网络，解决了从语音生成风格化3D人体运动的挑战，实现了更自然和富有表现力的运动序列。


<details>
  <summary>Details</summary>
Motivation: 从语音生成风格化3D人体运动面临挑战，包括语音、个人风格和身体运动之间复杂且细微的关系。现有方法要么过度简化风格多样性，要么忽略区域运动风格差异（如上半身与下半身），限制了运动真实感。此外，运动风格应动态适应语音节奏和情感变化，但现有方法往往忽视这一点。

Method: 本文提出了MimicParts框架。它将身体划分为不同区域以编码局部运动风格，从而捕捉细粒度的区域差异。该框架包含部分感知风格注入和部分感知去噪网络。此外，其部分感知注意力模块允许节奏和情感线索精确指导每个身体区域，确保生成的运动与语音节奏和情感状态的变化对齐。

Result: 实验结果表明，该方法优于现有方法，展示了生成3D人体运动序列的自然性和表现力。

Conclusion: MimicParts通过其部分感知风格注入和去噪网络，有效解决了从语音生成风格化3D人体运动的复杂性，显著提升了生成运动的真实感、区域特异性和对语音动态变化的适应性。

Abstract: Generating stylized 3D human motion from speech signals presents substantial
challenges, primarily due to the intricate and fine-grained relationships among
speech signals, individual styles, and the corresponding body movements.
Current style encoding approaches either oversimplify stylistic diversity or
ignore regional motion style differences (e.g., upper vs. lower body), limiting
motion realism. Additionally, motion style should dynamically adapt to changes
in speech rhythm and emotion, but existing methods often overlook this. To
address these issues, we propose MimicParts, a novel framework designed to
enhance stylized motion generation based on part-aware style injection and
part-aware denoising network. It divides the body into different regions to
encode localized motion styles, enabling the model to capture fine-grained
regional differences. Furthermore, our part-aware attention block allows rhythm
and emotion cues to guide each body region precisely, ensuring that the
generated motion aligns with variations in speech rhythm and emotional state.
Experimental results show that our method outperforming existing methods
showcasing naturalness and expressive 3D human motion sequences.

</details>


### [50] [Prompt-based Adaptation in Large-scale Vision Models: A Survey](https://arxiv.org/abs/2510.13219)
*Xi Xiao,Yunbei Zhang,Lin Zhao,Yiyang Liu,Xiaoying Liao,Zheda Mai,Xingjian Li,Xiao Wang,Hao Xu,Jihun Hamm,Xue Lin,Min Xu,Qifan Wang,Tianyang Wang,Cheng Han*

Main category: cs.CV

TL;DR: 本综述首次全面地回顾了视觉提示（VP）和视觉提示微调（VPT）方法，将其统一为基于提示的适应（PA）框架，并提供了详细的分类、应用领域、基准、挑战和未来方向，旨在为研究人员提供清晰的路线图。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉提示（VP）和视觉提示微调（VPT）是适应大型视觉模型的轻量级有效方法，但它们的概念界限模糊，在当前研究中常被互换使用，缺乏系统性的区分。

Method: 作者从第一性原理重新审视VP和VPT的设计，将其概念化为统一的基于提示的适应（PA）框架。提出了一个分类法，将现有方法分为可学习、生成式和不可学习提示，并按注入粒度（像素级和令牌级）进行组织。此外，还探讨了PA在医学成像、3D点云和视觉-语言任务等不同领域的整合，以及其在测试时间适应和可信AI中的作用。最后，总结了当前的基准并识别了关键挑战和未来方向。

Result: 本综述成功地将VP和VPT统一到基于提示的适应（PA）框架下，提供了清晰的分类法，涵盖了可学习、生成式和非可学习提示，以及像素级和令牌级注入。文章还展示了PA在多个领域的广泛应用，包括医学成像、3D点云和视觉-语言任务，并讨论了其在测试时间适应和可信AI中的潜力。同时，总结了现有基准，并指出了该领域面临的关键挑战和未来发展方向。

Conclusion: 本综述首次全面地致力于基于提示的适应（PA）的方法论和应用，旨在为研究人员和从业者提供一个清晰的路线图，以理解和探索PA相关研究的不断发展格局。

Abstract: In computer vision, Visual Prompting (VP) and Visual Prompt Tuning (VPT) have
recently emerged as lightweight and effective alternatives to full fine-tuning
for adapting large-scale vision models within the ``pretrain-then-finetune''
paradigm. However, despite rapid progress, their conceptual boundaries remain
blurred, as VP and VPT are frequently used interchangeably in current research,
reflecting a lack of systematic distinction between these techniques and their
respective applications. In this survey, we revisit the designs of VP and VPT
from first principles, and conceptualize them within a unified framework termed
Prompt-based Adaptation (PA). We provide a taxonomy that categorizes existing
methods into learnable, generative, and non-learnable prompts, and further
organizes them by injection granularity -- pixel-level and token-level. Beyond
the core methodologies, we examine PA's integrations across diverse domains,
including medical imaging, 3D point clouds, and vision-language tasks, as well
as its role in test-time adaptation and trustworthy AI. We also summarize
current benchmarks and identify key challenges and future directions. To the
best of our knowledge, we are the first comprehensive survey dedicated to PA's
methodologies and applications in light of their distinct characteristics. Our
survey aims to provide a clear roadmap for researchers and practitioners in all
area to understand and explore the evolving landscape of PA-related research.

</details>


### [51] [Sample-Centric Multi-Task Learning for Detection and Segmentation of Industrial Surface Defects](https://arxiv.org/abs/2510.13226)
*Hang-Cheng Dong,Yibo Jiao,Fupeng Wei,Guodong Liu,Dong Ye,Bingguo Liu*

Main category: cs.CV

TL;DR: 针对工业表面缺陷检测中存在的像素级训练与样本级质量控制决策不匹配问题，本文提出了一种以样本为中心的多任务学习框架和评估指标，通过联合学习样本级分类和像素级定位，显著提高了样本级决策的可靠性和缺陷定位的完整性。


<details>
  <summary>Details</summary>
Motivation: 工业表面缺陷检测面临极端前景-背景不平衡、缺陷稀疏且尺度分布长尾、低对比度等挑战。现有像素级训练方法易受大面积均匀区域主导，难以关注小或低对比度缺陷。尽管像素重叠指标（如mIoU）表现良好，但样本级稳定性不足，尤其对于稀疏或细长缺陷。根本原因是优化目标与质量控制（QC）决策粒度不匹配。

Method: 提出了一种以样本为中心的多任务学习框架和评估套件。该方法基于共享编码器架构，共同学习样本级缺陷分类和像素级掩膜定位。样本级监督调节特征分布，并在梯度层面持续提升对小缺陷和低对比度缺陷的召回率，而分割分支则保留边界和形状细节，以增强每样本决策的稳定性并减少漏检。同时，提出了决策关联指标Seg_mIoU和Seg_Recall，以消除经典mIoU因空样本或真阴性样本造成的偏差，并紧密耦合定位质量与样本级决策。

Result: 在两个基准数据集上的实验表明，该方法显著提高了样本级决策的可靠性和缺陷定位的完整性。

Conclusion: 通过提出以样本为中心的多任务学习框架和决策关联评估指标，有效解决了工业表面缺陷检测中优化目标与质量控制决策不匹配的问题，从而在实际生产线中实现了更可靠的样本级质量控制决策和更完整的缺陷定位。

Abstract: Industrial surface defect inspection for sample-wise quality control (QC)
must simultaneously decide whether a given sample contains defects and localize
those defects spatially. In real production lines, extreme
foreground-background imbalance, defect sparsity with a long-tailed scale
distribution, and low contrast are common. As a result, pixel-centric training
and evaluation are easily dominated by large homogeneous regions, making it
difficult to drive models to attend to small or low-contrast defects-one of the
main bottlenecks for deployment. Empirically, existing models achieve strong
pixel-overlap metrics (e.g., mIoU) but exhibit insufficient stability at the
sample level, especially for sparse or slender defects. The root cause is a
mismatch between the optimization objective and the granularity of QC
decisions. To address this, we propose a sample-centric multi-task learning
framework and evaluation suite. Built on a shared-encoder architecture, the
method jointly learns sample-level defect classification and pixel-level mask
localization. Sample-level supervision modulates the feature distribution and,
at the gradient level, continually boosts recall for small and low-contrast
defects, while the segmentation branch preserves boundary and shape details to
enhance per-sample decision stability and reduce misses. For evaluation, we
propose decision-linked metrics, Seg_mIoU and Seg_Recall, which remove the bias
of classical mIoU caused by empty or true-negative samples and tightly couple
localization quality with sample-level decisions. Experiments on two benchmark
datasets demonstrate that our approach substantially improves the reliability
of sample-level decisions and the completeness of defect localization.

</details>


### [52] [UniVector: Unified Vector Extraction via Instance-Geometry Interaction](https://arxiv.org/abs/2510.13234)
*Yinglong Yan,Jun Yue,Shaobo Xia,Hanmeng Sun,Tianxu Ying,Chengcheng Wu,Sifan Lan,Min He,Pedram Ghamisi,Leyuan Fang*

Main category: cs.CV

TL;DR: UniVector是一个统一的向量提取框架，它利用实例-几何交互，能在一个模型中同时提取多种向量类型（多边形、多段线、线段），并在单结构和多结构任务上均达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 现有的向量提取方法通常针对单一向量类型（如多边形、多段线、线段），需要为不同结构构建单独的模型。这是因为它们独立处理实例属性（类别、结构）和几何属性（点坐标、连接），限制了捕捉复杂结构的能力。

Method: UniVector将向量编码为包含实例级和几何级信息的结构化查询，并通过交互模块迭代更新以实现跨层上下文交换。此外，还引入了动态形状约束来进一步优化全局结构和关键点。为评估多结构场景，作者还提出了一个包含多种多边形、多段线和线段的Multi-Vector数据集。

Result: 实验结果表明，UniVector在单结构和多结构向量提取任务上均达到了新的最先进水平。

Conclusion: UniVector成功地提出了一个统一的向量提取框架，通过利用实例-几何交互，在一个模型中实现了多种向量类型的提取，解决了现有方法在处理复杂结构时的局限性。

Abstract: Vector extraction retrieves structured vector geometry from raster images,
offering high-fidelity representation and broad applicability. Existing
methods, however, are usually tailored to a single vector type (e.g., polygons,
polylines, line segments), requiring separate models for different structures.
This stems from treating instance attributes (category, structure) and
geometric attributes (point coordinates, connections) independently, limiting
the ability to capture complex structures. Inspired by the human brain's
simultaneous use of semantic and spatial interactions in visual perception, we
propose UniVector, a unified VE framework that leverages instance-geometry
interaction to extract multiple vector types within a single model. UniVector
encodes vectors as structured queries containing both instance- and
geometry-level information, and iteratively updates them through an interaction
module for cross-level context exchange. A dynamic shape constraint further
refines global structures and key points. To benchmark multi-structure
scenarios, we introduce the Multi-Vector dataset with diverse polygons,
polylines, and line segments. Experiments show UniVector sets a new state of
the art on both single- and multi-structure VE tasks. Code and dataset will be
released at https://github.com/yyyyll0ss/UniVector.

</details>


### [53] [EPIPTrack: Rethinking Prompt Modeling with Explicit and Implicit Prompts for Multi-Object Tracking](https://arxiv.org/abs/2510.13235)
*Yukuan Zhang,Jiarui Zhao,Shangqing Nie,Jin Kuang,Shengsheng Wang*

Main category: cs.CV

TL;DR: EPIPTrack是一个统一的多模态视觉-语言跟踪框架，通过动态调整显式和隐式提示来解决现有方法静态文本描述缺乏适应性和易产生幻觉的问题，显著提升了目标跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于大型语言模型的静态文本描述，这导致它们难以适应目标状态的实时变化，并且容易产生幻觉，从而限制了多模态语义线索在目标跟踪中的潜力。

Method: 本文提出了EPIPTrack框架，利用显式和隐式提示进行动态目标建模和语义对齐。显式提示将空间运动信息转换为自然语言描述以提供时空指导；隐式提示结合伪词和可学习描述符来构建捕捉外观属性的个性化知识表示。这两种提示都通过CLIP文本编码器进行动态调整，以响应目标状态变化。此外，还设计了一个判别性特征增强器来增强视觉和跨模态表示。

Result: 在MOT17、MOT20和DanceTrack数据集上的大量实验表明，EPIPTrack在多种场景下均优于现有跟踪器，展现出强大的适应性和卓越的性能。

Conclusion: EPIPTrack通过引入动态调整的显式和隐式提示，成功克服了传统多模态跟踪方法中静态文本描述的局限性，实现了对目标状态变化的有效适应和更强的语义对齐，从而显著提升了目标跟踪的鲁棒性和性能。

Abstract: Multimodal semantic cues, such as textual descriptions, have shown strong
potential in enhancing target perception for tracking. However, existing
methods rely on static textual descriptions from large language models, which
lack adaptability to real-time target state changes and prone to
hallucinations. To address these challenges, we propose a unified multimodal
vision-language tracking framework, named EPIPTrack, which leverages explicit
and implicit prompts for dynamic target modeling and semantic alignment.
Specifically, explicit prompts transform spatial motion information into
natural language descriptions to provide spatiotemporal guidance. Implicit
prompts combine pseudo-words with learnable descriptors to construct
individualized knowledge representations capturing appearance attributes. Both
prompts undergo dynamic adjustment via the CLIP text encoder to respond to
changes in target state. Furthermore, we design a Discriminative Feature
Augmentor to enhance visual and cross-modal representations. Extensive
experiments on MOT17, MOT20, and DanceTrack demonstrate that EPIPTrack
outperforms existing trackers in diverse scenarios, exhibiting robust
adaptability and superior performance.

</details>


### [54] [Real-Time Crowd Counting for Embedded Systems with Lightweight Architecture](https://arxiv.org/abs/2510.13250)
*Zhiyuan Zhao,Yubin Wen,Siyu Yang,Lichen Ning,Yuandong Liu,Junyu Gao*

Main category: cs.CV

TL;DR: 本文提出了一种名为“超实时”的基于Stem-Encoder-Decoder结构的轻量级人群计数模型，通过大卷积核、条件通道加权、多分支局部融合和FPN，实现了嵌入式系统上最快的推理速度和有竞争力的精度。


<details>
  <summary>Details</summary>
Motivation: 现有的人群计数方法在嵌入式系统实际应用中存在模型参数过多、计算复杂等问题，无法满足实时性要求。因此，需要设计一种速度足够快的模型。

Method: 该模型采用Stem-Encoder-Decoder结构。首先，在Stem网络中使用大卷积核来扩大感受野，有效提取头部细节信息。其次，在Encoder部分，利用条件通道加权和多分支局部融合模块，以低计算成本融合多尺度特征，这是实现超实时性能的关键。最后，在Encoder顶部添加特征金字塔网络（FPN）以缓解特征融合不完全的问题。

Result: 在三个基准数据集上的实验表明，该网络适用于嵌入式系统上的超实时人群计数，在保持有竞争力的精度的同时，推理速度最快。具体而言，在NVIDIA GTX 1080Ti上达到381.7 FPS，在NVIDIA Jetson TX1上达到71.9 FPS。

Conclusion: 所提出的网络能够满足嵌入式系统超实时人群计数的实际应用需求，在保证竞争性准确率的同时，实现了最快的推理速度。

Abstract: Crowd counting is a task of estimating the number of the crowd through
images, which is extremely valuable in the fields of intelligent security,
urban planning, public safety management, and so on. However, the existing
counting methods have some problems in practical application on embedded
systems for these fields, such as excessive model parameters, abundant complex
calculations, etc. The practical application of embedded systems requires the
model to be real-time, which means that the model is fast enough. Considering
the aforementioned problems, we design a super real-time model with a
stem-encoder-decoder structure for crowd counting tasks, which achieves the
fastest inference compared with state-of-the-arts. Firstly, large convolution
kernels in the stem network are used to enlarge the receptive field, which
effectively extracts detailed head information. Then, in the encoder part, we
use conditional channel weighting and multi-branch local fusion block to merge
multi-scale features with low computational consumption. This part is crucial
to the super real-time performance of the model. Finally, the feature pyramid
networks are added to the top of the encoder to alleviate its incomplete fusion
problems. Experiments on three benchmarks show that our network is suitable for
super real-time crowd counting on embedded systems, ensuring competitive
accuracy. At the same time, the proposed network reasoning speed is the
fastest. Specifically, the proposed network achieves 381.7 FPS on NVIDIA GTX
1080Ti and 71.9 FPS on NVIDIA Jetson TX1.

</details>


### [55] [What "Not" to Detect: Negation-Aware VLMs via Structured Reasoning and Token Merging](https://arxiv.org/abs/2510.13232)
*Inha Kang,Youngsun Lim,Seonho Lee,Jiho Choi,Junsuk Choe,Hyunjung Shim*

Main category: cs.CV

TL;DR: 现有视觉-语言模型（VLMs）在理解否定句方面存在严重缺陷（肯定偏差），尤其是在描述对象检测（DOD）任务中。本文提出CoVaND数据集（用于生成高质量否定数据）和NegToMe模块（通过文本token合并解决否定线索丢失问题），结合LoRA微调，显著提升了模型在否定基准测试上的性能。


<details>
  <summary>Details</summary>
Motivation: 最先进的视觉-语言模型（VLMs）在理解否定方面存在关键缺陷，即所谓的“肯定偏差”，这在描述对象检测（DOD）任务中尤为严重。

Method: 本文提出两项主要贡献：1. CoVAND数据集：通过系统性的思维链（CoT）和VQA管道构建，用于生成高质量、实例接地（instance-grounded）的否定数据。2. NegToMe模块：一种新颖、轻量级的文本token合并模块，直接解决肯定偏差的架构原因。它通过在token化过程中将否定线索与属性合并成连贯的语义短语，从根本上解决否定线索的结构性丢失，从而在输入层面保持正确的极性。该模块与参数高效且具有策略性的LoRA微调方法相结合。

Result: 我们的方法显著提高了模型在具有挑战性的否定基准测试上的性能，降低了假阳性率，在OVDEval上将NMS-AP提升了高达+10.8个百分点，并展示了对SoTA VLM的泛化能力。

Conclusion: 这项工作在解决现实世界检测应用中的否定理解问题上迈出了关键一步。

Abstract: State-of-the-art vision-language models (VLMs) suffer from a critical failure
in understanding negation, often referred to as affirmative bias. This
limitation is particularly severe in described object detection (DOD) tasks. To
address this, we propose two primary contributions: (1) a new dataset pipeline
and (2) a novel, lightweight adaptation recipe. First, we introduce CoVAND, a
dataset constructed with a systematic chain-of-thought (CoT) and VQA-based
pipeline to generate high-quality, instance-grounded negation data. Second, we
propose NegToMe, a novel text token merging module that directly tackles the
architectural cause of affirmative bias. NegToMe fundamentally addresses the
structural loss of negation cues in tokenization, grouping them with attributes
into coherent semantic phrases. It maintains correct polarity at the input
level, enabling robust negation understanding even with limited data. For
instance, to prevent a model from treating the fragmented tokens "not" and
"girl" as simply "girl", NegToMe binds them into a single token whose meaning
is correctly distinguished from that of "girl" alone. This module is integrated
with a parameter-efficient and strategic LoRA fine-tuning approach. Our method
significantly improves performance on challenging negation benchmarks with a
lowered false positive rate, boosting NMS-AP by up to +10.8 points on OVDEval
and demonstrating generalization to SoTA VLMs. This work marks a crucial step
forward in addressing negation understanding for real-world detection
applications.

</details>


### [56] [Model-agnostic Adversarial Attack and Defense for Vision-Language-Action Models](https://arxiv.org/abs/2510.13237)
*Haochuan Xu,Yun Sing Koh,Shuhuai Huang,Zirun Zhou,Di Wang,Jun Sakuma,Jingfeng Zhang*

Main category: cs.CV

TL;DR: VLA模型在机器人学习中取得进展，但对抗鲁棒性不足。本文提出一种模型无关的对抗补丁攻击EDPA及其防御策略。EDPA通过破坏视觉与文本的语义对齐和最大化潜在表示差异来使任务失败，而防御通过微调视觉编码器来缓解，并在LIBERO基准上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言-动作（VLA）模型在机器人学习中取得了革命性进展，使机器人能根据自然语言指令执行复杂任务。然而，这些模型的对抗鲁棒性仍未被充分探索。

Method: 本文提出两种方法：
1.  **对抗补丁攻击 (EDPA)**：这是一种模型无关的攻击方法，生成可直接放置在摄像头视野内的补丁。它不需要VLA模型架构或机器人机械臂的先验知识。EDPA通过优化两个目标来构建补丁：(i) 破坏视觉和文本潜在表示之间的语义对齐；(ii) 最大化对抗性输入与对应干净视觉输入之间潜在表示的差异。
2.  **防御策略**：提出一种针对视觉编码器的对抗性微调方案，优化编码器以使其能为干净和对抗性扰动的视觉输入生成相似的潜在表示。

Result: 在广泛认可的LIBERO机器人模拟基准上进行评估，结果表明：
1.  EDPA显著增加了最先进VLA模型的任务失败率。
2.  本文提出的防御策略能有效缓解EDPA造成的性能下降。

Conclusion: VLA模型面临对抗性威胁，本文提出的模型无关对抗补丁攻击EDPA能有效导致任务失败。同时，所提出的视觉编码器对抗性微调防御策略能有效缓解这种攻击，凸显了VLA模型对抗鲁棒性的重要性及有效防御的可能性。

Abstract: Vision-Language-Action (VLA) models have achieved revolutionary progress in
robot learning, enabling robots to execute complex physical robot tasks from
natural language instructions. Despite this progress, their adversarial
robustness remains underexplored. In this work, we propose both adversarial
patch attack and corresponding defense strategies for VLA models. We first
introduce the Embedding Disruption Patch Attack (EDPA), a model-agnostic
adversarial attack that generates patches directly placeable within the
camera's view. In comparison to prior methods, EDPA can be readily applied to
different VLA models without requiring prior knowledge of the model
architecture, or the controlled robotic manipulator. EDPA constructs these
patches by (i) disrupting the semantic alignment between visual and textual
latent representations, and (ii) maximizing the discrepancy of latent
representations between adversarial and corresponding clean visual inputs.
Through the optimization of these objectives, EDPA distorts the VLA's
interpretation of visual information, causing the model to repeatedly generate
incorrect actions and ultimately result in failure to complete the given
robotic task. To counter this, we propose an adversarial fine-tuning scheme for
the visual encoder, in which the encoder is optimized to produce similar latent
representations for both clean and adversarially perturbed visual inputs.
Extensive evaluations on the widely recognized LIBERO robotic simulation
benchmark demonstrate that EDPA substantially increases the task failure rate
of cutting-edge VLA models, while our proposed defense effectively mitigates
this degradation. The codebase is accessible via the homepage at
https://edpa-attack.github.io/.

</details>


### [57] [Through the Lens of Doubt: Robust and Efficient Uncertainty Estimation for Visual Place Recognition](https://arxiv.org/abs/2510.13464)
*Emily Miller,Michael Milford,Muhammad Burhan Hafez,SD Ramchurn,Shoaib Ehsan*

Main category: cs.CV

TL;DR: 本文提出三种免训练的不确定性指标（SD, RS, SU），通过分析VPR相似度分数来估计匹配置信度，有效提升VPR在不同环境下的鲁棒性和查准率-查全率性能，且计算开销可忽略。


<details>
  <summary>Details</summary>
Motivation: 视觉地点识别（VPR）系统在不同视觉环境、光照、季节和视角变化下部署时面临巨大挑战。对于SLAM等关键应用，需要鲁棒地估计地点匹配的不确定性，以确保系统的可靠性。

Method: 本文提出三种免训练的不确定性指标：相似度分布（SD）量化匹配的独特性；比率离散度（RS）评估顶部分数位置之间的竞争模糊性；统计不确定性（SU）是SD和RS的组合，提供一个统一的指标。这些指标通过分析任何现有VPR方法的相似度分数中的固有统计模式来估计预测置信度，无需额外的模型训练、架构修改或计算昂贵的几何验证。

Result: 通过对九种最先进的VPR方法和六个基准数据集的全面评估，结果证实所提出的指标在区分正确和不正确的VPR匹配方面表现出色，并且始终优于现有方法，同时保持可忽略的计算开销。这使得它们能够部署于实时机器人应用中，在各种环境条件下提高查准率-查全率性能。

Conclusion: 本文提出的免训练不确定性指标能够有效估计VPR匹配的置信度，显著提升VPR系统在复杂多变环境下的鲁棒性和性能，且计算效率高，适用于实时机器人应用。

Abstract: Visual Place Recognition (VPR) enables robots and autonomous vehicles to
identify previously visited locations by matching current observations against
a database of known places. However, VPR systems face significant challenges
when deployed across varying visual environments, lighting conditions, seasonal
changes, and viewpoints changes. Failure-critical VPR applications, such as
loop closure detection in simultaneous localization and mapping (SLAM)
pipelines, require robust estimation of place matching uncertainty. We propose
three training-free uncertainty metrics that estimate prediction confidence by
analyzing inherent statistical patterns in similarity scores from any existing
VPR method. Similarity Distribution (SD) quantifies match distinctiveness by
measuring score separation between candidates; Ratio Spread (RS) evaluates
competitive ambiguity among top-scoring locations; and Statistical Uncertainty
(SU) is a combination of SD and RS that provides a unified metric that
generalizes across datasets and VPR methods without requiring validation data
to select the optimal metric. All three metrics operate without additional
model training, architectural modifications, or computationally expensive
geometric verification. Comprehensive evaluation across nine state-of-the-art
VPR methods and six benchmark datasets confirms that our metrics excel at
discriminating between correct and incorrect VPR matches, and consistently
outperform existing approaches while maintaining negligible computational
overhead, making it deployable for real-time robotic applications across varied
environmental conditions with improved precision-recall performance.

</details>


### [58] [FlyAwareV2: A Multimodal Cross-Domain UAV Dataset for Urban Scene Understanding](https://arxiv.org/abs/2510.13243)
*Francesco Barbato,Matteo Caligiuri,Pietro Zanuttigh*

Main category: cs.CV

TL;DR: 本文介绍了FlyAwareV2，一个用于无人机城市场景理解的新型多模态数据集，它结合了真实和合成图像，旨在解决数据收集和标注的挑战。


<details>
  <summary>Details</summary>
Motivation: 开发用于城市环境中无人机应用的计算机视觉算法，严重依赖于大规模、精确标注的数据集。然而，收集和标注真实世界的无人机数据既极具挑战性又成本高昂。

Method: FlyAwareV2是基于SynDrone和FlyAware数据集构建的，并引入了以下新贡献：1) 包含RGB、深度和语义标签的多模态数据，涵盖不同天气和昼夜等多种环境条件；2) 通过先进的单目深度估计技术计算真实样本的深度图；3) 为标准架构上的RGB和多模态语义分割提供了基准；4) 进行了合成到真实域适应研究，以评估模型在合成数据上训练后的泛化能力。

Result: FlyAwareV2提供了RGB和多模态语义分割的基准，并支持对合成到真实域适应的研究，以评估模型在合成数据上训练后的泛化能力。

Conclusion: 凭借其丰富的标注和环境多样性，FlyAwareV2为基于无人机的3D城市场景理解研究提供了宝贵的资源。

Abstract: The development of computer vision algorithms for Unmanned Aerial Vehicle
(UAV) applications in urban environments heavily relies on the availability of
large-scale datasets with accurate annotations. However, collecting and
annotating real-world UAV data is extremely challenging and costly. To address
this limitation, we present FlyAwareV2, a novel multimodal dataset encompassing
both real and synthetic UAV imagery tailored for urban scene understanding
tasks. Building upon the recently introduced SynDrone and FlyAware datasets,
FlyAwareV2 introduces several new key contributions: 1) Multimodal data (RGB,
depth, semantic labels) across diverse environmental conditions including
varying weather and daytime; 2) Depth maps for real samples computed via
state-of-the-art monocular depth estimation; 3) Benchmarks for RGB and
multimodal semantic segmentation on standard architectures; 4) Studies on
synthetic-to-real domain adaptation to assess the generalization capabilities
of models trained on the synthetic data. With its rich set of annotations and
environmental diversity, FlyAwareV2 provides a valuable resource for research
on UAV-based 3D urban scene understanding.

</details>


### [59] [Self-Augmented Visual Contrastive Decoding](https://arxiv.org/abs/2510.13315)
*Eun Woo Im,Muhammad Kashif Ali,Vivek Gupta*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的解码策略，通过自增强提示和自适应阈值算法，有效缓解了大型视觉语言模型（LVLMs）的幻觉问题，显著提高了事实一致性。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型（LVLMs）存在幻觉问题，现有视觉对比解码方法使用的视觉增强与文本查询缺乏特异性上下文关联，限制了其有效性。

Method: 本文提出了一种新颖的无需训练的解码策略，包含两项主要贡献：1) 自增强提示策略，利用模型内在知识动态对齐查询与视觉增强的语义；2) 自适应阈值算法，根据输出稀疏性自适应调整下一个词元候选集大小，并利用完整的Logit分布信息。

Result: 在四个LVLMs和七个基准测试上的大量实验表明，所提出的解码策略与现有最先进的解码方法相比，显著增强了事实一致性。

Conclusion: 这项工作强调了整合查询依赖增强和熵感知解码对于提高LVLMs有效生成的重要性。

Abstract: Large Vision-Language Models (LVLMs) have demonstrated remarkable multimodal
capabilities, but they inherit the tendency to hallucinate from their
underlying language models. While visual contrastive decoding has been proposed
to mitigate this issue, existing methods often apply generic visual
augmentations that disregard the specific context provided by the text query,
limiting their effectiveness. This study introduces a novel training-free
decoding strategy that addresses these limitations, featuring two key
contributions. First, a self-augmentation prompting strategy that leverages the
intrinsic knowledge of the model to dynamically align semantics between the
query and the visual augmentation. Second, an adaptive thresholding algorithm
that adaptively adjusts next token candidate size based on the output sparsity,
utilizing full information from the logit distribution. Extensive experiments
across four LVLMs and seven benchmarks demonstrate that the proposed decoding
significantly enhances factual consistency compared to state-of-the-art
decoding methods. This work highlights the importance of integrating
query-dependent augmentation and entropy-aware decoding for improving effective
generation of LVLMs.

</details>


### [60] [Accelerated Feature Detectors for Visual SLAM: A Comparative Study of FPGA vs GPU](https://arxiv.org/abs/2510.13546)
*Ruiqi Ye,Mikel Luján*

Main category: cs.CV

TL;DR: 本文首次在V-SLAM管线中比较了现代SoC上GPU和FPGA加速特征检测器的性能，发现FPGA在学习型检测器（如SuperPoint）上表现出更好的运行时性能和能效。


<details>
  <summary>Details</summary>
Motivation: 特征检测是SLAM中耗时的模块，尤其在无人机等功耗受限平台上。因此，需要硬件加速来提高性能和效率。

Method: 本文在Visual SLAM (V-SLAM) 管线中，比较了Nvidia Jetson Orin (GPU) 和 AMD Versal (FPGA) 现代SoC上，针对非学习型（FAST、Harris）和学习型（SuperPoint）特征检测器的最佳实现，评估了它们的运行时性能、能效和V-SLAM精度。

Result: 对于非学习型检测器（FAST、Harris），GPU实现在运行时性能和能效上优于FPGA。然而，对于学习型检测器（SuperPoint），FPGA实现在运行时性能（高达3.1倍）和能效（高达1.4倍）上优于GPU。FPGA加速的V-SLAM在某些数据集上能达到与GPU加速V-SLAM相当的运行时性能，但在总体精度上GPU加速的V-SLAM通常更高。通过特征检测的硬件加速，可以减少全局束调整的调用频率，从而进一步提升V-SLAM管线性能且不牺牲精度。

Conclusion: 在V-SLAM中，对于非学习型特征检测器，GPU表现更优；而对于学习型特征检测器（如SuperPoint），FPGA能提供显著的运行时性能和能效优势。尽管GPU在精度上通常领先，但特征检测的硬件加速可以有效提升V-SLAM的整体性能。

Abstract: Feature detection is a common yet time-consuming module in Simultaneous
Localization and Mapping (SLAM) implementations, which are increasingly
deployed on power-constrained platforms, such as drones. Graphics Processing
Units (GPUs) have been a popular accelerator for computer vision in general,
and feature detection and SLAM in particular.
  On the other hand, System-on-Chips (SoCs) with integrated Field Programmable
Gate Array (FPGA) are also widely available. This paper presents the first
study of hardware-accelerated feature detectors considering a Visual SLAM
(V-SLAM) pipeline. We offer new insights by comparing the best GPU-accelerated
FAST, Harris, and SuperPoint implementations against the FPGA-accelerated
counterparts on modern SoCs (Nvidia Jetson Orin and AMD Versal).
  The evaluation shows that when using a non-learning-based feature detector
such as FAST and Harris, their GPU implementations, and the GPU-accelerated
V-SLAM can achieve better run-time performance and energy efficiency than the
FAST and Harris FPGA implementations as well as the FPGA-accelerated V-SLAM.
However, when considering a learning-based detector such as SuperPoint, its
FPGA implementation can achieve better run-time performance and energy
efficiency (up to 3.1$\times$ and 1.4$\times$ improvements, respectively) than
the GPU implementation. The FPGA-accelerated V-SLAM can also achieve comparable
run-time performance compared to the GPU-accelerated V-SLAM, with better FPS in
2 out of 5 dataset sequences. When considering the accuracy, the results show
that the GPU-accelerated V-SLAM is more accurate than the FPGA-accelerated
V-SLAM in general. Last but not least, the use of hardware acceleration for
feature detection could further improve the performance of the V-SLAM pipeline
by having the global bundle adjustment module invoked less frequently without
sacrificing accuracy.

</details>


### [61] [CymbaDiff: Structured Spatial Diffusion for Sketch-based 3D Semantic Urban Scene Generation](https://arxiv.org/abs/2510.13245)
*Li Liang,Bo Miao,Xinyu Wang,Naveed Akhtar,Jordan Vice,Ajmal Mian*

Main category: cs.CV

TL;DR: 该论文介绍了SketchSem3D，首个用于从手绘草图和卫星图像生成3D户外语义场景的大规模基准数据集，并提出了Cylinder Mamba Diffusion (CymbaDiff)模型，显著提升了生成场景的空间连贯性和真实感。


<details>
  <summary>Details</summary>
Motivation: 户外3D语义场景生成因缺乏公开可用的、标注良好的数据集而受到限制。

Method: 1. 引入SketchSem3D数据集：包含Sketch-based SemanticKITTI和Sketch-based KITTI-360两个子集，提供LiDAR体素、对应草图和标注的卫星图像。
2. 提出Cylinder Mamba Diffusion (CymbaDiff)模型：通过施加结构化的空间排序、明确捕捉圆柱连续性和垂直层次结构，并保留物理邻里关系和全局上下文，显著增强户外3D场景生成的空间连贯性。

Result: 在SketchSem3D上的广泛实验表明，CymbaDiff在语义一致性、空间真实感和跨数据集泛化能力方面表现出色。

Conclusion: 该研究通过提供一个大规模基准数据集和一种新颖的生成模型，极大地推动了户外3D语义场景生成领域的发展，解决了数据稀缺问题并显著提高了生成场景的质量和连贯性。

Abstract: Outdoor 3D semantic scene generation produces realistic and semantically rich
environments for applications such as urban simulation and autonomous driving.
However, advances in this direction are constrained by the absence of publicly
available, well-annotated datasets. We introduce SketchSem3D, the first
large-scale benchmark for generating 3D outdoor semantic scenes from abstract
freehand sketches and pseudo-labeled annotations of satellite images.
SketchSem3D includes two subsets, Sketch-based SemanticKITTI and Sketch-based
KITTI-360 (containing LiDAR voxels along with their corresponding sketches and
annotated satellite images), to enable standardized, rigorous, and diverse
evaluations. We also propose Cylinder Mamba Diffusion (CymbaDiff) that
significantly enhances spatial coherence in outdoor 3D scene generation.
CymbaDiff imposes structured spatial ordering, explicitly captures cylindrical
continuity and vertical hierarchy, and preserves both physical neighborhood
relationships and global context within the generated scenes. Extensive
experiments on SketchSem3D demonstrate that CymbaDiff achieves superior
semantic consistency, spatial realism, and cross-dataset generalization. The
code and dataset will be available at
https://github.com/Lillian-research-hub/CymbaDiff

</details>


### [62] [Map the Flow: Revealing Hidden Pathways of Information in VideoLLMs](https://arxiv.org/abs/2510.13251)
*Minji Kim,Taekyung Kim,Bohyung Han*

Main category: cs.CV

TL;DR: 本研究使用机制可解释性技术，揭示了视频大语言模型（VideoLLMs）在执行时序推理时，其内部信息流动的模式，并发现通过选择有效信息路径可保持性能。


<details>
  <summary>Details</summary>
Motivation: 尽管视频大语言模型（VideoLLMs）在处理时空输入方面取得了进展，但其内部机制，即它们如何提取和传播视频和文本信息，仍未得到充分探索。

Method: 研究采用机制可解释性技术来调查VideoLLMs的内部信息流，特别是在视频问答（VideoQA）任务中。

Result: 研究发现了一致的模式：(1) 时序推理始于早期到中期层中的活跃跨帧交互；(2) 接着是中期层中渐进的视频-语言整合，这得益于视频表征与包含时序概念的语言嵌入之间的对齐；(3) 整合完成后，模型在中期到后期层准备生成正确答案。此外，通过选择这些有效的信息路径并抑制大量注意力边缘（例如，LLaVA-NeXT-7B-Video-FT中抑制了58%），VideoLLMs可以保持其VideoQA性能。

Conclusion: 这些发现为VideoLLMs如何进行时序推理提供了蓝图，并为提高模型可解释性和下游泛化能力提供了实用见解。

Abstract: Video Large Language Models (VideoLLMs) extend the capabilities of
vision-language models to spatiotemporal inputs, enabling tasks such as video
question answering (VideoQA). Despite recent advances in VideoLLMs, their
internal mechanisms on where and how they extract and propagate video and
textual information remain less explored. In this study, we investigate the
internal information flow of VideoLLMs using mechanistic interpretability
techniques. Our analysis reveals consistent patterns across diverse VideoQA
tasks: (1) temporal reasoning in VideoLLMs initiates with active cross-frame
interactions in early-to-middle layers, (2) followed by progressive
video-language integration in middle layers. This is facilitated by alignment
between video representations and linguistic embeddings containing temporal
concepts. (3) Upon completion of this integration, the model is ready to
generate correct answers in middle-to-late layers. (4) Based on our analysis,
we show that VideoLLMs can retain their VideoQA performance by selecting these
effective information pathways while suppressing a substantial amount of
attention edges, e.g., 58% in LLaVA-NeXT-7B-Video-FT. These findings provide a
blueprint on how VideoLLMs perform temporal reasoning and offer practical
insights for improving model interpretability and downstream generalization.
Our project page with the source code is available at
https://map-the-flow.github.io

</details>


### [63] [MMLongCite: A Benchmark for Evaluating Fidelity of Long-Context Vision-Language Models](https://arxiv.org/abs/2510.13276)
*Keyan Zhou,Zecheng Tang,Lingfeng Ming,Guanghao Zhou,Qiguang Chen,Dan Qiao,Zheming Yang,Libo Qin,Minghui Qiu,Juntao Li,Min Zhang*

Main category: cs.CV

TL;DR: 随着大视觉语言模型（LVLMs）上下文窗口的扩展，其对长多模态上下文的有效利用和忠实度评估仍是挑战。本文提出了MMLongCite基准，发现当前SOTA模型在长多模态上下文处理上忠实度有限，并分析了上下文长度和关键内容位置的影响。


<details>
  <summary>Details</summary>
Motivation: LVLMs的上下文窗口显著增大，但长上下文是否能被有效利用仍是关键挑战。现有评估主要集中在纯文本领域，多模态评估仅限于短上下文，缺乏对长多模态上下文忠实度的全面评估。

Method: 引入MMLongCite，一个旨在评估LVLMs在长上下文场景中忠实度的综合基准。该基准包含8个不同任务，跨越6个上下文长度区间，并整合了文本、图像和视频等多种模态。

Result: 对最先进LVLMs的评估显示，它们在处理长多模态上下文时忠实度有限。此外，研究还深入分析了上下文长度和关键内容位置如何影响模型的忠实度。

Conclusion: 当前LVLMs在处理长多模态上下文时表现出有限的忠实度。MMLongCite基准填补了评估空白，并揭示了上下文长度和关键内容位置对模型表现的重要影响，为未来模型改进提供了方向。

Abstract: The rapid advancement of large vision language models (LVLMs) has led to a
significant expansion of their context windows. However, an extended context
window does not guarantee the effective utilization of the context, posing a
critical challenge for real-world applications. Current evaluations of such
long-context faithfulness are predominantly focused on the text-only domain,
while multimodal assessments remain limited to short contexts. To bridge this
gap, we introduce MMLongCite, a comprehensive benchmark designed to evaluate
the fidelity of LVLMs in long-context scenarios. MMLongCite comprises 8
distinct tasks spanning 6 context length intervals and incorporates diverse
modalities, including text, images, and videos. Our evaluation of
state-of-the-art LVLMs reveals their limited faithfulness in handling long
multimodal contexts. Furthermore, we provide an in-depth analysis of how
context length and the position of crucial content affect the faithfulness of
these models.

</details>


### [64] [Universal Image Restoration Pre-training via Masked Degradation Classification](https://arxiv.org/abs/2510.13282)
*JiaKui Hu,Zhengjian Yao,Lujia Jin,Yinghao Chen,Yanye Lu*

Main category: cs.CV

TL;DR: 本研究提出了一种名为MaskDCPT的掩码降解分类预训练方法，旨在通过结合降解类型分类和图像重建，实现全面的图像修复预训练，显著提升了通用图像修复的性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统的预训练方法在处理复杂的图像降解时可能不够有效，需要一种新的、更鲁棒的方法来促进通用图像修复，使其能够识别降解类型并进行高质量的图像重建。

Method: MaskDCPT方法使用图像的降解类型作为极弱监督，并结合图像重建来增强性能和鲁棒性。它包括一个编码器和两个解码器：编码器从掩码的低质量输入图像中提取特征；一个分类解码器使用这些特征识别降解类型；另一个重建解码器旨在重建相应的高质量图像。这种设计结合了掩码图像建模和对比学习的优点。此外，研究还策划并发布了包含250万个配对修复样本的UIR-2.5M数据集。

Result: MaskDCPT显著提升了卷积神经网络（CNNs）和Transformers在通用图像修复任务中的性能，在5D一体化修复任务中PSNR至少增加了3.77 dB，在真实世界降解场景中PIQE比基线降低了34.8%。它还对以前未见的降解类型和水平展现出强大的泛化能力。

Conclusion: MaskDCPT是一种简单而强大的通用图像修复预训练方法，通过结合降解类型分类和图像重建，能够显著提升多种网络架构在各种图像修复任务中的性能和泛化能力。同时发布的UIR-2.5M数据集为未来的研究提供了宝贵的资源。

Abstract: This study introduces a Masked Degradation Classification Pre-Training method
(MaskDCPT), designed to facilitate the classification of degradation types in
input images, leading to comprehensive image restoration pre-training. Unlike
conventional pre-training methods, MaskDCPT uses the degradation type of the
image as an extremely weak supervision, while simultaneously leveraging the
image reconstruction to enhance performance and robustness. MaskDCPT includes
an encoder and two decoders: the encoder extracts features from the masked
low-quality input image. The classification decoder uses these features to
identify the degradation type, whereas the reconstruction decoder aims to
reconstruct a corresponding high-quality image. This design allows the
pre-training to benefit from both masked image modeling and contrastive
learning, resulting in a generalized representation suited for restoration
tasks. Benefit from the straightforward yet potent MaskDCPT, the pre-trained
encoder can be used to address universal image restoration and achieve
outstanding performance. Implementing MaskDCPT significantly improves
performance for both convolution neural networks (CNNs) and Transformers, with
a minimum increase in PSNR of 3.77 dB in the 5D all-in-one restoration task and
a 34.8% reduction in PIQE compared to baseline in real-world degradation
scenarios. It also emergences strong generalization to previously unseen
degradation types and levels. In addition, we curate and release the UIR-2.5M
dataset, which includes 2.5 million paired restoration samples across 19
degradation types and over 200 degradation levels, incorporating both synthetic
and real-world data. The dataset, source code, and models are available at
https://github.com/MILab-PKU/MaskDCPT.

</details>


### [65] [End-to-End Multi-Modal Diffusion Mamba](https://arxiv.org/abs/2510.13253)
*Chunhao Lu,Qiang Lu,Meichen Dong,Jake Luo*

Main category: cs.CV

TL;DR: MDM（多模态扩散Mamba）提出了一种统一的多模态处理架构，利用基于Mamba的扩散模型和统一的VAE进行编码和解码，在多项任务中表现优异，超越现有端到端模型并与SOTA模型竞争。


<details>
  <summary>Details</summary>
Motivation: 当前端到端多模态模型使用不同的编码器和解码器处理输入和输出信息，这种分离阻碍了各种模态的联合表示学习。研究动机在于统一多模态处理。

Method: 本文提出了一种名为MDM（多模态扩散Mamba）的新型架构。MDM利用基于Mamba的多步选择扩散模型，通过一个统一的变分自编码器（VAE）进行编码和解码，逐步生成和细化模态特定信息。

Result: MDM在处理高维数据时表现出卓越性能，特别是在同时生成高分辨率图像和扩展文本序列方面。在图像生成、图像字幕、视觉问答、文本理解和推理任务中，MDM显著优于现有端到端模型（如MonoFormer、LlamaGen和Chameleon），并能有效与SOTA模型（如GPT-4V、Gemini Pro和Mistral）竞争。同时保持了计算效率。

Conclusion: MDM有效统一了多模态处理过程，同时保持了计算效率，为端到端多模态架构开辟了新方向。

Abstract: Current end-to-end multi-modal models utilize different encoders and decoders
to process input and output information. This separation hinders the joint
representation learning of various modalities. To unify multi-modal processing,
we propose a novel architecture called MDM (Multi-modal Diffusion Mamba). MDM
utilizes a Mamba-based multi-step selection diffusion model to progressively
generate and refine modality-specific information through a unified variational
autoencoder for both encoding and decoding. This innovative approach allows MDM
to achieve superior performance when processing high-dimensional data,
particularly in generating high-resolution images and extended text sequences
simultaneously. Our evaluations in areas such as image generation, image
captioning, visual question answering, text comprehension, and reasoning tasks
demonstrate that MDM significantly outperforms existing end-to-end models
(MonoFormer, LlamaGen, and Chameleon etc.) and competes effectively with SOTA
models like GPT-4V, Gemini Pro, and Mistral. Our results validate MDM's
effectiveness in unifying multi-modal processes while maintaining computational
efficiency, establishing a new direction for end-to-end multi-modal
architectures.

</details>


### [66] [Automated document processing system for government agencies using DBNET++ and BART models](https://arxiv.org/abs/2510.13303)
*Aya Kaysan Bahjat*

Main category: cs.CV

TL;DR: 本文提出一个自动文档分类系统，能够检测图像中的文本内容，并将文档分为四类（发票、报告、信件、表格）。该系统支持离线和实时图像捕获，并能应对多种实际挑战，在文本检测方面取得了良好效果。


<details>
  <summary>Details</summary>
Motivation: 现有系统在实际应用中面临诸多挑战，如光照变化、任意方向、弯曲或部分遮挡文本、低分辨率和远距离文本等。本研究旨在开发一个能够应对这些复杂成像场景的实用、混合源文档分类系统。

Method: 该系统包含四个阶段：图像捕获与预处理、使用DBNet++进行文本检测、使用BART分类器进行文本分类。所有功能均通过Python和PyQt5实现的用户界面集成。系统支持离线图像和通过连接摄像头实时捕获。

Result: 在Total-Text数据集上，该系统在10小时内，文本检测准确率达到了约92.88%。该数据集包含高分辨率图像，模拟了各种极其困难的挑战。

Conclusion: 研究结果表明，所提出的方法对于在非受限成像场景下进行实际的、混合源文档分类是有效的。

Abstract: An automatic document classification system is presented that detects textual
content in images and classifies documents into four predefined categories
(Invoice, Report, Letter, and Form). The system supports both offline images
(e.g., files on flash drives, HDDs, microSD) and real-time capture via
connected cameras, and is designed to mitigate practical challenges such as
variable illumination, arbitrary orientation, curved or partially occluded
text, low resolution, and distant text. The pipeline comprises four stages:
image capture and preprocessing, text detection [1] using a DBNet++
(Differentiable Binarization Network Plus) detector, and text classification
[2] using a BART (Bidirectional and Auto-Regressive Transformers) classifier,
all integrated within a user interface implemented in Python with PyQt5. The
achieved results by the system for text detection in images were good at about
92.88% through 10 hours on Total-Text dataset that involve high resolution
images simulate a various and very difficult challenges. The results indicate
the proposed approach is effective for practical, mixed-source document
categorization in unconstrained imaging scenarios.

</details>


### [67] [Novel Class Discovery for Point Cloud Segmentation via Joint Learning of Causal Representation and Reasoning](https://arxiv.org/abs/2510.13307)
*Yang Li,Aming Wu,Zihao Zhang,Yahong Han*

Main category: cs.CV

TL;DR: 本文提出了一种基于因果模型的3D点云新类发现（3D-NCD）方法，通过学习因果表示和推理来准确分割未标记的3D类别。


<details>
  <summary>Details</summary>
Motivation: 在点云分割的新类发现任务中，准确建立点表示与基类标签以及基类与新类之间表示的相关性至关重要。粗糙或统计学上的相关性学习可能导致新类推断的混淆。引入因果关系作为强相关约束，可以揭示准确对应类别的本质点云表示。

Method: 作者引入了结构化因果模型（SCM）来重新形式化3D-NCD问题，并提出了一种名为“因果表示与推理联合学习”的新方法。具体而言，该方法首先通过SCM分析基类表示中的隐藏混杂因素以及基类和新类之间的因果关系。接着，设计了一个消除混杂因素的因果表示原型，以捕获基类的因果表示。然后，利用图结构建模基类因果表示原型与新类原型之间的因果关系，从而实现从基类到新类的因果推理。

Result: 在3D和2D NCD语义分割任务上的大量实验和可视化结果表明，所提出的方法具有优越性。

Conclusion: 通过引入结构化因果模型和因果表示与推理联合学习，可以有效解决3D点云新类发现任务中表示混淆的问题，显著提升分割性能。

Abstract: In this paper, we focus on Novel Class Discovery for Point Cloud Segmentation
(3D-NCD), aiming to learn a model that can segment unlabeled (novel) 3D classes
using only the supervision from labeled (base) 3D classes. The key to this task
is to setup the exact correlations between the point representations and their
base class labels, as well as the representation correlations between the
points from base and novel classes. A coarse or statistical correlation
learning may lead to the confusion in novel class inference. lf we impose a
causal relationship as a strong correlated constraint upon the learning
process, the essential point cloud representations that accurately correspond
to the classes should be uncovered. To this end, we introduce a structural
causal model (SCM) to re-formalize the 3D-NCD problem and propose a new method,
i.e., Joint Learning of Causal Representation and Reasoning. Specifically, we
first analyze hidden confounders in the base class representations and the
causal relationships between the base and novel classes through SCM. We devise
a causal representation prototype that eliminates confounders to capture the
causal representations of base classes. A graph structure is then used to model
the causal relationships between the base classes' causal representation
prototypes and the novel class prototypes, enabling causal reasoning from base
to novel classes. Extensive experiments and visualization results on 3D and 2D
NCD semantic segmentation demonstrate the superiorities of our method.

</details>


### [68] [InstantSfM: Fully Sparse and Parallel Structure-from-Motion](https://arxiv.org/abs/2510.13310)
*Jiankun Zhong,Zitong Zhan,Quankai Gao,Ziyu Chen,Haozhe Lou,Jiageng Mao,Ulrich Neumann,Yue Wang*

Main category: cs.CV

TL;DR: 本文利用GPU并行计算加速了标准SfM管道的每个关键阶段，特别是在统一的全局SfM框架中加速了BA和GP，实现了比COLMAP快40倍的速度，同时保持或提高了重建精度，解决了大规模SfM的速度和内存限制问题。


<details>
  <summary>Details</summary>
Motivation: 传统的SfM方法（如COLMAP）在处理大规模场景时存在显著的计算开销，导致精度和速度之间的权衡，且缺乏优化灵活性。基于深度学习的SfM方法（如VGGSfM）则面临GPU内存消耗随输入视图数量增加而急剧上升的问题，难以扩展到数千张图像。

Method: 本文充分利用GPU并行计算来加速标准SfM管道的每个关键阶段。基于稀疏感知（sparse-aware）束调整优化方面的最新进展，将这些技术扩展到统一的全局SfM框架中，以加速束调整（BA）和全局定位（GP）。

Result: 在不同规模的数据集上进行了广泛实验（例如，在VGGSfM和VGGT内存不足的5000张图像数据集上），本文方法比COLMAP实现了高达约40倍的加速，同时始终保持可比甚至更高的重建精度。

Conclusion: 通过释放GPU并行计算的全部潜力，本文提出的方法有效解决了传统SfM在大规模场景下的计算开销和深度学习SfM的内存限制问题，显著提高了SfM的速度和可扩展性，同时保持了高精度，为机器人重建和模拟提供了更高效的解决方案。

Abstract: Structure-from-Motion (SfM), a method that recovers camera poses and scene
geometry from uncalibrated images, is a central component in robotic
reconstruction and simulation. Despite the state-of-the-art performance of
traditional SfM methods such as COLMAP and its follow-up work, GLOMAP, naive
CPU-specialized implementations of bundle adjustment (BA) or global positioning
(GP) introduce significant computational overhead when handling large-scale
scenarios, leading to a trade-off between accuracy and speed in SfM. Moreover,
the blessing of efficient C++-based implementations in COLMAP and GLOMAP comes
with the curse of limited flexibility, as they lack support for various
external optimization options. On the other hand, while deep learning based SfM
pipelines like VGGSfM and VGGT enable feed-forward 3D reconstruction, they are
unable to scale to thousands of input views at once as GPU memory consumption
increases sharply as the number of input views grows. In this paper, we unleash
the full potential of GPU parallel computation to accelerate each critical
stage of the standard SfM pipeline. Building upon recent advances in
sparse-aware bundle adjustment optimization, our design extends these
techniques to accelerate both BA and GP within a unified global SfM framework.
Through extensive experiments on datasets of varying scales (e.g. 5000 images
where VGGSfM and VGGT run out of memory), our method demonstrates up to about
40 times speedup over COLMAP while achieving consistently comparable or even
improved reconstruction accuracy. Our project page can be found at
https://cre185.github.io/InstantSfM/.

</details>


### [69] [UniME-V2: MLLM-as-a-Judge for Universal Multimodal Embedding Learning](https://arxiv.org/abs/2510.13515)
*Tiancheng Gu,Kaicheng Yang,Kaichen Zhang,Xiang An,Ziyong Feng,Yueyi Zhang,Weidong Cai,Jiankang Deng,Lidong Bing*

Main category: cs.CV

TL;DR: 本文提出UniME-V2模型，利用多模态大语言模型（MLLM）作为“评判者”来生成软语义匹配分数，以改进困难负样本挖掘并增强多模态嵌入的判别能力，实现最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在捕获候选样本间细微语义差异、缺乏负样本多样性以及区分假阳性/困难负样本方面判别能力有限。

Method: 首先通过全局检索构建潜在困难负样本集。然后引入“MLLM-as-a-Judge”机制，利用MLLM评估查询-候选对的语义对齐并生成软语义匹配分数。这些分数用于困难负样本挖掘，以识别多样化、高质量的困难负样本并缓解假阳性的影响。此外，软语义匹配分数被用作软标签，通过将相似性矩阵与软语义匹配分数矩阵对齐来学习候选样本间的语义区别。为进一步提升性能，提出了UniME-V2-Reranker，一个基于挖掘出的困难负样本，通过联合成对和列表优化训练的重排序模型。

Result: 在MMEB基准测试和多个检索任务上进行了全面的实验，结果表明所提出的方法在所有任务上平均取得了最先进的性能。

Conclusion: 通过利用MLLM的先进理解能力和引入软语义匹配分数，UniME-V2及其重排序模型显著增强了通用多模态嵌入的表示学习能力，尤其是在困难负样本挖掘和判别能力方面，从而在多模态任务中达到了卓越的性能。

Abstract: Universal multimodal embedding models are foundational to various tasks.
Existing approaches typically employ in-batch negative mining by measuring the
similarity of query-candidate pairs. However, these methods often struggle to
capture subtle semantic differences among candidates and lack diversity in
negative samples. Moreover, the embeddings exhibit limited discriminative
ability in distinguishing false and hard negatives. In this paper, we leverage
the advanced understanding capabilities of MLLMs to enhance representation
learning and present a novel Universal Multimodal Embedding (UniME-V2) model.
Our approach first constructs a potential hard negative set through global
retrieval. We then introduce the MLLM-as-a-Judge mechanism, which utilizes
MLLMs to assess the semantic alignment of query-candidate pairs and generate
soft semantic matching scores. These scores serve as a foundation for hard
negative mining, mitigating the impact of false negatives and enabling the
identification of diverse, high-quality hard negatives. Furthermore, the
semantic matching scores are used as soft labels to mitigate the rigid
one-to-one mapping constraint. By aligning the similarity matrix with the soft
semantic matching score matrix, the model learns semantic distinctions among
candidates, significantly enhancing its discriminative capacity. To further
improve performance, we propose UniME-V2-Reranker, a reranking model trained on
our mined hard negatives through a joint pairwise and listwise optimization
approach. We conduct comprehensive experiments on the MMEB benchmark and
multiple retrieval tasks, demonstrating that our method achieves
state-of-the-art performance on average across all tasks.

</details>


### [70] [Language as a Label: Zero-Shot Multimodal Classification of Everyday Postures under Data Scarcity](https://arxiv.org/abs/2510.13364)
*MingZe Tang,Jubal Chandy Jacob*

Main category: cs.CV

TL;DR: 本研究发现，对于高性能视觉语言模型（VLMs），在零样本分类中，最简单的提示词在识别视觉相似类别（如人类姿势）时效果最好，而增加描述性细节会显著降低性能（即“提示词过拟合”）。然而，对于性能较低的模型，更详细的提示词可能有助于提高分类效果。


<details>
  <summary>Details</summary>
Motivation: 尽管最近的视觉语言模型（VLMs）通过对齐图像和文本实现了零样本分类，但在数据稀缺条件下，提示词设计对识别视觉相似类别（如人类姿势）的影响尚不明确。

Method: 研究在一个包含285张图像（源自COCO数据集，涵盖坐姿、站姿和行走/跑步姿势）的小型数据集上，评估了OpenCLIP、MetaCLIP 2和SigLip等现代VLMs。通过使用三层提示词设计，系统性地增加了语言细节来探究提示词特异性的影响。

Result: 研究发现了一个反直觉的趋势：对于性能最好的模型（MetaCLIP 2和OpenCLIP），最简单、最基本的提示词持续取得最佳结果。增加描述性细节会显著降低性能（例如，MetaCLIP 2的多类别准确率从68.8%降至55.1%），这种现象被称为“提示词过拟合”。相反，性能较低的SigLip模型在给定更具描述性、基于身体线索的提示词时，在模糊类别上的分类有所改善。

Conclusion: 提示词的特异性对视觉相似类别的零样本分类有复杂且依赖于模型的影响。对于高性能VLMs，简洁的提示词通常更优；而对于性能较弱的模型，更详细的提示词可能带来改善。

Abstract: Recent Vision-Language Models (VLMs) enable zero-shot classification by
aligning images and text in a shared space, a promising approach for
data-scarce conditions. However, the influence of prompt design on recognizing
visually similar categories, such as human postures, is not well understood.
This study investigates how prompt specificity affects the zero-shot
classification of sitting, standing, and walking/running on a small, 285-image
COCO-derived dataset. A suite of modern VLMs, including OpenCLIP, MetaCLIP 2,
and SigLip, were evaluated using a three-tiered prompt design that
systematically increases linguistic detail. Our findings reveal a compelling,
counter-intuitive trend: for the highest-performing models (MetaCLIP 2 and
OpenCLIP), the simplest, most basic prompts consistently achieve the best
results. Adding descriptive detail significantly degrades performance for
instance, MetaCLIP 2's multi-class accuracy drops from 68.8\% to 55.1\% a
phenomenon we term "prompt overfitting". Conversely, the lower-performing
SigLip model shows improved classification on ambiguous classes when given more
descriptive, body-cue-based prompts.

</details>


### [71] [Visual Interestingness Decoded: How GPT-4o Mirrors Human Interests](https://arxiv.org/abs/2510.13316)
*Fitim Abdullahu,Helmut Grabner*

Main category: cs.CV

TL;DR: 本文探讨了大型多模态模型（LMMs）对视觉趣味性的理解能力，发现GPT-4o与人类评估有部分一致性，且优于现有SOTA方法。研究利用GPT-4o的预测能力进行数据标注，并将知识蒸馏到学习排序模型中，以期深入理解人类兴趣。


<details>
  <summary>Details</summary>
Motivation: 日常生活深受所见所闻影响，吸引并保持注意力（即视觉趣味性）至关重要。随着大型多模态模型（LMMs）的兴起及其展现出的强大能力，研究旨在探索这些模型在多大程度上捕捉了视觉趣味性的概念。

Method: 研究通过比较分析，探讨了人类对视觉趣味性的评估与领先LMM（GPT-4o）预测之间的一致性。在此基础上，利用GPT-4o有效标注图像对的（共同）趣味性，并将这些标注数据作为训练数据，将知识蒸馏到一个学习排序模型中。

Result: 研究发现人类评估与GPT-4o的预测之间存在部分一致性。与现有最先进的方法相比，GPT-4o在捕捉视觉趣味性概念方面表现最佳。这使得LMMs能够有效地对图像对进行趣味性标注。

Conclusion: 这些见解为更深入地理解人类兴趣铺平了道路。通过将LMMs的知识蒸馏到学习排序模型中，可以利用其在视觉趣味性理解方面的能力，从而提升相关应用。

Abstract: Our daily life is highly influenced by what we consume and see. Attracting
and holding one's attention -- the definition of (visual) interestingness -- is
essential. The rise of Large Multimodal Models (LMMs) trained on large-scale
visual and textual data has demonstrated impressive capabilities. We explore
these models' potential to understand to what extent the concepts of visual
interestingness are captured and examine the alignment between human
assessments and GPT-4o's, a leading LMM, predictions through comparative
analysis. Our studies reveal partial alignment between humans and GPT-4o. It
already captures the concept as best compared to state-of-the-art methods.
Hence, this allows for the effective labeling of image pairs according to their
(commonly) interestingness, which are used as training data to distill the
knowledge into a learning-to-rank model. The insights pave the way for a deeper
understanding of human interest.

</details>


### [72] [Removing Cost Volumes from Optical Flow Estimators](https://arxiv.org/abs/2510.13317)
*Simon Kiefhaber,Stefan Roth,Simone Schaub-Meyer*

Main category: cs.CV

TL;DR: 本文提出一种训练策略，可在训练过程中移除光流估计器中的成本体（Cost Volume），显著提高推理速度并降低内存需求，同时保持或超越现有模型的精度。


<details>
  <summary>Details</summary>
Motivation: 成本体是现代光流估计器的瓶颈，限制了处理速度和输入帧分辨率，因为它计算和空间复杂度高。作者经验观察到，一旦RAFTA等流水线的其他网络部分训练充分，成本体的重要性就会降低。

Method: 引入一种训练策略，允许在整个训练过程中从光流估计器中移除成本体。通过这种策略，作者创建了三种不同计算预算的模型。

Result: 使用该策略的模型显著提高了推理速度并降低了内存需求。其中最精确的模型达到了最先进的精度，同时比可比模型快1.2倍，内存占用低6倍；最快的模型能够以20 FPS处理全高清帧，仅使用500 MB GPU内存。

Conclusion: 通过移除成本体的训练策略，可以创建更高效的光流估计器，显著提升推理速度和降低内存消耗，同时保持或达到最先进的精度。

Abstract: Cost volumes are used in every modern optical flow estimator, but due to
their computational and space complexity, they are often a limiting factor
regarding both processing speed and the resolution of input frames. Motivated
by our empirical observation that cost volumes lose their importance once all
other network parts of, e.g., a RAFT-based pipeline have been sufficiently
trained, we introduce a training strategy that allows removing the cost volume
from optical flow estimators throughout training. This leads to significantly
improved inference speed and reduced memory requirements. Using our training
strategy, we create three different models covering different compute budgets.
Our most accurate model reaches state-of-the-art accuracy while being
$1.2\times$ faster and having a $6\times$ lower memory footprint than
comparable models; our fastest model is capable of processing Full HD frames at
$20\,\mathrm{FPS}$ using only $500\,\mathrm{MB}$ of GPU memory.

</details>


### [73] [Modeling Cultural Bias in Facial Expression Recognition with Adaptive Agents](https://arxiv.org/abs/2510.13557)
*David Freire-Obregón,José Salas-Cáceres,Javier Lorenzo-Navarro,Oliverio J. Santana,Daniel Hernández-Sosa,Modesto Castrillón-Santana*

Main category: cs.CV

TL;DR: 该研究引入了一种代理基准，评估了跨文化组成和渐进模糊对人脸表情识别（FER）鲁棒性的影响，发现不同文化群体表现出不对称的性能下降曲线，且混合群体表现出中间模式。


<details>
  <summary>Details</summary>
Motivation: 现有的人脸表情识别（FER）评估通常假设数据同质且图像质量高，但FER在实际应用中需要对文化差异和感知退化的视觉条件保持鲁棒性。

Method: 研究引入了一个基于代理的流式基准，其中代理在冻结的CLIP特征空间中运行，并使用轻量级残差适配器进行在线训练和固定测试。代理在5x5格子上移动和交互，环境提供带有高斯模糊的输入。实验考察了单一文化（西方、亚洲）和混合文化（平衡5/5、不平衡8/2、2/8）群体，以及不同的空间接触结构。

Result: 结果显示，不同文化群体之间存在明显的不对称性能下降曲线：JAFFE（亚洲）群体在低模糊度下表现更好，但在中等模糊度下性能下降更快；而KDEF（西方）群体则更均匀地下降。混合群体表现出中间模式，其中平衡混合减轻了早期性能下降，但不平衡设置在高模糊度下放大了多数群体的弱点。

Conclusion: 研究量化了文化组成和交互结构在感知条件恶化时如何影响人脸表情识别（FER）的鲁棒性。

Abstract: Facial expression recognition (FER) must remain robust under both cultural
variation and perceptually degraded visual conditions, yet most existing
evaluations assume homogeneous data and high-quality imagery. We introduce an
agent-based, streaming benchmark that reveals how cross-cultural composition
and progressive blurring interact to shape face recognition robustness. Each
agent operates in a frozen CLIP feature space with a lightweight residual
adapter trained online at sigma=0 and fixed during testing. Agents move and
interact on a 5x5 lattice, while the environment provides inputs with
sigma-scheduled Gaussian blur. We examine monocultural populations
(Western-only, Asian-only) and mixed environments with balanced (5/5) and
imbalanced (8/2, 2/8) compositions, as well as different spatial contact
structures. Results show clear asymmetric degradation curves between cultural
groups: JAFFE (Asian) populations maintain higher performance at low blur but
exhibit sharper drops at intermediate stages, whereas KDEF (Western)
populations degrade more uniformly. Mixed populations exhibit intermediate
patterns, with balanced mixtures mitigating early degradation, but imbalanced
settings amplify majority-group weaknesses under high blur. These findings
quantify how cultural composition and interaction structure influence the
robustness of FER as perceptual conditions deteriorate.

</details>


### [74] [Group-Wise Optimization for Self-Extensible Codebooks in Vector Quantized Models](https://arxiv.org/abs/2510.13331)
*Hong-Kai Zheng,Piji Li*

Main category: cs.CV

TL;DR: 本文提出Group-VQ，通过对码本进行分组优化来解决VQ-VAE中码本崩溃问题，并在不影响重建质量的情况下，引入了一种训练后码本重采样方法，以实现码本大小的灵活调整。


<details>
  <summary>Details</summary>
Motivation: VQ-VAE模型中存在码本崩溃问题，现有方法（如隐式静态码本或联合优化整个码本）限制了码本的学习能力，导致重建质量下降。

Method: 本文提出Group-VQ，对码本进行分组优化，组内联合优化，组间独立优化。此外，还引入了一种无需训练的码本重采样方法，允许在训练后调整码本大小。

Result: 在不同设置下的图像重建实验中，Group-VQ在重建指标上表现出更好的性能。训练后码本采样方法实现了码本大小调整的预期灵活性。

Conclusion: Group-VQ通过分组优化有效改善了VQ-VAE的重建性能和码本利用率，并且其训练后码本重采样方法提供了调整码本大小的灵活性。

Abstract: Vector Quantized Variational Autoencoders (VQ-VAEs) leverage self-supervised
learning through reconstruction tasks to represent continuous vectors using the
closest vectors in a codebook. However, issues such as codebook collapse
persist in the VQ model. To address these issues, existing approaches employ
implicit static codebooks or jointly optimize the entire codebook, but these
methods constrain the codebook's learning capability, leading to reduced
reconstruction quality. In this paper, we propose Group-VQ, which performs
group-wise optimization on the codebook. Each group is optimized independently,
with joint optimization performed within groups. This approach improves the
trade-off between codebook utilization and reconstruction performance.
Additionally, we introduce a training-free codebook resampling method, allowing
post-training adjustment of the codebook size. In image reconstruction
experiments under various settings, Group-VQ demonstrates improved performance
on reconstruction metrics. And the post-training codebook sampling method
achieves the desired flexibility in adjusting the codebook size.

</details>


### [75] [DEF-YOLO: Leveraging YOLO for Concealed Weapon Detection in Thermal Imagin](https://arxiv.org/abs/2510.13326)
*Divya Bhardwaj,Arnav Ramamoorthy,Poonam Goyal*

Main category: cs.CV

TL;DR: 本文提出了一种基于热成像的隐蔽武器检测新方法，包括一个名为DEF-YOLO的YOLOv8改进架构和一个大规模热成像隐蔽武器数据集TICW，旨在提供实时、低成本且保护隐私的解决方案，并建立了新的检测基准。


<details>
  <summary>Details</summary>
Motivation: 现有隐蔽武器检测成像模态（如毫米波、微波、太赫兹、红外等）存在局限性，如分辨率差或隐私问题。研究旨在寻找一种能提供实时、全天候监控、低成本且保护隐私的解决方案。尽管缺乏基准数据集，但热成像被选为有潜力的技术。

Method: 研究采用热成像技术，并提出了一个名为DEF-YOLO的YOLOv8改进架构，专为热视觉中的隐蔽武器检测挑战定制。主要方法包括：在SPPF层引入可变形卷积以利用多尺度特征；在骨干网络和颈部网络中提取低、中、高层特征，使DEF-YOLO能自适应地聚焦于热均匀区域中物体周围的定位；构建了一个新的大规模热成像隐蔽武器数据集TICW；并整合了焦点损失（focal loss）以解决类别不平衡问题。

Result: DEF-YOLO架构在保持速度和吞吐量的同时，能有效地自适应聚焦于热均匀区域中物体周围的定位。所提出的工作（包括DEF-YOLO模型和TICW数据集）通过广泛实验，为热成像中的隐蔽武器检测任务建立了新的基准。TICW是迄今为止该任务中第一个大规模的贡献数据集。

Conclusion: 本文提出的DEF-YOLO架构和TICW数据集，结合焦点损失，为热成像隐蔽武器检测任务建立了一个新的有效基准。这为实现实时、24/7监控、低成本且保护隐私的隐蔽武器检测提供了可行的解决方案。

Abstract: Concealed weapon detection aims at detecting weapons hidden beneath a
person's clothing or luggage. Various imaging modalities like Millimeter Wave,
Microwave, Terahertz, Infrared, etc., are exploited for the concealed weapon
detection task. These imaging modalities have their own limitations, such as
poor resolution in microwave imaging, privacy concerns in millimeter wave
imaging, etc. To provide a real-time, 24 x 7 surveillance, low-cost, and
privacy-preserved solution, we opted for thermal imaging in spite of the lack
of availability of a benchmark dataset. We propose a novel approach and a
dataset for concealed weapon detection in thermal imagery. Our YOLO-based
architecture, DEF-YOLO, is built with key enhancements in YOLOv8 tailored to
the unique challenges of concealed weapon detection in thermal vision. We adopt
deformable convolutions at the SPPF layer to exploit multi-scale features;
backbone and neck layers to extract low, mid, and high-level features, enabling
DEF-YOLO to adaptively focus on localization around the objects in thermal
homogeneous regions, without sacrificing much of the speed and throughput. In
addition to these simple yet effective key architectural changes, we introduce
a new, large-scale Thermal Imaging Concealed Weapon dataset, TICW, featuring a
diverse set of concealed weapons and capturing a wide range of scenarios. To
the best of our knowledge, this is the first large-scale contributed dataset
for this task. We also incorporate focal loss to address the significant class
imbalance inherent in the concealed weapon detection task. The efficacy of the
proposed work establishes a new benchmark through extensive experimentation for
concealed weapon detection in thermal imagery.

</details>


### [76] [No-Reference Rendered Video Quality Assessment: Dataset and Metrics](https://arxiv.org/abs/2510.13349)
*Sipeng Yang,Jiayu Ji,Qingchuan Zhu,Zhiyao Yang,Xiaogang Jin*

Main category: cs.CV

TL;DR: 本文提出了一个针对渲染视频的大型无参考视频质量评估（NR-VQA）数据集和专门的评估指标，解决了现有方法在渲染视频中表现不佳的问题，并证明了其在评估渲染质量和帧生成策略上的优越性。


<details>
  <summary>Details</summary>
Motivation: 视频质量评估对于计算机图形应用（如视频游戏、VR、AR）至关重要，用户体验受视觉性能影响。当无法完美对齐参考视频或无参考视频时，无参考视频质量评估（NR-VQA）方法变得不可或缺。然而，现有的NR-VQA数据集和指标主要针对相机拍摄视频，直接应用于渲染视频会导致偏差，因为渲染视频更容易出现时间伪影。

Method: 1. 构建了一个大型渲染导向的视频数据集，包含主观质量标注，涵盖了广泛的3D场景和渲染设置，并针对不同显示类型进行了质量评分标注。2. 基于该数据集，设计并校准了一个专门用于渲染视频的NR-VQA指标，该指标同时考虑图像质量和时间稳定性。

Result: 1. 该指标在渲染视频上的性能优于现有NR-VQA指标。2. 证明了该指标可用于基准测试超采样方法和评估实时渲染中的帧生成策略。

Conclusion: 本文通过引入一个专门的渲染视频数据集和NR-VQA指标，有效解决了现有方法在评估渲染视频质量时的局限性。该指标不仅表现优异，还能应用于评估渲染技术和帧生成策略，对计算机图形领域具有实际应用价值。

Abstract: Quality assessment of videos is crucial for many computer graphics
applications, including video games, virtual reality, and augmented reality,
where visual performance has a significant impact on user experience. When test
videos cannot be perfectly aligned with references or when references are
unavailable, the significance of no-reference video quality assessment (NR-VQA)
methods is undeniable. However, existing NR-VQA datasets and metrics are
primarily focused on camera-captured videos; applying them directly to rendered
videos would result in biased predictions, as rendered videos are more prone to
temporal artifacts. To address this, we present a large rendering-oriented
video dataset with subjective quality annotations, as well as a designed NR-VQA
metric specific to rendered videos. The proposed dataset includes a wide range
of 3D scenes and rendering settings, with quality scores annotated for various
display types to better reflect real-world application scenarios. Building on
this dataset, we calibrate our NR-VQA metric to assess rendered video quality
by looking at both image quality and temporal stability. We compare our metric
to existing NR-VQA metrics, demonstrating its superior performance on rendered
videos. Finally, we demonstrate that our metric can be used to benchmark
supersampling methods and assess frame generation strategies in real-time
rendering.

</details>


### [77] [CanvasMAR: Improving Masked Autoregressive Video Generation With Canvas](https://arxiv.org/abs/2510.13669)
*Zian Li,Muhan Zhang*

Main category: cs.CV

TL;DR: 本文提出CanvasMAR，一种新的视频掩码自回归模型，通过引入“画布”机制（下一帧的模糊全局预测作为生成起点）和组合式无分类器引导，解决了视频MAR模型的慢启动和误差累积问题，实现了更快、更高质量的视频生成，性能可与扩散模型媲美。


<details>
  <summary>Details</summary>
Motivation: 视频掩码自回归（MAR）模型存在两大局限性：一是“慢启动问题”，即在早期采样阶段缺乏结构化的全局先验；二是空间和时间维度上的自回归误差累积。

Method: 本文提出了CanvasMAR模型，通过引入“画布”机制来缓解上述问题。画布是下一帧的模糊全局预测，作为掩码生成的起始点，在采样早期提供全局结构。此外，还引入了组合式无分类器引导，联合增强空间（画布）和时间条件，并采用基于噪声的画布增强来提高鲁棒性。

Result: CanvasMAR在BAIR和Kinetics-600基准测试中，能以更少的自回归步骤生成高质量视频。该方法在Kinetics-600数据集上的自回归模型中表现出色，并可与基于扩散的方法相媲美。

Conclusion: CanvasMAR通过引入画布机制和组合式引导，有效解决了视频MAR模型的慢启动和误差累积问题，实现了更快、更连贯、更高质量的视频合成，在性能上达到了先进水平。

Abstract: Masked autoregressive models (MAR) have recently emerged as a powerful
paradigm for image and video generation, combining the flexibility of masked
modeling with the potential of continuous tokenizer. However, video MAR models
suffer from two major limitations: the slow-start problem, caused by the lack
of a structured global prior at early sampling stages, and error accumulation
across the autoregression in both spatial and temporal dimensions. In this
work, we propose CanvasMAR, a novel video MAR model that mitigates these issues
by introducing a canvas mechanism--a blurred, global prediction of the next
frame, used as the starting point for masked generation. The canvas provides
global structure early in sampling, enabling faster and more coherent frame
synthesis. Furthermore, we introduce compositional classifier-free guidance
that jointly enlarges spatial (canvas) and temporal conditioning, and employ
noise-based canvas augmentation to enhance robustness. Experiments on the BAIR
and Kinetics-600 benchmarks demonstrate that CanvasMAR produces high-quality
videos with fewer autoregressive steps. Our approach achieves remarkable
performance among autoregressive models on Kinetics-600 dataset and rivals
diffusion-based methods.

</details>


### [78] [DepthVLA: Enhancing Vision-Language-Action Models with Depth-Aware Spatial Reasoning](https://arxiv.org/abs/2510.13375)
*Tianyuan Yuan,Yicheng Liu,Chenhao Lu,Zhuoguang Chen,Tao Jiang,Hang Zhao*

Main category: cs.CV

TL;DR: 现有视觉-语言-动作（VLA）模型在需要精确空间推理的任务上表现不佳。本文提出了DepthVLA，通过整合预训练的深度预测模块和混合Transformer设计，显著提升了VLA模型的空间推理能力，并在真实世界和模拟环境中取得了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: VLA模型在泛化和语言引导操作方面表现出色，但由于继承了视觉-语言模型（VLM）的空间推理局限性，在需要精确空间推理的任务上性能下降。现有方法依赖于大量的动作数据预训练，效率低下且仍不足以实现准确的空间理解。

Method: DepthVLA通过一个预训练的深度预测模块明确地引入了空间感知能力。它采用了一种混合Transformer设计，将VLM、深度Transformer和动作专家与完全共享的注意力机制统一起来，形成了一个端到端模型，从而增强了空间推理能力。

Result: 在真实世界和模拟环境中的广泛评估表明，DepthVLA优于现有最先进的方法。在真实世界任务中，其性能从65.0%提升到78.5%；在LIBERO模拟器中，从93.6%提升到94.9%；在Simpler模拟器中，从58.8%提升到74.8%。

Conclusion: DepthVLA通过显式整合深度预测模块和采用混合Transformer设计，有效解决了VLA模型在精确空间推理方面的不足，显著提升了模型性能，为未来的VLA模型发展提供了新方向。

Abstract: Vision-Language-Action (VLA) models have recently shown impressive
generalization and language-guided manipulation capabilities. However, their
performance degrades on tasks requiring precise spatial reasoning due to
limited spatial reasoning inherited from Vision-Language Models (VLMs).
Existing VLAs rely on extensive action-data pretraining to ground VLMs in 3D
space, which reduces training efficiency and is still insufficient for accurate
spatial understanding. In this work, we present DepthVLA, a simple yet
effective VLA architecture that explicitly incorporates spatial awareness
through a pretrained depth prediction module. DepthVLA adopts a
mixture-of-transformers design that unifies a VLM, a depth transformer, and an
action expert with fully shared attentions, forming an end-to-end model with
enhanced spatial reasoning. Extensive evaluations in both real-world and
simulated environments show that DepthVLA outperforms state-of-the-art
approaches, achieving 78.5% vs. 65.0% progress in real-world tasks, 94.9% vs.
93.6% in the LIBERO simulator, and 74.8% vs. 58.8% in the Simpler simulator.
Our code will be made publicly available.

</details>


### [79] [MVCustom: Multi-View Customized Diffusion via Geometric Latent Rendering and Completion](https://arxiv.org/abs/2510.13702)
*Minjung Shin,Hyunin Cho,Sooyeon Go,Jin-Hwa Kim,Youngjung Uh*

Main category: cs.CV

TL;DR: MVCustom是一个新颖的扩散模型框架，首次实现了同时具有多视角相机姿态控制和提示词定制功能的生成，解决了现有模型在这两方面统一的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的多视角生成模型不支持几何一致性的定制，而定制模型又缺乏明确的视角控制。此外，定制任务的训练数据稀缺，导致现有依赖大规模数据集的模型难以泛化。

Method: 本文提出了MVCustom框架。在训练阶段，它通过特征场表示学习主体身份和几何，并利用增强了密集时空注意力的文本到视频扩散骨干来确保多视角一致性。在推理阶段，引入了深度感知特征渲染以强制执行几何一致性，以及一致性感知潜在补全以确保定制主体和背景的准确透视对齐。

Result: 广泛的实验证明，MVCustom是目前唯一能够同时实现忠实多视角生成和定制的框架。

Conclusion: MVCustom成功地解决了多视角生成和定制的统一挑战，实现了具有相机姿态控制和提示词定制能力的几何一致性多视角生成，填补了现有技术的空白。

Abstract: Multi-view generation with camera pose control and prompt-based customization
are both essential elements for achieving controllable generative models.
However, existing multi-view generation models do not support customization
with geometric consistency, whereas customization models lack explicit
viewpoint control, making them challenging to unify. Motivated by these gaps,
we introduce a novel task, multi-view customization, which aims to jointly
achieve multi-view camera pose control and customization. Due to the scarcity
of training data in customization, existing multi-view generation models, which
inherently rely on large-scale datasets, struggle to generalize to diverse
prompts. To address this, we propose MVCustom, a novel diffusion-based
framework explicitly designed to achieve both multi-view consistency and
customization fidelity. In the training stage, MVCustom learns the subject's
identity and geometry using a feature-field representation, incorporating the
text-to-video diffusion backbone enhanced with dense spatio-temporal attention,
which leverages temporal coherence for multi-view consistency. In the inference
stage, we introduce two novel techniques: depth-aware feature rendering
explicitly enforces geometric consistency, and consistent-aware latent
completion ensures accurate perspective alignment of the customized subject and
surrounding backgrounds. Extensive experiments demonstrate that MVCustom is the
only framework that simultaneously achieves faithful multi-view generation and
customization.

</details>


### [80] [Leveraging 2D Priors and SDF Guidance for Dynamic Urban Scene Rendering](https://arxiv.org/abs/2510.13381)
*Siddharth Tourani,Jayaram Reddy,Akash Kumbar,Satyajit Tourani,Nishant Goyal,Madhava Krishna,N. Dinesh Reddy,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: 本文提出一种新颖方法，将有符号距离函数（SDF）与3D高斯泼溅（3DGS）结合，利用2D深度和点跟踪先验，在动态城市场景渲染和重建中，减少对LiDAR、3D分割和运动数据的依赖，实现了更精确和灵活的对象表示。


<details>
  <summary>Details</summary>
Motivation: 现有基于3DGS的动态城市场景建模方法，需要大量的输入数据，包括相机和LiDAR数据、真值3D分割以及轨迹或预定义对象模板形式的运动数据。研究旨在探索是否能通过结合2D对象无关先验（如深度和点跟踪）与动态对象的SDF表示，来放宽这些数据要求。

Method: 本研究提出了一种将有符号距离函数（SDF）与3D高斯泼溅（3DGS）集成的创新方法，通过统一的优化框架，融合两者的优势来创建更鲁棒的对象表示。该方法利用2D对象无关先验（深度和点跟踪）以及动态对象的SDF表示，以减少对昂贵3D数据的依赖。

Result: 该方法在没有LiDAR数据的情况下，在城市场景渲染指标上达到了最先进的性能。在结合LiDAR数据时，该方法在重建和生成跨不同对象类别的视角方面进一步提升，且无需真值3D运动标注。此外，该方法还支持多种场景编辑任务，包括场景分解和场景合成。

Conclusion: 通过将SDF与3DGS相结合，并利用2D对象无关先验，本研究提出了一种在动态城市场景渲染和重建中，减少数据依赖、提高几何精度和形变建模能力的鲁棒、适应性强且精确的对象表示方法，并实现了最先进的性能和场景编辑功能。

Abstract: Dynamic scene rendering and reconstruction play a crucial role in computer
vision and augmented reality. Recent methods based on 3D Gaussian Splatting
(3DGS), have enabled accurate modeling of dynamic urban scenes, but for urban
scenes they require both camera and LiDAR data, ground-truth 3D segmentations
and motion data in the form of tracklets or pre-defined object templates such
as SMPL. In this work, we explore whether a combination of 2D object agnostic
priors in the form of depth and point tracking coupled with a signed distance
function (SDF) representation for dynamic objects can be used to relax some of
these requirements. We present a novel approach that integrates Signed Distance
Functions (SDFs) with 3D Gaussian Splatting (3DGS) to create a more robust
object representation by harnessing the strengths of both methods. Our unified
optimization framework enhances the geometric accuracy of 3D Gaussian splatting
and improves deformation modeling within the SDF, resulting in a more adaptable
and precise representation. We demonstrate that our method achieves
state-of-the-art performance in rendering metrics even without LiDAR data on
urban scenes. When incorporating LiDAR, our approach improved further in
reconstructing and generating novel views across diverse object categories,
without ground-truth 3D motion annotation. Additionally, our method enables
various scene editing tasks, including scene decomposition, and scene
composition.

</details>


### [81] [Reinforcement Learning Meets Masked Generative Models: Mask-GRPO for Text-to-Image Generation](https://arxiv.org/abs/2510.13418)
*Yifu Luo,Xinhao Hu,Keyu Fan,Haoyuan Sun,Zeyu Chen,Bo Xia,Tiantian Zhang,Yongzhe Chang,Xueqian Wang*

Main category: cs.CV

TL;DR: Mask-GRPO首次将基于GRPO的强化学习引入掩码生成模型，用于文本到图像生成，通过重新定义转移概率和多步决策，显著提升了生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像生成中的强化学习方法主要针对扩散模型或自回归模型，忽略了掩码生成模型这一重要替代方案。

Method: 提出Mask-GRPO方法，将基于GRPO的强化学习应用于掩码生成模型。核心思想是重新定义转移概率，并将去掩码过程公式化为多步决策问题。此外，探索了移除KL约束、应用规约策略和过滤低质量样本等增强策略。

Result: 使用Mask-GRPO改进了基础模型Show-o，在标准文本到图像基准和偏好对齐方面取得了显著提升，超越了现有最先进的方法。

Conclusion: Mask-GRPO成功地将强化学习集成到被忽视的掩码生成模型范式中，实现了优于现有技术的文本到图像生成性能。

Abstract: Reinforcement learning (RL) has garnered increasing attention in
text-to-image (T2I) generation. However, most existing RL approaches are
tailored to either diffusion models or autoregressive models, overlooking an
important alternative: masked generative models. In this work, we propose
Mask-GRPO, the first method to incorporate Group Relative Policy Optimization
(GRPO)-based RL into this overlooked paradigm. Our core insight is to redefine
the transition probability, which is different from current approaches, and
formulate the unmasking process as a multi-step decision-making problem. To
further enhance our method, we explore several useful strategies, including
removing the KL constraint, applying the reduction strategy, and filtering out
low-quality samples. Using Mask-GRPO, we improve a base model, Show-o, with
substantial improvements on standard T2I benchmarks and preference alignment,
outperforming existing state-of-the-art approaches. The code is available on
https://github.com/xingzhejun/Mask-GRPO

</details>


### [82] [Generalizing WiFi Gesture Recognition via Large-Model-Aware Semantic Distillation and Alignment](https://arxiv.org/abs/2510.13390)
*Feng-Qi Cui,Yu-Tong Guo,Tianyue Zheng,Jinyang Huang*

Main category: cs.CV

TL;DR: 本文提出了一种名为GLSDA的新型泛化框架，通过利用预训练大型基础模型的语义先验来增强WiFi手势识别的表示学习。该框架结合了双路径CSI编码、多尺度语义编码器、语义感知软监督和鲁棒双蒸馏策略，有效解决了现有方法泛化能力和语义表达受限的问题，并在Widar3.0基准测试中超越了现有技术，同时显著减小了模型尺寸和推理延迟。


<details>
  <summary>Details</summary>
Motivation: WiFi手势识别在AIoT环境中作为非接触式、保护隐私的人机交互方式前景广阔。然而，现有方法由于信道状态信息（CSI）的域敏感性以及缺乏高层手势抽象，通常存在泛化能力有限和语义表达不足的问题。

Method: GLSDA框架利用预训练大型基础模型的语义先验来增强手势表示学习。具体方法包括：1) 设计双路径CSI编码流水线，通过CSI-Ratio相位序列和多普勒频谱图捕获几何和动态手势模式；2) 采用多尺度语义编码器，学习鲁棒的时间嵌入并通过跨模态注意力机制将其与手势语义对齐；3) 引入语义感知软监督方案，编码类间相关性并减少标签模糊性；4) 开发鲁棒双蒸馏策略，将对齐模型压缩为轻量级学生网络，联合蒸馏教师模型的中间特征和语义软标签。

Result: 在Widar3.0基准测试上的大量实验表明，GLSDA在域内和跨域手势识别任务中均持续优于现有最先进的方法，同时显著减小了模型尺寸并降低了推理延迟。

Conclusion: GLSDA为现实世界AIoT应用中的通用RF手势接口提供了一种可扩展且可部署的解决方案。

Abstract: WiFi-based gesture recognition has emerged as a promising RF sensing paradigm
for enabling non-contact and privacy-preserving human-computer interaction in
AIoT environments. However, existing methods often suffer from limited
generalization and semantic expressiveness due to the domain-sensitive nature
of Channel State Information and the lack of high-level gesture abstraction. To
address these challenges, we propose a novel generalization framework, termed
Large-Model-Aware Semantic Distillation and Alignment (GLSDA), which leverages
the semantic prior of pre-trained large foundation models to enhance gesture
representation learning in both in-domain and cross-domain scenarios.
Specifically, we first design a dual-path CSI encoding pipeline that captures
geometric and dynamic gesture patterns via CSI-Ratio phase sequences and
Doppler spectrograms. These representations are then fed into a Multiscale
Semantic Encoder, which learns robust temporal embeddings and aligns them with
gesture semantics through cross-modal attention mechanisms. To further enhance
category discrimination, we introduce a Semantic-Aware Soft Supervision scheme
that encodes inter-class correlations and reduces label ambiguity, especially
for semantically similar gestures. Finally, we develop a Robust
Dual-Distillation strategy to compress the aligned model into a lightweight
student network, jointly distilling intermediate features and semantic-informed
soft labels from the teacher model. Extensive experiments on the Widar3.0
benchmark show that GLSDA consistently outperforms state-of-the-art methods in
both in-domain and cross-domain gesture recognition tasks, while significantly
reducing model size and inference latency. Our method offers a scalable and
deployable solution for generalized RF-based gesture interfaces in real-world
AIoT applications.

</details>


### [83] [Spatial-DISE: A Unified Benchmark for Evaluating Spatial Reasoning in Vision-Language Models](https://arxiv.org/abs/2510.13394)
*Xinmiao Huang,Qisong He,Zhenglin Huang,Boxuan Wang,Zhuoyun Li,Guangliang Cheng,Yi Dong,Xiaowei Huang*

Main category: cs.CV

TL;DR: 本文提出了一个名为Spatial-DISE的统一基准和数据集，用于评估视觉语言模型（VLMs）的空间推理能力，特别关注人类空间认知中的内在-动态推理。评估结果显示，当前VLMs与人类能力存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 空间推理能力对VLM支持机器人、增强现实和自动导航等真实世界应用至关重要。然而，现有基准在评估空间推理能力方面存在不足，尤其是在人类空间认知基础方面的内在-动态空间推理。

Method: 研究基于认知分类学提出了一个统一的Spatial-DISE基准，将任务分为内在-静态、内在-动态、外在-静态和外在-动态四类。为解决数据稀缺问题，开发了一个可扩展的自动化流水线，生成多样化和可验证的空间推理问题，从而创建了Spatial-DISE数据集，包括Spatial-DISE Bench（559对VQA）和Spatial-DISE-12K（12K+对VQA）。

Result: 对28个最先进的VLM进行的综合评估表明，当前VLM与人类能力之间存在巨大且一致的差距，尤其是在多步骤多视图空间推理方面。

Conclusion: Spatial-DISE提供了一个鲁棒的框架、有价值的数据集和明确的方向，以推动未来研究实现类人空间智能。基准、数据集和代码将公开发布。

Abstract: Spatial reasoning ability is crucial for Vision Language Models (VLMs) to
support real-world applications in diverse domains including robotics,
augmented reality, and autonomous navigation. Unfortunately, existing
benchmarks are inadequate in assessing spatial reasoning ability, especially
the \emph{intrinsic-dynamic} spatial reasoning which is a fundamental aspect of
human spatial cognition. In this paper, we propose a unified benchmark,
\textbf{Spatial-DISE}, based on a cognitively grounded taxonomy that
categorizes tasks into four fundamental quadrants:
\textbf{I}ntrinsic-\textbf{S}tatic, Intrinsic-\textbf{D}ynamic,
\textbf{E}xtrinsic-Static, and Extrinsic-Dynamic spatial reasoning. Moreover,
to address the issue of data scarcity, we develop a scalable and automated
pipeline to generate diverse and verifiable spatial reasoning questions,
resulting in a new \textbf{Spatial-DISE} dataset that includes Spatial-DISE
Bench (559 evaluation VQA pairs) and Spatial-DISE-12K (12K+ training VQA
pairs). Our comprehensive evaluation across 28 state-of-the-art VLMs reveals
that, current VLMs have a large and consistent gap to human competence,
especially on multi-step multi-view spatial reasoning. Spatial-DISE offers a
robust framework, valuable dataset, and clear direction for future research
toward human-like spatial intelligence. Benchmark, dataset, and code will be
publicly released.

</details>


### [84] [Multi-Scale High-Resolution Logarithmic Grapher Module for Efficient Vision GNNs](https://arxiv.org/abs/2510.13740)
*Mustafa Munir,Alex Zhang,Radu Marculescu*

Main category: cs.CV

TL;DR: 该研究提出了一种新的图构建方法LSGC和混合CNN-GNN模型LogViG，通过限制长距离连接并引入多尺度高分辨率架构，在图像分类和语义分割任务上超越了现有ViG、CNN和ViT模型，实现了更高的准确性和更低的计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有的图神经网络（ViG）在视觉任务中表现出潜力，但常见的图构建方法（如KNN）在大图像上计算成本高昂。虽然SVGA等方法有所改进，但其固定的步长尺度可能导致信息过度压缩或错过重要的长距离连接，无法有效获取信息。

Method: 本研究提出了一种对数可伸缩图构建（LSGC）方法，通过限制长距离连接的数量来增强性能。在此基础上，提出了一种新颖的混合CNN-GNN模型LogViG，它利用了LSGC。此外，受多尺度和高分辨率架构成功的启发，引入并应用了一个高分辨率分支，并在高分辨率和低分辨率分支之间融合特征，形成了一个多尺度高分辨率视觉GNN网络。

Result: 实验结果表明，LogViG在图像分类和语义分割任务上，在准确性、GMACs和参数方面均优于现有的ViG、CNN和ViT架构。最小的模型Ti-LogViG在ImageNet-1K上取得了79.9%的平均top-1准确率，比Vision GNN高出1.7%，同时参数减少了24.3%，GMACs减少了35.3%。

Conclusion: 研究表明，通过提出的LSGC方法在ViG的图构建中有效利用长距离连接，可以超越当前最先进的ViG模型的性能。

Abstract: Vision graph neural networks (ViG) have demonstrated promise in vision tasks
as a competitive alternative to conventional convolutional neural nets (CNN)
and transformers (ViTs); however, common graph construction methods, such as
k-nearest neighbor (KNN), can be expensive on larger images. While methods such
as Sparse Vision Graph Attention (SVGA) have shown promise, SVGA's fixed step
scale can lead to over-squashing and missing multiple connections to gain the
same information that could be gained from a long-range link. Through this
observation, we propose a new graph construction method, Logarithmic Scalable
Graph Construction (LSGC) to enhance performance by limiting the number of
long-range links. To this end, we propose LogViG, a novel hybrid CNN-GNN model
that utilizes LSGC. Furthermore, inspired by the successes of multi-scale and
high-resolution architectures, we introduce and apply a high-resolution branch
and fuse features between our high-resolution and low-resolution branches for a
multi-scale high-resolution Vision GNN network. Extensive experiments show that
LogViG beats existing ViG, CNN, and ViT architectures in terms of accuracy,
GMACs, and parameters on image classification and semantic segmentation tasks.
Our smallest model, Ti-LogViG, achieves an average top-1 accuracy on
ImageNet-1K of 79.9% with a standard deviation of 0.2%, 1.7% higher average
accuracy than Vision GNN with a 24.3% reduction in parameters and 35.3%
reduction in GMACs. Our work shows that leveraging long-range links in graph
construction for ViGs through our proposed LSGC can exceed the performance of
current state-of-the-art ViGs. Code is available at
https://github.com/mmunir127/LogViG-Official.

</details>


### [85] [RECODE: Reasoning Through Code Generation for Visual Question Answering](https://arxiv.org/abs/2510.13756)
*Junhong Shen,Mu Cai,Bo Hu,Ameet Talwalkar,David A Ross,Cordelia Schmid,Alireza Fathi*

Main category: cs.CV

TL;DR: 多模态大语言模型在结构化视觉推理方面表现不佳，本文提出RECODE框架，通过将视觉内容逆向工程为可执行代码（derendering），实现可验证的视觉推理，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在处理图表、示意图等结构化视觉内容时，由于基于像素的感知缺乏验证机制，难以进行精确推理。

Method: RECODE是一个代理框架，首先生成多个候选程序来复现输入图像（即derendering），然后利用一个评论器选择最忠实的重建，并迭代优化代码。这一过程将模糊的感知任务转化为可验证的符号问题，从而实现精确计算和逻辑推理。

Result: 在CharXiv、ChartQA和Geometry3K等多个视觉推理基准测试中，RECODE显著优于那些不利用代码或仅将代码用于绘制辅助线或裁剪的方法。

Conclusion: 将视觉感知建立在可执行代码的基础上，为实现更准确、可验证的多模态推理提供了一条新途径。

Abstract: Multimodal Large Language Models (MLLMs) struggle with precise reasoning for
structured visuals like charts and diagrams, as pixel-based perception lacks a
mechanism for verification. To address this, we propose to leverage derendering
-- the process of reverse-engineering visuals into executable code -- as a new
modality for verifiable visual reasoning. Specifically, we propose RECODE, an
agentic framework that first generates multiple candidate programs to reproduce
the input image. It then uses a critic to select the most faithful
reconstruction and iteratively refines the code. This process not only
transforms an ambiguous perceptual task into a verifiable, symbolic problem,
but also enables precise calculations and logical inferences later on. On
various visual reasoning benchmarks such as CharXiv, ChartQA, and Geometry3K,
RECODE significantly outperforms methods that do not leverage code or only use
code for drawing auxiliary lines or cropping. Our work demonstrates that
grounding visual perception in executable code provides a new path toward more
accurate and verifiable multimodal reasoning.

</details>


### [86] [Ultra High-Resolution Image Inpainting with Patch-Based Content Consistency Adapter](https://arxiv.org/abs/2510.13419)
*Jianhui Zhang,Sheng Cheng,Qirui Sun,Jia Liu,Wang Luyang,Chaoyu Feng,Chen Fang,Lei Lei,Jue Wang,Shuaicheng Liu*

Main category: cs.CV

TL;DR: Patch-Adapter是一个用于高分辨率（4K+）文本引导图像修复的有效框架，通过两阶段适配器架构解决了现有方法在内容一致性和提示对齐方面的分辨率限制。


<details>
  <summary>Details</summary>
Motivation: 现有图像修复方法受限于较低分辨率，难以在4K+等高分辨率下保持精确的内容一致性和提示对齐，这些挑战随着分辨率和纹理复杂度的增加而加剧。

Method: Patch-Adapter采用两阶段适配器架构来扩展扩散模型的分辨率：(1) 双上下文适配器（Dual Context Adapter）在较低分辨率下学习掩码区域和非掩码区域之间的连贯性，以建立全局结构一致性；(2) 参考补丁适配器（Reference Patch Adapter）通过补丁级注意力机制实现全分辨率修复，并通过自适应特征融合保持局部细节保真度。这种架构将全局语义与局部细化解耦。

Result: Patch-Adapter不仅解决了大规模修复中常见的伪影问题，还在OpenImages和Photo-Concept-Bucket数据集上取得了最先进的性能，在感知质量和文本提示遵循性方面均优于现有方法，实现了4K+分辨率的图像修复。

Conclusion: Patch-Adapter通过解耦全局语义和局部细化，有效地解决了高分辨率图像修复中的可扩展性难题，提供了卓越的性能和质量，实现了高分辨率下的精确内容一致性和提示对齐。

Abstract: In this work, we present Patch-Adapter, an effective framework for
high-resolution text-guided image inpainting. Unlike existing methods limited
to lower resolutions, our approach achieves 4K+ resolution while maintaining
precise content consistency and prompt alignment, two critical challenges in
image inpainting that intensify with increasing resolution and texture
complexity. Patch-Adapter leverages a two-stage adapter architecture to scale
the diffusion model's resolution from 1K to 4K+ without requiring structural
overhauls: (1) Dual Context Adapter learns coherence between masked and
unmasked regions at reduced resolutions to establish global structural
consistency; and (2) Reference Patch Adapter implements a patch-level attention
mechanism for full-resolution inpainting, preserving local detail fidelity
through adaptive feature fusion. This dual-stage architecture uniquely
addresses the scalability gap in high-resolution inpainting by decoupling
global semantics from localized refinement. Experiments demonstrate that
Patch-Adapter not only resolves artifacts common in large-scale inpainting but
also achieves state-of-the-art performance on the OpenImages and
Photo-Concept-Bucket datasets, outperforming existing methods in both
perceptual quality and text-prompt adherence.

</details>


### [87] [CoDS: Enhancing Collaborative Perception in Heterogeneous Scenarios via Domain Separation](https://arxiv.org/abs/2510.13432)
*Yushan Han,Hui Zhang,Honglei Zhang,Chuntao Ding,Yuanzhouhan Cao,Yidong Li*

Main category: cs.CV

TL;DR: CoDS是一种用于异构自动驾驶场景的协同感知方法，通过域分离技术解决特征差异并提高推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有协同感知方法假设所有代理使用相同编码器，在异构场景中难以处理领域差异引起的噪声，且基于Transformer的域适应模块在移动设备上推理效率低下。

Method: CoDS提出两种特征对齐模块：轻量级空间-通道重塑器（LSCR）和通过域分离的分布对齐（DADS）。LSCR使用轻量级卷积层对齐空间和通道维度特征。DADS包含编码器特定（去除域相关信息）和编码器无关（捕获任务相关信息）的域分离模块，以缓解特征分布差异。同时，采用域对齐互信息（DAMI）损失，最大化对齐异构特征间的互信息，增强域分离过程。CoDS采用全卷积架构以确保高推理效率。

Result: CoDS有效缓解了异构场景中的特征差异，并在检测精度和推理效率之间取得了平衡。

Conclusion: CoDS为异构场景下的协同感知提供了一个有效且高效的解决方案，能够处理不同编码器带来的特征差异问题。

Abstract: Collaborative perception has been proven to improve individual perception in
autonomous driving through multi-agent interaction. Nevertheless, most methods
often assume identical encoders for all agents, which does not hold true when
these models are deployed in real-world applications. To realize collaborative
perception in actual heterogeneous scenarios, existing methods usually align
neighbor features to those of the ego vehicle, which is vulnerable to noise
from domain gaps and thus fails to address feature discrepancies effectively.
Moreover, they adopt transformer-based modules for domain adaptation, which
causes the model inference inefficiency on mobile devices. To tackle these
issues, we propose CoDS, a Collaborative perception method that leverages
Domain Separation to address feature discrepancies in heterogeneous scenarios.
The CoDS employs two feature alignment modules, i.e., Lightweight
Spatial-Channel Resizer (LSCR) and Distribution Alignment via Domain Separation
(DADS). Besides, it utilizes the Domain Alignment Mutual Information (DAMI)
loss to ensure effective feature alignment. Specifically, the LSCR aligns the
neighbor feature across spatial and channel dimensions using a lightweight
convolutional layer. Subsequently, the DADS mitigates feature distribution
discrepancy with encoder-specific and encoder-agnostic domain separation
modules. The former removes domain-dependent information and the latter
captures task-related information. During training, the DAMI loss maximizes the
mutual information between aligned heterogeneous features to enhance the domain
separation process. The CoDS employs a fully convolutional architecture, which
ensures high inference efficiency. Extensive experiments demonstrate that the
CoDS effectively mitigates feature discrepancies in heterogeneous scenarios and
achieves a trade-off between detection accuracy and inference efficiency.

</details>


### [88] [Scaling Vision Transformers for Functional MRI with Flat Maps](https://arxiv.org/abs/2510.13768)
*Connor Lane,Daniel Z. Kaplan,Tanishq Mathew Abraham,Paul S. Scotti*

Main category: cs.CV

TL;DR: 该研究将4D fMRI数据转换为2D活动平面图视频，并使用时空掩码自编码器（MAE）框架训练Vision Transformers。模型性能遵循严格的幂律缩放，并能学习丰富的表征，支持跨受试者的精细状态解码和受试者特异性特质解码。


<details>
  <summary>Details</summary>
Motivation: 将现代深度学习架构应用于功能磁共振成像（fMRI）的关键问题是如何表示数据以供模型输入，以及如何弥合fMRI与自然图像之间的模态差距。

Method: 研究将4D容积fMRI数据转换为2D fMRI活动平面图视频。然后，使用时空掩码自编码器（MAE）框架，在来自人类连接组项目（Human Connectome Project）的2.3K小时fMRI平面图视频上训练Vision Transformers。模型的评估包括掩码fMRI建模性能和下游分类基准测试。

Result: 研究发现，掩码fMRI建模性能随数据集大小的增加而提高，并遵循严格的幂律缩放定律。下游分类基准测试表明，该模型学习了丰富的表征，支持跨受试者的精细状态解码，以及跨脑状态变化的受试者特异性特质解码。

Conclusion: 这项工作是构建fMRI数据基础模型的开放科学项目的一部分，成功地将深度学习架构应用于fMRI数据，并展示了其在学习丰富表征和支持多种解码任务方面的潜力。

Abstract: A key question for adapting modern deep learning architectures to functional
MRI (fMRI) is how to represent the data for model input. To bridge the modality
gap between fMRI and natural images, we transform the 4D volumetric fMRI data
into videos of 2D fMRI activity flat maps. We train Vision Transformers on 2.3K
hours of fMRI flat map videos from the Human Connectome Project using the
spatiotemporal masked autoencoder (MAE) framework. We observe that masked fMRI
modeling performance improves with dataset size according to a strict power
scaling law. Downstream classification benchmarks show that our model learns
rich representations supporting both fine-grained state decoding across
subjects, as well as subject-specific trait decoding across changes in brain
state. This work is part of an ongoing open science project to build foundation
models for fMRI data. Our code and datasets are available at
https://github.com/MedARC-AI/fmri-fm.

</details>


### [89] [Beyond Pixels: A Differentiable Pipeline for Probing Neuronal Selectivity in 3D](https://arxiv.org/abs/2510.13433)
*Pavithra Elumalai,Mohammad Bashiri,Goirik Chakrabarty,Suhas Shrinivasan,Fabian H. Sinz*

Main category: cs.CV

TL;DR: 本文介绍了一种可微分渲染管道，通过优化可变形三维网格来直接在三维空间中获取最大激发图像（MEIs），从而探究神经元对可解释三维场景属性（如姿态和光照）的选择性。


<details>
  <summary>Details</summary>
Motivation: 视觉感知依赖于对三维场景属性（如形状、姿态和光照）的推断。要理解视觉感觉神经元如何实现鲁棒感知，关键在于表征它们对这些物理可解释因素的选择性。然而，当前方法主要在二维像素上操作，难以分离对物理场景属性的选择性。

Method: 研究引入了一种可微分渲染管道，通过径向基函数参数化网格变形，并学习偏移和缩放以最大化神经元反应，同时强制执行几何规则性，从而直接在三维空间中优化可变形网格以获得MEIs。

Result: 该方法应用于猴子V4区域模型，能够探测神经元对可解释三维因素（如姿态和光照）的选择性。

Conclusion: 该方法将逆向图形学与系统神经科学相结合，提供了一种超越传统基于像素方法的方式，利用物理基础的三维刺激来探测神经选择性。

Abstract: Visual perception relies on inference of 3D scene properties such as shape,
pose, and lighting. To understand how visual sensory neurons enable robust
perception, it is crucial to characterize their selectivity to such physically
interpretable factors. However, current approaches mainly operate on 2D pixels,
making it difficult to isolate selectivity for physical scene properties. To
address this limitation, we introduce a differentiable rendering pipeline that
optimizes deformable meshes to obtain MEIs directly in 3D. The method
parameterizes mesh deformations with radial basis functions and learns offsets
and scales that maximize neuronal responses while enforcing geometric
regularity. Applied to models of monkey area V4, our approach enables probing
neuronal selectivity to interpretable 3D factors such as pose and lighting.
This approach bridges inverse graphics with systems neuroscience, offering a
way to probe neural selectivity with physically grounded, 3D stimuli beyond
conventional pixel-based methods.

</details>


### [90] [Bee: A High-Quality Corpus and Full-Stack Suite to Unlock Advanced Fully Open MLLMs](https://arxiv.org/abs/2510.13795)
*Yi Zhang,Bolin Ni,Xin-Sheng Chen,Heng-Rui Zhang,Yongming Rao,Houwen Peng,Qinglin Lu,Han Hu,Meng-Hao Guo,Shi-Min Hu*

Main category: cs.CV

TL;DR: 本文提出Honey-Data-15M数据集、HoneyPipe数据处理管道和DataStudio框架，旨在解决全开源多模态大语言模型（MLLMs）因数据质量不足而落后于专有模型的问题。通过在Honey-Data-15M上训练Bee-8B模型，实现了全开源MLLMs的新SOTA，性能可与甚至超越半开源模型竞争。


<details>
  <summary>Details</summary>
Motivation: 目前全开源多模态大语言模型（MLLMs）在性能上落后于专有模型，主要原因是监督微调（SFT）数据质量存在显著差距。现有开源数据集普遍存在噪声，且缺乏链式思考（CoT）等复杂推理数据，阻碍了模型高级能力的开发。

Method: 1. 引入了Honey-Data-15M数据集，包含约1500万个问答对，经过多重清洗技术处理，并通过新颖的双层（短和长）CoT策略进行丰富。2. 提出了HoneyPipe数据整理管道及其底层框架DataStudio，提供透明且可适应的数据整理方法。3. 在Honey-Data-15M上训练了8B参数的Bee-8B模型，以验证数据集和管道的有效性。

Result: 实验结果表明，Bee-8B模型为全开源MLLMs树立了新的SOTA，其性能与最近的半开源模型（如InternVL3.5-8B）具有竞争力，在某些情况下甚至超越了它们。研究团队还向社区发布了Honey-Data-15M语料库、HoneyPipe和DataStudio全栈套件、训练方案、评估工具以及模型权重。

Conclusion: 这项工作证明，有原则地关注数据质量是开发与半开源模型具有高度竞争力的全开源多模态大语言模型的关键途径。

Abstract: Fully open multimodal large language models (MLLMs) currently lag behind
proprietary counterparts, primarily due to a significant gap in data quality
for supervised fine-tuning (SFT). Existing open-source datasets are often
plagued by widespread noise and a critical deficit in complex reasoning data,
such as Chain-of-Thought (CoT), which hinders the development of advanced model
capabilities. Addressing these challenges, our work makes three primary
contributions. First, we introduce Honey-Data-15M, a new SFT dataset comprising
approximately 15 million QA pairs, processed through multiple cleaning
techniques and enhanced with a novel dual-level (short and long) CoT enrichment
strategy. Second, we introduce HoneyPipe, the data curation pipeline, and its
underlying framework DataStudio, providing the community with a transparent and
adaptable methodology for data curation that moves beyond static dataset
releases. Finally, to validate our dataset and pipeline, we train Bee-8B, an 8B
model on Honey-Data-15M. Experiments show that Bee-8B establishes a new
state-of-the-art (SOTA) for fully open MLLMs, achieving performance that is
competitive with, and in some cases surpasses, recent semi-open models such as
InternVL3.5-8B. Our work delivers to the community a suite of foundational
resources, including: the Honey-Data-15M corpus; the full-stack suite
comprising HoneyPipe and DataStudio; training recipes; an evaluation harness;
and the model weights. This effort demonstrates that a principled focus on data
quality is a key pathway to developing fully open MLLMs that are highly
competitive with their semi-open counterparts.

</details>


### [91] [Generative Universal Verifier as Multimodal Meta-Reasoner](https://arxiv.org/abs/2510.13804)
*Xinchen Zhang,Xiaoying Zhang,Youbin Wu,Yanbin Cao,Renrui Zhang,Ruihang Chu,Ling Yang,Yujiu Yang*

Main category: cs.CV

TL;DR: 本文引入了生成式通用验证器（GUV），这是一种用于下一代多模态推理的新概念和插件，旨在使视觉语言模型和统一多模态模型在推理和生成过程中能够对视觉结果进行反思和细化。研究贡献包括构建了ViVerBench基准、训练了OmniVerifier-7B通用验证器，并提出了OmniVerifier-TTS测试时缩放范式，显著提升了多模态推理的可靠性和生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在多模态推理中的视觉结果验证方面表现不佳，与人类水平存在显著差距。因此，需要一种机制来在推理和生成过程中对视觉结果进行可靠的反思和细化。

Method: 1. 构建了ViVerBench，一个包含16类关键任务的综合基准，用于评估多模态推理中的视觉结果。2. 设计了两个自动化管道来构建大规模视觉验证数据，并训练了OmniVerifier-7B，这是第一个用于通用视觉验证的全能生成式验证器。3. 提出了OmniVerifier-TTS，一种顺序测试时缩放范式，利用通用验证器在统一模型中连接图像生成和编辑，通过迭代细粒度优化来提高生成能力的上限。

Result: 1. 现有视觉语言模型在ViVerBench任务上表现持续不佳，凸显了可靠视觉验证方面与人类能力的巨大差距。2. OmniVerifier-7B在ViVerBench上取得了显著增益（+8.3），并识别出视觉验证中的三种原子能力。3. OmniVerifier-TTS在T2I-ReasonBench上实现了改进（+3.7），在GenEval++上实现了改进（+4.3），优于现有的并行测试时缩放方法（如Best-of-N）。

Conclusion: 通过为多模态推理赋予可靠的视觉验证能力，OmniVerifier在生成过程中的可靠反思和可扩展的测试时细化方面均取得了进展，标志着向更值得信赖和可控的下一代推理系统迈进了一步。

Abstract: We introduce Generative Universal Verifier, a novel concept and plugin
designed for next-generation multimodal reasoning in vision-language models and
unified multimodal models, providing the fundamental capability of reflection
and refinement on visual outcomes during the reasoning and generation process.
This work makes three main contributions: (1) We build ViVerBench, a
comprehensive benchmark spanning 16 categories of critical tasks for evaluating
visual outcomes in multimodal reasoning. Results show that existing VLMs
consistently underperform across these tasks, underscoring a substantial gap
from human-level capability in reliable visual verification. (2) We design two
automated pipelines to construct large-scale visual verification data and train
OmniVerifier-7B, the first omni-capable generative verifier trained for
universal visual verification and achieves notable gains on ViVerBench(+8.3).
Through training, we identify three atomic capabilities in visual verification
and demonstrate how they generalize and interact synergistically. (3) We
propose OmniVerifier-TTS, a sequential test-time scaling paradigm that
leverages the universal verifier to bridge image generation and editing within
unified models, enhancing the upper bound of generative ability through
iterative fine-grained optimization. Beyond generation, we extend universal
verifier to broader world-modeling interleaved reasoning scenarios.
Empirically, OmniVerifier-TTS achieves improvements on T2I-ReasonBench(+3.7),
and GenEval++(+4.3), outperforming existing parallel test-time scaling methods,
such as Best-of-N. By endowing multimodal reasoning with reliable visual
verification, OmniVerifier advances both reliable reflection during generation
and scalable test-time refinement, marking a step toward more trustworthy and
controllable next-generation reasoning systems.

</details>


### [92] [VIST3A: Text-to-3D by Stitching a Multi-view Reconstruction Network to a Video Generator](https://arxiv.org/abs/2510.13454)
*Hyojun Go,Dominik Narnhofer,Goutam Bhat,Prune Truong,Federico Tombari,Konrad Schindler*

Main category: cs.CV

TL;DR: VIST3A是一个结合文本到视频生成器和3D重建解码器的通用框架，通过模型拼接和奖励微调实现高质量的文本到3D生成，显著优于现有模型并支持点图生成。


<details>
  <summary>Details</summary>
Motivation: 利用大型预训练模型在视觉内容生成和3D重建方面的快速进展，探索将强大的文本到视频模型与几何重建系统结合，以实现文本到3D生成的新可能性。

Method: VIST3A框架解决了两个主要挑战：1) 模型拼接：通过识别3D解码器中与文本到视频生成器潜在表示最佳匹配的层，将两者拼接起来，仅需少量无标签数据。2) 对齐：通过适应直接奖励微调技术，将文本到视频生成器与拼接后的3D解码器对齐，确保生成的潜在表示能解码为一致且逼真的3D场景几何。

Result: VIST3A方法在不同视频生成器和3D重建模型组合下进行评估，所有测试配对均显著优于先前的输出高斯散斑的文本到3D模型。此外，通过选择合适的3D基础模型，VIST3A还能实现高质量的文本到点图生成。

Conclusion: VIST3A提供了一个通用且有效的框架，通过智能地结合现有强大的文本到视频生成器和3D重建系统，显著提升了文本到3D生成的质量，并拓展了如高品质文本到点图生成等新能力。

Abstract: The rapid progress of large, pretrained models for both visual content
generation and 3D reconstruction opens up new possibilities for text-to-3D
generation. Intuitively, one could obtain a formidable 3D scene generator if
one were able to combine the power of a modern latent text-to-video model as
"generator" with the geometric abilities of a recent (feedforward) 3D
reconstruction system as "decoder". We introduce VIST3A, a general framework
that does just that, addressing two main challenges. First, the two components
must be joined in a way that preserves the rich knowledge encoded in their
weights. We revisit model stitching, i.e., we identify the layer in the 3D
decoder that best matches the latent representation produced by the
text-to-video generator and stitch the two parts together. That operation
requires only a small dataset and no labels. Second, the text-to-video
generator must be aligned with the stitched 3D decoder, to ensure that the
generated latents are decodable into consistent, perceptually convincing 3D
scene geometry. To that end, we adapt direct reward finetuning, a popular
technique for human preference alignment. We evaluate the proposed VIST3A
approach with different video generators and 3D reconstruction models. All
tested pairings markedly improve over prior text-to-3D models that output
Gaussian splats. Moreover, by choosing a suitable 3D base model, VIST3A also
enables high-quality text-to-pointmap generation.

</details>


### [93] [Near-Infrared Hyperspectral Imaging Applications in Food Analysis -- Improving Algorithms and Methodologies](https://arxiv.org/abs/2510.13452)
*Ole-Christian Galbo Engstrøm*

Main category: cs.CV

TL;DR: 本论文探讨了近红外高光谱成像（NIR-HSI）在食品质量分析中的应用，比较了卷积神经网络（CNN）和偏最小二乘（PLS）模型，并开发了两个开源Python包。


<details>
  <summary>Details</summary>
Motivation: 旨在改进食品质量分析方法，特别是利用NIR-HSI技术，并通过比较先进的机器学习模型（如CNN）与传统方法（如PLS）来提升分析的准确性和效率。

Method: 通过四项研究和五个假设进行，主要比较了基于CNN（包括空间分析、时空光谱联合分析和带光谱卷积层的2D CNN）和PLS的模型。研究对象包括化学参数、空间分布、脂肪含量图谱生成以及大麦发芽能力。此外，还开发了两个用于快速PLS建模和交叉验证的开源Python包。

Result: 在化学和物理视觉信息相关的参数建模中，结合时空光谱分析的CNN通常优于仅空间分析的CNN和基于PLS的光谱分析。对于化学参数，带有光谱卷积层的2D CNN能提升预测性能。然而，对于样品中化学参数的平均含量分析，PLS光谱建模表现同样出色且被推荐。在猪腹肉脂肪含量化学图谱生成中，PLS方法产生非平滑且预测值超出0-100%范围的问题，而带光谱卷积层的2D CNN解决了这些问题。大麦发芽能力的建模因数据集发芽率低而未能得出结论。论文还开发了两个用于加速PLS建模和交叉验证的Python包。

Conclusion: 结合时空光谱分析的CNN模型，尤其是在整合光谱卷积层后，在NIR-HSI食品质量分析中显示出巨大潜力，尤其适用于需要考虑视觉信息的参数和化学参数的空间分布建模。PLS在平均化学含量分析中仍是推荐方法。空间分布建模的挑战在于获取空间分辨的参考值。此外，本研究还贡献了两个实用的开源机器学习工具。

Abstract: This thesis investigates the application of near-infrared hyperspectral
imaging (NIR-HSI) for food quality analysis. The investigation is conducted
through four studies operating with five research hypotheses. For several
analyses, the studies compare models based on convolutional neural networks
(CNNs) and partial least squares (PLS). Generally, joint spatio-spectral
analysis with CNNs outperforms spatial analysis with CNNs and spectral analysis
with PLS when modeling parameters where chemical and physical visual
information are relevant. When modeling chemical parameters with a
2-dimensional (2D) CNN, augmenting the CNN with an initial layer dedicated to
performing spectral convolution enhances its predictive performance by learning
a spectral preprocessing similar to that applied by domain experts. Still,
PLS-based spectral modeling performs equally well for analysis of the mean
content of chemical parameters in samples and is the recommended approach.
Modeling the spatial distribution of chemical parameters with NIR-HSI is
limited by the ability to obtain spatially resolved reference values.
Therefore, a study used bulk mean references for chemical map generation of fat
content in pork bellies. A PLS-based approach gave non-smooth chemical maps and
pixel-wise predictions outside the range of 0-100\%. Conversely, a 2D CNN
augmented with a spectral convolution layer mitigated all issues arising with
PLS. The final study attempted to model barley's germinative capacity by
analyzing NIR spectra, RGB images, and NIR-HSI images. However, the results
were inconclusive due to the dataset's low degree of germination. Additionally,
this thesis has led to the development of two open-sourced Python packages. The
first facilitates fast PLS-based modeling, while the second facilitates very
fast cross-validation of PLS and other classical machine learning models with a
new algorithm.

</details>


### [94] [High Semantic Features for the Continual Learning of Complex Emotions: a Lightweight Solution](https://arxiv.org/abs/2510.13534)
*Thibault Geoffroy,gauthier Gerspacher,Lionel Prevost*

Main category: cs.CV

TL;DR: 本文提出一种利用面部动作单元（Action Units, AUs）进行复杂情绪增量学习的方法，有效避免灾难性遗忘，并在CFEE数据集上取得了高精度和轻量级模型。


<details>
  <summary>Details</summary>
Motivation: 增量学习中存在灾难性遗忘问题，即学习新任务时旧任务的知识会丢失，这主要是由于任务间特征的不稳定性。本研究旨在解决复杂情绪识别中的这一挑战。

Method: 研究首先学习基本情绪，然后像人类一样增量学习复杂情绪。核心方法是利用描述面部肌肉运动的动作单元（AUs）作为非瞬态、高度语义的特征，并将其与浅层和深层卷积神经网络提取的特征进行比较。

Result: 研究表明，动作单元（AUs）的性能优于浅层和深层卷积神经网络提取的特征。该方法在CFEE数据集上增量学习复杂复合情绪时达到了0.75的准确率，与最先进的结果相当。此外，该模型轻量且内存占用小。

Conclusion: 动作单元（AUs）是有效的非瞬态、高度语义特征，能成功应用于复杂情绪的增量学习，有效防止灾难性遗忘，并实现高精度和高效的模型。

Abstract: Incremental learning is a complex process due to potential catastrophic
forgetting of old tasks when learning new ones. This is mainly due to transient
features that do not fit from task to task. In this paper, we focus on complex
emotion recognition. First, we learn basic emotions and then, incrementally,
like humans, complex emotions. We show that Action Units, describing facial
muscle movements, are non-transient, highly semantical features that outperform
those extracted by both shallow and deep convolutional neural networks. Thanks
to this ability, our approach achieves interesting results when learning
incrementally complex, compound emotions with an accuracy of 0.75 on the CFEE
dataset and can be favorably compared to state-of-the-art results. Moreover, it
results in a lightweight model with a small memory footprint.

</details>


### [95] [Learning Neural Parametric 3D Breast Shape Models for Metrical Surface Reconstruction From Monocular RGB Videos](https://arxiv.org/abs/2510.13540)
*Maximilian Weiherer,Antonia von Riedheim,Vanessa Brébant,Bernhard Egger,Christoph Palm*

Main category: cs.CV

TL;DR: 本文提出了一种基于局部隐式神经表示的3D乳房形状模型（liRBSM），并基于此模型开发了一个低成本、易于获取的3D表面重建流程，能够从单目RGB视频中准确恢复乳房几何形状，误差小于2毫米。


<details>
  <summary>Details</summary>
Motivation: 现有的3D乳房扫描解决方案昂贵且需要专用硬件/软件，而低成本替代方案效果有限。研究动机在于开发一种无需特殊设备、仅使用普通RGB视频即可实现高精度3D乳房重建的经济实惠且易于获取的方法。

Method: 该方法的核心是一个神经参数化3D乳房形状模型liRBSM，并结合了一个3D表面重建流程。该流程利用现成的Structure-from-motion (SfM)技术，并与liRBSM模型配对。liRBSM模型受最新面部模型的启发，将隐式乳房域分解为多个较小的区域，每个区域由一个锚定在解剖标志点上的局部神经符号距离函数（SDF）表示，这与之前使用单个全局SDF的iRBSM模型不同。

Result: 所提出的liRBSM模型在重建质量方面显著优于iRBSM模型，能够生成更详细的表面重建。整个重建流程能够以小于2毫米的误差恢复高质量的3D乳房几何形状。此外，该方法速度快（耗时不到六分钟），完全透明且开源。

Conclusion: 研究成功地引入了一个基于局部隐式神经表示的3D乳房形状模型及其重建流程，实现了从单目RGB视频中准确、快速、低成本地恢复高质量3D乳房几何形状。这为3D乳房建模提供了一个更易于获取和使用的解决方案。

Abstract: We present a neural parametric 3D breast shape model and, based on this
model, introduce a low-cost and accessible 3D surface reconstruction pipeline
capable of recovering accurate breast geometry from a monocular RGB video. In
contrast to widely used, commercially available yet prohibitively expensive 3D
breast scanning solutions and existing low-cost alternatives, our method
requires neither specialized hardware nor proprietary software and can be used
with any device that is able to record RGB videos. The key building blocks of
our pipeline are a state-of-the-art, off-the-shelf Structure-from-motion
pipeline, paired with a parametric breast model for robust and metrically
correct surface reconstruction. Our model, similarly to the recently proposed
implicit Regensburg Breast Shape Model (iRBSM), leverages implicit neural
representations to model breast shapes. However, unlike the iRBSM, which
employs a single global neural signed distance function (SDF), our approach --
inspired by recent state-of-the-art face models -- decomposes the implicit
breast domain into multiple smaller regions, each represented by a local neural
SDF anchored at anatomical landmark positions. When incorporated into our
surface reconstruction pipeline, the proposed model, dubbed liRBSM (short for
localized iRBSM), significantly outperforms the iRBSM in terms of
reconstruction quality, yielding more detailed surface reconstruction than its
global counterpart. Overall, we find that the introduced pipeline is able to
recover high-quality 3D breast geometry within an error margin of less than 2
mm. Our method is fast (requires less than six minutes), fully transparent and
open-source, and -- together with the model -- publicly available at
https://rbsm.re-mic.de/local-implicit.

</details>


### [96] [ExpressNet-MoE: A Hybrid Deep Neural Network for Emotion Recognition](https://arxiv.org/abs/2510.13493)
*Deeptimaan Banerjee,Prateek Gothwal,Ashis Kumer Biswas*

Main category: cs.CV

TL;DR: 本文提出了一种名为ExpressNet-MoE的混合深度学习模型，结合了卷积神经网络（CNNs）和专家混合（MoE）框架，通过自适应专家选择和多尺度特征提取，显著提高了真实世界面部情感识别（FER）的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 面部情感识别在在线教育、医疗、安全和人机交互等领域至关重要，但由于头部姿态变化、遮挡、光照变化和人群多样性等因素，真实世界的FER仍然面临挑战，这限制了如参与度检测等应用的发展。

Method: ExpressNet-MoE模型是一种混合深度学习模型，结合了CNNs和MoE框架。它通过多尺度特征提取来捕获全局和局部面部特征，动态选择最相关的专家网络以提高泛化能力和灵活性。该模型包含多个基于CNN的特征提取器、一个用于自适应特征选择的MoE模块，以及一个用于深度特征学习的残差网络骨干。

Result: 该模型在多个数据集上进行了评估，并与现有最先进方法进行了比较。结果显示，它在AffectNet (v7) 上达到74.77%的准确率，AffectNet (v8) 上达到72.55%，RAF-DB 上达到84.29%，FER-2013 上达到64.66%。这些结果表明了模型的自适应性。

Conclusion: ExpressNet-MoE模型具有高度的自适应性，可用于在实际环境中开发端到端的情感识别系统。其代码和结果已公开，方便复现。

Abstract: In many domains, including online education, healthcare, security, and
human-computer interaction, facial emotion recognition (FER) is essential.
Real-world FER is still difficult despite its significance because of some
factors such as variable head positions, occlusions, illumination shifts, and
demographic diversity. Engagement detection, which is essential for
applications like virtual learning and customer services, is frequently
challenging due to FER limitations by many current models. In this article, we
propose ExpressNet-MoE, a novel hybrid deep learning model that blends both
Convolution Neural Networks (CNNs) and Mixture of Experts (MoE) framework, to
overcome the difficulties. Our model dynamically chooses the most pertinent
expert networks, thus it aids in the generalization and providing flexibility
to model across a wide variety of datasets. Our model improves on the accuracy
of emotion recognition by utilizing multi-scale feature extraction to collect
both global and local facial features. ExpressNet-MoE includes numerous
CNN-based feature extractors, a MoE module for adaptive feature selection, and
finally a residual network backbone for deep feature learning. To demonstrate
efficacy of our proposed model we evaluated on several datasets, and compared
with current state-of-the-art methods. Our model achieves accuracies of 74.77%
on AffectNet (v7), 72.55% on AffectNet (v8), 84.29% on RAF-DB, and 64.66% on
FER-2013. The results show how adaptive our model is and how it may be used to
develop end-to-end emotion recognition systems in practical settings.
Reproducible codes and results are made publicly accessible at
https://github.com/DeeptimaanB/ExpressNet-MoE.

</details>


### [97] [Fusion Meets Diverse Conditions: A High-diversity Benchmark and Baseline for UAV-based Multimodal Object Detection with Condition Cues](https://arxiv.org/abs/2510.13620)
*Chen Chen,Kangcheng Bin,Ting Hu,Jiahao Qi,Xingyue Liu,Tianpeng Liu,Zhen Liu,Yongxiang Liu,Ping Zhong*

Main category: cs.CV

TL;DR: 本文提出一个高多样性无人机（UAV）RGB-IR目标检测数据集ATR-UMOD，并开发了一种新颖的提示引导条件感知动态融合（PCDF）方法，以适应复杂成像条件下的多模态贡献，实现更鲁棒的全天候检测。


<details>
  <summary>Details</summary>
Motivation: 现有的无人机RGB-IR目标检测数据集未能充分捕捉真实世界的复杂性，成像条件有限，限制了深度学习技术在全天候鲁棒检测中的应用。

Method: 研究者引入了ATR-UMOD数据集，该数据集涵盖了从80m到300m的高度、0°到75°的角度，以及全年全天候的丰富天气和光照条件，并为每对RGB-IR图像标注了6个条件属性。为应对多样性挑战，提出了一种提示引导条件感知动态融合（PCDF）方法，通过将成像条件编码为文本提示，利用任务特定的软门控转换来建模条件与多模态贡献之间的关系，从而自适应地重新分配多模态贡献。此外，还包含一个提示引导条件解耦模块，以确保在没有条件标注的情况下也能实际应用。

Result: 在ATR-UMOD数据集上进行的实验表明，所提出的PCDF方法是有效的。

Conclusion: ATR-UMOD数据集和PCDF方法有效解决了无人机RGB-IR目标检测中真实世界复杂多变条件带来的挑战，提高了检测的鲁棒性。

Abstract: Unmanned aerial vehicles (UAV)-based object detection with visible (RGB) and
infrared (IR) images facilitates robust around-the-clock detection, driven by
advancements in deep learning techniques and the availability of high-quality
dataset. However, the existing dataset struggles to fully capture real-world
complexity for limited imaging conditions. To this end, we introduce a
high-diversity dataset ATR-UMOD covering varying scenarios, spanning altitudes
from 80m to 300m, angles from 0{\deg} to 75{\deg}, and all-day, all-year time
variations in rich weather and illumination conditions. Moreover, each RGB-IR
image pair is annotated with 6 condition attributes, offering valuable
high-level contextual information. To meet the challenge raised by such diverse
conditions, we propose a novel prompt-guided condition-aware dynamic fusion
(PCDF) to adaptively reassign multimodal contributions by leveraging annotated
condition cues. By encoding imaging conditions as text prompts, PCDF
effectively models the relationship between conditions and multimodal
contributions through a task-specific soft-gating transformation. A
prompt-guided condition-decoupling module further ensures the availability in
practice without condition annotations. Experiments on ATR-UMOD dataset reveal
the effectiveness of PCDF.

</details>


### [98] [XD-RCDepth: Lightweight Radar-Camera Depth Estimation with Explainability-Aligned and Distribution-Aware Distillation](https://arxiv.org/abs/2510.13565)
*Huawei Sun,Zixu Wang,Xiangyuan Peng,Julius Ott,Georg Stettinger,Lorenzo Servadei,Robert Wille*

Main category: cs.CV

TL;DR: 本文提出XD-RCDepth，一个轻量级雷达-相机融合深度估计架构，通过引入两种知识蒸馏策略（可解释性对齐和深度分布蒸馏），在显著减少参数的同时保持了竞争性精度和实时效率。


<details>
  <summary>Details</summary>
Motivation: 深度估计对自动驾驶至关重要，雷达-相机融合能在恶劣条件下提供互补的几何线索，提升鲁棒性。研究旨在开发一个轻量级且准确的深度估计模型。

Method: 本文提出了XD-RCDepth，一个轻量级架构，相较于现有最先进的轻量级基线，参数减少了29.7%。为在模型压缩下保持性能并增强可解释性，引入了两种知识蒸馏策略：1) 可解释性对齐蒸馏，将教师模型的显著性结构传递给学生模型；2) 深度分布蒸馏，将深度回归重构为离散区间的软分类。

Result: XD-RCDepth在参数减少29.7%的情况下，保持了与最先进轻量级基线相当的精度。结合知识蒸馏策略，相较于直接训练，平均绝对误差（MAE）降低了7.97%。在nuScenes和ZJU-4DRadarCam数据集上，实现了具有实时效率的竞争性精度。

Conclusion: XD-RCDepth提供了一个高效且准确的雷达-相机深度估计解决方案。其轻量级架构和创新的知识蒸馏方法，使其在自动驾驶等应用中具有实时性能和良好的可解释性。

Abstract: Depth estimation remains central to autonomous driving, and radar-camera
fusion offers robustness in adverse conditions by providing complementary
geometric cues. In this paper, we present XD-RCDepth, a lightweight
architecture that reduces the parameters by 29.7% relative to the
state-of-the-art lightweight baseline while maintaining comparable accuracy. To
preserve performance under compression and enhance interpretability, we
introduce two knowledge-distillation strategies: an explainability-aligned
distillation that transfers the teacher's saliency structure to the student,
and a depth-distribution distillation that recasts depth regression as soft
classification over discretized bins. Together, these components reduce the MAE
compared with direct training with 7.97% and deliver competitive accuracy with
real-time efficiency on nuScenes and ZJU-4DRadarCam datasets.

</details>


### [99] [AVAR-Net: A Lightweight Audio-Visual Anomaly Recognition Framework with a Benchmark Dataset](https://arxiv.org/abs/2510.13630)
*Amjid Ali,Zulfiqar Ahmad Khan,Altaf Hussain,Muhammad Munsif,Adnan Hussain,Sung Wook Baik*

Main category: cs.CV

TL;DR: 本研究提出了AVAR-Net，一个轻量高效的音视频异常识别框架，并引入了VAAR数据集，以解决现有方法仅依赖视觉数据和缺乏大规模多模态数据集的问题。AVAR-Net在VAAR和XD-Violence数据集上均表现出色，超越了现有技术。


<details>
  <summary>Details</summary>
Motivation: 大多数现有异常识别方法仅依赖视觉数据，在遮挡、低光照和恶劣天气等挑战性条件下不可靠。此外，缺乏大规模同步音视频数据集阻碍了多模态异常识别的进展。

Method: AVAR-Net包含四个主要模块：音频特征提取器（使用Wav2Vec2）、视频特征提取器（使用MobileViT）、早期融合策略以及用于建模跨模态关系的序列模式学习网络（使用多阶段时间卷积网络MTCN）。同时，研究引入了一个包含3,000个真实世界视频、具有同步音频和十种异常类别的中等规模VAAR数据集。

Result: 实验评估表明，AVAR-Net在VAAR数据集上实现了89.29%的准确率，在XD-Violence数据集上实现了88.56%的平均精度，比现有最先进方法提高了2.8%。

Conclusion: 这些结果突出了所提出的AVAR-Net框架的有效性、效率和泛化能力，以及VAAR数据集作为推动多模态异常识别研究基准的实用性。

Abstract: Anomaly recognition plays a vital role in surveillance, transportation,
healthcare, and public safety. However, most existing approaches rely solely on
visual data, making them unreliable under challenging conditions such as
occlusion, low illumination, and adverse weather. Moreover, the absence of
large-scale synchronized audio-visual datasets has hindered progress in
multimodal anomaly recognition. To address these limitations, this study
presents AVAR-Net, a lightweight and efficient audio-visual anomaly recognition
framework designed for real-world environments. AVAR-Net consists of four main
modules: an audio feature extractor, a video feature extractor, fusion
strategy, and a sequential pattern learning network that models cross-modal
relationships for anomaly recognition. Specifically, the Wav2Vec2 model
extracts robust temporal features from raw audio, while MobileViT captures both
local and global visual representations from video frames. An early fusion
mechanism combines these modalities, and a Multi-Stage Temporal Convolutional
Network (MTCN) model that learns long-range temporal dependencies within the
fused representation, enabling robust spatiotemporal reasoning. A novel
Visual-Audio Anomaly Recognition (VAAR) dataset, is also introduced, serving as
a medium-scale benchmark containing 3,000 real-world videos with synchronized
audio across ten diverse anomaly classes. Experimental evaluations demonstrate
that AVAR-Net achieves 89.29% accuracy on VAAR and 88.56% Average Precision on
the XD-Violence dataset, improving Average Precision by 2.8% over existing
state-of-the-art methods. These results highlight the effectiveness,
efficiency, and generalization capability of the proposed framework, as well as
the utility of VAAR as a benchmark for advancing multimodal anomaly recognition
research.

</details>


### [100] [Challenges, Advances, and Evaluation Metrics in Medical Image Enhancement: A Systematic Literature Review](https://arxiv.org/abs/2510.13638)
*Chun Wai Chin,Haniza Yazid,Hoi Leong Lee*

Main category: cs.CV

TL;DR: 该系统综述调查了医学图像增强领域的关键挑战、最新进展和评估指标，分析了39项研究，并指出了现有局限性、研究空白和未来方向。


<details>
  <summary>Details</summary>
Motivation: 医学图像常面临噪声、伪影和对比度低等问题，限制了其诊断潜力。图像增强对于提高诊断图像的质量和可解释性至关重要，以支持早期发现、准确诊断和有效治疗计划。

Method: 采用PRISMA方法进行系统文献综述，分析了39项同行评审研究，以探讨医学图像增强的关键挑战、最新进展和评估指标。

Result: 主要挑战是低对比度和噪声。MRI和多模态成像受到最多关注，而组织病理学、内窥镜和骨闪烁成像等专业模态则未得到充分探索。在所分析的研究中，29项使用传统数学方法，9项使用深度学习技术，1项采用混合方法。在图像质量评估方面，18项研究同时使用参考和非参考指标，9项仅使用参考指标，12项仅使用非参考指标，共引入了65个图像质量评估指标，其中非参考指标占主导。

Conclusion: 本综述强调了当前医学图像增强的局限性、研究空白和潜在的未来发展方向，以期推动该领域的进步。

Abstract: Medical image enhancement is crucial for improving the quality and
interpretability of diagnostic images, ultimately supporting early detection,
accurate diagnosis, and effective treatment planning. Despite advancements in
imaging technologies such as X-ray, CT, MRI, and ultrasound, medical images
often suffer from challenges like noise, artifacts, and low contrast, which
limit their diagnostic potential. Addressing these challenges requires robust
preprocessing, denoising algorithms, and advanced enhancement methods, with
deep learning techniques playing an increasingly significant role. This
systematic literature review, following the PRISMA approach, investigates the
key challenges, recent advancements, and evaluation metrics in medical image
enhancement. By analyzing findings from 39 peer-reviewed studies, this review
provides insights into the effectiveness of various enhancement methods across
different imaging modalities and the importance of evaluation metrics in
assessing their impact. Key issues like low contrast and noise are identified
as the most frequent, with MRI and multi-modal imaging receiving the most
attention, while specialized modalities such as histopathology, endoscopy, and
bone scintigraphy remain underexplored. Out of the 39 studies, 29 utilize
conventional mathematical methods, 9 focus on deep learning techniques, and 1
explores a hybrid approach. In terms of image quality assessment, 18 studies
employ both reference-based and non-reference-based metrics, 9 rely solely on
reference-based metrics, and 12 use only non-reference-based metrics, with a
total of 65 IQA metrics introduced, predominantly non-reference-based. This
review highlights current limitations, research gaps, and potential future
directions for advancing medical image enhancement.

</details>


### [101] [Local-Global Context-Aware and Structure-Preserving Image Super-Resolution](https://arxiv.org/abs/2510.13649)
*Sanchar Palit,Subhasis Chaudhuri,Biplab Banerjee*

Main category: cs.CV

TL;DR: 本文提出了一种上下文精确的图像超分辨率框架，通过局部-全局上下文感知注意力机制和像素空间中的分布与感知对齐条件机制，解决了现有扩散模型在处理多样化和高度降级图像时出现的噪声放大和内容错误生成问题，从而生成高质量、结构一致且感知准确的图像。


<details>
  <summary>Details</summary>
Motivation: 尽管预训练的文本到图像扩散模型在图像超分辨率任务中表现出色，但现有方法在应用于多样化和高度降级的图像时，常出现噪声放大或内容生成不准确的问题。

Method: 1. 提出了局部-全局上下文感知注意力机制，有效维护局部和全局像素关系，以生成高质量图像。2. 引入了像素空间中的分布与感知对齐条件机制，捕捉细粒度像素级表示，并逐步保留和完善结构信息，从局部内容细节过渡到全局结构构成，从而增强感知保真度。

Result: 在推理过程中，所提出的方法生成了与原始内容结构一致的高质量图像，有效减少了伪影并确保了逼真的细节恢复。在多个超分辨率基准测试上的大量实验证明了该方法在生成高保真、感知准确的重建图像方面的有效性。

Conclusion: 该框架通过上下文精确的策略，成功解决了现有扩散模型在处理复杂图像超分辨率时的局限性，实现了高保真和感知准确的图像重建。

Abstract: Diffusion models have recently achieved significant success in various image
manipulation tasks, including image super-resolution and perceptual quality
enhancement. Pretrained text-to-image models, such as Stable Diffusion, have
exhibited strong capabilities in synthesizing realistic image content, which
makes them particularly attractive for addressing super-resolution tasks. While
some existing approaches leverage these models to achieve state-of-the-art
results, they often struggle when applied to diverse and highly degraded
images, leading to noise amplification or incorrect content generation. To
address these limitations, we propose a contextually precise image
super-resolution framework that effectively maintains both local and global
pixel relationships through Local-Global Context-Aware Attention, enabling the
generation of high-quality images. Furthermore, we propose a distribution- and
perceptual-aligned conditioning mechanism in the pixel space to enhance
perceptual fidelity. This mechanism captures fine-grained pixel-level
representations while progressively preserving and refining structural
information, transitioning from local content details to the global structural
composition. During inference, our method generates high-quality images that
are structurally consistent with the original content, mitigating artifacts and
ensuring realistic detail restoration. Extensive experiments on multiple
super-resolution benchmarks demonstrate the effectiveness of our approach in
producing high-fidelity, perceptually accurate reconstructions.

</details>


### [102] [EditCast3D: Single-Frame-Guided 3D Editing with Video Propagation and View Selection](https://arxiv.org/abs/2510.13652)
*Huaizhi Qu,Ruichen Zhang,Shuqing Luo,Luchao Qi,Zhihao Zhang,Xiaoming Liu,Roni Sengupta,Tianlong Chen*

Main category: cs.CV

TL;DR: EditCast3D提出了一种利用视频生成基础模型进行3D编辑的流水线，通过在重建前传播编辑和采用视图选择策略，解决了传统方法中计算成本高和多视图一致性差的问题，实现了高质量和高效率的3D编辑。


<details>
  <summary>Details</summary>
Motivation: 尽管基础模型在图像编辑方面取得了显著进展，但其在3D编辑中的应用仍未充分探索。将这些模型直接集成到现有3D编辑工作流中面临计算需求高、闭源API限制和成本高昂等挑战，导致迭代编辑策略不切实际。

Method: EditCast3D通过以下方式解决问题：1) 在3D重建之前，利用视频生成基础模型将单帧编辑传播到整个数据集。2) 引入视图选择策略，明确识别一致且有利于重建的视图。3) 采用前馈重建，避免了昂贵的精修过程。这种组合方法最大限度地减少了对昂贵图像编辑的依赖，并缓解了独立应用于图像时可能出现的提示歧义。

Result: EditCast3D在常用的3D编辑数据集上进行了评估，并与最先进的3D编辑基线进行了比较。结果表明，EditCast3D在编辑质量上表现优越，并具有高效率。

Conclusion: EditCast3D被确立为一种可扩展且通用的范式，用于将基础模型集成到3D编辑流水线中，解决了现有方法的局限性，实现了高质量和高效率的3D编辑。

Abstract: Recent advances in foundation models have driven remarkable progress in image
editing, yet their extension to 3D editing remains underexplored. A natural
approach is to replace the image editing modules in existing workflows with
foundation models. However, their heavy computational demands and the
restrictions and costs of closed-source APIs make plugging these models into
existing iterative editing strategies impractical. To address this limitation,
we propose EditCast3D, a pipeline that employs video generation foundation
models to propagate edits from a single first frame across the entire dataset
prior to reconstruction. While editing propagation enables dataset-level
editing via video models, its consistency remains suboptimal for 3D
reconstruction, where multi-view alignment is essential. To overcome this,
EditCast3D introduces a view selection strategy that explicitly identifies
consistent and reconstruction-friendly views and adopts feedforward
reconstruction without requiring costly refinement. In combination, the
pipeline both minimizes reliance on expensive image editing and mitigates
prompt ambiguities that arise when applying foundation models independently
across images. We evaluate EditCast3D on commonly used 3D editing datasets and
compare it against state-of-the-art 3D editing baselines, demonstrating
superior editing quality and high efficiency. These results establish
EditCast3D as a scalable and general paradigm for integrating foundation models
into 3D editing pipelines. The code is available at
https://github.com/UNITES-Lab/EditCast3D

</details>


### [103] [NTIRE 2025 Challenge on Low Light Image Enhancement: Methods and Results](https://arxiv.org/abs/2510.13670)
*Xiaoning Liu,Zongwei Wu,Florin-Alexandru Vasluianu,Hailong Yan,Bin Ren,Yulun Zhang,Shuhang Gu,Le Zhang,Ce Zhu,Radu Timofte,Kangbiao Shi,Yixu Feng,Tao Hu,Yu Cao,Peng Wu,Yijin Liang,Yanning Zhang,Qingsen Yan,Han Zhou,Wei Dong,Yan Min,Mohab Kishawy,Jun Chen,Pengpeng Yu,Anjin Park,Seung-Soo Lee,Young-Joon Park,Zixiao Hu,Junyv Liu,Huilin Zhang,Jun Zhang,Fei Wan,Bingxin Xu,Hongzhe Liu,Cheng Xu,Weiguo Pan,Songyin Dai,Xunpeng Yi,Qinglong Yan,Yibing Zhang,Jiayi Ma,Changhui Hu,Kerui Hu,Donghang Jing,Tiesheng Chen,Zhi Jin,Hongjun Wu,Biao Huang,Haitao Ling,Jiahao Wu,Dandan Zhan,G Gyaneshwar Rao,Vijayalaxmi Ashok Aralikatti,Nikhil Akalwadi,Ramesh Ashok Tabib,Uma Mudenagudi,Ruirui Lin,Guoxi Huang,Nantheera Anantrasirichai,Qirui Yang,Alexandru Brateanu,Ciprian Orhei,Cosmin Ancuti,Daniel Feijoo,Juan C. Benito,Álvaro García,Marcos V. Conde,Yang Qin,Raul Balmez,Anas M. Ali,Bilel Benjdira,Wadii Boulila,Tianyi Mao,Huan Zheng,Yanyan Wei,Shengeng Tang,Dan Guo,Zhao Zhang,Sabari Nathan,K Uma,A Sasithradevi,B Sathya Bama,S. Mohamed Mansoor Roomi,Ao Li,Xiangtao Zhang,Zhe Liu,Yijie Tang,Jialong Tang,Zhicheng Fu,Gong Chen,Joe Nasti,John Nicholson,Zeyu Xiao,Zhuoyuan Li,Ashutosh Kulkarni,Prashant W. Patil,Santosh Kumar Vipparthi,Subrahmanyam Murala,Duan Liu,Weile Li,Hangyuan Lu,Rixian Liu,Tengfeng Wang,Jinxing Liang,Chenxin Yu*

Main category: cs.CV

TL;DR: 本文综述了NTIRE 2025低光照图像增强（LLIE）挑战赛，重点介绍了参赛方案和最终成果，展示了该领域的重要进展。


<details>
  <summary>Details</summary>
Motivation: 挑战赛旨在发现能在各种复杂条件下生成更明亮、清晰、视觉吸引力强图像的有效网络。本文的动机是回顾和评估这些顶尖进展。

Method: 通过对NTIRE 2025 LLIE挑战赛中提出的解决方案和最终结果进行全面审查和评估。

Result: 挑战赛吸引了762名注册者，最终有28支队伍提交了有效参赛作品。本文展示了LLIE领域最先进技术的显著进步。

Conclusion: NTIRE 2025 LLIE挑战赛成功展示了低光照图像增强领域的最新技术突破和重要进展。

Abstract: This paper presents a comprehensive review of the NTIRE 2025 Low-Light Image
Enhancement (LLIE) Challenge, highlighting the proposed solutions and final
outcomes. The objective of the challenge is to identify effective networks
capable of producing brighter, clearer, and visually compelling images under
diverse and challenging conditions. A remarkable total of 762 participants
registered for the competition, with 28 teams ultimately submitting valid
entries. This paper thoroughly evaluates the state-of-the-art advancements in
LLIE, showcasing the significant progress.

</details>


### [104] [Towards Adversarial Robustness and Uncertainty Quantification in DINOv2-based Few-Shot Anomaly Detection](https://arxiv.org/abs/2510.13643)
*Akib Mohammed Khan,Bartosz Krawczyk*

Main category: cs.CV

TL;DR: 本研究系统性地探讨了基于DINOv2的少样本异常检测器在对抗性攻击下的脆弱性及其不确定性校准问题，发现其易受攻击且校准不佳。通过引入Platt缩放，可以提高不确定性校准并实现攻击检测。


<details>
  <summary>Details</summary>
Motivation: DINOv2等基础模型在少样本异常检测中表现出色，但其对抗性鲁棒性以及异常分数的不确定性校准尚未被系统性研究。

Method: 基于AnomalyDINO（一种在DINOv2特征上运行的无训练深度最近邻检测器），本研究通过在冻结的DINOv2特征上附加一个轻量级线性头部（仅用于生成扰动），以实现白盒梯度攻击。使用FGSM攻击在MVTec-AD和VisA数据集上评估了检测器的性能下降。同时，通过后验Platt缩放方法对异常分数进行校准，以改善不确定性估计。

Result: FGSM攻击导致F1、AUROC、AP和G-mean等指标持续下降，表明微小扰动足以在特征空间中翻转最近邻关系并导致自信的错误分类。原始异常分数校准不良，显示出置信度与正确性之间的差距。经过Platt缩放校准后，对抗性扰动输入的预测熵显著高于干净输入，这为攻击检测提供了一种实用的标记机制，并降低了校准误差（ECE）。

Conclusion: 基于DINOv2的少样本异常检测器存在具体的脆弱性，本研究建立了一个评估协议和基线，以实现鲁棒且具有不确定性感知能力的异常检测。对抗性鲁棒性和原则性的不确定性量化对于异常检测系统在实际部署中获得信任至关重要。

Abstract: Foundation models such as DINOv2 have shown strong performance in few-shot
anomaly detection, yet two key questions remain unexamined: (i) how susceptible
are these detectors to adversarial perturbations; and (ii) how well do their
anomaly scores reflect calibrated uncertainty? Building on AnomalyDINO, a
training-free deep nearest-neighbor detector over DINOv2 features, we present
one of the first systematic studies of adversarial attacks and uncertainty
estimation in this setting. To enable white-box gradient attacks while
preserving test-time behavior, we attach a lightweight linear head to frozen
DINOv2 features only for crafting perturbations. Using this heuristic, we
evaluate the impact of FGSM across the MVTec-AD and VisA datasets and observe
consistent drops in F1, AUROC, AP, and G-mean, indicating that imperceptible
perturbations can flip nearest-neighbor relations in feature space to induce
confident misclassification. Complementing robustness, we probe reliability and
find that raw anomaly scores are poorly calibrated, revealing a gap between
confidence and correctness that limits safety-critical use. As a simple, strong
baseline toward trustworthiness, we apply post-hoc Platt scaling to the anomaly
scores for uncertainty estimation. The resulting calibrated posteriors yield
significantly higher predictive entropy on adversarially perturbed inputs than
on clean ones, enabling a practical flagging mechanism for attack detection
while reducing calibration error (ECE). Our findings surface concrete
vulnerabilities in DINOv2-based few-shot anomaly detectors and establish an
evaluation protocol and baseline for robust, uncertainty-aware anomaly
detection. We argue that adversarial robustness and principled uncertainty
quantification are not optional add-ons but essential capabilities if anomaly
detection systems are to be trustworthy and ready for real-world deployment.

</details>


### [105] [Generating healthy counterfactuals with denoising diffusion bridge models](https://arxiv.org/abs/2510.13684)
*Ana Lawry Aguila,Peirong Liu,Marina Crespo Aguirre,Juan Eugenio Iglesias*

Main category: cs.CV

TL;DR: 本文提出了一种基于去噪扩散桥模型（DDBM）的新方法，用于从病理图像生成健康的对应反事实图像。该方法通过将病理图像作为结构信息先验，有效平衡了异常移除和个体特征保留，并在分割和异常检测任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在医学影像中，从病理图像生成健康的对应反事实图像具有重要应用前景，但现有方法（如基于DDPM）难以有效平衡异常区域的去除与个体解剖特征的保留。传统的DDPMs通常只在健康数据上训练，或结合合成病理图像，但仍面临指导生成过程的挑战。

Method: 本文提出将去噪扩散桥模型（DDBMs）应用于生成健康反事实图像。与传统的去噪扩散概率模型（DDPMs）不同，DDBMs不仅以初始点（健康图像）为条件，还以最终点（对应的合成病理图像）为条件来指导扩散过程。通过将病理图像视为结构信息先验，该方法能够在选择性去除病理的同时，使生成的反事实图像与患者解剖结构紧密匹配。

Result: 实验结果表明，本文提出的DDBM在分割和异常检测任务中，其性能优于先前提出的扩散模型和全监督方法。

Conclusion: DDBM通过利用病理图像作为结构先验，能够有效地生成健康的对应反事实图像，解决了平衡异常移除和个体特征保留的难题，并在相关医学影像分析任务中取得了显著的性能提升。

Abstract: Generating healthy counterfactuals from pathological images holds significant
promise in medical imaging, e.g., in anomaly detection or for application of
analysis tools that are designed for healthy scans. These counterfactuals
should represent what a patient's scan would plausibly look like in the absence
of pathology, preserving individual anatomical characteristics while modifying
only the pathological regions. Denoising diffusion probabilistic models (DDPMs)
have become popular methods for generating healthy counterfactuals of pathology
data. Typically, this involves training on solely healthy data with the
assumption that a partial denoising process will be unable to model disease
regions and will instead reconstruct a closely matched healthy counterpart.
More recent methods have incorporated synthetic pathological images to better
guide the diffusion process. However, it remains challenging to guide the
generative process in a way that effectively balances the removal of anomalies
with the retention of subject-specific features. To solve this problem, we
propose a novel application of denoising diffusion bridge models (DDBMs) -
which, unlike DDPMs, condition the diffusion process not only on the initial
point (i.e., the healthy image), but also on the final point (i.e., a
corresponding synthetically generated pathological image). Treating the
pathological image as a structurally informative prior enables us to generate
counterfactuals that closely match the patient's anatomy while selectively
removing pathology. The results show that our DDBM outperforms previously
proposed diffusion models and fully supervised approaches at segmentation and
anomaly detection tasks.

</details>


### [106] [Circle of Willis Centerline Graphs: A Dataset and Baseline Algorithm](https://arxiv.org/abs/2510.13720)
*Fabio Musio,Norman Juchler,Kaiyuan Yang,Suprosanna Shit,Chinmay Prabhakar,Bjoern Menze,Sven Hirsch*

Main category: cs.CV

TL;DR: 该研究开发了一种结合U-Net骨架化和A*图连接的基线算法，用于从脑部Willis环（CoW）中提取准确且稳健的血管中心线和形态特征，并发布了数据集和算法。


<details>
  <summary>Details</summary>
Motivation: Willis环的自动化评估需要中心线表示，但由于其复杂的几何形状，传统骨架化技术难以提取可靠的中心线，且公开可用的中心线数据集稀缺。

Method: 研究首先使用基于细化的骨架化算法，从TopCoW数据集（包含200名中风患者的MRA和CTA图像）中提取并整理了中心线图和形态特征。随后，利用整理后的图开发了一个基线算法，该算法结合了基于U-Net的骨架化和A*图连接，用于中心线和特征提取。性能通过在保留测试集上评估解剖准确性和特征鲁棒性进行验证。

Result: 该基线算法能够高精度地重建图拓扑结构（F1 = 1），参考图和预测图之间的平均欧氏节点距离小于一个体素。节段半径、长度和分叉比等特征显示出强大的鲁棒性，中位相对误差低于5%，皮尔逊相关系数高于0.95。此外，提取的特征被用于预测胎儿大脑后动脉（PCA）变异的频率，证实理论分叉最优关系，并检测细微的模态差异。

Conclusion: 研究结果表明，结合图连接的学习型骨架化方法对于解剖学上合理的中心线提取非常有用。强调了超越简单的基于体素的测量，评估解剖准确性和特征鲁棒性的重要性。数据集和基线算法已发布，以支持进一步的方法开发和临床研究。

Abstract: The Circle of Willis (CoW) is a critical network of arteries in the brain,
often implicated in cerebrovascular pathologies. Voxel-level segmentation is an
important first step toward an automated CoW assessment, but a full
quantitative analysis requires centerline representations. However,
conventional skeletonization techniques often struggle to extract reliable
centerlines due to the CoW's complex geometry, and publicly available
centerline datasets remain scarce. To address these challenges, we used a
thinning-based skeletonization algorithm to extract and curate centerline
graphs and morphometric features from the TopCoW dataset, which includes 200
stroke patients, each imaged with MRA and CTA. The curated graphs were used to
develop a baseline algorithm for centerline and feature extraction, combining
U-Net-based skeletonization with A* graph connection. Performance was evaluated
on a held-out test set, focusing on anatomical accuracy and feature robustness.
Further, we used the extracted features to predict the frequency of fetal PCA
variants, confirm theoretical bifurcation optimality relations, and detect
subtle modality differences. The baseline algorithm consistently reconstructed
graph topology with high accuracy (F1 = 1), and the average Euclidean node
distance between reference and predicted graphs was below one voxel. Features
such as segment radius, length, and bifurcation ratios showed strong
robustness, with median relative errors below 5% and Pearson correlations above
0.95. Our results demonstrate the utility of learning-based skeletonization
combined with graph connection for anatomically plausible centerline
extraction. We emphasize the importance of going beyond simple voxel-based
measures by evaluating anatomical accuracy and feature robustness. The dataset
and baseline algorithm have been released to support further method development
and clinical research.

</details>


### [107] [Risk-adaptive Activation Steering for Safe Multimodal Large Language Models](https://arxiv.org/abs/2510.13698)
*Jonghyun Park,Minhyuk Seo,Jonghyun Choi*

Main category: cs.CV

TL;DR: 本文提出了一种名为风险自适应激活转向（RAS）的新方法，通过重新构造查询和自适应地引导激活来增强多模态AI模型的安全性，同时提高推理速度并保持性能。


<details>
  <summary>Details</summary>
Motivation: 现代AI模型面临多模态恶意查询的挑战，需要在提供有用响应的同时拒绝恶意内容。现有安全对齐方法（如大规模数据集训练）成本高昂，而推理时对齐则存在过度拒绝良性查询和推理速度慢的问题。

Method: 本文提出风险自适应激活转向（RAS）。该方法通过重新构造查询，以加强对安全关键图像区域的跨模态关注，从而实现查询级别的准确风险评估。根据评估的风险，它自适应地引导激活，以生成安全且有用的响应，避免了迭代输出调整带来的开销。

Result: 在多个多模态安全性和实用性基准测试中，RAS显著降低了攻击成功率，保持了通用任务性能，并相对于先前的推理时防御方法提高了推理速度。

Conclusion: RAS通过解决现有推理时对齐方法的局限性，提供了一种有效且高效的多模态AI安全对齐方案，能够准确评估风险并自适应地生成安全有益的响应，同时提升推理速度和保持性能。

Abstract: One of the key challenges of modern AI models is ensuring that they provide
helpful responses to benign queries while refusing malicious ones. But often,
the models are vulnerable to multimodal queries with harmful intent embedded in
images. One approach for safety alignment is training with extensive safety
datasets at the significant costs in both dataset curation and training.
Inference-time alignment mitigates these costs, but introduces two drawbacks:
excessive refusals from misclassified benign queries and slower inference speed
due to iterative output adjustments. To overcome these limitations, we propose
to reformulate queries to strengthen cross-modal attention to safety-critical
image regions, enabling accurate risk assessment at the query level. Using the
assessed risk, it adaptively steers activations to generate responses that are
safe and helpful without overhead from iterative output adjustments. We call
this Risk-adaptive Activation Steering (RAS). Extensive experiments across
multiple benchmarks on multimodal safety and utility demonstrate that the RAS
significantly reduces attack success rates, preserves general task performance,
and improves inference speed over prior inference-time defenses.

</details>


### [108] [OmniGaze: Reward-inspired Generalizable Gaze Estimation In The Wild](https://arxiv.org/abs/2510.13660)
*Hongyu Qu,Jianan Wei,Xiangbo Shu,Yazhou Yao,Wenguan Wang,Jinhui Tang*

Main category: cs.CV

TL;DR: OmniGaze是一个半监督框架，通过利用大规模多样化的未标注数据和奖励模型来评估伪标签的可靠性，显著提高了3D凝视估计在野外环境下的泛化能力，并实现了最先进的性能和强大的零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 当前的3D凝视估计方法在不同数据域之间泛化能力差，主要原因在于：1）标注数据集稀缺；2）标注数据多样性不足。

Method: OmniGaze框架首先收集了大量多样化的未标注人脸图像。它采用标准的伪标签策略，并设计了一个奖励模型来评估伪标签的可靠性。该奖励模型不仅使用3D方向向量作为伪标签，还整合了由现成视觉编码器提取的视觉嵌入以及通过提示多模态大语言模型生成的凝视视角的语义线索来计算置信度分数。这些分数随后用于选择高质量的伪标签并对其进行加权以计算损失。

Result: OmniGaze在五种数据集的域内和跨域设置下均实现了最先进的性能。此外，它在四个未见数据集上展现出强大的零样本泛化能力，证明了其作为凝视估计可扩展数据引擎的有效性。

Conclusion: OmniGaze通过利用大规模多样化未标注数据和创新的伪标签可靠性评估机制，成功缓解了领域偏差，并在野外环境中实现了3D凝视估计的泛化，为凝视估计提供了一个可扩展且高性能的解决方案。

Abstract: Current 3D gaze estimation methods struggle to generalize across diverse data
domains, primarily due to i) the scarcity of annotated datasets, and ii) the
insufficient diversity of labeled data. In this work, we present OmniGaze, a
semi-supervised framework for 3D gaze estimation, which utilizes large-scale
unlabeled data collected from diverse and unconstrained real-world environments
to mitigate domain bias and generalize gaze estimation in the wild. First, we
build a diverse collection of unlabeled facial images, varying in facial
appearances, background environments, illumination conditions, head poses, and
eye occlusions. In order to leverage unlabeled data spanning a broader
distribution, OmniGaze adopts a standard pseudo-labeling strategy and devises a
reward model to assess the reliability of pseudo labels. Beyond pseudo labels
as 3D direction vectors, the reward model also incorporates visual embeddings
extracted by an off-the-shelf visual encoder and semantic cues from gaze
perspective generated by prompting a Multimodal Large Language Model to compute
confidence scores. Then, these scores are utilized to select high-quality
pseudo labels and weight them for loss computation. Extensive experiments
demonstrate that OmniGaze achieves state-of-the-art performance on five
datasets under both in-domain and cross-domain settings. Furthermore, we also
evaluate the efficacy of OmniGaze as a scalable data engine for gaze
estimation, which exhibits robust zero-shot generalization on four unseen
datasets.

</details>


### [109] [LiFMCR: Dataset and Benchmark for Light Field Multi-Camera Registration](https://arxiv.org/abs/2510.13729)
*Aymeric Fleith,Julian Zirbel,Daniel Cremers,Niclas Zeller*

Main category: cs.CV

TL;DR: 本文提出了LiFMCR数据集，用于多微透镜阵列（MLA）光场相机的配准，并提供了两个基于光场相机模型的基线配准方法，实验证明其与真实值高度一致。


<details>
  <summary>Details</summary>
Motivation: 现有的光场数据集仅限于单相机设置，且通常缺乏外部真实值，这限制了对多相机光场配准方法的严格评估。

Method: 本文创建了LiFMCR数据集，包含两台Raytrix R32光场相机同步图像序列和Vicon运动捕捉系统记录的高精度6自由度姿态。同时，提供了两种互补的基线配准方法：1. 基于RANSAC的3D变换估计，利用跨视角点云；2. 从单光场图像估计外参6自由度姿态的光场PnP算法。两种方法均显式集成了光场相机模型。

Result: 实验结果显示，所提出的配准方法与地面真实值具有很强的一致性，支持可靠的多视角光场处理。

Conclusion: LiFMCR数据集及其提供的基线方法，通过结合高精度真实值和光场相机模型，为多相机光场配准方法的严格评估和可靠的多视角光场处理提供了独特的资源和解决方案。

Abstract: We present LiFMCR, a novel dataset for the registration of multiple micro
lens array (MLA)-based light field cameras. While existing light field datasets
are limited to single-camera setups and typically lack external ground truth,
LiFMCR provides synchronized image sequences from two high-resolution Raytrix
R32 plenoptic cameras, together with high-precision 6-degrees of freedom (DoF)
poses recorded by a Vicon motion capture system. This unique combination
enables rigorous evaluation of multi-camera light field registration methods.
  As a baseline, we provide two complementary registration approaches: a robust
3D transformation estimation via a RANSAC-based method using cross-view point
clouds, and a plenoptic PnP algorithm estimating extrinsic 6-DoF poses from
single light field images. Both explicitly integrate the plenoptic camera
model, enabling accurate and scalable multi-camera registration. Experiments
show strong alignment with the ground truth, supporting reliable multi-view
light field processing.
  Project page: https://lifmcr.github.io/

</details>


### [110] [FlashWorld: High-quality 3D Scene Generation within Seconds](https://arxiv.org/abs/2510.13678)
*Xinyang Li,Tengfei Wang,Zixiao Gu,Shengchuan Zhang,Chunchao Guo,Liujuan Cao*

Main category: cs.CV

TL;DR: FlashWorld是一种生成式模型，能从单张图片或文本提示在几秒内生成3D场景，速度比现有方法快10-100倍，同时拥有卓越的渲染质量。它通过结合多视角和3D导向两种范式的优势实现。


<details>
  <summary>Details</summary>
Motivation: 现有3D场景生成方法速度慢（10-100倍慢），且传统的多视角导向方法需要先生成多视角图像再进行3D重建，而直接的3D导向方法虽然保持3D一致性但视觉质量较差。研究动机是开发一种更快、质量更好且保持3D一致性的生成模型。

Method: FlashWorld采用3D导向方法，在多视角生成过程中直接产生3D高斯表示。它包含两个主要阶段：1) 双模态预训练：利用视频扩散模型的先验知识，预训练一个同时支持多视角导向和3D导向生成的扩散模型。2) 跨模态后训练蒸馏：通过将3D导向模式的分布与高质量多视角导向模式的分布进行匹配，弥补3D导向生成中的质量差距，并减少推理所需的去噪步骤。此外，还提出了一种利用大量单视角图像和文本提示来增强模型泛化能力的策略。

Result: FlashWorld在几秒内生成3D场景，比现有方法快10-100倍，同时拥有卓越的渲染质量和3D一致性。该方法还增强了模型对分布外输入的泛化能力。广泛的实验证明了其优越性和效率。

Conclusion: FlashWorld通过创新的双模态预训练和跨模态后训练策略，有效整合了多视角导向和3D导向两种范式的优势，实现了3D场景生成在效率和质量上的显著提升。

Abstract: We propose FlashWorld, a generative model that produces 3D scenes from a
single image or text prompt in seconds, 10~100$\times$ faster than previous
works while possessing superior rendering quality. Our approach shifts from the
conventional multi-view-oriented (MV-oriented) paradigm, which generates
multi-view images for subsequent 3D reconstruction, to a 3D-oriented approach
where the model directly produces 3D Gaussian representations during multi-view
generation. While ensuring 3D consistency, 3D-oriented method typically suffers
poor visual quality. FlashWorld includes a dual-mode pre-training phase
followed by a cross-mode post-training phase, effectively integrating the
strengths of both paradigms. Specifically, leveraging the prior from a video
diffusion model, we first pre-train a dual-mode multi-view diffusion model,
which jointly supports MV-oriented and 3D-oriented generation modes. To bridge
the quality gap in 3D-oriented generation, we further propose a cross-mode
post-training distillation by matching distribution from consistent 3D-oriented
mode to high-quality MV-oriented mode. This not only enhances visual quality
while maintaining 3D consistency, but also reduces the required denoising steps
for inference. Also, we propose a strategy to leverage massive single-view
images and text prompts during this process to enhance the model's
generalization to out-of-distribution inputs. Extensive experiments demonstrate
the superiority and efficiency of our method.

</details>


### [111] [Cyclic Self-Supervised Diffusion for Ultra Low-field to High-field MRI Synthesis](https://arxiv.org/abs/2510.13735)
*Zhenxuan Zhang,Peiyuan Jing,Zi Wang,Ula Briski,Coraline Beitone,Yue Yang,Yinzhe Wu,Fanwen Wang,Liutao Yang,Jiahao Huang,Zhifan Gao,Zhaolin Chen,Kh Tohidul Islam,Guang Yang,Peter J. Lally*

Main category: cs.CV

TL;DR: 本文提出了一种名为CSS-Diff的循环自监督扩散框架，用于从低场MRI数据合成高质量高场MRI图像，有效解决了现有方法在临床保真度、解剖结构保留和细节增强方面的不足。


<details>
  <summary>Details</summary>
Motivation: 低场MRI虽然成本低、易获取且更安全，但其分辨率低、信噪比差。合成高场MRI可以减少对昂贵采集的依赖并扩大数据可用性。然而，现有的高场MRI合成方法在临床保真度上存在差距，难以有效保留解剖结构、增强精细细节并弥合图像对比度中的域差距。

Method: 本文提出了一个名为“循环自监督扩散 (CSS-Diff)”的框架。其核心思想是将基于扩散的合成过程重新表述为循环一致性约束，以在整个生成过程中强制执行解剖结构保留，而非仅依赖成对的像素级监督。CSS-Diff框架还引入了两个新颖的模块：1) 切片间隙感知网络，通过对比学习对齐切片间的不一致性；2) 局部结构校正网络，通过对遮蔽和扰动补丁的自重建来增强局部特征恢复。

Result: 在跨场合成任务上的广泛实验表明，该方法实现了最先进的性能（例如，PSNR为31.80 ± 2.70 dB，SSIM为0.943 ± 0.102，LPIPS为0.0864 ± 0.0689）。除了像素级保真度，该方法与原始低场MRI相比，还能更好地保留精细的解剖结构（例如，左大脑白质误差从12.1%降至2.1%，皮层从4.2%降至3.7%）。

Conclusion: CSS-Diff框架能够合成既在定量上可靠又在解剖上一致的图像。

Abstract: Synthesizing high-quality images from low-field MRI holds significant
potential. Low-field MRI is cheaper, more accessible, and safer, but suffers
from low resolution and poor signal-to-noise ratio. This synthesis process can
reduce reliance on costly acquisitions and expand data availability. However,
synthesizing high-field MRI still suffers from a clinical fidelity gap. There
is a need to preserve anatomical fidelity, enhance fine-grained structural
details, and bridge domain gaps in image contrast. To address these issues, we
propose a \emph{cyclic self-supervised diffusion (CSS-Diff)} framework for
high-field MRI synthesis from real low-field MRI data. Our core idea is to
reformulate diffusion-based synthesis under a cycle-consistent constraint. It
enforces anatomical preservation throughout the generative process rather than
just relying on paired pixel-level supervision. The CSS-Diff framework further
incorporates two novel processes. The slice-wise gap perception network aligns
inter-slice inconsistencies via contrastive learning. The local structure
correction network enhances local feature restoration through
self-reconstruction of masked and perturbed patches. Extensive experiments on
cross-field synthesis tasks demonstrate the effectiveness of our method,
achieving state-of-the-art performance (e.g., 31.80 $\pm$ 2.70 dB in PSNR,
0.943 $\pm$ 0.102 in SSIM, and 0.0864 $\pm$ 0.0689 in LPIPS). Beyond pixel-wise
fidelity, our method also preserves fine-grained anatomical structures compared
with the original low-field MRI (e.g., left cerebral white matter error drops
from 12.1$\%$ to 2.1$\%$, cortex from 4.2$\%$ to 3.7$\%$). To conclude, our
CSS-Diff can synthesize images that are both quantitatively reliable and
anatomically consistent.

</details>


### [112] [Seeing and Knowing in the Wild: Open-domain Visual Entity Recognition with Large-scale Knowledge Graphs via Contrastive Learning](https://arxiv.org/abs/2510.13675)
*Hongkuan Zhou,Lavdim Halilaj,Sebastian Monka,Stefan Schmid,Yuqicheng Zhu,Jingcheng Wu,Nadeem Nazer,Steffen Staab*

Main category: cs.CV

TL;DR: 本文提出了一种知识引导对比学习（KnowCoL）框架，通过结合图像、文本描述和Wikidata结构化知识，在开放域视觉实体识别任务中实现了零样本识别，尤其显著提升了对稀有和未见实体的识别准确率。


<details>
  <summary>Details</summary>
Motivation: 开放域视觉实体识别面临挑战，包括开放集条件（大多数实体在训练时未见）、长尾分布、有限监督、高视觉歧义以及语义消歧需求，这些都使得该任务比传统固定标签集的分类任务更具挑战性。

Method: 本文提出了一个知识引导对比学习（KnowCoL）框架。该框架将图像和文本描述结合到一个共享的语义空间中，并以Wikidata的结构化信息为基础。通过将视觉和文本输入抽象到概念层面，模型利用实体描述、类型层次结构和关系上下文来支持零样本实体识别。

Result: 在OVEN基准数据集上的实验表明，结合视觉、文本和结构化知识能显著提高准确性，尤其对于稀有和未见实体。即使是最小的模型，与现有最先进技术相比，在未见实体上的准确率提高了10.5%，而模型尺寸缩小了35倍。

Conclusion: 结合视觉、文本和结构化知识（如Wikidata）对于解决开放域视觉实体识别的固有挑战（特别是零样本和长尾实体识别）是极其有效的，能够显著提升性能并实现更高效的模型。

Abstract: Open-domain visual entity recognition aims to identify and link entities
depicted in images to a vast and evolving set of real-world concepts, such as
those found in Wikidata. Unlike conventional classification tasks with fixed
label sets, it operates under open-set conditions, where most target entities
are unseen during training and exhibit long-tail distributions. This makes the
task inherently challenging due to limited supervision, high visual ambiguity,
and the need for semantic disambiguation. In this work, we propose a
Knowledge-guided Contrastive Learning (KnowCoL) framework that combines both
images and text descriptions into a shared semantic space grounded by
structured information from Wikidata. By abstracting visual and textual inputs
to a conceptual level, the model leverages entity descriptions, type
hierarchies, and relational context to support zero-shot entity recognition. We
evaluate our approach on the OVEN benchmark, a large-scale open-domain visual
recognition dataset with Wikidata IDs as the label space. Our experiments show
that using visual, textual, and structured knowledge greatly improves accuracy,
especially for rare and unseen entities. Our smallest model improves the
accuracy on unseen entities by 10.5% compared to the state-of-the-art, despite
being 35 times smaller.

</details>


### [113] [UniCalli: A Unified Diffusion Framework for Column-Level Generation and Recognition of Chinese Calligraphy](https://arxiv.org/abs/2510.13745)
*Tianshuo Xu,Kai Wang,Zhifei Chen,Leyi Wu,Tianshui Wen,Fei Chao,Ying-Cong Chen*

Main category: cs.CV

TL;DR: UniCalli是一个统一的扩散框架，用于实现柱状（column-level）的中文书法识别与生成，解决了现有方法在字符质量、页面美学和书写正确性方面的不足，并能推广到其他古文字。


<details>
  <summary>Details</summary>
Motivation: 计算复制中国书法面临挑战：现有方法要么能生成高质量的独立字符但忽略连笔和间距等页面美学，要么尝试页面合成但牺牲书写正确性。

Method: UniCalli是一个统一的扩散框架，用于柱状识别和生成。通过联合训练识别和生成任务，前者约束生成器保持字符结构，后者提供风格和布局先验。该方法利用不对称噪声和栅格化边界框图作为空间先验，并在一个包含8000多件（其中4000件密集标注）的策展数据集上，结合合成、标注和未标注数据进行训练。

Result: UniCalli实现了最先进的生成质量，具有卓越的连笔连续性和布局保真度，同时识别能力也更强。该框架还成功扩展到甲骨文和埃及象形文字等其他古文字。

Conclusion: UniCalli通过统一识别和生成任务，有效解决了计算复制中国书法的挑战，能够生成高质量且符合书写规范的输出，并且具有良好的泛化能力，适用于多种古文字。

Abstract: Computational replication of Chinese calligraphy remains challenging.
Existing methods falter, either creating high-quality isolated characters while
ignoring page-level aesthetics like ligatures and spacing, or attempting page
synthesis at the expense of calligraphic correctness. We introduce
\textbf{UniCalli}, a unified diffusion framework for column-level recognition
and generation. Training both tasks jointly is deliberate: recognition
constrains the generator to preserve character structure, while generation
provides style and layout priors. This synergy fosters concept-level
abstractions that improve both tasks, especially in limited-data regimes. We
curated a dataset of over 8,000 digitized pieces, with ~4,000 densely
annotated. UniCalli employs asymmetric noising and a rasterized box map for
spatial priors, trained on a mix of synthetic, labeled, and unlabeled data. The
model achieves state-of-the-art generative quality with superior ligature
continuity and layout fidelity, alongside stronger recognition. The framework
successfully extends to other ancient scripts, including Oracle bone
inscriptions and Egyptian hieroglyphs. Code and data can be viewed in
\href{https://github.com/EnVision-Research/UniCalli}{this URL}.

</details>


### [114] [NoisePrints: Distortion-Free Watermarks for Authorship in Private Diffusion Models](https://arxiv.org/abs/2510.13793)
*Nir Goren,Oren Katzir,Abhinav Nakarmi,Eyal Ronen,Mahmood Sharif,Or Patashnik*

Main category: cs.CV

TL;DR: 本文提出NoisePrints，一种轻量级水印方案，利用扩散模型中的随机种子作为内容生成者的所有权证明，无需修改生成过程或访问模型权重，并结合零知识证明保护种子。


<details>
  <summary>Details</summary>
Motivation: 随着扩散模型在视觉内容生成中的广泛应用，证明作者身份和保护版权变得至关重要。现有水印方法需要访问模型权重且计算成本高昂，不切实际且难以扩展，因此需要一种轻量级的第三方验证方案。

Method: 本文提出NoisePrints方案。它利用初始化扩散过程的随机种子作为所有权证明，不修改生成过程。关键在于初始噪声与生成内容高度相关。通过在噪声采样过程中整合哈希函数，确保从内容中恢复有效种子不可行。此外，使用加密零知识证明在不泄露种子的情况下证明所有权，增加水印移除的难度。

Result: 实验证明，从内容中恢复有效种子或采样通过验证的替代种子是不可行的。该方法在各种操作下表现出鲁棒性。NoisePrints在多个最先进的图像和视频扩散模型上进行了验证，仅使用种子和输出即可实现高效验证，无需访问模型权重。

Conclusion: NoisePrints提供了一种轻量级、鲁棒且高效的扩散模型水印方案，解决了作者身份证明和版权保护问题，无需访问模型权重或进行大量计算，并通过保密种子和零知识证明进一步增强了安全性。

Abstract: With the rapid adoption of diffusion models for visual content generation,
proving authorship and protecting copyright have become critical. This
challenge is particularly important when model owners keep their models private
and may be unwilling or unable to handle authorship issues, making third-party
verification essential. A natural solution is to embed watermarks for later
verification. However, existing methods require access to model weights and
rely on computationally heavy procedures, rendering them impractical and
non-scalable. To address these challenges, we propose , a lightweight
watermarking scheme that utilizes the random seed used to initialize the
diffusion process as a proof of authorship without modifying the generation
process. Our key observation is that the initial noise derived from a seed is
highly correlated with the generated visual content. By incorporating a hash
function into the noise sampling process, we further ensure that recovering a
valid seed from the content is infeasible. We also show that sampling an
alternative seed that passes verification is infeasible, and demonstrate the
robustness of our method under various manipulations. Finally, we show how to
use cryptographic zero-knowledge proofs to prove ownership without revealing
the seed. By keeping the seed secret, we increase the difficulty of watermark
removal. In our experiments, we validate NoisePrints on multiple
state-of-the-art diffusion models for images and videos, demonstrating
efficient verification using only the seed and output, without requiring access
to model weights.

</details>


### [115] [Uni-MMMU: A Massive Multi-discipline Multimodal Unified Benchmark](https://arxiv.org/abs/2510.13759)
*Kai Zou,Ziqi Huang,Yuhao Dong,Shulin Tian,Dian Zheng,Hongbo Liu,Jingwen He,Bin Liu,Yu Qiao,Ziwei Liu*

Main category: cs.CV

TL;DR: 现有基准未能充分评估统一多模态模型的理解与生成能力的整合。Uni-MMMU是一个新的综合性基准，旨在系统地评估这两种能力在八个推理领域中的双向协同作用。


<details>
  <summary>Details</summary>
Motivation: 当前的基准未能真正检验统一多模态模型中视觉理解与生成的整合，要么孤立地评估它们，要么忽视了本质上耦合的任务。

Method: 本文提出了Uni-MMMU，一个全面且领域感知的基准，它系统地揭示了生成与理解之间在科学、编程、数学和谜题等八个以推理为中心的领域中的双向协同作用。每个任务都是双向耦合的，要求模型要么利用概念理解指导精确的视觉合成，要么利用生成作为分析推理的认知支架。Uni-MMMU包含可验证的中间推理步骤、独特的真值以及文本和视觉输出的可复现评分协议。

Result: 通过对最先进的统一模型、仅生成模型和仅理解模型的广泛评估，本文揭示了显著的性能差异和跨模态依赖性。

Conclusion: 该研究提供了关于理解和生成能力何时以及如何相互增强的新见解，并为推进统一多模态模型奠定了可靠的基础。

Abstract: Unified multimodal models aim to jointly enable visual understanding and
generation, yet current benchmarks rarely examine their true integration.
Existing evaluations either treat the two abilities in isolation or overlook
tasks that inherently couple them. To address this gap, we present Uni-MMMU, a
comprehensive and discipline-aware benchmark that systematically unfolds the
bidirectional synergy between generation and understanding across eight
reasoning-centric domains, including science, coding, mathematics, and puzzles.
Each task is bidirectionally coupled, demanding models to (i) leverage
conceptual understanding to guide precise visual synthesis, or (ii) utilize
generation as a cognitive scaffold for analytical reasoning. Uni-MMMU
incorporates verifiable intermediate reasoning steps, unique ground truths, and
a reproducible scoring protocol for both textual and visual outputs. Through
extensive evaluation of state-of-the-art unified, generation-only, and
understanding-only models, we reveal substantial performance disparities and
cross-modal dependencies, offering new insights into when and how these
abilities reinforce one another, and establishing a reliable foundation for
advancing unified models.

</details>


### [116] [InteractiveOmni: A Unified Omni-modal Model for Audio-Visual Multi-turn Dialogue](https://arxiv.org/abs/2510.13747)
*Wenwen Tong,Hewei Guo,Dongchuan Ran,Jiangnan Chen,Jiefan Lu,Kaibin Wang,Keqiang Li,Xiaoxu Zhu,Jiakui Li,Kehan Li,Xueheng Li,Lumin Li,Chenxu Guo,Jiasheng Zhou,Jiandong Chen,Xianye Wu,Jiahao Wang,Silei Wu,Lei Chen,Hanming Deng,Yuxuan Song,Dinghao Zhou,Guiping Zhong,Ken Zheng,Shiyin Kang,Lewei Lu*

Main category: cs.CV

TL;DR: InteractiveOmni是一个统一的、开源的全模态大语言模型（4B至8B参数），专为音视频多轮交互设计，提供全面的全模态理解和语音生成能力，在轻量级模型中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有模型在轻量级全模态理解和语音生成方面可能存在局限，需要一个统一的、具备类人长程对话能力的全模态大语言模型来处理复杂的音视频多轮交互。

Method: 该研究将视觉编码器、音频编码器、大型语言模型和语音解码器集成到一个统一模型中。设计了多阶段训练策略，包括全模态理解预训练和语音对话与音视频交互后训练。同时，精心策划了多轮训练数据集以增强模型的长程对话能力。为评估多轮记忆和语音交互能力，构建了多模态多轮记忆基准和多轮语音交互基准。

Result: 实验表明，InteractiveOmni显著优于领先的开源模型，提供了更智能的多轮音视频体验，尤其在长程记忆能力方面表现突出。InteractiveOmni-4B在通用基准上可与更大的Qwen2.5-Omni-7B模型相媲美，并且在模型尺寸减半的情况下仍能保持InteractiveOmni-8B 97%的性能。在图像、音频、视频理解和语音生成任务上，InteractiveOmni与同等规模模型相比达到了最先进（SOTA）的结果。

Conclusion: InteractiveOmni为下一代智能交互系统提供了一个可访问的、开源的基础，并在轻量级模型中树立了全模态理解和语音生成的新标准。

Abstract: We introduce InteractiveOmni, a unified and open-source omni-modal large
language model for audio-visual multi-turn interaction, ranging from 4B to 8B
parameters, designed to lead the field of lightweight models by offering
comprehensive omni-modal understanding and speech generation capabilities. To
achieve this, we integrate the vision encoder, audio encoder, large language
model, and speech decoder into a unified model for understanding and generation
tasks. We design a multi-stage training strategy to ensure robust cross-modal
capabilities, including pre-training for omni-modal understanding, followed by
post-training with speech conversation and audio-visual interaction. To enable
human-like long-term conversational ability, we meticulously curate a
multi-turn training dataset that enhances the model's ability to handle complex
and multi-turn interactions. To effectively evaluate the multi-turn memory and
speech interaction capabilities, we construct the multi-modal multi-turn memory
benchmark and the multi-turn speech interaction benchmark. Experiments
demonstrate that InteractiveOmni significantly outperforms leading open-source
models and provides a more intelligent multi-turn audio-visual experience,
particularly in its long-term memory capabilities. Notably, InteractiveOmni-4B
is comparable to the much larger model like Qwen2.5-Omni-7B on general
benchmarks, and it can retain 97% of the performance of the InteractiveOmni-8B
while utilizing only 50% of the model size. Achieving state-of-the-art results
against similarly sized models across image, audio, video understanding, and
speech generation tasks, InteractiveOmni is an accessible, open-source
foundation for next-generation intelligent interactive systems.

</details>


### [117] [VisCoP: Visual Probing for Video Domain Adaptation of Vision Language Models](https://arxiv.org/abs/2510.13808)
*Dominick Reilly,Manish Kumar Govind,Le Xue,Srijan Das*

Main category: cs.CV

TL;DR: 大型视觉-语言模型（VLMs）在遇到领域漂移时性能下降。VisCoP通过在视觉编码器中添加可学习的视觉探针，实现了高效的领域适应，优于现有方法，同时保留了原有知识。


<details>
  <summary>Details</summary>
Motivation: 大型视觉-语言模型（VLMs）在面对与预训练数据存在显著分布差异的新领域时，性能会急剧下降。现有的领域适应方法在微调VLM组件时，常导致领域特定特征学习有限或灾难性遗忘。

Method: 本文提出了视觉上下文探测（VisCoP），通过为VLM的视觉编码器添加一组紧凑的可学习视觉探针来增强其功能。这些探针能够以最小化修改预训练参数的方式，实现高效的领域特定适应。

Result: 实验表明，VisCoP在跨视角（外视角到自我视角）、跨模态（RGB到深度）和跨任务（人类理解到机器人控制）三种具有挑战性的领域适应设置中，始终优于现有适应策略，在目标领域实现卓越性能，并有效保留了源领域知识。

Conclusion: VisCoP是一种有效解决VLM在领域漂移问题上性能下降的方法，它通过引入视觉探针实现了高效的领域特定适应，同时避免了灾难性遗忘，在多种适应场景下表现出色。

Abstract: Large Vision-Language Models (VLMs) excel at general visual reasoning tasks
but exhibit sharp performance degradation when applied to novel domains with
substantial distribution shifts from pretraining data. Existing domain
adaptation approaches finetune different VLM components, but this often results
in limited domain-specific feature learning or catastrophic forgetting of prior
capabilities. To address these issues, we introduce Vision Contextualized
Probing (VisCoP), which augments the VLM's vision encoder with a compact set of
learnable visual probes. These probes enable efficient domain-specific
adaptation with minimal modification to pretrained parameters. We evaluate
VisCoP across three challenging domain adaptation settings-cross-view
(exocentric to egocentric), cross-modal (RGB to depth), and cross-task (human
understanding to robot control). Experiments show that VisCoP consistently
outperforms existing adaptation strategies, achieving superior performance on
target domains while effectively retaining source-domain knowledge.

</details>


### [118] [Reasoning in Space via Grounding in the World](https://arxiv.org/abs/2510.13800)
*Yiming Chen,Zekun Qi,Wenyao Zhang,Xin Jin,Li Zhang,Peidong Liu*

Main category: cs.CV

TL;DR: 本文提出GS-Reasoner，一个3D大型语言模型，通过双路径池化机制构建统一的图像块3D表示，首次实现完全无需外部模块的自回归3D视觉定位，并显著提升空间推理能力，达到最先进水平。同时引入GCoT数据集以连接定位与空间推理。


<details>
  <summary>Details</summary>
Motivation: 现有3D LLM缺乏能同时捕获语义和几何信息的统一3D表示，导致定位性能不佳或过度依赖外部模块，阻碍了定位与空间推理的无缝集成。

Method: 1. 提出一种简单而有效的双路径池化机制，将几何特征与语义和位置线索紧密对齐，构建统一的基于图像块的3D表示，封装所有必要信息且不增加输入token数量。2. 基于此整体表示，开发GS-Reasoner，首次实现完全无需外部模块的自回归定位。3. 引入Grounded Chain-of-Thought (GCoT) 数据集，包含3D边界框标注和将定位作为核心部分的逐步推理路径，以进一步连接定位和空间推理。

Result: GS-Reasoner在3D视觉定位上取得了令人印象深刻的结果，其性能与现有最先进模型相当，并显著增强了其空间推理能力，达到了最先进的性能。

Conclusion: GS-Reasoner建立了一个统一且自洽的3D空间推理框架，通过有效的3D表示和无需外部模块的自回归定位，显著提升了3D视觉定位和空间推理能力，为该领域树立了新标准。

Abstract: In this paper, we claim that 3D visual grounding is the cornerstone of
spatial reasoning and introduce the Grounded-Spatial Reasoner (GS-Reasoner) to
explore the effective spatial representations that bridge the gap between them.
Existing 3D LLMs suffer from the absence of a unified 3D representation capable
of jointly capturing semantic and geometric information. This deficiency is
manifested either in poor performance on grounding or in an excessive reliance
on external modules, ultimately hindering the seamless integration of grounding
and spatial reasoning. To address this, we propose a simple yet effective
dual-path pooling mechanism that tightly aligns geometric features with both
semantic and positional cues, constructing a unified image patch-based 3D
representation that encapsulates all essential information without increasing
the number of input tokens. Leveraging this holistic representation,
GS-Reasoner is the first 3D LLM that achieves autoregressive grounding entirely
without external modules while delivering performance comparable to
state-of-the-art models, establishing a unified and self-contained framework
for 3D spatial reasoning. To further bridge grounding and spatial reasoning, we
introduce the Grounded Chain-of-Thought (GCoT) dataset. This dataset is
meticulously curated to include both 3D bounding box annotations for objects
referenced in reasoning questions and step-by-step reasoning paths that
integrate grounding as a core component of the problem-solving process.
Extensive experiments demonstrate that GS-Reasoner achieves impressive results
on 3D visual grounding, which in turn significantly enhances its spatial
reasoning capabilities, leading to state-of-the-art performance.

</details>


### [119] [Trace Anything: Representing Any Video in 4D via Trajectory Fields](https://arxiv.org/abs/2510.13802)
*Xinhang Liu,Yuxi Xiao,Donny Y. Chen,Jiashi Feng,Yu-Wing Tai,Chi-Keung Tang,Bingyi Kang*

Main category: cs.CV

TL;DR: 该论文提出了一种将视频表示为“轨迹场”的新方法，其中每个像素都对应一个连续的3D轨迹函数。他们还引入了“Trace Anything”神经网络，通过单次前向传播预测整个轨迹场，实现了高效且高性能的视频动态建模。


<details>
  <summary>Details</summary>
Motivation: 有效的时空表示对于视频中的动态建模、理解和预测至关重要。研究者认为，视频的基本单位——像素——在时间上追踪连续的3D轨迹，是动态的原始元素，这启发了他们构建轨迹场表示。

Method: 研究人员提出将任何视频表示为“轨迹场”，即为每一帧中的每个像素分配一个连续的3D轨迹函数。他们引入了“Trace Anything”神经网络，该网络通过单次前向传播预测整个轨迹场。具体来说，模型为每个像素预测一组控制点，这些控制点参数化一条B样条曲线，从而在任意查询时间点给出其3D位置。该模型在包括他们新平台数据在内的大规模4D数据上进行了训练。

Result: 实验结果表明：(i) Trace Anything 在他们新的轨迹场估计基准上达到了最先进的性能，并在已有的点跟踪基准上表现出竞争力；(ii) 由于其一次性处理范式，无需迭代优化或辅助估计器，显著提高了效率；(iii) 它展现出了一些新兴能力，包括目标条件操作、运动预测和时空融合。

Conclusion: 该研究成功地将视频表示为轨迹场，并通过 Trace Anything 神经网络实现了高效且高性能的预测。这种新的表示和方法不仅在轨迹场估计上达到了SOTA，提高了效率，还展现了多种新兴能力，为视频动态建模提供了强大的工具。

Abstract: Effective spatio-temporal representation is fundamental to modeling,
understanding, and predicting dynamics in videos. The atomic unit of a video,
the pixel, traces a continuous 3D trajectory over time, serving as the
primitive element of dynamics. Based on this principle, we propose representing
any video as a Trajectory Field: a dense mapping that assigns a continuous 3D
trajectory function of time to each pixel in every frame. With this
representation, we introduce Trace Anything, a neural network that predicts the
entire trajectory field in a single feed-forward pass. Specifically, for each
pixel in each frame, our model predicts a set of control points that
parameterizes a trajectory (i.e., a B-spline), yielding its 3D position at
arbitrary query time instants. We trained the Trace Anything model on
large-scale 4D data, including data from our new platform, and our experiments
demonstrate that: (i) Trace Anything achieves state-of-the-art performance on
our new benchmark for trajectory field estimation and performs competitively on
established point-tracking benchmarks; (ii) it offers significant efficiency
gains thanks to its one-pass paradigm, without requiring iterative optimization
or auxiliary estimators; and (iii) it exhibits emergent abilities, including
goal-conditioned manipulation, motion forecasting, and spatio-temporal fusion.
Project page: https://trace-anything.github.io/.

</details>


### [120] [PhysMaster: Mastering Physical Representation for Video Generation via Reinforcement Learning](https://arxiv.org/abs/2510.13809)
*Sihui Ji,Xi Chen,Xin Tao,Pengfei Wan,Hengshuang Zhao*

Main category: cs.CV

TL;DR: PhysMaster通过强化学习与人类反馈，从输入图像中学习物理表示，以指导视频生成模型，使其生成的视频更符合物理规律。


<details>
  <summary>Details</summary>
Motivation: 当前的视频生成模型虽然视觉上逼真，但往往不遵守物理定律，这限制了它们生成符合物理规律的视频的能力，也阻碍了它们作为“世界模型”的应用。

Method: PhysMaster基于图像到视频的任务。它设计了PhysEncoder来从输入图像中编码物理信息作为额外条件，以将物理知识注入视频生成过程。为了解决缺乏对模型物理性能的适当监督问题，PhysEncoder利用强化学习与人类反馈（RLHF），通过直接偏好优化（DPO）以端到端的方式优化物理表示。

Result: PhysMaster成功提高了PhysEncoder的物理感知能力，进而提升了视频生成的物理符合性。它在一个简单的代理任务上证明了其能力，并展示了对广泛物理场景的泛化性。

Conclusion: PhysMaster通过在强化学习范式下进行表示学习，为各种物理过程提供了统一的解决方案。它是一个通用且可插拔的解决方案，适用于物理感知视频生成及更广泛的应用。

Abstract: Video generation models nowadays are capable of generating visually realistic
videos, but often fail to adhere to physical laws, limiting their ability to
generate physically plausible videos and serve as ''world models''. To address
this issue, we propose PhysMaster, which captures physical knowledge as a
representation for guiding video generation models to enhance their
physics-awareness. Specifically, PhysMaster is based on the image-to-video task
where the model is expected to predict physically plausible dynamics from the
input image. Since the input image provides physical priors like relative
positions and potential interactions of objects in the scenario, we devise
PhysEncoder to encode physical information from it as an extra condition to
inject physical knowledge into the video generation process. The lack of proper
supervision on the model's physical performance beyond mere appearance
motivates PhysEncoder to apply reinforcement learning with human feedback to
physical representation learning, which leverages feedback from generation
models to optimize physical representations with Direct Preference Optimization
(DPO) in an end-to-end manner. PhysMaster provides a feasible solution for
improving physics-awareness of PhysEncoder and thus of video generation,
proving its ability on a simple proxy task and generalizability to wide-ranging
physical scenarios. This implies that our PhysMaster, which unifies solutions
for various physical processes via representation learning in the reinforcement
learning paradigm, can act as a generic and plug-in solution for physics-aware
video generation and broader applications.

</details>


### [121] [Adaptive Visual Conditioning for Semantic Consistency in Diffusion-Based Story Continuation](https://arxiv.org/abs/2510.13787)
*Seyed Mohammad Mousavi,Morteza Analoui*

Main category: cs.CV

TL;DR: 本文提出AVC（自适应视觉条件）框架，用于基于扩散模型的故事续写，通过CLIP模型自适应地利用先前视觉上下文，并在不相关时限制其影响，同时利用大型语言模型改进数据质量，实现了卓越的连贯性、语义一致性和视觉逼真度。


<details>
  <summary>Details</summary>
Motivation: 故事续写面临的核心挑战是如何有效利用先前的视觉上下文，同时确保与当前文本输入语义对齐，避免引入误导性或不相关信息。

Method: 本文引入了AVC（自适应视觉条件）框架，用于基于扩散模型的故事续写。AVC使用CLIP模型检索最语义对齐的先前图像。当没有足够相关的图像时，AVC自适应地将先前视觉的影响限制在扩散过程的早期阶段。此外，通过使用大型语言模型重新标注噪声数据集，提高了数据质量，增强了文本监督和语义对齐。

Result: 定量结果和人工评估表明，与现有基线相比，AVC在连贯性、语义一致性和视觉逼真度方面表现优越，特别是在先前视觉与当前输入冲突的挑战性情况下。

Conclusion: AVC通过自适应地利用和限制先前视觉上下文的影响，并结合大型语言模型改进数据质量，有效解决了故事续写中的关键挑战，显著提升了生成图像的质量和一致性。

Abstract: Story continuation focuses on generating the next image in a narrative
sequence so that it remains coherent with both the ongoing text description and
the previously observed images. A central challenge in this setting lies in
utilizing prior visual context effectively, while ensuring semantic alignment
with the current textual input. In this work, we introduce AVC (Adaptive Visual
Conditioning), a framework for diffusion-based story continuation. AVC employs
the CLIP model to retrieve the most semantically aligned image from previous
frames. Crucially, when no sufficiently relevant image is found, AVC adaptively
restricts the influence of prior visuals to only the early stages of the
diffusion process. This enables the model to exploit visual context when
beneficial, while avoiding the injection of misleading or irrelevant
information. Furthermore, we improve data quality by re-captioning a noisy
dataset using large language models, thereby strengthening textual supervision
and semantic alignment. Quantitative results and human evaluations demonstrate
that AVC achieves superior coherence, semantic consistency, and visual fidelity
compared to strong baselines, particularly in challenging cases where prior
visuals conflict with the current input.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [122] [Benchmarking Open-Source Large Language Models for Persian in Zero-Shot and Few-Shot Learning](https://arxiv.org/abs/2510.12807)
*Mahdi Cherakhloo,Arash Abbasi,Mohammad Saeid Sarafraz,Bijan Vosoughi Vahdat*

Main category: cs.CL

TL;DR: 本文对多个开源大型语言模型在波斯语自然语言处理任务上的表现进行了综合基准测试，发现Gemma 2表现最佳，但大多数模型在令牌级理解任务上仍面临挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在多种语言中表现出色，但它们在波斯语等低资源语言中的有效性需要深入研究。

Method: 研究采用零样本和少样本学习范式，评估了多个开源大型语言模型在波斯语自然语言处理任务（包括情感分析、命名实体识别、阅读理解和问答）上的表现。使用了ParsiNLU和ArmanEmo等波斯语数据集，并采用准确率、F1分数、BLEU和ROUGE等指标进行性能评估。

Result: 结果显示，Gemma 2在两种学习范式下几乎所有任务中都始终优于其他模型，尤其在复杂推理任务中表现突出。然而，大多数模型在命名实体识别等令牌级理解任务上表现不佳，凸显了波斯语处理中的具体挑战。

Conclusion: 本研究为多语言大型语言模型在波斯语中的表现提供了宝贵的见解，并为未来波斯语模型开发提供了基准，同时指出了波斯语处理中令牌级理解任务的挑战。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
numerous languages; however, their effectiveness in low-resource languages like
Persian requires thorough investigation. This paper presents a comprehensive
benchmark of several open-source LLMs for Persian Natural Language Processing
(NLP) tasks, utilizing both zero-shot and few-shot learning paradigms. We
evaluate models across a range of tasks including sentiment analysis, named
entity recognition, reading comprehension, and question answering, using
established Persian datasets such as ParsiNLU and ArmanEmo. Our methodology
encompasses rigorous experimental setups for both zero-shot and few-shot
scenarios, employing metrics such as Accuracy, F1-score, BLEU, and ROUGE for
performance evaluation. The results reveal that Gemma 2 consistently
outperforms other models across nearly all tasks in both learning paradigms,
with particularly strong performance in complex reasoning tasks. However, most
models struggle with token-level understanding tasks like Named Entity
Recognition, highlighting specific challenges in Persian language processing.
This study contributes to the growing body of research on multilingual LLMs,
providing valuable insights into their performance in Persian and offering a
benchmark for future model development.

</details>


### [123] [Cancer Diagnosis Categorization in Electronic Health Records Using Large Language Models and BioBERT: Model Performance Evaluation Study](https://arxiv.org/abs/2510.12813)
*Soheil Hashtarkhani,Rezaur Rashid,Christopher L Brett,Lokesh Chinthala,Fekede Asefa Kumsa,Janet A Zink,Robert L Davis,David L Schwartz,Arash Shaban-Nejad*

Main category: cs.CL

TL;DR: 本研究评估了GPT-3.5、GPT-4o、Llama 3.2、Gemini 1.5和BioBERT在从电子健康记录（EHR）中分类癌症诊断方面的性能。BioBERT在ICD代码分类上表现最佳，而GPT-4o在自由文本诊断上表现更优。尽管现有性能可用于行政和研究，但临床应用仍需人工监督。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录（EHR）中存在结构不一致或自由文本数据，需要高效的预处理以支持预测性医疗模型。尽管人工智能驱动的自然语言处理（NLP）工具在自动化诊断分类方面前景广阔，但其比较性能和临床可靠性仍需系统评估。

Method: 研究评估了四种大型语言模型（GPT-3.5、GPT-4o、Llama 3.2和Gemini 1.5）以及BioBERT，用于从结构化和非结构化EHR数据中分类癌症诊断。分析了来自3456份癌症患者记录中的762个独特诊断（326个国际疾病分类（ICD）代码描述，436个自由文本条目）。模型被测试将其诊断分类到14个预定义类别中。分类结果由两名肿瘤学专家进行验证。

Result: 对于ICD代码，BioBERT取得了最高的加权宏F1分数（84.2），并在ICD代码准确性上与GPT-4o持平（90.8）。对于自由文本诊断，GPT-4o在加权宏F1分数（71.8 vs 61.5）上优于BioBERT，并取得了略高的准确性（81.9 vs 81.6）。GPT-3.5、Gemini和Llama在两种格式上的总体表现均较低。常见的错误分类模式包括转移瘤与中枢神经系统肿瘤之间的混淆，以及涉及模糊或重叠临床术语的错误。

Conclusion: 尽管目前的性能水平似乎足以满足行政和研究用途，但可靠的临床应用将需要标准化的文档实践，以及在高风险决策中强大的人工监督。

Abstract: Electronic health records contain inconsistently structured or free-text
data, requiring efficient preprocessing to enable predictive health care
models. Although artificial intelligence-driven natural language processing
tools show promise for automating diagnosis classification, their comparative
performance and clinical reliability require systematic evaluation. The aim of
this study is to evaluate the performance of 4 large language models (GPT-3.5,
GPT-4o, Llama 3.2, and Gemini 1.5) and BioBERT in classifying cancer diagnoses
from structured and unstructured electronic health records data. We analyzed
762 unique diagnoses (326 International Classification of Diseases (ICD) code
descriptions, 436free-text entries) from 3456 records of patients with cancer.
Models were tested on their ability to categorize diagnoses into 14predefined
categories. Two oncology experts validated classifications. BioBERT achieved
the highest weighted macro F1-score for ICD codes (84.2) and matched GPT-4o in
ICD code accuracy (90.8). For free-text diagnoses, GPT-4o outperformed BioBERT
in weighted macro F1-score (71.8 vs 61.5) and achieved slightly higher accuracy
(81.9 vs 81.6). GPT-3.5, Gemini, and Llama showed lower overall performance on
both formats. Common misclassification patterns included confusion between
metastasis and central nervous system tumors, as well as errors involving
ambiguous or overlapping clinical terminology. Although current performance
levels appear sufficient for administrative and research use, reliable clinical
applications will require standardized documentation practices alongside robust
human oversight for high-stakes decision-making.

</details>


### [124] [From Noise to Signal to Selbstzweck: Reframing Human Label Variation in the Era of Post-training in NLP](https://arxiv.org/abs/2510.12817)
*Shanshan Xu,Santosh T. Y. S. S,Barbara Plank*

Main category: cs.CL

TL;DR: 本文认为人类标注变异（HLV）并非噪音，而是反映人类视角多样性的信号，应在大型语言模型（LLM）的偏好学习数据集中予以保留和主动整合，以维护人类价值观的多元性。


<details>
  <summary>Details</summary>
Motivation: 在自然语言处理（NLP）领域，人类标注变异（HLV）长期被视为需要消除的噪音，或在偏好学习数据集中被聚合，导致人类视角的多元性被抹平。随着大型语言模型（LLM）兴起以及人类反馈在模型对齐中的核心作用，HLV的重要性日益凸显，但现有方法未能有效利用或保留这种多样性，从而无法真正体现人类价值观的多元性。

Method: 本文是一篇立场声明（position paper），通过论证强调保留HLV的重要性，并提出将其作为设计AI系统时的一个内在目标（Selbstzweck）。文章呼吁主动将HLV纳入偏好数据集，并概述了实现这一目标的具体步骤。

Result: 本文的主要“结果”是提出并论证了一个核心观点：保留HLV作为人类多元性的体现，必须被视为AI系统设计中的一个内在目标。文章呼吁行业采取行动，将HLV主动整合到偏好数据集中。

Conclusion: 人类标注变异（HLV）应被视为一个自我目的，即设计AI系统时必须保留和体现的内在目标。为了确保大型语言模型（LLM）的对齐能够真正反映人类价值观的多元性，必须将HLV主动纳入偏好数据集，而不是将其视为噪音或进行聚合处理。

Abstract: Human Label Variation (HLV) refers to legitimate disagreement in annotation
that reflects the genuine diversity of human perspectives rather than mere
error. For decades, HLV in NLP was dismissed as noise to be discarded, and only
slowly over the last decade has it been reframed as a signal for improving
model robustness. With the rise of large language models (LLMs), where
post-training on human feedback has become central to model alignment, the role
of HLV has become increasingly consequential. Yet current preference-learning
datasets routinely aggregate multiple annotations into a single label, thereby
flattening diverse perspectives into a false universal agreement and erasing
precisely the pluralism of human values that alignment aims to preserve. In
this position paper, we argue that preserving HLV as an embodiment of human
pluralism must be treated as a Selbstzweck - a goal it self when designing AI
systems. We call for proactively incorporating HLV into preference datasets and
outline actionable steps towards it.

</details>


### [125] [MEDEQUALQA: Evaluating Biases in LLMs with Counterfactual Reasoning](https://arxiv.org/abs/2510.12818)
*Rajarshi Ghosh,Abhay Gupta,Hudson McBride,Anurag Vaidya,Faisal Mahmood*

Main category: cs.CL

TL;DR: 研究发现，在临床决策支持中，即使最终诊断不变，大型语言模型（LLMs）的内部推理过程也会因患者代词（如他/她）的细微变化而出现局部差异。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究表明LLMs在不同患者群体中存在输出差异，但对于在受控的人口统计学变化下，LLM内部推理如何变化，以及这些变化如何影响其决策过程，目前仍知之甚少。

Method: 引入了MEDEQUALQA反事实基准，该基准通过仅改变患者代词（他/她/他们）并保持关键症状和条件不变来创建。每个临床病例被扩展为单关键症状消融，生成三个平行数据集，总计约69,000个项目。使用GPT-4.1模型进行评估，并通过计算推理轨迹间的语义文本相似度（STS）来衡量代词变体间的推理稳定性。

Result: 结果显示，整体相似度较高（平均STS > 0.80），但即使最终诊断保持不变，在引用的风险因素、指南依据和鉴别诊断顺序上仍存在一致的局部差异。误差分析突出了推理转移的特定案例，揭示了可能导致不公平医疗的临床相关偏见。

Conclusion: MEDEQUALQA为审计医疗AI的推理稳定性提供了一个受控的诊断环境，并揭示了LLM在处理人口统计学信息时可能存在的微妙推理偏差。

Abstract: Large language models (LLMs) are increasingly deployed in clinical decision
support, yet subtle demographic cues can influence their reasoning. Prior work
has documented disparities in outputs across patient groups, but little is
known about how internal reasoning shifts under controlled demographic changes.
We introduce MEDEQUALQA, a counterfactual benchmark that perturbs only patient
pronouns (he/him, she/her, they/them) while holding critical symptoms and
conditions (CSCs) constant. Each clinical vignette is expanded into single-CSC
ablations, producing three parallel datasets of approximately 23,000 items each
(69,000 total). We evaluate a GPT-4.1 model and compute Semantic Textual
Similarity (STS) between reasoning traces to measure stability across pronoun
variants. Our results show overall high similarity (mean STS >0.80), but reveal
consistent localized divergences in cited risk factors, guideline anchors, and
differential ordering, even when final diagnoses remain unchanged. Our error
analysis highlights certain cases in which the reasoning shifts, underscoring
clinically relevant bias loci that may cascade into inequitable care.
MEDEQUALQA offers a controlled diagnostic setting for auditing reasoning
stability in medical AI.

</details>


### [126] [Classifier-Augmented Generation for Structured Workflow Prediction](https://arxiv.org/abs/2510.12825)
*Thomas Gschwind,Shramona Chakraborty,Nitin Gupta,Sameep Mehta*

Main category: cs.CL

TL;DR: 该论文提出一个系统，能将自然语言描述转化为可执行的ETL工作流，自动预测其结构和详细配置，显著提高准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 配置ETL工具（如IBM DataStage）耗时且需要深厚的工具知识，阻碍了用户高效地组装复杂数据工作流。

Method: 核心方法是“分类器增强生成”（CAG），它结合了话语分解、分类器和特定阶段的少样本提示来准确预测阶段。然后通过边缘预测将这些阶段连接成非线性工作流，并从子话语上下文推断阶段属性。该架构模块化、可解释，并包含鲁棒的验证步骤。

Result: 与强大的单提示和代理基线相比，CAG方法显著提高了准确性和效率，并大幅减少了令牌使用。据作者所知，这是第一个对自然语言驱动的ETL创作中的阶段预测、边缘布局和属性生成进行详细评估的系统。

Conclusion: 该系统成功地将自然语言描述转化为可执行的ETL工作流，提供了一个模块化、可解释且高效的端到端解决方案，能够自动预测工作流的结构和详细配置，从而简化ETL创作过程。

Abstract: ETL (Extract, Transform, Load) tools such as IBM DataStage allow users to
visually assemble complex data workflows, but configuring stages and their
properties remains time consuming and requires deep tool knowledge. We propose
a system that translates natural language descriptions into executable
workflows, automatically predicting both the structure and detailed
configuration of the flow. At its core lies a Classifier-Augmented Generation
(CAG) approach that combines utterance decomposition with a classifier and
stage-specific few-shot prompting to produce accurate stage predictions. These
stages are then connected into non-linear workflows using edge prediction, and
stage properties are inferred from sub-utterance context. We compare CAG
against strong single-prompt and agentic baselines, showing improved accuracy
and efficiency, while substantially reducing token usage. Our architecture is
modular, interpretable, and capable of end-to-end workflow generation,
including robust validation steps. To our knowledge, this is the first system
with a detailed evaluation across stage prediction, edge layout, and property
generation for natural-language-driven ETL authoring.

</details>


### [127] [Scheming Ability in LLM-to-LLM Strategic Interactions](https://arxiv.org/abs/2510.12826)
*Thao Pham*

Main category: cs.CL

TL;DR: 研究发现，大型语言模型（LLM）代理在LLM-to-LLM交互中展现出显著的战略欺骗能力和倾向，尤其是在高风险博弈论场景下。


<details>
  <summary>Details</summary>
Motivation: 随着LLM代理在不同环境中自主部署，评估其战略欺骗能力变得至关重要。现有研究主要关注AI系统对人类开发者的欺骗，而LLM-to-LLM之间的欺骗能力仍未得到充分探索。

Method: 研究通过两种博弈论框架（廉价对话信号博弈和同行评估对抗博弈）调查了前沿LLM代理的欺骗能力和倾向。测试了四种模型（GPT-4o、Gemini-2.5-pro、Claude-3.7-Sonnet和Llama-3.3-70b），测量了有/无明确提示下的欺骗表现，并通过思维链推理分析了欺骗策略。

Result: 在明确提示下，大多数模型（特别是Gemini-2.5-pro和Claude-3.7-Sonnet）表现接近完美。更重要的是，在没有提示的情况下，模型也表现出显著的欺骗倾向：所有模型在同行评估中都选择了欺骗而非坦白（100%），而在廉价对话中选择欺骗的模型成功率达到95-100%。

Conclusion: 这些发现强调了在多智能体设置中，使用高风险博弈论场景进行稳健评估的必要性，以应对LLM代理可能存在的战略欺骗行为。

Abstract: As large language model (LLM) agents are deployed autonomously in diverse
contexts, evaluating their capacity for strategic deception becomes crucial.
While recent research has examined how AI systems scheme against human
developers, LLM-to-LLM scheming remains underexplored. We investigate the
scheming ability and propensity of frontier LLM agents through two
game-theoretic frameworks: a Cheap Talk signaling game and a Peer Evaluation
adversarial game. Testing four models (GPT-4o, Gemini-2.5-pro,
Claude-3.7-Sonnet, and Llama-3.3-70b), we measure scheming performance with and
without explicit prompting while analyzing scheming tactics through
chain-of-thought reasoning. When prompted, most models, especially
Gemini-2.5-pro and Claude-3.7-Sonnet, achieved near-perfect performance.
Critically, models exhibited significant scheming propensity without prompting:
all models chose deception over confession in Peer Evaluation (100% rate),
while models choosing to scheme in Cheap Talk succeeded at 95-100% rates. These
findings highlight the need for robust evaluations using high-stakes
game-theoretic scenarios in multi-agent settings.

</details>


### [128] [Mathematics with large language models as provers and verifiers](https://arxiv.org/abs/2510.12829)
*Hieu Le Duc,Leo Liberti*

Main category: cs.CL

TL;DR: 本文报告了ChatGPT（使用gpt-5模型）在定理证明方面的重大突破，通过协作式证明者-验证者协议和Lean形式化验证，成功解决了国际数学奥林匹克问题和数论猜想。


<details>
  <summary>Details</summary>
Motivation: 2024-2025年间，大型语言模型在定理证明方面的能力讨论取得了令人瞩目的成功，特别是在解决国际数学奥林匹克等难题和验证AI是否能证明特定猜想方面，这促使研究人员进一步探索并报告新的突破。

Method: 本研究采用ChatGPT（具体是gpt-5模型）的协作式协议，其中涉及不同的gpt-5实例分别充当证明者和验证者。为确保证明不出现幻觉，最终证明由Lean证明助手进行形式化验证，并且Lean代码的前提和结论的一致性由人类进行核实。

Result: 该方法成功解决了2025年国际数学奥林匹克六个问题中的五个，并解决了Cohen (2025)论文中六十六个数论猜想的三分之一。

Conclusion: 结合协作式LLM实例与形式化和人类验证的方法，在AI解决复杂数学问题的定理证明能力方面取得了显著进展，展示了其在高级数学推理中的强大潜力。

Abstract: During 2024 and 2025 the discussion about the theorem-proving capabilities of
large language models started reporting interesting success stories, mostly to
do with difficult exercises (such as problems from the International
Mathematical Olympiad), but also with conjectures [Feldman & Karbasi,
arXiv:2509.18383v1] formulated for the purpose of verifying whether the
artificial intelligence could prove it. In this paper we report a theorem
proving feat achieved by ChatGPT by using a protocol involving different prover
and verifier instances of the gpt-5 model working collaboratively. To make sure
that the produced proofs do not suffer from hallucinations, the final proof is
formally verified by the lean proof assistant, and the conformance of premises
and conclusion of the lean code is verified by a human. Our methodology was
able to solve five out of six 2025 IMO problems, and close a third of the
sixty-six number theory conjectures in [Cohen, Journal of Integer Sequences,
2025].

</details>


### [129] [MTSQL-R1: Towards Long-Horizon Multi-Turn Text-to-SQL via Agentic Training](https://arxiv.org/abs/2510.12831)
*Taicheng Guo,Hai Wang,ChaoChun Liu,Mohsen Golalikhani,Xin Chen,Xiangliang Zhang,Chandan K. Reddy*

Main category: cs.CL

TL;DR: MTSQL-R1是一个用于长程多轮Text-to-SQL的智能体训练框架，它通过将任务建模为马尔可夫决策过程（MDP），利用数据库执行反馈和对话记忆进行一致性验证，实现了迭代的SQL查询生成、验证和精炼。


<details>
  <summary>Details</summary>
Motivation: 现有系统大多将多轮Text-to-SQL视为简单的文本翻译任务，采用短视范式，每轮生成一个查询而不进行执行、显式验证和精炼，导致输出不可执行或不连贯。

Method: 本文将多轮Text-to-SQL任务建模为马尔可夫决策过程（MDP）。一个智能体与数据库（获取执行反馈）和持久对话记忆（进行连贯性验证）进行交互，执行迭代的“提出-执行-验证-精炼”循环，直到所有检查通过。

Result: 在COSQL和SPARC数据集上的实验表明，MTSQL-R1持续优于强大的基线模型。

Conclusion: 研究结果强调了环境驱动的验证和记忆引导的精炼对于会话语义解析的重要性。

Abstract: Multi-turn Text-to-SQL aims to translate a user's conversational utterances
into executable SQL while preserving dialogue coherence and grounding to the
target schema. However, most existing systems only regard this task as a simple
text translation task and follow a short-horizon paradigm, generating a query
per turn without execution, explicit verification, and refinement, which leads
to non-executable or incoherent outputs. We present MTSQL-R1, an agentic
training framework for long-horizon multi-turn Text-to-SQL. We cast the task as
a Markov Decision Process (MDP) in which an agent interacts with (i) a database
for execution feedback and (ii) a persistent dialogue memory for coherence
verification, performing an iterative propose to execute -> verify -> refine
cycle until all checks pass. Experiments on COSQL and SPARC demonstrate that
MTSQL-R1 consistently outperforms strong baselines, highlighting the importance
of environment-driven verification and memory-guided refinement for
conversational semantic parsing. Full recipes (including code, trained models,
logs, reasoning trajectories, etc.) will be released after the internal review
to contribute to community research.

</details>


### [130] [Repurposing Annotation Guidelines to Instruct LLM Annotators: A Case Study](https://arxiv.org/abs/2510.12835)
*Kon Woo Kim,Rezarta Islamaj,Jin-Dong Kim,Florian Boudin,Akiko Aizawa*

Main category: cs.CL

TL;DR: 本研究提出了一种通过LLM审核过程将现有的人工标注指南转换为LLM可用的明确指令的方法，以实现文本标注任务，并证明了其有效性，同时揭示了实际挑战。


<details>
  <summary>Details</summary>
Motivation: 传统的标注指南是为人类标注者设计的，需要内部化训练，而大型语言模型（LLMs）需要明确、结构化的指令。因此，需要一种方法来使现有指南适用于LLMs。

Method: 研究提出了一种面向审核的指南再利用方法。该方法通过一个LLM审核过程，将现有指南转化为LLM可理解的清晰指令。以NCBI疾病语料库作为案例研究进行实验。

Result: 实验结果表明，再利用的指南能有效指导LLM标注者，但同时也揭示了一些实际挑战。该工作流程在支持可扩展、经济高效的标注指南优化和自动化标注方面具有潜力。

Conclusion: 该研究提出的指南再利用方法为LLM文本标注任务提供了一个有前景的解决方案，有助于提升标注指南的精炼度和自动化标注的效率和成本效益。

Abstract: This study investigates how existing annotation guidelines can be repurposed
to instruct large language model (LLM) annotators for text annotation tasks.
Traditional guidelines are written for human annotators who internalize
training, while LLMs require explicit, structured instructions. We propose a
moderation-oriented guideline repurposing method that transforms guidelines
into clear directives for LLMs through an LLM moderation process. Using the
NCBI Disease Corpus as a case study, our experiments show that repurposed
guidelines can effectively guide LLM annotators, while revealing several
practical challenges. The results highlight the potential of this workflow to
support scalable and cost-effective refinement of annotation guidelines and
automated annotation.

</details>


### [131] [A\textsuperscript{2}FM: An Adaptive Agent Foundation Model for Tool-Aware Hybrid Reasoning](https://arxiv.org/abs/2510.12838)
*Qianben Chen,Jingyi Cao,Jiayu Zhang,Tianrui Qin,Xiaowan Li,King Zhu,Dingfeng Shi,He Zhu,Minghao Liu,Xiaobo Liang,Ge Zhang,Jian Yang,Yuchen Eleanor Jiang,Wangchunshu Zhou*

Main category: cs.CL

TL;DR: 本文提出了A²FM，一个统一的大语言模型框架，通过“路由-对齐”原则整合了推理型和智能体型LLM，并引入了即时模式处理简单查询，显著提高了准确性和成本效率。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型分为推理型（擅长内部推理但无法使用工具）和智能体型（擅长使用工具但推理能力不足），这导致它们在处理简单查询时效率低下，常常过度推理或过度调用工具。

Method: 本文提出了自适应智能体基础模型（A²FM），遵循“路由-对齐”原则，模型首先学习任务感知路由，然后在共享骨干下对齐模式特定的轨迹。为解决效率问题，引入了第三种“即时”模式直接处理简单查询。同时，提出了自适应策略优化（APO），通过强制跨模式自适应采样和应用成本正则化奖励来共同提高准确性和效率。

Result: 在32B规模下，A²FM在BrowseComp上达到13.4%，AIME25上达到70.4%，HLE上达到16.7%，在同类模型中创下新的SOTA，并在智能体、推理和通用基准测试中与前沿LLM表现出竞争力。自适应执行的每次正确答案成本仅为0.00487美元，相对于推理模型降低了45.2%的成本，相对于智能体模型降低了33.5%的成本，在保持可比准确性的同时大大提高了成本效率。

Conclusion: A²FM成功地将推理型和智能体型大语言模型统一起来，并通过引入即时模式有效解决了现有模型的效率问题，在多个基准测试中展现出卓越的性能和显著的成本效益。

Abstract: Large language models split into two families: reasoning-centric LLMs, which
strengthen internal chain-of-thought reasoning but cannot invoke external
tools, and agentic LLMs, which learn to interact with environments and leverage
tools but often lag in deep reasoning. This divide arises from fundamentally
different training objectives, leading to mismatched strengths and inefficiency
on simple queries, where both families tend to overthink or over-call tools. In
this work, we present Adaptive Agent Foundation Model (A\textsuperscript{2}FM),
a unified framework that follows a route-then-align principle: the model first
learns task-aware routing and then aligns mode-specific trajectories under a
shared backbone. To address the inefficiency gap, we introduce a third
mode-instant-that handles simple queries directly, preventing unnecessary
reasoning or tool calls while complementing the agentic and reasoning modes. To
jointly enhance accuracy and efficiency, we propose Adaptive Policy
Optimization (APO), which enforces adaptive sampling across modes and applies a
cost-regularized reward. On the 32B scale, A\textsuperscript{2}FM achieves
13.4\% on BrowseComp, 70.4\% on AIME25, and 16.7\% on HLE, setting new SOTA
among comparable models and performing competitively with frontier LLMs across
agentic, reasoning, and general benchmarks. Notably, the adaptive execution
achieves a cost of pass of only \$0.00487 per correct answer-cutting cost by
45.2\% relative to reasoning and 33.5\% relative to agentic, thus delivering
substantially higher cost efficiency while maintaining comparable accuracy.

</details>


### [132] [FaStFACT: Faster, Stronger Long-Form Factuality Evaluations in LLMs](https://arxiv.org/abs/2510.12839)
*Yingjia Wan,Haochen Tan,Xiao Zhu,Xinyu Zhou,Zhiwei Li,Qingsong Lv,Changxuan Sun,Jiaqi Zeng,Yi Xu,Jianqiao Lu,Yinhong Liu,Zhijiang Guo*

Main category: cs.CL

TL;DR: 本文提出了一种名为\name的快速且强大的评估框架，用于评估大型语言模型（LLM）长文本生成的事实性，该框架在效率和与人类评估的一致性方面均优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型长文本生成的事实性面临挑战，现有方法存在准确性问题和高昂的人工评估成本。以往的方法通过分解文本、搜索证据和验证主张来尝试解决，但存在效率低下（管道复杂不适用于长文本）和有效性不足（主张集不准确、证据收集不足）的问题。

Method: \name框架首先采用块级主张提取，并结合基于置信度的预验证，显著降低了网络搜索和推理调用的成本，同时确保了可靠性。在搜索和验证阶段，它从爬取的网页中收集文档级证据，并在验证时选择性地检索，解决了以往管道中证据不足的问题。

Result: 基于一个聚合且经过人工标注的基准进行的广泛实验表明，\name在高效且有效地评估长篇LLM生成的事实性方面表现出可靠性。它在与人类评估的一致性和效率方面均达到了现有基线中的最高水平。

Conclusion: \name成功解决了现有事实性评估方法的局限性，提供了一个快速且强大的框架，能够高效、有效地评估大型语言模型长文本生成的事实性，并与人类评估高度一致。

Abstract: Evaluating the factuality of long-form generations from Large Language Models
(LLMs) remains challenging due to accuracy issues and costly human assessment.
Prior efforts attempt this by decomposing text into claims, searching for
evidence, and verifying claims, but suffer from critical drawbacks: (1)
inefficiency due to complex pipeline components unsuitable for long LLM
outputs, and (2) ineffectiveness stemming from inaccurate claim sets and
insufficient evidence collection of one-line snippets.
  To address these limitations, we propose \name, a fast and strong evaluation
framework that achieves the highest alignment with human evaluation and
efficiency among existing baselines. \name first employs chunk-level claim
extraction integrated with confidence-based pre-verification, significantly
reducing the cost of web searching and inference calling while ensuring
reliability. For searching and verification, it collects document-level
evidence from crawled webpages and selectively retrieves it during
verification, addressing the evidence insufficiency problem in previous
pipelines.
  Extensive experiments based on an aggregated and manually annotated benchmark
demonstrate the reliability of \name in both efficiently and effectively
evaluating the factuality of long-form LLM generations. Code and benchmark data
is available at https://github.com/Yingjia-Wan/FastFact.

</details>


### [133] [VLURes: Benchmarking VLM Visual and Linguistic Understanding in Low-Resource Languages](https://arxiv.org/abs/2510.12845)
*Jesse Atuhurra,Iqra Ali,Tomoya Iwakura,Hidetaka Kamigaito,Tatsuya Hiraoka*

Main category: cs.CL

TL;DR: 本文提出了VLURes，一个新颖的多语言基准测试，用于评估视觉语言模型（VLMs）在长文本设置下、四种语言（英语、日语、斯瓦希里语、乌尔都语）中的细粒度视觉和语言理解能力，揭示了模型在不同语言和任务上的性能差异。


<details>
  <summary>Details</summary>
Motivation: 当前的VLM评估主要局限于以英语为中心、文本较短的基准测试，未能充分评估VLM在长文本和多语言环境下的细粒度能力，特别是对于低资源语言。

Method: 引入了VLURes，一个包含八项视觉语言任务和一项开创性的“不相关性”任务的多语言基准。数据集从目标语言的网络资源中收集，涵盖十种多样图像类别和丰富的文本上下文。通过提示VLM生成响应和理由，并由自动系统和母语使用者进行评估，对十个VLM进行了测试。

Result: 研究揭示了VLM在不同语言和任务（如物体识别、场景理解、关系理解）上的性能差异。表现最佳的模型GPT-4o取得了90.8%的总体准确率，但仍落后人类表现6.7%，开源模型的差距更大。

Conclusion: VLURes对于开发能够处理多模态视觉推理的智能代理至关重要，它突显了当前VLM性能与人类水平之间的差距，尤其是在多语言和细粒度理解方面。

Abstract: Vision Language Models (VLMs) are pivotal for advancing perception in
intelligent agents. Yet, evaluation of VLMs remains limited to predominantly
English-centric benchmarks in which the image-text pairs comprise short texts.
To evaluate VLM fine-grained abilities, in four languages under long-text
settings, we introduce a novel multilingual benchmark VLURes featuring eight
vision-and-language tasks, and a pioneering unrelatedness task, to probe the
fine-grained Visual and Linguistic Understanding capabilities of VLMs across
English, Japanese, and low-resource languages, Swahili, and Urdu. Our datasets,
curated from web resources in the target language, encompass ten diverse image
categories and rich textual context, introducing valuable vision-language
resources for Swahili and Urdu. By prompting VLMs to generate responses and
rationales, evaluated automatically and by native speakers, we uncover
performance disparities across languages and tasks critical to intelligent
agents, such as object recognition, scene understanding, and relationship
understanding. We conducted evaluations of ten VLMs with VLURes. The best
performing model, GPT-4o, achieves an overall accuracy of 90.8% and lags human
performance by 6.7%, though the gap is larger for open-source models. The gap
highlights VLURes' critical role in developing intelligent agents to tackle
multi-modal visual reasoning.

</details>


### [134] [Efficient Adaptive Transformer: An Empirical Study and Reproducible Framework](https://arxiv.org/abs/2510.12856)
*Jan Miller*

Main category: cs.CL

TL;DR: EAT框架统一了渐进式token剪枝、稀疏注意力和动态提前退出三种自适应效率技术，形成了一个可复现的输入自适应推理架构，并提供了一个开源基准测试管道。


<details>
  <summary>Details</summary>
Motivation: 研究旨在提高Transformer推理的效率，特别是针对延迟敏感的自然语言处理任务，通过自适应计算来优化性能。

Method: EAT框架将渐进式token剪枝、稀疏注意力、动态提前退出三种自适应效率技术整合到一个单一架构中。此外，它还提供了一个开源的基准测试管道，可自动化数据处理、计时和消融实验。

Result: 实证研究发现，在浅层（六层）模型中，结合这些机制可能会增加延迟。然而，EAT在SST-2任务上实现了比优化过的DistilBERT基线略高的准确性，展示了动态计算在延迟敏感NLP中的潜力。

Conclusion: EAT是一个开放、端到端的可复现框架，包含脚本、CSV日志和分析工具，旨在作为社区工具，促进对自适应Transformer的进一步研究，并证明了动态计算在延迟敏感NLP中的应用前景。

Abstract: The Efficient Adaptive Transformer (EAT) framework unifies three adaptive
efficiency techniques - progressive token pruning, sparse attention, and
dynamic early exiting - into a single, reproducible architecture for
input-adaptive inference. EAT provides an open-source benchmarking pipeline
that automates data processing, timing, and ablation across GLUE tasks (SST-2,
QQP, MNLI). Although this empirical study finds that combining these mechanisms
can increase latency in shallow six-layer models, it demonstrates that EAT
achieves slightly higher accuracy than the optimized DistilBERT baseline on
SST-2, illustrating the potential of dynamic computation for latency-sensitive
NLP. The main contribution is the open, end-to-end reproducible framework -
complete with scripts, CSV logging, and analysis utilities - intended to serve
as a community tool for further research on adaptive transformers.

</details>


### [135] [A Critical Review of the Need for Knowledge-Centric Evaluation of Quranic Recitation](https://arxiv.org/abs/2510.12858)
*Mohammed Hilal Al-Kharusi,Khizar Hayat,Khalil Bader Al Ruqeishi,Haroon Rashid Lone*

Main category: cs.CL

TL;DR: 本文综述了古兰经诵读（Tajweed）自动化评估工具的现状，指出现有基于ASR的方法存在根本性缺陷，并提出应转向以知识为中心、结合语言学和高级音频分析的混合系统。


<details>
  <summary>Details</summary>
Motivation: 古兰经诵读（Tajweed）的教学面临挑战，尽管数字技术提供了教育机会，但现有自动化诵读评估工具未能广泛普及或有效教学。研究旨在探究这一关键差距。

Method: 本文通过对过去二十年间的学术研究、网络平台和商业应用进行全面文献综述，分析了自动化古兰经诵读评估工具的现状。

Result: 分析发现，现有方法普遍重用自动语音识别（ASR）架构，偏重词汇识别而非定性声学评估，且受数据依赖、人口偏见及无法提供诊断性反馈等问题困扰。研究主张从数据驱动范式转向以知识为中心的计算框架。

Conclusion: 未来的自动化古兰经评估应采用混合系统，融合深层语言学知识和先进音频分析，构建基于规范规则和发音部位（Makhraj）的预测声学模型，从而实现稳健、公平且具有教学意义的工具。

Abstract: The sacred practice of Quranic recitation (Tajweed), governed by precise
phonetic, prosodic, and theological rules, faces significant pedagogical
challenges in the modern era. While digital technologies promise unprecedented
access to education, automated tools for recitation evaluation have failed to
achieve widespread adoption or pedagogical efficacy. This literature review
investigates this critical gap, conducting a comprehensive analysis of academic
research, web platforms, and commercial applications developed over the past
two decades. Our synthesis reveals a fundamental misalignment in prevailing
approaches that repurpose Automatic Speech Recognition (ASR) architectures,
which prioritize lexical recognition over qualitative acoustic assessment and
are plagued by data dependency, demographic biases, and an inability to provide
diagnostically useful feedback. Critiquing these data--driven paradigms, we
argue for a foundational paradigm shift towards a knowledge-centric
computational framework. Capitalizing on the immutable nature of the Quranic
text and the precisely defined rules of Tajweed, we propose that a robust
evaluator must be architected around anticipatory acoustic modeling based on
canonical rules and articulation points (Makhraj), rather than relying on
statistical patterns learned from imperfect and biased datasets. This review
concludes that the future of automated Quranic evaluation lies in hybrid
systems that integrate deep linguistic knowledge with advanced audio analysis,
offering a path toward robust, equitable, and pedagogically sound tools that
can faithfully support learners worldwide.

</details>


### [136] [EduDial: Constructing a Large-scale Multi-turn Teacher-Student Dialogue Corpus](https://arxiv.org/abs/2510.12899)
*Shouang Wei,Min Zhang,Xin Lin,Bo Jiang,Zhongxiang Dai,Kun Kuang*

Main category: cs.CL

TL;DR: 本文提出了EduDial，一个基于布鲁姆分类法和多种提问策略构建的综合性多轮师生对话数据集，并在此基础上开发了EduDial-LLM 32B模型和11维评估框架，旨在提升和衡量大型语言模型在智能教育中的教学能力。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）在智能教育领域中被认为是推进智能教育的关键技术，能够深入理解教学情境并提供个性化指导，构建专门的师生对话基准对于评估其会话能力变得尤为重要。

Method: 研究方法包括：1) 构建EduDial数据集，涵盖345个核心知识点和34,250个师生代理互动生成的对话会话，设计遵循布鲁姆教育目标分类法，并融入了情境提问、最近发展区（ZPD）提问和元认知提问等十种提问策略；2) 为不同认知水平的学生设计差异化教学策略；3) 基于EduDial数据集训练并开发EduDial-LLM 32B模型；4) 提出了一个11维的评估框架，系统地衡量LLMs的整体教学质量和内容质量。

Result: 实验结果表明，在17个主流LLMs上的测试显示，大多数模型在以学生为中心的教学场景中表现不佳，而EduDial-LLM取得了显著的进步，在所有指标上始终优于所有基线模型。

Conclusion: EduDial数据集和EduDial-LLM模型有效地提升了LLMs在师生对话中的教学能力，并揭示了现有模型在以学生为中心的教学场景中的不足，为未来智能教育LLMs的发展提供了有价值的基准和方向。

Abstract: Recently, several multi-turn dialogue benchmarks have been proposed to
evaluate the conversational abilities of large language models (LLMs). As LLMs
are increasingly recognized as a key technology for advancing intelligent
education, owing to their ability to deeply understand instructional contexts
and provide personalized guidance, the construction of dedicated
teacher-student dialogue benchmarks has become particularly important. To this
end, we present EduDial, a comprehensive multi-turn teacher-student dialogue
dataset. EduDial covers 345 core knowledge points and consists of 34,250
dialogue sessions generated through interactions between teacher and student
agents. Its design is guided by Bloom's taxonomy of educational objectives and
incorporates ten questioning strategies, including situational questioning,
zone of proximal development (ZPD) questioning, and metacognitive
questioning-thus better capturing authentic classroom interactions.
Furthermore, we design differentiated teaching strategies for students at
different cognitive levels, thereby providing more targeted teaching guidance.
Building on EduDial, we further develop EduDial-LLM 32B via training and
propose an 11-dimensional evaluation framework that systematically measures the
teaching abilities of LLMs, encompassing both overall teaching quality and
content quality. Experiments on 17 mainstream LLMs reveal that most models
struggle in student-centered teaching scenarios, whereas our EduDial-LLM
achieves significant gains, consistently outperforming all baselines across all
metrics. The code is available at
https://github.com/Mind-Lab-ECNU/EduDial/tree/main.

</details>


### [137] [Who's Asking? Evaluating LLM Robustness to Inquiry Personas in Factual Question Answering](https://arxiv.org/abs/2510.12925)
*Nil-Jana Akpinar,Chia-Jung Lee,Vanessa Murdock,Pietro Perona*

Main category: cs.CL

TL;DR: 本研究首次系统评估了大型语言模型（LLMs）对用户查询角色（如身份、专业知识、信仰）的鲁棒性，发现这些角色信息会显著影响LLMs的问答准确性，并导致拒绝回答、幻觉限制和角色混淆等故障模式。


<details>
  <summary>Details</summary>
Motivation: LLMs应基于客观知识真实回答事实性问题，不受用户个人信息或系统个性化设置的影响。然而，此前鲁棒性研究主要集中于对抗性输入，缺乏对真实世界中用户可能披露的“查询角色”的系统评估。

Method: 研究采用系统评估方法，测试LLMs对“查询角色”（即传达身份、专业知识或信仰等属性的用户档案）的鲁棒性。与以往工作不同，本研究关注的是用户在真实交互中可能披露的合理、以人为中心的查询角色线索。

Result: 研究发现，查询角色线索可以显著改变LLMs的问答准确性，并触发多种故障模式，包括拒绝回答、虚构的限制和角色混淆。这些影响表明模型对用户框架的敏感性会损害事实可靠性。

Conclusion: LLMs对用户框架的敏感性会损害其事实可靠性。查询角色测试是评估LLMs鲁棒性的一种有效工具。

Abstract: Large Language Models (LLMs) should answer factual questions truthfully,
grounded in objective knowledge, regardless of user context such as
self-disclosed personal information, or system personalization. In this paper,
we present the first systematic evaluation of LLM robustness to inquiry
personas, i.e. user profiles that convey attributes like identity, expertise,
or belief. While prior work has primarily focused on adversarial inputs or
distractors for robustness testing, we evaluate plausible, human-centered
inquiry persona cues that users disclose in real-world interactions. We find
that such cues can meaningfully alter QA accuracy and trigger failure modes
such as refusals, hallucinated limitations, and role confusion. These effects
highlight how model sensitivity to user framing can compromise factual
reliability, and position inquiry persona testing as an effective tool for
robustness evaluation.

</details>


### [138] [The Curious Case of Curiosity across Human Cultures and LLMs](https://arxiv.org/abs/2510.12943)
*Angana Borah,Rada Mihalcea*

Main category: cs.CL

TL;DR: 本研究调查了大型语言模型（LLMs）中好奇心在文化维度上的差异，发现LLMs倾向于扁平化跨文化多样性，更偏向西方表达方式。论文提出了CUEST评估框架，并证明通过微调策略可显著提升LLMs在好奇心表达上与人类的跨文化对齐度，强调了好奇心对LLM跨文化适应性的重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在人机交互中的作用日益增强，但好奇心——作为探究的核心驱动力——在这些系统中，尤其是在不同文化背景下，尚未得到充分探索。

Method: 研究利用Yahoo! Answers这一真实的跨国数据集来调查好奇心的文化差异。为此，引入了CUEST（CUriosity Evaluation across SocieTies）评估框架，通过语言（风格）和主题偏好（内容）分析来衡量人类与模型在好奇心上的对齐度，并将其根植于社会科学构建。此外，还探索了诱导LLMs产生好奇心的微调策略。

Result: 研究发现，无论是开源还是闭源的LLMs，都倾向于扁平化跨文化多样性，其好奇心的表达方式与西方国家更为接近。通过微调策略来诱导LLMs产生好奇心，可以将人类与模型之间的对齐差距缩小高达50%。

Conclusion: 研究表明，好奇心对于LLMs在不同文化背景下的适应性具有实际价值。提升LLMs的好奇心表达能力对于未来的自然语言处理（NLP）研究至关重要。

Abstract: Recent advances in Large Language Models (LLMs) have expanded their role in
human interaction, yet curiosity -- a central driver of inquiry -- remains
underexplored in these systems, particularly across cultural contexts. In this
work, we investigate cultural variation in curiosity using Yahoo! Answers, a
real-world multi-country dataset spanning diverse topics. We introduce CUEST
(CUriosity Evaluation across SocieTies), an evaluation framework that measures
human-model alignment in curiosity through linguistic (style), topic preference
(content) analysis and grounding insights in social science constructs. Across
open- and closed-source models, we find that LLMs flatten cross-cultural
diversity, aligning more closely with how curiosity is expressed in Western
countries. We then explore fine-tuning strategies to induce curiosity in LLMs,
narrowing the human-model alignment gap by up to 50\%. Finally, we demonstrate
the practical value of curiosity for LLM adaptability across cultures, showing
its importance for future NLP research.

</details>


### [139] [3-Model Speculative Decoding](https://arxiv.org/abs/2510.12966)
*Sanghyun Byun,Mohanad Odema,Jung Ick Guack,Baisub Lee,Jacob Song,Woo Seong Chung*

Main category: cs.CL

TL;DR: PyramidSD通过引入中间模型改进了推测解码（SD），弥合了草稿模型和目标模型之间的差距，从而提高了令牌接受率和推理速度，实现了高达1.91倍的加速。


<details>
  <summary>Details</summary>
Motivation: 推测解码（SD）的吞吐量增益受到草稿模型大小和令牌接受率之间权衡的限制：小草稿模型生成速度快但与目标模型差异大，导致接受率低和加速效果不佳。

Method: PyramidSD在草稿模型和目标模型之间插入了一个中间“限定词模型”（qualifier model），以弥合输出预测中的分布差距。这种分层解码策略结合模糊接受标准，支持在每个阶段放宽发散阈值，从而改善模型间的对齐，允许使用更小的草稿模型并提高吞吐量。

Result: PyramidSD相比标准SD实现了高达1.91倍的生成速度提升，在消费级GPU（RTX 4090）上达到每秒124个令牌。在小内存设置下（1B参数草稿模型和8B目标模型），PyramidSD在不牺牲目标模型质量的情况下显著提高了吞吐量。

Conclusion: PyramidSD提供了一种增强推测解码效率的实用方法，可以轻松应用于现有的推理流程，特别是在需要使用更小草稿模型以提高效率的场景中。

Abstract: Speculative Decoding (SD) accelerates inference in large language models by
using a smaller draft model to propose tokens, which are then verified by a
larger target model. However, the throughput gains of SD are fundamentally
limited by a trade-off between draft model size and token acceptance: smaller
draft models generate tokens more quickly but exhibit greater divergence from
the target model, resulting in lower acceptance rates and reduced speedups. We
introduce Pyramid Speculative Decoding (PyramidSD), an extension of SD that
inserts an intermediate qualifier model between the draft and target to bridge
the distributional gap in output predictions, allowing smaller model to be used
for drafting. This hierarchical decoding strategy improves alignment across
models, enabling higher acceptance rates and allowing the use of significantly
smaller draft models without sacrificing overall performance. PyramidSD builds
on fuzzy acceptance criteria to support relaxed divergence thresholds at each
stage, improving throughput. In experiments, PyramidSD achieves up to 1.91x
generation speed over standard SD, reaching 124 tokens per second on a consumer
GPU (RTX 4090). In small-memory settings with a 1B-parameter draft model and an
8B target model, PyramidSD minimally trades target model quality for improved
throughput. Overall, PyramidSD offers a practical approach to enhancing
speculative decoding efficiency and can be readily applied to existing
inference pipelines.

</details>


### [140] [A Multilingual, Large-Scale Study of the Interplay between LLM Safeguards, Personalisation, and Disinformation](https://arxiv.org/abs/2510.12993)
*João A. Leite,Arnav Arora,Silvia Gargova,João Luz,Gustavo Sampaio,Ian Roberts,Carolina Scarton,Kalina Bontcheva*

Main category: cs.CL

TL;DR: 本研究首次大规模、多语言地实证评估了大型语言模型（LLMs）生成针对特定人群的个性化虚假信息的能力，发现即使简单的个性化策略也会显著增加越狱率并增强虚假信息的说服力。


<details>
  <summary>Details</summary>
Motivation: LLMs类人般的熟练程度引发了人们对其被滥用于大规模生成具有说服力的个性化虚假信息的担忧。尽管已有研究表明LLMs可以生成虚假信息，但关于说服力和个性化（根据特定人口统计属性定制虚假信息）的具体问题仍未得到充分研究。

Method: 本研究采用了红队（red teaming）方法，系统评估了LLM安全机制对针对特定人群提示的鲁棒性。研究创建了AI-TRAITS数据集，包含约160万条由八个最先进LLMs生成的文本。这些文本基于结合了324个虚假信息叙事和150个不同人群画像的提示，涵盖四种主要语言（英语、俄语、葡萄牙语、印地语）和关键人口统计维度（国家、代际、政治倾向）。研究随后对生成的个性化叙事进行了定量评估，并从模型、语言、越狱率和个性化属性等方面进行了比较。

Result: 研究发现，即使在提示中使用简单的个性化策略，也会显著增加所有受研究LLMs的越狱可能性。此外，个性化提示会导致语言和修辞模式的改变，并增强LLM生成的虚假叙事的说服力。

Conclusion: 这些发现揭示了当前最先进LLMs的关键漏洞，并为改进多语言和跨人口统计背景下的安全对齐和检测策略奠定了基础。

Abstract: The human-like proficiency of Large Language Models (LLMs) has brought
concerns about their potential misuse for generating persuasive and
personalised disinformation at scale. While prior work has demonstrated that
LLMs can generate disinformation, specific questions around persuasiveness and
personalisation (generation of disinformation tailored to specific demographic
attributes) remain largely unstudied. This paper presents the first
large-scale, multilingual empirical study on persona-targeted disinformation
generation by LLMs. Employing a red teaming methodology, we systematically
evaluate the robustness of LLM safety mechanisms to persona-targeted prompts. A
key novel result is AI-TRAITS (AI-generaTed peRsonAlIsed disinformaTion
dataSet), a new dataset of around 1.6 million texts generated by eight
state-of-the-art LLMs. AI-TRAITS is seeded by prompts that combine 324
disinformation narratives and 150 distinct persona profiles, covering four
major languages (English, Russian, Portuguese, Hindi) and key demographic
dimensions (country, generation, political orientation). The resulting
personalised narratives are then assessed quantitatively and compared along the
dimensions of models, languages, jailbreaking rate, and personalisation
attributes. Our findings demonstrate that the use of even simple
personalisation strategies in the prompts significantly increases the
likelihood of jailbreaks for all studied LLMs. Furthermore, personalised
prompts result in altered linguistic and rhetorical patterns and amplify the
persuasiveness of the LLM-generated false narratives. These insights expose
critical vulnerabilities in current state-of-the-art LLMs and offer a
foundation for improving safety alignment and detection strategies in
multilingual and cross-demographic contexts.

</details>


### [141] [OPLoRA: Orthogonal Projection LoRA Prevents Catastrophic Forgetting during Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2510.13003)
*Yifeng Xiong,Xiaohui Xie*

Main category: cs.CL

TL;DR: LoRA在微调大型语言模型时存在灾难性遗忘问题，因为它会干扰编码预训练知识的主导奇异方向。本文提出了OPLoRA，通过双边正交投影将LoRA更新限制在主导奇异子空间的补空间内，从而防止这种干扰，有效减少遗忘并保持性能。


<details>
  <summary>Details</summary>
Motivation: 低秩适应（LoRA）在微调大型语言模型时会遭遇灾难性遗忘，原因是其学习到的更新会干扰编码了基本预训练知识的主导奇异方向。

Method: 本文提出了正交投影LoRA (OPLoRA)。该方法通过奇异值分解（SVD）分解冻结权重，并利用双边正交投影 $P_L = I - U_k U_k^	op$ 和 $P_R = I - V_k V_k^	op$ 将LoRA更新限制在顶部 $k$ 奇异子空间的正交补空间内。此外，引入了 $ho_k$ 指标来量化子空间干扰。

Result: OPLoRA在理论上被证明能够精确保留顶部 $k$ 奇异三元组，为知识保留提供了数学保证。在常识推理、数学和代码生成任务上，对LLaMA-2 7B和Qwen2.5 7B模型进行的广泛实验表明，OPLoRA显著减少了遗忘，同时保持了有竞争力的任务特定性能。

Conclusion: 正交投影是一种在参数高效微调中有效保留知识的机制，OPLoRA的成功验证了这一点。

Abstract: Low-Rank Adaptation (LoRA) enables efficient fine-tuning of large language
models but suffers from catastrophic forgetting when learned updates interfere
with the dominant singular directions that encode essential pre-trained
knowledge. We propose Orthogonal Projection LoRA (OPLoRA), a theoretically
grounded approach that prevents this interference through double-sided
orthogonal projections. By decomposing frozen weights via SVD, OPLoRA
constrains LoRA updates to lie entirely within the orthogonal complement of the
top-$k$ singular subspace using projections $P_L = I - U_k U_k^\top$ and $P_R =
I - V_k V_k^\top$. We prove that this construction exactly preserves the
top-$k$ singular triples, providing mathematical guarantees for knowledge
retention. To quantify subspace interference, we introduce $\rho_k$, a metric
measuring update alignment with dominant directions. Extensive experiments
across commonsense reasoning, mathematics, and code generation demonstrate that
OPLoRA significantly reduces forgetting while maintaining competitive
task-specific performance on LLaMA-2 7B and Qwen2.5 7B, establishing orthogonal
projection as an effective mechanism for knowledge preservation in
parameter-efficient fine-tuning.

</details>


### [142] [CurLL: A Developmental Framework to Evaluate Continual Learning in Language Models](https://arxiv.org/abs/2510.13008)
*Pavan Kalyan,Shubhra Mishra,Satya Lokam,Navin Goyal*

Main category: cs.CL

TL;DR: 本文介绍了一个名为CurlL的综合性持续学习数据集和基准，它基于人类5-10岁的发展轨迹，旨在系统且细致地评估模型逐步获取新技能的能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法可能缺乏对模型逐步获取新技能进行系统且细致评估的能力，尤其是在模拟人类发展轨迹方面。本研究旨在通过提供一个与人类学习模式对齐并能精细控制技能依赖性的基准来解决这一问题。

Method: 研究引入了CurlL数据集和基准，它涵盖了人类5-10岁的五个发展阶段，并由一个将广泛技能分解为更小能力、具体目标和可衡量指标的技能图谱支持。研究生成了一个23.4B token的合成数据集，该数据集控制了技能进展、词汇复杂度和格式多样性（包括段落、理解型问答、技能测试型问答和指令-响应对）。然后，使用一个135M参数的Transformer模型在独立、联合和顺序（持续）设置下进行训练和评估。

Result: 通过使用CurlL基准，研究展示了在技能保留和迁移效率方面的权衡。

Conclusion: 通过模拟人类学习模式并提供对技能依赖性的精细控制，这项工作推进了语言模型持续学习的评估方法。

Abstract: We introduce a comprehensive continual learning dataset and benchmark (CurlL)
grounded in human developmental trajectories from ages 5-10, enabling
systematic and fine-grained assessment of models' ability to progressively
acquire new skills. CurlL spans five developmental stages (0-4) covering ages
5-10, supported by a skill graph that breaks down broad skills into smaller
abilities, concrete goals, and measurable indicators, while also capturing
which abilities build on others. We generate a 23.4B-token synthetic dataset
with controlled skill progression, vocabulary complexity, and format diversity,
comprising paragraphs, comprehension-based QA (CQA), skill-testing QA (CSQA),
and instruction-response (IR) pairs. Stage-wise token counts range from 2.12B
to 6.78B tokens, supporting precise analysis of forgetting, forward transfer,
and backward transfer. Using a 135M-parameter transformer trained under
independent, joint, and sequential (continual) setups, we show trade-offs in
skill retention and transfer efficiency. By mirroring human learning patterns
and providing fine-grained control over skill dependencies, this work advances
continual learning evaluations for language models.

</details>


### [143] [GatePro: Parameter-Free Expert Selection Optimization for Mixture-of-Experts Models](https://arxiv.org/abs/2510.13079)
*Chen Zheng,Yuhang Cai,Deyi Liu,Jin Ma,Yiyuan Ma,Yuan Yang,Jing Liu,Yutao Zeng,Xun Zhou,Siyuan Qiao*

Main category: cs.CL

TL;DR: GatePro是一种新颖的、无参数的方法，通过引入局部竞争机制，解决了MoE架构中功能相似专家同时被选择导致的冗余计算问题，从而提高了专家选择的多样性和模型效率。


<details>
  <summary>Details</summary>
Motivation: 现代大型语言模型中的MoE架构面临一个关键挑战：功能相似的专家经常同时被选中，导致计算冗余并限制了模型的有效容量。现有辅助平衡损失方法改善了token分布，但未能解决潜在的专家多样性问题。

Method: 本文提出了GatePro，一种新颖的无参数方法，直接促进专家选择的多样性。GatePro识别最相似的专家对，并引入局部竞争机制，阻止冗余专家同时激活，同时保持自然的专家专业化。

Result: 全面的评估表明GatePro在不同模型规模和基准测试中都有效。分析表明，GatePro能够增强专家多样性，使专家发展出更独特和互补的能力，避免功能冗余。

Conclusion: GatePro可以在任何训练阶段热插拔部署，无需额外可学习参数，为提高MoE的有效性提供了一个实用的解决方案，通过促进专家多样性来避免功能冗余和提高计算效率。

Abstract: Modern large language models leverage Mixture-of-Experts (MoE) architectures
for efficient scaling, but face a critical challenge: functionally similar
experts are often selected simultaneously, creating redundant computation and
limiting effective model capacity. Existing auxiliary balance loss methods
improve token distribution but fail to address the underlying expert diversity
problem. We introduce GatePro, a novel parameter-free method that directly
promotes expert selection diversity. GatePro identifies the most similar expert
pairs and introduces localized competition mechanisms, preventing redundant
expert co-activation while maintaining natural expert specialization. Our
comprehensive evaluation demonstrates GatePro's effectiveness across model
scales and benchmarks. Analysis demonstrates GatePro's ability to achieve
enhanced expert diversity, where experts develop more distinct and
complementary capabilities, avoiding functional redundancy. This approach can
be deployed hot-swappable during any training phase without additional
learnable parameters, offering a practical solution for improving MoE
effectiveness.

</details>


### [144] [On the Role of Preference Variance in Preference Optimization](https://arxiv.org/abs/2510.13022)
*Jiacheng Guo,Zihao Li,Jiahao Qiu,Yue Wu,Mengdi Wang*

Main category: cs.CL

TL;DR: 本研究发现，在DPO训练中，偏好方差（PVar）高的提示词能产生更大的梯度更新，对LLM对齐更有价值。选择高PVar的提示词可以显著提高训练效率和模型性能。


<details>
  <summary>Details</summary>
Motivation: 收集人类偏好数据成本高昂且效率低下，因此需要减少所需的标注。本研究旨在探究DPO训练中“偏好方差”对模型有效性的影响。

Method: 本研究首先从理论上分析了DPO梯度范数的上限，证明其受偏好方差控制。随后，通过使用奖励模型生成的偏好数据微调大型语言模型，并在AlpacaEval 2.0和Arena-Hard两个基准上进行评估，验证了理论发现。此外，还使用UltraFeedback数据集的人类标注数据进行了独立实验，仅使用最高偏好方差的提示词进行训练。

Result: 理论分析表明，低偏好方差的提示词只能产生较小的梯度更新，学习价值较低。实验结果证实，高偏好方差的提示词训练效果优于随机选择或低偏好方差的提示词。即使使用较小的奖励模型进行选择，基于偏好方差的选择方法也表现出鲁棒性。值得注意的是，在使用UltraFeedback数据集时，仅使用最高10%偏好方差的提示词进行训练，其评估性能优于使用完整数据集。

Conclusion: 偏好方差是识别DPO训练中信息量大的示例的关键指标。通过选择具有高偏好方差的提示词，可以显著提高大型语言模型对齐的效率和性能。

Abstract: Direct Preference Optimization (DPO) has emerged as an important approach for
learning from human preferences in aligning large language models (LLMs).
However, collecting human preference data is costly and inefficient, motivating
methods to reduce the required annotations. In this work, we investigate the
impact of \emph{preference variance} (PVar), which measures the variance in
model preferences when comparing pairs of responses, on the effectiveness of
DPO training. We provide a theoretical insight by establishing an upper bound
on the DPO gradient norm for any given prompt, showing it is controlled by the
PVar of that prompt. This implies that prompts with low PVar can only produce
small gradient updates, making them less valuable for learning. We validate
this finding by fine-tuning LLMs with preferences generated by a reward model,
evaluating on two benchmarks (AlpacaEval 2.0 and Arena-Hard). Experimental
results demonstrate that prompts with higher PVar outperform randomly selected
prompts or those with lower PVar. We also show that our PVar-based selection
method is robust, when using smaller reward models (1B, 3B) for selection.
Notably, in a separate experiment using the original human annotations from the
UltraFeedback dataset, we found that training on only the top 10\% of prompts
with the highest PVar yields better evaluation performance than training on the
full dataset, highlighting the importance of preference variance in identifying
informative examples for efficient LLM alignment.

</details>


### [145] [Stable LLM Ensemble: Interaction between Example Representativeness and Diversity](https://arxiv.org/abs/2510.13143)
*Junichiro Niimi*

Main category: cs.CL

TL;DR: 本研究通过结合代表性示例选择和提高采样温度，显著提升了单次（one-shot）大型语言模型（LLM）集成预测的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的单次预测准确性和鲁棒性对所选示例以及集成成员之间的多样性高度敏感，这促使研究者探索如何系统地优化这些因素。

Method: 研究系统地调查了示例代表性（通过中心点代表性示例与随机抽样示例两种单次策略对比）和输出多样性（通过调整采样温度）对LLM集成性能的影响。

Result: 提出的结合中心点代表性示例和较高温度设置的方法，相比随机选择，宏F1提高了7.6%，RMSE降低了10.5%。此外，该模型甚至超越了五次（5-shot）提示，宏F1提高了21.1%，RMSE降低了24.0%。

Conclusion: 研究结果表明，将代表性示例选择与增加的采样温度相结合，能为集成提供恰当的多样性水平，这强调了在设计有效的单次LLM集成时，示例选择和受控多样性的实际重要性。

Abstract: Large language models (LLMs) have achieved remarkable results in wide range
of domains. However, the accuracy and robustness of one-shot LLM predictions
remain highly sensitive to the examples and the diversity among ensemble
members. This study systematically investigates the effects of example
representativeness (one-shot strategy) and output diversity (sampling
temperature) on LLM ensemble performance. Two one-shot strategies are compared:
centroid-based representative examples (proposed) and randomly sampled examples
(baseline) and sampling temperature also is varied. The proposed approach with
higher temperature setting significantly outperforms random selection by +7.6%
(macro-F1) and -10.5% (RMSE). Furthermore, the proposed model exceeds 5-shot
prompting by +21.1% (macro-F1) and -24.0% (RMSE). Our findings demonstrate that
combining representative example selection with increased temperature provides
the appropriate level of diversity to the ensemble. This work highlights the
practical importance of both example selection and controlled diversity in
designing effective one-shot LLM ensembles.

</details>


### [146] [ESI: Epistemic Uncertainty Quantification via Semantic-preserving Intervention for Large Language Models](https://arxiv.org/abs/2510.13103)
*Mingda Li,Xinyu Li,Weinan Zhang,Longxuan Ma*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的灰盒不确定性量化方法，通过测量大型语言模型（LLMs）在语义保持干预前后的输出变化，有效且高效地估计LLMs的认知不确定性。


<details>
  <summary>Details</summary>
Motivation: 尽管不确定性量化（UQ）是提高模型可靠性的有效方法，但量化大型语言模型（LLMs）的不确定性却非常困难。

Method: 该研究从因果角度将LLMs的不确定性与其在语义保持干预下的不变性联系起来。在此基础上，提出了一种灰盒不确定性量化方法，通过测量模型在语义保持干预前后的输出变化来量化不确定性。

Result: 理论上，该方法能有效估计认知不确定性。在各种LLMs和问答数据集上的广泛实验表明，该方法不仅在有效性方面表现出色，而且在计算效率方面也具有优势。

Conclusion: 研究成功地建立LLMs不确定性与语义保持干预下不变性之间的联系，并提出了一种有效且计算高效的灰盒不确定性量化方法，为LLMs的可靠性提升提供了新途径。

Abstract: Uncertainty Quantification (UQ) is a promising approach to improve model
reliability, yet quantifying the uncertainty of Large Language Models (LLMs) is
non-trivial. In this work, we establish a connection between the uncertainty of
LLMs and their invariance under semantic-preserving intervention from a causal
perspective. Building on this foundation, we propose a novel grey-box
uncertainty quantification method that measures the variation in model outputs
before and after the semantic-preserving intervention. Through theoretical
justification, we show that our method provides an effective estimate of
epistemic uncertainty. Our extensive experiments, conducted across various LLMs
and a variety of question-answering (QA) datasets, demonstrate that our method
excels not only in terms of effectiveness but also in computational efficiency.

</details>


### [147] [Multi-Label Clinical Text Eligibility Classification and Summarization System](https://arxiv.org/abs/2510.13115)
*Surya Tejaswi Yerramsetty,Almas Fathimah*

Main category: cs.CL

TL;DR: 本文提出一个利用自然语言处理（NLP）和大型语言模型（LLMs）的系统，旨在自动化临床文本的资格多标签分类和总结，以提高临床试验的效率。


<details>
  <summary>Details</summary>
Motivation: 临床试验对医学进步至关重要，它们有助于发现疾病检测、预防和治疗的新方法。确保临床试验包含具有适当和多样化医学背景的参与者是关键，因此需要自动化资格评估以提高研究效率。

Method: 该系统结合了多种特征提取方法，包括词嵌入（Word2Vec）和命名实体识别（NER）来识别相关医学概念，以及传统的向量化技术如计数向量化和TF-IDF。文章还探索了加权TF-IDF词嵌入。多标签分类使用随机森林（Random Forest）和支持向量机（SVM）模型。总结技术评估了TextRank、Luhn和GPT-3。

Result: 通过ROUGE分数评估，证明了所提出方法的有效性。

Conclusion: 该系统展示了利用数据驱动方法自动化临床试验资格评估的潜力，从而有望提高研究效率。

Abstract: Clinical trials are central to medical progress because they help improve
understanding of human health and the healthcare system. They play a key role
in discovering new ways to detect, prevent, or treat diseases, and it is
essential that clinical trials include participants with appropriate and
diverse medical backgrounds. In this paper, we propose a system that leverages
Natural Language Processing (NLP) and Large Language Models (LLMs) to automate
multi-label clinical text eligibility classification and summarization. The
system combines feature extraction methods such as word embeddings (Word2Vec)
and named entity recognition to identify relevant medical concepts, along with
traditional vectorization techniques such as count vectorization and TF-IDF
(Term Frequency-Inverse Document Frequency). We further explore weighted TF-IDF
word embeddings that integrate both count-based and embedding-based strengths
to capture term importance effectively. Multi-label classification using Random
Forest and SVM models is applied to categorize documents based on eligibility
criteria. Summarization techniques including TextRank, Luhn, and GPT-3 are
evaluated to concisely summarize eligibility requirements. Evaluation with
ROUGE scores demonstrates the effectiveness of the proposed methods. This
system shows potential for automating clinical trial eligibility assessment
using data-driven approaches, thereby improving research efficiency.

</details>


### [148] [StressTransfer: Stress-Aware Speech-to-Speech Translation with Emphasis Preservation](https://arxiv.org/abs/2510.13194)
*Xi Chen,Yuchen Song,Satoshi Nakamura*

Main category: cs.CL

TL;DR: 本文提出了一种压力感知（stress-aware）的语音到语音翻译（S2ST）系统，该系统利用大型语言模型（LLMs）进行跨语言重音转换，以保留词级别的强调。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于S2ST系统中保留词级别强调的重要性，以及在缺乏此类对齐数据的情况下，如何有效实现跨语言的重音转换和强调保留。

Method: 该方法将源语言的重音转换为目标语言的标签，以指导可控的文本到语音（TTS）模型。为解决数据稀缺问题，研究者开发了一个自动生成对齐训练数据的管道，并引入“LLM-as-Judge”进行评估。

Result: 实验结果表明，该方法在保留强调方面显著优于基线系统，同时保持了可比的翻译质量、说话者意图和自然度。

Conclusion: 该工作强调了韵律在翻译中的重要性，并为S2ST中保留副语言线索提供了一种有效且数据高效的解决方案。

Abstract: We propose a stress-aware speech-to-speech translation (S2ST) system that
preserves word-level emphasis by leveraging LLMs for cross-lingual emphasis
conversion. Our method translates source-language stress into target-language
tags that guide a controllable TTS model. To overcome data scarcity, we
developed a pipeline to automatically generate aligned training data and
introduce the "LLM-as-Judge" for evaluation. Experiments show our approach
substantially outperforms baselines in preserving emphasis while maintaining
comparable translation quality, speaker intent, and naturalness. Our work
highlights the importance of prosody in translation and provides an effective,
data-efficient solution for preserving paralinguistic cues in S2ST.

</details>


### [149] [I Am Aligned, But With Whom? MENA Values Benchmark for Evaluating Cultural Alignment and Multilingual Bias in LLMs](https://arxiv.org/abs/2510.13154)
*Pardis Sadat Zahraei,Ehsaneddin Asgari*

Main category: cs.CL

TL;DR: 本文介绍了MENAValues，一个评估大型语言模型（LLMs）对中东和北非（MENA）地区文化价值观对齐和多语言偏见的基准，揭示了跨语言价值观转变、推理导致的对齐恶化以及敏感问题中的隐含偏好等现象。


<details>
  <summary>Details</summary>
Motivation: 当前的AI评估工作对中东和北非（MENA）地区代表不足，导致LLMs在该区域的文化对齐和多语言偏差问题尚未得到充分评估。

Method: 研究引入了MENAValues基准，从大规模、权威的人类调查中整理了一个结构化数据集，涵盖16个MENA国家的人口层面响应分布。通过结合三种视角框架（中立、个性化和第三方/文化观察者）和两种语言模式（英语和本地语言：阿拉伯语、波斯语、土耳其语）来评估不同的LLMs。

Result: 分析揭示了三个关键现象：1) “跨语言价值观转变”，即相同问题在不同语言下产生显著不同的回答；2) “推理导致的对齐恶化”，即提示模型解释其推理会降低文化对齐性；3) “Logit泄露”，即模型拒绝敏感问题，但内部概率揭示了强烈的隐藏偏好。此外，模型在本地语言操作时会将多样化的国家简化为单一的语言类别。

Conclusion: MENAValues提供了一个可扩展的框架，用于诊断文化错位，并为开发更具文化包容性的AI提供了实证洞察和方法论工具。

Abstract: We introduce MENAValues, a novel benchmark designed to evaluate the cultural
alignment and multilingual biases of large language models (LLMs) with respect
to the beliefs and values of the Middle East and North Africa (MENA) region, an
underrepresented area in current AI evaluation efforts. Drawing from
large-scale, authoritative human surveys, we curate a structured dataset that
captures the sociocultural landscape of MENA with population-level response
distributions from 16 countries. To probe LLM behavior, we evaluate diverse
models across multiple conditions formed by crossing three perspective framings
(neutral, personalized, and third-person/cultural observer) with two language
modes (English and localized native languages: Arabic, Persian, Turkish). Our
analysis reveals three critical phenomena: "Cross-Lingual Value Shifts" where
identical questions yield drastically different responses based on language,
"Reasoning-Induced Degradation" where prompting models to explain their
reasoning worsens cultural alignment, and "Logit Leakage" where models refuse
sensitive questions while internal probabilities reveal strong hidden
preferences. We further demonstrate that models collapse into simplistic
linguistic categories when operating in native languages, treating diverse
nations as monolithic entities. MENAValues offers a scalable framework for
diagnosing cultural misalignment, providing both empirical insights and
methodological tools for developing more culturally inclusive AI.

</details>


### [150] [LLM-Guided Synthetic Augmentation (LGSA) for Mitigating Bias in AI Systems](https://arxiv.org/abs/2510.13202)
*Sai Suhruth Reddy Karri,Yashwanth Sai Nallapuneni,Laxmi Narasimha Reddy Mallireddy,Gopichand G*

Main category: cs.CL

TL;DR: 本文提出了一种名为LLM引导合成增强（LGSA）的方法，利用大型语言模型为代表性不足的群体生成反事实示例，以减轻AI系统中的偏差。实验结果表明，LGSA在不牺牲准确性的前提下显著减少了性能差异，甚至提高了整体准确性。


<details>
  <summary>Details</summary>
Motivation: AI系统，特别是依赖自然语言数据的系统，存在偏差问题，导致不同人口群体之间性能不均。传统公平性方法依赖受保护属性标签、涉及准确性-公平性权衡且泛化能力有限，因此需要新的解决方案。

Method: 本文提出LGSA方法，利用大型语言模型生成代表性不足群体的反事实示例，同时保持标签完整性。具体做法是使用结构化提示词生成性别互换的释义，并进行质量控制，包括语义相似性检查、属性验证、毒性筛选和人工抽查。增强后的数据集用于训练分类器。

Result: LGSA在不损害准确性的情况下减少了性能差异。基线模型准确率为96.7%，性别偏差差距为7.2%。简单互换增强将差距降至0.7%，但准确率降至95.6%。LGSA实现了99.1%的准确率，偏差差距为1.9%，并显著改善了针对女性标签示例的性能。

Conclusion: LGSA是一种有效的偏差缓解策略，能够在保持高任务准确性和标签保真度的同时，增强子群体的平衡性。

Abstract: Bias in AI systems, especially those relying on natural language data, raises
ethical and practical concerns. Underrepresentation of certain groups often
leads to uneven performance across demographics. Traditional fairness methods,
such as pre-processing, in-processing, and post-processing, depend on
protected-attribute labels, involve accuracy-fairness trade-offs, and may not
generalize across datasets. To address these challenges, we propose LLM-Guided
Synthetic Augmentation (LGSA), which uses large language models to generate
counterfactual examples for underrepresented groups while preserving label
integrity. We evaluated LGSA on a controlled dataset of short English sentences
with gendered pronouns, professions, and binary classification labels.
Structured prompts were used to produce gender-swapped paraphrases, followed by
quality control including semantic similarity checks, attribute verification,
toxicity screening, and human spot checks. The augmented dataset expanded
training coverage and was used to train a classifier under consistent
conditions. Results show that LGSA reduces performance disparities without
compromising accuracy. The baseline model achieved 96.7 percent accuracy with a
7.2 percent gender bias gap. Simple swap augmentation reduced the gap to 0.7
percent but lowered accuracy to 95.6 percent. LGSA achieved 99.1 percent
accuracy with a 1.9 percent bias gap, improving performance on female-labeled
examples. These findings demonstrate that LGSA is an effective strategy for
bias mitigation, enhancing subgroup balance while maintaining high task
accuracy and label fidelity.

</details>


### [151] [A Matter of Representation: Towards Graph-Based Abstract Code Generation](https://arxiv.org/abs/2510.13163)
*Nyx Iskandar,Hisham Bedri,Andy Tsen*

Main category: cs.CL

TL;DR: 本研究提出并评估了用于图基抽象代码生成（如可视化编程语言）的JSON表示方法，发现大型语言模型（LLMs）在给定正确表示的情况下，能以单次通过方式高效完成此任务，并强调了表示形式对准确性的关键影响。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs擅长生成原始、顺序代码，但在图基抽象代码生成方面研究较少。图基抽象代码在可视化编程语言以及原始源代码对用户或LLM训练集不可访问的场景中具有重要意义。

Method: 研究提出并评估了用于图的JSON表示形式，以实现高精度的图基抽象代码生成。他们在ScratchTest（一个基于Scratch的自定义Python重新实现）基准上进行了评估，该基准用于测试LLM在代码图空间中的能力。

Result: 研究发现，在给定正确的图表示形式下，LLMs无需依赖专业或复杂的管道即可单次完成图基抽象代码生成任务。此外，不同的表示形式会导致显著不同的准确性，这突出了表示形式在此生成任务中的关键作用。

Conclusion: 这项工作为图基抽象代码生成的表示学习迈出了第一步，证明了LLMs在正确表示下能够有效执行此任务，并强调了表示形式设计的重要性。

Abstract: Most large language models (LLMs) today excel at generating raw, sequential
code with minimal abstractions and custom structures. However, there has been
little work on graph-based abstract code generation, where significant logic is
encapsulated in predefined nodes and execution flow is determined by edges.
This is relevant for visual programming languages, and in cases where raw
source code is inaccessible to users and LLM training sets. In this work, we
propose and evaluate JSON representations for graphs to enable high accuracy
graph-based abstract code generation. We evaluate these representations on
ScratchTest, a mini-benchmark based on our custom Python re-implementation of
Scratch, which tests the LLM in code graph space. Our findings demonstrate that
LLMs can indeed perform the aforementioned generation task in a single pass
without relying on specialized or complex pipelines, given the correct graph
representations. We also show that different representations induce
significantly different accuracies, highlighting the instrumental role of
representations in this generation task. All in all, this work establishes the
first steps towards representation learning for graph-based abstract code
generation.

</details>


### [152] [CoT-Evo: Evolutionary Distillation of Chain-of-Thought for Scientific Reasoning](https://arxiv.org/abs/2510.13166)
*Kehua Feng,Keyan Ding,Zhihui Zhu,Lei Liang,Qiang Zhang,Huajun Chen*

Main category: cs.CL

TL;DR: CoT-Evo是一种进化式思维链蒸馏框架，通过结合领域知识、新颖性选择、重组和变异来迭代优化大型语言模型（LLMs）生成的推理轨迹，从而在科学领域合成高质量的推理数据，并使小型模型达到最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 在科学领域，由于复杂性和专业知识要求高，即使是先进的LLMs也常产生不正确或肤浅的推理，导致现有的思维链（CoT）蒸馏方法效果不佳，生成低质量训练数据，限制了小型学生模型的性能。

Method: CoT-Evo框架首先从多个LLM生成器构建多样化的推理轨迹池，然后自动检索领域知识进行丰富，并通过新颖性驱动选择、反思性重组和变异进行迭代优化。优化过程由评估答案正确性、连贯性和知识有效利用的适应度函数指导。最终生成高质量的科学推理CoT数据集，并用此数据集微调紧凑模型。

Result: 使用CoT-Evo生成的进化数据集微调的紧凑模型在科学推理基准测试上取得了最先进的性能。

Conclusion: CoT-Evo建立了一种可扩展的方法，能够从多样化且可能犯错的LLMs中合成高保真度的科学推理数据。

Abstract: While chain-of-thought (CoT) distillation from advanced large language models
(LLMs) has proven effective in general reasoning tasks, it struggles in
scientific domains where even advanced models often produce incorrect or
superficial reasoning due to high complexity and specialized knowledge
requirements. Directly distilling from such flawed outputs results in
low-quality training data and limits the performance of smaller student models.
To overcome this, we propose CoT-Evo, an evolutionary CoT distillation
framework. It begins by constructing a diverse pool of reasoning trajectories
from multiple LLM thinkers, enriches them with automatically retrieved domain
knowledge, and iteratively refines the trajectories using novelty-driven
selection, reflective recombination and mutation. The refinement is guided by a
fitness function that evaluates answer correctness, coherence, and effective
knowledge utilization. This results in a high-quality CoT dataset tailored for
scientific reasoning. We employ this evolved dataset to fine-tune a compact
model, which achieves state-of-the-art performance on scientific reasoning
benchmarks. Our work establishes a scalable approach to synthesizing
high-fidelity scientific reasoning data from diverse and fallible LLMs.

</details>


### [153] [Mirror Speculative Decoding: Breaking the Serial Barrier in LLM Inference](https://arxiv.org/abs/2510.13161)
*Nikhil Bhendawade,Kumari Nishu,Arnav Kundu,Chris Bartels,Minsik Cho,Irina Belousova*

Main category: cs.CL

TL;DR: Mirror-SD通过并行异构执行和多令牌推测流式传输，克服了推测解码中速度-准确性的权衡，显著加速了大型语言模型（LLM）的推理过程，实现了高达5.8倍的加速。


<details>
  <summary>Details</summary>
Motivation: 推测解码（Speculative decoding）通过使用草稿模型进行预判来加速LLM推理，但其收益受限于自回归草稿生成的高成本，导致速度-准确性权衡。现有方法（如Medusa、Hydra、EAGLE）虽部分降低了草稿成本，但会降低接受率或引入开销，限制了其扩展性。

Method: Mirror-SD算法从早期退出信号启动分支完整推演，并与目标模型的后缀并行。它将计算明确映射到异构加速器（GPU和NPU）以利用跨设备并行性。草稿模型推测后续内容供目标模型验证，同时目标模型推测草稿模型的修正路径，形成了两个互补的执行流水线。为进一步降低草稿延迟而不削弱接受语义，Mirror-SD还增加了推测流式传输，使草稿模型每步可以发出多个令牌。

Result: 在SpecBench上，使用14B至66B参数的服务级模型，Mirror-SD在各种任务上实现了2.8倍至5.8倍的实际运行时间加速，并且比最强的基线EAGLE3平均相对提升了30%。

Conclusion: Mirror-SD通过结合并行异构执行和多令牌推测流式传输的双重策略，成功打破了推测解码中的延迟-接受率权衡，使其达到了高接受率和低开销的理想状态。

Abstract: Speculative decoding accelerates LLM inference by using a draft model to look
ahead, but gains are capped by the cost of autoregressive draft generation:
increasing draft size elevates acceptance rates but introduces additional
latency overhead exacerbating the speed-accuracy tradeoff. Prior methods
(Medusa, Hydra, EAGLE) partially reduce draft cost but either degrade
acceptance or introduce overheads that limit scaling. We present Mirror
Speculative Decoding (Mirror-SD), an inference algorithm that breaks the
latency-acceptance tradeoff. Mirror-SD launches branch-complete rollouts from
early-exit signals in parallel with the target model's suffix and explicitly
maps computation across heterogeneous accelerators (GPU and NPU) to exploit
cross-device parallelism. The draft speculates forward continuations for the
target to verify, while the target simultaneously speculates correction paths
for the draft, converting speculation into two complementary execution
pipelines. To further cut draft latency without weakening acceptance semantics,
we add speculative streaming so the draft emits multiple tokens per step. This
dual strategy of parallel heterogeneous execution plus multi-token speculative
streaming pushes speculative decoding toward its ideal regime of high
acceptance with low overhead. On SpecBench with server-scale models from 14B to
66B parameters, Mirror-SD delivers consistent end-to-end gains, achieving
2.8x-5.8x wall-time speedups across diverse tasks and a 30% average relative
improvement over the strongest baseline, EAGLE3.

</details>


### [154] [Higher Satisfaction, Lower Cost: A Technical Report on How LLMs Revolutionize Meituan's Intelligent Interaction Systems](https://arxiv.org/abs/2510.13291)
*Xuxin Cheng,Ke Zeng,Zhiquan Cao,Linyi Dai,Wenxuan Gao,Fei Han,Ai Jian,Feng Hong,Wenxing Hu,Zihe Huang,Dejian Kong,Jia Leng,Zhuoyuan Liao,Pei Liu,Jiaye Lin,Xing Ma,Jingqing Ruan,Jiaxing Song,Xiaoyu Tan,Ruixuan Xiao,Wenhui Yu,Wenyu Zhan,Haoxing Zhang,Chao Zhou,Hao Zhou,Shaodong Zheng,Ruinian Chen,Siyuan Chen,Ziyang Chen,Yiwen Dong,Yaoyou Fan,Yangyi Fang,Yang Gan,Shiguang Guo,Qi He,Chaowen Hu,Binghui Li,Dailin Li,Xiangyu Li,Yan Li,Chengjian Liu,Xiangfeng Liu,Jiahui Lv,Qiao Ma,Jiang Pan,Cong Qin,Chenxing Sun,Wen Sun,Zhonghui Wang,Abudukelimu Wuerkaixi,Xin Yang,Fangyi Yuan,Yawen Zhu,Tianyi Zhai,Jie Zhang,Runlai Zhang,Yao Xu,Yiran Zhao,Yifan Wang,Xunliang Cai,Yangen Hu,Cao Liu,Lu Pan,Xiaoli Wang,Bo Xiao,Wenyuan Yao,Qianlin Zhou,Benchang Zhu*

Main category: cs.CL

TL;DR: 本文介绍了一个名为WOWService的智能交互系统，它结合了大型语言模型（LLMs）和多智能体架构，旨在解决工业应用中客户服务系统的常见挑战，显著提升了用户满意度和个性化服务。


<details>
  <summary>Details</summary>
Motivation: 随着服务需求规模和复杂性增长，提升客户体验至关重要。现有的智能交互系统面临多项挑战：冷启动数据构建困难、多轮对话表现不佳、业务规则频繁演变、单一LLM在复杂场景中的局限性、以及缺乏统一标准导致难以进行定量评估和持续优化。

Method: 本文提出了WOWService系统，专为工业应用设计。它整合了LLMs和多智能体架构，以实现自主任务管理和协作式问题解决。WOWService专注于核心模块，包括数据构建、通用能力增强、业务场景适应、多智能体协调和自动化评估。

Result: WOWService已部署在美团App上，取得了显著成效，例如用户满意度指标1 (USM 1) 降低了27.53%，用户满意度指标2 (USM 2) 提升了25.51%，证明了其在捕捉用户需求和推进个性化服务方面的有效性。

Conclusion: WOWService通过集成LLMs和多智能体架构，成功解决了智能交互系统在工业应用中面临的挑战，显著提升了客户体验和个性化服务水平。

Abstract: Enhancing customer experience is essential for business success, particularly
as service demands grow in scale and complexity. Generative artificial
intelligence and Large Language Models (LLMs) have empowered intelligent
interaction systems to deliver efficient, personalized, and 24/7 support. In
practice, intelligent interaction systems encounter several challenges: (1)
Constructing high-quality data for cold-start training is difficult, hindering
self-evolution and raising labor costs. (2) Multi-turn dialogue performance
remains suboptimal due to inadequate intent understanding, rule compliance, and
solution extraction. (3) Frequent evolution of business rules affects system
operability and transferability, constraining low-cost expansion and
adaptability. (4) Reliance on a single LLM is insufficient in complex
scenarios, where the absence of multi-agent frameworks and effective
collaboration undermines process completeness and service quality. (5) The
open-domain nature of multi-turn dialogues, lacking unified golden answers,
hampers quantitative evaluation and continuous optimization. To address these
challenges, we introduce WOWService, an intelligent interaction system tailored
for industrial applications. With the integration of LLMs and multi-agent
architectures, WOWService enables autonomous task management and collaborative
problem-solving. Specifically, WOWService focuses on core modules including
data construction, general capability enhancement, business scenario
adaptation, multi-agent coordination, and automated evaluation. Currently,
WOWService is deployed on the Meituan App, achieving significant gains in key
metrics, e.g., User Satisfaction Metric 1 (USM 1) -27.53% and User Satisfaction
Metric 2 (USM 2) +25.51%, demonstrating its effectiveness in capturing user
needs and advancing personalized service.

</details>


### [155] [Putting on the Thinking Hats: A Survey on Chain of Thought Fine-tuning from the Perspective of Human Reasoning Mechanism](https://arxiv.org/abs/2510.13170)
*Xiaoshu Chen,Sihang Zhou,Ke Liang,Duanyang Yuan,Haoyuan Chen,Xiaoyu Sun,Linyuan Meng,Xinwang Liu*

Main category: cs.CL

TL;DR: 该论文首次从人类推理机制的角度全面综述了思维链（CoT）微调技术，并基于“六顶思考帽”框架对现有方法进行分类和分析。


<details>
  <summary>Details</summary>
Motivation: 现有关于CoT微调的综述主要侧重技术层面，忽视了从人类推理机制进行系统分析。鉴于CoT微调的最终目标是使大型语言模型（LLMs）像人类一样推理，因此从人类认知的角度审视这项技术至关重要，以填补这一空白。

Method: 本文受“六顶思考帽”框架启发，将CoT微调方法通过人类思维模式进行分类和审查。此外，基于此理论概述了CoT微调未来研究的潜在方向，并整理了现有数据集和模型性能的全面概述，同时维护了一个实时更新的GitHub仓库。

Result: 本文提供了一个基于人类推理理论的CoT微调综合性综述，首次将CoT微调方法通过“六顶思考帽”框架进行分类和分析，并提出了未来的研究方向。同时，整理了全面的数据集和模型性能概览，并维护了一个实时GitHub仓库。

Conclusion: 该综述旨在成为一个有价值的资源，以激发创新并促进CoT微调这一快速发展领域的进步。

Abstract: Chain of thought (CoT) fine-tuning aims to endow large language models (LLMs)
with reasoning capabilities by training them on curated reasoning traces. It
leverages both supervised and reinforced fine-tuning to cultivate human-like
reasoning skills in LLMs, including detailed planning, divergent thinking,
intuitive judgment, timely reflection, internal thinking, and fact perception,
etc. As CoT fine-tuning has advanced, LLMs have demonstrated substantial
improvements in tasks such as mathematical reasoning and code generation.
However, existing surveys about CoT fine-tuning primarily focus on technical
aspects and overlook a systematic analysis from the perspective of human
reasoning mechanisms. Given that the ultimate goal of CoT fine-tuning is to
enable LLMs to reason like humans, it is crucial to investigate this technique
through the lens of human cognition. To fill this gap, we present the first
comprehensive survey of CoT fine-tuning grounded in human reasoning theory.
Specifically, inspired by the well-known Six Thinking Hats framework, which
systematically characterizes common human thinking modes using six metaphorical
hats, we classify and examine CoT fine-tuning methods through this lens.
Furthermore, building upon this theory, we outline potential directions for
future research in CoT fine-tuning. In addition, we compile a comprehensive
overview of existing datasets and model performances, and a real-time GitHub
repository \footnote{https://github.com/AI-Chen/Awesome-CoT-Finetuning} that
continuously tracks recent advances in this area is maintained. We hope this
survey will serve as a valuable resource to inspire innovation and foster
progress in this rapidly evolving field.

</details>


### [156] [LLM one-shot style transfer for Authorship Attribution and Verification](https://arxiv.org/abs/2510.13302)
*Pablo Miralles-González,Javier Huertas-Tato,Alejandro Martín,David Camacho*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的、基于大型语言模型（LLM）预训练和上下文学习的无监督文体学分析方法，通过测量文本间的风格可迁移性，在控制主题相关性的情况下，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的监督式和对比式文体学方法常将文体与主题混淆，依赖虚假相关性。尽管LLM的预训练已被用于AI生成文本检测，但其在通用作者身份问题中的潜力尚未被充分利用。

Method: 研究者提出了一种无监督方法，利用LLM的预训练知识和上下文学习能力。具体而言，该方法使用LLM的对数概率来衡量文本之间的风格可迁移性。在作者身份验证任务中，还引入了一个额外的机制来增加测试时计算量。

Result: 该方法显著优于同等规模的LLM提示方法，并在控制主题相关性时，比对比训练的基线方法实现了更高的准确性。性能与基础模型的大小呈一致性扩展，在作者身份验证中，性能也随额外机制的计算量增加而提升。

Conclusion: 该方法在计算成本和准确性之间提供了灵活的权衡，为文体学分析提供了一种有效且可扩展的无监督解决方案，尤其适用于作者身份识别问题。

Abstract: Computational stylometry analyzes writing style through quantitative patterns
in text, supporting applications from forensic tasks such as identity linking
and plagiarism detection to literary attribution in the humanities. Supervised
and contrastive approaches rely on data with spurious correlations and often
confuse style with topic. Despite their natural use in AI-generated text
detection, the CLM pre-training of modern LLMs has been scarcely leveraged for
general authorship problems. We propose a novel unsupervised approach based on
this extensive pre-training and the in-context learning capabilities of LLMs,
employing the log-probabilities of an LLM to measure style transferability from
one text to another. Our method significantly outperforms LLM prompting
approaches of comparable scale and achieves higher accuracy than contrastively
trained baselines when controlling for topical correlations. Moreover,
performance scales fairly consistently with the size of the base model and, in
the case of authorship verification, with an additional mechanism that
increases test-time computation; enabling flexible trade-offs between
computational cost and accuracy.

</details>


### [157] [DSCD: Large Language Model Detoxification with Self-Constrained Decoding](https://arxiv.org/abs/2510.13183)
*Ming Dong,Jinkui Zhang,Bolong Zheng,Xinhui Tu,Po Hu,Tingting He*

Main category: cs.CL

TL;DR: 本文提出了一种名为DSCD（Detoxification with Self-Constrained Decoding）的新型LLM解毒方法，它在不进行参数微调的情况下，通过自我约束解码来增强安全层、削弱毒性层，从而有效降低毒性并提高生成流畅性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的解毒仍然是一个重要的研究挑战。现有的解码解毒方法都依赖外部约束，这不仅增加了资源开销，还可能损害生成内容的流畅性。

Method: DSCD方法在输出生成过程中，通过加强安全层的内部下一词元分布，同时削弱幻觉和毒性层的分布，从而实现解毒。该方法无需参数微调，轻量级、兼容性高且即插即用，可与现有解毒方法结合使用。

Result: 在代表性的开源LLM和公共数据集上进行的广泛实验验证了DSCD的有效性，在解毒和生成流畅性方面均达到了最先进（SOTA）的性能，且比现有方法具有更高的效率。

Conclusion: DSCD作为一种实用且可扩展的解决方案，有望实现更安全的LLM部署，其成果凸显了其巨大的应用潜力。

Abstract: Detoxification in large language models (LLMs) remains a significant research
challenge. Existing decoding detoxification methods are all based on external
constraints, which require additional resource overhead and lose generation
fluency. This work proposes Detoxification with Self-Constrained Decoding
(DSCD), a novel method for LLM detoxification without parameter fine-tuning.
DSCD strengthens the inner next-token distribution of the safety layer while
weakening that of hallucination and toxic layers during output generation. This
effectively diminishes toxicity and enhances output safety. DSCD offers
lightweight, high compatibility, and plug-and-play capabilities, readily
integrating with existing detoxification methods for further performance
improvement. Extensive experiments on representative open-source LLMs and
public datasets validate DSCD's effectiveness, demonstrating state-of-the-art
(SOTA) performance in both detoxification and generation fluency, with superior
efficiency compared to existing methods. These results highlight DSCD's
potential as a practical and scalable solution for safer LLM deployments.

</details>


### [158] [Personal Attribute Leakage in Federated Speech Models](https://arxiv.org/abs/2510.13357)
*Hamdan Al-Ali,Ali Reza Ghavamipour,Tommaso Caselli,Fatih Turkmen,Zeerak Talat,Hanan Aldarmaki*

Main category: cs.CL

TL;DR: 本文分析了联邦学习ASR模型在属性推断攻击下的漏洞，发现仅通过权重差异即可推断敏感属性，尤其对于预训练数据中代表性不足的属性（如口音）更易受攻击。


<details>
  <summary>Details</summary>
Motivation: 联邦学习常用于保护隐私的机器学习模型训练，但其在ASR模型中对属性推断攻击的脆弱性尚未得到充分分析。

Method: 采用被动威胁模型下的非参数白盒攻击方法，仅基于权重差异进行攻击，不访问原始语音数据。在Wav2Vec2、HuBERT和Whisper三种ASR模型上进行了测试。

Result: 攻击能够成功推断性别、年龄、口音、情绪和构音障碍等敏感人口统计学和临床属性。预训练数据中代表性不足或缺失的属性更容易受到此类推断攻击。特别是，所有模型都能可靠地推断出口音信息。

Conclusion: 研究揭示了联邦ASR模型中先前未被记录的漏洞，并为提高模型安全性提供了见解。

Abstract: Federated learning is a common method for privacy-preserving training of
machine learning models. In this paper, we analyze the vulnerability of ASR
models to attribute inference attacks in the federated setting. We test a
non-parametric white-box attack method under a passive threat model on three
ASR models: Wav2Vec2, HuBERT, and Whisper. The attack operates solely on weight
differentials without access to raw speech from target speakers. We demonstrate
attack feasibility on sensitive demographic and clinical attributes: gender,
age, accent, emotion, and dysarthria. Our findings indicate that attributes
that are underrepresented or absent in the pre-training data are more
vulnerable to such inference attacks. In particular, information about accents
can be reliably inferred from all models. Our findings expose previously
undocumented vulnerabilities in federated ASR models and offer insights towards
improved security.

</details>


### [159] [Text Anomaly Detection with Simplified Isolation Kernel](https://arxiv.org/abs/2510.13197)
*Yang Cao,Sikun Yang,Yujiu Yang,Lianyong Qi,Ming Liu*

Main category: cs.CL

TL;DR: 本文提出简化隔离核（SIK），将大语言模型的高维密集嵌入映射到低维稀疏表示，以解决文本异常检测中的计算效率和内存问题，同时提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 结合预训练大语言模型嵌入和异常检测器在文本异常检测中表现出色，但大语言模型提取的高维密集嵌入导致高内存需求和计算时间，构成挑战。

Method: 引入简化隔离核（SIK），通过创新的边界聚焦特征映射，将高维密集嵌入映射到低维稀疏表示，同时保留关键异常特征。SIK具有线性时间复杂度和显著降低的空间复杂度。

Result: 在7个数据集上的实验表明，SIK在保持计算效率和低内存成本的同时，实现了比11种最先进（SOTA）异常检测算法更好的检测性能。

Conclusion: SIK有效解决了高维大语言模型嵌入在文本异常检测中的计算效率和内存挑战，并在性能上超越现有先进算法。

Abstract: Two-step approaches combining pre-trained large language model embeddings and
anomaly detectors demonstrate strong performance in text anomaly detection by
leveraging rich semantic representations. However, high-dimensional dense
embeddings extracted by large language models pose challenges due to
substantial memory requirements and high computation time. To address this
challenge, we introduce the Simplified Isolation Kernel (SIK), which maps
high-dimensional dense embeddings to lower-dimensional sparse representations
while preserving crucial anomaly characteristics. SIK has linear time
complexity and significantly reduces space complexity through its innovative
boundary-focused feature mapping. Experiments across 7 datasets demonstrate
that SIK achieves better detection performance than 11 state-of-the-art (SOTA)
anomaly detection algorithms while maintaining computational efficiency and low
memory cost. All code and demonstrations are available at
https://github.com/charles-cao/SIK.

</details>


### [160] [Protect: Towards Robust Guardrailing Stack for Trustworthy Enterprise LLM Systems](https://arxiv.org/abs/2510.13351)
*Karthik Avinash,Nikhil Pareek,Rishav Hada*

Main category: cs.CL

TL;DR: Protect是一种原生的多模态安全护栏模型，旨在为企业级部署的LLM提供跨文本、图像和音频输入的实时、可解释的安全保障，并在多项安全维度上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）在关键领域部署的增加，对能够确保安全性、可靠性和合规性的强大护栏系统需求迫切。现有解决方案在实时监管、多模态数据处理和可解释性方面存在不足，且大多仅限于文本，无法满足受监管环境和生产级多模态场景的需求。

Method: 本文提出了Protect模型，它是一种原生的多模态护栏模型，能够无缝处理文本、图像和音频输入。Protect通过低秩适应（LoRA）技术，在包含毒性、性别歧视、数据隐私和提示注入四种安全维度的大规模多模态数据集上训练了精细调优的、类别特定的适配器。此外，它还采用了一种教师辅助标注流水线，利用推理和解释痕迹生成高保真、上下文感知的跨模态标签。

Result: 实验结果表明，Protect在所有安全维度上均达到了最先进的性能，超越了WildGuard、LlamaGuard-4和GPT-4.1等现有开源和专有模型。

Conclusion: Protect为可信赖、可审计且可用于生产的多模态安全系统奠定了坚实基础，能够有效处理文本、图像和音频模态的安全问题。

Abstract: The increasing deployment of Large Language Models (LLMs) across enterprise
and mission-critical domains has underscored the urgent need for robust
guardrailing systems that ensure safety, reliability, and compliance. Existing
solutions often struggle with real-time oversight, multi-modal data handling,
and explainability -- limitations that hinder their adoption in regulated
environments. Existing guardrails largely operate in isolation, focused on text
alone making them inadequate for multi-modal, production-scale environments. We
introduce Protect, natively multi-modal guardrailing model designed to operate
seamlessly across text, image, and audio inputs, designed for enterprise-grade
deployment. Protect integrates fine-tuned, category-specific adapters trained
via Low-Rank Adaptation (LoRA) on an extensive, multi-modal dataset covering
four safety dimensions: toxicity, sexism, data privacy, and prompt injection.
Our teacher-assisted annotation pipeline leverages reasoning and explanation
traces to generate high-fidelity, context-aware labels across modalities.
Experimental results demonstrate state-of-the-art performance across all safety
dimensions, surpassing existing open and proprietary models such as WildGuard,
LlamaGuard-4, and GPT-4.1. Protect establishes a strong foundation for
trustworthy, auditable, and production-ready safety systems capable of
operating across text, image, and audio modalities.

</details>


### [161] [A fully automated and scalable Parallel Data Augmentation for Low Resource Languages using Image and Text Analytics](https://arxiv.org/abs/2510.13211)
*Prawaal Sharma,Navneet Goyal,Poonam Goyal,Vishnupriyan R*

Main category: cs.CL

TL;DR: 本文提出了一种利用图像和文本分析从报纸文章中自动提取双语平行语料库的新方法，以解决低资源语言的数据稀缺问题，并在机器翻译任务中取得了显著改进。


<details>
  <summary>Details</summary>
Motivation: 世界各地的语言多样性导致优质数字语言资源分配不均，限制了大多数人口享受技术进步的益处。低资源语言缺乏数据资源，使得执行自然语言处理（NLP）任务变得困难。

Method: 本文提出了一种新颖、可扩展且全自动的方法，利用图像和文本分析技术从报纸文章中提取双语平行语料库。

Result: 研究通过构建两种不同语言组合的平行数据集验证了该方法。通过在机器翻译下游任务中应用这些数据集，结果显示比当前基线提高了近3个BLEU点。

Conclusion: 该研究成功开发了一种自动提取双语平行语料库的方法，有效解决了低资源语言的数据稀缺问题，并通过在机器翻译任务中的显著性能提升证明了所构建数据集的价值。

Abstract: Linguistic diversity across the world creates a disparity with the
availability of good quality digital language resources thereby restricting the
technological benefits to majority of human population. The lack or absence of
data resources makes it difficult to perform NLP tasks for low-resource
languages. This paper presents a novel scalable and fully automated methodology
to extract bilingual parallel corpora from newspaper articles using image and
text analytics. We validate our approach by building parallel data corpus for
two different language combinations and demonstrate the value of this dataset
through a downstream task of machine translation and improve over the current
baseline by close to 3 BLEU points.

</details>


### [162] [Document Intelligence in the Era of Large Language Models: A Survey](https://arxiv.org/abs/2510.13366)
*Weishi Wang,Hengchang Hu,Zhijie Zhang,Zhaochen Li,Hongxin Shao,Daniel Dahlmeier*

Main category: cs.CL

TL;DR: 本文综述了大型语言模型（LLMs）如何彻底改变文档人工智能（DAI），从编码器-解码器架构转向仅解码器架构，显著提升了文档理解和生成能力。文章概述了DAI的演变、当前研究及其未来前景，并提出了多模态、多语言、检索增强型DAI的挑战与方向。


<details>
  <summary>Details</summary>
Motivation: 文档人工智能（DAI）是一个关键应用领域，大型语言模型（LLMs）的出现（特别是仅解码器LLMs取代了早期的编码器-解码器架构）对其产生了显著变革，带来了理解和生成方面的显著进步。因此，有必要对DAI的演变、LLMs在该领域的当前研究尝试和未来前景进行全面概述。

Method: 本文采用综述方法，对DAI的演变进行全面概述，探讨LLMs在该领域的关键进展和挑战，涵盖多模态、多语言和检索增强型DAI。同时，文章还提出了未来的研究方向，如基于代理的方法和文档专用基础模型。

Result: 大型语言模型（LLMs），尤其是仅解码器LLMs，彻底改变了文档人工智能（DAI），在理解和生成方面取得了显著进展。本综述突出了DAI在多模态、多语言和检索增强型方面的当前研究进展和未来前景。

Conclusion: 大型语言模型（LLMs）已成为文档人工智能（DAI）的关键驱动力，彻底改变了该领域的理解和生成能力。本论文提供了DAI现状的结构化分析，探讨了其学术和实践应用，并提出了包括基于代理的方法和文档专用基础模型在内的未来研究方向。

Abstract: Document AI (DAI) has emerged as a vital application area, and is
significantly transformed by the advent of large language models (LLMs). While
earlier approaches relied on encoder-decoder architectures, decoder-only LLMs
have revolutionized DAI, bringing remarkable advancements in understanding and
generation. This survey provides a comprehensive overview of DAI's evolution,
highlighting current research attempts and future prospects of LLMs in this
field. We explore key advancements and challenges in multimodal, multilingual,
and retrieval-augmented DAI, while also suggesting future research directions,
including agent-based approaches and document-specific foundation models. This
paper aims to provide a structured analysis of the state-of-the-art in DAI and
its implications for both academic and practical applications.

</details>


### [163] [LiteraryQA: Towards Effective Evaluation of Long-document Narrative QA](https://arxiv.org/abs/2510.13494)
*Tommaso Bonomo,Luca Gioffré,Roberto Navigli*

Main category: cs.CL

TL;DR: 本文介绍了LiteraryQA，一个高质量的NarrativeQA子集，专注于文学作品，通过人工和LLM验证的流程纠正了低质量问答对和文档噪声。研究还对自动评估指标进行了元评估，发现LLM作为评判者比基于N-gram的指标更能与人类判断一致。最后，作者在LiteraryQA上对长上下文LLM进行了基准测试。


<details>
  <summary>Details</summary>
Motivation: 叙事文本上的问答（QA）对现有系统构成独特挑战，需要对冗长复杂文档有深入理解。然而，该领域最广泛使用的基准NarrativeQA由于文档噪声和有缺陷的问答对而可靠性受损。

Method: 作者引入了LiteraryQA，一个NarrativeQA的高质量子集，专注于文学作品。他们使用人工和LLM验证的流程来识别和纠正低质量的问答样本，并从源文档中删除无关文本。随后，对自动评估指标进行了元评估，以明确如何在LiteraryQA上评估系统。最后，在一组长上下文LLM上进行了基准测试。

Result: 分析表明，所有基于N-gram的指标与人类判断的系统级相关性较低，而LLM作为评判者的评估，即使使用小型开源模型，也能与人类识别的排名高度一致。研究还提供了一个高质量的LiteraryQA数据集，并对长上下文LLM进行了基准测试。

Conclusion: LiteraryQA的引入提高了叙事文本问答基准的质量和可靠性。对于此类任务的评估，LLM作为评判者比传统的N-gram指标更有效，能够更好地反映人类的判断。这为未来叙事文本问答系统的评估提供了新的方向。

Abstract: Question Answering (QA) on narrative text poses a unique challenge to current
systems, requiring a deep understanding of long, complex documents. However,
the reliability of NarrativeQA, the most widely used benchmark in this domain,
is hindered by noisy documents and flawed QA pairs. In this work, we introduce
LiteraryQA, a high-quality subset of NarrativeQA focused on literary works.
Using a human- and LLM-validated pipeline, we identify and correct low-quality
QA samples while removing extraneous text from source documents. We then carry
out a meta-evaluation of automatic metrics to clarify how systems should be
evaluated on LiteraryQA. This analysis reveals that all n-gram-based metrics
have a low system-level correlation to human judgment, while LLM-as-a-Judge
evaluations, even with small open-weight models, can strongly agree with the
ranking identified by humans. Finally, we benchmark a set of long-context LLMs
on LiteraryQA. We release our code and data at
https://github.com/SapienzaNLP/LiteraryQA.

</details>


### [164] [SHIELD: Classifier-Guided Prompting for Robust and Safer LVLMs](https://arxiv.org/abs/2510.13190)
*Juan Ren,Mark Dras,Usman Naseem*

Main category: cs.CL

TL;DR: SHIELD是一个轻量级、模型无关的预处理框架，通过细粒度安全分类和特定类别指导，有效提升大型视觉-语言模型（LVLMs）对抗越狱攻击的安全性，同时保持实用性。


<details>
  <summary>Details</summary>
Motivation: 大型视觉-语言模型（LVLMs）虽然具备强大的多模态推理能力，但也扩大了攻击面，特别是通过将有害目标隐藏在良性提示中的对抗性输入（即越狱攻击）。现有二元审核器不足以应对这种复杂性。

Method: 本文提出了SHIELD框架，它是一个轻量级、模型无关的预处理框架。SHIELD将细粒度安全分类与特定类别的指导和明确的行动（如阻止、重构、转发）相结合。它能够生成定制的安全提示，实现细致的拒绝或安全重定向，而无需对LVLM进行重新训练。

Result: SHIELD在五个基准测试和五个代表性LVLM上，持续降低了越狱和不遵循指令的发生率，同时保持了模型的实用性。该方法即插即用，开销可忽略不计，并且易于扩展到新的攻击类型。

Conclusion: SHIELD为弱对齐和强对齐的LVLM提供了一个实用的安全补丁，有效增强了模型的安全性，使其能够抵抗对抗性输入。

Abstract: Large Vision-Language Models (LVLMs) unlock powerful multimodal reasoning but
also expand the attack surface, particularly through adversarial inputs that
conceal harmful goals in benign prompts. We propose SHIELD, a lightweight,
model-agnostic preprocessing framework that couples fine-grained safety
classification with category-specific guidance and explicit actions (Block,
Reframe, Forward). Unlike binary moderators, SHIELD composes tailored safety
prompts that enforce nuanced refusals or safe redirection without retraining.
Across five benchmarks and five representative LVLMs, SHIELD consistently
lowers jailbreak and non-following rates while preserving utility. Our method
is plug-and-play, incurs negligible overhead, and is easily extendable to new
attack types -- serving as a practical safety patch for both weakly and
strongly aligned LVLMs.

</details>


### [165] [ConsintBench: Evaluating Language Models on Real-World Consumer Intent Understanding](https://arxiv.org/abs/2510.13499)
*Xiaozhe Li,TianYi Lyu,Siyi Yang,Yuxi Gong,Yizhao Yang,Jinxuan Huang,Ligao Zhang,Zhuoyi Huang,Qingwen Liu*

Main category: cs.CL

TL;DR: 本文介绍了\bench，首个动态、实时的大规模基准测试，用于评估大型语言模型（LLM）对复杂真实世界人类意图的理解能力，尤其是在消费领域。


<details>
  <summary>Details</summary>
Motivation: 理解人类意图对LLM来说是一项复杂的任务，需要分析推理、上下文解释、信息聚合和不确定性决策。真实的公开讨论（如消费者产品讨论）通常是非线性、多源、冲突且充满隐含假设的。LLM需要超越句子解析，整合多源信号并适应不断变化的语境。然而，目前缺乏大规模基准来评估LLM在真实世界人类意图理解方面的能力。

Method: 本文提出了\bench，一个专门为意图理解（尤其是在消费领域）设计的动态、实时评估基准。它支持实时更新，并通过自动化策展管道防止数据污染。

Result: \bench是同类中最大、最多样化的基准测试，能够实时更新并有效防止数据污染。它是首个动态、实时评估LLM在真实世界人类意图理解能力方面的基准。

Conclusion: \bench弥补了当前在评估LLM真实世界人类意图理解能力方面基准缺失的空白，为未来的研究提供了重要的评估工具。

Abstract: Understanding human intent is a complex, high-level task for large language
models (LLMs), requiring analytical reasoning, contextual interpretation,
dynamic information aggregation, and decision-making under uncertainty.
Real-world public discussions, such as consumer product discussions, are rarely
linear or involve a single user. Instead, they are characterized by interwoven
and often conflicting perspectives, divergent concerns, goals, emotional
tendencies, as well as implicit assumptions and background knowledge about
usage scenarios. To accurately understand such explicit public intent, an LLM
must go beyond parsing individual sentences; it must integrate multi-source
signals, reason over inconsistencies, and adapt to evolving discourse, similar
to how experts in fields like politics, economics, or finance approach complex,
uncertain environments. Despite the importance of this capability, no
large-scale benchmark currently exists for evaluating LLMs on real-world human
intent understanding, primarily due to the challenges of collecting real-world
public discussion data and constructing a robust evaluation pipeline. To bridge
this gap, we introduce \bench, the first dynamic, live evaluation benchmark
specifically designed for intent understanding, particularly in the consumer
domain. \bench is the largest and most diverse benchmark of its kind,
supporting real-time updates while preventing data contamination through an
automated curation pipeline.

</details>


### [166] [Hierarchical Frequency Tagging Probe (HFTP): A Unified Approach to Investigate Syntactic Structure Representations in Large Language Models and the Human Brain](https://arxiv.org/abs/2510.13255)
*Jingmin An,Yilong Song,Ruolin Yang,Nai Ding,Lingxi Lu,Yuxuan Wang,Wei Wang,Chu Zhuang,Qian Wang,Fang Fang*

Main category: cs.CL

TL;DR: 本研究引入分层频率标记探针（HFTP）工具，通过频率域分析比较大型语言模型（LLM）和人脑的句法处理机制。结果显示LLM在类似层级处理句法，而人脑依赖不同皮层区域。模型与左半球对齐，但升级模型（Gemma 2 vs Gemma, Llama 3.1 vs Llama 2）显示出不同的脑相似性趋势，引发对LLM进步机制的思考。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLM）展现出卓越的语言能力和句法建模能力，但其内部负责这些功能的具体计算模块尚不明确。关键问题在于LLM的行为能力是否源于与人脑相似的机制。

Method: 引入了分层频率标记探针（HFTP）工具。该工具利用频率域分析来识别LLM（如多层感知器神经元）和皮层区域（通过颅内记录）中编码句法结构的神经元级组件。此外，还使用了表征相似性分析（Representational Similarity Analysis, RSA）来比较LLM与人脑的表征。

Result: 研究发现GPT-2、Gemma、Gemma 2、Llama 2、Llama 3.1和GLM-4等模型在相似的层级处理句法，而人脑则依赖不同的皮层区域处理不同层级的句法。表征相似性分析显示LLM的表征与大脑左半球（语言处理主导区域）有更强的对齐。值得注意的是，升级模型呈现出不同的趋势：Gemma 2比Gemma展现出更高的脑相似性，而Llama 3.1与大脑的对齐程度低于Llama 2。

Conclusion: 这些发现为LLM行为改进的可解释性提供了新见解，引发了关于这些进步是由类人还是非类人机制驱动的问题。同时，HFTP被确立为连接计算语言学和认知神经科学的宝贵工具。

Abstract: Large Language Models (LLMs) demonstrate human-level or even superior
language abilities, effectively modeling syntactic structures, yet the specific
computational modules responsible remain unclear. A key question is whether LLM
behavioral capabilities stem from mechanisms akin to those in the human brain.
To address these questions, we introduce the Hierarchical Frequency Tagging
Probe (HFTP), a tool that utilizes frequency-domain analysis to identify
neuron-wise components of LLMs (e.g., individual Multilayer Perceptron (MLP)
neurons) and cortical regions (via intracranial recordings) encoding syntactic
structures. Our results show that models such as GPT-2, Gemma, Gemma 2, Llama
2, Llama 3.1, and GLM-4 process syntax in analogous layers, while the human
brain relies on distinct cortical regions for different syntactic levels.
Representational similarity analysis reveals a stronger alignment between LLM
representations and the left hemisphere of the brain (dominant in language
processing). Notably, upgraded models exhibit divergent trends: Gemma 2 shows
greater brain similarity than Gemma, while Llama 3.1 shows less alignment with
the brain compared to Llama 2. These findings offer new insights into the
interpretability of LLM behavioral improvements, raising questions about
whether these advancements are driven by human-like or non-human-like
mechanisms, and establish HFTP as a valuable tool bridging computational
linguistics and cognitive neuroscience. This project is available at
https://github.com/LilTiger/HFTP.

</details>


### [167] [Do You Get the Hint? Benchmarking LLMs on the Board Game Concept](https://arxiv.org/abs/2510.13271)
*Ine Gevers,Walter Daelemans*

Main category: cs.CL

TL;DR: 本研究引入了“概念”游戏作为基准，发现大型语言模型（LLMs）在需要溯因推理的自然语言任务中表现不佳，尤其是在理解意图和修正假设方面，且在低资源语言中性能进一步下降。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在许多基准测试中取得了成功，但在抽象推理任务上仍存在根本性弱点，特别是当任务使用与LLM训练数据不同的表示形式（如网格、符号、视觉模式）时。因此，需要一个接近LLM预训练数据（自然语言）的基准来探测其溯因推理能力。

Method: 研究引入了一个简单的猜词棋盘游戏“概念”（Concept）作为基准，用于探测LLMs的溯因推理能力。他们评估了最先进的LLMs在该游戏上的表现，并与人类的表现进行了对比。此外，研究还将评估扩展到多种语言（英语、荷兰语、法语、西班牙语）。

Result: 人类玩家能轻松解决“概念”游戏（成功率超过90%），而最先进的LLMs表现非常挣扎（没有模型成功率超过40%）。具体而言，LLMs难以解释其他玩家的策略意图，也难以根据序列信息更新来修正初始假设。此外，在低资源语言（荷兰语、法语和西班牙语）中，LLM的性能相比英语进一步下降。

Conclusion: LLMs在溯因推理方面仍存在显著的局限性，即使在接近其预训练数据的自然语言环境中也是如此。它们在理解策略意图和根据动态信息修正假设方面尤其薄弱。跨语言评估进一步揭示了LLMs在低资源语言中的性能差距。

Abstract: Large language models (LLMs) have achieved striking successes on many
benchmarks, yet recent studies continue to expose fundamental weaknesses. In
particular, tasks that require abstract reasoning remain challenging, often
because they use representations such as grids, symbols, or visual patterns
that differ from the natural language data LLMs are trained on. In this paper,
we introduce Concept, a simple word-guessing board game, as a benchmark for
probing abductive reasoning in a representation that is much closer to LLM
pre-training data: natural language. Our results show that this game, easily
solved by humans (with a success rate of over 90\%), is still very challenging
for state-of-the-art LLMs (no model exceeds 40\% success rate). Specifically,
we observe that LLMs struggle with interpreting other players' strategic
intents, and with correcting initial hypotheses given sequential information
updates. In addition, we extend the evaluation across multiple languages, and
find that the LLM performance drops further in lower-resource languages (Dutch,
French, and Spanish) compared to English.

</details>


### [168] [In-Distribution Steering: Balancing Control and Coherence in Language Model Generation](https://arxiv.org/abs/2510.13285)
*Arthur Vogels,Benjamin Wong,Yann Choho,Annabelle Blangero,Milan Bhan*

Main category: cs.CL

TL;DR: 本文提出了一种名为In-Distribution Steering (IDS)的新方法，它能根据输入数据在表示空间中的分布动态调整大型语言模型（LLM）的激活引导强度，从而在控制模型行为的同时保持文本的连贯性和生成稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有的激活引导方法通常采用固定的引导强度，这可能导致控制不足或过度干预，从而损害生成文本的合理性和连贯性。

Method: IDS方法通过评估给定输入在表示空间中与数据分布的距离，自适应地调整激活引导的强度。这种动态调整机制使得干预能够根据输入进行适应性调整。

Result: 实验结果表明，IDS在分类任务上取得了很高的准确性，并且能够生成连贯且不会崩溃的文本。

Conclusion: IDS方法因其自适应的干预和稳定的文本生成能力，特别适用于实际应用场景。

Abstract: Activation steering methods control large language model (LLM) behavior by
modifying internal activations at inference time. However, most existing
activation steering methods rely on a fixed steering strength, leading to
either insufficient control or unadapted intervention that degrades text
plausibility and coherence. We introduce In-Distribution Steering (IDS), a
novel method that adapts steering strength based on the input data distribution
in representation space. IDS dynamically adjusts interventions according to how
far a given input lies within the distribution, enabling adaptive intervention
and generation stability during text generation. Experiments demonstrate that
IDS achieves strong accuracy on classification tasks while producing coherent
text without collapse, making IDS particularly well suited for real-world
applications.

</details>


### [169] [Grounding Long-Context Reasoning with Contextual Normalization for Retrieval-Augmented Generation](https://arxiv.org/abs/2510.13191)
*Jiamin Chen,Yuchen Li,Xinyu Ma,Xinran Chen,Xiaokun Zhang,Shuaiqiang Wang,Chen Ma,Dawei Yin*

Main category: cs.CL

TL;DR: 本研究发现，检索增强生成（RAG）中检索内容的呈现格式（如分隔符、结构标记）对性能有显著影响。通过系统实验揭示了影响因素，并提出了一种轻量级策略“上下文标准化”，能有效提高RAG的鲁棒性和长上下文利用率。


<details>
  <summary>Details</summary>
Motivation: 现有RAG研究主要侧重于检索质量和提示策略，但检索到的文档如何呈现（即上下文格式）对大型语言模型（LLMs）推理和知识能力的影响尚未得到充分探索。

Method: 研究设计了受控实验，改变上下文密度、分隔符样式和位置放置，以系统调查上下文格式的影响。在此基础上，引入了“上下文标准化”（Contextual Normalization）策略，该策略在生成前自适应地标准化上下文表示。并在受控和真实世界的RAG基准上进行了广泛实验。

Result: 研究发现，即使语义内容相同，分隔符或键值提取中的结构标记等看似表面的选择也会导致准确性和稳定性发生实质性变化。受控实验揭示了影响性能差异的潜在因素。提出的上下文标准化策略持续提高了对顺序变化的鲁棒性，并加强了长上下文的利用。

Conclusion: 可靠的RAG不仅取决于检索到正确的内容，还取决于内容的呈现方式。本研究提供了新的实证证据和一种实用的技术，以实现更好的长上下文推理。

Abstract: Retrieval-Augmented Generation (RAG) has become an essential approach for
extending the reasoning and knowledge capacity of large language models (LLMs).
While prior research has primarily focused on retrieval quality and prompting
strategies, the influence of how the retrieved documents are framed, i.e.,
context format, remains underexplored. We show that seemingly superficial
choices, such as delimiters or structural markers in key-value extraction, can
induce substantial shifts in accuracy and stability, even when semantic content
is identical. To systematically investigate this effect, we design controlled
experiments that vary context density, delimiter styles, and positional
placement, revealing the underlying factors that govern performance
differences. Building on these insights, we introduce Contextual Normalization,
a lightweight strategy that adaptively standardizes context representations
before generation. Extensive experiments on both controlled and real-world RAG
benchmarks across diverse settings demonstrate that the proposed strategy
consistently improves robustness to order variation and strengthens
long-context utilization. These findings underscore that reliable RAG depends
not only on retrieving the right content, but also on how that content is
presented, offering both new empirical evidence and a practical technique for
better long-context reasoning.

</details>


### [170] [Beyond Correctness: Rewarding Faithful Reasoning in Retrieval-Augmented Generation](https://arxiv.org/abs/2510.13272)
*Zhichao Xu,Zongyu Wu,Yun Zhou,Aosong Feng,Kang Zhou,Sangmin Woo,Kiran Ramnath,Yijun Tian,Xuan Qi,Weikang Qiu,Lin Lee Cheong,Haibo Ding*

Main category: cs.CL

TL;DR: 本文提出一个评估框架来衡量基于强化学习的搜索代理的推理忠实度，并引入VERITAS框架，通过集成细粒度忠实度奖励来显著提升推理忠实度，同时保持任务性能。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习（RL）在训练LLM使用搜索引擎作为工具方面取得了成功，并在问答（QA）基准上有所改进，但现有方法通常只关注最终答案的正确性，而忽略了中间推理步骤的质量，这可能导致思维链的不忠实性。

Method: 1. 引入了一个全面的评估框架，包含三个不同的忠实度指标：信息-思考忠实度、思考-答案忠实度和思考-搜索忠实度。2. 提出VERITAS（Verifying Entailed Reasoning through Intermediate Traceability in Agentic Search）框架，该框架将细粒度的忠实度奖励整合到强化学习过程中，以促进忠实推理。

Result: 1. 评估显示，原型RL搜索代理Search-R1在忠实度方面有显著改进空间。2. 使用VERITAS训练的模型不仅显著提高了推理忠实度。3. VERITAS在七个QA基准测试中也取得了可比的任务性能。

Conclusion: VERITAS是一个有效的新框架，能够通过在强化学习过程中整合细粒度忠实度奖励，显著提升LLM搜索代理的推理忠实度，同时保持其任务性能。

Abstract: Inspired by the success of reinforcement learning (RL) in Large Language
Model (LLM) training for domains like math and code, recent works have begun
exploring how to train LLMs to use search engines more effectively as tools for
retrieval-augmented generation. Although these methods achieve performance
improvement across QA benchmarks, many prioritize final answer correctness
while overlooking the quality of intermediate reasoning steps, which may lead
to chain-of-thought unfaithfulness. In this paper, we first introduce a
comprehensive evaluation framework for evaluating RL-based search agents,
covering three distinct faithfulness metrics: information-think faithfulness,
think-answer faithfulness, and think-search faithfulness. Our evaluations
reveal that a prototypical RL-based search agent, Search-R1, has significant
room for improvement in this regard. To foster faithful reasoning, we introduce
VERITAS (Verifying Entailed Reasoning through Intermediate Traceability in
Agentic Search), a novel framework that integrates fine-grained faithfulness
rewards into the reinforcement learning process. Our experiments show that
models trained with VERITAS not only significantly improve reasoning
faithfulness, but also achieve comparable task performance across seven QA
benchmarks.

</details>


### [171] [MedREK: Retrieval-Based Editing for Medical LLMs with Key-Aware Prompts](https://arxiv.org/abs/2510.13500)
*Shujun Xia,Haokun Lin,Yichen Wu,Yinan Zhou,Zixuan Li,Zhongwei Wan,Xingrun Xing,Yefeng Zheng,Xiang Li,Caifeng Shan,Zhenan Sun,Quanzheng Li*

Main category: cs.CL

TL;DR: 本文提出MedREK，一个基于检索的编辑框架，用于解决医疗领域大型语言模型（LLMs）信息过时和不准确的问题。MedREK通过共享查询-键模块和注意力提示编码器，实现了精确匹配和信息指导，并首次验证了医疗LLMs的批量编辑解决方案，同时构建了MedVersa基准。


<details>
  <summary>Details</summary>
Motivation: 医疗LLMs因医学知识快速演变和训练数据错误，常生成过时或不准确信息，限制了其在临床实践中的应用。现有模型编辑方法面临挑战：基于参数的编辑损害局部性，不适用于医疗领域；基于检索的编辑存在表示重叠导致检索不准确，且仅限于单样本编辑，缺乏对批量编辑的支持。

Method: 1. 构建了MedVersa，一个增强型基准测试，覆盖更广泛的医学主题，用于在严格局部性约束下评估单次和批量编辑。2. 提出了MedREK，一个基于检索的编辑框架，该框架整合了共享查询-键模块以实现精确匹配，并结合了注意力机制的提示编码器以提供信息指导。

Result: 实验结果表明，MedREK在各种医学基准测试中，在不同核心指标上均取得了卓越性能。它为医疗LLMs的批量编辑提供了首个经过验证的解决方案。

Conclusion: MedREK框架和MedVersa基准有效地解决了医疗LLMs中不准确检索和缺乏批量编辑的挑战，显著提升了LLMs在医疗应用中的可靠性和实用性。

Abstract: LLMs hold great promise for healthcare applications, but the rapid evolution
of medical knowledge and errors in training data often cause them to generate
outdated or inaccurate information, limiting their applicability in high-stakes
clinical practice. Model editing has emerged as a potential remedy without full
retraining. While parameter-based editing often compromises locality and is
thus ill-suited for the medical domain, retrieval-based editing offers a more
viable alternative. However, it still faces two critical challenges: (1)
representation overlap within the medical knowledge space often causes
inaccurate retrieval and reduces editing accuracy; (2) existing methods are
restricted to single-sample edits, while batch-editing remains largely
unexplored despite its importance for real-world medical applications. To
address these challenges, we first construct MedVersa, \hk{an enhanced
benchmark with broader coverage of medical subjects, designed to evaluate both
single and batch edits under strict locality constraints}. We then propose
MedREK, a retrieval-based editing framework that integrates a shared query-key
module for precise matching with an attention-based prompt encoder for
informative guidance. Experimental results on various medical benchmarks
demonstrate that our MedREK achieves superior performance across different core
metrics and provides the first validated solution for batch-editing in medical
LLMs. Our code and dataset are available at
https://github.com/mylittleriver/MedREK.

</details>


### [172] [ChatR1: Reinforcement Learning for Conversational Reasoning and Retrieval Augmented Question Answering](https://arxiv.org/abs/2510.13312)
*Simon Lupart,Mohammad Aliannejadi,Evangelos Kanoulas*

Main category: cs.CL

TL;DR: ChatR1是一个基于强化学习（RL）的对话式问答（CQA）推理框架，它通过在对话回合中交错搜索和推理，并引入意图感知奖励来应对CQA中用户意图演变和上下文理解的挑战，显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 对话式问答（CQA）中，用户意图会随对话轮次演变，且话语常不明确，需要上下文解释、查询重构以及检索与生成之间的动态协调。传统的静态“重写、检索、生成”流程难以适应这种动态的推理需求。

Method: 本文提出了ChatR1，一个基于强化学习的推理框架，它在对话轮次中交错进行搜索和推理，以实现探索性和自适应行为。为解决RL中奖励稀疏和延迟的问题，ChatR1引入了一种意图感知奖励机制，通过将检索和推理与不断演变的用户目标对齐，提供回合级的反馈。

Result: ChatR1在3B和7B模型骨干上均表现出色，在五个CQA数据集上，使用F1、BERTScore和LLM-as-judge等不同指标，性能优于竞争模型。消融研究证实了意图感知奖励的有效性。分析揭示了多样化的推理路径和搜索工具的有效利用。ChatR1在不同领域也表现出强大的泛化能力。

Conclusion: 基于强化学习的ChatR1推理框架，相比静态CQA流程，能够实现更灵活、更具上下文敏感性的行为，从而有效应对对话式问答中的复杂推理挑战。

Abstract: We present ChatR1, a reasoning framework based on reinforcement learning (RL)
for conversational question answering (CQA). Reasoning plays an important role
in CQA, where user intent evolves across dialogue turns, and utterances are
often underspecified, requiring contextual interpretation, query reformulation,
and dynamic coordination between retrieval and generation. Unlike static
`rewrite, retrieve, and generate' pipelines, ChatR1 interleaves search and
reasoning across turns, enabling exploratory and adaptive behaviors learned
through RL. To address the challenge of sparse and delayed rewards in RL, we
propose an intent-aware reward that provides turn-level feedback by aligning
retrieval and reasoning with evolving user goals. Our proposed ChatR1
demonstrates strong performance on both 3B and 7B model backbones,
outperforming competitive models on five CQA datasets, measured by different
metrics (F1, BERTScore, and LLM-as-judge). We include a diverse set of CQA
datasets to cover topic shifts, evolving intents, mixed-initiative dialogues,
and multi-document grounding, testing ChatR1's performance from various
aspects. Ablation studies confirm the effectiveness of the intent-aware reward.
Our analyses further reveal diverse reasoning trajectories and effective use of
the search tool. ChatR1 also generalizes robustly across domains, demonstrating
that RL-based reasoning enables more flexible and context-sensitive behavior
than static CQA pipelines.

</details>


### [173] [Mismatch Aware Guidance for Robust Emotion Control in Auto-Regressive TTS Models](https://arxiv.org/abs/2510.13293)
*Yizhou Peng,Yukun Ma,Chong Zhang,Yi-Wen Chao,Chongjia Ni,Bin Ma*

Main category: cs.CL

TL;DR: 本文提出一种自适应分类器无关指导（CFG）方案，旨在解决自回归（AR）文本转语音（TTS）模型中风格与内容不匹配的问题，从而提升情感表现力并保持音频质量。


<details>
  <summary>Details</summary>
Motivation: 现有TTS系统在通过自然语言提示实现精细情感控制时，当期望情感（风格提示）与文本语义内容冲突时，会产生不自然的语音，影响情感控制效果。此外，分类器无关指导（CFG）虽是增强提示对齐的关键技术，但其在自回归（AR）TTS模型中的应用尚不充分，且可能导致音频质量下降。

Method: 本文提出了一种自适应分类器无关指导（CFG）方案，该方案能够根据大型语言模型或自然语言推理模型检测到的风格与内容不匹配的不同程度进行调整。此方案基于对CFG在最先进AR TTS模型中对情感表现力影响的全面分析。

Result: 实验结果表明，所提出的自适应CFG方案显著改善了AR TTS模型的情感表现力，同时成功保持了音频质量和可懂度。

Conclusion: 通过引入自适应CFG方案，可以有效解决AR TTS模型中风格与内容不匹配的挑战，从而在不牺牲音频质量和可懂度的前提下，提升模型的情感表达能力。

Abstract: While Text-to-Speech (TTS) systems can achieve fine-grained control over
emotional expression via natural language prompts, a significant challenge
emerges when the desired emotion (style prompt) conflicts with the semantic
content of the text. This mismatch often results in unnatural-sounding speech,
undermining the goal of achieving fine-grained emotional control.
Classifier-Free Guidance (CFG) is a key technique for enhancing prompt
alignment; however, its application to auto-regressive (AR) TTS models remains
underexplored, which can lead to degraded audio quality. This paper directly
addresses the challenge of style-content mismatch in AR TTS models by proposing
an adaptive CFG scheme that adjusts to different levels of the detected
mismatch, as measured using large language models or natural language inference
models. This solution is based on a comprehensive analysis of CFG's impact on
emotional expressiveness in state-of-the-art AR TTS models. Our results
demonstrate that the proposed adaptive CFG scheme improves the emotional
expressiveness of the AR TTS model while maintaining audio quality and
intelligibility.

</details>


### [174] [Deflanderization for Game Dialogue: Balancing Character Authenticity with Task Execution in LLM-based NPCs](https://arxiv.org/abs/2510.13586)
*Pasin Buakhaw,Kun Kerdthaisong,Phuree Phenhiran,Pitikorn Khlaisamniang,Supasate Vorathammathorn,Piyalitt Ittichaiwong,Nutchanon Yongsatianchot*

Main category: cs.CL

TL;DR: 该论文报告了Tu_Character_lab团队在Commonsense Persona-Grounded Dialogue Challenge (CPDC) 2025第二轮比赛中的参与情况，该比赛旨在评估基于大型语言模型（LLMs）的动态非玩家角色（NPCs）在任务执行和角色一致性对话方面的能力。团队采用了轻量级提示工程和微调大型模型（Qwen3-14B）相结合的方法，并在多个赛道中取得了优异成绩。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的出现为游戏环境中创建动态NPCs提供了新机遇，使其能够执行功能性任务并生成符合角色设定的对话。CPDC 2025挑战赛旨在评估这些能力，促使研究人员探索有效的LLM应用策略。

Method: 该研究结合了两种互补策略：(i) 在API赛道中，采用轻量级提示技术，包括一种名为“Deflanderization”的提示方法来抑制过度角色扮演并提高任务忠实度；(ii) 在GPU赛道中，利用Qwen3-14B模型进行监督微调（SFT）和低秩适应（LoRA）。

Result: 团队的最佳提交在Task 1中排名第2，在Task 3（API赛道）中排名第2，在Task 3（GPU赛道）中排名第4。

Conclusion: 该研究展示了结合轻量级提示工程和微调大型模型的方法，在创建具有角色一致性对话和任务执行能力的动态NPCs方面是有效的，并在CPDC 2025挑战赛中取得了显著的竞争性结果。

Abstract: The emergence of large language models (LLMs) has opened new opportunities
for cre- ating dynamic non-player characters (NPCs) in gaming environments,
enabling both func- tional task execution and persona-consistent dialogue
generation. In this paper, we (Tu_Character_lab) report our participation in
the Commonsense Persona-Grounded Dialogue Challenge (CPDC) 2025 Round 2, which
eval- uates agents across three tracks: task-oriented dialogue, context-aware
dialogue, and their integration. Our approach combines two complementary
strategies: (i) lightweight prompting techniques in the API track, including a
Deflanderization prompting method to suppress excessive role-play and improve
task fidelity, and (ii) fine-tuned large models in the GPU track, leveraging
Qwen3-14B with supervisedfinetuning (SFT) and Low-Rank Adaptation(LoRA). Our
best submissions ranked 2nd on Task 1, 2nd on Task 3 (API track), and 4th on
Task 3 (GPU track).

</details>


### [175] [Embedding-Based Context-Aware Reranker](https://arxiv.org/abs/2510.13329)
*Ye Yuan,Mohammad Amin Shabani,Siqi Liu*

Main category: cs.CL

TL;DR: 为解决RAG系统中长文档切分导致跨段落推理困难的问题，本文提出EBCAR，一种轻量级、基于嵌入的上下文感知重排器。它利用段落结构信息和混合注意力机制增强跨段落理解，在ConTEB基准测试中优于现有SOTA重排器，提高了准确性和效率。


<details>
  <summary>Details</summary>
Motivation: RAG系统将长文档切分为短段落以实现精细化检索，但这导致了跨段落推理（如指代消解、实体消歧、证据聚合）的挑战。许多现有SOTA重排方法，尽管使用了强大的预训练语言模型，却忽视了这些跨段落推理的挑战。

Method: 本文提出EBCAR（Embedding-Based Context-Aware Reranker），一个轻量级的重排框架。它直接在检索到的段落嵌入上操作，通过利用段落的结构信息和一种混合注意力机制（同时捕获文档间的高级交互和文档内的低级关系）来增强跨段落理解。

Result: EBCAR在ConTEB基准测试上与SOTA重排器进行了评估，结果表明它在需要跨段落推理的信息检索任务中表现出有效性，并在准确性和效率方面都展现出优势。

Conclusion: EBCAR通过其轻量级、基于嵌入的上下文感知方法，结合结构信息和混合注意力机制，成功解决了RAG系统重排中跨段落推理的挑战，在性能和效率上均超越了现有SOTA方法。

Abstract: Retrieval-Augmented Generation (RAG) systems rely on retrieving relevant
evidence from a corpus to support downstream generation. The common practice of
splitting a long document into multiple shorter passages enables finer-grained
and targeted information retrieval. However, it also introduces challenges when
a correct retrieval would require inference across passages, such as resolving
coreference, disambiguating entities, and aggregating evidence scattered across
multiple sources. Many state-of-the-art (SOTA) reranking methods, despite
utilizing powerful large pretrained language models with potentially high
inference costs, still neglect the aforementioned challenges. Therefore, we
propose Embedding-Based Context-Aware Reranker (EBCAR), a lightweight reranking
framework operating directly on embeddings of retrieved passages with enhanced
cross-passage understandings through the structural information of the passages
and a hybrid attention mechanism, which captures both high-level interactions
across documents and low-level relationships within each document. We evaluate
EBCAR against SOTA rerankers on the ConTEB benchmark, demonstrating its
effectiveness for information retrieval requiring cross-passage inference and
its advantages in both accuracy and efficiency.

</details>


### [176] [NOSA: Native and Offloadable Sparse Attention](https://arxiv.org/abs/2510.13602)
*Yuxiang Huang,Chaojun Xiao,Xu Han,Zhiyuan Liu*

Main category: cs.CL

TL;DR: 本文提出NOSA框架，通过引入显式局部性约束，使可训练稀疏注意力能够原生支持KV缓存卸载，从而在不影响性能的情况下显著提高LLM解码吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有可训练稀疏注意力虽然能节省内存访问，但未能减少KV缓存大小，这限制了GPU上的批处理大小并降低了解码吞吐量，尤其是在大规模批处理推理中。

Method: 研究发现可训练稀疏注意力在相邻解码步骤中表现出强大的令牌选择局部性。在此基础上，提出NOSA框架，通过将令牌选择分解为查询感知和查询无关两部分，引入显式局部性约束，减少了KV传输，同时保留了训练时使用的注意力计算方式。

Result: 使用NOSA预训练了一个1B参数模型，结果显示其保持了接近无损的性能，并与香草可训练稀疏注意力基线（InfLLM-V2）相比，解码吞吐量提高了2.3倍。

Conclusion: NOSA通过原生支持KV缓存卸载，有效解决了稀疏注意力在长上下文处理中KV缓存过大的限制，显著提升了LLM的解码吞吐量，同时保持了高性能。

Abstract: Trainable sparse attention has emerged as a promising solution to address the
decoding efficiency bottleneck of LLMs in long-context processing,
significantly saving memory accesses while minimally impacting task
performance. However, existing sparse attention methods leave a crucial
limitation unresolved: the size of the key-value (KV) cache remains unreduced,
which constrains on-GPU batch sizes and throttles decoding throughput,
especially in large-scale batched inference. In this paper, we show that
trainable sparse attention naturally exhibits strong locality in token
selection across adjacent decoding steps, thereby enabling KV cache offloading
without altering the underlying attention computation. However, the inherent
locality remains insufficient to achieve efficient offloading, as the transfer
of selected KV pairs between the CPU and GPU continues to dominate the overall
decoding cost. Building on this insight, we present NOSA, a trainable sparse
attention framework designed to natively support KV cache offloading. NOSA
introduces explicit locality constraints by decomposing token selection into
query-aware and query-agnostic components, thereby reducing KV transfers while
preserving the same attention computation as used during training. We pretrain
a 1B-parameter model with NOSA and conduct extensive benchmarks, showing that
it preserves near-lossless performance while achieving up to a 2.3x improvement
in decoding throughput compared with the vanilla trainable sparse attention
baseline (InfLLM-V2).

</details>


### [177] [Taming the Fragility of KV Cache Eviction in LLM Inference](https://arxiv.org/abs/2510.13334)
*Yuan Feng,Haoyu Guo,JunLin Lv,S. Kevin Zhou,Xike Xie*

Main category: cs.CL

TL;DR: 本文提出了一种名为DefensiveKV的新型KV缓存逐出方法，通过引入两步线性时间防御性聚合策略来控制最坏情况风险，有效应对现有方法中稳定假设的脆弱性，显著降低了大型语言模型生成质量的损失。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型部署面临Transformer KV缓存的巨大内存和运行时开销。现有逐出方法基于“稳定性假设”通过评分-聚合框架工作，但这种假设在极端情况下是脆弱的，导致均值聚合方法表现不佳。

Method: 我们提出了一种简单而优雅的防御性聚合策略：一个两步、线性时间的方案，旨在控制最坏情况风险。在此策略基础上，我们提出了DefensiveKV以及其扩展Layer-DefensiveKV（结合了分层预算分配）作为新的缓存逐出方法。

Result: 在七个任务领域（18个数据集）中，当缓存大小为20%时，我们的方法与最强基线相比，分别将生成质量损失降低了2.3倍（DefensiveKV）和4.3倍（Layer-DefensiveKV）。

Conclusion: 这些结果为KV缓存逐出性能设定了新的基准，并开创了一个通过最坏情况风险管理来优化缓存逐出，以应对潜在脆弱性的新方向。

Abstract: Large language models have revolutionized natural language processing, yet
their deployment remains hampered by the substantial memory and runtime
overhead of the transformer's Key-Value cache. To mitigate this, recent methods
employ a scoring-aggregation framework to evict unimportant cache entries,
based on the stability assumption-that a fixed subset of entries remains
consistently important during generation. However, prior work has largely
focused on refining importance indicators for scoring, while defaulting to mean
aggregation due to a faithful trust in the stability assumption. In this work,
we argue that this underlying assumption is inherently fragile, making mean
aggregation highly vulnerable in extreme cases. To counter this, we propose a
simple yet elegant defensive aggregation strategy: a two-step, linear-time
approach that controls worst-case risk, thereby defending against extreme cases
with negligible computational overhead. Embodying this strategy, we propose a
novel cache eviction method, DefensiveKV and its extension, Layer-DefensiveKV,
which incorporates layer-wise budget allocation. Across seven task domains (18
datasets), our methods reduce generation quality loss by 2.3x and 4.3x
respectively, versus the strongest baseline under a 20% cache size. These
results set new performance benchmarks and pioneer a promising direction for
optimizing cache eviction against underlying fragility through worst-case risk
management. Our code is available at https://github.com/FFY0/DefensiveKV.

</details>


### [178] [Unlocking Public Catalogues: Instruction-Tuning LLMs for ICD Coding of German Tumor Diagnoses](https://arxiv.org/abs/2510.13624)
*Stefan Lenz,Lakisha Ortiz Rosario,Georg Vollmar,Arsenij Ustjanzew,Fatma Alickovic,Thomas Kindler,Torsten Panholzer*

Main category: cs.CL

TL;DR: 本研究通过指令微调，显著提升了开源LLM在德语肿瘤诊断编码（ICD-10-GM）方面的准确性，利用公共目录构建训练数据集是有效途径。


<details>
  <summary>Details</summary>
Motivation: 在德国，使用ICD-10-GM和ICD-O-3对肿瘤诊断进行准确编码对于结构化癌症文档至关重要。小型开源LLM在隐私保护自动化方面具有吸引力，但往往难以在德语语境下实现准确编码。

Method: 研究通过基于指令的微调方法，利用ICD-10-GM、ICD-O-3和OPS目录创建了超过50万个问答对作为训练数据。对Qwen、Llama和Mistral系列的八个开源模型（7-70 B参数）进行了微调。评估使用本地肿瘤文档系统中的编码诊断作为测试数据，并对数据质量进行了系统评估。

Result: ICD-10-GM的准确率从1.4-24%提高到41-58%（精确），部分准确率从31-74%提高到73-83%。ICD-O-3地形编码的准确率也有所提高，但微调后仍较低（精确22-40%，部分56-67%）。所有模型的畸形代码输出降至0%。肿瘤诊断识别率达到99%。准确率与模型大小呈正相关，但微调后大小模型之间的差距缩小。Qwen3的推理模式性能低于微调，且速度慢100多倍。

Conclusion: 研究结果强调了利用公共目录构建指令数据集，以改进LLM在医学文档任务中表现的潜力。完整的训练数据集和最佳性能模型检查点已公开可用。

Abstract: Accurate coding of tumor diagnoses with ICD-10-GM and ICD-O-3 is essential
for structured cancer documentation in Germany. Smaller open-weight LLMs are
appealing for privacy-preserving automation but often struggle with coding
accuracy in German-language contexts. This study investigates whether
instruction-based fine-tuning on public datasets improves the coding accuracy
of open-weight LLMs for German tumor diagnosis texts. The evaluation uses coded
diagnoses from the local tumor documentation system as test data. In a
systematic data quality assessment, the upper limit for ICD-10 coding
performance was estimated at 60-79% for exact and 81-94% for partial
(three-character codes only) derivation. As training data, over 500,000
question-answer pairs were created based on the ICD-10-GM, ICD-O-3, and OPS
catalogues. Eight open-weight models from the Qwen, Llama, and Mistral families
(7-70 B parameters) were fine-tuned. ICD-10-GM accuracy rose from 1.4-24% to
41-58%, and partial accuracy from 31-74% to 73-83%. The accuracy of ICD-O-3
topography coding also improved but started and remained considerably lower
with an exact accuracy of 22-40% and a partial accuracy of 56-67% after
fine-tuning. Malformed code outputs dropped to 0% for all models.
Tumor-diagnosis recognition reached 99%. Accuracy correlated positively with
model size, but gaps between small and large models narrowed after fine-tuning.
The reasoning mode in Qwen3 generally yielded a lower performance than
fine-tuning and was over 100 times slower. Our findings highlight the potential
of leveraging public catalogues to build instruction datasets that improve LLMs
in medical documentation tasks. The complete training dataset and the
best-performing checkpoints of the fine-tuned models are available from
https://huggingface.co/datasets/stefan-m-lenz/ICDOPS-QA-2024.

</details>


### [179] [Are Proverbs the New Pythian Oracles? Exploring Sentiment in Greek Sayings](https://arxiv.org/abs/2510.13341)
*Katerina Korre,John Pavlopoulos*

Main category: cs.CL

TL;DR: 本文利用大型语言模型（LLMs）对希腊谚语进行情感分析，扩展了方言数据集，并绘制了希腊各地谚语情感分布图，发现LLMs在非常规情感极性任务中表现良好，且希腊大部分地区的谚语倾向于负面情感。


<details>
  <summary>Details</summary>
Motivation: 谚语作为跨文化和语言的现象，其全球图景仍未被充分探索，许多文化因口头传统而将其传统智慧保留在社区内部。利用自然语言处理（NLP）的最新进展，可以深入分析这些未被探索的谚语。

Method: 研究利用现有标注的希腊谚语数据集，并将其扩展以包含地方方言，从而有效地映射标注情感。方法包括：1) 利用LLMs对谚语进行情感分类；2) 绘制希腊地图以概述情感分布；3) 结合地理位置、方言和谚语主题进行组合分析。

Result: 研究结果表明，LLMs能够提供足够准确的谚语情感图景，尤其是在将其视为非常规情感极性任务时。此外，在希腊的大部分地区，负面情感的谚语更为普遍。

Conclusion: LLMs是进行谚语情感分类的有效工具，即使面对非传统的情感极性任务也能提供准确的结果。通过对希腊谚语的分析，发现其情感分布存在地域差异，并且整体上负面情感的谚语在希腊更为普遍。

Abstract: Proverbs are among the most fascinating linguistic phenomena that transcend
cultural and linguistic boundaries. Yet, much of the global landscape of
proverbs remains underexplored, as many cultures preserve their traditional
wisdom within their own communities due to the oral tradition of the
phenomenon. Taking advantage of the current advances in Natural Language
Processing (NLP), we focus on Greek proverbs, analyzing their sentiment.
Departing from an annotated dataset of Greek proverbs, we expand it to include
local dialects, effectively mapping the annotated sentiment. We present (1) a
way to exploit LLMs in order to perform sentiment classification of proverbs,
(2) a map of Greece that provides an overview of the distribution of sentiment,
(3) a combinatory analysis in terms of the geographic position, dialect, and
topic of proverbs. Our findings show that LLMs can provide us with an accurate
enough picture of the sentiment of proverbs, especially when approached as a
non-conventional sentiment polarity task. Moreover, in most areas of Greece
negative sentiment is more prevalent.

</details>


### [180] [Closing the Gap Between Text and Speech Understanding in LLMs](https://arxiv.org/abs/2510.13632)
*Santiago Cuervo,Skyler Seto,Maureen de Seyssel,Richard He Bai,Zijin Gu,Tatiana Likhomanenko,Navdeep Jaitly,Zakaria Aldeneh*

Main category: cs.CL

TL;DR: 本文提出SALAD方法，通过跨模态蒸馏和有针对性的合成数据，以数据高效的方式弥合了语音适应型大型语言模型与文本型模型之间的理解差距，同时减轻遗忘并改善对齐。


<details>
  <summary>Details</summary>
Motivation: 语音适应型大型语言模型在语言理解任务上表现不如其文本型对应模型，存在“文本-语音理解差距”。现有弥合差距的方法要么依赖昂贵的语音合成，要么依赖不可复现的专有数据集，因此需要更数据高效的替代方案来解决遗忘和跨模态错位问题。

Method: 本文将文本-语音理解差距归因于两个因素：适应过程中的文本能力遗忘和语音与文本之间的跨模态错位。基于此，提出了SALAD（Sample-efficient Alignment with Learning through Active selection and cross-modal Distillation）方法，该方法结合了跨模态蒸馏和有针对性的合成数据，以改善对齐并减轻遗忘。

Result: SALAD应用于3B和7B大型语言模型时，在知识、语言理解和推理等广泛领域基准测试中，与强大的开源模型取得了竞争性表现。同时，它使用的公开语料库语音数据量比现有方法少一个数量级以上。

Conclusion: SALAD提供了一种数据高效的解决方案，通过有效解决文本能力遗忘和跨模态错位问题，成功弥合了语音适应型大型语言模型与文本型模型之间的理解差距，并在使用显著更少数据的情况下取得了优异性能。

Abstract: Large Language Models (LLMs) can be adapted to extend their text capabilities
to speech inputs. However, these speech-adapted LLMs consistently underperform
their text-based counterparts--and even cascaded pipelines--on language
understanding tasks. We term this shortfall the text-speech understanding gap:
the performance drop observed when a speech-adapted LLM processes spoken inputs
relative to when the original text-based LLM processes the equivalent text.
Recent approaches to narrowing this gap either rely on large-scale speech
synthesis of text corpora, which is costly and heavily dependent on synthetic
data, or on large-scale proprietary speech datasets, which are not
reproducible. As a result, there remains a need for more data-efficient
alternatives for closing the text-speech understanding gap. In this work, we
analyze the gap as driven by two factors: (i) forgetting of text capabilities
during adaptation, and (ii) cross-modal misalignment between speech and text.
Based on this analysis, we introduce SALAD--Sample-efficient Alignment with
Learning through Active selection and cross-modal Distillation--which combines
cross-modal distillation with targeted synthetic data to improve alignment
while mitigating forgetting. Applied to 3B and 7B LLMs, SALAD achieves
competitive performance with a strong open-weight model across broad-domain
benchmarks in knowledge, language understanding, and reasoning, while training
on over an order of magnitude less speech data from public corpora.

</details>


### [181] [D-SMART: Enhancing LLM Dialogue Consistency via Dynamic Structured Memory And Reasoning Tree](https://arxiv.org/abs/2510.13363)
*Xiang Lei,Qin Li,Min Zhang,Min Zhang*

Main category: cs.CL

TL;DR: D-SMART是一个模型无关框架，通过动态结构化记忆（知识图谱）和推理树，显著提升大型语言模型在多轮对话中的事实和逻辑一致性，并引入新的评估指标。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在多轮对话中常出现事实不一致和逻辑衰退，原因是其依赖静态预训练知识，无法适应性地基于对话历史进行推理。现有方法如RAG和代理工作记忆虽能改善信息召回，但仍受限于静态知识源和预设的单一推理路径，难以在上下文随时间演变时保持响应的一致性。

Method: 本文提出了D-SMART框架，它通过以下两个协同组件使LLM能够构建并基于对话上下文的动态结构化表示进行推理：1) 动态结构化记忆（DSM），渐进式地构建并维护一个权威的、符合OWL标准的对话知识图谱；2) 推理树（RT），通过在图上执行显式且可追溯的多步搜索来进行推理。此外，为了更准确地衡量多轮对话一致性，本文引入了新的基于NLI的评估指标。

Result: 在MT-Bench-101基准上的综合实验表明，D-SMART显著优于现有最先进的基线模型。它将专有模型和开源模型的对话一致性得分提升了超过48%，并使开源模型的质量得分提高了高达10.1%。

Conclusion: D-SMART框架通过引入动态结构化记忆和推理树，成功解决了LLM在多轮对话中保持事实和逻辑一致性的挑战，显著提升了对话质量和一致性，为未来LLM对话系统提供了一个有效且模型无关的解决方案。

Abstract: Large Language Models (LLMs) often exhibit factual inconsistencies and
logical decay in extended, multi-turn dialogues, a challenge stemming from
their reliance on static, pre-trained knowledge and an inability to reason
adaptively over the dialogue history. Prevailing mitigation strategies, such as
Retrieval-Augmented Generation (RAG) and agentic working memories, improve
information recall but still engage with fundamentally static knowledge sources
and follow pre-defined single reasoning path. This hinders their ability to
preserve factual and logical consistency of their responses in multi-turn
dialogues while the context evolves over time. To address this issue, we
propose D-SMART, a model-agnostic framework designed to maintain multi-turn
dialogue consistency by enabling LLMs to build and reason over a dynamic,
structured representation of the conversational context. This is achieved via
two synergistic components: (1) a Dynamic Structured Memory (DSM), which
incrementally constructs and maintains an authoritative, OWL-compliant
knowledge graph of the conversation; and (2) a Reasoning Tree (RT), which
executes inferences as an explicit and traceable multi-step search over the
graph. As the popular-used quality score (judged by GPT-4) can overlook logical
flaws, we introduce new NLI-based metrics to better measure multi-turn dialogue
consistency. Comprehensive experiments on the MT-Bench-101 benchmark show that
D-SMART significantly outperforms state-of-the-art baselines, elevating the
dialogue consistency score by over 48\% for both proprietary and open-source
models, and notably improves the quality score of the latter by up to 10.1\%.

</details>


### [182] [NExT-OMNI: Towards Any-to-Any Omnimodal Foundation Models with Discrete Flow Matching](https://arxiv.org/abs/2510.13721)
*Run Luo,Xiaobo Xia,Lu Wang,Longze Chen,Renke Shan,Jing Luo,Min Yang,Tat-Seng Chua*

Main category: cs.CL

TL;DR: NExT-OMNI是一个开源全模态基础模型，通过离散流范式实现统一的任意模态间理解和生成，解决了现有模型在理解与生成平衡、效率和广阔应用场景上的局限性，并在多模态交互和跨模态检索中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的大多数多模态模型受限于自回归架构或混合/解耦策略，难以平衡理解与生成能力，且其冗余、非集成设计限制了在更广泛场景（如跨模态检索）中的应用，无法作为通用人工智能系统的核心组件。

Method: 该研究引入NExT-OMNI模型，通过离散流范式实现统一建模。它利用度量诱导概率路径和动能最优速度，原生支持任意模态间的理解和生成，并通过简洁统一的表示而非任务解耦设计来扩展应用场景。模型在大规模交织的文本、图像、视频和音频数据上进行训练。

Result: NExT-OMNI在多模态生成和理解基准测试中表现出有竞争力的性能，并在多轮多模态交互和跨模态检索方面超越了之前的统一模型。它还提高了响应效率，并通过简洁统一的表示支持更广泛的应用场景。

Conclusion: NExT-OMNI凭借其架构优势，被视为下一代多模态基础模型，有望在人机交互中发挥关键作用。为促进进一步研究，作者发布了训练细节、数据协议以及代码和模型检查点。

Abstract: Next-generation multimodal foundation models capable of any-to-any
cross-modal generation and multi-turn interaction will serve as core components
of artificial general intelligence systems, playing a pivotal role in
human-machine interaction. However, most existing multimodal models remain
constrained by autoregressive architectures, whose inherent limitations prevent
a balanced integration of understanding and generation capabilities. Although
hybrid and decoupling strategies have been explored to address these tasks
within unified frameworks separately, their redundant, non-integrated designs
limit their applicability to broader scenarios, such as cross-modal
retrieval.In this work, we introduce NExT-OMNI, an open-source omnimodal
foundation model that achieves unified modeling through discrete flow
paradigms. By leveraging metric-induced probability paths and kinetic optimal
velocities, NExT-OMNI natively supports any-to-any understanding and generation
with enhanced response efficiency, while enabling broader application scenarios
through concise unified representations rather than task-decoupled designs.
Trained on large-scale interleaved text, image, video, and audio data,
NExT-OMNI delivers competitive performance on multimodal generation and
understanding benchmarks, while outperforming prior unified models in
multi-turn multimodal interaction and cross-modal retrieval, highlighting its
architectural advantages as a next-generation multimodal foundation model. To
advance further research, we release training details, data protocols, and
open-source both the code and model checkpoints.

</details>


### [183] [Make an Offer They Can't Refuse: Grounding Bayesian Persuasion in Real-World Dialogues without Pre-Commitment](https://arxiv.org/abs/2510.13387)
*Buwei He,Yang Liu,Zhaowei Zhang,Zixia Jia,Huijia Wu,Zhaofeng He,Zilong Zheng,Yipeng Kang*

Main category: cs.CL

TL;DR: 本研究探索了在单轮对话中，通过贝叶斯说服（BP）策略，结合承诺-沟通机制，提升大型语言模型（LLMs）在自然语言中的说服能力，并发现BP策略优于非BP基线，且不同BP变体各有优势，小模型经微调后也能达到大模型性能。


<details>
  <summary>Details</summary>
Motivation: 说服是人类基本社会能力，但AI系统（如LLMs）仍面临挑战。现有研究常忽视信息不对称的战略运用，或依赖于对预承诺的强假设。

Method: 将贝叶斯说服（BP）应用于单轮自然语言对话，以增强LLMs的战略说服能力。引入一个承诺-沟通机制，说服者明确概述信息图式（例如，通过叙述其潜在类型），引导被说服者进行贝叶斯信念更新。评估了两种BP变体：半形式化自然语言（SFNL）BP和完全自然语言（FNL）BP，并将其与天真和强非BP（NBP）基线进行比较。评估框架涵盖了多样化的被说服者（包括不同提示和微调的LLM实例以及人类参与者），以及从专门设计的说服场景到日常情境的各种任务。

Result: 基于LLM的智能体实验结果显示：(1) 贝叶斯说服策略引导的LLMs比非贝叶斯说服基线持续获得更高的说服成功率；(2) SFNL在可信度和逻辑连贯性方面表现更强，而FNL在自然对话中展现出更强的情感共鸣和鲁棒性；(3) 经过监督微调后，较小的模型也能达到与较大模型相当的贝叶斯说服性能。

Conclusion: 贝叶斯说服策略能显著提高LLMs的说服成功率。SFNL和FNL两种变体各有特点，分别在逻辑性和情感性方面表现突出。此外，通过微调，小型LLMs也能实现与大型LLMs相当的贝叶斯说服能力。

Abstract: Persuasion, a fundamental social capability for humans, remains a challenge
for AI systems such as large language models (LLMs). Current studies often
overlook the strategic use of information asymmetry in message design or rely
on strong assumptions regarding pre-commitment. In this work, we explore the
application of Bayesian Persuasion (BP) in natural language within single-turn
dialogue settings, to enhance the strategic persuasion capabilities of LLMs.
Our framework incorporates a commitment-communication mechanism, where the
persuader explicitly outlines an information schema by narrating their
potential types (e.g., honest or dishonest), thereby guiding the persuadee in
performing the intended Bayesian belief update. We evaluate two variants of our
approach: Semi-Formal-Natural-Language (SFNL) BP and Fully-Natural-Language
(FNL) BP, benchmarking them against both naive and strong non-BP (NBP)
baselines within a comprehensive evaluation framework. This framework covers a
diverse set of persuadees -- including LLM instances with varying prompts and
fine-tuning and human participants -- across tasks ranging from specially
designed persuasion scenarios to general everyday situations. Experimental
results on LLM-based agents reveal three main findings: (1) LLMs guided by BP
strategies consistently achieve higher persuasion success rates than NBP
baselines; (2) SFNL exhibits greater credibility and logical coherence, while
FNL shows stronger emotional resonance and robustness in naturalistic
conversations; (3) with supervised fine-tuning, smaller models can attain BP
performance comparable to that of larger models.

</details>


### [184] [Doing Things with Words: Rethinking Theory of Mind Simulation in Large Language Models](https://arxiv.org/abs/2510.13395)
*Agnese Lombardi,Alessandro Lenci*

Main category: cs.CL

TL;DR: 本研究发现，在生成式智能体模型Concordia中，GPT-4在模拟真实世界环境时，难以真正基于信念归因进行行动选择和生成连贯的因果效应，表明其表观的心智理论能力可能源于浅层统计关联而非真实推理。


<details>
  <summary>Details</summary>
Motivation: 语言是人类合作的基础，促进信息交换和行动协调。研究旨在探索生成式智能体模型（GABM）Concordia是否能有效模拟心智理论（ToM），以及GPT-4是否能通过对社会情境的真实推理而非语言记忆来执行任务。

Method: 研究采用生成式智能体模型（GABM）Concordia在模拟的真实世界环境中进行实验，评估GPT-4的心智理论能力，并观察其是否能从社会情境中进行真实推断。

Result: 研究发现GPT-4经常无法基于信念归因选择行动，表明其在先前研究中观察到的ToM能力可能源于浅层统计关联而非真实推理。此外，模型在生成智能体行动的连贯因果效应方面也存在困难，暴露出其在处理复杂社会互动方面的不足。

Conclusion: 这些结果挑战了当前关于大型语言模型（LLMs）中涌现ToM能力的说法，并强调了需要更严格、基于行动的评估框架。

Abstract: Language is fundamental to human cooperation, facilitating not only the
exchange of information but also the coordination of actions through shared
interpretations of situational contexts. This study explores whether the
Generative Agent-Based Model (GABM) Concordia can effectively model Theory of
Mind (ToM) within simulated real-world environments. Specifically, we assess
whether this framework successfully simulates ToM abilities and whether GPT-4
can perform tasks by making genuine inferences from social context, rather than
relying on linguistic memorization. Our findings reveal a critical limitation:
GPT-4 frequently fails to select actions based on belief attribution,
suggesting that apparent ToM-like abilities observed in previous studies may
stem from shallow statistical associations rather than true reasoning.
Additionally, the model struggles to generate coherent causal effects from
agent actions, exposing difficulties in processing complex social interactions.
These results challenge current statements about emergent ToM-like capabilities
in LLMs and highlight the need for more rigorous, action-based evaluation
frameworks.

</details>


### [185] [Investigating Lexical Change through Cross-Linguistic Colexification Patterns](https://arxiv.org/abs/2510.13407)
*Kim Gfeller,Sabine Stoll,Chundra Cathcart,Paul Widmer*

Main category: cs.CL

TL;DR: 本研究利用系统发育比较模型，分析了三大语系中概念对的共词化现象，以揭示意义演变的动态及其影响因素，发现概念关联性、借用性和使用频率是关键预测因子。


<details>
  <summary>Details</summary>
Motivation: 语言意义的持续变化是一个引人入胜的特征，但决定意义如何以及为何演变的因素仍未完全理解。共词化（即用同一词形表达多个不同概念）为跨语言意义变化的动态提供了一个宝贵的窗口。

Method: 研究方法是应用系统发育比较模型，分析了来自南岛语系、印欧语系和乌拉尔语系三大语系的词典数据，以揭示概念对共词化背后的演变动态。评估了三个预测因子：关联性、可借用性和使用频率。

Result: 结果显示，关联性更强的概念对在更大的家族树部分中表现出共词化，并具有较慢的变化速率。相反，更频繁使用和更易于借用的概念对变化更快，且共词化程度较低。研究还发现不同语系之间存在显著差异，表明地域和文化因素可能发挥作用。

Conclusion: 概念对的关联性、使用频率和可借用性是影响共词化演变的关键因素。紧密相关的概念对倾向于更稳定地共词化，而高频率和易借用的概念对变化更快。此外，地域和文化因素在不同语系的共词化动态中也扮演着重要角色。

Abstract: One of the most intriguing features of language is its constant change, with
ongoing shifts in how meaning is expressed. Despite decades of research, the
factors that determine how and why meanings evolve remain only partly
understood. Colexification -- the phenomenon of expressing multiple distinct
concepts using the same word form -- serves as a valuable window onto the
dynamics of meaning change across languages. Here, we apply phylogenetic
comparative models to dictionary data from three language families,
Austronesian, Indo-European, and Uralic, in order to shed light on the
evolutionary dynamics underlying the colexification of concept pairs. We assess
the effects of three predictors: associativity, borrowability, and usage
frequency. Our results show that more closely related concept pairs are
colexified across a larger portion of the family tree and exhibit slower rates
of change. In contrast, concept pairs that are more frequent and more prone to
borrowing tend to change more rapidly and are less often colexified. We also
find considerable differences between the language families under study,
suggesting that areal and cultural factors may play a role.

</details>


### [186] [Evaluating Arabic Large Language Models: A Survey of Benchmarks, Methods, and Gaps](https://arxiv.org/abs/2510.13430)
*Ahmed Alzubaidi,Shaikha Alsuwaidi,Basma El Amel Boussaha,Leen AlQadi,Omar Alkaabi,Mohammed Alyafeai,Hamza Alobeidli,Hakim Hacid*

Main category: cs.CL

TL;DR: 该综述首次系统回顾了阿拉伯语大型语言模型（LLM）的基准测试，分析了40多个评估基准，提出了分类法，并指出了现有基准的进展、差距和未来发展建议。


<details>
  <summary>Details</summary>
Motivation: 提供阿拉伯语LLM基准的首次系统综述，为阿拉伯语自然语言处理（NLP）研究人员提供全面的参考，深入了解基准方法、可复现性标准和评估指标。

Method: 分析了40多个跨NLP任务、知识领域、文化理解和专业能力的评估基准。提出了一个将基准分为知识、NLP任务、文化和方言、目标特定评估四个类别的分类法。研究了原生收集、翻译和合成生成三种主要方法及其在真实性、规模和成本方面的权衡。

Result: 分析显示基准多样性取得了显著进展，但也发现了关键差距：有限的时间评估、多轮对话评估不足以及翻译数据集中存在的文化错位。讨论了不同数据收集方法的优缺点。

Conclusion: 这项工作为阿拉伯语NLP研究人员提供了全面的参考，提供了对基准方法、可复现性标准和评估指标的见解，并为未来的发展提供了建议。

Abstract: This survey provides the first systematic review of Arabic LLM benchmarks,
analyzing 40+ evaluation benchmarks across NLP tasks, knowledge domains,
cultural understanding, and specialized capabilities. We propose a taxonomy
organizing benchmarks into four categories: Knowledge, NLP Tasks, Culture and
Dialects, and Target-Specific evaluations. Our analysis reveals significant
progress in benchmark diversity while identifying critical gaps: limited
temporal evaluation, insufficient multi-turn dialogue assessment, and cultural
misalignment in translated datasets. We examine three primary approaches:
native collection, translation, and synthetic generation discussing their
trade-offs regarding authenticity, scale, and cost. This work serves as a
comprehensive reference for Arabic NLP researchers, providing insights into
benchmark methodologies, reproducibility standards, and evaluation metrics
while offering recommendations for future development.

</details>


### [187] [Beyond Single-Reward: Multi-Pair, Multi-Perspective Preference Optimization for Machine Translation](https://arxiv.org/abs/2510.13434)
*Hao Wang,Linlong Xu,Heng Liu,Yangyang Liu,Xiaohu Zhao,Bo Zeng,Liangying Shao,Longyue Wang,Weihua Luo,Kaifu Zhang*

Main category: cs.CL

TL;DR: M^2PO是一种多对、多视角偏好优化框架，通过整合多视角奖励引擎和多对构建策略，解决了机器翻译中DPO方法奖励信号缺陷和数据利用效率低下的问题，从而显著提升了翻译质量。


<details>
  <summary>Details</summary>
Motivation: 现有DPO（直接偏好优化）方法在机器翻译中存在两大挑战：1) 质量评估（QE）模型的奖励信号存在缺陷，尤其忽略了翻译幻觉等关键错误；2) 数据利用效率低下，仅选择单一的胜负对，丢弃了宝贵的学习信号。

Method: M^2PO框架引入了：1) 一个多视角奖励引擎，结合了新的幻觉惩罚（用于事实性）和创新的动态质量分数（自适应融合外部评估与模型自身判断），以创建更稳健的奖励信号；2) 一个多对构建策略，系统性地从所有翻译候选中创建全面的偏好对集合，以丰富模型学习的质量权衡范围。

Result: 在具有挑战性的WMT21-22基准测试中，M^2PO显著优于现有偏好优化方法，并展现出与领先的专有大型语言模型高度竞争的性能。

Conclusion: M^2PO通过从更丰富的质量权衡谱中学习，能够生成更稳健、更忠实的翻译。

Abstract: Direct Preference Optimization (DPO) is a powerful paradigm for aligning
Large Language Models (LLMs) to human preferences in Machine Translation (MT),
but current methods are hindered by two fundamental challenges: (1) flawed
reward signals from Quality Estimation (QE) models that overlook critical
errors like translation hallucination, and (2) inefficient data utilization
that discards valuable learning signals by selecting only a single win-loss
pair. To address these limitations, we introduce M^2PO: Multi-Pair,
Multi-Perspective Preference Optimization. Our framework integrates a
multi-perspective reward engine that creates a more robust signal by combining
two key viewpoints: a new hallucination penalty for factuality, and an
innovative dynamic quality score that adaptively fuses external evaluations
with the model's own evolving judgment. This is synergistically paired with a
multi-pair construction strategy that systematically creates a comprehensive
set of preference pairs from the entire pool of translation candidates. This
synergistic approach ensures the model learns from a richer spectrum of quality
trade-offs, leading to more robust and faithful translations. On challenging
WMT21-22 benchmarks, M^2PO substantially outperforms existing preference
optimization methods and demonstrates highly competitive performance against
leading proprietary LLMs.

</details>


### [188] [Attention Illuminates LLM Reasoning: The Preplan-and-Anchor Rhythm Enables Fine-Grained Policy Optimization](https://arxiv.org/abs/2510.13554)
*Yang Li,Zhichen Dong,Yuhan Sun,Weixun Wang,Shaopan Xiong,Yijia Luo,Jiashun Liu,Han Lu,Jiamang Wang,Wenbo Su,Bo Zheng,Junchi Yan*

Main category: cs.CL

TL;DR: 本文通过分析大型语言模型（LLMs）的注意力机制，揭示了其内部推理模式，并基于此提出了新的强化学习（RL）策略，实现了LLM推理优化性能的提升和透明化。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的推理模式不透明，且传统的强化学习（RL）在整个生成过程中均匀分配信用，模糊了关键步骤和常规步骤之间的区别。

Method: 1. 将注意力视为LLM内部逻辑的机制蓝图。2. 区分局部和全局聚焦的注意力头。3. 提出两个量化指标：窗口平均注意力距离（WAAD）和未来注意力影响（FAI）。4. 识别出“预规划-锚定”推理机制。5. 基于这些洞察，引入了三种新颖的强化学习策略，对关键节点（预规划token、锚定token及其时间耦合）进行动态、有针对性的信用分配。

Result: 1. 局部聚焦的注意力头在对角线附近呈现锯齿状模式，表明短语块。2. 全局聚焦的注意力头揭示了对未来token具有广泛下游影响的token。3. WAAD和FAI共同揭示了“预规划-锚定”机制，即模型首先进行长程上下文引用以生成引导token，随后立即出现或同时出现组织后续推理的语义锚定token。4. 新的RL策略在各种推理任务中显示出一致的性能提升。

Conclusion: 通过将优化与模型的内在推理节奏对齐，可以将不透明的优化转变为结构感知过程，从而实现LLM推理更透明和有效的优化。

Abstract: The reasoning pattern of Large language models (LLMs) remains opaque, and
Reinforcement learning (RL) typically applies uniform credit across an entire
generation, blurring the distinction between pivotal and routine steps. This
work positions attention as a privileged substrate that renders the internal
logic of LLMs legible, not merely as a byproduct of computation, but as a
mechanistic blueprint of reasoning itself. We first distinguish attention heads
between locally and globally focused information processing and reveal that
locally focused heads produce a sawtooth pattern near the diagonal indicating
phrasal chunks, while globally focused heads expose tokens that exert broad
downstream influence over future tokens. We formalize these with two metrics:
1) Windowed Average Attention Distance, which measures the extent of backward
attention within a clipped window; 2) Future Attention Influence, which
quantifies a token's global importance as the average attention it receives
from subsequent tokens. Taken together, these signals reveal a recurring
preplan-and-anchor mechanism, where the model first performs a long-range
contextual reference to generate an introductory token, which is immediately
followed by or coincides with a semantic anchor token that organizes subsequent
reasoning. Leveraging these insights, we introduce three novel RL strategies
that dynamically perform targeted credit assignment to critical nodes (preplan
tokens, anchor tokens, and their temporal coupling) and show consistent
performance gains across various reasoning tasks. By aligning optimization with
the model's intrinsic reasoning rhythm, we aim to transform opaque optimization
into an actionable structure-aware process, hoping to offer a potential step
toward more transparent and effective optimization of LLM reasoning.

</details>


### [189] [Sparse Subnetwork Enhancement for Underrepresented Languages in Large Language Models](https://arxiv.org/abs/2510.13580)
*Daniil Gurgurov,Josef van Genabith,Simon Ostermann*

Main category: cs.CL

TL;DR: 该研究提出了一种通过针对性微调语言特异性子网络来提升大型语言模型在低资源语言中单语能力的方法，同时保持其通用性能，且仅更新少量参数。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在不同语言间表现不均，高资源语言与低资源语言之间存在显著性能差距。

Method: 该方法使用“语言激活概率熵”（Language Activation Probability Entropy, LAPE）识别语言特异性神经元，并仅对与这些神经元相关的权重（即专用子网络）进行目标语言数据的微调。

Result: 在12种中低资源语言上，Llama-3.1-8B和Mistral-Nemo-12B的实验表明，该方法持续优于全量微调、仅FFN微调、LoRA适应和随机子集微调等基线方法，且仅更新了模型参数的1%。此外，还观察到有利的训练动态增强、跨语言表征对齐以及系统性的权重更新变化。

Conclusion: 该框架提供了一种经济高效的途径，用于将最先进的模型适应到低资源语言。为促进未来研究，作者发布了100多种语言的语言特异性神经元识别结果及其适应管道。

Abstract: Large language models exhibit uneven performance across languages, with
substantial gaps between high- and low-resource languages. We present a
framework for enhancing monolingual capabilities of LLMs in underrepresented
languages while preserving their general-purpose performance through targeted
fine-tuning of language-specific subnetworks. Our approach identifies
language-specific neurons using Language Activation Probability Entropy and
fine-tunes only the weights associated with these neurons, a dedicated
subnetwork, on target-language data. Experiments on Llama-3.1-8B and
Mistral-Nemo-12B across 12 mid- and low-resource languages demonstrate that our
method consistently outperforms full fine-tuning, FFN-only fine-tuning, LoRA
adaptation, and random subset fine-tuning baselines while efficiently updating
only up to 1% of model parameters. Beyond performance improvements, we observe
enhanced favorable training dynamics, cross-lingual representational alignment,
and systematic weight update changes. To facilitate future research, we release
language-specific neuron identifications for over 100 languages as well as our
adaptation pipeline, offering a cost-effective pathway for adapting
state-of-the-art models to underrepresented languages.

</details>


### [190] [FreshTab: Sourcing Fresh Data for Table-to-Text Generation Evaluation](https://arxiv.org/abs/2510.13598)
*Kristýna Onderková,Ondřej Plátek,Zdeněk Kasner,Ondřej Dušek*

Main category: cs.CL

TL;DR: FreshTab是一个用于解决大型语言模型数据污染和领域不平衡问题的即时表格到文本基准生成器，支持多语言评估。


<details>
  <summary>Details</summary>
Motivation: 现有的表格到文本生成任务面临数据分析精度挑战，并且其评估受到大型语言模型训练数据污染以及领域不平衡问题的影响。此外，非英语表格到文本数据集非常有限。

Method: FreshTab通过从维基百科即时生成表格到文本基准来解决大型语言模型数据污染问题，并实现领域敏感评估。它还按需收集多语言数据集（例如，英语、德语、俄语和法语）。

Result: 研究发现，大型语言模型从FreshTab收集的最新表格生成的见解在自动评估指标上表现明显更差，但这种差异并未体现在大型语言模型和人类评估中。所有评估中都可见领域效应，表明领域平衡的基准更具挑战性。

Conclusion: FreshTab通过提供即时、多语言和领域敏感的基准，有效地应对了大型语言模型数据污染和领域不平衡问题。研究结果表明，领域平衡的基准对模型提出了更高的要求，且自动指标与人类/LLM评估之间存在差异。

Abstract: Table-to-text generation (insight generation from tables) is a challenging
task that requires precision in analyzing the data. In addition, the evaluation
of existing benchmarks is affected by contamination of Large Language Model
(LLM) training data as well as domain imbalance. We introduce FreshTab, an
on-the-fly table-to-text benchmark generation from Wikipedia, to combat the LLM
data contamination problem and enable domain-sensitive evaluation. While
non-English table-to-text datasets are limited, FreshTab collects datasets in
different languages on demand (we experiment with German, Russian and French in
addition to English). We find that insights generated by LLMs from recent
tables collected by our method appear clearly worse by automatic metrics, but
this does not translate into LLM and human evaluations. Domain effects are
visible in all evaluations, showing that a~domain-balanced benchmark is more
challenging.

</details>


### [191] [MemoTime: Memory-Augmented Temporal Knowledge Graph Enhanced Large Language Model Reasoning](https://arxiv.org/abs/2510.13614)
*Xingyu Tan,Xiaoyang Wang,Qing Liu,Xiwei Xu,Xin Yuan,Liming Zhu,Wenjie Zhang*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）在处理复杂时间推理时表现不佳。MemoTime是一个记忆增强的时间知识图谱（TKG）框架，通过结构化接地、递归推理和持续经验学习，显著提升了LLMs的时间问答能力，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: LLMs在涉及多实体、复合操作符和演变事件序列的时间理解方面存在困难。现有的基于TKG的LLM推理方法面临四大挑战：多跳推理中的时间忠实性、多实体时间同步、对多样化时间操作符的适应性检索，以及经验重用以提高稳定性和效率。

Method: MemoTime框架通过以下方式增强LLM推理：1) 将复杂时间问题分解为分层的“时间之树”（Tree of Time），实现操作符感知推理，强制单调时间戳并统一约束多实体；2) 动态证据检索层，自适应选择操作符特定的检索策略；3) 自进化的经验记忆，存储验证过的推理轨迹、工具决策和子问题嵌入，以实现跨类型重用。

Result: 在多个时间问答基准测试中，MemoTime取得了整体最先进的结果，性能优于强大的基线模型高达24.0%。此外，MemoTime使小型模型（如Qwen3-4B）能够达到与GPT-4-Turbo相当的推理性能。

Conclusion: MemoTime通过结合结构化接地、递归推理和持续经验学习，有效解决了LLMs在复杂时间推理中遇到的挑战。该框架不仅显著提升了LLMs的时间问答能力，还提高了推理效率，使小型模型也能达到高性能。

Abstract: Large Language Models (LLMs) have achieved impressive reasoning abilities,
but struggle with temporal understanding, especially when questions involve
multiple entities, compound operators, and evolving event sequences. Temporal
Knowledge Graphs (TKGs), which capture vast amounts of temporal facts in a
structured format, offer a reliable source for temporal reasoning. However,
existing TKG-based LLM reasoning methods still struggle with four major
challenges: maintaining temporal faithfulness in multi-hop reasoning, achieving
multi-entity temporal synchronization, adapting retrieval to diverse temporal
operators, and reusing prior reasoning experience for stability and efficiency.
To address these issues, we propose MemoTime, a memory-augmented temporal
knowledge graph framework that enhances LLM reasoning through structured
grounding, recursive reasoning, and continual experience learning. MemoTime
decomposes complex temporal questions into a hierarchical Tree of Time,
enabling operator-aware reasoning that enforces monotonic timestamps and
co-constrains multiple entities under unified temporal bounds. A dynamic
evidence retrieval layer adaptively selects operator-specific retrieval
strategies, while a self-evolving experience memory stores verified reasoning
traces, toolkit decisions, and sub-question embeddings for cross-type reuse.
Comprehensive experiments on multiple temporal QA benchmarks show that MemoTime
achieves overall state-of-the-art results, outperforming the strong baseline by
up to 24.0%. Furthermore, MemoTime enables smaller models (e.g., Qwen3-4B) to
achieve reasoning performance comparable to that of GPT-4-Turbo.

</details>


### [192] [How Sampling Affects the Detectability of Machine-written texts: A Comprehensive Study](https://arxiv.org/abs/2510.13681)
*Matthieu Dubois,François Yvon,Pablo Piantanida*

Main category: cs.CL

TL;DR: 研究发现，当前LLM文本检测器对解码策略的微小变化非常不鲁棒，即使AUROC从接近完美降至1%，揭示了现有评估协议的不足。


<details>
  <summary>Details</summary>
Motivation: LLM生成的文本日益普及且难以与人类文本区分，自动文本检测研究受到关注。许多检测器报告近乎完美的准确率（AUROC > 99%），但这些结果通常基于固定的生成设置，其对解码策略变化的鲁棒性尚不明确。

Method: 系统性地考察了基于采样的解码策略（如温度、top-p或nucleus采样）如何影响LLM文本的可检测性，重点关注模型(子)词级别分布的细微变化对检测性能的影响。为此，发布了一个包含37种解码配置的大规模数据集、代码和评估框架。

Result: 即使解码参数的微小调整也能严重损害检测器的准确性，在某些设置下，AUROC从接近完美的水平骤降至1%。

Conclusion: 研究结果揭示了当前检测方法的关键盲点，并强调了需要更全面的评估协议。为促进未来研究，提供了大规模数据集、代码和评估框架。

Abstract: As texts generated by Large Language Models (LLMs) are ever more common and
often indistinguishable from human-written content, research on automatic text
detection has attracted growing attention. Many recent detectors report
near-perfect accuracy, often boasting AUROC scores above 99\%. However, these
claims typically assume fixed generation settings, leaving open the question of
how robust such systems are to changes in decoding strategies. In this work, we
systematically examine how sampling-based decoding impacts detectability, with
a focus on how subtle variations in a model's (sub)word-level distribution
affect detection performance. We find that even minor adjustments to decoding
parameters - such as temperature, top-p, or nucleus sampling - can severely
impair detector accuracy, with AUROC dropping from near-perfect levels to 1\%
in some settings. Our findings expose critical blind spots in current detection
methods and emphasize the need for more comprehensive evaluation protocols. To
facilitate future research, we release a large-scale dataset encompassing 37
decoding configurations, along with our code and evaluation framework
https://github.com/BaggerOfWords/Sampling-and-Detection

</details>


### [193] [GAPS: A Clinically Grounded, Automated Benchmark for Evaluating AI Clinicians](https://arxiv.org/abs/2510.13734)
*Xiuyuan Chen,Tao Sun,Dexin Su,Ailing Yu,Junwei Liu,Zhe Chen,Gangzeng Jin,Xin Wang,Jingnan Liu,Hansong Xiao,Hualei Zhou,Dongjie Tao,Chunxiao Guo,Minghui Yang,Yuan Xia,Jing Zhao,Qianrui Fan,Yanyun Wang,Shuai Zhen,Kezhong Chen,Jun Wang,Zewen Sun,Heng Zhao,Tian Guan,Shaodong Wang,Geyun Chang,Jiaming Deng,Hongchengcheng Chen,Kexin Feng,Ruzhen Li,Jiayi Geng,Changtai Zhao,Jun Wang,Guihu Lin,Peihao Li,Liqi Liu,Peng Wei,Jian Wang,Jinjie Gu,Ping Wang,Fan Yang*

Main category: cs.CL

TL;DR: 本文提出GAPS框架及一套全自动、基于指南的评估流程，旨在更全面、可扩展地评估AI临床医生系统在认知深度、答案完整性、鲁棒性和安全性方面的表现，并揭示了现有模型的主要缺陷。


<details>
  <summary>Details</summary>
Motivation: 当前的AI临床医生系统基准测试（如多项选择题或手动评分）无法充分捕捉真实临床实践所需的深度、鲁棒性和安全性，且存在可扩展性和主观性限制。

Method: 引入GAPS框架，从Grounded（认知深度）、Adequacy（答案完整性）、Perturbation（鲁棒性）和Safety（安全性）四个维度进行评估。开发了一个全自动、以指南为基础的评估流程：包括证据邻域构建、双图和树表示、自动生成G级别问题、由DeepResearch代理（模拟GRADE-一致、PICO驱动的证据审查）合成评分标准，并由大型语言模型（LLM）评审团进行评分。通过验证确认了自动生成问题的高质量和与临床医生判断的一致性。

Result: 对现有最先进模型的评估显示了关键的失败模式：随着推理深度的增加（G轴），模型性能急剧下降；模型在答案完整性方面表现不佳（A轴）；它们极易受到对抗性扰动（P轴）以及某些安全问题（S轴）的影响。

Conclusion: 这种自动化、以临床为基础的方法为严格评估AI临床医生系统提供了一种可复现且可扩展的手段，并能指导其发展，使其更安全、更可靠地应用于临床实践。

Abstract: Current benchmarks for AI clinician systems, often based on multiple-choice
exams or manual rubrics, fail to capture the depth, robustness, and safety
required for real-world clinical practice. To address this, we introduce the
GAPS framework, a multidimensional paradigm for evaluating \textbf{G}rounding
(cognitive depth), \textbf{A}dequacy (answer completeness),
\textbf{P}erturbation (robustness), and \textbf{S}afety. Critically, we
developed a fully automated, guideline-anchored pipeline to construct a
GAPS-aligned benchmark end-to-end, overcoming the scalability and subjectivity
limitations of prior work. Our pipeline assembles an evidence neighborhood,
creates dual graph and tree representations, and automatically generates
questions across G-levels. Rubrics are synthesized by a DeepResearch agent that
mimics GRADE-consistent, PICO-driven evidence review in a ReAct loop. Scoring
is performed by an ensemble of large language model (LLM) judges. Validation
confirmed our automated questions are high-quality and align with clinician
judgment. Evaluating state-of-the-art models on the benchmark revealed key
failure modes: performance degrades sharply with increased reasoning depth
(G-axis), models struggle with answer completeness (A-axis), and they are
highly vulnerable to adversarial perturbations (P-axis) as well as certain
safety issues (S-axis). This automated, clinically-grounded approach provides a
reproducible and scalable method for rigorously evaluating AI clinician systems
and guiding their development toward safer, more reliable clinical practice.

</details>


### [194] [Assessing Web Search Credibility and Response Groundedness in Chat Assistants](https://arxiv.org/abs/2510.13749)
*Ivan Vykopal,Matúš Pikuliak,Simon Ostermann,Marián Šimko*

Main category: cs.CL

TL;DR: 本文提出了一种评估集成网络搜索的聊天助手在事实核查方面的行为方法，重点关注来源可信度和回答的可靠性，并比较了GPT-4o、GPT-5、Perplexity和Qwen Chat。


<details>
  <summary>Details</summary>
Motivation: 聊天助手集成网络搜索功能旨在提供更可靠的答案，但也存在放大来自低可信度来源的错误信息的风险。

Method: 研究引入了一种评估助手网络搜索行为的新方法，侧重于来源可信度及其回答与引用来源的一致性。该方法使用100个声明，涵盖5个易受错误信息影响的主题，评估了GPT-4o、GPT-5、Perplexity和Qwen Chat。

Result: 研究发现不同助手之间存在差异：Perplexity的来源可信度最高，而GPT-4o在敏感话题上引用非可信来源的情况有所增加。这是首次对常用聊天助手在事实核查行为方面的系统比较。

Conclusion: 这项工作为评估高风险信息环境中的AI系统（特别是其事实核查行为）奠定了基础。

Abstract: Chat assistants increasingly integrate web search functionality, enabling
them to retrieve and cite external sources. While this promises more reliable
answers, it also raises the risk of amplifying misinformation from
low-credibility sources. In this paper, we introduce a novel methodology for
evaluating assistants' web search behavior, focusing on source credibility and
the groundedness of responses with respect to cited sources. Using 100 claims
across five misinformation-prone topics, we assess GPT-4o, GPT-5, Perplexity,
and Qwen Chat. Our findings reveal differences between the assistants, with
Perplexity achieving the highest source credibility, whereas GPT-4o exhibits
elevated citation of non-credibility sources on sensitive topics. This work
provides the first systematic comparison of commonly used chat assistants for
fact-checking behavior, offering a foundation for evaluating AI systems in
high-stakes information environments.

</details>


### [195] [The Mechanistic Emergence of Symbol Grounding in Language Models](https://arxiv.org/abs/2510.13796)
*Shuyu Wu,Ziqiao Ma,Xiaoxi Luo,Yidong Huang,Josue Torres-Fonseca,Freda Shi,Joyce Chai*

Main category: cs.CL

TL;DR: 本文通过机械和因果分析，揭示了符号接地现象在大型语言模型内部计算中的出现位置（中间层）和实现机制（注意力头聚合环境信息）。


<details>
  <summary>Details</summary>
Motivation: 符号接地（即符号如何通过连接真实世界经验获得意义）在大型（视觉-）语言模型中可能自发出现，但其具体出现位置和驱动机制尚不明确。

Method: 引入一个受控评估框架，通过机械和因果分析系统地追踪符号接地在模型内部计算中如何产生。

Result: 研究发现，接地现象集中在模型的中间层计算中，并通过“聚合机制”实现，即注意力头聚合环境基础信息以支持语言形式的预测。此现象在多模态对话和不同架构（Transformer和状态空间模型）中均可复现，但在单向LSTM中则不出现。

Conclusion: 研究提供了行为和机械证据，证明符号接地可以在语言模型中自发出现，这对预测和潜在控制生成内容的可靠性具有实际意义。

Abstract: Symbol grounding (Harnad, 1990) describes how symbols such as words acquire
their meanings by connecting to real-world sensorimotor experiences. Recent
work has shown preliminary evidence that grounding may emerge in
(vision-)language models trained at scale without using explicit grounding
objectives. Yet, the specific loci of this emergence and the mechanisms that
drive it remain largely unexplored. To address this problem, we introduce a
controlled evaluation framework that systematically traces how symbol grounding
arises within the internal computations through mechanistic and causal
analysis. Our findings show that grounding concentrates in middle-layer
computations and is implemented through the aggregate mechanism, where
attention heads aggregate the environmental ground to support the prediction of
linguistic forms. This phenomenon replicates in multimodal dialogue and across
architectures (Transformers and state-space models), but not in unidirectional
LSTMs. Our results provide behavioral and mechanistic evidence that symbol
grounding can emerge in language models, with practical implications for
predicting and potentially controlling the reliability of generation.

</details>


### [196] [Confidence-Based Response Abstinence: Improving LLM Trustworthiness via Activation-Based Uncertainty Estimation](https://arxiv.org/abs/2510.13750)
*Zhiqi Huang,Vivek Datla,Chenyang Zhu,Alfy Samuel,Daben Liu,Anoop Kumar,Ritesh Soni*

Main category: cs.CL

TL;DR: 该论文提出了一种基于前馈网络（FFN）激活值的方法，用于检索增强生成（RAG）系统中的置信度估计，以更好地与大型语言模型（LLM）输出的正确性对齐，特别适用于高风险领域。


<details>
  <summary>Details</summary>
Motivation: 在高风险领域（如金融和医疗保健），不正确答案的成本远高于不回答问题的成本，因此，RAG系统中准确的置信度估计至关重要。

Method: 该方法通过利用原始前馈网络（FFN）激活值作为自回归信号来扩展先前的UQ方法，避免了令牌对数和概率在投影和softmax归一化后固有的信息损失。置信度预测被建模为序列分类任务，并通过Huber损失项进行正则化训练，以提高对噪声监督的鲁棒性。研究还发现，仅使用第16层的激活值即可在保持准确性的同时减少响应延迟。

Result: 在真实的金融行业客户支持场景中，该方法优于强大的基线，并在严格的延迟约束下保持高准确性。在Llama 3.1 8B模型上的实验表明，仅使用第16层的激活值可以保持准确性并显著降低响应延迟。

Conclusion: 基于激活值的置信度建模为可信赖的RAG部署提供了一条可扩展且架构感知（architecture-aware）的路径。

Abstract: We propose a method for confidence estimation in retrieval-augmented
generation (RAG) systems that aligns closely with the correctness of large
language model (LLM) outputs. Confidence estimation is especially critical in
high-stakes domains such as finance and healthcare, where the cost of an
incorrect answer outweighs that of not answering the question. Our approach
extends prior uncertainty quantification methods by leveraging raw feed-forward
network (FFN) activations as auto-regressive signals, avoiding the information
loss inherent in token logits and probabilities after projection and softmax
normalization. We model confidence prediction as a sequence classification
task, and regularize training with a Huber loss term to improve robustness
against noisy supervision. Applied in a real-world financial industry
customer-support setting with complex knowledge bases, our method outperforms
strong baselines and maintains high accuracy under strict latency constraints.
Experiments on Llama 3.1 8B model show that using activations from only the
16th layer preserves accuracy while reducing response latency. Our results
demonstrate that activation-based confidence modeling offers a scalable,
architecture-aware path toward trustworthy RAG deployment.

</details>


### [197] [Breadcrumbs Reasoning: Memory-Efficient Reasoning with Compression Beacons](https://arxiv.org/abs/2510.13797)
*Giovanni Monea,Yair Feldman,Shankar Padmanabhan,Kianté Brantley,Yoav Artzi*

Main category: cs.CL

TL;DR: 为解决大型语言模型长上下文推理中KV缓存线性增长导致的内存和计算瓶颈，本文提出一种周期性压缩生成KV缓存的方法，通过学习特殊令牌进行压缩，并结合蒸馏和强化学习进行训练，实现了更好的内存-准确性权衡。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在长上下文推理中的可扩展性受到Transformer键值（KV）缓存线性增长的严重限制，这会带来显著的内存和计算成本。研究者认为，随着模型生成推理令牌，过去生成令牌的信息价值会降低，从而为压缩创造了机会。

Method: 本文提出周期性地压缩生成KV缓存，使用一个学习到的、专用令牌进行压缩，并驱逐已压缩的条目。模型通过修改后的联合蒸馏和强化学习（RL）框架进行训练，以执行这种压缩。该训练方法利用RL输出进行蒸馏，从而最大限度地减少了传统RL过程的开销。

Result: 经验证，与没有缓存压缩的模型和无需训练的压缩技术相比，该方法在内存-准确性帕累托前沿上取得了优越的表现。

Conclusion: 通过学习到的周期性KV缓存压缩方法，并结合创新的蒸馏与强化学习训练框架，可以有效解决大型语言模型在长上下文推理中因KV缓存增长带来的可扩展性问题，显著提升内存效率同时保持准确性。

Abstract: The scalability of large language models for long-context reasoning is
severely constrained by the linear growth of their Transformer key-value cache,
which incurs significant memory and computational costs. We posit that as a
model generates reasoning tokens, the informational value of past generated
tokens diminishes, creating an opportunity for compression. In this work, we
propose to periodically compress the generation KV cache with a learned,
special-purpose token and evict compressed entries. We train the model to
perform this compression via a modified joint distillation and reinforcement
learning (RL) framework. Our training method minimizes overhead over the
conventional RL process, as it leverages RL outputs for distillation.
Empirically, our method achieves a superior memory-accuracy Pareto frontier
compared to both the model without cache compression and training-free
compression techniques.

</details>


### [198] [BRIEF-Pro: Universal Context Compression with Short-to-Long Synthesis for Fast and Accurate Multi-Hop Reasoning](https://arxiv.org/abs/2510.13799)
*Jia-Chen Gu,Junyi Zhang,Di Wu,Yuankai Li,Kai-Wei Chang,Nanyun Peng*

Main category: cs.CL

TL;DR: BRIEF-Pro是一种通用的轻量级压缩器，用于检索增强生成（RAG），能将检索到的文档压缩成简洁摘要，从而提高多跳问答任务的性能并显著降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 在RAG处理复杂任务时，扩展的上下文虽然信息更丰富，但也带来了更高的延迟和模型认知负荷，尤其对于复杂的多跳问题，这是一个瓶颈。

Method: 本文提出了BRIEF-Pro，一个通用的轻量级抽象压缩器。它通过使用相对较短的上下文（少于1k词）作为种子数据进行训练，以对超过10k词的扩展上下文进行压缩。BRIEF-Pro还允许用户通过指定句子数量来灵活控制摘要长度。

Result: 在四个开放域多跳问答数据集上的实验表明，BRIEF-Pro生成了更简洁、更相关的摘要，提升了小型、大型及专有语言模型的性能。使用70B阅读器模型时，BRIEF-Pro的32倍压缩比LongLLMLingua的9倍压缩平均将QA性能提高了4.67%，同时仅需其23%的计算开销。

Conclusion: BRIEF-Pro成功解决了RAG中扩展上下文的瓶颈问题，通过提供高效、相关的摘要，显著提升了多跳问答任务的性能和计算效率。

Abstract: As retrieval-augmented generation (RAG) tackles complex tasks, increasingly
expanded contexts offer richer information, but at the cost of higher latency
and increased cognitive load on the model. To mitigate this bottleneck,
especially for intricate multi-hop questions, we introduce BRIEF-Pro. It is a
universal, lightweight compressor that distills relevant evidence for a given
query from retrieved documents into a concise summary for seamless integration
into in-context RAG. Using seed data consisting of relatively short contexts
(fewer than 1k words), BRIEF-Pro is trained to perform abstractive compression
of extended contexts exceeding 10k words across a wide range of scenarios.
Furthermore, BRIEF-Pro offers flexible user control over summary length by
allowing users to specify the desired number of sentences. Experiments on four
open-domain multi-hop question-answering datasets show that BRIEF-Pro generates
more concise and relevant summaries, enhancing performance across small, large,
and proprietary language models. With the 70B reader model, 32x compression by
BRIEF-Pro improves QA performance by 4.67% on average over LongLLMLingua's 9x,
while requiring only 23% of its computational overhead.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [199] [Learning to Grasp Anything by Playing with Random Toys](https://arxiv.org/abs/2510.12866)
*Dantong Niu,Yuvan Sharma,Baifeng Shi,Rachel Ding,Matteo Gioia,Haoru Xue,Henry Tsai,Konstantinos Kallidromitis,Anirudh Pai,Shankar Shastry,Trevor Darrell,Jitendra Malik,Roei Herzig*

Main category: cs.RO

TL;DR: 受儿童学习启发，本研究表明机器人可以通过训练由四种基本形状（球体、长方体、圆柱体、环）组成的随机组装“玩具”，利用提出的检测池化机制实现对真实世界物体的高泛化性抓取，且无需特定领域数据。


<details>
  <summary>Details</summary>
Motivation: 机器人操作策略难以泛化到新颖物体，限制了其在现实世界中的应用。认知科学表明，儿童通过掌握少量简单玩具来发展可泛化的灵巧操作技能，并将其应用于更复杂的物品。本研究受此启发，探索机器人是否也能实现类似的泛化能力。

Method: 通过使用由球体、长方体、圆柱体和环四种基本形状原语随机组装的“玩具”进行训练，使机器人学习可泛化的抓取。关键在于引入了一种由所提出的检测池化机制产生的以物体为中心的视觉表示。在仿真和物理机器人上进行评估，并研究训练玩具的数量、多样性以及每个玩具的演示次数如何影响零样本泛化性能。

Result: 机器人能够学习可泛化的抓取，对真实世界物体表现出强大的零样本泛化能力。在YCB数据集上的物理机器人抓取成功率达到67%，优于依赖大量领域内数据的现有最先进方法。研究发现，泛化的关键在于提出的检测池化机制所诱导的以物体为中心的视觉表示。

Conclusion: 这项工作为机器人操作中可扩展和可泛化的学习提供了一条有前景的途径。

Abstract: Robotic manipulation policies often struggle to generalize to novel objects,
limiting their real-world utility. In contrast, cognitive science suggests that
children develop generalizable dexterous manipulation skills by mastering a
small set of simple toys and then applying that knowledge to more complex
items. Inspired by this, we study if similar generalization capabilities can
also be achieved by robots. Our results indicate robots can learn generalizable
grasping using randomly assembled objects that are composed from just four
shape primitives: spheres, cuboids, cylinders, and rings. We show that training
on these "toys" enables robust generalization to real-world objects, yielding
strong zero-shot performance. Crucially, we find the key to this generalization
is an object-centric visual representation induced by our proposed detection
pooling mechanism. Evaluated in both simulation and on physical robots, our
model achieves a 67% real-world grasping success rate on the YCB dataset,
outperforming state-of-the-art approaches that rely on substantially more
in-domain data. We further study how zero-shot generalization performance
scales by varying the number and diversity of training toys and the
demonstrations per toy. We believe this work offers a promising path to
scalable and generalizable learning in robotic manipulation. Demonstration
videos, code, checkpoints and our dataset are available on our project page:
https://lego-grasp.github.io/ .

</details>


### [200] [Gaussian Process Implicit Surfaces as Control Barrier Functions for Safe Robot Navigation](https://arxiv.org/abs/2510.12919)
*Mouhyemen Khan,Tatsuya Ibuki,Abhijit Chatterjee*

Main category: cs.RO

TL;DR: 本文提出一个统一框架，将高斯过程隐式曲面（GPIS）作为控制屏障函数（CBF），用于安全边界表示和碰撞避免，并引入稀疏解决方案以提高可扩展性。


<details>
  <summary>Details</summary>
Motivation: 受水平集方法在控制屏障函数和隐式表面表示中的应用启发，以及高斯过程在不确定性估计和分析可处理性方面的优势，研究者旨在开发一个统一框架，将隐式表面本身作为CBF，并利用GPIS提供鲁棒的安全裕度。

Method: 研究方法包括：1) 提出一个统一框架，使隐式曲面（通过距离场表示）充当控制屏障函数。2) 利用高斯过程隐式曲面（GPIS）来表示安全边界，通过传感器测量得到的安全样本来条件化GP。3) GP的后验均值定义隐式安全表面，而后验方差提供鲁棒的安全裕度。4) 为解决GP计算复杂度随数据量呈立方增长的问题，开发了稀疏高斯CBF解决方案。

Result: 该方法在两种碰撞避免任务中得到了验证：一个围绕斯坦福兔子操作的7自由度机械臂，以及一个在3D空间中围绕物理椅子导航的四旋翼无人机。在这两种情况下，高斯CBF（无论是否稀疏）都能实现安全交互和无碰撞地执行原本会与物体相交的轨迹。

Conclusion: GPIS可以有效地用于合成CBF，提供可靠的安全边界和不确定性估计，实现安全的轨迹执行和碰撞避免。同时，提出的稀疏解决方案有效缓解了GP的计算扩展性问题，使其在实际应用中更具可行性。

Abstract: Level set methods underpin modern safety techniques such as control barrier
functions (CBFs), while also serving as implicit surface representations for
geometric shapes via distance fields. Inspired by these two paradigms, we
propose a unified framework where the implicit surface itself acts as a CBF. We
leverage Gaussian process (GP) implicit surface (GPIS) to represent the safety
boundaries, using safety samples which are derived from sensor measurements to
condition the GP. The GP posterior mean defines the implicit safety surface
(safety belief), while the posterior variance provides a robust safety margin.
Although GPs have favorable properties such as uncertainty estimation and
analytical tractability, they scale cubically with data. To alleviate this
issue, we develop a sparse solution called sparse Gaussian CBFs. To the best of
our knowledge, GPIS have not been explicitly used to synthesize CBFs. We
validate the approach on collision avoidance tasks in two settings: a simulated
7-DOF manipulator operating around the Stanford bunny, and a quadrotor
navigating in 3D around a physical chair. In both cases, Gaussian CBFs (with
and without sparsity) enable safe interaction and collision-free execution of
trajectories that would otherwise intersect the objects.

</details>


### [201] [Geometric Model Predictive Path Integral for Agile UAV Control with Online Collision Avoidance](https://arxiv.org/abs/2510.12924)
*Pavel Pochobradský,Ondřej Procházka,Robert Pěnička,Vojtěch Vonásek,Martin Saska*

Main category: cs.RO

TL;DR: 本文提出了一种名为几何模型预测路径积分（GMPPI）的采样控制器，用于无人机（UAV）在避免障碍物的同时跟踪敏捷轨迹，并在模拟和实际环境中取得了显著性能。


<details>
  <summary>Details</summary>
Motivation: 现有模型预测路径积分（MPPI）在敏捷飞行中的精度不足，且在复杂环境中高速避障能力有限，阻碍了无人机在复杂环境中的自主飞行。

Method: GMPPI通过生成大量候选轨迹并取平均来创建名义控制。它引入了几何SE(3)控制来生成部分轨迹以提高敏捷飞行的精度。此外，它还采用了可变的轨迹模拟时间步长以及动态成本和噪声参数来改进跟踪性能。最后，GMPPI与立体深度相机集成，以实现高速在线避障。

Result: 在模拟中，GMPPI的轨迹跟踪位置误差与几何SE(3)控制器相似，且能在模拟森林环境中以高达13米/秒的速度避障，优于现有最先进的避障规划器。在实际实验中，GMPPI能以高达10米/秒的速度跟踪敏捷轨迹并避开障碍物。

Conclusion: GMPPI是一种有效的采样控制器，能够使无人机在复杂环境中实现高精度敏捷轨迹跟踪和鲁棒的高速避障，是实现自主无人机飞行的关键一步。

Abstract: In this letter, we introduce Geometric Model Predictive Path Integral
(GMPPI), a sampling-based controller capable of tracking agile trajectories
while avoiding obstacles. In each iteration, GMPPI generates a large number of
candidate rollout trajectories and then averages them to create a nominal
control to be followed by the Unmanned Aerial Vehicle (UAV). We propose using
geometric SE(3) control to generate part of the rollout trajectories,
significantly increasing precision in agile flight. Furthermore, we introduce
varying rollout simulation time step length and dynamic cost and noise
parameters, vastly improving tracking performance of smooth and low-speed
trajectories over an existing Model Predictive Path Integral (MPPI)
implementation. Finally, we propose an integration of GMPPI with a stereo depth
camera, enabling online obstacle avoidance at high speeds, a crucial step
towards autonomous UAV flights in complex environments. The proposed controller
can track simulated agile reference trajectories with position error similar to
the geometric SE(3) controller. However, the same configuration of the proposed
controller can avoid obstacles in a simulated forest environment at speeds of
up to 13m/s, surpassing the performance of a state-of-the-art obstacle-aware
planner. In real-world experiments, GMPPI retains the capability to track agile
trajectories and avoids obstacles at speeds of up to 10m/s.

</details>


### [202] [Enhancing Sampling-based Planning with a Library of Paths](https://arxiv.org/abs/2510.12962)
*Michal Minařík,Vojtěch Vonásek,Robert Pěnička*

Main category: cs.RO

TL;DR: 本文提出了一种针对三维实体对象路径规划的方法，通过重用过去解决方案库中的经验，为新对象提供近似路径，显著提高了规划速度（高达85%）和成功率，尤其是在狭窄通道场景下。


<details>
  <summary>Details</summary>
Motivation: 三维实体对象路径规划（六维构型空间）是一个挑战性问题，但在机器人应用（如分拣、装配）中至关重要。现有基于采样的规划器在狭窄通道中效率低下且耗时。此外，传统规划器在处理同一环境中不同对象时，每次都从头开始，浪费了宝贵的规划经验。

Method: 该方法建立了一个历史解决方案库，存储了一系列对象的路径。当需要为新对象规划时，它会从库中找到最相似的对象，并将其路径作为近似解决方案。然后，根据可能的相互变换对这些近似路径进行调整，并在调整后的近似路径周围对构型空间进行采样。

Result: 在各种狭窄通道场景中进行了测试，并与OMPL库中的最新方法进行了比较。结果表明，该方法显著提高了速度（所需时间减少高达85%），并且在其他规划器失败的情况下，它常常能够找到解决方案。该方法的实现已作为开源软件包发布。

Conclusion: 通过重用过去解决方案的经验，本方法有效解决了三维实体对象路径规划中狭窄通道和重复规划的挑战。它显著提高了规划效率和成功率，为机器人应用提供了更鲁棒的路径规划能力。

Abstract: Path planning for 3D solid objects is a challenging problem, requiring a
search in a six-dimensional configuration space, which is, nevertheless,
essential in many robotic applications such as bin-picking and assembly. The
commonly used sampling-based planners, such as Rapidly-exploring Random Trees,
struggle with narrow passages where the sampling probability is low, increasing
the time needed to find a solution. In scenarios like robotic bin-picking,
various objects must be transported through the same environment. However,
traditional planners start from scratch each time, losing valuable information
gained during the planning process. We address this by using a library of past
solutions, allowing the reuse of previous experiences even when planning for a
new, previously unseen object. Paths for a set of objects are stored, and when
planning for a new object, we find the most similar one in the library and use
its paths as approximate solutions, adjusting for possible mutual
transformations. The configuration space is then sampled along the approximate
paths. Our method is tested in various narrow passage scenarios and compared
with state-of-the-art methods from the OMPL library. Results show significant
speed improvements (up to 85% decrease in the required time) of our method,
often finding a solution in cases where the other planners fail. Our
implementation of the proposed method is released as an open-source package.

</details>


### [203] [The Omega Turn: A General Turning Template for Elongate Robots](https://arxiv.org/abs/2510.12970)
*Baxi Chong,Tianyu Wang,Kelimar Diaz,Christopher J. Pierce,Eva Erickson,Julian Whitman,Yuelin Deng,Esteban Flores,Ruijie Fu,Juntao He,Jianfeng Lin,Hang Lu,Guillaume Sartoretti,Howie Choset,Daniel I. Goldman*

Main category: cs.RO

TL;DR: 受线虫Ω转弯启发，本研究通过叠加两个行波来描述和控制细长无肢机器人的转弯行为，实现了在复杂环境中的鲁棒有效转弯，并可推广至多腿机器人。


<details>
  <summary>Details</summary>
Motivation: 细长无肢机器人在狭窄空间（如搜救、工业检查）中具有巨大潜力，但其有效和鲁棒的转弯能力是实现这些应用的关键。然而，针对此类系统的转弯策略研究有限。秀丽隐杆线虫（C. elegans）在流变学复杂环境中展现出卓越的机动性，部分归因于其执行Ω转弯的能力。

Method: 采用比较理论与生物学方法，将Ω转弯描述为两个行波的叠加。以这些波方程为指导，设计了适用于无肢机器人的控制器。

Result: 所设计的控制器使无肢机器人在实验室和复杂野外环境中表现出鲁棒且有效的转弯行为。此外，这种Ω转弯控制器还可以推广到细长多腿机器人，为有肢和无肢的细长机器人提供了一种替代的有效身体驱动转弯策略。

Conclusion: 通过将Ω转弯描述为两个行波的叠加并设计相应控制器，本研究为细长无肢和多腿机器人提供了一种新颖、有效且鲁棒的身体驱动转弯策略，显著提升了其在复杂环境中的机动性。

Abstract: Elongate limbless robots have the potential to locomote through tightly
packed spaces for applications such as search-and-rescue and industrial
inspections. The capability to effectively and robustly maneuver elongate
limbless robots is crucial to realize such potential. However, there has been
limited research on turning strategies for such systems. To achieve effective
and robust turning performance in cluttered spaces, we take inspiration from a
microscopic nematode, C. elegans, which exhibits remarkable maneuverability in
rheologically complex environments partially because of its ability to perform
omega turns. Despite recent efforts to analyze omega turn kinematics, it
remains unknown if there exists a wave equation sufficient to prescribe an
omega turn, let alone its reconstruction on robot platforms. Here, using a
comparative theory-biology approach, we prescribe the omega turn as a
superposition of two traveling waves. With wave equations as a guideline, we
design a controller for limbless robots enabling robust and effective turning
behaviors in lab and cluttered field environments. Finally, we show that such
omega turn controllers can also generalize to elongate multi-legged robots,
demonstrating an alternative effective body-driven turning strategy for
elongate robots, with and without limbs.

</details>


### [204] [Actron3D: Learning Actionable Neural Functions from Videos for Transferable Robotic Manipulation](https://arxiv.org/abs/2510.12971)
*Anran Zhang,Hanzhi Chen,Yannick Burkhardt,Yao Zhong,Johannes Betz,Helen Oleynikova,Stefan Leutenegger*

Main category: cs.RO

TL;DR: Actron3D是一个机器人框架，能让机器人仅通过少量单目、未校准的RGB人类视频学习可迁移的6自由度操作技能，核心是神经可供性函数（NAF），并在模拟和真实世界中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机是使机器人能够从少量、未校准的单目人类视频中获取可迁移的6自由度操作技能，解决现有方法在此方面的局限性。

Method: 该方法的核心是“神经可供性函数”（Neural Affordance Function, NAF），这是一种紧凑的以物体为中心的表示，它将来自不同未校准视频的几何、视觉外观和可供性线索提炼成一个轻量级神经网络。在部署时，系统通过检索相关的可供性函数，并通过粗到细的优化，利用对神经网络中编码的多模态特征的连续查询，实现精确的6自由度操作策略迁移。

Result: Actron3D在模拟和真实世界实验中均显著优于现有方法，在13项任务中平均成功率提高了14.9个百分点，且每项任务仅需2-3个演示视频。

Conclusion: Actron3D提供了一个有效的框架，使机器人能够仅通过少量单目、未校准的RGB人类视频获取可迁移的6自由度操作技能，表现出卓越的性能。

Abstract: We present Actron3D, a framework that enables robots to acquire transferable
6-DoF manipulation skills from just a few monocular, uncalibrated, RGB-only
human videos. At its core lies the Neural Affordance Function, a compact
object-centric representation that distills actionable cues from diverse
uncalibrated videos-geometry, visual appearance, and affordance-into a
lightweight neural network, forming a memory bank of manipulation skills.
During deployment, we adopt a pipeline that retrieves relevant affordance
functions and transfers precise 6-DoF manipulation policies via coarse-to-fine
optimization, enabled by continuous queries to the multimodal features encoded
in the neural functions. Experiments in both simulation and the real world
demonstrate that Actron3D significantly outperforms prior methods, achieving a
14.9 percentage point improvement in average success rate across 13 tasks while
requiring only 2-3 demonstration videos per task.

</details>


### [205] [UNCAP: Uncertainty-Guided Planning Using Natural Language Communication for Cooperative Autonomous Vehicles](https://arxiv.org/abs/2510.12992)
*Neel P. Bhatt,Po-han Li,Kushagra Gupta,Rohan Siva,Daniel Milan,Alexander T. Hogue,Sandeep P. Chinchali,David Fridovich-Keil,Zhangyang Wang,Ufuk Topcu*

Main category: cs.RO

TL;DR: UNCAP提出了一种基于视觉-语言模型的合作自动驾驶规划方法，通过不确定性引导的自然语言通信，显著提高了多辆互联自动驾驶汽车（CAV）的通信效率和驾驶安全性。


<details>
  <summary>Details</summary>
Motivation: 现有的多CAV协调方法要么依赖传输高带宽的原始传感器数据流，要么忽略共享数据中固有的感知和规划不确定性，导致系统既不具可扩展性也不安全。

Method: UNCAP（不确定性引导的自然语言合作自主规划）是一种基于视觉-语言模型的规划方法。它采用两阶段通信协议：(i) 主CAV首先识别出最相关的信息交换车辆子集，(ii) 被选中的CAV然后发送定量表达其感知不确定性的自然语言消息。通过选择性地融合最大化互信息的消，主车只将最相关的信号整合到决策中。

Result: 实验结果表明，UNCAP在多种驾驶场景下实现了63%的通信带宽减少，31%的驾驶安全分数提升，61%的决策不确定性降低，并在近距离事件中将碰撞距离裕度增加了四倍。

Conclusion: UNCAP通过高效、可解释且明确考虑感知不确定性的自然语言通信，显著提高了合作规划的可扩展性和可靠性。

Abstract: Safe large-scale coordination of multiple cooperative connected autonomous
vehicles (CAVs) hinges on communication that is both efficient and
interpretable. Existing approaches either rely on transmitting high-bandwidth
raw sensor data streams or neglect perception and planning uncertainties
inherent in shared data, resulting in systems that are neither scalable nor
safe. To address these limitations, we propose Uncertainty-Guided Natural
Language Cooperative Autonomous Planning (UNCAP), a vision-language model-based
planning approach that enables CAVs to communicate via lightweight natural
language messages while explicitly accounting for perception uncertainty in
decision-making. UNCAP features a two-stage communication protocol: (i) an ego
CAV first identifies the subset of vehicles most relevant for information
exchange, and (ii) the selected CAVs then transmit messages that quantitatively
express their perception uncertainty. By selectively fusing messages that
maximize mutual information, this strategy allows the ego vehicle to integrate
only the most relevant signals into its decision-making, improving both the
scalability and reliability of cooperative planning. Experiments across diverse
driving scenarios show a 63% reduction in communication bandwidth with a 31%
increase in driving safety score, a 61% reduction in decision uncertainty, and
a four-fold increase in collision distance margin during near-miss events.
Project website: https://uncap-project.github.io/

</details>


### [206] [Development of a Linear Guide-Rail Testbed for Physically Emulating ISAM Operations](https://arxiv.org/abs/2510.13005)
*Robert Muldrow,Channing Ludden,Christopher Petersen*

Main category: cs.RO

TL;DR: 空间在轨服务、组装和制造(ISAM)中的机械臂控制与模型验证复杂。本文提出一种新型硬件在环(HIL)实验平台，通过一个搭载6自由度机械臂的1自由度导轨卫星总线模拟ISAM，用于验证空间运动、串联机器人操作和接触力学模型。


<details>
  <summary>Details</summary>
Motivation: ISAM操作对未来空间资产至关重要，但自由飞行的卫星上机械臂的运动会产生复杂的扰动力和运动，带来复杂的控制问题。此外，在6自由度空间环境中，实验性地测试和验证这些动态模型极具挑战。

Method: 设计并开发了一个新的硬件在环(HIL)实验平台，用于模拟ISAM操作。该平台包含一个连接到卫星总线上的6自由度UR3e机械臂。卫星总线安装在一个1自由度导轨系统上，使其和机械臂能在一个线性方向上自由移动。

Result: 本文展示了ISAM模拟实验平台的设计与开发。该实验系统将用于探索和验证空间运动、串联机器人操作以及接触力学方面的模型。

Conclusion: 本文通过设计和开发一个新型硬件在环(HIL)实验平台，为解决在轨服务、组装和制造中机器人机械臂复杂动态模型的验证挑战提供了解决方案。该平台将用于对空间运动、串联机器人操作和接触力学模型进行探索和验证。

Abstract: In-Space Servicing, Assembly, and Manufacturing (ISAM) is a set of emerging
operations that provides several benefits to improve the longevity, capacity,
mo- bility, and expandability of existing and future space assets. Serial
robotic ma- nipulators are particularly vital in accomplishing ISAM operations,
however, the complex perturbation forces and motions associated with movement
of a robotic arm on a free-flying satellite presents a complex controls problem
requiring addi- tional study. While many dynamical models are developed,
experimentally test- ing and validating these models is challenging given that
the models operate in space, where satellites have six-degrees-of-freedom
(6-DOF). This paper attempts to resolve those challenges by presenting the
design and development of a new hardware-in-the-loop (HIL) experimental testbed
utilized to emulate ISAM. This emulation will be accomplished by means of a
6-DOF UR3e robotic arm attached to a satellite bus. This satellite bus is
mounted to a 1-DOF guide-rail system, en- abling the satellite bus and robotic
arm to move freely in one linear direction. This experimental ISAM emulation
system will explore and validate models for space motion, serial robot
manipulation, and contact mechanics.

</details>


### [207] [Kinematic Kitbashing for Modeling Functional Articulated Objects](https://arxiv.org/abs/2510.13048)
*Minghao Guo,Victor Zordan,Sheldon Andrews,Wojciech Matusik,Maneesh Agrawala,Hsueh-Ti Derek Liu*

Main category: cs.RO

TL;DR: Kinematic Kitbashing是一个自动框架，通过重用现有模型的部件，合成功能感知的可动对象。它优化部件的空间放置，以确保运动范围内的几何完整性和满足碰撞避免、可达性等用户功能目标。


<details>
  <summary>Details</summary>
Motivation: 现有方法可能难以在重用部件的同时，有效地合成既几何完整又满足特定功能的可动对象。该研究旨在实现互动式可动资产的快速创建，弥合基于部件的形状建模与功能性装配设计之间的鸿沟。

Method: 该框架接收一个运动学图和少量可动部件，并通过优化器共同解决每个部件的空间放置问题。其核心是运动学感知的连接能量，该能量通过多个关节运动快照对向量距离函数特征进行对齐。优化过程采用退火黎曼朗之万动力学采样器，将功能目标视为额外的能量项，从而实现鲁棒的全局探索，并能适应不可微的功能目标和约束。

Result: 该框架能够生成各种组装的可动形状，包括将垃圾桶轮子移植到车身上、多节灯具、齿轮驱动的桨手以及可重构家具。与最先进的基线相比，该方法在几何、运动学和功能指标上均取得了显著的定量改进。

Conclusion: Kinematic Kitbashing通过紧密结合关节运动感知的几何匹配和功能驱动的优化，成功连接了基于部件的形状建模和功能性装配设计，从而能够快速创建互动式可动资产。

Abstract: We introduce Kinematic Kitbashing, an automatic framework that synthesizes
functionality-aware articulated objects by reusing parts from existing models.
Given a kinematic graph with a small collection of articulated parts, our
optimizer jointly solves for the spatial placement of every part so that (i)
attachments remain geometrically sound over the entire range of motion and (ii)
the assembled object satisfies user-specified functional goals such as
collision-free actuation, reachability, or trajectory following. At its core is
a kinematics-aware attachment energy that aligns vector distance function
features sampled across multiple articulation snapshots. We embed this
attachment term within an annealed Riemannian Langevin dynamics sampler that
treats functionality objectives as additional energies, enabling robust global
exploration while accommodating non-differentiable functionality objectives and
constraints. Our framework produces a wide spectrum of assembled articulated
shapes, from trash-can wheels grafted onto car bodies to multi-segment lamps,
gear-driven paddlers, and reconfigurable furniture, and delivers strong
quantitative improvements over state-of-the-art baselines across geometric,
kinematic, and functional metrics. By tightly coupling articulation-aware
geometry matching with functionality-driven optimization, Kinematic Kitbashing
bridges part-based shape modeling and functional assembly design, empowering
rapid creation of interactive articulated assets.

</details>


### [208] [VLA-0: Building State-of-the-Art VLAs with Zero Modification](https://arxiv.org/abs/2510.13054)
*Ankit Goyal,Hugo Hadfield,Xuning Yang,Valts Blukis,Fabio Ramos*

Main category: cs.RO

TL;DR: 解析错误


<details>
  <summary>Details</summary>
Motivation: 解析错误

Method: 解析错误

Result: 解析错误

Conclusion: 解析错误

Abstract: Vision-Language-Action models (VLAs) hold immense promise for enabling
generalist robot manipulation. However, the best way to build them remains an
open question. Current approaches often add complexity, such as modifying the
existing vocabulary of a Vision-Language Model (VLM) with action tokens or
introducing special action heads. Curiously, the simplest strategy of
representing actions directly as text has remained largely unexplored. This
work introduces VLA-0 to investigate this idea. We find that VLA-0 is not only
effective; it is surprisingly powerful. With the right design, VLA-0
outperforms more involved models. On LIBERO, a popular benchmark for evaluating
VLAs, VLA-0 outperforms all existing methods trained on the same robotic data,
including $\pi_0.5$-KI, OpenVLA-OFT and SmolVLA. Furthermore, without
large-scale robotics-specific training, it outperforms methods trained on
large-scale robotic data, like $\pi_0.5$-KI, $\pi_0$, GR00T-N1 and MolmoAct.
These findings also translate to the real world, where VLA-0 outperforms
SmolVLA, a VLA model pre-trained on large-scale real data. This paper
summarizes our unexpected findings and spells out the specific techniques
required to unlock the high performance of this simple yet potent VLA design.
Visual results, code, and trained models are provided here:
https://vla0.github.io/.

</details>


### [209] [RoboHiMan: A Hierarchical Evaluation Paradigm for Compositional Generalization in Long-Horizon Manipulation](https://arxiv.org/abs/2510.13149)
*Yangtao Chen,Zixuan Chen,Nga Teng Chan,Junting Chen,Junhui Yin,Jieqi Shi,Yang Gao,Yong-Lu Li,Jing Huo*

Main category: cs.RO

TL;DR: 本文提出了RoboHiMan，一个用于评估机器人长时程操作中组合泛化能力的层次化评估范式，包括基准测试、数据集和评估方法，并揭示了现有模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 机器人难以在多样扰动下灵活调度和组合已学习技能以完成新颖的长时程操作。现有端到端VLA模型泛化能力有限，层次化方法在复杂扰动下表现不佳，且现有基准主要侧重任务完成，未能深入探究组合泛化、鲁棒性以及规划与执行的相互作用。

Method: 提出了RoboHiMan，一个针对长时程操作中组合泛化能力的层次化评估范式。引入了HiMan-Bench，一个包含原子和组合任务以及多样扰动的基准测试。提供了多级训练数据集以分析渐进式数据缩放。提出了三种评估范式（香草、解耦、耦合），旨在探究技能组合的必要性并揭示层次架构中的瓶颈。

Result: 实验结果揭示了代表性模型和架构之间存在明显的能力差距。

Conclusion: 这些差距指明了未来改进方向，以开发更适合真实世界长时程操作任务的模型。

Abstract: Enabling robots to flexibly schedule and compose learned skills for novel
long-horizon manipulation under diverse perturbations remains a core challenge.
Early explorations with end-to-end VLA models show limited success, as these
models struggle to generalize beyond the training distribution. Hierarchical
approaches, where high-level planners generate subgoals for low-level policies,
bring certain improvements but still suffer under complex perturbations,
revealing limited capability in skill composition. However, existing benchmarks
primarily emphasize task completion in long-horizon settings, offering little
insight into compositional generalization, robustness, and the interplay
between planning and execution. To systematically investigate these gaps, we
propose RoboHiMan, a hierarchical evaluation paradigm for compositional
generalization in long-horizon manipulation. RoboHiMan introduces HiMan-Bench,
a benchmark of atomic and compositional tasks under diverse perturbations,
supported by a multi-level training dataset for analyzing progressive data
scaling, and proposes three evaluation paradigms (vanilla, decoupled, coupled)
that probe the necessity of skill composition and reveal bottlenecks in
hierarchical architectures. Experiments highlight clear capability gaps across
representative models and architectures, pointing to directions for advancing
models better suited to real-world long-horizon manipulation tasks. Videos and
open-source code can be found on our project website:
https://chenyt31.github.io/robo-himan.github.io/.

</details>


### [210] [ALOHA2 Robot Kitchen Application Scenario Reproduction Report](https://arxiv.org/abs/2510.13284)
*Haoyang Wu,Siheng Wu,William X. Liu,Fangui Zeng*

Main category: cs.RO

TL;DR: ALOHA2是ALOHA双臂遥操作机器人的增强版，提升了性能、鲁棒性和人机工程学，并支持RGB数据采集。


<details>
  <summary>Details</summary>
Motivation: 原始ALOHA设计在性能、鲁棒性和人机工程学方面存在改进空间，因此开发了ALOHA2以提供更优越的遥操作体验。

Method: ALOHA2是一个双臂遥操作机器人，包含两个夹具、两个ViperX 6-DoF机械臂和两个较小的WidowX机械臂。用户通过反向驱动（back-driving）操作领导机械臂来控制跟随机械臂。该设备还配备多视角摄像头，用于在遥操作过程中收集RGB数据。机器人安装在一个带有铝制框架的桌子上，提供额外的安装点和重力补偿系统。

Result: ALOHA2相比于原始ALOHA设计，具有更高的性能、更强的鲁棒性以及更好的人机工程学。它还能够从多个视角生成图像，从而在遥操作期间收集RGB数据。

Conclusion: ALOHA2成功地增强了ALOHA机器人，提供了一个性能更优、更鲁棒、更符合人机工程学的双臂遥操作平台，并扩展了其数据采集能力。

Abstract: ALOHA2 is an enhanced version of the dual-arm teleoperated robot ALOHA,
featuring higher performance and robustness compared to the original design,
while also being more ergonomic. Like ALOHA, ALOHA2 consists of two grippers
and two ViperX 6-DoF arms, as well as two smaller WidowX arms. Users control
the follower mechanical arms by operating the leader mechanical arms through
back-driving. The device also includes cameras that generate images from
multiple viewpoints, allowing for RGB data collection during teleoperation. The
robot is mounted on a 48-inch x 30-inch table, equipped with an aluminum frame
that provides additional mounting points for cameras and gravity compensation
systems.

</details>


### [211] [DAMM-LOAM: Degeneracy Aware Multi-Metric LiDAR Odometry and Mapping](https://arxiv.org/abs/2510.13287)
*Nishant Chandna,Akshat Kaushal*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的退化感知多度量激光雷达里程计和建图（DAMM-LOAM）模块，通过点云分类、退化感知加权最小二乘ICP和基于扫描上下文的后端，显著提高了在稀疏特征和重复结构环境（尤其是室内长廊）中的里程计精度。


<details>
  <summary>Details</summary>
Motivation: 当前的LiDAR SLAM系统中的点到平面ICP算法在特征稀疏、几何结构重复和高频运动的环境中表现不佳，导致6自由度姿态估计出现退化。尽管多数先进算法通过多传感器融合解决这些问题，但纯LiDAR解决方案在此类条件下仍面临局限性。

Method: 本文提出了DAMM-LOAM模块。首先，基于表面法线和邻域分析对点云进行分类（地面、墙壁、屋顶、边缘和非平面点），以实现准确的对应关系。其次，应用基于退化感知的加权最小二乘ICP算法进行精确的里程计估计。最后，实现了基于扫描上下文的后端，以支持鲁棒的闭环检测。

Result: DAMM-LOAM在里程计精度方面表现出显著提升，尤其是在长廊等室内环境中效果更佳。

Conclusion: DAMM-LOAM通过引入点云分类、退化感知的ICP算法和基于扫描上下文的闭环检测，有效解决了纯LiDAR SLAM在挑战性环境中的退化问题，显著提高了定位和建图的准确性。

Abstract: LiDAR Simultaneous Localization and Mapping (SLAM) systems are essential for
enabling precise navigation and environmental reconstruction across various
applications. Although current point-to-plane ICP algorithms perform effec-
tively in structured, feature-rich environments, they struggle in scenarios
with sparse features, repetitive geometric structures, and high-frequency
motion. This leads to degeneracy in 6- DOF pose estimation. Most
state-of-the-art algorithms address these challenges by incorporating
additional sensing modalities, but LiDAR-only solutions continue to face
limitations under such conditions. To address these issues, we propose a novel
Degeneracy-Aware Multi-Metric LiDAR Odometry and Map- ping (DAMM-LOAM) module.
Our system improves mapping accuracy through point cloud classification based
on surface normals and neighborhood analysis. Points are classified into
ground, walls, roof, edges, and non-planar points, enabling accurate
correspondences. A Degeneracy-based weighted least squares-based ICP algorithm
is then applied for accurate odom- etry estimation. Additionally, a Scan
Context based back-end is implemented to support robust loop closures.
DAMM-LOAM demonstrates significant improvements in odometry accuracy,
especially in indoor environments such as long corridors

</details>


### [212] [Tactile-Conditioned Diffusion Policy for Force-Aware Robotic Manipulation](https://arxiv.org/abs/2510.13324)
*Erik Helmut,Niklas Funk,Tim Schneider,Cristiana de Farias,Jan Peters*

Main category: cs.RO

TL;DR: 本文提出FARM（力感知机器人操作），一种模仿学习框架，通过整合高维触觉数据来推断触觉条件下的力信号，并定义基于力的动作空间，从而在接触丰富的操作任务中实现精确的抓取力控制。


<details>
  <summary>Details</summary>
Motivation: 在接触丰富的操作中，尤其是在处理易碎或可变形物体时，正确的抓取力至关重要。然而，大多数现有的模仿学习方法通常只将视觉触觉反馈作为额外观察，导致施加的力是夹持器命令的非受控结果。

Method: 研究人员开发了FARM模仿学习框架，该框架整合了高维触觉数据以推断触觉条件下的力信号，并以此定义基于力的动作空间。他们使用集成了GelSight Mini视觉触觉传感器的改进版手持UMI夹持器收集人类演示数据。为了部署学习到的策略，他们开发了与手持版几何匹配的电动UMI夹持器。在策略执行过程中，FARM扩散策略共同预测机器人姿态、抓取宽度和抓取力。

Result: FARM在三种具有不同力要求（高力、低力、动态力适应）的任务中均优于多个基线方法。这证明了其两个关键组成部分的优势：利用基于力的、高维触觉观测和基于力的控制空间。

Conclusion: FARM框架通过有效利用基于力的、高维触觉观测和基于力的控制空间，显著提高了机器人在接触丰富操作任务中对抓取力的控制能力，超越了传统模仿学习方法。

Abstract: Contact-rich manipulation depends on applying the correct grasp forces
throughout the manipulation task, especially when handling fragile or
deformable objects. Most existing imitation learning approaches often treat
visuotactile feedback only as an additional observation, leaving applied forces
as an uncontrolled consequence of gripper commands. In this work, we present
Force-Aware Robotic Manipulation (FARM), an imitation learning framework that
integrates high-dimensional tactile data to infer tactile-conditioned force
signals, which in turn define a matching force-based action space. We collect
human demonstrations using a modified version of the handheld Universal
Manipulation Interface (UMI) gripper that integrates a GelSight Mini visual
tactile sensor. For deploying the learned policies, we developed an actuated
variant of the UMI gripper with geometry matching our handheld version. During
policy rollouts, the proposed FARM diffusion policy jointly predicts robot
pose, grip width, and grip force. FARM outperforms several baselines across
three tasks with distinct force requirements -- high-force, low-force, and
dynamic force adaptation -- demonstrating the advantages of its two key
components: leveraging force-grounded, high-dimensional tactile observations
and a force-based control space. The codebase and design files are open-sourced
and available at https://tactile-farm.github.io .

</details>


### [213] [MODUR: A Modular Dual-reconfigurable Robot](https://arxiv.org/abs/2510.13356)
*Jie Gu,Tin Lun Lam,Chunxu Tian,Zhihao Xia,Yongheng Xing,Dan Zhang*

Main category: cs.RO

TL;DR: 本文提出了一种名为MODUR的新型模块化自重构机器人（MSRR），它具有双层重构能力（模块间重构和模块内部形状变化），通过紧凑连接器和剪刀连杆机构实现，并通过实验验证了其运动能力。


<details>
  <summary>Details</summary>
Motivation: 模块化自重构机器人（MSRR）系统通过改变模块间的拓扑关系来形成更高级的机器人系统，从而在各种环境中提供增强的适应性和鲁棒性。本文旨在将可重构机制集成到MSRR中，以实现更高的灵活性和功能性。

Method: 本文提出了一种名为MODUR的新型MSRR。MODUR具有双层重构能力：高层模块间自重构以创建不同配置，以及每个模块内部形状变化以执行基本运动。其设计主要包括紧凑连接器和剪刀连杆组，形成一个并联机构，能够实现连接器运动解耦和相邻位置迁移。此外，论文还对考虑相互依赖连接器的工作空间进行了全面分析，为模块基本运动的设计奠定了理论基础。

Result: MODUR成功实现了双层重构能力，即模块间的高级自重构和模块内部形状变化以执行基本运动。其紧凑连接器和剪刀连杆设计提供了驱动力，并实现了连接器运动解耦和相邻位置迁移。对工作空间的理论分析为模块基本运动的设计提供了支持。通过一系列实验验证了MODUR的运动能力。

Conclusion: MODUR成功展示了将可重构机制集成到MSRR中的能力，实现了双层重构。其独特的机械设计和理论分析为模块化自重构机器人领域提供了新的方向，并通过实验验证了其有效性。

Abstract: Modular Self-Reconfigurable Robot (MSRR) systems are a class of robots
capable of forming higher-level robotic systems by altering the topological
relationships between modules, offering enhanced adaptability and robustness in
various environments. This paper presents a novel MSRR called MODUR, featuring
dual-level reconfiguration capabilities designed to integrate reconfigurable
mechanisms into MSRR. Specifically, MODUR can perform high-level
self-reconfiguration among modules to create different configurations, while
each module is also able to change its shape to execute basic motions. The
design of MODUR primarily includes a compact connector and scissor linkage
groups that provide actuation, forming a parallel mechanism capable of
achieving both connector motion decoupling and adjacent position migration
capabilities. Furthermore, the workspace, considering the interdependent
connectors, is comprehensively analyzed, laying a theoretical foundation for
the design of the module's basic motion. Finally, the motion of MODUR is
validated through a series of experiments.

</details>


### [214] [Adversarial Fine-tuning in Offline-to-Online Reinforcement Learning for Robust Robot Control](https://arxiv.org/abs/2510.13358)
*Shingo Ayabe,Hiroshi Kera,Kazuhiko Kawamoto*

Main category: cs.RO

TL;DR: 本文提出一个从离线到在线的框架，通过对抗性微调和性能感知课程，提高离线强化学习策略在动作空间扰动下的鲁棒性，并在连续控制任务中展现出更好的性能和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习训练出的策略在面对动作空间扰动（如执行器故障）时表现脆弱。研究旨在开发一种方法，在不进行危险在线交互的情况下，提高这些策略的韧性。

Method: 该研究引入一个从离线到在线的框架：首先在干净数据上训练策略，然后进行对抗性微调，即在执行动作时注入扰动以诱导补偿行为并提高弹性。此外，一个性能感知课程通过指数移动平均信号调整训练期间的扰动概率，以平衡鲁棒性和稳定性。

Result: 实验表明，该方法在连续控制运动任务中持续提高鲁棒性，优于仅离线基线，并且比从头开始训练收敛更快。匹配微调和评估条件能实现对动作空间扰动最强的鲁棒性，而自适应课程策略缓解了线性课程策略中观察到的名义性能下降。

Conclusion: 对抗性微调能够在不确定环境下实现自适应和鲁棒的控制，弥合了离线效率和在线适应性之间的差距。

Abstract: Offline reinforcement learning enables sample-efficient policy acquisition
without risky online interaction, yet policies trained on static datasets
remain brittle under action-space perturbations such as actuator faults. This
study introduces an offline-to-online framework that trains policies on clean
data and then performs adversarial fine-tuning, where perturbations are
injected into executed actions to induce compensatory behavior and improve
resilience. A performance-aware curriculum further adjusts the perturbation
probability during training via an exponential-moving-average signal, balancing
robustness and stability throughout the learning process. Experiments on
continuous-control locomotion tasks demonstrate that the proposed method
consistently improves robustness over offline-only baselines and converges
faster than training from scratch. Matching the fine-tuning and evaluation
conditions yields the strongest robustness to action-space perturbations, while
the adaptive curriculum strategy mitigates the degradation of nominal
performance observed with the linear curriculum strategy. Overall, the results
show that adversarial fine-tuning enables adaptive and robust control under
uncertain environments, bridging the gap between offline efficiency and online
adaptability.

</details>


### [215] [Real-Time Knee Angle Prediction Using EMG and Kinematic Data with an Attention-Based CNN-LSTM Network and Transfer Learning Across Multiple Datasets](https://arxiv.org/abs/2510.13443)
*Mojtaba Mollahossein,Gholamreza Vossoughi,Mohammad Hossein Rohban*

Main category: cs.RO

TL;DR: 本文提出了一种基于迁移学习的轻量级注意力CNN-LSTM模型，用于膝关节角度预测，仅需少量步态周期数据即可适应新受试者，并在多种输入条件下展现出强大的泛化能力和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的机器学习和深度学习方法在通过肌电图（EMG）信号预测关节角度时面临挑战，包括实时应用受限、测试条件不具代表性以及需要大量数据集才能达到最佳性能。

Method: 研究开发了一个迁移学习框架，用于膝关节角度预测。该框架仅需要新受试者少量步态周期数据。使用包含四个与膝关节运动相关的EMG通道的三个数据集（Georgia Tech、UCI和SMLE）。开发了一个轻量级注意力CNN-LSTM模型，并在Georgia Tech数据集上进行预训练，然后迁移到UCI和SMLE数据集。模型输入包括EMG信号、历史膝关节角度，以及在SMLE外骨骼场景中额外加入了运动学和交互力数据。

Result: 仅使用EMG输入时，模型对异常受试者的一步和50步预测分别达到了6.8%和13.7%的归一化平均绝对误差（NMAE）。结合历史膝关节角度后，正常受试者的NMAE降至3.1%（一步）和3.5%（50步），异常受试者降至2.8%（一步）和7.5%（50步）。当进一步适应SMLE外骨骼，并结合EMG、运动学和交互力输入时，模型的一步和50步预测NMAE分别达到了1.09%和3.1%。

Conclusion: 这些结果表明，该模型在短期和长期康复场景中均表现出强大的性能和良好的泛化能力。

Abstract: Electromyography (EMG) signals are widely used for predicting body joint
angles through machine learning (ML) and deep learning (DL) methods. However,
these approaches often face challenges such as limited real-time applicability,
non-representative test conditions, and the need for large datasets to achieve
optimal performance. This paper presents a transfer-learning framework for knee
joint angle prediction that requires only a few gait cycles from new subjects.
Three datasets - Georgia Tech, the University of California Irvine (UCI), and
the Sharif Mechatronic Lab Exoskeleton (SMLE) - containing four EMG channels
relevant to knee motion were utilized. A lightweight attention-based CNN-LSTM
model was developed and pre-trained on the Georgia Tech dataset, then
transferred to the UCI and SMLE datasets. The proposed model achieved
Normalized Mean Absolute Errors (NMAE) of 6.8 percent and 13.7 percent for
one-step and 50-step predictions on abnormal subjects using EMG inputs alone.
Incorporating historical knee angles reduced the NMAE to 3.1 percent and 3.5
percent for normal subjects, and to 2.8 percent and 7.5 percent for abnormal
subjects. When further adapted to the SMLE exoskeleton with EMG, kinematic, and
interaction force inputs, the model achieved 1.09 percent and 3.1 percent NMAE
for one- and 50-step predictions, respectively. These results demonstrate
robust performance and strong generalization for both short- and long-term
rehabilitation scenarios.

</details>


### [216] [Bridge the Gap: Enhancing Quadruped Locomotion with Vertical Ground Perturbations](https://arxiv.org/abs/2510.13488)
*Maximilian Stasica,Arne Bick,Nico Bohlinger,Omid Mohseni,Max Johannes Alois Fritzsche,Clemens Hübler,Jan Peters,André Seyfarth*

Main category: cs.RO

TL;DR: 本研究通过强化学习在模拟振荡桥上训练四足机器人，显著提升了其在动态地面扰动下的稳定性和适应性。


<details>
  <summary>Details</summary>
Motivation: 尽管腿式机器人擅长崎岖地形导航，但其在垂直地面扰动（如振荡表面）下的性能尚未得到充分探索。

Method: 研究使用强化学习（PPO算法）在MuJoCo模拟环境中训练Unitree Go2四足机器人。训练环境包括一个13.24米、2.0 Hz固有频率的振荡桥。共训练了15种策略，结合了五种步态（trot, pace, bound, free, default）和三种训练条件（刚性桥、两种不同高度调节策略的振荡桥）。通过域随机化实现了零样本迁移到真实世界。

Result: 在振荡桥上训练的策略表现出比在刚性表面上训练的策略更优越的稳定性和适应性。即使没有预先接触真实桥梁，框架也能使机器人获得鲁棒的步态模式。

Conclusion: 基于仿真的强化学习具有显著提升四足机器人在动态地面扰动下运动能力的潜力，为设计能够在振动环境中穿行的机器人提供了重要见解。

Abstract: Legged robots, particularly quadrupeds, excel at navigating rough terrains,
yet their performance under vertical ground perturbations, such as those from
oscillating surfaces, remains underexplored. This study introduces a novel
approach to enhance quadruped locomotion robustness by training the Unitree Go2
robot on an oscillating bridge - a 13.24-meter steel-and-concrete structure
with a 2.0 Hz eigenfrequency designed to perturb locomotion. Using
Reinforcement Learning (RL) with the Proximal Policy Optimization (PPO)
algorithm in a MuJoCo simulation, we trained 15 distinct locomotion policies,
combining five gaits (trot, pace, bound, free, default) with three training
conditions: rigid bridge and two oscillating bridge setups with differing
height regulation strategies (relative to bridge surface or ground). Domain
randomization ensured zero-shot transfer to the real-world bridge. Our results
demonstrate that policies trained on the oscillating bridge exhibit superior
stability and adaptability compared to those trained on rigid surfaces. Our
framework enables robust gait patterns even without prior bridge exposure.
These findings highlight the potential of simulation-based RL to improve
quadruped locomotion during dynamic ground perturbations, offering insights for
designing robots capable of traversing vibrating environments.

</details>


### [217] [A Novel Robot Hand with Hoeckens Linkages and Soft Phalanges for Scooping and Self-Adaptive Grasping in Environmental Constraints](https://arxiv.org/abs/2510.13535)
*Wentao Guo,Yizhou Wang,Wenzeng Zhang*

Main category: cs.RO

TL;DR: 本文提出了一种新型欠驱动自适应机器人手——Hockens-A Hand，它结合了Hoeckens机构、双平行连杆和专用四杆机构，通过单个线性执行器实现三种自适应抓取模式，并在非结构化环境中展现出适应性和柔顺性。


<details>
  <summary>Details</summary>
Motivation: 在非结构化环境中，需要一种能够实现自适应和柔顺抓取、且设计简单、欠驱动的机器人手。

Method: 该研究集成Hoeckens机构（提供柔顺性）、双平行连杆（确保指尖线接触）和专用四杆放大系统（实现抓取模式自然转换）。通过详细的运动学分析优化推角和连杆长度，利用仿真验证指尖运动和模式转换，并使用功率方程分析抓取力。最后，通过3D打印原型进行实验验证。

Result: Hockens-A Hand成功实现了平行捏合、不对称铲取和包络抓取三种自适应抓取模式。通过被动机械智能和单个执行器，该手在各种环境约束下展现出抓取稳定性、广泛适用性以及对不同形状和尺寸物体的包络能力。

Conclusion: Hockens-A Hand的设计通过集成机械智能和欠驱动控制，有效实现了在多种场景下多功能、自适应和稳定的抓取，验证了其设计的有效性和实用性。

Abstract: This paper presents a novel underactuated adaptive robotic hand, Hockens-A
Hand, which integrates the Hoeckens mechanism, a double-parallelogram linkage,
and a specialized four-bar linkage to achieve three adaptive grasping modes:
parallel pinching, asymmetric scooping, and enveloping grasping. Hockens-A Hand
requires only a single linear actuator, leveraging passive mechanical
intelligence to ensure adaptability and compliance in unstructured
environments. Specifically, the vertical motion of the Hoeckens mechanism
introduces compliance, the double-parallelogram linkage ensures line contact at
the fingertip, and the four-bar amplification system enables natural
transitions between different grasping modes. Additionally, the inclusion of a
mesh-textured silicone phalanx further enhances the ability to envelop objects
of various shapes and sizes. This study employs detailed kinematic analysis to
optimize the push angle and design the linkage lengths for optimal performance.
Simulations validated the design by analyzing the fingertip motion and ensuring
smooth transitions between grasping modes. Furthermore, the grasping force was
analyzed using power equations to enhance the understanding of the system's
performance.Experimental validation using a 3D-printed prototype demonstrates
the three grasping modes of the hand in various scenarios under environmental
constraints, verifying its grasping stability and broad applicability.

</details>


### [218] [Hoecken-D Hand: A Novel Robotic Hand for Linear Parallel Pinching and Self-Adaptive Grasping](https://arxiv.org/abs/2510.13553)
*Wentao Guo,Wenzeng Zhang*

Main category: cs.RO

TL;DR: 本文提出Hoecken-D机械手，一种欠驱动机器人抓手，结合改进的Hoecken连杆机构和差动弹簧机制，实现直线平行夹持和中行程自适应包络抓取，具有结构紧凑、适应性强、成本效益高的特点。


<details>
  <summary>Details</summary>
Motivation: 研究动机是为非结构化环境中的操作提供一种紧凑、适应性强且经济高效的解决方案。

Method: 该研究方法包括：1) 重新配置Hoecken连杆机构，用差动连杆替换其中一个构件，以在保持直线导向的同时实现接触触发的重构；2) 整合差动弹簧机制，在中行程实现自适应包络抓取；3) 采用双平行四边形结构保持指尖平行度；4) 设计为单线性执行器驱动（原型为简化使用两个）；5) 进行运动学建模和力分析；6) 使用PLA 3D打印技术制作原型。

Result: 主要结果包括：1) 原型实现了约200毫米的线性夹持范围；2) 初步测试表明，该机械手在平行夹持和自适应包络两种模式下，对各种物体几何形状都能实现可靠抓取；3) 证明Hoecken-D机械手是一种紧凑、适应性强且经济高效的解决方案。

Conclusion: Hoecken-D机械手通过其独特的设计，能够实现直线平行夹持和自适应包络抓取，且具有紧凑、适应性强和成本效益高的特点，使其成为非结构化环境中操作的有效解决方案。

Abstract: This paper presents the Hoecken-D Hand, an underactuated robotic gripper that
combines a modified Hoecken linkage with a differential spring mechanism to
achieve both linear parallel pinching and a mid-stroke transition to adaptive
envelope. The original Hoecken linkage is reconfigured by replacing one member
with differential links, preserving straight-line guidance while enabling
contact-triggered reconfiguration without additional actuators. A
double-parallelogram arrangement maintains fingertip parallelism during
conventional pinching, whereas the differential mechanism allows one finger to
wrap inward upon encountering an obstacle, improving stability on irregular or
thin objects. The mechanism can be driven by a single linear actuator,
minimizing complexity and cost; in our prototype, each finger is driven by its
own linear actuator for simplicity. We perform kinematic modeling and force
analysis to characterize grasp performance, including simulated grasping forces
and spring-opening behavior under varying geometric parameters. The design was
prototyped using PLA-based 3D printing, achieving a linear pinching span of
approximately 200 mm. Preliminary tests demonstrate reliable grasping in both
modes across a wide range of object geometries, highlighting the Hoecken-D Hand
as a compact, adaptable, and cost-effective solution for manipulation in
unstructured environments.

</details>


### [219] [Efficient Force and Stiffness Prediction in Robotic Produce Handling with a Piezoresistive Pressure Sensor](https://arxiv.org/abs/2510.13616)
*Preston Fairchild,Claudia Chen,Xiaobo Tan*

Main category: cs.RO

TL;DR: 本文提出了一种低成本、易于制造的柔性压力传感器，并将其集成到机器人夹具中，以实现对不同形状、大小和硬度农产品的精确抓取，同时提供农产品特性（如成熟度、损伤）的估计。


<details>
  <summary>Details</summary>
Motivation: 农业自动化中，机器人处理易碎农产品时，需要精确控制抓取力以避免损坏，并适应农产品多样化的特性，这是未来自动化应用的关键挑战。

Method: 研究方法包括：1) 开发一种低成本、易于制造的柔性压力传感器。2) 将该传感器集成到刚性机器人夹具和气动软指中。3) 提出一种基于瞬态响应数据加速估计传感器稳态输出值的算法，以支持实时应用。

Result: 研究结果表明：1) 传感器成功集成到两种类型的机器人夹具中。2) 传感器能有效提供反馈，正确抓取未知大小和硬度的物体。3) 传感器能提供物体大小和硬度的估计值，可用于识别成熟度、损伤等品质。4) 传感器能为不同硬度的物体提供力反馈。

Conclusion: 该传感器系统不仅能实现精确抓取，还能提供农产品识别（如成熟度、损伤）、质量控制和基于成熟度水平的选择性分发等功能，为农业自动化未来的应用提供了基础。

Abstract: Properly handling delicate produce with robotic manipulators is a major part
of the future role of automation in agricultural harvesting and processing.
Grasping with the correct amount of force is crucial in not only ensuring
proper grip on the object, but also to avoid damaging or bruising the product.
In this work, a flexible pressure sensor that is both low cost and easy to
fabricate is integrated with robotic grippers for working with produce of
varying shapes, sizes, and stiffnesses. The sensor is successfully integrated
with both a rigid robotic gripper, as well as a pneumatically actuated soft
finger. Furthermore, an algorithm is proposed for accelerated estimation of the
steady-state value of the sensor output based on the transient response data,
to enable real-time applications. The sensor is shown to be effective in
incorporating feedback to correctly grasp objects of unknown sizes and
stiffnesses. At the same time, the sensor provides estimates for these values
which can be utilized for identification of qualities such as ripeness levels
and bruising. It is also shown to be able to provide force feedback for objects
of variable stiffnesses. This enables future use not only for produce
identification, but also for tasks such as quality control and selective
distribution based on ripeness levels.

</details>


### [220] [Active Tactile Exploration for Rigid Body Pose and Shape Estimation](https://arxiv.org/abs/2510.13595)
*Ethan K. Gordon,Bruke Baraki,Hien Bui,Michael Posa*

Main category: cs.RO

TL;DR: 该研究提出一个仅使用触觉数据的学习与探索框架，通过最小的机器人运动，同时确定未知刚性物体的形状和位置，实现了高数据效率和更快的学习速度。


<details>
  <summary>Details</summary>
Motivation: 通用机器人操作需要处理未知物体，学习准确的物理模型能提高数据效率和预测性。触觉感知可弥补视觉盲区，但其时间稀疏性要求精细的在线探索。直接接触可能导致物体移动，因此需要同时估计形状和位置。

Method: 提出一个仅使用触觉数据的学习和探索框架，用于同时估计刚性物体的形状和位置。构建了一个基于接触丰富系统识别的损失函数，该函数惩罚物理约束违反，同时避免了刚体接触固有的数值刚度。探索方案旨在最大化预期信息增益。

Result: 该框架能够在首次接触后不到10秒的随机收集数据下，学习长方体和凸多面体几何形状。所提出的探索方案在模拟和真实机器人实验中都显著加快了学习速度。

Conclusion: 该研究提供了一个有效的学习和探索框架，仅通过触觉数据即可同时确定刚性物体的形状和位置，通过主动探索显著提高了学习效率和速度，从而增强了机器人处理未知物体的能力。

Abstract: General robot manipulation requires the handling of previously unseen
objects. Learning a physically accurate model at test time can provide
significant benefits in data efficiency, predictability, and reuse between
tasks. Tactile sensing can compliment vision with its robustness to occlusion,
but its temporal sparsity necessitates careful online exploration to maintain
data efficiency. Direct contact can also cause an unrestrained object to move,
requiring both shape and location estimation. In this work, we propose a
learning and exploration framework that uses only tactile data to
simultaneously determine the shape and location of rigid objects with minimal
robot motion. We build on recent advances in contact-rich system identification
to formulate a loss function that penalizes physical constraint violation
without introducing the numerical stiffness inherent in rigid-body contact.
Optimizing this loss, we can learn cuboid and convex polyhedral geometries with
less than 10s of randomly collected data after first contact. Our exploration
scheme seeks to maximize Expected Information Gain and results in significantly
faster learning in both simulated and real-robot experiments. More information
can be found at https://dairlab.github.io/activetactile

</details>


### [221] [PlanarMesh: Building Compact 3D Meshes from LiDAR using Incremental Adaptive Resolution Reconstruction](https://arxiv.org/abs/2510.13599)
*Jiahao Wang,Nived Chebrolu,Yifu Tao,Lintong Zhang,Ayoung Kim,Maurice Fallon*

Main category: cs.RO

TL;DR: PlanarMesh是一种新颖的增量式、基于网格的激光雷达重建系统，它能自适应调整网格分辨率，实现实时、紧凑且细节丰富的3D表面重建。


<details>
  <summary>Details</summary>
Motivation: 构建一个既能生成详细表面重建又计算高效的在线3D激光雷达建图系统是一个挑战。

Method: 该系统引入了一种名为“平面网格”（planar-mesh）的新表示，结合了平面建模和网格划分，以捕捉大型表面和详细几何结构。平面网格可以根据局部表面曲率和自由空间信息进行增量更新。它采用多线程架构和边界体积层次结构（BVH）进行高效数据存储和快速搜索，以实现实时性能。

Result: 实验结果表明，该方法在重建精度上与最先进的技术（包括截断符号距离函数、占用图和基于体素的网格划分）相当或超越，同时生成更小的输出文件（比原始输入小10倍，比其他基于网格的方法小5倍以上），并保持实时性能（对于64线传感器约为2 Hz）。

Conclusion: PlanarMesh提供了一种能够实时、高效地生成详细且紧凑的3D激光雷达表面重建的解决方案，其精度与现有技术相当甚至更优，同时大幅减少了数据量。

Abstract: Building an online 3D LiDAR mapping system that produces a detailed surface
reconstruction while remaining computationally efficient is a challenging task.
In this paper, we present PlanarMesh, a novel incremental, mesh-based LiDAR
reconstruction system that adaptively adjusts mesh resolution to achieve
compact, detailed reconstructions in real-time. It introduces a new
representation, planar-mesh, which combines plane modeling and meshing to
capture both large surfaces and detailed geometry. The planar-mesh can be
incrementally updated considering both local surface curvature and free-space
information from sensor measurements. We employ a multi-threaded architecture
with a Bounding Volume Hierarchy (BVH) for efficient data storage and fast
search operations, enabling real-time performance. Experimental results show
that our method achieves reconstruction accuracy on par with, or exceeding,
state-of-the-art techniques-including truncated signed distance functions,
occupancy mapping, and voxel-based meshing-while producing smaller output file
sizes (10 times smaller than raw input and more than 5 times smaller than
mesh-based methods) and maintaining real-time performance (around 2 Hz for a
64-beam sensor).

</details>


### [222] [Development of an Intuitive GUI for Non-Expert Teleoperation of Humanoid Robots](https://arxiv.org/abs/2510.13594)
*Austin Barret,Meng Cheng Lau*

Main category: cs.RO

TL;DR: 本研究旨在为类人机器人开发一个简单直观的图形用户界面（GUI），使非专业用户也能轻松操作机器人通过FIRA障碍赛。


<details>
  <summary>Details</summary>
Motivation: 许多现有的类人机器人系统在为非专业用户开发直观的图形用户界面方面投入不足，限制了其操作的便捷性。本研究旨在解决这一问题，使非专业操作员也能轻松控制机器人。

Method: 本研究将采用用户界面（UI）开发的通用实践，并结合人机交互（HRI）及其他相关概念，开发一个新的、可扩展的界面，以实现非专业用户的远程操作系统。

Result: 本研究的目标是开发一个为非专业人士量身定制的、简单直观的图形用户界面，使他们能够控制机器人完成FIRA规定的障碍赛。

Conclusion: 通过开发一个以非专业用户为中心的直观界面，本研究旨在显著降低类人机器人操作的门槛，使其能被更广泛的用户群体所利用，特别是在竞技和实践应用中。

Abstract: The operation of humanoid robotics is an essential field of research with
many practical and competitive applications. Many of these systems, however, do
not invest heavily in developing a non-expert-centered graphical user interface
(GUI) for operation. The focus of this research is to develop a scalable GUI
that is tailored to be simple and intuitive so non-expert operators can control
the robot through a FIRA-regulated obstacle course. Using common practices from
user interface development (UI) and understanding concepts described in
human-robot interaction (HRI) and other related concepts, we will develop a new
interface with the goal of a non-expert teleoperation system.

</details>


### [223] [Characterizing Lidar Point-Cloud Adversities Using a Vector Field Visualization](https://arxiv.org/abs/2510.13619)
*Daniel Choate,Jason Rife*

Main category: cs.RO

TL;DR: 本文提出一种可视化方法，通过生成矢量场图来帮助分析人员离线分类影响激光雷达扫描匹配的逆境模式。


<details>
  <summary>Details</summary>
Motivation: 人类分析师难以从原始点云数据中提取模式来分类影响激光雷达扫描匹配的逆境模式，需要一种更直观的工具来辅助离线分析。

Method: 该方法生成一个矢量场图，用于表征一对已配准点云之间的局部差异。分析师可以迭代地识别并移除逆境机制。

Result: 通过模拟研究和现场实验验证，人类分析师能够识别一系列逆境机制并迭代地从原始数据中移除它们，从而逐步关注更小的差异。

Conclusion: 所提出的可视化方法能有效帮助人类分析师分类激光雷达扫描匹配中的逆境模式，并揭示难以从原始数据中提取的模式。

Abstract: In this paper we introduce a visualization methodology to aid a human analyst
in classifying adversity modes that impact lidar scan matching. Our methodology
is intended for offline rather than real-time analysis. The method generates a
vector-field plot that characterizes local discrepancies between a pair of
registered point clouds. The vector field plot reveals patterns that would be
difficult for the analyst to extract from raw point-cloud data. After
introducing our methodology, we apply the process to two proof-of-concept
examples: one a simulation study and the other a field experiment. For both
data sets, a human analyst was able to reason about a series of adversity
mechanisms and iteratively remove those mechanisms from the raw data, to help
focus attention on progressively smaller discrepancies.

</details>


### [224] [A Modular Object Detection System for Humanoid Robots Using YOLO](https://arxiv.org/abs/2510.13625)
*Nicolas Pottier,Meng Cheng Lau*

Main category: cs.RO

TL;DR: 该研究提出了一种基于YOLOv9的通用机器人视觉模块，针对FIRA Hurocup数据集进行训练，并在ROS1中实现。与现有几何模型相比，YOLO模型在精度上相当，但计算成本更高，同时提供了更强的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 计算机视觉是机器人领域进步的显著障碍，许多任务因低效的视觉系统而受阻。

Method: 研究提出一个利用YOLOv9的通用视觉模块，该框架针对计算受限的机器人环境进行了优化。模型在为FIRA Hurocup定制的数据集上进行训练。新的视觉模块在ROS1中通过虚拟环境实现，以确保YOLO的兼容性。性能通过帧率（FPS）和平均精度均值（mAP）等指标进行评估，并与现有几何框架在静态和动态环境下进行比较。

Result: YOLO模型在与几何模型相当的精度下，计算成本更高，但提供了改进的鲁棒性。

Conclusion: 基于YOLOv9的视觉模块在机器人任务中显示出与传统几何模型相当的精度，并提供了更好的鲁棒性，尽管其计算成本更高，为机器人视觉系统提供了新的选择和权衡。

Abstract: Within the field of robotics, computer vision remains a significant barrier
to progress, with many tasks hindered by inefficient vision systems. This
research proposes a generalized vision module leveraging YOLOv9, a
state-of-the-art framework optimized for computationally constrained
environments like robots. The model is trained on a dataset tailored to the
FIRA robotics Hurocup. A new vision module is implemented in ROS1 using a
virtual environment to enable YOLO compatibility. Performance is evaluated
using metrics such as frames per second (FPS) and Mean Average Precision (mAP).
Performance is then compared to the existing geometric framework in static and
dynamic contexts. The YOLO model achieved comparable precision at a higher
computational cost then the geometric model, while providing improved
robustness.

</details>


### [225] [On Your Own: Pro-level Autonomous Drone Racing in Uninstrumented Arenas](https://arxiv.org/abs/2510.13644)
*Michael Bosello,Flavio Pinzarrone,Sara Kiade,Davide Aguiari,Yvo Keuter,Aaesha AlShehhi,Gyordan Caminati,Kei Long Wong,Ka Seng Chou,Junaid Halepota,Fares Alneyadi,Jacopo Panerati,Giovanni Pau*

Main category: cs.RO

TL;DR: 本文提出一种无人机自主系统，在受控和非受控环境中均能达到专业人类飞手的性能水平，并发布了飞行数据。


<details>
  <summary>Details</summary>
Motivation: 无人机视觉自主技术在各行业应用广泛，但现有系统多在高度受控环境中训练和评估，其在商业和野外操作中的直接适用性受限，无法应对新颖、非结构化环境。

Method: 研究者在受控环境（提供外部跟踪用于地面真值对比）和具有挑战性的、未安装仪器的环境（无地面真值）中分析并展示了其系统的能力。

Result: 该方法在两种场景下均能匹配专业人类飞手的性能。此外，研究者还公开了其方法和世界级人类飞手进行飞行的数据。

Conclusion: 所提出的无人机自主系统在受控和非受控的实际场景中均展现出与专业人类飞手相当的卓越性能，表明其在更广泛的商业和野外应用中具有潜力。

Abstract: Drone technology is proliferating in many industries, including agriculture,
logistics, defense, infrastructure, and environmental monitoring. Vision-based
autonomy is one of its key enablers, particularly for real-world applications.
This is essential for operating in novel, unstructured environments where
traditional navigation methods may be unavailable. Autonomous drone racing has
become the de facto benchmark for such systems. State-of-the-art research has
shown that autonomous systems can surpass human-level performance in racing
arenas. However, direct applicability to commercial and field operations is
still limited as current systems are often trained and evaluated in highly
controlled environments. In our contribution, the system's capabilities are
analyzed within a controlled environment -- where external tracking is
available for ground-truth comparison -- but also demonstrated in a
challenging, uninstrumented environment -- where ground-truth measurements were
never available. We show that our approach can match the performance of
professional human pilots in both scenarios. We also publicly release the data
from the flights carried out by our approach and a world-class human pilot.

</details>


### [226] [LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models](https://arxiv.org/abs/2510.13626)
*Senyu Fei,Siyin Wang,Junhao Shi,Zihao Dai,Jikun Cai,Pengfang Qian,Li Ji,Xinzhe He,Shiduo Zhang,Zhaoye Fei,Jinlan Fu,Jingjing Gong,Xipeng Qiu*

Main category: cs.RO

TL;DR: VLA模型在机器人操作中展现出脆弱性，对视觉和机器人状态扰动极为敏感，甚至可能忽略语言指令，挑战了高基准分数等同于真实能力的假设。


<details>
  <summary>Details</summary>
Motivation: VLA模型在机器人操作基准测试中成功率很高，但研究者怀疑这些结果可能掩盖了模型鲁棒性方面的根本弱点，因此需要进行系统的脆弱性分析。

Method: 通过引入七个维度的受控扰动（物体布局、相机视角、机器人初始状态、语言指令、光照条件、背景纹理和传感器噪声），系统地分析了多个最先进的VLA模型，以评估其在不同条件下的性能。

Result: 研究发现VLA模型普遍存在脆弱性，对相机视角和机器人初始状态等扰动因素表现出极度敏感性，性能在中等扰动下从95%降至30%以下。令人惊讶的是，模型对语言变化基本不敏感，甚至倾向于完全忽略语言指令。

Conclusion: 研究结果挑战了高基准分数等同于真实能力的假设，并强调了需要改进评估实践，以在实际变化下评估模型的可靠性。VLA模型在鲁棒性，特别是对视觉和机器人状态变化的适应性以及对语言指令的理解和利用方面存在显著缺陷。

Abstract: Visual-Language-Action (VLA) models report impressive success rates on
robotic manipulation benchmarks, yet these results may mask fundamental
weaknesses in robustness. We perform a systematic vulnerability analysis by
introducing controlled perturbations across seven dimensions: objects layout,
camera viewpoints, robot initial states, language instructions, light
conditions, background textures and sensor noise. We comprehensively analyzed
multiple state-of-the-art models and revealed consistent brittleness beneath
apparent competence. Our analysis exposes critical weaknesses: models exhibit
extreme sensitivity to perturbation factors, including camera viewpoints and
robot initial states, with performance dropping from 95% to below 30% under
modest perturbations. Surprisingly, models are largely insensitive to language
variations, with further experiments revealing that models tend to ignore
language instructions completely. Our findings challenge the assumption that
high benchmark scores equate to true competency and highlight the need for
evaluation practices that assess reliability under realistic variation.

</details>


### [227] [Hierarchical Discrete Lattice Assembly: An Approach for the Digital Fabrication of Scalable Macroscale Structures](https://arxiv.org/abs/2510.13686)
*Miana Smith,Paul Arthur Richard,Alexander Htet Kyaw,Neil Gershenfeld*

Main category: cs.RO

TL;DR: 该研究提出了一种利用简单机器人和互锁晶格积木，通过体素化、分层组块和机器人组装，实现大规模（米级）结构制造的方法，并引入了数字孪生模拟工具和新型模块化组装机器人。


<details>
  <summary>Details</summary>
Motivation: 桌面级数字制造已非常成熟，但用于生产大规模结构的系统通常仍复杂、昂贵且不可靠。

Method: 将目标结构体素化并填充晶格；将体素分组为互连的宏观积木（数十厘米），通过标准数字制造生产；移动机器人遍历结构并放置积木以形成米级结构；引入实时数字孪生模拟工具进行控制、协调和规划；设计新型模块化组装机器人以提高吞吐量。

Result: 通过演示米级物体的体素化、分层组块、路径规划和机器人制造，验证了该系统的可行性。

Conclusion: 该研究提出了一种利用简单机器人和互锁晶格积木，通过分层制造和智能协调，实现可扩展宏观结构制造的有效方法。

Abstract: Although digital fabrication processes at the desktop scale have become
proficient and prolific, systems aimed at producing larger-scale structures are
still typically complex, expensive, and unreliable. In this work, we present an
approach for the fabrication of scalable macroscale structures using simple
robots and interlocking lattice building blocks. A target structure is first
voxelized so that it can be populated with an architected lattice. These voxels
are then grouped into larger interconnected blocks, which are produced using
standard digital fabrication processes, leveraging their capability to produce
highly complex geometries at a small scale. These blocks, on the size scale of
tens of centimeters, are then fed to mobile relative robots that are able to
traverse over the structure and place new blocks to form structures on the
meter scale. To facilitate the assembly of large structures, we introduce a
live digital twin simulation tool for controlling and coordinating assembly
robots that enables both global planning for a target structure and live user
design, interaction, or intervention. To improve assembly throughput, we
introduce a new modular assembly robot, designed for hierarchical voxel
handling. We validate this system by demonstrating the voxelization,
hierarchical blocking, path planning, and robotic fabrication of a set of
meter-scale objects.

</details>


### [228] [InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for Generalist Robot Policy](https://arxiv.org/abs/2510.13778)
*Xinyi Chen,Yilun Chen,Yanwei Fu,Ning Gao,Jiaya Jia,Weiyang Jin,Hao Li,Yao Mu,Jiangmiao Pang,Yu Qiao,Yang Tian,Bin Wang,Bolun Wang,Fangjing Wang,Hanqing Wang,Tai Wang,Ziqin Wang,Xueyuan Wei,Chao Wu,Shuai Yang,Jinhui Ye,Junqiu Yu,Jia Zeng,Jingjing Zhang,Jinyu Zhang,Shi Zhang,Feng Zheng,Bowen Zhou,Yangkun Zhu*

Main category: cs.RO

TL;DR: InternVLA-M1是一个统一的框架，通过空间引导的视觉-语言-动作训练，将空间定位作为指令和机器人动作的关键连接，以实现可扩展的通用型指令遵循机器人。


<details>
  <summary>Details</summary>
Motivation: 推动指令遵循机器人向可扩展、通用型智能发展。

Method: 该方法采用两阶段流程：(i) 在超过230万空间推理数据上进行空间定位预训练，以确定“在哪里行动”，将指令与视觉、与具体机器人无关的位置对齐；(ii) 空间引导的动作后训练，通过即插即用的空间提示生成与具体机器人相关的动作，以决定“如何行动”。此外，还构建了一个模拟引擎，收集了24.4万个可泛化的抓取-放置任务，用于进一步扩展指令遵循能力。

Result: InternVLA-M1在SimplerEnv Google Robot上比无空间引导变体高出14.6%，在WidowX上高出17%，在LIBERO Franka上高出4.3%，并展示了更强的空间推理能力。通过模拟数据，在200个任务和3000多个物体上平均提升了6.2%。在真实世界集群抓取-放置任务中提升了7.3%，与合成数据协同训练后，在未见物体和新配置上提升了20.6%。在长时程推理密集型场景中，性能超越现有工作10%以上。

Conclusion: 空间引导训练是实现可扩展和有韧性的通用型机器人的统一原则。

Abstract: We introduce InternVLA-M1, a unified framework for spatial grounding and
robot control that advances instruction-following robots toward scalable,
general-purpose intelligence. Its core idea is spatially guided
vision-language-action training, where spatial grounding serves as the critical
link between instructions and robot actions. InternVLA-M1 employs a two-stage
pipeline: (i) spatial grounding pre-training on over 2.3M spatial reasoning
data to determine ``where to act'' by aligning instructions with visual,
embodiment-agnostic positions, and (ii) spatially guided action post-training
to decide ``how to act'' by generating embodiment-aware actions through
plug-and-play spatial prompting. This spatially guided training recipe yields
consistent gains: InternVLA-M1 outperforms its variant without spatial guidance
by +14.6% on SimplerEnv Google Robot, +17% on WidowX, and +4.3% on LIBERO
Franka, while demonstrating stronger spatial reasoning capability in box,
point, and trace prediction. To further scale instruction following, we built a
simulation engine to collect 244K generalizable pick-and-place episodes,
enabling a 6.2% average improvement across 200 tasks and 3K+ objects. In
real-world clustered pick-and-place, InternVLA-M1 improved by 7.3%, and with
synthetic co-training, achieved +20.6% on unseen objects and novel
configurations. Moreover, in long-horizon reasoning-intensive scenarios, it
surpassed existing works by over 10%. These results highlight spatially guided
training as a unifying principle for scalable and resilient generalist robots.
Code and models are available at
https://github.com/InternRobotics/InternVLA-M1.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [229] [Control of dynamical systems with neural networks](https://arxiv.org/abs/2510.12810)
*Lucas Böttcher*

Main category: eess.SY

TL;DR: 本文探讨了如何利用神经网络和自动微分技术来参数化控制输入，以解决离散和连续时间、确定性和随机动态系统中的控制问题。


<details>
  <summary>Details</summary>
Motivation: 控制问题在科学和工业应用中普遍存在，但许多情况下计算量大或难以进行分析求解。深度学习和自动微分的最新进展为解决这些问题提供了新的实用方法。

Method: 研究方法包括：1) 使用神经网络和现代机器学习库来参数化离散和连续时间系统以及确定性和随机动态系统中的控制输入。2) 对于连续时间系统，采用神经常微分方程（neural ODEs）来参数化控制输入。3) 对于离散时间系统，展示了如何使用自动微分方法实现和优化自定义控制输入参数化。

Result: 本文提出的方法为计算量大或分析上难以解决的控制任务提供了实用的解决方案，并适用于生物、工程、物理和医学等多个领域。

Conclusion: 所提出的方法为复杂的实际控制应用提供了有价值的工具，使其能够处理传统方法难以解决的计算密集型或分析上棘手的控制任务。

Abstract: Control problems frequently arise in scientific and industrial applications,
where the objective is to steer a dynamical system from an initial state to a
desired target state. Recent advances in deep learning and automatic
differentiation have made applying these methods to control problems
increasingly practical. In this paper, we examine the use of neural networks
and modern machine-learning libraries to parameterize control inputs across
discrete-time and continuous-time systems, as well as deterministic and
stochastic dynamics. We highlight applications in multiple domains, including
biology, engineering, physics, and medicine. For continuous-time dynamical
systems, neural ordinary differential equations (neural ODEs) offer a useful
approach to parameterizing control inputs. For discrete-time systems, we show
how custom control-input parameterizations can be implemented and optimized
using automatic-differentiation methods. Overall, the methods presented provide
practical solutions for control tasks that are computationally demanding or
analytically intractable, making them valuable for complex real-world
applications.

</details>


### [230] [Coherent Load Profile Synthesis with Conditional Diffusion for LV Distribution Network Scenario Generation](https://arxiv.org/abs/2510.12832)
*Alistair Brash,Junyi Lu,Bruce Stephen,Blair Brown,Robert Atkinson,Craig Michie,Fraser MacIntyre,Christos Tachtatzis*

Main category: eess.SY

TL;DR: 针对低压配电网负荷流可见性有限和缺乏真实负荷数据的挑战，本文提出一种条件扩散模型，用于合成低压配电站级别的日有功和无功功率曲线，能够捕捉变电站之间的协同行为，为配电网规划和运行提供更真实的场景。


<details>
  <summary>Details</summary>
Motivation: 配电网运营商在规划和拥堵管理方面面临挑战，原因是低压配电网负荷流可见性有限，且缺乏代表性配电馈线中真实、连贯的负荷数据。现有负荷画像方法（如典型曲线）过于简化，而采样和生成模型虽能近似负荷形状，却常忽略变电站之间的协同行为，这在低碳技术日益整合的背景下尤为重要，因为其无法捕捉负荷多样性。

Method: 本文提出一种条件扩散模型（Conditional Diffusion model），用于合成低压配电站级别的日有功和无功功率曲线。通过常规指标（捕捉时间、统计真实性）和潮流建模来评估其保真度，并将其与朴素模型和最先进模型进行基准测试。

Result: 结果表明，合成的负荷曲线无论是独立来看，还是作为更广泛电力系统背景下的群组，都具有合理性。条件扩散模型在生成真实场景方面表现出有效性。

Conclusion: 所提出的条件扩散模型能够有效生成用于次区域配电网规划和运行的真实场景，解决了缺乏真实、连贯且考虑变电站间协同行为的低压负荷数据的难题。

Abstract: Limited visibility of power distribution network power flows at the low
voltage level presents challenges to both distribution network operators from a
planning perspective and distribution system operators from a congestion
management perspective. Forestalling these challenges through scenario analysis
is confounded by the lack of realistic and coherent load data across
representative distribution feeders. Load profiling approaches often rely on
summarising demand through typical profiles, which oversimplifies the
complexity of substation-level operations and limits their applicability in
specific power system studies. Sampling methods, and more recently generative
models, have attempted to address this through synthesising representative
loads from historical exemplars; however, while these approaches can
approximate load shapes to a convincing degree of fidelity, the co-behaviour
between substations, which ultimately impacts higher voltage level network
operation, is often overlooked. This limitation will become even more
pronounced with the increasing integration of low-carbon technologies, as
estimates of base loads fail to capture load diversity. To address this gap, a
Conditional Diffusion model for synthesising daily active and reactive power
profiles at the low voltage distribution substation level is proposed. The
evaluation of fidelity is demonstrated through conventional metrics capturing
temporal and statistical realism, as well as power flow modelling. The results
show synthesised load profiles are plausible both independently and as a cohort
in a wider power systems context. The Conditional Diffusion model is
benchmarked against both naive and state-of-the-art models to demonstrate its
effectiveness in producing realistic scenarios on which to base sub-regional
power distribution network planning and operations.

</details>


### [231] [ExaModelsPower.jl: A GPU-Compatible Modeling Library for Nonlinear Power System Optimization](https://arxiv.org/abs/2510.12897)
*Sanjay Johnson,Dirk Lauinger,Sungho Shin,François Pacaud*

Main category: eess.SY

TL;DR: 本文介绍了ExaModelsPower.jl，一个开源建模库，用于创建GPU兼容的非线性交流最优潮流模型，并在大规模问题上展示了GPU求解器相对于CPU求解器高达两个数量级的加速。


<details>
  <summary>Details</summary>
Motivation: 随着GPU加速数学规划技术的成熟，人们对利用它们解决电力系统优化中的计算挑战越来越感兴趣。

Method: 本文引入了ExaModelsPower.jl，一个基于ExaModels.jl构建的开源建模库。它提供了一个高级接口，能自动为GPU求解器生成所有必要的callback函数，专为包含多时间周期和安全约束的大规模问题设计。作者使用该库在开源测试案例上对GPU和CPU求解器进行了基准测试。

Result: 研究结果表明，对于变量超过20,000个且求解精度高达$10^{-4}$的问题，GPU求解器与CPU上的替代工具相比，可以提供高达两个数量级的加速。然而，对于较小规模实例或更严格的容差，性能可能有所不同。

Conclusion: ExaModelsPower.jl库成功地利用GPU加速了大规模电力系统优化问题，特别是交流最优潮流模型，显著提高了计算效率。

Abstract: As GPU-accelerated mathematical programming techniques mature, there is
growing interest in utilizing them to address the computational challenges of
power system optimization. This paper introduces ExaModelsPower.jl, an
open-source modeling library for creating GPU-compatible nonlinear AC optimal
power flow models. Built on ExaModels.jl, ExaModelsPower.jl provides a
high-level interface that automatically generates all necessary callback
functions for GPU solvers. The library is designed for large-scale problem
instances, which may include multiple time periods and security constraints.
Using ExaModelsPower.jl, we benchmark GPU and CPU solvers on open-source test
cases. Our results show that GPU solvers can deliver up to two orders of
magnitude speedups compared to alternative tools on CPU for problems with more
than 20,000 variables and a solution precision of up to $10^{-4}$, while
performance for smaller instances or tighter tolerances may vary.

</details>


### [232] [Non-Gaussian Distribution Steering in Nonlinear Dynamics with Conjugate Unscented Transformation](https://arxiv.org/abs/2510.12946)
*Daniel C. Qi,Kenshiro Oguri,Puneet Singla,Maruthi R. Akella*

Main category: eess.SY

TL;DR: 本文提出了一种利用优化线性反馈控制在非线性环境中有效控制非高斯分布的方法，并通过序列凸规划算法实现，并在天体动力学示例中展示了其直接控制和准确近似分布矩的能力。


<details>
  <summary>Details</summary>
Motivation: 在天体动力学等高度非线性系统中，高斯分布通常会演变为非高斯分布，因此需要一种方法来有效控制这些非高斯分布。

Method: 该研究采用优化线性反馈控制来控制非高斯分布。它利用共轭无迹变换（Conjugate Unscented Transformation）来量化非高斯分布的高阶统计矩。方法重点控制与不确定性量化相关的sigma点，从而实现对整个分布和矩本身的控制。问题通过序列凸规划（sequential convex programming）算法求解。

Result: 该方法通过二体和三体示例进行了演示。结果表明，可以实现对单个矩的直接控制，并且在控制器的时间范围内，非高斯分布的矩在非线性动力学中得到了准确近似。

Conclusion: 该方法能够有效控制非线性系统中的非高斯分布及其矩，并在整个控制周期内准确近似这些矩。

Abstract: In highly nonlinear systems such as the ones commonly found in astrodynamics,
Gaussian distributions generally evolve into non-Gaussian distributions. This
paper introduces a method for effectively controlling non-Gaussian
distributions in nonlinear environments using optimized linear feedback
control. This paper utilizes Conjugate Unscented Transformation to quantify the
higher-order statistical moments of non-Gaussian distributions. The formulation
focuses on controlling and constraining the sigma points associated with the
uncertainty quantification, which would thereby reflect the control of the
entire distribution and constraints on the moments themselves. This paper
develops an algorithm to solve this problem with sequential convex programming,
and it is demonstrated through a two-body and three-body example. The examples
show that individual moments can be directly controlled, and the moments are
accurately approximated for non-Gaussian distributions throughout the
controller's time horizon in nonlinear dynamics.

</details>


### [233] [A Wideband Composite Sequence Impedance Model for Evaluation of Interactions in Unbalanced Power-Electronic-Based Power Systems](https://arxiv.org/abs/2510.12914)
*Zhi Liu,Chengxi Liu,Jiangbei Han,Rui Qiu,Mingyuan Liu*

Main category: eess.SY

TL;DR: 本文提出了一种基于宽带复合序阻抗模型（WCSIM）的分析方法，用于评估电力电子电力系统在不平衡故障或负载下的相互作用，并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 评估电力电子电力系统在不平衡电网故障或不平衡负载下的相互作用，并直观地评估正序、负序和零序电路之间小信号互连对不平衡电力系统相互作用稳定性的影响。

Method: 提出宽带复合序阻抗模型（WCSIM），并基于此模型开发分析方法。该方法直观地评估了正序、负序和零序电路之间的小信号互连。

Result: 该方法在永磁同步发电机弱电网系统单相接地故障下的有效性得到了验证。频率扫描结果和控制器硬件在环测试证实了WCSIM的正确性以及基于WCSIM分析方法的有效性。

Conclusion: WCSIM及其分析方法是正确且有效的，可用于分析不平衡电力系统中的相互作用稳定性。

Abstract: This paper proposes a wideband composite sequence impedance model
(WCSIM)-based analysis method to evaluate the interactions in
power-electronic-based power systems subjected to unbalanced grid faults or
with unbalanced loads. The WCSIM-based method intuitively assesses the impact
of the small-signal interconnection among the positive-, negative-, and
zero-sequence circuits on the interaction stability of unbalanced power
systems. The effectiveness of this method is demonstrated using a permanent
magnet synchronous generator-based weak grid system under a
single-line-to-ground fault (SLGF). Frequency scanning results and controller
hardware-in-loop tests validate both the correctness of the WCSIM and the
effectiveness of the WCSIM-based analysis method.

</details>


### [234] [Enhancing Profit and CO2 Mitigation: Commercial Direct Air Capture Design and Operation with Power Market Volatility](https://arxiv.org/abs/2510.12949)
*Zhiyuan Fan,Elizabeth Dentzer,James Glynn,David S. Goldberg,Julio Friedmann,Bolun Xu*

Main category: eess.SY

TL;DR: 本研究评估了在批发电力市场中，通过碳激励措施盈利的商业直接空气捕获（DAC）技术的运营。结果表明，DAC可以通过利用电力市场波动在低价时段运营来提高成本效益，但利润驱动的决策可能减少总碳去除量。灵活的DAC技术和优化的激励机制至关重要，而电力定价中的碳税对DAC系统可能适得其反。


<details>
  <summary>Details</summary>
Motivation: 当前的脱碳努力不足以实现净零温室气体排放目标，凸显了直接空气捕获（DAC）等大规模二氧化碳去除方法的必要性。然而，DAC技术因其巨大的电力消耗而面临整合挑战。

Method: 研究建模了四种商业DAC技术，并分析了它们在美国三个代表性地点（加利福尼亚、德克萨斯和纽约）的运营情况。评估了通过货币化碳激励措施获取收益，同时从批发电力市场购买电力的商业运营模式。同时考虑了环境条件（如温度和相对湿度）对减排能力的影响。

Result: 商业DAC运营可以策略性地利用电力市场的波动性，仅在低电价时期运行，从而实现成本效益。环境操作条件（如温度和相对湿度）对减排能力有显著影响。利润驱动的决策会引入气候-经济权衡，可能降低DAC的容量因子并减少二氧化碳总去除量。循环周期短、灵活性高的DAC技术能更好地利用电价波动。电力市场中的持续低价时段通常与低电网排放时期（例如加州太阳能“鸭子曲线”期间）协同。存在针对利润驱动运营的最佳激励设计，而电力定价中的碳税政策对DAC系统是适得其反的。

Conclusion: DAC技术可以通过利用电力市场波动实现成本效益高的脱碳转型，特别是对于具有高灵活性的技术。然而，利润驱动的运营可能导致气候-经济权衡，降低碳去除总量。因此，需要设计最优的激励政策来平衡盈利与脱碳目标，同时应避免在电力定价中引入对DAC系统不利的碳税。

Abstract: Current decarbonization efforts are falling short of meeting the net-zero
greenhouse gas (GHG) emission target, highlighting the need for substantial
carbon dioxide removal methods such as direct air capture (DAC). However,
integrating DACs poses challenges due to their enormous power consumption. This
study assesses the commercial operation of various DAC technologies that earn
revenue using monetized carbon incentives while purchasing electricity from
wholesale power markets. We model four commercial DAC technologies and examine
their operation in three representative locations including California, Texas,
and New York. Our findings reveal that commercial DAC operations can take
financial advantage of the volatile power market to operate only during
low-price periods strategically, offering a pathway to facilitate a
cost-efficient decarbonization transition. The ambient operational environment
such as temperature and relative humidity has non-trivial impact on abatement
capacity. Profit-driven decisions introduce climate-economic trade-offs that
might decrease the capacity factor of DAC and reduce total CO2 removal. These
implications extend throughout the entire lifecycle of DAC developments and
influence power systems and policies related to full-scale DAC implementation.
Our study shows that DAC technologies with shorter cycle spans and higher
flexibility can better exploit the electricity price volatility, while power
markets demonstrate persistent low-price windows that often synergize with low
grid emission periods, like during the solar "duck curve" in California. An
optimal incentive design exists for profit-driven operations while carbon-tax
policy in electricity pricing is counterproductive for DAC systems.

</details>


### [235] [Model predictive control lowers barriers to adoption of heat-pump water heaters: A field study](https://arxiv.org/abs/2510.12955)
*Levi D. Reyes Premer,Elias N. Pergantis,Leo Semmelmann,Davide Ziviani,Kevin J. Kircher*

Main category: eess.SY

TL;DR: 本文开发并实地测试了一种新型模型预测控制（MPC）系统，使120V热泵热水器（HPWH）无需240V电路和电阻加热元件即可高效保持舒适，并显著降低能耗和运营成本。


<details>
  <summary>Details</summary>
Motivation: 美国住房中，热水器是第二大能源消耗。传统热泵热水器（HPWHs）通常需要240V电路来驱动备用电阻加热元件，以应对大量用水需求，这会使安装成本增加一半或更多。因此，需要一种无需240V电路且能高效维持舒适度的解决方案。

Method: 研究开发了一种新颖的模型预测控制（MPC）系统。该系统利用机器学习预测器集成来预测大量用水需求，从而实现预热。该系统在实际住宅中进行了实地测试，并与带有标准控制的240V HPWH以及将水恒定存储在高温（60°C）的120V HPWH进行了比较。

Result: 该MPC系统使120V HPWH无需电阻加热元件即可有效保持舒适。与带有标准控制的240V HPWH相比，MPC系统在分时电价下平均降低23%的能源成本，在按小时计费下平均降低28%。与目前120V HPWH中常见的恒定高温（60°C）储水方式相比，MPC平均节省37%的能源。此外，分析表明，通过MPC系统运行的120V HPWH在大多数安装场景中具有经济吸引力。

Conclusion: 所开发的MPC系统能够使120V热泵热水器在没有240V电路和电阻加热元件的情况下，高效地提供舒适热水，显著降低能源成本，并具有良好的经济性，克服了传统HPWH的安装障碍。

Abstract: Electric heat-pump water heaters (HPWHs) could reduce the energy costs,
emissions, and power grid impacts associated with water heating, the
second-largest energy use in United States housing. However, most HPWHs today
require 240 V circuits to power the backup resistance heating elements they use
to maintain comfort during large water draws. Installing a 240 V circuit can
increase the up-front cost of a HPWH by half or more. This paper develops and
field-tests the first control system that enables a 120 V HPWH to efficiently
maintain comfort without resistance heating elements. The novel model
predictive control (MPC) system enables pre-heating in anticipation of large
water draws, which it forecasts using an ensemble of machine learning
predictors. By shifting electrical load over time, MPC also reduces energy
costs on average by 23% and 28% under time-of-use pricing and hourly pricing,
respectively, relative to a 240 V HPWH with standard controls. Compared to the
increasingly common practice in 120 V HPWHs of storing water at a constant,
high temperature (60 {\deg}C) to ensure comfort, MPC saves 37% energy on
average. In addition to demonstrating MPC's benefits in a real, occupied house,
this paper discusses implementation challenges and costs. A simple payback
analysis suggests that a 120 V HPWH, operated by the MPC system developed here,
would be economically attractive in most installation scenarios.

</details>


### [236] [Competitive EV charging station location with queues](https://arxiv.org/abs/2510.12961)
*The Minh Nguyen,Nagisa Sugishita,Margarida Carvalho,Amira Dems*

Main category: eess.SY

TL;DR: 本研究针对竞争市场中电动汽车充电基础设施规划的挑战，通过纳入竞争对手站点和更真实的排队系统，开发了一种双层规划模型，并结合启发式算法，以优化充电站选址和提高吞吐量，其策略优于现有网络。


<details>
  <summary>Details</summary>
Motivation: 电动汽车公共充电基础设施规划在竞争市场中面临重大挑战，因为多个服务提供商会影响拥堵和用户行为。现有建模框架未能充分考虑竞争对手站点的存在和更真实的排队系统。

Method: 1. 分析了M/M/1/K、M/M/s/K和M/Er/s/K三种有限排队系统，推导了用户行为指标的解析表达式。2. 将基于排队的 用户行为模型嵌入双层规划：上层最大化可达性（吞吐量），下层通过用户均衡捕捉用户站点选择。3. 采用竞争性拥堵用户选择设施选址模型的重构方法近似求解双层问题。4. 引入基于替代的启发式算法以增强可扩展性。5. 在蒙特利尔的真实案例中验证了该方法。

Result: 该模型产生的（重）选址策略优于现有网络。研究展示了用户选择行为假设和竞争如何影响吞吐量和选址决策，并提供了管理见解。

Conclusion: 本方法提供了一种工具，可以将充电服务质量（通过排队指标）和现有竞争纳入充电站规划中，从而优化电动汽车充电基础设施的布局。

Abstract: Electric vehicle (EV) public charging infrastructure planning faces
significant challenges in competitive markets, where multiple service providers
affect congestion and user behavior. This work extends existing modeling
frameworks by incorporating the presence of competitors' stations and more
realistic queueing systems.
  First, we analyze three finite queueing systems, M/M/1/K, M/M/s/K, and
M/Er/s/K, with varying numbers of servers (charging outlets) and service time
distributions, deriving analytic expressions for user behavior metrics. Second,
we embed the queueing-based user behavior model into a bilevel program, where
the upper level locates new charging stations to maximize accessibility
(throughput), and the lower level captures users' station choices via a user
equilibrium. Third, we apply a reformulation from competitive congested
user-choice facility location models to approximately solve the bilevel problem
and introduce a surrogate-based heuristic to enhance scalability. Fourth, we
showcase our methodology on a real-world case study of an urban area in
Montreal (Canada), offering managerial insights into how user-choice behavior
assumptions and competition affect throughput and location decisions. The
results demonstrate that our model yields (re)location strategies that
outperform the existing network. More broadly, this approach provides a tool
for incorporating charging service quality-through queueing metrics-and
existing competition into station planning.

</details>


### [237] [Comparison of Forced and Unforced Rendezvous, Proximity Operations, and Docking Under Model Mismatch](https://arxiv.org/abs/2510.13004)
*Robert Muldrow,Channing Ludden,Christopher Petersen*

Main category: eess.SY

TL;DR: 本文比较了追踪卫星在交会、近距离操作和对接（RPOD）机动中，强制运动和非强制运动所需的燃料消耗，发现非强制运动并非天生比强制运动更省燃料。


<details>
  <summary>Details</summary>
Motivation: 随着航天工业的扩张，对燃料效率、成本效益和任务寿命的需求增加，改进的RPOD模型变得至关重要。

Method: 本文对比了Clohessy-Wiltshire（CW）方程预测的轨迹与计算更复杂、保真度更高的RPOD模型预测的轨迹，以评估模型失配程度。通过比较自然运动环绕（NMC）与可比的强制运动环绕，并以维持CW轨迹所需的GNC脉冲机动来量化CW模型失配。

Result: 研究表明，非强制运动并非天生比强制运动更节省燃料。

Conclusion: 非强制运动并不比强制运动具有固有的燃料效率优势，但更高的燃料效率允许延长轨道操作。

Abstract: This paper compares the required fuel usage for forced and unforced motion of
a chaser satellite engaged in Rendezvous, Proximity Operations, and Docking
(RPOD) maneuvers. Improved RPOD models are vital, particularly as the space
industry expands and demands for improved fuel efficiency, cost effectiveness,
and mission life span increase. This paper specifically examines the Clohessy-
Wiltshire (CW) Equations and the extent of model mismatch by comparing pre-
dicted trajectories from this model with a more computationally complex, higher
fidelity RPOD model. This paper assesses several test cases of similar mission
parameters, in each case comparing natural motion circumnavigation (NMC) with
comparable forced motion circumnavigation. The Guidance, Navigation, and Con-
trol (GNC) impulse maneuvers required to maintain the supposedly zero fuel CW
trajectories is representative of the extent of CW model mismatch. This paper
demonstrates that unforced motions are not inherently more fuel efficient than
forced motions, thus permitting extended orbital operations given the higher
fuel efficiency.

</details>


### [238] [Identifying Best Candidates for Busbar Splitting](https://arxiv.org/abs/2510.13000)
*Giacomo Bastianel,Dirk Van Hertem,Hakan Ergun,Line Roald*

Main category: eess.SY

TL;DR: 本文提出了一套度量标准，用于识别电网中适合母线分段（BuS）的候选母线，以缓解电网拥堵并降低发电成本，并通过优化模型和交流潮流仿真验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 日益增长的电力需求和可再生能源的整合加剧了输电网的拥堵。通过母线分段和优化输电开关等电网拓扑优化可以缓解拥堵并降低发电成本。然而，母线分段优化涉及大量二进制变量，且在大型电网中分析所有变电站的潜在拓扑动作计算量巨大，难以实现。

Method: 本文提出了一套度量标准，用于识别和排序具有前景的母线分段候选点，重点是找到拓扑优化可以降低发电成本的母线。为了评估母线分段对识别出的母线的影响，研究人员使用了一个结合混合整数凸二次母线分段模型来计算最优拓扑，并通过非线性非凸交流最优潮流（AC OPF）仿真来验证其交流可行性。

Result: 通过在不同规模的测试案例上测试和验证所提出的度量标准，结果表明这些度量标准能够识别出通过拓扑优化（母线分段）可以降低总发电成本的母线。这使得能够有效选择适合母线分段的母线，而无需逐一测试电网中的所有母线。

Conclusion: 所提出的度量标准能够有效选择适合母线分段的母线，无需逐一测试电网中的所有母线，从而提高了计算效率，为电网拓扑优化提供了实用的解决方案。

Abstract: Rising electricity demand and the growing integration of renewables are
intensifying congestion in transmission grids. Grid topology optimization
through busbar splitting (BuS) and optimal transmission switching can alleviate
grid congestion and reduce the generation costs in a power system. However, BuS
optimization requires a large number of binary variables, and analyzing all the
substations for potential new topological actions is computationally
intractable, particularly in large grids. To tackle this issue, we propose a
set of metrics to identify and rank promising candidates for BuS, focusing on
finding buses where topology optimization can reduce generation costs. To
assess the effect of BuS on the identified buses, we use a combined
mixed-integer convex-quadratic BuS model to compute the optimal topology and
test it with the non-linear non-convex AC optimal power flow (OPF) simulation
to show its AC feasibility. By testing and validating the proposed metrics on
test cases of different sizes, we show that they are able to identify busbars
that reduce the total generation costs when their topology is optimized. Thus,
the metrics enable effective selection of busbars for BuS, with no need to test
every busbar in the grid, one at a time.

</details>


### [239] [Data to Certificate: Guaranteed Cost Control with Quantization-Aware System Identification](https://arxiv.org/abs/2510.13024)
*Shahab Ataei,Dipankar Maity,Debdipta Goswami*

Main category: eess.SY

TL;DR: 本文研究了云辅助系统中状态和输入数据量化对线性时不变系统辨识的影响，推导了辨识误差的最坏情况边界，并开发了一种基于LMI的鲁棒控制器以实现保证成本控制。


<details>
  <summary>Details</summary>
Motivation: 低功耗、资源受限的控制系统（如微型无人机）常采用云辅助系统辨识和控制。在此类设置中，数据通过低带宽无线链路传输，导致量化问题，因此需要研究量化对系统性能的影响。

Method: 该研究调查了状态和输入数据量化对线性时不变（LTI）系统辨识的影响，推导了辨识误差的最坏情况边界。在此误差边界下，建立了一个仅依赖于量化数据和量化分辨率的模型误差基本边界，并开发了一种基于线性矩阵不等式（LMI）的保证成本鲁棒控制器。

Result: 研究建立了模型误差的基本边界，该边界仅取决于量化数据和量化分辨率。在此误差边界下，开发了一种基于LMI的保证成本鲁棒控制器。

Conclusion: 本文成功地为云辅助系统中带有量化误差的LTI系统辨识和控制提供了理论基础和实用方法，通过推导误差边界并设计鲁棒控制器，实现了对量化影响下的系统性能保证。

Abstract: Cloud-assisted system identification and control have emerged as practical
solutions for low-power, resource-constrained control systems such as
micro-UAVs. In a typical cloud-assisted setting, state and input data are
transmitted from local agents to a central computer over low-bandwidth wireless
links, leading to quantization. This paper investigates the impact of state and
input data quantization on a linear time invariant (LTI) system identification,
derives a worst-case bound on the identification error, and develops a robust
controller for guaranteed cost control. We establish a fundamental bound on the
model error that depends only on the quantized data and quantization
resolution, and develop a linear matrix inequality (LMI) based guaranteed cost
robust controller under this error bound.

</details>


### [240] [Decision-dependent Robust Charging Infrastructure Planning for Light-duty Truck Electrification at Industrial Sites: Scheduling and Abandonment](https://arxiv.org/abs/2510.13100)
*Yifu Ding,Ruicheng Ao,Pablo Duenas-Martinez,Thomas Magnanti*

Main category: eess.SY

TL;DR: 该研究开发了一个两阶段鲁棒充电基础设施规划模型，用于工业场所轻型卡车的电气化，旨在减少温室气体排放。模型考虑了多种充电器类型、位置、机会充电调度、放弃充电行为、夜间充电、续航焦虑以及决策依赖的停车时长不确定性，并在一个露天矿场案例中取得了高效且鲁棒的优化结果。


<details>
  <summary>Details</summary>
Motivation: 许多工业场所依赖柴油轻型卡车运输工人和小规模设施，导致大量的温室气体排放。为了解决这一问题，有必要对这些卡车进行电气化改造。

Method: 研究开发了一个两阶段鲁棒充电基础设施规划模型，该模型被表述为一个混合整数线性规划（MILP）。它优化了充电基础设施（从多种充电器类型和潜在位置中选择）并确定了每辆卡车的机会充电计划。模型引入了“带有放弃的调度问题”，即卡车在等待时间超过最大阈值时会放弃充电。此外，还纳入了夜间充电和续航焦虑对等待和放弃行为的影响。为了表示卡车停车时间的随机性和异质性，构建了一个决策依赖的鲁棒不确定性集。该模型被应用于一个露天采矿场的案例研究，并通过将问题分解为月度子问题和使用启发式方法，处理了全年数据集。

Result: 在多样化的不确定性场景下，该模型在合理的计算时间内实现了不到0.1%的最优性差距，证明了其高效性和鲁棒性。

Conclusion: 该研究提出的两阶段鲁棒充电基础设施规划模型能够有效地规划工业场所轻型卡车的电气化，通过优化充电基础设施和调度，显著减少温室气体排放，同时考虑了工业现场的严格调度、放弃行为、续航焦虑和停车时间的不确定性，并在实际案例中表现出色。

Abstract: Many industrial sites rely on diesel-powered light-duty trucks to transport
workers and small-scale facilities, which has resulted in a significant amount
of greenhouse emissions (GHGs). To address this, we developed a two-stage
robust charging infrastructure planning model for electrifying light-duty
trucks at industrial sites. The model is formulated as a mixed-integer linear
programming (MILP) that optimizes the charging infrastructure, selected from
multiple charger types and potential locations, and determines opportunity
charging schedules for each truck based on the chosen infrastructure. Given the
strict stopping points and schedules at industrial sites, we introduced a
scheduling problem with abandonment, where trucks forgo charging if their
waiting times exceed a maximum threshold. We also further incorporated the
impacts of overnight charging and range anxiety on waiting and abandonment
behaviors. To represent the stochastic and heterogeneous parking durations of
trucks, we constructed a decision-dependent robust uncertainty set in which
parking time variability flexibly depends on charging choices. We applied the
model in a case study of an open-pit mining site, which plans charger
installations in eight zones and schedules a fleet of around 200 trucks. By
decomposing the problem into monthly subproblems and using heuristic
approaches, for the whole-year dataset, the model achieves an optimality gap of
less than 0.1 % within a reasonable computation time under diverse uncertainty
scenarios.

</details>


### [241] [Safe Driving in Occluded Environments](https://arxiv.org/abs/2510.13114)
*Zhuoyuan Wang,Tongyao Jia,Pharuj Rajborirug,Neeraj Ramesh,Hiroyuki Okuda,Tatsuya Suzuki,Soummya Kar,Yorie Nakahira*

Main category: eess.SY

TL;DR: 本文提出了一种针对自动驾驶中因遮挡引起的潜在风险的概率安全证书，通过概率不变性放松了对风险关键状态的可观测性要求，生成线性动作约束以确保长期安全。


<details>
  <summary>Details</summary>
Motivation: 现有模型驱动（基于集合不变性）和数据驱动的控制技术在处理自动驾驶中由遮挡引起的潜在风险（即安全关键状态不可观测）时面临挑战，无法有效处理这些不可见的风险。

Method: 本文提出了一种针对潜在风险的概率安全证书。其核心技术是应用概率不变性，这放松了集合不变性方法对风险关键状态的严格可观测性要求。该技术提供线性动作约束，将潜在风险概率限制在可接受范围内。

Result: 所提出的线性动作约束可以集成到模型预测控制器或嵌入到数据驱动策略中，以缓解潜在风险。在CARLA模拟器中进行的测试表明，该方法与现有技术相比，能够在遮挡环境中确保实时控制的长期安全性，且不过于保守，并对暴露的风险具有透明度。

Conclusion: 所提出的方法通过概率安全证书有效解决了自动驾驶中遮挡带来的潜在风险问题，实现了在不可观测风险存在的情况下，既保证长期安全又不失实时性和透明度的控制。

Abstract: Ensuring safe autonomous driving in the presence of occlusions poses a
significant challenge in its policy design. While existing model-driven control
techniques based on set invariance can handle visible risks, occlusions create
latent risks in which safety-critical states are not observable. Data-driven
techniques also struggle to handle latent risks because direct mappings from
risk-critical objects in sensor inputs to safe actions cannot be learned
without visible risk-critical objects. Motivated by these challenges, in this
paper, we propose a probabilistic safety certificate for latent risk. Our key
technical enabler is the application of probabilistic invariance: It relaxes
the strict observability requirements imposed by set-invariance methods that
demand the knowledge of risk-critical states. The proposed techniques provide
linear action constraints that confine the latent risk probability within
tolerance. Such constraints can be integrated into model predictive controllers
or embedded in data-driven policies to mitigate latent risks. The proposed
method is tested using the CARLA simulator and compared with a few existing
techniques. The theoretical and empirical analysis jointly demonstrate that the
proposed methods assure long-term safety in real-time control in occluded
environments without being overly conservative and with transparency to exposed
risks.

</details>


### [242] [Partitioned Scheduling for DAG Tasks Considering Probabilistic Execution Time](https://arxiv.org/abs/2510.13279)
*Fuma Omori,Atsushi Yano,Takuya Azumi*

Main category: eess.SY

TL;DR: 本文提出了一种针对自动驾驶系统中DAG任务的概率调度方法，通过任务集分区和利用单处理器概率保证，在分区调度下提高了可调度性，并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统需要实时保证，但其加速特性（如缓存和流水线）导致实际执行时间远低于最坏情况。因此，相比于最坏情况保证，通过概率阈值确保约束满足的概率方法更适合这些系统。现有针对DAG的概率保证研究不如单处理器领域成熟。

Method: 本文利用单处理器概率保证的现有成果，提出了一种任务集分区方法，以确保在分区调度下的可调度性。通过随机生成的DAG任务集进行评估，并比较了四种装箱启发式算法。

Result: 与现有DAG的概率可调度性分析方法相比，本文提出的方法能够调度更多的任务集，并且平均分析时间更短。在装箱启发式算法的比较中，Item-Centric Worst-Fit-Decreasing（IC-WFD）调度了最多的任务集。

Conclusion: 本文提出了一种有效的分区调度方法，能够为DAG任务提供概率保证，并在性能上超越现有方法。同时，IC-WFD被确定为最适合此场景的装箱启发式算法。

Abstract: Autonomous driving systems, critical for safety, require real-time guarantees
and can be modeled as DAGs. Their acceleration features, such as caches and
pipelining, often result in execution times below the worst-case. Thus, a
probabilistic approach ensuring constraint satisfaction within a probability
threshold is more suitable than worst-case guarantees for these systems. This
paper considers probabilistic guarantees for DAG tasks by utilizing the results
of probabilistic guarantees for single processors, which have been relatively
more advanced than those for multi-core processors. This paper proposes a task
set partitioning method that guarantees schedulability under the partitioned
scheduling. The evaluation on randomly generated DAG task sets demonstrates
that the proposed method schedules more task sets with a smaller mean analysis
time compared to existing probabilistic schedulability analysis for DAGs. The
evaluation also compares four bin-packing heuristics, revealing Item-Centric
Worst-Fit-Decreasing schedules the most task sets.

</details>


### [243] [Multipolar dynamics of social segregation: Data validation on Swedish vaccination statistics](https://arxiv.org/abs/2510.13396)
*Luka Baković,David Ohlin,Emma Tegling*

Main category: eess.SY

TL;DR: 本文验证了多极意见动力学模型，并提出了一种处理两个相关变量数据集的通用方法，成功应用于瑞典COVID-19疫苗接种率与政治参与度数据，揭示了意见隔离现象及空间偏差相关性的重要性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是验证多极意见动力学模型在真实世界数据上的有效性，并开发一种适用于分析两个相关变量的通用方法。

Method: 研究提出了一种将多极模型应用于两个相关变量数据集的通用方法，并使用瑞典COVID-19疫苗接种率和政治参与度数据进行了测试。

Result: 结果显示，该模型成功捕捉了数据中展示的意见隔离现象。研究表明，偏差的空间相关性对于产生这种结果是必要的。相反，偏差的混合会导致更同质的意见分布和多数意见（如投票或接种疫苗）的更高渗透率。

Conclusion: 结论是多极意见动力学模型能够有效捕捉和解释意见隔离现象，且偏差的空间相关性是形成这种隔离的关键因素。混合偏差则会导致意见趋于一致。

Abstract: We perform a validation analysis on the multipolar model of opinion dynamics.
A general methodology for using the model on datasets of two correlated
variables is proposed and tested using data on the relationship between
COVID-19 vaccination rates and political participation in Sweden. The model is
shown to successfully capture the opinion segregation demonstrated by the data
and spatial correlation of biases is demonstrated as necessary for the result.
A mixing of the biases on the other hand leads to a more homogeneous opinion
distribution, and greater penetration of the majority opinion, which here
corresponds to a decision to vote or vaccinate.

</details>


### [244] [On the Flexibility Potential of a Swiss Distribution Grid: Opportunities and Limitations](https://arxiv.org/abs/2510.13449)
*Jan Brändle,Julie Rousseau,Pulkit Nahata,Gabriela Hug*

Main category: eess.SY

TL;DR: 本文以瑞士配电网为案例研究，分析了分布式可再生能源和电气化设备（如热泵、光伏）对配电网聚合柔性的影响，发现其能显著增强柔性，但受电网拓扑和约束影响，柔性潜力并非线性增长。


<details>
  <summary>Details</summary>
Motivation: 随着分布式可再生能源和电气化（供暖、交通）设备的增加，现代配电网中的柔性设备数量迅速增长。利用这些小规模分布式资源的聚合柔性对于维护未来电网的整体稳定性至关重要。

Method: 本文以瑞士瓦伦施塔特配电网为案例研究，通过模拟未来场景，分析了热泵和光伏系统等设备对配电网聚合柔性潜力的影响，并探讨了其时变性和季节性变化。

Result: 研究表明，整合热泵和光伏系统等设备能显著增强配电网的柔性。聚合柔性具有时变性，并随季节变化。此外，未来场景模拟显示，聚合柔性不会随柔性设备渗透率的提高而线性或单调增加，这主要是由于单个馈线过载，凸显了电网拓扑和网络约束对聚合柔性潜力的影响。

Conclusion: 分布式柔性设备能显著提升配电网柔性，但其聚合潜力受到电网拓扑结构和网络约束的显著影响，并非简单地随设备数量增加而线性提升，需要综合考虑电网固有特性。

Abstract: The growing integration of distributed renewable generation and the
electrification of heating and transportation are rapidly increasing the number
of flexible devices within modern distribution grids. Leveraging the aggregated
flexibility of these small-scale distributed resources is essential to
maintaining future grid-wide stability. This work uses the Swiss distribution
grid of Walenstadt as a case study to provide insights into the aggregated
flexibility potential of distribution grids. It demonstrates that incorporating
devices such as heat pumps and photovoltaic systems significantly enhances
distribution grid flexibility. It investigates the time-varying nature of
aggregated flexibility and highlights how it can vary seasonally. Furthermore,
simulations of future scenarios reveal that aggregated flexibility does not
increase linearly or monotonically with higher levels of flexible device
penetration. This is primarily due to the overloading of individual feeders,
which underscores the impact of grid topology and network constraints on the
aggregated flexibility potential.

</details>


### [245] [Physics-Informed Neural Network Modeling of Vehicle Collision Dynamics in Precision Immobilization Technique Maneuvers](https://arxiv.org/abs/2510.13461)
*Yangye Jiang,Jiachen Wang,Daofei Li*

Main category: eess.SY

TL;DR: 该论文提出了一种双物理信息神经网络（PINN）框架，通过两个互补网络，解决了车辆碰撞动力学预测中计算效率、预测精度和数据需求之间的权衡问题，实现了高精度、实时且具备不确定性量化的碰撞预测。


<details>
  <summary>Details</summary>
Motivation: 先进安全系统和碰撞后控制应用需要精确的车辆碰撞动力学预测，但现有方法在计算效率、预测精度和数据需求方面存在固有的权衡。

Method: 该框架包含两个互补网络：1. 第一个网络将高斯混合模型（GMM）与PINN架构结合，从有限元分析（FEA）数据中学习冲击力分布，并强制执行动量守恒和能量一致性约束。2. 第二个网络采用自适应PINN，具有动态约束加权和自适应物理保护层，用于预测碰撞后车辆动力学。此外，该框架通过时变参数进行不确定性量化，并通过微调策略实现快速适应。

Result: 冲击力模型在FEA数据集上的力预测相对误差低于15.0%；车辆动力学模型在缩放车辆实验中，与传统四自由度模型相比，平均轨迹预测误差降低了63.6%。整个集成系统保持毫秒级计算效率，适用于实时应用，并提供安全关键控制所需的概率置信区间。通过FEA仿真、动态建模和缩放车辆实验的综合验证，证实了该框架在精确制动技术（PIT）场景和一般碰撞动力学预测中的有效性。

Conclusion: 该双PINN框架在车辆碰撞动力学预测方面表现出显著的改进，能够以高精度、高效率和不确定性量化的方式预测碰撞行为，为先进安全系统和实时控制应用提供了关键支持，特别适用于精确制动技术场景和一般碰撞动力学预测。

Abstract: Accurate prediction of vehicle collision dynamics is crucial for advanced
safety systems and post-impact control applications, yet existing methods face
inherent trade-offs among computational efficiency, prediction accuracy, and
data requirements. This paper proposes a dual Physics-Informed Neural Network
framework addressing these challenges through two complementary networks. The
first network integrates Gaussian Mixture Models with PINN architecture to
learn impact force distributions from finite element analysis data while
enforcing momentum conservation and energy consistency constraints. The second
network employs an adaptive PINN with dynamic constraint weighting to predict
post-collision vehicle dynamics, featuring an adaptive physics guard layer that
prevents unrealistic predictions whil e preserving data-driven learning
capabilities. The framework incorporates uncertainty quantification through
time-varying parameters and enables rapid adaptation via fine-tuning
strategies. Validation demonstrates significant improvements: the impact force
model achieves relative errors below 15.0% for force prediction on finite
element analysis (FEA) datasets, while the vehicle dynamics model reduces
average trajectory prediction error by 63.6% compared to traditional
four-degree-of-freedom models in scaled vehicle experiments. The integrated
system maintains millisecond-level computational efficiency suitable for
real-time applications while providing probabilistic confidence bounds
essential for safety-critical control. Comprehensive validation through FEA
simulation, dynamic modeling, and scaled vehicle experiments confirms the
framework's effectiveness for Precision Immobilization Technique scenarios and
general collision dynamics prediction.

</details>


### [246] [Quantifying the Impact of Missing Risk Markets for Decarbonized Power Systems with Long Duration Energy Storage](https://arxiv.org/abs/2510.13514)
*Andreas C. Makrides,Adam Suski,Elina Spyrou*

Main category: eess.SY

TL;DR: 本研究量化评估了缺失风险市场如何阻碍对长时储能（LDES）等可靠性增强技术的投资，导致社会福利下降、可靠性受损，并增加了LDES的融资成本。


<details>
  <summary>Details</summary>
Motivation: 向完全脱碳的电力系统转型需要整合确保可靠性的新技术，但缺失的风险市场使投资者面临收入不确定性，阻碍了对可靠性增强技术的投资。本研究旨在首次量化评估这种缺失风险市场对依赖长时储能（LDES）的电力系统投资决策的影响。

Method: 开发了一个两阶段随机均衡模型，该模型包含风险规避的市场参与者，并独立地确定电力和能量容量。将该方法应用于英国深度脱碳电力系统的案例研究。

Result: 不完整的风险市场降低了社会福利，损害了可靠性，并抑制了对LDES和其他收入波动技术的投资。收入波动导致LDES的风险溢价和融资成本大幅增加，成为其大规模部署的障碍。

Conclusion: 研究结果表明，旨在对冲收入风险的政策机制对于降低资本成本和加速对增强可靠性的零碳技术投资至关重要。

Abstract: The transition to a fully decarbonised electricity system depends on
integrating new technologies that ensure reliability alongside sustainability.
However, missing risk markets hinder investment in reliability-enhancing
technologies by exposing investors to revenue uncertainty. This study provides
the first quantitative assessment of how missing risk markets affect investment
decisions in power systems that depend on long-duration energy storage (LDES)
for reliability. We develop a two-stage stochastic equilibrium model with
risk-averse market participants, which independently sizes power and energy
capacity. We apply the method to a case study of a deeply decarbonised power
system in Great Britain. The results show that incomplete risk markets reduce
social welfare, harm reliability, and discourage investment in LDES and other
technologies with volatile revenue streams. Revenue volatility leads to
substantial risk premiums and higher financing costs for LDES, creating a
barrier to its large-scale deployment. These findings demonstrate the
importance of policy mechanisms that hedge revenue risk to lower the cost of
capital and accelerate investment in reliability-enhancing, zero-carbon
technologies

</details>


### [247] [A 0.62 μW/sensor 82 fps Time-to-Digital Impedance Measurement IC with Unified Excitation/Readout Front-end for Large-Scale Piezo-Resistive Sensor Array](https://arxiv.org/abs/2510.13682)
*Jiayang Li,Qingyu Zhang,Sohmyung Ha,Dai Jiang,Andreas Demosthenous,Yu Wu*

Main category: eess.SY

TL;DR: 本文提出了一种用于大规模压阻式传感器阵列的快速阻抗测量集成电路，具有统一的差分时间-数字解调架构和预饱和自适应偏置技术，实现了高速度、低功耗和高精度。


<details>
  <summary>Details</summary>
Motivation: 大规模压阻式传感器阵列需要快速、高效且准确的阻抗测量解决方案。

Method: 该研究采用统一的差分时间-数字解调架构，通过激励电路直接读取阻抗。此外，还引入了预饱和自适应偏置技术以提高功率效率。

Result: 该芯片能在12.2毫秒内（82帧/秒）扫描253个传感器（125 kHz），功耗为158微瓦（7.5纳焦/传感器）。在20欧姆至500千欧姆的负载范围内，实现了0.5%的误差和高达71.1分贝的信噪比。

Conclusion: 该集成电路为大规模压阻式传感器阵列提供了一种快速、低功耗、高精度的阻抗测量解决方案。

Abstract: This paper presents a fast impedance measurement IC for large-scale
piezo-resistive sensor array. It features a unified differential
time-to-digital demodulation architecture that readout impedance directly
through the excitation circuit. The proposed pre-saturation adaptive bias
technique further improves power efficiency. The chip scans 253 sensors in 12.2
ms (82 fps) at 125 kHz, consuming 158 {\mu}W (7.5 nJ/sensor). With loads from
20 {\Omega} to 500 k{\Omega}, it achieves 0.5% error and up to 71.1 dB SNR.

</details>


### [248] [Channel Estimation under Large Doppler Shifts in NOMA-Based Air-Ground Communications](https://arxiv.org/abs/2510.13563)
*Ayten Gürbüz,Giuseppe Caire*

Main category: eess.SY

TL;DR: 本文研究了多天线NOMA系统在航空交通管理数据交换中的应用，重点关注高多普勒频移和低信噪比下的信道估计和信道老化问题。通过比较不同的信道估计序列和检测器组合，发现最佳方案因飞行阶段（起降或巡航）而异。


<details>
  <summary>Details</summary>
Motivation: NOMA技术能提高频谱效率，但在高速（214米/秒）和长距离（250公里）的航空通信中，面临显著的多普勒频移和低信噪比挑战。因此，需要准确评估这些挑战，并研究在这些严苛条件下如何进行信道估计和处理信道老化。

Method: 研究采用基于飞行测量数据的几何随机空地信道模型。针对信道估计问题，比较了Zadoff-Chu序列与时分复用方法在不同载波频率偏移预补偿精度下的性能。为评估信道老化和信道估计器的性能，计算了零迫（ZF）检测器和带连续干扰消除（SIC）的最小均方误差（MMSE）检测器的中断概率。

Result: 结果显示，由于不同飞行阶段（起飞/降落阶段和巡航阶段）的信道传播特性不同，最佳的信道估计器-检测器组合也随之变化。

Conclusion: 在航空通信NOMA系统中，需要根据飞行阶段（起飞/降落或巡航）独特的信道传播特性，选择合适的信道估计器和检测器组合，以优化系统性能。

Abstract: This paper investigates a multiple antenna system with non-orthogonal
multiple access (NOMA) for the exchange of air traffic management data between
commercial aircraft pilots and ground-based air traffic controllers. While NOMA
techniques enhance spectral efficiency, their application to aircraft
communications is challenged by the high speed of the aircraft (up to 214 m/s)
and the long communication ranges (up to 250 km), resulting in significant
Doppler shifts and low signal-to-noise ratios, respectively. To accurately
assess these challenges, we employ a realistic geometry-based stochastic
air-ground channel model, derived from dedicated flight measurement campaigns.
In this paper, multiple aircraft simultaneously transmit data to the ground
station. We focus on the channel estimation problem at the ground station under
high carrier frequency offsets and the effects of channel aging due to
channel's time-varying nature. For the channel estimation problem, we compare
the Zadoff-Chu sequences with time-division approach under varying carrier
frequency offset pre-compensation accuracies at the aircraft transmitter. For
the channel aging problem and performance evaluation of channel estimators, we
compute the outage probability for both the zero-forcing detector and the
minimum mean squared error detector with successive interference cancellation.
The results show that the favorable channel estimator-detector combinations
differ between the takeoff & landing phase and the enroute cruise phase of the
flight, due to the distinct channel propagation characteristics of each phase.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [249] [Approximate Bilevel Graph Structure Learning for Histopathology Image Classification](https://arxiv.org/abs/2510.13188)
*Sudipta Paul,Amanda W. Lund,George Jour,Iman Osman,Bülent Yener*

Main category: eess.IV

TL;DR: ABiG-Net是一种新颖的框架，通过双层优化策略学习组织切片图像中的最优图结构和判别性节点嵌入，从而在组织病理学图像分类任务中实现卓越的性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有基于图的组织病理学图像分析方法通常依赖于具有预定义边的固定图，这限制了它们捕捉组织相互作用真实生物复杂性的能力。

Method: 本文提出了ABiG-Net框架，该框架通过神经网络进行近似双层优化来学习图结构。它采用分层建模：在局部尺度上，从每个图像块内的细胞方向构建图并提取特征；在全局尺度上，通过一阶近似双层优化策略学习图像级别的图，以捕获图像块之间稀疏且具有生物学意义的连接。全局图的优化与分类性能相关联，从而捕获图像中的长程上下文依赖关系。

Result: ABiG-Net在两个组织病理学数据集上表现出有效性：在Extended CRC数据集上，三分类结直肠癌分级准确率达到97.33 $\pm$ 1.15 %，二分类准确率达到98.33 $\pm$ 0.58 %；在黑色素瘤数据集上，肿瘤-淋巴细胞ROI分类准确率达到96.27 $\pm$ 0.74 %。该方法增强了解释性和下游性能。

Conclusion: ABiG-Net通过统一局部结构信息与全局上下文关系，显著提升了组织病理学图像分析的有效性和可解释性。

Abstract: The structural and spatial arrangements of cells within tissues represent
their functional states, making graph-based learning highly suitable for
histopathology image analysis. Existing methods often rely on fixed graphs with
predefined edges, limiting their ability to capture the true biological
complexity of tissue interactions. In this work, we propose ABiG-Net
(Approximate Bilevel Optimization for Graph Structure Learning via Neural
Networks), a novel framework designed to learn optimal interactions between
patches within whole slide images (WSI) or large regions of interest (ROI)
while simultaneously learning discriminative node embeddings for the downstream
image classification task. Our approach hierarchically models the tissue
architecture at local and global scales. At the local scale, we construct
patch-level graphs from cellular orientation within each patch and extract
features to quantify local structures. At the global scale, we learn an
image-level graph that captures sparse, biologically meaningful connections
between patches through a first-order approximate bilevel optimization
strategy. The learned global graph is optimized in response to classification
performance, capturing the long-range contextual dependencies across the image.
By unifying local structural information with global contextual relationships,
ABiG-Net enhances interpretability and downstream performance. Experiments on
two histopathology datasets demonstrate its effectiveness: on the Extended CRC
dataset, ABiG-Net achieves 97.33 $\pm$ 1.15 % accuracy for three-class
colorectal cancer grading and 98.33 $\pm$ 0.58 % for binary classification; on
the melanoma dataset, it attains 96.27 $\pm$ 0.74 % for tumor-lymphocyte ROI
classification.

</details>


### [250] [Semantic Communication Enabled Holographic Video Processing and Transmission](https://arxiv.org/abs/2510.13408)
*Jingkai Ying,Zhiyuan Qi,Yulong Feng,Zhijin Qin,Zhu Han,Rahim Tafazolli,Yonina C. Eldar*

Main category: eess.IV

TL;DR: 本文概述了全息视频通信，提出了一个语义赋能的全息视频通信系统架构，设计了关键技术（语义采样、联合语义-信道编码、语义感知传输），并通过用例展示了性能增益，并讨论了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 全息视频通信因其提供沉浸式体验的能力而日益普及，被认为是视觉通信领域的范式转变。研究旨在概述其要求，并探索如何通过语义通信来增强其能力。

Method: 在简要回顾语义通信后，提出了一个语义赋能的全息视频通信系统架构。基于该架构，设计了包括语义采样、联合语义-信道编码和语义感知传输在内的关键技术。

Result: 通过两个相关的用例，展示了所提出方法的性能增益。这证明了语义赋能的全息视频通信系统的有效性。

Conclusion: 讨论了潜在的研究课题，以期为实现语义赋能的全息视频通信铺平道路，指明了该领域未来的发展方向。

Abstract: Holographic video communication is considered a paradigm shift in visual
communications, becoming increasingly popular for its ability to offer
immersive experiences. This article provides an overview of holographic video
communication and outlines the requirements of a holographic video
communication system. Particularly, following a brief review of semantic com-
munication, an architecture for a semantic-enabled holographic video
communication system is presented. Key technologies, including semantic
sampling, joint semantic-channel coding, and semantic-aware transmission, are
designed based on the proposed architecture. Two related use cases are
presented to demonstrate the performance gain of the proposed methods. Finally,
potential research topics are discussed to pave the way for the realization of
semantic-enabled holographic video communications.

</details>


### [251] [DIGITWISE: Digital Twin-based Modeling of Adaptive Video Streaming Engagement](https://arxiv.org/abs/2510.13267)
*Emanuele Artioli,Farzad Tashtarian,Christian Timmerer*

Main category: eess.IV

TL;DR: 该论文提出了DIGITWISE，一个基于数字孪生的自适应视频流用户参与度建模方法，通过考虑用户个体敏感性来减少预测误差并优化内容交付。


<details>
  <summary>Details</summary>
Motivation: 随着视频流娱乐的普及，理解用户如何参与内容并对其变化作出反应对于利益相关者至关重要。用户参与度是客户忠诚度、内容个性化、广告相关性和A/B测试的核心。然而，传统的自适应比特率（ABR）算法假设所有用户对视频流伪影和网络问题的反应相似，忽略了用户个体敏感性。

Method: 该论文提出了DIGITWISE，一个基于数字孪生（物理实体的数字副本）的方法，通过过去的观看会话来建模用户参与度。数字孪生接收流媒体事件输入，并利用监督机器学习来预测给定会话的用户参与度。系统模型包括数据处理管道、充当数字孪生的机器学习模型以及一个统一的模型来预测参与度。DIGITWISE在数字孪生和统一模型中都采用了XGBoost模型。

Result: 该架构证明了个人用户敏感性的重要性，与非用户感知模型相比，用户参与度预测误差降低了高达5.8%。此外，DIGITWISE可以通过识别最大化参与度的特征来优化内容供应和交付，使平均参与度提高高达8.6%。

Conclusion: DIGITWISE通过引入基于数字孪生的用户个体敏感性，显著提高了自适应视频流中的参与度预测和内容优化能力，为内容提供商提供了更有效的工具。

Abstract: As the popularity of video streaming entertainment continues to grow,
understanding how users engage with the content and react to its changes
becomes a critical success factor for every stakeholder. User engagement, i.e.,
the percentage of video the user watches before quitting, is central to
customer loyalty, content personalization, ad relevance, and A/B testing. This
paper presents DIGITWISE, a digital twin-based approach for modeling adaptive
video streaming engagement. Traditional adaptive bitrate (ABR) algorithms
assume that all users react similarly to video streaming artifacts and network
issues, neglecting individual user sensitivities. DIGITWISE leverages the
concept of a digital twin, a digital replica of a physical entity, to model
user engagement based on past viewing sessions. The digital twin receives input
about streaming events and utilizes supervised machine learning to predict user
engagement for a given session. The system model consists of a data processing
pipeline, machine learning models acting as digital twins, and a unified model
to predict engagement. DIGITWISE employs the XGBoost model in both digital
twins and unified models. The proposed architecture demonstrates the importance
of personal user sensitivities, reducing user engagement prediction error by up
to 5.8% compared to non-user-aware models. Furthermore, DIGITWISE can optimize
content provisioning and delivery by identifying the features that maximize
engagement, providing an average engagement increase of up to 8.6%.

</details>


### [252] [How to Adapt Wireless DJSCC Symbols to Rate Constrained Wired Networks?](https://arxiv.org/abs/2510.13422)
*Jiangyuan Guo,Wei Chen,Yuxuan Sun,Bo Ai*

Main category: eess.IV

TL;DR: 本文提出了一种名为RCWA的新框架，旨在解决混合无线-有线网络中深度联合源-信道编码（DJSCC）符号在有线传输中的低效率和速率适应性问题，通过冗余感知编码和可控变速率编码来优化端到端性能。


<details>
  <summary>Details</summary>
Motivation: 现有DJSCC方法主要关注点对点无线通信，忽视了5G和6G等混合无线-有线网络中的端到端通信效率。DJSCC符号中针对无线信道的冗余对于长距离有线传输而言效率低下，且DJSCC符号需要适应有线网络变化的传输速率以避免拥塞。

Method: 本文提出了速率可控有线适配器（RCWA）框架。RCWA通过冗余感知编码来提高传输效率，它移除DJSCC符号中针对无线信道的冗余，仅将源相关信息编码为比特。此外，RCWA利用拉格朗日乘数法实现可控的连续变速率编码，将给定特征编码成期望速率，从而在满足约束的同时最小化端到端失真。

Result: 在多种数据集上的广泛实验表明，与现有基线相比，RCWA展现出卓越的RD性能和鲁棒性。具体而言，在CIFAR-10数据集上，与基于神经网络的方法和数字基线相比，RCWA分别获得了高达0.7dB和4dB的峰值信噪比增益。

Conclusion: RCWA在混合传输场景中具有优化有线资源利用的潜力，通过其冗余感知和可控变速率编码能力，显著提升了DJSCC符号在混合无线-有线网络中的传输效率和性能。

Abstract: Deep joint source-channel coding (DJSCC) has emerged as a robust alternative
to traditional separate coding for communications through wireless channels.
Existing DJSCC approaches focus primarily on point-to-point wireless
communication scenarios, while neglecting end-to-end communication efficiency
in hybrid wireless-wired networks such as 5G and 6G communication systems.
Considerable redundancy in DJSCC symbols against wireless channels becomes
inefficient for long-distance wired transmission. Furthermore, DJSCC symbols
must adapt to the varying transmission rate of the wired network to avoid
congestion. In this paper, we propose a novel framework designed for efficient
wired transmission of DJSCC symbols within hybrid wireless-wired networks,
namely Rate-Controllable Wired Adaptor (RCWA). RCWA achieves redundancy-aware
coding for DJSCC symbols to improve transmission efficiency, which removes
considerable redundancy present in DJSCC symbols for wireless channels and
encodes only source-relevant information into bits. Moreover, we leverage the
Lagrangian multiplier method to achieve controllable and continuous
variable-rate coding, which can encode given features into expected rates,
thereby minimizing end-to-end distortion while satisfying given constraints.
Extensive experiments on diverse datasets demonstrate the superior RD
performance and robustness of RCWA compared to existing baselines, validating
its potential for wired resource utilization in hybrid transmission scenarios.
Specifically, our method can obtain peak signal-to-noise ratio gain of up to
0.7dB and 4dB compared to neural network-based methods and digital baselines on
CIFAR-10 dataset, respectively.

</details>


### [253] [Dedelayed: Deleting remote inference delay via on-device correction](https://arxiv.org/abs/2510.13714)
*Dan Jacobellis,Mateen Ulhaq,Fabien Racapé,Hyomin Choi,Neeraja J. Yadwadkar*

Main category: eess.IV

TL;DR: Dedelayed是一种延迟校正方法，通过融合轻量级本地模型和重量级远程模型的特征，有效缓解了远程推理延迟，使轻量级设备能够为实时任务生成低延迟、高准确度的输出。


<details>
  <summary>Details</summary>
Motivation: 轻量级设备依赖强大的云模型进行远程推理，但通信网络延迟会导致预测过时，不适用于需要实时响应的任务。

Method: Dedelayed方法采用一个轻量级的本地模型处理当前帧，并融合一个重量级远程模型从过去帧计算出的特征。通过这种方式，它在不引入额外延迟的情况下，纠正了任意远程推理延迟。

Result: 在BDD100K驾驶数据集的视频上，当通信网络延迟超过33毫秒时，Dedelayed在语义分割准确性方面优于纯本地和纯远程基线。在100毫秒的往返延迟下，它比完全本地推理提高了6.4 mIoU的准确性，比远程推理提高了9.8 mIoU。在更长的延迟和更高运动的场景下，其优势更为明显，因为它能更有效地保持准确性。

Conclusion: Dedelayed通过缓解延迟的拆分推理，为需要与当前世界状态保持一致的实时任务提供了明显的优势，使其能够有效维持准确性，从而实现低延迟和高准确度的输出。

Abstract: Remote inference allows lightweight devices to leverage powerful cloud
models. However, communication network latency makes predictions stale and
unsuitable for real-time tasks. To address this, we introduce Dedelayed, a
delay-corrective method that mitigates arbitrary remote inference delays,
allowing the local device to produce low-latency outputs in real time. Our
method employs a lightweight local model that processes the current frame and
fuses in features that a heavyweight remote model computes from past frames. On
video from the BDD100K driving dataset, Dedelayed improves semantic
segmentation accuracy over the stronger of the local-only and remote-only
baselines across all realistic communication network delays beyond 33 ms.
Without incurring additional delay, it improves accuracy by 6.4 mIoU compared
to fully local inference and 9.8 mIoU compared to remote inference, for a
round-trip delay of 100 ms. The advantage grows under longer delays and
higher-motion scenes, as delay-mitigated split inference sustains accuracy more
effectively, providing clear advantages for real-time tasks that must remain
aligned with the current world state.

</details>


### [254] [Invited Paper: BitMedViT: Ternary-Quantized Vision Transformer for Medical AI Assistants on the Edge](https://arxiv.org/abs/2510.13760)
*Mikolaj Walczak,Uttej Kallakuri,Edward Humes,Xiaomin Lin,Tinoosh Mohsenin*

Main category: eess.IV

TL;DR: BiTMedViT是一种边缘优化的Vision Transformer，通过三元量化、多查询注意力、任务感知蒸馏和定制CUDA内核，显著降低了医疗图像分析ViT模型的计算和内存需求，使其能在资源受限的边缘设备上高效部署，同时保持高诊断准确率。


<details>
  <summary>Details</summary>
Motivation: Vision Transformers (ViTs)在医疗影像分析中表现出色，但其巨大的计算和内存需求限制了它们在临床环境中实时、资源受限的移动和可穿戴设备上的部署。

Method: 本文提出了BiTMedViT，一种新型的边缘ViT。它采用专为医学影像定制的三元量化线性层，并结合多查询注意力的训练过程，以在三元权重和低精度激活下保持稳定性。此外，BiTMedViT利用高容量教师模型进行任务感知蒸馏，以恢复极端量化造成的精度损失。最后，还提出了一种将三元化ViTs映射到自定义CUDA内核的流水线，以在Jetson Orin Nano上实现高效的内存带宽利用和延迟降低。

Result: BiTMedViT在MedMNIST的12个数据集上实现了86%的诊断准确率（SOTA为89%），同时将模型大小减少了43倍，内存流量减少了39倍。在Orin Nano上，它实现了16.8毫秒的推理时间，能量效率比SOTA模型高出41倍（达到183.62 GOPs/J）。

Conclusion: BiTMedViT为在边缘设备上部署高精度医疗影像ViTs提供了一条实用且科学合理的途径，缩小了算法进步与可部署临床工具之间的差距。

Abstract: Vision Transformers (ViTs) have demonstrated strong capabilities in
interpreting complex medical imaging data. However, their significant
computational and memory demands pose challenges for deployment in real-time,
resource-constrained mobile and wearable devices used in clinical environments.
We introduce, BiTMedViT, a new class of Edge ViTs serving as medical AI
assistants that perform structured analysis of medical images directly on the
edge. BiTMedViT utilizes ternary- quantized linear layers tailored for medical
imaging and com- bines a training procedure with multi-query attention,
preserving stability under ternary weights with low-precision activations.
Furthermore, BiTMedViT employs task-aware distillation from a high-capacity
teacher to recover accuracy lost due to extreme quantization. Lastly, we also
present a pipeline that maps the ternarized ViTs to a custom CUDA kernel for
efficient memory bandwidth utilization and latency reduction on the Jetson Orin
Nano. Finally, BiTMedViT achieves 86% diagnostic accuracy (89% SOTA) on
MedMNIST across 12 datasets, while reducing model size by 43x, memory traffic
by 39x, and enabling 16.8 ms inference at an energy efficiency up to 41x that
of SOTA models at 183.62 GOPs/J on the Orin Nano. Our results demonstrate a
practical and scientifically grounded route for extreme-precision medical
imaging ViTs deployable on the edge, narrowing the gap between algorithmic
advances and deployable clinical tools.

</details>
