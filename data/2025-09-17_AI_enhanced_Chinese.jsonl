{"id": "2509.12340", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.12340", "abs": "https://arxiv.org/abs/2509.12340", "authors": ["Nikolay Banar", "Ehsan Lotfi", "Jens Van Nooten", "Cristina Arhiliuc", "Marija Kliocaite", "Walter Daelemans"], "title": "MTEB-NL and E5-NL: Embedding Benchmark and Models for Dutch", "comment": null, "summary": "Recently, embedding resources, including models, benchmarks, and datasets,\nhave been widely released to support a variety of languages. However, the Dutch\nlanguage remains underrepresented, typically comprising only a small fraction\nof the published multilingual resources. To address this gap and encourage the\nfurther development of Dutch embeddings, we introduce new resources for their\nevaluation and generation. First, we introduce the Massive Text Embedding\nBenchmark for Dutch (MTEB-NL), which includes both existing Dutch datasets and\nnewly created ones, covering a wide range of tasks. Second, we provide a\ntraining dataset compiled from available Dutch retrieval datasets, complemented\nwith synthetic data generated by large language models to expand task coverage\nbeyond retrieval. Finally, we release a series of E5-NL models compact yet\nefficient embedding models that demonstrate strong performance across multiple\ntasks. We make our resources publicly available through the Hugging Face Hub\nand the MTEB package.", "AI": {"tldr": "该研究针对荷兰语嵌入资源不足的问题，推出了MTEB-NL基准测试、一个训练数据集以及一系列高效的E5-NL模型，以促进荷兰语嵌入的评估和发展。", "motivation": "荷兰语在多语言嵌入资源中代表性不足，仅占已发布资源的一小部分，导致缺乏针对荷兰语嵌入的评估和生成工具。", "method": "1. 引入了荷兰语大规模文本嵌入基准测试（MTEB-NL），包含现有和新创建的荷兰语数据集，覆盖广泛任务。2. 提供了一个训练数据集，该数据集由现有荷兰语检索数据集编译而成，并辅以大型语言模型生成的合成数据以扩大任务覆盖范围。3. 发布了一系列紧凑高效的E5-NL嵌入模型。", "result": "E5-NL模型在多项任务中表现出强大的性能。所有资源均通过Hugging Face Hub和MTEB包公开可用。", "conclusion": "该研究通过提供评估基准（MTEB-NL）、训练数据集和高性能的E5-NL模型，成功弥补了荷兰语嵌入资源的空白，鼓励了荷兰语嵌入的进一步发展。"}}
{"id": "2509.12371", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2509.12371", "abs": "https://arxiv.org/abs/2509.12371", "authors": ["Matteo Marcuzzo", "Alessandro Zangari", "Andrea Albarelli", "Jose Camacho-Collados", "Mohammad Taher Pilehvar"], "title": "MORABLES: A Benchmark for Assessing Abstract Moral Reasoning in LLMs with Fables", "comment": "Accepted to EMNLP 2025 Main Conference", "summary": "As LLMs excel on standard reading comprehension benchmarks, attention is\nshifting toward evaluating their capacity for complex abstract reasoning and\ninference. Literature-based benchmarks, with their rich narrative and moral\ndepth, provide a compelling framework for evaluating such deeper comprehension\nskills. Here, we present MORABLES, a human-verified benchmark built from fables\nand short stories drawn from historical literature. The main task is structured\nas multiple-choice questions targeting moral inference, with carefully crafted\ndistractors that challenge models to go beyond shallow, extractive question\nanswering. To further stress-test model robustness, we introduce adversarial\nvariants designed to surface LLM vulnerabilities and shortcuts due to issues\nsuch as data contamination. Our findings show that, while larger models\noutperform smaller ones, they remain susceptible to adversarial manipulation\nand often rely on superficial patterns rather than true moral reasoning. This\nbrittleness results in significant self-contradiction, with the best models\nrefuting their own answers in roughly 20% of cases depending on the framing of\nthe moral choice. Interestingly, reasoning-enhanced models fail to bridge this\ngap, suggesting that scale - not reasoning ability - is the primary driver of\nperformance.", "AI": {"tldr": "本研究引入了MORABLES基准测试，用于评估大型语言模型（LLMs）在文学作品中的复杂道德推理能力。结果显示，LLMs虽然规模越大表现越好，但仍易受对抗性操纵，依赖肤浅模式而非真正推理，且存在显著的自我矛盾。", "motivation": "尽管LLMs在标准阅读理解基准测试中表现出色，但人们对其复杂抽象推理和推断能力的评估仍有待深入。基于文学的基准测试因其丰富的叙事和道德深度，为评估这种更深层次的理解技能提供了有力的框架。", "method": "本研究构建了MORABLES，一个由历史文学中的寓言和短篇故事组成的人工验证基准测试。主要任务是多项选择题，旨在进行道德推断，并设计了精心制作的干扰项，以挑战模型超越肤浅的提取式问答。为进一步压力测试模型的鲁棒性，研究还引入了对抗性变体，旨在揭示LLM因数据污染等问题导致的漏洞和捷径。", "result": "研究发现，虽然大型模型优于小型模型，但它们仍然容易受到对抗性操纵，并且经常依赖肤浅模式而非真正的道德推理。这种脆弱性导致了显著的自我矛盾，即使是最好的模型，在约20%的情况下也会根据道德选择的表述方式推翻自己的答案。有趣的是，推理增强模型未能弥补这一差距，表明规模而非推理能力是性能的主要驱动因素。", "conclusion": "LLMs，即使是大型模型，在道德推理方面仍缺乏鲁棒性，容易受到对抗性攻击，并倾向于依赖表面模式。当前的“推理增强”方法未能有效解决此问题，暗示性能的提升主要源于模型规模的扩大而非推理能力的实质性进步。"}}
{"id": "2509.12382", "categories": ["cs.CL", "H.3.3; I.2.7; I.2.6"], "pdf": "https://arxiv.org/pdf/2509.12382", "abs": "https://arxiv.org/abs/2509.12382", "authors": ["Anu Pradhan", "Alexandra Ortan", "Apurv Verma", "Madhavan Seshadri"], "title": "LLM-as-a-Judge: Rapid Evaluation of Legal Document Recommendation for Retrieval-Augmented Generation", "comment": "Accepted in EARL 25: The 2nd Workshop on Evaluating and Applying\n  Recommender Systems with Large Language Models at RecSys 2025", "summary": "The evaluation bottleneck in recommendation systems has become particularly\nacute with the rise of Generative AI, where traditional metrics fall short of\ncapturing nuanced quality dimensions that matter in specialized domains like\nlegal research. Can we trust Large Language Models to serve as reliable judges\nof their own kind? This paper investigates LLM-as-a-Judge as a principled\napproach to evaluating Retrieval-Augmented Generation systems in legal\ncontexts, where the stakes of recommendation quality are exceptionally high.\n  We tackle two fundamental questions that determine practical viability: which\ninter-rater reliability metrics best capture the alignment between LLM and\nhuman assessments, and how do we conduct statistically sound comparisons\nbetween competing systems? Through systematic experimentation, we discover that\ntraditional agreement metrics like Krippendorff's alpha can be misleading in\nthe skewed distributions typical of AI system evaluations. Instead, Gwet's AC2\nand rank correlation coefficients emerge as more robust indicators for judge\nselection, while the Wilcoxon Signed-Rank Test with Benjamini-Hochberg\ncorrections provides the statistical rigor needed for reliable system\ncomparisons.\n  Our findings suggest a path toward scalable, cost-effective evaluation that\nmaintains the precision demanded by legal applications, transforming what was\nonce a human-intensive bottleneck into an automated, yet statistically\nprincipled, evaluation framework.", "AI": {"tldr": "本研究探讨了在法律领域中，使用大型语言模型（LLM）作为评估者（LLM-as-a-Judge）来评估检索增强生成（RAG）系统的可行性，并提出了更稳健的评估指标和统计比较方法，以解决传统评估瓶颈。", "motivation": "随着生成式AI的兴起，推荐系统（特别是RAG系统）的评估瓶颈日益突出，传统指标难以捕捉法律研究等专业领域中细致的质量维度。研究动机在于探究LLM是否能作为可靠的评估者，并解决在法律背景下，评估高质量推荐系统所面临的严峻挑战。", "method": "通过系统实验，研究了两个核心问题：哪些评估者间一致性指标能最好地捕捉LLM与人类评估之间的一致性，以及如何对竞争系统进行统计学上可靠的比较。方法包括测试不同的评估者间信度指标和统计检验方法。", "result": "研究发现，传统的协议指标（如Krippendorff's alpha）在AI系统评估中常见的偏斜分布下可能产生误导。Gwet's AC2和秩相关系数被证明是更稳健的评判者选择指标。此外，结合Benjamini-Hochberg校正的Wilcoxon符号秩检验为可靠的系统比较提供了必要的统计严谨性。", "conclusion": "研究结果为实现可扩展、成本效益高、自动化且统计学上严谨的评估框架指明了方向，能够满足法律应用对精度的严苛要求，将原本人力密集型的评估瓶颈转化为高效的自动化流程。"}}
{"id": "2509.12385", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.12385", "abs": "https://arxiv.org/abs/2509.12385", "authors": ["Mitchell Plyler", "Yilun Zhang", "Alexander Tuzhilin", "Saoud Khalifah", "Sen Tian"], "title": "SENTRA: Selected-Next-Token Transformer for LLM Text Detection", "comment": "EMNLP Findings 2025", "summary": "LLMs are becoming increasingly capable and widespread. Consequently, the\npotential and reality of their misuse is also growing. In this work, we address\nthe problem of detecting LLM-generated text that is not explicitly declared as\nsuch. We present a novel, general-purpose, and supervised LLM text detector,\nSElected-Next-Token tRAnsformer (SENTRA). SENTRA is a Transformer-based encoder\nleveraging selected-next-token-probability sequences and utilizing contrastive\npre-training on large amounts of unlabeled data. Our experiments on three\npopular public datasets across 24 domains of text demonstrate SENTRA is a\ngeneral-purpose classifier that significantly outperforms popular baselines in\nthe out-of-domain setting.", "AI": {"tldr": "本文提出了一种名为SENTRA的新型Transformer编码器，用于检测未明确声明的LLM生成文本，通过对比预训练和选定下一词元概率序列，在域外设置中显著优于现有基线。", "motivation": "随着大型语言模型（LLMs）的能力日益增强和普及，其被滥用的可能性和现实性也在增长。研究旨在解决检测未明确声明为LLM生成文本的问题。", "method": "本文提出了一种新颖的、通用的、监督式LLM文本检测器——SENTRA（SElected-Next-Token tRAnsformer）。SENTRA是一个基于Transformer的编码器，它利用选定下一词元概率序列，并通过对比预训练在大规模未标记数据上进行训练。", "result": "在涵盖24个文本领域的三个流行公共数据集上的实验表明，SENTRA作为一种通用分类器，在域外（out-of-domain）设置中显著优于流行的基线方法。", "conclusion": "SENTRA成功地解决了检测未明确声明的LLM生成文本的问题，并展示了其作为通用分类器在不同领域中的强大泛化能力和卓越性能。"}}
{"id": "2509.12367", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12367", "abs": "https://arxiv.org/abs/2509.12367", "authors": ["Daniel Lindmark", "Jonas Andersson", "Kenneth Bodin", "Tora Bodin", "Hugo Börjesson", "Fredrik Nordfeldth", "Martin Servin"], "title": "An integrated process for design and control of lunar robotics using AI and simulation", "comment": "14 pages, 6 figures", "summary": "We envision an integrated process for developing lunar construction\nequipment, where physical design and control are explored in parallel. In this\npaper, we describe a technical framework that supports this process. It relies\non OpenPLX, a readable/writable declarative language that links CAD-models and\nautonomous systems to high-fidelity, real-time 3D simulations of contacting\nmultibody dynamics, machine regolith interaction forces, and non-ideal sensors.\nTo demonstrate its capabilities, we present two case studies, including an\nautonomous lunar rover that combines a vision-language model for navigation\nwith a reinforcement learning-based control policy for locomotion.", "AI": {"tldr": "本文提出一个集成框架，用于并行开发月球施工设备的物理设计和控制，该框架基于OpenPLX语言，将CAD模型、自主系统与高保真实时3D仿真连接起来，并通过案例研究展示其能力。", "motivation": "研究动机是为了改进月球施工设备的开发过程，实现物理设计和控制的并行探索，以应对这类复杂系统在月球严苛环境下的开发挑战。", "method": "该研究采用了一个技术框架，核心是OpenPLX，这是一种可读写声明性语言。OpenPLX将CAD模型和自主系统与高保真、实时的3D仿真连接起来，仿真内容包括接触式多体动力学、机械-月壤相互作用力以及非理想传感器。框架通过视觉-语言模型和强化学习控制策略实现了导航和运动。", "result": "研究通过两个案例研究（包括一个结合视觉-语言模型进行导航和基于强化学习控制策略进行运动的自主月球车）展示了该框架的能力和有效性。", "conclusion": "该集成框架成功支持了月球施工设备的开发过程，实现了物理设计和控制的并行探索，并通过先进的仿真和AI技术验证了其可行性，为未来月球任务中的设备开发提供了有效工具。"}}
{"id": "2509.12242", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12242", "abs": "https://arxiv.org/abs/2509.12242", "authors": ["Mustafa Khanbhai", "Giulia Di Nardo", "Jun Ma", "Vivienne Freitas", "Caterina Masino", "Ali Dolatabadi", "Zhaoxun \"Lorenz\" Liu", "Wey Leong", "Wagner H. Souza", "Amin Madani"], "title": "Artificial Intelligence in Breast Cancer Care: Transforming Preoperative Planning and Patient Education with 3D Reconstruction", "comment": null, "summary": "Effective preoperative planning requires accurate algorithms for segmenting\nanatomical structures across diverse datasets, but traditional models struggle\nwith generalization. This study presents a novel machine learning methodology\nto improve algorithm generalization for 3D anatomical reconstruction beyond\nbreast cancer applications. We processed 120 retrospective breast MRIs (January\n2018-June 2023) through three phases: anonymization and manual segmentation of\nT1-weighted and dynamic contrast-enhanced sequences; co-registration and\nsegmentation of whole breast, fibroglandular tissue, and tumors; and 3D\nvisualization using ITK-SNAP. A human-in-the-loop approach refined\nsegmentations using U-Mamba, designed to generalize across imaging scenarios.\nDice similarity coefficient assessed overlap between automated segmentation and\nground truth. Clinical relevance was evaluated through clinician and patient\ninterviews. U-Mamba showed strong performance with DSC values of 0.97\n($\\pm$0.013) for whole organs, 0.96 ($\\pm$0.024) for fibroglandular tissue, and\n0.82 ($\\pm$0.12) for tumors on T1-weighted images. The model generated accurate\n3D reconstructions enabling visualization of complex anatomical features.\nClinician interviews indicated improved planning, intraoperative navigation,\nand decision support. Integration of 3D visualization enhanced patient\neducation, communication, and understanding. This human-in-the-loop machine\nlearning approach successfully generalizes algorithms for 3D reconstruction and\nanatomical segmentation across patient datasets, offering enhanced\nvisualization for clinicians, improved preoperative planning, and more\neffective patient education, facilitating shared decision-making and empowering\ninformed patient choices across medical applications.", "AI": {"tldr": "本研究提出了一种结合U-Mamba和人机协作的新型机器学习方法，显著提高了3D解剖结构分割算法的泛化能力，特别是在乳腺MRI数据上，并能生成精确的3D重建，从而改善术前规划、临床决策和患者教育。", "motivation": "有效的术前规划需要准确的解剖结构分割算法，但传统模型在处理多样化数据集时泛化能力不足，限制了其应用范围。", "method": "研究处理了120份回顾性乳腺MRI数据（2018-2023年），分为三个阶段：匿名化和手动分割T1加权及动态增强序列；全乳、纤维腺体组织和肿瘤的共同配准与分割；使用ITK-SNAP进行3D可视化。采用人机协作方法，利用U-Mamba模型优化分割，旨在提高跨成像场景的泛化能力。通过Dice相似系数评估自动化分割与金标准的一致性，并通过临床医生和患者访谈评估临床相关性。", "result": "U-Mamba模型表现出色，在T1加权图像上，全器官的Dice相似系数（DSC）为0.97 (±0.013)，纤维腺体组织为0.96 (±0.024)，肿瘤为0.82 (±0.12)。模型生成了精确的3D重建，能够可视化复杂的解剖特征。临床医生访谈表明，该方法改善了规划、术中导航和决策支持。3D可视化的整合增强了患者教育、沟通和理解。", "conclusion": "这种人机协作的机器学习方法成功地泛化了3D重建和解剖分割算法，适用于不同的患者数据集，为临床医生提供了增强的可视化效果，改善了术前规划，并提供了更有效的患者教育，促进了共同决策，使患者在各种医疗应用中做出明智的选择。"}}
{"id": "2509.12214", "categories": ["eess.SY", "cs.CE", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.12214", "abs": "https://arxiv.org/abs/2509.12214", "authors": ["An Nguyen", "Hung Pham", "Cuong Do"], "title": "A Cost-Optimization Model for EV Charging Stations Utilizing Solar Energy and Variable Pricing", "comment": null, "summary": "This paper presents a cost optimization framework for electric vehicle (EV)\ncharging stations that leverages on-site photovoltaic (PV) generation and\nexplicitly accounts for electricity price uncertainty through a Bertsimas--Sim\nrobust formulation. The model is formulated as a linear program that satisfies\nvehicle energy demands, respects charging and grid capacity constraints, and\nminimizes procurement cost. Evaluations on real charging data from the Caltech\nACN dataset show average savings of about 12\\% compared to a\nfirst-come--first-served baseline, with peak monthly reductions up to 19.2\\%. A\nlightweight sensitivity analysis indicates that a modest $\\sim$5\\% increase in\nnominal cost can reduce worst-case exposure by 14\\%. Computational tests\nconfirm real-time feasibility, with instances of up to 50 concurrent EVs solved\nin under 5 seconds on a standard laptop. The proposed method provides a\npractical, grid-friendly, and scalable solution for future EV charging\noperations.", "AI": {"tldr": "本文提出一个电动汽车充电站成本优化框架，利用光伏发电，并通过鲁棒优化处理电价不确定性，实现了显著的成本节约和实时可行性。", "motivation": "电动汽车充电站面临电力采购成本高昂且电价不确定的挑战。研究旨在开发一个能够有效利用现场光伏发电、降低采购成本并应对电价波动的优化方案。", "method": "该研究将模型构建为一个线性规划问题，以满足车辆能源需求、遵守充电和电网容量限制并最小化采购成本。它采用Bertsimas-Sim鲁棒公式来明确考虑电价不确定性。模型在Caltech ACN数据集的真实充电数据上进行评估。", "result": "与先到先得基线相比，该方法平均节省约12%的成本，月度峰值降幅高达19.2%。轻量级敏感性分析显示，名义成本适度增加约5%可将最坏情况下的风险降低14%。计算测试证实了实时可行性，在标准笔记本电脑上，多达50辆并发电动汽车的实例可在5秒内求解。", "conclusion": "所提出的方法为未来的电动汽车充电运营提供了一个实用、对电网友好且可扩展的解决方案。"}}
{"id": "2509.12251", "categories": ["cs.AI", "cs.CV", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.12251", "abs": "https://arxiv.org/abs/2509.12251", "authors": ["Duong Q. Nguyen", "Quy P. Nguyen", "Nguyen Van Nhon", "Quang-Thinh Bui", "H. Nguyen-Xuan"], "title": "V-Math: An Agentic Approach to the Vietnamese National High School Graduation Mathematics Exams", "comment": null, "summary": "This paper develops an autonomous agentic framework called V-Math that aims\nto assist Vietnamese high school students in preparing for the National High\nSchool Graduation Mathematics Exams (NHSGMEs). The salient framework integrates\nthree specialized AI agents: a specification-matrix-conditioned question\ngenerator, a solver/explainer for detailed step-by-step reasoning, and a\npersonalized tutor that adapts to student performance. Beyond enabling\nself-paced student practice, V-Math supports teachers by generating innovative,\ncompliant exam questions and building diverse, high-quality question banks.\nThis reduces manual workload and enriches instructional resources. We describe\nthe system architecture, focusing on practice modes for learners and\nteacher-oriented features for question generation. Preliminary evaluations\ndemonstrate that V-Math produces matrix-aligned exams with high solution\naccuracy, delivers coherent explanations, and enhances the variety of practice\nmaterials. These results highlight its potential to support scalable, equitable\nmathematics preparation aligned with national standards while also empowering\nteachers through AI-assisted exam creation.", "AI": {"tldr": "V-Math是一个自主智能代理框架，旨在帮助越南高中生准备数学毕业考试，并通过生成问题和提供个性化辅导来支持学生和教师。", "motivation": "帮助越南高中生准备全国高中毕业数学考试；减轻教师手动出题的工作量；丰富教学资源并提供多样化、高质量的题库。", "method": "该框架集成了三个专门的AI代理：一个基于规格矩阵的问题生成器、一个提供详细分步推理的解题/解释器，以及一个根据学生表现调整的个性化辅导器。它支持学生自主练习模式和教师的出题功能。", "result": "初步评估表明，V-Math能生成与矩阵对齐的考题，具有高解题准确性，提供连贯的解释，并增加了练习材料的多样性。", "conclusion": "V-Math有潜力支持符合国家标准的、可扩展和公平的数学备考，并通过AI辅助考试创建来赋能教师。"}}
{"id": "2509.12253", "categories": ["eess.IV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.12253", "abs": "https://arxiv.org/abs/2509.12253", "authors": ["Riyaadh Gani"], "title": "Physics-Informed Neural Networks vs. Physics Models for Non-Invasive Glucose Monitoring: A Comparative Study Under Realistic Synthetic Conditions", "comment": null, "summary": "Non-invasive glucose monitors often fail outside the lab because existing\ndatasets ignore hardware noise, environmental drift, and person-to-person\nphysiology. We introduce the first ultra-realistic near-infrared (NIR)\nsimulator that injects 12-bit ADC quantisation, +/-0.1% LED ageing, photodiode\ndark noise, 15-45 C temperature, 30-90% relative humidity, contact-pressure\nvariation, Fitzpatrick I-VI melanin, and diurnal glucose excursions (dawn\nphenomenon). Using this platform (rho glucose-NIR = 0.21), we benchmark six\nmethods: Enhanced Beer-Lambert (physics-engineered ridge regression), three\nphysics-informed neural networks (PINNs), a selective radiative-transfer PINN,\nand a shallow DNN. Beer-Lambert achieves 13.6 mg/dL RMSE, 95.8% Clarke-A and\n93.8% +/-15% accuracy with only 56 parameters and 0.01 ms inference,\noutperforming the best PINN (14.6 mg/dL) and the SDNN baseline (35.1 mg/dL).\nResults overturn the assumption that deeper PINNs dominate and supply an open,\nend-to-end reference stack for rapid prototyping of embedded optical glucose\nsensors.", "AI": {"tldr": "本文介绍了一个超现实的近红外（NIR）模拟器，用于解决非侵入式血糖监测器在实验室外表现不佳的问题。研究发现，增强型Beer-Lambert模型在模拟数据上优于更复杂的物理信息神经网络（PINNs），并提供了一个开放的参考堆栈。", "motivation": "现有的非侵入式血糖监测数据集忽略了硬件噪声、环境漂移和个体生理差异，导致设备在实际应用中表现不佳。这促使研究人员开发一个更真实的模拟器来评估监测方法。", "method": "研究引入了首个超现实近红外（NIR）模拟器，该模拟器注入了12位ADC量化、LED老化、光电二极管暗噪声、温度、相对湿度、接触压力变化、Fitzpatrick I-VI黑色素以及昼夜血糖波动等多种真实世界因素。在此平台上，研究人员对六种方法进行了基准测试：增强型Beer-Lambert（物理工程岭回归）、三种物理信息神经网络（PINNs）、一种选择性辐射传输PINN和一个浅层DNN。", "result": "该模拟平台（rho glucose-NIR = 0.21）的基准测试结果显示，增强型Beer-Lambert模型实现了13.6 mg/dL的RMSE，95.8%的Clarke-A精度和93.8%的+/-15%准确率，仅使用56个参数和0.01毫秒的推理时间。它优于表现最佳的PINN（14.6 mg/dL）和SDNN基线（35.1 mg/dL）。", "conclusion": "研究结果推翻了更深层PINNs在非侵入式血糖监测中占据主导地位的假设，表明一个更简单、物理驱动的模型（增强型Beer-Lambert）可以表现更优。该研究还提供了一个开放的、端到端的参考堆栈，用于嵌入式光学血糖传感器的快速原型开发。"}}
{"id": "2509.12405", "categories": ["cs.CL", "68T50 (Primary) 68T45 (Secondary)", "I.2.7; I.2.10"], "pdf": "https://arxiv.org/pdf/2509.12405", "abs": "https://arxiv.org/abs/2509.12405", "authors": ["Wen-wai Yim", "Asma Ben Abacha", "Zixuan Yu", "Robert Doerning", "Fei Xia", "Meliha Yetisgen"], "title": "MORQA: Benchmarking Evaluation Metrics for Medical Open-Ended Question Answering", "comment": "9 pages, 8 tables", "summary": "Evaluating natural language generation (NLG) systems in the medical domain\npresents unique challenges due to the critical demands for accuracy, relevance,\nand domain-specific expertise. Traditional automatic evaluation metrics, such\nas BLEU, ROUGE, and BERTScore, often fall short in distinguishing between\nhigh-quality outputs, especially given the open-ended nature of medical\nquestion answering (QA) tasks where multiple valid responses may exist. In this\nwork, we introduce MORQA (Medical Open-Response QA), a new multilingual\nbenchmark designed to assess the effectiveness of NLG evaluation metrics across\nthree medical visual and text-based QA datasets in English and Chinese. Unlike\nprior resources, our datasets feature 2-4+ gold-standard answers authored by\nmedical professionals, along with expert human ratings for three English and\nChinese subsets. We benchmark both traditional metrics and large language model\n(LLM)-based evaluators, such as GPT-4 and Gemini, finding that LLM-based\napproaches significantly outperform traditional metrics in correlating with\nexpert judgments. We further analyze factors driving this improvement,\nincluding LLMs' sensitivity to semantic nuances and robustness to variability\namong reference answers. Our results provide the first comprehensive,\nmultilingual qualitative study of NLG evaluation in the medical domain,\nhighlighting the need for human-aligned evaluation methods. All datasets and\nannotations will be publicly released to support future research.", "AI": {"tldr": "本文提出了MORQA，一个多语言医学开放式问答基准，旨在评估NLG评估指标。研究发现，LLM（大型语言模型）评估器在医学领域与专家判断的相关性显著优于传统指标。", "motivation": "由于对准确性、相关性和领域专业知识的严格要求，医学领域的自然语言生成（NLG）系统评估面临独特挑战。传统的自动评估指标在区分高质量输出方面表现不足，尤其是在医学开放式问答任务中，存在多种有效回答的情况。", "method": "研究引入了MORQA（Medical Open-Response QA），一个多语言（英语和中文）基准，用于评估NLG评估指标在三个医学视觉和文本问答数据集上的有效性。该数据集包含2-4个由医学专业人员撰写的黄金标准答案，以及专家对部分英语和中文子集的评分。研究基准测试了传统的评估指标（如BLEU、ROUGE、BERTScore）和基于大型语言模型（LLM）的评估器（如GPT-4和Gemini）。", "result": "研究发现，基于LLM的方法在与专家判断的相关性方面显著优于传统指标。进一步分析表明，LLM在语义细微差别方面的敏感性和对参考答案变异性的鲁棒性是其改进的关键因素。", "conclusion": "这项研究提供了医学领域NLG评估的首次全面、多语言定性研究，强调了对与人类判断对齐的评估方法的需求。所有数据集和注释将公开发布以支持未来的研究。"}}
{"id": "2509.12379", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.12379", "abs": "https://arxiv.org/abs/2509.12379", "authors": ["Divyam Goel", "Yufei Wang", "Tiancheng Wu", "Guixiu Qiao", "Pavel Piliptchak", "David Held", "Zackory Erickson"], "title": "Geometric Red-Teaming for Robotic Manipulation", "comment": "Accepted at the 9th Annual Conference on Robot Learning (CoRL 2025,\n  Oral)", "summary": "Standard evaluation protocols in robotic manipulation typically assess policy\nperformance over curated, in-distribution test sets, offering limited insight\ninto how systems fail under plausible variation. We introduce Geometric\nRed-Teaming (GRT), a red-teaming framework that probes robustness through\nobject-centric geometric perturbations, automatically generating CrashShapes --\nstructurally valid, user-constrained mesh deformations that trigger\ncatastrophic failures in pre-trained manipulation policies. The method\nintegrates a Jacobian field-based deformation model with a gradient-free,\nsimulator-in-the-loop optimization strategy. Across insertion, articulation,\nand grasping tasks, GRT consistently discovers deformations that collapse\npolicy performance, revealing brittle failure modes missed by static\nbenchmarks. By combining task-level policy rollouts with constraint-aware shape\nexploration, we aim to build a general purpose framework for structured,\nobject-centric robustness evaluation in robotic manipulation. We additionally\nshow that fine-tuning on individual CrashShapes, a process we refer to as\nblue-teaming, improves task success by up to 60 percentage points on those\nshapes, while preserving performance on the original object, demonstrating the\nutility of red-teamed geometries for targeted policy refinement. Finally, we\nvalidate both red-teaming and blue-teaming results with a real robotic arm,\nobserving that simulated CrashShapes reduce task success from 90% to as low as\n22.5%, and that blue-teaming recovers performance to up to 90% on the\ncorresponding real-world geometry -- closely matching simulation outcomes.\nVideos and code can be found on our project website:\nhttps://georedteam.github.io/ .", "AI": {"tldr": "本文提出了几何红队（GRT）框架，通过自动生成几何扰动（CrashShapes）来发现机器人操作策略的脆弱故障模式，并展示了通过“蓝队”微调可以显著提高策略在这些失败模式下的鲁棒性，并在真实机器人上进行了验证。", "motivation": "标准的机器人操作评估协议通常在精心策划的、同分布的测试集上进行，这限制了对系统在合理变化下如何失败的深入了解，无法揭示策略的真实鲁棒性。", "method": "引入了几何红队（GRT）框架，通过以物体为中心的几何扰动来探测鲁棒性。该方法结合了基于雅可比场（Jacobian field）的形变模型和无梯度（gradient-free）、模拟器在环（simulator-in-the-loop）的优化策略，自动生成“CrashShapes”——结构有效且用户受限的网格变形，这些变形能导致预训练操作策略的灾难性失败。此外，还提出了“蓝队”（blue-teaming）方法，即对单个CrashShapes进行微调以改进策略。", "result": "GRT在插入、关节操作和抓取任务中持续发现能使策略性能崩溃的几何变形，揭示了静态基准测试未能发现的脆弱故障模式。通过对单个CrashShapes进行“蓝队”微调，任务成功率提高了高达60个百分点，同时保持了原始物体上的性能。真实机器人验证显示，模拟的CrashShapes将任务成功率从90%降低到22.5%，而“蓝队”微调能将相应真实世界几何体的性能恢复到90%，与模拟结果高度吻合。", "conclusion": "GRT提供了一个通用的、以物体为中心的结构化鲁棒性评估框架，用于机器人操作。通过红队生成的几何体（CrashShapes）进行有针对性的策略优化（蓝队）对于提高机器人操作策略的鲁棒性和性能具有显著实用价值，并在模拟和真实世界中得到了有效验证。"}}
{"id": "2509.12244", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12244", "abs": "https://arxiv.org/abs/2509.12244", "authors": ["Lu Cai", "Fei Xu", "Min Xian", "Yalei Tang", "Shoukun Sun", "John Stempien"], "title": "RU-Net for Automatic Characterization of TRISO Fuel Cross Sections", "comment": null, "summary": "During irradiation, phenomena such as kernel swelling and buffer\ndensification may impact the performance of tristructural isotropic (TRISO)\nparticle fuel. Post-irradiation microscopy is often used to identify these\nirradiation-induced morphologic changes. However, each fuel compact generally\ncontains thousands of TRISO particles. Manually performing the work to get\nstatistical information on these phenomena is cumbersome and subjective. To\nreduce the subjectivity inherent in that process and to accelerate data\nanalysis, we used convolutional neural networks (CNNs) to automatically segment\ncross-sectional images of microscopic TRISO layers. CNNs are a class of\nmachine-learning algorithms specifically designed for processing structured\ngrid data. They have gained popularity in recent years due to their remarkable\nperformance in various computer vision tasks, including image classification,\nobject detection, and image segmentation. In this research, we generated a\nlarge irradiated TRISO layer dataset with more than 2,000 microscopic images of\ncross-sectional TRISO particles and the corresponding annotated images. Based\non these annotated images, we used different CNNs to automatically segment\ndifferent TRISO layers. These CNNs include RU-Net (developed in this study), as\nwell as three existing architectures: U-Net, Residual Network (ResNet), and\nAttention U-Net. The preliminary results show that the model based on RU-Net\nperforms best in terms of Intersection over Union (IoU). Using CNN models, we\ncan expedite the analysis of TRISO particle cross sections, significantly\nreducing the manual labor involved and improving the objectivity of the\nsegmentation results.", "AI": {"tldr": "本研究利用卷积神经网络（CNNs），特别是新开发的RU-Net模型，对辐照后的TRISO燃料颗粒截面图像进行自动分层分割，以提高数据分析的效率和客观性。", "motivation": "辐照会导致TRISO燃料颗粒发生核膨胀和缓冲层致密化等现象，通过显微镜进行人工分析来获取这些形态变化的统计信息，耗时、繁琐且主观性强，难以处理每个燃料压块中数千个TRISO颗粒。", "method": "研究生成了一个包含2000多张辐照TRISO颗粒截面显微图像及其对应标注图像的大型数据集。基于这些标注图像，使用了多种CNN模型（包括本研究开发的RU-Net以及U-Net、残差网络ResNet和注意力U-Net等现有架构）来自动分割TRISO的不同层。模型性能通过交并比（IoU）进行评估。", "result": "初步结果显示，基于RU-Net的模型在交并比（IoU）方面表现最佳，优于其他测试的CNN模型。", "conclusion": "利用CNN模型可以大大加快TRISO颗粒截面分析的速度，显著减少人工劳动，并提高分割结果的客观性。"}}
{"id": "2509.12225", "categories": ["eess.SY", "cs.GT", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.12225", "abs": "https://arxiv.org/abs/2509.12225", "authors": ["Siying Huang", "Yifen Mu", "Ge Chen"], "title": "Private Markovian Equilibrium in Stackelberg Markov Games for Smart Grid Demand Response", "comment": null, "summary": "The increasing integration of renewable energy introduces a great challenge\nto the supply and demand balance of the power grid. To address this challenge,\nthis paper formulates a Stackelberg Markov game (SMG) between an aggregator and\nmultiple users, where the aggregator sets electricity prices and users make\ndemand and storage decisions. Considering that users' storage levels are\nprivate information, we introduce private states and propose the new concepts\nof private Markovian strategies (PMS) and private Markovian equilibrium (PME).\nWe establish the existence of a pure PME in the lower-level Markov game and\nprove that it can be computed in polynomial time. Notably, computing\nequilibrium in general Markov games is hard, and polynomial-time algorithms are\nrarely available. Based on these theoretical results, we develop a scalable\nsolution framework combining centralized and decentralized algorithms for the\nlower-level PME computation with upper-level pricing optimization. Numerical\nsimulations with up to 50 users based on real data validate the effectiveness\nand scalability of the proposed methods, whereas prior studies typically\nconsider no more than 5 users.", "AI": {"tldr": "本文提出了一种斯塔克尔伯格马尔可夫博弈（SMG）模型，用于解决可再生能源集成带来的电力供需平衡挑战。该模型考虑了用户存储水平的隐私性，引入了私有马尔可夫策略和均衡（PME），并证明了其在多项式时间内的可计算性。通过结合中心化和去中心化算法，该方法在多达50个用户的场景下展现出有效性和可扩展性。", "motivation": "可再生能源的日益整合给电网的供需平衡带来了巨大挑战，需要有效的机制来管理电力价格、用户需求和储能决策。", "method": "研究方法包括：1) 建立了一个聚合商与多个用户之间的斯塔克尔伯格马尔可夫博弈（SMG），其中聚合商设定电价，用户做出需求和储能决策。2) 针对用户储能水平的私有性，引入了私有状态、私有马尔可夫策略（PMS）和私有马尔可夫均衡（PME）的新概念。3) 提出了一种结合中心化和去中心化算法的可扩展解决方案框架，用于下层PME计算和上层定价优化。", "result": "主要成果有：1) 证明了下层马尔可夫博弈中纯PME的存在性。2) 证明了PME可以在多项式时间内计算，这在一般马尔可夫博弈中是罕见的。3) 基于真实数据的数值模拟（多达50个用户）验证了所提出方法的有效性和可扩展性，显著优于以往通常只考虑不超过5个用户的研究。", "conclusion": "本文成功开发了一种针对多用户环境下电力定价和需求响应的有效且可扩展的解决方案，该方案能够处理用户私有信息，并在计算复杂性上取得了突破，为应对可再生能源整合挑战提供了新思路。"}}
{"id": "2509.12254", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12254", "abs": "https://arxiv.org/abs/2509.12254", "authors": ["Oddvar Kloster", "Bjørnar Luteberget", "Carlo Mannino", "Giorgio Sartor"], "title": "DISPLIB: a library of train dispatching problems", "comment": null, "summary": "Optimization-based decision support systems have a significant potential to\nreduce delays, and thus improve efficiency on the railways, by automatically\nre-routing and re-scheduling trains after delays have occurred. The operations\nresearch community has dedicated a lot of effort to developing optimization\nalgorithms for this problem, but each study is typically tightly connected with\na specific industrial use case. Code and data are seldom shared publicly. This\nfact hinders reproducibility, and has led to a proliferation of papers\ndescribing algorithms for more or less compatible problem definitions, without\nany real opportunity for readers to assess their relative performance. Inspired\nby the successful communities around MILP, SAT, TSP, VRP, etc., we introduce a\ncommon problem definition and file format, DISPLIB, which captures all the main\nfeatures of train re-routing and re-scheduling. We have gathered problem\ninstances from multiple real-world use cases and made them openly available. In\nthis paper, we describe the problem definition, the industrial instances, and a\nreference solver implementation. This allows any researcher or developer to\nwork on the train dispatching problem without an industrial connection, and\nenables the research community to perform empirical comparisons between\nsolvers. All materials are available online at https://displib.github.io.", "AI": {"tldr": "本文介绍了DISPLIB，一个用于列车重新调度和重新路由问题的通用问题定义、文件格式、工业实例和参考求解器，旨在促进可重复性、算法比较和开放研究。", "motivation": "优化决策支持系统在减少列车延误方面潜力巨大，但现有研究通常与特定工业应用紧密相连，代码和数据很少公开共享。这阻碍了研究的可重复性，导致算法难以比较其相对性能，因为问题定义不兼容。", "method": "受MILP、SAT、TSP、VRP等成功社区的启发，本文引入了一个名为DISPLIB的通用问题定义和文件格式，它捕获了列车重新调度和重新路由的所有主要特征。同时，收集了多个真实工业案例的问题实例并公开可用，并提供了一个参考求解器实现。", "result": "本文描述了DISPLIB问题定义、工业实例和参考求解器实现。所有材料均在线公开，使任何研究人员或开发者无需工业背景即可研究列车调度问题，并使研究社区能够对求解器进行实证比较。", "conclusion": "DISPLIB的推出，通过提供通用问题定义、文件格式、真实实例和参考求解器，极大地促进了列车调度优化领域的可重复性、开放研究和算法之间的实证比较。"}}
{"id": "2509.12287", "categories": ["eess.IV", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.12287", "abs": "https://arxiv.org/abs/2509.12287", "authors": ["Nathan He", "Cody Chen"], "title": "Enhancing Radiographic Disease Detection with MetaCheX, a Context-Aware Multimodal Model", "comment": "All authors contributed equally, 5 pages, 2 figures, 1 table", "summary": "Existing deep learning models for chest radiology often neglect patient\nmetadata, limiting diagnostic accuracy and fairness. To bridge this gap, we\nintroduce MetaCheX, a novel multimodal framework that integrates chest X-ray\nimages with structured patient metadata to replicate clinical decision-making.\nOur approach combines a convolutional neural network (CNN) backbone with\nmetadata processed by a multilayer perceptron through a shared classifier.\nEvaluated on the CheXpert Plus dataset, MetaCheX consistently outperformed\nradiograph-only baseline models across multiple CNN architectures. By\nintegrating metadata, the overall diagnostic accuracy was significantly\nimproved, measured by an increase in AUROC. The results of this study\ndemonstrate that metadata reduces algorithmic bias and enhances model\ngeneralizability across diverse patient populations. MetaCheX advances clinical\nartificial intelligence toward robust, context-aware radiographic disease\ndetection.", "AI": {"tldr": "MetaCheX是一个多模态框架，结合胸部X射线图像和患者元数据，显著提高了胸部放射学诊断的准确性，并减少了算法偏见。", "motivation": "现有深度学习模型在胸部放射学诊断中忽视了患者元数据，这限制了诊断的准确性和公平性。", "method": "MetaCheX框架将卷积神经网络（CNN）骨干用于处理X射线图像，将多层感知器（MLP）用于处理结构化患者元数据，然后将两者通过一个共享分类器进行整合。", "result": "在CheXpert Plus数据集上，MetaCheX在多种CNN架构下均优于仅使用放射图像的基线模型。通过整合元数据，诊断准确性（AUROC）显著提高。研究结果表明，元数据减少了算法偏见，并增强了模型在不同患者群体中的泛化能力。", "conclusion": "MetaCheX将临床人工智能推向了更稳健、更具上下文感知能力的放射学疾病检测，复制了临床决策过程。"}}
{"id": "2509.12440", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12440", "abs": "https://arxiv.org/abs/2509.12440", "authors": ["Jiayi He", "Yangmin Huang", "Qianyun Du", "Xiangying Zhou", "Zhiyang He", "Jiaxue Hu", "Xiaodong Tao", "Lixian Lai"], "title": "MedFact: Benchmarking the Fact-Checking Capabilities of Large Language Models on Chinese Medical Texts", "comment": null, "summary": "The increasing deployment of Large Language Models (LLMs) in healthcare\nnecessitates a rigorous evaluation of their factual reliability. However,\nexisting benchmarks are often limited by narrow domains of data, failing to\ncapture the complexity of real-world medical information. To address this\ncritical gap, we introduce MedFact, a new and challenging benchmark for Chinese\nmedical fact-checking. MedFact comprises 2,116 expert-annotated instances\ncurated from diverse real-world texts, spanning 13 medical specialties, 8\nfine-grained error types, 4 writing styles, and multiple difficulty levels. Its\nconstruction employs a hybrid AI-human framework where iterative expert\nfeedback refines an AI-driven, multi-criteria filtering process, ensuring both\nhigh data quality and difficulty. We conduct a comprehensive evaluation of 20\nleading LLMs, benchmarking their performance on veracity classification and\nerror localization against a human expert baseline. Our results reveal that\nwhile models can often determine if a text contains an error, precisely\nlocalizing it remains a substantial challenge, with even top-performing models\nfalling short of human performance. Furthermore, our analysis uncovers a\nfrequent ``over-criticism'' phenomenon, a tendency for models to misidentify\ncorrect information as erroneous, which is exacerbated by advanced reasoning\ntechniques such as multi-agent collaboration and inference-time scaling. By\nhighlighting these critical challenges for deploying LLMs in medical\napplications, MedFact provides a robust resource to drive the development of\nmore factually reliable and medically aware models.", "AI": {"tldr": "本文介绍了MedFact，一个用于中文医疗事实核查的新基准，旨在评估大型语言模型（LLMs）的真实性。结果显示LLMs在错误定位方面表现不佳，并存在“过度批评”现象，突出了其在医疗应用中的局限性。", "motivation": "随着LLMs在医疗领域的广泛应用，对其事实可靠性进行严格评估的需求日益增长。然而，现有基准往往受限于狭窄的数据领域，无法捕捉真实世界医疗信息的复杂性，因此需要一个更全面、更具挑战性的评估工具。", "method": "研究引入了MedFact基准，包含2,116个由专家标注的实例，涵盖13个医疗专业、8种细粒度错误类型、4种写作风格和多个难度级别。其构建采用混合AI-人工框架，通过迭代专家反馈和AI驱动的多标准过滤过程，确保数据的高质量和难度。研究评估了20个主流LLMs在真实性分类和错误定位方面的表现，并与人类专家基线进行了对比。", "result": "评估结果表明，LLMs通常能够判断文本是否包含错误，但在精确错误定位方面仍面临巨大挑战，即使是表现最佳的模型也远低于人类水平。此外，研究发现LLMs存在频繁的“过度批评”现象，即倾向于将正确信息误判为错误，这种现象在使用多智能体协作和推理时扩展等高级推理技术时尤为突出。", "conclusion": "MedFact基准揭示了LLMs在医疗应用中部署所面临的关键挑战，并提供了一个强大的资源，以推动开发出更具事实可靠性和医学意识的模型。"}}
{"id": "2509.12390", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.12390", "abs": "https://arxiv.org/abs/2509.12390", "authors": ["Evangelos Psomiadis", "Panagiotis Tsiotras"], "title": "Distributed Event-Triggered Distance-Based Formation Control for Multi-Agent Systems", "comment": "8 pages, 7 figures", "summary": "This paper addresses the problem of collaborative formation control for\nmulti-agent systems with limited resources. We consider a team of robots tasked\nwith achieving a desired formation from arbitrary initial configurations. To\nreduce unnecessary control updates and conserve resources, we propose a\ndistributed event-triggered formation controller that relies on inter-agent\ndistance measurements. Control updates are triggered only when the measurement\nerror exceeds a predefined threshold, ensuring system stability. The proposed\ncontroller is validated through extensive simulations and real-world\nexperiments involving different formations, communication topologies,\nscalability tests, and variations in design parameters, while also being\ncompared against periodic triggering strategies. Results demonstrate that the\nevent-triggered approach significantly reduces control efforts while preserving\nformation performance.", "AI": {"tldr": "本文提出了一种分布式事件触发编队控制器，用于资源受限的多智能体系统，通过基于智能体间距离测量，仅在测量误差超过阈值时更新控制，从而显著减少控制工作量并保持编队性能。", "motivation": "在资源有限的多智能体系统中，需要从任意初始配置实现期望编队，并减少不必要的控制更新以节约资源。", "method": "提出了一种分布式事件触发编队控制器，该控制器依赖于智能体间的距离测量。只有当测量误差超过预定义阈值时，才会触发控制更新，同时确保系统稳定性。", "result": "通过广泛的仿真和真实世界实验验证，并与周期性触发策略进行比较，结果表明事件触发方法在保持编队性能的同时，显著减少了控制工作量。", "conclusion": "所提出的事件触发方法能够有效解决资源受限多智能体系统的协同编队控制问题，通过减少控制更新来节约资源，同时不牺牲编队性能。"}}
{"id": "2509.12247", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12247", "abs": "https://arxiv.org/abs/2509.12247", "authors": ["Abigail R. Cohen", "Yuming Sun", "Zhihao Qin", "Harsh S. Muriki", "Zihao Xiao", "Yeonju Lee", "Matthew Housley", "Andrew F. Sharkey", "Rhuanito S. Ferrarezi", "Jing Li", "Lu Gan", "Yongsheng Chen"], "title": "Modular, On-Site Solutions with Lightweight Anomaly Detection for Sustainable Nutrient Management in Agriculture", "comment": null, "summary": "Efficient nutrient management is critical for crop growth and sustainable\nresource consumption (e.g., nitrogen, energy). Current approaches require\nlengthy analyses, preventing real-time optimization; similarly, imaging\nfacilitates rapid phenotyping but can be computationally intensive, preventing\ndeployment under resource constraints. This study proposes a flexible, tiered\npipeline for anomaly detection and status estimation (fresh weight, dry mass,\nand tissue nutrients), including a comprehensive energy analysis of approaches\nthat span the efficiency-accuracy spectrum. Using a nutrient depletion\nexperiment with three treatments (T1-100%, T2-50%, and T3-25% fertilizer\nstrength) and multispectral imaging (MSI), we developed a hierarchical pipeline\nusing an autoencoder (AE) for early warning. Further, we compared two status\nestimation modules of different complexity for more detailed analysis:\nvegetation index (VI) features with machine learning (Random Forest, RF) and\nraw whole-image deep learning (Vision Transformer, ViT). Results demonstrated\nhigh-efficiency anomaly detection (73% net detection of T3 samples 9 days after\ntransplanting) at substantially lower energy than embodied energy in wasted\nnitrogen. The state estimation modules show trade-offs, with ViT outperforming\nRF on phosphorus and calcium estimation (R2 0.61 vs. 0.58, 0.48 vs. 0.35) at\nhigher energy cost. With our modular pipeline, this work opens opportunities\nfor edge diagnostics and practical opportunities for agricultural\nsustainability.", "AI": {"tldr": "本研究提出了一种分层管道，结合多光谱成像和机器学习/深度学习方法，用于作物养分异常检测和状态估计，并在效率和准确性之间进行了权衡分析，以实现农业可持续性。", "motivation": "当前的养分管理方法分析耗时，无法实现实时优化；快速表型分析计算密集，难以在资源受限环境下部署。因此，需要一种高效、实时的解决方案来优化作物生长和可持续资源消耗。", "method": "本研究提出了一个灵活的分层管道，用于异常检测和状态估计（鲜重、干物质和组织养分），并对不同效率-准确度范围的方法进行了全面的能耗分析。通过一项包含三种处理（T1-100%、T2-50%、T3-25%肥料强度）和多光谱成像（MSI）的养分耗竭实验，开发了一个使用自编码器（AE）进行早期预警的层次管道。此外，比较了两种不同复杂度的状态估计模块：基于植被指数（VI）特征的机器学习（随机森林，RF）和基于原始全图像的深度学习（Vision Transformer，ViT）。", "result": "结果表明，异常检测效率高（移植后9天对T3样本的净检测率为73%），且能耗远低于浪费氮肥所隐含的能耗。状态估计模块显示出权衡：ViT在磷和钙估计上优于RF（R2分别为0.61 vs. 0.58，0.48 vs. 0.35），但能耗更高。", "conclusion": "该模块化管道为边缘诊断和农业可持续性提供了实际机会。"}}
{"id": "2509.12281", "categories": ["eess.SY", "cs.LG", "cs.SY", "I.2.0"], "pdf": "https://arxiv.org/pdf/2509.12281", "abs": "https://arxiv.org/abs/2509.12281", "authors": ["Sel Ly", "Kapil Chauhan", "Anshuman Singh", "Hung Dinh Nguyen"], "title": "Meta-model Neural Process for Probabilistic Power Flow under Varying N-1 System Topologies", "comment": "An improved version for the conference paper at PESGM 2025", "summary": "The probabilistic power flow (PPF) problem is essential to quantifying the\ndistribution of the nodal voltages due to uncertain injections. The\nconventional PPF problem considers a fixed topology, and the solutions to such\na PPF problem are associated with this topology. A change in the topology might\nalter the power flow patterns and thus require the PPF problem to be solved\nagain. The previous PPF model and its solutions are no longer valid for the new\ntopology. This practice incurs both inconvenience and computation burdens as\nmore contingencies are foreseen due to high renewables and a large share of\nelectric vehicles. This paper presents a novel topology-adaptive approach,\nbased on the meta-model Neural Process (MMNP), for finding the solutions to PPF\nproblems under varying N-1 topologies, particularly with one-line failures. By\nleveraging context set-based topology representation and conditional\ndistribution over function learning techniques, the proposed MMNP enhances the\nrobustness of PPF models to topology variations, mitigating the need for\nretraining PPF models on a new configuration. Simulations on an IEEE 9-bus\nsystem and IEEE 118-bus system validate the model's performance. The maximum\n%L1-relative error norm was observed as 1.11% and 0.77% in 9-bus and 118-bus,\nrespectively. This adaptive approach fills a critical gap in PPF methodology in\nan era of increasing grid volatility.", "AI": {"tldr": "本文提出了一种基于元模型神经过程（MMNP）的拓扑自适应方法，用于在N-1拓扑变化（单线故障）下求解概率潮流问题，避免了传统方法因拓扑变化而需要重新训练的弊端。", "motivation": "传统的概率潮流（PPF）问题假设固定拓扑，当拓扑发生变化时（如N-1故障），需要重新求解PPF，导致不便和计算负担，尤其是在可再生能源和电动汽车普及导致电网不确定性增加的背景下。", "method": "提出了一种基于元模型神经过程（MMNP）的拓扑自适应方法。该方法利用基于上下文集的拓扑表示和函数学习上的条件分布技术，增强了PPF模型对拓扑变化的鲁棒性。", "result": "在IEEE 9节点系统和IEEE 118节点系统上的仿真验证了模型的性能。在9节点和118节点系统中，观察到的最大L1相对误差范数分别为1.11%和0.77%。", "conclusion": "该自适应方法增强了PPF模型对拓扑变化的鲁棒性，减少了在新配置下重新训练PPF模型的需要。它填补了在电网波动性日益增加的时代PPF方法学中的关键空白。"}}
{"id": "2509.12263", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.12263", "abs": "https://arxiv.org/abs/2509.12263", "authors": ["Gautam Sreekumar", "Vishnu Naresh Boddeti"], "title": "InPhyRe Discovers: Large Multimodal Models Struggle in Inductive Physical Reasoning", "comment": "35 pages including appendix", "summary": "Large multimodal models (LMMs) encode universal physical laws observed during\ntraining, such as momentum conservation, as parametric knowledge. It allows\nLMMs to answer physical reasoning queries, such as the outcome of a potential\ncollision event from visual input. However, since parametric knowledge includes\nonly the physical laws seen during training, it is insufficient for reasoning\nwhen the inference scenario violates these physical laws. In contrast, humans\npossess the skill to adapt their physical reasoning to unseen physical\nenvironments from a few visual examples. This ability, which we refer to as\ninductive physical reasoning, is indispensable for LMMs if they are to replace\nhuman agents in safety-critical applications. Despite its importance, existing\nvisual benchmarks evaluate only the parametric knowledge in LMMs, and not\ninductive physical reasoning. To this end, we propose InPhyRe, the first visual\nquestion answering benchmark to measure inductive physical reasoning in LMMs.\nInPhyRe evaluates LMMs on their ability to predict the outcome of collision\nevents in algorithmically generated synthetic collision videos. By inspecting\n13 LMMs, InPhyRe informs us that (1) LMMs struggle to apply their limited\nparametric knowledge about universal physical laws to reasoning, (2) inductive\nphysical reasoning in LMMs is weak when demonstration samples violate universal\nphysical laws, and (3) inductive physical reasoning in LMMs suffers from\nlanguage bias and largely ignores the visual inputs, questioning the\ntrustworthiness of LMMs regarding visual inputs.", "AI": {"tldr": "本文提出了InPhyRe，首个用于评估大型多模态模型（LMMs）归纳物理推理能力的视觉问答基准。研究发现LMMs在该能力上表现不佳，尤其是在物理定律被违反时，且存在语言偏见。", "motivation": "LMMs的参数化知识不足以应对违反已知物理定律的推理场景，而人类具备从少量视觉示例中适应新物理环境的归纳物理推理能力。现有视觉基准只评估LMMs的参数化知识，未能衡量其归纳物理推理能力，这对于LMMs在安全关键应用中的部署至关重要。", "method": "提出了InPhyRe，一个视觉问答基准，通过算法生成的合成碰撞视频来评估LMMs预测碰撞事件结果的能力，以此衡量其归纳物理推理水平。", "result": "对13个LMMs的测试结果表明：(1) LMMs难以将有限的通用物理定律参数化知识应用于推理；(2) 当演示样本违反通用物理定律时，LMMs的归纳物理推理能力很弱；(3) LMMs的归纳物理推理受语言偏见影响，在很大程度上忽略了视觉输入，对其视觉输入的可靠性提出质疑。", "conclusion": "LMMs的归纳物理推理能力尚弱，尤其是在物理定律被违反的情况下。此外，它们在推理过程中存在语言偏见，倾向于忽略视觉输入，这使得LMMs在需要高度信任视觉信息的应用中存在风险。"}}
{"id": "2509.12512", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12512", "abs": "https://arxiv.org/abs/2509.12512", "authors": ["Fazle Rafsani", "Jay Shah", "Catherine D. Chong", "Todd J. Schwedt", "Teresa Wu"], "title": "DinoAtten3D: Slice-Level Attention Aggregation of DinoV2 for 3D Brain MRI Anomaly Classification", "comment": "ACCEPTED at the ICCV 2025 Workshop on Anomaly Detection with\n  Foundation Models", "summary": "Anomaly detection and classification in medical imaging are critical for\nearly diagnosis but remain challenging due to limited annotated data, class\nimbalance, and the high cost of expert labeling. Emerging vision foundation\nmodels such as DINOv2, pretrained on extensive, unlabeled datasets, offer\ngeneralized representations that can potentially alleviate these limitations.\nIn this study, we propose an attention-based global aggregation framework\ntailored specifically for 3D medical image anomaly classification. Leveraging\nthe self-supervised DINOv2 model as a pretrained feature extractor, our method\nprocesses individual 2D axial slices of brain MRIs, assigning adaptive\nslice-level importance weights through a soft attention mechanism. To further\naddress data scarcity, we employ a composite loss function combining supervised\ncontrastive learning with class-variance regularization, enhancing inter-class\nseparability and intra-class consistency. We validate our framework on the ADNI\ndataset and an institutional multi-class headache cohort, demonstrating strong\nanomaly classification performance despite limited data availability and\nsignificant class imbalance. Our results highlight the efficacy of utilizing\npretrained 2D foundation models combined with attention-based slice aggregation\nfor robust volumetric anomaly detection in medical imaging. Our implementation\nis publicly available at https://github.com/Rafsani/DinoAtten3D.git.", "AI": {"tldr": "本研究提出一种基于注意力机制的全局聚合框架，利用预训练的DINOv2模型和复合损失函数，在有限数据和类别不平衡的情况下，实现了3D医学图像异常分类的强大性能。", "motivation": "医学影像中的异常检测和分类对于早期诊断至关重要，但面临标注数据有限、类别不平衡和专家标注成本高昂等挑战。新兴的视觉基础模型（如DINOv2）在大量无标签数据上进行预训练，提供了广义的表示，有望缓解这些限制。", "method": "该方法利用自监督的DINOv2模型作为预训练的2D特征提取器，处理脑部MRI的单个2D轴向切片。通过软注意力机制分配自适应的切片级重要性权重，并提出一个专门针对3D医学图像异常分类的基于注意力机制的全局聚合框架。为解决数据稀缺问题，采用结合监督对比学习和类别方差正则化的复合损失函数，以增强类间可分离性和类内一致性。", "result": "该框架在ADNI数据集和一个机构多类别头痛队列上进行了验证，尽管数据可用性有限且类别严重不平衡，但仍展示出强大的异常分类性能。", "conclusion": "研究结果强调了利用预训练的2D基础模型结合基于注意力的切片聚合，在医学影像中实现鲁棒的体积异常检测的有效性。"}}
{"id": "2509.12451", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.12451", "abs": "https://arxiv.org/abs/2509.12451", "authors": ["Wonbin Kweon", "SeongKu Kang", "Runchu Tian", "Pengcheng Jiang", "Jiawei Han", "Hwanjo Yu"], "title": "Topic Coverage-based Demonstration Retrieval for In-Context Learning", "comment": "EMNLP 2025 Main", "summary": "The effectiveness of in-context learning relies heavily on selecting\ndemonstrations that provide all the necessary information for a given test\ninput. To achieve this, it is crucial to identify and cover fine-grained\nknowledge requirements. However, prior methods often retrieve demonstrations\nbased solely on embedding similarity or generation probability, resulting in\nirrelevant or redundant examples. In this paper, we propose TopicK, a topic\ncoverage-based retrieval framework that selects demonstrations to\ncomprehensively cover topic-level knowledge relevant to both the test input and\nthe model. Specifically, TopicK estimates the topics required by the input and\nassesses the model's knowledge on those topics. TopicK then iteratively selects\ndemonstrations that introduce previously uncovered required topics, in which\nthe model exhibits low topical knowledge. We validate the effectiveness of\nTopicK through extensive experiments across various datasets and both open- and\nclosed-source LLMs. Our source code is available at\nhttps://github.com/WonbinKweon/TopicK_EMNLP2025.", "AI": {"tldr": "本文提出TopicK框架，通过全面覆盖主题级别知识来选择上下文学习的示例，以解决现有方法选择不相关或冗余示例的问题。", "motivation": "上下文学习的效果严重依赖于选择能提供所有必要信息的示例。然而，现有方法通常仅基于嵌入相似性或生成概率检索示例，导致选出不相关或冗余的示例，未能覆盖细粒度知识需求。", "method": "本文提出TopicK，一个基于主题覆盖的检索框架。它估算输入所需的主题，评估模型在这些主题上的知识水平，然后迭代选择能引入先前未覆盖的所需主题且模型在该主题上知识较少的示例。", "result": "通过在各种数据集以及开源和闭源大型语言模型上的广泛实验，验证了TopicK的有效性。", "conclusion": "TopicK通过全面覆盖与测试输入和模型相关的主题级别知识，显著提高了上下文学习中示例选择的质量和效率。"}}
{"id": "2509.12398", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.12398", "abs": "https://arxiv.org/abs/2509.12398", "authors": ["Michael Lorenz", "Bertram Taetz", "Gabriele Bleser-Taetz", "Didier Stricker"], "title": "MinJointTracker: Real-time inertial kinematic chain tracking with joint position estimation and minimal state size", "comment": "10 pages, 2 figures", "summary": "Inertial motion capture is a promising approach for capturing motion outside\nthe laboratory. However, as one major drawback, most of the current methods\nrequire different quantities to be calibrated or computed offline as part of\nthe setup process, such as segment lengths, relative orientations between\ninertial measurement units (IMUs) and segment coordinate frames (IMU-to-segment\ncalibrations) or the joint positions in the IMU frames. This renders the setup\nprocess inconvenient. This work contributes to real-time capable\ncalibration-free inertial tracking of a kinematic chain, i.e. simultaneous\nrecursive Bayesian estimation of global IMU angular kinematics and joint\npositions in the IMU frames, with a minimal state size. Experimental results on\nsimulated IMU data from a three-link kinematic chain (manipulator study) as\nwell as re-simulated IMU data from healthy humans walking (lower body study)\nshow that the calibration-free and lightweight algorithm provides not only\ndrift-free relative but also drift-free absolute orientation estimates with a\nglobal heading reference for only one IMU as well as robust and fast\nconvergence of joint position estimates in the different movement scenarios.", "AI": {"tldr": "本文提出了一种实时、免校准的惯性运动捕捉算法，通过递归贝叶斯估计同时获取全局IMU角运动学和IMU坐标系中的关节位置，实现了无漂移的姿态估计和鲁棒的关节位置收敛。", "motivation": "目前的惯性运动捕捉方法需要大量的离线校准（如肢体长度、IMU与肢体坐标系之间的相对方向、IMU坐标系中的关节位置），使得设置过程非常不便，限制了其在实验室外的应用。", "method": "该研究贡献了一种实时、免校准的运动学链惯性跟踪方法，采用递归贝叶斯估计，以最小状态尺寸同时估计全局IMU角运动学和IMU坐标系中的关节位置。", "result": "在三连杆运动链（机械臂研究）的模拟IMU数据和健康人行走（下半身研究）的重模拟IMU数据上的实验结果表明，该免校准、轻量级算法不仅提供了无漂移的相对姿态估计，还为单个IMU提供了具有全局航向参考的无漂移绝对姿态估计，并在不同运动场景中实现了关节位置估计的鲁棒且快速收敛。", "conclusion": "所提出的免校准、轻量级算法成功解决了现有惯性运动捕捉方法的设置不便问题，能够提供无漂移的相对和绝对姿态估计以及鲁棒的关节位置估计，适用于实时运动跟踪。"}}
{"id": "2509.12248", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.12248", "abs": "https://arxiv.org/abs/2509.12248", "authors": ["Yuriel Ryan", "Rui Yang Tan", "Kenny Tsu Wei Choo", "Roy Ka-Wei Lee"], "title": "Humor in Pixels: Benchmarking Large Multimodal Models Understanding of Online Comics", "comment": "27 pages, 8 figures, EMNLP 2025", "summary": "Understanding humor is a core aspect of social intelligence, yet it remains a\nsignificant challenge for Large Multimodal Models (LMMs). We introduce\nPixelHumor, a benchmark dataset of 2,800 annotated multi-panel comics designed\nto evaluate LMMs' ability to interpret multimodal humor and recognize narrative\nsequences. Experiments with state-of-the-art LMMs reveal substantial gaps: for\ninstance, top models achieve only 61% accuracy in panel sequencing, far below\nhuman performance. This underscores critical limitations in current models'\nintegration of visual and textual cues for coherent narrative and humor\nunderstanding. By providing a rigorous framework for evaluating multimodal\ncontextual and narrative reasoning, PixelHumor aims to drive the development of\nLMMs that better engage in natural, socially aware interactions.", "AI": {"tldr": "该研究引入了PixelHumor数据集，用于评估大型多模态模型（LMMs）在理解多模态幽默和叙事序列方面的能力，发现当前LMMs表现远低于人类水平。", "motivation": "理解幽默是社交智能的核心，但对大型多模态模型（LMMs）来说仍是巨大挑战。现有模型在整合视觉和文本线索以理解连贯叙事和幽默方面存在显著局限。", "method": "引入了PixelHumor，一个包含2,800个带注释的多面板漫画的基准数据集。该数据集旨在评估LMMs解释多模态幽默和识别叙事序列的能力。研究人员使用最先进的LMMs进行了实验。", "result": "实验结果显示，最先进的LMMs存在显著差距。例如，在面板排序任务中，顶级模型仅达到61%的准确率，远低于人类表现，这揭示了当前模型在整合视觉和文本线索以理解连贯叙事和幽默方面的关键局限。", "conclusion": "当前LMMs在多模态幽默和叙事理解方面存在显著不足。PixelHumor提供了一个严格的评估框架，旨在推动LMMs的发展，使其能更好地参与自然、具有社会意识的互动。"}}
{"id": "2509.12364", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.12364", "abs": "https://arxiv.org/abs/2509.12364", "authors": ["Nacira Agram", "Fred Espen Benth", "Giulia Pucci", "Jan Rems"], "title": "A Deep Learning Approach to Renewable Capacity Installation under Jump Uncertainty", "comment": "29 pages, 12 figures", "summary": "We study a stochastic model for the installation of renewable energy capacity\nunder demand uncertainty and jump driven dynamics. The system is governed by a\nmultidimensional Ornstein-Uhlenbeck (OU) process driven by a subordinator,\ncapturing abrupt variations in renewable generation and electricity load.\nInstallation decisions are modeled through control actions that increase\ncapacity in response to environmental and economic conditions.\n  We consider two distinct solution approaches. First, we implement a\nstructured threshold based control rule, where capacity is increased\nproportionally when the stochastic capacity factor falls below a fixed level.\nThis formulation leads to a nonlinear partial integro-differential equation\n(PIDE), which we solve by reformulating it as a backward stochastic\ndifferential equation with jumps. We extend the DBDP solver in\n\\cite{hure2020deep} to the pure jump setting, employing a dual neural network\narchitecture to approximate both the value function and the jump sensitivity.\n  Second, we propose a fully data driven deep control algorithm that directly\nlearns the optimal feedback policy by minimizing the expected cost functional\nusing neural networks. This approach avoids assumptions on the form of the\ncontrol rule and enables adaptive interventions based on the evolving system\nstate.\n  Numerical experiments highlight the strengths of both methods. While the\nthreshold based BSDE approach offers interpretability and tractability, the\ndeep control strategy achieves improved performance through flexibility in\ncapacity allocation. Together, these tools provide a robust framework for\ndecision support in long term renewable energy expansion under uncertainty.", "AI": {"tldr": "本研究提出了一个随机模型，用于在需求不确定性和跳跃动态下安装可再生能源容量。它采用了两种控制方法：基于阈值的跳跃BSDE方法（具有可解释性）和数据驱动的深度控制算法（具有更好的性能），为长期可再生能源扩张提供决策支持。", "motivation": "在需求不确定性和可再生能源发电、电力负荷的突发变化（由跳跃驱动的动态捕捉）下，有效管理可再生能源容量的安装是一个重要的挑战。", "method": "研究采用由次序器驱动的多维Ornstein-Uhlenbeck (OU) 过程来建模系统。提出了两种解决方案：1. 基于阈值的控制规则：当随机容量因子低于固定水平时，按比例增加容量，这导致一个非线性偏积分微分方程(PIDE)，通过将其重构为带跳跃的后向随机微分方程(BSDE)并扩展DBDP求解器（使用双神经网络）来解决。2. 数据驱动的深度控制算法：使用神经网络直接学习最优反馈策略，通过最小化预期成本函数来避免对控制规则形式的假设。", "result": "数值实验表明，基于阈值的BSDE方法提供了可解释性和易处理性，而深度控制策略通过容量分配的灵活性实现了改进的性能。", "conclusion": "这两种方法共同提供了一个鲁棒的框架，用于在不确定性下进行长期可再生能源扩张的决策支持。"}}
{"id": "2509.12273", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.12273", "abs": "https://arxiv.org/abs/2509.12273", "authors": ["Liangqi Yuan", "Dong-Jun Han", "Christopher G. Brinton", "Sabine Brunswicker"], "title": "LLMAP: LLM-Assisted Multi-Objective Route Planning with User Preferences", "comment": null, "summary": "The rise of large language models (LLMs) has made natural language-driven\nroute planning an emerging research area that encompasses rich user objectives.\nCurrent research exhibits two distinct approaches: direct route planning using\nLLM-as-Agent and graph-based searching strategies. However, LLMs in the former\napproach struggle to handle extensive map data, while the latter shows limited\ncapability in understanding natural language preferences. Additionally, a more\ncritical challenge arises from the highly heterogeneous and unpredictable\nspatio-temporal distribution of users across the globe. In this paper, we\nintroduce a novel LLM-Assisted route Planning (LLMAP) system that employs an\nLLM-as-Parser to comprehend natural language, identify tasks, and extract user\npreferences and recognize task dependencies, coupled with a Multi-Step Graph\nconstruction with iterative Search (MSGS) algorithm as the underlying solver\nfor optimal route finding. Our multi-objective optimization approach adaptively\ntunes objective weights to maximize points of interest (POI) quality and task\ncompletion rate while minimizing route distance, subject to three key\nconstraints: user time limits, POI opening hours, and task dependencies. We\nconduct extensive experiments using 1,000 routing prompts sampled with varying\ncomplexity across 14 countries and 27 cities worldwide. The results demonstrate\nthat our approach achieves superior performance with guarantees across multiple\nconstraints.", "AI": {"tldr": "本文提出了一种名为LLMAP的新型LLM辅助路径规划系统。该系统利用LLM作为解析器来理解自然语言偏好和任务依赖，并结合多步图构建与迭代搜索（MSGS）算法进行多目标优化，以解决现有方法在处理大规模地图数据和自然语言理解方面的局限性，同时应对用户时空分布的异质性。", "motivation": "随着大型语言模型（LLMs）的兴起，自然语言驱动的路径规划成为新兴研究领域。然而，现有方法存在局限：直接使用LLM作为Agent难以处理大量地图数据，而基于图的搜索策略在理解自然语言偏好方面能力有限。此外，全球用户高度异构且不可预测的时空分布带来了关键挑战。", "method": "本文引入了LLM辅助路径规划（LLMAP）系统。该系统采用：1) 一个LLM作为解析器（LLM-as-Parser），用于理解自然语言、识别任务、提取用户偏好和识别任务依赖；2) 一个多步图构建与迭代搜索（MSGS）算法作为底层求解器，用于寻找最优路径。系统采用多目标优化方法，自适应调整目标权重，以最大化兴趣点（POI）质量和任务完成率，同时最小化路线距离，并受用户时间限制、POI开放时间和任务依赖这三个关键约束。", "result": "研究团队在全球14个国家和27个城市，使用1,000个不同复杂度的路由提示进行了广泛实验。结果表明，所提出的方法在多个约束条件下实现了卓越的性能。", "conclusion": "LLMAP系统通过结合LLM的自然语言理解能力和强大的图搜索算法，有效解决了自然语言驱动路径规划中的挑战，并在多重约束下实现了优异的路径规划性能。"}}
{"id": "2509.12534", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12534", "abs": "https://arxiv.org/abs/2509.12534", "authors": ["Jia-Hong Huang"], "title": "DeepEyeNet: Generating Medical Report for Retinal Images", "comment": "The paper is accepted by the Conference on Information and Knowledge\n  Management (CIKM), 2025", "summary": "The increasing prevalence of retinal diseases poses a significant challenge\nto the healthcare system, as the demand for ophthalmologists surpasses the\navailable workforce. This imbalance creates a bottleneck in diagnosis and\ntreatment, potentially delaying critical care. Traditional methods of\ngenerating medical reports from retinal images rely on manual interpretation,\nwhich is time-consuming and prone to errors, further straining\nophthalmologists' limited resources. This thesis investigates the potential of\nArtificial Intelligence (AI) to automate medical report generation for retinal\nimages. AI can quickly analyze large volumes of image data, identifying subtle\npatterns essential for accurate diagnosis. By automating this process, AI\nsystems can greatly enhance the efficiency of retinal disease diagnosis,\nreducing doctors' workloads and enabling them to focus on more complex cases.\nThe proposed AI-based methods address key challenges in automated report\ngeneration: (1) A multi-modal deep learning approach captures interactions\nbetween textual keywords and retinal images, resulting in more comprehensive\nmedical reports; (2) Improved methods for medical keyword representation\nenhance the system's ability to capture nuances in medical terminology; (3)\nStrategies to overcome RNN-based models' limitations, particularly in capturing\nlong-range dependencies within medical descriptions; (4) Techniques to enhance\nthe interpretability of the AI-based report generation system, fostering trust\nand acceptance in clinical practice. These methods are rigorously evaluated\nusing various metrics and achieve state-of-the-art performance. This thesis\ndemonstrates AI's potential to revolutionize retinal disease diagnosis by\nautomating medical report generation, ultimately improving clinical efficiency,\ndiagnostic accuracy, and patient care.", "AI": {"tldr": "本论文研究了利用人工智能自动化视网膜图像医疗报告生成，以提高诊断效率和准确性，应对眼科医生短缺问题。", "motivation": "视网膜疾病日益普遍，但眼科医生数量不足，导致诊断和治疗瓶颈。传统手动报告生成耗时且易出错，进一步加重医生负担，可能延误关键治疗。", "method": "本文提出了一系列基于AI的方法：1) 多模态深度学习方法，捕获文本关键词与视网膜图像之间的交互，生成更全面的报告；2) 改进医疗关键词表示方法，增强系统捕捉医学术语细微差别的能力；3) 克服基于RNN模型局限性的策略，尤其是在捕获医学描述中长距离依赖方面的能力；4) 增强AI报告生成系统可解释性的技术，以提高临床实践中的信任和接受度。", "result": "所提出的方法经过严格评估，使用了多种指标，并达到了最先进的性能。", "conclusion": "本论文展示了AI通过自动化医疗报告生成，在彻底改变视网膜疾病诊断方面的巨大潜力，最终提高临床效率、诊断准确性和患者护理水平。"}}
{"id": "2509.12459", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.12459", "abs": "https://arxiv.org/abs/2509.12459", "authors": ["Suvojit Acharjee", "Utathya Aich", "Asfak Ali"], "title": "Does Language Model Understand Language?", "comment": null, "summary": "Despite advances in natural language generation and understanding, LM still\nstruggle with fine grained linguistic phenomena such as tense, negation, voice,\nand modality which are the elements central to effective human communication.\nIn the context of the United Nations SDG 4, where linguistic clarity is\ncritical, the deployment of LMs in educational technologies demands careful\nscrutiny. As LMs are increasingly powering applications like tutoring systems,\nautomated grading, and translation, their alignment with human linguistic\ninterpretation becomes essential for effective learning. In this study, we\nconduct a evaluation of SOTA language models across these challenging contexts\nin both English and Bengali. To ensure a structured assessment, we introduce a\nnew Route for Evaluation of Cognitive Inference in Systematic Environments\nguidelines. Our proposed LUCID dataset, composed of carefully crafted sentence\npairs in English and Bengali, specifically challenges these models on critical\naspects of language comprehension, including negation, tense, voice variations.\nWe assess the performance of SOTA models including MISTRAL-SABA-24B,\nLLaMA-4-Scout-17B, LLaMA-3.3-70B, Gemma2-9B, and Compound-Beta using standard\nmetrics like Pearson correlation, Spearman correlation, and Mean Absolute\nError, as well as novel, linguistically inspired metric the HCE accuracy. The\nHCE accuracy measures how often model predictions fall within one standard\ndeviation of the mean human rating, thus capturing human like tolerance for\nvariability in language interpretation. Our findings highlight Compound-Beta as\nthe most balanced model, consistently achieving high correlations and low MAEs\nacross diverse language conditions. It records the highest Pearson correlation\nin English and demonstrates robust performance on mixed-language data,\nindicating a strong alignment with human judgments in cross lingual scenarios.", "AI": {"tldr": "本研究评估了SOTA语言模型在英语和孟加拉语中处理时态、否定、语态等细粒度语言现象的能力。通过引入新的LUCID数据集和HCE准确度等指标，发现Compound-Beta模型表现最均衡，与人类判断高度一致。", "motivation": "尽管自然语言生成和理解技术有所进步，但语言模型在处理时态、否定、语态和情态等对有效人类交流至关重要的细粒度语言现象时仍面临挑战。在联合国可持续发展目标4（教育）的背景下，语言清晰度至关重要，因此评估语言模型在教育技术中部署的准确性及其与人类语言解释的一致性变得尤为关键。", "method": "本研究评估了MISTRAL-SABA-24B、LLaMA-4-Scout-17B、LLaMA-3.3-70B、Gemma2-9B和Compound-Beta等SOTA语言模型。引入了一套新的“系统环境中认知推理评估路线”（Route for Evaluation of Cognitive Inference in Systematic Environments）指导方针，并构建了LUCID数据集，该数据集包含精心设计的英语和孟加拉语句对，专门用于挑战模型在否定、时态、语态变化等方面的理解能力。评估指标包括皮尔逊相关系数、斯皮尔曼相关系数、平均绝对误差（MAE），以及一种新颖的、受语言学启发的HCE准确度，该指标衡量模型预测落在人类平均评分一个标准差范围内的频率。", "result": "研究发现Compound-Beta是表现最均衡的模型，在不同的语言条件下均能持续实现高相关性和低MAE。它在英语中取得了最高的皮尔逊相关系数，并在混合语言数据上表现出稳健性，表明在跨语言场景中与人类判断高度一致。", "conclusion": "Compound-Beta模型在处理细粒度语言现象和跨语言场景方面表现出卓越的性能，与人类语言解释高度对齐。这一发现对于在教育技术等需要高语言清晰度的应用中部署语言模型具有重要意义。"}}
{"id": "2509.12444", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.12444", "abs": "https://arxiv.org/abs/2509.12444", "authors": ["Weiting Feng", "Kyle L. Walker", "Yunjie Yang", "Francesco Giorgio-Serchi"], "title": "Computing forward statics from tendon-length in flexible-joint hyper-redundant manipulators", "comment": "To be presented at IROS 2025, Hangzhou, China", "summary": "Hyper-redundant tendon-driven manipulators offer greater flexibility and\ncompliance over traditional manipulators. A common way of controlling such\nmanipulators relies on adjusting tendon lengths, which is an accessible control\nparameter. This approach works well when the kinematic configuration is\nrepresentative of the real operational conditions. However, when dealing with\nmanipulators of larger size subject to gravity, it becomes necessary to solve a\nstatic force problem, using tendon force as the input and employing a mapping\nfrom the configuration space to retrieve tendon length. Alternatively,\nmeasurements of the manipulator posture can be used to iteratively adjust\ntendon lengths to achieve a desired posture. Hence, either tension measurement\nor state estimation of the manipulator are required, both of which are not\nalways accurately available. Here, we propose a solution by reconciling cables\ntension and length as the input for the solution of the system forward statics.\nWe develop a screw-based formulation for a tendon-driven, multi-segment,\nhyper-redundant manipulator with elastic joints and introduce a forward statics\niterative solution method that equivalently makes use of either tendon length\nor tension as the input. This strategy is experimentally validated using a\ntraditional tension input first, subsequently showing the efficacy of the\nmethod when exclusively tendon lengths are used. The results confirm the\npossibility to perform open-loop control in static conditions using a kinematic\ninput only, thus bypassing some of the practical problems with tension\nmeasurement and state estimation of hyper-redundant systems.", "AI": {"tldr": "本文提出了一种超冗余腱驱动机械臂的正向静力学迭代解法，该方法可同时考虑肌腱张力和长度作为输入，并基于螺旋理论公式化。实验验证表明，该方法仅使用肌腱长度即可实现静态条件下的开环控制，从而避免了张力测量和状态估计的实际问题。", "motivation": "传统的腱驱动机械臂控制依赖于调整肌腱长度，但在考虑重力的大型机械臂中，需要解决静态力问题，通常以肌腱力作为输入或通过姿态测量迭代调整。然而，肌腱张力测量或准确的机械臂状态估计往往难以获得。", "method": "本文通过协调缆绳张力和长度作为系统正向静力学求解的输入，提出了一种解决方案。具体方法包括：开发基于螺旋理论的腱驱动、多段、弹性关节超冗余机械臂的公式；引入一种正向静力学迭代解法，该方法可等效地使用肌腱长度或张力作为输入。该策略首先通过传统的张力输入进行实验验证，随后展示了仅使用肌腱长度时的有效性。", "result": "实验结果证实了该方法在仅使用肌腱长度作为输入时也能有效工作。这表明在静态条件下，仅使用运动学输入（肌腱长度）即可实现开环控制。", "conclusion": "该方法通过仅使用运动学输入（肌腱长度）实现静态条件下的开环控制，从而绕过了超冗余系统在张力测量和状态估计方面的一些实际问题。"}}
{"id": "2509.12250", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.12250", "abs": "https://arxiv.org/abs/2509.12250", "authors": ["Yihong Ji", "Yunze Liu", "Yiyao Zhuo", "Weijiang Yu", "Fei Ma", "Joshua Huang", "Fei Yu"], "title": "OnlineHOI: Towards Online Human-Object Interaction Generation and Perception", "comment": "Accepted at ACM MM 2025", "summary": "The perception and generation of Human-Object Interaction (HOI) are crucial\nfor fields such as robotics, AR/VR, and human behavior understanding. However,\ncurrent approaches model this task in an offline setting, where information at\neach time step can be drawn from the entire interaction sequence. In contrast,\nin real-world scenarios, the information available at each time step comes only\nfrom the current moment and historical data, i.e., an online setting. We find\nthat offline methods perform poorly in an online context. Based on this\nobservation, we propose two new tasks: Online HOI Generation and Perception. To\naddress this task, we introduce the OnlineHOI framework, a network architecture\nbased on the Mamba framework that employs a memory mechanism. By leveraging\nMamba's powerful modeling capabilities for streaming data and the Memory\nmechanism's efficient integration of historical information, we achieve\nstate-of-the-art results on the Core4D and OAKINK2 online generation tasks, as\nwell as the online HOI4D perception task.", "AI": {"tldr": "本文提出在线人机交互（HOI）生成与感知任务，并引入OnlineHOI框架。该框架基于Mamba并结合记忆机制，解决了传统离线HOI方法在实时场景中的不足，并在多个在线HOI任务上取得了最先进的性能。", "motivation": "人机交互（HOI）在机器人、AR/VR和人类行为理解等领域至关重要。然而，现有方法多为离线模式，可以访问整个交互序列的信息。实际场景中，信息仅来自当前和历史数据（在线模式）。研究发现离线方法在在线环境中表现不佳，因此需要新的在线HOI建模方法。", "method": "作者提出了两个新任务：在线HOI生成和在线HOI感知。为解决这些任务，他们引入了OnlineHOI框架，该框架是一种基于Mamba架构的网络，并结合了记忆机制。Mamba擅长处理流式数据，而记忆机制则能高效整合历史信息。", "result": "OnlineHOI框架在Core4D和OAKINK2的在线生成任务以及HOI4D的在线感知任务上均取得了最先进的（state-of-the-art）结果。", "conclusion": "OnlineHOI框架通过结合Mamba强大的流数据建模能力和记忆机制对历史信息的有效整合，成功解决了在线人机交互的生成与感知问题，显著提升了该领域在实时场景中的应用潜力。"}}
{"id": "2509.12378", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.12378", "abs": "https://arxiv.org/abs/2509.12378", "authors": ["Ruining Yang", "Jingyuan Zhou", "Qiqing Wang", "Jinhao Liang", "Kaidi Yang"], "title": "Platoon-Centric Green Light Optimal Speed Advisory Using Safe Reinforcement Learning", "comment": null, "summary": "With recent advancements in Connected Autonomous Vehicles (CAVs), Green Light\nOptimal Speed Advisory (GLOSA) emerges as a promising eco-driving strategy to\nreduce the number of stops and idle time at intersections, thereby reducing\nenergy consumption and emissions. Existing studies typically improve energy and\ntravel efficiency for individual CAVs without considering their impacts on the\nentire mixed-traffic platoon, leading to inefficient traffic flow. While\nReinforcement Learning (RL) has the potential to achieve platoon-level control\nin a mixed-traffic environment, the training of RL is still challenged by (i)\ncar-following safety, i.e., CAVs should not collide with their immediate\npreceding vehicles, and (ii) red-light safety, i.e., CAVs should not run red\nlights. To address these challenges, this paper develops a platoon-centric,\nsafe RL-based GLOSA system that uses a multi-agent controller to optimize CAV\nspeed while achieving a balance between energy consumption and travel\nefficiency. We further incorporate Control Barrier Functions (CBFs) into the\nRL-based policy to provide explicit safety guarantees in terms of car-following\nsafety and red-light safety. Our simulation results illustrate that our\nproposed method outperforms state-of-the-art methods in terms of driving safety\nand platoon energy consumption.", "AI": {"tldr": "本文提出了一种以车队为中心的安全强化学习（RL）绿色信号最优速度建议（GLOSA）系统，该系统通过结合控制障碍函数（CBF）来确保车辆跟驰和闯红灯安全，同时优化混合交通中自动驾驶车辆（CAV）车队的能耗和出行效率。", "motivation": "现有GLOSA研究主要关注单个CAV的能耗和出行效率，忽略了对整个混合交通车队的影响，导致交通流效率低下。强化学习虽有潜力实现车队级控制，但面临跟驰安全和闯红灯安全挑战。", "method": "开发了一个以车队为中心、基于安全强化学习的GLOSA系统，采用多智能体控制器优化CAV速度，以平衡能耗和出行效率。此外，将控制障碍函数（CBF）融入到基于RL的策略中，为跟驰安全和闯红灯安全提供明确的安全保障。", "result": "仿真结果表明，所提出的方法在驾驶安全性和车队能耗方面优于现有最先进的方法。", "conclusion": "该研究成功地为混合交通环境下的CAV车队设计了一个安全的RL-GLOSA系统，有效解决了安全挑战，并在节能和出行效率之间取得了良好平衡，同时提升了车队整体的驾驶安全性和能耗表现。"}}
{"id": "2509.12274", "categories": ["cs.AI", "cs.CV", "cs.LG", "68T07, 68T45, 68U10", "I.4.8; I.2.6; I.5.4; C.3"], "pdf": "https://arxiv.org/pdf/2509.12274", "abs": "https://arxiv.org/abs/2509.12274", "authors": ["Mohammadreza Narimani", "Ali Hajiahmad", "Ali Moghimi", "Reza Alimardani", "Shahin Rafiee", "Amir Hossein Mirzabe"], "title": "Developing an aeroponic smart experimental greenhouse for controlling irrigation and plant disease detection using deep learning and IoT", "comment": "Author-accepted version. Presented at ASABE Annual International\n  Meeting (AIM) 2021 (virtual), Paper 2101252. Please cite the published\n  meeting paper: doi:10.13031/aim.202101252. Minor wording and formatting\n  updates in this preprint", "summary": "Controlling environmental conditions and monitoring plant status in\ngreenhouses is critical to promptly making appropriate management decisions\naimed at promoting crop production. The primary objective of this research\nstudy was to develop and test a smart aeroponic greenhouse on an experimental\nscale where the status of Geranium plant and environmental conditions are\ncontinuously monitored through the integration of the internet of things (IoT)\nand artificial intelligence (AI). An IoT-based platform was developed to\ncontrol the environmental conditions of plants more efficiently and provide\ninsights to users to make informed management decisions. In addition, we\ndeveloped an AI-based disease detection framework using VGG-19,\nInceptionResNetV2, and InceptionV3 algorithms to analyze the images captured\nperiodically after an intentional inoculation. The performance of the AI\nframework was compared with an expert's evaluation of disease status.\nPreliminary results showed that the IoT system implemented in the greenhouse\nenvironment is able to publish data such as temperature, humidity, water flow,\nand volume of charge tanks online continuously to users and adjust the\ncontrolled parameters to provide an optimal growth environment for the plants.\nFurthermore, the results of the AI framework demonstrate that the VGG-19\nalgorithm was able to identify drought stress and rust leaves from healthy\nleaves with the highest accuracy, 92% among the other algorithms.", "AI": {"tldr": "本研究开发并测试了一个智能气培温室，通过物联网（IoT）连续监测环境条件，并利用人工智能（AI）框架（基于VGG-19、InceptionResNetV2和InceptionV3）检测植物病害，以优化作物生产。", "motivation": "在温室中控制环境条件和监测植物状态对于及时做出适当的管理决策以促进作物生产至关重要。", "method": "研究开发了一个基于IoT的平台，用于持续监测和控制温室环境条件。此外，还开发了一个基于AI的疾病检测框架，利用VGG-19、InceptionResNetV2和InceptionV3算法，分析有意接种后定期捕获的图像。AI框架的性能与专家对疾病状态的评估进行了比较。", "result": "初步结果显示，IoT系统能够持续在线发布温度、湿度、水流量和充电罐容量等数据，并调整受控参数以提供最佳植物生长环境。AI框架的结果表明，VGG-19算法在识别干旱胁迫和锈病叶片方面表现出最高准确率（92%），优于其他算法。", "conclusion": "集成的IoT系统能够有效监测和控制温室环境，而AI框架（特别是VGG-19）能够准确检测植物病害，为智能温室管理和优化作物生产提供了可行方案。"}}
{"id": "2509.12596", "categories": ["eess.IV", "cs.CE"], "pdf": "https://arxiv.org/pdf/2509.12596", "abs": "https://arxiv.org/abs/2509.12596", "authors": ["Jiasong Chen", "Linchen Qian", "Ruonan Gong", "Christina Sun", "Tongran Qin", "Thuy Pham", "Caitlin Martin", "Mohammad Zafar", "John Elefteriades", "Wei Sun", "Liang Liang"], "title": "A Computational Pipeline for Patient-Specific Modeling of Thoracic Aortic Aneurysm: From Medical Image to Finite Element Analysis", "comment": null, "summary": "The aorta is the body's largest arterial vessel, serving as the primary\npathway for oxygenated blood within the systemic circulation. Aortic aneurysms\nconsistently rank among the top twenty causes of mortality in the United\nStates. Thoracic aortic aneurysm (TAA) arises from abnormal dilation of the\nthoracic aorta and remains a clinically significant disease, ranking as one of\nthe leading causes of death in adults. A thoracic aortic aneurysm ruptures when\nthe integrity of all aortic wall layers is compromised due to elevated blood\npressure. Currently, three-dimensional computed tomography (3D CT) is\nconsidered the gold standard for diagnosing TAA. The geometric characteristics\nof the aorta, which can be quantified from medical imaging, and stresses on the\naortic wall, which can be obtained by finite element analysis (FEA), are\ncritical in evaluating the risk of rupture and dissection. Deep learning based\nimage segmentation has emerged as a reliable method for extracting anatomical\nregions of interest from medical images. Voxel based segmentation masks of\nanatomical structures are typically converted into structured mesh\nrepresentation to enable accurate simulation. Hexahedral meshes are commonly\nused in finite element simulations of the aorta due to their computational\nefficiency and superior simulation accuracy. Due to anatomical variability,\npatient specific modeling enables detailed assessment of individual anatomical\nand biomechanics behaviors, supporting precise simulations, accurate diagnoses,\nand personalized treatment strategies. Finite element (FE) simulations provide\nvaluable insights into the biomechanical behaviors of tissues and organs in\nclinical studies. Developing accurate FE models represents a crucial initial\nstep in establishing a patient-specific, biomechanically based framework for\npredicting the risk of TAA.", "AI": {"tldr": "胸主动脉瘤(TAA)是导致死亡的主要原因之一，本研究旨在通过结合三维CT、深度学习图像分割和有限元分析(FEA)构建患者特异性生物力学模型，以准确评估TAA破裂风险并支持个性化治疗。", "motivation": "胸主动脉瘤(TAA)破裂是成人死亡的主要原因之一，目前诊断主要依赖3D CT。为了更准确地评估TAA的破裂和夹层风险，需要整合主动脉的几何特征和壁应力。开发患者特异性的生物力学模型对于精确预测TAA风险和指导个性化治疗至关重要。", "method": "使用三维计算机断层扫描(3D CT)作为诊断金标准；采用基于深度学习的图像分割技术提取解剖区域；将体素分割掩膜转换为结构化六面体网格表示；利用有限元分析(FEA)模拟主动脉壁应力，以实现患者特异性建模和生物力学行为分析。", "result": "通过整合医学成像、深度学习分割和有限元分析，能够实现对个体解剖结构和生物力学行为的详细评估，从而支持精确模拟、准确诊断和个性化治疗策略。构建准确的有限元模型是建立患者特异性、基于生物力学的TAA风险预测框架的关键第一步。", "conclusion": "开发结合医学成像、深度学习和有限元分析的患者特异性生物力学模型，对于准确预测胸主动脉瘤(TAA)的破裂风险、实现精确诊断和提供个性化治疗策略具有至关重要的意义。"}}
{"id": "2509.12476", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.12476", "abs": "https://arxiv.org/abs/2509.12476", "authors": ["Sumanta Bhattacharyya", "Sara Riaz", "Pedram Rooshenas"], "title": "Audited Reasoning Refinement: Fine-Tuning Language Models via LLM-Guided Step-Wise Evaluation and Correction", "comment": null, "summary": "Training a task-specific small reasoning model is challenging when direct\nhuman supervision or high-quality labels are scarce. However, LLMs with\nreasoning capabilities produce abundant intermediate reasoning traces that can\nbe systematically refined to create effective supervision signals. We propose\nReason-Refine-then-Align (R2tA), which turns refined model rationales into\nsupervision for training task-specific reasoning models. Our method generates\ninitial reasoning and responses from an open-source base model on task-specific\ninputs, then refines these traces, fixing hallucinations and inconsistencies,\nto form a high-fidelity dataset. We perform a two-stage alignment, supervised\nfine-tuning (SFT), followed by direct preference optimization (DPO) to\ncalibrate the model's intermediate reasoning with human-validated conceptual\npreferences and then condition the final output on that aligned reasoning. As a\ncase study, we apply R2tA to evaluate extended entity relationship diagrams\n(EERDs) in database system design, a structurally complex task where\nprompt-only methods miss or hallucinate errors. We curated a dataset of 600\nEERD variants (train/test split of 450/150, respectively) with induced mistakes\nspanning 11 categories. Empirical evaluation suggests R2tA provides a\npractical, cost-effective path to scalable LLM adaptation in data-scarce\ndomains, enabling reproducible AI tools for education and beyond.", "AI": {"tldr": "该研究提出Reason-Refine-then-Align (R2tA)方法，利用大型语言模型（LLMs）生成的、经过精炼的推理轨迹作为监督信号，来训练特定任务的小型推理模型，尤其适用于数据稀缺的领域，并通过评估扩展实体关系图（EERD）进行了验证。", "motivation": "在缺乏直接人工监督或高质量标签的情况下，训练特定任务的小型推理模型极具挑战性。然而，具有推理能力的LLMs能够生成大量的中间推理轨迹，这些轨迹可以被系统地精炼，从而创建有效的监督信号。", "method": "R2tA方法首先使用开源基础模型生成任务特定的初始推理和响应，然后精炼这些轨迹，纠正幻觉和不一致性，以形成高质量的数据集。接着，进行两阶段对齐：首先是监督微调（SFT），然后是直接偏好优化（DPO），以使模型的中间推理与人类验证的概念偏好对齐，并在此对齐的推理基础上生成最终输出。案例研究应用于数据库系统设计中的EERD评估，构建了一个包含600个EERD变体的定制数据集。", "result": "实证评估表明，R2tA为在数据稀缺领域实现LLM的可扩展适应提供了一条实用且成本效益高的方法，从而能够为教育及其他领域提供可复现的AI工具。", "conclusion": "R2tA是一种有效且经济的方法，可以在数据稀缺的环境下，利用大型语言模型精炼的推理过程作为监督，训练出特定任务的小型推理模型，并有望推广应用于教育等领域，提供可靠的AI工具。"}}
{"id": "2509.12458", "categories": ["cs.RO", "cs.AR", "cs.CV", "cs.ET", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.12458", "abs": "https://arxiv.org/abs/2509.12458", "authors": ["Àlmos Veres-Vitàlyos", "Genis Castillo Gomez-Raya", "Filip Lemic", "Daniel Johannes Bugelnig", "Bernhard Rinner", "Sergi Abadal", "Xavier Costa-Pérez"], "title": "Neural 3D Object Reconstruction with Small-Scale Unmanned Aerial Vehicles", "comment": "13 pages, 16 figures, 3 tables, 45 references", "summary": "Small Unmanned Aerial Vehicles (UAVs) exhibit immense potential for\nnavigating indoor and hard-to-reach areas, yet their significant constraints in\npayload and autonomy have largely prevented their use for complex tasks like\nhigh-quality 3-Dimensional (3D) reconstruction. To overcome this challenge, we\nintroduce a novel system architecture that enables fully autonomous,\nhigh-fidelity 3D scanning of static objects using UAVs weighing under 100\ngrams. Our core innovation lies in a dual-reconstruction pipeline that creates\na real-time feedback loop between data capture and flight control. A\nnear-real-time (near-RT) process uses Structure from Motion (SfM) to generate\nan instantaneous pointcloud of the object. The system analyzes the model\nquality on the fly and dynamically adapts the UAV's trajectory to intelligently\ncapture new images of poorly covered areas. This ensures comprehensive data\nacquisition. For the final, detailed output, a non-real-time (non-RT) pipeline\nemploys a Neural Radiance Fields (NeRF)-based Neural 3D Reconstruction (N3DR)\napproach, fusing SfM-derived camera poses with precise Ultra Wide-Band (UWB)\nlocation data to achieve superior accuracy. We implemented and validated this\narchitecture using Crazyflie 2.1 UAVs. Our experiments, conducted in both\nsingle- and multi-UAV configurations, conclusively show that dynamic trajectory\nadaptation consistently improves reconstruction quality over static flight\npaths. This work demonstrates a scalable and autonomous solution that unlocks\nthe potential of miniaturized UAVs for fine-grained 3D reconstruction in\nconstrained environments, a capability previously limited to much larger\nplatforms.", "AI": {"tldr": "本文提出了一种新颖的系统架构，使重量小于100克的无人机能够实现全自主、高保真地对静态物体进行3D扫描，通过双重重建流程和实时反馈，显著提高了重建质量。", "motivation": "小型无人机在有效载荷和自主性方面存在显著限制，这阻碍了它们执行复杂任务，如高质量的3D重建，尤其是在室内和难以到达的区域。", "method": "该系统采用双重重建流程：一个近实时（near-RT）流程使用SfM生成即时点云，并根据模型质量动态调整无人机轨迹以捕捉未覆盖区域；一个非实时（non-RT）流程使用基于NeRF的神经网络3D重建（N3DR），结合SfM相机姿态和UWB定位数据，实现卓越的精度。该架构在Crazyflie 2.1无人机上实现并验证，支持单无人机和多无人机配置。", "result": "实验结果表明，动态轨迹适应相比静态飞行路径，能够持续提高重建质量。", "conclusion": "这项工作展示了一种可扩展的自主解决方案，解锁了微型无人机在受限环境中进行精细3D重建的潜力，而此前这种能力仅限于更大的平台。"}}
{"id": "2509.12258", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12258", "abs": "https://arxiv.org/abs/2509.12258", "authors": ["Li Kun", "Milena Radenkovic"], "title": "EfficientNet-Based Multi-Class Detection of Real, Deepfake, and Plastic Surgery Faces", "comment": null, "summary": "Currently, deep learning has been utilised to tackle several difficulties in\nour everyday lives. It not only exhibits progress in computer vision but also\nconstitutes the foundation for several revolutionary technologies. Nonetheless,\nsimilar to all phenomena, the use of deep learning in diverse domains has\nproduced a multifaceted interaction of advantages and disadvantages for human\nsociety. Deepfake technology has advanced, significantly impacting social life.\nHowever, developments in this technology can affect privacy, the reputations of\nprominent personalities, and national security via software development. It can\nproduce indistinguishable counterfeit photographs and films, potentially\nimpairing the functionality of facial recognition systems, so presenting a\nsignificant risk.\n  The improper application of deepfake technology produces several detrimental\neffects on society. Face-swapping programs mislead users by altering persons'\nappearances or expressions to fulfil particular aims or to appropriate personal\ninformation. Deepfake technology permeates daily life through such techniques.\nCertain individuals endeavour to sabotage election campaigns or subvert\nprominent political figures by creating deceptive pictures to influence public\nperception, causing significant harm to a nation's political and economic\nstructure.", "AI": {"tldr": "深度学习，特别是Deepfake技术，在带来进步的同时，也对社会生活、个人隐私、国家安全及政治经济结构构成严重威胁。", "motivation": "深度学习在各领域的广泛应用及其产生的双面性，特别是Deepfake技术对社会造成的负面影响和潜在风险，促使本文对其进行分析。", "method": "本文主要通过描述和分析Deepfake技术的运作方式及其在不同场景下的不良应用，来阐述其对社会造成的危害。文中未提及具体的实验或研究方法。", "result": "Deepfake技术能够生成难以区分的虚假图像和视频，损害面部识别系统功能，侵犯隐私，损害名人声誉，威胁国家安全。它还被用于误导用户、操纵选举活动，并破坏国家政治和经济结构。", "conclusion": "尽管深度学习带来了技术进步，但Deepfake等技术的不当应用给人类社会带来了多方面的严重风险，需要警惕其对个人和社会稳定造成的潜在危害。"}}
{"id": "2509.12522", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.12522", "abs": "https://arxiv.org/abs/2509.12522", "authors": ["Devin Hunter", "Chinwendu Enyioha"], "title": "Hybrid State Estimation of Uncertain Nonlinear Dynamics Using Neural Processes", "comment": "32 pages (single column) - 6 figures", "summary": "Various neural network architectures are used in many of the state-of-the-art\napproaches for real-time nonlinear state estimation in dynamical systems. With\nthe ever-increasing incorporation of these data-driven models into the\nestimation domain, models with reliable margins of error are required --\nespecially for safety-critical applications. This paper discusses a novel\nhybrid, data-driven state estimation approach based on the physics-informed\nattentive neural process (PI-AttNP), a model-informed extension of the\nattentive neural process (AttNP). We augment this estimation approach with the\nregression-based split conformal prediction (CP) framework to obtain quantified\nmodel uncertainty with probabilistic guarantees. After presenting the algorithm\nin a generic form, we validate its performance in the task of grey-box state\nestimation of a simulated under-actuated six-degree-of-freedom quadrotor with\nmultimodal Gaussian sensor noise and several external perturbations typical to\nquadrotors. Further, we compare outcomes with state-of-the-art data-driven\nmethods, which provide significant evidence of the physics-informed neural\nprocess as a viable novel approach for model-driven estimation.", "AI": {"tldr": "本文提出了一种基于物理信息注意力神经过程（PI-AttNP）和分层共形预测（CP）的混合数据驱动状态估计方法，用于实时非线性系统，并提供具有概率保证的不确定性量化。", "motivation": "实时非线性状态估计中，数据驱动模型（特别是神经网络）的应用日益增多，但安全关键应用需要具有可靠误差范围的模型。现有方法缺乏量化的模型不确定性。", "method": "提出了一种新颖的混合数据驱动状态估计方法，其核心是物理信息注意力神经过程（PI-AttNP），这是注意力神经过程（AttNP）的模型信息扩展。该方法通过基于回归的分层共形预测（CP）框架进行增强，以获得具有概率保证的量化模型不确定性。", "result": "该算法在模拟欠驱动六自由度四旋翼飞行器的灰盒状态估计任务中得到了验证，该任务包含多模态高斯传感器噪声和多种外部扰动。与最先进的数据驱动方法进行比较，结果显著证明了物理信息神经过程作为一种可行的模型驱动估计新方法的有效性。", "conclusion": "结合共形预测的物理信息神经过程（PI-AttNP）是一种可行的模型驱动状态估计新方法，它能为数据驱动模型提供量化的不确定性，并在复杂动态系统（如四旋翼飞行器）的实时状态估计中展现出优越性能。"}}
{"id": "2509.12282", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.12282", "abs": "https://arxiv.org/abs/2509.12282", "authors": ["Sasi Kiran Gaddipati", "Farhana Keya", "Gollam Rabby", "Sören Auer"], "title": "AIssistant: An Agentic Approach for Human--AI Collaborative Scientific Work on Reviews and Perspectives in Machine Learning", "comment": null, "summary": "Advances in AI-assisted research have introduced powerful tools for\nliterature retrieval, hypothesis generation, experimentation, and manuscript\npreparation. However, systems remain fragmented and lack human-centred\nworkflows. To address these gaps, we introduce AIssistant, an agentic,\nopen-source Human-AI collaborative framework designed to simplify the\nend-to-end creation of scientific workflows. Since our development is still in\nan early stage, we present here the first experiments with AIssistant for\nperspective and review research papers in machine learning. Our system\nintegrates modular tools and agents for literature synthesis, section-wise\nexperimentation, citation management, and automatic LaTeX paper text\ngeneration, while maintaining human oversight at every stage to ensure\naccuracy, coherence, and scholarly rigour. We conducted a comprehensive\nevaluation across three layers: (1) Independent Human Review, following NeurIPS\ndouble-blind standards; (2) Automated LLM Review, using GPT-5 as a scalable\nhuman review proxy; and (3) Program Chair Oversight, where the chair monitors\nthe entire review process and makes final validation and acceptance decisions.\nThe results demonstrate that AIssistant improves drafting efficiency and\nthematic consistency. Nonetheless, Human-AI collaboration remains essential for\nmaintaining factual correctness, methodological soundness, and ethical\ncompliance. Despite its effectiveness, we identify key limitations, including\nhallucinated citations, difficulty adapting to dynamic paper structures, and\nincomplete integration of multimodal content.", "AI": {"tldr": "本文介绍了一个名为AIssistant的开源人机协作框架，旨在简化科学工作流程的端到端创建，提高论文草稿的效率和主题一致性，同时强调人工监督在确保准确性和合规性方面的重要性。", "motivation": "现有的AI辅助研究工具存在碎片化且缺乏以人为中心的工作流程的问题。", "method": "本文提出了AIssistant，一个代理式、开源的人机协作框架，集成了文献合成、分节实验、引用管理和自动LaTeX文本生成等模块化工具，并全程保持人工监督。评估通过三个层面进行：独立人工评审、自动化大型语言模型（LLM）评审（使用GPT-5作为代理）以及程序主席监督。", "result": "结果表明，AIssistant提高了草稿撰写效率和主题一致性。然而，人机协作对于保持事实正确性、方法严谨性和伦理合规性至关重要。研究也指出了关键局限性，包括幻觉引用、难以适应动态论文结构以及多模态内容集成不完整。", "conclusion": "AIssistant在提高科学论文草稿效率方面表现出有效性，但人机协作在确保准确性、严谨性和合规性方面仍然不可或缺。未来的工作应解决幻觉引用和多模态内容集成等现有局限性。"}}
{"id": "2509.12772", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.12772", "abs": "https://arxiv.org/abs/2509.12772", "authors": ["Damola Agbelese", "Krishna Chaitanya", "Pushpak Pati", "Chaitanya Parmar", "Pooya Mobadersany", "Shreyas Fadnavis", "Lindsey Surace", "Shadi Yarandi", "Louis R. Ghanem", "Molly Lucas", "Tommaso Mansi", "Oana Gabriela Cula", "Pablo F. Damasceno", "Kristopher Standish"], "title": "MEGAN: Mixture of Experts for Robust Uncertainty Estimation in Endoscopy Videos", "comment": "11 pages, 2 figures, 1 table, accepted at UNSURE, MICCAI", "summary": "Reliable uncertainty quantification (UQ) is essential in medical AI.\nEvidential Deep Learning (EDL) offers a computationally efficient way to\nquantify model uncertainty alongside predictions, unlike traditional methods\nsuch as Monte Carlo (MC) Dropout and Deep Ensembles (DE). However, all these\nmethods often rely on a single expert's annotations as ground truth for model\ntraining, overlooking the inter-rater variability in healthcare. To address\nthis issue, we propose MEGAN, a Multi-Expert Gating Network that aggregates\nuncertainty estimates and predictions from multiple AI experts via EDL models\ntrained with diverse ground truths and modeling strategies. MEGAN's gating\nnetwork optimally combines predictions and uncertainties from each EDL model,\nenhancing overall prediction confidence and calibration. We extensively\nbenchmark MEGAN on endoscopy videos for Ulcerative colitis (UC) disease\nseverity estimation, assessed by visual labeling of Mayo Endoscopic Subscore\n(MES), where inter-rater variability is prevalent. In large-scale prospective\nUC clinical trial, MEGAN achieved a 3.5% improvement in F1-score and a 30.5%\nreduction in Expected Calibration Error (ECE) compared to existing methods.\nFurthermore, MEGAN facilitated uncertainty-guided sample stratification,\nreducing the annotation burden and potentially increasing efficiency and\nconsistency in UC trials.", "AI": {"tldr": "该论文提出了MEGAN，一个多专家门控网络，通过聚合来自多个EDL模型的预测和不确定性估计来解决医学AI中由于专家间差异导致的单一标注问题，从而提高预测置信度和校准度，并在溃疡性结肠炎严重程度估计中取得了显著改进。", "motivation": "传统的AI不确定性量化（UQ）方法（如MC Dropout和Deep Ensembles）以及Evidential Deep Learning（EDL）通常依赖于单一专家的标注作为真值进行模型训练，忽略了医疗领域中普遍存在的专家间差异性。", "method": "该研究提出了MEGAN（Multi-Expert Gating Network），它通过聚合来自多个AI专家（EDL模型）的不确定性估计和预测来解决问题。这些EDL模型使用不同的真值和建模策略进行训练。MEGAN的门控网络能够优化结合每个EDL模型的预测和不确定性，从而增强整体预测的置信度和校准度。", "result": "在溃疡性结肠炎（UC）疾病严重程度估计的内窥镜视频基准测试中，MEGAN相比现有方法F1分数提高了3.5%，预期校准误差（ECE）降低了30.5%。此外，MEGAN还促进了不确定性引导的样本分层，减少了标注负担，并可能提高UC试验的效率和一致性。", "conclusion": "MEGAN通过有效处理专家间差异性，显著提高了医学AI在不确定性量化方面的性能、预测置信度和校准度。它还通过不确定性引导的样本分层，为临床试验带来了潜在的效率和一致性提升。"}}
{"id": "2509.12508", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.12508", "abs": "https://arxiv.org/abs/2509.12508", "authors": ["Keyu An", "Yanni Chen", "Chong Deng", "Changfeng Gao", "Zhifu Gao", "Bo Gong", "Xiangang Li", "Yabin Li", "Xiang Lv", "Yunjie Ji", "Yiheng Jiang", "Bin Ma", "Haoneng Luo", "Chongjia Ni", "Zexu Pan", "Yiping Peng", "Zhendong Peng", "Peiyao Wang", "Hao Wang", "Wen Wang", "Wupeng Wang", "Biao Tian", "Zhentao Tan", "Nan Yang", "Bin Yuan", "Jieping Ye", "Jixing Yu", "Qinglin Zhang", "Kun Zou", "Han Zhao", "Shengkui Zhao", "Jingren Zhou"], "title": "FunAudio-ASR Technical Report", "comment": null, "summary": "In recent years, automatic speech recognition (ASR) has witnessed\ntransformative advancements driven by three complementary paradigms: data\nscaling, model size scaling, and deep integration with large language models\n(LLMs). However, LLMs are prone to hallucination, which can significantly\ndegrade user experience in real-world ASR applications. In this paper, we\npresent FunAudio-ASR, a large-scale, LLM-based ASR system that synergistically\ncombines massive data, large model capacity, LLM integration, and reinforcement\nlearning to achieve state-of-the-art performance across diverse and complex\nspeech recognition scenarios. Moreover, FunAudio-ASR is specifically optimized\nfor practical deployment, with enhancements in streaming capability, noise\nrobustness, code-switching, hotword customization, and satisfying other\nreal-world application requirements. Experimental results show that while most\nLLM-based ASR systems achieve strong performance on open-source benchmarks,\nthey often underperform on real industry evaluation sets. Thanks to\nproduction-oriented optimizations, FunAudio-ASR achieves SOTA performance on\nreal application datasets, demonstrating its effectiveness and robustness in\npractical settings.", "AI": {"tldr": "本文提出了FunAudio-ASR，一个大规模、基于LLM的ASR系统，它结合了大数据、大模型、LLM集成和强化学习，并针对实际部署进行了优化，在真实应用数据上取得了最先进的性能。", "motivation": "尽管LLM驱动的ASR系统取得了显著进步，但它们容易产生幻觉，并且在开源基准测试上表现良好，但在真实的工业评估集上往往表现不佳，这会严重影响用户体验。", "method": "FunAudio-ASR通过协同结合海量数据、大模型容量、LLM集成和强化学习来实现最先进的性能。此外，它还针对实际部署进行了优化，包括流式处理能力、噪声鲁棒性、语码转换、热词定制等。", "result": "实验结果表明，FunAudio-ASR在真实应用数据集上取得了最先进（SOTA）的性能，证明了其在实际环境中的有效性和鲁棒性，而其他LLM-based ASR系统在工业评估集上常表现不佳。", "conclusion": "FunAudio-ASR通过其面向生产的优化和综合方法，在复杂多样的语音识别场景中展现了卓越的性能和鲁棒性，有效解决了LLM-based ASR在实际部署中的挑战。"}}
{"id": "2509.12468", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.12468", "abs": "https://arxiv.org/abs/2509.12468", "authors": ["Shipeng Liu", "Meghana Sagare", "Shubham Patil", "Feifei Qian"], "title": "Bio-inspired tail oscillation enables robot fast crawling on deformable granular terrains", "comment": null, "summary": "Deformable substrates such as sand and mud present significant challenges for\nterrestrial robots due to complex robot-terrain interactions. Inspired by\nmudskippers, amphibious animals that naturally adjust their tail morphology and\nmovement jointly to navigate such environments, we investigate how tail design\nand control can jointly enhance flipper-driven locomotion on granular media.\nUsing a bio-inspired robot modeled after the mudskipper, we experimentally\ncompared locomotion performance between idle and actively oscillating tail\nconfigurations. Tail oscillation increased robot speed by 67% and reduced body\ndrag by 46%. Shear force measurements revealed that this improvement was\nenabled by tail oscillation fluidizing the substrate, thereby reducing\nresistance. Additionally, tail morphology strongly influenced the oscillation\nstrategy: designs with larger horizontal surface areas leveraged the\noscillation-reduced shear resistance more effectively by limiting insertion\ndepth. Based on these findings, we present a design principle to inform tail\naction selection based on substrate strength and tail morphology. Our results\noffer new insights into tail design and control for improving robot locomotion\non deformable substrates, with implications for agricultural robotics, search\nand rescue, and environmental exploration.", "AI": {"tldr": "受弹涂鱼启发，研究发现机器人通过主动摆动尾巴能显著提高在颗粒介质上的移动速度，并通过流化基质来减少阻力，并提出了基于基质强度和尾巴形态的尾巴动作选择设计原则。", "motivation": "变形基质（如沙子和泥土）对陆地机器人构成重大挑战，因为机器人与地形之间的相互作用复杂。受弹涂鱼（一种能自然调整尾巴形态和运动以在这些环境中导航的两栖动物）的启发，本研究旨在探索尾巴设计和控制如何共同增强鳍驱动机器人在颗粒介质上的移动能力。", "method": "使用一个仿生弹涂鱼机器人，通过实验比较了闲置尾巴和主动摆动尾巴配置下的移动性能。同时进行了剪切力测量，以理解性能提升的机制。", "result": "尾巴摆动使机器人速度提高了67%，身体阻力降低了46%。剪切力测量显示，这种改进是通过尾巴摆动流化基质，从而减少阻力实现的。此外，尾巴形态强烈影响了摆动策略：具有较大水平表面积的设计通过限制插入深度，更有效地利用了摆动减小的剪切阻力。", "conclusion": "研究提出了一个设计原则，用于根据基质强度和尾巴形态来选择尾巴动作。这些结果为改进机器人在变形基质上的移动提供了新的尾巴设计和控制见解，对农业机器人、搜救和环境探索具有重要意义。"}}
{"id": "2509.12265", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12265", "abs": "https://arxiv.org/abs/2509.12265", "authors": ["Xiaoguang Chang", "Teng Wang", "Changyin Sun"], "title": "A Modern Look at Simplicity Bias in Image Classification Tasks", "comment": null, "summary": "The simplicity Bias (SB) of neural networks, i.e.\\ their tendency to\nrepresent simple functions, is a key factor in their generalization\ncapabilities. Recent studies show that an excessive SB may harm performance on\ncomplex tasks, and the need for this bias varies across tasks. Many of these\nstudies focus on simple models or synthetic tasks. It remains challenging to\nmeasure the SB in large models and little is known about the relevance of the\nSB to various image classification tasks.\n  In this paper, we investigate the relationship between the SB in CLIP models\nand their performance across image classification tasks. First, we\ntheoretically analyze the potential limitation of existing measures of\ncomplexity that have been used to characterize small models. To address this,\nwe propose a frequency-aware measure capturing finer-grained SB differences. We\nvalidate this measure on CLIP models subjected to two recent SB-modulation\nmethods, demonstrating that it is more informative and consistent than previous\nmeasures. Second, we examine the relation between the SB of those models and\ntheir performance across a range of image classification tasks, including\nzero-shot and fine-tuning settings. These experiments reveal a range of\nbehaviors. For example, a stronger SB correlates with a better performance on\nOOD generalization than on adversarial robustness. These results highlight the\nbenefits of aligning a model's inductive biases with the characteristics of the\ntarget task.", "AI": {"tldr": "本文研究了CLIP模型中的“简单性偏差”（SB）及其在各种图像分类任务中的表现。作者提出了一种新的频率感知度量方法来衡量SB，并发现SB的强度与不同任务（如OOD泛化和对抗鲁棒性）的性能表现出不同的相关性，强调了模型归纳偏置与任务特性对齐的重要性。", "motivation": "现有研究大多集中在简单模型或合成任务上，难以在大模型中衡量简单性偏差（SB），且对SB在各种图像分类任务中的作用知之甚少。然而，SB是神经网络泛化能力的关键因素，过度的SB可能损害复杂任务的性能，且所需偏差因任务而异。", "method": "首先，理论分析了现有用于小模型的复杂性度量方法的局限性。其次，提出了一种新的“频率感知度量”方法，以捕捉更细粒度的SB差异。该方法在经过两种SB调制方法的CLIP模型上进行了验证，并与现有度量进行了比较。最后，研究了这些模型的SB与一系列图像分类任务（包括零样本和微调设置）性能之间的关系。", "result": "提出的频率感知度量方法比现有方法更具信息量和一致性。实验揭示了SB与模型性能之间存在一系列不同的行为，例如，更强的SB与更好的OOD泛化能力相关，但与对抗鲁棒性则呈负相关。", "conclusion": "研究结果强调了将模型的归纳偏置（如简单性偏差）与目标任务的特性对齐的重要性，这有助于提升模型性能。"}}
{"id": "2509.12617", "categories": ["eess.SY", "cs.CY", "cs.NI", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.12617", "abs": "https://arxiv.org/abs/2509.12617", "authors": ["Srijesh Pillai", "M. I. Jawid Nazir"], "title": "CattleSense -- A Multisensory Approach to Optimize Cattle Well-Being", "comment": "5 pages, 9 figures. Author's accepted manuscript of a paper published\n  in the 2024 ASET Conference. The final version is available at:\n  https://doi.org/10.1109/ASET60340.2024.10708764", "summary": "CattleSense is an innovative application of Internet of Things (IoT)\ntechnology for the comprehensive monitoring and management of cattle\nwell-being. This research paper outlines the design and implementation of a\nsophisticated system using a Raspberry Pi Module 4B, RFID Card Reader, Electret\nArduino Microphone Module, DHT11 Sensor, Arduino UNO, Neo-6M GPS Sensor, and\nHeartbeat Sensor. The system aims to provide real-time surveillance of the\nenvironment in which Cows are present and individual Cow parameters such as\nlocation, milking frequency, and heartbeat fluctuations. The primary objective\nis to simplify managing the Cattle in the shed, ensuring that the Cattle are\nhealthy and safe.", "AI": {"tldr": "CattleSense是一个基于物联网的系统，旨在通过实时监测牛只的环境和个体参数（如位置、挤奶频率、心跳）来全面管理牛只的健康和安全。", "motivation": "研究的动机是简化牛棚中的牛只管理，确保牛只的健康和安全。", "method": "系统设计和实现采用了多种硬件组件，包括Raspberry Pi Module 4B、RFID读卡器、驻极体Arduino麦克风模块、DHT11传感器、Arduino UNO、Neo-6M GPS传感器和心跳传感器。", "result": "该系统能够提供牛只所在环境的实时监控，以及牛只个体参数（如位置、挤奶频率和心跳波动）的实时数据。", "conclusion": "CattleSense系统的主要目标是简化牛棚中的牛只管理，确保牛只的健康和安全。"}}
{"id": "2509.12423", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.12423", "abs": "https://arxiv.org/abs/2509.12423", "authors": ["Danielle Cohen", "Yoni Halpern", "Noam Kahlon", "Joel Oren", "Omri Berkovitch", "Sapir Caduri", "Ido Dagan", "Anatoly Efros"], "title": "Small Models, Big Results: Achieving Superior Intent Extraction through Decomposition", "comment": null, "summary": "Understanding user intents from UI interaction trajectories remains a\nchallenging, yet crucial, frontier in intelligent agent development. While\nmassive, datacenter-based, multi-modal large language models (MLLMs) possess\ngreater capacity to handle the complexities of such sequences, smaller models\nwhich can run on-device to provide a privacy-preserving, low-cost, and\nlow-latency user experience, struggle with accurate intent inference. We\naddress these limitations by introducing a novel decomposed approach: first, we\nperform structured interaction summarization, capturing key information from\neach user action. Second, we perform intent extraction using a fine-tuned model\noperating on the aggregated summaries. This method improves intent\nunderstanding in resource-constrained models, even surpassing the base\nperformance of large MLLMs.", "AI": {"tldr": "本文提出了一种新颖的分解方法，通过结构化交互摘要和意图提取，显著提高了资源受限模型理解用户意图的能力，甚至超越了大型多模态语言模型的基础性能。", "motivation": "理解用户界面交互轨迹中的用户意图对于智能代理开发至关重要，但对需要隐私保护、低成本、低延迟的设备端小型模型来说，准确推断意图仍面临挑战，因为它们难以处理复杂序列。", "method": "该研究采用了一种分解方法：首先，进行结构化交互摘要，从每个用户动作中捕获关键信息；其次，使用一个在聚合摘要上运行的微调模型进行意图提取。", "result": "该方法显著改善了资源受限模型中的意图理解能力，甚至超越了大型多模态语言模型（MLLMs）的基础性能。", "conclusion": "所提出的分解方法为设备端模型提供了一种有效且高性能的解决方案，以克服在隐私保护、低成本和低延迟场景下准确理解用户意图的挑战。"}}
{"id": "2509.13255", "categories": ["cs.CV", "cs.AI", "cs.IR", "eess.IV"], "pdf": "https://arxiv.org/pdf/2509.13255", "abs": "https://arxiv.org/abs/2509.13255", "authors": ["Mattia Soldan", "Fabian Caba Heilbron", "Bernard Ghanem", "Josef Sivic", "Bryan Russell"], "title": "ResidualViT for Efficient Temporally Dense Video Encoding", "comment": null, "summary": "Several video understanding tasks, such as natural language temporal video\ngrounding, temporal activity localization, and audio description generation,\nrequire \"temporally dense\" reasoning over frames sampled at high temporal\nresolution. However, computing frame-level features for these tasks is\ncomputationally expensive given the temporal resolution requirements. In this\npaper, we make three contributions to reduce the cost of computing features for\ntemporally dense tasks. First, we introduce a vision transformer (ViT)\narchitecture, dubbed ResidualViT, that leverages the large temporal redundancy\nin videos to efficiently compute temporally dense frame-level features. Our\narchitecture incorporates (i) learnable residual connections that ensure\ntemporal consistency across consecutive frames and (ii) a token reduction\nmodule that enhances processing speed by selectively discarding temporally\nredundant information while reusing weights of a pretrained foundation model.\nSecond, we propose a lightweight distillation strategy to approximate the\nframe-level features of the original foundation model. Finally, we evaluate our\napproach across four tasks and five datasets, in both zero-shot and fully\nsupervised settings, demonstrating significant reductions in computational cost\n(up to 60%) and improvements in inference speed (up to 2.5x faster), all while\nclosely approximating the accuracy of the original foundation model.", "AI": {"tldr": "本文提出ResidualViT架构和轻量级蒸馏策略，旨在高效计算视频中时间密集型任务的帧级特征，显著降低计算成本并提高推理速度，同时保持与原始基础模型相近的准确性。", "motivation": "视频理解中的多项任务（如自然语言时间视频定位、时间活动定位和音频描述生成）需要对高时间分辨率的帧进行“时间密集型”推理。然而，计算这些任务所需的帧级特征在计算上成本高昂。", "method": "1. 引入ResidualViT架构：一种Vision Transformer，利用视频中大量的时间冗余来高效计算时间密集型帧级特征。该架构包含可学习的残差连接以确保连续帧之间的时间一致性，以及一个令牌减少模块通过选择性丢弃时间冗余信息并重用预训练基础模型的权重来提高处理速度。2. 提出一种轻量级蒸馏策略，用于近似原始基础模型的帧级特征。", "result": "在四项任务和五个数据集上（包括零样本和全监督设置）评估了该方法，结果显示计算成本显著降低（高达60%），推理速度提高（快达2.5倍），同时准确性与原始基础模型非常接近。", "conclusion": "所提出的ResidualViT架构和轻量级蒸馏策略能够有效降低时间密集型视频任务中帧级特征计算的成本，提高推理速度，且在保持高准确性的前提下实现这些改进。"}}
{"id": "2509.12514", "categories": ["cs.CL", "cs.CE", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.12514", "abs": "https://arxiv.org/abs/2509.12514", "authors": ["Chiara Bonfanti", "Michele Colombino", "Giulia Coucourde", "Faeze Memari", "Stefano Pinardi", "Rosa Meo"], "title": "A comparison of pipelines for the translation of a low resource language based on transformers", "comment": "9 pages, 4 figures", "summary": "This work compares three pipelines for training transformer-based neural\nnetworks to produce machine translators for Bambara, a Mand\\`e language spoken\nin Africa by about 14,188,850 people. The first pipeline trains a simple\ntransformer to translate sentences from French into Bambara. The second\nfine-tunes LLaMA3 (3B-8B) instructor models using decoder-only architectures\nfor French-to-Bambara translation. Models from the first two pipelines were\ntrained with different hyperparameter combinations to improve BLEU and chrF\nscores, evaluated on both test sentences and official Bambara benchmarks. The\nthird pipeline uses language distillation with a student-teacher dual neural\nnetwork to integrate Bambara into a pre-trained LaBSE model, which provides\nlanguage-agnostic embeddings. A BERT extension is then applied to LaBSE to\ngenerate translations. All pipelines were tested on Dokotoro (medical) and\nBayelemagaba (mixed domains). Results show that the first pipeline, although\nsimpler, achieves the best translation accuracy (10% BLEU, 21% chrF on\nBayelemagaba), consistent with low-resource translation results. On the Yiri\ndataset, created for this work, it achieves 33.81% BLEU and 41% chrF.\nInstructor-based models perform better on single datasets than on aggregated\ncollections, suggesting they capture dataset-specific patterns more\neffectively.", "AI": {"tldr": "本研究比较了三种用于法语-班巴拉语机器翻译的Transformer神经网络训练流程，发现尽管结构更简单，但直接训练的Transformer模型在低资源语言翻译中表现最佳。", "motivation": "旨在改进班巴拉语（一种非洲低资源语言）的机器翻译质量，并比较不同Transformer训练方法（包括LLaMA3微调和语言蒸馏）在该任务上的效果。", "method": "1. 简单Transformer模型直接训练法语-班巴拉语翻译。2. 微调LLaMA3 (3B-8B) 指导模型，采用仅解码器架构。3. 使用语言蒸馏将班巴拉语整合到预训练的LaBSE模型中，并应用BERT扩展进行翻译。所有模型均使用BLEU和chrF分数进行评估，并在Dokotoro、Bayelemagaba和Yiri数据集上进行测试。", "result": "结果显示，第一个更简单的Transformer流程实现了最佳翻译准确性（Bayelemagaba数据集上BLEU 10%，chrF 21%；Yiri数据集上BLEU 33.81%，chrF 41%）。基于指导者的模型在单一数据集上的表现优于聚合数据集，表明它们更有效地捕捉数据集特有的模式。", "conclusion": "对于低资源的法语-班巴拉语翻译任务，直接训练的简单Transformer模型表现最佳，这与低资源翻译的普遍发现一致。基于指导者的模型在特定数据集上表现出优势，可能更适合捕获数据集的特定特征。"}}
{"id": "2509.12507", "categories": ["cs.RO", "cs.HC", "cs.LG", "68T07, 68T40", "I.2.9; I.2.6"], "pdf": "https://arxiv.org/pdf/2509.12507", "abs": "https://arxiv.org/abs/2509.12507", "authors": ["Anna Deichler", "Siyang Wang", "Simon Alexanderson", "Jonas Beskow"], "title": "Learning to Generate Pointing Gestures in Situated Embodied Conversational Agents", "comment": "DOI: 10.3389/frobt.2023.1110534. This is the author's LaTeX version", "summary": "One of the main goals of robotics and intelligent agent research is to enable\nnatural communication with humans in physically situated settings. While recent\nwork has focused on verbal modes such as language and speech, non-verbal\ncommunication is crucial for flexible interaction. We present a framework for\ngenerating pointing gestures in embodied agents by combining imitation and\nreinforcement learning. Using a small motion capture dataset, our method learns\na motor control policy that produces physically valid, naturalistic gestures\nwith high referential accuracy. We evaluate the approach against supervised\nlearning and retrieval baselines in both objective metrics and a virtual\nreality referential game with human users. Results show that our system\nachieves higher naturalness and accuracy than state-of-the-art supervised\nmodels, highlighting the promise of imitation-RL for communicative gesture\ngeneration and its potential application to robots.", "AI": {"tldr": "本文提出了一种结合模仿学习和强化学习的框架，用于在具身智能体中生成自然且准确的指点手势，并在自然度和准确性方面优于监督学习模型。", "motivation": "机器人和智能体研究的主要目标是实现与人类的自然交流。虽然近期工作关注语言和语音等口头交流模式，但非口头交流（如指点手势）对于灵活互动至关重要。", "method": "该研究结合了模仿学习和强化学习来生成指点手势。利用小型动作捕捉数据集学习运动控制策略，以生成物理有效、自然且指代准确的手势。通过客观指标和与人类用户进行的虚拟现实指代游戏进行评估。", "result": "结果表明，与最先进的监督学习模型相比，该系统在自然度和准确性方面表现更优。", "conclusion": "模仿学习与强化学习结合的方法在生成交流手势方面前景广阔，并有望应用于机器人。"}}
{"id": "2509.12277", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12277", "abs": "https://arxiv.org/abs/2509.12277", "authors": ["Mehdi Yousefzadeh", "Parsa Esfahanian", "Sara Rashidifar", "Hossein Salahshoor Gavalan", "Negar Sadat Rafiee Tabatabaee", "Saeid Gorgin", "Dara Rahmati", "Maryam Daneshpazhooh"], "title": "GraphDerm: Fusing Imaging, Physical Scale, and Metadata in a Population-Graph Classifier for Dermoscopic Lesions", "comment": null, "summary": "Introduction. Dermoscopy aids melanoma triage, yet image-only AI often\nignores patient metadata (age, sex, site) and the physical scale needed for\ngeometric analysis. We present GraphDerm, a population-graph framework that\nfuses imaging, millimeter-scale calibration, and metadata for multiclass\ndermoscopic classification, to the best of our knowledge the first ISIC-scale\napplication of GNNs to dermoscopy. Methods. We curate ISIC 2018/2019,\nsynthesize ruler-embedded images with exact masks, and train U-Nets\n(SE-ResNet-18) for lesion and ruler segmentation. Pixels-per-millimeter are\nregressed from the ruler-mask two-point correlation via a lightweight 1D-CNN.\nFrom lesion masks we compute real-scale descriptors (area, perimeter, radius of\ngyration). Node features use EfficientNet-B3; edges encode metadata/geometry\nsimilarity (fully weighted or thresholded). A spectral GNN performs\nsemi-supervised node classification; an image-only ANN is the baseline.\nResults. Ruler and lesion segmentation reach Dice 0.904 and 0.908; scale\nregression attains MAE 1.5 px (RMSE 6.6). The graph attains AUC 0.9812, with a\nthresholded variant using about 25% of edges preserving AUC 0.9788 (vs. 0.9440\nfor the image-only baseline); per-class AUCs typically fall in the 0.97-0.99\nrange. Conclusion. Unifying calibrated scale, lesion geometry, and metadata in\na population graph yields substantial gains over image-only pipelines on\nISIC-2019. Sparser graphs retain near-optimal accuracy, suggesting efficient\ndeployment. Scale-aware, graph-based AI is a promising direction for\ndermoscopic decision support; future work will refine learned edge semantics\nand evaluate on broader curated benchmarks.", "AI": {"tldr": "GraphDerm是一个将图像、毫米级校准和患者元数据融合到人群图框架中的皮肤镜多类别分类系统，它利用图神经网络（GNNs）显著优于纯图像基线方法。", "motivation": "皮肤镜AI通常忽略了患者元数据（年龄、性别、部位）和几何分析所需的物理尺度信息，而这些信息对提高诊断准确性至关重要。", "method": "研究人员整理了ISIC 2018/2019数据集，合成了带标尺的图像，并训练U-Net（SE-ResNet-18）进行病灶和标尺分割。通过轻量级1D-CNN从标尺掩码中回归每毫米像素数，并计算真实尺度的病灶描述符。节点特征使用EfficientNet-B3提取，边编码元数据/几何相似性。最终使用谱GNN进行半监督节点分类，并以纯图像ANN作为基线。", "result": "标尺和病灶分割的Dice系数分别达到0.904和0.908；尺度回归的平均绝对误差（MAE）为1.5像素。GraphDerm的AUC达到0.9812（稀疏图版本为0.9788），远高于纯图像基线的0.9440。各类别AUC通常在0.97-0.99范围内。稀疏图也能保持接近最优的准确性。", "conclusion": "在人群图中统一校准尺度、病灶几何形状和元数据，在ISIC-2019数据集上比纯图像管道取得了显著的性能提升。稀疏图保持了接近最优的准确性，表明可以高效部署。这种尺度感知、基于图的AI为皮肤镜决策支持提供了有前景的方向。"}}
{"id": "2509.12681", "categories": ["eess.SY", "cs.SY", "93C10, 93C57, 93C62"], "pdf": "https://arxiv.org/pdf/2509.12681", "abs": "https://arxiv.org/abs/2509.12681", "authors": ["Yutaka Yamamoto", "Kaoru Yamamoto"], "title": "Nonlinear Sampled-data Systems--A Lifting Framework", "comment": null, "summary": "This short note gives a new framework for dealing with nonlinear sampled-data\nsystems. We introduce a new idea of lifting, which is well known for linear\nsystems, but not successfully generalized to nonlinear systems. This paper\nintroduces a new lifting technique for nonlinear, time-invariant systems, which\nare different from the linear counterpart as developed in [Bamieh et al. 1991,\nYamamoto 1994], etc. The main difficulty is that the direct feedthrough term\neffective in the linear case cannot be generalized to the nonlinear case.\nInstead, we will further lift the state trajectory, and obtain an equivalent\ntime-invariant discrete-time system with function-space input and output\nspaces. The basic framework, as well as the closed-loop equation with a\ndiscrete-time controller, is given. As an application of this framework, we\ngive a representation for the Koopman operator derived from the given original\nnonlinear system.", "AI": {"tldr": "本文提出了一种处理非线性采样数据系统的新框架，通过引入一种新的“提升”技术，将非线性系统转换为具有函数空间输入输出的等效时不变离散时间系统。", "motivation": "线性系统中的“提升”技术已成熟，但未能成功推广到非线性系统。主要困难在于线性系统中的直接馈通项无法直接推广到非线性情况。", "method": "本文提出了一种针对非线性时不变系统的新型“提升”技术。与线性系统不同，该方法通过进一步提升状态轨迹，获得一个具有函数空间输入和输出的等效时不变离散时间系统。", "result": "给出了基本框架和带有离散时间控制器的闭环方程。作为该框架的应用，还提供了一种表示从原始非线性系统导出的Koopman算子表示方法。", "conclusion": "本文为处理非线性采样数据系统提供了一个新的通用框架，通过创新的提升技术克服了现有线性方法的局限性，并为非线性系统的分析和控制开辟了新的途径，尤其是在Koopman算子表示方面展示了其应用潜力。"}}
{"id": "2509.12434", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12434", "abs": "https://arxiv.org/abs/2509.12434", "authors": ["Jiahao Yu", "Zelei Cheng", "Xian Wu", "Xinyu Xing"], "title": "Building Coding Agents via Entropy-Enhanced Multi-Turn Preference Optimization", "comment": null, "summary": "Software engineering presents complex, multi-step challenges for Large\nLanguage Models (LLMs), requiring reasoning over large codebases and\ncoordinated tool use. The difficulty of these tasks is exemplified by\nbenchmarks like SWE-bench, where current LLMs still struggle to resolve\nreal-world issues.\n  A promising approach to enhance performance is test-time scaling (TTS), but\nits gains are heavily dependent on the diversity of model outputs.\n  While standard alignment methods such as Direct Preference Optimization (DPO)\nand Kahneman-Tversky Optimization (KTO) are effective at aligning model outputs\nwith human preferences, this process can come at the cost of reduced diversity,\nlimiting the effectiveness of TTS.\n  Additionally, existing preference optimization algorithms are typically\ndesigned for single-turn tasks and do not fully address the complexities of\nmulti-turn reasoning and tool integration required for interactive coding\nagents.\n  To bridge this gap, we introduce \\sys, an entropy-enhanced framework that\nadapts existing preference optimization algorithms to the multi-turn,\ntool-assisted setting.\n  \\sys augments the preference objective to explicitly preserve policy entropy\nand generalizes learning to optimize over multi-turn interactions rather than\nsingle-turn responses.\n  We validate \\sys by fine-tuning a diverse suite of models from different\nfamilies and sizes (up to 106B parameters).\n  To maximize performance gains from TTS, we further propose a hybrid\nbest-trajectory selection scheme combining a learned verifier model with model\nfree approaches.\n  On the \\swebench leaderboard, our approach establishes new state-of-the-art\nresults among open-weight models. A 30B parameter model trained with \\sys ranks\n1st on \\lite and 4th on \\verified on the open-weight leaderboard, surpassed\nonly by models with over 10x more parameters(\\eg$>$350B).", "AI": {"tldr": "本文提出了一种名为 \\sys 的熵增强框架，旨在改进大型语言模型（LLMs）在复杂软件工程任务上的表现。该框架通过增强偏好优化算法以保留策略多样性并适应多轮交互和工具使用，在 SWE-bench 基准测试中取得了开源模型的最新 SOTA 结果。", "motivation": "LLMs 在需要推理大型代码库和协调工具使用的复杂、多步软件工程任务（如 SWE-bench）上表现不佳。测试时扩展（TTS）是一种有前景的方法，但其效果严重依赖于模型输出的多样性。然而，现有的偏好优化方法（如 DPO、KTO）在使模型输出与人类偏好对齐的同时，可能会降低多样性，从而限制 TTS 的有效性。此外，现有算法主要针对单轮任务设计，未能充分解决交互式编码代理所需的多轮推理和工具集成的复杂性。", "method": "本文引入了 \\sys 框架，该框架将现有偏好优化算法调整为多轮、工具辅助设置。\\sys 通过明确保留策略熵来增强偏好目标，并将学习推广到优化多轮交互而非单轮响应。为了最大化 TTS 的性能增益，作者还提出了一种结合学习验证模型和无模型方法的混合最佳轨迹选择方案。该方法通过微调不同系列和规模（高达 106B 参数）的模型进行了验证。", "result": "在 SWE-bench 排行榜上，\\sys 方法在开源模型中取得了新的 SOTA 结果。一个使用 \\sys 训练的 30B 参数模型在 \\lite 上排名第一，在 \\verified 上排名第四，仅次于参数量超过 10 倍（例如 >350B）的模型。", "conclusion": "\\sys 框架通过在偏好优化中引入熵增强以保持多样性，并将其推广到多轮、工具辅助的交互中，成功解决了 LLMs 在复杂软件工程任务中面临的挑战。这使得开源模型在 SWE-bench 基准测试中取得了显著的性能提升，证明了该方法在提升 LLMs 解决真实世界软件问题能力方面的有效性。"}}
{"id": "2509.13289", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2509.13289", "abs": "https://arxiv.org/abs/2509.13289", "authors": ["Lovish Kaushik", "Agnij Biswas", "Somdyuti Paul"], "title": "Image Realness Assessment and Localization with Multimodal Features", "comment": null, "summary": "A reliable method of quantifying the perceptual realness of AI-generated\nimages and identifying visually inconsistent regions is crucial for practical\nuse of AI-generated images and for improving photorealism of generative AI via\nrealness feedback during training. This paper introduces a framework that\naccomplishes both overall objective realness assessment and local inconsistency\nidentification of AI-generated images using textual descriptions of visual\ninconsistencies generated by vision-language models trained on large datasets\nthat serve as reliable substitutes for human annotations. Our results\ndemonstrate that the proposed multimodal approach improves objective realness\nprediction performance and produces dense realness maps that effectively\ndistinguish between realistic and unrealistic spatial regions.", "AI": {"tldr": "本文提出一个框架，利用视觉-语言模型生成的文本描述，量化AI生成图像的整体真实感并识别局部视觉不一致区域，有效提升真实感预测性能并生成稠密真实感图。", "motivation": "量化AI生成图像的感知真实感并识别视觉不一致区域对于AI图像的实际应用至关重要，同时也能为生成式AI的训练提供真实感反馈，以提升其照片真实感。", "method": "该研究引入了一个多模态框架，利用在大型数据集上训练的视觉-语言模型（VLM）生成的视觉不一致性文本描述，来代替人工标注，实现AI生成图像的整体客观真实感评估和局部不一致性识别。", "result": "实验结果表明，所提出的多模态方法提高了客观真实感预测的性能，并生成了稠密的真实感图，能够有效区分图像中真实和不真实的区域。", "conclusion": "该多模态方法能有效量化AI生成图像的真实感并识别局部不一致性，为AI图像的实用性和照片真实感改进提供了可靠的工具。"}}
{"id": "2509.12591", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.12591", "abs": "https://arxiv.org/abs/2509.12591", "authors": ["Vijay Govindarajan", "Pratik Patel", "Sahil Tripathi", "Md Azizul Hoque", "Gautam Siddharth Kashyap"], "title": "MAGIC-Enhanced Keyword Prompting for Zero-Shot Audio Captioning with CLIP Models", "comment": "Accepted in The 26th International Conference on Web Information\n  Systems Engineering (WISE), scheduled for 15-17 December 2025 in Marrakech,\n  Morocco", "summary": "Automated Audio Captioning (AAC) generates captions for audio clips but faces\nchallenges due to limited datasets compared to image captioning. To overcome\nthis, we propose the zero-shot AAC system that leverages pre-trained models,\neliminating the need for extensive training. Our approach uses a pre-trained\naudio CLIP model to extract auditory features and generate a structured prompt,\nwhich guides a Large Language Model (LLM) in caption generation. Unlike\ntraditional greedy decoding, our method refines token selection through the\naudio CLIP model, ensuring alignment with the audio content. Experimental\nresults demonstrate a 35% improvement in NLG mean score (from 4.7 to 7.3) using\nMAGIC search with the WavCaps model. The performance is heavily influenced by\nthe audio-text matching model and keyword selection, with optimal results\nachieved using a single keyword prompt, and a 50% performance drop when no\nkeyword list is used.", "AI": {"tldr": "本文提出了一种零样本自动音频字幕 (AAC) 系统，利用预训练的音频 CLIP 模型提取特征并引导大型语言模型 (LLM) 生成字幕。该系统通过音频 CLIP 模型精炼 token 选择，实现了显著的性能提升，特别是在使用 MAGIC 搜索和关键词提示时。", "motivation": "自动音频字幕 (AAC) 面临数据集有限的挑战，远不及图像字幕领域。为了克服这一限制，研究旨在开发一个无需大量训练数据，利用预训练模型即可工作的零样本 AAC 系统。", "method": "该方法构建了一个零样本 AAC 系统，利用预训练的音频 CLIP 模型提取听觉特征并生成结构化提示。这些提示随后引导大型语言模型 (LLM) 生成字幕。与传统的贪婪解码不同，该系统通过音频 CLIP 模型精炼 token 选择，以确保与音频内容的对齐。", "result": "实验结果表明，使用 WavCaps 模型进行 MAGIC 搜索时，NLG 平均得分提高了 35%（从 4.7 提高到 7.3）。性能受音频-文本匹配模型和关键词选择的严重影响，使用单个关键词提示可获得最佳结果，而未使用关键词列表时性能下降 50%。", "conclusion": "所提出的零样本 AAC 系统通过利用预训练模型有效解决了数据限制问题。音频-文本匹配模型和关键词选择对系统性能至关重要，MAGIC 搜索策略显著提高了字幕生成的质量和与音频内容的对齐度。"}}
{"id": "2509.12516", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.12516", "abs": "https://arxiv.org/abs/2509.12516", "authors": ["William Ward", "Sarah Etter", "Jesse Quattrociocchi", "Christian Ellis", "Adam J. Thorpe", "Ufuk Topcu"], "title": "Zero to Autonomy in Real-Time: Online Adaptation of Dynamics in Unstructured Environments", "comment": "Submitted to ICRA 2026", "summary": "Autonomous robots must go from zero prior knowledge to safe control within\nseconds to operate in unstructured environments. Abrupt terrain changes, such\nas a sudden transition to ice, create dynamics shifts that can destabilize\nplanners unless the model adapts in real-time. We present a method for online\nadaptation that combines function encoders with recursive least squares,\ntreating the function encoder coefficients as latent states updated from\nstreaming odometry. This yields constant-time coefficient estimation without\ngradient-based inner-loop updates, enabling adaptation from only a few seconds\nof data. We evaluate our approach on a Van der Pol system to highlight\nalgorithmic behavior, in a Unity simulator for high-fidelity off-road\nnavigation, and on a Clearpath Jackal robot, including on a challenging terrain\nat a local ice rink. Across these settings, our method improves model accuracy\nand downstream planning, reducing collisions compared to static and\nmeta-learning baselines.", "AI": {"tldr": "该论文提出了一种结合函数编码器和递归最小二乘的在线自适应方法，使自主机器人在几秒内从零先验知识实现安全控制，尤其是在地形突变（如冰面）时，能实时调整模型以提高规划准确性和安全性。", "motivation": "自主机器人在非结构化环境中运行时，需要从零先验知识迅速实现安全控制。地形的突然变化（如过渡到冰面）会导致动力学模型发生偏移，若模型不能实时自适应，则会使规划器不稳定。", "method": "该方法将函数编码器与递归最小二乘法相结合，将函数编码器系数视为潜在状态，并从流式里程计数据中进行更新。这种方法实现了常数时间系数估计，无需基于梯度的内循环更新，从而只需几秒数据即可实现自适应。", "result": "在Van der Pol系统、Unity模拟器和Clearpath Jackal机器人（包括冰面挑战地形）上的评估表明，该方法提高了模型准确性和下游规划效果，与静态和元学习基线相比，显著减少了碰撞。", "conclusion": "所提出的在线自适应方法能使自主机器人快速适应动态环境中的模型变化，显著提升了模型准确性和规划性能，从而在非结构化环境中实现更安全的控制，尤其适用于地形突变的情况。"}}
{"id": "2509.12278", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12278", "abs": "https://arxiv.org/abs/2509.12278", "authors": ["Wanru Zhuang", "Wenbo Li", "Zhibin Lan", "Xu Han", "Peng Li", "Jinsong Su"], "title": "PATIMT-Bench: A Multi-Scenario Benchmark for Position-Aware Text Image Machine Translation in Large Vision-Language Models", "comment": null, "summary": "Text Image Machine Translation (TIMT) aims to translate texts embedded within\nan image into another language. Current TIMT studies primarily focus on\nproviding translations for all the text within an image, while neglecting to\nprovide bounding boxes and covering limited scenarios. In this work, we extend\ntraditional TIMT into position-aware TIMT (PATIMT), aiming to support\nfine-grained and layoutpreserving translation, which holds great practical\nvalue but remains largely unexplored. This task comprises two key sub-tasks:\nregionspecific translation and full-image translation with grounding. To\nsupport existing models on PATIMT and conduct fair evaluation, we construct the\nPATIMT benchmark (PATIMTBench), which consists of 10 diverse real-world\nscenarios. Specifically, we introduce an Adaptive Image OCR Refinement\nPipeline, which adaptively selects appropriate OCR tools based on scenario and\nrefines the results of text-rich images. To ensure evaluation reliability, we\nfurther construct a test set, which contains 1,200 high-quality instances\nmanually annotated and reviewed by human experts. After fine-tuning on our\ndata, compact Large Vision-Language Models (LVLMs) achieve state-of-the-art\nperformance on both sub-tasks. Experimental results also highlight the\nscalability and generalizability of our training data", "AI": {"tldr": "本文将传统文本图像机器翻译（TIMT）扩展为位置感知TIMT（PATIMT），以实现细粒度和布局保留翻译。为此，作者构建了PATIMT基准（PATIMTBench），包含10种真实场景，并引入自适应OCR精炼流程。通过在高质量数据集上微调，紧凑型大视觉语言模型（LVLMs）在PATIMT的两个子任务上均达到了最先进的性能。", "motivation": "当前的文本图像机器翻译（TIMT）研究主要关注提供图像中所有文本的翻译，但忽略了提供文本的边界框，并且只覆盖了有限的场景。这导致了缺乏细粒度和布局保留的翻译，而这在实际应用中具有重要价值但尚未得到充分探索。", "method": "本文提出了位置感知TIMT（PATIMT）任务，旨在支持细粒度和布局保留的翻译，并将其分为区域特定翻译和带接地的全图像翻译两个子任务。为支持PATIMT并进行公平评估，作者构建了PATIMT基准（PATIMTBench），该基准包含10种多样化的真实世界场景。具体方法包括：引入自适应图像OCR精炼流程，根据场景自适应选择OCR工具并精炼文本丰富图像的结果；构建一个包含1200个由人工专家手动标注和审查的高质量测试集，以确保评估可靠性；最后，在所构建的数据集上微调紧凑型大视觉语言模型（LVLMs）。", "result": "经过数据微调后，紧凑型大视觉语言模型（LVLMs）在PATIMT的两个子任务上均取得了最先进的性能。实验结果还突出显示了训练数据的可扩展性和泛化能力。", "conclusion": "PATIMT是一个具有巨大实际价值但尚未充分探索的任务，本文通过定义PATIMT任务、构建全面的PATIMTBench基准和高质量测试集，并引入自适应OCR精炼流程，为该领域奠定了基础。紧凑型LVLMs在PATIMT上表现出色，证明了所构建数据和方法的有效性，并为未来的研究提供了有力的支持。"}}
{"id": "2509.12695", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.12695", "abs": "https://arxiv.org/abs/2509.12695", "authors": ["Taehun Kim", "Guntae Kim", "Cheolmin Jeong", "Chang Mook Kang"], "title": "MAPS: A Mode-Aware Probabilistic Scheduling Framework for LPV-Based Adaptive Control", "comment": null, "summary": "This paper proposes Mode-Aware Probabilistic Scheduling (MAPS), a novel\nadaptive control framework tailored for DC motor systems experiencing varying\nfriction. MAPS uniquely integrates an Interacting Multiple Model (IMM)\nestimator with a Linear Parameter-Varying (LPV) based control strategy,\nleveraging real-time mode probability estimates to perform probabilistic gain\nscheduling. A key innovation of MAPS lies in directly using the updated mode\nprobabilities as the interpolation weights for online gain synthesis in the LPV\ncontroller, thereby tightly coupling state estimation with adaptive control.\nThis seamless integration enables the controller to dynamically adapt control\ngains in real time, effectively responding to changes in frictional operating\nmodes without requiring explicit friction model identification. Validation on a\nHardware-in-the-Loop Simulation (HILS) environment demonstrates that MAPS\nsignificantly enhances both state estimation accuracy and reference tracking\nperformance compared to Linear Quadratic Regulator (LQR) controllers relying on\npredefined scheduling variables. These results establish MAPS as a robust,\ngeneralizable solution for friction-aware adaptive control in uncertain,\ntime-varying environments, with practical real-time applicability.", "AI": {"tldr": "本文提出了一种名为MAPS的自适应控制框架，它将交互式多模型（IMM）估计器与基于线性参数变化（LPV）的控制策略相结合，通过实时模式概率进行增益调度，以应对直流电机系统中的变摩擦问题。", "motivation": "直流电机系统常遇到变化的摩擦力，这使得传统控制难以维持高性能。研究旨在开发一种无需明确摩擦模型识别，能动态适应摩擦操作模式变化的自适应控制框架。", "method": "MAPS框架独特地将交互式多模型（IMM）估计器与基于线性参数变化（LPV）的控制策略相结合。它利用更新的模式概率作为LPV控制器中在线增益合成的插值权重，从而紧密耦合状态估计与自适应控制，实现控制增益的实时动态调整。", "result": "在硬件在环仿真（HILS）环境中的验证表明，与依赖预定义调度变量的线性二次调节器（LQR）控制器相比，MAPS显著提高了状态估计精度和参考跟踪性能。", "conclusion": "MAPS被确立为一种鲁棒、可泛化的解决方案，适用于不确定、时变环境中的摩擦感知自适应控制，并具有实际的实时应用潜力。"}}
{"id": "2509.12437", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12437", "abs": "https://arxiv.org/abs/2509.12437", "authors": ["Dingrui Wang", "Zhexiao Sun", "Zhouheng Li", "Cheng Wang", "Youlun Peng", "Hongyuan Ye", "Baha Zarrouki", "Wei Li", "Mattia Piccinini", "Lei Xie", "Johannes Betz"], "title": "Enhancing Physical Consistency in Lightweight World Models", "comment": "8 pages", "summary": "A major challenge in deploying world models is the trade-off between size and\nperformance. Large world models can capture rich physical dynamics but require\nmassive computing resources, making them impractical for edge devices. Small\nworld models are easier to deploy but often struggle to learn accurate physics,\nleading to poor predictions. We propose the Physics-Informed BEV World Model\n(PIWM), a compact model designed to efficiently capture physical interactions\nin bird's-eye-view (BEV) representations. PIWM uses Soft Mask during training\nto improve dynamic object modeling and future prediction. We also introduce a\nsimple yet effective technique, Warm Start, for inference to enhance prediction\nquality with a zero-shot model. Experiments show that at the same parameter\nscale (400M), PIWM surpasses the baseline by 60.6% in weighted overall score.\nMoreover, even when compared with the largest baseline model (400M), the\nsmallest PIWM (130M Soft Mask) achieves a 7.4% higher weighted overall score\nwith a 28% faster inference speed.", "AI": {"tldr": "本文提出了一种紧凑的物理信息BEV世界模型（PIWM），通过Soft Mask和Warm Start技术，在保持小尺寸的同时显著提升了预测性能和推理速度，解决了世界模型在边缘设备部署中的大小与性能权衡问题。", "motivation": "部署世界模型面临主要挑战：大型模型能捕捉丰富物理动态但计算资源消耗巨大，不适用于边缘设备；小型模型易于部署但难以准确学习物理，导致预测不佳。", "method": "本文提出了物理信息BEV世界模型（PIWM）。它在训练过程中使用Soft Mask来改进动态对象建模和未来预测。此外，还引入了一种简单有效的Warm Start技术用于推理，以零样本模型提高预测质量。", "result": "实验结果显示，在相同参数规模（400M）下，PIWM的加权总分比基线高出60.6%。即使与最大的基线模型（400M）相比，最小的PIWM（130M Soft Mask）也能实现7.4%更高的加权总分，且推理速度快28%。", "conclusion": "PIWM成功地解决了世界模型在尺寸和性能之间的权衡问题，通过紧凑的设计和创新的技术，在边缘设备上实现了卓越的物理交互捕捉能力和预测性能，同时保持了高效的推理速度。"}}
{"id": "2509.12603", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12603", "abs": "https://arxiv.org/abs/2509.12603", "authors": ["Mukai Li", "Linfeng Song", "Zhenwen Liang", "Jiahao Xu", "Shansan Gong", "Qi Liu", "Haitao Mi", "Dong Yu"], "title": "EconProver: Towards More Economical Test-Time Scaling for Automated Theorem Proving", "comment": null, "summary": "Large Language Models (LLMs) have recently advanced the field of Automated\nTheorem Proving (ATP), attaining substantial performance gains through widely\nadopted test-time scaling strategies, notably reflective Chain-of-Thought (CoT)\nreasoning and increased sampling passes. However, they both introduce\nsignificant computational overhead for inference. Moreover, existing cost\nanalyses typically regulate only the number of sampling passes, while\nneglecting the substantial disparities in sampling costs introduced by\ndifferent scaling strategies. In this paper, we systematically compare the\nefficiency of different test-time scaling strategies for ATP models and\ndemonstrate the inefficiency of the current state-of-the-art (SOTA) open-source\napproaches. We then investigate approaches to significantly reduce token usage\nand sample passes while maintaining the original performance. Specifically, we\npropose two complementary methods that can be integrated into a unified EconRL\npipeline for amplified benefits: (1) a dynamic Chain-of-Thought (CoT) switching\nmechanism designed to mitigate unnecessary token consumption, and (2) Diverse\nparallel-scaled reinforcement learning (RL) with trainable prefixes to enhance\npass rates under constrained sampling passes. Experiments on miniF2F and\nProofNet demonstrate that our EconProver achieves comparable performance to\nbaseline methods with only 12% of the computational cost. This work provides\nactionable insights for deploying lightweight ATP models without sacrificing\nperformance.", "AI": {"tldr": "大型语言模型（LLMs）在自动化定理证明（ATP）中表现出色，但其测试时扩展策略（如CoT和增加采样次数）导致高昂的计算成本。本文提出了EconProver，通过动态CoT切换和可训练前缀的RL方法，显著降低了计算成本，同时保持了原有性能。", "motivation": "LLMs在ATP中通过反射式思维链（CoT）推理和增加采样次数等测试时扩展策略取得了显著性能提升，但这引入了巨大的推理计算开销。此外，现有成本分析通常只关注采样次数，而忽略了不同扩展策略引入的采样成本差异。", "method": "本文首先系统地比较了ATP模型不同测试时扩展策略的效率，并指出了现有最先进开源方法的低效性。然后，提出了EconRL统一管道，包含两种互补方法：1) 动态CoT切换机制，旨在减少不必要的token消耗；2) 具有可训练前缀的多样化并行强化学习（RL），以在受限采样次数下提高通过率。", "result": "在miniF2F和ProofNet数据集上的实验表明，EconProver在保持与基线方法相当的性能的同时，计算成本仅为基线方法的12%。", "conclusion": "这项工作为部署轻量级ATP模型提供了可行的见解，且无需牺牲性能。"}}
{"id": "2509.12531", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.SY", "eess.SY", "68T07, 68T40 (Primary) 93C85, 62L20 (Secondary)", "I.2.6; I.2.9; I.4.8; F.2.2"], "pdf": "https://arxiv.org/pdf/2509.12531", "abs": "https://arxiv.org/abs/2509.12531", "authors": ["Scott Jones", "Liyou Zhou", "Sebastian W. Pattinson"], "title": "Pre-trained Visual Representations Generalize Where it Matters in Model-Based Reinforcement Learning", "comment": null, "summary": "In visuomotor policy learning, the control policy for the robotic agent is\nderived directly from visual inputs. The typical approach, where a policy and\nvision encoder are trained jointly from scratch, generalizes poorly to novel\nvisual scene changes. Using pre-trained vision models (PVMs) to inform a policy\nnetwork improves robustness in model-free reinforcement learning (MFRL). Recent\ndevelopments in Model-based reinforcement learning (MBRL) suggest that MBRL is\nmore sample-efficient than MFRL. However, counterintuitively, existing work has\nfound PVMs to be ineffective in MBRL. Here, we investigate PVM's effectiveness\nin MBRL, specifically on generalization under visual domain shifts. We show\nthat, in scenarios with severe shifts, PVMs perform much better than a baseline\nmodel trained from scratch. We further investigate the effects of varying\nlevels of fine-tuning of PVMs. Our results show that partial fine-tuning can\nmaintain the highest average task performance under the most extreme\ndistribution shifts. Our results demonstrate that PVMs are highly successful in\npromoting robustness in visual policy learning, providing compelling evidence\nfor their wider adoption in model-based robotic learning applications.", "AI": {"tldr": "本文研究了预训练视觉模型（PVMs）在基于模型的强化学习（MBRL）中对视觉泛化的有效性，发现PVMs在严重视觉域偏移下表现优异，且部分微调能保持最佳性能。", "motivation": "传统的视觉运动策略学习在视觉场景变化下泛化性差。预训练视觉模型（PVMs）在无模型强化学习（MFRL）中能提高鲁棒性，但现有研究发现PVMs在样本效率更高的基于模型的强化学习（MBRL）中却无效。本文旨在探究PVMs在MBRL中对视觉域偏移泛化的有效性。", "method": "研究了PVMs在MBRL中，特别是在视觉域偏移下的泛化能力。通过与从头训练的基线模型进行比较，并进一步探究了不同程度的PVMs微调（fine-tuning）对性能的影响。", "result": "在严重视觉域偏移场景下，PVMs的表现远优于从头训练的基线模型。研究还发现，部分微调（partial fine-tuning）的PVMs在最极端的分布偏移下能保持最高的平均任务性能。", "conclusion": "PVMs在促进视觉策略学习的鲁棒性方面非常成功，为它们在基于模型的机器人学习应用中更广泛的采用提供了有力证据。"}}
{"id": "2509.12279", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12279", "abs": "https://arxiv.org/abs/2509.12279", "authors": ["He Gao", "Baoxiang Huang", "Milena Radenkovic", "Borui Li", "Ge Chen"], "title": "Domain Adaptive SAR Wake Detection: Leveraging Similarity Filtering and Memory Guidance", "comment": null, "summary": "Synthetic Aperture Radar (SAR), with its all-weather and wide-area\nobservation capabilities, serves as a crucial tool for wake detection. However,\ndue to its complex imaging mechanism, wake features in SAR images often appear\nabstract and noisy, posing challenges for accurate annotation. In contrast,\noptical images provide more distinct visual cues, but models trained on optical\ndata suffer from performance degradation when applied to SAR images due to\ndomain shift. To address this cross-modal domain adaptation challenge, we\npropose a Similarity-Guided and Memory-Guided Domain Adaptation (termed\nSimMemDA) framework for unsupervised domain adaptive ship wake detection via\ninstance-level feature similarity filtering and feature memory guidance.\nSpecifically, to alleviate the visual discrepancy between optical and SAR\nimages, we first utilize WakeGAN to perform style transfer on optical images,\ngenerating pseudo-images close to the SAR style. Then, instance-level feature\nsimilarity filtering mechanism is designed to identify and prioritize source\nsamples with target-like distributions, minimizing negative transfer.\nMeanwhile, a Feature-Confidence Memory Bank combined with a K-nearest neighbor\nconfidence-weighted fusion strategy is introduced to dynamically calibrate\npseudo-labels in the target domain, improving the reliability and stability of\npseudo-labels. Finally, the framework further enhances generalization through\nregion-mixed training, strategically combining source annotations with\ncalibrated target pseudo-labels. Experimental results demonstrate that the\nproposed SimMemDA method can improve the accuracy and robustness of cross-modal\nship wake detection tasks, validating the effectiveness and feasibility of the\nproposed method.", "AI": {"tldr": "本文提出了一种名为SimMemDA的相似性引导和记忆引导域适应框架，用于解决合成孔径雷达（SAR）图像中船舶尾迹检测的跨模态域适应挑战，通过实例级特征相似性过滤和特征记忆指导来提高检测的准确性和鲁棒性。", "motivation": "SAR图像中的尾迹特征抽象且嘈杂，难以准确标注。而光学图像虽然提供清晰的视觉线索，但基于光学数据训练的模型在应用于SAR图像时会因域偏移而性能下降。因此，需要解决跨模态域适应问题以实现SAR图像的船舶尾迹检测。", "method": "该方法首先利用WakeGAN对光学图像进行风格迁移，生成接近SAR风格的伪图像。其次，设计了实例级特征相似性过滤机制来识别并优先处理具有目标域分布特征的源样本，以减少负迁移。同时，引入了结合K近邻置信度加权融合策略的特征-置信度记忆库，以动态校准目标域的伪标签。最后，通过区域混合训练，结合源域标注和校准后的目标域伪标签，进一步增强泛化能力。", "result": "实验结果表明，所提出的SimMemDA方法能够有效提高跨模态船舶尾迹检测任务的准确性和鲁棒性。", "conclusion": "SimMemDA方法在解决SAR图像船舶尾迹检测的跨模态域适应问题上是有效且可行的，显著提升了检测性能。"}}
{"id": "2509.12758", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.12758", "abs": "https://arxiv.org/abs/2509.12758", "authors": ["Ping Zhang", "Xiaodong Xu", "Mengying Sun", "Haixiao Gao", "Nan Ma", "Xiaoyun Wang", "Ruichen Zhang", "Jiacheng Wang", "Dusit Niyato"], "title": "Towards Native AI in 6G Standardization: The Roadmap of Semantic Communication", "comment": null, "summary": "Semantic communication (SemCom) has emerged as a transformative paradigm for\nfuture 6G networks, offering task-oriented and meaning-aware transmission that\nfundamentally redefines traditional bit-centric design. Recognized by leading\nstandardization bodies including the institute of electrical and electronics\nengineers (IEEE) and the international telecommunication union (ITU), and\nactively discussed within the 3rd generation partnership project (3GPP) working\ngroups, SemCom is rapidly gaining traction as a foundational enabler for\nnative-AI 6G. This paper presents a comprehensive overview of recent progress\nin SemCom from both academic and industrial perspectives, with a focus on its\nongoing and upcoming standardization activities. We systematically examine\nadvances in representative application scenarios, architectural design,\nsemantic-traditional system compatibility, unified evaluation metrics, and\nvalidation methodologies. Furthermore, we highlight several key enabling\ntechnologies, such as joint source-channel coding (JSCC), SemCom-based multiple\naccess (MA) technologies such as model division MA (MDMA), and semantic\nknowledge base (KB), that support the practical implementation of SemCom in\nstandard-compliant systems. Additionally, we present a case study for channel\nstate information (CSI) feedback, illustrating the concrete performance gains\nof SemCom under 3GPP-compliant fading channels. Finally, we discuss emerging\nchallenges and research opportunities for incorporating semantic-native\nmechanisms into the evolving 6G standardization landscape, and provide\nforward-looking insights into its development and global adoption.", "AI": {"tldr": "本文全面概述了语义通信（SemCom）在6G网络中的最新进展、标准化活动、关键使能技术，并讨论了其面临的挑战和未来发展。", "motivation": "语义通信（SemCom）作为一种变革性范式，为未来6G网络提供了面向任务和意义感知的传输，重新定义了传统的以比特为中心的设计，并被领先的标准化机构（如IEEE、ITU、3GPP）广泛认可和讨论。", "method": "本文通过学术界和工业界的视角，全面回顾了SemCom的最新进展，重点关注其标准化活动。具体方法包括：系统性地审查代表性应用场景、架构设计、语义-传统系统兼容性、统一评估指标和验证方法；强调联合源信道编码（JSCC）、模型分割多址（MDMA）和语义知识库（KB）等关键使能技术；并通过信道状态信息（CSI）反馈的案例研究，展示了在符合3GPP标准的衰落信道下的具体性能增益。", "result": "SemCom在符合3GPP标准的衰落信道下，如CSI反馈场景，展现了具体的性能增益。论文还识别了支持SemCom在符合标准的系统中实际实现的关键使能技术，并讨论了将其融入6G标准化中的新兴挑战和研究机会。", "conclusion": "SemCom正迅速成为原生AI 6G的基础性使能技术。本文探讨了将语义原生机制纳入不断发展的6G标准化进程中的挑战和机遇，并对其发展和全球采纳提供了前瞻性见解。"}}
{"id": "2509.12464", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12464", "abs": "https://arxiv.org/abs/2509.12464", "authors": ["Ryan Lucas", "Kayhan Behdin", "Zhipeng Wang", "Qingquan Song", "Shao Tang", "Rahul Mazumder"], "title": "Reasoning Models Can be Accurately Pruned Via Chain-of-Thought Reconstruction", "comment": null, "summary": "Reasoning language models such as DeepSeek-R1 produce long chain-of-thought\ntraces during inference time which make them costly to deploy at scale. We show\nthat using compression techniques such as neural network pruning produces\ngreater performance loss than in typical language modeling tasks, and in some\ncases can make the model slower since they cause the model to produce more\nthinking tokens but with worse performance. We show that this is partly due to\nthe fact that standard LLM pruning methods often focus on input reconstruction,\nwhereas reasoning is a decode-dominated task. We introduce a simple, drop-in\nfix: during pruning we jointly reconstruct activations from the input and the\nmodel's on-policy chain-of-thought traces. This \"Reasoning-Aware Compression\"\n(RAC) integrates seamlessly into existing pruning workflows such as SparseGPT,\nand boosts their performance significantly. Code reproducing the results in the\npaper can be found at: https://github.com/RyanLucas3/RAC", "AI": {"tldr": "推理语言模型因长链式思考轨迹部署成本高。传统压缩方法效果差甚至更慢。本文提出“推理感知压缩”（RAC），通过联合重构输入和链式思考轨迹的激活来改进剪枝，显著提升性能。", "motivation": "DeepSeek-R1等推理语言模型在推理时产生冗长的链式思考（CoT）轨迹，导致部署成本高昂。传统的神经网络剪枝等压缩技术在推理任务上表现不佳，甚至可能使模型变慢，因为它们导致模型生成更多思考token但性能更差。这部分原因是标准LLM剪枝方法侧重于输入重构，而推理是解码主导的任务。", "method": "本文提出了一种名为“推理感知压缩”（Reasoning-Aware Compression, RAC）的简单即插即用修复方案。RAC在剪枝过程中联合重构来自输入和模型在策略链式思考轨迹的激活。该方法可以无缝集成到现有的剪枝工作流中，例如SparseGPT。", "result": "传统压缩技术（如剪枝）在推理语言模型上会导致比典型语言建模任务更大的性能损失，甚至可能使模型变慢，因为它导致模型生成更多思考token但性能更差。而RAC显著提升了现有剪枝工作流（如SparseGPT）的性能。", "conclusion": "推理感知压缩（RAC）通过在剪枝过程中联合重构输入和模型链式思考轨迹的激活，有效解决了推理语言模型部署成本高的问题，并显著提高了压缩性能，表明在推理任务中考虑解码过程中的激活重构对于有效的模型压缩至关重要。"}}
{"id": "2509.12635", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12635", "abs": "https://arxiv.org/abs/2509.12635", "authors": ["Yu", "Wang", "Sheng Shen", "Rémi Munos", "Hongyuan Zhan", "Yuandong Tian"], "title": "Positional Encoding via Token-Aware Phase Attention", "comment": "21 pages", "summary": "We prove under practical assumptions that Rotary Positional Embedding (RoPE)\nintroduces an intrinsic distance-dependent bias in attention scores that limits\nRoPE's ability to model long-context. RoPE extension methods may alleviate this\nissue, but they typically require post-hoc adjustments after pretraining, such\nas rescaling or hyperparameters retuning. This paper introduces Token-Aware\nPhase Attention (TAPA), a new positional encoding method that incorporates a\nlearnable phase function into the attention mechanism. TAPA preserves token\ninteractions over long range, extends to longer contexts with direct and light\nfine-tuning, extrapolates to unseen lengths, and attains significantly lower\nperplexity on long-context than RoPE families.", "AI": {"tldr": "研究发现RoPE在长文本中存在距离依赖偏差，限制了其性能。本文提出TAPA，一种新的位置编码方法，通过可学习的相位函数解决了此问题，在长文本建模和泛化能力上优于RoPE。", "motivation": "RoPE（旋转位置嵌入）在注意力分数中引入了固有的距离依赖偏差，限制了其建模长文本的能力。现有的RoPE扩展方法通常需要在预训练后进行额外调整（如重缩放或超参数调优）。", "method": "本文提出Token-Aware Phase Attention (TAPA)，一种新的位置编码方法。TAPA将一个可学习的相位函数整合到注意力机制中。", "result": "TAPA能够保持长距离的token交互，通过直接且轻量的微调扩展到更长的上下文，能够外推到未见的长度，并且在长文本上比RoPE系列模型取得了显著更低的困惑度。", "conclusion": "TAPA通过引入可学习的相位函数，有效解决了RoPE在长文本建模中的局限性，实现了更好的长距离交互、上下文扩展和外推能力，并在长文本性能上超越了RoPE家族。"}}
{"id": "2509.12562", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.12562", "abs": "https://arxiv.org/abs/2509.12562", "authors": ["Zhefei Gong", "Shangke Lyu", "Pengxiang Ding", "Wei Xiao", "Donglin Wang"], "title": "Robust Online Residual Refinement via Koopman-Guided Dynamics Modeling", "comment": null, "summary": "Imitation learning (IL) enables efficient skill acquisition from\ndemonstrations but often struggles with long-horizon tasks and high-precision\ncontrol due to compounding errors. Residual policy learning offers a promising,\nmodel-agnostic solution by refining a base policy through closed-loop\ncorrections. However, existing approaches primarily focus on local corrections\nto the base policy, lacking a global understanding of state evolution, which\nlimits robustness and generalization to unseen scenarios. To address this, we\npropose incorporating global dynamics modeling to guide residual policy\nupdates. Specifically, we leverage Koopman operator theory to impose linear\ntime-invariant structure in a learned latent space, enabling reliable state\ntransitions and improved extrapolation for long-horizon prediction and unseen\nenvironments. We introduce KORR (Koopman-guided Online Residual Refinement), a\nsimple yet effective framework that conditions residual corrections on\nKoopman-predicted latent states, enabling globally informed and stable action\nrefinement. We evaluate KORR on long-horizon, fine-grained robotic furniture\nassembly tasks under various perturbations. Results demonstrate consistent\ngains in performance, robustness, and generalization over strong baselines. Our\nfindings further highlight the potential of Koopman-based modeling to bridge\nmodern learning methods with classical control theory.", "AI": {"tldr": "为解决模仿学习在长周期高精度任务中的复合误差问题，本文提出了KORR框架。它通过Koopman算子理论引入全局动力学模型指导残差策略更新，显著提升了性能、鲁棒性和泛化能力。", "motivation": "模仿学习在长周期任务和高精度控制中因复合误差而表现不佳。现有残差策略学习方法侧重局部修正，缺乏对状态演化的全局理解，限制了鲁棒性和泛化能力。", "method": "提出KORR（Koopman-guided Online Residual Refinement）框架。该方法利用Koopman算子理论在学习到的潜在空间中施加线性时不变结构，以实现可靠的状态转换和改进的外推。残差修正基于Koopman预测的潜在状态进行条件化，从而实现全局信息引导和稳定的动作修正。", "result": "在受各种扰动影响的长周期、精细机器人家具组装任务中，KORR在性能、鲁棒性和泛化能力方面均持续优于强基线。研究结果还强调了基于Koopman的建模在连接现代学习方法与经典控制理论方面的潜力。", "conclusion": "KORR框架通过将Koopman算子理论引入残差策略学习，有效解决了模仿学习在长周期高精度任务中的挑战。Koopman建模在弥合现代学习与经典控制理论之间鸿沟方面具有巨大潜力。"}}
{"id": "2509.12329", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.12329", "abs": "https://arxiv.org/abs/2509.12329", "authors": ["Shengjie Kris Liu", "Siqin Wang", "Lu Zhang"], "title": "Uncertainty-Aware Hourly Air Temperature Mapping at 2 km Resolution via Physics-Guided Deep Learning", "comment": null, "summary": "Near-surface air temperature is a key physical property of the Earth's\nsurface. Although weather stations offer continuous monitoring and satellites\nprovide broad spatial coverage, no single data source offers seamless data in a\nspatiotemporal fashion. Here, we propose a data-driven, physics-guided deep\nlearning approach to generate hourly air temperature data at 2 km resolution\nover the contiguous United States. The approach, called Amplifier\nAir-Transformer, first reconstructs GOES-16 surface temperature data obscured\nby clouds. It does so through a neural network encoded with the annual\ntemperature cycle, incorporating a linear term to amplify ERA5 temperature\nvalues at finer scales and convolutional layers to capture spatiotemporal\nvariations. Then, another neural network transforms the reconstructed surface\ntemperature into air temperature by leveraging its latent relationship with key\nEarth surface properties. The approach is further enhanced with predictive\nuncertainty estimation through deep ensemble learning to improve reliability.\nThe proposed approach is built and tested on 77.7 billion surface temperature\npixels and 155 million air temperature records from weather stations across the\ncontiguous United States (2018-2024), achieving hourly air temperature mapping\naccuracy of 1.93 C in station-based validation. The proposed approach\nstreamlines surface temperature reconstruction and air temperature prediction,\nand it can be extended to other satellite sources for seamless air temperature\nmonitoring at high spatiotemporal resolution. The generated data of this study\ncan be downloaded at https://doi.org/10.5281/zenodo.15252812, and the project\nwebpage can be found at https://skrisliu.com/HourlyAirTemp2kmUSA/.", "AI": {"tldr": "本研究提出了一种数据驱动、物理引导的深度学习方法（Amplifier Air-Transformer），用于生成美国本土每小时2公里分辨率的近地表气温数据，通过重建云层遮挡的卫星地表温度并将其转换为气温，实现了1.93°C的精度。", "motivation": "尽管气象站提供连续监测，卫星提供广阔空间覆盖，但目前没有单一数据源能无缝地提供时空连续的近地表气温数据。", "method": "该方法名为Amplifier Air-Transformer，首先通过一个结合了年度温度周期编码、线性项放大ERA5温度和卷积层捕捉时空变化的神经网络，重建被云层遮挡的GOES-16地表温度数据。然后，另一个神经网络利用地表关键属性的潜在关系，将重建的地表温度转换为气温。此外，该方法通过深度集成学习增强了预测不确定性估计，以提高可靠性。该模型在777亿地表温度像素和1.55亿气象站气温记录（2018-2024年）上进行构建和测试。", "result": "在基于气象站的验证中，该方法实现了每小时气温映射精度为1.93°C。", "conclusion": "所提出的方法简化了地表温度重建和气温预测过程，可扩展到其他卫星数据源，以实现高时空分辨率的无缝气温监测。"}}
{"id": "2509.12792", "categories": ["eess.SY", "cs.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.12792", "abs": "https://arxiv.org/abs/2509.12792", "authors": ["Moritz Heinlein", "Florian Messerer", "Moritz Diehl", "Sergio Lucia"], "title": "Ellipsoidal partitions for improved multi-stage robust model predictive control", "comment": "Paper accepted for CDC 2025, Code available under:\n  https://github.com/MoritzHein/Ellipsoid_Partition", "summary": "Ellipsoidal tube-based model predictive control methods effectively account\nfor the propagation of the reachable set, typically employing linear feedback\npolicies. In contrast, scenario-based approaches offer more flexibility in the\nfeedback structure by considering different control actions for different\nbranches of a scenario tree. However, they face challenges in ensuring rigorous\nguarantees. This work aims to integrate the strengths of both methodologies by\nenhancing ellipsoidal tube-based MPC with a scenario tree formulation. The\nuncertainty ellipsoids are partitioned by halfspaces such that each partitioned\nset can be controlled independently. The proposed ellipsoidal multi-stage\napproach is demonstrated in a human-robot system, highlighting its advantages\nin handling uncertainty while maintaining computational tractability.", "AI": {"tldr": "本文提出一种结合椭球管式模型预测控制和场景树方法的新型多阶段MPC，通过半空间划分不确定性椭球，实现更灵活的反馈控制，并在人机系统中验证其在处理不确定性方面的优势及计算可行性。", "motivation": "椭球管式MPC在可达集传播方面有效且提供严格保证，但反馈策略通常限于线性；而基于场景的方法提供更灵活的反馈结构，但难以保证严格性。研究动机是结合两者的优点。", "method": "通过场景树公式增强椭球管式MPC。不确定性椭球被半空间划分，使得每个划分的集合可以独立控制。该方法被称为椭球多阶段方法。", "result": "在人机系统中进行了演示，结果突出显示了该方法在处理不确定性方面的优势，同时保持了计算的可行性。", "conclusion": "所提出的椭球多阶段方法成功整合了两种方法的优点，在处理不确定性方面表现出色，并能保持计算上的可行性。"}}
{"id": "2509.12471", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12471", "abs": "https://arxiv.org/abs/2509.12471", "authors": ["Yiwen Lu", "Lu Li", "Dazheng Zhang", "Xinyao Jian", "Tingyin Wang", "Siqi Chen", "Yuqing Lei", "Jiayi Tong", "Zhaohan Xi", "Haitao Chu", "Chongliang Luo", "Alexis Ogdie", "Brian Athey", "Alparslan Turan", "Michael Abramoff", "Joseph C Cappelleri", "Hua Xu", "Yun Lu", "Jesse Berlin", "Daniel I. Sessler", "David A. Asch", "Xiaoqian Jiang", "Yong Chen"], "title": "Empowering Clinical Trial Design through AI: A Randomized Evaluation of PowerGPT", "comment": null, "summary": "Sample size calculations for power analysis are critical for clinical\nresearch and trial design, yet their complexity and reliance on statistical\nexpertise create barriers for many researchers. We introduce PowerGPT, an\nAI-powered system integrating large language models (LLMs) with statistical\nengines to automate test selection and sample size estimation in trial design.\nIn a randomized trial to evaluate its effectiveness, PowerGPT significantly\nimproved task completion rates (99.3% vs. 88.9% for test selection, 99.3% vs.\n77.8% for sample size calculation) and accuracy (94.1% vs. 55.4% in sample size\nestimation, p < 0.001), while reducing average completion time (4.0 vs. 9.3\nminutes, p < 0.001). These gains were consistent across various statistical\ntests and benefited both statisticians and non-statisticians as well as\nbridging expertise gaps. Already under deployment across multiple institutions,\nPowerGPT represents a scalable AI-driven approach that enhances accessibility,\nefficiency, and accuracy in statistical power analysis for clinical research.", "AI": {"tldr": "PowerGPT是一个AI系统，结合大语言模型和统计引擎，自动化临床试验中的统计检验选择和样本量计算，显著提升了任务完成率、准确性并缩短了时间。", "motivation": "临床研究和试验设计中的样本量计算对于功效分析至关重要，但其复杂性及对统计专业知识的依赖对许多研究人员构成了障碍。", "method": "PowerGPT是一个AI驱动的系统，它将大语言模型（LLMs）与统计引擎集成，以自动化试验设计中的检验选择和样本量估计。通过一项随机对照试验评估其有效性。", "result": "PowerGPT显著提高了任务完成率（检验选择：99.3% vs 88.9%；样本量计算：99.3% vs 77.8%）和准确性（样本量估计：94.1% vs 55.4%，p < 0.001），同时减少了平均完成时间（4.0分钟 vs 9.3分钟，p < 0.001）。这些改进在各种统计检验中均保持一致，并惠及统计学家和非统计学家，弥合了专业知识差距。", "conclusion": "PowerGPT代表了一种可扩展的AI驱动方法，提高了临床研究中统计功效分析的可及性、效率和准确性，并已在多个机构部署。"}}
{"id": "2509.12647", "categories": ["cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.12647", "abs": "https://arxiv.org/abs/2509.12647", "authors": ["Li Fu", "Yu Xin", "Sunlu Zeng", "Lu Fan", "Youzheng Wu", "Xiaodong He"], "title": "PAC: Pronunciation-Aware Contextualized Large Language Model-based Automatic Speech Recognition", "comment": "Submitted to ICASSP 2026", "summary": "This paper presents a Pronunciation-Aware Contextualized (PAC) framework to\naddress two key challenges in Large Language Model (LLM)-based Automatic Speech\nRecognition (ASR) systems: effective pronunciation modeling and robust\nhomophone discrimination. Both are essential for raw or long-tail word\nrecognition. The proposed approach adopts a two-stage learning paradigm. First,\nwe introduce a pronunciation-guided context learning method. It employs an\ninterleaved grapheme-phoneme context modeling strategy that incorporates\ngrapheme-only distractors, encouraging the model to leverage phonemic cues for\naccurate recognition. Then, we propose a pronunciation-discriminative\nreinforcement learning method with perturbed label sampling to further enhance\nthe model\\'s ability to distinguish contextualized homophones. Experimental\nresults on the public English Librispeech and Mandarin AISHELL-1 datasets\nindicate that PAC: (1) reduces relative Word Error Rate (WER) by 30.2% and\n53.8% compared to pre-trained LLM-based ASR models, and (2) achieves 31.8% and\n60.5% relative reductions in biased WER for long-tail words compared to strong\nbaselines, respectively.", "AI": {"tldr": "本文提出了一种发音感知语境化（PAC）框架，通过两阶段学习方法，有效解决了基于LLM的ASR系统中发音建模和同音异义词辨别两大挑战，尤其改善了对生词和长尾词的识别。", "motivation": "基于大型语言模型（LLM）的自动语音识别（ASR）系统在有效的发音建模和鲁棒的同音异义词辨别方面面临关键挑战，而这两点对于生词或长尾词的识别至关重要。", "method": "该方法采用两阶段学习范式：1. 引入发音引导的语境学习方法，采用交错的字形-音素语境建模策略，并结合纯字形干扰项，鼓励模型利用音素线索进行准确识别。2. 提出发音判别强化学习方法，通过扰动标签采样进一步增强模型区分语境化同音异义词的能力。", "result": "在公开的英语Librispeech和普通话AISHELL-1数据集上的实验结果表明，PAC框架相对于预训练的基于LLM的ASR模型，相对词错误率（WER）分别降低了30.2%和53.8%；相对于强基线，长尾词的偏置WER相对降低了31.8%和60.5%。", "conclusion": "PAC框架显著提升了基于LLM的ASR系统的性能，尤其在发音建模和同音异义词辨别方面表现出色，有效改善了对生词和长尾词的识别能力。"}}
{"id": "2509.12594", "categories": ["cs.RO", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12594", "abs": "https://arxiv.org/abs/2509.12594", "authors": ["Titong Jiang", "Xuefeng Jiang", "Yuan Ma", "Xin Wen", "Bailin Li", "Kun Zhan", "Peng Jia", "Yahui Liu", "Sheng Sun", "Xianpeng Lang"], "title": "The Better You Learn, The Smarter You Prune: Towards Efficient Vision-language-action Models via Differentiable Token Pruning", "comment": "Under review. Project site:\n  https://liauto-research.github.io/LightVLA", "summary": "We present LightVLA, a simple yet effective differentiable token pruning\nframework for vision-language-action (VLA) models. While VLA models have shown\nimpressive capability in executing real-world robotic tasks, their deployment\non resource-constrained platforms is often bottlenecked by the heavy\nattention-based computation over large sets of visual tokens. LightVLA\naddresses this challenge through adaptive, performance-driven pruning of visual\ntokens: It generates dynamic queries to evaluate visual token importance, and\nadopts Gumbel softmax to enable differentiable token selection. Through\nfine-tuning, LightVLA learns to preserve the most informative visual tokens\nwhile pruning tokens which do not contribute to task execution, thereby\nimproving efficiency and performance simultaneously. Notably, LightVLA requires\nno heuristic magic numbers and introduces no additional trainable parameters,\nmaking it compatible with modern inference frameworks. Experimental results\ndemonstrate that LightVLA outperforms different VLA models and existing token\npruning methods across diverse tasks on the LIBERO benchmark, achieving higher\nsuccess rates with substantially reduced computational overhead. Specifically,\nLightVLA reduces FLOPs and latency by 59.1% and 38.2% respectively, with a 2.9%\nimprovement in task success rate. Meanwhile, we also investigate the learnable\nquery-based token pruning method LightVLA* with additional trainable\nparameters, which also achieves satisfactory performance. Our work reveals that\nas VLA pursues optimal performance, LightVLA spontaneously learns to prune\ntokens from a performance-driven perspective. To the best of our knowledge,\nLightVLA is the first work to apply adaptive visual token pruning to VLA tasks\nwith the collateral goals of efficiency and performance, marking a significant\nstep toward more efficient, powerful and practical real-time robotic systems.", "AI": {"tldr": "LightVLA提出了一种简单有效的可微分视觉令牌剪枝框架，用于视觉-语言-动作（VLA）模型，通过自适应地修剪视觉令牌，同时提高效率和任务成功率。", "motivation": "VLA模型在机器人任务中表现出色，但在资源受限平台上部署时，由于对大量视觉令牌进行基于注意力的计算，效率受到瓶颈。", "method": "LightVLA通过自适应、性能驱动的方式修剪视觉令牌。它生成动态查询来评估视觉令牌的重要性，并采用Gumbel softmax实现可微分的令牌选择。通过微调，LightVLA学习保留信息最丰富的视觉令牌，同时剪枝对任务执行无贡献的令牌。LightVLA无需启发式参数，不引入额外可训练参数。此外，还探讨了具有额外可训练参数的LightVLA*方法。", "result": "LightVLA在LIBERO基准测试中，在多样化任务上优于不同的VLA模型和现有令牌剪枝方法。它以显著降低的计算开销实现了更高的成功率：FLOPs减少59.1%，延迟减少38.2%，任务成功率提高2.9%。LightVLA*也取得了令人满意的性能。研究表明，LightVLA自发地从性能驱动的角度学习剪枝令牌。", "conclusion": "LightVLA是首个将自适应视觉令牌剪枝应用于VLA任务，同时兼顾效率和性能的工作，为更高效、强大和实用的实时机器人系统迈出了重要一步。"}}
{"id": "2509.12353", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12353", "abs": "https://arxiv.org/abs/2509.12353", "authors": ["Anthony Miyaguchi", "Chandrasekaran Maruthaiyannan", "Charles R. Clark"], "title": "DS@GT AnimalCLEF: Triplet Learning over ViT Manifolds with Nearest Neighbor Classification for Animal Re-identification", "comment": "CLEF 2025 working notes", "summary": "This paper details the DS@GT team's entry for the AnimalCLEF 2025\nre-identification challenge. Our key finding is that the effectiveness of\npost-hoc metric learning is highly contingent on the initial quality and\ndomain-specificity of the backbone embeddings. We compare a general-purpose\nmodel (DINOv2) with a domain-specific model (MegaDescriptor) as a backbone. A\nK-Nearest Neighbor classifier with robust thresholding then identifies known\nindividuals or flags new ones. While a triplet-learning projection head\nimproved the performance of the specialized MegaDescriptor model by 0.13\npoints, it yielded minimal gains (0.03) for the general-purpose DINOv2 on\naveraged BAKS and BAUS. We demonstrate that the general-purpose manifold is\nmore difficult to reshape for fine-grained tasks, as evidenced by stagnant\nvalidation loss and qualitative visualizations. This work highlights the\ncritical limitations of refining general-purpose features for specialized,\nlimited-data re-ID tasks and underscores the importance of domain-specific\npre-training. The implementation for this work is publicly available at\ngithub.com/dsgt-arc/animalclef-2025.", "AI": {"tldr": "本文参与AnimalCLEF 2025重识别挑战，发现后置度量学习的有效性高度依赖于骨干嵌入的初始质量和领域特异性。领域特定预训练对细粒度重识别至关重要。", "motivation": "参与AnimalCLEF 2025重识别挑战赛，旨在探索不同骨干模型（通用型与领域特定型）对后置度量学习效果的影响，并解决细粒度、数据有限的重识别任务。", "method": "团队比较了两种骨干模型：通用型DINOv2和领域特定型MegaDescriptor。在此基础上，应用了三元组学习（triplet-learning）投影头进行后置度量学习，并结合K-近邻（KNN）分类器和鲁棒阈值化来识别已知个体或标记新个体。", "result": "三元组学习使领域特定MegaDescriptor模型的性能提高了0.13点，但对通用型DINOv2模型的性能提升微乎其微（仅0.03点）。研究表明，通用型流形更难通过精炼来适应细粒度任务，这体现在验证损失停滞和可视化结果上。", "conclusion": "研究强调了为专业化、数据有限的重识别任务精炼通用特征的局限性，并突出了领域特定预训练的重要性。"}}
{"id": "2509.12839", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.12839", "abs": "https://arxiv.org/abs/2509.12839", "authors": ["Liuxun Xue", "Shu Sun", "Hangsong Yan"], "title": "Spatial Correlation and Degrees of Freedom in Arched HMIMO Arrays: A Closed-Form Analysis", "comment": null, "summary": "This paper presents a closed-form analysis of spatial correlation and degrees\nof freedom (DoF) for arched holographic multiple-input multiple-output (HMIMO)\narrays, which can be viewed as a special form of fluid antenna systems (FAS)\nwhen their geometry is fluidically adaptable. Unlike traditional planar\nconfigurations, practical HMIMO surfaces may exhibit curvature, significantly\ninfluencing their spatial characteristics and performance. We derive exact\ncorrelation expressions for both arched uniform linear arrays and arched\nuniform rectangular arrays, capturing curvature effects under far field\npropagation. Our results reveal that isotropic scattering results in DoF being\ndominated by the maximum span of the HMIMO array, such that shape effects are\nweakened, and bending does not significantly reduce the available spatial DoF.\nNumerical simulations validate the accuracy of the closed-form formulas and\ndemonstrate the robustness of DoF against curvature variations, supporting\nflexible array designs. These findings offer fundamental insights into\ngeometry-aware optimization for next-generation HMIMO/FAS systems and pave the\nway for practical implementations of curved HMIMO arrays.", "AI": {"tldr": "本文对拱形全息多输入多输出（HMIMO）阵列的空间相关性和自由度（DoF）进行了闭式分析，发现DoF主要由阵列的最大跨度决定，并且对曲率变化具有鲁棒性。", "motivation": "传统的平面阵列配置与实际中可能存在的弯曲HMIMO表面之间存在差异。这种曲率会显著影响阵列的空间特性和性能，因此需要深入理解其影响。", "method": "该研究推导了拱形均匀线阵（ULA）和拱形均匀矩形阵列（URA）在远场传播条件下的精确闭式相关表达式，以捕捉曲率效应。并通过数值仿真验证了闭式公式的准确性。", "result": "研究结果表明，在各向同性散射下，DoF主要由HMIMO阵列的最大跨度决定，形状效应被削弱，弯曲不会显著降低可用的空间DoF。数值模拟证实了公式的准确性以及DoF对曲率变化的鲁棒性。", "conclusion": "这些发现为下一代HMIMO/FAS系统的几何感知优化提供了基础性见解，并为弯曲HMIMO阵列的实际实现铺平了道路，支持灵活的阵列设计。"}}
{"id": "2509.12495", "categories": ["cs.AI", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.12495", "abs": "https://arxiv.org/abs/2509.12495", "authors": ["Gülce Kardeş", "David Krakauer", "Joshua Grochow"], "title": "Physical Complexity of a Cognitive Artifact", "comment": null, "summary": "Cognitive science and theoretical computer science both seek to classify and\nexplain the difficulty of tasks. Mechanisms of intelligence are those that\nreduce task difficulty. Here we map concepts from the computational complexity\nof a physical puzzle, the Soma Cube, onto cognitive problem-solving strategies\nthrough a ``Principle of Materiality''. By analyzing the puzzle's branching\nfactor, measured through search tree outdegree, we quantitatively assess task\ndifficulty and systematically examine how different strategies modify\ncomplexity. We incrementally refine a trial-and-error search by layering\npreprocessing (cognitive chunking), value ordering (cognitive free-sorting),\nvariable ordering (cognitive scaffolding), and pruning (cognitive inference).\nWe discuss how the competent use of artifacts reduces effective time complexity\nby exploiting physical constraints and propose a model of intelligence as a\nlibrary of algorithms that recruit the capabilities of both mind and matter.", "AI": {"tldr": "本文通过“物质性原则”将索玛立方体的计算复杂性与认知问题解决策略联系起来，量化了任务难度，并分析了不同认知策略如何通过利用物理约束来降低复杂性，提出了一种结合心智与物质能力的智能模型。", "motivation": "认知科学和理论计算机科学都致力于分类和解释任务的难度，并理解智能机制如何降低这些难度。", "method": "研究者将索玛立方体这一物理谜题的计算复杂性概念映射到认知问题解决策略上，提出了“物质性原则”。通过分析谜题的搜索树出度（分支因子）来量化任务难度，并系统地考察了预处理（认知组块）、价值排序（认知自由分类）、变量排序（认知支架）和剪枝（认知推理）等策略如何逐步优化试错搜索，从而修改复杂性。此外，还探讨了人工制品如何通过利用物理约束来降低有效时间复杂性。", "result": "研究表明，通过分层应用预处理、价值排序、变量排序和剪枝等认知策略，可以逐步优化试错搜索。熟练使用人工制品能够通过利用物理约束来有效降低任务的有效时间复杂性。", "conclusion": "智能可以被建模为一个算法库，该算法库能够整合心智和物质的能力来降低任务难度。"}}
{"id": "2509.12652", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12652", "abs": "https://arxiv.org/abs/2509.12652", "authors": ["Paul Kröger", "Emilio Barkett"], "title": "Don't Change My View: Ideological Bias Auditing in Large Language Models", "comment": null, "summary": "As large language models (LLMs) become increasingly embedded in products used\nby millions, their outputs may influence individual beliefs and, cumulatively,\nshape public opinion. If the behavior of LLMs can be intentionally steered\ntoward specific ideological positions, such as political or religious views,\nthen those who control these systems could gain disproportionate influence over\npublic discourse. Although it remains an open question whether LLMs can\nreliably be guided toward coherent ideological stances and whether such\nsteering can be effectively prevented, a crucial first step is to develop\nmethods for detecting when such steering attempts occur. In this work, we adapt\na previously proposed statistical method to the new context of ideological bias\nauditing. Our approach carries over the model-agnostic design of the original\nframework, which does not require access to the internals of the language\nmodel. Instead, it identifies potential ideological steering by analyzing\ndistributional shifts in model outputs across prompts that are thematically\nrelated to a chosen topic. This design makes the method particularly suitable\nfor auditing proprietary black-box systems. We validate our approach through a\nseries of experiments, demonstrating its practical applicability and its\npotential to support independent post hoc audits of LLM behavior.", "AI": {"tldr": "本文提出了一种模型无关的统计方法，通过分析大型语言模型（LLMs）输出的分布变化，来检测其是否存在意识形态偏见或被故意引导的情况，尤其适用于审计专有黑盒系统。", "motivation": "大型语言模型（LLMs）日益嵌入产品中，其输出可能影响个人信仰并塑造公众舆论。如果LLMs的行为可以被故意引导至特定的意识形态立场（如政治或宗教观点），那么控制这些系统的人将对公共话语产生不成比例的影响。因此，开发检测此类引导尝试的方法至关重要。", "method": "研究团队调整了一种先前提出的统计方法，以适应意识形态偏见审计的新情境。该方法保持了原始框架的模型无关设计，无需访问语言模型的内部结构。相反，它通过分析LLM在与特定主题相关的提示上的输出分布变化来识别潜在的意识形态引导。这种设计使其特别适用于审计专有的黑盒系统。", "result": "通过一系列实验验证了该方法的有效性，展示了其在实际应用中的可行性，以及支持对LLM行为进行独立的、事后审计的潜力。", "conclusion": "所提出的方法能够有效检测大型语言模型中潜在的意识形态引导，尤其适用于黑盒系统，为独立审计LLM行为提供了关键工具，有助于防止其对公共话语产生不当影响。"}}
{"id": "2509.12618", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12618", "abs": "https://arxiv.org/abs/2509.12618", "authors": ["Zekai Zhang", "Weiye Zhu", "Hewei Pan", "Xiangchen Wang", "Rongtao Xu", "Xing Sun", "Feng Zheng"], "title": "ActiveVLN: Towards Active Exploration via Multi-Turn RL in Vision-and-Language Navigation", "comment": null, "summary": "The Vision-and-Language Navigation (VLN) task requires an agent to follow\nnatural language instructions and navigate through complex environments.\nExisting MLLM-based VLN methods primarily rely on imitation learning (IL) and\noften use DAgger for post-training to mitigate covariate shift. While\neffective, these approaches incur substantial data collection and training\ncosts. Reinforcement learning (RL) offers a promising alternative. However,\nprior VLN RL methods lack dynamic interaction with the environment and depend\non expert trajectories for reward shaping, rather than engaging in open-ended\nactive exploration. This restricts the agent's ability to discover diverse and\nplausible navigation routes. To address these limitations, we propose\nActiveVLN, a VLN framework that explicitly enables active exploration through\nmulti-turn RL. In the first stage, a small fraction of expert trajectories is\nused for IL to bootstrap the agent. In the second stage, the agent iteratively\npredicts and executes actions, automatically collects diverse trajectories, and\noptimizes multiple rollouts via the GRPO objective. To further improve RL\nefficiency, we introduce a dynamic early-stopping strategy to prune long-tail\nor likely failed trajectories, along with additional engineering optimizations.\nExperiments show that ActiveVLN achieves the largest performance gains over IL\nbaselines compared to both DAgger-based and prior RL-based post-training\nmethods, while reaching competitive performance with state-of-the-art\napproaches despite using a smaller model. Code and data will be released soon.", "AI": {"tldr": "本文提出ActiveVLN框架，通过多轮强化学习实现主动探索，解决视觉与语言导航（VLN）任务中现有方法数据成本高昂和探索受限的问题。ActiveVLN结合少量专家轨迹的模仿学习和GRPO优化，显著提升了导航性能。", "motivation": "现有基于MLLM的VLN方法主要依赖模仿学习（IL）和DAgger后训练，导致数据收集和训练成本高昂。而现有的VLN强化学习（RL）方法缺乏与环境的动态交互，并依赖专家轨迹进行奖励塑造，限制了代理发现多样且合理导航路径的能力。", "method": "本文提出了ActiveVLN框架，通过多轮强化学习实现主动探索。该方法分为两个阶段：第一阶段，利用少量专家轨迹进行模仿学习以引导代理；第二阶段，代理迭代预测并执行动作，自动收集多样轨迹，并通过GRPO目标优化多个rollout。为提高RL效率，还引入了动态早停策略来剪枝长尾或可能失败的轨迹，并进行了额外的工程优化。", "result": "实验结果表明，与基于DAgger和现有RL的后训练方法相比，ActiveVLN在模仿学习基线上取得了最大的性能提升。尽管使用了较小的模型，ActiveVLN仍能达到与最先进方法相媲美的性能。", "conclusion": "ActiveVLN通过主动探索的多轮强化学习，有效解决了现有VLN方法在数据成本和探索多样性方面的局限性。该框架在性能上显著优于现有基线，并与最先进方法具有竞争力，同时提高了训练效率。"}}
{"id": "2509.12380", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.12380", "abs": "https://arxiv.org/abs/2509.12380", "authors": ["Florian Zager", "Hamza A. A. Gardi"], "title": "GhostNetV3-Small: A Tailored Architecture and Comparative Study of Distillation Strategies for Tiny Images", "comment": null, "summary": "Deep neural networks have achieved remarkable success across a range of\ntasks, however their computational demands often make them unsuitable for\ndeployment on resource-constrained edge devices. This paper explores strategies\nfor compressing and adapting models to enable efficient inference in such\nenvironments. We focus on GhostNetV3, a state-of-the-art architecture for\nmobile applications, and propose GhostNetV3-Small, a modified variant designed\nto perform better on low-resolution inputs such as those in the CIFAR-10\ndataset. In addition to architectural adaptation, we provide a comparative\nevaluation of knowledge distillation techniques, including traditional\nknowledge distillation, teacher assistants, and teacher ensembles. Experimental\nresults show that GhostNetV3-Small significantly outperforms the original\nGhostNetV3 on CIFAR-10, achieving an accuracy of 93.94%. Contrary to\nexpectations, all examined distillation strategies led to reduced accuracy\ncompared to baseline training. These findings indicate that architectural\nadaptation can be more impactful than distillation in small-scale image\nclassification tasks, highlighting the need for further research on effective\nmodel design and advanced distillation techniques for low-resolution domains.", "AI": {"tldr": "本文探讨了为资源受限设备压缩和调整深度学习模型的方法，特别是提出并评估了GhostNetV3-Small模型在低分辨率图像分类任务上的表现，并比较了不同知识蒸馏策略的效果。", "motivation": "深度神经网络计算需求高，难以部署在资源受限的边缘设备上，因此需要探索模型压缩和适应策略。", "method": "研究以GhostNetV3为基础，提出了GhostNetV3-Small变体，专门针对CIFAR-10等低分辨率输入进行架构调整。同时，比较了传统知识蒸馏、教师助理和教师集成等多种知识蒸馏技术。", "result": "GhostNetV3-Small在CIFAR-10数据集上显著优于原始GhostNetV3，达到了93.94%的准确率。然而，所有测试的知识蒸馏策略均导致准确率下降。", "conclusion": "研究表明，在小规模图像分类任务中，架构适应可能比知识蒸馏更有效。未来需要进一步研究低分辨率领域中有效的模型设计和高级蒸馏技术。"}}
{"id": "2509.12847", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.12847", "abs": "https://arxiv.org/abs/2509.12847", "authors": ["Alireza Shooshtari", "Antonio Pepiciello", "José Luis Domínguez-García"], "title": "Grid-informed Sharing Coefficients in Renewable Energy Communities", "comment": null, "summary": "The role of energy communities in grid operations is highly dependent on the\nspatial distribution of their participants. In particular, when local energy\nproducers and consumers are concentrated in different feeders, economic\nincentives from energy communities have the potential to affect local grid\ncongestion. To address this challenge, we propose a feeder-aware allocation\nstrategy that reflects grid topology in energy sharing. This strategy\nprioritizes energy sharing within the same feeder, thus incentivizing local\ngeneration-demand balance and improving grid operation. Different sharing\ncoefficients are tested, such as equal, proportional, and rank-based, in both\nstatic and dynamic formulations. The proposed strategy is tested on data from a\nreal energy community, whose participants are assumed to be distributed across\nfour feeders. The analysis is carried out from the perspectives of the\ncommunity as a whole, individual feeders, and single participants. Simulation\nresults show that the feeder-aware strategy, in addition to promoting local\nenergy balance, leads to higher and more stable revenues for most participants.", "AI": {"tldr": "本文提出了一种“馈线感知”的能源分配策略，通过优先考虑同一馈线内的能源共享来应对能源社区中参与者空间分布可能导致的局部电网拥堵问题，实验结果表明该策略能促进本地能源平衡并提高参与者收益。", "motivation": "能源社区中参与者的空间分布对电网运行影响显著，特别是当本地能源生产者和消费者分布在不同馈线时，经济激励可能导致局部电网拥堵。因此，需要一种考虑电网拓扑的能源共享策略来解决这一挑战。", "method": "研究提出了一种“馈线感知”的能源分配策略，该策略将电网拓扑结构纳入能源共享机制，优先在同一馈线内进行能源共享，以激励本地供需平衡。该策略在静态和动态公式下测试了不同的共享系数，如平均、按比例和基于排名的分配方式。", "result": "仿真结果表明，所提出的馈线感知策略不仅能有效促进本地能源平衡，还能为大多数参与者带来更高且更稳定的收益。", "conclusion": "馈线感知策略通过将电网拓扑纳入能源共享机制，能够有效改善电网运行，并通过促进本地能源平衡和提高参与者收益来解决能源社区中空间分布引起的电网拥堵问题。"}}
{"id": "2509.12524", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12524", "abs": "https://arxiv.org/abs/2509.12524", "authors": ["Rohit Chakraborty", "Subasish Das"], "title": "A Dimensionality-Reduced XAI Framework for Roundabout Crash Severity Insights", "comment": "This is the author's preprint version of a paper accepted for\n  presentation at HICSS 59 (Hawaii International Conference on System\n  Sciences), 2026, Hawaii, USA. The final published version will appear in the\n  official conference proceedings. Conference site: https://hicss.hawaii.edu/", "summary": "Roundabouts reduce severe crashes, yet risk patterns vary by conditions. This\nstudy analyzes 2017-2021 Ohio roundabout crashes using a two-step, explainable\nworkflow. Cluster Correspondence Analysis (CCA) identifies co-occurring factors\nand yields four crash patterns. A tree-based severity model is then interpreted\nwith SHAP to quantify drivers of injury within and across patterns. Results\nshow higher severity when darkness, wet surfaces, and higher posted speeds\ncoincide with fixed-object or angle events, and lower severity in clear,\nlow-speed settings. Pattern-specific explanations highlight mechanisms at\nentries (fail-to-yield, gap acceptance), within multi-lane circulation\n(improper maneuvers), and during slow-downs (rear-end). The workflow links\npattern discovery with case-level explanations, supporting site screening,\ncountermeasure selection, and audit-ready reporting. The contribution to\nInformation Systems is a practical template for usable XAI in public safety\nanalytics.", "AI": {"tldr": "本研究通过两步可解释工作流（聚类对应分析和基于SHAP解释的树模型），分析了俄亥俄州环形交叉口在2017-2021年间的事故数据，识别了四种事故模式及其严重性驱动因素，为公共安全分析提供了可用的可解释人工智能（XAI）模板。", "motivation": "环形交叉口能减少严重事故，但其风险模式因条件而异，需要深入分析以理解并量化事故严重性的驱动因素。", "method": "采用两步可解释工作流：1. 使用聚类对应分析（CCA）识别共同发生的因素并得出四种事故模式。2. 构建树模型来预测事故严重性，并使用SHAP方法解释模型，以量化模式内部和跨模式的伤害驱动因素。数据源自2017-2021年俄亥俄州环形交叉口事故。", "result": "研究发现，在黑暗、湿滑路面、较高限速与固定物体碰撞或角度碰撞事件同时发生时，事故严重性更高；而在晴朗、低速环境下，事故严重性较低。模式特定的解释揭示了事故机制，包括入口处的未能让行/间隙接受问题、多车道环岛内的不当操作以及减速时的追尾事件。", "conclusion": "该工作流将模式发现与案例级解释相结合，支持地点筛选、对策选择和可审计报告。它为公共安全分析中可用的可解释人工智能（XAI）提供了一个实用的模板。"}}
{"id": "2509.12661", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.12661", "abs": "https://arxiv.org/abs/2509.12661", "authors": ["Yougen Zhou", "Qin Chen", "Ningning Zhou", "Jie Zhou", "Xingjiao Wu", "Liang He"], "title": "Mitigating Strategy Preference Bias in Emotional Support Conversation via Uncertainty Estimations", "comment": null, "summary": "Emotional support conversation (ESC) aims to alleviate distress through\nempathetic dialogue, yet large language models (LLMs) face persistent\nchallenges in delivering effective ESC due to low accuracy in strategy\nplanning. Moreover, there is a considerable preference bias towards specific\nstrategies. Prior methods using fine-tuned strategy planners have shown\npotential in reducing such bias, while the underlying causes of the preference\nbias in LLMs have not well been studied. To address these issues, we first\nreveal the fundamental causes of the bias by identifying the knowledge\nboundaries of LLMs in strategy planning. Then, we propose an approach to\nmitigate the bias by reinforcement learning with a dual reward function, which\noptimizes strategy planning via both accuracy and entropy-based confidence for\neach region according to the knowledge boundaries. Experiments on the ESCov and\nExTES datasets with multiple LLM backbones show that our approach outperforms\nthe baselines, confirming the effectiveness of our approach.", "AI": {"tldr": "本文揭示了大型语言模型在情感支持对话策略规划中存在偏好的根本原因，并提出了一种基于双重奖励函数的强化学习方法来缓解这种偏好，从而提高了策略规划的准确性。", "motivation": "大型语言模型在情感支持对话中因策略规划准确性低和存在显著的策略偏好而面临挑战。尽管现有方法能减少偏好，但其根本原因尚未被充分研究。", "method": "首先，通过识别大型语言模型在策略规划中的知识边界，揭示了偏好的根本原因。然后，提出了一种使用双重奖励函数的强化学习方法，该方法根据知识边界，通过准确性和基于熵的置信度来优化每个区域的策略规划。", "result": "在ESCov和ExTES数据集上，使用多种大型语言模型骨干进行的实验表明，所提出的方法优于基线方法，证实了其有效性。", "conclusion": "所提出的方法通过识别知识边界和采用双重奖励强化学习，有效缓解了大型语言模型在情感支持对话策略规划中的偏好问题，提高了其表现。"}}
{"id": "2509.12620", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.12620", "abs": "https://arxiv.org/abs/2509.12620", "authors": ["Yikai Chen", "Zhi Zheng", "Jin Wang", "Bingye He", "Xiangyu Xu", "Jialu Zhang", "Huan Yu", "Guodong Lu"], "title": "PerchMobi^3: A Multi-Modal Robot with Power-Reuse Quad-Fan Mechanism for Air-Ground-Wall Locomotion", "comment": "7 pages, 8 figures. This work has been submitted to the IEEE for\n  possible publication", "summary": "Achieving seamless integration of aerial flight, ground driving, and wall\nclimbing within a single robotic platform remains a major challenge, as\nexisting designs often rely on additional adhesion actuators that increase\ncomplexity, reduce efficiency, and compromise reliability. To address these\nlimitations, we present PerchMobi^3, a quad-fan, negative-pressure,\nair-ground-wall robot that implements a propulsion-adhesion power-reuse\nmechanism. By repurposing four ducted fans to simultaneously provide aerial\nthrust and negative-pressure adhesion, and integrating them with four actively\ndriven wheels, PerchMobi^3 eliminates dedicated pumps while maintaining a\nlightweight and compact design. To the best of our knowledge, this is the first\nquad-fan prototype to demonstrate functional power reuse for multi-modal\nlocomotion. A modeling and control framework enables coordinated operation\nacross ground, wall, and aerial domains with fan-assisted transitions. The\nfeasibility of the design is validated through a comprehensive set of\nexperiments covering ground driving, payload-assisted wall climbing, aerial\nflight, and cross-mode transitions, demonstrating robust adaptability across\nlocomotion scenarios. These results highlight the potential of PerchMobi^3 as a\nnovel design paradigm for multi-modal robotic mobility, paving the way for\nfuture extensions toward autonomous and application-oriented deployment.", "AI": {"tldr": "PerchMobi^3 是一款四风扇、负压式陆空壁多模态机器人，通过将推进和吸附功能集成到同一风扇中实现功率复用，从而简化设计、提高效率，并实现了地面驱动、壁面攀爬和空中飞行的无缝切换。", "motivation": "现有陆空壁多模态机器人通常依赖额外的吸附执行器，这增加了系统的复杂性、降低了效率并损害了可靠性。本研究旨在解决这些局限性。", "method": "本文提出了PerchMobi^3机器人，它采用四涵道风扇，同时提供空中推力和负压吸附力，并与四个主动驱动轮集成。这种推进-吸附功率复用机制消除了对专用泵的需求，保持了轻量化和紧凑的设计。此外，还开发了建模和控制框架以实现陆地、壁面和空中域的协调操作及风扇辅助的模式转换。", "result": "通过一系列综合实验（包括地面驱动、载荷辅助壁面攀爬、空中飞行和跨模式转换），验证了该设计的可行性，展示了在不同运动场景下的强大适应性。这是首个展示多模态运动功能性功率复用的四风扇原型。", "conclusion": "PerchMobi^3 凸显了作为多模态机器人移动性新设计范式的潜力，为未来自主和面向应用的部署铺平了道路。"}}
{"id": "2509.12400", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12400", "abs": "https://arxiv.org/abs/2509.12400", "authors": ["Rongkun Zhu", "Kangning Cui", "Wei Tang", "Rui-Feng Wang", "Sarra Alqahtani", "David Lutz", "Fan Yang", "Paul Fine", "Jordan Karubian", "Robert Plemmons", "Jean-Michel Morel", "Victor Pauca", "Miles Silman"], "title": "From Orthomosaics to Raw UAV Imagery: Enhancing Palm Detection and Crown-Center Localization", "comment": "7 pages, 2 figures, 2 tables", "summary": "Accurate mapping of individual trees is essential for ecological monitoring\nand forest management. Orthomosaic imagery from unmanned aerial vehicles (UAVs)\nis widely used, but stitching artifacts and heavy preprocessing limit its\nsuitability for field deployment. This study explores the use of raw UAV\nimagery for palm detection and crown-center localization in tropical forests.\nTwo research questions are addressed: (1) how detection performance varies\nacross orthomosaic and raw imagery, including within-domain and cross-domain\ntransfer, and (2) to what extent crown-center annotations improve localization\naccuracy beyond bounding-box centroids. Using state-of-the-art detectors and\nkeypoint models, we show that raw imagery yields superior performance in\ndeployment-relevant scenarios, while orthomosaics retain value for robust\ncross-domain generalization. Incorporating crown-center annotations in training\nfurther improves localization and provides precise tree positions for\ndownstream ecological analyses. These findings offer practical guidance for\nUAV-based biodiversity and conservation monitoring.", "AI": {"tldr": "本研究发现，与正射影像相比，原始无人机影像结合树冠中心标注在个体树木检测和定位方面表现更优，尤其适用于实地部署；而正射影像在跨域泛化方面仍有价值。", "motivation": "个体树木的精确测绘对生态监测和森林管理至关重要。现有无人机正射影像存在拼接伪影和繁重预处理的局限性，不适合实地部署，因此需要探索原始无人机影像的应用潜力。", "method": "研究通过比较正射影像和原始影像在棕榈树检测和树冠中心定位方面的性能，包括域内和跨域迁移。同时，评估了树冠中心标注对定位精度的提升作用（超越边界框中心）。研究使用了最先进的检测器和关键点模型。", "result": "结果表明，在与部署相关的场景中，原始影像表现更优；而正射影像在稳健的跨域泛化方面仍有价值。在训练中加入树冠中心标注进一步提高了定位精度，并能为后续生态分析提供精确的树木位置。", "conclusion": "研究结果为基于无人机的生物多样性和保护监测提供了实用指导，建议在实地部署场景中优先使用原始无人机影像并结合树冠中心标注以获得更精准的定位。"}}
{"id": "2509.12900", "categories": ["eess.SY", "cs.SI", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.12900", "abs": "https://arxiv.org/abs/2509.12900", "authors": ["Bálint Hartmann", "Michelle T. Cirunay"], "title": "Topology and Fragility of European High-Voltage Networks: A Cross-Country Comparative Analysis", "comment": null, "summary": "Reliable electricity supply depends on the seamless operation of high-voltage\ngrid infrastructure spanning both transmission and sub-transmission levels.\nBeneath this apparent uniformity lies a striking structural diversity, which\nleaves a clear imprint on system vulnerability. In this paper, we present\nharmonized topological models of the high-voltage grids of 15 European\ncountries, integrating all elements at voltage levels above 110 kV. Topological\nanalysis of these networks reveals a simple yet robust pattern: node degree\ndistributions consistently follow an exponential decay, but the rate of decay\nvaries significantly across countries. Through a detailed and systematic\nevaluation of network tolerance to node and edge removals, we show that the\ndecay rate delineates the boundary between systems that are more resilient to\nfailures and those that are prone to large-scale disruptions. Furthermore, we\ndemonstrate that this numerical boundary is highly sensitive to which layers of\nthe infrastructure are included in the models. To our knowledge, this study\nprovides the first quantitative cross-country comparison of 15 European\nhigh-voltage networks, linking topological properties with vulnerability\ncharacteristics.", "AI": {"tldr": "本研究对15个欧洲国家的高压电网进行了拓扑建模和分析，发现节点度分布遵循指数衰减，且衰减率与系统对故障的弹性或脆弱性密切相关，同时强调了模型中包含的基础设施层级的重要性。", "motivation": "高压电网基础设施的结构多样性对系统脆弱性有显著影响，但缺乏对欧洲高压电网的定量跨国比较，以了解其拓扑特性与脆弱性之间的关系。", "method": "研究构建了15个欧洲国家高压电网（电压等级高于110 kV）的统一拓扑模型。通过拓扑分析，研究了节点度分布，并系统评估了网络对节点和边缘移除的容忍度。", "result": "节点度分布一致遵循指数衰减，但衰减率在不同国家间差异显著。衰减率划定了系统对故障更具弹性或更容易出现大规模中断的边界。此外，该数值边界对模型中包含的基础设施层级高度敏感。", "conclusion": "本研究首次对15个欧洲高压电网进行了定量跨国比较，成功将拓扑特性（特别是指数衰减率）与脆弱性特征联系起来，揭示了电网结构多样性对系统韧性的关键影响。"}}
{"id": "2509.12541", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12541", "abs": "https://arxiv.org/abs/2509.12541", "authors": ["Nicholas Pipitone", "Ghita Houir Alami", "Advaith Avadhanam", "Anton Kaminskyi", "Ashley Khoo"], "title": "zELO: ELO-inspired Training Method for Rerankers and Embedding Models", "comment": "13 pages, 9 sections, 17 figures and tables", "summary": "We introduce a novel training methodology named zELO, which optimizes\nretrieval performance via the analysis that ranking tasks are statically\nequivalent to a Thurstone model. Based on the zELO method, we use unsupervised\ndata in order train a suite of state-of-the-art open-weight reranker models:\nzerank-1 and zerank-1-small. These models achieve the highest retrieval scores\nin multiple domains, including finance, legal, code, and STEM, outperforming\nclosed-source proprietary rerankers on both NDCG@10 and Recall. These models\nalso demonstrate great versatility, maintaining their 0-shot performance on\nout-of-domain and private customer datasets. The training data included 112,000\nqueries and 100 documents per query, and was trained end-to-end from\nunannotated queries and documents in less than 10,000 H100-hours.", "AI": {"tldr": "本文介绍了一种名为zELO的新型训练方法，通过将排序任务等同于Thurstone模型来优化检索性能。基于此方法，作者训练了开源重排序模型zerank-1和zerank-1-small，它们在多个领域（金融、法律、代码、STEM）取得了最先进的检索分数，超越了闭源专有模型，并展现了出色的泛化能力。", "motivation": "研究动机是找到一种新的训练方法来优化检索性能，特别是针对重排序任务，并利用无监督数据训练出高性能的开源重排序模型。", "method": "引入了zELO训练方法，该方法通过分析排序任务与Thurstone模型的静态等价性来优化检索性能。基于此方法，使用无监督数据（包括112,000个查询和每个查询100个文档）端到端地训练了zerank-1和zerank-1-small等开源重排序模型，总训练时间少于10,000 H100小时。", "result": "zerank模型在金融、法律、代码和STEM等多个领域取得了最高的检索分数，在NDCG@10和Recall指标上均超越了闭源专有重排序器。这些模型还展现了极佳的多功能性，在域外和客户私有数据集上保持了零样本性能。", "conclusion": "zELO是一种有效的训练方法，能够利用无监督数据训练出高性能、多功能且在多个领域表现卓越的开源重排序模型，其性能甚至优于闭源专有解决方案。"}}
{"id": "2509.12662", "categories": ["cs.CL", "I.2.7; I.4.9"], "pdf": "https://arxiv.org/pdf/2509.12662", "abs": "https://arxiv.org/abs/2509.12662", "authors": ["Zequn Xie", "Chuxin Wang", "Sihang Cai", "Yeqiang Wang", "Shulei Wang", "Tao Jin"], "title": "Chat-Driven Text Generation and Interaction for Person Retrieval", "comment": "Accepted by EMNLP 2025. 13 pages, 3 figures", "summary": "Text-based person search (TBPS) enables the retrieval of person images from\nlarge-scale databases using natural language descriptions, offering critical\nvalue in surveillance applications. However, a major challenge lies in the\nlabor-intensive process of obtaining high-quality textual annotations, which\nlimits scalability and practical deployment. To address this, we introduce two\ncomplementary modules: Multi-Turn Text Generation (MTG) and Multi-Turn Text\nInteraction (MTI). MTG generates rich pseudo-labels through simulated dialogues\nwith MLLMs, producing fine-grained and diverse visual descriptions without\nmanual supervision. MTI refines user queries at inference time through dynamic,\ndialogue-based reasoning, enabling the system to interpret and resolve vague,\nincomplete, or ambiguous descriptions - characteristics often seen in\nreal-world search scenarios. Together, MTG and MTI form a unified and\nannotation-free framework that significantly improves retrieval accuracy,\nrobustness, and usability. Extensive evaluations demonstrate that our method\nachieves competitive or superior results while eliminating the need for manual\ncaptions, paving the way for scalable and practical deployment of TBPS systems.", "AI": {"tldr": "本文提出一个无需人工标注的文本行人搜索框架，通过多轮文本生成（MTG）生成伪标签，并利用多轮文本交互（MTI）在推理时细化用户查询，显著提高检索精度和实用性。", "motivation": "文本行人搜索（TBPS）在监控应用中具有重要价值，但获取高质量文本标注的过程劳动密集，限制了其可扩展性和实际部署。", "method": "引入了两个互补模块：1. 多轮文本生成（MTG）：通过与多模态大语言模型（MLLMs）模拟对话，生成丰富、细粒度且多样化的视觉描述作为伪标签，无需人工监督。2. 多轮文本交互（MTI）：在推理时通过动态、基于对话的推理来细化用户查询，以解释和解决现实搜索场景中常见的模糊、不完整或模棱两可的描述。", "result": "广泛评估表明，该方法在消除人工标注需求的同时，取得了具有竞争力或更优异的结果，显著提高了检索精度、鲁棒性和可用性。", "conclusion": "所提出的统一无标注框架为可扩展和实际部署的文本行人搜索系统铺平了道路，解决了人工标注的瓶颈问题。"}}
{"id": "2509.12674", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.12674", "abs": "https://arxiv.org/abs/2509.12674", "authors": ["Anna Johansson", "Daniel Lindmark", "Viktor Wiberg", "Martin Servin"], "title": "Safety filtering of robotic manipulation under environment uncertainty: a computational approach", "comment": "8 pages, 8 figures", "summary": "Robotic manipulation in dynamic and unstructured environments requires safety\nmechanisms that exploit what is known and what is uncertain about the world.\nExisting safety filters often assume full observability, limiting their\napplicability in real-world tasks. We propose a physics-based safety filtering\nscheme that leverages high-fidelity simulation to assess control policies under\nuncertainty in world parameters. The method combines dense rollout with nominal\nparameters and parallelizable sparse re-evaluation at critical\nstate-transitions, quantified through generalized factors of safety for stable\ngrasping and actuator limits, and targeted uncertainty reduction through\nprobing actions. We demonstrate the approach in a simulated bimanual\nmanipulation task with uncertain object mass and friction, showing that unsafe\ntrajectories can be identified and filtered efficiently. Our results highlight\nphysics-based sparse safety evaluation as a scalable strategy for safe robotic\nmanipulation under uncertainty.", "AI": {"tldr": "本文提出了一种基于物理的稀疏安全过滤方案，利用高保真模拟和不确定性参数评估控制策略，以实现机器人动态和非结构化环境下的安全操作。", "motivation": "机器人需要在动态和非结构化环境中安全操作，但现有的安全过滤器通常假设完全可观测性，这限制了它们在实际任务中的适用性。", "method": "该方法结合了使用标称参数的密集展开（dense rollout）和在关键状态转换点进行可并行化的稀疏重新评估（sparse re-evaluation）。通过广义安全系数（generalized factors of safety）量化稳定抓取和执行器限制，并通过探测动作（probing actions）实现有针对性的不确定性减少。", "result": "在模拟的双臂操作任务中，面对不确定的物体质量和摩擦力，该方法能够高效地识别并过滤不安全的轨迹。", "conclusion": "研究结果表明，基于物理的稀疏安全评估是应对不确定性环境下机器人安全操作的一种可扩展策略。"}}
{"id": "2509.12430", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12430", "abs": "https://arxiv.org/abs/2509.12430", "authors": ["Mayank Patel", "Rahul Jain", "Asim Unmesh", "Karthik Ramani"], "title": "DYNAMO: Dependency-Aware Deep Learning Framework for Articulated Assembly Motion Prediction", "comment": null, "summary": "Understanding the motion of articulated mechanical assemblies from static\ngeometry remains a core challenge in 3D perception and design automation. Prior\nwork on everyday articulated objects such as doors and laptops typically\nassumes simplified kinematic structures or relies on joint annotations.\nHowever, in mechanical assemblies like gears, motion arises from geometric\ncoupling, through meshing teeth or aligned axes, making it difficult for\nexisting methods to reason about relational motion from geometry alone. To\naddress this gap, we introduce MechBench, a benchmark dataset of 693 diverse\nsynthetic gear assemblies with part-wise ground-truth motion trajectories.\nMechBench provides a structured setting to study coupled motion, where part\ndynamics are induced by contact and transmission rather than predefined joints.\nBuilding on this, we propose DYNAMO, a dependency-aware neural model that\npredicts per-part SE(3) motion trajectories directly from segmented CAD point\nclouds. Experiments show that DYNAMO outperforms strong baselines, achieving\naccurate and temporally consistent predictions across varied gear\nconfigurations. Together, MechBench and DYNAMO establish a novel systematic\nframework for data-driven learning of coupled mechanical motion in CAD\nassemblies.", "AI": {"tldr": "本文提出MechBench数据集和DYNAMO神经网络模型，旨在从CAD点云中学习复杂机械装配体（如齿轮）的耦合运动，实现高精度和时间一致性的运动轨迹预测。", "motivation": "现有方法在理解机械装配体（如齿轮）的运动方面存在不足，因为其运动源于几何耦合（通过啮合齿或对齐轴），而非简化的运动学结构或预定义关节。这使得仅凭几何信息推断关系运动变得困难。", "method": "本文引入MechBench，一个包含693个合成齿轮装配体的基准数据集，提供逐零件的真实运动轨迹，用于研究耦合运动。在此基础上，提出DYNAMO，一个依赖感知神经网络模型，能够直接从分割的CAD点云预测每个零件的SE(3)运动轨迹。", "result": "实验结果表明，DYNAMO模型在各种齿轮配置下均优于现有基线方法，实现了准确且时间一致的预测。", "conclusion": "MechBench数据集和DYNAMO模型共同建立了一个新颖的系统框架，用于CAD装配体中耦合机械运动的数据驱动学习。"}}
{"id": "2509.12944", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.12944", "abs": "https://arxiv.org/abs/2509.12944", "authors": ["Felix Wieberneit", "Emanuele Crisostomi", "Wynita Griggs", "Robert Shorten"], "title": "Momentum-Based Access and Speed Control for Improved Safety in Heterogeneous Road Networks", "comment": null, "summary": "The increasing variety of means of transportation, including light vehicles\nlike e-scooters and e-bikes, together with the increasing weight of\nconventional vehicles due to electrification and consumer preferences for SUVs,\nare raising serious concerns regarding the safety of road networks. In this\npaper we design a two-level control algorithm to improve the safety of\nheterogeneous networks: first, an access control strategy decreases the\nheterogeneity of the network depending on actual traffic conditions; then, a\nspeed control strategy mitigates the probability of serious injuries in\npotential collisions. Both control strategies are designed based on momentum\nconsiderations, as this is regarded as the most influential variable to assess\ninjury risk. The road network mobility simulator SUMO is adopted to implement\nand validate our proposed control strategies.", "AI": {"tldr": "本文提出了一种两级控制算法，通过准入控制降低交通异质性，并通过速度控制减轻碰撞伤害，从而提高异质交通网络的安全性，该算法基于动量考虑，并在SUMO模拟器中进行了验证。", "motivation": "交通工具种类日益增多（如电动滑板车、电动自行车等轻型车辆），同时传统车辆因电动化和消费者偏好SUV而重量增加，导致道路网络的异质性加剧，引发了对道路安全的严重担忧。", "method": "设计了一个两级控制算法：首先，基于实际交通状况的准入控制策略以降低网络异质性；其次，速度控制策略旨在降低潜在碰撞中严重受伤的可能性。这两种策略均基于动量考量设计，因为动量被认为是评估受伤风险最有影响力的变量。采用SUMO道路网络移动模拟器来实施和验证所提出的控制策略。", "result": "通过在SUMO模拟器中实施和验证，证明了所提出的基于动量的两级控制策略（准入控制和速度控制）能够有效改善异质交通网络的安全状况，通过降低网络异质性和减轻碰撞伤害风险。", "conclusion": "所提出的基于动量考量的两级控制算法，通过准入控制和速度控制，能够有效提高异质道路网络的安全水平，是应对当前交通安全挑战的有效方法。"}}
{"id": "2509.12543", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.12543", "abs": "https://arxiv.org/abs/2509.12543", "authors": ["Harshit Rajgarhia", "Shivali Dalmia", "Mengyang Zhao", "Mukherji Abhishek", "Kiran Ganesh"], "title": "Human + AI for Accelerating Ad Localization Evaluation", "comment": null, "summary": "Adapting advertisements for multilingual audiences requires more than simple\ntext translation; it demands preservation of visual consistency, spatial\nalignment, and stylistic integrity across diverse languages and formats. We\nintroduce a structured framework that combines automated components with human\noversight to address the complexities of advertisement localization. To the\nbest of our knowledge, this is the first work to integrate scene text\ndetection, inpainting, machine translation (MT), and text reimposition\nspecifically for accelerating ad localization evaluation workflows. Qualitative\nresults across six locales demonstrate that our approach produces semantically\naccurate and visually coherent localized advertisements, suitable for\ndeployment in real-world workflows.", "AI": {"tldr": "本文提出一个结合自动化工具与人工监督的结构化框架，用于广告本地化，旨在保持视觉一致性、空间对齐和风格完整性，并加速评估流程。", "motivation": "为多语言受众调整广告不仅仅是文本翻译，还需要在不同语言和格式中保持视觉一致性、空间对齐和风格完整性，这是一个复杂挑战。", "method": "引入了一个结构化框架，该框架结合了自动化组件（场景文本检测、图像修复、机器翻译和文本重新放置）与人工监督，专门用于加速广告本地化评估工作流。据称这是首次将这些技术整合用于此目的。", "result": "在六个地区进行的定性结果表明，该方法能够生成语义准确且视觉连贯的本地化广告。", "conclusion": "该方法适用于实际工作流中的部署，能够有效解决广告本地化的复杂性问题。"}}
{"id": "2509.12672", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.12672", "abs": "https://arxiv.org/abs/2509.12672", "authors": ["Shaz Furniturewala", "Arkaitz Zubiaga"], "title": "Towards Inclusive Toxic Content Moderation: Addressing Vulnerabilities to Adversarial Attacks in Toxicity Classifiers Tackling LLM-generated Content", "comment": null, "summary": "The volume of machine-generated content online has grown dramatically due to\nthe widespread use of Large Language Models (LLMs), leading to new challenges\nfor content moderation systems. Conventional content moderation classifiers,\nwhich are usually trained on text produced by humans, suffer from\nmisclassifications due to LLM-generated text deviating from their training data\nand adversarial attacks that aim to avoid detection. Present-day defence\ntactics are reactive rather than proactive, since they rely on adversarial\ntraining or external detection models to identify attacks. In this work, we aim\nto identify the vulnerable components of toxicity classifiers that contribute\nto misclassification, proposing a novel strategy based on mechanistic\ninterpretability techniques. Our study focuses on fine-tuned BERT and RoBERTa\nclassifiers, testing on diverse datasets spanning a variety of minority groups.\nWe use adversarial attacking techniques to identify vulnerable circuits.\nFinally, we suppress these vulnerable circuits, improving performance against\nadversarial attacks. We also provide demographic-level insights into these\nvulnerable circuits, exposing fairness and robustness gaps in model training.\nWe find that models have distinct heads that are either crucial for performance\nor vulnerable to attack and suppressing the vulnerable heads improves\nperformance on adversarial input. We also find that different heads are\nresponsible for vulnerability across different demographic groups, which can\ninform more inclusive development of toxicity detection models.", "AI": {"tldr": "针对大型语言模型（LLM）生成内容带来的挑战，本研究利用机械可解释性技术识别并抑制BERT和RoBERTa毒性分类器中易受攻击的组件，以提高其对对抗性攻击的鲁棒性，并揭示不同人口群体间的公平性差距。", "motivation": "LLM生成内容的大量涌现导致传统内容审核分类器（通常在人类文本上训练）因数据偏差和对抗性攻击而产生误分类。现有防御策略多为被动反应，而非主动识别和修复模型内部的脆弱性。", "method": "研究对象是微调的BERT和RoBERTa毒性分类器，通过对抗性攻击技术识别模型中脆弱的“电路”（如注意力头）。利用机械可解释性技术分析这些组件，并在包含多种少数群体的多样化数据集上进行测试。最终，通过抑制这些脆弱电路来改善模型性能。", "result": "研究发现模型中存在对性能至关重要或易受攻击的独立注意力头。抑制这些脆弱的注意力头可以提高模型在对抗性输入上的表现。此外，不同的人口群体在模型中表现出不同的脆弱注意力头，这揭示了模型训练中存在的公平性和鲁棒性缺陷。", "conclusion": "机械可解释性技术能够主动识别并修复毒性分类器中的脆弱组件，从而提高模型对抗性攻击的鲁棒性。对不同人口群体脆弱性的深入理解，有助于开发更具包容性的毒性检测模型。"}}
{"id": "2509.12702", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.12702", "abs": "https://arxiv.org/abs/2509.12702", "authors": ["Hongrui Zhao", "Xunlan Zhou", "Boris Ivanovic", "Negar Mehr"], "title": "UDON: Uncertainty-weighted Distributed Optimization for Multi-Robot Neural Implicit Mapping under Extreme Communication Constraints", "comment": null, "summary": "Multi-robot mapping with neural implicit representations enables the compact\nreconstruction of complex environments. However, it demands robustness against\ncommunication challenges like packet loss and limited bandwidth. While prior\nworks have introduced various mechanisms to mitigate communication disruptions,\nperformance degradation still occurs under extremely low communication success\nrates. This paper presents UDON, a real-time multi-agent neural implicit\nmapping framework that introduces a novel uncertainty-weighted distributed\noptimization to achieve high-quality mapping under severe communication\ndeterioration. The uncertainty weighting prioritizes more reliable portions of\nthe map, while the distributed optimization isolates and penalizes mapping\ndisagreement between individual pairs of communicating agents. We conduct\nextensive experiments on standard benchmark datasets and real-world robot\nhardware. We demonstrate that UDON significantly outperforms existing\nbaselines, maintaining high-fidelity reconstructions and consistent scene\nrepresentations even under extreme communication degradation (as low as 1%\nsuccess rate).", "AI": {"tldr": "UDON是一个实时多智能体神经隐式建图框架，通过新颖的不确定性加权分布式优化，在极低通信成功率下仍能实现高质量建图。", "motivation": "多机器人神经隐式建图能紧凑重建复杂环境，但面临通信挑战（如丢包、带宽限制），在极低通信成功率下性能会显著下降，现有方法无法有效解决。", "method": "本文提出了UDON框架，核心是其不确定性加权分布式优化方法。不确定性加权优先处理地图中更可靠的部分，而分布式优化则隔离并惩罚通信代理对之间在建图上的分歧。", "result": "UDON在标准基准数据集和真实机器人硬件上进行了广泛实验，结果表明它显著优于现有基线，即使在极端通信退化（成功率低至1%）的情况下，也能保持高保真重建和一致的场景表示。", "conclusion": "UDON能够有效解决多智能体神经隐式建图在严重通信受损下的性能问题，实现了鲁棒且高质量的建图。"}}
{"id": "2509.12442", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.12442", "abs": "https://arxiv.org/abs/2509.12442", "authors": ["Rui-Feng Wang", "Mingrui Xu", "Matthew C Bauer", "Iago Beffart Schardong", "Xiaowen Ma", "Kangning Cui"], "title": "Cott-ADNet: Lightweight Real-Time Cotton Boll and Flower Detection Under Field Conditions", "comment": "14 pages, 5 figures, 1 table", "summary": "Cotton is one of the most important natural fiber crops worldwide, yet\nharvesting remains limited by labor-intensive manual picking, low efficiency,\nand yield losses from missing the optimal harvest window. Accurate recognition\nof cotton bolls and their maturity is therefore essential for automation, yield\nestimation, and breeding research. We propose Cott-ADNet, a lightweight\nreal-time detector tailored to cotton boll and flower recognition under complex\nfield conditions. Building on YOLOv11n, Cott-ADNet enhances spatial\nrepresentation and robustness through improved convolutional designs, while\nintroducing two new modules: a NeLU-enhanced Global Attention Mechanism to\nbetter capture weak and low-contrast features, and a Dilated Receptive Field\nSPPF to expand receptive fields for more effective multi-scale context modeling\nat low computational cost. We curate a labeled dataset of 4,966 images, and\nrelease an external validation set of 1,216 field images to support future\nresearch. Experiments show that Cott-ADNet achieves 91.5% Precision, 89.8%\nRecall, 93.3% mAP50, 71.3% mAP, and 90.6% F1-Score with only 7.5 GFLOPs,\nmaintaining stable performance under multi-scale and rotational variations.\nThese results demonstrate Cott-ADNet as an accurate and efficient solution for\nin-field deployment, and thus provide a reliable basis for automated cotton\nharvesting and high-throughput phenotypic analysis. Code and dataset is\navailable at https://github.com/SweefongWong/Cott-ADNet.", "AI": {"tldr": "本文提出了Cott-ADNet，一个基于YOLOv11n的轻量级实时检测器，用于复杂田间条件下棉铃和棉花的识别。该模型通过改进的卷积设计、NeLU增强的全局注意力机制和扩张感受野SPPF模块，实现了高精度和高效率，为自动化采摘和表型分析提供了可靠基础。", "motivation": "棉花采摘仍受限于劳动密集型人工采摘、效率低下以及错过最佳采摘窗口导致的产量损失。因此，准确识别棉铃及其成熟度对于自动化、产量估算和育种研究至关重要。", "method": "本文提出了Cott-ADNet，一个基于YOLOv11n的轻量级实时检测器。它通过改进的卷积设计增强了空间表示和鲁棒性，并引入了两个新模块：NeLU增强的全局注意力机制（用于捕获弱和低对比度特征）和扩张感受野SPPF（以低计算成本扩展感受野以实现更有效的多尺度上下文建模）。此外，还整理了一个包含4,966张图像的标注数据集，并发布了一个包含1,216张田间图像的外部验证集。", "result": "实验表明，Cott-ADNet在仅7.5 GFLOPs的计算量下，达到了91.5%的精确率、89.8%的召回率、93.3%的mAP50、71.3%的mAP和90.6%的F1-分数，并在多尺度和旋转变化下保持了稳定的性能。", "conclusion": "这些结果表明Cott-ADNet是一种准确高效的田间部署解决方案，为自动化棉花采摘和高通量表型分析提供了可靠的基础。"}}
{"id": "2509.13166", "categories": ["eess.SY", "cs.LG", "cs.SY", "eess.SP", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.13166", "abs": "https://arxiv.org/abs/2509.13166", "authors": ["Filippo Fabiani", "Andrea Simonetto"], "title": "Concentration inequalities for semidefinite least squares based on data", "comment": null, "summary": "We study data-driven least squares (LS) problems with semidefinite (SD)\nconstraints and derive finite-sample guarantees on the spectrum of their\noptimal solutions when these constraints are relaxed. In particular, we provide\na high confidence bound allowing one to solve a simpler program in place of the\nfull SDLS problem, while ensuring that the eigenvalues of the resulting\nsolution are $\\varepsilon$-close of those enforced by the SD constraints. The\ndeveloped certificate, which consistently shrinks as the number of data\nincreases, turns out to be easy-to-compute, distribution-free, and only\nrequires independent and identically distributed samples. Moreover, when the\nSDLS is used to learn an unknown quadratic function, we establish bounds on the\nerror between a gradient descent iterate minimizing the surrogate cost obtained\nwith no SD constraints and the true minimizer.", "AI": {"tldr": "本文研究数据驱动的带半定（SD）约束的最小二乘（LS）问题，并通过放宽这些约束，为其最优解的谱提供了有限样本保证。", "motivation": "动机是为了解决全SDLS问题的复杂性，并提供一种更简单的替代方案，同时仍能保证解的质量。", "method": "通过推导一个高置信度界限，允许用一个更简单的程序代替完整的SDLS问题。这个界限是易于计算、与分布无关的，并且只需要独立同分布的样本。", "result": "研究结果表明，通过放宽约束得到的解的特征值与SD约束强制的特征值之间的距离在$\\varepsilon$范围内。该证书随着数据量的增加而缩小。此外，当SDLS用于学习未知二次函数时，还建立了无SD约束的替代成本的梯度下降迭代与真实最小值之间的误差界限。", "conclusion": "研究提供了一种在解决SDLS问题时，通过放宽约束来简化计算的方法，并提供了关于其最优解谱的强有力、易于计算且与分布无关的有限样本保证。"}}
{"id": "2509.12589", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12589", "abs": "https://arxiv.org/abs/2509.12589", "authors": ["Garima Agrawal", "Riccardo De Maria", "Kiran Davuluri", "Daniele Spera", "Charlie Read", "Cosimo Spera", "Jack Garrett", "Don Miller"], "title": "Redefining CX with Agentic AI: Minerva CQ Case Study", "comment": null, "summary": "Despite advances in AI for contact centers, customer experience (CX)\ncontinues to suffer from high average handling time (AHT), low first-call\nresolution, and poor customer satisfaction (CSAT). A key driver is the\ncognitive load on agents, who must navigate fragmented systems, troubleshoot\nmanually, and frequently place customers on hold. Existing AI-powered\nagent-assist tools are often reactive driven by static rules, simple prompting,\nor retrieval-augmented generation (RAG) without deeper contextual reasoning. We\nintroduce Agentic AI goal-driven, autonomous, tool-using systems that\nproactively support agents in real time. Unlike conventional approaches,\nAgentic AI identifies customer intent, triggers modular workflows, maintains\nevolving context, and adapts dynamically to conversation state. This paper\npresents a case study of Minerva CQ, a real-time Agent Assist product deployed\nin voice-based customer support. Minerva CQ integrates real-time transcription,\nintent and sentiment detection, entity recognition, contextual retrieval,\ndynamic customer profiling, and partial conversational summaries enabling\nproactive workflows and continuous context-building. Deployed in live\nproduction, Minerva CQ acts as an AI co-pilot, delivering measurable\nimprovements in agent efficiency and customer experience across multiple\ndeployments.", "AI": {"tldr": "本文介绍了一种名为“Agentic AI”的目标驱动型自主AI系统，通过Minerva CQ产品在实时语音客户支持中部署，旨在主动协助客服代表，从而提高效率并改善客户体验。", "motivation": "尽管AI在联络中心有所发展，但客户体验仍因高平均处理时间（AHT）、低首次呼叫解决率和差的客户满意度（CSAT）而受损。主要原因是客服代表面临认知负荷，现有AI辅助工具（如基于规则、简单提示或RAG）通常是被动且缺乏深层上下文推理能力。", "method": "引入Agentic AI，这是一种目标驱动、自主、工具使用的系统，能主动实时支持客服代表。它能识别客户意图、触发模块化工作流、维护不断演变的上下文并动态适应对话状态。本文以Minerva CQ为例，该产品集成了实时转录、意图和情感检测、实体识别、上下文检索、动态客户画像和部分对话摘要，以实现主动工作流和持续上下文构建。", "result": "Minerva CQ作为AI副驾驶，在多个部署中实现了客服效率和客户体验的显著可衡量改进。", "conclusion": "Agentic AI，通过Minerva CQ产品在实时客户支持中的成功部署，证明了其能够作为AI副驾驶，通过主动协助客服代表来显著提升效率和改善客户体验。"}}
{"id": "2509.12677", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.12677", "abs": "https://arxiv.org/abs/2509.12677", "authors": ["Hiroyuki Deguchi", "Masaaki Nagata"], "title": "Case-Based Decision-Theoretic Decoding with Quality Memories", "comment": "Accepted at EMNLP2025 main", "summary": "Minimum Bayes risk (MBR) decoding is a decision rule of text generation,\nwhich selects the hypothesis that maximizes the expected utility and robustly\ngenerates higher-quality texts than maximum a posteriori (MAP) decoding.\nHowever, it depends on sample texts drawn from the text generation model; thus,\nit is difficult to find a hypothesis that correctly captures the knowledge or\ninformation of out-of-domain. To tackle this issue, we propose case-based\ndecision-theoretic (CBDT) decoding, another method to estimate the expected\nutility using examples of domain data. CBDT decoding not only generates\nhigher-quality texts than MAP decoding, but also the combination of MBR and\nCBDT decoding outperformed MBR decoding in seven domain De--En and\nJa$\\leftrightarrow$En translation tasks and image captioning tasks on MSCOCO\nand nocaps datasets.", "AI": {"tldr": "本文提出了一种基于案例的决策理论（CBDT）解码方法，用于解决最小贝叶斯风险（MBR）解码在域外知识捕获上的局限性。CBDT解码通过使用领域数据示例来估计预期效用，并与MBR解码结合后，在多项翻译和图像字幕任务中超越了单独的MBR解码。", "motivation": "最小贝叶斯风险（MBR）解码虽然能生成高质量文本，但它依赖于从文本生成模型中抽取的样本，导致难以正确捕获域外知识或信息。", "method": "提出了一种基于案例的决策理论（CBDT）解码方法，该方法利用领域数据示例来估计预期效用。此外，还研究了将CBDT解码与MBR解码相结合的策略。", "result": "CBDT解码不仅能生成比最大后验（MAP）解码更高质量的文本，而且MBR和CBDT解码的组合在七个德语-英语和日语-英语翻译任务以及MSCOCO和nocaps数据集上的图像字幕任务中，均优于单独的MBR解码。", "conclusion": "CBDT解码有效地解决了MBR解码在处理域外知识时的不足。将CBDT与MBR解码结合可以进一步提升文本生成质量，在多语言翻译和图像字幕等任务中表现出优越性。"}}
{"id": "2509.12714", "categories": ["cs.RO", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.12714", "abs": "https://arxiv.org/abs/2509.12714", "authors": ["Kit-Wa Sou", "Junhao Gong", "Shoujie Li", "Chuqiao Lyu", "Ziwu Song", "Shilong Mu", "Wenbo Ding"], "title": "MoiréTac: A Dual-Mode Visuotactile Sensor for Multidimensional Perception Using Moiré Pattern Amplification", "comment": null, "summary": "Visuotactile sensors typically employ sparse marker arrays that limit spatial\nresolution and lack clear analytical force-to-image relationships. To solve\nthis problem, we present \\textbf{Moir\\'eTac}, a dual-mode sensor that generates\ndense interference patterns via overlapping micro-gratings within a transparent\narchitecture. When two gratings overlap with misalignment, they create moir\\'e\npatterns that amplify microscopic deformations. The design preserves optical\nclarity for vision tasks while producing continuous moir\\'e fields for tactile\nsensing, enabling simultaneous 6-axis force/torque measurement, contact\nlocalization, and visual perception. We combine physics-based features\n(brightness, phase gradient, orientation, and period) from moir\\'e patterns\nwith deep spatial features. These are mapped to 6-axis force/torque\nmeasurements, enabling interpretable regression through end-to-end learning.\nExperimental results demonstrate three capabilities: force/torque measurement\nwith R^2 > 0.98 across tested axes; sensitivity tuning through geometric\nparameters (threefold gain adjustment); and vision functionality for object\nclassification despite moir\\'e overlay. Finally, we integrate the sensor into a\nrobotic arm for cap removal with coordinated force and torque control,\nvalidating its potential for dexterous manipulation.", "AI": {"tldr": "MoiréTac是一种双模视觉触觉传感器，利用重叠微光栅产生莫尔条纹，实现高分辨率触觉感知和视觉功能，能够同时进行六轴力/扭矩测量、接触定位和视觉感知，并已成功应用于机器人灵巧操作。", "motivation": "现有视觉触觉传感器通常使用稀疏标记阵列，导致空间分辨率有限，并且缺乏明确的力与图像之间的分析关系。", "method": "该传感器通过透明架构中重叠的微光栅生成密集的莫尔干涉图案，莫尔图案能放大微观变形。它结合了莫尔图案的物理特征（亮度、相位梯度、方向和周期）与深度空间特征，并通过端到端学习将其映射到六轴力/扭矩测量。设计同时保留了光学清晰度以进行视觉任务。", "result": "实验结果表明，该传感器在所有测试轴上实现了R^2 > 0.98的力/扭矩测量精度；可以通过几何参数调节灵敏度（三倍增益调整）；即使有莫尔条纹叠加，也能保持物体分类的视觉功能。此外，该传感器成功集成到机械臂中，实现了带协调力/扭矩控制的瓶盖移除。", "conclusion": "MoiréTac传感器通过其同时进行六轴力/扭矩测量、接触定位和视觉感知的能力，验证了其在灵巧操作方面的巨大潜力。"}}
{"id": "2509.12452", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12452", "abs": "https://arxiv.org/abs/2509.12452", "authors": ["Zhenxin Zhang", "Zhihua Xu", "Yuwei Cao", "Ningli Xu", "Shuye Wang", "Shen'ao Cui", "Zhen Li", "Rongjun Qin"], "title": "Deep learning for 3D point cloud processing -- from approaches, tasks to its implications on urban and environmental applications", "comment": "57 Pages, 4 Figures", "summary": "Point cloud processing as a fundamental task in the field of geomatics and\ncomputer vision, has been supporting tasks and applications at different scales\nfrom air to ground, including mapping, environmental monitoring, urban/tree\nstructure modeling, automated driving, robotics, disaster responses etc. Due to\nthe rapid development of deep learning, point cloud processing algorithms have\nnowadays been almost explicitly dominated by learning-based approaches, most of\nwhich are yet transitioned into real-world practices. Existing surveys\nprimarily focus on the ever-updating network architecture to accommodate\nunordered point clouds, largely ignoring their practical values in typical\npoint cloud processing applications, in which extra-large volume of data,\ndiverse scene contents, varying point density, data modality need to be\nconsidered. In this paper, we provide a meta review on deep learning approaches\nand datasets that cover a selection of critical tasks of point cloud processing\nin use such as scene completion, registration, semantic segmentation, and\nmodeling. By reviewing a broad range of urban and environmental applications\nthese tasks can support, we identify gaps to be closed as these methods\ntransformed into applications and draw concluding remarks in both the\nalgorithmic and practical aspects of the surveyed methods.", "AI": {"tldr": "本文对深度学习在点云处理中的应用进行了元综述，重点关注其在实际应用中的价值和挑战，并指出了将这些方法转化为实际应用时存在的差距。", "motivation": "现有综述主要关注不断更新的网络架构以适应无序点云，却大多忽视了其在典型点云处理应用中的实际价值，这些应用需要考虑超大数据量、多样化的场景内容、变化的密度和数据模态。", "method": "本文对深度学习方法和数据集进行了元综述，涵盖了场景补全、配准、语义分割和建模等关键点云处理任务，并通过回顾这些任务所支持的广泛城市和环境应用来识别差距。", "result": "本文识别了在将深度学习方法转化为实际应用过程中需要弥补的差距。", "conclusion": "本文对所综述方法的算法和实践方面都提出了总结性意见，旨在弥合深度学习方法从研究到实际应用之间的鸿沟。"}}
{"id": "2509.13257", "categories": ["eess.SY", "cs.RO", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.13257", "abs": "https://arxiv.org/abs/2509.13257", "authors": ["Sriram S. K. S. Narayanan", "Sajad Ahmadi", "Javad Mohammadpour Velni", "Umesh Vaidya"], "title": "Safety Critical Model Predictive Control Using Discrete-Time Control Density Functions", "comment": null, "summary": "This paper presents MPC-CDF, a new approach integrating control density\nfunctions (CDFs) within a model predictive control (MPC) framework to ensure\nsafety-critical control in nonlinear dynamical systems. By using the dual\nformulation of the navigation problem, we incorporate CDFs into the MPC\nframework, ensuring both convergence and safety in a discrete-time setting.\nThese density functions are endowed with a physical interpretation, where the\nassociated measure signifies the occupancy of system trajectories. Leveraging\nthis occupancy-based perspective, we synthesize safety-critical controllers\nusing the proposed MPC-CDF framework. We illustrate the safety properties of\nthis framework using a unicycle model and compare it with a control barrier\nfunction-based method. The efficacy of this approach is demonstrated in the\nautonomous safe navigation of an underwater vehicle, which avoids complex and\narbitrary obstacles while achieving the desired level of safety.", "AI": {"tldr": "本文提出MPC-CDF框架，将控制密度函数(CDFs)整合到模型预测控制(MPC)中，以实现非线性动态系统的安全关键控制。", "motivation": "需要在非线性动态系统中确保安全关键控制，并同时保证收敛性。", "method": "通过导航问题的对偶公式，将控制密度函数(CDFs)整合到MPC框架中，确保离散时间设置下的收敛性和安全性。CDF被赋予物理意义，其关联测度表示系统轨迹的占用率。利用这种基于占用率的视角合成安全关键控制器。", "result": "该框架确保了收敛性和安全性，并通过独轮车模型和水下航行器自主安全导航进行了验证。与基于控制障碍函数的方法相比，它能有效避免复杂任意障碍物并达到所需的安全性水平。", "conclusion": "MPC-CDF框架能有效合成非线性动态系统的安全关键控制器，并已在实际应用中展示了其有效性和安全性。"}}
{"id": "2509.12592", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.12592", "abs": "https://arxiv.org/abs/2509.12592", "authors": ["Aaron Baughman", "Gozde Akay", "Eduardo Morales", "Rahul Agarwal", "Preetika Srivastava"], "title": "Match Chat: Real Time Generative AI and Generative Computing for Tennis", "comment": "12 pages, 5 Figures, 4 Tables", "summary": "We present Match Chat, a real-time, agent-driven assistant designed to\nenhance the tennis fan experience by delivering instant, accurate responses to\nmatch-related queries. Match Chat integrates Generative Artificial Intelligence\n(GenAI) with Generative Computing (GenComp) techniques to synthesize key\ninsights during live tennis singles matches. The system debuted at the 2025\nWimbledon Championships and the 2025 US Open, where it provided about 1 million\nusers with seamless access to streaming and static data through natural\nlanguage queries. The architecture is grounded in an Agent-Oriented\nArchitecture (AOA) combining rule engines, predictive models, and agents to\npre-process and optimize user queries before passing them to GenAI components.\nThe Match Chat system had an answer accuracy of 92.83% with an average response\ntime of 6.25 seconds under loads of up to 120 requests per second (RPS). Over\n96.08% of all queries were guided using interactive prompt design, contributing\nto a user experience that prioritized clarity, responsiveness, and minimal\neffort. The system was designed to mask architectural complexity, offering a\nfrictionless and intuitive interface that required no onboarding or technical\nfamiliarity. Across both Grand Slam deployments, Match Chat maintained 100%\nuptime and supported nearly 1 million unique users, underscoring the\nscalability and reliability of the platform. This work introduces key design\npatterns for real-time, consumer-facing AI systems that emphasize speed,\nprecision, and usability that highlights a practical path for deploying\nperformant agentic systems in dynamic environments.", "AI": {"tldr": "Match Chat是一个实时、由智能体驱动的网球助理，它结合了生成式AI和生成式计算，旨在为球迷提供即时、准确的比赛相关查询响应。该系统在2025年温网和美网成功部署，服务了近百万用户，并展示了高准确性、低延迟和高可用性。", "motivation": "该研究的动机是希望通过提供即时、准确的比赛相关查询响应，来增强网球球迷的观赛体验。", "method": "Match Chat系统整合了生成式人工智能（GenAI）和生成式计算（GenComp）技术。其架构基于智能体导向架构（AOA），结合了规则引擎、预测模型和智能体来预处理和优化用户查询，然后将其传递给GenAI组件。系统还采用了交互式提示设计来引导用户查询。", "result": "Match Chat系统在2025年温网和美网首次亮相，为约100万用户提供了服务。它在高达每秒120个请求的负载下，实现了92.83%的答案准确率和平均6.25秒的响应时间。超过96.08%的查询通过交互式提示设计进行引导。在两次大满贯部署中，系统保持了100%的正常运行时间，并支持了近100万独立用户，展现了其可扩展性和可靠性。", "conclusion": "该工作为实时、面向消费者的AI系统引入了关键的设计模式，强调了速度、精确性和可用性。它为在动态环境中部署高性能智能体系统提供了一条实用的途径。"}}
{"id": "2509.12720", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.12720", "abs": "https://arxiv.org/abs/2509.12720", "authors": ["Biswadip Mandal", "Anant Khandelwal", "Manish Gupta"], "title": "HistoryBankQA: Multilingual Temporal Question Answering on Historical Events", "comment": null, "summary": "Temporal reasoning about historical events is a critical skill for NLP tasks\nlike event extraction, historical entity linking, temporal question answering,\ntimeline summarization, temporal event clustering and temporal natural language\ninference. Yet efforts on benchmarking temporal reasoning capabilities of large\nlanguage models (LLMs) are rather limited. Existing temporal reasoning datasets\nare limited in scale, lack multilingual coverage and focus more on contemporary\nevents. To address these limitations, we present HistoryBank, a multilingual\ndatabase of 10M+ historical events extracted from Wikipedia timeline pages and\narticle infoboxes. Our database provides unprecedented coverage in both\nhistorical depth and linguistic breadth with 10 languages. Additionally, we\nconstruct a comprehensive question answering benchmark for temporal reasoning\nacross all languages. This benchmark covers a diverse set of 6 temporal QA\nreasoning tasks, and we evaluate a suite of popular language models\n(LLaMA-3-8B, Mistral-7B, Gemma-2-9b, Qwen3-8B, GPT4o) to assess their\nperformance on these tasks. As expected GPT4o performs best across all answer\ntypes and languages; Gemma-2 outperforms the other small language models. Our\nwork aims to provide a comprehensive resource for advancing multilingual and\ntemporally-aware natural language understanding of historical events. To\nfacilitate further research, we will make our code and datasets publicly\navailable upon acceptance of this paper.", "AI": {"tldr": "本文提出了HistoryBank，一个包含1000万+历史事件的多语言数据库，并构建了一个涵盖6种时间推理任务的综合问答基准，用于评估大型语言模型在历史事件时间推理方面的能力。", "motivation": "现有的大型语言模型时间推理基准测试工作有限，数据集规模不足，缺乏多语言覆盖，且主要关注当代事件。为了解决这些局限性，需要一个更全面、多语言、涵盖历史深度的数据集和基准。", "method": "研究人员从维基百科的时间轴页面和文章信息框中提取了1000万+历史事件，构建了HistoryBank多语言数据库，覆盖10种语言。在此基础上，他们构建了一个包含6种不同时间问答推理任务的综合基准，并评估了包括LLaMA-3-8B、Mistral-7B、Gemma-2-9b、Qwen3-8B和GPT4o在内的流行语言模型。", "result": "HistoryBank数据库在历史深度和语言广度上提供了前所未有的覆盖，包含10种语言的1000万+历史事件。在时间推理问答基准测试中，GPT4o在所有答案类型和语言上表现最佳；Gemma-2在其他小型语言模型中表现突出。", "conclusion": "这项工作提供了一个全面的资源，旨在促进多语言和时间感知的历史事件自然语言理解研究。研究成果（代码和数据集）将在论文被接受后公开，以进一步推动相关研究。"}}
{"id": "2509.12723", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.12723", "abs": "https://arxiv.org/abs/2509.12723", "authors": ["Kai Zhang", "Eric Lucet", "Julien Alexandre Dit Sandretto", "Shoubin Chen", "David Filait"], "title": "NAMOUnc: Navigation Among Movable Obstacles with Decision Making on Uncertainty Interval", "comment": "11 pages, ICINCO2025", "summary": "Navigation among movable obstacles (NAMO) is a critical task in robotics,\noften challenged by real-world uncertainties such as observation noise, model\napproximations, action failures, and partial observability. Existing solutions\nfrequently assume ideal conditions, leading to suboptimal or risky decisions.\nThis paper introduces NAMOUnc, a novel framework designed to address these\nuncertainties by integrating them into the decision-making process. We first\nestimate them and compare the corresponding time cost intervals for removing\nand bypassing obstacles, optimizing both the success rate and time efficiency,\nensuring safer and more efficient navigation. We validate our method through\nextensive simulations and real-world experiments, demonstrating significant\nimprovements over existing NAMO frameworks. More details can be found in our\nwebsite: https://kai-zhang-er.github.io/namo-uncertainty/", "AI": {"tldr": "本文提出NAMOUnc框架，通过整合并估计不确定性来解决可移动障碍物导航（NAMO）中的实际挑战，从而实现更安全、高效的导航。", "motivation": "现有的可移动障碍物导航（NAMO）解决方案常假设理想条件，导致在面对观察噪声、模型近似、动作失败和部分可观察性等现实世界不确定性时，决策次优或存在风险。", "method": "本文引入NAMOUnc框架，将不确定性整合到决策过程中。具体方法是首先估计这些不确定性，然后比较移除和绕过障碍物所需的时间成本区间，同时优化成功率和时间效率。", "result": "通过广泛的模拟和真实世界实验验证，NAMOUnc方法在性能上显著优于现有NAMO框架。", "conclusion": "NAMOUnc框架通过有效处理不确定性，能够确保在可移动障碍物导航任务中实现更安全、更高效的导航。"}}
{"id": "2509.12453", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12453", "abs": "https://arxiv.org/abs/2509.12453", "authors": ["Yiran Song", "Yikai Zhang", "Silvia Orengo-Nania", "Nian Wang", "Fenglong Ma", "Rui Zhang", "Yifan Peng", "Mingquan Lin"], "title": "Two-Stage Decoupling Framework for Variable-Length Glaucoma Prognosis", "comment": "11 pages.2 figures, 4 tables", "summary": "Glaucoma is one of the leading causes of irreversible blindness worldwide.\nGlaucoma prognosis is essential for identifying at-risk patients and enabling\ntimely intervention to prevent blindness. Many existing approaches rely on\nhistorical sequential data but are constrained by fixed-length inputs, limiting\ntheir flexibility. Additionally, traditional glaucoma prognosis methods often\nemploy end-to-end models, which struggle with the limited size of glaucoma\ndatasets. To address these challenges, we propose a Two-Stage Decoupling\nFramework (TSDF) for variable-length glaucoma prognosis. In the first stage, we\nemploy a feature representation module that leverages self-supervised learning\nto aggregate multiple glaucoma datasets for training, disregarding differences\nin their supervisory information. This approach enables datasets of varying\nsizes to learn better feature representations. In the second stage, we\nintroduce a temporal aggregation module that incorporates an attention-based\nmechanism to process sequential inputs of varying lengths, ensuring flexible\nand efficient utilization of all available data. This design significantly\nenhances model performance while maintaining a compact parameter size.\nExtensive experiments on two benchmark glaucoma datasets:the Ocular\nHypertension Treatment Study (OHTS) and the Glaucoma Real-world Appraisal\nProgression Ensemble (GRAPE),which differ significantly in scale and clinical\nsettings,demonstrate the effectiveness and robustness of our approach.", "AI": {"tldr": "本文提出了一种两阶段解耦框架（TSDF），用于可变长度的青光眼预后。该框架通过自监督学习进行特征表示，并利用基于注意力的机制进行时间聚合，以解决现有方法在固定长度输入和有限数据集上的局限性。", "motivation": "目前的青光眼预后方法依赖于固定长度的输入，限制了灵活性；同时，端到端模型在面对有限的青光眼数据集时表现不佳。这些问题促使研究者寻求一种能处理可变长度数据并有效利用多源数据的解决方案。", "method": "本文提出一个两阶段解耦框架（TSDF）：\n1. **第一阶段（特征表示模块）**：利用自监督学习聚合多个青光眼数据集进行训练，忽略监督信息的差异，以学习更好的特征表示。\n2. **第二阶段（时间聚合模块）**：引入基于注意力机制来处理可变长度的序列输入，确保所有可用数据都能被灵活高效地利用。", "result": "该方法显著提升了模型性能，同时保持了紧凑的参数规模。在两个规模和临床设置差异显著的基准青光眼数据集（OHTS和GRAPE）上的广泛实验证明了其有效性和鲁棒性。", "conclusion": "所提出的两阶段解耦框架（TSDF）成功解决了青光眼预后中可变长度输入和有限数据集的挑战，显著提高了模型性能和鲁棒性，为青光眼患者的及时干预提供了更好的预测能力。"}}
{"id": "2509.12739", "categories": ["cs.RO", "cs.AI", "cs.ET", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.12739", "abs": "https://arxiv.org/abs/2509.12739", "authors": ["Trung Kien La", "Eric Guiffo Kaigom"], "title": "Deep Learning for Model-Free Prediction of Thermal States of Robot Joint Motors", "comment": "$\\copyright$ 2025 the authors. This work has been accepted to the\n  10th IFAC Symposium on Mechatronic Systems & 14th IFAC Symposium on Robotics\n  July 15-18, 2025 || Paris, France for publication under a Creative Commons\n  Licence CC-BY-NC-ND", "summary": "In this work, deep neural networks made up of multiple hidden Long Short-Term\nMemory (LSTM) and Feedforward layers are trained to predict the thermal\nbehavior of the joint motors of robot manipulators. A model-free and scalable\napproach is adopted. It accommodates complexity and uncertainty challenges\nstemming from the derivation, identification, and validation of a large number\nof parameters of an approximation model that is hardly available. To this end,\nsensed joint torques are collected and processed to foresee the thermal\nbehavior of joint motors. Promising prediction results of the machine learning\nbased capture of the temperature dynamics of joint motors of a redundant robot\nwith seven joints are presented.", "AI": {"tldr": "本文利用由LSTM和前馈层组成的深度神经网络，采用无模型方法，预测机器人机械臂关节电机的热行为，并取得了良好的预测结果。", "motivation": "传统近似模型在推导、识别和验证大量参数时面临复杂性和不确定性挑战，且模型本身难以获得。因此，需要一种无需复杂模型的方法来预测机器人关节电机的热行为。", "method": "采用无模型的深度神经网络方法，该网络由多个隐藏的长短期记忆（LSTM）层和前馈层组成。通过收集和处理感测到的关节扭矩数据来训练网络，以预测关节电机的热行为。", "result": "研究展示了基于机器学习的方法在捕获一个七关节冗余机器人关节电机温度动态方面的良好预测结果。", "conclusion": "深度神经网络能够有效地、无模型地预测机器人机械臂关节电机的热行为，为解决传统建模的复杂性提供了可行的方案。"}}
{"id": "2509.12602", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.12602", "abs": "https://arxiv.org/abs/2509.12602", "authors": ["Minyu Chen", "Guoqiang Li"], "title": "DaSAThco: Data-Aware SAT Heuristics Combinations Optimization via Large Language Models", "comment": "11 pages", "summary": "The performance of Conflict-Driven Clause Learning solvers hinges on internal\nheuristics, yet the heterogeneity of SAT problems makes a single, universally\noptimal configuration unattainable. While prior automated methods can find\nspecialized configurations for specific problem families, this dataset-specific\napproach lacks generalizability and requires costly re-optimization for new\nproblem types. We introduce DaSAThco, a framework that addresses this challenge\nby learning a generalizable mapping from instance features to tailored\nheuristic ensembles, enabling a train-once, adapt-broadly model. Our framework\nuses a Large Language Model, guided by systematically defined Problem\nArchetypes, to generate a diverse portfolio of specialized heuristic ensembles\nand subsequently learns an adaptive selection mechanism to form the final\nmapping. Experiments show that DaSAThco achieves superior performance and, most\nnotably, demonstrates robust out-of-domain generalization where non-adaptive\nmethods show limitations. Our work establishes a more scalable and practical\npath toward automated algorithm design for complex, configurable systems.", "AI": {"tldr": "DaSAThco是一个框架，它利用大型语言模型和问题原型，学习从SAT实例特征到定制启发式集合的可泛化映射，从而在SAT求解器中实现卓越的性能和强大的域外泛化能力。", "motivation": "CDCL求解器的性能依赖于内部启发式，但SAT问题的多样性使得单一的最优配置难以实现。现有的自动化方法虽然能找到特定问题族的配置，但缺乏泛化性，且在新问题类型上需要昂贵的重新优化。", "method": "DaSAThco框架通过一个大型语言模型（LLM），在系统定义的问题原型指导下，生成多样化的专业启发式集合。随后，它学习一个自适应选择机制，以形成从实例特征到这些启发式集合的最终映射。", "result": "实验表明，DaSAThco取得了卓越的性能，最值得注意的是，它展示了强大的域外泛化能力，而传统非自适应方法在此方面存在局限性。", "conclusion": "这项工作为复杂、可配置系统的自动化算法设计提供了一条更具可扩展性和实用性的路径。"}}
{"id": "2509.12771", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.12771", "abs": "https://arxiv.org/abs/2509.12771", "authors": ["Omri Suissa", "Muhiim Ali", "Shengmai Chen", "Yinuo Cai", "Shekhar Pradhan"], "title": "Contrastive Learning with Enhanced Abstract Representations using Grouped Loss of Abstract Semantic Supervision", "comment": null, "summary": "Humans can recognize an image as an instance of a general concept, beyond\nsimply identifying its objects and their relationships. In this paper, we\ninvestigate 1. The extent to which VLMs have this concept abstraction capacity,\nand 2. Strategies for encoding the sort of higher-concept information in images\nthat would enable the resulting VLM model (CLEAR GLASS model) to have this\ncapability to a greater degree. To this end, we introduce a grouped\nimage-caption dataset (MAGIC), which consists of several groups of image\ncaptions and for each group a set of associated images and higher-level\nconceptual labels. We use a novel contrastive loss technique to induce the\nmodel to encode in the representation of each image (caption) in a group the\ninformation that is common to all members of the image-caption group. Our main\ncontribution is a grouped contrastive loss function based on text-image\ncontrastive groups (outer contrastive loss) as well as an inner loss which\nmeasures the distances between image-caption instances in the group. Our\ntraining methodology results in the CLEAR GLASS model having the concept\nabstraction capacity as an emergent capacity because the model is not exposed\nto the higher-level concepts associated with each group. Instead, the training\nforces the model to create for each image-caption group a semantic\nrepresentation that brings it closer to the semantic representation of the\nhigher-level concepts in the latent semantic space. Our experiments show that\nthis training methodology results in a model which shows improvement in\nabstract concept recognition compared to SOTA models.", "AI": {"tldr": "本文研究了视觉语言模型（VLMs）识别抽象概念的能力，并提出了一种新的训练策略，包括一个分组图像-文本数据集（MAGIC）和一种分组对比损失函数，使CLEAR GLASS模型在不直接接触高层概念的情况下，通过学习组内共性信息，提升了抽象概念识别能力，超越了现有最先进模型。", "motivation": "人类能够识别图像中的抽象概念，而不仅仅是物体及其关系。研究旨在探讨VLMs是否具备这种概念抽象能力，并探索如何编码图像中的高层概念信息，以增强VLMs的这种能力。", "method": "1. 引入了一个分组图像-文本数据集（MAGIC），其中包含分组的图像描述、相关图像和高层概念标签。2. 提出了一种新颖的对比损失技术，包含基于文本-图像对比分组的“外部对比损失”和衡量组内图像-文本实例间距离的“内部损失”。3. 训练方法使模型在不直接接触高层概念的情况下，为每个图像-文本组创建语义表示，使其在潜在语义空间中更接近高层概念的语义表示，从而使概念抽象能力作为一种“涌现能力”出现。", "result": "实验结果表明，这种训练方法使得CLEAR GLASS模型在抽象概念识别方面表现出显著改进，超越了现有最先进（SOTA）模型。", "conclusion": "通过引入MAGIC数据集和新颖的分组对比损失函数，本研究提出的训练方法能够使VLMs（CLEAR GLASS模型）在不直接暴露于高层概念的情况下，自发地获得概念抽象能力，并在抽象概念识别任务上取得了优于SOTA模型的性能。"}}
{"id": "2509.12740", "categories": ["cs.RO", "cs.AI", "cs.ET", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.12740", "abs": "https://arxiv.org/abs/2509.12740", "authors": ["Eric Guiffo Kaigom"], "title": "Deep Generative and Discriminative Digital Twin endowed with Variational Autoencoder for Unsupervised Predictive Thermal Condition Monitoring of Physical Robots in Industry 6.0 and Society 6.0", "comment": "$\\copyright$ 2025 the authors. This work has been accepted to the to\n  the 10th IFAC Symposium on Mechatronic Systems & 14th IFAC Symposium on\n  Robotics July 15-18, 2025 || Paris, France for publication under a Creative\n  Commons Licence CC-BY-NC-ND", "summary": "Robots are unrelentingly used to achieve operational efficiency in Industry\n4.0 along with symbiotic and sustainable assistance for the work-force in\nIndustry 5.0. As resilience, robustness, and well-being are required in\nanti-fragile manufacturing and human-centric societal tasks, an autonomous\nanticipation and adaption to thermal saturation and burns due to motors\noverheating become instrumental for human safety and robot availability. Robots\nare thereby expected to self-sustain their performance and deliver user\nexperience, in addition to communicating their capability to other agents in\nadvance to ensure fully automated thermally feasible tasks, and prolong their\nlifetime without human intervention. However, the traditional robot shutdown,\nwhen facing an imminent thermal saturation, inhibits productivity in factories\nand comfort in the society, while cooling strategies are hard to implement\nafter the robot acquisition. In this work, smart digital twins endowed with\ngenerative AI, i.e., variational autoencoders, are leveraged to manage\nthermally anomalous and generate uncritical robot states. The notion of thermal\ndifficulty is derived from the reconstruction error of variational\nautoencoders. A robot can use this score to predict, anticipate, and share the\nthermal feasibility of desired motion profiles to meet requirements from\nemerging applications in Industry 6.0 and Society 6.0.", "AI": {"tldr": "本研究利用配备生成式AI（变分自编码器）的智能数字孪生来管理机器人热异常状态，并生成无危状态，通过重建误差推导热难度分数，使机器人能预测和分享运动配置的热可行性，以满足未来工业和社会需求。", "motivation": "机器人是工业4.0和5.0效率与协作的关键，但电机过热导致的热饱和和烧伤会威胁人类安全、降低机器人可用性并抑制生产力。传统的机器人关机策略会阻碍生产，而事后冷却策略难以实施。因此，需要一种自主的预测和适应热饱和的方法，以确保机器人性能、延长寿命并实现人机安全。", "method": "本研究利用配备生成式AI（即变分自编码器V AE）的智能数字孪生来管理热异常并生成无危的机器人状态。通过变分自编码器的重建误差推导出“热难度”的概念。机器人可以利用这个分数来预测、预期并分享所需运动配置的热可行性。", "result": "机器人能够利用派生出的“热难度”分数来预测、预期并分享所需运动配置的热可行性。这有助于满足工业6.0和社会6.0新兴应用的需求，确保全自动化、热可行的任务，并延长机器人寿命，无需人工干预。", "conclusion": "通过利用生成式AI驱动的数字孪生，机器人可以自主管理热饱和问题，预测热可行性，从而实现性能自维持、延长使用寿命、保障人类安全，并能提前与其他智能体沟通其能力，以支持未来工业和社会的完全自动化、热可行任务，克服了传统关机策略的局限性。"}}
{"id": "2509.12474", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12474", "abs": "https://arxiv.org/abs/2509.12474", "authors": ["Kai Qiu", "Xiang Li", "Hao Chen", "Jason Kuen", "Xiaohao Xu", "Jiuxiang Gu", "Yinyi Luo", "Bhiksha Raj", "Zhe Lin", "Marios Savvides"], "title": "Image Tokenizer Needs Post-Training", "comment": "21 pages, 16 figures, 10 tables. arXiv admin note: substantial text\n  overlap with arXiv:2503.08354", "summary": "Recent image generative models typically capture the image distribution in a\npre-constructed latent space, relying on a frozen image tokenizer. However,\nthere exists a significant discrepancy between the reconstruction and\ngeneration distribution, where current tokenizers only prioritize the\nreconstruction task that happens before generative training without considering\nthe generation errors during sampling. In this paper, we comprehensively\nanalyze the reason for this discrepancy in a discrete latent space, and, from\nwhich, we propose a novel tokenizer training scheme including both\nmain-training and post-training, focusing on improving latent space\nconstruction and decoding respectively. During the main training, a latent\nperturbation strategy is proposed to simulate sampling noises, \\ie, the\nunexpected tokens generated in generative inference. Specifically, we propose a\nplug-and-play tokenizer training scheme, which significantly enhances the\nrobustness of tokenizer, thus boosting the generation quality and convergence\nspeed, and a novel tokenizer evaluation metric, \\ie, pFID, which successfully\ncorrelates the tokenizer performance to generation quality. During\npost-training, we further optimize the tokenizer decoder regarding a\nwell-trained generative model to mitigate the distribution difference between\ngenerated and reconstructed tokens. With a $\\sim$400M generator, a discrete\ntokenizer trained with our proposed main training achieves a notable 1.60 gFID\nand further obtains 1.36 gFID with the additional post-training. Further\nexperiments are conducted to broadly validate the effectiveness of our\npost-training strategy on off-the-shelf discrete and continuous tokenizers,\ncoupled with autoregressive and diffusion-based generators.", "AI": {"tldr": "本文提出了一种新颖的图像生成模型分词器训练方案，包括主训练（通过潜在扰动模拟采样噪声）和后训练（优化解码器），旨在解决重建与生成分布之间的差异，从而显著提升生成质量和收敛速度。", "motivation": "现有图像生成模型中的分词器主要关注重建任务，导致重建分布与生成分布之间存在显著差异，并且在采样过程中产生的生成错误未被充分考虑。", "method": "本文提出了一种包含主训练和后训练的分词器训练方案。主训练引入潜在扰动策略以模拟生成推理中的采样噪声（即意外生成的标记），从而增强分词器的鲁棒性，并提出了一种新的评估指标pFID。后训练则针对已训练好的生成模型优化分词器解码器，以减小生成标记与重建标记之间的分布差异。该方案具有即插即用特性，并对离散和连续分词器以及自回归和扩散基生成器进行了广泛验证。", "result": "通过本文提出的主训练，一个约400M的生成器取得了1.60 gFID的性能；进一步结合后训练，性能提升至1.36 gFID。实验还广泛验证了后训练策略对现有离散和连续分词器与不同类型生成器的有效性。提出的pFID指标成功地将分词器性能与生成质量关联起来。", "conclusion": "本文提出的分词器训练方案（包括主训练中的潜在扰动策略和后训练中的解码器优化）有效解决了图像生成模型中重建与生成分布的差异问题，显著提高了生成质量和收敛速度，并为分词器评估提供了新的有效指标。"}}
{"id": "2509.12813", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.12813", "abs": "https://arxiv.org/abs/2509.12813", "authors": ["Bowen Ye", "Junyue Huang", "Yang Liu", "Xiaozhen Qiao", "Xiang Yin"], "title": "Bridging Perception and Planning: Towards End-to-End Planning for Signal Temporal Logic Tasks", "comment": null, "summary": "We investigate the task and motion planning problem for Signal Temporal Logic\n(STL) specifications in robotics. Existing STL methods rely on pre-defined maps\nor mobility representations, which are ineffective in unstructured real-world\nenvironments. We propose the \\emph{Structured-MoE STL Planner}\n(\\textbf{S-MSP}), a differentiable framework that maps synchronized multi-view\ncamera observations and an STL specification directly to a feasible trajectory.\nS-MSP integrates STL constraints within a unified pipeline, trained with a\ncomposite loss that combines trajectory reconstruction and STL robustness. A\n\\emph{structure-aware} Mixture-of-Experts (MoE) model enables horizon-aware\nspecialization by projecting sub-tasks into temporally anchored embeddings. We\nevaluate S-MSP using a high-fidelity simulation of factory-logistics scenarios\nwith temporally constrained tasks. Experiments show that S-MSP outperforms\nsingle-expert baselines in STL satisfaction and trajectory feasibility. A\nrule-based \\emph{safety filter} at inference improves physical executability\nwithout compromising logical correctness, showcasing the practicality of the\napproach.", "AI": {"tldr": "本文提出S-MSP，一个可微分框架，用于在非结构化环境中解决机器人信号时序逻辑（STL）任务与运动规划问题，通过多视图观测和STL规范直接生成可行轨迹，并结合专家混合模型和安全滤波器。", "motivation": "现有的STL方法依赖预定义地图或移动表示，在非结构化真实世界环境中效率低下。", "method": "提出S-MSP（Structured-MoE STL Planner），一个可微分框架，直接将同步多视图相机观测和STL规范映射到可行轨迹。它在一个统一的管道中集成STL约束，并使用结合轨迹重建和STL鲁棒性的复合损失进行训练。一个结构感知的专家混合（MoE）模型通过将子任务投射到时间锚定嵌入中实现面向时间范围的专业化。推理时还引入了一个基于规则的安全滤波器。", "result": "S-MSP在具有时间约束任务的工厂物流场景高保真模拟中，在STL满足度和轨迹可行性方面优于单一专家基线。推理时的安全滤波器在不损害逻辑正确性的前提下提高了物理可执行性。", "conclusion": "S-MSP为非结构化环境中的STL任务与运动规划提供了一个实用方法，在STL满足度、轨迹可行性和物理可执行性方面表现出色。"}}
{"id": "2509.12611", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12611", "abs": "https://arxiv.org/abs/2509.12611", "authors": ["Anmol Singhal Navya Singhal"], "title": "Analogy-Driven Financial Chain-of-Thought (AD-FCoT): A Prompting Approach for Financial Sentiment Analysis", "comment": "IEEE AIxB 2025", "summary": "Financial news sentiment analysis is crucial for anticipating market\nmovements. With the rise of AI techniques such as Large Language Models (LLMs),\nwhich demonstrate strong text understanding capabilities, there has been\nrenewed interest in enhancing these systems. Existing methods, however, often\nstruggle to capture the complex economic context of news and lack transparent\nreasoning, which undermines their reliability. We propose Analogy-Driven\nFinancial Chain-of-Thought (AD-FCoT), a prompting framework that integrates\nanalogical reasoning with chain-of-thought (CoT) prompting for sentiment\nprediction on historical financial news. AD-FCoT guides LLMs to draw parallels\nbetween new events and relevant historical scenarios with known outcomes,\nembedding these analogies into a structured, step-by-step reasoning chain. To\nour knowledge, this is among the first approaches to explicitly combine\nanalogical examples with CoT reasoning in finance. Operating purely through\nprompting, AD-FCoT requires no additional training data or fine-tuning and\nleverages the model's internal financial knowledge to generate rationales that\nmirror human analytical reasoning. Experiments on thousands of news articles\nshow that AD-FCoT outperforms strong baselines in sentiment classification\naccuracy and achieves substantially higher correlation with market returns. Its\ngenerated explanations also align with domain expertise, providing\ninterpretable insights suitable for real-world financial analysis.", "AI": {"tldr": "本文提出了一种名为AD-FCoT（类比驱动金融思维链）的提示框架，将类比推理与思维链（CoT）提示相结合，用于金融新闻情感分析，旨在提高准确性、市场相关性和解释性，通过引导大型语言模型（LLMs）从历史事件中汲取经验。", "motivation": "现有的大型语言模型（LLMs）在金融新闻情感分析中，往往难以捕捉复杂的经济背景，并且缺乏透明的推理过程，这降低了它们的可靠性。", "method": "本文提出了AD-FCoT（类比驱动金融思维链），一个纯粹基于提示的框架。它引导LLMs在新事件和已知结果的历史情景之间建立类比，并将这些类比整合到一个结构化的、逐步的推理链中。该方法无需额外的训练数据或微调，利用模型内部的金融知识生成类人分析推理。", "result": "在数千篇新闻文章上的实验表明，AD-FCoT在情感分类准确性方面优于强大的基线模型，并与市场回报表现出显著更高的相关性。其生成的解释也与领域专业知识相符，提供了适用于实际金融分析的可解释性见解。", "conclusion": "AD-FCoT是一种有效且可解释的金融新闻情感分析方法，它通过结合类比推理和思维链提示，显著提升了LLMs在捕捉经济背景和提供透明推理方面的能力，从而提高了预测性能和实用性。"}}
{"id": "2509.12811", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.12811", "abs": "https://arxiv.org/abs/2509.12811", "authors": ["Binquan Ji", "Jiaqi Wang", "Ruiting Li", "Xingchen Han", "Yiyang Qi", "Shichao Wang", "Yifei Lu", "Yuantao Han", "Feiliang Ren"], "title": "ConvergeWriter: Data-Driven Bottom-Up Article Construction", "comment": null, "summary": "Large Language Models (LLMs) have shown remarkable prowess in text\ngeneration, yet producing long-form, factual documents grounded in extensive\nexternal knowledge bases remains a significant challenge. Existing \"top-down\"\nmethods, which first generate a hypothesis or outline and then retrieve\nevidence, often suffer from a disconnect between the model's plan and the\navailable knowledge, leading to content fragmentation and factual inaccuracies.\nTo address these limitations, we propose a novel \"bottom-up,\" data-driven\nframework that inverts the conventional generation pipeline. Our approach is\npredicated on a \"Retrieval-First for Knowledge, Clustering for Structure\"\nstrategy, which first establishes the \"knowledge boundaries\" of the source\ncorpus before any generative planning occurs. Specifically, we perform\nexhaustive iterative retrieval from the knowledge base and then employ an\nunsupervised clustering algorithm to organize the retrieved documents into\ndistinct \"knowledge clusters.\" These clusters form an objective, data-driven\nfoundation that directly guides the subsequent generation of a hierarchical\noutline and the final document content. This bottom-up process ensures that the\ngenerated text is strictly constrained by and fully traceable to the source\nmaterial, proactively adapting to the finite scope of the knowledge base and\nfundamentally mitigating the risk of hallucination. Experimental results on\nboth 14B and 32B parameter models demonstrate that our method achieves\nperformance comparable to or exceeding state-of-the-art baselines, and is\nexpected to demonstrate unique advantages in knowledge-constrained scenarios\nthat demand high fidelity and structural coherence. Our work presents an\neffective paradigm for generating reliable, structured, long-form documents,\npaving the way for more robust LLM applications in high-stakes,\nknowledge-intensive domains.", "AI": {"tldr": "本文提出了一种“自下而上”的数据驱动框架，通过先从知识库中检索并聚类知识，再以此指导大语言模型生成分层大纲和文档内容，从而解决长篇事实性文档生成中的幻觉和内容碎片化问题。", "motivation": "尽管大语言模型在文本生成方面表现出色，但在生成基于大量外部知识库的长篇事实性文档时仍面临挑战。现有的“自上而下”方法（先生成假设/大纲再检索证据）常导致模型规划与可用知识脱节，进而产生内容碎片化和事实不准确。", "method": "提出“先检索知识，再聚类结构”的策略。具体而言，首先对知识库进行详尽的迭代检索，然后使用无监督聚类算法将检索到的文档组织成“知识簇”。这些知识簇形成客观、数据驱动的基础，直接指导后续分层大纲的生成和最终文档内容的生成。这种“自下而上”的过程确保生成文本严格受限于源材料并可追溯，主动适应知识库的有限范围，并从根本上降低幻觉的风险。", "result": "在14B和32B参数模型上的实验结果表明，该方法达到了与现有最先进基线相当或超越的性能。预计在需要高保真度和结构连贯性的知识受限场景中展现独特优势。", "conclusion": "本研究为生成可靠、结构化的长篇文档提供了一种有效范式，为高风险、知识密集型领域中更强大的大语言模型应用铺平了道路。"}}
{"id": "2509.12741", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.12741", "abs": "https://arxiv.org/abs/2509.12741", "authors": ["Alexis Yihong Hao", "Yufei Wang", "Navin Sriram Ravie", "Bharath Hegde", "David Held", "Zackory Erickson"], "title": "Force-Modulated Visual Policy for Robot-Assisted Dressing with Arm Motions", "comment": "CoRL 2025", "summary": "Robot-assisted dressing has the potential to significantly improve the lives\nof individuals with mobility impairments. To ensure an effective and\ncomfortable dressing experience, the robot must be able to handle challenging\ndeformable garments, apply appropriate forces, and adapt to limb movements\nthroughout the dressing process. Prior work often makes simplifying assumptions\n-- such as static human limbs during dressing -- which limits real-world\napplicability. In this work, we develop a robot-assisted dressing system\ncapable of handling partial observations with visual occlusions, as well as\nrobustly adapting to arm motions during the dressing process. Given a policy\ntrained in simulation with partial observations, we propose a method to\nfine-tune it in the real world using a small amount of data and multi-modal\nfeedback from vision and force sensing, to further improve the policy's\nadaptability to arm motions and enhance safety. We evaluate our method in\nsimulation with simplified articulated human meshes and in a real world human\nstudy with 12 participants across 264 dressing trials. Our policy successfully\ndresses two long-sleeve everyday garments onto the participants while being\nadaptive to various kinds of arm motions, and greatly outperforms prior\nbaselines in terms of task completion and user feedback. Video are available at\nhttps://dressing-motion.github.io/.", "AI": {"tldr": "该研究开发了一种机器人辅助穿衣系统，能够处理部分观测、视觉遮挡和手臂运动，并通过少量真实世界多模态数据对仿真训练的策略进行微调，显著优于现有基线。", "motivation": "机器人辅助穿衣能显著改善行动不便者的生活。然而，现有工作常做简化假设（如穿衣过程中人体肢体静止），限制了实际应用。机器人需要处理可变形衣物、施加适当力并适应肢体运动。", "method": "开发了一个机器人辅助穿衣系统，能处理部分观测和视觉遮挡，并鲁棒地适应手臂运动。首先在仿真环境中用部分观测训练策略，然后提出一种方法，利用少量真实世界数据和视觉、力传感等多模态反馈进行微调，以提高策略对手臂运动的适应性和安全性。该方法在仿真（简化关节人体网格）和真实世界人体研究（12名参与者，264次穿衣试验）中进行了评估。", "result": "该策略成功地为参与者穿戴了两种长袖日常服装，同时能适应各种手臂运动，在任务完成度和用户反馈方面大大优于现有基线。", "conclusion": "本研究开发了一个能够处理复杂现实世界条件（如部分观测和手臂运动）的机器人辅助穿衣系统，通过仿真到真实世界的微调方法，显著提高了穿衣任务的成功率和用户满意度，为行动不便者提供了更有效的解决方案。"}}
{"id": "2509.12482", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12482", "abs": "https://arxiv.org/abs/2509.12482", "authors": ["Tianshu Huang", "Akarsh Prabhakara", "Chuhan Chen", "Jay Karhade", "Deva Ramanan", "Matthew O'Toole", "Anthony Rowe"], "title": "Towards Foundational Models for Single-Chip Radar", "comment": "To appear in ICCV 2025", "summary": "mmWave radars are compact, inexpensive, and durable sensors that are robust\nto occlusions and work regardless of environmental conditions, such as weather\nand darkness. However, this comes at the cost of poor angular resolution,\nespecially for inexpensive single-chip radars, which are typically used in\nautomotive and indoor sensing applications. Although many have proposed\nlearning-based methods to mitigate this weakness, no standardized foundational\nmodels or large datasets for the mmWave radar have emerged, and practitioners\nhave largely trained task-specific models from scratch using relatively small\ndatasets.\n  In this paper, we collect (to our knowledge) the largest available raw radar\ndataset with 1M samples (29 hours) and train a foundational model for 4D\nsingle-chip radar, which can predict 3D occupancy and semantic segmentation\nwith quality that is typically only possible with much higher resolution\nsensors. We demonstrate that our Generalizable Radar Transformer (GRT)\ngeneralizes across diverse settings, can be fine-tuned for different tasks, and\nshows logarithmic data scaling of 20\\% per $10\\times$ data. We also run\nextensive ablations on common design decisions, and find that using raw radar\ndata significantly outperforms widely-used lossy representations, equivalent to\na $10\\times$ increase in training data. Finally, we roughly estimate that\n$\\approx$100M samples (3000 hours) of data are required to fully exploit the\npotential of GRT.", "AI": {"tldr": "本文收集了迄今为止最大的原始雷达数据集（100万样本），并训练了一个名为通用雷达Transformer（GRT）的基础模型，显著提高了单芯片毫米波雷达的3D占用和语义分割能力，超越了传统有损表示方法，并展现了良好的泛化性。", "motivation": "毫米波雷达虽然紧凑、廉价且耐用，不受环境条件影响，但其角分辨率差是主要缺点，尤其对于汽车和室内应用中常用的廉价单芯片雷达。尽管已有许多基于学习的方法来缓解此问题，但缺乏标准化的基础模型和大型数据集，导致从业者通常使用相对较小的数据集从头开始训练特定任务模型。", "method": "研究人员收集了据称是目前最大的原始雷达数据集，包含100万个样本（29小时）。在此基础上，他们训练了一个名为通用雷达Transformer（GRT）的基础模型，用于4D单芯片雷达，能够预测3D占用和语义分割。论文还对常见设计决策进行了广泛的消融实验，特别是比较了使用原始雷达数据与广泛使用的有损表示方法的效果。", "result": "GRT模型在3D占用和语义分割方面达到了通常只有更高分辨率传感器才能实现的质量。它在不同设置下表现出良好的泛化能力，可以针对不同任务进行微调，并显示出每10倍数据量可带来20%性能提升的对数数据缩放效应。消融实验发现，使用原始雷达数据显著优于广泛使用的有损表示，其效果相当于增加了10倍的训练数据。研究人员还初步估计，大约需要1亿个样本（3000小时）的数据才能充分发挥GRT的潜力。", "conclusion": "本研究成功构建了迄今最大的原始雷达数据集，并开发了GRT这一基础模型，显著提升了单芯片毫米波雷达在3D占用和语义分割任务上的性能，证明了原始数据的重要性及模型在不同任务和环境下的泛化能力。同时，研究也指出，要充分挖掘GRT的潜力，未来还需要更大规模的数据集。"}}
{"id": "2509.13109", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.13109", "abs": "https://arxiv.org/abs/2509.13109", "authors": ["Fabian Flürenbrock", "Yanick Büchel", "Johannes Köhler", "Marianne Schmid Daners", "Melanie N. Zeilinger"], "title": "Model Predictive Control with Reference Learning for Soft Robotic Intracranial Pressure Waveform Modulation", "comment": null, "summary": "This paper introduces a learning-based control framework for a soft robotic\nactuator system designed to modulate intracranial pressure (ICP) waveforms,\nwhich is essential for studying cerebrospinal fluid dynamics and pathological\nprocesses underlying neurological disorders. A two-layer framework is proposed\nto safely achieve a desired ICP waveform modulation. First, a model predictive\ncontroller (MPC) with a disturbance observer is used for offset-free tracking\nof the system's motor position reference trajectory under safety constraints.\nSecond, to address the unknown nonlinear dependence of ICP on the motor\nposition, we employ a Bayesian optimization (BO) algorithm used for online\nlearning of a motor position reference trajectory that yields the desired ICP\nmodulation. The framework is experimentally validated using a test bench with a\nbrain phantom that replicates realistic ICP dynamics in vitro. Compared to a\npreviously employed proportional-integral-derivative controller, the MPC\nreduces mean and maximum motor position reference tracking errors by 83 % and\n73 %, respectively. In less than 20 iterations, the BO algorithm learns a motor\nposition reference trajectory that yields an ICP waveform with the desired mean\nand amplitude.", "AI": {"tldr": "本文提出了一种基于学习的软体机器人控制框架，用于精确调节颅内压（ICP）波形，通过两层控制器实现对电机位置的无偏差跟踪和对ICP的在线学习调制。", "motivation": "研究脑脊液动力学和神经系统疾病的病理过程需要精确调节颅内压（ICP）波形，而现有方法可能无法满足高精度和在线学习的需求。", "method": "该研究提出一个两层控制框架：1. 使用带有扰动观测器的模型预测控制器（MPC）实现电机位置参考轨迹的无偏差跟踪，并满足安全约束。2. 采用贝叶斯优化（BO）算法在线学习电机位置参考轨迹，以应对ICP对电机位置的未知非线性依赖，从而实现所需的ICP波形调制。该框架通过一个模拟真实ICP动态的脑部模型测试台进行了实验验证。", "result": "与传统的PID控制器相比，MPC将平均和最大电机位置参考跟踪误差分别降低了83%和73%。贝叶斯优化算法在不到20次迭代中，学习到了能产生所需平均值和振幅的ICP波形的电机位置参考轨迹。", "conclusion": "所提出的学习型控制框架能够安全有效地调制颅内压波形，显著提高了电机位置跟踪精度，并通过在线学习实现了对复杂非线性系统ICP的精确控制，为神经系统疾病研究提供了有力的工具。"}}
{"id": "2509.12612", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12612", "abs": "https://arxiv.org/abs/2509.12612", "authors": ["Daojun Chen", "Xi Wang", "Shenyuan Ren", "Qingzhi Ma", "Pengpeng Zhao", "An Liu"], "title": "GBV-SQL: Guided Generation and SQL2Text Back-Translation Validation for Multi-Agent Text2SQL", "comment": null, "summary": "While Large Language Models have significantly advanced Text2SQL generation,\na critical semantic gap persists where syntactically valid queries often\nmisinterpret user intent. To mitigate this challenge, we propose GBV-SQL, a\nnovel multi-agent framework that introduces Guided Generation with SQL2Text\nBack-translation Validation. This mechanism uses a specialized agent to\ntranslate the generated SQL back into natural language, which verifies its\nlogical alignment with the original question. Critically, our investigation\nreveals that current evaluation is undermined by a systemic issue: the poor\nquality of the benchmarks themselves. We introduce a formal typology for \"Gold\nErrors\", which are pervasive flaws in the ground-truth data, and demonstrate\nhow they obscure true model performance. On the challenging BIRD benchmark,\nGBV-SQL achieves 63.23% execution accuracy, a 5.8% absolute improvement. After\nremoving flawed examples, GBV-SQL achieves 96.5% (dev) and 97.6% (test)\nexecution accuracy on the Spider benchmark. Our work offers both a robust\nframework for semantic validation and a critical perspective on benchmark\nintegrity, highlighting the need for more rigorous dataset curation.", "AI": {"tldr": "本文提出GBV-SQL，一个多智能体框架，通过SQL2Text反向翻译验证来解决Text2SQL中的语义鸿沟。同时，揭示了基准测试数据中“黄金错误”的普遍存在，并强调了数据集质量的重要性。GBV-SQL在BIRD和Spider基准测试上取得了显著改进。", "motivation": "尽管大型语言模型在Text2SQL生成方面取得了进展，但仍存在语义鸿沟，导致语法正确的查询误解用户意图。此外，现有评估受基准测试数据质量低下（即“黄金错误”）的严重影响，掩盖了模型的真实性能。", "method": "本文提出了GBV-SQL，一个新颖的多智能体框架，引入了“引导生成与SQL2Text反向翻译验证”机制。该机制使用一个专门的智能体将生成的SQL翻译回自然语言，以验证其与原始问题的逻辑一致性。此外，还引入了“黄金错误”的正式类型学，以识别基准测试数据中的普遍缺陷。", "result": "在挑战性的BIRD基准测试上，GBV-SQL实现了63.23%的执行准确率，绝对提升了5.8%。在移除有缺陷的示例后，GBV-SQL在Spider基准测试上实现了96.5%（开发集）和97.6%（测试集）的执行准确率。", "conclusion": "本文提供了一个强大的语义验证框架，并对基准测试的完整性提出了批判性视角，强调了更严格的数据集管理和策展的必要性。"}}
{"id": "2509.12853", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.12853", "abs": "https://arxiv.org/abs/2509.12853", "authors": ["Kurt Micallef", "Nizar Habash", "Claudia Borg"], "title": "Data Augmentation for Maltese NLP using Transliterated and Machine Translated Arabic Data", "comment": "EMNLP Camera-Ready", "summary": "Maltese is a unique Semitic language that has evolved under extensive\ninfluence from Romance and Germanic languages, particularly Italian and\nEnglish. Despite its Semitic roots, its orthography is based on the Latin\nscript, creating a gap between it and its closest linguistic relatives in\nArabic. In this paper, we explore whether Arabic-language resources can support\nMaltese natural language processing (NLP) through cross-lingual augmentation\ntechniques. We investigate multiple strategies for aligning Arabic textual data\nwith Maltese, including various transliteration schemes and machine translation\n(MT) approaches. As part of this, we also introduce novel transliteration\nsystems that better represent Maltese orthography. We evaluate the impact of\nthese augmentations on monolingual and mutlilingual models and demonstrate that\nArabic-based augmentation can significantly benefit Maltese NLP tasks.", "AI": {"tldr": "本文探讨了通过跨语言增强技术，利用阿拉伯语资源支持马耳他语自然语言处理（NLP）的可行性。", "motivation": "马耳他语是一种独特的闪米特语，使用拉丁字母书写，与同为闪米特语的阿拉伯语存在巨大差异。研究旨在弥合这一差距，探索能否利用丰富的阿拉伯语资源来提升马耳他语NLP的性能。", "method": "研究采用了多种策略将阿拉伯语文本数据与马耳他语对齐，包括各种音译方案（其中包含新提出的系统以更好地表示马耳他语正字法）和机器翻译（MT）方法。然后，这些增强数据被用于评估单语和多语模型。", "result": "评估结果表明，基于阿拉伯语的增强技术能显著提升马耳他语NLP任务的性能。", "conclusion": "阿拉伯语资源能够通过跨语言增强技术有效支持马耳他语的自然语言处理任务。"}}
{"id": "2509.12747", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.12747", "abs": "https://arxiv.org/abs/2509.12747", "authors": ["Botao He", "Amir Hossein Shahidzadeh", "Yu Chen", "Jiayi Wu", "Tianrui Guan", "Guofei Chen", "Howie Choset", "Dinesh Manocha", "Glen Chou", "Cornelia Fermuller", "Yiannis Aloimonos"], "title": "NavMoE: Hybrid Model- and Learning-based Traversability Estimation for Local Navigation via Mixture of Experts", "comment": null, "summary": "This paper explores traversability estimation for robot navigation. A key\nbottleneck in traversability estimation lies in efficiently achieving reliable\nand robust predictions while accurately encoding both geometric and semantic\ninformation across diverse environments. We introduce Navigation via Mixture of\nExperts (NAVMOE), a hierarchical and modular approach for traversability\nestimation and local navigation. NAVMOE combines multiple specialized models\nfor specific terrain types, each of which can be either a classical model-based\nor a learning-based approach that predicts traversability for specific terrain\ntypes. NAVMOE dynamically weights the contributions of different models based\non the input environment through a gating network. Overall, our approach offers\nthree advantages: First, NAVMOE enables traversability estimation to adaptively\nleverage specialized approaches for different terrains, which enhances\ngeneralization across diverse and unseen environments. Second, our approach\nsignificantly improves efficiency with negligible cost of solution quality by\nintroducing a training-free lazy gating mechanism, which is designed to\nminimize the number of activated experts during inference. Third, our approach\nuses a two-stage training strategy that enables the training for the gating\nnetworks within the hybrid MoE method that contains nondifferentiable modules.\nExtensive experiments show that NAVMOE delivers a better efficiency and\nperformance balance than any individual expert or full ensemble across\ndifferent domains, improving cross- domain generalization and reducing average\ncomputational cost by 81.2% via lazy gating, with less than a 2% loss in path\nquality.", "AI": {"tldr": "本文提出了一种名为NAVMOE的分层模块化专家混合（MoE）方法，用于机器人可穿越性估计和局部导航，通过动态加权特定地形专家模型，提高了效率和泛化能力。", "motivation": "可穿越性估计面临的关键瓶颈在于，如何在有效编码几何和语义信息的同时，高效地实现可靠且鲁棒的预测，以适应各种不同的环境。", "method": "NAVMOE是一种分层模块化方法，结合了多个针对特定地形的专家模型（可以是基于模型的或基于学习的方法）。它通过一个门控网络根据输入环境动态调整不同模型的贡献。该方法具有三个特点：1) 适应性利用专业方法处理不同地形；2) 引入免训练的惰性门控机制，最小化推理时激活的专家数量，显著提高效率；3) 采用两阶段训练策略，支持混合MoE方法中包含不可微分模块的门控网络训练。", "result": "实验表明，NAVMOE在不同领域提供了比任何单个专家或完整集成方法更好的效率和性能平衡，改善了跨领域泛化能力。通过惰性门控，平均计算成本降低了81.2%，而路径质量损失不到2%。", "conclusion": "NAVMOE通过结合专业模型和创新的门控机制，成功解决了机器人可穿越性估计中的效率和泛化挑战，在保持高性能的同时显著降低了计算成本。"}}
{"id": "2509.12492", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12492", "abs": "https://arxiv.org/abs/2509.12492", "authors": ["Purushoth", "Alireza"], "title": "Evaluating Robustness of Vision-Language Models Under Noisy Conditions", "comment": null, "summary": "Vision-Language Models (VLMs) have attained exceptional success across\nmultimodal tasks such as image captioning and visual question answering.\nHowever, their robustness under noisy conditions remains unfamiliar. In this\nstudy, we present a comprehensive evaluation framework to evaluate the\nperformance of several state-of-the-art VLMs under controlled perturbations,\nincluding lighting variation, motion blur, and compression artifacts. We used\nboth lexical-based metrics (BLEU, METEOR, ROUGE, CIDEr) and neural-based\nsimilarity measures using sentence embeddings to quantify semantic alignment.\nOur experiments span diverse datasets, revealing key insights: (1)\ndescriptiveness of ground-truth captions significantly influences model\nperformance; (2) larger models like LLaVA excel in semantic understanding but\ndo not universally outperform smaller models; and (3) certain noise types, such\nas JPEG compression and motion blur, dramatically degrade performance across\nmodels. Our findings highlight the nuanced trade-offs between model size,\ndataset characteristics, and noise resilience, offering a standardized\nbenchmark for future robust multimodal learning.", "AI": {"tldr": "本研究全面评估了最先进的视觉语言模型（VLMs）在光照变化、运动模糊和压缩伪影等受控噪声条件下的鲁棒性，发现真实标注的描述性、模型大小与噪声类型对性能有显著影响。", "motivation": "视觉语言模型（VLMs）在多模态任务中取得了巨大成功，但它们在噪声条件下的鲁棒性尚不明确。", "method": "本研究提出了一个全面的评估框架，用于在受控扰动（包括光照变化、运动模糊和压缩伪影）下评估多个最先进的VLMs的性能。评估指标包括基于词汇的度量（BLEU, METEOR, ROUGE, CIDEr）和使用句子嵌入的神经语义相似性度量。实验涵盖了多样化的数据集。", "result": "主要发现包括：(1) 真实标注的描述性显著影响模型性能；(2) LLaVA等大型模型在语义理解方面表现出色，但并非普遍优于小型模型；(3) 某些噪声类型，如JPEG压缩和运动模糊，会显著降低所有模型的性能。", "conclusion": "研究结果揭示了模型大小、数据集特性和噪声弹性之间微妙的权衡，为未来鲁棒的多模态学习提供了一个标准化的基准。"}}
{"id": "2509.13164", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.13164", "abs": "https://arxiv.org/abs/2509.13164", "authors": ["Jiawei Wang", "Haowei Sun", "Xintao Yan", "Shuo Feng", "Jun Gao", "Henry X. Liu"], "title": "TeraSim-World: Worldwide Safety-Critical Data Synthesis for End-to-End Autonomous Driving", "comment": "8 pages, 6 figures. Codes and videos are available at\n  https://wjiawei.com/terasim-world-web/", "summary": "Safe and scalable deployment of end-to-end (E2E) autonomous driving requires\nextensive and diverse data, particularly safety-critical events. Existing data\nare mostly generated from simulators with a significant sim-to-real gap or\ncollected from on-road testing that is costly and unsafe. This paper presents\nTeraSim-World, an automated pipeline that synthesizes realistic and\ngeographically diverse safety-critical data for E2E autonomous driving at\nanywhere in the world. Starting from an arbitrary location, TeraSim-World\nretrieves real-world maps and traffic demand from geospatial data sources.\nThen, it simulates agent behaviors from naturalistic driving datasets, and\norchestrates diverse adversities to create corner cases. Informed by street\nviews of the same location, it achieves photorealistic, geographically grounded\nsensor rendering via the frontier video generation model Cosmos-Drive. By\nbridging agent and sensor simulations, TeraSim-World provides a scalable and\ncritical~data synthesis framework for training and evaluation of E2E autonomous\ndriving systems.", "AI": {"tldr": "TeraSim-World是一个自动化流程，用于为端到端自动驾驶合成逼真、地理多样化的安全关键数据，以解决现有数据源的局限性。", "motivation": "端到端自动驾驶需要大量多样化的安全关键数据，但现有数据（模拟器或路测）存在模拟到现实的差距，且成本高昂或不安全。", "method": "TeraSim-World从任意位置获取地理空间数据（真实世界地图和交通需求），然后从自然驾驶数据集中模拟智能体行为并编排各种逆境以创建极端情况。它利用街景信息，通过前沿视频生成模型Cosmos-Drive实现逼真且地理上准确的传感器渲染，从而连接智能体和传感器模拟。", "result": "该方法提供了一个可扩展且关键的数据合成框架，能够生成逼真、地理多样化的安全关键数据，用于端到端自动驾驶系统的训练和评估。", "conclusion": "TeraSim-World通过弥合智能体和传感器模拟之间的鸿沟，为端到端自动驾驶系统的训练和评估提供了一个可扩展且至关重要的数据合成框架。"}}
{"id": "2509.12615", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.12615", "abs": "https://arxiv.org/abs/2509.12615", "authors": ["Muhammad Riaz Hasib Hossain", "Rafiqul Islam", "Shawn R McGrath", "Md Zahidul Islam", "David Lamb"], "title": "Mob-based cattle weight gain forecasting using ML models", "comment": null, "summary": "Forecasting mob based cattle weight gain (MB CWG) may benefit large livestock\nfarms, allowing farmers to refine their feeding strategies, make educated\nbreeding choices, and reduce risks linked to climate variability and market\nfluctuations. In this paper, a novel technique termed MB CWG is proposed to\nforecast the one month advanced weight gain of herd based cattle using\nhistorical data collected from the Charles Sturt University Farm. This research\nemploys a Random Forest (RF) model, comparing its performance against Support\nVector Regression (SVR) and Long Short Term Memory (LSTM) models for monthly\nweight gain prediction. Four datasets were used to evaluate the performance of\nmodels, using 756 sample data from 108 herd-based cattle, along with weather\ndata (rainfall and temperature) influencing CWG. The RF model performs better\nthan the SVR and LSTM models across all datasets, achieving an R^2 of 0.973,\nRMSE of 0.040, and MAE of 0.033 when both weather and age factors were\nincluded. The results indicate that including both weather and age factors\nsignificantly improves the accuracy of weight gain predictions, with the RF\nmodel outperforming the SVR and LSTM models in all scenarios. These findings\ndemonstrate the potential of RF as a robust tool for forecasting cattle weight\ngain in variable conditions, highlighting the influence of age and climatic\nfactors on herd based weight trends. This study has also developed an\ninnovative automated pre processing tool to generate a benchmark dataset for MB\nCWG predictive models. The tool is publicly available on GitHub and can assist\nin preparing datasets for current and future analytical research..", "AI": {"tldr": "本研究提出了一种预测基于群体的牛只月度体重增加（MB CWG）的新技术，并使用随机森林（RF）模型进行预测，结果优于支持向量回归（SVR）和长短期记忆（LSTM）模型，尤其是在纳入天气和年龄因素时。", "motivation": "预测基于群体的牛只体重增加（MB CWG）可以帮助大型畜牧农场优化饲喂策略、做出明智的育种选择，并降低气候多变性和市场波动带来的风险。", "method": "本研究提出了一种名为MB CWG的新技术，用于预测牛只提前一个月的体重增加。它采用随机森林（RF）模型，并将其性能与支持向量回归（SVR）和长短期记忆（LSTM）模型进行比较。模型使用来自查尔斯斯图尔特大学农场的历史数据，包括108头牛的756个样本数据以及影响CWG的天气数据（降雨量和温度）。此外，本研究还开发了一个创新的自动化预处理工具，用于生成MB CWG预测模型的基准数据集。", "result": "在所有数据集上，随机森林（RF）模型的表现均优于SVR和LSTM模型。当同时包含天气和年龄因素时，RF模型取得了0.973的R^2、0.040的RMSE和0.033的MAE。结果表明，纳入天气和年龄因素显著提高了体重增加预测的准确性，并且RF模型在所有情况下都优于SVR和LSTM模型。", "conclusion": "研究结果表明，随机森林（RF）是一种预测多变条件下牛只体重增加的强大工具，并强调了年龄和气候因素对群体体重趋势的影响。同时，开发的自动化预处理工具可用于当前和未来的分析研究，以协助准备数据集。"}}
{"id": "2509.12876", "categories": ["cs.CL", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.12876", "abs": "https://arxiv.org/abs/2509.12876", "authors": ["Fuyu Xing", "Zimu Wang", "Wei Wang", "Haiyang Zhang"], "title": "Benchmarking and Improving LVLMs on Event Extraction from Multimedia Documents", "comment": "Accepted at INLG 2025. Camera-ready version", "summary": "The proliferation of multimedia content necessitates the development of\neffective Multimedia Event Extraction (M2E2) systems. Though Large\nVision-Language Models (LVLMs) have shown strong cross-modal capabilities,\ntheir utility in the M2E2 task remains underexplored. In this paper, we present\nthe first systematic evaluation of representative LVLMs, including DeepSeek-VL2\nand the Qwen-VL series, on the M2E2 dataset. Our evaluations cover text-only,\nimage-only, and cross-media subtasks, assessed under both few-shot prompting\nand fine-tuning settings. Our key findings highlight the following valuable\ninsights: (1) Few-shot LVLMs perform notably better on visual tasks but\nstruggle significantly with textual tasks; (2) Fine-tuning LVLMs with LoRA\nsubstantially enhances model performance; and (3) LVLMs exhibit strong synergy\nwhen combining modalities, achieving superior performance in cross-modal\nsettings. We further provide a detailed error analysis to reveal persistent\nchallenges in areas such as semantic precision, localization, and cross-modal\ngrounding, which remain critical obstacles for advancing M2E2 capabilities.", "AI": {"tldr": "本文首次系统评估了大型视觉语言模型（LVLMs）在多媒体事件提取（M2E2）任务上的表现，涵盖少样本提示和微调设置，并揭示了其在视觉任务上的优势、微调的有效性以及跨模态融合的协同作用，同时指出了语义精度、定位和跨模态接地等挑战。", "motivation": "多媒体内容的激增需要高效的多媒体事件提取（M2E2）系统。尽管大型视觉语言模型（LVLMs）展现出强大的跨模态能力，但其在M2E2任务中的应用尚未得到充分探索。", "method": "研究人员对DeepSeek-VL2和Qwen-VL系列等代表性LVLMs在M2E2数据集上进行了首次系统评估。评估涵盖了纯文本、纯图像和跨媒体子任务，并在少样本提示和微调（使用LoRA）两种设置下进行性能评估。此外，还进行了详细的错误分析。", "result": "(1) 少样本LVLMs在视觉任务上表现显著更好，但在文本任务上表现较差；(2) 使用LoRA对LVLMs进行微调可大幅提升模型性能；(3) LVLMs在结合不同模态时展现出强大的协同作用，在跨模态设置中取得卓越性能。错误分析揭示了语义精度、定位和跨模态接地等方面的持续挑战。", "conclusion": "LVLMs在M2E2任务中具有巨大潜力，尤其是在微调和跨模态融合的场景下表现出色。然而，在语义精度、定位和跨模态接地等关键领域仍面临挑战，这些是未来提升M2E2能力的关键障碍。"}}
{"id": "2509.12754", "categories": ["cs.RO", "cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.12754", "abs": "https://arxiv.org/abs/2509.12754", "authors": ["Saki Hashimoto", "Shoichi Hasegawa", "Tomochika Ishikawa", "Akira Taniguchi", "Yoshinobu Hagiwara", "Lotfi El Hafi", "Tadahiro Taniguchi"], "title": "Toward Ownership Understanding of Objects: Active Question Generation with Large Language Model and Probabilistic Generative Model", "comment": "Submitted to AROB-ISBC 2026 (Journal Track option)", "summary": "Robots operating in domestic and office environments must understand object\nownership to correctly execute instructions such as ``Bring me my cup.''\nHowever, ownership cannot be reliably inferred from visual features alone. To\naddress this gap, we propose Active Ownership Learning (ActOwL), a framework\nthat enables robots to actively generate and ask ownership-related questions to\nusers. ActOwL employs a probabilistic generative model to select questions that\nmaximize information gain, thereby acquiring ownership knowledge efficiently to\nimprove learning efficiency. Additionally, by leveraging commonsense knowledge\nfrom Large Language Models (LLM), objects are pre-classified as either shared\nor owned, and only owned objects are targeted for questioning. Through\nexperiments in a simulated home environment and a real-world laboratory\nsetting, ActOwL achieved significantly higher ownership clustering accuracy\nwith fewer questions than baseline methods. These findings demonstrate the\neffectiveness of combining active inference with LLM-guided commonsense\nreasoning, advancing the capability of robots to acquire ownership knowledge\nfor practical and socially appropriate task execution.", "AI": {"tldr": "该论文提出了ActOwL框架，使机器人能够主动向用户提问以学习物品所有权，并通过结合LLM常识推理和主动提问来高效获取知识。", "motivation": "在家庭和办公环境中操作的机器人需要理解物品所有权，以正确执行“把我的杯子拿给我”等指令。然而，仅凭视觉特征无法可靠地推断所有权。", "method": "本文提出了主动所有权学习（ActOwL）框架，该框架使机器人能够主动生成并向用户提出与所有权相关的问题。ActOwL采用概率生成模型来选择能够最大化信息增益的问题，从而高效获取所有权知识。此外，通过利用大型语言模型（LLM）的常识知识，物品被预先分类为共享或私有，仅对私有物品进行提问。", "result": "在模拟家庭环境和真实实验室环境中的实验表明，ActOwL以比基线方法更少的问题实现了显著更高的所有权聚类准确性。", "conclusion": "这些发现证明了将主动推理与LLM引导的常识推理相结合的有效性，提升了机器人获取所有权知识的能力，以实现实用且符合社交规范的任务执行。"}}
{"id": "2509.12496", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12496", "abs": "https://arxiv.org/abs/2509.12496", "authors": ["Ali Torabi", "Sanjog Gaihre", "MD Mahbubur Rahman", "Yaqoob Majeed"], "title": "Instance-Guided Class Activation Mapping for Weakly Supervised Semantic Segmentation", "comment": null, "summary": "Weakly Supervised Semantic Segmentation (WSSS) addresses the challenge of\ntraining segmentation models using only image-level annotations, eliminating\nthe need for expensive pixel-level labeling. While existing methods struggle\nwith precise object boundary localization and often focus only on the most\ndiscriminative regions, we propose IG-CAM (Instance-Guided Class Activation\nMapping), a novel approach that leverages instance-level cues and influence\nfunctions to generate high-quality, boundary-aware localization maps. Our\nmethod introduces three key innovations: (1) Instance-Guided Refinement that\nuses ground truth segmentation masks to guide CAM generation, ensuring complete\nobject coverage rather than just discriminative parts; (2) Influence Function\nIntegration that captures the relationship between training samples and model\npredictions, leading to more robust feature representations; and (3)\nMulti-Scale Boundary Enhancement that employs progressive refinement strategies\nto achieve sharp, precise object boundaries. IG-CAM achieves state-of-the-art\nperformance on the PASCAL VOC 2012 dataset with an mIoU of 82.3% before\npost-processing, which further improves to 86.6% after applying Conditional\nRandom Field (CRF) refinement, significantly outperforming previous WSSS\nmethods. Our approach demonstrates superior localization accuracy, with\ncomplete object coverage and precise boundary delineation, while maintaining\ncomputational efficiency. Extensive ablation studies validate the contribution\nof each component, and qualitative comparisons across 600 diverse images\nshowcase the method's robustness and generalization capability. The results\nestablish IG-CAM as a new benchmark for weakly supervised semantic\nsegmentation, offering a practical solution for scenarios where pixel-level\nannotations are unavailable or prohibitively expensive.", "AI": {"tldr": "IG-CAM是一种新的弱监督语义分割（WSSS）方法，利用实例级线索和影响函数生成高质量、边界感知的定位图，在PASCAL VOC 2012数据集上实现了最先进的性能，解决了现有方法在边界精度和完整对象覆盖方面的不足。", "motivation": "现有弱监督语义分割方法在精确对象边界定位方面表现不佳，且往往只关注最具判别性的区域，未能实现完整的对象覆盖。此外，像素级标注成本高昂，限制了模型训练。", "method": "本文提出了IG-CAM方法，包含三项关键创新：1) 实例引导细化：利用真实分割掩码引导CAM生成，确保完整对象覆盖；2) 影响函数集成：捕获训练样本与模型预测之间的关系，学习更鲁棒的特征表示；3) 多尺度边界增强：采用渐进式细化策略，实现锐利、精确的对象边界。", "result": "IG-CAM在PASCAL VOC 2012数据集上取得了最先进的性能，后处理前mIoU达到82.3%，应用条件随机场（CRF）细化后进一步提升至86.6%，显著优于之前的WSSS方法。该方法展示了卓越的定位精度、完整的对象覆盖和精确的边界描绘，同时保持了计算效率。", "conclusion": "IG-CAM为弱监督语义分割设定了新的基准，为像素级标注不可用或成本过高的场景提供了一个实用的解决方案。其在完整对象覆盖和精确边界描绘方面的优越性得到了广泛验证。"}}
{"id": "2509.12625", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12625", "abs": "https://arxiv.org/abs/2509.12625", "authors": ["Yong Xia", "Jingxuan Li", "YeTeng Sun", "Jiarui Bu"], "title": "ECG-aBcDe: Overcoming Model Dependence, Encoding ECG into a Universal Language for Any LLM", "comment": "14pages, 6 figures", "summary": "Large Language Models (LLMs) hold significant promise for electrocardiogram\n(ECG) analysis, yet challenges remain regarding transferability, time-scale\ninformation learning, and interpretability. Current methods suffer from\nmodel-specific ECG encoders, hindering transfer across LLMs. Furthermore, LLMs\nstruggle to capture crucial time-scale information inherent in ECGs due to\nTransformer limitations. And their black-box nature limits clinical adoption.\nTo address these limitations, we introduce ECG-aBcDe, a novel ECG encoding\nmethod that transforms ECG signals into a universal ECG language readily\ninterpretable by any LLM. By constructing a hybrid dataset of ECG language and\nnatural language, ECG-aBcDe enables direct fine-tuning of pre-trained LLMs\nwithout architectural modifications, achieving \"construct once, use anywhere\"\ncapability. Moreover, the bidirectional convertibility between ECG and ECG\nlanguage of ECG-aBcDe allows for extracting attention heatmaps from ECG\nsignals, significantly enhancing interpretability. Finally, ECG-aBcDe\nexplicitly represents time-scale information, mitigating Transformer\nlimitations. This work presents a new paradigm for integrating ECG analysis\nwith LLMs. Compared with existing methods, our method achieves competitive\nperformance on ROUGE-L and METEOR. Notably, it delivers significant\nimprovements in the BLEU-4, with improvements of 2.8 times and 3.9 times in\nin-dataset and cross-dataset evaluations, respectively, reaching scores of\n42.58 and 30.76. These results provide strong evidence for the feasibility of\nthe new paradigm.", "AI": {"tldr": "本文提出ECG-aBcDe，一种将心电图（ECG）信号转换为通用ECG语言的新方法，以解决大型语言模型（LLMs）在ECG分析中的可迁移性、时间尺度信息学习和可解释性挑战，实现了“一次构建，随处使用”并显著提升性能。", "motivation": "当前LLMs在心电图分析中存在三大挑战：1) 模型特定的ECG编码器阻碍了LLMs之间的迁移；2) Transformer的局限性导致LLMs难以捕捉ECG中关键的时间尺度信息；3) LLMs的黑盒性质限制了临床应用。", "method": "ECG-aBcDe将ECG信号转换为一种通用的ECG语言，任何LLM都能理解。它构建了一个包含ECG语言和自然语言的混合数据集，可以直接对预训练LLMs进行微调，无需修改架构。此外，ECG和ECG语言之间的双向可转换性允许从ECG信号中提取注意力热图，增强了可解释性。ECG-aBcDe还明确表示时间尺度信息，缓解了Transformer的局限性。", "result": "ECG-aBcDe在ROUGE-L和METEOR指标上取得了与现有方法相当的性能。在BLEU-4指标上，它实现了显著提升，在数据集内评估中提高了2.8倍（达到42.58分），在跨数据集评估中提高了3.9倍（达到30.76分）。", "conclusion": "这项工作为将ECG分析与LLMs集成提供了一个新范式。实验结果有力证明了该新范式的可行性，解决了当前方法在可迁移性、时间尺度信息和可解释性方面的不足。"}}
{"id": "2509.12886", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12886", "abs": "https://arxiv.org/abs/2509.12886", "authors": ["Yubo Zhu", "Dongrui Liu", "Zecheng Lin", "Wei Tong", "Sheng Zhong", "Jing Shao"], "title": "The LLM Already Knows: Estimating LLM-Perceived Question Difficulty via Hidden Representations", "comment": null, "summary": "Estimating the difficulty of input questions as perceived by large language\nmodels (LLMs) is essential for accurate performance evaluation and adaptive\ninference. Existing methods typically rely on repeated response sampling,\nauxiliary models, or fine-tuning the target model itself, which may incur\nsubstantial computational costs or compromise generality. In this paper, we\npropose a novel approach for difficulty estimation that leverages only the\nhidden representations produced by the target LLM. We model the token-level\ngeneration process as a Markov chain and define a value function to estimate\nthe expected output quality given any hidden state. This allows for efficient\nand accurate difficulty estimation based solely on the initial hidden state,\nwithout generating any output tokens. Extensive experiments across both textual\nand multimodal tasks demonstrate that our method consistently outperforms\nexisting baselines in difficulty estimation. Moreover, we apply our difficulty\nestimates to guide adaptive reasoning strategies, including Self-Consistency,\nBest-of-N, and Self-Refine, achieving higher inference efficiency with fewer\ngenerated tokens.", "AI": {"tldr": "该论文提出了一种新颖的方法，仅利用大型语言模型（LLM）的隐藏表示来估计输入问题的难度，无需生成任何输出token。通过将token生成过程建模为马尔可夫链并定义价值函数，实现了高效准确的难度估计，并在实验中优于现有基线，还能指导自适应推理策略以提高效率。", "motivation": "准确估计LLM感知的问题难度对于性能评估和自适应推理至关重要。现有方法通常依赖重复采样、辅助模型或微调目标模型，这会导致高计算成本或泛化性受损。", "method": "本研究提出一种仅利用目标LLM隐藏表示的难度估计方法。具体而言，将token级别的生成过程建模为马尔可夫链，并定义一个价值函数来估计给定任何隐藏状态下的预期输出质量。这种方法仅基于初始隐藏状态即可进行难度估计，无需生成任何输出token。", "result": "在文本和多模态任务上进行的广泛实验表明，该方法在难度估计方面持续优于现有基线。此外，将该难度估计应用于指导自适应推理策略（包括Self-Consistency、Best-of-N和Self-Refine）时，能够以更少的生成token实现更高的推理效率。", "conclusion": "该研究提出了一种高效且准确的LLM问题难度估计方法，该方法仅依赖于模型的隐藏表示，无需生成输出token。这不仅提升了难度估计的性能，还能有效指导自适应推理策略，从而提高推理效率。"}}
{"id": "2509.12776", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.12776", "abs": "https://arxiv.org/abs/2509.12776", "authors": ["Renjie Wang", "Shangke Lyu", "Xin Lang", "Wei Xiao", "Donglin Wang"], "title": "Integrating Trajectory Optimization and Reinforcement Learning for Quadrupedal Jumping with Terrain-Adaptive Landing", "comment": "Accepted by IROS 2025", "summary": "Jumping constitutes an essential component of quadruped robots' locomotion\ncapabilities, which includes dynamic take-off and adaptive landing. Existing\nquadrupedal jumping studies mainly focused on the stance and flight phase by\nassuming a flat landing ground, which is impractical in many real world cases.\nThis work proposes a safe landing framework that achieves adaptive landing on\nrough terrains by combining Trajectory Optimization (TO) and Reinforcement\nLearning (RL) together. The RL agent learns to track the reference motion\ngenerated by TO in the environments with rough terrains. To enable the learning\nof compliant landing skills on challenging terrains, a reward relaxation\nstrategy is synthesized to encourage exploration during landing recovery\nperiod. Extensive experiments validate the accurate tracking and safe landing\nskills benefiting from our proposed method in various scenarios.", "AI": {"tldr": "本文提出了一种结合轨迹优化（TO）和强化学习（RL）的框架，使四足机器人能够在崎岖地形上安全自适应着陆，并引入奖励松弛策略以促进探索和柔顺着陆。", "motivation": "现有的四足机器人跳跃研究主要假设平坦着陆地面，这在许多实际场景中不切实际，无法应对崎岖地形的挑战。", "method": "该研究结合了轨迹优化（TO）和强化学习（RL）。RL智能体学习跟踪TO生成的参考运动，并在崎岖地形环境中进行训练。为实现在挑战性地形上的柔顺着陆技能学习，引入了奖励松弛策略以鼓励着陆恢复期间的探索。", "result": "广泛的实验验证了所提出的方法在各种场景下实现了精确的轨迹跟踪和安全的着陆技能。", "conclusion": "该框架通过结合TO和RL，并辅以奖励松弛策略，成功解决了四足机器人在崎岖地形上自适应着陆的难题，显著提升了机器人的运动能力。"}}
{"id": "2509.12501", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12501", "abs": "https://arxiv.org/abs/2509.12501", "authors": ["Yao He", "Youngjoong Kwon", "Wenxiao Cai", "Ehsan Adeli"], "title": "Artist-Created Mesh Generation from Raw Observation", "comment": null, "summary": "We present an end-to-end framework for generating artist-style meshes from\nnoisy or incomplete point clouds, such as those captured by real-world sensors\nlike LiDAR or mobile RGB-D cameras. Artist-created meshes are crucial for\ncommercial graphics pipelines due to their compatibility with animation and\ntexturing tools and their efficiency in rendering. However, existing approaches\noften assume clean, complete inputs or rely on complex multi-stage pipelines,\nlimiting their applicability in real-world scenarios. To address this, we\npropose an end-to-end method that refines the input point cloud and directly\nproduces high-quality, artist-style meshes. At the core of our approach is a\nnovel reformulation of 3D point cloud refinement as a 2D inpainting task,\nenabling the use of powerful generative models. Preliminary results on the\nShapeNet dataset demonstrate the promise of our framework in producing clean,\ncomplete meshes.", "AI": {"tldr": "本文提出一个端到端框架，能从噪声或不完整的点云（如LiDAR或移动RGB-D相机捕获的数据）生成艺术家风格的网格模型。", "motivation": "商业图形管线中，艺术家创建的网格对于动画、纹理和渲染效率至关重要。然而，现有方法通常要求干净完整的输入，或依赖复杂的多阶段管线，限制了其在真实世界场景中的应用。", "method": "该方法是一个端到端框架，直接从精炼的输入点云生成高质量、艺术家风格的网格。核心创新是将3D点云精炼重新表述为2D修复任务，从而能够利用强大的生成模型。", "result": "在ShapeNet数据集上的初步结果表明，该框架在生成干净、完整的网格方面具有良好前景。", "conclusion": "该框架有望从真实世界的噪声数据中生成高质量、艺术家风格的网格模型，解决了现有方法在处理不完整或噪声输入时的局限性。"}}
{"id": "2509.12643", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12643", "abs": "https://arxiv.org/abs/2509.12643", "authors": ["Beidan Liu", "Zhengqiu Zhu", "Chen Gao", "Yong Zhao", "Wei Qi", "Quanjun Yin"], "title": "Learn to Relax with Large Language Models: Solving Nonlinear Combinatorial Optimization Problems via Bidirectional Coevolution", "comment": null, "summary": "Nonlinear Combinatorial Optimization Problems (NCOPs) present a formidable\ncomputational hurdle in practice, as their nonconvex nature gives rise to\nmulti-modal solution spaces that defy efficient optimization. Traditional\nconstraint relaxation approaches rely heavily on expert-driven, iterative\ndesign processes that lack systematic automation and scalable adaptability.\nWhile recent Large Language Model (LLM)-based optimization methods show promise\nfor autonomous problem-solving, they predominantly function as passive\nconstraint validators rather than proactive strategy architects, failing to\nhandle the sophisticated constraint interactions inherent to NCOPs.To address\nthese limitations, we introduce the first end-to-end \\textbf{Auto}mated\n\\textbf{C}onstraint \\textbf{O}ptimization (AutoCO) method, which revolutionizes\nNCOPs resolution through learning to relax with LLMs.Specifically, we leverage\nstructured LLM reasoning to generate constraint relaxation strategies, which\nare dynamically evolving with algorithmic principles and executable code\nthrough a unified triple-representation scheme. We further establish a novel\nbidirectional (global-local) coevolution mechanism that synergistically\nintegrates Evolutionary Algorithms for intensive local refinement with Monte\nCarlo Tree Search for systematic global strategy space exploration, ensuring\noptimal balance between intensification and diversification in fragmented\nsolution spaces. Finally, comprehensive experiments on three challenging NCOP\nbenchmarks validate AutoCO's consistent effectiveness and superior performance\nover the baselines.", "AI": {"tldr": "本文提出AutoCO，一个端到端的方法，利用大型语言模型（LLM）学习放松约束来解决非线性组合优化问题（NCOPs），并通过双向协同进化机制实现了卓越性能。", "motivation": "非线性组合优化问题（NCOPs）因其非凸性和多模态解空间而难以有效优化。传统约束松弛方法依赖专家驱动的迭代设计，缺乏自动化和可扩展性。现有基于LLM的优化方法主要作为被动约束验证器，而非主动策略设计者，难以处理NCOPs中复杂的约束交互。", "method": "本文引入了AutoCO方法，通过LLM学习放松约束来解决NCOPs。具体而言，该方法利用结构化LLM推理生成约束松弛策略，这些策略通过统一的三重表示方案，结合算法原理和可执行代码动态演化。此外，AutoCO建立了一种新颖的双向（全局-局部）协同进化机制，将进化算法（用于密集的局部精炼）与蒙特卡洛树搜索（用于系统的全局策略空间探索）相结合，以在碎片化解空间中平衡强化和多样化。", "result": "在三个具有挑战性的NCOP基准测试上的综合实验验证了AutoCO的持续有效性，并显示其性能优于现有基线方法。", "conclusion": "AutoCO通过利用LLM学习约束松弛和创新的双向协同进化机制，革新了NCOPs的解决方式，并在实践中展现出卓越的有效性和性能。"}}
{"id": "2509.12892", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12892", "abs": "https://arxiv.org/abs/2509.12892", "authors": ["Shiyu Li", "Yang Tang", "Ruijie Liu", "Shi-Zhe Chen", "Xi Chen"], "title": "Conan-Embedding-v2: Training an LLM from Scratch for Text Embeddings", "comment": "EMNLP 2025 Oral", "summary": "Large language models (LLMs) have recently demonstrated excellent performance\nin text embedding tasks. Previous work usually use LoRA to fine-tune existing\nLLMs, which are limited by the data and training gap between LLMs and embedding\nmodels. In this work, we introduce Conan-embedding-v2, a new 1.4B-parameter LLM\ntrained from scratch and fine-tuned as a text embedder. First, we add news data\nand multilingual pairs for LLM pretraining to bridge the data gap. Based on\nthis, we propose a cross-lingual retrieval dataset that enables the LLM to\nbetter integrate embeddings across different languages. Second, whereas LLMs\nuse a causal mask with token-level loss, embedding models use a bidirectional\nmask with sentence-level loss. This training gap makes full fine-tuning less\neffective than LoRA. We introduce a soft-masking mechanism to gradually\ntransition between these two types of masks, enabling the model to learn more\ncomprehensive representations. Based on this, we propose a dynamic hard\nnegative mining method that exposes the model to more difficult negative\nexamples throughout the training process. Being intuitive and effective, with\nonly approximately 1.4B parameters, Conan-embedding-v2 achieves SOTA\nperformance on both the Massive Text Embedding Benchmark (MTEB) and Chinese\nMTEB (May 19, 2025).", "AI": {"tldr": "本文提出了Conan-embedding-v2，一个从头训练并微调的1.4B参数大型语言模型，通过弥合LLM与嵌入模型之间的数据和训练鸿沟，在文本嵌入任务上达到了SOTA性能。", "motivation": "现有的大型语言模型（LLMs）在文本嵌入任务中表现出色，但通常使用LoRA进行微调，受限于LLM与嵌入模型之间的数据和训练差异，导致效果不佳。", "method": "1. 从头训练一个1.4B参数的LLM。2. 弥合数据鸿沟：在预训练中加入新闻数据和多语言对，并构建跨语言检索数据集。3. 弥合训练鸿沟：引入软掩码机制，逐步实现因果掩码和双向掩码之间的过渡。4. 优化训练：提出动态难负例挖掘方法。", "result": "Conan-embedding-v2（约1.4B参数）在MTEB和中文MTEB上均取得了最先进（SOTA）的性能（截至2025年5月19日）。", "conclusion": "Conan-embedding-v2通过从头训练并创新性地弥合了LLM与嵌入模型之间的数据和训练鸿沟，以其直观有效的方法和相对较小的模型规模，在文本嵌入任务中展现出卓越的性能。"}}
{"id": "2509.12838", "categories": ["cs.RO", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.12838", "abs": "https://arxiv.org/abs/2509.12838", "authors": ["Kento Murata", "Shoichi Hasegawa", "Tomochika Ishikawa", "Yoshinobu Hagiwara", "Akira Taniguchi", "Lotfi El Hafi", "Tadahiro Taniguchi"], "title": "Multi-Robot Task Planning for Multi-Object Retrieval Tasks with Distributed On-Site Knowledge via Large Language Models", "comment": "Submitted to AROB-ISBC 2026 (Journal Track option)", "summary": "It is crucial to efficiently execute instructions such as \"Find an apple and\na banana\" or \"Get ready for a field trip,\" which require searching for multiple\nobjects or understanding context-dependent commands. This study addresses the\nchallenging problem of determining which robot should be assigned to which part\nof a task when each robot possesses different situational on-site\nknowledge-specifically, spatial concepts learned from the area designated to it\nby the user. We propose a task planning framework that leverages large language\nmodels (LLMs) and spatial concepts to decompose natural language instructions\ninto subtasks and allocate them to multiple robots. We designed a novel\nfew-shot prompting strategy that enables LLMs to infer required objects from\nambiguous commands and decompose them into appropriate subtasks. In our\nexperiments, the proposed method achieved 47/50 successful assignments,\noutperforming random (28/50) and commonsense-based assignment (26/50).\nFurthermore, we conducted qualitative evaluations using two actual mobile\nmanipulators. The results demonstrated that our framework could handle\ninstructions, including those involving ad hoc categories such as \"Get ready\nfor a field trip,\" by successfully performing task decomposition, assignment,\nsequential planning, and execution.", "AI": {"tldr": "该研究提出一个基于大型语言模型（LLM）和空间概念的任务规划框架，用于将复杂的自然语言指令分解为子任务，并分配给具有不同现场知识的多个机器人。", "motivation": "现有系统难以高效执行涉及多对象搜索或上下文相关命令的复杂指令，并且在机器人拥有不同现场空间知识时，如何有效分配任务是一个挑战。", "method": "研究提出一个任务规划框架，利用LLM和空间概念将自然语言指令分解为子任务，并分配给多个机器人。设计了一种新颖的少样本提示策略，使LLM能够从模糊命令中推断所需对象并分解为合适的子任务。", "result": "该方法在任务分配中实现了47/50的成功率，优于随机分配（28/50）和基于常识的分配（26/50）。通过两台实际移动机械臂的定性评估表明，该框架能成功处理包括“准备野外考察”等临时类别指令的任务分解、分配、顺序规划和执行。", "conclusion": "该框架通过结合LLM和空间概念，能够有效处理复杂的自然语言指令，实现任务分解和机器人分配，并在实验中展现出卓越的性能和实际应用潜力。"}}
{"id": "2509.12511", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12511", "abs": "https://arxiv.org/abs/2509.12511", "authors": ["Benjamin Vail", "Rahul Harsha Cheppally", "Ajay Sharda", "Sidharth Rai"], "title": "Axis-Aligned 3D Stalk Diameter Estimation from RGB-D Imagery", "comment": "13 pages, 8 figures, 4 tables", "summary": "Accurate, high-throughput phenotyping is a critical component of modern crop\nbreeding programs, especially for improving traits such as mechanical\nstability, biomass production, and disease resistance. Stalk diameter is a key\nstructural trait, but traditional measurement methods are labor-intensive,\nerror-prone, and unsuitable for scalable phenotyping. In this paper, we present\na geometry-aware computer vision pipeline for estimating stalk diameter from\nRGB-D imagery. Our method integrates deep learning-based instance segmentation,\n3D point cloud reconstruction, and axis-aligned slicing via Principal Component\nAnalysis (PCA) to perform robust diameter estimation. By mitigating the effects\nof curvature, occlusion, and image noise, this approach offers a scalable and\nreliable solution to support high-throughput phenotyping in breeding and\nagronomic research.", "AI": {"tldr": "该论文提出了一种基于RGB-D图像的几何感知计算机视觉方法，用于高通量、准确地估算作物茎秆直径，以克服传统方法的局限性。", "motivation": "准确、高通量表型分析是现代作物育种的关键，尤其对于机械稳定性、生物量生产和抗病性等性状。茎秆直径是一个重要的结构性状，但传统测量方法劳动密集、易出错且不适用于可扩展的表型分析。", "method": "该方法整合了深度学习的实例分割、3D点云重建和通过主成分分析（PCA）进行的轴对齐切片，以实现鲁棒的直径估算。该管道利用RGB-D图像进行几何感知计算机视觉处理。", "result": "该方法能够缓解曲率、遮挡和图像噪声的影响，从而实现鲁棒的直径估算。它提供了一种可扩展且可靠的解决方案。", "conclusion": "这种基于计算机视觉的方法为育种和农学研究中的高通量表型分析提供了支持，解决了传统茎秆直径测量方法的痛点。"}}
{"id": "2509.12645", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2509.12645", "abs": "https://arxiv.org/abs/2509.12645", "authors": ["Lachlan McGinness", "Peter Baumgartner"], "title": "Large Language Models Imitate Logical Reasoning, but at what Cost?", "comment": "This work has been accepted as a main track paper for publication in\n  the proceedings of the Australasian Joint Conference on Artificial\n  Intelligence 2025 held in Canberra, Australia", "summary": "We present a longitudinal study which evaluates the reasoning capability of\nfrontier Large Language Models over an eighteen month period. We measured the\naccuracy of three leading models from December 2023, September 2024 and June\n2025 on true or false questions from the PrOntoQA dataset and their\nfaithfulness to reasoning strategies provided through in-context learning. The\nimprovement in performance from 2023 to 2024 can be attributed to hidden Chain\nof Thought prompting. The introduction of thinking models allowed for\nsignificant improvement in model performance between 2024 and 2025.\n  We then present a neuro-symbolic architecture which uses LLMs of less than 15\nbillion parameters to translate the problems into a standardised form. We then\nparse the standardised forms of the problems into a program to be solved by Z3,\nan SMT solver, to determine the satisfiability of the query. We report the\nnumber of prompt and completion tokens as well as the computational cost in\nFLOPs for open source models. The neuro-symbolic approach significantly reduces\nthe computational cost while maintaining near perfect performance. The common\napproximation that the number of inference FLOPs is double the product of the\nactive parameters and total tokens was accurate within 10\\% for all\nexperiments.", "AI": {"tldr": "本研究评估了前沿大型语言模型（LLM）在18个月内的推理能力演变，并提出了一种神经符号架构，该架构在保持高性能的同时显著降低了计算成本。", "motivation": "研究旨在评估前沿LLM在长时间跨度内的推理能力变化，并探索一种更高效、计算成本更低的推理问题解决方案。", "method": "研究方法包括：1) 对三款领先LLM进行了为期18个月的纵向研究（2023年12月、2024年9月、2025年6月），评估它们在PrOntoQA数据集上的真假问题准确性及其对情境学习推理策略的忠实度。2) 提出了一种神经符号架构，使用参数少于150亿的LLM将问题翻译成标准化形式，然后由SMT求解器Z3解析和解决。3) 报告了提示和完成令牌数量，以及开源模型的FLOPs计算成本。", "result": "研究发现：1) LLM的性能在2023年至2024年间有所提升（归因于隐藏的思维链提示），并在2024年至2025年间因引入“思考模型”而显著提升。2) 神经符号方法在保持近乎完美性能的同时，显著降低了计算成本。3) 推理FLOPs的常用近似公式（活跃参数与总令牌数乘积的两倍）在所有实验中误差在10%以内。", "conclusion": "LLM的推理能力在持续进步，且神经符号架构为推理任务提供了一种高效且高性能的替代方案，能够大幅降低计算成本。"}}
{"id": "2509.12908", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12908", "abs": "https://arxiv.org/abs/2509.12908", "authors": ["Caiqi Zhang", "Chang Shu", "Ehsan Shareghi", "Nigel Collier"], "title": "All Roads Lead to Rome: Graph-Based Confidence Estimation for Large Language Model Reasoning", "comment": "EMNLP 2025 Main", "summary": "Confidence estimation is essential for the reliable deployment of large\nlanguage models (LLMs). Existing methods are primarily designed for factual QA\ntasks and often fail to generalize to reasoning tasks. To address this gap, we\npropose a set of training-free, graph-based confidence estimation methods\ntailored to reasoning tasks. Our approach models reasoning paths as directed\ngraphs and estimates confidence by exploiting graph properties such as\ncentrality, path convergence, and path weighting. Experiments with two LLMs on\nthree reasoning datasets demonstrate improved confidence estimation and\nenhanced performance on two downstream tasks.", "AI": {"tldr": "该论文提出了一套无需训练的、基于图的置信度估计方法，专为大型语言模型（LLMs）的推理任务设计，并通过图属性提高了置信度估计的准确性。", "motivation": "现有的大语言模型置信度估计方法主要针对事实问答任务，在推理任务上表现不佳。为弥补这一空白，研究旨在开发适用于推理任务的置信度估计方法。", "method": "研究将推理路径建模为有向图，并通过利用图的属性（如中心性、路径收敛性和路径加权）来估计置信度。该方法无需训练。", "result": "在两个LLMs和三个推理数据集上的实验表明，所提出的方法显著改善了置信度估计，并提升了两个下游任务的性能。", "conclusion": "所提出的基于图的、无需训练的置信度估计方法能有效提高大型语言模型在推理任务中的置信度估计和下游任务表现。"}}
{"id": "2509.12846", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12846", "abs": "https://arxiv.org/abs/2509.12846", "authors": ["Junlin Song", "Antoine Richard", "Miguel Olivares-Mendez"], "title": "Unleashing the Power of Discrete-Time State Representation: Ultrafast Target-based IMU-Camera Spatial-Temporal Calibration", "comment": null, "summary": "Visual-inertial fusion is crucial for a large amount of intelligent and\nautonomous applications, such as robot navigation and augmented reality. To\nbootstrap and achieve optimal state estimation, the spatial-temporal\ndisplacements between IMU and cameras must be calibrated in advance. Most\nexisting calibration methods adopt continuous-time state representation, more\nspecifically the B-spline. Despite these methods achieve precise\nspatial-temporal calibration, they suffer from high computational cost caused\nby continuous-time state representation. To this end, we propose a novel and\nextremely efficient calibration method that unleashes the power of\ndiscrete-time state representation. Moreover, the weakness of discrete-time\nstate representation in temporal calibration is tackled in this paper. With the\nincreasing production of drones, cellphones and other visual-inertial\nplatforms, if one million devices need calibration around the world, saving one\nminute for the calibration of each device means saving 2083 work days in total.\nTo benefit both the research and industry communities, our code will be\nopen-source.", "AI": {"tldr": "本文提出了一种新颖且高效的离散时间状态表示视觉惯性标定方法，旨在解决现有连续时间方法计算成本高的问题，同时克服离散时间在时间标定上的弱点。", "motivation": "视觉惯性融合对于机器人导航和增强现实等智能自主应用至关重要。为实现最佳状态估计，IMU和相机之间的时空位移需要提前标定。现有方法多采用连续时间（B样条）表示，虽精确但计算成本高昂。随着视觉惯性平台设备数量的激增，亟需一种更高效的标定方法。", "method": "提出了一种利用离散时间状态表示的标定方法，以降低计算成本。同时，该方法解决了离散时间状态表示在时间标定方面的固有弱点。", "result": "该方法实现了“极其高效”的标定，能显著节省计算时间。例如，为一百万台设备标定每台节省一分钟，总共可节省2083个工作日。", "conclusion": "本文开发了一种高效的离散时间视觉惯性标定方法，显著降低了计算成本，对学术界和工业界均有益，并将开源代码。"}}
{"id": "2509.12544", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12544", "abs": "https://arxiv.org/abs/2509.12544", "authors": ["Can Peng", "Yuyuan Liu", "Yingyu Yang", "Pramit Saha", "Qianye Yang", "J. Alison Noble"], "title": "Neural Collapse-Inspired Multi-Label Federated Learning under Label-Distribution Skew", "comment": null, "summary": "Federated Learning (FL) enables collaborative model training across\ndistributed clients while preserving data privacy. However, the performance of\ndeep learning often deteriorates in FL due to decentralized and heterogeneous\ndata. This challenge is further amplified in multi-label scenarios, where data\nexhibit complex characteristics such as label co-occurrence, inter-label\ndependency, and discrepancies between local and global label relationships.\nWhile most existing FL research primarily focuses on single-label\nclassification, many real-world applications, particularly in domains such as\nmedical imaging, often involve multi-label settings. In this paper, we address\nthis important yet underexplored scenario in FL, where clients hold multi-label\ndata with skewed label distributions. Neural Collapse (NC) describes a\ngeometric structure in the latent feature space where features of each class\ncollapse to their class mean with vanishing intra-class variance, and the class\nmeans form a maximally separated configuration. Motivated by this theory, we\npropose a method to align feature distributions across clients and to learn\nhigh-quality, well-clustered representations. To make the NC-structure\napplicable to multi-label settings, where image-level features may contain\nmultiple semantic concepts, we introduce a feature disentanglement module that\nextracts semantically specific features. The clustering of these disentangled\nclass-wise features is guided by a predefined shared NC structure, which\nmitigates potential conflicts between client models due to diverse local data\ndistributions. In addition, we design regularisation losses to encourage\ncompact clustering in the latent feature space. Experiments conducted on four\nbenchmark datasets across eight diverse settings demonstrate that our approach\noutperforms existing methods, validating its effectiveness in this challenging\nFL scenario.", "AI": {"tldr": "本文提出了一种联邦学习（FL）方法，旨在解决多标签数据场景下由于数据异构性和复杂标签关系导致的性能下降问题。该方法结合了神经坍缩（Neural Collapse）理论和特征解耦模块，以对齐客户端特征分布并学习高质量、聚类良好的表示。", "motivation": "联邦学习在去中心化和异构数据下性能下降，在多标签场景中，由于标签共现、标签间依赖以及局部与全局标签关系差异等复杂特性，这一挑战被进一步放大。现有FL研究主要关注单标签分类，但许多实际应用（如医学影像）涉及多标签设置，这是一个重要但未被充分探索的领域。", "method": "本文提出了一种基于神经坍缩（NC）理论的方法，用于对齐客户端间的特征分布并学习高质量的聚类表示。为适应多标签设置，引入了一个特征解耦模块来提取语义特定的特征。这些解耦后的类别特征的聚类由预定义的共享NC结构引导，以缓解客户端模型间的冲突。此外，还设计了正则化损失以促进潜在特征空间中的紧凑聚类。", "result": "在四个基准数据集和八种不同设置下的实验表明，本文提出的方法优于现有方法，验证了其在这一具有挑战性的FL场景中的有效性。", "conclusion": "本文提出的方法有效解决了联邦学习中多标签数据分布倾斜的挑战性问题。通过结合神经坍缩理论和特征解耦，该方法能够对齐特征分布并学习高质量的表示，并在实验中展现出优越的性能。"}}
{"id": "2509.12743", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.12743", "abs": "https://arxiv.org/abs/2509.12743", "authors": ["Hanqing Li", "Kiran Sheena Jyothi", "Henry Liang", "Sharika Mahadevan", "Diego Klabjan"], "title": "Zero-shot Graph Reasoning via Retrieval Augmented Framework with LLMs", "comment": null, "summary": "We propose a new, training-free method, Graph Reasoning via Retrieval\nAugmented Framework (GRRAF), that harnesses retrieval-augmented generation\n(RAG) alongside the code-generation capabilities of large language models\n(LLMs) to address a wide range of graph reasoning tasks. In GRRAF, the target\ngraph is stored in a graph database, and the LLM is prompted to generate\nexecutable code queries that retrieve the necessary information. This approach\ncircumvents the limitations of existing methods that require extensive\nfinetuning or depend on predefined algorithms, and it incorporates an error\nfeedback loop with a time-out mechanism to ensure both correctness and\nefficiency. Experimental evaluations on the GraphInstruct dataset reveal that\nGRRAF achieves 100% accuracy on most graph reasoning tasks, including cycle\ndetection, bipartite graph checks, shortest path computation, and maximum flow,\nwhile maintaining consistent token costs regardless of graph sizes. Imperfect\nbut still very high performance is observed on subgraph matching. Notably,\nGRRAF scales effectively to large graphs with up to 10,000 nodes.", "AI": {"tldr": "本文提出了一种名为GRRAF的免训练方法，它结合检索增强生成（RAG）和大型语言模型（LLM）的代码生成能力，以解决广泛的图推理任务，并在大多数任务上实现了高准确率和良好的可扩展性。", "motivation": "现有的图推理方法通常需要大量的微调或依赖预定义的算法，这限制了它们的灵活性和应用范围。GRRAF旨在克服这些限制。", "method": "GRRAF将目标图存储在图数据库中，并提示LLM生成可执行的代码查询来检索所需信息。该方法包含一个带有超时机制的错误反馈循环，以确保正确性和效率。", "result": "在GraphInstruct数据集上的实验评估显示，GRRAF在大多数图推理任务（包括环检测、二分图检查、最短路径计算和最大流）上实现了100%的准确率，并且无论图大小如何，都能保持一致的token成本。在子图匹配任务上，性能虽不完美但仍非常高。值得注意的是，GRRAF能有效扩展到多达10,000个节点的大型图。", "conclusion": "GRRAF是一种有效、免训练的图推理方法，它利用RAG和LLM的代码生成能力，在多种图推理任务上表现出色，具有高准确性、效率和良好的可扩展性，并且不受图大小影响，能有效处理大规模图。"}}
{"id": "2509.12955", "categories": ["cs.CL", "cs.DL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.12955", "abs": "https://arxiv.org/abs/2509.12955", "authors": ["Heng Zhang", "Chengzhi Zhang"], "title": "Automated Generation of Research Workflows from Academic Papers: A Full-text Mining Framework", "comment": null, "summary": "The automated generation of research workflows is essential for improving the\nreproducibility of research and accelerating the paradigm of \"AI for Science\".\nHowever, existing methods typically extract merely fragmented procedural\ncomponents and thus fail to capture complete research workflows. To address\nthis gap, we propose an end-to-end framework that generates comprehensive,\nstructured research workflows by mining full-text academic papers. As a case\nstudy in the Natural Language Processing (NLP) domain, our paragraph-centric\napproach first employs Positive-Unlabeled (PU) Learning with SciBERT to\nidentify workflow-descriptive paragraphs, achieving an F1-score of 0.9772.\nSubsequently, we utilize Flan-T5 with prompt learning to generate workflow\nphrases from these paragraphs, yielding ROUGE-1, ROUGE-2, and ROUGE-L scores of\n0.4543, 0.2877, and 0.4427, respectively. These phrases are then systematically\ncategorized into data preparation, data processing, and data analysis stages\nusing ChatGPT with few-shot learning, achieving a classification precision of\n0.958. By mapping categorized phrases to their document locations in the\ndocuments, we finally generate readable visual flowcharts of the entire\nresearch workflows. This approach facilitates the analysis of workflows derived\nfrom an NLP corpus and reveals key methodological shifts over the past two\ndecades, including the increasing emphasis on data analysis and the transition\nfrom feature engineering to ablation studies. Our work offers a validated\ntechnical framework for automated workflow generation, along with a novel,\nprocess-oriented perspective for the empirical investigation of evolving\nscientific paradigms. Source code and data are available at:\nhttps://github.com/ZH-heng/research_workflow.", "AI": {"tldr": "该研究提出了一个端到端框架，通过挖掘学术论文全文，自动生成全面、结构化的研究工作流程，以提高研究可复现性并加速“AI for Science”范式。", "motivation": "现有方法通常只能提取碎片化的程序组件，无法捕获完整的研究工作流程，这阻碍了研究可复现性和“AI for Science”范式的发展。", "method": "该方法以段落为中心，首先使用带有SciBERT的PU学习识别描述工作流程的段落；接着，利用带有提示学习的Flan-T5从这些段落生成工作流程短语；然后，使用带有少样本学习的ChatGPT将短语分类到数据准备、数据处理和数据分析阶段；最后，将分类后的短语映射到文档位置，生成可读的视觉流程图。以自然语言处理（NLP）领域为例进行了案例研究。", "result": "在段落识别方面，F1分数达到0.9772；在短语生成方面，ROUGE-1、ROUGE-2和ROUGE-L分数分别为0.4543、0.2877和0.4427；在短语分类方面，分类精度达到0.958。该方法成功生成了可读的视觉流程图，并揭示了过去二十年NLP领域关键的方法学转变，例如对数据分析的日益重视以及从特征工程到消融研究的转变。", "conclusion": "该工作提供了一个经过验证的自动化工作流程生成技术框架，以及一个新颖的、面向过程的视角，用于实证研究不断演变的科学范式。"}}
{"id": "2509.12851", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.12851", "abs": "https://arxiv.org/abs/2509.12851", "authors": ["Antoine Lénat", "Olivier Cheminat", "Damien Chablat", "Camilo Charron"], "title": "A Novel Skill Modeling Approach: Integrating Vergnaud's Scheme with Cognitive Architectures", "comment": null, "summary": "Human-machine interaction is increasingly important in industry, and this\ntrend will only intensify with the rise of Industry 5.0. Human operators have\nskills that need to be adapted when using machines to achieve the best results.\nIt is crucial to highlight the operator's skills and understand how they use\nand adapt them [18]. A rigorous description of these skills is necessary to\ncompare performance with and without robot assistance. Predicate logic, used by\nVergnaud within Piaget's scheme concept, offers a promising approach. However,\nthis theory doesn't account for cognitive system constraints, such as the\ntiming of actions, the limitation of cognitive resources, the parallelization\nof tasks, or the activation of automatic gestures contrary to optimal\nknowledge. Integrating these constraints is essential for representing agent\nskills understanding skill transfer between biological and mechanical\nstructures. Cognitive architectures models [2] address these needs by\ndescribing cognitive structure and can be combined with the scheme for mutual\nbenefit. Welding provides a relevant case study, as it highlights the\nchallenges faced by operators, even highly skilled ones. Welding's complexity\nstems from the need for constant skill adaptation to variable parameters like\npart position and process. This adaptation is crucial, as weld quality, a key\nfactor, is only assessed afterward via destructive testing. Thus, the welder is\nconfronted with a complex perception-decision-action cycle, where the\nevaluation of the impact of his actions is delayed and where errors are\ndefinitive. This dynamic underscores the importance of understanding and\nmodeling the skills of operators.", "AI": {"tldr": "随着工业5.0的兴起，人机交互中的人类技能适应性变得至关重要。本文提出将谓词逻辑与认知架构相结合，以更全面地建模操作员技能，并考虑认知约束，尤其是在焊接等复杂任务中，以实现更好的技能理解和转移。", "motivation": "工业5.0背景下人机交互日益重要，需要理解和适应人类操作员技能以优化结果。现有理论（如谓词逻辑）未能充分考虑认知系统约束（如行动时序、认知资源限制）。焊接等复杂任务中，操作员需持续适应变量，且行动后果延迟评估，凸显了精确建模操作员技能的迫切性。", "method": "提议结合谓词逻辑（基于皮亚杰的图式概念）与认知架构模型。谓词逻辑用于描述技能，而认知架构则用于整合认知约束，如行动时序、认知资源限制、任务并行化和自动手势激活。以焊接作为相关案例研究，来验证和应用这种集成方法。", "result": "通过整合谓词逻辑和认知架构，可以更全面、更严谨地描述和理解操作员技能，包括其适应性及认知约束。这将有助于比较有无机器人辅助时的性能，并促进生物结构与机械结构之间的技能转移理解，从而提升人机协作效率和任务质量。", "conclusion": "深入理解和建模操作员技能，特别是整合认知约束的技能模型，对于优化人机交互和技能转移至关重要。将谓词逻辑与认知架构相结合，为解决这一挑战提供了一个有前景的方法，尤其适用于焊接等需要高度技能适应性的复杂工业场景。"}}
{"id": "2509.12546", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12546", "abs": "https://arxiv.org/abs/2509.12546", "authors": ["Yingxin Lai", "Zitong Yu", "Jun Wang", "Linlin Shen", "Yong Xu", "Xiaochun Cao"], "title": "Agent4FaceForgery: Multi-Agent LLM Framework for Realistic Face Forgery Detection", "comment": null, "summary": "Face forgery detection faces a critical challenge: a persistent gap between\noffline benchmarks and real-world efficacy,which we attribute to the ecological\ninvalidity of training data.This work introduces Agent4FaceForgery to address\ntwo fundamental problems: (1) how to capture the diverse intents and iterative\nprocesses of human forgery creation, and (2) how to model the complex, often\nadversarial, text-image interactions that accompany forgeries in social media.\nTo solve this,we propose a multi-agent framework where LLM-poweredagents,\nequipped with profile and memory modules, simulate the forgery creation\nprocess. Crucially, these agents interact in a simulated social environment to\ngenerate samples labeled for nuanced text-image consistency, moving beyond\nsimple binary classification. An Adaptive Rejection Sampling (ARS) mechanism\nensures data quality and diversity. Extensive experiments validate that the\ndata generated by our simulationdriven approach brings significant performance\ngains to detectors of multiple architectures, fully demonstrating the\neffectiveness and value of our framework.", "AI": {"tldr": "为解决人脸伪造检测中基准与实际应用之间的差距，本文提出Agent4FaceForgery多智能体框架，通过模拟人类伪造创建过程和社交互动，生成高质量、生态有效的训练数据，显著提升了检测器的性能。", "motivation": "人脸伪造检测面临的挑战是离线基准与实际应用效果之间存在持续差距，这归因于训练数据的生态无效性。具体而言，需要解决如何捕捉人类伪造的多样意图和迭代过程，以及如何建模社交媒体中伪造伴随的复杂、对抗性文本-图像互动。", "method": "本文提出了Agent4FaceForgery框架，采用由大型语言模型（LLM）驱动的多智能体，这些智能体配备了档案和记忆模块，模拟伪造创建过程。智能体在一个模拟社交环境中互动，生成带有细致文本-图像一致性标签的样本（超越简单的二分类）。此外，采用自适应拒绝采样（ARS）机制确保数据质量和多样性。", "result": "通过广泛实验验证，该模拟驱动方法生成的数据显著提升了多种架构检测器的性能，充分证明了该框架的有效性和价值。", "conclusion": "Agent4FaceForgery框架通过模拟伪造过程和社交互动，生成了具有生态有效性的训练数据，有效解决了人脸伪造检测在基准与实际应用之间的差距，显著提升了检测性能，具有重要的实际应用价值。"}}
{"id": "2509.12810", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12810", "abs": "https://arxiv.org/abs/2509.12810", "authors": ["Shicheng Ye", "Chao Yu", "Kaiqiang Ke", "Chengdong Xu", "Yinqi Wei"], "title": "H$^2$R: Hierarchical Hindsight Reflection for Multi-Task LLM Agents", "comment": null, "summary": "Large language model (LLM)-based agents have shown strong potential in\nmulti-task scenarios, owing to their ability to transfer knowledge across\ndiverse tasks. However, existing approaches often treat prior experiences and\nknowledge as monolithic units, leading to inefficient and coarse-grained\nknowledge transfer. In this work, we propose a novel hierarchical memory\narchitecture that enables fine-grained knowledge transfer by decoupling\nhigh-level planning memory from low-level execution memory. To construct and\nrefine these hierarchical memories, we introduce Hierarchical Hindsight\nReflection (H$^2$R), a mechanism that distills reusable and hierarchical\nknowledge from past agent-environment interactions. At test time, H$^2$R\nperforms retrievals of high-level and low-level memories separately, allowing\nLLM-based agents to efficiently access and utilize task-relevant knowledge for\nnew tasks.Experimental results across two benchmarks demonstrate that H$^2$R\ncan improve generalization and decision-making performance, outperforming prior\nbaselines such as Expel.", "AI": {"tldr": "本文提出了一种名为H$^2$R的新型分层记忆架构，通过解耦高层规划和低层执行记忆，实现大型语言模型（LLM）代理的细粒度知识迁移，从而提高泛化和决策性能。", "motivation": "现有LLM代理的知识迁移方法将先验经验和知识视为单一整体，导致知识迁移效率低下且粒度粗糙。", "method": "引入了分层记忆架构，将高层规划记忆与低层执行记忆解耦。同时，提出了分层事后反思（Hierarchical Hindsight Reflection, H$^2$R）机制，用于从过去的代理-环境交互中提炼可重用且分层的知识，并在测试时分别检索高层和低层记忆。", "result": "在两个基准测试中，H$^2$R显著提高了泛化能力和决策性能，优于Expel等现有基线方法。", "conclusion": "H$^2$R通过其分层记忆架构和反思机制，有效实现了LLM代理的细粒度知识迁移，提升了其在新任务中的适应性和表现。"}}
{"id": "2509.12960", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12960", "abs": "https://arxiv.org/abs/2509.12960", "authors": ["Yuval Weiss", "David Demitri Africa", "Paula Buttery", "Richard Diehl Martinez"], "title": "Investigating ReLoRA: Effects on the Learning Dynamics of Small Language Models", "comment": "12 Pages, 6 Tables, 8 Figures", "summary": "Parameter-efficient methods such as LoRA have revolutionised the fine-tuning\nof LLMs. Still, their extension to pretraining via ReLoRA is less well\nunderstood, especially for small language models (SLMs), which offer lower\ncomputational and environmental costs. This work is the first systematic study\nof ReLoRA in SLMs (11M-66M parameters), evaluating both performance and\nlearning dynamics. Through ablation experiments, we find that ReLoRA generally\nperforms worse than standard training on loss, Paloma perplexity and BLiMP,\nwith the gap widening for the larger models. Further analysis of the learning\ndynamics of the models indicates that ReLoRA reinforces the rank deficiencies\nfound in smaller models. These results indicate that low-rank update strategies\nmay not transfer easily to SLM pretraining, highlighting the need for more\nresearch in the low-compute regime.", "AI": {"tldr": "本研究首次系统性评估了ReLoRA在小型语言模型(SLM)预训练中的表现，发现其性能普遍不如标准训练，并可能加剧模型中的秩亏损问题。", "motivation": "LoRA等参数高效方法彻底改变了大型语言模型(LLM)的微调，但ReLoRA在预训练中的扩展，特别是对于计算和环境成本较低的小型语言模型(SLM)，尚不清楚。", "method": "对11M-66M参数范围内的SLM进行了ReLoRA的首次系统性研究，通过消融实验评估了其性能和学习动态，包括损失、Paloma困惑度和BLiMP指标。", "result": "ReLoRA在损失、Paloma困惑度和BLiMP上普遍比标准训练表现更差，且模型越大，性能差距越明显。学习动态分析表明，ReLoRA强化了较小模型中存在的秩亏损问题。", "conclusion": "研究结果表明低秩更新策略可能不易直接迁移到SLM的预训练中，凸显了在低计算量条件下进行更多相关研究的必要性。"}}
{"id": "2509.12858", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.12858", "abs": "https://arxiv.org/abs/2509.12858", "authors": ["Yidan Lu", "Rurui Yang", "Qiran Kou", "Mengting Chen", "Tao Fan", "Peter Cui", "Yinzhao Dong", "Peng Lu"], "title": "Contrastive Representation Learning for Robust Sim-to-Real Transfer of Adaptive Humanoid Locomotion", "comment": null, "summary": "Reinforcement learning has produced remarkable advances in humanoid\nlocomotion, yet a fundamental dilemma persists for real-world deployment:\npolicies must choose between the robustness of reactive proprioceptive control\nor the proactivity of complex, fragile perception-driven systems. This paper\nresolves this dilemma by introducing a paradigm that imbues a purely\nproprioceptive policy with proactive capabilities, achieving the foresight of\nperception without its deployment-time costs. Our core contribution is a\ncontrastive learning framework that compels the actor's latent state to encode\nprivileged environmental information from simulation. Crucially, this\n``distilled awareness\" empowers an adaptive gait clock, allowing the policy to\nproactively adjust its rhythm based on an inferred understanding of the\nterrain. This synergy resolves the classic trade-off between rigid, clocked\ngaits and unstable clock-free policies. We validate our approach with zero-shot\nsim-to-real transfer to a full-sized humanoid, demonstrating highly robust\nlocomotion over challenging terrains, including 30 cm high steps and 26.5{\\deg}\nslopes, proving the effectiveness of our method. Website:\nhttps://lu-yidan.github.io/cra-loco.", "AI": {"tldr": "本文提出一种新范式，通过对比学习让纯本体感知策略获得前瞻性感知能力，实现自适应步态，从而在无需额外感知部署成本的情况下，使人形机器人能在复杂地形上实现高度鲁棒的运动。", "motivation": "在人形机器人运动控制中，存在一个核心困境：是选择鲁棒但反应式的本体感知控制，还是选择前瞻性但脆弱的感知驱动系统。这限制了强化学习在真实世界中的部署。", "method": "核心贡献是引入一个对比学习框架，强制执行器（actor）的潜在状态编码来自模拟的特权环境信息。这种“蒸馏意识”赋能了自适应步态时钟，使策略能够根据对地形的推断理解主动调整其节奏，解决了刚性固定步态与不稳定无时钟策略之间的权衡问题。", "result": "该方法通过零样本（zero-shot）从模拟到真实世界的迁移，在一个全尺寸人形机器人上得到验证，在具有挑战性的地形（包括30厘米高的台阶和26.5度斜坡）上展示了高度鲁棒的运动能力。", "conclusion": "该研究通过为纯本体感知策略赋予前瞻性能力，有效解决了鲁棒性与前瞻性之间的困境，在不增加部署感知成本的情况下，实现了人形机器人在复杂地形上的高度鲁棒运动。"}}
{"id": "2509.12554", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12554", "abs": "https://arxiv.org/abs/2509.12554", "authors": ["Wenxuan Ji", "Haichao Shi", "Xiao-Yu zhang"], "title": "Explicit Multimodal Graph Modeling for Human-Object Interaction Detection", "comment": null, "summary": "Transformer-based methods have recently become the prevailing approach for\nHuman-Object Interaction (HOI) detection. However, the Transformer architecture\ndoes not explicitly model the relational structures inherent in HOI detection,\nwhich impedes the recognition of interactions. In contrast, Graph Neural\nNetworks (GNNs) are inherently better suited for this task, as they explicitly\nmodel the relationships between human-object pairs. Therefore, in this paper,\nwe propose \\textbf{M}ultimodal \\textbf{G}raph \\textbf{N}etwork\n\\textbf{M}odeling (MGNM) that leverages GNN-based relational structures to\nenhance HOI detection. Specifically, we design a multimodal graph network\nframework that explicitly models the HOI task in a four-stage graph structure.\nFurthermore, we introduce a multi-level feature interaction mechanism within\nour graph network. This mechanism leverages multi-level vision and language\nfeatures to enhance information propagation across human-object pairs.\nConsequently, our proposed MGNM achieves state-of-the-art performance on two\nwidely used benchmarks: HICO-DET and V-COCO. Moreover, when integrated with a\nmore advanced object detector, our method demonstrates a significant\nperformance gain and maintains an effective balance between rare and non-rare\nclasses.", "AI": {"tldr": "针对Transformer在HOI检测中缺乏显式关系建模的问题，本文提出了多模态图网络建模（MGNM），通过GNN-based的多阶段图结构和多级特征交互机制，显著提升了HOI检测性能，并在HICO-DET和V-COCO数据集上取得了最先进的结果。", "motivation": "Transformer在人体-物体交互（HOI）检测中表现出色，但其架构并未显式建模HOI固有的关系结构，这阻碍了交互识别。相比之下，图神经网络（GNNs）天生更适合这项任务，因为它们能够显式建模人-物体对之间的关系。", "method": "本文提出了多模态图网络建模（MGNM）。该方法利用基于GNN的关系结构来增强HOI检测。具体来说，设计了一个多模态图网络框架，以四阶段图结构显式建模HOI任务。此外，在图网络中引入了多级特征交互机制，利用多级视觉和语言特征来增强人-物体对之间的信息传播。", "result": "所提出的MGNM在HICO-DET和V-COCO这两个广泛使用的基准测试上均取得了最先进的性能。此外，当与更先进的物体检测器集成时，该方法显示出显著的性能提升，并能在稀有和非稀有类别之间保持有效的平衡。", "conclusion": "MGNM通过显式建模HOI任务中的关系结构，并结合多模态多级特征交互，克服了Transformer在该任务中的局限性，实现了卓越的HOI检测性能，并有效处理了类别不平衡问题。"}}
{"id": "2509.12875", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12875", "abs": "https://arxiv.org/abs/2509.12875", "authors": ["Jiaqi Wang", "Binquan Ji", "Haibo Luo", "Yiyang Qi", "Ruiting Li", "Huiyan Wang", "Yuantao Han", "Cangyi Yang", "jiaxu Zhang", "Feiliang Ren"], "title": "LTA-thinker: Latent Thought-Augmented Training Framework for Large Language Models on Complex Reasoning", "comment": null, "summary": "Complex Reasoning in Large Language Models can be dynamically optimized using\nTest-Time Scaling (TTS) to mitigate Overthinking. Methods such as Coconut,\nSoftCoT and its variant are effective in continuous latent space inference, the\ncore bottleneck still lies in the efficient generation and utilization of\nhigh-quality Latent Thought. Drawing from the theory of SoftCoT++ that a larger\nvariance in the generated Latent Thought distribution more closely approximates\nthe golden truth distribution, we propose a Latent Thought-Augmented Training\nFramework--LTA-Thinker, which improves distributional variance and enhances\nreasoning performance from two perspectives. First, LTA-Thinker constructs a\nLatent Thought generation architecture based on a learnable prior. This\narchitecture aims to increase the variance distribution of generated Latent\nThought Vectors in order to simplify the overall structure and raise the\nperformance ceiling. Second, LTA-Thinker introduces a distribution-based\ndirectional optimization paradigm that jointly constrains both distribution\nlocality and distribution scale. This mechanism improves information efficiency\nand computational cost through a multi-objective co-training strategy, which\ncombines standard Supervised Fine-Tuning (SFT) loss with two novel losses:\nSemantic Alignment Loss, which utilizes KL divergence to ensure that the Latent\nThought is highly relevant to the semantics of the question; Reasoning Focus\nLoss, which utilizes a contrastive learning mechanism to guide the model to\nfocus on the most critical reasoning steps. Experiments show that LTA-thinker\nachieves state-of-the-art (SOTA) performance among various baselines and\ndemonstrates a higher performance ceiling and better scaling effects.", "AI": {"tldr": "该研究提出了LTA-Thinker框架，通过优化潜在思维（Latent Thought）的生成和利用，增强了大型语言模型的复杂推理能力，并缓解了“过度思考”问题。", "motivation": "现有方法（如SoftCoT）在连续潜在空间推理中有效，但核心瓶颈在于高效生成和利用高质量潜在思维。理论表明，潜在思维分布的更大方差能更好地近似“黄金真理”分布，这促使研究者寻求提高方差和推理性能的方法。", "method": "LTA-Thinker框架从两个方面提升潜在思维的分布方差和推理性能：1. 构建基于可学习先验的潜在思维生成架构，旨在增加生成潜在思维向量的方差分布。2. 引入基于分布的方向优化范式，通过多目标协同训练策略（结合标准SFT损失、语义对齐损失和推理焦点损失）共同约束分布局部性和分布尺度，以提高信息效率和计算成本。", "result": "实验表明，LTA-Thinker在各种基线测试中取得了最先进（SOTA）的性能，并展现出更高的性能上限和更好的扩展效应。", "conclusion": "LTA-Thinker通过创新的潜在思维生成架构和分布优化范式，有效提高了大型语言模型的复杂推理能力，并实现了卓越的性能表现。"}}
{"id": "2509.12961", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.12961", "abs": "https://arxiv.org/abs/2509.12961", "authors": ["Chenye Zou", "Xingyue Wen", "Tianyi Hu", "Qian Janice Wang", "Daniel Hershcovich"], "title": "Do LLMs Understand Wine Descriptors Across Cultures? A Benchmark for Cultural Adaptations of Wine Reviews", "comment": "EMNLP 2025 Findings", "summary": "Recent advances in large language models (LLMs) have opened the door to\nculture-aware language tasks. We introduce the novel problem of adapting wine\nreviews across Chinese and English, which goes beyond literal translation by\nincorporating regional taste preferences and culture-specific flavor\ndescriptors. In a case study on cross-cultural wine review adaptation, we\ncompile the first parallel corpus of professional reviews, containing 8k\nChinese and 16k Anglophone reviews. We benchmark both\nneural-machine-translation baselines and state-of-the-art LLMs with automatic\nmetrics and human evaluation. For the latter, we propose three culture-oriented\ncriteria -- Cultural Proximity, Cultural Neutrality, and Cultural Genuineness\n-- to assess how naturally a translated review resonates with target-culture\nreaders. Our analysis shows that current models struggle to capture cultural\nnuances, especially in translating wine descriptions across different cultures.\nThis highlights the challenges and limitations of translation models in\nhandling cultural content.", "AI": {"tldr": "该研究引入了跨文化葡萄酒评论改编的新问题，构建了首个中英文平行语料库，并评估了当前模型在处理地域口味和文化特定描述方面的不足。", "motivation": "随着大型语言模型(LLMs)在文化感知语言任务方面的进展，研究者发现现有翻译模型难以超越字面翻译，未能融入地域口味偏好和文化特定风味描述，因此提出了跨文化葡萄酒评论改编这一新问题。", "method": "该研究定义了中英文葡萄酒评论的跨文化改编问题，并为此构建了首个包含8k中文和16k英文专业评论的平行语料库。研究者使用自动指标和人工评估对神经机器翻译基线模型和先进的LLMs进行了基准测试，并提出了文化接近度、文化中立性和文化真实性三个面向文化的人工评估标准。", "result": "分析结果显示，当前模型在捕捉文化细微差别方面表现不佳，尤其是在跨文化翻译葡萄酒描述时难以有效处理，无法自然地与目标文化读者产生共鸣。", "conclusion": "该研究强调了当前翻译模型在处理复杂文化内容方面的挑战和局限性，特别是在需要整合地域偏好和文化特定描述的语言任务中。"}}
{"id": "2509.12863", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.12863", "abs": "https://arxiv.org/abs/2509.12863", "authors": ["Haozhan Ni", "Jingsong Liang", "Chenyu He", "Yuhong Cao", "Guillaume Sartoretti"], "title": "GRATE: a Graph transformer-based deep Reinforcement learning Approach for Time-efficient autonomous robot Exploration", "comment": null, "summary": "Autonomous robot exploration (ARE) is the process of a robot autonomously\nnavigating and mapping an unknown environment. Recent Reinforcement Learning\n(RL)-based approaches typically formulate ARE as a sequential decision-making\nproblem defined on a collision-free informative graph. However, these methods\noften demonstrate limited reasoning ability over graph-structured data.\nMoreover, due to the insufficient consideration of robot motion, the resulting\nRL policies are generally optimized to minimize travel distance, while\nneglecting time efficiency. To overcome these limitations, we propose GRATE, a\nDeep Reinforcement Learning (DRL)-based approach that leverages a Graph\nTransformer to effectively capture both local structure patterns and global\ncontextual dependencies of the informative graph, thereby enhancing the model's\nreasoning capability across the entire environment. In addition, we deploy a\nKalman filter to smooth the waypoint outputs, ensuring that the resulting path\nis kinodynamically feasible for the robot to follow. Experimental results\ndemonstrate that our method exhibits better exploration efficiency (up to 21.5%\nin distance and 21.3% in time to complete exploration) than state-of-the-art\nconventional and learning-based baselines in various simulation benchmarks. We\nalso validate our planner in real-world scenarios.", "AI": {"tldr": "本文提出了一种名为GRATE的深度强化学习方法，通过结合图Transformer和卡尔曼滤波器，解决了现有自主机器人探索（ARE）方法在图数据推理能力和运动效率方面的不足，显著提高了探索效率和路径可行性。", "motivation": "现有的基于强化学习的自主机器人探索（ARE）方法在处理图结构数据时推理能力有限，并且由于对机器人运动考虑不足，导致优化策略往往只关注最短距离而忽略了时间效率。", "method": "本文提出了GRATE，一种基于深度强化学习（DRL）的方法。它利用图Transformer有效捕捉信息图的局部结构模式和全局上下文依赖性，从而增强模型在整个环境中的推理能力。此外，部署了卡尔曼滤波器来平滑路径点输出，确保生成的路径在运动学上对机器人是可行的。", "result": "实验结果表明，与现有最先进的传统和基于学习的基线相比，GRATE在各种模拟基准测试中展现出更高的探索效率（完成探索的距离减少高达21.5%，时间减少高达21.3%）。该规划器也在真实世界场景中得到了验证。", "conclusion": "GRATE通过结合图Transformer和卡尔曼滤波器，有效克服了现有ARE方法在图数据推理和运动效率方面的局限性，实现了更高效且运动学可行的自主机器人探索。"}}
{"id": "2509.12556", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12556", "abs": "https://arxiv.org/abs/2509.12556", "authors": ["Kunliang Xie"], "title": "VQT-Light:Lightweight HDR Illumination Map Prediction with Richer Texture.pdf", "comment": "11 pages, 8 figures", "summary": "Accurate lighting estimation is a significant yet challenging task in\ncomputer vision and graphics. However, existing methods either struggle to\nrestore detailed textures of illumination map, or face challenges in running\nspeed and texture fidelity. To tackle this problem, we propose a novel\nframework (VQT-Light) based on VQVAE and ViT architecture. VQT-Light includes\ntwo modules: feature extraction and lighting estimation. First, we take\nadvantages of VQVAE to extract discrete features of illumination map rather\nthan continuous features to avoid \"posterior collapse\". Second, we capture\nglobal context and dependencies of input image through ViT rather than CNNs to\nimprove the prediction of illumination outside the field of view. Combining the\nabove two modules, we formulate the lighting estimation as a multiclass\nclassification task, which plays a key role in our pipeline. As a result, our\nmodel predicts light map with richer texture and better fidelity while keeping\nlightweight and fast. VQT-Light achieves an inference speed of 40FPS and\nimproves multiple evaluation metrics. Qualitative and quantitative experiments\ndemonstrate that the proposed method realizes superior results compared to\nexisting state-of-the-art methods.", "AI": {"tldr": "本文提出了一种名为VQT-Light的新型框架，结合VQVAE和ViT架构，旨在解决现有光照估计方法在纹理细节、运行速度和保真度方面的挑战，实现了更丰富纹理、更高保真度、更轻量且快速的光照图预测。", "motivation": "现有的光照估计方法在恢复光照图的详细纹理方面表现不佳，或者在运行速度和纹理保真度之间面临权衡挑战。", "method": "VQT-Light框架包含特征提取和光照估计两个模块。首先，利用VQVAE提取光照图的离散特征而非连续特征，以避免“后验坍塌”。其次，通过ViT而非CNN捕捉输入图像的全局上下文和依赖关系，以改善视野外光照的预测。最终，将光照估计公式化为多类别分类任务。", "result": "该模型能够预测具有更丰富纹理和更好保真度的光照图，同时保持轻量和快速（推理速度达到40FPS）。VQT-Light改进了多项评估指标，并通过定性和定量实验证明其优于现有最先进的方法。", "conclusion": "VQT-Light通过结合VQVAE和ViT的优势，有效地解决了光照估计中纹理细节、速度和保真度的问题，实现了卓越的性能，提供了更优质、更快速的光照估计结果。"}}
{"id": "2509.12914", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12914", "abs": "https://arxiv.org/abs/2509.12914", "authors": ["Tairan Fu", "David Campo-Nazareno", "Javier Coronado-Blázquez", "Javier Conde", "Pedro Reviriego", "Fabrizio Lombardi"], "title": "Stochastic Streets: A Walk Through Random LLM Address Generation in four European Cities", "comment": null, "summary": "Large Language Models (LLMs) are capable of solving complex math problems or\nanswer difficult questions on almost any topic, but can they generate random\nstreet addresses for European cities?", "AI": {"tldr": "该摘要质疑大型语言模型（LLMs）在解决复杂数学问题和回答各种难题之外，是否能够生成欧洲城市的随机街道地址。", "motivation": "研究动机在于探索LLMs除了其已知的复杂推理和广泛知识能力之外，是否具备生成特定格式的、随机且地理位置精确数据的能力。", "method": "摘要中未提及具体研究方法。", "result": "摘要中未提供研究结果，仅提出一个问题。", "conclusion": "摘要中未得出任何结论，仅提出一个待验证的问题。"}}
{"id": "2509.12994", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.12994", "abs": "https://arxiv.org/abs/2509.12994", "authors": ["Jian Gao", "Fufangchen Zhao", "Yiyang Zhang", "Danfeng Yan"], "title": "SitLLM: Large Language Models for Sitting Posture Health Understanding via Pressure Sensor Data", "comment": null, "summary": "Poor sitting posture is a critical yet often overlooked factor contributing\nto long-term musculoskeletal disorders and physiological dysfunctions. Existing\nsitting posture monitoring systems, although leveraging visual, IMU, or\npressure-based modalities, often suffer from coarse-grained recognition and\nlack the semantic expressiveness necessary for personalized feedback. In this\npaper, we propose \\textbf{SitLLM}, a lightweight multimodal framework that\nintegrates flexible pressure sensing with large language models (LLMs) to\nenable fine-grained posture understanding and personalized health-oriented\nresponse generation. SitLLM comprises three key components: (1) a\n\\textit{Gaussian-Robust Sensor Embedding Module} that partitions pressure maps\ninto spatial patches and injects local noise perturbations for robust feature\nextraction; (2) a \\textit{Prompt-Driven Cross-Modal Alignment Module} that\nreprograms sensor embeddings into the LLM's semantic space via multi-head\ncross-attention using the pre-trained vocabulary embeddings; and (3) a\n\\textit{Multi-Context Prompt Module} that fuses feature-level, structure-level,\nstatistical-level, and semantic-level contextual information to guide\ninstruction comprehension.", "AI": {"tldr": "本文提出了SitLLM，一个轻量级多模态框架，结合柔性压力传感和大型语言模型（LLMs），以实现细粒度的坐姿理解和个性化的健康导向响应生成。", "motivation": "不良坐姿是导致长期肌肉骨骼疾病和生理功能障碍的关键因素，但常被忽视。现有坐姿监测系统（基于视觉、IMU或压力）存在识别粒度粗糙、缺乏语义表达能力以及无法提供个性化反馈的问题。", "method": "SitLLM框架包含三个关键组件：1) 一个高斯鲁棒传感器嵌入模块，将压力图划分为空间块并注入局部噪声以提取鲁棒特征；2) 一个提示驱动跨模态对齐模块，通过多头交叉注意力将传感器嵌入重编程到LLM的语义空间；3) 一个多上下文提示模块，融合特征级、结构级、统计级和语义级上下文信息以指导指令理解。", "result": "SitLLM能够实现细粒度的坐姿理解，并生成个性化的、面向健康的响应。", "conclusion": "SitLLM通过整合柔性压力传感和LLMs，为解决现有坐姿监测系统的局限性提供了一种新颖且有效的方法，有望实现更精确和个性化的坐姿健康管理。"}}
{"id": "2509.12880", "categories": ["cs.RO", "cs.HC", "cs.LG", "68T05, 68T40", "I.2.9; I.2.6; H.5.2"], "pdf": "https://arxiv.org/pdf/2509.12880", "abs": "https://arxiv.org/abs/2509.12880", "authors": ["Anna Deichler", "Siyang Wang", "Simon Alexanderson", "Jonas Beskow"], "title": "Towards Context-Aware Human-like Pointing Gestures with RL Motion Imitation", "comment": "Presented at the Context-Awareness in HRI (CONAWA) Workshop, ACM/IEEE\n  International Conference on Human-Robot Interaction (HRI 2022), March 7, 2022", "summary": "Pointing is a key mode of interaction with robots, yet most prior work has\nfocused on recognition rather than generation. We present a motion capture\ndataset of human pointing gestures covering diverse styles, handedness, and\nspatial targets. Using reinforcement learning with motion imitation, we train\npolicies that reproduce human-like pointing while maximizing precision. Results\nshow our approach enables context-aware pointing behaviors in simulation,\nbalancing task performance with natural dynamics.", "AI": {"tldr": "该研究通过构建人类指向手势数据集，并结合强化学习和运动模仿，训练机器人生成类人且精确的指向行为。", "motivation": "以往研究多集中于机器人对指向手势的识别，而非生成。本研究旨在填补这一空白，使机器人能主动生成自然且有效的指向手势。", "method": "研究首先创建了一个包含多种风格、惯用手和空间目标的人类指向手势动作捕捉数据集。然后，利用强化学习和运动模仿技术，训练策略以复现类人指向动作，并最大化指向精度。", "result": "结果表明，该方法使机器人在模拟环境中能够生成情境感知（context-aware）的指向行为，成功平衡了任务性能与自然动态。", "conclusion": "该研究成功地开发了一种使机器人生成类人、精确且情境感知的指向手势的方法，为机器人与人类的自然交互提供了新的途径。"}}
{"id": "2509.12569", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12569", "abs": "https://arxiv.org/abs/2509.12569", "authors": ["Qi Wang", "Shuliang Zhu", "Jinjia Zhou"], "title": "Adaptive Sampling Scheduler", "comment": "10 pages, 10 figures,2 Tables, 18 Equations", "summary": "Consistent distillation methods have evolved into effective techniques that\nsignificantly accelerate the sampling process of diffusion models. Although\nexisting methods have achieved remarkable results, the selection of target\ntimesteps during distillation mainly relies on deterministic or stochastic\nstrategies, which often require sampling schedulers to be designed specifically\nfor different distillation processes. Moreover, this pattern severely limits\nflexibility, thereby restricting the full sampling potential of diffusion\nmodels in practical applications. To overcome these limitations, this paper\nproposes an adaptive sampling scheduler that is applicable to various\nconsistency distillation frameworks. The scheduler introduces three innovative\nstrategies: (i) dynamic target timestep selection, which adapts to different\nconsistency distillation frameworks by selecting timesteps based on their\ncomputed importance; (ii) Optimized alternating sampling along the solution\ntrajectory by guiding forward denoising and backward noise addition based on\nthe proposed time step importance, enabling more effective exploration of the\nsolution space to enhance generation performance; and (iii) Utilization of\nsmoothing clipping and color balancing techniques to achieve stable and\nhigh-quality generation results at high guidance scales, thereby expanding the\napplicability of consistency distillation models in complex generation\nscenarios. We validated the effectiveness and flexibility of the adaptive\nsampling scheduler across various consistency distillation methods through\ncomprehensive experimental evaluations. Experimental results consistently\ndemonstrated significant improvements in generative performance, highlighting\nthe strong adaptability achieved by our method.", "AI": {"tldr": "本文提出了一种自适应采样调度器，通过动态时间步选择、优化的交替采样和图像处理技术，显著提升了各种一致性蒸馏扩散模型的灵活性和生成性能。", "motivation": "现有的一致性蒸馏方法在目标时间步选择上依赖确定性或随机策略，导致采样调度器需要针对不同蒸馏过程进行专门设计，限制了灵活性和扩散模型在实际应用中的采样潜力。", "method": "本文引入了一个自适应采样调度器，包含三项创新策略：(i) 基于计算出的重要性动态选择目标时间步，以适应不同的一致性蒸馏框架；(ii) 沿着解轨迹优化交替采样，通过时间步重要性指导前向去噪和后向加噪，更有效地探索解空间；(iii) 利用平滑裁剪和色彩平衡技术，在高引导尺度下实现稳定且高质量的生成结果。", "result": "通过全面的实验评估，该自适应采样调度器在各种一致性蒸馏方法中展现出有效性和灵活性。实验结果一致表明，生成性能显著提升，突显了该方法强大的适应性。", "conclusion": "所提出的自适应采样调度器克服了现有蒸馏方法的局限性，通过创新的时间步选择和采样策略，显著提高了扩散模型在复杂生成场景中的灵活性、稳定性和生成质量。"}}
{"id": "2509.12926", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12926", "abs": "https://arxiv.org/abs/2509.12926", "authors": ["Jai Singla", "Peal Jotania", "Keivalya Pandya"], "title": "Population Estimation using Deep Learning over Gandhinagar Urban Area", "comment": null, "summary": "Population estimation is crucial for various applications, from resource\nallocation to urban planning. Traditional methods such as surveys and censuses\nare expensive, time-consuming and also heavily dependent on human resources,\nrequiring significant manpower for data collection and processing. In this\nstudy a deep learning solution is proposed to estimate population using high\nresolution (0.3 m) satellite imagery, Digital Elevation Models (DEM) of 0.5m\nresolution and vector boundaries. Proposed method combines Convolution Neural\nNetwork (CNN) architecture for classification task to classify buildings as\nresidential and non-residential and Artificial Neural Network (ANN)\narchitecture to estimate the population. Approx. 48k building footprints over\nGandhinagar urban area are utilized containing both residential and\nnon-residential, with residential categories further used for building-level\npopulation estimation. Experimental results on a large-scale dataset\ndemonstrate the effectiveness of our model, achieving an impressive overall\nF1-score of 0.9936. The proposed system employs advanced geospatial analysis\nwith high spatial resolution to estimate Gandhinagar population at 278,954. By\nintegrating real-time data updates, standardized metrics, and infrastructure\nplanning capabilities, this automated approach addresses critical limitations\nof conventional census-based methodologies. The framework provides\nmunicipalities with a scalable and replicable tool for optimized resource\nmanagement in rapidly urbanizing cities, showcasing the efficiency of AI-driven\ngeospatial analytics in enhancing data-driven urban governance.", "AI": {"tldr": "本研究提出了一种深度学习解决方案，利用高分辨率卫星图像、DEM和矢量边界，结合CNN和ANN模型，实现建筑分类和人口估算，旨在提供一种高效、自动化的城市人口统计工具。", "motivation": "传统的人口普查和调查方法成本高昂、耗时且严重依赖人力资源，存在效率和资源限制。因此，需要一种更高效、自动化的解决方案。", "method": "该方法结合了卷积神经网络（CNN）和人工神经网络（ANN）。CNN用于将建筑物分类为住宅或非住宅，而ANN则用于估算人口。研究使用了高分辨率（0.3米）卫星图像、0.5米分辨率的数字高程模型（DEM）和矢量边界数据。在甘地讷格尔城市区域利用了约4.8万个建筑足迹进行训练和评估。", "result": "实验结果显示，该模型在建筑分类任务上取得了令人印象深刻的0.9936的F1-分数。通过该系统，估算出甘地讷格尔的人口为278,954人。该方法通过整合实时数据更新、标准化指标和基础设施规划能力，有效克服了传统普查方法的局限性。", "conclusion": "该框架为市政当局提供了一个可扩展、可复制的工具，用于快速城市化地区优化资源管理，展示了AI驱动的地理空间分析在增强数据驱动的城市治理方面的效率和潜力。"}}
{"id": "2509.13047", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50 68T50", "I.2.7; I.2.6"], "pdf": "https://arxiv.org/pdf/2509.13047", "abs": "https://arxiv.org/abs/2509.13047", "authors": ["Nolan Platt", "Pragyansmita Nayak"], "title": "Multi-Model Synthetic Training for Mission-Critical Small Language Models", "comment": "8 pages. Accepted as a full paper to the 3rd International Conference\n  on Foundation and Large Language Models (IEEE FLLM) 2025", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nmany domains, yet their application to specialized fields remains constrained\nby the scarcity and complexity of domain-specific training data. We present a\nnovel approach that achieves a 261x cost reduction for maritime intelligence by\nusing LLMs as one-time teachers rather than using them directly for inference.\nOur method transforms 3.2 billion Automatic Identification System (AIS) vessel\ntracking records into 21,543 synthetic question and answer pairs through\nmulti-model generation (GPT-4o and o3-mini), preventing overfitting and\nensuring accurate reasoning. The resulting fine-tuned Qwen2.5-7B model achieves\n75% accuracy on maritime tasks, while being substantially cheaper than using a\nlarger model for inference. We show that smaller, cheaper models -- when fine\ntuned properly -- can provide similar accuracy compared to larger models that\nare prohibitively expensive. Our work contributes to the growing field of\nsynthetic dataset generation for specialized AI applications and presents a\nhighly reproducible framework for domains where manual annotation is\ninfeasible. Beyond expanding research in the growing field of specialized small\nlanguage models, our approach has immediate applications in maritime safety,\nsecurity operations, and vessel traffic management systems in various\nindustries.", "AI": {"tldr": "该研究提出一种新方法，通过使用大型语言模型（LLMs）作为一次性教师生成合成数据，将32亿条船舶追踪记录转化为2万多条问答对，并用这些数据微调小型模型（Qwen2.5-7B），在海事智能领域实现了261倍的成本降低和75%的准确率。", "motivation": "大型语言模型（LLMs）在许多领域表现出色，但在专业领域的应用受限于领域特定训练数据的稀缺性和复杂性。此外，直接使用大型LLMs进行推理成本高昂。", "method": "该方法将LLMs（GPT-4o和o3-mini）用作一次性教师，通过多模型生成将32亿条自动识别系统（AIS）船舶追踪记录转换为21,543个合成问答对，以防止过拟合并确保推理准确性。随后，使用这些合成数据微调了Qwen2.5-7B模型。", "result": "该方法实现了261倍的海事智能成本降低。微调后的Qwen2.5-7B模型在海事任务上达到了75%的准确率，且比直接使用大型模型进行推理成本大幅降低。研究表明，经过适当微调的小型廉价模型可以提供与昂贵大型模型相似的准确性。", "conclusion": "该工作证明了通过合成数据集生成，可以使小型、廉价模型在专业领域达到与大型、昂贵模型相媲美的性能。这为专业AI应用中的合成数据集生成提供了贡献，并为手动标注不可行的领域提供了一个高度可复现的框架，在海事安全、安保运营和船舶交通管理系统等领域具有直接应用价值。"}}
{"id": "2509.12890", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.12890", "abs": "https://arxiv.org/abs/2509.12890", "authors": ["Malte Probst", "Raphael Wenzel", "Monica Dasi"], "title": "Responsibility and Engagement -- Evaluating Interactions in Social Robot Navigation", "comment": "under review for 2026 IEEE International Conference on Robotics &\n  Automation (ICRA)", "summary": "In Social Robot Navigation (SRN), the availability of meaningful metrics is\ncrucial for evaluating trajectories from human-robot interactions. In the SRN\ncontext, such interactions often relate to resolving conflicts between two or\nmore agents. Correspondingly, the shares to which agents contribute to the\nresolution of such conflicts are important. This paper builds on recent work,\nwhich proposed a Responsibility metric capturing such shares. We extend this\nframework in two directions: First, we model the conflict buildup phase by\nintroducing a time normalization. Second, we propose the related Engagement\nmetric, which captures how the agents' actions intensify a conflict. In a\ncomprehensive series of simulated scenarios with dyadic, group and crowd\ninteractions, we show that the metrics carry meaningful information about the\ncooperative resolution of conflicts in interactions. They can be used to assess\nbehavior quality and foresightedness. We extensively discuss applicability,\ndesign choices and limitations of the proposed metrics.", "AI": {"tldr": "本文扩展了现有的责任度量框架，引入了时间归一化以模拟冲突累积阶段，并提出了一个新的参与度量，用于评估社会机器人导航（SRN）中代理之间冲突解决的质量和前瞻性。", "motivation": "在社会机器人导航（SRN）中，评估人机交互轨迹（尤其是在解决代理间冲突时）需要有意义的度量标准。了解代理在解决冲突中的贡献份额至关重要。", "method": "本文在现有责任度量（捕获代理贡献份额）的基础上进行了两方面扩展：1. 引入时间归一化来建模冲突累积阶段。2. 提出了相关的参与度量，以捕获代理行为如何加剧冲突。通过一系列双人、群体和人群交互的模拟场景对这些度量进行了测试。", "result": "研究表明，所提出的责任度和参与度量能够提供关于交互中合作解决冲突的有意义信息。它们可以用于评估行为质量和前瞻性。", "conclusion": "所提出的时间归一化责任度量和新的参与度量是评估社会机器人导航中冲突解决质量的有效工具，并可用于评估行为质量和前瞻性，对未来的机器人行为设计具有指导意义。"}}
{"id": "2509.12595", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12595", "abs": "https://arxiv.org/abs/2509.12595", "authors": ["Yizhen Lao", "Yu Zhang", "Ziting Wang", "Chengbo Wang", "Yifei Xue", "Wanpeng Shao"], "title": "DisorientLiDAR: Physical Attacks on LiDAR-based Localization", "comment": null, "summary": "Deep learning models have been shown to be susceptible to adversarial attacks\nwith visually imperceptible perturbations. Even this poses a serious security\nchallenge for the localization of self-driving cars, there has been very little\nexploration of attack on it, as most of adversarial attacks have been applied\nto 3D perception. In this work, we propose a novel adversarial attack framework\ncalled DisorientLiDAR targeting LiDAR-based localization. By\nreverse-engineering localization models (e.g., feature extraction networks),\nadversaries can identify critical keypoints and strategically remove them,\nthereby disrupting LiDAR-based localization. Our proposal is first evaluated on\nthree state-of-the-art point-cloud registration models (HRegNet, D3Feat, and\nGeoTransformer) using the KITTI dataset. Experimental results demonstrate that\nremoving regions containing Top-K keypoints significantly degrades their\nregistration accuracy. We further validate the attack's impact on the Autoware\nautonomous driving platform, where hiding merely a few critical regions induces\nnoticeable localization drift. Finally, we extended our attacks to the physical\nworld by hiding critical regions with near-infrared absorptive materials,\nthereby successfully replicate the attack effects observed in KITTI data. This\nstep has been closer toward the realistic physical-world attack that\ndemonstrate the veracity and generality of our proposal.", "AI": {"tldr": "本文提出了一种名为 DisorientLiDAR 的新型对抗性攻击框架，通过移除关键点来扰乱基于 LiDAR 的自动驾驶定位系统，并在数字和物理世界中验证了其有效性。", "motivation": "深度学习模型易受对抗性攻击，但针对自动驾驶汽车 LiDAR 定位的攻击研究非常少，大多数攻击集中在 3D 感知。LiDAR 定位对自动驾驶至关重要，因此探索其安全漏洞具有现实意义。", "method": "研究人员通过逆向工程定位模型（如特征提取网络）来识别关键关键点，并策略性地移除这些关键点，从而破坏基于 LiDAR 的定位。该框架首先在点云配准模型上进行评估，然后扩展到 Autoware 自动驾驶平台。最后，通过使用近红外吸收材料隐藏关键区域，在物理世界中复制了攻击效果。", "result": "实验结果表明，移除 Top-K 关键点区域会显著降低 HRegNet、D3Feat 和 GeoTransformer 等最先进点云配准模型的配准精度。在 Autoware 平台上，隐藏少量关键区域会导致明显的定位漂移。此外，通过物理世界攻击成功复现了在 KITTI 数据上观察到的攻击效果。", "conclusion": "DisorientLiDAR 框架能够有效且普遍地攻击基于 LiDAR 的定位系统，通过移除关键区域即可导致定位精度显著下降和漂移。该研究证实了此类攻击在物理世界中的可行性，对自动驾驶的安全性提出了新的挑战。"}}
{"id": "2509.12927", "categories": ["cs.AI", "cs.CV", "cs.GT", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.12927", "abs": "https://arxiv.org/abs/2509.12927", "authors": ["Xingxing Hong", "Yungong Wang", "Dexin Jin", "Ye Yuan", "Ximing Huang", "Zijian Wu", "Wenxin Li"], "title": "HLSMAC: A New StarCraft Multi-Agent Challenge for High-Level Strategic Decision-Making", "comment": "30 pages, 13 figures with appendix", "summary": "Benchmarks are crucial for assessing multi-agent reinforcement learning\n(MARL) algorithms. While StarCraft II-related environments have driven\nsignificant advances in MARL, existing benchmarks like SMAC focus primarily on\nmicromanagement, limiting comprehensive evaluation of high-level strategic\nintelligence. To address this, we introduce HLSMAC, a new cooperative MARL\nbenchmark with 12 carefully designed StarCraft II scenarios based on classical\nstratagems from the Thirty-Six Stratagems. Each scenario corresponds to a\nspecific stratagem and is designed to challenge agents with diverse strategic\nelements, including tactical maneuvering, timing coordination, and deception,\nthereby opening up avenues for evaluating high-level strategic decision-making\ncapabilities. We also propose novel metrics across multiple dimensions beyond\nconventional win rate, such as ability utilization and advancement efficiency,\nto assess agents' overall performance within the HLSMAC environment. We\nintegrate state-of-the-art MARL algorithms and LLM-based agents with our\nbenchmark and conduct comprehensive experiments. The results demonstrate that\nHLSMAC serves as a robust testbed for advancing multi-agent strategic\ndecision-making.", "AI": {"tldr": "本文提出了HLSMAC，一个基于《三十六计》的星际争霸II合作多智能体强化学习（MARL）基准，旨在评估高层战略决策能力，并引入了新的评估指标。", "motivation": "现有的MARL基准（如SMAC）主要关注微观操作，限制了对高层战略智能的全面评估。", "method": "引入HLSMAC，一个包含12个基于《三十六计》精心设计的星际争霸II场景的合作MARL基准。每个场景都旨在挑战智能体的战术机动、时机协调和欺骗等多样化战略元素。提出了超越传统胜率的新指标（如能力利用率和推进效率）。将最先进的MARL算法和基于LLM的智能体整合到基准中进行综合实验。", "result": "实验结果表明，HLSMAC是一个强大的测试平台，能够推动多智能体战略决策能力的发展。", "conclusion": "HLSMAC有效填补了现有MARL基准在评估高层战略智能方面的空白，为未来的多智能体战略决策研究提供了一个全面的平台。"}}
{"id": "2509.13081", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13081", "abs": "https://arxiv.org/abs/2509.13081", "authors": ["Francesco Pappone", "Ruggero Marino Lazzaroni", "Federico Califano", "Niccolò Gentile", "Roberto Marras"], "title": "Shaping Explanations: Semantic Reward Modeling with Encoder-Only Transformers for GRPO", "comment": null, "summary": "While Large Language Models (LLMs) excel at generating human-like text,\naligning their outputs with complex, qualitative goals like pedagogical\nsoundness remains a significant challenge. Standard reinforcement learning\ntechniques often rely on slow and expensive LLM-as-a-judge evaluations or on\nbrittle, keyword-based metrics like ROUGE, which fail to capture the semantic\nessence of a high-quality explanation. In this work, we introduce a novel\napproach to reward shaping within the Group Relative Policy Optimisation (GRPO)\nframework. Our central contribution is the use of a small, efficient\nencoder-only transformer as a semantic reward model. This model provides a\ndense, semantically rich reward signal based on the cosine similarity between a\ngenerated explanation and a ground-truth reference, guiding the policy towards\nexplanations that are not just factually correct but also structurally and\nconceptually aligned with expert reasoning. We apply this method to the task of\ntraining a model for the Italian medical-school entrance examinations,\nfollowing standard domain-adaptive continued pre-training (CPT) and supervised\nfine-tuning (SFT). Our results demonstrate that GRPO with our proposed semantic\nreward significantly improves explanation faithfulness and clarity over a\nstrong SFT baseline, showcasing the power of using lightweight encoder models\nfor nuanced reward shaping in complex generation tasks", "AI": {"tldr": "本文提出在GRPO框架内使用小型编码器Transformer作为语义奖励模型，通过余弦相似度提供密集、语义丰富的奖励信号，显著提升了大型语言模型生成解释的忠实性和清晰度，尤其在意大利医学院入学考试解释任务中表现优异。", "motivation": "大型语言模型在生成类人文本方面表现出色，但难以实现教学合理性等复杂的定性目标。标准的强化学习技术依赖于缓慢昂贵的“LLM作为评判者”评估或脆弱的基于关键词的指标（如ROUGE），这些都无法捕捉高质量解释的语义本质。", "method": "本研究在Group Relative Policy Optimisation (GRPO) 框架内引入了一种新颖的奖励塑造方法。核心贡献是使用一个小型、高效的编码器Transformer作为语义奖励模型。该模型通过计算生成解释与真实参考之间的余弦相似度，提供密集、语义丰富的奖励信号，引导策略生成在结构和概念上与专家推理一致的解释。该方法应用于意大利医学院入学考试的解释生成任务，遵循标准的领域自适应持续预训练(CPT)和监督微调(SFT)步骤。", "result": "实验结果表明，结合所提出的语义奖励的GRPO方法，相对于强大的SFT基线，显著提高了解释的忠实性和清晰度。", "conclusion": "研究展示了使用轻量级编码器模型在复杂生成任务中进行细致奖励塑造的强大潜力。"}}
{"id": "2509.12912", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.12912", "abs": "https://arxiv.org/abs/2509.12912", "authors": ["Raphael Wenzel", "Malte Probst"], "title": "Spotting the Unfriendly Robot -- Towards better Metrics for Interactions", "comment": "Presented at 2025 IEEE Conference on Robotics and Automation (ICRA)\n  Workshop: Advances in Social Navigation: Planning, HRI and Beyond", "summary": "Establishing standardized metrics for Social Robot Navigation (SRN)\nalgorithms for assessing the quality and social compliance of robot behavior\naround humans is essential for SRN research. Currently, commonly used\nevaluation metrics lack the ability to quantify how cooperative an agent\nbehaves in interaction with humans. Concretely, in a simple frontal approach\nscenario, no metric specifically captures if both agents cooperate or if one\nagent stays on collision course and the other agent is forced to evade. To\naddress this limitation, we propose two new metrics, a conflict intensity\nmetric and the responsibility metric. Together, these metrics are capable of\nevaluating the quality of human-robot interactions by showing how much a given\nalgorithm has contributed to reducing a conflict and which agent actually took\nresponsibility of the resolution. This work aims to contribute to the\ndevelopment of a comprehensive and standardized evaluation methodology for SRN,\nultimately enhancing the safety, efficiency, and social acceptance of robots in\nhuman-centric environments.", "AI": {"tldr": "为社交机器人导航(SRN)算法评估提出新的标准化指标，以量化人类-机器人互动中的合作程度和冲突解决责任，弥补现有指标的不足。", "motivation": "目前常用的SRN评估指标无法量化智能体在与人类互动中的合作行为，特别是在冲突场景中，无法区分是合作解决还是被迫规避，也无法明确哪个智能体承担了解决冲突的责任。", "method": "提出了两个新的指标：冲突强度指标（conflict intensity metric）和责任指标（responsibility metric）。", "result": "这些指标能够评估人机交互的质量，通过展示给定算法在减少冲突中的贡献以及哪个智能体实际承担了解决冲突的责任。", "conclusion": "本研究旨在为SRN开发一套全面且标准化的评估方法，最终提升机器人在以人为中心环境中的安全性、效率和社会接受度。"}}
{"id": "2509.12627", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12627", "abs": "https://arxiv.org/abs/2509.12627", "authors": ["Pengbo Guo", "Chengxu Liu", "Guoshuai Zhao", "Xingsong Hou", "Jialie Shen", "Xueming Qian"], "title": "Exploring Spectral Characteristics for Single Image Reflection Removal", "comment": null, "summary": "Eliminating reflections caused by incident light interacting with reflective\nmedium remains an ill-posed problem in the image restoration area. The primary\nchallenge arises from the overlapping of reflection and transmission components\nin the captured images, which complicates the task of accurately distinguishing\nand recovering the clean background. Existing approaches typically address\nreflection removal solely in the image domain, ignoring the spectral property\nvariations of reflected light, which hinders their ability to effectively\ndiscern reflections. In this paper, we start with a new perspective on spectral\nlearning, and propose the Spectral Codebook to reconstruct the optical spectrum\nof the reflection image. The reflections can be effectively distinguished by\nperceiving the wavelength differences between different light sources in the\nspectrum. To leverage the reconstructed spectrum, we design two spectral prior\nrefinement modules to re-distribute pixels in the spatial dimension and\nadaptively enhance the spectral differences along the wavelength dimension.\nFurthermore, we present the Spectrum-Aware Transformer to jointly recover the\ntransmitted content in spectral and pixel domains. Experimental results on\nthree different reflection benchmarks demonstrate the superiority and\ngeneralization ability of our method compared to state-of-the-art models.", "AI": {"tldr": "本文提出了一种基于光谱学习的新方法，通过构建光谱码本重建反射图像的光谱，并结合光谱先验细化模块和光谱感知Transformer，有效去除图像反射。", "motivation": "图像反射消除是一个难题，因为反射和透射分量在捕获图像中重叠，难以区分和恢复干净背景。现有方法通常只在图像域处理，忽略反射光的频谱特性变化，导致无法有效识别反射。", "method": "该方法从光谱学习的新视角出发，提出光谱码本（Spectral Codebook）来重建反射图像的光学光谱，通过感知不同光源在光谱中的波长差异来区分反射。此外，设计了两个光谱先验细化模块，用于在空间维度重新分配像素并在波长维度自适应增强光谱差异。最后，引入光谱感知Transformer（Spectrum-Aware Transformer）来联合恢复光谱域和像素域的透射内容。", "result": "在三个不同的反射基准测试上，实验结果表明所提出的方法比现有最先进模型具有优越性和泛化能力。", "conclusion": "通过利用光谱特性并结合光谱学习与Transformer架构，该方法能有效区分和去除图像反射，提供了一种性能卓越的解决方案。"}}
{"id": "2509.12934", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12934", "abs": "https://arxiv.org/abs/2509.12934", "authors": ["Jeremias Ferrao", "Matthijs van der Lende", "Ilija Lichkovski", "Clement Neo"], "title": "The Anatomy of Alignment: Decomposing Preference Optimization by Steering Sparse Features", "comment": "Work in Progress", "summary": "Aligning large language models is critical for their usability and safety.\nHowever, the prevailing approach of Reinforcement Learning from Human Feedback\n(RLHF) induces diffuse, opaque parameter changes, making it difficult to\ndiscern what the model has internalized. Hence, we introduce Feature Steering\nwith Reinforcement Learning (FSRL), a transparent alignment framework that\ntrains a lightweight adapter to steer behavior by modulating interpretable\nfeatures from a Sparse Autoencoder (SAE). First, we demonstrate that FSRL is an\neffective method for preference optimization and is comparable with current\nRLHF methods. We then perform mechanistic analysis on the trained adapter, and\nfind that its policy systematically promotes style features over explicit\nalignment concepts, suggesting that the preference optimization process rewards\nstylistic presentation as a proxy for quality. Ultimately, we hope that FSRL\nprovides a tool for both interpretable model control and diagnosing the\ninternal mechanisms of alignment.", "AI": {"tldr": "本文提出FSRL（基于强化学习的特征引导），一种透明的对齐框架，通过轻量级适配器调节稀疏自编码器（SAE）的解释性特征来引导大型语言模型行为，效果与RLHF相当，并提供机制分析。", "motivation": "现有RLHF方法导致模型参数变化不透明，难以理解模型内部学习了什么。因此，需要一种更透明的对齐框架来理解和控制模型行为。", "method": "FSRL框架训练一个轻量级适配器，通过调节来自稀疏自编码器（SAE）的解释性特征来引导模型行为。该方法使用强化学习进行偏好优化。", "result": "FSRL在偏好优化方面表现有效，与当前的RLHF方法相当。对训练后的适配器进行机制分析发现，其策略系统性地优先选择风格特征而非明确的对齐概念，这表明偏好优化过程将风格化表达作为质量的代理指标。", "conclusion": "FSRL提供了一种工具，既可以实现可解释的模型控制，又可以诊断对齐过程的内部机制。"}}
{"id": "2509.13127", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13127", "abs": "https://arxiv.org/abs/2509.13127", "authors": ["Sijia Cui", "Shuai Xu", "Aiyao He", "Yanna Wang", "Bo Xu"], "title": "Empowering LLMs with Parameterized Skills for Adversarial Long-Horizon Planning", "comment": "Accepted to IJCNN 2025", "summary": "Recent advancements in Large Language Models(LLMs) have led to the\ndevelopment of LLM-based AI agents. A key challenge is the creation of agents\nthat can effectively ground themselves in complex, adversarial long-horizon\nenvironments. Existing methods mainly focus on (1) using LLMs as policies to\ninteract with the environment through generating low-level feasible actions,\nand (2) utilizing LLMs to generate high-level tasks or language guides to\nstimulate action generation. However, the former struggles to generate reliable\nactions, while the latter relies heavily on expert experience to translate\nhigh-level tasks into specific action sequences. To address these challenges,\nwe introduce the Plan with Language, Act with Parameter (PLAP) planning\nframework that facilitates the grounding of LLM-based agents in long-horizon\nenvironments. The PLAP method comprises three key components: (1) a skill\nlibrary containing environment-specific parameterized skills, (2) a skill\nplanner powered by LLMs, and (3) a skill executor converting the parameterized\nskills into executable action sequences. We implement PLAP in MicroRTS, a\nlong-horizon real-time strategy game that provides an unfamiliar and\nchallenging environment for LLMs. The experimental results demonstrate the\neffectiveness of PLAP. In particular, GPT-4o-driven PLAP in a zero-shot setting\noutperforms 80% of baseline agents, and Qwen2-72B-driven PLAP, with carefully\ncrafted few-shot examples, surpasses the top-tier scripted agent, CoacAI.\nAdditionally, we design comprehensive evaluation metrics and test 6\nclosed-source and 2 open-source LLMs within the PLAP framework, ultimately\nreleasing an LLM leaderboard ranking long-horizon skill planning ability. Our\ncode is available at https://github.com/AI-Research-TeamX/PLAP.", "AI": {"tldr": "本文提出PLAP框架，通过结合语言规划和参数化动作，使LLM代理在复杂、长周期环境中有效落地。PLAP包含技能库、LLM技能规划器和技能执行器，在MicroRTS中表现出色，甚至超越顶尖脚本代理，并发布了LLM长周期技能规划能力排行榜。", "motivation": "现有LLM代理在复杂、对抗性长周期环境中落地面临挑战：直接生成低级动作不可靠；利用LLM生成高级任务或语言指导又过度依赖专家经验将高级任务转化为具体动作序列。", "method": "PLAP（Plan with Language, Act with Parameter）规划框架包含三个核心组件：1) 一个包含环境特定参数化技能的技能库；2) 一个由LLM驱动的技能规划器；3) 一个将参数化技能转换为可执行动作序列的技能执行器。", "result": "实验结果表明PLAP框架的有效性。在零样本设置下，由GPT-4o驱动的PLAP超越了80%的基线代理；通过精心设计的少样本示例，由Qwen2-72B驱动的PLAP甚至超越了顶尖脚本代理CoacAI。此外，论文还设计了全面的评估指标，测试了6个闭源和2个开源LLM，并发布了LLM长周期技能规划能力排行榜。", "conclusion": "PLAP框架有效解决了LLM代理在复杂长周期环境中落地的问题，通过结合LLM的规划能力与参数化技能的精确执行，显著提升了代理的表现。研究还为评估LLM的长周期技能规划能力提供了新的基准和工具。"}}
{"id": "2509.12928", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.12928", "abs": "https://arxiv.org/abs/2509.12928", "authors": ["Peiwen Yang", "Mingquan Jiang", "Xinyue Shen", "Heping Zhang"], "title": "Spatiotemporal Calibration for Laser Vision Sensor in Hand-eye System Based on Straight-line Constraint", "comment": "Submitted to IEEE RAL", "summary": "Laser vision sensors (LVS) are critical perception modules for industrial\nrobots, facilitating real-time acquisition of workpiece geometric data in\nwelding applications. However, the camera communication delay will lead to a\ntemporal desynchronization between captured images and the robot motions.\nAdditionally, hand-eye extrinsic parameters may vary during prolonged\nmeasurement. To address these issues, we introduce a measurement model of LVS\nconsidering the effect of the camera's time-offset and propose a teaching-free\nspatiotemporal calibration method utilizing line constraints. This method\ninvolves a robot equipped with an LVS repeatedly scanning straight-line fillet\nwelds using S-shaped trajectories. Regardless of the robot's orientation\nchanges, all measured welding positions are constrained to a straight-line,\nrepresented by Plucker coordinates. Moreover, a nonlinear optimization model\nbased on straight-line constraints is established. Subsequently, the\nLevenberg-Marquardt algorithm (LMA) is employed to optimize parameters,\nincluding time-offset, hand-eye extrinsic parameters, and straight-line\nparameters. The feasibility and accuracy of the proposed approach are\nquantitatively validated through experiments on curved weld scanning. We\nopen-sourced the code, dataset, and simulation report at\nhttps://anonymous.4open.science/r/LVS_ST_CALIB-015F/README.md.", "AI": {"tldr": "本文提出了一种免示教的时空标定方法，用于解决工业机器人激光视觉传感器（LVS）中存在的相机时间偏移和手眼外参变化问题，通过直线约束和非线性优化实现。", "motivation": "工业机器人激光视觉传感器在焊接应用中，相机通信延迟导致图像与机器人运动不同步，且长时间测量可能导致手眼外参变化，影响数据采集精度。", "method": "引入考虑相机时间偏移的LVS测量模型；提出基于直线约束的免示教时空标定方法；机器人携带LVS以S形轨迹重复扫描直线角焊缝，将所有测量点约束为直线（用Plucker坐标表示）；建立基于直线约束的非线性优化模型；使用Levenberg-Marquardt算法优化时间偏移、手眼外参和直线参数。", "result": "通过对曲线焊缝扫描的实验，定量验证了所提方法的 E 可行性和准确性。", "conclusion": "该方法成功解决了工业机器人LVS的时空同步和手眼参数漂移问题，提供了一种准确且无需示教的标定方案，提升了LVS在工业应用中的精度和鲁棒性。"}}
{"id": "2509.12632", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12632", "abs": "https://arxiv.org/abs/2509.12632", "authors": ["Pengxin Chen", "Zhipeng Luo", "Xiaoqi Jiang", "Zhangcai Yin", "Jonathan Li"], "title": "Maps for Autonomous Driving: Full-process Survey and Frontiers", "comment": null, "summary": "Maps have always been an essential component of autonomous driving. With the\nadvancement of autonomous driving technology, both the representation and\nproduction process of maps have evolved substantially. The article categorizes\nthe evolution of maps into three stages: High-Definition (HD) maps, Lightweight\n(Lite) maps, and Implicit maps. For each stage, we provide a comprehensive\nreview of the map production workflow, with highlighting technical challenges\ninvolved and summarizing relevant solutions proposed by the academic community.\nFurthermore, we discuss cutting-edge research advances in map representations\nand explore how these innovations can be integrated into end-to-end autonomous\ndriving frameworks.", "AI": {"tldr": "本文回顾了自动驾驶地图的演变，将其分为高清（HD）地图、轻量级（Lite）地图和隐式地图三个阶段，并分析了各阶段的生产流程、技术挑战、解决方案以及前沿研究，探讨了其与端到端自动驾驶框架的整合。", "motivation": "地图一直是自动驾驶的核心组成部分，随着技术进步，地图的表示和生产过程发生了显著演变。本研究旨在对这种演变进行分类和全面回顾。", "method": "文章将地图演变分为三个阶段：高清（HD）地图、轻量级（Lite）地图和隐式地图。针对每个阶段，文章全面回顾了地图生产流程，强调了技术挑战，并总结了学术界提出的相关解决方案。此外，还讨论了地图表示的前沿研究进展，并探讨了如何将这些创新整合到端到端自动驾驶框架中。", "result": "本文提供了自动驾驶地图从高清地图到隐式地图的演变综述，详细阐述了每个阶段的地图生产工作流程、面临的技术挑战及学术解决方案。同时，探讨了地图表示的前沿研究进展，并讨论了这些创新如何融入端到端自动驾驶系统。", "conclusion": "自动驾驶地图的表示和生产已从高清地图发展到轻量级地图和隐式地图。未来的研究将聚焦于解决当前的技术挑战，并探索如何将先进的地图表示与端到端自动驾驶框架更有效地整合。"}}
{"id": "2509.12951", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12951", "abs": "https://arxiv.org/abs/2509.12951", "authors": ["Shilian Chen", "Jie Zhou", "Tianyu Huai", "Yujiang Lu", "Junsong Li", "Bihao Zhan", "Qianjun Pan", "Yutao Yang", "Xin Li", "Qin Chen", "Hang Yan", "Liang He"], "title": "Black-box Model Merging for Language-Model-as-a-Service with Massive Model Repositories", "comment": null, "summary": "Model merging refers to the process of integrating multiple distinct models\ninto a unified model that preserves and combines the strengths and capabilities\nof the individual models. Most existing approaches rely on task vectors to\ncombine models, typically under the assumption that model parameters are\naccessible. However, for extremely large language models (LLMs) such as GPT-4,\nwhich are often provided solely as black-box services through API interfaces\n(Language-Model-as-a-Service), model weights are not available to end users.\nThis presents a significant challenge, which we refer to as black-box model\nmerging (BMM) with massive LLMs. To address this challenge, we propose a\nderivative-free optimization framework based on the evolutionary algorithm\n(Evo-Merging) that enables effective model merging using only inference-time\nAPI queries. Our method consists of two key components: (1) sparsity-based\ndenoising, designed to identify and filter out irrelevant or redundant\ninformation across models, and (2) sign-aware scaling, which dynamically\ncomputes optimal combination weights for the relevant models based on their\nperformance. We also provide a formal justification, along with a theoretical\nanalysis, for our asymmetric sparsification. Extensive experimental evaluations\ndemonstrate that our approach achieves state-of-the-art results on a range of\ntasks, significantly outperforming existing strong baselines.", "AI": {"tldr": "针对大型语言模型（LLMs）的黑盒模型合并问题，本文提出了一种基于进化算法的无导数优化框架Evo-Merging，仅通过API查询即可有效合并模型。", "motivation": "大多数现有模型合并方法依赖于可访问的模型参数。然而，对于GPT-4等大型LLMs，通常以黑盒API服务形式提供，用户无法获取模型权重。这给模型合并带来了巨大挑战，即黑盒模型合并（BMM）。", "method": "提出Evo-Merging框架，采用基于进化算法的无导数优化方法，仅利用推理时的API查询。该方法包含两个核心组件：1) 基于稀疏性的去噪，用于识别和过滤模型间不相关或冗余的信息；2) 符号感知缩放，根据相关模型的性能动态计算最优组合权重。同时提供了非对称稀疏化的形式化论证和理论分析。", "result": "广泛的实验评估表明，该方法在多种任务上取得了最先进的结果，显著优于现有强大的基线方法。", "conclusion": "Evo-Merging框架通过无导数优化和API查询，成功解决了大规模LLMs的黑盒模型合并难题，并在性能上展现出卓越的效果。"}}
{"id": "2509.13154", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13154", "abs": "https://arxiv.org/abs/2509.13154", "authors": ["Jinxin Li", "Gang Tu", "ShengYu Cheng", "Junjie Hu", "Jinting Wang", "Rui Chen", "Zhilong Zhou", "Dongbo Shan"], "title": "LLM Hallucination Detection: A Fast Fourier Transform Method Based on Hidden Layer Temporal Signals", "comment": null, "summary": "Hallucination remains a critical barrier for deploying large language models\n(LLMs) in reliability-sensitive applications. Existing detection methods\nlargely fall into two categories: factuality checking, which is fundamentally\nconstrained by external knowledge coverage, and static hidden-state analysis,\nthat fails to capture deviations in reasoning dynamics. As a result, their\neffectiveness and robustness remain limited. We propose HSAD (Hidden Signal\nAnalysis-based Detection), a novel hallucination detection framework that\nmodels the temporal dynamics of hidden representations during autoregressive\ngeneration. HSAD constructs hidden-layer signals by sampling activations across\nlayers, applies Fast Fourier Transform (FFT) to obtain frequency-domain\nrepresentations, and extracts the strongest non-DC frequency component as\nspectral features. Furthermore, by leveraging the autoregressive nature of\nLLMs, HSAD identifies optimal observation points for effective and reliable\ndetection. Across multiple benchmarks, including TruthfulQA, HSAD achieves over\n10 percentage points improvement compared to prior state-of-the-art methods. By\nintegrating reasoning-process modeling with frequency-domain analysis, HSAD\nestablishes a new paradigm for robust hallucination detection in LLMs.", "AI": {"tldr": "本文提出了一种名为HSAD的新型幻觉检测框架，通过分析大型语言模型自回归生成过程中隐藏层表示的时间动态和频域特征，显著提升了幻觉检测的准确性。", "motivation": "幻觉是限制大型语言模型在可靠性敏感应用中部署的关键障碍。现有检测方法受限于外部知识覆盖范围（事实核查）或未能捕捉推理动态偏差（静态隐藏状态分析），导致其有效性和鲁棒性有限。", "method": "HSAD通过以下步骤实现：1) 对跨层激活进行采样以构建隐藏层信号；2) 应用快速傅里叶变换（FFT）获取频域表示；3) 提取最强的非直流（non-DC）频率分量作为光谱特征；4) 利用LLM的自回归特性识别最佳观察点，以实现有效可靠的检测。", "result": "在包括TruthfulQA在内的多个基准测试中，HSAD比现有最先进的方法提高了10个百分点以上的性能。", "conclusion": "HSAD通过将推理过程建模与频域分析相结合，为大型语言模型中鲁棒的幻觉检测建立了一个新范式。"}}
{"id": "2509.12969", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.12969", "abs": "https://arxiv.org/abs/2509.12969", "authors": ["Jae-Hyun Lee", "Jonghoo Park", "Kyu-Jin Cho"], "title": "Tendon-Based Proprioception in an Anthropomorphic Underactuated Robotic Hand with Series Elastic Actuators", "comment": "8 pages, 10 figures, Supplementary video, Submitted to IEEE Robotics\n  and Automation Letters (RA-L)", "summary": "Anthropomorphic underactuated hands are widely employed for their versatility\nand structural simplicity. In such systems, compact sensing integration and\nproper interpretation aligned with underactuation are crucial for realizing\npractical grasp functionalities. This study proposes an anthropomorphic\nunderactuated hand that achieves comprehensive situational awareness of\nhand-object interaction, utilizing tendon-based proprioception provided by\nseries elastic actuators (SEAs). We developed a compact SEA with high accuracy\nand reliability that can be seamlessly integrated into sensorless fingers. By\ncoupling proprioceptive sensing with potential energy-based modeling, the\nsystem estimates key grasp-related variables, including contact timing, joint\nangles, relative object stiffness, and finger configuration changes indicating\nexternal disturbances. These estimated variables enable grasp posture\nreconstruction, safe handling of deformable objects, and blind grasping with\nproprioceptive-only recognition of objects with varying geometry and stiffness.\nFinger-level experiments and hand-level demonstrations confirmed the\neffectiveness of the proposed approach. The results demonstrate that\ntendon-based proprioception serves as a compact and robust sensing modality for\npractical manipulation without reliance on vision or tactile feedback.", "AI": {"tldr": "本研究提出了一种仿人欠驱动手，利用串联弹性驱动器（SEA）提供的肌腱本体感受，实现对人手-物体交互的全面态势感知，无需视觉或触觉反馈。", "motivation": "仿人欠驱动手因其多功能性和结构简单性而被广泛采用，但在实际抓取功能中，紧凑的传感集成和与欠驱动系统相符的正确解释至关重要。", "method": "该研究开发了一种紧凑、高精度、高可靠性的SEA，可无缝集成到无传感器的手指中。通过将本体感受传感与基于势能的建模相结合，系统能够估计关键的抓取相关变量，包括接触时间、关节角度、相对物体刚度以及指示外部干扰的手指配置变化。", "result": "估计的变量能够实现抓取姿态重建、安全处理可变形物体，以及仅依靠本体感受识别不同几何形状和刚度物体的盲抓取。手指级实验和手部级演示证实了该方法的有效性。", "conclusion": "研究结果表明，基于肌腱的本体感受是一种紧凑而强大的传感模式，可用于实际操作，而无需依赖视觉或触觉反馈。"}}
{"id": "2509.12633", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12633", "abs": "https://arxiv.org/abs/2509.12633", "authors": ["Liming Lu", "Shuchao Pang", "Xu Zheng", "Xiang Gu", "Anan Du", "Yunhuai Liu", "Yongbin Zhou"], "title": "CIARD: Cyclic Iterative Adversarial Robustness Distillation", "comment": null, "summary": "Adversarial robustness distillation (ARD) aims to transfer both performance\nand robustness from teacher model to lightweight student model, enabling\nresilient performance on resource-constrained scenarios. Though existing ARD\napproaches enhance student model's robustness, the inevitable by-product leads\nto the degraded performance on clean examples. We summarize the causes of this\nproblem inherent in existing methods with dual-teacher framework as: 1. The\ndivergent optimization objectives of dual-teacher models, i.e., the clean and\nrobust teachers, impede effective knowledge transfer to the student model, and\n2. The iteratively generated adversarial examples during training lead to\nperformance deterioration of the robust teacher model. To address these\nchallenges, we propose a novel Cyclic Iterative ARD (CIARD) method with two key\ninnovations: a. A multi-teacher framework with contrastive push-loss alignment\nto resolve conflicts in dual-teacher optimization objectives, and b. Continuous\nadversarial retraining to maintain dynamic teacher robustness against\nperformance degradation from the varying adversarial examples. Extensive\nexperiments on CIFAR-10, CIFAR-100, and Tiny-ImageNet demonstrate that CIARD\nachieves remarkable performance with an average 3.53 improvement in adversarial\ndefense rates across various attack scenarios and a 5.87 increase in clean\nsample accuracy, establishing a new benchmark for balancing model robustness\nand generalization. Our code is available at https://github.com/eminentgu/CIARD", "AI": {"tldr": "现有对抗鲁棒性蒸馏(ARD)方法在提升鲁棒性的同时会降低干净样本性能。本文提出CIARD方法，通过多教师框架和持续对抗重训练，有效平衡了模型的鲁棒性和泛化能力，并取得了显著提升。", "motivation": "现有ARD方法在提高学生模型鲁棒性的同时，会导致在干净样本上的性能下降。作者将此问题归因于双教师框架中优化目标的分歧以及训练过程中迭代生成对抗样本导致的鲁棒教师性能下降。", "method": "本文提出了一种名为循环迭代ARD (CIARD) 的新方法。其核心创新包括：1. 采用多教师框架与对比推拉损失对齐，以解决双教师优化目标冲突；2. 引入持续对抗重训练机制，以动态维护教师模型的鲁棒性，防止性能下降。", "result": "在CIFAR-10、CIFAR-100和Tiny-ImageNet上的广泛实验表明，CIARD方法在各种攻击场景下平均提高了3.53%的对抗防御率，并使干净样本准确率提升了5.87%。", "conclusion": "CIARD方法在平衡模型鲁棒性和泛化能力方面建立了新的基准，实现了卓越的性能，有效解决了现有ARD方法的弊端。"}}
{"id": "2509.12958", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12958", "abs": "https://arxiv.org/abs/2509.12958", "authors": ["Bihao Zhan", "Jie Zhou", "Junsong Li", "Yutao Yang", "Shilian Chen", "Qianjun Pan", "Xin Li", "Wen Wu", "Xingjiao Wu", "Qin Chen", "Hang Yan", "Liang He"], "title": "Forget What's Sensitive, Remember What Matters: Token-Level Differential Privacy in Memory Sculpting for Continual Learning", "comment": null, "summary": "Continual Learning (CL) models, while adept at sequential knowledge\nacquisition, face significant and often overlooked privacy challenges due to\naccumulating diverse information. Traditional privacy methods, like a uniform\nDifferential Privacy (DP) budget, indiscriminately protect all data, leading to\nsubstantial model utility degradation and hindering CL deployment in\nprivacy-sensitive areas. To overcome this, we propose a privacy-enhanced\ncontinual learning (PeCL) framework that forgets what's sensitive and remembers\nwhat matters. Our approach first introduces a token-level dynamic Differential\nPrivacy strategy that adaptively allocates privacy budgets based on the\nsemantic sensitivity of individual tokens. This ensures robust protection for\nprivate entities while minimizing noise injection for non-sensitive, general\nknowledge. Second, we integrate a privacy-guided memory sculpting module. This\nmodule leverages the sensitivity analysis from our dynamic DP mechanism to\nintelligently forget sensitive information from the model's memory and\nparameters, while explicitly preserving the task-invariant historical knowledge\ncrucial for mitigating catastrophic forgetting. Extensive experiments show that\nPeCL achieves a superior balance between privacy preserving and model utility,\noutperforming baseline models by maintaining high accuracy on previous tasks\nwhile ensuring robust privacy.", "AI": {"tldr": "本文提出了一种隐私增强的持续学习（PeCL）框架，通过令牌级动态差分隐私和隐私引导的记忆雕刻，平衡了隐私保护和模型效用，解决了持续学习中隐私敏感信息累积的问题。", "motivation": "持续学习（CL）模型在顺序知识获取中面临显著的隐私挑战，因为它们会积累多样化信息。传统的差分隐私（DP）方法无差别地保护所有数据，导致模型效用大幅下降，阻碍了CL在隐私敏感领域的部署。", "method": "PeCL框架包含两个核心方法：1. 令牌级动态差分隐私策略：根据单个令牌的语义敏感性自适应分配隐私预算，为私有实体提供强保护，同时最小化非敏感通用知识的噪声注入。2. 隐私引导的记忆雕刻模块：利用敏感性分析，智能地从模型记忆和参数中遗忘敏感信息，同时明确保留对缓解灾难性遗忘至关重要的任务不变历史知识。", "result": "PeCL在隐私保护和模型效用之间实现了卓越的平衡，在保持先前任务高准确性的同时确保了强大的隐私性，优于基线模型。", "conclusion": "PeCL框架通过选择性地保护敏感数据并保留关键历史知识，有效解决了持续学习中的隐私挑战，实现了隐私保护和模型效用的双赢。"}}
{"id": "2509.13196", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13196", "abs": "https://arxiv.org/abs/2509.13196", "authors": ["Yongjian Tang", "Doruk Tuncel", "Christian Koerner", "Thomas Runkler"], "title": "The Few-shot Dilemma: Over-prompting Large Language Models", "comment": "accepted for the main track of FLLM", "summary": "Over-prompting, a phenomenon where excessive examples in prompts lead to\ndiminished performance in Large Language Models (LLMs), challenges the\nconventional wisdom about in-context few-shot learning. To investigate this\nfew-shot dilemma, we outline a prompting framework that leverages three\nstandard few-shot selection methods - random sampling, semantic embedding, and\nTF-IDF vectors - and evaluate these methods across multiple LLMs, including\nGPT-4o, GPT-3.5-turbo, DeepSeek-V3, Gemma-3, LLaMA-3.1, LLaMA-3.2, and Mistral.\nOur experimental results reveal that incorporating excessive domain-specific\nexamples into prompts can paradoxically degrade performance in certain LLMs,\nwhich contradicts the prior empirical conclusion that more relevant few-shot\nexamples universally benefit LLMs. Given the trend of LLM-assisted software\nengineering and requirement analysis, we experiment with two real-world\nsoftware requirement classification datasets. By gradually increasing the\nnumber of TF-IDF-selected and stratified few-shot examples, we identify their\noptimal quantity for each LLM. This combined approach achieves superior\nperformance with fewer examples, avoiding the over-prompting problem, thus\nsurpassing the state-of-the-art by 1% in classifying functional and\nnon-functional requirements.", "AI": {"tldr": "研究发现，在大型语言模型（LLMs）中，过多的提示示例（“过度提示”）反而会降低性能。通过TF-IDF和分层选择少量示例，可以避免过度提示问题，并在软件需求分类任务中实现更优性能。", "motivation": "传统观念认为上下文中的少量示例学习（in-context few-shot learning）中，更多示例普遍有益于LLMs。然而，“过度提示”现象——即过多示例导致LLM性能下降——挑战了这一传统认知，促使研究者深入探究这一“少量示例困境”。", "method": "研究构建了一个提示框架，利用三种标准少量示例选择方法：随机抽样、语义嵌入和TF-IDF向量。这些方法在包括GPT-4o、GPT-3.5-turbo、DeepSeek-V3、Gemma-3、LLaMA-3.1、LLaMA-3.2和Mistral在内的多个LLMs上进行了评估。此外，还在两个真实的软件需求分类数据集上，通过逐步增加TF-IDF选择和分层的少量示例数量，以识别每个LLM的最佳数量。", "result": "实验结果表明，在某些LLMs中，加入过多的领域特定示例反而会降低性能，这与之前“越多相关少量示例普遍有益于LLMs”的经验结论相悖。通过结合TF-IDF选择和分层抽样，研究者为每个LLM确定了最佳的示例数量，从而避免了过度提示问题。这种方法使用更少的示例实现了卓越性能，在功能和非功能需求分类方面超越了现有最佳水平1%。", "conclusion": "过度提示是一个真实存在的问题，过多示例可能损害LLM性能。通过精心选择和优化少量示例的数量（例如使用TF-IDF和分层抽样），可以在不触发过度提示的情况下，显著提高LLM在特定任务（如软件需求分类）上的表现，挑战了“越多越好”的传统范式。"}}
{"id": "2509.12982", "categories": ["cs.RO", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.12982", "abs": "https://arxiv.org/abs/2509.12982", "authors": ["Erblin Isaku", "Hassan Sartaj", "Shaukat Ali", "Beatriz Sanguino", "Tongtong Wang", "Guoyuan Li", "Houxiang Zhang", "Thomas Peyrucain"], "title": "Out of Distribution Detection in Self-adaptive Robots with AI-powered Digital Twins", "comment": "15 pages, 4 figures, 3 tables", "summary": "Self-adaptive robots (SARs) in complex, uncertain environments must\nproactively detect and address abnormal behaviors, including\nout-of-distribution (OOD) cases. To this end, digital twins offer a valuable\nsolution for OOD detection. Thus, we present a digital twin-based approach for\nOOD detection (ODiSAR) in SARs. ODiSAR uses a Transformer-based digital twin to\nforecast SAR states and employs reconstruction error and Monte Carlo dropout\nfor uncertainty quantification. By combining reconstruction error with\npredictive variance, the digital twin effectively detects OOD behaviors, even\nin previously unseen conditions. The digital twin also includes an\nexplainability layer that links potential OOD to specific SAR states, offering\ninsights for self-adaptation. We evaluated ODiSAR by creating digital twins of\ntwo industrial robots: one navigating an office environment, and another\nperforming maritime ship navigation. In both cases, ODiSAR forecasts SAR\nbehaviors (i.e., robot trajectories and vessel motion) and proactively detects\nOOD events. Our results showed that ODiSAR achieved high detection performance\n-- up to 98\\% AUROC, 96\\% TNR@TPR95, and 95\\% F1-score -- while providing\ninterpretable insights to support self-adaptation.", "AI": {"tldr": "本文提出了一种基于数字孪生的自适应机器人（SARs）异常行为（OOD）检测方法ODiSAR。该方法利用基于Transformer的数字孪生预测机器人状态，并通过重建误差和蒙特卡洛dropout量化不确定性，实现了对OOD行为的高效检测和可解释性，以支持机器人的自我适应。", "motivation": "在复杂不确定的环境中，自适应机器人（SARs）需要主动检测并处理异常行为，包括分布外（OOD）情况。数字孪生为OOD检测提供了一种有价值的解决方案。", "method": "ODiSAR是一种基于数字孪生的SARs OOD检测方法。它使用基于Transformer的数字孪生来预测SARs状态，并结合重建误差和蒙特卡洛dropout进行不确定性量化。通过将重建误差与预测方差结合，该数字孪生能够有效检测OOD行为。此外，数字孪生还包含一个可解释性层，将潜在的OOD事件与特定的SARs状态关联起来，为自我适应提供洞察。", "result": "ODiSAR在两个工业机器人（一个办公室导航机器人，一个海上船舶导航机器人）的数字孪生评估中表现出色。它成功预测了SARs行为（机器人轨迹和船舶运动）并主动检测了OOD事件。结果显示，ODiSAR实现了高达98%的AUROC、96%的TNR@TPR95和95%的F1-score的高检测性能，同时提供了可解释的洞察力以支持自我适应。", "conclusion": "ODiSAR提供了一种有效且可解释的基于数字孪生的方法，用于自适应机器人（SARs）的OOD检测，即使在以前未见过的条件下也能实现高精度检测，并为机器人的自我适应提供了宝贵的见解。"}}
{"id": "2509.12653", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12653", "abs": "https://arxiv.org/abs/2509.12653", "authors": ["Jinjie Shen", "Yaxiong Wang", "Lechao Cheng", "Nan Pu", "Zhun Zhong"], "title": "Beyond Artificial Misalignment: Detecting and Grounding Semantic-Coordinated Multimodal Manipulations", "comment": null, "summary": "The detection and grounding of manipulated content in multimodal data has\nemerged as a critical challenge in media forensics. While existing benchmarks\ndemonstrate technical progress, they suffer from misalignment artifacts that\npoorly reflect real-world manipulation patterns: practical attacks typically\nmaintain semantic consistency across modalities, whereas current datasets\nartificially disrupt cross-modal alignment, creating easily detectable\nanomalies. To bridge this gap, we pioneer the detection of\nsemantically-coordinated manipulations where visual edits are systematically\npaired with semantically consistent textual descriptions. Our approach begins\nwith constructing the first Semantic-Aligned Multimodal Manipulation (SAMM)\ndataset, generated through a two-stage pipeline: 1) applying state-of-the-art\nimage manipulations, followed by 2) generation of contextually-plausible\ntextual narratives that reinforce the visual deception. Building on this\nfoundation, we propose a Retrieval-Augmented Manipulation Detection and\nGrounding (RamDG) framework. RamDG commences by harnessing external knowledge\nrepositories to retrieve contextual evidence, which serves as the auxiliary\ntexts and encoded together with the inputs through our image forgery grounding\nand deep manipulation detection modules to trace all manipulations. Extensive\nexperiments demonstrate our framework significantly outperforms existing\nmethods, achieving 2.06\\% higher detection accuracy on SAMM compared to\nstate-of-the-art approaches. The dataset and code are publicly available at\nhttps://github.com/shen8424/SAMM-RamDG-CAP.", "AI": {"tldr": "该论文提出了首个语义对齐多模态篡改（SAMM）数据集，用于检测视觉编辑与语义一致文本描述相结合的篡改内容，并提出了一个检索增强的篡改检测与定位（RamDG）框架，显著优于现有方法。", "motivation": "现有基准数据集存在不对齐伪影，无法真实反映现实世界中保持模态间语义一致性的篡改模式，导致检测到的是容易发现的异常而非实际攻击。研究旨在弥补这一差距，检测语义协调的篡改。", "method": "1. 构建SAMM数据集：采用两阶段管道生成，首先应用最先进的图像篡改技术，然后生成上下文合理且强化视觉欺骗的文本叙述。2. 提出RamDG框架：利用外部知识库检索上下文证据作为辅助文本，并与输入一起通过图像伪造定位和深度篡改检测模块进行编码，以追踪所有篡改。", "result": "实验证明，所提出的框架显著优于现有方法，在SAMM数据集上的检测准确率比最先进的方法高出2.06%。", "conclusion": "该研究通过引入SAMM数据集和RamDG框架，开创了语义协调多模态篡改检测的新方向，有效解决了现有基准与现实世界篡改模式不符的问题，为媒体取证领域提供了更真实、更有效的检测工具。"}}
{"id": "2509.12987", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12987", "abs": "https://arxiv.org/abs/2509.12987", "authors": ["Yarin Benyamin", "Argaman Mordoch", "Shahaf S. Shperberg", "Roni Stern"], "title": "Toward PDDL Planning Copilot", "comment": null, "summary": "Large Language Models (LLMs) are increasingly being used as autonomous agents\ncapable of performing complicated tasks. However, they lack the ability to\nperform reliable long-horizon planning on their own. This paper bridges this\ngap by introducing the Planning Copilot, a chatbot that integrates multiple\nplanning tools and allows users to invoke them through instructions in natural\nlanguage. The Planning Copilot leverages the Model Context Protocol (MCP), a\nrecently developed standard for connecting LLMs with external tools and\nsystems. This approach allows using any LLM that supports MCP without\ndomain-specific fine-tuning. Our Planning Copilot supports common planning\ntasks such as checking the syntax of planning problems, selecting an\nappropriate planner, calling it, validating the plan it generates, and\nsimulating their execution. We empirically evaluate the ability of our Planning\nCopilot to perform these tasks using three open-source LLMs. The results show\nthat the Planning Copilot highly outperforms using the same LLMs without the\nplanning tools. We also conducted a limited qualitative comparison of our tool\nagainst Chat GPT-5, a very recent commercial LLM. Our results shows that our\nPlanning Copilot significantly outperforms GPT-5 despite relying on a much\nsmaller LLM. This suggests dedicated planning tools may be an effective way to\nenable LLMs to perform planning tasks.", "AI": {"tldr": "本文提出Planning Copilot，一个通过集成规划工具并利用Model Context Protocol (MCP) 来增强大型语言模型(LLM)长期规划能力的聊天机器人。它显著优于单独使用LLM以及GPT-5。", "motivation": "大型语言模型（LLM）作为自主代理在执行复杂任务时，缺乏独立执行可靠的长期规划的能力。", "method": "引入Planning Copilot，一个集成多个规划工具的聊天机器人，用户可以通过自然语言指令调用这些工具。它利用Model Context Protocol (MCP) 标准连接LLM与外部工具，支持任何兼容MCP的LLM，无需特定领域微调。Planning Copilot支持规划任务，如语法检查、规划器选择、调用、计划验证和执行模拟。", "result": "实验结果表明，Planning Copilot在使用相同LLM的情况下，其性能远超不使用规划工具的LLM。与最新的商业LLM GPT-5进行有限定性比较，Planning Copilot即使依赖较小的LLM，也显著优于GPT-5。", "conclusion": "研究表明，专用的规划工具可能是使LLM执行规划任务的有效途径，从而弥补了LLM在长期规划方面的不足。"}}
{"id": "2509.13244", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13244", "abs": "https://arxiv.org/abs/2509.13244", "authors": ["Jianfeng Zhu", "Julina Maharjan", "Xinyu Li", "Karin G. Coifman", "Ruoming Jin"], "title": "Evaluating LLM Alignment on Personality Inference from Real-World Interview Data", "comment": "8 pages, 3 figures", "summary": "Large Language Models (LLMs) are increasingly deployed in roles requiring\nnuanced psychological understanding, such as emotional support agents,\ncounselors, and decision-making assistants. However, their ability to interpret\nhuman personality traits, a critical aspect of such applications, remains\nunexplored, particularly in ecologically valid conversational settings. While\nprior work has simulated LLM \"personas\" using discrete Big Five labels on\nsocial media data, the alignment of LLMs with continuous, ground-truth\npersonality assessments derived from natural interactions is largely\nunexamined. To address this gap, we introduce a novel benchmark comprising\nsemi-structured interview transcripts paired with validated continuous Big Five\ntrait scores. Using this dataset, we systematically evaluate LLM performance\nacross three paradigms: (1) zero-shot and chain-of-thought prompting with\nGPT-4.1 Mini, (2) LoRA-based fine-tuning applied to both RoBERTa and Meta-LLaMA\narchitectures, and (3) regression using static embeddings from pretrained BERT\nand OpenAI's text-embedding-3-small. Our results reveal that all Pearson\ncorrelations between model predictions and ground-truth personality traits\nremain below 0.26, highlighting the limited alignment of current LLMs with\nvalidated psychological constructs. Chain-of-thought prompting offers minimal\ngains over zero-shot, suggesting that personality inference relies more on\nlatent semantic representation than explicit reasoning. These findings\nunderscore the challenges of aligning LLMs with complex human attributes and\nmotivate future work on trait-specific prompting, context-aware modeling, and\nalignment-oriented fine-tuning.", "AI": {"tldr": "本研究评估了大型语言模型（LLMs）在自然对话中解释人类人格特质的能力，发现现有模型与经验证的心理学构念的对齐程度有限。", "motivation": "LLMs越来越多地被部署在需要细致心理理解的角色中，如情感支持、咨询和决策辅助。然而，它们解释人类人格特质的能力，尤其是在生态有效对话设置中，尚未被充分探索。先前的研究主要使用离散标签，而非来自自然交互的连续、真实人格评估。", "method": "研究引入了一个新的基准数据集，包含半结构化访谈记录和经过验证的连续大五人格特质分数。系统评估了LLMs在三种范式下的表现：1) GPT-4.1 Mini的零样本和思维链提示；2) 对RoBERTa和Meta-LLaMA架构进行基于LoRA的微调；3) 使用预训练BERT和OpenAI的text-embedding-3-small的静态嵌入进行回归。", "result": "所有模型预测与真实人格特质之间的皮尔逊相关系数均低于0.26，表明当前LLMs与经验证的心理学构念的对齐程度有限。思维链提示相对于零样本提示的提升微乎其微，暗示人格推断更多依赖于潜在语义表示而非显式推理。", "conclusion": "当前LLMs在解释复杂人类人格特质方面存在显著挑战。研究结果强调了LLMs与复杂人类属性对齐的困难，并启发了未来在特质特定提示、上下文感知建模和对齐导向微调方面的工作。"}}
{"id": "2509.13024", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13024", "abs": "https://arxiv.org/abs/2509.13024", "authors": ["Haohan Min", "Zhoujian Li", "Yu Yang", "Jinyu Chen", "Shenghai Yuan"], "title": "DVDP: An End-to-End Policy for Mobile Robot Visual Docking with RGB-D Perception", "comment": null, "summary": "Automatic docking has long been a significant challenge in the field of\nmobile robotics. Compared to other automatic docking methods, visual docking\nmethods offer higher precision and lower deployment costs, making them an\nefficient and promising choice for this task. However, visual docking methods\nimpose strict requirements on the robot's initial position at the start of the\ndocking process. To overcome the limitations of current vision-based methods,\nwe propose an innovative end-to-end visual docking method named DVDP(direct\nvisual docking policy). This approach requires only a binocular RGB-D camera\ninstalled on the mobile robot to directly output the robot's docking path,\nachieving end-to-end automatic docking. Furthermore, we have collected a\nlarge-scale dataset of mobile robot visual automatic docking dataset through a\ncombination of virtual and real environments using the Unity 3D platform and\nactual mobile robot setups. We developed a series of evaluation metrics to\nquantify the performance of the end-to-end visual docking method. Extensive\nexperiments, including benchmarks against leading perception backbones adapted\ninto our framework, demonstrate that our method achieves superior performance.\nFinally, real-world deployment on the SCOUT Mini confirmed DVDP's efficacy,\nwith our model generating smooth, feasible docking trajectories that meet\nphysical constraints and reach the target pose.", "AI": {"tldr": "本文提出了一种名为DVDP的端到端视觉对接方法，仅需RGB-D相机即可直接输出机器人对接路径。通过混合数据集训练并在真实机器人上验证，该方法展现出卓越性能，解决了传统视觉对接对初始位置要求严格的问题。", "motivation": "移动机器人自动对接是一个重大挑战。视觉对接方法因其高精度和低部署成本而具有前景，但对机器人的初始位置有严格要求。现有视觉方法存在局限性，促使研究者寻求更高效的解决方案。", "method": "本文提出了一种名为DVDP（直接视觉对接策略）的创新端到端视觉对接方法。该方法仅需移动机器人上安装的双目RGB-D相机，即可直接输出机器人的对接路径。研究团队通过Unity 3D平台和实际移动机器人结合，收集了一个大规模的虚拟与真实环境混合数据集，并开发了一系列评估指标来量化端到端视觉对接方法的性能。", "result": "广泛的实验（包括与领先感知骨干网络的基准测试）表明，DVDP方法实现了卓越的性能。在SCOUT Mini上的实际部署证实了DVDP的有效性，其模型生成了符合物理约束并能到达目标姿态的平滑、可行的对接轨迹。", "conclusion": "DVDP是一种高效且有前景的端到端视觉对接方法，它克服了现有视觉方法对机器人初始位置的严格要求，能够生成平滑、可行的对接轨迹，并在真实世界环境中表现出强大的实用性。"}}
{"id": "2509.12673", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.12673", "abs": "https://arxiv.org/abs/2509.12673", "authors": ["YiTong Liu", "TianZhu Liu", "YanFeng GU"], "title": "MFAF: An EVA02-Based Multi-scale Frequency Attention Fusion Method for Cross-View Geo-Localization", "comment": "17 pages, 13 figures", "summary": "Cross-view geo-localization aims to determine the geographical location of a\nquery image by matching it against a gallery of images. This task is\nchallenging due to the significant appearance variations of objects observed\nfrom variable views, along with the difficulty in extracting discriminative\nfeatures. Existing approaches often rely on extracting features through feature\nmap segmentation while neglecting spatial and semantic information. To address\nthese issues, we propose the EVA02-based Multi-scale Frequency Attention Fusion\n(MFAF) method. The MFAF method consists of Multi-Frequency Branch-wise Block\n(MFB) and the Frequency-aware Spatial Attention (FSA) module. The MFB block\neffectively captures both low-frequency structural features and high-frequency\nedge details across multiple scales, improving the consistency and robustness\nof feature representations across various viewpoints. Meanwhile, the FSA module\nadaptively focuses on the key regions of frequency features, significantly\nmitigating the interference caused by background noise and viewpoint\nvariability. Extensive experiments on widely recognized benchmarks, including\nUniversity-1652, SUES-200, and Dense-UAV, demonstrate that the MFAF method\nachieves competitive performance in both drone localization and drone\nnavigation tasks.", "AI": {"tldr": "本文提出了一种基于EVA02的多尺度频率注意力融合（MFAF）方法，通过多频率分支块（MFB）和频率感知空间注意力（FSA）模块，有效处理跨视角地理定位中的外观变化和特征提取难题，并在无人机定位和导航任务中取得了有竞争力的性能。", "motivation": "跨视角地理定位任务面临着物体在不同视角下外观差异显著以及难以提取判别性特征的挑战。现有方法常依赖特征图分割来提取特征，但却忽略了空间和语义信息。", "method": "本文提出了EVA02-based的多尺度频率注意力融合（MFAF）方法。该方法包含：1. 多频率分支块（MFB），用于在多尺度上有效捕获低频结构特征和高频边缘细节，增强特征表示的跨视角一致性和鲁棒性。2. 频率感知空间注意力（FSA）模块，用于自适应地关注频率特征的关键区域，显著减轻背景噪声和视角变化带来的干扰。", "result": "在University-1652、SUES-200和Dense-UAV等广泛认可的基准测试中，MFAF方法在无人机定位和无人机导航任务中均取得了有竞争力的性能。", "conclusion": "MFAF方法通过其独特的MFB和FSA模块，有效解决了跨视角地理定位中特征表示和注意力分配的挑战，从而在相关任务中展现出卓越的性能。"}}
{"id": "2509.12999", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.12999", "abs": "https://arxiv.org/abs/2509.12999", "authors": ["Shinichi Honna", "Taichi Murayama", "Akira Matsui"], "title": "Data-driven Methods of Extracting Text Structure and Information Transfer", "comment": null, "summary": "The Anna Karenina Principle (AKP) holds that success requires satisfying a\nsmall set of essential conditions, whereas failure takes diverse forms. We test\nAKP, its reverse, and two further patterns described as ordered and noisy\nacross novels, online encyclopedias, research papers, and movies. Texts are\nrepresented as sequences of functional blocks, and convergence is assessed in\ntransition order and position. Results show that structural principles vary by\nmedium: novels follow reverse AKP in order, Wikipedia combines AKP with ordered\npatterns, academic papers display reverse AKP in order but remain noisy in\nposition, and movies diverge by genre. Success therefore depends on structural\nconstraints that are specific to each medium, while failure assumes different\nshapes across domains.", "AI": {"tldr": "本文测试了安娜·卡列尼娜原则（AKP）及其变体在小说、维基百科、学术论文和电影等不同媒体中的适用性，发现成功所需的结构性约束因媒体而异，而失败形式则跨领域多样化。", "motivation": "了解成功和失败的结构模式是否遵循安娜·卡列尼娜原则（AKP），以及这些模式在不同信息载体（如文学、百科全书、学术著作、电影）中如何体现和变化。", "method": "将不同媒体的文本表示为功能块序列，并通过评估其在转换顺序和位置上的收敛性来测试AKP、其逆向以及有序和嘈杂的结构模式。", "result": "研究发现结构原则因媒体而异：小说遵循逆向AKP的顺序模式；维基百科结合了AKP和有序模式；学术论文在顺序上显示逆向AKP但在位置上保持嘈杂；电影则按类型分化。", "conclusion": "成功依赖于特定媒体的结构性约束，而失败则在不同领域呈现出多样化的形式。"}}
{"id": "2509.13282", "categories": ["cs.CL", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13282", "abs": "https://arxiv.org/abs/2509.13282", "authors": ["Ali Salamatian", "Amirhossein Abaskohi", "Wan-Cyuan Fan", "Mir Rayat Imtiaz Hossain", "Leonid Sigal", "Giuseppe Carenini"], "title": "ChartGaze: Enhancing Chart Understanding in LVLMs with Eye-Tracking Guided Attention Refinement", "comment": "EMNLP 2025", "summary": "Charts are a crucial visual medium for communicating and representing\ninformation. While Large Vision-Language Models (LVLMs) have made progress on\nchart question answering (CQA), the task remains challenging, particularly when\nmodels attend to irrelevant regions of the chart. In this work, we present\nChartGaze, a new eye-tracking dataset that captures human gaze patterns during\nchart reasoning tasks. Through a systematic comparison of human and model\nattention, we find that LVLMs often diverge from human gaze, leading to reduced\ninterpretability and accuracy. To address this, we propose a gaze-guided\nattention refinement that aligns image-text attention with human fixations. Our\napproach improves both answer accuracy and attention alignment, yielding gains\nof up to 2.56 percentage points across multiple models. These results\ndemonstrate the promise of incorporating human gaze to enhance both the\nreasoning quality and interpretability of chart-focused LVLMs.", "AI": {"tldr": "针对LVLM在图表问答中注意力偏差导致的问题，本文提出ChartGaze眼动数据集，并引入眼动引导的注意力优化方法，显著提升了模型的准确性和可解释性。", "motivation": "大型视觉-语言模型（LVLMs）在图表问答（CQA）中仍面临挑战，尤其当模型关注图表中不相关区域时。研究旨在解决LVLM注意力与人类注视模式的偏差，以提高其可解释性和准确性。", "method": "1. 构建了ChartGaze眼动数据集，捕捉人类在图表推理任务中的注视模式。2. 系统比较了人类与模型注意力，发现LVLMs的注意力常与人类注视模式存在差异。3. 提出了一种眼动引导的注意力优化方法，旨在将图像-文本注意力与人类注视对齐。", "result": "所提出的方法在多个模型上将答案准确性提高了高达2.56个百分点，并显著改善了注意力对齐。", "conclusion": "将人类眼动数据融入图表LVLM中，有望增强其推理质量和可解释性。"}}
{"id": "2509.13069", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13069", "abs": "https://arxiv.org/abs/2509.13069", "authors": ["James C. Ward", "Arthur Richards", "Edmund R. Hunt"], "title": "Practical Handling of Dynamic Environments in Decentralised Multi-Robot Patrol", "comment": null, "summary": "Persistent monitoring using robot teams is of interest in fields such as\nsecurity, environmental monitoring, and disaster recovery. Performing such\nmonitoring in a fully on-line decentralised fashion has significant potential\nadvantages for robustness, adaptability, and scalability of monitoring\nsolutions, including, in principle, the capacity to effectively adapt in\nreal-time to a changing environment. We examine this through the lens of\nmulti-robot patrol, in which teams of patrol robots must persistently minimise\ntime between visits to points of interest, within environments where\ntraversability of routes is highly dynamic. These dynamics must be observed by\npatrol agents and accounted for in a fully decentralised on-line manner. In\nthis work, we present a new method of monitoring and adjusting for environment\ndynamics in a decentralised multi-robot patrol team. We demonstrate that our\nmethod significantly outperforms realistic baselines in highly dynamic\nscenarios, and also investigate dynamic scenarios in which explicitly\naccounting for environment dynamics may be unnecessary or impractical.", "AI": {"tldr": "本文提出了一种新的去中心化多机器人巡逻方法，用于在路线可遍历性高度动态变化的环境中，持续监控并最小化对兴趣点的访问时间。", "motivation": "在安全、环境监测和灾难恢复等领域，机器人团队的持续监控具有重要意义。完全在线、去中心化的监控方式在鲁棒性、适应性和可扩展性方面具有显著优势，能够实时适应变化的环境。", "method": "本文提出了一种新的方法，用于去中心化多机器人巡逻团队中监测和调整环境动态。该方法旨在持续最小化机器人团队访问兴趣点之间的时间，并在线地观察和处理动态变化。", "result": "实验结果表明，在高度动态的场景中，所提出的方法显著优于实际基线。研究还探讨了在某些动态场景下，明确考虑环境动态可能不必要或不切实际的情况。", "conclusion": "所提出的去中心化多机器人巡逻方法在处理高度动态环境方面表现出色，能够有效提高监控性能。同时，也对该方法的适用边界进行了探讨。"}}
{"id": "2509.12682", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12682", "abs": "https://arxiv.org/abs/2509.12682", "authors": ["Gordon Hung", "Ivan Felipe Rodriguez"], "title": "A Comparative Study of YOLOv8 to YOLOv11 Performance in Underwater Vision Tasks", "comment": "9 pages, 8 figures, 10 tables", "summary": "Autonomous underwater vehicles (AUVs) increasingly rely on on-board\ncomputer-vision systems for tasks such as habitat mapping, ecological\nmonitoring, and infrastructure inspection. However, underwater imagery is\nhindered by light attenuation, turbidity, and severe class imbalance, while the\ncomputational resources available on AUVs are limited. One-stage detectors from\nthe YOLO family are attractive because they fuse localization and\nclassification in a single, low-latency network; however, their terrestrial\nbenchmarks (COCO, PASCAL-VOC, Open Images) leave open the question of how\nsuccessive YOLO releases perform in the marine domain. We curate two openly\navailable datasets that span contrasting operating conditions: a Coral Disease\nset (4,480 images, 18 classes) and a Fish Species set (7,500 images, 20\nclasses). For each dataset, we create four training regimes (25 %, 50 %, 75 %,\n100 % of the images) while keeping balanced validation and test partitions\nfixed. We train YOLOv8-s, YOLOv9-s, YOLOv10-s, and YOLOv11-s with identical\nhyperparameters (100 epochs, 640 px input, batch = 16, T4 GPU) and evaluate\nprecision, recall, mAP50, mAP50-95, per-image inference time, and\nframes-per-second (FPS). Post-hoc Grad-CAM visualizations probe feature\nutilization and localization faithfulness. Across both datasets, accuracy\nsaturates after YOLOv9, suggesting architectural innovations primarily target\nefficiency rather than accuracy. Inference speed, however, improves markedly.\nOur results (i) provide the first controlled comparison of recent YOLO variants\non underwater imagery, (ii) show that lightweight YOLOv10 offers the best\nspeed-accuracy trade-off for embedded AUV deployment, and (iii) deliver an\nopen, reproducible benchmark and codebase to accelerate future marine-vision\nresearch.", "AI": {"tldr": "本文对YOLOv8-s、YOLOv9-s、YOLOv10-s和YOLOv11-s在水下图像数据集上进行了首次受控比较，发现YOLOv10在AUV部署中提供了最佳的速度-精度权衡。", "motivation": "自主水下航行器（AUV）依赖车载计算机视觉系统执行任务，但水下图像质量受限且AUV计算资源有限。YOLO系列检测器因其低延迟而有吸引力，但其在陆地数据集上的表现无法直接推断在海洋领域的性能。", "method": "研究者整理了两个水下数据集（珊瑚病害和鱼类物种），并为每个数据集创建了四种训练方案（25%、50%、75%、100%图像）。他们使用相同的超参数训练了YOLOv8-s、YOLOv9-s、YOLOv10-s和YOLOv11-s模型，并评估了精度、召回率、mAP50、mAP50-95、每图像推理时间和帧率（FPS）。通过Grad-CAM可视化探测特征利用和定位忠实度。", "result": "研究发现，在YOLOv9之后，模型的准确性趋于饱和，表明架构创新主要针对效率而非准确性。然而，推理速度显著提高。YOLOv10-s在两个数据集上都提供了AUV嵌入式部署的最佳速度-精度权衡。", "conclusion": "本文首次对最新YOLO变体在水下图像上的性能进行了受控比较，指出轻量级YOLOv10为AUV部署提供了最佳的速度-精度平衡，并提供了一个开放、可复现的基准和代码库，以加速未来的海洋视觉研究。"}}
{"id": "2509.13011", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13011", "abs": "https://arxiv.org/abs/2509.13011", "authors": ["Yuyang Tian", "Shunqiang Mao", "Wenchang Gao", "Lanlan Qiu", "Tianxing He"], "title": "A Visualized Framework for Event Cooperation with Generative Agents", "comment": null, "summary": "Large Language Models (LLMs) have revolutionized the simulation of agent\nsocieties, enabling autonomous planning, memory formation, and social\ninteractions. However, existing frameworks often overlook systematic\nevaluations for event organization and lack visualized integration with\nphysically grounded environments, limiting agents' ability to navigate spaces\nand interact with items realistically. We develop MiniAgentPro, a visualization\nplatform featuring an intuitive map editor for customizing environments and a\nsimulation player with smooth animations. Based on this tool, we introduce a\ncomprehensive test set comprising eight diverse event scenarios with basic and\nhard variants to assess agents' ability. Evaluations using GPT-4o demonstrate\nstrong performance in basic settings but highlight coordination challenges in\nhard variants.", "AI": {"tldr": "MiniAgentPro是一个用于LLM代理社会模拟的可视化平台，具有地图编辑器和仿真播放器。它引入了一个包含八种事件场景的综合测试集，评估结果显示GPT-4o在基础设置中表现良好，但在复杂协调任务中面临挑战。", "motivation": "现有的大语言模型（LLM）代理社会模拟框架，尽管在自主规划、记忆形成和社交互动方面表现出色，但往往忽视了事件组织的系统性评估，并缺乏与物理环境的可视化集成，从而限制了代理在空间中导航和与物品互动的真实性。", "method": "本文开发了MiniAgentPro平台，该平台具有直观的地图编辑器用于自定义环境，以及一个带有流畅动画的仿真播放器。基于此工具，研究者引入了一个包含八种多样化事件场景（包括基础和困难变体）的综合测试集，以评估代理的能力。", "result": "使用GPT-4o进行的评估显示，在基础设置中表现强劲，但在困难变体中凸显了协调方面的挑战。", "conclusion": "MiniAgentPro平台及其测试集揭示了LLM代理在模拟社会中的能力和局限性，特别是在复杂事件组织和多代理协调方面的挑战，为未来的研究提供了方向。"}}
{"id": "2509.13309", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13309", "abs": "https://arxiv.org/abs/2509.13309", "authors": ["Zile Qiao", "Guoxin Chen", "Xuanzhong Chen", "Donglei Yu", "Wenbiao Yin", "Xinyu Wang", "Zhen Zhang", "Baixuan Li", "Huifeng Yin", "Kuan Li", "Rui Min", "Minpeng Liao", "Yong Jiang", "Pengjun Xie", "Fei Huang", "Jingren Zhou"], "title": "WebResearcher: Unleashing unbounded reasoning capability in Long-Horizon Agents", "comment": "https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/", "summary": "Recent advances in deep-research systems have demonstrated the potential for\nAI agents to autonomously discover and synthesize knowledge from external\nsources. In this paper, we introduce WebResearcher, a novel framework for\nbuilding such agents through two key components: (1) WebResearcher, an\niterative deep-research paradigm that reformulates deep research as a Markov\nDecision Process, where agents periodically consolidate findings into evolving\nreports while maintaining focused workspaces, overcoming the context\nsuffocation and noise contamination that plague existing mono-contextual\napproaches; and (2) WebFrontier, a scalable data synthesis engine that\ngenerates high-quality training data through tool-augmented complexity\nescalation, enabling systematic creation of research tasks that bridge the gap\nbetween passive knowledge recall and active knowledge construction. Notably, we\nfind that the training data from our paradigm significantly enhances tool-use\ncapabilities even for traditional mono-contextual methods. Furthermore, our\nparadigm naturally scales through parallel thinking, enabling concurrent\nmulti-agent exploration for more comprehensive conclusions. Extensive\nexperiments across 6 challenging benchmarks demonstrate that WebResearcher\nachieves state-of-the-art performance, even surpassing frontier proprietary\nsystems.", "AI": {"tldr": "WebResearcher是一个新颖的AI代理框架，通过迭代式深度研究范式（WebResearcher）和可扩展数据合成引擎（WebFrontier），使AI代理能够自主发现和合成知识，解决了现有方法的上下文限制和噪音污染问题，并实现了最先进的性能。", "motivation": "现有AI代理在自主发现和合成知识时，面临上下文溢出和噪音污染的困扰，且缺乏系统性地创建能连接被动知识回忆与主动知识构建的研究任务的方法。", "method": "该研究引入了WebResearcher框架，包含两个核心组件：1) WebResearcher，一种迭代式深度研究范式，将深度研究重构为马尔可夫决策过程，代理周期性地将发现整合到报告中并维护专注的工作区，以克服上下文溢出和噪音污染。2) WebFrontier，一个可扩展的数据合成引擎，通过工具增强的复杂性升级生成高质量训练数据，系统性地创建连接知识回忆和构建的研究任务。该范式还支持通过并行思维实现多代理并发探索。", "result": "WebResearcher范式生成的训练数据显著增强了工具使用能力，即使对于传统的单上下文方法也有效。该范式通过并行思维自然地实现扩展。在6个具有挑战性的基准测试中，WebResearcher实现了最先进的性能，甚至超越了前沿的专有系统。", "conclusion": "WebResearcher框架通过其迭代式深度研究范式和可扩展数据合成引擎，为AI代理自主发现和合成知识提供了一种卓越的方法。它成功解决了现有方法的关键限制，提升了工具使用能力，并实现了领先的性能，为未来的AI知识构建奠定了基础。"}}
{"id": "2509.13074", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13074", "abs": "https://arxiv.org/abs/2509.13074", "authors": ["Simon Fritsch", "Liam Achenbach", "Riccardo Bianco", "Nicola Irmiger", "Gawain Marti", "Samuel Visca", "Chenyu Yang", "Davide Liconti", "Barnabas Gavin Cangan", "Robert Jomar Malate", "Ronan J. Hinchet", "Robert K. Katzschmann"], "title": "Beyond Anthropomorphism: Enhancing Grasping and Eliminating a Degree of Freedom by Fusing the Abduction of Digits Four and Five", "comment": "First five listed authors have equal contribution", "summary": "This paper presents the SABD hand, a 16-degree-of-freedom (DoF) robotic hand\nthat departs from purely anthropomorphic designs to achieve an expanded grasp\nenvelope, enable manipulation poses beyond human capability, and reduce the\nrequired number of actuators. This is achieved by combining the\nadduction/abduction (Add/Abd) joint of digits four and five into a single joint\nwith a large range of motion. The combined joint increases the workspace of the\ndigits by 400\\% and reduces the required DoFs while retaining dexterity.\nExperimental results demonstrate that the combined Add/Abd joint enables the\nhand to grasp objects with a side distance of up to 200 mm. Reinforcement\nlearning-based investigations show that the design enables grasping policies\nthat are effective not only for handling larger objects but also for achieving\nenhanced grasp stability. In teleoperated trials, the hand successfully\nperformed 86\\% of attempted grasps on suitable YCB objects, including\nchallenging non-anthropomorphic configurations. These findings validate the\ndesign's ability to enhance grasp stability, flexibility, and dexterous\nmanipulation without added complexity, making it well-suited for a wide range\nof applications.", "AI": {"tldr": "本文介绍了一种16自由度（DoF）的SABD机器人手，它通过结合第四和第五指的内收/外展关节，实现了更大的抓取范围、超越人类能力的操纵姿态，并减少了所需执行器数量，同时提高了抓取稳定性和灵活性。", "motivation": "传统的拟人化机器人手设计在抓取范围、操纵姿态以及所需执行器数量方面存在局限性，研究旨在克服这些限制，实现更广阔的抓取能力和更灵活的操纵。", "method": "SABD手设计通过将第四和第五指的内收/外展（Add/Abd）关节合并为一个具有大运动范围的单一关节。通过实验测量了关节工作空间，进行了抓取200毫米侧向距离物体的测试，使用强化学习（RL）探索抓取策略，并进行了遥操作试验以评估其在YCB物体上的抓取成功率。", "result": "合并后的内收/外展关节使手指工作空间增加了400%，并在保持灵巧性的同时减少了所需自由度。实验证明，该手能够抓取侧向距离达200毫米的物体。基于强化学习的研究表明，该设计不仅能有效处理大型物体，还能提高抓取稳定性。在遥操作试验中，该手成功完成了86%的YCB物体抓取尝试，包括具有挑战性的非拟人化配置。", "conclusion": "SABD手的设计在不增加复杂性的前提下，显著增强了抓取稳定性、灵活性和灵巧操纵能力，使其非常适合广泛的应用场景。"}}
{"id": "2509.12683", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12683", "abs": "https://arxiv.org/abs/2509.12683", "authors": ["Xianda Guo", "Chenming Zhang", "Ruilin Wang", "Youmin Zhang", "Wenzhao Zheng", "Matteo Poggi", "Hao Zhao", "Qin Zou", "Long Chen"], "title": "StereoCarla: A High-Fidelity Driving Dataset for Generalizable Stereo", "comment": null, "summary": "Stereo matching plays a crucial role in enabling depth perception for\nautonomous driving and robotics. While recent years have witnessed remarkable\nprogress in stereo matching algorithms, largely driven by learning-based\nmethods and synthetic datasets, the generalization performance of these models\nremains constrained by the limited diversity of existing training data. To\naddress these challenges, we present StereoCarla, a high-fidelity synthetic\nstereo dataset specifically designed for autonomous driving scenarios. Built on\nthe CARLA simulator, StereoCarla incorporates a wide range of camera\nconfigurations, including diverse baselines, viewpoints, and sensor placements\nas well as varied environmental conditions such as lighting changes, weather\neffects, and road geometries. We conduct comprehensive cross-domain experiments\nacross four standard evaluation datasets (KITTI2012, KITTI2015, Middlebury,\nETH3D) and demonstrate that models trained on StereoCarla outperform those\ntrained on 11 existing stereo datasets in terms of generalization accuracy\nacross multiple benchmarks. Furthermore, when integrated into multi-dataset\ntraining, StereoCarla contributes substantial improvements to generalization\naccuracy, highlighting its compatibility and scalability. This dataset provides\na valuable benchmark for developing and evaluating stereo algorithms under\nrealistic, diverse, and controllable settings, facilitating more robust depth\nperception systems for autonomous vehicles. Code can be available at\nhttps://github.com/XiandaGuo/OpenStereo, and data can be available at\nhttps://xiandaguo.net/StereoCarla.", "AI": {"tldr": "本文提出了StereoCarla，一个为自动驾驶场景设计的高保真合成立体数据集，旨在解决现有训练数据多样性不足导致立体匹配模型泛化性能受限的问题。实验表明，使用StereoCarla训练的模型在跨域泛化准确性方面优于现有数据集，并能显著提升多数据集训练效果。", "motivation": "立体匹配算法的泛化性能受限于现有训练数据的多样性不足，尤其是在学习型方法和合成数据集驱动下取得显著进展后，这一问题变得更为突出。", "method": "研究人员基于CARLA模拟器创建了StereoCarla数据集，其中包含了广泛的相机配置（如不同的基线、视角和传感器放置）以及多样的环境条件（如光照变化、天气效应和道路几何形状）。他们还在四个标准评估数据集上进行了全面的跨域实验。", "result": "在KITTI2012、KITTI2015、Middlebury和ETH3D等基准测试中，使用StereoCarla训练的模型在泛化准确性方面超越了11个现有立体数据集。此外，将StereoCarla整合到多数据集训练中，显著提高了泛化准确性，表明其良好的兼容性和可扩展性。", "conclusion": "StereoCarla数据集为在真实、多样化和可控设置下开发和评估立体算法提供了一个有价值的基准，有助于为自动驾驶车辆构建更鲁棒的深度感知系统。"}}
{"id": "2509.13131", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13131", "abs": "https://arxiv.org/abs/2509.13131", "authors": ["Marylou Fauchard", "Florian Carichon", "Margarida Carvalho", "Golnoosh Farnadi"], "title": "Reasoning with Preference Constraints: A Benchmark for Language Models in Many-to-One Matching Markets", "comment": null, "summary": "Recent advances in reasoning with large language models (LLMs) have\ndemonstrated strong performance on complex mathematical tasks, including\ncombinatorial optimization. Techniques such as Chain-of-Thought and In-Context\nLearning have further enhanced this capability, making LLMs both powerful and\naccessible tools for a wide range of users, including non-experts. However,\napplying LLMs to matching problems, which require reasoning under preferential\nand structural constraints, remains underexplored. To address this gap, we\nintroduce a novel benchmark of 369 instances of the College Admission Problem,\na canonical example of a matching problem with preferences, to evaluate LLMs\nacross key dimensions: feasibility, stability, and optimality. We employ this\nbenchmark to assess the performance of several open-weight LLMs. Our results\nfirst reveal that while LLMs can satisfy certain constraints, they struggle to\nmeet all evaluation criteria consistently. They also show that reasoning LLMs,\nlike QwQ and GPT-oss, significantly outperform traditional models such as\nLlama, Qwen or Mistral, defined here as models used without any dedicated\nreasoning mechanisms. Moreover, we observed that LLMs reacted differently to\nthe various prompting strategies tested, which include Chain-of-Thought,\nIn-Context Learning and role-based prompting, with no prompt consistently\noffering the best performance. Finally, we report the performances from\niterative prompting with auto-generated feedback and show that they are not\nmonotonic; they can peak early and then significantly decline in later\nattempts. Overall, this work offers a new perspective on model reasoning\nperformance and the effectiveness of prompting strategies in combinatorial\noptimization problems with preferential constraints.", "AI": {"tldr": "本研究引入了一个新的大学招生问题基准，用于评估大型语言模型（LLMs）在具有偏好和结构约束的匹配问题上的表现。结果显示，LLMs难以持续满足所有评估标准，推理型LLMs表现优于传统LLMs，且不同的提示策略效果不一，迭代提示的效果也非单调。", "motivation": "尽管LLMs在复杂数学任务和组合优化方面表现出色，但其在需要偏好和结构约束推理的匹配问题上的应用尚未得到充分探索。为了填补这一空白，需要评估LLMs在此类问题上的能力。", "method": "研究引入了一个包含369个大学招生问题实例的新基准，这是一个典型的带偏好匹配问题。使用该基准评估了多个开源LLMs在可行性、稳定性和最优性等关键维度上的性能。测试了包括思维链（Chain-of-Thought）、上下文学习（In-Context Learning）和基于角色的提示等多种提示策略，并探讨了带有自动生成反馈的迭代提示效果。", "result": "研究发现，LLMs虽然能满足某些约束，但难以持续满足所有评估标准。QwQ和GPT-oss等推理型LLMs显著优于Llama、Qwen或Mistral等传统模型。此外，LLMs对不同的提示策略反应各异，没有单一策略能始终提供最佳性能。带有自动生成反馈的迭代提示表现并非单调，可能早期达到峰值，随后显著下降。", "conclusion": "本工作为模型推理性能和提示策略在具有偏好约束的组合优化问题中的有效性提供了新视角。LLMs在匹配问题上仍面临挑战，但推理型LLMs表现出潜力，且提示策略的选择和迭代过程的理解至关重要。"}}
{"id": "2509.13310", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13310", "abs": "https://arxiv.org/abs/2509.13310", "authors": ["Liangcai Su", "Zhen Zhang", "Guangyu Li", "Zhuo Chen", "Chenxi Wang", "Maojia Song", "Xinyu Wang", "Kuan Li", "Jialong Wu", "Xuanzhong Chen", "Zile Qiao", "Zhongwang Zhang", "Huifeng Yin", "Shihao Cai", "Runnan Fang", "Zhengwei Tao", "Wenbiao Yin", "Chenxiong Qian", "Yong Jiang", "Pengjun Xie", "Fei Huang", "Jingren Zhou"], "title": "Scaling Agents via Continual Pre-training", "comment": "https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/", "summary": "Large language models (LLMs) have evolved into agentic systems capable of\nautonomous tool use and multi-step reasoning for complex problem-solving.\nHowever, post-training approaches building upon general-purpose foundation\nmodels consistently underperform in agentic tasks, particularly in open-source\nimplementations. We identify the root cause: the absence of robust agentic\nfoundation models forces models during post-training to simultaneously learn\ndiverse agentic behaviors while aligning them to expert demonstrations, thereby\ncreating fundamental optimization tensions. To this end, we are the first to\npropose incorporating Agentic Continual Pre-training (Agentic CPT) into the\ndeep research agents training pipeline to build powerful agentic foundational\nmodels. Based on this approach, we develop a deep research agent model named\nAgentFounder. We evaluate our AgentFounder-30B on 10 benchmarks and achieve\nstate-of-the-art performance while retains strong tool-use ability, notably\n39.9% on BrowseComp-en, 43.3% on BrowseComp-zh, and 31.5% Pass@1 on HLE.", "AI": {"tldr": "现有LLM代理系统在后训练中表现不佳，原因是缺乏强大的代理基础模型。本文首次提出代理持续预训练（Agentic CPT）方法，并基于此开发了AgentFounder模型，在多项基准测试中取得了最先进的性能。", "motivation": "大型语言模型（LLMs）作为代理系统在工具使用和多步推理方面能力不足，尤其是在开源实现中。现有后训练方法表现不佳，根源在于缺乏强大的代理基础模型，导致模型在后训练阶段需同时学习多样化的代理行为并与专家演示对齐，从而产生根本性的优化冲突。", "method": "本文首次提出将代理持续预训练（Agentic CPT）整合到深度研究代理的训练流程中，以构建强大的代理基础模型。基于此方法，开发了一个名为AgentFounder的深度研究代理模型。", "result": "AgentFounder-30B在10个基准测试中实现了最先进的性能，并保持了强大的工具使用能力，特别是在BrowseComp-en上达到39.9%，BrowseComp-zh上达到43.3%，以及HLE上Pass@1达到31.5%。", "conclusion": "通过引入代理持续预训练（Agentic CPT），可以有效构建强大的代理基础模型，从而克服现有LLM代理系统在后训练中的性能瓶颈，AgentFounder的优异表现证明了这一方法的有效性。"}}
{"id": "2509.13077", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13077", "abs": "https://arxiv.org/abs/2509.13077", "authors": ["Jonathan Külz", "Sehoon Ha", "Matthias Althoff"], "title": "A Design Co-Pilot for Task-Tailored Manipulators", "comment": null, "summary": "Although robotic manipulators are used in an ever-growing range of\napplications, robot manufacturers typically follow a ``one-fits-all''\nphilosophy, employing identical manipulators in various settings. This often\nleads to suboptimal performance, as general-purpose designs fail to exploit\nparticularities of tasks. The development of custom, task-tailored robots is\nhindered by long, cost-intensive development cycles and the high cost of\ncustomized hardware. Recently, various computational design methods have been\ndevised to overcome the bottleneck of human engineering. In addition, a surge\nof modular robots allows quick and economical adaptation to changing industrial\nsettings. This work proposes an approach to automatically designing and\noptimizing robot morphologies tailored to a specific environment. To this end,\nwe learn the inverse kinematics for a wide range of different manipulators. A\nfully differentiable framework realizes gradient-based fine-tuning of designed\nrobots and inverse kinematics solutions. Our generative approach accelerates\nthe generation of specialized designs from hours with optimization-based\nmethods to seconds, serving as a design co-pilot that enables instant\nadaptation and effective human-AI collaboration. Numerical experiments show\nthat our approach finds robots that can navigate cluttered environments,\nmanipulators that perform well across a specified workspace, and can be adapted\nto different hardware constraints. Finally, we demonstrate the real-world\napplicability of our method by setting up a modular robot designed in\nsimulation that successfully moves through an obstacle course.", "AI": {"tldr": "本文提出了一种自动化设计和优化机器人形态的方法，该方法能针对特定任务和环境定制机器人，通过可微分框架和学习逆运动学，将设计时间从数小时缩短到数秒，实现快速适应和人机协作。", "motivation": "现有机器人制造商普遍采用“一刀切”的设计理念，导致机器人在特定应用中性能不佳。定制化机器人设计周期长、成本高昂。尽管已有计算设计方法和模块化机器人，但仍需要更高效的自动化设计工具来克服人工工程的瓶颈。", "method": "本研究提出了一种自动设计和优化机器人形态的方法。其核心是学习各种机械手的逆运动学，并利用一个完全可微分的框架实现梯度优化的机器人设计和逆运动学解的微调。这是一种生成式方法。", "result": "该方法将专业化设计生成时间从数小时缩短到数秒，可作为设计副驾驶，实现即时适应和有效的人机协作。数值实验表明，该方法能够找到在杂乱环境中导航的机器人、在指定工作空间内表现良好的机械手，并能适应不同的硬件约束。此外，研究通过在模拟中设计的模块化机器人在现实障碍赛中成功移动，验证了该方法的实际应用性。", "conclusion": "本研究提供了一种高效的生成式、可微分框架，能够显著加速特定任务机器人形态的设计和优化过程，从而提升性能、实现快速适应，并促进有效的人机协作，且已在实际应用中得到验证。"}}
{"id": "2509.12701", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12701", "abs": "https://arxiv.org/abs/2509.12701", "authors": ["Wenzhuo Jin", "Qianfeng Yang", "Xianhao Wu", "Hongming Chen", "Pengpeng Li", "Xiang Chen"], "title": "SmokeBench: A Real-World Dataset for Surveillance Image Desmoking in Early-Stage Fire Scenes", "comment": "Accepted by ACMMM 2025 Datasets Track", "summary": "Early-stage fire scenes (0-15 minutes after ignition) represent a crucial\ntemporal window for emergency interventions. During this stage, the smoke\nproduced by combustion significantly reduces the visibility of surveillance\nsystems, severely impairing situational awareness and hindering effective\nemergency response and rescue operations. Consequently, there is an urgent need\nto remove smoke from images to obtain clear scene information. However, the\ndevelopment of smoke removal algorithms remains limited due to the lack of\nlarge-scale, real-world datasets comprising paired smoke-free and\nsmoke-degraded images. To address these limitations, we present a real-world\nsurveillance image desmoking benchmark dataset named SmokeBench, which contains\nimage pairs captured under diverse scenes setup and smoke concentration. The\ncurated dataset provides precisely aligned degraded and clean images, enabling\nsupervised learning and rigorous evaluation. We conduct comprehensive\nexperiments by benchmarking a variety of desmoking methods on our dataset. Our\ndataset provides a valuable foundation for advancing robust and practical image\ndesmoking in real-world fire scenes. This dataset has been released to the\npublic and can be downloaded from https://github.com/ncfjd/SmokeBench.", "AI": {"tldr": "该论文提出了一个名为SmokeBench的真实世界监控图像去烟基准数据集，以解决早期火灾场景中烟雾降低能见度以及缺乏配对真实世界数据集的问题，从而推动图像去烟算法的发展。", "motivation": "早期火灾场景（点燃后0-15分钟）的烟雾严重降低了监控系统的能见度，阻碍了应急响应和救援行动。因此，迫切需要从图像中去除烟雾以获取清晰的场景信息。然而，由于缺乏大规模、真实的配对无烟和烟雾降级图像数据集，去烟算法的发展受到了限制。", "method": "为了解决数据集限制，作者构建了一个名为SmokeBench的真实世界监控图像去烟基准数据集。该数据集包含在不同场景设置和烟雾浓度下捕获的图像对，提供了精确对齐的降级图像和清晰图像，以支持监督学习和严格评估。", "result": "论文发布了SmokeBench数据集，其中包含多样化场景和烟雾浓度的配对烟雾降级和清晰图像。作者还通过在该数据集上对各种去烟方法进行基准测试，进行了全面的实验。", "conclusion": "所提出的SmokeBench数据集为推动真实世界火灾场景中鲁棒且实用的图像去烟技术提供了宝贵的基础。该数据集已公开发布。"}}
{"id": "2509.13137", "categories": ["cs.AI", "cs.HC", "cs.MA", "K.4.4; K.6.5; I.2.11"], "pdf": "https://arxiv.org/pdf/2509.13137", "abs": "https://arxiv.org/abs/2509.13137", "authors": ["Henrik Axelsen", "Valdemar Licht", "Jan Damsgaard"], "title": "Agentic AI for Financial Crime Compliance", "comment": "Accepted for presentation at HICSS-59 (2026), forthcoming in\n  Proceedings", "summary": "The cost and complexity of financial crime compliance (FCC) continue to rise,\noften without measurable improvements in effectiveness. While AI offers\npotential, most solutions remain opaque and poorly aligned with regulatory\nexpectations. This paper presents the design and deployment of an agentic AI\nsystem for FCC in digitally native financial platforms. Developed through an\nAction Design Research (ADR) process with a fintech firm and regulatory\nstakeholders, the system automates onboarding, monitoring, investigation, and\nreporting, emphasizing explainability, traceability, and compliance-by-design.\nUsing artifact-centric modeling, it assigns clearly bounded roles to autonomous\nagents and enables task-specific model routing and audit logging. The\ncontribution includes a reference architecture, a real-world prototype, and\ninsights into how Agentic AI can reconfigure FCC workflows under regulatory\nconstraints. Our findings extend IS literature on AI-enabled compliance by\ndemonstrating how automation, when embedded within accountable governance\nstructures, can support transparency and institutional trust in high-stakes,\nregulated environments.", "AI": {"tldr": "本文介绍了一个在数字原生金融平台中设计和部署的代理式AI系统，用于金融犯罪合规（FCC），该系统强调可解释性、可追溯性和合规性设计，并由行动设计研究（ADR）流程开发。", "motivation": "金融犯罪合规（FCC）的成本和复杂性持续上升，但有效性提升不明显。现有AI解决方案往往不透明且不符合监管期望。", "method": "通过与金融科技公司和监管利益相关者的行动设计研究（ADR）流程，开发了一个代理式AI系统。该系统采用以构件为中心的建模，为自主代理分配明确角色，并实现任务特定模型路由和审计日志，从而自动化了入职、监控、调查和报告等流程，并强调可解释性、可追溯性和合规性设计。", "result": "研究贡献包括一个参考架构、一个真实世界的原型，以及关于代理式AI如何在监管约束下重构FCC工作流程的见解。研究还展示了自动化如何通过负责任的治理结构支持高风险、受监管环境中的透明度和机构信任。", "conclusion": "自动化，当嵌入到负责任的治理结构中时，能够支持高风险、受监管环境中的透明度和机构信任，这扩展了信息系统（IS）领域关于AI赋能合规的文献。"}}
{"id": "2509.13311", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13311", "abs": "https://arxiv.org/abs/2509.13311", "authors": ["Runnan Fang", "Shihao Cai", "Baixuan Li", "Jialong Wu", "Guangyu Li", "Wenbiao Yin", "Xinyu Wang", "Xiaobin Wang", "Liangcai Su", "Zhen Zhang", "Shibin Wu", "Zhengwei Tao", "Yong Jiang", "Pengjun Xie", "Fei Huang", "Jingren Zhou"], "title": "Towards General Agentic Intelligence via Environment Scaling", "comment": "https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/", "summary": "Advanced agentic intelligence is a prerequisite for deploying Large Language\nModels in practical, real-world applications. Diverse real-world APIs demand\nprecise, robust function-calling intelligence, which needs agents to develop\nthese capabilities through interaction in varied environments. The breadth of\nfunction-calling competence is closely tied to the diversity of environments in\nwhich agents are trained. In this work, we scale up environments as a step\ntowards advancing general agentic intelligence. This gives rise to two central\nchallenges: (i) how to scale environments in a principled manner, and (ii) how\nto effectively train agentic capabilities from experiences derived through\ninteractions with these environments. To address these, we design a scalable\nframework that automatically constructs heterogeneous environments that are\nfully simulated, systematically broadening the space of function-calling\nscenarios. We further adapt a two-phase agent fine-tuning strategy: first\nendowing agents with fundamental agentic capabilities, then specializing them\nfor domain-specific contexts. Extensive experiments on agentic benchmarks,\ntau-bench, tau2-Bench, and ACEBench, demonstrate that our trained model,\nAgentScaler, significantly enhances the function-calling capability of models.", "AI": {"tldr": "该研究通过大规模扩展模拟环境和两阶段微调策略，显著提升了大型语言模型（LLM）的智能体功能调用能力，以适应复杂的现实世界应用。", "motivation": "将大型语言模型部署到实际应用中需要先进的智能体智能，尤其是精确且鲁棒的功能调用能力。这种能力需要智能体在多样化环境中通过交互来发展，而功能调用能力的广度与训练环境的多样性密切相关。然而，如何以有原则的方式扩展环境以及如何有效地从这些交互经验中训练智能体能力是核心挑战。", "method": "研究设计了一个可扩展的框架，该框架能够自动构建异构的、完全模拟的环境，系统性地拓宽功能调用场景。此外，研究还采用了一种两阶段的智能体微调策略：首先赋予智能体基本智能体能力，然后针对特定领域上下文进行专业化训练。", "result": "在智能体基准测试（tau-bench、tau2-Bench和ACEBench）上进行的广泛实验表明，所训练的模型AgentScaler显著增强了模型的功能调用能力。", "conclusion": "通过设计可扩展的环境构建框架和两阶段的智能体微调策略，本研究成功解决了扩展环境和有效训练智能体能力的挑战，从而显著提升了大型语言模型在复杂现实世界应用中的功能调用能力。"}}
{"id": "2509.13095", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13095", "abs": "https://arxiv.org/abs/2509.13095", "authors": ["Zijie Zhao", "Honglei Guo", "Shengqian Chen", "Kaixuan Xu", "Bo Jiang", "Yuanheng Zhu", "Dongbin Zhao"], "title": "Empowering Multi-Robot Cooperation via Sequential World Models", "comment": null, "summary": "Model-based reinforcement learning (MBRL) has shown significant potential in\nrobotics due to its high sample efficiency and planning capability. However,\nextending MBRL to multi-robot cooperation remains challenging due to the\ncomplexity of joint dynamics. To address this, we propose the Sequential World\nModel (SeqWM), a novel framework that integrates the sequential paradigm into\nmodel-based multi-agent reinforcement learning. SeqWM employs independent,\nsequentially structured agent-wise world models to decompose complex joint\ndynamics. Latent rollouts and decision-making are performed through sequential\ncommunication, where each agent generates its future trajectory and plans its\nactions based on the predictions of its predecessors. This design enables\nexplicit intention sharing, enhancing cooperative performance, and reduces\ncommunication overhead to linear complexity. Results in challenging simulated\nenvironments (Bi-DexHands and Multi-Quad) show that SeqWM outperforms existing\nstate-of-the-art model-free and model-based baselines in both overall\nperformance and sample efficiency, while exhibiting advanced cooperative\nbehaviors such as predictive adaptation and role division. Furthermore, SeqWM\nhas been success fully deployed on physical quadruped robots, demonstrating its\neffectiveness in real-world multi-robot systems. Demos and code are available\nat: https://github.com/zhaozijie2022/seqwm-marl", "AI": {"tldr": "SeqWM是一种新颖的模型化多智能体强化学习框架，通过独立、序列化的智能体世界模型和顺序通信，有效解决了多机器人协作中复杂的联合动力学问题，提高了协作性能和样本效率，并成功应用于物理机器人。", "motivation": "模型化强化学习(MBRL)在机器人领域显示出高样本效率和规划能力，但由于联合动力学的复杂性，将其扩展到多机器人协作仍然具有挑战性。", "method": "本文提出了序列世界模型(SeqWM)框架，将序列范式整合到模型化多智能体强化学习中。SeqWM采用独立的、序列化结构的智能体世界模型来分解复杂的联合动力学。通过序列通信进行潜在轨迹预测和决策，每个智能体根据其前驱的预测生成未来轨迹并规划行动。这种设计实现了显式意图共享，并使通信开销降至线性复杂度。", "result": "在具有挑战性的模拟环境（Bi-DexHands和Multi-Quad）中，SeqWM在整体性能和样本效率方面均优于现有的最先进无模型和有模型基线，并展现出预测性适应和角色分工等高级协作行为。此外，SeqWM已成功部署到物理四足机器人上，证明了其在真实世界多机器人系统中的有效性。", "conclusion": "SeqWM通过其独特的序列化世界模型和通信机制，为多机器人协作提供了一个高效且鲁棒的解决方案，显著提升了协作性能和样本效率，并具备在真实世界应用的能力。"}}
{"id": "2509.12710", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12710", "abs": "https://arxiv.org/abs/2509.12710", "authors": ["Siju Ma", "Changsiyu Gong", "Xiaofeng Fan", "Yong Ma", "Chengjie Jiang"], "title": "RIS-FUSION: Rethinking Text-Driven Infrared and Visible Image Fusion from the Perspective of Referring Image Segmentation", "comment": "5 pages, 2 figures", "summary": "Text-driven infrared and visible image fusion has gained attention for\nenabling natural language to guide the fusion process. However, existing\nmethods lack a goal-aligned task to supervise and evaluate how effectively the\ninput text contributes to the fusion outcome. We observe that referring image\nsegmentation (RIS) and text-driven fusion share a common objective:\nhighlighting the object referred to by the text. Motivated by this, we propose\nRIS-FUSION, a cascaded framework that unifies fusion and RIS through joint\noptimization. At its core is the LangGatedFusion module, which injects textual\nfeatures into the fusion backbone to enhance semantic alignment. To support\nmultimodal referring image segmentation task, we introduce MM-RIS, a\nlarge-scale benchmark with 12.5k training and 3.5k testing triplets, each\nconsisting of an infrared-visible image pair, a segmentation mask, and a\nreferring expression. Extensive experiments show that RIS-FUSION achieves\nstate-of-the-art performance, outperforming existing methods by over 11% in\nmIoU. Code and dataset will be released at\nhttps://github.com/SijuMa2003/RIS-FUSION.", "AI": {"tldr": "现有文本驱动红外与可见光图像融合方法缺乏目标对齐监督。本文提出RIS-FUSION框架，通过联合优化融合与指代图像分割（RIS）任务，并引入LangGatedFusion模块注入文本特征。同时构建MM-RIS大规模基准，实验证明RIS-FUSION在mIoU上超越现有方法11%以上，达到最先进水平。", "motivation": "现有文本驱动的图像融合方法缺乏一个目标对齐的任务来有效监督和评估输入文本对融合结果的贡献。作者观察到指代图像分割（RIS）与文本驱动融合具有共同目标：突出文本所指代的物体，这促使他们将这两个任务统一起来。", "method": "本文提出了RIS-FUSION，一个级联框架，通过联合优化统一了融合和指代图像分割（RIS）任务。其核心是LangGatedFusion模块，该模块将文本特征注入到融合骨干网络中以增强语义对齐。为支持多模态指代图像分割任务，作者还引入了MM-RIS，一个大规模基准数据集，包含12.5k训练和3.5k测试三元组，每个三元组由红外-可见光图像对、分割掩码和指代表达组成。", "result": "RIS-FUSION取得了最先进的性能，在mIoU上比现有方法高出11%以上。", "conclusion": "通过将文本驱动的红外与可见光图像融合与指代图像分割任务进行统一，并采用联合优化和LangGatedFusion模块，RIS-FUSION框架能够有效解决现有方法缺乏目标对齐监督的问题，显著提升了融合性能，并为多模态指代图像分割任务提供了新的大规模基准。"}}
{"id": "2509.13203", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13203", "abs": "https://arxiv.org/abs/2509.13203", "authors": ["Kanishk Garg", "Saranya D.", "Sanal Kumar", "Saurabh Singh", "Anupam Purwar"], "title": "G-CSEA: A Graph-Based Conflict Set Extraction Algorithm for Identifying Infeasibility in Pseudo-Boolean Models", "comment": "This paper presents G-CSEA, a novel graph-based algorithm for rapidly\n  diagnosing infeasibility in workforce scheduling models. Inspired by\n  Conflict-Driven Clause Learning (CDCL), our method efficiently extracts a\n  compact conflict set from an implication graph, reducing the initial\n  constraint set by approximately 94%", "summary": "Workforce scheduling involves a variety of rule-based constraints-such as\nshift limits, staffing policies, working hour restrictions, and many similar\nscheduling rules-which can interact in conflicting ways, leading to infeasible\nmodels. Identifying the underlying causes of such infeasibility is critical for\nresolving scheduling issues and restoring feasibility. A common diagnostic\napproach is to compute Irreducible Infeasible Subsets (IISs): minimal sets of\nconstraints that are jointly infeasible but become feasible when any one is\nremoved. We consider models formulated using pseudo-Boolean constraints with\ninequality relations over binary variables, which naturally encode scheduling\nlogic. Existing IIS extraction methods such as Additive Deletion and\nQuickXplain rely on repeated feasibility checks, often incurring large numbers\nof solver calls. Dual ray analysis, while effective for LP-based models, may\nfail when the relaxed problem is feasible but the underlying pseudo-Boolean\nmodel is not. To address these limitations, we propose Graph-based Conflict Set\nExtraction Algorithm (G-CSEA) to extract a conflict set, an approach inspired\nby Conflict-Driven Clause Learning (CDCL) in SAT solvers. Our method constructs\nan implication graph during constraint propagation and, upon detecting a\nconflict, traces all contributing constraints across both decision branches.\nThe resulting conflict set can optionally be minimized using QuickXplain to\nproduce an IIS.", "AI": {"tldr": "本文提出了一种名为G-CSEA的图基冲突集提取算法，旨在更高效、更鲁棒地识别排班模型中导致不可行的约束集（IIS），以克服现有方法的局限性。", "motivation": "排班模型中复杂的规则约束（如班次限制、人员配置策略等）可能相互冲突，导致模型不可行。识别这些不可行性的根本原因（即不可约不可行子集IIS）对于解决排班问题至关重要。现有IIS提取方法（如Additive Deletion和QuickXplain）需要大量求解器调用，效率低下；而双射线分析对于伪布尔模型可能失效。", "method": "本文提出图基冲突集提取算法（G-CSEA），其灵感来源于SAT求解器中的冲突驱动子句学习（CDCL）。该方法在约束传播过程中构建一个蕴含图，并在检测到冲突时，追踪所有在决策分支中导致冲突的约束。生成的冲突集可以选择性地通过QuickXplain进行最小化以产生IIS。", "result": "G-CSEA能够有效地提取冲突集，并能通过后续最小化生成IIS。该方法旨在解决现有IIS提取方法在求解器调用次数过多和对伪布尔模型适用性不足的问题，提供了一种更高效、更适用于伪布尔模型的不可行性诊断工具。", "conclusion": "G-CSEA为排班模型中基于伪布尔约束的不可行性诊断提供了一种新颖且高效的方法，通过构建蕴含图和追踪冲突，能够有效地识别导致模型不可行的冲突约束集，从而帮助解决排班问题。"}}
{"id": "2509.13312", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13312", "abs": "https://arxiv.org/abs/2509.13312", "authors": ["Zijian Li", "Xin Guan", "Bo Zhang", "Shen Huang", "Houquan Zhou", "Shaopeng Lai", "Ming Yan", "Yong Jiang", "Pengjun Xie", "Fei Huang", "Jun Zhang", "Jingren Zhou"], "title": "WebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for Open-Ended Deep Research", "comment": "An agent system for open-ended deep research", "summary": "This paper tackles open-ended deep research (OEDR), a complex challenge where\nAI agents must synthesize vast web-scale information into insightful reports.\nCurrent approaches are plagued by dual-fold limitations: static research\npipelines that decouple planning from evidence acquisition and one-shot\ngeneration paradigms that easily suffer from long-context failure issues like\n\"loss in the middle\" and hallucinations. To address these challenges, we\nintroduce WebWeaver, a novel dual-agent framework that emulates the human\nresearch process. The planner operates in a dynamic cycle, iteratively\ninterleaving evidence acquisition with outline optimization to produce a\ncomprehensive, source-grounded outline linking to a memory bank of evidence.\nThe writer then executes a hierarchical retrieval and writing process,\ncomposing the report section by section. By performing targeted retrieval of\nonly the necessary evidence from the memory bank for each part, it effectively\nmitigates long-context issues. Our framework establishes a new state-of-the-art\nacross major OEDR benchmarks, including DeepResearch Bench, DeepConsult, and\nDeepResearchGym. These results validate our human-centric, iterative\nmethodology, demonstrating that adaptive planning and focused synthesis are\ncrucial for producing high-quality, reliable, and well-structured reports.", "AI": {"tldr": "本文提出WebWeaver，一个双智能体框架，通过模拟人类研究过程，解决开放式深度研究（OEDR）中现有方法存在的静态管道和长上下文失败问题，并在主流OEDR基准测试中达到最先进水平。", "motivation": "当前的开放式深度研究（OEDR）方法存在两大局限性：一是研究流程僵化，规划与证据获取分离；二是采用一次性生成范式，容易出现“中间丢失”和幻觉等长上下文失败问题。这些限制促使研究者寻求更有效的方法来综合海量信息并生成有洞察力的报告。", "method": "本文引入WebWeaver框架，它由两个智能体组成：规划器和撰写器。规划器采用动态循环，迭代地交织证据获取与大纲优化，生成一个全面、有来源依据的大纲，并链接到证据记忆库。撰写器则执行分层检索和写作过程，逐节撰写报告，通过针对性地从记忆库中检索每部分所需证据，有效缓解长上下文问题。", "result": "WebWeaver框架在DeepResearch Bench、DeepConsult和DeepResearchGym等主要的OEDR基准测试中均取得了新的最先进（state-of-the-art）成果。", "conclusion": "研究结果验证了WebWeaver以人为中心、迭代式的方法论，表明自适应规划和聚焦式综合对于生成高质量、可靠且结构良好的报告至关重要。"}}
{"id": "2509.13126", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13126", "abs": "https://arxiv.org/abs/2509.13126", "authors": ["Miquel Oller", "An Dang", "Nima Fazeli"], "title": "Hydrosoft: Non-Holonomic Hydroelastic Models for Compliant Tactile Manipulation", "comment": null, "summary": "Tactile sensors have long been valued for their perceptual capabilities,\noffering rich insights into the otherwise hidden interface between the robot\nand grasped objects. Yet their inherent compliance -- a key driver of\nforce-rich interactions -- remains underexplored. The central challenge is to\ncapture the complex, nonlinear dynamics introduced by these passive-compliant\nelements. Here, we present a computationally efficient non-holonomic\nhydroelastic model that accurately models path-dependent contact force\ndistributions and dynamic surface area variations. Our insight is to extend the\nobject's state space, explicitly incorporating the distributed forces generated\nby the compliant sensor. Our differentiable formulation not only accounts for\npath-dependent behavior but also enables gradient-based trajectory\noptimization, seamlessly integrating with high-resolution tactile feedback. We\ndemonstrate the effectiveness of our approach across a range of simulated and\nreal-world experiments and highlight the importance of modeling the path\ndependence of sensor dynamics.", "AI": {"tldr": "本文提出了一种计算高效的非完整水弹性模型，用于精确模拟机器人触觉传感器的路径依赖接触力分布和动态表面积变化，并支持基于梯度的轨迹优化。", "motivation": "触觉传感器能提供机器人与抓取物体之间界面的丰富信息，但其固有的柔顺性（力丰富交互的关键驱动因素）带来的复杂非线性动力学，尤其是路径依赖行为，尚未得到充分探索。", "method": "研究人员通过扩展物体状态空间，明确纳入柔顺传感器产生的分布式力，提出了一种计算高效的非完整水弹性模型。该模型具有可微分特性，不仅能解释路径依赖行为，还能实现基于梯度的轨迹优化，并与高分辨率触觉反馈无缝集成。", "result": "该方法能够准确建模路径依赖的接触力分布和动态表面积变化。在模拟和真实世界实验中，该方法均表现出有效性，并强调了建模传感器动力学路径依赖性的重要性。", "conclusion": "精确建模柔顺触觉传感器动力学的路径依赖性对于捕捉机器人与物体之间复杂的交互至关重要，本文提出的模型有效解决了这一挑战，并为基于梯度的轨迹优化提供了支持。"}}
{"id": "2509.12711", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12711", "abs": "https://arxiv.org/abs/2509.12711", "authors": ["Haozhe Zhang", "Chenchen Jing", "Mingyu Liu", "Qingsheng Wang", "Hao Chen"], "title": "Learning by Imagining: Debiased Feature Augmentation for Compositional Zero-Shot Learning", "comment": null, "summary": "Compositional Zero-Shot Learning (CZSL) aims to recognize unseen\nattribute-object compositions by learning prior knowledge of seen primitives,\n\\textit{i.e.}, attributes and objects. Learning generalizable compositional\nrepresentations in CZSL remains challenging due to the entangled nature of\nattributes and objects as well as the prevalence of long-tailed distributions\nin real-world data. Inspired by neuroscientific findings that imagination and\nperception share similar neural processes, we propose a novel approach called\nDebiased Feature Augmentation (DeFA) to address these challenges. The proposed\nDeFA integrates a disentangle-and-reconstruct framework for feature\naugmentation with a debiasing strategy. DeFA explicitly leverages the prior\nknowledge of seen attributes and objects by synthesizing high-fidelity\ncomposition features to support compositional generalization. Extensive\nexperiments on three widely used datasets demonstrate that DeFA achieves\nstate-of-the-art performance in both \\textit{closed-world} and\n\\textit{open-world} settings.", "AI": {"tldr": "该论文提出了一种名为DeFA（Debiased Feature Augmentation）的新方法，通过结合解耦-重建框架和去偏策略，解决组合零样本学习（CZSL）中属性-对象纠缠和长尾分布的挑战，并在多个数据集上达到了最先进的性能。", "motivation": "组合零样本学习（CZSL）面临挑战，主要原因在于属性和对象之间相互纠缠的特性，以及现实世界数据中普遍存在的长尾分布问题。", "method": "受神经科学中想象和感知共享相似神经过程的启发，论文提出了一种名为Debiased Feature Augmentation (DeFA) 的方法。DeFA将一个用于特征增强的解耦-重建框架与一个去偏策略相结合，通过合成高保真度的组合特征来利用已知属性和对象的先验知识，以支持组合泛化。", "result": "在三个广泛使用的数据集上进行的广泛实验表明，DeFA在“封闭世界”和“开放世界”设置中均达到了最先进的性能。", "conclusion": "DeFA通过其解耦-重建和去偏策略，有效解决了CZSL中属性-对象纠缠和长尾分布的挑战，通过合成高保真组合特征实现了卓越的泛化能力，并在多个场景下取得了最先进的结果。"}}
{"id": "2509.13234", "categories": ["cs.AI", "cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.13234", "abs": "https://arxiv.org/abs/2509.13234", "authors": ["Nadim Barakat", "William Lotter"], "title": "Simulating Clinical AI Assistance using Multimodal LLMs: A Case Study in Diabetic Retinopathy", "comment": null, "summary": "Diabetic retinopathy (DR) is a leading cause of blindness worldwide, and AI\nsystems can expand access to fundus photography screening. Current FDA-cleared\nsystems primarily provide binary referral outputs, where this minimal output\nmay limit clinical trust and utility. Yet, determining the most effective\noutput format to enhance clinician-AI performance is an empirical challenge\nthat is difficult to assess at scale. We evaluated multimodal large language\nmodels (MLLMs) for DR detection and their ability to simulate clinical AI\nassistance across different output types. Two models were tested on IDRiD and\nMessidor-2: GPT-4o, a general-purpose MLLM, and MedGemma, an open-source\nmedical model. Experiments included: (1) baseline evaluation, (2) simulated AI\nassistance with synthetic predictions, and (3) actual AI-to-AI collaboration\nwhere GPT-4o incorporated MedGemma outputs. MedGemma outperformed GPT-4o at\nbaseline, achieving higher sensitivity and AUROC, while GPT-4o showed\nnear-perfect specificity but low sensitivity. Both models adjusted predictions\nbased on simulated AI inputs, but GPT-4o's performance collapsed with incorrect\nones, whereas MedGemma remained more stable. In actual collaboration, GPT-4o\nachieved strong results when guided by MedGemma's descriptive outputs, even\nwithout direct image access (AUROC up to 0.96). These findings suggest MLLMs\nmay improve DR screening pipelines and serve as scalable simulators for\nstudying clinical AI assistance across varying output configurations. Open,\nlightweight models such as MedGemma may be especially valuable in low-resource\nsettings, while descriptive outputs could enhance explainability and clinician\ntrust in clinical workflows.", "AI": {"tldr": "本研究评估了多模态大语言模型（MLLMs）在糖尿病视网膜病变（DR）检测中的应用及其模拟临床AI辅助的能力。MedGemma在基线测试中表现优于GPT-4o，且当GPT-4o结合MedGemma的描述性输出时，即使没有直接图像访问也能实现强大的检测性能。结果表明MLLMs可改进DR筛查流程，并作为研究临床AI辅助的模拟器。", "motivation": "目前的FDA批准的糖尿病视网膜病变AI系统主要提供二元转诊结果，这种极简输出可能限制临床信任和实用性。确定最有效的输出格式以增强临床医生-AI性能是一个难以大规模评估的实证挑战。", "method": "研究在IDRiD和Messidor-2数据集上测试了两个多模态大语言模型：通用型GPT-4o和开源医疗模型MedGemma。实验包括：1) 基线评估，2) 使用合成预测模拟AI辅助，以及3) GPT-4o整合MedGemma输出的实际AI-to-AI协作。", "result": "基线测试中，MedGemma在灵敏度和AUROC方面优于GPT-4o，而GPT-4o表现出接近完美的特异性但灵敏度较低。两个模型都根据模拟AI输入调整了预测，但GPT-4o在接收到不正确输入时性能崩溃，而MedGemma则更稳定。在实际协作中，当由MedGemma的描述性输出引导时，即使没有直接图像访问，GPT-4o也取得了很好的结果（AUROC高达0.96）。", "conclusion": "这些发现表明MLLMs可以改进糖尿病视网膜病变筛查流程，并作为研究不同输出配置下临床AI辅助的可扩展模拟器。像MedGemma这样的开放、轻量级模型在资源匮乏的环境中可能特别有价值，而描述性输出可以增强临床工作流程中的可解释性和临床医生信任。"}}
{"id": "2509.13313", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13313", "abs": "https://arxiv.org/abs/2509.13313", "authors": ["Xixi Wu", "Kuan Li", "Yida Zhao", "Liwen Zhang", "Litu Ou", "Huifeng Yin", "Zhongwang Zhang", "Yong Jiang", "Pengjun Xie", "Fei Huang", "Minhao Cheng", "Shuai Wang", "Hong Cheng", "Jingren Zhou"], "title": "ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization", "comment": "https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/", "summary": "Large Language Model (LLM)-based web agents demonstrate strong performance on\nknowledge-intensive tasks but are hindered by context window limitations in\nparadigms like ReAct. Complex queries involving multiple entities, intertwined\nrelationships, and high uncertainty demand extensive search cycles that rapidly\nexhaust context budgets before reaching complete solutions. To overcome this\nchallenge, we introduce ReSum, a novel paradigm that enables indefinite\nexploration through periodic context summarization. ReSum converts growing\ninteraction histories into compact reasoning states, maintaining awareness of\nprior discoveries while bypassing context constraints. For paradigm adaptation,\nwe propose ReSum-GRPO, integrating GRPO with segmented trajectory training and\nadvantage broadcasting to familiarize agents with summary-conditioned\nreasoning. Extensive experiments on web agents of varying scales across three\nbenchmarks demonstrate that ReSum delivers an average absolute improvement of\n4.5\\% over ReAct, with further gains of up to 8.2\\% following ReSum-GRPO\ntraining. Notably, with only 1K training samples, our WebResummer-30B (a\nReSum-GRPO-trained version of WebSailor-30B) achieves 33.3\\% Pass@1 on\nBrowseComp-zh and 18.3\\% on BrowseComp-en, surpassing existing open-source web\nagents.", "AI": {"tldr": "ReSum是一种新范式，通过周期性上下文总结克服了大型语言模型（LLM）网络代理的上下文窗口限制，实现了无限探索并显著提高了性能。", "motivation": "基于LLM的网络代理（如ReAct）在知识密集型任务中表现出色，但受限于上下文窗口大小，难以处理涉及多实体、复杂关系和高不确定性的复杂查询，这些查询需要大量搜索，导致上下文预算迅速耗尽。", "method": "本文提出了ReSum范式，通过周期性上下文总结实现无限探索，将不断增长的交互历史转换为紧凑的推理状态。为了适应这一范式，引入了ReSum-GRPO，它通过结合GRPO、分段轨迹训练和优势广播，使代理熟悉基于总结的推理。", "result": "在三个基准测试中，ReSum相对于ReAct平均绝对性能提升了4.5%，经过ReSum-GRPO训练后，性能进一步提升高达8.2%。值得注意的是，仅使用1K训练样本，WebResummer-30B（ReSum-GRPO训练的WebSailor-30B版本）在BrowseComp-zh上实现了33.3%的Pass@1，在BrowseComp-en上实现了18.3%的Pass@1，超越了现有开源网络代理。", "conclusion": "ReSum及其训练方法ReSum-GRPO有效解决了LLM网络代理的上下文窗口限制问题，通过周期性总结实现了无限探索，显著提升了代理在复杂网络任务上的性能，并超越了现有SOTA开源代理。"}}
{"id": "2509.13132", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13132", "abs": "https://arxiv.org/abs/2509.13132", "authors": ["Zhihao Zhang", "Chengyang Peng", "Minghao Zhu", "Ekim Yurtsever", "Keith A. Redmill"], "title": "An Uncertainty-Weighted Decision Transformer for Navigation in Dense, Complex Driving Scenarios", "comment": null, "summary": "Autonomous driving in dense, dynamic environments requires decision-making\nsystems that can exploit both spatial structure and long-horizon temporal\ndependencies while remaining robust to uncertainty. This work presents a novel\nframework that integrates multi-channel bird's-eye-view occupancy grids with\ntransformer-based sequence modeling for tactical driving in complex roundabout\nscenarios. To address the imbalance between frequent low-risk states and rare\nsafety-critical decisions, we propose the Uncertainty-Weighted Decision\nTransformer (UWDT). UWDT employs a frozen teacher transformer to estimate\nper-token predictive entropy, which is then used as a weight in the student\nmodel's loss function. This mechanism amplifies learning from uncertain,\nhigh-impact states while maintaining stability across common low-risk\ntransitions. Experiments in a roundabout simulator, across varying traffic\ndensities, show that UWDT consistently outperforms other baselines in terms of\nreward, collision rate, and behavioral stability. The results demonstrate that\nuncertainty-aware, spatial-temporal transformers can deliver safer and more\nefficient decision-making for autonomous driving in complex traffic\nenvironments.", "AI": {"tldr": "本文提出了一种不确定性加权决策Transformer (UWDT) 框架，结合鸟瞰图占用栅格和Transformer序列建模，用于复杂环岛场景下的自动驾驶决策，通过加权学习不确定性高的关键状态，显著提升了安全性和效率。", "motivation": "自动驾驶在密集动态环境中需要决策系统能够利用空间结构和长时序依赖，同时对不确定性保持鲁棒性，尤其要解决低风险常见状态与罕见安全关键决策之间的不平衡问题。", "method": "研究整合了多通道鸟瞰图占用栅格与基于Transformer的序列建模。为解决决策不平衡问题，提出了不确定性加权决策Transformer (UWDT)。UWDT使用一个冻结的教师Transformer来估计每个token的预测熵，该熵作为学生模型损失函数中的权重，从而放大对不确定性高、影响大的状态的学习，同时保持对常见低风险转换的稳定性。", "result": "在环岛模拟器中，UWDT在不同交通密度下，在奖励、碰撞率和行为稳定性方面始终优于其他基线方法。", "conclusion": "实验结果表明，不确定性感知、时空Transformer能够为复杂交通环境中的自动驾驶提供更安全、更高效的决策能力。"}}
{"id": "2509.12715", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.12715", "abs": "https://arxiv.org/abs/2509.12715", "authors": ["Heng Zhang", "Haichuan Hu", "Yaomin Shen", "Weihao Yu", "Yilei Yuan", "Haochen You", "Guo Cheng", "Zijian Zhang", "Lubin Gan", "Huihui Wei", "Hao Zhang", "Jin Huang"], "title": "AsyMoE: Leveraging Modal Asymmetry for Enhanced Expert Specialization in Large Vision-Language Models", "comment": null, "summary": "Large Vision-Language Models (LVLMs) have demonstrated impressive performance\non multimodal tasks through scaled architectures and extensive training.\nHowever, existing Mixture of Experts (MoE) approaches face challenges due to\nthe asymmetry between visual and linguistic processing. Visual information is\nspatially complete, while language requires maintaining sequential context. As\na result, MoE models struggle to balance modality-specific features and\ncross-modal interactions. Through systematic analysis, we observe that language\nexperts in deeper layers progressively lose contextual grounding and rely more\non parametric knowledge rather than utilizing the provided visual and\nlinguistic information. To address this, we propose AsyMoE, a novel\narchitecture that models this asymmetry using three specialized expert groups.\nWe design intra-modality experts for modality-specific processing, hyperbolic\ninter-modality experts for hierarchical cross-modal interactions, and\nevidence-priority language experts to suppress parametric biases and maintain\ncontextual grounding. Extensive experiments demonstrate that AsyMoE achieves\n26.58% and 15.45% accuracy improvements over vanilla MoE and modality-specific\nMoE respectively, with 25.45% fewer activated parameters than dense models.", "AI": {"tldr": "本文提出AsyMoE，一种新型架构，通过专门的专家组解决大型视觉-语言模型（LVLMs）中视觉和语言处理的不对称性，显著提升性能并减少激活参数。", "motivation": "现有MoE方法在LVLMs中面临挑战，因为视觉信息（空间完整）和语言信息（需要维护序列上下文）之间存在不对称性。这导致MoE模型难以平衡模态特定特征和跨模态交互，尤其是在深层中，语言专家会失去语境基础并过度依赖参数知识。", "method": "本文提出了AsyMoE架构，通过三种专门的专家组来建模这种不对称性：1) 模态内专家，用于模态特定处理；2) 双曲跨模态专家，用于分层跨模态交互；3) 证据优先语言专家，用于抑制参数偏差并保持语境基础。", "result": "实验表明，AsyMoE在准确性上比普通MoE提升了26.58%，比模态特定MoE提升了15.45%。同时，它比密集模型减少了25.45%的激活参数。", "conclusion": "AsyMoE通过其独特的三组专家设计，成功解决了LVLMs中视觉和语言处理的不对称性问题，显著提高了性能和效率，并有效抑制了语言专家在深层中失去语境基础的问题。"}}
{"id": "2509.13235", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13235", "abs": "https://arxiv.org/abs/2509.13235", "authors": ["Linyue Cai", "Yuyang Cheng", "Xiaoding Shao", "Huiming Wang", "Yong Zhao", "Wei Zhang", "Kang Li"], "title": "A Scenario-Driven Cognitive Approach to Next-Generation AI Memory", "comment": null, "summary": "As artificial intelligence advances toward artificial general intelligence\n(AGI), the need for robust and human-like memory systems has become\nincreasingly evident. Current memory architectures often suffer from limited\nadaptability, insufficient multimodal integration, and an inability to support\ncontinuous learning. To address these limitations, we propose a scenario-driven\nmethodology that extracts essential functional requirements from representative\ncognitive scenarios, leading to a unified set of design principles for\nnext-generation AI memory systems. Based on this approach, we introduce the\n\\textbf{COgnitive Layered Memory Architecture (COLMA)}, a novel framework that\nintegrates cognitive scenarios, memory processes, and storage mechanisms into a\ncohesive design. COLMA provides a structured foundation for developing AI\nsystems capable of lifelong learning and human-like reasoning, thereby\ncontributing to the pragmatic development of AGI.", "AI": {"tldr": "本文提出了一种名为COLMA的认知分层记忆架构，通过情景驱动的方法提取功能需求和设计原则，旨在解决当前AI记忆系统在适应性、多模态集成和持续学习方面的局限性，以促进通用人工智能（AGI）的发展。", "motivation": "随着人工智能向通用人工智能（AGI）发展，对强大且类人记忆系统的需求日益增长。当前的记忆架构存在适应性有限、多模态集成不足以及无法支持持续学习等问题。", "method": "研究采用情景驱动的方法，从具代表性的认知情景中提取核心功能需求，从而推导出一套统一的下一代AI记忆系统设计原则。基于此方法，提出了认知分层记忆架构（COLMA）。", "result": "提出了COgnitive Layered Memory Architecture (COLMA)，这是一个新颖的框架，它将认知情景、记忆过程和存储机制整合到一个内聚的设计中。", "conclusion": "COLMA为开发具备终身学习和类人推理能力的AI系统提供了结构化基础，从而有助于通用人工智能（AGI）的务实发展。"}}
{"id": "2509.13316", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13316", "abs": "https://arxiv.org/abs/2509.13316", "authors": ["Millicent Li", "Alberto Mario Ceballos Arroyo", "Giordano Rogers", "Naomi Saphra", "Byron C. Wallace"], "title": "Do Natural Language Descriptions of Model Activations Convey Privileged Information?", "comment": "34 pages, 6 figures", "summary": "Recent interpretability methods have proposed to translate LLM internal\nrepresentations into natural language descriptions using a second verbalizer\nLLM. This is intended to illuminate how the target model represents and\noperates on inputs. But do such activation verbalization approaches actually\nprovide privileged knowledge about the internal workings of the target model,\nor do they merely convey information about its inputs? We critically evaluate\npopular verbalization methods across datasets used in prior work and find that\nthey succeed at benchmarks without any access to target model internals,\nsuggesting that these datasets are not ideal for evaluating verbalization\nmethods. We then run controlled experiments which reveal that verbalizations\noften reflect the parametric knowledge of the verbalizer LLM which generated\nthem, rather than the activations of the target LLM being decoded. Taken\ntogether, our results indicate a need for targeted benchmarks and experimental\ncontrols to rigorously assess whether verbalization methods provide meaningful\ninsights into the operations of LLMs.", "AI": {"tldr": "该研究质疑了使用第二个LLM将目标LLM内部表示转换为自然语言描述的口语化解释方法。研究发现，这些方法在没有访问目标模型内部的情况下也能成功，且口语化内容常反映口语化LLM的参数知识而非目标LLM的激活。因此，研究呼吁需要更具针对性的基准和实验控制来评估这些方法的有效性。", "motivation": "现有LLM可解释性方法通过“口语化LLM”将目标LLM的内部表示转换为自然语言描述，旨在揭示目标模型如何处理输入。然而，研究者质疑这些激活口语化方法是否真正提供了关于目标模型内部运作的“特权知识”，抑或仅仅传达了关于其输入的信息。", "method": "研究首先对现有工作中使用的流行口语化方法和数据集进行了批判性评估。随后，研究进行了对照实验。", "result": "1. 发现这些口语化方法在未访问目标模型内部的情况下也能在基准测试中取得成功，这表明当前的数据集不适合评估口语化方法。\n2. 对照实验揭示，口语化内容通常反映生成它们的口语化LLM的参数知识，而非被解码的目标LLM的激活。", "conclusion": "研究结果表明，需要有针对性的基准和实验控制，才能严格评估口语化方法是否能为LLM的运作提供有意义的见解。"}}
{"id": "2509.13177", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13177", "abs": "https://arxiv.org/abs/2509.13177", "authors": ["Salvatore Esposito", "Matías Mattamala", "Daniel Rebain", "Francis Xiatian Zhang", "Kevin Dhaliwal", "Mohsen Khadem", "Subramanian Ramamoorthy"], "title": "ROOM: A Physics-Based Continuum Robot Simulator for Photorealistic Medical Datasets Generation", "comment": null, "summary": "Continuum robots are advancing bronchoscopy procedures by accessing complex\nlung airways and enabling targeted interventions. However, their development is\nlimited by the lack of realistic training and test environments: Real data is\ndifficult to collect due to ethical constraints and patient safety concerns,\nand developing autonomy algorithms requires realistic imaging and physical\nfeedback. We present ROOM (Realistic Optical Observation in Medicine), a\ncomprehensive simulation framework designed for generating photorealistic\nbronchoscopy training data. By leveraging patient CT scans, our pipeline\nrenders multi-modal sensor data including RGB images with realistic noise and\nlight specularities, metric depth maps, surface normals, optical flow and point\nclouds at medically relevant scales. We validate the data generated by ROOM in\ntwo canonical tasks for medical robotics -- multi-view pose estimation and\nmonocular depth estimation, demonstrating diverse challenges that\nstate-of-the-art methods must overcome to transfer to these medical settings.\nFurthermore, we show that the data produced by ROOM can be used to fine-tune\nexisting depth estimation models to overcome these challenges, also enabling\nother downstream applications such as navigation. We expect that ROOM will\nenable large-scale data generation across diverse patient anatomies and\nprocedural scenarios that are challenging to capture in clinical settings. Code\nand data: https://github.com/iamsalvatore/room.", "AI": {"tldr": "本文提出了ROOM，一个用于生成光真实感支气管镜检查训练数据的综合模拟框架，旨在解决连续体机器人在支气管镜检查中缺乏真实训练环境的问题，并验证了其数据在姿态估计和单目深度估计任务中的有效性。", "motivation": "连续体机器人在支气管镜检查中的发展受到限制，因为缺乏真实的训练和测试环境。收集真实数据存在伦理和患者安全问题，而开发自主算法需要真实的成像和物理反馈。", "method": "ROOM（Realistic Optical Observation in Medicine）是一个综合模拟框架。它利用患者CT扫描，生成多模态传感器数据，包括带有真实噪声和光斑的RGB图像、度量深度图、表面法线、光流和点云，所有这些都在医学相关尺度上实现。", "result": "ROOM生成的数据在多视角姿态估计和单目深度估计这两个医学机器人典型任务中得到了验证，表明了最先进方法在适应这些医疗场景时需要克服的挑战。此外，研究表明ROOM生成的数据可以用于微调现有深度估计算法以克服这些挑战，并支持导航等下游应用。", "conclusion": "ROOM有望实现跨越多样患者解剖结构和手术场景的大规模数据生成，这些场景在临床环境中难以捕捉。"}}
{"id": "2509.12718", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12718", "abs": "https://arxiv.org/abs/2509.12718", "authors": ["Pukun Zhao", "Longxiang Wang", "Miaowei Wang", "Chen Chen", "Fanqing Zhou", "Haojian Huang"], "title": "EvoEmpirBench: Dynamic Spatial Reasoning with Agent-ExpVer", "comment": "Ongoing Work, 29 pages, 3 figures, 7 tables", "summary": "Most existing spatial reasoning benchmarks focus on static or globally\nobservable environments, failing to capture the challenges of long-horizon\nreasoning and memory utilization under partial observability and dynamic\nchanges. We introduce two dynamic spatial benchmarks, locally observable maze\nnavigation and match-2 elimination that systematically evaluate models'\nabilities in spatial understanding and adaptive planning when local perception,\nenvironment feedback, and global objectives are tightly coupled. Each action\ntriggers structural changes in the environment, requiring continuous update of\ncognition and strategy. We further propose a subjective experience-based memory\nmechanism for cross-task experience transfer and validation. Experiments show\nthat our benchmarks reveal key limitations of mainstream models in dynamic\nspatial reasoning and long-term memory, providing a comprehensive platform for\nfuture methodological advances. Our code and data are available at\nhttps://anonymous.4open.science/r/EvoEmpirBench-143C/.", "AI": {"tldr": "本文引入了两个动态空间基准测试，用于评估模型在局部可观察、动态变化环境下进行空间理解和自适应规划的能力，并提出了基于主观经验的记忆机制。", "motivation": "大多数现有空间推理基准侧重于静态或全局可观察环境，未能捕捉到在部分可观察和动态变化下进行长周期推理和记忆利用的挑战。", "method": "引入了两个动态空间基准测试：局部可观察迷宫导航和配对消除（match-2 elimination），每个动作都会触发环境结构变化。此外，提出了一个基于主观经验的记忆机制，用于跨任务经验转移和验证。", "result": "实验表明，本文提出的基准测试揭示了主流模型在动态空间推理和长期记忆方面的关键局限性。", "conclusion": "这些基准测试为未来方法学进展提供了一个全面的平台。"}}
{"id": "2509.13281", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13281", "abs": "https://arxiv.org/abs/2509.13281", "authors": ["Vincent Siu", "Nathan W. Henry", "Nicholas Crispino", "Yang Liu", "Dawn Song", "Chenguang Wang"], "title": "RepIt: Representing Isolated Targets to Steer Language Models", "comment": null, "summary": "While activation steering in large language models (LLMs) is a growing area\nof research, methods can often incur broader effects than desired. This\nmotivates isolation of purer concept vectors to enable targeted interventions\nand understand LLM behavior at a more granular level. We present RepIt, a\nsimple and data-efficient framework for isolating concept-specific\nrepresentations. Across five frontier LLMs, RepIt enables precise\ninterventions: it selectively suppresses refusal on targeted concepts while\npreserving refusal elsewhere, producing models that answer WMD-related\nquestions while still scoring as safe on standard benchmarks. We further show\nthat the corrective signal localizes to just 100-200 neurons and that robust\ntarget representations can be extracted from as few as a dozen examples on a\nsingle A6000. This efficiency raises a dual concern: manipulations can be\nperformed with modest compute and data to extend to underrepresented\ndata-scarce topics while evading existing benchmarks. By disentangling refusal\nvectors with RepIt, this work demonstrates that targeted interventions can\ncounteract overgeneralization, laying the foundation for more granular control\nof model behavior.", "AI": {"tldr": "RepIt是一种简单且数据高效的框架，用于隔离大语言模型中的概念特定表示。它实现了精确干预，可以有选择地抑制模型对特定概念的拒绝，同时保持其他方面的安全性，并揭示了干预的局部性和高效性。", "motivation": "目前大语言模型中的激活操纵方法往往会产生超出预期的广泛影响。因此，需要分离出更纯粹的概念向量，以实现更具针对性的干预，并更细粒度地理解LLM行为。", "method": "本文提出了RepIt，一个简单且数据高效的框架，用于隔离概念特定的表示。", "result": "RepIt在五种前沿LLM上实现了精确干预：它能选择性地抑制模型对特定概念（如大规模杀伤性武器相关问题）的拒绝，同时保留其他方面的拒绝，使模型能回答WMD问题但仍在标准基准上保持安全评分。研究还发现，纠正信号仅局限于100-200个神经元，并且只需少量（十几个）示例和一块A6000显卡即可提取出鲁棒的目标表示。这种高效性也引发了担忧，即可能以较低的计算和数据成本进行操作，从而规避现有基准。", "conclusion": "通过RepIt解耦拒绝向量，这项工作证明了有针对性的干预可以抵消模型的过度泛化，为更细粒度地控制模型行为奠定了基础。"}}
{"id": "2509.13279", "categories": ["cs.RO", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.13279", "abs": "https://arxiv.org/abs/2509.13279", "authors": ["Sanjay Oruganti", "Sergei Nirenburg", "Marjorie McShane", "Jesse English", "Michael K. Roberts", "Christian Arndt", "Carlos Gonzalez", "Mingyo Seo", "Luis Sentis"], "title": "HARMONIC: A Content-Centric Cognitive Robotic Architecture", "comment": null, "summary": "This paper introduces HARMONIC, a cognitive-robotic architecture designed for\nrobots in human-robotic teams. HARMONIC supports semantic perception\ninterpretation, human-like decision-making, and intentional language\ncommunication. It addresses the issues of safety and quality of results; aims\nto solve problems of data scarcity, explainability, and safety; and promotes\ntransparency and trust. Two proof-of-concept HARMONIC-based robotic systems are\ndemonstrated, each implemented in both a high-fidelity simulation environment\nand on physical robotic platforms.", "AI": {"tldr": "本文介绍了一种名为HARMONIC的认知机器人架构，旨在增强人机团队中机器人的语义感知、类人决策和意图语言交流能力，以解决安全性、数据稀缺性、可解释性等问题。", "motivation": "研究动机是为了解决人机团队中机器人面临的安全和结果质量问题，以及数据稀缺性、可解释性和安全性挑战，并提升系统的透明度和人机之间的信任。", "method": "提出了HARMONIC认知机器人架构，该架构支持语义感知解释、类人决策和意图语言交流。通过在高保真仿真环境和物理机器人平台上实现并演示了两个基于HARMONIC的概念验证机器人系统。", "result": "成功演示了两个基于HARMONIC的机器人概念验证系统，证明了该架构在仿真和物理平台上的可行性。", "conclusion": "HARMONIC架构通过其语义感知、类人决策和意图语言交流能力，为解决人机团队中的安全、可解释性和信任等关键问题提供了有效的解决方案，提升了人机协作的质量。"}}
{"id": "2509.13200", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13200", "abs": "https://arxiv.org/abs/2509.13200", "authors": ["Moonyoung Lee", "Dong Ki Kim", "Jai Krishna Bandi", "Max Smith", "Aileen Liao", "Ali-akbar Agha-mohammadi", "Shayegan Omidshafiei"], "title": "StageACT: Stage-Conditioned Imitation for Robust Humanoid Door Opening", "comment": "7 pages", "summary": "Humanoid robots promise to operate in everyday human environments without\nrequiring modifications to the surroundings. Among the many skills needed,\nopening doors is essential, as doors are the most common gateways in built\nspaces and often limit where a robot can go. Door opening, however, poses\nunique challenges as it is a long-horizon task under partial observability,\nsuch as reasoning about the door's unobservable latch state that dictates\nwhether the robot should rotate the handle or push the door. This ambiguity\nmakes standard behavior cloning prone to mode collapse, yielding blended or\nout-of-sequence actions. We introduce StageACT, a stage-conditioned imitation\nlearning framework that augments low-level policies with task-stage inputs.\nThis effective addition increases robustness to partial observability, leading\nto higher success rates and shorter completion times. On a humanoid operating\nin a real-world office environment, StageACT achieves a 55% success rate on\npreviously unseen doors, more than doubling the best baseline. Moreover, our\nmethod supports intentional behavior guidance through stage prompting, enabling\nrecovery behaviors. These results highlight stage conditioning as a lightweight\nyet powerful mechanism for long-horizon humanoid loco-manipulation.", "AI": {"tldr": "StageACT是一种阶段条件模仿学习框架，通过将任务阶段输入到低级策略中，提高了人形机器人在部分可观察的长程开门任务中的鲁棒性和成功率。", "motivation": "人形机器人在人类环境中操作时，开门是一项基本但具有挑战性的技能。开门是一个长程、部分可观察的任务（例如，门闩状态不可见），这使得标准行为克隆容易出现模式崩溃，导致动作混合或乱序。", "method": "本文提出了StageACT，一个阶段条件模仿学习框架。它通过任务阶段输入来增强低级策略，以提高对部分可观察性的鲁棒性。", "result": "在真实世界办公环境中，StageACT在未曾见过的门上实现了55%的成功率，是最佳基线的两倍多。此外，该方法通过阶段提示支持意图行为引导，实现恢复行为。", "conclusion": "阶段条件化是一种轻量级但强大的机制，适用于人形机器人长程的运动-操作任务。"}}
{"id": "2509.12721", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12721", "abs": "https://arxiv.org/abs/2509.12721", "authors": ["Jingdong Zhang", "Weikai Chen", "Yuan Liu", "Jionghao Wang", "Zhengming Yu", "Zhuowen Shen", "Bo Yang", "Wenping Wang", "Xin Li"], "title": "SPGen: Spherical Projection as Consistent and Flexible Representation for Single Image 3D Shape Generation", "comment": null, "summary": "Existing single-view 3D generative models typically adopt multiview diffusion\npriors to reconstruct object surfaces, yet they remain prone to inter-view\ninconsistencies and are unable to faithfully represent complex internal\nstructure or nontrivial topologies. In particular, we encode geometry\ninformation by projecting it onto a bounding sphere and unwrapping it into a\ncompact and structural multi-layer 2D Spherical Projection (SP) representation.\nOperating solely in the image domain, SPGen offers three key advantages\nsimultaneously: (1) Consistency. The injective SP mapping encodes surface\ngeometry with a single viewpoint which naturally eliminates view inconsistency\nand ambiguity; (2) Flexibility. Multi-layer SP maps represent nested internal\nstructures and support direct lifting to watertight or open 3D surfaces; (3)\nEfficiency. The image-domain formulation allows the direct inheritance of\npowerful 2D diffusion priors and enables efficient finetuning with limited\ncomputational resources. Extensive experiments demonstrate that SPGen\nsignificantly outperforms existing baselines in geometric quality and\ncomputational efficiency.", "AI": {"tldr": "SPGen是一种单视角3D生成模型，通过将几何信息投影到2D球面投影（SP）表示，解决了现有模型在视角间不一致性和复杂内部结构表示上的问题，并提高了效率。", "motivation": "现有的单视角3D生成模型通常采用多视角扩散先验来重建物体表面，但它们容易出现视角间不一致性，并且无法忠实地表示复杂的内部结构或非平凡的拓扑结构。", "method": "该方法将几何信息投影到包围球上，并将其展开为紧凑且结构化的多层2D球面投影（SP）表示。SPGen完全在图像域中操作，利用了强大的2D扩散先验，并通过内射SP映射自然消除了视角不一致性和模糊性，同时多层SP图能够表示嵌套的内部结构并支持直接提升到水密或开放的3D表面。", "result": "SPGen在几何质量和计算效率方面显著优于现有基线模型。", "conclusion": "SPGen通过引入独特的球面投影（SP）表示，成功克服了现有单视角3D生成模型在一致性、灵活性（复杂结构表示）和效率方面的局限性，实现了卓越的性能。"}}
{"id": "2509.13288", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13288", "abs": "https://arxiv.org/abs/2509.13288", "authors": ["Marjorie McShane", "Sergei Nirenburg", "Sanjay Oruganti", "Jesse English"], "title": "Shapes of Cognition for Computational Cognitive Modeling", "comment": null, "summary": "Shapes of cognition is a new conceptual paradigm for the computational\ncognitive modeling of Language-Endowed Intelligent Agents (LEIAs). Shapes are\nremembered constellations of sensory, linguistic, conceptual, episodic, and\nprocedural knowledge that allow agents to cut through the complexity of real\nlife the same way as people do: by expecting things to be typical, recognizing\npatterns, acting by habit, reasoning by analogy, satisficing, and generally\nminimizing cognitive load to the degree situations permit. Atypical outcomes\nare treated using shapes-based recovery methods, such as learning on the fly,\nasking a human partner for help, or seeking an actionable, even if imperfect,\nsituational understanding. Although shapes is an umbrella term, it is not\nvague: shapes-based modeling involves particular objectives, hypotheses,\nmodeling strategies, knowledge bases, and actual models of wide-ranging\nphenomena, all implemented within a particular cognitive architecture. Such\nspecificity is needed both to vet our hypotheses and to achieve our practical\naims of building useful agent systems that are explainable, extensible, and\nworthy of our trust, even in critical domains. However, although the LEIA\nexample of shapes-based modeling is specific, the principles can be applied\nmore broadly, giving new life to knowledge-based and hybrid AI.", "AI": {"tldr": "“认知形状”是一种新的计算认知建模范式，用于语言赋能智能体（LEIAs），通过记忆各种知识模式来模拟人类处理复杂世界、最小化认知负荷并应对异常情况的方式。", "motivation": "研究动机在于为语言赋能智能体（LEIAs）提供一种新的计算认知建模范式，使其能够像人类一样处理现实世界的复杂性，通过预期典型情况、识别模式、习惯性行动、类比推理等方式最小化认知负荷。同时，旨在构建可解释、可扩展且值得信任的智能体系统。", "method": "该方法的核心是“形状”，即感官、语言、概念、情景和程序性知识的记忆星座。智能体利用这些形状来预期典型情况、识别模式、习惯性行动、类比推理、满意化以及最小化认知负荷。对于异常结果，采用基于形状的恢复方法，如即时学习、寻求人类帮助或寻求可操作的理解。该建模涉及特定的目标、假设、建模策略、知识库和认知架构。", "result": "“认知形状”范式并非模糊概念，它通过具体的建模策略和认知架构，成功应用于构建可解释、可扩展且值得信任的有用智能体系统（如LEIAs）。此外，该原则具有广泛适用性，能够为基于知识和混合人工智能注入新的活力。", "conclusion": "“认知形状”为计算认知建模，特别是语言赋能智能体（LEIAs），提供了一种具体且可广泛应用的范式。它通过模拟人类处理复杂性的方式，提升了智能体系统的可解释性、可扩展性和可信赖性，并有望振兴知识型和混合人工智能。"}}
{"id": "2509.13239", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13239", "abs": "https://arxiv.org/abs/2509.13239", "authors": ["Tianxu An", "Flavio De Vincenti", "Yuntao Ma", "Marco Hutter", "Stelian Coros"], "title": "Collaborative Loco-Manipulation for Pick-and-Place Tasks with Dynamic Reward Curriculum", "comment": null, "summary": "We present a hierarchical RL pipeline for training one-armed legged robots to\nperform pick-and-place (P&P) tasks end-to-end -- from approaching the payload\nto releasing it at a target area -- in both single-robot and cooperative\ndual-robot settings. We introduce a novel dynamic reward curriculum that\nenables a single policy to efficiently learn long-horizon P&P operations by\nprogressively guiding the agents through payload-centered sub-objectives.\nCompared to state-of-the-art approaches for long-horizon RL tasks, our method\nimproves training efficiency by 55% and reduces execution time by 18.6% in\nsimulation experiments. In the dual-robot case, we show that our policy enables\neach robot to attend to different components of its observation space at\ndistinct task stages, promoting effective coordination via autonomous attention\nshifts. We validate our method through real-world experiments using ANYmal D\nplatforms in both single- and dual-robot scenarios. To our knowledge, this is\nthe first RL pipeline that tackles the full scope of collaborative P&P with two\nlegged manipulators.", "AI": {"tldr": "本文提出了一种分层强化学习（RL）流程，用于训练单臂腿足机器人执行端到端的抓取-放置（P&P）任务，包括单机器人和协作双机器人场景，并通过动态奖励课程显著提高了训练效率和执行速度。", "motivation": "训练腿足机器人执行从接近物体到释放的端到端、长周期抓取-放置任务，尤其是在协作双机器人设置下，是一个复杂且具有挑战性的问题。", "method": "本文引入了一个分层强化学习（RL）流程，并提出了一种新颖的动态奖励课程。该课程通过逐步引导智能体完成以载荷为中心的子目标，使单一策略能够高效学习长周期的P&P操作。在双机器人情况下，策略使每个机器人在不同任务阶段关注其观察空间的不同部分，通过自主注意力转移促进有效协调。该方法通过ANYmal D平台进行了仿真和真实世界验证。", "result": "在仿真实验中，与现有长周期RL方法相比，本文方法将训练效率提高了55%，执行时间缩短了18.6%。在双机器人场景中，策略通过自主注意力转移实现了有效协调。据作者所知，这是第一个解决两个腿足机械臂协作P&P全范围的RL流程。该方法已通过ANYmal D平台在单机器人和双机器人场景下进行了真实世界实验验证。", "conclusion": "本文提出的分层RL流程和动态奖励课程成功地使单臂腿足机器人在单机器人和协作双机器人设置下，高效地执行了端到端的长周期抓取-放置任务，并在训练效率和执行速度上取得了显著提升，为协作腿足机械臂P&P任务提供了有效的解决方案。"}}
{"id": "2509.12724", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12724", "abs": "https://arxiv.org/abs/2509.12724", "authors": ["Yunhan Zhao", "Xiang Zheng", "Xingjun Ma"], "title": "Defense-to-Attack: Bypassing Weak Defenses Enables Stronger Jailbreaks in Vision-Language Models", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Despite their superb capabilities, Vision-Language Models (VLMs) have been\nshown to be vulnerable to jailbreak attacks. While recent jailbreaks have\nachieved notable progress, their effectiveness and efficiency can still be\nimproved. In this work, we reveal an interesting phenomenon: incorporating weak\ndefense into the attack pipeline can significantly enhance both the\neffectiveness and the efficiency of jailbreaks on VLMs. Building on this\ninsight, we propose Defense2Attack, a novel jailbreak method that bypasses the\nsafety guardrails of VLMs by leveraging defensive patterns to guide jailbreak\nprompt design. Specifically, Defense2Attack consists of three key components:\n(1) a visual optimizer that embeds universal adversarial perturbations with\naffirmative and encouraging semantics; (2) a textual optimizer that refines the\ninput using a defense-styled prompt; and (3) a red-team suffix generator that\nenhances the jailbreak through reinforcement fine-tuning. We empirically\nevaluate our method on four VLMs and four safety benchmarks. The results\ndemonstrate that Defense2Attack achieves superior jailbreak performance in a\nsingle attempt, outperforming state-of-the-art attack methods that often\nrequire multiple tries. Our work offers a new perspective on jailbreaking VLMs.", "AI": {"tldr": "本文提出Defense2Attack，一种新颖的越狱方法，通过将弱防御模式融入攻击流程来显著提高对视觉语言模型（VLMs）的越狱效率和效果。", "motivation": "尽管现有VLM越狱方法取得了进展，但其有效性和效率仍有提升空间。研究发现，将弱防御整合到攻击流程中可以显著增强VLM越狱的效果和效率。", "method": "Defense2Attack方法包含三个核心组件：1) 一个视觉优化器，嵌入带有肯定和鼓励语义的通用对抗性扰动；2) 一个文本优化器，使用防御风格的提示来精炼输入；3) 一个红队后缀生成器，通过强化微调增强越狱效果。", "result": "在四个VLM和四个安全基准上的评估表明，Defense2Attack在单次尝试中实现了卓越的越狱性能，优于通常需要多次尝试的现有最先进攻击方法。", "conclusion": "该工作通过利用防御模式为VLM越狱提供了一个新颖的视角。"}}
{"id": "2508.12176", "categories": ["cs.CV", "cs.AI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.12176", "abs": "https://arxiv.org/abs/2508.12176", "authors": ["Zhiwei Zheng", "Dongyin Hu", "Mingmin Zhao"], "title": "Scalable RF Simulation in Generative 4D Worlds", "comment": null, "summary": "Radio Frequency (RF) sensing has emerged as a powerful, privacy-preserving\nalternative to vision-based methods for indoor perception tasks. However,\ncollecting high-quality RF data in dynamic and diverse indoor environments\nremains a major challenge. To address this, we introduce WaveVerse, a\nprompt-based, scalable framework that simulates realistic RF signals from\ngenerated indoor scenes with human motions. WaveVerse introduces a\nlanguage-guided 4D world generator, which includes a state-aware causal\ntransformer for human motion generation conditioned on spatial constraints and\ntexts, and a phase-coherent ray tracing simulator that enables the simulation\nof accurate and coherent RF signals. Experiments demonstrate the effectiveness\nof our approach in conditioned human motion generation and highlight how phase\ncoherence is applied to beamforming and respiration monitoring. We further\npresent two case studies in ML-based high-resolution imaging and human activity\nrecognition, demonstrating that WaveVerse not only enables data generation for\nRF imaging for the first time, but also consistently achieves performance gain\nin both data-limited and data-adequate scenarios.", "AI": {"tldr": "WaveVerse是一个基于提示的、可扩展的框架，通过语言引导的4D世界生成器和相位相干射线追踪模拟器，从生成的人体运动室内场景中模拟逼真的射频（RF）信号，解决了RF传感数据收集的挑战，并首次实现了RF成像数据生成，显著提升了数据有限和充足场景下的性能。", "motivation": "射频（RF）传感作为室内感知任务中保护隐私的替代方案日益重要，但动态多样的室内环境中高质量RF数据收集仍是一大挑战。", "method": "WaveVerse框架通过以下方式模拟RF信号：1) 引入一个语言引导的4D世界生成器，其中包含一个状态感知因果变换器，用于基于空间约束和文本生成人体运动；2) 使用一个相位相干的射线追踪模拟器，生成准确且相干的RF信号。", "result": "实验证明了WaveVerse在条件人体运动生成方面的有效性，并展示了相位相干性在波束成形和呼吸监测中的应用。此外，在ML高分辨率成像和人体活动识别的案例研究中，WaveVerse不仅首次实现了RF成像数据生成，还在数据有限和数据充足的情况下持续获得性能提升。", "conclusion": "WaveVerse成功解决了RF传感数据收集的难题，通过生成逼真的RF信号，首次实现了RF成像的数据生成，并显著提升了人体活动识别等任务的性能，为RF感知应用提供了强大的数据生成能力。"}}
{"id": "2509.13249", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13249", "abs": "https://arxiv.org/abs/2509.13249", "authors": ["Ye Li", "Daming Liu", "Yanhe Zhu", "Junming Zhang", "Yongsheng Luo", "Ziqi Wang", "Chenyu Liu", "Jie Zhao"], "title": "Design and Control of a Perching Drone Inspired by the Prey-Capturing Mechanism of Venus Flytrap", "comment": null, "summary": "The endurance and energy efficiency of drones remain critical challenges in\ntheir design and operation. To extend mission duration, numerous studies\nexplored perching mechanisms that enable drones to conserve energy by\ntemporarily suspending flight. This paper presents a new perching drone that\nutilizes an active flexible perching mechanism inspired by the rapid predation\nmechanism of the Venus flytrap, achieving perching in less than 100 ms. The\nproposed system is designed for high-speed adaptability to the perching\ntargets. The overall drone design is outlined, followed by the development and\nvalidation of the biomimetic perching structure. To enhance the system\nstability, a cascade extended high-gain observer (EHGO) based control method is\ndeveloped, which can estimate and compensate for the external disturbance in\nreal time. The experimental results demonstrate the adaptability of the\nperching structure and the superiority of the cascaded EHGO in resisting wind\nand perching disturbances.", "AI": {"tldr": "本文提出了一种受捕蝇草启发的新型快速仿生栖息无人机，结合级联扩展高增益观测器（EHGO）控制方法，以提高无人机的续航能力、栖息速度和抗干扰稳定性。", "motivation": "无人机的续航能力和能源效率是其设计和操作中的关键挑战。通过栖息机制暂时停止飞行以节省能量，是延长任务时间的一种有效方法。", "method": "该研究开发了一种新型栖息无人机，其主动柔性栖息机制灵感来源于捕蝇草的快速捕食机制，可在100毫秒内完成栖息。系统设计旨在实现对栖息目标的高速适应性。为增强系统稳定性，开发了一种基于级联扩展高增益观测器（EHGO）的控制方法，能够实时估计和补偿外部干扰。", "result": "实验结果表明，该栖息结构具有良好的适应性，能在100毫秒内完成栖息。同时，级联EHGO在抵抗风扰和栖息干扰方面表现出卓越的性能，有效提升了系统稳定性。", "conclusion": "该研究成功开发了一种具有快速仿生栖息机制和EHGO控制的无人机系统，有效解决了无人机续航和能量效率的关键挑战，并通过实验验证了其栖息结构适应性和控制方法的抗干扰优越性。"}}
{"id": "2509.12742", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12742", "abs": "https://arxiv.org/abs/2509.12742", "authors": ["Jiateng Liu", "Hao Gao", "Jiu-Cheng Xie", "Chi-Man Pun", "Jian Xiong", "Haolun Li", "Feng Xu"], "title": "Effective Gaussian Management for High-fidelity Object Reconstruction", "comment": null, "summary": "This paper proposes an effective Gaussian management approach for\nhigh-fidelity object reconstruction. Departing from recent Gaussian Splatting\n(GS) methods that employ indiscriminate attribute assignment, our approach\nintroduces a novel densification strategy that dynamically activates spherical\nharmonics (SHs) or normals under the supervision of a surface reconstruction\nmodule, which effectively mitigates the gradient conflicts caused by dual\nsupervision and achieves superior reconstruction results. To further improve\nrepresentation efficiency, we develop a lightweight Gaussian representation\nthat adaptively adjusts the SH orders of each Gaussian based on gradient\nmagnitudes and performs task-decoupled pruning to remove Gaussian with minimal\nimpact on a reconstruction task without sacrificing others, which balances the\nrepresentational capacity with parameter quantity. Notably, our management\napproach is model-agnostic and can be seamlessly integrated into other\nframeworks, enhancing performance while reducing model size. Extensive\nexperiments demonstrate that our approach consistently outperforms\nstate-of-the-art approaches in both reconstruction quality and efficiency,\nachieving superior performance with significantly fewer parameters.", "AI": {"tldr": "本文提出了一种高效的高斯管理方法，用于高保真物体重建。该方法通过动态激活球谐函数或法线的新型稠密化策略，以及自适应调整球谐阶数和任务解耦剪枝的轻量级高斯表示，解决了现有高斯泼溅方法的局限性，实现了卓越的重建质量和效率，并显著减少了参数。", "motivation": "现有高斯泼溅（GS）方法存在属性分配不加区分和双重监督导致的梯度冲突问题，同时在表示效率方面仍有提升空间，需要更有效地平衡表示能力与参数数量。", "method": "本文提出以下方法：1. 新型稠密化策略：在表面重建模块的监督下动态激活球谐函数（SHs）或法线，以缓解梯度冲突。2. 轻量级高斯表示：根据梯度大小自适应调整每个高斯点的球谐阶数。3. 任务解耦剪枝：移除对重建任务影响最小的高斯点，同时不牺牲其他任务。该方法是模型无关的，可无缝集成到其他框架中。", "result": "实验证明，该方法在重建质量和效率方面均持续优于最先进的方法，以显著更少的参数实现了卓越的性能。", "conclusion": "本文提出的高斯管理方法通过创新的稠密化策略和轻量级高斯表示，有效解决了高保真物体重建中的挑战，显著提升了重建质量和效率，同时减少了模型大小，并具有良好的通用性。"}}
{"id": "2509.13089", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.13089", "abs": "https://arxiv.org/abs/2509.13089", "authors": ["Jonas Werheid", "Shengjie He", "Aymen Gannouni", "Anas Abdelrazeq", "Robert H. Schmitt"], "title": "A Synthetic Data Pipeline for Supporting Manufacturing SMEs in Visual Assembly Control", "comment": null, "summary": "Quality control of assembly processes is essential in manufacturing to ensure\nnot only the quality of individual components but also their proper integration\ninto the final product. To assist in this matter, automated assembly control\nusing computer vision methods has been widely implemented. However, the costs\nassociated with image acquisition, annotation, and training of computer vision\nalgorithms pose challenges for integration, especially for small- and\nmedium-sized enterprises (SMEs), which often lack the resources for extensive\ntraining, data collection, and manual image annotation. Synthetic data offers\nthe potential to reduce manual data collection and labeling. Nevertheless, its\npractical application in the context of assembly quality remains limited. In\nthis work, we present a novel approach for easily integrable and data-efficient\nvisual assembly control. Our approach leverages simulated scene generation\nbased on computer-aided design (CAD) data and object detection algorithms. The\nresults demonstrate a time-saving pipeline for generating image data in\nmanufacturing environments, achieving a mean Average Precision (mAP@0.5:0.95)\nup to 99,5% for correctly identifying instances of synthetic planetary gear\nsystem components within our simulated training data, and up to 93% when\ntransferred to real-world camera-captured testing data. This research\nhighlights the effectiveness of synthetic data generation within an adaptable\npipeline and underscores its potential to support SMEs in implementing\nresource-efficient visual assembly control solutions.", "AI": {"tldr": "本文提出一种基于CAD数据和目标检测算法的合成数据生成方法，用于实现易于集成且数据高效的视觉装配质量控制，尤其适用于资源有限的中小型企业。", "motivation": "自动化视觉装配控制在制造业中至关重要，但图像采集、标注和算法训练成本高昂，对中小型企业（SMEs）构成挑战。合成数据有潜力降低这些成本，但在装配质量控制中的实际应用仍有限。", "method": "该方法利用基于计算机辅助设计（CAD）数据的模拟场景生成技术，结合目标检测算法，构建了一个省时的数据生成管道，用于视觉装配控制。", "result": "该管道在生成制造环境图像数据方面实现了省时，在模拟训练数据中正确识别合成行星齿轮系统组件实例的平均精度（mAP@0.5:0.95）高达99.5%，转移到真实相机捕获的测试数据时达到93%。", "conclusion": "研究表明，在可适应的管道中生成合成数据是有效的，并强调了其在支持中小型企业实施资源高效的视觉装配控制解决方案方面的潜力。"}}
{"id": "2509.12746", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12746", "abs": "https://arxiv.org/abs/2509.12746", "authors": ["Tony Lindeberg", "Zahra Babaiee", "Peyman M. Kiasari"], "title": "Modelling and analysis of the 8 filters from the \"master key filters hypothesis\" for depthwise-separable deep networks in relation to idealized receptive fields based on scale-space theory", "comment": "24 pages, 11 figures, 17 tables", "summary": "This paper presents the results of analysing and modelling a set of 8\n``master key filters'', which have been extracted by applying a clustering\napproach to the receptive fields learned in depthwise-separable deep networks\nbased on the ConvNeXt architecture.\n  For this purpose, we first compute spatial spread measures in terms of\nweighted mean values and weighted variances of the absolute values of the\nlearned filters, which support the working hypotheses that: (i) the learned\nfilters can be modelled by separable filtering operations over the spatial\ndomain, and that (ii) the spatial offsets of the those learned filters that are\nnon-centered are rather close to half a grid unit. Then, we model the clustered\n``master key filters'' in terms of difference operators applied to a spatial\nsmoothing operation in terms of the discrete analogue of the Gaussian kernel,\nand demonstrate that the resulting idealized models of the receptive fields\nshow good qualitative similarity to the learned filters.\n  This modelling is performed in two different ways: (i) using possibly\ndifferent values of the scale parameters in the coordinate directions for each\nfilter, and (ii) using the same value of the scale parameter in both coordinate\ndirections. Then, we perform the actual model fitting by either (i) requiring\nspatial spread measures in terms of spatial variances of the absolute values of\nthe receptive fields to be equal, or (ii) minimizing the discrete $l_1$- or\n$l_2$-norms between the idealized receptive field models and the learned\nfilters.\n  Complementary experimental results then demonstrate the idealized models of\nreceptive fields have good predictive properties for replacing the learned\nfilters by idealized filters in depthwise-separable deep networks, thus showing\nthat the learned filters in depthwise-separable deep networks can be well\napproximated by discrete scale-space filters.", "AI": {"tldr": "本文分析并建模了从基于ConvNeXt的深度可分离网络中提取的“主键滤波器”，发现它们可以用基于高斯核的差分算子形式的离散尺度空间滤波器进行良好近似和替换。", "motivation": "研究的动机是为了理解和建模深度可分离深度网络（特别是ConvNeXt架构）中学习到的感受野，并验证这些滤波器是否可以被空间可分离的操作以及基于简单数学模型（如高斯核的差分算子）的理想化滤波器所近似。", "method": "首先，通过聚类从ConvNeXt中提取了8个“主键滤波器”。接着，计算了这些滤波器的空间传播度量（加权均值和方差），以支持滤波器可分离性和非中心滤波器偏移量接近半格单位的假设。然后，将聚类的“主键滤波器”建模为应用于离散高斯核的空间平滑操作的差分算子。建模通过两种方式进行：(i) 对每个滤波器在坐标方向上使用可能不同的尺度参数，或(ii) 使用相同的尺度参数。模型拟合通过两种方法实现：(i) 要求感受野绝对值的空间方差相等，或(ii) 最小化理想化模型与学习滤波器之间的离散$l_1$-或$l_2$-范数。最后，通过用理想化滤波器替换学习滤波器来评估其预测性能。", "result": "研究结果表明，学习到的滤波器可以由空间域上的可分离滤波操作建模，并且非中心滤波器的空间偏移量接近半格单位。理想化的感受野模型与学习到的滤波器表现出良好的定性相似性。此外，理想化的感受野模型在深度可分离深度网络中替换学习到的滤波器时，表现出良好的预测特性，证明学习到的滤波器可以很好地被离散尺度空间滤波器近似。", "conclusion": "深度可分离深度网络中学习到的滤波器（如ConvNeXt）可以被基于高斯核的差分算子形式的理想化离散尺度空间滤波器有效地近似和替换，这表明这些学习到的滤波器具有内在的结构化特性。"}}
{"id": "2509.12750", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12750", "abs": "https://arxiv.org/abs/2509.12750", "authors": ["Rishab Parthasarathy", "Jasmine Collins", "Cory Stephenson"], "title": "What Makes a Good Generated Image? Investigating Human and Multimodal LLM Image Preference Alignment", "comment": "7 pages, 9 figures, 3 tables; appendix 16 pages, 9 figures, 6 tables", "summary": "Automated evaluation of generative text-to-image models remains a challenging\nproblem. Recent works have proposed using multimodal LLMs to judge the quality\nof images, but these works offer little insight into how multimodal LLMs make\nuse of concepts relevant to humans, such as image style or composition, to\ngenerate their overall assessment. In this work, we study what attributes of an\nimage--specifically aesthetics, lack of artifacts, anatomical accuracy,\ncompositional correctness, object adherence, and style--are important for both\nLLMs and humans to make judgments on image quality. We first curate a dataset\nof human preferences using synthetically generated image pairs. We use\ninter-task correlation between each pair of image quality attributes to\nunderstand which attributes are related in making human judgments. Repeating\nthe same analysis with LLMs, we find that the relationships between image\nquality attributes are much weaker. Finally, we study individual image quality\nattributes by generating synthetic datasets with a high degree of control for\neach axis. Humans are able to easily judge the quality of an image with respect\nto all of the specific image quality attributes (e.g. high vs. low aesthetic\nimage), however we find that some attributes, such as anatomical accuracy, are\nmuch more difficult for multimodal LLMs to learn to judge. Taken together,\nthese findings reveal interesting differences between how humans and multimodal\nLLMs perceive images.", "AI": {"tldr": "本研究探讨了多模态大语言模型（MLLMs）和人类在评估生成图像质量时，对图像美学、构图等属性的感知差异，发现两者在属性关联性和某些特定属性的判断能力上存在显著不同。", "motivation": "生成式文本到图像模型的自动化评估是一个挑战，现有研究虽提出使用多模态大语言模型（MLLMs）评估图像质量，但对MLLMs如何利用人类相关的概念（如图像风格或构图）进行整体评估缺乏深入理解。", "method": "首先，通过合成图像对收集人类偏好数据集，并使用任务间相关性分析来理解人类在判断图像质量时各属性之间的关系。然后，对MLLMs重复相同的分析。最后，通过生成高度可控的合成数据集，研究人类和MLLMs对各个特定图像质量属性（如美学、无伪影、解剖准确性、构图正确性、对象依从性和风格）的判断能力。", "result": "研究发现，与人类相比，MLLMs在判断图像质量属性时的属性间关系要弱得多。此外，人类能够轻松判断所有特定图像质量属性，但MLLMs在判断某些属性（如解剖准确性）时面临更大困难。", "conclusion": "这些发现揭示了人类和多模态大语言模型在感知和评估图像时存在的有趣差异，表明MLLMs在理解和判断某些人类关注的图像质量属性方面仍有不足。"}}
{"id": "2509.12757", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12757", "abs": "https://arxiv.org/abs/2509.12757", "authors": ["Xiaohan Zhang", "Si-Yuan Cao", "Xiaokai Bai", "Yiming Li", "Zhangkai Shen", "Zhe Wu", "Xiaoxi Hu", "Hui-liang Shen"], "title": "Recurrent Cross-View Object Geo-Localization", "comment": null, "summary": "Cross-view object geo-localization (CVOGL) aims to determine the location of\na specific object in high-resolution satellite imagery given a query image with\na point prompt. Existing approaches treat CVOGL as a one-shot detection task,\ndirectly regressing object locations from cross-view information aggregation,\nbut they are vulnerable to feature noise and lack mechanisms for error\ncorrection. In this paper, we propose ReCOT, a Recurrent Cross-view Object\ngeo-localization Transformer, which reformulates CVOGL as a recurrent\nlocalization task. ReCOT introduces a set of learnable tokens that encode\ntask-specific intent from the query image and prompt embeddings, and\niteratively attend to the reference features to refine the predicted location.\nTo enhance this recurrent process, we incorporate two complementary modules:\n(1) a SAM-based knowledge distillation strategy that transfers segmentation\npriors from the Segment Anything Model (SAM) to provide clearer semantic\nguidance without additional inference cost, and (2) a Reference Feature\nEnhancement Module (RFEM) that introduces a hierarchical attention to emphasize\nobject-relevant regions in the reference features. Extensive experiments on\nstandard CVOGL benchmarks demonstrate that ReCOT achieves state-of-the-art\n(SOTA) performance while reducing parameters by 60% compared to previous SOTA\napproaches.", "AI": {"tldr": "ReCOT将跨视图目标地理定位(CVOGL)重构为循环定位任务，通过可学习的token、迭代优化以及SAM知识蒸馏和参考特征增强模块，实现了SOTA性能并显著减少了参数。", "motivation": "现有的CVOGL方法将任务视为一次性检测，直接回归目标位置，但它们容易受到特征噪声的影响，并且缺乏错误纠正机制。", "method": "本文提出了ReCOT（Recurrent Cross-view Object geo-localization Transformer），将CVOGL重构为循环定位任务。ReCOT引入了一组可学习的token，编码查询图像和提示嵌入中的任务特定意图，并迭代地关注参考特征以细化预测位置。为增强循环过程，引入了两个补充模块：1) 基于SAM的知识蒸馏策略，转移分割先验以提供更清晰的语义指导；2) 参考特征增强模块（RFEM），引入分层注意力以强调参考特征中与目标相关的区域。", "result": "在标准CVOGL基准测试上，ReCOT实现了最先进（SOTA）的性能，同时与之前的SOTA方法相比，参数减少了60%。", "conclusion": "ReCOT通过将CVOGL重新定义为循环定位任务，并结合创新的知识蒸馏和特征增强机制，有效克服了现有方法的局限性，实现了卓越的定位精度和参数效率。"}}
{"id": "2509.12777", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12777", "abs": "https://arxiv.org/abs/2509.12777", "authors": ["Zhifang Gong", "Shuo Gao", "Ben Zhao", "Yingjing Xu", "Yijun Yang", "Shenghong Ju", "Guangquan Zhou"], "title": "CECT-Mamba: a Hierarchical Contrast-enhanced-aware Model for Pancreatic Tumor Subtyping from Multi-phase CECT", "comment": null, "summary": "Contrast-enhanced computed tomography (CECT) is the primary imaging technique\nthat provides valuable spatial-temporal information about lesions, enabling the\naccurate diagnosis and subclassification of pancreatic tumors. However, the\nhigh heterogeneity and variability of pancreatic tumors still pose substantial\nchallenges for precise subtyping diagnosis. Previous methods fail to\neffectively explore the contextual information across multiple CECT phases\ncommonly used in radiologists' diagnostic workflows, thereby limiting their\nperformance. In this paper, we introduce, for the first time, an automatic way\nto combine the multi-phase CECT data to discriminate between pancreatic tumor\nsubtypes, among which the key is using Mamba with promising learnability and\nsimplicity to encourage both temporal and spatial modeling from multi-phase\nCECT. Specifically, we propose a dual hierarchical contrast-enhanced-aware\nMamba module incorporating two novel spatial and temporal sampling sequences to\nexplore intra and inter-phase contrast variations of lesions. A\nsimilarity-guided refinement module is also imposed into the temporal scanning\nmodeling to emphasize the learning on local tumor regions with more obvious\ntemporal variations. Moreover, we design the space complementary integrator and\nmulti-granularity fusion module to encode and aggregate the semantics across\ndifferent scales, achieving more efficient learning for subtyping pancreatic\ntumors. The experimental results on an in-house dataset of 270 clinical cases\nachieve an accuracy of 97.4% and an AUC of 98.6% in distinguishing between\npancreatic ductal adenocarcinoma (PDAC) and pancreatic neuroendocrine tumors\n(PNETs), demonstrating its potential as a more accurate and efficient tool.", "AI": {"tldr": "本文首次提出一种基于Mamba模型的自动化方法，结合多期增强CT（CECT）数据，用于精确区分胰腺肿瘤亚型，尤其擅长建模多期CECT中的时空信息。", "motivation": "胰腺肿瘤的高度异质性和变异性给精确亚型诊断带来了巨大挑战。以往方法未能有效利用放射科医生诊断流程中常用的多期CECT图像的上下文信息，从而限制了其性能。", "method": "该研究引入Mamba模型来整合多期CECT数据，以实现胰腺肿瘤亚型鉴别，利用其学习能力和简洁性进行时空建模。具体而言，提出了一种双层分级对比增强感知Mamba模块，结合两种新颖的时空采样序列来探索病灶的期内和期间对比度变化。此外，在时间扫描建模中引入了相似性引导细化模块，以强调对具有明显时间变化的局部肿瘤区域的学习。还设计了空间互补整合器和多粒度融合模块，用于编码和聚合不同尺度的语义信息。", "result": "在包含270个临床病例的内部数据集上进行实验，该方法在区分胰腺导管腺癌（PDAC）和胰腺神经内分泌肿瘤（PNETs）方面达到了97.4%的准确率和98.6%的AUC。", "conclusion": "该研究提出的方法在胰腺肿瘤亚型诊断中表现出更高的准确性和效率，有望成为一种有前景的诊断工具。"}}
{"id": "2509.12759", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12759", "abs": "https://arxiv.org/abs/2509.12759", "authors": ["Yiwei Xu", "Xiang Wang", "Yifei Yu", "Wentian Gan", "Luca Morelli", "Giulio Perda", "Xiongwu Xiao", "Zongqian Zhan", "Xin Wang", "Fabio Remondino"], "title": "A-TDOM: Active TDOM via On-the-Fly 3DGS", "comment": null, "summary": "True Digital Orthophoto Map (TDOM) serves as a crucial geospatial product in\nvarious fields such as urban management, city planning, land surveying, etc.\nHowever, traditional TDOM generation methods generally rely on a complex\noffline photogrammetric pipeline, resulting in delays that hinder real-time\napplications. Moreover, the quality of TDOM may degrade due to various\nchallenges, such as inaccurate camera poses or Digital Surface Model (DSM) and\nscene occlusions. To address these challenges, this work introduces A-TDOM, a\nnear real-time TDOM generation method based on On-the-Fly 3DGS optimization. As\neach image is acquired, its pose and sparse point cloud are computed via\nOn-the-Fly SfM. Then new Gaussians are integrated and optimized into previously\nunseen or coarsely reconstructed regions. By integrating with orthogonal\nsplatting, A-TDOM can render just after each update of a new 3DGS field.\nInitial experiments on multiple benchmarks show that the proposed A-TDOM is\ncapable of actively rendering TDOM in near real-time, with 3DGS optimization\nfor each new image in seconds while maintaining acceptable rendering quality\nand TDOM geometric accuracy.", "AI": {"tldr": "本文提出A-TDOM，一种基于实时三维高斯溅射（3DGS）优化的近实时真数字正射影像图（TDOM）生成方法，解决了传统方法耗时且质量受损的问题。", "motivation": "真数字正射影像图（TDOM）在城市管理、规划、土地测量等领域至关重要，但传统生成方法依赖复杂的离线摄影测量流程，导致延迟，无法支持实时应用。此外，不准确的相机姿态、数字表面模型（DSM）或场景遮挡可能导致TDOM质量下降。", "method": "A-TDOM方法基于实时三维高斯溅射（3DGS）优化。每当获取新图像时，通过实时运动恢复结构（SfM）计算其姿态和稀疏点云。然后，将新的高斯函数集成并优化到先前未见或粗略重建的区域中。通过与正交溅射相结合，A-TDOM可以在每次新的3DGS场更新后立即进行渲染。", "result": "在多个基准测试上的初步实验表明，所提出的A-TDOM能够以近实时方式主动渲染TDOM，每张新图像的3DGS优化仅需数秒，同时保持可接受的渲染质量和TDOM的几何精度。", "conclusion": "A-TDOM成功实现了近实时TDOM生成，解决了传统方法的效率和质量挑战，为实时地理空间应用提供了高精度和高效率的解决方案。"}}
{"id": "2509.12818", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12818", "abs": "https://arxiv.org/abs/2509.12818", "authors": ["Maximilian Ilse", "Harshita Sharma", "Anton Schwaighofer", "Sam Bond-Taylor", "Fernando Pérez-García", "Olesya Melnichenko", "Anne-Marie G. Sykes", "Kelly K. Horst", "Ashish Khandelwal", "Maxwell Reynolds", "Maria T. Wetscherek", "Noel C. F. Codella", "Javier Alvarez-Valle", "Korfiatis Panagiotis", "Valentina Salvatelli"], "title": "Data Scaling Laws for Radiology Foundation Models", "comment": null, "summary": "Foundation vision encoders such as CLIP and DINOv2, trained on web-scale\ndata, exhibit strong transfer performance across tasks and datasets. However,\nmedical imaging foundation models remain constrained by smaller datasets,\nlimiting our understanding of how data scale and pretraining paradigms affect\nperformance in this setting. In this work, we systematically study continual\npretraining of two vision encoders, MedImageInsight (MI2) and RAD-DINO\nrepresenting the two major encoder paradigms CLIP and DINOv2, on up to 3.5M\nchest x-rays from a single institution, holding compute and evaluation\nprotocols constant. We evaluate on classification (radiology findings, lines\nand tubes), segmentation (lines and tubes), and radiology report generation.\nWhile prior work has primarily focused on tasks related to radiology findings,\nwe include lines and tubes tasks to counterbalance this bias and evaluate a\nmodel's ability to extract features that preserve continuity along elongated\nstructures. Our experiments show that MI2 scales more effectively for\nfinding-related tasks, while RAD-DINO is stronger on tube-related tasks.\nSurprisingly, continually pretraining MI2 with both reports and structured\nlabels using UniCL improves performance, underscoring the value of structured\nsupervision at scale. We further show that for some tasks, as few as 30k\nin-domain samples are sufficient to surpass open-weights foundation models.\nThese results highlight the utility of center-specific continual pretraining,\nenabling medical institutions to derive significant performance gains by\nutilizing in-domain data.", "AI": {"tldr": "本研究系统地探讨了在大量胸部X光数据上对医学视觉编码器（MI2和RAD-DINO）进行持续预训练的效果，发现不同模型在不同任务上表现出不同的扩展能力，并强调了结构化监督的价值以及中心特定预训练的实用性。", "motivation": "通用视觉基础模型（如CLIP和DINOv2）在网络规模数据上表现出色，但在医学成像领域，基础模型受限于较小的数据集，导致对数据规模和预训练范式如何影响性能的理解不足。", "method": "研究人员系统地研究了两种代表性视觉编码器（MI2，代表CLIP范式；RAD-DINO，代表DINOv2范式）的持续预训练。他们在来自单一机构的350万张胸部X光片上进行预训练，并保持计算和评估协议不变。评估任务包括分类（放射学发现、线条和管路）、分割（线条和管路）以及放射学报告生成。", "result": "实验结果显示，MI2在与发现相关的任务上扩展更有效，而RAD-DINO在与管路相关的任务上表现更强。令人惊讶的是，使用UniCL结合报告和结构化标签对MI2进行持续预训练能提升性能，突显了大规模结构化监督的价值。此外，研究发现对于某些任务，仅需3万个域内样本就足以超越开放权重的基础模型。", "conclusion": "这些结果突出了中心特定持续预训练的实用性，使医疗机构能够通过利用其域内数据获得显著的性能提升。"}}
{"id": "2509.12763", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12763", "abs": "https://arxiv.org/abs/2509.12763", "authors": ["Yican Zhao", "Ce Wang", "You Hao", "Lei Li", "Tianli Liao"], "title": "DyGLNet: Hybrid Global-Local Feature Fusion with Dynamic Upsampling for Medical Image Segmentation", "comment": "18pages, under review", "summary": "Medical image segmentation grapples with challenges including multi-scale\nlesion variability, ill-defined tissue boundaries, and computationally\nintensive processing demands. This paper proposes the DyGLNet, which achieves\nefficient and accurate segmentation by fusing global and local features with a\ndynamic upsampling mechanism. The model innovatively designs a hybrid feature\nextraction module (SHDCBlock), combining single-head self-attention and\nmulti-scale dilated convolutions to model local details and global context\ncollaboratively. We further introduce a dynamic adaptive upsampling module\n(DyFusionUp) to realize high-fidelity reconstruction of feature maps based on\nlearnable offsets. Then, a lightweight design is adopted to reduce\ncomputational overhead. Experiments on seven public datasets demonstrate that\nDyGLNet outperforms existing methods, particularly excelling in boundary\naccuracy and small-object segmentation. Meanwhile, it exhibits lower\ncomputation complexity, enabling an efficient and reliable solution for\nclinical medical image analysis. The code will be made available soon.", "AI": {"tldr": "本文提出DyGLNet，一种高效准确的医学图像分割模型，通过融合全局与局部特征并采用动态上采样机制，解决了多尺度病变、模糊边界和计算量大的挑战。", "motivation": "医学图像分割面临多尺度病变变异性、组织边界不明确以及计算密集型处理需求等挑战。", "method": "DyGLNet通过以下方式实现：1) 融合全局和局部特征；2) 引入动态上采样机制；3) 设计混合特征提取模块（SHDCBlock），结合单头自注意力与多尺度空洞卷积，协同建模局部细节和全局上下文；4) 引入动态自适应上采样模块（DyFusionUp），基于可学习偏移实现特征图的高保真重建；5) 采用轻量化设计以降低计算开销。", "result": "在七个公共数据集上的实验表明，DyGLNet优于现有方法，特别是在边界精度和小目标分割方面表现出色。同时，它展现出更低的计算复杂度。", "conclusion": "DyGLNet为临床医学图像分析提供了一个高效可靠的解决方案，能够实现准确且计算成本较低的分割。"}}
{"id": "2509.12888", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12888", "abs": "https://arxiv.org/abs/2509.12888", "authors": ["Weiming Chen", "Zhihan Zhu", "Yijia Wang", "Zhihai He"], "title": "Runge-Kutta Approximation and Decoupled Attention for Rectified Flow Inversion and Semantic Editing", "comment": null, "summary": "Rectified flow (RF) models have recently demonstrated superior generative\nperformance compared to DDIM-based diffusion models. However, in real-world\napplications, they suffer from two major challenges: (1) low inversion accuracy\nthat hinders the consistency with the source image, and (2) entangled\nmultimodal attention in diffusion transformers, which hinders precise attention\ncontrol. To address the first challenge, we propose an efficient high-order\ninversion method for rectified flow models based on the Runge-Kutta solver of\ndifferential equations. To tackle the second challenge, we introduce Decoupled\nDiffusion Transformer Attention (DDTA), a novel mechanism that disentangles\ntext and image attention inside the multimodal diffusion transformers, enabling\nmore precise semantic control. Extensive experiments on image reconstruction\nand text-guided editing tasks demonstrate that our method achieves\nstate-of-the-art performance in terms of fidelity and editability. Code is\navailable at https://github.com/wmchen/RKSovler_DDTA.", "AI": {"tldr": "本文提出了一种高效高阶反演方法和解耦扩散Transformer注意力（DDTA）机制，以解决整流流（RF）模型在反演精度和注意力控制方面的挑战，从而在图像重建和文本引导编辑任务中实现最先进的保真度和可编辑性。", "motivation": "整流流（RF）模型虽然生成性能优越，但在实际应用中面临两大挑战：一是反演精度低，影响与源图像的一致性；二是扩散Transformer中多模态注意力纠缠不清，阻碍了精确的注意力控制。", "method": "为解决第一个挑战，提出了一种基于Runge-Kutta微分方程求解器的高效高阶整流流模型反演方法。为解决第二个挑战，引入了去耦扩散Transformer注意力（DDTA），这是一种新颖的机制，可在多模态扩散Transformer内部解耦文本和图像注意力，从而实现更精确的语义控制。", "result": "在图像重建和文本引导编辑任务上的大量实验表明，该方法在保真度和可编辑性方面均达到了最先进的性能。", "conclusion": "通过提出高效高阶反演方法和解耦扩散Transformer注意力（DDTA），显著提升了整流流模型的图像重建保真度和文本引导编辑能力，有效解决了其在实际应用中的关键问题。"}}
{"id": "2509.12768", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.12768", "abs": "https://arxiv.org/abs/2509.12768", "authors": ["Mohammed Al-Habib", "Zuping Zhang", "Abdulrahman Noman"], "title": "BATR-FST: Bi-Level Adaptive Token Refinement for Few-Shot Transformers", "comment": "This paper has been accepted for publication at the IEEE\n  International Joint Conference on Neural Networks (IJCNN), Rome, Italy 2025", "summary": "Vision Transformers (ViTs) have shown significant promise in computer vision\napplications. However, their performance in few-shot learning is limited by\nchallenges in refining token-level interactions, struggling with limited\ntraining data, and developing a strong inductive bias. Existing methods often\ndepend on inflexible token matching or basic similarity measures, which limit\nthe effective incorporation of global context and localized feature refinement.\nTo address these challenges, we propose Bi-Level Adaptive Token Refinement for\nFew-Shot Transformers (BATR-FST), a two-stage approach that progressively\nimproves token representations and maintains a robust inductive bias for\nfew-shot classification. During the pre-training phase, Masked Image Modeling\n(MIM) provides Vision Transformers (ViTs) with transferable patch-level\nrepresentations by recreating masked image regions, providing a robust basis\nfor subsequent adaptation. In the meta-fine-tuning phase, BATR-FST incorporates\na Bi-Level Adaptive Token Refinement module that utilizes Token Clustering to\ncapture localized interactions, Uncertainty-Aware Token Weighting to prioritize\ndependable features, and a Bi-Level Attention mechanism to balance\nintra-cluster and inter-cluster relationships, thereby facilitating thorough\ntoken refinement. Furthermore, Graph Token Propagation ensures semantic\nconsistency between support and query instances, while a Class Separation\nPenalty preserves different class borders, enhancing discriminative capability.\nExtensive experiments on three benchmark few-shot datasets demonstrate that\nBATR-FST achieves superior results in both 1-shot and 5-shot scenarios and\nimproves the few-shot classification via transformers.", "AI": {"tldr": "本文提出BATR-FST，一种两阶段方法，通过双层自适应令牌精炼（包括令牌聚类、不确定性感知令牌加权、双层注意力、图令牌传播和类别分离惩罚）来提升Vision Transformer在小样本学习中的性能。", "motivation": "Vision Transformer在小样本学习中表现受限，原因在于令牌级交互精炼不足、训练数据有限、归纳偏置较弱。现有方法依赖僵化的令牌匹配或基本相似性度量，限制了全局上下文整合和局部特征精炼。", "method": "BATR-FST采用两阶段方法：预训练阶段利用掩码图像建模（MIM）提供可迁移的补丁级表示。元微调阶段引入双层自适应令牌精炼模块，该模块包含令牌聚类以捕获局部交互、不确定性感知令牌加权以优先处理可靠特征、以及双层注意力机制以平衡簇内和簇间关系。此外，还采用图令牌传播确保支持和查询实例间的语义一致性，并通过类别分离惩罚增强判别能力。", "result": "在三个基准小样本数据集上进行的广泛实验表明，BATR-FST在1-shot和5-shot场景下均取得了优异结果。", "conclusion": "BATR-FST有效提升了基于Transformer的小样本分类性能，通过其独特的双层自适应令牌精炼机制，解决了Vision Transformer在小样本学习中的挑战。"}}
{"id": "2509.12897", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.12897", "abs": "https://arxiv.org/abs/2509.12897", "authors": ["Jianfei Zhao", "Feng Zhang", "Xin Sun", "Lingxing Kong", "Zhixing Tan", "Chong Feng"], "title": "Cross-Layer Vision Smoothing: Enhancing Visual Understanding via Sustained Focus on Key Objects in Large Vision-Language Models", "comment": null, "summary": "Large Vision-Language Models (LVLMs) can accurately locate key objects in\nimages, yet their attention to these objects tends to be very brief. Motivated\nby the hypothesis that sustained focus on key objects can improve LVLMs' visual\ncapabilities, we propose Cross-Layer Vision Smoothing (CLVS). The core idea of\nCLVS is to incorporate a vision memory that smooths the attention distribution\nacross layers. Specifically, we initialize this vision memory with\nposition-unbiased visual attention in the first layer. In subsequent layers,\nthe model's visual attention jointly considers the vision memory from previous\nlayers, while the memory is updated iteratively, thereby maintaining smooth\nattention on key objects. Given that visual understanding primarily occurs in\nthe early and middle layers of the model, we use uncertainty as an indicator of\ncompleted visual understanding and terminate the smoothing process accordingly.\nExperiments on four benchmarks across three LVLMs confirm the effectiveness and\ngeneralizability of our method. CLVS achieves state-of-the-art performance on a\nvariety of visual understanding tasks, with particularly significant\nimprovements in relation and attribute understanding.", "AI": {"tldr": "大型视觉语言模型（LVLMs）对关键对象的关注短暂，本文提出跨层视觉平滑（CLVS），通过引入视觉记忆来平滑跨层的注意力分布，从而提升LVLMs的视觉理解能力。", "motivation": "LVLMs虽然能准确识别图像中的关键对象，但它们对这些对象的注意力往往非常短暂。研究者假设，对关键对象持续的关注可以显著提升LVLMs的视觉能力。", "method": "本文提出了跨层视觉平滑（CLVS）方法。其核心思想是引入一个视觉记忆，用于平滑跨层级的注意力分布。具体来说，该视觉记忆在第一层用位置无偏的视觉注意力进行初始化。在后续层中，模型的视觉注意力会联合考虑前一层的视觉记忆，同时记忆也会迭代更新，从而在关键对象上保持平滑的注意力。鉴于视觉理解主要发生在模型的早期和中期层，该方法使用不确定性作为视觉理解完成的指标，并据此终止平滑过程。", "result": "在三个LVLM上的四个基准测试中，CLVS证明了其有效性和泛化性。该方法在多种视觉理解任务上取得了最先进的性能，尤其在关系和属性理解方面有显著提升。", "conclusion": "CLVS通过在LVLMs中引入跨层视觉平滑机制，有效解决了模型对关键对象注意力短暂的问题，显著提升了LVLMs的视觉理解能力，尤其在关系和属性理解方面表现突出，并具有良好的泛化性。"}}
{"id": "2509.12784", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12784", "abs": "https://arxiv.org/abs/2509.12784", "authors": ["Zhehao Li", "Yucheng Qian", "Chong Wang", "Yinghao Lu", "Zhihao Yang", "Jiafei Wu"], "title": "Modeling the Multivariate Relationship with Contextualized Representations for Effective Human-Object Interaction Detection", "comment": null, "summary": "Human-Object Interaction (HOI) detection aims to simultaneously localize\nhuman-object pairs and recognize their interactions. While recent two-stage\napproaches have made significant progress, they still face challenges due to\nincomplete context modeling. In this work, we introduce a Contextualized\nRepresentation Learning Network that integrates both affordance-guided\nreasoning and contextual prompts with visual cues to better capture complex\ninteractions. We enhance the conventional HOI detection framework by expanding\nit beyond simple human-object pairs to include multivariate relationships\ninvolving auxiliary entities like tools. Specifically, we explicitly model the\nfunctional role (affordance) of these auxiliary objects through triplet\nstructures <human, tool, object>. This enables our model to identify\ntool-dependent interactions such as 'filling'. Furthermore, the learnable\nprompt is enriched with instance categories and subsequently integrated with\ncontextual visual features using an attention mechanism. This process aligns\nlanguage with image content at both global and regional levels. These\ncontextualized representations equip the model with enriched relational cues\nfor more reliable reasoning over complex, context-dependent interactions. Our\nproposed method demonstrates superior performance on both the HICO-Det and\nV-COCO datasets in most scenarios. Codes will be released upon acceptance.", "AI": {"tldr": "本文提出了一种情境化表示学习网络，通过结合辅助对象的功能角色（可供性）推理和上下文提示，增强了人-物交互（HOI）检测，特别是在复杂和依赖工具的交互场景中。", "motivation": "现有的两阶段HOI检测方法在上下文建模方面存在不足，难以捕捉复杂的人-物交互。", "method": "该方法引入了一个情境化表示学习网络：1) 将HOI检测扩展到包含辅助实体（如工具）的多元关系，通过三元组<人, 工具, 物体>显式建模工具的功能角色（可供性）。2) 使用实例类别丰富可学习提示，并通过注意力机制将其与上下文视觉特征集成，以在全局和局部层面对齐语言和图像内容。", "result": "所提出的方法在HICO-Det和V-COCO数据集的大多数场景中都表现出卓越的性能。", "conclusion": "情境化表示为模型提供了丰富的关系线索，从而能够更可靠地推断复杂、依赖上下文的交互，显著提升了HOI检测能力。"}}
{"id": "2509.12990", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.12990", "abs": "https://arxiv.org/abs/2509.12990", "authors": ["Boyu Han", "Qianqian Xu", "Shilong Bao", "Zhiyong Yang", "Sicong Li", "Qingming Huang"], "title": "Dual-Stage Reweighted MoE for Long-Tailed Egocentric Mistake Detection", "comment": null, "summary": "In this report, we address the problem of determining whether a user performs\nan action incorrectly from egocentric video data. To handle the challenges\nposed by subtle and infrequent mistakes, we propose a Dual-Stage Reweighted\nMixture-of-Experts (DR-MoE) framework. In the first stage, features are\nextracted using a frozen ViViT model and a LoRA-tuned ViViT model, which are\ncombined through a feature-level expert module. In the second stage, three\nclassifiers are trained with different objectives: reweighted cross-entropy to\nmitigate class imbalance, AUC loss to improve ranking under skewed\ndistributions, and label-aware loss with sharpness-aware minimization to\nenhance calibration and generalization. Their predictions are fused using a\nclassification-level expert module. The proposed method achieves strong\nperformance, particularly in identifying rare and ambiguous mistake instances.\nThe code is available at https://github.com/boyuh/DR-MoE.", "AI": {"tldr": "本报告提出了一种双阶段重加权专家混合（DR-MoE）框架，用于从自我中心视频数据中识别用户是否错误执行动作，尤其擅长检测细微和罕见的错误。", "motivation": "研究动机在于需要从自我中心视频数据中判断用户是否错误执行动作，并应对细微且不频繁的错误所带来的挑战。", "method": "该方法提出DR-MoE框架。第一阶段，使用冻结的ViViT模型和LoRA微调的ViViT模型提取特征，并通过特征级专家模块进行组合。第二阶段，训练三个具有不同目标的分类器：使用重加权交叉熵解决类别不平衡、使用AUC损失改进倾斜分布下的排名，以及使用带有锐度感知最小化的标签感知损失增强校准和泛化能力。最后，通过分类级专家模块融合它们的预测。", "result": "所提出的方法取得了强大的性能，尤其在识别罕见和模糊的错误实例方面表现出色。", "conclusion": "DR-MoE框架能有效处理自我中心视频中检测细微和不频繁错误动作的挑战，并取得了优异的性能。"}}
{"id": "2509.12787", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12787", "abs": "https://arxiv.org/abs/2509.12787", "authors": ["Linchun Wu", "Qin Zou", "Xianbiao Qi", "Bo Du", "Zhongyuan Wang", "Qingquan Li"], "title": "Double Helix Diffusion for Cross-Domain Anomaly Image Generation", "comment": null, "summary": "Visual anomaly inspection is critical in manufacturing, yet hampered by the\nscarcity of real anomaly samples for training robust detectors. Synthetic data\ngeneration presents a viable strategy for data augmentation; however, current\nmethods remain constrained by two principal limitations: 1) the generation of\nanomalies that are structurally inconsistent with the normal background, and 2)\nthe presence of undesirable feature entanglement between synthesized images and\ntheir corresponding annotation masks, which undermines the perceptual realism\nof the output. This paper introduces Double Helix Diffusion (DH-Diff), a novel\ncross-domain generative framework designed to simultaneously synthesize\nhigh-fidelity anomaly images and their pixel-level annotation masks, explicitly\naddressing these challenges. DH-Diff employs a unique architecture inspired by\na double helix, cycling through distinct modules for feature separation,\nconnection, and merging. Specifically, a domain-decoupled attention mechanism\nmitigates feature entanglement by enhancing image and annotation features\nindependently, and meanwhile a semantic score map alignment module ensures\nstructural authenticity by coherently integrating anomaly foregrounds. DH-Diff\noffers flexible control via text prompts and optional graphical guidance.\nExtensive experiments demonstrate that DH-Diff significantly outperforms\nstate-of-the-art methods in diversity and authenticity, leading to significant\nimprovements in downstream anomaly detection performance.", "AI": {"tldr": "DH-Diff是一种新颖的跨域生成框架，受双螺旋启发，能同时合成高保真异常图像及其像素级注释掩码，解决了现有方法中结构不一致和特征纠缠的问题，显著提升了下游异常检测性能。", "motivation": "在制造业中，视觉异常检测因缺乏真实的异常样本来训练鲁棒的检测器而受阻。合成数据生成是数据增强的有效策略，但现有方法存在两个主要限制：1) 生成的异常与正常背景在结构上不一致；2) 合成图像与其注释掩码之间存在不希望的特征纠缠，损害了输出的感知真实性。", "method": "本文提出了Double Helix Diffusion (DH-Diff)，一个新颖的跨域生成框架。其架构灵感来源于双螺旋，通过特征分离、连接和合并的独特模块循环。具体而言，它采用域解耦注意力机制独立增强图像和注释特征以减轻特征纠缠，并通过语义分数图对齐模块连贯地整合异常前景以确保结构真实性。DH-Diff还支持通过文本提示和可选图形指导进行灵活控制。", "result": "DH-Diff在多样性和真实性方面显著优于现有最先进的方法，并在下游异常检测性能方面带来了显著改进。", "conclusion": "DH-Diff成功解决了合成异常数据生成中的结构不一致和特征纠缠问题，能够同时合成高保真异常图像和精确的像素级注释掩码，从而有效提升了异常检测的性能。"}}
{"id": "2509.13031", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13031", "abs": "https://arxiv.org/abs/2509.13031", "authors": ["Yan Chen", "Long Li", "Teng Xi", "Long Zeng", "Jingdong Wang"], "title": "Perception Before Reasoning: Two-Stage Reinforcement Learning for Visual Reasoning in Vision-Language Models", "comment": null, "summary": "Reinforcement learning (RL) has proven highly effective in eliciting the\nreasoning capabilities of large language models (LLMs). Inspired by this\nsuccess, recent studies have explored applying similar techniques to\nvision-language models (VLMs), aiming to enhance their reasoning performance.\nHowever, directly transplanting RL methods from LLMs to VLMs is suboptimal, as\nthe tasks faced by VLMs are inherently more complex. Specifically, VLMs must\nfirst accurately perceive and understand visual inputs before reasoning can be\neffectively performed. To address this challenge, we propose a two-stage\nreinforcement learning framework designed to jointly enhance both the\nperceptual and reasoning capabilities of VLMs. To mitigate the vanishing\nadvantage issue commonly observed in RL training, we first perform\ndataset-level sampling to selectively strengthen specific capabilities using\ndistinct data sources. During training, the first stage focuses on improving\nthe model's visual perception through coarse- and fine-grained visual\nunderstanding, while the second stage targets the enhancement of reasoning\nabilities. After the proposed two-stage reinforcement learning process, we\nobtain PeBR-R1, a vision-language model with significantly enhanced perceptual\nand reasoning capabilities. Experimental results on seven benchmark datasets\ndemonstrate the effectiveness of our approach and validate the superior\nperformance of PeBR-R1 across diverse visual reasoning tasks.", "AI": {"tldr": "本文提出了一种两阶段强化学习框架，旨在联合提升视觉-语言模型（VLM）的感知和推理能力，以克服直接将LLM的RL方法应用于VLM的局限性。", "motivation": "尽管强化学习（RL）在提升大型语言模型（LLM）的推理能力方面表现出色，但直接将这些技术应用于视觉-语言模型（VLM）是次优的。VLM面临的任务更为复杂，需要首先准确感知和理解视觉输入，然后才能有效进行推理。", "method": "研究者提出了一种两阶段强化学习框架。首先进行数据集级别的采样，以利用不同的数据源选择性地强化特定能力，从而缓解RL训练中常见的“优势消失”问题。训练的第一阶段侧重于通过粗粒度和细粒度视觉理解来提高模型的视觉感知能力，而第二阶段则旨在增强推理能力。", "result": "通过所提出的两阶段强化学习过程，获得了PeBR-R1模型，该模型显著增强了感知和推理能力。在七个基准数据集上的实验结果表明，该方法有效，并且PeBR-R1在各种视觉推理任务中表现出卓越的性能。", "conclusion": "所提出的两阶段强化学习框架能够有效地联合提升视觉-语言模型的感知和推理能力，解决了VLM在处理复杂任务时对视觉理解的内在需求，并取得了显著的性能提升。"}}
{"id": "2509.12791", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12791", "abs": "https://arxiv.org/abs/2509.12791", "authors": ["Julien Walther", "Rémi Giraud", "Michaël Clément"], "title": "Superpixel Anything: A general object-based framework for accurate yet regular superpixel segmentation", "comment": null, "summary": "Superpixels are widely used in computer vision to simplify image\nrepresentation and reduce computational complexity. While traditional methods\nrely on low-level features, deep learning-based approaches leverage high-level\nfeatures but also tend to sacrifice regularity of superpixels to capture\ncomplex objects, leading to accurate but less interpretable segmentations. In\nthis work, we introduce SPAM (SuperPixel Anything Model), a versatile framework\nfor segmenting images into accurate yet regular superpixels. We train a model\nto extract image features for superpixel generation, and at inference, we\nleverage a large-scale pretrained model for semantic-agnostic segmentation to\nensure that superpixels align with object masks. SPAM can handle any prior\nhigh-level segmentation, resolving uncertainty regions, and is able to\ninteractively focus on specific objects. Comprehensive experiments demonstrate\nthat SPAM qualitatively and quantitatively outperforms state-of-the-art methods\non segmentation tasks, making it a valuable and robust tool for various\napplications. Code and pre-trained models are available here:\nhttps://github.com/waldo-j/spam.", "AI": {"tldr": "SPAM (SuperPixel Anything Model) 是一个多功能框架，旨在生成准确且规则的超像素。它结合了深度学习特征提取和大规模预训练模型的语义无关分割，以确保超像素与对象掩码对齐，并在分割任务上优于现有方法。", "motivation": "传统超像素方法依赖低级特征，而深度学习方法虽然利用高级特征，但常牺牲超像素的规则性以捕获复杂对象，导致分割准确但可解释性差。因此，需要一种既能保持超像素规则性又能实现准确分割的方法。", "method": "SPAM框架训练一个模型来提取图像特征以生成超像素。在推理阶段，它利用一个大规模预训练模型进行语义无关分割，以确保生成的超像素与对象掩码对齐。该方法能够处理任何先验的高级分割，解决不确定区域，并支持交互式地关注特定对象。", "result": "实验结果表明，SPAM在分割任务上无论从定性还是定量角度都优于目前最先进的方法。", "conclusion": "SPAM是一个有价值且鲁棒的工具，适用于各种计算机视觉应用。"}}
{"id": "2509.13070", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13070", "abs": "https://arxiv.org/abs/2509.13070", "authors": ["Qianqi Lu", "Yuxiang Xie", "Jing Zhang", "Shiwei Zou", "Yan Chen", "Xidao Luan"], "title": "TFANet: Three-Stage Image-Text Feature Alignment Network for Robust Referring Image Segmentation", "comment": null, "summary": "Referring Image Segmentation (RIS) is a task that segments image regions\nbased on language expressions, requiring fine-grained alignment between two\nmodalities. However, existing methods often struggle with multimodal\nmisalignment and language semantic loss, especially in complex scenes\ncontaining multiple visually similar objects, where uniquely described targets\nare frequently mislocalized or incompletely segmented. To tackle these\nchallenges, this paper proposes TFANet, a Three-stage Image-Text Feature\nAlignment Network that systematically enhances multimodal alignment through a\nhierarchical framework comprising three stages: Knowledge Plus Stage (KPS),\nKnowledge Fusion Stage (KFS), and Knowledge Intensification Stage (KIS). In the\nfirst stage, we design the Multiscale Linear Cross-Attention Module (MLAM),\nwhich facilitates bidirectional semantic exchange between visual features and\ntextual representations across multiple scales. This establishes rich and\nefficient alignment between image regions and different granularities of\nlinguistic descriptions. Subsequently, the KFS further strengthens feature\nalignment through the Cross-modal Feature Scanning Module (CFSM), which applies\nmultimodal selective scanning to capture long-range dependencies and construct\na unified multimodal representation. This is essential for modeling long-range\ncross-modal dependencies and enhancing alignment accuracy in complex scenes.\nFinally, in the KIS, we propose the Word-level Linguistic Feature-guided\nSemantic Deepening Module (WFDM) to compensate for semantic degradation\nintroduced in earlier stages.", "AI": {"tldr": "本文提出TFANet，一个三阶段图像-文本特征对齐网络，通过多尺度注意力、跨模态特征扫描和词级语义深化模块，系统性地增强多模态对齐，以解决参照图像分割中多模态错位和语言语义损失问题，尤其是在复杂场景下。", "motivation": "现有参照图像分割（RIS）方法在多模态错位和语言语义损失方面存在困难，尤其是在包含多个视觉相似对象的复杂场景中，导致目标定位错误或分割不完整。", "method": "本文提出了TFANet，一个三阶段图像-文本特征对齐网络：\n1.  **知识增强阶段 (KPS)**：设计了多尺度线性交叉注意力模块 (MLAM)，促进视觉特征与文本表征在多尺度上的双向语义交换。\n2.  **知识融合阶段 (KFS)**：通过跨模态特征扫描模块 (CFSM) 进一步强化特征对齐，进行多模态选择性扫描以捕获长距离依赖并构建统一的多模态表示。\n3.  **知识深化阶段 (KIS)**：提出了词级语言特征引导语义深化模块 (WFDM)，以弥补早期阶段可能引入的语义退化。", "result": "TFANet旨在通过分层框架系统地增强多模态对齐，解决参照图像分割中多模态错位和语言语义损失问题，特别是在复杂场景中提高对目标对象的定位和分割精度。", "conclusion": "本文提出TFANet，一个系统性的三阶段图像-文本特征对齐网络，通过创新的模块设计，旨在解决参照图像分割任务中存在的跨模态错位和语言语义损失问题，特别是在复杂场景下提升分割性能。"}}
{"id": "2509.12815", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12815", "abs": "https://arxiv.org/abs/2509.12815", "authors": ["Biwen Lei", "Yang Li", "Xinhai Liu", "Shuhui Yang", "Lixin Xu", "Jingwei Huang", "Ruining Tang", "Haohan Weng", "Jian Liu", "Jing Xu", "Zhen Zhou", "Yiling Zhu", "Jiankai Xing", "Jiachen Xu", "Changfeng Ma", "Xinhao Yan", "Yunhan Yang", "Chunshi Wang", "Duoteng Xu", "Xueqi Ma", "Yuguang Chen", "Jing Li", "Mingxin Yang", "Sheng Zhang", "Yifei Feng", "Xin Huang", "Di Luo", "Zebin He", "Puhua Jiang", "Changrong Hu", "Zihan Qin", "Shiwei Miao", "Haolin Liu", "Yunfei Zhao", "Zeqiang Lai", "Qingxiang Lin", "Zibo Zhao", "Kunhong Li", "Xianghui Yang", "Huiwen Shi", "Xin Yang", "Yuxuan Wang", "Zebin Yao", "Yihang Lian", "Sicong Liu", "Xintong Han", "Wangchen Qin", "Caisheng Ouyang", "Jianyin Liu", "Tianwen Yuan", "Shuai Jiang", "Hong Duan", "Yanqi Niu", "Wencong Lin", "Yifu Sun", "Shirui Huang", "Lin Niu", "Gu Gong", "Guojian Xiao", "Bojian Zheng", "Xiang Yuan", "Qi Chen", "Jie Xiao", "Dongyang Zheng", "Xiaofeng Yang", "Kai Liu", "Jianchen Zhu", "Lifu Wang", "Qinglin Lu", "Jie Liu", "Liang Dong", "Fan Jiang", "Ruibin Chen", "Lei Wang", "Chao Zhang", "Jiaxin Lin", "Hao Zhang", "Zheng Ye", "Peng He", "Runzhou Wu", "Yinhe Wu", "Jiayao Du", "Jupeng Chen", "Xinyue Mao", "Dongyuan Guo", "Yixuan Tang", "Yulin Tsai", "Yonghao Tan", "Jiaao Yu", "Junlin Yu", "Keren Zhang", "Yifan Li", "Peng Chen", "Tian Liu", "Di Wang", "Yuhong Liu", "Linus", "Jie Jiang", "Zhuo Chen", "Chunchao Guo"], "title": "Hunyuan3D Studio: End-to-End AI Pipeline for Game-Ready 3D Asset Generation", "comment": "Technical Report", "summary": "The creation of high-quality 3D assets, a cornerstone of modern game\ndevelopment, has long been characterized by labor-intensive and specialized\nworkflows. This paper presents Hunyuan3D Studio, an end-to-end AI-powered\ncontent creation platform designed to revolutionize the game production\npipeline by automating and streamlining the generation of game-ready 3D assets.\nAt its core, Hunyuan3D Studio integrates a suite of advanced neural modules\n(such as Part-level 3D Generation, Polygon Generation, Semantic UV, etc.) into\na cohesive and user-friendly system. This unified framework allows for the\nrapid transformation of a single concept image or textual description into a\nfully-realized, production-quality 3D model complete with optimized geometry\nand high-fidelity PBR textures. We demonstrate that assets generated by\nHunyuan3D Studio are not only visually compelling but also adhere to the\nstringent technical requirements of contemporary game engines, significantly\nreducing iteration time and lowering the barrier to entry for 3D content\ncreation. By providing a seamless bridge from creative intent to technical\nasset, Hunyuan3D Studio represents a significant leap forward for AI-assisted\nworkflows in game development and interactive media.", "AI": {"tldr": "本文提出了Hunyuan3D Studio，一个端到端的人工智能内容创作平台，旨在通过自动化和简化游戏级3D资产的生成，彻底改变游戏制作流程。", "motivation": "现代游戏开发中高质量3D资产的创建工作量大且专业性强，导致流程耗时耗力。", "method": "Hunyuan3D Studio整合了一套先进的神经网络模块（如部件级3D生成、多边形生成、语义UV等），形成一个统一且用户友好的系统。该平台能将单一概念图像或文本描述快速转换为具备优化几何结构和高保真PBR纹理的生产级3D模型。", "result": "Hunyuan3D Studio生成的资产不仅视觉效果引人注目，还符合当代游戏引擎严格的技术要求，显著减少了迭代时间，并降低了3D内容创作的门槛。", "conclusion": "Hunyuan3D Studio在创意意图与技术资产之间建立了无缝桥梁，代表着游戏开发和互动媒体中AI辅助工作流程的重大飞跃。"}}
{"id": "2509.13107", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13107", "abs": "https://arxiv.org/abs/2509.13107", "authors": ["Kohou Wang", "Huan Hu", "Xiang Liu", "Zezhou Chen", "Ping Chen", "Zhaoxiang Liu", "Shiguo Lian"], "title": "Hierarchical Deep Fusion Framework for Multi-dimensional Facial Forgery Detection -- The 2024 Global Deepfake Image Detection Challenge", "comment": "The 2024 Global Deepfake Image Detection Challenge Top20 Reward, 5\n  pages", "summary": "The proliferation of sophisticated deepfake technology poses significant\nchallenges to digital security and authenticity. Detecting these forgeries,\nespecially across a wide spectrum of manipulation techniques, requires robust\nand generalized models. This paper introduces the Hierarchical Deep Fusion\nFramework (HDFF), an ensemble-based deep learning architecture designed for\nhigh-performance facial forgery detection. Our framework integrates four\ndiverse pre-trained sub-models, Swin-MLP, CoAtNet, EfficientNetV2, and DaViT,\nwhich are meticulously fine-tuned through a multi-stage process on the\nMultiFFDI dataset. By concatenating the feature representations from these\nspecialized models and training a final classifier layer, HDFF effectively\nleverages their collective strengths. This approach achieved a final score of\n0.96852 on the competition's private leaderboard, securing the 20th position\nout of 184 teams, demonstrating the efficacy of hierarchical fusion for complex\nimage classification tasks.", "AI": {"tldr": "本文提出分层深度融合框架（HDFF），一个基于集成学习的深度学习架构，用于高性能面部伪造检测。", "motivation": "复杂的深度伪造技术对数字安全和真实性构成重大挑战，需要鲁棒和通用的模型来检测各种操纵技术。", "method": "HDFF框架整合了四种不同的预训练子模型（Swin-MLP、CoAtNet、EfficientNetV2和DaViT），这些模型通过多阶段过程在MultiFFDI数据集上进行了精细调整。通过连接这些专用模型的特征表示并训练一个最终的分类器层，HDFF有效利用了它们的集体优势。", "result": "该方法在比赛的私人排行榜上取得了0.96852的最终分数，在184支队伍中排名第20位。", "conclusion": "研究结果证明了分层融合在复杂图像分类任务（特别是面部伪造检测）中的有效性。"}}
{"id": "2509.12817", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12817", "abs": "https://arxiv.org/abs/2509.12817", "authors": ["Yuan Cao", "Dong Wang"], "title": "SAGA: Selective Adaptive Gating for Efficient and Expressive Linear Attention", "comment": null, "summary": "While Transformer architecture excel at modeling long-range dependencies\ncontributing to its widespread adoption in vision tasks the quadratic\ncomplexity of softmax-based attention mechanisms imposes a major bottleneck,\nparticularly when processing high-resolution images. Linear attention presents\na promising alternative by reformulating the attention computation from $(QK)V$\nto $Q(KV)$, thereby reducing the complexity from $\\mathcal{O}(N^2)$ to\n$\\mathcal{O}(N)$ while preserving the global receptive field. However, most\nexisting methods compress historical key-value (KV) information uniformly,\nwhich can lead to feature redundancy and the loss of directional alignment with\nthe query (Q). This uniform compression results in low-rank $KV$ feature maps,\ncontributing to a performance gap compared to softmax attention. To mitigate\nthis limitation, we propose \\textbf{S}elective \\textbf{A}daptive\n\\textbf{GA}ting for Efficient and Expressive Linear Attention (SAGA) , which\nintroduces input-adaptive learnable gates to selectively modulate information\naggregation into the $KV$ feature map. These gates enhance semantic diversity\nand alleviate the low-rank constraint inherent in conventional linear\nattention. Additionally, we propose an efficient Hadamard-product decomposition\nmethod for gate computation, which introduces no additional memory overhead.\nExperiments demonstrate that SAGA achieves a 1.76$\\times$ improvement in\nthroughput and a 2.69$\\times$ reduction in peak GPU memory compared to PVT-T at\na resolution of $1280 \\times 1280$. Moreover, it improves top-1 accuracy by up\nto 4.4\\% on the ImageNet dataset, demonstrating both computational efficiency\nand model effectiveness.", "AI": {"tldr": "Transformer的二次复杂度是瓶颈。线性注意力是替代方案，但现有方法存在特征冗余和低秩问题。SAGA通过引入输入自适应门控来选择性聚合信息，解决了这些问题，显著提高了效率和性能。", "motivation": "Transformer在视觉任务中表现出色，但其softmax注意力机制的二次复杂度在高分辨率图像处理中成为主要瓶颈。虽然线性注意力能将复杂度降至线性，但现有方法通常统一压缩历史KV信息，导致特征冗余、失去与Q的方向对齐，并产生低秩KV特征图，从而与softmax注意力存在性能差距。", "method": "本文提出了SAGA（Selective Adaptive Gating for Efficient and Expressive Linear Attention）。SAGA引入了输入自适应的可学习门控机制，用于选择性地调节信息聚合到KV特征图中，以增强语义多样性并缓解传统线性注意力固有的低秩约束。此外，SAGA还提出了一种高效的Hadamard积分解方法来计算门控，且不引入额外的内存开销。", "result": "实验结果表明，在1280x1280分辨率下，SAGA相比PVT-T吞吐量提升了1.76倍，峰值GPU内存减少了2.69倍。在ImageNet数据集上，其top-1准确率提升高达4.4%。", "conclusion": "SAGA通过引入选择性自适应门控机制，有效解决了传统线性注意力中存在的特征冗余和低秩问题，在显著提升计算效率的同时，也大幅提高了模型在图像分类任务上的性能。"}}
{"id": "2509.13229", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.13229", "abs": "https://arxiv.org/abs/2509.13229", "authors": ["Hugo Carlesso", "Josiane Mothe", "Radu Tudor Ionescu"], "title": "Curriculum Multi-Task Self-Supervision Improves Lightweight Architectures for Onboard Satellite Hyperspectral Image Segmentation", "comment": null, "summary": "Hyperspectral imaging (HSI) captures detailed spectral signatures across\nhundreds of contiguous bands per pixel, being indispensable for remote sensing\napplications such as land-cover classification, change detection, and\nenvironmental monitoring. Due to the high dimensionality of HSI data and the\nslow rate of data transfer in satellite-based systems, compact and efficient\nmodels are required to support onboard processing and minimize the transmission\nof redundant or low-value data, e.g. cloud-covered areas. To this end, we\nintroduce a novel curriculum multi-task self-supervised learning (CMTSSL)\nframework designed for lightweight architectures for HSI analysis. CMTSSL\nintegrates masked image modeling with decoupled spatial and spectral jigsaw\npuzzle solving, guided by a curriculum learning strategy that progressively\nincreases data complexity during self-supervision. This enables the encoder to\njointly capture fine-grained spectral continuity, spatial structure, and global\nsemantic features. Unlike prior dual-task SSL methods, CMTSSL simultaneously\naddresses spatial and spectral reasoning within a unified and computationally\nefficient design, being particularly suitable for training lightweight models\nfor onboard satellite deployment. We validate our approach on four public\nbenchmark datasets, demonstrating consistent gains in downstream segmentation\ntasks, using architectures that are over 16,000x lighter than some\nstate-of-the-art models. These results highlight the potential of CMTSSL in\ngeneralizable representation learning with lightweight architectures for\nreal-world HSI applications. Our code is publicly available at\nhttps://github.com/hugocarlesso/CMTSSL.", "AI": {"tldr": "本文提出了一种名为CMTSSL的新型课程多任务自监督学习框架，专为轻量级高光谱图像（HSI）模型设计，旨在解决星载处理中高维数据传输效率低的问题，并在下游分割任务中取得了显著效果，同时保持模型极轻量化。", "motivation": "高光谱图像数据维度高，卫星系统数据传输速率慢，需要紧凑高效的模型支持星载处理，以最小化冗余或低价值数据的传输，例如云覆盖区域。现有的方法可能不适合轻量级架构或无法同时有效处理空间和光谱推理。", "method": "CMTSSL框架集成了掩码图像建模（MIM）与解耦的空间和光谱拼图（jigsaw puzzle）求解。该框架由课程学习策略指导，逐步增加自监督期间的数据复杂性，使编码器能够共同捕捉精细的光谱连续性、空间结构和全局语义特征。与之前的双任务自监督学习方法不同，CMTSSL在一个统一且计算高效的设计中同时解决了空间和光谱推理问题。", "result": "该方法在四个公开基准数据集上进行了验证，在下游分割任务中显示出持续的性能提升。所使用的架构比一些最先进的模型轻了超过16,000倍。这些结果突出了CMTSSL在轻量级架构下进行可泛化表示学习在实际高光谱图像应用中的潜力。", "conclusion": "CMTSSL为高光谱图像分析提供了一种新颖且高效的自监督学习范式，特别适用于星载部署的轻量级模型。它通过结合多任务自监督和课程学习，成功地使模型能够学习到丰富的空间和光谱特征，同时显著降低了计算和存储需求，为实时高光谱数据处理开辟了道路。"}}
{"id": "2509.12836", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12836", "abs": "https://arxiv.org/abs/2509.12836", "authors": ["Shreyas Shivakumara", "Gabriel Eilertsen", "Karljohan Lundin Palmerius"], "title": "Exploring Metric Fusion for Evaluation of NeRFs", "comment": "Accepted for 17th International Conference on Quality of Multimedia\n  Experience (QoMEX 25)", "summary": "Neural Radiance Fields (NeRFs) have demonstrated significant potential in\nsynthesizing novel viewpoints. Evaluating the NeRF-generated outputs, however,\nremains a challenge due to the unique artifacts they exhibit, and no individual\nmetric performs well across all datasets. We hypothesize that combining two\nsuccessful metrics, Deep Image Structure and Texture Similarity (DISTS) and\nVideo Multi-Method Assessment Fusion (VMAF), based on different perceptual\nmethods, can overcome the limitations of individual metrics and achieve\nimproved correlation with subjective quality scores. We experiment with two\nnormalization strategies for the individual metrics and two fusion strategies\nto evaluate their impact on the resulting correlation with the subjective\nscores. The proposed pipeline is tested on two distinct datasets, Synthetic and\nOutdoor, and its performance is evaluated across three different\nconfigurations. We present a detailed analysis comparing the correlation\ncoefficients of fusion methods and individual scores with subjective scores to\ndemonstrate the robustness and generalizability of the fusion metrics.", "AI": {"tldr": "本文提出了一种结合DISTS和VMAF的融合度量方法，以克服单个度量在评估NeRF生成视图时的局限性，并实现了与主观质量评分更好的相关性。", "motivation": "神经辐射场（NeRFs）在合成新视角方面潜力巨大，但评估其输出具有挑战性，因为NeRFs会产生独特的伪影，且现有单个度量在所有数据集上表现不佳。研究者假设结合不同感知方法的度量可以克服这些限制。", "method": "结合了深度图像结构和纹理相似度（DISTS）与视频多方法评估融合（VMAF）这两种成功的度量。实验了两种个体度量的归一化策略和两种融合策略，以评估它们对与主观评分相关性的影响。提出的流程在Synthetic和Outdoor两个数据集上进行了测试。", "result": "通过详细分析融合方法和个体度量与主观评分的相关系数，证明了融合度量的鲁棒性和泛化能力。实验结果显示，融合度量能克服个体度量的局限性，并实现了改进的与主观质量评分的相关性。", "conclusion": "结合DISTS和VMAF的融合度量方法在评估NeRF生成输出时表现出更强的鲁棒性和泛化能力，并能更准确地反映主观质量，从而克服了单个度量的局限性。"}}
{"id": "2509.13270", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.13270", "abs": "https://arxiv.org/abs/2509.13270", "authors": ["Mohammed Baharoon", "Siavash Raissi", "John S. Jun", "Thibault Heintz", "Mahmoud Alabbad", "Ali Alburkani", "Sung Eun Kim", "Kent Kleinschmidt", "Abdulrahman O. Alhumaydhi", "Mohannad Mohammed G. Alghamdi", "Jeremy Francis Palacio", "Mohammed Bukhaytan", "Noah Michael Prudlo", "Rithvik Akula", "Brady Chrisler", "Benjamin Galligos", "Mohammed O. Almutairi", "Mazeen Mohammed Alanazi", "Nasser M. Alrashdi", "Joel Jihwan Hwang", "Sri Sai Dinesh Jaliparthi", "Luke David Nelson", "Nathaniel Nguyen", "Sathvik Suryadevara", "Steven Kim", "Mohammed F. Mohammed", "Yevgeniy R. Semenov", "Kun-Hsing Yu", "Abdulrhman Aljouie", "Hassan AlOmaish", "Adam Rodman", "Pranav Rajpurkar"], "title": "RadGame: An AI-Powered Platform for Radiology Education", "comment": null, "summary": "We introduce RadGame, an AI-powered gamified platform for radiology education\nthat targets two core skills: localizing findings and generating reports.\nTraditional radiology training is based on passive exposure to cases or active\npractice with real-time input from supervising radiologists, limiting\nopportunities for immediate and scalable feedback. RadGame addresses this gap\nby combining gamification with large-scale public datasets and automated,\nAI-driven feedback that provides clear, structured guidance to human learners.\nIn RadGame Localize, players draw bounding boxes around abnormalities, which\nare automatically compared to radiologist-drawn annotations from public\ndatasets, and visual explanations are generated by vision-language models for\nuser missed findings. In RadGame Report, players compose findings given a chest\nX-ray, patient age and indication, and receive structured AI feedback based on\nradiology report generation metrics, highlighting errors and omissions compared\nto a radiologist's written ground truth report from public datasets, producing\na final performance and style score. In a prospective evaluation, participants\nusing RadGame achieved a 68% improvement in localization accuracy compared to\n17% with traditional passive methods and a 31% improvement in report-writing\naccuracy compared to 4% with traditional methods after seeing the same cases.\nRadGame highlights the potential of AI-driven gamification to deliver scalable,\nfeedback-rich radiology training and reimagines the application of medical AI\nresources in education.", "AI": {"tldr": "RadGame是一个AI驱动的放射学教育游戏化平台，旨在提高定位病灶和撰写报告的核心技能，并提供可扩展的即时反馈。", "motivation": "传统的放射学培训缺乏即时和可扩展的反馈机会，限制了学习者获得及时指导。", "method": "RadGame结合了游戏化、大规模公共数据集和AI驱动的自动化反馈。它包含两个模块：RadGame Localize（玩家标注异常，AI与放射科医生标注对比并提供视觉解释）和RadGame Report（玩家撰写报告，AI根据报告生成指标提供结构化反馈，并生成表现和风格评分）。", "result": "在预期评估中，使用RadGame的参与者在定位准确性方面提高了68%（传统方法为17%），在报告撰写准确性方面提高了31%（传统方法为4%），显著优于传统被动学习方法。", "conclusion": "RadGame展示了AI驱动的游戏化在提供可扩展、反馈丰富的放射学培训方面的潜力，并重新构想了医学AI资源在教育中的应用。"}}
{"id": "2509.12866", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12866", "abs": "https://arxiv.org/abs/2509.12866", "authors": ["Martin Thißen", "Thi Ngoc Diep Tran", "Barbara Esteve Ratsch", "Ben Joel Schönbein", "Ute Trapp", "Beate Egner", "Romana Piat", "Elke Hergenröther"], "title": "Leveraging Large Language Models to Effectively Generate Visual Data for Canine Musculoskeletal Diagnoses", "comment": null, "summary": "It is well-established that more data generally improves AI model\nperformance. However, data collection can be challenging for certain tasks due\nto the rarity of occurrences or high costs. These challenges are evident in our\nuse case, where we apply AI models to a novel approach for visually documenting\nthe musculoskeletal condition of dogs. Here, abnormalities are marked as\ncolored strokes on a body map of a dog. Since these strokes correspond to\ndistinct muscles or joints, they can be mapped to the textual domain in which\nlarge language models (LLMs) operate. LLMs have demonstrated impressive\ncapabilities across a wide range of tasks, including medical applications,\noffering promising potential for generating synthetic training data. In this\nwork, we investigate whether LLMs can effectively generate synthetic visual\ntraining data for canine musculoskeletal diagnoses. For this, we developed a\nmapping that segments visual documentations into over 200 labeled regions\nrepresenting muscles or joints. Using techniques like guided decoding,\nchain-of-thought reasoning, and few-shot prompting, we generated 1,000\nsynthetic visual documentations for patellar luxation (kneecap dislocation)\ndiagnosis, the diagnosis for which we have the most real-world data. Our\nanalysis shows that the generated documentations are sensitive to location and\nseverity of the diagnosis while remaining independent of the dog's sex. We\nfurther generated 1,000 visual documentations for various other diagnoses to\ncreate a binary classification dataset. A model trained solely on this\nsynthetic data achieved an F1 score of 88% on 70 real-world documentations.\nThese results demonstrate the potential of LLM-generated synthetic data, which\nis particularly valuable for addressing data scarcity in rare diseases. While\nour methodology is tailored to the medical domain, the insights and techniques\ncan be adapted to other fields.", "AI": {"tldr": "本研究探讨了使用大型语言模型（LLMs）为犬类肌肉骨骼疾病诊断生成合成视觉训练数据的潜力，以解决数据稀缺问题，并展示了其在真实世界数据上的有效性。", "motivation": "人工智能模型性能通常随数据量增加而提升，但在某些任务中，由于事件罕见或成本高昂，数据收集面临挑战。本研究的应用场景——犬类肌肉骨骼状况的视觉记录——就存在这种挑战。LLMs在文本领域表现出色，有望生成合成训练数据来克服这一难题。", "method": "研究开发了一种映射机制，将犬只身体图上的视觉异常标记（彩色笔触）分割成200多个代表肌肉或关节的带标签区域，从而将其转换为LLMs可操作的文本域。利用引导解码、思维链推理和少样本提示等技术，生成了1,000份髌骨脱位（拥有最多真实数据）的合成视觉文档，并额外生成了1,000份其他诊断的视觉文档以创建二元分类数据集。", "result": "分析显示，生成的髌骨脱位文档对诊断的位置和严重程度敏感，但与犬只性别无关。一个完全由合成数据训练的模型，在70份真实世界文档上达到了88%的F1分数。", "conclusion": "研究结果表明，由LLMs生成的合成数据具有巨大潜力，尤其对于解决罕见疾病中的数据稀缺问题具有重要价值。尽管本方法针对医学领域，但其见解和技术可推广应用于其他领域。"}}
{"id": "2509.12871", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12871", "abs": "https://arxiv.org/abs/2509.12871", "authors": ["Avinaash Manoharan", "Xiangyu Yin", "Domenik Helm", "Chih-Hong Cheng"], "title": "Cumulative Consensus Score: Label-Free and Model-Agnostic Evaluation of Object Detectors in Deployment", "comment": null, "summary": "Evaluating object detection models in deployment is challenging because\nground-truth annotations are rarely available. We introduce the Cumulative\nConsensus Score (CCS), a label-free metric that enables continuous monitoring\nand comparison of detectors in real-world settings. CCS applies test-time data\naugmentation to each image, collects predicted bounding boxes across augmented\nviews, and computes overlaps using Intersection over Union. Maximum overlaps\nare normalized and averaged across augmentation pairs, yielding a measure of\nspatial consistency that serves as a proxy for reliability without annotations.\nIn controlled experiments on Open Images and KITTI, CCS achieved over 90%\ncongruence with F1-score, Probabilistic Detection Quality, and Optimal\nCorrection Cost. The method is model-agnostic, working across single-stage and\ntwo-stage detectors, and operates at the case level to highlight\nunder-performing scenarios. Altogether, CCS provides a robust foundation for\nDevOps-style monitoring of object detectors.", "AI": {"tldr": "本文提出累积共识分数（CCS），这是一种无需标签的度量标准，用于在部署环境中持续监控和比较目标检测模型，通过评估空间一致性来衡量可靠性。", "motivation": "在实际部署中，由于缺乏真实标注，评估目标检测模型具有挑战性。", "method": "CCS对每张图像应用测试时数据增强，收集增强视图的预测边界框，使用IoU计算重叠。最大重叠经过归一化并对所有增强对取平均，从而得到一个空间一致性度量，作为无需标注的可靠性代理。", "result": "在Open Images和KITTI数据集上的受控实验表明，CCS与F1分数、概率检测质量（PDQ）和最优校正成本（OCC）的吻合度超过90%。该方法与模型无关，适用于单阶段和两阶段检测器，并能识别性能不佳的场景。", "conclusion": "CCS为目标检测器的DevOps风格监控提供了一个稳健的基础。"}}
{"id": "2509.12878", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12878", "abs": "https://arxiv.org/abs/2509.12878", "authors": ["Qianguang Zhao", "Dongli Wang", "Yan Zhou", "Jianxun Li", "Richard Irampa"], "title": "Few to Big: Prototype Expansion Network via Diffusion Learner for Point Cloud Few-shot Semantic Segmentation", "comment": null, "summary": "Few-shot 3D point cloud semantic segmentation aims to segment novel\ncategories using a minimal number of annotated support samples. While existing\nprototype-based methods have shown promise, they are constrained by two\ncritical challenges: (1) Intra-class Diversity, where a prototype's limited\nrepresentational capacity fails to cover a class's full variations, and (2)\nInter-set Inconsistency, where prototypes derived from the support set are\nmisaligned with the query feature space. Motivated by the powerful generative\ncapability of diffusion model, we re-purpose its pre-trained conditional\nencoder to provide a novel source of generalizable features for expanding the\nprototype's representational range. Under this setup, we introduce the\nPrototype Expansion Network (PENet), a framework that constructs big-capacity\nprototypes from two complementary feature sources. PENet employs a dual-stream\nlearner architecture: it retains a conventional fully supervised Intrinsic\nLearner (IL) to distill representative features, while introducing a novel\nDiffusion Learner (DL) to provide rich generalizable features. The resulting\ndual prototypes are then processed by a Prototype Assimilation Module (PAM),\nwhich adopts a novel push-pull cross-guidance attention block to iteratively\nalign the prototypes with the query space. Furthermore, a Prototype Calibration\nMechanism (PCM) regularizes the final big capacity prototype to prevent\nsemantic drift. Extensive experiments on the S3DIS and ScanNet datasets\ndemonstrate that PENet significantly outperforms state-of-the-art methods\nacross various few-shot settings.", "AI": {"tldr": "本文提出PENet框架，通过利用扩散模型预训练编码器和双流学习器构建大容量原型，解决了少样本3D点云语义分割中类内多样性和集合间不一致性问题，并显著优于现有方法。", "motivation": "现有的基于原型的少样本3D点云语义分割方法存在两个主要挑战：1) 类内多样性，即原型有限的表示能力无法覆盖类别所有变体；2) 集合间不一致性，即支持集导出的原型与查询特征空间不匹配。受扩散模型强大生成能力的启发，作者旨在利用其预训练条件编码器提供可泛化特征，以扩展原型的表示范围。", "method": "本文提出了原型扩展网络（PENet），一个从两个互补特征源构建大容量原型的框架。PENet采用双流学习器架构：保留一个传统的全监督内在学习器（IL）来提取代表性特征，同时引入一个新颖的扩散学习器（DL）来提供丰富的可泛化特征。然后，生成的双原型由原型同化模块（PAM）处理，该模块采用推拉交叉引导注意力块迭代地将原型与查询空间对齐。此外，原型校准机制（PCM）对最终的大容量原型进行正则化，以防止语义漂移。", "result": "在S3DIS和ScanNet数据集上进行的广泛实验表明，PENet在各种少样本设置下显著优于最先进的方法。", "conclusion": "PENet通过结合扩散模型的预训练编码器和双流学习架构，有效解决了少样本3D点云语义分割中原型表示不足和特征空间不一致的问题，并通过原型同化和校准机制进一步增强了性能。"}}
{"id": "2509.12883", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12883", "abs": "https://arxiv.org/abs/2509.12883", "authors": ["Qifei Jia", "Yu Liu", "Yajie Chai", "Xintong Yao", "Qiming Lu", "Yasen Zhang", "Runyu Shi", "Ying Huang", "Guoquan Zhang"], "title": "Lego-Edit: A General Image Editing Framework with Model-Level Bricks and MLLM Builder", "comment": null, "summary": "Instruction-based image editing has garnered significant attention due to its\ndirect interaction with users. However, real-world user instructions are\nimmensely diverse, and existing methods often fail to generalize effectively to\ninstructions outside their training domain, limiting their practical\napplication. To address this, we propose Lego-Edit, which leverages the\ngeneralization capability of Multi-modal Large Language Model (MLLM) to\norganize a suite of model-level editing tools to tackle this challenge.\nLego-Edit incorporates two key designs: (1) a model-level toolkit comprising\ndiverse models efficiently trained on limited data and several image\nmanipulation functions, enabling fine-grained composition of editing actions by\nthe MLLM; and (2) a three-stage progressive reinforcement learning approach\nthat uses feedback on unannotated, open-domain instructions to train the MLLM,\nequipping it with generalized reasoning capabilities for handling real-world\ninstructions. Experiments demonstrate that Lego-Edit achieves state-of-the-art\nperformance on GEdit-Bench and ImgBench. It exhibits robust reasoning\ncapabilities for open-domain instructions and can utilize newly introduced\nediting tools without additional fine-tuning.\n  Code is available: https://github.com/xiaomi-research/lego-edit.", "AI": {"tldr": "Lego-Edit提出了一种利用多模态大语言模型（MLLM）来组织模型级编辑工具的框架，以解决现有指令式图像编辑方法在处理多样化、开放域指令时泛化能力不足的问题。", "motivation": "指令式图像编辑因其直接的用户交互而备受关注，但实际用户指令的多样性极高，现有方法往往难以有效泛化到训练域之外的指令，限制了其实用性。", "method": "Lego-Edit包含两项关键设计：1) 一个模型级工具包，由高效训练的多种模型和图像操作功能组成，支持MLLM进行细粒度的编辑动作组合；2) 一个三阶段渐进式强化学习方法，利用未标注的开放域指令反馈来训练MLLM，赋予其处理真实世界指令的泛化推理能力。", "result": "实验证明，Lego-Edit在GEdit-Bench和ImgBench上实现了最先进的性能。它对开放域指令表现出强大的推理能力，并且无需额外微调即可利用新引入的编辑工具。", "conclusion": "Lego-Edit通过结合MLLM的泛化能力和模型级工具包，有效解决了指令式图像编辑在处理多样化、开放域指令时的泛化挑战，并展现出卓越的性能和灵活性。"}}
{"id": "2509.12893", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12893", "abs": "https://arxiv.org/abs/2509.12893", "authors": ["Yiyi Zhang", "Yuchen Yuan", "Ying Zheng", "Jialun Pei", "Jinpeng Li", "Zheng Li", "Pheng-Ann Heng"], "title": "MEJO: MLLM-Engaged Surgical Triplet Recognition via Inter- and Intra-Task Joint Optimization", "comment": null, "summary": "Surgical triplet recognition, which involves identifying instrument, verb,\ntarget, and their combinations, is a complex surgical scene understanding\nchallenge plagued by long-tailed data distribution. The mainstream multi-task\nlearning paradigm benefiting from cross-task collaborative promotion has shown\npromising performance in identifying triples, but two key challenges remain: 1)\ninter-task optimization conflicts caused by entangling task-generic and\ntask-specific representations; 2) intra-task optimization conflicts due to\nclass-imbalanced training data. To overcome these difficulties, we propose the\nMLLM-Engaged Joint Optimization (MEJO) framework that empowers both inter- and\nintra-task optimization for surgical triplet recognition. For inter-task\noptimization, we introduce the Shared-Specific-Disentangled (S$^2$D) learning\nscheme that decomposes representations into task-shared and task-specific\ncomponents. To enhance task-shared representations, we construct a Multimodal\nLarge Language Model (MLLM) powered probabilistic prompt pool to dynamically\naugment visual features with expert-level semantic cues. Additionally,\ncomprehensive task-specific cues are modeled via distinct task prompts covering\nthe temporal-spatial dimensions, effectively mitigating inter-task ambiguities.\nTo tackle intra-task optimization conflicts, we develop a Coordinated Gradient\nLearning (CGL) strategy, which dissects and rebalances the positive-negative\ngradients originating from head and tail classes for more coordinated learning\nbehaviors. Extensive experiments on the CholecT45 and CholecT50 datasets\ndemonstrate the superiority of our proposed framework, validating its\neffectiveness in handling optimization conflicts.", "AI": {"tldr": "本文提出了一种名为MEJO的框架，用于解决手术三元组识别中因长尾数据分布和多任务学习范式导致的跨任务和任务内优化冲突。MEJO通过S^2D学习方案解耦共享和特定任务表示（利用MLLM增强共享表示，使用任务提示建模特定表示），并通过CGL策略协调处理任务内不平衡类别的梯度，在CholecT45和CholecT50数据集上表现出优越性。", "motivation": "手术三元组识别（识别器械、动作、目标及其组合）是一个复杂的手术场景理解挑战，存在长尾数据分布问题。主流的多任务学习范式在识别三元组方面表现出潜力，但仍面临两大挑战：1) 任务通用和任务特定表示纠缠导致的跨任务优化冲突；2) 类别不平衡训练数据导致的任务内优化冲突。", "method": "本文提出了MLLM-Engaged Joint Optimization (MEJO) 框架。为解决跨任务优化冲突，引入了Shared-Specific-Disentangled (S^2D) 学习方案，将表示分解为任务共享和任务特定组件。其中，任务共享表示通过一个由多模态大语言模型 (MLLM) 驱动的概率提示池进行动态增强，以引入专家级语义线索；任务特定线索则通过涵盖时空维度的独立任务提示进行建模。为解决任务内优化冲突，开发了Coordinated Gradient Learning (CGL) 策略，该策略剖析并重新平衡来自头部和尾部类别的正负梯度，以实现更协调的学习行为。", "result": "在CholecT45和CholecT50数据集上进行的广泛实验证明了所提出框架的优越性，并验证了其在处理优化冲突方面的有效性。", "conclusion": "MEJO框架通过解耦跨任务表示和协调任务内梯度，成功解决了手术三元组识别中的优化冲突，显著提升了识别性能。"}}
{"id": "2509.12894", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12894", "abs": "https://arxiv.org/abs/2509.12894", "authors": ["Leekyeung Han", "Hyunji Min", "Gyeom Hwangbo", "Jonghyun Choi", "Paul Hongsuck Seo"], "title": "DialNav: Multi-turn Dialog Navigation with a Remote Guide", "comment": "18 pages, 8 figures, ICCV 2025", "summary": "We introduce DialNav, a novel collaborative embodied dialog task, where a\nnavigation agent (Navigator) and a remote guide (Guide) engage in multi-turn\ndialog to reach a goal location. Unlike prior work, DialNav aims for holistic\nevaluation and requires the Guide to infer the Navigator's location, making\ncommunication essential for task success. To support this task, we collect and\nrelease the Remote Assistance in Navigation (RAIN) dataset, human-human dialog\npaired with navigation trajectories in photorealistic environments. We design a\ncomprehensive benchmark to evaluate both navigation and dialog, and conduct\nextensive experiments analyzing the impact of different Navigator and Guide\nmodels. We highlight key challenges and publicly release the dataset, code, and\nevaluation framework to foster future research in embodied dialog.", "AI": {"tldr": "本文提出了DialNav，一项新颖的具身对话任务，其中导航员和远程向导通过多轮对话协作到达目标位置。该任务要求向导推断导航员的位置，并发布了RAIN数据集、基准和代码以促进研究。", "motivation": "现有研究缺乏全面的评估，且在具身导航对话中，向导需要推断导航员的位置，这使得有效沟通对任务成功至关重要。", "method": "引入了DialNav协作具身对话任务，收集并发布了RAIN数据集（包含在真实感环境中人类-人类对话与导航轨迹），设计了评估导航和对话的综合基准，并进行了大量实验分析不同导航员和向导模型的影响。", "result": "实验分析了不同导航员和向导模型对任务表现的影响，并突出了该领域面临的关键挑战。", "conclusion": "DialNav是一个新颖的具身对话任务，其核心在于向导需推断导航员位置。研究团队发布了RAIN数据集、代码和评估框架，旨在推动未来具身对话领域的研究。"}}
{"id": "2509.12901", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12901", "abs": "https://arxiv.org/abs/2509.12901", "authors": ["Guihui Li", "Bowei Dong", "Kaizhi Dong", "Jiayi Li", "Haiyong Zheng"], "title": "MSGFusion: Multimodal Scene Graph-Guided Infrared and Visible Image Fusion", "comment": null, "summary": "Infrared and visible image fusion has garnered considerable attention owing\nto the strong complementarity of these two modalities in complex, harsh\nenvironments. While deep learning-based fusion methods have made remarkable\nadvances in feature extraction, alignment, fusion, and reconstruction, they\nstill depend largely on low-level visual cues, such as texture and contrast,\nand struggle to capture the high-level semantic information embedded in images.\nRecent attempts to incorporate text as a source of semantic guidance have\nrelied on unstructured descriptions that neither explicitly model entities,\nattributes, and relationships nor provide spatial localization, thereby\nlimiting fine-grained fusion performance. To overcome these challenges, we\nintroduce MSGFusion, a multimodal scene graph-guided fusion framework for\ninfrared and visible imagery. By deeply coupling structured scene graphs\nderived from text and vision, MSGFusion explicitly represents entities,\nattributes, and spatial relations, and then synchronously refines high-level\nsemantics and low-level details through successive modules for scene graph\nrepresentation, hierarchical aggregation, and graph-driven fusion. Extensive\nexperiments on multiple public benchmarks show that MSGFusion significantly\noutperforms state-of-the-art approaches, particularly in detail preservation\nand structural clarity, and delivers superior semantic consistency and\ngeneralizability in downstream tasks such as low-light object detection,\nsemantic segmentation, and medical image fusion.", "AI": {"tldr": "MSGFusion是一种多模态场景图引导的红外与可见光图像融合框架，通过深度耦合结构化场景图来提升融合图像的细节、结构清晰度及语义一致性。", "motivation": "现有基于深度学习的图像融合方法在特征提取方面取得进展，但仍过度依赖低级视觉线索（如纹理和对比度），难以捕捉图像中嵌入的高级语义信息。近期结合文本作为语义指导的方法依赖非结构化描述，未能明确建模实体、属性和关系，也未提供空间定位，从而限制了细粒度融合性能。", "method": "本文提出了MSGFusion框架，通过深度耦合从文本和视觉中提取的结构化场景图，显式表示实体、属性和空间关系。然后，通过场景图表示、分层聚合和图驱动融合等模块，同步细化高级语义和低级细节。", "result": "在多个公共基准上的广泛实验表明，MSGFusion显著优于最先进的方法，特别是在细节保留和结构清晰度方面。它还在低光照目标检测、语义分割和医学图像融合等下游任务中提供了卓越的语义一致性和泛化能力。", "conclusion": "结构化场景图能够有效提升红外与可见光图像融合的性能，尤其是在保留细节、增强结构清晰度以及提供更强的语义一致性和泛化能力方面，为复杂环境下的图像融合提供了新的解决方案。"}}
{"id": "2509.12905", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12905", "abs": "https://arxiv.org/abs/2509.12905", "authors": ["Branko Mitic", "Philipp Seeböck", "Helmut Prosch", "Georg Langs"], "title": "AREPAS: Anomaly Detection in Fine-Grained Anatomy with Reconstruction-Based Semantic Patch-Scoring", "comment": null, "summary": "Early detection of newly emerging diseases, lesion severity assessment,\ndifferentiation of medical conditions and automated screening are examples for\nthe wide applicability and importance of anomaly detection (AD) and\nunsupervised segmentation in medicine. Normal fine-grained tissue variability\nsuch as present in pulmonary anatomy is a major challenge for existing\ngenerative AD methods. Here, we propose a novel generative AD approach\naddressing this issue. It consists of an image-to-image translation for\nanomaly-free reconstruction and a subsequent patch similarity scoring between\nobserved and generated image-pairs for precise anomaly localization. We\nvalidate the new method on chest computed tomography (CT) scans for the\ndetection and segmentation of infectious disease lesions. To assess\ngeneralizability, we evaluate the method on an ischemic stroke lesion\nsegmentation task in T1-weighted brain MRI. Results show improved pixel-level\nanomaly segmentation in both chest CTs and brain MRIs, with relative DICE score\nimprovements of +1.9% and +4.4%, respectively, compared to other\nstate-of-the-art reconstruction-based methods.", "AI": {"tldr": "本文提出了一种新颖的生成式异常检测方法，通过图像到图像翻译和块相似性评分，解决了医学图像中细粒度组织变异性带来的挑战，并在胸部CT和脑部MRI上实现了像素级异常分割的改进。", "motivation": "异常检测和无监督分割在医学领域具有广泛应用（如疾病早期检测、病变严重性评估、疾病鉴别和自动化筛查），但现有生成式异常检测方法难以处理医学图像中正常的细粒度组织变异性（例如肺部解剖结构）。", "method": "该方法是一种生成式异常检测新方法，包括两个主要步骤：首先进行图像到图像翻译以实现无异常重建；然后对观察到的图像和生成的图像对之间进行块相似性评分，以实现精确的异常定位。", "result": "该方法在胸部CT（用于感染性疾病病变检测与分割）和T1加权脑部MRI（用于缺血性卒中病变分割）上均表现出改进的像素级异常分割性能。与现有最先进的基于重建的方法相比，DICE分数分别相对提高了+1.9%和+4.4%。", "conclusion": "所提出的生成式异常检测方法能够有效应对医学图像中细粒度组织变异性的挑战，并在不同医学影像模态和任务中展现出优越的异常检测和分割性能及泛化能力。"}}
{"id": "2509.12913", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12913", "abs": "https://arxiv.org/abs/2509.12913", "authors": ["Hojat Ardi", "Amir Jahanshahi", "Ali Diba"], "title": "T-SiamTPN: Temporal Siamese Transformer Pyramid Networks for Robust and Efficient UAV Tracking", "comment": null, "summary": "Aerial object tracking remains a challenging task due to scale variations,\ndynamic backgrounds, clutter, and frequent occlusions. While most existing\ntrackers emphasize spatial cues, they often overlook temporal dependencies,\nresulting in limited robustness in long-term tracking and under occlusion.\nFurthermore, correlation-based Siamese trackers are inherently constrained by\nthe linear nature of correlation operations, making them ineffective against\ncomplex, non-linear appearance changes. To address these limitations, we\nintroduce T-SiamTPN, a temporal-aware Siamese tracking framework that extends\nthe SiamTPN architecture with explicit temporal modeling. Our approach\nincorporates temporal feature fusion and attention-based interactions,\nstrengthening temporal consistency and enabling richer feature representations.\nThese enhancements yield significant improvements over the baseline and achieve\nperformance competitive with state-of-the-art trackers. Crucially, despite the\nadded temporal modules, T-SiamTPN preserves computational efficiency. Deployed\non the resource-constrained Jetson Nano, the tracker runs in real time at 7.1\nFPS, demonstrating its suitability for real-world embedded applications without\nnotable runtime overhead. Experimental results highlight substantial gains:\ncompared to the baseline, T-SiamTPN improves success rate by 13.7% and\nprecision by 14.7%. These findings underscore the importance of temporal\nmodeling in Siamese tracking frameworks and establish T-SiamTPN as a strong and\nefficient solution for aerial object tracking. Code is available at:\nhttps://github.com/to/be/released", "AI": {"tldr": "本文提出T-SiamTPN，一个时间感知的Siamese跟踪框架，通过显式时间建模、特征融合和注意力机制，显著提升了空中目标跟踪的鲁棒性和精度，同时保持了计算效率，适用于嵌入式应用。", "motivation": "空中目标跟踪面临尺度变化、动态背景、杂波和频繁遮挡等挑战。现有跟踪器多侧重空间线索，忽视时间依赖性，导致长期跟踪和遮挡下鲁棒性不足。此外，基于相关性的Siamese跟踪器受限于相关操作的线性性质，难以应对复杂的非线性外观变化。", "method": "本文引入T-SiamTPN，一个时间感知的Siamese跟踪框架，通过显式时间建模扩展了SiamTPN架构。该方法整合了时间特征融合和基于注意力的交互，以增强时间一致性并实现更丰富的特征表示。", "result": "T-SiamTPN在基线上取得了显著改进，性能与最先进的跟踪器相当。尽管增加了时间模块，但仍保持了计算效率，在资源受限的Jetson Nano上能以7.1 FPS实时运行。实验结果显示，T-SiamTPN相比基线，成功率提高了13.7%，精度提高了14.7%。", "conclusion": "这些发现强调了时间建模在Siamese跟踪框架中的重要性，并确立T-SiamTPN作为一种强大且高效的空中目标跟踪解决方案。"}}
{"id": "2509.12918", "categories": ["cs.CV", "68T07", "I.4.8"], "pdf": "https://arxiv.org/pdf/2509.12918", "abs": "https://arxiv.org/abs/2509.12918", "authors": ["Melika Sabaghian", "Mohammad Ali Keyvanrad", "Seyyedeh Mahila Moghadami"], "title": "A Novel Compression Framework for YOLOv8: Achiev-ing Real-Time Aerial Object Detection on Edge Devices via Structured Pruning and Channel-Wise Distillation", "comment": "28 pages, 11 figures", "summary": "Efficient deployment of deep learning models for aerial object detection on\nresource-constrained devices requires significant compression without\ncom-promising performance. In this study, we propose a novel three-stage\ncompression pipeline for the YOLOv8 object detection model, integrating\nsparsity-aware training, structured channel pruning, and Channel-Wise Knowledge\nDistillation (CWD). First, sparsity-aware training introduces dynamic sparsity\nduring model optimization, effectively balancing parameter reduction and\ndetection accuracy. Second, we apply structured channel pruning by leveraging\nbatch normalization scaling factors to eliminate redundant channels,\nsignificantly reducing model size and computational complexity. Finally, to\nmitigate the accuracy drop caused by pruning, we employ CWD to transfer\nknowledge from the original model, using an adjustable temperature and loss\nweighting scheme tailored for small and medium object detection. Extensive\nexperiments on the VisDrone dataset demonstrate the effectiveness of our\napproach across multiple YOLOv8 variants. For YOLOv8m, our method reduces model\nparameters from 25.85M to 6.85M (a 73.51% reduction), FLOPs from 49.6G to\n13.3G, and MACs from 101G to 34.5G, while reducing AP50 by only 2.7%. The\nresulting compressed model achieves 47.9 AP50 and boosts inference speed from\n26 FPS (YOLOv8m baseline) to 45 FPS, enabling real-time deployment on edge\ndevices. We further apply TensorRT as a lightweight optimization step. While\nthis introduces a minor drop in AP50 (from 47.9 to 47.6), it significantly\nimproves inference speed from 45 to 68 FPS, demonstrating the practicality of\nour approach for high-throughput, re-source-constrained scenarios.", "AI": {"tldr": "该研究提出了一种针对YOLOv8模型的三阶段压缩流水线，结合稀疏感知训练、结构化通道剪枝和通道级知识蒸馏，显著减少了模型大小和计算量，同时保持了高精度，实现了在资源受限设备上的实时空中目标检测。", "motivation": "在资源受限设备上高效部署用于空中目标检测的深度学习模型，需要在不牺牲性能的前提下进行显著的模型压缩。", "method": "该方法包含一个新颖的三阶段压缩流水线：1. 稀疏感知训练：在模型优化过程中引入动态稀疏性，平衡参数减少和检测精度。2. 结构化通道剪枝：利用批归一化缩放因子消除冗余通道，大幅减少模型大小和计算复杂性。3. 通道级知识蒸馏（CWD）：采用可调节的温度和损失权重方案，将知识从原始模型迁移到剪枝后的模型，以缓解精度下降，特别针对小型和中型目标检测进行了优化。最后，还应用了TensorRT进行轻量级优化。", "result": "在VisDrone数据集上，对于YOLOv8m模型，该方法将模型参数从25.85M减少到6.85M（减少73.51%），FLOPs从49.6G减少到13.3G，MACs从101G减少到34.5G，而AP50仅下降2.7%。压缩后的模型实现了47.9的AP50，推理速度从26 FPS（YOLOv8m基线）提升到45 FPS。通过TensorRT优化，AP50略降至47.6，但推理速度进一步提升至68 FPS。", "conclusion": "该三阶段压缩方法在YOLOv8模型上表现出卓越的有效性，显著减少了模型规模和计算量，同时保持了高检测精度和推理速度，使其在资源受限的边缘设备上实现实时、高吞吐量的空中目标检测变得可行。"}}
{"id": "2509.12924", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12924", "abs": "https://arxiv.org/abs/2509.12924", "authors": ["Shipeng Liu", "Ziliang Xiong", "Khac-Hoang Ngo", "Per-Erik Forssén"], "title": "MATTER: Multiscale Attention for Registration Error Regression", "comment": null, "summary": "Point cloud registration (PCR) is crucial for many downstream tasks, such as\nsimultaneous localization and mapping (SLAM) and object tracking. This makes\ndetecting and quantifying registration misalignment, i.e.,~{\\it PCR quality\nvalidation}, an important task. All existing methods treat validation as a\nclassification task, aiming to assign the PCR quality to a few classes. In this\nwork, we instead use regression for PCR validation, allowing for a more\nfine-grained quantification of the registration quality. We also extend\npreviously used misalignment-related features by using multiscale extraction\nand attention-based aggregation. This leads to accurate and robust registration\nerror estimation on diverse datasets, especially for point clouds with\nheterogeneous spatial densities. Furthermore, when used to guide a mapping\ndownstream task, our method significantly improves the mapping quality for a\ngiven amount of re-registered frames, compared to the state-of-the-art\nclassification-based method.", "AI": {"tldr": "本文提出了一种基于回归的点云配准质量验证方法，通过多尺度特征提取和注意力机制聚合，实现了对配准误差的精细量化和鲁棒估计，显著提升了下游建图任务的质量。", "motivation": "点云配准（PCR）对SLAM和目标跟踪等下游任务至关重要，因此检测和量化配准失准（即PCR质量验证）是一项重要任务。现有方法将验证视为分类任务，无法提供细粒度的配准质量量化。", "method": "本研究将PCR质量验证视为回归任务，以实现更细粒度的质量量化。方法扩展了现有与失准相关的特征，采用多尺度提取和基于注意力的聚合机制。", "result": "该方法在多样化数据集上，尤其是在空间密度不均匀的点云上，实现了准确且鲁棒的配准误差估计。此外，当用于指导下游建图任务时，与最先进的基于分类的方法相比，在相同数量的重新配准帧下，显著提高了建图质量。", "conclusion": "通过采用回归方法并结合多尺度特征提取和注意力聚合，本研究为点云配准质量验证提供了一种更精细、准确和鲁棒的量化方案，并能有效提升下游任务的性能。"}}
{"id": "2509.12931", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12931", "abs": "https://arxiv.org/abs/2509.12931", "authors": ["Xiao Tang", "Guirong Zhuo", "Cong Wang", "Boyuan Zheng", "Minqing Huang", "Lianqing Zheng", "Long Chen", "Shouyi Lu"], "title": "4DRadar-GS: Self-Supervised Dynamic Driving Scene Reconstruction with 4D Radar", "comment": null, "summary": "3D reconstruction and novel view synthesis are critical for validating\nautonomous driving systems and training advanced perception models. Recent\nself-supervised methods have gained significant attention due to their\ncost-effectiveness and enhanced generalization in scenarios where annotated\nbounding boxes are unavailable. However, existing approaches, which often rely\non frequency-domain decoupling or optical flow, struggle to accurately\nreconstruct dynamic objects due to imprecise motion estimation and weak\ntemporal consistency, resulting in incomplete or distorted representations of\ndynamic scene elements. To address these challenges, we propose 4DRadar-GS, a\n4D Radar-augmented self-supervised 3D reconstruction framework tailored for\ndynamic driving scenes. Specifically, we first present a 4D Radar-assisted\nGaussian initialization scheme that leverages 4D Radar's velocity and spatial\ninformation to segment dynamic objects and recover monocular depth scale,\ngenerating accurate Gaussian point representations. In addition, we propose a\nVelocity-guided PointTrack (VGPT) model, which is jointly trained with the\nreconstruction pipeline under scene flow supervision, to track fine-grained\ndynamic trajectories and construct temporally consistent representations.\nEvaluated on the OmniHD-Scenes dataset, 4DRadar-GS achieves state-of-the-art\nperformance in dynamic driving scene 3D reconstruction.", "AI": {"tldr": "本文提出4DRadar-GS，一个基于4D雷达增强的自监督3D重建框架，用于动态驾驶场景，通过雷达辅助的高斯初始化和速度引导的点轨迹跟踪，有效解决了动态物体重建不准确和时间一致性弱的问题。", "motivation": "现有的自监督3D重建方法在处理动态物体时，由于运动估计不精确和时间一致性差，导致动态场景元素重建不完整或扭曲。这限制了它们在自动驾驶系统验证和感知模型训练中的应用。", "method": "本文提出4DRadar-GS框架：1) 4D雷达辅助的高斯初始化方案，利用4D雷达的速度和空间信息分割动态物体并恢复单目深度尺度，生成精确的高斯点表示。2) 速度引导的点轨迹跟踪(VGPT)模型，在场景流监督下与重建管线联合训练，以跟踪细粒度动态轨迹并构建时间上一致的表示。", "result": "在OmniHD-Scenes数据集上进行评估，4DRadar-GS在动态驾驶场景3D重建方面取得了最先进的性能。", "conclusion": "4DRadar-GS框架通过结合4D雷达信息和改进的动态物体跟踪机制，成功克服了现有自监督3D重建方法在动态驾驶场景中面临的挑战，实现了更准确和时间一致的动态物体重建。"}}
{"id": "2509.12938", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12938", "abs": "https://arxiv.org/abs/2509.12938", "authors": ["Abdalla Arafa", "Didier Stricker"], "title": "Beyond Averages: Open-Vocabulary 3D Scene Understanding with Gaussian Splatting and Bag of Embeddings", "comment": null, "summary": "Novel view synthesis has seen significant advancements with 3D Gaussian\nSplatting (3DGS), enabling real-time photorealistic rendering. However, the\ninherent fuzziness of Gaussian Splatting presents challenges for 3D scene\nunderstanding, restricting its broader applications in AR/VR and robotics.\nWhile recent works attempt to learn semantics via 2D foundation model\ndistillation, they inherit fundamental limitations: alpha blending averages\nsemantics across objects, making 3D-level understanding impossible. We propose\na paradigm-shifting alternative that bypasses differentiable rendering for\nsemantics entirely. Our key insight is to leverage predecomposed object-level\nGaussians and represent each object through multiview CLIP feature aggregation,\ncreating comprehensive \"bags of embeddings\" that holistically describe objects.\nThis allows: (1) accurate open-vocabulary object retrieval by comparing text\nqueries to object-level (not Gaussian-level) embeddings, and (2) seamless task\nadaptation: propagating object IDs to pixels for 2D segmentation or to\nGaussians for 3D extraction. Experiments demonstrate that our method\neffectively overcomes the challenges of 3D open-vocabulary object extraction\nwhile remaining comparable to state-of-the-art performance in 2D\nopen-vocabulary segmentation, ensuring minimal compromise.", "AI": {"tldr": "该论文提出一种新范式，通过聚合预分解的对象级高斯和多视图CLIP特征来创建“嵌入包”，从而实现准确的开放词汇3D对象提取，解决了3D高斯溅射在语义理解方面的局限性。", "motivation": "3D高斯溅射（3DGS）虽然实现了实时逼真的渲染，但其固有的模糊性阻碍了3D场景理解，限制了其在AR/VR和机器人领域的应用。现有方法通过2D基础模型蒸馏学习语义，但由于alpha混合平均了对象间的语义，无法实现3D层面的理解。", "method": "我们提出一种绕过可微分渲染进行语义理解的替代方案。核心思想是利用预分解的对象级高斯，并通过多视图CLIP特征聚合来表示每个对象，创建全面的“嵌入包”来整体描述对象。这允许：1) 通过将文本查询与对象级（而非高斯级）嵌入进行比较，实现准确的开放词汇对象检索；2) 无缝任务适应：将对象ID传播到像素进行2D分割或传播到高斯进行3D提取。", "result": "实验表明，我们的方法有效克服了3D开放词汇对象提取的挑战，同时在2D开放词汇分割方面与最先进的性能相当，确保了最小的折衷。", "conclusion": "该方法通过引入对象级特征聚合的范式，成功解决了3D高斯溅射在3D语义理解和开放词汇对象提取方面的难题，同时保持了在2D分割任务上的竞争力，为3D场景理解开辟了新的途径。"}}
{"id": "2509.12959", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12959", "abs": "https://arxiv.org/abs/2509.12959", "authors": ["Yuqi Xie", "Shuhan Ye", "Chong Wang", "Jiazhen Xu", "Le Shen", "Yuanbin Qian", "Jiangbo Qian"], "title": "Time-step Mixup for Efficient Spiking Knowledge Transfer from Appearance to Event Domain", "comment": null, "summary": "The integration of event cameras and spiking neural networks holds great\npromise for energy-efficient visual processing. However, the limited\navailability of event data and the sparse nature of DVS outputs pose challenges\nfor effective training. Although some prior work has attempted to transfer\nsemantic knowledge from RGB datasets to DVS, they often overlook the\nsignificant distribution gap between the two modalities. In this paper, we\npropose Time-step Mixup knowledge transfer (TMKT), a novel fine-grained mixing\nstrategy that exploits the asynchronous nature of SNNs by interpolating RGB and\nDVS inputs at various time-steps. To enable label mixing in cross-modal\nscenarios, we further introduce modality-aware auxiliary learning objectives.\nThese objectives support the time-step mixup process and enhance the model's\nability to discriminate effectively across different modalities. Our approach\nenables smoother knowledge transfer, alleviates modality shift during training,\nand achieves superior performance in spiking image classification tasks.\nExtensive experiments demonstrate the effectiveness of our method across\nmultiple datasets. The code will be released after the double-blind review\nprocess.", "AI": {"tldr": "本文提出了一种名为时间步混合知识迁移（TMKT）的新方法，通过在不同时间步插值RGB和DVS输入，并结合模态感知辅助学习目标，解决了将语义知识从RGB数据集迁移到事件相机（DVS）数据的挑战，从而实现更平滑的知识迁移和在脉冲图像分类任务中更优异的性能。", "motivation": "事件相机与脉冲神经网络（SNNs）的结合在能效视觉处理方面具有巨大潜力。然而，有限的事件数据、DVS输出的稀疏性以及从RGB数据集迁移语义知识时存在的显著模态分布差异，给有效的训练带来了挑战。", "method": "本文提出了时间步混合知识迁移（TMKT）方法。该方法利用SNN的异步特性，通过在不同时间步插值RGB和DVS输入，实现了一种细粒度的混合策略。为了在跨模态场景中实现标签混合，还引入了模态感知辅助学习目标，以支持时间步混合过程并增强模型在不同模态间有效区分的能力。", "result": "该方法实现了更平滑的知识迁移，缓解了训练过程中的模态偏移，并在脉冲图像分类任务中取得了卓越的性能。大量的实验证明了该方法在多个数据集上的有效性。", "conclusion": "TMKT通过新颖的时间步混合策略和模态感知辅助学习目标，有效解决了事件相机和脉冲神经网络中RGB到DVS知识迁移的挑战，显著提升了脉冲图像分类任务的性能。"}}
{"id": "2509.12963", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.12963", "abs": "https://arxiv.org/abs/2509.12963", "authors": ["Robin Schön", "Julian Lorenz", "Katja Ludwig", "Daniel Kienzle", "Rainer Lienhart"], "title": "MMMS: Multi-Modal Multi-Surface Interactive Segmentation", "comment": "19 pages, 11 figures, 10 pages", "summary": "In this paper, we present a method to interactively create segmentation masks\non the basis of user clicks. We pay particular attention to the segmentation of\nmultiple surfaces that are simultaneously present in the same image. Since\nthese surfaces may be heavily entangled and adjacent, we also present a novel\nextended evaluation metric that accounts for the challenges of this scenario.\nAdditionally, the presented method is able to use multi-modal inputs to\nfacilitate the segmentation task. At the center of this method is a network\narchitecture which takes as input an RGB image, a number of non-RGB modalities,\nan erroneous mask, and encoded clicks. Based on this input, the network\npredicts an improved segmentation mask. We design our architecture such that it\nadheres to two conditions: (1) The RGB backbone is only available as a\nblack-box. (2) To reduce the response time, we want our model to integrate the\ninteraction-specific information after the image feature extraction and the\nmulti-modal fusion. We refer to the overall task as Multi-Modal Multi-Surface\ninteractive segmentation (MMMS). We are able to show the effectiveness of our\nmulti-modal fusion strategy. Using additional modalities, our system reduces\nthe NoC@90 by up to 1.28 clicks per surface on average on DeLiVER and up to\n1.19 on MFNet. On top of this, we are able to show that our RGB-only baseline\nachieves competitive, and in some cases even superior performance when tested\nin a classical, single-mask interactive segmentation scenario.", "AI": {"tldr": "本文提出了一种基于用户点击的交互式多模态多表面图像分割方法，通过特定网络架构处理纠缠表面，并引入新的评估指标，显著提升了分割效率和性能。", "motivation": "研究动机在于解决图像中同时存在多个可能高度纠缠和相邻表面的交互式分割难题，并探索利用多模态输入来辅助分割任务，以提高效率和准确性。", "method": "该方法的核心是一个网络架构，输入包括RGB图像、非RGB模态、错误掩码和编码点击，输出是改进的分割掩码。架构设计遵循两个条件：1) RGB主干作为黑盒；2) 为减少响应时间，交互特定信息在图像特征提取和多模态融合后集成。此外，论文还提出了一种新的扩展评估指标来应对多表面纠缠场景的挑战。该任务被称为多模态多表面交互式分割（MMMS）。", "result": "研究结果表明多模态融合策略有效，额外的模态输入使系统在DeLiVER数据集上平均每表面NoC@90减少高达1.28次点击，在MFNet上减少高达1.19次。此外，纯RGB基线在经典的单掩码交互式分割场景中表现出有竞争力，甚至在某些情况下更优的性能。", "conclusion": "本文提出的多模态多表面交互式分割方法能够有效处理复杂的多表面分割任务，通过多模态融合显著提高了分割效率，并且其RGB基线在传统场景下也表现出色。"}}
{"id": "2509.12965", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12965", "abs": "https://arxiv.org/abs/2509.12965", "authors": ["Silvia Zottin", "Axel De Nardin", "Giuseppe Branca", "Claudio Piciarelli", "Gian Luca Foresti"], "title": "ICDAR 2025 Competition on FEw-Shot Text line segmentation of ancient handwritten documents (FEST)", "comment": "Accepted to ICDAR 2025", "summary": "Text line segmentation is a critical step in handwritten document image\nanalysis. Segmenting text lines in historical handwritten documents, however,\npresents unique challenges due to irregular handwriting, faded ink, and complex\nlayouts with overlapping lines and non-linear text flow. Furthermore, the\nscarcity of large annotated datasets renders fully supervised learning\napproaches impractical for such materials. To address these challenges, we\nintroduce the Few-Shot Text Line Segmentation of Ancient Handwritten Documents\n(FEST) Competition. Participants are tasked with developing systems capable of\nsegmenting text lines in U-DIADS-TL dataset, using only three annotated images\nper manuscript for training. The competition dataset features a diverse\ncollection of ancient manuscripts exhibiting a wide range of layouts,\ndegradation levels, and non-standard formatting, closely reflecting real-world\nconditions. By emphasizing few-shot learning, FEST competition aims to promote\nthe development of robust and adaptable methods that can be employed by\nhumanities scholars with minimal manual annotation effort, thus fostering\nbroader adoption of automated document analysis tools in historical research.", "AI": {"tldr": "本文介绍了一项名为FEST的比赛，旨在解决古代手写文档中文字行分割的挑战，特别是在标注数据稀缺的条件下，通过少量样本学习（few-shot learning）来开发鲁棒且适应性强的分割方法。", "motivation": "历史手写文档的文字行分割面临独特挑战，如不规则笔迹、墨迹褪色、复杂布局、行重叠和非线性文本流。此外，缺乏大型标注数据集使得全监督学习方法不切实际，因此需要少量样本学习方法。", "method": "本文提出并组织了“古代手写文档少量样本文字行分割（FEST）”竞赛。参赛者需开发系统，仅使用每份手稿三张标注图像对U-DIADS-TL数据集进行训练，以分割文字行。", "result": "竞赛数据集包含多样化的古代手稿，涵盖了广泛的布局、退化程度和非标准格式，反映了真实世界的条件。竞赛旨在促进开发出鲁棒且适应性强的方法。", "conclusion": "FEST竞赛旨在推动少量样本学习方法的发展，使人文科学学者能够以最少的手动标注工作使用这些工具，从而促进自动化文档分析工具在历史研究中的广泛应用。"}}
{"id": "2509.12976", "categories": ["cs.CV", "q-bio.BM", "I.3.8; I.5.4; J.3"], "pdf": "https://arxiv.org/pdf/2509.12976", "abs": "https://arxiv.org/abs/2509.12976", "authors": ["Taher Yacoub", "Camille Depenveiller", "Atsushi Tatsuma", "Tin Barisin", "Eugen Rusakov", "Udo Gobel", "Yuxu Peng", "Shiqiang Deng", "Yuki Kagaya", "Joon Hong Park", "Daisuke Kihara", "Marco Guerra", "Giorgio Palmieri", "Andrea Ranieri", "Ulderico Fugacci", "Silvia Biasotti", "Ruiwen He", "Halim Benhabiles", "Adnane Cabani", "Karim Hammoudi", "Haotian Li", "Hao Huang", "Chunyan Li", "Alireza Tehrani", "Fanwang Meng", "Farnaz Heidar-Zadeh", "Tuan-Anh Yang", "Matthieu Montes"], "title": "SHREC 2025: Protein surface shape retrieval including electrostatic potential", "comment": "Published in Computers & Graphics, Elsevier. 59 pages, 12 figures", "summary": "This SHREC 2025 track dedicated to protein surface shape retrieval involved 9\nparticipating teams. We evaluated the performance in retrieval of 15 proposed\nmethods on a large dataset of 11,555 protein surfaces with calculated\nelectrostatic potential (a key molecular surface descriptor). The performance\nin retrieval of the proposed methods was evaluated through different metrics\n(Accuracy, Balanced accuracy, F1 score, Precision and Recall). The best\nretrieval performance was achieved by the proposed methods that used the\nelectrostatic potential complementary to molecular surface shape. This\nobservation was also valid for classes with limited data which highlights the\nimportance of taking into account additional molecular surface descriptors.", "AI": {"tldr": "SHREC 2025蛋白质表面形状检索赛道评估了15种方法，发现结合静电势的检索方法表现最佳，尤其在数据有限的类别中。", "motivation": "旨在评估和比较不同方法在蛋白质表面形状检索任务上的性能，并探索关键分子表面描述符（如静电势）的作用。", "method": "SHREC 2025赛道组织，9支团队参与，评估了15种方法。使用包含11,555个蛋白质表面及其静电势的大型数据集。通过准确率、平衡准确率、F1分数、精确率和召回率等指标评估检索性能。", "result": "结合静电势与分子表面形状的方法取得了最佳检索性能。这一优势在数据量有限的类别中也成立。", "conclusion": "静电势是蛋白质表面形状检索的关键分子表面描述符，其对检索性能的提升，尤其在数据稀缺的情况下，具有重要意义。"}}
{"id": "2509.12980", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.12980", "abs": "https://arxiv.org/abs/2509.12980", "authors": ["Hemanth Chandravamsi", "Dhanush V. Shenoy", "Steven H. Frankel"], "title": "Improving Accuracy and Efficiency of Implicit Neural Representations: Making SIREN a WINNER", "comment": null, "summary": "We identify and address a fundamental limitation of sinusoidal representation\nnetworks (SIRENs), a class of implicit neural representations. SIRENs Sitzmann\net al. (2020), when not initialized appropriately, can struggle at fitting\nsignals that fall outside their frequency support. In extreme cases, when the\nnetwork's frequency support misaligns with the target spectrum, a 'spectral\nbottleneck' phenomenon is observed, where the model yields to a near-zero\noutput and fails to recover even the frequency components that are within its\nrepresentational capacity. To overcome this, we propose WINNER - Weight\nInitialization with Noise for Neural Representations. WINNER perturbs uniformly\ninitialized weights of base SIREN with Gaussian noise - whose noise scales are\nadaptively determined by the spectral centroid of the target signal. Similar to\nrandom Fourier embeddings, this mitigates 'spectral bias' but without\nintroducing additional trainable parameters. Our method achieves\nstate-of-the-art audio fitting and significant gains in image and 3D shape\nfitting tasks over base SIREN. Beyond signal fitting, WINNER suggests new\navenues in adaptive, target-aware initialization strategies for optimizing deep\nneural network training. For code and data visit\ncfdlabtechnion.github.io/siren_square/.", "AI": {"tldr": "本文解决了正弦表示网络（SIRENs）在拟合超出其频率支持的信号时出现的“频谱瓶颈”问题。我们提出了WINNER方法，通过引入自适应高斯噪声初始化权重，有效地缓解了频谱偏差，并在音频、图像和3D形状拟合任务上取得了显著改进。", "motivation": "SIRENs在未适当初始化时，难以拟合超出其频率支持范围的信号。在极端情况下，当网络频率支持与目标频谱不匹配时，会出现“频谱瓶颈”现象，模型输出接近零，甚至无法恢复在其表示能力范围内的频率分量。", "method": "我们提出了WINNER（Weight Initialization with Noise for Neural Representations）方法。WINNER通过高斯噪声扰动基础SIREN的均匀初始化权重，其中噪声尺度根据目标信号的频谱质心自适应确定。这类似于随机傅里叶嵌入，但无需引入额外的可训练参数，从而缓解了“频谱偏差”。", "result": "WINNER方法在音频拟合方面达到了最先进的水平，并在图像和3D形状拟合任务中相较于基础SIREN取得了显著提升。", "conclusion": "WINNER成功克服了SIRENs的频谱瓶颈问题，提高了信号拟合性能。此外，WINNER为深度神经网络训练中自适应、目标感知的初始化策略开辟了新的途径。"}}
{"id": "2509.12989", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12989", "abs": "https://arxiv.org/abs/2509.12989", "authors": ["Xu Zheng", "Chenfei Liao", "Ziqiao Weng", "Kaiyu Lei", "Zihao Dongfang", "Haocong He", "Yuanhuiyi Lyu", "Lutao Jiang", "Lu Qi", "Li Chen", "Danda Pani Paudel", "Kailun Yang", "Linfeng Zhang", "Luc Van Gool", "Xuming Hu"], "title": "PANORAMA: The Rise of Omnidirectional Vision in the Embodied AI Era", "comment": "This paper presents a draft overview of the emerging field of\n  omnidirectional vision in the context of embodied AI", "summary": "Omnidirectional vision, using 360-degree vision to understand the\nenvironment, has become increasingly critical across domains like robotics,\nindustrial inspection, and environmental monitoring. Compared to traditional\npinhole vision, omnidirectional vision provides holistic environmental\nawareness, significantly enhancing the completeness of scene perception and the\nreliability of decision-making. However, foundational research in this area has\nhistorically lagged behind traditional pinhole vision. This talk presents an\nemerging trend in the embodied AI era: the rapid development of omnidirectional\nvision, driven by growing industrial demand and academic interest. We highlight\nrecent breakthroughs in omnidirectional generation, omnidirectional perception,\nomnidirectional understanding, and related datasets. Drawing on insights from\nboth academia and industry, we propose an ideal panoramic system architecture\nin the embodied AI era, PANORAMA, which consists of four key subsystems.\nMoreover, we offer in-depth opinions related to emerging trends and\ncross-community impacts at the intersection of panoramic vision and embodied\nAI, along with the future roadmap and open challenges. This overview\nsynthesizes state-of-the-art advancements and outlines challenges and\nopportunities for future research in building robust, general-purpose\nomnidirectional AI systems in the embodied AI era.", "AI": {"tldr": "全向视觉在具身AI时代发展迅速，本文综述了其在生成、感知、理解及数据集方面的最新突破，提出了理想的全景系统架构PANORAMA，并探讨了未来趋势和挑战。", "motivation": "全向视觉能提供全面的环境感知，显著提升场景理解和决策可靠性，优于传统针孔视觉。然而，其基础研究曾滞后。当前，工业需求和学术兴趣的增长正推动其在具身AI时代快速发展，因此需要对其进行深入探讨和展望。", "method": "本文回顾了全向生成、全向感知、全向理解以及相关数据集的最新进展。它结合学术界和工业界的见解，提出了一个包含四个关键子系统的理想全景系统架构PANORAMA。此外，还就全景视觉与具身AI交叉领域的新兴趋势、跨社区影响、未来路线图和开放挑战提供了深入见解。", "result": "本文展示了全向视觉在生成、感知、理解和数据集方面的最新突破。提出了一个理想的具身AI时代全景系统架构PANORAMA。同时，阐述了全景视觉与具身AI交叉领域的新兴趋势、跨社区影响、未来路线图和开放挑战。", "conclusion": "全向视觉在具身AI时代正快速发展，对于构建鲁棒、通用的全向AI系统至关重要。本文综合了最新进展，提出了一个系统架构，并指明了未来研究的挑战和机遇。"}}
{"id": "2509.12995", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12995", "abs": "https://arxiv.org/abs/2509.12995", "authors": ["Yue Zhou", "Xinan He", "Kaiqing Lin", "Bing Fan", "Feng Ding", "Jinhua Zeng", "Bin Li"], "title": "Brought a Gun to a Knife Fight: Modern VFM Baselines Outgun Specialized Detectors on In-the-Wild AI Image Detection", "comment": null, "summary": "While specialized detectors for AI-generated images excel on curated\nbenchmarks, they fail catastrophically in real-world scenarios, as evidenced by\ntheir critically high false-negative rates on `in-the-wild' benchmarks. Instead\nof crafting another specialized `knife' for this problem, we bring a `gun' to\nthe fight: a simple linear classifier on a modern Vision Foundation Model\n(VFM). Trained on identical data, this baseline decisively `outguns' bespoke\ndetectors, boosting in-the-wild accuracy by a striking margin of over 20\\%.\n  Our analysis pinpoints the source of the VFM's `firepower': First, by probing\ntext-image similarities, we find that recent VLMs (e.g., Perception Encoder,\nMeta CLIP2) have learned to align synthetic images with forgery-related\nconcepts (e.g., `AI-generated'), unlike previous versions. Second, we speculate\nthat this is due to data exposure, as both this alignment and overall accuracy\nplummet on a novel dataset scraped after the VFM's pre-training cut-off date,\nensuring it was unseen during pre-training. Our findings yield two critical\nconclusions: 1) For the real-world `gunfight' of AI-generated image detection,\nthe raw `firepower' of an updated VFM is far more effective than the\n`craftsmanship' of a static detector. 2) True generalization evaluation\nrequires test data to be independent of the model's entire training history,\nincluding pre-training.", "AI": {"tldr": "针对AI生成图像的检测，基于视觉基础模型（VFM）的简单线性分类器在真实世界场景中显著优于专用检测器，准确率提升超过20%。这得益于VFM学习到合成图像与伪造概念的对齐，但其泛化能力受预训练数据限制。", "motivation": "现有AI生成图像专用检测器在真实世界（in-the-wild）场景中表现灾难性，假阴性率极高，无法有效应对实际挑战。", "method": "研究采用一种基于现代视觉基础模型（VFM）的简单线性分类器作为检测基线。通过探测文本-图像相似性来分析VFM的“火力”来源，并使用在VFM预训练截止日期之后收集的全新数据集进行测试，以评估其在未见数据上的泛化能力。", "result": "基于VFM的基线模型在真实世界场景中，准确率比专用检测器高出超过20%。分析发现，最近的视觉语言模型（VLM）已学会将合成图像与“AI生成”等伪造相关概念对齐。然而，在VFM预训练截止日期之后收集的全新数据集上，这种对齐和整体准确率均大幅下降。", "conclusion": "1) 在AI生成图像的真实世界检测中，更新的VFM的原始“火力”远比静态专用检测器的“工艺”更有效。2) 真正的泛化能力评估要求测试数据独立于模型的整个训练历史，包括预训练数据。"}}
{"id": "2509.12997", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.12997", "abs": "https://arxiv.org/abs/2509.12997", "authors": ["Anton Eldeborg Lundin", "Rasmus Winzell", "Hanna Hamrell", "David Gustafsson", "Hannes Ovrén"], "title": "Drone Detection Using a Low-Power Neuromorphic Virtual Tripwire", "comment": null, "summary": "Small drones are an increasing threat to both military personnel and civilian\ninfrastructure, making early and automated detection crucial. In this work we\ndevelop a system that uses spiking neural networks and neuromorphic cameras\n(event cameras) to detect drones. The detection model is deployed on a\nneuromorphic chip making this a fully neuromorphic system. Multiple detection\nunits can be deployed to create a virtual tripwire which detects when and where\ndrones enter a restricted zone. We show that our neuromorphic solution is\nseveral orders of magnitude more energy efficient than a reference solution\ndeployed on an edge GPU, allowing the system to run for over a year on battery\npower. We investigate how synthetically generated data can be used for\ntraining, and show that our model most likely relies on the shape of the drone\nrather than the temporal characteristics of its propellers. The small size and\nlow power consumption allows easy deployment in contested areas or locations\nthat lack power infrastructure.", "AI": {"tldr": "该研究开发了一个基于脉冲神经网络和神经形态相机（事件相机）的无人机检测系统，部署在神经形态芯片上，实现了极高的能效比，可用于创建虚拟绊线，并在无电源基础设施区域轻松部署。", "motivation": "小型无人机对军事人员和民用基础设施构成日益增长的威胁，因此早期和自动化检测至关重要。", "method": "该系统采用脉冲神经网络（SNNs）和神经形态相机（事件相机）进行无人机检测。检测模型部署在神经形态芯片上，形成一个全神经形态系统。多个检测单元可部署为虚拟绊线。研究还探讨了合成数据在训练中的应用。", "result": "与边缘GPU上的参考解决方案相比，该神经形态解决方案的能效高出几个数量级，使其能够依靠电池供电运行一年以上。模型主要依赖无人机的形状而非螺旋桨的时间特性进行检测。系统体积小、功耗低，易于在争议区域或缺乏电源基础设施的地点部署。", "conclusion": "该全神经形态无人机检测系统具有极高的能效和部署灵活性，通过利用神经形态技术解决了小型无人机威胁的早期自动化检测问题，特别适用于对功耗和部署环境有严格要求的场景。"}}
{"id": "2509.13013", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13013", "abs": "https://arxiv.org/abs/2509.13013", "authors": ["Gaofeng Liu", "Hengsen Li", "Ruoyu Gao", "Xuetong Li", "Zhiyuan Ma", "Tao Fang"], "title": "Dream3DAvatar: Text-Controlled 3D Avatar Reconstruction from a Single Image", "comment": null, "summary": "With the rapid advancement of 3D representation techniques and generative\nmodels, substantial progress has been made in reconstructing full-body 3D\navatars from a single image. However, this task remains fundamentally\nill-posedness due to the limited information available from monocular input,\nmaking it difficult to control the geometry and texture of occluded regions\nduring generation. To address these challenges, we redesign the reconstruction\npipeline and propose Dream3DAvatar, an efficient and text-controllable\ntwo-stage framework for 3D avatar generation. In the first stage, we develop a\nlightweight, adapter-enhanced multi-view generation model. Specifically, we\nintroduce the Pose-Adapter to inject SMPL-X renderings and skeletal information\ninto SDXL, enforcing geometric and pose consistency across views. To preserve\nfacial identity, we incorporate ID-Adapter-G, which injects high-resolution\nfacial features into the generation process. Additionally, we leverage BLIP2 to\ngenerate high-quality textual descriptions of the multi-view images, enhancing\ntext-driven controllability in occluded regions. In the second stage, we design\na feedforward Transformer model equipped with a multi-view feature fusion\nmodule to reconstruct high-fidelity 3D Gaussian Splat representations (3DGS)\nfrom the generated images. Furthermore, we introduce ID-Adapter-R, which\nutilizes a gating mechanism to effectively fuse facial features into the\nreconstruction process, improving high-frequency detail recovery. Extensive\nexperiments demonstrate that our method can generate realistic, animation-ready\n3D avatars without any post-processing and consistently outperforms existing\nbaselines across multiple evaluation metrics.", "AI": {"tldr": "本文提出Dream3DAvatar，一个高效且文本可控的两阶段框架，用于从单张图像生成高质量、动画就绪的3D人体模型，解决了单目输入信息不足导致的遮挡区域几何和纹理控制难题。", "motivation": "从单张图像重建全身3D人体模型是一个固有的病态问题，由于单目输入信息有限，难以控制生成过程中遮挡区域的几何形状和纹理。", "method": "本研究重新设计了重建流程，提出了一个两阶段框架：\n1. **第一阶段（多视角生成）**：开发了一个轻量级、适配器增强的多视角生成模型。引入Pose-Adapter将SMPL-X渲染和骨骼信息注入SDXL，以确保跨视角的几何和姿态一致性。集成ID-Adapter-G注入高分辨率面部特征以保持面部身份。利用BLIP2生成多视角图像的高质量文本描述，增强对遮挡区域的文本驱动可控性。\n2. **第二阶段（3D重建）**：设计了一个前馈Transformer模型，配备多视角特征融合模块，从生成的图像中重建高保真3D高斯飞溅（3DGS）表示。此外，引入ID-Adapter-R，利用门控机制有效融合面部特征到重建过程中，以改善高频细节恢复。", "result": "实验证明，该方法能够生成逼真、动画就绪的3D人体模型，无需任何后处理，并在多个评估指标上持续优于现有基线方法。", "conclusion": "Dream3DAvatar通过其创新的两阶段框架和适配器增强机制，成功解决了从单张图像生成3D人体模型中遮挡区域控制和信息不足的挑战，实现了高效、文本可控且高保真的3D人体模型生成。"}}
{"id": "2509.13067", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13067", "abs": "https://arxiv.org/abs/2509.13067", "authors": ["Xu Li", "Yuxuan Liang", "Xiaolei Chen", "Yi Zheng", "Haotian Chen", "Bin Li", "Xiangyang Xue"], "title": "HERO: Rethinking Visual Token Early Dropping in High-Resolution Large Vision-Language Models", "comment": null, "summary": "By cropping high-resolution images into local tiles and encoding them\nindependently, High-Resolution Large Vision-Language Models (HR-LVLMs) have\ndemonstrated remarkable fine-grained visual understanding capabilities.\nHowever, this divide-and-conquer paradigm significantly increases the number of\nvisual tokens, resulting in substantial computational and memory overhead. To\nbetter understand and address this challenge, we empirically investigate visual\ntoken utilization in HR-LVLMs and uncover three key findings: (1) the local\ntiles have varying importance, jointly determined by visual saliency and task\nrelevance; (2) the CLS token in CLIP-based vision encoders exhibits a two-stage\nattention pattern across layers, with each stage attending to different types\nof visual tokens; (3) the visual tokens emphasized at different stages encode\ninformation at varying levels of granularity, playing complementary roles\nwithin LVLMs. Building on these insights, we propose HERO, a High-resolution\nvisual token early dropping framework that integrates content-adaptive token\nbudget allocation with function-aware token selection. By accurately estimating\ntile-level importance and selectively retaining visual tokens with\ncomplementary roles, HERO achieves superior efficiency-accuracy trade-offs\nacross diverse benchmarks and model scales, all in a training-free manner. This\nstudy provides both empirical insights and practical solutions toward efficient\ninference in HR-LVLMs.", "AI": {"tldr": "高分辨率大视觉-语言模型（HR-LVLMs）通过图像分块处理，导致视觉tokens数量庞大，计算和内存开销高。本文通过实证研究揭示了视觉tokens利用率的特点，并提出了HERO框架，通过内容自适应预算分配和功能感知token选择，在不训练的情况下显著提高了HR-LVLMs的效率和准确性。", "motivation": "HR-LVLMs通过将高分辨率图像裁剪成局部图块并独立编码，实现了出色的细粒度视觉理解能力。然而，这种“分而治之”的范式显著增加了视觉tokens的数量，导致了巨大的计算和内存开销。本研究旨在更好地理解和解决这一挑战。", "method": "本文首先对HR-LVLMs中的视觉token利用率进行了实证研究。在此基础上，提出了HERO（High-resolution visual token early dropping）框架，该框架整合了内容自适应的token预算分配和功能感知的token选择。HERO通过准确估计图块级别的重要性，并选择性地保留具有互补作用的视觉token。", "result": "1. 发现局部图块的重要性各不相同，由视觉显著性和任务相关性共同决定。\n2. CLIP基视觉编码器中的CLS token在不同层级展现出两阶段的注意力模式，每个阶段关注不同类型的视觉token。\n3. 在不同阶段被强调的视觉token编码了不同粒度级别的信息，并在LVLMs中扮演互补角色。\n4. HERO框架在各种基准测试和模型规模上实现了卓越的效率-准确性权衡，且无需训练。", "conclusion": "本研究为HR-LVLMs的有效推理提供了实证见解和实用的解决方案。"}}
{"id": "2509.13083", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13083", "abs": "https://arxiv.org/abs/2509.13083", "authors": ["Yan Xingyang", "Huang Xiaohong", "Zhang Zhao", "You Tian", "Xu Ziheng"], "title": "Using KL-Divergence to Focus Frequency Information in Low-Light Image Enhancement", "comment": null, "summary": "In the Fourier domain, luminance information is primarily encoded in the\namplitude spectrum, while spatial structures are captured in the phase\ncomponents. The traditional Fourier Frequency information fitting employs\npixel-wise loss functions, which tend to focus excessively on local information\nand may lead to global information loss. In this paper, we present LLFDisc, a\nU-shaped deep enhancement network that integrates cross-attention and gating\nmechanisms tailored for frequency-aware enhancement. We propose a novel\ndistribution-aware loss that directly fits the Fourier-domain information and\nminimizes their divergence using a closed-form KL-Divergence objective. This\nenables the model to align Fourier-domain information more robustly than with\nconventional MSE-based losses. Furthermore, we enhance the perceptual loss\nbased on VGG by embedding KL-Divergence on extracted deep features, enabling\nbetter structural fidelity. Extensive experiments across multiple benchmarks\ndemonstrate that LLFDisc achieves state-of-the-art performance in both\nqualitative and quantitative evaluations. Our code will be released at:\nhttps://github.com/YanXY000/LLFDisc", "AI": {"tldr": "本文提出LLFDisc，一个U型深度增强网络，通过引入新的傅里叶域分布感知损失（基于KL散度）和增强的感知损失（将KL散度嵌入VGG特征），解决了传统像素级损失在傅里叶域信息拟合中可能导致全局信息丢失的问题，实现了最先进的图像增强性能。", "motivation": "傅里叶域中，传统傅里叶频率信息拟合采用像素级损失函数，往往过度关注局部信息，可能导致全局信息丢失。", "method": "本文提出了LLFDisc，一个U型深度增强网络，集成了交叉注意力（cross-attention）和门控机制（gating mechanisms），专为频率感知增强设计。核心方法包括：1) 提出了一种新颖的分布感知损失（distribution-aware loss），直接拟合傅里叶域信息，并使用闭式KL散度目标最小化其散度。2) 通过在提取的深度特征上嵌入KL散度，增强了基于VGG的感知损失，以实现更好的结构保真度。", "result": "在多个基准测试中进行的广泛实验表明，LLFDisc在定性和定量评估中均达到了最先进的性能。", "conclusion": "LLFDisc通过其新颖的傅里叶域分布感知损失和增强的感知损失，能够比传统基于MSE的损失更稳健地对齐傅里叶域信息，并实现了卓越的图像增强效果和结构保真度。"}}
{"id": "2509.13084", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13084", "abs": "https://arxiv.org/abs/2509.13084", "authors": ["Yunyao Lu", "Yihang Wu", "Ahmad Chaddad", "Tareef Daqqaq", "Reem Kateb"], "title": "Enhancing Dual Network Based Semi-Supervised Medical Image Segmentation with Uncertainty-Guided Pseudo-Labeling", "comment": "Accpeted in Knowledge-Based Systems", "summary": "Despite the remarkable performance of supervised medical image segmentation\nmodels, relying on a large amount of labeled data is impractical in real-world\nsituations. Semi-supervised learning approaches aim to alleviate this challenge\nusing unlabeled data through pseudo-label generation. Yet, existing\nsemi-supervised segmentation methods still suffer from noisy pseudo-labels and\ninsufficient supervision within the feature space. To solve these challenges,\nthis paper proposes a novel semi-supervised 3D medical image segmentation\nframework based on a dual-network architecture. Specifically, we investigate a\nCross Consistency Enhancement module using both cross pseudo and\nentropy-filtered supervision to reduce the noisy pseudo-labels, while we design\na dynamic weighting strategy to adjust the contributions of pseudo-labels using\nan uncertainty-aware mechanism (i.e., Kullback-Leibler divergence). In\naddition, we use a self-supervised contrastive learning mechanism to align\nuncertain voxel features with reliable class prototypes by effectively\ndifferentiating between trustworthy and uncertain predictions, thus reducing\nprediction uncertainty. Extensive experiments are conducted on three 3D\nsegmentation datasets, Left Atrial, NIH Pancreas and BraTS-2019. The proposed\napproach consistently exhibits superior performance across various settings\n(e.g., 89.95\\% Dice score on left Atrial with 10\\% labeled data) compared to\nthe state-of-the-art methods. Furthermore, the usefulness of the proposed\nmodules is further validated via ablation experiments.", "AI": {"tldr": "本文提出了一种基于双网络架构的半监督3D医学图像分割框架，通过交叉一致性增强、动态权重策略和自监督对比学习，有效解决了伪标签噪声和特征空间监督不足的问题，并在多个数据集上取得了最先进的性能。", "motivation": "监督式医学图像分割模型需要大量标注数据，这在实际应用中不切实际。现有半监督方法存在伪标签噪声和特征空间监督不足的问题。", "method": "本文提出了一个基于双网络架构的半监督3D医学图像分割框架。具体方法包括：1) 交叉一致性增强模块，结合交叉伪监督和熵过滤监督来减少伪标签噪声。2) 动态权重策略，利用不确定性感知机制（Kullback-Leibler散度）调整伪标签的贡献。3) 自监督对比学习机制，通过区分可信和不确定的预测，将不确定体素特征与可靠类别原型对齐，从而降低预测不确定性。", "result": "在三个3D分割数据集（左心房、NIH胰腺和BraTS-2019）上进行了广泛实验，所提出的方法在各种设置下（例如，在10%标注数据的左心房数据集上Dice分数为89.95%）均表现出优于现有最先进方法的性能。消融实验进一步验证了所提出模块的有效性。", "conclusion": "所提出的半监督3D医学图像分割框架通过引入交叉一致性增强、动态权重策略和自监督对比学习，成功解决了伪标签噪声和特征空间监督不足的挑战，实现了卓越的分割性能。"}}
{"id": "2509.13116", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13116", "abs": "https://arxiv.org/abs/2509.13116", "authors": ["Ruibo Li", "Hanyu Shi", "Zhe Wang", "Guosheng Lin"], "title": "Weakly and Self-Supervised Class-Agnostic Motion Prediction for Autonomous Driving", "comment": "An extension of our CVPR 2023 paper, \"Weakly Supervised\n  Class-Agnostic Motion Prediction for Autonomous Driving,\" accepted for\n  publication in TPAMI", "summary": "Understanding motion in dynamic environments is critical for autonomous\ndriving, thereby motivating research on class-agnostic motion prediction. In\nthis work, we investigate weakly and self-supervised class-agnostic motion\nprediction from LiDAR point clouds. Outdoor scenes typically consist of mobile\nforegrounds and static backgrounds, allowing motion understanding to be\nassociated with scene parsing. Based on this observation, we propose a novel\nweakly supervised paradigm that replaces motion annotations with fully or\npartially annotated (1%, 0.1%) foreground/background masks for supervision. To\nthis end, we develop a weakly supervised approach utilizing\nforeground/background cues to guide the self-supervised learning of motion\nprediction models. Since foreground motion generally occurs in non-ground\nregions, non-ground/ground masks can serve as an alternative to\nforeground/background masks, further reducing annotation effort. Leveraging\nnon-ground/ground cues, we propose two additional approaches: a weakly\nsupervised method requiring fewer (0.01%) foreground/background annotations,\nand a self-supervised method without annotations. Furthermore, we design a\nRobust Consistency-aware Chamfer Distance loss that incorporates multi-frame\ninformation and robust penalty functions to suppress outliers in\nself-supervised learning. Experiments show that our weakly and self-supervised\nmodels outperform existing self-supervised counterparts, and our weakly\nsupervised models even rival some supervised ones. This demonstrates that our\napproaches effectively balance annotation effort and performance.", "AI": {"tldr": "本文提出了一种基于激光雷达点云的弱监督和自监督的类别无关运动预测方法，通过利用前景/背景或非地面/地面掩码作为监督信号，并设计了新的鲁棒一致性Chamfer距离损失，在减少标注工作量的同时，实现了与部分监督方法相当的性能。", "motivation": "自动驾驶需要理解动态环境中的运动，因此类别无关的运动预测至关重要。传统的运动预测方法通常需要大量的运动标注，这促使研究人员探索弱监督和自监督的方法。", "method": "本文提出了一种新的弱监督范式，用前景/背景(F/B)掩码（完全、1%或0.1%标注）取代运动标注进行监督。基于此，开发了一种利用F/B线索引导自监督运动预测模型的方法。为进一步减少标注，提出使用非地面/地面掩码作为F/B掩码的替代。在此基础上，提出了两种额外的方法：一种仅需0.01%F/B标注的弱监督方法，以及一种无需任何标注的自监督方法。此外，设计了一种鲁棒一致性感知Chamfer距离损失，该损失结合了多帧信息和鲁棒惩罚函数，以抑制自监督学习中的异常值。", "result": "实验结果表明，本文提出的弱监督和自监督模型优于现有自监督方法。更重要的是，本文的弱监督模型甚至可以与一些监督模型相媲美。这表明所提出的方法有效地平衡了标注工作量和性能。", "conclusion": "本文提出的弱监督和自监督方法，通过利用前景/背景或非地面/地面掩码作为监督信号，并结合新的鲁棒一致性Chamfer距离损失，能够有效地从激光雷达点云中进行类别无关的运动预测。这些方法在显著减少标注需求的同时，实现了出色的性能，为自动驾驶领域提供了有前景的解决方案。"}}
{"id": "2509.13133", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13133", "abs": "https://arxiv.org/abs/2509.13133", "authors": ["Zhihao Zhang", "Chunyu Lin", "Lang Nie", "Jiyuan Wang", "Yao Zhao"], "title": "Advancing Real-World Parking Slot Detection with Large-Scale Dataset and Semi-Supervised Baseline", "comment": "IEEE Transactions on Intelligent Transportation Systems (T-ITS)", "summary": "As automatic parking systems evolve, the accurate detection of parking slots\nhas become increasingly critical. This study focuses on parking slot detection\nusing surround-view cameras, which offer a comprehensive bird's-eye view of the\nparking environment. However, the current datasets are limited in scale, and\nthe scenes they contain are seldom disrupted by real-world noise (e.g., light,\nocclusion, etc.). Moreover, manual data annotation is prone to errors and\nomissions due to the complexity of real-world conditions, significantly\nincreasing the cost of annotating large-scale datasets. To address these\nissues, we first construct a large-scale parking slot detection dataset (named\nCRPS-D), which includes various lighting distributions, diverse weather\nconditions, and challenging parking slot variants. Compared with existing\ndatasets, the proposed dataset boasts the largest data scale and consists of a\nhigher density of parking slots, particularly featuring more slanted parking\nslots. Additionally, we develop a semi-supervised baseline for parking slot\ndetection, termed SS-PSD, to further improve performance by exploiting\nunlabeled data. To our knowledge, this is the first semi-supervised approach in\nparking slot detection, which is built on the teacher-student model with\nconfidence-guided mask consistency and adaptive feature perturbation.\nExperimental results demonstrate the superiority of SS-PSD over the existing\nstate-of-the-art (SoTA) solutions on both the proposed dataset and the existing\ndataset. Particularly, the more unlabeled data there is, the more significant\nthe gains brought by our semi-supervised scheme. The relevant source codes and\nthe dataset have been made publicly available at\nhttps://github.com/zzh362/CRPS-D.", "AI": {"tldr": "本研究针对环视摄像头停车位检测，解决了现有数据集规模小、缺乏真实世界噪声以及手动标注成本高的问题。为此，我们构建了一个大规模数据集CRPS-D，并提出了一种半监督检测方法SS-PSD，该方法在多个数据集上均优于现有技术，且利用未标注数据效果更显著。", "motivation": "自动泊车系统中停车位检测的准确性至关重要。然而，现有数据集规模有限，缺乏真实世界的噪声（如光照、遮挡等），且手动标注成本高昂且容易出错，限制了大规模数据集的构建和算法的性能提升。", "method": "1. 构建了一个大规模停车位检测数据集CRPS-D，包含多样化的光照、天气条件和具有挑战性的停车位变体（特别是更多倾斜停车位），且数据规模和停车位密度均高于现有数据集。2. 开发了一种名为SS-PSD的半监督停车位检测基线方法，该方法基于师生模型，并结合了置信度引导的掩码一致性和自适应特征扰动，以利用未标注数据来提高性能。", "result": "实验结果表明，SS-PSD在所提出的CRPS-D数据集和现有数据集上均优于现有最先进（SoTA）的解决方案。尤其值得注意的是，未标注数据越多，我们的半监督方案带来的性能提升越显著。", "conclusion": "本研究通过构建大规模、多样化的CRPS-D数据集，并提出创新的半监督SS-PSD方法，有效解决了环视摄像头停车位检测领域的数据和算法挑战，显著提升了检测性能，尤其在利用未标注数据方面表现出色。"}}
{"id": "2509.13149", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13149", "abs": "https://arxiv.org/abs/2509.13149", "authors": ["Minqing Huang", "Shouyi Lu", "Boyuan Zheng", "Ziyao Li", "Xiao Tang", "Guirong Zhuo"], "title": "MSDNet: Efficient 4D Radar Super-Resolution via Multi-Stage Distillation", "comment": "8 pages, 5 figures", "summary": "4D radar super-resolution, which aims to reconstruct sparse and noisy point\nclouds into dense and geometrically consistent representations, is a\nfoundational problem in autonomous perception. However, existing methods often\nsuffer from high training cost or rely on complex diffusion-based sampling,\nresulting in high inference latency and poor generalization, making it\ndifficult to balance accuracy and efficiency. To address these limitations, we\npropose MSDNet, a multi-stage distillation framework that efficiently transfers\ndense LiDAR priors to 4D radar features to achieve both high reconstruction\nquality and computational efficiency. The first stage performs\nreconstruction-guided feature distillation, aligning and densifying the\nstudent's features through feature reconstruction. In the second stage, we\npropose diffusion-guided feature distillation, which treats the stage-one\ndistilled features as a noisy version of the teacher's representations and\nrefines them via a lightweight diffusion network. Furthermore, we introduce a\nnoise adapter that adaptively aligns the noise level of the feature with a\npredefined diffusion timestep, enabling a more precise denoising. Extensive\nexperiments on the VoD and in-house datasets demonstrate that MSDNet achieves\nboth high-fidelity reconstruction and low-latency inference in the task of 4D\nradar point cloud super-resolution, and consistently improves performance on\ndownstream tasks. The code will be publicly available upon publication.", "AI": {"tldr": "MSDNet是一个多阶段蒸馏框架，通过将LiDAR先验知识高效转移到4D雷达特征，解决了4D雷达超分辨率中精度与效率难以平衡的问题，实现了高保真重建和低延迟推理。", "motivation": "现有的4D雷达超分辨率方法存在训练成本高、推理延迟高、泛化性差等问题，难以同时实现高精度和高效率。", "method": "本文提出了MSDNet，一个多阶段蒸馏框架：第一阶段是重建引导特征蒸馏，通过特征重建对齐并稠密化学生特征；第二阶段是扩散引导特征蒸馏，将第一阶段的蒸馏特征视为教师表示的噪声版本，并通过轻量级扩散网络进行精炼。此外，引入了噪声适配器以自适应对齐特征噪声水平与预定义扩散时间步，实现更精确的去噪。", "result": "在VoD和内部数据集上的大量实验表明，MSDNet在4D雷达点云超分辨率任务中实现了高保真重建和低延迟推理，并持续提升了下游任务的性能。", "conclusion": "MSDNet通过创新的多阶段蒸馏框架，有效解决了4D雷达超分辨率中精度与效率的平衡难题，为自主感知领域提供了高效且高质量的解决方案。"}}
{"id": "2509.13151", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13151", "abs": "https://arxiv.org/abs/2509.13151", "authors": ["Rohan Kumar", "Jyothi Swaroopa Jinka", "Ravi Kiran Sarvadevabhatla"], "title": "TexTAR : Textual Attribute Recognition in Multi-domain and Multi-lingual Document Images", "comment": "Accepted at ICDAR 2025 (Oral)", "summary": "Recognizing textual attributes such as bold, italic, underline and strikeout\nis essential for understanding text semantics, structure, and visual\npresentation. These attributes highlight key information, making them crucial\nfor document analysis. Existing methods struggle with computational efficiency\nor adaptability in noisy, multilingual settings. To address this, we introduce\nTexTAR, a multi-task, context-aware Transformer for Textual Attribute\nRecognition (TAR). Our novel data selection pipeline enhances context\nawareness, and our architecture employs a 2D RoPE (Rotary Positional\nEmbedding)-style mechanism to incorporate input context for more accurate\nattribute predictions. We also introduce MMTAD, a diverse, multilingual,\nmulti-domain dataset annotated with text attributes across real-world documents\nsuch as legal records, notices, and textbooks. Extensive evaluations show\nTexTAR outperforms existing methods, demonstrating that contextual awareness\ncontributes to state-of-the-art TAR performance.", "AI": {"tldr": "该论文提出了TexTAR，一个多任务、上下文感知的Transformer模型，用于文本属性识别（TAR），并通过新颖的数据选择管道和2D RoPE机制增强上下文感知。同时引入了多语言、多领域数据集MMTAD。实验证明TexTAR优于现有方法，并强调了上下文感知对TAR性能的重要性。", "motivation": "识别文本属性（如粗体、斜体、下划线、删除线）对于理解文本语义、结构和视觉呈现至关重要。现有方法在嘈杂、多语言环境中计算效率或适应性方面存在不足。", "method": "引入了TexTAR，一个多任务、上下文感知的Transformer模型用于文本属性识别。设计了新颖的数据选择管道以增强上下文感知。架构中采用了2D RoPE（旋转位置嵌入）风格的机制来融入输入上下文。同时，构建了MMTAD，一个多样化、多语言、多领域的数据集，其中包含真实世界文档（如法律记录、通知、教科书）的文本属性标注。", "result": "TexTAR在广泛评估中超越了现有方法。", "conclusion": "上下文感知有助于实现最先进的文本属性识别（TAR）性能。"}}
{"id": "2509.13161", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13161", "abs": "https://arxiv.org/abs/2509.13161", "authors": ["Zhihao He", "Tianyao He", "Tieyuan Chen", "Yun Xu", "Huabin Liu", "Chaofan Gan", "Gui Zou", "Weiyao Lin"], "title": "Enhancing Video Large Language Models with Structured Multi-Video Collaborative Reasoning (early version)", "comment": null, "summary": "Despite the prosperity of the video language model, the current pursuit of\ncomprehensive video reasoning is thwarted by the inherent spatio-temporal\nincompleteness within individual videos, resulting in hallucinations and\ninaccuracies. A promising solution is to augment the reasoning performance with\nmultiple related videos. However, video tokens are numerous and contain\nredundant information, so directly feeding the relevant video data into a large\nlanguage model to enhance responses could be counterproductive. To address this\nchallenge, we propose a multi-video collaborative framework for video language\nmodels. For efficient and flexible video representation, we establish a Video\nStructuring Module to represent the video's knowledge as a spatio-temporal\ngraph. Based on the structured video representation, we design the Graph Fusion\nModule to fuse the structured knowledge and valuable information from related\nvideos into the augmented graph node tokens. Finally, we construct an elaborate\nmulti-video structured prompt to integrate the graph, visual, and textual\ntokens as the input to the large language model. Extensive experiments\nsubstantiate the effectiveness of our framework, showcasing its potential as a\npromising avenue for advancing video language models.", "AI": {"tldr": "本文提出一个多视频协作框架，通过将视频知识表示为时空图并融合相关视频信息，以解决单视频固有时空不完整性导致视频语言模型产生幻觉和不准确性问题。", "motivation": "当前的视频语言模型（VLM）在综合视频推理方面受限于单个视频固有的时空不完整性，导致幻觉和不准确性。直接将大量相关视频数据输入大型语言模型效率低下且可能适得其反。", "method": "该研究提出一个多视频协作框架：1. 建立一个视频结构化模块（Video Structuring Module），将视频知识表示为时空图。2. 设计一个图融合模块（Graph Fusion Module），将相关视频的结构化知识和有价值信息融合到增强的图节点标记中。3. 构建一个精巧的多视频结构化提示（multi-video structured prompt），将图、视觉和文本标记整合作为大型语言模型的输入。", "result": "广泛的实验证明了该框架的有效性，展示了其作为推进视频语言模型的一个有前景的途径的潜力。", "conclusion": "该多视频协作框架通过结构化视频表示和信息融合，有效解决了视频语言模型在处理单个视频时面临的时空不完整性问题，为提升视频语言模型的推理能力提供了一个有前景的解决方案。"}}
{"id": "2509.13172", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13172", "abs": "https://arxiv.org/abs/2509.13172", "authors": ["Ruifei Ding", "Zhe Chen", "Wen Fan", "Chen Long", "Huijuan Xiao", "Yelu Zeng", "Zhen Dong", "Bisheng Yang"], "title": "WHU-STree: A Multi-modal Benchmark Dataset for Street Tree Inventory", "comment": null, "summary": "Street trees are vital to urban livability, providing ecological and social\nbenefits. Establishing a detailed, accurate, and dynamically updated street\ntree inventory has become essential for optimizing these multifunctional assets\nwithin space-constrained urban environments. Given that traditional field\nsurveys are time-consuming and labor-intensive, automated surveys utilizing\nMobile Mapping Systems (MMS) offer a more efficient solution. However, existing\nMMS-acquired tree datasets are limited by small-scale scene, limited\nannotation, or single modality, restricting their utility for comprehensive\nanalysis. To address these limitations, we introduce WHU-STree, a cross-city,\nrichly annotated, and multi-modal urban street tree dataset. Collected across\ntwo distinct cities, WHU-STree integrates synchronized point clouds and\nhigh-resolution images, encompassing 21,007 annotated tree instances across 50\nspecies and 2 morphological parameters. Leveraging the unique characteristics,\nWHU-STree concurrently supports over 10 tasks related to street tree inventory.\nWe benchmark representative baselines for two key tasks--tree species\nclassification and individual tree segmentation. Extensive experiments and\nin-depth analysis demonstrate the significant potential of multi-modal data\nfusion and underscore cross-domain applicability as a critical prerequisite for\npractical algorithm deployment. In particular, we identify key challenges and\noutline potential future works for fully exploiting WHU-STree, encompassing\nmulti-modal fusion, multi-task collaboration, cross-domain generalization,\nspatial pattern learning, and Multi-modal Large Language Model for street tree\nasset management. The WHU-STree dataset is accessible at:\nhttps://github.com/WHU-USI3DV/WHU-STree.", "AI": {"tldr": "本文介绍了WHU-STree数据集，这是一个跨城市、多模态、标注丰富的城市行道树数据集，旨在克服传统调查和现有MMS数据集的局限性，支持多种行道树资产管理任务。", "motivation": "行道树对城市宜居性至关重要，但传统的实地调查耗时费力。现有的移动测绘系统（MMS）获取的树木数据集存在场景规模小、标注有限或单一模态的局限性，阻碍了全面分析和优化这些多功能资产。", "method": "本文提出了WHU-STree数据集，它是一个跨城市、标注丰富且多模态的城市行道树数据集。该数据集在两个不同城市采集，整合了同步的点云数据和高分辨率图像，包含21,007个标注的树木实例，涵盖50个树种和2个形态参数。研究人员利用该数据集为树种分类和单株树分割这两个关键任务建立了基线。", "result": "WHU-STree数据集能够同时支持超过10项与行道树清查相关的任务。广泛的实验和深入分析表明，多模态数据融合具有显著潜力，并强调了跨领域适用性是实际算法部署的关键先决条件。研究还指出了充分利用WHU-STree面临的关键挑战和未来工作方向，包括多模态融合、多任务协作、跨领域泛化、空间模式学习以及用于行道树资产管理的多模态大语言模型。", "conclusion": "WHU-STree数据集通过提供一个跨城市、多模态、标注丰富的资源，有效解决了现有行道树数据集的局限性。它为开发高效、自动化的行道树清查和管理算法提供了重要基础，并强调了多模态数据融合和跨领域泛化在实际应用中的重要性。"}}
{"id": "2509.13175", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13175", "abs": "https://arxiv.org/abs/2509.13175", "authors": ["Yingtai Li", "Haoran Lai", "Xiaoqian Zhou", "Shuai Ming", "Wenxin Ma", "Wei Wei", "Shaohua Kevin Zhou"], "title": "More performant and scalable: Rethinking contrastive vision-language pre-training of radiology in the LLM era", "comment": "MICCAI 2025", "summary": "The emergence of Large Language Models (LLMs) presents unprecedented\nopportunities to revolutionize medical contrastive vision-language\npre-training. In this paper, we show how LLMs can facilitate large-scale\nsupervised pre-training, thereby advancing vision-language alignment. We begin\nby demonstrate that modern LLMs can automatically extract diagnostic labels\nfrom radiology reports with remarkable precision (>96\\% AUC in our experiments)\nwithout complex prompt engineering, enabling the creation of large-scale\n\"silver-standard\" datasets at a minimal cost (~\\$3 for 50k CT image-report\npairs). Further, we find that vision encoder trained on this \"silver-standard\"\ndataset achieves performance comparable to those trained on labels extracted by\nspecialized BERT-based models, thereby democratizing the access to large-scale\nsupervised pre-training. Building on this foundation, we proceed to reveal that\nsupervised pre-training fundamentally improves contrastive vision-language\nalignment. Our approach achieves state-of-the-art performance using only a 3D\nResNet-18 with vanilla CLIP training, including 83.8\\% AUC for zero-shot\ndiagnosis on CT-RATE, 77.3\\% AUC on RAD-ChestCT, and substantial improvements\nin cross-modal retrieval (MAP@50=53.7\\% for image-image, Recall@100=52.2\\% for\nreport-image). These results demonstrate the potential of utilizing LLMs to\nfacilitate {\\bf more performant and scalable} medical AI systems. Our code is\navaiable at https://github.com/SadVoxel/More-performant-and-scalable.", "AI": {"tldr": "该研究展示了大型语言模型（LLMs）如何高效地从放射学报告中提取诊断标签，构建大规模“银标准”数据集，从而实现高性能的医学对比视觉-语言预训练，并以更简单的模型架构达到最先进的性能。", "motivation": "大型语言模型的出现为医学对比视觉-语言预训练带来了前所未有的机遇。研究旨在探索LLMs如何促进大规模监督预训练，从而提升视觉-语言对齐效果。", "method": "研究方法包括：1) 利用LLMs自动从放射学报告中提取诊断标签，以极高的精度（>96% AUC）和低成本（5万对CT图像-报告约3美元）创建大规模“银标准”数据集。2) 使用这些“银标准”标签训练视觉编码器（例如，3D ResNet-18）。3) 采用普通的CLIP训练方法进行视觉-语言对齐。4) 比较其性能与基于专用BERT模型提取标签训练的模型的性能。", "result": "主要结果包括：1) LLMs能够以高精度（>96% AUC）从放射学报告中提取诊断标签，以极低成本（约3美元）创建了5万对CT图像-报告的大规模数据集。2) 在此“银标准”数据集上训练的视觉编码器，其性能与在专用BERT模型提取标签上训练的模型相当。3) 监督预训练显著改善了对比视觉-语言对齐。4) 仅使用3D ResNet-18和普通的CLIP训练，就在多项任务上取得了最先进的性能，包括CT-RATE零样本诊断达到83.8% AUC，RAD-ChestCT达到77.3% AUC，以及跨模态检索的显著提升（图像-图像MAP@50=53.7%，报告-图像Recall@100=52.2%）。", "conclusion": "研究结果证明，利用大型语言模型能够促进更高效、更具扩展性的医学人工智能系统，通过实现大规模监督预训练来提升视觉-语言对齐能力。"}}
{"id": "2509.13181", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13181", "abs": "https://arxiv.org/abs/2509.13181", "authors": ["Shyam Nandan Rai", "Shyamgopal Karthik", "Mariana-Iuliana Georgescu", "Barbara Caputo", "Carlo Masone", "Zeynep Akata"], "title": "Road Obstacle Video Segmentation", "comment": "GCPR 2025", "summary": "With the growing deployment of autonomous driving agents, the detection and\nsegmentation of road obstacles have become critical to ensure safe autonomous\nnavigation. However, existing road-obstacle segmentation methods are applied on\nindividual frames, overlooking the temporal nature of the problem, leading to\ninconsistent prediction maps between consecutive frames. In this work, we\ndemonstrate that the road-obstacle segmentation task is inherently temporal,\nsince the segmentation maps for consecutive frames are strongly correlated. To\naddress this, we curate and adapt four evaluation benchmarks for road-obstacle\nvideo segmentation and evaluate 11 state-of-the-art image- and video-based\nsegmentation methods on these benchmarks. Moreover, we introduce two strong\nbaseline methods based on vision foundation models. Our approach establishes a\nnew state-of-the-art in road-obstacle video segmentation for long-range video\nsequences, providing valuable insights and direction for future research.", "AI": {"tldr": "本文指出道路障碍物分割任务本质上是时序性的，现有方法忽略了这一点导致预测不一致。为此，作者创建并调整了四个视频分割基准，评估了11种现有方法，并提出了基于视觉基础模型的两个强基线方法，在长距离视频序列的道路障碍物视频分割方面达到了新的最先进水平。", "motivation": "随着自动驾驶代理的部署，道路障碍物的检测和分割对于确保安全导航至关重要。然而，现有方法仅应用于单个帧，忽视了问题的时序性质，导致连续帧之间的预测图不一致。研究认为，由于连续帧的分割图之间存在强相关性，道路障碍物分割任务本身就是时序性的。", "method": "本文首先论证了道路障碍物分割任务的固有时间性。然后，策划并调整了四个用于道路障碍物视频分割的评估基准。在此基础上，评估了11种最先进的基于图像和视频的分割方法。此外，引入了两种基于视觉基础模型的强大基线方法。", "result": "研究结果表明，道路障碍物分割任务确实是内在时序性的。所提出的方法在长距离视频序列的道路障碍物视频分割方面建立了新的最先进水平。", "conclusion": "这项工作为未来的研究提供了有价值的见解和方向，强调了在道路障碍物分割中考虑时序信息的重要性。"}}
{"id": "2509.13210", "categories": ["cs.CV", "I.2.10; I.4.8"], "pdf": "https://arxiv.org/pdf/2509.13210", "abs": "https://arxiv.org/abs/2509.13210", "authors": ["Ligang Chang", "Shengkai Xu", "Liangchang Shen", "Binhan Xu", "Junqiao Wang", "Tianyu Shi", "Yanhui Du"], "title": "Vi-SAFE: A Spatial-Temporal Framework for Efficient Violence Detection in Public Surveillance", "comment": null, "summary": "Violence detection in public surveillance is critical for public safety. This\nstudy addresses challenges such as small-scale targets, complex environments,\nand real-time temporal analysis. We propose Vi-SAFE, a spatial-temporal\nframework that integrates an enhanced YOLOv8 with a Temporal Segment Network\n(TSN) for video surveillance. The YOLOv8 model is optimized with GhostNetV3 as\na lightweight backbone, an exponential moving average (EMA) attention\nmechanism, and pruning to reduce computational cost while maintaining accuracy.\nYOLOv8 and TSN are trained separately on pedestrian and violence datasets,\nwhere YOLOv8 extracts human regions and TSN performs binary classification of\nviolent behavior. Experiments on the RWF-2000 dataset show that Vi-SAFE\nachieves an accuracy of 0.88, surpassing TSN alone (0.77) and outperforming\nexisting methods in both accuracy and efficiency, demonstrating its\neffectiveness for public safety surveillance. Code is available at\nhttps://anonymous.4open.science/r/Vi-SAFE-3B42/README.md.", "AI": {"tldr": "本文提出Vi-SAFE，一个结合增强型YOLOv8和时间片段网络（TSN）的时空框架，用于公共监控中的暴力行为检测，实现了高准确性和效率。", "motivation": "公共安全监控中的暴力检测至关重要，但面临小目标、复杂环境和实时时间分析等挑战。", "method": "Vi-SAFE框架整合了增强型YOLOv8和TSN。YOLOv8通过GhostNetV3轻量级骨干网络、指数移动平均（EMA）注意力机制和剪枝进行优化，以降低计算成本。YOLOv8负责提取人体区域，TSN则进行暴力行为的二分类。这两个模型分别在行人和暴力数据集上进行训练。", "result": "在RWF-2000数据集上，Vi-SAFE的准确率达到0.88，优于单独使用TSN（0.77），并在准确性和效率上均超越现有方法。", "conclusion": "Vi-SAFE框架被证明对公共安全监控中的暴力检测是有效且高效的，提高了准确性并降低了计算成本。"}}
{"id": "2509.13214", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13214", "abs": "https://arxiv.org/abs/2509.13214", "authors": ["Fei Wang", "Xuecheng Wu", "Zheng Zhang", "Danlei Huang", "Yuheng Huang", "BoWang"], "title": "End4: End-to-end Denoising Diffusion for Diffusion-Based Inpainting Detection", "comment": null, "summary": "The powerful generative capabilities of diffusion models have significantly\nadvanced the field of image synthesis, enhancing both full image generation and\ninpainting-based image editing. Despite their remarkable advancements,\ndiffusion models also raise concerns about potential misuse for malicious\npurposes. However, existing approaches struggle to identify images generated by\ndiffusion-based inpainting models, even when similar inpainted images are\nincluded in their training data. To address this challenge, we propose a novel\ndetection method based on End-to-end denoising diffusion (End4). Specifically,\nEnd4 designs a denoising reconstruction model to improve the alignment degree\nbetween the latent spaces of the reconstruction and detection processes, thus\nreconstructing features that are more conducive to detection. Meanwhile, it\nleverages a Scale-aware Pyramid-like Fusion Module (SPFM) that refines local\nimage features under the guidance of attention pyramid layers at different\nscales, enhancing feature discriminability. Additionally, to evaluate detection\nperformance on inpainted images, we establish a comprehensive benchmark\ncomprising images generated from five distinct masked regions. Extensive\nexperiments demonstrate that our End4 effectively generalizes to unseen masking\npatterns and remains robust under various perturbations. Our code and dataset\nwill be released soon.", "AI": {"tldr": "扩散模型在图像合成方面表现出色，但也引发了滥用担忧。现有方法难以检测扩散模型生成的修复图像。本文提出End4方法，通过改进潜在空间对齐和多尺度特征融合来有效检测扩散修复图像，并在新掩模模式和扰动下表现鲁棒。", "motivation": "扩散模型强大的生成能力带来了图像合成的显著进步，但也带来了恶意使用的风险。然而，现有方法在识别扩散模型生成的修复图像方面存在困难，即使训练数据中包含类似的修复图像。", "method": "本文提出了一种基于端到端去噪扩散（End4）的新型检测方法。具体来说，End4设计了一个去噪重建模型，以改善重建和检测过程潜在空间之间的对齐程度，从而重建更有利于检测的特征。同时，它利用了一个尺度感知金字塔式融合模块（SPFM），在不同尺度的注意力金字塔层的指导下，精炼局部图像特征，增强特征的可区分性。此外，为评估修复图像的检测性能，建立了一个包含五种不同掩模区域生成图像的综合基准。", "result": "广泛的实验表明，End4能有效泛化到未见的掩模模式，并在各种扰动下保持鲁棒性。", "conclusion": "End4是一种有效检测扩散模型生成的修复图像的新方法，解决了现有方法在识别这类图像上的挑战，并在泛化性和鲁棒性方面表现出色。"}}
{"id": "2509.13250", "categories": ["cs.CV", "cs.LG", "I.2.10; I.4.9"], "pdf": "https://arxiv.org/pdf/2509.13250", "abs": "https://arxiv.org/abs/2509.13250", "authors": ["Andi Kuswoyo", "Christos Margadji", "Sebastian W. Pattinson"], "title": "Intelligent Vacuum Thermoforming Process", "comment": "Contains 6 figures in total, 15 pages. Under revision for Journal of\n  Intelligent Manufacturing", "summary": "Ensuring consistent quality in vacuum thermoforming presents challenges due\nto variations in material properties and tooling configurations. This research\nintroduces a vision-based quality control system to predict and optimise\nprocess parameters, thereby enhancing part quality with minimal data\nrequirements. A comprehensive dataset was developed using visual data from\nvacuum-formed samples subjected to various process parameters, supplemented by\nimage augmentation techniques to improve model training. A k-Nearest Neighbour\nalgorithm was subsequently employed to identify adjustments needed in process\nparameters by mapping low-quality parts to their high-quality counterparts. The\nmodel exhibited strong performance in adjusting heating power, heating time,\nand vacuum time to reduce defects and improve production efficiency.", "AI": {"tldr": "本研究提出一种基于视觉的质量控制系统，利用k-NN算法预测和优化真空热成型工艺参数，以少量数据提高零件质量和生产效率。", "motivation": "真空热成型过程中，材料特性和模具配置的变化导致产品质量难以保持一致性，因此需要一种方法来预测和优化工艺参数以确保质量。", "method": "研究开发了一个包含不同工艺参数下真空成型样品视觉数据的综合数据集，并使用图像增强技术进行扩充。随后，采用k-近邻（k-NN）算法将低质量零件映射到高质量零件，以识别所需的工艺参数调整。", "result": "该模型在调整加热功率、加热时间和真空时间方面表现出强大的性能，有效减少了缺陷并提高了生产效率。", "conclusion": "所提出的基于视觉的k-NN质量控制系统能够以最少的数据需求，有效优化真空热成型工艺参数，从而提高零件质量和生产效率。"}}
{"id": "2509.13301", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13301", "abs": "https://arxiv.org/abs/2509.13301", "authors": ["Zefan Qu", "Zhenwei Wang", "Haoyuan Wang", "Ke Xu", "Gerhard Hancke", "Rynson W. H. Lau"], "title": "StyleSculptor: Zero-Shot Style-Controllable 3D Asset Generation with Texture-Geometry Dual Guidance", "comment": "SIGGRAPH Asia 2025 Conference Paper", "summary": "Creating 3D assets that follow the texture and geometry style of existing\nones is often desirable or even inevitable in practical applications like video\ngaming and virtual reality. While impressive progress has been made in\ngenerating 3D objects from text or images, creating style-controllable 3D\nassets remains a complex and challenging problem. In this work, we propose\nStyleSculptor, a novel training-free approach for generating style-guided 3D\nassets from a content image and one or more style images. Unlike previous\nworks, StyleSculptor achieves style-guided 3D generation in a zero-shot manner,\nenabling fine-grained 3D style control that captures the texture, geometry, or\nboth styles of user-provided style images. At the core of StyleSculptor is a\nnovel Style Disentangled Attention (SD-Attn) module, which establishes a\ndynamic interaction between the input content image and style image for\nstyle-guided 3D asset generation via a cross-3D attention mechanism, enabling\nstable feature fusion and effective style-guided generation. To alleviate\nsemantic content leakage, we also introduce a style-disentangled feature\nselection strategy within the SD-Attn module, which leverages the variance of\n3D feature patches to disentangle style- and content-significant channels,\nallowing selective feature injection within the attention framework. With\nSD-Attn, the network can dynamically compute texture-, geometry-, or\nboth-guided features to steer the 3D generation process. Built upon this, we\nfurther propose the Style Guided Control (SGC) mechanism, which enables\nexclusive geometry- or texture-only stylization, as well as adjustable style\nintensity control. Extensive experiments demonstrate that StyleSculptor\noutperforms existing baseline methods in producing high-fidelity 3D assets.", "AI": {"tldr": "StyleSculptor是一种无需训练、零样本的方法，可根据内容图像和风格图像生成风格引导的3D资产，实现对纹理和几何风格的细粒度控制。", "motivation": "在视频游戏和虚拟现实等实际应用中，创建与现有3D资产纹理和几何风格一致的3D资产是常见需求。尽管从文本或图像生成3D对象已取得进展，但创建风格可控的3D资产仍是一个复杂且具有挑战性的问题。", "method": "本文提出了StyleSculptor，一种新颖的无需训练方法，用于从内容图像和一个或多个风格图像生成风格引导的3D资产。其核心是一个新颖的“风格解耦注意力”（SD-Attn）模块，通过跨3D注意力机制在内容图像和风格图像之间建立动态交互，实现稳定的特征融合和有效的风格引导生成。为减轻语义内容泄露，SD-Attn模块引入了风格解耦特征选择策略，利用3D特征块的方差来解耦风格和内容相关通道，实现选择性特征注入。在此基础上，进一步提出了“风格引导控制”（SGC）机制，支持纯几何或纯纹理风格化，以及可调节的风格强度控制。", "result": "广泛的实验表明，StyleSculptor在生成高保真3D资产方面优于现有基线方法。", "conclusion": "StyleSculptor提供了一种新颖有效的零样本方法，能够实现对3D资产纹理、几何或两者风格的细粒度控制，从而生成高质量的风格引导3D内容。"}}
{"id": "2509.13317", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.13317", "abs": "https://arxiv.org/abs/2509.13317", "authors": ["An-Chieh Cheng", "Yang Fu", "Yukang Chen", "Zhijian Liu", "Xiaolong Li", "Subhashree Radhakrishnan", "Song Han", "Yao Lu", "Jan Kautz", "Pavlo Molchanov", "Hongxu Yin", "Xiaolong Wang", "Sifei Liu"], "title": "3D Aware Region Prompted Vision Language Model", "comment": "Project Website: https://www.anjiecheng.me/sr3d", "summary": "We present Spatial Region 3D (SR-3D) aware vision-language model that\nconnects single-view 2D images and multi-view 3D data through a shared visual\ntoken space. SR-3D supports flexible region prompting, allowing users to\nannotate regions with bounding boxes, segmentation masks on any frame, or\ndirectly in 3D, without the need for exhaustive multi-frame labeling. We\nachieve this by enriching 2D visual features with 3D positional embeddings,\nwhich allows the 3D model to draw upon strong 2D priors for more accurate\nspatial reasoning across frames, even when objects of interest do not co-occur\nwithin the same view. Extensive experiments on both general 2D vision language\nand specialized 3D spatial benchmarks demonstrate that SR-3D achieves\nstate-of-the-art performance, underscoring its effectiveness for unifying 2D\nand 3D representation space on scene understanding. Moreover, we observe\napplicability to in-the-wild videos without sensory 3D inputs or ground-truth\n3D annotations, where SR-3D accurately infers spatial relationships and metric\nmeasurements.", "AI": {"tldr": "SR-3D是一个连接2D图像和3D数据的视觉语言模型，通过共享视觉token空间实现灵活的区域提示和增强的3D空间推理，并在多项基准测试中达到SOTA性能。", "motivation": "现有模型在连接单视角2D图像和多视角3D数据，以及在缺乏详尽多帧标注的情况下进行准确的跨帧空间推理方面存在挑战，尤其当感兴趣的物体不在同一视图中共同出现时。", "method": "本文提出了Spatial Region 3D (SR-3D) 感知视觉语言模型。它通过共享的视觉token空间连接单视角2D图像和多视角3D数据。SR-3D支持灵活的区域提示（2D边界框、分割掩码或直接在3D中），无需详尽的多帧标注。其核心是通过3D位置嵌入丰富2D视觉特征，使3D模型能够利用强大的2D先验知识进行更准确的跨帧空间推理。", "result": "SR-3D在通用2D视觉语言和专用3D空间基准测试上均达到了最先进的性能，证明了其在场景理解中统一2D和3D表示空间的有效性。此外，即使在没有传感器3D输入或真值3D标注的野外视频中，SR-3D也能准确推断空间关系和度量测量。", "conclusion": "SR-3D成功地统一了2D和3D表示空间，显著提高了场景理解能力，并在灵活区域提示和跨帧空间推理方面表现出色，展现了其在实际应用中的广泛潜力。"}}
