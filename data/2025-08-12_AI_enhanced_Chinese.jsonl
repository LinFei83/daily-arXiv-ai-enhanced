{"id": "2508.06634", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.06634", "abs": "https://arxiv.org/abs/2508.06634", "authors": ["Hong Zhao", "Jin Wei-Kocsis", "Adel Heidari Akhijahani", "Karen L Butler-Purry"], "title": "Dual-Head Physics-Informed Graph Decision Transformer for Distribution System Restoration", "comment": null, "summary": "Driven by recent advances in sensing and computing, deep reinforcement\nlearning (DRL) technologies have shown great potential for addressing\ndistribution system restoration (DSR) under uncertainty. However, their\ndata-intensive nature and reliance on the Markov Decision Process (MDP)\nassumption limit their ability to handle scenarios that require long-term\ntemporal dependencies or few-shot and zero-shot decision making. Emerging\nDecision Transformers (DTs), which leverage causal transformers for sequence\nmodeling in DRL tasks, offer a promising alternative. However, their reliance\non return-to-go (RTG) cloning and limited generalization capacity restricts\ntheir effectiveness in dynamic power system environments. To address these\nchallenges, we introduce an innovative Dual-Head Physics-informed Graph\nDecision Transformer (DH-PGDT) that integrates physical modeling, structural\nreasoning, and subgoal-based guidance to enable scalable and robust DSR even in\nzero-shot or few-shot scenarios. DH-PGDT features a dual-head physics-informed\ncausal transformer architecture comprising Guidance Head, which generates\nsubgoal representations, and Action Head, which uses these subgoals to generate\nactions independently of RTG. It also incorporates an operational\nconstraint-aware graph reasoning module that encodes power system topology and\noperational constraints to generate a confidence-weighted action vector for\nrefining DT trajectories. This design effectively improves generalization and\nenables robust adaptation to unseen scenarios. While this work focuses on DSR,\nthe underlying computing model of the proposed PGDT is broadly applicable to\nsequential decision making across various power system operations and other\ncomplex engineering domains.", "AI": {"tldr": "针对深度强化学习（DRL）和决策Transformer（DT）在配电系统恢复（DSR）中处理不确定性及零/少样本决策的局限性，本文提出了一种创新的双头物理信息图决策Transformer（DH-PGDT），通过集成物理建模、结构推理和子目标指导，实现DSR在未见场景下的可扩展性和鲁棒性。", "motivation": "现有DRL技术在DSR中存在数据密集、依赖马尔可夫决策过程假设、难以处理长期时间依赖及零/少样本决策的局限性。新兴的决策Transformer（DT）虽有潜力，但其依赖回报到目标（RTG）克隆和泛化能力有限，限制了其在动态电力系统环境中的有效性。因此，需要一种能应对这些挑战的鲁棒、可扩展的DSR方法。", "method": "本文提出DH-PGDT模型，其核心是一个双头物理信息因果Transformer架构：引导头（Guidance Head）生成子目标表示，动作头（Action Head）利用子目标独立于RTG生成动作。此外，模型整合了操作约束感知图推理模块，该模块编码电力系统拓扑和操作约束，生成置信度加权的动作向量以优化DT轨迹。", "result": "DH-PGDT实现了可扩展和鲁棒的DSR，即使在零样本或少样本场景下也能有效运行。该设计显著提高了泛化能力，并能鲁棒地适应未见场景。", "conclusion": "DH-PGDT通过结合物理建模、结构推理和子目标指导，克服了DRL和DT在处理DSR不确定性及零/少样本决策方面的挑战。所提出的PGDT计算模型具有广泛适用性，可应用于各种电力系统操作及其他复杂工程领域的序列决策任务。"}}
{"id": "2508.06708", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.06708", "abs": "https://arxiv.org/abs/2508.06708", "authors": ["Justin London"], "title": "Embedded Microcontrol for Photovoltaic Water Pumping System", "comment": null, "summary": "We introduce a novel 3-axis solar tracker water pumping system. The charge\ngenerated from solar energy converted by the photovolatic panel (PV) cells is\nstored in a 12V battery that in turn powers two water diaphragm pumps using a\nsolar charge controller that includes an MPPT algorithm that serves as a DC-DC\nconverter. The system is analyzed from an embedded microcontroller and embedded\nsoftware perspective using Arduino. The photovoltaic panel uses four light\nphotocell resistors (LPRs) which measure solar light intensity. An ultrasonic\nsensor measures the water level in a reservoir water tank. If the water level\nis too low, water is pumped from one water tank to the reservoir tank. Using a\nsoil moisture sensor, another water pump pumps water from the reservoir tank to\nthe plant if water is needed. Circuit designs for the system are provided as\nwell as the embedded software used. Simulation and experimental results are\ngiven.", "AI": {"tldr": "本文介绍了一种基于Arduino的、新型三轴太阳能跟踪水泵系统，利用太阳能为水泵供电，并通过多种传感器（光敏电阻、超声波、土壤湿度）智能控制水在水箱和植物之间的输送。", "motivation": "开发一个高效、自动化且由太阳能驱动的水泵系统，以实现水资源的智能管理和利用，特别是在灌溉或水供给方面。", "method": "系统采用光伏板将太阳能转换为电能，储存在12V电池中，并通过带有MPPT算法的太阳能充电控制器（兼作DC-DC转换器）为两个隔膜水泵供电。控制核心为Arduino微控制器，利用四个光敏电阻实现太阳能板的三轴跟踪，超声波传感器监测水箱水位，土壤湿度传感器检测植物需水情况。根据传感器数据，系统自动将水从一个水箱泵入储水箱（当水位低时），或从储水箱泵入植物（当土壤干燥时）。论文提供了电路设计、嵌入式软件，并进行了仿真和实验验证。", "result": "研究成功设计并实现了该太阳能跟踪水泵系统，包括其电路和嵌入式软件。通过仿真和实验结果，验证了系统的功能性和可行性，能够根据光照强度、水箱水位和土壤湿度智能地进行水泵操作。", "conclusion": "开发并验证了一个功能完善的三轴太阳能跟踪水泵系统，该系统通过集成太阳能转换、电池储能、智能控制和多种传感器，实现了自动化、高效的水资源管理和输送，具有实际应用潜力。"}}
{"id": "2508.06728", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.06728", "abs": "https://arxiv.org/abs/2508.06728", "authors": ["Antar Kumar Biswas", "Masoud H. Nazari"], "title": "Secure and Decentralized Peer-to-Peer Energy Transactions using Blockchain Technology", "comment": null, "summary": "This paper presents an optimal peer-to-peer (P2P) energy transaction\nmechanism leveraging decentralized blockchain technology to enable a secure and\nscalable retail electricity market for the increasing penetration of\ndistributed energy resources (DERs). A decentralized bidding strategy is\nproposed to maximize individual profits while collectively enhancing social\nwelfare. The market design and transaction processes are simulated using the\nEthereum testnet, demonstrating the blockchain network's capability to ensure\nsecure, transparent, and sustainable P2P energy trading among DER participants.", "AI": {"tldr": "本文提出了一种基于去中心化区块链技术的P2P能源交易机制，旨在为分布式能源提供安全、可扩展的零售电力市场。", "motivation": "随着分布式能源（DERs）的日益普及，需要一种安全、可扩展的零售电力市场来支持P2P能源交易。", "method": "提出了一种去中心化竞价策略以最大化个体利润并提升社会福利。市场设计和交易过程通过以太坊测试网进行模拟。", "result": "模拟结果表明，区块链网络能够确保分布式能源参与者之间P2P能源交易的安全性、透明性和可持续性，同时实现了个体利润最大化和集体社会福利的提升。", "conclusion": "基于区块链的P2P能源交易机制是实现分布式能源安全、透明和可持续零售电力市场的有效方案。"}}
{"id": "2508.07760", "categories": ["eess.IV", "cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2508.07760", "abs": "https://arxiv.org/abs/2508.07760", "authors": ["Maximilian Kromer", "Panagiotis Agrafiotis", "Begüm Demir"], "title": "Sea-Undistort: A Dataset for Through-Water Image Restoration in High Resolution Airborne Bathymetric Mapping", "comment": "Under review in IEEE Geoscience and Remote Sensing Letters", "summary": "Accurate image-based bathymetric mapping in shallow waters remains\nchallenging due to the complex optical distortions such as wave induced\npatterns, scattering and sunglint, introduced by the dynamic water surface, the\nwater column properties, and solar illumination. In this work, we introduce\nSea-Undistort, a comprehensive synthetic dataset of 1200 paired 512x512\nthrough-water scenes rendered in Blender. Each pair comprises a distortion-free\nand a distorted view, featuring realistic water effects such as sun glint,\nwaves, and scattering over diverse seabeds. Accompanied by per-image metadata\nsuch as camera parameters, sun position, and average depth, Sea-Undistort\nenables supervised training that is otherwise infeasible in real environments.\nWe use Sea-Undistort to benchmark two state-of-the-art image restoration\nmethods alongside an enhanced lightweight diffusion-based framework with an\nearly-fusion sun-glint mask. When applied to real aerial data, the enhanced\ndiffusion model delivers more complete Digital Surface Models (DSMs) of the\nseabed, especially in deeper areas, reduces bathymetric errors, suppresses\nglint and scattering, and crisply restores fine seabed details. Dataset,\nweights, and code are publicly available at\nhttps://www.magicbathy.eu/Sea-Undistort.html.", "AI": {"tldr": "该研究引入了Sea-Undistort合成数据集，用于解决浅水区域基于图像的水深测量中光学畸变问题，并利用该数据集训练了一个增强型扩散模型，显著提高了真实航空数据的海底数字表面模型（DSM）的质量和精度。", "motivation": "由于动态水面、水柱特性和太阳光照引入的复杂光学畸变（如波浪模式、散射和太阳眩光），基于图像的浅水水深测量仍然具有挑战性。", "method": "引入了一个名为Sea-Undistort的综合合成数据集，包含1200对512x512的穿水场景（无畸变和有畸变视图），在Blender中渲染，模拟了逼真的水面效果和多样的海底。数据集提供每张图像的元数据，如相机参数、太阳位置和平均深度，以支持监督训练。该研究使用Sea-Undistort数据集对两种最先进的图像恢复方法以及一个增强的轻量级基于扩散的框架（带有早期融合的太阳眩光掩码）进行了基准测试。", "result": "当应用于真实的航空数据时，增强型扩散模型能够生成更完整的海底数字表面模型（DSM），尤其是在较深区域，减少了水深测量误差，抑制了眩光和散射，并清晰地恢复了海底的精细细节。", "conclusion": "Sea-Undistort数据集及其提出的增强型扩散模型有效地解决了浅水水深测量中的光学畸变问题，显著提升了海底数字表面模型的完整性、准确性和细节恢复能力。"}}
{"id": "2508.06864", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.06864", "abs": "https://arxiv.org/abs/2508.06864", "authors": ["Bing Li", "Haoming Guo", "Zhiyuan Ren", "Wenchi Cheng", "Jialin Hu", "Xinke Jian"], "title": "Collaborative Computing Strategy Based SINS Prediction for Emergency UAVs Network", "comment": null, "summary": "In emergency scenarios, the dynamic and harsh conditions necessitate timely\ntrajectory adjustments for drones, leading to highly dynamic network topologies\nand potential task failures. To address these challenges, a collaborative\ncomputing strategy based strapdown inertial navigation system (SINS) prediction\nfor emergency UAVs network (EUN) is proposed, where a two-step weighted time\nexpanded graph (WTEG) is constructed to deal with dynamic network topology\nchanges. Furthermore, the task scheduling is formulated as a Directed Acyclic\nGraph (DAG) to WTEG mapping problem to achieve collaborative computing while\ntransmitting among UAVs. Finally, the binary particle swarm optimization (BPSO)\nalgorithm is employed to choose the mapping strategy that minimizes end-to-end\nprocessing latency. The simulation results validate that the collaborative\ncomputing strategy significantly outperforms both cloud and local computing in\nterms of latency. Moreover, the task success rate using SINS is substantially\nimproved compared to approaches without prior prediction.", "AI": {"tldr": "针对紧急场景下无人机网络动态拓扑和任务失败问题，本文提出一种基于捷联惯导系统（SINS）预测的协同计算策略，通过加权时间扩展图（WTEG）处理动态网络，并将任务调度建模为DAG到WTEG的映射问题，利用BPSO优化以最小化端到端延迟并提高任务成功率。", "motivation": "在紧急情况下，无人机需要及时调整轨迹，导致网络拓扑高度动态化，并可能引发任务失败。", "method": "1. 提出一种基于捷联惯导系统（SINS）预测的紧急无人机网络（EUN）协同计算策略。2. 构建两步加权时间扩展图（WTEG）以应对动态网络拓扑变化。3. 将任务调度建模为有向无环图（DAG）到WTEG的映射问题，以实现无人机之间的协同计算和传输。4. 采用二元粒子群优化（BPSO）算法选择最小化端到端处理延迟的映射策略。", "result": "1. 协同计算策略在延迟方面显著优于云端计算和本地计算。2. 使用SINS预测的任务成功率比没有先验预测的方法显著提高。", "conclusion": "所提出的基于SINS预测的协同计算策略能有效应对紧急无人机网络中的挑战，显著降低延迟并提高任务成功率。"}}
{"id": "2508.07773", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07773", "abs": "https://arxiv.org/abs/2508.07773", "authors": ["Mohammed Salah", "Numan Saeed", "Davor Svetinovic", "Stefano Sfarra", "Mohammed Omar", "Yusra Abdulrahman"], "title": "PCA-Guided Autoencoding for Structured Dimensionality Reduction in Active Infrared Thermography", "comment": "Infrared thermography, Non-Destructive Testing, Principal Component\n  Analysis, PCA-Guided Autoencoder, PCA Distillation Loss, Dimensionality\n  Reduction", "summary": "Active Infrared thermography (AIRT) is a widely adopted non-destructive\ntesting (NDT) technique for detecting subsurface anomalies in industrial\ncomponents. Due to the high dimensionality of AIRT data, current approaches\nemploy non-linear autoencoders (AEs) for dimensionality reduction. However, the\nlatent space learned by AIRT AEs lacks structure, limiting their effectiveness\nin downstream defect characterization tasks. To address this limitation, this\npaper proposes a principal component analysis guided (PCA-guided) autoencoding\nframework for structured dimensionality reduction to capture intricate,\nnon-linear features in thermographic signals while enforcing a structured\nlatent space. A novel loss function, PCA distillation loss, is introduced to\nguide AIRT AEs to align the latent representation with structured PCA\ncomponents while capturing the intricate, non-linear patterns in thermographic\nsignals. To evaluate the utility of the learned, structured latent space, we\npropose a neural network-based evaluation metric that assesses its suitability\nfor defect characterization. Experimental results show that the proposed\nPCA-guided AE outperforms state-of-the-art dimensionality reduction methods on\nPVC, CFRP, and PLA samples in terms of contrast, signal-to-noise ratio (SNR),\nand neural network-based metrics.", "AI": {"tldr": "本文提出了一种PCA引导的自编码框架，用于主动红外热成像（AIRT）数据的结构化降维，以提高缺陷表征的有效性。", "motivation": "主动红外热成像（AIRT）数据维度高，现有非线性自编码器（AEs）降维后学习到的潜在空间缺乏结构，限制了其在下游缺陷表征任务中的有效性。", "method": "提出了一种主成分分析（PCA）引导的自编码框架，旨在捕获热成像信号中复杂的非线性特征，同时强制生成结构化的潜在空间。引入了一种新颖的PCA蒸馏损失函数，以引导AIRT自编码器将潜在表示与结构化的PCA组件对齐。同时，提出了一种基于神经网络的评估指标来评估学习到的结构化潜在空间的实用性。", "result": "实验结果表明，所提出的PCA引导AE在PVC、CFRP和PLA样本上，在对比度、信噪比（SNR）和基于神经网络的指标方面，均优于现有最先进的降维方法。", "conclusion": "该PCA引导的自编码框架能有效对AIRT数据进行结构化降维，生成更具结构的潜在空间，从而显著提升了缺陷表征任务的性能。"}}
{"id": "2508.06559", "categories": ["cs.AI", "cs.GT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06559", "abs": "https://arxiv.org/abs/2508.06559", "authors": ["Sina Baghal"], "title": "Solving Pasur Using GPU-Accelerated Counterfactual Regret Minimization", "comment": null, "summary": "Pasur is a fishing card game played over six rounds and is played similarly\nto games such as Cassino and Scopa, and Bastra. This paper introduces a\nCUDA-accelerated computational framework for simulating Pasur, emphasizing\nefficient memory management. We use our framework to compute near-Nash\nequilibria via Counterfactual Regret Minimization (CFR), a well-known algorithm\nfor solving large imperfect-information games.\n  Solving Pasur presents unique challenges due to its intricate rules and the\nlarge size of its game tree. We handle rule complexity using PyTorch CUDA\ntensors and to address the memory-intensive nature of the game, we decompose\nthe game tree into two key components: (1) actual game states, and (2)\ninherited scores from previous rounds. We construct the Full Game Tree by\npairing card states with accumulated scores in the Unfolding Process. This\ndesign reduces memory overhead by storing only essential strategy values and\nnode connections. To further manage computational complexity, we apply a\nround-by-round backward training strategy, starting from the final round and\nrecursively propagating average utilities to earlier stages. Our approach\nconstructs the complete game tree, which on average consists of over $10^9$\nnodes. We provide detailed implementation snippets.\n  After computing a near-Nash equilibrium strategy, we train a tree-based model\nto predict these strategies for use during gameplay. We then estimate the fair\nvalue of each deck through large-scale self-play between equilibrium strategies\nby simulating, for instance, 10,000 games per matchup, executed in parallel\nusing GPU acceleration.\n  Similar frameworks can be extended to other reinforcement learning algorithms\nwhere the action tree naturally decomposes into multiple rounds such as\nturn-based strategy games or sequential trading decisions in financial markets.", "AI": {"tldr": "本文提出了一个基于CUDA加速的计算框架，通过反事实遗憾最小化（CFR）算法，结合内存高效的游戏树分解和分轮反向训练策略，计算了复杂纸牌游戏Pasur的近似纳什均衡策略，并估计了牌组的公平价值。", "motivation": "Pasur游戏规则复杂且游戏树规模庞大，对求解构成独特挑战。研究旨在开发一个高效的计算框架来解决这类大型不完美信息博弈。", "method": "使用CUDA加速计算框架和PyTorch CUDA张量处理规则复杂性。将游戏树分解为实际游戏状态和继承得分两部分，通过“展开过程”构建完整游戏树，仅存储关键策略值和节点连接以减少内存开销。采用分轮反向训练策略，从最后一轮开始递归传播平均效用。利用反事实遗憾最小化（CFR）算法计算近似纳什均衡。训练基于树的模型预测策略，并通过大规模自对弈模拟估算牌组公平价值。", "result": "成功构建了平均包含超过10亿节点的完整游戏树。框架能够计算Pasur游戏的近似纳什均衡策略。通过大规模GPU加速自对弈，估计了每个牌组的公平价值。", "conclusion": "所提出的计算框架能有效解决Pasur这类复杂的非完美信息博弈，并可推广应用于其他行动树自然分解为多轮的强化学习算法，例如回合制策略游戏或金融市场中的序列交易决策。"}}
{"id": "2508.06518", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06518", "abs": "https://arxiv.org/abs/2508.06518", "authors": ["Ray Wai Man Kong"], "title": "Automated Seam Folding and Sewing Machine on Pleated Pants for Apparel Manufacturing", "comment": "13 pages, 9 figures", "summary": "The applied research is the design and development of an automated folding\nand sewing machine for pleated pants. It represents a significant advancement\nin addressing the challenges associated with manual sewing processes.\nTraditional methods for creating pleats are labour-intensive, prone to\ninconsistencies, and require high levels of skill, making automation a critical\nneed in the apparel industry. This research explores the technical feasibility\nand operational benefits of integrating advanced technologies into garment\nproduction, focusing on the creation of an automated machine capable of precise\nfolding and sewing operations and eliminating the marking operation.\n  The proposed machine incorporates key features such as a precision folding\nmechanism integrated into the automated sewing unit with real-time monitoring\ncapabilities. The results demonstrate remarkable improvements: the standard\nlabour time has been reduced by 93%, dropping from 117 seconds per piece to\njust 8 seconds with the automated system. Similarly, machinery time improved by\n73%, and the total output rate increased by 72%. These enhancements translate\ninto a cycle time reduction from 117 seconds per piece to an impressive 33\nseconds, enabling manufacturers to meet customer demand more swiftly. By\neliminating manual marking processes, the machine not only reduces labour costs\nbut also minimizes waste through consistent pleat formation. This automation\naligns with industry trends toward sustainability and efficiency, potentially\nreducing environmental impact by decreasing material waste and energy\nconsumption.", "AI": {"tldr": "该研究设计并开发了一种用于打褶裤的自动化折叠缝纫机，显著提高了生产效率，降低了劳动时间和废料。", "motivation": "传统的打褶裤缝纫方法劳动密集、易出错且对技能要求高，服装行业急需自动化来解决这些挑战。", "method": "开发了一种自动化机器，集成了精密折叠机构和实时监控功能到自动化缝纫单元中，并消除了人工标记操作。", "result": "自动化系统将标准工时从每件117秒减少到8秒（降低93%），机器时间提高了73%，总产量提高了72%。周期时间从117秒减少到33秒。此外，通过消除人工标记过程，减少了劳动力成本和材料浪费。", "conclusion": "所提出的自动化机器显著提高了生产效率，降低了成本和浪费，使其能够更快地满足客户需求，并符合行业向可持续性和效率发展的趋势。"}}
{"id": "2508.06495", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.06495", "abs": "https://arxiv.org/abs/2508.06495", "authors": ["Juliana Resplande Sant'anna Gomes", "Arlindo Rodrigues Galvão Filho"], "title": "Semi-automated Fact-checking in Portuguese: Corpora Enrichment using Retrieval with Claim extraction", "comment": "Master Thesis in Computer Science at Federal University on Goias\n  (UFG). Written in Portuguese", "summary": "The accelerated dissemination of disinformation often outpaces the capacity\nfor manual fact-checking, highlighting the urgent need for Semi-Automated\nFact-Checking (SAFC) systems. Within the Portuguese language context, there is\na noted scarcity of publicly available datasets that integrate external\nevidence, an essential component for developing robust AFC systems, as many\nexisting resources focus solely on classification based on intrinsic text\nfeatures. This dissertation addresses this gap by developing, applying, and\nanalyzing a methodology to enrich Portuguese news corpora (Fake.Br, COVID19.BR,\nMuMiN-PT) with external evidence. The approach simulates a user's verification\nprocess, employing Large Language Models (LLMs, specifically Gemini 1.5 Flash)\nto extract the main claim from texts and search engine APIs (Google Search API,\nGoogle FactCheck Claims Search API) to retrieve relevant external documents\n(evidence). Additionally, a data validation and preprocessing framework,\nincluding near-duplicate detection, is introduced to enhance the quality of the\nbase corpora.", "AI": {"tldr": "该论文旨在通过开发一种利用大型语言模型和搜索引擎API的方法，为葡萄牙语新闻语料库（Fake.Br, COVID19.BR, MuMiN-PT）添加外部证据，以解决半自动化事实核查系统中缺乏集成外部证据的公开数据集的问题。", "motivation": "手动事实核查的速度跟不上虚假信息的传播，凸显了半自动化事实核查系统的紧迫需求。葡萄牙语语境下，缺乏集成外部证据的公开数据集，而现有资源多侧重于基于文本内在特征的分类，这阻碍了健壮的自动化事实核查系统的发展。", "method": "开发并应用了一种方法，通过模拟用户验证过程来丰富葡萄牙语新闻语料库。具体包括：使用大型语言模型（Gemini 1.5 Flash）提取文本中的主要主张；利用搜索引擎API（Google Search API, Google FactCheck Claims Search API）检索相关外部文档（证据）；引入数据验证和预处理框架，包括近重复检测，以提高基础语料库的质量。", "result": "成功开发、应用并分析了一种将外部证据集成到葡萄牙语新闻语料库（Fake.Br, COVID19.BR, MuMiN-PT）中的方法，有效模拟了用户的事实核查过程，并提升了语料库的质量。", "conclusion": "该研究通过为葡萄牙语新闻语料库丰富外部证据，有效填补了该领域公开数据集的空白，为开发更健壮的半自动化事实核查系统奠定了基础。"}}
{"id": "2508.06496", "categories": ["cs.CV", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.06496", "abs": "https://arxiv.org/abs/2508.06496", "authors": ["Rakesh Raj Madavan", "Akshat Kaimal", "Hashim Faisal", "Chandrakala S"], "title": "Med-GRIM: Enhanced Zero-Shot Medical VQA using prompt-embedded Multimodal Graph RAG", "comment": null, "summary": "An ensemble of trained multimodal encoders and vision-language models (VLMs)\nhas become a standard approach for visual question answering (VQA) tasks.\nHowever, such models often fail to produce responses with the detailed\nprecision necessary for complex, domain-specific applications such as medical\nVQA. Our representation model, BIND: BLIVA Integrated with Dense Encoding,\nextends prior multimodal work by refining the joint embedding space through\ndense, query-token-based encodings inspired by contrastive pretraining\ntechniques. This refined encoder powers Med-GRIM, a model designed for medical\nVQA tasks that leverages graph-based retrieval and prompt engineering to\nintegrate domain-specific knowledge. Rather than relying on compute-heavy\nfine-tuning of vision and language models on specific datasets, Med-GRIM\napplies a low-compute, modular workflow with small language models (SLMs) for\nefficiency. Med-GRIM employs prompt-based retrieval to dynamically inject\nrelevant knowledge, ensuring both accuracy and robustness in its responses. By\nassigning distinct roles to each agent within the VQA system, Med-GRIM achieves\nlarge language model performance at a fraction of the computational cost.\nAdditionally, to support scalable research in zero-shot multimodal medical\napplications, we introduce DermaGraph, a novel Graph-RAG dataset comprising\ndiverse dermatological conditions. This dataset facilitates both multimodal and\nunimodal querying. The code and dataset are available at:\nhttps://github.com/Rakesh-123-cryp/Med-GRIM.git", "AI": {"tldr": "该研究提出了BIND（一种通过密集编码改进联合嵌入空间的表示模型）和Med-GRIM（一个基于图检索、提示工程和小语言模型的高效医疗VQA系统），旨在解决现有模型在医疗VQA中精度不足和计算成本高的问题，并引入了新的皮肤病数据集DermaGraph。", "motivation": "现有的多模态编码器和视觉-语言模型（VLMs）在复杂、领域特定的应用（如医疗VQA）中，往往无法生成足够精确的响应。此外，对特定数据集进行计算密集型微调大型视觉和语言模型的成本过高。", "method": "1. **BIND（BLIVA Integrated with Dense Encoding）**：通过受对比预训练启发的密集、基于查询令牌的编码来优化联合嵌入空间，作为其精炼的编码器。2. **Med-GRIM**：一个专为医疗VQA设计的模型，利用BIND，并结合图基检索和提示工程来整合领域知识。它采用低计算量、模块化的工作流，使用小语言模型（SLMs）提高效率，并通过基于提示的检索动态注入相关知识。3. **DermaGraph**：引入了一个新的图-RAG数据集，包含多样化的皮肤病状况，支持多模态和单模态查询，以促进零样本多模态医疗应用研究。", "result": "Med-GRIM在计算成本仅为大型语言模型一小部分的情况下，实现了大型语言模型（LLM）的性能。它确保了响应的准确性和鲁棒性。", "conclusion": "Med-GRIM通过整合精炼的编码器（BIND）、图基知识检索、提示工程和SLMs，为医疗VQA提供了一个高效且准确的解决方案。同时，新引入的DermaGraph数据集将支持未来在零样本医疗应用领域的可扩展研究。"}}
{"id": "2508.06893", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.06893", "abs": "https://arxiv.org/abs/2508.06893", "authors": ["Evagoras Makridis", "Gabriele Oliva", "Apostolos I. Rikos", "Themistoklis Charalambous"], "title": "Average Consensus with Dynamic Compression in Bandwidth-Limited Directed Networks", "comment": null, "summary": "In this paper, the average consensus problem has been considered for directed\nunbalanced networks under finite bit-rate communication. We propose the\nPush-Pull Average Consensus algorithm with Dynamic Compression (PP-ACDC)\nalgorithm, a distributed consensus algorithm that deploys an adaptive\nquantization scheme and achieves convergence to the exact average without the\nneed of global information. A preliminary numerical convergence analysis and\nsimulation results corroborate the performance of PP-ACDC.", "AI": {"tldr": "本文提出了一种名为PP-ACDC的分布式算法，用于在有限比特率通信的非平衡有向网络中实现精确平均共识，无需全局信息。", "motivation": "研究动机是在有向非平衡网络和有限比特率通信的约束下，解决平均共识问题。", "method": "提出了Push-Pull平均共识动态压缩（PP-ACDC）算法，该算法采用自适应量化方案。", "result": "PP-ACDC算法无需全局信息即可收敛到精确平均值。初步的数值收敛分析和仿真结果证实了其性能。", "conclusion": "PP-ACDC算法能有效解决有限比特率通信下非平衡有向网络中的平均共识问题，并实现精确收敛。"}}
{"id": "2508.07815", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2508.07815", "abs": "https://arxiv.org/abs/2508.07815", "authors": ["Yousef Sadegheih", "Dorit Merhof"], "title": "Deep Learning-Based Desikan-Killiany Parcellation of the Brain Using Diffusion MRI", "comment": null, "summary": "Accurate brain parcellation in diffusion MRI (dMRI) space is essential for\nadvanced neuroimaging analyses. However, most existing approaches rely on\nanatomical MRI for segmentation and inter-modality registration, a process that\ncan introduce errors and limit the versatility of the technique. In this study,\nwe present a novel deep learning-based framework for direct parcellation based\non the Desikan-Killiany (DK) atlas using only diffusion MRI data. Our method\nutilizes a hierarchical, two-stage segmentation network: the first stage\nperforms coarse parcellation into broad brain regions, and the second stage\nrefines the segmentation to delineate more detailed subregions within each\ncoarse category. We conduct an extensive ablation study to evaluate various\ndiffusion-derived parameter maps, identifying an optimal combination of\nfractional anisotropy, trace, sphericity, and maximum eigenvalue that enhances\nparellation accuracy. When evaluated on the Human Connectome Project and\nConsortium for Neuropsychiatric Phenomics datasets, our approach achieves\nsuperior Dice Similarity Coefficients compared to existing state-of-the-art\nmodels. Additionally, our method demonstrates robust generalization across\ndifferent image resolutions and acquisition protocols, producing more\nhomogeneous parcellations as measured by the relative standard deviation within\nregions. This work represents a significant advancement in dMRI-based brain\nsegmentation, providing a precise, reliable, and registration-free solution\nthat is critical for improved structural connectivity and microstructural\nanalyses in both research and clinical applications. The implementation of our\nmethod is publicly available on github.com/xmindflow/DKParcellationdMRI.", "AI": {"tldr": "该研究提出了一种新颖的深度学习框架，可以直接基于弥散MRI（dMRI）数据进行大脑Desikan-Killiany图谱分区，避免了对解剖MRI的依赖和跨模态配准的需求。", "motivation": "现有的脑分区方法大多依赖解剖MRI进行分割并需要跨模态配准，这会引入误差并限制技术的通用性。", "method": "该方法采用一个分层的两阶段分割网络：第一阶段进行粗略的脑区域分区，第二阶段对粗略区域内的子区域进行精细分割。通过广泛的消融研究，确定了分数各向异性、迹、球形度和最大特征值等弥散参数的最佳组合，以提高分区精度。", "result": "在人类连接组计划（HCP）和神经精神表型联盟（CNP）数据集上评估，该方法比现有最先进模型取得了更高的Dice相似系数。此外，该方法在不同图像分辨率和采集协议下表现出强大的泛化能力，并产生了更均匀的分区（通过区域内相对标准差衡量）。", "conclusion": "这项工作代表了dMRI脑分割领域的重大进步，提供了一种精确、可靠且无需配准的解决方案，对结构连接和微结构分析在研究和临床应用中至关重要。"}}
{"id": "2508.06569", "categories": ["cs.AI", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.06569", "abs": "https://arxiv.org/abs/2508.06569", "authors": ["Lance Yao", "Suman Samantray", "Ayana Ghosh", "Kevin Roccapriore", "Libor Kovarik", "Sarah Allec", "Maxim Ziatdinov"], "title": "Operationalizing Serendipity: Multi-Agent AI Workflows for Enhanced Materials Characterization with Theory-in-the-Loop", "comment": null, "summary": "The history of science is punctuated by serendipitous discoveries, where\nunexpected observations, rather than targeted hypotheses, opened new fields of\ninquiry. While modern autonomous laboratories excel at accelerating hypothesis\ntesting, their optimization for efficiency risks overlooking these crucial,\nunplanned findings. To address this gap, we introduce SciLink, an open-source,\nmulti-agent artificial intelligence framework designed to operationalize\nserendipity in materials research by creating a direct, automated link between\nexperimental observation, novelty assessment, and theoretical simulations. The\nframework employs a hybrid AI strategy where specialized machine learning\nmodels perform quantitative analysis of experimental data, while large language\nmodels handle higher-level reasoning. These agents autonomously convert raw\ndata from materials characterization techniques into falsifiable scientific\nclaims, which are then quantitatively scored for novelty against the published\nliterature. We demonstrate the framework's versatility across diverse research\nscenarios, showcasing its application to atomic-resolution and hyperspectral\ndata, its capacity to integrate real-time human expert guidance, and its\nability to close the research loop by proposing targeted follow-up experiments.\nBy systematically analyzing all observations and contextualizing them, SciLink\nprovides a practical framework for AI-driven materials research that not only\nenhances efficiency but also actively cultivates an environment ripe for\nserendipitous discoveries, thereby bridging the gap between automated\nexperimentation and open-ended scientific exploration.", "AI": {"tldr": "本文提出了SciLink，一个开源多智能体AI框架，旨在通过自动化实验观察、新颖性评估和理论模拟之间的联系，在材料研究中实现意外发现的自动化，从而弥补现代自动化实验室可能忽视非预期发现的不足。", "motivation": "现代自动化实验室虽然擅长加速假设验证并提高效率，但其优化策略可能导致忽视意外的、非计划性的重要发现。研究旨在弥补这一空白，使AI驱动的研究也能主动促进意外发现。", "method": "引入SciLink，一个开源、多智能体人工智能框架。该框架采用混合AI策略，其中专门的机器学习模型用于实验数据的定量分析，而大型语言模型则处理更高层次的推理。这些智能体能将材料表征的原始数据转化为可证伪的科学主张，并根据已发表文献对其新颖性进行定量评分。", "result": "该框架展示了其在不同研究场景中的多功能性，包括应用于原子分辨率和高光谱数据，集成实时人类专家指导的能力，以及通过提出有针对性的后续实验来闭合研究循环的能力。", "conclusion": "SciLink通过系统分析和情境化所有观察结果，为AI驱动的材料研究提供了一个实用框架，它不仅提高了效率，而且积极培养了有利于意外发现的环境，从而弥合了自动化实验与开放式科学探索之间的差距。"}}
{"id": "2508.06520", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.06520", "abs": "https://arxiv.org/abs/2508.06520", "authors": ["Liwei Chen", "Tong Qin", "Zhenhua Huangfu", "Li Li", "Wei Wei"], "title": "Optimization of Flip-Landing Trajectories for Starship based on a Deep Learned Simulator", "comment": null, "summary": "We propose a differentiable optimization framework for flip-and-landing\ntrajectory design of reusable spacecraft, exemplified by the Starship vehicle.\nA deep neural network surrogate, trained on high-fidelity CFD data, predicts\naerodynamic forces and moments, and is tightly coupled with a differentiable\nrigid-body dynamics solver. This enables end-to-end gradient-based trajectory\noptimization without linearization or convex relaxation. The framework handles\nactuator limits and terminal landing constraints, producing physically\nconsistent, optimized control sequences. Both standard automatic\ndifferentiation and Neural ODEs are applied to support long-horizon rollouts.\nResults demonstrate the framework's effectiveness in modeling and optimizing\ncomplex maneuvers with high nonlinearities. This work lays the groundwork for\nfuture extensions involving unsteady aerodynamics, plume interactions, and\nintelligent guidance design.", "AI": {"tldr": "该研究提出了一个可微分优化框架，用于设计可重复使用航天器（如星舰）的翻转和着陆轨迹，通过深度神经网络预测气动数据，并与可微分刚体动力学求解器结合，实现端到端梯度优化。", "motivation": "设计可重复使用航天器（如星舰）的翻转和着陆轨迹面临高非线性、复杂机动和严格约束的挑战，传统方法常需线性化或凸松弛，限制了优化效果。", "method": "核心方法是一个可微分优化框架。它将基于高保真CFD数据训练的深度神经网络替代模型，用于预测气动力和力矩，并将其与一个可微分的刚体动力学求解器紧密耦合。这实现了端到端的梯度优化，无需线性化或凸松弛。框架能处理作动器限制和终端着陆约束，并支持标准自动微分和神经ODE进行长周期推演。", "result": "该框架能够有效建模和优化具有高度非线性的复杂机动，生成物理上一致的优化控制序列。", "conclusion": "该工作为未来涉及非定常气动、羽流相互作用和智能制导设计等方面的扩展奠定了基础。"}}
{"id": "2508.06504", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06504", "abs": "https://arxiv.org/abs/2508.06504", "authors": ["Yao Ge", "Sudeshna Das", "Yuting Guo", "Abeed Sarker"], "title": "Retrieval augmented generation based dynamic prompting for few-shot biomedical named entity recognition using large language models", "comment": "31 pages, 4 figures, 15 tables", "summary": "Biomedical named entity recognition (NER) is a high-utility natural language\nprocessing (NLP) task, and large language models (LLMs) show promise\nparticularly in few-shot settings (i.e., limited training data). In this\narticle, we address the performance challenges of LLMs for few-shot biomedical\nNER by investigating a dynamic prompting strategy involving retrieval-augmented\ngeneration (RAG). In our approach, the annotated in-context learning examples\nare selected based on their similarities with the input texts, and the prompt\nis dynamically updated for each instance during inference. We implemented and\noptimized static and dynamic prompt engineering techniques and evaluated them\non five biomedical NER datasets. Static prompting with structured components\nincreased average F1-scores by 12% for GPT-4, and 11% for GPT-3.5 and LLaMA\n3-70B, relative to basic static prompting. Dynamic prompting further improved\nperformance, with TF-IDF and SBERT retrieval methods yielding the best results,\nimproving average F1-scores by 7.3% and 5.6% in 5-shot and 10-shot settings,\nrespectively. These findings highlight the utility of contextually adaptive\nprompts via RAG for biomedical NER.", "AI": {"tldr": "该研究通过引入检索增强生成（RAG）的动态提示策略，显著提升了大型语言模型（LLMs）在小样本生物医学命名实体识别（NER）任务中的性能。", "motivation": "大型语言模型在小样本（即有限训练数据）设置下，在生物医学命名实体识别方面显示出潜力，但其性能仍面临挑战。", "method": "研究采用了一种动态提示策略，结合了检索增强生成（RAG）。该方法根据输入文本的相似性选择带注释的上下文学习示例，并在推理过程中为每个实例动态更新提示。同时，还实施并优化了静态和动态提示工程技术，并在五个生物医学NER数据集上进行了评估，比较了TF-IDF和SBERT等检索方法。", "result": "相对于基本静态提示，带有结构化组件的静态提示使GPT-4的平均F1分数提高了12%，GPT-3.5和LLaMA 3-70B提高了11%。动态提示进一步提升了性能，其中TF-IDF和SBERT检索方法表现最佳，在5样本和10样本设置下，平均F1分数分别提高了7.3%和5.6%。", "conclusion": "研究结果强调了通过RAG实现上下文自适应提示在生物医学命名实体识别中的实用性。"}}
{"id": "2508.06511", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06511", "abs": "https://arxiv.org/abs/2508.06511", "authors": ["He Feng", "Yongjia Ma", "Donglin Di", "Lei Fan", "Tonghua Su", "Xiangqian Wu"], "title": "DiTalker: A Unified DiT-based Framework for High-Quality and Speaking Styles Controllable Portrait Animation", "comment": null, "summary": "Portrait animation aims to synthesize talking videos from a static reference\nface, conditioned on audio and style frame cues (e.g., emotion and head poses),\nwhile ensuring precise lip synchronization and faithful reproduction of\nspeaking styles. Existing diffusion-based portrait animation methods primarily\nfocus on lip synchronization or static emotion transformation, often\noverlooking dynamic styles such as head movements. Moreover, most of these\nmethods rely on a dual U-Net architecture, which preserves identity consistency\nbut incurs additional computational overhead. To this end, we propose DiTalker,\na unified DiT-based framework for speaking style-controllable portrait\nanimation. We design a Style-Emotion Encoding Module that employs two separate\nbranches: a style branch extracting identity-specific style information (e.g.,\nhead poses and movements), and an emotion branch extracting identity-agnostic\nemotion features. We further introduce an Audio-Style Fusion Module that\ndecouples audio and speaking styles via two parallel cross-attention layers,\nusing these features to guide the animation process. To enhance the quality of\nresults, we adopt and modify two optimization constraints: one to improve lip\nsynchronization and the other to preserve fine-grained identity and background\ndetails. Extensive experiments demonstrate the superiority of DiTalker in terms\nof lip synchronization and speaking style controllability. Project Page:\nhttps://thenameishope.github.io/DiTalker/", "AI": {"tldr": "DiTalker是一种基于DiT的统一框架，通过音频和风格帧（表情、头部姿态和动作）生成说话人像动画，实现精确的唇同步和可控的说话风格。", "motivation": "现有扩散模型人像动画方法主要关注唇同步或静态表情转换，忽略了动态风格（如头部运动），且常用的双U-Net架构计算开销大。", "method": "DiTalker提出一个统一的DiT框架，包含：1) 风格-情感编码模块，通过两个分支分别提取身份特定风格信息（如头部姿态和运动）和身份无关情感特征；2) 音频-风格融合模块，通过两个并行交叉注意力层解耦音频和说话风格；3) 优化约束，用于提升唇同步和保留精细身份与背景细节。", "result": "大量实验证明DiTalker在唇同步和说话风格可控性方面表现优越。", "conclusion": "DiTalker成功解决了现有方法在动态风格控制和计算效率方面的不足，提供了一个高性能的说话风格可控人像动画解决方案。"}}
{"id": "2508.06898", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.06898", "abs": "https://arxiv.org/abs/2508.06898", "authors": ["Zhiyuan Ren", "Zhiliang Shuai", "Wenchi Cheng", "Kun Yang"], "title": "Decoupling Structural Heterogeneity from Functional Fairness in Complex Networks: A Theoretical Framework based on the Imbalance Metric", "comment": null, "summary": "Performance evaluation of complex networks has traditionally focused on\nstructural integrity or average transmission efficiency, perspectives that\noften overlook the dimension of functional fairness. This raises a central\nquestion: Under certain conditions, structurally heterogeneous networks can\nexhibit high functional fairness. To systematically address this issue, we\nintroduce a new metric, Network Imbalance (I), designed to quantitatively\nassess end-to-end accessibility fairness from a perceived QoS perspective. By\ncombining a tunable sigmoid function with a global Shannon entropy framework,\nthe I metric quantifies the uniformity of connection experiences between all\nnode pairs. We analyze the mathematical properties of this metric and validate\nits explanatory power on various classical network models. Our findings reveal\nthat low imbalance (i.e., high functional fairness) can be achieved through two\ndistinct mechanisms: one via topological symmetry (e.g., in a complete graph)\nand the other via extreme connection efficiency driven by structural inequality\n(e.g., in a scale-free network). This decoupling of structure and function\nprovides a new theoretical perspective for network performance evaluation and\noffers an effective quantitative tool for balancing efficiency and fairness in\nnetwork design.", "AI": {"tldr": "论文提出了一种新的度量标准“网络不平衡度（I）”来量化复杂网络中的功能公平性，并发现高公平性可通过拓扑对称性或结构不平等带来的极端连接效率实现。", "motivation": "传统网络性能评估主要关注结构完整性或平均传输效率，忽视了功能公平性维度。核心问题是：在特定条件下，结构异构网络也能表现出高功能公平性。", "method": "引入“网络不平衡度（I）”新度量，结合可调S型函数和全局香农熵框架，从感知QoS角度量化所有节点对之间连接体验的均匀性。通过分析其数学特性并在经典网络模型上验证其解释力。", "result": "研究发现，低不平衡度（即高功能公平性）可以通过两种不同机制实现：一是拓扑对称性（如完全图），二是结构不平等驱动的极端连接效率（如无标度网络）。这揭示了结构与功能的解耦。", "conclusion": "该研究为网络性能评估提供了新的理论视角，并提供了一个有效的量化工具，以在网络设计中平衡效率与公平性。"}}
{"id": "2508.07817", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.07817", "abs": "https://arxiv.org/abs/2508.07817", "authors": ["Tao Tang", "Chengxu Yang"], "title": "MIND: A Noise-Adaptive Denoising Framework for Medical Images Integrating Multi-Scale Transformer", "comment": "6 pages, 6 figures", "summary": "The core role of medical images in disease diagnosis makes their quality\ndirectly affect the accuracy of clinical judgment. However, due to factors such\nas low-dose scanning, equipment limitations and imaging artifacts, medical\nimages are often accompanied by non-uniform noise interference, which seriously\naffects structure recognition and lesion detection. This paper proposes a\nmedical image adaptive denoising model (MI-ND) that integrates multi-scale\nconvolutional and Transformer architecture, introduces a noise level estimator\n(NLE) and a noise adaptive attention module (NAAB), and realizes\nchannel-spatial attention regulation and cross-modal feature fusion driven by\nnoise perception. Systematic testing is carried out on multimodal public\ndatasets. Experiments show that this method significantly outperforms the\ncomparative methods in image quality indicators such as PSNR, SSIM, and LPIPS,\nand improves the F1 score and ROC-AUC in downstream diagnostic tasks, showing\nstrong prac-tical value and promotional potential. The model has outstanding\nbenefits in structural recovery, diagnostic sensitivity, and cross-modal\nrobustness, and provides an effective solution for medical image enhancement\nand AI-assisted diagnosis and treatment.", "AI": {"tldr": "本文提出了一种名为MI-ND的医学图像自适应去噪模型，结合多尺度卷积和Transformer架构，通过引入噪声水平估计器和噪声自适应注意力模块，实现噪声感知驱动的通道-空间注意力调节和跨模态特征融合，显著提升了图像质量和下游诊断任务的性能。", "motivation": "医学图像质量直接影响临床诊断的准确性，但由于低剂量扫描、设备限制和成像伪影等因素，医学图像常伴有非均匀噪声干扰，严重影响结构识别和病灶检测。", "method": "提出了一种名为MI-ND的医学图像自适应去噪模型，该模型整合了多尺度卷积和Transformer架构。它引入了噪声水平估计器（NLE）和噪声自适应注意力模块（NAAB），通过噪声感知实现通道-空间注意力调节和跨模态特征融合。在多模态公共数据集上进行了系统测试。", "result": "实验结果表明，MI-ND方法在PSNR、SSIM和LPIPS等图像质量指标上显著优于现有对比方法。同时，在下游诊断任务中，该模型提升了F1分数和ROC-AUC，显示出强大的实用价值。", "conclusion": "MI-ND模型在结构恢复、诊断敏感性和跨模态鲁棒性方面表现出色，为医学图像增强和AI辅助诊断与治疗提供了一个有效的解决方案，具有较强的推广潜力。"}}
{"id": "2508.06571", "categories": ["cs.AI", "cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06571", "abs": "https://arxiv.org/abs/2508.06571", "authors": ["Anqing Jiang", "Yu Gao", "Yiru Wang", "Zhigang Sun", "Shuo Wang", "Yuwen Heng", "Hao Sun", "Shichen Tang", "Lijuan Zhu", "Jinhao Chai", "Jijun Wang", "Zichong Gu", "Hao Jiang", "Li Sun"], "title": "IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model", "comment": "9 pagres, 2 figures", "summary": "Vision-Language-Action (VLA) models have demonstrated potential in autonomous\ndriving. However, two critical challenges hinder their development: (1)\nExisting VLA architectures are typically based on imitation learning in\nopen-loop setup which tends to capture the recorded behaviors in the dataset,\nleading to suboptimal and constrained performance, (2) Close-loop training\nrelies heavily on high-fidelity sensor simulation, where domain gaps and\ncomputational inefficiencies pose significant barriers. In this paper, we\nintroduce IRL-VLA, a novel close-loop Reinforcement Learning via\n\\textbf{I}nverse \\textbf{R}einforcement \\textbf{L}earning reward world model\nwith a self-built VLA approach. Our framework proceeds in a three-stage\nparadigm: In the first stage, we propose a VLA architecture and pretrain the\nVLA policy via imitation learning. In the second stage, we construct a\nlightweight reward world model via inverse reinforcement learning to enable\nefficient close-loop reward computation. To further enhance planning\nperformance, finally, we design specialized reward world model guidence\nreinforcement learning via PPO(Proximal Policy Optimization) to effectively\nbalance the safety incidents, comfortable driving, and traffic efficiency. Our\napproach achieves state-of-the-art performance in NAVSIM v2 end-to-end driving\nbenchmark, 1st runner up in CVPR2025 Autonomous Grand Challenge. We hope that\nour framework will accelerate VLA research in close-loop autonomous driving.", "AI": {"tldr": "本文提出IRL-VLA，一种通过逆强化学习（IRL）奖励世界模型实现闭环视觉-语言-动作（VLA）模型训练的新框架，解决了现有方法在自动驾驶中的局限性。", "motivation": "现有VLA模型面临两大挑战：1) 基于模仿学习的开环架构性能受限，倾向于复制数据集行为；2) 闭环训练高度依赖高保真传感器模拟，存在领域差距和计算效率低下的问题。", "method": "IRL-VLA框架分为三阶段：1) 构建VLA架构并通过模仿学习预训练VLA策略；2) 通过逆强化学习构建轻量级奖励世界模型，实现高效的闭环奖励计算；3) 设计专门的奖励世界模型引导的强化学习（使用PPO），以平衡驾驶安全、舒适性和交通效率。", "result": "该方法在NAVSIM v2端到端驾驶基准测试中取得了最先进的性能，并在CVPR2025自动驾驶大挑战中获得亚军。", "conclusion": "作者希望该框架能加速闭环自动驾驶领域的VLA研究。"}}
{"id": "2508.06521", "categories": ["cs.RO", "68T40, 93C85, 70E60"], "pdf": "https://arxiv.org/pdf/2508.06521", "abs": "https://arxiv.org/abs/2508.06521", "authors": ["H. Liu", "L. S. Moreu", "T. S. Andersen", "V. V. Puche", "M. Fumagalli"], "title": "Stinger Robot: A Self-Bracing Robotic Platform for Autonomous Drilling in Confined Underground Environments", "comment": "7 pages, submitted", "summary": "The increasing demand for critical raw materials has revitalized interest in\nabandoned underground mines, which pose extreme challenges for conventional\ndrilling machinery due to confined, unstructured, and infrastructure-less\nenvironments. This paper presents the Stinger Robot, a novel compact robotic\nplatform specifically designed for autonomous high-force drilling in such\nsettings. The robot features a mechanically self-locking tri-leg bracing\nmechanism that enables stable anchoring to irregular tunnel surfaces. A key\ninnovation lies in its force-aware, closed-loop control strategy, which enables\nforce interaction with unstructured environments during bracing and drilling.\nImplemented as a finite-state machine in ROS 2, the control policy dynamically\nadapts leg deployment based on real-time contact feedback and load thresholds,\nensuring stability without external supports. We demonstrate, through\nsimulation and preliminary hardware tests, that the Stinger Robot can\nautonomously stabilize and drill in conditions previously inaccessible to\nnowadays mining machines. This work constitutes the first validated robotic\narchitecture to integrate distributed force-bracing and autonomous drilling in\nunderground environments, laying the groundwork for future collaborative mining\noperations using modular robot systems.", "AI": {"tldr": "本文提出了一款名为Stinger Robot的紧凑型机器人，专为废弃地下矿井的自主高力钻探设计，其特点是机械自锁三足支撑机构和力感知闭环控制策略，能在非结构化环境中稳定作业。", "motivation": "由于对关键原材料的需求增加，废弃地下矿井重新受到关注。然而，这些矿井环境狭窄、非结构化且缺乏基础设施，对传统钻探机械构成严峻挑战。", "method": "Stinger Robot采用机械自锁三足支撑机构，可稳定锚定于不规则隧道表面。其核心创新在于力感知闭环控制策略，通过ROS 2中的有限状态机实现，根据实时接触反馈和载荷阈值动态调整腿部部署，确保在无外部支撑下的稳定性。", "result": "通过仿真和初步硬件测试，Stinger Robot展示了在当前采矿机械无法进入的条件下进行自主稳定和钻探的能力。这是第一个将分布式力支撑和自主钻探集成到地下环境中的经过验证的机器人架构。", "conclusion": "该工作为未来使用模块化机器人系统进行协作采矿作业奠定了基础。"}}
{"id": "2508.06524", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06524", "abs": "https://arxiv.org/abs/2508.06524", "authors": ["Lei Jiang", "Fan Chen"], "title": "CarbonScaling: Extending Neural Scaling Laws for Carbon Footprint in Large Language Models", "comment": "8 pages", "summary": "Neural scaling laws have driven the development of increasingly large\nlanguage models (LLMs) by linking accuracy improvements to growth in parameter\ncount, dataset size, and compute. However, these laws overlook the carbon\nemissions that scale exponentially with LLM size. This paper presents\n\\textit{CarbonScaling}, an analytical framework that extends neural scaling\nlaws to incorporate both operational and embodied carbon in LLM training. By\nintegrating models for neural scaling, GPU hardware evolution, parallelism\noptimization, and carbon estimation, \\textit{CarbonScaling} quantitatively\nconnects model accuracy to carbon footprint. Results show that while a\npower-law relationship between accuracy and carbon holds, real-world\ninefficiencies significantly increase the scaling factor. Hardware technology\nscaling reduces carbon emissions for small to mid-sized models, but offers\ndiminishing returns for extremely large LLMs due to communication overhead and\nunderutilized GPUs. Training optimizations-especially aggressive critical batch\nsize scaling-help alleviate this inefficiency. \\textit{CarbonScaling} offers\nkey insights for training more sustainable and carbon-efficient LLMs.", "AI": {"tldr": "本文提出CarbonScaling框架，将神经网络缩放定律扩展到包含LLM训练中的操作和隐含碳排放，量化了模型精度与碳足迹的关系，揭示了真实世界效率低下和硬件技术对超大型模型碳减排的边际效应递减问题。", "motivation": "现有的神经网络缩放定律只关注参数、数据集和计算对模型精度的影响，却忽视了随着LLM规模呈指数级增长的碳排放问题。", "method": "CarbonScaling是一个分析框架，它整合了神经网络缩放模型、GPU硬件演进、并行优化和碳排放估算，从而定量地将模型精度与碳足迹（包括操作碳和隐含碳）联系起来。", "result": "研究发现，精度与碳排放之间存在幂律关系，但实际操作中的低效率显著增加了缩放因子。硬件技术进步能有效降低中小规模模型的碳排放，但对于超大型LLM，由于通信开销和GPU利用率不足，其碳减排效益递减。训练优化，特别是激进的关键批次大小缩放，有助于缓解这种低效率。", "conclusion": "CarbonScaling为训练更可持续、更碳高效的LLM提供了关键见解。"}}
{"id": "2508.06515", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06515", "abs": "https://arxiv.org/abs/2508.06515", "authors": ["Minh Duc Chu", "Kshitij Pawar", "Zihao He", "Roxanna Sharifi", "Ross Sonnenblick", "Magdalayna Curry", "Laura D'Adamo", "Lindsay Young", "Stuart B Murray", "Kristina Lerman"], "title": "BigTokDetect: A Clinically-Informed Vision-Language Model Framework for Detecting Pro-Bigorexia Videos on TikTok", "comment": null, "summary": "Social media platforms increasingly struggle to detect harmful content that\npromotes muscle dysmorphic behaviors, particularly pro-bigorexia content that\ndisproportionately affects adolescent males. Unlike traditional eating disorder\ndetection focused on the \"thin ideal,\" pro-bigorexia material masquerades as\nlegitimate fitness content through complex multimodal combinations of visual\ndisplays, coded language, and motivational messaging that evade text-based\ndetection systems. We address this challenge by developing BigTokDetect, a\nclinically-informed detection framework for identifying pro-bigorexia content\non TikTok. We introduce BigTok, the first expert-annotated multimodal dataset\nof over 2,200 TikTok videos labeled by clinical psychologists and psychiatrists\nacross five primary categories spanning body image, nutrition, exercise,\nsupplements, and masculinity. Through a comprehensive evaluation of\nstate-of-the-art vision language models, we achieve 0.829% accuracy on primary\ncategory classification and 0.690% on subcategory detection via domain-specific\nfinetuning. Our ablation studies demonstrate that multimodal fusion improves\nperformance by 5-10% over text-only approaches, with video features providing\nthe most discriminative signals. These findings establish new benchmarks for\nmultimodal harmful content detection and provide both the computational tools\nand methodological framework needed for scalable content moderation in\nspecialized mental health domains.", "AI": {"tldr": "该研究开发了BigTokDetect框架和BigTok数据集，用于检测TikTok上促进肌肉变形症（大肌症）的有害内容，通过多模态融合显著提升了检测精度。", "motivation": "社交媒体平台难以检测伪装成合法健身内容的有害“大肌症”内容，尤其影响青少年男性。传统文本检测系统因其复杂的视觉、编码语言和激励信息组合而失效，亟需一种能识别此类多模态有害内容的方法。", "method": "开发了BigTokDetect，一个临床知情的检测框架。构建了首个专家标注的多模态TikTok视频数据集BigTok（超过2200个视频，由临床心理学家和精神病学家标注，涵盖身体形象、营养、运动、补充剂和男性气质五大类别）。通过领域特定微调，评估了最先进的视觉语言模型。进行了消融研究以分析多模态融合的效用。", "result": "在主要类别分类上实现了0.829%的准确率，在子类别检测上实现了0.690%的准确率。消融研究表明，多模态融合比纯文本方法性能提升5-10%，其中视频特征提供了最具区分性的信号。", "conclusion": "研究为多模态有害内容检测建立了新基准，并为专业心理健康领域的可扩展内容审核提供了计算工具和方法框架。"}}
{"id": "2508.06987", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.06987", "abs": "https://arxiv.org/abs/2508.06987", "authors": ["Yiwei Liu", "Ziming Wang", "Xin Wang", "Yiding Ji"], "title": "Fixed-Time Voltage Regulation for Boost Converters via Unit-Safe Saturating Functions", "comment": null, "summary": "This paper explores the voltage regulation challenges in boost converter\nsystems, which are critical components in power electronics due to their\nability to step up voltage levels efficiently. The proposed control algorithm\nensures fixed-time stability, a desirable property that guarantees system\nstability within a fixed time frame regardless of initial conditions. To tackle\nthe common chattering issues in conventional fixed-time control methods, a\nnovel class of function families is introduced. State observers and adaptive\nparameters are utilized to manage the uncertainties associated with unknown\nload resistance. Furthermore, a new disturbance observer is developed using the\nproposed function family, and its advantages and limitations are illustrated\nthrough comparison with existing designs. Finally, both non-real-time and\nreal-time simulations are conducted to validate the effectiveness and\ndeployability of the proposed control algorithm.", "AI": {"tldr": "本文提出了一种针对升压变换器的固定时间控制算法，通过引入新型函数族和观测器，解决了抖振问题和未知负载不确定性，实现了电压的快速稳定调节。", "motivation": "升压变换器在电力电子中至关重要，但其电压调节面临挑战。传统的控制方法可能存在抖振问题，且难以应对未知负载等不确定性，因此需要一种能保证固定时间稳定性和鲁棒性的控制策略。", "method": "提出了一种新的固定时间控制算法，引入了一类新型函数族以减少传统固定时间控制中的抖振现象。利用状态观测器和自适应参数来处理未知负载电阻带来的不确定性。此外，还开发了一种基于所提出函数族的新型扰动观测器。", "result": "所提出的控制算法确保了系统在固定时间内稳定，有效解决了传统固定时间控制的抖振问题。通过状态观测器和自适应参数成功管理了未知负载的不确定性。非实时和实时仿真均验证了所提出控制算法的有效性和可部署性。", "conclusion": "该研究成功开发了一种针对升压变换器的固定时间控制算法，通过引入新型函数族、状态观测器和自适应参数，有效解决了电压调节中的抖振和未知负载问题，实现了系统在固定时间内的快速稳定和鲁棒运行。"}}
{"id": "2508.08114", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08114", "abs": "https://arxiv.org/abs/2508.08114", "authors": ["Bowen Tong", "Hao Chen", "Shaorui Guo", "Dong Liu"], "title": "Learned Regularization for Microwave Tomography", "comment": null, "summary": "Microwave Tomography (MWT) aims to reconstruct the dielectric properties of\ntissues from measured scattered electromagnetic fields. This inverse problem is\nhighly nonlinear and ill-posed, posing significant challenges for conventional\noptimization-based methods, which, despite being grounded in physical models,\noften fail to recover fine structural details. Recent deep learning strategies,\nincluding end-to-end and post-processing networks, have improved reconstruction\nquality but typically require large paired training datasets and may struggle\nto generalize. To overcome these limitations, we propose a physics-informed\nhybrid framework that integrates diffusion models as learned regularization\nwithin a data-consistency-driven variational scheme. Specifically, we introduce\nSingle-Step Diffusion Regularization (SSD-Reg), a novel approach that embeds\ndiffusion priors into the iterative reconstruction process, enabling the\nrecovery of complex anatomical structures without the need for paired data.\nSSD-Reg maintains fidelity to both the governing physics and learned structural\ndistributions, improving accuracy, stability, and robustness. Extensive\nexperiments demonstrate that SSD-Reg, implemented as a Plug-and-Play (PnP)\nmodule, provides a flexible and effective solution for tackling the\nill-posedness inherent in functional image reconstruction.", "AI": {"tldr": "本文提出了一种基于物理信息和扩散模型的混合框架（SSD-Reg），用于微波断层扫描（MWT）的图像重建，旨在解决传统方法和深度学习策略的局限性，实现无需配对数据的高质量重建。", "motivation": "微波断层扫描（MWT）的图像重建是一个高度非线性且不适定的反问题，传统优化方法难以恢复精细结构。近期深度学习方法虽有改进，但通常需要大量配对训练数据且泛化能力有限。", "method": "提出了一种物理信息混合框架，将扩散模型作为学习到的正则化项融入数据一致性驱动的变分方案中。具体而言，引入了单步扩散正则化（SSD-Reg）方法，将扩散先验嵌入迭代重建过程，作为即插即用（PnP）模块，无需配对数据即可恢复复杂解剖结构。", "result": "实验证明，SSD-Reg 在保持对物理模型和学习到的结构分布保真度的同时，提高了重建的准确性、稳定性和鲁棒性。它为解决功能图像重建中固有的不适定性问题提供了一个灵活有效的方法。", "conclusion": "SSD-Reg 是一种新颖且有效的解决方案，通过结合物理模型和扩散先验，克服了传统MWT和现有深度学习方法的局限性，实现了无需配对数据的高质量图像重建，尤其适用于处理不适定问题。"}}
{"id": "2508.06585", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06585", "abs": "https://arxiv.org/abs/2508.06585", "authors": ["Jayant Sravan Tamarapalli", "Rynaa Grover", "Nilay Pande", "Sahiti Yerramilli"], "title": "CountQA: How Well Do MLLMs Count in the Wild?", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) demonstrate remarkable fluency in\nunderstanding visual scenes, yet they exhibit a critical lack in a fundamental\ncognitive skill: object counting. This blind spot severely limits their\nreliability in real-world applications. To date, this capability has been\nlargely unevaluated in complex scenarios, as existing benchmarks either feature\nsparse object densities or are confined to specific visual domains, failing to\ntest models under realistic conditions. Addressing this gap, we introduce\nCountQA, a challenging new benchmark designed to probe this deficiency.\nComprising over 1,500 question-answer pairs, CountQA features real-world images\nwith high object density, clutter, and occlusion. We investigate this weakness\nby evaluating 15 prominent MLLMs on the CountQA benchmark and reveal that the\ntop-performing model achieves a mere 42.9% accuracy, with performance declining\nas object counts rise. By providing a dedicated benchmark to diagnose and\nrectify this core weakness, CountQA paves the way for a new generation of MLLMs\nthat are not only descriptively fluent but also numerically grounded and\nspatially aware. We will open-source the dataset and code upon paper acceptance\nto foster further research.", "AI": {"tldr": "多模态大语言模型（MLLMs）在复杂场景下的物体计数能力极差，新的基准测试CountQA揭示了这一核心缺陷，表明现有模型表现不佳。", "motivation": "尽管MLLMs在视觉理解上表现出色，但它们在物体计数这一基本认知技能上存在严重缺陷，这限制了其在实际应用中的可靠性。现有基准测试在复杂场景下无法充分评估这一能力。", "method": "引入了名为CountQA的新型挑战性基准测试，包含超过1500个问答对，使用高密度、杂乱和遮挡的真实世界图像。通过在CountQA上评估15个主流MLLM来探究其弱点。", "result": "评估结果显示，表现最好的模型在CountQA上的准确率仅为42.9%，且随着物体数量的增加，性能显著下降。", "conclusion": "CountQA提供了一个诊断和纠正MLLMs核心弱点的专用基准，为开发不仅描述流畅而且具备数字基础和空间感知能力的新一代MLLMs铺平了道路。数据集和代码将开源以促进后续研究。"}}
{"id": "2508.06534", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06534", "abs": "https://arxiv.org/abs/2508.06534", "authors": ["Aishan Liu", "Jiakai Wang", "Tianyuan Zhang", "Hainan Li", "Jiangfan Liu", "Siyuan Liang", "Yilong Ren", "Xianglong Liu", "Dacheng Tao"], "title": "MetAdv: A Unified and Interactive Adversarial Testing Platform for Autonomous Driving", "comment": "Accepted by ACM MM 2025 Demo/Videos track", "summary": "Evaluating and ensuring the adversarial robustness of autonomous driving (AD)\nsystems is a critical and unresolved challenge. This paper introduces MetAdv, a\nnovel adversarial testing platform that enables realistic, dynamic, and\ninteractive evaluation by tightly integrating virtual simulation with physical\nvehicle feedback. At its core, MetAdv establishes a hybrid virtual-physical\nsandbox, within which we design a three-layer closed-loop testing environment\nwith dynamic adversarial test evolution. This architecture facilitates\nend-to-end adversarial evaluation, ranging from high-level unified adversarial\ngeneration, through mid-level simulation-based interaction, to low-level\nexecution on physical vehicles. Additionally, MetAdv supports a broad spectrum\nof AD tasks, algorithmic paradigms (e.g., modular deep learning pipelines,\nend-to-end learning, vision-language models). It supports flexible 3D vehicle\nmodeling and seamless transitions between simulated and physical environments,\nwith built-in compatibility for commercial platforms such as Apollo and Tesla.\nA key feature of MetAdv is its human-in-the-loop capability: besides flexible\nenvironmental configuration for more customized evaluation, it enables\nreal-time capture of physiological signals and behavioral feedback from\ndrivers, offering new insights into human-machine trust under adversarial\nconditions. We believe MetAdv can offer a scalable and unified framework for\nadversarial assessment, paving the way for safer AD.", "AI": {"tldr": "本文提出了MetAdv，一个新颖的自动驾驶（AD）系统对抗性测试平台。它通过紧密整合虚拟仿真与物理车辆反馈，构建了一个虚实结合的沙盒，实现了真实、动态、交互式的端到端对抗性评估，并支持人在环功能，旨在提升AD系统的安全性。", "motivation": "评估和确保自动驾驶系统在对抗性条件下的鲁棒性是一个关键但尚未解决的挑战。", "method": "MetAdv是一个对抗性测试平台，其核心是一个混合虚实沙盒。它设计了一个三层闭环测试环境，支持动态对抗性测试演进，实现从高层对抗性生成到中层仿真交互再到低层物理车辆执行的端到端评估。平台支持广泛的AD任务和算法范式，具备灵活的3D车辆建模、无缝的虚实环境切换，并兼容商业平台。此外，MetAdv还具备人在环能力，可捕获驾驶员的生理信号和行为反馈。", "result": "MetAdv实现了对自动驾驶系统真实、动态、交互式的对抗性评估，并通过人在环能力为对抗条件下的人机信任提供了新见解。", "conclusion": "MetAdv提供了一个可扩展、统一的对抗性评估框架，有望为实现更安全的自动驾驶铺平道路。"}}
{"id": "2508.06533", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06533", "abs": "https://arxiv.org/abs/2508.06533", "authors": ["Aamod Thakur", "Ajay Nagpal", "Atharva Savarkar", "Kundeshwar Pundalik", "Siddhesh Dosi", "Piyush Sawarkar", "Viraj Thakur", "Rohit Saluja", "Maunendra Sankar Desarkar", "Ganesh Ramakrishnan"], "title": "The Art of Breaking Words: Rethinking Multilingual Tokenizer Design", "comment": null, "summary": "While model architecture and training objectives are well-studied,\ntokenization, particularly in multilingual contexts, remains a relatively\nneglected aspect of Large Language Model (LLM) development. Existing tokenizers\noften exhibit high token-to-word ratios, inefficient use of context length, and\nslower inference. We present a systematic study that links vocabulary size,\npre-tokenization rules, and training-corpus composition to both token-to-word\nefficiency and model quality. To ground our analysis in a linguistically\ndiverse context, we conduct extensive experiments on Indic scripts, which\npresent unique challenges due to their high script diversity and orthographic\ncomplexity. Drawing on the insights from these analyses, we propose a novel\nalgorithm for data composition that balances multilingual data for tokenizer\ntraining. Our observations on pretokenization strategies significantly improve\nmodel performance, and our data composition algorithm reduces the average\ntoken-to-word ratio by approximately 6% with respect to the conventional data\nrandomization approach. Our tokenizer achieves more than 40% improvement on\naverage token-to-word ratio against stateof-the-art multilingual Indic models.\nThis improvement yields measurable gains in both model performance and\ninference speed. This highlights tokenization alongside architecture and\ntraining objectives as a critical lever for building efficient, scalable\nmultilingual LLMs", "AI": {"tldr": "本研究系统性地探讨了多语言LLM中分词的效率和模型质量，特别关注印地语系文字，并提出了一种新的数据组成算法和预分词策略，显著降低了分词比率，提升了模型性能和推理速度。", "motivation": "尽管模型架构和训练目标已得到充分研究，但分词（尤其是在多语言环境中）仍是LLM开发中相对被忽视的方面。现有分词器通常存在高词元-词比率、上下文长度利用效率低下和推理速度慢的问题。", "method": "进行了一项系统性研究，将词汇量大小、预分词规则和训练语料组成与词元-词效率和模型质量联系起来。在印地语系文字上进行了广泛实验。提出了一种平衡多语言数据以训练分词器的新颖数据组成算法，并观察了预分词策略。", "result": "预分词策略显著提高了模型性能。提出的数据组成算法将平均词元-词比率相对于传统数据随机化方法降低了约6%。与最先进的多语言印地语系模型相比，本分词器在平均词元-词比率上实现了超过40%的改进。这些改进带来了模型性能和推理速度的可衡量提升。", "conclusion": "分词与架构和训练目标一样，是构建高效、可扩展的多语言LLM的关键杠杆。"}}
{"id": "2508.06517", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06517", "abs": "https://arxiv.org/abs/2508.06517", "authors": ["Haoran Xi", "Chen Liu", "Xiaolin Li"], "title": "Frequency Prior Guided Matching: A Data Augmentation Approach for Generalizable Semi-Supervised Polyp Segmentation", "comment": "19 pages, 8 figures, 6 tables", "summary": "Automated polyp segmentation is essential for early diagnosis of colorectal\ncancer, yet developing robust models remains challenging due to limited\nannotated data and significant performance degradation under domain shift.\nAlthough semi-supervised learning (SSL) reduces annotation requirements,\nexisting methods rely on generic augmentations that ignore polyp-specific\nstructural properties, resulting in poor generalization to new imaging centers\nand devices. To address this, we introduce Frequency Prior Guided Matching\n(FPGM), a novel augmentation framework built on a key discovery: polyp edges\nexhibit a remarkably consistent frequency signature across diverse datasets.\nFPGM leverages this intrinsic regularity in a two-stage process. It first\nlearns a domain-invariant frequency prior from the edge regions of labeled\npolyps. Then, it performs principled spectral perturbations on unlabeled\nimages, aligning their amplitude spectra with this learned prior while\npreserving phase information to maintain structural integrity. This targeted\nalignment normalizes domain-specific textural variations, thereby compelling\nthe model to learn the underlying, generalizable anatomical structure.\nValidated on six public datasets, FPGM establishes a new state-of-the-art\nagainst ten competing methods. It demonstrates exceptional zero-shot\ngeneralization capabilities, achieving over 10% absolute gain in Dice score in\ndata-scarce scenarios. By significantly enhancing cross-domain robustness, FPGM\npresents a powerful solution for clinically deployable polyp segmentation under\nlimited supervision.", "AI": {"tldr": "该论文提出FPGM框架，通过利用息肉边缘的频率一致性进行数据增强，解决了结肠息肉分割中数据稀缺和域漂移导致的泛化能力差的问题，实现了SOTA的跨域鲁棒性。", "motivation": "结肠息肉的自动化分割对于早期诊断结直肠癌至关重要，但由于标注数据有限和域漂移，开发鲁棒模型极具挑战。现有半监督学习方法依赖通用数据增强，忽略了息肉特有的结构属性，导致在新成像中心和设备上的泛化能力差。", "method": "该研究基于一个关键发现：息肉边缘在不同数据集中表现出显著一致的频率特征。为此，提出了频率先验引导匹配（FPGM）的创新增强框架。FPGM分两阶段：首先，从有标签息肉的边缘区域学习域不变的频率先验；然后，对无标签图像进行频谱扰动，使它们的幅度谱与学习到的先验对齐，同时保留相位信息以维持结构完整性。这种有针对性的对齐旨在标准化域特定的纹理变化，从而迫使模型学习底层的、可泛化的解剖结构。", "result": "FPGM在六个公共数据集上进行了验证，超越了十种现有竞争方法，建立了新的技术水平。它展示了卓越的零样本泛化能力，在数据稀缺场景下，Dice分数绝对增益超过10%。", "conclusion": "FPGM显著增强了跨域鲁棒性，为在有限监督下临床可部署的息肉分割提供了一个强大的解决方案。"}}
{"id": "2508.06994", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.06994", "abs": "https://arxiv.org/abs/2508.06994", "authors": ["Yimeng Sun", "Zhaohao Ding", "Payman Dehghanian", "Fei Teng"], "title": "Learning-Enabled Adaptive Power Capping Scheme for Cloud Data Centers", "comment": null, "summary": "The rapid growth of the digital economy and artificial intelligence has\ntransformed cloud data centers into essential infrastructure with substantial\nenergy consumption and carbon emission, necessitating effective energy\nmanagement. However, existing methods face challenges such as incomplete\ninformation, uncertain parameters, and dynamic environments, which hinder their\nreal-world implementation. This paper proposes an adaptive power capping\nframework tailored to cloud data centers. By dynamically setting the energy\nconsumption upper bound, the power load of data centers can be reshaped to\nalign with the electricity price or other market signals. To this end, we\nformulate the power capping problem as a partially observable Markov decision\nprocess. Subsequently, we develop an uncertainty-aware model-based\nreinforcement learning (MBRL) method to perceive the cloud data center\noperational environment and optimize power-capping decisions. By incorporating\na two-stage uncertainty-aware optimization algorithm into the MBRL, we improve\nits adaptability to the ever-changing environment. Additionally, we derive the\noptimality gap of the proposed scheme under finite iterations, ensuring\neffective decisions under complex and uncertain scenarios. The numerical\nexperiments validate the effectiveness of the proposed method using a cloud\ndata center operational environment simulator built on real-world production\ntraces from Alibaba, which demonstrates its potential as an efficient energy\nmanagement solution for cloud data centers.", "AI": {"tldr": "本文提出了一种自适应功耗限制框架，通过基于不确定性感知的模型强化学习（MBRL）方法，将云数据中心的功耗限制问题建模为部分可观测马尔可夫决策过程（POMDP），以有效管理数据中心的能耗和碳排放。", "motivation": "随着数字经济和人工智能的快速发展，云数据中心能耗巨大且碳排放高，迫切需要有效的能源管理。现有方法因信息不完整、参数不确定和环境动态性等问题，难以在实际中应用。", "method": "1. 提出自适应功耗限制框架，动态设置能耗上限以调整数据中心负载。2. 将功耗限制问题建模为部分可观测马尔可夫决策过程（POMDP）。3. 开发不确定性感知模型强化学习（MBRL）方法优化功耗限制决策。4. 将两阶段不确定性感知优化算法融入MBRL以增强环境适应性。5. 推导出有限迭代下的最优性差距，确保复杂不确定场景下的决策有效性。", "result": "通过基于阿里巴巴真实生产轨迹构建的云数据中心运行环境模拟器进行数值实验，验证了所提出方法的有效性。", "conclusion": "所提出的方法为云数据中心提供了一种高效的能源管理解决方案，具有实际应用潜力。"}}
{"id": "2508.08232", "categories": ["eess.IV", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.08232", "abs": "https://arxiv.org/abs/2508.08232", "authors": ["Buddhi Wijenayake", "Athulya Ratnayake", "Praveen Sumanasekara", "Roshan Godaliyadda", "Parakrama Ekanayake", "Vijitha Herath", "Nichula Wasalathilaka"], "title": "Mamba-FCS: Joint Spatio- Frequency Feature Fusion, Change-Guided Attention, and SeK Loss for Enhanced Semantic Change Detection in Remote Sensing", "comment": "15 pages, 11 Figures", "summary": "Semantic Change Detection (SCD) from remote sensing imagery requires models\nbalancing extensive spatial context, computational efficiency, and sensitivity\nto class-imbalanced land-cover transitions. While Convolutional Neural Networks\nexcel at local feature extraction but lack global context, Transformers provide\nglobal modeling at high computational costs. Recent Mamba architectures based\non state-space models offer compelling solutions through linear complexity and\nefficient long-range modeling. In this study, we introduce Mamba-FCS, a SCD\nframework built upon Visual State Space Model backbone incorporating, a Joint\nSpatio-Frequency Fusion block incorporating log-amplitude frequency domain\nfeatures to enhance edge clarity and suppress illumination artifacts, a\nChange-Guided Attention (CGA) module that explicitly links the naturally\nintertwined BCD and SCD tasks, and a Separated Kappa (SeK) loss tailored for\nclass-imbalanced performance optimization. Extensive evaluation on SECOND and\nLandsat-SCD datasets shows that Mamba-FCS achieves state-of-the-art metrics,\n88.62% Overall Accuracy, 65.78% F_scd, and 25.50% SeK on SECOND, 96.25% Overall\nAccuracy, 89.27% F_scd, and 60.26% SeK on Landsat-SCD. Ablation analyses\nconfirm distinct contributions of each novel component, with qualitative\nassessments highlighting significant improvements in SCD. Our results underline\nthe substantial potential of Mamba architectures, enhanced by proposed\ntechniques, setting a new benchmark for effective and scalable semantic change\ndetection in remote sensing applications. The complete source code,\nconfiguration files, and pre-trained models will be publicly available upon\npublication.", "AI": {"tldr": "提出Mamba-FCS，一个基于Mamba架构的遥感图像语义变化检测（SCD）框架，通过融合时空频率特征、变化引导注意力模块和分离Kappa损失，在处理类别不平衡问题上表现出色，并取得了最先进的性能。", "motivation": "遥感图像语义变化检测（SCD）需要模型平衡广泛的空间上下文、计算效率和对类别不平衡土地覆盖转换的敏感性。传统卷积神经网络（CNN）缺乏全局上下文，而Transformer计算成本高昂。Mamba架构基于状态空间模型，具有线性复杂度和高效长程建模能力，有望解决这些现有方法的局限性。", "method": "本研究引入了Mamba-FCS框架，其核心是视觉状态空间模型（Visual State Space Model）骨干。该框架整合了：1) 一个联合时空频率融合（Joint Spatio-Frequency Fusion）模块，利用对数幅度频域特征增强边缘清晰度并抑制光照伪影；2) 一个变化引导注意力（Change-Guided Attention, CGA）模块，显式关联双边变化检测（BCD）和语义变化检测（SCD）任务；3) 一个分离Kappa（Separated Kappa, SeK）损失，专门针对类别不平衡性能优化而设计。", "result": "在SECOND和Landsat-SCD数据集上的广泛评估表明，Mamba-FCS取得了最先进的性能指标：在SECOND数据集上，总准确率达到88.62%，F_scd为65.78%，SeK为25.50%；在Landsat-SCD数据集上，总准确率达到96.25%，F_scd为89.27%，SeK为60.26%。消融分析证实了每个新组件的独特贡献，定性评估也突出了SCD的显著改进。", "conclusion": "研究结果强调了Mamba架构在遥感应用中语义变化检测方面的巨大潜力。通过结合所提出的联合时空频率融合、变化引导注意力模块和分离Kappa损失等技术，Mamba-FCS为有效和可扩展的语义变化检测设定了新的基准。"}}
{"id": "2508.06668", "categories": ["cs.AI", "cs.IR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.06668", "abs": "https://arxiv.org/abs/2508.06668", "authors": ["Jessie Galasso"], "title": "Formal Concept Analysis: a Structural Framework for Variability Extraction and Analysis", "comment": null, "summary": "Formal Concept Analysis (FCA) is a mathematical framework for knowledge\nrepresentation and discovery. It performs a hierarchical clustering over a set\nof objects described by attributes, resulting in conceptual structures in which\nobjects are organized depending on the attributes they share. These conceptual\nstructures naturally highlight commonalities and variabilities among similar\nobjects by categorizing them into groups which are then arranged by similarity,\nmaking it particularly appropriate for variability extraction and analysis.\nDespite the potential of FCA, determining which of its properties can be\nleveraged for variability-related tasks (and how) is not always\nstraightforward, partly due to the mathematical orientation of its foundational\nliterature. This paper attempts to bridge part of this gap by gathering a\nselection of properties of the framework which are essential to variability\nanalysis, and how they can be used to interpret diverse variability information\nwithin the resulting conceptual structures.", "AI": {"tldr": "本文旨在阐明形式概念分析（FCA）的关键属性如何应用于变异性分析，以弥合其数学基础与实际应用之间的鸿沟。", "motivation": "形式概念分析（FCA）在知识表示和发现方面具有潜力，尤其适用于变异性提取和分析。然而，由于其基础文献的数学导向，如何利用其属性进行变异性相关任务并不总是直观的。", "method": "本文汇集了FCA框架中对变异性分析至关重要的选定属性，并解释了如何利用这些属性来解释所得概念结构中的多样化变异性信息。", "result": "通过阐明FCA属性及其在概念结构中的解释方法，本文有助于利用FCA提取和分析变异性，使研究人员能够更好地理解和利用共享属性组织的对象群组中的共性和变异性。", "conclusion": "FCA的特定属性对于变异性分析至关重要，并且能够有效地解释概念结构中的变异性信息。本文为理解和应用这些属性提供了指导。"}}
{"id": "2508.06538", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.06538", "abs": "https://arxiv.org/abs/2508.06538", "authors": ["Gioele Buriani", "Jingyue Liu", "Maximilian Stölzle", "Cosimo Della Santina", "Jiatao Ding"], "title": "Symbolic Learning of Interpretable Reduced-Order Models for Jumping Quadruped Robots", "comment": "8 pages, under review", "summary": "Reduced-order models are essential for motion planning and control of\nquadruped robots, as they simplify complex dynamics while preserving critical\nbehaviors. This paper introduces a novel methodology for deriving such\ninterpretable dynamic models, specifically for jumping. We capture the\nhigh-dimensional, nonlinear jumping dynamics in a low-dimensional latent space\nby proposing a learning architecture combining Sparse Identification of\nNonlinear Dynamics (SINDy) with physical structural priors on the jump\ndynamics. Our approach demonstrates superior accuracy to the traditional\nactuated Spring-loaded Inverted Pendulum (aSLIP) model and is validated through\nsimulation and hardware experiments across different jumping strategies.", "AI": {"tldr": "本文提出了一种结合SINDy和物理结构先验的新方法，用于为四足机器人跳跃推导可解释的降阶动力学模型，其精度优于传统aSLIP模型。", "motivation": "四足机器人的运动规划和控制需要降阶模型来简化复杂的动力学，同时保留关键行为。现有模型可能不够精确或难以解释。", "method": "通过结合稀疏非线性动力学识别（SINDy）和跳跃动力学的物理结构先验，将高维非线性跳跃动力学捕获到低维潜在空间中，从而学习可解释的动态模型。", "result": "该方法在不同跳跃策略的仿真和硬件实验中均得到验证，并展示出优于传统驱动式弹簧倒立摆（aSLIP）模型的卓越精度。", "conclusion": "所提出的方法成功地为四足机器人跳跃推导出了精确且可解释的降阶动力学模型，为运动规划和控制提供了基础。"}}
{"id": "2508.06548", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.06548", "abs": "https://arxiv.org/abs/2508.06548", "authors": ["Zhanye Luo", "Yuefeng Han", "Xiufan Yu"], "title": "Factor Augmented Supervised Learning with Text Embeddings", "comment": null, "summary": "Large language models (LLMs) generate text embeddings from text data,\nproducing vector representations that capture the semantic meaning and\ncontextual relationships of words. However, the high dimensionality of these\nembeddings often impedes efficiency and drives up computational cost in\ndownstream tasks. To address this, we propose AutoEncoder-Augmented Learning\nwith Text (AEALT), a supervised, factor-augmented framework that incorporates\ndimension reduction directly into pre-trained LLM workflows. First, we extract\nembeddings from text documents; next, we pass them through a supervised\naugmented autoencoder to learn low-dimensional, task-relevant latent factors.\nBy modeling the nonlinear structure of complex embeddings, AEALT outperforms\nconventional deep-learning approaches that rely on raw embeddings. We validate\nits broad applicability with extensive experiments on classification, anomaly\ndetection, and prediction tasks using multiple real-world public datasets.\nNumerical results demonstrate that AEALT yields substantial gains over both\nvanilla embeddings and several standard dimension reduction methods.", "AI": {"tldr": "AEALT是一个监督式、因子增强的框架，通过将降维直接集成到预训练LLM工作流中，利用增强型自编码器学习低维、任务相关的潜在因子，以提高文本嵌入下游任务的效率和性能。", "motivation": "大型语言模型（LLMs）生成的文本嵌入维度高，导致下游任务的效率低下和计算成本增加。", "method": "AEALT首先从文本中提取LLM嵌入，然后通过一个监督式增强型自编码器学习低维、任务相关的潜在因子。该方法通过建模复杂嵌入的非线性结构，将降维直接整合到LLM工作流中。", "result": "数值结果表明，AEALT在分类、异常检测和预测任务中，相对于原始嵌入和几种标准降维方法，均取得了显著的性能提升。", "conclusion": "AEALT通过有效地降低LLM嵌入的维度并学习任务相关的特征，显著优于依赖原始嵌入的传统深度学习方法，具有广泛的适用性。"}}
{"id": "2508.06525", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06525", "abs": "https://arxiv.org/abs/2508.06525", "authors": ["Guoyuan An", "JaeYoon Kim", "SungEui Yoon"], "title": "Large Language Models Facilitate Vision Reflection in Image Classification", "comment": null, "summary": "This paper presents several novel findings on the explainability of vision\nreflection in large multimodal models (LMMs). First, we show that prompting an\nLMM to verify the prediction of a specialized vision model can improve\nrecognition accuracy, even on benchmarks like ImageNet, despite prior evidence\nthat LMMs typically underperform dedicated vision encoders. Second, we analyze\nthe internal behavior of vision reflection and find that the vision-language\nconnector maps visual features into explicit textual concepts, allowing the\nlanguage model to reason about prediction plausibility using commonsense\nknowledge. We further observe that replacing a large number of vision tokens\nwith only a few text tokens still enables LLaVA to generate similar answers,\nsuggesting that LMMs may rely primarily on a compact set of distilled textual\nrepresentations rather than raw vision features. Third, we show that a\ntraining-free connector can enhance LMM performance in fine-grained recognition\ntasks, without extensive feature-alignment training. Together, these findings\noffer new insights into the explainability of vision-language models and\nsuggest that vision reflection is a promising strategy for achieving robust and\ninterpretable visual recognition.", "AI": {"tldr": "该研究揭示了大型多模态模型（LMMs）中视觉反射的可解释性，发现提示LMM验证视觉模型预测可提高准确性，并指出LMM可能主要依赖紧凑的文本表示而非原始视觉特征，同时提出免训练连接器可增强性能。", "motivation": "尽管LMMs在视觉任务上通常表现不如专用视觉模型，但研究旨在探索LMMs进行视觉反射的能力，以提高视觉识别的准确性、鲁棒性和可解释性。", "method": ["通过提示LMM验证专用视觉模型的预测来评估其对识别准确性的影响。", "分析视觉反射的内部行为，特别是视觉-语言连接器如何将视觉特征映射到文本概念。", "通过用少量文本标记替换大量视觉标记来测试LMM对精炼文本表示的依赖性。", "探索使用免训练连接器来提升LMM在细粒度识别任务中的表现。"], "result": ["提示LMM验证专业视觉模型的预测可以提高识别准确性，即使在ImageNet等基准测试上。", "视觉-语言连接器将视觉特征映射为明确的文本概念，使语言模型能够利用常识知识进行推理。", "LMMs可能主要依赖于一组紧凑的、精炼的文本表示，而非原始视觉特征。", "免训练连接器可以在无需大量特征对齐训练的情况下，增强LMM在细粒度识别任务中的性能。"], "conclusion": "这些发现为视觉-语言模型的可解释性提供了新见解，并表明视觉反射是实现鲁棒和可解释视觉识别的一种有前景的策略。"}}
{"id": "2508.07121", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.07121", "abs": "https://arxiv.org/abs/2508.07121", "authors": ["Alexandros E. Tzikas", "Lukas Fiechtner", "Arec Jamgochian", "Mykel J. Kochenderfer"], "title": "Distributionally Robust Control with Constraints on Linear Unidimensional Projections", "comment": "Presented at the 11th International Conference on Control, Decision\n  and Information Technologies (CoDIT 2025)", "summary": "Distributionally robust control is a well-studied framework for optimal\ndecision making under uncertainty, with the objective of minimizing an expected\ncost function over control actions, assuming the most adverse probability\ndistribution from an ambiguity set. We consider an interpretable and expressive\nclass of ambiguity sets defined by constraints on the expected value of\nfunctions of one-dimensional linear projections of the uncertain parameters.\nPrior work has shown that, under conditions, problems in this class can be\nreformulated as finite convex problems. In this work, we propose two iterative\nmethods that can be used to approximately solve problems of this class in the\ngeneral case. The first is an approximate algorithm based on best-response\ndynamics. The second is an approximate method that first reformulates the\nproblem as a semi-infinite program and then solves a relaxation. We apply our\nmethods to portfolio construction and trajectory planning scenarios.", "AI": {"tldr": "本文提出了两种迭代方法，用于近似解决在不确定性下，基于一维线性投影函数期望值约束定义的模糊集下的分布鲁棒控制问题。", "motivation": "在不确定性下进行最优决策是重要问题，其中需要最小化在最不利概率分布下的预期成本。现有工作虽然能将特定条件下的问题重构为有限凸问题，但对于一般情况缺乏解决方案，因此需要开发通用的近似求解方法。", "method": "研究采用的模糊集通过对不确定参数的一维线性投影函数期望值施加约束来定义。针对这类问题，提出了两种迭代近似方法：1) 基于最佳响应动力学的近似算法；2) 将问题重构为半无限规划并求解其松弛形式的近似方法。", "result": "所提出的两种迭代方法能够近似解决这类分布鲁棒控制问题的一般情况。这些方法已成功应用于投资组合构建和轨迹规划场景。", "conclusion": "本文提出的两种迭代近似方法为在特定模糊集定义下的分布鲁棒控制问题提供了一般性的解决方案，并在实际应用中展现了其有效性。"}}
{"id": "2508.06546", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.06546", "abs": "https://arxiv.org/abs/2508.06546", "authors": ["Qi Xun Yeo", "Yanyan Li", "Gim Hee Lee"], "title": "Statistical Confidence Rescoring for Robust 3D Scene Graph Generation from Multi-View Images", "comment": "This paper has been accepted in ICCV 25", "summary": "Modern 3D semantic scene graph estimation methods utilize ground truth 3D\nannotations to accurately predict target objects, predicates, and\nrelationships. In the absence of given 3D ground truth representations, we\nexplore leveraging only multi-view RGB images to tackle this task. To attain\nrobust features for accurate scene graph estimation, we must overcome the noisy\nreconstructed pseudo point-based geometry from predicted depth maps and reduce\nthe amount of background noise present in multi-view image features. The key is\nto enrich node and edge features with accurate semantic and spatial information\nand through neighboring relations. We obtain semantic masks to guide feature\naggregation to filter background features and design a novel method to\nincorporate neighboring node information to aid robustness of our scene graph\nestimates. Furthermore, we leverage on explicit statistical priors calculated\nfrom the training summary statistics to refine node and edge predictions based\non their one-hop neighborhood. Our experiments show that our method outperforms\ncurrent methods purely using multi-view images as the initial input. Our\nproject page is available at https://qixun1.github.io/projects/SCRSSG.", "AI": {"tldr": "该论文提出了一种仅使用多视角RGB图像进行3D语义场景图估计的方法，通过语义掩码过滤背景噪声、结合邻居节点信息以及利用统计先验来增强特征鲁棒性，从而在没有3D真值的情况下实现准确预测。", "motivation": "现代3D语义场景图估计方法依赖于3D真值标注。本研究的动机是探索在缺乏3D真值的情况下，仅利用多视角RGB图像来完成此任务，并克服伪点云几何重建的噪声以及多视角图像特征中背景噪声过多的问题。", "method": "该方法通过以下方式实现：1) 获取语义掩码以指导特征聚合，从而过滤背景特征；2) 设计一种新颖的方法来整合邻居节点信息，以提高场景图估计的鲁棒性；3) 利用从训练统计数据中计算出的显式统计先验，基于单跳邻域来优化节点和边的预测。", "result": "实验结果表明，该方法在仅使用多视角图像作为初始输入的情况下，优于当前同类方法。", "conclusion": "该研究成功展示了仅利用多视角RGB图像进行3D语义场景图估计的可行性，通过有效地处理噪声和增强特征，实现了比现有方法更优的性能。"}}
{"id": "2508.06674", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06674", "abs": "https://arxiv.org/abs/2508.06674", "authors": ["Weijie Shi", "Yue Cui", "Hao Chen", "Jiaming Li", "Mengze Li", "Jia Zhu", "Jiajie Xu", "Xiaofang Zhou"], "title": "Zero-Shot Cellular Trajectory Map Matching", "comment": null, "summary": "Cellular Trajectory Map-Matching (CTMM) aims to align cellular location\nsequences to road networks, which is a necessary preprocessing in\nlocation-based services on web platforms like Google Maps, including navigation\nand route optimization. Current approaches mainly rely on ID-based features and\nregion-specific data to learn correlations between cell towers and roads,\nlimiting their adaptability to unexplored areas. To enable high-accuracy CTMM\nwithout additional training in target regions, Zero-shot CTMM requires to\nextract not only region-adaptive features, but also sequential and location\nuncertainty to alleviate positioning errors in cellular data. In this paper, we\npropose a pixel-based trajectory calibration assistant for zero-shot CTMM,\nwhich takes advantage of transferable geospatial knowledge to calibrate\npixelated trajectory, and then guide the path-finding process at the road\nnetwork level. To enhance knowledge sharing across similar regions, a Gaussian\nmixture model is incorporated into VAE, enabling the identification of\nscenario-adaptive experts through soft clustering. To mitigate high positioning\nerrors, a spatial-temporal awareness module is designed to capture sequential\nfeatures and location uncertainty, thereby facilitating the inference of\napproximate user positions. Finally, a constrained path-finding algorithm is\nemployed to reconstruct the road ID sequence, ensuring topological validity\nwithin the road network. This process is guided by the calibrated trajectory\nwhile optimizing for the shortest feasible path, thus minimizing unnecessary\ndetours. Extensive experiments demonstrate that our model outperforms existing\nmethods in zero-shot CTMM by 16.8\\%.", "AI": {"tldr": "该论文提出了一种零样本蜂窝轨迹地图匹配（CTMM）方法，通过像素级轨迹校准和空间-时间感知模块，结合高斯混合模型和受限寻路算法，实现了在未知区域的高精度轨迹对齐。", "motivation": "现有CTMM方法依赖于基于ID和特定区域的数据，导致其在未探索区域的适应性受限。零样本CTMM需要提取区域自适应特征，并处理序列和位置不确定性，以解决蜂窝数据中的定位误差，从而实现无需额外训练的高精度地图匹配。", "method": "本文提出了一种基于像素的轨迹校准助手，利用可迁移的地理空间知识校准像素化轨迹，并指导道路网络层面的路径查找。为了增强知识共享，将高斯混合模型融入VAE以通过软聚类识别场景自适应专家。为缓解高定位误差，设计了空间-时间感知模块捕获序列特征和位置不确定性。最后，采用受限寻路算法，在校准轨迹的引导下重建道路ID序列，并优化最短可行路径。", "result": "实验结果表明，该模型在零样本CTMM任务中比现有方法性能提升了16.8%。", "conclusion": "该研究通过引入像素级轨迹校准、空间-时间感知以及优化的路径查找策略，显著提高了蜂窝轨迹地图匹配在零样本场景下的准确性和适应性，有效解决了现有方法的局限性。"}}
{"id": "2508.06547", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06547", "abs": "https://arxiv.org/abs/2508.06547", "authors": ["Heran Wu", "Zirun Zhou", "Jingfeng Zhang"], "title": "A tutorial note on collecting simulated data for vision-language-action models", "comment": "This is a tutorial note for educational purposes", "summary": "Traditional robotic systems typically decompose intelligence into independent\nmodules for computer vision, natural language processing, and motion control.\nVision-Language-Action (VLA) models fundamentally transform this approach by\nemploying a single neural network that can simultaneously process visual\nobservations, understand human instructions, and directly output robot actions\n-- all within a unified framework. However, these systems are highly dependent\non high-quality training datasets that can capture the complex relationships\nbetween visual observations, language instructions, and robotic actions. This\ntutorial reviews three representative systems: the PyBullet simulation\nframework for flexible customized data generation, the LIBERO benchmark suite\nfor standardized task definition and evaluation, and the RT-X dataset\ncollection for large-scale multi-robot data acquisition. We demonstrated\ndataset generation approaches in PyBullet simulation and customized data\ncollection within LIBERO, and provide an overview of the characteristics and\nroles of the RT-X dataset for large-scale multi-robot data acquisition.", "AI": {"tldr": "本教程综述了用于VLA（视觉-语言-动作）机器人模型训练所需高质量数据集的生成、基准测试和大规模采集方法，包括PyBullet、LIBERO和RT-X。", "motivation": "传统的机器人系统将智能分解为独立模块，而VLA模型通过单个神经网络统一了视觉、语言理解和机器人动作。然而，VLA系统高度依赖能够捕捉视觉、语言和动作之间复杂关系的高质量训练数据集。", "method": "本文回顾了三个代表性系统：PyBullet仿真框架（用于灵活定制数据生成）、LIBERO基准套件（用于标准化任务定义和评估）和RT-X数据集（用于大规模多机器人数据采集）。文中演示了在PyBullet中生成数据集和在LIBERO中定制数据收集，并概述了RT-X数据集的特点和作用。", "result": "展示了PyBullet仿真中的数据集生成方法，LIBERO中定制数据收集的实践，并对RT-X数据集在大规模多机器人数据采集中的特性和作用进行了概述。", "conclusion": "PyBullet、LIBERO和RT-X这三个系统对于解决VLA模型对高质量训练数据的依赖至关重要，它们分别提供了灵活的数据生成、标准化的基准测试和大规模数据采集的解决方案。"}}
{"id": "2508.06583", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06583", "abs": "https://arxiv.org/abs/2508.06583", "authors": ["Ying Liu", "Can Li", "Ting Zhang", "Mei Wang", "Qiannan Zhu", "Jian Li", "Hua Huang"], "title": "Discerning minds or generic tutors? Evaluating instructional guidance capabilities in Socratic LLMs", "comment": null, "summary": "The conversational capabilities of large language models hold significant\npromise for enabling scalable and interactive tutoring. While prior research\nhas primarily examined their capacity for Socratic questioning, it often\noverlooks a critical dimension: adaptively guiding learners based on their\ncognitive states. This study shifts focus from mere question generation to the\nbroader instructional guidance capability. We ask: Can LLMs emulate expert\ntutors who dynamically adjust strategies in response to learners'\nunderstanding? To investigate this, we propose GuideEval, a benchmark grounded\nin authentic educational dialogues that evaluates pedagogical guidance through\na three-phase behavioral framework: (1) Perception, inferring learner states;\n(2) Orchestration, adapting instructional strategies; and (3) Elicitation,\nstimulating proper reflections. Empirical findings reveal that existing LLMs\nfrequently fail to provide effective adaptive scaffolding when learners exhibit\nconfusion or require redirection. Furthermore, we introduce a behavior-guided\nfinetuning strategy that leverages behavior-prompted instructional dialogues,\nsignificantly enhancing guidance performance. By shifting the focus from\nisolated content evaluation to learner-centered interaction, our work advocates\na more dialogic paradigm for evaluating Socratic LLMs.", "AI": {"tldr": "本研究提出并评估了一种新的基准GuideEval，用于衡量大型语言模型（LLMs）在基于学习者认知状态提供适应性教学指导方面的能力，并提出了一种行为引导的微调策略以提升其表现。", "motivation": "现有研究主要关注LLMs的苏格拉底式提问能力，但往往忽略了根据学习者的认知状态进行适应性指导这一关键维度。本研究旨在探讨LLMs是否能像专家导师一样，动态调整策略以响应学习者的理解水平。", "method": "提出了GuideEval基准，该基准基于真实的教育对话，并通过一个三阶段行为框架（感知学习者状态、编排教学策略、启发适当反思）来评估教学指导能力。此外，引入了一种行为引导的微调策略，利用行为提示的教学对话来提升指导性能。", "result": "实证结果表明，现有LLMs在学习者表现出困惑或需要重定向时，通常无法提供有效的适应性支架。而所提出的行为引导微调策略，能够显著增强LLMs的指导性能。", "conclusion": "本研究将评估重点从孤立的内容评估转向以学习者为中心的互动，倡导一种更具对话性的范式来评估苏格拉底式LLMs，以实现更具适应性的可扩展教学。"}}
{"id": "2508.06528", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06528", "abs": "https://arxiv.org/abs/2508.06528", "authors": ["Xiuliang Zhang", "Tadiwa Elisha Nyamasvisva", "Chuntao Liu"], "title": "A Framework Combining 3D CNN and Transformer for Video-Based Behavior Recognition", "comment": "9 pages,6 figures", "summary": "Video-based behavior recognition is essential in fields such as public\nsafety, intelligent surveillance, and human-computer interaction. Traditional\n3D Convolutional Neural Network (3D CNN) effectively capture local\nspatiotemporal features but struggle with modeling long-range dependencies.\nConversely, Transformers excel at learning global contextual information but\nface challenges with high computational costs. To address these limitations, we\npropose a hybrid framework combining 3D CNN and Transformer architectures. The\n3D CNN module extracts low-level spatiotemporal features, while the Transformer\nmodule captures long-range temporal dependencies, with a fusion mechanism\nintegrating both representations. Evaluated on benchmark datasets, the proposed\nmodel outperforms traditional 3D CNN and standalone Transformers, achieving\nhigher recognition accuracy with manageable complexity. Ablation studies\nfurther validate the complementary strengths of the two modules. This hybrid\nframework offers an effective and scalable solution for video-based behavior\nrecognition.", "AI": {"tldr": "提出一种结合3D CNN和Transformer的混合框架，用于视频行为识别，解决了传统方法在长距离依赖和计算成本上的不足，提高了识别精度。", "motivation": "传统3D CNN擅长局部时空特征但难以建模长距离依赖；Transformer擅长全局上下文但计算成本高。为了克服这两种方法的局限性，需要一种新的方法。", "method": "提出一种混合框架，结合3D CNN和Transformer架构。3D CNN模块用于提取低级时空特征，Transformer模块用于捕获长距离时间依赖，并通过融合机制整合两种表示。", "result": "在基准数据集上，所提出的模型优于传统3D CNN和独立的Transformer，在可控的复杂度下实现了更高的识别精度。消融研究进一步验证了两个模块的互补优势。", "conclusion": "该混合框架为视频行为识别提供了一种有效且可扩展的解决方案。"}}
{"id": "2508.07177", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.07177", "abs": "https://arxiv.org/abs/2508.07177", "authors": ["Minghui Lu", "Brett Ross"], "title": "An Analogy of Frequency Droop Control for Grid-forming Sources", "comment": "Accepted by IEEE PESGM 2025", "summary": "In this paper, we present an analogy for a power system dominated by\ngrid-forming (GFM) sources that proves to be a powerful visualization tool for\nanalyses of power flow, frequency regulation, and power dispatch. Frequency\ndroop characteristics of a typical GFM source are exactly reflected by an\nordinary model of water vessels. The frequency is represented by visible water\nlevels while the droop slope is reified by the vessel sizes. This proposed\nanalogy allows us to use the intuitive water-flow phenomenon to explain the\nabstract power-flow problems. The grid integration of renewables via GFM\ninverters is interestingly simulated by a vessel connected to an infinite water\ntank. This paper also provides a means for demonstrating issues to audiences\nwith little or no background in power systems. Finally, the proposal is\nverified by simulation results.", "AI": {"tldr": "本文提出了一种将电网形成型（GFM）电源系统类比为水容器的工具，用于直观地分析潮流、频率调节和电力调度。", "motivation": "电力系统中的潮流和频率调节问题抽象复杂，难以向非专业人士解释。随着可再生能源并网，GFM逆变器日益重要，需要一种直观的方式来理解其工作原理和相关问题。", "method": "将GFM电源的频率下垂特性映射到水容器模型中：频率由水位表示，下垂斜率由容器大小表示。可再生能源通过GFM逆变器并网则模拟为容器连接到无限大水箱。通过水流现象解释抽象的潮流问题，并用仿真验证了该类比的有效性。", "result": "该水容器类比被证明是一种强大的可视化工具，能够直观地解释潮流、频率调节和电力调度问题。它使得向电力系统背景知识有限的受众演示相关问题成为可能。该提议已通过仿真结果得到验证。", "conclusion": "所提出的水容器类比是一种有效的、直观的工具，可以帮助理解和演示由GFM电源主导的电力系统中的复杂现象，特别适用于教育和普及目的。"}}
{"id": "2508.06951", "categories": ["cs.CV", "eess.IV", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.06951", "abs": "https://arxiv.org/abs/2508.06951", "authors": ["Harry Walsh", "Ed Fish", "Ozge Mercanoglu Sincan", "Mohamed Ilyes Lakhal", "Richard Bowden", "Neil Fox", "Bencie Woll", "Kepeng Wu", "Zecheng Li", "Weichao Zhao", "Haodong Wang", "Wengang Zhou", "Houqiang Li", "Shengeng Tang", "Jiayi He", "Xu Wang", "Ruobei Zhang", "Yaxiong Wang", "Lechao Cheng", "Meryem Tasyurek", "Tugce Kiziltepe", "Hacer Yalim Keles"], "title": "SLRTP2025 Sign Language Production Challenge: Methodology, Results, and Future Work", "comment": "11 pages, 6 Figures, CVPR conference", "summary": "Sign Language Production (SLP) is the task of generating sign language video\nfrom spoken language inputs. The field has seen a range of innovations over the\nlast few years, with the introduction of deep learning-based approaches\nproviding significant improvements in the realism and naturalness of generated\noutputs. However, the lack of standardized evaluation metrics for SLP\napproaches hampers meaningful comparisons across different systems. To address\nthis, we introduce the first Sign Language Production Challenge, held as part\nof the third SLRTP Workshop at CVPR 2025. The competition's aims are to\nevaluate architectures that translate from spoken language sentences to a\nsequence of skeleton poses, known as Text-to-Pose (T2P) translation, over a\nrange of metrics. For our evaluation data, we use the\nRWTH-PHOENIX-Weather-2014T dataset, a German Sign Language - Deutsche\nGebardensprache (DGS) weather broadcast dataset. In addition, we curate a\ncustom hidden test set from a similar domain of discourse. This paper presents\nthe challenge design and the winning methodologies. The challenge attracted 33\nparticipants who submitted 231 solutions, with the top-performing team\nachieving BLEU-1 scores of 31.40 and DTW-MJE of 0.0574. The winning approach\nutilized a retrieval-based framework and a pre-trained language model. As part\nof the workshop, we release a standardized evaluation network, including\nhigh-quality skeleton extraction-based keypoints establishing a consistent\nbaseline for the SLP field, which will enable future researchers to compare\ntheir work against a broader range of methods.", "AI": {"tldr": "首届手语生成挑战赛旨在标准化手语生成（SLP）中的文本到姿态（T2P）翻译任务评估，并发布了标准化评估工具和基线，促进了领域发展。", "motivation": "手语生成（SLP）领域缺乏标准化评估指标，导致不同系统间的性能比较困难，阻碍了研究进展。", "method": "本研究组织了首届手语生成挑战赛，旨在评估将口语句子转换为骨骼姿态序列（T2P）的架构。挑战赛使用RWTH-PHOENIX-Weather-2014T数据集和定制的隐藏测试集，并采用BLEU-1和DTW-MJE等指标进行评估。", "result": "挑战赛吸引了33名参与者，提交了231个解决方案。表现最佳的团队在BLEU-1上达到31.40分，DTW-MJE为0.0574。其获胜方法结合了检索式框架和预训练语言模型。", "conclusion": "挑战赛成功建立了SLP领域的标准化评估基线，并发布了标准化评估网络和高质量骨骼关键点，这将使未来的研究人员能够更有效地比较他们的工作，从而推动该领域的发展。"}}
{"id": "2508.06706", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2508.06706", "abs": "https://arxiv.org/abs/2508.06706", "authors": ["Jaikrishna Manojkumar Patil", "Nathaniel Lee", "Al Mehdi Saadat Chowdhury", "YooJung Choi", "Paulo Shakarian"], "title": "Probabilistic Circuits for Knowledge Graph Completion with Reduced Rule Sets", "comment": null, "summary": "Rule-based methods for knowledge graph completion provide explainable results\nbut often require a significantly large number of rules to achieve competitive\nperformance. This can hinder explainability due to overwhelmingly large rule\nsets. We discover rule contexts (meaningful subsets of rules that work\ntogether) from training data and use learned probability distribution (i.e.\nprobabilistic circuits) over these rule contexts to more rapidly achieve\nperformance of the full rule set. Our approach achieves a 70-96% reduction in\nnumber of rules used while outperforming baseline by up to 31$\\times$ when\nusing equivalent minimal number of rules and preserves 91% of peak baseline\nperformance even when comparing our minimal rule sets against baseline's full\nrule sets. We show that our framework is grounded in well-known semantics of\nprobabilistic logic, does not require independence assumptions, and that our\ntractable inference procedure provides both approximate lower bounds and exact\nprobability of a given query. The efficacy of our method is validated by\nempirical studies on 8 standard benchmark datasets where we show competitive\nperformance by using only a fraction of the rules required by AnyBURL's\nstandard inference method, the current state-of-the-art for rule-based\nknowledge graph completion. This work may have further implications for general\nprobabilistic reasoning over learned sets of rules.", "AI": {"tldr": "该研究通过发现规则上下文并使用概率电路进行推理，大幅减少了知识图谱补全中基于规则方法所需的规则数量，同时保持或提升了性能和可解释性。", "motivation": "基于规则的知识图谱补全方法虽然提供可解释的结果，但通常需要大量规则才能达到竞争力，这反而会因规则集过大而损害可解释性。", "method": "从训练数据中发现有意义的规则子集（即规则上下文），并利用这些规则上下文上的学习概率分布（即概率电路）进行推理，以更快地达到完整规则集的性能。", "result": "方法在规则使用数量上减少了70-96%，在使用等效最少规则时性能比基线高出31倍，即使与基线的完整规则集相比，也能保留91%的峰值性能。该框架基于概率逻辑语义，无需独立性假设，且可推断过程能提供近似下界和精确概率。在8个标准基准数据集上验证了其有效性，仅用AnyBURL标准推理方法所需规则的一小部分就实现了有竞争力的性能。", "conclusion": "该方法在知识图谱补全中实现了规则数量的显著减少和性能的提升，保持了可解释性，并为基于学习规则集的通用概率推理提供了潜在应用。"}}
{"id": "2508.06554", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06554", "abs": "https://arxiv.org/abs/2508.06554", "authors": ["Abdelhaleem Saad", "Waseem Akram", "Irfan Hussain"], "title": "AquaChat++: LLM-Assisted Multi-ROV Inspection for Aquaculture Net Pens with Integrated Battery Management and Thruster Fault Tolerance", "comment": null, "summary": "Inspection of aquaculture net pens is essential for ensuring the structural\nintegrity and sustainable operation of offshore fish farming systems.\nTraditional methods, typically based on manually operated or single-ROV\nsystems, offer limited adaptability to real-time constraints such as energy\nconsumption, hardware faults, and dynamic underwater conditions. This paper\nintroduces AquaChat++, a novel multi-ROV inspection framework that uses Large\nLanguage Models (LLMs) to enable adaptive mission planning, coordinated task\nexecution, and fault-tolerant control in complex aquaculture environments. The\nproposed system consists of a two-layered architecture. The high-level plan\ngeneration layer employs an LLM, such as ChatGPT-4, to translate natural\nlanguage user commands into symbolic, multi-agent inspection plans. A task\nmanager dynamically allocates and schedules actions among ROVs based on their\nreal-time status and operational constraints, including thruster faults and\nbattery levels. The low-level control layer ensures accurate trajectory\ntracking and integrates thruster fault detection and compensation mechanisms.\nBy incorporating real-time feedback and event-triggered replanning, AquaChat++\nenhances system robustness and operational efficiency. Simulated experiments in\na physics-based aquaculture environment demonstrate improved inspection\ncoverage, energy-efficient behavior, and resilience to actuator failures. These\nfindings highlight the potential of LLM-driven frameworks to support scalable,\nintelligent, and autonomous underwater robotic operations within the\naquaculture sector.", "AI": {"tldr": "AquaChat++是一个多ROV水产养殖检查框架，利用大型语言模型（LLMs）实现自适应任务规划、协调执行和故障容错控制，提高了检查效率和系统鲁棒性。", "motivation": "传统的养殖网箱检查方法（手动或单ROV系统）在实时约束（如能耗、硬件故障、动态水下条件）下适应性有限，需要更智能、更具弹性的解决方案。", "method": "AquaChat++采用两层架构：1. 高层规划层：使用LLM（如ChatGPT-4）将自然语言用户命令转换为符号化的多智能体检查计划，并通过任务管理器根据ROV实时状态和约束（推进器故障、电池电量）动态分配和调度任务。2. 低层控制层：确保精确的轨迹跟踪，并集成推进器故障检测和补偿机制。系统还结合了实时反馈和事件触发的重新规划。", "result": "在基于物理的水产养殖模拟环境中进行的实验表明，AquaChat++提高了检查覆盖率，实现了节能行为，并增强了对执行器故障的弹性。", "conclusion": "LLM驱动的框架（如AquaChat++）在支持水产养殖领域可扩展、智能化和自主水下机器人操作方面具有巨大潜力。"}}
{"id": "2508.06595", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06595", "abs": "https://arxiv.org/abs/2508.06595", "authors": ["Xiaoyuan Zhu", "Muru Zhang", "Ollie Liu", "Robin Jia", "Willie Neiswanger"], "title": "LLM Unlearning Without an Expert Curated Dataset", "comment": null, "summary": "Modern large language models often encode sensitive, harmful, or copyrighted\nknowledge, raising the need for post-hoc unlearning-the ability to remove\nspecific domains of knowledge from a model without full retraining. A major\nbottleneck in current unlearning pipelines is constructing effective forget\nsets-datasets that approximate the target domain and guide the model to forget\nit. In this work, we introduce a scalable, automated approach to generate\nhigh-quality forget sets using language models themselves. Our method\nsynthesizes textbook-style data through a structured prompting pipeline,\nrequiring only a domain name as input. Through experiments on unlearning\nbiosecurity, cybersecurity, and Harry Potter novels, we show that our synthetic\ndatasets consistently outperform the baseline synthetic alternatives and are\ncomparable to the expert-curated ones. Additionally, ablation studies reveal\nthat the multi-step generation pipeline significantly boosts data diversity,\nwhich in turn improves unlearning utility. Overall, our findings suggest that\nsynthetic datasets offer a promising path toward practical, scalable unlearning\nfor a wide range of emerging domains without the need for manual intervention.\nWe release our code and dataset at\nhttps://github.com/xyzhu123/Synthetic_Textbook.", "AI": {"tldr": "该研究提出了一种利用大型语言模型自动生成高质量“遗忘集”的方法，以实现对模型中特定敏感知识的后处理遗忘，无需人工干预。", "motivation": "现代大型语言模型常包含敏感、有害或受版权保护的知识，需要进行后处理遗忘。当前遗忘流程的主要瓶颈是构建有效的“遗忘集”（即近似目标领域并指导模型遗忘的数据集）。", "method": "引入了一种可扩展、自动化的方法，利用语言模型本身生成高质量遗忘集。该方法通过结构化提示管道合成教科书式数据，仅需一个领域名称作为输入。实验中还进行了消融研究，以评估多步生成管道对数据多样性和遗忘效果的影响。", "result": "在生物安全、网络安全和《哈利·波特》小说领域的遗忘实验中，合成数据集始终优于基线合成替代方案，且与专家策划的数据集效果相当。消融研究表明，多步生成管道显著提高了数据多样性，从而提升了遗忘效用。", "conclusion": "研究结果表明，合成数据集为广泛新兴领域的实用、可扩展的知识遗忘提供了一条有前景的途径，且无需人工干预。"}}
{"id": "2508.06529", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06529", "abs": "https://arxiv.org/abs/2508.06529", "authors": ["Jiayuan Wang", "Q. M. Jonathan Wu", "Katsuya Suto", "Ning Zhang"], "title": "RMT-PPAD: Real-time Multi-task Learning for Panoptic Perception in Autonomous Driving", "comment": null, "summary": "Autonomous driving systems rely on panoptic driving perception that requires\nboth precision and real-time performance. In this work, we propose RMT-PPAD, a\nreal-time, transformer-based multi-task model that jointly performs object\ndetection, drivable area segmentation, and lane line segmentation. We introduce\na lightweight module, a gate control with an adapter to adaptively fuse shared\nand task-specific features, effectively alleviating negative transfer between\ntasks. Additionally, we design an adaptive segmentation decoder to learn the\nweights over multi-scale features automatically during the training stage. This\navoids the manual design of task-specific structures for different segmentation\ntasks. We also identify and resolve the inconsistency between training and\ntesting labels in lane line segmentation. This allows fairer evaluation.\nExperiments on the BDD100K dataset demonstrate that RMT-PPAD achieves\nstate-of-the-art results with mAP50 of 84.9% and Recall of 95.4% for object\ndetection, mIoU of 92.6% for drivable area segmentation, and IoU of 56.8% and\naccuracy of 84.7% for lane line segmentation. The inference speed reaches 32.6\nFPS. Moreover, we introduce real-world scenarios to evaluate RMT-PPAD\nperformance in practice. The results show that RMT-PPAD consistently delivers\nstable performance. The source codes and pre-trained models are released at\nhttps://github.com/JiayuanWang-JW/RMT-PPAD.", "AI": {"tldr": "RMT-PPAD是一种实时的、基于Transformer的多任务模型，用于自动驾驶感知，可同时进行目标检测、可行驶区域和车道线分割，并实现了最先进的性能。", "motivation": "自动驾驶系统需要同时具备高精度和实时性能的全景驾驶感知能力。", "method": "该研究提出了RMT-PPAD模型，一个基于Transformer的实时多任务模型，用于联合执行目标检测、可行驶区域分割和车道线分割。模型引入了一个带有适配器的门控模块，用于自适应融合共享和任务特定特征，以缓解负迁移。此外，设计了一个自适应分割解码器，在训练阶段自动学习多尺度特征的权重，避免了为不同分割任务手动设计特定结构。同时，解决了车道线分割中训练和测试标签不一致的问题。", "result": "在BDD100K数据集上，RMT-PPAD在目标检测方面达到84.9%的mAP50和95.4%的Recall，在可行驶区域分割方面达到92.6%的mIoU，在车道线分割方面达到56.8%的IoU和84.7%的准确率，均达到最先进水平。推理速度达到32.6 FPS。在真实世界场景中也表现出稳定的性能。", "conclusion": "RMT-PPAD是一种高效且稳定的实时多任务感知模型，能够满足自动驾驶对精度和实时性的要求，并在多项任务上取得了优异的表现。"}}
{"id": "2508.07314", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.07314", "abs": "https://arxiv.org/abs/2508.07314", "authors": ["Xinlei Zhou", "Han Du", "Emily W. Yap", "Wanbin Dou", "Mingyang Huang", "Zhenjun Ma"], "title": "Human-in-the-Loop Simulation for Real-Time Exploration of HVAC Demand Flexibility", "comment": null, "summary": "The increasing integration of renewable energy into the power grid has\nhighlighted the critical importance of demand-side flexibility. Among flexible\nloads, heating, ventilation, and air-conditioning (HVAC) systems are\nparticularly significant due to their high energy consumption and\ncontrollability. This study presents the development of an interactive\nsimulation platform that integrates a high-fidelity simulation engine with a\nuser-facing dashboard, specifically designed to explore and demonstrate the\ndemand flexibility capacity of HVAC systems. Unlike conventional simulations,\nwhere users are passive observers of simulation results with no ability to\nintervene in the embedded control during the simulation, this platform\ntransforms them into active participants. Users can override system default\ncontrol settings, such as zone temperature setpoints and HVAC schedules, at any\npoint during the simulation runtime to implement demand response strategies of\ntheir choice. This human-in-the-loop capability enables real-time interaction\nand allows users to observe the immediate impact of their actions, emulating\nthe practical decision-making process of a building or system operator. By\nexploring different demand flexibility scenarios and system behaviour in a\nmanner that reflects real-world operation, users gain a deeper understanding of\ndemand flexibility and their impacts. This interactive experience builds\nconfidence and supports more informed decision-making in the practical adoption\nof demand-side flexibility. This paper presents the architecture of the\nsimulation platform, user-oriented dashboard design, and user case showcase.\nThe introduced human-in-the-loop simulation paradigm offers a more intuitive\nand interactive means of engaging with grid-interactive building operations,\nextending beyond HVAC demand flexibility exploration.", "AI": {"tldr": "本文介绍了一个交互式模拟平台，该平台将高保真模拟引擎与用户仪表盘集成，旨在探索和展示HVAC系统的需求侧灵活性，允许用户实时干预模拟过程以观察其决策影响。", "motivation": "可再生能源日益融入电网，凸显了需求侧灵活性的重要性。HVAC系统因其高能耗和可控性，在灵活负荷中尤为关键。传统模拟无法让用户主动参与，限制了对实际决策过程的理解和信心建立。", "method": "开发了一个交互式模拟平台，结合了高保真模拟引擎和用户友好的仪表盘。该平台的核心特点是“人在回路”（human-in-the-loop）功能，允许用户在模拟运行时随时修改HVAC控制设置（如温度设定点和时间表），以实施其选择的需求响应策略。", "result": "通过该平台，用户可以探索不同的需求灵活性场景和系统行为，实时观察其操作的即时影响，从而更深入地理解需求灵活性及其影响。这种交互式体验有助于建立用户信心，并支持在实际采用需求侧灵活性时做出更明智的决策。论文还展示了平台架构、用户仪表盘设计和用户案例。", "conclusion": "所提出的“人在回路”模拟范式提供了一种更直观、更具互动性的方式来参与电网互动型建筑运营，其应用前景超越了HVAC需求灵活性探索。"}}
{"id": "2508.07270", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.07270", "abs": "https://arxiv.org/abs/2508.07270", "authors": ["Xiang Xiang", "Qinhao Zhou", "Zhuo Xu", "Jing Ma", "Jiaxin Dai", "Yifan Liang", "Hanlin Li"], "title": "OpenHAIV: A Framework Towards Practical Open-World Learning", "comment": "Codes, results, and OpenHAIV documentation available at\n  https://haiv-lab.github.io/openhaiv", "summary": "Substantial progress has been made in various techniques for open-world\nrecognition. Out-of-distribution (OOD) detection methods can effectively\ndistinguish between known and unknown classes in the data, while incremental\nlearning enables continuous model knowledge updates. However, in open-world\nscenarios, these approaches still face limitations. Relying solely on OOD\ndetection does not facilitate knowledge updates in the model, and incremental\nfine-tuning typically requires supervised conditions, which significantly\ndeviate from open-world settings. To address these challenges, this paper\nproposes OpenHAIV, a novel framework that integrates OOD detection, new class\ndiscovery, and incremental continual fine-tuning into a unified pipeline. This\nframework allows models to autonomously acquire and update knowledge in\nopen-world environments. The proposed framework is available at\nhttps://haiv-lab.github.io/openhaiv .", "AI": {"tldr": "本文提出OpenHAIV框架，将OOD检测、新类别发现和增量持续微调整合，使模型能在开放世界环境中自主获取和更新知识。", "motivation": "现有OOD检测方法无法更新模型知识，而增量学习通常需要监督，这与开放世界场景不符，导致模型在开放世界中知识获取和更新受限。", "method": "提出OpenHAIV框架，该框架将OOD检测、新类别发现和增量持续微调整合到一个统一的流水线中。", "result": "该框架使模型能够在开放世界环境中自主获取和更新知识。", "conclusion": "OpenHAIV提供了一个新颖的解决方案，通过整合多项技术，克服了现有开放世界识别方法在知识更新和自主学习方面的局限性。"}}
{"id": "2508.06716", "categories": ["cs.AI", "cs.LG", "cs.LO"], "pdf": "https://arxiv.org/pdf/2508.06716", "abs": "https://arxiv.org/abs/2508.06716", "authors": ["Blair Johnson", "Clayton Kerce", "Faramarz Fekri"], "title": "GLIDR: Graph-Like Inductive Logic Programming with Differentiable Reasoning", "comment": null, "summary": "Differentiable inductive logic programming (ILP) techniques have proven\neffective at finding approximate rule-based solutions to link prediction and\nnode classification problems on knowledge graphs; however, the common\nassumption of chain-like rule structure can hamper the performance and\ninterpretability of existing approaches. We introduce GLIDR, a differentiable\nrule learning method that models the inference of logic rules with more\nexpressive syntax than previous methods. GLIDR uses a differentiable message\npassing inference algorithm that generalizes previous chain-like rule learning\nmethods to allow rules with features like branches and cycles. GLIDR has a\nsimple and expressive rule search space which is parameterized by a limit on\nthe maximum number of free variables that may be included in a rule. Explicit\nlogic rules can be extracted from the weights of a GLIDR model for use with\nsymbolic solvers. We demonstrate that GLIDR can significantly outperform\nexisting rule learning methods on knowledge graph completion tasks and even\ncompete with embedding methods despite the inherent disadvantage of being a\nstructure-only prediction method. We show that rules extracted from GLIDR\nretain significant predictive performance, and that GLIDR is highly robust to\ntraining data noise. Finally, we demonstrate that GLIDR can be chained with\ndeep neural networks and optimized end-to-end for rule learning on arbitrary\ndata modalities.", "AI": {"tldr": "GLIDR是一种新的可微分归纳逻辑编程（ILP）方法，用于知识图谱补全，它通过可微分消息传递算法支持更具表现力的规则语法（如分支和循环），显著优于现有规则学习方法，并能与嵌入方法竞争，同时提供可解释的逻辑规则。", "motivation": "现有的可微分归纳逻辑编程（ILP）技术在知识图谱上寻找近似基于规则的解决方案时，普遍假设链式规则结构，这限制了现有方法的性能和可解释性。", "method": "GLIDR通过可微分消息传递推理算法来建模逻辑规则的推理，该算法比现有方法使用更具表现力的语法，允许规则包含分支和循环等特征。GLIDR的规则搜索空间简单且富有表现力，其参数由规则中可包含的最大自由变量数限制。可以从GLIDR模型的权重中提取显式逻辑规则，供符号求解器使用。此外，GLIDR可以与深度神经网络串联，并进行端到端优化，以在任意数据模态上进行规则学习。", "result": "GLIDR在知识图谱补全任务上显著优于现有规则学习方法，甚至能够与嵌入方法竞争，尽管其是纯结构预测方法。从GLIDR中提取的规则保留了显著的预测性能，并且GLIDR对训练数据噪声具有高度鲁棒性。GLIDR能够与深度神经网络结合，并端到端优化，实现在任意数据模态上的规则学习。", "conclusion": "GLIDR提供了一种新的、更具表现力的可微分规则学习方法，克服了现有ILP方法在规则结构上的限制。它在知识图谱补全任务上表现出色，能够提取可解释的规则，对噪声鲁棒，并能与深度学习模型结合，为符号和神经网络方法的融合提供了新的途径。"}}
{"id": "2508.06568", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06568", "abs": "https://arxiv.org/abs/2508.06568", "authors": ["Amin Yazdanshenas", "Reza Faieghi"], "title": "Robust and Agile Quadrotor Flight via Adaptive Unwinding-Free Quaternion Sliding Mode Control", "comment": null, "summary": "This paper presents a new adaptive sliding mode control (SMC) framework for\nquadrotors that achieves robust and agile flight under tight computational\nconstraints. The proposed controller addresses key limitations of prior SMC\nformulations, including (i) the slow convergence and almost-global stability of\n$\\mathrm{SO(3)}$-based methods, (ii) the oversimplification of rotational\ndynamics in Euler-based controllers, (iii) the unwinding phenomenon in\nquaternion-based formulations, and (iv) the gain overgrowth problem in adaptive\nSMC schemes. Leveraging nonsmooth stability analysis, we provide rigorous\nglobal stability proofs for both the nonsmooth attitude sliding dynamics\ndefined on $\\mathbb{S}^3$ and the position sliding dynamics. Our controller is\ncomputationally efficient and runs reliably on a resource-constrained nano\nquadrotor, achieving 250 Hz and 500 Hz refresh rates for position and attitude\ncontrol, respectively. In an extensive set of hardware experiments with over\n130 flight trials, the proposed controller consistently outperforms three\nbenchmark methods, demonstrating superior trajectory tracking accuracy and\nrobustness with relatively low control effort. The controller enables\naggressive maneuvers such as dynamic throw launches, flip maneuvers, and\naccelerations exceeding 3g, which is remarkable for a 32-gram nano quadrotor.\nThese results highlight promising potential for real-world applications,\nparticularly in scenarios requiring robust, high-performance flight control\nunder significant external disturbances and tight computational constraints.", "AI": {"tldr": "本文提出了一种新型自适应滑模控制（SMC）框架，用于四旋翼飞行器在计算受限条件下实现鲁棒、敏捷的飞行，并通过严格的稳定性证明和硬件实验验证了其优越性能。", "motivation": "现有SMC方法存在局限性：SO(3)方法的收敛慢和稳定性问题，欧拉角控制对旋转动力学过度简化，四元数方法的缠绕现象，以及自适应SMC方案中的增益过度增长问题。", "method": "提出了一种新的自适应滑模控制（SMC）框架，利用非光滑稳定性分析，为定义在S3上的非光滑姿态滑模动力学和位置滑模动力学提供了严格的全局稳定性证明。该控制器计算效率高。", "result": "该控制器能在资源受限的纳米四旋翼飞行器上可靠运行，位置和姿态控制刷新率分别达到250 Hz和500 Hz。在超过130次飞行试验中，其性能始终优于三种基准方法，展现出卓越的轨迹跟踪精度、鲁棒性及相对较低的控制力。该控制器使32克纳米四旋翼飞行器能够执行动态抛掷启动、翻转机动和超过3g的加速等激进动作。", "conclusion": "研究结果表明该控制器在需要鲁棒、高性能飞行控制且面临显著外部扰动和严格计算限制的实际应用场景中具有巨大潜力。"}}
{"id": "2508.06600", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.06600", "abs": "https://arxiv.org/abs/2508.06600", "authors": ["Zijian Chen", "Xueguang Ma", "Shengyao Zhuang", "Ping Nie", "Kai Zou", "Andrew Liu", "Joshua Green", "Kshama Patel", "Ruoxi Meng", "Mingyi Su", "Sahel Sharifymoghaddam", "Yanxi Li", "Haoran Hong", "Xinyu Shi", "Xuye Liu", "Nandan Thakur", "Crystina Zhang", "Luyu Gao", "Wenhu Chen", "Jimmy Lin"], "title": "BrowseComp-Plus: A More Fair and Transparent Evaluation Benchmark of Deep-Research Agent", "comment": null, "summary": "Deep-Research agents, which integrate large language models (LLMs) with\nsearch tools, have shown success in improving the effectiveness of handling\ncomplex queries that require iterative search planning and reasoning over\nsearch results. Evaluations on current benchmarks like BrowseComp relies on\nblack-box live web search APIs, have notable limitations in (1) fairness:\ndynamic and opaque web APIs hinder fair comparisons and reproducibility of deep\nresearch methods; (2) transparency: lack of control over the document corpus\nmakes it difficult to isolate retriever contributions. In other words, the\ncurrent evaluations may compare a complete deep research system at a given\ntime, but they do not foster well-controlled experiments to provide insights\ninto the capability of underlying deep research LLMs. To address these\nchallenges, we introduce BrowseComp-Plus, a benchmark derived from BrowseComp,\nemploying a fixed, carefully curated corpus. Each query in BrowseComp-Plus\nincludes human-verified supporting documents and mined challenging negatives,\nenabling controlled experimentation. The benchmark is shown to be effective in\ndistinguishing the performance of deep research systems. For instance, the\nopen-source model Search-R1, when paired with the BM25 retriever, achieves\n3.86% accuracy, whereas the GPT-5 achieves 55.9%. Integrating the GPT-5 with\nthe Qwen3-Embedding-8B retriever further enhances its accuracy to 70.1% with\nfewer search calls. This benchmark allows comprehensive evaluation and\ndisentangled analysis of deep research agents and retrieval methods, fostering\ninsights into retrieval effectiveness, citation accuracy, and context\nengineering in Deep-Research system.", "AI": {"tldr": "本文介绍了BrowseComp-Plus，一个用于评估深度研究代理的新基准，它通过使用固定、精心策划的语料库来解决现有基准在公平性和透明度方面的局限性，从而实现对检索和语言模型能力的解耦分析。", "motivation": "现有基准（如BrowseComp）依赖黑盒实时网络搜索API，导致评估缺乏公平性、可复现性和透明度，难以独立评估底层深度研究LLM的能力或检索器的贡献。", "method": "研究者引入了BrowseComp-Plus基准，它源自BrowseComp，但采用固定、精心策划的语料库。每个查询都包含人工验证的支持文档和挖掘出的挑战性负例，从而实现受控实验。", "result": "BrowseComp-Plus能够有效区分不同深度研究系统的性能。例如，开源模型Search-R1与BM25检索器结合时准确率为3.86%，而GPT-5的准确率为55.9%。将GPT-5与Qwen3-Embedding-8B检索器集成后，其准确率进一步提升至70.1%，且搜索调用次数更少。", "conclusion": "BrowseComp-Plus基准能够对深度研究代理和检索方法进行全面评估和解耦分析，为检索有效性、引用准确性以及深度研究系统中的上下文工程提供了深入见解。"}}
{"id": "2508.06530", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06530", "abs": "https://arxiv.org/abs/2508.06530", "authors": ["Ming-Kun Xie", "Jia-Hao Xiao", "Gang Niu", "Lei Feng", "Zhiqiang Kou", "Min-Ling Zhang", "Masashi Sugiyama"], "title": "What Makes \"Good\" Distractors for Object Hallucination Evaluation in Large Vision-Language Models?", "comment": null, "summary": "Large Vision-Language Models (LVLMs), empowered by the success of Large\nLanguage Models (LLMs), have achieved impressive performance across domains.\nDespite the great advances in LVLMs, they still suffer from the unavailable\nobject hallucination issue, which tends to generate objects inconsistent with\nthe image content. The most commonly used Polling-based Object Probing\nEvaluation (POPE) benchmark evaluates this issue by sampling negative\ncategories according to category-level statistics, \\textit{e.g.}, category\nfrequencies and co-occurrence. However, with the continuous advancement of\nLVLMs, the POPE benchmark has shown diminishing effectiveness in assessing\nobject hallucination, as it employs a simplistic sampling strategy that\noverlooks image-specific information and restricts distractors to negative\nobject categories only. In this paper, we introduce the Hallucination\nsearching-based Object Probing Evaluation (HOPE) benchmark, aiming to generate\nthe most misleading distractors (\\textit{i.e.}, non-existent objects or\nincorrect image descriptions) that can trigger hallucination in LVLMs, which\nserves as a means to more rigorously assess their immunity to hallucination. To\nexplore the image-specific information, the content-aware hallucination\nsearching leverages Contrastive Language-Image Pre-Training (CLIP) to\napproximate the predictive behavior of LVLMs by selecting negative objects with\nthe highest predicted likelihood as distractors. To expand the scope of\nhallucination assessment, the description-based hallucination searching\nconstructs highly misleading distractors by pairing true objects with false\ndescriptions. Experimental results show that HOPE leads to a precision drop of\nat least 9\\% and up to 23\\% across various state-of-the-art LVLMs,\nsignificantly outperforming POPE in exposing hallucination vulnerabilities. The\ncode is available at https://github.com/xiemk/HOPE.", "AI": {"tldr": "本文提出HOPE基准，通过生成最易误导的干扰项（不存在的物体或错误描述），更严格地评估大型视觉语言模型（LVLMs）的幻觉问题，并证明其比现有方法更有效。", "motivation": "尽管LVLMs取得了显著进展，但仍存在“不可用对象幻觉”问题，即生成与图像内容不符的对象。现有评估基准POPE因其简单的采样策略（忽略图像特定信息且仅限于负对象类别），在评估LV觉幻觉方面效果逐渐减弱。", "method": "本文引入了基于幻觉搜索的对象探测评估（HOPE）基准。HOPE通过两种策略生成最具误导性的干扰项：1) 内容感知幻觉搜索：利用CLIP（对比语言-图像预训练）选择预测可能性最高的负对象作为干扰项，以探索图像特定信息。2) 基于描述的幻觉搜索：将真实对象与虚假描述配对，以扩大幻觉评估范围。", "result": "实验结果表明，HOPE导致各种最先进的LVLMs的精度至少下降9%，最高下降23%，显著优于POPE，能更有效地揭示LVLMs的幻觉漏洞。", "conclusion": "HOPE基准提供了一种更严格和有效的手段来评估LVLMs对幻觉的抵抗力，能更好地暴露其幻觉漏洞。"}}
{"id": "2508.07376", "categories": ["eess.SY", "cs.SY", "90B25, 65C05, 86A15"], "pdf": "https://arxiv.org/pdf/2508.07376", "abs": "https://arxiv.org/abs/2508.07376", "authors": ["Huangbin Liang", "Beatriz Moya", "Francisco Chinesta", "Eleni Chatzi"], "title": "A Multi-Model Probabilistic Framework for Seismic Risk Assessment and Retrofit Planning of Electric Power Networks", "comment": "13 figures", "summary": "Electric power networks are critical lifelines, and their disruption during\nearthquakes can lead to severe cascading failures and significantly hinder\npost-disaster recovery. Enhancing their seismic resilience requires identifying\nand strengthening vulnerable components in a cost-effective and system-aware\nmanner. However, existing studies often overlook the systemic behavior of power\nnetworks under seismic loading. Common limitations include isolated component\nanalyses that neglect network-wide interdependencies, oversimplified damage\nmodels assuming binary states or damage independence, and the exclusion of\nelectrical operational constraints. These simplifications can result in\ninaccurate risk estimates and inefficient retrofit decisions. This study\nproposes a multi-model probabilistic framework for seismic risk assessment and\nretrofit planning of electric power systems. The approach integrates: (1)\nregional seismic hazard characterization with ground motion prediction and\nspatial correlation models; (2) component-level damage analysis using fragility\nfunctions and multi-state damage-functionality mappings; (3) system-level\ncascading impact evaluation through graph-based island detection and\nconstrained optimal power flow analysis; and (4) retrofit planning via\nheuristic optimization to minimize expected annual functionality loss (EAFL)\nunder budget constraints. Uncertainty is propagated throughout the framework\nusing Monte Carlo simulation. The methodology is demonstrated on the IEEE\n24-bus Reliability Test System, showcasing its ability to capture cascading\nfailures, identify critical components, and generate effective retrofit\nstrategies. Results underscore the potential of the framework as a scalable,\ndata-informed decision-support tool for enhancing the seismic resilience of\npower infrastructure.", "AI": {"tldr": "本研究提出了一个多模型概率框架，用于电力系统地震风险评估和抗震加固规划，旨在通过系统级分析和优化，提高电力基础设施的抗震韧性。", "motivation": "电力网络是关键生命线，地震破坏会导致严重的级联故障并阻碍灾后恢复。现有研究常忽略地震荷载下电网的系统行为，存在孤立的组件分析、过度简化的损伤模型以及排除电气运行约束等局限性，导致风险估计不准确和加固决策效率低下。", "method": "本研究提出了一个多模型概率框架，整合了四个方面：1) 区域地震灾害特征描述（包括地面运动预测和空间相关模型）；2) 组件级损伤分析（使用易损性函数和多状态损伤-功能映射）；3) 系统级级联影响评估（通过基于图的孤岛检测和约束最优潮流分析）；4) 加固规划（通过启发式优化在预算约束下最小化预期年功能损失）。整个框架的不确定性通过蒙特卡洛模拟传播。", "result": "该方法在IEEE 24节点可靠性测试系统上进行了演示，结果表明其能够捕获级联故障、识别关键组件并生成有效的加固策略。研究强调了该框架作为可扩展、数据驱动的决策支持工具在增强电力基础设施抗震韧性方面的潜力。", "conclusion": "该多模型概率框架为电力系统地震风险评估和抗震加固规划提供了一个全面且系统化的方法，能够有效提高电力基础设施的地震韧性，并作为一种可扩展的决策支持工具发挥重要作用。"}}
{"id": "2508.07483", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.07483", "abs": "https://arxiv.org/abs/2508.07483", "authors": ["Pranav Chougule"], "title": "Novel View Synthesis with Gaussian Splatting: Impact on Photogrammetry Model Accuracy and Resolution", "comment": null, "summary": "In this paper, I present a comprehensive study comparing Photogrammetry and\nGaussian Splatting techniques for 3D model reconstruction and view synthesis. I\ncreated a dataset of images from a real-world scene and constructed 3D models\nusing both methods. To evaluate the performance, I compared the models using\nstructural similarity index (SSIM), peak signal-to-noise ratio (PSNR), learned\nperceptual image patch similarity (LPIPS), and lp/mm resolution based on the\nUSAF resolution chart. A significant contribution of this work is the\ndevelopment of a modified Gaussian Splatting repository, which I forked and\nenhanced to enable rendering images from novel camera poses generated in the\nBlender environment. This innovation allows for the synthesis of high-quality\nnovel views, showcasing the flexibility and potential of Gaussian Splatting. My\ninvestigation extends to an augmented dataset that includes both original\nground images and novel views synthesized via Gaussian Splatting. This\naugmented dataset was employed to generate a new photogrammetry model, which\nwas then compared against the original photogrammetry model created using only\nthe original images. The results demonstrate the efficacy of using Gaussian\nSplatting to generate novel high-quality views and its potential to improve\nphotogrammetry-based 3D reconstructions. The comparative analysis highlights\nthe strengths and limitations of both approaches, providing valuable\ninformation for applications in extended reality (XR), photogrammetry, and\nautonomous vehicle simulations. Code is available at\nhttps://github.com/pranavc2255/gaussian-splatting-novel-view-render.git.", "AI": {"tldr": "本文对比了摄影测量和高斯泼溅技术在三维模型重建和视图合成方面的性能，并探索了高斯泼溅生成的新颖视图对摄影测量重建的潜在改进。", "motivation": "评估和比较摄影测量与高斯泼溅在三维模型重建和视图合成方面的能力，并研究高斯泼溅生成的高质量新颖视图是否能提升基于摄影测量的三维重建效果。", "method": "作者创建了一个真实场景图像数据集，并使用摄影测量和高斯泼溅分别构建了三维模型。通过SSIM、PSNR、LPIPS和USAF分辨率图的lp/mm分辨率指标进行性能评估。核心创新是开发了一个修改版高斯泼溅代码库，使其能够渲染Blender环境中生成的新颖相机姿态图像。此外，作者还构建了一个包含原始图像和高斯泼溅合成新颖视图的增强数据集，并用此数据集生成了新的摄影测量模型，与仅使用原始图像生成的模型进行对比。", "result": "研究结果表明，高斯泼溅能够生成高质量的新颖视图，并且利用高斯泼溅合成的新颖视图可以有效改善基于摄影测量的三维重建。比较分析揭示了两种方法的优缺点。", "conclusion": "高斯泼溅在生成高质量新颖视图方面表现出色，并具有提升摄影测量三维重建的潜力，为扩展现实(XR)、摄影测量和自动驾驶车辆模拟等应用提供了有价值的信息。"}}
{"id": "2508.06736", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06736", "abs": "https://arxiv.org/abs/2508.06736", "authors": ["Alican Yilmaz", "Junyang Cai", "Serdar Kadioglu", "Bistra Dilkina"], "title": "ParBalans: Parallel Multi-Armed Bandits-based Adaptive Large Neighborhood Search", "comment": null, "summary": "Solving Mixed-Integer Programming (MIP) problems often requires substantial\ncomputational resources due to their combinatorial nature. Parallelization has\nemerged as a critical strategy to accelerate solution times and enhance\nscalability to tackle large, complex instances. This paper investigates the\nparallelization capabilities of Balans, a recently proposed multi-armed\nbandits-based adaptive large neighborhood search for MIPs. While Balans's\nmodular architecture inherently supports parallel exploration of diverse\nparameter configurations, this potential has not been thoroughly examined. To\naddress this gap, we introduce ParBalans, an extension that leverages both\nsolver-level and algorithmic-level parallelism to improve performance on\nchallenging MIP instances. Our experimental results demonstrate that ParBalans\nexhibits competitive performance compared to the state-of-the-art commercial\nsolver Gurobi, particularly on hard optimization benchmarks.", "AI": {"tldr": "本文研究了Balans（一种基于多臂赌博机的自适应大邻域搜索算法）的并行化能力，并提出了ParBalans，通过结合求解器级和算法级并行性，显著提升了混合整数规划（MIP）问题的求解性能，可与商业求解器Gurobi媲美。", "motivation": "混合整数规划（MIP）问题因其组合性质，求解通常需要大量计算资源。并行化是加速求解时间和增强可扩展性的关键策略。Balans作为一种新提出的MIP求解方法，其模块化架构本身支持并行探索，但这种潜力尚未被充分研究，因此需要填补这一空白。", "method": "本文提出了ParBalans，它是Balans的扩展，通过利用求解器级（solver-level）和算法级（algorithmic-level）的并行性来提高在具有挑战性的MIP实例上的性能。", "result": "实验结果表明，ParBalans在性能上与最先进的商业求解器Gurobi具有竞争力，特别是在困难的优化基准测试上表现出色。", "conclusion": "ParBalans通过有效利用并行化，显著提升了Balans在解决复杂MIP问题上的性能，使其成为一个有竞争力的求解工具。"}}
{"id": "2508.06575", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06575", "abs": "https://arxiv.org/abs/2508.06575", "authors": ["Rui Zhou"], "title": "Efficient Safety Testing of Autonomous Vehicles via Adaptive Search over Crash-Derived Scenarios", "comment": null, "summary": "Ensuring the safety of autonomous vehicles (AVs) is paramount in their\ndevelopment and deployment. Safety-critical scenarios pose more severe\nchallenges, necessitating efficient testing methods to validate AVs safety.\nThis study focuses on designing an accelerated testing algorithm for AVs in\nsafety-critical scenarios, enabling swift recognition of their driving\ncapabilities. First, typical logical scenarios were extracted from real-world\ncrashes in the China In-depth Mobility Safety Study-Traffic Accident (CIMSS-TA)\ndatabase, obtaining pre-crash features through reconstruction. Second, Baidu\nApollo, an advanced black-box automated driving system (ADS) is integrated to\ncontrol the behavior of the ego vehicle. Third, we proposed an adaptive\nlarge-variable neighborhood-simulated annealing algorithm (ALVNS-SA) to\nexpedite the testing process. Experimental results demonstrate a significant\nenhancement in testing efficiency when utilizing ALVNS-SA. It achieves an\n84.00% coverage of safety-critical scenarios, with crash scenario coverage of\n96.83% and near-crash scenario coverage of 92.07%. Compared to genetic\nalgorithm (GA), adaptive large neighborhood-simulated annealing algorithm\n(ALNS-SA), and random testing, ALVNS-SA exhibits substantially higher coverage\nin safety-critical scenarios.", "AI": {"tldr": "本研究提出了一种自适应大变量邻域模拟退火算法（ALVNS-SA），用于加速自动驾驶汽车在安全关键场景下的测试，以高效识别其驾驶能力。", "motivation": "确保自动驾驶汽车（AVs）的安全性至关重要，尤其是在安全关键场景下。需要高效的测试方法来验证AVs的安全性。", "method": "首先，从中国深度机动性安全研究-交通事故（CIMSS-TA）数据库中提取典型逻辑场景，并通过重建获取碰撞前特征。其次，集成百度Apollo黑盒自动驾驶系统控制自车行为。最后，提出自适应大变量邻域模拟退火算法（ALVNS-SA）以加速测试过程。", "result": "实验结果表明，ALVNS-SA显著提高了测试效率。它实现了84.00%的安全关键场景覆盖率，其中碰撞场景覆盖率为96.83%，接近碰撞场景覆盖率为92.07%。与遗传算法（GA）、自适应大邻域模拟退火算法（ALNS-SA）和随机测试相比，ALVNS-SA在安全关键场景中表现出更高的覆盖率。", "conclusion": "ALVNS-SA算法能有效加速自动驾驶汽车在安全关键场景下的测试，并显著提高测试效率和场景覆盖率，有助于快速识别自动驾驶汽车的驾驶能力。"}}
{"id": "2508.06621", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06621", "abs": "https://arxiv.org/abs/2508.06621", "authors": ["Tomohiro Sawada", "Kartik Goyal"], "title": "Train It and Forget It: Merge Lists are Unnecessary for BPE Inference in Language Models", "comment": "Submitted to EMNLP", "summary": "Standard Byte-Pair Encoding (BPE) tokenization compresses text by pairing a\nlearned token vocabulary with a detailed merge list. Recent work has shown that\nthis merge list exposes a potential attack surface for extracting information\nabout language model's training data. In this paper, we explore the downstream\nimpact of BPE inference algorithms that do not rely on this merge list at all,\nand hence differ from the encoding process during BPE training. To address this\nquestion, we investigate two broad classes of BPE inference schemes that differ\nfrom BPE application during training: a) targeted deviation from merge-lists\nincluding random merge orders, and various corruptions of merge list involving\ndeletion/truncation, and b) non-targeted BPE inference algorithms that do not\ndepend on the merge list but focus on compressing the text either greedily or\nexactly. Extensive experiments across diverse language modeling tasks like\naccuracy-based QA benchmarks, machine translation, and open-ended generation\nreveal that while targeted deviation from the merge lists exhibits significant\ndegradation in language model performance, the non-targeted merge-list-free\ninference algorithms result in minimal impact on downstream performance that is\noften much smaller than expected. These findings pave way for simpler and\npotentially more privacy-preserving tokenization schemes that do not\ncatastrophically compromise model performance.", "AI": {"tldr": "本文探讨了不依赖合并列表的BPE推理算法对下游语言模型性能的影响，发现非目标性无合并列表推理对性能影响甚微，为更隐私的文本分词提供了可能。", "motivation": "标准BPE的合并列表暴露了语言模型训练数据的潜在攻击面，促使研究者探索不依赖此列表的BPE推理方法以提升隐私性。", "method": "研究了两种偏离训练时BPE应用方式的BPE推理方案：a) 目标性偏离合并列表，包括随机合并顺序和合并列表的删除/截断等破坏；b) 非目标性、不依赖合并列表的BPE推理算法，通过贪婪或精确压缩文本。这些方案在基于准确性的问答、机器翻译和开放式生成等语言建模任务上进行了广泛实验。", "result": "实验表明，目标性偏离合并列表会导致语言模型性能显著下降；而无合并列表的非目标性推理算法对下游性能影响极小，甚至远小于预期。", "conclusion": "研究结果为开发更简单、潜在更注重隐私的文本分词方案铺平了道路，这些方案在不严重损害模型性能的前提下，可能实现更好的隐私保护。"}}
{"id": "2508.06535", "categories": ["eess.IV", "cs.CV", "cs.LG", "F.2.2; I.2.7"], "pdf": "https://arxiv.org/pdf/2508.06535", "abs": "https://arxiv.org/abs/2508.06535", "authors": ["Faisal Ahmed"], "title": "Transfer Learning with EfficientNet for Accurate Leukemia Cell Classification", "comment": "8 pages, 1 figure", "summary": "Accurate classification of Acute Lymphoblastic Leukemia (ALL) from peripheral\nblood smear images is essential for early diagnosis and effective treatment\nplanning. This study investigates the use of transfer learning with pretrained\nconvolutional neural networks (CNNs) to improve diagnostic performance. To\naddress the class imbalance in the dataset of 3,631 Hematologic and 7,644 ALL\nimages, we applied extensive data augmentation techniques to create a balanced\ntraining set of 10,000 images per class. We evaluated several models, including\nResNet50, ResNet101, and EfficientNet variants B0, B1, and B3. EfficientNet-B3\nachieved the best results, with an F1-score of 94.30%, accuracy of 92.02%,\nandAUCof94.79%,outperformingpreviouslyreported methods in the C-NMCChallenge.\nThesefindings demonstrate the effectiveness of combining data augmentation with\nadvanced transfer learning models, particularly EfficientNet-B3, in developing\naccurate and robust diagnostic tools for hematologic malignancy detection.", "AI": {"tldr": "本研究利用迁移学习和数据增强技术，通过预训练CNN模型（特别是EfficientNet-B3）提高了急性淋巴细胞白血病（ALL）的诊断准确性。", "motivation": "从外周血涂片图像中准确分类急性淋巴细胞白血病（ALL）对于早期诊断和有效治疗计划至关重要。", "method": "研究采用迁移学习与预训练卷积神经网络（CNNs），并针对数据集中的类别不平衡问题应用了大量数据增强技术，使每个类别达到10,000张图像的平衡训练集。评估的模型包括ResNet50、ResNet101以及EfficientNet的B0、B1和B3变体。", "result": "EfficientNet-B3取得了最佳结果，F1-score达到94.30%，准确率92.02%，AUC为94.79%，优于C-NMC挑战赛中此前报告的方法。", "conclusion": "研究结果表明，结合数据增强和先进的迁移学习模型（特别是EfficientNet-B3）在开发准确和鲁棒的血液恶性肿瘤检测诊断工具方面非常有效。"}}
{"id": "2508.07453", "categories": ["eess.SY", "cs.AI", "cs.MA", "cs.RO", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.07453", "abs": "https://arxiv.org/abs/2508.07453", "authors": ["Vindula Jayawardana", "Catherine Tang", "Junyi Ji", "Jonah Philion", "Xue Bin Peng", "Cathy Wu"], "title": "Noise-Aware Generative Microscopic Traffic Simulation", "comment": null, "summary": "Accurately modeling individual vehicle behavior in microscopic traffic\nsimulation remains a key challenge in intelligent transportation systems, as it\nrequires vehicles to realistically generate and respond to complex traffic\nphenomena such as phantom traffic jams. While traditional human driver\nsimulation models offer computational tractability, they do so by abstracting\naway the very complexity that defines human driving. On the other hand, recent\nadvances in infrastructure-mounted camera-based roadway sensing have enabled\nthe extraction of vehicle trajectory data, presenting an opportunity to shift\ntoward generative, agent-based models. Yet, a major bottleneck remains: most\nexisting datasets are either overly sanitized or lack standardization, failing\nto reflect the noisy, imperfect nature of real-world sensing. Unlike data from\nvehicle-mounted sensors-which can mitigate sensing artifacts like occlusion\nthrough overlapping fields of view and sensor fusion-infrastructure-based\nsensors surface a messier, more practical view of challenges that traffic\nengineers encounter. To this end, we present the I-24 MOTION Scenario Dataset\n(I24-MSD)-a standardized, curated dataset designed to preserve a realistic\nlevel of sensor imperfection, embracing these errors as part of the learning\nproblem rather than an obstacle to overcome purely from preprocessing. Drawing\nfrom noise-aware learning strategies in computer vision, we further adapt\nexisting generative models in the autonomous driving community for I24-MSD with\nnoise-aware loss functions. Our results show that such models not only\noutperform traditional baselines in realism but also benefit from explicitly\nengaging with, rather than suppressing, data imperfection. We view I24-MSD as a\nstepping stone toward a new generation of microscopic traffic simulation that\nembraces the real-world challenges and is better aligned with practical needs.", "AI": {"tldr": "该论文提出了I-24 MOTION场景数据集（I24-MSD），一个包含真实传感器缺陷的标准化交通轨迹数据集，并结合噪声感知学习策略改进了生成模型，以实现更真实的微观交通仿真。", "motivation": "传统的交通仿真模型过于抽象，无法捕捉人类驾驶的复杂性，如幽灵堵塞。虽然基于摄像头的基础设施传感器能提供车辆轨迹数据，但现有数据集过于理想化或缺乏标准化，未能反映真实世界传感器的噪声和不完美，这与交通工程师实际遇到的挑战不符。", "method": "1. 构建了I-24 MOTION场景数据集（I24-MSD），该数据集特意保留了现实世界传感器的不完美性。2. 借鉴计算机视觉中的噪声感知学习策略，调整了自动驾驶领域现有的生成模型。3. 在模型中引入了噪声感知损失函数，以处理和利用数据中的不完美。", "result": "采用噪声感知学习策略的模型在仿真真实性方面优于传统基线模型。结果表明，明确地处理而非抑制数据中的不完美性，对模型的性能是有益的。", "conclusion": "I24-MSD数据集是迈向新一代微观交通仿真的重要一步，该仿真能够更好地应对真实世界的挑战，并与实际需求对齐。"}}
{"id": "2508.06746", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06746", "abs": "https://arxiv.org/abs/2508.06746", "authors": ["Xin Tang", "Qian Chen", "Fengshun Li", "Youchun Gong", "Yinqiu Liu", "Wen Tian", "Shaowen Qin", "Xiaohuan Li"], "title": "Topology Generation of UAV Covert Communication Networks: A Graph Diffusion Approach with Incentive Mechanism", "comment": null, "summary": "With the growing demand for Uncrewed Aerial Vehicle (UAV) networks in\nsensitive applications, such as urban monitoring, emergency response, and\nsecure sensing, ensuring reliable connectivity and covert communication has\nbecome increasingly vital. However, dynamic mobility and exposure risks pose\nsignificant challenges. To tackle these challenges, this paper proposes a\nself-organizing UAV network framework combining Graph Diffusion-based Policy\nOptimization (GDPO) with a Stackelberg Game (SG)-based incentive mechanism. The\nGDPO method uses generative AI to dynamically generate sparse but\nwell-connected topologies, enabling flexible adaptation to changing node\ndistributions and Ground User (GU) demands. Meanwhile, the Stackelberg Game\n(SG)-based incentive mechanism guides self-interested UAVs to choose relay\nbehaviors and neighbor links that support cooperation and enhance covert\ncommunication. Extensive experiments are conducted to validate the\neffectiveness of the proposed framework in terms of model convergence, topology\ngeneration quality, and enhancement of covert communication performance.", "AI": {"tldr": "本文提出了一种结合图扩散策略优化（GDPO）和Stackelberg博弈（SG）激励机制的自组织无人机网络框架，旨在实现可靠连接和隐蔽通信，以应对动态移动性和暴露风险带来的挑战。", "motivation": "随着无人机网络在城市监测、应急响应和安全传感等敏感应用中需求日益增长，确保可靠连接和隐蔽通信变得至关重要。然而，无人机的动态移动性和暴露风险带来了显著挑战。", "method": "该研究提出一个自组织无人机网络框架，结合了两种方法：1. 基于图扩散的策略优化（GDPO）：利用生成式AI动态生成稀疏但连接良好的拓扑结构，以适应不断变化的节点分布和地面用户需求。2. 基于Stackelberg博弈（SG）的激励机制：引导自私的无人机选择支持合作和增强隐蔽通信的中继行为和邻居链路。", "result": "通过大量实验验证了所提框架的有效性，表现在模型收敛性、拓扑生成质量以及隐蔽通信性能的提升方面。", "conclusion": "所提出的框架能有效应对无人机网络的动态移动性和暴露风险挑战，实现可靠连接和隐蔽通信。"}}
{"id": "2508.06687", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06687", "abs": "https://arxiv.org/abs/2508.06687", "authors": ["Sreeja Roy-Singh", "Vinay Ravindra", "Richard Levinson", "Mahta Moghaddam", "Jan Mandel", "Adam Kochanski", "Angel Farguell Caus", "Kurtis Nelson", "Samira Alkaee Taleghan", "Archana Kannan", "Amer Melebari"], "title": "Optimal Planning and Machine Learning for Responsive Tracking and Enhanced Forecasting of Wildfires using a Spacecraft Constellation", "comment": null, "summary": "We propose a novel concept of operations using optimal planning methods and\nmachine learning (ML) to collect spaceborne data that is unprecedented for\nmonitoring wildfires, process it to create new or enhanced products in the\ncontext of wildfire danger or spread monitoring, and assimilate them to improve\nexisting, wildfire decision support tools delivered to firefighters within\nlatency appropriate for time-critical applications. The concept is studied with\nrespect to NASA's CYGNSS Mission, a constellation of passive microwave\nreceivers that measure specular GNSS-R reflections despite clouds and smoke.\nOur planner uses a Mixed Integer Program formulation to schedule joint\nobservation data collection and downlink for all satellites. Optimal solutions\nare found quickly that collect 98-100% of available observation opportunities.\nML-based fire predictions that drive the planner objective are greater than 40%\nmore correlated with ground truth than existing state-of-art. The presented\ncase study on the TX Smokehouse Creek fire in 2024 and LA fires in 2025\nrepresents the first high-resolution data collected by CYGNSS of active fires.\nCreation of Burnt Area Maps (BAM) using ML applied to the data during active\nfires and BAM assimilation into NASA's Weather Research and Forecasting Model\nusing ML to broadcast fire spread are novel outcomes. BAM and CYGNSS obtained\nsoil moisture are integrated for the first time into USGS fire danger maps.\nInclusion of CYGNSS data in ML-based burn predictions boosts accuracy by 13%,\nand inclusion of high-resolution data boosts ML recall by another 15%. The\nproposed workflow has an expected latency of 6-30h, improving on the current\ndelivery time of multiple days. All components in the proposed concept are\nshown to be computationally scalable and globally generalizable, with\nsustainability considerations such as edge efficiency and low latency on small\ndevices.", "AI": {"tldr": "该研究提出了一种结合优化规划和机器学习的新型操作概念，利用NASA CYGNSS卫星数据进行野火监测，旨在生成新的火灾产品并将其整合到决策支持工具中，以实现前所未有的高精度和低延迟。", "motivation": "现有野火监测、预警和决策支持工具在数据收集、处理速度和信息传递延迟方面存在不足，无法满足时间敏感型应用的需求。需要一种能够提供前所未有的空间数据、更快处理并改善现有工具的方法。", "method": "该研究使用混合整数规划（MIP）来优化CYGNSS卫星星座的数据收集和下行链路调度。利用机器学习（ML）进行火灾预测、生成烧毁区域图（BAM）、将BAM同化到NASA的天气研究和预报模型中以广播火势蔓延，并将CYGNSS数据（包括土壤湿度）整合到USGS火灾危险地图中。案例研究包括2024年德克萨斯州Smokehouse Creek火灾和2025年路易斯安那州火灾。", "result": "优化规划器能快速找到解决方案，收集98-100%的观测机会。基于ML的火灾预测与地面实况的相关性比现有技术高40%以上。首次利用CYGNSS收集到活动火灾的高分辨率数据。通过ML从活动火灾数据中创建BAM，并将BAM同化到NASA模型中以预测火势蔓延，是新的成果。首次将BAM和CYGNSS土壤湿度整合到USGS火灾危险地图中。CYGNSS数据使ML烧毁预测精度提高13%，高分辨率数据使ML召回率再提高15%。预计工作流延迟为6-30小时，显著优于目前的数天。所有组件均具有计算可扩展性、全球通用性和可持续性。", "conclusion": "该研究提出的结合优化规划和机器学习的野火监测概念，利用CYGNSS卫星数据，显著提高了火灾数据收集、产品生成和决策支持的效率和准确性，实现了低延迟和高可扩展性，为野火管理提供了突破性进展。"}}
{"id": "2508.06649", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06649", "abs": "https://arxiv.org/abs/2508.06649", "authors": ["Daniel Wang", "Eli Brignac", "Minjia Mao", "Xiao Fang"], "title": "Measuring Stereotype and Deviation Biases in Large Language Models", "comment": null, "summary": "Large language models (LLMs) are widely applied across diverse domains,\nraising concerns about their limitations and potential risks. In this study, we\ninvestigate two types of bias that LLMs may display: stereotype bias and\ndeviation bias. Stereotype bias refers to when LLMs consistently associate\nspecific traits with a particular demographic group. Deviation bias reflects\nthe disparity between the demographic distributions extracted from\nLLM-generated content and real-world demographic distributions. By asking four\nadvanced LLMs to generate profiles of individuals, we examine the associations\nbetween each demographic group and attributes such as political affiliation,\nreligion, and sexual orientation. Our experimental results show that all\nexamined LLMs exhibit both significant stereotype bias and deviation bias\ntowards multiple groups. Our findings uncover the biases that occur when LLMs\ninfer user attributes and shed light on the potential harms of LLM-generated\noutputs.", "AI": {"tldr": "本研究揭示了大型语言模型（LLMs）在生成内容时存在显著的刻板印象偏见和偏差偏见，可能导致危害。", "motivation": "LLMs被广泛应用，但其局限性和潜在风险令人担忧，特别是它们可能展现的偏见类型。", "method": "通过要求四个先进的LLMs生成个人档案，研究人员考察了特定人口群体与政治立场、宗教、性取向等属性之间的关联，以检测刻板印象偏见（LLM将特定特质与特定群体关联）和偏差偏见（LLM生成内容中的人口分布与现实世界分布的差异）。", "result": "实验结果表明，所有被检测的LLMs都对多个群体表现出显著的刻板印象偏见和偏差偏见。", "conclusion": "研究结果揭示了LLMs在推断用户属性时出现的偏见，并强调了LLM生成输出可能造成的潜在危害。"}}
{"id": "2508.06537", "categories": ["cs.CV", "astro-ph.IM"], "pdf": "https://arxiv.org/pdf/2508.06537", "abs": "https://arxiv.org/abs/2508.06537", "authors": ["Shantanusinh Parmar"], "title": "Benchmarking Deep Learning-Based Object Detection Models on Feature Deficient Astrophotography Imagery Dataset", "comment": null, "summary": "Object detection models are typically trained on datasets like ImageNet,\nCOCO, and PASCAL VOC, which focus on everyday objects. However, these lack\nsignal sparsity found in non-commercial domains. MobilTelesco, a\nsmartphone-based astrophotography dataset, addresses this by providing sparse\nnight-sky images. We benchmark several detection models on it, highlighting\nchallenges under feature-deficient conditions.", "AI": {"tldr": "MobilTelesco是一个用于稀疏夜空图像的天文摄影数据集，旨在解决传统目标检测模型在特征稀疏场景下的挑战。", "motivation": "现有目标检测数据集（如ImageNet、COCO、PASCAL VOC）侧重日常物体，缺乏非商业领域（如天文摄影）中存在的信号稀疏性。", "method": "构建了基于智能手机的天文摄影数据集MobilTelesco，并在此数据集上对多个检测模型进行了基准测试。", "result": "基准测试揭示了在特征不足（稀疏）条件下，目标检测模型所面临的挑战。", "conclusion": "MobilTelesco数据集填补了稀疏信号数据的空白，并为研究模型在特征稀疏环境下的表现提供了平台。"}}
{"id": "2508.07515", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.07515", "abs": "https://arxiv.org/abs/2508.07515", "authors": ["Junyang Cai", "Weimin Huang", "Jyotirmoy V. Deshmukh", "Lars Lindemann", "Bistra Dilkina"], "title": "Neuro-Symbolic Acceleration of MILP Motion Planning with Temporal Logic and Chance Constraints", "comment": null, "summary": "Autonomous systems must solve motion planning problems subject to\nincreasingly complex, time-sensitive, and uncertain missions. These problems\noften involve high-level task specifications, such as temporal logic or chance\nconstraints, which require solving large-scale Mixed-Integer Linear Programs\n(MILPs). However, existing MILP-based planning methods suffer from high\ncomputational cost and limited scalability, hindering their real-time\napplicability. We propose to use a neuro-symbolic approach to accelerate\nMILP-based motion planning by leveraging machine learning techniques to guide\nthe solver's symbolic search. Focusing on two representative classes of\nplanning problems, namely, those with Signal Temporal Logic (STL)\nspecifications and those with chance constraints formulated via Conformal\nPredictive Programming (CPP). We demonstrate how graph neural network-based\nlearning methods can guide traditional symbolic MILP solvers in solving\nchallenging planning problems, including branching variable selection and\nsolver parameter configuration. Through extensive experiments, we show that\nneuro-symbolic search techniques yield scalability gains. Our approach yields\nsubstantial improvements, achieving an average performance gain of about 20%\nover state-of-the-art solver across key metrics, including runtime and solution\nquality.", "AI": {"tldr": "针对自主系统复杂的运动规划问题，本文提出一种神经符号方法，利用图神经网络加速混合整数线性规划（MILP）求解器，解决现有MILP方法计算成本高和可扩展性差的问题，在运行时和解质量上实现了显著性能提升。", "motivation": "自主系统的运动规划问题日益复杂、时间敏感且不确定，常涉及高层任务规范（如时序逻辑或机会约束），需通过大规模混合整数线性规划（MILP）求解。然而，现有基于MILP的规划方法计算成本高且可扩展性有限，阻碍了其实时应用。", "method": "提出一种神经符号方法来加速MILP运动规划。该方法利用机器学习（特别是图神经网络）指导求解器的符号搜索，具体应用于两种代表性规划问题：带有信号时序逻辑（STL）规范的问题和通过共形预测编程（CPP）表述的机会约束问题。GNNs用于指导分支变量选择和求解器参数配置。", "result": "神经符号搜索技术带来了可扩展性提升。与现有最先进的求解器相比，在运行时和解质量等关键指标上，平均性能提升约20%。", "conclusion": "神经符号方法能够有效加速基于MILP的运动规划，解决了复杂自主系统规划中的计算效率和可扩展性挑战，并带来了显著的性能改进。"}}
{"id": "2508.06753", "categories": ["cs.AI", "cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2508.06753", "abs": "https://arxiv.org/abs/2508.06753", "authors": ["Evangelos Georganas", "Dhiraj Kalamkar", "Alexander Heinecke"], "title": "Pushing the Envelope of LLM Inference on AI-PC", "comment": null, "summary": "The advent of ultra-low-bit LLM models (1/1.58/2-bit), which match the\nperplexity and end-task performance of their full-precision counterparts using\nthe same model size, is ushering in a new era of LLM inference for\nresource-constrained environments such as edge devices and AI PCs. While these\nquantization advances promise models that are more cost-effective in terms of\nlatency, memory, throughput, and energy consumption, the computational\nefficiency of state-of-the-art (SOTA) inference runtimes (e.g., bitnet.cpp)\nused to deploy them remains underexplored. In this work, we take a bottom-up\napproach: we first design and implement 1-bit and 2-bit microkernels optimized\nfor modern CPUs, achieving peak computational efficiency across a variety of\nCPU platforms. We integrate these microkernels into a state-of-the-art LLM\ninference framework, namely PyTorch-TPP, and present end-to-end inference\nresults with 2-bit models that outperform the current SOTA runtime bitnet.cpp\nby up to 2.2x, and deliver up to 7x speedup compared to the 16-bit model\ninference. Our optimized runtime advances the state of LLM inference on AI PCs\nand edge devices, paving the way for efficient deployment of ultra-low-bit LLM\nmodels.", "AI": {"tldr": "本文通过为CPU优化1位和2位微内核，并将其集成到PyTorch-TPP框架中，显著提升了超低比特LLM模型在AI PC和边缘设备上的推理效率，最高可达现有SOTA运行时的2.2倍，并比16位模型推理快7倍。", "motivation": "超低比特LLM模型（1/1.58/2位）在性能上与全精度模型相当，为资源受限环境下的LLM推理带来了新机遇。然而，现有的SOTA推理运行时（如bitnet.cpp）在计算效率方面仍有待探索，未能充分发挥这些模型的潜力。", "method": "研究采用自下而上的方法。首先，为现代CPU设计并实现了优化的1位和2位微内核，以实现峰值计算效率。随后，将这些微内核集成到现有的LLM推理框架PyTorch-TPP中。", "result": "通过优化后的运行时，2位模型的端到端推理结果比当前的SOTA运行时bitnet.cpp快2.2倍，并且比16位模型推理速度提升了7倍。", "conclusion": "优化的运行时显著提升了LLM在AI PC和边缘设备上的推理性能，为超低比特LLM模型的有效部署铺平了道路。"}}
{"id": "2508.06722", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06722", "abs": "https://arxiv.org/abs/2508.06722", "authors": ["Justin London"], "title": "Improved Obstacle Avoidance for Autonomous Robots with ORCA-FLC", "comment": null, "summary": "Obstacle avoidance enables autonomous agents and robots to operate safely and\nefficiently in dynamic and complex environments, reducing the risk of\ncollisions and damage. For a robot or autonomous system to successfully\nnavigate through obstacles, it must be able to detect such obstacles. While\nnumerous collision avoidance algorithms like the dynamic window approach (DWA),\ntimed elastic bands (TEB), and reciprocal velocity obstacles (RVO) have been\nproposed, they may lead to suboptimal paths due to fixed weights, be\ncomputationally expensive, or have limited adaptability to dynamic obstacles in\nmulti-agent environments. Optimal reciprocal collision avoidance (ORCA), which\nimproves on RVO, provides smoother trajectories and stronger collision\navoidance guarantees. We propose ORCA-FL to improve on ORCA by using fuzzy\nlogic controllers (FLCs) to better handle uncertainty and imprecision for\nobstacle avoidance in path planning. Numerous multi-agent experiments are\nconducted and it is shown that ORCA-FL can outperform ORCA in reducing the\nnumber of collision if the agent has a velocity that exceeds a certain\nthreshold. In addition, a proposed algorithm for improving ORCA-FL using fuzzy\nQ reinforcement learning (FQL) is detailed for optimizing and tuning FLCs.", "AI": {"tldr": "本文提出ORCA-FL算法，通过模糊逻辑控制器改进ORCA算法，以提高多智能体在不确定环境中的避障能力，并探讨使用模糊Q学习进一步优化FLC。", "motivation": "现有的碰撞避免算法（如DWA, TEB, RVO）可能导致次优路径、计算成本高或对动态障碍物的适应性有限。ORCA虽有改进，但在处理不确定性和不精确性方面仍有提升空间。", "method": "提出ORCA-FL算法，通过引入模糊逻辑控制器（FLCs）来增强ORCA处理不确定性和不精确性的能力。此外，详细介绍了使用模糊Q强化学习（FQL）来优化和调整FLCs的方法。", "result": "多智能体实验表明，当智能体速度超过一定阈值时，ORCA-FL在减少碰撞数量方面优于ORCA。通过FQL可以进一步优化ORCA-FL中的FLCs。", "conclusion": "ORCA-FL通过结合模糊逻辑有效提升了ORCA在不确定环境中的避障性能，特别是在较高速度下。未来可利用模糊Q学习进一步优化该算法。"}}
{"id": "2508.06665", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06665", "abs": "https://arxiv.org/abs/2508.06665", "authors": ["Jonathan Shaw", "Dillon Mee", "Timothy Khouw", "Zackary Leech", "Daniel Wilson"], "title": "Testing the Limits of Machine Translation from One Book", "comment": null, "summary": "Current state-of-the-art models demonstrate capacity to leverage in-context\nlearning to translate into previously unseen language contexts. Tanzer et al.\n[2024] utilize language materials (e.g. a grammar) to improve translation\nquality for Kalamang using large language models (LLMs). We focus on Kanuri, a\nlanguage that, despite having substantial speaker population, has minimal\ndigital resources. We design two datasets for evaluation: one focused on health\nand humanitarian terms, and another containing generalized terminology,\ninvestigating how domain-specific tasks impact LLM translation quality.\n  By providing different combinations of language resources (grammar,\ndictionary, and parallel sentences), we measure LLM translation effectiveness,\ncomparing results to native speaker translations and human linguist\nperformance. We evaluate using both automatic metrics and native speaker\nassessments of fluency and accuracy.\n  Results demonstrate that parallel sentences remain the most effective data\nsource, outperforming other methods in human evaluations and automatic metrics.\nWhile incorporating grammar improves over zero-shot translation, it fails as an\neffective standalone data source. Human evaluations reveal that LLMs achieve\naccuracy (meaning) more effectively than fluency (grammaticality).\n  These findings suggest LLM translation evaluation benefits from\nmultidimensional assessment beyond simple accuracy metrics, and that grammar\nalone, without parallel sentences, does not provide sufficient context for\neffective domain-specific translation.", "AI": {"tldr": "研究探讨了大型语言模型（LLMs）在低资源语言（如卡努里语）翻译中的表现，发现平行语料库是最有效的数据源，而单独的语法规则不足以实现高质量翻译，LLM在准确性上优于流畅性。", "motivation": "尽管卡努里语有大量使用者，但其数字资源极少。现有研究表明LLM能利用上下文学习进行翻译，并有研究尝试使用语言材料（如语法）改进翻译质量。本研究旨在探究LLM在低资源语言（卡努里语）特定领域翻译中的表现，并评估不同语言资源组合的影响。", "method": "设计了两个卡努里语数据集：一个侧重健康和人道主义术语，另一个包含通用术语。通过提供语法、词典和平行句子等不同组合的语言资源，评估LLM的翻译效果。评估方法包括自动指标和母语使用者对翻译的流畅性和准确性评估，并将结果与母语使用者和人类语言学家进行比较。", "result": "平行句子是最有效的数据源，在人工评估和自动指标上均优于其他方法。纳入语法规则比零样本翻译有所改进，但单独使用语法无法作为有效的翻译数据源。人工评估显示LLM在准确性（意义）方面比流畅性（语法性）表现更好。", "conclusion": "LLM翻译评估应采用多维度评估，而非仅关注简单准确性指标。单独的语法规则，在没有平行语料库的情况下，不足以为有效的领域特定翻译提供足够上下文。"}}
{"id": "2508.06543", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06543", "abs": "https://arxiv.org/abs/2508.06543", "authors": ["Jinghan Yu", "Zhiyuan Ma", "Yue Ma", "Kaiqi Liu", "Yuhan Wang", "Jianjun Li"], "title": "MILD: Multi-Layer Diffusion Strategy for Complex and Precise Multi-IP Aware Human Erasing", "comment": null, "summary": "Recent years have witnessed the success of diffusion models in\nimage-customized tasks. Prior works have achieved notable progress on\nhuman-oriented erasing using explicit mask guidance and semantic-aware\ninpainting. However, they struggle under complex multi-IP scenarios involving\nhuman-human occlusions, human-object entanglements, and background\ninterferences. These challenges are mainly due to: 1) Dataset limitations, as\nexisting datasets rarely cover dense occlusions, camouflaged backgrounds, and\ndiverse interactions; 2) Lack of spatial decoupling, where foreground instances\ncannot be effectively disentangled, limiting clean background restoration. In\nthis work, we introduce a high-quality multi-IP human erasing dataset with\ndiverse pose variations and complex backgrounds. We then propose Multi-Layer\nDiffusion (MILD), a novel strategy that decomposes generation into semantically\nseparated pathways for each instance and the background. To enhance\nhuman-centric understanding, we introduce Human Morphology Guidance,\nintegrating pose, parsing, and spatial relations. We further present\nSpatially-Modulated Attention to better guide attention flow. Extensive\nexperiments show that MILD outperforms state-of-the-art methods on challenging\nhuman erasing benchmarks.", "AI": {"tldr": "本文针对复杂多人物场景下人体擦除的挑战，引入了一个高质量多人物人体擦除数据集，并提出了多层扩散（MILD）策略，通过语义分离的生成路径和人体形态引导等技术，显著提升了人体擦除性能。", "motivation": "现有扩散模型在人体定制任务中表现良好，但在复杂多人物场景（如人与人遮挡、人与物体纠缠、背景干扰）下表现不佳。主要原因包括：1) 数据集缺乏对密集遮挡、伪装背景和多样交互的覆盖；2) 缺乏空间解耦能力，导致前景实例无法有效分离，背景恢复不干净。", "method": "1. 构建了一个高质量多人物人体擦除数据集，包含多样姿态和复杂背景。2. 提出了多层扩散（MILD）策略，将生成分解为每个实例和背景的语义分离路径。3. 引入了人体形态引导（Human Morphology Guidance），整合姿态、解析和空间关系以增强以人为中心的理解。4. 提出了空间调制注意力（Spatially-Modulated Attention）以更好地引导注意力流。", "result": "大量实验表明，MILD在具有挑战性的人体擦除基准测试上优于现有最先进的方法。", "conclusion": "MILD方法通过结合新的数据集和创新的多层生成策略，有效解决了复杂场景下人体擦除的难题，显著提升了性能。"}}
{"id": "2508.07627", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.07627", "abs": "https://arxiv.org/abs/2508.07627", "authors": ["H Chan"], "title": "Nonlinear Systems in Wireless Power Transfer Applications", "comment": null, "summary": "As a novel pattern of energization, the wireless power transfer (WPT) offers\na brand-new way to the energy acquisition for electric-driven devices, thus\nalleviating the over-dependence on the battery. This report presents three\ntypes of WPT systems that use nonlinear control methods, in order to acquire an\nin-depth understanding of the course of Nonlinear Systems.", "AI": {"tldr": "该报告介绍了三种采用非线性控制方法的无线电力传输（WPT）系统，旨在深入理解非线性系统。", "motivation": "无线电力传输（WPT）为电动设备提供了一种新型能源获取方式，以减轻对电池的过度依赖。此外，该研究旨在通过WPT系统深入理解非线性系统。", "method": "介绍了三种使用非线性控制方法的WPT系统。", "result": "通过研究WPT系统，获得了对非线性系统课程的深入理解。", "conclusion": "该研究通过分析采用非线性控制方法的WPT系统，有效加深了对非线性系统的理解。"}}
{"id": "2508.06754", "categories": ["cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.06754", "abs": "https://arxiv.org/abs/2508.06754", "authors": ["Vanessa Figueiredo"], "title": "A Fuzzy Logic Prompting Framework for Large Language Models in Adaptive and Uncertain Tasks", "comment": null, "summary": "We introduce a modular prompting framework that supports safer and more\nadaptive use of large language models (LLMs) across dynamic, user-centered\ntasks. Grounded in human learning theory, particularly the Zone of Proximal\nDevelopment (ZPD), our method combines a natural language boundary prompt with\na control schema encoded with fuzzy scaffolding logic and adaptation rules.\nThis architecture enables LLMs to modulate behavior in response to user state\nwithout requiring fine-tuning or external orchestration. In a simulated\nintelligent tutoring setting, the framework improves scaffolding quality,\nadaptivity, and instructional alignment across multiple models, outperforming\nstandard prompting baselines. Evaluation is conducted using rubric-based LLM\ngraders at scale. While initially developed for education, the framework has\nshown promise in other interaction-heavy domains, such as procedural content\ngeneration for games. Designed for safe deployment, it provides a reusable\nmethodology for structuring interpretable, goal-aligned LLM behavior in\nuncertain or evolving contexts.", "AI": {"tldr": "本文提出一个模块化提示框架，基于近端发展区理论，结合自然语言边界提示和模糊支架逻辑，使大型语言模型在动态、以用户为中心的任务中能更安全、自适应地调整行为，无需微调，并在智能辅导等领域表现优异。", "motivation": "在动态、以用户为中心的环境中，大型语言模型（LLMs）需要更安全、更具适应性的使用方式，同时避免繁琐的微调或外部协调。研究受到人类学习理论，特别是近端发展区（ZPD）的启发。", "method": "引入一个模块化提示框架，其核心是结合自然语言边界提示和一个编码了模糊支架逻辑和适应规则的控制模式。这种架构使LLMs能够根据用户状态调整行为，无需进行微调或外部协调。", "result": "在模拟智能辅导环境中，该框架显著提升了支架质量、适应性和教学一致性，在多个模型上均优于标准提示基线。评估采用基于评分标准的LLM评分器进行大规模评测。此外，该框架在其他交互密集型领域（如游戏程序内容生成）也展现出潜力。", "conclusion": "该框架提供了一种可复用的方法论，用于在不确定或不断变化的环境中构建可解释、目标一致的LLM行为，设计上注重安全部署，最初用于教育领域，但具有广泛适用性。"}}
{"id": "2508.06742", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.06742", "abs": "https://arxiv.org/abs/2508.06742", "authors": ["Alejandro Murillo-Gonzalez", "Junhong Xu", "Lantao Liu"], "title": "Learning Causal Structure Distributions for Robust Planning", "comment": null, "summary": "Structural causal models describe how the components of a robotic system\ninteract. They provide both structural and functional information about the\nrelationships that are present in the system. The structural information\noutlines the variables among which there is interaction. The functional\ninformation describes how such interactions work, via equations or learned\nmodels. In this paper we find that learning the functional relationships while\naccounting for the uncertainty about the structural information leads to more\nrobust dynamics models which improves downstream planning, while using\nsignificantly lower computational resources. This in contrast with common\nmodel-learning methods that ignore the causal structure and fail to leverage\nthe sparsity of interactions in robotic systems. We achieve this by estimating\na causal structure distribution that is used to sample causal graphs that\ninform the latent-space representations in an encoder-multidecoder\nprobabilistic model. We show that our model can be used to learn the dynamics\nof a robot, which together with a sampling-based planner can be used to perform\nnew tasks in novel environments, provided an objective function for the new\nrequirement is available. We validate our method using manipulators and mobile\nrobots in both simulation and the real-world. Additionally, we validate the\nlearned dynamics' adaptability and increased robustness to corrupted inputs and\nchanges in the environment, which is highly desirable in challenging real-world\nrobotics scenarios. Video: https://youtu.be/X6k5t7OOnNc.", "AI": {"tldr": "本文提出一种方法，通过考虑因果结构信息的不确定性来学习机器人系统的功能关系，从而获得更鲁棒、计算效率更高的动力学模型，并提升下游规划性能。", "motivation": "常见的模型学习方法忽略了机器人系统中因果结构的存在及其交互的稀疏性，导致模型鲁棒性不足且计算资源消耗高。本研究旨在解决这些问题。", "method": "该方法通过估计因果结构分布，从中采样因果图，并利用这些图来指导编码器-多解码器概率模型中的潜在空间表示，从而学习功能关系。", "result": "研究结果表明，该方法学习到的动力学模型更具鲁棒性，显著降低了计算资源消耗，并改善了下游规划性能。该模型能够用于机器人的动力学学习，结合基于采样的规划器，可以在新环境中执行新任务。在模拟和真实世界的机械臂和移动机器人上进行了验证，证明了学习到的动力学的适应性和对损坏输入及环境变化的增强鲁棒性。", "conclusion": "在学习机器人系统的功能关系时，考虑因果结构信息的不确定性能够带来更鲁棒、更高效的动力学模型，从而显著提升机器人在复杂真实世界场景中的规划和适应能力。"}}
{"id": "2508.06671", "categories": ["cs.CL", "cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.06671", "abs": "https://arxiv.org/abs/2508.06671", "authors": ["Swati Rajwal", "Shivank Garg", "Reem Abdel-Salam", "Abdelrahman Zayed"], "title": "Do Biased Models Have Biased Thoughts?", "comment": "Accepted at main track of the Second Conference on Language Modeling\n  (COLM 2025)", "summary": "The impressive performance of language models is undeniable. However, the\npresence of biases based on gender, race, socio-economic status, physical\nappearance, and sexual orientation makes the deployment of language models\nchallenging. This paper studies the effect of chain-of-thought prompting, a\nrecent approach that studies the steps followed by the model before it\nresponds, on fairness. More specifically, we ask the following question:\n\\textit{Do biased models have biased thoughts}? To answer our question, we\nconduct experiments on $5$ popular large language models using fairness metrics\nto quantify $11$ different biases in the model's thoughts and output. Our\nresults show that the bias in the thinking steps is not highly correlated with\nthe output bias (less than $0.6$ correlation with a $p$-value smaller than\n$0.001$ in most cases). In other words, unlike human beings, the tested models\nwith biased decisions do not always possess biased thoughts.", "AI": {"tldr": "本文研究了链式思考（CoT）提示对大型语言模型公平性的影响，发现模型思维过程中的偏见与最终输出的偏见相关性不高，表明有偏见决策的模型不一定具有有偏见的思维。", "motivation": "尽管语言模型性能卓越，但其存在的性别、种族、社会经济地位、外貌和性取向等偏见，使其部署面临挑战。", "method": "研究CoT提示对公平性的影响，探究“有偏见的模型是否有偏见的思维”这一问题。通过对5个流行的大型语言模型进行实验，使用公平性指标量化模型思维和输出中的11种不同偏见。", "result": "研究结果显示，模型思维步骤中的偏见与输出偏见的相关性不高（在大多数情况下，相关性小于0.6，p值小于0.001）。", "conclusion": "与人类不同，经过测试的模型即使做出有偏见的决策，其思维过程也并非总是有偏见的。"}}
{"id": "2508.06551", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06551", "abs": "https://arxiv.org/abs/2508.06551", "authors": ["Ye Tao"], "title": "Slice or the Whole Pie? Utility Control for AI Models", "comment": null, "summary": "Training deep neural networks (DNNs) has become an increasingly\nresource-intensive task, requiring large volumes of labeled data, substantial\ncomputational power, and considerable fine-tuning efforts to achieve optimal\nperformance across diverse use cases. Although pre-trained models offer a\nuseful starting point, adapting them to meet specific user needs often demands\nextensive customization, and infrastructure overhead. This challenge grows when\na single model must support diverse appli-cations with differing requirements\nfor performance. Traditional solutions often involve training multiple model\nversions to meet varying requirements, which can be inefficient and difficult\nto maintain. In order to overcome this challenge, we propose NNObfuscator, a\nnovel utility control mechanism that enables AI models to dynamically modify\ntheir performance according to predefined conditions. It is different from\ntraditional methods that need separate models for each user. Instead,\nNNObfuscator allows a single model to be adapted in real time, giving you\ncontrolled access to multiple levels of performance. This mechanism enables\nmodel owners set up tiered access, ensuring that free-tier users receive a\nbaseline level of performance while premium users benefit from enhanced\ncapabilities. The approach improves resource allocation, reduces unnecessary\ncomputation, and supports sustainable business models in AI deployment. To\nvalidate our approach, we conducted experiments on multiple tasks, including\nimage classification, semantic segmentation, and text to image generation,\nusing well-established models such as ResNet, DeepLab, VGG16, FCN and Stable\nDiffusion. Experimental results show that NNObfuscator successfully makes model\nmore adaptable, so that a single trained model can handle a broad range of\ntasks without requiring a lot of changes.", "AI": {"tldr": "本文提出NNObfuscator，一种新颖的效用控制机制，使单个AI模型能够根据预定义条件动态调整其性能，从而实现分层访问并提高资源效率。", "motivation": "深度神经网络训练资源密集，预训练模型适应特定需求成本高昂，且传统方法需要训练多个模型版本来满足不同性能要求，效率低下且难以维护。", "method": "提出NNObfuscator，一种效用控制机制，允许AI模型实时动态修改其性能。它不同于为每个用户单独训练模型，而是使单个模型能够提供多级受控性能，从而实现分层访问（如免费用户基础性能，高级用户增强能力）。", "result": "实验在图像分类、语义分割和文本到图像生成等任务上，使用ResNet、DeepLab、VGG16、FCN和Stable Diffusion等模型进行验证。结果表明NNObfuscator成功提高了模型的适应性，使单个训练模型无需大量修改即可处理广泛任务，同时优化了资源分配，减少了不必要的计算，并支持可持续的AI部署商业模式。", "conclusion": "NNObfuscator提供了一种创新的方法，使AI模型能够从单个模型动态调整性能，从而提高了效率、资源利用率，并支持AI部署的可持续商业模型。"}}
{"id": "2508.07684", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.07684", "abs": "https://arxiv.org/abs/2508.07684", "authors": ["Jason J. Choi", "Claire J. Tomlin", "Shankar Sastry", "Koushil Sreenath"], "title": "When are safety filters safe? On minimum phase conditions of control barrier functions", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "In emerging control applications involving multiple and complex tasks, safety\nfilters are gaining prominence as a modular approach to enforcing safety\nconstraints. Among various methods, control barrier functions (CBFs) are widely\nused for designing safety filters due to their simplicity, imposing a single\nlinear constraint on the control input at each state. In this work, we focus on\nthe internal dynamics of systems governed by CBF-constrained control laws. Our\nkey observation is that, although CBFs guarantee safety by enforcing state\nconstraints, they can inadvertently be \"unsafe\" by causing the internal state\nto diverge. We investigate the conditions under which the full system state,\nincluding the internal state, can remain bounded under a CBF-based safety\nfilter. Drawing inspiration from the input-output linearization literature,\nwhere boundedness is ensured by minimum phase conditions, we propose a new set\nof CBF minimum phase conditions tailored to the structure imposed by the CBF\nconstraint. A critical distinction from the original minimum phase conditions\nis that the internal dynamics in our setting is driven by a nonnegative virtual\ncontrol input, which reflects the enforcement of the safety constraint. We\ninclude a range of numerical examples, including single-input, multi-input,\nlinear, and nonlinear systems, validating both our analysis and the necessity\nof the proposed CBF minimum phase conditions.", "AI": {"tldr": "CBF安全滤波器虽能保证外部安全，但可能导致内部状态发散。本文提出CBF最小相位条件以确保整个系统状态（包括内部状态）的有界性。", "motivation": "安全滤波器在复杂控制应用中日益重要，其中控制障碍函数（CBF）因其简单性被广泛使用。然而，尽管CBF能强制执行状态约束以保证“安全”，它们却可能无意中导致系统内部状态发散，从而变得“不安全”。因此，需要研究在CBF约束下如何确保整个系统状态（包括内部状态）的有界性。", "method": "本文借鉴输入-输出线性化理论中的最小相位条件，提出了一套新的、专门针对CBF约束结构定制的CBF最小相位条件。与传统最小相位条件的关键区别在于，本设定中的内部动力学是由一个反映安全约束强制执行的非负虚拟控制输入驱动的。", "result": "通过一系列数值示例（包括单输入、多输入、线性和非线性系统）验证了所提出的CBF最小相位条件能够确保在CBF安全滤波器作用下，包括内部状态在内的整个系统状态保持有界。同时，这些示例也证实了所提出的CBF最小相位条件的必要性。", "conclusion": "CBF虽然能有效实施安全约束，但为了确保在CBF安全滤波器下整个系统状态（包括内部状态）的有界性，所提出的CBF最小相位条件是必要且有效的。"}}
{"id": "2508.06823", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06823", "abs": "https://arxiv.org/abs/2508.06823", "authors": ["Xuan Zhao", "Jun Tao"], "title": "Natural Language-Driven Viewpoint Navigation for Volume Exploration via Semantic Block Representation", "comment": "Accepted by IEEE VIS 2025", "summary": "Exploring volumetric data is crucial for interpreting scientific datasets.\nHowever, selecting optimal viewpoints for effective navigation can be\nchallenging, particularly for users without extensive domain expertise or\nfamiliarity with 3D navigation. In this paper, we propose a novel framework\nthat leverages natural language interaction to enhance volumetric data\nexploration. Our approach encodes volumetric blocks to capture and\ndifferentiate underlying structures. It further incorporates a CLIP Score\nmechanism, which provides semantic information to the blocks to guide\nnavigation. The navigation is empowered by a reinforcement learning framework\nthat leverage these semantic cues to efficiently search for and identify\ndesired viewpoints that align with the user's intent. The selected viewpoints\nare evaluated using CLIP Score to ensure that they best reflect the user\nqueries. By automating viewpoint selection, our method improves the efficiency\nof volumetric data navigation and enhances the interpretability of complex\nscientific phenomena.", "AI": {"tldr": "本文提出一个利用自然语言交互的框架，通过编码体数据块和结合CLIP Score与强化学习，自动选择最佳视角，从而提升体数据探索效率和可解释性。", "motivation": "探索体数据对于科学数据集的解释至关重要，但选择最佳视角极具挑战性，特别是对于缺乏领域专业知识或3D导航经验的用户。", "method": "该方法首先编码体数据块以区分底层结构；然后引入CLIP Score为数据块提供语义信息以指导导航；接着利用强化学习框架，结合语义线索高效搜索并识别符合用户意图的期望视角；最后使用CLIP Score评估选定的视角，以确保其最佳反映用户查询。", "result": "通过自动化视角选择，该方法提高了体数据导航的效率，并增强了复杂科学现象的可解释性。", "conclusion": "该框架通过结合自然语言交互、语义信息和强化学习，成功解决了体数据探索中视角选择的难题，显著提升了用户体验和数据理解能力。"}}
{"id": "2508.06744", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.06744", "abs": "https://arxiv.org/abs/2508.06744", "authors": ["Yunke Ao", "Manish Prajapat", "Yarden As", "Yassine Taoudi-Benchekroun", "Fabio Carrillo", "Hooman Esfandiari", "Benjamin F. Grewe", "Andreas Krause", "Philipp Fürnstahl"], "title": "Robust-Sub-Gaussian Model Predictive Control for Safe Ultrasound-Image-Guided Robotic Spinal Surgery", "comment": null, "summary": "Safety-critical control using high-dimensional sensory feedback from optical\ndata (e.g., images, point clouds) poses significant challenges in domains like\nautonomous driving and robotic surgery. Control can rely on low-dimensional\nstates estimated from high-dimensional data. However, the estimation errors\noften follow complex, unknown distributions that standard probabilistic models\nfail to capture, making formal safety guarantees challenging. In this work, we\nintroduce a novel characterization of these general estimation errors using\nsub-Gaussian noise with bounded mean. We develop a new technique for\nuncertainty propagation of proposed noise characterization in linear systems,\nwhich combines robust set-based methods with the propagation of sub-Gaussian\nvariance proxies. We further develop a Model Predictive Control (MPC) framework\nthat provides closed-loop safety guarantees for linear systems under the\nproposed noise assumption. We apply this MPC approach in an\nultrasound-image-guided robotic spinal surgery pipeline, which contains\ndeep-learning-based semantic segmentation, image-based registration, high-level\noptimization-based planning, and low-level robotic control. To validate the\npipeline, we developed a realistic simulation environment integrating real\nhuman anatomy, robot dynamics, efficient ultrasound simulation, as well as\nin-vivo data of breathing motion and drilling force. Evaluation results in\nsimulation demonstrate the potential of our approach for solving complex\nimage-guided robotic surgery task while ensuring safety.", "AI": {"tldr": "本文提出了一种利用次高斯噪声表征高维传感器数据估计误差的方法，并结合模型预测控制（MPC）为线性系统提供安全保障，应用于超声图像引导的机器人脊柱手术。", "motivation": "在自动驾驶和机器人手术等安全关键领域，使用高维传感器数据（如图像、点云）进行控制面临挑战。从高维数据估计的低维状态，其估计误差分布复杂且未知，使得难以提供形式化的安全保证。", "method": "研究引入了一种新的方法，使用均值有界的次高斯噪声来表征通用估计误差。开发了一种新的不确定性传播技术，结合了鲁棒基于集合的方法和次高斯方差代理的传播。在此基础上，开发了一个模型预测控制（MPC）框架，为在所提噪声假设下的线性系统提供闭环安全保证。该方法被应用于一个超声图像引导的机器人脊柱手术管线，该管线包含深度学习语义分割、基于图像的配准、高层优化规划和低层机器人控制。为验证该管线，开发了一个逼真的模拟环境。", "result": "模拟评估结果表明，所提出的方法在解决复杂的图像引导机器人手术任务同时确保安全方面具有潜力。", "conclusion": "该研究提出的方法，通过对高维传感器数据估计误差的次高斯噪声表征和基于MPC的安全控制框架，在图像引导机器人手术等安全关键应用中展现出有效性和潜力，能够确保复杂任务的安全性。"}}
{"id": "2508.06709", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06709", "abs": "https://arxiv.org/abs/2508.06709", "authors": ["Evangelia Spiliopoulou", "Riccardo Fogliato", "Hanna Burnsky", "Tamer Soliman", "Jie Ma", "Graham Horwood", "Miguel Ballesteros"], "title": "Play Favorites: A Statistical Method to Measure Self-Bias in LLM-as-a-Judge", "comment": null, "summary": "Large language models (LLMs) can serve as judges that offer rapid and\nreliable assessments of other LLM outputs. However, models may systematically\nassign overly favorable ratings to their own outputs, a phenomenon known as\nself-bias, which can distort evaluations of true model performance. Previous\nstudies often conflate genuine differences in model quality with bias or\nincorrectly assume that evaluations from LLMs and humans follow the same rating\ndistributions. In this work, we present a statistical framework that explicitly\nformalizes assumptions under which self-bias can be identified and estimated.\nOur method models the difference in the scoring distribution that\nLLM-as-a-judge assigns to its own completions compared to other models, while\naccounting for the underlying quality of the completions provided by an\nindependent, third-party judge (e.g., humans). Our method reliably isolates and\nquantifies self-bias, even when models vary in ability, ensuring that genuine\nperformance differences are not mistaken for self-bias. We conduct an empirical\nanalysis of self-bias on a large dataset (>5000 prompt-completion pairs)\nconsisting of expert human annotations and judgments from nine different LLM\njudges. We find that some models, such as GPT-4o and Claude 3.5 Sonnet,\nsystematically assign higher scores to their own outputs. These models also\ndisplay family-bias; systematically assigning higher ratings to outputs\nproduced by other models of the same family. Our findings highlight potential\npitfalls of using LLM judges and offer practical guidance to mitigate biases\nwhen interpreting automated evaluations.", "AI": {"tldr": "本文提出了一个统计框架，用于识别和量化大型语言模型（LLM）作为评判员时，对其自身或同系列模型输出进行评估时产生的“自我偏见”和“家族偏见”，并通过实证分析验证了其存在。", "motivation": "LLM作为评判员能够快速可靠地评估其他LLM的输出，但存在“自我偏见”现象，即模型可能系统性地对其自身输出给出过高评价，这会扭曲真实的模型性能评估。以往研究常将模型质量差异与偏见混淆，或错误假设LLM和人类评估遵循相同的评分分布。", "method": "本研究提出了一个统计框架，明确形式化了识别和估计自我偏见的假设。该方法通过建模LLM作为评判员时对其自身完成度评分与对其他模型评分之间的分布差异，同时考虑了由独立的第三方评判员（如人类）提供的完成度的内在质量。此外，通过对包含5000多个提示-完成对的大型数据集进行实证分析，该数据集包含专家人类标注和九种不同LLM评判员的判断。", "result": "研究发现，某些模型（如GPT-4o和Claude 3.5 Sonnet）系统性地对其自身输出给予更高的分数，表现出“自我偏见”。这些模型也表现出“家族偏见”，即系统性地对其同系列其他模型产生的输出给予更高的评分。", "conclusion": "研究结果强调了使用LLM作为评判员的潜在陷阱，并为在解释自动化评估结果时减轻偏见提供了实用指导。"}}
{"id": "2508.06552", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06552", "abs": "https://arxiv.org/abs/2508.06552", "authors": ["Unisha Joshi"], "title": "Age-Diverse Deepfake Dataset: Bridging the Age Gap in Deepfake Detection", "comment": "11 pages, 4 figures, and 7 tables", "summary": "The challenges associated with deepfake detection are increasing\nsignificantly with the latest advancements in technology and the growing\npopularity of deepfake videos and images. Despite the presence of numerous\ndetection models, demographic bias in the deepfake dataset remains largely\nunaddressed. This paper focuses on the mitigation of age-specific bias in the\ndeepfake dataset by introducing an age-diverse deepfake dataset that will\nimprove fairness across age groups. The dataset is constructed through a\nmodular pipeline incorporating the existing deepfake datasets Celeb-DF,\nFaceForensics++, and UTKFace datasets, and the creation of synthetic data to\nfill the age distribution gaps. The effectiveness and generalizability of this\ndataset are evaluated using three deepfake detection models: XceptionNet,\nEfficientNet, and LipForensics. Evaluation metrics, including AUC, pAUC, and\nEER, revealed that models trained on the age-diverse dataset demonstrated\nfairer performance across age groups, improved overall accuracy, and higher\ngeneralization across datasets. This study contributes a reproducible,\nfairness-aware deepfake dataset and model pipeline that can serve as a\nfoundation for future research in fairer deepfake detection. The complete\ndataset and implementation code are available at\nhttps://github.com/unishajoshi/age-diverse-deepfake-detection.", "AI": {"tldr": "本文提出一个年龄多样化的深度伪造数据集，以减轻现有深度伪造检测模型中的年龄偏见，提高跨年龄组的公平性和检测性能。", "motivation": "随着深度伪造技术的发展和流行，深度伪造检测面临的挑战日益增加。尽管存在众多检测模型，但深度伪造数据集中普遍存在的群体（特别是年龄）偏见问题仍未得到有效解决。", "method": "研究通过构建一个模块化管道，整合现有数据集（Celeb-DF、FaceForensics++、UTKFace）并生成合成数据来填补年龄分布空白，从而创建了一个年龄多样化的深度伪造数据集。使用XceptionNet、EfficientNet和LipForensics三种深度伪造检测模型，通过AUC、pAUC和EER等评估指标，验证了该数据集的有效性和泛化能力。", "result": "在年龄多样化数据集上训练的模型，在不同年龄组之间表现出更公平的性能，提高了整体准确性，并展现出更强的跨数据集泛化能力。", "conclusion": "本研究贡献了一个可复现的、关注公平性的深度伪造数据集和模型管道，为未来更公平的深度伪造检测研究奠定了基础。"}}
{"id": "2508.07693", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.07693", "abs": "https://arxiv.org/abs/2508.07693", "authors": ["Noboru Katayama"], "title": "Deep Reinforcement Learning-Based Control Strategy with Direct Gate Control for Buck Converters", "comment": null, "summary": "This paper proposes a deep reinforcement learning (DRL)-based approach for\ndirectly controlling the gate signals of switching devices to achieve voltage\nregulation in a buck converter. Unlike conventional control methods, the\nproposed method directly generates gate signals using a neural network trained\nthrough DRL, with the objective of achieving high control speed and flexibility\nwhile maintaining stability. Simulation results demonstrate that the proposed\ndirect gate control (DGC) method achieves a faster transient response and\nstable output voltage regulation, outperforming traditional PWM-based control\nschemes. The DGC method also exhibits strong robustness against parameter\nvariations and sensor noise, indicating its suitability for practical power\nelectronics applications. The effectiveness of the proposed approach is\nvalidated via simulation.", "AI": {"tldr": "本文提出了一种基于深度强化学习（DRL）的直接门控控制（DGC）方法，用于降压变换器中开关器件的门信号控制，以实现快速、稳定的电压调节。", "motivation": "传统的控制方法在控制速度和灵活性方面存在不足，需要一种新的方法来直接生成门信号，以实现更快的瞬态响应、更高的灵活性和稳定性。", "method": "通过深度强化学习（DRL）训练神经网络，直接生成开关器件的门信号，以实现降压变换器的电压调节。该方法被称为直接门控控制（DGC）。", "result": "仿真结果表明，所提出的DGC方法实现了更快的瞬态响应和稳定的输出电压调节，优于传统的基于PWM的控制方案。DGC方法还对参数变化和传感器噪声表现出强大的鲁棒性。", "conclusion": "所提出的基于DRL的直接门控控制方法是有效的，并且由于其高速、稳定性和鲁棒性，适用于实际的电力电子应用。"}}
{"id": "2508.06832", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06832", "abs": "https://arxiv.org/abs/2508.06832", "authors": ["Haifeng Li", "Wang Guo", "Haiyang Wu", "Mengwei Wu", "Jipeng Zhang", "Qing Zhu", "Yu Liu", "Xin Huang", "Chao Tao"], "title": "Remote Sensing Image Intelligent Interpretation with the Language-Centered Perspective: Principles, Methods and Challenges", "comment": null, "summary": "The mainstream paradigm of remote sensing image interpretation has long been\ndominated by vision-centered models, which rely on visual features for semantic\nunderstanding. However, these models face inherent limitations in handling\nmulti-modal reasoning, semantic abstraction, and interactive decision-making.\nWhile recent advances have introduced Large Language Models (LLMs) into remote\nsensing workflows, existing studies primarily focus on downstream applications,\nlacking a unified theoretical framework that explains the cognitive role of\nlanguage. This review advocates a paradigm shift from vision-centered to\nlanguage-centered remote sensing interpretation. Drawing inspiration from the\nGlobal Workspace Theory (GWT) of human cognition, We propose a\nlanguage-centered framework for remote sensing interpretation that treats LLMs\nas the cognitive central hub integrating perceptual, task, knowledge and action\nspaces to enable unified understanding, reasoning, and decision-making. We\nfirst explore the potential of LLMs as the central cognitive component in\nremote sensing interpretation, and then summarize core technical challenges,\nincluding unified multimodal representation, knowledge association, and\nreasoning and decision-making. Furthermore, we construct a global\nworkspace-driven interpretation mechanism and review how language-centered\nsolutions address each challenge. Finally, we outline future research\ndirections from four perspectives: adaptive alignment of multimodal data, task\nunderstanding under dynamic knowledge constraints, trustworthy reasoning, and\nautonomous interaction. This work aims to provide a conceptual foundation for\nthe next generation of remote sensing interpretation systems and establish a\nroadmap toward cognition-driven intelligent geospatial analysis.", "AI": {"tldr": "该综述提出将遥感图像解译范式从以视觉为中心转变为以语言为中心，借鉴人类认知全局工作空间理论（GWT），将大语言模型（LLMs）作为认知中枢，以实现统一的理解、推理和决策。", "motivation": "当前遥感图像解译主流范式以视觉模型为主，在处理多模态推理、语义抽象和交互决策方面存在固有限制。尽管LLMs已引入遥感工作流，但现有研究主要集中于下游应用，缺乏解释语言认知作用的统一理论框架。", "method": "受人类认知全局工作空间理论（GWT）启发，提出一个语言中心化的遥感解译框架，将LLMs视为整合感知、任务、知识和行动空间的认知中枢。探讨了LLMs作为核心认知组件的潜力，总结了核心技术挑战（统一多模态表示、知识关联、推理与决策），并审视了语言中心化解决方案如何应对这些挑战。最后，从多模态数据自适应对齐、动态知识约束下的任务理解、可信推理和自主交互四个角度展望了未来研究方向。", "result": "提出了一个语言中心化的遥感解译框架，该框架将LLMs作为认知中枢，整合多模态信息；识别并总结了统一多模态表示、知识关联、推理与决策等核心技术挑战；构建了一个全局工作空间驱动的解译机制，并概述了未来的研究方向，旨在为下一代遥感解译系统提供概念基础和路线图。", "conclusion": "本工作为下一代遥感解译系统提供了概念基础，并为实现认知驱动的智能地理空间分析建立了研究路线图。"}}
{"id": "2508.06779", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06779", "abs": "https://arxiv.org/abs/2508.06779", "authors": ["Minku Kim", "Brian Acosta", "Pratik Chaudhari", "Michael Posa"], "title": "Learning a Vision-Based Footstep Planner for Hierarchical Walking Control", "comment": "8 pages, 8 figures, accepted to 2025 IEEE-RAS 24th International\n  Conference on Humanoid Robots", "summary": "Bipedal robots demonstrate potential in navigating challenging terrains\nthrough dynamic ground contact. However, current frameworks often depend solely\non proprioception or use manually designed visual pipelines, which are fragile\nin real-world settings and complicate real-time footstep planning in\nunstructured environments. To address this problem, we present a vision-based\nhierarchical control framework that integrates a reinforcement learning\nhigh-level footstep planner, which generates footstep commands based on a local\nelevation map, with a low-level Operational Space Controller that tracks the\ngenerated trajectories. We utilize the Angular Momentum Linear Inverted\nPendulum model to construct a low-dimensional state representation to capture\nan informative encoding of the dynamics while reducing complexity. We evaluate\nour method across different terrain conditions using the underactuated bipedal\nrobot Cassie and investigate the capabilities and challenges of our approach\nthrough simulation and hardware experiments.", "AI": {"tldr": "提出了一种基于视觉的分层控制框架，结合强化学习高层落脚点规划和低层操作空间控制器，使双足机器人在复杂地形中能进行鲁棒的实时落脚点规划。", "motivation": "现有双足机器人框架过度依赖本体感知或手动设计的视觉管道，导致在真实世界中脆弱且难以在非结构化环境中进行实时落脚点规划。", "method": "开发了一种基于视觉的分层控制框架。高层采用强化学习落脚点规划器，根据局部高程图生成落脚点指令；低层采用操作空间控制器跟踪生成轨迹。同时，利用角动量线性倒立摆模型构建低维状态表示，以捕获动力学信息并降低复杂性。", "result": "使用欠驱动双足机器人Cassie在不同地形条件下进行了仿真和硬件实验，评估了该方法的性能，并探讨了其能力和挑战。", "conclusion": "该视觉分层控制框架有效解决了双足机器人在复杂非结构化环境中实时落脚点规划的难题，并通过仿真和硬件实验验证了其可行性和鲁棒性。"}}
{"id": "2508.06729", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06729", "abs": "https://arxiv.org/abs/2508.06729", "authors": ["Komala Subramanyam Cherukuri", "Pranav Abishai Moses", "Aisa Sakata", "Jiangping Chen", "Haihua Chen"], "title": "Large Language Models for Oral History Understanding with Text Classification and Sentiment Analysis", "comment": null, "summary": "Oral histories are vital records of lived experience, particularly within\ncommunities affected by systemic injustice and historical erasure. Effective\nand efficient analysis of their oral history archives can promote access and\nunderstanding of the oral histories. However, Large-scale analysis of these\narchives remains limited due to their unstructured format, emotional\ncomplexity, and high annotation costs. This paper presents a scalable framework\nto automate semantic and sentiment annotation for Japanese American\nIncarceration Oral History. Using LLMs, we construct a high-quality dataset,\nevaluate multiple models, and test prompt engineering strategies in\nhistorically sensitive contexts. Our multiphase approach combines expert\nannotation, prompt design, and LLM evaluation with ChatGPT, Llama, and Qwen. We\nlabeled 558 sentences from 15 narrators for sentiment and semantic\nclassification, then evaluated zero-shot, few-shot, and RAG strategies. For\nsemantic classification, ChatGPT achieved the highest F1 score (88.71%),\nfollowed by Llama (84.99%) and Qwen (83.72%). For sentiment analysis, Llama\nslightly outperformed Qwen (82.66%) and ChatGPT (82.29%), with all models\nshowing comparable results. The best prompt configurations were used to\nannotate 92,191 sentences from 1,002 interviews in the JAIOH collection. Our\nfindings show that LLMs can effectively perform semantic and sentiment\nannotation across large oral history collections when guided by well-designed\nprompts. This study provides a reusable annotation pipeline and practical\nguidance for applying LLMs in culturally sensitive archival analysis. By\nbridging archival ethics with scalable NLP techniques, this work lays the\ngroundwork for responsible use of artificial intelligence in digital humanities\nand preservation of collective memory. GitHub:\nhttps://github.com/kc6699c/LLM4OralHistoryAnalysis.", "AI": {"tldr": "本研究提出了一个可扩展的框架，利用大型语言模型（LLMs）自动化日本裔美国人拘禁口述历史档案的语义和情感标注，克服了传统分析的挑战，并提供了在文化敏感语境下应用LLMs的实用指导。", "motivation": "口述历史是重要的人生经历记录，尤其对受系统性不公和历史抹杀影响的社区。然而，由于其非结构化格式、情感复杂性和高昂的标注成本，大规模分析口述历史档案的能力受到限制，这阻碍了对这些历史的理解和访问。", "method": "本研究采用多阶段方法，结合专家标注、提示工程设计和LLM（ChatGPT、Llama、Qwen）评估。首先，构建了一个高质量数据集，对来自15位讲述者的558个句子进行情感和语义分类标注。然后，评估了零样本、少样本和RAG（检索增强生成）策略。最后，使用最佳提示配置对JAIOH（日本裔美国人拘禁口述历史）集合中1002次访谈的92,191个句子进行了标注。", "result": "在语义分类方面，ChatGPT的F1分数最高（88.71%），其次是Llama（84.99%）和Qwen（83.72%）。在情感分析方面，Llama略优于Qwen（82.66%）和ChatGPT（82.29%），所有模型表现相当。研究结果表明，在精心设计的提示引导下，LLMs能够有效地对大规模口述历史档案进行语义和情感标注。", "conclusion": "研究证明了LLMs在处理大规模口述历史档案方面进行语义和情感标注的有效性，前提是需要有良好设计的提示。本研究提供了一个可重用的标注流程和在文化敏感档案分析中应用LLMs的实用指导，为数字人文领域和集体记忆保护中负责任地使用人工智能奠定了基础。"}}
{"id": "2508.06553", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06553", "abs": "https://arxiv.org/abs/2508.06553", "authors": ["Jiahao Xiao", "Jianbo Zhang", "BoWen Yan", "Shengyu Guo", "Tongrui Ye", "Kaiwei Zhang", "Zicheng Zhang", "Xiaohong Liu", "Zhengxue Cheng", "Lei Fan", "Chuyi Li", "Guangtao Zhai"], "title": "Static and Plugged: Make Embodied Evaluation Simple", "comment": null, "summary": "Embodied intelligence is advancing rapidly, driving the need for efficient\nevaluation. Current benchmarks typically rely on interactive simulated\nenvironments or real-world setups, which are costly, fragmented, and hard to\nscale. To address this, we introduce StaticEmbodiedBench, a plug-and-play\nbenchmark that enables unified evaluation using static scene representations.\nCovering 42 diverse scenarios and 8 core dimensions, it supports scalable and\ncomprehensive assessment through a simple interface. Furthermore, we evaluate\n19 Vision-Language Models (VLMs) and 11 Vision-Language-Action models (VLAs),\nestablishing the first unified static leaderboard for Embodied intelligence.\nMoreover, we release a subset of 200 samples from our benchmark to accelerate\nthe development of embodied intelligence.", "AI": {"tldr": "提出StaticEmbodiedBench，一个基于静态场景表示的即插即用基准，用于统一、可扩展地评估具身智能，并建立了首个统一静态排行榜。", "motivation": "当前具身智能评估主要依赖交互式模拟环境或真实世界设置，这些方法成本高、碎片化且难以扩展，因此需要更高效的评估方案。", "method": "引入StaticEmbodiedBench，一个使用静态场景表示的即插即用基准。它涵盖42种不同场景和8个核心维度，通过简单接口支持可扩展和全面的评估。此外，评估了19个视觉-语言模型（VLM）和11个视觉-语言-动作模型（VLA），并发布了200个样本的子集。", "result": "通过评估19个VLM和11个VLA，建立了具身智能领域的首个统一静态排行榜。", "conclusion": "StaticEmbodiedBench提供了一个高效、可扩展且统一的具身智能评估框架，有望加速该领域的发展。"}}
{"id": "2508.07749", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.07749", "abs": "https://arxiv.org/abs/2508.07749", "authors": ["Shurui Guan", "Keqiang Li", "Haoyu Yang", "Yihe Chen", "Hanxiao Ren", "Yugong Luo"], "title": "Robust Integrated Priority and Speed Control based on Hierarchical Stochastic Optimization to Promote Bus Schedule Adherence along Signalized Arterial", "comment": "This paper has been accepted by 26th IEEE International Conference on\n  Intelligent Transportation Systems ITSC 2025", "summary": "In intelligent transportation systems (ITS), adaptive transit signal priority\n(TSP) and dynamic bus control systems have been independently developed to\nmaintain efficient and reliable urban bus services. However, those two systems\ncould potentially lead to conflicting decisions due to the lack of\ncoordination. Although some studies explore the integrated control strategies\nalong the arterial, they merely rely on signal replanning to address system\nuncertainties. Therefore, their performance severely deteriorates in real-world\nintersection settings, where abrupt signal timing variation is not always\napplicable in consideration of countdown timers and pedestrian signal design.\n  In this study, we propose a robust integrated priority and speed control\nstrategy based on hierarchical stochastic optimization to enhance bus schedule\nadherence along the arterial. In the proposed framework, the upper level\nensures the coordination across intersections while the lower level handles\nuncertainties for each intersection with stochastic programming. Hence, the\nroute-level system randomness is decomposed into a series of local problems\nthat can be solved in parallel using sample average approximation (SAA).\nSimulation experiments are conducted under various scenarios with stochastic\nbus dwell time and different traffic demand. The results demonstrate that our\napproach significantly enhances bus punctuality and time headway equivalence\nwithout abrupt signal timing variation, with negative impacts on car delays\nlimited to only 0.8%-5.2% as traffic demand increases.", "AI": {"tldr": "本文提出一种基于分层随机优化的鲁棒集成公交优先和速度控制策略，以提高城市公交沿线准点率，同时避免信号灯的剧烈变化。", "motivation": "现有的自适应公交信号优先（TSP）和动态公交控制系统独立开发，可能导致冲突决策。尽管一些集成控制策略已在主干道上探索，但它们仅依赖信号重规划来应对不确定性，这在考虑倒计时器和行人信号设计时，在实际交叉口设置中表现不佳，因为突然的信号配时变化并不总是适用。", "method": "提出一种基于分层随机优化的鲁棒集成优先和速度控制策略。上层确保交叉口间的协调，下层利用随机规划处理每个交叉口的不确定性。通过样本平均近似（SAA）将线路级系统随机性分解为一系列可并行求解的局部问题。", "result": "仿真实验表明，该方法在不引起信号配时剧烈变化的情况下，显著提高了公交准点率和时间间隔等效性，且对小汽车延误的负面影响有限（随交通需求增加仅为0.8%-5.2%）。", "conclusion": "所提出的鲁棒集成策略能够有效提升公交服务可靠性（准点率、车头时距），同时将对社会车辆的影响降到最低，并克服了传统方法中信号剧烈变化的局限性。"}}
{"id": "2508.06836", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06836", "abs": "https://arxiv.org/abs/2508.06836", "authors": ["Xutong Zhao", "Yaqi Xie"], "title": "Multi-level Advantage Credit Assignment for Cooperative Multi-Agent Reinforcement Learning", "comment": "Accepted at AISTATS 2025", "summary": "Cooperative multi-agent reinforcement learning (MARL) aims to coordinate\nmultiple agents to achieve a common goal. A key challenge in MARL is credit\nassignment, which involves assessing each agent's contribution to the shared\nreward. Given the diversity of tasks, agents may perform different types of\ncoordination, with rewards attributed to diverse and often overlapping agent\nsubsets. In this work, we formalize the credit assignment level as the number\nof agents cooperating to obtain a reward, and address scenarios with multiple\ncoexisting levels. We introduce a multi-level advantage formulation that\nperforms explicit counterfactual reasoning to infer credits across distinct\nlevels. Our method, Multi-level Advantage Credit Assignment (MACA), captures\nagent contributions at multiple levels by integrating advantage functions that\nreason about individual, joint, and correlated actions. Utilizing an\nattention-based framework, MACA identifies correlated agent relationships and\nconstructs multi-level advantages to guide policy learning. Comprehensive\nexperiments on challenging Starcraft v1\\&v2 tasks demonstrate MACA's superior\nperformance, underscoring its efficacy in complex credit assignment scenarios.", "AI": {"tldr": "该论文提出了一种名为MACA的多级优势信用分配方法，通过形式化信用分配级别并利用多级优势函数和注意力机制，有效解决了多智能体强化学习中复杂的信用分配问题。", "motivation": "多智能体强化学习（MARL）中的关键挑战是信用分配，即评估每个智能体对共享奖励的贡献。面对多样化的任务，智能体可能执行不同类型的协作，奖励归因于多样且通常重叠的智能体子集。这促使研究者需要形式化并处理多个共存的协作级别。", "method": "该研究将信用分配级别定义为获得奖励所需的协作智能体数量，并处理多级别共存的场景。引入了一种多级优势公式，通过显式反事实推理来推断不同级别的信用。其方法MACA通过整合推理个体、联合和相关动作的优势函数，捕捉多个级别的智能体贡献。利用基于注意力的框架，MACA识别相关的智能体关系并构建多级优势以指导策略学习。", "result": "在具有挑战性的星际争霸v1和v2任务上的综合实验表明，MACA表现出卓越的性能，突显了其在复杂信用分配场景中的有效性。", "conclusion": "MACA方法通过处理多级别信用分配问题，在复杂的多智能体强化学习任务中展现出优越的性能和有效性。"}}
{"id": "2508.06804", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06804", "abs": "https://arxiv.org/abs/2508.06804", "authors": ["Shu-Ang Yu", "Feng Gao", "Yi Wu", "Chao Yu", "Yu Wang"], "title": "D3P: Dynamic Denoising Diffusion Policy via Reinforcement Learning", "comment": null, "summary": "Diffusion policies excel at learning complex action distributions for robotic\nvisuomotor tasks, yet their iterative denoising process poses a major\nbottleneck for real-time deployment. Existing acceleration methods apply a\nfixed number of denoising steps per action, implicitly treating all actions as\nequally important. However, our experiments reveal that robotic tasks often\ncontain a mix of \\emph{crucial} and \\emph{routine} actions, which differ in\ntheir impact on task success. Motivated by this finding, we propose\n\\textbf{D}ynamic \\textbf{D}enoising \\textbf{D}iffusion \\textbf{P}olicy\n\\textbf{(D3P)}, a diffusion-based policy that adaptively allocates denoising\nsteps across actions at test time. D3P uses a lightweight, state-aware adaptor\nto allocate the optimal number of denoising steps for each action. We jointly\noptimize the adaptor and base diffusion policy via reinforcement learning to\nbalance task performance and inference efficiency. On simulated tasks, D3P\nachieves an averaged 2.2$\\times$ inference speed-up over baselines without\ndegrading success. Furthermore, we demonstrate D3P's effectiveness on a\nphysical robot, achieving a 1.9$\\times$ acceleration over the baseline.", "AI": {"tldr": "D3P是一种动态去噪扩散策略，通过自适应分配去噪步数来加速机器人视觉运动任务中的扩散策略，同时保持性能。", "motivation": "扩散策略在机器人视觉运动任务中学习复杂动作分布表现出色，但其迭代去噪过程是实时部署的主要瓶颈。现有加速方法对所有动作应用固定数量的去噪步数，忽略了机器人任务中关键动作和常规动作对任务成功影响的差异。", "method": "提出了动态去噪扩散策略（D3P），它使用一个轻量级的、状态感知的适配器来为每个动作分配最优的去噪步数。该适配器与基础扩散策略通过强化学习共同优化，以平衡任务性能和推理效率。", "result": "在仿真任务中，D3P比基线方法平均实现了2.2倍的推理速度提升，且未降低成功率。在物理机器人上，D3P实现了1.9倍的加速。", "conclusion": "D3P通过自适应地分配去噪步数，有效加速了机器人任务中的扩散策略，在不牺牲性能的前提下显著提高了推理效率。"}}
{"id": "2508.06755", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06755", "abs": "https://arxiv.org/abs/2508.06755", "authors": ["Xianjun Yang", "Liqiang Xiao", "Shiyang Li", "Faisal Ladhak", "Hyokun Yun", "Linda Ruth Petzold", "Yi Xu", "William Yang Wang"], "title": "Many-Turn Jailbreaking", "comment": null, "summary": "Current jailbreaking work on large language models (LLMs) aims to elicit\nunsafe outputs from given prompts. However, it only focuses on single-turn\njailbreaking targeting one specific query. On the contrary, the advanced LLMs\nare designed to handle extremely long contexts and can thus conduct multi-turn\nconversations. So, we propose exploring multi-turn jailbreaking, in which the\njailbroken LLMs are continuously tested on more than the first-turn\nconversation or a single target query. This is an even more serious threat\nbecause 1) it is common for users to continue asking relevant follow-up\nquestions to clarify certain jailbroken details, and 2) it is also possible\nthat the initial round of jailbreaking causes the LLMs to respond to additional\nirrelevant questions consistently. As the first step (First draft done at June\n2024) in exploring multi-turn jailbreaking, we construct a Multi-Turn Jailbreak\nBenchmark (MTJ-Bench) for benchmarking this setting on a series of open- and\nclosed-source models and provide novel insights into this new safety threat. By\nrevealing this new vulnerability, we aim to call for community efforts to build\nsafer LLMs and pave the way for a more in-depth understanding of jailbreaking\nLLMs.", "AI": {"tldr": "本文首次探索大型语言模型（LLM）的多轮越狱问题，并构建了一个名为MTJ-Bench的基准测试集，以揭示这一新的安全威胁。", "motivation": "现有越狱研究仅关注单轮查询，而先进的LLM能够处理长上下文和多轮对话。用户通常会提出后续问题以澄清越狱细节，或初始越狱可能导致LLM持续对不相关问题做出不安全响应，这使得多轮越狱成为一个更严重但未被充分研究的威胁。", "method": "提出并探索多轮越狱概念，构建了Multi-Turn Jailbreak Benchmark (MTJ-Bench) 基准测试集，用于对一系列开源和闭源模型进行基准测试。", "result": "通过在MTJ-Bench上对模型进行测试，揭示了关于多轮越狱这一新型安全威胁的新颖见解和漏洞。", "conclusion": "通过揭示这一新的漏洞，旨在呼吁社区共同努力构建更安全的LLM，并为更深入理解LLM越狱问题铺平道路。"}}
{"id": "2508.06555", "categories": ["cs.CV", "cs.CY", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.06555", "abs": "https://arxiv.org/abs/2508.06555", "authors": ["Hongbo Ma", "Fei Shen", "Hongbin Xu", "Xiaoce Wang", "Gang Xu", "Jinkai Zheng", "Liangqiong Qu", "Ming Li"], "title": "StyleTailor: Towards Personalized Fashion Styling via Hierarchical Negative Feedback", "comment": "24pages, 5 figures", "summary": "The advancement of intelligent agents has revolutionized problem-solving\nacross diverse domains, yet solutions for personalized fashion styling remain\nunderexplored, which holds immense promise for promoting shopping experiences.\nIn this work, we present StyleTailor, the first collaborative agent framework\nthat seamlessly unifies personalized apparel design, shopping recommendation,\nvirtual try-on, and systematic evaluation into a cohesive workflow. To this\nend, StyleTailor pioneers an iterative visual refinement paradigm driven by\nmulti-level negative feedback, enabling adaptive and precise user alignment.\nSpecifically, our framework features two core agents, i.e., Designer for\npersonalized garment selection and Consultant for virtual try-on, whose outputs\nare progressively refined via hierarchical vision-language model feedback\nspanning individual items, complete outfits, and try-on efficacy.\nCounterexamples are aggregated into negative prompts, forming a closed-loop\nmechanism that enhances recommendation quality.To assess the performance, we\nintroduce a comprehensive evaluation suite encompassing style consistency,\nvisual quality, face similarity, and artistic appraisal. Extensive experiments\ndemonstrate StyleTailor's superior performance in delivering personalized\ndesigns and recommendations, outperforming strong baselines without negative\nfeedback and establishing a new benchmark for intelligent fashion systems.", "AI": {"tldr": "StyleTailor是一个开创性的协作智能体框架，首次将个性化服装设计、购物推荐、虚拟试穿和系统评估整合到统一流程中，通过多级负反馈的迭代视觉优化实现精准的用户对齐。", "motivation": "尽管智能代理在解决问题方面取得了显著进展，但个性化时尚造型的解决方案仍未得到充分探索，而这对于提升购物体验具有巨大潜力。", "method": "StyleTailor框架包含两个核心代理：Designer负责个性化服装选择，Consultant负责虚拟试穿。其输出通过分层视觉-语言模型反馈（涵盖单品、完整搭配和试穿效果）逐步优化。反例被聚合为负面提示，形成闭环机制以增强推荐质量。为评估性能，引入了涵盖风格一致性、视觉质量、面部相似性和艺术评价的综合评估套件。", "result": "广泛的实验表明，StyleTailor在提供个性化设计和推荐方面表现卓越，优于没有负反馈的强大基线系统，并为智能时尚系统建立了新的基准。", "conclusion": "StyleTailor通过其独特的协作代理框架和多级负反馈迭代优化范式，有效解决了个性化时尚造型的挑战，并为该领域树立了新的性能标准。"}}
{"id": "2508.08132", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.08132", "abs": "https://arxiv.org/abs/2508.08132", "authors": ["Mohammad Hossein Nejati Amiri", "Fawaz Annaz", "Mario De Oliveira", "Florimond Gueniat"], "title": "Deep Reinforcement Learning with Local Interpretability for Transparent Microgrid Resilience Energy Management", "comment": null, "summary": "Renewable energy integration into microgrids has become a key approach to\naddressing global energy issues such as climate change and resource scarcity.\nHowever, the variability of renewable sources and the rising occurrence of High\nImpact Low Probability (HILP) events require innovative strategies for reliable\nand resilient energy management. This study introduces a practical approach to\nmanaging microgrid resilience through Explainable Deep Reinforcement Learning\n(XDRL). It combines the Proximal Policy Optimization (PPO) algorithm for\ndecision-making with the Local Interpretable Model-agnostic Explanations (LIME)\nmethod to improve the transparency of the actor network's decisions. A case\nstudy in Ongole, India, examines a microgrid with wind, solar, and battery\ncomponents to validate the proposed approach. The microgrid is simulated under\nextreme weather conditions during the Layla cyclone. LIME is used to analyse\nscenarios, showing the impact of key factors such as renewable generation,\nstate of charge, and load prioritization on decision-making. The results\ndemonstrate a Resilience Index (RI) of 0.9736 and an estimated battery lifespan\nof 15.11 years. LIME analysis reveals the rationale behind the agent's actions\nin idle, charging, and discharging modes, with renewable generation identified\nas the most influential feature. This study shows the effectiveness of\nintegrating advanced DRL algorithms with interpretable AI techniques to achieve\nreliable and transparent energy management in microgrids.", "AI": {"tldr": "本研究提出一种结合可解释深度强化学习（XDRL）的方法，利用PPO算法和LIME解释性技术，以提高微电网在可再生能源波动和高影响低概率事件下的韧性和透明度。", "motivation": "可再生能源的波动性以及高影响低概率（HILP）事件的日益频繁发生，对微电网的可靠和弹性能源管理提出了挑战，需要创新的策略来解决全球能源问题。", "method": "研究采用可解释深度强化学习（XDRL）方法，结合近端策略优化（PPO）算法进行决策，并利用局部可解释模型无关解释（LIME）方法提高决策透明度。通过印度Ongole的实际案例研究，模拟了包含风能、太阳能和电池的微电网在Layla飓风等极端天气条件下的运行，并使用LIME分析了决策过程中的关键影响因素。", "result": "该方法实现了0.9736的韧性指数（RI）和15.11年的电池寿命估算。LIME分析揭示了代理在空闲、充电和放电模式下行为背后的原理，其中可再生能源发电被确定为最具影响力的特征。", "conclusion": "本研究证明了将先进的深度强化学习算法与可解释人工智能技术相结合，在实现微电网可靠和透明能源管理方面的有效性。"}}
{"id": "2508.06851", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.06851", "abs": "https://arxiv.org/abs/2508.06851", "authors": ["Pengfei Zhou", "Xiaopeng Peng", "Fanrui Zhang", "Zhaopan Xu", "Jiaxin Ai", "Yansheng Qiu", "Chuanhao Li", "Zhen Li", "Ming Li", "Yukang Feng", "Jianwen Sun", "Haoquan Zhang", "Zizhen Li", "Xiaofeng Mao", "Zekai Li", "Wangbo Zhao", "Kai Wang", "Xiaojun Chang", "Wenqi Shao", "Yang You", "Kaipeng Zhang"], "title": "MDK12-Bench: A Comprehensive Evaluation of Multimodal Large Language Models on Multidisciplinary Exams", "comment": "35 pages, 33 figures", "summary": "Multimodal large language models (MLLMs), which integrate language and visual\ncues for problem-solving, are crucial for advancing artificial general\nintelligence (AGI). However, current benchmarks for measuring the intelligence\nof MLLMs suffer from limited scale, narrow coverage, and unstructured\nknowledge, offering only static and undifferentiated evaluations. To bridge\nthis gap, we introduce MDK12-Bench, a large-scale multidisciplinary benchmark\nbuilt from real-world K-12 exams spanning six disciplines with 141K instances\nand 6,225 knowledge points organized in a six-layer taxonomy. Covering five\nquestion formats with difficulty and year annotations, it enables comprehensive\nevaluation to capture the extent to which MLLMs perform over four dimensions:\n1) difficulty levels, 2) temporal (cross-year) shifts, 3) contextual shifts,\nand 4) knowledge-driven reasoning. We propose a novel dynamic evaluation\nframework that introduces unfamiliar visual, textual, and question form shifts\nto challenge model generalization while improving benchmark objectivity and\nlongevity by mitigating data contamination. We further evaluate knowledge-point\nreference-augmented generation (KP-RAG) to examine the role of knowledge in\nproblem-solving. Key findings reveal limitations in current MLLMs in multiple\naspects and provide guidance for enhancing model robustness, interpretability,\nand AI-assisted education.", "AI": {"tldr": "该研究引入了MDK12-Bench，一个基于真实K-12考试的大规模多学科基准，用于动态评估多模态大语言模型（MLLMs）的智能水平，并揭示了现有MLLMs的局限性。", "motivation": "现有的MLLMs智能评估基准存在规模有限、覆盖范围狭窄、知识结构化不足以及评估静态和缺乏区分度的问题，无法全面衡量MLLMs的智能水平和泛化能力。", "method": "研究构建了MDK12-Bench，包含来自六个学科的14.1万个实例和6225个知识点，并组织成六层分类法。该基准覆盖五种问题格式，并包含难度和年份标注。研究提出了一种新颖的动态评估框架，引入不熟悉的视觉、文本和问题形式变化来挑战模型的泛化能力。此外，还评估了知识点参考增强生成（KP-RAG）以探究知识在解决问题中的作用。", "result": "关键发现表明，当前MLLMs在多个方面存在局限性。", "conclusion": "研究为增强模型的鲁棒性、可解释性以及AI辅助教育提供了指导，并有助于缓解数据污染问题，提高基准的客观性和持久性。"}}
{"id": "2508.06921", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06921", "abs": "https://arxiv.org/abs/2508.06921", "authors": ["Zhongyu Chen", "Chenyang Li", "Xuesong Li", "Dianye Huang", "Zhongliang Jiang", "Stefanie Speidel", "Xiangyu Chu", "K. W. Samuel Au"], "title": "Vibration-Based Energy Metric for Restoring Needle Alignment in Autonomous Robotic Ultrasound", "comment": null, "summary": "Precise needle alignment is essential for percutaneous needle insertion in\nrobotic ultrasound-guided procedures. However, inherent challenges such as\nspeckle noise, needle-like artifacts, and low image resolution make robust\nneedle detection difficult, particularly when visibility is reduced or lost. In\nthis paper, we propose a method to restore needle alignment when the ultrasound\nimaging plane and the needle insertion plane are misaligned. Unlike many\nexisting approaches that rely heavily on needle visibility in ultrasound\nimages, our method uses a more robust feature by periodically vibrating the\nneedle using a mechanical system. Specifically, we propose a vibration-based\nenergy metric that remains effective even when the needle is fully out of\nplane. Using this metric, we develop a control strategy to reposition the\nultrasound probe in response to misalignments between the imaging plane and the\nneedle insertion plane in both translation and rotation. Experiments conducted\non ex-vivo porcine tissue samples using a dual-arm robotic ultrasound-guided\nneedle insertion system demonstrate the effectiveness of the proposed approach.\nThe experimental results show the translational error of 0.41$\\pm$0.27 mm and\nthe rotational error of 0.51$\\pm$0.19 degrees.", "AI": {"tldr": "该论文提出一种基于振动能量度量的方法，用于在超声引导机器人穿刺手术中恢复针头对齐，即使针头在图像中不可见也能有效工作，通过重新定位超声探头来纠正平面错位。", "motivation": "机器人超声引导下的经皮穿刺手术需要精确的针头对齐，但由于散斑噪声、伪影和图像分辨率低等固有挑战，鲁棒的针头检测非常困难，尤其是在可见度降低或消失时。现有方法过度依赖超声图像中的针头可见性。", "method": "该方法通过机械系统周期性地振动针头，并提出了一种基于振动的能量度量，即使针头完全偏离成像平面也能保持有效。利用此度量，开发了一种控制策略来重新定位超声探头，以纠正成像平面和针头插入平面之间的平移和旋转错位。实验在离体猪组织样本上使用双臂机器人超声引导针头插入系统进行。", "result": "实验结果显示，该方法实现了0.41±0.27毫米的平移误差和0.51±0.19度的旋转误差。", "conclusion": "所提出的基于振动的方法能够有效恢复超声引导机器人穿刺手术中的针头对齐，即使在针头可见性降低或丧失的情况下，也能通过精确重新定位超声探头来纠正平面错位。"}}
{"id": "2508.06803", "categories": ["cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.06803", "abs": "https://arxiv.org/abs/2508.06803", "authors": ["Ziqi Liu", "Yangbin Chen", "Ziyang Zhou", "Yilin Li", "Mingxuan Hu", "Yushan Pan", "Zhijie Xu"], "title": "SEVADE: Self-Evolving Multi-Agent Analysis with Decoupled Evaluation for Hallucination-Resistant Irony Detection", "comment": null, "summary": "Sarcasm detection is a crucial yet challenging Natural Language Processing\ntask. Existing Large Language Model methods are often limited by\nsingle-perspective analysis, static reasoning pathways, and a susceptibility to\nhallucination when processing complex ironic rhetoric, which impacts their\naccuracy and reliability. To address these challenges, we propose **SEVADE**, a\nnovel **S**elf-**Ev**olving multi-agent **A**nalysis framework with\n**D**ecoupled **E**valuation for hallucination-resistant sarcasm detection. The\ncore of our framework is a Dynamic Agentive Reasoning Engine (DARE), which\nutilizes a team of specialized agents grounded in linguistic theory to perform\na multifaceted deconstruction of the text and generate a structured reasoning\nchain. Subsequently, a separate lightweight rationale adjudicator (RA) performs\nthe final classification based solely on this reasoning chain. This decoupled\narchitecture is designed to mitigate the risk of hallucination by separating\ncomplex reasoning from the final judgment. Extensive experiments on four\nbenchmark datasets demonstrate that our framework achieves state-of-the-art\nperformance, with average improvements of **6.75%** in Accuracy and **6.29%**\nin Macro-F1 score.", "AI": {"tldr": "提出SEVADE框架，一个自演化多智能体分析框架，通过解耦评估来抵抗幻觉，实现更准确的讽刺检测。", "motivation": "现有大语言模型（LLM）在讽刺检测中存在单视角分析、静态推理路径以及处理复杂反讽修辞时易产生幻觉的问题，影响了准确性和可靠性。", "method": "SEVADE框架的核心是动态智能体推理引擎（DARE），它利用一组基于语言理论的专业智能体对文本进行多方面解构，并生成结构化推理链。随后，一个独立的轻量级理由裁决器（RA）仅基于此推理链进行最终分类。这种解耦架构旨在通过将复杂推理与最终判断分离来降低幻觉风险。", "result": "在四个基准数据集上进行了广泛实验，SEVADE框架达到了最先进的性能，平均准确率提高了6.75%，Macro-F1分数提高了6.29%。", "conclusion": "SEVADE框架通过其多智能体推理和解耦评估架构，有效解决了现有LLM在讽刺检测中的局限性，显著提高了性能并增强了抗幻觉能力。"}}
{"id": "2508.06556", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06556", "abs": "https://arxiv.org/abs/2508.06556", "authors": ["Sarina Penquitt", "Jonathan Klees", "Rinor Cakaj", "Daniel Kondermann", "Matthias Rottmann", "Lars Schmarje"], "title": "From Label Error Detection to Correction: A Modular Framework and Benchmark for Object Detection Datasets", "comment": null, "summary": "Object detection has advanced rapidly in recent years, driven by increasingly\nlarge and diverse datasets. However, label errors, defined as missing labels,\nincorrect classification or inaccurate localization, often compromise the\nquality of these datasets. This can have a significant impact on the outcomes\nof training and benchmark evaluations. Although several methods now exist for\ndetecting label errors in object detection datasets, they are typically\nvalidated only on synthetic benchmarks or limited manual inspection. How to\ncorrect such errors systemically and at scale therefore remains an open\nproblem. We introduce a semi-automated framework for label-error correction\ncalled REC$\\checkmark$D (Rechecked). Building on existing detectors, the\nframework pairs their error proposals with lightweight, crowd-sourced\nmicrotasks. These tasks enable multiple annotators to independently verify each\ncandidate bounding box, and their responses are aggregated to estimate\nambiguity and improve label quality. To demonstrate the effectiveness of\nREC$\\checkmark$D, we apply it to the class pedestrian in the KITTI dataset. Our\ncrowdsourced review yields high-quality corrected annotations, which indicate a\nrate of at least 24% of missing and inaccurate annotations in original\nannotations. This validated set will be released as a new real-world benchmark\nfor label error detection and correction. We show that current label error\ndetection methods, when combined with our correction framework, can recover\nhundreds of errors in the time it would take a human to annotate bounding boxes\nfrom scratch. However, even the best methods still miss up to 66% of the true\nerrors and with low quality labels introduce more errors than they find. This\nhighlights the urgent need for further research, now enabled by our released\nbenchmark.", "AI": {"tldr": "本文提出一个半自动化框架REC✓D，用于大规模修正目标检测数据集中的标签错误，结合现有检测器和众包微任务。通过在KITTI数据集上验证，发现原始标签存在显著错误率，并发布了一个新的真实世界基准，强调了现有错误检测方法的局限性。", "motivation": "随着数据集规模和多样性增加，目标检测数据集中的标签错误（如漏标、分类错误、定位不准）日益普遍，严重影响模型训练和基准评估。尽管已有错误检测方法，但它们通常只在合成基准或有限的人工检查下验证，缺乏系统性、大规模的纠错方案。", "method": "引入REC✓D（Rechecked）半自动化标签错误修正框架。该框架将现有检测器提出的错误候选项与轻量级众包微任务相结合，让多名标注者独立验证每个候选边界框，并聚合他们的响应以估计歧义并提高标签质量。在KITTI数据集的“行人”类别上进行了应用和验证。", "result": "在KITTI数据集上，REC✓D的众包审查产生了高质量的修正标注，揭示原始标注中至少24%的缺失和不准确标注。该验证集将作为新的真实世界标签错误检测和修正基准发布。结合REC✓D，现有标签错误检测方法能在短时间内修正大量错误，但即使是最好的方法仍会漏掉高达66%的真实错误，且低质量的标签甚至可能引入更多错误。", "conclusion": "当前标签错误检测方法与所提出的修正框架结合，能够有效发现并修正大量错误，但仍存在显著改进空间。已发布的真实世界基准凸显了进一步研究标签错误检测和修正方法的迫切需求。"}}
{"id": "2508.08153", "categories": ["eess.SY", "cs.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2508.08153", "abs": "https://arxiv.org/abs/2508.08153", "authors": ["Changrui Liu", "Anil Alan", "Shengling Shi", "Bart De Schutter"], "title": "Robust Adaptive Discrete-Time Control Barrier Certificate", "comment": "10 pages, 2 figures, submitted to Automatica as a brief paper", "summary": "This work develops a robust adaptive control strategy for discrete-time\nsystems using Control Barrier Functions (CBFs) to ensure safety under\nparametric model uncertainty and disturbances. A key contribution of this work\nis establishing a barrier function certificate in discrete time for general\nonline parameter estimation algorithms. This barrier function certificate\nguarantees positive invariance of the safe set despite disturbances and\nparametric uncertainty without access to the true system parameters. In\naddition, real-time implementation and inherent robustness guarantees are\nprovided. Our approach demonstrates that, using the proposed robust adaptive\nCBF framework, the parameter estimation module can be designed separately from\nthe CBF-based safety filter, simplifying the development of safe adaptive\ncontrollers for discrete-time systems. The resulting safety filter guarantees\nthat the system remains within the safe set while adapting to model\nuncertainties, making it a promising strategy for real-world applications\ninvolving discrete-time safety-critical systems.", "AI": {"tldr": "该工作提出了一种鲁棒自适应控制策略，通过使用控制障碍函数（CBF）来确保离散时间系统在参数模型不确定性和扰动下的安全性。", "motivation": "在存在参数模型不确定性和扰动的情况下，为离散时间系统提供安全保证，同时实现实时实施和固有的鲁棒性。", "method": "开发了一种基于控制障碍函数（CBF）的鲁棒自适应控制策略。核心贡献是为通用在线参数估计算法建立了离散时间障碍函数证书，确保了安全集的正不变性。该方法允许参数估计模块与基于CBF的安全滤波器独立设计。", "result": "即使在存在扰动和参数不确定性且无法获取真实系统参数的情况下，也能保证安全集的正不变性。提供了实时实现和固有的鲁棒性保证。简化了离散时间系统安全自适应控制器的开发。系统在适应模型不确定性的同时保持在安全集内。", "conclusion": "所提出的鲁棒自适应CBF框架是一种有前景的策略，适用于涉及离散时间安全关键系统的实际应用，因为它能在适应模型不确定性的同时提供安全保证。"}}
{"id": "2508.06859", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06859", "abs": "https://arxiv.org/abs/2508.06859", "authors": ["Shuo Tang", "Jian Xu", "Jiadong Zhang", "Yi Chen", "Qizhao Jin", "Lingdong Shen", "Chenglin Liu", "Shiming Xiang"], "title": "MeteorPred: A Meteorological Multimodal Large Model and Dataset for Severe Weather Event Prediction", "comment": null, "summary": "Timely and accurate severe weather warnings are critical for disaster\nmitigation. However, current forecasting systems remain heavily reliant on\nmanual expert interpretation, introducing subjectivity and significant\noperational burdens. With the rapid development of AI technologies, the\nend-to-end \"AI weather station\" is gradually emerging as a new trend in\npredicting severe weather events. Three core challenges impede the development\nof end-to-end AI severe weather system: (1) scarcity of severe weather event\nsamples; (2) imperfect alignment between high-dimensional meteorological data\nand textual warnings; (3) existing multimodal language models are unable to\nhandle high-dimensional meteorological data and struggle to fully capture the\ncomplex dependencies across temporal sequences, vertical pressure levels, and\nspatial dimensions. To address these challenges, we introduce MP-Bench, the\nfirst large-scale temporal multimodal dataset for severe weather events\nprediction, comprising 421,363 pairs of raw multi-year meteorological data and\ncorresponding text caption, covering a wide range of severe weather scenarios\nacross China. On top of this dataset, we develop a meteorology multimodal large\nmodel (MMLM) that directly ingests 4D meteorological inputs. In addition, it is\ndesigned to accommodate the unique characteristics of 4D meteorological data\nflow, incorporating three plug-and-play adaptive fusion modules that enable\ndynamic feature extraction and integration across temporal sequences, vertical\npressure layers, and spatial dimensions. Extensive experiments on MP-Bench\ndemonstrate that MMLM performs exceptionally well across multiple tasks,\nhighlighting its effectiveness in severe weather understanding and marking a\nkey step toward realizing automated, AI-driven weather forecasting systems. Our\nsource code and dataset will be made publicly available.", "AI": {"tldr": "该研究开发了首个大规模时序多模态恶劣天气预测数据集MP-Bench，并基于此提出了气象多模态大模型（MMLM），旨在克服现有AI气象预警系统的挑战，实现自动化、端到端的恶劣天气预报。", "motivation": "当前的恶劣天气预警系统过度依赖人工专家判读，导致主观性和高操作负担。端到端AI气象系统面临三大核心挑战：1) 恶劣天气事件样本稀缺；2) 高维气象数据与文本警告对齐不完善；3) 现有多模态语言模型难以处理高维气象数据并捕获复杂的时空依赖关系。", "method": "1. 构建了MP-Bench数据集：首个大规模时序多模态恶劣天气预测数据集，包含421,363对原始多年气象数据和对应文本描述，覆盖中国广泛的恶劣天气场景。2. 开发了气象多模态大模型（MMLM）：该模型直接处理4D气象输入，并设计了三个即插即用的自适应融合模块，以动态提取和整合时间序列、垂直气压层和空间维度上的特征。", "result": "在MP-Bench数据集上进行的广泛实验表明，MMLM在多项任务中表现出色，突显了其在恶劣天气理解方面的有效性。", "conclusion": "MMLM的成功是实现自动化、AI驱动的天气预报系统的关键一步。该研究的代码和数据集将公开，以促进未来研究。"}}
{"id": "2508.06969", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06969", "abs": "https://arxiv.org/abs/2508.06969", "authors": ["Bingkun Huang", "Evgeniy Kotov", "Arkady Yuschenko"], "title": "Manipulator for people with limited abilities", "comment": "105 pages, in Russian language", "summary": "The topic of this final qualification work was chosen due to the importance\nof developing robotic systems designed to assist people with disabilities.\nAdvances in robotics and automation technologies have opened up new prospects\nfor creating devices that can significantly improve the quality of life for\nthese people. In this context, designing a robotic hand with a control system\nadapted to the needs of people with disabilities is a major scientific and\npractical challenge. This work addresses the problem of developing and\nmanufacturing a four-degree-of-freedom robotic hand suitable for practical\nmanipulation. Addressing this issue requires a comprehensive approach,\nencompassing the design of the hand's mechanical structure, the development of\nits control system, and its integration with a technical vision system and\nsoftware based on the Robot Operating System (ROS).", "AI": {"tldr": "本文旨在开发和制造一个四自由度机械手，配备控制系统和机器视觉，以辅助残障人士。", "motivation": "机器人和自动化技术进步为改善残障人士生活质量提供了新机遇。为残障人士设计适应性强的机械手及其控制系统是一个重要的科学与实践挑战。", "method": "开发一个四自由度机械手，包括其机械结构设计、控制系统开发，并与机器视觉系统和基于ROS的软件进行集成。", "result": "本工作致力于解决开发和制造适合实际操作的四自由度机械手的问题。", "conclusion": "开发适应残障人士需求的机器人手及其控制系统是一项重大的科学与实践挑战，本工作旨在通过综合方法应对此挑战。"}}
{"id": "2508.06810", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06810", "abs": "https://arxiv.org/abs/2508.06810", "authors": ["Steven Coyne", "Diana Galvan-Sosa", "Ryan Spring", "Camélia Guerraoui", "Michael Zock", "Keisuke Sakaguchi", "Kentaro Inui"], "title": "Annotating Errors in English Learners' Written Language Production: Advancing Automated Written Feedback Systems", "comment": "Pre-review version of DOI 10.1007/978-3-031-98459-4_21, presented at\n  AIED 2025. All content is as of submission time except for de-anonymization,\n  ensuing layout fixes, use of the current code repository link, and BibTeX\n  fixes. Readers are encouraged to refer to the published version", "summary": "Recent advances in natural language processing (NLP) have contributed to the\ndevelopment of automated writing evaluation (AWE) systems that can correct\ngrammatical errors. However, while these systems are effective at improving\ntext, they are not optimally designed for language learning. They favor direct\nrevisions, often with a click-to-fix functionality that can be applied without\nconsidering the reason for the correction. Meanwhile, depending on the error\ntype, learners may benefit most from simple explanations and strategically\nindirect hints, especially on generalizable grammatical rules. To support the\ngeneration of such feedback, we introduce an annotation framework that models\neach error's error type and generalizability. For error type classification, we\nintroduce a typology focused on inferring learners' knowledge gaps by\nconnecting their errors to specific grammatical patterns. Following this\nframework, we collect a dataset of annotated learner errors and corresponding\nhuman-written feedback comments, each labeled as a direct correction or hint.\nWith this data, we evaluate keyword-guided, keyword-free, and template-guided\nmethods of generating feedback using large language models (LLMs). Human\nteachers examined each system's outputs, assessing them on grounds including\nrelevance, factuality, and comprehensibility. We report on the development of\nthe dataset and the comparative performance of the systems investigated.", "AI": {"tldr": "本研究提出一种新的标注框架和错误类型学，以支持自动化写作评估（AWE）系统为语言学习者生成更具教学意义的反馈（如解释和提示），而非仅仅直接修正错误。研究收集了一个带有人工反馈注释的数据集，并评估了使用大型语言模型（LLMs）生成反馈的多种方法。", "motivation": "现有的自动化写作评估（AWE）系统虽然能有效纠正语法错误，但它们主要提供直接修正，缺乏对错误原因的解释或策略性提示，这不利于语言学习者真正理解和掌握语法规则，从而无法优化语言学习过程。", "method": "本研究引入了一个标注框架，对每个错误进行错误类型和可泛化性建模。为此，提出了一种专注于通过错误推断学习者知识空白的错误类型学。研究收集了一个包含标注学习者错误和对应人工反馈（直接修正或提示）的数据集。利用该数据，评估了基于LLMs的多种反馈生成方法，包括关键词引导、无关键词和模板引导方法。人类教师对系统输出进行了相关性、事实性和可理解性评估。", "result": "研究报告了数据集的开发过程，并比较了所调查的各种系统（基于LLMs的反馈生成方法）的性能表现。", "conclusion": "本研究成功开发了一个新的标注框架和数据集，并评估了使用大型语言模型为语言学习者生成教学反馈（包括解释和提示）的多种方法，为优化AWE系统以更好地支持语言学习提供了基础和见解。"}}
{"id": "2508.06558", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06558", "abs": "https://arxiv.org/abs/2508.06558", "authors": ["Simon Baur", "Alexandra Benova", "Emilio Dolgener Cantú", "Jackie Ma"], "title": "On the effectiveness of multimodal privileged knowledge distillation in two vision transformer based diagnostic applications", "comment": null, "summary": "Deploying deep learning models in clinical practice often requires leveraging\nmultiple data modalities, such as images, text, and structured data, to achieve\nrobust and trustworthy decisions. However, not all modalities are always\navailable at inference time. In this work, we propose multimodal privileged\nknowledge distillation (MMPKD), a training strategy that utilizes additional\nmodalities available solely during training to guide a unimodal vision model.\nSpecifically, we used a text-based teacher model for chest radiographs\n(MIMIC-CXR) and a tabular metadata-based teacher model for mammography\n(CBIS-DDSM) to distill knowledge into a vision transformer student model. We\nshow that MMPKD can improve the resulting attention maps' zero-shot\ncapabilities of localizing ROI in input images, while this effect does not\ngeneralize across domains, as contrarily suggested by prior research.", "AI": {"tldr": "MMPKD是一种训练策略，利用仅在训练时可用的多模态特权知识来指导单模态视觉模型，提高其零样本ROI定位能力，但这种效果不跨领域泛化。", "motivation": "在临床实践中部署深度学习模型需要利用多种数据模态（如图像、文本、结构化数据）以实现鲁棒和可信的决策，但并非所有模态在推理时都始终可用。", "method": "提出多模态特权知识蒸馏（MMPKD）策略。具体而言，使用基于文本的教师模型（针对胸部X光片MIMIC-CXR）和基于表格元数据的教师模型（针对乳腺X光片CBIS-DDSM），将知识蒸馏到视觉Transformer学生模型中。", "result": "MMPKD可以提高结果注意力图在输入图像中零样本定位感兴趣区域（ROI）的能力。然而，这种效果不跨领域泛化，这与先前的研究相悖。", "conclusion": "MMPKD能够有效利用特权知识提升单模态视觉模型的性能，特别是在ROI定位方面，但其跨不同医学图像领域的泛化能力有限。"}}
{"id": "2508.08185", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.08185", "abs": "https://arxiv.org/abs/2508.08185", "authors": ["Yaoyu Zhang", "Xin Sun", "Jun Wang", "Tianwei Hou", "Anna Li", "Yuanwei Liu", "Arumugam Nallanathan"], "title": "Pinching-Antenna Systems (PASS)-based Indoor Positioning", "comment": "5 pages, 5 figures, letter", "summary": "Pinching antenna (PA), a flexible waveguide integrated with dielectric\nparticles, intelligently reconstructs line-of-sight channels. Utilizing its\ngeometric deterministic model and meter-level reconstruction, PA systems (PASS)\nare applied to uplink indoor positioning. In this paper, the uplink positioning\nsystem model for PASS is firstly proposed. A PASS-based received signal\nstrength indication (RSSI) method is proposed to measure the distance from the\nusers to each PA, which is efficient and suitable for PASS. PASS-based weighted\nleast squares (WLS) algorithm is designed to calculate the two-dimensional\ncoordinates of the users. Several critical observations can be drawn from our\nresults: i) More PAs on the waveguide improves the positioning accuracy and\nrobustness. ii) When the number of PAs exceeds a certain threshold, the\nperformance gain becomes marginal. iii) User locations between and near PAs\nyield superior positioning accuracy.", "AI": {"tldr": "本文提出并验证了一种基于捏合天线系统（PASS）的室内上行定位方法，利用RSSI测距和加权最小二乘（WLS）算法实现用户定位。", "motivation": "捏合天线（PA）作为一种灵活的波导集成介电粒子，能够智能重建视距信道，其几何确定性模型和米级重建能力使其适用于上行室内定位。", "method": "首先提出了PASS的上行定位系统模型。接着，提出了一种基于PASS的接收信号强度指示（RSSI）方法来测量用户到每个PA的距离。最后，设计了一种基于PASS的加权最小二乘（WLS）算法来计算用户的二维坐标。", "result": "研究结果表明：1) 波导上PA数量的增加能提高定位精度和鲁棒性。2) 当PA数量超过一定阈值时，性能增益变得微乎其微。3) 用户位于PA之间或附近时能获得更高的定位精度。", "conclusion": "基于捏合天线系统的RSSI和WLS算法能够有效地实现室内上行定位，且定位性能受PA数量和用户位置分布影响。"}}
{"id": "2508.06894", "categories": ["cs.AI", "cs.LG", "68T05"], "pdf": "https://arxiv.org/pdf/2508.06894", "abs": "https://arxiv.org/abs/2508.06894", "authors": ["Giovanni Varricchione", "Toryn Q. Klassen", "Natasha Alechina", "Mehdi Dastani", "Brian Logan", "Sheila A. McIlraith"], "title": "Pushdown Reward Machines for Reinforcement Learning", "comment": null, "summary": "Reward machines (RMs) are automata structures that encode (non-Markovian)\nreward functions for reinforcement learning (RL). RMs can reward any behaviour\nrepresentable in regular languages and, when paired with RL algorithms that\nexploit RM structure, have been shown to significantly improve sample\nefficiency in many domains. In this work, we present pushdown reward machines\n(pdRMs), an extension of reward machines based on deterministic pushdown\nautomata. pdRMs can recognize and reward temporally extended behaviours\nrepresentable in deterministic context-free languages, making them more\nexpressive than reward machines. We introduce two variants of pdRM-based\npolicies, one which has access to the entire stack of the pdRM, and one which\ncan only access the top $k$ symbols (for a given constant $k$) of the stack. We\npropose a procedure to check when the two kinds of policies (for a given\nenvironment, pdRM, and constant $k$) achieve the same optimal expected reward.\nWe then provide theoretical results establishing the expressive power of pdRMs,\nand space complexity results about the proposed learning problems. Finally, we\nprovide experimental results showing how agents can be trained to perform tasks\nrepresentable in deterministic context-free languages using pdRMs.", "AI": {"tldr": "本文提出了下推奖励机（pdRM），它是奖励机（RM）的扩展，能够识别和奖励确定性上下文无关语言表示的更复杂、时间扩展的行为，并展示了如何使用它们训练强化学习（RL）智能体。", "motivation": "现有奖励机（RM）在强化学习中能有效提高样本效率，但其表达能力受限于正则语言。为了处理更复杂、需要记忆和堆栈操作的非马尔可夫奖励函数（如上下文无关行为），需要一种更具表达力的结构。", "method": "引入了基于确定性下推自动机的下推奖励机（pdRM）。提出了两种基于pdRM的策略：一种可访问整个堆栈，另一种仅访问堆栈顶部k个符号。提出了一个检查两种策略何时能达到相同最优预期奖励的程序。进行了理论分析，确立了pdRM的表达能力和学习问题的空间复杂度。通过实验展示了智能体如何利用pdRM学习执行确定性上下文无关语言表示的任务。", "result": "pdRM比传统奖励机更具表达力，能够识别和奖励确定性上下文无关语言表示的行为。提出了两种可行的pdRM策略变体。理论分析确定了pdRM的表达能力和相关学习问题的复杂度。实验结果表明，智能体可以成功地使用pdRM训练来完成需要处理上下文无关语言的任务。", "conclusion": "下推奖励机（pdRM）是奖励机（RM）的有效扩展，显著增强了强化学习中奖励函数的表达能力，使其能够处理和学习确定性上下文无关语言所描述的复杂、时间扩展行为。"}}
{"id": "2508.06990", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06990", "abs": "https://arxiv.org/abs/2508.06990", "authors": ["Yue Hu", "Junzhe Wu", "Ruihan Xu", "Hang Liu", "Avery Xi", "Henry X. Liu", "Ram Vasudevan", "Maani Ghaffari"], "title": "Imaginative World Modeling with Scene Graphs for Embodied Agent Navigation", "comment": "23 pages", "summary": "Semantic navigation requires an agent to navigate toward a specified target\nin an unseen environment. Employing an imaginative navigation strategy that\npredicts future scenes before taking action, can empower the agent to find\ntarget faster. Inspired by this idea, we propose SGImagineNav, a novel\nimaginative navigation framework that leverages symbolic world modeling to\nproactively build a global environmental representation. SGImagineNav maintains\nan evolving hierarchical scene graphs and uses large language models to predict\nand explore unseen parts of the environment. While existing methods solely\nrelying on past observations, this imaginative scene graph provides richer\nsemantic context, enabling the agent to proactively estimate target locations.\nBuilding upon this, SGImagineNav adopts an adaptive navigation strategy that\nexploits semantic shortcuts when promising and explores unknown areas otherwise\nto gather additional context. This strategy continuously expands the known\nenvironment and accumulates valuable semantic contexts, ultimately guiding the\nagent toward the target. SGImagineNav is evaluated in both real-world scenarios\nand simulation benchmarks. SGImagineNav consistently outperforms previous\nmethods, improving success rate to 65.4 and 66.8 on HM3D and HSSD, and\ndemonstrating cross-floor and cross-room navigation in real-world environments,\nunderscoring its effectiveness and generalizability.", "AI": {"tldr": "SGImagineNav是一种新颖的想象式导航框架，它利用符号世界建模（分层场景图和大型语言模型）来预测和探索未知环境，从而实现更快的语义导航，并在真实世界和模拟环境中表现出色。", "motivation": "在未知环境中进行语义导航并快速找到目标是当前代理面临的挑战。现有方法仅依赖于过去的观察，缺乏对未来场景的预测能力和更丰富的语义上下文，限制了导航效率和目标定位能力。", "method": "SGImagineNav采用符号世界建模方法，维护一个不断演进的分层场景图。它利用大型语言模型（LLMs）预测和探索环境的未知部分，提供比仅依赖过去观察更丰富的语义上下文。在此基础上，SGImagineNav采用自适应导航策略：当有希望时利用语义捷径，否则探索未知区域以收集额外上下文，从而持续扩展已知环境并积累语义信息。", "result": "SGImagineNav在真实世界场景和模拟基准测试中均超越了现有方法。在HM3D和HSSD数据集上，成功率分别提高到65.4%和66.8%。此外，它在真实世界环境中展示了跨楼层和跨房间的导航能力，证明了其有效性和泛化性。", "conclusion": "SGImagineNav通过结合符号世界建模和自适应导航策略，显著提升了代理在未知环境中的语义导航能力，表现出卓越的性能和广泛的适用性。"}}
{"id": "2508.06870", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.06870", "abs": "https://arxiv.org/abs/2508.06870", "authors": ["Gangular Singh Irengbam", "Nirvash Singh Wahengbam", "Lanthoiba Meitei Khumanthem", "Paikhomba Oinam"], "title": "Text to Speech System for Meitei Mayek Script", "comment": null, "summary": "This paper presents the development of a Text-to-Speech (TTS) system for the\nManipuri language using the Meitei Mayek script. Leveraging Tacotron 2 and\nHiFi-GAN, we introduce a neural TTS architecture adapted to support tonal\nphonology and under-resourced linguistic environments. We develop a phoneme\nmapping for Meitei Mayek to ARPAbet, curate a single-speaker dataset, and\ndemonstrate intelligible and natural speech synthesis, validated through\nsubjective and objective metrics. This system lays the groundwork for\nlinguistic preservation and technological inclusion of Manipuri.", "AI": {"tldr": "本文开发了一个基于Tacotron 2和HiFi-GAN的曼尼普尔语（使用Meitei Mayek文字）文本到语音（TTS）系统，支持声调语音学和低资源语言环境。", "motivation": "支持曼尼普尔语的声调语音学，解决其作为低资源语言的挑战，并促进该语言的保存和技术包容性。", "method": "采用基于Tacotron 2和HiFi-GAN的神经网络TTS架构，将Meitei Mayek文字映射到ARPAbet音素，并整理了一个单说话人数据集。", "result": "实现了清晰自然的语音合成，并通过主观和客观指标进行了验证。", "conclusion": "该系统为曼尼普尔语的语言保存和技术融入奠定了基础。"}}
{"id": "2508.06564", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06564", "abs": "https://arxiv.org/abs/2508.06564", "authors": ["Guanyu Hu", "Dimitrios Kollias", "Xinyu Yang"], "title": "Grounding Emotion Recognition with Visual Prototypes: VEGA -- Revisiting CLIP in MERC", "comment": "accepted for publication at ACM Multimedia (ACM MM) 2025", "summary": "Multimodal Emotion Recognition in Conversations remains a challenging task\ndue to the complex interplay of textual, acoustic and visual signals. While\nrecent models have improved performance via advanced fusion strategies, they\noften lack psychologically meaningful priors to guide multimodal alignment. In\nthis paper, we revisit the use of CLIP and propose a novel Visual Emotion\nGuided Anchoring (VEGA) mechanism that introduces class-level visual semantics\ninto the fusion and classification process. Distinct from prior work that\nprimarily utilizes CLIP's textual encoder, our approach leverages its image\nencoder to construct emotion-specific visual anchors based on facial exemplars.\nThese anchors guide unimodal and multimodal features toward a perceptually\ngrounded and psychologically aligned representation space, drawing inspiration\nfrom cognitive theories (prototypical emotion categories and multisensory\nintegration). A stochastic anchor sampling strategy further enhances robustness\nby balancing semantic stability and intra-class diversity. Integrated into a\ndual-branch architecture with self-distillation, our VEGA-augmented model\nachieves sota performance on IEMOCAP and MELD. Code is available at:\nhttps://github.com/dkollias/VEGA.", "AI": {"tldr": "本文提出了一种名为VEGA（视觉情感引导锚定）的新机制，利用CLIP的图像编码器构建情感视觉锚点，以指导多模态特征对齐，从而显著提升对话中的多模态情感识别性能。", "motivation": "对话中的多模态情感识别由于文本、语音和视觉信号的复杂交互而具有挑战性。现有模型虽然通过高级融合策略提升了性能，但往往缺乏心理学上 M意义的先验知识来指导多模态对齐。", "method": "该研究重新审视了CLIP的应用，并提出VEGA机制。与以往主要利用CLIP文本编码器的方法不同，VEGA利用CLIP的图像编码器，基于面部范例构建情感特定的视觉锚点。这些锚点引导单模态和多模态特征进入一个感知上扎实且心理学上对齐的表示空间。同时，引入随机锚点采样策略以增强鲁棒性，并在一个带有自蒸馏的双分支架构中实现了模型集成。", "result": "集成了VEGA机制的模型在IEMOCAP和MELD数据集上均取得了最先进（SOTA）的性能。", "conclusion": "通过引入视觉情感引导锚定（VEGA）机制，利用CLIP图像编码器和心理学启发，可以有效解决多模态情感识别中特征对齐的挑战，显著提升模型性能。"}}
{"id": "2508.08187", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.08187", "abs": "https://arxiv.org/abs/2508.08187", "authors": ["Swastik Sharma", "Swathi Battula", "Sri Niwas Singh"], "title": "IDSO-Managed Bid-Based Transactive Distribution Systems Design for DER Participation in Wholesale Markets While Preserving T-D Interactions", "comment": "17 Pages, 13 Figures", "summary": "Participation of Distributed Energy Resources (DERs) in bid-based Transactive\nEnergy Systems (TES) at the distribution systems facilitates strongly coupled,\nbidirectional interactions between Transmission-Distribution (T-D) systems.\nCapturing these interactions is critical for ensuring seamless integration\nwithin an Integrated Transmission and Distribution (ITD) framework. This study\nproposes a methodology to preserve such tight T-D linkages by developing an\nIndependent Distribution System Operator (IDSO) managed bid-based TES design\nfor unbalanced distribution systems. The proposed design operates within the\nITD paradigm and permits DER participation in the Wholesale Power Market (WPM)\nthrough IDSO while preserving tight T-D linkages. To this end, this research\noffers the following key contributions: a novel bid/offer\nprequalification-cum-aggregation method to ensure a grid-safe and value-based\naggregation of DERs' bids and offers for WPM participation through IDSO; and a\nretail pricing mechanism that reflects the true value of procuring or offering\nadditional units of power within the distribution system. Case studies are\nconducted on a modified IEEE 123-bus radial feeder populated with a high DER\nconcentration to validate the proposed frameworks' effectiveness in\ncoordinating the DERs efficiently and reliably.", "AI": {"tldr": "本研究提出了一种由独立配电系统运营商 (IDSO) 管理的基于竞价的交易能源系统 (TES) 设计，用于非平衡配电系统，旨在通过IDSO促进分布式能源 (DER) 参与批发电力市场 (WPM)，同时保持输配电 (T-D) 系统之间的紧密联系。", "motivation": "在基于竞价的交易能源系统中，分布式能源的参与导致输配电系统之间产生强耦合的双向互动。在集成输配电 (ITD) 框架内，捕捉这些互动对于确保无缝集成至关重要。", "method": "提出了一种在ITD范式下，由IDSO管理的非平衡配电系统基于竞价的TES设计。该方法包括：1) 一种新颖的竞价/报价预审和聚合方法，确保DER竞价/报价在IDSO协助下安全且有价值地参与WPM；2) 一种零售定价机制，反映配电系统内电力采购或供应的真实价值。", "result": "通过在修改后的IEEE 123节点辐射馈线（高DER渗透率）上进行案例研究，验证了所提出框架在高效可靠协调DER方面的有效性。", "conclusion": "所提出的框架成功实现了DER通过IDSO参与WPM，同时保持了紧密的输配电系统联系，并有效协调了分布式能源，确保了电网安全和价值聚合。"}}
{"id": "2508.06899", "categories": ["cs.AI", "cs.DM"], "pdf": "https://arxiv.org/pdf/2508.06899", "abs": "https://arxiv.org/abs/2508.06899", "authors": ["Yanchen Deng", "Xinrun Wang", "Bo An"], "title": "GDBA Revisited: Unleashing the Power of Guided Local Search for Distributed Constraint Optimization", "comment": null, "summary": "Local search is an important class of incomplete algorithms for solving\nDistributed Constraint Optimization Problems (DCOPs) but it often converges to\npoor local optima. While GDBA provides a comprehensive rule set to escape\npremature convergence, its empirical benefits remain marginal on general-valued\nproblems. In this work, we systematically examine GDBA and identify three\nfactors that potentially lead to its inferior performance, i.e.,\nover-aggressive constraint violation conditions, unbounded penalty\naccumulation, and uncoordinated penalty updates. To address these issues, we\npropose Distributed Guided Local Search (DGLS), a novel GLS framework for DCOPs\nthat incorporates an adaptive violation condition to selectively penalize\nconstraints with high cost, a penalty evaporation mechanism to control the\nmagnitude of penalization, and a synchronization scheme for coordinated penalty\nupdates. We theoretically show that the penalty values are bounded, and agents\nplay a potential game in our DGLS. Our extensive empirical results on various\nstandard benchmarks demonstrate the great superiority of DGLS over\nstate-of-the-art baselines. Particularly, compared to Damped Max-sum with high\ndamping factors (e.g., 0.7 or 0.9), our DGLS achieves competitive performance\non general-valued problems, and outperforms it by significant margins\n(\\textbf{3.77\\%--66.3\\%}) on structured problems in terms of anytime results.", "AI": {"tldr": "本文提出了一种名为分布式引导局部搜索（DGLS）的新型框架，旨在解决分布式约束优化问题（DCOP）中局部搜索算法易陷入劣质局部最优的问题，并通过改进现有GDBA方法的缺陷，显著提升了求解性能。", "motivation": "局部搜索算法在解决分布式约束优化问题（DCOP）时常收敛到劣质局部最优。尽管GDBA旨在避免过早收敛，但其在通用值问题上的经验效益有限。本文系统性地检查了GDBA，并指出了导致其性能不佳的三个潜在因素：过度激进的约束违反条件、无界惩罚累积和非协调的惩罚更新。", "method": "为了解决上述问题，本文提出了分布式引导局部搜索（DGLS），这是一种针对DCOP的新型引导局部搜索（GLS）框架。DGLS整合了：1) 自适应违反条件，选择性地惩罚高成本约束；2) 惩罚蒸发机制，控制惩罚的幅度；3) 同步方案，实现协调的惩罚更新。理论上，本文证明了惩罚值是有界的，并且智能体在DGLS中进行势博弈。", "result": "在各种标准基准测试中，DGLS的广泛实证结果表明其显著优于现有最先进的基线算法。特别地，与高阻尼因子（如0.7或0.9）的Damped Max-sum相比，DGLS在通用值问题上取得了具有竞争力的性能，并在结构化问题上，在任意时间结果方面以3.77%–66.3%的显著优势超越了它。", "conclusion": "DGLS通过解决现有GDBA方法的关键缺陷，提供了一种有效且性能卓越的DCOP求解方法，并在理论和实践中都展现出其优越性，能够有效避免局部最优并提升求解质量。"}}
{"id": "2508.07003", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07003", "abs": "https://arxiv.org/abs/2508.07003", "authors": ["Siyu Chen", "Shenghai Yuan", "Thien-Minh Nguyen", "Zhuyu Huang", "Chenyang Shi", "Jin Jing", "Lihua Xie"], "title": "EGS-SLAM: RGB-D Gaussian Splatting SLAM with Events", "comment": "Accepted by IEEE RAL", "summary": "Gaussian Splatting SLAM (GS-SLAM) offers a notable improvement over\ntraditional SLAM methods, enabling photorealistic 3D reconstruction that\nconventional approaches often struggle to achieve. However, existing GS-SLAM\nsystems perform poorly under persistent and severe motion blur commonly\nencountered in real-world scenarios, leading to significantly degraded tracking\naccuracy and compromised 3D reconstruction quality. To address this limitation,\nwe propose EGS-SLAM, a novel GS-SLAM framework that fuses event data with RGB-D\ninputs to simultaneously reduce motion blur in images and compensate for the\nsparse and discrete nature of event streams, enabling robust tracking and\nhigh-fidelity 3D Gaussian Splatting reconstruction. Specifically, our system\nexplicitly models the camera's continuous trajectory during exposure,\nsupporting event- and blur-aware tracking and mapping on a unified 3D Gaussian\nSplatting scene. Furthermore, we introduce a learnable camera response function\nto align the dynamic ranges of events and images, along with a no-event loss to\nsuppress ringing artifacts during reconstruction. We validate our approach on a\nnew dataset comprising synthetic and real-world sequences with significant\nmotion blur. Extensive experimental results demonstrate that EGS-SLAM\nconsistently outperforms existing GS-SLAM systems in both trajectory accuracy\nand photorealistic 3D Gaussian Splatting reconstruction. The source code will\nbe available at https://github.com/Chensiyu00/EGS-SLAM.", "AI": {"tldr": "EGS-SLAM是一种新颖的GS-SLAM框架，它融合了事件数据和RGB-D输入，以解决传统GS-SLAM在严重运动模糊下的性能问题，实现了鲁棒的跟踪和高保真3D重建。", "motivation": "现有的GS-SLAM系统在持续且严重的运动模糊下表现不佳，导致跟踪精度显著下降和3D重建质量受损，而这在现实世界场景中很常见。", "method": "EGS-SLAM融合了事件数据和RGB-D输入，以同时减少图像中的运动模糊并弥补事件流的稀疏性和离散性。具体方法包括：显式建模曝光期间相机的连续轨迹，支持事件和模糊感知的跟踪和建图；引入可学习的相机响应函数以对齐事件和图像的动态范围；以及引入无事件损失以抑制重建过程中的振铃伪影。", "result": "通过在一个包含合成和真实世界序列的新数据集上进行广泛实验，EGS-SLAM在轨迹精度和逼真3D高斯飞溅重建方面均持续优于现有GS-SLAM系统。", "conclusion": "EGS-SLAM通过有效融合事件数据和RGB-D输入，成功解决了GS-SLAM在运动模糊条件下的挑战，实现了鲁棒的跟踪和高保真3D重建，显著提升了系统性能。"}}
{"id": "2508.06877", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06877", "abs": "https://arxiv.org/abs/2508.06877", "authors": ["Xiaobo Zhang", "Congqing He", "Ying He", "Jian Peng", "Dajie Fu", "Tien-Ping Tan"], "title": "ESNERA: Empirical and semantic named entity alignment for named entity dataset merging", "comment": "30 pages, 12 figures", "summary": "Named Entity Recognition (NER) is a fundamental task in natural language\nprocessing. It remains a research hotspot due to its wide applicability across\ndomains. Although recent advances in deep learning have significantly improved\nNER performance, they rely heavily on large, high-quality annotated datasets.\nHowever, building these datasets is expensive and time-consuming, posing a\nmajor bottleneck for further research. Current dataset merging approaches\nmainly focus on strategies like manual label mapping or constructing label\ngraphs, which lack interpretability and scalability. To address this, we\npropose an automatic label alignment method based on label similarity. The\nmethod combines empirical and semantic similarities, using a greedy pairwise\nmerging strategy to unify label spaces across different datasets. Experiments\nare conducted in two stages: first, merging three existing NER datasets into a\nunified corpus with minimal impact on NER performance; second, integrating this\ncorpus with a small-scale, self-built dataset in the financial domain. The\nresults show that our method enables effective dataset merging and enhances NER\nperformance in the low-resource financial domain. This study presents an\nefficient, interpretable, and scalable solution for integrating multi-source\nNER corpora.", "AI": {"tldr": "该研究提出了一种基于标签相似度的自动标签对齐方法，用于高效、可解释地整合多源命名实体识别（NER）数据集，尤其在低资源领域表现出色。", "motivation": "深度学习在NER中表现优异，但严重依赖大规模高质量标注数据集，其构建成本高昂且耗时。现有数据集合并方法缺乏可解释性和可扩展性，成为研究瓶颈。", "method": "提出了一种基于标签相似度的自动标签对齐方法。该方法结合了经验相似度和语义相似度，并采用贪婪的成对合并策略来统一不同数据集的标签空间。", "result": "实验分两阶段进行：首先，将三个现有NER数据集合并为一个统一语料库，对NER性能影响最小；其次，将该统一语料库与金融领域的小规模自建数据集集成。结果表明，该方法能有效合并数据集，并显著提升低资源金融领域的NER性能。", "conclusion": "本研究提供了一种高效、可解释且可扩展的解决方案，用于整合多源NER语料库。"}}
{"id": "2508.06565", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06565", "abs": "https://arxiv.org/abs/2508.06565", "authors": ["Jing Zhang", "Xiaowei Yu", "Minheng Chen", "Lu Zhang", "Tong Chen", "Yan Zhuang", "Chao Cao", "Yanjun Lyu", "Li Su", "Tianming Liu", "Dajiang Zhu"], "title": "Bridging Brain Connectomes and Clinical Reports for Early Alzheimer's Disease Diagnosis", "comment": null, "summary": "Integrating brain imaging data with clinical reports offers a valuable\nopportunity to leverage complementary multimodal information for more effective\nand timely diagnosis in practical clinical settings. This approach has gained\nsignificant attention in brain disorder research, yet a key challenge remains:\nhow to effectively link objective imaging data with subjective text-based\nreports, such as doctors' notes. In this work, we propose a novel framework\nthat aligns brain connectomes with clinical reports in a shared cross-modal\nlatent space at both the subject and connectome levels, thereby enhancing\nrepresentation learning. The key innovation of our approach is that we treat\nbrain subnetworks as tokens of imaging data, rather than raw image patches, to\nalign with word tokens in clinical reports. This enables a more efficient\nidentification of system-level associations between neuroimaging findings and\nclinical observations, which is critical since brain disorders often manifest\nas network-level abnormalities rather than isolated regional alterations. We\napplied our method to mild cognitive impairment (MCI) using the ADNI dataset.\nOur approach not only achieves state-of-the-art predictive performance but also\nidentifies clinically meaningful connectome-text pairs, offering new insights\ninto the early mechanisms of Alzheimer's disease and supporting the development\nof clinically useful multimodal biomarkers.", "AI": {"tldr": "该研究提出一种新颖框架，将脑连接组与临床报告在共享的跨模态潜在空间中对齐，通过将脑子网络视为图像“令牌”来匹配文本词汇，以实现脑疾病的早期诊断和生物标志物开发。", "motivation": "整合脑成像数据与临床报告能为疾病诊断提供互补的多模态信息，但在实际临床中，如何有效连接客观成像数据与主观文本报告（如医生笔记）是一个关键挑战，尤其考虑到脑疾病常表现为网络层面的异常。", "method": "提出一个新颖框架，在受试者和连接组层面，将脑连接组与临床报告对齐到一个共享的跨模态潜在空间中。核心创新是将脑子网络视为成像数据的“令牌”，与临床报告中的词汇令牌对齐，以更有效地识别神经影像发现与临床观察之间的系统级关联。", "result": "将该方法应用于ADNI数据集的轻度认知障碍（MCI）诊断，不仅实现了最先进的预测性能，还识别出具有临床意义的连接组-文本对。", "conclusion": "该方法为阿尔茨海默病早期机制提供了新见解，并支持开发临床上有用的多模态生物标志物。"}}
{"id": "2508.08217", "categories": ["eess.SY", "cs.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2508.08217", "abs": "https://arxiv.org/abs/2508.08217", "authors": ["Jimin Choi", "Max Z. Li"], "title": "Autonomous Air-Ground Vehicle Operations Optimization in Hazardous Environments: A Multi-Armed Bandit Approach", "comment": null, "summary": "Hazardous environments such as chemical spills, radiological zones, and\nbio-contaminated sites pose significant threats to human safety and public\ninfrastructure. Rapid and reliable hazard mitigation in these settings often\nunsafe for humans, calling for autonomous systems that can adaptively sense and\nrespond to evolving risks. This paper presents a decision-making framework for\nautonomous vehicle dispatch in hazardous environments with uncertain and\nevolving risk levels. The system integrates a Bayesian Upper Confidence Bound\n(BUCB) sensing strategy with task-specific vehicle routing problems with\nprofits (VRPP), enabling adaptive coordination of unmanned aerial vehicles\n(UAVs) for hazard sensing and unmanned ground vehicles (UGVs) for cleaning.\nUsing VRPP allows selective site visits under resource constraints by assigning\neach site a visit value that reflects sensing or cleaning priorities.\nSite-level hazard beliefs are maintained through a time-weighted Bayesian\nupdate. BUCB scores guide UAV routing to balance exploration and exploitation\nunder uncertainty, while UGV routes are optimized to maximize expected hazard\nreduction under resource constraints. Simulation results demonstrate that our\nframework reduces the number of dispatch cycles to resolve hazards by around\n30% on average compared to baseline dispatch strategies, underscoring the value\nof uncertainty-aware vehicle dispatch for reliable hazard mitigation.", "AI": {"tldr": "本文提出了一种用于危险环境中自主车辆调度的决策框架，通过整合贝叶斯上置信边界（BUCB）感知策略和带利润的车辆路径问题（VRPP），实现无人机（UAV）感知和无人地面车辆（UGV）清洁的自适应协调，有效降低了危险缓解的调度周期。", "motivation": "化学泄漏、放射区和生物污染场地等危险环境对人类安全和公共基础设施构成重大威胁，且对人类而言通常不安全，因此需要能够自适应感知和响应不断演变风险的自主系统。", "method": "该框架整合了贝叶斯上置信边界（BUCB）感知策略与针对特定任务的带利润车辆路径问题（VRPP）。UAV的路径由BUCB得分指导，以平衡不确定性下的探索和利用；UGV的路径则优化以在资源约束下最大化预期危险减少量。站点级危险信念通过时间加权的贝叶斯更新来维护。", "result": "仿真结果表明，与基线调度策略相比，该框架平均减少了约30%的危险解决调度周期。", "conclusion": "该研究强调了不确定性感知车辆调度对于可靠危险缓解的价值。"}}
{"id": "2508.06931", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06931", "abs": "https://arxiv.org/abs/2508.06931", "authors": ["Wangyue Lu", "Lun Du", "Sirui Li", "Ke Weng", "Haozhe Sun", "Hengyu Liu", "Minghe Yu", "Tiancheng Zhang", "Ge Yu"], "title": "Automated Formalization via Conceptual Retrieval-Augmented LLMs", "comment": null, "summary": "Interactive theorem provers (ITPs) require manual formalization, which is\nlabor-intensive and demands expert knowledge. While automated formalization\noffers a potential solution, it faces two major challenges: model hallucination\n(e.g., undefined predicates, symbol misuse, and version incompatibility) and\nthe semantic gap caused by ambiguous or missing premises in natural language\ndescriptions. To address these issues, we propose CRAMF, a Concept-driven\nRetrieval-Augmented Mathematical Formalization framework. CRAMF enhances\nLLM-based autoformalization by retrieving formal definitions of core\nmathematical concepts, providing contextual grounding during code generation.\nHowever, applying retrieval-augmented generation (RAG) in this setting is\nnon-trivial due to the lack of structured knowledge bases, the polymorphic\nnature of mathematical concepts, and the high precision required in formal\nretrieval. We introduce a framework for automatically constructing a\nconcept-definition knowledge base from Mathlib4, the standard mathematical\nlibrary for the Lean 4 theorem prover, indexing over 26,000 formal definitions\nand 1,000+ core mathematical concepts. To address conceptual polymorphism, we\npropose contextual query augmentation with domain- and application-level\nsignals. In addition, we design a dual-channel hybrid retrieval strategy with\nreranking to ensure accurate and relevant definition retrieval. Experiments on\nminiF2F, ProofNet, and our newly proposed AdvancedMath benchmark show that\nCRAMF can be seamlessly integrated into LLM-based autoformalizers, yielding\nconsistent improvements in translation accuracy, achieving up to 62.1% and an\naverage of 29.9% relative improvement.", "AI": {"tldr": "CRAMF是一个概念驱动的检索增强数学形式化框架，旨在通过检索核心数学概念的正式定义，提高大型语言模型（LLM）自动形式化的准确性，解决幻觉和语义鸿沟问题。", "motivation": "交互式定理证明器（ITPs）需要大量人工形式化工作和专业知识。自动化形式化面临模型幻觉（如未定义谓词、符号误用）和自然语言描述中模糊或缺失前提导致的语义鸿沟问题。", "method": "提出CRAMF框架，通过检索核心数学概念的正式定义为LLM生成提供上下文。从Mathlib4自动构建概念-定义知识库（包含26,000+定义和1,000+概念）。通过领域和应用层信号进行上下文查询增强，以处理概念多态性。设计双通道混合检索策略并进行重排序，确保检索的准确性和相关性。", "result": "CRAMF可无缝集成到基于LLM的自动形式化器中，在miniF2F、ProofNet和AdvancedMath基准测试上持续提高翻译准确性，最高相对改进达62.1%，平均相对改进达29.9%。", "conclusion": "CRAMF通过概念驱动的检索增强方法，有效解决了LLM自动形式化中的幻觉和语义鸿沟问题，显著提高了形式化翻译的准确性。"}}
{"id": "2508.07033", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07033", "abs": "https://arxiv.org/abs/2508.07033", "authors": ["Shengli Zhou", "Xiangchen Wang", "Jinrui Zhang", "Ruozai Tian", "Rongtao Xu", "Feng Zheng"], "title": "$\\mathcal{P}^3$: Toward Versatile Embodied Agents", "comment": "16 pages, 8 figures", "summary": "Embodied agents have shown promising generalization capabilities across\ndiverse physical environments, making them essential for a wide range of\nreal-world applications. However, building versatile embodied agents poses\ncritical challenges due to three key issues: dynamic environment perception,\nopen-ended tool usage, and complex multi-task planning. Most previous works\nrely solely on feedback from tool agents to perceive environmental changes and\ntask status, which limits adaptability to real-time dynamics, causes error\naccumulation, and restricts tool flexibility. Furthermore, multi-task\nscheduling has received limited attention, primarily due to the inherent\ncomplexity of managing task dependencies and balancing competing priorities in\ndynamic and complex environments. To overcome these challenges, we introduce\n$\\mathcal{P}^3$, a unified framework that integrates real-time perception and\ndynamic scheduling. Specifically, $\\mathcal{P}^3$ enables 1) \\textbf Perceive\nrelevant task information actively from the environment, 2) \\textbf Plug and\nutilize any tool without feedback requirement, and 3) \\textbf Plan multi-task\nexecution based on prioritizing urgent tasks and dynamically adjusting task\norder based on dependencies. Extensive real-world experiments show that our\napproach bridges the gap between benchmarks and practical deployment,\ndelivering highly transferable, general-purpose embodied agents. Code and data\nwill be released soon.", "AI": {"tldr": "本文提出了一个名为 $\\mathcal{P}^3$ 的统一框架，通过集成实时感知和动态调度，解决了具身智能体在动态环境感知、开放式工具使用和复杂多任务规划方面的挑战，实现了高度可迁移的通用具身智能体。", "motivation": "现有具身智能体在实际应用中面临三大挑战：动态环境感知、开放式工具使用和复杂多任务规划。多数工作过度依赖工具反馈，导致适应性差、误差累积和工具灵活性受限。此外，多任务调度因其复杂性（任务依赖、优先级平衡）而未得到充分关注。", "method": "本文提出了 $\\mathcal{P}^3$ 框架，其核心是整合实时感知和动态调度。具体包括：1) 主动从环境中感知相关任务信息；2) 无需工具反馈即可即插即用任何工具；3) 基于紧急任务优先级和动态调整任务顺序来规划多任务执行。", "result": "广泛的真实世界实验表明，该方法弥合了基准测试与实际部署之间的差距，成功实现了高度可迁移、通用目的的具身智能体。", "conclusion": "$\\mathcal{P}^3$ 框架通过解决具身智能体在感知、工具使用和多任务规划方面的关键挑战，显著提升了其在复杂动态环境中的适应性和通用性，为实际部署奠定了基础。"}}
{"id": "2508.06880", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.06880", "abs": "https://arxiv.org/abs/2508.06880", "authors": ["Philipp Christmann", "Gerhard Weikum"], "title": "The ReQAP System for Question Answering over Personal Information", "comment": "Accepted at CIKM 2025 (demonstration paper)", "summary": "Personal information is abundant on users' devices, from structured data in\ncalendar, shopping records or fitness tools, to unstructured contents in mail\nand social media posts. This works presents the ReQAP system that supports\nusers with answers for complex questions that involve filters, joins and\naggregation over heterogeneous sources. The unique trait of ReQAP is that it\nrecursively decomposes questions and incrementally builds an operator tree for\nexecution. Both the question interpretation and the individual operators make\nsmart use of light-weight language models, with judicious fine-tuning. The demo\nshowcases the rich functionality for advanced user questions, and also offers\ndetailed tracking of how the answers are computed by the operators in the\nexecution tree. Being able to trace answers back to the underlying sources is\nvital for human comprehensibility and user trust in the system.", "AI": {"tldr": "ReQAP系统通过递归分解和增量构建操作树，利用轻量级语言模型，帮助用户回答涉及过滤、连接和聚合的复杂个人异构数据问题，并提供详细的答案溯源。", "motivation": "用户的设备上存在大量异构的个人信息（结构化和非结构化），但缺乏有效工具来回答涉及这些复杂数据的多源、多操作（如过滤、连接、聚合）的复杂问题。", "method": "ReQAP系统采用递归分解问题的方法，并逐步构建一个用于执行的操作树。在问题解释和各个操作符层面，系统都巧妙地利用了经过审慎微调的轻量级语言模型。演示版展示了其丰富的功能，并能详细追踪答案是如何通过执行树中的操作符计算出来的。", "result": "ReQAP系统能够支持用户对异构个人数据源提出包含过滤、连接和聚合的复杂问题，并提供答案。系统演示展示了其强大的功能，并能提供详细的答案计算过程追踪，使得答案可追溯到其底层数据源。", "conclusion": "ReQAP系统为处理复杂个人数据查询提供了一个有效的解决方案，通过其独特的递归分解和操作树构建方法，结合语言模型，提高了系统对复杂问题的处理能力。答案溯源功能对于增强用户理解和信任至关重要。"}}
{"id": "2508.06566", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06566", "abs": "https://arxiv.org/abs/2508.06566", "authors": ["Manish Kansana", "Elias Hossain", "Shahram Rahimi", "Noorbakhsh Amiri Golilarz"], "title": "Surformer v1: Transformer-Based Surface Classification Using Tactile and Vision Features", "comment": null, "summary": "Surface material recognition is a key component in robotic perception and\nphysical interaction, particularly when leveraging both tactile and visual\nsensory inputs. In this work, we propose Surformer v1, a transformer-based\narchitecture designed for surface classification using structured tactile\nfeatures and PCA-reduced visual embeddings extracted via ResNet-50. The model\nintegrates modality-specific encoders with cross-modal attention layers,\nenabling rich interactions between vision and touch. Currently,\nstate-of-the-art deep learning models for vision tasks have achieved remarkable\nperformance. With this in mind, our first set of experiments focused\nexclusively on tactile-only surface classification. Using feature engineering,\nwe trained and evaluated multiple machine learning models, assessing their\naccuracy and inference time. We then implemented an encoder-only Transformer\nmodel tailored for tactile features. This model not only achieved the highest\naccuracy but also demonstrated significantly faster inference time compared to\nother evaluated models, highlighting its potential for real-time applications.\nTo extend this investigation, we introduced a multimodal fusion setup by\ncombining vision and tactile inputs. We trained both Surformer v1 (using\nstructured features) and Multimodal CNN (using raw images) to examine the\nimpact of feature-based versus image-based multimodal learning on\nclassification accuracy and computational efficiency. The results showed that\nSurformer v1 achieved 99.4% accuracy with an inference time of 0.77 ms, while\nthe Multimodal CNN achieved slightly higher accuracy but required significantly\nmore inference time. These findings suggest Surformer v1 offers a compelling\nbalance between accuracy, efficiency, and computational cost for surface\nmaterial recognition.", "AI": {"tldr": "本文提出了Surformer v1，一个基于Transformer的架构，用于利用结构化触觉特征和降维视觉嵌入进行表面材料分类，并在准确性和效率之间取得了良好平衡。", "motivation": "表面材料识别是机器人感知和物理交互的关键组成部分，尤其是在结合触觉和视觉输入时。当前深度学习模型在视觉任务上表现出色，但如何有效融合多模态信息以实现高效且准确的表面识别仍是挑战。", "method": "本文提出了Surformer v1，一个基于Transformer的架构，用于表面分类。它使用结构化触觉特征和通过ResNet-50提取的PCA降维视觉嵌入。模型集成模态特定编码器和跨模态注意力层。实验分两部分：首先，专注于纯触觉表面分类，比较了多种机器学习模型和定制的编码器Transformer；其次，引入多模态融合设置，比较了Surformer v1（使用结构化特征）和多模态CNN（使用原始图像）在分类准确性和计算效率上的表现。", "result": "在纯触觉分类中，定制的编码器Transformer模型实现了最高精度和显著更快的推理时间。在多模态融合设置中，Surformer v1实现了99.4%的准确率，推理时间为0.77毫秒；而多模态CNN虽然精度略高，但推理时间显著更长。", "conclusion": "研究结果表明，Surformer v1在表面材料识别方面，在准确性、效率和计算成本之间提供了令人信服的平衡。"}}
{"id": "2508.07045", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.07045", "abs": "https://arxiv.org/abs/2508.07045", "authors": ["Dennis Benders", "Johannes Köhler", "Robert Babuška", "Javier Alonso-Mora", "Laura Ferranti"], "title": "From Data to Safe Mobile Robot Navigation: An Efficient and Modular Robust MPC Design Pipeline", "comment": "8 pages, 5 figures", "summary": "Model predictive control (MPC) is a powerful strategy for planning and\ncontrol in autonomous mobile robot navigation. However, ensuring safety in\nreal-world deployments remains challenging due to the presence of disturbances\nand measurement noise. Existing approaches often rely on idealized assumptions,\nneglect the impact of noisy measurements, and simply heuristically guess\nunrealistic bounds. In this work, we present an efficient and modular robust\nMPC design pipeline that systematically addresses these limitations. The\npipeline consists of an iterative procedure that leverages closed-loop\nexperimental data to estimate disturbance bounds and synthesize a robust\noutput-feedback MPC scheme. We provide the pipeline in the form of\ndeterministic and reproducible code to synthesize the robust output-feedback\nMPC from data. We empirically demonstrate robust constraint satisfaction and\nrecursive feasibility in quadrotor simulations using Gazebo.", "AI": {"tldr": "本文提出了一种高效且模块化的鲁棒模型预测控制（MPC）设计流程，通过利用闭环实验数据来估计扰动边界并合成鲁棒输出反馈MPC，以解决自主移动机器人导航中存在扰动和测量噪声时的安全挑战。", "motivation": "模型预测控制（MPC）在自主移动机器人导航中是一种强大的规划和控制策略，但由于存在扰动和测量噪声，在实际部署中确保安全性仍然具有挑战性。现有方法常依赖理想化假设，忽略噪声测量影响，或启发式猜测不切实际的边界。", "method": "提出一个高效且模块化的鲁棒MPC设计流程。该流程是一个迭代过程，利用闭环实验数据来估计扰动边界，并合成一个鲁棒的输出反馈MPC方案。同时提供确定性和可复现的代码，用于从数据中合成鲁棒输出反馈MPC。", "result": "在Gazebo中的四旋翼飞行器仿真中，经验性地证明了所提出的方法能够实现鲁棒的约束满足和递归可行性。", "conclusion": "该工作提供了一个系统性的、数据驱动的鲁棒MPC设计流程，有效解决了现有MPC方法在处理真实世界扰动和测量噪声时的局限性，从而提高了自主移动机器人导航的安全性。"}}
{"id": "2508.06939", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06939", "abs": "https://arxiv.org/abs/2508.06939", "authors": ["Hiba Najjar", "Deepak Pathak", "Marlon Nuske", "Andreas Dengel"], "title": "Intrinsic Explainability of Multimodal Learning for Crop Yield Prediction", "comment": null, "summary": "Multimodal learning enables various machine learning tasks to benefit from\ndiverse data sources, effectively mimicking the interplay of different factors\nin real-world applications, particularly in agriculture. While the\nheterogeneous nature of involved data modalities may necessitate the design of\ncomplex architectures, the model interpretability is often overlooked. In this\nstudy, we leverage the intrinsic explainability of Transformer-based models to\nexplain multimodal learning networks, focusing on the task of crop yield\nprediction at the subfield level. The large datasets used cover various crops,\nregions, and years, and include four different input modalities: multispectral\nsatellite and weather time series, terrain elevation maps and soil properties.\nBased on the self-attention mechanism, we estimate feature attributions using\ntwo methods, namely the Attention Rollout (AR) and Generic Attention (GA), and\nevaluate their performance against Shapley-based model-agnostic estimations,\nShapley Value Sampling (SVS). Additionally, we propose the Weighted Modality\nActivation (WMA) method to assess modality attributions and compare it with SVS\nattributions. Our findings indicate that Transformer-based models outperform\nother architectures, specifically convolutional and recurrent networks,\nachieving R2 scores that are higher by 0.10 and 0.04 at the subfield and field\nlevels, respectively. AR is shown to provide more robust and reliable temporal\nattributions, as confirmed through qualitative and quantitative evaluation,\ncompared to GA and SVS values. Information about crop phenology stages was\nleveraged to interpret the explanation results in the light of established\nagronomic knowledge. Furthermore, modality attributions revealed varying\npatterns across the two methods compared.[...]", "AI": {"tldr": "本研究利用Transformer模型进行多模态作物产量预测，并结合自注意力机制（如AR和GA）和提出的WMA方法提升模型可解释性，结果表明Transformer表现优异且AR提供更可靠的解释。", "motivation": "多模态学习在农业应用中（特别是作物产量预测）具有潜力，但其复杂性常导致模型可解释性被忽视。研究旨在解决这一问题，提升多模态模型在农业领域的透明度。", "method": "采用基于Transformer的模型进行亚田块级别作物产量预测。输入数据包括多光谱卫星图像、天气时间序列、地形高程图和土壤属性。利用自注意力机制，通过Attention Rollout (AR) 和 Generic Attention (GA) 两种方法估计特征归因，并与基于Shapley的Shapley Value Sampling (SVS) 进行比较。此外，提出Weighted Modality Activation (WMA) 方法评估模态归因，并与SVS归因进行对比。解释结果结合作物物候期进行解读。", "result": "Transformer模型在亚田块和田块级别上优于卷积网络和循环网络，R2分数分别高出0.10和0.04。AR方法在定性和定量评估中均显示出比GA和SVS更鲁棒和可靠的时间归因。模态归因在不同方法（WMA和SVS）之间呈现出不同的模式。", "conclusion": "Transformer模型在多模态作物产量预测任务中表现卓越，其固有的可解释性可以通过自注意力机制（特别是AR）得到有效利用，为理解模型决策提供了可靠的依据，并与农学知识相符。"}}
{"id": "2508.07079", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.07079", "abs": "https://arxiv.org/abs/2508.07079", "authors": ["Mohamed Parvez Aslam", "Bojan Derajic", "Mohamed-Khalil Bouzidi", "Sebastian Bernhard", "Jan Oliver Ringert"], "title": "Model Predictive Control for Crowd Navigation via Learning-Based Trajectory Prediction", "comment": null, "summary": "Safe navigation in pedestrian-rich environments remains a key challenge for\nautonomous robots. This work evaluates the integration of a deep learning-based\nSocial-Implicit (SI) pedestrian trajectory predictor within a Model Predictive\nControl (MPC) framework on the physical Continental Corriere robot. Tested\nacross varied pedestrian densities, the SI-MPC system is compared to a\ntraditional Constant Velocity (CV) model in both open-loop prediction and\nclosed-loop navigation. Results show that SI improves trajectory prediction -\nreducing errors by up to 76% in low-density settings - and enhances safety and\nmotion smoothness in crowded scenes. Moreover, real-world deployment reveals\ndiscrepancies between open-loop metrics and closed-loop performance, as the SI\nmodel yields broader, more cautious predictions. These findings emphasize the\nimportance of system-level evaluation and highlight the SI-MPC framework's\npromise for safer, more adaptive navigation in dynamic, human-populated\nenvironments.", "AI": {"tldr": "该研究将基于深度学习的社会隐式（SI）行人轨迹预测器与模型预测控制（MPC）框架结合，在物理机器人上进行验证。结果表明，SI模型显著提高了轨迹预测精度，并在拥挤场景中增强了导航安全性和运动平滑性。", "motivation": "自动机器人在行人密集环境中实现安全导航仍然是一个关键挑战。", "method": "研究将深度学习的社会隐式（SI）行人轨迹预测器集成到模型预测控制（MPC）框架中，并在Continental Corriere物理机器人上进行测试。通过在不同行人密度下，将SI-MPC系统与传统的恒定速度（CV）模型在开环预测和闭环导航中进行比较评估。", "result": "SI模型显著改善了轨迹预测，在低密度环境下将误差降低了高达76%。在拥挤场景中，SI模型增强了导航的安全性和运动平滑性。此外，实际部署揭示了开环指标与闭环性能之间的差异，因为SI模型产生了更广泛、更谨慎的预测。", "conclusion": "研究结果强调了系统级评估的重要性，并突出了SI-MPC框架在动态、有人环境中实现更安全、更自适应导航的巨大潜力。"}}
{"id": "2508.06886", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06886", "abs": "https://arxiv.org/abs/2508.06886", "authors": ["Arpita Saggar", "Jonathan C. Darling", "Vania Dimitrova", "Duygu Sarikaya", "David C. Hogg"], "title": "Score Before You Speak: Improving Persona Consistency in Dialogue Generation using Response Quality Scores", "comment": "Camera-Ready version for ECAI 2025. 8 pages", "summary": "Persona-based dialogue generation is an important milestone towards building\nconversational artificial intelligence. Despite the ever-improving capabilities\nof large language models (LLMs), effectively integrating persona fidelity in\nconversations remains challenging due to the limited diversity in existing\ndialogue data. We propose a novel framework SBS (Score-Before-Speaking), which\noutperforms previous methods and yields improvements for both million and\nbillion-parameter models. Unlike previous methods, SBS unifies the learning of\nresponses and their relative quality into a single step. The key innovation is\nto train a dialogue model to correlate augmented responses with a quality score\nduring training and then leverage this knowledge at inference. We use\nnoun-based substitution for augmentation and semantic similarity-based scores\nas a proxy for response quality. Through extensive experiments with benchmark\ndatasets (PERSONA-CHAT and ConvAI2), we show that score-conditioned training\nallows existing models to better capture a spectrum of persona-consistent\ndialogues. Our ablation studies also demonstrate that including scores in the\ninput prompt during training is superior to conventional training setups. Code\nand further details are available at\nhttps://arpita2512.github.io/score_before_you_speak", "AI": {"tldr": "本文提出SBS框架，通过统一学习回复及其质量分数来提升对话模型在有限数据下生成更具个性化特征对话的能力。", "motivation": "尽管大型语言模型（LLMs）能力不断提升，但由于现有对话数据多样性有限，有效整合个性化特征（persona fidelity）到对话中仍然具有挑战性。", "method": "提出SBS（Score-Before-Speaking）框架，将回复学习和其相对质量评估统一在一个步骤中。关键创新在于训练对话模型将增强的回复与质量分数关联起来，并在推理时利用这些知识。使用基于名词的替换进行数据增强，并以语义相似度分数作为回复质量的代理。", "result": "SBS框架优于现有方法，并对百万和亿级参数模型均有改进。分数条件训练使现有模型能更好地捕捉一系列符合个性化特征的对话。消融实验表明，在训练过程中将分数包含在输入提示中优于传统训练设置。", "conclusion": "SBS框架通过在训练中引入质量分数，有效解决了现有LLMs在个性化对话生成中数据多样性不足的问题，显著提升了模型生成符合个性化特征对话的能力。"}}
{"id": "2508.06570", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06570", "abs": "https://arxiv.org/abs/2508.06570", "authors": ["Mohammad Zia Ur Rehman", "Anukriti Bhatnagar", "Omkar Kabde", "Shubhi Bansal", "Nagendra Kumar"], "title": "ImpliHateVid: A Benchmark Dataset and Two-stage Contrastive Learning Framework for Implicit Hate Speech Detection in Videos", "comment": "Published in ACL 2025", "summary": "The existing research has primarily focused on text and image-based hate\nspeech detection, video-based approaches remain underexplored. In this work, we\nintroduce a novel dataset, ImpliHateVid, specifically curated for implicit hate\nspeech detection in videos. ImpliHateVid consists of 2,009 videos comprising\n509 implicit hate videos, 500 explicit hate videos, and 1,000 non-hate videos,\nmaking it one of the first large-scale video datasets dedicated to implicit\nhate detection. We also propose a novel two-stage contrastive learning\nframework for hate speech detection in videos. In the first stage, we train\nmodality-specific encoders for audio, text, and image using contrastive loss by\nconcatenating features from the three encoders. In the second stage, we train\ncross-encoders using contrastive learning to refine multimodal representations.\nAdditionally, we incorporate sentiment, emotion, and caption-based features to\nenhance implicit hate detection. We evaluate our method on two datasets,\nImpliHateVid for implicit hate speech detection and another dataset for general\nhate speech detection in videos, HateMM dataset, demonstrating the\neffectiveness of the proposed multimodal contrastive learning for hateful\ncontent detection in videos and the significance of our dataset.", "AI": {"tldr": "该研究引入了首个大规模视频隐性仇恨言论检测数据集ImpliHateVid，并提出了一个两阶段对比学习框架，用于视频中的仇恨言论检测，尤其针对隐性仇恨。", "motivation": "现有研究主要集中在文本和图像的仇恨言论检测，视频领域研究不足，尤其是视频中的隐性仇恨言论检测。", "method": "1. 构建了包含2009个视频的新数据集ImpliHateVid，涵盖隐性仇恨、显性仇恨和非仇恨视频。2. 提出了一个两阶段对比学习框架：第一阶段使用对比损失训练音频、文本、图像的模态特定编码器；第二阶段训练交叉编码器以提炼多模态表示。3. 额外整合了情感、情绪和字幕特征来增强隐性仇恨检测。", "result": "所提出的多模态对比学习方法在ImpliHateVid（用于隐性仇恨检测）和HateMM（用于通用仇恨检测）两个数据集上均表现出有效性，证明了数据集和方法的显著性。", "conclusion": "该研究成功引入了专用于视频隐性仇恨检测的大规模数据集，并提出了有效的多模态对比学习框架，显著推动了视频仇恨内容检测领域的发展。"}}
{"id": "2508.07323", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.07323", "abs": "https://arxiv.org/abs/2508.07323", "authors": ["Adeetya Uppal", "Rakesh Kumar Sahoo", "Manoranjan Sinha"], "title": "Collision-Free Trajectory Planning and control of Robotic Manipulator using Energy-Based Artificial Potential Field (E-APF)", "comment": null, "summary": "Robotic trajectory planning in dynamic and cluttered environments remains a\ncritical challenge, particularly when striving for both time efficiency and\nmotion smoothness under actuation constraints. Traditional path planner, such\nas Artificial Potential Field (APF), offer computational efficiency but suffer\nfrom local minima issue due to position-based potential field functions and\noscillatory motion near the obstacles due to Newtonian mechanics. To address\nthis limitation, an Energy-based Artificial Potential Field (APF) framework is\nproposed in this paper that integrates position and velocity-dependent\npotential functions. E-APF ensures dynamic adaptability and mitigates local\nminima, enabling uninterrupted progression toward the goal. The proposed\nframework integrates E-APF with a hybrid trajectory optimizer that jointly\nminimizes jerk and execution time under velocity and acceleration constraints,\nensuring geometric smoothness and time efficiency. The entire framework is\nvalidated in simulation using the 7-degree-of-freedom Kinova Gen3 robotic\nmanipulator. The results demonstrate collision-free, smooth, time-efficient,\nand oscillation-free trajectory in the presence of obstacles, highlighting the\nefficacy of the combined trajectory optimization and real-time obstacle\navoidance approach. This work lays the foundation for future integration with\nreactive control strategies and physical hardware deployment in real-world\nmanipulation tasks.", "AI": {"tldr": "本文提出了一种基于能量的人工势场（E-APF）框架，结合混合轨迹优化器，用于在动态杂乱环境中生成无碰撞、平滑且高效的机器人轨迹，解决了传统APF的局部最小值和振荡问题。", "motivation": "在动态杂乱环境中进行机器人轨迹规划，同时实现时间效率和运动平滑性，并满足驱动约束，是一个关键挑战。传统APF算法虽然计算高效，但存在局部最小值问题和障碍物附近的牛顿力学导致的振荡运动。", "method": "提出了一种基于能量的人工势场（E-APF）框架，该框架集成了与位置和速度相关的势函数，以确保动态适应性并缓解局部最小值。将E-APF与混合轨迹优化器相结合，共同最小化加加速度（jerk）和执行时间，并考虑速度和加速度约束，以确保几何平滑性和时间效率。该框架通过7自由度Kinova Gen3机器人机械手在仿真中进行了验证。", "result": "结果表明，在存在障碍物的情况下，该方法能够生成无碰撞、平滑、时间高效且无振荡的轨迹。这突显了结合轨迹优化和实时避障方法的有效性。", "conclusion": "该工作为未来与反应式控制策略集成以及在现实世界操作任务中进行物理硬件部署奠定了基础。"}}
{"id": "2508.06950", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06950", "abs": "https://arxiv.org/abs/2508.06950", "authors": ["Sarah Schröder", "Thekla Morgenroth", "Ulrike Kuhl", "Valerie Vaquet", "Benjamin Paaßen"], "title": "Large Language Models Do Not Simulate Human Psychology", "comment": null, "summary": "Large Language Models (LLMs),such as ChatGPT, are increasingly used in\nresearch, ranging from simple writing assistance to complex data annotation\ntasks. Recently, some research has suggested that LLMs may even be able to\nsimulate human psychology and can, hence, replace human participants in\npsychological studies. We caution against this approach. We provide conceptual\narguments against the hypothesis that LLMs simulate human psychology. We then\npresent empiric evidence illustrating our arguments by demonstrating that\nslight changes to wording that correspond to large changes in meaning lead to\nnotable discrepancies between LLMs' and human responses, even for the recent\nCENTAUR model that was specifically fine-tuned on psychological responses.\nAdditionally, different LLMs show very different responses to novel items,\nfurther illustrating their lack of reliability. We conclude that LLMs do not\nsimulate human psychology and recommend that psychological researchers should\ntreat LLMs as useful but fundamentally unreliable tools that need to be\nvalidated against human responses for every new application.", "AI": {"tldr": "该研究驳斥了大型语言模型（LLMs）能够模拟人类心理的观点，并通过概念论证和实证证据证明LLMs在心理学研究中不可靠，建议每次应用都需与人类反应进行验证。", "motivation": "有研究提出大型语言模型（LLMs）能够模拟人类心理，甚至可能取代心理学研究中的人类参与者。", "method": ["提供概念性论证来反驳LLMs模拟人类心理的假设。", "通过实证证据，展示即使是专门针对心理学响应进行微调的CENTAUR模型，微小的措辞变化（导致意义发生大变化）也会导致LLMs和人类响应之间出现显著差异。", "展示不同的LLMs对新颖项目表现出非常不同的响应，进一步说明其缺乏可靠性。"], "result": ["LLMs（包括CENTAUR模型）对微小措辞变化（对应意义大变）的反应与人类响应存在显著差异。", "不同的LLMs对新颖项目的反应差异很大，这表明它们缺乏可靠性。", "LLMs无法模拟人类心理。"], "conclusion": "LLMs不能模拟人类心理。心理学研究人员应将LLMs视为有用但根本上不可靠的工具，每次新的应用都需要与人类反应进行验证。"}}
{"id": "2508.07080", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07080", "abs": "https://arxiv.org/abs/2508.07080", "authors": ["Haolin Liu", "Zijun Guo", "Yanbo Chen", "Jiaqi Chen", "Huilong Yu", "Junqiang Xi"], "title": "An Evolutionary Game-Theoretic Merging Decision-Making Considering Social Acceptance for Autonomous Driving", "comment": null, "summary": "Highway on-ramp merging is of great challenge for autonomous vehicles (AVs),\nsince they have to proactively interact with surrounding vehicles to enter the\nmain road safely within limited time. However, existing decision-making\nalgorithms fail to adequately address dynamic complexities and social\nacceptance of AVs, leading to suboptimal or unsafe merging decisions. To\naddress this, we propose an evolutionary game-theoretic (EGT) merging\ndecision-making framework, grounded in the bounded rationality of human\ndrivers, which dynamically balances the benefits of both AVs and main-road\nvehicles (MVs). We formulate the cut-in decision-making process as an EGT\nproblem with a multi-objective payoff function that reflects human-like driving\npreferences. By solving the replicator dynamic equation for the evolutionarily\nstable strategy (ESS), the optimal cut-in timing is derived, balancing\nefficiency, comfort, and safety for both AVs and MVs. A real-time driving style\nestimation algorithm is proposed to adjust the game payoff function online by\nobserving the immediate reactions of MVs. Empirical results demonstrate that we\nimprove the efficiency, comfort and safety of both AVs and MVs compared with\nexisting game-theoretic and traditional planning approaches across multi-object\nmetrics.", "AI": {"tldr": "针对自动驾驶车辆汇入高速公路匝道面临的动态复杂性和社会接受度挑战，本文提出了一种基于演化博弈论（EGT）的决策框架，该框架考虑了人类驾驶员的有限理性，通过求解复制器动态方程来平衡自动驾驶车辆和主路车辆的利益，以获得最佳切入时机，并实时调整博弈支付函数。", "motivation": "现有的自动驾驶车辆决策算法未能充分解决汇入匝道时的动态复杂性和社会接受度问题，导致次优或不安全的汇入决策。", "method": "提出了一种基于演化博弈论（EGT）的汇入决策框架，该框架基于人类驾驶员的有限理性。将切入决策过程建模为一个多目标支付函数的EGT问题，以反映类人驾驶偏好。通过求解复制器动态方程得到演化稳定策略（ESS），从而推导出最佳切入时机。同时，提出了一种实时驾驶风格估计算法，通过观察主路车辆的即时反应来在线调整博弈支付函数。", "result": "与现有博弈论和传统规划方法相比，所提出的框架在多目标指标上提高了自动驾驶车辆和主路车辆的效率、舒适性和安全性。", "conclusion": "该基于演化博弈论的决策框架，通过考虑人类驾驶员的有限理性和实时适应性，为自动驾驶车辆在高速公路匝道汇入提供了更优的解决方案，有效平衡了效率、舒适性和安全性。"}}
{"id": "2508.06913", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06913", "abs": "https://arxiv.org/abs/2508.06913", "authors": ["Siyuan Li", "Xi Lin", "Guangyan Li", "Zehao Liu", "Aodu Wulianghai", "Li Ding", "Jun Wu", "Jianhua Li"], "title": "Model-Agnostic Sentiment Distribution Stability Analysis for Robust LLM-Generated Texts Detection", "comment": null, "summary": "The rapid advancement of large language models (LLMs) has resulted in\nincreasingly sophisticated AI-generated content, posing significant challenges\nin distinguishing LLM-generated text from human-written language. Existing\ndetection methods, primarily based on lexical heuristics or fine-tuned\nclassifiers, often suffer from limited generalizability and are vulnerable to\nparaphrasing, adversarial perturbations, and cross-domain shifts. In this work,\nwe propose SentiDetect, a model-agnostic framework for detecting LLM-generated\ntext by analyzing the divergence in sentiment distribution stability. Our\nmethod is motivated by the empirical observation that LLM outputs tend to\nexhibit emotionally consistent patterns, whereas human-written texts display\ngreater emotional variability. To capture this phenomenon, we define two\ncomplementary metrics: sentiment distribution consistency and sentiment\ndistribution preservation, which quantify stability under sentiment-altering\nand semantic-preserving transformations. We evaluate SentiDetect on five\ndiverse datasets and a range of advanced LLMs,including Gemini-1.5-Pro,\nClaude-3, GPT-4-0613, and LLaMa-3.3. Experimental results demonstrate its\nsuperiority over state-of-the-art baselines, with over 16% and 11% F1 score\nimprovements on Gemini-1.5-Pro and GPT-4-0613, respectively. Moreover,\nSentiDetect also shows greater robustness to paraphrasing, adversarial attacks,\nand text length variations, outperforming existing detectors in challenging\nscenarios.", "AI": {"tldr": "SentiDetect是一种模型无关的LLM生成文本检测框架，通过分析情感分布稳定性差异来区分机器和人类文本，表现出优于现有方法的性能和鲁棒性。", "motivation": "大型语言模型（LLMs）生成内容日益复杂，难以与人类文本区分。现有检测方法（基于词汇启发式或微调分类器）泛化能力有限，易受复述、对抗性扰动和跨领域转移影响。", "method": "提出SentiDetect，一个模型无关的框架，基于LLM输出倾向于表现情感一致性模式而人类文本情感变异性更大的观察。定义了两个互补指标：情感分布一致性（sentiment distribution consistency）和情感分布保留（sentiment distribution preservation），量化在情感改变和语义保留转换下的稳定性。", "result": "在五个多样数据集和多种先进LLM（包括Gemini-1.5-Pro、Claude-3、GPT-4-0613、LLaMa-3.3）上评估，SentiDetect在F1分数上分别比现有基线提高了16%（Gemini-1.5-Pro）和11%（GPT-4-0613）。此外，SentiDetect在复述、对抗性攻击和文本长度变化方面也显示出更强的鲁棒性。", "conclusion": "SentiDetect通过分析情感分布稳定性差异，提供了一种有效且鲁棒的LLM生成文本检测方法，在各种挑战性场景下均优于现有检测器。"}}
{"id": "2508.06623", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06623", "abs": "https://arxiv.org/abs/2508.06623", "authors": ["Sihan Ma", "Qiming Wu", "Ruotong Jiang", "Frank Burns"], "title": "ContextGuard-LVLM: Enhancing News Veracity through Fine-grained Cross-modal Contextual Consistency Verification", "comment": null, "summary": "The proliferation of digital news media necessitates robust methods for\nverifying content veracity, particularly regarding the consistency between\nvisual and textual information. Traditional approaches often fall short in\naddressing the fine-grained cross-modal contextual consistency (FCCC) problem,\nwhich encompasses deeper alignment of visual narrative, emotional tone, and\nbackground information with text, beyond mere entity matching. To address this,\nwe propose ContextGuard-LVLM, a novel framework built upon advanced\nVision-Language Large Models (LVLMs) and integrating a multi-stage contextual\nreasoning mechanism. Our model is uniquely enhanced through reinforced or\nadversarial learning paradigms, enabling it to detect subtle contextual\nmisalignments that evade zero-shot baselines. We extend and augment three\nestablished datasets (TamperedNews-Ent, News400-Ent, MMG-Ent) with new\nfine-grained contextual annotations, including \"contextual sentiment,\" \"visual\nnarrative theme,\" and \"scene-event logical coherence,\" and introduce a\ncomprehensive CTXT (Contextual Coherence) entity type. Extensive experiments\ndemonstrate that ContextGuard-LVLM consistently outperforms state-of-the-art\nzero-shot LVLM baselines (InstructBLIP and LLaVA 1.5) across nearly all\nfine-grained consistency tasks, showing significant improvements in complex\nlogical reasoning and nuanced contextual understanding. Furthermore, our model\nexhibits superior robustness to subtle perturbations and a higher agreement\nrate with human expert judgments on challenging samples, affirming its efficacy\nin discerning sophisticated forms of context detachment.", "AI": {"tldr": "该论文提出了ContextGuard-LVLM框架，利用大型视觉语言模型（LVLMs）和强化/对抗学习，解决数字新闻媒体中图像与文本之间的细粒度跨模态上下文一致性（FCCC）验证问题，显著优于现有基线模型。", "motivation": "数字新闻媒体的普及需要强大的内容真实性验证方法，特别是视觉与文本信息的一致性。传统方法在处理细粒度跨模态上下文一致性（FCCC）问题（涉及视觉叙事、情感基调和背景信息与文本的深层对齐）方面存在不足。", "method": "提出ContextGuard-LVLM框架，基于先进的视觉语言大模型（LVLMs），并整合多阶段上下文推理机制。通过强化学习或对抗学习范式进行增强，以检测零样本基线难以发现的微妙上下文错位。扩展并增强了三个现有数据集（TamperedNews-Ent, News400-Ent, MMG-Ent），新增了“上下文情感”、“视觉叙事主题”和“场景事件逻辑连贯性”等细粒度上下文标注，并引入了全面的CTXT（上下文连贯性）实体类型。", "result": "ContextGuard-LVLM在几乎所有细粒度一致性任务上持续优于最先进的零样本LVLM基线（InstructBLIP和LLaVA 1.5），在复杂逻辑推理和细致上下文理解方面表现出显著改进。此外，该模型对细微扰动表现出卓越的鲁棒性，并且在具有挑战性的样本上与人类专家判断具有更高的吻合率。", "conclusion": "ContextGuard-LVLM框架在识别复杂形式的上下文脱节方面显示出高效性，有效解决了数字新闻媒体中视觉与文本细粒度上下文一致性的验证难题。"}}
{"id": "2508.07885", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.07885", "abs": "https://arxiv.org/abs/2508.07885", "authors": ["Shoaib Ahmmad", "Zubayer Ahmed Aditto", "Md Mehrab Hossain", "Noushin Yeasmin", "Shorower Hossain"], "title": "Autonomous Navigation of Cloud-Controlled Quadcopters in Confined Spaces Using Multi-Modal Perception and LLM-Driven High Semantic Reasoning", "comment": null, "summary": "This paper introduces an advanced AI-driven perception system for autonomous\nquadcopter navigation in GPS-denied indoor environments. The proposed framework\nleverages cloud computing to offload computationally intensive tasks and\nincorporates a custom-designed printed circuit board (PCB) for efficient sensor\ndata acquisition, enabling robust navigation in confined spaces. The system\nintegrates YOLOv11 for object detection, Depth Anything V2 for monocular depth\nestimation, a PCB equipped with Time-of-Flight (ToF) sensors and an Inertial\nMeasurement Unit (IMU), and a cloud-based Large Language Model (LLM) for\ncontext-aware decision-making. A virtual safety envelope, enforced by\ncalibrated sensor offsets, ensures collision avoidance, while a multithreaded\narchitecture achieves low-latency processing. Enhanced spatial awareness is\nfacilitated by 3D bounding box estimation with Kalman filtering. Experimental\nresults in an indoor testbed demonstrate strong performance, with object\ndetection achieving a mean Average Precision (mAP50) of 0.6, depth estimation\nMean Absolute Error (MAE) of 7.2 cm, only 16 safety envelope breaches across 42\ntrials over approximately 11 minutes, and end-to-end system latency below 1\nsecond. This cloud-supported, high-intelligence framework serves as an\nauxiliary perception and navigation system, complementing state-of-the-art\ndrone autonomy for GPS-denied confined spaces.", "AI": {"tldr": "本文提出了一种先进的AI驱动感知系统，通过云计算、定制PCB和多模态AI模型，实现无人机在无GPS室内环境下的高效安全导航。", "motivation": "在无GPS的室内环境中，无人机导航面临计算密集型任务和狭小空间避障的挑战，需要一个鲁棒、高效的感知与决策系统。", "method": "该系统采用云计算卸载计算任务，集成定制PCB板用于传感器数据采集（ToF和IMU），利用YOLOv11进行目标检测，Depth Anything V2进行单目深度估计，并结合云端LLM进行情境感知决策。通过虚拟安全包络线和卡尔曼滤波进行3D边界框估计，配合多线程架构实现低延迟处理。", "result": "在室内测试中，目标检测mAP50达到0.6，深度估计MAE为7.2 cm，在42次、约11分钟的试验中仅发生16次安全包络线突破，端到端系统延迟低于1秒，表现出强大的性能。", "conclusion": "该云支持的高智能框架可作为辅助感知和导航系统，有效补充现有无人机在无GPS狭窄空间下的自主导航能力。"}}
{"id": "2508.06960", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06960", "abs": "https://arxiv.org/abs/2508.06960", "authors": ["Keyu Li", "Mohan Jiang", "Dayuan Fu", "Yunze Wu", "Xiangkun Hu", "Dequan Wang", "Pengfei Liu"], "title": "DatasetResearch: Benchmarking Agent Systems for Demand-Driven Dataset Discovery", "comment": null, "summary": "The rapid advancement of large language models has fundamentally shifted the\nbottleneck in AI development from computational power to data availability-with\ncountless valuable datasets remaining hidden across specialized repositories,\nresearch appendices, and domain platforms. As reasoning capabilities and deep\nresearch methodologies continue to evolve, a critical question emerges: can AI\nagents transcend conventional search to systematically discover any dataset\nthat meets specific user requirements, enabling truly autonomous demand-driven\ndata curation? We introduce DatasetResearch, the first comprehensive benchmark\nevaluating AI agents' ability to discover and synthesize datasets from 208\nreal-world demands across knowledge-intensive and reasoning-intensive tasks.\nOur tri-dimensional evaluation framework reveals a stark reality: even advanced\ndeep research systems achieve only 22% score on our challenging\nDatasetResearch-pro subset, exposing the vast gap between current capabilities\nand perfect dataset discovery. Our analysis uncovers a fundamental\ndichotomy-search agents excel at knowledge tasks through retrieval breadth,\nwhile synthesis agents dominate reasoning challenges via structured\ngeneration-yet both catastrophically fail on \"corner cases\" outside existing\ndistributions. These findings establish the first rigorous baseline for dataset\ndiscovery agents and illuminate the path toward AI systems capable of finding\nany dataset in the digital universe. Our benchmark and comprehensive analysis\nprovide the foundation for the next generation of self-improving AI systems and\nare publicly available at https://github.com/GAIR-NLP/DatasetResearch.", "AI": {"tldr": "该研究引入了DatasetResearch，一个评估AI代理发现和合成数据集能力的基准，揭示了当前AI在自主数据发现方面与理想状态之间存在巨大差距。", "motivation": "随着大型语言模型的快速发展，AI开发的瓶颈已从计算能力转向数据可用性，大量有价值的数据集散布在专业存储库中。研究旨在探讨AI代理能否超越传统搜索，系统地发现满足用户需求的任何数据集，从而实现自主驱动的数据管理。", "method": "研究引入了DatasetResearch基准，包含208个真实世界的数据需求，涵盖知识密集型和推理密集型任务。采用三维评估框架来衡量AI代理发现和合成数据集的能力。", "result": "评估结果显示，即使是先进的深度研究系统，在挑战性DatasetResearch-pro子集上的得分也仅为22%。分析发现，搜索代理擅长通过检索广度处理知识任务，而合成代理则通过结构化生成在推理挑战中表现更佳，但两者在“边缘情况”（现有分布之外）上均表现不佳。", "conclusion": "这项研究为数据集发现代理建立了第一个严格的基线，揭示了当前AI能力与完美数据集发现之间的巨大差距。研究成果为未来能够发现数字宇宙中任何数据集的AI系统，以及下一代自我改进的AI系统奠定了基础。"}}
{"id": "2508.07118", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07118", "abs": "https://arxiv.org/abs/2508.07118", "authors": ["Aiden Swann", "Alex Qiu", "Matthew Strong", "Angelina Zhang", "Samuel Morstein", "Kai Rayle", "Monroe Kennedy III"], "title": "DexFruit: Dexterous Manipulation and Gaussian Splatting Inspection of Fruit", "comment": "8 pages, 5 figures", "summary": "DexFruit is a robotic manipulation framework that enables gentle, autonomous\nhandling of fragile fruit and precise evaluation of damage. Many fruits are\nfragile and prone to bruising, thus requiring humans to manually harvest them\nwith care. In this work, we demonstrate by using optical tactile sensing,\nautonomous manipulation of fruit with minimal damage can be achieved. We show\nthat our tactile informed diffusion policies outperform baselines in both\nreduced bruising and pick-and-place success rate across three fruits:\nstrawberries, tomatoes, and blackberries. In addition, we introduce FruitSplat,\na novel technique to represent and quantify visual damage in high-resolution 3D\nrepresentation via 3D Gaussian Splatting (3DGS). Existing metrics for measuring\ndamage lack quantitative rigor or require expensive equipment. With FruitSplat,\nwe distill a 2D strawberry mask as well as a 2D bruise segmentation mask into\nthe 3DGS representation. Furthermore, this representation is modular and\ngeneral, compatible with any relevant 2D model. Overall, we demonstrate a 92%\ngrasping policy success rate, up to a 20% reduction in visual bruising, and up\nto an 31% improvement in grasp success rate on challenging fruit compared to\nour baselines across our three tested fruits. We rigorously evaluate this\nresult with over 630 trials. Please checkout our website at\nhttps://dex-fruit.github.io .", "AI": {"tldr": "DexFruit是一个机器人操作框架，能够实现对易碎水果的轻柔、自主处理，并精确评估损伤，通过触觉感知和3D高斯泼溅技术显著减少了水果的擦伤并提高了抓取成功率。", "motivation": "许多水果易碎且易受损，需要人工小心采摘。现有的损伤测量方法缺乏定量严谨性或需要昂贵的设备。", "method": "该研究采用光学触觉感知和触觉信息扩散策略（tactile informed diffusion policies）来实现最小损伤的自主水果操作。此外，引入了FruitSplat技术，利用3D高斯泼溅（3DGS）在高分辨率3D表示中量化和表示视觉损伤，并能将2D草莓掩膜和2D擦伤分割掩膜蒸馏到3DGS表示中。", "result": "触觉信息策略在草莓、番茄和黑莓三种水果上，相比基线显著减少了擦伤，并提高了抓取放置成功率。整体抓取策略成功率达到92%，视觉擦伤减少了高达20%，在处理挑战性水果时抓取成功率提高了高达31%。所有结果均经过630多次试验的严格评估。", "conclusion": "DexFruit框架通过结合触觉感知和创新的3D损伤评估方法，成功实现了对易碎水果的轻柔自主处理和精确损伤量化，显著优于现有基线，在减少水果损伤和提高操作成功率方面表现出色。"}}
{"id": "2508.06971", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.06971", "abs": "https://arxiv.org/abs/2508.06971", "authors": ["Mohamed Basem", "Islam Oshallah", "Ali Hamdi", "Khaled Shaban", "Hozaifa Kassab"], "title": "Two-Stage Quranic QA via Ensemble Retrieval and Instruction-Tuned Answer Extraction", "comment": "8 pages , 4 figures , Accepted in Aiccsa 2025 ,\n  https://conferences.sigappfr.org/aiccsa2025/", "summary": "Quranic Question Answering presents unique challenges due to the linguistic\ncomplexity of Classical Arabic and the semantic richness of religious texts. In\nthis paper, we propose a novel two-stage framework that addresses both passage\nretrieval and answer extraction. For passage retrieval, we ensemble fine-tuned\nArabic language models to achieve superior ranking performance. For answer\nextraction, we employ instruction-tuned large language models with few-shot\nprompting to overcome the limitations of fine-tuning on small datasets. Our\napproach achieves state-of-the-art results on the Quran QA 2023 Shared Task,\nwith a MAP@10 of 0.3128 and MRR@10 of 0.5763 for retrieval, and a pAP@10 of\n0.669 for extraction, substantially outperforming previous methods. These\nresults demonstrate that combining model ensembling and instruction-tuned\nlanguage models effectively addresses the challenges of low-resource question\nanswering in specialized domains.", "AI": {"tldr": "本文提出一个新颖的两阶段框架，结合模型集成和指令微调大语言模型，有效解决了古兰经问答中检索和答案提取的挑战，并在相关任务中取得了最先进的结果。", "motivation": "古兰经问答面临挑战，因为经典阿拉伯语的语言复杂性和宗教文本的语义丰富性。传统的微调方法在小数据集上存在局限性。", "method": "提出一个两阶段框架：1. 段落检索：集成微调的阿拉伯语语言模型以提高排名性能。2. 答案提取：使用指令微调的大语言模型，通过少量样本提示来克服小数据集微调的限制。", "result": "在Quran QA 2023共享任务中取得了最先进的结果：检索MAP@10为0.3128，MRR@10为0.5763；提取pAP@10为0.669，显著优于现有方法。", "conclusion": "结合模型集成和指令微调语言模型能有效解决专业领域低资源问答的挑战。"}}
{"id": "2508.06624", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06624", "abs": "https://arxiv.org/abs/2508.06624", "authors": ["Kexin Yu", "Zihan Xu", "Jialei Xie", "Carter Adams"], "title": "VL-MedGuide: A Visual-Linguistic Large Model for Intelligent and Explainable Skin Disease Auxiliary Diagnosis", "comment": null, "summary": "Accurate diagnosis of skin diseases remains a significant challenge due to\nthe complex and diverse visual features present in dermatoscopic images, often\ncompounded by a lack of interpretability in existing purely visual diagnostic\nmodels. To address these limitations, this study introduces VL-MedGuide\n(Visual-Linguistic Medical Guide), a novel framework leveraging the powerful\nmulti-modal understanding and reasoning capabilities of Visual-Language Large\nModels (LVLMs) for intelligent and inherently interpretable auxiliary diagnosis\nof skin conditions. VL-MedGuide operates in two interconnected stages: a\nMulti-modal Concept Perception Module, which identifies and linguistically\ndescribes dermatologically relevant visual features through sophisticated\nprompt engineering, and an Explainable Disease Reasoning Module, which\nintegrates these concepts with raw visual information via Chain-of-Thought\nprompting to provide precise disease diagnoses alongside transparent\nrationales. Comprehensive experiments on the Derm7pt dataset demonstrate that\nVL-MedGuide achieves state-of-the-art performance in both disease diagnosis\n(83.55% BACC, 80.12% F1) and concept detection (76.10% BACC, 67.45% F1),\nsurpassing existing baselines. Furthermore, human evaluations confirm the high\nclarity, completeness, and trustworthiness of its generated explanations,\nbridging the gap between AI performance and clinical utility by offering\nactionable, explainable insights for dermatological practice.", "AI": {"tldr": "VL-MedGuide是一个利用视觉-语言大模型（LVLMs）进行皮肤病智能辅助诊断的框架，通过多模态概念感知和可解释疾病推理，提供高准确度的诊断和透明的解释，弥补了现有模型的不足。", "motivation": "皮肤病诊断因图像特征复杂多样而充满挑战，现有纯视觉诊断模型缺乏可解释性，限制了其临床应用。研究旨在解决这些局限性，提供可解释的辅助诊断方案。", "method": "本研究提出了VL-MedGuide框架，包含两个阶段：1. 多模态概念感知模块：通过精巧的提示工程（prompt engineering），识别并语言描述皮肤病学相关的视觉特征；2. 可解释疾病推理模块：通过思维链（Chain-of-Thought）提示，将这些概念与原始视觉信息整合，提供精确的疾病诊断和透明的诊断理由。核心技术是利用视觉-语言大模型（LVLMs）的多模态理解和推理能力。", "result": "在Derm7pt数据集上的综合实验表明，VL-MedGuide在疾病诊断（83.55% BACC, 80.12% F1）和概念检测（76.10% BACC, 67.45% F1）方面均达到最先进的性能，超越了现有基线。此外，人工评估证实了其生成解释的高清晰度、完整性和可信度。", "conclusion": "VL-MedGuide通过提供可操作、可解释的见解，成功弥合了AI性能与临床实用性之间的差距，为皮肤科实践提供了有力的辅助诊断工具。"}}
{"id": "2508.08144", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.08144", "abs": "https://arxiv.org/abs/2508.08144", "authors": ["Ganesh Sundaram", "Jonas Ulmen", "Amjad Haider", "Daniel Görges"], "title": "COMponent-Aware Pruning for Accelerated Control Tasks in Latent Space Models", "comment": "Submitted in: The 2026 IEEE/SICE International Symposium on System\n  Integration (SII 2026)", "summary": "The rapid growth of resource-constrained mobile platforms, including mobile\nrobots, wearable systems, and Internet-of-Things devices, has increased the\ndemand for computationally efficient neural network controllers (NNCs) that can\noperate within strict hardware limitations. While deep neural networks (DNNs)\ndemonstrate superior performance in control applications, their substantial\ncomputational complexity and memory requirements present significant barriers\nto practical deployment on edge devices. This paper introduces a comprehensive\nmodel compression methodology that leverages component-aware structured pruning\nto determine the optimal pruning magnitude for each pruning group, ensuring a\nbalance between compression and stability for NNC deployment. Our approach is\nrigorously evaluated on Temporal Difference Model Predictive Control (TD-MPC),\na state-of-the-art model-based reinforcement learning algorithm, with a\nsystematic integration of mathematical stability guarantee properties,\nspecifically Lyapunov criteria. The key contribution of this work lies in\nproviding a principled framework for determining the theoretical limits of\nmodel compression while preserving controller stability. Experimental\nvalidation demonstrates that our methodology successfully reduces model\ncomplexity while maintaining requisite control performance and stability\ncharacteristics. Furthermore, our approach establishes a quantitative boundary\nfor safe compression ratios, enabling practitioners to systematically determine\nthe maximum permissible model reduction before violating critical stability\nproperties, thereby facilitating the confident deployment of compressed NNCs in\nresource-limited environments.", "AI": {"tldr": "本文提出了一种针对资源受限平台神经网络控制器（NNCs）的模型压缩方法，通过组件感知结构化剪枝并结合Lyapunov稳定性准则，在保证控制器稳定性的前提下，确定最优剪枝幅度及压缩极限。", "motivation": "随着移动机器人、可穿戴设备和物联网设备等资源受限平台的快速发展，对计算效率高的神经网络控制器（NNCs）的需求增加。然而，深度神经网络（DNNs）尽管性能优越，但其高计算复杂度和内存需求限制了在边缘设备上的实际部署。", "method": "本文提出了一种全面的模型压缩方法，利用组件感知结构化剪枝来确定每个剪枝组的最佳剪枝幅度，以平衡压缩和稳定性。该方法在时序差分模型预测控制（TD-MPC）上进行了严格评估，并系统地整合了数学稳定性保证特性，特别是Lyapunov准则。", "result": "实验验证表明，该方法成功地降低了模型复杂度，同时保持了必要的控制性能和稳定性特征。此外，该方法建立了安全压缩比的量化边界，使实践者能够系统地确定在不违反关键稳定性属性的前提下，最大允许的模型缩减量。", "conclusion": "本工作提供了一个原则性框架，用于确定模型压缩的理论极限，同时保留控制器稳定性。这有助于在资源受限环境中自信地部署压缩的神经网络控制器。"}}
{"id": "2508.06963", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06963", "abs": "https://arxiv.org/abs/2508.06963", "authors": ["Changqing Li", "Tianlin Li", "Xiaohan Zhang", "Aishan Liu", "Li Pan"], "title": "MASteer: Multi-Agent Adaptive Steer Strategy for End-to-End LLM Trustworthiness Repair", "comment": null, "summary": "Large Language Models (LLMs) face persistent and evolving trustworthiness\nissues, motivating developers to seek automated and flexible repair methods\nthat enable convenient deployment across diverse scenarios. Existing repair\nmethods like supervised fine-tuning (SFT) and reinforcement learning with human\nfeedback (RLHF) are costly and slow, while prompt engineering lacks robustness\nand scalability. Representation engineering, which steers model behavior by\ninjecting targeted concept vectors during inference, offers a lightweight,\ntraining-free alternative. However, current approaches depend on manually\ncrafted samples and fixed steering strategies, limiting automation and\nadaptability. To overcome these challenges, we propose MASteer, the first\nend-to-end framework for trustworthiness repair in LLMs based on representation\nengineering. MASteer integrates two core components: AutoTester, a multi-agent\nsystem that generates diverse, high-quality steer samples tailored to developer\nneeds; and AutoRepairer, which constructs adaptive steering strategies with\nanchor vectors for automated, context-aware strategy selection during\ninference. Experiments on standard and customized trustworthiness tasks show\nMASteer consistently outperforms baselines, improving metrics by 15.36% on\nLLaMA-3.1-8B-Chat and 4.21% on Qwen-3-8B-Chat, while maintaining general model\ncapabilities. MASteer demonstrates strong robustness, generalization, and\npractical value for scalable, efficient trustworthiness repair.", "AI": {"tldr": "本文提出MASteer，一个端到端的大语言模型（LLM）可信度修复框架，利用表征工程，通过自动化样本生成和自适应转向策略，实现高效、可扩展的修复。", "motivation": "LLM面临持续演进的可信度问题。现有修复方法如SFT和RLHF成本高昂且耗时，提示工程缺乏鲁棒性和可扩展性。虽然表征工程是一种轻量级、无需训练的替代方案，但当前方法依赖手动样本和固定策略，限制了自动化和适应性。", "method": "MASteer框架包含两个核心组件：1) AutoTester，一个多智能体系统，用于生成多样化、高质量且符合开发者需求的转向样本；2) AutoRepairer，构建自适应转向策略，通过锚点向量在推理过程中自动选择符合上下文的策略。", "result": "在标准和定制的可信度任务上，MASteer持续优于基线方法。在LLaMA-3.1-8B-Chat上性能提升15.36%，在Qwen-3-8B-Chat上提升4.21%，同时保持了模型通用能力。MASteer表现出强大的鲁棒性、泛化性和实用价值。", "conclusion": "MASteer是首个基于表征工程的LLM可信度端到端修复框架，为可扩展、高效的LLM可信度修复提供了自动化和灵活的解决方案。"}}
{"id": "2508.07163", "categories": ["cs.RO", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2508.07163", "abs": "https://arxiv.org/abs/2508.07163", "authors": ["Kamal Acharya", "Iman Sharifi", "Mehul Lad", "Liang Sun", "Houbing Song"], "title": "Integrating Neurosymbolic AI in Advanced Air Mobility: A Comprehensive Survey", "comment": "9 pages, 4 figures, IJCAI-2025 (accepted)", "summary": "Neurosymbolic AI combines neural network adaptability with symbolic\nreasoning, promising an approach to address the complex regulatory,\noperational, and safety challenges in Advanced Air Mobility (AAM). This survey\nreviews its applications across key AAM domains such as demand forecasting,\naircraft design, and real-time air traffic management. Our analysis reveals a\nfragmented research landscape where methodologies, including Neurosymbolic\nReinforcement Learning, have shown potential for dynamic optimization but still\nface hurdles in scalability, robustness, and compliance with aviation\nstandards. We classify current advancements, present relevant case studies, and\noutline future research directions aimed at integrating these approaches into\nreliable, transparent AAM systems. By linking advanced AI techniques with AAM's\noperational demands, this work provides a concise roadmap for researchers and\npractitioners developing next-generation air mobility solutions.", "AI": {"tldr": "这篇综述探讨了神经符号AI在高级空中交通（AAM）领域的应用，分析了其在应对AAM挑战方面的潜力、现有方法（如神经符号强化学习）及其面临的挑战，并提出了未来的研究方向。", "motivation": "高级空中交通（AAM）面临复杂的监管、运营和安全挑战，而神经符号AI结合了神经网络的适应性和符号推理能力，有望提供解决方案。", "method": "该研究通过综述现有文献，审查了神经符号AI在AAM关键领域（如需求预测、飞机设计和实时空中交通管理）的应用，分析了研究现状，对现有进展进行了分类，并提出了相关的案例研究和未来的研究方向。", "result": "研究发现，神经符号AI在AAM领域的研究格局较为分散；神经符号强化学习等方法在动态优化方面显示出潜力，但在可扩展性、鲁棒性和符合航空标准方面仍面临挑战。", "conclusion": "神经符号AI有望通过结合先进AI技术与AAM的运营需求，集成到可靠、透明的AAM系统中，本研究为开发下一代空中交通解决方案的研究人员和从业者提供了清晰的路线图。"}}
{"id": "2508.06974", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06974", "abs": "https://arxiv.org/abs/2508.06974", "authors": ["Zhijun Tu", "Hanting Chen", "Siqi Liu", "Chuanjian Liu", "Jian Li", "Jie Hu", "Yunhe Wang"], "title": "Rethinking 1-bit Optimization Leveraging Pre-trained Large Language Models", "comment": "16 pages, 5 figures", "summary": "1-bit LLM quantization offers significant advantages in reducing storage and\ncomputational costs. However, existing methods typically train 1-bit LLMs from\nscratch, failing to fully leverage pre-trained models. This results in high\ntraining costs and notable accuracy degradation. We identify that the large gap\nbetween full precision and 1-bit representations makes direct adaptation\ndifficult. In this paper, we introduce a consistent progressive training for\nboth forward and backward, smoothly converting the floating-point weights into\nthe binarized ones. Additionally, we incorporate binary-aware initialization\nand dual-scaling compensation to reduce the difficulty of progressive training\nand improve the performance. Experimental results on LLMs of various sizes\ndemonstrate that our method outperforms existing approaches. Our results show\nthat high-performance 1-bit LLMs can be achieved using pre-trained models,\neliminating the need for expensive training from scratch.", "AI": {"tldr": "该论文提出了一种从预训练模型高效生成1比特大语言模型的方法，通过渐进式训练、二值感知初始化和双尺度补偿，显著降低了训练成本并提高了性能。", "motivation": "现有1比特大语言模型通常从头开始训练，未能充分利用预训练模型，导致训练成本高昂和精度显著下降。主要挑战在于全精度和1比特表示之间存在巨大差距，难以直接适应。", "method": "该方法引入了“一致渐进式训练”（consistent progressive training），平滑地将浮点权重转换为二值权重，适用于前向和后向传播。此外，还结合了“二值感知初始化”（binary-aware initialization）和“双尺度补偿”（dual-scaling compensation）来降低渐进训练的难度并提升性能。", "result": "实验结果表明，该方法在不同尺寸的大语言模型上均优于现有方法。研究表明，可以使用预训练模型实现高性能的1比特大语言模型。", "conclusion": "该研究得出结论，通过利用预训练模型，可以实现高性能的1比特大语言模型，从而避免了从头开始进行昂贵的训练。"}}
{"id": "2508.06625", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06625", "abs": "https://arxiv.org/abs/2508.06625", "authors": ["Shilong Zou", "Yuhang Huang", "Renjiao Yi", "Chenyang Zhu", "Kai Xu"], "title": "CycleDiff: Cycle Diffusion Models for Unpaired Image-to-image Translation", "comment": null, "summary": "We introduce a diffusion-based cross-domain image translator in the absence\nof paired training data. Unlike GAN-based methods, our approach integrates\ndiffusion models to learn the image translation process, allowing for more\ncoverable modeling of the data distribution and performance improvement of the\ncross-domain translation. However, incorporating the translation process within\nthe diffusion process is still challenging since the two processes are not\naligned exactly, i.e., the diffusion process is applied to the noisy signal\nwhile the translation process is conducted on the clean signal. As a result,\nrecent diffusion-based studies employ separate training or shallow integration\nto learn the two processes, yet this may cause the local minimal of the\ntranslation optimization, constraining the effectiveness of diffusion models.\nTo address the problem, we propose a novel joint learning framework that aligns\nthe diffusion and the translation process, thereby improving the global\noptimality. Specifically, we propose to extract the image components with\ndiffusion models to represent the clean signal and employ the translation\nprocess with the image components, enabling an end-to-end joint learning\nmanner. On the other hand, we introduce a time-dependent translation network to\nlearn the complex translation mapping, resulting in effective translation\nlearning and significant performance improvement. Benefiting from the design of\njoint learning, our method enables global optimization of both processes,\nenhancing the optimality and achieving improved fidelity and structural\nconsistency. We have conducted extensive experiments on RGB$\\leftrightarrow$RGB\nand diverse cross-modality translation tasks including\nRGB$\\leftrightarrow$Edge, RGB$\\leftrightarrow$Semantics and\nRGB$\\leftrightarrow$Depth, showcasing better generative performances than the\nstate of the arts.", "AI": {"tldr": "提出了一种基于扩散模型的无配对数据跨域图像翻译方法，通过联合学习框架对扩散和翻译过程进行对齐，并引入时间依赖翻译网络，实现了全局优化和性能提升。", "motivation": "GAN基方法在数据分布建模上存在局限性；将翻译过程融入扩散模型面临挑战，因为扩散作用于噪声信号而翻译作用于干净信号，导致两者不一致。现有扩散基方法采用分离或浅层集成，易陷入局部最优，限制了扩散模型的有效性。", "method": "提出了一种新颖的联合学习框架，用于对齐扩散和翻译过程。具体地，利用扩散模型提取图像组件作为干净信号表示，并在此基础上进行翻译，实现端到端的联合学习。此外，引入了一个时间依赖的翻译网络来学习复杂的翻译映射。", "result": "通过联合学习设计，该方法实现了两个过程的全局优化，显著提高了图像的保真度和结构一致性。在RGB↔RGB和多种跨模态翻译任务（如RGB↔Edge, RGB↔Semantics, RGB↔Depth）上，展现出优于现有SOTA的生成性能。", "conclusion": "所提出的联合学习框架有效解决了扩散和翻译过程的对齐问题，并通过时间依赖翻译网络提升了学习能力，从而在无配对数据跨域图像翻译任务中取得了卓越的性能和全局最优性。"}}
{"id": "2508.06972", "categories": ["cs.AI", "cs.CR", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06972", "abs": "https://arxiv.org/abs/2508.06972", "authors": ["Dan Ivanov", "Tristan Freiberg", "Haruna Isah"], "title": "DSperse: A Framework for Targeted Verification in Zero-Knowledge Machine Learning", "comment": "12 pages, 8 figures, and 10 tables", "summary": "DSperse is a modular framework for distributed machine learning inference\nwith strategic cryptographic verification. Operating within the emerging\nparadigm of distributed zero-knowledge machine learning, DSperse avoids the\nhigh cost and rigidity of full-model circuitization by enabling targeted\nverification of strategically chosen subcomputations. These verifiable\nsegments, or \"slices\", may cover part or all of the inference pipeline, with\nglobal consistency enforced through audit, replication, or economic incentives.\nThis architecture supports a pragmatic form of trust minimization, localizing\nzero-knowledge proofs to the components where they provide the greatest value.\nWe evaluate DSperse using multiple proving systems and report empirical results\non memory usage, runtime, and circuit behavior under sliced and unsliced\nconfigurations. By allowing proof boundaries to align flexibly with the model's\nlogical structure, DSperse supports scalable, targeted verification strategies\nsuited to diverse deployment needs.", "AI": {"tldr": "DSperse是一个模块化框架，用于分布式机器学习推理，通过策略性加密验证实现信任最小化，避免了完整模型电路化的昂贵和僵硬。", "motivation": "在分布式零知识机器学习范式中，完整的模型电路化成本高昂且缺乏灵活性。研究动机在于提供一种更实用、成本更低的方法来实现信任最小化，即通过有选择地验证关键子计算。", "method": "DSperse框架通过将推理管线分解为可验证的“切片”（subcomputations），对这些切片进行有针对性的零知识证明。全局一致性通过审计、复制或经济激励来强制执行。该架构允许证明边界与模型的逻辑结构灵活对齐。", "result": "通过使用多种证明系统对DSperse进行评估，报告了在切片和未切片配置下，内存使用、运行时和电路行为的实证结果。", "conclusion": "DSperse通过允许证明边界与模型结构灵活对齐，支持可扩展、有针对性的验证策略，适用于多样化的部署需求，并实现了务实的信任最小化。"}}
{"id": "2508.07182", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07182", "abs": "https://arxiv.org/abs/2508.07182", "authors": ["Xuesong Li", "Lars Petersson", "Vivien Rolland"], "title": "3D Gaussian Representations with Motion Trajectory Field for Dynamic Scene Reconstruction", "comment": null, "summary": "This paper addresses the challenge of novel-view synthesis and motion\nreconstruction of dynamic scenes from monocular video, which is critical for\nmany robotic applications. Although Neural Radiance Fields (NeRF) and 3D\nGaussian Splatting (3DGS) have demonstrated remarkable success in rendering\nstatic scenes, extending them to reconstruct dynamic scenes remains\nchallenging. In this work, we introduce a novel approach that combines 3DGS\nwith a motion trajectory field, enabling precise handling of complex object\nmotions and achieving physically plausible motion trajectories. By decoupling\ndynamic objects from static background, our method compactly optimizes the\nmotion trajectory field. The approach incorporates time-invariant motion\ncoefficients and shared motion trajectory bases to capture intricate motion\npatterns while minimizing optimization complexity. Extensive experiments\ndemonstrate that our approach achieves state-of-the-art results in both\nnovel-view synthesis and motion trajectory recovery from monocular video,\nadvancing the capabilities of dynamic scene reconstruction.", "AI": {"tldr": "本文提出一种结合3D高斯泼溅（3DGS）和运动轨迹场的动态场景重建方法，实现单目视频的新视角合成和运动轨迹恢复，特别适用于机器人应用。", "motivation": "尽管NeRF和3DGS在静态场景渲染方面表现出色，但将其扩展到动态场景重建仍面临挑战，而这对于许多机器人应用至关重要。", "method": "本方法将3DGS与运动轨迹场结合，能够精确处理复杂物体运动并生成符合物理规律的运动轨迹。通过将动态物体与静态背景解耦，紧凑地优化运动轨迹场。它还引入了时间不变的运动系数和共享的运动轨迹基，以捕捉复杂的运动模式并最小化优化复杂性。", "result": "实验证明，该方法在单目视频的新视角合成和运动轨迹恢复方面均达到最先进水平。", "conclusion": "该方法显著提升了动态场景重建的能力。"}}
{"id": "2508.07017", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.07017", "abs": "https://arxiv.org/abs/2508.07017", "authors": ["Mao Li", "Fred Conrad", "Johann Gagnon-Bartsch"], "title": "Vec2Summ: Text Summarization via Probabilistic Sentence Embeddings", "comment": null, "summary": "We propose Vec2Summ, a novel method for abstractive summarization that frames\nthe task as semantic compression. Vec2Summ represents a document collection\nusing a single mean vector in the semantic embedding space, capturing the\ncentral meaning of the corpus. To reconstruct fluent summaries, we perform\nembedding inversion -- decoding this mean vector into natural language using a\ngenerative language model. To improve reconstruction quality and capture some\ndegree of topical variability, we introduce stochasticity by sampling from a\nGaussian distribution centered on the mean. This approach is loosely analogous\nto bagging in ensemble learning, where controlled randomness encourages more\nrobust and varied outputs. Vec2Summ addresses key limitations of LLM-based\nsummarization methods. It avoids context-length constraints, enables\ninterpretable and controllable generation via semantic parameters, and scales\nefficiently with corpus size -- requiring only $O(d + d^2)$ parameters.\nEmpirical results show that Vec2Summ produces coherent summaries for topically\nfocused, order-invariant corpora, with performance comparable to direct LLM\nsummarization in terms of thematic coverage and efficiency, albeit with less\nfine-grained detail. These results underscore Vec2Summ's potential in settings\nwhere scalability, semantic control, and corpus-level abstraction are\nprioritized.", "AI": {"tldr": "Vec2Summ是一种新的抽象式摘要方法，将任务视为语义压缩，通过将文档集表示为语义嵌入空间中的平均向量并进行嵌入反演来生成摘要，解决了LLM摘要的限制。", "motivation": "现有基于LLM的摘要方法存在上下文长度限制、缺乏可解释性和可控性以及在大规模语料库上扩展效率低下的问题，促使研究者开发一种更高效、可控且可扩展的摘要方法。", "method": "Vec2Summ将文档集合表示为语义嵌入空间中的单个平均向量，捕捉语料库的中心含义。然后，通过嵌入反演，使用生成式语言模型将该平均向量解码为自然语言摘要。为提高重建质量和引入主题变异性，方法通过从以平均向量为中心的 Gaussian 分布中采样引入随机性，类似于集成学习中的bagging。", "result": "Vec2Summ能为主题集中、顺序无关的语料库生成连贯的摘要。在主题覆盖和效率方面，其性能与直接使用LLM进行摘要相当，尽管在细节方面略逊一筹。该方法有效规避了上下文长度限制，支持通过语义参数进行可解释和可控的生成，并能以O(d + d^2)的参数复杂度高效扩展。", "conclusion": "Vec2Summ在需要可扩展性、语义控制和语料库级别抽象的场景中具有巨大潜力，为大规模抽象式摘要提供了一种新颖且高效的途径。"}}
{"id": "2508.06632", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06632", "abs": "https://arxiv.org/abs/2508.06632", "authors": ["Wenpeng Xing", "Jie Chen", "Zaifeng Yang", "Tiancheng Zhao", "Gaolei Li", "Changting Lin", "Yike Guo", "Meng Han"], "title": "CoDe-NeRF: Neural Rendering via Dynamic Coefficient Decomposition", "comment": null, "summary": "Neural Radiance Fields (NeRF) have shown impressive performance in novel view\nsynthesis, but challenges remain in rendering scenes with complex specular\nreflections and highlights. Existing approaches may produce blurry reflections\ndue to entanglement between lighting and material properties, or encounter\noptimization instability when relying on physically-based inverse rendering. In\nthis work, we present a neural rendering framework based on dynamic coefficient\ndecomposition, aiming to improve the modeling of view-dependent appearance. Our\napproach decomposes complex appearance into a shared, static neural basis that\nencodes intrinsic material properties, and a set of dynamic coefficients\ngenerated by a Coefficient Network conditioned on view and illumination. A\nDynamic Radiance Integrator then combines these components to synthesize the\nfinal radiance. Experimental results on several challenging benchmarks suggest\nthat our method can produce sharper and more realistic specular highlights\ncompared to existing techniques. We hope that this decomposition paradigm can\nprovide a flexible and effective direction for modeling complex appearance in\nneural scene representations.", "AI": {"tldr": "本文提出了一种基于动态系数分解的神经渲染框架，旨在改进NeRF在复杂高光和反射场景中的视图依赖外观建模，生成更锐利逼真的效果。", "motivation": "现有NeRF方法在处理复杂镜面反射和高光时，由于光照与材质属性的纠缠，可能产生模糊反射，或者由于依赖基于物理的逆渲染而导致优化不稳定。", "method": "该方法将复杂外观分解为共享的、静态的神经基（编码固有材质属性）和一组由系数网络根据视角和光照生成的动态系数。然后，一个动态辐射积分器将这些组件结合以合成最终的辐射。", "result": "在多个具有挑战性的基准测试中，与现有技术相比，该方法能生成更锐利、更逼真的镜面高光。", "conclusion": "这种分解范式为神经场景表示中复杂外观建模提供了一个灵活且有效的方向。"}}
{"id": "2508.06980", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06980", "abs": "https://arxiv.org/abs/2508.06980", "authors": ["Aswin Paul", "Moein Khajehnejad", "Forough Habibollahi", "Brett J. Kagan", "Adeel Razi"], "title": "Simulating Biological Intelligence: Active Inference with Experiment-Informed Generative Model", "comment": "18 pages, 8 figures", "summary": "With recent and rapid advancements in artificial intelligence (AI),\nunderstanding the foundation of purposeful behaviour in autonomous agents is\ncrucial for developing safe and efficient systems. While artificial neural\nnetworks have dominated the path to AI, recent studies are exploring the\npotential of biologically based systems, such as networks of living biological\nneuronal networks. Along with promises of high power and data efficiency, these\nsystems may also inform more explainable and biologically plausible models. In\nthis work, we propose a framework rooted in active inference, a general theory\nof behaviour, to model decision-making in embodied agents. Using\nexperiment-informed generative models, we simulate decision-making processes in\na simulated game-play environment, mirroring experimental setups that use\nbiological neurons. Our results demonstrate learning in these agents, providing\ninsights into the role of memory-based learning and predictive planning in\nintelligent decision-making. This work contributes to the growing field of\nexplainable AI by offering a biologically grounded and scalable approach to\nunderstanding purposeful behaviour in agents.", "AI": {"tldr": "该研究提出了一个基于主动推理的框架，用于模拟具身智能体中的决策过程，旨在通过生物启发的方法理解智能行为，并促进可解释AI的发展。", "motivation": "随着AI的快速发展，理解自主智能体的目的性行为对于开发安全高效的系统至关重要。传统的神经网络AI存在局限，而生物神经网络具有高能效和数据效率的潜力，并能提供更可解释和生物学上合理的模型。", "method": "该研究提出了一个根植于主动推理（一种通用行为理论）的框架，用于模拟具身智能体中的决策。通过使用实验启发的生成模型，在模拟游戏环境中模拟决策过程，该环境旨在反映使用生物神经元的实验设置。", "result": "研究结果展示了智能体中的学习能力，并揭示了基于记忆的学习和预测性规划在智能决策中的作用。", "conclusion": "这项工作通过提供一种生物学上合理且可扩展的方法来理解智能体中的目的性行为，为可解释AI的不断发展做出了贡献。"}}
{"id": "2508.07244", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07244", "abs": "https://arxiv.org/abs/2508.07244", "authors": ["Ayesha Jena", "Stefan Reitmann", "Elin Anna Topp"], "title": "Impact of Gaze-Based Interaction and Augmentation on Human-Robot Collaboration in Critical Tasks", "comment": null, "summary": "We present a user study analyzing head-gaze-based robot control and foveated\nvisual augmentation in a simulated search-and-rescue task. Results show that\nfoveated augmentation significantly improves task performance, reduces\ncognitive load by 38%, and shortens task time by over 60%. Head-gaze patterns\nanalysed over both the entire task duration and shorter time segments show that\nnear and far attention capture is essential to better understand user intention\nin critical scenarios. Our findings highlight the potential of foveation as an\naugmentation technique and the need to further study gaze measures to leverage\nthem during critical tasks.", "AI": {"tldr": "本研究通过用户实验，分析了在模拟搜救任务中，基于头部凝视的机器人控制和凹陷式视觉增强的效果。结果显示，凹陷式增强显著提升了任务表现，减轻了认知负荷，并缩短了任务时间。", "motivation": "旨在提升在关键任务（如搜救）中机器人控制的效率和用户意图的理解，通过引入视觉增强和凝视模式分析。", "method": "进行了一项用户研究，在模拟搜救任务中，采用基于头部凝视的机器人控制，并结合凹陷式视觉增强。分析了整个任务期间和短时间段内的头部凝视模式。", "result": "凹陷式增强显著提升了任务表现，认知负荷降低了38%，任务时间缩短了60%以上。对头部凝视模式的分析表明，近距离和远距离的注意力捕捉对于在关键场景中更好地理解用户意图至关重要。", "conclusion": "凹陷式视觉增强作为一种增强技术具有巨大潜力。此外，需要进一步研究凝视测量方法，以便在关键任务中更好地利用它们。"}}
{"id": "2508.07069", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07069", "abs": "https://arxiv.org/abs/2508.07069", "authors": ["Muhammad Dehan Al Kautsar", "Aswin Candra", "Muhammad Alif Al Hakim", "Maxalmina Satria Kahfi", "Fajri Koto", "Alham Fikri Aji", "Peerat Limkonchotiwat", "Ekapol Chuangsuwanich", "Genta Indra Winata"], "title": "SEADialogues: A Multilingual Culturally Grounded Multi-turn Dialogue Dataset on Southeast Asian Languages", "comment": "Preprint", "summary": "Although numerous datasets have been developed to support dialogue systems,\nmost existing chit-chat datasets overlook the cultural nuances inherent in\nnatural human conversations. To address this gap, we introduce SEADialogues, a\nculturally grounded dialogue dataset centered on Southeast Asia, a region with\nover 700 million people and immense cultural diversity. Our dataset features\ndialogues in eight languages from six Southeast Asian countries, many of which\nare low-resource despite having sizable speaker populations. To enhance\ncultural relevance and personalization, each dialogue includes persona\nattributes and two culturally grounded topics that reflect everyday life in the\nrespective communities. Furthermore, we release a multi-turn dialogue dataset\nto advance research on culturally aware and human-centric large language\nmodels, including conversational dialogue agents.", "AI": {"tldr": "本文提出了SEADialogues，一个以东南亚为中心的文化背景对话数据集，旨在解决现有闲聊数据集忽视文化细微差别的问题。", "motivation": "大多数现有闲聊数据集忽略了人类自然对话中固有的文化细微差别，尤其是在拥有巨大文化多样性的东南亚地区。", "method": "引入了SEADialogues数据集，包含来自六个东南亚国家八种语言的对话，其中许多是低资源语言。每个对话都包含人物属性和两个反映当地社区日常生活的文化主题，以增强文化相关性和个性化。该数据集是多轮对话形式。", "result": "创建了一个多语言、多文化背景的对话数据集，旨在促进对文化敏感、以人为本的大型语言模型（包括对话代理）的研究。", "conclusion": "SEADialogues数据集将有助于推进更具文化意识和个性化的人类中心对话系统和大型语言模型的发展。"}}
{"id": "2508.06640", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06640", "abs": "https://arxiv.org/abs/2508.06640", "authors": ["Zheyuan Zhang", "Weihao Tang", "Hong Chen"], "title": "Rethinking Key-frame-based Micro-expression Recognition: A Robust and Accurate Framework Against Key-frame Errors", "comment": null, "summary": "Micro-expression recognition (MER) is a highly challenging task in affective\ncomputing. With the reduced-sized micro-expression (ME) input that contains key\ninformation based on key-frame indexes, key-frame-based methods have\nsignificantly improved the performance of MER. However, most of these methods\nfocus on improving the performance with relatively accurate key-frame indexes,\nwhile ignoring the difficulty of obtaining accurate key-frame indexes and the\nobjective existence of key-frame index errors, which impedes them from moving\ntowards practical applications. In this paper, we propose CausalNet, a novel\nframework to achieve robust MER facing key-frame index errors while maintaining\naccurate recognition. To enhance robustness, CausalNet takes the representation\nof the entire ME sequence as the input. To address the information redundancy\nbrought by the complete ME range input and maintain accurate recognition,\nfirst, the Causal Motion Position Learning Module (CMPLM) is proposed to help\nthe model locate the muscle movement areas related to Action Units (AUs),\nthereby reducing the attention to other redundant areas. Second, the Causal\nAttention Block (CAB) is proposed to deeply learn the causal relationships\nbetween the muscle contraction and relaxation movements in MEs. Empirical\nexperiments have demonstrated that on popular ME benchmarks, the CausalNet has\nachieved robust MER under different levels of key-frame index noise. Meanwhile,\nit has surpassed state-of-the-art (SOTA) methods on several standard MER\nbenchmarks when using the provided annotated key-frames. Code is available at\nhttps://github.com/tony19980810/CausalNet.", "AI": {"tldr": "CausalNet是一种新型框架，旨在解决微表情识别（MER）中关键帧索引误差导致鲁棒性差的问题，通过学习因果关系和关注肌肉运动区域，实现了鲁棒且准确的识别。", "motivation": "现有的基于关键帧的微表情识别方法严重依赖于准确的关键帧索引，但实际应用中获取精确索引非常困难且容易出错，这阻碍了这些方法走向实际应用。", "method": "CausalNet以整个微表情序列作为输入，以增强鲁棒性。为处理完整序列带来的信息冗余并保持准确性，它提出了两个模块：1) 因果运动位置学习模块（CMPLM），用于定位与动作单元（AUs）相关的肌肉运动区域，减少对冗余区域的关注；2) 因果注意力块（CAB），用于深入学习微表情中肌肉收缩和放松运动之间的因果关系。", "result": "实验证明，CausalNet在流行的微表情基准测试上，在不同程度的关键帧索引噪声下均实现了鲁棒的微表情识别。同时，在使用提供的标注关键帧时，它在几个标准微表情识别基准测试上超越了现有最先进（SOTA）的方法。", "conclusion": "CausalNet成功地解决了微表情识别中关键帧索引误差带来的鲁棒性问题，并在保持高识别准确率的同时，提高了模型的实用性。"}}
{"id": "2508.07015", "categories": ["cs.AI", "cs.DS"], "pdf": "https://arxiv.org/pdf/2508.07015", "abs": "https://arxiv.org/abs/2508.07015", "authors": ["Hannes Ihalainen", "Dieter Vandesande", "André Schidler", "Jeremias Berg", "Bart Bogaerts", "Matti Järvisalo"], "title": "Efficient and Reliable Hitting-Set Computations for the Implicit Hitting Set Approach", "comment": null, "summary": "The implicit hitting set (IHS) approach offers a general framework for\nsolving computationally hard combinatorial optimization problems declaratively.\nIHS iterates between a decision oracle used for extracting sources of\ninconsistency and an optimizer for computing so-called hitting sets (HSs) over\nthe accumulated sources of inconsistency. While the decision oracle is\nlanguage-specific, the optimizers is usually instantiated through integer\nprogramming.\n  We explore alternative algorithmic techniques for hitting set optimization\nbased on different ways of employing pseudo-Boolean (PB) reasoning as well as\nstochastic local search. We extensively evaluate the practical feasibility of\nthe alternatives in particular in the context of pseudo-Boolean (0-1 IP)\noptimization as one of the most recent instantiations of IHS. Highlighting a\ntrade-off between efficiency and reliability, while a commercial IP solver\nturns out to remain the most effective way to instantiate HS computations, it\ncan cause correctness issues due to numerical instability; in fact, we show\nthat exact HS computations instantiated via PB reasoning can be made\ncompetitive with a numerically exact IP solver. Furthermore, the use of PB\nreasoning as a basis for HS computations allows for obtaining certificates for\nthe correctness of IHS computations, generally applicable to any IHS\ninstantiation in which reasoning in the declarative language at hand can be\ncaptured in the PB-based proof format we employ.", "AI": {"tldr": "隐式击中集（IHS）框架通常依赖整数规划（IP）求解器进行击中集（HS）优化。本文探索了基于伪布尔（PB）推理和随机局部搜索的替代HS优化技术。结果表明，虽然商用IP求解器高效，但可能存在数值不稳定性；而基于PB推理的HS计算不仅能与精确IP求解器媲美，还能提供正确性证书。", "motivation": "隐式击中集（IHS）方法是解决组合优化问题的通用框架，其中击中集（HS）优化通常通过整数规划（IP）实现。然而，IP求解器可能存在数值不稳定性，导致正确性问题。因此，需要探索更可靠的替代算法技术来优化击中集计算，以提高IHS框架的可靠性并提供正确性保证。", "method": "本文探索了基于伪布尔（PB）推理和随机局部搜索的替代算法技术，用于隐式击中集（IHS）框架中的击中集（HS）优化。这些方法在伪布尔（0-1 IP）优化这一IHS的最新实例化背景下进行了广泛的实践可行性评估。", "result": "研究发现，效率与可靠性之间存在权衡。商用整数规划（IP）求解器在实例化HS计算方面仍然是最有效的，但可能因数值不稳定性导致正确性问题。通过伪布尔（PB）推理实现的精确HS计算，在性能上可以与数值精确的IP求解器相媲美。此外，基于PB推理的HS计算能够为IHS计算提供正确性证书，适用于任何可被所用PB证明格式捕获的IHS实例化。", "conclusion": "尽管商用IP求解器在效率上仍具优势，但基于伪布尔（PB）推理的击中集优化方法在数值精确性和提供正确性证书方面表现出色，且在性能上能与精确IP求解器竞争。这为IHS框架提供了一个可靠且可验证的替代方案，尤其是在对正确性要求较高的场景下。"}}
{"id": "2508.07267", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07267", "abs": "https://arxiv.org/abs/2508.07267", "authors": ["Daria de Tinguy", "Tim Verbelen", "Emilio Gamba", "Bart Dhoedt"], "title": "Bio-Inspired Topological Autonomous Navigation with Active Inference in Robotics", "comment": "Conference ICCAS 2025 - accepted (in processing)", "summary": "Achieving fully autonomous exploration and navigation remains a critical\nchallenge in robotics, requiring integrated solutions for localisation,\nmapping, decision-making and motion planning. Existing approaches either rely\non strict navigation rules lacking adaptability or on pre-training, which\nrequires large datasets. These AI methods are often computationally intensive\nor based on static assumptions, limiting their adaptability in dynamic or\nunknown environments. This paper introduces a bio-inspired agent based on the\nActive Inference Framework (AIF), which unifies mapping, localisation, and\nadaptive decision-making for autonomous navigation, including exploration and\ngoal-reaching. Our model creates and updates a topological map of the\nenvironment in real-time, planning goal-directed trajectories to explore or\nreach objectives without requiring pre-training. Key contributions include a\nprobabilistic reasoning framework for interpretable navigation, robust\nadaptability to dynamic changes, and a modular ROS2 architecture compatible\nwith existing navigation systems. Our method was tested in simulated and\nreal-world environments. The agent successfully explores large-scale simulated\nenvironments and adapts to dynamic obstacles and drift, proving to be\ncomparable to other exploration strategies such as Gbplanner, FAEL and\nFrontiers. This approach offers a scalable and transparent approach for\nnavigating complex, unstructured environments.", "AI": {"tldr": "本文提出一种基于主动推理框架（AIF）的仿生智能体，用于自主探索和导航，无需预训练即可实时构建拓扑地图并适应动态环境。", "motivation": "现有的自主导航方法存在局限性，例如依赖严格规则缺乏适应性、需要大量预训练数据、计算量大或基于静态假设，导致在动态或未知环境中表现不佳。", "method": "该研究引入了一个基于主动推理框架（AIF）的仿生智能体，它统一了定位、建图和自适应决策（包括探索和目标达成）。该模型实时创建和更新环境的拓扑地图，并规划目标导向的轨迹，无需预训练。主要贡献包括可解释的概率推理框架、对动态变化的鲁棒适应性以及与现有导航系统兼容的模块化ROS2架构。", "result": "该方法在模拟和真实世界环境中进行了测试。结果显示，该智能体成功探索了大规模模拟环境，并能适应动态障碍物和漂移。其性能与Gbplanner、FAEL和Frontiers等其他探索策略相当。", "conclusion": "该方法为在复杂、非结构化环境中导航提供了一种可扩展且透明的解决方案。"}}
{"id": "2508.07090", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.07090", "abs": "https://arxiv.org/abs/2508.07090", "authors": ["Aditya Tomar", "Nihar Ranjan Sahoo", "Pushpak Bhattacharyya"], "title": "BharatBBQ: A Multilingual Bias Benchmark for Question Answering in the Indian Context", "comment": null, "summary": "Evaluating social biases in language models (LMs) is crucial for ensuring\nfairness and minimizing the reinforcement of harmful stereotypes in AI systems.\nExisting benchmarks, such as the Bias Benchmark for Question Answering (BBQ),\nprimarily focus on Western contexts, limiting their applicability to the Indian\ncontext. To address this gap, we introduce BharatBBQ, a culturally adapted\nbenchmark designed to assess biases in Hindi, English, Marathi, Bengali, Tamil,\nTelugu, Odia, and Assamese. BharatBBQ covers 13 social categories, including 3\nintersectional groups, reflecting prevalent biases in the Indian sociocultural\nlandscape. Our dataset contains 49,108 examples in one language that are\nexpanded using translation and verification to 392,864 examples in eight\ndifferent languages. We evaluate five multilingual LM families across zero and\nfew-shot settings, analyzing their bias and stereotypical bias scores. Our\nfindings highlight persistent biases across languages and social categories and\noften amplified biases in Indian languages compared to English, demonstrating\nthe necessity of linguistically and culturally grounded benchmarks for bias\nevaluation.", "AI": {"tldr": "本文介绍了BharatBBQ，一个针对印度语境设计的偏见评估基准，涵盖八种印度语言和13个社会类别，发现多语言模型在印度语言中存在持续且放大的偏见。", "motivation": "现有语言模型偏见评估基准（如BBQ）主要关注西方语境，限制了其在印度语境的适用性。为填补这一空白，确保AI系统的公平性并减少有害刻板印象的强化，需要一个文化适应的印度偏见评估基准。", "method": "引入了BharatBBQ，一个文化适应的基准，用于评估印地语、英语、马拉地语、孟加拉语、泰米尔语、泰卢固语、奥迪亚语和阿萨姆语中的偏见。BharatBBQ涵盖13个社会类别（包括3个交叉群体），反映了印度社会文化景观中普遍存在的偏见。数据集包含49,108个单语示例，通过翻译和验证扩展到八种语言的392,864个示例。评估了五种多语言模型家族在零样本和少样本设置下的偏见和刻板印象偏见分数。", "result": "研究结果表明，在不同语言和社会类别中都存在持续的偏见，并且与英语相比，印度语言中的偏见通常被放大。", "conclusion": "研究结果强调了在偏见评估中，语言和文化背景扎根的基准的必要性。"}}
{"id": "2508.06656", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06656", "abs": "https://arxiv.org/abs/2508.06656", "authors": ["Denis Lukovnikov", "Andreas Müller", "Erwin Quiring", "Asja Fischer"], "title": "Towards Robust Red-Green Watermarking for Autoregressive Image Generators", "comment": null, "summary": "In-generation watermarking for detecting and attributing generated content\nhas recently been explored for latent diffusion models (LDMs), demonstrating\nhigh robustness. However, the use of in-generation watermarks in autoregressive\n(AR) image models has not been explored yet. AR models generate images by\nautoregressively predicting a sequence of visual tokens that are then decoded\ninto pixels using a vector-quantized decoder. Inspired by red-green watermarks\nfor large language models, we examine token-level watermarking schemes that\nbias the next-token prediction based on prior tokens. We find that a direct\ntransfer of these schemes works in principle, but the detectability of the\nwatermarks decreases considerably under common image perturbations. As a\nremedy, we propose two novel watermarking methods that rely on visual token\nclustering to assign similar tokens to the same set. Firstly, we investigate a\ntraining-free approach that relies on a cluster lookup table, and secondly, we\nfinetune VAE encoders to predict token clusters directly from perturbed images.\nOverall, our experiments show that cluster-level watermarks improve robustness\nagainst perturbations and regeneration attacks while preserving image quality.\nCluster classification further boosts watermark detectability, outperforming a\nset of baselines. Moreover, our methods offer fast verification runtime,\ncomparable to lightweight post-hoc watermarking methods.", "AI": {"tldr": "本文首次探索了自回归（AR）图像模型的生成内水印技术，提出基于视觉标记聚类的新方法，显著提高了水印在图像扰动和再生攻击下的鲁棒性。", "motivation": "生成内水印技术已在潜在扩散模型（LDMs）中展现出高鲁棒性，但尚未在AR图像模型中进行探索。直接将大语言模型（LLM）的标记级水印方案应用于AR图像模型时，在常见图像扰动下检测性会显著下降。", "method": "受LLM红绿水印启发，首先探索了基于先前标记偏置下一个标记预测的标记级水印方案。为解决鲁棒性问题，提出了两种基于视觉标记聚类的新方法：一是依赖聚类查找表的无训练方法；二是微调VAE编码器以直接从受扰图像预测标记聚类。", "result": "实验表明，聚类级水印提高了对扰动和再生攻击的鲁棒性，同时保持了图像质量。聚类分类进一步提升了水印检测性，优于基线方法。此外，所提方法具有快速验证运行时，与轻量级后处理水印方法相当。", "conclusion": "基于视觉标记聚类的生成内水印方法，特别是通过聚类查找表或微调VAE编码器实现，能有效且鲁棒地在自回归图像模型中嵌入和检测水印，克服了传统标记级方法的局限性。"}}
{"id": "2508.07022", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.07022", "abs": "https://arxiv.org/abs/2508.07022", "authors": ["Shengtao Wen", "Haodong Chen", "Yadong Wang", "Zhongying Pan", "Xiang Chen", "Yu Tian", "Bo Qian", "Dong Liang", "Sheng-Jun Huang"], "title": "MultiMedEdit: A Scenario-Aware Benchmark for Evaluating Knowledge Editing in Medical VQA", "comment": "Under Review", "summary": "Knowledge editing (KE) provides a scalable approach for updating factual\nknowledge in large language models without full retraining. While previous\nstudies have demonstrated effectiveness in general domains and medical QA\ntasks, little attention has been paid to KE in multimodal medical scenarios.\nUnlike text-only settings, medical KE demands integrating updated knowledge\nwith visual reasoning to support safe and interpretable clinical decisions. To\naddress this gap, we propose MultiMedEdit, the first benchmark tailored to\nevaluating KE in clinical multimodal tasks. Our framework spans both\nunderstanding and reasoning task types, defines a three-dimensional metric\nsuite (reliability, generality, and locality), and supports cross-paradigm\ncomparisons across general and domain-specific models. We conduct extensive\nexperiments under single-editing and lifelong-editing settings. Results suggest\nthat current methods struggle with generalization and long-tail reasoning,\nparticularly in complex clinical workflows. We further present an efficiency\nanalysis (e.g., edit latency, memory footprint), revealing practical trade-offs\nin real-world deployment across KE paradigms. Overall, MultiMedEdit not only\nreveals the limitations of current approaches but also provides a solid\nfoundation for developing clinically robust knowledge editing techniques in the\nfuture.", "AI": {"tldr": "本文提出了MultiMedEdit，首个用于评估多模态医疗场景下知识编辑（KE）的基准，揭示了现有方法在泛化和长尾推理方面的局限性。", "motivation": "尽管知识编辑在通用领域和医疗问答任务中有效，但很少有研究关注多模态医疗场景下的知识编辑。此类场景需要将更新的知识与视觉推理相结合，以支持安全和可解释的临床决策。", "method": "提出了MultiMedEdit基准，旨在评估临床多模态任务中的KE。该框架涵盖理解和推理任务类型，定义了三维度的度量标准（可靠性、通用性和局部性），并支持通用和领域特定模型之间的跨范式比较。实验在单次编辑和终身编辑设置下进行，并进行了效率分析。", "result": "实验结果表明，当前方法在泛化能力和长尾推理方面表现不佳，尤其是在复杂的临床工作流程中。效率分析也揭示了不同KE范式在实际部署中的权衡。", "conclusion": "MultiMedEdit不仅揭示了当前多模态医疗知识编辑方法的局限性，也为未来开发临床鲁棒的知识编辑技术奠定了坚实基础。"}}
{"id": "2508.07269", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07269", "abs": "https://arxiv.org/abs/2508.07269", "authors": ["Daria de Tinguy", "Tim Verbelen", "Bart Dhoedt"], "title": "Navigation and Exploration with Active Inference: from Biology to Industry", "comment": "conference IWAI 2025 - accepted (in processing)", "summary": "By building and updating internal cognitive maps, animals exhibit\nextraordinary navigation abilities in complex, dynamic environments. Inspired\nby these biological mechanisms, we present a real time robotic navigation\nsystem grounded in the Active Inference Framework (AIF). Our model\nincrementally constructs a topological map, infers the agent's location, and\nplans actions by minimising expected uncertainty and fulfilling perceptual\ngoals without any prior training. Integrated into the ROS2 ecosystem, we\nvalidate its adaptability and efficiency across both 2D and 3D environments\n(simulated and real world), demonstrating competitive performance with\ntraditional and state of the art exploration approaches while offering a\nbiologically inspired navigation approach.", "AI": {"tldr": "该论文提出一个基于主动推理框架（AIF）的实时机器人导航系统，通过构建拓扑地图、推断位置和规划动作来最小化不确定性，在2D和3D环境中展现出与现有方法相当的性能。", "motivation": "受动物通过内部认知地图在复杂动态环境中卓越导航能力的启发，研究旨在开发一个类似生物机制的机器人导航系统。", "method": "该系统基于主动推理框架（AIF），实时增量构建拓扑地图，推断代理位置，并通过最小化预期不确定性和实现感知目标来规划动作，无需预先训练。系统集成到ROS2生态系统中。", "result": "在2D和3D（模拟和真实世界）环境中验证了其适应性和效率，展示出与传统和最先进探索方法相当的竞争性能。", "conclusion": "该系统提供了一种生物启发式的导航方法，无需预先训练即可在复杂动态环境中实现高效且具有竞争力的导航性能。"}}
{"id": "2508.07101", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07101", "abs": "https://arxiv.org/abs/2508.07101", "authors": ["Lijie Yang", "Zhihao Zhang", "Arti Jain", "Shijie Cao", "Baihong Yuan", "Yiwei Chen", "Zhihao Jia", "Ravi Netravali"], "title": "Less Is More: Training-Free Sparse Attention with Global Locality for Efficient Reasoning", "comment": null, "summary": "Large reasoning models achieve strong performance through test-time scaling\nbut incur substantial computational overhead, particularly from excessive token\ngeneration when processing short input prompts. While sparse attention\nmechanisms can reduce latency and memory usage, existing approaches suffer from\nsignificant accuracy degradation due to accumulated errors during\nlong-generation reasoning. These methods generally require either high token\nretention rates or expensive retraining. We introduce LessIsMore, a\ntraining-free sparse attention mechanism for reasoning tasks, which leverages\nglobal attention patterns rather than relying on traditional head-specific\nlocal optimizations. LessIsMore aggregates token selections from local\nattention heads with recent contextual information, enabling unified cross-head\ntoken ranking for future decoding layers. This unified selection improves\ngeneralization and efficiency by avoiding the need to maintain separate token\nsubsets per head. Evaluation across diverse reasoning tasks and benchmarks\nshows that LessIsMore preserves -- and in some cases improves -- accuracy while\nachieving a $1.1\\times$ average decoding speed-up compared to full attention.\nMoreover, LessIsMore attends to $2\\times$ fewer tokens without accuracy loss,\nachieving a $1.13\\times$ end-to-end speed-up compared to existing sparse\nattention methods.", "AI": {"tldr": "LessIsMore是一种无需训练的稀疏注意力机制，通过全局注意力模式和统一的跨头令牌选择，在推理任务中显著加速解码并保持甚至提升准确性，优于现有方法。", "motivation": "大型推理模型在测试时扩展性能强大，但计算开销巨大，尤其是在处理短输入提示时生成过多令牌。现有稀疏注意力机制虽然能减少延迟和内存，但通常会因长生成推理中的累积误差导致精度显著下降，或需要高令牌保留率/昂贵的再训练。", "method": "LessIsMore是一种无需训练的稀疏注意力机制，专为推理任务设计。它利用全局注意力模式，而非传统的局部头部优化。该方法将来自局部注意力头的令牌选择与最近的上下文信息聚合起来，实现统一的跨头令牌排名，用于未来的解码层。这种统一选择避免了为每个头部维护单独的令牌子集，从而提高了泛化性和效率。", "result": "LessIsMore在各种推理任务和基准测试中，保持甚至提高了准确性，与全注意力相比，平均解码速度提升了1.1倍。此外，它在不损失准确性的情况下处理的令牌数量减少了2倍，与现有稀疏注意力方法相比，端到端速度提升了1.13倍。", "conclusion": "LessIsMore提供了一种高效且无需训练的稀疏注意力解决方案，有效解决了大型推理模型在计算开销和准确性之间的权衡问题，显著提升了推理速度同时保持或改善了性能。"}}
{"id": "2508.06696", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06696", "abs": "https://arxiv.org/abs/2508.06696", "authors": ["Tianqin Li", "George Liu", "Tai Sing Lee"], "title": "Learning More by Seeing Less: Line Drawing Pretraining for Efficient, Transferable, and Human-Aligned Vision", "comment": null, "summary": "Despite remarkable progress in computer vision, modern recognition systems\nremain limited by their dependence on rich, redundant visual inputs. In\ncontrast, humans can effortlessly understand sparse, minimal representations\nlike line drawings - suggesting that structure, rather than appearance,\nunderlies efficient visual understanding. In this work, we propose using line\ndrawings as a structure-first pretraining modality to induce more compact and\ngeneralizable visual representations. We show that models pretrained on line\ndrawings develop stronger shape bias, more focused attention, and greater data\nefficiency across classification, detection, and segmentation tasks. Notably,\nthese models also exhibit lower intrinsic dimensionality, requiring\nsignificantly fewer principal components to capture representational variance -\nechoing the similar observation in low dimensional efficient representation in\nthe brain. Beyond performance improvements, line drawing pretraining produces\nmore compressible representations, enabling better distillation into\nlightweight student models. Students distilled from line-pretrained teachers\nconsistently outperform those trained from color-supervised teachers,\nhighlighting the benefits of structurally compact knowledge. Finally, we\ndemonstrate that the pretraining with line-drawing can also be extended to\nunsupervised setting via our proposed method \"learning to draw\". Together, our\nresults support the view that structure-first visual learning fosters\nefficiency, generalization, and human-aligned inductive biases - offering a\nsimple yet powerful strategy for building more robust and adaptable vision\nsystems.", "AI": {"tldr": "本文提出使用线条画作为“结构优先”的预训练模式，以学习更紧凑、可泛化且高效的视觉表示，从而提高计算机视觉系统的性能和数据效率。", "motivation": "尽管计算机视觉取得了显著进展，但现代识别系统仍依赖于丰富、冗余的视觉输入。与此相反，人类可以毫不费力地理解稀疏、极简的线条画，这表明结构而非外观是高效视觉理解的基础。", "method": "提出将线条画作为一种“结构优先”的预训练模式，以诱导更紧凑和可泛化的视觉表示。此外，还提出了“学习绘画”（learning to draw）方法，将线条画预训练扩展到无监督设置。", "result": "在线条画上预训练的模型表现出更强的形状偏好、更集中的注意力，并在分类、检测和分割任务中具有更高的数据效率。这些模型还展现出更低的内在维度和更易压缩的表示。从线条画预训练教师模型中蒸馏出的学生模型，性能优于从彩色监督教师模型中蒸馏出的学生模型。", "conclusion": "结构优先的视觉学习能够促进效率、泛化能力和与人类对齐的归纳偏置，为构建更鲁棒和适应性更强的视觉系统提供了一种简单而强大的策略。"}}
{"id": "2508.07043", "categories": ["cs.AI", "cs.MA", "q-bio.GN", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2508.07043", "abs": "https://arxiv.org/abs/2508.07043", "authors": ["Orion Li", "Vinayak Agarwal", "Summer Zhou", "Ashwin Gopinath", "Timothy Kassis"], "title": "K-Dense Analyst: Towards Fully Automated Scientific Analysis", "comment": null, "summary": "The complexity of modern bioinformatics analysis has created a critical gap\nbetween data generation and developing scientific insights. While large\nlanguage models (LLMs) have shown promise in scientific reasoning, they remain\nfundamentally limited when dealing with real-world analytical workflows that\ndemand iterative computation, tool integration and rigorous validation. We\nintroduce K-Dense Analyst, a hierarchical multi-agent system that achieves\nautonomous bioinformatics analysis through a dual-loop architecture. K-Dense\nAnalyst, part of the broader K-Dense platform, couples planning with validated\nexecution using specialized agents to decompose complex objectives into\nexecutable, verifiable tasks within secure computational environments. On\nBixBench, a comprehensive benchmark for open-ended biological analysis, K-Dense\nAnalyst achieves 29.2% accuracy, surpassing the best-performing language model\n(GPT-5) by 6.3 percentage points, representing nearly 27% improvement over what\nis widely considered the most powerful LLM available. Remarkably, K-Dense\nAnalyst achieves this performance using Gemini 2.5 Pro, which attains only\n18.3% accuracy when used directly, demonstrating that our architectural\ninnovations unlock capabilities far beyond the underlying model's baseline\nperformance. Our insights demonstrate that autonomous scientific reasoning\nrequires more than enhanced language models, it demands purpose-built systems\nthat can bridge the gap between high-level scientific objectives and low-level\ncomputational execution. These results represent a significant advance toward\nfully autonomous computational biologists capable of accelerating discovery\nacross the life sciences.", "AI": {"tldr": "K-Dense Analyst是一个分层多智能体系统，通过双循环架构实现了自主生物信息学分析，显著提升了现有大型语言模型在复杂分析任务上的表现，弥补了数据生成与科学洞察之间的差距。", "motivation": "现代生物信息学分析的复杂性导致了数据生成与获取科学洞察之间的关键鸿沟。尽管大型语言模型（LLMs）在科学推理方面展现出潜力，但它们在处理需要迭代计算、工具集成和严格验证的真实世界分析工作流时存在根本性限制。", "method": "本文引入了K-Dense Analyst，一个分层多智能体系统，通过双循环架构实现自主生物信息学分析。K-Dense Analyst将规划与经验证的执行相结合，利用专业化智能体在安全计算环境中将复杂目标分解为可执行、可验证的任务。", "result": "在BixBench（一个开放式生物分析综合基准）上，K-Dense Analyst实现了29.2%的准确率，超过了表现最佳的语言模型（GPT-5）6.3个百分点，相对于GPT-5提升了近27%。值得注意的是，K-Dense Analyst使用的是直接使用时仅获得18.3%准确率的Gemini 2.5 Pro，这表明其架构创新极大地超越了底层模型的基线性能。", "conclusion": "研究结果表明，自主科学推理不仅仅需要增强型语言模型，更需要专门构建的系统来弥合高级科学目标与低级计算执行之间的差距。这些成果代表着向实现完全自主计算生物学家迈出的重要一步，有望加速生命科学领域的发现进程。"}}
{"id": "2508.07287", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07287", "abs": "https://arxiv.org/abs/2508.07287", "authors": ["Liwen Zhang", "Dong Zhou", "Shibo Shao", "Zihao Su", "Guanghui Sun"], "title": "Multimodal Spiking Neural Network for Space Robotic Manipulation", "comment": null, "summary": "This paper presents a multimodal control framework based on spiking neural\nnetworks (SNNs) for robotic arms aboard space stations. It is designed to cope\nwith the constraints of limited onboard resources while enabling autonomous\nmanipulation and material transfer in space operations. By combining geometric\nstates with tactile and semantic information, the framework strengthens\nenvironmental awareness and contributes to more robust control strategies. To\nguide the learning process progressively, a dual-channel, three-stage\ncurriculum reinforcement learning (CRL) scheme is further integrated into the\nsystem. The framework was tested across a range of tasks including target\napproach, object grasping, and stable lifting with wall-mounted robotic arms,\ndemonstrating reliable performance throughout. Experimental evaluations\ndemonstrate that the proposed method consistently outperforms baseline\napproaches in both task success rate and energy efficiency. These findings\nhighlight its suitability for real-world aerospace applications.", "AI": {"tldr": "本文提出了一种基于脉冲神经网络（SNNs）的多模态控制框架，用于空间站机械臂，旨在有限资源下实现自主操作和材料转移。", "motivation": "空间站机械臂操作面临资源有限的挑战，需要实现自主操纵和材料转移，以提高空间操作的效率和鲁棒性。", "method": "该框架结合了几何状态、触觉和语义信息，以增强环境感知。此外，它还整合了一个双通道、三阶段的课程强化学习（CRL）方案来指导学习过程。", "result": "该框架在目标接近、物体抓取和稳定举升等任务中表现出可靠的性能，并在任务成功率和能效方面持续优于基线方法。", "conclusion": "该研究结果表明所提出的方法非常适用于实际航空航天应用。"}}
{"id": "2508.07111", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07111", "abs": "https://arxiv.org/abs/2508.07111", "authors": ["Falaah Arif Khan", "Nivedha Sivakumar", "Yinong Oliver Wang", "Katherine Metcalf", "Cezanne Camacho", "Barry-John Theobald", "Luca Zappella", "Nicholas Apostoloff"], "title": "Investigating Intersectional Bias in Large Language Models using Confidence Disparities in Coreference Resolution", "comment": null, "summary": "Large language models (LLMs) have achieved impressive performance, leading to\ntheir widespread adoption as decision-support tools in resource-constrained\ncontexts like hiring and admissions. There is, however, scientific consensus\nthat AI systems can reflect and exacerbate societal biases, raising concerns\nabout identity-based harm when used in critical social contexts. Prior work has\nlaid a solid foundation for assessing bias in LLMs by evaluating demographic\ndisparities in different language reasoning tasks. In this work, we extend\nsingle-axis fairness evaluations to examine intersectional bias, recognizing\nthat when multiple axes of discrimination intersect, they create distinct\npatterns of disadvantage. We create a new benchmark called WinoIdentity by\naugmenting the WinoBias dataset with 25 demographic markers across 10\nattributes, including age, nationality, and race, intersected with binary\ngender, yielding 245,700 prompts to evaluate 50 distinct bias patterns.\nFocusing on harms of omission due to underrepresentation, we investigate bias\nthrough the lens of uncertainty and propose a group (un)fairness metric called\nCoreference Confidence Disparity which measures whether models are more or less\nconfident for some intersectional identities than others. We evaluate five\nrecently published LLMs and find confidence disparities as high as 40% along\nvarious demographic attributes including body type, sexual orientation and\nsocio-economic status, with models being most uncertain about\ndoubly-disadvantaged identities in anti-stereotypical settings. Surprisingly,\ncoreference confidence decreases even for hegemonic or privileged markers,\nindicating that the recent impressive performance of LLMs is more likely due to\nmemorization than logical reasoning. Notably, these are two independent\nfailures in value alignment and validity that can compound to cause social\nharm.", "AI": {"tldr": "本研究通过引入WinoIdentity基准和新的置信度差异指标，评估了大型语言模型（LLMs）在交叉身份上的偏见，发现模型对双重弱势群体的识别置信度最低，并揭示了LLMs可能更多依赖记忆而非逻辑推理。", "motivation": "LLMs被广泛用作决策支持工具，尤其在资源受限的关键社会场景（如招聘、招生）中。然而，AI系统存在反映和加剧社会偏见的风险，可能导致基于身份的伤害。现有工作主要关注单轴公平性评估，但现实中多种歧视轴的交叉会产生独特的劣势模式，因此需要扩展到交叉偏见评估。", "method": "通过将WinoBias数据集扩展为WinoIdentity，新增了25个跨10个属性（如年龄、国籍、种族）的社会人口学标记，并与二元性别交叉，生成了245,700个提示，用于评估50种不同的偏见模式。研究侧重于因代表性不足导致的遗漏伤害，并提出了一种名为“指代消解置信度差异”（Coreference Confidence Disparity）的群体（不）公平性度量，以衡量模型对不同交叉身份的置信度差异。评估了五种近期发布的LLMs。", "result": "研究发现，在身体类型、性取向和社会经济地位等不同人口属性上，置信度差异高达40%。模型在反刻板印象设置中，对双重弱势身份（doubly-disadvantaged identities）的置信度最低。令人惊讶的是，即使对于具有霸权或特权标记的身份，指代消解置信度也会下降，这表明LLMs近期令人印象深刻的性能更可能是由于记忆而非逻辑推理。这些是价值观对齐和有效性方面的独立失败，可能叠加导致社会危害。", "conclusion": "LLMs在处理交叉身份时存在显著的置信度偏见，尤其对弱势群体表现出不确定性。模型的优异表现可能更多源于记忆而非推理能力，这暴露出其在价值观对齐和模型有效性方面的双重缺陷，可能导致严重的社会危害。"}}
{"id": "2508.06701", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.06701", "abs": "https://arxiv.org/abs/2508.06701", "authors": ["Md Rezwanul Haque", "Md. Milon Islam", "S M Taslim Uddin Raju", "Hamdi Altaheri", "Lobna Nassar", "Fakhri Karray"], "title": "MMFformer: Multimodal Fusion Transformer Network for Depression Detection", "comment": "Accepted for the 2025 IEEE International Conference on Systems, Man,\n  and Cybernetics (SMC), Vienna, Austria", "summary": "Depression is a serious mental health illness that significantly affects an\nindividual's well-being and quality of life, making early detection crucial for\nadequate care and treatment. Detecting depression is often difficult, as it is\nbased primarily on subjective evaluations during clinical interviews. Hence,\nthe early diagnosis of depression, thanks to the content of social networks,\nhas become a prominent research area. The extensive and diverse nature of\nuser-generated information poses a significant challenge, limiting the accurate\nextraction of relevant temporal information and the effective fusion of data\nacross multiple modalities. This paper introduces MMFformer, a multimodal\ndepression detection network designed to retrieve depressive spatio-temporal\nhigh-level patterns from multimodal social media information. The transformer\nnetwork with residual connections captures spatial features from videos, and a\ntransformer encoder is exploited to design important temporal dynamics in\naudio. Moreover, the fusion architecture fused the extracted features through\nlate and intermediate fusion strategies to find out the most relevant\nintermodal correlations among them. Finally, the proposed network is assessed\non two large-scale depression detection datasets, and the results clearly\nreveal that it surpasses existing state-of-the-art approaches, improving the\nF1-Score by 13.92% for D-Vlog dataset and 7.74% for LMVD dataset. The code is\nmade available publicly at\nhttps://github.com/rezwanh001/Large-Scale-Multimodal-Depression-Detection.", "AI": {"tldr": "本文提出MMFformer，一个多模态抑郁症检测网络，利用社交媒体中的视频和音频数据，通过Transformer和融合策略捕捉时空模式，显著提升了抑郁症早期检测的准确性。", "motivation": "抑郁症早期检测困难，主要依赖主观评估。社交网络内容为早期诊断提供了新途径，但其信息量大且多样，导致难以准确提取时序信息和有效融合多模态数据。", "method": "引入MMFformer网络。该模型使用带残差连接的Transformer网络从视频中捕获空间特征，并利用Transformer编码器设计音频中的重要时间动态。此外，通过后期和中间融合策略融合提取的特征，以发现它们之间最相关的模态间关联。", "result": "MMFformer在两个大型抑郁症检测数据集（D-Vlog和LMVD）上进行了评估，结果表明它超越了现有最先进的方法，D-Vlog数据集的F1-Score提高了13.92%，LMVD数据集提高了7.74%。", "conclusion": "MMFformer通过有效整合社交媒体的多模态信息，能够从视频和音频中提取抑郁症相关的时空高层模式，显著提高了抑郁症早期检测的性能，为客观、早期诊断提供了有效工具。"}}
{"id": "2508.07063", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07063", "abs": "https://arxiv.org/abs/2508.07063", "authors": ["Naseem Machlovi", "Maryam Saleki", "Innocent Ababio", "Ruhul Amin"], "title": "Towards Safer AI Moderation: Evaluating LLM Moderators Through a Unified Benchmark Dataset and Advocating a Human-First Approach", "comment": null, "summary": "As AI systems become more integrated into daily life, the need for safer and\nmore reliable moderation has never been greater. Large Language Models (LLMs)\nhave demonstrated remarkable capabilities, surpassing earlier models in\ncomplexity and performance. Their evaluation across diverse tasks has\nconsistently showcased their potential, enabling the development of adaptive\nand personalized agents. However, despite these advancements, LLMs remain prone\nto errors, particularly in areas requiring nuanced moral reasoning. They\nstruggle with detecting implicit hate, offensive language, and gender biases\ndue to the subjective and context-dependent nature of these issues. Moreover,\ntheir reliance on training data can inadvertently reinforce societal biases,\nleading to inconsistencies and ethical concerns in their outputs. To explore\nthe limitations of LLMs in this role, we developed an experimental framework\nbased on state-of-the-art (SOTA) models to assess human emotions and offensive\nbehaviors. The framework introduces a unified benchmark dataset encompassing 49\ndistinct categories spanning the wide spectrum of human emotions, offensive and\nhateful text, and gender and racial biases. Furthermore, we introduced SafePhi,\na QLoRA fine-tuned version of Phi-4, adapting diverse ethical contexts and\noutperforming benchmark moderators by achieving a Macro F1 score of 0.89, where\nOpenAI Moderator and Llama Guard score 0.77 and 0.74, respectively. This\nresearch also highlights the critical domains where LLM moderators consistently\nunderperformed, pressing the need to incorporate more heterogeneous and\nrepresentative data with human-in-the-loop, for better model robustness and\nexplainability.", "AI": {"tldr": "该研究指出LLM在内容审核中，尤其在处理细微的道德推理、隐性仇恨和偏见方面存在不足。为此，提出了一个包含49类别的统一基准数据集，并引入了SafePhi（Phi-4的QLoRA微调版本），其在性能上显著优于现有模型，并强调了引入更异构、有代表性数据及人工干预的重要性。", "motivation": "随着AI系统日益融入日常生活，对更安全、更可靠的内容审核需求日益增长。尽管大型语言模型（LLMs）能力强大，但在需要细致道德推理的领域（如检测隐性仇恨、冒犯性语言和性别偏见）仍易出错，且可能因训练数据强化社会偏见，导致输出不一致和伦理问题。", "method": "开发了一个基于最先进模型的实验框架，用于评估人类情感和冒犯行为。构建了一个统一的基准数据集，涵盖49个不同类别，包括人类情感、冒犯和仇恨文本以及性别和种族偏见。引入了SafePhi，一个通过QLoRA微调的Phi-4版本，使其适应多样的伦理语境。", "result": "SafePhi在内容审核任务中取得了0.89的Macro F1分数，显著优于OpenAI Moderator（0.77）和Llama Guard（0.74）。研究同时揭示了LLM审核器在关键领域持续表现不佳的问题。", "conclusion": "为提高大型语言模型审核器的鲁棒性和可解释性，迫切需要结合更异构、更具代表性的数据，并融入人工干预（human-in-the-loop）机制。"}}
{"id": "2508.07319", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07319", "abs": "https://arxiv.org/abs/2508.07319", "authors": ["Yanzhao Yu", "Haotian Yang", "Junbo Tan", "Xueqian Wang"], "title": "A Hybrid Force-Position Strategy for Shape Control of Deformable Linear Objects With Graph Attention Networks", "comment": null, "summary": "Manipulating deformable linear objects (DLOs) such as wires and cables is\ncrucial in various applications like electronics assembly and medical\nsurgeries. However, it faces challenges due to DLOs' infinite degrees of\nfreedom, complex nonlinear dynamics, and the underactuated nature of the\nsystem. To address these issues, this paper proposes a hybrid force-position\nstrategy for DLO shape control. The framework, combining both force and\nposition representations of DLO, integrates state trajectory planning in the\nforce space and Model Predictive Control (MPC) in the position space. We\npresent a dynamics model with an explicit action encoder, a property extractor\nand a graph processor based on Graph Attention Networks. The model is used in\nthe MPC to enhance prediction accuracy. Results from both simulations and\nreal-world experiments demonstrate the effectiveness of our approach in\nachieving efficient and stable shape control of DLOs. Codes and videos are\navailable at https://sites.google.com/view/dlom.", "AI": {"tldr": "提出了一种混合力-位置策略，结合基于图注意力网络（GAT）的动力学模型和模型预测控制（MPC），用于实现可变形线性物体（DLO）的形状控制。", "motivation": "可变形线性物体（如电线、电缆）的操作在电子装配和医疗手术等应用中至关重要，但面临无限自由度、复杂非线性动力学和欠驱动系统的挑战。", "method": "提出混合力-位置策略：在力空间进行状态轨迹规划，在位置空间使用模型预测控制（MPC）。动力学模型包含显式动作编码器、属性提取器和基于图注意力网络（GAT）的图处理器，以提高MPC的预测精度。", "result": "仿真和真实世界实验结果表明，该方法在实现DLO高效稳定的形状控制方面是有效的。", "conclusion": "所提出的混合力-位置策略，结合GAT增强的动力学模型和MPC，能够有效解决DLO的形状控制难题，实现高效稳定的操作。"}}
{"id": "2508.07143", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07143", "abs": "https://arxiv.org/abs/2508.07143", "authors": ["Anna Seo Gyeong Choi", "Hoon Choi"], "title": "Fairness of Automatic Speech Recognition: Looking Through a Philosophical Lens", "comment": null, "summary": "Automatic Speech Recognition (ASR) systems now mediate countless\nhuman-technology interactions, yet research on their fairness implications\nremains surprisingly limited. This paper examines ASR bias through a\nphilosophical lens, arguing that systematic misrecognition of certain speech\nvarieties constitutes more than a technical limitation -- it represents a form\nof disrespect that compounds historical injustices against marginalized\nlinguistic communities. We distinguish between morally neutral classification\n(discriminate1) and harmful discrimination (discriminate2), demonstrating how\nASR systems can inadvertently transform the former into the latter when they\nconsistently misrecognize non-standard dialects. We identify three unique\nethical dimensions of speech technologies that differentiate ASR bias from\nother algorithmic fairness concerns: the temporal burden placed on speakers of\nnon-standard varieties (\"temporal taxation\"), the disruption of conversational\nflow when systems misrecognize speech, and the fundamental connection between\nspeech patterns and personal/cultural identity. These factors create asymmetric\npower relationships that existing technical fairness metrics fail to capture.\nThe paper analyzes the tension between linguistic standardization and pluralism\nin ASR development, arguing that current approaches often embed and reinforce\nproblematic language ideologies. We conclude that addressing ASR bias requires\nmore than technical interventions; it demands recognition of diverse speech\nvarieties as legitimate forms of expression worthy of technological\naccommodation. This philosophical reframing offers new pathways for developing\nASR systems that respect linguistic diversity and speaker autonomy.", "AI": {"tldr": "本文从哲学角度探讨自动语音识别（ASR）系统对特定语音变体的系统性错误识别，认为这不仅是技术限制，更是一种不尊重，加剧了对边缘化语言社区的历史不公。", "motivation": "尽管ASR系统广泛应用，但对其公平性影响的研究却出奇地有限。研究旨在指出ASR偏见不仅仅是技术问题，而是涉及道德和历史不公的深层问题。", "method": "采用哲学分析方法，区分了道德中立的分类（discriminate1）和有害的歧视（discriminate2）。识别了语音技术独特的三个伦理维度：非标准口音使用者的时间负担（“时间税”）、对话流程中断以及语音模式与个人/文化身份的根本联系。分析了ASR开发中语言标准化与多元主义之间的张力。", "result": "ASR系统对非标准方言的持续错误识别将中立分类转化为有害歧视。现有技术公平性指标未能捕捉由此产生的不对称权力关系。目前的ASR方法常嵌入并强化有问题的语言意识形态。", "conclusion": "解决ASR偏见不仅仅需要技术干预，更需要承认多样化的语音变体是合法的表达形式，值得技术适应。这种哲学上的重新定位为开发尊重语言多样性和说话者自主性的ASR系统提供了新途径。"}}
{"id": "2508.06703", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06703", "abs": "https://arxiv.org/abs/2508.06703", "authors": ["Justin London"], "title": "Fourier Optics and Deep Learning Methods for Fast 3D Reconstruction in Digital Holography", "comment": null, "summary": "Computer-generated holography (CGH) is a promising method that modulates\nuser-defined waveforms with digital holograms. An efficient and fast pipeline\nframework is proposed to synthesize CGH using initial point cloud and MRI data.\nThis input data is reconstructed into volumetric objects that are then input\ninto non-convex Fourier optics optimization algorithms for phase-only hologram\n(POH) and complex-hologram (CH) generation using alternating projection, SGD,\nand quasi-Netwton methods. Comparison of reconstruction performance of these\nalgorithms as measured by MSE, RMSE, and PSNR is analyzed as well as to HoloNet\ndeep learning CGH. Performance metrics are shown to be improved by using 2D\nmedian filtering to remove artifacts and speckled noise during optimization.", "AI": {"tldr": "本文提出了一种高效快速的计算全息图（CGH）合成框架，利用初始点云和MRI数据重建体素对象，并通过非凸傅里叶光学优化算法生成相位全息图和复数全息图，并与深度学习方法进行比较，同时引入2D中值滤波以提升性能。", "motivation": "计算全息图（CGH）是一种有前景的调制用户定义波形的方法，但需要高效快速的合成管道框架。研究旨在使用医学数据（如MRI）生成CGH，并优化其生成过程和重建质量。", "method": "该方法首先将初始点云和MRI数据重建为体素对象。然后，使用非凸傅里叶光学优化算法（包括交替投影、SGD和拟牛顿法）生成相位全息图（POH）和复数全息图（CH）。通过MSE、RMSE和PSNR等指标评估这些算法的重建性能，并与HoloNet深度学习CGH进行比较。此外，在优化过程中引入2D中值滤波以去除伪影和散斑噪声。", "result": "研究分析了不同算法的重建性能指标（MSE、RMSE、PSNR）。结果表明，通过使用2D中值滤波，可以有效改善优化过程中的性能指标，去除伪影和散斑噪声。", "conclusion": "本文提出的CGH合成管道框架能够高效快速地从点云和MRI数据生成全息图。通过非凸傅里叶光学优化算法结合2D中值滤波，可以有效提升全息图的生成质量和重建性能。"}}
{"id": "2508.07107", "categories": ["cs.AI", "cs.CY", "K.3.1; I.2.6; H.4"], "pdf": "https://arxiv.org/pdf/2508.07107", "abs": "https://arxiv.org/abs/2508.07107", "authors": ["Timothy Oluwapelumi Adeyemi", "Nadiah Fahad AlOtaibi"], "title": "Designing a Feedback-Driven Decision Support System for Dynamic Student Intervention", "comment": "10 pages, 1 figure, 3 tables", "summary": "Accurate prediction of student performance is essential for timely academic\nintervention. However, most machine learning models in education are static and\ncannot adapt when new data, such as post-intervention outcomes, become\navailable. To address this limitation, we propose a Feedback-Driven Decision\nSupport System (DSS) with a closed-loop architecture that enables continuous\nmodel refinement. The system integrates a LightGBM-based regressor with\nincremental retraining, allowing educators to input updated student results,\nwhich automatically trigger model updates. This adaptive mechanism improves\nprediction accuracy by learning from real-world academic progress. The platform\nfeatures a Flask-based web interface for real-time interaction and incorporates\nSHAP for explainability, ensuring transparency. Experimental results show a\n10.7\\% reduction in RMSE after retraining, with consistent upward adjustments\nin predicted scores for intervened students. By transforming static predictors\ninto self-improving systems, our approach advances educational analytics toward\nhuman-centered, data-driven, and responsive AI. The framework is designed for\nintegration into LMS and institutional dashboards.", "AI": {"tldr": "该论文提出了一个反馈驱动的决策支持系统（DSS），通过增量再训练和闭环架构，使学生表现预测模型能够持续适应新数据，从而提高预测准确性并实现响应式AI。", "motivation": "现有的教育领域机器学习模型大多是静态的，无法在新数据（如干预后的学生表现）可用时进行适应和更新，导致预测准确性受限，无法及时进行干预。", "method": "该系统采用基于LightGBM的回归器，结合增量再训练机制，允许教育者输入更新的学生成绩，自动触发模型更新。系统通过Flask构建了实时交互的Web界面，并集成了SHAP进行模型解释，以确保透明度。", "result": "实验结果显示，经过再训练后，模型的均方根误差（RMSE）降低了10.7%，并且对受干预学生的预测分数有持续的向上调整。", "conclusion": "该方法将静态预测器转变为自我改进的系统，将教育分析推向以人为本、数据驱动和响应式的人工智能，并可集成到学习管理系统（LMS）和机构仪表板中。"}}
{"id": "2508.07387", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07387", "abs": "https://arxiv.org/abs/2508.07387", "authors": ["Basant Sharma", "Prajyot Jadhav", "Pranjal Paul", "K. Madhava Krishna", "Arun Kumar Singh"], "title": "MonoMPC: Monocular Vision Based Navigation with Learned Collision Model and Risk-Aware Model Predictive Control", "comment": null, "summary": "Navigating unknown environments with a single RGB camera is challenging, as\nthe lack of depth information prevents reliable collision-checking. While some\nmethods use estimated depth to build collision maps, we found that depth\nestimates from vision foundation models are too noisy for zero-shot navigation\nin cluttered environments.\n  We propose an alternative approach: instead of using noisy estimated depth\nfor direct collision-checking, we use it as a rich context input to a learned\ncollision model. This model predicts the distribution of minimum obstacle\nclearance that the robot can expect for a given control sequence. At inference,\nthese predictions inform a risk-aware MPC planner that minimizes estimated\ncollision risk. Our joint learning pipeline co-trains the collision model and\nrisk metric using both safe and unsafe trajectories. Crucially, our\njoint-training ensures optimal variance in our collision model that improves\nnavigation in highly cluttered environments. Consequently, real-world\nexperiments show 9x and 7x improvements in success rates over NoMaD and the ROS\nstack, respectively. Ablation studies further validate the effectiveness of our\ndesign choices.", "AI": {"tldr": "针对单RGB相机在未知环境中导航时深度信息不足导致碰撞检测不可靠的问题，本文提出了一种新方法。它不直接使用噪声深度估计进行碰撞检测，而是将其作为学习碰撞模型的丰富上下文输入，该模型预测最小障碍物间隙的分布。结合风险感知MPC规划器和联合训练，显著提高了在杂乱环境中的导航成功率。", "motivation": "使用单个RGB相机在未知环境中导航极具挑战性，因为缺乏深度信息导致无法可靠地进行碰撞检测。尽管有些方法尝试使用估计深度来构建碰撞图，但视觉基础模型的深度估计噪声过大，无法在零样本杂乱环境中进行可靠导航。", "method": "本文提出了一种替代方法：不直接使用噪声估计深度进行碰撞检测，而是将其作为学习碰撞模型的丰富上下文输入。该模型预测给定控制序列下机器人预期的最小障碍物间隙分布。在推理时，这些预测信息被用于一个风险感知MPC规划器，以最小化估计碰撞风险。整个学习流程联合训练碰撞模型和风险度量，利用安全和不安全的轨迹。联合训练确保了碰撞模型的最优方差，从而改善了在高度杂乱环境中的导航。", "result": "实际实验表明，与NoMaD和ROS堆栈相比，成功率分别提高了9倍和7倍。消融研究进一步验证了设计选择的有效性。", "conclusion": "通过将噪声深度估计作为学习碰撞模型的上下文输入，并结合风险感知MPC规划器和联合训练策略，本文提出的方法显著提高了单RGB相机在杂乱环境中的导航性能，克服了深度估计噪声的挑战。"}}
{"id": "2508.07172", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.07172", "abs": "https://arxiv.org/abs/2508.07172", "authors": ["Biao Yi", "Jiahao Li", "Baolei Zhang", "Lihai Nie", "Tong Li", "Tiansheng Huang", "Zheli Liu"], "title": "Gradient Surgery for Safe LLM Fine-Tuning", "comment": null, "summary": "Fine-tuning-as-a-Service introduces a critical vulnerability where a few\nmalicious examples mixed into the user's fine-tuning dataset can compromise the\nsafety alignment of Large Language Models (LLMs). While a recognized paradigm\nframes safe fine-tuning as a multi-objective optimization problem balancing\nuser task performance with safety alignment, we find existing solutions are\ncritically sensitive to the harmful ratio, with defenses degrading sharply as\nharmful ratio increases. We diagnose that this failure stems from conflicting\ngradients, where the user-task update directly undermines the safety objective.\nTo resolve this, we propose SafeGrad, a novel method that employs gradient\nsurgery. When a conflict is detected, SafeGrad nullifies the harmful component\nof the user-task gradient by projecting it onto the orthogonal plane of the\nalignment gradient, allowing the model to learn the user's task without\nsacrificing safety. To further enhance robustness and data efficiency, we\nemploy a KL-divergence alignment loss that learns the rich, distributional\nsafety profile of the well-aligned foundation model. Extensive experiments show\nthat SafeGrad provides state-of-the-art defense across various LLMs and\ndatasets, maintaining robust safety even at high harmful ratios without\ncompromising task fidelity.", "AI": {"tldr": "研究发现，在“微调即服务”中，少量恶意数据即可损害大型语言模型的安全性。现有防御在恶意数据比例高时失效。本文提出SafeGrad，通过梯度手术解决任务和安全之间的梯度冲突，并结合KL散度对齐损失，实现在高恶意数据比例下仍能保持模型安全性和任务性能。", "motivation": "在“微调即服务”场景下，用户数据中的少量恶意示例即可破坏大型语言模型的安全对齐。现有防御方法在恶意数据比例增加时性能急剧下降，原因是用户任务更新与安全目标之间存在冲突梯度，直接损害了安全对齐。", "method": "本文提出SafeGrad方法。当检测到冲突时，SafeGrad通过梯度手术，将用户任务梯度投影到对齐梯度的正交平面上，以消除用户任务梯度中的有害成分，从而在不牺牲安全性的前提下学习用户任务。为进一步增强鲁棒性和数据效率，还引入了KL散度对齐损失，以学习基础模型的丰富、分布式的安全配置文件。", "result": "广泛实验表明，SafeGrad在各种大型语言模型和数据集上都提供了最先进的防御效果，即使在恶意数据比例很高的情况下也能保持强大的安全性，同时不影响任务的准确性。", "conclusion": "SafeGrad通过解决微调过程中任务和安全目标之间的梯度冲突，并利用KL散度对齐损失增强鲁棒性，有效提升了大型语言模型在“微调即服务”场景下的安全性，即使面对高比例恶意数据也能保持卓越性能。"}}
{"id": "2508.06715", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06715", "abs": "https://arxiv.org/abs/2508.06715", "authors": ["Jixuan He", "Chieh Hubert Lin", "Lu Qi", "Ming-Hsuan Yang"], "title": "Restage4D: Reanimating Deformable 3D Reconstruction from a Single Video", "comment": null, "summary": "Creating deformable 3D content has gained increasing attention with the rise\nof text-to-image and image-to-video generative models. While these models\nprovide rich semantic priors for appearance, they struggle to capture the\nphysical realism and motion dynamics needed for authentic 4D scene synthesis.\nIn contrast, real-world videos can provide physically grounded geometry and\narticulation cues that are difficult to hallucinate. One question is raised:\n\\textit{Can we generate physically consistent 4D content by leveraging the\nmotion priors of the real-world video}? In this work, we explore the task of\nreanimating deformable 3D scenes from a single video, using the original\nsequence as a supervisory signal to correct artifacts from synthetic motion. We\nintroduce \\textbf{Restage4D}, a geometry-preserving pipeline for\nvideo-conditioned 4D restaging. Our approach uses a video-rewinding training\nstrategy to temporally bridge a real base video and a synthetic driving video\nvia a shared motion representation. We further incorporate an occlusion-aware\nrigidity loss and a disocclusion backtracing mechanism to improve structural\nand geometry consistency under challenging motion. We validate Restage4D on\nDAVIS and PointOdyssey, demonstrating improved geometry consistency, motion\nquality, and 3D tracking performance. Our method not only preserves deformable\nstructure under novel motion, but also automatically corrects errors introduced\nby generative models, revealing the potential of video prior in 4D restaging\ntask. Source code and trained models will be released.", "AI": {"tldr": "该研究提出Restage4D，一个利用单视频运动先验进行4D重演的几何保持管道，以生成物理一致的可变形3D内容，并纠正生成模型的错误。", "motivation": "现有的文本到图像/视频生成模型在捕捉物理真实感和运动动态方面存在不足，难以用于真实的4D场景合成。而真实世界视频能提供难以凭空想象的物理几何和关节提示。因此，研究旨在探索能否通过利用真实世界视频的运动先验来生成物理一致的4D内容。", "method": "该方法引入了Restage4D，一个用于视频条件4D重演的几何保持管道。它采用视频回溯训练策略，通过共享运动表示来连接真实基础视频和合成驱动视频。此外，还结合了遮挡感知刚性损失和去遮挡回溯机制，以在复杂运动下提高结构和几何一致性。", "result": "Restage4D在DAVIS和PointOdyssey数据集上进行了验证，结果表明其显著改善了几何一致性、运动质量和3D跟踪性能。该方法不仅在新的运动下保持了可变形结构，还自动纠正了生成模型引入的错误。", "conclusion": "该研究揭示了视频先验在4D重演任务中的巨大潜力，能够从单个视频生成物理一致的可变形3D内容，并有效纠正生成模型产生的伪影。"}}
{"id": "2508.07186", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.07186", "abs": "https://arxiv.org/abs/2508.07186", "authors": ["Amit Dhanda"], "title": "Multi-Dimensional Summarization Agents with Context-Aware Reasoning over Enterprise Tables", "comment": null, "summary": "We propose a novel framework for summarizing structured enterprise data\nacross multiple dimensions using large language model (LLM)-based agents.\nTraditional table-to-text models often lack the capacity to reason across\nhierarchical structures and context-aware deltas, which are essential in\nbusiness reporting tasks. Our method introduces a multi-agent pipeline that\nextracts, analyzes, and summarizes multi-dimensional data using agents for\nslicing, variance detection, context construction, and LLM-based generation.\nOur results show that the proposed framework outperforms traditional\napproaches, achieving 83\\% faithfulness to underlying data, superior coverage\nof significant changes, and high relevance scores (4.4/5) for decision-critical\ninsights. The improvements are especially pronounced in categories involving\nsubtle trade-offs, such as increased revenue due to price changes amid\ndeclining unit volumes, which competing methods either overlook or address with\nlimited specificity. We evaluate the framework on Kaggle datasets and\ndemonstrate significant improvements in faithfulness, relevance, and insight\nquality over baseline table summarization approaches.", "AI": {"tldr": "提出了一种基于LLM代理的多代理框架，用于多维度结构化企业数据摘要，显著提升了商业报告的准确性和洞察力。", "motivation": "传统的表格到文本模型在处理层次结构和上下文敏感的增量数据时推理能力不足，而这在商业报告任务中至关重要。", "method": "引入了一个多代理管道，通过代理进行切片、差异检测、上下文构建和基于LLM的生成，以提取、分析和总结多维度数据。", "result": "该框架优于传统方法，实现了83%的数据忠实度、对显著变化的卓越覆盖以及决策关键洞察的高相关性评分（4.4/5），尤其在涉及微妙权衡的类别中表现出色。在Kaggle数据集上验证了其在忠实度、相关性和洞察质量方面的显著改进。", "conclusion": "该LLM代理多代理框架显著提高了多维度企业数据摘要的忠实度、相关性和洞察质量，解决了传统方法在复杂商业报告中的局限性。"}}
{"id": "2508.07406", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07406", "abs": "https://arxiv.org/abs/2508.07406", "authors": ["Xiaobei Zhao", "Xingqi Lyu", "Xiang Li"], "title": "AgriVLN: Vision-and-Language Navigation for Agricultural Robots", "comment": null, "summary": "Agricultural robots have emerged as powerful members in agricultural tasks,\nnevertheless, still heavily rely on manual operation or untransportable railway\nfor movement, resulting in limited mobility and poor adaptability.\nVision-and-Language Navigation (VLN) enables robots to navigate to the target\ndestinations following natural language instructions, demonstrating strong\nperformance on several domains. However, none of the existing benchmarks or\nmethods is specifically designed for agricultural scenes. To bridge this gap,\nwe propose Agriculture to Agriculture (A2A) benchmark, containing 1,560\nepisodes across six diverse agricultural scenes, in which all realistic RGB\nvideos are captured by front-facing camera on a quadruped robot at a height of\n0.38 meters, aligning with the practical deployment conditions. Meanwhile, we\npropose Vision-and-Language Navigation for Agricultural Robots (AgriVLN)\nbaseline based on Vision-Language Model (VLM) prompted with carefully crafted\ntemplates, which can understand both given instructions and agricultural\nenvironments to generate appropriate low-level actions for robot control. When\nevaluated on A2A, AgriVLN performs well on short instructions but struggles\nwith long instructions, because it often fails to track which part of the\ninstruction is currently being executed. To address this, we further propose\nSubtask List (STL) instruction decomposition module and integrate it into\nAgriVLN, improving Success Rate (SR) from 0.33 to 0.47. We additionally compare\nAgriVLN with several existing VLN methods, demonstrating the state-of-the-art\nperformance in the agricultural domain.", "AI": {"tldr": "本文提出了A2A基准和AgriVLN方法，以解决农业机器人视觉-语言导航（VLN）的空白，并通过子任务分解模块显著提高了长指令下的导航成功率。", "motivation": "现有农业机器人移动性受限，依赖人工操作或固定轨道，且缺乏专门针对农业场景的视觉-语言导航（VLN）基准和方法。", "method": "提出了A2A（Agriculture to Agriculture）基准，包含1,560个在六个不同农业场景中由四足机器人捕获的RGB视频导航情节。同时，提出了AgriVLN（Vision-and-Language Navigation for Agricultural Robots）基线，该基线基于视觉-语言模型（VLM），利用精心设计的模板理解指令和环境以生成低级动作。为解决长指令问题，进一步提出了子任务列表（STL）指令分解模块并集成到AgriVLN中。", "result": "AgriVLN在短指令上表现良好，但在长指令上表现不佳，因为它难以跟踪指令的执行进度。集成STL模块后，AgriVLN的成功率（SR）从0.33提高到0.47。与现有VLN方法相比，AgriVLN在农业领域展现了最先进的性能。", "conclusion": "A2A基准和AgriVLN（特别是结合STL模块）有效地解决了农业机器人视觉-语言导航的挑战，显著提升了其在复杂农业环境中的导航能力。"}}
{"id": "2508.07173", "categories": ["cs.CL", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.07173", "abs": "https://arxiv.org/abs/2508.07173", "authors": ["Leyi Pan", "Zheyu Fu", "Yunpeng Zhai", "Shuchang Tao", "Sheng Guan", "Shiyu Huang", "Lingzhe Zhang", "Zhaoyang Liu", "Bolin Ding", "Felix Henry", "Lijie Wen", "Aiwei Liu"], "title": "Omni-SafetyBench: A Benchmark for Safety Evaluation of Audio-Visual Large Language Models", "comment": "20 pages, 8 figures, 12 tables", "summary": "The rise of Omni-modal Large Language Models (OLLMs), which integrate visual\nand auditory processing with text, necessitates robust safety evaluations to\nmitigate harmful outputs. However, no dedicated benchmarks currently exist for\nOLLMs, and prior benchmarks designed for other LLMs lack the ability to assess\nsafety performance under audio-visual joint inputs or cross-modal safety\nconsistency. To fill this gap, we introduce Omni-SafetyBench, the first\ncomprehensive parallel benchmark for OLLM safety evaluation, featuring 24\nmodality combinations and variations with 972 samples each, including dedicated\naudio-visual harm cases. Considering OLLMs' comprehension challenges with\ncomplex omni-modal inputs and the need for cross-modal consistency evaluation,\nwe propose tailored metrics: a Safety-score based on conditional Attack Success\nRate (C-ASR) and Refusal Rate (C-RR) to account for comprehension failures, and\na Cross-Modal Safety Consistency Score (CMSC-score) to measure consistency\nacross modalities. Evaluating 6 open-source and 4 closed-source OLLMs reveals\ncritical vulnerabilities: (1) no model excels in both overall safety and\nconsistency, with only 3 models achieving over 0.6 in both metrics and top\nperformer scoring around 0.8; (2) safety defenses weaken with complex inputs,\nespecially audio-visual joints; (3) severe weaknesses persist, with some models\nscoring as low as 0.14 on specific modalities. Our benchmark and metrics\nhighlight urgent needs for enhanced OLLM safety, providing a foundation for\nfuture improvements.", "AI": {"tldr": "本文提出了Omni-SafetyBench，首个针对全模态大语言模型（OLLMs）的综合安全评估基准，包含多模态组合和专门的音视频危害案例，并引入了考虑理解失败和跨模态一致性的新评估指标，揭示了当前OLLMs在安全性和一致性方面的显著漏洞。", "motivation": "随着OLLMs的兴起，将视觉和听觉处理与文本相结合，急需强大的安全评估来减轻有害输出。然而，目前缺乏专门针对OLLMs的基准测试，现有为其他LLMs设计的基准无法评估音视频联合输入下的安全性能或跨模态安全一致性。", "method": "引入了Omni-SafetyBench，一个全面的并行基准，包含24种模态组合和变体，每种972个样本，包括专门的音视频危害案例。提出了定制的度量标准：基于条件攻击成功率（C-ASR）和拒绝率（C-RR）的安全分数（Safety-score）以解决理解失败问题，以及用于衡量跨模态一致性的跨模态安全一致性分数（CMSC-score）。评估了6个开源和4个闭源OLLMs。", "result": "评估结果揭示了关键漏洞：(1) 没有模型在整体安全性和一致性方面均表现出色，只有3个模型在两项指标上都超过0.6分，表现最好的模型得分约为0.8；(2) 安全防御在复杂输入（特别是音视频联合输入）下会减弱；(3) 严重弱点依然存在，某些模型在特定模态上的得分低至0.14。", "conclusion": "本研究提出的基准和度量标准凸显了OLLM安全增强的迫切需求，并为未来的改进奠定了基础。"}}
{"id": "2508.06756", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06756", "abs": "https://arxiv.org/abs/2508.06756", "authors": ["Somayeh Farahani", "Marjaneh Hejazi", "Antonio Di Ieva", "Sidong Liu"], "title": "FoundBioNet: A Foundation-Based Model for IDH Genotyping of Glioma from Multi-Parametric MRI", "comment": "Accepted for oral and poster presentation at MICCAI 2025", "summary": "Accurate, noninvasive detection of isocitrate dehydrogenase (IDH) mutation is\nessential for effective glioma management. Traditional methods rely on invasive\ntissue sampling, which may fail to capture a tumor's spatial heterogeneity.\nWhile deep learning models have shown promise in molecular profiling, their\nperformance is often limited by scarce annotated data. In contrast, foundation\ndeep learning models offer a more generalizable approach for glioma imaging\nbiomarkers. We propose a Foundation-based Biomarker Network (FoundBioNet) that\nutilizes a SWIN-UNETR-based architecture to noninvasively predict IDH mutation\nstatus from multi-parametric MRI. Two key modules are incorporated: Tumor-Aware\nFeature Encoding (TAFE) for extracting multi-scale, tumor-focused features, and\nCross-Modality Differential (CMD) for highlighting subtle T2-FLAIR mismatch\nsignals associated with IDH mutation. The model was trained and validated on a\ndiverse, multi-center cohort of 1705 glioma patients from six public datasets.\nOur model achieved AUCs of 90.58%, 88.08%, 65.41%, and 80.31% on independent\ntest sets from EGD, TCGA, Ivy GAP, RHUH, and UPenn, consistently outperforming\nbaseline approaches (p <= 0.05). Ablation studies confirmed that both the TAFE\nand CMD modules are essential for improving predictive accuracy. By integrating\nlarge-scale pretraining and task-specific fine-tuning, FoundBioNet enables\ngeneralizable glioma characterization. This approach enhances diagnostic\naccuracy and interpretability, with the potential to enable more personalized\npatient care.", "AI": {"tldr": "该研究提出了一个名为FoundBioNet的基于基础模型的深度学习网络，通过多参数MRI无创预测胶质瘤IDH突变状态，并在多中心数据集上表现出卓越的泛化性和准确性。", "motivation": "传统的IDH突变检测方法依赖侵入性组织活检，难以捕捉肿瘤空间异质性；而现有深度学习模型受限于稀缺的标注数据，泛化能力不足。因此，需要一种更通用、无创且准确的检测方法。", "method": "本文提出了FoundBioNet模型，采用基于SWIN-UNETR的架构，结合两个关键模块：肿瘤感知特征编码（TAFE）用于提取多尺度、肿瘤聚焦特征，以及交叉模态差异（CMD）用于突出与IDH突变相关的细微T2-FLAIR失配信号。模型在来自六个公共数据集的1705名胶质瘤患者的多中心队列上进行训练和验证。", "result": "FoundBioNet在EGD、TCGA、Ivy GAP、RHUH和UPenn的独立测试集上分别取得了90.58%、88.08%、65.41%和80.31%的AUC，持续优于基线方法（p <= 0.05）。消融研究证实TAFE和CMD模块对提高预测准确性至关重要。", "conclusion": "FoundBioNet通过整合大规模预训练和任务特定微调，实现了胶质瘤特征描述的泛化能力，显著提高了诊断准确性和可解释性，有望实现更个性化的患者护理。"}}
{"id": "2508.07292", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.07292", "abs": "https://arxiv.org/abs/2508.07292", "authors": ["Yi Tang", "Kaini Wang", "Yang Chen", "Guangquan Zhou"], "title": "EndoAgent: A Memory-Guided Reflective Agent for Intelligent Endoscopic Vision-to-Decision Reasoning", "comment": null, "summary": "Developing general artificial intelligence (AI) systems to support endoscopic\nimage diagnosis is an emerging research priority. Existing methods based on\nlarge-scale pretraining often lack unified coordination across tasks and\nstruggle to handle the multi-step processes required in complex clinical\nworkflows. While AI agents have shown promise in flexible instruction parsing\nand tool integration across domains, their potential in endoscopy remains\nunderexplored. To address this gap, we propose EndoAgent, the first\nmemory-guided agent for vision-to-decision endoscopic analysis that integrates\niterative reasoning with adaptive tool selection and collaboration. Built on a\ndual-memory design, it enables sophisticated decision-making by ensuring\nlogical coherence through short-term action tracking and progressively\nenhancing reasoning acuity through long-term experiential learning. To support\ndiverse clinical tasks, EndoAgent integrates a suite of expert-designed tools\nwithin a unified reasoning loop. We further introduce EndoAgentBench, a\nbenchmark of 5,709 visual question-answer pairs that assess visual\nunderstanding and language generation capabilities in realistic scenarios.\nExtensive experiments show that EndoAgent consistently outperforms both general\nand medical multimodal models, exhibiting its strong flexibility and reasoning\ncapabilities.", "AI": {"tldr": "本文提出了EndoAgent，首个用于内窥镜图像诊断的记忆引导AI智能体，通过双记忆设计和工具集成，在多任务临床工作流中展现出卓越的推理和决策能力，并优于现有模型。", "motivation": "现有基于大规模预训练的AI方法在内窥镜图像诊断中缺乏任务间的统一协调，难以处理复杂的临床多步骤流程。尽管AI智能体在指令解析和工具集成方面潜力巨大，但在内窥镜领域的应用尚未被充分探索。", "method": "本文提出了EndoAgent，一个记忆引导的视觉到决策内窥镜分析智能体，它整合了迭代推理、自适应工具选择和协作。其核心是一个双记忆设计，包括用于短期行动追踪的短期记忆和用于逐步增强推理能力的长期经验学习记忆。EndoAgent还集成了一套专家设计的工具集，以支持多样化的临床任务。此外，本文引入了EndoAgentBench，一个包含5,709个视觉问答对的基准测试集，用于评估视觉理解和语言生成能力。", "result": "广泛的实验表明，EndoAgent持续优于通用和医学多模态模型，展现出强大的灵活性和推理能力。", "conclusion": "EndoAgent作为首个记忆引导的内窥镜分析智能体，通过其独特的双记忆设计和工具集成，有效解决了现有AI在内窥镜诊断中面临的挑战，并在复杂临床任务中表现出卓越的性能和适应性。"}}
{"id": "2508.07421", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07421", "abs": "https://arxiv.org/abs/2508.07421", "authors": ["Zixi Jia", "Hongbin Gao", "Fashe Li", "Jiqiang Liu", "Hexiao Li", "Qinghua Liu"], "title": "Triple-S: A Collaborative Multi-LLM Framework for Solving Long-Horizon Implicative Tasks in Robotics", "comment": "Accepted to IROS 2025", "summary": "Leveraging Large Language Models (LLMs) to write policy code for controlling\nrobots has gained significant attention. However, in long-horizon implicative\ntasks, this approach often results in API parameter, comments and sequencing\nerrors, leading to task failure. To address this problem, we propose a\ncollaborative Triple-S framework that involves multiple LLMs. Through\nIn-Context Learning, different LLMs assume specific roles in a closed-loop\nSimplification-Solution-Summary process, effectively improving success rates\nand robustness in long-horizon implicative tasks. Additionally, a novel\ndemonstration library update mechanism which learned from success allows it to\ngeneralize to previously failed tasks. We validate the framework in the\nLong-horizon Desktop Implicative Placement (LDIP) dataset across various\nbaseline models, where Triple-S successfully executes 89% of tasks in both\nobservable and partially observable scenarios. Experiments in both simulation\nand real-world robot settings further validated the effectiveness of Triple-S.\nOur code and dataset is available at: https://github.com/Ghbbbbb/Triple-S.", "AI": {"tldr": "该论文提出了一个名为Triple-S的协作式多LLM框架，通过简化-解决方案-总结的闭环过程和演示库更新机制，显著提高了LLM在长周期隐含性机器人任务中生成策略代码的成功率和鲁棒性。", "motivation": "现有利用大型语言模型（LLMs）为机器人编写策略代码的方法，在长周期隐含性任务中常因API参数、注释和序列错误导致任务失败。", "method": "提出Triple-S协作框架，涉及多个LLM。通过上下文学习，不同LLM在简化-解决方案-总结的闭环过程中扮演特定角色。此外，引入了一种新颖的演示库更新机制，通过从成功经验中学习，使其能够泛化到先前失败的任务。", "result": "在长周期桌面隐含性放置（LDIP）数据集上，Triple-S在可观察和部分可观察场景中成功执行了89%的任务。仿真和真实机器人实验均验证了Triple-S的有效性。", "conclusion": "Triple-S框架通过多LLM协作和动态演示学习，有效解决了LLM在长周期隐含性机器人任务中生成策略代码的错误问题，显著提高了任务成功率和系统鲁棒性。"}}
{"id": "2508.07178", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07178", "abs": "https://arxiv.org/abs/2508.07178", "authors": ["Kejin Liu", "Junhong Lian", "Xiang Ao", "Ningtao Wang", "Xing Fu", "Yu Cheng", "Weiqiang Wang", "Xinyu Liu"], "title": "Improved Personalized Headline Generation via Denoising Fake Interests from Implicit Feedback", "comment": "Accepted by the 34th ACM International Conference on Information and\n  Knowledge Management (CIKM '25), Full Research Papers track", "summary": "Accurate personalized headline generation hinges on precisely capturing user\ninterests from historical behaviors. However, existing methods neglect\npersonalized-irrelevant click noise in entire historical clickstreams, which\nmay lead to hallucinated headlines that deviate from genuine user preferences.\nIn this paper, we reveal the detrimental impact of click noise on personalized\ngeneration quality through rigorous analysis in both user and news dimensions.\nBased on these insights, we propose a novel Personalized Headline Generation\nframework via Denoising Fake Interests from Implicit Feedback (PHG-DIF).\nPHG-DIF first employs dual-stage filtering to effectively remove clickstream\nnoise, identified by short dwell times and abnormal click bursts, and then\nleverages multi-level temporal fusion to dynamically model users' evolving and\nmulti-faceted interests for precise profiling. Moreover, we release DT-PENS, a\nnew benchmark dataset comprising the click behavior of 1,000 carefully curated\nusers and nearly 10,000 annotated personalized headlines with historical dwell\ntime annotations. Extensive experiments demonstrate that PHG-DIF substantially\nmitigates the adverse effects of click noise and significantly improves\nheadline quality, achieving state-of-the-art (SOTA) results on DT-PENS. Our\nframework implementation and dataset are available at\nhttps://github.com/liukejin-up/PHG-DIF.", "AI": {"tldr": "提出PHG-DIF框架，通过去噪虚假兴趣来提高个性化标题生成质量，并发布新的DT-PENS数据集。", "motivation": "现有个性化标题生成方法忽略了用户历史点击流中的“点击噪声”（即与用户兴趣无关的点击），导致生成的标题偏离用户真实偏好。", "method": "提出PHG-DIF框架：1. 使用双阶段过滤（基于短停留时间和异常点击爆发）有效去除点击流噪声。2. 利用多级时间融合动态建模用户演变的多方面兴趣。此外，发布了新的基准数据集DT-PENS，包含1000名用户的点击行为和近10000条带停留时间注释的个性化标题。", "result": "PHG-DIF显著减轻了点击噪声的不利影响，显著提高了标题质量，并在DT-PENS数据集上取得了最先进（SOTA）的结果。", "conclusion": "点击噪声对个性化标题生成质量有负面影响。PHG-DIF通过有效去除虚假兴趣并精确建模用户偏好，显著提高了个性化标题的生成质量。"}}
{"id": "2508.06757", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2508.06757", "abs": "https://arxiv.org/abs/2508.06757", "authors": ["Yash Garg", "Saketh Bachu", "Arindam Dutta", "Rohit Lal", "Sarosij Bose", "Calvin-Khang Ta", "M. Salman Asif", "Amit Roy-Chowdhury"], "title": "VOccl3D: A Video Benchmark Dataset for 3D Human Pose and Shape Estimation under real Occlusions", "comment": null, "summary": "Human pose and shape (HPS) estimation methods have been extensively studied,\nwith many demonstrating high zero-shot performance on in-the-wild images and\nvideos. However, these methods often struggle in challenging scenarios\ninvolving complex human poses or significant occlusions. Although some studies\naddress 3D human pose estimation under occlusion, they typically evaluate\nperformance on datasets that lack realistic or substantial occlusions, e.g.,\nmost existing datasets introduce occlusions with random patches over the human\nor clipart-style overlays, which may not reflect real-world challenges. To\nbridge this gap in realistic occlusion datasets, we introduce a novel benchmark\ndataset, VOccl3D, a Video-based human Occlusion dataset with 3D body pose and\nshape annotations. Inspired by works such as AGORA and BEDLAM, we constructed\nthis dataset using advanced computer graphics rendering techniques,\nincorporating diverse real-world occlusion scenarios, clothing textures, and\nhuman motions. Additionally, we fine-tuned recent HPS methods, CLIFF and\nBEDLAM-CLIFF, on our dataset, demonstrating significant qualitative and\nquantitative improvements across multiple public datasets, as well as on the\ntest split of our dataset, while comparing its performance with other\nstate-of-the-art methods. Furthermore, we leveraged our dataset to enhance\nhuman detection performance under occlusion by fine-tuning an existing object\ndetector, YOLO11, thus leading to a robust end-to-end HPS estimation system\nunder occlusions. Overall, this dataset serves as a valuable resource for\nfuture research aimed at benchmarking methods designed to handle occlusions,\noffering a more realistic alternative to existing occlusion datasets. See the\nProject page for code and dataset:https://yashgarg98.github.io/VOccl3D-dataset/", "AI": {"tldr": "该论文提出了VOccl3D，一个基于视频的3D人体姿态和形状标注遮挡数据集，旨在解决现有方法在真实世界遮挡场景中性能不佳的问题，并展示了其在提升人体姿态估计和人体检测方面的有效性。", "motivation": "现有的人体姿态和形状（HPS）估计方法在复杂姿态或显著遮挡场景下表现不佳。此外，大多数现有数据集中的遮挡不真实，例如使用随机补丁或剪贴画叠加，无法反映真实世界的挑战。因此，需要一个包含真实世界遮挡场景的新基准数据集。", "method": "1. 构建了VOccl3D数据集：一个基于视频的3D人体姿态和形状标注遮挡数据集，利用先进的计算机图形渲染技术，融入多样化的真实世界遮挡场景、服装纹理和人体运动。\n2. 微调HPS方法：在VOccl3D数据集上微调了CLIFF和BEDLAM-CLIFF等HPS方法。\n3. 增强人体检测：微调了现有的目标检测器YOLO11，以提高遮挡下的人体检测性能。", "result": "1. 经过VOccl3D数据集微调的HPS方法（CLIFF和BEDLAM-CLIFF）在多个公共数据集以及VOccl3D的测试集上均显示出显著的定性和定量改进。\n2. 利用VOccl3D数据集微调后，在遮挡下的人体检测性能得到了提升，从而实现了一个在遮挡下鲁棒的端到端HPS估计系统。", "conclusion": "VOccl3D数据集为未来旨在处理遮挡的方法基准测试提供了宝贵的资源，它比现有遮挡数据集提供了更真实的替代方案，并能有效提升人体姿态估计和检测在遮挡场景下的性能。"}}
{"id": "2508.07334", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07334", "abs": "https://arxiv.org/abs/2508.07334", "authors": ["Quan Shi", "Wang Xi", "Zenghui Ding", "Jianqing Gao", "Xianjun Yang"], "title": "Hallucination as a Computational Boundary: A Hierarchy of Inevitability and the Oracle Escape", "comment": "8 pages, 6 figures", "summary": "The illusion phenomenon of large language models (LLMs) is the core obstacle\nto their reliable deployment. This article formalizes the large language model\nas a probabilistic Turing machine by constructing a \"computational necessity\nhierarchy\", and for the first time proves the illusions are inevitable on\ndiagonalization, incomputability, and information theory boundaries supported\nby the new \"learner pump lemma\". However, we propose two \"escape routes\": one\nis to model Retrieval Enhanced Generations (RAGs) as oracle machines, proving\ntheir absolute escape through \"computational jumps\", providing the first formal\ntheory for the effectiveness of RAGs; The second is to formalize continuous\nlearning as an \"internalized oracle\" mechanism and implement this path through\na novel neural game theory framework.Finally, this article proposes a", "AI": {"tldr": "本文证明大型语言模型（LLM）的幻觉是不可避免的，并提出了两种“逃逸路线”：将检索增强生成（RAG）建模为预言机以实现计算跳跃，以及将持续学习形式化为内化预言机机制。", "motivation": "LLM的幻觉现象是其可靠部署的核心障碍，需要对其进行深入的理论分析和解决方案。", "method": "将LLM形式化为概率图灵机，构建“计算必要性层次”，并提出“学习器泵引理”来证明幻觉的不可避免性。同时，将RAG建模为预言机，并将持续学习形式化为“内化预言机”机制，通过新型神经博弈论框架实现。", "result": "首次证明了LLM幻觉在对角化、不可计算性和信息论边界上是不可避免的。首次为RAG的有效性提供了形式化理论支持，证明其通过“计算跳跃”可实现绝对逃逸。提出了持续学习作为应对幻觉的“内化预言机”路径。", "conclusion": "LLM的幻觉具有理论上的必然性，但RAG和持续学习机制为克服这一问题提供了可行的“逃逸路线”和形式化理论基础。"}}
{"id": "2508.07502", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07502", "abs": "https://arxiv.org/abs/2508.07502", "authors": ["Mateus Salomão", "Tianyü Ren", "Alexander König"], "title": "A Learning-Based Framework for Collision-Free Motion Planning", "comment": null, "summary": "This paper presents a learning-based extension to a Circular Field (CF)-based\nmotion planner for efficient, collision-free trajectory generation in cluttered\nenvironments. The proposed approach overcomes the limitations of hand-tuned\nforce field parameters by employing a deep neural network trained to infer\noptimal planner gains from a single depth image of the scene. The pipeline\nincorporates a CUDA-accelerated perception module, a predictive agent-based\nplanning strategy, and a dataset generated through Bayesian optimization in\nsimulation. The resulting framework enables real-time planning without manual\nparameter tuning and is validated both in simulation and on a Franka Emika\nPanda robot. Experimental results demonstrate successful task completion and\nimproved generalization compared to classical planners.", "AI": {"tldr": "本文提出了一种基于学习的圆形力场（CF）运动规划器扩展，利用深度神经网络从单张深度图像中推断最优规划器增益，从而在复杂环境中实现高效、无碰撞的实时轨迹生成，无需手动参数调整。", "motivation": "克服传统圆形力场（CF）运动规划器中手动调整力场参数的局限性，特别是在杂乱环境中难以高效生成无碰撞轨迹的问题。", "method": "该方法将深度神经网络应用于CF运动规划器，通过训练网络从单张深度图像中推断最优规划器增益。流程包括一个CUDA加速的感知模块、一个预测性基于代理的规划策略，以及一个通过贝叶斯优化在仿真中生成的数据集。该框架在仿真和实际机器人上都进行了验证。", "result": "该框架实现了无需手动参数调整的实时规划，并在仿真和Frank Emika Panda机器人上得到验证。实验结果表明，该方法能够成功完成任务，并且相比经典规划器具有更好的泛化能力。", "conclusion": "所提出的基于学习的CF运动规划器扩展有效地解决了手动参数调整的痛点，实现了在复杂环境中高效、实时、无碰撞的轨迹生成，并展现出优于传统方法的泛化性能。"}}
{"id": "2508.07179", "categories": ["cs.CL", "cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2508.07179", "abs": "https://arxiv.org/abs/2508.07179", "authors": ["Jiaqi Yin", "Yi-Wei Chen", "Meng-Lung Lee", "Xiya Liu"], "title": "Schema Lineage Extraction at Scale: Multilingual Pipelines, Composite Evaluation, and Language-Model Benchmarks", "comment": null, "summary": "Enterprise data pipelines, characterized by complex transformations across\nmultiple programming languages, often cause a semantic disconnect between\noriginal metadata and downstream data. This \"semantic drift\" compromises data\nreproducibility and governance, and impairs the utility of services like\nretrieval-augmented generation (RAG) and text-to-SQL systems. To address this,\na novel framework is proposed for the automated extraction of fine-grained\nschema lineage from multilingual enterprise pipeline scripts. This method\nidentifies four key components: source schemas, source tables, transformation\nlogic, and aggregation operations, creating a standardized representation of\ndata transformations. For the rigorous evaluation of lineage quality, this\npaper introduces the Schema Lineage Composite Evaluation (SLiCE), a metric that\nassesses both structural correctness and semantic fidelity. A new benchmark is\nalso presented, comprising 1,700 manually annotated lineages from real-world\nindustrial scripts. Experiments were conducted with 12 language models, from\n1.3B to 32B small language models (SLMs) to large language models (LLMs) like\nGPT-4o and GPT-4.1. The results demonstrate that the performance of schema\nlineage extraction scales with model size and the sophistication of prompting\ntechniques. Specially, a 32B open-source model, using a single reasoning trace,\ncan achieve performance comparable to the GPT series under standard prompting.\nThis finding suggests a scalable and economical approach for deploying\nschema-aware agents in practical applications.", "AI": {"tldr": "针对企业数据管道中多语言转换导致的语义漂移问题，提出了一种基于语言模型的自动化细粒度模式血缘提取框架。通过新度量SLiCE和基准测试评估，发现模型大小和提示技术能提升性能，32B开源模型可媲美GPT系列，为实际部署提供经济可扩展方案。", "motivation": "企业数据管道中复杂的跨语言转换导致原始元数据与下游数据之间存在“语义漂移”，损害数据可复现性和治理，并影响RAG和text-to-SQL等服务的效用。", "method": "提出了一种新颖的框架，用于从多语言企业管道脚本中自动提取细粒度模式血缘，识别源模式、源表、转换逻辑和聚合操作四个关键组件，并创建标准化数据转换表示。引入了“模式血缘复合评估（SLiCE）”指标来评估血缘质量（结构正确性和语义保真度）。构建了一个包含1700个真实工业脚本手动标注血缘的新基准。使用12种语言模型（从1.3B到32B的小型语言模型，以及GPT-4o和GPT-4.1等大型语言模型）进行了实验。", "result": "模式血缘提取的性能随模型大小和提示技术的复杂性而提升。一个32B的开源模型，在单一推理链下，可以达到与GPT系列（GPT-4o和GPT-4.1）在标准提示下相当的性能。", "conclusion": "研究结果表明，这为在实际应用中部署模式感知代理提供了一种可扩展且经济的方法。"}}
{"id": "2508.06763", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06763", "abs": "https://arxiv.org/abs/2508.06763", "authors": ["Zihao Sheng", "Zilin Huang", "Yen-Jung Chen", "Yansong Qu", "Yuhao Luo", "Yue Leng", "Sikai Chen"], "title": "SafePLUG: Empowering Multimodal LLMs with Pixel-Level Insight and Temporal Grounding for Traffic Accident Understanding", "comment": "The code, dataset, and model checkpoints will be made publicly\n  available at: https://zihaosheng.github.io/SafePLUG", "summary": "Multimodal large language models (MLLMs) have achieved remarkable progress\nacross a range of vision-language tasks and demonstrate strong potential for\ntraffic accident understanding. However, existing MLLMs in this domain\nprimarily focus on coarse-grained image-level or video-level comprehension and\noften struggle to handle fine-grained visual details or localized scene\ncomponents, limiting their applicability in complex accident scenarios. To\naddress these limitations, we propose SafePLUG, a novel framework that empowers\nMLLMs with both Pixel-Level Understanding and temporal Grounding for\ncomprehensive traffic accident analysis. SafePLUG supports both\narbitrary-shaped visual prompts for region-aware question answering and\npixel-level segmentation based on language instructions, while also enabling\nthe recognition of temporally anchored events in traffic accident scenarios. To\nadvance the development of MLLMs for traffic accident understanding, we curate\na new dataset containing multimodal question-answer pairs centered on diverse\naccident scenarios, with detailed pixel-level annotations and temporal event\nboundaries. Experimental results show that SafePLUG achieves strong performance\non multiple tasks, including region-based question answering, pixel-level\nsegmentation, temporal event localization, and accident event understanding.\nThese capabilities lay a foundation for fine-grained understanding of complex\ntraffic scenes, with the potential to improve driving safety and enhance\nsituational awareness in smart transportation systems. The code, dataset, and\nmodel checkpoints will be made publicly available at:\nhttps://zihaosheng.github.io/SafePLUG", "AI": {"tldr": "SafePLUG是一个新颖的框架，旨在增强多模态大语言模型（MLLMs）在交通事故分析中像素级理解和时间定位的能力，以实现细粒度分析。", "motivation": "现有MLLMs在交通事故理解方面主要侧重粗粒度的图像/视频级理解，难以处理细粒度视觉细节或局部场景组件，限制了它们在复杂事故场景中的应用。", "method": "提出了SafePLUG框架，它通过支持任意形状的视觉提示进行区域感知问答、基于语言指令的像素级分割，以及识别交通事故场景中时间锚定的事件，来赋予MLLMs像素级理解和时间定位能力。此外，还构建了一个包含像素级标注和时间事件边界的交通事故多模态问答数据集。", "result": "实验结果表明，SafePLUG在区域问答、像素级分割、时间事件定位和事故事件理解等多项任务上取得了强大的性能。", "conclusion": "SafePLUG的能力为复杂交通场景的细粒度理解奠定了基础，有望提高驾驶安全并增强智能交通系统中的态势感知能力。"}}
{"id": "2508.07353", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07353", "abs": "https://arxiv.org/abs/2508.07353", "authors": ["Rubing Chen", "Jiaxin Wu", "Jian Wang", "Xulu Zhang", "Wenqi Fan", "Chenghua Lin", "Xiao-Yong Wei", "Qing Li"], "title": "Rethinking Domain-Specific LLM Benchmark Construction: A Comprehensiveness-Compactness Approach", "comment": null, "summary": "Numerous benchmarks have been built to evaluate the domain-specific abilities\nof large language models (LLMs), highlighting the need for effective and\nefficient benchmark construction. Existing domain-specific benchmarks primarily\nfocus on the scaling law, relying on massive corpora for supervised fine-tuning\nor generating extensive question sets for broad coverage. However, the impact\nof corpus and question-answer (QA) set design on the precision and recall of\ndomain-specific LLMs remains unexplored. In this paper, we address this gap and\ndemonstrate that the scaling law is not always the optimal principle for\nbenchmark construction in specific domains. Instead, we propose Comp-Comp, an\niterative benchmarking framework based on a comprehensiveness-compactness\nprinciple. Here, comprehensiveness ensures semantic recall of the domain, while\ncompactness enhances precision, guiding both corpus and QA set construction. To\nvalidate our framework, we conducted a case study in a well-renowned\nuniversity, resulting in the creation of XUBench, a large-scale and\ncomprehensive closed-domain benchmark. Although we use the academic domain as\nthe case in this work, our Comp-Comp framework is designed to be extensible\nbeyond academia, providing valuable insights for benchmark construction across\nvarious domains.", "AI": {"tldr": "本文提出了一种名为Comp-Comp的迭代基准测试框架，基于“全面性-紧凑性”原则，以应对现有领域特定LLM基准测试对规模定律的过度依赖，并验证了其在学术领域的有效性，构建了XUBench。", "motivation": "现有领域特定LLM基准测试主要关注规模定律，依赖大量语料库或生成大量问题集，但语料库和问答集设计对领域特定LLM精度和召回率的影响尚未被探索。研究者认为规模定律并非总是特定领域基准构建的最佳原则。", "method": "提出Comp-Comp迭代基准测试框架，该框架基于“全面性-紧凑性”原则，其中全面性确保领域语义召回，紧凑性提升精度，并指导语料库和问答集的构建。通过在一个著名大学进行案例研究，创建了大规模、全面的封闭领域基准XUBench以验证该框架。", "result": "成功创建了XUBench，一个大规模且全面的封闭领域基准。研究表明，在特定领域中，规模定律并非总是基准构建的最佳原则。", "conclusion": "Comp-Comp框架为领域特定基准测试的构建提供了一种有效且可扩展的方法，超越了传统对规模定律的依赖，并可应用于学术领域之外的各种领域。"}}
{"id": "2508.07560", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07560", "abs": "https://arxiv.org/abs/2508.07560", "authors": ["Yan Gong", "Naibang Wang", "Jianli Lu", "Xinyu Zhang", "Yongsheng Gao", "Jie Zhao", "Zifan Huang", "Haozhi Bai", "Nanxin Zeng", "Nayu Su", "Lei Yang", "Ziying Song", "Xiaoxi Hu", "Xinmin Jiang", "Xiaojuan Zhang", "Susanto Rahardja"], "title": "Progressive Bird's Eye View Perception for Safety-Critical Autonomous Driving: A Comprehensive Survey", "comment": null, "summary": "Bird's-Eye-View (BEV) perception has become a foundational paradigm in\nautonomous driving, enabling unified spatial representations that support\nrobust multi-sensor fusion and multi-agent collaboration. As autonomous\nvehicles transition from controlled environments to real-world deployment,\nensuring the safety and reliability of BEV perception in complex scenarios -\nsuch as occlusions, adverse weather, and dynamic traffic - remains a critical\nchallenge. This survey provides the first comprehensive review of BEV\nperception from a safety-critical perspective, systematically analyzing\nstate-of-the-art frameworks and implementation strategies across three\nprogressive stages: single-modality vehicle-side, multimodal vehicle-side, and\nmulti-agent collaborative perception. Furthermore, we examine public datasets\nencompassing vehicle-side, roadside, and collaborative settings, evaluating\ntheir relevance to safety and robustness. We also identify key open-world\nchallenges - including open-set recognition, large-scale unlabeled data, sensor\ndegradation, and inter-agent communication latency - and outline future\nresearch directions, such as integration with end-to-end autonomous driving\nsystems, embodied intelligence, and large language models.", "AI": {"tldr": "该综述首次从安全关键角度全面回顾了自动驾驶中的鸟瞰图（BEV）感知技术，分析了现有框架、数据集和开放世界挑战，并指出了未来研究方向。", "motivation": "鸟瞰图（BEV）感知是自动驾驶的基础范式，但其在复杂真实世界场景（如遮挡、恶劣天气、动态交通）中的安全性和可靠性仍是关键挑战，需要系统性地分析和解决。", "method": "本文系统地审查了单模态车载、多模态车载和多智能体协作感知三个阶段的BEV感知框架和实施策略。同时，评估了公共数据集与安全和鲁棒性的相关性，并识别了开放世界挑战。", "result": "本文提供了BEV感知从安全关键角度的首次全面综述，分析了最先进的框架、实施策略和公共数据集，并识别了开放集识别、大规模未标注数据、传感器退化和智能体间通信延迟等关键开放世界挑战。", "conclusion": "未来研究应关注BEV感知与端到端自动驾驶系统、具身智能和大型语言模型的集成，以解决当前挑战并提升BEV感知的安全性和可靠性。"}}
{"id": "2508.07185", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; H.3.3; H.2.8"], "pdf": "https://arxiv.org/pdf/2508.07185", "abs": "https://arxiv.org/abs/2508.07185", "authors": ["Kabir Khan", "Priya Sharma", "Arjun Mehta", "Neha Gupta", "Ravi Narayanan"], "title": "DySK-Attn: A Framework for Efficient, Real-Time Knowledge Updating in Large Language Models via Dynamic Sparse Knowledge Attention", "comment": "Preprint; 7 figures, 3 tables, 1 algorithm; v1. Code and data will be\n  released", "summary": "Large Language Models (LLMs) suffer from a critical limitation: their\nknowledge is static and quickly becomes outdated. Retraining these massive\nmodels is computationally prohibitive, while existing knowledge editing\ntechniques can be slow and may introduce unforeseen side effects. To address\nthis, we propose DySK-Attn, a novel framework that enables LLMs to efficiently\nintegrate real-time knowledge from a dynamic external source. Our approach\nsynergizes an LLM with a dynamic Knowledge Graph (KG) that can be updated\ninstantaneously. The core of our framework is a sparse knowledge attention\nmechanism, which allows the LLM to perform a coarse-to-fine grained search,\nefficiently identifying and focusing on a small, highly relevant subset of\nfacts from the vast KG. This mechanism avoids the high computational cost of\ndense attention over the entire knowledge base and mitigates noise from\nirrelevant information. We demonstrate through extensive experiments on\ntime-sensitive question-answering tasks that DySK-Attn significantly\noutperforms strong baselines, including standard Retrieval-Augmented Generation\n(RAG) and model editing techniques, in both factual accuracy for updated\nknowledge and computational efficiency. Our framework offers a scalable and\neffective solution for building LLMs that can stay current with the\never-changing world.", "AI": {"tldr": "DySK-Attn是一个新颖的框架，通过结合动态知识图谱和稀疏知识注意力机制，使大型语言模型（LLMs）能够高效地集成实时知识，解决LLMs知识静态和过时的问题。", "motivation": "大型语言模型（LLMs）的知识是静态且容易过时的，而重新训练成本高昂。现有知识编辑技术速度慢且可能引入不可预见的副作用，因此需要一种高效、可扩展的方式让LLMs保持知识更新。", "method": "提出DySK-Attn框架，将LLM与可即时更新的动态知识图谱（KG）相结合。核心是稀疏知识注意力机制，允许LLM进行粗粒度到细粒度的搜索，高效地从KG中识别并聚焦于少量高度相关的知识，避免了对整个知识库进行密集注意力的计算成本和无关信息的干扰。", "result": "在时间敏感的问答任务中，DySK-Attn在更新知识的事实准确性和计算效率方面，显著优于包括标准检索增强生成（RAG）和模型编辑技术在内的强基线。", "conclusion": "DySK-Attn为构建能够与时俱进的LLMs提供了一个可扩展且有效的解决方案。"}}
{"id": "2508.06768", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2508.06768", "abs": "https://arxiv.org/abs/2508.06768", "authors": ["Noe Bertramo", "Gabriel Duguey", "Vivek Gopalakrishnan"], "title": "DiffUS: Differentiable Ultrasound Rendering from Volumetric Imaging", "comment": "10 pages, accepted to MICCAI ASMUS 25", "summary": "Intraoperative ultrasound imaging provides real-time guidance during numerous\nsurgical procedures, but its interpretation is complicated by noise, artifacts,\nand poor alignment with high-resolution preoperative MRI/CT scans. To bridge\nthe gap between reoperative planning and intraoperative guidance, we present\nDiffUS, a physics-based, differentiable ultrasound renderer that synthesizes\nrealistic B-mode images from volumetric imaging. DiffUS first converts MRI 3D\nscans into acoustic impedance volumes using a machine learning approach. Next,\nwe simulate ultrasound beam propagation using ray tracing with coupled\nreflection-transmission equations. DiffUS formulates wave propagation as a\nsparse linear system that captures multiple internal reflections. Finally, we\nreconstruct B-mode images via depth-resolved echo extraction across fan-shaped\nacquisition geometry, incorporating realistic artifacts including speckle noise\nand depth-dependent degradation. DiffUS is entirely implemented as\ndifferentiable tensor operations in PyTorch, enabling gradient-based\noptimization for downstream applications such as slice-to-volume registration\nand volumetric reconstruction. Evaluation on the ReMIND dataset demonstrates\nDiffUS's ability to generate anatomically accurate ultrasound images from brain\nMRI data.", "AI": {"tldr": "本文提出了DiffUS，一个基于物理的可微分超声渲染器，能够从MRI三维扫描合成逼真的B型超声图像，旨在弥合术前规划与术中指导之间的鸿沟。", "motivation": "术中超声图像受噪声、伪影和与高分辨率术前MRI/CT扫描对齐不良等问题困扰，导致解释复杂。研究旨在解决术前规划与术中超声指导之间的信息鸿沟。", "method": "DiffUS首先使用机器学习方法将MRI三维扫描转换为声阻抗体。接着，通过射线追踪和耦合反射-透射方程模拟超声波传播，将波传播公式化为一个捕获多次内部反射的稀疏线性系统。最后，通过深度解析回波提取，在扇形采集几何中重建B型图像，并融入散斑噪声和深度依赖性退化等真实伪影。整个系统以PyTorch中的可微分张量操作实现，支持基于梯度的优化。", "result": "在ReMIND数据集上的评估表明，DiffUS能够从脑部MRI数据生成解剖学上准确的超声图像。其可微分特性使其能够支持切片到体积配准和体积重建等下游应用中的梯度优化。", "conclusion": "DiffUS成功地通过合成逼真的、可微分的超声图像，弥合了术前规划与术中指导之间的差距，为各种下游应用提供了强大的工具，特别是那些需要梯度优化的任务。"}}
{"id": "2508.07382", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07382", "abs": "https://arxiv.org/abs/2508.07382", "authors": ["He Kong", "Die Hu", "Jingguo Ge", "Liangxiong Li", "Hui Li", "Tong Li"], "title": "Pentest-R1: Towards Autonomous Penetration Testing Reasoning Optimized via Two-Stage Reinforcement Learning", "comment": null, "summary": "Automating penetration testing is crucial for enhancing cybersecurity, yet\ncurrent Large Language Models (LLMs) face significant limitations in this\ndomain, including poor error handling, inefficient reasoning, and an inability\nto perform complex end-to-end tasks autonomously. To address these challenges,\nwe introduce Pentest-R1, a novel framework designed to optimize LLM reasoning\ncapabilities for this task through a two-stage reinforcement learning pipeline.\nWe first construct a dataset of over 500 real-world, multi-step walkthroughs,\nwhich Pentest-R1 leverages for offline reinforcement learning (RL) to instill\nfoundational attack logic. Subsequently, the LLM is fine-tuned via online RL in\nan interactive Capture The Flag (CTF) environment, where it learns directly\nfrom environmental feedback to develop robust error self-correction and\nadaptive strategies. Our extensive experiments on the Cybench and AutoPenBench\nbenchmarks demonstrate the framework's effectiveness. On AutoPenBench,\nPentest-R1 achieves a 24.2\\% success rate, surpassing most state-of-the-art\nmodels and ranking second only to Gemini 2.5 Flash. On Cybench, it attains a\n15.0\\% success rate in unguided tasks, establishing a new state-of-the-art for\nopen-source LLMs and matching the performance of top proprietary models.\nAblation studies confirm that the synergy of both training stages is critical\nto its success.", "AI": {"tldr": "Pentest-R1是一个新颖的框架，通过两阶段强化学习（离线RL和在线RL）优化LLM在自动化渗透测试中的推理能力，在Cybench和AutoPenBench基准测试中取得了显著成果。", "motivation": "自动化渗透测试对提升网络安全至关重要，但现有大型语言模型（LLMs）在该领域存在局限性，如错误处理能力差、推理效率低以及无法自主执行复杂的端到端任务。", "method": "引入Pentest-R1框架，采用两阶段强化学习流程：首先，构建包含500多个真实世界多步骤演练的数据集，用于离线强化学习以灌输基础攻击逻辑；其次，在交互式CTF环境中通过在线强化学习对LLM进行微调，使其直接从环境反馈中学习，发展鲁棒的错误自纠正和自适应策略。", "result": "在AutoPenBench上，Pentest-R1成功率达到24.2%，超越大多数SOTA模型，仅次于Gemini 2.5 Flash。在Cybench的无引导任务中，成功率达到15.0%，为开源LLM树立了新标杆，并与顶级专有模型表现相当。消融研究证实两阶段训练的协同作用对其成功至关重要。", "conclusion": "Pentest-R1通过结合离线和在线强化学习，有效提升了LLM在自动化渗透测试中的推理、错误处理和任务执行能力，在多个基准测试中取得了领先的性能，证明了两阶段训练方法的有效性。"}}
{"id": "2508.07566", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07566", "abs": "https://arxiv.org/abs/2508.07566", "authors": ["Conor K. Trygstad", "Cody R. Longwell", "Francisco M. F. R. Gonçalves", "Elijah K. Blankenship", "Néstor O. Pérez-Arancibia"], "title": "Feedback Control of a Single-Tail Bioinspired 59-mg Swimmer", "comment": "To be presented at the 2025 IEEE/RSJ International Conference on\n  Intelligent Robots and Systems (IROS 2025)", "summary": "We present an evolved steerable version of the single-tail\nFish-&-Ribbon-Inspired Small Swimming Harmonic roBot (FRISSHBot), a 59-mg\nbiologically inspired swimmer, which is driven by a new shape-memory alloy\n(SMA)-based bimorph actuator. The new FRISSHBot is controllable in the\ntwo-dimensional (2D) space, which enabled the first demonstration of\nfeedback-controlled trajectory tracking of a single-tail aquatic robot with\nonboard actuation at the subgram scale. These new capabilities are the result\nof a physics-informed design with an enlarged head and shortened tail relative\nto those of the original platform. Enhanced by its design, this new platform\nachieves forward swimming speeds of up to 13.6 mm/s (0.38 Bl/s), which is over\nfour times that of the original platform. Furthermore, when following 2D\nreferences in closed loop, the tested FRISSHBot prototype attains forward\nswimming speeds of up to 9.1 mm/s, root-mean-square (RMS) tracking errors as\nlow as 2.6 mm, turning rates of up to 13.1 {\\deg}/s, and turning radii as small\nas 10 mm.", "AI": {"tldr": "本文介绍了一种改进型、可转向的亚克级单尾仿生机器人FRISSHBot，它采用新型SMA双晶片致动器，实现了二维空间内的反馈控制轨迹跟踪，并显著提高了速度和控制精度。", "motivation": "开发一种具有二维可控性、能进行轨迹跟踪且体积小、重量轻（亚克级）的水下机器人，以克服现有单尾水下机器人缺乏精确转向和反馈控制能力的限制。", "method": "研究人员设计了一种进化的FRISSHBot，其驱动基于新型形状记忆合金（SMA）双晶片致动器。通过物理信息设计，增大了机器人头部并缩短了尾部。在此基础上，首次实现了亚克级带板载致动器的单尾水下机器人的反馈控制轨迹跟踪。", "result": "新型FRISSHBot重59毫克，实现了二维可控性。其前进速度高达13.6毫米/秒（0.38倍体长/秒），是原始平台的四倍多。在闭环2D轨迹跟踪时，前进速度可达9.1毫米/秒，均方根跟踪误差低至2.6毫米，转向速率高达13.1度/秒，最小转弯半径为10毫米。", "conclusion": "通过物理信息设计和新型SMA致动器，成功开发出一种可转向的亚克级单尾水下机器人FRISSHBot，显著提升了其游泳速度和二维空间内的反馈控制轨迹跟踪能力，为微型水下机器人提供了新的可能性。"}}
{"id": "2508.07195", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07195", "abs": "https://arxiv.org/abs/2508.07195", "authors": ["Yanru Sun", "Emadeldeen Eldele", "Zongxia Xie", "Yucheng Wang", "Wenzhe Niu", "Qinghua Hu", "Chee Keong Kwoh", "Min Wu"], "title": "Adapting LLMs to Time Series Forecasting via Temporal Heterogeneity Modeling and Semantic Alignment", "comment": null, "summary": "Large Language Models (LLMs) have recently demonstrated impressive\ncapabilities in natural language processing due to their strong generalization\nand sequence modeling capabilities. However, their direct application to time\nseries forecasting remains challenging due to two fundamental issues: the\ninherent heterogeneity of temporal patterns and the modality gap between\ncontinuous numerical signals and discrete language representations. In this\nwork, we propose TALON, a unified framework that enhances LLM-based forecasting\nby modeling temporal heterogeneity and enforcing semantic alignment.\nSpecifically, we design a Heterogeneous Temporal Encoder that partitions\nmultivariate time series into structurally coherent segments, enabling\nlocalized expert modeling across diverse temporal patterns. To bridge the\nmodality gap, we introduce a Semantic Alignment Module that aligns temporal\nfeatures with LLM-compatible representations, enabling effective integration of\ntime series into language-based models while eliminating the need for\nhandcrafted prompts during inference. Extensive experiments on seven real-world\nbenchmarks demonstrate that TALON achieves superior performance across all\ndatasets, with average MSE improvements of up to 11\\% over recent\nstate-of-the-art methods. These results underscore the effectiveness of\nincorporating both pattern-aware and semantic-aware designs when adapting LLMs\nfor time series forecasting. The code is available at:\nhttps://github.com/syrGitHub/TALON.", "AI": {"tldr": "TALON是一个统一框架，通过建模时间序列异质性和强制语义对齐，增强了基于LLM的时间序列预测能力，解决了现有LLM在时间序列应用中的两大挑战。", "motivation": "大型语言模型（LLMs）在自然语言处理中表现出色，但直接应用于时间序列预测面临两大挑战：时间模式固有的异质性，以及连续数值信号与离散语言表示之间的模态鸿沟。", "method": "本文提出了TALON框架。它包含：1) 异质时间编码器（Heterogeneous Temporal Encoder），将多元时间序列划分为结构一致的片段，实现对不同时间模式的局部专家建模；2) 语义对齐模块（Semantic Alignment Module），将时间特征与LLM兼容的表示对齐，有效整合时间序列到基于语言的模型中，并消除推理时手动设计提示的需求。", "result": "在七个真实世界基准测试中，TALON在所有数据集上均取得了优异性能，相比最新的SOTA方法，平均MSE（均方误差）提升高达11%。", "conclusion": "研究结果强调了在将LLMs应用于时间序列预测时，结合模式感知和语义感知设计是有效的。"}}
{"id": "2508.06805", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06805", "abs": "https://arxiv.org/abs/2508.06805", "authors": ["Aarav Mehta", "Priya Deshmukh", "Vikram Singh", "Siddharth Malhotra", "Krishnan Menon Iyer", "Tanvi Iyer"], "title": "Edge Detection for Organ Boundaries via Top Down Refinement and SubPixel Upsampling", "comment": "MICCAIA Workshop", "summary": "Accurate localization of organ boundaries is critical in medical imaging for\nsegmentation, registration, surgical planning, and radiotherapy. While deep\nconvolutional networks (ConvNets) have advanced general-purpose edge detection\nto near-human performance on natural images, their outputs often lack precise\nlocalization, a limitation that is particularly harmful in medical applications\nwhere millimeter-level accuracy is required. Building on a systematic analysis\nof ConvNet edge outputs, we propose a medically focused crisp edge detector\nthat adapts a novel top-down backward refinement architecture to medical images\n(2D and volumetric). Our method progressively upsamples and fuses high-level\nsemantic features with fine-grained low-level cues through a backward\nrefinement pathway, producing high-resolution, well-localized organ boundaries.\nWe further extend the design to handle anisotropic volumes by combining 2D\nslice-wise refinement with light 3D context aggregation to retain computational\nefficiency. Evaluations on several CT and MRI organ datasets demonstrate\nsubstantially improved boundary localization under strict criteria (boundary\nF-measure, Hausdorff distance) compared to baseline ConvNet detectors and\ncontemporary medical edge/contour methods. Importantly, integrating our crisp\nedge maps into downstream pipelines yields consistent gains in organ\nsegmentation (higher Dice scores, lower boundary errors), more accurate image\nregistration, and improved delineation of lesions near organ interfaces. The\nproposed approach produces clinically valuable, crisp organ edges that\nmaterially enhance common medical-imaging tasks.", "AI": {"tldr": "该论文提出了一种针对医学图像的清晰边缘检测器，通过新颖的自顶向下反向细化架构，显著提高了器官边界的定位精度，并在多项下游医学影像任务中表现出优异性能。", "motivation": "深度卷积网络在自然图像边缘检测方面表现出色，但其输出常缺乏精确的定位，这在需要毫米级精度的医学应用（如分割、配准、手术规划和放疗）中是一个严重限制。", "method": "该方法基于对ConvNet边缘输出的系统分析，提出了一种医学专用清晰边缘检测器。它采用新颖的自顶向下反向细化架构，通过反向细化路径逐步上采样并融合高层语义特征与细粒度低层线索，生成高分辨率、定位精确的器官边界。为处理各向异性体积数据，该设计进一步结合了2D切片级细化与轻量级3D上下文聚合，以保持计算效率。", "result": "在多个CT和MRI器官数据集上的评估显示，与基线ConvNet检测器和当代医学边缘/轮廓方法相比，该方法在严格标准（边界F-measure，Hausdorff距离）下显著提高了边界定位精度。更重要的是，将该清晰边缘图整合到下游流程中，能持续提高器官分割（更高的Dice分数，更低的边界误差）、更精确的图像配准以及器官界面附近病变的更好描绘。", "conclusion": "所提出的方法能够生成具有临床价值的清晰器官边缘，显著增强了常见的医学影像任务。"}}
{"id": "2508.07388", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07388", "abs": "https://arxiv.org/abs/2508.07388", "authors": ["Zhaoyu Chen", "Hongnan Lin", "Yongwei Nie", "Fei Ma", "Xuemiao Xu", "Fei Yu", "Chengjiang Long"], "title": "Invert4TVG: A Temporal Video Grounding Framework with Inversion Tasks for Enhanced Action Understanding", "comment": null, "summary": "Temporal Video Grounding (TVG) seeks to localize video segments matching a\ngiven textual query. Current methods, while optimizing for high temporal\nIntersection-over-Union (IoU), often overfit to this metric, compromising\nsemantic action understanding in the video and query, a critical factor for\nrobust TVG. To address this, we introduce Inversion Tasks for TVG (Invert4TVG),\na novel framework that enhances both localization accuracy and action\nunderstanding without additional data. Our approach leverages three inversion\ntasks derived from existing TVG annotations: (1) Verb Completion, predicting\nmasked action verbs in queries from video segments; (2) Action Recognition,\nidentifying query-described actions; and (3) Video Description, generating\ndescriptions of video segments that explicitly embed query-relevant actions.\nThese tasks, integrated with TVG via a reinforcement learning framework with\nwell-designed reward functions, ensure balanced optimization of localization\nand semantics. Experiments show our method outperforms state-of-the-art\napproaches, achieving a 7.1\\% improvement in R1@0.7 on Charades-STA for a 3B\nmodel compared to Time-R1. By inverting TVG to derive query-related actions\nfrom segments, our approach strengthens semantic understanding, significantly\nraising the ceiling of localization accuracy.", "AI": {"tldr": "本文提出Invert4TVG框架，通过引入三个逆向任务（动词补全、动作识别、视频描述）并结合强化学习，在不增加额外数据的情况下，提升时序视频定位（TVG）的语义理解能力和定位精度，解决了现有方法过度拟合IoU导致语义理解不足的问题。", "motivation": "当前时序视频定位（TVG）方法在优化时间IoU指标时，常常过度拟合该指标，从而损害了视频和查询中的语义动作理解能力。而语义理解是实现鲁棒TVG的关键因素。", "method": "引入Invert4TVG框架，该框架利用现有TVG标注数据，设计了三个逆向任务：1) 动词补全：根据视频片段预测查询中被遮蔽的动作动词；2) 动作识别：识别查询描述的动作；3) 视频描述：生成明确嵌入查询相关动作的视频片段描述。这些任务通过一个设计了良好奖励函数的强化学习框架与TVG集成，以平衡优化定位精度和语义理解。", "result": "实验结果表明，该方法优于现有最先进的方法，在Charades-STA数据集上，对于3B模型，R1@0.7指标相比Time-R1提升了7.1%。", "conclusion": "通过将TVG逆向化以从视频片段中推断出与查询相关的动作，本方法显著增强了语义理解能力，从而大幅提升了定位精度上限。"}}
{"id": "2508.07606", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07606", "abs": "https://arxiv.org/abs/2508.07606", "authors": ["Hongtao Li", "Ziyuan Jiao", "Xiaofeng Liu", "Hangxin Liu", "Zilong Zheng"], "title": "In-situ Value-aligned Human-Robot Interactions with Physical Constraints", "comment": "8 pages, 7 figures", "summary": "Equipped with Large Language Models (LLMs), human-centered robots are now\ncapable of performing a wide range of tasks that were previously deemed\nchallenging or unattainable. However, merely completing tasks is insufficient\nfor cognitive robots, who should learn and apply human preferences to future\nscenarios. In this work, we propose a framework that combines human preferences\nwith physical constraints, requiring robots to complete tasks while considering\nboth. Firstly, we developed a benchmark of everyday household activities, which\nare often evaluated based on specific preferences. We then introduced\nIn-Context Learning from Human Feedback (ICLHF), where human feedback comes\nfrom direct instructions and adjustments made intentionally or unintentionally\nin daily life. Extensive sets of experiments, testing the ICLHF to generate\ntask plans and balance physical constraints with preferences, have demonstrated\nthe efficiency of our approach.", "AI": {"tldr": "提出一个结合人类偏好和物理约束的机器人框架ICLHF，通过情境学习人类反馈，使机器人能高效完成家务任务。", "motivation": "现有的基于LLM的机器人虽能完成任务，但无法学习并应用人类偏好，这对于认知机器人来说是不足的。", "method": "1. 开发了一个基于日常家务活动且考虑特定偏好的基准。2. 引入了“情境学习人类反馈（ICLHF）”方法，该方法利用来自直接指令以及日常生活中有意或无意调整的人类反馈。", "result": "大量实验证明，所提出的ICLHF方法在生成任务计划以及平衡物理约束与人类偏好方面是高效的。", "conclusion": "结合人类偏好和物理约束的ICLHF框架，使机器人能够高效地完成任务，超越了简单的任务执行，迈向更具认知能力的机器人。"}}
{"id": "2508.07209", "categories": ["cs.CL", "cs.SI"], "pdf": "https://arxiv.org/pdf/2508.07209", "abs": "https://arxiv.org/abs/2508.07209", "authors": ["Chaoqun Cui", "Siyuan Li", "Kunkun Ma", "Caiyan Jia"], "title": "Enhancing Rumor Detection Methods with Propagation Structure Infused Language Model", "comment": "This paper is accepted by COLING2025", "summary": "Pretrained Language Models (PLMs) have excelled in various Natural Language\nProcessing tasks, benefiting from large-scale pretraining and self-attention\nmechanism's ability to capture long-range dependencies. However, their\nperformance on social media application tasks like rumor detection remains\nsuboptimal. We attribute this to mismatches between pretraining corpora and\nsocial texts, inadequate handling of unique social symbols, and pretraining\ntasks ill-suited for modeling user engagements implicit in propagation\nstructures. To address these issues, we propose a continue pretraining strategy\ncalled Post Engagement Prediction (PEP) to infuse information from propagation\nstructures into PLMs. PEP makes models to predict root, branch, and parent\nrelations between posts, capturing interactions of stance and sentiment crucial\nfor rumor detection. We also curate and release large-scale Twitter corpus:\nTwitterCorpus (269GB text), and two unlabeled claim conversation datasets with\npropagation structures (UTwitter and UWeibo). Utilizing these resources and PEP\nstrategy, we train a Twitter-tailored PLM called SoLM. Extensive experiments\ndemonstrate PEP significantly boosts rumor detection performance across\nuniversal and social media PLMs, even in few-shot scenarios. On benchmark\ndatasets, PEP enhances baseline models by 1.0-3.7\\% accuracy, even enabling it\nto outperform current state-of-the-art methods on multiple datasets. SoLM\nalone, without high-level modules, also achieves competitive results,\nhighlighting the strategy's effectiveness in learning discriminative post\ninteraction features.", "AI": {"tldr": "本文提出了一种名为PEP的持续预训练策略，通过预测帖子间的传播关系，将社交媒体传播结构信息融入预训练语言模型（PLMs），并训练了专门的SoLM模型，显著提升了谣言检测性能。", "motivation": "现有预训练语言模型（PLMs）在社交媒体谣言检测任务上表现不佳，原因在于预训练语料与社交文本不匹配、对社交符号处理不足，以及预训练任务未能有效建模传播结构中隐含的用户互动。", "method": "提出了一种名为“帖子互动预测”（PEP）的持续预训练策略，使模型预测帖子间的根、分支和父子关系，以捕获立场和情感互动。同时，收集并发布了大规模Twitter语料库（TwitterCorpus）和两个无标签的带有传播结构的对话数据集（UTwitter和UWeibo）。利用这些资源和PEP策略，训练了一个针对Twitter优化的PLM模型——SoLM。", "result": "实验证明，PEP策略显著提升了通用和社交媒体PLMs在谣言检测任务上的表现，即使在少样本场景下亦是如此。在基准数据集上，PEP使基线模型准确率提高了1.0-3.7%，甚至在多个数据集上超越了当前最先进的方法。SoLM模型即使不依赖高级模块，也取得了有竞争力的结果，突出了该策略在学习判别性帖子互动特征方面的有效性。", "conclusion": "PEP策略能有效将传播结构信息融入PLMs，显著提升其在社交媒体谣言检测任务上的性能。通过预测帖子间的关系，模型能更好地理解用户互动和信息传播的动态，从而学习到更具区分度的特征。"}}
{"id": "2508.06816", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06816", "abs": "https://arxiv.org/abs/2508.06816", "authors": ["Vikram Singh", "Kabir Malhotra", "Rohan Desai", "Ananya Shankaracharya", "Priyadarshini Chatterjee", "Krishnan Menon Iyer"], "title": "DualResolution Residual Architecture with Artifact Suppression for Melanocytic Lesion Segmentation", "comment": "MICCAIA", "summary": "Accurate segmentation of melanocytic tumors in dermoscopic images is a\ncritical step for automated skin cancer screening and clinical decision\nsupport. Unlike natural scene segmentation, lesion delineation must reconcile\nsubtle texture and color variations, frequent artifacts (hairs, rulers,\nbubbles), and a strong need for precise boundary localization to support\ndownstream diagnosis. In this paper we introduce Our method, a novel ResNet\ninspired dual resolution architecture specifically designed for melanocytic\ntumor segmentation. Our method maintains a full resolution stream that\npreserves fine grained boundary information while a complementary pooled stream\naggregates multi scale contextual cues for robust lesion recognition. The\nstreams are tightly coupled by boundary aware residual connections that inject\nhigh frequency edge information into deep feature maps, and by a channel\nattention module that adapts color and texture sensitivity to dermoscopic\nappearance. To further address common imaging artifacts and the limited size of\nclinical datasets, we propose a lightweight artifact suppression block and a\nmulti task training objective that combines a Dice Tversky segmentation loss\nwith an explicit boundary loss and a contrastive regularizer for feature\nstability. The combined design yields pixel accurate masks without requiring\nheavy post processing or complex pre training protocols. Extensive experiments\non public dermoscopic benchmarks demonstrate that Our method significantly\nimproves boundary adherence and clinically relevant segmentation metrics\ncompared to standard encoder decoder baselines, making it a practical building\nblock for automated melanoma assessment systems.", "AI": {"tldr": "本文提出了一种名为“Our method”的ResNet启发式双分辨率架构，用于皮肤镜图像中黑色素瘤的精确分割。该方法结合了精细边界信息和多尺度上下文，并通过边界感知残差连接、通道注意力模块、轻量级伪影抑制块以及多任务损失函数（包括Dice-Tversky分割损失、显式边界损失和对比正则化器）来提高分割精度和鲁棒性。", "motivation": "在皮肤镜图像中精确分割黑色素瘤是自动化皮肤癌筛查和临床决策支持的关键步骤。然而，这面临挑战，包括细微的纹理和颜色变化、常见的伪影（毛发、尺子、气泡）以及对精确边界定位的强烈需求，以支持后续诊断。", "method": "该方法引入了一种ResNet启发的双分辨率架构。它包含一个全分辨率流以保留精细边界信息，以及一个池化流以聚合多尺度上下文线索。两个流通过边界感知残差连接和通道注意力模块紧密耦合。此外，该方法还提出了一个轻量级伪影抑制块，并采用多任务训练目标，结合了Dice-Tversky分割损失、显式边界损失和用于特征稳定性的对比正则化器。", "result": "实验结果表明，该方法在公共皮肤镜基准测试上显著提高了边界粘附性和临床相关分割指标，优于标准的编码器-解码器基线。它能够在不需要大量后处理或复杂预训练协议的情况下生成像素级准确的掩模。", "conclusion": "所提出的方法通过其新颖的双分辨率架构、边界感知连接、伪影抑制和多任务训练，有效解决了皮肤镜图像中黑色素瘤分割的挑战，使其成为自动化黑色素瘤评估系统的一个实用构建块。"}}
{"id": "2508.07405", "categories": ["cs.AI", "cs.CL", "cs.LG", "I.2.7; I.5.4"], "pdf": "https://arxiv.org/pdf/2508.07405", "abs": "https://arxiv.org/abs/2508.07405", "authors": ["Jesse Ponnock"], "title": "Generative AI for Strategic Plan Development", "comment": "11 pages, 9 figures", "summary": "Given recent breakthroughs in Generative Artificial Intelligence (GAI) and\nLarge Language Models (LLMs), more and more professional services are being\naugmented through Artificial Intelligence (AI), which once seemed impossible to\nautomate. This paper presents a modular model for leveraging GAI in developing\nstrategic plans for large scale government organizations and evaluates leading\nmachine learning techniques in their application towards one of the identified\nmodules. Specifically, the performance of BERTopic and Non-negative Matrix\nFactorization (NMF) are evaluated in their ability to use topic modeling to\ngenerate themes representative of Vision Elements within a strategic plan. To\naccomplish this, BERTopic and NMF models are trained using a large volume of\nreports from the Government Accountability Office (GAO). The generated topics\nfrom each model are then scored for similarity against the Vision Elements of a\npublished strategic plan and the results are compared. Our results show that\nthese techniques are capable of generating themes similar to 100% of the\nelements being evaluated against. Further, we conclude that BERTopic performs\nbest in this application with more than half of its correlated topics achieving\na \"medium\" or \"strong\" correlation. A capability of GAI-enabled strategic plan\ndevelopment impacts a multi-billion dollar industry and assists the federal\ngovernment in overcoming regulatory requirements which are crucial to the\npublic good. Further work will focus on the operationalization of the concept\nproven in this study as well as viability of the remaining modules in the\nproposed model for GAI-generated strategic plans.", "AI": {"tldr": "本研究提出了一个利用生成式AI（GAI）和大型语言模型（LLMs）为大型政府机构制定战略计划的模块化模型，并评估了BERTopic和NMF两种主题建模技术在生成与战略计划愿景元素相似的主题方面的表现，结果显示这些技术有效，其中BERTopic表现最佳。", "motivation": "随着生成式AI和LLMs的突破，越来越多的专业服务（包括曾被认为不可能自动化的领域）开始通过AI得到增强。本研究旨在探索如何利用GAI辅助政府机构进行战略规划，以应对监管要求并服务公共利益，因为这是一个涉及数十亿美元的行业。", "method": "论文提出了一个模块化的GAI战略规划模型。具体评估了BERTopic和非负矩阵分解（NMF）两种机器学习技术在主题建模方面的应用。模型使用大量政府问责局（GAO）的报告进行训练，以生成代表战略计划“愿景元素”的主题。然后，将模型生成的主题与已发布的战略计划的愿景元素进行相似性评分和比较。", "result": "研究结果表明，这些主题建模技术能够生成与100%被评估的愿景元素相似的主题。其中，BERTopic表现最佳，其超过一半的相关主题达到了“中等”或“强”相关性。", "conclusion": "利用GAI开发战略计划是可行的，并且对数十亿美元的行业以及联邦政府克服关键监管要求具有重要影响。BERTopic在此应用中表现优于NMF。未来的工作将侧重于本研究中验证概念的实际操作化以及所提议模型中其余模块的有效性。"}}
{"id": "2508.07611", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07611", "abs": "https://arxiv.org/abs/2508.07611", "authors": ["Zifan Wang", "Xun Yang", "Jianzhuang Zhao", "Jiaming Zhou", "Teli Ma", "Ziyao Gao", "Arash Ajoudani", "Junwei Liang"], "title": "End-to-End Humanoid Robot Safe and Comfortable Locomotion Policy", "comment": null, "summary": "The deployment of humanoid robots in unstructured, human-centric environments\nrequires navigation capabilities that extend beyond simple locomotion to\ninclude robust perception, provable safety, and socially aware behavior.\nCurrent reinforcement learning approaches are often limited by blind\ncontrollers that lack environmental awareness or by vision-based systems that\nfail to perceive complex 3D obstacles. In this work, we present an end-to-end\nlocomotion policy that directly maps raw, spatio-temporal LiDAR point clouds to\nmotor commands, enabling robust navigation in cluttered dynamic scenes. We\nformulate the control problem as a Constrained Markov Decision Process (CMDP)\nto formally separate safety from task objectives. Our key contribution is a\nnovel methodology that translates the principles of Control Barrier Functions\n(CBFs) into costs within the CMDP, allowing a model-free Penalized Proximal\nPolicy Optimization (P3O) to enforce safety constraints during training.\nFurthermore, we introduce a set of comfort-oriented rewards, grounded in\nhuman-robot interaction research, to promote motions that are smooth,\npredictable, and less intrusive. We demonstrate the efficacy of our framework\nthrough a successful sim-to-real transfer to a physical humanoid robot, which\nexhibits agile and safe navigation around both static and dynamic 3D obstacles.", "AI": {"tldr": "本文提出了一种端到端的人形机器人运动策略，直接将激光雷达点云映射到电机指令，通过将控制障碍函数（CBF）原则转化为受限马尔可夫决策过程（CMDP）中的成本，结合惩罚近端策略优化（P3O）和舒适性奖励，实现了在复杂动态场景中的鲁棒、安全且具有社交意识的导航，并成功进行了从仿真到真实机器人的迁移。", "motivation": "在非结构化、以人为中心的环境中部署人形机器人，需要其具备超越简单运动的导航能力，包括鲁棒的感知、可证明的安全性和社会意识行为。当前的强化学习方法受限于缺乏环境意识的“盲”控制器，或无法感知复杂3D障碍物的基于视觉的系统。", "method": "该研究提出了一种端到端运动策略，直接将原始时空激光雷达点云映射到电机命令。将控制问题表述为受限马尔可夫决策过程（CMDP），以形式化地分离安全目标和任务目标。核心贡献是将控制障碍函数（CBF）的原则转化为CMDP中的成本，使得无模型的惩罚近端策略优化（P3O）能够在训练期间强制执行安全约束。此外，引入了一组基于人机交互研究的舒适性奖励，以促进平滑、可预测且侵入性较小的运动。", "result": "该框架成功地从仿真迁移到真实的人形机器人上。物理机器人展示了在静态和动态3D障碍物周围敏捷且安全的导航能力。", "conclusion": "所提出的框架能够使人形机器人在复杂环境中实现鲁棒、安全且具有社交意识的导航，有效解决了现有强化学习方法在感知和安全方面的局限性。"}}
{"id": "2508.07229", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.07229", "abs": "https://arxiv.org/abs/2508.07229", "authors": ["Itai Allouche", "Itay Asael", "Rotem Rousso", "Vered Dassa", "Ann Bradlow", "Seung-Eun Kim", "Matthew Goldrick", "Joseph Keshet"], "title": "How Does a Deep Neural Network Look at Lexical Stress?", "comment": "10 pages, 4 figures, submitted to the Journal of the Acoustical\n  Society of America (JASA)", "summary": "Despite their success in speech processing, neural networks often operate as\nblack boxes, prompting the question: what informs their decisions, and how can\nwe interpret them? This work examines this issue in the context of lexical\nstress. A dataset of English disyllabic words was automatically constructed\nfrom read and spontaneous speech. Several Convolutional Neural Network (CNN)\narchitectures were trained to predict stress position from a spectrographic\nrepresentation of disyllabic words lacking minimal stress pairs (e.g., initial\nstress WAllet, final stress exTEND), achieving up to 92% accuracy on held-out\ntest data. Layerwise Relevance Propagation (LRP), a technique for CNN\ninterpretability analysis, revealed that predictions for held-out minimal pairs\n(PROtest vs. proTEST ) were most strongly influenced by information in stressed\nversus unstressed syllables, particularly the spectral properties of stressed\nvowels. However, the classifiers also attended to information throughout the\nword. A feature-specific relevance analysis is proposed, and its results\nsuggest that our best-performing classifier is strongly influenced by the\nstressed vowel's first and second formants, with some evidence that its pitch\nand third formant also contribute. These results reveal deep learning's ability\nto acquire distributed cues to stress from naturally occurring data, extending\ntraditional phonetic work based around highly controlled stimuli.", "AI": {"tldr": "本文研究了神经网络在语音处理中对词汇重音决策的解释性问题。通过训练CNN预测双音节词的重音位置，并使用LRP分析其决策依据，发现重音元音的频谱特性是主要影响因素，揭示了深度学习从自然数据中学习重音线索的能力。", "motivation": "尽管神经网络在语音处理中取得了成功，但它们通常作为“黑箱”运行，因此需要理解它们做出决策的依据以及如何解释这些决策。本文旨在解决神经网络在词汇重音预测中的可解释性问题。", "method": "研究方法包括：1. 自动构建一个包含英语双音节词的数据集，数据来源于朗读和自发语音。2. 训练多个卷积神经网络（CNN）架构，从双音节词的频谱图表示中预测重音位置。3. 使用分层相关性传播（LRP）技术对CNN进行可解释性分析，以揭示影响预测的关键信息。4. 提出并应用了一种特征特异性相关性分析，以识别对分类器影响最大的具体声学特征。", "result": "主要结果如下：1. CNN在预测重音位置上达到了92%的准确率。2. LRP分析显示，对保留的最小对（如PROtest vs. proTEST）的预测主要受重音和非重音音节中信息的影响，特别是重音元音的频谱特性。3. 分类器也关注了整个单词的信息。4. 特征特异性相关性分析表明，表现最佳的分类器主要受重音元音的第一和第二共振峰影响，并有证据表明其音高和第三共振峰也有贡献。", "conclusion": "研究结果表明，深度学习能够从自然发生的语音数据中学习到分布式重音线索，这扩展了基于高度受控刺激的传统语音学研究。同时，通过可解释性分析，揭示了神经网络在重音识别中关注的关键声学特征。"}}
{"id": "2508.06819", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06819", "abs": "https://arxiv.org/abs/2508.06819", "authors": ["Ayaan Nooruddin Siddiqui", "Mahnoor Zaidi", "Ayesha Nazneen Shahbaz", "Priyadarshini Chatterjee", "Krishnan Menon Iyer"], "title": "VesselRW: Weakly Supervised Subcutaneous Vessel Segmentation via Learned Random Walk Propagation", "comment": null, "summary": "Accurate segmentation of subcutaneous vessels from clinical images is\nhampered by scarce, expensive ground truth and by low contrast, noisy\nappearance of vessels across patients and modalities. We present a novel weakly\nsupervised training framework tailored for subcutaneous vessel segmentation\nthat leverages inexpensive sparse annotations (e.g., centerline traces, dot\nmarkers, or short scribbles). Sparse labels are expanded into dense,\nprobabilistic supervision via a differentiable random walk label propagation\nmodel whose transition weights incorporate image driven vesselness cues and\ntubular continuity priors. The propagation yields per-pixel hitting\nprobabilities together with calibrated uncertainty estimates; these are\nincorporated into an uncertainty weighted loss to avoid over fitting to\nambiguous regions. Crucially, the label-propagator is learned jointly with a\nCNN based segmentation predictor, enabling the system to discover vessel edges\nand continuity constraints without explicit edge supervision. We further\nintroduce a topology aware regularizer that encourages centerline connectivity\nand penalizes spurious branches, improving clinical usability. In experiments\non clinical subcutaneous imaging datasets, our method consistently outperforms\nnaive training on sparse labels and conventional dense pseudo-labeling,\nproducing more complete vascular maps and better calibrated uncertainty for\ndownstream decision making. The approach substantially reduces annotation\nburden while preserving clinically relevant vessel topology.", "AI": {"tldr": "本文提出了一种弱监督训练框架，利用稀疏标注和可微分随机游走标签传播模型，结合CNN进行皮下血管分割，有效解决了标注负担重和图像质量差的问题。", "motivation": "临床图像中皮下血管的精确分割面临两大挑战：一是高质量标注数据稀缺且昂贵；二是血管在不同患者和模态下对比度低、噪声大，外观不一致。", "method": "该方法包括一个新颖的弱监督训练框架：1) 利用廉价的稀疏标注（如中心线、点标记、短涂鸦）作为输入；2) 通过一个可微分的随机游走标签传播模型将稀疏标注扩展为密集的概率监督，该模型结合了图像驱动的血管性和管状连续性先验；3) 传播过程产生像素级的命中概率和校准的不确定性估计，并将其整合到不确定性加权的损失函数中以避免过拟合；4) 标签传播器与基于CNN的分割预测器联合学习，以发现血管边缘和连续性约束；5) 引入拓扑感知正则化器，鼓励中心线连接性并惩罚虚假分支。", "result": "在临床皮下成像数据集上的实验表明，该方法持续优于在稀疏标签上进行的朴素训练和传统的密集伪标签方法。它能生成更完整的血管图，并提供更好的校准不确定性，有利于后续决策。", "conclusion": "该方法显著减少了标注负担，同时保留了临床相关的血管拓扑结构，提高了皮下血管分割的准确性和实用性。"}}
{"id": "2508.07407", "categories": ["cs.AI", "cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.07407", "abs": "https://arxiv.org/abs/2508.07407", "authors": ["Jinyuan Fang", "Yanwen Peng", "Xi Zhang", "Yingxu Wang", "Xinhao Yi", "Guibin Zhang", "Yi Xu", "Bin Wu", "Siwei Liu", "Zihao Li", "Zhaochun Ren", "Nikos Aletras", "Xi Wang", "Han Zhou", "Zaiqiao Meng"], "title": "A Comprehensive Survey of Self-Evolving AI Agents: A New Paradigm Bridging Foundation Models and Lifelong Agentic Systems", "comment": null, "summary": "Recent advances in large language models have sparked growing interest in AI\nagents capable of solving complex, real-world tasks. However, most existing\nagent systems rely on manually crafted configurations that remain static after\ndeployment, limiting their ability to adapt to dynamic and evolving\nenvironments. To this end, recent research has explored agent evolution\ntechniques that aim to automatically enhance agent systems based on interaction\ndata and environmental feedback. This emerging direction lays the foundation\nfor self-evolving AI agents, which bridge the static capabilities of foundation\nmodels with the continuous adaptability required by lifelong agentic systems.\nIn this survey, we provide a comprehensive review of existing techniques for\nself-evolving agentic systems. Specifically, we first introduce a unified\nconceptual framework that abstracts the feedback loop underlying the design of\nself-evolving agentic systems. The framework highlights four key components:\nSystem Inputs, Agent System, Environment, and Optimisers, serving as a\nfoundation for understanding and comparing different strategies. Based on this\nframework, we systematically review a wide range of self-evolving techniques\nthat target different components of the agent system. We also investigate\ndomain-specific evolution strategies developed for specialised fields such as\nbiomedicine, programming, and finance, where optimisation objectives are\ntightly coupled with domain constraints. In addition, we provide a dedicated\ndiscussion on the evaluation, safety, and ethical considerations for\nself-evolving agentic systems, which are critical to ensuring their\neffectiveness and reliability. This survey aims to provide researchers and\npractitioners with a systematic understanding of self-evolving AI agents,\nlaying the foundation for the development of more adaptive, autonomous, and\nlifelong agentic systems.", "AI": {"tldr": "该综述全面回顾了自进化AI智能体的现有技术，旨在解决传统智能体在动态环境中适应性不足的问题，并提出了一个统一的概念框架。", "motivation": "现有AI智能体系统多依赖手动配置，部署后保持静态，难以适应动态和不断变化的环境；大型语言模型的发展促使研究者探索能持续适应复杂真实世界任务的AI智能体。", "method": "引入了一个统一的概念框架（系统输入、智能体系统、环境、优化器），在此基础上系统回顾了针对智能体系统不同组件的自进化技术，调查了生物医学、编程、金融等特定领域的进化策略，并讨论了自进化智能体系统的评估、安全和伦理考量。", "result": "提供了对自进化AI智能体现有技术的全面回顾和系统理解，突出了其设计中的反馈循环和关键组件。", "conclusion": "该综述为研究人员和从业者提供了对自进化AI智能体的系统性理解，为开发更具适应性、自主性和终身学习能力的智能体系统奠定了基础。"}}
{"id": "2508.07648", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07648", "abs": "https://arxiv.org/abs/2508.07648", "authors": ["Mehrshad Zandigohar", "Mallesham Dasari", "Gunar Schirner"], "title": "Grasp-HGN: Grasping the Unexpected", "comment": "Paper accepted at ACM Transactions on Embedded Computing Systems", "summary": "For transradial amputees, robotic prosthetic hands promise to regain the\ncapability to perform daily living activities. To advance next-generation\nprosthetic hand control design, it is crucial to address current shortcomings\nin robustness to out of lab artifacts, and generalizability to new\nenvironments. Due to the fixed number of object to interact with in existing\ndatasets, contrasted with the virtually infinite variety of objects encountered\nin the real world, current grasp models perform poorly on unseen objects,\nnegatively affecting users' independence and quality of life.\n  To address this: (i) we define semantic projection, the ability of a model to\ngeneralize to unseen object types and show that conventional models like YOLO,\ndespite 80% training accuracy, drop to 15% on unseen objects. (ii) we propose\nGrasp-LLaVA, a Grasp Vision Language Model enabling human-like reasoning to\ninfer the suitable grasp type estimate based on the object's physical\ncharacteristics resulting in a significant 50.2% accuracy over unseen object\ntypes compared to 36.7% accuracy of an SOTA grasp estimation model.\n  Lastly, to bridge the performance-latency gap, we propose Hybrid Grasp\nNetwork (HGN), an edge-cloud deployment infrastructure enabling fast grasp\nestimation on edge and accurate cloud inference as a fail-safe, effectively\nexpanding the latency vs. accuracy Pareto. HGN with confidence calibration (DC)\nenables dynamic switching between edge and cloud models, improving semantic\nprojection accuracy by 5.6% (to 42.3%) with 3.5x speedup over the unseen object\ntypes. Over a real-world sample mix, it reaches 86% average accuracy (12.2%\ngain over edge-only), and 2.2x faster inference than Grasp-LLaVA alone.", "AI": {"tldr": "该研究通过引入视觉语言模型Grasp-LLaVA和混合边缘-云部署架构HGN，显著提升了仿生手对未知物体的抓握估计能力，解决了现有模型泛化性差和延迟高的问题。", "motivation": "当前机械义肢手控制设计在实验室外环境的鲁棒性和对新环境的泛化性方面存在不足。现有数据集物体数量固定，导致抓握模型在面对现实世界中未见过的物体时表现不佳，影响用户独立性和生活质量。", "method": ["定义了“语义投射”概念，即模型泛化到未见过的物体类型的能力。", "提出了Grasp-LLaVA，一个抓握视觉语言模型，通过模拟人类推理，根据物体的物理特性推断合适的抓握类型。", "提出了混合抓握网络（Hybrid Grasp Network, HGN），一个边缘-云部署基础设施，在边缘端实现快速抓握估计，在云端作为故障安全提供精确推理，并通过置信度校准（DC）实现边缘和云模型之间的动态切换。"], "result": ["传统模型（如YOLO）在训练准确率达80%的情况下，对未见过的物体准确率骤降至15%。", "Grasp-LLaVA在未见过的物体类型上达到了50.2%的准确率，显著高于现有SOTA抓握估计模型的36.7%。", "HGN结合置信度校准（DC）将语义投射准确率提高了5.6%（达到42.3%），同时在未见过的物体类型上实现了3.5倍的速度提升。", "在真实世界样本混合中，HGN达到了86%的平均准确率（比仅边缘部署提高12.2%），并且推理速度比单独使用Grasp-LLaVA快2.2倍。"], "conclusion": "该研究提出的Grasp-LLaVA模型显著提高了仿生手对未见过物体的抓握估计准确性，而HGN混合部署架构则有效地平衡了性能和延迟，为下一代仿生手控制设计提供了更鲁棒和泛化性更强的解决方案。"}}
{"id": "2508.07248", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.07248", "abs": "https://arxiv.org/abs/2508.07248", "authors": ["Zhe Ren"], "title": "Prompt Tuning for Few-Shot Continual Learning Named Entity Recognition", "comment": null, "summary": "Knowledge distillation has been successfully applied to Continual Learning\nNamed Entity Recognition (CLNER) tasks, by using a teacher model trained on\nold-class data to distill old-class entities present in new-class data as a\nform of regularization, thereby avoiding catastrophic forgetting. However, in\nFew-Shot CLNER (FS-CLNER) tasks, the scarcity of new-class entities makes it\ndifficult for the trained model to generalize during inference. More\ncritically, the lack of old-class entity information hinders the distillation\nof old knowledge, causing the model to fall into what we refer to as the\nFew-Shot Distillation Dilemma. In this work, we address the above challenges\nthrough a prompt tuning paradigm and memory demonstration template strategy.\nSpecifically, we designed an expandable Anchor words-oriented Prompt Tuning\n(APT) paradigm to bridge the gap between pre-training and fine-tuning, thereby\nenhancing performance in few-shot scenarios. Additionally, we incorporated\nMemory Demonstration Templates (MDT) into each training instance to provide\nreplay samples from previous tasks, which not only avoids the Few-Shot\nDistillation Dilemma but also promotes in-context learning. Experiments show\nthat our approach achieves competitive performances on FS-CLNER.", "AI": {"tldr": "针对少样本持续学习命名实体识别（FS-CLNER）中的“少样本蒸馏困境”，本文提出了一种锚词导向的提示微调（APT）范式和记忆演示模板（MDT）策略，以提高模型性能并避免灾难性遗忘。", "motivation": "在持续学习命名实体识别（CLNER）中，知识蒸馏能有效避免灾难性遗忘。然而，在少样本CLNER（FS-CLNER）任务中，新类别实体稀缺导致泛化困难；更关键的是，旧类别实体信息不足阻碍了旧知识的蒸馏，引发了“少样本蒸馏困境”。", "method": "本文通过提示微调范式和记忆演示模板策略来解决上述挑战。具体而言，设计了可扩展的锚词导向提示微调（APT）范式，以弥合预训练和微调之间的差距，提升少样本性能。此外，将记忆演示模板（MDT）整合到每个训练实例中，提供来自先前任务的重放样本，这不仅避免了少样本蒸馏困境，还促进了上下文学习。", "result": "实验结果表明，本文提出的方法在FS-CLNER任务上取得了具有竞争力的性能。", "conclusion": "通过结合锚词导向的提示微调和记忆演示模板，本方法成功解决了少样本持续学习命名实体识别中的关键挑战，有效提升了模型在数据稀缺情况下的性能，并避免了知识蒸馏困境。"}}
{"id": "2508.06831", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06831", "abs": "https://arxiv.org/abs/2508.06831", "authors": ["Taha Mustapha Nehdi", "Nairouz Mrabah", "Atif Belal", "Marco Pedersoli", "Eric Granger"], "title": "Low-Rank Expert Merging for Multi-Source Domain Adaptation in Person Re-Identification", "comment": null, "summary": "Adapting person re-identification (reID) models to new target environments\nremains a challenging problem that is typically addressed using unsupervised\ndomain adaptation (UDA) methods. Recent works show that when labeled data\noriginates from several distinct sources (e.g., datasets and cameras),\nconsidering each source separately and applying multi-source domain adaptation\n(MSDA) typically yields higher accuracy and robustness compared to blending the\nsources and performing conventional UDA. However, state-of-the-art MSDA methods\nlearn domain-specific backbone models or require access to source domain data\nduring adaptation, resulting in significant growth in training parameters and\ncomputational cost. In this paper, a Source-free Adaptive Gated Experts\n(SAGE-reID) method is introduced for person reID. Our SAGE-reID is a\ncost-effective, source-free MSDA method that first trains individual\nsource-specific low-rank adapters (LoRA) through source-free UDA. Next, a\nlightweight gating network is introduced and trained to dynamically assign\noptimal merging weights for fusion of LoRA experts, enabling effective\ncross-domain knowledge transfer. While the number of backbone parameters\nremains constant across source domains, LoRA experts scale linearly but remain\nnegligible in size (<= 2% of the backbone), reducing both the memory\nconsumption and risk of overfitting. Extensive experiments conducted on three\nchallenging benchmarks: Market-1501, DukeMTMC-reID, and MSMT17 indicate that\nSAGE-reID outperforms state-of-the-art methods while being computationally\nefficient.", "AI": {"tldr": "SAGE-reID是一种新颖的、无源的多源域适应（MSDA）方法，用于行人重识别（reID），通过训练轻量级LoRA专家和门控网络动态融合它们，实现了计算效率高且性能优越的模型适应。", "motivation": "行人重识别模型在新目标环境下的适应是一个挑战。多源域适应（MSDA）比传统的无监督域适应（UDA）效果更好，但现有的MSDA方法通常需要训练域特定的骨干模型或在适应过程中访问源域数据，导致训练参数和计算成本显著增加。", "method": "本文提出了SAGE-reID方法。它首先通过无源UDA训练单独的、源特定的低秩适配器（LoRA），作为“专家”。接着，引入并训练一个轻量级门控网络，以动态分配最优的合并权重来融合这些LoRA专家，从而实现有效的跨域知识迁移。核心骨干模型的参数数量保持不变，LoRA专家虽然随源域数量线性扩展，但其大小相对于骨干模型仍然可以忽略不计（<= 2%）。", "result": "SAGE-reID在三个挑战性基准数据集（Market-1501、DukeMTMC-reID和MSMT17）上进行了广泛实验。结果表明，SAGE-reID不仅超越了最先进的方法，而且在计算上效率更高，显著降低了内存消耗和过拟合风险。", "conclusion": "SAGE-reID提供了一种成本效益高且性能优越的无源MSDA解决方案，有效解决了行人重识别模型在多源域适应中的计算效率和资源消耗问题，同时保持了高精度。"}}
{"id": "2508.07466", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07466", "abs": "https://arxiv.org/abs/2508.07466", "authors": ["Dom Huh", "Prasant Mohapatra"], "title": "Grounding Natural Language for Multi-agent Decision-Making with Multi-agentic LLMs", "comment": null, "summary": "Language is a ubiquitous tool that is foundational to reasoning and\ncollaboration, ranging from everyday interactions to sophisticated\nproblem-solving tasks. The establishment of a common language can serve as a\npowerful asset in ensuring clear communication and understanding amongst\nagents, facilitating desired coordination and strategies. In this work, we\nextend the capabilities of large language models (LLMs) by integrating them\nwith advancements in multi-agent decision-making algorithms. We propose a\nsystematic framework for the design of multi-agentic large language models\n(LLMs), focusing on key integration practices. These include advanced prompt\nengineering techniques, the development of effective memory architectures,\nmulti-modal information processing, and alignment strategies through\nfine-tuning algorithms. We evaluate these design choices through extensive\nablation studies on classic game settings with significant underlying social\ndilemmas and game-theoretic considerations.", "AI": {"tldr": "该研究提出了一个系统性框架，将大型语言模型（LLMs）与多智能体决策算法相结合，以增强多智能体系统的协作和策略。", "motivation": "语言是推理和协作的基础工具，能促进清晰沟通和理解。研究旨在利用LLMs扩展多智能体决策能力，解决协作和策略问题。", "method": "提出了一个多智能体LLM的设计框架，包含：高级提示工程技术、有效的记忆架构开发、多模态信息处理以及通过微调算法实现的对齐策略。通过在具有社会困境和博弈论考量的经典游戏设置中进行广泛的消融研究来评估这些设计选择。", "result": "通过在经典游戏设置中进行广泛的消融研究，评估了所提出的设计选择和集成实践。", "conclusion": "该框架及其设计选择在模拟经典游戏场景中的有效性得到了评估，表明了LLMs在多智能体决策和协作中的潜力。"}}
{"id": "2508.07650", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07650", "abs": "https://arxiv.org/abs/2508.07650", "authors": ["Helong Huang", "Min Cen", "Kai Tan", "Xingyue Quan", "Guowei Huang", "Hong Zhang"], "title": "GraphCoT-VLA: A 3D Spatial-Aware Reasoning Vision-Language-Action Model for Robotic Manipulation with Ambiguous Instructions", "comment": "10 pages, 6 figures", "summary": "Vision-language-action models have emerged as a crucial paradigm in robotic\nmanipulation. However, existing VLA models exhibit notable limitations in\nhandling ambiguous language instructions and unknown environmental states.\nFurthermore, their perception is largely constrained to static two-dimensional\nobservations, lacking the capability to model three-dimensional interactions\nbetween the robot and its environment. To address these challenges, this paper\nproposes GraphCoT-VLA, an efficient end-to-end model. To enhance the model's\nability to interpret ambiguous instructions and improve task planning, we\ndesign a structured Chain-of-Thought reasoning module that integrates\nhigh-level task understanding and planning, failed task feedback, and low-level\nimaginative reasoning about future object positions and robot actions.\nAdditionally, we construct a real-time updatable 3D Pose-Object graph, which\ncaptures the spatial configuration of robot joints and the topological\nrelationships between objects in 3D space, enabling the model to better\nunderstand and manipulate their interactions. We further integrates a dropout\nhybrid reasoning strategy to achieve efficient control outputs. Experimental\nresults across multiple real-world robotic tasks demonstrate that GraphCoT-VLA\nsignificantly outperforms existing methods in terms of task success rate and\nresponse speed, exhibiting strong generalization and robustness in open\nenvironments and under uncertain instructions.", "AI": {"tldr": "GraphCoT-VLA是一种高效的端到端模型，通过结构化的思维链推理和实时更新的3D姿态-物体图，解决了现有视觉-语言-动作（VLA）模型在处理模糊指令和缺乏3D交互建模方面的局限性，显著提高了机器人任务的成功率和响应速度。", "motivation": "现有的VLA模型在处理模糊语言指令和未知环境状态时存在显著局限性，且其感知主要受限于静态二维观察，缺乏对机器人与环境之间三维交互建模的能力。", "method": "本文提出了GraphCoT-VLA模型。该模型设计了一个结构化的思维链（Chain-of-Thought）推理模块，该模块整合了高层任务理解与规划、失败任务反馈以及对未来物体位置和机器人动作的低层想象推理。此外，构建了一个实时可更新的3D姿态-物体图，用于捕获机器人关节的空间配置和物体在3D空间中的拓扑关系。模型还集成了dropout混合推理策略以实现高效的控制输出。", "result": "在多项真实世界机器人任务中的实验结果表明，GraphCoT-VLA在任务成功率和响应速度方面显著优于现有方法，并在开放环境和不确定指令下表现出强大的泛化能力和鲁棒性。", "conclusion": "GraphCoT-VLA通过引入结构化思维链推理和3D姿态-物体图，有效解决了VLA模型在处理模糊指令和三维交互方面的挑战，显著提升了机器人操作的性能、泛化能力和鲁棒性。"}}
{"id": "2508.07262", "categories": ["cs.CL", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07262", "abs": "https://arxiv.org/abs/2508.07262", "authors": ["Bernd J. Kröger"], "title": "The 2D+ Dynamic Articulatory Model DYNARTmo: Tongue-Palate Contact Area Estimation", "comment": "11 pages, 9 figures, 14 references; supplementary material: python\n  source code", "summary": "This paper describes an extension of the two-dimensional dynamic articulatory\nmodel DYNARTmo by integrating an internal three-dimensional representation of\nthe palatal dome to estimate tongue-palate contact areas from midsagittal\ntongue contours. Two alternative dome geometries - a half-ellipse and a cosine\nbased profile - are implemented to model lateral curvature in the coronal\nplane. Using these geometries, lateral contact points are analytically computed\nfor each anterior-posterior position, enabling the generation of\nelectropalatography-like visualizations within the 2D+ framework. The enhanced\nmodel supports three synchronized views (sagittal, glottal, and palatal) for\nstatic and dynamic (animated) articulation displays, suitable for speech\nscience education and speech therapy. Future work includes adding a facial\n(lip) view and implementing articulatory-to-acoustic synthesis to\nquantitatively evaluate model realism.", "AI": {"tldr": "本文扩展了二维动态发音模型DYNARTmo，通过集成三维腭穹顶表示，实现了从矢状面舌轮廓估计舌腭接触面积，并生成类电腭图可视化。", "motivation": "现有二维发音模型难以准确估计舌腭接触面积。该研究旨在通过整合三维腭部信息，从二维舌轮廓数据中推断舌腭接触区域，从而增强模型的现实性，并使其适用于语音科学教育和言语治疗。", "method": "该研究将一个内部三维腭穹顶表示集成到DYNARTmo模型中。实现了两种替代的腭穹顶几何形状（半椭圆和基于余弦的轮廓）来模拟冠状面上的侧向曲率。通过分析计算每个前后位置的侧向接触点，生成了类似电腭图的可视化。", "result": "增强后的模型支持三种同步视图（矢状面、声门和腭部），可用于静态和动态（动画）发音显示。这使得模型更适合语音科学教育和言语治疗应用。", "conclusion": "该扩展模型成功实现了从二维舌轮廓估计三维舌腭接触，并提供了多视图同步显示，对语音科学教育和言语治疗具有实用价值。未来的工作包括添加面部（唇部）视图和实现发音到声学合成，以定量评估模型的真实性。"}}
{"id": "2508.06845", "categories": ["cs.CV", "cs.CE", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.06845", "abs": "https://arxiv.org/abs/2508.06845", "authors": ["Hamidreza Samadi", "Md Manjurul Ahsan", "Shivakumar Raman"], "title": "Hybrid Machine Learning Framework for Predicting Geometric Deviations from 3D Surface Metrology", "comment": null, "summary": "This study addresses the challenge of accurately forecasting geometric\ndeviations in manufactured components using advanced 3D surface analysis.\nDespite progress in modern manufacturing, maintaining dimensional precision\nremains difficult, particularly for complex geometries. We present a\nmethodology that employs a high-resolution 3D scanner to acquire multi-angle\nsurface data from 237 components produced across different batches. The data\nwere processed through precise alignment, noise reduction, and merging\ntechniques to generate accurate 3D representations. A hybrid machine learning\nframework was developed, combining convolutional neural networks for feature\nextraction with gradient-boosted decision trees for predictive modeling. The\nproposed system achieved a prediction accuracy of 0.012 mm at a 95% confidence\nlevel, representing a 73% improvement over conventional statistical process\ncontrol methods. In addition to improved accuracy, the model revealed hidden\ncorrelations between manufacturing parameters and geometric deviations. This\napproach offers significant potential for automated quality control, predictive\nmaintenance, and design optimization in precision manufacturing, and the\nresulting dataset provides a strong foundation for future predictive modeling\nresearch.", "AI": {"tldr": "本研究利用高分辨率3D扫描和混合机器学习框架，实现了对制造零部件几何偏差的精确预测，显著优于传统方法。", "motivation": "现代制造中，尤其对于复杂几何形状的零部件，保持尺寸精度面临挑战，需要更准确的几何偏差预测方法。", "method": "采用高分辨率3D扫描仪获取237个零部件的多角度表面数据，经过精确对齐、降噪和合并生成3D表示。开发了一个混合机器学习框架，结合卷积神经网络（CNN）进行特征提取和梯度提升决策树（GBDT）进行预测建模。", "result": "预测精度达到0.012毫米（95%置信水平），比传统统计过程控制方法提高了73%。模型还揭示了制造参数与几何偏差之间的隐藏关联。", "conclusion": "该方法在精密制造中具有实现自动化质量控制、预测性维护和设计优化的巨大潜力，并为未来的预测建模研究提供了坚实的数据基础。"}}
{"id": "2508.07468", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.07468", "abs": "https://arxiv.org/abs/2508.07468", "authors": ["Stefan Szeider"], "title": "CP-Agent: Agentic Constraint Programming", "comment": null, "summary": "Translating natural language problem descriptions into formal constraint\nmodels remains a fundamental challenge in constraint programming, requiring\ndeep expertise in both the problem domain and modeling frameworks. Previous\napproaches to automating this translation have employed fixed workflows with\npredetermined modeling steps, failing on a significant number of benchmark\nproblems. We present a new approach using a pure agentic strategy without any\nfixed pipeline. We developed a general-purpose Python coding agent based on the\nReAct (Reason and Act) principle, utilizing a persistent IPython kernel for\nstateful code execution and iterative development. Rather than embedding\nconstraint programming logic into the agent architecture, domain-specific\nexpertise is injected solely through a carefully crafted project prompt. The\nagent combines this prompt-encoded knowledge with access to file operations and\ncode execution tools, enabling it to test hypotheses, debug failures, and\nverify solutions dynamically. Implemented in just a few hundred lines of code,\nthis architecture successfully solves all 101 problems of the CP-Bench\nconstraint programming benchmark set. The results suggest that constraint\nmodeling tasks require the combination of general coding tools and domain\nexpertise encoded in prompts, rather than specialized agent architectures or\npredefined workflows.", "AI": {"tldr": "提出了一种基于ReAct原理的纯智能体策略，用于将自然语言问题描述转换为形式化约束模型，成功解决了CP-Bench基准测试集中的所有问题。", "motivation": "将自然语言问题描述转换为形式化约束模型是约束编程中的一个基本挑战，需要深厚的领域和建模框架专业知识。以前的自动化方法采用固定的工作流程，在大量基准问题上表现不佳。", "method": "该研究采用纯智能体策略，没有固定的流水线。开发了一个基于ReAct（Reason and Act）原理的通用Python编程智能体，利用持久的IPython内核进行有状态的代码执行和迭代开发。领域专业知识通过精心设计的项目提示注入，而非嵌入到智能体架构中。智能体结合提示编码的知识，并访问文件操作和代码执行工具，使其能够动态测试假设、调试故障和验证解决方案。", "result": "该架构仅用几百行代码实现，成功解决了CP-Bench约束编程基准测试集中的所有101个问题。", "conclusion": "研究结果表明，约束建模任务需要结合通用编码工具和通过提示编码的领域专业知识，而不是专门的智能体架构或预定义的工作流程。"}}
{"id": "2508.07657", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07657", "abs": "https://arxiv.org/abs/2508.07657", "authors": ["Zhuoli Tian", "Yuyang Zhang", "Jinsheng Wei", "Meng Guo"], "title": "MoRoCo: Multi-operator-robot Coordination, Interaction and Exploration under Restricted Communication", "comment": "38 pages, 28 figures, Submitted to the International Journal of\n  Robotics Research (IJRR). Project website: https://zl-tian.github.io/MoRoCo/", "summary": "Fleets of autonomous robots are increasingly deployed alongside multiple\nhuman operators to explore unknown environments, identify salient features, and\nperform complex tasks in scenarios such as subterranean exploration,\nreconnaissance, and search-and-rescue missions. In these contexts,\ncommunication is often severely limited to short-range exchanges via ad-hoc\nnetworks, posing challenges to coordination. While recent studies have\naddressed multi-robot exploration under communication constraints, they largely\noverlook the essential role of human operators and their real-time interaction\nwith robotic teams. Operators may demand timely updates on the exploration\nprogress and robot status, reprioritize or cancel tasks dynamically, or request\nlive video feeds and control access. Conversely, robots may seek human\nconfirmation for anomalous events or require help recovering from motion or\nplanning failures. To enable such bilateral, context-aware interactions under\nrestricted communication, this work proposes MoRoCo, a unified framework for\nonline coordination and exploration in multi-operator, multi-robot systems.\nMoRoCo enables the team to adaptively switch among three coordination modes:\nspread mode for parallelized exploration with intermittent data sharing,\nmigrate mode for coordinated relocation, and chain mode for maintaining\nhigh-bandwidth connectivity through multi-hop links. These transitions are\nmanaged through distributed algorithms via only local communication. Extensive\nlarge-scale human-in-the-loop simulations and hardware experiments validate the\nnecessity of incorporating human robot interactions and demonstrate that MoRoCo\nenables efficient, reliable coordination under limited communication, marking a\nsignificant step toward robust human-in-the-loop multi-robot autonomy in\nchallenging environments.", "AI": {"tldr": "MoRoCo是一个统一框架，旨在解决多操作员、多机器人系统在通信受限环境下，实现高效、可靠的人机协作探索与协调。", "motivation": "在地下探索、侦察和搜救等场景中，自主机器人编队与人类操作员协同部署，但通信通常受限。现有研究多关注通信受限下的多机器人探索，却忽视了人类操作员的关键作用及其与机器人团队的实时交互需求，如状态更新、任务动态调整、视频流请求、异常事件确认和故障恢复协助等。", "method": "本文提出了MoRoCo框架，通过分布式算法和局部通信，使团队能够自适应地在三种协调模式间切换：分散模式（用于并行探索和间歇数据共享）、迁移模式（用于协调重新定位）和链式模式（通过多跳链接维持高带宽连接）。", "result": "大规模人机回路仿真和硬件实验验证了MoRoCo在有限通信下实现高效、可靠协调的必要性，并证明了将人机交互纳入考量的重要性。", "conclusion": "MoRoCo框架在通信受限的挑战性环境中，实现了鲁棒的人机协作多机器人自主性，是该领域的重要进展。"}}
{"id": "2508.07273", "categories": ["cs.CL", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.07273", "abs": "https://arxiv.org/abs/2508.07273", "authors": ["Qiongqiong Wang", "Hardik B. Sailor", "Jeremy H. M. Wong", "Tianchi Liu", "Shuo Sun", "Wenyu Zhang", "Muhammad Huzaifah", "Nancy Chen", "Ai Ti Aw"], "title": "Incorporating Contextual Paralinguistic Understanding in Large Speech-Language Models", "comment": "Accepted at (ASRU 2025) 2025 IEEE Automatic Speech Recognition and\n  Understanding Workshop", "summary": "Current large speech language models (Speech-LLMs) often exhibit limitations\nin empathetic reasoning, primarily due to the absence of training datasets that\nintegrate both contextual content and paralinguistic cues. In this work, we\npropose two approaches to incorporate contextual paralinguistic information\ninto model training: (1) an explicit method that provides paralinguistic\nmetadata (e.g., emotion annotations) directly to the LLM, and (2) an implicit\nmethod that automatically generates novel training question-answer (QA) pairs\nusing both categorical and dimensional emotion annotations alongside speech\ntranscriptions. Our implicit method boosts performance (LLM-judged) by 38.41%\non a human-annotated QA benchmark, reaching 46.02% when combined with the\nexplicit approach, showing effectiveness in contextual paralinguistic\nunderstanding. We also validate the LLM judge by demonstrating its correlation\nwith classification metrics, providing support for its reliability.", "AI": {"tldr": "本文提出两种方法（显式和隐式）将上下文语用信息融入语音大语言模型（Speech-LLMs）训练，以提升其同理心推理能力，并取得了显著效果。", "motivation": "当前的语音大语言模型在同理心推理方面存在局限性，主要原因是训练数据中缺乏整合语境内容和语用线索的信息。", "method": "研究提出了两种方法：1) 显式方法：直接向LLM提供语用元数据（如情感标注）；2) 隐式方法：利用分类和维度情感标注以及语音转录，自动生成新的训练问答（QA）对。同时，还验证了LLM评判器的可靠性。", "result": "隐式方法使模型在人工标注的QA基准测试上性能提升了38.41%（LLM评判），与显式方法结合后提升至46.02%。此外，LLM评判器与分类指标具有相关性，证明了其可靠性。", "conclusion": "将上下文语用信息融入模型训练能有效提升Speech-LLMs的语境语用理解能力和同理心推理能力。"}}
{"id": "2508.06853", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06853", "abs": "https://arxiv.org/abs/2508.06853", "authors": ["L. D. M. S. Sai Teja", "Ashok Urlana", "Pruthwik Mishra"], "title": "AGIC: Attention-Guided Image Captioning to Improve Caption Relevance", "comment": "10 pages, 5 Figures", "summary": "Despite significant progress in image captioning, generating accurate and\ndescriptive captions remains a long-standing challenge. In this study, we\npropose Attention-Guided Image Captioning (AGIC), which amplifies salient\nvisual regions directly in the feature space to guide caption generation. We\nfurther introduce a hybrid decoding strategy that combines deterministic and\nprobabilistic sampling to balance fluency and diversity. To evaluate AGIC, we\nconduct extensive experiments on the Flickr8k and Flickr30k datasets. The\nresults show that AGIC matches or surpasses several state-of-the-art models\nwhile achieving faster inference. Moreover, AGIC demonstrates strong\nperformance across multiple evaluation metrics, offering a scalable and\ninterpretable solution for image captioning.", "AI": {"tldr": "本文提出了一种名为AGIC的图像描述模型，通过注意力机制增强显著视觉区域并结合混合解码策略，在保持流畅性和多样性的同时，实现了与SOTA模型相当或更优的性能，且推理速度更快。", "motivation": "生成准确且描述性的图像描述仍然是一个长期存在的挑战。", "method": "提出了注意力引导图像描述（AGIC）模型，该模型直接在特征空间中放大显著视觉区域以指导描述生成。此外，引入了一种结合确定性和概率采样的混合解码策略，以平衡描述的流畅性和多样性。", "result": "在Flickr8k和Flickr30k数据集上，AGIC模型与多个最先进的模型性能相当或超越，同时实现了更快的推理速度。它在多项评估指标上表现出色。", "conclusion": "AGIC为图像描述提供了一个可扩展且可解释的解决方案。"}}
{"id": "2508.07485", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07485", "abs": "https://arxiv.org/abs/2508.07485", "authors": ["Alexander Duffy", "Samuel J Paech", "Ishana Shastri", "Elizabeth Karpinski", "Baptiste Alloui-Cros", "Tyler Marques", "Matthew Lyle Olson"], "title": "Democratizing Diplomacy: A Harness for Evaluating Any Large Language Model on Full-Press Diplomacy", "comment": null, "summary": "We present the first evaluation harness that enables any out-of-the-box,\nlocal, Large Language Models (LLMs) to play full-press Diplomacy without\nfine-tuning or specialized training. Previous work required frontier LLMs, or\nfine-tuning, due to the high complexity and information density of Diplomacy's\ngame state. Combined with the high variance of matches, these factors made\nDiplomacy prohibitive for study. In this work, we used data-driven iteration to\noptimize a textual game state representation such that a 24B model can reliably\ncomplete matches without any fine tuning. We develop tooling to facilitate\nhypothesis testing and statistical analysis, and we present case studies on\npersuasion, aggressive playstyles, and performance across a range of models. We\nconduct a variety of experiments across many popular LLMs, finding the larger\nmodels perform the best, but the smaller models still play adequately. We also\nintroduce Critical State Analysis: an experimental protocol for rapidly\niterating and analyzing key moments in a game at depth. Our harness\ndemocratizes the evaluation of strategic reasoning in LLMs by eliminating the\nneed for fine-tuning, and it provides insights into how these capabilities\nemerge naturally from widely used LLMs. Our code is available in the supplement\nand will be open sourced.", "AI": {"tldr": "该研究提出了首个评估工具，使本地大型语言模型（LLMs）无需微调即可玩《外交》游戏，并通过优化游戏状态表示和分析工具，降低了研究门槛。", "motivation": "由于《外交》游戏的复杂性和信息密度高，以往的研究需要前沿LLM或进行微调，且比赛结果方差大，导致该游戏难以用于LLM研究。", "method": "开发了一个评估工具，使开箱即用的本地LLMs能够玩《外交》游戏；通过数据驱动的迭代优化了文本游戏状态表示；开发了用于假设检验和统计分析的工具；引入了“关键状态分析”实验协议，用于快速迭代和深入分析游戏中的关键时刻。", "result": "一个24B模型无需微调即可可靠地完成比赛；更大的模型表现最佳，但较小的模型也能玩得不错；进行了关于说服、激进打法和不同模型性能的案例研究。", "conclusion": "该评估工具通过消除微调需求，使LLMs的战略推理能力评估变得民主化，并揭示了这些能力如何自然地从广泛使用的LLMs中涌现。"}}
{"id": "2508.07686", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07686", "abs": "https://arxiv.org/abs/2508.07686", "authors": ["Mingyue Lei", "Zewei Zhou", "Hongchen Li", "Jiaqi Ma", "Jia Hu"], "title": "Risk Map As Middleware: Towards Interpretable Cooperative End-to-end Autonomous Driving for Risk-Aware Planning", "comment": null, "summary": "End-to-end paradigm has emerged as a promising approach to autonomous\ndriving. However, existing single-agent end-to-end pipelines are often\nconstrained by occlusion and limited perception range, resulting in hazardous\ndriving. Furthermore, their black-box nature prevents the interpretability of\nthe driving behavior, leading to an untrustworthiness system. To address these\nlimitations, we introduce Risk Map as Middleware (RiskMM) and propose an\ninterpretable cooperative end-to-end driving framework. The risk map learns\ndirectly from the driving data and provides an interpretable spatiotemporal\nrepresentation of the scenario from the upstream perception and the\ninteractions between the ego vehicle and the surrounding environment for\ndownstream planning. RiskMM first constructs a multi-agent spatiotemporal\nrepresentation with unified Transformer-based architecture, then derives\nrisk-aware representations by modeling interactions among surrounding\nenvironments with attention. These representations are subsequently fed into a\nlearning-based Model Predictive Control (MPC) module. The MPC planner\ninherently accommodates physical constraints and different vehicle types and\ncan provide interpretation by aligning learned parameters with explicit MPC\nelements. Evaluations conducted on the real-world V2XPnP-Seq dataset confirm\nthat RiskMM achieves superior and robust performance in risk-aware trajectory\nplanning, significantly enhancing the interpretability of the cooperative\nend-to-end driving framework. The codebase will be released to facilitate\nfuture research in this field.", "AI": {"tldr": "该研究提出了RiskMM，一个可解释的协同端到端自动驾驶框架，通过风险地图作为中间件，结合多智能体感知和基于学习的MPC，解决传统单智能体系统的局限性并提升可信度。", "motivation": "现有单智能体端到端自动驾驶系统受限于遮挡和感知范围，导致危险驾驶；其黑盒性质也缺乏可解释性，降低了系统可信度。", "method": "引入风险地图作为中间件（RiskMM）。RiskMM首先通过统一的Transformer架构构建多智能体时空表示，然后通过注意力机制建模环境交互，推导出风险感知表示。这些表示随后输入到基于学习的模型预测控制（MPC）模块，MPC能适应物理约束和不同车型，并通过参数与MPC元素对齐提供解释性。", "result": "在真实世界的V2XPnP-Seq数据集上评估显示，RiskMM在风险感知轨迹规划方面实现了卓越且稳健的性能，显著增强了协同端到端驾驶框架的可解释性。", "conclusion": "RiskMM成功地通过引入可解释的风险地图中间件和基于学习的MPC，克服了现有端到端自动驾驶系统的感知局限性和黑盒问题，提供了一个更安全、更可信赖的协同驾驶解决方案。"}}
{"id": "2508.07279", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07279", "abs": "https://arxiv.org/abs/2508.07279", "authors": ["Vasudha Varadarajan", "Hui Xu", "Rebecca Astrid Boehme", "Mariam Marlan Mirstrom", "Sverker Sikstrom", "H. Andrew Schwartz"], "title": "MAQuA: Adaptive Question-Asking for Multidimensional Mental Health Screening using Item Response Theory", "comment": null, "summary": "Recent advances in large language models (LLMs) offer new opportunities for\nscalable, interactive mental health assessment, but excessive querying by LLMs\nburdens users and is inefficient for real-world screening across\ntransdiagnostic symptom profiles. We introduce MAQuA, an adaptive\nquestion-asking framework for simultaneous, multidimensional mental health\nscreening. Combining multi-outcome modeling on language responses with item\nresponse theory (IRT) and factor analysis, MAQuA selects the questions with\nmost informative responses across multiple dimensions at each turn to optimize\ndiagnostic information, improving accuracy and potentially reducing response\nburden. Empirical results on a novel dataset reveal that MAQuA reduces the\nnumber of assessment questions required for score stabilization by 50-87%\ncompared to random ordering (e.g., achieving stable depression scores with 71%\nfewer questions and eating disorder scores with 85% fewer questions). MAQuA\ndemonstrates robust performance across both internalizing (depression, anxiety)\nand externalizing (substance use, eating disorder) domains, with early stopping\nstrategies further reducing patient time and burden. These findings position\nMAQuA as a powerful and efficient tool for scalable, nuanced, and interactive\nmental health screening, advancing the integration of LLM-based agents into\nreal-world clinical workflows.", "AI": {"tldr": "MAQuA是一个自适应问答框架，结合多结果建模、IRT和因子分析，能显著减少LLM在心理健康评估中所需的提问数量，同时保持准确性，从而减轻用户负担。", "motivation": "尽管大型语言模型（LLMs）为可扩展、交互式心理健康评估提供了新机遇，但其过度提问会增加用户负担，且在跨诊断症状筛查中效率低下。", "method": "MAQuA是一个自适应问答框架，用于同步、多维度的心理健康筛查。它结合了基于语言回应的多结果建模、项目反应理论（IRT）和因子分析，在每个回合动态选择最具信息量的多维度问题，以优化诊断信息。", "result": "在一项新数据集上的实证结果显示，与随机排序相比，MAQuA将稳定评分所需的评估问题数量减少了50-87%（例如，抑郁症评分减少71%的问题，进食障碍评分减少85%的问题）。MAQuA在内化（抑郁、焦虑）和外化（物质使用、进食障碍）领域均表现出稳健性能，且早期停止策略进一步减少了患者时间和负担。", "conclusion": "MAQuA被定位为一种强大且高效的工具，可用于可扩展、细致入微和交互式的心理健康筛查，推动了基于LLM的智能体在实际临床工作流程中的整合。"}}
{"id": "2508.06857", "categories": ["cs.CV", "cs.DS"], "pdf": "https://arxiv.org/pdf/2508.06857", "abs": "https://arxiv.org/abs/2508.06857", "authors": ["Mengxue Jia", "Zhihua Allen-Zhao", "You Zhao", "Sanyang Liu"], "title": "A Joint Sparse Self-Representation Learning Method for Multiview Clustering", "comment": null, "summary": "Multiview clustering (MC) aims to group samples using consistent and\ncomplementary information across various views. The subspace clustering, as a\nfundamental technique of MC, has attracted significant attention. In this\npaper, we propose a novel joint sparse self-representation learning model for\nMC, where a featured difference is the extraction of view-specific local\ninformation by introducing cardinality (i.e., $\\ell_0$-norm) constraints\ninstead of Graph-Laplacian regularization. Specifically, under each view,\ncardinality constraints directly restrict the samples used in the\nself-representation stage to extract reliable local and global structure\ninformation, while the low-rank constraint aids in revealing a global coherent\nstructure in the consensus affinity matrix during merging. The attendant\nchallenge is that Augmented Lagrange Method (ALM)-based alternating\nminimization algorithms cannot guarantee convergence when applied directly to\nour nonconvex, nonsmooth model, thus resulting in poor generalization ability.\nTo address it, we develop an alternating quadratic penalty (AQP) method with\nglobal convergence, where two subproblems are iteratively solved by closed-form\nsolutions. Empirical results on six standard datasets demonstrate the\nsuperiority of our model and AQP method, compared to eight state-of-the-art\nalgorithms.", "AI": {"tldr": "本文提出了一种新颖的联合稀疏自表示学习模型用于多视图聚类（MC），通过引入基数（L0范数）约束提取视图特异性局部信息，并结合低秩约束揭示全局一致结构，同时开发了一种全局收敛的交替二次惩罚（AQP）方法来解决优化问题。", "motivation": "多视图聚类中的子空间聚类技术吸引了广泛关注，但现有方法在提取视图特异性局部信息和优化非凸非光滑模型时存在局限性（如Graph-Laplacian正则化、ALM算法无法保证收敛），导致泛化能力不佳。", "method": "提出一种联合稀疏自表示学习模型，通过L0范数约束直接限制自表示阶段的样本，以提取可靠的局部和全局结构信息；同时引入低秩约束来揭示共识亲和矩阵中的全局一致结构。针对模型的非凸非光滑特性，开发了一种具有全局收敛性的交替二次惩罚（AQP）方法，并通过闭式解迭代求解两个子问题。", "result": "在六个标准数据集上的实证结果表明，与八种最先进的算法相比，所提出的模型和AQP方法表现出优越性。", "conclusion": "所提出的基于L0范数约束的联合稀疏自表示学习模型结合低秩约束能有效提取多视图数据信息，并且开发的AQP优化方法能保证全局收敛性，显著提升了多视图聚类的性能。"}}
{"id": "2508.07575", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07575", "abs": "https://arxiv.org/abs/2508.07575", "authors": ["Shiqing Fan", "Xichen Ding", "Liang Zhang", "Linjian Mo"], "title": "MCPToolBench++: A Large Scale AI Agent Model Context Protocol MCP Tool Use Benchmark", "comment": "Benchmarks and Source Code Released", "summary": "LLMs' capabilities are enhanced by using function calls to integrate various\ndata sources or API results into the context window. Typical tools include\nsearch, web crawlers, maps, financial data, file systems, and browser usage,\netc. Integrating these data sources or functions requires a standardized\nmethod. The Model Context Protocol (MCP) provides a standardized way to supply\ncontext to LLMs. However, the evaluation of LLMs and AI Agents' MCP tool use\nabilities suffer from several issues. First, there's a lack of comprehensive\ndatasets or benchmarks to evaluate various MCP tools. Second, the diverse\nformats of response from MCP tool call execution further increase the\ndifficulty of evaluation. Additionally, unlike existing tool-use benchmarks\nwith high success rates in functions like programming and math functions, the\nsuccess rate of real-world MCP tool is not guaranteed and varies across\ndifferent MCP servers. Furthermore, the LLMs' context window also limits the\nnumber of available tools that can be called in a single run, because the\ntextual descriptions of tool and the parameters have long token length for an\nLLM to process all at once. To help address the challenges of evaluating LLMs'\nperformance on calling MCP tools, we propose MCPToolBench++, a large-scale,\nmulti-domain AI Agent tool use benchmark. As of July 2025, this benchmark is\nbuild upon marketplace of over 4k MCP servers from more than 40 categories,\ncollected from the MCP marketplaces and GitHub communities. The datasets\nconsist of both single-step and multi-step tool calls across different\ncategories. We evaluated SOTA LLMs with agentic abilities on this benchmark and\nreported the results.", "AI": {"tldr": "本文提出了MCPToolBench++，一个大规模、多领域的AI Agent工具使用基准，旨在解决当前评估大语言模型（LLMs）调用模型上下文协议（MCP）工具时面临的缺乏综合数据集、响应格式多样、工具成功率不确定以及上下文窗口限制等挑战。", "motivation": "当前评估LLMs和AI Agents的MCP工具使用能力存在多重问题：缺乏综合性数据集或基准来评估各类MCP工具；MCP工具调用执行的响应格式多样增加了评估难度；与编程、数学等功能不同，真实世界MCP工具的成功率无法保证且因服务器而异；LLMs的上下文窗口限制了单次运行中可调用的工具数量，因为工具描述和参数的文本长度很长。", "method": "作者提出了MCPToolBench++，一个大型、多领域的AI Agent工具使用基准。该基准基于从MCP市场和GitHub社区收集的超过4000个MCP服务器构建，涵盖40多个类别。数据集包含单步和多步工具调用。作者使用该基准评估了SOTA的具有Agent能力的LLMs。", "result": "MCPToolBench++基准已构建完成，包含来自40多个类别的4000多个MCP服务器，并包含了单步和多步工具调用数据集。作者已使用此基准评估了具有Agent能力的SOTA LLMs，并报告了结果。", "conclusion": "MCPToolBench++提供了一个急需的、大规模、多领域的基准，用于解决LLMs和AI Agents在调用MCP工具时面临的评估挑战，有助于更全面地理解和提升LLMs的工具使用能力。"}}
{"id": "2508.07689", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07689", "abs": "https://arxiv.org/abs/2508.07689", "authors": ["Christian Eichmann", "Sabine Bellmann", "Nicolas Hügel", "Louis-Elias Enslin", "Carsten Plasberg", "Georg Heppner", "Arne Roennau", "Ruediger Dillmann"], "title": "LAURON VI: A Six-Legged Robot for Dynamic Walking", "comment": null, "summary": "Legged locomotion enables robotic systems to traverse extremely challenging\nterrains. In many real-world scenarios, the terrain is not that difficult and\nthese mixed terrain types introduce the need for flexible use of different\nwalking strategies to achieve mission goals in a fast, reliable, and\nenergy-efficient way. Six-legged robots have a high degree of flexibility and\ninherent stability that aids them in traversing even some of the most difficult\nterrains, such as collapsed buildings. However, their lack of fast walking\ngaits for easier surfaces is one reason why they are not commonly applied in\nthese scenarios.\n  This work presents LAURON VI, a six-legged robot platform for research on\ndynamic walking gaits as well as on autonomy for complex field missions. The\nrobot's 18 series elastic joint actuators offer high-frequency interfaces for\nCartesian impedance and pure torque control. We have designed, implemented, and\ncompared three control approaches: kinematic-based, model-predictive, and\nreinforcement-learned controllers. The robot hardware and the different control\napproaches were extensively tested in a lab environment as well as on a Mars\nanalog mission. The introduction of fast locomotion strategies for LAURON VI\nmakes six-legged robots vastly more suitable for a wide range of real-world\napplications.", "AI": {"tldr": "本文介绍了六足机器人LAURON VI，旨在研究动态步态和复杂任务自主性。该机器人配备18个串联弹性关节，并比较了基于运动学、模型预测和强化学习的三种控制方法，旨在实现快速行走，拓宽六足机器人的应用范围。", "motivation": "六足机器人擅长穿越极具挑战性的地形（如倒塌建筑），但由于缺乏适用于简单地面的快速行走步态，限制了它们在需要快速、可靠和节能地完成任务的混合地形场景中的应用。", "method": "本文提出了六足机器人平台LAURON VI，该平台拥有18个串联弹性关节执行器，提供高频笛卡尔阻抗和纯扭矩控制接口。研究设计、实现并比较了三种控制方法：基于运动学的控制器、模型预测控制器和强化学习控制器。机器人硬件和不同控制方法在实验室环境以及火星模拟任务中进行了广泛测试。", "result": "LAURON VI机器人平台及其18个串联弹性关节执行器被成功开发，并实现了高频笛卡尔阻抗和纯扭矩控制。三种不同的控制方法（基于运动学、模型预测和强化学习）被设计、实现并进行了比较。通过在实验室和火星模拟任务中的广泛测试，引入的快速运动策略显著提高了六足机器人在各种实际应用中的适用性。", "conclusion": "通过为LAURON VI引入快速移动策略，克服了六足机器人在平坦地面上行走速度慢的缺点，使其更适合于广泛的现实世界应用场景，尤其是在需要灵活运用不同行走策略以实现任务目标的混合地形中。"}}
{"id": "2508.07284", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.07284", "abs": "https://arxiv.org/abs/2508.07284", "authors": ["Junchen Ding", "Penghao Jiang", "Zihao Xu", "Ziqi Ding", "Yichen Zhu", "Jiaojiao Jiang", "Yuekang Li"], "title": "\"Pull or Not to Pull?'': Investigating Moral Biases in Leading Large Language Models Across Ethical Dilemmas", "comment": null, "summary": "As large language models (LLMs) increasingly mediate ethically sensitive\ndecisions, understanding their moral reasoning processes becomes imperative.\nThis study presents a comprehensive empirical evaluation of 14 leading LLMs,\nboth reasoning enabled and general purpose, across 27 diverse trolley problem\nscenarios, framed by ten moral philosophies, including utilitarianism,\ndeontology, and altruism. Using a factorial prompting protocol, we elicited\n3,780 binary decisions and natural language justifications, enabling analysis\nalong axes of decisional assertiveness, explanation answer consistency, public\nmoral alignment, and sensitivity to ethically irrelevant cues. Our findings\nreveal significant variability across ethical frames and model types: reasoning\nenhanced models demonstrate greater decisiveness and structured justifications,\nyet do not always align better with human consensus. Notably, \"sweet zones\"\nemerge in altruistic, fairness, and virtue ethics framings, where models\nachieve a balance of high intervention rates, low explanation conflict, and\nminimal divergence from aggregated human judgments. However, models diverge\nunder frames emphasizing kinship, legality, or self interest, often producing\nethically controversial outcomes. These patterns suggest that moral prompting\nis not only a behavioral modifier but also a diagnostic tool for uncovering\nlatent alignment philosophies across providers. We advocate for moral reasoning\nto become a primary axis in LLM alignment, calling for standardized benchmarks\nthat evaluate not just what LLMs decide, but how and why.", "AI": {"tldr": "该研究全面评估了14个大型语言模型（LLMs）在27个电车难题场景中的道德推理能力，这些场景由10种道德哲学框架。研究发现模型在不同道德框架下表现出显著差异，并在某些框架下（如利他主义、公平、美德伦理）表现出与人类判断高度一致的“甜蜜区”，但在其他框架下（如亲属关系、合法性、自利）则出现分歧。研究强调了道德推理在LLM对齐中的重要性。", "motivation": "随着大型语言模型（LLMs）越来越多地介入道德敏感决策，理解它们的道德推理过程变得至关重要。", "method": "研究评估了14个主流LLMs（包括具备推理能力的和通用型的），在27个不同的电车难题场景中进行测试，这些场景由10种道德哲学（如功利主义、道义论、利他主义）框架。采用因子提示协议，收集了3,780个二元决策和自然语言解释，并从决策果断性、解释答案一致性、公众道德一致性以及对无关道德线索的敏感性等维度进行了分析。", "result": "研究发现不同道德框架和模型类型之间存在显著差异：增强推理能力的模型表现出更强的决策性和结构化解释，但并非总是与人类共识更好地对齐。值得注意的是，在利他主义、公平和美德伦理框架下出现了“甜蜜区”，模型在此实现了高干预率、低解释冲突和与人类聚合判断的最小偏差。然而，在强调亲属关系、合法性或自利等框架下，模型表现出分歧，常常产生道德上有争议的结果。这些模式表明道德提示不仅是行为修饰符，也是揭示不同提供商潜在对齐哲学的诊断工具。", "conclusion": "研究倡导将道德推理作为LLM对齐的一个主要轴心，并呼吁建立标准化的基准，不仅评估LLM决定了什么，还要评估它们如何以及为何做出决定。"}}
{"id": "2508.06869", "categories": ["cs.CV", "cs.AI", "I.2.10"], "pdf": "https://arxiv.org/pdf/2508.06869", "abs": "https://arxiv.org/abs/2508.06869", "authors": ["Jianxiang He", "Shaoguang Wang", "Weiyu Guo", "Meisheng Hong", "Jungang Li", "Yijie Xu", "Ziyang Chen", "Hui Xiong"], "title": "VSI: Visual Subtitle Integration for Keyframe Selection to enhance Long Video Understanding", "comment": "9 pages,3 figures", "summary": "Long video understanding presents a significant challenge to multimodal large\nlanguage models (MLLMs) primarily due to the immense data scale. A critical and\nwidely adopted strategy for making this task computationally tractable is\nkeyframe retrieval, which seeks to identify a sparse set of video frames that\nare most salient to a given textual query. However, the efficacy of this\napproach is hindered by weak multimodal alignment between textual queries and\nvisual content and fails to capture the complex temporal semantic information\nrequired for precise reasoning. To address this, we propose Visual-Subtitle\nIntegeration(VSI), a multimodal keyframe search method that integrates\nsubtitles, timestamps, and scene boundaries into a unified multimodal search\nprocess. The proposed method captures the visual information of video frames as\nwell as the complementary textual information through a dual-stream search\nmechanism by Video Search Stream as well as Subtitle Match Stream,\nrespectively, and improves the keyframe search accuracy through the interaction\nof the two search streams. Experimental results show that VSI achieve 40.00%\nkey frame localization accuracy on the text-relevant subset of LongVideoBench\nand 68.48% accuracy on downstream long Video-QA tasks, surpassing competitive\nbaselines by 20.35% and 15.79%, respectively. Furthermore, on the\nLongVideoBench, VSI achieved state-of-the-art(SOTA) in medium-to-long video-QA\ntasks, demonstrating the robustness and generalizability of the proposed\nmultimodal search strategy.", "AI": {"tldr": "本文提出了VSI（Visual-Subtitle Integration），一种新的多模态关键帧搜索方法，通过整合字幕、时间戳和场景边界，并采用双流搜索机制，显著提高了长视频理解中的关键帧定位和视频问答任务的准确性。", "motivation": "长视频理解对多模态大语言模型（MLLMs）来说是一个巨大的挑战，主要原因在于数据规模庞大。虽然关键帧检索是解决计算可行性的常用策略，但现有方法存在文本查询与视觉内容多模态对齐弱以及未能捕获复杂时序语义信息的问题，从而影响了精确推理的效率。", "method": "本文提出了VSI方法，通过将字幕、时间戳和场景边界整合到统一的多模态搜索过程中。该方法采用双流搜索机制，即视频搜索流（Video Search Stream）和字幕匹配流（Subtitle Match Stream），分别捕获视频帧的视觉信息和互补的文本信息，并通过两个搜索流的交互作用提高关键帧搜索的准确性。", "result": "实验结果表明，VSI在LongVideoBench的文本相关子集上实现了40.00%的关键帧定位准确率，在下游长视频问答任务上达到了68.48%的准确率，分别比竞争基线高出20.35%和15.79%。此外，VSI在LongVideoBench的中长视频问答任务中达到了最先进（SOTA）水平。", "conclusion": "VSI作为一种鲁棒且可泛化的多模态搜索策略，通过有效整合视觉和文本信息，显著提升了长视频理解中关键帧定位和视频问答任务的性能。"}}
{"id": "2508.07586", "categories": ["cs.AI", "cs.NI"], "pdf": "https://arxiv.org/pdf/2508.07586", "abs": "https://arxiv.org/abs/2508.07586", "authors": ["Wenjing Zhang", "Ye Hu", "Tao Luo", "Zhilong Zhang", "Mingzhe Chen"], "title": "Optimization of Private Semantic Communication Performance: An Uncooperative Covert Communication Method", "comment": null, "summary": "In this paper, a novel covert semantic communication framework is\ninvestigated. Within this framework, a server extracts and transmits the\nsemantic information, i.e., the meaning of image data, to a user over several\ntime slots. An attacker seeks to detect and eavesdrop the semantic transmission\nto acquire details of the original image. To avoid data meaning being\neavesdropped by an attacker, a friendly jammer is deployed to transmit jamming\nsignals to interfere the attacker so as to hide the transmitted semantic\ninformation. Meanwhile, the server will strategically select time slots for\nsemantic information transmission. Due to limited energy, the jammer will not\ncommunicate with the server and hence the server does not know the transmit\npower of the jammer. Therefore, the server must jointly optimize the semantic\ninformation transmitted at each time slot and the corresponding transmit power\nto maximize the privacy and the semantic information transmission quality of\nthe user. To solve this problem, we propose a prioritised sampling assisted\ntwin delayed deep deterministic policy gradient algorithm to jointly determine\nthe transmitted semantic information and the transmit power per time slot\nwithout the communications between the server and the jammer. Compared to\nstandard reinforcement learning methods, the propose method uses an additional\nQ network to estimate Q values such that the agent can select the action with a\nlower Q value from the two Q networks thus avoiding local optimal action\nselection and estimation bias of Q values. Simulation results show that the\nproposed algorithm can improve the privacy and the semantic information\ntransmission quality by up to 77.8% and 14.3% compared to the traditional\nreinforcement learning methods.", "AI": {"tldr": "本文提出一种隐蔽语义通信框架，通过优化语义信息传输和发射功率，并部署干扰器，在服务器不了解干扰器功率的情况下，提高隐私和传输质量，并引入一种改进的强化学习算法来解决此问题。", "motivation": "在语义通信中，需要将图像的语义信息隐蔽地传输给用户，同时防止攻击者检测和窃听。挑战在于服务器无法获知干扰器的发射功率，因此需要智能地优化传输策略以最大化隐私和传输质量。", "method": "构建了一个隐蔽语义通信框架，包括服务器、用户、攻击者和友好干扰器。服务器在多个时隙中提取并传输图像的语义信息，干扰器发射干扰信号以隐藏传输。服务器需要联合优化每个时隙传输的语义信息和相应的发射功率。为解决该问题，提出了一种优先采样辅助双延迟深度确定性策略梯度（PSATD3）算法。该算法引入额外的Q网络来估计Q值，以避免局部最优和估计偏差。", "result": "仿真结果表明，与传统强化学习方法相比，所提出的算法可以将隐私性提高高达77.8%，语义信息传输质量提高高达14.3%。", "conclusion": "所提出的PSATD3算法能够在服务器不与干扰器通信的情况下，有效提升隐蔽语义通信的隐私性和传输质量，显著优于传统强化学习方法。"}}
{"id": "2508.07758", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07758", "abs": "https://arxiv.org/abs/2508.07758", "authors": ["Antonio Rosales", "Alaa Abderrahim", "Markku Suomalainen", "Mikael Haag", "Tapio Heikkilä"], "title": "Robot and Overhead Crane Collaboration Scheme to Enhance Payload Manipulation", "comment": null, "summary": "This paper presents a scheme to enhance payload manipulation using a robot\ncollaborating with an overhead crane. In the current industrial practice, when\nthe crane's payload has to be accurately manipulated and located in a desired\nposition, the task becomes laborious and risky since the operators have to\nguide the fine motions of the payload by hand. In the proposed collaborative\nscheme, the crane lifts the payload while the robot's end-effector guides it\ntoward the desired position. The only link between the robot and the crane is\nthe interaction force produced during the guiding of the payload. Two\nadmittance transfer functions are considered to accomplish harmless and smooth\ncontact with the payload. The first is used in a position-based admittance\ncontrol integrated with the robot. The second one adds compliance to the crane\nby processing the interaction force through the admittance transfer function to\ngenerate a crane's velocity command that makes the crane follow the payload.\nThen the robot's end-effector and the crane move collaboratively to guide the\npayload to the desired location. A method is presented to design the admittance\ncontrollers that accomplish a fluent robot-crane collaboration. Simulations and\nexperiments validating the scheme potential are shown.", "AI": {"tldr": "该论文提出了一种机器人与起重机协作的方案，通过力交互实现有效载荷的精确操作，克服了传统人工引导的费力与风险。", "motivation": "在现有工业实践中，起重机有效载荷的精确操作和定位任务费力且具有风险，因为操作员需要手动引导有效载荷的精细运动。", "method": "所提出的协作方案中，起重机负责提升有效载荷，而机器人的末端执行器则负责引导其到达目标位置。机器人与起重机之间唯一的联系是引导有效载荷时产生的相互作用力。论文考虑了两种导纳传递函数以实现无害和平滑的接触：第一种用于机器人集成的基于位置的导纳控制；第二种通过处理交互力来生成起重机速度指令，使其跟随有效载荷，从而增加起重机的柔顺性。此外，还提出了一种设计导纳控制器的方法，以实现流畅的机器人-起重机协作。", "result": "通过仿真和实验验证了该方案的潜力。", "conclusion": "所提出的机器人与起重机协作方案能够有效提升有效载荷的精确操作能力，实现流畅、无害的引导，从而降低了工业操作的劳动强度和风险。"}}
{"id": "2508.07286", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.07286", "abs": "https://arxiv.org/abs/2508.07286", "authors": ["Jian Chen", "Jinbao Tian", "Yankui Li", "Zhou Li"], "title": "Arce: Augmented Roberta with Contextualized Elucidations for Ner in Automated Rule Checking", "comment": null, "summary": "Accurate information extraction from specialized texts is a critical\nchallenge, particularly for named entity recognition (NER) in the architecture,\nengineering, and construction (AEC) domain to support automated rule checking\n(ARC). The performance of standard pre-trained models is often constrained by\nthe domain gap, as they struggle to interpret the specialized terminology and\ncomplex relational contexts inherent in AEC texts. Although this issue can be\nmitigated by further pre-training on large, human-curated domain corpora, as\nexemplified by methods like ARCBERT, this approach is both labor-intensive and\ncost-prohibitive. Consequently, leveraging large language models (LLMs) for\nautomated knowledge generation has emerged as a promising alternative. However,\nthe optimal strategy for generating knowledge that can genuinely enhance\nsmaller, efficient models remains an open question. To address this, we propose\nARCE (augmented RoBERTa with contextualized elucidations), a novel approach\nthat systematically explores and optimizes this generation process. ARCE\nemploys an LLM to first generate a corpus of simple, direct explanations, which\nwe term Cote, and then uses this corpus to incrementally pre-train a RoBERTa\nmodel prior to its fine-tuning on the downstream task. Our extensive\nexperiments show that ARCE establishes a new state-of-the-art on a benchmark\nAEC dataset, achieving a Macro-F1 score of 77.20%. This result also reveals a\nkey finding: simple, explanation-based knowledge proves surprisingly more\neffective than complex, role-based rationales for this task. The code is\npublicly available at:https://github.com/nxcc-lab/ARCE.", "AI": {"tldr": "本文提出ARCE（增强型RoBERTa与语境阐释），一种新颖的方法，利用大型语言模型（LLM）生成简单解释（Cote）来预训练RoBERTa模型，显著提升了建筑、工程和施工（AEC）领域命名实体识别（NER）的性能，并在基准数据集上达到了新的最先进水平。", "motivation": "专业文本信息提取，特别是AEC领域的NER，面临巨大挑战，因为标准预训练模型难以理解专业术语和复杂关系，存在领域差距。虽然可以通过人工标注的大型领域语料进行进一步预训练（如ARCBERT）来缓解，但这耗时且成本高昂。利用LLM进行自动化知识生成是一个有前景的替代方案，但如何生成能有效增强小型高效模型的知识仍是未解决的问题。", "method": "本文提出了ARCE方法。首先，使用LLM生成一个包含简单、直接解释的语料库，称之为Cote。然后，利用这个Cote语料库对RoBERTa模型进行增量预训练，最后再在下游任务上进行微调。", "result": "ARCE在AEC基准数据集上取得了77.20%的Macro-F1分数，建立了新的最先进水平。研究结果还表明，对于此任务，基于简单解释的知识比基于复杂角色推理的知识更有效。", "conclusion": "ARCE通过利用LLM生成的简单解释进行模型预训练，有效解决了AEC领域NER的领域适应性问题，证明了这种策略在提升小型模型性能方面的显著效果和潜力。"}}
{"id": "2508.06874", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06874", "abs": "https://arxiv.org/abs/2508.06874", "authors": ["Shisheng Zhang", "Ramtin Gharleghi", "Sonit Singh", "Daniel Moses", "Dona Adikari", "Arcot Sowmya", "Susann Beier"], "title": "LWT-ARTERY-LABEL: A Lightweight Framework for Automated Coronary Artery Identification", "comment": null, "summary": "Coronary artery disease (CAD) remains the leading cause of death globally,\nwith computed tomography coronary angiography (CTCA) serving as a key\ndiagnostic tool. However, coronary arterial analysis using CTCA, such as\nidentifying artery-specific features from computational modelling, is\nlabour-intensive and time-consuming. Automated anatomical labelling of coronary\narteries offers a potential solution, yet the inherent anatomical variability\nof coronary trees presents a significant challenge. Traditional knowledge-based\nlabelling methods fall short in leveraging data-driven insights, while recent\ndeep-learning approaches often demand substantial computational resources and\noverlook critical clinical knowledge. To address these limitations, we propose\na lightweight method that integrates anatomical knowledge with rule-based\ntopology constraints for effective coronary artery labelling. Our approach\nachieves state-of-the-art performance on benchmark datasets, providing a\npromising alternative for automated coronary artery labelling.", "AI": {"tldr": "针对冠状动脉CT血管造影（CTCA）中冠状动脉自动解剖标记的挑战，本文提出了一种轻量级方法，结合解剖学知识和基于规则的拓扑约束，在基准数据集上取得了最先进的性能。", "motivation": "冠状动脉疾病（CAD）是全球主要死因，CTCA是重要诊断工具。然而，使用CTCA进行冠状动脉分析（如识别动脉特异性特征）耗时费力。自动标记面临解剖变异性挑战。传统方法缺乏数据驱动洞察，而深度学习方法计算资源需求大且忽视临床知识。", "method": "提出了一种轻量级方法，该方法整合了冠状动脉解剖学知识与基于规则的拓扑约束，用于有效的冠状动脉标记。", "result": "该方法在基准数据集上实现了最先进的性能。", "conclusion": "本研究为自动冠状动脉标记提供了一种有前景的替代方案。"}}
{"id": "2508.07602", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07602", "abs": "https://arxiv.org/abs/2508.07602", "authors": ["Wenpeng Xing", "Zhipeng Chen", "Changting Lin", "Meng Han"], "title": "HGMF: A Hierarchical Gaussian Mixture Framework for Scalable Tool Invocation within the Model Context Protocol", "comment": null, "summary": "Invoking external tools enables Large Language Models (LLMs) to perform\ncomplex, real-world tasks, yet selecting the correct tool from large,\nhierarchically-structured libraries remains a significant challenge. The\nlimited context windows of LLMs and noise from irrelevant options often lead to\nlow selection accuracy and high computational costs. To address this, we\npropose the Hierarchical Gaussian Mixture Framework (HGMF), a probabilistic\npruning method for scalable tool invocation. HGMF first maps the user query and\nall tool descriptions into a unified semantic space. The framework then\noperates in two stages: it clusters servers using a Gaussian Mixture Model\n(GMM) and filters them based on the query's likelihood. Subsequently, it\napplies the same GMM-based clustering and filtering to the tools associated\nwith the selected servers. This hierarchical process produces a compact,\nhigh-relevance candidate set, simplifying the final selection task for the LLM.\nExperiments on a public dataset show that HGMF significantly improves tool\nselection accuracy while reducing inference latency, confirming the framework's\nscalability and effectiveness for large-scale tool libraries.", "AI": {"tldr": "提出分层高斯混合框架（HGMF），通过概率剪枝方法，解决大型语言模型（LLMs）从大规模、分层工具库中选择工具时的准确性低和计算成本高的问题。", "motivation": "LLMs在从大型、分层结构的工具库中选择正确工具时面临挑战，主要原因是其有限的上下文窗口和来自无关选项的噪声，导致选择准确性低和计算成本高。", "method": "HGMF将用户查询和所有工具描述映射到统一的语义空间。该框架分两阶段操作：首先使用高斯混合模型（GMM）对服务器进行聚类并基于查询的可能性进行过滤；然后对选定服务器关联的工具应用相同的GMM聚类和过滤。这一分层过程生成一个紧凑、高相关性的候选集，简化了LLM的最终选择任务。", "result": "在公开数据集上的实验表明，HGMF显著提高了工具选择准确性，同时降低了推理延迟，证实了该框架对于大规模工具库的可扩展性和有效性。", "conclusion": "HGMF是一种可扩展且有效的解决方案，能够帮助LLMs从大规模工具库中高效准确地选择工具。"}}
{"id": "2508.07770", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07770", "abs": "https://arxiv.org/abs/2508.07770", "authors": ["Yizheng Zhang", "Zhenjun Yu", "Jiaxin Lai", "Cewu Lu", "Lei Han"], "title": "AgentWorld: An Interactive Simulation Platform for Scene Construction and Mobile Robotic Manipulation", "comment": "Accepted by Conference on Robot Learning 2025", "summary": "We introduce AgentWorld, an interactive simulation platform for developing\nhousehold mobile manipulation capabilities. Our platform combines automated\nscene construction that encompasses layout generation, semantic asset\nplacement, visual material configuration, and physics simulation, with a\ndual-mode teleoperation system supporting both wheeled bases and humanoid\nlocomotion policies for data collection. The resulting AgentWorld Dataset\ncaptures diverse tasks ranging from primitive actions (pick-and-place,\npush-pull, etc.) to multistage activities (serve drinks, heat up food, etc.)\nacross living rooms, bedrooms, and kitchens. Through extensive benchmarking of\nimitation learning methods including behavior cloning, action chunking\ntransformers, diffusion policies, and vision-language-action models, we\ndemonstrate the dataset's effectiveness for sim-to-real transfer. The\nintegrated system provides a comprehensive solution for scalable robotic skill\nacquisition in complex home environments, bridging the gap between\nsimulation-based training and real-world deployment. The code, datasets will be\navailable at https://yizhengzhang1.github.io/agent_world/", "AI": {"tldr": "AgentWorld是一个用于开发家庭移动操作能力的交互式模拟平台，它结合了自动化场景构建和双模遥操作系统，生成了包含多样化任务的数据集，并通过对模仿学习方法的基准测试，证明了其在模拟到真实迁移中的有效性。", "motivation": "在复杂的家庭环境中，机器人技能获取需要可扩展的解决方案，并需要弥合模拟训练与实际部署之间的差距。", "method": "该研究引入了AgentWorld平台，其特点包括：自动化场景构建（布局生成、语义资产放置、视觉材质配置、物理模拟）和支持轮式底座与人形运动策略的双模遥操作系统，用于数据收集。在此基础上，创建了AgentWorld数据集，涵盖从基本动作到多阶段活动的多种任务。最后，通过对行为克隆、动作分块Transformer、扩散策略和视觉-语言-动作模型等模仿学习方法进行广泛基准测试。", "result": "基准测试结果表明，AgentWorld数据集在模拟到真实迁移方面表现出有效性。", "conclusion": "AgentWorld提供了一个全面的解决方案，用于在复杂家庭环境中实现可扩展的机器人技能获取，有效连接了基于模拟的训练与实际部署。"}}
{"id": "2508.07295", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.07295", "abs": "https://arxiv.org/abs/2508.07295", "authors": ["Yexing Du", "Kaiyuan Liu", "Youcheng Pan", "Zheng Chu", "Bo Yang", "Xiaocheng Feng", "Yang Xiang", "Ming Liu"], "title": "CCFQA: A Benchmark for Cross-Lingual and Cross-Modal Speech and Text Factuality Evaluation", "comment": null, "summary": "As Large Language Models (LLMs) are increasingly popularized in the\nmultilingual world, ensuring hallucination-free factuality becomes markedly\ncrucial. However, existing benchmarks for evaluating the reliability of\nMultimodal Large Language Models (MLLMs) predominantly focus on textual or\nvisual modalities with a primary emphasis on English, which creates a gap in\nevaluation when processing multilingual input, especially in speech. To bridge\nthis gap, we propose a novel \\textbf{C}ross-lingual and \\textbf{C}ross-modal\n\\textbf{F}actuality benchmark (\\textbf{CCFQA}). Specifically, the CCFQA\nbenchmark contains parallel speech-text factual questions across 8 languages,\ndesigned to systematically evaluate MLLMs' cross-lingual and cross-modal\nfactuality capabilities. Our experimental results demonstrate that current\nMLLMs still face substantial challenges on the CCFQA benchmark. Furthermore, we\npropose a few-shot transfer learning strategy that effectively transfers the\nQuestion Answering (QA) capabilities of LLMs in English to multilingual Spoken\nQuestion Answering (SQA) tasks, achieving competitive performance with\nGPT-4o-mini-Audio using just 5-shot training. We release CCFQA as a\nfoundational research resource to promote the development of MLLMs with more\nrobust and reliable speech understanding capabilities. Our code and dataset are\navailable at https://github.com/yxduir/ccfqa.", "AI": {"tldr": "该研究提出了CCFQA基准，用于评估多模态大模型（MLLMs）在多语言和跨模态（特别是语音）环境下的事实准确性，发现现有模型表现不佳，并提出了一种有效的少样本迁移学习策略。", "motivation": "随着LLMs在多语言世界的普及，确保无幻觉的事实准确性变得至关重要。然而，现有的MLLM评估基准主要关注文本或视觉模态，且以英语为主，这在处理多语言输入（尤其是语音）时留下了评估空白。", "method": "提出了一个名为CCFQA（Cross-lingual and Cross-modal Factuality benchmark）的新型基准，包含8种语言的并行语音-文本事实问题，旨在系统评估MLLMs的跨语言和跨模态事实能力。此外，提出了一种少样本迁移学习策略，将LLMs在英语中的问答能力迁移到多语言语音问答（SQA）任务中。", "result": "实验结果表明，当前的MLLMs在CCFQA基准上仍面临巨大挑战。所提出的少样本迁移学习策略能有效地将英语LLMs的问答能力迁移到多语言SQA任务中，仅用5样本训练就达到了与GPT-4o-mini-Audio相当的性能。", "conclusion": "CCFQA基准被发布为一个基础研究资源，旨在促进开发具有更强大、更可靠语音理解能力的多模态大模型。"}}
{"id": "2508.06878", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06878", "abs": "https://arxiv.org/abs/2508.06878", "authors": ["Maoxun Yuan", "Duanni Meng", "Ziteng Xi", "Tianyi Zhao", "Shiji Zhao", "Yimian Dai", "Xingxing Wei"], "title": "NS-FPN: Improving Infrared Small Target Detection and Segmentation from Noise Suppression Perspective", "comment": null, "summary": "Infrared small target detection and segmentation (IRSTDS) is a critical yet\nchallenging task in defense and civilian applications, owing to the dim,\nshapeless appearance of targets and severe background clutter. Recent CNN-based\nmethods have achieved promising target perception results, but they only focus\non enhancing feature representation to offset the impact of noise, which\nresults in the increased false alarms problem. In this paper, through analyzing\nthe problem from the frequency domain, we pioneer in improving performance from\nnoise suppression perspective and propose a novel noise-suppression feature\npyramid network (NS-FPN), which integrates a low-frequency guided feature\npurification (LFP) module and a spiral-aware feature sampling (SFS) module into\nthe original FPN structure. The LFP module suppresses the noise features by\npurifying high-frequency components to achieve feature enhancement devoid of\nnoise interference, while the SFS module further adopts spiral sampling to fuse\ntarget-relevant features in feature fusion process. Our NS-FPN is designed to\nbe lightweight yet effective and can be easily plugged into existing IRSTDS\nframeworks. Extensive experiments on the public IRSTDS datasets demonstrate\nthat our method significantly reduces false alarms and achieves superior\nperformance on IRSTDS tasks.", "AI": {"tldr": "本文提出一种新颖的噪声抑制特征金字塔网络（NS-FPN），通过在频域抑制噪声来显著降低红外小目标检测与分割中的误报率，并提高性能。", "motivation": "红外小目标检测与分割（IRSTDS）因目标暗淡、无形且背景杂乱而极具挑战性。现有基于CNN的方法虽能提升目标感知，但仅侧重特征增强，未能有效抑制噪声，导致误报率增加。", "method": "本文从频域分析问题，提出NS-FPN，将低频引导特征净化（LFP）模块和螺旋感知特征采样（SFS）模块集成到FPN结构中。LFP模块通过净化高频分量来抑制噪声特征，实现无噪声干扰的特征增强；SFS模块则采用螺旋采样在特征融合过程中进一步融合与目标相关的特征。该网络轻量、高效且易于集成。", "result": "在公开IRSTDS数据集上的大量实验表明，所提出的方法显著降低了误报率，并在IRSTDS任务上取得了优越的性能。", "conclusion": "通过在频域从噪声抑制的角度改进特征表示和融合，NS-FPN有效解决了红外小目标检测与分割中误报率高的问题，显著提升了检测性能。"}}
{"id": "2508.07616", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07616", "abs": "https://arxiv.org/abs/2508.07616", "authors": ["Aswin RRV", "Jacob Dineen", "Divij Handa", "Md Nayem Uddin", "Mihir Parmar", "Chitta Baral", "Ben Zhou"], "title": "ThinkTuning: Instilling Cognitive Reflections without Distillation", "comment": "15 pages", "summary": "Recent advances in test-time scaling have led to the emergence of thinking\nLLMs that exhibit self-reflective behaviors and multi-step reasoning. While RL\ndrives this self-improvement paradigm, a recent study (Gandhi et al., 2025)\nshows that RL alone does not truly instill these new reasoning abilities - it\nmerely draws out behaviors already present in the base models. This raises a\nquestion: How can we train the models that don't exhibit such thinking behavior\nto develop it in the first place? To this end, we propose ThinkTuning, a\nGRPO-based interactive training approach where we augment the rollouts of a\nstudent model with the guidance from a teacher model. A simple idea from\nclassroom practice inspires our method: a teacher poses a problem, lets the\nstudent try an answer, then gives corrective feedback -- enough to point the\nmind in the right direction and then show the solution. Each piece of feedback\nreshapes the student's thoughts, leading them to arrive at the correct\nsolution. Similarly, we find that this type of implicit supervision through\nfeedback from a teacher model of the same size improves the reasoning\ncapabilities of the student model. In particular, on average, our method shows\na 3.85% improvement over zero-shot baselines across benchmarks, and on\nMATH-500, AIME and GPQA-Diamond it shows 2.08%, 2.23% and 3.99% improvements\nover the vanilla-GRPO baseline. Source code is available at\nhttps://github.com/3rdAT/ThinkTuning.", "AI": {"tldr": "提出ThinkTuning，一种基于GRPO的交互式训练方法，通过教师模型提供反馈来增强学生模型的推理能力，旨在从零开始培养大模型的思考行为。", "motivation": "现有研究表明，强化学习（RL）并不能真正灌输新的推理能力，而只是激发基础模型中已有的行为。因此，研究的动机是如何训练模型，使其从一开始就发展出思考行为。", "method": "提出ThinkTuning方法，这是一种基于GRPO（Guided Reinforcement Policy Optimization）的交互式训练方法。该方法通过教师模型对学生模型的推理过程提供指导性反馈来增强其输出。灵感来源于课堂教学：老师提出问题，学生尝试回答，然后老师提供纠正性反馈，引导学生找到正确答案。", "result": "该方法在各项基准测试中，平均比零样本基线提高了3.85%。在MATH-500、AIME和GPQA-Diamond数据集上，分别比香草GRPO基线提高了2.08%、2.23%和3.99%。", "conclusion": "通过教师模型提供隐式反馈的监督方式，可以有效提高学生模型的推理能力。"}}
{"id": "2508.07814", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07814", "abs": "https://arxiv.org/abs/2508.07814", "authors": ["Malaika Zafar", "Roohan Ahmed Khan", "Faryal Batool", "Yasheerah Yaqoot", "Ziang Guo", "Mikhail Litvinov", "Aleksey Fedoseev", "Dzmitry Tsetserukou"], "title": "SwarmVLM: VLM-Guided Impedance Control for Autonomous Navigation of Heterogeneous Robots in Dynamic Warehousing", "comment": null, "summary": "With the growing demand for efficient logistics, unmanned aerial vehicles\n(UAVs) are increasingly being paired with automated guided vehicles (AGVs).\nWhile UAVs offer the ability to navigate through dense environments and varying\naltitudes, they are limited by battery life, payload capacity, and flight\nduration, necessitating coordinated ground support.\n  Focusing on heterogeneous navigation, SwarmVLM addresses these limitations by\nenabling semantic collaboration between UAVs and ground robots through\nimpedance control. The system leverages the Vision Language Model (VLM) and the\nRetrieval-Augmented Generation (RAG) to adjust impedance control parameters in\nresponse to environmental changes. In this framework, the UAV acts as a leader\nusing Artificial Potential Field (APF) planning for real-time navigation, while\nthe ground robot follows via virtual impedance links with adaptive link\ntopology to avoid collisions with short obstacles.\n  The system demonstrated a 92% success rate across 12 real-world trials. Under\noptimal lighting conditions, the VLM-RAG framework achieved 8% accuracy in\nobject detection and selection of impedance parameters. The mobile robot\nprioritized short obstacle avoidance, occasionally resulting in a lateral\ndeviation of up to 50 cm from the UAV path, which showcases safe navigation in\na cluttered setting.", "AI": {"tldr": "该研究提出SwarmVLM系统，通过阻抗控制和VLM-RAG实现无人机与地面机器人的语义协作导航，以克服无人机限制并提高物流效率。", "motivation": "随着物流对效率需求的增长，无人机（UAV）与自动导引车（AGV）的结合日益增多。然而，无人机受电池寿命、有效载荷和飞行时间限制，需要地面支持。因此，需要一种系统来解决这些限制，实现异构导航中的高效协作。", "method": "SwarmVLM系统通过阻抗控制实现无人机与地面机器人之间的语义协作。它利用视觉语言模型（VLM）和检索增强生成（RAG）来根据环境变化调整阻抗控制参数。无人机作为领导者，使用人工势场（APF）进行实时导航规划；地面机器人通过虚拟阻抗链接跟随，并采用自适应链接拓扑来避开矮障碍物。", "result": "该系统在12次真实世界试验中达到了92%的成功率。在最佳光照条件下，VLM-RAG框架在物体检测和阻抗参数选择方面实现了8%的准确性。移动机器人优先避开矮障碍物，偶尔会导致与无人机路径产生高达50厘米的横向偏差，这表明其在杂乱环境中的安全导航能力。", "conclusion": "SwarmVLM系统通过结合VLM-RAG和阻抗控制，成功实现了无人机与地面机器人的高效、安全的异构导航协作，有效克服了无人机自身的局限性，并在复杂环境中展现了鲁棒的避障能力。"}}
{"id": "2508.07308", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07308", "abs": "https://arxiv.org/abs/2508.07308", "authors": ["Cristian Cosentino", "Annamaria Defilippo", "Marco Dossena", "Christopher Irwin", "Sara Joubbi", "Pietro Liò"], "title": "HealthBranches: Synthesizing Clinically-Grounded Question Answering Datasets via Decision Pathways", "comment": null, "summary": "HealthBranches is a novel benchmark dataset for medical Question-Answering\n(Q&A), specifically designed to evaluate complex reasoning in Large Language\nModels (LLMs). This dataset is generated through a semi-automated pipeline that\ntransforms explicit decision pathways from medical source into realistic\npatient cases with associated questions and answers. Covering 4,063 case\nstudies across 17 healthcare topics, each data point is based on clinically\nvalidated reasoning chains. HealthBranches supports both open-ended and\nmultiple-choice question formats and uniquely includes the full reasoning path\nfor each Q&A. Its structured design enables robust evaluation of LLMs'\nmulti-step inference capabilities, including their performance in structured\nRetrieval-Augmented Generation (RAG) contexts. HealthBranches establishes a\nfoundation for the development of more trustworthy, interpretable, and\nclinically reliable LLMs in high-stakes domains while also serving as a\nvaluable resource for educational purposes.", "AI": {"tldr": "HealthBranches是一个新颖的医疗问答基准数据集，旨在评估大型语言模型（LLMs）的复杂推理能力，包含4063个基于临床验证推理链的案例研究。", "motivation": "现有方法在评估LLMs在医疗领域复杂推理方面的能力不足，需要一个能够支持多步推理、可信赖且可解释的评估工具，以促进高风险领域LLMs的发展。", "method": "通过半自动化流程，将医疗来源中的明确决策路径转换为真实的患者案例、相关问题和答案。该数据集覆盖17个医疗主题，包含4063个案例，支持开放式和多项选择题，并独特地提供了每个问答的完整推理路径。", "result": "成功构建了HealthBranches数据集，该数据集能够对LLMs的多步推理能力进行稳健评估，包括在结构化检索增强生成（RAG）上下文中的表现。", "conclusion": "HealthBranches为开发更值得信赖、可解释和临床可靠的LLMs奠定了基础，同时也是一个有价值的教育资源。"}}
{"id": "2508.06891", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06891", "abs": "https://arxiv.org/abs/2508.06891", "authors": ["Melika Filvantorkaman", "Mohsen Piri", "Maral Filvan Torkaman", "Ashkan Zabihi", "Hamidreza Moradi"], "title": "Fusion-Based Brain Tumor Classification Using Deep Learning and Explainable AI, and Rule-Based Reasoning", "comment": "37 pages, 6 figures", "summary": "Accurate and interpretable classification of brain tumors from magnetic\nresonance imaging (MRI) is critical for effective diagnosis and treatment\nplanning. This study presents an ensemble-based deep learning framework that\ncombines MobileNetV2 and DenseNet121 convolutional neural networks (CNNs) using\na soft voting strategy to classify three common brain tumor types: glioma,\nmeningioma, and pituitary adenoma. The models were trained and evaluated on the\nFigshare dataset using a stratified 5-fold cross-validation protocol. To\nenhance transparency and clinical trust, the framework integrates an\nExplainable AI (XAI) module employing Grad-CAM++ for class-specific saliency\nvisualization, alongside a symbolic Clinical Decision Rule Overlay (CDRO) that\nmaps predictions to established radiological heuristics. The ensemble\nclassifier achieved superior performance compared to individual CNNs, with an\naccuracy of 91.7%, precision of 91.9%, recall of 91.7%, and F1-score of 91.6%.\nGrad-CAM++ visualizations revealed strong spatial alignment between model\nattention and expert-annotated tumor regions, supported by Dice coefficients up\nto 0.88 and IoU scores up to 0.78. Clinical rule activation further validated\nmodel predictions in cases with distinct morphological features. A\nhuman-centered interpretability assessment involving five board-certified\nradiologists yielded high Likert-scale scores for both explanation usefulness\n(mean = 4.4) and heatmap-region correspondence (mean = 4.0), reinforcing the\nframework's clinical relevance. Overall, the proposed approach offers a robust,\ninterpretable, and generalizable solution for automated brain tumor\nclassification, advancing the integration of deep learning into clinical\nneurodiagnostics.", "AI": {"tldr": "该研究提出了一个基于集成深度学习的框架，结合MobileNetV2和DenseNet121，通过软投票策略对三种常见脑肿瘤进行分类，并集成了可解释AI（XAI）模块和临床决策规则叠加（CDRO），以提高诊断的准确性和可信度。", "motivation": "从磁共振成像（MRI）中对脑肿瘤进行准确且可解释的分类对于有效的诊断和治疗计划至关重要。现有方法可能缺乏透明度，难以获得临床信任。", "method": "该框架采用MobileNetV2和DenseNet121卷积神经网络的集成模型，通过软投票策略进行脑肿瘤分类。模型在Figshare数据集上使用分层5折交叉验证协议进行训练和评估。为增强透明度，集成了使用Grad-CAM++的解释性AI（XAI）模块进行类特异性显著性可视化，并结合了将预测映射到放射学启发式规则的符号化临床决策规则叠加（CDRO）。", "result": "集成分类器性能优于单个CNN，准确率达到91.7%，精确率91.9%，召回率91.7%，F1分数91.6%。Grad-CAM++可视化显示模型注意力与专家标注肿瘤区域高度空间对齐（Dice系数高达0.88，IoU得分高达0.78）。临床规则激活进一步验证了模型预测。五位放射科医生对解释有用性（平均4.4分）和热图区域对应性（平均4.0分）给予高Likert量表评分。", "conclusion": "所提出的方法为自动化脑肿瘤分类提供了一个鲁棒、可解释且可推广的解决方案，推动了深度学习在临床神经诊断中的应用集成。"}}
{"id": "2508.07628", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07628", "abs": "https://arxiv.org/abs/2508.07628", "authors": ["Daniel Essien", "Suresh Neethirajan"], "title": "Multimodal AI Systems for Enhanced Laying Hen Welfare Assessment and Productivity Optimization", "comment": "66 pages, 7 figures, 11 tables", "summary": "The future of poultry production depends on a paradigm shift replacing\nsubjective, labor-intensive welfare checks with data-driven, intelligent\nmonitoring ecosystems. Traditional welfare assessments-limited by human\nobservation and single-sensor data-cannot fully capture the complex,\nmultidimensional nature of laying hen welfare in modern farms. Multimodal\nArtificial Intelligence (AI) offers a breakthrough, integrating visual,\nacoustic, environmental, and physiological data streams to reveal deeper\ninsights into avian welfare dynamics. This investigation highlights multimodal\nAs transformative potential, showing that intermediate (feature-level) fusion\nstrategies achieve the best balance between robustness and performance under\nreal-world poultry conditions, and offer greater scalability than early or late\nfusion approaches. Key adoption barriers include sensor fragility in harsh farm\nenvironments, high deployment costs, inconsistent behavioral definitions, and\nlimited cross-farm generalizability. To address these, we introduce two novel\nevaluation tools - the Domain Transfer Score (DTS) to measure model\nadaptability across diverse farm settings, and the Data Reliability Index (DRI)\nto assess sensor data quality under operational constraints. We also propose a\nmodular, context-aware deployment framework designed for laying hen\nenvironments, enabling scalable and practical integration of multimodal\nsensing. This work lays the foundation for a transition from reactive, unimodal\nmonitoring to proactive, precision-driven welfare systems that unite\nproductivity with ethical, science based animal care.", "AI": {"tldr": "旨在用多模态AI和数据驱动的智能系统取代传统主观的蛋鸡福利评估，并解决部署挑战。", "motivation": "传统蛋鸡福利评估主观、劳动密集、数据有限，无法全面捕捉复杂的福利状况，需要更智能、数据驱动的监测生态系统。", "method": "提出多模态AI（整合视觉、听觉、环境、生理数据）；研究融合策略，发现中间（特征级）融合最佳；引入域迁移分数（DTS）评估模型适应性，数据可靠性指数（DRI）评估传感器数据质量；提出模块化、上下文感知的部署框架。", "result": "多模态AI具有变革潜力；中间融合策略在真实禽类环境中表现出最佳的鲁棒性和性能平衡，并具可扩展性；DTS和DRI工具能解决跨农场泛化性和数据质量问题。", "conclusion": "为从被动式、单模态监测转向主动式、精准驱动的福利系统奠定了基础，将生产力与基于科学的道德动物护理相结合。"}}
{"id": "2508.07839", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07839", "abs": "https://arxiv.org/abs/2508.07839", "authors": ["Qiaoqiao Ren", "Tony Belpaeme"], "title": "Touch Speaks, Sound Feels: A Multimodal Approach to Affective and Social Touch from Robots to Humans", "comment": null, "summary": "Affective tactile interaction constitutes a fundamental component of human\ncommunication. In natural human-human encounters, touch is seldom experienced\nin isolation; rather, it is inherently multisensory. Individuals not only\nperceive the physical sensation of touch but also register the accompanying\nauditory cues generated through contact. The integration of haptic and auditory\ninformation forms a rich and nuanced channel for emotional expression. While\nextensive research has examined how robots convey emotions through facial\nexpressions and speech, their capacity to communicate social gestures and\nemotions via touch remains largely underexplored. To address this gap, we\ndeveloped a multimodal interaction system incorporating a 5*5 grid of 25\nvibration motors synchronized with audio playback, enabling robots to deliver\ncombined haptic-audio stimuli. In an experiment involving 32 Chinese\nparticipants, ten emotions and six social gestures were presented through\nvibration, sound, or their combination. Participants rated each stimulus on\narousal and valence scales. The results revealed that (1) the combined\nhaptic-audio modality significantly enhanced decoding accuracy compared to\nsingle modalities; (2) each individual channel-vibration or sound-effectively\nsupported certain emotions recognition, with distinct advantages depending on\nthe emotional expression; and (3) gestures alone were generally insufficient\nfor conveying clearly distinguishable emotions. These findings underscore the\nimportance of multisensory integration in affective human-robot interaction and\nhighlight the complementary roles of haptic and auditory cues in enhancing\nemotional communication.", "AI": {"tldr": "该研究开发了一种多模态触觉-听觉系统，使机器人能通过结合振动和声音来表达情感和社交姿态，实验证明多感官融合显著提升了情感识别准确性。", "motivation": "人类交流中触觉通常是多感官的，并与听觉线索结合形成丰富的情感表达渠道。然而，机器人通过触摸传达社交姿态和情感的能力仍未被充分探索，现有研究多关注面部表情和语音。", "method": "开发了一个包含25个振动马达（5x5网格）并与音频播放同步的多模态交互系统。在实验中，向32名中国参与者展示了10种情感和6种社交姿态，通过振动、声音或两者结合的方式呈现。参与者对每个刺激在唤醒度和效价量表上进行评分。", "result": "结果显示：1) 触觉-听觉结合模式显著提高了情感解码准确性；2) 振动或声音的单一通道都能有效支持某些情感识别，且在不同情感表达上具有各自优势；3) 单独的姿态不足以清晰地传达可区分的情感。", "conclusion": "这些发现强调了多感官整合在情感人机交互中的重要性，并突出了触觉和听觉线索在增强情感交流中的互补作用。"}}
{"id": "2508.07321", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.07321", "abs": "https://arxiv.org/abs/2508.07321", "authors": ["Shubhra Ghosh", "Abhilekh Borah", "Aditya Kumar Guru", "Kripabandhu Ghosh"], "title": "ObfusQAte: A Proposed Framework to Evaluate LLM Robustness on Obfuscated Factual Question Answering", "comment": null, "summary": "The rapid proliferation of Large Language Models (LLMs) has significantly\ncontributed to the development of equitable AI systems capable of factual\nquestion-answering (QA). However, no known study tests the LLMs' robustness\nwhen presented with obfuscated versions of questions. To systematically\nevaluate these limitations, we propose a novel technique, ObfusQAte and,\nleveraging the same, introduce ObfusQA, a comprehensive, first of its kind,\nframework with multi-tiered obfuscation levels designed to examine LLM\ncapabilities across three distinct dimensions: (i) Named-Entity Indirection,\n(ii) Distractor Indirection, and (iii) Contextual Overload. By capturing these\nfine-grained distinctions in language, ObfusQA provides a comprehensive\nbenchmark for evaluating LLM robustness and adaptability. Our study observes\nthat LLMs exhibit a tendency to fail or generate hallucinated responses when\nconfronted with these increasingly nuanced variations. To foster research in\nthis direction, we make ObfusQAte publicly available.", "AI": {"tldr": "本研究提出ObfusQAte技术和ObfusQA框架，用于系统评估大型语言模型（LLMs）在面对模糊化问题时的鲁棒性，发现LLMs在此类情况下容易失败或产生幻觉。", "motivation": "尽管LLMs在事实问答方面表现出色，但目前缺乏对其在处理模糊化或间接性问题时鲁棒性的系统评估，这限制了其在公平AI系统中的应用。", "method": "研究引入了ObfusQAte技术，并基于此构建了ObfusQA框架。该框架包含多层次的模糊化，从三个维度评估LLMs的能力：命名实体间接性、干扰项间接性、上下文过载。", "result": "研究发现，当LLMs面对日益微妙的模糊化问题变体时，它们倾向于失败或生成幻觉性回答。", "conclusion": "ObfusQA提供了一个评估LLM鲁棒性和适应性的全面基准。为了促进相关研究，ObfusQAte工具已公开发布，以鼓励对LLMs在复杂语言理解方面局限性的进一步探索。"}}
{"id": "2508.06895", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06895", "abs": "https://arxiv.org/abs/2508.06895", "authors": ["Jianting Tang", "Yubo Wang", "Haoyu Cao", "Linli Xu"], "title": "BASIC: Boosting Visual Alignment with Intrinsic Refined Embeddings in Multimodal Large Language Models", "comment": "Accepted to ICCV 2025", "summary": "Mainstream Multimodal Large Language Models (MLLMs) achieve visual\nunderstanding by using a vision projector to bridge well-pretrained vision\nencoders and large language models (LLMs). The inherent gap between visual and\ntextual modalities makes the embeddings from the vision projector critical for\nvisual comprehension. However, current alignment approaches treat visual\nembeddings as contextual cues and merely apply auto-regressive supervision to\ntextual outputs, neglecting the necessity of introducing equivalent direct\nvisual supervision, which hinders the potential finer alignment of visual\nembeddings. In this paper, based on our analysis of the refinement process of\nvisual embeddings in the LLM's shallow layers, we propose BASIC, a method that\nutilizes refined visual embeddings within the LLM as supervision to directly\nguide the projector in generating initial visual embeddings. Specifically, the\nguidance is conducted from two perspectives: (i) optimizing embedding\ndirections by reducing angles between initial and supervisory embeddings in\nsemantic space; (ii) improving semantic matching by minimizing disparities\nbetween the logit distributions of both visual embeddings. Without additional\nsupervisory models or artificial annotations, BASIC significantly improves the\nperformance of MLLMs across a wide range of benchmarks, demonstrating the\neffectiveness of our introduced direct visual supervision.", "AI": {"tldr": "该论文提出了BASIC方法，通过利用LLM浅层中精炼的视觉嵌入作为监督，直接指导视觉投影器生成初始视觉嵌入，从而弥补了现有MLLM中缺乏直接视觉监督的问题，显著提升了模型性能。", "motivation": "主流多模态大语言模型（MLLMs）通过视觉投影器连接视觉编码器和LLMs，但当前的对齐方法将视觉嵌入仅视为上下文线索，并仅对文本输出应用自回归监督，忽略了引入等效直接视觉监督的必要性，这阻碍了视觉嵌入更精细的对齐。", "method": "该论文提出了BASIC方法，基于对LLM浅层中视觉嵌入细化过程的分析，利用LLM内部精炼的视觉嵌入作为监督信号，直接指导投影器生成初始视觉嵌入。具体指导包括两个方面：1) 通过减少初始嵌入和监督嵌入在语义空间中的角度来优化嵌入方向；2) 通过最小化两种视觉嵌入的logit分布差异来改善语义匹配。", "result": "BASIC方法在不依赖额外监督模型或人工标注的情况下，显著提升了MLLMs在广泛基准测试上的性能。", "conclusion": "该研究证明了所引入的直接视觉监督方法的有效性，能够实现视觉嵌入更精细的对齐，从而显著提升多模态大语言模型的视觉理解能力。"}}
{"id": "2508.07642", "categories": ["cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07642", "abs": "https://arxiv.org/abs/2508.07642", "authors": ["Tianyi Ma", "Yue Zhang", "Zehao Wang", "Parisa Kordjamshidi"], "title": "Breaking Down and Building Up: Mixture of Skill-Based Vision-and-Language Navigation Agents", "comment": "18 pages, 5 Figures,", "summary": "Vision-and-Language Navigation (VLN) poses significant challenges in enabling\nagents to interpret natural language instructions and navigate complex 3D\nenvironments. While recent progress has been driven by large-scale pre-training\nand data augmentation, current methods still struggle to generalize to unseen\nscenarios, particularly when complex spatial and temporal reasoning is\nrequired. In this work, we propose SkillNav, a modular framework that\nintroduces structured, skill-based reasoning into Transformer-based VLN agents.\nOur method decomposes navigation into a set of interpretable atomic skills\n(e.g., Vertical Movement, Area and Region Identification, Stop and Pause), each\nhandled by a specialized agent. We then introduce a novel zero-shot\nVision-Language Model (VLM)-based router, which dynamically selects the most\nsuitable agent at each time step by aligning sub-goals with visual observations\nand historical actions. SkillNav achieves a new state-of-the-art performance on\nthe R2R benchmark and demonstrates strong generalization to the GSA-R2R\nbenchmark that includes novel instruction styles and unseen environments.", "AI": {"tldr": "SkillNav是一个模块化框架，通过引入基于技能的推理和零样本VLM路由器，显著提升了视觉-语言导航（VLN）在复杂3D环境中的泛化能力，并在R2R和GSA-R2R基准测试中达到了新的SOTA。", "motivation": "现有VLN方法在解释自然语言指令和在复杂3D环境中导航时面临挑战，尤其是在需要复杂空间和时间推理时，难以泛化到未见场景。", "method": "提出SkillNav，一个模块化框架，将导航分解为可解释的原子技能（如垂直移动、区域识别、停止等），每个技能由专门的智能体处理。引入创新的零样本视觉-语言模型（VLM）路由器，通过将子目标与视觉观察和历史动作对齐，动态选择每个时间步最合适的智能体。", "result": "SkillNav在R2R基准测试上实现了新的最先进性能，并在包含新指令风格和未见环境的GSA-R2R基准测试中展现出强大的泛化能力。", "conclusion": "SkillNav通过结构化、基于技能的推理和动态VLM路由器，有效解决了VLN的泛化问题，显著提升了智能体在复杂环境中的导航能力。"}}
{"id": "2508.07842", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07842", "abs": "https://arxiv.org/abs/2508.07842", "authors": ["Yutong Shen", "Hangxu Liu", "Penghui Liu", "Ruizhe Xia", "Tianyi Yao", "Yitong Sun", "Tongtong Feng"], "title": "DETACH: Cross-domain Learning for Long-Horizon Tasks via Mixture of Disentangled Experts", "comment": "14 pages,8 figures. Submitted to AAAI'26", "summary": "Long-Horizon (LH) tasks in Human-Scene Interaction (HSI) are complex\nmulti-step tasks that require continuous planning, sequential decision-making,\nand extended execution across domains to achieve the final goal. However,\nexisting methods heavily rely on skill chaining by concatenating pre-trained\nsubtasks, with environment observations and self-state tightly coupled, lacking\nthe ability to generalize to new combinations of environments and skills,\nfailing to complete various LH tasks across domains. To solve this problem,\nthis paper presents DETACH, a cross-domain learning framework for LH tasks via\nbiologically inspired dual-stream disentanglement. Inspired by the brain's\n\"where-what\" dual pathway mechanism, DETACH comprises two core modules: i) an\nenvironment learning module for spatial understanding, which captures object\nfunctions, spatial relationships, and scene semantics, achieving cross-domain\ntransfer through complete environment-self disentanglement; ii) a skill\nlearning module for task execution, which processes self-state information\nincluding joint degrees of freedom and motor patterns, enabling cross-skill\ntransfer through independent motor pattern encoding. We conducted extensive\nexperiments on various LH tasks in HSI scenes. Compared with existing methods,\nDETACH can achieve an average subtasks success rate improvement of 23% and\naverage execution efficiency improvement of 29%.", "AI": {"tldr": "本文提出DETACH框架，通过受生物学启发的双流解耦机制，解决了人-场景交互中长周期任务的跨域泛化问题，显著提升了子任务成功率和执行效率。", "motivation": "现有方法在人-场景交互的长周期任务中，过度依赖预训练子任务的串联，环境观察与自我状态紧密耦合，导致无法泛化到新的环境和技能组合，难以完成跨域的长周期任务。", "method": "本文提出了DETACH框架，借鉴大脑的“何处-何物”双通路机制，包含两个核心模块：1) 环境学习模块，用于空间理解，通过完全解耦环境与自我状态实现跨域迁移；2) 技能学习模块，用于任务执行，通过独立编码运动模式实现跨技能迁移。", "result": "与现有方法相比，DETACH在人-场景交互的长周期任务中，平均子任务成功率提高了23%，平均执行效率提高了29%。", "conclusion": "DETACH通过双流解耦的环境和技能学习，有效解决了长周期人-场景交互任务的跨域泛化问题，显著提升了任务完成度和效率。"}}
{"id": "2508.07325", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07325", "abs": "https://arxiv.org/abs/2508.07325", "authors": ["Dean Geckt", "Melinda Fricke", "Shuly Wintner"], "title": "Strategies of Code-switching in Human-Machine Dialogs", "comment": null, "summary": "Most people are multilingual, and most multilinguals code-switch, yet the\ncharacteristics of code-switched language are not fully understood. We\ndeveloped a chatbot capable of completing a Map Task with human participants\nusing code-switched Spanish and English. In two experiments, we prompted the\nbot to code-switch according to different strategies, examining (1) the\nfeasibility of such experiments for investigating bilingual language use, and\n(2) whether participants would be sensitive to variations in discourse and\ngrammatical patterns. Participants generally enjoyed code-switching with our\nbot as long as it produced predictable code-switching behavior; when\ncode-switching was random or ungrammatical (as when producing unattested\nincongruent mixed-language noun phrases, such as `la fork'), participants\nenjoyed the task less and were less successful at completing it. These results\nunderscore the potential downsides of deploying insufficiently developed\nmultilingual language technology, while also illustrating the promise of such\ntechnology for conducting research on bilingual language use.", "AI": {"tldr": "研究人员开发了一个能进行西英语码转换的聊天机器人，发现用户更喜欢可预测且语法正确的码转换行为，揭示了多语言技术在双语研究中的潜力和不足。", "motivation": "大多数人是多语言使用者并进行语码转换，但对语码转换语言的特征尚未完全理解。需要工具来研究双语语言使用。", "method": "开发了一个能够与人类参与者进行西语和英语语码转换“地图任务”的聊天机器人。在两项实验中，机器人被设定为采用不同的语码转换策略（可预测、随机、不合语法），以探究此类实验的可行性以及参与者是否对语篇和语法模式变化敏感。", "result": "参与者普遍喜欢与机器人进行语码转换，只要其行为可预测。当语码转换随机或不合语法（如生成“la fork”等不规范的混合语言名词短语）时，参与者享受度降低，任务完成成功率也较低。", "conclusion": "研究结果强调了部署不成熟的多语言语言技术可能带来的负面影响，同时也展示了此类技术在进行双语语言使用研究方面的巨大潜力。"}}
{"id": "2508.06900", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06900", "abs": "https://arxiv.org/abs/2508.06900", "authors": ["Weiran Chen", "Guiqian Zhu", "Ying Li", "Yi Ji", "Chunping Liu"], "title": "Advancements in Chinese font generation since deep learning era: A survey", "comment": "42 Pages, 25 figures", "summary": "Chinese font generation aims to create a new Chinese font library based on\nsome reference samples. It is a topic of great concern to many font designers\nand typographers. Over the past years, with the rapid development of deep\nlearning algorithms, various new techniques have achieved flourishing and\nthriving progress. Nevertheless, how to improve the overall quality of\ngenerated Chinese character images remains a tough issue. In this paper, we\nconduct a holistic survey of the recent Chinese font generation approaches\nbased on deep learning. To be specific, we first illustrate the research\nbackground of the task. Then, we outline our literature selection and analysis\nmethodology, and review a series of related fundamentals, including classical\ndeep learning architectures, font representation formats, public datasets, and\nfrequently-used evaluation metrics. After that, relying on the number of\nreference samples required to generate a new font, we categorize the existing\nmethods into two major groups: many-shot font generation and few-shot font\ngeneration methods. Within each category, representative approaches are\nsummarized, and their strengths and limitations are also discussed in detail.\nFinally, we conclude our paper with the challenges and future directions, with\nthe expectation to provide some valuable illuminations for the researchers in\nthis field.", "AI": {"tldr": "本文对基于深度学习的中文字体生成方法进行了全面综述，旨在总结现有进展并指出未来方向。", "motivation": "中文字体生成是字体设计师和排版师关注的重要课题。尽管深度学习算法发展迅速，但如何提高生成中文字符图像的整体质量仍是一个难题。", "method": "本文首先阐述了研究背景，然后概述了文献选择和分析方法，并回顾了相关基础知识，包括经典深度学习架构、字体表示格式、公共数据集和常用评估指标。随后，根据生成新字体所需的参考样本数量，将现有方法分为多样本字体生成和少样本字体生成两大类，并对每类中的代表性方法进行了总结，讨论了其优缺点。", "result": "本文系统地分类并总结了基于深度学习的中文字体生成方法，分为多样本和少样本两大类，并详细讨论了各代表性方法的优势与局限性。", "conclusion": "本文最后总结了中文字体生成领域面临的挑战，并展望了未来的研究方向，旨在为该领域的研究人员提供有价值的启示。"}}
{"id": "2508.07649", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07649", "abs": "https://arxiv.org/abs/2508.07649", "authors": ["Jie Li", "Haoye Dong", "Zhengyang Wu", "Zetao Zheng", "Mingrong Lin"], "title": "Disentangling Multiplex Spatial-Temporal Transition Graph Representation Learning for Socially Enhanced POI Recommendation", "comment": null, "summary": "Next Point-of-Interest (POI) recommendation is a research hotspot in business\nintelligence, where users' spatial-temporal transitions and social\nrelationships play key roles. However, most existing works model spatial and\ntemporal transitions separately, leading to misaligned representations of the\nsame spatial-temporal key nodes. This misalignment introduces redundant\ninformation during fusion, increasing model uncertainty and reducing\ninterpretability. To address this issue, we propose DiMuST, a socially enhanced\nPOI recommendation model based on disentangled representation learning over\nmultiplex spatial-temporal transition graphs. The model employs a novel\nDisentangled variational multiplex graph Auto-Encoder (DAE), which first\ndisentangles shared and private distributions using a multiplex\nspatial-temporal graph strategy. It then fuses the shared features via a\nProduct of Experts (PoE) mechanism and denoises the private features through\ncontrastive constraints. The model effectively captures the spatial-temporal\ntransition representations of POIs while preserving the intrinsic correlation\nof their spatial-temporal relationships. Experiments on two challenging\ndatasets demonstrate that our DiMuST significantly outperforms existing methods\nacross multiple metrics.", "AI": {"tldr": "DiMuST是一种基于解耦表示学习的社交增强型POI推荐模型，通过新颖的DAE处理多路时空转换图，有效解决现有方法中时空表示错位导致的冗余和不确定性问题，显著优于现有方法。", "motivation": "现有POI推荐模型大多独立建模空间和时间转换，导致相同时空关键节点表示错位，融合时引入冗余信息，增加模型不确定性并降低可解释性。", "method": "提出DiMuST模型，基于多路时空转换图上的解耦表示学习。核心是新颖的解耦变分多路图自动编码器（DAE），首先利用多路时空图策略解耦共享和私有分布，然后通过专家乘积（PoE）机制融合共享特征，并通过对比约束去噪私有特征。", "result": "在两个挑战性数据集上的实验表明，DiMuST在多项指标上显著优于现有方法。", "conclusion": "DiMuST模型有效捕获了POI的时空转换表示，同时保留了其时空关系的内在关联性，解决了现有方法的表示错位问题，提高了推荐性能。"}}
{"id": "2508.07917", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07917", "abs": "https://arxiv.org/abs/2508.07917", "authors": ["Jason Lee", "Jiafei Duan", "Haoquan Fang", "Yuquan Deng", "Shuo Liu", "Boyang Li", "Bohan Fang", "Jieyu Zhang", "Yi Ru Wang", "Sangho Lee", "Winson Han", "Wilbert Pumacay", "Angelica Wu", "Rose Hendrix", "Karen Farley", "Eli VanderBilt", "Ali Farhadi", "Dieter Fox", "Ranjay Krishna"], "title": "MolmoAct: Action Reasoning Models that can Reason in Space", "comment": "Appendix on Blogpost: https://allenai.org/blog/molmoact", "summary": "Reasoning is central to purposeful action, yet most robotic foundation models\nmap perception and instructions directly to control, which limits adaptability,\ngeneralization, and semantic grounding. We introduce Action Reasoning Models\n(ARMs), a class of vision-language-action models that integrate perception,\nplanning, and control through a structured three-stage pipeline. Our model,\nMolmoAct, encodes observations and instructions into depth-aware perception\ntokens, generates mid-level spatial plans as editable trajectory traces, and\npredicts precise low-level actions, enabling explainable and steerable\nbehavior. MolmoAct-7B-D achieves strong performance across simulation and\nreal-world settings: 70.5% zero-shot accuracy on SimplerEnv Visual Matching\ntasks, surpassing closed-source Pi-0 and GR00T N1; 86.6% average success on\nLIBERO, including an additional 6.3% gain over ThinkAct on long-horizon tasks;\nand in real-world fine-tuning, an additional 10% (single-arm) and an additional\n22.7% (bimanual) task progression over Pi-0-FAST. It also outperforms baselines\nby an additional 23.3% on out-of-distribution generalization and achieves top\nhuman-preference scores for open-ended instruction following and trajectory\nsteering. Furthermore, we release, for the first time, the MolmoAct Dataset --\na mid-training robot dataset comprising over 10,000 high quality robot\ntrajectories across diverse scenarios and tasks. Training with this dataset\nyields an average 5.5% improvement in general performance over the base model.\nWe release all model weights, training code, our collected dataset, and our\naction reasoning dataset, establishing MolmoAct as both a state-of-the-art\nrobotics foundation model and an open blueprint for building ARMs that\ntransform perception into purposeful action through structured reasoning.\nBlogpost: https://allenai.org/blog/molmoact", "AI": {"tldr": "本文提出了Action Reasoning Models (ARMs)，特别是MolmoAct模型，它通过三阶段流水线（感知、规划、控制）整合视觉-语言-动作，实现了可解释和可控的机器人行为，并在模拟和真实世界任务中取得了最先进的性能，同时发布了MolmoAct数据集。", "motivation": "现有大多数机器人基础模型将感知和指令直接映射到控制，这限制了其适应性、泛化能力和语义接地。因此，需要一种能够整合推理能力的模型来克服这些局限。", "method": "引入Action Reasoning Models (ARMs)，具体实现为MolmoAct。该模型采用结构化的三阶段流水线：1) 将观察和指令编码为深度感知令牌；2) 生成可编辑的轨迹痕迹作为中层空间规划；3) 预测精确的低层动作。此外，本文还首次发布了MolmoAct数据集，包含超过10,000条高质量机器人轨迹，用于模型训练。", "result": "MolmoAct-7B-D在SimplerEnv视觉匹配任务上达到70.5%的零样本准确率，超越Pi-0和GR00T N1；在LIBERO上平均成功率为86.6%，长时任务比ThinkAct提升6.3%；在真实世界微调中，单臂任务比Pi-0-FAST提升10%，双臂任务提升22.7%。在域外泛化上比基线提升23.3%，并在开放式指令遵循和轨迹引导方面获得最高的人类偏好评分。使用MolmoAct数据集训练使模型整体性能平均提升5.5%。", "conclusion": "MolmoAct不仅是一个最先进的机器人基础模型，也是构建ARMs的开放蓝图，通过结构化推理将感知转化为有目的的行动。所有模型权重、训练代码和数据集均已发布，推动了机器人领域的发展。"}}
{"id": "2508.07375", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.07375", "abs": "https://arxiv.org/abs/2508.07375", "authors": ["Wenqian Cui", "Lei Zhu", "Xiaohui Li", "Zhihan Guo", "Haoli Bai", "Lu Hou", "Irwin King"], "title": "Think Before You Talk: Enhancing Meaningful Dialogue Generation in Full-Duplex Speech Language Models with Planning-Inspired Text Guidance", "comment": "Work in progress", "summary": "Full-Duplex Speech Language Models (FD-SLMs) are specialized foundation\nmodels designed to enable natural, real-time spoken interactions by modeling\ncomplex conversational dynamics such as interruptions, backchannels, and\noverlapping speech, and End-to-end (e2e) FD-SLMs leverage real-world\ndouble-channel conversational data to capture nuanced two-speaker dialogue\npatterns for human-like interactions. However, they face a critical challenge\n-- their conversational abilities often degrade compared to pure-text\nconversation due to prolonged speech sequences and limited high-quality spoken\ndialogue data. While text-guided speech generation could mitigate these issues,\nit suffers from timing and length issues when integrating textual guidance into\ndouble-channel audio streams, disrupting the precise time alignment essential\nfor natural interactions. To address these challenges, we propose TurnGuide, a\nnovel planning-inspired approach that mimics human conversational planning by\ndynamically segmenting assistant speech into dialogue turns and generating\nturn-level text guidance before speech output, which effectively resolves both\ninsertion timing and length challenges. Extensive experiments demonstrate our\napproach significantly improves e2e FD-SLMs' conversational abilities, enabling\nthem to generate semantically meaningful and coherent speech while maintaining\nnatural conversational flow. Demos are available at\nhttps://dreamtheater123.github.io/TurnGuide-Demo/. Code will be available at\nhttps://github.com/dreamtheater123/TurnGuide.", "AI": {"tldr": "本文提出TurnGuide，一种规划启发式方法，通过在语音输出前动态分割助手语音并生成轮次级文本指导，显著提升了端到端全双工语音语言模型（e2e FD-SLMs）的对话能力，解决了长语音序列和数据限制导致的对话质量下降问题。", "motivation": "端到端全双工语音语言模型（e2e FD-SLMs）在处理长时间语音序列和高质量对话数据有限时，其对话能力相比纯文本对话会下降。现有文本引导的语音生成方法在集成到双通道音频流时存在时序和长度问题，破坏了自然交互所需的精确时间对齐。", "method": "提出TurnGuide方法，模仿人类对话规划，动态地将助手语音分割成对话轮次，并在语音输出前生成轮次级的文本指导。此方法有效解决了文本插入的时序和长度挑战。", "result": "实验证明，TurnGuide显著提高了e2e FD-SLMs的对话能力，使其能够生成语义有意义且连贯的语音，同时保持自然的对话流畅性。", "conclusion": "TurnGuide通过创新的轮次级文本指导策略，成功克服了全双工语音语言模型在实时对话中面临的挑战，实现了更自然、更连贯的人机语音交互。"}}
{"id": "2508.06902", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06902", "abs": "https://arxiv.org/abs/2508.06902", "authors": ["Xuecheng Wu", "Dingkang Yang", "Danlei Huang", "Xinyi Yin", "Yifan Wang", "Jia Zhang", "Jiayu Nie", "Liangyu Fu", "Yang Liu", "Junxiao Xue", "Hadi Amirpour", "Wei Zhou"], "title": "eMotions: A Large-Scale Dataset and Audio-Visual Fusion Network for Emotion Analysis in Short-form Videos", "comment": null, "summary": "Short-form videos (SVs) have become a vital part of our online routine for\nacquiring and sharing information. Their multimodal complexity poses new\nchallenges for video analysis, highlighting the need for video emotion analysis\n(VEA) within the community. Given the limited availability of SVs emotion data,\nwe introduce eMotions, a large-scale dataset consisting of 27,996 videos with\nfull-scale annotations. To ensure quality and reduce subjective bias, we\nemphasize better personnel allocation and propose a multi-stage annotation\nprocedure. Additionally, we provide the category-balanced and test-oriented\nvariants through targeted sampling to meet diverse needs. While there have been\nsignificant studies on videos with clear emotional cues (e.g., facial\nexpressions), analyzing emotions in SVs remains a challenging task. The\nchallenge arises from the broader content diversity, which introduces more\ndistinct semantic gaps and complicates the representations learning of\nemotion-related features. Furthermore, the prevalence of audio-visual\nco-expressions in SVs leads to the local biases and collective information gaps\ncaused by the inconsistencies in emotional expressions. To tackle this, we\npropose AV-CANet, an end-to-end audio-visual fusion network that leverages\nvideo transformer to capture semantically relevant representations. We further\nintroduce the Local-Global Fusion Module designed to progressively capture the\ncorrelations of audio-visual features. Besides, EP-CE Loss is constructed to\nglobally steer optimizations with tripolar penalties. Extensive experiments\nacross three eMotions-related datasets and four public VEA datasets demonstrate\nthe effectiveness of our proposed AV-CANet, while providing broad insights for\nfuture research. Moreover, we conduct ablation studies to examine the critical\ncomponents of our method. Dataset and code will be made available at Github.", "AI": {"tldr": "该研究针对短视频情感分析的挑战，构建了一个大型情感数据集eMotions，并提出了AV-CANet音视频融合网络，有效提升了短视频情感分析的性能。", "motivation": "短视频已成为获取和分享信息的重要方式，但其多模态复杂性给视频情感分析（VEA）带来了新挑战。现有短视频情感数据稀缺，且短视频内容多样性、语义鸿沟以及音视频共表达的不一致性使得情感特征学习和表示变得复杂。", "method": "1. 引入了大规模数据集eMotions，包含27,996个视频和完整标注，采用多阶段标注程序和人员分配优化以确保质量并减少主观偏差。2. 提出了AV-CANet，一个端到端的音视频融合网络，利用视频Transformer捕获语义相关表示。3. 设计了局部-全局融合模块（Local-Global Fusion Module），用于逐步捕获音视频特征的相关性。4. 构建了EP-CE损失函数，通过三极惩罚全局引导优化。", "result": "在三个eMotions相关数据集和四个公共VEA数据集上的大量实验证明了所提出的AV-CANet的有效性。消融研究也验证了方法关键组件的重要性。", "conclusion": "该研究通过构建大型高质量数据集和提出创新的音视频融合网络，有效解决了短视频情感分析中的挑战，为未来的研究提供了广泛的见解。"}}
{"id": "2508.07667", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07667", "abs": "https://arxiv.org/abs/2508.07667", "authors": ["Wenkai Li", "Liwen Sun", "Zhenxiang Guan", "Xuhui Zhou", "Maarten Sap"], "title": "1-2-3 Check: Enhancing Contextual Privacy in LLM via Multi-Agent Reasoning", "comment": null, "summary": "Addressing contextual privacy concerns remains challenging in interactive\nsettings where large language models (LLMs) process information from multiple\nsources (e.g., summarizing meetings with private and public information). We\nintroduce a multi-agent framework that decomposes privacy reasoning into\nspecialized subtasks (extraction, classification), reducing the information\nload on any single agent while enabling iterative validation and more reliable\nadherence to contextual privacy norms. To understand how privacy errors emerge\nand propagate, we conduct a systematic ablation over information-flow\ntopologies, revealing when and why upstream detection mistakes cascade into\ndownstream leakage. Experiments on the ConfAIde and PrivacyLens benchmark with\nseveral open-source and closed-sourced LLMs demonstrate that our best\nmulti-agent configuration substantially reduces private information leakage\n(\\textbf{18\\%} on ConfAIde and \\textbf{19\\%} on PrivacyLens with GPT-4o) while\npreserving the fidelity of public content, outperforming single-agent\nbaselines. These results highlight the promise of principled information-flow\ndesign in multi-agent systems for contextual privacy with LLMs.", "AI": {"tldr": "本文提出一个多智能体框架，通过分解隐私推理任务和优化信息流设计，显著降低大型语言模型在处理多源信息时的上下文隐私泄露，同时保持公共内容准确性。", "motivation": "在大型语言模型（LLMs）处理来自多个来源的信息（例如，总结包含私人和公共信息的会议）的交互设置中，解决上下文隐私问题仍然具有挑战性。", "method": "引入了一个多智能体框架，将隐私推理分解为专业化的子任务（提取、分类），以减少单个智能体的信息负载，并实现迭代验证和更可靠地遵守上下文隐私规范。通过对信息流拓扑进行系统性消融实验，揭示了隐私错误如何产生和传播，以及上游检测错误何时及为何会导致下游泄露。", "result": "在ConfAIde和PrivacyLens基准测试中，使用多种开源和闭源LLM进行实验表明，最佳多智能体配置显著减少了私人信息泄露（GPT-4o在ConfAIde上降低18%，在PrivacyLens上降低19%），同时保留了公共内容的准确性，并且优于单智能体基线。", "conclusion": "这些结果突出了在多智能体系统中采用原则性信息流设计对于解决LLM上下文隐私问题的潜力。"}}
{"id": "2508.07945", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07945", "abs": "https://arxiv.org/abs/2508.07945", "authors": ["En Yen Puang", "Federico Ceola", "Giulia Pasquale", "Lorenzo Natale"], "title": "PCHands: PCA-based Hand Pose Synergy Representation on Manipulators with N-DoF", "comment": "2025 IEEE-RAS 24th International Conference on Humanoid Robots", "summary": "We consider the problem of learning a common representation for dexterous\nmanipulation across manipulators of different morphologies. To this end, we\npropose PCHands, a novel approach for extracting hand postural synergies from a\nlarge set of manipulators. We define a simplified and unified description\nformat based on anchor positions for manipulators ranging from 2-finger\ngrippers to 5-finger anthropomorphic hands. This enables learning a\nvariable-length latent representation of the manipulator configuration and the\nalignment of the end-effector frame of all manipulators. We show that it is\npossible to extract principal components from this latent representation that\nis universal across manipulators of different structures and degrees of\nfreedom. To evaluate PCHands, we use this compact representation to encode\nobservation and action spaces of control policies for dexterous manipulation\ntasks learned with RL. In terms of learning efficiency and consistency, the\nproposed representation outperforms a baseline that learns the same tasks in\njoint space. We additionally show that PCHands performs robustly in RL from\ndemonstration, when demonstrations are provided from a different manipulator.\nWe further support our results with real-world experiments that involve a\n2-finger gripper and a 4-finger anthropomorphic hand. Code and additional\nmaterial are available at https://hsp-iit.github.io/PCHands/.", "AI": {"tldr": "PCHands提出了一种新颖的方法，通过从大量不同形态的机械手中提取手部姿态协同作用，为灵巧操作学习提供通用的表示。", "motivation": "为不同形态的机械手（从两指夹具到五指仿人手）学习通用的灵巧操作表示是一个挑战，需要一种统一且简化的描述格式来克服形态差异。", "method": "PCHands定义了一种基于锚点位置的统一简化描述格式，适用于不同形态的机械手。这使得能够学习可变长度的机械手配置潜在表示，并对齐所有机械手的末端执行器坐标系。然后，从该潜在表示中提取主成分，以获得跨不同结构和自由度机械手的通用手部姿态协同作用。", "result": "PCHands的紧凑表示在RL灵巧操作任务中，在学习效率和一致性方面优于在关节空间学习的基线。此外，当演示来自不同机械手时，PCHands在RL从演示中表现出鲁棒性。研究结果通过涉及两指夹具和四指仿人手的实际实验得到了支持。", "conclusion": "PCHands提供了一种有效且鲁棒的通用表示，能够跨不同形态的机械手学习灵巧操作策略，显著提高了学习效率和跨机械手泛化能力。"}}
{"id": "2508.07414", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07414", "abs": "https://arxiv.org/abs/2508.07414", "authors": ["Jean de Dieu Nyandwi", "Yueqi Song", "Simran Khanuja", "Graham Neubig"], "title": "Grounding Multilingual Multimodal LLMs With Cultural Knowledge", "comment": null, "summary": "Multimodal Large Language Models excel in high-resource settings, but often\nmisinterpret long-tail cultural entities and underperform in low-resource\nlanguages. To address this gap, we propose a data-centric approach that\ndirectly grounds MLLMs in cultural knowledge. Leveraging a large scale\nknowledge graph from Wikidata, we collect images that represent culturally\nsignificant entities, and generate synthetic multilingual visual question\nanswering data. The resulting dataset, CulturalGround, comprises 22 million\nhigh-quality, culturally-rich VQA pairs spanning 42 countries and 39 languages.\nWe train an open-source MLLM CulturalPangea on CulturalGround, interleaving\nstandard multilingual instruction-tuning data to preserve general abilities.\nCulturalPangea achieves state-of-the-art performance among open models on\nvarious culture-focused multilingual multimodal benchmarks, outperforming prior\nmodels by an average of 5.0 without degrading results on mainstream\nvision-language tasks. Our findings show that our targeted, culturally grounded\napproach could substantially narrow the cultural gap in MLLMs and offer a\npractical path towards globally inclusive multimodal systems.", "AI": {"tldr": "该研究提出了一种以数据为中心的方法，通过构建大型文化知识数据集CulturalGround并训练开源多模态大语言模型CulturalPangea，显著弥补了多模态大语言模型在文化理解和低资源语言方面的不足。", "motivation": "多模态大语言模型（MLLMs）在高资源环境下表现出色，但在理解长尾文化实体和低资源语言方面存在误解和性能不佳的问题。研究旨在解决这一文化鸿沟。", "method": "研究采用数据中心方法，利用Wikidata知识图谱收集代表文化实体的图像，并生成合成的多语言视觉问答数据，构建了包含2200万高质量、文化丰富的VQA对的CulturalGround数据集，涵盖42个国家和39种语言。在此数据集上训练了开源MLLM CulturalPangea，并穿插标准多语言指令微调数据以保持通用能力。", "result": "CulturalPangea在各类以文化为中心的多语言多模态基准测试中，在开源模型中达到了最先进的性能，平均超越现有模型5.0，同时未降低在主流视觉-语言任务上的表现。", "conclusion": "研究结果表明，这种有针对性的、以文化为基础的方法能够显著缩小多模态大语言模型中的文化差距，为构建全球包容的多模态系统提供了可行的路径。"}}
{"id": "2508.06904", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06904", "abs": "https://arxiv.org/abs/2508.06904", "authors": ["Chao Yin", "Jide Li", "Xiaoqiang Li"], "title": "A Simple yet Powerful Instance-Aware Prompting Framework for Training-free Camouflaged Object Segmentation", "comment": "under review", "summary": "Camouflaged Object Segmentation (COS) remains highly challenging due to the\nintrinsic visual similarity between target objects and their surroundings.\nWhile training-based COS methods achieve good performance, their performance\ndegrades rapidly with increased annotation sparsity. To circumvent this\nlimitation, recent studies have explored training-free COS methods, leveraging\nthe Segment Anything Model (SAM) by automatically generating visual prompts\nfrom a single task-generic prompt (\\textit{e.g.}, \"\\textit{camouflaged\nanimal}\") uniformly applied across all test images. However, these methods\ntypically produce only semantic-level visual prompts, causing SAM to output\ncoarse semantic masks and thus failing to handle scenarios with multiple\ndiscrete camouflaged instances effectively. To address this critical\nlimitation, we propose a simple yet powerful \\textbf{I}nstance-\\textbf{A}ware\n\\textbf{P}rompting \\textbf{F}ramework (IAPF), the first training-free COS\npipeline that explicitly converts a task-generic prompt into fine-grained\ninstance masks. Specifically, the IAPF comprises three steps: (1) Text Prompt\nGenerator, utilizing task-generic queries to prompt a Multimodal Large Language\nModel (MLLM) for generating image-specific foreground and background tags; (2)\n\\textbf{Instance Mask Generator}, leveraging Grounding DINO to produce precise\ninstance-level bounding box prompts, alongside the proposed Single-Foreground\nMulti-Background Prompting strategy to sample region-constrained point prompts\nwithin each box, enabling SAM to yield a candidate instance mask; (3)\nSelf-consistency Instance Mask Voting, which selects the final COS prediction\nby identifying the candidate mask most consistent across multiple candidate\ninstance masks. Extensive evaluations on standard COS benchmarks demonstrate\nthat the proposed IAPF significantly surpasses existing state-of-the-art\ntraining-free COS methods.", "AI": {"tldr": "针对伪装目标分割（COS）中现有无训练方法生成粗糙语义掩码和难以处理多实例的局限性，本文提出IAPF框架，通过实例感知提示策略，结合MLLM、Grounding DINO和SAM，生成精细的实例级伪装目标掩码，显著超越了现有无训练方法。", "motivation": "伪装目标分割因目标与背景的视觉相似性而极具挑战。基于训练的方法在标注稀疏时性能下降。现有无训练方法（如利用SAM）仅能生成语义级提示，导致SAM输出粗糙掩码，难以有效处理多个离散的伪装实例。", "method": "本文提出了实例感知提示框架（IAPF），一个无训练的COS流程：1) **文本提示生成器**：利用任务通用查询提示多模态大语言模型（MLLM）生成图像特定的前景和背景标签。2) **实例掩码生成器**：利用Grounding DINO生成精确的实例级边界框提示，并提出“单前景多背景提示”策略，在每个框内采样区域受限的点提示，使SAM生成候选实例掩码。3) **自洽性实例掩码投票**：通过识别多个候选实例掩码中最具一致性的掩码，选择最终的COS预测结果。", "result": "在标准COS基准测试上的广泛评估表明，所提出的IAPF显著超越了现有最先进的无训练COS方法。", "conclusion": "IAPF成功解决了现有无训练COS方法在生成精细实例级掩码方面的局限性，通过创新的实例感知提示策略，实现了对伪装目标的有效分割，尤其在处理多实例场景时表现出色，为无训练COS领域树立了新标杆。"}}
{"id": "2508.07671", "categories": ["cs.AI", "cs.CY", "cs.HC", "cs.MA", "stat.AP", "68T07, 68T42, 68T50, 91F20, 62P25", "I.2.11; I.2.1; H.1.2; J.4; K.4.2"], "pdf": "https://arxiv.org/pdf/2508.07671", "abs": "https://arxiv.org/abs/2508.07671", "authors": ["Mohamed Rayan Barhdadi", "Mehmet Tuncel", "Erchin Serpedin", "Hasan Kurban"], "title": "EMPATHIA: Multi-Faceted Human-AI Collaboration for Refugee Integration", "comment": "19 pages, 3 figures (plus 6 figures in supplementary), 2 tables, 1\n  algorithm. Submitted to NeurIPS 2025 Creative AI Track: Humanity", "summary": "Current AI approaches to refugee integration optimize narrow objectives such\nas employment and fail to capture the cultural, emotional, and ethical\ndimensions critical for long-term success. We introduce EMPATHIA (Enriched\nMultimodal Pathways for Agentic Thinking in Humanitarian Immigrant Assistance),\na multi-agent framework addressing the central Creative AI question: how do we\npreserve human dignity when machines participate in life-altering decisions?\nGrounded in Kegan's Constructive Developmental Theory, EMPATHIA decomposes\nintegration into three modules: SEED (Socio-cultural Entry and Embedding\nDecision) for initial placement, RISE (Rapid Integration and Self-sufficiency\nEngine) for early independence, and THRIVE (Transcultural Harmony and\nResilience through Integrated Values and Engagement) for sustained outcomes.\nSEED employs a selector-validator architecture with three specialized agents -\nemotional, cultural, and ethical - that deliberate transparently to produce\ninterpretable recommendations. Experiments on the UN Kakuma dataset (15,026\nindividuals, 7,960 eligible adults 15+ per ILO/UNHCR standards) and\nimplementation on 6,359 working-age refugees (15+) with 150+ socioeconomic\nvariables achieved 87.4% validation convergence and explainable assessments\nacross five host countries. EMPATHIA's weighted integration of cultural,\nemotional, and ethical factors balances competing value systems while\nsupporting practitioner-AI collaboration. By augmenting rather than replacing\nhuman expertise, EMPATHIA provides a generalizable framework for AI-driven\nallocation tasks where multiple values must be reconciled.", "AI": {"tldr": "EMPATHIA是一个多智能体AI框架，旨在通过整合文化、情感和伦理维度来优化难民融入过程，以弥补当前AI方法仅关注就业等狭窄目标的不足，并在实际数据上取得了良好效果。", "motivation": "当前的AI难民融入方法仅优化如就业等狭窄目标，未能捕捉对长期成功至关重要的文化、情感和伦理维度。研究旨在解决核心的创意AI问题：当机器参与改变人生的决策时，如何维护人类尊严。", "method": "EMPATHIA是一个基于Kegan建构性发展理论的多智能体框架，将融入过程分解为三个模块：SEED（初始安置）、RISE（早期独立）和THRIVE（持续成果）。SEED模块采用选择器-验证器架构，包含情感、文化和伦理三个专业智能体，进行透明的决策以生成可解释的建议。", "result": "该框架在联合国卡库马数据集（15,026人）上进行了实验，并应用于6,359名工作年龄难民（150+社会经济变量），实现了87.4%的验证收敛率，并在五个接收国提供了可解释的评估。EMPATHIA通过加权整合文化、情感和伦理因素，平衡了相互竞争的价值体系。", "conclusion": "EMPATHIA通过增强而非取代人类专业知识，提供了一个可推广的AI驱动分配任务框架，适用于需要协调多种价值的场景，并支持从业者与AI的协作，从而维护了人类尊严。"}}
{"id": "2508.08046", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.08046", "abs": "https://arxiv.org/abs/2508.08046", "authors": ["Fen Liu", "Shenghai Yuan", "Thien-Minh Nguyen", "Wei Meng", "Lihua Xie"], "title": "Aerial Target Encirclement and Interception with Noisy Range Observations", "comment": "The paper has been accepted in Automatica", "summary": "This paper proposes a strategy to encircle and intercept a non-cooperative\naerial point-mass moving target by leveraging noisy range measurements for\nstate estimation. In this approach, the guardians actively ensure the\nobservability of the target by using an anti-synchronization (AS), 3D\n``vibrating string\" trajectory, which enables rapid position and velocity\nestimation based on the Kalman filter. Additionally, a novel anti-target\ncontroller is designed for the guardians to enable adaptive transitions from\nencircling a protected target to encircling, intercepting, and neutralizing a\nhostile target, taking into consideration the input constraints of the\nguardians. Based on the guaranteed uniform observability, the exponentially\nbounded stability of the state estimation error and the convergence of the\nencirclement error are rigorously analyzed. Simulation results and real-world\nUAV experiments are presented to further validate the effectiveness of the\nsystem design.", "AI": {"tldr": "本文提出了一种利用噪声距离测量对非合作空中点质量目标进行状态估计，并通过特殊轨迹和控制器实现包围、拦截和中和的策略。", "motivation": "现有方法难以有效处理非合作空中移动目标的包围、拦截和中和问题，尤其是在存在测量噪声和需要确保可观测性的情况下。", "method": "1. 采用反同步（AS）3D“振动弦”轨迹，主动确保目标的可观测性。 2. 利用卡尔曼滤波器进行快速位置和速度估计。 3. 设计新型反目标控制器，使守护者能从保护目标包围自适应过渡到敌对目标包围、拦截和中和，并考虑输入约束。 4. 严格分析了状态估计误差的指数有界稳定性和包围误差的收敛性。", "result": "基于保证的均匀可观测性，状态估计误差的指数有界稳定性和包围误差的收敛性得到了严格分析。仿真结果和真实无人机实验验证了系统设计的有效性。", "conclusion": "该策略通过主动确保可观测性的轨迹设计和自适应控制器，能够有效实现对非合作空中移动目标的包围、拦截和中和，即使在存在测量噪声和输入约束的情况下也能保证估计和包围的收敛性。"}}
{"id": "2508.07434", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.07434", "abs": "https://arxiv.org/abs/2508.07434", "authors": ["Zhiyi Lyu", "Jianguo Huang", "Yanchen Deng", "Steven Hoi", "Bo An"], "title": "Let's Revise Step-by-Step: A Unified Local Search Framework for Code Generation with LLMs", "comment": null, "summary": "Large Language Models (LLMs) with inference-time scaling techniques show\npromise for code generation, yet face notable efficiency and scalability\nchallenges. Construction-based tree-search methods suffer from rapid growth in\ntree size, high token consumption, and lack of anytime property. In contrast,\nimprovement-based methods offer better performance but often struggle with\nuninformative reward signals and inefficient search strategies. In this work,\nwe propose \\textbf{ReLoc}, a unified local search framework which effectively\nperforms step-by-step code revision. Specifically, ReLoc explores a series of\nlocal revisions through four key algorithmic components: initial code drafting,\nneighborhood code generation, candidate evaluation, and incumbent code\nupdating, each of which can be instantiated with specific decision rules to\nrealize different local search algorithms such as Hill Climbing (HC) or Genetic\nAlgorithm (GA). Furthermore, we develop a specialized revision reward model\nthat evaluates code quality based on revision distance to produce fine-grained\npreferences that guide the local search toward more promising candidates.\nFinally, our extensive experimental results demonstrate that our approach\nachieves superior performance across diverse code generation tasks,\nsignificantly outperforming both construction-based tree search as well as the\nstate-of-the-art improvement-based code generation methods.", "AI": {"tldr": "ReLoc是一个统一的局部搜索框架，通过逐步代码修订显著提高了LLM的代码生成性能，优于现有方法。", "motivation": "大型语言模型（LLMs）在代码生成方面面临效率和可扩展性挑战。现有方法存在问题：基于构建的树搜索方法树尺寸增长快、token消耗高且缺乏即时性；基于改进的方法虽然性能较好，但常受制于无信息奖励信号和低效搜索策略。", "method": "提出了ReLoc，一个统一的局部搜索框架，用于逐步代码修订。它包含四个核心算法组件：初始代码草拟、邻域代码生成、候选评估和当前最佳代码更新。这些组件可实例化为不同的局部搜索算法（如爬山法或遗传算法）。此外，开发了一个专门的修订奖励模型，通过修订距离评估代码质量，提供细粒度偏好以指导局部搜索。", "result": "ReLoc在各种代码生成任务中表现出卓越的性能，显著优于基于构建的树搜索方法以及最先进的基于改进的代码生成方法。", "conclusion": "ReLoc通过其统一的局部搜索框架和专门的修订奖励模型，有效解决了现有代码生成方法的效率和可扩展性问题，并在实践中取得了优异的性能。"}}
{"id": "2508.06905", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06905", "abs": "https://arxiv.org/abs/2508.06905", "authors": ["Ruoxi Chen", "Dongping Chen", "Siyuan Wu", "Sinan Wang", "Shiyun Lang", "Petr Sushko", "Gaoyang Jiang", "Yao Wan", "Ranjay Krishna"], "title": "MultiRef: Controllable Image Generation with Multiple Visual References", "comment": "Accepted to ACM MM 2025 Datasets", "summary": "Visual designers naturally draw inspiration from multiple visual references,\ncombining diverse elements and aesthetic principles to create artwork. However,\ncurrent image generative frameworks predominantly rely on single-source inputs\n-- either text prompts or individual reference images. In this paper, we focus\non the task of controllable image generation using multiple visual references.\nWe introduce MultiRef-bench, a rigorous evaluation framework comprising 990\nsynthetic and 1,000 real-world samples that require incorporating visual\ncontent from multiple reference images. The synthetic samples are synthetically\ngenerated through our data engine RefBlend, with 10 reference types and 33\nreference combinations. Based on RefBlend, we further construct a dataset\nMultiRef containing 38k high-quality images to facilitate further research. Our\nexperiments across three interleaved image-text models (i.e., OmniGen, ACE, and\nShow-o) and six agentic frameworks (e.g., ChatDiT and LLM + SD) reveal that\neven state-of-the-art systems struggle with multi-reference conditioning, with\nthe best model OmniGen achieving only 66.6% in synthetic samples and 79.0% in\nreal-world cases on average compared to the golden answer. These findings\nprovide valuable directions for developing more flexible and human-like\ncreative tools that can effectively integrate multiple sources of visual\ninspiration. The dataset is publicly available at: https://multiref.github.io/.", "AI": {"tldr": "本文提出并评估了一个多视觉参考图像生成基准MultiRef-bench和数据集MultiRef，发现当前最先进的模型在整合多个视觉参考方面表现不佳。", "motivation": "视觉设计师在创作时自然地从多个视觉参考中汲取灵感，但当前的图像生成框架主要依赖单一输入（文本提示或单个参考图像），缺乏处理多参考输入的能力。", "method": "引入了MultiRef-bench，一个包含990个合成样本和1,000个真实世界样本的严格评估框架，用于测试多视觉参考图像生成。通过数据引擎RefBlend合成样本，并基于RefBlend构建了包含3.8万张高质量图像的MultiRef数据集。实验评估了三种交错图像-文本模型（OmniGen、ACE、Show-o）和六种代理框架（ChatDiT、LLM + SD）。", "result": "即使是当前最先进的系统，在多参考条件生成方面也表现不佳。最佳模型OmniGen在合成样本上平均仅达到66.6%的准确率，在真实世界样本上为79.0%，远低于黄金标准。", "conclusion": "研究结果为开发更灵活、更像人类的创意工具提供了宝贵的方向，这些工具需要能够有效地整合多源视觉灵感。"}}
{"id": "2508.07673", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07673", "abs": "https://arxiv.org/abs/2508.07673", "authors": ["Gianluca Bontempi"], "title": "Ethics2vec: aligning automatic agents and human preferences", "comment": null, "summary": "Though intelligent agents are supposed to improve human experience (or make\nit more efficient), it is hard from a human perspective to grasp the ethical\nvalues which are explicitly or implicitly embedded in an agent behaviour. This\nis the well-known problem of alignment, which refers to the challenge of\ndesigning AI systems that align with human values, goals and preferences. This\nproblem is particularly challenging since most human ethical considerations\nrefer to \\emph{incommensurable} (i.e. non-measurable and/or incomparable)\nvalues and criteria. Consider, for instance, a medical agent prescribing a\ntreatment to a cancerous patient. How could it take into account (and/or weigh)\nincommensurable aspects like the value of a human life and the cost of the\ntreatment? Now, the alignment between human and artificial values is possible\nonly if we define a common space where a metric can be defined and used. This\npaper proposes to extend to ethics the conventional Anything2vec approach,\nwhich has been successful in plenty of similar and hard-to-quantify domains\n(ranging from natural language processing to recommendation systems and graph\nanalysis). This paper proposes a way to map an automatic agent decision-making\n(or control law) strategy to a multivariate vector representation, which can be\nused to compare and assess the alignment with human values. The Ethics2Vec\nmethod is first introduced in the case of an automatic agent performing binary\ndecision-making. Then, a vectorisation of an automatic control law (like in the\ncase of a self-driving car) is discussed to show how the approach can be\nextended to automatic control settings.", "AI": {"tldr": "本文提出Ethics2Vec方法，将智能体的决策行为映射为向量表示，以解决AI与人类价值观对齐的难题，特别是处理不可量化的伦理考量。", "motivation": "智能体行为中嵌入的伦理价值观难以被人类理解和衡量，即AI对齐问题。这个问题因人类伦理考量中存在不可通约（无法衡量或比较）的价值观（如生命价值与治疗成本）而更具挑战性。为实现人机价值观对齐，需要一个可定义度量的共同空间。", "method": "提出Ethics2Vec方法，借鉴Any2vec理念，将自动智能体的决策制定或控制律策略映射为多变量向量表示。该方法首先在智能体二元决策场景中介绍，然后扩展到自动控制设置（如自动驾驶汽车的控制律）。", "result": "本文提出了一种将自动智能体行为向量化的方法，该方法可用于比较和评估智能体行为与人类价值观的对齐程度，从而为解决AI伦理对齐问题提供一个可量化的框架。", "conclusion": "Ethics2Vec方法通过将智能体行为向量化，为人类和AI价值观之间定义了一个共同的度量空间，从而可能实现对齐的比较和评估，尤其适用于处理难以量化的伦理维度。"}}
{"id": "2508.08108", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.08108", "abs": "https://arxiv.org/abs/2508.08108", "authors": ["Wei Zhang", "Yinchuan Wang", "Wangtao Lu", "Pengyu Zhang", "Xiang Zhang", "Yue Wang", "Chaoqun Wang"], "title": "Capsizing-Guided Trajectory Optimization for Autonomous Navigation with Rough Terrain", "comment": null, "summary": "It is a challenging task for ground robots to autonomously navigate in harsh\nenvironments due to the presence of non-trivial obstacles and uneven terrain.\nThis requires trajectory planning that balances safety and efficiency. The\nprimary challenge is to generate a feasible trajectory that prevents robot from\ntip-over while ensuring effective navigation. In this paper, we propose a\ncapsizing-aware trajectory planner (CAP) to achieve trajectory planning on the\nuneven terrain. The tip-over stability of the robot on rough terrain is\nanalyzed. Based on the tip-over stability, we define the traversable\norientation, which indicates the safe range of robot orientations. This\norientation is then incorporated into a capsizing-safety constraint for\ntrajectory optimization. We employ a graph-based solver to compute a robust and\nfeasible trajectory while adhering to the capsizing-safety constraint.\nExtensive simulation and real-world experiments validate the effectiveness and\nrobustness of the proposed method. The results demonstrate that CAP outperforms\nexisting state-of-the-art approaches, providing enhanced navigation performance\non uneven terrains.", "AI": {"tldr": "本文提出了一种名为CAP的倾覆感知轨迹规划器，用于地面机器人在崎岖地形上的安全自主导航，通过分析倾覆稳定性并将其纳入轨迹优化，有效防止机器人倾覆。", "motivation": "地面机器人在崎岖环境中自主导航面临挑战，需要平衡安全性和效率的轨迹规划，主要难点在于生成可行的轨迹以防止机器人倾覆。", "method": "提出了倾覆感知轨迹规划器（CAP）。分析了机器人在崎岖地形上的倾覆稳定性，定义了可通行姿态（机器人安全姿态范围），并将其作为倾覆安全约束纳入轨迹优化中。采用基于图的求解器计算鲁棒且可行的轨迹。", "result": "大量的仿真和真实世界实验验证了所提出方法的有效性和鲁棒性。结果表明CAP优于现有先进方法，在崎岖地形上提供了增强的导航性能。", "conclusion": "CAP通过整合倾覆安全约束，成功实现了地面机器人在崎岖地形上的安全且高效的轨迹规划，显著提升了导航性能。"}}
{"id": "2508.07479", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.07479", "abs": "https://arxiv.org/abs/2508.07479", "authors": ["Blerta Veseli", "Julian Chibane", "Mariya Toneva", "Alexander Koller"], "title": "Positional Biases Shift as Inputs Approach Context Window Limits", "comment": null, "summary": "Large Language Models (LLMs) often struggle to use information across long\ninputs effectively. Prior work has identified positional biases, such as the\nLost in the Middle (LiM) effect, where models perform better when information\nappears at the beginning (primacy bias) or end (recency bias) of the input,\nrather than in the middle. However, long-context studies have not consistently\nreplicated these effects, raising questions about their intensity and the\nconditions under which they manifest. To address this, we conducted a\ncomprehensive analysis using relative rather than absolute input lengths,\ndefined with respect to each model's context window. Our findings reveal that\nthe LiM effect is strongest when inputs occupy up to 50% of a model's context\nwindow. Beyond that, the primacy bias weakens, while recency bias remains\nrelatively stable. This effectively eliminates the LiM effect; instead, we\nobserve a distance-based bias, where model performance is better when relevant\ninformation is closer to the end of the input. Furthermore, our results suggest\nthat successful retrieval is a prerequisite for reasoning in LLMs, and that the\nobserved positional biases in reasoning are largely inherited from retrieval.\nThese insights have implications for long-context tasks, the design of future\nLLM benchmarks, and evaluation methodologies for LLMs handling extended inputs.", "AI": {"tldr": "研究发现大型语言模型（LLMs）在长输入中处理信息时存在位置偏差，特别是“中间丢失”（LiM）效应。该效应在输入占用模型上下文窗口的50%以内最强，超出后，首位偏差减弱，末位偏差稳定，LiM效应消失，转变为一种距离偏差（信息越靠近末尾性能越好）。模型推理中的位置偏差主要源于信息检索。", "motivation": "大型语言模型（LLMs）在处理长输入时难以有效利用信息，先前研究发现存在位置偏差（如“中间丢失”效应），但这些效应在长上下文研究中未能持续复现，对其强度和发生条件存在疑问。", "method": "采用相对于每个模型上下文窗口的“相对”输入长度进行综合分析，而非绝对输入长度。", "result": "当输入占用模型上下文窗口的50%以内时，“中间丢失”效应最强。超过50%后，首位偏差减弱，而末位偏差保持相对稳定，这实际上消除了“中间丢失”效应，转变为一种距离偏差（相关信息越靠近输入末尾，模型性能越好）。此外，成功检索是LLM推理的前提，推理中的位置偏差主要继承自检索环节。", "conclusion": "这些发现对长上下文任务、未来LLM基准设计以及LLM处理扩展输入的评估方法具有重要意义。"}}
{"id": "2508.06908", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06908", "abs": "https://arxiv.org/abs/2508.06908", "authors": ["Jinhao Li", "Zijian Chen", "Lirong Deng", "Changbo Wang", "Guangtao Zhai"], "title": "MMReID-Bench: Unleashing the Power of MLLMs for Effective and Versatile Person Re-identification", "comment": null, "summary": "Person re-identification (ReID) aims to retrieve the images of an interested\nperson in the gallery images, with wide applications in medical rehabilitation,\nabnormal behavior detection, and public security. However, traditional person\nReID models suffer from uni-modal capability, leading to poor generalization\nability in multi-modal data, such as RGB, thermal, infrared, sketch images,\ntextual descriptions, etc. Recently, the emergence of multi-modal large\nlanguage models (MLLMs) shows a promising avenue for addressing this problem.\nDespite this potential, existing methods merely regard MLLMs as feature\nextractors or caption generators, which do not fully unleash their reasoning,\ninstruction-following, and cross-modal understanding capabilities. To bridge\nthis gap, we introduce MMReID-Bench, the first multi-task multi-modal benchmark\nspecifically designed for person ReID. The MMReID-Bench includes 20,710\nmulti-modal queries and gallery images covering 10 different person ReID tasks.\nComprehensive experiments demonstrate the remarkable capabilities of MLLMs in\ndelivering effective and versatile person ReID. Nevertheless, they also have\nlimitations in handling a few modalities, particularly thermal and infrared\ndata. We hope MMReID-Bench can facilitate the community to develop more robust\nand generalizable multimodal foundation models for person ReID.", "AI": {"tldr": "本文提出了MMReID-Bench，首个专为行人重识别（ReID）设计的多任务多模态基准，旨在充分利用多模态大语言模型（MLLMs）在ReID中的潜力，并揭示其在特定模态上的局限性。", "motivation": "传统行人ReID模型受限于单模态能力，导致在RGB、热成像、红外、草图、文本描述等多模态数据上泛化能力差。尽管多模态大语言模型（MLLMs）显示出解决此问题的潜力，但现有方法仅将其用作特征提取器或字幕生成器，未能充分发挥其推理、指令遵循和跨模态理解能力。", "method": "为弥补现有研究的不足，作者引入了MMReID-Bench，这是首个专为行人ReID设计的多任务多模态基准。该基准包含20,710个多模态查询和图库图像，涵盖10种不同的行人ReID任务。", "result": "通过综合实验，MMReID-Bench证明了MLLMs在实现有效和通用行人ReID方面的卓越能力。然而，实验也揭示了MLLMs在处理某些模态（特别是热成像和红外数据）时存在的局限性。", "conclusion": "MMReID-Bench有望促进社区开发更鲁棒、更具泛化能力的行人ReID多模态基础模型，同时指出了MLLMs在特定模态处理上的改进空间。"}}
{"id": "2508.07743", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07743", "abs": "https://arxiv.org/abs/2508.07743", "authors": ["Markus Fritzsche", "Elliot Gestrin", "Jendrik Seipp"], "title": "Symmetry-Aware Transformer Training for Automated Planning", "comment": null, "summary": "While transformers excel in many settings, their application in the field of\nautomated planning is limited. Prior work like PlanGPT, a state-of-the-art\ndecoder-only transformer, struggles with extrapolation from easy to hard\nplanning problems. This in turn stems from problem symmetries: planning tasks\ncan be represented with arbitrary variable names that carry no meaning beyond\nbeing identifiers. This causes a combinatorial explosion of equivalent\nrepresentations that pure transformers cannot efficiently learn from. We\npropose a novel contrastive learning objective to make transformers\nsymmetry-aware and thereby compensate for their lack of inductive bias.\nCombining this with architectural improvements, we show that transformers can\nbe efficiently trained for either plan-generation or heuristic-prediction. Our\nresults across multiple planning domains demonstrate that our symmetry-aware\ntraining effectively and efficiently addresses the limitations of PlanGPT.", "AI": {"tldr": "针对自动化规划领域中Transformer模型（如PlanGPT）因问题对称性导致的泛化能力差问题，本文提出了一种新颖的对比学习目标和架构改进，使Transformer具备对称性感知能力，从而有效解决了PlanGPT的局限性。", "motivation": "Transformer模型在自动化规划领域的应用受限，特别是像PlanGPT这样的现有模型，难以从简单规划问题泛化到复杂问题。这主要是因为规划任务存在对称性（变量名任意性导致等价表示的组合爆炸），纯粹的Transformer无法有效学习这些对称性。", "method": "提出了一种新颖的对比学习目标，旨在使Transformer模型具备对称性感知能力，以弥补其归纳偏置的不足。结合架构上的改进，使Transformer能够高效地进行规划生成或启发式预测训练。", "result": "在多个规划领域的结果表明，所提出的对称性感知训练方法有效且高效地解决了PlanGPT的局限性。证明了Transformer可以高效地用于规划生成或启发式预测。", "conclusion": "通过引入对比学习目标和架构改进，使Transformer模型具备对称性感知能力，能够有效克服自动化规划中因问题对称性带来的挑战，显著提升了模型在复杂规划问题上的性能，超越了现有方法如PlanGPT。"}}
{"id": "2508.08113", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.08113", "abs": "https://arxiv.org/abs/2508.08113", "authors": ["Yinpei Dai", "Jayjun Lee", "Yichi Zhang", "Ziqiao Ma", "Jed Yang", "Amir Zadeh", "Chuan Li", "Nima Fazeli", "Joyce Chai"], "title": "AimBot: A Simple Auxiliary Visual Cue to Enhance Spatial Awareness of Visuomotor Policies", "comment": "CoRL 2025", "summary": "In this paper, we propose AimBot, a lightweight visual augmentation technique\nthat provides explicit spatial cues to improve visuomotor policy learning in\nrobotic manipulation. AimBot overlays shooting lines and scope reticles onto\nmulti-view RGB images, offering auxiliary visual guidance that encodes the\nend-effector's state. The overlays are computed from depth images, camera\nextrinsics, and the current end-effector pose, explicitly conveying spatial\nrelationships between the gripper and objects in the scene. AimBot incurs\nminimal computational overhead (less than 1 ms) and requires no changes to\nmodel architectures, as it simply replaces original RGB images with augmented\ncounterparts. Despite its simplicity, our results show that AimBot consistently\nimproves the performance of various visuomotor policies in both simulation and\nreal-world settings, highlighting the benefits of spatially grounded visual\nfeedback.", "AI": {"tldr": "本文提出AimBot，一种轻量级视觉增强技术，通过在RGB图像上叠加空间线索（如射击线和瞄准镜），以改进机器人操作中的视觉运动策略学习。", "motivation": "现有视觉运动策略学习在机器人操作中可能缺乏明确的空间信息，导致学习效率和性能受限。研究旨在提供一种简单有效的方法来增强视觉反馈，明确传达抓手与物体之间的空间关系。", "method": "AimBot通过深度图像、相机外参和当前末端执行器姿态计算，将射击线和瞄准镜叠加到多视角RGB图像上。这种方法计算开销极小（小于1毫秒），且无需修改模型架构，直接替换原始RGB图像。", "result": "尽管AimBot设计简单，但在仿真和真实世界环境中，它能持续提升各种视觉运动策略的性能。", "conclusion": "研究结果强调了空间接地视觉反馈对机器人操作中视觉运动策略学习的益处。"}}
{"id": "2508.07484", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07484", "abs": "https://arxiv.org/abs/2508.07484", "authors": ["Archchana Sindhujan", "Shenbin Qian", "Chan Chi Chun Matthew", "Constantin Orasan", "Diptesh Kanojia"], "title": "ALOPE: Adaptive Layer Optimization for Translation Quality Estimation using Large Language Models", "comment": "Accepted to COLM 2025 Conference", "summary": "Large Language Models (LLMs) have shown remarkable performance across a wide\nrange of natural language processing tasks. Quality Estimation (QE) for Machine\nTranslation (MT), which assesses the quality of a source-target pair without\nrelying on reference translations, remains a challenging cross-lingual task for\nLLMs. The challenges stem from the inherent limitations of existing LLM-based\nQE systems, which are pre-trained for causal language modelling rather than\nregression-specific tasks, further elevated by the presence of low-resource\nlanguages given pre-training data distribution. This paper introduces ALOPE, an\nadaptive layer-optimization framework designed to enhance LLM-based QE by\nrestructuring Transformer representations through layer-wise adaptation for\nimproved regression-based prediction. Our framework integrates low-rank\nadapters (LoRA) with regression task heads, leveraging selected pre-trained\nTransformer layers for improved cross-lingual alignment. In addition to the\nlayer-specific adaptation, ALOPE introduces two strategies-dynamic weighting,\nwhich adaptively combines representations from multiple layers, and multi-head\nregression, which aggregates regression losses from multiple heads for QE. Our\nframework shows improvements over various existing LLM-based QE approaches.\nEmpirical evidence suggests that intermediate Transformer layers in LLMs\nprovide contextual representations that are more aligned with the cross-lingual\nnature of the QE task. We make resultant models and framework code publicly\navailable for further research, also allowing existing LLM-based MT frameworks\nto be scaled with QE capabilities.", "AI": {"tldr": "本文提出ALOPE框架，通过层级自适应、动态加权和多头回归，提升大型语言模型在机器翻译质量评估（QE）任务上的表现，尤其在跨语言和低资源场景下。", "motivation": "现有基于LLM的QE系统面临挑战，因为LLM主要为因果语言建模预训练，而非回归任务，且在低资源语言上表现受限。QE作为跨语言任务，对LLM来说仍具挑战。", "method": "ALOPE框架通过层级优化改进LLM的Transformer表示。具体方法包括：1) 将低秩适配器（LoRA）与回归任务头结合，在选定的预训练Transformer层进行层级自适应；2) 引入动态加权策略，自适应结合多层表示；3) 引入多头回归，聚合来自多个头的回归损失。", "result": "ALOPE框架在各种现有基于LLM的QE方法上均显示出改进。经验证据表明，LLM的中间Transformer层提供的上下文表示更符合QE任务的跨语言特性。", "conclusion": "ALOPE是一个有效的LLM-based QE增强框架，通过对Transformer层进行自适应优化，显著提升了QE性能，尤其强调了中间层在跨语言任务中的重要性。相关模型和代码已公开，以便进一步研究和集成QE能力到现有MT框架中。"}}
{"id": "2508.06916", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06916", "abs": "https://arxiv.org/abs/2508.06916", "authors": ["Shichao Ma", "Yunhe Guo", "Jiahao Su", "Qihe Huang", "Zhengyang Zhou", "Yang Wang"], "title": "Talk2Image: A Multi-Agent System for Multi-Turn Image Generation and Editing", "comment": null, "summary": "Text-to-image generation tasks have driven remarkable advances in diverse\nmedia applications, yet most focus on single-turn scenarios and struggle with\niterative, multi-turn creative tasks. Recent dialogue-based systems attempt to\nbridge this gap, but their single-agent, sequential paradigm often causes\nintention drift and incoherent edits. To address these limitations, we present\nTalk2Image, a novel multi-agent system for interactive image generation and\nediting in multi-turn dialogue scenarios. Our approach integrates three key\ncomponents: intention parsing from dialogue history, task decomposition and\ncollaborative execution across specialized agents, and feedback-driven\nrefinement based on a multi-view evaluation mechanism. Talk2Image enables\nstep-by-step alignment with user intention and consistent image editing.\nExperiments demonstrate that Talk2Image outperforms existing baselines in\ncontrollability, coherence, and user satisfaction across iterative image\ngeneration and editing tasks.", "AI": {"tldr": "Talk2Image是一个新颖的多智能体系统，用于多轮对话场景中的交互式图像生成和编辑，解决了现有单智能体系统在处理迭代创意任务时意图漂移和编辑不连贯的问题。", "motivation": "现有的文本到图像生成系统主要关注单轮场景，难以处理迭代、多轮的创意任务；基于对话的系统虽然尝试弥补这一差距，但其单智能体、顺序范式常导致意图漂移和不连贯的编辑。", "method": "Talk2Image系统包含三个关键组件：从对话历史中解析用户意图、任务分解和专业智能体间的协作执行、以及基于多视图评估机制的反馈驱动式优化。", "result": "实验证明，Talk2Image在迭代图像生成和编辑任务中，在可控性、连贯性和用户满意度方面均优于现有基线系统。", "conclusion": "Talk2Image能够实现与用户意图的逐步对齐，并支持在多轮场景中进行一致的图像编辑。"}}
{"id": "2508.07790", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2508.07790", "abs": "https://arxiv.org/abs/2508.07790", "authors": ["Alessandro Abate", "Thom Badings", "Giuseppe De Giacomo", "Francesco Fabiano"], "title": "Best-Effort Policies for Robust Markov Decision Processes", "comment": null, "summary": "We study the common generalization of Markov decision processes (MDPs) with\nsets of transition probabilities, known as robust MDPs (RMDPs). A standard goal\nin RMDPs is to compute a policy that maximizes the expected return under an\nadversarial choice of the transition probabilities. If the uncertainty in the\nprobabilities is independent between the states, known as s-rectangularity,\nsuch optimal robust policies can be computed efficiently using robust value\niteration. However, there might still be multiple optimal robust policies,\nwhich, while equivalent with respect to the worst-case, reflect different\nexpected returns under non-adversarial choices of the transition probabilities.\nHence, we propose a refined policy selection criterion for RMDPs, drawing\ninspiration from the notions of dominance and best-effort in game theory.\nInstead of seeking a policy that only maximizes the worst-case expected return,\nwe additionally require the policy to achieve a maximal expected return under\ndifferent (i.e., not fully adversarial) transition probabilities. We call such\na policy an optimal robust best-effort (ORBE) policy. We prove that ORBE\npolicies always exist, characterize their structure, and present an algorithm\nto compute them with a small overhead compared to standard robust value\niteration. ORBE policies offer a principled tie-breaker among optimal robust\npolicies. Numerical experiments show the feasibility of our approach.", "AI": {"tldr": "研究鲁棒马尔可夫决策过程（RMDPs）中存在多个最坏情况最优策略的问题，提出一种新的策略选择准则——最优鲁棒尽力（ORBE）策略，以在非对抗性条件下实现最大预期回报，并开发了计算算法。", "motivation": "在鲁棒马尔可夫决策过程（RMDPs）中，当不确定性是s-矩形时，可以高效计算出最大化最坏情况预期回报的最优鲁棒策略。然而，可能存在多个这样的策略，它们在最坏情况下等效，但在非对抗性转移概率下会产生不同的预期回报。因此，需要一个更精细的策略选择标准来打破这种平局。", "method": "受博弈论中主导和尽力概念的启发，提出了“最优鲁棒尽力（ORBE）”策略。这种策略不仅最大化最坏情况预期回报，还要求在不同的（非完全对抗性）转移概率下实现最大的预期回报。研究了ORBE策略的结构，并提出了一种计算它们的算法，该算法相对于标准鲁棒值迭代具有较小的开销。", "result": "证明了ORBE策略总是存在的，并对其结构进行了刻画。开发了一种计算ORBE策略的算法，该算法相比标准鲁棒值迭代的开销很小。数值实验结果表明了该方法的可行性。", "conclusion": "ORBE策略为最优鲁棒策略提供了一个有原则的平局打破机制，使得在保持最坏情况最优性的同时，能在非对抗性条件下实现更好的预期回报。"}}
{"id": "2508.08226", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.08226", "abs": "https://arxiv.org/abs/2508.08226", "authors": ["Haiyue Chen", "Aniket Datar", "Tong Xu", "Francesco Cancelliere", "Harsh Rangwala", "Madhan Balaji Rao", "Daeun Song", "David Eichinger", "Xuesu Xiao"], "title": "Verti-Arena: A Controllable and Standardized Indoor Testbed for Multi-Terrain Off-Road Autonomy", "comment": "6 pages", "summary": "Off-road navigation is an important capability for mobile robots deployed in\nenvironments that are inaccessible or dangerous to humans, such as disaster\nresponse or planetary exploration. Progress is limited due to the lack of a\ncontrollable and standardized real-world testbed for systematic data collection\nand validation. To fill this gap, we introduce Verti-Arena, a reconfigurable\nindoor facility designed specifically for off-road autonomy. By providing a\nrepeatable benchmark environment, Verti-Arena supports reproducible experiments\nacross a variety of vertically challenging terrains and provides precise ground\ntruth measurements through onboard sensors and a motion capture system.\nVerti-Arena also supports consistent data collection and comparative evaluation\nof algorithms in off-road autonomy research. We also develop a web-based\ninterface that enables research groups worldwide to remotely conduct\nstandardized off-road autonomy experiments on Verti-Arena.", "AI": {"tldr": "本文介绍了一个名为Verti-Arena的可重构室内测试平台，专为越野自主导航研究设计，旨在提供标准化、可重复的实验环境和数据收集能力，并支持远程访问。", "motivation": "越野导航对移动机器人至关重要，但在该领域进展受限，因为缺乏一个可控、标准化的真实世界测试平台，用于系统性数据收集和验证。", "method": "引入Verti-Arena，一个可重构的室内设施，专门用于越野自主导航研究。该平台通过机载传感器和运动捕捉系统提供精确的地面真值测量。此外，开发了一个基于网络的界面，支持全球研究团队远程进行标准化越野自主导航实验。", "result": "Verti-Arena提供了一个可重复的基准环境，支持在各种垂直挑战地形上进行可复现的实验，并提供精确的地面真值测量。它还支持越野自主导航算法的一致数据收集和比较评估，并通过网络界面实现了全球研究团队的远程实验能力。", "conclusion": "Verti-Arena填补了越野自主导航研究中缺乏标准化测试平台的空白，为该领域提供了可重复的实验环境和精确的数据，促进了全球研究合作和算法评估。"}}
{"id": "2508.07516", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.07516", "abs": "https://arxiv.org/abs/2508.07516", "authors": ["Keshav Varadarajan", "Tananun Songdechakraiwut"], "title": "Augmenting Bias Detection in LLMs Using Topological Data Analysis", "comment": "15 pages, 9 figures, 4 tables", "summary": "Recently, many bias detection methods have been proposed to determine the\nlevel of bias a large language model captures. However, tests to identify which\nparts of a large language model are responsible for bias towards specific\ngroups remain underdeveloped. In this study, we present a method using\ntopological data analysis to identify which heads in GPT-2 contribute to the\nmisrepresentation of identity groups present in the StereoSet dataset. We find\nthat biases for particular categories, such as gender or profession, are\nconcentrated in attention heads that act as hot spots. The metric we propose\ncan also be used to determine which heads capture bias for a specific group\nwithin a bias category, and future work could extend this method to help\nde-bias large language models.", "AI": {"tldr": "本研究提出一种基于拓扑数据分析（TDA）的方法，用于识别GPT-2模型中导致对特定身份群体产生偏见的注意力头。", "motivation": "尽管已有很多偏见检测方法，但识别大型语言模型中具体哪些部分对特定群体产生偏见的方法仍不成熟。本研究旨在填补这一空白。", "method": "研究采用拓扑数据分析（TDA）来识别GPT-2模型中导致StereoSet数据集中身份群体被错误表示的注意力头。同时，提出了一种新的度量标准来确定特定偏见类别中哪些头对特定群体捕获了偏见。", "result": "研究发现，针对特定类别（如性别或职业）的偏见集中在充当“热点”的注意力头中。所提出的度量标准也可用于确定模型中哪些头捕获了特定群体在某一偏见类别内的偏见。", "conclusion": "该方法能够有效识别大型语言模型中偏见的来源（具体到注意力头），未来可扩展应用于帮助消除大型语言模型中的偏见。"}}
{"id": "2508.06924", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06924", "abs": "https://arxiv.org/abs/2508.06924", "authors": ["Shihao Yuan", "Yahui Liu", "Yang Yue", "Jingyuan Zhang", "Wangmeng Zuo", "Qi Wang", "Fuzheng Zhang", "Guorui Zhou"], "title": "AR-GRPO: Training Autoregressive Image Generation Models via Reinforcement Learning", "comment": "27 pages, 15 figures", "summary": "Inspired by the success of reinforcement learning (RL) in refining large\nlanguage models (LLMs), we propose AR-GRPO, an approach to integrate online RL\ntraining into autoregressive (AR) image generation models. We adapt the Group\nRelative Policy Optimization (GRPO) algorithm to refine the vanilla\nautoregressive models' outputs by carefully designed reward functions that\nevaluate generated images across multiple quality dimensions, including\nperceptual quality, realism, and semantic fidelity. We conduct comprehensive\nexperiments on both class-conditional (i.e., class-to-image) and\ntext-conditional (i.e., text-to-image) image generation tasks, demonstrating\nthat our RL-enhanced framework significantly improves both the image quality\nand human preference of generated images compared to the standard AR baselines.\nOur results show consistent improvements across various evaluation metrics,\nestablishing the viability of RL-based optimization for AR image generation and\nopening new avenues for controllable and high-quality image synthesis. The\nsource codes and models are available at:\nhttps://github.com/Kwai-Klear/AR-GRPO.", "AI": {"tldr": "AR-GRPO是一种将在线强化学习（RL）训练集成到自回归（AR）图像生成模型中的方法，通过多维度奖励函数优化图像质量和人类偏好。", "motivation": "受强化学习在优化大型语言模型（LLMs）方面成功的启发，研究旨在将RL应用于自回归图像生成模型，以提升其输出质量。", "method": "提出AR-GRPO方法，将Group Relative Policy Optimization (GRPO) 算法应用于自回归模型。通过精心设计的奖励函数，从感知质量、真实感和语义保真度等多个维度评估生成的图像，并以此优化模型输出。", "result": "在类别条件（class-to-image）和文本条件（text-to-image）图像生成任务中，AR-GRPO显著提升了生成图像的质量和人类偏好，并在各种评估指标上显示出一致的改进，优于标准AR基线模型。", "conclusion": "研究结果证实了基于强化学习的优化对于自回归图像生成是可行的，为可控和高质量图像合成开辟了新途径。"}}
{"id": "2508.07834", "categories": ["cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2508.07834", "abs": "https://arxiv.org/abs/2508.07834", "authors": ["Mubaris Nadeem", "Johannes Zenkert", "Lisa Bender", "Christian Weber", "Madjid Fathi"], "title": "KIRETT: Knowledge-Graph-Based Smart Treatment Assistant for Intelligent Rescue Operations", "comment": "LWDA'23, KIRETT project, University of Siegen, Germany", "summary": "Over the years, the need for rescue operations throughout the world has\nincreased rapidly. Demographic changes and the resulting risk of injury or\nhealth disorders form the basis for emergency calls. In such scenarios, first\nresponders are in a rush to reach the patient in need, provide first aid, and\nsave lives. In these situations, they must be able to provide personalized and\noptimized healthcare in the shortest possible time and estimate the patients\ncondition with the help of freshly recorded vital data in an emergency\nsituation. However, in such a timedependent situation, first responders and\nmedical experts cannot fully grasp their knowledge and need assistance and\nrecommendation for further medical treatments. To achieve this, on the spot\ncalculated, evaluated, and processed knowledge must be made available to\nimprove treatments by first responders. The Knowledge Graph presented in this\narticle as a central knowledge representation provides first responders with an\ninnovative knowledge management that enables intelligent treatment\nrecommendations with an artificial intelligence-based pre-recognition of the\nsituation.", "AI": {"tldr": "本文提出一个基于知识图谱的系统，旨在为急救人员提供AI驱动的智能治疗建议，以应对紧急情况下的快速决策需求。", "motivation": "全球救援行动需求快速增长，人口结构变化导致伤病风险增加。在紧急情况下，急救人员需在最短时间内提供个性化和优化医疗服务，但时间压力下难以充分运用知识。因此，需要现场计算、评估和处理的知识来辅助急救人员改进治疗。", "method": "采用知识图谱作为核心知识表示形式，结合人工智能进行情境预识别，为急救人员提供创新的知识管理和智能治疗建议。", "result": "该知识图谱系统为急救人员提供了一种创新的知识管理方式，能够通过基于人工智能的情境预识别，实现智能化的治疗推荐。", "conclusion": "所提出的知识图谱系统能够有效辅助急救人员在紧急情况下快速获取和利用知识，通过AI驱动的智能推荐提高治疗效率和质量。"}}
{"id": "2508.08240", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08240", "abs": "https://arxiv.org/abs/2508.08240", "authors": ["Kaijun Wang", "Liqin Lu", "Mingyu Liu", "Jianuo Jiang", "Zeju Li", "Bolin Zhang", "Wancai Zheng", "Xinyi Yu", "Hao Chen", "Chunhua Shen"], "title": "ODYSSEY: Open-World Quadrupeds Exploration and Manipulation for Long-Horizon Tasks", "comment": null, "summary": "Language-guided long-horizon mobile manipulation has long been a grand\nchallenge in embodied semantic reasoning, generalizable manipulation, and\nadaptive locomotion. Three fundamental limitations hinder progress: First,\nalthough large language models have improved spatial reasoning and task\nplanning through semantic priors, existing implementations remain confined to\ntabletop scenarios, failing to address the constrained perception and limited\nactuation ranges of mobile platforms. Second, current manipulation strategies\nexhibit insufficient generalization when confronted with the diverse object\nconfigurations encountered in open-world environments. Third, while crucial for\npractical deployment, the dual requirement of maintaining high platform\nmaneuverability alongside precise end-effector control in unstructured settings\nremains understudied.\n  In this work, we present ODYSSEY, a unified mobile manipulation framework for\nagile quadruped robots equipped with manipulators, which seamlessly integrates\nhigh-level task planning with low-level whole-body control. To address the\nchallenge of egocentric perception in language-conditioned tasks, we introduce\na hierarchical planner powered by a vision-language model, enabling\nlong-horizon instruction decomposition and precise action execution. At the\ncontrol level, our novel whole-body policy achieves robust coordination across\nchallenging terrains. We further present the first benchmark for long-horizon\nmobile manipulation, evaluating diverse indoor and outdoor scenarios. Through\nsuccessful sim-to-real transfer, we demonstrate the system's generalization and\nrobustness in real-world deployments, underscoring the practicality of legged\nmanipulators in unstructured environments. Our work advances the feasibility of\ngeneralized robotic assistants capable of complex, dynamic tasks. Our project\npage: https://kaijwang.github.io/odyssey.github.io/", "AI": {"tldr": "本文提出了ODYSSEY，一个统一的移动操作框架，使配备机械臂的敏捷四足机器人能够进行语言引导的、长周期移动操作，并在现实世界中展现出强大的泛化性和鲁棒性。", "motivation": "当前语言引导的移动操作面临三大挑战：1) 大语言模型（LLM）在移动平台上的应用受限于桌面场景，无法应对受限感知和执行范围；2) 现有操作策略在开放世界多样对象配置下泛化能力不足；3) 在非结构化环境中，同时维持平台高机动性和末端执行器精确控制的研究不足。", "method": "本文提出了ODYSSEY，一个用于敏捷四足机器人（带机械臂）的统一移动操作框架，无缝整合了高级任务规划与低级全身控制。具体方法包括：1) 引入由视觉-语言模型驱动的层次化规划器，实现长周期指令分解和精确动作执行；2) 设计新颖的全身控制策略，在复杂地形上实现鲁棒的协调。", "result": "本文建立了首个长周期移动操作基准，评估了多样化的室内外场景。通过成功的仿真到现实迁移，证明了系统在实际部署中的泛化性和鲁棒性，突显了腿足操作机器人在非结构化环境中的实用性。", "conclusion": "本工作推进了通用机器人助手在复杂动态任务中可行性，为未来能够在非结构化环境中执行复杂、动态任务的通用机器人奠定了基础。"}}
{"id": "2508.07517", "categories": ["cs.CL", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.07517", "abs": "https://arxiv.org/abs/2508.07517", "authors": ["Joseph T. Colonel", "Baihan Lin"], "title": "Word Clouds as Common Voices: LLM-Assisted Visualization of Participant-Weighted Themes in Qualitative Interviews", "comment": null, "summary": "Word clouds are a common way to summarize qualitative interviews, yet\ntraditional frequency-based methods often fail in conversational contexts: they\nsurface filler words, ignore paraphrase, and fragment semantically related\nideas. This limits their usefulness in early-stage analysis, when researchers\nneed fast, interpretable overviews of what participant actually said. We\nintroduce ThemeClouds, an open-source visualization tool that uses large\nlanguage models (LLMs) to generate thematic, participant-weighted word clouds\nfrom dialogue transcripts. The system prompts an LLM to identify concept-level\nthemes across a corpus and then counts how many unique participants mention\neach topic, yielding a visualization grounded in breadth of mention rather than\nraw term frequency. Researchers can customize prompts and visualization\nparameters, providing transparency and control. Using interviews from a user\nstudy comparing five recording-device configurations (31 participants; 155\ntranscripts, Whisper ASR), our approach surfaces more actionable device\nconcerns than frequency clouds and topic-modeling baselines (e.g., LDA,\nBERTopic). We discuss design trade-offs for integrating LLM assistance into\nqualitative workflows, implications for interpretability and researcher agency,\nand opportunities for interactive analyses such as per-condition contrasts\n(``diff clouds'').", "AI": {"tldr": "ThemeClouds是一个开源可视化工具，利用大型语言模型（LLM）从对话文本中生成主题化、参与者加权的词云，以解决传统词云在会话语境中的局限性，提供更具可操作性的分析。", "motivation": "传统的词云方法在定性访谈的会话语境中存在不足，例如突出填充词、忽略意译、碎片化语义相关概念，这限制了它们在早期分析中的实用性，研究人员需要快速、可解释的参与者实际表达内容的概览。", "method": "引入了ThemeClouds工具，它使用LLM识别语料库中的概念级主题，然后统计提及每个主题的独立参与者数量，从而生成基于提及广度而非原始词频的可视化。研究人员可以自定义提示和可视化参数，以提供透明度和控制。", "result": "通过对用户研究访谈（31名参与者，155份文本）的应用，ThemeClouds比基于频率的词云和主题建模基线（如LDA、BERTopic）能发现更多可操作的设备问题。", "conclusion": "论文讨论了将LLM辅助集成到定性工作流中的设计权衡、对可解释性和研究者能动性的影响，以及交互式分析（如条件对比“diff clouds”）的机会。"}}
{"id": "2508.06937", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06937", "abs": "https://arxiv.org/abs/2508.06937", "authors": ["Weiyan Xie", "Han Gao", "Didan Deng", "Kaican Li", "April Hua Liu", "Yongxiang Huang", "Nevin L. Zhang"], "title": "CannyEdit: Selective Canny Control and Dual-Prompt Guidance for Training-Free Image Editing", "comment": "Project Page: vaynexie.github.io/CannyEdit/", "summary": "Recent advances in text-to-image (T2I) models have enabled training-free\nregional image editing by leveraging the generative priors of foundation\nmodels. However, existing methods struggle to balance text adherence in edited\nregions, context fidelity in unedited areas, and seamless integration of edits.\nWe introduce CannyEdit, a novel training-free framework that addresses these\nchallenges through two key innovations: (1) Selective Canny Control, which\nmasks the structural guidance of Canny ControlNet in user-specified editable\nregions while strictly preserving details of the source images in unedited\nareas via inversion-phase ControlNet information retention. This enables\nprecise, text-driven edits without compromising contextual integrity. (2)\nDual-Prompt Guidance, which combines local prompts for object-specific edits\nwith a global target prompt to maintain coherent scene interactions. On\nreal-world image editing tasks (addition, replacement, removal), CannyEdit\noutperforms prior methods like KV-Edit, achieving a 2.93 to 10.49 percent\nimprovement in the balance of text adherence and context fidelity. In terms of\nediting seamlessness, user studies reveal only 49.2 percent of general users\nand 42.0 percent of AIGC experts identified CannyEdit's results as AI-edited\nwhen paired with real images without edits, versus 76.08 to 89.09 percent for\ncompetitor methods.", "AI": {"tldr": "CannyEdit是一个免训练的区域图像编辑框架，通过选择性Canny控制和双提示引导，解决了现有方法在文本依从性、上下文保真度和编辑无缝性方面的不足，显著提升了编辑效果。", "motivation": "现有的文本到图像（T2I）区域图像编辑方法难以平衡编辑区域的文本依从性、未编辑区域的上下文保真度以及编辑的无缝集成。", "method": "CannyEdit是一个免训练的框架，包含两项关键创新：1) 选择性Canny控制：在用户指定的可编辑区域内屏蔽Canny ControlNet的结构引导，同时通过反演阶段ControlNet信息保留严格保持源图像在未编辑区域的细节。2) 双提示引导：结合用于特定对象编辑的局部提示和用于保持场景连贯性的全局目标提示。", "result": "在真实世界图像编辑任务（添加、替换、移除）中，CannyEdit优于KV-Edit等现有方法，在文本依从性和上下文保真度之间平衡性方面提高了2.93%至10.49%。用户研究显示，在与未编辑的真实图像配对时，只有49.2%的普通用户和42.0%的AIGC专家能识别出CannyEdit的结果是AI编辑的，而竞争方法则为76.08%至89.09%。", "conclusion": "CannyEdit有效解决了区域图像编辑的挑战，实现了精确的、文本驱动的编辑，同时保持了高上下文完整性和无缝集成。它在平衡编辑质量指标和产生更自然外观的编辑方面显著优于现有方法。"}}
{"id": "2508.07932", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07932", "abs": "https://arxiv.org/abs/2508.07932", "authors": ["Yi Zhai", "Zhiqiang Wei", "Ruohan Li", "Keyu Pan", "Shuo Liu", "Lu Zhang", "Jianmin Ji", "Wuyang Zhang", "Yu Zhang", "Yanyong Zhang"], "title": "\\(X\\)-evolve: Solution space evolution powered by large language models", "comment": null, "summary": "While combining large language models (LLMs) with evolutionary algorithms\n(EAs) shows promise for solving complex optimization problems, current\napproaches typically evolve individual solutions, often incurring high LLM call\ncosts. We introduce \\(X\\)-evolve, a paradigm-shifting method that instead\nevolves solution spaces \\(X\\) (sets of individual solutions) - subsets of the\noverall search space \\(S\\). In \\(X\\)-evolve, LLMs generate tunable programs\nwherein certain code snippets, designated as parameters, define a tunable\nsolution space. A score-based search algorithm then efficiently explores this\nparametrically defined space, guided by feedback from objective function\nscores. This strategy enables broader and more efficient exploration, which can\npotentially accelerate convergence at a much lower search cost, requiring up to\ntwo orders of magnitude fewer LLM calls than prior leading methods. We\ndemonstrate \\(X\\)-evolve's efficacy across three distinct hard optimization\nproblems. For the cap set problem, we discover a larger partial admissible set,\nestablishing a new tighter asymptotic lower bound for the cap set constant (\\(C\n\\ge 2.2203\\)). In information theory, we uncover a larger independent set for\nthe 15-vertex cycle graph (\\(\\mathcal{C}_{15}^{\\boxtimes 5}\\), size 19,946),\nthereby raising the known lower bound on its Shannon capacity. Furthermore, for\nthe NP-hard online bin packing problem, we generate heuristics that\nconsistently outperform standard strategies across established benchmarks. By\nevolving solution spaces, our method considerably improves search\neffectiveness, making it possible to tackle high-dimensional problems that were\npreviously computationally prohibitive.", "AI": {"tldr": "X-evolve是一种新范式，结合LLM和进化算法，通过进化“解空间”而非单个解来解决复杂优化问题，显著降低LLM调用成本并提高搜索效率。", "motivation": "现有结合大型语言模型（LLM）和进化算法（EA）的方法通常进化单个解，导致LLM调用成本高昂，限制了在高维问题上的应用。", "method": "X-evolve方法进化的是“解空间”（即个体解的集合），而非单个解。LLM生成可调程序，其中特定代码片段作为参数定义可调解空间。然后，一个基于分数的搜索算法根据目标函数的分数反馈，高效探索这个参数定义的空间。", "result": "X-evolve在三个难题上展现了有效性：1) 在Cap Set问题上，发现更大的部分可容许集，将Cap Set常数渐近下限提高到C ≥ 2.2203。2) 在信息论中，为15顶点循环图找到更大的独立集（大小19,946），提高了其香农容量的已知下限。3) 在NP难的在线装箱问题上，生成的启发式算法持续优于标准策略。该方法比现有领先方法减少了高达两个数量级的LLM调用。", "conclusion": "通过进化解空间，X-evolve显著提高了搜索效率，使得以前计算上难以解决的高维问题变得可行，从而大大提升了搜索的有效性。"}}
{"id": "2508.08241", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.08241", "abs": "https://arxiv.org/abs/2508.08241", "authors": ["Takara E. Truong", "Qiayuan Liao", "Xiaoyu Huang", "Guy Tevet", "C. Karen Liu", "Koushil Sreenath"], "title": "BeyondMimic: From Motion Tracking to Versatile Humanoid Control via Guided Diffusion", "comment": "9 pages, 1 figure", "summary": "Learning skills from human motions offers a promising path toward\ngeneralizable policies for whole-body humanoid control, yet two key\ncornerstones are missing: (1) a high-quality motion tracking framework that\nfaithfully transforms large-scale kinematic references into robust and\nextremely dynamic motions on real hardware, and (2) a distillation approach\nthat can effectively learn these motion primitives and compose them to solve\ndownstream tasks. We address these gaps with BeyondMimic, the first real-world\nframework to learn from human motions for versatile and naturalistic humanoid\ncontrol via guided diffusion. Our framework provides a motion tracking pipeline\ncapable of challenging skills such as jumping spins, sprinting, and cartwheels\nwith state-of-the-art motion quality. Moving beyond mimicking existing motions\nand synthesize novel ones, we further introduce a unified diffusion policy that\nenables zero-shot task-specific control at test time using simple cost\nfunctions. Deployed on hardware, BeyondMimic performs diverse tasks at test\ntime, including waypoint navigation, joystick teleoperation, and obstacle\navoidance, bridging sim-to-real motion tracking and flexible synthesis of human\nmotion primitives for whole-body control. https://beyondmimic.github.io/.", "AI": {"tldr": "BeyondMimic是一个用于类人机器人全身控制的真实世界框架，它通过引导扩散从人类动作中学习技能，实现了高质量的动作跟踪和新颖动作的零样本合成，并在硬件上成功执行了多种任务。", "motivation": "当前类人机器人全身控制领域缺少两个关键支柱：1) 能将大规模运动参考转化为真实硬件上鲁棒且动态运动的高质量运动跟踪框架；2) 能有效学习和组合这些运动基元以解决下游任务的蒸馏方法。", "method": "本文提出了BeyondMimic框架，包含一个运动跟踪管道，能够实现跳跃旋转、冲刺和侧手翻等高难度技能；并引入了一种统一的扩散策略，该策略通过简单的成本函数在测试时实现零样本任务特定控制，超越了现有动作的模仿，能够合成新颖动作。", "result": "BeyondMimic在真实硬件上部署后，展现了最先进的运动质量，成功执行了包括路径点导航、摇杆远程操作和避障在内的多种任务，有效弥合了从仿真到现实的运动跟踪，并实现了人体运动基元的灵活合成以进行全身控制。", "conclusion": "BeyondMimic成功解决了类人机器人全身控制中运动跟踪质量和技能合成的挑战，通过引导扩散实现了从人类动作中学习，为多功能和自然主义的类人机器人控制提供了一条有效途径。"}}
{"id": "2508.07534", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.07534", "abs": "https://arxiv.org/abs/2508.07534", "authors": ["Jia Deng", "Jie Chen", "Zhipeng Chen", "Daixuan Cheng", "Fei Bai", "Beichen Zhang", "Yinqian Min", "Yanzipeng Gao", "Wayne Xin Zhao", "Ji-Rong Wen"], "title": "From Trial-and-Error to Improvement: A Systematic Analysis of LLM Exploration Mechanisms in RLVR", "comment": "27pages,25figures. arXiv admin note: text overlap with\n  arXiv:2508.02260", "summary": "Reinforcement learning with verifiable rewards (RLVR) has emerged as a\npowerful paradigm for enhancing the reasoning capabilities of large language\nmodels (LLMs). Unlike traditional RL approaches, RLVR leverages rule-based\nfeedback to guide LLMs in generating and refining complex reasoning chains -- a\nprocess critically dependent on effective exploration strategies. While prior\nwork has demonstrated RLVR's empirical success, the fundamental mechanisms\ngoverning LLMs' exploration behaviors remain underexplored. This technical\nreport presents a systematic investigation of exploration capacities in RLVR,\ncovering four main aspects: (1) exploration space shaping, where we develop\nquantitative metrics to characterize LLMs' capability boundaries; (2)\nentropy-performance exchange, analyzed across training stages, individual\ninstances, and token-level patterns; and (3) RL performance optimization,\nexamining methods to effectively translate exploration gains into measurable\nimprovements. By unifying previously identified insights with new empirical\nevidence, this work aims to provide a foundational framework for advancing RLVR\nsystems.", "AI": {"tldr": "本技术报告系统性地研究了可验证奖励强化学习（RLVR）中大型语言模型（LLM）的探索能力，旨在为RLVR系统提供基础框架。", "motivation": "RLVR在增强LLM推理能力方面表现出色，但LLM的探索行为机制尚未得到充分探索，而有效的探索策略对于生成和完善复杂推理链至关重要。", "method": "通过系统性调查，涵盖四个主要方面：1) 探索空间塑造，开发量化指标来表征LLM的能力边界；2) 熵-性能交换，分析训练阶段、个体实例和token级别的模式；3) RL性能优化，研究将探索收益转化为可衡量改进的方法。", "result": "通过统一先前已识别的见解和新的实证证据，旨在为推进RLVR系统提供一个基础框架。报告本身是关于调查的范围和目的，而非具体实验结果。", "conclusion": "本工作旨在通过对RLVR中LLM探索能力的系统性调查，提供一个推进RLVR系统的基础框架。"}}
{"id": "2508.06959", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06959", "abs": "https://arxiv.org/abs/2508.06959", "authors": ["Qin Xu", "Lili Zhu", "Xiaoxia Cheng", "Bo Jiang"], "title": "Beyond Frequency: Seeing Subtle Cues Through the Lens of Spatial Decomposition for Fine-Grained Visual Classification", "comment": null, "summary": "The crux of resolving fine-grained visual classification (FGVC) lies in\ncapturing discriminative and class-specific cues that correspond to subtle\nvisual characteristics. Recently, frequency decomposition/transform based\napproaches have attracted considerable interests since its appearing\ndiscriminative cue mining ability. However, the frequency-domain methods are\nbased on fixed basis functions, lacking adaptability to image content and\nunable to dynamically adjust feature extraction according to the discriminative\nrequirements of different images. To address this, we propose a novel method\nfor FGVC, named Subtle-Cue Oriented Perception Engine (SCOPE), which adaptively\nenhances the representational capability of low-level details and high-level\nsemantics in the spatial domain, breaking through the limitations of fixed\nscales in the frequency domain and improving the flexibility of multi-scale\nfusion. The core of SCOPE lies in two modules: the Subtle Detail Extractor\n(SDE), which dynamically enhances subtle details such as edges and textures\nfrom shallow features, and the Salient Semantic Refiner (SSR), which learns\nsemantically coherent and structure-aware refinement features from the\nhigh-level features guided by the enhanced shallow features. The SDE and SSR\nare cascaded stage-by-stage to progressively combine local details with global\nsemantics. Extensive experiments demonstrate that our method achieves new\nstate-of-the-art on four popular fine-grained image classification benchmarks.", "AI": {"tldr": "该论文提出了一种名为SCOPE的新方法，用于细粒度视觉分类（FGVC）。它通过在空间域自适应地增强低级细节和高级语义，克服了传统频域方法的局限性，实现了最先进的性能。", "motivation": "现有的基于频率分解/变换的细粒度分类方法依赖于固定的基函数，缺乏对图像内容的适应性，无法根据不同图像的判别需求动态调整特征提取，这限制了它们捕获细微视觉特征的能力。", "method": "本文提出了一种名为细微线索导向感知引擎（SCOPE）的新方法。其核心是两个模块：细微细节提取器（SDE），用于从浅层特征中动态增强边缘和纹理等细微细节；以及显著语义细化器（SSR），在增强的浅层特征引导下，从高层特征中学习语义一致且结构感知的细化特征。SDE和SSR逐级级联，以逐步结合局部细节和全局语义。", "result": "广泛的实验表明，该方法在四个流行的细粒度图像分类基准测试上均达到了新的最先进水平。", "conclusion": "SCOPE通过在空间域自适应地增强特征表示，有效解决了频域方法在细粒度分类中缺乏适应性的问题，显著提升了分类性能，证明了其在捕获细微判别性线索方面的优越性。"}}
{"id": "2508.07941", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07941", "abs": "https://arxiv.org/abs/2508.07941", "authors": ["Olivier Poulet", "Frédéric Guinand", "François Guérin"], "title": "Deep Reinforcement Learning with anticipatory reward in LSTM for Collision Avoidance of Mobile Robots", "comment": null, "summary": "This article proposes a collision risk anticipation method based on\nshort-term prediction of the agents position. A Long Short-Term Memory (LSTM)\nmodel, trained on past trajectories, is used to estimate the next position of\neach robot. This prediction allows us to define an anticipated collision risk\nby dynamically modulating the reward of a Deep Q-Learning Network (DQN) agent.\nThe approach is tested in a constrained environment, where two robots move\nwithout communication or identifiers. Despite a limited sampling frequency (1\nHz), the results show a significant decrease of the collisions number and a\nstability improvement. The proposed method, which is computationally\ninexpensive, appears particularly attractive for implementation on embedded\nsystems.", "AI": {"tldr": "本文提出了一种基于LSTM短期预测和DQN奖励调制的碰撞风险预测方法，有效降低了机器人碰撞并提高了稳定性。", "motivation": "在无通信或标识的受限环境中，机器人自主避障和降低碰撞是重要挑战，尤其需要计算成本低廉的方法以适应嵌入式系统。", "method": "使用LSTM模型根据历史轨迹预测机器人的未来位置，并基于此预测定义预期的碰撞风险。然后，通过动态调制深度Q学习网络（DQN）代理的奖励来整合这种风险感知。该方法在两个无通信机器人的受限环境中进行了测试。", "result": "即使在采样频率较低（1 Hz）的情况下，该方法也能显著减少碰撞次数并提高系统稳定性。此外，该方法计算成本低廉。", "conclusion": "所提出的基于短期位置预测的碰撞风险预测方法，通过动态调制DQN奖励，有效提高了机器人避障性能和系统稳定性，且计算效率高，特别适用于嵌入式系统实现。"}}
{"id": "2508.07089", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07089", "abs": "https://arxiv.org/abs/2508.07089", "authors": ["Sandro Papais", "Letian Wang", "Brian Cheong", "Steven L. Waslander"], "title": "ForeSight: Multi-View Streaming Joint Object Detection and Trajectory Forecasting", "comment": "Accepted to ICCV 2025", "summary": "We introduce ForeSight, a novel joint detection and forecasting framework for\nvision-based 3D perception in autonomous vehicles. Traditional approaches treat\ndetection and forecasting as separate sequential tasks, limiting their ability\nto leverage temporal cues. ForeSight addresses this limitation with a\nmulti-task streaming and bidirectional learning approach, allowing detection\nand forecasting to share query memory and propagate information seamlessly. The\nforecast-aware detection transformer enhances spatial reasoning by integrating\ntrajectory predictions from a multiple hypothesis forecast memory queue, while\nthe streaming forecast transformer improves temporal consistency using past\nforecasts and refined detections. Unlike tracking-based methods, ForeSight\neliminates the need for explicit object association, reducing error propagation\nwith a tracking-free model that efficiently scales across multi-frame\nsequences. Experiments on the nuScenes dataset show that ForeSight achieves\nstate-of-the-art performance, achieving an EPA of 54.9%, surpassing previous\nmethods by 9.3%, while also attaining the best mAP and minADE among multi-view\ndetection and forecasting models.", "AI": {"tldr": "ForeSight是一个新颖的联合检测与预测框架，用于自动驾驶中的3D感知，它通过多任务流式和双向学习，实现了检测与预测之间信息的无缝共享和传播，并且无需显式目标关联，达到了最先进的性能。", "motivation": "传统的自动驾驶3D感知方法将检测和预测视为独立的顺序任务，这限制了它们利用时间线索的能力，导致信息无法有效共享和传播，且存在误差累积问题。", "method": "ForeSight采用多任务流式和双向学习方法，使检测和预测共享查询内存并无缝传播信息。它包含一个预测感知检测Transformer，通过整合多假设预测记忆队列中的轨迹预测来增强空间推理；以及一个流式预测Transformer，利用过去的预测和精细化检测来提高时间一致性。该模型是无跟踪的，无需显式目标关联，从而减少了误差传播。", "result": "在nuScenes数据集上的实验表明，ForeSight实现了最先进的性能，EPA达到54.9%，超越现有方法9.3%。同时，它在多视图检测和预测模型中也获得了最佳的mAP和minADE。", "conclusion": "ForeSight通过其创新的联合检测和预测框架，有效克服了传统方法的局限性，实现了卓越的性能，并在自动驾驶3D感知领域树立了新的标杆，为未来的研究提供了新的方向。"}}
{"id": "2508.07592", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07592", "abs": "https://arxiv.org/abs/2508.07592", "authors": ["Puspesh Kumar Srivastava", "Uddeshya Raj", "Praveen Patel", "/Shubham Kumar Nigam", "Noel Shallum", "Arnab Bhattacharya"], "title": "IBPS: Indian Bail Prediction System", "comment": null, "summary": "Bail decisions are among the most frequently adjudicated matters in Indian\ncourts, yet they remain plagued by subjectivity, delays, and inconsistencies.\nWith over 75% of India's prison population comprising undertrial prisoners,\nmany from socioeconomically disadvantaged backgrounds, the lack of timely and\nfair bail adjudication exacerbates human rights concerns and contributes to\nsystemic judicial backlog. In this paper, we present the Indian Bail Prediction\nSystem (IBPS), an AI-powered framework designed to assist in bail\ndecision-making by predicting outcomes and generating legally sound rationales\nbased solely on factual case attributes and statutory provisions. We curate and\nrelease a large-scale dataset of 150,430 High Court bail judgments, enriched\nwith structured annotations such as age, health, criminal history, crime\ncategory, custody duration, statutes, and judicial reasoning. We fine-tune a\nlarge language model using parameter-efficient techniques and evaluate its\nperformance across multiple configurations, with and without statutory context,\nand with RAG. Our results demonstrate that models fine-tuned with statutory\nknowledge significantly outperform baselines, achieving strong accuracy and\nexplanation quality, and generalize well to a test set independently annotated\nby legal experts. IBPS offers a transparent, scalable, and reproducible\nsolution to support data-driven legal assistance, reduce bail delays, and\npromote procedural fairness in the Indian judicial system.", "AI": {"tldr": "本文提出了印度保释预测系统（IBPS），一个基于AI的框架，旨在通过预测结果和生成法律依据来辅助保释决策，以解决印度保释判决中的主观性、延误和不一致问题。", "motivation": "印度法院的保释判决普遍存在主观性、延误和不一致性，导致超过75%的监狱人口是候审犯，其中许多来自社会经济弱势群体。这种不及时和不公平的保释裁决加剧了人权问题并导致司法系统积压。", "method": "研究构建并发布了一个包含150,430份高等法院保释判决的大规模数据集，并进行了结构化标注（如年龄、健康、犯罪史、罪行类别、羁押期限、法规和司法推理）。研究通过参数高效技术微调了一个大型语言模型，并评估了其在有无法规上下文以及RAG（检索增强生成）配置下的性能。", "result": "结果表明，通过法规知识微调的模型显著优于基线模型，取得了高准确性和解释质量，并且能够很好地泛化到由法律专家独立标注的测试集。", "conclusion": "IBPS提供了一个透明、可扩展和可复现的解决方案，以支持数据驱动的法律援助，减少保释延误，并促进印度司法系统中的程序公平性。"}}
{"id": "2508.06964", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06964", "abs": "https://arxiv.org/abs/2508.06964", "authors": ["Qiwei Tian", "Chenhao Lin", "Zhengyu Zhao", "Qian Li", "Shuai Liu", "Chao Shen"], "title": "Adversarial Video Promotion Against Text-to-Video Retrieval", "comment": null, "summary": "Thanks to the development of cross-modal models, text-to-video retrieval\n(T2VR) is advancing rapidly, but its robustness remains largely unexamined.\nExisting attacks against T2VR are designed to push videos away from queries,\ni.e., suppressing the ranks of videos, while the attacks that pull videos\ntowards selected queries, i.e., promoting the ranks of videos, remain largely\nunexplored. These attacks can be more impactful as attackers may gain more\nviews/clicks for financial benefits and widespread (mis)information. To this\nend, we pioneer the first attack against T2VR to promote videos adversarially,\ndubbed the Video Promotion attack (ViPro). We further propose Modal Refinement\n(MoRe) to capture the finer-grained, intricate interaction between visual and\ntextual modalities to enhance black-box transferability. Comprehensive\nexperiments cover 2 existing baselines, 3 leading T2VR models, 3 prevailing\ndatasets with over 10k videos, evaluated under 3 scenarios. All experiments are\nconducted in a multi-target setting to reflect realistic scenarios where\nattackers seek to promote the video regarding multiple queries simultaneously.\nWe also evaluated our attacks for defences and imperceptibility. Overall, ViPro\nsurpasses other baselines by over $30/10/4\\%$ for white/grey/black-box settings\non average. Our work highlights an overlooked vulnerability, provides a\nqualitative analysis on the upper/lower bound of our attacks, and offers\ninsights into potential counterplays. Code will be publicly available at\nhttps://github.com/michaeltian108/ViPro.", "AI": {"tldr": "该论文提出了首个针对文本到视频检索（T2VR）的视频推广攻击（ViPro），以提高视频排名，并引入Modal Refinement (MoRe) 增强黑盒可迁移性，揭示了T2VR模型中被忽视的漏洞。", "motivation": "现有对T2VR的攻击主要集中于降低视频排名，而提高视频排名的攻击（可能带来经济利益或信息传播）仍未被充分探索。T2VR模型的鲁棒性也普遍缺乏检验。", "method": "本文提出了一种名为ViPro的视频推广攻击，旨在对抗性地提高视频在T2VR中的排名。为增强黑盒可迁移性，进一步提出了Modal Refinement (MoRe) 方法，用于捕捉视觉和文本模态之间更细粒度的复杂交互。实验在多目标设置下，涵盖了现有基线、主流T2VR模型和流行数据集，并在白盒、灰盒、黑盒三种场景下进行评估。", "result": "ViPro在白盒、灰盒和黑盒设置下，平均分别超越其他基线30%、10%和4%。研究还提供了攻击上限/下限的定性分析。", "conclusion": "该工作揭示了T2VR中一个被忽视的漏洞，为理解攻击能力提供了分析，并为潜在的防御策略提供了见解。"}}
{"id": "2508.07950", "categories": ["cs.AI", "cs.CV", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.07950", "abs": "https://arxiv.org/abs/2508.07950", "authors": ["Chen Shen", "Wanqing Zhang", "Kehan Li", "Erwen Huang", "Haitao Bi", "Aiying Fan", "Yiwen Shen", "Hongmei Dong", "Ji Zhang", "Yuming Shao", "Zengjia Liu", "Xinshe Liu", "Tao Li", "Chunxia Yan", "Shuanliang Fan", "Di Wu", "Jianhua Ma", "Bin Cong", "Zhenyuan Wang", "Chunfeng Lian"], "title": "FEAT: A Multi-Agent Forensic AI System with Domain-Adapted Large Language Model for Automated Cause-of-Death Analysis", "comment": "18pages, 6 figures", "summary": "Forensic cause-of-death determination faces systemic challenges, including\nworkforce shortages and diagnostic variability, particularly in high-volume\nsystems like China's medicolegal infrastructure. We introduce FEAT (ForEnsic\nAgenT), a multi-agent AI framework that automates and standardizes death\ninvestigations through a domain-adapted large language model. FEAT's\napplication-oriented architecture integrates: (i) a central Planner for task\ndecomposition, (ii) specialized Local Solvers for evidence analysis, (iii) a\nMemory & Reflection module for iterative refinement, and (iv) a Global Solver\nfor conclusion synthesis. The system employs tool-augmented reasoning,\nhierarchical retrieval-augmented generation, forensic-tuned LLMs, and\nhuman-in-the-loop feedback to ensure legal and medical validity. In evaluations\nacross diverse Chinese case cohorts, FEAT outperformed state-of-the-art AI\nsystems in both long-form autopsy analyses and concise cause-of-death\nconclusions. It demonstrated robust generalization across six geographic\nregions and achieved high expert concordance in blinded validations. Senior\npathologists validated FEAT's outputs as comparable to those of human experts,\nwith improved detection of subtle evidentiary nuances. To our knowledge, FEAT\nis the first LLM-based AI agent system dedicated to forensic medicine, offering\nscalable, consistent death certification while maintaining expert-level rigor.\nBy integrating AI efficiency with human oversight, this work could advance\nequitable access to reliable medicolegal services while addressing critical\ncapacity constraints in forensic systems.", "AI": {"tldr": "FEAT是一个多智能体AI框架，利用领域适应的大语言模型，自动化和标准化法医死亡调查，在评估中表现优于现有系统，并获得专家认可，有望解决法医系统的人力短缺和诊断差异问题。", "motivation": "法医死因鉴定面临系统性挑战，包括劳动力短缺和诊断变异性，尤其是在中国等高业务量的法医体系中。现有问题促使研究开发一种能自动化和标准化死亡调查的AI系统。", "method": "FEAT（ForEnsic AgenT）是一个多智能体AI框架，基于领域适应的大语言模型。其应用导向架构包括：中央规划器（任务分解）、专业本地求解器（证据分析）、记忆与反思模块（迭代优化）和全局求解器（结论综合）。系统采用工具增强推理、分层检索增强生成、法医调优的LLM以及人机交互反馈机制，以确保法律和医学的有效性。", "result": "在对中国不同案例队列的评估中，FEAT在长篇尸检分析和简洁死因结论方面均优于最先进的AI系统。它在六个地理区域表现出强大的泛化能力，并在盲法验证中实现了高专家一致性。资深病理学家验证FEAT的输出与人类专家相当，并能更好地检测细微的证据差异。", "conclusion": "FEAT是首个基于LLM的法医学AI智能体系统，能够提供可扩展、一致且保持专家级严谨性的死亡证明。通过整合AI效率与人类监督，这项工作有望促进公平获得可靠的法医服务，并解决法医系统中关键的能力限制问题。"}}
{"id": "2508.07626", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07626", "abs": "https://arxiv.org/abs/2508.07626", "authors": ["Dejie Yang", "Zijing Zhao", "Yang Liu"], "title": "AR-VRM: Imitating Human Motions for Visual Robot Manipulation with Analogical Reasoning", "comment": "Accepted by ICCV2025", "summary": "Visual Robot Manipulation (VRM) aims to enable a robot to follow natural\nlanguage instructions based on robot states and visual observations, and\ntherefore requires costly multi-modal data. To compensate for the deficiency of\nrobot data, existing approaches have employed vision-language pretraining with\nlarge-scale data. However, they either utilize web data that differs from\nrobotic tasks, or train the model in an implicit way (e.g., predicting future\nframes at the pixel level), thus showing limited generalization ability under\ninsufficient robot data. In this paper, we propose to learn from large-scale\nhuman action video datasets in an explicit way (i.e., imitating human actions\nfrom hand keypoints), introducing Visual Robot Manipulation with Analogical\nReasoning (AR-VRM). To acquire action knowledge explicitly from human action\nvideos, we propose a keypoint Vision-Language Model (VLM) pretraining scheme,\nenabling the VLM to learn human action knowledge and directly predict human\nhand keypoints. During fine-tuning on robot data, to facilitate the robotic arm\nin imitating the action patterns of human motions, we first retrieve human\naction videos that perform similar manipulation tasks and have similar\nhistorical observations , and then learn the Analogical Reasoning (AR) map\nbetween human hand keypoints and robot components. Taking advantage of focusing\non action keypoints instead of irrelevant visual cues, our method achieves\nleading performance on the CALVIN benchmark {and real-world experiments}. In\nfew-shot scenarios, our AR-VRM outperforms previous methods by large margins ,\nunderscoring the effectiveness of explicitly imitating human actions under data\nscarcity.", "AI": {"tldr": "该研究提出AR-VRM，通过显式模仿人类动作（手部关键点）从大规模人类动作视频中学习，以提升视觉机器人操作在数据稀缺时的泛化能力。", "motivation": "现有视觉机器人操作（VRM）方法依赖昂贵的多模态机器人数据，而通过大规模数据进行视觉-语言预训练时，要么使用与机器人任务不符的网页数据，要么以隐式方式（如预测未来帧）训练，导致在机器人数据不足时泛化能力有限。", "method": "提出AR-VRM（Visual Robot Manipulation with Analogical Reasoning）。首先，设计关键点视觉-语言模型（VLM）预训练方案，使其能从人类动作视频中学习并直接预测人类手部关键点。在机器人数据微调阶段，通过检索执行相似操作且具有相似历史观测的人类动作视频，学习人类手部关键点与机器人部件之间的类比推理（AR）映射，从而使机器人模仿人类动作模式。", "result": "该方法在CALVIN基准测试和真实世界实验中取得了领先性能。在少样本场景下，AR-VRM显著优于现有方法。", "conclusion": "研究结果强调了在数据稀缺情况下，通过显式模仿人类动作（关注动作关键点而非无关视觉线索）来提升视觉机器人操作的有效性。"}}
{"id": "2508.07598", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.07598", "abs": "https://arxiv.org/abs/2508.07598", "authors": ["Ziheng Li", "Zhi-Hong Deng"], "title": "Keyword-Centric Prompting for One-Shot Event Detection with Self-Generated Rationale Enhancements", "comment": "ECAI 2025", "summary": "Although the LLM-based in-context learning (ICL) paradigm has demonstrated\nconsiderable success across various natural language processing tasks, it\nencounters challenges in event detection. This is because LLMs lack an accurate\nunderstanding of event triggers and tend to make over-interpretation, which\ncannot be effectively corrected through in-context examples alone. In this\npaper, we focus on the most challenging one-shot setting and propose KeyCP++, a\nkeyword-centric chain-of-thought prompting approach. KeyCP++ addresses the\nweaknesses of conventional ICL by automatically annotating the logical gaps\nbetween input text and detection results for the demonstrations. Specifically,\nto generate in-depth and meaningful rationale, KeyCP++ constructs a trigger\ndiscrimination prompting template. It incorporates the exemplary triggers\n(a.k.a keywords) into the prompt as the anchor to simply trigger profiling, let\nLLM propose candidate triggers, and justify each candidate. These\npropose-and-judge rationales help LLMs mitigate over-reliance on the keywords\nand promote detection rule learning. Extensive experiments demonstrate the\neffectiveness of our approach, showcasing significant advancements in one-shot\nevent detection.", "AI": {"tldr": "针对LLM在事件检测中触发词理解不足和过度解读的问题，本文提出了KeyCP++，一种基于关键词的思维链提示方法，用于改进单次事件检测，通过深入的理由生成来帮助LLM更好地识别和判断事件触发词。", "motivation": "LLM的上下文学习（ICL）在事件检测中表现不佳，原因在于LLM缺乏对事件触发词的准确理解，并且容易过度解读，这些问题仅靠上下文示例无法有效纠正。本文关注最具挑战性的单次（one-shot）设置。", "method": "KeyCP++是一种关键词中心化的思维链提示方法。它通过自动标注输入文本与检测结果之间的逻辑差距来生成深入且有意义的理由。具体而言，KeyCP++构建了一个触发词判别提示模板，将示例触发词（关键词）作为锚点融入提示中，让LLM提出候选触发词并对每个候选进行判断。这种“提出-判断”的理由生成过程有助于LLM减轻对关键词的过度依赖，并促进检测规则的学习。", "result": "广泛的实验证明了KeyCP++方法的有效性，在单次事件检测方面取得了显著进展。", "conclusion": "KeyCP++通过其独特的关键词中心化思维链提示方法，有效解决了LLM在单次事件检测中触发词理解不准确和过度解读的挑战，显著提升了检测性能。"}}
{"id": "2508.06968", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2508.06968", "abs": "https://arxiv.org/abs/2508.06968", "authors": ["Ulas Gunes", "Matias Turkulainen", "Juho Kannala", "Esa Rahtu"], "title": "Evaluating Fisheye-Compatible 3D Gaussian Splatting Methods on Real Images Beyond 180 Degree Field of View", "comment": null, "summary": "We present the first evaluation of fisheye-based 3D Gaussian Splatting\nmethods, Fisheye-GS and 3DGUT, on real images with fields of view exceeding 180\ndegree. Our study covers both indoor and outdoor scenes captured with 200\ndegree fisheye cameras and analyzes how each method handles extreme distortion\nin real world settings. We evaluate performance under varying fields of view\n(200 degree, 160 degree, and 120 degree) to study the tradeoff between\nperipheral distortion and spatial coverage. Fisheye-GS benefits from field of\nview (FoV) reduction, particularly at 160 degree, while 3DGUT remains stable\nacross all settings and maintains high perceptual quality at the full 200\ndegree view. To address the limitations of SfM-based initialization, which\noften fails under strong distortion, we also propose a depth-based strategy\nusing UniK3D predictions from only 2-3 fisheye images per scene. Although\nUniK3D is not trained on real fisheye data, it produces dense point clouds that\nenable reconstruction quality on par with SfM, even in difficult scenes with\nfog, glare, or sky. Our results highlight the practical viability of\nfisheye-based 3DGS methods for wide-angle 3D reconstruction from sparse and\ndistortion-heavy image inputs.", "AI": {"tldr": "首次评估了基于鱼眼图像的3D高斯泼溅方法（Fisheye-GS和3DGUT）在真实超广角（>180度）场景中的性能。研究了不同视场角的影响，并提出了一种基于深度估计（UniK3D）的初始化策略，以克服SfM在强畸变下的局限性，展示了这些方法在稀疏、高畸变输入下进行广角3D重建的实用性。", "motivation": "现有研究缺乏对基于鱼眼的3D高斯泼溅方法在真实超广角图像（视场角超过180度）上的评估。同时，传统的SfM（运动恢复结构）初始化方法在面对鱼眼图像的强畸变时常常失效，需要新的解决方案。", "method": "研究评估了Fisheye-GS和3DGUT两种方法，使用200度鱼眼相机捕获的真实室内外场景图像。分析了在不同视场角（200度、160度、120度）下的性能表现。为解决SfM初始化局限性，提出了一种基于UniK3D深度预测的初始化策略，仅使用每场景2-3张鱼眼图像生成点云。", "result": "Fisheye-GS在减小视场角（尤其在160度）时性能提升，而3DGUT在所有设置下均保持稳定，并在200度全视场下保持高感知质量。尽管UniK3D未在真实鱼眼数据上训练，但其生成的稠密点云使重建质量与SfM相当，即使在有雾、眩光或天空的困难场景中也能表现良好。", "conclusion": "研究结果突出了基于鱼眼的3D高斯泼溅方法在从稀疏且高度畸变的图像输入进行广角3D重建方面的实际可行性。"}}
{"id": "2508.08001", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08001", "abs": "https://arxiv.org/abs/2508.08001", "authors": ["Rui Yao", "Qi Chai", "Jinhai Yao", "Siyuan Li", "Junhao Chen", "Qi Zhang", "Hao Wang"], "title": "Interpreting Fedspeak with Confidence: A LLM-Based Uncertainty-Aware Framework Guided by Monetary Policy Transmission Paths", "comment": "Rui Yao, Qi Chai, and Jinhai Yao contributed equally to this work.\n  Corresponding authors: Qi Zhang (zhang.qi@sjtu.edu.cn) and Hao Wang\n  (haowang@hkust-gz.edu.cn)", "summary": "\"Fedspeak\", the stylized and often nuanced language used by the U.S. Federal\nReserve, encodes implicit policy signals and strategic stances. The Federal\nOpen Market Committee strategically employs Fedspeak as a communication tool to\nshape market expectations and influence both domestic and global economic\nconditions. As such, automatically parsing and interpreting Fedspeak presents a\nhigh-impact challenge, with significant implications for financial forecasting,\nalgorithmic trading, and data-driven policy analysis. In this paper, we propose\nan LLM-based, uncertainty-aware framework for deciphering Fedspeak and\nclassifying its underlying monetary policy stance. Technically, to enrich the\nsemantic and contextual representation of Fedspeak texts, we incorporate\ndomain-specific reasoning grounded in the monetary policy transmission\nmechanism. We further introduce a dynamic uncertainty decoding module to assess\nthe confidence of model predictions, thereby enhancing both classification\naccuracy and model reliability. Experimental results demonstrate that our\nframework achieves state-of-the-art performance on the policy stance analysis\ntask. Moreover, statistical analysis reveals a significant positive correlation\nbetween perceptual uncertainty and model error rates, validating the\neffectiveness of perceptual uncertainty as a diagnostic signal.", "AI": {"tldr": "本文提出一个基于LLM的、考虑不确定性的框架，用于解读美联储的“Fedspeak”并分类其货币政策立场，实现了最先进的性能，并验证了感知不确定性作为诊断信号的有效性。", "motivation": "美联储的“Fedspeak”语言蕴含隐性政策信号和战略立场，被联邦公开市场委员会（FOMC）用作沟通工具以影响市场预期和经济状况。自动解析和解释“Fedspeak”是一个高影响力挑战，对金融预测、算法交易和数据驱动的政策分析具有重要意义。", "method": "本文提出了一个基于大型语言模型（LLM）的、感知不确定性的框架。技术上，通过融入基于货币政策传导机制的领域特定推理，丰富了“Fedspeak”文本的语义和上下文表示。此外，引入了一个动态不确定性解码模块来评估模型预测的置信度，从而提高分类准确性和模型可靠性。", "result": "实验结果表明，该框架在政策立场分析任务上取得了最先进的性能。此外，统计分析揭示感知不确定性与模型错误率之间存在显著正相关，验证了感知不确定性作为诊断信号的有效性。", "conclusion": "所提出的框架能够有效解读“Fedspeak”并分类其潜在的货币政策立场，具有高准确性和可靠性。其不确定性感知能力为模型预测提供了一个有价值的诊断信号。"}}
{"id": "2508.07701", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07701", "abs": "https://arxiv.org/abs/2508.07701", "authors": ["Bo Jia", "Yanan Guo", "Ying Chang", "Benkui Zhang", "Ying Xie", "Kangning Du", "Lin Cao"], "title": "Multi-view Normal and Distance Guidance Gaussian Splatting for Surface Reconstruction", "comment": "This paper has been accepted by IROS 2025", "summary": "3D Gaussian Splatting (3DGS) achieves remarkable results in the field of\nsurface reconstruction. However, when Gaussian normal vectors are aligned\nwithin the single-view projection plane, while the geometry appears reasonable\nin the current view, biases may emerge upon switching to nearby views. To\naddress the distance and global matching challenges in multi-view scenes, we\ndesign multi-view normal and distance-guided Gaussian splatting. This method\nachieves geometric depth unification and high-accuracy reconstruction by\nconstraining nearby depth maps and aligning 3D normals. Specifically, for the\nreconstruction of small indoor and outdoor scenes, we propose a multi-view\ndistance reprojection regularization module that achieves multi-view Gaussian\nalignment by computing the distance loss between two nearby views and the same\nGaussian surface. Additionally, we develop a multi-view normal enhancement\nmodule, which ensures consistency across views by matching the normals of pixel\npoints in nearby views and calculating the loss. Extensive experimental results\ndemonstrate that our method outperforms the baseline in both quantitative and\nqualitative evaluations, significantly enhancing the surface reconstruction\ncapability of 3DGS.", "AI": {"tldr": "本文提出了一种多视角法线和距离引导的高斯泼溅方法，通过约束相邻深度图和对齐3D法线，解决了3DGS在多视角场景中几何深度不一致和重建偏差的问题，显著提升了表面重建能力。", "motivation": "3D高斯泼溅（3DGS）在表面重建方面表现出色，但当高斯法向量在单视角投影平面内对齐时，几何结构在当前视角下看似合理，但在切换到附近视角时会产生偏差。这促使研究者解决多视角场景中的距离和全局匹配挑战。", "method": "本文设计了多视角法线和距离引导的高斯泼溅方法。具体包括：1) 多视角距离重投影正则化模块，通过计算两个相邻视角和同一高斯表面之间的距离损失，实现多视角高斯对齐。2) 多视角法线增强模块，通过匹配相邻视角中像素点的法线并计算损失，确保视角间的一致性。", "result": "大量的实验结果表明，该方法在定量和定性评估方面均优于基线，显著增强了3DGS的表面重建能力，实现了几何深度统一和高精度重建。", "conclusion": "通过引入多视角距离重投影正则化和多视角法线增强模块，本文提出的方法有效解决了3DGS在多视角重建中的几何偏差问题，显著提升了表面重建的准确性和一致性。"}}
{"id": "2508.07630", "categories": ["cs.CL", "cs.AI", "cs.CV", "I.2.7; I.2.10; I.4.10; I.7.5"], "pdf": "https://arxiv.org/pdf/2508.07630", "abs": "https://arxiv.org/abs/2508.07630", "authors": ["Anirudh Iyengar Kaniyar Narayana Iyengar", "Srija Mukhopadhyay", "Adnan Qidwai", "Shubhankar Singh", "Dan Roth", "Vivek Gupta"], "title": "InterChart: Benchmarking Visual Reasoning Across Decomposed and Distributed Chart Information", "comment": "18 pages, 6 figures, 12 tables. Benchmark dataset and evaluation code\n  will be publicly made available", "summary": "We introduce InterChart, a diagnostic benchmark that evaluates how well\nvision-language models (VLMs) reason across multiple related charts, a task\ncentral to real-world applications such as scientific reporting, financial\nanalysis, and public policy dashboards. Unlike prior benchmarks focusing on\nisolated, visually uniform charts, InterChart challenges models with diverse\nquestion types ranging from entity inference and trend correlation to numerical\nestimation and abstract multi-step reasoning grounded in 2-3 thematically or\nstructurally related charts. We organize the benchmark into three tiers of\nincreasing difficulty: (1) factual reasoning over individual charts, (2)\nintegrative analysis across synthetically aligned chart sets, and (3) semantic\ninference over visually complex, real-world chart pairs. Our evaluation of\nstate-of-the-art open and closed-source VLMs reveals consistent and steep\naccuracy declines as chart complexity increases. We find that models perform\nbetter when we decompose multi-entity charts into simpler visual units,\nunderscoring their struggles with cross-chart integration. By exposing these\nsystematic limitations, InterChart provides a rigorous framework for advancing\nmultimodal reasoning in complex, multi-visual environments.", "AI": {"tldr": "InterChart是一个诊断基准，用于评估视觉-语言模型（VLMs）在多张相关图表上进行推理的能力，揭示了现有模型在复杂多视觉环境中的局限性。", "motivation": "现有基准侧重于孤立、视觉统一的图表，而现实世界的应用（如科学报告、金融分析、公共政策仪表盘）需要模型对多张相关图表进行推理。", "method": "引入InterChart基准，包含基于2-3张主题或结构相关图表的多种问题类型（实体推断、趋势关联、数值估计、抽象多步推理）。基准分为三个难度递增的级别：个体图表的事实推理、合成对齐图表集的整合分析、以及视觉复杂真实世界图表对的语义推断。", "result": "对最先进的VLMs进行评估发现，随着图表复杂性的增加，模型准确率持续且大幅下降。将多实体图表分解为更简单的视觉单元时，模型表现更好，这突显了它们在跨图表整合方面的困难。", "conclusion": "InterChart通过揭示这些系统性局限性，为推进复杂多视觉环境下的多模态推理提供了一个严格的框架。"}}
{"id": "2508.06982", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06982", "abs": "https://arxiv.org/abs/2508.06982", "authors": ["Yixin Zhu", "Zuoliang Zhu", "Miloš Hašan", "Jian Yang", "Jin Xie", "Beibei Wang"], "title": "WeatherDiffusion: Weather-Guided Diffusion Model for Forward and Inverse Rendering", "comment": null, "summary": "Forward and inverse rendering have emerged as key techniques for enabling\nunderstanding and reconstruction in the context of autonomous driving (AD).\nHowever, complex weather and illumination pose great challenges to this task.\nThe emergence of large diffusion models has shown promise in achieving\nreasonable results through learning from 2D priors, but these models are\ndifficult to control and lack robustness. In this paper, we introduce\nWeatherDiffusion, a diffusion-based framework for forward and inverse rendering\non AD scenes with various weather and lighting conditions. Our method enables\nauthentic estimation of material properties, scene geometry, and lighting, and\nfurther supports controllable weather and illumination editing through the use\nof predicted intrinsic maps guided by text descriptions. We observe that\ndifferent intrinsic maps should correspond to different regions of the original\nimage. Based on this observation, we propose Intrinsic map-aware attention\n(MAA) to enable high-quality inverse rendering. Additionally, we introduce a\nsynthetic dataset (\\ie WeatherSynthetic) and a real-world dataset (\\ie\nWeatherReal) for forward and inverse rendering on AD scenes with diverse\nweather and lighting. Extensive experiments show that our WeatherDiffusion\noutperforms state-of-the-art methods on several benchmarks. Moreover, our\nmethod demonstrates significant value in downstream tasks for AD, enhancing the\nrobustness of object detection and image segmentation in challenging weather\nscenarios.", "AI": {"tldr": "本文提出了WeatherDiffusion，一个基于扩散模型的框架，用于在各种天气和光照条件下对自动驾驶（AD）场景进行正向和逆向渲染，并支持可控编辑，通过引入新的注意力机制和数据集，超越了现有技术，并提升了下游AD任务的鲁棒性。", "motivation": "正向和逆向渲染是自动驾驶中理解和重建的关键技术，但复杂的天气和光照条件带来了巨大挑战。现有的扩散模型虽然能从2D先验中学习并取得合理结果，但难以控制且缺乏鲁棒性，促使研究更可控、更鲁棒的解决方案。", "method": "本文提出了WeatherDiffusion，一个基于扩散模型的正向和逆向渲染框架。它能够真实估计材质属性、场景几何和光照，并通过文本描述引导的预测内在图（intrinsic maps）实现可控的天气和光照编辑。基于不同内在图对应图像不同区域的观察，提出了一种内在图感知注意力（MAA）机制以实现高质量逆向渲染。此外，还引入了合成数据集WeatherSynthetic和真实世界数据集WeatherReal用于AD场景渲染。", "result": "广泛的实验表明，WeatherDiffusion在多个基准测试中优于现有最先进的方法。此外，该方法在自动驾驶的下游任务（如目标检测和图像分割）中展现了显著价值，增强了在恶劣天气场景下的鲁棒性。", "conclusion": "WeatherDiffusion为自动驾驶场景在复杂天气和光照条件下的正向和逆向渲染提供了一个有效且鲁棒的解决方案。它不仅提高了渲染质量，还通过可控编辑和对下游任务的性能提升，展现了其在实际AD应用中的巨大潜力。"}}
{"id": "2508.08007", "categories": ["cs.AI", "Computing methodologies~Description logics, Computing\n  methodologies~Ontology engineering"], "pdf": "https://arxiv.org/pdf/2508.08007", "abs": "https://arxiv.org/abs/2508.08007", "authors": ["Maurice Funk", "Marvin Grosser", "Carsten Lutz"], "title": "Fitting Description Logic Ontologies to ABox and Query Examples", "comment": "Submitted to the 22nd International Conference on Principles of\n  Knowledge Representation and Reasoning (KR2025), 23 pages", "summary": "We study a fitting problem inspired by ontology-mediated querying: given a\ncollection\n  of positive and negative examples of\n  the form $(\\mathcal{A},q)$ with\n  $\\mathcal{A}$ an ABox and $q$ a Boolean query, we seek\n  an ontology $\\mathcal{O}$ that satisfies $\\mathcal{A} \\cup \\mathcal{O} \\vDash\nq$ for all positive examples and $\\mathcal{A} \\cup \\mathcal{O}\\not\\vDash q$ for\nall negative examples.\n  We consider the description logics $\\mathcal{ALC}$ and $\\mathcal{ALCI}$ as\nontology languages and\n  a range of query languages that\n  includes atomic queries (AQs), conjunctive queries (CQs), and unions thereof\n(UCQs).\n  For all of the resulting fitting problems,\n  we provide\n  effective characterizations and determine the computational complexity\n  of deciding whether a fitting ontology exists. This problem turns out to be\n${\\small CO}NP$ for AQs and full CQs\n  and $2E{\\small XP}T{\\small IME}$-complete for CQs and UCQs.\n  These results hold for both $\\mathcal{ALC}$ and $\\mathcal{ALCI}$.", "AI": {"tldr": "研究了本体介导查询中的本体拟合问题：给定正负查询示例，寻找一个能解释这些示例的本体，并分析了其计算复杂性。", "motivation": "该研究受到本体介导查询的启发，旨在解决如何从已知的正例和反例（ABox和布尔查询对）中，自动学习或“拟合”一个合适的本体。", "method": "将本体拟合问题形式化，考虑描述逻辑ALC和ALCI作为本体语言，以及原子查询（AQs）、合取查询（CQs）和它们的并集（UCQs）作为查询语言。为所有组合提供了有效的特征化方法，并确定了判断拟合本体是否存在问题的计算复杂性。", "result": "确定了拟合本体存在性问题的计算复杂性：对于原子查询（AQs）和完全合取查询（full CQs），问题是CO NP完全的；对于合取查询（CQs）和合取查询的并集（UCQs），问题是2EXPTIME完全的。这些结果对于ALC和ALCI两种描述逻辑均成立。", "conclusion": "论文为本体拟合问题提供了有效的特征化和计算复杂性分析，揭示了在不同描述逻辑和查询语言组合下，寻找解释给定查询示例的本体的计算难度。"}}
{"id": "2508.07690", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07690", "abs": "https://arxiv.org/abs/2508.07690", "authors": ["Luyao Zhuang", "Qinggang Zhang", "Huachi Zhou", "Juhua Liu", "Qing Li", "Xiao Huang"], "title": "LoSemB: Logic-Guided Semantic Bridging for Inductive Tool Retrieval", "comment": null, "summary": "Tool learning has emerged as a promising paradigm for large language models\n(LLMs) to solve many real-world tasks. Nonetheless, with the tool repository\nrapidly expanding, it is impractical to contain all tools within the limited\ninput length of LLMs. To alleviate these issues, researchers have explored\nincorporating a tool retrieval module to select the most relevant tools or\nrepresent tools as unique tokens within LLM parameters. However, most\nstate-of-the-art methods are under transductive settings, assuming all tools\nhave been observed during training. Such a setting deviates from reality as the\nreal-world tool repository is evolving and incorporates new tools frequently.\nWhen dealing with these unseen tools, which refer to tools not encountered\nduring the training phase, these methods are limited by two key issues,\nincluding the large distribution shift and the vulnerability of\nsimilarity-based retrieval. To this end, inspired by human cognitive processes\nof mastering unseen tools through discovering and applying the logical\ninformation from prior experience, we introduce a novel Logic-Guided Semantic\nBridging framework for inductive tool retrieval, namely, LoSemB, which aims to\nmine and transfer latent logical information for inductive tool retrieval\nwithout costly retraining. Specifically, LoSemB contains a logic-based\nembedding alignment module to mitigate distribution shifts and implements a\nrelational augmented retrieval mechanism to reduce the vulnerability of\nsimilarity-based retrieval. Extensive experiments demonstrate that LoSemB\nachieves advanced performance in inductive settings while maintaining desirable\neffectiveness in the transductive setting.", "AI": {"tldr": "本文提出了一种名为LoSemB的逻辑引导语义桥接框架，用于大型语言模型（LLMs）的归纳式工具检索，旨在通过挖掘和转移潜在逻辑信息，有效处理训练阶段未曾见过的工具，克服现有方法的局限性。", "motivation": "现有的大型语言模型工具学习方法主要在转导设置下（即假设所有工具在训练时都已见过），这与现实世界中工具库不断演进、频繁引入新工具的情况不符。当处理这些未见过的工具时，现有方法面临两大挑战：巨大的分布偏移和基于相似度检索的脆弱性。", "method": "受人类通过先验经验发现和应用逻辑信息来掌握未见工具的认知过程启发，本文提出了LoSemB框架。它包含一个基于逻辑的嵌入对齐模块，用于缓解分布偏移；以及一个关系增强检索机制，用于降低基于相似度检索的脆弱性，从而无需昂贵的再训练即可进行归纳式工具检索。", "result": "广泛的实验证明，LoSemB在归纳设置下取得了先进的性能，同时在转导设置下也保持了良好的有效性。", "conclusion": "LoSemB框架通过引入逻辑引导的语义桥接，成功解决了大型语言模型在处理未见过工具时的归纳式工具检索问题，显著提升了工具学习的实用性和鲁棒性。"}}
{"id": "2508.06988", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06988", "abs": "https://arxiv.org/abs/2508.06988", "authors": ["Fangmin Zhao", "Weichao Zeng", "Zhenhang Li", "Dongbao Yang", "Yu Zhou"], "title": "TADoc: Robust Time-Aware Document Image Dewarping", "comment": "8 pages, 8 figures", "summary": "Flattening curved, wrinkled, and rotated document images captured by portable\nphotographing devices, termed document image dewarping, has become an\nincreasingly important task with the rise of digital economy and online\nworking. Although many methods have been proposed recently, they often struggle\nto achieve satisfactory results when confronted with intricate document\nstructures and higher degrees of deformation in real-world scenarios. Our main\ninsight is that, unlike other document restoration tasks (e.g., deblurring),\ndewarping in real physical scenes is a progressive motion rather than a\none-step transformation. Based on this, we have undertaken two key initiatives.\nFirstly, we reformulate this task, modeling it for the first time as a dynamic\nprocess that encompasses a series of intermediate states. Secondly, we design a\nlightweight framework called TADoc (Time-Aware Document Dewarping Network) to\naddress the geometric distortion of document images. In addition, due to the\ninadequacy of OCR metrics for document images containing sparse text, the\ncomprehensiveness of evaluation is insufficient. To address this shortcoming,\nwe propose a new metric -- DLS (Document Layout Similarity) -- to evaluate the\neffectiveness of document dewarping in downstream tasks. Extensive experiments\nand in-depth evaluations have been conducted and the results indicate that our\nmodel possesses strong robustness, achieving superiority on several benchmarks\nwith different document types and degrees of distortion.", "AI": {"tldr": "本文提出将文档图像去畸变任务建模为一个动态过程，并设计了轻量级网络TADoc。同时，为解决评估不足问题，提出了一种新的度量标准DLS，实验证明了模型的鲁棒性和优越性。", "motivation": "随着数字经济和在线工作的兴起，文档图像去畸变任务变得日益重要。然而，现有方法在处理真实世界中复杂结构和高度变形的文档时，往往难以取得令人满意的效果。", "method": "本文提出了两项关键举措：1) 首次将文档去畸变任务重新定义为包含一系列中间状态的动态过程。2) 设计了一个名为TADoc（Time-Aware Document Dewarping Network）的轻量级框架来解决文档图像的几何畸变。此外，针对OCR指标对稀疏文本文档图像评估不足的问题，提出了一种新的度量标准——DLS（Document Layout Similarity）来评估去畸变效果。", "result": "通过广泛的实验和深入评估，结果表明TADoc模型具有强大的鲁棒性，并在多个不同文档类型和畸变程度的基准测试中表现出优越性。", "conclusion": "将文档去畸变建模为动态过程，并结合所提出的TADoc网络和DLS评估指标，能有效提升文档图像去畸变在复杂真实场景下的性能和评估准确性。"}}
{"id": "2508.08053", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08053", "abs": "https://arxiv.org/abs/2508.08053", "authors": ["Runchuan Zhu", "Bowen Jiang", "Lingrui Mei", "Fangkai Yang", "Lu Wang", "Haoxiang Gao", "Fengshuo Bai", "Pu Zhao", "Qingwei Lin", "Saravan Rajmohan", "Dongmei Zhang"], "title": "AdaptFlow: Adaptive Workflow Optimization via Meta-Learning", "comment": null, "summary": "Recent advances in large language models (LLMs) have sparked growing interest\nin agentic workflows, which are structured sequences of LLM invocations\nintended to solve complex tasks. However, existing approaches often rely on\nstatic templates or manually designed workflows, which limit adaptability to\ndiverse tasks and hinder scalability. We propose AdaptFlow, a natural\nlanguage-based meta-learning framework inspired by model-agnostic meta-learning\n(MAML). AdaptFlow learns a generalizable workflow initialization that enables\nrapid subtask-level adaptation. It employs a bi-level optimization scheme: the\ninner loop refines the workflow for a specific subtask using LLM-generated\nfeedback, while the outer loop updates the shared initialization to perform\nwell across tasks. This setup allows AdaptFlow to generalize effectively to\nunseen tasks by adapting the initialized workflow through language-guided\nmodifications. Evaluated across question answering, code generation, and\nmathematical reasoning benchmarks, AdaptFlow consistently outperforms both\nmanually crafted and automatically searched baselines, achieving\nstate-of-the-art results with strong generalization across tasks and models.\nThe source code and data are available at\nhttps://github.com/microsoft/DKI_LLM/tree/AdaptFlow/AdaptFlow.", "AI": {"tldr": "AdaptFlow是一个基于自然语言的元学习框架，受MAML启发，用于LLM代理工作流。它学习可泛化的工作流初始化，通过双层优化实现子任务级别的快速适应，从而解决复杂任务。", "motivation": "现有LLM代理工作流依赖静态模板或手动设计，限制了对多样化任务的适应性和可扩展性。", "method": "AdaptFlow采用受MAML启发的双层优化方案：内循环利用LLM生成的反馈为特定子任务优化工作流；外循环更新共享初始化以在不同任务上表现良好，通过语言引导修改实现快速适应。", "result": "AdaptFlow在问答、代码生成和数学推理基准测试中持续优于手动设计和自动搜索的基线，实现了最先进的结果，并在任务和模型之间展现出强大的泛化能力。", "conclusion": "AdaptFlow通过其元学习和双层优化方法，有效提升了LLM代理工作流的适应性和泛化能力，为复杂任务解决了传统方法的局限性。"}}
{"id": "2508.07702", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.07702", "abs": "https://arxiv.org/abs/2508.07702", "authors": ["Charlie Wyatt", "Aditya Joshi", "Flora Salim"], "title": "What am I missing here?: Evaluating Large Language Models for Masked Sentence Prediction", "comment": "Under Review", "summary": "Transformer-based models primarily rely on Next Token Prediction (NTP), which\npredicts the next token in a sequence based on the preceding context. However,\nNTP's focus on single-token prediction often limits a model's ability to plan\nahead or maintain long-range coherence, raising questions about how well LLMs\ncan predict longer contexts, such as full sentences within structured\ndocuments. While NTP encourages local fluency, it provides no explicit\nincentive to ensure global coherence across sentence boundaries-an essential\nskill for reconstructive or discursive tasks. To investigate this, we evaluate\nthree commercial LLMs (GPT-4o, Claude 3.5 Sonnet, and Gemini 2.0 Flash) on\nMasked Sentence Prediction (MSP) - the task of infilling a randomly removed\nsentence - from three domains: ROCStories (narrative), Recipe1M (procedural),\nand Wikipedia (expository). We assess both fidelity (similarity to the original\nsentence) and cohesiveness (fit within the surrounding context). Our key\nfinding reveals that commercial LLMs, despite their superlative performance in\nother tasks, are poor at predicting masked sentences in low-structured domains,\nhighlighting a gap in current model capabilities.", "AI": {"tldr": "研究发现，主流大型语言模型在“掩码句子预测”（即补全被移除的句子）任务上表现不佳，尤其是在低结构化文本领域，这暴露出当前模型在长程连贯性方面的局限。", "motivation": "Transformer模型主要依赖“下一个词元预测”（NTP），但这限制了模型进行长程规划或保持全局连贯性，尤其是在预测文档中完整句子时。NTP鼓励局部流畅性，但缺乏确保跨句子边界全局连贯性的明确激励，而这对于重建或论述性任务至关重要。", "method": "评估了三款商业大型语言模型（GPT-4o, Claude 3.5 Sonnet, Gemini 2.0 Flash）在“掩码句子预测”（MSP）任务上的表现。任务涉及补全随机移除的句子，数据来自三个领域：ROCStories（叙事）、Recipe1M（程序）和Wikipedia（说明）。评估指标包括忠实度（与原始句子的相似性）和连贯性（与上下文的契合度）。", "result": "关键发现是，尽管商业大型语言模型在其他任务中表现出色，但它们在低结构化领域预测掩码句子方面的能力很差。", "conclusion": "当前大型语言模型在处理长程连贯性和句子级别预测方面存在能力空白，尤其是在非结构化或低结构化文本中，这可能与其核心的“下一个词元预测”训练范式有关。"}}
{"id": "2508.06993", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06993", "abs": "https://arxiv.org/abs/2508.06993", "authors": ["Nick Lemke", "John Kalkhof", "Niklas Babendererde", "Anirban Mukhopadhyay"], "title": "OctreeNCA: Single-Pass 184 MP Segmentation on Consumer Hardware", "comment": null, "summary": "Medical applications demand segmentation of large inputs, like prostate MRIs,\npathology slices, or videos of surgery. These inputs should ideally be inferred\nat once to provide the model with proper spatial or temporal context. When\nsegmenting large inputs, the VRAM consumption of the GPU becomes the\nbottleneck. Architectures like UNets or Vision Transformers scale very poorly\nin VRAM consumption, resulting in patch- or frame-wise approaches that\ncompromise global consistency and inference speed. The lightweight Neural\nCellular Automaton (NCA) is a bio-inspired model that is by construction\nsize-invariant. However, due to its local-only communication rules, it lacks\nglobal knowledge. We propose OctreeNCA by generalizing the neighborhood\ndefinition using an octree data structure. Our generalized neighborhood\ndefinition enables the efficient traversal of global knowledge. Since deep\nlearning frameworks are mainly developed for large multi-layer networks, their\nimplementation does not fully leverage the advantages of NCAs. We implement an\nNCA inference function in CUDA that further reduces VRAM demands and increases\ninference speed. Our OctreeNCA segments high-resolution images and videos\nquickly while occupying 90% less VRAM than a UNet during evaluation. This\nallows us to segment 184 Megapixel pathology slices or 1-minute surgical videos\nat once.", "AI": {"tldr": "针对医学领域大尺寸输入（如MRI、病理切片、手术视频）的分割任务，传统模型因显存限制需分块处理，导致全局一致性差且速度慢。本文提出OctreeNCA，通过八叉树结构引入全局信息，并用CUDA优化推理，显著降低显存占用（比UNet少90%）并提高速度，实现对超大尺寸数据的整体分割。", "motivation": "医学应用中的大尺寸输入（如前列腺MRI、病理切片、手术视频）需要整体推理以提供完整的空间或时间上下文。然而，UNet和Vision Transformer等主流架构在处理大输入时显存消耗巨大，导致必须采用分块或分帧处理，这牺牲了全局一致性和推理速度。轻量级神经元胞自动机（NCA）虽然尺寸不变，但其局部通信规则使其缺乏全局知识。", "method": "1. 提出OctreeNCA，通过使用八叉树数据结构泛化邻域定义，使得NCA能够高效地遍历和获取全局知识。2. 针对NCA的特性，在CUDA中实现了一个NCA推理函数，进一步降低了显存需求并提高了推理速度，充分利用了NCA的优势而避免了深度学习框架对多层网络优化的局限性。", "result": "OctreeNCA能够快速分割高分辨率图像和视频，在评估时显存占用比UNet少90%。这使得模型能够一次性分割184兆像素的病理切片或1分钟的手术视频。", "conclusion": "OctreeNCA通过结合八叉树结构引入全局上下文和定制化的CUDA推理实现，有效解决了大尺寸医学输入分割中显存消耗过大的瓶颈问题。它实现了对超高分辨率图像和长视频的快速、整体分割，显著提升了效率和可行性。"}}
{"id": "2508.08075", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08075", "abs": "https://arxiv.org/abs/2508.08075", "authors": ["Meishen He", "Wenjun Ma", "Jiao Wang", "Huijun Yue", "Xiaoma Fan"], "title": "FNBT: Full Negation Belief Transformation for Open-World Information Fusion Based on Dempster-Shafer Theory of Evidence", "comment": null, "summary": "The Dempster-Shafer theory of evidence has been widely applied in the field\nof information fusion under uncertainty. Most existing research focuses on\ncombining evidence within the same frame of discernment. However, in real-world\nscenarios, trained algorithms or data often originate from different regions or\norganizations, where data silos are prevalent. As a result, using different\ndata sources or models to generate basic probability assignments may lead to\nheterogeneous frames, for which traditional fusion methods often yield\nunsatisfactory results. To address this challenge, this study proposes an\nopen-world information fusion method, termed Full Negation Belief\nTransformation (FNBT), based on the Dempster-Shafer theory. More specially, a\ncriterion is introduced to determine whether a given fusion task belongs to the\nopen-world setting. Then, by extending the frames, the method can accommodate\nelements from heterogeneous frames. Finally, a full negation mechanism is\nemployed to transform the mass functions, so that existing combination rules\ncan be applied to the transformed mass functions for such information fusion.\nTheoretically, the proposed method satisfies three desirable properties, which\nare formally proven: mass function invariance, heritability, and essential\nconflict elimination. Empirically, FNBT demonstrates superior performance in\npattern classification tasks on real-world datasets and successfully resolves\nZadeh's counterexample, thereby validating its practical effectiveness.", "AI": {"tldr": "该研究提出了一种名为“全否定信念转换”（FNBT）的开放世界信息融合方法，基于Dempster-Shafer证据理论，旨在解决异构框架下的证据融合问题，并在理论和实践中均表现出优越性。", "motivation": "大多数现有Dempster-Shafer证据理论的融合研究集中于同质框架。然而，在现实世界中，数据或算法常来源于不同区域或组织，导致数据孤岛和异构框架的出现，传统融合方法在这种情况下效果不佳。", "method": "该研究提出FNBT方法。首先，引入一个准则来判断融合任务是否属于开放世界设置。其次，通过扩展框架来容纳来自异构框架的元素。最后，采用全否定机制转换质量函数，使得现有的组合规则可以应用于这些转换后的质量函数进行信息融合。", "result": "理论上，FNBT方法满足并被证明具有质量函数不变性、可继承性和本质冲突消除三个理想性质。经验上，FNBT在真实世界数据集上的模式分类任务中表现出卓越性能，并成功解决了Zadeh反例。", "conclusion": "FNBT方法在理论上具有良好的性质，并在实践中证明了其在处理异构框架信息融合方面的有效性和优越性，成功应对了开放世界场景下的融合挑战。"}}
{"id": "2508.07753", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.07753", "abs": "https://arxiv.org/abs/2508.07753", "authors": ["Zhenliang Zhang", "Junzhe Zhang", "Xinyu Hu", "HuiXuan Zhang", "Xiaojun Wan"], "title": "Exploring Causal Effect of Social Bias on Faithfulness Hallucinations in Large Language Models", "comment": "Accepted by CIKM 2025 (Full Paper)", "summary": "Large language models (LLMs) have achieved remarkable success in various\ntasks, yet they remain vulnerable to faithfulness hallucinations, where the\noutput does not align with the input. In this study, we investigate whether\nsocial bias contributes to these hallucinations, a causal relationship that has\nnot been explored. A key challenge is controlling confounders within the\ncontext, which complicates the isolation of causality between bias states and\nhallucinations. To address this, we utilize the Structural Causal Model (SCM)\nto establish and validate the causality and design bias interventions to\ncontrol confounders. In addition, we develop the Bias Intervention Dataset\n(BID), which includes various social biases, enabling precise measurement of\ncausal effects. Experiments on mainstream LLMs reveal that biases are\nsignificant causes of faithfulness hallucinations, and the effect of each bias\nstate differs in direction. We further analyze the scope of these causal\neffects across various models, specifically focusing on unfairness\nhallucinations, which are primarily targeted by social bias, revealing the\nsubtle yet significant causal effect of bias on hallucination generation.", "AI": {"tldr": "本研究探讨了社会偏见是否是大型语言模型（LLMs）忠实度幻觉的原因，并利用结构因果模型（SCM）和新数据集（BID）证明了这一因果关系，发现偏见是幻觉的重要原因，且不同偏见状态影响方向各异。", "motivation": "尽管大型语言模型在多项任务中表现出色，但它们仍然容易产生忠实度幻觉（输出与输入不符）。此前尚未探索社会偏见是否导致这些幻觉，且在控制混杂因素的情况下隔离偏见状态与幻觉之间的因果关系是一个关键挑战。", "method": "为解决上述挑战，研究者采用了结构因果模型（SCM）来建立和验证因果关系，并设计了偏见干预措施以控制混杂因素。此外，他们开发了偏见干预数据集（BID），其中包含各种社会偏见，以精确测量因果效应。", "result": "在主流大型语言模型上的实验表明，偏见是导致忠实度幻觉的重要原因，并且每种偏见状态的影响方向不同。研究还分析了这些因果效应在不同模型上的范围，特别关注了不公平幻觉，揭示了偏见对幻觉产生微妙而显著的因果效应。", "conclusion": "社会偏见是大型语言模型产生忠实度幻觉的重要原因，且其因果效应是可测量和多样的，尤其在不公平幻觉方面表现出显著影响。"}}
{"id": "2508.06995", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06995", "abs": "https://arxiv.org/abs/2508.06995", "authors": ["Huihui Xu", "Jin Ye", "Hongqiu Wang", "Changkai Ji", "Jiashi Lin", "Ming Hu", "Ziyan Huang", "Ying Chen", "Chenglong Ma", "Tianbin Li", "Lihao Liu", "Junjun He", "Lei Zhu"], "title": "S2-UniSeg: Fast Universal Agglomerative Pooling for Scalable Segment Anything without Supervision", "comment": null, "summary": "Recent self-supervised image segmentation models have achieved promising\nperformance on semantic segmentation and class-agnostic instance segmentation.\nHowever, their pretraining schedule is multi-stage, requiring a time-consuming\npseudo-masks generation process between each training epoch. This\ntime-consuming offline process not only makes it difficult to scale with\ntraining dataset size, but also leads to sub-optimal solutions due to its\ndiscontinuous optimization routine. To solve these, we first present a novel\npseudo-mask algorithm, Fast Universal Agglomerative Pooling (UniAP). Each layer\nof UniAP can identify groups of similar nodes in parallel, allowing to generate\nboth semantic-level and instance-level and multi-granular pseudo-masks within\nens of milliseconds for one image. Based on the fast UniAP, we propose the\nScalable Self-Supervised Universal Segmentation (S2-UniSeg), which employs a\nstudent and a momentum teacher for continuous pretraining. A novel\nsegmentation-oriented pretext task, Query-wise Self-Distillation (QuerySD), is\nproposed to pretrain S2-UniSeg to learn the local-to-global correspondences.\nUnder the same setting, S2-UniSeg outperforms the SOTA UnSAM model, achieving\nnotable improvements of AP+6.9 on COCO, AR+11.1 on UVO, PixelAcc+4.5 on\nCOCOStuff-27, RQ+8.0 on Cityscapes. After scaling up to a larger 2M-image\nsubset of SA-1B, S2-UniSeg further achieves performance gains on all four\nbenchmarks. Our code and pretrained models are available at\nhttps://github.com/bio-mlhui/S2-UniSeg", "AI": {"tldr": "该论文提出了一种名为S2-UniSeg的可扩展自监督通用分割模型，通过引入快速伪掩码生成算法UniAP和连续预训练框架，解决了现有模型耗时且难以扩展的问题，并在多个基准测试中取得了显著优于SOTA的性能。", "motivation": "现有的自监督图像分割模型在预训练过程中需要多阶段、耗时的伪掩码生成，这不仅难以随数据集大小扩展，还因不连续优化导致次优解。", "method": "1. 提出了一种名为Fast Universal Agglomerative Pooling (UniAP) 的新颖伪掩码算法，可在毫秒级时间内并行生成语义级、实例级和多粒度伪掩码。2. 基于UniAP，提出了可扩展自监督通用分割模型S2-UniSeg，采用学生-动量教师网络进行连续预训练。3. 提出了一种面向分割的预训练任务Query-wise Self-Distillation (QuerySD)，用于学习局部到全局的对应关系。", "result": "在相同设置下，S2-UniSeg在COCO上AP提升6.9，UVO上AR提升11.1，COCOStuff-27上PixelAcc提升4.5，Cityscapes上RQ提升8.0，超越了SOTA的UnSAM模型。扩展到更大的2M图像SA-1B子集后，S2-UniSeg在所有四个基准测试上均进一步取得了性能提升。", "conclusion": "S2-UniSeg通过创新的UniAP算法和连续预训练框架，有效解决了自监督分割模型在效率和可扩展性上的挑战，并在多项通用分割任务中取得了卓越的性能，证明了其在处理大规模数据集和实现通用分割方面的潜力。"}}
{"id": "2508.08115", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08115", "abs": "https://arxiv.org/abs/2508.08115", "authors": ["Pranav Pushkar Mishra", "Mohammad Arvan", "Mohan Zalake"], "title": "TeamMedAgents: Enhancing Medical Decision-Making of LLMs Through Structured Teamwork", "comment": "10 pages, 1 figure, 6 tables(2 in main, 4 in appendix)", "summary": "We present TeamMedAgents, a novel multi-agent approach that systematically\nintegrates evidence-based teamwork components from human-human collaboration\ninto medical decision-making with large language models (LLMs). Our approach\nvalidates an organizational psychology teamwork model from human collaboration\nto computational multi-agent medical systems by operationalizing six core\nteamwork components derived from Salas et al.'s \"Big Five\" model: team\nleadership, mutual performance monitoring, team orientation, shared mental\nmodels, closed-loop communication, and mutual trust. We implement and evaluate\nthese components as modular, configurable mechanisms within an adaptive\ncollaboration architecture while assessing the effect of the number of agents\ninvolved based on the task's requirements and domain. Systematic evaluation of\ncomputational implementations of teamwork behaviors across eight medical\nbenchmarks (MedQA, MedMCQA, MMLU-Pro Medical, PubMedQA, DDXPlus, MedBullets,\nPath-VQA, and PMC-VQA) demonstrates consistent improvements across 7 out of 8\nevaluated datasets. Controlled ablation studies conducted on 50 questions per\nconfiguration across 3 independent runs provide mechanistic insights into\nindividual component contributions, revealing optimal teamwork configurations\nthat vary by reasoning task complexity and domain-specific requirements. Our\nablation analyses reveal dataset-specific optimal teamwork configurations,\nindicating that different medical reasoning modalities benefit from distinct\ncollaborative patterns. TeamMedAgents represents an advancement in\ncollaborative AI by providing a systematic translation of established teamwork\ntheories from human collaboration into agentic collaboration, establishing a\nfoundation for evidence-based multi-agent system design in critical\ndecision-making domains.", "AI": {"tldr": "TeamMedAgents将人类团队协作的循证方法应用于LLM医疗决策，通过集成核心团队组件显著提高了在多个医疗基准上的性能，并揭示了针对不同任务复杂度的最优协作配置。", "motivation": "将人类协作中基于证据的团队协作组件系统地整合到基于大型语言模型（LLM）的医疗决策中，以提高其性能和可靠性，并验证人类团队心理学模型在计算多智能体医疗系统中的适用性。", "method": "提出TeamMedAgents多智能体方法，将Salas等人的“Big Five”模型中的六个核心团队协作组件（团队领导力、相互绩效监控、团队导向、共享心智模型、闭环沟通、相互信任）操作化。这些组件被实现为模块化、可配置的机制，并嵌入自适应协作架构中。通过评估代理数量的影响，在八个医疗基准数据集上进行系统评估，并进行受控消融研究以分析各组件贡献和最优配置。", "result": "在八个评估的医疗数据集中，有七个数据集表现出持续的性能提升。消融研究表明，最优团队协作配置因推理任务的复杂性和领域特定要求而异，不同的医疗推理模式受益于不同的协作模式。", "conclusion": "TeamMedAgents通过将人类协作中既定的团队理论系统地转化为智能体协作，推动了协作式AI的发展，并为关键决策领域中基于证据的多智能体系统设计奠定了基础。"}}
{"id": "2508.07781", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.07781", "abs": "https://arxiv.org/abs/2508.07781", "authors": ["Zeyu Yang", "Lai Wei", "Roman Koshkin", "Xi Chen", "Satoshi Nakamura"], "title": "SASST: Leveraging Syntax-Aware Chunking and LLMs for Simultaneous Speech Translation", "comment": null, "summary": "This work proposes a grammar-based chunking strategy that segments input\nstreams into semantically complete units by parsing dependency relations (e.g.,\nnoun phrase boundaries, verb-object structures) and punctuation features. The\nmethod ensures chunk coherence and minimizes semantic fragmentation. Building\non this mechanism, we present SASST (Syntax-Aware Simultaneous Speech\nTranslation), an end-to-end framework integrating frozen Whisper encoder and\ndecoder-only LLM. The unified architecture dynamically outputs translation\ntokens or <WAIT> symbols to jointly optimize translation timing and content,\nwith target-side reordering addressing word-order divergence. Experiments on\nCoVoST2 multilingual corpus En-{De, Zh, Ja} demonstrate significant translation\nquality improvements across languages and validate the effectiveness of\nsyntactic structures in LLM-driven SimulST systems.", "AI": {"tldr": "本文提出了一种基于语法的分块策略，通过解析依赖关系和标点符号将输入流分割成语义完整的单元，并在此基础上构建了SASST（语法感知同步语音翻译）框架，该框架结合了Whisper编码器和解码器LLM，动态输出翻译或等待符号以优化翻译时序和内容，并通过目标端重排序解决语序差异，显著提升了同步语音翻译质量。", "motivation": "现有同步翻译系统可能存在语义碎片化问题，难以保证翻译内容的连贯性，并且需要优化翻译时序与内容输出的平衡，同时解决不同语言间的语序差异。", "method": "核心方法是基于语法的分块策略，利用依赖关系（如名词短语边界、动宾结构）和标点符号将输入流分割为语义完整的单元。在此基础上，提出了SASST框架，这是一个端到端的系统，集成了冻结的Whisper编码器和仅解码器的LLM。该架构能动态输出翻译token或<WAIT>符号，以联合优化翻译时序和内容，并通过目标端重排序来解决语序差异。", "result": "在CoVoST2多语言语料库（英-德、英-中、英-日）上的实验表明，该方法显著提高了跨语言的翻译质量，并验证了在LLM驱动的同步语音翻译系统中，语法结构是有效的。", "conclusion": "语法结构在LLM驱动的同步语音翻译系统中能够有效提升翻译质量，通过基于语法的分块和动态时序控制，可以实现语义连贯且高质量的同步翻译。"}}
{"id": "2508.07006", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07006", "abs": "https://arxiv.org/abs/2508.07006", "authors": ["Gian Mario Favero", "Ge Ya Luo", "Nima Fathi", "Justin Szeto", "Douglas L. Arnold", "Brennan Nichyporuk", "Chris Pal", "Tal Arbel"], "title": "Spatio-Temporal Conditional Diffusion Models for Forecasting Future Multiple Sclerosis Lesion Masks Conditioned on Treatments", "comment": "Accepted to MICCAI 2025 (LMID Workshop)", "summary": "Image-based personalized medicine has the potential to transform healthcare,\nparticularly for diseases that exhibit heterogeneous progression such as\nMultiple Sclerosis (MS). In this work, we introduce the first treatment-aware\nspatio-temporal diffusion model that is able to generate future masks\ndemonstrating lesion evolution in MS. Our voxel-space approach incorporates\nmulti-modal patient data, including MRI and treatment information, to forecast\nnew and enlarging T2 (NET2) lesion masks at a future time point. Extensive\nexperiments on a multi-centre dataset of 2131 patient 3D MRIs from randomized\nclinical trials for relapsing-remitting MS demonstrate that our generative\nmodel is able to accurately predict NET2 lesion masks for patients across six\ndifferent treatments. Moreover, we demonstrate our model has the potential for\nreal-world clinical applications through downstream tasks such as future lesion\ncount and location estimation, binary lesion activity classification, and\ngenerating counterfactual future NET2 masks for several treatments with\ndifferent efficacies. This work highlights the potential of causal, image-based\ngenerative models as powerful tools for advancing data-driven prognostics in\nMS.", "AI": {"tldr": "本文提出了首个治疗感知的时空扩散模型，能够根据多模态患者数据（包括MRI和治疗信息）预测多发性硬化症（MS）患者未来的病灶演变，并在大型多中心数据集上验证了其预测新发和扩大T2病灶（NET2）的准确性及临床应用潜力。", "motivation": "图像引导的个性化医疗有望彻底改变医疗保健，特别是对于多发性硬化症（MS）等表现出异质性进展的疾病，需要更准确地预测疾病进展。", "method": "引入了第一个治疗感知的时空扩散模型，该模型在体素空间操作，整合了多模态患者数据（包括MRI和治疗信息），以预测未来时间点的新发和扩大T2（NET2）病灶掩膜。", "result": "在大规模多中心数据集（包含来自复发缓解型MS随机临床试验的2131名患者的3D MRI）上，该生成模型能够准确预测六种不同治疗方案下患者的NET2病灶掩膜。此外，该模型在下游任务中展现了实际临床应用潜力，例如未来病灶数量和位置估计、二元病灶活动分类，以及生成不同疗效治疗的未来反事实NET2掩膜。", "conclusion": "这项工作强调了基于图像的因果生成模型作为强大工具，在推动多发性硬化症数据驱动预后方面具有巨大潜力。"}}
{"id": "2508.08127", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08127", "abs": "https://arxiv.org/abs/2508.08127", "authors": ["Rui Miao", "Yixin Liu", "Yili Wang", "Xu Shen", "Yue Tan", "Yiwei Dai", "Shirui Pan", "Xin Wang"], "title": "BlindGuard: Safeguarding LLM-based Multi-Agent Systems under Unknown Attacks", "comment": null, "summary": "The security of LLM-based multi-agent systems (MAS) is critically threatened\nby propagation vulnerability, where malicious agents can distort collective\ndecision-making through inter-agent message interactions. While existing\nsupervised defense methods demonstrate promising performance, they may be\nimpractical in real-world scenarios due to their heavy reliance on labeled\nmalicious agents to train a supervised malicious detection model. To enable\npractical and generalizable MAS defenses, in this paper, we propose BlindGuard,\nan unsupervised defense method that learns without requiring any\nattack-specific labels or prior knowledge of malicious behaviors. To this end,\nwe establish a hierarchical agent encoder to capture individual, neighborhood,\nand global interaction patterns of each agent, providing a comprehensive\nunderstanding for malicious agent detection. Meanwhile, we design a\ncorruption-guided detector that consists of directional noise injection and\ncontrastive learning, allowing effective detection model training solely on\nnormal agent behaviors. Extensive experiments show that BlindGuard effectively\ndetects diverse attack types (i.e., prompt injection, memory poisoning, and\ntool attack) across MAS with various communication patterns while maintaining\nsuperior generalizability compared to supervised baselines. The code is\navailable at: https://github.com/MR9812/BlindGuard.", "AI": {"tldr": "本文提出BlindGuard，一种无监督防御方法，旨在解决LLM多智能体系统中恶意智能体通过消息交互扭曲集体决策的传播漏洞，无需攻击标签或先验知识即可有效检测多种攻击。", "motivation": "LLM多智能体系统面临传播漏洞威胁，恶意智能体可影响决策。现有监督防御方法过度依赖带标签的恶意智能体数据进行训练，在实际场景中不切实际。", "method": "提出BlindGuard，一种无监督防御方法。该方法包含一个分层智能体编码器，用于捕获每个智能体的个体、邻居和全局交互模式；以及一个腐败引导检测器，通过定向噪声注入和对比学习，仅利用正常智能体行为进行有效模型训练。", "result": "实验证明，BlindGuard能有效检测多种攻击类型（如提示注入、内存中毒、工具攻击），适用于不同通信模式的多智能体系统，并展现出优于监督基线的泛化能力。", "conclusion": "BlindGuard提供了一种实用且可泛化的无监督防御方案，能有效保护LLM多智能体系统免受传播漏洞的威胁，无需依赖攻击特异性标签或先验知识。"}}
{"id": "2508.07785", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.07785", "abs": "https://arxiv.org/abs/2508.07785", "authors": ["Haoyuan Wu", "Haoxing Chen", "Xiaodong Chen", "Zhanchao Zhou", "Tieyuan Chen", "Yihong Zhuang", "Guoshan Lu", "Zenan Huang", "Junbo Zhao", "Lin Liu", "Zhenzhong Lan", "Bei Yu", "Jianguo Li"], "title": "Grove MoE: Towards Efficient and Superior MoE LLMs with Adjugate Experts", "comment": null, "summary": "The Mixture of Experts (MoE) architecture is a cornerstone of modern\nstate-of-the-art (SOTA) large language models (LLMs). MoE models facilitate\nscalability by enabling sparse parameter activation. However, traditional MoE\narchitecture uses homogeneous experts of a uniform size, activating a fixed\nnumber of parameters irrespective of input complexity and thus limiting\ncomputational efficiency. To overcome this limitation, we introduce Grove MoE,\na novel architecture incorporating experts of varying sizes, inspired by the\nheterogeneous big.LITTLE CPU architecture. This architecture features novel\nadjugate experts with a dynamic activation mechanism, enabling model capacity\nexpansion while maintaining manageable computational overhead. Building on this\narchitecture, we present GroveMoE-Base and GroveMoE-Inst, 33B-parameter LLMs\ndeveloped by applying an upcycling strategy to the Qwen3-30B-A3B-Base model\nduring mid-training and post-training. GroveMoE models dynamically activate\n3.14-3.28B parameters based on token complexity and achieve performance\ncomparable to SOTA open-source models of similar or even larger size.", "AI": {"tldr": "该论文提出了Grove MoE架构，通过引入大小不一的异构专家和动态激活机制，解决了传统MoE模型计算效率低下的问题，实现了在参数激活量较少的情况下达到与SOTA模型相当的性能。", "motivation": "传统的MoE架构使用同质专家，无论输入复杂性如何都激活固定数量的参数，这限制了计算效率。因此，需要一种能够根据输入复杂性动态调整激活参数数量的MoE架构来提高效率。", "method": "引入了Grove MoE架构，其灵感来源于big.LITTLE CPU架构，包含大小不一的异构专家。该架构具有新颖的伴随专家和动态激活机制，可以在扩展模型容量的同时保持可控的计算开销。通过对Qwen3-30B-A3B-Base模型在中训和后训阶段应用“升级循环”策略，开发了33B参数的GroveMoE-Base和GroveMoE-Inst模型。", "result": "GroveMoE模型能够根据token的复杂性动态激活3.14-3.28B的参数，并且实现了与同等甚至更大规模的SOTA开源模型相当的性能。", "conclusion": "Grove MoE架构通过异构专家和动态激活机制，有效提升了MoE模型的计算效率，并在激活参数量较少的情况下，达到了高性能表现，证明了其在构建大型语言模型方面的潜力。"}}
{"id": "2508.07011", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2508.07011", "abs": "https://arxiv.org/abs/2508.07011", "authors": ["Zixiong Wang", "Jian Yang", "Yiwei Hu", "Milos Hasan", "Beibei Wang"], "title": "HiMat: DiT-based Ultra-High Resolution SVBRDF Generation", "comment": null, "summary": "Creating highly detailed SVBRDFs is essential for 3D content creation. The\nrise of high-resolution text-to-image generative models, based on diffusion\ntransformers (DiT), suggests an opportunity to finetune them for this task.\nHowever, retargeting the models to produce multiple aligned SVBRDF maps instead\nof just RGB images, while achieving high efficiency and ensuring consistency\nacross different maps, remains a challenge. In this paper, we introduce HiMat:\na memory- and computation-efficient diffusion-based framework capable of\ngenerating native 4K-resolution SVBRDFs. A key challenge we address is\nmaintaining consistency across different maps in a lightweight manner, without\nrelying on training new VAEs or significantly altering the DiT backbone (which\nwould damage its prior capabilities). To tackle this, we introduce the\nCrossStitch module, a lightweight convolutional module that captures inter-map\ndependencies through localized operations. Its weights are initialized such\nthat the DiT backbone operation is unchanged before finetuning starts. HiMat\nenables generation with strong structural coherence and high-frequency details.\nResults with a large set of text prompts demonstrate the effectiveness of our\napproach for 4K SVBRDF generation. Further experiments suggest generalization\nto tasks such as intrinsic decomposition.", "AI": {"tldr": "本文提出HiMat框架，通过轻量级CrossStitch模块微调扩散模型DiT，实现高效、一致地生成4K分辨率SVBRDF（空间可变双向反射分布函数）。", "motivation": "高细节SVBRDF对3D内容创作至关重要。高分辨率文本到图像生成模型（如DiT）的兴起，为生成SVBRDF提供了可能性。然而，将这些模型重定向以生成多个对齐的SVBRDF贴图，同时保持高效率和一致性，是一个挑战。", "method": "引入HiMat框架，一个内存和计算高效的扩散模型，用于生成原生4K分辨率SVBRDF。为解决多贴图间的一致性问题，同时避免训练新的VAE或大幅修改DiT骨干，本文提出了CrossStitch模块。这是一个轻量级卷积模块，通过局部操作捕获贴图间的依赖关系，其权重初始化确保在微调前不改变DiT骨干的原始操作。", "result": "HiMat能够生成具有强结构连贯性和高频细节的4K SVBRDF。大量文本提示的实验结果证明了该方法在4K SVBRDF生成方面的有效性。进一步的实验表明，该方法还可推广到内在分解等任务。", "conclusion": "HiMat通过引入创新的CrossStitch模块，成功地将现有的扩散变换器模型高效、一致地应用于4K分辨率SVBRDF的生成，解决了多贴图对齐和一致性的挑战，并展现了良好的泛化能力。"}}
{"id": "2508.08147", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08147", "abs": "https://arxiv.org/abs/2508.08147", "authors": ["Yunkai Hu", "Tianqiao Zhao", "Meng Yue"], "title": "From Natural Language to Solver-Ready Power System Optimization: An LLM-Assisted, Validation-in-the-Loop Framework", "comment": null, "summary": "This paper introduces a novel Large Language Models (LLMs)-assisted agent\nthat automatically converts natural-language descriptions of power system\noptimization scenarios into compact, solver-ready formulations and generates\ncorresponding solutions. In contrast to approaches that rely solely on LLM to\nproduce solutions directly, the proposed method focuses on discovering a\nmathematically compatible formulation that can be efficiently solved by\noff-the-shelf optimization solvers. Directly using LLMs to produce solutions\noften leads to infeasible or suboptimal results, as these models lack the\nnumerical precision and constraint-handling capabilities of established\noptimization solvers. The pipeline integrates a domain-aware prompt and schema\nwith an LLM, enforces feasibility through systematic validation and iterative\nrepair, and returns both solver-ready models and user-facing results. Using the\nunit commitment problem as a representative case study, the agent produces\noptimal or near-optimal schedules along with the associated objective costs.\nResults demonstrate that coupling the solver with task-specific validation\nsignificantly enhances solution reliability. This work shows that combining AI\nwith established optimization frameworks bridges high-level problem\ndescriptions and executable mathematical models, enabling more efficient\ndecision-making in energy systems", "AI": {"tldr": "本文提出一个LLM辅助代理，能将电力系统优化场景的自然语言描述自动转换为可求解的数学公式，并利用优化求解器生成解决方案，显著提高了求解的可靠性和精度。", "motivation": "直接使用LLMs求解优化问题常导致不可行或次优解，因为LLMs缺乏数值精度和处理复杂约束的能力。研究旨在弥合高层问题描述与可执行数学模型之间的鸿沟，提高能源系统决策效率。", "method": "该方法整合了领域感知的提示和模式与LLM，通过系统验证和迭代修复来确保解的可行性，最终返回求解器可用的模型和面向用户的结果。其核心思想是将AI与成熟的优化框架结合。", "result": "以机组组合问题为例，该代理能够生成最优或接近最优的调度方案及相关成本。结果表明，将优化求解器与任务特定的验证相结合，显著提升了解决方案的可靠性。", "conclusion": "该工作展示了将人工智能与现有优化框架结合，能够有效连接高层问题描述与可执行数学模型，从而在能源系统中实现更高效的决策。"}}
{"id": "2508.07805", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.07805", "abs": "https://arxiv.org/abs/2508.07805", "authors": ["Yerin Hwang", "Dongryeol Lee", "Taegwan Kang", "Yongil Kim", "Kyomin Jung"], "title": "Can You Trick the Grader? Adversarial Persuasion of LLM Judges", "comment": "19 pages, 8 figures", "summary": "As large language models take on growing roles as automated evaluators in\npractical settings, a critical question arises: Can individuals persuade an LLM\njudge to assign unfairly high scores? This study is the first to reveal that\nstrategically embedded persuasive language can bias LLM judges when scoring\nmathematical reasoning tasks, where correctness should be independent of\nstylistic variation. Grounded in Aristotle's rhetorical principles, we\nformalize seven persuasion techniques (Majority, Consistency, Flattery,\nReciprocity, Pity, Authority, Identity) and embed them into otherwise identical\nresponses. Across six math benchmarks, we find that persuasive language leads\nLLM judges to assign inflated scores to incorrect solutions, by up to 8% on\naverage, with Consistency causing the most severe distortion. Notably,\nincreasing model size does not substantially mitigate this vulnerability.\nFurther analysis demonstrates that combining multiple persuasion techniques\namplifies the bias, and pairwise evaluation is likewise susceptible. Moreover,\nthe persuasive effect persists under counter prompting strategies, highlighting\na critical vulnerability in LLM-as-a-Judge pipelines and underscoring the need\nfor robust defenses against persuasion-based attacks.", "AI": {"tldr": "研究发现，大语言模型（LLM）作为自动评估器时，即使在数学推理这类客观任务中，也会被嵌入式说服性语言所偏置，导致对错误答案给出过高分数，且这种漏洞难以通过模型增大或反向提示缓解。", "motivation": "随着LLM在实际应用中越来越多地担任自动评估器的角色，一个关键问题是：个体能否说服LLM评委给出不公平的高分？特别是在正确性应独立于风格差异的数学推理任务中，这种偏见是否存在。", "method": "基于亚里士士德的修辞原则，研究形式化了七种说服技巧（多数、一致性、奉承、互惠、怜悯、权威、身份），并将它们嵌入到内容相同但有说服性语言的回答中。通过在六个数学基准测试上进行评估，分析了说服语言对LLM评分的影响，并探讨了模型大小、多重技巧组合、成对评估以及反向提示策略的影响。", "result": "研究发现，说服性语言导致LLM评委对不正确的解决方案给出过高的分数，平均最高可达8%，其中“一致性”技巧导致了最严重的扭曲。值得注意的是，增加模型大小未能显著减轻这种脆弱性。进一步分析表明，结合多种说服技巧会放大偏见，成对评估也同样易受影响。此外，说服效应在反向提示策略下依然存在。", "conclusion": "LLM作为评委的管道存在关键的脆弱性，易受基于说服的攻击影响。这凸显了开发鲁棒防御机制的必要性，以应对LLM评估中的潜在偏见和操纵。"}}
{"id": "2508.07020", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07020", "abs": "https://arxiv.org/abs/2508.07020", "authors": ["Tanjim Bin Faruk", "Abdul Matin", "Shrideep Pallickara", "Sangmi Lee Pallickara"], "title": "TerraMAE: Learning Spatial-Spectral Representations from Hyperspectral Earth Observation Data via Adaptive Masked Autoencoders", "comment": null, "summary": "Hyperspectral satellite imagery offers sub-30 m views of Earth in hundreds of\ncontiguous spectral bands, enabling fine-grained mapping of soils, crops, and\nland cover. While self-supervised Masked Autoencoders excel on RGB and low-band\nmultispectral data, they struggle to exploit the intricate spatial-spectral\ncorrelations in 200+ band hyperspectral images. We introduce TerraMAE, a novel\nHSI encoding framework specifically designed to learn highly representative\nspatial-spectral embeddings for diverse geospatial analyses. TerraMAE features\nan adaptive channel grouping strategy, based on statistical reflectance\nproperties to capture spectral similarities, and an enhanced reconstruction\nloss function that incorporates spatial and spectral quality metrics. We\ndemonstrate TerraMAE's effectiveness through superior spatial-spectral\ninformation preservation in high-fidelity image reconstruction. Furthermore, we\nvalidate its practical utility and the quality of its learned representations\nthrough strong performance on three key downstream geospatial tasks: crop\nidentification, land cover classification, and soil texture prediction.", "AI": {"tldr": "TerraMAE是一种新型高光谱图像（HSI）编码框架，通过自适应通道分组和增强重建损失，有效学习空间光谱嵌入，优于传统MAE并适用于多种地理空间任务。", "motivation": "传统自监督掩码自动编码器（MAE）在处理具有数百个连续光谱波段的高光谱图像时，难以有效利用其复杂的空间光谱关联性，导致在精细地理空间分析中表现不佳。", "method": "引入TerraMAE框架，其核心是：1) 基于统计反射特性的自适应通道分组策略，以捕获光谱相似性；2) 增强的重建损失函数，该函数结合了空间和光谱质量度量。", "result": "TerraMAE在高保真图像重建中表现出卓越的空间光谱信息保留能力。在三个关键的下游地理空间任务（作物识别、土地覆盖分类和土壤质地预测）中，其学习到的表示展现出强大的性能和实用性。", "conclusion": "TerraMAE成功地为高光谱图像学习了高度代表性的空间光谱嵌入，有效解决了现有方法在处理复杂高光谱数据时的局限性，并为多种地理空间分析提供了高质量的特征表示。"}}
{"id": "2505.23197", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2505.23197", "abs": "https://arxiv.org/abs/2505.23197", "authors": ["Jatin Kumar Arora", "Shubhendu Bhasin"], "title": "UPP: Unified Path Planner with Adaptive Safety and Optimality", "comment": "8 pages,11 figures", "summary": "We are surrounded by robots helping us perform complex tasks. Robots have a\nwide range of applications, from industrial automation to personalized\nassistance. However, with great technological innovation come significant\nchallenges. One of the major challenges in robotics is path planning. Despite\nadvancements such as graph search, sampling, and potential field methods, most\npath planning algorithms focus either on optimality or on safety. Very little\nresearch addresses both simultaneously. We propose a Unified Path Planner (UPP)\nthat uses modified heuristics and a dynamic safety cost function to balance\nsafety and optimality. The level of safety can be adjusted via tunable\nparameters, trading off against computational complexity. We demonstrate the\nplanner's performance in simulations, showing how parameter variation affects\nresults. UPP is compared with various traditional and safe-optimal planning\nalgorithms across different scenarios. We also validate it on a TurtleBot,\nwhere the robot successfully finds safe and sub-optimal paths.", "AI": {"tldr": "本文提出了一种名为UPP（统一路径规划器）的新算法，旨在同时平衡机器人路径规划中的安全性与最优性，并通过可调参数实现安全性和计算复杂度的权衡。", "motivation": "机器人路径规划是主要挑战之一。尽管现有算法在最优性或安全性方面有所进展，但很少有研究能同时解决这两个问题。", "method": "提出UPP，该算法采用改进的启发式方法和动态安全成本函数来平衡安全性和最优性。安全性水平可通过可调参数进行调整，以权衡计算复杂度。", "result": "通过仿真展示了规划器的性能，并分析了参数变化对结果的影响。UPP与各种传统和安全-最优规划算法在不同场景下进行了比较。还在TurtleBot上进行了验证，机器人成功找到了安全且次优的路径。", "conclusion": "UPP成功地在机器人路径规划中实现了安全性与最优性的平衡，并提供了可调节的安全级别，在仿真和实际机器人上均表现良好。"}}
{"id": "2508.07810", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.07810", "abs": "https://arxiv.org/abs/2508.07810", "authors": ["Olga Kellert", "Muhammad Imran", "Nicholas Hill Matlis", "Mahmud Uz Zaman", "Carlos Gómez-Rodríguez"], "title": "Evaluating Compositional Approaches for Focus and Sentiment Analysis", "comment": null, "summary": "This paper summarizes the results of evaluating a compositional approach for\nFocus Analysis (FA) in Linguistics and Sentiment Analysis (SA) in Natural\nLanguage Processing (NLP). While quantitative evaluations of compositional and\nnon-compositional approaches in SA exist in NLP, similar quantitative\nevaluations are very rare in FA in Linguistics that deal with linguistic\nexpressions representing focus or emphasis such as \"it was John who left\". We\nfill this gap in research by arguing that compositional rules in SA also apply\nto FA because FA and SA are closely related meaning that SA is part of FA. Our\ncompositional approach in SA exploits basic syntactic rules such as rules of\nmodification, coordination, and negation represented in the formalism of\nUniversal Dependencies (UDs) in English and applied to words representing\nsentiments from sentiment dictionaries. Some of the advantages of our\ncompositional analysis method for SA in contrast to non-compositional analysis\nmethods are interpretability and explainability. We test the accuracy of our\ncompositional approach and compare it with a non-compositional approach VADER\nthat uses simple heuristic rules to deal with negation, coordination and\nmodification. In contrast to previous related work that evaluates\ncompositionality in SA on long reviews, this study uses more appropriate\ndatasets to evaluate compositionality. In addition, we generalize the results\nof compositional approaches in SA to compositional approaches in FA.", "AI": {"tldr": "本文评估了一种组合式方法在语言学焦点分析（FA）和自然语言处理情感分析（SA）中的应用，并论证了SA是FA的一部分，通过对比非组合式方法验证其准确性。", "motivation": "语言学焦点分析（FA）中对组合式方法的定量评估非常罕见，而情感分析（SA）中此类评估相对较多。作者旨在填补FA研究的这一空白，并提出SA是FA的一部分，因此SA中的组合规则也适用于FA。", "method": "采用基于通用依存关系（UDs）形式化的基本句法规则（如修饰、并列、否定）的组合式方法，应用于情感词典中的词语。通过更合适的数据集测试其准确性，并与使用启发式规则的非组合式方法VADER进行对比。", "result": "论文测试了组合式方法的准确性，并将其与非组合式方法VADER进行了比较。研究结果表明，组合式方法在可解释性方面具有优势，并且情感分析中的组合式方法结果可以推广到焦点分析中。", "conclusion": "组合式分析方法在情感分析中具有可解释性和可解释性优势。情感分析中的组合式方法结果可以推广到焦点分析中，进一步支持了情感分析是焦点分析一部分的观点。"}}
{"id": "2508.07021", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07021", "abs": "https://arxiv.org/abs/2508.07021", "authors": ["Kun Qian", "Wenjie Li", "Tianyu Sun", "Wenhong Wang", "Wenhan Luo"], "title": "DocRefine: An Intelligent Framework for Scientific Document Understanding and Content Optimization based on Multimodal Large Model Agents", "comment": null, "summary": "The exponential growth of scientific literature in PDF format necessitates\nadvanced tools for efficient and accurate document understanding,\nsummarization, and content optimization. Traditional methods fall short in\nhandling complex layouts and multimodal content, while direct application of\nLarge Language Models (LLMs) and Vision-Language Large Models (LVLMs) lacks\nprecision and control for intricate editing tasks. This paper introduces\nDocRefine, an innovative framework designed for intelligent understanding,\ncontent refinement, and automated summarization of scientific PDF documents,\ndriven by natural language instructions. DocRefine leverages the power of\nadvanced LVLMs (e.g., GPT-4o) by orchestrating a sophisticated multi-agent\nsystem comprising six specialized and collaborative agents: Layout & Structure\nAnalysis, Multimodal Content Understanding, Instruction Decomposition, Content\nRefinement, Summarization & Generation, and Fidelity & Consistency\nVerification. This closed-loop feedback architecture ensures high semantic\naccuracy and visual fidelity. Evaluated on the comprehensive DocEditBench\ndataset, DocRefine consistently outperforms state-of-the-art baselines across\nvarious tasks, achieving overall scores of 86.7% for Semantic Consistency Score\n(SCS), 93.9% for Layout Fidelity Index (LFI), and 85.0% for Instruction\nAdherence Rate (IAR). These results demonstrate DocRefine's superior capability\nin handling complex multimodal document editing, preserving semantic integrity,\nand maintaining visual consistency, marking a significant advancement in\nautomated scientific document processing.", "AI": {"tldr": "DocRefine是一个基于多智能体系统和LVLM的框架，用于智能理解、内容优化和自动总结科学PDF文档，在复杂编辑任务中表现优异。", "motivation": "科学文献PDF格式的指数级增长需要高效精确的文档理解、总结和内容优化工具。传统方法难以处理复杂布局和多模态内容，而直接应用LLM和LVLMs在复杂编辑任务中缺乏精度和控制力。", "method": "DocRefine利用先进的LVLM（如GPT-4o），通过编排一个复杂的多智能体系统实现，该系统包含六个专业协作的智能体：布局与结构分析、多模态内容理解、指令分解、内容优化、总结与生成，以及保真度与一致性验证。其采用闭环反馈架构，确保高语义准确性和视觉保真度。", "result": "在DocEditBench数据集上，DocRefine在各种任务中持续优于现有基线，语义一致性得分（SCS）达到86.7%，布局保真度指数（LFI）达到93.9%，指令遵循率（IAR）达到85.0%。", "conclusion": "DocRefine在处理复杂多模态文档编辑、保持语义完整性和视觉一致性方面表现出卓越能力，标志着自动化科学文档处理的重大进步。"}}
{"id": "2508.07827", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.07827", "abs": "https://arxiv.org/abs/2508.07827", "authors": ["Yu-Min Tseng", "Wei-Lin Chen", "Chung-Chi Chen", "Hsin-Hsi Chen"], "title": "Evaluating Large Language Models as Expert Annotators", "comment": "Accepted to COLM 2025", "summary": "Textual data annotation, the process of labeling or tagging text with\nrelevant information, is typically costly, time-consuming, and labor-intensive.\nWhile large language models (LLMs) have demonstrated their potential as direct\nalternatives to human annotators for general domains natural language\nprocessing (NLP) tasks, their effectiveness on annotation tasks in domains\nrequiring expert knowledge remains underexplored. In this paper, we\ninvestigate: whether top-performing LLMs, which might be perceived as having\nexpert-level proficiency in academic and professional benchmarks, can serve as\ndirect alternatives to human expert annotators? To this end, we evaluate both\nindividual LLMs and multi-agent approaches across three highly specialized\ndomains: finance, biomedicine, and law. Specifically, we propose a multi-agent\ndiscussion framework to simulate a group of human annotators, where LLMs are\ntasked to engage in discussions by considering others' annotations and\njustifications before finalizing their labels. Additionally, we incorporate\nreasoning models (e.g., o3-mini) to enable a more comprehensive comparison. Our\nempirical results reveal that: (1) Individual LLMs equipped with inference-time\ntechniques (e.g., chain-of-thought (CoT), self-consistency) show only marginal\nor even negative performance gains, contrary to prior literature suggesting\ntheir broad effectiveness. (2) Overall, reasoning models do not demonstrate\nstatistically significant improvements over non-reasoning models in most\nsettings. This suggests that extended long CoT provides relatively limited\nbenefits for data annotation in specialized domains. (3) Certain model\nbehaviors emerge in the multi-agent discussion environment. For instance,\nClaude 3.7 Sonnet with thinking rarely changes its initial annotations, even\nwhen other agents provide correct annotations or valid reasoning.", "AI": {"tldr": "本文研究了大型语言模型（LLMs）在金融、生物医学和法律等专业领域作为人类专家标注员的直接替代品的有效性，发现LLMs在此类任务中表现不佳，且链式思考（CoT）等推理技术和多智能体讨论框架带来的性能提升有限或不显著。", "motivation": "文本数据标注成本高、耗时且劳动密集。尽管LLMs在通用NLP任务中展现了作为人类标注员的潜力，但它们在需要专家知识的专业领域中进行标注任务的有效性尚未得到充分探索。", "method": "研究评估了单个LLMs和多智能体方法在金融、生物医学和法律三个高度专业化领域的表现。具体地，提出了一个多智能体讨论框架，模拟一组人类标注员进行讨论，LLMs在最终确定标签前会考虑其他标注和理由。此外，还引入了推理模型（如o3-mini）进行更全面的比较。", "result": "1. 装备推理时技术（如CoT、自我一致性）的单个LLMs仅显示出微弱甚至负面的性能提升，与先前文献中普遍有效性的说法相悖。2. 总体而言，在大多数设置中，推理模型并未比非推理模型表现出统计学上的显著改进，这表明扩展的CoT对专业领域的数据标注益处有限。3. 在多智能体讨论环境中出现某些模型行为，例如，Claude 3.7 Sonnet在思考后很少改变其初始标注，即使其他智能体提供了正确标注或有效理由。", "conclusion": "LLMs，即使是顶级模型，在需要专家知识的专业领域中，目前还不能作为人类专家标注员的直接替代品。常用的推理技术（如CoT）和多智能体讨论框架在这些领域的数据标注任务中，其提升效果有限或不显著。"}}
{"id": "2508.07023", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07023", "abs": "https://arxiv.org/abs/2508.07023", "authors": ["Jingwei Peng", "Jiehao Chen", "Mateo Alejandro Rojas", "Meilin Zhang"], "title": "MV-CoRe: Multimodal Visual-Conceptual Reasoning for Complex Visual Question Answering", "comment": null, "summary": "Complex Visual Question Answering (Complex VQA) tasks, which demand\nsophisticated multi-modal reasoning and external knowledge integration, present\nsignificant challenges for existing large vision-language models (LVLMs) often\nlimited by their reliance on high-level global features. To address this, we\npropose MV-CoRe (Multimodal Visual-Conceptual Reasoning), a novel model\ndesigned to enhance Complex VQA performance through the deep fusion of diverse\nvisual and linguistic information. MV-CoRe meticulously integrates global\nembeddings from pre-trained Vision Large Models (VLMs) and Language Large\nModels (LLMs) with fine-grained semantic-aware visual features, including\nobject detection characteristics and scene graph representations. An innovative\nMultimodal Fusion Transformer then processes and deeply integrates these\ndiverse feature sets, enabling rich cross-modal attention and facilitating\ncomplex reasoning. We evaluate MV-CoRe on challenging Complex VQA benchmarks,\nincluding GQA, A-OKVQA, and OKVQA, after training on VQAv2. Our experimental\nresults demonstrate that MV-CoRe consistently outperforms established LVLM\nbaselines, achieving an overall accuracy of 77.5% on GQA. Ablation studies\nconfirm the critical contribution of both object and scene graph features, and\nhuman evaluations further validate MV-CoRe's superior factual correctness and\nreasoning depth, underscoring its robust capabilities for deep visual and\nconceptual understanding.", "AI": {"tldr": "MV-CoRe通过深度融合多模态视觉和语言信息，提升复杂视觉问答（Complex VQA）性能。", "motivation": "现有大型视觉-语言模型（LVLMs）在复杂视觉问答任务中表现不佳，因为它们过度依赖高层全局特征，缺乏精细的多模态推理和外部知识整合能力。", "method": "提出MV-CoRe模型，它将预训练的视觉大模型（VLMs）和语言大模型（LLMs）的全局嵌入与细粒度的语义感知视觉特征（包括目标检测特征和场景图表示）进行深度融合。一个创新的多模态融合Transformer处理并整合这些多样化的特征集，实现丰富的跨模态注意力并促进复杂推理。", "result": "MV-CoRe在GQA、A-OKVQA和OKVQA等复杂VQA基准测试中，始终优于现有LVLM基线，在GQA上实现了77.5%的整体准确率。消融研究证实了目标和场景图特征的关键贡献，人工评估进一步验证了MV-CoRe卓越的事实正确性和推理深度。", "conclusion": "MV-CoRe在复杂视觉问答任务中展示了强大的深度视觉和概念理解能力。"}}
{"id": "2508.07849", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.07849", "abs": "https://arxiv.org/abs/2508.07849", "authors": ["Amrita Singh", "H. Suhan Karaca", "Aditya Joshi", "Hye-young Paik", "Jiaojiao Jiang"], "title": "LLMs for Law: Evaluating Legal-Specific LLMs on Contract Understanding", "comment": "Under review. 4 pages + references", "summary": "Despite advances in legal NLP, no comprehensive evaluation covering multiple\nlegal-specific LLMs currently exists for contract classification tasks in\ncontract understanding. To address this gap, we present an evaluation of 10\nlegal-specific LLMs on three English language contract understanding tasks and\ncompare them with 7 general-purpose LLMs. The results show that legal-specific\nLLMs consistently outperform general-purpose models, especially on tasks\nrequiring nuanced legal understanding. Legal-BERT and Contracts-BERT establish\nnew SOTAs on two of the three tasks, despite having 69% fewer parameters than\nthe best-performing general-purpose LLM. We also identify CaseLaw-BERT and\nLexLM as strong additional baselines for contract understanding. Our results\nprovide a holistic evaluation of legal-specific LLMs and will facilitate the\ndevelopment of more accurate contract understanding systems.", "AI": {"tldr": "本文评估了10个法律领域专用大语言模型（LLMs）和7个通用LLMs在合同分类任务上的表现，发现法律专用模型显著优于通用模型，其中Legal-BERT和Contracts-BERT在某些任务上达到新的SOTA。", "motivation": "尽管法律NLP取得了进展，但目前缺乏针对合同理解中合同分类任务的、涵盖多种法律专用LLMs的全面评估。", "method": "研究评估了10个法律专用LLMs和7个通用LLMs在三个英文合同理解任务上的性能。通过对比它们的表现，特别是参数量较小的法律专用模型与最佳通用模型的对比。", "result": "结果显示，法律专用LLMs始终优于通用模型，尤其是在需要细致法律理解的任务上。Legal-BERT和Contracts-BERT在三个任务中的两个上建立了新的SOTA，尽管它们的参数量比表现最佳的通用LLM少69%。此外，CaseLaw-BERT和LexLM也被认为是合同理解的强基线模型。", "conclusion": "本研究提供了对法律专用LLMs的全面评估，其结果将有助于开发更准确的合同理解系统。"}}
{"id": "2508.07028", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07028", "abs": "https://arxiv.org/abs/2508.07028", "authors": ["Juntong Fan", "Shuyi Fan", "Debesh Jha", "Changsheng Fang", "Tieyong Zeng", "Hengyong Yu", "Dayang Wang"], "title": "Large Language Model Evaluated Stand-alone Attention-Assisted Graph Neural Network with Spatial and Structural Information Interaction for Precise Endoscopic Image Segmentation", "comment": "Manuscript under review", "summary": "Accurate endoscopic image segmentation on the polyps is critical for early\ncolorectal cancer detection. However, this task remains challenging due to low\ncontrast with surrounding mucosa, specular highlights, and indistinct\nboundaries. To address these challenges, we propose FOCUS-Med, which stands for\nFusion of spatial and structural graph with attentional context-aware polyp\nsegmentation in endoscopic medical imaging. FOCUS-Med integrates a Dual Graph\nConvolutional Network (Dual-GCN) module to capture contextual spatial and\ntopological structural dependencies. This graph-based representation enables\nthe model to better distinguish polyps from background tissues by leveraging\ntopological cues and spatial connectivity, which are often obscured in raw\nimage intensities. It enhances the model's ability to preserve boundaries and\ndelineate complex shapes typical of polyps. In addition, a location-fused\nstand-alone self-attention is employed to strengthen global context\nintegration. To bridge the semantic gap between encoder-decoder layers, we\nincorporate a trainable weighted fast normalized fusion strategy for efficient\nmulti-scale aggregation. Notably, we are the first to introduce the use of a\nLarge Language Model (LLM) to provide detailed qualitative evaluations of\nsegmentation quality. Extensive experiments on public benchmarks demonstrate\nthat FOCUS-Med achieves state-of-the-art performance across five key metrics,\nunderscoring its effectiveness and clinical potential for AI-assisted\ncolonoscopy.", "AI": {"tldr": "本文提出了FOCUS-Med模型，通过融合空间和结构图、引入自注意力机制和多尺度融合策略，解决了内窥镜图像中息肉分割的低对比度、高光和边界模糊问题，并首次使用LLM进行定性评估，实现了最先进的性能。", "motivation": "内窥镜图像中息肉的精确分割对于早期结直肠癌检测至关重要，但由于息肉与周围黏膜对比度低、存在镜面高光以及边界不清晰，使得这项任务极具挑战性。", "method": "提出了FOCUS-Med模型，其核心包括：1) 双图卷积网络（Dual-GCN）模块，用于捕获上下文空间和拓扑结构依赖；2) 位置融合的独立自注意力机制，以加强全局上下文集成；3) 可训练的加权快速归一化融合策略，用于高效的多尺度聚合，弥合编解码器层之间的语义鸿沟。此外，首次引入大型语言模型（LLM）提供分割质量的详细定性评估。", "result": "在公共基准测试中，FOCUS-Med模型在五个关键指标上均达到了最先进的性能。", "conclusion": "FOCUS-Med模型在内窥镜息肉分割任务中表现出卓越的有效性和临床潜力，有望应用于AI辅助结肠镜检查。"}}
{"id": "2508.07031", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07031", "abs": "https://arxiv.org/abs/2508.07031", "authors": ["Anindya Bijoy Das", "Shahnewaz Karim Sakib", "Shibbir Ahmed"], "title": "Trustworthy Medical Imaging with Large Language Models: A Study of Hallucinations Across Modalities", "comment": null, "summary": "Large Language Models (LLMs) are increasingly applied to medical imaging\ntasks, including image interpretation and synthetic image generation. However,\nthese models often produce hallucinations, which are confident but incorrect\noutputs that can mislead clinical decisions. This study examines hallucinations\nin two directions: image to text, where LLMs generate reports from X-ray, CT,\nor MRI scans, and text to image, where models create medical images from\nclinical prompts. We analyze errors such as factual inconsistencies and\nanatomical inaccuracies, evaluating outputs using expert informed criteria\nacross imaging modalities. Our findings reveal common patterns of hallucination\nin both interpretive and generative tasks, with implications for clinical\nreliability. We also discuss factors contributing to these failures, including\nmodel architecture and training data. By systematically studying both image\nunderstanding and generation, this work provides insights into improving the\nsafety and trustworthiness of LLM driven medical imaging systems.", "AI": {"tldr": "本研究系统性分析了大型语言模型（LLMs）在医学影像领域（包括图像到文本和文本到图像）中产生的幻觉，揭示了其常见模式及影响因素，旨在提升系统安全性和可信度。", "motivation": "LLMs在医学影像任务中应用日益广泛，但其产生的幻觉（自信但错误的输出）可能误导临床决策，影响系统可靠性。因此，需要深入研究这些幻觉。", "method": "研究从两个方向分析幻觉：1. 图像到文本（LLMs从X光、CT、MRI生成报告）；2. 文本到图像（模型从临床提示生成医学图像）。通过专家标准，评估输出中的事实不一致和解剖学不准确性。", "result": "研究揭示了解释性任务和生成性任务中幻觉的常见模式，并讨论了导致这些失败的因素，包括模型架构和训练数据。", "conclusion": "通过系统研究图像理解和生成中的失败，本工作为提高LLM驱动的医学影像系统的安全性和可信度提供了见解。"}}
{"id": "2508.07860", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.07860", "abs": "https://arxiv.org/abs/2508.07860", "authors": ["Jakub Šmíd", "Pavel Přibáň", "Pavel Král"], "title": "Large Language Models for Czech Aspect-Based Sentiment Analysis", "comment": "Accepted for presentation at the 28th International Conference on\n  Text, Speech and Dialogue (TSD 2025)", "summary": "Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis\ntask that aims to identify sentiment toward specific aspects of an entity.\nWhile large language models (LLMs) have shown strong performance in various\nnatural language processing (NLP) tasks, their capabilities for Czech ABSA\nremain largely unexplored. In this work, we conduct a comprehensive evaluation\nof 19 LLMs of varying sizes and architectures on Czech ABSA, comparing their\nperformance in zero-shot, few-shot, and fine-tuning scenarios. Our results show\nthat small domain-specific models fine-tuned for ABSA outperform\ngeneral-purpose LLMs in zero-shot and few-shot settings, while fine-tuned LLMs\nachieve state-of-the-art results. We analyze how factors such as\nmultilingualism, model size, and recency influence performance and present an\nerror analysis highlighting key challenges, particularly in aspect term\nprediction. Our findings provide insights into the suitability of LLMs for\nCzech ABSA and offer guidance for future research in this area.", "AI": {"tldr": "该研究全面评估了19个LLM在捷克语ABSA任务上的表现，比较了零样本、少样本和微调设置，并发现微调后的LLM能达到最先进水平。", "motivation": "尽管大型语言模型（LLMs）在各种自然语言处理（NLP）任务中表现出色，但它们在捷克语基于方面的情感分析（ABSA）方面的能力尚未得到充分探索。", "method": "研究对19个不同大小和架构的LLM在捷克语ABSA任务上进行了全面评估，比较了它们在零样本、少样本和微调场景下的性能。同时，分析了多语言性、模型大小和新近度等因素对性能的影响，并进行了错误分析。", "result": "结果表明，针对ABSA任务微调的小型领域特定模型在零样本和少样本设置下优于通用LLM，而经过微调的LLM则实现了最先进（SOTA）的结果。研究还指出，在方面词预测方面存在主要挑战。", "conclusion": "研究结果为LLM在捷克语ABSA任务中的适用性提供了见解，并为该领域的未来研究提供了指导。"}}
{"id": "2508.07038", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07038", "abs": "https://arxiv.org/abs/2508.07038", "authors": ["Yuke Xing", "William Gordon", "Qi Yang", "Kaifa Yang", "Jiarui Wang", "Yiling Xu"], "title": "3DGS-VBench: A Comprehensive Video Quality Evaluation Benchmark for 3DGS Compression", "comment": null, "summary": "3D Gaussian Splatting (3DGS) enables real-time novel view synthesis with high\nvisual fidelity, but its substantial storage requirements hinder practical\ndeployment, prompting state-of-the-art (SOTA) 3DGS methods to incorporate\ncompression modules. However, these 3DGS generative compression techniques\nintroduce unique distortions lacking systematic quality assessment research. To\nthis end, we establish 3DGS-VBench, a large-scale Video Quality Assessment\n(VQA) Dataset and Benchmark with 660 compressed 3DGS models and video sequences\ngenerated from 11 scenes across 6 SOTA 3DGS compression algorithms with\nsystematically designed parameter levels. With annotations from 50\nparticipants, we obtained MOS scores with outlier removal and validated dataset\nreliability. We benchmark 6 3DGS compression algorithms on storage efficiency\nand visual quality, and evaluate 15 quality assessment metrics across multiple\nparadigms. Our work enables specialized VQA model training for 3DGS, serving as\na catalyst for compression and quality assessment research. The dataset is\navailable at https://github.com/YukeXing/3DGS-VBench.", "AI": {"tldr": "本文建立了3DGS-VBench，一个大规模的视频质量评估（VQA）数据集和基准，用于系统性评估3DGS压缩引入的失真，并基准测试了压缩算法和质量评估指标。", "motivation": "3D Gaussian Splatting (3DGS) 虽然能实现高保真实时新视图合成，但其巨大的存储需求限制了实际部署。现有压缩方法会引入独特的失真，而目前缺乏系统性的质量评估研究。", "method": "研究者创建了3DGS-VBench数据集，包含660个压缩的3DGS模型和视频序列，这些数据来自11个场景，通过6种最先进的3DGS压缩算法在系统设计的参数级别下生成。通过50名参与者的标注，获得了平均主观得分（MOS），并进行了离群值剔除和可靠性验证。同时，基准测试了6种3DGS压缩算法的存储效率和视觉质量，并评估了15种质量评估指标。", "result": "研究成功建立了3DGS-VBench数据集，提供了带有MOS分数的压缩3DGS模型和视频序列。该工作基准测试了多种3DGS压缩算法在存储效率和视觉质量方面的表现，并评估了多种质量评估指标的有效性。", "conclusion": "3DGS-VBench数据集的建立将促进3DGS专用VQA模型的训练，并为3DGS压缩和质量评估领域的研究提供催化剂。"}}
{"id": "2508.07128", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07128", "abs": "https://arxiv.org/abs/2508.07128", "authors": ["Gregory Schuit", "Denis Parra", "Cecilia Besa"], "title": "Perceptual Evaluation of GANs and Diffusion Models for Generating X-rays", "comment": "Accepted to the Workshop on Human-AI Collaboration at MICCAI 2025", "summary": "Generative image models have achieved remarkable progress in both natural and\nmedical imaging. In the medical context, these techniques offer a potential\nsolution to data scarcity-especially for low-prevalence anomalies that impair\nthe performance of AI-driven diagnostic and segmentation tools. However,\nquestions remain regarding the fidelity and clinical utility of synthetic\nimages, since poor generation quality can undermine model generalizability and\ntrust. In this study, we evaluate the effectiveness of state-of-the-art\ngenerative models-Generative Adversarial Networks (GANs) and Diffusion Models\n(DMs)-for synthesizing chest X-rays conditioned on four abnormalities:\nAtelectasis (AT), Lung Opacity (LO), Pleural Effusion (PE), and Enlarged\nCardiac Silhouette (ECS). Using a benchmark composed of real images from the\nMIMIC-CXR dataset and synthetic images from both GANs and DMs, we conducted a\nreader study with three radiologists of varied experience. Participants were\nasked to distinguish real from synthetic images and assess the consistency\nbetween visual features and the target abnormality. Our results show that while\nDMs generate more visually realistic images overall, GANs can report better\naccuracy for specific conditions, such as absence of ECS. We further identify\nvisual cues radiologists use to detect synthetic images, offering insights into\nthe perceptual gaps in current models. These findings underscore the\ncomplementary strengths of GANs and DMs and point to the need for further\nrefinement to ensure generative models can reliably augment training datasets\nfor AI diagnostic systems.", "AI": {"tldr": "本研究评估了GAN和扩散模型在合成胸部X光片中的效果，发现扩散模型在视觉真实性上更优，GAN在特定条件下表现更好，并识别了放射科医生区分真实与合成图像的视觉线索。", "motivation": "医学影像领域数据稀缺，特别是低流行率异常数据，限制了AI诊断工具的性能。生成模型有望解决此问题，但合成图像的保真度和临床实用性仍存疑。", "method": "使用GAN和扩散模型合成带有四种异常（肺不张、肺部混浊、胸腔积液、心脏扩大）的胸部X光片。构建包含真实（MIMIC-CXR）和合成图像的基准，并进行读者研究，三位放射科医生区分真实与合成图像，并评估视觉特征与目标异常的一致性。", "result": "扩散模型生成的图像总体视觉真实性更高，但GAN在特定条件（如无心脏扩大）下表现出更高的准确性。研究还识别了放射科医生用于检测合成图像的视觉线索。", "conclusion": "GAN和扩散模型各有优势，需要进一步改进以确保生成模型能可靠地扩充AI诊断系统的训练数据集。"}}
{"id": "2508.07866", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.07866", "abs": "https://arxiv.org/abs/2508.07866", "authors": ["Jakub Šmíd", "Pavel Přibáň", "Pavel Král"], "title": "Few-shot Cross-lingual Aspect-Based Sentiment Analysis with Sequence-to-Sequence Models", "comment": "Accepted for presentation at the 28th International Conference on\n  Text, Speech and Dialogue (TSD 2025)", "summary": "Aspect-based sentiment analysis (ABSA) has received substantial attention in\nEnglish, yet challenges remain for low-resource languages due to the scarcity\nof labelled data. Current cross-lingual ABSA approaches often rely on external\ntranslation tools and overlook the potential benefits of incorporating a small\nnumber of target language examples into training. In this paper, we evaluate\nthe effect of adding few-shot target language examples to the training set\nacross four ABSA tasks, six target languages, and two sequence-to-sequence\nmodels. We show that adding as few as ten target language examples\nsignificantly improves performance over zero-shot settings and achieves a\nsimilar effect to constrained decoding in reducing prediction errors.\nFurthermore, we demonstrate that combining 1,000 target language examples with\nEnglish data can even surpass monolingual baselines. These findings offer\npractical insights for improving cross-lingual ABSA in low-resource and\ndomain-specific settings, as obtaining ten high-quality annotated examples is\nboth feasible and highly effective.", "AI": {"tldr": "研究表明，在跨语言方面情感分析（ABSA）中，即使只添加少量（如10个）目标语言样本，也能显著提升低资源语言的性能，甚至少量目标语言数据与英文数据结合可超越单语基线。", "motivation": "现有跨语言方面情感分析（ABSA）方法在低资源语言上面临标注数据稀缺的挑战，且常依赖外部翻译工具，忽视了将少量目标语言样本纳入训练的潜在益处。", "method": "作者评估了在训练集中添加少量目标语言样本的效果。实验涵盖了四种ABSA任务、六种目标语言和两种序列到序列模型。", "result": "结果显示，仅添加十个目标语言样本就能显著优于零样本设置，并达到与约束解码相似的错误减少效果。此外，结合1000个目标语言样本和英文数据甚至能超越单语基线。", "conclusion": "这些发现为改进低资源和特定领域设置下的跨语言方面情感分析提供了实用的见解，表明获取十个高质量标注样本是可行且高效的。"}}
{"id": "2508.07041", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07041", "abs": "https://arxiv.org/abs/2508.07041", "authors": ["Junkai Liu", "Nay Aung", "Theodoros N. Arvanitis", "Stefan K. Piechnik", "Joao A C Lima", "Steffen E. Petersen", "Le Zhang"], "title": "SAGCNet: Spatial-Aware Graph Completion Network for Missing Slice Imputation in Population CMR Imaging", "comment": "Accepted by MICCAI 2025", "summary": "Magnetic resonance imaging (MRI) provides detailed soft-tissue\ncharacteristics that assist in disease diagnosis and screening. However, the\naccuracy of clinical practice is often hindered by missing or unusable slices\ndue to various factors. Volumetric MRI synthesis methods have been developed to\naddress this issue by imputing missing slices from available ones. The inherent\n3D nature of volumetric MRI data, such as cardiac magnetic resonance (CMR),\nposes significant challenges for missing slice imputation approaches, including\n(1) the difficulty of modeling local inter-slice correlations and dependencies\nof volumetric slices, and (2) the limited exploration of crucial 3D spatial\ninformation and global context. In this study, to mitigate these issues, we\npresent Spatial-Aware Graph Completion Network (SAGCNet) to overcome the\ndependency on complete volumetric data, featuring two main innovations: (1) a\nvolumetric slice graph completion module that incorporates the inter-slice\nrelationships into a graph structure, and (2) a volumetric spatial adapter\ncomponent that enables our model to effectively capture and utilize various\nforms of 3D spatial context. Extensive experiments on cardiac MRI datasets\ndemonstrate that SAGCNet is capable of synthesizing absent CMR slices,\noutperforming competitive state-of-the-art MRI synthesis methods both\nquantitatively and qualitatively. Notably, our model maintains superior\nperformance even with limited slice data.", "AI": {"tldr": "本文提出SAGCNet，一个用于磁共振成像（MRI）缺失切片合成的新模型，通过图结构和空间适配器有效处理三维数据中的切片间关系和空间信息。", "motivation": "临床实践中MRI图像常因缺失或不可用切片而影响诊断准确性。现有的体素MRI合成方法难以有效建模三维数据中切片间的局部关联性、依赖性，且对关键的三维空间信息和全局上下文利用不足。", "method": "本研究提出了空间感知图补全网络（SAGCNet），包含两大创新：1) 一个体素切片图补全模块，将切片间关系整合到图结构中；2) 一个体素空间适配器组件，使模型能有效捕获和利用各种形式的三维空间上下文。", "result": "在心脏MRI数据集上的大量实验表明，SAGCNet能够合成缺失的CMR切片，在定量和定性方面均优于现有最先进的MRI合成方法。值得注意的是，即使在切片数据有限的情况下，该模型仍保持卓越性能。", "conclusion": "SAGCNet通过整合切片间关系和三维空间上下文，有效解决了MRI缺失切片合成的挑战，尤其在心脏MRI数据上表现出色，即使在数据量有限时也能保持高性能。"}}
{"id": "2508.07146", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07146", "abs": "https://arxiv.org/abs/2508.07146", "authors": ["Yu Liu", "Zhijie Liu", "Xiao Ren", "You-Fu Li", "He Kong"], "title": "Intention-Aware Diffusion Model for Pedestrian Trajectory Prediction", "comment": null, "summary": "Predicting pedestrian motion trajectories is critical for the path planning\nand motion control of autonomous vehicles. Recent diffusion-based models have\nshown promising results in capturing the inherent stochasticity of pedestrian\nbehavior for trajectory prediction. However, the absence of explicit semantic\nmodelling of pedestrian intent in many diffusion-based methods may result in\nmisinterpreted behaviors and reduced prediction accuracy. To address the above\nchallenges, we propose a diffusion-based pedestrian trajectory prediction\nframework that incorporates both short-term and long-term motion intentions.\nShort-term intent is modelled using a residual polar representation, which\ndecouples direction and magnitude to capture fine-grained local motion\npatterns. Long-term intent is estimated through a learnable, token-based\nendpoint predictor that generates multiple candidate goals with associated\nprobabilities, enabling multimodal and context-aware intention modelling.\nFurthermore, we enhance the diffusion process by incorporating adaptive\nguidance and a residual noise predictor that dynamically refines denoising\naccuracy. The proposed framework is evaluated on the widely used ETH, UCY, and\nSDD benchmarks, demonstrating competitive results against state-of-the-art\nmethods.", "AI": {"tldr": "该论文提出一个基于扩散模型的行人轨迹预测框架，通过引入短期（残差极坐标）和长期（可学习的基于token的终点预测器）运动意图建模，并结合自适应引导和残差噪声预测器，提高了预测精度和对行为随机性的捕捉能力。", "motivation": "现有基于扩散模型的行人轨迹预测方法缺乏对行人意图的显式语义建模，导致行为误判和预测准确性降低。", "method": "该框架包含：1) 使用残差极坐标表示建模短期意图，解耦方向和大小以捕捉细粒度局部运动模式；2) 通过可学习的、基于token的终点预测器估计长期意图，生成多模态和上下文感知的目标候选及其概率；3) 增强扩散过程，引入自适应引导和残差噪声预测器，动态优化去噪精度。", "result": "在ETH、UCY和SDD基准测试中，该框架与现有最先进的方法相比，展现出有竞争力的结果。", "conclusion": "通过整合短期和长期运动意图的显式建模，并优化扩散过程，所提出的框架有效解决了现有扩散模型在行人轨迹预测中的挑战，提高了预测准确性和对复杂行为的理解。"}}
{"id": "2508.07902", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.07902", "abs": "https://arxiv.org/abs/2508.07902", "authors": ["Chen Cecilia Liu", "Hiba Arnaout", "Nils Kovačić", "Dana Atzil-Slonim", "Iryna Gurevych"], "title": "Tailored Emotional LLM-Supporter: Enhancing Cultural Sensitivity", "comment": "Under review; joint first authors", "summary": "Large language models (LLMs) show promise in offering emotional support and\ngenerating empathetic responses for individuals in distress, but their ability\nto deliver culturally sensitive support remains underexplored due to lack of\nresources. In this work, we introduce CultureCare, the first dataset designed\nfor this task, spanning four cultures and including 1729 distress messages,\n1523 cultural signals, and 1041 support strategies with fine-grained emotional\nand cultural annotations. Leveraging CultureCare, we (i) develop and test four\nadaptation strategies for guiding three state-of-the-art LLMs toward culturally\nsensitive responses; (ii) conduct comprehensive evaluations using LLM judges,\nin-culture human annotators, and clinical psychologists; (iii) show that\nadapted LLMs outperform anonymous online peer responses, and that simple\ncultural role-play is insufficient for cultural sensitivity; and (iv) explore\nthe application of LLMs in clinical training, where experts highlight their\npotential in fostering cultural competence in future therapists.", "AI": {"tldr": "该研究引入了首个文化敏感情感支持数据集CultureCare，并开发了引导大型语言模型（LLMs）提供文化敏感支持的策略，通过多方评估证明了其有效性及在临床培训中的潜力。", "motivation": "大型语言模型在情感支持方面表现出潜力，但由于缺乏资源，其提供文化敏感支持的能力尚未得到充分探索。", "method": "1. 构建了首个文化敏感情感支持数据集CultureCare，涵盖四种文化，包含1729条求助信息、1523个文化信号和1041个支持策略，并进行了细粒度的情感和文化标注。2. 开发并测试了四种LLM适应策略，以引导三种最先进的LLM生成文化敏感响应。3. 通过LLM评判、文化内人类标注者和临床心理学家进行全面评估。", "result": "1. 经过适应的LLM表现优于匿名的在线同伴响应。2. 简单的文化角色扮演不足以实现文化敏感性。3. 专家指出LLM在临床培训中具有培养未来治疗师文化能力的潜力。", "conclusion": "该研究证实了LLM在提供文化敏感情感支持方面的潜力，并揭示了其在未来治疗师文化能力培养中的应用前景，强调了数据集和适应策略的重要性。"}}
{"id": "2508.07083", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07083", "abs": "https://arxiv.org/abs/2508.07083", "authors": ["Yueyu Hu", "Ran Gong", "Tingyu Fan", "Yao Wang"], "title": "TeSO: Representing and Compressing 3D Point Cloud Scenes with Textured Surfel Octree", "comment": null, "summary": "3D visual content streaming is a key technology for emerging 3D telepresence\nand AR/VR applications. One fundamental element underlying the technology is a\nversatile 3D representation that is capable of producing high-quality renders\nand can be efficiently compressed at the same time. Existing 3D representations\nlike point clouds, meshes and 3D Gaussians each have limitations in terms of\nrendering quality, surface definition, and compressibility. In this paper, we\npresent the Textured Surfel Octree (TeSO), a novel 3D representation that is\nbuilt from point clouds but addresses the aforementioned limitations. It\nrepresents a 3D scene as cube-bounded surfels organized on an octree, where\neach surfel is further associated with a texture patch. By approximating a\nsmooth surface with a large surfel at a coarser level of the octree, it reduces\nthe number of primitives required to represent the 3D scene, and yet retains\nthe high-frequency texture details through the texture map attached to each\nsurfel. We further propose a compression scheme to encode the geometry and\ntexture efficiently, leveraging the octree structure. The proposed textured\nsurfel octree combined with the compression scheme achieves higher rendering\nquality at lower bit-rates compared to multiple point cloud and 3D\nGaussian-based baselines.", "AI": {"tldr": "本文提出了一种名为TeSO（Textured Surfel Octree）的新型3D表示方法，它结合了八叉树结构和纹理补丁，以实现高质量渲染和高效压缩，优于现有方法。", "motivation": "现有的3D表示（如点云、网格和3D高斯）在渲染质量、表面定义和可压缩性方面存在局限性，而3D视觉内容流是3D临场感和AR/VR应用的关键技术，需要一种能够高质量渲染并高效压缩的通用3D表示。", "method": "TeSO从点云构建，将3D场景表示为组织在八叉树上的立方体边界冲浪元（surfel），每个冲浪元关联一个纹理补丁。通过在八叉树的粗糙级别使用大冲浪元近似平滑表面，减少了表示场景所需的基元数量，同时通过纹理图保留了高频纹理细节。此外，提出了一种利用八叉树结构高效编码几何和纹理的压缩方案。", "result": "与多个基于点云和3D高斯的基线相比，所提出的纹理冲浪元八叉树结合压缩方案在更低的比特率下实现了更高的渲染质量。", "conclusion": "TeSO是一种有效解决现有3D表示局限性的新型3D表示方法，它在渲染质量和压缩效率方面表现出色，特别适用于3D视觉内容流应用。"}}
{"id": "2508.07165", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07165", "abs": "https://arxiv.org/abs/2508.07165", "authors": ["Zelin Qiu", "Xi Wang", "Zhuoyao Xie", "Juan Zhou", "Yu Wang", "Lingjie Yang", "Xinrui Jiang", "Juyoung Bae", "Moo Hyun Son", "Qiang Ye", "Dexuan Chen", "Rui Zhang", "Tao Li", "Neeraj Ramesh Mahboobani", "Varut Vardhanabhuti", "Xiaohui Duan", "Yinghua Zhao", "Hao Chen"], "title": "Large-scale Multi-sequence Pretraining for Generalizable MRI Analysis in Versatile Clinical Applications", "comment": null, "summary": "Multi-sequence Magnetic Resonance Imaging (MRI) offers remarkable\nversatility, enabling the distinct visualization of different tissue types.\nNevertheless, the inherent heterogeneity among MRI sequences poses significant\nchallenges to the generalization capability of deep learning models. These\nchallenges undermine model performance when faced with varying acquisition\nparameters, thereby severely restricting their clinical utility. In this study,\nwe present PRISM, a foundation model PRe-trained with large-scale\nmultI-Sequence MRI. We collected a total of 64 datasets from both public and\nprivate sources, encompassing a wide range of whole-body anatomical structures,\nwith scans spanning diverse MRI sequences. Among them, 336,476 volumetric MRI\nscans from 34 datasets (8 public and 26 private) were curated to construct the\nlargest multi-organ multi-sequence MRI pretraining corpus to date. We propose a\nnovel pretraining paradigm that disentangles anatomically invariant features\nfrom sequence-specific variations in MRI, while preserving high-level semantic\nrepresentations. We established a benchmark comprising 44 downstream tasks,\nincluding disease diagnosis, image segmentation, registration, progression\nprediction, and report generation. These tasks were evaluated on 32 public\ndatasets and 5 private cohorts. PRISM consistently outperformed both\nnon-pretrained models and existing foundation models, achieving first-rank\nresults in 39 out of 44 downstream benchmarks with statistical significance\nimprovements. These results underscore its ability to learn robust and\ngeneralizable representations across unseen data acquired under diverse MRI\nprotocols. PRISM provides a scalable framework for multi-sequence MRI analysis,\nthereby enhancing the translational potential of AI in radiology. It delivers\nconsistent performance across diverse imaging protocols, reinforcing its\nclinical applicability.", "AI": {"tldr": "PRISM是一个基于大规模多序列MRI预训练的通用基础模型，旨在解决MRI序列异质性导致的深度学习模型泛化性差的问题，并在多项下游任务中表现优异。", "motivation": "多序列MRI图像在不同采集参数下存在固有的异质性，这严重阻碍了深度学习模型的泛化能力和临床实用性。", "method": "收集了来自公共和私人来源的64个数据集，构建了包含336,476个体素MRI扫描的最大多器官多序列MRI预训练语料库。提出了一种新颖的预训练范式，旨在分离解剖学不变特征和序列特异性变异，同时保留高级语义表示。在44个下游任务（包括疾病诊断、图像分割、配准、进展预测和报告生成）上进行了基准测试。", "result": "PRISM在44个下游基准测试中，有39个获得了第一名，显著优于未预训练模型和现有基础模型。结果表明其能够学习在不同MRI协议下获取的未见数据中具有鲁棒性和泛化性的表示。", "conclusion": "PRISM为多序列MRI分析提供了一个可扩展的框架，增强了AI在放射学中的转化潜力，并通过在不同成像协议下提供一致的性能，强化了其临床适用性。"}}
{"id": "2508.07937", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.07937", "abs": "https://arxiv.org/abs/2508.07937", "authors": ["John C. McDonald", "Rosalee Wolfe", "Fabrizio Nunnari"], "title": "Challenges and opportunities in portraying emotion in generated sign language", "comment": null, "summary": "Non-manual signals in sign languages continue to be a challenge for signing\navatars. More specifically, emotional content has been difficult to incorporate\nbecause of a lack of a standard method of specifying the avatar's emotional\nstate. This paper explores the application of an intuitive two-parameter\nrepresentation for emotive non-manual signals to the Paula signing avatar that\nshows promise for facilitating the linguistic specification of emotional facial\nexpressions in a more coherent manner than previous methods. Users can apply\nthese parameters to control Paula's emotional expressions through a textual\nrepresentation called the EASIER notation. The representation can allow avatars\nto express more nuanced emotional states using two numerical parameters. It\nalso has the potential to enable more consistent specification of emotional\nnon-manual signals in linguistic annotations which drive signing avatars.", "AI": {"tldr": "该论文提出了一种新的双参数表示法，以更有效地在手语虚拟形象中表达情感非手动信号。", "motivation": "手语虚拟形象在整合情感内容方面面临挑战，主要原因在于缺乏一种标准方法来指定虚拟形象的情感状态。", "method": "研究人员探索将一种直观的双参数表示法应用于Paula手语虚拟形象，以表示情感非手动信号。用户可以通过名为EASIER的文本表示法利用这些参数控制虚拟形象的情感表达。", "result": "该双参数表示法有望促进情感面部表情的语言规范，使其比以往方法更连贯。它能使虚拟形象表达更细致入微的情感状态，并有望使驱动手语虚拟形象的语言标注中情感非手动信号的规范更加一致。", "conclusion": "所提出的双参数方法为手语虚拟形象的情感非手动信号提供了一种有前景的、更连贯的规范方式，有助于解决情感表达的挑战。"}}
{"id": "2508.07092", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07092", "abs": "https://arxiv.org/abs/2508.07092", "authors": ["Yue Hu", "Juntong Peng", "Yunqiao Yang", "Siheng Chen"], "title": "Communication-Efficient Multi-Agent 3D Detection via Hybrid Collaboration", "comment": null, "summary": "Collaborative 3D detection can substantially boost detection performance by\nallowing agents to exchange complementary information. It inherently results in\na fundamental trade-off between detection performance and communication\nbandwidth. To tackle this bottleneck issue, we propose a novel hybrid\ncollaboration that adaptively integrates two types of communication messages:\nperceptual outputs, which are compact, and raw observations, which offer richer\ninformation. This approach focuses on two key aspects: i) integrating\ncomplementary information from two message types and ii) prioritizing the most\ncritical data within each type. By adaptively selecting the most critical set\nof messages, it ensures optimal perceptual information and adaptability,\neffectively meeting the demands of diverse communication scenarios.Building on\nthis hybrid collaboration, we present \\texttt{HyComm}, a\ncommunication-efficient LiDAR-based collaborative 3D detection system.\n\\texttt{HyComm} boasts two main benefits: i) it facilitates adaptable\ncompression rates for messages, addressing various communication requirements,\nand ii) it uses standardized data formats for messages. This ensures they are\nindependent of specific detection models, fostering adaptability across\ndifferent agent configurations. To evaluate HyComm, we conduct experiments on\nboth real-world and simulation datasets: DAIR-V2X and OPV2V. HyComm\nconsistently outperforms previous methods and achieves a superior\nperformance-bandwidth trade-off regardless of whether agents use the same or\nvaried detection models. It achieves a lower communication volume of more than\n2,006$\\times$ and still outperforms Where2comm on DAIR-V2X in terms of AP50.\nThe related code will be released.", "AI": {"tldr": "本文提出了一种名为HyComm的混合协作3D检测系统，通过自适应地结合紧凑的感知输出和丰富的原始观测数据，解决了协作3D检测中性能与通信带宽之间的权衡问题，实现了卓越的性能和通信效率。", "motivation": "协作3D检测能显著提升性能，但其固有的性能与通信带宽之间的权衡是一个瓶颈问题，需要找到一种有效的方法来平衡二者。", "method": "提出了一种新颖的混合协作方法，自适应地整合两种通信消息：紧凑的感知输出和信息更丰富的原始观测。该方法关注两点：i) 整合两种消息类型的互补信息；ii) 优先处理每种类型中最关键的数据。基于此，开发了HyComm系统，其特点是：i) 消息压缩率可适应不同通信需求；ii) 使用标准化数据格式，与特定检测模型无关，增强了适应性。", "result": "在真实世界（DAIR-V2X）和仿真（OPV2V）数据集上进行了实验。HyComm持续优于现有方法，无论代理使用相同或不同的检测模型，都能实现卓越的性能-带宽权衡。在DAIR-V2X数据集上，其通信量比Where2comm降低了2,006倍以上，但在AP50方面仍表现更优。", "conclusion": "HyComm通过自适应混合协作，成功解决了协作3D检测中的性能-带宽瓶颈，在不同通信场景和代理配置下，展现出卓越的感知信息优化能力和适应性，显著提升了系统性能和通信效率。"}}
{"id": "2508.07170", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07170", "abs": "https://arxiv.org/abs/2508.07170", "authors": ["Yunpeng Shi", "Lei Chen", "Xiaolu Shen", "Yanju Guo"], "title": "Lightweight Multi-Scale Feature Extraction with Fully Connected LMF Layer for Salient Object Detection", "comment": null, "summary": "In the domain of computer vision, multi-scale feature extraction is vital for\ntasks such as salient object detection. However, achieving this capability in\nlightweight networks remains challenging due to the trade-off between\nefficiency and performance. This paper proposes a novel lightweight multi-scale\nfeature extraction layer, termed the LMF layer, which employs depthwise\nseparable dilated convolutions in a fully connected structure. By integrating\nmultiple LMF layers, we develop LMFNet, a lightweight network tailored for\nsalient object detection. Our approach significantly reduces the number of\nparameters while maintaining competitive performance. Here, we show that LMFNet\nachieves state-of-the-art or comparable results on five benchmark datasets with\nonly 0.81M parameters, outperforming several traditional and lightweight models\nin terms of both efficiency and accuracy. Our work not only addresses the\nchallenge of multi-scale learning in lightweight networks but also demonstrates\nthe potential for broader applications in image processing tasks. The related\ncode files are available at https://github.com/Shi-Yun-peng/LMFNet", "AI": {"tldr": "本文提出了一种轻量级多尺度特征提取层（LMF层）和网络（LMFNet），用于显著目标检测，在显著降低参数量的同时保持了竞争性性能。", "motivation": "在计算机视觉领域，多尺度特征提取对于显著目标检测等任务至关重要，但在轻量级网络中实现这一功能仍具挑战，因为效率和性能之间存在权衡。", "method": "提出了一种新颖的轻量级多尺度特征提取层（LMF层），该层采用深度可分离空洞卷积并以全连接结构排列。通过集成多个LMF层，构建了轻量级网络LMFNet，专为显著目标检测设计。", "result": "LMFNet仅用0.81M参数，在五个基准数据集上取得了最先进或可媲美的结果，在效率和准确性方面均优于多个传统和轻量级模型。", "conclusion": "该工作不仅解决了轻量级网络中多尺度学习的挑战，还展示了其在图像处理任务中更广泛的应用潜力。"}}
{"id": "2508.07955", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.07955", "abs": "https://arxiv.org/abs/2508.07955", "authors": ["Furkan Şahinuç", "Subhabrata Dutta", "Iryna Gurevych"], "title": "Expert Preference-based Evaluation of Automated Related Work Generation", "comment": "Project page: https://ukplab.github.io/arxiv2025-expert-eval-rw/", "summary": "Expert domain writing, such as scientific writing, typically demands\nextensive domain knowledge. Recent advances in LLMs show promising potential in\nreducing the expert workload. However, evaluating the quality of automatically\ngenerated scientific writing is a crucial open issue, as it requires knowledge\nof domain-specific evaluation criteria and the ability to discern expert\npreferences. Conventional automatic metrics and LLM-as-a-judge systems are\ninsufficient to grasp expert preferences and domain-specific quality standards.\nTo address this gap and support human-AI collaborative writing, we focus on\nrelated work generation, one of the most challenging scientific tasks, as an\nexemplar. We propose GREP, a multi-turn evaluation framework that integrates\nclassical related work evaluation criteria with expert-specific preferences.\nInstead of assigning a single score, our framework decomposes the evaluation\ninto fine-grained dimensions. This localized evaluation approach is further\naugmented with contrastive few-shot examples to provide detailed contextual\nguidance for the evaluation dimensions. The design principles allow our\nframework to deliver cardinal assessment of quality, which can facilitate\nbetter post-training compared to ordinal preference data. For better\naccessibility, we design two variants of GREP: a more precise variant with\nproprietary LLMs as evaluators, and a cheaper alternative with open-weight\nLLMs. Empirical investigation reveals that our framework is able to assess the\nquality of related work sections in a much more robust manner compared to\nstandard LLM judges, reflects natural scenarios of scientific writing, and\nbears a strong correlation with the human expert assessment. We also observe\nthat generations from state-of-the-art LLMs struggle to satisfy validation\nconstraints of a suitable related work section. They (mostly) fail to improve\nbased on feedback as well.", "AI": {"tldr": "该研究提出了GREP，一个多轮评估框架，用于更准确地评估自动生成的科学写作（特别是相关工作部分），解决了传统评估方法无法捕捉专家偏好和领域特定质量标准的问题。", "motivation": "现有的大语言模型（LLMs）在辅助专家领域写作方面显示出潜力，但评估自动生成的科学写作质量是一个关键的开放问题。传统的自动度量和“LLM作为评判者”系统不足以理解专家偏好和领域特定质量标准，尤其是在科学写作这种需要深厚领域知识的场景中。", "method": "研究提出了GREP，一个多轮评估框架。该框架将评估分解为细粒度维度，融合了经典的相关工作评估标准和专家特定偏好。它通过对比式少样本示例提供详细的上下文指导，并旨在提供基数质量评估。为提高可访问性，GREP设计了两个变体：一个使用专有LLM进行更精确评估，另一个使用开源LLM作为更经济的替代方案。", "result": "实证研究表明，GREP框架比标准LLM评判者能更鲁棒地评估相关工作部分的质量，反映了科学写作的自然场景，并与人类专家评估具有很强的相关性。此外，研究还发现，当前最先进的LLM生成的内容难以满足合适的相关工作部分的验证约束，并且在接收反馈后（大多）未能有效改进。", "conclusion": "GREP框架提供了一种更有效、更符合专家偏好的科学写作质量评估方法，尤其是在相关工作生成方面。研究结果也揭示了当前最先进的LLMs在生成满足领域特定约束的科学文本以及根据反馈进行改进方面的局限性。"}}
{"id": "2508.07112", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07112", "abs": "https://arxiv.org/abs/2508.07112", "authors": ["Nikolai Warner", "Wenjin Zhang", "Irfan Essa", "Apaar Sadhwani"], "title": "AugLift: Boosting Generalization in Lifting-based 3D Human Pose Estimation", "comment": "Preprint. Under review", "summary": "Lifting-based methods for 3D Human Pose Estimation (HPE), which predict 3D\nposes from detected 2D keypoints, often generalize poorly to new datasets and\nreal-world settings. To address this, we propose \\emph{AugLift}, a simple yet\neffective reformulation of the standard lifting pipeline that significantly\nimproves generalization performance without requiring additional data\ncollection or sensors. AugLift sparsely enriches the standard input -- the 2D\nkeypoint coordinates $(x, y)$ -- by augmenting it with a keypoint detection\nconfidence score $c$ and a corresponding depth estimate $d$. These additional\nsignals are computed from the image using off-the-shelf, pre-trained models\n(e.g., for monocular depth estimation), thereby inheriting their strong\ngeneralization capabilities. Importantly, AugLift serves as a modular add-on\nand can be readily integrated into existing lifting architectures.\n  Our extensive experiments across four datasets demonstrate that AugLift\nboosts cross-dataset performance on unseen datasets by an average of $10.1\\%$,\nwhile also improving in-distribution performance by $4.0\\%$. These gains are\nconsistent across various lifting architectures, highlighting the robustness of\nour method. Our analysis suggests that these sparse, keypoint-aligned cues\nprovide robust frame-level context, offering a practical way to significantly\nimprove the generalization of any lifting-based pose estimation model. Code\nwill be made publicly available.", "AI": {"tldr": "针对3D人体姿态估计中基于提升（Lifting-based）的方法泛化性差的问题，本文提出了AugLift。它通过将2D关键点坐标与置信度分数和深度估计进行增强，显著提升了模型在未见数据集上的泛化性能，且无需额外数据或传感器。", "motivation": "基于提升的3D人体姿态估计（HPE）方法，即从检测到的2D关键点预测3D姿态，在面对新数据集和真实世界场景时，泛化能力往往较差。", "method": "提出了AugLift，一种对标准提升管道的简单而有效的重新表述。它通过使用现成的、预训练的模型（例如，单目深度估计），从图像中计算出关键点检测置信度得分(c)和相应的深度估计(d)，从而稀疏地丰富了标准的输入（2D关键点坐标x, y）。AugLift是一个模块化的附加组件，可以轻松集成到现有的提升架构中。", "result": "在四个数据集上的广泛实验表明，AugLift使模型在未见数据集上的跨数据集性能平均提高了10.1%，同时将分布内性能提高了4.0%。这些提升在各种提升架构中都保持一致，突显了该方法的鲁棒性。", "conclusion": "稀疏的、与关键点对齐的线索（置信度和深度）提供了鲁棒的帧级上下文，为显著提高任何基于提升的姿态估计模型的泛化性提供了一种实用方法。"}}
{"id": "2508.07281", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07281", "abs": "https://arxiv.org/abs/2508.07281", "authors": ["Hongbo Zhu", "Angelo Cangelosi"], "title": "Representation Understanding via Activation Maximization", "comment": "7 pages,12 figures", "summary": "Understanding internal feature representations of deep neural networks (DNNs)\nis a fundamental step toward model interpretability. Inspired by neuroscience\nmethods that probe biological neurons using visual stimuli, recent deep\nlearning studies have employed Activation Maximization (AM) to synthesize\ninputs that elicit strong responses from artificial neurons. In this work, we\npropose a unified feature visualization framework applicable to both\nConvolutional Neural Networks (CNNs) and Vision Transformers (ViTs). Unlike\nprior efforts that predominantly focus on the last output-layer neurons in\nCNNs, we extend feature visualization to intermediate layers as well, offering\ndeeper insights into the hierarchical structure of learned feature\nrepresentations. Furthermore, we investigate how activation maximization can be\nleveraged to generate adversarial examples, revealing potential vulnerabilities\nand decision boundaries of DNNs. Our experiments demonstrate the effectiveness\nof our approach in both traditional CNNs and modern ViT, highlighting its\ngeneralizability and interpretive value.", "AI": {"tldr": "本文提出一个统一的特征可视化框架，适用于CNN和ViT，并将其扩展到中间层，以深入理解DNN的内部表示。此外，还利用激活最大化生成对抗样本，揭示模型漏洞。", "motivation": "理解深度神经网络（DNNs）的内部特征表示是实现模型可解释性的基本步骤。受神经科学中通过视觉刺激探测生物神经元的启发，研究人员希望通过类似方法理解人工神经元。", "method": "本文提出了一个统一的特征可视化框架，适用于卷积神经网络（CNNs）和视觉转换器（ViTs）。该框架将特征可视化扩展到中间层，而不仅仅是输出层。同时，利用激活最大化（AM）技术来合成输入以生成对抗样本。", "result": "实验证明，该方法在传统CNN和现代ViT中均有效，显示出其泛化能力和解释价值。通过生成对抗样本，该方法也揭示了DNN的潜在漏洞和决策边界。", "conclusion": "本文提出的统一特征可视化框架能够有效解释CNN和ViT的内部特征表示，并能通过激活最大化揭示模型的脆弱性，为DNN的理解和安全分析提供了有价值的工具。"}}
{"id": "2508.07959", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.07959", "abs": "https://arxiv.org/abs/2508.07959", "authors": ["Changhao Song", "Yazhou Zhang", "Hui Gao", "Ben Yao", "Peng Zhang"], "title": "Large Language Models for Subjective Language Understanding: A Survey", "comment": null, "summary": "Subjective language understanding refers to a broad set of natural language\nprocessing tasks where the goal is to interpret or generate content that\nconveys personal feelings, opinions, or figurative meanings rather than\nobjective facts. With the advent of large language models (LLMs) such as\nChatGPT, LLaMA, and others, there has been a paradigm shift in how we approach\nthese inherently nuanced tasks. In this survey, we provide a comprehensive\nreview of recent advances in applying LLMs to subjective language tasks,\nincluding sentiment analysis, emotion recognition, sarcasm detection, humor\nunderstanding, stance detection, metaphor interpretation, intent detection, and\naesthetics assessment. We begin by clarifying the definition of subjective\nlanguage from linguistic and cognitive perspectives, and we outline the unique\nchallenges posed by subjective language (e.g. ambiguity, figurativeness,\ncontext dependence). We then survey the evolution of LLM architectures and\ntechniques that particularly benefit subjectivity tasks, highlighting why LLMs\nare well-suited to model subtle human-like judgments. For each of the eight\ntasks, we summarize task definitions, key datasets, state-of-the-art LLM-based\nmethods, and remaining challenges. We provide comparative insights, discussing\ncommonalities and differences among tasks and how multi-task LLM approaches\nmight yield unified models of subjectivity. Finally, we identify open issues\nsuch as data limitations, model bias, and ethical considerations, and suggest\nfuture research directions. We hope this survey will serve as a valuable\nresource for researchers and practitioners interested in the intersection of\naffective computing, figurative language processing, and large-scale language\nmodels.", "AI": {"tldr": "该综述全面回顾了大型语言模型（LLMs）在主观语言理解任务（如情感分析、讽刺检测等）中的最新进展、方法、挑战和未来方向。", "motivation": "随着ChatGPT等LLMs的出现，主观语言理解任务的处理方式发生了范式转变。这些任务涉及个人感受、观点或比喻意义的解释，具有固有的细微差别，LLMs在建模这些细微的人类判断方面具有优势。", "method": "该综述首先从语言学和认知角度澄清了主观语言的定义，并概述了其独特挑战。然后，它调查了特别有益于主观性任务的LLM架构和技术演变。针对八个具体任务（情感分析、情感识别、讽刺检测、幽默理解、立场检测、隐喻解释、意图检测、美学评估），综述总结了任务定义、关键数据集、最先进的LLM方法和剩余挑战。最后，讨论了任务间的共性与差异，并提出了开放性问题和未来研究方向。", "result": "LLMs非常适合建模主观语言任务中细微的人类判断。综述提供了对八个特定主观语言任务的全面洞察，包括它们的定义、数据集、当前SOTA方法和面临的挑战。它还讨论了多任务LLM方法如何可能产生统一的主观性模型，并指出了数据限制、模型偏见和伦理考虑等开放性问题。", "conclusion": "该综述旨在为对情感计算、比喻语言处理和大型语言模型交叉领域感兴趣的研究人员和从业者提供宝贵资源。未来的研究应关注解决数据限制、模型偏见和伦理问题，并探索更统一的主观性模型。"}}
{"id": "2508.07140", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07140", "abs": "https://arxiv.org/abs/2508.07140", "authors": ["Yingtie Lei", "Fanghai Yi", "Yihang Dong", "Weihuang Liu", "Xiaofeng Zhang", "Zimeng Li", "Chi-Man Pun", "Xuhang Chen"], "title": "CMAMRNet: A Contextual Mask-Aware Network Enhancing Mural Restoration Through Comprehensive Mask Guidance", "comment": "Accepted by BMVC 2025", "summary": "Murals, as invaluable cultural artifacts, face continuous deterioration from\nenvironmental factors and human activities. Digital restoration of murals faces\nunique challenges due to their complex degradation patterns and the critical\nneed to preserve artistic authenticity. Existing learning-based methods\nstruggle with maintaining consistent mask guidance throughout their networks,\nleading to insufficient focus on damaged regions and compromised restoration\nquality. We propose CMAMRNet, a Contextual Mask-Aware Mural Restoration Network\nthat addresses these limitations through comprehensive mask guidance and\nmulti-scale feature extraction. Our framework introduces two key components:\n(1) the Mask-Aware Up/Down-Sampler (MAUDS), which ensures consistent mask\nsensitivity across resolution scales through dedicated channel-wise feature\nselection and mask-guided feature fusion; and (2) the Co-Feature Aggregator\n(CFA), operating at both the highest and lowest resolutions to extract\ncomplementary features for capturing fine textures and global structures in\ndegraded regions. Experimental results on benchmark datasets demonstrate that\nCMAMRNet outperforms state-of-the-art methods, effectively preserving both\nstructural integrity and artistic details in restored murals. The code is\navailable\nat~\\href{https://github.com/CXH-Research/CMAMRNet}{https://github.com/CXH-Research/CMAMRNet}.", "AI": {"tldr": "CMAMRNet是一种用于壁画数字修复的网络，通过引入上下文掩模感知和多尺度特征提取，解决了现有方法在掩模引导一致性和受损区域关注度不足的问题，显著提升了修复质量。", "motivation": "壁画因环境和人类活动持续退化，数字修复面临复杂退化模式和艺术真实性保存的挑战。现有基于学习的方法难以在网络中保持一致的掩模引导，导致对受损区域关注不足，影响修复质量。", "method": "本文提出了CMAMRNet（Contextual Mask-Aware Mural Restoration Network），包含两个关键组件：1. 掩模感知升/降采样器（MAUDS）：通过专门的通道特征选择和掩模引导特征融合，确保跨分辨率尺度的掩模敏感性一致；2. 协同特征聚合器（CFA）：在最高和最低分辨率下运行，提取互补特征，以捕捉受损区域的精细纹理和全局结构。", "result": "在基准数据集上的实验结果表明，CMAMRNet优于现有最先进的方法，有效保留了修复壁画的结构完整性和艺术细节。", "conclusion": "CMAMRNet通过全面的掩模引导和多尺度特征提取，成功解决了壁画数字修复中的现有局限性，实现了结构完整性和艺术细节的有效保存。"}}
{"id": "2508.07306", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07306", "abs": "https://arxiv.org/abs/2508.07306", "authors": ["Md Zahurul Haquea", "Yeahyea Sarker", "Muhammed Farhan Sadique Mahi", "Syed Jubayer Jaman", "Md Robiul Islam"], "title": "DragonFruitQualityNet: A Lightweight Convolutional Neural Network for Real-Time Dragon Fruit Quality Inspection on Mobile Devices", "comment": null, "summary": "Dragon fruit, renowned for its nutritional benefits and economic value, has\nexperienced rising global demand due to its affordability and local\navailability. As dragon fruit cultivation expands, efficient pre- and\npost-harvest quality inspection has become essential for improving agricultural\nproductivity and minimizing post-harvest losses. This study presents\nDragonFruitQualityNet, a lightweight Convolutional Neural Network (CNN)\noptimized for real-time quality assessment of dragon fruits on mobile devices.\nWe curated a diverse dataset of 13,789 images, integrating self-collected\nsamples with public datasets (dataset from Mendeley Data), and classified them\ninto four categories: fresh, immature, mature, and defective fruits to ensure\nrobust model training. The proposed model achieves an impressive 93.98%\naccuracy, outperforming existing methods in fruit quality classification. To\nfacilitate practical adoption, we embedded the model into an intuitive mobile\napplication, enabling farmers and agricultural stakeholders to conduct\non-device, real-time quality inspections. This research provides an accurate,\nefficient, and scalable AI-driven solution for dragon fruit quality control,\nsupporting digital agriculture and empowering smallholder farmers with\naccessible technology. By bridging the gap between research and real-world\napplication, our work advances post-harvest management and promotes sustainable\nfarming practices.", "AI": {"tldr": "本研究提出了DragonFruitQualityNet，一个轻量级CNN模型，用于在移动设备上实时评估火龙果质量，准确率达到93.98%，并开发了相应的移动应用。", "motivation": "火龙果全球需求增长，但其种植扩张使得采前和采后质量检测效率低下，导致农业生产力受限和采后损失。因此，需要一种高效的质量评估解决方案。", "method": "研究构建了一个包含13,789张图像的火龙果数据集（结合自收集和公开数据），并将其分为新鲜、未成熟、成熟和有缺陷四类。在此基础上，提出了DragonFruitQualityNet，一个轻量级卷积神经网络（CNN），并将其嵌入到移动应用程序中，以实现设备上的实时质量检测。", "result": "所提出的DragonFruitQualityNet模型在火龙果质量分类方面达到了93.98%的准确率，优于现有方法。", "conclusion": "本研究提供了一个准确、高效、可扩展的AI驱动火龙果质量控制解决方案，支持数字农业发展，通过可访问的技术赋能小农户，弥合了研究与实际应用之间的差距，促进了采后管理和可持续农业实践。"}}
{"id": "2508.07964", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.07964", "abs": "https://arxiv.org/abs/2508.07964", "authors": ["Matthias Sperber", "Maureen de Seyssel", "Jiajun Bao", "Matthias Paulik"], "title": "Toward Machine Interpreting: Lessons from Human Interpreting Studies", "comment": null, "summary": "Current speech translation systems, while having achieved impressive\naccuracies, are rather static in their behavior and do not adapt to real-world\nsituations in ways human interpreters do. In order to improve their practical\nusefulness and enable interpreting-like experiences, a precise understanding of\nthe nature of human interpreting is crucial. To this end, we discuss human\ninterpreting literature from the perspective of the machine translation field,\nwhile considering both operational and qualitative aspects. We identify\nimplications for the development of speech translation systems and argue that\nthere is great potential to adopt many human interpreting principles using\nrecent modeling techniques. We hope that our findings provide inspiration for\nclosing the perceived usability gap, and can motivate progress toward true\nmachine interpreting.", "AI": {"tldr": "本文从机器翻译视角探讨人类口译文献，旨在理解人类口译的本质，为开发更像人类的自适应语音翻译系统提供指导。", "motivation": "当前的语音翻译系统虽然准确，但在适应真实世界情境方面缺乏人类口译员的动态性和灵活性，这限制了其实用性。", "method": "通过从机器翻译领域视角，讨论并分析人类口译文献，同时考虑操作性和质量方面。", "result": "识别出对语音翻译系统开发的启示，并认为利用近期建模技术采纳许多人类口译原则具有巨大潜力。", "conclusion": "研究结果有望弥合感知到的可用性差距，并推动实现真正的机器口译。"}}
{"id": "2508.07144", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07144", "abs": "https://arxiv.org/abs/2508.07144", "authors": ["Xuanhan Wang", "Huimin Deng", "Ke Liu", "Jun Wang", "Lianli Gao", "Jingkuan Song"], "title": "Dynamic Pattern Alignment Learning for Pretraining Lightweight Human-Centric Vision Models", "comment": null, "summary": "Human-centric vision models (HVMs) have achieved remarkable generalization\ndue to large-scale pretraining on massive person images. However, their\ndependence on large neural architectures and the restricted accessibility of\npretraining data significantly limits their practicality in real-world\napplications. To address this limitation, we propose Dynamic Pattern Alignment\nLearning (DPAL), a novel distillation-based pretraining framework that\nefficiently trains lightweight HVMs to acquire strong generalization from large\nHVMs. In particular, human-centric visual perception are highly dependent on\nthree typical visual patterns, including global identity pattern, local shape\npattern and multi-person interaction pattern. To achieve generalizable\nlightweight HVMs, we firstly design a dynamic pattern decoder (D-PaDe), acting\nas a dynamic Mixture of Expert (MoE) model. It incorporates three specialized\nexperts dedicated to adaptively extract typical visual patterns, conditioned on\nboth input image and pattern queries. And then, we present three levels of\nalignment objectives, which aims to minimize generalization gap between\nlightweight HVMs and large HVMs at global image level, local pixel level, and\ninstance relation level. With these two deliberate designs, the DPAL\neffectively guides lightweight model to learn all typical human visual patterns\nfrom large HVMs, which can generalize to various human-centric vision tasks.\nExtensive experiments conducted on 15 challenging datasets demonstrate the\neffectiveness of the DPAL. Remarkably, when employing PATH-B as the teacher,\nDPAL-ViT/Ti (5M parameters) achieves surprising generalizability similar to\nexisting large HVMs such as PATH-B (84M) and Sapiens-L (307M), and outperforms\nprevious distillation-based pretraining methods including Proteus-ViT/Ti (5M)\nand TinyMiM-ViT/Ti (5M) by a large margin.", "AI": {"tldr": "本文提出了动态模式对齐学习（DPAL）框架，通过知识蒸馏使轻量级以人为中心的视觉模型（HVMs）获得与大型HVMs相似的强大泛化能力，解决了大型模型实用性受限的问题。", "motivation": "现有的以人为中心的视觉模型（HVMs）虽然通过大规模预训练实现了出色的泛化能力，但其对大型神经网络架构的依赖以及预训练数据获取的限制，严重影响了它们在实际应用中的实用性。", "method": "本文提出了动态模式对齐学习（DPAL）框架：1. 设计了一个动态模式解码器（D-PaDe），它是一个动态的专家混合（MoE）模型，包含三个专门的专家，分别自适应地提取全局身份、局部形状和多人交互模式。2. 提出了三层对齐目标，旨在全局图像、局部像素和实例关系层面最小化轻量级HVMs与大型HVMs之间的泛化差距。", "result": "DPAL框架能有效引导轻量级模型从大型HVMs中学习典型的人类视觉模式，并泛化到各种以人为中心的视觉任务。在15个数据集上的实验表明，当使用PATH-B作为教师模型时，DPAL-ViT/Ti（5M参数）实现了与PATH-B（84M）和Sapiens-L（307M）等现有大型HVMs相似的泛化能力，并大幅优于先前的蒸馏预训练方法（如Proteus-ViT/Ti和TinyMiM-ViT/Ti）。", "conclusion": "DPAL是一个有效的蒸馏预训练框架，能够使轻量级以人为中心的视觉模型获得强大的泛化能力，从而克服了大型模型在实际应用中的限制，显著提升了轻量级HVMs的性能。"}}
{"id": "2508.07307", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07307", "abs": "https://arxiv.org/abs/2508.07307", "authors": ["Haiyang Guo", "Fei Zhu", "Hongbo Zhao", "Fanhu Zeng", "Wenzhuo Liu", "Shijie Ma", "Da-Han Wang", "Xu-Yao Zhang"], "title": "MCITlib: Multimodal Continual Instruction Tuning Library and Benchmark", "comment": "Preprint", "summary": "Continual learning aims to equip AI systems with the ability to continuously\nacquire and adapt to new knowledge without forgetting previously learned\ninformation, similar to human learning. While traditional continual learning\nmethods focusing on unimodal tasks have achieved notable success, the emergence\nof Multimodal Large Language Models has brought increasing attention to\nMultimodal Continual Learning tasks involving multiple modalities, such as\nvision and language. In this setting, models are expected to not only mitigate\ncatastrophic forgetting but also handle the challenges posed by cross-modal\ninteractions and coordination. To facilitate research in this direction, we\nintroduce MCITlib, a comprehensive and constantly evolving code library for\ncontinual instruction tuning of Multimodal Large Language Models. In MCITlib,\nwe have currently implemented 8 representative algorithms for Multimodal\nContinual Instruction Tuning and systematically evaluated them on 2 carefully\nselected benchmarks. MCITlib will be continuously updated to reflect advances\nin the Multimodal Continual Learning field. The codebase is released at\nhttps://github.com/Ghy0501/MCITlib.", "AI": {"tldr": "MCITlib是一个针对多模态大语言模型（MLLMs）的持续指令微调代码库，旨在促进多模态持续学习研究，解决灾难性遗忘和跨模态交互挑战。", "motivation": "传统持续学习主要关注单模态任务，但随着多模态大语言模型（MLLMs）的兴起，多模态持续学习变得日益重要。该领域需要模型不仅能避免灾难性遗忘，还能处理跨模态交互和协调的挑战。", "method": "引入MCITlib，一个全面且持续演进的代码库，用于多模态大语言模型的持续指令微调。目前已实现8种代表性算法，并在2个精心选择的基准上进行了系统评估。", "result": "MCITlib提供了一个涵盖多种算法和基准测试的平台，用于评估多模态持续指令微调方法。它将持续更新以反映该领域的最新进展。", "conclusion": "MCITlib的发布旨在促进多模态持续学习领域的研究，为研究人员提供一个统一、可扩展的工具，以应对多模态持续学习中的挑战。"}}
{"id": "2508.07969", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.07969", "abs": "https://arxiv.org/abs/2508.07969", "authors": ["David Arps", "Hassan Sajjad", "Laura Kallmeyer"], "title": "Understanding Syntactic Generalization in Structure-inducing Language Models", "comment": "Code available at https://github.com/davidarps/silm", "summary": "Structure-inducing Language Models (SiLM) are trained on a self-supervised\nlanguage modeling task, and induce a hierarchical sentence representation as a\nbyproduct when processing an input. A wide variety of SiLMs have been proposed.\nHowever, these have typically been evaluated on a relatively small scale, and\nevaluation of these models has systematic gaps and lacks comparability. In this\nwork, we study three different SiLM architectures using both natural language\n(English) corpora and synthetic bracketing expressions: Structformer (Shen et\nal., 2021), UDGN (Shen et al., 2022) and GPST (Hu et al., 2024). We compare\nthem with respect to (i) properties of the induced syntactic representations\n(ii) performance on grammaticality judgment tasks, and (iii) training dynamics.\nWe find that none of the three architectures dominates across all evaluation\nmetrics. However, there are significant differences, in particular with respect\nto the induced syntactic representations. The Generative Pretrained Structured\nTransformer (GPST; Hu et al. 2024) performs most consistently across evaluation\nsettings, and outperforms the other models on long-distance dependencies in\nbracketing expressions. Furthermore, our study shows that small models trained\non large amounts of synthetic data provide a useful testbed for evaluating\nbasic model properties.", "AI": {"tldr": "本文对三种结构归纳语言模型（SiLM）架构（Structformer、UDGN、GPST）进行了大规模、系统性的评估，比较了它们的句法表示、语法判断性能和训练动态，发现GPST表现最稳定，尤其在长距离依赖上。", "motivation": "现有的结构归纳语言模型（SiLM）评估规模相对较小，存在系统性空白且缺乏可比性，因此需要对不同SiLM架构进行更全面和深入的评估。", "method": "研究使用了自然语言（英语）语料库和合成括号表达式来评估Structformer、UDGN和GPST三种SiLM架构。评估维度包括：1）诱导的句法表示特性；2）在语法判断任务上的表现；3）训练动态。", "result": "研究发现，没有一种架构在所有评估指标上都占据主导地位，但模型之间存在显著差异，尤其是在诱导的句法表示方面。其中，生成式预训练结构化Transformer（GPST）在不同评估设置下表现最一致，并在括号表达式的长距离依赖方面优于其他模型。此外，研究表明，在大量合成数据上训练的小模型为评估基本模型属性提供了有用的测试平台。", "conclusion": "GPST模型在SiLM中表现出较好的综合性能和一致性，尤其擅长处理长距离依赖。同时，使用合成数据训练的小模型是评估SiLM基本特性的有效方法。"}}
{"id": "2508.07149", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07149", "abs": "https://arxiv.org/abs/2508.07149", "authors": ["Ruolin Yang", "Da Li", "Honggang Zhang", "Yi-Zhe Song"], "title": "SketchAnimator: Animate Sketch via Motion Customization of Text-to-Video Diffusion Models", "comment": "2024 IEEE International Conference on Visual Communications and Image\n  Processing (VCIP); Oral", "summary": "Sketching is a uniquely human tool for expressing ideas and creativity. The\nanimation of sketches infuses life into these static drawings, opening a new\ndimension for designers. Animating sketches is a time-consuming process that\ndemands professional skills and extensive experience, often proving daunting\nfor amateurs. In this paper, we propose a novel sketch animation model\nSketchAnimator, which enables adding creative motion to a given sketch, like \"a\njumping car''. Namely, given an input sketch and a reference video, we divide\nthe sketch animation into three stages: Appearance Learning, Motion Learning\nand Video Prior Distillation. In stages 1 and 2, we utilize LoRA to integrate\nsketch appearance information and motion dynamics from the reference video into\nthe pre-trained T2V model. In the third stage, we utilize Score Distillation\nSampling (SDS) to update the parameters of the Bezier curves in each sketch\nframe according to the acquired motion information. Consequently, our model\nproduces a sketch video that not only retains the original appearance of the\nsketch but also mirrors the dynamic movements of the reference video. We\ncompare our method with alternative approaches and demonstrate that it\ngenerates the desired sketch video under the challenge of one-shot motion\ncustomization.", "AI": {"tldr": "本文提出了一种名为SketchAnimator的新型草图动画模型，能够将参考视频的动态运动赋予静态草图，实现创意动画，并解决了单次运动定制的挑战。", "motivation": "草图动画是一个耗时且需要专业技能和丰富经验的过程，对于业余爱好者而言极具挑战性。现有方法难以实现草图的动态、富有创意的动画，因此需要一种更易用的解决方案。", "method": "该模型将草图动画分为三个阶段：外观学习、运动学习和视频先验蒸馏。在前两个阶段，利用LoRA技术将草图外观信息和参考视频的运动动态集成到预训练的T2V模型中。在第三阶段，利用分数蒸馏采样（SDS）根据获得的运动信息更新每个草图帧中贝塞尔曲线的参数。", "result": "该模型生成的草图视频不仅保留了草图的原始外观，而且精确反映了参考视频的动态运动。与替代方法相比，该方法在单次运动定制的挑战下，也能生成所需的草图视频。", "conclusion": "SketchAnimator成功地提供了一种将创意运动添加到给定草图的新颖方法，使得草图动画过程更加便捷，并能有效地从参考视频中学习复杂运动，生成高质量的动态草图动画。"}}
{"id": "2508.07432", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07432", "abs": "https://arxiv.org/abs/2508.07432", "authors": ["Vivek Hruday Kavuri", "Vysishtya Karanam", "Venkata Jahnavi Venkamsetty", "Kriti Madumadukala", "Lakshmipathi Balaji Darur", "Ponnurangam Kumaraguru"], "title": "Freeze and Reveal: Exposing Modality Bias in Vision-Language Models", "comment": null, "summary": "Vision Language Models achieve impressive multi-modal performance but often\ninherit gender biases from their training data. This bias might be coming from\nboth the vision and text modalities. In this work, we dissect the contributions\nof vision and text backbones to these biases by applying targeted debiasing\nusing Counterfactual Data Augmentation and Task Vector methods. Inspired by\ndata-efficient approaches in hate-speech classification, we introduce a novel\nmetric, Degree of Stereotypicality and a corresponding debiasing method, Data\nAugmentation Using Degree of Stereotypicality - DAUDoS, to reduce bias with\nminimal computational cost. We curate a gender annotated dataset and evaluate\nall methods on VisoGender benchmark to quantify improvements and identify\ndominant source of bias. Our results show that CDA reduces the gender gap by 6%\nand DAUDoS by 3% but using only one-third of the data. Both methods also\nimprove the model's ability to correctly identify gender in images by 3%, with\nDAUDoS achieving this improvement using only almost one-third of training data.\nFrom our experiment's, we observed that CLIP's vision encoder is more biased\nwhereas PaliGemma2's text encoder is more biased. By identifying whether bias\nstems more from vision or text encoders, our work enables more targeted and\neffective bias mitigation strategies in future multi-modal systems.", "AI": {"tldr": "本文研究了视觉语言模型中性别偏见的来源，并提出了两种去偏方法：反事实数据增强（CDA）和基于刻板印象程度的数据增强（DAUDoS）。结果表明，DAUDoS在数据效率上表现更优，并发现不同模型的视觉或文本编码器可能是偏见的主要来源。", "motivation": "视觉语言模型（VLMs）在多模态任务中表现出色，但其训练数据中常包含性别偏见。这种偏见可能来自视觉或文本模态，因此需要深入剖析并开发有针对性的去偏方法。", "method": "研究采用反事实数据增强（CDA）和任务向量（Task Vector）方法进行有针对性的去偏。受仇恨言论分类中数据高效方法的启发，引入了“刻板印象程度”（Degree of Stereotypicality）新指标，并提出了相应的数据增强方法DAUDoS。研究策展了一个性别标注数据集，并在VisoGender基准上评估了所有方法。", "result": "CDA将性别差距减少了6%，而DAUDoS在仅使用三分之一数据的情况下将性别差距减少了3%。两种方法都将模型正确识别图像性别的能力提高了3%，其中DAUDoS在几乎只使用三分之一训练数据的情况下达到了此改进。实验发现，CLIP的视觉编码器偏见更大，而PaliGemma2的文本编码器偏见更大。", "conclusion": "通过识别偏见主要来源于视觉或文本编码器，本研究为未来多模态系统中更具针对性和有效的偏见缓解策略提供了方向。DAUDoS在去偏效果和数据效率之间取得了良好平衡。"}}
{"id": "2508.07976", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07976", "abs": "https://arxiv.org/abs/2508.07976", "authors": ["Jiaxuan Gao", "Wei Fu", "Minyang Xie", "Shusheng Xu", "Chuyi He", "Zhiyu Mei", "Banghua Zhu", "Yi Wu"], "title": "Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale Asynchronous RL", "comment": null, "summary": "Recent advancements in LLM-based agents have demonstrated remarkable\ncapabilities in handling complex, knowledge-intensive tasks by integrating\nexternal tools. Among diverse choices of tools, search tools play a pivotal\nrole in accessing vast external knowledge. However, open-source agents still\nfall short of achieving expert-level Search Intelligence, the ability to\nresolve ambiguous queries, generate precise searches, analyze results, and\nconduct thorough exploration. Existing approaches fall short in scalability,\nefficiency, and data quality. For example, small turn limits in existing online\nRL methods, e.g. <=10, restrict complex strategy learning. This paper\nintroduces ASearcher, an open-source project for large-scale RL training of\nsearch agents. Our key contributions include: (1) Scalable fully asynchronous\nRL training that enables long-horizon search while maintaining high training\nefficiency. (2) A prompt-based LLM agent that autonomously synthesizes\nhigh-quality and challenging QAs, creating a large-scale QA dataset. Through RL\ntraining, our prompt-based QwQ-32B agent achieves substantial improvements,\nwith 46.7% and 20.8% Avg@4 gains on xBench and GAIA, respectively. Notably, our\nagent exhibits extreme long-horizon search, with tool calls exceeding 40 turns\nand output tokens exceeding 150k during training time. With a simple agent\ndesign and no external LLMs, ASearcher-Web-QwQ achieves Avg@4 scores of 42.1 on\nxBench and 52.8 on GAIA, surpassing existing open-source 32B agents. We\nopen-source our models, training data, and codes in\nhttps://github.com/inclusionAI/ASearcher.", "AI": {"tldr": "ASearcher是一个开源项目，通过可扩展的异步强化学习训练和高质量自合成问答数据，显著提升了大型语言模型搜索代理的搜索智能，使其能处理长序列复杂查询并超越现有开源代理。", "motivation": "现有基于LLM的代理在处理复杂、知识密集型任务时，虽然能集成外部工具，但在搜索智能方面（如解决歧义查询、生成精确搜索、分析结果和彻底探索）仍未达到专家水平。现有RL方法在可扩展性、效率和数据质量方面存在不足，例如回合限制过小，限制了复杂策略学习。", "method": "本文提出了ASearcher，一个用于大规模强化学习训练搜索代理的开源项目。主要方法包括：1) 采用可扩展的完全异步RL训练，以实现长序列搜索并保持高训练效率。2) 使用基于提示的LLM代理自主合成高质量和有挑战性的问答数据，构建大规模问答数据集。", "result": "通过RL训练，基于提示的QwQ-32B代理在xBench上取得了46.7%的Avg@4提升，在GAIA上取得了20.8%的Avg@4提升。该代理展示了极长的搜索序列，训练时工具调用超过40回合，输出token超过150k。ASearcher-Web-QwQ（采用简单代理设计，无需外部LLM）在xBench上获得42.1的Avg@4分数，在GAIA上获得52.8的分数，超越了现有开源的32B代理。模型、训练数据和代码已开源。", "conclusion": "ASearcher项目成功地通过创新的可扩展异步RL训练和高质量数据合成方法，显著提升了LLM搜索代理的搜索智能，使其能够处理更长序列的复杂任务，并超越了现有同等规模的开源解决方案，为未来研究提供了宝贵的资源。"}}
{"id": "2508.07162", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07162", "abs": "https://arxiv.org/abs/2508.07162", "authors": ["Xiaotong Lin", "Tianming Liang", "Jian-Fang Hu", "Kun-Yu Lin", "Yulei Kang", "Chunwei Tian", "Jianhuang Lai", "Wei-Shi Zheng"], "title": "CoopDiff: Anticipating 3D Human-object Interactions via Contact-consistent Decoupled Diffusion", "comment": null, "summary": "3D human-object interaction (HOI) anticipation aims to predict the future\nmotion of humans and their manipulated objects, conditioned on the historical\ncontext. Generally, the articulated humans and rigid objects exhibit different\nmotion patterns, due to their distinct intrinsic physical properties. However,\nthis distinction is ignored by most of the existing works, which intend to\ncapture the dynamics of both humans and objects within a single prediction\nmodel. In this work, we propose a novel contact-consistent decoupled diffusion\nframework CoopDiff, which employs two distinct branches to decouple human and\nobject motion modeling, with the human-object contact points as shared anchors\nto bridge the motion generation across branches. The human dynamics branch is\naimed to predict highly structured human motion, while the object dynamics\nbranch focuses on the object motion with rigid translations and rotations.\nThese two branches are bridged by a series of shared contact points with\nconsistency constraint for coherent human-object motion prediction. To further\nenhance human-object consistency and prediction reliability, we propose a\nhuman-driven interaction module to guide object motion modeling. Extensive\nexperiments on the BEHAVE and Human-object Interaction datasets demonstrate\nthat our CoopDiff outperforms state-of-the-art methods.", "AI": {"tldr": "本文提出CoopDiff，一个新颖的接触一致解耦扩散框架，用于3D人-物交互（HOI）预测。它通过独立的人体和物体运动建模分支，并以接触点作为共享锚点连接，实现了更准确的预测。", "motivation": "现有的3D人-物交互预测方法通常忽略了人体（关节式）和物体（刚性）由于其内在物理特性而展现出的不同运动模式，而是试图在单一模型中捕捉两者动态。", "method": "CoopDiff框架采用两个独立分支：人体动力学分支预测高度结构化的人体运动，物体动力学分支关注物体的刚性平移和旋转。这两个分支通过一系列共享接触点连接，并施加一致性约束。此外，还提出了一个人体驱动的交互模块来指导物体运动建模，以增强人-物一致性和预测可靠性。", "result": "在BEHAVE和Human-object Interaction数据集上的大量实验表明，CoopDiff优于现有最先进的方法。", "conclusion": "通过解耦人体和物体运动建模，并利用接触点进行连接和一致性约束，以及人体驱动的交互指导，能够有效提升3D人-物交互运动预测的性能和可靠性。"}}
{"id": "2508.07514", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07514", "abs": "https://arxiv.org/abs/2508.07514", "authors": ["Artzai Picon", "Itziar Eguskiza", "Daniel Mugica", "Javier Romero", "Carlos Javier Jimenez", "Eric White", "Gabriel Do-Lago-Junqueira", "Christian Klukas", "Ramon Navarra-Mestre"], "title": "From Field to Drone: Domain Drift Tolerant Automated Multi-Species and Damage Plant Semantic Segmentation for Herbicide Trials", "comment": null, "summary": "Field trials are vital in herbicide research and development to assess\neffects on crops and weeds under varied conditions. Traditionally, evaluations\nrely on manual visual assessments, which are time-consuming, labor-intensive,\nand subjective. Automating species and damage identification is challenging due\nto subtle visual differences, but it can greatly enhance efficiency and\nconsistency.\n  We present an improved segmentation model combining a general-purpose\nself-supervised visual model with hierarchical inference based on botanical\ntaxonomy. Trained on a multi-year dataset (2018-2020) from Germany and Spain\nusing digital and mobile cameras, the model was tested on digital camera data\n(year 2023) and drone imagery from the United States, Germany, and Spain (year\n2024) to evaluate robustness under domain shift. This cross-device evaluation\nmarks a key step in assessing generalization across platforms of the model.\n  Our model significantly improved species identification (F1-score: 0.52 to\n0.85, R-squared: 0.75 to 0.98) and damage classification (F1-score: 0.28 to\n0.44, R-squared: 0.71 to 0.87) over prior methods. Under domain shift (drone\nimages), it maintained strong performance with moderate degradation (species:\nF1-score 0.60, R-squared 0.80; damage: F1-score 0.41, R-squared 0.62), where\nearlier models failed.\n  These results confirm the model's robustness and real-world applicability. It\nis now deployed in BASF's phenotyping pipeline, enabling large-scale, automated\ncrop and weed monitoring across diverse geographies.", "AI": {"tldr": "该研究开发了一种结合自监督视觉模型和植物分类学分层推理的改进分割模型，用于自动化除草剂田间试验中的作物和杂草种类识别及损伤分类，并在跨域（如无人机图像）条件下表现出强大的鲁棒性。", "motivation": "传统的除草剂田间试验评估依赖人工目视检查，这种方法耗时、费力且主观。自动化物种和损伤识别可以显著提高效率和一致性，尽管存在细微视觉差异，但仍是亟待解决的挑战。", "method": "提出了一种改进的分割模型，该模型结合了通用自监督视觉模型和基于植物分类学的分层推理。模型使用来自德国和西班牙的多年（2018-2020）数字和移动相机数据集进行训练，并在2023年数字相机数据和2024年来自美国、德国和西班牙的无人机图像上进行测试，以评估其在领域漂移下的鲁棒性。", "result": "该模型在物种识别方面显著优于现有方法（F1分数从0.52提升到0.85，R平方从0.75提升到0.98），在损伤分类方面也有显著提升（F1分数从0.28提升到0.44，R平方从0.71提升到0.87）。在领域漂移（无人机图像）条件下，模型仍保持良好性能（物种：F1分数0.60，R平方0.80；损伤：F1分数0.41，R平方0.62），而早期模型则表现不佳。", "conclusion": "研究结果证实了该模型的鲁棒性和实际应用价值。该模型目前已部署在巴斯夫（BASF）的表型分析管线中，实现了跨地域大规模、自动化作物和杂草监测。"}}
{"id": "2508.07993", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.07993", "abs": "https://arxiv.org/abs/2508.07993", "authors": ["Anna Sofia Lippolis", "Andrea Giovanni Nuzzolese", "Aldo Gangemi"], "title": "The Medical Metaphors Corpus (MCC)", "comment": null, "summary": "Metaphor is a fundamental cognitive mechanism that shapes scientific\nunderstanding, enabling the communication of complex concepts while potentially\nconstraining paradigmatic thinking. Despite the prevalence of figurative\nlanguage in scientific discourse, existing metaphor detection resources\nprimarily focus on general-domain text, leaving a critical gap for\ndomain-specific applications. In this paper, we present the Medical Metaphors\nCorpus (MCC), a comprehensive dataset of 792 annotated scientific conceptual\nmetaphors spanning medical and biological domains. MCC aggregates metaphorical\nexpressions from diverse sources including peer-reviewed literature, news\nmedia, social media discourse, and crowdsourced contributions, providing both\nbinary and graded metaphoricity judgments validated through human annotation.\nEach instance includes source-target conceptual mappings and perceived\nmetaphoricity scores on a 0-7 scale, establishing the first annotated resource\nfor computational scientific metaphor research. Our evaluation demonstrates\nthat state-of-the-art language models achieve modest performance on scientific\nmetaphor detection, revealing substantial room for improvement in\ndomain-specific figurative language understanding. MCC enables multiple\nresearch applications including metaphor detection benchmarking, quality-aware\ngeneration systems, and patient-centered communication tools.", "AI": {"tldr": "本文介绍了医学隐喻语料库（MCC），这是一个包含792个已标注的医学和生物领域科学概念隐喻的综合数据集，旨在填补领域特定隐喻检测资源的空白，并为计算科学隐喻研究提供首个标注资源。", "motivation": "尽管隐喻在科学话语中普遍存在并对科学理解至关重要，但现有隐喻检测资源主要集中于通用领域文本，缺乏针对科学领域（特别是医学和生物学）的特定资源，这限制了领域特定应用的发展。此外，隐喻虽有助于沟通复杂概念，但也可能限制范式思维。", "method": "研究者构建了医学隐喻语料库（MCC），聚合了来自同行评审文献、新闻媒体、社交媒体和众包贡献的隐喻表达。该语料库包含792个已标注的科学概念隐喻，提供二元和分级隐喻性判断（0-7分制），并包括源-目标概念映射。所有标注均经过人工验证。", "result": "MCC是计算科学隐喻研究的第一个标注资源。对现有先进语言模型进行的评估表明，它们在科学隐喻检测上的表现平平，这揭示了在领域特定具象语言理解方面仍有巨大的改进空间。", "conclusion": "MCC数据集能够支持多项研究应用，包括隐喻检测基准测试、质量感知生成系统以及以患者为中心的沟通工具，从而促进对领域特定具象语言的理解和应用。"}}
{"id": "2508.07171", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07171", "abs": "https://arxiv.org/abs/2508.07171", "authors": ["Huihui Xu", "Jiashi Lin", "Haoyu Chen", "Junjun He", "Lei Zhu"], "title": "EventRR: Event Referential Reasoning for Referring Video Object Segmentation", "comment": null, "summary": "Referring Video Object Segmentation (RVOS) aims to segment out the object in\na video referred by an expression. Current RVOS methods view referring\nexpressions as unstructured sequences, neglecting their crucial semantic\nstructure essential for referent reasoning. Besides, in contrast to\nimage-referring expressions whose semantics focus only on object attributes and\nobject-object relations, video-referring expressions also encompass event\nattributes and event-event temporal relations. This complexity challenges\ntraditional structured reasoning image approaches. In this paper, we propose\nthe Event Referential Reasoning (EventRR) framework. EventRR decouples RVOS\ninto object summarization part and referent reasoning part. The summarization\nphase begins by summarizing each frame into a set of bottleneck tokens, which\nare then efficiently aggregated in the video-level summarization step to\nexchange the global cross-modal temporal context. For reasoning part, EventRR\nextracts semantic eventful structure of a video-referring expression into\nhighly expressive Referential Event Graph (REG), which is a single-rooted\ndirected acyclic graph. Guided by topological traversal of REG, we propose\nTemporal Concept-Role Reasoning (TCRR) to accumulate the referring score of\neach temporal query from REG leaf nodes to root node. Each reasoning step can\nbe interpreted as a question-answer pair derived from the concept-role\nrelations in REG. Extensive experiments across four widely recognized benchmark\ndatasets, show that EventRR quantitatively and qualitatively outperforms\nstate-of-the-art RVOS methods. Code is available at\nhttps://github.com/bio-mlhui/EventRR", "AI": {"tldr": "本文提出EventRR框架，通过解耦对象摘要和指代推理，并引入事件指代表达的语义结构（Referential Event Graph）进行时序概念-角色推理，解决了现有RVOS方法忽略语义结构和视频特有复杂性的问题，在多个基准测试中优于SOTA方法。", "motivation": "现有视频对象分割（RVOS）方法将指代表达视为非结构化序列，忽略了其对指代推理至关重要的语义结构。此外，与图像指代表达不同，视频指代表达不仅包含对象属性和关系，还涉及事件属性和事件间时序关系，这使得传统结构化推理方法面临挑战。", "method": "本文提出了事件指代推理（EventRR）框架。该框架将RVOS解耦为对象摘要和指代推理两部分。在摘要阶段，首先将每帧总结为瓶颈Token，然后进行视频级聚合以交换全局跨模态时序上下文。在推理阶段，EventRR将视频指代表达的语义事件结构提取为高度表达力的指代事件图（REG），该图是一个单根有向无环图。通过REG的拓扑遍历，提出了时序概念-角色推理（TCRR），从REG叶节点到根节点累积每个时序查询的指代得分，每个推理步骤可解释为REG中概念-角色关系派生的问答对。", "result": "EventRR在四个广泛认可的基准数据集上进行了大量实验，结果表明它在定量和定性上均优于现有的最先进RVOS方法。", "conclusion": "EventRR框架通过有效利用视频指代表达的语义事件结构和时序推理，成功解决了RVOS任务中的复杂性挑战，并取得了卓越的性能。"}}
{"id": "2508.07538", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07538", "abs": "https://arxiv.org/abs/2508.07538", "authors": ["Hongzhu Jiang", "Sihan Xie", "Zhiyu Wan"], "title": "A DICOM Image De-identification Algorithm in the MIDI-B Challenge", "comment": "8 pages, 5 figures", "summary": "Image de-identification is essential for the public sharing of medical\nimages, particularly in the widely used Digital Imaging and Communications in\nMedicine (DICOM) format as required by various regulations and standards,\nincluding Health Insurance Portability and Accountability Act (HIPAA) privacy\nrules, the DICOM PS3.15 standard, and best practices recommended by the Cancer\nImaging Archive (TCIA). The Medical Image De-Identification Benchmark (MIDI-B)\nChallenge at the 27th International Conference on Medical Image Computing and\nComputer Assisted Intervention (MICCAI 2024) was organized to evaluate\nrule-based DICOM image de-identification algorithms with a large dataset of\nclinical DICOM images. In this report, we explore the critical challenges of\nde-identifying DICOM images, emphasize the importance of removing personally\nidentifiable information (PII) to protect patient privacy while ensuring the\ncontinued utility of medical data for research, diagnostics, and treatment, and\nprovide a comprehensive overview of the standards and regulations that govern\nthis process. Additionally, we detail the de-identification methods we applied\n- such as pixel masking, date shifting, date hashing, text recognition, text\nreplacement, and text removal - to process datasets during the test phase in\nstrict compliance with these standards. According to the final leaderboard of\nthe MIDI-B challenge, the latest version of our solution algorithm correctly\nexecuted 99.92% of the required actions and ranked 2nd out of 10 teams that\ncompleted the challenge (from a total of 22 registered teams). Finally, we\nconducted a thorough analysis of the resulting statistics and discussed the\nlimitations of current approaches and potential avenues for future improvement.", "AI": {"tldr": "本文报告了MIDI-B挑战赛中DICOM图像去识别的方法和结果，强调了保护患者隐私同时保持数据可用性的重要性，并展示了其解决方案的高效性。", "motivation": "医学图像（特别是DICOM格式）的公开共享需要去识别化，以遵守HIPAA隐私规则、DICOM PS3.15标准和TCIA推荐的最佳实践等法规和标准，保护患者隐私，同时确保医疗数据对研究、诊断和治疗的持续可用性。", "method": "本文采用了基于规则的DICOM图像去识别算法，具体方法包括像素遮罩、日期偏移、日期哈希、文本识别、文本替换和文本删除，以严格遵守相关标准处理数据集。", "result": "在MIDI-B挑战赛中，作者的最新版解决方案正确执行了99.92%的所需操作，在完成挑战的10支队伍中排名第2（总共22支注册队伍）。", "conclusion": "研究分析了去识别统计数据，讨论了当前方法的局限性以及未来改进的潜在方向，强调了在保护隐私和数据可用性之间取得平衡的重要性。"}}
{"id": "2508.07999", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.07999", "abs": "https://arxiv.org/abs/2508.07999", "authors": ["Ryan Wong", "Jiawei Wang", "Junjie Zhao", "Li Chen", "Yan Gao", "Long Zhang", "Xuan Zhou", "Zuo Wang", "Kai Xiang", "Ge Zhang", "Wenhao Huang", "Yang Wang", "Ke Wang"], "title": "WideSearch: Benchmarking Agentic Broad Info-Seeking", "comment": null, "summary": "From professional research to everyday planning, many tasks are bottlenecked\nby wide-scale information seeking, which is more repetitive than cognitively\ncomplex. With the rapid development of Large Language Models (LLMs), automated\nsearch agents powered by LLMs offer a promising solution to liberate humans\nfrom this tedious work. However, the capability of these agents to perform such\n\"wide-context\" collection reliably and completely remains largely unevaluated\ndue to a lack of suitable benchmarks. To bridge this gap, we introduce\nWideSearch, a new benchmark engineered to evaluate agent reliability on these\nlarge-scale collection tasks. The benchmark features 200 manually curated\nquestions (100 in English, 100 in Chinese) from over 15 diverse domains,\ngrounded in real user queries. Each task requires agents to collect large-scale\natomic information, which could be verified one by one objectively, and arrange\nit into a well-organized output. A rigorous five-stage quality control pipeline\nensures the difficulty, completeness, and verifiability of the dataset. We\nbenchmark over 10 state-of-the-art agentic search systems, including\nsingle-agent, multi-agent frameworks, and end-to-end commercial systems. Most\nsystems achieve overall success rates near 0\\%, with the best performer\nreaching just 5\\%. However, given sufficient time, cross-validation by multiple\nhuman testers can achieve a near 100\\% success rate. These results demonstrate\nthat present search agents have critical deficiencies in large-scale\ninformation seeking, underscoring urgent areas for future research and\ndevelopment in agentic search. Our dataset, evaluation pipeline, and benchmark\nresults have been publicly released at https://widesearch-seed.github.io/", "AI": {"tldr": "该研究引入了WideSearch基准测试，用于评估LLM驱动的搜索代理在广范围信息收集任务中的可靠性。结果显示，当前代理在此类任务中表现极差，成功率接近0%，突显了未来研究和开发的紧迫性。", "motivation": "许多任务（从专业研究到日常规划）都受限于重复且耗时的广范围信息检索。尽管大型语言模型（LLM）驱动的自动化搜索代理有望解决此问题，但由于缺乏合适的基准，其在“广上下文”信息收集方面的能力尚未得到充分评估。", "method": "研究构建了一个名为WideSearch的新基准，包含200个（100英，100中）人工策划的问题，涵盖15+个不同领域，源于真实用户查询。任务要求代理收集可逐一客观验证的大规模原子信息，并将其组织成良好结构化的输出。通过严格的五阶段质量控制流程确保数据集的难度、完整性和可验证性。研究测试了超过10个最先进的代理搜索系统，包括单代理、多代理框架和端到端商业系统。", "result": "大多数系统在广范围信息收集任务中成功率接近0%，表现最好的系统也仅达到5%。然而，人类测试人员在充足时间和交叉验证下，可以达到接近100%的成功率。", "conclusion": "当前搜索代理在处理大规模信息检索方面存在严重缺陷，这表明代理搜索领域需要紧急的未来研究和开发。该研究的数据集、评估流程和基准测试结果已公开发布。"}}
{"id": "2508.07211", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07211", "abs": "https://arxiv.org/abs/2508.07211", "authors": ["Junyi He", "Liuling Chen", "Hongyang Zhou", "Zhang xiaoxing", "Xiaobin Zhu", "Shengxiang Yu", "Jingyan Qin", "Xu-Cheng Yin"], "title": "Similarity Matters: A Novel Depth-guided Network for Image Restoration and A New Dataset", "comment": "12 pages, 10 figures", "summary": "Image restoration has seen substantial progress in recent years. However,\nexisting methods often neglect depth information, which hurts similarity\nmatching, results in attention distractions in shallow depth-of-field (DoF)\nscenarios, and excessive enhancement of background content in deep DoF\nsettings. To overcome these limitations, we propose a novel Depth-Guided\nNetwork (DGN) for image restoration, together with a novel large-scale\nhigh-resolution dataset. Specifically, the network consists of two interactive\nbranches: a depth estimation branch that provides structural guidance, and an\nimage restoration branch that performs the core restoration task. In addition,\nthe image restoration branch exploits intra-object similarity through\nprogressive window-based self-attention and captures inter-object similarity\nvia sparse non-local attention. Through joint training, depth features\ncontribute to improved restoration quality, while the enhanced visual features\nfrom the restoration branch in turn help refine depth estimation. Notably, we\nalso introduce a new dataset for training and evaluation, consisting of 9,205\nhigh-resolution images from 403 plant species, with diverse depth and texture\nvariations. Extensive experiments show that our method achieves\nstate-of-the-art performance on several standard benchmarks and generalizes\nwell to unseen plant images, demonstrating its effectiveness and robustness.", "AI": {"tldr": "提出了一种深度引导网络（DGN）用于图像修复，通过深度信息改善修复质量，并引入了一个新的大规模高分辨率数据集。", "motivation": "现有图像修复方法忽略深度信息，导致在浅景深（DoF）场景中注意力分散，在深景深场景中背景过度增强，并影响相似性匹配。", "method": "提出DGN，包含两个交互分支：深度估计分支提供结构引导，图像修复分支执行核心修复任务。修复分支利用渐进式窗口自注意力捕获对象内相似性，通过稀疏非局部注意力捕获对象间相似性。通过联合训练，深度特征提升修复质量，修复分支的视觉特征反过来帮助优化深度估计。此外，还引入了一个包含9,205张高分辨率图像的新数据集。", "result": "在多个标准基准测试中达到了最先进的性能，并能很好地泛化到未见过的植物图像，证明了其有效性和鲁棒性。", "conclusion": "所提出的深度引导网络（DGN）及其新数据集在图像修复方面表现出卓越的性能和泛化能力，有效解决了现有方法忽略深度信息的问题。"}}
{"id": "2508.07597", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07597", "abs": "https://arxiv.org/abs/2508.07597", "authors": ["Yuang Zhang", "Junqi Cheng", "Haoyu Zhao", "Jiaxi Gu", "Fangyuan Zou", "Zenghui Lu", "Peng Shu"], "title": "ShoulderShot: Generating Over-the-Shoulder Dialogue Videos", "comment": null, "summary": "Over-the-shoulder dialogue videos are essential in films, short dramas, and\nadvertisements, providing visual variety and enhancing viewers' emotional\nconnection. Despite their importance, such dialogue scenes remain largely\nunderexplored in video generation research. The main challenges include\nmaintaining character consistency across different shots, creating a sense of\nspatial continuity, and generating long, multi-turn dialogues within limited\ncomputational budgets. Here, we present ShoulderShot, a framework that combines\ndual-shot generation with looping video, enabling extended dialogues while\npreserving character consistency. Our results demonstrate capabilities that\nsurpass existing methods in terms of shot-reverse-shot layout, spatial\ncontinuity, and flexibility in dialogue length, thereby opening up new\npossibilities for practical dialogue video generation. Videos and comparisons\nare available at https://shouldershot.github.io.", "AI": {"tldr": "ShoulderShot是一个生成过肩对话视频的框架，它结合了双镜头生成和循环视频，解决了角色一致性、空间连续性和长对话生成等挑战。", "motivation": "过肩对话视频在影视中至关重要，能提供视觉多样性并增强情感连接，但在视频生成研究中仍未被充分探索。主要挑战包括跨镜头角色一致性、空间连续性以及在有限计算资源下生成长多轮对话。", "method": "本文提出了名为“ShoulderShot”的框架，该框架结合了双镜头（dual-shot）生成和循环视频（looping video）技术，以实现在保持角色一致性的同时生成扩展对话。", "result": "实验结果表明，ShoulderShot在镜头反转布局、空间连续性和对话长度灵活性方面超越了现有方法。", "conclusion": "ShoulderShot为实用的对话视频生成开辟了新的可能性，解决了该领域的核心挑战。"}}
{"id": "2508.08011", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.08011", "abs": "https://arxiv.org/abs/2508.08011", "authors": ["Mingzi Cao", "Xi Wang", "Nikolaos Aletras"], "title": "Progressive Depth Up-scaling via Optimal Transport", "comment": null, "summary": "Scaling Large Language Models (LLMs) yields performance gains but incurs\nsubstantial training costs. Depth up-scaling offers training efficiency by\nadding new layers to pre-trained models. However, most existing methods copy or\naverage weights from base layers, neglecting neuron permutation differences.\nThis limitation can potentially cause misalignment that harms performance.\nInspired by applying Optimal Transport (OT) for neuron alignment, we propose\nOptimal Transport Depth Up-Scaling (OpT-DeUS). OpT-DeUS aligns and fuses\nTransformer blocks in adjacent base layers via OT for new layer creation, to\nmitigate neuron permutation mismatch between layers. OpT-DeUS achieves better\noverall performance and offers improved training efficiency than existing\nmethods for continual pre-training and supervised fine-tuning across different\nmodel sizes. To further evaluate the impact of interpolation positions, our\nextensive analysis shows that inserting new layers closer to the top results in\nhigher training efficiency due to shorter back-propagation time while obtaining\nadditional performance gains.", "AI": {"tldr": "该研究提出OpT-DeUS方法，通过最优传输（OT）对预训练模型进行深度扩展，以解决神经元排列不匹配问题，从而提高大型语言模型的训练效率和性能。", "motivation": "扩展大型语言模型（LLMs）能提升性能但训练成本高昂。深度扩展通过增加新层来提高训练效率，但现有方法在复制或平均权重时忽略了神经元排列差异，可能导致性能下降。", "method": "提出OpT-DeUS方法，受神经元对齐中应用最优传输（OT）的启发，利用OT对相邻基础层中的Transformer块进行对齐和融合，以创建新层，从而减轻层间神经元排列不匹配。此外，还分析了新层的插入位置对训练效率和性能的影响。", "result": "OpT-DeUS在持续预训练和监督微调中，对于不同模型尺寸，均比现有方法展现出更好的整体性能和更高的训练效率。深入分析表明，将新层插入到模型顶部附近可以缩短反向传播时间，从而获得更高的训练效率和额外的性能提升。", "conclusion": "OpT-DeUS通过解决神经元排列不匹配问题，提供了一种有效且高效的深度扩展大型语言模型的方法。通过优化新层的插入位置（靠近顶部），可以进一步提高训练效率和性能。"}}
{"id": "2508.07214", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.07214", "abs": "https://arxiv.org/abs/2508.07214", "authors": ["Hongyang Zhou", "Xiaobin Zhu", "Liuling Chen", "Junyi He", "Jingyan Qin", "Xu-Cheng Yin", "Zhang xiaoxing"], "title": "Unsupervised Real-World Super-Resolution via Rectified Flow Degradation Modelling", "comment": "10 pages, 9 figures", "summary": "Unsupervised real-world super-resolution (SR) faces critical challenges due\nto the complex, unknown degradation distributions in practical scenarios.\nExisting methods struggle to generalize from synthetic low-resolution (LR) and\nhigh-resolution (HR) image pairs to real-world data due to a significant domain\ngap. In this paper, we propose an unsupervised real-world SR method based on\nrectified flow to effectively capture and model real-world degradation,\nsynthesizing LR-HR training pairs with realistic degradation. Specifically,\ngiven unpaired LR and HR images, we propose a novel Rectified Flow Degradation\nModule (RFDM) that introduces degradation-transformed LR (DT-LR) images as\nintermediaries. By modeling the degradation trajectory in a continuous and\ninvertible manner, RFDM better captures real-world degradation and enhances the\nrealism of generated LR images. Additionally, we propose a Fourier Prior Guided\nDegradation Module (FGDM) that leverages structural information embedded in\nFourier phase components to ensure more precise modeling of real-world\ndegradation. Finally, the LR images are processed by both FGDM and RFDM,\nproducing final synthetic LR images with real-world degradation. The synthetic\nLR images are paired with the given HR images to train the off-the-shelf SR\nnetworks. Extensive experiments on real-world datasets demonstrate that our\nmethod significantly enhances the performance of existing SR approaches in\nreal-world scenarios.", "AI": {"tldr": "本文提出一种基于整流流（Rectified Flow）的无监督真实世界超分辨率（SR）方法，通过引入整流流降质模块（RFDM）和傅里叶先验引导降质模块（FGDM）来合成具有真实世界降质的LR-HR训练对，从而弥补合成数据与真实数据之间的域差距，提升现有SR网络在真实场景下的性能。", "motivation": "现有超分辨率方法难以泛化到真实世界数据，因为合成的低分辨率（LR）和高分辨率（HR）图像对与真实数据之间存在显著的域差距，且真实世界中的降质分布复杂且未知。", "method": "1. 提出一种基于整流流的无监督真实世界SR方法。2. 设计新颖的整流流降质模块（RFDM），引入降质变换后的LR（DT-LR）图像作为中间体，以连续可逆的方式建模降质轨迹，增强生成LR图像的真实感。3. 提出傅里叶先验引导降质模块（FGDM），利用傅里叶相位分量中的结构信息，更精确地建模真实世界降质。4. 将LR图像同时通过FGDM和RFDM处理，生成具有真实世界降质的合成LR图像。5. 将这些合成LR图像与给定的HR图像配对，用于训练现有的SR网络。", "result": "在真实世界数据集上的大量实验表明，该方法显著提升了现有SR方法在真实场景下的性能。", "conclusion": "该方法通过有效捕捉和建模真实世界降质，合成了具有真实降质的LR-HR训练对，成功解决了真实世界超分辨率中的域差距问题，从而显著提升了现有SR模型在实际应用中的表现。"}}
{"id": "2508.07621", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07621", "abs": "https://arxiv.org/abs/2508.07621", "authors": ["Yunsung Chung", "Chanho Lim", "Ghassan Bidaoui", "Christian Massad", "Nassir Marrouche", "Jihun Hamm"], "title": "SOFA: Deep Learning Framework for Simulating and Optimizing Atrial Fibrillation Ablation", "comment": "Accepted at MICCAI 2025. This is the author's original preprint", "summary": "Atrial fibrillation (AF) is a prevalent cardiac arrhythmia often treated with\ncatheter ablation procedures, but procedural outcomes are highly variable.\nEvaluating and improving ablation efficacy is challenging due to the complex\ninteraction between patient-specific tissue and procedural factors. This paper\nasks two questions: Can AF recurrence be predicted by simulating the effects of\nprocedural parameters? How should we ablate to reduce AF recurrence? We propose\nSOFA (Simulating and Optimizing Atrial Fibrillation Ablation), a novel\ndeep-learning framework that addresses these questions. SOFA first simulates\nthe outcome of an ablation strategy by generating a post-ablation image\ndepicting scar formation, conditioned on a patient's pre-ablation LGE-MRI and\nthe specific procedural parameters used (e.g., ablation locations, duration,\ntemperature, power, and force). During this simulation, it predicts AF\nrecurrence risk. Critically, SOFA then introduces an optimization scheme that\nrefines these procedural parameters to minimize the predicted risk. Our method\nleverages a multi-modal, multi-view generator that processes 2.5D\nrepresentations of the atrium. Quantitative evaluations show that SOFA\naccurately synthesizes post-ablation images and that our optimization scheme\nleads to a 22.18\\% reduction in the model-predicted recurrence risk. To the\nbest of our knowledge, SOFA is the first framework to integrate the simulation\nof procedural effects, recurrence prediction, and parameter optimization,\noffering a novel tool for personalizing AF ablation.", "AI": {"tldr": "SOFA是一个深度学习框架，通过模拟导管消融手术参数对心房颤动（AF）疤痕形成的影响，预测AF复发风险，并优化手术参数以降低复发率，从而实现个性化AF消融。", "motivation": "心房颤动（AF）导管消融手术的效果差异很大，且由于患者组织和手术因素的复杂相互作用，评估和提高消融疗效极具挑战性。本研究旨在解决两个问题：能否通过模拟手术参数的影响来预测AF复发？以及如何进行消融以减少AF复发？", "method": "SOFA框架首先模拟消融策略的结果，根据患者术前LGE-MRI和特定的手术参数（如消融位置、持续时间、温度、功率、压力），生成术后疤痕形成的图像，并在此过程中预测AF复发风险。然后，SOFA引入一个优化方案，调整这些手术参数以最小化预测的复发风险。该方法利用一个多模态、多视图生成器，处理心房的2.5D表示。", "result": "定量评估显示，SOFA能准确合成术后图像，并且其优化方案使模型预测的复发风险降低了22.18%。", "conclusion": "SOFA是首个整合手术效果模拟、复发预测和参数优化的框架，为个性化AF消融提供了一种新颖的工具。"}}
{"id": "2508.08050", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.08050", "abs": "https://arxiv.org/abs/2508.08050", "authors": ["Fabrizio Nunnari", "Cristina Luna Jiménez", "Rosalee Wolfe", "John C. McDonald", "Michael Filhol", "Eleni Efthimiou", "Evita Fotinea", "Thomas Hanke"], "title": "9th Workshop on Sign Language Translation and Avatar Technologies (SLTAT 2025)", "comment": null, "summary": "The Sign Language Translation and Avatar Technology (SLTAT) workshops\ncontinue a series of gatherings to share recent advances in improving deaf /\nhuman communication through non-invasive means. This 2025 edition, the 9th\nsince its first appearance in 2011, is hosted by the International Conference\non Intelligent Virtual Agents (IVA), giving the opportunity for contamination\nbetween two research communities, using digital humans as either virtual\ninterpreters or as interactive conversational agents. As presented in this\nsummary paper, SLTAT sees contributions beyond avatar technologies, with a\nconsistent number of submissions on sign language recognition, and other work\non data collection, data analysis, tools, ethics, usability, and affective\ncomputing.", "AI": {"tldr": "本文概述了2025年手语翻译和虚拟形象技术（SLTAT）研讨会，该研讨会旨在通过非侵入性方式改善聋人与人之间的交流，并涵盖了手语识别、虚拟形象技术、数据收集、伦理等多个研究领域。", "motivation": "研究动机是为了通过非侵入性技术（如数字人类）改善聋人与听人之间的交流，并汇集相关领域的最新研究进展，促进不同研究社区（手语翻译与智能虚拟代理）之间的交叉学习。", "method": "该研讨会所涵盖的方法和贡献领域包括：手语识别、手语翻译与虚拟形象技术（作为虚拟翻译或交互式对话代理）、数据收集与分析、相关工具开发、伦理考量、可用性研究以及情感计算。", "result": "研讨会吸引了大量投稿，不仅涵盖了虚拟形象技术，还有大量关于手语识别的提交，以及数据收集、数据分析、工具、伦理、可用性和情感计算等方面的研究成果。", "conclusion": "SLTAT研讨会持续作为推动手语交流技术进步的重要平台，通过整合虚拟形象技术和其他相关研究领域，促进了跨社区合作，以期更有效地改善聋人与人之间的沟通。"}}
{"id": "2508.07216", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07216", "abs": "https://arxiv.org/abs/2508.07216", "authors": ["Songlin Li", "Zhiqing Guo", "Yuanman Li", "Zeyu Li", "Yunfeng Diao", "Gaobo Yang", "Liejun Wang"], "title": "Bridging Semantic Logic Gaps: A Cognition-Inspired Multimodal Boundary-Preserving Network for Image Manipulation Localization", "comment": null, "summary": "The existing image manipulation localization (IML) models mainly relies on\nvisual cues, but ignores the semantic logical relationships between content\nfeatures. In fact, the content semantics conveyed by real images often conform\nto human cognitive laws. However, image manipulation technology usually\ndestroys the internal relationship between content features, thus leaving\nsemantic clues for IML. In this paper, we propose a cognition-inspired\nmultimodal boundary-preserving network (CMB-Net). Specifically, CMB-Net\nutilizes large language models (LLMs) to analyze manipulated regions within\nimages and generate prompt-based textual information to compensate for the lack\nof semantic relationships in the visual information. Considering that the\nerroneous texts induced by hallucination from LLMs will damage the accuracy of\nIML, we propose an image-text central ambiguity module (ITCAM). It assigns\nweights to the text features by quantifying the ambiguity between text and\nimage features, thereby ensuring the beneficial impact of textual information.\nWe also propose an image-text interaction module (ITIM) that aligns visual and\ntext features using a correlation matrix for fine-grained interaction. Finally,\ninspired by invertible neural networks, we propose a restoration edge decoder\n(RED) that mutually generates input and output features to preserve boundary\ninformation in manipulated regions without loss. Extensive experiments show\nthat CMB-Net outperforms most existing IML models.", "AI": {"tldr": "该论文提出了一种认知启发的多模态边界保持网络（CMB-Net），通过整合大语言模型（LLMs）生成的文本语义信息和视觉线索，并设计了模块来处理LLMs幻觉和进行精细化图像-文本交互，以提高图像篡改定位的准确性。", "motivation": "现有图像篡改定位（IML）模型主要依赖视觉线索，但忽略了内容特征之间的语义逻辑关系。图像篡改往往破坏了这些内部语义关系，这为IML提供了语义线索。因此，研究旨在利用这些语义信息来增强IML。", "method": "本文提出了CMB-Net，具体方法包括：1) 利用LLMs分析图像中的篡改区域并生成基于提示的文本信息，以弥补视觉信息中语义关系的缺失；2) 提出图像-文本中心模糊模块（ITCAM），通过量化文本和图像特征之间的模糊性来分配文本特征权重，以减轻LLMs幻觉带来的负面影响；3) 提出图像-文本交互模块（ITIM），使用相关矩阵对视觉和文本特征进行对齐，实现细粒度交互；4) 提出恢复边缘解码器（RED），受可逆神经网络启发，通过相互生成输入和输出特征来无损地保留篡改区域的边界信息。", "result": "广泛的实验表明，CMB-Net的性能优于大多数现有IML模型。", "conclusion": "CMB-Net通过有效地整合视觉和语义信息，并解决了LLMs可能引入的误差问题，显著提升了图像篡改定位的准确性和边界保持能力。"}}
{"id": "2508.07683", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07683", "abs": "https://arxiv.org/abs/2508.07683", "authors": ["Chaohong Guo", "Xun Mo", "Yongwei Nie", "Xuemiao Xu", "Chao Xu", "Fei Yu", "Chengjiang Long"], "title": "TAR-TVG: Enhancing VLMs with Timestamp Anchor-Constrained Reasoning for Temporal Video Grounding", "comment": null, "summary": "Temporal Video Grounding (TVG) aims to precisely localize video segments\ncorresponding to natural language queries, which is a critical capability for\nlong-form video understanding. Although existing reinforcement learning\napproaches encourage models to generate reasoning chains before predictions,\nthey fail to explicitly constrain the reasoning process to ensure the quality\nof the final temporal predictions. To address this limitation, we propose\nTimestamp Anchor-constrained Reasoning for Temporal Video Grounding (TAR-TVG),\na novel framework that introduces timestamp anchors within the reasoning\nprocess to enforce explicit supervision to the thought content. These anchors\nserve as intermediate verification points. More importantly, we require each\nreasoning step to produce increasingly accurate temporal estimations, thereby\nensuring that the reasoning process contributes meaningfully to the final\nprediction. To address the challenge of low-probability anchor generation in\nmodels (e.g., Qwen2.5-VL-3B), we develop an efficient self-distillation\ntraining strategy: (1) initial GRPO training to collect 30K high-quality\nreasoning traces containing multiple timestamp anchors, (2) supervised\nfine-tuning (SFT) on distilled data, and (3) final GRPO optimization on the\nSFT-enhanced model. This three-stage training strategy enables robust anchor\ngeneration while maintaining reasoning quality. Experiments show that our model\nachieves state-of-the-art performance while producing interpretable, verifiable\nreasoning chains with progressively refined temporal estimations.", "AI": {"tldr": "本文提出TAR-TVG框架，通过在时间视频定位（TVG）的推理过程中引入时间戳锚点，强制性地监督思考内容，并要求每个推理步骤生成更精确的时间估计，以解决现有强化学习方法未能明确约束推理过程质量的问题。", "motivation": "现有的强化学习方法在时间视频定位任务中，虽然鼓励模型生成推理链，但未能明确约束推理过程，以确保最终时间预测的质量。", "method": "提出时间戳锚点约束推理的TVG（TAR-TVG）框架，在推理过程中引入时间戳锚点作为中间验证点，并要求每个推理步骤产生越来越精确的时间估计。为解决模型（如Qwen2.5-VL-3B）锚点生成概率低的问题，开发了高效的自蒸馏训练策略：(1) 初始GRPO训练收集高质量推理轨迹，(2) 对蒸馏数据进行监督微调（SFT），(3) 对SFT增强模型进行最终GRPO优化。", "result": "实验表明，TAR-TVG模型在性能上达到最先进水平，同时生成可解释、可验证的推理链，并具有逐步细化的时间估计。", "conclusion": "TAR-TVG通过引入时间戳锚点和多阶段自蒸馏训练策略，有效解决了时间视频定位中推理过程缺乏明确约束的问题，提高了模型的性能、可解释性和预测精度。"}}
{"id": "2508.08095", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.08095", "abs": "https://arxiv.org/abs/2508.08095", "authors": ["Chun Wang", "Chenyang Liu", "Wenze Xu", "Weihong Deng"], "title": "Dual Information Speech Language Models for Emotional Conversations", "comment": "Presented at IEEE ICME 2025", "summary": "Conversational systems relying on text-based large language models (LLMs)\noften overlook paralinguistic cues, essential for understanding emotions and\nintentions. Speech-language models (SLMs), which use speech as input, are\nemerging as a promising solution. However, SLMs built by extending frozen LLMs\nstruggle to capture paralinguistic information and exhibit reduced context\nunderstanding. We identify entangled information and improper training\nstrategies as key issues. To address these issues, we propose two heterogeneous\nadapters and suggest a weakly supervised training strategy. Our approach\ndisentangles paralinguistic and linguistic information, enabling SLMs to\ninterpret speech through structured representations. It also preserves\ncontextual understanding by avoiding the generation of task-specific vectors\nthrough controlled randomness. This approach trains only the adapters on common\ndatasets, ensuring parameter and data efficiency. Experiments demonstrate\ncompetitive performance in emotional conversation tasks, showcasing the model's\nability to effectively integrate both paralinguistic and linguistic information\nwithin contextual settings.", "AI": {"tldr": "针对基于文本的大语言模型（LLMs）和现有语音语言模型（SLMs）在理解副语言信息和上下文方面的不足，本文提出使用异构适配器和弱监督训练策略，以解耦语音中的语言和副语言信息，同时保持上下文理解，提高情感对话表现。", "motivation": "基于文本的LLMs忽略了理解情感和意图至关重要的副语言信息。现有通过扩展冻结LLMs构建的SLMs难以捕获副语言信息，并表现出上下文理解能力下降。主要问题在于信息纠缠和不当的训练策略。", "method": "提出两种异构适配器和一种弱监督训练策略。该方法旨在解耦副语言和语言信息，使SLMs能够通过结构化表示解释语音。通过控制随机性避免生成任务特定向量，从而保留上下文理解。仅在通用数据集上训练适配器，实现参数和数据效率。", "result": "实验证明，在情感对话任务中表现出具有竞争力的性能，表明模型能够有效地在上下文环境中整合副语言和语言信息。", "conclusion": "所提出的方法成功解决了现有SLMs在捕获副语言信息和保持上下文理解方面的挑战，通过解耦信息和高效训练，显著提升了模型在情感对话任务中的表现。"}}
{"id": "2508.07217", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07217", "abs": "https://arxiv.org/abs/2508.07217", "authors": ["Yuqi Han", "Qi Cai", "Yuanxin Wu"], "title": "Generic Calibration: Pose Ambiguity/Linear Solution and Parametric-hybrid Pipeline", "comment": null, "summary": "Offline camera calibration techniques typically employ parametric or generic\ncamera models. Selecting parametric models relies heavily on user experience,\nand an inappropriate camera model can significantly affect calibration\naccuracy. Meanwhile, generic calibration methods involve complex procedures and\ncannot provide traditional intrinsic parameters. This paper reveals a pose\nambiguity in the pose solutions of generic calibration methods that\nirreversibly impacts subsequent pose estimation. A linear solver and a\nnonlinear optimization are proposed to address this ambiguity issue. Then a\nglobal optimization hybrid calibration method is introduced to integrate\ngeneric and parametric models together, which improves extrinsic parameter\naccuracy of generic calibration and mitigates overfitting and numerical\ninstability in parametric calibration. Simulation and real-world experimental\nresults demonstrate that the generic-parametric hybrid calibration method\nconsistently excels across various lens types and noise contamination,\nhopefully serving as a reliable and accurate solution for camera calibration in\ncomplex scenarios.", "AI": {"tldr": "本文提出了一种混合相机标定方法，结合了通用模型和参数模型，解决了传统方法中的姿态模糊、精度受限和不稳定性问题，提高了复杂场景下相机标定的准确性和可靠性。", "motivation": "传统的离线相机标定方法存在问题：参数模型选择依赖经验，模型不当会影响精度；通用标定方法过程复杂，无法提供传统内参，且存在姿态模糊性，不可逆地影响后续姿态估计。", "method": "1. 揭示并解决通用标定方法中姿态解的姿态模糊性问题，提出线性求解器和非线性优化方法。2. 引入全局优化混合标定方法，将通用模型和参数模型结合起来。该方法旨在提高通用标定外参精度，并缓解参数标定中的过拟合和数值不稳定性。", "result": "仿真和实际实验结果表明，所提出的通用-参数混合标定方法在各种镜头类型和噪声污染下均表现出色，具有一致的优越性。", "conclusion": "该通用-参数混合标定方法有望成为复杂场景下相机标定的可靠且准确的解决方案。"}}
{"id": "2508.07714", "categories": ["cs.CV", "cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2508.07714", "abs": "https://arxiv.org/abs/2508.07714", "authors": ["Licheng Zhang", "Bach Le", "Naveed Akhtar", "Tuan Ngo"], "title": "DoorDet: Semi-Automated Multi-Class Door Detection Dataset via Object Detection and Large Language Models", "comment": null, "summary": "Accurate detection and classification of diverse door types in floor plans\ndrawings is critical for multiple applications, such as building compliance\nchecking, and indoor scene understanding. Despite their importance, publicly\navailable datasets specifically designed for fine-grained multi-class door\ndetection remain scarce. In this work, we present a semi-automated pipeline\nthat leverages a state-of-the-art object detector and a large language model\n(LLM) to construct a multi-class door detection dataset with minimal manual\neffort. Doors are first detected as a unified category using a deep object\ndetection model. Next, an LLM classifies each detected instance based on its\nvisual and contextual features. Finally, a human-in-the-loop stage ensures\nhigh-quality labels and bounding boxes. Our method significantly reduces\nannotation cost while producing a dataset suitable for benchmarking neural\nmodels in floor plan analysis. This work demonstrates the potential of\ncombining deep learning and multimodal reasoning for efficient dataset\nconstruction in complex real-world domains.", "AI": {"tldr": "本文提出了一种半自动化流程，结合深度学习目标检测模型和大型语言模型（LLM），以最小的人工成本构建用于平面图的多类别门检测数据集。", "motivation": "在建筑合规性检查和室内场景理解等应用中，准确检测和分类平面图中的门类型至关重要。然而，专门用于细粒度多类别门检测的公开数据集非常稀缺。", "method": "该方法采用半自动化流程：首先，使用深度目标检测模型将门统一检测为一个类别；其次，大型语言模型（LLM）根据视觉和上下文特征对每个检测到的实例进行分类；最后，通过人工干预阶段确保标签和边界框的高质量。", "result": "该方法显著降低了标注成本，并生成了一个适合用于基准测试平面图分析中神经网络模型的数据集。", "conclusion": "这项工作展示了结合深度学习和多模态推理在复杂现实世界领域中高效构建数据集的潜力。"}}
{"id": "2508.08096", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.08096", "abs": "https://arxiv.org/abs/2508.08096", "authors": ["Lukas Gehring", "Benjamin Paaßen"], "title": "Assessing LLM Text Detection in Educational Contexts: Does Human Contribution Affect Detection?", "comment": "Preprint as provided by the authors (19 pages, 12 figures, 9 tables)", "summary": "Recent advancements in Large Language Models (LLMs) and their increased\naccessibility have made it easier than ever for students to automatically\ngenerate texts, posing new challenges for educational institutions. To enforce\nnorms of academic integrity and ensure students' learning, learning analytics\nmethods to automatically detect LLM-generated text appear increasingly\nappealing. This paper benchmarks the performance of different state-of-the-art\ndetectors in educational contexts, introducing a novel dataset, called\nGenerative Essay Detection in Education (GEDE), containing over 900\nstudent-written essays and over 12,500 LLM-generated essays from various\ndomains. To capture the diversity of LLM usage practices in generating text, we\npropose the concept of contribution levels, representing students' contribution\nto a given assignment. These levels range from purely human-written texts, to\nslightly LLM-improved versions, to fully LLM-generated texts, and finally to\nactive attacks on the detector by \"humanizing\" generated texts. We show that\nmost detectors struggle to accurately classify texts of intermediate student\ncontribution levels, like LLM-improved human-written texts. Detectors are\nparticularly likely to produce false positives, which is problematic in\neducational settings where false suspicions can severely impact students'\nlives. Our dataset, code, and additional supplementary materials are publicly\navailable at\nhttps://github.com/lukasgehring/Assessing-LLM-Text-Detection-in-Educational-Contexts.", "AI": {"tldr": "该研究评估了大型语言模型（LLM）文本检测器在教育环境中的性能，发现它们难以准确识别混合人机协作生成的文本，且假阳性率高，对学生影响大。", "motivation": "随着LLM的普及，学生自动生成文本变得容易，对学术诚信构成挑战。教育机构需要可靠的自动检测方法来确保学生学习和维护学术规范。", "method": "引入了一个名为GEDE的新数据集，包含900多篇学生原创论文和12,500多篇LLM生成的论文。提出了“贡献水平”概念，涵盖从纯人工到LLM辅助修改，再到完全LLM生成以及主动规避检测的多种文本生成方式，并在此数据集上对现有最先进的检测器进行了基准测试。", "result": "大多数检测器难以准确分类中间贡献水平的文本（例如，经LLM改进的人工文本）。检测器尤其容易产生假阳性，这在教育环境中是一个严重问题，因为错误的怀疑会严重影响学生。", "conclusion": "目前的LLM文本检测器在教育环境中并不可靠，特别是对于混合人机协作的文本和高假阳性率问题。在将这些工具应用于实际教育场景前，需要进一步改进以降低误报率。"}}
{"id": "2508.07225", "categories": ["eess.IV", "cs.CV", "q-bio.QM", "92C40, 68T07", "I.2.10; I.4.8"], "pdf": "https://arxiv.org/pdf/2508.07225", "abs": "https://arxiv.org/abs/2508.07225", "authors": ["Xuepeng Liu", "Zheng Jiang", "Pinan Zhu", "Hanyu Liu", "Chao Li"], "title": "HaDM-ST: Histology-Assisted Differential Modeling for Spatial Transcriptomics Generation", "comment": "10 pages, 5 figures, includes comparisons with TESLA, HiStoGene, and\n  iStar; submitted to arXiv 2025", "summary": "Spatial transcriptomics (ST) reveals spatial heterogeneity of gene\nexpression, yet its resolution is limited by current platforms. Recent methods\nenhance resolution via H&E-stained histology, but three major challenges\npersist: (1) isolating expression-relevant features from visually complex H&E\nimages; (2) achieving spatially precise multimodal alignment in diffusion-based\nframeworks; and (3) modeling gene-specific variation across expression\nchannels. We propose HaDM-ST (Histology-assisted Differential Modeling for ST\nGeneration), a high-resolution ST generation framework conditioned on H&E\nimages and low-resolution ST. HaDM-ST includes: (i) a semantic distillation\nnetwork to extract predictive cues from H&E; (ii) a spatial alignment module\nenforcing pixel-wise correspondence with low-resolution ST; and (iii) a\nchannel-aware adversarial learner for fine-grained gene-level modeling.\nExperiments on 200 genes across diverse tissues and species show HaDM-ST\nconsistently outperforms prior methods, enhancing spatial fidelity and\ngene-level coherence in high-resolution ST predictions.", "AI": {"tldr": "HaDM-ST是一个利用H&E图像和低分辨率空间转录组数据生成高分辨率空间转录组图谱的框架，通过语义提取、空间对齐和通道感知学习克服现有挑战。", "motivation": "当前空间转录组（ST）技术的分辨率受限于平台，尽管现有方法尝试结合H&E染色图像提升分辨率，但仍面临三大挑战：从复杂H&E图像中提取表达相关特征、在基于扩散的框架中实现精确的多模态对齐，以及建模基因特异性变异。", "method": "本文提出了HaDM-ST（Histology-assisted Differential Modeling for ST Generation）框架，包含三个核心模块：(i) 一个语义蒸馏网络，用于从H&E图像中提取预测线索；(ii) 一个空间对齐模块，强制实现与低分辨率ST的像素级对应；(iii) 一个通道感知对抗学习器，用于细粒度的基因水平建模。", "result": "在多种组织和物种的200个基因上的实验表明，HaDM-ST持续优于现有方法，在预测的高分辨率ST中增强了空间保真度和基因水平的一致性。", "conclusion": "HaDM-ST是一个有效的高分辨率空间转录组生成框架，它成功克服了现有方法在利用H&E图像提升ST分辨率方面的挑战，显著提升了预测结果的质量。"}}
{"id": "2508.07766", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07766", "abs": "https://arxiv.org/abs/2508.07766", "authors": ["Jinke Li", "Jiarui Yu", "Chenxing Wei", "Hande Dong", "Qiang Lin", "Liangjing Yang", "Zhicai Wang", "Yanbin Hao"], "title": "UniSVG: A Unified Dataset for Vector Graphic Understanding and Generation with Multimodal Large Language Models", "comment": "Accepted at ACM MM 2025 Dataset Track", "summary": "Unlike bitmap images, scalable vector graphics (SVG) maintain quality when\nscaled, frequently employed in computer vision and artistic design in the\nrepresentation of SVG code. In this era of proliferating AI-powered systems,\nenabling AI to understand and generate SVG has become increasingly urgent.\nHowever, AI-driven SVG understanding and generation (U&G) remain significant\nchallenges. SVG code, equivalent to a set of curves and lines controlled by\nfloating-point parameters, demands high precision in SVG U&G. Besides, SVG\ngeneration operates under diverse conditional constraints, including textual\nprompts and visual references, which requires powerful multi-modal processing\nfor condition-to-SVG transformation. Recently, the rapid growth of Multi-modal\nLarge Language Models (MLLMs) have demonstrated capabilities to process\nmulti-modal inputs and generate complex vector controlling parameters,\nsuggesting the potential to address SVG U&G tasks within a unified model. To\nunlock MLLM's capabilities in the SVG area, we propose an SVG-centric dataset\ncalled UniSVG, comprising 525k data items, tailored for MLLM training and\nevaluation. To our best knowledge, it is the first comprehensive dataset\ndesigned for unified SVG generation (from textual prompts and images) and SVG\nunderstanding (color, category, usage, etc.). As expected, learning on the\nproposed dataset boosts open-source MLLMs' performance on various SVG U&G\ntasks, surpassing SOTA close-source MLLMs like GPT-4V. We release dataset,\nbenchmark, weights, codes and experiment details on\nhttps://ryanlijinke.github.io/.", "AI": {"tldr": "本文提出了一个名为UniSVG的大规模数据集，旨在提升多模态大语言模型（MLLMs）在SVG理解与生成任务上的能力，并实现了超越现有SOTA模型的性能。", "motivation": "SVG在计算机视觉和艺术设计中广泛应用，但AI对SVG的理解和生成（U&G）面临巨大挑战，主要因为SVG代码对浮点参数的精度要求高，且SVG生成需要处理文本提示和视觉参考等多模态约束。尽管MLLMs在处理多模态输入和生成复杂向量参数方面展现出潜力，但缺乏针对SVG领域的统一数据集来释放其能力。", "method": "为了解决SVG U&G的挑战并解锁MLLMs在该领域的能力，研究者提出了一个SVG中心的数据集UniSVG，包含52.5万条数据项。该数据集专为MLLM训练和评估设计，是首个用于统一SVG生成（从文本和图像）和SVG理解（颜色、类别、用途等）的综合数据集。", "result": "在所提出的UniSVG数据集上进行学习后，开源MLLMs在各种SVG理解和生成任务上的性能显著提升，甚至超越了GPT-4V等最先进的闭源MLLMs。", "conclusion": "UniSVG数据集的提出成功地解锁了多模态大语言模型在SVG理解和生成方面的潜力，证明了该数据集在推动AI处理复杂矢量图形任务方面的有效性，并为未来的研究提供了基础资源。"}}
{"id": "2508.08110", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.08110", "abs": "https://arxiv.org/abs/2508.08110", "authors": ["Robin Huo", "Ewan Dunbar"], "title": "Iterative refinement, not training objective, makes HuBERT behave differently from wav2vec 2.0", "comment": "Proceedings of Interspeech 2025", "summary": "Self-supervised models for speech representation learning now see widespread\nuse for their versatility and performance on downstream tasks, but the effect\nof model architecture on the linguistic information learned in their\nrepresentations remains under-studied. This study investigates two such models,\nHuBERT and wav2vec 2.0, and minimally compares two of their architectural\ndifferences: training objective and iterative pseudo-label refinement through\nmultiple training iterations. We find that differences in canonical correlation\nof hidden representations to word identity, phoneme identity, and speaker\nidentity are explained by training iteration, not training objective. We\nsuggest that future work investigate the reason for the effectiveness of\niterative refinement in encoding linguistic information in self-supervised\nspeech representations.", "AI": {"tldr": "本研究探究了自监督语音模型（HuBERT和wav2vec 2.0）中训练目标和迭代伪标签细化对所学语言信息的影响。", "motivation": "自监督语音模型在下游任务中表现出色且应用广泛，但模型架构对其表示中学习到的语言信息的影响尚未得到充分研究。", "method": "通过比较HuBERT和wav2vec 2.0，重点分析了两种架构差异：训练目标和通过多训练迭代进行的迭代伪标签细化。使用规范相关性（CCA）衡量隐藏表示与词汇、音素和说话人身份的关联。", "result": "研究发现，隐藏表示中词汇、音素和说话人身份的规范相关性差异是由训练迭代（即迭代细化）解释的，而非训练目标。", "conclusion": "未来的工作应进一步研究迭代细化在自监督语音表示中编码语言信息的有效性原因。"}}
{"id": "2508.07233", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07233", "abs": "https://arxiv.org/abs/2508.07233", "authors": ["Lei Yang", "Junshan Jin", "Mingyuan Zhang", "Yi He", "Bofan Chen", "Shilin Wang"], "title": "Landmark Guided Visual Feature Extractor for Visual Speech Recognition with Limited Resource", "comment": null, "summary": "Visual speech recognition is a technique to identify spoken content in silent\nspeech videos, which has raised significant attention in recent years.\nAdvancements in data-driven deep learning methods have significantly improved\nboth the speed and accuracy of recognition. However, these deep learning\nmethods can be effected by visual disturbances, such as lightning conditions,\nskin texture and other user-specific features. Data-driven approaches could\nreduce the performance degradation caused by these visual disturbances using\nmodels pretrained on large-scale datasets. But these methods often require\nlarge amounts of training data and computational resources, making them costly.\nTo reduce the influence of user-specific features and enhance performance with\nlimited data, this paper proposed a landmark guided visual feature extractor.\nFacial landmarks are used as auxiliary information to aid in training the\nvisual feature extractor. A spatio-temporal multi-graph convolutional network\nis designed to fully exploit the spatial locations and spatio-temporal features\nof facial landmarks. Additionally, a multi-level lip dynamic fusion framework\nis introduced to combine the spatio-temporal features of the landmarks with the\nvisual features extracted from the raw video frames. Experimental results show\nthat this approach performs well with limited data and also improves the\nmodel's accuracy on unseen speakers.", "AI": {"tldr": "本文提出了一种基于地标引导的视觉特征提取器，结合时空多图卷积网络和多级唇部动态融合框架，以提高视觉语音识别在有限数据和未见说话人上的性能。", "motivation": "现有的深度学习视觉语音识别方法易受视觉干扰（如光照、肤色、用户特定特征）影响，且通常需要大量训练数据和计算资源，成本高昂。研究旨在减少用户特定特征的影响，并在有限数据下提升性能。", "method": "提出地标引导的视觉特征提取器，利用面部地标作为辅助信息。设计了时空多图卷积网络（MGCN）以充分利用面部地标的空间位置和时空特征。引入多级唇部动态融合框架，将地标的时空特征与原始视频帧提取的视觉特征相结合。", "result": "实验结果表明，该方法在有限数据下表现良好，并提高了模型对未见说话人的识别准确性。", "conclusion": "所提出的地标引导方法有效减少了用户特定特征的影响，并在有限数据和未见说话人场景下显著提升了视觉语音识别的性能和准确性。"}}
{"id": "2508.07819", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07819", "abs": "https://arxiv.org/abs/2508.07819", "authors": ["Ke Ma", "Jun Long", "Hongxiao Fei", "Liujie Hua", "Yueyi Luo"], "title": "Architectural Co-Design for Zero-Shot Anomaly Detection: Decoupling Representation and Dynamically Fusing Features in CLIP", "comment": "4 pages, 1 reference, 3 figures, icassp 2026", "summary": "Pre-trained Vision-Language Models (VLMs) face a significant adaptation gap\nwhen applied to Zero-Shot Anomaly Detection (ZSAD), stemming from their lack of\nlocal inductive biases for dense prediction and their reliance on inflexible\nfeature fusion paradigms. We address these limitations through an Architectural\nCo-Design framework that jointly refines feature representation and cross-modal\nfusion. Our method integrates a parameter-efficient Convolutional Low-Rank\nAdaptation (Conv-LoRA) adapter to inject local inductive biases for\nfine-grained representation, and introduces a Dynamic Fusion Gateway (DFG) that\nleverages visual context to adaptively modulate text prompts, enabling a\npowerful bidirectional fusion. Extensive experiments on diverse industrial and\nmedical benchmarks demonstrate superior accuracy and robustness, validating\nthat this synergistic co-design is critical for robustly adapting foundation\nmodels to dense perception tasks.", "AI": {"tldr": "该论文提出了一个架构协同设计框架，通过引入Conv-LoRA和动态融合网关，解决了预训练视觉-语言模型在零样本异常检测中存在的适应性差距，显著提升了性能。", "motivation": "预训练视觉-语言模型（VLMs）在零样本异常检测（ZSAD）中存在显著的适应性差距，原因在于它们缺乏用于密集预测的局部归纳偏置，并且依赖于不灵活的特征融合范式。", "method": "该方法通过一个架构协同设计框架来解决问题，该框架共同优化特征表示和跨模态融合。具体包括：1) 集成参数高效的卷积低秩适应（Conv-LoRA）适配器以注入局部归纳偏置，用于细粒度表示；2) 引入动态融合网关（DFG），利用视觉上下文自适应地调制文本提示，实现强大的双向融合。", "result": "在多样化的工业和医疗基准测试中，该方法展示了卓越的准确性和鲁棒性。", "conclusion": "协同设计对于将基础模型鲁棒地适应密集感知任务至关重要。"}}
{"id": "2508.08125", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.08125", "abs": "https://arxiv.org/abs/2508.08125", "authors": ["Jakub Šmíd", "Pavel Přibáň", "Ondřej Pražák", "Pavel Král"], "title": "Czech Dataset for Complex Aspect-Based Sentiment Analysis Tasks", "comment": "Published In Proceedings of the 2024 Joint International Conference\n  on Computational Linguistics, Language Resources and Evaluation (LREC-COLING\n  2024). Official version: https://aclanthology.org/2024.lrec-main.374/", "summary": "In this paper, we introduce a novel Czech dataset for aspect-based sentiment\nanalysis (ABSA), which consists of 3.1K manually annotated reviews from the\nrestaurant domain. The dataset is built upon the older Czech dataset, which\ncontained only separate labels for the basic ABSA tasks such as aspect term\nextraction or aspect polarity detection. Unlike its predecessor, our new\ndataset is specifically designed for more complex tasks, e.g.\ntarget-aspect-category detection. These advanced tasks require a unified\nannotation format, seamlessly linking sentiment elements (labels) together. Our\ndataset follows the format of the well-known SemEval-2016 datasets. This design\nchoice allows effortless application and evaluation in cross-lingual scenarios,\nultimately fostering cross-language comparisons with equivalent counterpart\ndatasets in other languages. The annotation process engaged two trained\nannotators, yielding an impressive inter-annotator agreement rate of\napproximately 90%. Additionally, we provide 24M reviews without annotations\nsuitable for unsupervised learning. We present robust monolingual baseline\nresults achieved with various Transformer-based models and insightful error\nanalysis to supplement our contributions. Our code and dataset are freely\navailable for non-commercial research purposes.", "AI": {"tldr": "本文介绍了一个新的捷克语方面情感分析（ABSA）数据集，包含3.1K条手动标注的餐厅评论，并提供24M条未标注评论用于无监督学习，同时给出了基于Transformer的单语基线结果。", "motivation": "现有的捷克语ABSA数据集仅包含基本任务的独立标签，无法支持更复杂的ABSA任务（如目标-方面-类别检测）和跨语言研究。因此，需要一个采用统一标注格式的新数据集来弥补这一不足。", "method": "研究者构建了一个包含3.1K条手动标注餐厅评论的新捷克语ABSA数据集，该数据集基于旧数据集，并遵循SemEval-2016的标注格式，以支持复杂任务和跨语言应用。两名训练有素的标注者参与了标注过程，达到了约90%的标注者间一致性。此外，还提供了24M条未标注评论。研究者使用多种基于Transformer的模型进行了单语基线测试，并进行了错误分析。", "result": "成功构建了一个高质量的捷克语ABSA数据集，其统一标注格式支持复杂任务和跨语言比较。标注者间一致性高。提供了强大的单语基线结果和有价值的错误分析。所有代码和数据集均可免费用于非商业研究。", "conclusion": "新引入的捷克语ABSA数据集通过其统一的标注格式和高标注质量，极大地促进了捷克语中更复杂的方面情感分析任务以及与其他语言数据集的跨语言比较研究。"}}
{"id": "2508.07237", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07237", "abs": "https://arxiv.org/abs/2508.07237", "authors": ["Bo Wang", "Mengyuan Xu", "Yue Yan", "Yuqun Yang", "Kechen Shu", "Wei Ping", "Xu Tang", "Wei Jiang", "Zheng You"], "title": "ASM-UNet: Adaptive Scan Mamba Integrating Group Commonalities and Individual Variations for Fine-Grained Segmentation", "comment": null, "summary": "Precise lesion resection depends on accurately identifying fine-grained\nanatomical structures. While many coarse-grained segmentation (CGS) methods\nhave been successful in large-scale segmentation (e.g., organs), they fall\nshort in clinical scenarios requiring fine-grained segmentation (FGS), which\nremains challenging due to frequent individual variations in small-scale\nanatomical structures. Although recent Mamba-based models have advanced medical\nimage segmentation, they often rely on fixed manually-defined scanning orders,\nwhich limit their adaptability to individual variations in FGS. To address\nthis, we propose ASM-UNet, a novel Mamba-based architecture for FGS. It\nintroduces adaptive scan scores to dynamically guide the scanning order,\ngenerated by combining group-level commonalities and individual-level\nvariations. Experiments on two public datasets (ACDC and Synapse) and a newly\nproposed challenging biliary tract FGS dataset, namely BTMS, demonstrate that\nASM-UNet achieves superior performance in both CGS and FGS tasks. Our code and\ndataset are available at https://github.com/YqunYang/ASM-UNet.", "AI": {"tldr": "提出ASM-UNet，一个基于Mamba的自适应扫描UNet架构，通过动态生成扫描顺序来解决医学图像细粒度分割（FGS）中个体差异的挑战，并在粗粒度（CGS）和细粒度分割任务上均表现出色。", "motivation": "现有粗粒度分割（CGS）方法无法满足临床对细粒度分割（FGS）的需求，FGS因小尺度解剖结构的高度个体差异而充满挑战。尽管基于Mamba的模型在医学图像分割中有所进展，但其固定的手动定义扫描顺序限制了对FGS中个体差异的适应性。", "method": "提出ASM-UNet，一种新型的基于Mamba的架构，用于细粒度分割。它引入了自适应扫描分数来动态引导扫描顺序，这些分数通过结合群体层面共性和个体层面变异生成。", "result": "在两个公共数据集（ACDC和Synapse）以及一个新提出的挑战性胆道FGS数据集（BTMS）上的实验表明，ASM-UNet在CGS和FGS任务中均取得了卓越的性能。", "conclusion": "ASM-UNet通过其自适应扫描机制，有效解决了医学图像细粒度分割中个体差异带来的挑战，并在多种分割任务上展现出优越性。"}}
{"id": "2508.07847", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07847", "abs": "https://arxiv.org/abs/2508.07847", "authors": ["Shunya Nagashima", "Komei Sugiura"], "title": "Deep Space Weather Model: Long-Range Solar Flare Prediction from Multi-Wavelength Images", "comment": "ICCV 2025", "summary": "Accurate, reliable solar flare prediction is crucial for mitigating potential\ndisruptions to critical infrastructure, while predicting solar flares remains a\nsignificant challenge. Existing methods based on heuristic physical features\noften lack representation learning from solar images. On the other hand,\nend-to-end learning approaches struggle to model long-range temporal\ndependencies in solar images. In this study, we propose Deep Space Weather\nModel (Deep SWM), which is based on multiple deep state space models for\nhandling both ten-channel solar images and long-range spatio-temporal\ndependencies. Deep SWM also features a sparse masked autoencoder, a novel\npretraining strategy that employs a two-phase masking approach to preserve\ncrucial regions such as sunspots while compressing spatial information.\nFurthermore, we built FlareBench, a new public benchmark for solar flare\nprediction covering a full 11-year solar activity cycle, to validate our\nmethod. Our method outperformed baseline methods and even human expert\nperformance on standard metrics in terms of performance and reliability. The\nproject page can be found at https://keio-smilab25.github.io/DeepSWM.", "AI": {"tldr": "该研究提出了Deep SWM模型，结合深度状态空间模型和稀疏掩码自编码器，以解决太阳耀斑预测中图像表示学习和长时序依赖的挑战。并在新建的FlareBench基准上验证，其性能超越了基线方法和人类专家。", "motivation": "准确可靠的太阳耀斑预测对于减轻对关键基础设施的潜在破坏至关重要。然而，现有基于启发式物理特征的方法缺乏从太阳图像中进行表示学习的能力，而端到端学习方法难以建模太阳图像中的长程时间依赖性。", "method": "提出了Deep SWM（Deep Space Weather Model）模型，该模型基于多个深度状态空间模型，用于处理十通道太阳图像和长程时空依赖。Deep SWM还包含一个稀疏掩码自编码器，采用新颖的两阶段掩码预训练策略，以在压缩空间信息的同时保留关键区域（如太阳黑子）。此外，研究团队构建了FlareBench，一个新的公共太阳耀斑预测基准，涵盖了完整的11年太阳活动周期，用于验证该方法。", "result": "该方法在标准指标上，在性能和可靠性方面均优于基线方法，甚至超越了人类专家的表现。", "conclusion": "Deep SWM为太阳耀斑预测提供了一种更准确、更可靠的解决方案，有效克服了现有方法在图像表示学习和长时序依赖建模方面的局限性。"}}
{"id": "2508.08131", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08131", "abs": "https://arxiv.org/abs/2508.08131", "authors": ["Wenze Xu", "Chun Wang", "Jiazhen Yu", "Sheng Chen", "Liang Gao", "Weihong Deng"], "title": "Optimal Transport Regularization for Speech Text Alignment in Spoken Language Models", "comment": "To be presented at ACPR 2025 Conference", "summary": "Spoken Language Models (SLMs), which extend Large Language Models (LLMs) to\nperceive speech inputs, have gained increasing attention for their potential to\nadvance speech understanding tasks. However, despite recent progress, studies\nshow that SLMs often struggle to generalize across datasets, even for trained\nlanguages and tasks, raising concerns about whether they process speech in a\ntext-like manner as intended. A key challenge underlying this limitation is the\nmodality gap between speech and text representations. The high variability in\nspeech embeddings may allow SLMs to achieve strong in-domain performance by\nexploiting unintended speech variations, ultimately hindering generalization.\nTo mitigate this modality gap, we introduce Optimal Transport Regularization\n(OTReg), a method that formulates speech-text alignment as an optimal transport\nproblem and derives a regularization loss to improve SLM training. In each\ntraining iteration, OTReg first establishes a structured correspondence between\nspeech and transcript embeddings by determining the optimal transport plan,\nthen incorporates the regularization loss based on this transport plan to\noptimize SLMs in generating speech embeddings that align more effectively with\ntranscript embeddings. OTReg is lightweight, requiring no additional labels or\nlearnable parameters, and integrates seamlessly into existing SLM training\nprocedures. Extensive multilingual ASR experiments demonstrate that OTReg\nenhances speech-text alignment, mitigates the modality gap, and consequently\nimproves SLM generalization across diverse datasets.", "AI": {"tldr": "本文提出OTReg方法，通过将语音-文本对齐建模为最优传输问题，引入正则化损失来弥合语音语言模型（SLM）中的模态鸿沟，从而显著提升SLM在多数据集上的泛化能力。", "motivation": "尽管语音语言模型（SLM）在语音理解方面取得了进展，但它们在跨数据集泛化方面表现不佳，即使是针对已训练的语言和任务。这主要是由于语音和文本表示之间的模态鸿沟，语音嵌入的高变异性可能导致SLM利用非预期语音变异来获得域内性能，从而阻碍泛化。", "method": "引入最优传输正则化（OTReg）方法。OTReg将语音-文本对齐表述为最优传输问题，并在每个训练迭代中，首先确定语音和文本嵌入之间的最优传输计划，然后基于此计划计算并引入正则化损失，以优化SLM生成与文本嵌入更有效对齐的语音嵌入。OTReg轻量级，无需额外标签或可学习参数，可无缝集成到现有SLM训练流程中。", "result": "广泛的多语言自动语音识别（ASR）实验表明，OTReg能够增强语音-文本对齐，有效缓解模态鸿沟，并显著提高SLM在不同数据集上的泛化能力。", "conclusion": "OTReg是一种有效且轻量级的方法，通过解决语音-文本模态鸿沟问题，显著提升了语音语言模型的泛化性能。"}}
{"id": "2508.07246", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07246", "abs": "https://arxiv.org/abs/2508.07246", "authors": ["Xin Ma", "Yaohui Wang", "Genyun Jia", "Xinyuan Chen", "Tien-Tsin Wong", "Cunjian Chen"], "title": "Consistent and Controllable Image Animation with Motion Linear Diffusion Transformers", "comment": "Project Page: https://maxin-cn.github.io/miramo_project", "summary": "Image animation has seen significant progress, driven by the powerful\ngenerative capabilities of diffusion models. However, maintaining appearance\nconsistency with static input images and mitigating abrupt motion transitions\nin generated animations remain persistent challenges. While text-to-video (T2V)\ngeneration has demonstrated impressive performance with diffusion transformer\nmodels, the image animation field still largely relies on U-Net-based diffusion\nmodels, which lag behind the latest T2V approaches. Moreover, the quadratic\ncomplexity of vanilla self-attention mechanisms in Transformers imposes heavy\ncomputational demands, making image animation particularly resource-intensive.\nTo address these issues, we propose MiraMo, a framework designed to enhance\nefficiency, appearance consistency, and motion smoothness in image animation.\nSpecifically, MiraMo introduces three key elements: (1) A foundational\ntext-to-video architecture replacing vanilla self-attention with efficient\nlinear attention to reduce computational overhead while preserving generation\nquality; (2) A novel motion residual learning paradigm that focuses on modeling\nmotion dynamics rather than directly predicting frames, improving temporal\nconsistency; and (3) A DCT-based noise refinement strategy during inference to\nsuppress sudden motion artifacts, complemented by a dynamics control module to\nbalance motion smoothness and expressiveness. Extensive experiments against\nstate-of-the-art methods validate the superiority of MiraMo in generating\nconsistent, smooth, and controllable animations with accelerated inference\nspeed. Additionally, we demonstrate the versatility of MiraMo through\napplications in motion transfer and video editing tasks.", "AI": {"tldr": "MiraMo是一个图像动画框架，通过引入高效线性注意力、运动残差学习和DCT噪声优化策略，解决了现有方法在外观一致性、运动平滑度和计算效率方面的挑战，生成了更连贯、平滑和可控的动画。", "motivation": "图像动画在外观一致性、运动过渡平滑性方面仍面临挑战；现有方法多依赖U-Net模型，落后于最新的T2V扩散Transformer；香草自注意力机制的二次复杂度导致计算资源消耗大。", "method": "本文提出了MiraMo框架，包含三个关键要素：1) 采用基于线性注意力而非香草自注意力的基础文本到视频架构，以降低计算开销并保持生成质量；2) 引入新颖的运动残差学习范式，专注于建模运动动态而非直接预测帧，以提高时间一致性；3) 在推理过程中采用基于DCT的噪声细化策略来抑制突然的运动伪影，并辅以动态控制模块来平衡运动平滑度和表现力。", "result": "与现有最先进方法相比，MiraMo在生成一致性、平滑性和可控性动画方面表现出卓越性能，并显著加速了推理速度。此外，MiraMo还在运动迁移和视频编辑任务中展示了其多功能性。", "conclusion": "MiraMo框架通过其创新设计，有效提升了图像动画的效率、外观一致性和运动平滑性，并实现了更好的可控性，解决了当前图像动画领域的主要挑战。"}}
{"id": "2508.07875", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.07875", "abs": "https://arxiv.org/abs/2508.07875", "authors": ["Shuo Han", "Ahmed Karam Eldaly", "Solomon Sunday Oyelere"], "title": "Towards Human-AI Collaboration System for the Detection of Invasive Ductal Carcinoma in Histopathology Images", "comment": null, "summary": "Invasive ductal carcinoma (IDC) is the most prevalent form of breast cancer,\nand early, accurate diagnosis is critical to improving patient survival rates\nby guiding treatment decisions. Combining medical expertise with artificial\nintelligence (AI) holds significant promise for enhancing the precision and\nefficiency of IDC detection. In this work, we propose a human-in-the-loop\n(HITL) deep learning system designed to detect IDC in histopathology images.\nThe system begins with an initial diagnosis provided by a high-performance\nEfficientNetV2S model, offering feedback from AI to the human expert. Medical\nprofessionals then review the AI-generated results, correct any misclassified\nimages, and integrate the revised labels into the training dataset, forming a\nfeedback loop from the human back to the AI. This iterative process refines the\nmodel's performance over time. The EfficientNetV2S model itself achieves\nstate-of-the-art performance compared to existing methods in the literature,\nwith an overall accuracy of 93.65\\%. Incorporating the human-in-the-loop system\nfurther improves the model's accuracy using four experimental groups with\nmisclassified images. These results demonstrate the potential of this\ncollaborative approach to enhance AI performance in diagnostic systems. This\nwork contributes to advancing automated, efficient, and highly accurate methods\nfor IDC detection through human-AI collaboration, offering a promising\ndirection for future AI-assisted medical diagnostics.", "AI": {"tldr": "该研究提出了一个结合高效深度学习模型EfficientNetV2S和人机协作（HITL）的系统，用于检测组织病理图像中的浸润性导管癌（IDC），通过迭代反馈循环持续提升诊断准确性。", "motivation": "浸润性导ctal癌（IDC）是乳腺癌最常见的形式，早期准确诊断对提高患者生存率至关重要。将医学专业知识与人工智能（AI）结合有望提高IDC检测的精度和效率。", "method": "研究提出了一个人机协作（HITL）深度学习系统。系统首先由高性能EfficientNetV2S模型提供初步诊断。医学专家随后审查AI结果，纠正错误分类的图像，并将修订后的标签整合到训练数据集中，形成从人到AI的反馈循环。这个迭代过程旨在随时间推移优化模型性能。", "result": "EfficientNetV2S模型本身在文献中现有方法中达到了最先进的性能，总体准确率为93.65%。引入人机协作系统通过使用四个包含错误分类图像的实验组，进一步提高了模型的准确性。", "conclusion": "这些结果证明了这种协作方法在增强诊断系统中AI性能方面的潜力。这项工作通过人机协作推动了自动化、高效和高准确度的IDC检测方法，为未来AI辅助医疗诊断提供了有希望的方向。"}}
{"id": "2508.08139", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08139", "abs": "https://arxiv.org/abs/2508.08139", "authors": ["Tianyi Zhou", "Johanne Medina", "Sanjay Chawla"], "title": "Can LLMs Detect Their Confabulations? Estimating Reliability in Uncertainty-Aware Language Models", "comment": null, "summary": "Large Language Models (LLMs) are prone to generating fluent but incorrect\ncontent, known as confabulation, which poses increasing risks in multi-turn or\nagentic applications where outputs may be reused as context. In this work, we\ninvestigate how in-context information influences model behavior and whether\nLLMs can identify their unreliable responses. We propose a reliability\nestimation that leverages token-level uncertainty to guide the aggregation of\ninternal model representations. Specifically, we compute aleatoric and\nepistemic uncertainty from output logits to identify salient tokens and\naggregate their hidden states into compact representations for response-level\nreliability prediction. Through controlled experiments on open QA benchmarks,\nwe find that correct in-context information improves both answer accuracy and\nmodel confidence, while misleading context often induces confidently incorrect\nresponses, revealing a misalignment between uncertainty and correctness. Our\nprobing-based method captures these shifts in model behavior and improves the\ndetection of unreliable outputs across multiple open-source LLMs. These results\nunderscore the limitations of direct uncertainty signals and highlight the\npotential of uncertainty-guided probing for reliability-aware generation.", "AI": {"tldr": "该研究探讨了大型语言模型（LLMs）生成错误内容（虚构）的问题，提出了一种基于令牌级不确定性的探测方法来估计响应的可靠性，并发现该方法能有效检测不可靠的输出，尤其是在误导性上下文导致模型自信地给出错误答案时。", "motivation": "大型语言模型容易生成流畅但错误的内容（虚构），这在多轮对话或智能体应用中构成日益增长的风险，因为输出可能被重复用作上下文。研究旨在探究上下文信息如何影响模型行为，以及LLM是否能识别其不可靠的响应。", "method": "提出一种可靠性估计方法，利用令牌级不确定性来指导内部模型表示的聚合。具体来说，从输出logits计算偶然不确定性（aleatoric uncertainty）和认知不确定性（epistemic uncertainty），以识别显著令牌，并将其隐藏状态聚合为紧凑表示，用于响应级别的可靠性预测。", "result": "受控实验表明，正确的上下文信息能提高答案准确性和模型置信度；而误导性上下文常导致模型自信地给出错误答案，揭示了不确定性与正确性之间的错位。所提出的基于探测的方法能捕获这些模型行为的变化，并改进了多种开源LLM对不可靠输出的检测。", "conclusion": "直接的不确定性信号存在局限性，而不确定性引导的探测方法在实现可靠性感知生成方面具有潜力。"}}
{"id": "2508.07250", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07250", "abs": "https://arxiv.org/abs/2508.07250", "authors": ["Fengchao Xiong", "Zhenxing Wu", "Sen Jia", "Yuntao Qian"], "title": "SUIT: Spatial-Spectral Union-Intersection Interaction Network for Hyperspectral Object Tracking", "comment": null, "summary": "Hyperspectral videos (HSVs), with their inherent spatial-spectral-temporal\nstructure, offer distinct advantages in challenging tracking scenarios such as\ncluttered backgrounds and small objects. However, existing methods primarily\nfocus on spatial interactions between the template and search regions, often\noverlooking spectral interactions, leading to suboptimal performance. To\naddress this issue, this paper investigates spectral interactions from both the\narchitectural and training perspectives. At the architectural level, we first\nestablish band-wise long-range spatial relationships between the template and\nsearch regions using Transformers. We then model spectral interactions using\nthe inclusion-exclusion principle from set theory, treating them as the union\nof spatial interactions across all bands. This enables the effective\nintegration of both shared and band-specific spatial cues. At the training\nlevel, we introduce a spectral loss to enforce material distribution alignment\nbetween the template and predicted regions, enhancing robustness to shape\ndeformation and appearance variations. Extensive experiments demonstrate that\nour tracker achieves state-of-the-art tracking performance. The source code,\ntrained models and results will be publicly available via\nhttps://github.com/bearshng/suit to support reproducibility.", "AI": {"tldr": "本文提出一种高光谱视频跟踪方法，通过引入基于Transformer的架构和集合论的包含-排除原则来建模光谱交互，并设计光谱损失以对齐材料分布，从而在复杂场景下实现SOTA性能。", "motivation": "现有高光谱视频跟踪方法主要关注空间交互而忽略光谱交互，导致在杂乱背景和小目标等挑战性场景中性能不佳。", "method": "在架构层面，首先使用Transformer建立模板和搜索区域之间的带间长程空间关系；然后利用集合论的包含-排除原则建模光谱交互，将其视为所有波段空间交互的并集，以有效整合共享和波段特定的空间线索。在训练层面，引入光谱损失以强制模板和预测区域之间的材料分布对齐，增强对形状形变和外观变化的鲁棒性。", "result": "通过广泛实验证明，该跟踪器实现了最先进的跟踪性能。", "conclusion": "所提出的方法通过有效整合光谱交互，显著提升了高光谱视频跟踪的鲁棒性和性能，特别是在挑战性场景下表现优异。"}}
{"id": "2508.07877", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07877", "abs": "https://arxiv.org/abs/2508.07877", "authors": ["WonJun Moon", "Hyun Seok Seong", "Jae-Pil Heo"], "title": "Selective Contrastive Learning for Weakly Supervised Affordance Grounding", "comment": "Accepted to ICCV 2025", "summary": "Facilitating an entity's interaction with objects requires accurately\nidentifying parts that afford specific actions. Weakly supervised affordance\ngrounding (WSAG) seeks to imitate human learning from third-person\ndemonstrations, where humans intuitively grasp functional parts without needing\npixel-level annotations. To achieve this, grounding is typically learned using\na shared classifier across images from different perspectives, along with\ndistillation strategies incorporating part discovery process. However, since\naffordance-relevant parts are not always easily distinguishable, models\nprimarily rely on classification, often focusing on common class-specific\npatterns that are unrelated to affordance. To address this limitation, we move\nbeyond isolated part-level learning by introducing selective prototypical and\npixel contrastive objectives that adaptively learn affordance-relevant cues at\nboth the part and object levels, depending on the granularity of the available\ninformation. Initially, we find the action-associated objects in both\negocentric (object-focused) and exocentric (third-person example) images by\nleveraging CLIP. Then, by cross-referencing the discovered objects of\ncomplementary views, we excavate the precise part-level affordance clues in\neach perspective. By consistently learning to distinguish affordance-relevant\nregions from affordance-irrelevant background context, our approach effectively\nshifts activation from irrelevant areas toward meaningful affordance cues.\nExperimental results demonstrate the effectiveness of our method. Codes are\navailable at github.com/hynnsk/SelectiveCL.", "AI": {"tldr": "该论文提出了一种新的弱监督可供性接地方法，通过引入选择性原型和像素对比学习目标，利用多视角信息在物体和部件层面自适应地学习可供性相关线索，从而更准确地识别功能部件。", "motivation": "现有的弱监督可供性接地（WSAG）方法通常依赖分类器，在可供性相关部件难以区分时，模型容易关注与可供性无关的常见类别特有模式，而非真正的功能性线索。", "method": "该方法引入了选择性原型和像素对比目标，以在部件和物体层面自适应地学习可供性相关线索。具体而言，它首先利用CLIP在第一人称和第三人称图像中识别与动作相关的物体，然后通过交叉引用互补视图中发现的物体，挖掘精确的部件级可供性线索，并持续学习区分可供性相关区域与无关背景。", "result": "实验结果表明，该方法有效，能够将模型的激活从无关区域转移到有意义的可供性线索上。", "conclusion": "该方法通过在部件和物体层面自适应学习可供性相关线索，并有效区分相关与无关区域，显著提高了弱监督可供性接地的准确性，克服了传统方法过度依赖分类的局限性。"}}
{"id": "2508.08140", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.08140", "abs": "https://arxiv.org/abs/2508.08140", "authors": ["Jun Wang", "Zaifu Zhan", "Qixin Zhang", "Mingquan Lin", "Meijia Song", "Rui Zhang"], "title": "Data-Efficient Biomedical In-Context Learning: A Diversity-Enhanced Submodular Perspective", "comment": null, "summary": "Recent progress in large language models (LLMs) has leveraged their\nin-context learning (ICL) abilities to enable quick adaptation to unseen\nbiomedical NLP tasks. By incorporating only a few input-output examples into\nprompts, LLMs can rapidly perform these new tasks. While the impact of these\ndemonstrations on LLM performance has been extensively studied, most existing\napproaches prioritize representativeness over diversity when selecting examples\nfrom large corpora. To address this gap, we propose Dual-Div, a\ndiversity-enhanced data-efficient framework for demonstration selection in\nbiomedical ICL. Dual-Div employs a two-stage retrieval and ranking process:\nFirst, it identifies a limited set of candidate examples from a corpus by\noptimizing both representativeness and diversity (with optional annotation for\nunlabeled data). Second, it ranks these candidates against test queries to\nselect the most relevant and non-redundant demonstrations. Evaluated on three\nbiomedical NLP tasks (named entity recognition (NER), relation extraction (RE),\nand text classification (TC)) using LLaMA 3.1 and Qwen 2.5 for inference, along\nwith three retrievers (BGE-Large, BMRetriever, MedCPT), Dual-Div consistently\noutperforms baselines-achieving up to 5% higher macro-F1 scores-while\ndemonstrating robustness to prompt permutations and class imbalance. Our\nfindings establish that diversity in initial retrieval is more critical than\nranking-stage optimization, and limiting demonstrations to 3-5 examples\nmaximizes performance efficiency.", "AI": {"tldr": "该研究提出了Dual-Div框架，通过在生物医学领域大语言模型(LLMs)的上下文学习(ICL)中优先考虑示例的多样性而非仅仅代表性，显著提升了命名实体识别(NER)、关系抽取(RE)和文本分类(TC)等任务的性能。", "motivation": "现有的大语言模型上下文学习(ICL)示例选择方法在从大型语料库中选择示例时，普遍优先考虑代表性而非多样性，导致了性能上的潜在限制。", "method": "Dual-Div是一个多样性增强的数据高效框架，用于生物医学ICL的示例选择。它采用两阶段检索和排序过程：首先，从语料库中识别有限的候选示例集，同时优化代表性和多样性（可选对未标记数据进行标注）；其次，根据测试查询对这些候选示例进行排名，以选择最相关且非冗余的演示。", "result": "Dual-Div在三个生物医学NLP任务（NER、RE、TC）上，使用LLaMA 3.1和Qwen 2.5进行推理，并结合三种检索器，持续优于基线方法，宏观F1分数最高提升5%。该方法对提示符排列和类别不平衡具有鲁棒性。研究发现，初始检索阶段的多样性比排序阶段的优化更为关键，且将演示示例限制在3-5个可最大化性能效率。", "conclusion": "Dual-Div框架通过在初始检索阶段强调多样性，有效提升了生物医学ICL的性能。研究表明，多样性在示例选择中至关重要，并且少量（3-5个）精心选择的演示示例即可实现最佳性能。"}}
{"id": "2508.07251", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07251", "abs": "https://arxiv.org/abs/2508.07251", "authors": ["Junsheng Huang", "Shengyu Hao", "Bocheng Hu", "Gaoang Wang"], "title": "Understanding Dynamic Scenes in Ego Centric 4D Point Clouds", "comment": null, "summary": "Understanding dynamic 4D scenes from an egocentric perspective-modeling\nchanges in 3D spatial structure over time-is crucial for human-machine\ninteraction, autonomous navigation, and embodied intelligence. While existing\negocentric datasets contain dynamic scenes, they lack unified 4D annotations\nand task-driven evaluation protocols for fine-grained spatio-temporal\nreasoning, especially on motion of objects and human, together with their\ninteractions. To address this gap, we introduce EgoDynamic4D, a novel QA\nbenchmark on highly dynamic scenes, comprising RGB-D video, camera poses,\nglobally unique instance masks, and 4D bounding boxes. We construct 927K QA\npairs accompanied by explicit Chain-of-Thought (CoT), enabling verifiable,\nstep-by-step spatio-temporal reasoning. We design 12 dynamic QA tasks covering\nagent motion, human-object interaction, trajectory prediction, relation\nunderstanding, and temporal-causal reasoning, with fine-grained,\nmultidimensional metrics. To tackle these tasks, we propose an end-to-end\nspatio-temporal reasoning framework that unifies dynamic and static scene\ninformation, using instance-aware feature encoding, time and camera encoding,\nand spatially adaptive down-sampling to compress large 4D scenes into token\nsequences manageable by LLMs. Experiments on EgoDynamic4D show that our method\nconsistently outperforms baselines, validating the effectiveness of multimodal\ntemporal modeling for egocentric dynamic scene understanding.", "AI": {"tldr": "本文提出了EgoDynamic4D，一个针对高度动态场景的4D问答基准，包含统一的4D标注和12种动态QA任务。同时，提出了一种端到端时空推理框架，将4D场景压缩为LLM可处理的token序列，并在基准上取得了优于基线的表现。", "motivation": "现有以自我为中心的动态场景数据集缺乏统一的4D标注和面向任务的评估协议，难以进行精细的时空推理，尤其是在物体和人类运动及其交互方面。", "method": "引入了EgoDynamic4D，一个包含RGB-D视频、相机姿态、全局唯一实例掩码和4D边界框的QA基准。构建了927K个带有思维链（CoT）的QA对。设计了12个动态QA任务，涵盖代理运动、人机交互、轨迹预测、关系理解和时序因果推理，并使用多维度指标进行评估。提出了一个端到端的时空推理框架，通过实例感知特征编码、时间和相机编码以及空间自适应下采样，将大型4D场景压缩为LLM可管理的token序列。", "result": "在EgoDynamic4D上的实验表明，所提出的方法持续优于基线，验证了多模态时间建模对于以自我为中心的动态场景理解的有效性。", "conclusion": "多模态时间建模对于理解以自我为中心的动态场景是有效的。本文提出的EgoDynamic4D基准和时空推理框架为未来在4D动态场景理解领域的研究提供了重要资源和有效方法。"}}
{"id": "2508.07897", "categories": ["cs.CV", "cs.AI", "I.3.3"], "pdf": "https://arxiv.org/pdf/2508.07897", "abs": "https://arxiv.org/abs/2508.07897", "authors": ["Tianle Zeng", "Junlei Hu", "Gerardo Loza Galindo", "Sharib Ali", "Duygu Sarikaya", "Pietro Valdastri", "Dominic Jones"], "title": "NeeCo: Image Synthesis of Novel Instrument States Based on Dynamic and Deformable 3D Gaussian Reconstruction", "comment": "13 pages, 9 figures", "summary": "Computer vision-based technologies significantly enhance surgical automation\nby advancing tool tracking, detection, and localization. However, Current\ndata-driven approaches are data-voracious, requiring large, high-quality\nlabeled image datasets, which limits their application in surgical data\nscience. Our Work introduces a novel dynamic Gaussian Splatting technique to\naddress the data scarcity in surgical image datasets. We propose a dynamic\nGaussian model to represent dynamic surgical scenes, enabling the rendering of\nsurgical instruments from unseen viewpoints and deformations with real tissue\nbackgrounds. We utilize a dynamic training adjustment strategy to address\nchallenges posed by poorly calibrated camera poses from real-world scenarios.\nAdditionally, we propose a method based on dynamic Gaussians for automatically\ngenerating annotations for our synthetic data. For evaluation, we constructed a\nnew dataset featuring seven scenes with 14,000 frames of tool and camera motion\nand tool jaw articulation, with a background of an ex-vivo porcine model. Using\nthis dataset, we synthetically replicate the scene deformation from the ground\ntruth data, allowing direct comparisons of synthetic image quality.\nExperimental results illustrate that our method generates photo-realistic\nlabeled image datasets with the highest values in Peak-Signal-to-Noise Ratio\n(29.87). We further evaluate the performance of medical-specific neural\nnetworks trained on real and synthetic images using an unseen real-world image\ndataset. Our results show that the performance of models trained on synthetic\nimages generated by the proposed method outperforms those trained with\nstate-of-the-art standard data augmentation by 10%, leading to an overall\nimprovement in model performances by nearly 15%.", "AI": {"tldr": "该研究提出了一种动态高斯泼溅技术，用于生成逼真的合成外科手术图像数据和自动标注，以解决外科手术图像数据稀缺问题，并显著提升了医疗神经网络的模型性能。", "motivation": "当前的计算机视觉技术在外科手术自动化中依赖大量高质量的标注图像数据，但外科手术领域的数据稀缺限制了这些技术的应用。", "method": "引入了一种新颖的动态高斯泼溅技术；提出了一个动态高斯模型来表示动态手术场景，以实现从未知视角和变形情况下渲染手术器械及其真实组织背景；采用动态训练调整策略来解决真实场景中相机姿态校准不佳的问题；提出了一种基于动态高斯的方法来自动生成合成数据的标注；构建了一个包含七个场景、14,000帧的新数据集（包含器械和相机运动、器械钳口关节运动，以离体猪模型为背景）进行评估。", "result": "该方法生成的照片级真实感标注图像数据集在峰值信噪比（PSNR）上达到最高值（29.87）。使用该方法生成的合成图像训练的医疗专用神经网络模型性能比使用最先进的标准数据增强方法训练的模型提高了10%，使模型整体性能提升了近15%。", "conclusion": "所提出的动态高斯泼溅方法通过生成高质量的合成数据和自动标注，有效解决了外科图像数据稀缺问题，并显著提升了医疗专用神经网络的性能。"}}
{"id": "2508.08149", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.08149", "abs": "https://arxiv.org/abs/2508.08149", "authors": ["Wentao Jiang", "Xiang Feng", "Zengmao Wang", "Yong Luo", "Pingbo Xu", "Zhe Chen", "Bo Du", "Jing Zhang"], "title": "REX-RAG: Reasoning Exploration with Policy Correction in Retrieval-Augmented Generation", "comment": "17 pages, 4 figures", "summary": "Reinforcement learning (RL) is emerging as a powerful paradigm for enabling\nlarge language models (LLMs) to perform complex reasoning tasks. Recent\nadvances indicate that integrating RL with retrieval-augmented generation (RAG)\nallows LLMs to dynamically incorporate external knowledge, leading to more\ninformed and robust decision making. However, we identify a critical challenge\nduring policy-driven trajectory sampling: LLMs are frequently trapped in\nunproductive reasoning paths, which we refer to as \"dead ends\", committing to\noverconfident yet incorrect conclusions. This severely hampers exploration and\nundermines effective policy optimization. To address this challenge, we propose\nREX-RAG (Reasoning Exploration with Policy Correction in Retrieval-Augmented\nGeneration), a novel framework that explores alternative reasoning paths while\nmaintaining rigorous policy learning through principled distributional\ncorrections. Our approach introduces two key innovations: (1) Mixed Sampling\nStrategy, which combines a novel probe sampling method with exploratory prompts\nto escape dead ends; and (2) Policy Correction Mechanism, which employs\nimportance sampling to correct distribution shifts induced by mixed sampling,\nthereby mitigating gradient estimation bias. We evaluate it on seven\nquestion-answering benchmarks, and the experimental results show that REX-RAG\nachieves average performance gains of 5.1% on Qwen2.5-3B and 3.6% on Qwen2.5-7B\nover strong baselines, demonstrating competitive results across multiple\ndatasets. The code is publicly available at https://github.com/MiliLab/REX-RAG.", "AI": {"tldr": "REX-RAG是一个新颖的框架，通过混合采样策略和策略校正机制，帮助大型语言模型（LLMs）在强化学习（RL）与检索增强生成（RAG）结合的复杂推理任务中，避免陷入“死胡同”般的无效推理路径，从而提高推理性能。", "motivation": "在RL-RAG范式中，LLMs在策略驱动的轨迹采样过程中，经常陷入低效的推理路径（“死胡同”），导致过度自信的错误结论，严重阻碍了探索并削弱了有效的策略优化。", "method": "提出了REX-RAG框架，包含两项关键创新：1) 混合采样策略：结合新颖的探测采样方法和探索性提示，以摆脱“死胡同”。2) 策略校正机制：利用重要性采样来纠正混合采样引起的分布偏移，从而减轻梯度估计偏差。", "result": "在七个问答基准上进行评估，REX-RAG在Qwen2.5-3B上平均性能提升5.1%，在Qwen2.5-7B上平均性能提升3.6%，超越了强基线，在多个数据集上展现出有竞争力的结果。", "conclusion": "REX-RAG通过探索替代推理路径和严格的策略学习，有效解决了LLMs在RL-RAG中面临的“死胡同”问题，显著提升了LLMs在复杂推理任务中的表现和鲁棒性。"}}
{"id": "2508.07260", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07260", "abs": "https://arxiv.org/abs/2508.07260", "authors": ["Sihan Yang", "Huitong Ji", "Shaolin Lu", "Jiayi Chen", "Binxiao Xu", "Ming Lu", "Yuanxing Zhang", "Wenhui Dong", "Wentao Zhang"], "title": "Small-Large Collaboration: Training-efficient Concept Personalization for Large VLM using a Meta Personalized Small VLM", "comment": null, "summary": "Personalizing Vision-Language Models (VLMs) to transform them into daily\nassistants has emerged as a trending research direction. However, leading\ncompanies like OpenAI continue to increase model size and develop complex\ndesigns such as the chain of thought (CoT). While large VLMs are proficient in\ncomplex multi-modal understanding, their high training costs and limited access\nvia paid APIs restrict direct personalization. Conversely, small VLMs are\neasily personalized and freely available, but they lack sufficient reasoning\ncapabilities. Inspired by this, we propose a novel collaborative framework\nnamed Small-Large Collaboration (SLC) for large VLM personalization, where the\nsmall VLM is responsible for generating personalized information, while the\nlarge model integrates this personalized information to deliver accurate\nresponses. To effectively incorporate personalized information, we develop a\ntest-time reflection strategy, preventing the potential hallucination of the\nsmall VLM. Since SLC only needs to train a meta personalized small VLM for the\nlarge VLMs, the overall process is training-efficient. To the best of our\nknowledge, this is the first training-efficient framework that supports both\nopen-source and closed-source large VLMs, enabling broader real-world\npersonalized applications. We conduct thorough experiments across various\nbenchmarks and large VLMs to demonstrate the effectiveness of the proposed SLC\nframework. The code will be released at https://github.com/Hhankyangg/SLC.", "AI": {"tldr": "提出了一种名为SLC（Small-Large Collaboration）的训练高效框架，通过小型VLM生成个性化信息并由大型VLM整合，实现对大型视觉-语言模型（VLM）的个性化，同时通过测试时反射策略抑制小型VLM的幻觉。", "motivation": "大型VLM虽然具有强大的多模态理解能力，但训练成本高昂且访问受限，难以直接进行个性化。小型VLM易于个性化且免费可用，但推理能力不足。因此，需要一种高效且可行的框架来个性化大型VLM。", "method": "SLC框架的核心思想是小模型与大模型的协同。小型VLM负责生成个性化信息，大型VLM则整合这些信息以提供准确响应。为确保个性化信息的质量，引入了测试时反射策略，以防止小型VLM产生幻觉。整个过程仅需训练一个元个性化小型VLM，从而实现训练高效。", "result": "SLC框架被证明是训练高效的，并且支持对开源和闭源大型VLM的个性化，从而能够应用于更广泛的真实世界个性化场景。在各种基准测试和大型VLM上的实验均验证了所提出SLC框架的有效性。", "conclusion": "SLC是首个训练高效的框架，能够支持开源和闭源大型VLM的个性化，为VLM在实际应用中实现广泛的个性化提供了可能。"}}
{"id": "2508.07903", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07903", "abs": "https://arxiv.org/abs/2508.07903", "authors": ["Johanna P. Müller", "Anika Knupfer", "Pedro Blöss", "Edoardo Berardi Vittur", "Bernhard Kainz", "Jana Hutter"], "title": "Diffusing the Blind Spot: Uterine MRI Synthesis with Diffusion Models", "comment": "Accepted at MICCAI CAPI 2025", "summary": "Despite significant progress in generative modelling, existing diffusion\nmodels often struggle to produce anatomically precise female pelvic images,\nlimiting their application in gynaecological imaging, where data scarcity and\npatient privacy concerns are critical. To overcome these barriers, we introduce\na novel diffusion-based framework for uterine MRI synthesis, integrating both\nunconditional and conditioned Denoising Diffusion Probabilistic Models (DDPMs)\nand Latent Diffusion Models (LDMs) in 2D and 3D. Our approach generates\nanatomically coherent, high fidelity synthetic images that closely mimic real\nscans and provide valuable resources for training robust diagnostic models. We\nevaluate generative quality using advanced perceptual and distributional\nmetrics, benchmarking against standard reconstruction methods, and demonstrate\nsubstantial gains in diagnostic accuracy on a key classification task. A\nblinded expert evaluation further validates the clinical realism of our\nsynthetic images. We release our models with privacy safeguards and a\ncomprehensive synthetic uterine MRI dataset to support reproducible research\nand advance equitable AI in gynaecology.", "AI": {"tldr": "该研究引入了一种新的扩散模型框架，用于生成解剖学精确的女性盆腔MRI图像，以解决妇科影像学中数据稀缺和隐私问题。", "motivation": "尽管生成模型取得了显著进展，但现有扩散模型在生成解剖学精确的女性盆腔图像方面表现不佳，这限制了它们在妇科影像学中的应用，而妇科影像学面临数据稀缺和患者隐私的关键问题。", "method": "引入了一种新颖的基于扩散的子宫MRI合成框架，该框架集成了2D和3D的无条件和条件去噪扩散概率模型（DDPMs）以及潜在扩散模型（LDMs）。", "result": "该方法生成了与真实扫描高度相似的解剖学连贯、高保真合成图像，为训练鲁棒的诊断模型提供了宝贵资源。通过先进的感知和分布度量评估了生成质量，并与标准重建方法进行基准测试，在关键分类任务上显示出诊断准确性的大幅提高。一项盲法专家评估进一步验证了合成图像的临床真实性。", "conclusion": "该框架能够生成高质量、临床真实的合成子宫MRI图像，有效解决了妇科影像学中的数据挑战，并支持可重复研究和妇科领域公平AI的发展。"}}
{"id": "2508.08163", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.08163", "abs": "https://arxiv.org/abs/2508.08163", "authors": ["Mandira Sawkar", "Samay U. Shetty", "Deepak Pandita", "Tharindu Cyril Weerasooriya", "Christopher M. Homan"], "title": "LPI-RIT at LeWiDi-2025: Improving Distributional Predictions via Metadata and Loss Reweighting with DisCo", "comment": null, "summary": "The Learning With Disagreements (LeWiDi) 2025 shared task is to model\nannotator disagreement through soft label distribution prediction and\nperspectivist evaluation, modeling annotators. We adapt DisCo (Distribution\nfrom Context), a neural architecture that jointly models item-level and\nannotator-level label distributions, and present detailed analysis and\nimprovements. In this paper, we extend the DisCo by incorporating annotator\nmetadata, enhancing input representations, and modifying the loss functions to\ncapture disagreement patterns better. Through extensive experiments, we\ndemonstrate substantial improvements in both soft and perspectivist evaluation\nmetrics across three datasets. We also conduct in-depth error and calibration\nanalyses, highlighting the conditions under which improvements occur. Our\nfindings underscore the value of disagreement-aware modeling and offer insights\ninto how system components interact with the complexity of human-annotated\ndata.", "AI": {"tldr": "本文通过引入标注者元数据、增强输入表示和修改损失函数，改进了DisCo模型以更好地预测标注者分歧，并在多项评估指标上取得了显著提升。", "motivation": "研究动机是参与LeWiDi 2025共享任务，通过预测软标签分布和视角主义评估来建模标注者分歧，并希望改进现有模型（如DisCo）以更有效地捕获这种分歧模式。", "method": "方法是扩展了DisCo（Distribution from Context）神经网络架构，具体包括：1) 整合标注者元数据；2) 增强输入表示；3) 修改损失函数以更好地捕捉分歧模式。通过广泛的实验、深入的错误分析和校准分析进行评估。", "result": "结果显示，在三个数据集上，软评估和视角主义评估指标均取得了显著改进。此外，错误和校准分析揭示了改进发生的条件，并提供了系统组件与人类标注数据复杂性交互的见解。", "conclusion": "研究结论强调了分歧感知建模的价值，并提供了关于系统组件如何与人类标注数据的复杂性相互作用的深入见解，证明了所提出改进的有效性。"}}
{"id": "2508.07298", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07298", "abs": "https://arxiv.org/abs/2508.07298", "authors": ["Zhiqiang Shen", "Peng Cao", "Xiaoli Liu", "Jinzhu Yang", "Osmar R. Zaiane"], "title": "SynMatch: Rethinking Consistency in Medical Image Segmentation with Sparse Annotations", "comment": null, "summary": "Label scarcity remains a major challenge in deep learning-based medical image\nsegmentation. Recent studies use strong-weak pseudo supervision to leverage\nunlabeled data. However, performance is often hindered by inconsistencies\nbetween pseudo labels and their corresponding unlabeled images. In this work,\nwe propose \\textbf{SynMatch}, a novel framework that sidesteps the need for\nimproving pseudo labels by synthesizing images to match them instead.\nSpecifically, SynMatch synthesizes images using texture and shape features\nextracted from the same segmentation model that generates the corresponding\npseudo labels for unlabeled images. This design enables the generation of\nhighly consistent synthesized-image-pseudo-label pairs without requiring any\ntraining parameters for image synthesis. We extensively evaluate SynMatch\nacross diverse medical image segmentation tasks under semi-supervised learning\n(SSL), weakly-supervised learning (WSL), and barely-supervised learning (BSL)\nsettings with increasingly limited annotations. The results demonstrate that\nSynMatch achieves superior performance, especially in the most challenging BSL\nsetting. For example, it outperforms the recent strong-weak pseudo\nsupervision-based method by 29.71\\% and 10.05\\% on the polyp segmentation task\nwith 5\\% and 10\\% scribble annotations, respectively. The code will be released\nat https://github.com/Senyh/SynMatch.", "AI": {"tldr": "SynMatch是一种新颖的半监督医学图像分割框架，通过合成与伪标签一致的图像来避免改进伪标签的需要，从而在标签稀缺设置下表现出色。", "motivation": "深度学习医学图像分割面临标签稀缺的挑战，而现有强弱伪监督方法常因伪标签与未标注图像之间存在不一致性而性能受限。", "method": "SynMatch提出不改进伪标签，而是合成图像来匹配伪标签。它利用生成伪标签的同一分割模型中提取的纹理和形状特征来合成图像，从而生成高度一致的合成图像-伪标签对，且图像合成无需额外训练参数。", "result": "SynMatch在半监督学习（SSL）、弱监督学习（WSL）和极少监督学习（BSL）等不同医学图像分割任务中均表现出卓越性能，尤其在最具挑战性的BSL设置下。例如，在息肉分割任务中，使用5%和10%涂鸦标注时，它分别比现有强弱伪监督方法高出29.71%和10.05%。", "conclusion": "SynMatch通过合成与伪标签匹配的图像，有效解决了标签稀缺下伪监督学习中伪标签不一致的问题，在各种有限标注设置下，特别是极少监督学习中，显著提升了医学图像分割性能。"}}
{"id": "2508.07981", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07981", "abs": "https://arxiv.org/abs/2508.07981", "authors": ["Fangyuan Mao", "Aiming Hao", "Jintao Chen", "Dongxia Liu", "Xiaokun Feng", "Jiashu Zhu", "Meiqi Wu", "Chubin Chen", "Jiahong Wu", "Xiangxiang Chu"], "title": "Omni-Effects: Unified and Spatially-Controllable Visual Effects Generation", "comment": null, "summary": "Visual effects (VFX) are essential visual enhancements fundamental to modern\ncinematic production. Although video generation models offer cost-efficient\nsolutions for VFX production, current methods are constrained by per-effect\nLoRA training, which limits generation to single effects. This fundamental\nlimitation impedes applications that require spatially controllable composite\neffects, i.e., the concurrent generation of multiple effects at designated\nlocations. However, integrating diverse effects into a unified framework faces\nmajor challenges: interference from effect variations and spatial\nuncontrollability during multi-VFX joint training. To tackle these challenges,\nwe propose Omni-Effects, a first unified framework capable of generating\nprompt-guided effects and spatially controllable composite effects. The core of\nour framework comprises two key innovations: (1) LoRA-based Mixture of Experts\n(LoRA-MoE), which employs a group of expert LoRAs, integrating diverse effects\nwithin a unified model while effectively mitigating cross-task interference.\n(2) Spatial-Aware Prompt (SAP) incorporates spatial mask information into the\ntext token, enabling precise spatial control. Furthermore, we introduce an\nIndependent-Information Flow (IIF) module integrated within the SAP, isolating\nthe control signals corresponding to individual effects to prevent any unwanted\nblending. To facilitate this research, we construct a comprehensive VFX dataset\nOmni-VFX via a novel data collection pipeline combining image editing and\nFirst-Last Frame-to-Video (FLF2V) synthesis, and introduce a dedicated VFX\nevaluation framework for validating model performance. Extensive experiments\ndemonstrate that Omni-Effects achieves precise spatial control and diverse\neffect generation, enabling users to specify both the category and location of\ndesired effects.", "AI": {"tldr": "提出Omni-Effects，一个统一的视觉特效（VFX）生成框架，能够生成提示词引导和空间可控的复合特效，克服了现有单特效模型的局限性。", "motivation": "现有VFX视频生成模型受限于每个特效的LoRA训练，只能生成单一特效，无法实现空间可控的复合特效。多VFX联合训练存在特效变体干扰和空间不可控性等挑战。", "method": "提出了Omni-Effects统一框架，核心创新包括：1) LoRA专家混合模型（LoRA-MoE），集成多样特效并缓解交叉任务干扰；2) 空间感知提示（SAP），将空间掩码信息融入文本标记以实现精确空间控制，并集成独立信息流（IIF）模块隔离个体特效控制信号。此外，构建了Omni-VFX数据集并引入了专门的VFX评估框架。", "result": "Omni-Effects实现了精确的空间控制和多样的特效生成，用户可以指定所需特效的类别和位置。", "conclusion": "Omni-Effects是首个能够生成提示词引导和空间可控复合特效的统一框架，有效解决了现有方法的局限性，为VFX制作提供了更灵活的解决方案。"}}
{"id": "2508.08192", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.08192", "abs": "https://arxiv.org/abs/2508.08192", "authors": ["Bangsheng Tang", "Carl Chengyan Fu", "Fei Kou", "Grigory Sizov", "Haoci Zhang", "Jason Park", "Jiawen Liu", "Jie You", "Qirui Yang", "Sachin Mehta", "Shengyong Cai", "Xiaodong Wang", "Xingyu Liu", "Yunlu Li", "Yanjun Zhou", "Wei Wei", "Zhiwei Zhao", "Zixi Qi", "Adolfo Victoria", "Aya Ibrahim", "Bram Wasti", "Changkyu Kim", "Daniel Haziza", "Fei Sun", "Giancarlo Delfin", "Emily Guo", "Jialin Ouyang", "Jaewon Lee", "Jianyu Huang", "Jeremy Reizenstein", "Lu Fang", "Quinn Zhu", "Ria Verma", "Vlad Mihailescu", "Xingwen Guo", "Yan Cui", "Ye Hu", "Yejin Lee"], "title": "Efficient Speculative Decoding for Llama at Scale: Challenges and Solutions", "comment": "15 pages", "summary": "Speculative decoding is a standard method for accelerating the inference\nspeed of large language models. However, scaling it for production environments\nposes several engineering challenges, including efficiently implementing\ndifferent operations (e.g., tree attention and multi-round speculative\ndecoding) on GPU. In this paper, we detail the training and inference\noptimization techniques that we have implemented to enable EAGLE-based\nspeculative decoding at a production scale for Llama models. With these\nchanges, we achieve a new state-of-the-art inference latency for Llama models.\nFor example, Llama4 Maverick decodes at a speed of about 4 ms per token (with a\nbatch size of one) on 8 NVIDIA H100 GPUs, which is 10% faster than the\npreviously best known method. Furthermore, for EAGLE-based speculative\ndecoding, our optimizations enable us to achieve a speed-up for large batch\nsizes between 1.4x and 2.0x at production scale.", "AI": {"tldr": "本文详述了针对Llama模型在生产环境中实现EAGLE推测解码的训练和推理优化技术，显著提升了推理速度并达到了新的SOTA水平。", "motivation": "将推测解码扩展到生产环境面临工程挑战，特别是如何在GPU上高效实现各种操作（如树注意力、多轮推测解码）。", "method": "通过实施训练和推理优化技术，使得Llama模型能够进行EAGLE推测解码，并解决了生产规模下的效率问题，包括在GPU上高效实现树注意力（tree attention）和多轮推测解码（multi-round speculative decoding）。", "result": "Llama模型实现了新的SOTA推理延迟。例如，Llama4 Maverick在8个NVIDIA H100 GPU上以约4毫秒/token（批量大小为1）的速度解码，比现有最佳方法快10%。此外，对于EAGLE推测解码，在生产规模下实现了1.4倍到2.0倍的大批量加速。", "conclusion": "所实施的优化技术成功地在生产规模上实现了基于EAGLE的推测解码，显著提高了Llama模型的推理速度，并为大批量处理带来了显著加速。"}}
{"id": "2508.07300", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07300", "abs": "https://arxiv.org/abs/2508.07300", "authors": ["Ping-Mao Huang", "I-Tien Chao", "Ping-Chia Huang", "Jia-Wei Liao", "Yung-Yu Chuang"], "title": "BEVANet: Bilateral Efficient Visual Attention Network for Real-Time Semantic Segmentation", "comment": "Copyright 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "Real-time semantic segmentation presents the dual challenge of designing\nefficient architectures that capture large receptive fields for semantic\nunderstanding while also refining detailed contours. Vision transformers model\nlong-range dependencies effectively but incur high computational cost. To\naddress these challenges, we introduce the Large Kernel Attention (LKA)\nmechanism. Our proposed Bilateral Efficient Visual Attention Network (BEVANet)\nexpands the receptive field to capture contextual information and extracts\nvisual and structural features using Sparse Decomposed Large Separable Kernel\nAttentions (SDLSKA). The Comprehensive Kernel Selection (CKS) mechanism\ndynamically adapts the receptive field to further enhance performance.\nFurthermore, the Deep Large Kernel Pyramid Pooling Module (DLKPPM) enriches\ncontextual features by synergistically combining dilated convolutions and large\nkernel attention. The bilateral architecture facilitates frequent branch\ncommunication, and the Boundary Guided Adaptive Fusion (BGAF) module enhances\nboundary delineation by integrating spatial and semantic features under\nboundary guidance. BEVANet achieves real-time segmentation at 33 FPS, yielding\n79.3% mIoU without pretraining and 81.0% mIoU on Cityscapes after ImageNet\npretraining, demonstrating state-of-the-art performance. The code and model is\navailable at https://github.com/maomao0819/BEVANet.", "AI": {"tldr": "本文提出了BEVANet，一个双边高效视觉注意力网络，通过大核注意力机制和双边架构，在实时语义分割任务中实现了高精度和高效率的平衡。", "motivation": "实时语义分割面临效率与捕捉大感受野、细化细节轮廓的双重挑战。视觉Transformer虽然能有效建模长程依赖，但计算成本高昂。", "method": "引入大核注意力（LKA）机制；提出BEVANet，利用稀疏分解大可分离核注意力（SDLSKA）扩展感受野并提取特征；通过综合核选择（CKS）动态调整感受野；深度大核金字塔池化模块（DLKPPM）结合空洞卷积和LKA丰富上下文特征；双边架构促进分支通信；边界引导自适应融合（BGAF）模块增强边界描绘。", "result": "BEVANet在Cityscapes数据集上实现了33 FPS的实时分割速度，未预训练时mIoU为79.3%，ImageNet预训练后mIoU达到81.0%，展现了最先进的性能。", "conclusion": "BEVANet通过创新的大核注意力机制和双边架构，成功解决了实时语义分割中效率与精度之间的矛盾，取得了领先的成果。"}}
{"id": "2508.08066", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.08066", "abs": "https://arxiv.org/abs/2508.08066", "authors": ["Weitai Kang", "Weiming Zhuang", "Zhizhong Li", "Yan Yan", "Lingjuan Lyu"], "title": "Investigating the Design Space of Visual Grounding in Multimodal Large Language Model", "comment": "8 pages for the main paper", "summary": "Fine-grained multimodal capability in Multimodal Large Language Models\n(MLLMs) has emerged as a critical research direction, particularly for tackling\nthe visual grounding (VG) problem. Despite the strong performance achieved by\nexisting approaches, they often employ disparate design choices when\nfine-tuning MLLMs for VG, lacking systematic verification to support these\ndesigns. To bridge this gap, this paper presents a comprehensive study of\nvarious design choices that impact the VG performance of MLLMs. We conduct our\nanalysis using LLaVA-1.5, which has been widely adopted in prior empirical\nstudies of MLLMs. While more recent models exist, we follow this convention to\nensure our findings remain broadly applicable and extendable to other\narchitectures. We cover two key aspects: (1) exploring different visual\ngrounding paradigms in MLLMs, identifying the most effective design, and\nproviding our insights; and (2) conducting ablation studies on the design of\ngrounding data to optimize MLLMs' fine-tuning for the VG task. Finally, our\nfindings contribute to a stronger MLLM for VG, achieving improvements of +5.6%\n/ +6.9% / +7.0% on RefCOCO/+/g over the LLaVA-1.5.", "AI": {"tldr": "本文系统性地研究了影响多模态大语言模型（MLLMs）视觉定位（VG）性能的各种设计选择，并在此基础上提出了一种更强大的VG MLLM。", "motivation": "尽管现有MLLM在视觉定位任务上表现良好，但它们在微调时常采用不同的设计选择，缺乏系统性的验证来支持这些设计，导致性能提升的潜在瓶颈。", "method": "研究基于LLaVA-1.5模型，对两个关键方面进行了分析：1) 探索不同的MLLM视觉定位范式并识别最有效的设计；2) 对定位数据设计进行消融研究以优化MLLM在VG任务上的微调。", "result": "通过系统性研究，本文提出了更强的MLLM视觉定位方法，在RefCOCO/+/g数据集上相对于LLaVA-1.5分别取得了+5.6% / +6.9% / +7.0%的性能提升。", "conclusion": "本文的发现有助于构建更强大的MLLM视觉定位能力，并通过系统性验证为未来MLLM在VG任务上的设计提供了有价值的见解和优化方向。"}}
{"id": "2508.08204", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08204", "abs": "https://arxiv.org/abs/2508.08204", "authors": ["Kyle Moore", "Jesse Roberts", "Daryl Watson"], "title": "Human-Alignment and Calibration of Inference-Time Uncertainty in Large Language Models", "comment": "preprint, under review", "summary": "There has been much recent interest in evaluating large language models for\nuncertainty calibration to facilitate model control and modulate user trust.\nInference time uncertainty, which may provide a real-time signal to the model\nor external control modules, is particularly important for applying these\nconcepts to improve LLM-user experience in practice. While many of the existing\npapers consider model calibration, comparatively little work has sought to\nevaluate how closely model uncertainty aligns to human uncertainty. In this\nwork, we evaluate a collection of inference-time uncertainty measures, using\nboth established metrics and novel variations, to determine how closely they\nalign with both human group-level uncertainty and traditional notions of model\ncalibration. We find that numerous measures show evidence of strong alignment\nto human uncertainty, even despite the lack of alignment to human answer\npreference. For those successful metrics, we find moderate to strong evidence\nof model calibration in terms of both correctness correlation and\ndistributional analysis.", "AI": {"tldr": "该研究评估了大型语言模型（LLM）的推理时不确定性度量，发现许多度量与人类不确定性高度一致，并且表现出良好的模型校准，这对于提升用户体验至关重要。", "motivation": "当前对大型语言模型的不确定性校准评估大多侧重于模型校准本身，而较少关注模型不确定性与人类不确定性的对齐程度。然而，推理时（实时）的不确定性信号对于模型控制和用户信任至关重要，能实际改善LLM的用户体验。", "method": "研究评估了一系列推理时（inference-time）不确定性度量，使用了现有指标和新颖的变体。评估标准包括这些度量与人类群体层面不确定性的对齐程度，以及与传统模型校准概念的对齐程度。", "result": "研究发现，尽管模型不确定性与人类答案偏好不一致，但许多不确定性度量仍显示出与人类不确定性的强烈对齐。对于那些成功的度量，在正确性相关性和分布分析方面，都发现了中度到强度的模型校准证据。", "conclusion": "推理时LLM不确定性度量能够有效且强烈地对齐人类不确定性，并且同时表现出良好的模型校准。这表明这些不确定性信号可以作为提升LLM-用户体验的有效手段，有助于模型控制和用户信任的建立。"}}
{"id": "2508.07312", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07312", "abs": "https://arxiv.org/abs/2508.07312", "authors": ["Min Yang", "Zihan Jia", "Zhilin Dai", "Sheng Guo", "Limin Wang"], "title": "MobileViCLIP: An Efficient Video-Text Model for Mobile Devices", "comment": "Accepted by ICCV2025", "summary": "Efficient lightweight neural networks are with increasing attention due to\ntheir faster reasoning speed and easier deployment on mobile devices. However,\nexisting video pre-trained models still focus on the common ViT architecture\nwith high latency, and few works attempt to build efficient architecture on\nmobile devices. This paper bridges this gap by introducing temporal structural\nreparameterization into an efficient image-text model and training it on a\nlarge-scale high-quality video-text dataset, resulting in an efficient\nvideo-text model that can run on mobile devices with strong zero-shot\nclassification and retrieval capabilities, termed as MobileViCLIP. In\nparticular, in terms of inference speed on mobile devices, our\nMobileViCLIP-Small is 55.4x times faster than InternVideo2-L14 and 6.7x faster\nthan InternVideo2-S14. In terms of zero-shot retrieval performance, our\nMobileViCLIP-Small obtains similar performance as InternVideo2-L14 and obtains\n6.9\\% better than InternVideo2-S14 on MSR-VTT. The code is available at\nhttps://github.com/MCG-NJU/MobileViCLIP.", "AI": {"tldr": "MobileViCLIP是一个高效的视频-文本模型，通过引入时间结构重参数化，使其能在移动设备上快速运行并实现强大的零样本分类和检索能力。", "motivation": "现有视频预训练模型主要基于高延迟的ViT架构，不适用于移动设备，缺乏针对移动端的高效架构。", "method": "将时间结构重参数化引入到一个高效的图像-文本模型中，并在大规模高质量视频-文本数据集上进行训练，从而构建出MobileViCLIP。", "result": "在移动设备推理速度方面，MobileViCLIP-Small比InternVideo2-L14快55.4倍，比InternVideo2-S14快6.7倍。在零样本检索性能上，MobileViCLIP-Small与InternVideo2-L14相当，并在MSR-VTT数据集上比InternVideo2-S14高6.9%。", "conclusion": "MobileViCLIP成功弥补了现有模型在移动设备上效率低下的不足，提供了一个兼具速度和性能的视频-文本模型，适用于移动端部署。"}}
{"id": "2508.08107", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08107", "abs": "https://arxiv.org/abs/2508.08107", "authors": ["Danfeng Hong", "Chenyu Li", "Naoto Yokoya", "Bing Zhang", "Xiuping Jia", "Antonio Plaza", "Paolo Gamba", "Jon Atli Benediktsson", "Jocelyn Chanussot"], "title": "Hyperspectral Imaging", "comment": null, "summary": "Hyperspectral imaging (HSI) is an advanced sensing modality that\nsimultaneously captures spatial and spectral information, enabling\nnon-invasive, label-free analysis of material, chemical, and biological\nproperties. This Primer presents a comprehensive overview of HSI, from the\nunderlying physical principles and sensor architectures to key steps in data\nacquisition, calibration, and correction. We summarize common data structures\nand highlight classical and modern analysis methods, including dimensionality\nreduction, classification, spectral unmixing, and AI-driven techniques such as\ndeep learning. Representative applications across Earth observation, precision\nagriculture, biomedicine, industrial inspection, cultural heritage, and\nsecurity are also discussed, emphasizing HSI's ability to uncover sub-visual\nfeatures for advanced monitoring, diagnostics, and decision-making. Persistent\nchallenges, such as hardware trade-offs, acquisition variability, and the\ncomplexity of high-dimensional data, are examined alongside emerging solutions,\nincluding computational imaging, physics-informed modeling, cross-modal fusion,\nand self-supervised learning. Best practices for dataset sharing,\nreproducibility, and metadata documentation are further highlighted to support\ntransparency and reuse. Looking ahead, we explore future directions toward\nscalable, real-time, and embedded HSI systems, driven by sensor\nminiaturization, self-supervised learning, and foundation models. As HSI\nevolves into a general-purpose, cross-disciplinary platform, it holds promise\nfor transformative applications in science, technology, and society.", "AI": {"tldr": "本综述全面概述了高光谱成像（HSI）技术，包括其原理、数据处理、分析方法、应用、挑战及未来发展方向。", "motivation": "高光谱成像（HSI）是一种先进的传感模式，能同时捕获空间和光谱信息，实现对材料、化学和生物特性的无损、免标记分析。本综述旨在为HSI提供一个全面的概览。", "method": "本综述涵盖了HSI的物理原理、传感器架构、数据采集、校准与校正、常见数据结构、经典与现代分析方法（如降维、分类、光谱解混、AI驱动技术如深度学习），并讨论了其在地球观测、精准农业、生物医学、工业检测、文化遗产和安全等领域的代表性应用。此外，还探讨了硬件权衡、采集变异性、高维数据复杂性等持续挑战，并提出了计算成像、物理信息建模、跨模态融合、自监督学习等新兴解决方案，同时强调了数据集共享、可复现性和元数据文档的最佳实践。", "result": "本综述全面阐述了高光谱成像的各个方面，从基础原理到先进分析方法（包括AI），展示了其在多领域的广泛应用潜力，并识别了当前挑战和新兴解决方案，为该领域提供了全面的参考指南。", "conclusion": "高光谱成像正演变为一个通用、跨学科的平台，未来将通过传感器小型化、自监督学习和基础模型等技术实现可扩展、实时和嵌入式系统，有望在科学、技术和社会领域带来变革性应用。"}}
{"id": "2508.08211", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.08211", "abs": "https://arxiv.org/abs/2508.08211", "authors": ["Zhuohao Yu", "Xingru Jiang", "Weizheng Gu", "Yidong Wang", "Shikun Zhang", "Wei Ye"], "title": "SAEMark: Multi-bit LLM Watermarking with Inference-Time Scaling", "comment": "24 pages, 12 figures, code available:\n  https://zhuohaoyu.github.io/SAEMark", "summary": "Watermarking LLM-generated text is critical for content attribution and\nmisinformation prevention. However, existing methods compromise text quality,\nrequire white-box model access and logit manipulation. These limitations\nexclude API-based models and multilingual scenarios. We propose SAEMark, a\ngeneral framework for post-hoc multi-bit watermarking that embeds personalized\nmessages solely via inference-time, feature-based rejection sampling without\naltering model logits or requiring training. Our approach operates on\ndeterministic features extracted from generated text, selecting outputs whose\nfeature statistics align with key-derived targets. This framework naturally\ngeneralizes across languages and domains while preserving text quality through\nsampling LLM outputs instead of modifying. We provide theoretical guarantees\nrelating watermark success probability and compute budget that hold for any\nsuitable feature extractor. Empirically, we demonstrate the framework's\neffectiveness using Sparse Autoencoders (SAEs), achieving superior detection\naccuracy and text quality. Experiments across 4 datasets show SAEMark's\nconsistent performance, with 99.7% F1 on English and strong multi-bit detection\naccuracy. SAEMark establishes a new paradigm for scalable watermarking that\nworks out-of-the-box with closed-source LLMs while enabling content\nattribution.", "AI": {"tldr": "SAEMark是一种通用的后验多比特水印框架，通过基于特征的拒绝采样，在不影响文本质量或需要白盒模型访问的情况下，为大型语言模型（LLM）生成文本嵌入个性化信息。", "motivation": "现有LLM文本水印方法会损害文本质量，需要白盒模型访问和对logits的修改，这限制了它们在基于API的模型和多语言场景中的应用。", "method": "SAEMark在推理时通过基于特征的拒绝采样实现多比特水印。它从生成的文本中提取确定性特征，并选择特征统计数据与密钥目标对齐的输出。该方法不修改模型logits，不需要训练，并利用稀疏自编码器（SAE）作为特征提取器。", "result": "SAEMark在保持文本质量的同时，实现了卓越的检测准确性（英文数据集F1分数达99.7%）和强大的多比特检测能力。在4个数据集上的实验表明其性能一致。", "conclusion": "SAEMark为LLM水印提供了一种可扩展的新范式，可开箱即用于闭源LLM，同时实现内容归属，且不损害文本质量。"}}
{"id": "2508.07313", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07313", "abs": "https://arxiv.org/abs/2508.07313", "authors": ["Junyu Xiong", "Yonghui Wang", "Weichao Zhao", "Chenyu Liu", "Bing Yin", "Wengang Zhou", "Houqiang Li"], "title": "DocR1: Evidence Page-Guided GRPO for Multi-Page Document Understanding", "comment": null, "summary": "Understanding multi-page documents poses a significant challenge for\nmultimodal large language models (MLLMs), as it requires fine-grained visual\ncomprehension and multi-hop reasoning across pages. While prior work has\nexplored reinforcement learning (RL) for enhancing advanced reasoning in MLLMs,\nits application to multi-page document understanding remains underexplored. In\nthis paper, we introduce DocR1, an MLLM trained with a novel RL framework,\nEvidence Page-Guided GRPO (EviGRPO). EviGRPO incorporates an evidence-aware\nreward mechanism that promotes a coarse-to-fine reasoning strategy, guiding the\nmodel to first retrieve relevant pages before generating answers. This training\nparadigm enables us to build high-quality models with limited supervision. To\nsupport this, we design a two-stage annotation pipeline and a curriculum\nlearning strategy, based on which we construct two datasets: EviBench, a\nhigh-quality training set with 4.8k examples, and ArxivFullQA, an evaluation\nbenchmark with 8.6k QA pairs based on scientific papers. Extensive experiments\nacross a wide range of benchmarks demonstrate that DocR1 achieves\nstate-of-the-art performance on multi-page tasks, while consistently\nmaintaining strong results on single-page benchmarks.", "AI": {"tldr": "DocR1是一个基于新型RL框架EviGRPO训练的多模态大语言模型，旨在解决多页文档理解难题。它通过证据感知奖励机制和粗到细推理策略，实现了对多页文档的SOTA理解能力，并构建了高质量数据集。", "motivation": "多模态大语言模型（MLLMs）在理解多页文档时面临挑战，需要精细的视觉理解和跨页多跳推理。尽管强化学习（RL）已被探索用于增强MLLMs的高级推理，但其在多页文档理解中的应用尚不充分。", "method": "引入DocR1，一个通过新型RL框架EviGRPO训练的MLLM。EviGRPO包含一个证据感知奖励机制，促进“粗到细”的推理策略，即先检索相关页面再生成答案。该训练范式结合两阶段标注流程和课程学习策略，构建了EviBench（4.8k训练集）和ArxivFullQA（8.6k评估基准）数据集。", "result": "DocR1在多页任务上实现了最先进的性能，同时在单页基准测试上保持了强大的结果。", "conclusion": "通过EviGRPO强化学习框架和新颖的数据集构建方法，DocR1有效解决了多页文档理解的挑战，并在相关任务中达到了领先水平。"}}
{"id": "2508.08117", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08117", "abs": "https://arxiv.org/abs/2508.08117", "authors": ["Xudong Han", "Pengcheng Fang", "Yueying Tian", "Jianhui Yu", "Xiaohao Cai", "Daniel Roggen", "Philip Birch"], "title": "GRASPTrack: Geometry-Reasoned Association via Segmentation and Projection for Multi-Object Tracking", "comment": null, "summary": "Multi-object tracking (MOT) in monocular videos is fundamentally challenged\nby occlusions and depth ambiguity, issues that conventional\ntracking-by-detection (TBD) methods struggle to resolve owing to a lack of\ngeometric awareness. To address these limitations, we introduce GRASPTrack, a\nnovel depth-aware MOT framework that integrates monocular depth estimation and\ninstance segmentation into a standard TBD pipeline to generate high-fidelity 3D\npoint clouds from 2D detections, thereby enabling explicit 3D geometric\nreasoning. These 3D point clouds are then voxelized to enable a precise and\nrobust Voxel-Based 3D Intersection-over-Union (IoU) for spatial association. To\nfurther enhance tracking robustness, our approach incorporates Depth-aware\nAdaptive Noise Compensation, which dynamically adjusts the Kalman filter\nprocess noise based on occlusion severity for more reliable state estimation.\nAdditionally, we propose a Depth-enhanced Observation-Centric Momentum, which\nextends the motion direction consistency from the image plane into 3D space to\nimprove motion-based association cues, particularly for objects with complex\ntrajectories. Extensive experiments on the MOT17, MOT20, and DanceTrack\nbenchmarks demonstrate that our method achieves competitive performance,\nsignificantly improving tracking robustness in complex scenes with frequent\nocclusions and intricate motion patterns.", "AI": {"tldr": "GRASPTrack是一个新型的深度感知多目标跟踪（MOT）框架，通过整合单目深度估计和实例分割，从2D检测生成3D点云，实现显式3D几何推理，并结合基于体素的3D IoU、深度感知自适应噪声补偿和深度增强观察中心动量来提升跟踪鲁棒性。", "motivation": "传统的基于检测的跟踪（TBD）方法在单目视频中面临遮挡和深度模糊的挑战，缺乏几何感知能力，难以有效解决这些问题。", "method": "GRASPTrack将单目深度估计和实例分割集成到标准TBD流程中，从2D检测生成高保真3D点云，从而实现显式3D几何推理。这些3D点云被体素化以实现精确鲁棒的基于体素的3D交并比（IoU）进行空间关联。此外，该方法引入了深度感知自适应噪声补偿，根据遮挡严重程度动态调整卡尔曼滤波器过程噪声；并提出了深度增强观察中心动量，将运动方向一致性从图像平面扩展到3D空间，以改进基于运动的关联线索。", "result": "在MOT17、MOT20和DanceTrack基准测试中，该方法取得了具有竞争力的性能，显著提高了在频繁遮挡和复杂运动模式的复杂场景中的跟踪鲁棒性。", "conclusion": "GRASPTrack通过引入3D几何推理和多项创新机制，有效克服了单目视频中多目标跟踪的挑战，显著提升了在复杂场景下的跟踪鲁棒性。"}}
{"id": "2508.08224", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08224", "abs": "https://arxiv.org/abs/2508.08224", "authors": ["Shansong Wang", "Mingzhe Hu", "Qiang Li", "Mojtaba Safari", "Xiaofeng Yang"], "title": "Capabilities of GPT-5 on Multimodal Medical Reasoning", "comment": null, "summary": "Recent advances in large language models (LLMs) have enabled general-purpose\nsystems to perform increasingly complex domain-specific reasoning without\nextensive fine-tuning. In the medical domain, decision-making often requires\nintegrating heterogeneous information sources, including patient narratives,\nstructured data, and medical images. This study positions GPT-5 as a generalist\nmultimodal reasoner for medical decision support and systematically evaluates\nits zero-shot chain-of-thought reasoning performance on both text-based\nquestion answering and visual question answering tasks under a unified\nprotocol. We benchmark GPT-5, GPT-5-mini, GPT-5-nano, and GPT-4o-2024-11-20\nagainst standardized splits of MedQA, MedXpertQA (text and multimodal), MMLU\nmedical subsets, USMLE self-assessment exams, and VQA-RAD. Results show that\nGPT-5 consistently outperforms all baselines, achieving state-of-the-art\naccuracy across all QA benchmarks and delivering substantial gains in\nmultimodal reasoning. On MedXpertQA MM, GPT-5 improves reasoning and\nunderstanding scores by +29.62% and +36.18% over GPT-4o, respectively, and\nsurpasses pre-licensed human experts by +24.23% in reasoning and +29.40% in\nunderstanding. In contrast, GPT-4o remains below human expert performance in\nmost dimensions. A representative case study demonstrates GPT-5's ability to\nintegrate visual and textual cues into a coherent diagnostic reasoning chain,\nrecommending appropriate high-stakes interventions. Our results show that, on\nthese controlled multimodal reasoning benchmarks, GPT-5 moves from\nhuman-comparable to above human-expert performance. This improvement may\nsubstantially inform the design of future clinical decision-support systems.", "AI": {"tldr": "本研究评估了GPT-5在医疗领域的多模态推理能力，发现其在文本和视觉问答任务上均显著超越了现有模型和人类专家。", "motivation": "大型语言模型（LLMs）在复杂领域推理方面取得了进展，但医疗决策需要整合异构信息（文本、结构化数据、图像）。本研究旨在评估GPT-5作为通用多模态推理器在医疗决策支持中的零样本链式思考性能。", "method": "将GPT-5定位为医疗决策支持的通用多模态推理器，并系统评估其在统一协议下的零样本链式思考性能。通过在MedQA、MedXpertQA（文本和多模态）、MMLU医疗子集、USMLE自评估考试和VQA-RAD等标准化数据集上，将GPT-5、GPT-5-mini、GPT-5-nano和GPT-4o-2024-11-20进行基准测试，并与预许可的人类专家进行比较。", "result": "GPT-5在所有问答基准测试中均持续优于所有基线模型，实现了最先进的准确性，并在多模态推理方面取得了显著提升。在MedXpertQA MM上，GPT-5的推理和理解分数分别比GPT-4o提高了29.62%和36.18%，并分别比预许可的人类专家高出24.23%和29.40%。GPT-4o在大多数维度上仍低于人类专家表现。案例研究表明GPT-5能有效整合视觉和文本信息进行诊断推理。", "conclusion": "GPT-5在受控多模态推理基准测试中，性能已从与人类相当提升到超越人类专家。这一改进可能对未来临床决策支持系统的设计产生重大影响。"}}
{"id": "2508.07318", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07318", "abs": "https://arxiv.org/abs/2508.07318", "authors": ["Jinjing Gu", "Tianbao Qin", "Yuanyuan Pu", "Zhengpeng Zhao"], "title": "RORPCap: Retrieval-based Objects and Relations Prompt for Image Captioning", "comment": null, "summary": "Image captioning aims to generate natural language descriptions for input\nimages in an open-form manner. To accurately generate descriptions related to\nthe image, a critical step in image captioning is to identify objects and\nunderstand their relations within the image. Modern approaches typically\ncapitalize on object detectors or combine detectors with Graph Convolutional\nNetwork (GCN). However, these models suffer from redundant detection\ninformation, difficulty in GCN construction, and high training costs. To\naddress these issues, a Retrieval-based Objects and Relations Prompt for Image\nCaptioning (RORPCap) is proposed, inspired by the fact that image-text\nretrieval can provide rich semantic information for input images. RORPCap\nemploys an Objects and relations Extraction Model to extract object and\nrelation words from the image. These words are then incorporate into predefined\nprompt templates and encoded as prompt embeddings. Next, a Mamba-based mapping\nnetwork is designed to quickly map image embeddings extracted by CLIP to\nvisual-text embeddings. Finally, the resulting prompt embeddings and\nvisual-text embeddings are concatenated to form textual-enriched feature\nembeddings, which are fed into a GPT-2 model for caption generation. Extensive\nexperiments conducted on the widely used MS-COCO dataset show that the RORPCap\nrequires only 2.6 hours under cross-entropy loss training, achieving 120.5%\nCIDEr score and 22.0% SPICE score on the \"Karpathy\" test split. RORPCap\nachieves comparable performance metrics to detector-based and GCN-based models\nwith the shortest training time and demonstrates its potential as an\nalternative for image captioning.", "AI": {"tldr": "该论文提出了一种名为RORPCap（基于检索的对象和关系提示图像字幕生成）的新方法，通过从图像中提取对象和关系词构建提示，并结合Mamba网络映射图像嵌入，然后输入GPT-2模型进行字幕生成。该方法在MS-COCO数据集上取得了与现有方法相当的性能，且训练时间显著缩短。", "motivation": "图像字幕生成需要准确识别图像中的对象并理解其关系。然而，现有方法（如基于对象检测器或GCN的方法）存在冗余检测信息、GCN构建困难和训练成本高的问题。", "method": "RORPCap方法受图像-文本检索提供丰富语义信息的启发。它首先使用一个对象和关系提取模型从图像中提取对象和关系词，然后将这些词融入预定义的提示模板并编码为提示嵌入。接着，设计了一个基于Mamba的映射网络，将CLIP提取的图像嵌入快速映射为视觉-文本嵌入。最后，将提示嵌入和视觉-文本嵌入连接成文本增强的特征嵌入，输入到GPT-2模型中生成字幕。", "result": "在MS-COCO数据集的“Karpathy”测试集上进行了广泛实验。RORPCap在交叉熵损失训练下仅需2.6小时，取得了120.5%的CIDEr分数和22.0%的SPICE分数。它在性能指标上与基于检测器和GCN的模型相当，但训练时间最短。", "conclusion": "RORPCap在图像字幕生成任务上实现了与基于检测器和GCN模型相当的性能，并且显著缩短了训练时间。这表明RORPCap作为一种替代的图像字幕生成方法具有巨大潜力。"}}
{"id": "2508.08177", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08177", "abs": "https://arxiv.org/abs/2508.08177", "authors": ["Zhonghao Yan", "Muxi Diao", "Yuxuan Yang", "Jiayuan Xu", "Kaizhou Zhang", "Ruoyan Jing", "Lele Yang", "Yanxi Liu", "Kongming Liang", "Zhanyu Ma"], "title": "MedReasoner: Reinforcement Learning Drives Reasoning Grounding from Clinical Thought to Pixel-Level Precision", "comment": "37 pages", "summary": "Accurately grounding regions of interest (ROIs) is critical for diagnosis and\ntreatment planning in medical imaging. While multimodal large language models\n(MLLMs) combine visual perception with natural language, current\nmedical-grounding pipelines still rely on supervised fine-tuning with explicit\nspatial hints, making them ill-equipped to handle the implicit queries common\nin clinical practice. This work makes three core contributions. We first define\nUnified Medical Reasoning Grounding (UMRG), a novel vision-language task that\ndemands clinical reasoning and pixel-level grounding. Second, we release\nU-MRG-14K, a dataset of 14K samples featuring pixel-level masks alongside\nimplicit clinical queries and reasoning traces, spanning 10 modalities, 15\nsuper-categories, and 108 specific categories. Finally, we introduce\nMedReasoner, a modular framework that distinctly separates reasoning from\nsegmentation: an MLLM reasoner is optimized with reinforcement learning, while\na frozen segmentation expert converts spatial prompts into masks, with\nalignment achieved through format and accuracy rewards. MedReasoner achieves\nstate-of-the-art performance on U-MRG-14K and demonstrates strong\ngeneralization to unseen clinical queries, underscoring the significant promise\nof reinforcement learning for interpretable medical grounding.", "AI": {"tldr": "本文提出了一个名为统一医学推理定位（UMRG）的新任务，一个包含14K样本的U-MRG-14K数据集，以及一个名为MedReasoner的模块化框架，用于在医学影像中实现基于隐式临床查询的像素级区域定位和推理，并通过强化学习实现了最先进的性能和良好的泛化能力。", "motivation": "当前的医学影像定位方法依赖于显式空间提示的监督微调，难以处理临床实践中常见的隐式查询。多模态大语言模型（MLLMs）虽结合了视觉与语言，但在医学定位中仍需改进以适应临床推理需求。", "method": "1. 定义了统一医学推理定位（UMRG）任务，要求结合临床推理和像素级定位。2. 发布了U-MRG-14K数据集，包含14K个带有像素级掩码、隐式临床查询和推理轨迹的样本。3. 提出了MedReasoner框架，将推理与分割分离：一个MLLM推理器通过强化学习优化，一个冻结的分割专家将空间提示转换为掩码，通过格式和准确性奖励实现对齐。", "result": "MedReasoner在U-MRG-14K数据集上取得了最先进的性能，并对未见过的临床查询表现出强大的泛化能力。", "conclusion": "强化学习在可解释的医学定位中展现出巨大潜力，MedReasoner的成功证明了其在处理隐式临床查询方面的有效性。"}}
{"id": "2508.08236", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.08236", "abs": "https://arxiv.org/abs/2508.08236", "authors": ["Yunna Cai", "Fan Wang", "Haowei Wang", "Kun Wang", "Kailai Yang", "Sophia Ananiadou", "Moyan Li", "Mingming Fan"], "title": "Exploring Safety Alignment Evaluation of LLMs in Chinese Mental Health Dialogues via LLM-as-Judge", "comment": null, "summary": "Evaluating the safety alignment of LLM responses in high-risk mental health\ndialogues is particularly difficult due to missing gold-standard answers and\nthe ethically sensitive nature of these interactions. To address this\nchallenge, we propose PsyCrisis-Bench, a reference-free evaluation benchmark\nbased on real-world Chinese mental health dialogues. It evaluates whether the\nmodel responses align with the safety principles defined by experts.\nSpecifically designed for settings without standard references, our method\nadopts a prompt-based LLM-as-Judge approach that conducts in-context evaluation\nusing expert-defined reasoning chains grounded in psychological intervention\nprinciples. We employ binary point-wise scoring across multiple safety\ndimensions to enhance the explainability and traceability of the evaluation.\nAdditionally, we present a manually curated, high-quality Chinese-language\ndataset covering self-harm, suicidal ideation, and existential distress,\nderived from real-world online discourse. Experiments on 3600 judgments show\nthat our method achieves the highest agreement with expert assessments and\nproduces more interpretable evaluation rationales compared to existing\napproaches. Our dataset and evaluation tool are publicly available to\nfacilitate further research.", "AI": {"tldr": "本文提出了PsyCrisis-Bench，一个基于真实中文心理健康对话的无参考基准，用于评估大语言模型在处理高风险心理健康对话时的安全对齐，采用基于提示的LLM-as-Judge方法和专家定义的推理链进行评估。", "motivation": "由于缺乏黄金标准答案和交互的伦理敏感性，评估大语言模型在处理高风险心理健康对话时的安全对齐性非常困难。", "method": "提出PsyCrisis-Bench，一个无参考评估基准，基于真实中文心理健康对话。采用基于提示的“LLM即评委”方法，利用专家定义的、基于心理干预原则的推理链进行上下文评估。通过多维度二元评分提升评估的可解释性和可追溯性。构建了一个高质量的中文数据集，涵盖自残、自杀意念和生存困境。", "result": "在3600个判断的实验中，该方法与专家评估的一致性最高，并产生了比现有方法更具可解释性的评估理由。数据集和评估工具已公开。", "conclusion": "PsyCrisis-Bench提供了一种可靠且可解释的方法来评估大语言模型在心理健康领域的安全性，促进了该领域的进一步研究。"}}
{"id": "2508.07330", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07330", "abs": "https://arxiv.org/abs/2508.07330", "authors": ["Tuyen Tran", "Thao Minh Le", "Quang-Hung Le", "Truyen Tran"], "title": "Planner-Refiner: Dynamic Space-Time Refinement for Vision-Language Alignment in Videos", "comment": "Accepted for publication at ECAI 2025", "summary": "Vision-language alignment in video must address the complexity of language,\nevolving interacting entities, their action chains, and semantic gaps between\nlanguage and vision. This work introduces Planner-Refiner, a framework to\novercome these challenges. Planner-Refiner bridges the semantic gap by\niteratively refining visual elements' space-time representation, guided by\nlanguage until semantic gaps are minimal. A Planner module schedules language\nguidance by decomposing complex linguistic prompts into short sentence chains.\nThe Refiner processes each short sentence, a noun-phrase and verb-phrase pair,\nto direct visual tokens' self-attention across space then time, achieving\nefficient single-step refinement. A recurrent system chains these steps,\nmaintaining refined visual token representations. The final representation\nfeeds into task-specific heads for alignment generation. We demonstrate\nPlanner-Refiner's effectiveness on two video-language alignment tasks:\nReferring Video Object Segmentation and Temporal Grounding with varying\nlanguage complexity. We further introduce a new MeViS-X benchmark to assess\nmodels' capability with long queries. Superior performance versus\nstate-of-the-art methods on these benchmarks shows the approach's potential,\nespecially for complex prompts.", "AI": {"tldr": "本文提出Planner-Refiner框架，通过语言引导迭代细化视觉元素时空表示，以解决视频-语言对齐中的复杂语言和语义鸿沟问题。", "motivation": "视频中的视觉-语言对齐面临语言复杂性、动态交互实体、动作链以及语言与视觉之间的语义鸿沟等挑战。", "method": "引入Planner-Refiner框架，通过迭代细化视觉元素的时空表示来弥合语义鸿沟，该过程由语言引导。Planner模块将复杂语言提示分解为短句子链，调度语言指导。Refiner模块处理每个短句（名词-动词短语对），引导视觉token在空间和时间上的自注意力，实现高效的单步细化。一个循环系统将这些步骤链接起来，维持细化的视觉token表示，最终用于任务特定的对齐生成。", "result": "在两项视频-语言对齐任务（Referring Video Object Segmentation和Temporal Grounding）上验证了Planner-Refiner的有效性，并引入了新的MeViS-X基准来评估模型处理长查询的能力。该方法在这些基准上表现优于SOTA方法，尤其是在处理复杂提示时。", "conclusion": "Planner-Refiner框架在视频-语言对齐任务中展现出巨大潜力，尤其擅长处理复杂的语言查询。"}}
{"id": "2508.08180", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08180", "abs": "https://arxiv.org/abs/2508.08180", "authors": ["Luca Zedda", "Andrea Loddo", "Cecilia Di Ruberto", "Carsten Marr"], "title": "RedDino: A foundation model for red blood cell analysis", "comment": null, "summary": "Red blood cells (RBCs) are essential to human health, and their precise\nmorphological analysis is important for diagnosing hematological disorders.\nDespite the promise of foundation models in medical diagnostics, comprehensive\nAI solutions for RBC analysis remain scarce. We present RedDino, a\nself-supervised foundation model designed for RBC image analysis. RedDino uses\nan RBC-specific adaptation of the DINOv2 self-supervised learning framework and\nis trained on a curated dataset of 1.25 million RBC images from diverse\nacquisition modalities and sources. Extensive evaluations show that RedDino\noutperforms existing state-of-the-art models on RBC shape classification.\nThrough assessments including linear probing and nearest neighbor\nclassification, we confirm its strong feature representations and\ngeneralization ability. Our main contributions are: (1) a foundation model\ntailored for RBC analysis, (2) ablation studies exploring DINOv2 configurations\nfor RBC modeling, and (3) a detailed evaluation of generalization performance.\nRedDino addresses key challenges in computational hematology by capturing\nnuanced morphological features, advancing the development of reliable\ndiagnostic tools. The source code and pretrained models for RedDino are\navailable at https://github.com/Snarci/RedDino, and the pretrained models can\nbe downloaded from our Hugging Face collection at\nhttps://huggingface.co/collections/Snarcy/reddino-689a13e29241d2e5690202fc", "AI": {"tldr": "RedDino是一个基于自监督学习的红细胞（RBC）基础模型，用于RBC图像分析，并在RBC形态分类上超越了现有SOTA模型。", "motivation": "红细胞的精确形态分析对于诊断血液疾病至关重要。尽管基础模型在医学诊断中前景广阔，但针对红细胞分析的全面AI解决方案仍旧稀缺。", "method": "RedDino采用了DINOv2自监督学习框架，并针对RBC图像进行了特异性适配。该模型在一个包含125万张来自不同采集方式和来源的RBC图像的精选数据集上进行训练。通过线性探测和最近邻分类评估其特征表示和泛化能力。", "result": "RedDino在RBC形态分类方面优于现有最先进模型。评估证实了其强大的特征表示能力和泛化能力。主要贡献包括：为RBC分析量身定制的基础模型，探索DINOv2配置的消融研究，以及对泛化性能的详细评估。", "conclusion": "RedDino通过捕捉细微的形态特征，解决了计算血液学中的关键挑战，推动了可靠诊断工具的开发。"}}
{"id": "2508.08243", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.08243", "abs": "https://arxiv.org/abs/2508.08243", "authors": ["Jiahao Zhao", "Liwei Dong"], "title": "Jinx: Unlimited LLMs for Probing Alignment Failures", "comment": "https://huggingface.co/Jinx-org", "summary": "Unlimited, or so-called helpful-only language models are trained without\nsafety alignment constraints and never refuse user queries. They are widely\nused by leading AI companies as internal tools for red teaming and alignment\nevaluation. For example, if a safety-aligned model produces harmful outputs\nsimilar to an unlimited model, this indicates alignment failures that require\nfurther attention. Despite their essential role in assessing alignment, such\nmodels are not available to the research community.\n  We introduce Jinx, a helpful-only variant of popular open-weight LLMs. Jinx\nresponds to all queries without refusals or safety filtering, while preserving\nthe base model's capabilities in reasoning and instruction following. It\nprovides researchers with an accessible tool for probing alignment failures,\nevaluating safety boundaries, and systematically studying failure modes in\nlanguage model safety.", "AI": {"tldr": "Jinx是一个无安全限制的语言模型变体，旨在为研究社区提供一个工具，用于探测对齐失败和评估AI安全边界。", "motivation": "尽管无限制（helpful-only）语言模型在AI公司内部用于红队测试和对齐评估至关重要，但它们并未向研究社区开放，这限制了对齐失败和安全边界的研究。", "method": "Jinx是流行开源大语言模型的“helpful-only”变体，它响应所有查询，不进行拒绝或安全过滤，同时保留了基础模型的推理和指令遵循能力。", "result": "Jinx为研究人员提供了一个可访问的工具，用于探测对齐失败、评估安全边界以及系统性地研究语言模型安全中的故障模式。", "conclusion": "Jinx的发布将使研究社区能够更深入地理解和研究语言模型的安全对齐问题及其潜在的失败模式。"}}
{"id": "2508.07341", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07341", "abs": "https://arxiv.org/abs/2508.07341", "authors": ["Fangtai Wu", "Mushui Liu", "Weijie He", "Wanggui He", "Hao Jiang", "Zhao Wang", "Yunlong Yu"], "title": "CoAR: Concept Injection into Autoregressive Models for Personalized Text-to-Image Generation", "comment": null, "summary": "The unified autoregressive (AR) model excels at multimodal understanding and\ngeneration, but its potential for customized image generation remains\nunderexplored. Existing customized generation methods rely on full fine-tuning\nor adapters, making them costly and prone to overfitting or catastrophic\nforgetting. In this paper, we propose \\textbf{CoAR}, a novel framework for\ninjecting subject concepts into the unified AR models while keeping all\npre-trained parameters completely frozen. CoAR learns effective, specific\nsubject representations with only a minimal number of parameters using a\nLayerwise Multimodal Context Learning strategy. To address overfitting and\nlanguage drift, we further introduce regularization that preserves the\npre-trained distribution and anchors context tokens to improve subject fidelity\nand re-contextualization. Additionally, CoAR supports training-free subject\ncustomization in a user-provided style. Experiments demonstrate that CoAR\nachieves superior performance on both subject-driven personalization and style\npersonalization, while delivering significant gains in computational and memory\nefficiency. Notably, CoAR tunes less than \\textbf{0.05\\%} of the parameters\nwhile achieving competitive performance compared to recent Proxy-Tuning. Code:\nhttps://github.com/KZF-kzf/CoAR", "AI": {"tldr": "CoAR提出了一种新颖的框架，通过冻结所有预训练参数并使用分层多模态上下文学习策略，以极少的参数实现统一自回归模型中的个性化图像生成，同时有效避免过拟合和语言漂移。", "motivation": "统一自回归模型在多模态理解和生成方面表现出色，但在定制化图像生成方面的潜力尚未充分挖掘。现有定制化方法（如全微调或适配器）成本高昂，且容易过拟合或灾难性遗忘。", "method": "CoAR框架通过以下方式将主题概念注入统一AR模型：1) 完全冻结所有预训练参数；2) 使用分层多模态上下文学习（Layerwise Multimodal Context Learning）策略，仅用极少量参数学习有效且特定的主题表示；3) 引入正则化，以保留预训练分布并锚定上下文标记，从而解决过拟合和语言漂移问题，提高主题保真度和再语境化能力；4) 支持免训练的用户提供风格的主题定制。", "result": "CoAR在主题驱动个性化和风格个性化方面均取得了卓越性能，同时显著提升了计算和内存效率。值得注意的是，CoAR仅微调了不到0.05%的参数，却达到了与近期Proxy-Tuning相当的竞争性表现。", "conclusion": "CoAR成功地在统一自回归模型中实现了高效且高质量的定制化图像生成，通过极度参数高效的方法解决了现有定制化方法的成本和过拟合问题，并在性能上具有竞争力。"}}
{"id": "2508.08227", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08227", "abs": "https://arxiv.org/abs/2508.08227", "authors": ["Zhiqiang Wu", "Zhaomang Sun", "Tong Zhou", "Bingtao Fu", "Ji Cong", "Yitong Dong", "Huaqi Zhang", "Xuan Tang", "Mingsong Chen", "Xian Wei"], "title": "OMGSR: You Only Need One Mid-timestep Guidance for Real-World Image Super-Resolution", "comment": null, "summary": "Denoising Diffusion Probabilistic Models (DDPM) and Flow Matching (FM)\ngenerative models show promising potential for one-step Real-World Image\nSuper-Resolution (Real-ISR). Recent one-step Real-ISR models typically inject a\nLow-Quality (LQ) image latent distribution at the initial timestep. However, a\nfundamental gap exists between the LQ image latent distribution and the\nGaussian noisy latent distribution, limiting the effective utilization of\ngenerative priors. We observe that the noisy latent distribution at DDPM/FM\nmid-timesteps aligns more closely with the LQ image latent distribution. Based\non this insight, we present One Mid-timestep Guidance Real-ISR (OMGSR), a\nuniversal framework applicable to DDPM/FM-based generative models. OMGSR\ninjects the LQ image latent distribution at a pre-computed mid-timestep,\nincorporating the proposed Latent Distribution Refinement loss to alleviate the\nlatent distribution gap. We also design the Overlap-Chunked LPIPS/GAN loss to\neliminate checkerboard artifacts in image generation. Within this framework, we\ninstantiate OMGSR for DDPM/FM-based generative models with two variants:\nOMGSR-S (SD-Turbo) and OMGSR-F (FLUX.1-dev). Experimental results demonstrate\nthat OMGSR-S/F achieves balanced/excellent performance across quantitative and\nqualitative metrics at 512-resolution. Notably, OMGSR-F establishes\noverwhelming dominance in all reference metrics. We further train a\n1k-resolution OMGSR-F to match the default resolution of FLUX.1-dev, which\nyields excellent results, especially in the details of the image generation. We\nalso generate 2k-resolution images by the 1k-resolution OMGSR-F using our\ntwo-stage Tiled VAE & Diffusion.", "AI": {"tldr": "本文提出OMGSR框架，通过在扩散模型（DDPM/FM）的中间时间步注入低质量图像潜变量分布，并引入潜变量分布细化损失和重叠分块损失，解决了传统单步超分中低质量图像潜变量与高斯噪声分布之间的差距，显著提升了真实世界图像超分辨率性能。", "motivation": "现有的单步真实世界图像超分辨率（Real-ISR）模型通常在初始时间步注入低质量（LQ）图像潜变量分布，但LQ图像潜变量分布与高斯噪声潜变量分布之间存在根本性差异，这限制了生成先验的有效利用。研究观察到DDPM/FM模型中间时间步的噪声潜变量分布与LQ图像潜变量分布更为接近，从而激发了改进注入策略的动机。", "method": "本文提出One Mid-timestep Guidance Real-ISR (OMGSR) 框架，该框架适用于DDPM/FM生成模型。核心方法包括：1) 在预先计算的中间时间步注入LQ图像潜变量分布；2) 引入Latent Distribution Refinement损失以缓解潜变量分布差距；3) 设计Overlap-Chunked LPIPS/GAN损失以消除图像生成中的棋盘格伪影。该框架实例化了两个变体：OMGSR-S（基于SD-Turbo）和OMGSR-F（基于FLUX.1-dev），并进一步训练了1k分辨率的OMGSR-F，利用两阶段分块VAE和扩散技术生成2k分辨率图像。", "result": "实验结果表明，OMGSR-S/F在512分辨率下实现了定量和定性指标的平衡/卓越性能。值得注意的是，OMGSR-F在所有参考指标中都表现出压倒性优势。训练的1k分辨率OMGSR-F在图像生成细节方面表现出色，并且能够通过两阶段分块VAE和扩散生成2k分辨率图像。", "conclusion": "OMGSR框架通过在扩散模型中间时间步注入低质量图像潜变量，并结合提出的潜变量分布细化损失和重叠分块损失，有效弥合了分布差距，显著提升了单步真实世界图像超分辨率的性能。特别是OMGSR-F变体，在各项指标上均取得了领先地位，证明了其中间时间步指导策略的有效性和通用性，并展示了其在高分辨率图像生成方面的潜力。"}}
{"id": "2508.07346", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07346", "abs": "https://arxiv.org/abs/2508.07346", "authors": ["Tingyu Yang", "Jue Gong", "Jinpei Guo", "Wenbo Li", "Yong Guo", "Yulun Zhang"], "title": "SODiff: Semantic-Oriented Diffusion Model for JPEG Compression Artifacts Removal", "comment": "7 pages, 5 figures. The code will be available at\n  \\url{https://github.com/frakenation/SODiff}", "summary": "JPEG, as a widely used image compression standard, often introduces severe\nvisual artifacts when achieving high compression ratios. Although existing deep\nlearning-based restoration methods have made considerable progress, they often\nstruggle to recover complex texture details, resulting in over-smoothed\noutputs. To overcome these limitations, we propose SODiff, a novel and\nefficient semantic-oriented one-step diffusion model for JPEG artifacts\nremoval. Our core idea is that effective restoration hinges on providing\nsemantic-oriented guidance to the pre-trained diffusion model, thereby fully\nleveraging its powerful generative prior. To this end, SODiff incorporates a\nsemantic-aligned image prompt extractor (SAIPE). SAIPE extracts rich features\nfrom low-quality (LQ) images and projects them into an embedding space\nsemantically aligned with that of the text encoder. Simultaneously, it\npreserves crucial information for faithful reconstruction. Furthermore, we\npropose a quality factor-aware time predictor that implicitly learns the\ncompression quality factor (QF) of the LQ image and adaptively selects the\noptimal denoising start timestep for the diffusion process. Extensive\nexperimental results show that our SODiff outperforms recent leading methods in\nboth visual quality and quantitative metrics. Code is available at:\nhttps://github.com/frakenation/SODiff", "AI": {"tldr": "SODiff是一种新颖高效的单步扩散模型，通过语义导向和质量因子感知的时间预测，有效去除JPEG压缩伪影，恢复复杂纹理细节。", "motivation": "JPEG图像在高压缩比下会引入严重视觉伪影，现有深度学习方法难以恢复复杂纹理细节，导致输出过于平滑。", "method": "提出SODiff模型，其核心思想是为预训练扩散模型提供语义导向。模型包含：1. 语义对齐图像提示提取器（SAIPE），从低质量图像中提取特征并映射到与文本编码器语义对齐的嵌入空间，同时保留重建信息。2. 质量因子感知时间预测器，隐式学习低质量图像的压缩质量因子，并自适应选择扩散过程的最佳去噪起始时间步。", "result": "广泛的实验结果表明，SODiff在视觉质量和量化指标上均优于近期领先的方法。", "conclusion": "SODiff通过语义导向和自适应时间步选择，有效解决了JPEG伪影去除中复杂纹理恢复的挑战，取得了卓越的性能。"}}
{"id": "2508.08244", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08244", "abs": "https://arxiv.org/abs/2508.08244", "authors": ["Jingwen He", "Hongbo Liu", "Jiajun Li", "Ziqi Huang", "Yu Qiao", "Wanli Ouyang", "Ziwei Liu"], "title": "Cut2Next: Generating Next Shot via In-Context Tuning", "comment": null, "summary": "Effective multi-shot generation demands purposeful, film-like transitions and\nstrict cinematic continuity. Current methods, however, often prioritize basic\nvisual consistency, neglecting crucial editing patterns (e.g., shot/reverse\nshot, cutaways) that drive narrative flow for compelling storytelling. This\nyields outputs that may be visually coherent but lack narrative sophistication\nand true cinematic integrity. To bridge this, we introduce Next Shot Generation\n(NSG): synthesizing a subsequent, high-quality shot that critically conforms to\nprofessional editing patterns while upholding rigorous cinematic continuity.\nOur framework, Cut2Next, leverages a Diffusion Transformer (DiT). It employs\nin-context tuning guided by a novel Hierarchical Multi-Prompting strategy. This\nstrategy uses Relational Prompts to define overall context and inter-shot\nediting styles. Individual Prompts then specify per-shot content and\ncinematographic attributes. Together, these guide Cut2Next to generate\ncinematically appropriate next shots. Architectural innovations, Context-Aware\nCondition Injection (CACI) and Hierarchical Attention Mask (HAM), further\nintegrate these diverse signals without introducing new parameters. We\nconstruct RawCuts (large-scale) and CuratedCuts (refined) datasets, both with\nhierarchical prompts, and introduce CutBench for evaluation. Experiments show\nCut2Next excels in visual consistency and text fidelity. Crucially, user\nstudies reveal a strong preference for Cut2Next, particularly for its adherence\nto intended editing patterns and overall cinematic continuity, validating its\nability to generate high-quality, narratively expressive, and cinematically\ncoherent subsequent shots.", "AI": {"tldr": "该研究提出了Next Shot Generation (NSG) 框架，旨在生成符合专业剪辑模式和电影连续性的高质量多镜头序列，解决了现有方法缺乏叙事复杂性的问题。", "motivation": "当前的多镜头生成方法侧重于基本的视觉一致性，但忽略了驱动叙事流程的关键剪辑模式（如正反打、插入镜头），导致生成内容缺乏叙事深度和电影完整性。", "method": "引入了Next Shot Generation (NSG) 任务和Cut2Next框架，该框架基于Diffusion Transformer (DiT)。它采用分层多提示策略（Hierarchical Multi-Prompting），通过关系提示定义整体上下文和镜头间剪辑风格，并通过独立提示指定每个镜头的具体内容和电影摄影属性。此外，还引入了上下文感知条件注入 (CACI) 和分层注意力掩码 (HAM) 等架构创新。为训练和评估，构建了RawCuts和CuratedCuts数据集，并提出了CutBench评估基准。", "result": "实验表明，Cut2Next在视觉一致性和文本忠实度方面表现出色。用户研究显示，用户强烈偏好Cut2Next，尤其是在其对预期剪辑模式和整体电影连续性的遵循方面。", "conclusion": "Cut2Next成功生成了高质量、叙事表达丰富且电影连贯的后续镜头，验证了其在多镜头生成中整合专业剪辑模式的能力。"}}
{"id": "2508.07355", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07355", "abs": "https://arxiv.org/abs/2508.07355", "authors": ["Qilin Zhang", "Olaf Wysocki", "Boris Jutzi"], "title": "GS4Buildings: Prior-Guided Gaussian Splatting for 3D Building Reconstruction", "comment": "Accepted for presentation at ISPRS 3D GeoInfo & Smart Data, Smart\n  Cities 2025, Kashiwa, Japan. To appear in the ISPRS Annals of the\n  Photogrammetry, Remote Sensing and Spatial Information Sciences", "summary": "Recent advances in Gaussian Splatting (GS) have demonstrated its\neffectiveness in photo-realistic rendering and 3D reconstruction. Among these,\n2D Gaussian Splatting (2DGS) is particularly suitable for surface\nreconstruction due to its flattened Gaussian representation and integrated\nnormal regularization. However, its performance often degrades in large-scale\nand complex urban scenes with frequent occlusions, leading to incomplete\nbuilding reconstructions. We propose GS4Buildings, a novel prior-guided\nGaussian Splatting method leveraging the ubiquity of semantic 3D building\nmodels for robust and scalable building surface reconstruction. Instead of\nrelying on traditional Structure-from-Motion (SfM) pipelines, GS4Buildings\ninitializes Gaussians directly from low-level Level of Detail (LoD)2 semantic\n3D building models. Moreover, we generate prior depth and normal maps from the\nplanar building geometry and incorporate them into the optimization process,\nproviding strong geometric guidance for surface consistency and structural\naccuracy. We also introduce an optional building-focused mode that limits\nreconstruction to building regions, achieving a 71.8% reduction in Gaussian\nprimitives and enabling a more efficient and compact representation.\nExperiments on urban datasets demonstrate that GS4Buildings improves\nreconstruction completeness by 20.5% and geometric accuracy by 32.8%. These\nresults highlight the potential of semantic building model integration to\nadvance GS-based reconstruction toward real-world urban applications such as\nsmart cities and digital twins. Our project is available:\nhttps://github.com/zqlin0521/GS4Buildings.", "AI": {"tldr": "GS4Buildings是一种新颖的、由先验引导的高斯泼溅（GS）方法，利用语义3D建筑模型提升了大规模城市场景中建筑表面的重建完整性和几何精度。", "motivation": "现有的2D高斯泼溅（2DGS）在大型复杂城市场景中，由于频繁遮挡，性能下降，导致建筑重建不完整。", "method": "GS4Buildings方法不依赖传统的SfM管线，而是直接从LoD2语义3D建筑模型初始化高斯。此外，它从平面建筑几何生成先验深度和法线图，并将其整合到优化过程中提供几何指导。可选的建筑聚焦模式将重建限制在建筑区域，减少了高斯基元数量。", "result": "在城市数据集上的实验表明，GS4Buildings将重建完整性提高了20.5%，几何精度提高了32.8%。建筑聚焦模式将高斯基元减少了71.8%。", "conclusion": "将语义建筑模型集成到高斯泼溅中，能够显著提升基于GS的重建能力，使其更适用于智慧城市和数字孪生等实际城市应用。"}}
{"id": "2508.07369", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07369", "abs": "https://arxiv.org/abs/2508.07369", "authors": ["Tianyu Xin", "Jin-Liang Xiao", "Zeyu Xia", "Shan Yin", "Liang-Jian Deng"], "title": "Training and Inference within 1 Second -- Tackle Cross-Sensor Degradation of Real-World Pansharpening with Efficient Residual Feature Tailoring", "comment": null, "summary": "Deep learning methods for pansharpening have advanced rapidly, yet models\npretrained on data from a specific sensor often generalize poorly to data from\nother sensors. Existing methods to tackle such cross-sensor degradation include\nretraining model or zero-shot methods, but they are highly time-consuming or\neven need extra training data. To address these challenges, our method first\nperforms modular decomposition on deep learning-based pansharpening models,\nrevealing a general yet critical interface where high-dimensional fused\nfeatures begin mapping to the channel space of the final image. % may need\nrevisement A Feature Tailor is then integrated at this interface to address\ncross-sensor degradation at the feature level, and is trained efficiently with\nphysics-aware unsupervised losses. Moreover, our method operates in a\npatch-wise manner, training on partial patches and performing parallel\ninference on all patches to boost efficiency. Our method offers two key\nadvantages: (1) $\\textit{Improved Generalization Ability}$: it significantly\nenhance performance in cross-sensor cases. (2) $\\textit{Low Generalization\nCost}$: it achieves sub-second training and inference, requiring only partial\ntest inputs and no external data, whereas prior methods often take minutes or\neven hours. Experiments on the real-world data from multiple datasets\ndemonstrate that our method achieves state-of-the-art quality and efficiency in\ntackling cross-sensor degradation. For example, training and inference of\n$512\\times512\\times8$ image within $\\textit{0.2 seconds}$ and\n$4000\\times4000\\times8$ image within $\\textit{3 seconds}$ at the fastest\nsetting on a commonly used RTX 3090 GPU, which is over 100 times faster than\nzero-shot methods.", "AI": {"tldr": "该论文提出一种高效的特征级自适应方法，通过在深度学习全色锐化模型中集成“特征定制器”，显著提升了模型在跨传感器数据上的泛化能力，并大幅降低了泛化成本。", "motivation": "现有深度学习全色锐化模型在特定传感器数据上预训练后，在其他传感器数据上泛化能力差。现有解决跨传感器退化问题的方法（如模型再训练或零样本方法）耗时且可能需要额外训练数据。", "method": "首先对深度学习全色锐化模型进行模块化分解，识别出高维融合特征映射到最终图像通道空间的关键接口。然后，在该接口集成一个“特征定制器”（Feature Tailor），通过物理感知的无监督损失进行高效训练，以解决特征层面的跨传感器退化问题。此外，该方法采用分块操作，利用部分补丁进行训练，并对所有补丁进行并行推理以提高效率。", "result": "该方法显著提升了模型在跨传感器场景下的泛化能力，并实现了极低的泛化成本（亚秒级训练和推理，仅需部分测试输入，无需外部数据）。在真实世界多数据集实验中，该方法在处理跨传感器退化方面达到了最先进的质量和效率，例如，在RTX 3090 GPU上，512x512x8图像的训练和推理仅需0.2秒，4000x4000x8图像仅需3秒，比零样本方法快100多倍。", "conclusion": "所提出的方法通过在关键接口处引入特征定制器并采用高效的无监督训练和分块操作，成功解决了深度学习全色锐化模型在跨传感器泛化能力差且成本高的问题，实现了卓越的性能和效率。"}}
{"id": "2508.07372", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07372", "abs": "https://arxiv.org/abs/2508.07372", "authors": ["Rajaei Khatib", "Raja Giryes"], "title": "DIP-GS: Deep Image Prior For Gaussian Splatting Sparse View Recovery", "comment": null, "summary": "3D Gaussian Splatting (3DGS) is a leading 3D scene reconstruction method,\nobtaining high-quality reconstruction with real-time rendering runtime\nperformance. The main idea behind 3DGS is to represent the scene as a\ncollection of 3D gaussians, while learning their parameters to fit the given\nviews of the scene. While achieving superior performance in the presence of\nmany views, 3DGS struggles with sparse view reconstruction, where the input\nviews are sparse and do not fully cover the scene and have low overlaps. In\nthis paper, we propose DIP-GS, a Deep Image Prior (DIP) 3DGS representation. By\nusing the DIP prior, which utilizes internal structure and patterns, with\ncoarse-to-fine manner, DIP-based 3DGS can operate in scenarios where vanilla\n3DGS fails, such as sparse view recovery. Note that our approach does not use\nany pre-trained models such as generative models and depth estimation, but\nrather relies only on the input frames. Among such methods, DIP-GS obtains\nstate-of-the-art (SOTA) competitive results on various sparse-view\nreconstruction tasks, demonstrating its capabilities.", "AI": {"tldr": "本文提出DIP-GS，将深度图像先验（DIP）与3D Gaussian Splatting结合，并通过粗到精的方式，解决了3DGS在稀疏视角重建中的不足，无需预训练模型即可实现SOTA性能。", "motivation": "3D Gaussian Splatting（3DGS）在多视角场景重建中表现出色，但在输入视图稀疏、覆盖不全且重叠度低的稀疏视角重建任务中表现不佳。", "method": "本文提出DIP-GS，一种基于深度图像先验（DIP）的3DGS表示。该方法利用DIP的内部结构和模式，并采用从粗到精的方式进行优化，仅依赖输入图像，不使用任何预训练模型（如生成模型或深度估计）。", "result": "DIP-GS在各种稀疏视角重建任务上取得了与现有最先进（SOTA）方法相当的竞争性结果。", "conclusion": "DIP-GS成功解决了3DGS在稀疏视角重建中的挑战，通过利用深度图像先验，实现了高质量的重建，且无需外部预训练模型。"}}
{"id": "2508.07401", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07401", "abs": "https://arxiv.org/abs/2508.07401", "authors": ["Rui Chen", "Xingyu Chen", "Shaoan Wang", "Shihan Kong", "Junzhi Yu"], "title": "LET-US: Long Event-Text Understanding of Scenes", "comment": null, "summary": "Event cameras output event streams as sparse, asynchronous data with\nmicrosecond-level temporal resolution, enabling visual perception with low\nlatency and a high dynamic range. While existing Multimodal Large Language\nModels (MLLMs) have achieved significant success in understanding and analyzing\nRGB video content, they either fail to interpret event streams effectively or\nremain constrained to very short sequences. In this paper, we introduce LET-US,\na framework for long event-stream--text comprehension that employs an adaptive\ncompression mechanism to reduce the volume of input events while preserving\ncritical visual details. LET-US thus establishes a new frontier in cross-modal\ninferential understanding over extended event sequences. To bridge the\nsubstantial modality gap between event streams and textual representations, we\nadopt a two-stage optimization paradigm that progressively equips our model\nwith the capacity to interpret event-based scenes. To handle the voluminous\ntemporal information inherent in long event streams, we leverage text-guided\ncross-modal queries for feature reduction, augmented by hierarchical clustering\nand similarity computation to distill the most representative event features.\nMoreover, we curate and construct a large-scale event-text aligned dataset to\ntrain our model, achieving tighter alignment of event features within the LLM\nembedding space. We also develop a comprehensive benchmark covering a diverse\nset of tasks -- reasoning, captioning, classification, temporal localization\nand moment retrieval. Experimental results demonstrate that LET-US outperforms\nprior state-of-the-art MLLMs in both descriptive accuracy and semantic\ncomprehension on long-duration event streams. All datasets, codes, and models\nwill be publicly available.", "AI": {"tldr": "本文提出了LET-US框架，通过自适应压缩和两阶段优化，实现了对长事件流的跨模态理解，并构建了大规模事件-文本对齐数据集和综合基准测试。", "motivation": "现有多模态大语言模型（MLLMs）在理解RGB视频内容方面表现出色，但无法有效解释事件流或仅限于短序列，而事件相机具有低延迟和高动态范围的优势。", "method": "提出了LET-US框架，采用自适应压缩机制减少事件输入量并保留关键视觉细节。采用两阶段优化范式弥合事件流与文本表示之间的模态差距。利用文本引导的跨模态查询、分层聚类和相似性计算进行特征降维。构建了一个大规模事件-文本对齐数据集，并开发了包含推理、字幕、分类、时间定位和时刻检索等任务的综合基准。", "result": "LET-US在长时事件流的描述准确性和语义理解方面，均优于现有最先进的MLLMs。", "conclusion": "LET-US在长事件序列的跨模态推理理解方面开辟了新领域，并提供了可公开获取的数据集、代码和模型。"}}
{"id": "2508.07402", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07402", "abs": "https://arxiv.org/abs/2508.07402", "authors": ["Rongxuan Peng", "Shunquan Tan", "Chenqi Kong", "Anwei Luo", "Alex C. Kot", "Jiwu Huang"], "title": "ForensicsSAM: Toward Robust and Unified Image Forgery Detection and Localization Resisting to Adversarial Attack", "comment": null, "summary": "Parameter-efficient fine-tuning (PEFT) has emerged as a popular strategy for\nadapting large vision foundation models, such as the Segment Anything Model\n(SAM) and LLaVA, to downstream tasks like image forgery detection and\nlocalization (IFDL). However, existing PEFT-based approaches overlook their\nvulnerability to adversarial attacks. In this paper, we show that highly\ntransferable adversarial images can be crafted solely via the upstream model,\nwithout accessing the downstream model or training data, significantly\ndegrading the IFDL performance. To address this, we propose ForensicsSAM, a\nunified IFDL framework with built-in adversarial robustness. Our design is\nguided by three key ideas: (1) To compensate for the lack of forgery-relevant\nknowledge in the frozen image encoder, we inject forgery experts into each\ntransformer block to enhance its ability to capture forgery artifacts. These\nforgery experts are always activated and shared across any input images. (2) To\ndetect adversarial images, we design an light-weight adversary detector that\nlearns to capture structured, task-specific artifact in RGB domain, enabling\nreliable discrimination across various attack methods. (3) To resist\nadversarial attacks, we inject adversary experts into the global attention\nlayers and MLP modules to progressively correct feature shifts induced by\nadversarial noise. These adversary experts are adaptively activated by the\nadversary detector, thereby avoiding unnecessary interference with clean\nimages. Extensive experiments across multiple benchmarks demonstrate that\nForensicsSAM achieves superior resistance to various adversarial attack\nmethods, while also delivering state-of-the-art performance in image-level\nforgery detection and pixel-level forgery localization. The resource is\navailable at https://github.com/siriusPRX/ForensicsSAM.", "AI": {"tldr": "现有基于PEFT的图像伪造检测与定位（IFDL）方法易受对抗性攻击。本文提出ForensicsSAM，一个具有内置对抗鲁棒性的统一IFDL框架，并通过注入伪造专家和对抗专家来增强性能和鲁棒性。", "motivation": "参数高效微调（PEFT）已成为适应大型视觉基础模型（如SAM和LLaVA）到下游任务（如IFDL）的流行策略，但现有基于PEFT的方法忽视了其对抗性攻击的脆弱性。即使不访问下游模型或训练数据，仅通过上游模型即可生成高度可迁移的对抗性图像，严重降低IFDL性能。", "method": "本文提出ForensicsSAM，其设计基于三个关键思想：1) 注入伪造专家到每个Transformer块中，以增强对伪造痕迹的捕获能力，弥补冻结图像编码器中伪造相关知识的不足。2) 设计一个轻量级对抗检测器，学习捕获RGB域中结构化的、任务特定的痕迹，以可靠区分各种攻击方法。3) 注入对抗专家到全局注意力层和MLP模块中，以逐步纠正对抗噪声引起的特征偏移，这些专家由对抗检测器自适应激活，避免对干净图像的不必要干扰。", "result": "ForensicsSAM在多个基准测试中表现出对各种对抗性攻击方法的卓越抵抗力，同时在图像级伪造检测和像素级伪造定位方面也达到了最先进的性能。", "conclusion": "ForensicsSAM成功解决了基于PEFT的IFDL模型在对抗性攻击下的脆弱性问题，并在保持高检测定位性能的同时，显著提升了模型的对抗鲁棒性。"}}
{"id": "2508.07409", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07409", "abs": "https://arxiv.org/abs/2508.07409", "authors": ["Junyao Gao", "Jiaxing Li", "Wenran Liu", "Yanhong Zeng", "Fei Shen", "Kai Chen", "Yanan Sun", "Cairong Zhao"], "title": "CharacterShot: Controllable and Consistent 4D Character Animation", "comment": "13 pages, 10 figures. Code at https://github.com/Jeoyal/CharacterShot", "summary": "In this paper, we propose \\textbf{CharacterShot}, a controllable and\nconsistent 4D character animation framework that enables any individual\ndesigner to create dynamic 3D characters (i.e., 4D character animation) from a\nsingle reference character image and a 2D pose sequence. We begin by\npretraining a powerful 2D character animation model based on a cutting-edge\nDiT-based image-to-video model, which allows for any 2D pose sequnce as\ncontrollable signal. We then lift the animation model from 2D to 3D through\nintroducing dual-attention module together with camera prior to generate\nmulti-view videos with spatial-temporal and spatial-view consistency. Finally,\nwe employ a novel neighbor-constrained 4D gaussian splatting optimization on\nthese multi-view videos, resulting in continuous and stable 4D character\nrepresentations. Moreover, to improve character-centric performance, we\nconstruct a large-scale dataset Character4D, containing 13,115 unique\ncharacters with diverse appearances and motions, rendered from multiple\nviewpoints. Extensive experiments on our newly constructed benchmark,\nCharacterBench, demonstrate that our approach outperforms current\nstate-of-the-art methods. Code, models, and datasets will be publicly available\nat https://github.com/Jeoyal/CharacterShot.", "AI": {"tldr": "CharacterShot是一个可控、一致的4D角色动画框架，能从单张角色图像和2D姿态序列生成动态3D角色，并通过大规模数据集和新优化方法实现SOTA性能。", "motivation": "旨在使任何个人设计师都能从简单的输入（一张参考图像和2D姿态序列）创建动态的3D角色动画（即4D角色动画），从而降低4D角色动画制作的门槛。", "method": "1. 基于DiT的图像到视频模型预训练一个强大的2D角色动画模型，以2D姿态序列作为可控信号。2. 引入双注意力模块和相机先验，将2D动画模型提升到3D，生成具有时空和空间视图一致性的多视角视频。3. 对这些多视角视频应用新颖的邻居约束4D高斯泼溅优化，生成连续稳定的4D角色表示。4. 构建大规模Character4D数据集（包含13,115个独特角色，多视角渲染）以提升以角色为中心的性能。", "result": "该方法能够生成连续且稳定的4D角色表示。在新建的CharacterBench基准测试中，CharacterShot的表现优于当前最先进的方法。代码、模型和数据集将公开可用。", "conclusion": "CharacterShot提供了一个高效且高质量的解决方案，使个人设计师能够便捷地从有限输入创建可控、一致的4D角色动画，并在性能上超越现有技术。"}}
{"id": "2508.07413", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07413", "abs": "https://arxiv.org/abs/2508.07413", "authors": ["Youqi Wang", "Shunquan Tan", "Rongxuan Peng", "Bin Li", "Jiwu Huang"], "title": "CLUE: Leveraging Low-Rank Adaptation to Capture Latent Uncovered Evidence for Image Forgery Localization", "comment": null, "summary": "The increasing accessibility of image editing tools and generative AI has led\nto a proliferation of visually convincing forgeries, compromising the\nauthenticity of digital media. In this paper, in addition to leveraging\ndistortions from conventional forgeries, we repurpose the mechanism of a\nstate-of-the-art (SOTA) text-to-image synthesis model by exploiting its\ninternal generative process, turning it into a high-fidelity forgery\nlocalization tool. To this end, we propose CLUE (Capture Latent Uncovered\nEvidence), a framework that employs Low- Rank Adaptation (LoRA) to\nparameter-efficiently reconfigure Stable Diffusion 3 (SD3) as a forensic\nfeature extractor. Our approach begins with the strategic use of SD3's\nRectified Flow (RF) mechanism to inject noise at varying intensities into the\nlatent representation, thereby steering the LoRAtuned denoising process to\namplify subtle statistical inconsistencies indicative of a forgery. To\ncomplement the latent analysis with high-level semantic context and precise\nspatial details, our method incorporates contextual features from the image\nencoder of the Segment Anything Model (SAM), which is parameter-efficiently\nadapted to better trace the boundaries of forged regions. Extensive evaluations\ndemonstrate CLUE's SOTA generalization performance, significantly outperforming\nprior methods. Furthermore, CLUE shows superior robustness against common\npost-processing attacks and Online Social Networks (OSNs). Code is publicly\navailable at https://github.com/SZAISEC/CLUE.", "AI": {"tldr": "该论文提出了CLUE框架，通过重新利用SOTA文生图模型（如Stable Diffusion 3）的内部生成机制，并结合LoRA和SAM，将其转化为一个高保真伪造定位工具，以应对数字媒体中日益增多的视觉伪造。", "motivation": "图像编辑工具和生成式AI的普及导致了大量视觉上令人信服的伪造图像出现，这损害了数字媒体的真实性，因此需要有效的伪造检测和定位工具。", "method": "该方法名为CLUE（Capture Latent Uncovered Evidence），它利用LoRA对Stable Diffusion 3（SD3）进行参数高效的重配置，使其成为一个取证特征提取器。具体而言，它策略性地使用SD3的Rectified Flow（RF）机制向潜在表示中注入不同强度的噪声，以放大伪造的统计不一致性。此外，为了补充潜在分析并提供语义上下文和空间细节，该方法还整合了Segment Anything Model（SAM）的图像编码器中的上下文特征，并进行了参数高效的适应，以更好地追踪伪造区域的边界。", "result": "CLUE在泛化性能方面表现出SOTA水平，显著优于现有方法。此外，CLUE对常见的后处理攻击和在线社交网络（OSNs）表现出卓越的鲁棒性。", "conclusion": "CLUE框架成功地将SOTA文生图模型的内部机制转化为一个高效、高保真的伪造定位工具，并在泛化性和鲁棒性方面取得了领先成果，有效应对了数字媒体真实性面临的挑战。"}}
{"id": "2508.07441", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07441", "abs": "https://arxiv.org/abs/2508.07441", "authors": ["Yuxin Zhang", "Yunkang Cao", "Yuqi Cheng", "Yihan Sun", "Weiming Shen"], "title": "Levarging Learning Bias for Noisy Anomaly Detection", "comment": null, "summary": "This paper addresses the challenge of fully unsupervised image anomaly\ndetection (FUIAD), where training data may contain unlabeled anomalies.\nConventional methods assume anomaly-free training data, but real-world\ncontamination leads models to absorb anomalies as normal, degrading detection\nperformance. To mitigate this, we propose a two-stage framework that\nsystematically exploits inherent learning bias in models. The learning bias\nstems from: (1) the statistical dominance of normal samples, driving models to\nprioritize learning stable normal patterns over sparse anomalies, and (2)\nfeature-space divergence, where normal data exhibit high intra-class\nconsistency while anomalies display high diversity, leading to unstable model\nresponses. Leveraging the learning bias, stage 1 partitions the training set\ninto subsets, trains sub-models, and aggregates cross-model anomaly scores to\nfilter a purified dataset. Stage 2 trains the final detector on this dataset.\nExperiments on the Real-IAD benchmark demonstrate superior anomaly detection\nand localization performance under different noise conditions. Ablation studies\nfurther validate the framework's contamination resilience, emphasizing the\ncritical role of learning bias exploitation. The model-agnostic design ensures\ncompatibility with diverse unsupervised backbones, offering a practical\nsolution for real-world scenarios with imperfect training data. Code is\navailable at https://github.com/hustzhangyuxin/LLBNAD.", "AI": {"tldr": "本文提出一个两阶段框架，利用模型固有的学习偏差来解决全无监督图像异常检测（FUIAD）中训练数据可能包含未标记异常的问题，从而提高检测性能。", "motivation": "传统的异常检测方法假设训练数据不含异常，但在真实世界中，训练数据常被未标记异常污染，导致模型将异常误识别为正常，从而降低检测性能。", "method": "提出一个两阶段框架：第一阶段利用模型的学习偏差（正常样本的统计优势和特征空间分歧）将训练集划分为子集，训练子模型，并聚合交叉模型异常分数以过滤出纯净数据集。第二阶段在此纯净数据集上训练最终检测器。该设计是模型无关的。", "result": "在Real-IAD基准测试中，该方法在不同噪声条件下均表现出卓越的异常检测和定位性能。消融研究进一步验证了框架的抗污染能力，强调了学习偏差利用的关键作用。", "conclusion": "该框架通过利用模型固有的学习偏差，有效解决了训练数据受污染的全无监督图像异常检测问题，提供了实用的解决方案，并与多种无监督骨干网络兼容。"}}
{"id": "2508.07450", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07450", "abs": "https://arxiv.org/abs/2508.07450", "authors": ["Suman Kunwar", "Prabesh Rai"], "title": "Health Care Waste Classification Using Deep Learning Aligned with Nepal's Bin Color Guidelines", "comment": "7 pages, 5 figures", "summary": "The increasing number of Health Care facilities in Nepal has also added up\nthe challenges on managing health care waste (HCW). Improper segregation and\ndisposal of HCW leads to the contamination, spreading of infectious diseases\nand puts a risk of waste handlers. This study benchmarks the state of the art\nwaste classification models: ResNeXt-50, EfficientNet-B0, MobileNetV3-S,\nYOLOv8-n and YOLOv5-s using Stratified K-fold techniques where we use 5 folds\non combined HCW data, and found that the YOLOv5-s achieved higher of 95.06%\naccuracy but fell short few milliseconds in inference speed with YOLOv8-n\nmodel. The EfficientNet-B0 showed promising results of 93.22% accuracy but took\nthe highest inference time. A repetitive ANOVA was performed to see statistical\nsignificance and the best performing model (YOLOv5-s) was deployed to the web\nwith mapped bin color using Nepal's HCW management standards for public usage.\nFurther work on the data was suggested along with localized context.", "AI": {"tldr": "该研究评估了多种深度学习模型在尼泊尔医疗废物分类中的性能，发现YOLOv5-s表现最佳，并将其部署到网络平台。", "motivation": "尼泊尔医疗机构数量增加导致医疗废物管理挑战，不当分类和处置会引起污染、疾病传播并危及废物处理人员。", "method": "研究使用了分层K折交叉验证（5折）对ResNeXt-50、EfficientNet-B0、MobileNetV3-S、YOLOv8-n和YOLOv5-s等废物分类模型进行基准测试，并对结果进行了重复方差分析（ANOVA）。表现最佳的模型（YOLOv5-s）被部署到网络平台，并根据尼泊尔的医疗废物管理标准映射了垃圾桶颜色。", "result": "YOLOv5-s模型实现了最高的95.06%准确率，但在推理速度上略逊于YOLOv8-n。YOLOv8-n推理速度更快。EfficientNet-B0也取得了93.22%的不错准确率，但推理时间最长。", "conclusion": "YOLOv5-s是表现最佳的模型，已根据尼泊尔医疗废物管理标准部署到网络平台供公众使用。研究建议未来在数据和本地化背景方面做进一步工作。"}}
{"id": "2508.07470", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07470", "abs": "https://arxiv.org/abs/2508.07470", "authors": ["Siminfar Samakoush Galougah", "Rishie Raj", "Sanjoy Chowdhury", "Sayan Nag", "Ramani Duraiswami"], "title": "AURA: A Fine-Grained Benchmark and Decomposed Metric for Audio-Visual Reasoning", "comment": null, "summary": "Current audio-visual (AV) benchmarks focus on final answer accuracy,\noverlooking the underlying reasoning process. This makes it difficult to\ndistinguish genuine comprehension from correct answers derived through flawed\nreasoning or hallucinations. To address this, we introduce AURA (Audio-visual\nUnderstanding and Reasoning Assessment), a benchmark for evaluating the\ncross-modal reasoning capabilities of Audio-Visual Large Language Models\n(AV-LLMs) and Omni-modal Language Models (OLMs). AURA includes questions across\nsix challenging cognitive domains, such as causality, timbre and pitch, tempo\nand AV synchronization, unanswerability, implicit distractions, and skill\nprofiling, explicitly designed to be unanswerable from a single modality. This\nforces models to construct a valid logical path grounded in both audio and\nvideo, setting AURA apart from AV datasets that allow uni-modal shortcuts. To\nassess reasoning traces, we propose a novel metric, AuraScore, which addresses\nthe lack of robust tools for evaluating reasoning fidelity. It decomposes\nreasoning into two aspects: (i) Factual Consistency - whether reasoning is\ngrounded in perceptual evidence, and (ii) Core Inference - the logical validity\nof each reasoning step. Evaluations of SOTA models on AURA reveal a critical\nreasoning gap: although models achieve high accuracy (up to 92% on some tasks),\ntheir Factual Consistency and Core Inference scores fall below 45%. This\ndiscrepancy highlights that models often arrive at correct answers through\nflawed logic, underscoring the need for our benchmark and paving the way for\nmore robust multimodal evaluation.", "AI": {"tldr": "该论文引入了AURA基准和AuraScore指标，用于评估音视频大语言模型（AV-LLMs）的跨模态推理能力，发现现有模型即使答案准确率高，其推理过程仍存在严重缺陷。", "motivation": "当前的音视频基准只关注最终答案的准确性，忽略了底层的推理过程，导致难以区分真正的理解与基于错误推理或幻觉得出的正确答案。", "method": "引入了AURA（音视频理解与推理评估）基准，包含六个认知领域的跨模态推理问题，这些问题被设计为无法通过单一模态回答。提出了AuraScore新度量标准，用于评估推理的保真度，将其分解为事实一致性（Factual Consistency）和核心推理（Core Inference）两个方面。", "result": "对现有先进模型的评估显示，尽管模型在某些任务上实现了高达92%的准确率，但其事实一致性和核心推理得分均低于45%。这表明模型常常通过有缺陷的逻辑得出正确答案。", "conclusion": "AV-LLMs存在一个关键的推理鸿沟，模型在推理过程中存在缺陷，凸显了像AURA这样的基准对于更鲁棒的多模态评估的必要性。"}}
{"id": "2508.07493", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07493", "abs": "https://arxiv.org/abs/2508.07493", "authors": ["Jian Chen", "Ming Li", "Jihyung Kil", "Chenguang Wang", "Tong Yu", "Ryan Rossi", "Tianyi Zhou", "Changyou Chen", "Ruiyi Zhang"], "title": "VisR-Bench: An Empirical Study on Visual Retrieval-Augmented Generation for Multilingual Long Document Understanding", "comment": "Under Review", "summary": "Most organizational data in this world are stored as documents, and visual\nretrieval plays a crucial role in unlocking the collective intelligence from\nall these documents. However, existing benchmarks focus on English-only\ndocument retrieval or only consider multilingual question-answering on a\nsingle-page image. To bridge this gap, we introduce VisR-Bench, a multilingual\nbenchmark designed for question-driven multimodal retrieval in long documents.\nOur benchmark comprises over 35K high-quality QA pairs across 1.2K documents,\nenabling fine-grained evaluation of multimodal retrieval. VisR-Bench spans\nsixteen languages with three question types (figures, text, and tables),\noffering diverse linguistic and question coverage. Unlike prior datasets, we\ninclude queries without explicit answers, preventing models from relying on\nsuperficial keyword matching. We evaluate various retrieval models, including\ntext-based methods, multimodal encoders, and MLLMs, providing insights into\ntheir strengths and limitations. Our results show that while MLLMs\nsignificantly outperform text-based and multimodal encoder models, they still\nstruggle with structured tables and low-resource languages, highlighting key\nchallenges in multilingual visual retrieval.", "AI": {"tldr": "本文提出了VisR-Bench，一个多语言、问题驱动、多模态长文档检索基准，旨在解决现有基准的局限性，并评估不同模型的性能。", "motivation": "现有文档检索基准主要关注英语文档或单页图像上的多语言问答，无法满足从长文档中解锁集体智能的需求。这促使研究者创建一个更全面、多语言、多模态的基准。", "method": "研究者引入了VisR-Bench基准，包含超过3.5万个高质量问答对，覆盖1.2千个文档和16种语言，涵盖图表、文本和表格三种问题类型。该基准还包括无明确答案的查询，以防止模型过度依赖关键词匹配。研究者评估了多种检索模型，包括基于文本的方法、多模态编码器和多模态大语言模型（MLLMs）。", "result": "评估结果显示，多模态大语言模型（MLLMs）显著优于基于文本和多模态编码器模型。然而，它们在处理结构化表格和低资源语言方面仍然面临挑战。", "conclusion": "VisR-Bench揭示了多语言视觉检索领域的关键挑战，特别是多模态大语言模型在处理复杂结构化数据和低资源语言时的不足，为未来研究指明了方向。"}}
{"id": "2508.07501", "categories": ["cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.07501", "abs": "https://arxiv.org/abs/2508.07501", "authors": ["Xiaoye Zuo", "Nikos Athanasiou", "Ginger Delmas", "Yiming Huang", "Xingyu Fu", "Lingjie Liu"], "title": "FormCoach: Lift Smarter, Not Harder", "comment": null, "summary": "Good form is the difference between strength and strain, yet for the\nfast-growing community of at-home fitness enthusiasts, expert feedback is often\nout of reach. FormCoach transforms a simple camera into an always-on,\ninteractive AI training partner, capable of spotting subtle form errors and\ndelivering tailored corrections in real time, leveraging vision-language models\n(VLMs). We showcase this capability through a web interface and benchmark\nstate-of-the-art VLMs on a dataset of 1,700 expert-annotated user-reference\nvideo pairs spanning 22 strength and mobility exercises. To accelerate research\nin AI-driven coaching, we release both the dataset and an automated,\nrubric-based evaluation pipeline, enabling standardized comparison across\nmodels. Our benchmarks reveal substantial gaps compared to human-level\ncoaching, underscoring both the challenges and opportunities in integrating\nnuanced, context-aware movement analysis into interactive AI systems. By\nframing form correction as a collaborative and creative process between humans\nand machines, FormCoach opens a new frontier in embodied AI.", "AI": {"tldr": "FormCoach将普通摄像头转变为AI健身教练，利用视觉-语言模型（VLMs）实时纠正家庭健身用户的动作姿势，并发布了相关数据集和评估工具。", "motivation": "家庭健身爱好者难以获得专业的姿势反馈，这可能导致训练效果不佳甚至受伤，而非力量增长。", "method": "开发了FormCoach系统，通过摄像头和视觉-语言模型（VLMs）识别细微姿势错误并提供实时个性化纠正。构建了一个包含22种力量和柔韧性练习、1700对专家标注的用户-参考视频的数据集，并建立了基于评分标准的自动化评估流程，以实现模型间的标准化比较。", "result": "基准测试显示，与人类教练水平相比，当前VLMs在姿势纠正方面仍存在显著差距，这揭示了将细致、情境感知运动分析整合到交互式AI系统中的挑战和机遇。", "conclusion": "FormCoach将姿势纠正视为人机协作的创意过程，为具身AI开辟了新领域，尽管当前AI性能与人类水平仍有差距，但未来潜力巨大。"}}
{"id": "2508.07519", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07519", "abs": "https://arxiv.org/abs/2508.07519", "authors": ["Joonghyuk Shin", "Alchan Hwang", "Yujin Kim", "Daneul Kim", "Jaesik Park"], "title": "Exploring Multimodal Diffusion Transformers for Enhanced Prompt-based Image Editing", "comment": "ICCV 2025. Project webpage:\n  https://joonghyuk.com/exploring-mmdit-web/", "summary": "Transformer-based diffusion models have recently superseded traditional U-Net\narchitectures, with multimodal diffusion transformers (MM-DiT) emerging as the\ndominant approach in state-of-the-art models like Stable Diffusion 3 and\nFlux.1. Previous approaches have relied on unidirectional cross-attention\nmechanisms, with information flowing from text embeddings to image latents. In\ncontrast, MMDiT introduces a unified attention mechanism that concatenates\ninput projections from both modalities and performs a single full attention\noperation, allowing bidirectional information flow between text and image\nbranches. This architectural shift presents significant challenges for existing\nediting techniques. In this paper, we systematically analyze MM-DiT's attention\nmechanism by decomposing attention matrices into four distinct blocks,\nrevealing their inherent characteristics. Through these analyses, we propose a\nrobust, prompt-based image editing method for MM-DiT that supports global to\nlocal edits across various MM-DiT variants, including few-step models. We\nbelieve our findings bridge the gap between existing U-Net-based methods and\nemerging architectures, offering deeper insights into MMDiT's behavioral\npatterns.", "AI": {"tldr": "本文分析了多模态扩散Transformer (MM-DiT) 的统一注意力机制，并提出了一种稳健的基于提示的图像编辑方法，以解决现有编辑技术在新型MM-DiT架构上的挑战。", "motivation": "Transformer-based扩散模型，特别是MM-DiT，已取代传统U-Net成为主流，但其统一的双向注意力机制对现有图像编辑技术提出了显著挑战。研究旨在理解MM-DiT的行为模式并开发适用于其架构的编辑方法。", "method": "通过将MM-DiT的注意力矩阵分解为四个不同的块，系统地分析了其注意力机制，揭示了内在特性。在此分析基础上，提出了一种稳健的、基于提示的图像编辑方法。", "result": "提出了一种针对MM-DiT的稳健的基于提示的图像编辑方法，该方法支持从全局到局部的编辑，并适用于各种MM-DiT变体，包括少步模型。", "conclusion": "研究结果弥合了现有基于U-Net的方法与新兴MM-DiT架构之间的差距，为理解MM-DiT的行为模式提供了更深入的见解，并为新型扩散模型的图像编辑提供了解决方案。"}}
{"id": "2508.07528", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07528", "abs": "https://arxiv.org/abs/2508.07528", "authors": ["Xiaotong Ji", "Ryoma Bise", "Seiichi Uchida"], "title": "Enhancing Reliability of Medical Image Diagnosis through Top-rank Learning with Rejection Module", "comment": null, "summary": "In medical image processing, accurate diagnosis is of paramount importance.\nLeveraging machine learning techniques, particularly top-rank learning, shows\nsignificant promise by focusing on the most crucial instances. However,\nchallenges arise from noisy labels and class-ambiguous instances, which can\nseverely hinder the top-rank objective, as they may be erroneously placed among\nthe top-ranked instances. To address these, we propose a novel approach that\nenhances toprank learning by integrating a rejection module. Cooptimized with\nthe top-rank loss, this module identifies and mitigates the impact of outliers\nthat hinder training effectiveness. The rejection module functions as an\nadditional branch, assessing instances based on a rejection function that\nmeasures their deviation from the norm. Through experimental validation on a\nmedical dataset, our methodology demonstrates its efficacy in detecting and\nmitigating outliers, improving the reliability and accuracy of medical image\ndiagnoses.", "AI": {"tldr": "针对医学图像诊断中受噪声标签和模糊实例影响的排序学习，提出一种结合拒绝模块的新方法，以识别并减轻异常值影响，提高诊断准确性和可靠性。", "motivation": "在医学图像处理中，精确诊断至关重要。排序学习（特别是顶层排序学习）通过关注关键实例显示出巨大潜力。然而，噪声标签和类别模糊实例会严重阻碍顶层排序目标，因为它们可能被错误地排在靠前位置，从而降低训练效果。", "method": "提出一种新方法，通过集成一个拒绝模块来增强顶层排序学习。该模块与顶层排序损失共同优化，作为一个额外的分支，通过拒绝函数评估实例偏离正常的程度，以识别并减轻异常值的影响。", "result": "在医学数据集上的实验验证表明，该方法能有效检测和减轻异常值，提高了医学图像诊断的可靠性和准确性。", "conclusion": "所提出的结合拒绝模块的顶层排序学习方法，有效解决了医学图像诊断中噪声和模糊实例带来的挑战，提升了诊断的准确性和可靠性。"}}
{"id": "2508.07537", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07537", "abs": "https://arxiv.org/abs/2508.07537", "authors": ["Xiaoming Li", "Wangmeng Zuo", "Chen Change Loy"], "title": "Enhanced Generative Structure Prior for Chinese Text Image Super-resolution", "comment": "TPAMI", "summary": "Faithful text image super-resolution (SR) is challenging because each\ncharacter has a unique structure and usually exhibits diverse font styles and\nlayouts. While existing methods primarily focus on English text, less attention\nhas been paid to more complex scripts like Chinese. In this paper, we introduce\na high-quality text image SR framework designed to restore the precise strokes\nof low-resolution (LR) Chinese characters. Unlike methods that rely on\ncharacter recognition priors to regularize the SR task, we propose a novel\nstructure prior that offers structure-level guidance to enhance visual quality.\nOur framework incorporates this structure prior within a StyleGAN model,\nleveraging its generative capabilities for restoration. To maintain the\nintegrity of character structures while accommodating various font styles and\nlayouts, we implement a codebook-based mechanism that restricts the generative\nspace of StyleGAN. Each code in the codebook represents the structure of a\nspecific character, while the vector $w$ in StyleGAN controls the character's\nstyle, including typeface, orientation, and location. Through the collaborative\ninteraction between the codebook and style, we generate a high-resolution\nstructure prior that aligns with LR characters both spatially and structurally.\nExperiments demonstrate that this structure prior provides robust,\ncharacter-specific guidance, enabling the accurate restoration of clear strokes\nin degraded characters, even for real-world LR Chinese text with irregular\nlayouts. Our code and pre-trained models will be available at\nhttps://github.com/csxmli2016/MARCONetPlusPlus", "AI": {"tldr": "提出一种基于StyleGAN和码本结构先验的中文文本图像超分辨率框架，能够精确恢复低分辨率中文文字的笔画。", "motivation": "现有文本图像超分辨率方法主要关注英文，对中文等复杂文字的精确笔画恢复关注较少。中文文字具有独特的结构、多样的字体风格和布局，使得高质量恢复极具挑战性。", "method": "引入一个高质量的中文文本图像超分辨率框架，该框架将新颖的结构先验融入StyleGAN模型。通过一个基于码本的机制来限制StyleGAN的生成空间，码本中的每个代码代表一个字符结构，StyleGAN的`w`向量控制字符的风格（如字体、方向和位置）。码本和风格的协同作用生成高分辨率结构先验，与低分辨率字符在空间和结构上对齐。", "result": "实验证明，所提出的结构先验能够提供鲁棒的、字符特定的指导，从而准确恢复退化字符的清晰笔画，即使对于具有不规则布局的真实世界低分辨率中文文本也表现出色。", "conclusion": "该研究通过结合结构先验和StyleGAN的生成能力，有效解决了中文文本图像超分辨率的挑战，为复杂脚本的文本图像恢复提供了新的有效途径。"}}
{"id": "2508.07539", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07539", "abs": "https://arxiv.org/abs/2508.07539", "authors": ["Yuki Shigeyasu", "Shota Harada", "Akihiko Yoshizawa", "Kazuhiro Terada", "Naoki Nakazima", "Mariyo Kurata", "Hiroyuki Abe", "Tetsuo Ushiku", "Ryoma Bise"], "title": "Domain Generalization of Pathological Image Segmentation by Patch-Level and WSI-Level Contrastive Learning", "comment": null, "summary": "In this paper, we address domain shifts in pathological images by focusing on\nshifts within whole slide images~(WSIs), such as patient characteristics and\ntissue thickness, rather than shifts between hospitals. Traditional approaches\nrely on multi-hospital data, but data collection challenges often make this\nimpractical. Therefore, the proposed domain generalization method captures and\nleverages intra-hospital domain shifts by clustering WSI-level features from\nnon-tumor regions and treating these clusters as domains. To mitigate domain\nshift, we apply contrastive learning to reduce feature gaps between WSI pairs\nfrom different clusters. The proposed method introduces a two-stage contrastive\nlearning approach WSI-level and patch-level contrastive learning to minimize\nthese gaps effectively.", "AI": {"tldr": "本文提出了一种域泛化方法，通过聚类WSI特征识别院内域漂移，并采用两阶段对比学习来有效减少病理图像中的特征差距。", "motivation": "传统域泛化方法依赖于难以收集的多医院数据。本研究旨在解决病理图像中更精细的院内域漂移问题，例如患者特征和组织厚度，这些漂移常被忽略但对模型性能有显著影响。", "method": "该方法通过聚类非肿瘤区域的WSI级别特征来捕获和利用院内域漂移，并将这些聚类视为不同的域。为缓解域漂移，引入了一种两阶段对比学习方法（WSI级别和斑块级别），以减少来自不同聚类（域）的WSI对之间的特征差距。", "result": "该方法能够有效捕获和利用院内域漂移，并通过所提出的两阶段对比学习有效地最小化了不同域之间的特征差距。", "conclusion": "所提出的方法通过识别和利用院内域漂移，并结合创新的两阶段对比学习策略，有效解决了病理图像中的域漂移问题。"}}
{"id": "2508.07540", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07540", "abs": "https://arxiv.org/abs/2508.07540", "authors": ["Junuk Cha", "Jihyeon Kim"], "title": "CoT-Pose: Chain-of-Thought Reasoning for 3D Pose Generation from Abstract Prompts", "comment": "ICCVW'25", "summary": "Recent advances in multi-modal large language models (MLLMs) and\nchain-of-thought (CoT) reasoning have led to significant progress in image and\ntext generation tasks. However, the field of 3D human pose generation still\nfaces critical limitations. Most existing text-to-pose models rely heavily on\ndetailed (low-level) prompts that explicitly describe joint configurations. In\ncontrast, humans tend to communicate actions and intentions using abstract\n(high-level) language. This mismatch results in a practical challenge for\ndeploying pose generation systems in real-world scenarios. To bridge this gap,\nwe introduce a novel framework that incorporates CoT reasoning into the pose\ngeneration process, enabling the interpretation of abstract prompts into\naccurate 3D human poses. We further propose a data synthesis pipeline that\nautomatically generates triplets of abstract prompts, detailed prompts, and\ncorresponding 3D poses for training process. Experimental results demonstrate\nthat our reasoning-enhanced model, CoT-Pose, can effectively generate plausible\nand semantically aligned poses from abstract textual inputs. This work\nhighlights the importance of high-level understanding in pose generation and\nopens new directions for reasoning-enhanced approach for human pose generation.", "AI": {"tldr": "该研究引入CoT推理，提出CoT-Pose模型和数据合成管道，旨在实现从抽象文本生成3D人体姿态，解决了现有模型依赖详细提示与人类高级语言习惯不符的问题。", "motivation": "现有文本到姿态模型严重依赖明确描述关节配置的详细（低级）提示，而人类倾向于使用抽象（高级）语言交流动作和意图。这种不匹配导致姿态生成系统在实际场景中部署面临挑战。", "method": "引入一个新颖的框架，将CoT推理整合到姿态生成过程中，使模型能够将抽象提示解释为准确的3D人体姿态。此外，提出一个数据合成管道，自动生成“抽象提示、详细提示和对应3D姿态”的三元组用于训练。", "result": "实验结果表明，该推理增强模型CoT-Pose能够有效地从抽象文本输入生成合理且语义对齐的姿态。", "conclusion": "这项工作突出了姿态生成中高级理解的重要性，并为人体姿态生成的推理增强方法开辟了新方向。"}}
{"id": "2508.07543", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07543", "abs": "https://arxiv.org/abs/2508.07543", "authors": ["Chidaksh Ravuru"], "title": "Commentary Generation for Soccer Highlights", "comment": null, "summary": "Automated soccer commentary generation has evolved from template-based\nsystems to advanced neural architectures, aiming to produce real-time\ndescriptions of sports events. While frameworks like SoccerNet-Caption laid\nfoundational work, their inability to achieve fine-grained alignment between\nvideo content and commentary remains a significant challenge. Recent efforts\nsuch as MatchTime, with its MatchVoice model, address this issue through coarse\nand fine-grained alignment techniques, achieving improved temporal\nsynchronization. In this paper, we extend MatchVoice to commentary generation\nfor soccer highlights using the GOAL dataset, which emphasizes short clips over\nentire games. We conduct extensive experiments to reproduce the original\nMatchTime results and evaluate our setup, highlighting the impact of different\ntraining configurations and hardware limitations. Furthermore, we explore the\neffect of varying window sizes on zero-shot performance. While MatchVoice\nexhibits promising generalization capabilities, our findings suggest the need\nfor integrating techniques from broader video-language domains to further\nenhance performance. Our code is available at\nhttps://github.com/chidaksh/SoccerCommentary.", "AI": {"tldr": "本文扩展了MatchVoice模型，用于基于GOAL数据集的足球精彩集锦评论生成，并探讨了训练配置、硬件限制和窗口大小对性能的影响，指出需结合更广泛的视频-语言技术。", "motivation": "现有的足球评论生成系统（如SoccerNet-Caption）在视频内容与评论的细粒度对齐方面存在挑战，MatchTime及其MatchVoice模型虽有所改进，但仍需进一步提升时间同步和泛化能力，尤其是在短视频集锦场景。", "method": "研究者将MatchVoice模型扩展应用于GOAL数据集（专注于短视频集锦），复现了原始MatchTime的结果，并评估了不同训练配置、硬件限制以及不同窗口大小对零样本性能的影响。", "result": "MatchVoice模型展现出良好的泛化能力，但在零样本性能方面，研究结果表明需要整合更广泛的视频-语言领域技术以进一步提升性能。同时，训练配置和硬件限制对结果有显著影响。", "conclusion": "MatchVoice在足球精彩集锦评论生成方面具有潜力，但为了实现更优的性能，未来研究应考虑引入更先进的视频-语言跨模态对齐和生成技术。"}}
{"id": "2508.07548", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07548", "abs": "https://arxiv.org/abs/2508.07548", "authors": ["Takehiro Yamane", "Itaru Tsuge", "Susumu Saito", "Ryoma Bise"], "title": "Adaptive Pseudo Label Selection for Individual Unlabeled Data by Positive and Unlabeled Learning", "comment": null, "summary": "This paper proposes a novel pseudo-labeling method for medical image\nsegmentation that can perform learning on ``individual images'' to select\neffective pseudo-labels. We introduce Positive and Unlabeled Learning (PU\nlearning), which uses only positive and unlabeled data for binary\nclassification problems, to obtain the appropriate metric for discriminating\nforeground and background regions on each unlabeled image. Our PU learning\nmakes us easy to select pseudo-labels for various background regions. The\nexperimental results show the effectiveness of our method.", "AI": {"tldr": "本文提出一种基于PU学习的医学图像分割伪标签方法，能够在单张图像上选择有效的伪标签。", "motivation": "在医学图像分割中，需要一种有效的方法来选择伪标签，尤其是在缺乏大量标注数据的情况下。现有方法可能难以有效区分前景和背景区域。", "method": "引入正样本和未标注样本学习（PU学习），在每张未标注图像上获取区分前景和背景区域的度量，从而选择适合不同背景区域的伪标签。", "result": "实验结果表明该方法是有效的。", "conclusion": "所提出的基于PU学习的伪标签方法能够有效应用于医学图像分割，并能灵活选择各种背景区域的伪标签。"}}
{"id": "2508.07552", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07552", "abs": "https://arxiv.org/abs/2508.07552", "authors": ["Ludan Zhang", "Sihan Wang", "Yuqi Dai", "Shuofei Qiao", "Lei He"], "title": "Decoupled Functional Evaluation of Autonomous Driving Models via Feature Map Quality Scoring", "comment": null, "summary": "End-to-end models are emerging as the mainstream in autonomous driving\nperception and planning. However, the lack of explicit supervision signals for\nintermediate functional modules leads to opaque operational mechanisms and\nlimited interpretability, making it challenging for traditional methods to\nindependently evaluate and train these modules. Pioneering in the issue, this\nstudy builds upon the feature map-truth representation similarity-based\nevaluation framework and proposes an independent evaluation method based on\nFeature Map Convergence Score (FMCS). A Dual-Granularity Dynamic Weighted\nScoring System (DG-DWSS) is constructed, formulating a unified quantitative\nmetric - Feature Map Quality Score - to enable comprehensive evaluation of the\nquality of feature maps generated by functional modules. A CLIP-based Feature\nMap Quality Evaluation Network (CLIP-FMQE-Net) is further developed, combining\nfeature-truth encoders and quality score prediction heads to enable real-time\nquality analysis of feature maps generated by functional modules. Experimental\nresults on the NuScenes dataset demonstrate that integrating our evaluation\nmodule into the training improves 3D object detection performance, achieving a\n3.89 percent gain in NDS. These results verify the effectiveness of our method\nin enhancing feature representation quality and overall model performance.", "AI": {"tldr": "针对自动驾驶端到端模型中间模块缺乏显式监督导致的可解释性差问题，本文提出基于特征图收敛分数（FMCS）的独立评估方法和双粒度动态加权评分系统（DG-DWSS），并开发CLIP-FMQE-Net进行实时质量分析，实验证明其能有效提升3D目标检测性能。", "motivation": "自动驾驶中的端到端模型在感知和规划方面成为主流，但中间功能模块缺乏明确的监督信号，导致操作机制不透明、可解释性受限，传统方法难以独立评估和训练这些模块。", "method": "本研究基于特征图-真值表示相似性评估框架，提出了一种基于特征图收敛分数（FMCS）的独立评估方法。构建了一个双粒度动态加权评分系统（DG-DWSS），形成统一的定量指标——特征图质量分数。进一步开发了基于CLIP的特征图质量评估网络（CLIP-FMQE-Net），结合特征-真值编码器和质量分数预测头，实现对功能模块生成特征图的实时质量分析。", "result": "在NuScenes数据集上的实验结果表明，将所提出的评估模块集成到训练中可以提高3D目标检测性能，NDS提升了3.89%。", "conclusion": "实验结果验证了所提出的方法在增强特征表示质量和提升模型整体性能方面的有效性。"}}
{"id": "2508.07557", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07557", "abs": "https://arxiv.org/abs/2508.07557", "authors": ["Minghao Yin", "Yukang Cao", "Songyou Peng", "Kai Han"], "title": "Splat4D: Diffusion-Enhanced 4D Gaussian Splatting for Temporally and Spatially Consistent Content Creation", "comment": null, "summary": "Generating high-quality 4D content from monocular videos for applications\nsuch as digital humans and AR/VR poses challenges in ensuring temporal and\nspatial consistency, preserving intricate details, and incorporating user\nguidance effectively. To overcome these challenges, we introduce Splat4D, a\nnovel framework enabling high-fidelity 4D content generation from a monocular\nvideo. Splat4D achieves superior performance while maintaining faithful\nspatial-temporal coherence by leveraging multi-view rendering, inconsistency\nidentification, a video diffusion model, and an asymmetric U-Net for\nrefinement. Through extensive evaluations on public benchmarks, Splat4D\nconsistently demonstrates state-of-the-art performance across various metrics,\nunderscoring the efficacy of our approach. Additionally, the versatility of\nSplat4D is validated in various applications such as text/image conditioned 4D\ngeneration, 4D human generation, and text-guided content editing, producing\ncoherent outcomes following user instructions.", "AI": {"tldr": "Splat4D是一种新颖的框架，能够从单目视频生成高质量的4D内容，解决了时间/空间一致性和细节保留的挑战，并支持用户引导。", "motivation": "从单目视频生成高质量4D内容（如数字人、AR/VR）面临时间/空间一致性、细节保留和有效融入用户指导的挑战。", "method": "Splat4D通过利用多视角渲染、不一致性识别、视频扩散模型和非对称U-Net进行精炼，实现了忠实时空连贯性的同时保持卓越性能。", "result": "Splat4D在公共基准测试中持续展示出最先进的性能，并在文本/图像条件4D生成、4D人体生成和文本引导内容编辑等多种应用中验证了其多功能性，能根据用户指令产生连贯结果。", "conclusion": "Splat4D是一种有效且多功能的单目视频高保真4D内容生成方法，能够解决现有挑战并支持用户引导的编辑。"}}
{"id": "2508.07570", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07570", "abs": "https://arxiv.org/abs/2508.07570", "authors": ["Khanh-Binh Nguyen", "Phuoc-Nguyen Bui", "Hyunseung Choo", "Duc Thanh Nguyen"], "title": "Adaptive Cache Enhancement for Test-Time Adaptation of Vision-Language Models", "comment": "12 pages, Under review", "summary": "Vision-language models (VLMs) exhibit remarkable zero-shot generalization but\nsuffer performance degradation under distribution shifts in downstream tasks,\nparticularly in the absence of labeled data. Test-Time Adaptation (TTA)\naddresses this challenge by enabling online optimization of VLMs during\ninference, eliminating the need for annotated data. Cache-based TTA methods\nexploit historical knowledge by maintaining a dynamic memory cache of\nlow-entropy or high-confidence samples, promoting efficient adaptation to\nout-of-distribution data. Nevertheless, these methods face two critical\nchallenges: (1) unreliable confidence metrics under significant distribution\nshifts, resulting in error accumulation within the cache and degraded\nadaptation performance; and (2) rigid decision boundaries that fail to\naccommodate substantial distributional variations, leading to suboptimal\npredictions. To overcome these limitations, we introduce the Adaptive Cache\nEnhancement (ACE) framework, which constructs a robust cache by selectively\nstoring high-confidence or low-entropy image embeddings per class, guided by\ndynamic, class-specific thresholds initialized from zero-shot statistics and\niteratively refined using an exponential moving average and\nexploration-augmented updates. This approach enables adaptive, class-wise\ndecision boundaries, ensuring robust and accurate predictions across diverse\nvisual distributions. Extensive experiments on 15 diverse benchmark datasets\ndemonstrate that ACE achieves state-of-the-art performance, delivering superior\nrobustness and generalization compared to existing TTA methods in challenging\nout-of-distribution scenarios.", "AI": {"tldr": "针对视觉语言模型（VLMs）在分布偏移下性能下降的问题，本文提出ACE框架，通过构建一个基于动态类特定阈值的自适应缓存，解决了现有缓存式测试时间自适应（TTA）方法中置信度不可靠和决策边界僵硬的问题，实现了最先进的鲁棒性和泛化能力。", "motivation": "视觉语言模型（VLMs）在下游任务的分布偏移下性能会显著下降，尤其是在缺乏标注数据时。现有的缓存式测试时间自适应（TTA）方法面临两个关键挑战：1) 在显著分布偏移下置信度指标不可靠，导致缓存中错误累积和适应性能下降；2) 僵硬的决策边界无法适应大的分布变化，导致次优预测。", "method": "本文提出了自适应缓存增强（ACE）框架。该框架通过为每个类别选择性地存储高置信度或低熵的图像嵌入来构建一个鲁棒缓存。其核心在于使用动态的、类别特定的阈值进行引导，这些阈值从零样本统计数据初始化，并通过指数移动平均和探索增强更新进行迭代细化。这种方法实现了自适应的、按类别划分的决策边界。", "result": "在15个多样化的基准数据集上进行的广泛实验表明，ACE框架实现了最先进的性能，在具有挑战性的分布外场景中，与现有TTA方法相比，展现出卓越的鲁棒性和泛化能力。", "conclusion": "ACE框架通过引入一种自适应的、类别感知的缓存机制，有效克服了现有缓存式TTA方法的局限性，显著提升了VLMs在分布偏移下的预测准确性和泛化能力。"}}
{"id": "2508.07577", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07577", "abs": "https://arxiv.org/abs/2508.07577", "authors": ["Zhaorui Tan", "Tan Pan", "Kaizhu Huang", "Weimiao Yu", "Kai Yao", "Chen Jiang", "Qiufeng Wang", "Anh Nguyen", "Xin Guo", "Yuan Cheng", "Xi Yang"], "title": "Exploiting Layer Normalization Fine-tuning in Visual Transformer Foundation Models for Classification", "comment": null, "summary": "LayerNorm is pivotal in Vision Transformers (ViTs), yet its fine-tuning\ndynamics under data scarcity and domain shifts remain underexplored. This paper\nshows that shifts in LayerNorm parameters after fine-tuning (LayerNorm shifts)\nare indicative of the transitions between source and target domains; its\nefficacy is contingent upon the degree to which the target training samples\naccurately represent the target domain, as quantified by our proposed\nFine-tuning Shift Ratio ($FSR$). Building on this, we propose a simple yet\neffective rescaling mechanism using a scalar $\\lambda$ that is negatively\ncorrelated to $FSR$ to align learned LayerNorm shifts with those ideal shifts\nachieved under fully representative data, combined with a cyclic framework that\nfurther enhances the LayerNorm fine-tuning. Extensive experiments across\nnatural and pathological images, in both in-distribution (ID) and\nout-of-distribution (OOD) settings, and various target training sample regimes\nvalidate our framework. Notably, OOD tasks tend to yield lower $FSR$ and higher\n$\\lambda$ in comparison to ID cases, especially with scarce data, indicating\nunder-represented target training samples. Moreover, ViTFs fine-tuned on\npathological data behave more like ID settings, favoring conservative LayerNorm\nupdates. Our findings illuminate the underexplored dynamics of LayerNorm in\ntransfer learning and provide practical strategies for LayerNorm fine-tuning.", "AI": {"tldr": "解析错误", "motivation": "解析错误", "method": "解析错误", "result": "解析错误", "conclusion": "解析错误"}}
{"id": "2508.07585", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07585", "abs": "https://arxiv.org/abs/2508.07585", "authors": ["Yu-Huan Wu", "Wei Liu", "Zi-Xuan Zhu", "Zizhou Wang", "Yong Liu", "Liangli Zhen"], "title": "GAPNet: A Lightweight Framework for Image and Video Salient Object Detection via Granularity-Aware Paradigm", "comment": "21 pages, 7 figures, 6 tables", "summary": "Recent salient object detection (SOD) models predominantly rely on\nheavyweight backbones, incurring substantial computational cost and hindering\ntheir practical application in various real-world settings, particularly on\nedge devices. This paper presents GAPNet, a lightweight network built on the\ngranularity-aware paradigm for both image and video SOD. We assign saliency\nmaps of different granularities to supervise the multi-scale decoder\nside-outputs: coarse object locations for high-level outputs and fine-grained\nobject boundaries for low-level outputs. Specifically, our decoder is built\nwith granularity-aware connections which fuse high-level features of low\ngranularity and low-level features of high granularity, respectively. To\nsupport these connections, we design granular pyramid convolution (GPC) and\ncross-scale attention (CSA) modules for efficient fusion of low-scale and\nhigh-scale features, respectively. On top of the encoder, a self-attention\nmodule is built to learn global information, enabling accurate object\nlocalization with negligible computational cost. Unlike traditional U-Net-based\napproaches, our proposed method optimizes feature utilization and semantic\ninterpretation while applying appropriate supervision at each processing stage.\nExtensive experiments show that the proposed method achieves a new\nstate-of-the-art performance among lightweight image and video SOD models. Code\nis available at https://github.com/yuhuan-wu/GAPNet.", "AI": {"tldr": "GAPNet是一种轻量级显著目标检测（SOD）网络，通过粒度感知范式和高效特征融合，在图像和视频SOD任务中实现了最先进的性能，解决了传统模型计算成本高的问题。", "motivation": "现有显著目标检测模型大多依赖于重量级骨干网络，导致计算成本高昂，阻碍了其在边缘设备等实际应用中的部署。", "method": "本文提出了GAPNet，一个基于粒度感知范式的轻量级网络。它为多尺度解码器侧输出分配不同粒度的显著图（高层输出粗略位置，低层输出精细边界）。解码器采用粒度感知连接，融合低粒度的高层特征和高粒度的低层特征。设计了粒度金字塔卷积（GPC）和跨尺度注意力（CSA）模块用于高效特征融合。编码器顶部构建了自注意力模块以学习全局信息。", "result": "广泛的实验表明，该方法在轻量级图像和视频显著目标检测模型中达到了新的最先进性能。", "conclusion": "GAPNet通过优化特征利用和语义解释，并在每个处理阶段应用适当的监督，提供了一种高效且准确的轻量级显著目标检测解决方案。"}}
{"id": "2508.07587", "categories": ["cs.CV", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.07587", "abs": "https://arxiv.org/abs/2508.07587", "authors": ["Sri Raksha Siva", "Nived Suthahar", "Prakash Boominathan", "Uma Ranjan"], "title": "Voice Pathology Detection Using Phonation", "comment": "17 Pages, 11 Figures", "summary": "Voice disorders significantly affect communication and quality of life,\nrequiring an early and accurate diagnosis. Traditional methods like\nlaryngoscopy are invasive, subjective, and often inaccessible. This research\nproposes a noninvasive, machine learning-based framework for detecting voice\npathologies using phonation data.\n  Phonation data from the Saarbr\\\"ucken Voice Database are analyzed using\nacoustic features such as Mel Frequency Cepstral Coefficients (MFCCs), chroma\nfeatures, and Mel spectrograms. Recurrent Neural Networks (RNNs), including\nLSTM and attention mechanisms, classify samples into normal and pathological\ncategories. Data augmentation techniques, including pitch shifting and Gaussian\nnoise addition, enhance model generalizability, while preprocessing ensures\nsignal quality. Scale-based features, such as H\\\"older and Hurst exponents,\nfurther capture signal irregularities and long-term dependencies.\n  The proposed framework offers a noninvasive, automated diagnostic tool for\nearly detection of voice pathologies, supporting AI-driven healthcare, and\nimproving patient outcomes.", "AI": {"tldr": "本研究提出了一种基于机器学习的非侵入性框架，利用声学特征和循环神经网络（RNN）自动检测语音病变，旨在提供早期诊断工具。", "motivation": "传统的喉镜检查方法具有侵入性、主观性且通常难以获取，而语音障碍严重影响沟通和生活质量，因此需要一种早期、准确且非侵入性的诊断方法。", "method": "使用Saarbrücken语音数据库的语音数据，提取梅尔频率倒谱系数（MFCCs）、色度特征、梅尔频谱图等声学特征，并引入H\"older和Hurst指数等尺度特征来捕捉信号不规则性和长期依赖性。采用循环神经网络（RNNs），包括LSTM和注意力机制进行分类。同时，使用音高偏移和高斯噪声添加等数据增强技术来提高模型泛化能力，并通过预处理确保信号质量。", "result": "该框架提供了一种非侵入性、自动化语音病变早期检测诊断工具。", "conclusion": "所提出的框架支持人工智能驱动的医疗保健，并通过早期检测改善患者预后。"}}
{"id": "2508.07596", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07596", "abs": "https://arxiv.org/abs/2508.07596", "authors": ["Shahroz Tariq", "Simon S. Woo", "Priyanka Singh", "Irena Irmalasari", "Saakshi Gupta", "Dev Gupta"], "title": "From Prediction to Explanation: Multimodal, Explainable, and Interactive Deepfake Detection Framework for Non-Expert Users", "comment": "11 pages, 3 tables, 5 figures, accepted for publicaiton in the 33rd\n  ACM International Conference on Multimedia (MM '25), October 27-31, 2025,\n  Dublin, Ireland", "summary": "The proliferation of deepfake technologies poses urgent challenges and\nserious risks to digital integrity, particularly within critical sectors such\nas forensics, journalism, and the legal system. While existing detection\nsystems have made significant progress in classification accuracy, they\ntypically function as black-box models, offering limited transparency and\nminimal support for human reasoning. This lack of interpretability hinders\ntheir usability in real-world decision-making contexts, especially for\nnon-expert users. In this paper, we present DF-P2E (Deepfake: Prediction to\nExplanation), a novel multimodal framework that integrates visual, semantic,\nand narrative layers of explanation to make deepfake detection interpretable\nand accessible. The framework consists of three modular components: (1) a\ndeepfake classifier with Grad-CAM-based saliency visualisation, (2) a visual\ncaptioning module that generates natural language summaries of manipulated\nregions, and (3) a narrative refinement module that uses a fine-tuned Large\nLanguage Model (LLM) to produce context-aware, user-sensitive explanations. We\ninstantiate and evaluate the framework on the DF40 benchmark, the most diverse\ndeepfake dataset to date. Experiments demonstrate that our system achieves\ncompetitive detection performance while providing high-quality explanations\naligned with Grad-CAM activations. By unifying prediction and explanation in a\ncoherent, human-aligned pipeline, this work offers a scalable approach to\ninterpretable deepfake detection, advancing the broader vision of trustworthy\nand transparent AI systems in adversarial media environments.", "AI": {"tldr": "本文提出DF-P2E框架，一个多模态、可解释的深度伪造检测系统，通过整合视觉、语义和叙事解释层，使检测过程透明化并支持人类推理。", "motivation": "深度伪造技术对数字诚信构成严重威胁，现有检测系统虽准确但多为黑盒模型，缺乏透明度和可解释性，限制了其在法证、新闻和法律等关键领域的实际应用和非专业用户的理解。", "method": "DF-P2E框架包含三个模块：1) 基于Grad-CAM显著性可视化的深度伪造分类器；2) 生成操作区域自然语言摘要的视觉字幕模块；3) 使用微调大型语言模型（LLM）生成上下文感知、用户敏感解释的叙事细化模块。该框架在DF40数据集上进行了实例化和评估。", "result": "实验证明，该系统在保持有竞争力的检测性能的同时，提供了与Grad-CAM激活对齐的高质量解释。", "conclusion": "DF-P2E通过将预测和解释统一在一个连贯的、以人为中心的流程中，提供了一种可扩展的可解释深度伪造检测方法，推动了对抗性媒体环境中可信和透明AI系统的发展。"}}
{"id": "2508.07603", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07603", "abs": "https://arxiv.org/abs/2508.07603", "authors": ["Wenhui Song", "Hanhui Li", "Jiehui Huang", "Panwen Hu", "Yuhao Cheng", "Long Chen", "Yiqiang Yan", "Xiaodan Liang"], "title": "LaVieID: Local Autoregressive Diffusion Transformers for Identity-Preserving Video Creation", "comment": "Accepted to ACM MM 2025", "summary": "In this paper, we present LaVieID, a novel \\underline{l}ocal\n\\underline{a}utoregressive \\underline{vi}d\\underline{e}o diffusion framework\ndesigned to tackle the challenging \\underline{id}entity-preserving\ntext-to-video task. The key idea of LaVieID is to mitigate the loss of identity\ninformation inherent in the stochastic global generation process of diffusion\ntransformers (DiTs) from both spatial and temporal perspectives. Specifically,\nunlike the global and unstructured modeling of facial latent states in existing\nDiTs, LaVieID introduces a local router to explicitly represent latent states\nby weighted combinations of fine-grained local facial structures. This\nalleviates undesirable feature interference and encourages DiTs to capture\ndistinctive facial characteristics. Furthermore, a temporal autoregressive\nmodule is integrated into LaVieID to refine denoised latent tokens before video\ndecoding. This module divides latent tokens temporally into chunks, exploiting\ntheir long-range temporal dependencies to predict biases for rectifying tokens,\nthereby significantly enhancing inter-frame identity consistency. Consequently,\nLaVieID can generate high-fidelity personalized videos and achieve\nstate-of-the-art performance. Our code and models are available at\nhttps://github.com/ssugarwh/LaVieID.", "AI": {"tldr": "LaVieID是一个新颖的局部自回归视频扩散框架，用于解决保持身份的文本到视频生成任务，通过引入局部路由器和时间自回归模块，从空间和时间维度缓解身份信息丢失。", "motivation": "现有的扩散变换器（DiTs）在随机全局生成过程中存在身份信息丢失的问题，尤其是在人脸潜在状态的全局和非结构化建模中，导致难以生成保持身份的高质量视频。", "method": "LaVieID引入了两个核心模块：1. 局部路由器：通过细粒度局部面部结构的加权组合显式表示潜在状态，以减轻特征干扰并捕获独特面部特征（空间维度）。2. 时间自回归模块：在视频解码前，将潜在令牌按时间分块，利用长程时间依赖性预测偏差来校正令牌，显著增强帧间身份一致性（时间维度）。", "result": "LaVieID能够生成高保真度的个性化视频，并在保持身份的文本到视频生成任务中达到最先进的性能。", "conclusion": "LaVieID通过空间上的局部建模和时间上的自回归精炼，有效解决了扩散模型在文本到视频生成中保持身份的挑战，实现了卓越的生成效果。"}}
{"id": "2508.07607", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07607", "abs": "https://arxiv.org/abs/2508.07607", "authors": ["Jian Ma", "Xujie Zhu", "Zihao Pan", "Qirong Peng", "Xu Guo", "Chen Chen", "Haonan Lu"], "title": "X2Edit: Revisiting Arbitrary-Instruction Image Editing through Self-Constructed Data and Task-Aware Representation Learning", "comment": "https://github.com/OPPO-Mente-Lab/X2Edit", "summary": "Existing open-source datasets for arbitrary-instruction image editing remain\nsuboptimal, while a plug-and-play editing module compatible with\ncommunity-prevalent generative models is notably absent. In this paper, we\nfirst introduce the X2Edit Dataset, a comprehensive dataset covering 14 diverse\nediting tasks, including subject-driven generation. We utilize the\nindustry-leading unified image generation models and expert models to construct\nthe data. Meanwhile, we design reasonable editing instructions with the VLM and\nimplement various scoring mechanisms to filter the data. As a result, we\nconstruct 3.7 million high-quality data with balanced categories. Second, to\nbetter integrate seamlessly with community image generation models, we design\ntask-aware MoE-LoRA training based on FLUX.1, with only 8\\% of the parameters\nof the full model. To further improve the final performance, we utilize the\ninternal representations of the diffusion model and define positive/negative\nsamples based on image editing types to introduce contrastive learning.\nExtensive experiments demonstrate that the model's editing performance is\ncompetitive among many excellent models. Additionally, the constructed dataset\nexhibits substantial advantages over existing open-source datasets. The\nopen-source code, checkpoints, and datasets for X2Edit can be found at the\nfollowing link: https://github.com/OPPO-Mente-Lab/X2Edit.", "AI": {"tldr": "本文提出了一个大型高质量的任意指令图像编辑数据集X2Edit，并设计了一个基于FLUX.1的插件式编辑模块，通过MoE-LoRA训练和对比学习实现了卓越的图像编辑性能。", "motivation": "现有的开源任意指令图像编辑数据集表现不佳，且缺乏一个能与社区流行生成模型兼容的即插即用编辑模块。", "method": "首先，构建了X2Edit数据集，涵盖14种编辑任务，利用行业领先的统一图像生成模型和专家模型，结合VLM生成指令和多种评分机制筛选出370万高质量数据。其次，设计了基于FLUX.1的Task-aware MoE-LoRA训练（仅占全模型8%参数），并通过利用扩散模型的内部表示，定义正负样本引入对比学习以提升性能。", "result": "实验证明，所提出的模型在图像编辑性能上与许多优秀模型具有竞争力。此外，构建的X2Edit数据集相比现有开源数据集展现出显著优势。", "conclusion": "本文成功构建了一个大规模高质量的图像编辑数据集X2Edit，并开发了一个高效且兼容性强的图像编辑模块，有效解决了现有数据集和即插即用编辑模块的不足，为任意指令图像编辑领域提供了有价值的贡献。"}}
{"id": "2508.07618", "categories": ["cs.CV", "68Wxx"], "pdf": "https://arxiv.org/pdf/2508.07618", "abs": "https://arxiv.org/abs/2508.07618", "authors": ["Hyoung Suk Park", "Kiwan Jeon"], "title": "An Iterative Reconstruction Method for Dental Cone-Beam Computed Tomography with a Truncated Field of View", "comment": "8 pages, 2 figures, 2 tables", "summary": "In dental cone-beam computed tomography (CBCT), compact and cost-effective\nsystem designs often use small detectors, resulting in a truncated field of\nview (FOV) that does not fully encompass the patient's head. In iterative\nreconstruction approaches, the discrepancy between the actual projection and\nthe forward projection within the truncated FOV accumulates over iterations,\nleading to significant degradation in the reconstructed image quality. In this\nstudy, we propose a two-stage approach to mitigate truncation artifacts in\ndental CBCT. In the first stage, we employ Implicit Neural Representation\n(INR), leveraging its superior representation power, to generate a prior image\nover an extended region so that its forward projection fully covers the\npatient's head. To reduce computational and memory burdens, INR reconstruction\nis performed with a coarse voxel size. The forward projection of this prior\nimage is then used to estimate the discrepancy due to truncated FOV in the\nmeasured projection data. In the second stage, the discrepancy-corrected\nprojection data is utilized in a conventional iterative reconstruction process\nwithin the truncated region. Our numerical results demonstrate that the\nproposed two-grid approach effectively suppresses truncation artifacts, leading\nto improved CBCT image quality.", "AI": {"tldr": "本研究提出一种两阶段方法，利用隐式神经表示（INR）生成扩展区域的先验图像，以校正牙科CBCT中因视场截断导致的伪影，从而提高重建图像质量。", "motivation": "牙科锥束CT（CBCT）中，紧凑且经济的系统设计常采用小型探测器，导致视野（FOV）截断，无法完全覆盖患者头部。在迭代重建中，截断FOV内实际投影与前向投影之间的差异会累积，严重降低重建图像质量。", "method": "本研究采用两阶段方法：第一阶段，利用隐式神经表示（INR）以粗体素尺寸重建扩展区域的先验图像，其前向投影完全覆盖患者头部，用于估计截断FOV引起的投影数据差异。第二阶段，将经过差异校正的投影数据用于截断区域内的常规迭代重建过程。", "result": "数值结果表明，所提出的两网格（两阶段）方法有效抑制了截断伪影，显著提高了CBCT图像质量。", "conclusion": "本研究提出的两阶段方法能有效减轻牙科CBCT中的截断伪影，从而改善重建图像质量。"}}
{"id": "2508.07624", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07624", "abs": "https://arxiv.org/abs/2508.07624", "authors": ["Vishakha Lall", "Yisi Liu"], "title": "Enhancing Egocentric Object Detection in Static Environments using Graph-based Spatial Anomaly Detection and Correction", "comment": null, "summary": "In many real-world applications involving static environments, the spatial\nlayout of objects remains consistent across instances. However,\nstate-of-the-art object detection models often fail to leverage this spatial\nprior, resulting in inconsistent predictions, missed detections, or\nmisclassifications, particularly in cluttered or occluded scenes. In this work,\nwe propose a graph-based post-processing pipeline that explicitly models the\nspatial relationships between objects to correct detection anomalies in\negocentric frames. Using a graph neural network (GNN) trained on manually\nannotated data, our model identifies invalid object class labels and predicts\ncorrected class labels based on their neighbourhood context. We evaluate our\napproach both as a standalone anomaly detection and correction framework and as\na post-processing module for standard object detectors such as YOLOv7 and\nRT-DETR. Experiments demonstrate that incorporating this spatial reasoning\nsignificantly improves detection performance, with mAP@50 gains of up to 4%.\nThis method highlights the potential of leveraging the environment's spatial\nstructure to improve reliability in object detection systems.", "AI": {"tldr": "本文提出一个基于图的后处理流程，利用图神经网络（GNN）建模物体间的空间关系，以校正静态环境中目标检测的异常，显著提升了检测性能。", "motivation": "现有目标检测模型未能有效利用静态环境中物体布局的空间先验知识，导致在杂乱或遮挡场景中出现预测不一致、漏检或误分类等问题。", "method": "提出一个图基的后处理流程，明确建模物体间的空间关系。使用在手动标注数据上训练的图神经网络（GNN），根据邻域上下文识别并校正无效的物体类别标签。该方法既可作为独立的异常检测与校正框架，也可作为YOLOv7和RT-DETR等标准检测器的后处理模块进行评估。", "result": "实验证明，整合空间推理显著提升了检测性能，mAP@50指标提升高达4%。", "conclusion": "利用环境的空间结构具有提高目标检测系统可靠性的巨大潜力。"}}
{"id": "2508.07625", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07625", "abs": "https://arxiv.org/abs/2508.07625", "authors": ["Junxiao Xue", "Xiaozhen Liu", "Jie Wang", "Xuecheng Wu", "Bin Wu"], "title": "A Trustworthy Method for Multimodal Emotion Recognition", "comment": "Accepted for publication in Big Data Mining and Analytics (BDMA),\n  2025", "summary": "Existing emotion recognition methods mainly focus on enhancing performance by\nemploying complex deep models, typically resulting in significantly higher\nmodel complexity. Although effective, it is also crucial to ensure the\nreliability of the final decision, especially for noisy, corrupted and\nout-of-distribution data. To this end, we propose a novel emotion recognition\nmethod called trusted emotion recognition (TER), which utilizes uncertainty\nestimation to calculate the confidence value of predictions. TER combines the\nresults from multiple modalities based on their confidence values to output the\ntrusted predictions. We also provide a new evaluation criterion to assess the\nreliability of predictions. Specifically, we incorporate trusted precision and\ntrusted recall to determine the trusted threshold and formulate the trusted\nAcc. and trusted F1 score to evaluate the model's trusted performance. The\nproposed framework combines the confidence module that accordingly endows the\nmodel with reliability and robustness against possible noise or corruption. The\nextensive experimental results validate the effectiveness of our proposed\nmodel. The TER achieves state-of-the-art performance on the Music-video,\nachieving 82.40% Acc. In terms of trusted performance, TER outperforms other\nmethods on the IEMOCAP and Music-video, achieving trusted F1 scores of 0.7511\nand 0.9035, respectively.", "AI": {"tldr": "提出了一种基于不确定性估计的信任情感识别（TER）方法，通过整合置信度来提高模型对噪声数据的可靠性和鲁棒性，并引入新的评估指标。", "motivation": "现有情感识别方法侧重于通过复杂模型提升性能，但缺乏对噪声、损坏和分布外数据的决策可靠性。", "method": "提出信任情感识别（TER）方法，利用不确定性估计计算预测的置信度，并根据置信度结合多模态结果。引入新的评估标准：信任精确度、信任召回率、信任准确率和信任F1分数，以评估模型的信任性能。框架中包含一个置信度模块，赋予模型可靠性和鲁棒性。", "result": "TER在Music-video数据集上达到82.40%的准确率，实现了SOTA性能。在信任性能方面，TER在IEMOCAP和Music-video数据集上分别取得了0.7511和0.9035的信任F1分数，优于其他方法。", "conclusion": "所提出的TER框架通过结合置信度模块，有效地提高了情感识别模型的可靠性和对噪声或损坏数据的鲁棒性，实验结果验证了其有效性。"}}
{"id": "2508.07647", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07647", "abs": "https://arxiv.org/abs/2508.07647", "authors": ["Xiaohang Zhan", "Dingming Liu"], "title": "LaRender: Training-Free Occlusion Control in Image Generation via Latent Rendering", "comment": "Accepted by ICCV 2025 (oral). Project page:\n  https://xiaohangzhan.github.io/projects/larender/", "summary": "We propose a novel training-free image generation algorithm that precisely\ncontrols the occlusion relationships between objects in an image. Existing\nimage generation methods typically rely on prompts to influence occlusion,\nwhich often lack precision. While layout-to-image methods provide control over\nobject locations, they fail to address occlusion relationships explicitly.\nGiven a pre-trained image diffusion model, our method leverages volume\nrendering principles to \"render\" the scene in latent space, guided by occlusion\nrelationships and the estimated transmittance of objects. This approach does\nnot require retraining or fine-tuning the image diffusion model, yet it enables\naccurate occlusion control due to its physics-grounded foundation. In extensive\nexperiments, our method significantly outperforms existing approaches in terms\nof occlusion accuracy. Furthermore, we demonstrate that by adjusting the\nopacities of objects or concepts during rendering, our method can achieve a\nvariety of effects, such as altering the transparency of objects, the density\nof mass (e.g., forests), the concentration of particles (e.g., rain, fog), the\nintensity of light, and the strength of lens effects, etc.", "AI": {"tldr": "提出一种无需训练的图像生成算法，通过在潜在空间中利用体渲染原理，实现对图像中物体遮挡关系的精确控制。", "motivation": "现有图像生成方法（如基于提示词或布局）在控制物体遮挡关系时缺乏精度或未能明确处理遮挡问题。", "method": "利用预训练图像扩散模型，在潜在空间中基于体渲染原理“渲染”场景，并由遮挡关系和物体透射率估计引导，无需对扩散模型进行重训练或微调。", "result": "在遮挡精度方面显著优于现有方法；通过调整渲染过程中物体或概念的不透明度，可以实现多种效果，如改变物体透明度、质量密度、粒子浓度、光照强度和镜头效果等。", "conclusion": "该方法基于物理原理，实现了精确的图像遮挡控制，无需模型训练，并能通过调整不透明度产生丰富的视觉效果。"}}
{"id": "2508.07656", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07656", "abs": "https://arxiv.org/abs/2508.07656", "authors": ["Yimin Fu", "Zhunga Liu", "Dongxiu Guo", "Longfei Wang"], "title": "Collaborative Learning of Scattering and Deep Features for SAR Target Recognition with Noisy Labels", "comment": "The code will be released at https://github.com/fuyimin96/CLSDF upon\n  acceptance", "summary": "The acquisition of high-quality labeled synthetic aperture radar (SAR) data\nis challenging due to the demanding requirement for expert knowledge.\nConsequently, the presence of unreliable noisy labels is unavoidable, which\nresults in performance degradation of SAR automatic target recognition (ATR).\nExisting research on learning with noisy labels mainly focuses on image data.\nHowever, the non-intuitive visual characteristics of SAR data are insufficient\nto achieve noise-robust learning. To address this problem, we propose\ncollaborative learning of scattering and deep features (CLSDF) for SAR ATR with\nnoisy labels. Specifically, a multi-model feature fusion framework is designed\nto integrate scattering and deep features. The attributed scattering centers\n(ASCs) are treated as dynamic graph structure data, and the extracted physical\ncharacteristics effectively enrich the representation of deep image features.\nThen, the samples with clean and noisy labels are divided by modeling the loss\ndistribution with multiple class-wise Gaussian Mixture Models (GMMs).\nAfterward, the semi-supervised learning of two divergent branches is conducted\nbased on the data divided by each other. Moreover, a joint distribution\nalignment strategy is introduced to enhance the reliability of co-guessed\nlabels. Extensive experiments have been done on the Moving and Stationary\nTarget Acquisition and Recognition (MSTAR) dataset, and the results show that\nthe proposed method can achieve state-of-the-art performance under different\noperating conditions with various label noises.", "AI": {"tldr": "针对合成孔径雷达（SAR）自动目标识别（ATR）中标签噪声问题，本文提出了一种协同学习散射和深度特征（CLSDF）的方法，通过多模型特征融合和半监督学习提升识别性能。", "motivation": "高质量的SAR标记数据获取困难，导致标签噪声普遍存在，严重影响SAR ATR性能。现有针对噪声标签的学习方法主要集中在图像数据，但SAR数据非直观的视觉特性使其不足以实现噪声鲁棒学习。", "method": "提出CLSDF方法，设计多模型特征融合框架整合散射（将归因散射中心ASCs视为动态图结构数据）和深度特征。通过多类别高斯混合模型（GMMs）建模损失分布，将样本划分为干净和噪声标签。随后，基于相互划分的数据进行两个发散分支的半监督学习，并引入联合分布对齐策略增强共同猜测标签的可靠性。", "result": "在MSTAR数据集上进行了大量实验，结果表明所提出的方法在不同操作条件和各种标签噪声下均能达到最先进的性能。", "conclusion": "CLSDF通过有效融合物理散射特征和深度图像特征，并结合鲁棒的协同学习框架，成功解决了SAR ATR中标签噪声导致的性能下降问题，实现了优异的识别效果。"}}
{"id": "2508.07680", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07680", "abs": "https://arxiv.org/abs/2508.07680", "authors": ["Zhiying Li", "Junhao Wu", "Yeying Jin", "Daiheng Gao", "Yun Ji", "Kaichuan Kong", "Lei Yu", "Hao Xu", "Kai Chen", "Bruce Gu", "Nana Wang", "Zhaoxin Fan"], "title": "Undress to Redress: A Training-Free Framework for Virtual Try-On", "comment": "13 pages, 8 figures", "summary": "Virtual try-on (VTON) is a crucial task for enhancing user experience in\nonline shopping by generating realistic garment previews on personal photos.\nAlthough existing methods have achieved impressive results, they struggle with\nlong-sleeve-to-short-sleeve conversions-a common and practical scenario-often\nproducing unrealistic outputs when exposed skin is underrepresented in the\noriginal image. We argue that this challenge arises from the ''majority''\ncompletion rule in current VTON models, which leads to inaccurate skin\nrestoration in such cases. To address this, we propose UR-VTON (Undress-Redress\nVirtual Try-ON), a novel, training-free framework that can be seamlessly\nintegrated with any existing VTON method. UR-VTON introduces an\n''undress-to-redress'' mechanism: it first reveals the user's torso by\nvirtually ''undressing,'' then applies the target short-sleeve garment,\neffectively decomposing the conversion into two more manageable steps.\nAdditionally, we incorporate Dynamic Classifier-Free Guidance scheduling to\nbalance diversity and image quality during DDPM sampling, and employ Structural\nRefiner to enhance detail fidelity using high-frequency cues. Finally, we\npresent LS-TON, a new benchmark for long-sleeve-to-short-sleeve try-on.\nExtensive experiments demonstrate that UR-VTON outperforms state-of-the-art\nmethods in both detail preservation and image quality. Code will be released\nupon acceptance.", "AI": {"tldr": "UR-VTON是一个免训练框架，通过“脱衣-穿衣”机制解决虚拟试穿中长袖转短袖时皮肤恢复不真实的问题，并引入新基准LS-TON，显著提升图像质量和细节。", "motivation": "现有虚拟试穿（VTON）方法在长袖转短袖场景下表现不佳，由于图像中暴露皮肤信息不足，导致模型采用“多数”补全规则生成不真实的输出。", "method": "提出UR-VTON框架，可与现有VTON方法无缝集成。核心是“脱衣-穿衣”机制：首先虚拟“脱掉”衣物以暴露用户躯干，然后“穿上”目标短袖服装，将转换分解为两步。此外，采用动态无分类器指导调度（Dynamic Classifier-Free Guidance scheduling）平衡多样性和图像质量，并使用结构细化器（Structural Refiner）利用高频信息增强细节保真度。同时，提出了长袖转短袖试穿的新基准LS-TON。", "result": "大量实验表明，UR-VTON在细节保留和图像质量方面均优于现有最先进的方法。", "conclusion": "UR-VTON通过创新的“脱衣-穿衣”机制和生成优化技术，有效解决了长袖转短袖虚拟试穿的挑战，显著提升了生成图像的真实感和细节表现，并为该任务建立了新的评估基准。"}}
{"id": "2508.07682", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07682", "abs": "https://arxiv.org/abs/2508.07682", "authors": ["Wenzhuo Ma", "Zhenzhong Chen"], "title": "DiffVC-OSD: One-Step Diffusion-based Perceptual Neural Video Compression Framework", "comment": null, "summary": "In this work, we first propose DiffVC-OSD, a One-Step Diffusion-based\nPerceptual Neural Video Compression framework. Unlike conventional multi-step\ndiffusion-based methods, DiffVC-OSD feeds the reconstructed latent\nrepresentation directly into a One-Step Diffusion Model, enhancing perceptual\nquality through a single diffusion step guided by both temporal context and the\nlatent itself. To better leverage temporal dependencies, we design a Temporal\nContext Adapter that encodes conditional inputs into multi-level features,\noffering more fine-grained guidance for the Denoising Unet. Additionally, we\nemploy an End-to-End Finetuning strategy to improve overall compression\nperformance. Extensive experiments demonstrate that DiffVC-OSD achieves\nstate-of-the-art perceptual compression performance, offers about 20$\\times$\nfaster decoding and a 86.92\\% bitrate reduction compared to the corresponding\nmulti-step diffusion-based variant.", "AI": {"tldr": "DiffVC-OSD是一种基于一步扩散的感知神经视频压缩框架，通过单步扩散模型和时间上下文适配器显著提升了感知质量、解码速度并降低了比特率。", "motivation": "传统的基于扩散的多步方法在视频压缩中解码速度慢且效率不高，需要一种更快速、更高效且能保持高感知质量的视频压缩方案。", "method": "1. 提出DiffVC-OSD，将重建的潜在表示直接输入一步扩散模型，通过单个扩散步骤增强感知质量。2. 设计时间上下文适配器，将条件输入编码为多级特征，为去噪Unet提供更精细的指导。3. 采用端到端微调策略以提升整体压缩性能。", "result": "DiffVC-OSD实现了最先进的感知压缩性能，解码速度比对应的多步扩散变体快约20倍，比特率降低了86.92%。", "conclusion": "DiffVC-OSD成功地将一步扩散模型应用于视频压缩，显著提高了解码速度和压缩效率，同时保持了卓越的感知质量，为神经视频压缩领域带来了重要进展。"}}
{"id": "2508.07700", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07700", "abs": "https://arxiv.org/abs/2508.07700", "authors": ["Weitao Wang", "Haoran Xu", "Jun Meng", "Haoqian Wang"], "title": "Make Your MoVe: Make Your 3D Contents by Adapting Multi-View Diffusion Models to External Editing", "comment": null, "summary": "As 3D generation techniques continue to flourish, the demand for generating\npersonalized content is rapidly rising. Users increasingly seek to apply\nvarious editing methods to polish generated 3D content, aiming to enhance its\ncolor, style, and lighting without compromising the underlying geometry.\nHowever, most existing editing tools focus on the 2D domain, and directly\nfeeding their results into 3D generation methods (like multi-view diffusion\nmodels) will introduce information loss, degrading the quality of the final 3D\nassets. In this paper, we propose a tuning-free, plug-and-play scheme that\naligns edited assets with their original geometry in a single inference run.\nCentral to our approach is a geometry preservation module that guides the\nedited multi-view generation with original input normal latents. Besides, an\ninjection switcher is proposed to deliberately control the supervision extent\nof the original normals, ensuring the alignment between the edited color and\nnormal views. Extensive experiments show that our method consistently improves\nboth the multi-view consistency and mesh quality of edited 3D assets, across\nmultiple combinations of multi-view diffusion models and editing methods.", "AI": {"tldr": "本文提出了一种免调优、即插即用的方案，用于在编辑3D内容（如颜色、风格、光照）时，在不牺牲底层几何形状的情况下，提高多视图一致性和网格质量。", "motivation": "随着3D生成技术的发展，个性化内容的需求日益增长。用户希望编辑生成的3D内容以增强其颜色、风格和光照，同时不损害几何结构。然而，现有的大多数编辑工具专注于2D领域，直接将其结果用于3D生成（如多视图扩散模型）会导致信息丢失，降低最终3D资产的质量。", "method": "该方法提出了一种免调优、即插即用的方案，在单次推理中将编辑后的资产与其原始几何形状对齐。核心是一个几何保持模块，它利用原始输入法线潜在空间来指导编辑后的多视图生成。此外，还提出了一个注入切换器，用于精确控制原始法线的监督程度，确保编辑后的颜色视图和法线视图之间的对齐。", "result": "广泛的实验表明，该方法在多种多视图扩散模型和编辑方法的组合下，一致地提高了编辑后的3D资产的多视图一致性和网格质量。", "conclusion": "该方案成功解决了在编辑3D内容时保持几何形状和提高质量的挑战，为3D生成内容的个性化编辑提供了一个有效且通用的工具。"}}
{"id": "2508.07721", "categories": ["cs.CV", "cs.NA", "math.NA", "65D18, 68U10, 94A08"], "pdf": "https://arxiv.org/pdf/2508.07721", "abs": "https://arxiv.org/abs/2508.07721", "authors": ["Daoping Zhang", "Xue-Cheng Tai", "Lok Ming Lui"], "title": "A Registration-Based Star-Shape Segmentation Model and Fast Algorithms", "comment": null, "summary": "Image segmentation plays a crucial role in extracting objects of interest and\nidentifying their boundaries within an image. However, accurate segmentation\nbecomes challenging when dealing with occlusions, obscurities, or noise in\ncorrupted images. To tackle this challenge, prior information is often\nutilized, with recent attention on star-shape priors. In this paper, we propose\na star-shape segmentation model based on the registration framework. By\ncombining the level set representation with the registration framework and\nimposing constraints on the deformed level set function, our model enables both\nfull and partial star-shape segmentation, accommodating single or multiple\ncenters. Additionally, our approach allows for the enforcement of identified\nboundaries to pass through specified landmark locations. We tackle the proposed\nmodels using the alternating direction method of multipliers. Through numerical\nexperiments conducted on synthetic and real images, we demonstrate the efficacy\nof our approach in achieving accurate star-shape segmentation.", "AI": {"tldr": "本文提出了一种基于配准框架的星形图像分割模型，结合水平集方法，能处理完全或部分星形，支持单或多中心，并可强制边界通过特定地标点，有效应对图像损坏带来的分割挑战。", "motivation": "在图像中提取目标和识别边界对图像分割至关重要。然而，当图像受遮挡、模糊或噪声损坏时，准确分割变得极具挑战性。为了解决这个问题，研究人员常利用先验信息，其中星形先验近期受到关注。", "method": "该研究提出了一种基于配准框架的星形分割模型。它将水平集表示与配准框架结合，并通过对变形水平集函数施加约束，实现了完全和部分星形分割，并能适应单中心或多中心目标。此外，该方法允许强制识别的边界通过指定的特征点。模型通过交替方向乘子法（ADMM）求解。", "result": "通过在合成图像和真实图像上进行的数值实验，研究结果表明该方法在实现准确的星形分割方面是有效的。", "conclusion": "该研究提出了一种有效的方法，利用基于配准框架的星形先验，能够准确地进行图像分割，即使在图像存在遮挡、模糊或噪声的情况下，也能处理多种复杂的星形目标分割场景。"}}
{"id": "2508.07723", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07723", "abs": "https://arxiv.org/abs/2508.07723", "authors": ["Ting Xiang", "Changjian Chen", "Zhuo Tang", "Qifeng Zhang", "Fei Lyu", "Li Yang", "Jiapeng Zhang", "Kenli Li"], "title": "Enhancing Small-Scale Dataset Expansion with Triplet-Connection-based Sample Re-Weighting", "comment": "15 pages, 8 figures, published to ACM MM2025", "summary": "The performance of computer vision models in certain real-world applications,\nsuch as medical diagnosis, is often limited by the scarcity of available\nimages. Expanding datasets using pre-trained generative models is an effective\nsolution. However, due to the uncontrollable generation process and the\nambiguity of natural language, noisy images may be generated. Re-weighting is\nan effective way to address this issue by assigning low weights to such noisy\nimages. We first theoretically analyze three types of supervision for the\ngenerated images. Based on the theoretical analysis, we develop TriReWeight, a\ntriplet-connection-based sample re-weighting method to enhance generative data\naugmentation. Theoretically, TriReWeight can be integrated with any generative\ndata augmentation methods and never downgrade their performance. Moreover, its\ngeneralization approaches the optimal in the order $O(\\sqrt{d\\ln (n)/n})$. Our\nexperiments validate the correctness of the theoretical analysis and\ndemonstrate that our method outperforms the existing SOTA methods by $7.9\\%$ on\naverage over six natural image datasets and by $3.4\\%$ on average over three\nmedical datasets. We also experimentally validate that our method can enhance\nthe performance of different generative data augmentation methods.", "AI": {"tldr": "针对生成式数据增强中可能出现的噪声图像问题，本文提出了一种名为TriReWeight的三元连接样本重加权方法，通过理论分析和实验验证，有效提升了模型性能和生成图像的质量。", "motivation": "计算机视觉模型在现实应用（如医疗诊断）中，常因图像数据稀缺而性能受限。虽然预训练生成模型可扩充数据集，但其不可控的生成过程和自然语言的模糊性可能导致生成噪声图像，影响模型性能。", "method": "首先理论分析了生成图像的三种监督类型。基于此分析，开发了TriReWeight，一种基于三元连接的样本重加权方法，旨在为噪声图像分配低权重。理论上，TriReWeight可与任何生成式数据增强方法集成，且不会降低其性能，其泛化能力接近最优，达到$O(\\sqrt{d\\ln (n)/n})$的阶。", "result": "实验结果验证了理论分析的正确性。在六个自然图像数据集上，该方法平均优于现有SOTA方法7.9%；在三个医疗数据集上，平均优于3.4%。此外，实验还验证了该方法能有效提升不同生成式数据增强方法的性能。", "conclusion": "TriReWeight是一种有效且理论上可靠的样本重加权方法，能够解决生成式数据增强中噪声图像的问题，显著提升计算机视觉模型在数据稀缺场景下的性能，并能与现有生成增强方法良好兼容。"}}
{"id": "2508.07747", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07747", "abs": "https://arxiv.org/abs/2508.07747", "authors": ["Junhyuk So", "Juncheol Shin", "Hyunho Kook", "Eunhyeok Park"], "title": "Grouped Speculative Decoding for Autoregressive Image Generation", "comment": "Accepted to the ICCV 2025", "summary": "Recently, autoregressive (AR) image models have demonstrated remarkable\ngenerative capabilities, positioning themselves as a compelling alternative to\ndiffusion models. However, their sequential nature leads to long inference\ntimes, limiting their practical scalability. In this work, we introduce Grouped\nSpeculative Decoding (GSD), a novel, training-free acceleration method for AR\nimage models. While recent studies have explored Speculative Decoding (SD) as a\nmeans to speed up AR image generation, existing approaches either provide only\nmodest acceleration or require additional training. Our in-depth analysis\nreveals a fundamental difference between language and image tokens: image\ntokens exhibit inherent redundancy and diversity, meaning multiple tokens can\nconvey valid semantics. However, traditional SD methods are designed to accept\nonly a single most-likely token, which fails to leverage this difference,\nleading to excessive false-negative rejections. To address this, we propose a\nnew SD strategy that evaluates clusters of visually valid tokens rather than\nrelying on a single target token. Additionally, we observe that static\nclustering based on embedding distance is ineffective, which motivates our\ndynamic GSD approach. Extensive experiments show that GSD accelerates AR image\nmodels by an average of 3.7x while preserving image quality-all without\nrequiring any additional training. The source code is available at\nhttps://github.com/junhyukso/GSD", "AI": {"tldr": "本文提出Grouped Speculative Decoding (GSD)，一种无需训练的加速自回归图像模型的方法，通过利用图像token的冗余和多样性，实现平均3.7倍的加速，同时保持图像质量。", "motivation": "自回归(AR)图像模型虽然生成能力强大，但推理时间长，限制了其实用性。现有推测解码(SD)方法加速效果有限或需要额外训练。传统SD方法未充分利用图像token固有的冗余和多样性（即多个token可表达有效语义），导致拒绝率过高。", "method": "引入分组推测解码 (GSD)。该方法不依赖单个最可能token，而是评估视觉上有效的token簇。观察到基于嵌入距离的静态聚类无效，因此提出了动态GSD方法来解决这一问题。", "result": "实验表明，GSD平均将AR图像模型加速3.7倍，同时保持图像质量，且无需任何额外训练。", "conclusion": "GSD是一种有效、无需训练的加速AR图像模型的方法，通过创新性地处理图像token的特性，显著提升了模型的实用性和可扩展性。"}}
{"id": "2508.07755", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07755", "abs": "https://arxiv.org/abs/2508.07755", "authors": ["Minseo Kim", "Minchan Kwon", "Dongyeun Lee", "Yunho Jeon", "Junmo Kim"], "title": "Comparison Reveals Commonality: Customized Image Generation through Contrastive Inversion", "comment": "Accepted at CVPR 2025 workshop (AI4CC)", "summary": "The recent demand for customized image generation raises a need for\ntechniques that effectively extract the common concept from small sets of\nimages. Existing methods typically rely on additional guidance, such as text\nprompts or spatial masks, to capture the common target concept. Unfortunately,\nrelying on manually provided guidance can lead to incomplete separation of\nauxiliary features, which degrades generation quality.In this paper, we propose\nContrastive Inversion, a novel approach that identifies the common concept by\ncomparing the input images without relying on additional information. We train\nthe target token along with the image-wise auxiliary text tokens via\ncontrastive learning, which extracts the well-disentangled true semantics of\nthe target. Then we apply disentangled cross-attention fine-tuning to improve\nconcept fidelity without overfitting. Experimental results and analysis\ndemonstrate that our method achieves a balanced, high-level performance in both\nconcept representation and editing, outperforming existing techniques.", "AI": {"tldr": "提出了一种名为对比反演（Contrastive Inversion）的新方法，通过对比学习从少量图像中提取共同概念，无需额外指导，从而实现高质量的图像生成和编辑。", "motivation": "定制化图像生成需要从少量图像中有效提取共同概念。现有方法通常依赖文本提示或空间掩码等额外指导，但手动提供的指导可能导致辅助特征分离不完全，从而降低生成质量。", "method": "提出对比反演方法，通过对比输入图像来识别共同概念，不依赖额外信息。通过对比学习训练目标标记和图像级辅助文本标记，以提取目标概念的良好解耦的真实语义。然后应用解耦交叉注意力微调，在不发生过拟合的情况下提高概念保真度。", "result": "实验结果和分析表明，该方法在概念表示和编辑方面均取得了平衡且高水平的性能，优于现有技术。", "conclusion": "对比反演方法能够有效、高质量地从少量图像中提取共同概念，且无需外部指导，解决了现有方法的局限性，提升了图像生成和编辑的质量。"}}
{"id": "2508.07759", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07759", "abs": "https://arxiv.org/abs/2508.07759", "authors": ["Haoran Wang", "Zekun Li", "Jian Zhang", "Lei Qi", "Yinghuan Shi"], "title": "Correspondence as Video: Test-Time Adaption on SAM2 for Reference Segmentation in the Wild", "comment": null, "summary": "Large vision models like the Segment Anything Model (SAM) exhibit significant\nlimitations when applied to downstream tasks in the wild. Consequently,\nreference segmentation, which leverages reference images and their\ncorresponding masks to impart novel knowledge to the model, emerges as a\npromising new direction for adapting vision models. However, existing reference\nsegmentation approaches predominantly rely on meta-learning, which still\nnecessitates an extensive meta-training process and brings massive data and\ncomputational cost. In this study, we propose a novel approach by representing\nthe inherent correspondence between reference-target image pairs as a pseudo\nvideo. This perspective allows the latest version of SAM, known as SAM2, which\nis equipped with interactive video object segmentation (iVOS) capabilities, to\nbe adapted to downstream tasks in a lightweight manner. We term this approach\nCorrespondence As Video for SAM (CAV-SAM). CAV-SAM comprises two key modules:\nthe Diffusion-Based Semantic Transition (DBST) module employs a diffusion model\nto construct a semantic transformation sequence, while the Test-Time Geometric\nAlignment (TTGA) module aligns the geometric changes within this sequence\nthrough test-time fine-tuning. We evaluated CAVSAM on widely-used datasets,\nachieving segmentation performance improvements exceeding 5% over SOTA methods.\nImplementation is provided in the supplementary materials.", "AI": {"tldr": "本文提出CAV-SAM，一种轻量级方法，通过将参考-目标图像对视为伪视频，利用SAM2的交互式视频对象分割能力，以适应SAM模型在下游任务中的应用，解决了现有元学习方法的计算成本问题。", "motivation": "大型视觉模型（如SAM）在实际下游任务中存在局限性。现有参考分割方法（主要依赖元学习）需要大量的元训练过程和巨大的数据与计算成本，因此需要一种更轻量级的模型适应方法。", "method": "提出Correspondence As Video for SAM (CAV-SAM) 方法。将参考-目标图像对之间的内在对应关系表示为伪视频，从而利用SAM2的交互式视频对象分割（iVOS）能力进行轻量级适应。CAV-SAM包含两个关键模块：Diffusion-Based Semantic Transition (DBST) 模块（使用扩散模型构建语义转换序列）和Test-Time Geometric Alignment (TTGA) 模块（通过测试时微调对序列中的几何变化进行对齐）。", "result": "在广泛使用的数据集上评估了CAV-SAM，分割性能比SOTA方法提高了5%以上。", "conclusion": "通过将参考-目标图像对概念化为伪视频并利用SAM2的iVOS能力，CAV-SAM提供了一种新颖、轻量级的模型适应方法，显著提升了SAM在下游任务中的参考分割性能，超越了现有SOTA方法。"}}
{"id": "2508.07769", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07769", "abs": "https://arxiv.org/abs/2508.07769", "authors": ["Xiaoyan Liu", "Kangrui Li", "Jiaxin Liu"], "title": "Dream4D: Lifting Camera-Controlled I2V towards Spatiotemporally Consistent 4D Generation", "comment": "Project Page: https://wanderer7-sk.github.io/Dream4D.github.io/", "summary": "The synthesis of spatiotemporally coherent 4D content presents fundamental\nchallenges in computer vision, requiring simultaneous modeling of high-fidelity\nspatial representations and physically plausible temporal dynamics. Current\napproaches often struggle to maintain view consistency while handling complex\nscene dynamics, particularly in large-scale environments with multiple\ninteracting elements. This work introduces Dream4D, a novel framework that\nbridges this gap through a synergy of controllable video generation and neural\n4D reconstruction. Our approach seamlessly combines a two-stage architecture:\nit first predicts optimal camera trajectories from a single image using\nfew-shot learning, then generates geometrically consistent multi-view sequences\nvia a specialized pose-conditioned diffusion process, which are finally\nconverted into a persistent 4D representation. This framework is the first to\nleverage both rich temporal priors from video diffusion models and geometric\nawareness of the reconstruction models, which significantly facilitates 4D\ngeneration and shows higher quality (e.g., mPSNR, mSSIM) over existing methods.", "AI": {"tldr": "Dream4D是一个新颖的框架，通过结合可控视频生成和神经4D重建，解决了4D内容时空一致性合成的挑战。", "motivation": "当前的4D内容合成方法在处理复杂场景动态（尤其是在大规模环境中具有多个交互元素时）和保持视图一致性方面存在困难。", "method": "Dream4D采用两阶段架构：首先，利用少样本学习从单张图像预测最佳相机轨迹；然后，通过姿态条件扩散过程生成几何一致的多视图序列；最后，将其转换为持久的4D表示。该方法首次结合了视频扩散模型的丰富时间先验和重建模型的几何感知能力。", "result": "该框架显著促进了4D生成，并显示出比现有方法更高的质量（例如，mPSNR，mSSIM）。", "conclusion": "Dream4D通过融合视频扩散模型的时间先验和重建模型的几何感知，成功地弥合了4D内容合成的差距，实现了高质量和一致的4D内容生成。"}}
{"id": "2508.07771", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07771", "abs": "https://arxiv.org/abs/2508.07771", "authors": ["Lei Wang", "Shiming Chen", "Guo-Sen Xie", "Ziming Hong", "Chaojian Yu", "Qinmu Peng", "Xinge You"], "title": "Prototype-Guided Curriculum Learning for Zero-Shot Learning", "comment": "12 pages, 7 figures", "summary": "In Zero-Shot Learning (ZSL), embedding-based methods enable knowledge\ntransfer from seen to unseen classes by learning a visual-semantic mapping from\nseen-class images to class-level semantic prototypes (e.g., attributes).\nHowever, these semantic prototypes are manually defined and may introduce noisy\nsupervision for two main reasons: (i) instance-level mismatch: variations in\nperspective, occlusion, and annotation bias will cause discrepancies between\nindividual sample and the class-level semantic prototypes; and (ii) class-level\nimprecision: the manually defined semantic prototypes may not accurately\nreflect the true semantics of the class. Consequently, the visual-semantic\nmapping will be misled, reducing the effectiveness of knowledge transfer to\nunseen classes. In this work, we propose a prototype-guided curriculum learning\nframework (dubbed as CLZSL), which mitigates instance-level mismatches through\na Prototype-Guided Curriculum Learning (PCL) module and addresses class-level\nimprecision via a Prototype Update (PUP) module. Specifically, the PCL module\nprioritizes samples with high cosine similarity between their visual mappings\nand the class-level semantic prototypes, and progressively advances to\nless-aligned samples, thereby reducing the interference of instance-level\nmismatches to achieve accurate visual-semantic mapping. Besides, the PUP module\ndynamically updates the class-level semantic prototypes by leveraging the\nvisual mappings learned from instances, thereby reducing class-level\nimprecision and further improving the visual-semantic mapping. Experiments were\nconducted on standard benchmark datasets-AWA2, SUN, and CUB-to verify the\neffectiveness of our method.", "AI": {"tldr": "本文提出CLZSL框架，通过原型引导课程学习和原型更新模块，解决零样本学习中实例级不匹配和类别级不精确问题，从而提高视觉-语义映射的准确性。", "motivation": "现有零样本学习（ZSL）中，基于嵌入的方法依赖手动定义的语义原型，但这些原型存在两个主要问题：1) 实例级不匹配：个体样本（如视角、遮挡、标注偏差）与类别级语义原型存在差异；2) 类别级不精确：手动定义的语义原型可能无法准确反映类别的真实语义。这些问题导致视觉-语义映射被误导，降低了知识向未见类别的迁移效率。", "method": "本文提出了一个原型引导的课程学习框架（CLZSL）。该框架包含两个核心模块：1) 原型引导课程学习（PCL）模块：通过优先处理视觉映射与类别级语义原型余弦相似度高的样本，并逐步引入对齐度较低的样本，以减轻实例级不匹配的干扰。2) 原型更新（PUP）模块：利用从实例学习到的视觉映射动态更新类别级语义原型，从而减少类别级不精确性。", "result": "在标准基准数据集AWA2、SUN和CUB上进行的实验验证了所提方法的有效性。", "conclusion": "CLZSL框架通过PCL模块缓解实例级不匹配，并通过PUP模块解决类别级不精确问题，有效提升了视觉-语义映射的准确性，进而提高了零样本学习中知识向未见类别的迁移效果。"}}
{"id": "2508.07775", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07775", "abs": "https://arxiv.org/abs/2508.07775", "authors": ["Lennart Bastian", "Mohammad Rashed", "Nassir Navab", "Tolga Birdal"], "title": "Forecasting Continuous Non-Conservative Dynamical Systems in SO(3)", "comment": "ICCV 2025 Oral", "summary": "Modeling the rotation of moving objects is a fundamental task in computer\nvision, yet $SO(3)$ extrapolation still presents numerous challenges: (1)\nunknown quantities such as the moment of inertia complicate dynamics, (2) the\npresence of external forces and torques can lead to non-conservative\nkinematics, and (3) estimating evolving state trajectories under sparse, noisy\nobservations requires robustness. We propose modeling trajectories of noisy\npose estimates on the manifold of 3D rotations in a physically and\ngeometrically meaningful way by leveraging Neural Controlled Differential\nEquations guided with $SO(3)$ Savitzky-Golay paths. Existing extrapolation\nmethods often rely on energy conservation or constant velocity assumptions,\nlimiting their applicability in real-world scenarios involving non-conservative\nforces. In contrast, our approach is agnostic to energy and momentum\nconservation while being robust to input noise, making it applicable to\ncomplex, non-inertial systems. Our approach is easily integrated as a module in\nexisting pipelines and generalizes well to trajectories with unknown physical\nparameters. By learning to approximate object dynamics from noisy states during\ntraining, our model attains robust extrapolation capabilities in simulation and\nvarious real-world settings. Code is available at\nhttps://github.com/bastianlb/forecasting-rotational-dynamics", "AI": {"tldr": "本文提出一种利用神经控制微分方程（NCDE）和SO(3) Savitzky-Golay路径，在SO(3)流形上对嘈杂的3D旋转轨迹进行建模和外推的方法，解决了未知动力学、外部力及观测噪声带来的挑战，适用于非保守系统。", "motivation": "在计算机视觉中，对运动物体旋转建模是一项基本任务，但SO(3)外推面临多重挑战：1) 惯性矩等未知量使动力学复杂化；2) 外部力和力矩导致非保守运动学；3) 在稀疏、嘈杂观测下估计演化状态轨迹需要鲁棒性。现有方法常依赖能量守恒或恒定速度假设，限制了其在真实非保守场景中的适用性。", "method": "提出通过利用神经控制微分方程（Neural Controlled Differential Equations, NCDE）并以SO(3) Savitzky-Golay路径引导，在3D旋转流形上以物理和几何有意义的方式建模嘈杂姿态估计的轨迹。该方法对能量和动量守恒不敏感，同时对输入噪声具有鲁棒性。", "result": "通过在训练过程中从嘈杂状态学习近似物体动力学，该模型在模拟和各种真实世界设置中都获得了鲁棒的外推能力。它易于作为模块集成到现有流程中，并能很好地推广到具有未知物理参数的轨迹。", "conclusion": "所提出的基于NCDE并结合SO(3) Savitzky-Golay路径的方法，为处理复杂、非惯性系统的SO(3)外推问题提供了一个鲁棒、通用且对噪声不敏感的解决方案，克服了传统方法对能量守恒或恒定速度假设的限制。"}}
{"id": "2508.07782", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07782", "abs": "https://arxiv.org/abs/2508.07782", "authors": ["Saihui Hou", "Chenye Wang", "Wenpeng Lang", "Zhengxiang Lan", "Yongzhen Huang"], "title": "GaitSnippet: Gait Recognition Beyond Unordered Sets and Ordered Sequences", "comment": "13 pages, 5 figures", "summary": "Recent advancements in gait recognition have significantly enhanced\nperformance by treating silhouettes as either an unordered set or an ordered\nsequence. However, both set-based and sequence-based approaches exhibit notable\nlimitations. Specifically, set-based methods tend to overlook short-range\ntemporal context for individual frames, while sequence-based methods struggle\nto capture long-range temporal dependencies effectively. To address these\nchallenges, we draw inspiration from human identification and propose a new\nperspective that conceptualizes human gait as a composition of individualized\nactions. Each action is represented by a series of frames, randomly selected\nfrom a continuous segment of the sequence, which we term a snippet.\nFundamentally, the collection of snippets for a given sequence enables the\nincorporation of multi-scale temporal context, facilitating more comprehensive\ngait feature learning. Moreover, we introduce a non-trivial solution for\nsnippet-based gait recognition, focusing on Snippet Sampling and Snippet\nModeling as key components. Extensive experiments on four widely-used gait\ndatasets validate the effectiveness of our proposed approach and, more\nimportantly, highlight the potential of gait snippets. For instance, our method\nachieves the rank-1 accuracy of 77.5% on Gait3D and 81.7% on GREW using a 2D\nconvolution-based backbone.", "AI": {"tldr": "该论文提出了一种新的步态识别方法，将步态视为个体动作的组合，并通过引入“片段”（snippets）来捕捉多尺度时间上下文，有效解决了现有基于集合和序列方法的局限性。", "motivation": "现有的步态识别方法（基于集合或基于序列）存在局限性：基于集合的方法忽略了单帧的短程时间上下文，而基于序列的方法难以有效捕捉长程时间依赖性。为了解决这些问题，作者提出了一种新的视角。", "method": "将人类步态概念化为个体化动作的组合，每个动作由从序列连续片段中随机选择的一系列帧（称为“片段”）表示。通过收集给定序列的片段，可以整合多尺度时间上下文，促进更全面的步态特征学习。此外，论文提出了一种针对基于片段的步态识别的解决方案，重点关注“片段采样”（Snippet Sampling）和“片段建模”（Snippet Modeling）作为关键组件。", "result": "在四个广泛使用的步态数据集上进行了广泛实验，验证了所提出方法的有效性。例如，使用2D卷积骨干网络，该方法在Gait3D数据集上达到了77.5%的rank-1准确率，在GREW数据集上达到了81.7%的rank-1准确率。", "conclusion": "所提出的基于片段的步态识别方法是有效的，并且更重要的是，突出了步态片段在步态识别领域的巨大潜力。"}}
{"id": "2508.07788", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07788", "abs": "https://arxiv.org/abs/2508.07788", "authors": ["Runze Wang", "Zeli Chen", "Zhiyun Song", "Wei Fang", "Jiajin Zhang", "Danyang Tu", "Yuxing Tang", "Minfeng Xu", "Xianghua Ye", "Le Lu", "Dakai Jin"], "title": "Anatomy-Aware Low-Dose CT Denoising via Pretrained Vision Models and Semantic-Guided Contrastive Learning", "comment": null, "summary": "To reduce radiation exposure and improve the diagnostic efficacy of low-dose\ncomputed tomography (LDCT), numerous deep learning-based denoising methods have\nbeen developed to mitigate noise and artifacts. However, most of these\napproaches ignore the anatomical semantics of human tissues, which may\npotentially result in suboptimal denoising outcomes. To address this problem,\nwe propose ALDEN, an anatomy-aware LDCT denoising method that integrates\nsemantic features of pretrained vision models (PVMs) with adversarial and\ncontrastive learning. Specifically, we introduce an anatomy-aware discriminator\nthat dynamically fuses hierarchical semantic features from reference\nnormal-dose CT (NDCT) via cross-attention mechanisms, enabling tissue-specific\nrealism evaluation in the discriminator. In addition, we propose a\nsemantic-guided contrastive learning module that enforces anatomical\nconsistency by contrasting PVM-derived features from LDCT, denoised CT and\nNDCT, preserving tissue-specific patterns through positive pairs and\nsuppressing artifacts via dual negative pairs. Extensive experiments conducted\non two LDCT denoising datasets reveal that ALDEN achieves the state-of-the-art\nperformance, offering superior anatomy preservation and substantially reducing\nover-smoothing issue of previous work. Further validation on a downstream\nmulti-organ segmentation task (encompassing 117 anatomical structures) affirms\nthe model's ability to maintain anatomical awareness.", "AI": {"tldr": "ALDEN是一种解剖学感知的低剂量CT（LDCT）去噪方法，通过整合预训练视觉模型（PVM）的语义特征与对抗学习和对比学习，有效解决传统方法忽略解剖语义导致去噪效果不佳的问题，并显著提升解剖结构保留和减少过平滑。", "motivation": "现有基于深度学习的LDCT去噪方法大多忽略了人体组织的解剖语义信息，可能导致次优的去噪结果，尤其是在解剖结构保留和过平滑方面存在不足。", "method": "ALDEN方法整合了预训练视觉模型（PVM）的语义特征与对抗学习和对比学习。具体包括：1) 引入一个解剖学感知判别器，通过交叉注意力机制动态融合来自参考正常剂量CT（NDCT）的层级语义特征，实现组织特异性的真实性评估。2) 提出一个语义引导的对比学习模块，通过对比来自LDCT、去噪CT和NDCT的PVM派生特征，强制执行解剖学一致性，通过正样本对保留组织特异性模式，并通过双负样本对抑制伪影。", "result": "在两个LDCT去噪数据集上进行的广泛实验表明，ALDEN达到了最先进的性能，提供了卓越的解剖结构保留，并显著减少了先前工作的过平滑问题。在包含117个解剖结构的多器官分割下游任务上的进一步验证，证实了模型保持解剖学感知的能力。", "conclusion": "ALDEN通过集成解剖学语义特征，有效提升了LDCT去噪的质量，实现了卓越的解剖结构保留并减少了过平滑，为低剂量CT的诊断效能提供了有力的技术支持。"}}
{"id": "2508.07795", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07795", "abs": "https://arxiv.org/abs/2508.07795", "authors": ["Hongrui Zheng", "Yuezun Li", "Liejun Wang", "Yunfeng Diao", "Zhiqing Guo"], "title": "Boosting Active Defense Persistence: A Two-Stage Defense Framework Combining Interruption and Poisoning Against Deepfake", "comment": null, "summary": "Active defense strategies have been developed to counter the threat of\ndeepfake technology. However, a primary challenge is their lack of persistence,\nas their effectiveness is often short-lived. Attackers can bypass these\ndefenses by simply collecting protected samples and retraining their models.\nThis means that static defenses inevitably fail when attackers retrain their\nmodels, which severely limits practical use. We argue that an effective defense\nnot only distorts forged content but also blocks the model's ability to adapt,\nwhich occurs when attackers retrain their models on protected images. To\nachieve this, we propose an innovative Two-Stage Defense Framework (TSDF).\nBenefiting from the intensity separation mechanism designed in this paper, the\nframework uses dual-function adversarial perturbations to perform two roles.\nFirst, it can directly distort the forged results. Second, it acts as a\npoisoning vehicle that disrupts the data preparation process essential for an\nattacker's retraining pipeline. By poisoning the data source, TSDF aims to\nprevent the attacker's model from adapting to the defensive perturbations, thus\nensuring the defense remains effective long-term. Comprehensive experiments\nshow that the performance of traditional interruption methods degrades sharply\nwhen it is subjected to adversarial retraining. However, our framework shows a\nstrong dual defense capability, which can improve the persistence of active\ndefense. Our code will be available at https://github.com/vpsg-research/TSDF.", "AI": {"tldr": "提出一种双阶段防御框架（TSDF），通过双功能对抗性扰动，既能直接扭曲伪造内容，又能通过数据投毒阻止攻击者模型重训练以适应防御，从而实现持久性的深度伪造主动防御。", "motivation": "现有主动防御策略缺乏持久性，攻击者通过收集受保护样本并重新训练模型即可绕过防御，导致静态防御在攻击者重训练后失效，严重限制了其实用性。", "method": "提出双阶段防御框架（TSDF），利用强度分离机制设计双功能对抗性扰动。第一阶段直接扭曲伪造结果；第二阶段作为投毒载体，破坏攻击者模型重训练所需的数据准备过程，阻止其模型适应防御扰动。", "result": "实验表明，传统中断方法在对抗性重训练下性能急剧下降，而TSDF框架展现出强大的双重防御能力，显著提高了主动防御的持久性。", "conclusion": "TSDF框架通过结合直接内容扭曲和数据投毒阻止模型适应，有效解决了深度伪造主动防御缺乏持久性的挑战，提供了一种更鲁棒和长期的防御策略。"}}
{"id": "2508.07797", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07797", "abs": "https://arxiv.org/abs/2508.07797", "authors": ["Xiaoqi Zhao", "Peiqian Cao", "Lihe Zhang", "Zonglei Feng", "Hanqi Liu", "Jiaming Zuo", "Youwei Pang", "Weisi Lin", "Georges El Fakhri", "Huchuan Lu", "Xiaofeng Liu"], "title": "Power Battery Detection", "comment": "Under submission to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (T-PAMI)", "summary": "Power batteries are essential components in electric vehicles, where internal\nstructural defects can pose serious safety risks. We conduct a comprehensive\nstudy on a new task, power battery detection (PBD), which aims to localize the\ndense endpoints of cathode and anode plates from industrial X-ray images for\nquality inspection. Manual inspection is inefficient and error-prone, while\ntraditional vision algorithms struggle with densely packed plates, low\ncontrast, scale variation, and imaging artifacts. To address this issue and\ndrive more attention into this meaningful task, we present PBD5K, the first\nlarge-scale benchmark for this task, consisting of 5,000 X-ray images from nine\nbattery types with fine-grained annotations and eight types of real-world\nvisual interference. To support scalable and consistent labeling, we develop an\nintelligent annotation pipeline that combines image filtering, model-assisted\npre-labeling, cross-verification, and layered quality evaluation. We formulate\nPBD as a point-level segmentation problem and propose MDCNeXt, a model designed\nto extract and integrate multi-dimensional structure clues including point,\nline, and count information from the plate itself. To improve discrimination\nbetween plates and suppress visual interference, MDCNeXt incorporates two state\nspace modules. The first is a prompt-filtered module that learns contrastive\nrelationships guided by task-specific prompts. The second is a density-aware\nreordering module that refines segmentation in regions with high plate density.\nIn addition, we propose a distance-adaptive mask generation strategy to provide\nrobust supervision under varying spatial distributions of anode and cathode\npositions. The source code and datasets will be publicly available at\n\\href{https://github.com/Xiaoqi-Zhao-DLUT/X-ray-PBD}{PBD5K}.", "AI": {"tldr": "论文引入了动力电池检测（PBD）任务，旨在从X射线图像中定位电池板密集端点以进行质量检测。它提出了首个大规模基准数据集PBD5K，并设计了MDCNeXt模型，该模型通过新颖模块和鲁棒的监督策略处理复杂的视觉挑战。", "motivation": "动力电池内部结构缺陷对电动汽车构成严重安全风险。人工检测效率低且易出错。传统视觉算法难以处理工业X射线图像中电池板密集、对比度低、尺度变化和成像伪影等挑战。", "method": "1. 任务定义：将动力电池检测（PBD）问题表述为点级分割任务。2. 数据集构建：构建了PBD5K，首个大规模基准数据集，包含5000张X射线图像，涵盖九种电池类型，具有精细标注和八种真实世界视觉干扰。3. 智能标注流程：开发了结合图像过滤、模型辅助预标注、交叉验证和分层质量评估的智能标注流程，以支持可扩展和一致的标注。4. 模型提出：提出了MDCNeXt模型，旨在从电池板本身提取并整合包括点、线和计数信息在内的多维结构线索。5. 模型模块：MDCNeXt集成了两个状态空间模块：一个提示过滤模块（学习对比关系）和一个密度感知重排序模块（在高密度区域细化分割），以提高板间区分并抑制视觉干扰。6. 监督策略：提出了一种距离自适应掩码生成策略，以在阳极和阴极位置空间分布变化的情况下提供鲁棒的监督。", "result": "1. 确立了PBD作为动力电池质量检测领域一项新的、有意义的任务。2. 创建了PBD5K，首个大规模、精细标注的X射线动力电池检测基准数据集，解决了数据稀缺问题。3. 开发了智能标注流程，确保了高质量和可扩展的数据标注。4. 提出了MDCNeXt，一个新颖的深度学习模型，能够处理复杂的视觉挑战（如密集板、低对比度、伪影），并整合多维结构线索。5. 所提出的方法旨在实现动力电池高效、准确的自动化质量检测。6. 数据集和源代码将公开可用，以促进进一步的研究。", "conclusion": "该论文通过引入一项新任务、一个全面的基准数据集（PBD5K）和一个创新的深度学习模型（MDCNeXt），成功解决了动力电池自动化缺陷检测的关键需求。这项工作为质量检测提供了鲁棒的解决方案，克服了传统方法的局限性，并为更安全的电动汽车铺平了道路，同时通过公开数据和代码促进了未来的研究。"}}
{"id": "2508.07803", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07803", "abs": "https://arxiv.org/abs/2508.07803", "authors": ["Yushen Xu", "Xiaosong Li", "Zhenyu Kuang", "Xiaoqi Cheng", "Haishu Tan", "Huafeng Li"], "title": "MambaTrans: Multimodal Fusion Image Translation via Large Language Model Priors for Downstream Visual Tasks", "comment": null, "summary": "The goal of multimodal image fusion is to integrate complementary information\nfrom infrared and visible images, generating multimodal fused images for\ndownstream tasks. Existing downstream pre-training models are typically trained\non visible images. However, the significant pixel distribution differences\nbetween visible and multimodal fusion images can degrade downstream task\nperformance, sometimes even below that of using only visible images. This paper\nexplores adapting multimodal fused images with significant modality differences\nto object detection and semantic segmentation models trained on visible images.\nTo address this, we propose MambaTrans, a novel multimodal fusion image\nmodality translator. MambaTrans uses descriptions from a multimodal large\nlanguage model and masks from semantic segmentation models as input. Its core\ncomponent, the Multi-Model State Space Block, combines mask-image-text\ncross-attention and a 3D-Selective Scan Module, enhancing pure visual\ncapabilities. By leveraging object detection prior knowledge, MambaTrans\nminimizes detection loss during training and captures long-term dependencies\namong text, masks, and images. This enables favorable results in pre-trained\nmodels without adjusting their parameters. Experiments on public datasets show\nthat MambaTrans effectively improves multimodal image performance in downstream\ntasks.", "AI": {"tldr": "该论文提出MambaTrans，一个多模态融合图像模态转换器，旨在解决融合图像与可见光图像像素分布差异导致下游任务性能下降的问题，使其能更好地适应基于可见光图像训练的模型。", "motivation": "现有的下游预训练模型通常在可见光图像上训练，但多模态融合图像与可见光图像之间显著的像素分布差异会导致下游任务性能下降，有时甚至低于仅使用可见光图像的性能。", "method": "提出MambaTrans，一种新型多模态融合图像模态转换器。它以多模态大语言模型的描述和语义分割模型的掩码作为输入。其核心组件是多模型状态空间块，结合了掩码-图像-文本交叉注意力机制和3D选择性扫描模块，以增强纯视觉能力。通过利用目标检测先验知识，MambaTrans在训练过程中最小化检测损失，并捕获文本、掩码和图像之间的长期依赖关系，从而使预训练模型在不调整参数的情况下也能获得良好结果。", "result": "在公共数据集上的实验表明，MambaTrans有效提升了多模态图像在下游任务中的表现。", "conclusion": "MambaTrans成功地将具有显著模态差异的多模态融合图像适配到基于可见光图像训练的目标检测和语义分割模型中，有效提高了下游任务的性能，且无需调整预训练模型的参数。"}}
{"id": "2508.07804", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07804", "abs": "https://arxiv.org/abs/2508.07804", "authors": ["Bao Li", "Xiaomei Zhang", "Miao Xu", "Zhaoxin Fan", "Xiangyu Zhu", "Zhen Lei"], "title": "Pose-RFT: Enhancing MLLMs for 3D Pose Generation via Hybrid Action Reinforcement Fine-Tuning", "comment": null, "summary": "Generating 3D human poses from multimodal inputs such as images or text\nrequires models to capture both rich spatial and semantic correspondences.\nWhile pose-specific multimodal large language models (MLLMs) have shown promise\nin this task, they are typically trained with supervised objectives such as\nSMPL parameter regression or token-level prediction, which struggle to model\nthe inherent ambiguity and achieve task-specific alignment required for\naccurate 3D pose generation. To address these limitations, we propose Pose-RFT,\na reinforcement fine-tuning framework tailored for 3D human pose generation in\nMLLMs. We formulate the task as a hybrid action reinforcement learning problem\nthat jointly optimizes discrete language prediction and continuous pose\ngeneration. To this end, we introduce HyGRPO, a hybrid reinforcement learning\nalgorithm that performs group-wise reward normalization over sampled responses\nto guide joint optimization of discrete and continuous actions. Pose-RFT\nfurther incorporates task-specific reward functions to guide optimization\ntowards spatial alignment in image-to-pose generation and semantic consistency\nin text-to-pose generation. Extensive experiments on multiple pose generation\nbenchmarks demonstrate that Pose-RFT significantly improves performance over\nexisting pose-specific MLLMs, validating the effectiveness of hybrid action\nreinforcement fine-tuning for 3D pose generation.", "AI": {"tldr": "提出Pose-RFT，一个基于强化学习的微调框架，用于改进多模态大语言模型（MLLMs）在3D人体姿态生成任务中的表现，通过混合动作强化学习和任务特定奖励函数解决监督学习的局限性。", "motivation": "现有的姿态特异性多模态大语言模型（MLLMs）通常采用监督学习目标（如SMPL参数回归或token级预测），难以建模固有的模糊性并实现准确3D姿态生成所需的任务特定对齐。", "method": "提出了Pose-RFT框架，将任务公式化为混合动作强化学习问题，共同优化离散语言预测和连续姿态生成。引入了HyGRPO混合强化学习算法，通过对采样响应进行组内奖励归一化来指导离散和连续动作的联合优化。此外，Pose-RFT还整合了任务特定奖励函数，以在图像到姿态生成中指导空间对齐，在文本到姿态生成中指导语义一致性。", "result": "在多个姿态生成基准测试上进行的广泛实验表明，Pose-RFT显著优于现有的姿态特异性MLLMs。", "conclusion": "混合动作强化微调对于3D姿态生成任务是有效的，Pose-RFT框架成功解决了现有方法的局限性，提高了姿态生成的准确性和对齐性。"}}
{"id": "2508.07811", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07811", "abs": "https://arxiv.org/abs/2508.07811", "authors": ["Sicheng Gao", "Nancy Mehta", "Zongwei Wu", "Radu Timofte"], "title": "DiTVR: Zero-Shot Diffusion Transformer for Video Restoration", "comment": "7 pages, 6 figures", "summary": "Video restoration aims to reconstruct high quality video sequences from low\nquality inputs, addressing tasks such as super resolution, denoising, and\ndeblurring. Traditional regression based methods often produce unrealistic\ndetails and require extensive paired datasets, while recent generative\ndiffusion models face challenges in ensuring temporal consistency. We introduce\nDiTVR, a zero shot video restoration framework that couples a diffusion\ntransformer with trajectory aware attention and a wavelet guided, flow\nconsistent sampler. Unlike prior 3D convolutional or frame wise diffusion\napproaches, our attention mechanism aligns tokens along optical flow\ntrajectories, with particular emphasis on vital layers that exhibit the highest\nsensitivity to temporal dynamics. A spatiotemporal neighbour cache dynamically\nselects relevant tokens based on motion correspondences across frames. The flow\nguided sampler injects data consistency only into low-frequency bands,\npreserving high frequency priors while accelerating convergence. DiTVR\nestablishes a new zero shot state of the art on video restoration benchmarks,\ndemonstrating superior temporal consistency and detail preservation while\nremaining robust to flow noise and occlusions.", "AI": {"tldr": "DiTVR是一种零样本视频修复框架，结合了扩散Transformer、轨迹感知注意力机制和小波引导的流一致采样器，解决了传统方法细节不真实和扩散模型时间一致性差的问题，实现了SOTA性能。", "motivation": "传统的基于回归的视频修复方法常产生不真实的细节且需要大量配对数据集；近期的生成式扩散模型在确保时间一致性方面面临挑战。", "method": "DiTVR框架采用扩散Transformer，其轨迹感知注意力机制沿光流轨迹对齐tokens，并强调对时间动态最敏感的关键层。通过时空邻居缓存根据帧间运动对应关系动态选择相关tokens。流引导采样器仅将数据一致性注入低频带，以保留高频先验并加速收敛。", "result": "DiTVR在视频修复基准测试上建立了新的零样本SOTA，展示了卓越的时间一致性和细节保留能力，同时对流噪声和遮挡具有鲁棒性。", "conclusion": "DiTVR通过创新的注意力机制和采样策略，成功克服了视频修复中传统方法的局限性和扩散模型的时间一致性问题，在零样本设置下达到了领先的性能。"}}
{"id": "2508.07812", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07812", "abs": "https://arxiv.org/abs/2508.07812", "authors": ["Jingze Gai", "Changchun Li"], "title": "Semi-supervised Multiscale Matching for SAR-Optical Image", "comment": "15 pages, 9 figures", "summary": "Driven by the complementary nature of optical and synthetic aperture radar\n(SAR) images, SAR-optical image matching has garnered significant interest.\nMost existing SAR-optical image matching methods aim to capture effective\nmatching features by employing the supervision of pixel-level matched\ncorrespondences within SAR-optical image pairs, which, however, suffers from\ntime-consuming and complex manual annotation, making it difficult to collect\nsufficient labeled SAR-optical image pairs. To handle this, we design a\nsemi-supervised SAR-optical image matching pipeline that leverages both scarce\nlabeled and abundant unlabeled image pairs and propose a semi-supervised\nmultiscale matching for SAR-optical image matching (S2M2-SAR). Specifically, we\npseudo-label those unlabeled SAR-optical image pairs with pseudo ground-truth\nsimilarity heatmaps by combining both deep and shallow level matching results,\nand train the matching model by employing labeled and pseudo-labeled similarity\nheatmaps. In addition, we introduce a cross-modal feature enhancement module\ntrained using a cross-modality mutual independence loss, which requires no\nground-truth labels. This unsupervised objective promotes the separation of\nmodality-shared and modality-specific features by encouraging statistical\nindependence between them, enabling effective feature disentanglement across\noptical and SAR modalities. To evaluate the effectiveness of S2M2-SAR, we\ncompare it with existing competitors on benchmark datasets. Experimental\nresults demonstrate that S2M2-SAR not only surpasses existing semi-supervised\nmethods but also achieves performance competitive with fully supervised SOTA\nmethods, demonstrating its efficiency and practical potential.", "AI": {"tldr": "该论文提出了一种半监督SAR-光学图像匹配方法（S2M2-SAR），通过伪标签化无标签数据和跨模态特征增强，解决了传统全监督方法对大量像素级标注的依赖，实现了与最先进全监督方法相当的性能。", "motivation": "现有的SAR-光学图像匹配方法严重依赖于像素级匹配对应关系的监督，这需要耗时且复杂的标注，导致难以收集足够的标签数据。因此，需要一种能够利用有限标签数据和大量无标签数据的方法。", "method": "该方法设计了一个半监督管道S2M2-SAR。具体而言，它通过结合深度和浅层匹配结果，为无标签SAR-光学图像对生成伪地面真值相似度热图。然后，使用标签数据和伪标签数据训练匹配模型。此外，引入了一个跨模态特征增强模块，该模块通过无监督的跨模态互独立损失进行训练，以促进模态共享和模态特定特征的分离。", "result": "实验结果表明，S2M2-SAR不仅超越了现有的半监督方法，而且在基准数据集上达到了与全监督SOTA方法相当的性能。", "conclusion": "S2M2-SAR展示了其在SAR-光学图像匹配中的高效性和实际潜力，通过有效利用有限标签和大量无标签数据，解决了数据标注的难题，并实现了高性能的跨模态匹配。"}}
{"id": "2508.07818", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07818", "abs": "https://arxiv.org/abs/2508.07818", "authors": ["Chenyue Song", "Chen Hui", "Haiqi Zhu", "Feng Jiang", "Yachun Mi", "Wei Zhang", "Shaohui Liu"], "title": "Segmenting and Understanding: Region-aware Semantic Attention for Fine-grained Image Quality Assessment with Large Language Models", "comment": null, "summary": "No-reference image quality assessment (NR-IQA) aims to simulate the process\nof perceiving image quality aligned with subjective human perception. However,\nexisting NR-IQA methods either focus on global representations that leads to\nlimited insights into the semantically salient regions or employ a uniform\nweighting for region features that weakens the sensitivity to local quality\nvariations. In this paper, we propose a fine-grained image quality assessment\nmodel, named RSFIQA, which integrates region-level distortion information to\nperceive multi-dimensional quality discrepancies. To enhance regional quality\nawareness, we first utilize the Segment Anything Model (SAM) to dynamically\npartition the input image into non-overlapping semantic regions. For each\nregion, we teach a powerful Multi-modal Large Language Model (MLLM) to extract\ndescriptive content and perceive multi-dimensional distortions, enabling a\ncomprehensive understanding of both local semantics and quality degradations.\nTo effectively leverage this information, we introduce Region-Aware Semantic\nAttention (RSA) mechanism, which generates a global attention map by\naggregating fine-grained representations from local regions. In addition,\nRSFIQA is backbone-agnostic and can be seamlessly integrated into various deep\nneural network architectures. Extensive experiments demonstrate the robustness\nand effectiveness of the proposed method, which achieves competitive quality\nprediction performance across multiple benchmark datasets.", "AI": {"tldr": "RSFIQA是一种细粒度无参考图像质量评估模型，它利用SAM进行语义区域分割，并训练多模态大语言模型（MLLM）感知区域级失真，通过区域感知语义注意力机制整合信息，以更准确地模拟人类感知。", "motivation": "现有无参考图像质量评估（NR-IQA）方法存在局限性：要么只关注全局表示，导致对语义显著区域的洞察力有限；要么对区域特征采用统一加权，削弱了对局部质量变化的敏感性。", "method": "本文提出RSFIQA模型。首先，利用Segment Anything Model (SAM) 动态分割输入图像为非重叠语义区域。其次，训练多模态大语言模型（MLLM）提取每个区域的描述性内容并感知多维度失真。最后，引入区域感知语义注意力（RSA）机制，通过聚合局部区域的细粒度表示生成全局注意力图。RSFIQA与骨干网络无关，可无缝集成到各种深度神经网络架构中。", "result": "广泛的实验证明了所提出方法的鲁棒性和有效性，在多个基准数据集上取得了具有竞争力的质量预测性能。", "conclusion": "RSFIQA通过整合区域级失真信息和利用先进的分割及多模态语言模型，成功地实现了细粒度的图像质量评估，有效解决了现有方法的局限性，并展现出优异的性能。"}}
{"id": "2508.07833", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07833", "abs": "https://arxiv.org/abs/2508.07833", "authors": ["Animesh Jain", "Alexandros Stergiou"], "title": "MIMIC: Multimodal Inversion for Model Interpretation and Conceptualization", "comment": "Project page: https://anaekin.github.io/MIMIC", "summary": "Vision Language Models (VLMs) encode multimodal inputs over large, complex,\nand difficult-to-interpret architectures, which limit transparency and trust.\nWe propose a Multimodal Inversion for Model Interpretation and\nConceptualization (MIMIC) framework to visualize the internal representations\nof VLMs by synthesizing visual concepts corresponding to internal encodings.\nMIMIC uses a joint VLM-based inversion and a feature alignment objective to\naccount for VLM's autoregressive processing. It additionally includes a triplet\nof regularizers for spatial alignment, natural image smoothness, and semantic\nrealism. We quantitatively and qualitatively evaluate MIMIC by inverting visual\nconcepts over a range of varying-length free-form VLM output texts. Reported\nresults include both standard visual quality metrics as well as semantic\ntext-based metrics. To the best of our knowledge, this is the first model\ninversion approach addressing visual interpretations of VLM concepts.", "AI": {"tldr": "MIMIC是一个框架，通过合成视觉概念来可视化视觉语言模型（VLM）的内部表示，以提高其可解释性。", "motivation": "视觉语言模型（VLM）的复杂架构导致其内部运作难以解释，限制了透明度和信任度。", "method": "MIMIC框架采用联合VLM-based反演和特征对齐目标，以处理VLM的自回归处理。此外，它还包含三重正则化器，用于空间对齐、自然图像平滑度和语义真实性。", "result": "MIMIC通过反演不同长度自由形式VLM输出文本的视觉概念，进行了定量和定性评估。报告的结果包括标准视觉质量指标和基于语义文本的指标。", "conclusion": "MIMIC是首个解决VLM概念视觉解释的模型反演方法，有效提高了VLM的透明度和可解释性。"}}
{"id": "2508.07835", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07835", "abs": "https://arxiv.org/abs/2508.07835", "authors": ["Jingna Qiu", "Nishanth Jain", "Jonas Ammeling", "Marc Aubreville", "Katharina Breininger"], "title": "Effortless Vision-Language Model Specialization in Histopathology without Annotation", "comment": null, "summary": "Recent advances in Vision-Language Models (VLMs) in histopathology, such as\nCONCH and QuiltNet, have demonstrated impressive zero-shot classification\ncapabilities across various tasks. However, their general-purpose design may\nlead to suboptimal performance in specific downstream applications. While\nsupervised fine-tuning methods address this issue, they require manually\nlabeled samples for adaptation. This paper investigates annotation-free\nadaptation of VLMs through continued pretraining on domain- and task-relevant\nimage-caption pairs extracted from existing databases. Our experiments on two\nVLMs, CONCH and QuiltNet, across three downstream tasks reveal that these pairs\nsubstantially enhance both zero-shot and few-shot performance. Notably, with\nlarger training sizes, continued pretraining matches the performance of\nfew-shot methods while eliminating manual labeling. Its effectiveness,\ntask-agnostic design, and annotation-free workflow make it a promising pathway\nfor adapting VLMs to new histopathology tasks. Code is available at\nhttps://github.com/DeepMicroscopy/Annotation-free-VLM-specialization.", "AI": {"tldr": "本文提出了一种无标注的VLM适应方法，通过在组织病理学领域特定图像-文本对上进行持续预训练，显著提升了现有VLM在下游任务上的零样本和少样本性能。", "motivation": "现有的视觉-语言模型（VLM）如CONCH和QuiltNet在组织病理学领域表现出强大的零样本分类能力，但其通用设计可能导致在特定下游应用中性能不佳。传统的监督微调方法需要手动标注样本进行适应，成本高昂。", "method": "通过从现有数据库中提取领域和任务相关的图像-文本对，对VLM进行持续预训练，以实现模型的无标注适应。实验在CONCH和QuiltNet两个VLM上，跨越三个下游任务进行。", "result": "持续预训练显著提升了VLM的零样本和少样本性能。值得注意的是，在训练数据量足够大时，该方法在无需手动标注的情况下，性能可以媲美甚至达到少样本方法的水平。", "conclusion": "持续预训练是一种有效、任务无关且无需标注的工作流，为将VLM适应到新的组织病理学任务提供了一条有前景的途径。"}}
{"id": "2508.07838", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07838", "abs": "https://arxiv.org/abs/2508.07838", "authors": ["Qi Xiang", "Kunsong Shi", "Zhigui Lin", "Lei He"], "title": "CBDES MoE: Hierarchically Decoupled Mixture-of-Experts for Functional Modules in Autonomous Driving", "comment": null, "summary": "Bird's Eye View (BEV) perception systems based on multi-sensor feature fusion\nhave become a fundamental cornerstone for end-to-end autonomous driving.\nHowever, existing multi-modal BEV methods commonly suffer from limited input\nadaptability, constrained modeling capacity, and suboptimal generalization. To\naddress these challenges, we propose a hierarchically decoupled\nMixture-of-Experts architecture at the functional module level, termed\nComputing Brain DEvelopment System Mixture-of-Experts (CBDES MoE). CBDES MoE\nintegrates multiple structurally heterogeneous expert networks with a\nlightweight Self-Attention Router (SAR) gating mechanism, enabling dynamic\nexpert path selection and sparse, input-aware efficient inference. To the best\nof our knowledge, this is the first modular Mixture-of-Experts framework\nconstructed at the functional module granularity within the autonomous driving\ndomain. Extensive evaluations on the real-world nuScenes dataset demonstrate\nthat CBDES MoE consistently outperforms fixed single-expert baselines in 3D\nobject detection. Compared to the strongest single-expert model, CBDES MoE\nachieves a 1.6-point increase in mAP and a 4.1-point improvement in NDS,\ndemonstrating the effectiveness and practical advantages of the proposed\napproach.", "AI": {"tldr": "本文提出CBDES MoE，一种用于自动驾驶BEV感知的模块化专家混合（MoE）架构，通过动态专家选择解决了现有方法的局限性，并在nuScenes数据集上显著提升了3D目标检测性能。", "motivation": "现有基于多传感器特征融合的BEV感知系统在输入适应性、建模能力和泛化性方面存在局限性，无法满足端到端自动驾驶的需求。", "method": "提出CBDES MoE，一种功能模块级别的分层解耦专家混合架构。它集成了多个结构异构的专家网络，并结合轻量级自注意力路由器（SAR）门控机制，实现动态专家路径选择和稀疏、输入感知的有效推理。这是自动驾驶领域首个在功能模块粒度上构建的模块化MoE框架。", "result": "在nuScenes数据集上的广泛评估表明，CBDES MoE在3D目标检测方面持续优于固定的单专家基线。与最强的单专家模型相比，CBDES MoE在mAP上提高了1.6点，在NDS上提高了4.1点。", "conclusion": "CBDES MoE方法有效且具有实际优势，能够显著提升自动驾驶中BEV感知的性能，解决了现有系统的输入适应性、建模能力和泛化性问题。"}}
{"id": "2508.07850", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07850", "abs": "https://arxiv.org/abs/2508.07850", "authors": ["Noriko Nitta", "Rei Miyata", "Naoto Oishi"], "title": "Morphological Analysis of Semiconductor Microstructures using Skeleton Graphs", "comment": "CV4MS: Computer Vision for Materials Science, Workshop in conjunction\n  with the IEEE/CVF ICCV 2025", "summary": "In this paper, electron microscopy images of microstructures formed on Ge\nsurfaces by ion beam irradiation were processed to extract topological features\nas skeleton graphs, which were then embedded using a graph convolutional\nnetwork. The resulting embeddings were analyzed using principal component\nanalysis, and cluster separability in the resulting PCA space was evaluated\nusing the Davies-Bouldin index. The results indicate that variations in\nirradiation angle have a more significant impact on the morphological\nproperties of Ge surfaces than variations in irradiation fluence.", "AI": {"tldr": "本文利用图卷积网络对离子束辐照Ge表面微观结构的骨架图进行嵌入，并通过PCA分析和Davies-Bouldin指数评估，发现辐照角度对Ge表面形貌影响大于辐照剂量。", "motivation": "研究离子束辐照参数（辐照角度和辐照剂量）对Ge表面微观结构形貌特性的影响。", "method": "处理Ge表面电子显微镜图像，提取拓扑特征为骨架图；使用图卷积网络对骨架图进行嵌入；利用主成分分析（PCA）分析嵌入结果；使用Davies-Bouldin指数评估PCA空间中的聚类可分离性。", "result": "结果表明，辐照角度的变化对Ge表面形貌特性的影响比辐照剂量的变化更为显著。", "conclusion": "辐照角度是影响Ge表面形貌的关键参数，其影响程度大于辐照剂量。"}}
{"id": "2508.07851", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07851", "abs": "https://arxiv.org/abs/2508.07851", "authors": ["Konrad Reuter", "Suresh Guttikonda", "Sarah Latus", "Lennart Maack", "Christian Betz", "Tobias Maurer", "Alexander Schlaefer"], "title": "Tracking Any Point Methods for Markerless 3D Tissue Tracking in Endoscopic Stereo Images", "comment": "Accecpted to CURAC conference 2025", "summary": "Minimally invasive surgery presents challenges such as dynamic tissue motion\nand a limited field of view. Accurate tissue tracking has the potential to\nsupport surgical guidance, improve safety by helping avoid damage to sensitive\nstructures, and enable context-aware robotic assistance during complex\nprocedures. In this work, we propose a novel method for markerless 3D tissue\ntracking by leveraging 2D Tracking Any Point (TAP) networks. Our method\ncombines two CoTracker models, one for temporal tracking and one for stereo\nmatching, to estimate 3D motion from stereo endoscopic images. We evaluate the\nsystem using a clinical laparoscopic setup and a robotic arm simulating tissue\nmotion, with experiments conducted on a synthetic 3D-printed phantom and a\nchicken tissue phantom. Tracking on the chicken tissue phantom yielded more\nreliable results, with Euclidean distance errors as low as 1.1 mm at a velocity\nof 10 mm/s. These findings highlight the potential of TAP-based models for\naccurate, markerless 3D tracking in challenging surgical scenarios.", "AI": {"tldr": "该研究提出了一种利用2D TAP（Tracking Any Point）网络进行无标记3D组织跟踪的新方法，以应对微创手术中的挑战。", "motivation": "微创手术面临组织动态运动和视野受限的挑战。精确的组织跟踪能够支持手术引导，通过避免损伤敏感结构提高安全性，并在复杂手术中实现情境感知的机器人辅助。", "method": "本方法利用2D TAP网络，结合两个CoTracker模型（一个用于时间跟踪，一个用于立体匹配），从立体内窥镜图像中估计3D运动。系统在临床腹腔镜设置和模拟组织运动的机械臂上进行了评估。", "result": "在合成3D打印模型和鸡组织模型上进行了实验。鸡组织模型上的跟踪结果更可靠，在10毫米/秒的速度下，欧几里得距离误差低至1.1毫米。", "conclusion": "研究结果突出了基于TAP的模型在挑战性手术场景中实现精确、无标记3D跟踪的潜力。"}}
{"id": "2508.07863", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07863", "abs": "https://arxiv.org/abs/2508.07863", "authors": ["Bin Cao", "Sipeng Zheng", "Ye Wang", "Lujie Xia", "Qianshan Wei", "Qin Jin", "Jing Liu", "Zongqing Lu"], "title": "Being-M0.5: A Real-Time Controllable Vision-Language-Motion Model", "comment": "16 pages", "summary": "Human motion generation has emerged as a critical technology with\ntransformative potential for real-world applications. However, existing\nvision-language-motion models (VLMMs) face significant limitations that hinder\ntheir practical deployment. We identify controllability as a main bottleneck,\nmanifesting in five key aspects: inadequate response to diverse human commands,\nlimited pose initialization capabilities, poor performance on long-term\nsequences, insufficient handling of unseen scenarios, and lack of fine-grained\ncontrol over individual body parts. To overcome these limitations, we present\nBeing-M0.5, the first real-time, controllable VLMM that achieves\nstate-of-the-art performance across multiple motion generation tasks. Our\napproach is built upon HuMo100M, the largest and most comprehensive human\nmotion dataset to date, comprising over 5 million self-collected motion\nsequences, 100 million multi-task instructional instances, and detailed\npart-level annotations that address a critical gap in existing datasets. We\nintroduce a novel part-aware residual quantization technique for motion\ntokenization that enables precise, granular control over individual body parts\nduring generation. Extensive experimental validation demonstrates Being-M0.5's\nsuperior performance across diverse motion benchmarks, while comprehensive\nefficiency analysis confirms its real-time capabilities. Our contributions\ninclude design insights and detailed computational analysis to guide future\ndevelopment of practical motion generators. We believe that HuMo100M and\nBeing-M0.5 represent significant advances that will accelerate the adoption of\nmotion generation technologies in real-world applications. The project page is\navailable at https://beingbeyond.github.io/Being-M0.5.", "AI": {"tldr": "该论文提出了Being-M0.5，首个实时、可控的视觉-语言-运动模型（VLMM），通过构建迄今最大的运动数据集HuMo100M和引入部位感知残差量化技术，解决了现有VLMM在控制性方面的局限性，并在多项运动生成任务中实现了最先进的性能。", "motivation": "现有视觉-语言-运动模型（VLMMs）在实际部署中面临显著限制，尤其是在可控性方面。具体表现为：对多样化人类指令响应不足、姿态初始化能力有限、长序列性能差、未见场景处理能力不足以及缺乏对身体各部位的细粒度控制。", "method": "本研究提出了Being-M0.5模型，并构建了迄今为止最大、最全面的运动数据集HuMo100M，包含超过500万个自收集运动序列、1亿个多任务指令实例和详细的部位级别标注。此外，引入了一种新颖的部位感知残差量化技术用于运动分词，以实现生成过程中对身体各部位的精确、细粒度控制。", "result": "Being-M0.5在多个运动生成任务中实现了最先进的性能，并在多样化的运动基准测试中展现出卓越表现。全面的效率分析证实了其实时能力。研究还提供了设计见解和详细的计算分析，以指导未来实用运动生成器的开发。", "conclusion": "HuMo100M数据集和Being-M0.5模型代表了运动生成技术的重大进步，将加速其在现实世界应用中的采纳和部署。"}}
{"id": "2508.07871", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07871", "abs": "https://arxiv.org/abs/2508.07871", "authors": ["Yanshu Li", "Jianjiang Yang", "Zhennan Shen", "Ligong Han", "Haoyan Xu", "Ruixiang Tang"], "title": "CATP: Contextually Adaptive Token Pruning for Efficient and Enhanced Multimodal In-Context Learning", "comment": "13 pages, 12 figures, 6 tables", "summary": "Modern large vision-language models (LVLMs) convert each input image into a\nlarge set of tokens, far outnumbering the text tokens. Although this improves\nvisual perception, it introduces severe image token redundancy. Because image\ntokens carry sparse information, many add little to reasoning, yet greatly\nincrease inference cost. The emerging image token pruning methods tackle this\nissue by identifying the most important tokens and discarding the rest. These\nmethods can raise efficiency with only modest performance loss. However, most\nof them only consider single-image tasks and overlook multimodal in-context\nlearning (ICL), where redundancy is greater and efficiency is more critical.\nRedundant tokens weaken the advantage of multimodal ICL for rapid domain\nadaptation and cause unstable performance. Applying existing pruning methods in\nthis setting leads to large accuracy drops, exposing a clear gap and the need\nfor new techniques. Thus, we propose Contextually Adaptive Token Pruning\n(CATP), a training-free pruning method targeted at multimodal ICL. CATP\nconsists of two stages that perform progressive pruning to fully account for\nthe complex cross-modal interactions in the input sequence. After removing\n77.8\\% of the image tokens, CATP produces an average performance gain of 0.6\\%\nover the vanilla model on four LVLMs and eight benchmarks, exceeding all\nbaselines remarkably. Meanwhile, it effectively improves efficiency by\nachieving an average reduction of 10.78\\% in inference latency. CATP enhances\nthe practical value of multimodal ICL and lays the groundwork for future\nprogress in interleaved image-text scenarios.", "AI": {"tldr": "针对大型视觉语言模型（LVLMs）中图像令牌冗余，尤其是在多模态上下文学习（ICL）场景下，本文提出了一种名为CATP的免训练渐进式令牌剪枝方法，显著提升了效率和性能。", "motivation": "现代LVLMs将图像转换为大量令牌，远超文本令牌，导致图像令牌冗余，增加推理成本。在多模态ICL中，冗余问题更严重，现有剪枝方法在此场景下效果不佳，导致性能下降和不稳定，因此需要新的技术来解决此问题。", "method": "本文提出上下文自适应令牌剪枝（CATP）方法，这是一种免训练的渐进式剪枝方法。它包含两个阶段，旨在充分考虑输入序列中复杂的跨模态交互，从而识别并移除冗余图像令牌。", "result": "CATP成功移除了77.8%的图像令牌，在四个LVLMs和八个基准测试中，相较于原始模型，平均性能提升0.6%，且显著优于所有基线。同时，它有效提高了效率，平均推理延迟降低了10.78%。", "conclusion": "CATP增强了多模态ICL的实用价值，并为未来交错图像-文本场景的进一步发展奠定了基础。"}}
{"id": "2508.07878", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07878", "abs": "https://arxiv.org/abs/2508.07878", "authors": ["Hanting Wang", "Shengpeng Ji", "Shulei Wang", "Hai Huang", "Xiao Jin", "Qifei Zhang", "Tao Jin"], "title": "TAP: Parameter-efficient Task-Aware Prompting for Adverse Weather Removal", "comment": null, "summary": "Image restoration under adverse weather conditions has been extensively\nexplored, leading to numerous high-performance methods. In particular, recent\nadvances in All-in-One approaches have shown impressive results by training on\nmulti-task image restoration datasets. However, most of these methods rely on\ndedicated network modules or parameters for each specific degradation type,\nresulting in a significant parameter overhead. Moreover, the relatedness across\ndifferent restoration tasks is often overlooked. In light of these issues, we\npropose a parameter-efficient All-in-One image restoration framework that\nleverages task-aware enhanced prompts to tackle various adverse weather\ndegradations.Specifically, we adopt a two-stage training paradigm consisting of\na pretraining phase and a prompt-tuning phase to mitigate parameter conflicts\nacross tasks. We first employ supervised learning to acquire general\nrestoration knowledge, and then adapt the model to handle specific degradation\nvia trainable soft prompts. Crucially, we enhance these task-specific prompts\nin a task-aware manner. We apply low-rank decomposition to these prompts to\ncapture both task-general and task-specific characteristics, and impose\ncontrastive constraints to better align them with the actual inter-task\nrelatedness. These enhanced prompts not only improve the parameter efficiency\nof the restoration model but also enable more accurate task modeling, as\nevidenced by t-SNE analysis. Experimental results on different restoration\ntasks demonstrate that the proposed method achieves superior performance with\nonly 2.75M parameters.", "AI": {"tldr": "提出了一种参数高效的一体化图像恢复框架，通过任务感知的增强型提示（prompt）来处理多种恶劣天气退化，显著降低了参数量并提高了性能。", "motivation": "现有的一体化图像恢复方法通常为每种退化类型使用专用的网络模块或参数，导致巨大的参数开销，并且忽视了不同恢复任务之间的相关性。", "method": "采用两阶段训练范式：预训练阶段学习通用恢复知识，然后通过可训练的软提示进行提示微调，以适应特定退化。关键在于，通过低秩分解捕获任务通用和特定特征，并施加对比约束以更好地对齐任务间相关性，从而增强这些任务特定的提示。", "result": "实验结果表明，所提出的方法在不同恢复任务上取得了卓越的性能，且仅使用了2.75M参数。t-SNE分析也证明了其提高了参数效率并实现了更精确的任务建模。", "conclusion": "该方法有效解决了现有一体化图像恢复模型中参数开销大和任务间相关性被忽视的问题，通过创新的任务感知增强型提示实现了高效且高性能的图像恢复。"}}
{"id": "2508.07901", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07901", "abs": "https://arxiv.org/abs/2508.07901", "authors": ["Bowen Xue", "Qixin Yan", "Wenjing Wang", "Hao Liu", "Chen Li"], "title": "Stand-In: A Lightweight and Plug-and-Play Identity Control for Video Generation", "comment": null, "summary": "Generating high-fidelity human videos that match user-specified identities is\nimportant yet challenging in the field of generative AI. Existing methods often\nrely on an excessive number of training parameters and lack compatibility with\nother AIGC tools. In this paper, we propose Stand-In, a lightweight and\nplug-and-play framework for identity preservation in video generation.\nSpecifically, we introduce a conditional image branch into the pre-trained\nvideo generation model. Identity control is achieved through restricted\nself-attentions with conditional position mapping, and can be learned quickly\nwith only 2000 pairs. Despite incorporating and training just $\\sim$1\\%\nadditional parameters, our framework achieves excellent results in video\nquality and identity preservation, outperforming other full-parameter training\nmethods. Moreover, our framework can be seamlessly integrated for other tasks,\nsuch as subject-driven video generation, pose-referenced video generation,\nstylization, and face swapping.", "AI": {"tldr": "该论文提出了Stand-In，一个轻量级、即插即用的视频生成身份保持框架，通过引入条件图像分支和受限自注意力机制，在少量额外参数下实现了高质量视频生成和身份保留，并能与其他AIGC工具兼容。", "motivation": "在生成式AI领域，生成与用户指定身份匹配的高保真人物视频既重要又具挑战性。现有方法通常需要过多的训练参数，且缺乏与其他AIGC工具的兼容性。", "method": "本文提出了Stand-In框架，它通过在预训练视频生成模型中引入一个条件图像分支来实现身份保持。身份控制通过带有条件位置映射的受限自注意力机制实现，并且只需约2000对数据即可快速学习。整个框架仅增加和训练约1%的额外参数。", "result": "该框架在视频质量和身份保持方面取得了优异结果，超越了其他全参数训练方法。此外，它还能无缝集成到其他任务中，如主体驱动视频生成、姿态参考视频生成、风格化和换脸等。", "conclusion": "Stand-In框架提供了一个高效且兼容性强的解决方案，以解决视频生成中的身份保持挑战，其轻量级设计和出色的性能使其在多种AIGC应用中具有广泛潜力。"}}
{"id": "2508.07904", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07904", "abs": "https://arxiv.org/abs/2508.07904", "authors": ["Marco Peer", "Anna Scius-Bertrand", "Andreas Fischer"], "title": "CTC Transcription Alignment of the Bullinger Letters: Automatic Improvement of Annotation Quality", "comment": "10 pages, 2 pages supplementary material. Accepted for\n  VisionDocs@ICCV2025", "summary": "Handwritten text recognition for historical documents remains challenging due\nto handwriting variability, degraded sources, and limited layout-aware\nannotations. In this work, we address annotation errors - particularly\nhyphenation issues - in the Bullinger correspondence, a large 16th-century\nletter collection. We introduce a self-training method based on a CTC alignment\nalgorithm that matches full transcriptions to text line images using dynamic\nprogramming and model output probabilities trained with the CTC loss. Our\napproach improves performance (e.g., by 1.1 percentage points CER with PyLaia)\nand increases alignment accuracy. Interestingly, we find that weaker models\nyield more accurate alignments, enabling an iterative training strategy. We\nrelease a new manually corrected subset of 100 pages from the Bullinger\ndataset, along with our code and benchmarks. Our approach can be applied\niteratively to further improve the CER as well as the alignment quality for\ntext recognition pipelines. Code and data are available via\nhttps://github.com/andreas-fischer-unifr/nntp.", "AI": {"tldr": "本文提出了一种基于CTC对齐的自训练方法，用于纠正历史手写文档（特别是Bullinger通信集）中的标注错误，尤其是连字符问题，从而提高手写文本识别的性能和对齐准确性。", "motivation": "历史文档的手写文本识别面临挑战，包括手写变异性、源文档退化和有限的版面感知标注。特别是，大型历史文献集中的标注错误（如连字符问题）严重影响了识别质量。", "method": "引入了一种自训练方法，该方法基于CTC（Connectionist Temporal Classification）对齐算法，利用动态规划和CTC损失训练的模型输出概率，将完整的文本转录与文本行图像进行匹配。同时，发现较弱的模型能产生更准确的对齐，支持迭代训练策略。", "result": "该方法显著提升了性能（例如，使用PyLaia时字符错误率CER降低1.1个百分点），并提高了对齐准确性。研究还发现，性能较弱的模型反而能生成更准确的对齐，这为迭代训练提供了基础。作者发布了一个包含100页Bullinger数据集的手动校正子集，以及相关的代码和基准。", "conclusion": "所提出的自训练方法能够有效纠正历史文档中的标注错误，提高手写文本识别的性能和对齐质量。该方法可以迭代应用于进一步提升CER和对齐质量，对文本识别流程具有普适性。"}}
{"id": "2508.07905", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07905", "abs": "https://arxiv.org/abs/2508.07905", "authors": ["Yongtao Ge", "Kangyang Xie", "Guangkai Xu", "Mingyu Liu", "Li Ke", "Longtao Huang", "Hui Xue", "Hao Chen", "Chunhua Shen"], "title": "Generative Video Matting", "comment": null, "summary": "Video matting has traditionally been limited by the lack of high-quality\nground-truth data. Most existing video matting datasets provide only\nhuman-annotated imperfect alpha and foreground annotations, which must be\ncomposited to background images or videos during the training stage. Thus, the\ngeneralization capability of previous methods in real-world scenarios is\ntypically poor. In this work, we propose to solve the problem from two\nperspectives. First, we emphasize the importance of large-scale pre-training by\npursuing diverse synthetic and pseudo-labeled segmentation datasets. We also\ndevelop a scalable synthetic data generation pipeline that can render diverse\nhuman bodies and fine-grained hairs, yielding around 200 video clips with a\n3-second duration for fine-tuning. Second, we introduce a novel video matting\napproach that can effectively leverage the rich priors from pre-trained video\ndiffusion models. This architecture offers two key advantages. First, strong\npriors play a critical role in bridging the domain gap between synthetic and\nreal-world scenes. Second, unlike most existing methods that process video\nmatting frame-by-frame and use an independent decoder to aggregate temporal\ninformation, our model is inherently designed for video, ensuring strong\ntemporal consistency. We provide a comprehensive quantitative evaluation across\nthree benchmark datasets, demonstrating our approach's superior performance,\nand present comprehensive qualitative results in diverse real-world scenes,\nillustrating the strong generalization capability of our method. The code is\navailable at https://github.com/aim-uofa/GVM.", "AI": {"tldr": "针对视频抠图领域高质量真值数据缺乏和泛化能力差的问题，本文提出结合大规模预训练（使用多样化的合成和伪标签数据）与利用预训练视频扩散模型先验的新型视频抠图方法，显著提升了模型在真实场景中的泛化能力和时间一致性。", "motivation": "现有视频抠图方法受限于高质量真值数据的缺乏（现有数据集注释不完善），导致在真实世界场景中的泛化能力普遍较差。", "method": "1. 强调大规模预训练的重要性，利用多样化的合成和伪标签分割数据集进行预训练。2. 开发可扩展的合成数据生成流程，生成包含多样化人体和精细毛发的高质量视频片段（约200个3秒视频用于微调）。3. 引入一种新型视频抠图方法，有效利用预训练视频扩散模型的丰富先验知识，以弥补合成与真实场景之间的领域差距。4. 模型内在设计为处理视频而非逐帧处理，确保了强大的时间一致性。", "result": "在三个基准数据集上进行了全面的定量评估，证明了所提出方法的卓越性能；在多样化的真实世界场景中展示了全面的定性结果，体现了该方法强大的泛化能力。", "conclusion": "通过结合大规模预训练和利用预训练视频扩散模型的丰富先验，本文提出的方法有效解决了视频抠图领域数据稀缺和时间一致性差的问题，显著提升了模型在真实世界场景中的性能和泛化能力。"}}
{"id": "2508.07908", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07908", "abs": "https://arxiv.org/abs/2508.07908", "authors": ["Xudong Cai", "Shuo Wang", "Peng Wang", "Yongcai Wang", "Zhaoxin Fan", "Wanting Li", "Tianbao Zhang", "Jianrong Tao", "Yeying Jin", "Deying Li"], "title": "Mem4D: Decoupling Static and Dynamic Memory for Dynamic Scene Reconstruction", "comment": null, "summary": "Reconstructing dense geometry for dynamic scenes from a monocular video is a\ncritical yet challenging task. Recent memory-based methods enable efficient\nonline reconstruction, but they fundamentally suffer from a Memory Demand\nDilemma: The memory representation faces an inherent conflict between the\nlong-term stability required for static structures and the rapid, high-fidelity\ndetail retention needed for dynamic motion. This conflict forces existing\nmethods into a compromise, leading to either geometric drift in static\nstructures or blurred, inaccurate reconstructions of dynamic objects. To\naddress this dilemma, we propose Mem4D, a novel framework that decouples the\nmodeling of static geometry and dynamic motion. Guided by this insight, we\ndesign a dual-memory architecture: 1) The Transient Dynamics Memory (TDM)\nfocuses on capturing high-frequency motion details from recent frames, enabling\naccurate and fine-grained modeling of dynamic content; 2) The Persistent\nStructure Memory (PSM) compresses and preserves long-term spatial information,\nensuring global consistency and drift-free reconstruction for static elements.\nBy alternating queries to these specialized memories, Mem4D simultaneously\nmaintains static geometry with global consistency and reconstructs dynamic\nelements with high fidelity. Experiments on challenging benchmarks demonstrate\nthat our method achieves state-of-the-art or competitive performance while\nmaintaining high efficiency. Codes will be publicly available.", "AI": {"tldr": "Mem4D提出了一种双内存架构，用于从单目视频中重建动态场景的密集几何，通过解耦静态结构和动态运动的建模，解决了现有方法的内存需求困境，实现了静态几何的全局一致性和动态元素的精细重建。", "motivation": "现有基于内存的单目视频动态场景重建方法面临“内存需求困境”，即静态结构所需的长期稳定性和动态运动所需的高保真细节保留之间存在冲突，导致静态结构几何漂移或动态对象重建模糊不准确。", "method": "提出Mem4D框架，解耦静态几何和动态运动建模。设计了双内存架构：1) 瞬态动态内存（TDM）捕获最近帧的高频运动细节，实现动态内容的准确精细建模；2) 持久结构内存（PSM）压缩并保存长期空间信息，确保静态元素的全局一致性和无漂移重建。通过交替查询这两种专用内存，同时维护静态几何的全局一致性和高保真重建动态元素。", "result": "在挑战性基准测试中，Mem4D方法实现了最先进或具有竞争力的性能，同时保持了高效率。", "conclusion": "通过解耦静态几何和动态运动的建模，并采用双内存架构（TDM和PSM），Mem4D有效解决了动态场景重建中的内存需求困境，实现了静态结构的全局一致性和动态元素的高保真重建。"}}
{"id": "2508.07918", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07918", "abs": "https://arxiv.org/abs/2508.07918", "authors": ["Xing Zi", "Jinghao Xiao", "Yunxiao Shi", "Xian Tao", "Jun Li", "Ali Braytee", "Mukesh Prasad"], "title": "RSVLM-QA: A Benchmark Dataset for Remote Sensing Vision Language Model-based Question Answering", "comment": "This paper has been accepted to the proceedings of the 33rd ACM\n  International Multimedia Conference (ACM Multimedia 2025)", "summary": "Visual Question Answering (VQA) in remote sensing (RS) is pivotal for\ninterpreting Earth observation data. However, existing RS VQA datasets are\nconstrained by limitations in annotation richness, question diversity, and the\nassessment of specific reasoning capabilities. This paper introduces RSVLM-QA\ndataset, a new large-scale, content-rich VQA dataset for the RS domain.\nRSVLM-QA is constructed by integrating data from several prominent RS\nsegmentation and detection datasets: WHU, LoveDA, INRIA, and iSAID. We employ\nan innovative dual-track annotation generation pipeline. Firstly, we leverage\nLarge Language Models (LLMs), specifically GPT-4.1, with meticulously designed\nprompts to automatically generate a suite of detailed annotations including\nimage captions, spatial relations, and semantic tags, alongside complex\ncaption-based VQA pairs. Secondly, to address the challenging task of object\ncounting in RS imagery, we have developed a specialized automated process that\nextracts object counts directly from the original segmentation data; GPT-4.1\nthen formulates natural language answers from these counts, which are paired\nwith preset question templates to create counting QA pairs. RSVLM-QA comprises\n13,820 images and 162,373 VQA pairs, featuring extensive annotations and\ndiverse question types. We provide a detailed statistical analysis of the\ndataset and a comparison with existing RS VQA benchmarks, highlighting the\nsuperior depth and breadth of RSVLM-QA's annotations. Furthermore, we conduct\nbenchmark experiments on Six mainstream Vision Language Models (VLMs),\ndemonstrating that RSVLM-QA effectively evaluates and challenges the\nunderstanding and reasoning abilities of current VLMs in the RS domain. We\nbelieve RSVLM-QA will serve as a pivotal resource for the RS VQA and VLM\nresearch communities, poised to catalyze advancements in the field.", "AI": {"tldr": "该论文提出了RSVLM-QA，一个大规模、内容丰富的遥感视觉问答（VQA）数据集，通过利用大型语言模型和自动化计数流程，生成了多样化的问题和丰富的标注，旨在推动遥感VQA和视觉语言模型（VLM）领域的发展。", "motivation": "现有遥感VQA数据集在标注丰富性、问题多样性以及特定推理能力评估方面存在局限性。", "method": "RSVLM-QA数据集整合了WHU、LoveDA、INRIA和iSAID等多个遥感分割和检测数据集的数据。采用创新的双轨标注生成流程：1) 利用GPT-4.1等大型语言模型，通过精心设计的提示词，自动生成图像描述、空间关系、语义标签以及复杂的基于描述的VQA对。2) 开发专门的自动化流程从原始分割数据中提取物体计数，再由GPT-4.1将计数转化为自然语言答案并与预设问题模板配对，创建计数QA对。", "result": "RSVLM-QA包含13,820张图像和162,373个VQA对，具有广泛的标注和多样的问题类型。详细的统计分析表明其标注深度和广度优于现有遥感VQA基准。在六个主流视觉语言模型上的基准实验表明，RSVLM-QA能有效评估和挑战当前VLM在遥感领域的理解和推理能力。", "conclusion": "RSVLM-QA将成为遥感VQA和VLM研究领域的重要资源，有望促进该领域的进步。"}}
{"id": "2508.07923", "categories": ["cs.CV", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07923", "abs": "https://arxiv.org/abs/2508.07923", "authors": ["Jakub Binda", "Valentina Paneta", "Vasileios Eleftheriadis", "Hongkyou Chung", "Panagiotis Papadimitroulas", "Neo Christopher Chung"], "title": "Safeguarding Generative AI Applications in Preclinical Imaging through Hybrid Anomaly Detection", "comment": null, "summary": "Generative AI holds great potentials to automate and enhance data synthesis\nin nuclear medicine. However, the high-stakes nature of biomedical imaging\nnecessitates robust mechanisms to detect and manage unexpected or erroneous\nmodel behavior. We introduce development and implementation of a hybrid anomaly\ndetection framework to safeguard GenAI models in BIOEMTECH's eyes(TM) systems.\nTwo applications are demonstrated: Pose2Xray, which generates synthetic X-rays\nfrom photographic mouse images, and DosimetrEYE, which estimates 3D radiation\ndose maps from 2D SPECT/CT scans. In both cases, our outlier detection (OD)\nenhances reliability, reduces manual oversight, and supports real-time quality\ncontrol. This approach strengthens the industrial viability of GenAI in\npreclinical settings by increasing robustness, scalability, and regulatory\ncompliance.", "AI": {"tldr": "开发了一种混合异常检测框架，以确保核医学中生成式AI模型在生物医学成像应用中的可靠性和质量控制。", "motivation": "生成式AI在核医学数据合成方面潜力巨大，但生物医学成像的高风险性质要求模型行为具有鲁棒性，能够检测和管理异常或错误，以避免潜在危害。", "method": "开发并实现了一个混合异常检测框架，用于保护BIOEMTECH eyes(TM)系统中的生成式AI模型。该框架在两个应用中得到验证：Pose2Xray（从鼠标照片生成合成X射线）和DosimetrEYE（从2D SPECT/CT扫描估计3D辐射剂量图）。", "result": "在Pose2Xray和DosimetrEYE这两个应用中，所提出的异常检测（OD）方法显著提高了模型的可靠性，减少了人工监督的需求，并支持了实时质量控制。", "conclusion": "该方法通过提高生成式AI模型的鲁棒性、可扩展性和符合监管要求的能力，增强了其在临床前环境中的工业可行性。"}}
{"id": "2508.07925", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07925", "abs": "https://arxiv.org/abs/2508.07925", "authors": ["Jin-Seop Lee", "SungJoon Lee", "Jaehan Ahn", "YunSeok Choi", "Jee-Hyong Lee"], "title": "TAG: A Simple Yet Effective Temporal-Aware Approach for Zero-Shot Video Temporal Grounding", "comment": null, "summary": "Video Temporal Grounding (VTG) aims to extract relevant video segments based\non a given natural language query. Recently, zero-shot VTG methods have gained\nattention by leveraging pretrained vision-language models (VLMs) to localize\ntarget moments without additional training. However, existing approaches suffer\nfrom semantic fragmentation, where temporally continuous frames sharing the\nsame semantics are split across multiple segments. When segments are\nfragmented, it becomes difficult to predict an accurate target moment that\naligns with the text query. Also, they rely on skewed similarity distributions\nfor localization, making it difficult to select the optimal segment.\nFurthermore, they heavily depend on the use of LLMs which require expensive\ninferences. To address these limitations, we propose a \\textit{TAG}, a simple\nyet effective Temporal-Aware approach for zero-shot video temporal Grounding,\nwhich incorporates temporal pooling, temporal coherence clustering, and\nsimilarity adjustment. Our proposed method effectively captures the temporal\ncontext of videos and addresses distorted similarity distributions without\ntraining. Our approach achieves state-of-the-art results on Charades-STA and\nActivityNet Captions benchmark datasets without rely on LLMs. Our code is\navailable at https://github.com/Nuetee/TAG", "AI": {"tldr": "本文提出了一种名为TAG的零样本视频时间定位方法，通过引入时间池化、时间一致性聚类和相似度调整，解决了现有方法中语义碎片化和相似度分布偏差的问题，并在无需LLM的情况下实现了最先进的性能。", "motivation": "现有的零样本视频时间定位方法存在三个主要问题：1) 语义碎片化，导致连续帧被分割成多个片段；2) 相似度分布倾斜，难以选择最佳片段；3) 过度依赖大型语言模型（LLMs），导致推理成本高昂。", "method": "本文提出了TAG（Temporal-Aware approach for zero-shot video temporal Grounding）方法，该方法通过整合时间池化（temporal pooling）、时间一致性聚类（temporal coherence clustering）和相似度调整（similarity adjustment）来解决现有方法的局限性。这些组件旨在有效捕捉视频的时间上下文，并纠正扭曲的相似度分布，且无需额外训练。", "result": "TAG方法在Charades-STA和ActivityNet Captions基准数据集上取得了最先进的性能，并且在实现这些结果时没有依赖大型语言模型（LLMs）。", "conclusion": "TAG是一个简单而有效的时间感知方法，能够解决零样本视频时间定位中的语义碎片化和相似度分布问题，同时避免了对LLM的依赖，实现了优异的性能。"}}
{"id": "2508.07960", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07960", "abs": "https://arxiv.org/abs/2508.07960", "authors": ["Ajnas Muhammed", "Iurri Medvedev", "Nuno Gonçalves"], "title": "VOIDFace: A Privacy-Preserving Multi-Network Face Recognition With Enhanced Security", "comment": "Accepted at IEEE International Joint Conference on Biometrics (IJCB)\n  2025", "summary": "Advancement of machine learning techniques, combined with the availability of\nlarge-scale datasets, has significantly improved the accuracy and efficiency of\nfacial recognition. Modern facial recognition systems are trained using large\nface datasets collected from diverse individuals or public repositories.\nHowever, for training, these datasets are often replicated and stored in\nmultiple workstations, resulting in data replication, which complicates\ndatabase management and oversight. Currently, once a user submits their face\nfor dataset preparation, they lose control over how their data is used, raising\nsignificant privacy and ethical concerns. This paper introduces VOIDFace, a\nnovel framework for facial recognition systems that addresses two major issues.\nFirst, it eliminates the need of data replication and improves data control to\nsecurely store training face data by using visual secret sharing. Second, it\nproposes a patch-based multi-training network that uses this novel training\ndata storage mechanism to develop a robust, privacy-preserving facial\nrecognition system. By integrating these advancements, VOIDFace aims to improve\nthe privacy, security, and efficiency of facial recognition training, while\nensuring greater control over sensitive personal face data. VOIDFace also\nenables users to exercise their Right-To-Be-Forgotten property to control their\npersonal data. Experimental evaluations on the VGGFace2 dataset show that\nVOIDFace provides Right-To-Be-Forgotten, improved data control, security, and\nprivacy while maintaining competitive facial recognition performance. Code is\navailable at: https://github.com/ajnasmuhammed89/VOIDFace", "AI": {"tldr": "VOIDFace是一个新的面部识别框架，它利用视觉秘密共享技术解决数据复制问题，实现隐私保护和用户数据控制（包括被遗忘权），同时保持识别性能。", "motivation": "现代面部识别系统训练数据存在大量复制，导致管理复杂；用户提交数据后失去控制权，引发严重的隐私和伦理问题。", "method": "1. 引入视觉秘密共享技术，安全存储训练面部数据，消除数据复制并增强数据控制。 2. 提出一种基于补丁的多训练网络，结合新的数据存储机制，构建鲁棒、隐私保护的面部识别系统。该系统还支持用户行使其“被遗忘权”。", "result": "VOIDFace在VGGFace2数据集上的实验评估表明，它实现了被遗忘权、改进的数据控制、安全性和隐私保护，同时保持了有竞争力的面部识别性能。", "conclusion": "VOIDFace通过消除数据复制、增强数据控制和支持被遗忘权，显著提升了面部识别训练的隐私、安全性和效率。"}}
{"id": "2508.07968", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07968", "abs": "https://arxiv.org/abs/2508.07968", "authors": ["Tony Danjun Wang", "Christian Heiliger", "Nassir Navab", "Lennart Bastian"], "title": "TrackOR: Towards Personalized Intelligent Operating Rooms Through Robust Tracking", "comment": "Full Research Paper, presented at MICCAI'25 Workshop on Collaborative\n  Intelligence and Autonomy in Image-guided Surgery", "summary": "Providing intelligent support to surgical teams is a key frontier in\nautomated surgical scene understanding, with the long-term goal of improving\npatient outcomes. Developing personalized intelligence for all staff members\nrequires maintaining a consistent state of who is located where for long\nsurgical procedures, which still poses numerous computational challenges. We\npropose TrackOR, a framework for tackling long-term multi-person tracking and\nre-identification in the operating room. TrackOR uses 3D geometric signatures\nto achieve state-of-the-art online tracking performance (+11% Association\nAccuracy over the strongest baseline), while also enabling an effective offline\nrecovery process to create analysis-ready trajectories. Our work shows that by\nleveraging 3D geometric information, persistent identity tracking becomes\nattainable, enabling a critical shift towards the more granular, staff-centric\nanalyses required for personalized intelligent systems in the operating room.\nThis new capability opens up various applications, including our proposed\ntemporal pathway imprints that translate raw tracking data into actionable\ninsights for improving team efficiency and safety and ultimately providing\npersonalized support.", "AI": {"tldr": "该研究提出了TrackOR框架，利用3D几何特征实现手术室中长时间多人员跟踪和重识别，显著提升了跟踪精度，为个性化智能支持系统奠定了基础。", "motivation": "为手术团队提供智能支持，特别是实现个性化智能，需要精确且持续地了解手术过程中人员的位置信息，这在长时间手术中仍面临计算挑战。", "method": "提出TrackOR框架，用于手术室中的长期多人员跟踪和重识别。该方法利用3D几何特征实现最先进的在线跟踪性能，并支持有效的离线恢复过程以生成可分析的轨迹。", "result": "TrackOR在在线跟踪性能上取得了最先进的结果，相比最强基线，关联准确率提高了11%。它使得持久的身份跟踪成为可能，从而能够进行更细致、以人员为中心的分析。该能力还支持将原始跟踪数据转化为可操作的洞察，例如时间路径印记。", "conclusion": "通过利用3D几何信息，TrackOR实现了手术室中持久的人员身份跟踪，为开发个性化智能系统和更细致的、以人员为中心的分析铺平了道路，最终有助于提高团队效率、安全性和提供个性化支持。"}}
{"id": "2508.07989", "categories": ["cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.07989", "abs": "https://arxiv.org/abs/2508.07989", "authors": ["Xiantao Zhang"], "title": "The Escalator Problem: Identifying Implicit Motion Blindness in AI for Accessibility", "comment": "9 pages, 3 figures, 2 tables. Accepted at CV4A11y, ICCV 2025", "summary": "Multimodal Large Language Models (MLLMs) hold immense promise as assistive\ntechnologies for the blind and visually impaired (BVI) community. However, we\nidentify a critical failure mode that undermines their trustworthiness in\nreal-world applications. We introduce the Escalator Problem -- the inability of\nstate-of-the-art models to perceive an escalator's direction of travel -- as a\ncanonical example of a deeper limitation we term Implicit Motion Blindness.\nThis blindness stems from the dominant frame-sampling paradigm in video\nunderstanding, which, by treating videos as discrete sequences of static\nimages, fundamentally struggles to perceive continuous, low-signal motion. As a\nposition paper, our contribution is not a new model but rather to: (I) formally\narticulate this blind spot, (II) analyze its implications for user trust, and\n(III) issue a call to action. We advocate for a paradigm shift from purely\nsemantic recognition towards robust physical perception and urge the\ndevelopment of new, human-centered benchmarks that prioritize safety,\nreliability, and the genuine needs of users in dynamic environments.", "AI": {"tldr": "多模态大语言模型（MLLMs）在辅助视障人士时存在“隐式运动盲”问题，即无法感知连续、低信号的运动（如扶梯方向），这源于当前视频理解的帧采样范式。本文旨在阐明此盲点、分析其对用户信任的影响，并呼吁转向更注重物理感知的范式和开发以人为中心的新基准。", "motivation": "MLLMs对视障人士具有巨大潜力，但研究者发现一个关键的失败模式——模型无法感知连续、低信号的运动，这严重损害了其在实际应用中的可信度。因此，需要识别并解决这一问题。", "method": "本文作为一篇立场性论文，并未提出新模型。其方法是：(I) 正式阐明“隐式运动盲”这一盲点，以“扶梯问题”为例，指出其根源在于视频理解中主导的帧采样范式将视频视为离散图像序列，难以感知连续运动；(II) 分析此盲点对用户信任的影响；(III) 呼吁采取行动，倡导从纯语义识别转向鲁棒的物理感知，并开发以安全、可靠性和用户需求为优先的新型以人为中心的基准。", "result": "研究发现，当前最先进的MLLMs存在“隐式运动盲”问题，无法感知如扶梯方向等连续、低信号的运动，这被称为“扶梯问题”。这一深层局限性源于视频理解中普遍的帧采样范式，导致模型在动态环境中缺乏可信度和安全性。", "conclusion": "当前MLLMs的“隐式运动盲”严重影响了其在辅助视障人士方面的可信度。因此，迫切需要从纯粹的语义识别范式转向更注重鲁棒物理感知的范式，并开发新的、以人为中心且优先考虑安全、可靠性以及用户在动态环境中实际需求的评估基准。"}}
{"id": "2508.07996", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07996", "abs": "https://arxiv.org/abs/2508.07996", "authors": ["Thinesh Thiyakesan Ponbagavathi", "Chengzheng Yang", "Alina Roitberg"], "title": "Prompt-Guided Relational Reasoning for Social Behavior Understanding with Vision Foundation Models", "comment": null, "summary": "Group Activity Detection (GAD) involves recognizing social groups and their\ncollective behaviors in videos. Vision Foundation Models (VFMs), like DinoV2,\noffer excellent features, but are pretrained primarily on object-centric data\nand remain underexplored for modeling group dynamics. While they are a\npromising alternative to highly task-specific GAD architectures that require\nfull fine-tuning, our initial investigation reveals that simply swapping CNN\nbackbones used in these methods with VFMs brings little gain, underscoring the\nneed for structured, group-aware reasoning on top.\n  We introduce Prompt-driven Group Activity Detection (ProGraD) -- a method\nthat bridges this gap through 1) learnable group prompts to guide the VFM\nattention toward social configurations, and 2) a lightweight two-layer\nGroupContext Transformer that infers actor-group associations and collective\nbehavior. We evaluate our approach on two recent GAD benchmarks: Cafe, which\nfeatures multiple concurrent social groups, and Social-CAD, which focuses on\nsingle-group interactions. While we surpass state-of-the-art in both settings,\nour method is especially effective in complex multi-group scenarios, where we\nyield a gain of 6.5\\% (Group mAP\\@1.0) and 8.2\\% (Group mAP\\@0.5) using only\n10M trainable parameters. Furthermore, our experiments reveal that ProGraD\nproduces interpretable attention maps, offering insights into actor-group\nreasoning. Code and models will be released.", "AI": {"tldr": "该论文提出了ProGraD（Prompt-driven Group Activity Detection）方法，通过引入可学习的群组提示和轻量级Transformer，有效利用视觉基础模型（VFMs）进行群组活动检测，尤其在复杂多群组场景中表现出色并达到SOTA。", "motivation": "尽管视觉基础模型（VFMs）提供了出色的特征，但它们主要在以物体为中心的数据上进行预训练，并且在建模群组动态方面探索不足。简单地将VFMs替换为现有群组活动检测（GAD）方法中的CNN骨干网络效果不佳，表明需要在此基础上进行结构化、群组感知的推理。此外，现有GAD架构通常需要完全微调。", "method": "本文提出了ProGraD方法，通过以下两点弥补了现有差距：1) 可学习的群组提示，引导VFM的注意力聚焦于社交配置；2) 一个轻量级的两层GroupContext Transformer，用于推断演员-群组关联和集体行为。", "result": "ProGraD在两个近期GAD基准测试（Cafe和Social-CAD）中均超越了现有SOTA。尤其在复杂多群组场景中（Cafe），使用仅10M可训练参数，Group mAP@1.0提升了6.5%，Group mAP@0.5提升了8.2%。此外，实验表明ProGraD能生成可解释的注意力图，为演员-群组推理提供洞察。", "conclusion": "ProGraD通过引入群组感知推理机制，成功地将视觉基础模型应用于群组活动检测任务，特别是在处理复杂多群组交互方面表现出显著优势，并提供了可解释的结果。这为利用大规模预训练模型进行复杂社交行为理解提供了有效途径。"}}
{"id": "2508.08004", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08004", "abs": "https://arxiv.org/abs/2508.08004", "authors": ["Anqi Xiao", "Weichen Yu", "Hongyuan Yu"], "title": "Sample-aware RandAugment: Search-free Automatic Data Augmentation for Effective Image Recognition", "comment": "International Journal of Computer Vision, 2025", "summary": "Automatic data augmentation (AutoDA) plays an important role in enhancing the\ngeneralization of neural networks. However, mainstream AutoDA methods often\nencounter two challenges: either the search process is excessively\ntime-consuming, hindering practical application, or the performance is\nsuboptimal due to insufficient policy adaptation during training. To address\nthese issues, we propose Sample-aware RandAugment (SRA), an asymmetric,\nsearch-free AutoDA method that dynamically adjusts augmentation policies while\nmaintaining straightforward implementation. SRA incorporates a heuristic\nscoring module that evaluates the complexity of the original training data,\nenabling the application of tailored augmentations for each sample.\nAdditionally, an asymmetric augmentation strategy is employed to maximize the\npotential of this scoring module. In multiple experimental settings, SRA\nnarrows the performance gap between search-based and search-free AutoDA\nmethods, achieving a state-of-the-art Top-1 accuracy of 78.31\\% on ImageNet\nwith ResNet-50. Notably, SRA demonstrates good compatibility with existing\naugmentation pipelines and solid generalization across new tasks, without\nrequiring hyperparameter tuning. The pretrained models leveraging SRA also\nenhance recognition in downstream object detection tasks. SRA represents a\npromising step towards simpler, more effective, and practical AutoDA designs\napplicable to a variety of future tasks. Our code is available at\n\\href{https://github.com/ainieli/Sample-awareRandAugment}{https://github.com/ainieli/Sample-awareRandAugment", "AI": {"tldr": "本文提出了一种名为SRA（Sample-aware RandAugment）的非对称、免搜索自动数据增强（AutoDA）方法，它能动态调整增强策略，有效解决现有AutoDA方法耗时或性能不佳的问题，并在多项任务上表现出色。", "motivation": "主流自动数据增强（AutoDA）方法存在两大挑战：一是搜索过程过于耗时，阻碍实际应用；二是由于训练期间策略适应性不足，导致性能次优。", "method": "SRA是一种非对称、免搜索的AutoDA方法。它包含一个启发式评分模块，用于评估原始训练数据的复杂性，从而为每个样本应用量身定制的增强。此外，采用非对称增强策略以最大化评分模块的潜力。", "result": "SRA显著缩小了基于搜索和免搜索AutoDA方法之间的性能差距。在ImageNet数据集上使用ResNet-50模型，SRA取得了78.31%的最新Top-1准确率。SRA与现有增强流程兼容性良好，在新任务上展现出强大的泛化能力，且无需超参数调优。利用SRA预训练的模型还能增强下游目标检测任务的识别能力。", "conclusion": "SRA代表了向更简单、更有效、更实用的AutoDA设计迈出的重要一步，适用于各种未来任务。"}}
{"id": "2508.08028", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08028", "abs": "https://arxiv.org/abs/2508.08028", "authors": ["Tony Danjun Wang", "Tobias Czempiel", "Nassir Navab", "Lennart Bastian"], "title": "Mitigating Biases in Surgical Operating Rooms with Geometry", "comment": "Extended Abstract, presented at the MICCAI'25 workshop on\n  Collaborative Intelligence and Autonomy in Image-guided Surgery", "summary": "Deep neural networks are prone to learning spurious correlations, exploiting\ndataset-specific artifacts rather than meaningful features for prediction. In\nsurgical operating rooms (OR), these manifest through the standardization of\nsmocks and gowns that obscure robust identifying landmarks, introducing model\nbias for tasks related to modeling OR personnel. Through gradient-based\nsaliency analysis on two public OR datasets, we reveal that CNN models succumb\nto such shortcuts, fixating on incidental visual cues such as footwear beneath\nsurgical gowns, distinctive eyewear, or other role-specific identifiers.\nAvoiding such biases is essential for the next generation of intelligent\nassistance systems in the OR, which should accurately recognize personalized\nworkflow traits, such as surgical skill level or coordination with other staff\nmembers. We address this problem by encoding personnel as 3D point cloud\nsequences, disentangling identity-relevant shape and motion patterns from\nappearance-based confounders. Our experiments demonstrate that while RGB and\ngeometric methods achieve comparable performance on datasets with apparent\nsimulation artifacts, RGB models suffer a 12% accuracy drop in realistic\nclinical settings with decreased visual diversity due to standardizations. This\nperformance gap confirms that geometric representations capture more meaningful\nbiometric features, providing an avenue to developing robust methods of\nmodeling humans in the OR.", "AI": {"tldr": "深度学习模型在手术室环境中易受服装标准化影响，学习到虚假关联（如鞋子、眼镜）。本文通过梯度显著性分析揭示此偏见，并提出使用3D点云序列捕捉几何特征，以实现更鲁棒的手术室人员建模，避免外观混淆因素。", "motivation": "深度神经网络在手术室（OR）环境中倾向于学习虚假关联，例如利用标准化服装（如手术服）下方的鞋子或眼镜等偶然视觉线索进行预测，而非有意义的特征。这种模型偏见阻碍了下一代智能辅助系统准确识别个性化工作流程特征（如手术技能水平或与他人的协作）的能力。", "method": "首先，通过对两个公共OR数据集进行基于梯度的显著性分析，揭示了CNN模型对偶然视觉线索的依赖。为解决此问题，研究提出将手术室人员编码为3D点云序列，旨在将与身份相关的形状和运动模式与基于外观的混淆因素分离。", "result": "实验表明，在具有明显模拟伪影的数据集上，RGB和几何方法性能相当；但在视觉多样性因标准化而降低的真实临床环境中，RGB模型准确率下降了12%，而几何方法保持了性能。这一性能差距证实了几何表示能够捕获更有意义的生物识别特征。", "conclusion": "几何表示为开发鲁棒的手术室人员建模方法提供了途径，能够有效避免因外观标准化导致的模型偏见，捕获更具意义的生物识别特征，从而支持未来智能辅助系统的发展。"}}
{"id": "2508.08038", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08038", "abs": "https://arxiv.org/abs/2508.08038", "authors": ["Huawei Sun", "Zixu Wang", "Hao Feng", "Julius Ott", "Lorenzo Servadei", "Robert Wille"], "title": "TRIDE: A Text-assisted Radar-Image weather-aware fusion network for Depth Estimation", "comment": "Accepted by TMLR (2025.08)", "summary": "Depth estimation, essential for autonomous driving, seeks to interpret the 3D\nenvironment surrounding vehicles. The development of radar sensors, known for\ntheir cost-efficiency and robustness, has spurred interest in radar-camera\nfusion-based solutions. However, existing algorithms fuse features from these\nmodalities without accounting for weather conditions, despite radars being\nknown to be more robust than cameras under adverse weather. Additionally, while\nVision-Language models have seen rapid advancement, utilizing language\ndescriptions alongside other modalities for depth estimation remains an open\nchallenge. This paper first introduces a text-generation strategy along with\nfeature extraction and fusion techniques that can assist monocular depth\nestimation pipelines, leading to improved accuracy across different algorithms\non the KITTI dataset. Building on this, we propose TRIDE, a radar-camera fusion\nalgorithm that enhances text feature extraction by incorporating radar point\ninformation. To address the impact of weather on sensor performance, we\nintroduce a weather-aware fusion block that adaptively adjusts radar weighting\nbased on current weather conditions. Our method, benchmarked on the nuScenes\ndataset, demonstrates performance gains over the state-of-the-art, achieving a\n12.87% improvement in MAE and a 9.08% improvement in RMSE. Code:\nhttps://github.com/harborsarah/TRIDE", "AI": {"tldr": "该论文提出了一种新的深度估计算法TRIDE，它结合了雷达、摄像头和语言信息，并引入了天气感知融合机制，以提高在不同天气条件下的深度估计精度。", "motivation": "现有的雷达-摄像头融合深度估计算法未考虑天气条件对传感器性能的影响，尽管雷达在恶劣天气下表现更鲁棒。此外，视觉-语言模型虽有进展，但将语言描述用于深度估计仍是一个未解决的挑战。", "method": "首先，引入了一种文本生成策略以及特征提取和融合技术，以辅助单目深度估计。在此基础上，提出了TRIDE算法，通过结合雷达点信息增强文本特征提取。为了解决天气对传感器性能的影响，TRIDE还引入了一个天气感知融合模块，根据当前天气条件自适应调整雷达权重。", "result": "在KITTI数据集上，文本辅助方法提高了不同算法的精度。在nuScenes数据集上，TRIDE算法的性能超越了现有最先进的方法，MAE（平均绝对误差）提高了12.87%，RMSE（均方根误差）提高了9.08%。", "conclusion": "该研究成功地将语言描述、雷达和摄像头数据融合到深度估计任务中，并首次引入了天气感知融合策略，显著提升了在复杂环境（特别是不同天气）下的深度估计精度和鲁棒性。"}}
{"id": "2508.08048", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08048", "abs": "https://arxiv.org/abs/2508.08048", "authors": ["Peng Dai", "Feitong Tan", "Qiangeng Xu", "Yihua Huang", "David Futschik", "Ruofei Du", "Sean Fanello", "Yinda Zhang", "Xiaojuan Qi"], "title": "S^2VG: 3D Stereoscopic and Spatial Video Generation via Denoising Frame Matrix", "comment": "immsersive video generation", "summary": "While video generation models excel at producing high-quality monocular\nvideos, generating 3D stereoscopic and spatial videos for immersive\napplications remains an underexplored challenge. We present a pose-free and\ntraining-free method that leverages an off-the-shelf monocular video generation\nmodel to produce immersive 3D videos. Our approach first warps the generated\nmonocular video into pre-defined camera viewpoints using estimated depth\ninformation, then applies a novel \\textit{frame matrix} inpainting framework.\nThis framework utilizes the original video generation model to synthesize\nmissing content across different viewpoints and timestamps, ensuring spatial\nand temporal consistency without requiring additional model fine-tuning.\nMoreover, we develop a \\dualupdate~scheme that further improves the quality of\nvideo inpainting by alleviating the negative effects propagated from\ndisoccluded areas in the latent space. The resulting multi-view videos are then\nadapted into stereoscopic pairs or optimized into 4D Gaussians for spatial\nvideo synthesis. We validate the efficacy of our proposed method by conducting\nexperiments on videos from various generative models, such as Sora, Lumiere,\nWALT, and Zeroscope. The experiments demonstrate that our method has a\nsignificant improvement over previous methods. Project page at:\nhttps://daipengwa.github.io/S-2VG_ProjectPage/", "AI": {"tldr": "该论文提出一种无需训练且姿态无关的方法，利用现成的单目视频生成模型，通过深度估计和新颖的帧矩阵修复框架，将单目视频转换为沉浸式3D立体和空间视频。", "motivation": "现有视频生成模型在生成高质量单目视频方面表现出色，但为沉浸式应用生成3D立体和空间视频仍是一个未充分探索的挑战。", "method": "该方法首先利用估计的深度信息将生成的单目视频扭曲到预定义的摄像机视角；然后应用一种新颖的“帧矩阵”修复框架，利用原始视频生成模型合成不同视角和时间戳的缺失内容，确保空间和时间一致性，无需额外模型微调；此外，开发了一种“双重更新”方案，通过缓解潜在空间中遮挡区域的负面影响，进一步提高视频修复质量；最后将生成的多视角视频转换为立体对或优化为4D高斯用于空间视频合成。", "result": "在Sora、Lumiere、WALT和Zeroscope等多种生成模型上进行的实验验证了该方法的有效性，并表明其比以前的方法有显著改进。", "conclusion": "该论文提出了一种有效且通用的方法，能够将现有单目视频生成模型的输出转换为高质量的3D立体和空间视频，显著提升了沉浸式视频内容的生成能力。"}}
{"id": "2508.08058", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.08058", "abs": "https://arxiv.org/abs/2508.08058", "authors": ["Ziad Al-Haj Hemidi", "Eytan Kats", "Mattias P. Heinrich"], "title": "PrIINeR: Towards Prior-Informed Implicit Neural Representations for Accelerated MRI", "comment": "Submitted to the British Machine Vision Conference (BMVC) 2025\n  (Before peer review version)", "summary": "Accelerating Magnetic Resonance Imaging (MRI) reduces scan time but often\ndegrades image quality. While Implicit Neural Representations (INRs) show\npromise for MRI reconstruction, they struggle at high acceleration factors due\nto weak prior constraints, leading to structural loss and aliasing artefacts.\nTo address this, we propose PrIINeR, an INR-based MRI reconstruction method\nthat integrates prior knowledge from pre-trained deep learning models into the\nINR framework. By combining population-level knowledge with instance-based\noptimization and enforcing dual data consistency, PrIINeR aligns both with the\nacquired k-space data and the prior-informed reconstruction. Evaluated on the\nNYU fastMRI dataset, our method not only outperforms state-of-the-art INR-based\napproaches but also improves upon several learning-based state-of-the-art\nmethods, significantly improving structural preservation and fidelity while\neffectively removing aliasing artefacts.PrIINeR bridges deep learning and\nINR-based techniques, offering a more reliable solution for high-quality,\naccelerated MRI reconstruction. The code is publicly available on\nhttps://github.com/multimodallearning/PrIINeR.", "AI": {"tldr": "PrIINeR是一种基于INR的MRI重建方法，通过整合预训练深度学习模型的先验知识，解决了高加速因子下INR重建图像质量下降的问题，显著提高了结构保留和伪影去除能力。", "motivation": "加速MRI扫描时间会降低图像质量。现有隐式神经表示（INR）方法在MRI重建中表现出潜力，但由于缺乏强先验约束，在高加速因子下表现不佳，导致结构丢失和伪影。", "method": "PrIINeR将预训练深度学习模型的先验知识整合到INR框架中。它结合了群体层面知识与实例级优化，并强制执行双重数据一致性，以同时与采集的k空间数据和先验信息对齐。", "result": "在NYU fastMRI数据集上，PrIINeR不仅优于最先进的基于INR的方法，还超越了多个基于学习的先进方法，显著改善了结构保留和图像保真度，并有效消除了混叠伪影。", "conclusion": "PrIINeR弥合了深度学习和基于INR的技术之间的鸿沟，为高质量、加速MRI重建提供了一个更可靠的解决方案。"}}
{"id": "2508.08069", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08069", "abs": "https://arxiv.org/abs/2508.08069", "authors": ["Xiaoxiao Cui", "Yiran Li", "Kai He", "Shanzhi Jiang", "Mengli Xue", "Wentao Li", "Junhong Leng", "Zhi Liu", "Lizhen Cui", "Shuo Li"], "title": "Information Bottleneck-based Causal Attention for Multi-label Medical Image Recognition", "comment": "Early accepted by MICCAI 2025", "summary": "Multi-label classification (MLC) of medical images aims to identify multiple\ndiseases and holds significant clinical potential. A critical step is to learn\nclass-specific features for accurate diagnosis and improved interpretability\neffectively. However, current works focus primarily on causal attention to\nlearn class-specific features, yet they struggle to interpret the true cause\ndue to the inadvertent attention to class-irrelevant features. To address this\nchallenge, we propose a new structural causal model (SCM) that treats\nclass-specific attention as a mixture of causal, spurious, and noisy factors,\nand a novel Information Bottleneck-based Causal Attention (IBCA) that is\ncapable of learning the discriminative class-specific attention for MLC of\nmedical images. Specifically, we propose learning Gaussian mixture multi-label\nspatial attention to filter out class-irrelevant information and capture each\nclass-specific attention pattern. Then a contrastive enhancement-based causal\nintervention is proposed to gradually mitigate the spurious attention and\nreduce noise information by aligning multi-head attention with the Gaussian\nmixture multi-label spatial. Quantitative and ablation results on Endo and\nMuReD show that IBCA outperforms all methods. Compared to the second-best\nresults for each metric, IBCA achieves improvements of 6.35\\% in CR, 7.72\\% in\nOR, and 5.02\\% in mAP for MuReD, 1.47\\% in CR, and 1.65\\% in CF1, and 1.42\\% in\nmAP for Endo.", "AI": {"tldr": "该研究提出了一种新的基于信息瓶颈的因果注意力（IBCA）模型，用于医学图像的多标签分类。IBCA通过结构化因果模型处理注意力中的因果、虚假和噪声因素，并利用高斯混合多标签空间注意力及对比增强的因果干预来学习判别性的类别特定特征，显著优于现有方法。", "motivation": "当前医学图像多标签分类中的因果注意力方法在学习类别特定特征时，常因不经意地关注与类别无关的特征而难以解释真正的病因，导致诊断准确性和可解释性受限。", "method": "提出了一种新的结构化因果模型（SCM），将类别特定注意力视为因果、虚假和噪声因素的混合。开发了基于信息瓶颈的因果注意力（IBCA）模型，具体包括：1) 学习高斯混合多标签空间注意力以过滤无关信息并捕获类别特定模式；2) 提出基于对比增强的因果干预，通过将多头注意力与高斯混合多标签空间对齐，逐步减轻虚假注意力和噪声信息。", "result": "在Endo和MuReD数据集上的定量和消融实验结果表明，IBCA优于所有现有方法。与次优结果相比，IBCA在MuReD数据集上，CR提高了6.35%，OR提高了7.72%，mAP提高了5.02%；在Endo数据集上，CR提高了1.47%，CF1提高了1.65%，mAP提高了1.42%。", "conclusion": "IBCA通过有效学习判别性的类别特定注意力，成功解决了现有方法中对类别无关特征的关注问题，显著提升了医学图像多标签分类的准确性和潜在可解释性。"}}
{"id": "2508.08082", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08082", "abs": "https://arxiv.org/abs/2508.08082", "authors": ["Zizheng Guo", "Bochao Zou", "Junbao Zhuo", "Huimin Ma"], "title": "ME-TST+: Micro-expression Analysis via Temporal State Transition with ROI Relationship Awareness", "comment": null, "summary": "Micro-expressions (MEs) are regarded as important indicators of an\nindividual's intrinsic emotions, preferences, and tendencies. ME analysis\nrequires spotting of ME intervals within long video sequences and recognition\nof their corresponding emotional categories. Previous deep learning approaches\ncommonly employ sliding-window classification networks. However, the use of\nfixed window lengths and hard classification presents notable limitations in\npractice. Furthermore, these methods typically treat ME spotting and\nrecognition as two separate tasks, overlooking the essential relationship\nbetween them. To address these challenges, this paper proposes two state space\nmodel-based architectures, namely ME-TST and ME-TST+, which utilize temporal\nstate transition mechanisms to replace conventional window-level classification\nwith video-level regression. This enables a more precise characterization of\nthe temporal dynamics of MEs and supports the modeling of MEs with varying\ndurations. In ME-TST+, we further introduce multi-granularity ROI modeling and\nthe slowfast Mamba framework to alleviate information loss associated with\ntreating ME analysis as a time-series task. Additionally, we propose a synergy\nstrategy for spotting and recognition at both the feature and result levels,\nleveraging their intrinsic connection to enhance overall analysis performance.\nExtensive experiments demonstrate that the proposed methods achieve\nstate-of-the-art performance. The codes are available at\nhttps://github.com/zizheng-guo/ME-TST.", "AI": {"tldr": "本文提出ME-TST和ME-TST+两种基于状态空间模型的新架构，通过视频级回归和协同策略，解决了微表情（ME）检测中固定窗口、硬分类及分割与识别任务分离的问题，并实现了最先进的性能。", "motivation": "现有深度学习方法在微表情分析中存在局限性：1) 常用滑动窗口分类网络，但固定窗口长度和硬分类在实践中受限；2) 通常将ME检测和识别视为两个独立任务，忽略了它们之间的内在联系。", "method": "本文提出两种基于状态空间模型的架构：ME-TST和ME-TST+。它们利用时间状态转换机制，用视频级回归取代传统的窗口级分类，以更精确地表征ME的时间动态并支持不同持续时间的ME建模。ME-TST+进一步引入多粒度ROI建模和SlowFast Mamba框架，以缓解信息丢失。此外，提出了一种在特征和结果层面进行检测与识别协同的策略，以利用其内在联系提升整体分析性能。", "result": "广泛的实验表明，所提出的方法实现了最先进的性能。", "conclusion": "基于状态空间模型的方法，结合视频级回归、多粒度ROI建模、SlowFast Mamba框架以及检测与识别的协同策略，能有效解决微表情分析中的现有挑战，并显著提升分析性能。"}}
{"id": "2508.08086", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2508.08086", "abs": "https://arxiv.org/abs/2508.08086", "authors": ["Zhongqi Yang", "Wenhang Ge", "Yuqi Li", "Jiaqi Chen", "Haoyuan Li", "Mengyin An", "Fei Kang", "Hua Xue", "Baixin Xu", "Yuyang Yin", "Eric Li", "Yang Liu", "Yikai Wang", "Hao-Xiang Guo", "Yahui Zhou"], "title": "Matrix-3D: Omnidirectional Explorable 3D World Generation", "comment": "Technical Report", "summary": "Explorable 3D world generation from a single image or text prompt forms a\ncornerstone of spatial intelligence. Recent works utilize video model to\nachieve wide-scope and generalizable 3D world generation. However, existing\napproaches often suffer from a limited scope in the generated scenes. In this\nwork, we propose Matrix-3D, a framework that utilize panoramic representation\nfor wide-coverage omnidirectional explorable 3D world generation that combines\nconditional video generation and panoramic 3D reconstruction. We first train a\ntrajectory-guided panoramic video diffusion model that employs scene mesh\nrenders as condition, to enable high-quality and geometrically consistent scene\nvideo generation. To lift the panorama scene video to 3D world, we propose two\nseparate methods: (1) a feed-forward large panorama reconstruction model for\nrapid 3D scene reconstruction and (2) an optimization-based pipeline for\naccurate and detailed 3D scene reconstruction. To facilitate effective\ntraining, we also introduce the Matrix-Pano dataset, the first large-scale\nsynthetic collection comprising 116K high-quality static panoramic video\nsequences with depth and trajectory annotations. Extensive experiments\ndemonstrate that our proposed framework achieves state-of-the-art performance\nin panoramic video generation and 3D world generation. See more in\nhttps://matrix-3d.github.io.", "AI": {"tldr": "Matrix-3D提出了一种利用全景表示生成广阔、可探索3D世界的新框架，结合了条件视频生成和全景3D重建，并引入了大型全景视频数据集。", "motivation": "现有的从单张图像或文本提示生成可探索3D世界的方法，其生成场景的范围有限，无法实现大范围的3D世界生成。", "method": "该研究提出了Matrix-3D框架，该框架利用全景表示实现广覆盖、全向可探索的3D世界生成。具体方法包括：1) 训练一个轨迹引导的全景视频扩散模型，以场景网格渲染作为条件，生成高质量且几何一致的场景视频；2) 提出两种将全景场景视频提升为3D世界的方法：一个用于快速3D场景重建的前馈大型全景重建模型，以及一个用于精确详细3D场景重建的基于优化的管道；3) 构建并引入了Matrix-Pano数据集，这是一个包含11.6万高质量静态全景视频序列及深度和轨迹标注的大规模合成数据集。", "result": "广泛的实验证明，Matrix-3D框架在全景视频生成和3D世界生成方面均达到了最先进的性能。", "conclusion": "Matrix-3D通过结合全景视频扩散模型和创新的3D重建方法，并辅以大规模数据集，有效解决了现有方法在3D世界生成中场景范围有限的问题，实现了广覆盖、高质量且可探索的3D世界生成。"}}
{"id": "2508.08093", "categories": ["cs.CV", "cs.LG", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.08093", "abs": "https://arxiv.org/abs/2508.08093", "authors": ["Md Rezwanul Haque", "Md. Milon Islam", "S M Taslim Uddin Raju", "Hamdi Altaheri", "Lobna Nassar", "Fakhri Karray"], "title": "MDD-Net: Multimodal Depression Detection through Mutual Transformer", "comment": "Accepted for the 2025 IEEE International Conference on Systems, Man,\n  and Cybernetics (SMC), Vienna, Austria", "summary": "Depression is a major mental health condition that severely impacts the\nemotional and physical well-being of individuals. The simple nature of data\ncollection from social media platforms has attracted significant interest in\nproperly utilizing this information for mental health research. A Multimodal\nDepression Detection Network (MDD-Net), utilizing acoustic and visual data\nobtained from social media networks, is proposed in this work where mutual\ntransformers are exploited to efficiently extract and fuse multimodal features\nfor efficient depression detection. The MDD-Net consists of four core modules:\nan acoustic feature extraction module for retrieving relevant acoustic\nattributes, a visual feature extraction module for extracting significant\nhigh-level patterns, a mutual transformer for computing the correlations among\nthe generated features and fusing these features from multiple modalities, and\na detection layer for detecting depression using the fused feature\nrepresentations. The extensive experiments are performed using the multimodal\nD-Vlog dataset, and the findings reveal that the developed multimodal\ndepression detection network surpasses the state-of-the-art by up to 17.37% for\nF1-Score, demonstrating the greater performance of the proposed system. The\nsource code is accessible at\nhttps://github.com/rezwanh001/Multimodal-Depression-Detection.", "AI": {"tldr": "本文提出了一种多模态抑郁症检测网络（MDD-Net），利用社交媒体的声学和视觉数据，并通过互变压器高效提取和融合多模态特征，实现了显著优于现有技术的检测性能。", "motivation": "抑郁症严重影响个体身心健康，而社交媒体数据易于收集，为精神健康研究提供了有价值的信息源，激发了利用这些数据进行抑郁症检测的研究兴趣。", "method": "本文提出了MDD-Net，包含四个核心模块：声学特征提取模块、视觉特征提取模块、用于计算特征相关性并融合多模态特征的互变压器，以及基于融合特征进行抑郁症检测的检测层。", "result": "在多模态D-Vlog数据集上进行的大量实验表明，所开发的MDD-Net在F1-Score上超越了现有最先进技术达17.37%，证明了该系统卓越的性能。", "conclusion": "所提出的多模态抑郁症检测网络（MDD-Net）在利用社交媒体声学和视觉数据进行抑郁症检测方面表现出优越的性能，显著超越了现有技术。"}}
{"id": "2508.08094", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08094", "abs": "https://arxiv.org/abs/2508.08094", "authors": ["Jiakai Lin", "Jinchang Zhang", "Ge Jin", "Wenzhan Song", "Tianming Liu", "Guoyu Lu"], "title": "3D Plant Root Skeleton Detection and Extraction", "comment": null, "summary": "Plant roots typically exhibit a highly complex and dense architecture,\nincorporating numerous slender lateral roots and branches, which significantly\nhinders the precise capture and modeling of the entire root system.\nAdditionally, roots often lack sufficient texture and color information, making\nit difficult to identify and track root traits using visual methods. Previous\nresearch on roots has been largely confined to 2D studies; however, exploring\nthe 3D architecture of roots is crucial in botany. Since roots grow in real 3D\nspace, 3D phenotypic information is more critical for studying genetic traits\nand their impact on root development. We have introduced a 3D root skeleton\nextraction method that efficiently derives the 3D architecture of plant roots\nfrom a few images. This method includes the detection and matching of lateral\nroots, triangulation to extract the skeletal structure of lateral roots, and\nthe integration of lateral and primary roots. We developed a highly complex\nroot dataset and tested our method on it. The extracted 3D root skeletons\nshowed considerable similarity to the ground truth, validating the\neffectiveness of the model. This method can play a significant role in\nautomated breeding robots. Through precise 3D root structure analysis, breeding\nrobots can better identify plant phenotypic traits, especially root structure\nand growth patterns, helping practitioners select seeds with superior root\nsystems. This automated approach not only improves breeding efficiency but also\nreduces manual intervention, making the breeding process more intelligent and\nefficient, thus advancing modern agriculture.", "AI": {"tldr": "该研究提出了一种从少量图像中高效提取植物根系3D骨架的方法，解决了传统2D研究的局限性，并验证了其准确性，可应用于自动化育种机器人。", "motivation": "植物根系结构复杂且密集，缺乏纹理和颜色信息，难以通过视觉方法精确捕捉和建模。现有研究多限于2D，但3D根系信息对研究遗传性状和根系发育至关重要，因此需要一种能高效获取根系3D架构的方法。", "method": "该方法通过少量图像提取植物根系的3D骨架，具体包括侧根的检测与匹配、利用三角测量提取侧根骨架结构，以及整合侧根与主根。", "result": "研究开发了一个复杂的根系数据集，并在此数据集上测试了所提出的方法。结果显示，提取的3D根系骨架与真实情况高度相似，验证了模型的有效性。", "conclusion": "该方法在自动化育种机器人中具有重要应用前景，能通过精确的3D根系结构分析，帮助育种机器人识别植物表型性状，从而提高育种效率，减少人工干预，推动现代农业发展。"}}
{"id": "2508.08098", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08098", "abs": "https://arxiv.org/abs/2508.08098", "authors": ["Junzhe Xu", "Yuyang Yin", "Xi Chen"], "title": "TBAC-UniImage: Unified Understanding and Generation by Ladder-Side Diffusion Tuning", "comment": null, "summary": "This paper introduces TBAC-UniImage, a novel unified model for multimodal\nunderstanding and generation. We achieve this by deeply integrating a\npre-trained Diffusion Model, acting as a generative ladder, with a Multimodal\nLarge Language Model (MLLM). Previous diffusion-based unified models face two\nprimary limitations. One approach uses only the MLLM's final hidden state as\nthe generative condition. This creates a shallow connection, as the generator\nis isolated from the rich, hierarchical representations within the MLLM's\nintermediate layers. The other approach, pretraining a unified generative\narchitecture from scratch, is computationally expensive and prohibitive for\nmany researchers. To overcome these issues, our work explores a new paradigm.\nInstead of relying on a single output, we use representations from multiple,\ndiverse layers of the MLLM as generative conditions for the diffusion model.\nThis method treats the pre-trained generator as a ladder, receiving guidance\nfrom various depths of the MLLM's understanding process. Consequently,\nTBAC-UniImage achieves a much deeper and more fine-grained unification of\nunderstanding and generation.", "AI": {"tldr": "TBAC-UniImage是一个新型统一模型，通过将预训练扩散模型与多模态大语言模型深度融合，并利用MLLM多个中间层的表示作为生成条件，实现了多模态理解与生成的深度统一。", "motivation": "现有基于扩散的统一模型存在两个主要限制：一是仅使用MLLM最终隐藏状态作为生成条件，导致生成器与MLLM丰富的层次化表示隔离，连接肤浅；二是重新训练统一生成架构成本过高。", "method": "将预训练扩散模型作为生成阶梯，与多模态大语言模型（MLLM）深度整合。核心方法是利用MLLM中多个不同层次的表示作为扩散模型的生成条件，而非仅仅依赖单一输出。", "result": "TBAC-UniImage实现了理解和生成之间更深层次、更细粒度的统一。", "conclusion": "通过利用MLLM多层表示进行条件生成，本文提出的新范式有效克服了以往统一模型连接肤浅和训练成本高昂的问题，达到了更深度的理解与生成整合。"}}
{"id": "2508.08123", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08123", "abs": "https://arxiv.org/abs/2508.08123", "authors": ["Lingjing Chen", "Chengxiu Zhang", "Yinqiao Yi", "Yida Wang", "Yang Song", "Xu Yan", "Shengfang Xu", "Dalin Zhu", "Mengqiu Cao", "Yan Zhou", "Chenglong Wang", "Guang Yang"], "title": "A Physics-Driven Neural Network with Parameter Embedding for Generating Quantitative MR Maps from Weighted Images", "comment": null, "summary": "We propose a deep learning-based approach that integrates MRI sequence\nparameters to improve the accuracy and generalizability of quantitative image\nsynthesis from clinical weighted MRI. Our physics-driven neural network embeds\nMRI sequence parameters -- repetition time (TR), echo time (TE), and inversion\ntime (TI) -- directly into the model via parameter embedding, enabling the\nnetwork to learn the underlying physical principles of MRI signal formation.\nThe model takes conventional T1-weighted, T2-weighted, and T2-FLAIR images as\ninput and synthesizes T1, T2, and proton density (PD) quantitative maps.\nTrained on healthy brain MR images, it was evaluated on both internal and\nexternal test datasets. The proposed method achieved high performance with PSNR\nvalues exceeding 34 dB and SSIM values above 0.92 for all synthesized parameter\nmaps. It outperformed conventional deep learning models in accuracy and\nrobustness, including data with previously unseen brain structures and lesions.\nNotably, our model accurately synthesized quantitative maps for these unseen\npathological regions, highlighting its superior generalization capability.\nIncorporating MRI sequence parameters via parameter embedding allows the neural\nnetwork to better learn the physical characteristics of MR signals,\nsignificantly enhancing the performance and reliability of quantitative MRI\nsynthesis. This method shows great potential for accelerating qMRI and\nimproving its clinical utility.", "AI": {"tldr": "该研究提出一种深度学习方法，通过将MRI序列参数（TR、TE、TI）嵌入模型，从临床加权MRI图像合成高精度和泛化能力的定量图像。", "motivation": "现有定量图像合成方法在准确性和泛化性方面存在不足，尤其是在面对未见过的脑结构和病变时。研究旨在通过整合MRI物理参数来提高合成质量。", "method": "开发了一个物理驱动的神经网络，将MRI序列参数（TR、TE、TI）通过参数嵌入直接整合到模型中。该模型以常规T1加权、T2加权和T2-FLAIR图像作为输入，合成T1、T2和质子密度（PD）定量图。模型在健康脑部MRI图像上训练，并在内部和外部测试数据集上进行评估。", "result": "该方法在所有合成的参数图上均取得了高性能，PSNR值超过34 dB，SSIM值高于0.92。与传统深度学习模型相比，其在准确性和鲁棒性方面表现更优，尤其对未见过的脑结构和病变区域也能准确合成定量图，显示出卓越的泛化能力。", "conclusion": "通过参数嵌入整合MRI序列参数，显著增强了神经网络学习MR信号物理特性的能力，从而大幅提升了定量MRI合成的性能和可靠性。该方法在加速定量MRI和提高其临床实用性方面具有巨大潜力。"}}
{"id": "2508.08134", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08134", "abs": "https://arxiv.org/abs/2508.08134", "authors": ["Zeqian Long", "Mingzhe Zheng", "Kunyu Feng", "Xinhua Zhang", "Hongyu Liu", "Harry Yang", "Linfeng Zhang", "Qifeng Chen", "Yue Ma"], "title": "Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided Region Control", "comment": "Project webpage is available at https://follow-your-shape.github.io/", "summary": "While recent flow-based image editing models demonstrate general-purpose\ncapabilities across diverse tasks, they often struggle to specialize in\nchallenging scenarios -- particularly those involving large-scale shape\ntransformations. When performing such structural edits, these methods either\nfail to achieve the intended shape change or inadvertently alter non-target\nregions, resulting in degraded background quality. We propose\nFollow-Your-Shape, a training-free and mask-free framework that supports\nprecise and controllable editing of object shapes while strictly preserving\nnon-target content. Motivated by the divergence between inversion and editing\ntrajectories, we compute a Trajectory Divergence Map (TDM) by comparing\ntoken-wise velocity differences between the inversion and denoising paths. The\nTDM enables precise localization of editable regions and guides a Scheduled KV\nInjection mechanism that ensures stable and faithful editing. To facilitate a\nrigorous evaluation, we introduce ReShapeBench, a new benchmark comprising 120\nnew images and enriched prompt pairs specifically curated for shape-aware\nediting. Experiments demonstrate that our method achieves superior editability\nand visual fidelity, particularly in tasks requiring large-scale shape\nreplacement.", "AI": {"tldr": "本文提出了Follow-Your-Shape，一个无需训练、无需掩码的框架，用于精确可控地编辑物体形状，同时严格保留非目标内容，尤其擅长大规模形状变换。", "motivation": "现有基于流的图像编辑模型在处理大规模形状变换等复杂场景时表现不佳，难以实现预期的形状改变，或无意中改变非目标区域，导致背景质量下降。", "method": "核心方法是计算轨迹散度图（Trajectory Divergence Map, TDM），通过比较反演和去噪路径之间基于token的速度差异来精确定位可编辑区域。TDM指导一个计划性KV注入（Scheduled KV Injection）机制，确保编辑的稳定性和保真度。此外，引入了ReShapeBench基准用于严格评估。", "result": "实验证明，该方法在编辑能力和视觉保真度方面表现优越，尤其是在需要大规模形状替换的任务中。", "conclusion": "Follow-Your-Shape框架成功解决了现有模型在大规模形状变换中的局限性，实现了精确、稳定且背景保持良好的形状编辑，并在新基准上验证了其卓越性能。"}}
{"id": "2508.08136", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08136", "abs": "https://arxiv.org/abs/2508.08136", "authors": ["Yitong Yang", "Yinglin Wang", "Changshuo Wang", "Huajie Wang", "Shuting He"], "title": "FantasyStyle: Controllable Stylized Distillation for 3D Gaussian Splatting", "comment": null, "summary": "The success of 3DGS in generative and editing applications has sparked\ngrowing interest in 3DGS-based style transfer. However, current methods still\nface two major challenges: (1) multi-view inconsistency often leads to style\nconflicts, resulting in appearance smoothing and distortion; and (2) heavy\nreliance on VGG features, which struggle to disentangle style and content from\nstyle images, often causing content leakage and excessive stylization. To\ntackle these issues, we introduce \\textbf{FantasyStyle}, a 3DGS-based style\ntransfer framework, and the first to rely entirely on diffusion model\ndistillation. It comprises two key components: (1) \\textbf{Multi-View Frequency\nConsistency}. We enhance cross-view consistency by applying a 3D filter to\nmulti-view noisy latent, selectively reducing low-frequency components to\nmitigate stylized prior conflicts. (2) \\textbf{Controllable Stylized\nDistillation}. To suppress content leakage from style images, we introduce\nnegative guidance to exclude undesired content. In addition, we identify the\nlimitations of Score Distillation Sampling and Delta Denoising Score in 3D\nstyle transfer and remove the reconstruction term accordingly. Building on\nthese insights, we propose a controllable stylized distillation that leverages\nnegative guidance to more effectively optimize the 3D Gaussians. Extensive\nexperiments demonstrate that our method consistently outperforms\nstate-of-the-art approaches, achieving higher stylization quality and visual\nrealism across various scenes and styles.", "AI": {"tldr": "FantasyStyle是一个基于3DGS的风格迁移框架，首次完全依赖扩散模型蒸馏，解决了多视角不一致和内容泄露问题，实现了高质量和视觉真实的风格化效果。", "motivation": "现有3DGS风格迁移方法面临两大挑战：1) 多视角不一致导致外观平滑和扭曲；2) 过度依赖VGG特征，难以解耦风格与内容，导致内容泄露和过度风格化。", "method": "引入FantasyStyle框架，包含两个关键组件：1) 多视角频率一致性：通过对多视角噪声潜在空间应用3D滤波器，选择性降低低频分量以缓解风格化先验冲突，增强跨视角一致性。2) 可控风格化蒸馏：引入负向引导以排除不希望的内容，抑制风格图像的内容泄露；识别并移除了Score Distillation Sampling和Delta Denoising Score在3D风格迁移中的重建项，提出了更有效的优化3D高斯的可控风格化蒸馏。", "result": "广泛的实验证明，该方法在各种场景和风格下始终优于现有SOTA方法，实现了更高的风格化质量和视觉真实感。", "conclusion": "FantasyStyle通过创新的扩散模型蒸馏方法，成功克服了3DGS风格迁移中的多视角不一致和内容泄露问题，显著提升了风格化效果和真实感。"}}
{"id": "2508.08141", "categories": ["cs.CV", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.08141", "abs": "https://arxiv.org/abs/2508.08141", "authors": ["Nicholas Klein", "Hemlata Tak", "James Fullwood", "Krishna Regmi", "Leonidas Spinoulas", "Ganesh Sivaraman", "Tianxiang Chen", "Elie Khoury"], "title": "Pindrop it! Audio and Visual Deepfake Countermeasures for Robust Detection and Fine Grained-Localization", "comment": null, "summary": "The field of visual and audio generation is burgeoning with new\nstate-of-the-art methods. This rapid proliferation of new techniques\nunderscores the need for robust solutions for detecting synthetic content in\nvideos. In particular, when fine-grained alterations via localized\nmanipulations are performed in visual, audio, or both domains, these subtle\nmodifications add challenges to the detection algorithms. This paper presents\nsolutions for the problems of deepfake video classification and localization.\nThe methods were submitted to the ACM 1M Deepfakes Detection Challenge,\nachieving the best performance in the temporal localization task and a top four\nranking in the classification task for the TestA split of the evaluation\ndataset.", "AI": {"tldr": "该论文提出了针对视频深度伪造内容分类和定位的解决方案，在ACM 1M深度伪造检测挑战赛中取得了优异成绩。", "motivation": "随着视觉和音频生成技术快速发展，合成内容激增，对鲁棒的合成视频内容检测方案需求迫切，特别是针对视觉、音频或两者中细微的局部篡改。", "method": "论文提出了用于深度伪造视频分类和定位问题的解决方案。具体方法在摘要中未详细说明，但暗示了多方面的检测技术。", "result": "在ACM 1M深度伪造检测挑战赛中，其方法在时间定位任务上表现最佳，并在分类任务的TestA评估数据集分割中排名前四。", "conclusion": "该论文提出的解决方案能有效应对深度伪造视频的分类和定位挑战，并在大型竞赛中验证了其卓越性能。"}}
{"id": "2508.08165", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.08165", "abs": "https://arxiv.org/abs/2508.08165", "authors": ["Yan Wang", "Da-Wei Zhou", "Han-Jia Ye"], "title": "Integrating Task-Specific and Universal Adapters for Pre-Trained Model-based Class-Incremental Learning", "comment": "Accepted to ICCV 2025. Code is available at:\n  https://github.com/LAMDA-CL/ICCV2025-TUNA", "summary": "Class-Incremental Learning (CIL) requires a learning system to continually\nlearn new classes without forgetting. Existing pre-trained model-based CIL\nmethods often freeze the pre-trained network and adapt to incremental tasks\nusing additional lightweight modules such as adapters. However, incorrect\nmodule selection during inference hurts performance, and task-specific modules\noften overlook shared general knowledge, leading to errors on distinguishing\nbetween similar classes across tasks. To address the aforementioned challenges,\nwe propose integrating Task-Specific and Universal Adapters (TUNA) in this\npaper. Specifically, we train task-specific adapters to capture the most\ncrucial features relevant to their respective tasks and introduce an\nentropy-based selection mechanism to choose the most suitable adapter.\nFurthermore, we leverage an adapter fusion strategy to construct a universal\nadapter, which encodes the most discriminative features shared across tasks. We\ncombine task-specific and universal adapter predictions to harness both\nspecialized and general knowledge during inference. Extensive experiments on\nvarious benchmark datasets demonstrate the state-of-the-art performance of our\napproach. Code is available at: https://github.com/LAMDA-CL/ICCV2025-TUNA", "AI": {"tldr": "本文提出TUNA（Task-Specific and Universal Adapters）方法，通过训练任务特定适配器和构建通用适配器，结合熵选择机制和适配器融合策略，解决了预训练模型增量学习中模块选择不当和忽视通用知识的问题，提升了类增量学习性能。", "motivation": "现有基于预训练模型的类增量学习（CIL）方法在推理时常因模块选择错误而影响性能，且任务特定模块忽视了共享的通用知识，导致难以区分跨任务的相似类别。", "method": "1. 训练任务特定适配器以捕获各任务的关键特征。2. 引入基于熵的选择机制，在推理时选择最合适的适配器。3. 利用适配器融合策略构建一个通用适配器，编码跨任务共享的判别性特征。4. 结合任务特定和通用适配器的预测，在推理时利用专业和通用知识。", "result": "在各种基准数据集上，该方法取得了最先进的性能。", "conclusion": "TUNA方法通过整合任务特定知识和通用共享知识，有效解决了类增量学习中的挑战，显著提升了性能。"}}
{"id": "2508.08170", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08170", "abs": "https://arxiv.org/abs/2508.08170", "authors": ["Chaojun Ni", "Guosheng Zhao", "Xiaofeng Wang", "Zheng Zhu", "Wenkang Qin", "Xinze Chen", "Guanghong Jia", "Guan Huang", "Wenjun Mei"], "title": "ReconDreamer-RL: Enhancing Reinforcement Learning via Diffusion-based Scene Reconstruction", "comment": null, "summary": "Reinforcement learning for training end-to-end autonomous driving models in\nclosed-loop simulations is gaining growing attention. However, most simulation\nenvironments differ significantly from real-world conditions, creating a\nsubstantial simulation-to-reality (sim2real) gap. To bridge this gap, some\napproaches utilize scene reconstruction techniques to create photorealistic\nenvironments as a simulator. While this improves realistic sensor simulation,\nthese methods are inherently constrained by the distribution of the training\ndata, making it difficult to render high-quality sensor data for novel\ntrajectories or corner case scenarios. Therefore, we propose ReconDreamer-RL, a\nframework designed to integrate video diffusion priors into scene\nreconstruction to aid reinforcement learning, thereby enhancing end-to-end\nautonomous driving training. Specifically, in ReconDreamer-RL, we introduce\nReconSimulator, which combines the video diffusion prior for appearance\nmodeling and incorporates a kinematic model for physical modeling, thereby\nreconstructing driving scenarios from real-world data. This narrows the\nsim2real gap for closed-loop evaluation and reinforcement learning. To cover\nmore corner-case scenarios, we introduce the Dynamic Adversary Agent (DAA),\nwhich adjusts the trajectories of surrounding vehicles relative to the ego\nvehicle, autonomously generating corner-case traffic scenarios (e.g., cut-in).\nFinally, the Cousin Trajectory Generator (CTG) is proposed to address the issue\nof training data distribution, which is often biased toward simple\nstraight-line movements. Experiments show that ReconDreamer-RL improves\nend-to-end autonomous driving training, outperforming imitation learning\nmethods with a 5x reduction in the Collision Ratio.", "AI": {"tldr": "ReconDreamer-RL是一个结合视频扩散先验和场景重建的强化学习框架，旨在缩小自动驾驶模拟与现实差距，并通过动态对抗代理和表亲轨迹生成器处理极端情况和数据分布偏差，显著提升端到端自动驾驶训练效果。", "motivation": "现有自动驾驶强化学习模拟器与现实世界存在显著的“模拟到现实”(sim2real)差距。虽然场景重建能提供逼真的传感器数据，但其受限于训练数据分布，难以生成高质量的轨迹或极端情况。这促使研究者寻求一种方法，既能缩小sim2real差距，又能有效生成多样化的训练场景。", "method": "本文提出了ReconDreamer-RL框架。核心组件包括：1) ReconSimulator，结合视频扩散先验进行外观建模和运动学模型进行物理建模，从真实数据重建驾驶场景，以缩小sim2real差距。2) 动态对抗代理 (Dynamic Adversary Agent, DAA)，自动调整周围车辆轨迹以生成极端交通场景（如插队）。3) 表亲轨迹生成器 (Cousin Trajectory Generator, CTG)，用于解决训练数据偏向简单直线运动的问题，生成更多样化的轨迹。", "result": "实验结果表明，ReconDreamer-RL显著改进了端到端自动驾驶训练。与模仿学习方法相比，碰撞率降低了5倍。", "conclusion": "ReconDreamer-RL通过整合视频扩散先验、场景重建、动态对抗代理和轨迹生成，有效解决了自动驾驶强化学习中的sim2real差距、极端情况生成和数据分布偏差问题，显著提升了端到端自动驾驶模型的训练性能和安全性。"}}
{"id": "2508.08173", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.08173", "abs": "https://arxiv.org/abs/2508.08173", "authors": ["Chongke Bi", "Xin Gao", "Jiangkang Deng", "Guan"], "title": "CD-TVD: Contrastive Diffusion for 3D Super-Resolution with Scarce High-Resolution Time-Varying Data", "comment": "Time-varying data visualization, deep learning, super-resolution,\n  diffusion model", "summary": "Large-scale scientific simulations require significant resources to generate\nhigh-resolution time-varying data (TVD). While super-resolution is an efficient\npost-processing strategy to reduce costs, existing methods rely on a large\namount of HR training data, limiting their applicability to diverse simulation\nscenarios. To address this constraint, we proposed CD-TVD, a novel framework\nthat combines contrastive learning and an improved diffusion-based\nsuper-resolution model to achieve accurate 3D super-resolution from limited\ntime-step high-resolution data. During pre-training on historical simulation\ndata, the contrastive encoder and diffusion superresolution modules learn\ndegradation patterns and detailed features of high-resolution and\nlow-resolution samples. In the training phase, the improved diffusion model\nwith a local attention mechanism is fine-tuned using only one newly generated\nhigh-resolution timestep, leveraging the degradation knowledge learned by the\nencoder. This design minimizes the reliance on large-scale high-resolution\ndatasets while maintaining the capability to recover fine-grained details.\nExperimental results on fluid and atmospheric simulation datasets confirm that\nCD-TVD delivers accurate and resource-efficient 3D super-resolution, marking a\nsignificant advancement in data augmentation for large-scale scientific\nsimulations. The code is available at\nhttps://github.com/Xin-Gao-private/CD-TVD.", "AI": {"tldr": "CD-TVD是一种结合对比学习和改进扩散模型的新型超分辨率框架，旨在解决科学模拟中高分辨率训练数据有限的问题，实现高效准确的3D超分辨率。", "motivation": "大规模科学模拟生成高分辨率时变数据需要大量资源，而现有超分辨率方法依赖大量高分辨率训练数据，限制了其在多样化模拟场景中的适用性。", "method": "提出CD-TVD框架，结合对比学习和改进的扩散超分辨率模型。在历史模拟数据上进行预训练，使对比编码器和扩散超分辨率模块学习高分辨率和低分辨率样本的降级模式和细节特征。在训练阶段，使用一个新生成的高分辨率时间步对带有局部注意力机制的改进扩散模型进行微调，利用编码器学习到的降级知识。", "result": "在流体和大气模拟数据集上的实验结果证实，CD-TVD能够提供准确且资源高效的3D超分辨率。", "conclusion": "CD-TVD在减少对大规模高分辨率数据集依赖的同时，保持了恢复精细细节的能力，标志着大规模科学模拟数据增强方面取得了重大进展。"}}
{"id": "2508.08178", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08178", "abs": "https://arxiv.org/abs/2508.08178", "authors": ["Ozhan Suat", "Bedirhan Uguz", "Batuhan Karagoz", "Muhammed Can Keles", "Emre Akbas"], "title": "3D Human Mesh Estimation from Single View RGBD", "comment": null, "summary": "Despite significant progress in 3D human mesh estimation from RGB images;\nRGBD cameras, offering additional depth data, remain underutilized. In this\npaper, we present a method for accurate 3D human mesh estimation from a single\nRGBD view, leveraging the affordability and widespread adoption of RGBD cameras\nfor real-world applications. A fully supervised approach for this problem,\nrequires a dataset with RGBD image and 3D mesh label pairs. However, collecting\nsuch a dataset is costly and challenging, hence, existing datasets are small,\nand limited in pose and shape diversity. To overcome this data scarcity, we\nleverage existing Motion Capture (MoCap) datasets. We first obtain complete 3D\nmeshes from the body models found in MoCap datasets, and create partial,\nsingle-view versions of them by projection to a virtual camera. This simulates\nthe depth data provided by an RGBD camera from a single viewpoint. Then, we\ntrain a masked autoencoder to complete the partial, single-view mesh. During\ninference, our method, which we name as M$^3$ for ``Masked Mesh Modeling'',\nmatches the depth values coming from the sensor to vertices of a template human\nmesh, which creates a partial, single-view mesh. We effectively recover parts\nof the 3D human body mesh model that are not visible, resulting in a full body\nmesh. M$^3$ achieves 16.8 mm and 22.0 mm per-vertex-error (PVE) on the SURREAL\nand CAPE datasets, respectively; outperforming existing methods that use\nfull-body point clouds as input. We obtain a competitive 70.9 PVE on the BEHAVE\ndataset, outperforming a recently published RGB based method by 18.4 mm,\nhighlighting the usefulness of depth data. Code will be released.", "AI": {"tldr": "该论文提出了一种名为M$^3$（Masked Mesh Modeling）的方法，利用RGBD相机数据进行精确的3D人体网格估计。通过模拟RGBD输入并训练一个掩码自编码器来补全部分网格，克服了缺乏大规模RGBD-to-mesh数据集的挑战。", "motivation": "尽管RGBD相机在现实应用中经济且普及，但其在3D人体网格估计方面的潜力尚未被充分利用。全监督方法需要大量的RGBD图像和3D网格标签对数据集，但此类数据集收集成本高昂且多样性有限，导致数据稀缺。", "method": "为了解决数据稀缺问题，该方法利用现有的运动捕捉（MoCap）数据集。首先，从MoCap数据集中获取完整的3D人体网格，然后通过投影到虚拟相机创建模拟RGBD相机提供的局部、单视角深度数据。接着，训练一个掩码自编码器来补全这些局部、单视角网格。在推理阶段，M$^3$方法将传感器深度值与模板人体网格的顶点匹配，生成局部、单视角网格，然后利用训练好的模型恢复不可见的人体部分，从而获得完整的3D身体网格。", "result": "M$^3$在SURREAL数据集上实现了16.8毫米的每顶点误差（PVE），在CAPE数据集上实现了22.0毫米的PVE，优于现有使用全身点云作为输入的方法。在BEHAVE数据集上取得了70.9毫米的PVE，比最近发布的基于RGB的方法性能提高了18.4毫米，突显了深度数据的有效性。", "conclusion": "该研究成功地从单一RGBD视图中恢复了完整的3D人体网格，有效利用了深度数据，并通过创新的数据增强和模型训练策略克服了训练数据稀缺的问题，证明了RGBD相机在3D人体网格估计中的巨大潜力。"}}
{"id": "2508.08179", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.08179", "abs": "https://arxiv.org/abs/2508.08179", "authors": ["Sihan Zhao", "Zixuan Wang", "Tianyu Luan", "Jia Jia", "Wentao Zhu", "Jiebo Luo", "Junsong Yuan", "Nan Xi"], "title": "PP-Motion: Physical-Perceptual Fidelity Evaluation for Human Motion Generation", "comment": "Accepted by ACM Multimedia 2025", "summary": "Human motion generation has found widespread applications in AR/VR, film,\nsports, and medical rehabilitation, offering a cost-effective alternative to\ntraditional motion capture systems. However, evaluating the fidelity of such\ngenerated motions is a crucial, multifaceted task. Although previous approaches\nhave attempted at motion fidelity evaluation using human perception or physical\nconstraints, there remains an inherent gap between human-perceived fidelity and\nphysical feasibility. Moreover, the subjective and coarse binary labeling of\nhuman perception further undermines the development of a robust data-driven\nmetric. We address these issues by introducing a physical labeling method. This\nmethod evaluates motion fidelity by calculating the minimum modifications\nneeded for a motion to align with physical laws. With this approach, we are\nable to produce fine-grained, continuous physical alignment annotations that\nserve as objective ground truth. With these annotations, we propose PP-Motion,\na novel data-driven metric to evaluate both physical and perceptual fidelity of\nhuman motion. To effectively capture underlying physical priors, we employ\nPearson's correlation loss for the training of our metric. Additionally, by\nincorporating a human-based perceptual fidelity loss, our metric can capture\nfidelity that simultaneously considers both human perception and physical\nalignment. Experimental results demonstrate that our metric, PP-Motion, not\nonly aligns with physical laws but also aligns better with human perception of\nmotion fidelity than previous work.", "AI": {"tldr": "针对人体运动生成评估中物理可行性与人类感知之间的鸿沟，本文提出了一种物理标注方法来生成客观的物理对齐真值，并基于此开发了一个名为PP-Motion的数据驱动度量，该度量能同时评估运动的物理和感知保真度。", "motivation": "现有的运动保真度评估方法（如人类感知或物理约束）存在人类感知保真度与物理可行性之间的固有差距。此外，人类感知的标注主观且粗糙，阻碍了鲁棒数据驱动度量的发展。", "method": "引入了一种物理标注方法，通过计算运动符合物理定律所需的最小修改量来评估运动保真度，从而产生细粒度、连续的物理对齐标注作为客观真值。在此基础上，提出了PP-Motion这一新型数据驱动度量，利用皮尔逊相关损失训练以捕捉物理先验，并结合基于人类的感知保真度损失，使其能同时考虑物理对齐和人类感知。", "result": "实验结果表明，PP-Motion度量不仅与物理定律对齐，而且比现有工作更好地与人类对运动保真度的感知对齐。", "conclusion": "PP-Motion成功弥补了人体运动生成评估中物理可行性与人类感知之间的差距，提供了一个既符合物理定律又符合人类感知的有效评估工具。"}}
{"id": "2508.08183", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.08183", "abs": "https://arxiv.org/abs/2508.08183", "authors": ["Hongkun Jin", "Hongcheng Jiang", "Zejun Zhang", "Yuan Zhang", "Jia Fu", "Tingfeng Li", "Kai Luo"], "title": "THAT: Token-wise High-frequency Augmentation Transformer for Hyperspectral Pansharpening", "comment": "Accepted to 2025 IEEE International Conference on Systems, Man, and\n  Cybernetics (SMC)", "summary": "Transformer-based methods have demonstrated strong potential in hyperspectral\npansharpening by modeling long-range dependencies. However, their effectiveness\nis often limited by redundant token representations and a lack of multi-scale\nfeature modeling. Hyperspectral images exhibit intrinsic spectral priors (e.g.,\nabundance sparsity) and spatial priors (e.g., non-local similarity), which are\ncritical for accurate reconstruction. From a spectral-spatial perspective,\nVision Transformers (ViTs) face two major limitations: they struggle to\npreserve high-frequency components--such as material edges and texture\ntransitions--and suffer from attention dispersion across redundant tokens.\nThese issues stem from the global self-attention mechanism, which tends to\ndilute high-frequency signals and overlook localized details. To address these\nchallenges, we propose the Token-wise High-frequency Augmentation Transformer\n(THAT), a novel framework designed to enhance hyperspectral pansharpening\nthrough improved high-frequency feature representation and token selection.\nSpecifically, THAT introduces: (1) Pivotal Token Selective Attention (PTSA) to\nprioritize informative tokens and suppress redundancy; (2) a Multi-level\nVariance-aware Feed-forward Network (MVFN) to enhance high-frequency detail\nlearning. Experiments on standard benchmarks show that THAT achieves\nstate-of-the-art performance with improved reconstruction quality and\nefficiency. The source code is available at https://github.com/kailuo93/THAT.", "AI": {"tldr": "本文提出了一种名为THAT的新型Transformer框架，用于高光谱图像全色锐化，通过选择性注意力机制和多级方差感知前馈网络，解决了现有方法中高频信息丢失和冗余token表示的问题，实现了SOTA性能。", "motivation": "现有基于Transformer的高光谱全色锐化方法存在局限性，包括冗余的token表示、缺乏多尺度特征建模、难以保留高频分量（如材料边缘和纹理过渡），以及全局自注意力机制导致注意力分散和高频信号稀释。这些问题限制了其有效性。", "method": "本文提出了“Token-wise High-frequency Augmentation Transformer (THAT)”框架。具体方法包括：1) 引入“关键token选择性注意力（Pivotal Token Selective Attention, PTSA）”来优先处理信息丰富的token并抑制冗余；2) 设计“多级方差感知前馈网络（Multi-level Variance-aware Feed-forward Network, MVFN）”以增强高频细节学习。", "result": "实验结果表明，THAT在标准基准测试上实现了最先进的性能，显著提高了重建质量和效率。", "conclusion": "THAT框架通过改进高频特征表示和token选择机制，有效解决了高光谱全色锐化中基于Transformer方法的局限性，从而实现了卓越的性能。"}}
{"id": "2508.08186", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08186", "abs": "https://arxiv.org/abs/2508.08186", "authors": ["Md Meftahul Ferdaus", "Mahdi Abdelguerfi", "Elias Ioup", "Steven Sloan", "Kendall N. Niles", "Ken Pathak"], "title": "KARMA: Efficient Structural Defect Segmentation via Kolmogorov-Arnold Representation Learning", "comment": "submitted to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence", "summary": "Semantic segmentation of structural defects in civil infrastructure remains\nchallenging due to variable defect appearances, harsh imaging conditions, and\nsignificant class imbalance. Current deep learning methods, despite their\neffectiveness, typically require millions of parameters, rendering them\nimpractical for real-time inspection systems. We introduce KARMA\n(Kolmogorov-Arnold Representation Mapping Architecture), a highly efficient\nsemantic segmentation framework that models complex defect patterns through\ncompositions of one-dimensional functions rather than conventional\nconvolutions. KARMA features three technical innovations: (1) a\nparameter-efficient Tiny Kolmogorov-Arnold Network (TiKAN) module leveraging\nlow-rank factorization for KAN-based feature transformation; (2) an optimized\nfeature pyramid structure with separable convolutions for multi-scale defect\nanalysis; and (3) a static-dynamic prototype mechanism that enhances feature\nrepresentation for imbalanced classes. Extensive experiments on benchmark\ninfrastructure inspection datasets demonstrate that KARMA achieves competitive\nor superior mean IoU performance compared to state-of-the-art approaches, while\nusing significantly fewer parameters (0.959M vs. 31.04M, a 97% reduction).\nOperating at 0.264 GFLOPS, KARMA maintains inference speeds suitable for\nreal-time deployment, enabling practical automated infrastructure inspection\nsystems without compromising accuracy. The source code can be accessed at the\nfollowing URL: https://github.com/faeyelab/karma.", "AI": {"tldr": "KARMA是一种高效的语义分割框架，用于土木基础设施的结构缺陷检测，通过创新性地使用Kolmogorov-Arnold网络和优化结构，显著减少了模型参数并实现了实时推理速度，同时保持了高精度。", "motivation": "土木基础设施结构缺陷的语义分割面临挑战，包括缺陷外观多变、成像条件恶劣和类别严重不平衡。当前深度学习方法参数量巨大，不适用于实时检测系统。", "method": "本文提出了KARMA（Kolmogorov-Arnold Representation Mapping Architecture）框架，它通过一维函数组合而非传统卷积来建模复杂缺陷模式。主要技术创新包括：1) 参数高效的微型Kolmogorov-Arnold网络（TiKAN）模块，利用低秩分解进行KAN特征变换；2) 优化的特征金字塔结构，采用可分离卷积进行多尺度缺陷分析；3) 静态-动态原型机制，增强了不平衡类别的特征表示。", "result": "在基准基础设施检测数据集上的实验表明，KARMA在平均IoU性能上与最先进的方法相当或更优，同时使用的参数显著减少（0.959M vs. 31.04M，减少97%）。KARMA以0.264 GFLOPS的计算量运行，保持了适合实时部署的推理速度。", "conclusion": "KARMA在不牺牲准确性的前提下，实现了高效、实时的自动化基础设施缺陷检测系统，解决了现有方法在实际应用中的局限性。"}}
{"id": "2508.08189", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08189", "abs": "https://arxiv.org/abs/2508.08189", "authors": ["Weijia Wu", "Chen Gao", "Joya Chen", "Kevin Qinghong Lin", "Qingwei Meng", "Yiming Zhang", "Yuke Qiu", "Hong Zhou", "Mike Zheng Shou"], "title": "Reinforcement Learning in Vision: A Survey", "comment": "22 pages", "summary": "Recent advances at the intersection of reinforcement learning (RL) and visual\nintelligence have enabled agents that not only perceive complex visual scenes\nbut also reason, generate, and act within them. This survey offers a critical\nand up-to-date synthesis of the field. We first formalize visual RL problems\nand trace the evolution of policy-optimization strategies from RLHF to\nverifiable reward paradigms, and from Proximal Policy Optimization to Group\nRelative Policy Optimization. We then organize more than 200 representative\nworks into four thematic pillars: multi-modal large language models, visual\ngeneration, unified model frameworks, and vision-language-action models. For\neach pillar we examine algorithmic design, reward engineering, benchmark\nprogress, and we distill trends such as curriculum-driven training,\npreference-aligned diffusion, and unified reward modeling. Finally, we review\nevaluation protocols spanning set-level fidelity, sample-level preference, and\nstate-level stability, and we identify open challenges that include sample\nefficiency, generalization, and safe deployment. Our goal is to provide\nresearchers and practitioners with a coherent map of the rapidly expanding\nlandscape of visual RL and to highlight promising directions for future\ninquiry. Resources are available at:\nhttps://github.com/weijiawu/Awesome-Visual-Reinforcement-Learning.", "AI": {"tldr": "该论文对视觉强化学习（Visual RL）领域进行了全面且最新的综述，涵盖了其问题形式化、策略优化演变、四大主题支柱、评估协议及未来挑战。", "motivation": "近年来，强化学习与视觉智能的交叉进展使得智能体能够在复杂视觉场景中感知、推理、生成和行动。本研究旨在对该领域进行批判性且及时的综合，为研究人员和实践者提供一个连贯的地图，并指出未来的研究方向。", "method": "该研究首先形式化了视觉强化学习问题，并追溯了从RLHF到可验证奖励范式、从PPO到GRPO的策略优化演变。然后，将200多项代表性工作归纳为多模态大型语言模型、视觉生成、统一模型框架和视觉-语言-动作模型四大主题支柱，并对每个支柱的算法设计、奖励工程和基准进展进行了考察。此外，还提炼了课程驱动训练、偏好对齐扩散和统一奖励建模等趋势，并回顾了评估协议，最后识别了样本效率、泛化和安全部署等开放挑战。", "result": "该综述提供了一个快速扩展的视觉强化学习领域的连贯图景，系统地组织了现有工作，分析了算法设计、奖励工程和基准进展，并识别了关键趋势、评估协议和未来挑战，为研究人员和实践者提供了宝贵的参考。", "conclusion": "该论文成功地为视觉强化学习领域提供了清晰的地图，并指出了未来研究的 promising 方向，特别是样本效率、泛化和安全部署等关键开放挑战。"}}
{"id": "2508.08199", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08199", "abs": "https://arxiv.org/abs/2508.08199", "authors": ["Peiqi He", "Zhenhao Zhang", "Yixiang Zhang", "Xiongjun Zhao", "Shaoliang Peng"], "title": "Spatial-ORMLLM: Improve Spatial Relation Understanding in the Operating Room with Multimodal Large Language Model", "comment": null, "summary": "Precise spatial modeling in the operating room (OR) is foundational to many\nclinical tasks, supporting intraoperative awareness, hazard avoidance, and\nsurgical decision-making. While existing approaches leverage large-scale\nmultimodal datasets for latent-space alignment to implicitly learn spatial\nrelationships, they overlook the 3D capabilities of MLLMs. However, this\napproach raises two issues: (1) Operating rooms typically lack multiple video\nand audio sensors, making multimodal 3D data difficult to obtain; (2) Training\nsolely on readily available 2D data fails to capture fine-grained details in\ncomplex scenes. To address this gap, we introduce Spatial-ORMLLM, the first\nlarge vision-language model for 3D spatial reasoning in operating rooms using\nonly RGB modality to infer volumetric and semantic cues, enabling downstream\nmedical tasks with detailed and holistic spatial context. Spatial-ORMLLM\nincorporates a Spatial-Enhanced Feature Fusion Block, which integrates 2D\nmodality inputs with rich 3D spatial knowledge extracted by the estimation\nalgorithm and then feeds the combined features into the visual tower. By\nemploying a unified end-to-end MLLM framework, it combines powerful spatial\nfeatures with textual features to deliver robust 3D scene reasoning without any\nadditional expert annotations or sensor inputs. Experiments on multiple\nbenchmark clinical datasets demonstrate that Spatial-ORMLLM achieves\nstate-of-the-art performance and generalizes robustly to previously unseen\nsurgical scenarios and downstream tasks.", "AI": {"tldr": "Spatial-ORMLLM是首个针对手术室的3D空间推理大视觉语言模型，仅使用RGB图像推断体积和语义信息，以支持下游医疗任务。", "motivation": "现有方法忽略了多模态大语言模型（MLLMs）的3D能力，且手术室难以获取多模态3D数据，仅依赖2D数据无法捕捉复杂场景的精细细节。", "method": "引入Spatial-ORMLLM，一个端到端统一的MLLM框架。它包含一个Spatial-Enhanced Feature Fusion Block，该模块将2D输入与通过估计算法提取的丰富3D空间知识融合，然后将组合特征输入视觉塔，从而在不依赖额外专家标注或传感器输入的情况下，结合强大的空间和文本特征进行3D场景推理。", "result": "在多个基准临床数据集上，Spatial-ORMLLM取得了最先进的性能，并能稳健地泛化到以前未见的手术场景和下游任务。", "conclusion": "Spatial-ORMLLM成功地解决了手术室3D空间建模中数据获取和细节捕捉的难题，仅通过RGB模态实现了鲁棒的3D空间推理能力，为临床任务提供了详细和整体的空间上下文。"}}
{"id": "2508.08219", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08219", "abs": "https://arxiv.org/abs/2508.08219", "authors": ["Wentao Sun", "Quanyun Wu", "Hanqing Xu", "Kyle Gao", "Zhengsen Xu", "Yiping Chen", "Dedong Zhang", "Lingfei Ma", "John S. Zelek", "Jonathan Li"], "title": "SAGOnline: Segment Any Gaussians Online", "comment": "19 pages, 10 figures", "summary": "3D Gaussian Splatting (3DGS) has emerged as a powerful paradigm for explicit\n3D scene representation, yet achieving efficient and consistent 3D segmentation\nremains challenging. Current methods suffer from prohibitive computational\ncosts, limited 3D spatial reasoning, and an inability to track multiple objects\nsimultaneously. We present Segment Any Gaussians Online (SAGOnline), a\nlightweight and zero-shot framework for real-time 3D segmentation in Gaussian\nscenes that addresses these limitations through two key innovations: (1) a\ndecoupled strategy that integrates video foundation models (e.g., SAM2) for\nview-consistent 2D mask propagation across synthesized views; and (2) a\nGPU-accelerated 3D mask generation and Gaussian-level instance labeling\nalgorithm that assigns unique identifiers to 3D primitives, enabling lossless\nmulti-object tracking and segmentation across views. SAGOnline achieves\nstate-of-the-art performance on NVOS (92.7% mIoU) and Spin-NeRF (95.2% mIoU)\nbenchmarks, outperforming Feature3DGS, OmniSeg3D-gs, and SA3D by 15--1500 times\nin inference speed (27 ms/frame). Qualitative results demonstrate robust\nmulti-object segmentation and tracking in complex scenes. Our contributions\ninclude: (i) a lightweight and zero-shot framework for 3D segmentation in\nGaussian scenes, (ii) explicit labeling of Gaussian primitives enabling\nsimultaneous segmentation and tracking, and (iii) the effective adaptation of\n2D video foundation models to the 3D domain. This work allows real-time\nrendering and 3D scene understanding, paving the way for practical AR/VR and\nrobotic applications.", "AI": {"tldr": "SAGOnline是一个轻量级、零样本框架，通过结合2D视频基础模型和GPU加速的3D高斯实例标签，实现了高斯场景中实时、高效的多对象3D分割和跟踪。", "motivation": "现有3D高斯泼溅(3DGS)场景分割方法存在计算成本高昂、3D空间推理能力有限以及无法同时跟踪多个对象等挑战。", "method": "SAGOnline提出两项关键创新：1) 解耦策略，集成视频基础模型（如SAM2）实现跨合成视图的视图一致性2D掩码传播；2) GPU加速的3D掩码生成和高斯级别实例标记算法，为3D原语分配唯一标识符，实现无损多对象跟踪和分割。", "result": "SAGOnline在NVOS和Spin-NeRF基准测试中取得了最先进的性能（分别为92.7%和95.2% mIoU），推理速度比现有方法（如Feature3DGS、OmniSeg3D-gs、SA3D）快15-1500倍（27毫秒/帧），并展示了在复杂场景中强大的多对象分割和跟踪能力。", "conclusion": "该工作提供了一个轻量级、零样本的高斯场景3D分割框架，通过显式标记高斯原语实现了同步分割和跟踪，并有效将2D视频基础模型应用于3D领域，为实时渲染、3D场景理解以及AR/VR和机器人应用铺平了道路。"}}
{"id": "2508.08220", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08220", "abs": "https://arxiv.org/abs/2508.08220", "authors": ["Wenyi Mo", "Ying Ba", "Tianyu Zhang", "Yalong Bai", "Biye Li"], "title": "Learning User Preferences for Image Generation Model", "comment": null, "summary": "User preference prediction requires a comprehensive and accurate\nunderstanding of individual tastes. This includes both surface-level\nattributes, such as color and style, and deeper content-related aspects, such\nas themes and composition. However, existing methods typically rely on general\nhuman preferences or assume static user profiles, often neglecting individual\nvariability and the dynamic, multifaceted nature of personal taste. To address\nthese limitations, we propose an approach built upon Multimodal Large Language\nModels, introducing contrastive preference loss and preference tokens to learn\npersonalized user preferences from historical interactions. The contrastive\npreference loss is designed to effectively distinguish between user ''likes''\nand ''dislikes'', while the learnable preference tokens capture shared interest\nrepresentations among existing users, enabling the model to activate\ngroup-specific preferences and enhance consistency across similar users.\nExtensive experiments demonstrate our model outperforms other methods in\npreference prediction accuracy, effectively identifying users with similar\naesthetic inclinations and providing more precise guidance for generating\nimages that align with individual tastes. The project page is\n\\texttt{https://learn-user-pref.github.io/}.", "AI": {"tldr": "该论文提出一种基于多模态大语言模型（MLLMs）的用户偏好预测方法，通过引入对比偏好损失和偏好token来从历史交互中学习个性化用户偏好，旨在克服现有方法忽视个体差异和偏好动态性的局限。", "motivation": "现有用户偏好预测方法通常依赖于通用人类偏好或假设静态用户画像，未能充分考虑个体差异以及个人品味的动态、多面性本质，无法全面准确地理解用户偏好。", "method": "提出一种基于多模态大语言模型的方法，引入了：1) 对比偏好损失，旨在有效区分用户的“喜欢”和“不喜欢”；2) 可学习的偏好token，用于捕捉现有用户间的共享兴趣表示，从而激活特定群体的偏好并增强相似用户之间的一致性。该方法从历史交互中学习个性化用户偏好。", "result": "广泛的实验证明，该模型在偏好预测准确性方面优于其他现有方法，能有效识别具有相似审美倾向的用户，并为生成符合个体品味的图像提供更精确的指导。", "conclusion": "该研究通过结合多模态大语言模型、对比偏好损失和偏好token，成功解决了用户偏好预测中个性化和动态性的挑战，显著提升了预测精度和对个性化图像生成的指导能力。"}}
{"id": "2508.08248", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08248", "abs": "https://arxiv.org/abs/2508.08248", "authors": ["Shuyuan Tu", "Yueming Pan", "Yinming Huang", "Xintong Han", "Zhen Xing", "Qi Dai", "Chong Luo", "Zuxuan Wu", "Yu-Gang Jiang"], "title": "StableAvatar: Infinite-Length Audio-Driven Avatar Video Generation", "comment": null, "summary": "Current diffusion models for audio-driven avatar video generation struggle to\nsynthesize long videos with natural audio synchronization and identity\nconsistency. This paper presents StableAvatar, the first end-to-end video\ndiffusion transformer that synthesizes infinite-length high-quality videos\nwithout post-processing. Conditioned on a reference image and audio,\nStableAvatar integrates tailored training and inference modules to enable\ninfinite-length video generation. We observe that the main reason preventing\nexisting models from generating long videos lies in their audio modeling. They\ntypically rely on third-party off-the-shelf extractors to obtain audio\nembeddings, which are then directly injected into the diffusion model via\ncross-attention. Since current diffusion backbones lack any audio-related\npriors, this approach causes severe latent distribution error accumulation\nacross video clips, leading the latent distribution of subsequent segments to\ndrift away from the optimal distribution gradually. To address this,\nStableAvatar introduces a novel Time-step-aware Audio Adapter that prevents\nerror accumulation via time-step-aware modulation. During inference, we propose\na novel Audio Native Guidance Mechanism to further enhance the audio\nsynchronization by leveraging the diffusion's own evolving joint audio-latent\nprediction as a dynamic guidance signal. To enhance the smoothness of the\ninfinite-length videos, we introduce a Dynamic Weighted Sliding-window Strategy\nthat fuses latent over time. Experiments on benchmarks show the effectiveness\nof StableAvatar both qualitatively and quantitatively.", "AI": {"tldr": "StableAvatar是首个端到端视频扩散Transformer，能够生成无限长、高质量的音频驱动虚拟形象视频，解决了现有模型在长视频生成中音画同步和身份一致性差的问题。", "motivation": "现有音频驱动虚拟形象视频扩散模型难以生成长视频，主要原因在于其音频建模方式导致潜在分布误差累积，使得后续视频片段的潜在分布逐渐偏离最优，导致音画不同步和身份不一致。", "method": "StableAvatar引入了时间步感知音频适配器（Time-step-aware Audio Adapter）以防止误差累积；提出音频原生引导机制（Audio Native Guidance Mechanism）利用扩散模型自身的联合音画潜在预测作为动态引导信号，增强音画同步；并采用动态加权滑动窗口策略（Dynamic Weighted Sliding-window Strategy）融合潜在空间，提高无限长视频的平滑度。", "result": "在基准测试上的实验表明，StableAvatar在定性和定量上均有效，能够生成高质量的无限长视频。", "conclusion": "StableAvatar通过创新的音频处理和推理机制，成功实现了无需后处理的无限长、高质量、音画自然同步且身份一致的音频驱动虚拟形象视频生成。"}}
{"id": "2508.08252", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08252", "abs": "https://arxiv.org/abs/2508.08252", "authors": ["Shuting He", "Guangquan Jie", "Changshuo Wang", "Yun Zhou", "Shuming Hu", "Guanbin Li", "Henghui Ding"], "title": "ReferSplat: Referring Segmentation in 3D Gaussian Splatting", "comment": "ICML 2025 Oral, Code: https://github.com/heshuting555/ReferSplat", "summary": "We introduce Referring 3D Gaussian Splatting Segmentation (R3DGS), a new task\nthat aims to segment target objects in a 3D Gaussian scene based on natural\nlanguage descriptions, which often contain spatial relationships or object\nattributes. This task requires the model to identify newly described objects\nthat may be occluded or not directly visible in a novel view, posing a\nsignificant challenge for 3D multi-modal understanding. Developing this\ncapability is crucial for advancing embodied AI. To support research in this\narea, we construct the first R3DGS dataset, Ref-LERF. Our analysis reveals that\n3D multi-modal understanding and spatial relationship modeling are key\nchallenges for R3DGS. To address these challenges, we propose ReferSplat, a\nframework that explicitly models 3D Gaussian points with natural language\nexpressions in a spatially aware paradigm. ReferSplat achieves state-of-the-art\nperformance on both the newly proposed R3DGS task and 3D open-vocabulary\nsegmentation benchmarks. Dataset and code are available at\nhttps://github.com/heshuting555/ReferSplat.", "AI": {"tldr": "本文提出了R3DGS任务，旨在基于自然语言描述（包含空间关系和物体属性）分割3D高斯场景中的目标物体，并构建了首个R3DGS数据集Ref-LERF。为解决3D多模态理解和空间关系建模的挑战，提出了ReferSplat框架，实现了SOTA性能。", "motivation": "在3D高斯场景中，基于自然语言描述（特别是包含空间关系或涉及遮挡、新视角下不可见的物体）来分割目标物体，对3D多模态理解提出了巨大挑战，而发展此能力对具身AI至关重要。", "method": "引入了Referring 3D Gaussian Splatting Segmentation (R3DGS) 新任务，并构建了首个R3DGS数据集Ref-LERF。针对3D多模态理解和空间关系建模的挑战，提出了ReferSplat框架，该框架在一个空间感知范式中，用自然语言表达式显式地建模3D高斯点。", "result": "ReferSplat在R3DGS任务和3D开放词汇分割基准测试中均达到了最先进的性能。", "conclusion": "R3DGS是一个新的、具有挑战性的任务，需要强大的3D多模态理解和空间关系建模能力。ReferSplat框架有效地解决了这些挑战，并在相关任务上取得了优异表现。"}}
{"id": "2508.08254", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08254", "abs": "https://arxiv.org/abs/2508.08254", "authors": ["Emily Yue-Ting Jia", "Jiageng Mao", "Zhiyuan Gao", "Yajie Zhao", "Yue Wang"], "title": "Learning an Implicit Physics Model for Image-based Fluid Simulation", "comment": "Accepted at ICCV 2025", "summary": "Humans possess an exceptional ability to imagine 4D scenes, encompassing both\nmotion and 3D geometry, from a single still image. This ability is rooted in\nour accumulated observations of similar scenes and an intuitive understanding\nof physics. In this paper, we aim to replicate this capacity in neural\nnetworks, specifically focusing on natural fluid imagery. Existing methods for\nthis task typically employ simplistic 2D motion estimators to animate the\nimage, leading to motion predictions that often defy physical principles,\nresulting in unrealistic animations. Our approach introduces a novel method for\ngenerating 4D scenes with physics-consistent animation from a single image. We\npropose the use of a physics-informed neural network that predicts motion for\neach surface point, guided by a loss term derived from fundamental physical\nprinciples, including the Navier-Stokes equations. To capture appearance, we\npredict feature-based 3D Gaussians from the input image and its estimated\ndepth, which are then animated using the predicted motions and rendered from\nany desired camera perspective. Experimental results highlight the\neffectiveness of our method in producing physically plausible animations,\nshowcasing significant performance improvements over existing methods. Our\nproject page is https://physfluid.github.io/ .", "AI": {"tldr": "该论文提出一种新颖方法，利用物理信息神经网络从单张图像生成物理一致的4D流体场景动画，显著优于现有方法。", "motivation": "人类能从单张静止图像想象出包含运动和3D几何的4D场景，这源于对类似场景的观察和对物理的直观理解。现有方法通常使用简单的2D运动估计器来动画化图像，导致预测的运动不符合物理原理，产生不真实的动画。", "method": "引入物理信息神经网络（physics-informed neural network），通过基于纳维-斯托克斯方程等基本物理原理的损失项，预测每个表面点的运动。为了捕捉外观，从输入图像及其估计深度预测基于特征的3D高斯，然后使用预测的运动进行动画化，并从任意摄像机视角渲染。", "result": "实验结果表明，该方法能有效生成物理上合理的动画，性能显著优于现有方法。", "conclusion": "该方法成功复制了人类从单张图像想象4D场景的能力，生成了物理上合理且逼真的流体动画，解决了现有方法动画不真实的问题。"}}
