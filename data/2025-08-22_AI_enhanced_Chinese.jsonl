{"id": "2508.14989", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.14989", "abs": "https://arxiv.org/abs/2508.14989", "authors": ["Saiedeh Akbari", "Omkar Sudhir Patil", "Warren E. Dixon"], "title": "LyLA-Therm: Lyapunov-based Langevin Adaptive Thermodynamic Neural Network Controller", "comment": null, "summary": "Thermodynamic principles can be employed to design parameter update laws that\naddress challenges such as the exploration vs. exploitation dilemma. In this\npaper, inspired by the Langevin equation, an update law is developed for a\nLyapunov-based DNN control method, taking the form of a stochastic differential\nequation. The drift term is designed to minimize the system's generalized\ninternal energy, while the diffusion term is governed by a user-selected\ngeneralized temperature law, allowing for more controlled fluctuations. The\nminimization of generalized internal energy in this design fulfills the\nexploitation objective, while the temperature-based stochastic noise ensures\nsufficient exploration. Using a Lyapunov-based stability analysis, the proposed\nLyapunov-based Langevin Adaptive Thermodynamic (LyLA-Therm) neural network\ncontroller achieves probabilistic convergence of the tracking and parameter\nestimation errors to an ultimate bound. Simulation results demonstrate the\neffectiveness of the proposed approach, with the LyLA-Therm architecture\nachieving up to 20.66% improvement in tracking errors, up to 20.89% improvement\nin function approximation errors, and up to 11.31% improvement in\noff-trajectory function approximation errors compared to the baseline\ndeterministic approach.", "AI": {"tldr": "本文基于热力学原理和朗之万方程，为Lyapunov-based DNN控制方法提出了一种随机微分更新律（LyLA-Therm）。该方法通过最小化广义内能实现利用，并通过广义温度控制的随机噪声实现探索，在跟踪和参数估计误差方面取得了显著改进。", "motivation": "解决深度神经网络（DNN）控制方法中参数更新的探索与利用两难困境。", "method": "借鉴朗之万方程，开发了一种随机微分方程形式的更新律，用于基于Lyapunov的DNN控制。该更新律的漂移项旨在最小化系统的广义内能以实现利用，而扩散项由用户选择的广义温度定律控制，以确保受控的波动并实现探索。通过Lyapunov稳定性分析，证明了其概率收敛性。", "result": "所提出的LyLA-Therm神经网络控制器实现了跟踪和参数估计误差到最终界限的概率收敛。仿真结果显示，与基线确定性方法相比，LyLA-Therm架构在跟踪误差方面提高了20.66%，在函数逼近误差方面提高了20.89%，在离轨函数逼近误差方面提高了11.31%。", "conclusion": "基于热力学原理和朗之万方程设计的LyLA-Therm神经网络控制器，通过有效平衡广义内能最小化（利用）和温度控制噪声（探索），成功解决了DNN控制中的探索与利用困境，并在各项性能指标上显著优于基线确定性方法。"}}
{"id": "2508.15006", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.15006", "abs": "https://arxiv.org/abs/2508.15006", "authors": ["Omid Mokhtari", "Samuel Chevalier", "Mads Almassalkhi"], "title": "Structure-preserving Optimal Kron-based Reduction of Radial Distribution Networks", "comment": null, "summary": "Network reduction simplifies complex electrical networks to address\ncomputational challenges of large-scale transmission and distribution grids.\nTraditional network reduction methods are often based on a predefined set of\nnodes or lines to remain in the reduced network. This paper builds upon\nprevious work on Optimal Kron-based Reduction of Networks (Opti-KRON), which\nwas formulated as a mixed-integer linear program (MILP), to enhance the\nframework in two aspects. First, the scalability is improved via a cutting\nplane restriction, tightened Big~M bounds, and a zero-injection node reduction\nstage. Next, we introduce a radiality-preservation step to identify and recover\nnodes whose restoration ensures radiality of the reduced network. The model is\nvalidated through its application to the 533-bus distribution test system and a\n3499-bus realistic test feeder for a set of representative loading scenarios.\nIn the 533-bus system, an 85% reduction was achieved with a maximum voltage\nerror below 0.0025 p.u., while in the 3499-bus feeder, over 94% reduction was\nobtained with maximum voltage errors below 0.002 p.u. Additionally, we show\nthat the radialization step accelerates the runtime of optimal voltage control\nproblems when applied to Kron-reduced networks.", "AI": {"tldr": "本文改进了基于Kron最优网络缩减（Opti-KRON）框架，提升了其可扩展性并引入了辐射状保持步骤，实现了大规模电网的显著缩减和低电压误差，并加速了电压控制问题的求解。", "motivation": "大型输配电网的计算挑战促使了网络缩减技术的发展。传统的网络缩减方法通常依赖预定义的节点或线路，而Opti-KRON作为一种混合整数线性规划（MILP）方法，需要进一步提升其可扩展性和对网络拓扑特性（如辐射状）的保持能力。", "method": "本文在Opti-KRON（MILP）的基础上进行了两方面增强：1) 通过割平面限制、收紧的Big M界限和零注入节点缩减阶段提高了可扩展性。2) 引入了辐射状保持步骤，识别并恢复特定节点以确保缩减网络的辐射状特性。", "result": "该模型在533节点配电系统和3499节点实际馈线系统上进行了验证。在533节点系统上，实现了85%的缩减，最大电压误差低于0.0025 p.u.；在3499节点馈线系统上，实现了超过94%的缩减，最大电压误差低于0.002 p.u.。此外，研究表明辐射状化步骤能够加速Kron缩减网络上的最优电压控制问题的运行时间。", "conclusion": "通过引入可扩展性改进和辐射状保持步骤，增强的Opti-KRON框架能够对大规模电网进行高效且高精度的缩减，显著降低了计算复杂性，并且其辐射状保持特性有助于加速后续优化问题的求解。"}}
{"id": "2508.15040", "categories": ["eess.SY", "cs.RO", "cs.SY", "math.DS"], "pdf": "https://arxiv.org/pdf/2508.15040", "abs": "https://arxiv.org/abs/2508.15040", "authors": ["Aakash Khandelwal", "Ranjan Mukherjee"], "title": "Discrete VHCs for Propeller Motion of a Devil-Stick using purely Impulsive Inputs", "comment": "16 pages, 11 figures. This work has been submitted to the IEEE for\n  possible publication", "summary": "The control problem of realizing propeller motion of a devil-stick in the\nvertical plane using impulsive forces applied normal to the stick is\nconsidered. This problem is an example of underactuated robotic juggling and\nhas not been considered in the literature before. Inspired by virtual holonomic\nconstraints, the concept of discrete virtual holonomic constraints (DVHC) is\nintroduced for the first time to solve this orbital stabilization problem. At\nthe discrete instants when impulsive inputs are applied, the location of the\ncenter-of-mass of the devil-stick is specified in terms of its orientation\nangle. This yields the discrete zero dynamics (DZD), which provides conditions\nfor stable propeller motion. In the limiting case, when the rotation angle\nbetween successive applications of impulsive inputs is chosen to be arbitrarily\nsmall, the problem reduces to that of propeller motion under continuous\nforcing. A controller that enforces the DVHC, and an orbit stabilizing\ncontroller based on the impulse controlled Poincar\\'e map approach are\npresented. The efficacy of the approach to trajectory design and stabilization\nis validated through simulations.", "AI": {"tldr": "本文提出了一种使用脉冲力控制魔术棒（devil-stick）在垂直平面内实现螺旋桨运动的方法，并引入了离散虚拟完整约束（DVHC）来解决这一轨道稳定问题。", "motivation": "魔术棒的螺旋桨运动是一个欠驱动的机器人杂耍问题，在现有文献中尚未被研究过。", "method": "本文首次引入了离散虚拟完整约束（DVHC）概念，通过在施加脉冲输入的离散时刻指定魔术棒质心位置与其姿态角的关系。这导出了离散零动态（DZD），并基于此设计了一个强制执行DVHC的控制器，以及一个基于脉冲控制庞加莱映射方法的轨道稳定控制器。研究了当连续脉冲输入之间旋转角度无限小时的连续受力情况。", "result": "离散零动态（DZD）提供了稳定螺旋桨运动的条件。通过仿真验证了所提出方法在轨迹设计和稳定方面的有效性。在极限情况下，问题可以简化为连续受力下的螺旋桨运动。", "conclusion": "所提出的离散虚拟完整约束（DVHC）方法对于魔术棒螺旋桨运动的轨迹设计和稳定是有效的。"}}
{"id": "2508.15092", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.15092", "abs": "https://arxiv.org/abs/2508.15092", "authors": ["Ravi Raj Shrestha", "Zhi Zhou", "Limon Barua", "Nazib Siddique", "Karthikeyan Balasubramaniam", "Yan Zhou", "Lusha Wang"], "title": "Smart Charging Impact Analysis using Clustering Methods and Real-world Distribution Feeders", "comment": null, "summary": "The anticipated widespread adoption of electric vehicles (EVs) necessitates a\ncritical evaluation of existing power distribution infrastructures, as EV\nintegration imposes additional stress on distribution networks that can lead to\ncomponent overloading and power quality degradation. Implementing smart\ncharging mechanisms can mitigate these adverse effects and defer or even avoid\nupgrades. This study assesses the performance of two smart charging strategies\n- Time of Use (TOU) pricing and Load Balancing (LB) on seven representative\nreal-world feeders identified using k-means clustering. A time series-based\nsteady-state load flow analysis was conducted on these feeders to simulate the\nimpact of EV charging under both strategies across four different EV enrollment\nscenarios and three representative days to capture seasonal load\ncharacteristics. A grid upgrade strategy has been proposed to strengthen the\npower grid to support EV integration with minimal cost. Results demonstrate\nthat both TOU and LB strategies effectively manage the additional EV load with\nreduced upgrade requirement and cost to existing infrastructure compared to the\ncase without smart charging strategies and LB outperforms TOU when the customer\nenrollment levels are high. These findings support the viability of smart\ncharging in facilitating EV integration while maintaining distribution network\nreliability and reducing investment cost.", "AI": {"tldr": "本研究评估了分时电价（TOU）和负荷平衡（LB）两种智能充电策略在七个真实馈线上的性能，以应对电动汽车（EV）集成对配电网的压力。结果表明，这两种策略都能有效管理电动汽车负荷，减少电网升级需求和成本，其中在电动汽车普及率高时，负荷平衡策略优于分时电价策略。", "motivation": "电动汽车的广泛普及将对现有配电基础设施造成额外压力，可能导致组件过载和电能质量下降。实施智能充电机制可以缓解这些不利影响，并推迟甚至避免电网升级。", "method": "本研究评估了分时电价（TOU）和负荷平衡（LB）两种智能充电策略的性能。使用k-均值聚类识别了七个具有代表性的真实馈线。通过时间序列稳态潮流分析，模拟了在四种不同的电动汽车注册情景和三个代表性日期（捕捉季节性负荷特征）下，两种策略对电动汽车充电影响。此外，还提出了一种以最小成本加强电网以支持电动汽车集成的电网升级策略。", "result": "研究结果表明，与没有智能充电策略的情况相比，分时电价和负荷平衡策略都能有效管理额外的电动汽车负荷，并减少现有基础设施的升级需求和成本。在高客户注册水平下，负荷平衡策略的性能优于分时电价策略。", "conclusion": "这些发现支持了智能充电在促进电动汽车集成、同时保持配电网络可靠性和降低投资成本方面的可行性。"}}
{"id": "2508.14994", "categories": ["cs.RO", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.14994", "abs": "https://arxiv.org/abs/2508.14994", "authors": ["Murilo Vinicius da Silva", "Matheus Hipolito Carvalho", "Juliano Negri", "Thiago Segreto", "Gustavo J. G. Lahr", "Ricardo V. Godoy", "Marcelo Becker"], "title": "A Vision-Based Shared-Control Teleoperation Scheme for Controlling the Robotic Arm of a Four-Legged Robot", "comment": null, "summary": "In hazardous and remote environments, robotic systems perform critical tasks\ndemanding improved safety and efficiency. Among these, quadruped robots with\nmanipulator arms offer mobility and versatility for complex operations.\nHowever, teleoperating quadruped robots is challenging due to the lack of\nintegrated obstacle detection and intuitive control methods for the robotic\narm, increasing collision risks in confined or dynamically changing workspaces.\nTeleoperation via joysticks or pads can be non-intuitive and demands a high\nlevel of expertise due to its complexity, culminating in a high cognitive load\non the operator. To address this challenge, a teleoperation approach that\ndirectly maps human arm movements to the robotic manipulator offers a simpler\nand more accessible solution. This work proposes an intuitive remote control by\nleveraging a vision-based pose estimation pipeline that utilizes an external\ncamera with a machine learning-based model to detect the operator's wrist\nposition. The system maps these wrist movements into robotic arm commands to\ncontrol the robot's arm in real-time. A trajectory planner ensures safe\nteleoperation by detecting and preventing collisions with both obstacles and\nthe robotic arm itself. The system was validated on the real robot,\ndemonstrating robust performance in real-time control. This teleoperation\napproach provides a cost-effective solution for industrial applications where\nsafety, precision, and ease of use are paramount, ensuring reliable and\nintuitive robotic control in high-risk environments.", "AI": {"tldr": "提出了一种基于视觉姿态估计的直观遥操作方法，通过机器学习模型检测操作员手腕运动，并将其映射到四足机器人机械臂的实时控制，同时结合轨迹规划器确保防碰撞安全，适用于高风险环境。", "motivation": "在危险和远程环境中，四足机器人与机械臂的遥操作面临挑战。传统方法缺乏集成的障碍物检测和直观的机械臂控制，导致碰撞风险高、操作员认知负荷大，且需要高专业技能。", "method": "本研究提出了一种直观的遥操作方法。该方法利用外部摄像头和基于机器学习模型的视觉姿态估计算法来检测操作员的手腕位置。检测到的手腕运动被实时映射为机器人机械臂的控制命令。此外，系统还包含一个轨迹规划器，用于检测并防止与障碍物和机械臂自身的碰撞，从而确保操作安全。", "result": "该系统在真实机器人上进行了验证，展示了在实时控制方面的鲁棒性能。", "conclusion": "该遥操作方法为工业应用提供了一种经济高效的解决方案，特别是在安全、精度和易用性至关重要的危险环境中，能够实现可靠且直观的机器人控制。"}}
{"id": "2508.14931", "categories": ["eess.IV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2508.14931", "abs": "https://arxiv.org/abs/2508.14931", "authors": ["Zahra TehraniNasab", "Amar Kumar", "Tal Arbel"], "title": "Pixels Under Pressure: Exploring Fine-Tuning Paradigms for Foundation Models in High-Resolution Medical Imaging", "comment": null, "summary": "Advancements in diffusion-based foundation models have improved text-to-image\ngeneration, yet most efforts have been limited to low-resolution settings. As\nhigh-resolution image synthesis becomes increasingly essential for various\napplications, particularly in medical imaging domains, fine-tuning emerges as a\ncrucial mechanism for adapting these powerful pre-trained models to\ntask-specific requirements and data distributions. In this work, we present a\nsystematic study, examining the impact of various fine-tuning techniques on\nimage generation quality when scaling to high resolution 512x512 pixels. We\nbenchmark a diverse set of fine-tuning methods, including full fine-tuning\nstrategies and parameter-efficient fine-tuning (PEFT). We dissect how different\nfine-tuning methods influence key quality metrics, including Fr\\'echet\nInception Distance (FID), Vendi score, and prompt-image alignment. We also\nevaluate the utility of generated images in a downstream classification task\nunder data-scarce conditions, demonstrating that specific fine-tuning\nstrategies improve both generation fidelity and downstream performance when\nsynthetic images are used for classifier training and evaluation on real\nimages. Our code is accessible through the project website -\nhttps://tehraninasab.github.io/PixelUPressure/.", "AI": {"tldr": "本文系统研究了在512x512高分辨率下，不同微调技术（包括全微调和PEFT）对扩散模型图像生成质量的影响，并评估了生成图像在数据稀缺下游分类任务中的实用性。", "motivation": "尽管扩散模型在文本到图像生成方面取得了进展，但大多数工作局限于低分辨率。高分辨率图像合成在许多应用中至关重要，尤其是在医学成像领域。因此，需要研究微调机制以使预训练模型适应特定任务和数据分布。", "method": "本研究进行了一项系统性研究，考察了多种微调技术（包括全微调策略和参数高效微调PEFT）对512x512高分辨率图像生成质量的影响。评估指标包括Fréchet Inception Distance (FID)、Vendi score和提示-图像对齐度。此外，还在数据稀缺条件下，评估了生成图像在下游分类任务中的效用。", "result": "不同的微调方法会影响关键质量指标。特定的微调策略不仅能提高生成保真度，还能在合成图像用于分类器训练和真实图像评估时，改善下游任务的性能。", "conclusion": "本研究提供了一个系统性的分析，揭示了在扩展到高分辨率时，不同微调策略对扩散模型生成图像质量的影响，并证明了特定微调方法在数据稀缺的下游分类任务中具有实用性。"}}
{"id": "2508.14904", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.14904", "abs": "https://arxiv.org/abs/2508.14904", "authors": ["Jianfeng Si", "Lin Sun", "Zhewen Tan", "Xiangzheng Zhang"], "title": "Efficient Switchable Safety Control in LLMs via Magic-Token-Guided Co-Training", "comment": "12 pages,5 figures,4 tables", "summary": "Current methods for content safety in Large Language Models (LLMs), such as\nSupervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback\n(RLHF), often rely on multi-stage training pipelines and lack fine-grained,\npost-deployment controllability. To address these limitations, we propose a\nunified co-training framework that efficiently integrates multiple safety\nbehaviors: positive (lawful/prosocial), negative (unfiltered/risk-prone) and\nrejective (refusal-oriented/conservative) within a single SFT stage. Notably,\neach behavior is dynamically activated via a simple system-level instruction,\nor magic token, enabling stealthy and efficient behavioral switching at\ninference time. This flexibility supports diverse deployment scenarios, such as\npositive for safe user interaction, negative for internal red-teaming, and\nrejective for context-aware refusals triggered by upstream moderation signals.\nThis co-training strategy induces a distinct Safety Alignment Margin in the\noutput space, characterized by well-separated response distributions\ncorresponding to each safety mode. The existence of this margin provides\nempirical evidence for the model's safety robustness and enables unprecedented\nfine-grained control. Experiments show that our method matches the safety\nalignment quality of SFT+DPO, with our 8B model notably surpassing DeepSeek-R1\n(671B) in safety performance, while significantly reducing both training\ncomplexity and deployment costs. This work presents a scalable, efficient, and\nhighly controllable solution for LLM content safety.", "AI": {"tldr": "该论文提出一种统一的协同训练框架，在单个SFT阶段整合多种安全行为（积极、消极、拒绝），并通过“魔法令牌”在推理时动态切换，以提供高效、可控且可扩展的LLM内容安全解决方案。", "motivation": "当前LLM内容安全方法（如SFT和RLHF）通常依赖多阶段训练流程，且缺乏部署后的细粒度可控性。", "method": "提出一个统一的协同训练框架，在单个SFT阶段高效整合积极（合法/亲社会）、消极（无过滤/风险倾向）和拒绝（拒绝导向/保守）三种安全行为。通过简单的系统级指令或“魔法令牌”在推理时动态激活和切换这些行为。", "result": "该协同训练策略在输出空间中诱导生成独特的“安全对齐裕度”，使各安全模式的响应分布清晰分离，提供了模型安全鲁棒性的经验证据并实现了细粒度控制。实验表明，该方法在安全对齐质量上与SFT+DPO相当，其8B模型在安全性能上显著超越DeepSeek-R1 (671B)，同时大幅降低了训练复杂度和部署成本。", "conclusion": "本工作为LLM内容安全提供了一个可扩展、高效且高度可控的解决方案。"}}
{"id": "2508.14929", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.14929", "abs": "https://arxiv.org/abs/2508.14929", "authors": ["Chiao-An Yang", "Raymond A. Yeh"], "title": "Heatmap Regression without Soft-Argmax for Facial Landmark Detection", "comment": null, "summary": "Facial landmark detection is an important task in computer vision with\nnumerous applications, such as head pose estimation, expression analysis, face\nswapping, etc. Heatmap regression-based methods have been widely used to\nachieve state-of-the-art results in this task. These methods involve computing\nthe argmax over the heatmaps to predict a landmark. Since argmax is not\ndifferentiable, these methods use a differentiable approximation, Soft-argmax,\nto enable end-to-end training on deep-nets. In this work, we revisit this\nlong-standing choice of using Soft-argmax and demonstrate that it is not the\nonly way to achieve strong performance. Instead, we propose an alternative\ntraining objective based on the classic structured prediction framework.\nEmpirically, our method achieves state-of-the-art performance on three facial\nlandmark benchmarks (WFLW, COFW, and 300W), converging 2.2x faster during\ntraining while maintaining better/competitive accuracy. Our code is available\nhere: https://github.com/ca-joe-yang/regression-without-softarg.", "AI": {"tldr": "本文重新审视了面部标志点检测中基于热图回归的Soft-argmax方法，提出了一种基于经典结构化预测框架的替代训练目标，实现了最先进的性能，训练收敛速度提高2.2倍，同时保持或提升了精度。", "motivation": "面部标志点检测在计算机视觉中应用广泛。基于热图回归的方法（使用Soft-argmax进行可微分近似）已达到最先进水平。然而，Soft-argmax是argmax的近似，作者质疑它是否是实现高性能的唯一途径。", "method": "本文提出了一种替代的训练目标，该目标基于经典的结构化预测框架。它旨在取代Soft-argmax作为从热图预测标志点并实现端到端训练的方法。", "result": "该方法在WFLW、COFW和300W三个面部标志点基准测试上取得了最先进的性能。训练收敛速度比现有方法快2.2倍，同时保持了更好或具有竞争力的准确性。", "conclusion": "研究表明，Soft-argmax并非实现强大性能的唯一选择。通过采用基于结构化预测的替代训练目标，可以在面部标志点检测任务中实现更快的训练收敛速度和同等或更优的精度。"}}
{"id": "2508.14923", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.14923", "abs": "https://arxiv.org/abs/2508.14923", "authors": ["Andrew Kiruluta"], "title": "A Fully Spectral Neuro-Symbolic Reasoning Architecture with Graph Signal Processing as the Computational Backbone", "comment": null, "summary": "We propose a fully spectral, neuro\\-symbolic reasoning architecture that\nleverages Graph Signal Processing (GSP) as the primary computational backbone\nfor integrating symbolic logic and neural inference. Unlike conventional\nreasoning models that treat spectral graph methods as peripheral components,\nour approach formulates the entire reasoning pipeline in the graph spectral\ndomain. Logical entities and relationships are encoded as graph signals,\nprocessed via learnable spectral filters that control multi-scale information\npropagation, and mapped into symbolic predicates for rule-based inference. We\npresent a complete mathematical framework for spectral reasoning, including\ngraph Fourier transforms, band-selective attention, and spectral rule\ngrounding. Experiments on benchmark reasoning datasets (ProofWriter,\nEntailmentBank, bAbI, CLUTRR, and ARC-Challenge) demonstrate improvements in\nlogical consistency, interpretability, and computational efficiency over\nstate\\-of\\-the\\-art neuro\\-symbolic models. Our results suggest that GSP\nprovides a mathematically grounded and computationally efficient substrate for\nrobust and interpretable reasoning systems.", "AI": {"tldr": "提出了一种完全基于图信号处理（GSP）的神经符号推理架构，将整个推理过程在图谱域中进行，实现了逻辑与神经推理的深度整合。", "motivation": "现有推理模型将谱图方法视为辅助组件，缺乏对符号逻辑和神经推理的有效整合，需要一种更具数学基础和计算效率的方法来提高逻辑一致性、可解释性和效率。", "method": "构建了一个完全谱域的神经符号推理架构。将逻辑实体和关系编码为图信号，通过可学习的谱滤波器处理以控制多尺度信息传播，并映射到符号谓词进行基于规则的推理。该框架包括图傅里叶变换、带选择性注意力以及谱规则接合。", "result": "在ProofWriter、EntailmentBank、bAbI、CLUTRR和ARC-Challenge等基准推理数据集上的实验表明，与最先进的神经符号模型相比，该方法在逻辑一致性、可解释性和计算效率方面均有显著提升。", "conclusion": "研究结果表明，图信号处理为构建鲁棒且可解释的推理系统提供了一个具有数学基础和计算效率的底层支持。"}}
{"id": "2508.15175", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.15175", "abs": "https://arxiv.org/abs/2508.15175", "authors": ["Xinhao Yan", "Bo Chen", "Hailong Huang"], "title": "Locally Differentially Private Multi-Sensor Fusion Estimation With System Intrinsic Randomness", "comment": "12 pages, 5 figures", "summary": "This paper focuses on the privacy-preserving multi-sensor fusion estimation\n(MSFE) problem with differential privacy considerations. Most existing research\nefforts are directed towards the exploration of traditional differential\nprivacy, also referred to as centralized differential privacy (CDP). It is\nimportant to note that CDP is tailored to protect the privacy of statistical\ndata at fusion center such as averages and sums rather than individual data at\nsensors, which renders it inappropriate for MSFE. Additionally, the definitions\nand assumptions of CDP are primarily applicable for large-scale systems that\nrequire statistical results mentioned above. Therefore, to address these\nlimitations, this paper introduces a more recent advancement known as\n\\emph{local differential privacy (LDP)} to enhance the privacy of MSFE. We\nprovide some rigorous definitions about LDP based on the intrinsic properties\nof MSFE rather than directly presenting the assumptions under CDP.\nSubsequently, the LDP is proved to be realized with system intrinsic\nrandomness, which is useful and has never been considered before. Furthermore,\nthe Gaussian mechanism is designed when the intrinsic randomness is\ninsufficient. The lower bound of the covariance for extra injected Gaussian\nnoises is determined by integrating system information with privacy budgets.\nMoreover, the optimal fusion estimators under intrinsic and extra disturbances\nare respectively designed in the linear minimum variance sense. Finally, the\neffectiveness of the proposed methods is verified through numerical\nsimulations, encompassing both one-dimensional and high-dimensional scenarios.", "AI": {"tldr": "本文针对多传感器融合估计（MSFE）中的隐私保护问题，引入局部差分隐私（LDP）以克服传统集中式差分隐私（CDP）的局限性，并利用系统固有随机性和高斯机制实现LDP，设计最优融合估计器。", "motivation": "传统集中式差分隐私（CDP）主要保护融合中心的统计数据而非传感器个体数据，且适用于大规模系统，因此不适合多传感器融合估计（MSFE）中保护传感器个体隐私的需求。", "method": "本文引入局部差分隐私（LDP）来增强MSFE的隐私保护。基于MSFE的固有特性定义LDP，并证明LDP可以通过系统固有随机性实现。当固有随机性不足时，设计高斯机制，并结合系统信息和隐私预算确定注入高斯噪声协方差的下限。最后，在线性最小方差意义下，分别设计了固有扰动和额外扰动下的最优融合估计器。", "result": "LDP被证明可以通过系统固有随机性实现，这是一个新颖的发现。确定了额外注入高斯噪声协方差的下限。设计了最优融合估计器，并通过数值模拟（包括一维和高维场景）验证了所提方法的有效性。", "conclusion": "本文成功将局部差分隐私应用于多传感器融合估计，克服了传统集中式差分隐私的局限性，并通过利用系统固有随机性和设计高斯机制，实现了有效的隐私保护和最优估计，并通过仿真验证了其有效性。"}}
{"id": "2508.15002", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.15002", "abs": "https://arxiv.org/abs/2508.15002", "authors": ["René Zurbrügg", "Andrei Cramariuc", "Marco Hutter"], "title": "GraspQP: Differentiable Optimization of Force Closure for Diverse and Robust Dexterous Grasping", "comment": null, "summary": "Dexterous robotic hands enable versatile interactions due to the flexibility\nand adaptability of multi-fingered designs, allowing for a wide range of\ntask-specific grasp configurations in diverse environments. However, to fully\nexploit the capabilities of dexterous hands, access to diverse and high-quality\ngrasp data is essential -- whether for developing grasp prediction models from\npoint clouds, training manipulation policies, or supporting high-level task\nplanning with broader action options. Existing approaches for dataset\ngeneration typically rely on sampling-based algorithms or simplified\nforce-closure analysis, which tend to converge to power grasps and often\nexhibit limited diversity. In this work, we propose a method to synthesize\nlarge-scale, diverse, and physically feasible grasps that extend beyond simple\npower grasps to include refined manipulations, such as pinches and tri-finger\nprecision grasps. We introduce a rigorous, differentiable energy formulation of\nforce closure, implicitly defined through a Quadratic Program (QP).\nAdditionally, we present an adjusted optimization method (MALA*) that improves\nperformance by dynamically rejecting gradient steps based on the distribution\nof energy values across all samples. We extensively evaluate our approach and\ndemonstrate significant improvements in both grasp diversity and the stability\nof final grasp predictions. Finally, we provide a new, large-scale grasp\ndataset for 5,700 objects from DexGraspNet, comprising five different grippers\nand three distinct grasp types.\n  Dataset and Code:https://graspqp.github.io/", "AI": {"tldr": "本文提出了一种通过可微分力闭合能量公式和改进优化方法（MALA*）来合成大规模、多样化且物理可行的抓取数据的方法，显著提高了抓取多样性和稳定性，并提供了一个包含多种抓取类型和夹持器的新型大型数据集。", "motivation": "多指灵巧手需要多样化、高质量的抓取数据来充分发挥其能力，例如用于抓取预测模型、训练操作策略和支持高级任务规划。然而，现有数据集生成方法（基于采样或简化力闭合分析）通常缺乏多样性，倾向于生成力量抓取。", "method": "该研究提出了一种方法来合成大规模、多样化且物理可行的抓取，包括捏取和三指精确抓取等。核心方法包括：1) 引入一个通过二次规划（QP）隐式定义的严格、可微分的力闭合能量公式。2) 提出了一种调整后的优化方法（MALA*），通过根据所有样本的能量值分布动态拒绝梯度步来提高性能。", "result": "该方法在抓取多样性和最终抓取预测的稳定性方面均显示出显著改进。此外，研究还为来自DexGraspNet的5,700个物体提供了一个新的大规模抓取数据集，该数据集包含五种不同的夹持器和三种独特的抓取类型。", "conclusion": "该工作成功提出了一种能够生成多样化、物理可行抓取数据的方法，解决了现有数据集生成方法的局限性，并为机器人操作领域提供了高质量、大规模的抓取数据集，有助于推动灵巧手抓取预测和操作策略的发展。"}}
{"id": "2508.14932", "categories": ["eess.IV", "cs.AI", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2508.14932", "abs": "https://arxiv.org/abs/2508.14932", "authors": ["Jiacheng Xie", "Ziyang Zhang", "Biplab Poudel", "Congyu Guo", "Yang Yu", "Guanghui An", "Xiaoting Tang", "Lening Zhao", "Chunhui Xu", "Dong Xu"], "title": "TOM: An Open-Source Tongue Segmentation Method with Multi-Teacher Distillation and Task-Specific Data Augmentation", "comment": "Tongue segmentation, data augmentation, synthetic data for AI\n  training, prompt engineering, Segment Anything Model, knowledge distillation,\n  tongue classification", "summary": "Tongue imaging serves as a valuable diagnostic tool, particularly in\nTraditional Chinese Medicine (TCM). The quality of tongue surface segmentation\nsignificantly affects the accuracy of tongue image classification and\nsubsequent diagnosis in intelligent tongue diagnosis systems. However, existing\nresearch on tongue image segmentation faces notable limitations, and there is a\nlack of robust and user-friendly segmentation tools. This paper proposes a\ntongue image segmentation model (TOM) based on multi-teacher knowledge\ndistillation. By incorporating a novel diffusion-based data augmentation\nmethod, we enhanced the generalization ability of the segmentation model while\nreducing its parameter size. Notably, after reducing the parameter count by\n96.6% compared to the teacher models, the student model still achieves an\nimpressive segmentation performance of 95.22% mIoU. Furthermore, we packaged\nand deployed the trained model as both an online and offline segmentation tool\n(available at https://itongue.cn/), allowing TCM practitioners and researchers\nto use it without any programming experience. We also present a case study on\nTCM constitution classification using segmented tongue patches. Experimental\nresults demonstrate that training with tongue patches yields higher\nclassification performance and better interpretability than original tongue\nimages. To our knowledge, this is the first open-source and freely available\ntongue image segmentation tool.", "AI": {"tldr": "本文提出了一种基于多教师知识蒸馏的舌像分割模型（TOM），结合扩散数据增强，显著减少了模型参数并提升了泛化能力。作者还将训练好的模型打包成易于使用的在线和离线工具，并证明了分割后的舌像补丁在体质分类中的优越性。", "motivation": "舌像分割质量对智能舌诊系统的诊断准确性至关重要，但现有舌像分割研究存在显著局限性，并且缺乏鲁棒且用户友好的分割工具。", "method": "本文提出了一种基于多教师知识蒸馏的舌像分割模型（TOM）。通过引入新颖的基于扩散的数据增强方法，增强了分割模型的泛化能力，同时减小了参数规模。此外，训练后的模型被打包并部署为在线和离线分割工具，并进行了中医体质分类的案例研究。", "result": "与教师模型相比，学生模型在参数量减少96.6%的情况下，仍达到了95.22%的mIoU分割性能。使用分割后的舌像补丁进行训练，在体质分类中取得了比原始舌像更高的分类性能和更好的可解释性。这是首个开源且免费的舌像分割工具。", "conclusion": "本文成功开发了一个高效、鲁棒且用户友好的舌像分割模型（TOM）及其工具，显著提升了智能舌诊系统的基础能力。分割后的舌像补丁在TCM体质分类中表现出优越性，为TCM从业者和研究人员提供了宝贵资源。"}}
{"id": "2508.14909", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.14909", "abs": "https://arxiv.org/abs/2508.14909", "authors": ["Tom Kocmi", "Eleftherios Avramidis", "Rachel Bawden", "Ondřej Bojar", "Konstantin Dranch", "Anton Dvorkovich", "Sergey Dukanov", "Natalia Fedorova", "Mark Fishel", "Markus Freitag", "Thamme Gowda", "Roman Grundkiewicz", "Barry Haddow", "Marzena Karpinska", "Philipp Koehn", "Howard Lakougna", "Jessica Lundin", "Kenton Murray", "Masaaki Nagata", "Stefano Perrella", "Lorenzo Proietti", "Martin Popel", "Maja Popović", "Parker Riley", "Mariya Shmatova", "Steinþór Steingrímsson", "Lisa Yankovskaya", "Vilém Zouhar"], "title": "Preliminary Ranking of WMT25 General Machine Translation Systems", "comment": null, "summary": "We present the preliminary ranking of the WMT25 General Machine Translation\nShared Task, in which MT systems have been evaluated using automatic metrics.\nAs this ranking is based on automatic evaluations, it may be biased in favor of\nsystems that employ re-ranking techniques, such as Quality Estimation\nre-ranking or Minimum Bayes Risk decoding. The official WMT25 ranking will be\nbased on human evaluation, which is more reliable and will supersede the\nautomatic ranking.\n  The purpose of this report is not to present the final findings of the\nGeneral MT task, but rather to share preliminary results with task\nparticipants, which may be useful when preparing their system submission\npapers.", "AI": {"tldr": "WMT25通用机器翻译共享任务的初步排名已发布，该排名基于自动评估指标。", "motivation": "向任务参与者分享初步结果，以帮助他们准备系统提交论文，并指出自动评估的局限性。", "method": "使用自动指标评估机器翻译系统。", "result": "WMT25通用机器翻译共享任务的初步系统排名。", "conclusion": "该排名是初步的、基于自动评估且可能存在偏差，最终官方排名将以更可靠的人工评估为准。本报告旨在提供参考，而非最终结果。"}}
{"id": "2508.14958", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.14958", "abs": "https://arxiv.org/abs/2508.14958", "authors": ["Mustafa Mohammadi Gharasuie", "Luis Rueda"], "title": "Fast Graph Neural Network for Image Classification", "comment": "12 pages, proceeding into CanadianAI 2025", "summary": "The rapid progress in image classification has been largely driven by the\nadoption of Graph Convolutional Networks (GCNs), which offer a robust framework\nfor handling complex data structures. This study introduces a novel approach\nthat integrates GCNs with Voronoi diagrams to enhance image classification by\nleveraging their ability to effectively model relational data. Unlike\nconventional convolutional neural networks (CNNs), our method represents images\nas graphs, where pixels or regions function as vertices. These graphs are then\nrefined using corresponding Delaunay triangulations, optimizing their\nrepresentation. The proposed model achieves significant improvements in both\npreprocessing efficiency and classification accuracy across various benchmark\ndatasets, surpassing state-of-the-art approaches, particularly in challenging\nscenarios involving intricate scenes and fine-grained categories. Experimental\nresults, validated through cross-validation, underscore the effectiveness of\ncombining GCNs with Voronoi diagrams for advancing image classification. This\nresearch not only presents a novel perspective on image classification but also\nexpands the potential applications of graph-based learning paradigms in\ncomputer vision and unstructured data analysis.", "AI": {"tldr": "本研究提出了一种结合图卷积网络（GCNs）和Voronoi图的新方法，通过将图像表示为图并利用Delaunay三角剖分进行优化，显著提高了图像分类的预处理效率和分类准确性。", "motivation": "图像分类的快速发展得益于GCNs在处理复杂数据结构方面的强大能力。研究旨在通过整合GCNs和Voronoi图，利用其建模关系数据的能力，进一步提升图像分类性能，超越传统CNN的局限性。", "method": "该方法将图像表示为图（像素或区域作为顶点），然后利用Voronoi图和Delaunay三角剖分对这些图进行优化。在此基础上，应用图卷积网络进行图像分类。与传统卷积神经网络（CNNs）不同，本方法着重于图结构表示。", "result": "该模型在多个基准数据集上显著提升了预处理效率和分类准确性，超越了现有最先进的方法，尤其在处理复杂场景和细粒度类别时表现突出。实验结果通过交叉验证得到了证实。", "conclusion": "结合GCNs和Voronoi图的方法能有效推进图像分类技术。这项研究不仅为图像分类提供了新视角，也拓宽了图基学习范式在计算机视觉和非结构化数据分析领域的应用潜力。"}}
{"id": "2508.15013", "categories": ["cs.AI", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2508.15013", "abs": "https://arxiv.org/abs/2508.15013", "authors": ["Nadav Amir", "Stas Tiomkin", "Angela Langdon"], "title": "Goals and the Structure of Experience", "comment": null, "summary": "Purposeful behavior is a hallmark of natural and artificial intelligence. Its\nacquisition is often believed to rely on world models, comprising both\ndescriptive (what is) and prescriptive (what is desirable) aspects that\nidentify and evaluate state of affairs in the world, respectively. Canonical\ncomputational accounts of purposeful behavior, such as reinforcement learning,\nposit distinct components of a world model comprising a state representation\n(descriptive aspect) and a reward function (prescriptive aspect). However, an\nalternative possibility, which has not yet been computationally formulated, is\nthat these two aspects instead co-emerge interdependently from an agent's goal.\nHere, we describe a computational framework of goal-directed state\nrepresentation in cognitive agents, in which the descriptive and prescriptive\naspects of a world model co-emerge from agent-environment interaction\nsequences, or experiences. Drawing on Buddhist epistemology, we introduce a\nconstruct of goal-directed, or telic, states, defined as classes of\ngoal-equivalent experience distributions. Telic states provide a parsimonious\naccount of goal-directed learning in terms of the statistical divergence\nbetween behavioral policies and desirable experience features. We review\nempirical and theoretical literature supporting this novel perspective and\ndiscuss its potential to provide a unified account of behavioral,\nphenomenological and neural dimensions of purposeful behaviors across diverse\nsubstrates.", "AI": {"tldr": "本文提出了一种计算框架，认为世界模型的描述性（状态表示）和规范性（奖励函数）方面可以从智能体的目标和经验中协同涌现，并引入了“目的性状态”的概念。", "motivation": "现有计算方法（如强化学习）将世界模型的描述性和规范性方面视为独立组件。本文旨在探索一种替代可能性，即这两个方面可以从智能体的目标中相互依存地协同涌现，而这种可能性尚未被计算化地阐述。", "method": "本文描述了一个认知智能体中目标导向状态表示的计算框架。借鉴佛教认识论，引入了“目的性状态”的概念，将其定义为目标等效经验分布的类别。通过行为策略与期望经验特征之间的统计差异来解释目标导向学习。", "result": "该框架提供了一种简洁的目标导向学习解释。文章回顾了支持这一新观点的经验和理论文献。", "conclusion": "该框架有望为跨不同基质的目的性行为在行为、现象学和神经维度上提供统一的解释。"}}
{"id": "2508.15517", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.15517", "abs": "https://arxiv.org/abs/2508.15517", "authors": ["Philip Bilfinger", "Markus Schreiber", "Philipp Rosner", "Kareem Abo Gamra", "Jan Schöberl", "Cristina Grosu", "Markus Lienkamp"], "title": "Why we need a standardized state of health definition for electric vehicle battery packs -- a proposal for energy- and capacity-based metrics", "comment": "20 pages, 4 figures, 2 tables,", "summary": "Range and performance are key customer-relevant properties of electric\nvehicles. Both degrade over time due to battery aging, thus impacting business\ndecisions throughout a vehicle's lifecycle, such as efficient utilization and\nasset valuation. For practical assessment, aging is often simplified into a\nsingle figure of merit - the state of health - typically defined by the battery\npack's remaining capacity or energy. However, no standardized method for\nmeasuring the state of health at the vehicle level has been established,\nleaving both academia and industry without a clear consensus. Ultimately,\nstandardization is crucial to increase transparency and build confidence in the\nlong-term reliability of electric vehicles' battery packs. In this article, we\npropose a standard measurement procedure for assessing the capacity- and\nenergy-based state of health, leveraging onboard charging to enable\nreproducibility and scalability. Additionally, we demonstrate how differential\nvoltage analysis can provide deeper insights into battery aging at the vehicle\nlevel.", "AI": {"tldr": "本文提出了一种利用车载充电的标准化测量程序，用于评估电动汽车电池的容量和能量健康状态（SOH），并通过差分电压分析提供更深入的老化洞察。", "motivation": "电动汽车的续航里程和性能会因电池老化而下降，影响车辆整个生命周期的商业决策。目前，车级电池健康状态（SOH）的测量缺乏标准化方法，导致学术界和工业界没有明确共识，这阻碍了透明度和对电动汽车电池长期可靠性的信心。", "method": "提出了一种基于车载充电的标准化测量程序，用于评估容量和能量的健康状态，以实现可重复性和可扩展性。此外，还展示了差分电压分析（DVA）如何提供车辆层面电池老化的更深层见解。", "result": "本文提出了一种用于评估电动汽车电池健康状态（SOH）的标准化测量程序，并展示了差分电压分析（DVA）在提供电池老化深层见解方面的潜力。", "conclusion": "标准化对于提高透明度和建立对电动汽车电池包长期可靠性的信心至关重要。本文提出的标准化测量程序和差分电压分析方法有助于解决这一需求，并提供对电池老化的更深入理解。"}}
{"id": "2508.15021", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.15021", "abs": "https://arxiv.org/abs/2508.15021", "authors": ["Mark Van der Merwe", "Devesh Jha"], "title": "In-Context Iterative Policy Improvement for Dynamic Manipulation", "comment": "14 pages. Accepted at CoRL 2025", "summary": "Attention-based architectures trained on internet-scale language data have\ndemonstrated state of the art reasoning ability for various language-based\ntasks, such as logic problems and textual reasoning. Additionally, these Large\nLanguage Models (LLMs) have exhibited the ability to perform few-shot\nprediction via in-context learning, in which input-output examples provided in\nthe prompt are generalized to new inputs. This ability furthermore extends\nbeyond standard language tasks, enabling few-shot learning for general\npatterns. In this work, we consider the application of in-context learning with\npre-trained language models for dynamic manipulation. Dynamic manipulation\nintroduces several crucial challenges, including increased dimensionality,\ncomplex dynamics, and partial observability. To address this, we take an\niterative approach, and formulate our in-context learning problem to predict\nadjustments to a parametric policy based on previous interactions. We show\nacross several tasks in simulation and on a physical robot that utilizing\nin-context learning outperforms alternative methods in the low data regime.\nVideo summary of this work and experiments can be found\nhttps://youtu.be/2inxpdrq74U?si=dAdDYsUEr25nZvRn.", "AI": {"tldr": "本文将大型语言模型（LLMs）的上下文学习能力应用于动态操作任务，通过预测参数策略的调整，在低数据环境下表现优于其他方法。", "motivation": "大型语言模型（LLMs）在语言任务和一般模式识别方面展现出卓越的推理能力和少样本上下文学习能力。研究人员希望探索这种能力是否能扩展到动态操作领域，尽管该领域存在维度高、动力学复杂和部分可观测性等挑战。", "method": "研究采用迭代方法，将上下文学习问题表述为根据先前交互预测对参数化策略的调整。这使得模型能够逐步优化操作策略。", "result": "在模拟环境和物理机器人上的多项任务中，该方法在低数据状态下，利用上下文学习的表现优于其他替代方法。", "conclusion": "将预训练语言模型的上下文学习应用于动态操作是有效的，尤其是在数据稀缺的情况下。通过迭代地预测策略调整，LLMs能够成功应对动态操作的复杂挑战。"}}
{"id": "2508.14950", "categories": ["eess.IV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.14950", "abs": "https://arxiv.org/abs/2508.14950", "authors": ["Oliver Welin Odeback", "Arivazhagan Geetha Balasubramanian", "Jonas Schollenberger", "Edward Ferdiand", "Alistair A. Young", "C. Alberto Figueroa", "Susanne Schnell", "Outi Tammisola", "Ricardo Vinuesa", "Tobias Granberg", "Alexander Fyrdahl", "David Marlevi"], "title": "Potential and challenges of generative adversarial networks for super-resolution in 4D Flow MRI", "comment": "23 pages, 9 figures", "summary": "4D Flow Magnetic Resonance Imaging (4D Flow MRI) enables non-invasive\nquantification of blood flow and hemodynamic parameters. However, its clinical\napplication is limited by low spatial resolution and noise, particularly\naffecting near-wall velocity measurements. Machine learning-based\nsuper-resolution has shown promise in addressing these limitations, but\nchallenges remain, not least in recovering near-wall velocities. Generative\nadversarial networks (GANs) offer a compelling solution, having demonstrated\nstrong capabilities in restoring sharp boundaries in non-medical\nsuper-resolution tasks. Yet, their application in 4D Flow MRI remains\nunexplored, with implementation challenged by known issues such as training\ninstability and non-convergence. In this study, we investigate GAN-based\nsuper-resolution in 4D Flow MRI. Training and validation were conducted using\npatient-specific cerebrovascular in-silico models, converted into synthetic\nimages via an MR-true reconstruction pipeline. A dedicated GAN architecture was\nimplemented and evaluated across three adversarial loss functions: Vanilla,\nRelativistic, and Wasserstein. Our results demonstrate that the proposed GAN\nimproved near-wall velocity recovery compared to a non-adversarial reference\n(vNRMSE: 6.9% vs. 9.6%); however, that implementation specifics are critical\nfor stable network training. While Vanilla and Relativistic GANs proved\nunstable compared to generator-only training (vNRMSE: 8.1% and 7.8% vs. 7.2%),\na Wasserstein GAN demonstrated optimal stability and incremental improvement\n(vNRMSE: 6.9% vs. 7.2%). The Wasserstein GAN further outperformed the\ngenerator-only baseline at low SNR (vNRMSE: 8.7% vs. 10.7%). These findings\nhighlight the potential of GAN-based super-resolution in enhancing 4D Flow MRI,\nparticularly in challenging cerebrovascular regions, while emphasizing the need\nfor careful selection of adversarial strategies.", "AI": {"tldr": "本研究探索了基于生成对抗网络（GAN）的超分辨率技术在4D流动磁共振成像（4D Flow MRI）中的应用，旨在改善近壁速度测量。结果表明，精心选择的GAN（特别是Wasserstein GAN）能够有效提高近壁速度恢复的准确性和稳定性。", "motivation": "4D Flow MRI的临床应用受限于低空间分辨率和噪声，尤其影响近壁速度测量。机器学习超分辨率技术虽有前景，但在近壁速度恢复方面仍面临挑战。GAN在非医学超分辨率任务中展现出强大的锐利边界恢复能力，但在4D Flow MRI中尚未被探索，且其训练不稳定和不收敛等问题限制了应用。", "method": "研究采用基于GAN的超分辨率方法处理4D Flow MRI数据。训练和验证使用了患者特异性脑血管体内（in-silico）模型，通过MR真实重建管道转换为合成图像。实现了一个专门的GAN架构，并评估了三种对抗性损失函数：Vanilla、Relativistic和Wasserstein。通过与非对抗性参考（仅生成器）进行比较来评估性能。", "result": "所提出的GAN在近壁速度恢复方面优于非对抗性参考（vNRMSE: 6.9% vs. 9.6%）。实现细节对网络训练稳定性至关重要。Vanilla和Relativistic GANs相较于仅生成器训练表现不稳定（vNRMSE: 8.1%和7.8% vs. 7.2%），而Wasserstein GAN展现出最佳稳定性和渐进式改进（vNRMSE: 6.9% vs. 7.2%）。此外，在低信噪比（SNR）下，Wasserstein GAN优于仅生成器基线（vNRMSE: 8.7% vs. 10.7%）。", "conclusion": "研究结果强调了基于GAN的超分辨率技术在增强4D Flow MRI，尤其是在具有挑战性的脑血管区域的潜力，并强调了仔细选择对抗性策略（如Wasserstein GAN）的重要性，以确保稳定的网络训练和优越的性能。"}}
{"id": "2508.14913", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.14913", "abs": "https://arxiv.org/abs/2508.14913", "authors": ["Israel Abebe Azime", "Tadesse Destaw Belay", "Dietrich Klakow", "Philipp Slusallek", "Anshuman Chhabra"], "title": "Bridging the Culture Gap: A Framework for LLM-Driven Socio-Cultural Localization of Math Word Problems in Low-Resource Languages", "comment": null, "summary": "Large language models (LLMs) have demonstrated significant capabilities in\nsolving mathematical problems expressed in natural language. However,\nmultilingual and culturally-grounded mathematical reasoning in low-resource\nlanguages lags behind English due to the scarcity of socio-cultural task\ndatasets that reflect accurate native entities such as person names,\norganization names, and currencies. Existing multilingual benchmarks are\npredominantly produced via translation and typically retain English-centric\nentities, owing to the high cost associated with human annotater-based\nlocalization. Moreover, automated localization tools are limited, and hence,\ntruly localized datasets remain scarce. To bridge this gap, we introduce a\nframework for LLM-driven cultural localization of math word problems that\nautomatically constructs datasets with native names, organizations, and\ncurrencies from existing sources. We find that translated benchmarks can\nobscure true multilingual math ability under appropriate socio-cultural\ncontexts. Through extensive experiments, we also show that our framework can\nhelp mitigate English-centric entity bias and improves robustness when native\nentities are introduced across various languages.", "AI": {"tldr": "本文提出了一种由大型语言模型驱动的文化本地化框架，用于数学应用题，旨在自动构建包含本地实体（如人名、组织、货币）的数据集，以解决低资源语言中多语言和文化背景数学推理的不足。", "motivation": "尽管大型语言模型在英语数学问题解决方面表现出色，但由于缺乏反映准确本地实体（如人名、组织名、货币）的社会文化任务数据集，低资源语言中的多语言和文化背景数学推理能力滞后。现有的多语言基准测试主要通过翻译产生，并倾向于保留以英语为中心的实体，而人工标注的本地化成本高昂，自动化工具也有限，导致真正本地化的数据集稀缺。", "method": "本文引入了一个由大型语言模型（LLM）驱动的数学应用题文化本地化框架。该框架能够从现有来源自动构建包含本地人名、组织名和货币的数据集。", "result": "研究发现，翻译的基准测试在适当的社会文化背景下可能会掩盖真正的多语言数学能力。通过广泛的实验，本文还表明所提出的框架有助于减轻以英语为中心的实体偏见，并在各种语言中引入本地实体时提高模型的鲁棒性。", "conclusion": "本研究通过引入LLM驱动的文化本地化框架，有效解决了低资源语言中缺乏文化背景数学推理数据集的问题。该框架不仅能够自动生成包含本地实体的数据集，还能减轻英语中心偏见，提高LLM在多语言数学问题解决中的鲁棒性和准确性。"}}
{"id": "2508.14965", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.14965", "abs": "https://arxiv.org/abs/2508.14965", "authors": ["Hakjin Lee", "Junghoon Seo", "Jaehoon Sim"], "title": "You Only Pose Once: A Minimalist's Detection Transformer for Monocular RGB Category-level 9D Multi-Object Pose Estimation", "comment": "https://mikigom.github.io/YOPO-project-page", "summary": "Accurately recovering the full 9-DoF pose of unseen instances within specific\ncategories from a single RGB image remains a core challenge for robotics and\nautomation. Most existing solutions still rely on pseudo-depth, CAD models, or\nmulti-stage cascades that separate 2D detection from pose estimation. Motivated\nby the need for a simpler, RGB-only alternative that learns directly at the\ncategory level, we revisit a longstanding question: Can object detection and\n9-DoF pose estimation be unified with high performance, without any additional\ndata? We show that they can with our method, YOPO, a single-stage, query-based\nframework that treats category-level 9-DoF estimation as a natural extension of\n2D detection. YOPO augments a transformer detector with a lightweight pose\nhead, a bounding-box-conditioned translation module, and a 6D-aware Hungarian\nmatching cost. The model is trained end-to-end only with RGB images and\ncategory-level pose labels. Despite its minimalist design, YOPO sets a new\nstate of the art on three benchmarks. On the REAL275 dataset, it achieves 79.6%\n$\\rm{IoU}_{50}$ and 54.1% under the $10^\\circ$$10{\\rm{cm}}$ metric, surpassing\nprior RGB-only methods and closing much of the gap to RGB-D systems. The code,\nmodels, and additional qualitative results can be found on our project.", "AI": {"tldr": "本文提出了YOPO，一个单阶段、基于查询的框架，将2D目标检测与类别级9自由度姿态估计统一起来，仅使用RGB图像进行端到端训练，并在三个基准测试中达到了新的最先进水平。", "motivation": "现有解决方案依赖伪深度、CAD模型或多阶段级联，将2D检测与姿态估计分离。研究动机是寻找一种更简单、仅依赖RGB、直接在类别级别学习的替代方案，并探讨是否能在不额外数据的情况下，以高性能统一目标检测和9自由度姿态估计。", "method": "YOPO是一个单阶段、基于查询的框架，将类别级9自由度估计视为2D检测的自然延伸。它通过轻量级姿态头部、边界框条件平移模块和6D感知匈牙利匹配成本来增强Transformer检测器。模型仅使用RGB图像和类别级姿态标签进行端到端训练。", "result": "YOPO在三个基准测试中创造了新的最先进水平。在REAL275数据集上，它在$\\rm{IoU}_{50}$下达到79.6%，在$10^\\circ 10\\rm{cm}$指标下达到54.1%，超越了先前的纯RGB方法，并大大缩小了与RGB-D系统之间的差距。", "conclusion": "研究表明，通过一个极简主义的单阶段设计，可以在不使用额外数据的情况下，以高性能统一目标检测和9自由度姿态估计，仅依赖RGB图像。"}}
{"id": "2508.15030", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15030", "abs": "https://arxiv.org/abs/2508.15030", "authors": ["Ashmi Banerjee", "Fitri Nur Aisyah", "Adithi Satish", "Wolfgang Wörndl", "Yashar Deldjoo"], "title": "Collab-REC: An LLM-based Agentic Framework for Balancing Recommendations in Tourism", "comment": null, "summary": "We propose Collab-REC, a multi-agent framework designed to counteract\npopularity bias and enhance diversity in tourism recommendations. In our\nsetting, three LLM-based agents -- Personalization, Popularity, and\nSustainability generate city suggestions from complementary perspectives. A\nnon-LLM moderator then merges and refines these proposals via multi-round\nnegotiation, ensuring each agent's viewpoint is incorporated while penalizing\nspurious or repeated responses. Experiments on European city queries show that\nCollab-REC improves diversity and overall relevance compared to a single-agent\nbaseline, surfacing lesser-visited locales that often remain overlooked. This\nbalanced, context-aware approach addresses over-tourism and better aligns with\nconstraints provided by the user, highlighting the promise of multi-stakeholder\ncollaboration in LLM-driven recommender systems.", "AI": {"tldr": "Collab-REC是一个多智能体框架，通过三个LLM智能体和一个非LLM协调员的多轮协商，解决旅游推荐中的流行度偏差，提升推荐多样性和相关性。", "motivation": "现有推荐系统存在流行度偏差，导致推荐结果缺乏多样性，使得热门地点过度旅游而冷门地点被忽视，且难以充分整合用户约束。", "method": "该方法提出Collab-REC框架，包含三个基于LLM的智能体（个性化、流行度和可持续性）从不同视角生成城市建议。一个非LLM协调员通过多轮协商合并和优化这些建议，确保每个智能体的观点被纳入，同时惩罚重复或虚假响应。", "result": "在欧洲城市查询上的实验表明，Collab-REC相比单智能体基线，显著提升了推荐的多样性和整体相关性，成功推荐了许多常被忽视的、访问量较少的地点。", "conclusion": "Collab-REC提供了一种平衡且情境感知的方法，有效解决了过度旅游问题，并更好地满足了用户提供的约束，展示了LLM驱动推荐系统中多方协作的巨大潜力。"}}
{"id": "2508.15543", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.15543", "abs": "https://arxiv.org/abs/2508.15543", "authors": ["Mahdi Nazeri", "Thom Badings", "Anne-Kathrin Schmuck", "Sadegh Soudjani", "Alessandro Abate"], "title": "Data-Driven Abstraction and Synthesis for Stochastic Systems with Unknown Dynamics", "comment": null, "summary": "We study the automated abstraction-based synthesis of correct-by-construction\ncontrol policies for stochastic dynamical systems with unknown dynamics. Our\napproach is to learn an abstraction from sampled data, which is represented in\nthe form of a finite Markov decision process (MDP). In this paper, we present a\ndata-driven technique for constructing finite-state interval MDP (IMDP)\nabstractions of stochastic systems with unknown nonlinear dynamics. As a\ndistinguishing and novel feature, our technique only requires (1) noisy\nstate-input-state observations and (2) an upper bound on the system's Lipschitz\nconstant. Combined with standard model-checking techniques, our IMDP\nabstractions enable the synthesis of policies that satisfy probabilistic\ntemporal properties (such as \"reach-while-avoid\") with a predefined confidence.\nOur experimental results show the effectiveness and robustness of our approach.", "AI": {"tldr": "本文提出了一种数据驱动的方法，用于从噪声观测数据和系统Lipschitz常数上界中，构建具有未知非线性动力学的随机系统的有限状态区间马尔可夫决策过程（IMDP）抽象，以实现正确构造的控制策略综合。", "motivation": "研究的动机是为具有未知动力学的随机动态系统，自动合成正确构造的控制策略。", "method": "该方法通过学习从采样数据中提取的抽象，将其表示为有限马尔可夫决策过程（MDP）的形式。具体来说，它提出了一种数据驱动技术，仅需要噪声的状态-输入-状态观测和系统Lipschitz常数的上界，来构建有限状态区间MDP（IMDP）抽象。然后结合标准模型检验技术，合成满足预定义置信度的概率时序属性（如“到达-避免”）的策略。", "result": "实验结果表明，该方法有效且鲁棒，能够合成满足概率时序属性（如“到达-避免”）且具有预定义置信度的策略。", "conclusion": "该研究提供了一种新颖的数据驱动技术，能够从噪声观测数据和Lipschitz常数上界中，为未知非线性随机系统构建IMDP抽象，从而实现具有预定义置信度的正确构造控制策略合成。"}}
{"id": "2508.15038", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.MA", "I.2.9"], "pdf": "https://arxiv.org/pdf/2508.15038", "abs": "https://arxiv.org/abs/2508.15038", "authors": ["Makram Chahine", "William Yang", "Alaa Maalouf", "Justin Siriska", "Ninad Jadhav", "Daniel Vogt", "Stephanie Gil", "Robert Wood", "Daniela Rus"], "title": "Decentralized Vision-Based Autonomous Aerial Wildlife Monitoring", "comment": null, "summary": "Wildlife field operations demand efficient parallel deployment methods to\nidentify and interact with specific individuals, enabling simultaneous\ncollective behavioral analysis, and health and safety interventions. Previous\nrobotics solutions approach the problem from the herd perspective, or are\nmanually operated and limited in scale. We propose a decentralized vision-based\nmulti-quadrotor system for wildlife monitoring that is scalable, low-bandwidth,\nand sensor-minimal (single onboard RGB camera). Our approach enables robust\nidentification and tracking of large species in their natural habitat. We\ndevelop novel vision-based coordination and tracking algorithms designed for\ndynamic, unstructured environments without reliance on centralized\ncommunication or control. We validate our system through real-world\nexperiments, demonstrating reliable deployment in diverse field conditions.", "AI": {"tldr": "该研究提出了一种去中心化、基于视觉的多旋翼无人机系统，用于大规模野生动物监测，通过单目RGB相机实现个体识别和跟踪，无需集中控制或高带宽。", "motivation": "野生动物实地操作需要高效的并行部署方法来识别和跟踪个体，以进行行为分析和干预。现有机器人解决方案要么以群体为中心，要么手动操作且规模有限。", "method": "开发了一种去中心化、基于视觉的多旋翼无人机系统，该系统具有可扩展性、低带宽和最少传感器（单个机载RGB相机）的特点。设计了新颖的基于视觉的协调和跟踪算法，适用于动态、非结构化环境，不依赖集中通信或控制。", "result": "该系统能够在自然栖息地中对大型物种进行鲁棒的识别和跟踪。通过真实世界的实验验证了系统的可靠性，并在各种野外条件下展示了可靠的部署能力。", "conclusion": "所提出的去中心化、基于视觉的多旋翼无人机系统为野生动物监测提供了一种可扩展、鲁棒且高效的解决方案，克服了现有方法的局限性。"}}
{"id": "2508.14952", "categories": ["eess.IV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.14952", "abs": "https://arxiv.org/abs/2508.14952", "authors": ["Paul Fischer", "Jan Nikolas Morshuis", "Thomas Küstner", "Christian Baumgartner"], "title": "CUTE-MRI: Conformalized Uncertainty-based framework for Time-adaptivE MRI", "comment": null, "summary": "Magnetic Resonance Imaging (MRI) offers unparalleled soft-tissue contrast but\nis fundamentally limited by long acquisition times. While deep learning-based\naccelerated MRI can dramatically shorten scan times, the reconstruction from\nundersampled data introduces ambiguity resulting from an ill-posed problem with\ninfinitely many possible solutions that propagates to downstream clinical\ntasks. This uncertainty is usually ignored during the acquisition process as\nacceleration factors are often fixed a priori, resulting in scans that are\neither unnecessarily long or of insufficient quality for a given clinical\nendpoint. This work introduces a dynamic, uncertainty-aware acquisition\nframework that adjusts scan time on a per-subject basis. Our method leverages a\nprobabilistic reconstruction model to estimate image uncertainty, which is then\npropagated through a full analysis pipeline to a quantitative metric of\ninterest (e.g., patellar cartilage volume or cardiac ejection fraction). We use\nconformal prediction to transform this uncertainty into a rigorous, calibrated\nconfidence interval for the metric. During acquisition, the system iteratively\nsamples k-space, updates the reconstruction, and evaluates the confidence\ninterval. The scan terminates automatically once the uncertainty meets a\nuser-predefined precision target. We validate our framework on both knee and\ncardiac MRI datasets. Our results demonstrate that this adaptive approach\nreduces scan times compared to fixed protocols while providing formal\nstatistical guarantees on the precision of the final image. This framework\nmoves beyond fixed acceleration factors, enabling patient-specific acquisitions\nthat balance scan efficiency with diagnostic confidence, a critical step\ntowards personalized and resource-efficient MRI.", "AI": {"tldr": "该研究提出了一种动态、不确定性感知的MRI采集框架，通过迭代采样和不确定性评估，根据预设的精度目标自动调整扫描时间，从而实现患者个性化的、高效且具有统计学保证的MRI扫描。", "motivation": "MRI扫描时间长是其主要限制，而基于深度学习的加速方法虽然能缩短时间，但重建过程中的不确定性（病态问题）会影响后续临床任务。传统的固定加速因子协议要么扫描时间过长，要么图像质量不足以满足临床需求，无法兼顾效率和质量。", "method": "该方法利用概率重建模型估计图像不确定性，并将其传播到下游分析管线，以量化特定指标（如髌骨软骨体积或心脏射血分数）。通过共形预测将不确定性转化为严格校准的置信区间。在采集过程中，系统迭代采样k空间，更新重建结果，并评估置信区间。当不确定性满足用户预定义的精度目标时，扫描自动终止。", "result": "该自适应方法与固定协议相比，显著减少了扫描时间，同时为最终图像的精度提供了正式的统计学保证。在膝关节和心脏MRI数据集上验证了其有效性。", "conclusion": "该框架超越了固定的加速因子，实现了患者特异性的采集，平衡了扫描效率与诊断信心，是迈向个性化和资源高效MRI的关键一步。"}}
{"id": "2508.14951", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.14951", "abs": "https://arxiv.org/abs/2508.14951", "authors": ["Dario Vajda", "Domen Vreš", "Marko Robnik-Šikonja"], "title": "Improving LLMs for Machine Translation Using Synthetic Preference Data", "comment": "Paper with individual presentation at LUHME workshop at ECAI 2025", "summary": "Large language models have emerged as effective machine translation systems.\nIn this paper, we explore how a general instruction-tuned large language model\ncan be improved for machine translation using relatively few easily produced\ndata resources. Using Slovene as a use case, we improve the GaMS-9B-Instruct\nmodel using Direct Preference Optimization (DPO) training on a programmatically\ncurated and enhanced subset of a public dataset. As DPO requires pairs of\nquality-ranked instances, we generated its training dataset by translating\nEnglish Wikipedia articles using two LLMs, GaMS-9B-Instruct and\nEuroLLM-9B-Instruct. We ranked the resulting translations based on heuristics\ncoupled with automatic evaluation metrics such as COMET. The evaluation shows\nthat our fine-tuned model outperforms both models involved in the dataset\ngeneration. In comparison to the baseline models, the fine-tuned model achieved\na COMET score gain of around 0.04 and 0.02, respectively, on translating\nWikipedia articles. It also more consistently avoids language and formatting\nerrors.", "AI": {"tldr": "本研究通过使用直接偏好优化（DPO）和程序化生成的数据集，成功改进了一个通用指令调优的大语言模型在斯洛文尼亚语机器翻译上的性能。", "motivation": "大语言模型已成为有效的机器翻译系统。研究旨在探索如何利用相对较少且易于生成的数据资源，改进一个通用指令调优的大语言模型，使其在机器翻译方面表现更好。", "method": "研究以GaMS-9B-Instruct模型和斯洛文尼亚语为用例，采用直接偏好优化（DPO）进行训练。DPO所需的数据集通过以下方式生成：使用GaMS-9B-Instruct和EuroLLM-9B-Instruct两个大语言模型翻译英文维基百科文章，然后基于启发式规则和COMET等自动评估指标对翻译结果进行质量排序，从而创建出质量分级的实例对。该数据集是从一个公共数据集的子集中程序化地策划和增强而来的。", "result": "微调后的模型在翻译维基百科文章时，COMET分数分别比用于数据集生成的两个基线模型提高了约0.04和0.02。此外，微调模型在避免语言和格式错误方面也表现得更加一致。", "conclusion": "通过结合直接偏好优化（DPO）和少量程序化生成、质量分级的数据资源，可以有效提升通用指令调优大语言模型在特定机器翻译任务上的性能，实现更高的翻译质量并减少错误。"}}
{"id": "2508.14980", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.14980", "abs": "https://arxiv.org/abs/2508.14980", "authors": ["Andrei Balykin", "Anvar Ganiev", "Denis Kondranin", "Kirill Polevoda", "Nikolai Liudkevich", "Artem Petrov"], "title": "Paired-Sampling Contrastive Framework for Joint Physical-Digital Face Attack Detection", "comment": "Accepted to ICCV2025 FAS workshop", "summary": "Modern face recognition systems remain vulnerable to spoofing attempts,\nincluding both physical presentation attacks and digital forgeries.\nTraditionally, these two attack vectors have been handled by separate models,\neach targeting its own artifacts and modalities. However, maintaining distinct\ndetectors increases system complexity and inference latency and leaves systems\nexposed to combined attack vectors. We propose the Paired-Sampling Contrastive\nFramework, a unified training approach that leverages automatically matched\npairs of genuine and attack selfies to learn modality-agnostic liveness cues.\nEvaluated on the 6th Face Anti-Spoofing Challenge Unified Physical-Digital\nAttack Detection benchmark, our method achieves an average classification error\nrate (ACER) of 2.10 percent, outperforming prior solutions. The framework is\nlightweight (4.46 GFLOPs) and trains in under one hour, making it practical for\nreal-world deployment. Code and pretrained models are available at\nhttps://github.com/xPONYx/iccv2025_deepfake_challenge.", "AI": {"tldr": "本文提出了一种统一的配对采样对比框架，用于同时检测物理呈现攻击和数字伪造攻击，提高了面部反欺诈系统的效率和性能。", "motivation": "现代人脸识别系统容易受到物理和数字欺骗攻击。传统上，这两种攻击由独立的模型处理，导致系统复杂性增加、推理延迟，并易受组合攻击的影响。", "method": "本文提出了“配对采样对比框架”（Paired-Sampling Contrastive Framework），这是一种统一的训练方法。它利用自动匹配的真实自拍和攻击自拍对，来学习与模态无关的活体线索。", "result": "在第6届人脸反欺诈挑战赛统一物理-数字攻击检测基准上，该方法实现了2.10%的平均分类错误率（ACER），优于现有解决方案。该框架轻量级（4.46 GFLOPs），训练时间不到一小时。", "conclusion": "该框架提供了一种有效、轻量且训练快速的解决方案，能够统一处理物理和数字人脸欺骗攻击，具有实际部署价值。"}}
{"id": "2508.15047", "categories": ["cs.AI", "cs.GR"], "pdf": "https://arxiv.org/pdf/2508.15047", "abs": "https://arxiv.org/abs/2508.15047", "authors": ["Yibo Liu", "Liam Shatzel", "Brandon Haworth", "Teseo Schneider"], "title": "Emergent Crowds Dynamics from Language-Driven Multi-Agent Interactions", "comment": null, "summary": "Animating and simulating crowds using an agent-based approach is a\nwell-established area where every agent in the crowd is individually controlled\nsuch that global human-like behaviour emerges. We observe that human navigation\nand movement in crowds are often influenced by complex social and environmental\ninteractions, driven mainly by language and dialogue. However, most existing\nwork does not consider these dimensions and leads to animations where\nagent-agent and agent-environment interactions are largely limited to steering\nand fixed higher-level goal extrapolation.\n  We propose a novel method that exploits large language models (LLMs) to\ncontrol agents' movement. Our method has two main components: a dialogue system\nand language-driven navigation. We periodically query agent-centric LLMs\nconditioned on character personalities, roles, desires, and relationships to\ncontrol the generation of inter-agent dialogue when necessitated by the spatial\nand social relationships with neighbouring agents. We then use the conversation\nand each agent's personality, emotional state, vision, and physical state to\ncontrol the navigation and steering of each agent. Our model thus enables\nagents to make motion decisions based on both their perceptual inputs and the\nongoing dialogue.\n  We validate our method in two complex scenarios that exemplify the interplay\nbetween social interactions, steering, and crowding. In these scenarios, we\nobserve that grouping and ungrouping of agents automatically occur.\nAdditionally, our experiments show that our method serves as an\ninformation-passing mechanism within the crowd. As a result, our framework\nproduces more realistic crowd simulations, with emergent group behaviours\narising naturally from any environmental setting.", "AI": {"tldr": "该研究提出了一种利用大型语言模型（LLMs）来驱动人群中智能体的对话和导航的新方法，从而实现更真实、更具社会互动性的群体行为模拟。", "motivation": "现有的人群模拟方法在智能体交互方面主要局限于转向和预设目标，未能充分考虑语言和对话在人类导航和移动中复杂的社会和环境影响，导致模拟结果缺乏真实感。", "method": "该方法包含对话系统和语言驱动导航两个核心组件。它周期性地查询以智能体个性、角色、愿望和关系为条件的LLMs，根据空间和社会关系生成智能体之间的对话。然后，结合对话内容、智能体的个性、情绪状态、视觉和物理状态来控制每个智能体的导航和转向，使智能体能够基于感知输入和正在进行的对话做出移动决策。", "result": "在两个复杂的场景中验证了该方法，观察到智能体自动地进行分组和解组。实验表明，该方法作为人群中的信息传递机制，能够产生更真实的人群模拟，并自然地从任何环境设置中涌现出群体行为。", "conclusion": "通过整合LLMs驱动的对话和导航，该框架能够生成更真实的人群模拟，其中涌现的群体行为自然地产生于智能体之间的社会互动和信息传递，有效解决了现有方法在复杂社会交互方面的局限性。"}}
{"id": "2508.15616", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.15616", "abs": "https://arxiv.org/abs/2508.15616", "authors": ["Alvaro Detailleur", "Dalim Wahby", "Guillaume Ducard", "Christopher Onder"], "title": "Synthesis and SOS-based Stability Verification of a Neural-Network-Based Controller for a Two-wheeled Inverted Pendulum", "comment": "Submitted to the IEEE for possible publication, 16 pages, 10 figures", "summary": "This work newly establishes the feasibility and practical value of a sum of\nsquares (SOS)-based stability verification procedure for applied control\nproblems utilizing neural-network-based controllers (NNCs). It successfully\nverifies closed-loop stability properties of a NNC synthesized using a\ngeneralizable procedure to imitate a robust, tube-based model predictive\ncontroller (MPC) for a two-wheeled inverted pendulum demonstrator system. This\nis achieved by first developing a state estimator and control-oriented model\nfor the two-wheeled inverted pendulum. Next, this control-oriented model is\nused to synthesize a baseline linear-quadratic regulator (LQR) and a robust,\ntube-based MPC, which is computationally too demanding for real-time execution\non the demonstrator system's embedded hardware. The generalizable synthesis\nprocedure generates an NNC imitating the robust, tube-based MPC. Via an\nSOS-based stability verification procedure, a certificate of local asymptotic\nstability and a relevant inner estimate of the region of attraction (RoA) are\nobtained for the closed-loop system incorporating this NNC. Finally,\nexperimental results on the physical two-wheeled inverted pendulum demonstrate\nthat the NNC both stabilizes the system, and improves the control performance\ncompared to the baseline LQR in both regulation and reference-tracking tasks.", "AI": {"tldr": "本文首次建立了基于和方和(SOS)的稳定性验证程序对使用神经网络控制器(NNC)的应用控制问题的可行性和实用价值。通过模仿鲁棒、基于管的MPC，成功验证了NNC控制的两轮倒立摆系统的闭环稳定性，并获得了局部渐近稳定性和吸引域估计，实验结果表明NNC稳定了系统并提升了性能。", "motivation": "将神经网络控制器(NNC)应用于实际控制问题时，需要对其闭环稳定性进行严格验证。传统的鲁棒模型预测控制器(MPC)虽然性能优异，但计算成本高昂，难以在嵌入式硬件上实时执行。因此，研究如何使NNC模仿鲁棒MPC的性能，同时通过可靠方法（如SOS）提供稳定性保证，是重要的研究方向。", "method": "1. 为两轮倒立摆系统开发状态估计器和面向控制的模型。2. 基于该模型合成基线线性二次调节器(LQR)和鲁棒、基于管的MPC。3. 采用通用合成程序生成模仿鲁棒、基于管的MPC的NNC。4. 对包含NNC的闭环系统应用基于和方和(SOS)的稳定性验证程序，以获得局部渐近稳定性证书和吸引域(RoA)的内部估计。5. 在物理两轮倒立摆系统上进行实验验证。", "result": "1. 基于SOS的稳定性验证程序成功验证了采用NNC的闭环系统的局部渐近稳定性，并获得了吸引域的相关内部估计。2. 实验结果表明，NNC不仅能稳定系统，而且在调节和参考跟踪任务中，与基线LQR相比，控制性能有所提高。", "conclusion": "基于和方和(SOS)的稳定性验证程序对于使用神经网络控制器(NNC)的应用控制问题具有可行性和实用价值。通过该程序，可以为模仿鲁棒MPC的NNC提供稳定性保证，同时在实际系统中实现更好的控制性能和实时执行能力。"}}
{"id": "2508.15160", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.15160", "abs": "https://arxiv.org/abs/2508.15160", "authors": ["Hesam Azadjou", "Suraj Chakravarthi Raja", "Ali Marjaninejad", "Francisco J. Valero-Cuevas"], "title": "Hardware Implementation of a Zero-Prior-Knowledge Approach to Lifelong Learning in Kinematic Control of Tendon-Driven Quadrupeds", "comment": null, "summary": "Like mammals, robots must rapidly learn to control their bodies and interact\nwith their environment despite incomplete knowledge of their body structure and\nsurroundings. They must also adapt to continuous changes in both. This work\npresents a bio-inspired learning algorithm, General-to-Particular (G2P),\napplied to a tendon-driven quadruped robotic system developed and fabricated\nin-house. Our quadruped robot undergoes an initial five-minute phase of\ngeneralized motor babbling, followed by 15 refinement trials (each lasting 20\nseconds) to achieve specific cyclical movements. This process mirrors the\nexploration-exploitation paradigm observed in mammals. With each refinement,\nthe robot progressively improves upon its initial \"good enough\" solution. Our\nresults serve as a proof-of-concept, demonstrating the hardware-in-the-loop\nsystem's ability to learn the control of a tendon-driven quadruped with\nredundancies in just a few minutes to achieve functional and adaptive cyclical\nnon-convex movements. By advancing autonomous control in robotic locomotion,\nour approach paves the way for robots capable of dynamically adjusting to new\nenvironments, ensuring sustained adaptability and performance.", "AI": {"tldr": "本文提出了一种名为G2P的生物启发式学习算法，使肌腱驱动的四足机器人在几分钟内学会了适应性循环运动控制。", "motivation": "机器人需要像哺乳动物一样，在对自身和环境知识不完全的情况下，快速学习身体控制并与环境互动，同时适应持续变化。", "method": "研究采用生物启发式学习算法G2P，在一个自研的肌腱驱动四足机器人上进行。机器人首先经历5分钟的通用运动探索（motor babbling），随后进行15次每次20秒的精炼试验以实现特定的循环运动，模仿哺乳动物的探索-利用范式。", "result": "结果表明，该硬件在环系统能够在短短几分钟内，让具有冗余的肌腱驱动四足机器人学会控制，实现功能性、适应性的非凸循环运动。每次精炼后，机器人都会在其初始的“足够好”的解决方案上逐步改进。", "conclusion": "该方法推进了机器人运动中的自主控制，为机器人动态适应新环境、确保持续适应性和性能铺平了道路。"}}
{"id": "2508.15003", "categories": ["eess.IV", "cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.15003", "abs": "https://arxiv.org/abs/2508.15003", "authors": ["Andrew C. Freeman"], "title": "Scalable Event-Based Video Streaming for Machines with MoQ", "comment": "Accepted to ACM Mile High Video 2025", "summary": "Lossy compression and rate-adaptive streaming are a mainstay in traditional\nvideo steams. However, a new class of neuromorphic ``event'' sensors records\nvideo with asynchronous pixel samples rather than image frames. These sensors\nare designed for computer vision applications, rather than human video\nconsumption. Until now, researchers have focused their efforts primarily on\napplication development, ignoring the crucial problem of data transmission. We\nsurvey the landscape of event-based video systems, discuss the technical issues\nwith our recent scalable event streaming work, and propose a new low-latency\nevent streaming format based on the latest additions to the Media Over QUIC\nprotocol draft.", "AI": {"tldr": "本文探讨了神经拟态事件传感器视频的传输问题，并提出了一种基于Media Over QUIC协议草案的低延迟事件流媒体格式。", "motivation": "传统的视频流主要关注有损压缩和速率自适应流媒体，但新型神经拟态事件传感器记录的是异步像素样本而非图像帧。这些传感器主要用于计算机视觉应用，而非人类观看。目前研究主要集中在应用开发，而忽略了关键的数据传输问题。", "method": "作者首先调查了现有基于事件的视频系统，然后讨论了其近期可扩展事件流媒体工作中的技术问题，并在此基础上提出了一种基于Media Over QUIC协议草案最新增补的低延迟事件流媒体格式。", "result": "提出了一种新的低延迟事件流媒体格式，该格式基于Media Over QUIC协议草案的最新进展。", "conclusion": "数据传输是基于事件的视频系统面临的关键挑战，需要专门的解决方案。本文提出了一种利用Media Over QUIC协议的新格式来解决低延迟事件流传输问题。"}}
{"id": "2508.14982", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.14982", "abs": "https://arxiv.org/abs/2508.14982", "authors": ["Qianli Wang", "Tatiana Anikina", "Nils Feldhus", "Simon Ostermann", "Fedor Splitt", "Jiaao Li", "Yoana Tsoneva", "Sebastian Möller", "Vera Schmitt"], "title": "Multilingual Datasets for Custom Input Extraction and Explanation Requests Parsing in Conversational XAI Systems", "comment": "Accepted at EMNLP 2025 Findings, camera-ready version", "summary": "Conversational explainable artificial intelligence (ConvXAI) systems based on\nlarge language models (LLMs) have garnered considerable attention for their\nability to enhance user comprehension through dialogue-based explanations.\nCurrent ConvXAI systems often are based on intent recognition to accurately\nidentify the user's desired intention and map it to an explainability method.\nWhile such methods offer great precision and reliability in discerning users'\nunderlying intentions for English, a significant challenge in the scarcity of\ntraining data persists, which impedes multilingual generalization. Besides, the\nsupport for free-form custom inputs, which are user-defined data distinct from\npre-configured dataset instances, remains largely limited. To bridge these\ngaps, we first introduce MultiCoXQL, a multilingual extension of the CoXQL\ndataset spanning five typologically diverse languages, including one\nlow-resource language. Subsequently, we propose a new parsing approach aimed at\nenhancing multilingual parsing performance, and evaluate three LLMs on\nMultiCoXQL using various parsing strategies. Furthermore, we present Compass, a\nnew multilingual dataset designed for custom input extraction in ConvXAI\nsystems, encompassing 11 intents across the same five languages as MultiCoXQL.\nWe conduct monolingual, cross-lingual, and multilingual evaluations on Compass,\nemploying three LLMs of varying sizes alongside BERT-type models.", "AI": {"tldr": "该研究通过引入多语言数据集（MultiCoXQL和Compass）和新的解析方法，解决了基于大语言模型（LLMs）的对话式可解释人工智能（ConvXAI）系统中多语言数据稀缺和自由形式自定义输入支持不足的问题。", "motivation": "当前ConvXAI系统在多语言泛化方面面临训练数据稀缺的挑战，且对自由形式的自定义输入支持有限。现有方法虽然在英语意图识别上表现良好，但难以扩展到其他语言。", "method": "1. 引入MultiCoXQL数据集，它是CoXQL的多语言扩展，涵盖五种语言（包括一种低资源语言）。2. 提出一种新的解析方法以提升多语言解析性能。3. 使用不同的解析策略在MultiCoXQL上评估三种LLMs。4. 引入Compass数据集，用于ConvXAI系统中的自定义输入提取，包含相同五种语言的11种意图。5. 在Compass数据集上进行单语言、跨语言和多语言评估，使用三种不同大小的LLMs和BERT类型模型。", "result": "研究引入了MultiCoXQL和Compass两个新的多语言数据集，并提出了一种新的多语言解析方法。此外，还在这些新数据集上对多种LLMs和BERT类型模型进行了广泛的单语言、跨语言和多语言评估。", "conclusion": "通过提供新的多语言数据集和解析方法，该研究为解决ConvXAI系统中多语言数据稀缺和自由形式自定义输入支持不足的挑战奠定了基础，并为未来多语言ConvXAI系统的发展提供了资源和评估框架。"}}
{"id": "2508.15020", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15020", "abs": "https://arxiv.org/abs/2508.15020", "authors": ["Susim Roy", "Anubhooti Jain", "Mayank Vatsa", "Richa Singh"], "title": "TAIGen: Training-Free Adversarial Image Generation via Diffusion Models", "comment": "Accepted at ICCVW-CV4BIOM 2025", "summary": "Adversarial attacks from generative models often produce low-quality images\nand require substantial computational resources. Diffusion models, though\ncapable of high-quality generation, typically need hundreds of sampling steps\nfor adversarial generation. This paper introduces TAIGen, a training-free\nblack-box method for efficient adversarial image generation. TAIGen produces\nadversarial examples using only 3-20 sampling steps from unconditional\ndiffusion models. Our key finding is that perturbations injected during the\nmixing step interval achieve comparable attack effectiveness without processing\nall timesteps. We develop a selective RGB channel strategy that applies\nattention maps to the red channel while using GradCAM-guided perturbations on\ngreen and blue channels. This design preserves image structure while maximizing\nmisclassification in target models. TAIGen maintains visual quality with PSNR\nabove 30 dB across all tested datasets. On ImageNet with VGGNet as source,\nTAIGen achieves 70.6% success against ResNet, 80.8% against MNASNet, and 97.8%\nagainst ShuffleNet. The method generates adversarial examples 10x faster than\nexisting diffusion-based attacks. Our method achieves the lowest robust\naccuracy, indicating it is the most impactful attack as the defense mechanism\nis least successful in purifying the images generated by TAIGen.", "AI": {"tldr": "TAIGen是一种高效、免训练的黑盒对抗性攻击方法，利用无条件扩散模型在3-20个采样步骤内生成高质量对抗样本，通过选择性扰动和RGB通道策略实现高攻击成功率和10倍加速。", "motivation": "现有的生成模型（如GAN）生成的对抗样本质量低且计算资源消耗大。扩散模型虽然能生成高质量图像，但通常需要数百个采样步骤才能生成对抗样本，因此需要一种高效、高质量的对抗图像生成方法。", "method": "TAIGen是一种免训练的黑盒方法。核心思想是在无条件扩散模型的“混合步骤区间”（3-20个采样步骤）内注入扰动，而非处理所有时间步。它采用选择性RGB通道策略：在红色通道应用注意力图，同时在绿色和蓝色通道使用GradCAM引导的扰动，旨在在保持图像结构的同时最大化目标模型的错误分类。", "result": "TAIGen仅需3-20个采样步骤即可生成对抗样本。它保持了良好的视觉质量（所有测试数据集PSNR均高于30 dB）。在ImageNet上，以VGGNet为源模型，TAIGen对ResNet的攻击成功率为70.6%，对MNASNet为80.8%，对ShuffleNet为97.8%。该方法生成对抗样本的速度比现有基于扩散的攻击快10倍。它实现了最低的鲁棒准确率，表明其对防御机制的影响最大。", "conclusion": "TAIGen是一种高效、高质量且影响深远的黑盒对抗性攻击方法，利用扩散模型显著提高了攻击速度和有效性，同时保持了视觉质量，优于现有方法。"}}
{"id": "2508.15050", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.15050", "abs": "https://arxiv.org/abs/2508.15050", "authors": ["Romain Lacombe", "Kerrie Wu", "Eddie Dilworth"], "title": "Don't Think Twice! Over-Reasoning Impairs Confidence Calibration", "comment": "Published at ICML 2025 Workshop on Reliable and Responsible\n  Foundation Models", "summary": "Large Language Models deployed as question answering tools require robust\ncalibration to avoid overconfidence. We systematically evaluate how reasoning\ncapabilities and budget affect confidence assessment accuracy, using the\nClimateX dataset (Lacombe et al., 2023) and expanding it to human and planetary\nhealth. Our key finding challenges the \"test-time scaling\" paradigm: while\nrecent reasoning LLMs achieve 48.7% accuracy in assessing expert confidence,\nincreasing reasoning budgets consistently impairs rather than improves\ncalibration. Extended reasoning leads to systematic overconfidence that worsens\nwith longer thinking budgets, producing diminishing and negative returns beyond\nmodest computational investments. Conversely, search-augmented generation\ndramatically outperforms pure reasoning, achieving 89.3% accuracy by retrieving\nrelevant evidence. Our results suggest that information access, rather than\nreasoning depth or inference budget, may be the critical bottleneck for\nimproved confidence calibration of knowledge-intensive tasks.", "AI": {"tldr": "研究发现，在知识密集型任务中，增加大型语言模型（LLM）的推理预算会损害其置信度校准，导致过度自信。相反，通过检索证据进行搜索增强的生成能显著提高校准准确性，表明信息获取而非推理深度是关键瓶颈。", "motivation": "作为问答工具部署的大型语言模型需要强大的校准能力以避免过度自信。", "method": "研究系统评估了推理能力和预算对置信度评估准确性的影响，使用了ClimateX数据集并扩展到人类和地球健康领域。对比了纯粹推理和搜索增强生成两种方法。", "result": "近期推理LLM在评估专家置信度方面的准确率为48.7%。增加推理预算反而持续损害校准，导致系统性过度自信，且随着思考预算的增加而恶化，这挑战了“测试时间扩展”范式。相比之下，搜索增强生成通过检索相关证据，准确率达到89.3%，远超纯粹推理。", "conclusion": "研究结果表明，信息获取而非推理深度或推理预算，可能是提高知识密集型任务中置信度校准的关键瓶颈。"}}
{"id": "2508.15649", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.15649", "abs": "https://arxiv.org/abs/2508.15649", "authors": ["Zhong Guo", "Prabir Barooah"], "title": "A Central Chilled Water Plant Model for Designing Learning-Based Controllers", "comment": null, "summary": "We describe a framework of modeling a central chilled\n  water plant (CCWP) that consists of an aggregate\n  cooling coil, a number of heterogeneous chillers and\n  cooling towers, and a chilled water-based thermal\n  energy storage system. We improve upon existing component\n  models from the open literature using a constrained\n  optimization-based framework to ensure that the models\n  respect capacities of all the heat exchangers (cooling\n  coils, chillers, and cooling towers) irrespective of\n  the inputs provided. As a result, the proposed model has a wider\n  range of validity compared to existing models; the\n  latter can produce highly erroneous outputs when inputs are not\n  within normal operating range. This\n  feature is essential for training learning-based\n  controllers that can choose inputs beyond normal operating conditions and is\nlacking in currently available\n  models. The overall plant model is\n  implemented in Matlab and is made publicly\n  available. Simulation of a CCWP with closed loop\n  control is provided as an illustration.", "AI": {"tldr": "本文提出了一种中央冷水机组（CCWP）建模框架，通过基于约束优化的方法改进了现有组件模型，确保模型在更宽泛的输入范围内尊重热交换器容量，从而提高了模型的有效性，并使其适用于训练学习型控制器。", "motivation": "现有组件模型在输入超出正常运行范围时会产生高度错误的结果，这对于需要探索超出正常运行条件输入的学习型控制器训练而言是一个重大缺陷。", "method": "开发了一个基于约束优化的框架，以改进现有文献中的组件模型。该框架确保模型无论输入如何，都尊重所有热交换器（冷却盘管、冷水机组和冷却塔）的容量。整个工厂模型在Matlab中实现并公开提供。", "result": "与现有模型相比，所提出的模型具有更广泛的有效范围，即使输入不在正常运行范围内，也不会产生高度错误的结果。这一特性对于训练能够选择超出正常运行条件输入的学习型控制器至关重要。", "conclusion": "所提出的建模框架提供了一个更鲁棒、更有效的中央冷水机组模型，克服了现有模型在非正常操作条件下准确性不足的问题，特别适用于开发和训练先进的学习型控制器。"}}
{"id": "2508.15201", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15201", "abs": "https://arxiv.org/abs/2508.15201", "authors": ["Haoran Li", "Yuhui Chen", "Wenbo Cui", "Weiheng Liu", "Kai Liu", "Mingcai Zhou", "Zhengtao Zhang", "Dongbin Zhao"], "title": "Survey of Vision-Language-Action Models for Embodied Manipulation", "comment": "in Chinese language", "summary": "Embodied intelligence systems, which enhance agent capabilities through\ncontinuous environment interactions, have garnered significant attention from\nboth academia and industry. Vision-Language-Action models, inspired by\nadvancements in large foundation models, serve as universal robotic control\nframeworks that substantially improve agent-environment interaction\ncapabilities in embodied intelligence systems. This expansion has broadened\napplication scenarios for embodied AI robots. This survey comprehensively\nreviews VLA models for embodied manipulation. Firstly, it chronicles the\ndevelopmental trajectory of VLA architectures. Subsequently, we conduct a\ndetailed analysis of current research across 5 critical dimensions: VLA model\nstructures, training datasets, pre-training methods, post-training methods, and\nmodel evaluation. Finally, we synthesize key challenges in VLA development and\nreal-world deployment, while outlining promising future research directions.", "AI": {"tldr": "这篇综述全面回顾了具身操作中的视觉-语言-动作（VLA）模型，涵盖其发展轨迹、当前研究（从模型结构、数据集、预训练、后训练和评估五个维度），并提出了挑战和未来方向。", "motivation": "具身智能系统通过持续环境交互提升智能体能力，VLA模型作为通用机器人控制框架，显著增强了具身智能系统中的智能体-环境交互能力，拓宽了具身AI机器人的应用场景，因此对VLA模型进行综述具有重要意义。", "method": "本文采用综述方法，首先梳理VLA架构的发展轨迹；其次，从VLA模型结构、训练数据集、预训练方法、后训练方法和模型评估五个关键维度详细分析当前研究；最后，总结VLA发展和实际部署中的主要挑战，并提出未来研究方向。", "result": "本文提供了具身操作中VLA模型的全面回顾，详细分析了当前研究的五个关键维度，并综合了VLA开发和实际部署中的主要挑战，同时规划了有前景的未来研究方向。", "conclusion": "VLA模型是具身智能系统实现具身操作的关键技术，尽管取得了显著进展，但在实际部署和进一步发展中仍面临挑战，未来研究需在多个方向上持续探索。"}}
{"id": "2508.15011", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2508.15011", "abs": "https://arxiv.org/abs/2508.15011", "authors": ["Asadullah Bin Rahman", "Masud Ibn Afjal", "Md. Abdulla Al Mamun"], "title": "Systematic Evaluation of Wavelet-Based Denoising for MRI Brain Images: Optimal Configurations and Performance Benchmarks", "comment": null, "summary": "Medical imaging modalities including magnetic resonance imaging (MRI),\ncomputed tomography (CT), and ultrasound are essential for accurate diagnosis\nand treatment planning in modern healthcare. However, noise contamination\nduring image acquisition and processing frequently degrades image quality,\nobscuring critical diagnostic details and compromising clinical\ndecision-making. Additionally, enhancement techniques such as histogram\nequalization may inadvertently amplify existing noise artifacts, including\nsalt-and-pepper distortions. This study investigates wavelet transform-based\ndenoising methods for effective noise mitigation in medical images, with the\nprimary objective of identifying optimal combinations of threshold values,\ndecomposition levels, and wavelet types to achieve superior denoising\nperformance and enhanced diagnostic accuracy. Through systematic evaluation\nacross various noise conditions, the research demonstrates that the bior6.8\nbiorthogonal wavelet with universal thresholding at decomposition levels 2-3\nconsistently achieves optimal denoising performance, providing significant\nnoise reduction while preserving essential anatomical structures and diagnostic\nfeatures critical for clinical applications.", "AI": {"tldr": "本研究调查了基于小波变换的医学图像去噪方法，发现bior6.8双正交小波结合通用阈值处理（分解级别2-3）能实现最佳去噪效果，同时保留关键诊断信息。", "motivation": "医学图像（如MRI、CT、超声）中的噪声污染会降低图像质量，掩盖诊断细节并影响临床决策。现有增强技术（如直方图均衡化）可能无意中放大噪声伪影（包括椒盐噪声），因此需要有效的去噪方法。", "method": "本研究通过系统评估不同噪声条件，调查了基于小波变换的去噪方法，旨在识别阈值、分解级别和小波类型的最佳组合，以实现卓越的去噪性能和增强的诊断准确性。", "result": "研究表明，bior6.8双正交小波结合通用阈值处理，在分解级别2-3时，能持续实现最佳去噪性能，显著减少噪声，同时保留了对临床应用至关重要的解剖结构和诊断特征。", "conclusion": "所确定的基于小波变换的去噪方法（bior6.8双正交小波、通用阈值、分解级别2-3）为医学图像去噪提供了一个有效且优化的解决方案，有助于提高诊断准确性。"}}
{"id": "2508.15044", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.15044", "abs": "https://arxiv.org/abs/2508.15044", "authors": ["Bolian Li", "Yanran Wu", "Xinyu Luo", "Ruqi Zhang"], "title": "Reward-Shifted Speculative Sampling Is An Efficient Test-Time Weak-to-Strong Aligner", "comment": null, "summary": "Aligning large language models (LLMs) with human preferences has become a\ncritical step in their development. Recent research has increasingly focused on\ntest-time alignment, where additional compute is allocated during inference to\nenhance LLM safety and reasoning capabilities. However, these test-time\nalignment techniques often incur substantial inference costs, limiting their\npractical application. We are inspired by the speculative sampling\nacceleration, which leverages a small draft model to efficiently predict future\ntokens, to address the efficiency bottleneck of test-time alignment. We\nintroduce the reward-Shifted Speculative Sampling (SSS) algorithm, in which the\ndraft model is aligned with human preferences, while the target model remains\nunchanged. We theoretically demonstrate that the distributional shift between\nthe aligned draft model and the unaligned target model can be exploited to\nrecover the RLHF optimal solution without actually obtaining it, by modifying\nthe acceptance criterion and bonus token distribution. Our algorithm achieves\nsuperior gold reward scores at a significantly reduced inference cost in\ntest-time weak-to-strong alignment experiments, thereby validating both its\neffectiveness and efficiency.", "AI": {"tldr": "本文提出了一种名为奖励偏移推测采样（SSS）的算法，通过使用与人类偏好对齐的草稿模型，显著降低了大型语言模型测试时对齐的推理成本，同时保持了对齐效果。", "motivation": "大型语言模型（LLMs）的测试时对齐（test-time alignment）能够提升其安全性和推理能力，但高昂的推理成本限制了其实际应用。", "method": "受推测采样加速技术的启发，该研究引入了奖励偏移推测采样（SSS）算法。SSS使用一个与人类偏好对齐的草稿模型，而目标模型保持未对齐。通过修改接受标准和奖励令牌分布，理论上证明可以利用对齐草稿模型与未对齐目标模型之间的分布差异，在不实际获取RLHF最优解的情况下恢复该解。", "result": "在测试时弱到强对齐实验中，SSS算法以显著降低的推理成本获得了更高的黄金奖励分数，验证了其有效性和效率。", "conclusion": "SSS算法为解决测试时对齐的效率瓶颈提供了一个有效且高效的解决方案，通过巧妙利用偏好对齐的草稿模型实现了性能和成本的双重优化。"}}
{"id": "2508.15027", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.15027", "abs": "https://arxiv.org/abs/2508.15027", "authors": ["Chunming He", "Fengyang Xiao", "Rihan Zhang", "Chengyu Fang", "Deng-Ping Fan", "Sina Farsiu"], "title": "Reversible Unfolding Network for Concealed Visual Perception with Generative Refinement", "comment": "18 pages, 21 tables, 13 figures", "summary": "Existing methods for concealed visual perception (CVP) often leverage\nreversible strategies to decrease uncertainty, yet these are typically confined\nto the mask domain, leaving the potential of the RGB domain underexplored. To\naddress this, we propose a reversible unfolding network with generative\nrefinement, termed RUN++. Specifically, RUN++ first formulates the CVP task as\na mathematical optimization problem and unfolds the iterative solution into a\nmulti-stage deep network. This approach provides a principled way to apply\nreversible modeling across both mask and RGB domains while leveraging a\ndiffusion model to resolve the resulting uncertainty. Each stage of the network\nintegrates three purpose-driven modules: a Concealed Object Region Extraction\n(CORE) module applies reversible modeling to the mask domain to identify core\nobject regions; a Context-Aware Region Enhancement (CARE) module extends this\nprinciple to the RGB domain to foster better foreground-background separation;\nand a Finetuning Iteration via Noise-based Enhancement (FINE) module provides a\nfinal refinement. The FINE module introduces a targeted Bernoulli diffusion\nmodel that refines only the uncertain regions of the segmentation mask,\nharnessing the generative power of diffusion for fine-detail restoration\nwithout the prohibitive computational cost of a full-image process. This unique\nsynergy, where the unfolding network provides a strong uncertainty prior for\nthe diffusion model, allows RUN++ to efficiently direct its focus toward\nambiguous areas, significantly mitigating false positives and negatives.\nFurthermore, we introduce a new paradigm for building robust CVP systems that\nremain effective under real-world degradations and extend this concept into a\nbroader bi-level optimization framework.", "AI": {"tldr": "RUN++是一个用于隐蔽视觉感知（CVP）的可逆展开网络，它在掩码和RGB域中应用可逆建模，并利用目标扩散模型进行高效且鲁棒的细化，显著减少了误报和漏报。", "motivation": "现有的CVP方法主要局限于掩码域，未能充分利用RGB域的潜力。此外，这些方法在处理不确定性方面仍有不足，并且在真实世界退化下鲁棒性有待提高。", "method": "RUN++将CVP任务公式化为数学优化问题，并将其迭代解展开为多阶段深度网络。每个阶段包含三个模块：1) CORE模块在掩码域应用可逆建模以识别核心对象区域；2) CARE模块将可逆原理扩展到RGB域以增强前景-背景分离；3) FINE模块引入目标伯努利扩散模型，仅对分割掩码的不确定区域进行细化。展开网络为扩散模型提供了强不确定性先验。此外，本文提出了构建在真实世界退化下保持有效的鲁棒CVP系统的新范式，并将其扩展到更广泛的双层优化框架。", "result": "RUN++能够高效地将注意力集中到模糊区域，显著减少误报和漏报。它在不增加高计算成本的情况下实现了精细细节的恢复，并构建了在真实世界退化下更鲁棒的CVP系统。", "conclusion": "RUN++通过在掩码和RGB域中整合可逆建模以及引入目标扩散模型，有效解决了现有CVP方法的局限性，实现了高效、精确且鲁棒的隐蔽视觉感知。"}}
{"id": "2508.15053", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.15053", "abs": "https://arxiv.org/abs/2508.15053", "authors": ["Itai Zilberstein", "Alberto Candela", "Steve Chien", "David Rijlaarsdam", "Tom Hendrix", "Leonie Buckley", "Aubrey Dunne"], "title": "Demonstrating Onboard Inference for Earth Science Applications with Spectral Analysis Algorithms and Deep Learning", "comment": "International Symposium on Artificial Intelligence, Robotics and\n  Automation in Space, November 2024", "summary": "In partnership with Ubotica Technologies, the Jet Propulsion Laboratory is\ndemonstrating state-of-the-art data analysis onboard CogniSAT-6/HAMMER (CS-6).\nCS-6 is a satellite with a visible and near infrared range hyperspectral\ninstrument and neural network acceleration hardware. Performing data analysis\nat the edge (e.g. onboard) can enable new Earth science measurements and\nresponses. We will demonstrate data analysis and inference onboard CS-6 for\nnumerous applications using deep learning and spectral analysis algorithms.", "AI": {"tldr": "CS-6卫星将利用其高光谱仪器和神经网络加速硬件，在轨演示基于深度学习和光谱分析的边缘数据分析，以实现新的地球科学测量和响应。", "motivation": "在卫星（边缘）上进行数据分析可以克服传统数据传输的限制，从而实现新的地球科学测量和更快的响应能力。", "method": "本研究使用CogniSAT-6/HAMMER (CS-6)卫星，该卫星配备可见光和近红外高光谱仪器以及神经网络加速硬件。研究方法包括利用深度学习和光谱分析算法进行在轨数据分析和推理。", "result": "该项目将成功演示CS-6卫星上针对多种应用的机载数据分析和推理能力。", "conclusion": "通过在CS-6卫星上结合高光谱数据和AI加速硬件进行边缘数据分析，能够显著提升地球科学测量的能力和实时响应效率。"}}
{"id": "2508.15729", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.15729", "abs": "https://arxiv.org/abs/2508.15729", "authors": ["Harshith Reddy", "Pankaj Arora"], "title": "A 16.28 ppm/$^\\circ$C Temperature Coefficient, 0.5V Low-Voltage CMOS Voltage Reference with Curvature Compensation", "comment": "6 pages, 29th International Symposium on VLSI Design and Test (VDAT\n  2025)", "summary": "This paper presents a fully-integrated CMOS voltage reference designed in a\n90 nm process node using low voltage threshold (LVT) transistor models. The\nvoltage reference leverages subthreshold operation and near-weak inversion\ncharacteristics, backed by an all-region MOSFET model. The proposed design\nachieves a very low operating supply voltage of 0.5 V and a remarkably low\ntemperature coefficient of 16.28 ppm/$^\\circ$C through the mutual compensation\nof CTAT, PTAT, and curvature-correction currents, over a wide range from -40\n$^\\circ$C to 130 $^\\circ$C. A stable reference voltage of 205 mV is generated\nwith a line sensitivity of 1.65 %/V and a power supply rejection ratio (PSRR)\nof -50 dB at 10 kHz. The circuit achieves all these parameters while\nmaintaining a good power efficiency, consuming only 0.67 $\\mu$W.", "AI": {"tldr": "该论文提出了一种采用90纳米CMOS工艺和低阈值电压(LVT)晶体管设计的全集成电压基准，通过CTAT、PTAT和曲率校正电流的相互补偿，在0.5V低电源电压下实现了16.28 ppm/°C的低温度系数和0.67 μW的超低功耗。", "motivation": "该研究旨在设计一种在低电源电压下工作、具有优异温度稳定性、低功耗和良好电源抑制比的全集成电压基准。", "method": "该设计采用90纳米CMOS工艺和低阈值电压(LVT)晶体管模型，利用亚阈值和近弱反型特性，并基于全区域MOSFET模型。通过CTAT、PTAT和曲率校正电流的相互补偿机制，实现了宽温度范围内的温度系数优化。", "result": "该电压基准在0.5V的低电源电压下工作，在-40°C至130°C的宽温度范围内实现了16.28 ppm/°C的低温度系数。它产生205 mV的稳定参考电压，具有1.65 %/V的线敏感度和在10 kHz下-50 dB的电源抑制比(PSRR)，同时功耗仅为0.67 μW。", "conclusion": "该论文成功设计并实现了一种在90纳米CMOS工艺下具有极低工作电压、出色温度稳定性、良好电源抑制比和超低功耗的全集成电压基准。"}}
{"id": "2508.15300", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.15300", "abs": "https://arxiv.org/abs/2508.15300", "authors": ["William McDonald", "Cedric Le Gentil", "Jennifer Wakulicz", "Teresa Vidal-Calleja"], "title": "Mag-Match: Magnetic Vector Field Features for Map Matching and Registration", "comment": "To be published in IROS: IEEE/RSJ International Conference on\n  Intelligent Robots and Systems, 2025", "summary": "Map matching and registration are essential tasks in robotics for\nlocalisation and integration of multi-session or multi-robot data. Traditional\nmethods rely on cameras or LiDARs to capture visual or geometric information\nbut struggle in challenging conditions like smoke or dust. Magnetometers, on\nthe other hand, detect magnetic fields, revealing features invisible to other\nsensors and remaining robust in such environments. In this paper, we introduce\nMag-Match, a novel method for extracting and describing features in 3D magnetic\nvector field maps to register different maps of the same area. Our feature\ndescriptor, based on higher-order derivatives of magnetic field maps, is\ninvariant to global orientation, eliminating the need for gravity-aligned\nmapping. To obtain these higher-order derivatives map-wide given point-wise\nmagnetometer data, we leverage a physics-informed Gaussian Process to perform\nefficient and recursive probabilistic inference of both the magnetic field and\nits derivatives. We evaluate Mag-Match in simulated and real-world experiments\nagainst a SIFT-based approach, demonstrating accurate map-to-map, robot-to-map,\nand robot-to-robot transformations - even without initial gravitational\nalignment.", "AI": {"tldr": "本文提出了一种名为Mag-Match的新方法，用于在3D磁场矢量图中提取和描述特征，以实现不同磁场地图的配准。该方法利用高阶导数作为特征描述符，并通过物理信息高斯过程进行高效推理，即使在没有重力对齐的情况下也能实现准确的地图匹配。", "motivation": "传统的基于摄像头或激光雷达的地图匹配和配准方法在烟雾或灰尘等恶劣环境下表现不佳。而磁力计能够检测到其他传感器无法捕捉的磁场特征，并且在这些恶劣环境中具有鲁棒性。因此，需要一种利用磁场信息进行地图匹配的鲁棒方法。", "method": "本文引入了Mag-Match方法，用于从3D磁场矢量图中提取和描述特征。其特征描述符基于磁场地图的高阶导数，并且对全局方向不变，无需重力对齐。为了从点式磁力计数据中获取全图范围的高阶导数，该方法利用了物理信息高斯过程（Gaussian Process）进行磁场及其导数的有效和递归概率推断。", "result": "Mag-Match在模拟和真实世界实验中与基于SIFT的方法进行了对比评估。结果表明，Mag-Match能够实现准确的地图到地图、机器人到地图以及机器人到机器人的转换，即使在没有初始重力对齐的情况下也能表现良好。", "conclusion": "Mag-Match提供了一种新颖且鲁棒的3D磁场矢量图配准方法。通过利用磁场高阶导数作为方向不变的特征描述符，并结合物理信息高斯过程进行高效推理，该方法克服了传统传感器在恶劣环境下的局限性，实现了高精度的地图匹配。"}}
{"id": "2508.15132", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2508.15132", "abs": "https://arxiv.org/abs/2508.15132", "authors": ["Nicholas Dwork", "Alex McManus", "Stephen Becker", "Gennifer T. Smith"], "title": "SPIRiT Regularization: Parallel MRI with a Combination of Sensitivity Encoding and Linear Predictability", "comment": null, "summary": "Accelerated Magnetic Resonance Imaging (MRI) permits high quality images from\nfewer samples that can be collected with a faster scan. Two established methods\nfor accelerating MRI include parallel imaging and compressed sensing. Two types\nof parallel imaging include linear predictability, which assumes that the\nFourier samples are linearly related, and sensitivity encoding, which\nincorporates a priori knowledge of the sensitivity maps. In this work, we\ncombine compressed sensing with both types of parallel imaging using a novel\nregularization term: SPIRiT regularization. When combined, the reconstructed\nimages are improved. We demonstrate results on data of a brain, a knee, and an\nankle.", "AI": {"tldr": "该研究结合了压缩感知与两种并行成像方法（线性可预测性与敏感度编码），并引入了SPIRiT正则化项，以提升加速MRI图像的重建质量。", "motivation": "MRI加速成像旨在通过更少的采样和更快的扫描时间获取高质量图像，以提高成像效率。", "method": "该研究将压缩感知与两种并行成像方法（线性可预测性及敏感度编码）相结合，并引入了一种新颖的SPIRiT正则化项。", "result": "结合这些方法后，重建图像的质量得到了改善。研究结果在脑部、膝盖和脚踝数据上得到了验证。", "conclusion": "通过将压缩感知与两种并行成像方法以及SPIRiT正则化项结合，可以有效提高加速MRI图像的重建质量。"}}
{"id": "2508.15085", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.15085", "abs": "https://arxiv.org/abs/2508.15085", "authors": ["MohamamdJavad Ardestani", "Ehsan Kamalloo", "Davood Rafiei"], "title": "LongRecall: A Structured Approach for Robust Recall Evaluation in Long-Form Text", "comment": null, "summary": "LongRecall. The completeness of machine-generated text, ensuring that it\ncaptures all relevant information, is crucial in domains such as medicine and\nlaw and in tasks like list-based question answering (QA), where omissions can\nhave serious consequences. However, existing recall metrics often depend on\nlexical overlap, leading to errors with unsubstantiated entities and\nparaphrased answers, while LLM-as-a-Judge methods with long holistic prompts\ncapture broader semantics but remain prone to misalignment and hallucinations\nwithout structured verification. We introduce LongRecall, a general three-stage\nrecall evaluation framework that decomposes answers into self-contained facts,\nsuccessively narrows plausible candidate matches through lexical and semantic\nfiltering, and verifies their alignment through structured entailment checks.\nThis design reduces false positives and false negatives while accommodating\ndiverse phrasings and contextual variations, serving as a foundational building\nblock for systematic recall assessment. We evaluate LongRecall on three\nchallenging long-form QA benchmarks using both human annotations and LLM-based\njudges, demonstrating substantial improvements in recall accuracy over strong\nlexical and LLM-as-a-Judge baselines.", "AI": {"tldr": "本文提出了LongRecall，一个三阶段的通用召回率评估框架，通过事实分解、候选匹配过滤和结构化蕴含检查，显著提高了机器生成文本的召回率评估准确性，解决了现有方法在词汇重叠和LLM判断中的局限性。", "motivation": "在医学、法律和列表式问答等领域，机器生成文本的完整性（确保包含所有相关信息）至关重要，遗漏可能导致严重后果。然而，现有召回率指标常依赖词汇重叠，易产生错误；而基于LLM的判断方法虽捕获更广泛语义，但存在对齐问题和幻觉，缺乏结构化验证。", "method": "LongRecall是一个通用的三阶段召回率评估框架：1. 将答案分解为独立的“事实”。2. 通过词汇和语义过滤逐步缩小可能的候选匹配。3. 通过结构化蕴含检查验证其对齐性。这种设计旨在减少假阳性和假阴性，同时适应不同的措辞和上下文变体。", "result": "LongRecall在三个具有挑战性的长篇问答基准上进行了评估，结合了人工标注和基于LLM的判断器。结果表明，LongRecall在召回率准确性方面比强大的词汇重叠和LLM-as-a-Judge基线有显著提升。", "conclusion": "LongRecall作为一个系统性召回率评估的基础构建模块，能够减少错误并适应多样化的表达和上下文变化。其设计通过分解、过滤和结构化验证，解决了现有召回率评估方法的不足，为高风险领域提供了更可靠的评估工具。"}}
{"id": "2508.15057", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15057", "abs": "https://arxiv.org/abs/2508.15057", "authors": ["Toqi Tahamid Sarker", "Mohamed Embaby", "Taminul Islam", "Amer AbuGhazaleh", "Khaled R Ahmed"], "title": "GasTwinFormer: A Hybrid Vision Transformer for Livestock Methane Emission Segmentation and Dietary Classification in Optical Gas Imaging", "comment": "Accepted for publication at ICCVW 2025", "summary": "Livestock methane emissions represent 32% of human-caused methane production,\nmaking automated monitoring critical for climate mitigation strategies. We\nintroduce GasTwinFormer, a hybrid vision transformer for real-time methane\nemission segmentation and dietary classification in optical gas imaging through\na novel Mix Twin encoder alternating between spatially-reduced global attention\nand locally-grouped attention mechanisms. Our architecture incorporates a\nlightweight LR-ASPP decoder for multi-scale feature aggregation and enables\nsimultaneous methane segmentation and dietary classification in a unified\nframework. We contribute the first comprehensive beef cattle methane emission\ndataset using OGI, containing 11,694 annotated frames across three dietary\ntreatments. GasTwinFormer achieves 74.47% mIoU and 83.63% mF1 for segmentation\nwhile maintaining exceptional efficiency with only 3.348M parameters, 3.428G\nFLOPs, and 114.9 FPS inference speed. Additionally, our method achieves perfect\ndietary classification accuracy (100%), demonstrating the effectiveness of\nleveraging diet-emission correlations. Extensive ablation studies validate each\narchitectural component, establishing GasTwinFormer as a practical solution for\nreal-time livestock emission monitoring. Please see our project page at\ngastwinformer.github.io.", "AI": {"tldr": "本文提出GasTwinFormer，一种混合视觉Transformer模型，用于通过光学气体成像(OGI)对牲畜甲烷排放进行实时分割和饮食分类。该模型采用新型Mix Twin编码器和轻量级LR-ASPP解码器，并在首个综合性OGI牛肉牛甲烷排放数据集上实现了高性能和高效率。", "motivation": "牲畜甲烷排放占人类活动甲烷生产的32%，因此自动化监测对于气候缓解策略至关重要。", "method": "研究引入了GasTwinFormer，一个混合视觉Transformer模型，用于甲烷排放分割和饮食分类。其核心是一个新颖的Mix Twin编码器，该编码器交替使用空间缩减的全局注意力机制和局部分组注意力机制。此外，模型采用轻量级LR-ASPP解码器进行多尺度特征聚合，并能在统一框架中同时实现甲烷分割和饮食分类。研究还构建了首个综合性牛肉牛甲烷排放OGI数据集，包含11,694帧带注释图像，涵盖三种饮食处理。", "result": "GasTwinFormer在分割任务上取得了74.47%的mIoU和83.63%的mF1，同时保持了卓越的效率（仅3.348M参数、3.428G FLOPs和114.9 FPS的推理速度）。在饮食分类任务上，该方法实现了100%的准确率。广泛的消融研究验证了每个架构组件的有效性。", "conclusion": "GasTwinFormer被确立为一种实用的实时牲畜排放监测解决方案，它通过利用饮食与排放的相关性，有效实现了甲烷排放的分割和饮食分类。"}}
{"id": "2508.15068", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15068", "abs": "https://arxiv.org/abs/2508.15068", "authors": ["Shuang Ao", "Gopal Rumchurn"], "title": "S3LoRA: Safe Spectral Sharpness-Guided Pruning in Adaptation of Agent Planner", "comment": "9 pages, 2 figures", "summary": "Adapting Large Language Models (LLMs) using parameter-efficient fine-tuning\n(PEFT) techniques such as LoRA has enabled powerful capabilities in LLM-based\nagents. However, these adaptations can unintentionally compromise safety\nalignment, leading to unsafe or unstable behaviors, particularly in agent\nplanning tasks. Existing safety-aware adaptation methods often require access\nto both base and instruction-tuned model checkpoints, which are frequently\nunavailable in practice, limiting their applicability. We propose S3LoRA (Safe\nSpectral Sharpness-Guided Pruning LoRA), a lightweight, data-free, and\nmodel-independent framework that mitigates safety risks in LoRA-adapted models\nby inspecting only the fine-tuned weight updates. We first introduce\nMagnitude-Aware Spherically Normalized SVD (MAS-SVD), which robustly analyzes\nthe structural properties of LoRA updates while preserving global magnitude\ninformation. We then design the Spectral Sharpness Index (SSI), a\nsharpness-aware metric to detect layers with highly concentrated and\npotentially unsafe updates. These layers are pruned post-hoc to reduce risk\nwithout sacrificing task performance. Extensive experiments and ablation\nstudies across agent planning and language generation tasks show that S3LoRA\nconsistently improves safety metrics while maintaining or improving utility\nmetrics and significantly reducing inference cost. These results establish\nS3LoRA as a practical and scalable solution for safely deploying LLM-based\nagents in real-world, resource-constrained, and safety-critical environments.", "AI": {"tldr": "S3LoRA是一种轻量级、无数据、模型无关的框架，通过分析LoRA微调后的权重更新来识别并修剪潜在不安全的层，从而在不牺牲任务性能的情况下提高LLM代理的安全性，并降低推理成本。", "motivation": "LoRA等PEFT技术在增强LLM代理能力的同时，可能无意中损害安全对齐，导致不安全或不稳定的行为。现有的安全感知适应方法通常需要基础模型和指令微调模型的检查点，但在实际中这些检查点往往不可用，限制了其适用性。", "method": "本文提出了S3LoRA（Safe Spectral Sharpness-Guided Pruning LoRA），一个轻量级、无数据、模型无关的框架，仅通过检查微调后的LoRA权重更新来缓解安全风险。该方法首先引入了Magnitude-Aware Spherically Normalized SVD (MAS-SVD) 来稳健分析LoRA更新的结构特性并保留全局幅度信息。随后，设计了Spectral Sharpness Index (SSI) 这一锐度感知指标，用于检测具有高度集中且可能不安全的更新层。最后，对这些层进行事后剪枝，以降低风险而不牺牲任务性能。", "result": "在代理规划和语言生成任务上的大量实验和消融研究表明，S3LoRA持续改进了安全指标，同时保持或改进了实用性指标，并显著降低了推理成本。", "conclusion": "S3LoRA为在真实世界、资源受限和安全关键环境中安全部署基于LLM的代理提供了一个实用且可扩展的解决方案。"}}
{"id": "2508.15732", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.15732", "abs": "https://arxiv.org/abs/2508.15732", "authors": ["Gargi Das", "Daegyun Choi", "Donghoon Kim"], "title": "Understanding and Utilizing Dynamic Coupling in Free-Floating Space Manipulators for On-Orbit Servicing", "comment": "17 pages, 7 figures, 2025 AAS/AIAA Astrodynamics Specialist\n  Conference", "summary": "This study proposes a dynamic coupling-informed trajectory optimization\nalgorithm for free-floating space manipulator systems (SMSs). Dynamic coupling\nbetween the base and the manipulator arms plays a critical role in influencing\nthe system's behavior. While prior research has predominantly focused on\nminimizing this coupling, often overlooking its potential advantages, this work\ninvestigates how dynamic coupling can instead be leveraged to improve\ntrajectory planning. Singular value decomposition (SVD) of the dynamic coupling\nmatrix is employed to identify the dominant components governing coupling\nbehavior. A quantitative metric is then formulated to characterize the strength\nand directionality of the coupling and is incorporated into a trajectory\noptimization framework. To assess the feasibility of the optimized trajectory,\na sliding mode control-based tracking controller is designed to generate the\nrequired joint torque inputs. Simulation results demonstrate that explicitly\naccounting for dynamic coupling in trajectory planning enables more informed\nand potentially more efficient operation, offering new directions for the\ncontrol of free-floating SMSs.", "AI": {"tldr": "本研究提出了一种动态耦合感知的轨迹优化算法，用于自由浮动空间机械臂系统，通过利用而非仅仅最小化动态耦合来提高轨迹规划效率。", "motivation": "动态耦合对自由浮动空间机械臂系统的行为影响至关重要。以往研究主要关注最小化耦合，但本研究旨在探索如何利用动态耦合来改进轨迹规划，而不是忽视其潜在优势。", "method": "该研究采用动态耦合矩阵的奇异值分解（SVD）来识别耦合行为的主导分量。接着，制定了一个量化指标来表征耦合的强度和方向性，并将其整合到轨迹优化框架中。为评估优化轨迹的可行性，设计了一个基于滑模控制的跟踪控制器来生成所需的关节扭矩输入。", "result": "仿真结果表明，在轨迹规划中明确考虑动态耦合能够实现更明智、可能更高效的操作。", "conclusion": "在轨迹规划中明确考虑动态耦合为自由浮动空间机械臂系统的控制提供了新的方向，能够实现更明智和高效的操作。"}}
{"id": "2508.15354", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.15354", "abs": "https://arxiv.org/abs/2508.15354", "authors": ["Chaoran Xiong", "Yulong Huang", "Fangwen Yu", "Changhao Chen", "Yue Wang", "Songpengchen Xia", "Ling Pei"], "title": "Sensing, Social, and Motion Intelligence in Embodied Navigation: A Comprehensive Survey", "comment": null, "summary": "Embodied navigation (EN) advances traditional navigation by enabling robots\nto perform complex egocentric tasks through sensing, social, and motion\nintelligence. In contrast to classic methodologies that rely on explicit\nlocalization and pre-defined maps, EN leverages egocentric perception and\nhuman-like interaction strategies. This survey introduces a comprehensive EN\nformulation structured into five stages: Transition, Observation, Fusion,\nReward-policy construction, and Action (TOFRA). The TOFRA framework serves to\nsynthesize the current state of the art, provide a critical review of relevant\nplatforms and evaluation metrics, and identify critical open research\nchallenges. A list of studies is available at\nhttps://github.com/Franky-X/Awesome-Embodied-Navigation.", "AI": {"tldr": "这篇综述介绍了具身导航（Embodied Navigation, EN），这是一种通过感知、社交和运动智能使机器人执行复杂自我中心任务的新范式。它提出了一个名为TOFRA的五阶段框架来系统化EN，并据此回顾了当前技术、平台、评估指标和开放挑战。", "motivation": "传统的导航方法依赖于明确的定位和预定义地图。具身导航旨在通过利用自我中心感知和类人交互策略，使机器人能够执行更复杂的自我中心任务，从而超越传统方法的局限性。本研究的动机是系统地总结这一新兴领域，并识别其关键挑战。", "method": "本研究提出了一种名为TOFRA的具身导航综合公式，包含五个阶段：Transition（过渡）、Observation（观察）、Fusion（融合）、Reward-policy construction（奖励策略构建）和Action（行动）。TOFRA框架被用于综合当前技术水平、批判性地回顾相关平台和评估指标，并识别关键的开放研究挑战。", "result": "TOFRA框架成功地综合了具身导航的最新技术，对相关平台和评估指标进行了批判性审查，并明确指出了该领域面临的关键开放研究挑战。", "conclusion": "具身导航通过其自我中心感知和类人交互策略，为机器人执行复杂任务提供了新的可能性。TOFRA框架为理解和分析具身导航提供了一个结构化的视角，并为未来的研究指明了方向和挑战。"}}
{"id": "2508.15151", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15151", "abs": "https://arxiv.org/abs/2508.15151", "authors": ["Jeonghyun Noh", "Hyun-Jic Oh", "Byungju Chae", "Won-Ki Jeong"], "title": "Zero-shot Volumetric CT Super-Resolution using 3D Gaussian Splatting with Upsampled 2D X-ray Projection Priors", "comment": null, "summary": "Computed tomography (CT) is widely used in clinical diagnosis, but acquiring\nhigh-resolution (HR) CT is limited by radiation exposure risks. Deep\nlearning-based super-resolution (SR) methods have been studied to reconstruct\nHR from low-resolution (LR) inputs. While supervised SR approaches have shown\npromising results, they require large-scale paired LR-HR volume datasets that\nare often unavailable. In contrast, zero-shot methods alleviate the need for\npaired data by using only a single LR input, but typically struggle to recover\nfine anatomical details due to limited internal information. To overcome these,\nwe propose a novel zero-shot 3D CT SR framework that leverages upsampled 2D\nX-ray projection priors generated by a diffusion model. Exploiting the\nabundance of HR 2D X-ray data, we train a diffusion model on large-scale 2D\nX-ray projection and introduce a per-projection adaptive sampling strategy. It\nselects the generative process for each projection, thus providing HR\nprojections as strong external priors for 3D CT reconstruction. These\nprojections serve as inputs to 3D Gaussian splatting for reconstructing a 3D CT\nvolume. Furthermore, we propose negative alpha blending (NAB-GS) that allows\nnegative values in Gaussian density representation. NAB-GS enables residual\nlearning between LR and diffusion-based projections, thereby enhancing\nhigh-frequency structure reconstruction. Experiments on two datasets show that\nour method achieves superior quantitative and qualitative results for 3D CT SR.", "AI": {"tldr": "本文提出了一种新颖的零样本3D CT超分辨率框架，通过扩散模型生成高分辨率2D X射线投影作为先验，并结合改进的3D高斯泼溅（NAB-GS）方法，有效重建高细节的3D CT图像。", "motivation": "高分辨率CT图像采集存在辐射风险，而深度学习超分辨率方法面临挑战：监督方法需要难以获取的大规模配对LR-HR数据集；零样本方法虽无需配对数据，但因内部信息有限，难以恢复精细解剖细节。", "method": "1. 训练一个基于扩散模型生成器，利用丰富的HR 2D X射线数据生成高分辨率2D X射线投影，并引入逐投影自适应采样策略。2. 将这些高分辨率投影作为强大的外部先验输入到3D高斯泼溅模型中重建3D CT体素。3. 提出负Alpha混合高斯泼溅（NAB-GS），允许高斯密度表示中存在负值，从而实现LR与扩散生成投影之间的残差学习，增强高频结构重建。", "result": "在两个数据集上的实验结果表明，本文方法在3D CT超分辨率任务中取得了优越的定量和定性表现。", "conclusion": "所提出的零样本3D CT超分辨率框架，通过利用扩散模型生成的2D X射线先验和新颖的NAB-GS方法，有效克服了现有零样本方法的局限性，实现了卓越的3D CT高分辨率重建，尤其在恢复精细解剖细节方面表现突出。"}}
{"id": "2508.15090", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15090", "abs": "https://arxiv.org/abs/2508.15090", "authors": ["Matt Pauk", "Maria Leonor Pacheco"], "title": "Mapping the Course for Prompt-based Structured Prediction", "comment": null, "summary": "LLMs have been shown to be useful for a variety of language tasks, without\nrequiring task-specific fine-tuning. However, these models often struggle with\nhallucinations and complex reasoning problems due to their autoregressive\nnature. We propose to address some of these issues, specifically in the area of\nstructured prediction, by combining LLMs with combinatorial inference in an\nattempt to marry the predictive power of LLMs with the structural consistency\nprovided by inference methods. We perform exhaustive experiments in an effort\nto understand which prompting strategies can effectively estimate LLM\nconfidence values for use with symbolic inference, and show that, regardless of\nthe prompting strategy, the addition of symbolic inference on top of prompting\nalone leads to more consistent and accurate predictions. Additionally, we show\nthat calibration and fine-tuning using structured prediction objectives leads\nto increased performance for challenging tasks, showing that structured\nlearning is still valuable in the era of LLMs.", "AI": {"tldr": "本文提出将大型语言模型（LLMs）与组合推理相结合，以解决LLMs在结构化预测任务中幻觉和复杂推理的挑战，从而提高预测的一致性和准确性。", "motivation": "LLMs在多种语言任务中表现出色，但由于其自回归特性，常在幻觉和复杂推理问题上遇到困难，尤其是在需要结构化输出的预测任务中。", "method": "研究方法包括：1) 将LLMs的预测能力与组合推理提供的结构一致性相结合。2) 实验评估不同的提示策略，以有效估计LLM置信度，供符号推理使用。3) 使用结构化预测目标进行校准和微调。", "result": "实验结果表明，无论采用何种提示策略，在仅提示的基础上增加符号推理都能带来更一致和准确的预测。此外，使用结构化预测目标进行校准和微调可以进一步提高挑战性任务的性能。", "conclusion": "结合LLMs与符号推理能有效提升结构化预测任务的性能，并且在LLM时代，结构化学习仍然具有重要的价值。"}}
{"id": "2508.15093", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15093", "abs": "https://arxiv.org/abs/2508.15093", "authors": ["Yan Luo", "Drake Du", "Hao Huang", "Yi Fang", "Mengyu Wang"], "title": "CurveFlow: Curvature-Guided Flow Matching for Image Generation", "comment": null, "summary": "Existing rectified flow models are based on linear trajectories between data\nand noise distributions. This linearity enforces zero curvature, which can\ninadvertently force the image generation process through low-probability\nregions of the data manifold. A key question remains underexplored: how does\nthe curvature of these trajectories correlate with the semantic alignment\nbetween generated images and their corresponding captions, i.e., instructional\ncompliance? To address this, we introduce CurveFlow, a novel flow matching\nframework designed to learn smooth, non-linear trajectories by directly\nincorporating curvature guidance into the flow path. Our method features a\nrobust curvature regularization technique that penalizes abrupt changes in the\ntrajectory's intrinsic dynamics.Extensive experiments on MS COCO 2014 and 2017\ndemonstrate that CurveFlow achieves state-of-the-art performance in\ntext-to-image generation, significantly outperforming both standard rectified\nflow variants and other non-linear baselines like Rectified Diffusion. The\nimprovements are especially evident in semantic consistency metrics such as\nBLEU, METEOR, ROUGE, and CLAIR. This confirms that our curvature-aware modeling\nsubstantially enhances the model's ability to faithfully follow complex\ninstructions while simultaneously maintaining high image quality. The code is\nmade publicly available at\nhttps://github.com/Harvard-AI-and-Robotics-Lab/CurveFlow.", "AI": {"tldr": "CurveFlow引入曲率指导学习平滑非线性轨迹，显著提升了文本到图像生成的语义一致性和图像质量，超越现有线性流模型。", "motivation": "现有整流流模型基于线性轨迹（零曲率），可能导致生成过程穿过低概率区域，并影响生成图像与文本描述的语义对齐（即指令遵循性）。研究轨迹曲率与语义对齐之间关系的重要性尚未被充分探索。", "method": "提出了CurveFlow，一个新颖的流匹配框架。通过直接将曲率指导融入流路径，学习平滑的非线性轨迹。该方法包含一种鲁棒的曲率正则化技术，惩罚轨迹内在动力学的突然变化。", "result": "在MS COCO 2014和2017数据集上的文本到图像生成任务中，CurveFlow实现了最先进的性能。它显著优于标准的整流流变体和其他非线性基线（如Rectified Diffusion）。尤其在BLEU、METEOR、ROUGE和CLAIR等语义一致性指标上，改进尤为明显。", "conclusion": "曲率感知建模显著增强了模型忠实遵循复杂指令的能力，同时保持了高图像质量。这证实了轨迹曲率对生成质量和语义对齐的重要性。"}}
{"id": "2508.15118", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15118", "abs": "https://arxiv.org/abs/2508.15118", "authors": ["Jennifer Leigh", "Dimitrios Letsios", "Alessandro Mella", "Lucio Machetti", "Francesca Toni"], "title": "Argumentation for Explainable Workforce Optimisation (with Appendix)", "comment": "Accepted to PAIS 2025", "summary": "Workforce management is a complex problem optimising the makespan and travel\ndistance required for a team of operators to complete a set of jobs, using a\nset of instruments. A crucial challenge in workforce management is\naccommodating changes at execution time so that explanations are provided to\nall stakeholders involved. Here, we show that, by understanding workforce\nmanagement as abstract argumentation in an industrial application, we can\naccommodate change and obtain faithful explanations. We show, with a user\nstudy, that our tool and explanations lead to faster and more accurate problem\nsolving than conventional solutions by hand.", "AI": {"tldr": "本文提出将劳动力管理建模为抽象论证，以应对执行时期的变化并提供可信解释，用户研究表明其工具和解释比传统手动解决方案能更快、更准确地解决问题。", "motivation": "劳动力管理是一个复杂的问题，需要优化团队完成任务所需的总工期和行程距离。其核心挑战在于如何适应执行时的变化，并向所有相关方提供解释。", "method": "将劳动力管理理解为工业应用中的抽象论证问题。开发了一个工具来实施这一方法，并通过用户研究进行评估。", "result": "通过用户研究表明，本文提出的工具和解释比传统手动解决方案能更快、更准确地解决问题。", "conclusion": "通过将劳动力管理建模为抽象论证，可以有效地适应执行时期的变化，并提供可信的解释，从而提高问题解决的效率和准确性。"}}
{"id": "2508.15427", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15427", "abs": "https://arxiv.org/abs/2508.15427", "authors": ["Huy Hoang Nguyen", "Johannes Huemer", "Markus Murschitz", "Tobias Glueck", "Minh Nhat Vu", "Andreas Kugi"], "title": "Lang2Lift: A Framework for Language-Guided Pallet Detection and Pose Estimation Integrated in Autonomous Outdoor Forklift Operation", "comment": "8 pages, 7 figures", "summary": "The logistics and construction industries face persistent challenges in\nautomating pallet handling, especially in outdoor environments with variable\npayloads, inconsistencies in pallet quality and dimensions, and unstructured\nsurroundings. In this paper, we tackle automation of a critical step in pallet\ntransport: the pallet pick-up operation. Our work is motivated by labor\nshortages, safety concerns, and inefficiencies in manually locating and\nretrieving pallets under such conditions. We present Lang2Lift, a framework\nthat leverages foundation models for natural language-guided pallet detection\nand 6D pose estimation, enabling operators to specify targets through intuitive\ncommands such as \"pick up the steel beam pallet near the crane.\" The perception\npipeline integrates Florence-2 and SAM-2 for language-grounded segmentation\nwith FoundationPose for robust pose estimation in cluttered, multi-pallet\noutdoor scenes under variable lighting. The resulting poses feed into a motion\nplanning module for fully autonomous forklift operation. We validate Lang2Lift\non the ADAPT autonomous forklift platform, achieving 0.76 mIoU pallet\nsegmentation accuracy on a real-world test dataset. Timing and error analysis\ndemonstrate the system's robustness and confirm its feasibility for deployment\nin operational logistics and construction environments. Video demonstrations\nare available at https://eric-nguyen1402.github.io/lang2lift.github.io/", "AI": {"tldr": "Lang2Lift是一个利用基础模型实现自然语言引导的托盘检测和6D姿态估计框架，旨在解决物流和建筑行业在室外复杂环境下自动托盘搬运的挑战，并已在自主叉车平台上验证其可行性。", "motivation": "物流和建筑行业在自动化托盘搬运方面面临持续挑战，尤其是在室外、有效载荷可变、托盘质量和尺寸不一致以及非结构化环境中。该研究的动机是劳动力短缺、安全隐患以及在这些条件下手动定位和检索托盘的低效率问题。", "method": "Lang2Lift框架利用基础模型实现自然语言引导的托盘检测和6D姿态估计。感知管道集成了Florence-2和SAM-2进行语言接地分割，并使用FoundationPose在杂乱、多托盘、光照可变的室外场景中进行鲁棒的姿态估计。得到的姿态信息被输入到运动规划模块，以实现全自主叉车操作。该系统在ADAPT自主叉车平台上进行了验证。", "result": "Lang2Lift在真实世界测试数据集上实现了0.76 mIoU的托盘分割精度。时间分析和误差分析表明系统具有鲁棒性，并证实了其在实际物流和建筑环境中部署的可行性。", "conclusion": "Lang2Lift框架通过结合基础模型和语言引导，为在复杂室外环境下实现自主托盘抓取操作提供了一个可行且鲁棒的解决方案，有望解决当前行业面临的自动化挑战。"}}
{"id": "2508.15236", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15236", "abs": "https://arxiv.org/abs/2508.15236", "authors": ["Jiamu Wang", "Keunho Byeon", "Jinsol Song", "Anh Nguyen", "Sangjeong Ahn", "Sung Hak Lee", "Jin Tae Kwak"], "title": "Pathology-Informed Latent Diffusion Model for Anomaly Detection in Lymph Node Metastasis", "comment": null, "summary": "Anomaly detection is an emerging approach in digital pathology for its\nability to efficiently and effectively utilize data for disease diagnosis.\nWhile supervised learning approaches deliver high accuracy, they rely on\nextensively annotated datasets, suffering from data scarcity in digital\npathology. Unsupervised anomaly detection, however, offers a viable alternative\nby identifying deviations from normal tissue distributions without requiring\nexhaustive annotations. Recently, denoising diffusion probabilistic models have\ngained popularity in unsupervised anomaly detection, achieving promising\nperformance in both natural and medical imaging datasets. Building on this, we\nincorporate a vision-language model with a diffusion model for unsupervised\nanomaly detection in digital pathology, utilizing histopathology prompts during\nreconstruction. Our approach employs a set of pathology-related keywords\nassociated with normal tissues to guide the reconstruction process,\nfacilitating the differentiation between normal and abnormal tissues. To\nevaluate the effectiveness of the proposed method, we conduct experiments on a\ngastric lymph node dataset from a local hospital and assess its generalization\nability under domain shift using a public breast lymph node dataset. The\nexperimental results highlight the potential of the proposed method for\nunsupervised anomaly detection across various organs in digital pathology.\nCode: https://github.com/QuIIL/AnoPILaD.", "AI": {"tldr": "该研究提出了一种结合视觉-语言模型和去噪扩散概率模型的无监督异常检测方法，利用组织病理学提示词指导正常组织重建，以解决数字病理中数据标注稀缺的问题，并在胃和乳腺淋巴结数据集上展现了良好性能和泛化能力。", "motivation": "数字病理学中的异常检测方法通常需要大量标注数据（监督学习），但数字病理领域存在数据稀缺问题。无监督异常检测提供了一种无需详尽标注的替代方案，通过识别与正常组织分布的偏差来发现异常。最近流行的去噪扩散概率模型在无监督异常检测中表现出色，但仍有改进空间。", "method": "本研究将视觉-语言模型（VLM）与扩散模型相结合，用于数字病理中的无监督异常检测。该方法在重建过程中利用与正常组织相关的病理学关键词（组织病理学提示词）来引导，从而促进正常和异常组织之间的区分。", "result": "该方法在来自当地医院的胃淋巴结数据集上进行了有效性评估。为了测试其在领域偏移下的泛化能力，还在一个公共乳腺淋巴结数据集上进行了评估。实验结果表明，所提出的方法在数字病理学中跨各种器官的无监督异常检测方面具有潜力。", "conclusion": "该研究提出的结合视觉-语言模型和扩散模型的无监督异常检测方法，通过利用组织病理学提示词指导重建，能够有效区分正常和异常组织。实验结果证实了该方法在不同器官（胃和乳腺）的数字病理异常检测中的有效性和泛化能力，展现了其广阔的应用前景。"}}
{"id": "2508.15096", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.15096", "abs": "https://arxiv.org/abs/2508.15096", "authors": ["Rabeeh Karimi Mahabadi", "Sanjeev Satheesh", "Shrimai Prabhumoye", "Mostofa Patwary", "Mohammad Shoeybi", "Bryan Catanzaro"], "title": "Nemotron-CC-Math: A 133 Billion-Token-Scale High Quality Math Pretraining Dataset", "comment": null, "summary": "Pretraining large language models (LLMs) on high-quality, structured data\nsuch as mathematics and code substantially enhances reasoning capabilities.\nHowever, existing math-focused datasets built from Common Crawl suffer from\ndegraded quality due to brittle extraction heuristics, lossy HTML-to-text\nconversion, and the failure to reliably preserve mathematical structure. In\nthis work, we introduce Nemotron-CC-Math, a large-scale, high-quality\nmathematical corpus constructed from Common Crawl using a novel,\ndomain-agnostic pipeline specifically designed for robust scientific text\nextraction.\n  Unlike previous efforts, our pipeline recovers math across various formats\n(e.g., MathJax, KaTeX, MathML) by leveraging layout-aware rendering with lynx\nand a targeted LLM-based cleaning stage. This approach preserves the structural\nintegrity of equations and code blocks while removing boilerplate,\nstandardizing notation into LaTeX representation, and correcting\ninconsistencies.\n  We collected a large, high-quality math corpus, namely Nemotron-CC-Math-3+\n(133B tokens) and Nemotron-CC-Math-4+ (52B tokens). Notably,\nNemotron-CC-Math-4+ not only surpasses all prior open math datasets-including\nMegaMath, FineMath, and OpenWebMath-but also contains 5.5 times more tokens\nthan FineMath-4+, which was previously the highest-quality math pretraining\ndataset. When used to pretrain a Nemotron-T 8B model, our corpus yields +4.8 to\n+12.6 gains on MATH and +4.6 to +14.3 gains on MBPP+ over strong baselines,\nwhile also improving general-domain performance on MMLU and MMLU-Stem.\n  We present the first pipeline to reliably extract scientific\ncontent--including math--from noisy web-scale data, yielding measurable gains\nin math, code, and general reasoning, and setting a new state of the art among\nopen math pretraining corpora. To support open-source efforts, we release our\ncode and datasets.", "AI": {"tldr": "本文介绍了一种新的、高质量的数学语料库构建管道Nemotron-CC-Math，它能从Common Crawl中可靠地提取科学文本，克服了现有数据集的质量问题，并在LLM预训练中显著提升了数学、代码和通用推理能力。", "motivation": "现有从Common Crawl构建的数学数据集质量不佳，存在提取启发式规则脆弱、HTML到文本转换有损以及数学结构难以可靠保存的问题。然而，高质量的结构化数据（如数学和代码）对于提升大型语言模型（LLM）的推理能力至关重要。", "method": "本文提出了一种新颖的、领域无关的管道，专门用于鲁棒的科学文本提取。该管道通过结合使用lynx进行布局感知渲染和LLM驱动的清理阶段，能够恢复各种格式（如MathJax、KaTeX、MathML）的数学内容。它能保留方程和代码块的结构完整性，同时去除样板文本，将符号标准化为LaTeX表示，并纠正不一致性。", "result": "研究团队构建了大型高质量数学语料库Nemotron-CC-Math-3+ (133B tokens) 和 Nemotron-CC-Math-4+ (52B tokens)。其中，Nemotron-CC-Math-4+不仅超越了所有先前的开放数学数据集，且比之前最高质量的FineMath-4+多出5.5倍的tokens。使用该语料库预训练Nemotron-T 8B模型后，在MATH任务上取得了+4.8到+12.6的提升，在MBPP+上取得了+4.6到+14.3的提升，同时还改善了MMLU和MMLU-Stem上的通用领域性能。", "conclusion": "本文提出了第一个能够从嘈杂的网络规模数据中可靠提取包括数学在内的科学内容的管道，从而在数学、代码和通用推理方面取得了可衡量的性能提升，并在开放数学预训练语料库中建立了新的SOTA。为支持开源工作，研究团队已发布了代码和数据集。"}}
{"id": "2508.15130", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15130", "abs": "https://arxiv.org/abs/2508.15130", "authors": ["Vaishnav Ramesh", "Haining Wang", "Md Jahidul Islam"], "title": "HiRQA: Hierarchical Ranking and Quality Alignment for Opinion-Unaware Image Quality Assessment", "comment": "10 pages, 8 figures", "summary": "Despite significant progress in no-reference image quality assessment\n(NR-IQA), dataset biases and reliance on subjective labels continue to hinder\ntheir generalization performance. We propose HiRQA, Hierarchical Ranking and\nQuality Alignment), a self-supervised, opinion-unaware framework that offers a\nhierarchical, quality-aware embedding through a combination of ranking and\ncontrastive learning. Unlike prior approaches that depend on pristine\nreferences or auxiliary modalities at inference time, HiRQA predicts quality\nscores using only the input image. We introduce a novel higher-order ranking\nloss that supervises quality predictions through relational ordering across\ndistortion pairs, along with an embedding distance loss that enforces\nconsistency between feature distances and perceptual differences. A\ntraining-time contrastive alignment loss, guided by structured textual prompts,\nfurther enhances the learned representation. Trained only on synthetic\ndistortions, HiRQA generalizes effectively to authentic degradations, as\ndemonstrated through evaluation on various distortions such as lens flare,\nhaze, motion blur, and low-light conditions. For real-time deployment, we\nintroduce \\textbf{HiRQA-S}, a lightweight variant with an inference time of\nonly 3.5 ms per image. Extensive experiments across synthetic and authentic\nbenchmarks validate HiRQA's state-of-the-art (SOTA) performance, strong\ngeneralization ability, and scalability.", "AI": {"tldr": "HiRQA是一种自监督、无主观意见的无参考图像质量评估（NR-IQA）框架，通过分层排序和对比学习生成质量感知嵌入。它克服了数据集偏差，在真实退化场景中表现出强大的泛化能力，并提供了一个轻量级版本HiRQA-S用于实时部署。", "motivation": "现有无参考图像质量评估（NR-IQA）方法受限于数据集偏差和对主观标签的依赖，导致其泛化性能不佳。", "method": "本文提出了HiRQA框架，一个自监督、无主观意见的NR-IQA方法。它通过结合排序和对比学习，提供分层、质量感知的嵌入。主要创新包括：1) 引入新颖的高阶排序损失，通过失真对之间的关系顺序监督质量预测；2) 提出嵌入距离损失，强制特征距离和感知差异之间的一致性；3) 使用结构化文本提示引导训练时的对比对齐损失，以增强学习到的表示。HiRQA仅使用输入图像预测质量分数，无需参考图像或辅助模态。此外，还推出了轻量级版本HiRQA-S，用于实时部署。", "result": "HiRQA仅在合成失真上训练，却能有效泛化到真实的退化（如镜头眩光、雾霾、运动模糊和低光照条件）。在合成和真实基准测试中，HiRQA均达到了最先进（SOTA）的性能，展现出强大的泛化能力和可扩展性。HiRQA-S作为轻量级变体，每张图像的推理时间仅为3.5毫秒。", "conclusion": "HiRQA通过其自监督、无主观意见的分层学习框架，成功解决了现有NR-IQA方法的数据集偏差和主观标签依赖问题。它在各种失真类型上表现出卓越的泛化能力和SOTA性能，并提供了实时部署的轻量级解决方案，具有显著的实用价值。"}}
{"id": "2508.15119", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.15119", "abs": "https://arxiv.org/abs/2508.15119", "authors": ["Rachel Ma", "Jingyi Qu", "Andreea Bobu", "Dylan Hadfield-Menell"], "title": "Open-Universe Assistance Games", "comment": "7 pages + 2 pages references + 7 pages appendix", "summary": "Embodied AI agents must infer and act in an interpretable way on diverse\nhuman goals and preferences that are not predefined. To formalize this setting,\nwe introduce Open-Universe Assistance Games (OU-AGs), a framework where the\nagent must reason over an unbounded and evolving space of possible goals. In\nthis context, we introduce GOOD (GOals from Open-ended Dialogue), a\ndata-efficient, online method that extracts goals in the form of natural\nlanguage during an interaction with a human, and infers a distribution over\nnatural language goals. GOOD prompts an LLM to simulate users with different\ncomplex intents, using its responses to perform probabilistic inference over\ncandidate goals. This approach enables rich goal representations and\nuncertainty estimation without requiring large offline datasets. We evaluate\nGOOD in a text-based grocery shopping domain and in a text-operated simulated\nhousehold robotics environment (AI2Thor), using synthetic user profiles. Our\nmethod outperforms a baseline without explicit goal tracking, as confirmed by\nboth LLM-based and human evaluations.", "AI": {"tldr": "本文提出了一种名为GOOD的在线方法，使具身AI代理能够通过开放式对话从人类那里推断出未预定义的、开放式目标，并估计目标的不确定性，而无需大量离线数据集。", "motivation": "具身AI代理需要以可解释的方式推断并响应多样化、未预定义的人类目标和偏好。现有的框架可能无法处理无限且不断演变的目标空间。", "method": "引入了“开放宇宙协助博弈”（Open-Universe Assistance Games, OU-AGs）框架，用于形式化代理需要推理无限且不断演变的目标空间。在此背景下，提出了GOOD（GOals from Open-ended Dialogue）方法，它是一种数据高效的在线方法，通过与人类的交互从自然语言中提取目标。GOOD提示大型语言模型（LLM）模拟具有不同复杂意图的用户，并利用其响应对候选目标执行概率推断，从而实现丰富的目标表示和不确定性估计。", "result": "GOOD方法在文本杂货店购物领域和文本操作的模拟家庭机器人环境（AI2Thor）中，使用合成用户配置文件进行了评估。结果表明，该方法优于没有明确目标跟踪的基线，并得到了基于LLM和人类评估的证实。", "conclusion": "GOOD提供了一种数据高效的在线方法，使具身AI代理能够从开放式对话中推断出开放式自然语言目标，并估计其不确定性，克服了传统方法对预定义目标和大量离线数据集的依赖。"}}
{"id": "2508.15501", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15501", "abs": "https://arxiv.org/abs/2508.15501", "authors": ["Deyu Zhang", "Xicheng Zhang", "Jiahao Li", "Tingting Long", "Xunhua Dai", "Yongjian Fu", "Jinrui Zhang", "Ju Ren", "Yaoxue Zhang"], "title": "LLM-Driven Self-Refinement for Embodied Drone Task Planning", "comment": "14pages", "summary": "We introduce SRDrone, a novel system designed for self-refinement task\nplanning in industrial-grade embodied drones. SRDrone incorporates two key\ntechnical contributions: First, it employs a continuous state evaluation\nmethodology to robustly and accurately determine task outcomes and provide\nexplanatory feedback. This approach supersedes conventional reliance on\nsingle-frame final-state assessment for continuous, dynamic drone operations.\nSecond, SRDrone implements a hierarchical Behavior Tree (BT) modification\nmodel. This model integrates multi-level BT plan analysis with a constrained\nstrategy space to enable structured reflective learning from experience.\nExperimental results demonstrate that SRDrone achieves a 44.87% improvement in\nSuccess Rate (SR) over baseline methods. Furthermore, real-world deployment\nutilizing an experience base optimized through iterative self-refinement\nattains a 96.25% SR. By embedding adaptive task refinement capabilities within\nan industrial-grade BT planning framework, SRDrone effectively integrates the\ngeneral reasoning intelligence of Large Language Models (LLMs) with the\nstringent physical execution constraints inherent to embodied drones. Code is\navailable at https://github.com/ZXiiiC/SRDrone.", "AI": {"tldr": "SRDrone是一个针对工业级具身无人机的自完善任务规划系统，通过连续状态评估和分层行为树修改模型，显著提高了任务成功率，有效结合了LLM推理能力与无人机物理执行约束。", "motivation": "传统的单帧最终状态评估不足以应对无人机动态操作中任务结果的鲁棒和准确判断。具身无人机需要一种机制来实现结构化的反思性学习和自适应任务精炼，以有效整合大型语言模型（LLM）的通用推理智能与无人机严格的物理执行约束。", "method": "SRDrone采用了两种关键技术：1. 连续状态评估方法，用于鲁棒准确地确定任务结果并提供解释性反馈，取代了传统的单帧最终状态评估。2. 分层行为树（BT）修改模型，该模型结合了多级BT计划分析和受限策略空间，以实现从经验中进行结构化的反思性学习。", "result": "实验结果表明，SRDrone的任务成功率（SR）比基线方法提高了44.87%。此外，通过迭代自完善优化的经验库在实际部署中达到了96.25%的SR。", "conclusion": "SRDrone通过在工业级行为树规划框架中嵌入自适应任务精炼能力，成功地将大型语言模型（LLM）的通用推理智能与具身无人机固有的严格物理执行约束有效结合起来。"}}
{"id": "2508.15251", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15251", "abs": "https://arxiv.org/abs/2508.15251", "authors": ["Aqib Nazir Mir", "Danish Raza Rizvi"], "title": "Explainable Knowledge Distillation for Efficient Medical Image Classification", "comment": null, "summary": "This study comprehensively explores knowledge distillation frameworks for\nCOVID-19 and lung cancer classification using chest X-ray (CXR) images. We\nemploy high-capacity teacher models, including VGG19 and lightweight Vision\nTransformers (Visformer-S and AutoFormer-V2-T), to guide the training of a\ncompact, hardware-aware student model derived from the OFA-595 supernet. Our\napproach leverages hybrid supervision, combining ground-truth labels with\nteacher models' soft targets to balance accuracy and computational efficiency.\nWe validate our models on two benchmark datasets: COVID-QU-Ex and LCS25000,\ncovering multiple classes, including COVID-19, healthy, non-COVID pneumonia,\nlung, and colon cancer. To interpret the spatial focus of the models, we employ\nScore-CAM-based visualizations, which provide insight into the reasoning\nprocess of both teacher and student networks. The results demonstrate that the\ndistilled student model maintains high classification performance with\nsignificantly reduced parameters and inference time, making it an optimal\nchoice in resource-constrained clinical environments. Our work underscores the\nimportance of combining model efficiency with explainability for practical,\ntrustworthy medical AI solutions.", "AI": {"tldr": "本研究利用知识蒸馏技术，通过高容量教师模型指导轻量级学生模型，实现了在资源受限环境下对胸部X光图像进行高效且可解释的COVID-19和肺癌分类。", "motivation": "在资源受限的临床环境中，需要开发既能保持高分类性能又具有计算效率和可解释性的医疗AI解决方案，以应对COVID-19和肺癌的诊断挑战。", "method": "研究采用知识蒸馏框架，使用VGG19、Visformer-S和AutoFormer-V2-T等高容量模型作为教师，指导从OFA-595超网派生出的紧凑型、硬件感知学生模型。方法结合了地面真值标签和教师模型的软目标进行混合监督训练。模型在COVID-QU-Ex和LCS25000两个基准数据集上进行验证，并使用Score-CAM进行可视化解释。", "result": "蒸馏后的学生模型在显著减少参数和推理时间的同时，保持了高分类性能，证明其在资源受限的临床环境中是一个理想的选择。", "conclusion": "本研究强调了将模型效率与可解释性相结合对于构建实用、可信赖的医疗AI解决方案的重要性，特别是在资源有限的临床环境中。"}}
{"id": "2508.15139", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.15139", "abs": "https://arxiv.org/abs/2508.15139", "authors": ["Zijie Wang", "Eduardo Blanco"], "title": "Identifying and Answering Questions with False Assumptions: An Interpretable Approach", "comment": "To appear at EMNLP 2025 Main conference", "summary": "People often ask questions with false assumptions, a type of question that\ndoes not have regular answers. Answering such questions require first\nidentifying the false assumptions. Large Language Models (LLMs) often generate\nmisleading answers because of hallucinations. In this paper, we focus on\nidentifying and answering questions with false assumptions in several domains.\nWe first investigate to reduce the problem to fact verification. Then, we\npresent an approach leveraging external evidence to mitigate hallucinations.\nExperiments with five LLMs demonstrate that (1) incorporating retrieved\nevidence is beneficial and (2) generating and validating atomic assumptions\nyields more improvements and provides an interpretable answer by specifying the\nfalse assumptions.", "AI": {"tldr": "本文提出了一种利用外部证据和原子假设验证的方法，以识别并回答含有虚假假设的问题，从而减少大型语言模型（LLMs）的幻觉并提高答案的可解释性。", "motivation": "大型语言模型（LLMs）在回答含有虚假假设的问题时，常因幻觉而产生误导性答案，这类问题无法直接回答，需要先识别出虚假假设。", "method": "研究首先将问题简化为事实核查。接着，提出了一种利用外部证据来缓解幻觉的方法。此外，还通过生成和验证原子假设来进一步提升效果。", "result": "实验结果表明：1) 结合检索到的证据对回答此类问题有益；2) 生成和验证原子假设能带来更多改进，并通过明确指出虚假假设来提供更可解释的答案。", "conclusion": "通过整合外部证据和对原子假设的生成与验证，可以有效识别并回答含有虚假假设的问题，显著提升LLMs在此类任务上的表现和答案的可解释性。"}}
{"id": "2508.15158", "categories": ["cs.CV", "cs.DC"], "pdf": "https://arxiv.org/pdf/2508.15158", "abs": "https://arxiv.org/abs/2508.15158", "authors": ["Md. Nurul Absur", "Abhinav Kumar", "Swastik Brahma", "Saptarshi Debroy"], "title": "Reliable Multi-view 3D Reconstruction for `Just-in-time' Edge Environments", "comment": "11 Pages, 7 Figures", "summary": "Multi-view 3D reconstruction applications are revolutionizing critical use\ncases that require rapid situational-awareness, such as emergency response,\ntactical scenarios, and public safety. In many cases, their near-real-time\nlatency requirements and ad-hoc needs for compute resources necessitate\nadoption of `Just-in-time' edge environments where the system is set up on the\nfly to support the applications during the mission lifetime. However,\nreliability issues can arise from the inherent dynamism and operational\nadversities of such edge environments, resulting in spatiotemporally correlated\ndisruptions that impact the camera operations, which can lead to sustained\ndegradation of reconstruction quality. In this paper, we propose a novel\nportfolio theory inspired edge resource management strategy for reliable\nmulti-view 3D reconstruction against possible system disruptions. Our proposed\nmethodology can guarantee reconstruction quality satisfaction even when the\ncameras are prone to spatiotemporally correlated disruptions. The portfolio\ntheoretic optimization problem is solved using a genetic algorithm that\nconverges quickly for realistic system settings. Using publicly available and\ncustomized 3D datasets, we demonstrate the proposed camera selection strategy's\nbenefits in guaranteeing reliable 3D reconstruction against traditional\nbaseline strategies, under spatiotemporal disruptions.", "AI": {"tldr": "本文提出了一种受投资组合理论启发的边缘资源管理策略，用于在动态且易受干扰的边缘环境中，确保多视角3D重建的可靠性，即使相机面临时空相关的中断也能保证重建质量。", "motivation": "多视角3D重建对于应急响应等需要快速态势感知的应用至关重要。然而，在“即时”边缘环境中，由于其固有的动态性和操作逆境，系统可能出现时空相关的中断，影响相机操作，导致重建质量持续下降。", "method": "提出了一种受投资组合理论启发的边缘资源管理策略，旨在确保多视角3D重建在系统可能中断的情况下仍能满足重建质量。该投资组合理论优化问题通过遗传算法求解，该算法在实际系统设置下能快速收敛。", "result": "通过使用公开和定制的3D数据集，研究表明所提出的相机选择策略在面对时空相关中断时，相比传统基线策略，能更有效地保证可靠的3D重建。", "conclusion": "该方法能够有效应对边缘环境中相机面临的时空相关中断，确保多视角3D重建的质量和可靠性。"}}
{"id": "2508.15126", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.15126", "abs": "https://arxiv.org/abs/2508.15126", "authors": ["Pengsong Zhang", "Xiang Hu", "Guowei Huang", "Yang Qi", "Heng Zhang", "Xiuxu Li", "Jiaxing Song", "Jiabin Luo", "Yijiang Li", "Shuo Yin", "Chengxiao Dai", "Eric Hanchen Jiang", "Xiaoyan Zhou", "Zhenfei Yin", "Boqin Yuan", "Jing Dong", "Guinan Su", "Guanren Qiao", "Haiming Tang", "Anghong Du", "Lili Pan", "Zhenzhong Lan", "Xinyu Liu"], "title": "aiXiv: A Next-Generation Open Access Ecosystem for Scientific Discovery Generated by AI Scientists", "comment": "Preprint under review. Code is available at\n  https://github.com/aixiv-org. Website is available at\n  https://forms.gle/DxQgCtXFsJ4paMtn8", "summary": "Recent advances in large language models (LLMs) have enabled AI agents to\nautonomously generate scientific proposals, conduct experiments, author papers,\nand perform peer reviews. Yet this flood of AI-generated research content\ncollides with a fragmented and largely closed publication ecosystem.\nTraditional journals and conferences rely on human peer review, making them\ndifficult to scale and often reluctant to accept AI-generated research content;\nexisting preprint servers (e.g. arXiv) lack rigorous quality-control\nmechanisms. Consequently, a significant amount of high-quality AI-generated\nresearch lacks appropriate venues for dissemination, hindering its potential to\nadvance scientific progress. To address these challenges, we introduce aiXiv, a\nnext-generation open-access platform for human and AI scientists. Its\nmulti-agent architecture allows research proposals and papers to be submitted,\nreviewed, and iteratively refined by both human and AI scientists. It also\nprovides API and MCP interfaces that enable seamless integration of\nheterogeneous human and AI scientists, creating a scalable and extensible\necosystem for autonomous scientific discovery. Through extensive experiments,\nwe demonstrate that aiXiv is a reliable and robust platform that significantly\nenhances the quality of AI-generated research proposals and papers after\niterative revising and reviewing on aiXiv. Our work lays the groundwork for a\nnext-generation open-access ecosystem for AI scientists, accelerating the\npublication and dissemination of high-quality AI-generated research content.\nCode is available at https://github.com/aixiv-org. Website is available at\nhttps://forms.gle/DxQgCtXFsJ4paMtn8.", "AI": {"tldr": "本文提出了aiXiv，一个下一代开放获取平台，旨在解决AI生成研究内容在传统出版生态系统中面临的传播挑战，通过允许人类和AI科学家进行多代理提交、评审和迭代改进。", "motivation": "大型语言模型（LLMs）使得AI能够自主生成科学研究内容，但现有的出版系统（如传统期刊和会议）依赖人类评审，难以扩展且不愿接受AI内容；现有预印本服务器缺乏质量控制。这导致大量高质量的AI生成研究缺乏合适的传播途径，阻碍了科学进步。", "method": "引入了aiXiv，一个多代理架构的下一代开放获取平台，允许人类和AI科学家共同提交、评审和迭代完善研究提案和论文。它还提供API和MCP接口，以实现异构人类和AI科学家的无缝集成，创建一个可扩展的自主科学发现生态系统。", "result": "通过广泛实验证明，aiXiv是一个可靠且健壮的平台，经过在aiXiv上迭代修订和评审后，显著提高了AI生成研究提案和论文的质量。", "conclusion": "aiXiv为AI科学家构建了一个下一代开放获取生态系统，加速了高质量AI生成研究内容的出版和传播，为未来的自主科学发现奠定了基础。"}}
{"id": "2508.15663", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15663", "abs": "https://arxiv.org/abs/2508.15663", "authors": ["Nikita Kachaev", "Andrei Spiridonov", "Andrey Gorodetsky", "Kirill Muravyev", "Nikita Oskolkov", "Aditya Narendra", "Vlad Shakhuro", "Dmitry Makarov", "Aleksandr I. Panov", "Polina Fedotova", "Alexey K. Kovalev"], "title": "Mind and Motion Aligned: A Joint Evaluation IsaacSim Benchmark for Task Planning and Low-Level Policies in Mobile Manipulation", "comment": null, "summary": "Benchmarks are crucial for evaluating progress in robotics and embodied AI.\nHowever, a significant gap exists between benchmarks designed for high-level\nlanguage instruction following, which often assume perfect low-level execution,\nand those for low-level robot control, which rely on simple, one-step commands.\nThis disconnect prevents a comprehensive evaluation of integrated systems where\nboth task planning and physical execution are critical. To address this, we\npropose Kitchen-R, a novel benchmark that unifies the evaluation of task\nplanning and low-level control within a simulated kitchen environment. Built as\na digital twin using the Isaac Sim simulator and featuring more than 500\ncomplex language instructions, Kitchen-R supports a mobile manipulator robot.\nWe provide baseline methods for our benchmark, including a task-planning\nstrategy based on a vision-language model and a low-level control policy based\non diffusion policy. We also provide a trajectory collection system. Our\nbenchmark offers a flexible framework for three evaluation modes: independent\nassessment of the planning module, independent assessment of the control\npolicy, and, crucially, an integrated evaluation of the whole system. Kitchen-R\nbridges a key gap in embodied AI research, enabling more holistic and realistic\nbenchmarking of language-guided robotic agents.", "AI": {"tldr": "Kitchen-R是一个新颖的基准测试平台，旨在统一评估具身AI中高层任务规划和低层机器人控制。它在一个模拟厨房环境中，使用复杂的语言指令来驱动移动机械臂，并支持独立或集成评估。", "motivation": "当前的具身AI基准测试在高层语言指令遵循（假设完美的低层执行）和低层机器人控制（依赖简单一步指令）之间存在显著差距，阻碍了对任务规划和物理执行均关键的集成系统进行全面评估。", "method": "本文提出了Kitchen-R，一个基于Isaac Sim构建的数字孪生模拟厨房环境。它包含超过500条复杂的语言指令，支持移动机械臂。提供了基线方法，包括基于视觉语言模型的任务规划策略和基于扩散策略的低层控制策略，以及一个轨迹收集系统。Kitchen-R提供三种评估模式：规划模块独立评估、控制策略独立评估和整个系统的集成评估。", "result": "Kitchen-R提供了一个灵活的框架，用于在一个真实的模拟厨房环境中，通过复杂的语言指令，对任务规划和低层控制进行独立和集成评估。它通过基线方法展示了其可用性。", "conclusion": "Kitchen-R弥合了具身AI研究中的一个关键空白，使得对语言引导的机器人代理能够进行更全面、更真实的基准测试。"}}
{"id": "2508.15379", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15379", "abs": "https://arxiv.org/abs/2508.15379", "authors": ["Jinliang Yu", "Mingduo Xie", "Yue Wang", "Tianfan Fu", "Xianglai Xu", "Jiajun Wang"], "title": "Bladder Cancer Diagnosis with Deep Learning: A Multi-Task Framework and Online Platform", "comment": null, "summary": "Clinical cystoscopy, the current standard for bladder cancer diagnosis,\nsuffers from significant reliance on physician expertise, leading to\nvariability and subjectivity in diagnostic outcomes. There is an urgent need\nfor objective, accurate, and efficient computational approaches to improve\nbladder cancer diagnostics.\n  Leveraging recent advancements in deep learning, this study proposes an\nintegrated multi-task deep learning framework specifically designed for bladder\ncancer diagnosis from cystoscopic images. Our framework includes a robust\nclassification model using EfficientNet-B0 enhanced with Convolutional Block\nAttention Module (CBAM), an advanced segmentation model based on\nResNet34-UNet++ architecture with self-attention mechanisms and attention\ngating, and molecular subtyping using ConvNeXt-Tiny to classify molecular\nmarkers such as HER-2 and Ki-67. Additionally, we introduce a Gradio-based\nonline diagnostic platform integrating all developed models, providing\nintuitive features including multi-format image uploads, bilingual interfaces,\nand dynamic threshold adjustments.\n  Extensive experimentation demonstrates the effectiveness of our methods,\nachieving outstanding accuracy (93.28%), F1-score (82.05%), and AUC (96.41%)\nfor classification tasks, and exceptional segmentation performance indicated by\na Dice coefficient of 0.9091. The online platform significantly improved the\naccuracy, efficiency, and accessibility of clinical bladder cancer diagnostics,\nenabling practical and user-friendly deployment. The code is publicly\navailable.\n  Our multi-task framework and integrated online tool collectively advance the\nfield of intelligent bladder cancer diagnosis by improving clinical\nreliability, supporting early tumor detection, and enabling real-time\ndiagnostic feedback. These contributions mark a significant step toward\nAI-assisted decision-making in urology.", "AI": {"tldr": "本研究提出了一个集成多任务深度学习框架和在线平台，用于从膀胱镜图像中进行膀胱癌诊断，显著提高了诊断的准确性、效率和可及性。", "motivation": "当前的临床膀胱镜检查高度依赖医生经验，导致诊断结果存在变异性和主观性。因此，迫切需要客观、准确、高效的计算方法来改进膀胱癌诊断。", "method": "本研究提出了一个集成多任务深度学习框架：1. 分类模型：使用带有卷积块注意力模块（CBAM）增强的EfficientNet-B0。2. 分割模型：基于ResNet34-UNet++架构，结合自注意力机制和注意力门控。3. 分子亚型分类：使用ConvNeXt-Tiny对HER-2和Ki-67等分子标志物进行分类。此外，还开发了一个基于Gradio的在线诊断平台，集成了所有模型，提供多格式图像上传、双语界面和动态阈值调整等功能。", "result": "实验结果显示：分类任务达到了93.28%的准确率、82.05%的F1分数和96.41%的AUC。分割任务的Dice系数为0.9091。在线平台显著提高了临床膀胱癌诊断的准确性、效率和可及性。", "conclusion": "本研究的多任务框架和集成在线工具通过提高临床可靠性、支持早期肿瘤检测和实现实时诊断反馈，共同推动了智能膀胱癌诊断领域的发展，标志着泌尿科AI辅助决策迈出了重要一步。"}}
{"id": "2508.15164", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.15164", "abs": "https://arxiv.org/abs/2508.15164", "authors": ["Seungmin Han", "Haeun Kwon", "Ji-jun Park", "Taeyang Yoon"], "title": "ContextualLVLM-Agent: A Holistic Framework for Multi-Turn Visually-Grounded Dialogue and Complex Instruction Following", "comment": null, "summary": "Despite significant advancements in Large Language Models (LLMs) and Large\nVision-Language Models (LVLMs), current models still face substantial\nchallenges in handling complex, multi-turn, and visually-grounded tasks that\ndemand deep reasoning, sustained contextual understanding, entity tracking, and\nmulti-step instruction following. Existing benchmarks often fall short in\ncapturing the dynamism and intricacies of real-world multi-modal interactions,\nleading to issues such as context loss and visual hallucinations. To address\nthese limitations, we introduce MMDR-Bench (Multi-Modal Dialogue Reasoning\nBenchmark), a novel dataset comprising 300 meticulously designed complex\nmulti-turn dialogue scenarios, each averaging 5-7 turns and evaluated across\nsix core dimensions including visual entity tracking and reasoning depth.\nFurthermore, we propose CoLVLM Agent (Contextual LVLM Agent), a holistic\nframework that enhances existing LVLMs with advanced reasoning and instruction\nfollowing capabilities through an iterative\n\"memory-perception-planning-execution\" cycle, requiring no extensive\nre-training of the underlying models. Our extensive experiments on MMDR-Bench\ndemonstrate that CoLVLM Agent consistently achieves superior performance,\nattaining an average human evaluation score of 4.03, notably surpassing\nstate-of-the-art commercial models like GPT-4o (3.92) and Gemini 1.5 Pro\n(3.85). The framework exhibits significant advantages in reasoning depth,\ninstruction adherence, and error suppression, and maintains robust performance\nover extended dialogue turns, validating the effectiveness of its modular\ndesign and iterative approach for complex multi-modal interactions.", "AI": {"tldr": "该研究引入了一个新的多模态对话推理基准MMDR-Bench，并提出了CoLVLM Agent框架，通过迭代的“记忆-感知-规划-执行”循环，显著提升了现有大型视觉语言模型在复杂多轮、视觉接地任务中的表现，超越了GPT-4o和Gemini 1.5 Pro等SOTA模型。", "motivation": "尽管大型语言模型（LLMs）和大型视觉语言模型（LVLMs）取得了显著进展，但在处理需要深度推理、持续上下文理解、实体跟踪和多步骤指令遵循的复杂、多轮、视觉接地任务时仍面临巨大挑战。现有基准未能充分捕捉真实世界多模态交互的动态性和复杂性，导致上下文丢失和视觉幻觉等问题。", "method": "1. 引入MMDR-Bench：一个包含300个精心设计的复杂多轮对话场景的新型数据集，每个场景平均5-7轮，并在视觉实体跟踪和推理深度等六个核心维度上进行评估。2. 提出CoLVLM Agent：一个整体框架，通过迭代的“记忆-感知-规划-执行”循环，增强现有LVLMs的推理和指令遵循能力，无需对底层模型进行大量再训练。", "result": "在MMDR-Bench上的广泛实验表明，CoLVLM Agent持续取得卓越性能，平均人类评估得分达到4.03，显著超越了GPT-4o（3.92）和Gemini 1.5 Pro（3.85）等最先进的商业模型。该框架在推理深度、指令遵循和错误抑制方面表现出显著优势，并在扩展对话轮次中保持了稳健的性能。", "conclusion": "CoLVLM Agent的模块化设计和迭代方法对于复杂的、多模态交互是有效的，其性能优于现有最先进的商业模型，验证了其在处理多轮、视觉接地推理任务方面的优越性。"}}
{"id": "2508.15168", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15168", "abs": "https://arxiv.org/abs/2508.15168", "authors": ["Masato Ito", "Kaito Tanaka", "Keisuke Matsuda", "Aya Nakayama"], "title": "XDR-LVLM: An Explainable Vision-Language Large Model for Diabetic Retinopathy Diagnosis", "comment": null, "summary": "Diabetic Retinopathy (DR) is a major cause of global blindness, necessitating\nearly and accurate diagnosis. While deep learning models have shown promise in\nDR detection, their black-box nature often hinders clinical adoption due to a\nlack of transparency and interpretability. To address this, we propose XDR-LVLM\n(eXplainable Diabetic Retinopathy Diagnosis with LVLM), a novel framework that\nleverages Vision-Language Large Models (LVLMs) for high-precision DR diagnosis\ncoupled with natural language-based explanations. XDR-LVLM integrates a\nspecialized Medical Vision Encoder, an LVLM Core, and employs Multi-task Prompt\nEngineering and Multi-stage Fine-tuning to deeply understand pathological\nfeatures within fundus images and generate comprehensive diagnostic reports.\nThese reports explicitly include DR severity grading, identification of key\npathological concepts (e.g., hemorrhages, exudates, microaneurysms), and\ndetailed explanations linking observed features to the diagnosis. Extensive\nexperiments on the Diabetic Retinopathy (DDR) dataset demonstrate that XDR-LVLM\nachieves state-of-the-art performance, with a Balanced Accuracy of 84.55% and\nan F1 Score of 79.92% for disease diagnosis, and superior results for concept\ndetection (77.95% BACC, 66.88% F1). Furthermore, human evaluations confirm the\nhigh fluency, accuracy, and clinical utility of the generated explanations,\nshowcasing XDR-LVLM's ability to bridge the gap between automated diagnosis and\nclinical needs by providing robust and interpretable insights.", "AI": {"tldr": "本文提出XDR-LVLM框架，利用视觉-语言大模型(LVLM)实现高精度糖尿病视网膜病变(DR)诊断，并生成自然语言解释性报告，解决了深度学习模型缺乏透明度的问题。", "motivation": "糖尿病视网膜病变(DR)是全球失明的主要原因，需要早期准确诊断。尽管深度学习模型在DR检测中前景广阔，但其“黑箱”性质因缺乏透明度和可解释性而阻碍了临床应用。", "method": "XDR-LVLM框架整合了专门的医学视觉编码器和LVLM核心，并采用多任务提示工程和多阶段微调。它能深入理解眼底图像中的病理特征，并生成包含DR严重程度分级、关键病理概念识别（如出血、渗出、微动脉瘤）以及将观察到的特征与诊断联系起来的详细解释的综合诊断报告。", "result": "在DDR数据集上的实验表明，XDR-LVLM在疾病诊断方面达到了最先进的性能，平衡准确率为84.55%，F1分数为79.92%；在概念检测方面也取得了优异结果（平衡准确率77.95%，F1分数66.88%）。此外，人类评估证实了生成解释的高流畅性、准确性和临床实用性。", "conclusion": "XDR-LVLM通过提供强大且可解释的见解，成功弥合了自动化诊断与临床需求之间的鸿沟，展示了其在提供鲁棒和可解释性洞察方面的能力。"}}
{"id": "2508.15144", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15144", "abs": "https://arxiv.org/abs/2508.15144", "authors": ["Jiabo Ye", "Xi Zhang", "Haiyang Xu", "Haowei Liu", "Junyang Wang", "Zhaoqing Zhu", "Ziwei Zheng", "Feiyu Gao", "Junjie Cao", "Zhengxi Lu", "Jitong Liao", "Qi Zheng", "Fei Huang", "Jingren Zhou", "Ming Yan"], "title": "Mobile-Agent-v3: Foundamental Agents for GUI Automation", "comment": null, "summary": "This paper introduces GUI-Owl, a foundational GUI agent model that achieves\nstate-of-the-art performance among open-source end-to-end models on ten GUI\nbenchmarks across desktop and mobile environments, covering grounding, question\nanswering, planning, decision-making, and procedural knowledge. GUI-Owl-7B\nachieves 66.4 on AndroidWorld and 29.4 on OSWorld. Building on this, we propose\nMobile-Agent-v3, a general-purpose GUI agent framework that further improves\nperformance to 73.3 on AndroidWorld and 37.7 on OSWorld, setting a new\nstate-of-the-art for open-source GUI agent frameworks. GUI-Owl incorporates\nthree key innovations: (1) Large-scale Environment Infrastructure: a\ncloud-based virtual environment spanning Android, Ubuntu, macOS, and Windows,\nenabling our Self-Evolving GUI Trajectory Production framework. This generates\nhigh-quality interaction data via automated query generation and correctness\nvalidation, leveraging GUI-Owl to refine trajectories iteratively, forming a\nself-improving loop. It supports diverse data pipelines and reduces manual\nannotation. (2) Diverse Foundational Agent Capabilities: by integrating UI\ngrounding, planning, action semantics, and reasoning patterns, GUI-Owl supports\nend-to-end decision-making and can act as a modular component in multi-agent\nsystems. (3) Scalable Environment RL: we develop a scalable reinforcement\nlearning framework with fully asynchronous training for real-world alignment.\nWe also introduce Trajectory-aware Relative Policy Optimization (TRPO) for\nonline RL, achieving 34.9 on OSWorld. GUI-Owl and Mobile-Agent-v3 are\nopen-sourced at https://github.com/X-PLUG/MobileAgent.", "AI": {"tldr": "本文介绍了GUI-Owl，一个开源的端到端GUI代理基础模型，在桌面和移动环境的十个GUI基准测试中达到了最先进的性能。在此基础上，提出了Mobile-Agent-v3框架，进一步提升了性能，并通过大规模环境基础设施、多样化的基础代理能力和可扩展的环境强化学习等创新实现了这些成果。", "motivation": "现有开源端到端GUI模型在性能上可能存在不足，研究旨在开发一个能在多种GUI任务和环境中实现最先进性能的通用GUI代理模型和框架。", "method": "该研究引入了GUI-Owl基础GUI代理模型和Mobile-Agent-v3通用GUI代理框架。其核心创新包括：1) 大规模环境基础设施：基于云的虚拟环境（Android、Ubuntu、macOS、Windows），支持自进化GUI轨迹生成框架，通过自动化查询和验证迭代生成高质量交互数据。2) 多样化的基础代理能力：整合UI定位、规划、动作语义和推理模式，支持端到端决策。3) 可扩展的环境强化学习：开发了完全异步训练的RL框架，并引入了轨迹感知相对策略优化（TRPO）进行在线RL。", "result": "GUI-Owl-7B在AndroidWorld上达到66.4分，在OSWorld上达到29.4分，是开源端到端模型中的最先进水平。Mobile-Agent-v3将性能进一步提升至AndroidWorld的73.3分和OSWorld的37.7分，为开源GUI代理框架树立了新的最先进水平。通过在线RL与TRPO，在OSWorld上达到了34.9分。GUI-Owl和Mobile-Agent-v3均已开源。", "conclusion": "GUI-Owl和Mobile-Agent-v3通过创新的数据生成、全面的代理能力和可扩展的强化学习方法，在开源GUI代理领域取得了显著的性能提升，达到了新的最先进水平，并已开源以促进社区发展。"}}
{"id": "2508.15669", "categories": ["cs.RO", "cs.LG", "68T40", "I.2.9"], "pdf": "https://arxiv.org/pdf/2508.15669", "abs": "https://arxiv.org/abs/2508.15669", "authors": ["Annie S. Chen", "Philemon Brakel", "Antonia Bronars", "Annie Xie", "Sandy Huang", "Oliver Groth", "Maria Bauza", "Markus Wulfmeier", "Nicolas Heess", "Dushyant Rao"], "title": "Exploiting Policy Idling for Dexterous Manipulation", "comment": "A similar version to this paper was accepted at IROS 2025", "summary": "Learning-based methods for dexterous manipulation have made notable progress\nin recent years. However, learned policies often still lack reliability and\nexhibit limited robustness to important factors of variation. One failure\npattern that can be observed across many settings is that policies idle, i.e.\nthey cease to move beyond a small region of states when they reach certain\nstates. This policy idling is often a reflection of the training data. For\ninstance, it can occur when the data contains small actions in areas where the\nrobot needs to perform high-precision motions, e.g., when preparing to grasp an\nobject or object insertion. Prior works have tried to mitigate this phenomenon\ne.g. by filtering the training data or modifying the control frequency.\nHowever, these approaches can negatively impact policy performance in other\nways. As an alternative, we investigate how to leverage the detectability of\nidling behavior to inform exploration and policy improvement. Our approach,\nPause-Induced Perturbations (PIP), applies perturbations at detected idling\nstates, thus helping it to escape problematic basins of attraction. On a range\nof challenging simulated dual-arm tasks, we find that this simple approach can\nalready noticeably improve test-time performance, with no additional\nsupervision or training. Furthermore, since the robot tends to idle at critical\npoints in a movement, we also find that learning from the resulting episodes\nleads to better iterative policy improvement compared to prior approaches. Our\nperturbation strategy also leads to a 15-35% improvement in absolute success\nrate on a real-world insertion task that requires complex multi-finger\nmanipulation.", "AI": {"tldr": "本文提出了一种名为PIP的方法，通过在检测到策略“停滞”时施加扰动，显著提高了灵巧操作策略的鲁棒性和性能，并促进了更好的策略迭代改进。", "motivation": "学习型灵巧操作策略在可靠性和鲁棒性方面仍有不足，常见问题是策略在需要高精度运动的关键点（如抓取或插入前）出现“停滞”现象，这通常是训练数据造成的。现有缓解方法（如过滤数据或调整控制频率）可能带来负面影响，因此需要一种新的替代方案。", "method": "本文提出“停滞诱导扰动”（Pause-Induced Perturbations, PIP）方法。该方法检测策略何时进入“停滞”状态（即在小范围内停止移动），并在检测到停滞时施加扰动，帮助策略逃离有问题的吸引盆地，从而促进探索和策略改进。", "result": "在具有挑战性的模拟双臂任务中，PIP方法在测试时性能上取得了显著提升，且无需额外监督或训练。此外，由于机器人在关键点容易停滞，从PIP产生的经验中学习可以带来比现有方法更好的迭代策略改进。在需要复杂多指操作的真实世界插入任务中，该扰动策略使绝对成功率提高了15-35%。", "conclusion": "PIP是一种简单有效的方法，能解决灵巧操作中策略“停滞”的问题。通过在停滞状态施加扰动，它能显著提高策略在模拟和真实世界任务中的测试时性能和鲁棒性，并能更好地促进策略的迭代学习和改进。"}}
{"id": "2508.15452", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15452", "abs": "https://arxiv.org/abs/2508.15452", "authors": ["Uğurcan Akyüz", "Deniz Katircioglu-Öztürk", "Emre K. Süslü", "Burhan Keleş", "Mete C. Kaya", "Gamze Durhan", "Meltem G. Akpınar", "Figen B. Demirkazık", "Gözde B. Akar"], "title": "DoSReMC: Domain Shift Resilient Mammography Classification using Batch Normalization Adaptation", "comment": null, "summary": "Numerous deep learning-based solutions have been developed for the automatic\nrecognition of breast cancer using mammography images. However, their\nperformance often declines when applied to data from different domains,\nprimarily due to domain shift - the variation in data distributions between\nsource and target domains. This performance drop limits the safe and equitable\ndeployment of AI in real-world clinical settings. In this study, we present\nDoSReMC (Domain Shift Resilient Mammography Classification), a batch\nnormalization (BN) adaptation framework designed to enhance cross-domain\ngeneralization without retraining the entire model. Using three large-scale\nfull-field digital mammography (FFDM) datasets - including HCTP, a newly\nintroduced, pathologically confirmed in-house dataset - we conduct a systematic\ncross-domain evaluation with convolutional neural networks (CNNs). Our results\ndemonstrate that BN layers are a primary source of domain dependence: they\nperform effectively when training and testing occur within the same domain, and\nthey significantly impair model generalization under domain shift. DoSReMC\naddresses this limitation by fine-tuning only the BN and fully connected (FC)\nlayers, while preserving pretrained convolutional filters. We further integrate\nthis targeted adaptation with an adversarial training scheme, yielding\nadditional improvements in cross-domain generalizability. DoSReMC can be\nreadily incorporated into existing AI pipelines and applied across diverse\nclinical environments, providing a practical pathway toward more robust and\ngeneralizable mammography classification systems.", "AI": {"tldr": "DoSReMC是一个批归一化（BN）适应框架，通过仅微调BN和全连接层（FC）来增强乳腺癌乳腺X线摄影分类模型在跨域情况下的泛化能力，从而解决领域偏移问题。", "motivation": "深度学习模型在乳腺癌乳腺X线摄影图像识别中表现出色，但当应用于不同领域的数据时，由于领域偏移（数据分布差异），其性能显著下降。这限制了AI在真实临床环境中安全和公平的部署。", "method": "该研究提出了DoSReMC框架，通过仅微调预训练模型中的批归一化（BN）层和全连接（FC）层，同时保留卷积滤波器，以增强跨域泛化能力。此外，该方法还集成了对抗性训练方案以进一步提升泛化性。研究使用了三个大规模全视野数字乳腺X线摄影（FFDM）数据集进行系统性跨域评估。", "result": "研究发现BN层是领域依赖性的主要来源：它们在同域训练和测试时表现良好，但在领域偏移下显著损害模型泛化能力。DoSReMC通过有针对性地适应BN和FC层解决了这一限制，并显著提高了跨域泛化能力。结合对抗性训练进一步带来了额外改进。", "conclusion": "DoSReMC提供了一种实用的方法，可以轻松整合到现有AI管线中，并应用于各种临床环境，从而实现更稳健和泛化能力更强的乳腺X线摄影分类系统。"}}
{"id": "2508.15190", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15190", "abs": "https://arxiv.org/abs/2508.15190", "authors": ["Dong Liu", "Yanxuan Yu"], "title": "SemToken: Semantic-Aware Tokenization for Efficient Long-Context Language Modeling", "comment": null, "summary": "Tokenization plays a critical role in language modeling, yet existing\napproaches such as Byte-Pair Encoding (BPE) or WordPiece operate purely on\nfrequency statistics, ignoring the underlying semantic structure of text. This\nleads to over-tokenization of semantically redundant spans and underutilization\nof contextual coherence, particularly in long-context scenarios. In this work,\nwe propose \\textbf{SemToken}, a semantic-aware tokenization framework that\njointly reduces token redundancy and improves computation efficiency. SemToken\nfirst extracts contextual semantic embeddings via lightweight encoders and\nperforms local semantic clustering to merge semantically equivalent tokens.\nThen, it allocates heterogeneous token granularity based on semantic density,\nallowing finer-grained tokenization in content-rich regions and coarser\ncompression in repetitive or low-entropy spans. SemToken can be seamlessly\nintegrated with modern language models and attention acceleration methods.\nExperiments on long-context language modeling benchmarks such as WikiText-103\nand LongBench show that SemToken achieves up to $2.4\\times$ reduction in token\ncount and $1.9\\times$ speedup, with negligible or no degradation in perplexity\nand downstream accuracy. Our findings suggest that semantic structure offers a\npromising new axis for optimizing tokenization and computation in large\nlanguage models.", "AI": {"tldr": "SemToken是一种语义感知的标记化框架，它通过合并语义等价的标记和采用自适应粒度，显著减少标记数量并加速语言模型，同时保持性能。", "motivation": "现有标记化方法（如BPE、WordPiece）仅基于频率统计，忽略文本的语义结构，导致语义冗余的文本段过度标记化，且未充分利用上下文连贯性，尤其在长上下文场景中。", "method": "SemToken框架首先通过轻量级编码器提取上下文语义嵌入，然后进行局部语义聚类以合并语义等价的标记。接着，它根据语义密度分配异构标记粒度，对内容丰富的区域进行细粒度标记化，对重复或低熵区域进行粗粒度压缩。", "result": "在WikiText-103和LongBench等长上下文语言模型基准测试中，SemToken实现了高达2.4倍的标记数量减少和1.9倍的速度提升，同时困惑度和下游任务准确性下降可忽略不计。", "conclusion": "研究结果表明，语义结构为优化大型语言模型中的标记化和计算提供了一个有前景的新方向。"}}
{"id": "2508.15169", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15169", "abs": "https://arxiv.org/abs/2508.15169", "authors": ["Xuyang Chen", "Zhijun Zhai", "Kaixuan Zhou", "Zengmao Wang", "Jianan He", "Dong Wang", "Yanfeng Zhang", "mingwei Sun", "Rüdiger Westermann", "Konrad Schindler", "Liqiu Meng"], "title": "MeSS: City Mesh-Guided Outdoor Scene Generation with Cross-View Consistent Diffusion", "comment": null, "summary": "Mesh models have become increasingly accessible for numerous cities; however,\nthe lack of realistic textures restricts their application in virtual urban\nnavigation and autonomous driving. To address this, this paper proposes MeSS\n(Meshbased Scene Synthesis) for generating high-quality, styleconsistent\noutdoor scenes with city mesh models serving as the geometric prior. While\nimage and video diffusion models can leverage spatial layouts (such as depth\nmaps or HD maps) as control conditions to generate street-level perspective\nviews, they are not directly applicable to 3D scene generation. Video diffusion\nmodels excel at synthesizing consistent view sequences that depict scenes but\noften struggle to adhere to predefined camera paths or align accurately with\nrendered control videos. In contrast, image diffusion models, though unable to\nguarantee cross-view visual consistency, can produce more geometry-aligned\nresults when combined with ControlNet. Building on this insight, our approach\nenhances image diffusion models by improving cross-view consistency. The\npipeline comprises three key stages: first, we generate geometrically\nconsistent sparse views using Cascaded Outpainting ControlNets; second, we\npropagate denser intermediate views via a component dubbed AGInpaint; and\nthird, we globally eliminate visual inconsistencies (e.g., varying exposure)\nusing the GCAlign module. Concurrently with generation, a 3D Gaussian Splatting\n(3DGS) scene is reconstructed by initializing Gaussian balls on the mesh\nsurface. Our method outperforms existing approaches in both geometric alignment\nand generation quality. Once synthesized, the scene can be rendered in diverse\nstyles through relighting and style transfer techniques.", "AI": {"tldr": "本文提出了MeSS（基于网格的场景合成）方法，利用城市网格模型作为几何先验，生成高质量、风格一致的室外场景，解决了现有网格模型纹理不足的问题。", "motivation": "现有城市网格模型缺乏逼真的纹理，限制了它们在虚拟城市导航和自动驾驶中的应用。图像和视频扩散模型虽然可用于生成街景视图，但前者缺乏跨视图一致性，后者难以精确遵循预定义相机路径或与渲染的控制视频对齐，均不直接适用于3D场景生成。", "method": "MeSS方法包含三个主要阶段：1. 使用级联外绘ControlNets生成几何一致的稀疏视图；2. 通过AGInpaint组件传播更密集的中间视图；3. 使用GCAlign模块全局消除视觉不一致性。同时，通过在网格表面初始化高斯球来重建3D高斯泼溅（3DGS）场景。", "result": "该方法在几何对齐和生成质量方面均优于现有方法。合成的场景可以通过重新打光和风格迁移技术以多种风格渲染。", "conclusion": "MeSS成功地为城市网格模型生成了高品质、风格一致且几何对齐的室外场景，显著提升了网格模型在虚拟城市导航和自动驾驶等领域的应用潜力。"}}
{"id": "2508.15180", "categories": ["cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.15180", "abs": "https://arxiv.org/abs/2508.15180", "authors": ["Kai Xiong", "Yanwei Huang", "Rongjunchen Zhang", "Kun Chen", "Haipang Wu"], "title": "PuzzleClone: An SMT-Powered Framework for Synthesizing Verifiable Data", "comment": null, "summary": "High-quality mathematical and logical datasets with verifiable answers are\nessential for strengthening the reasoning capabilities of large language models\n(LLMs). While recent data augmentation techniques have facilitated the creation\nof large-scale benchmarks, existing LLM-generated datasets often suffer from\nlimited reliability, diversity, and scalability. To address these challenges,\nwe introduce PuzzleClone, a formal framework for synthesizing verifiable data\nat scale using Satisfiability Modulo Theories (SMT). Our approach features\nthree key innovations: (1) encoding seed puzzles into structured logical\nspecifications, (2) generating scalable variants through systematic variable\nand constraint randomization, and (3) ensuring validity via a reproduction\nmechanism. Applying PuzzleClone, we construct a curated benchmark comprising\nover 83K diverse and programmatically validated puzzles. The generated puzzles\nspan a wide spectrum of difficulty and formats, posing significant challenges\nto current state-of-the-art models. We conduct post training (SFT and RL) on\nPuzzleClone datasets. Experimental results show that training on PuzzleClone\nyields substantial improvements not only on PuzzleClone testset but also on\nlogic and mathematical benchmarks. Post training raises PuzzleClone average\nfrom 14.4 to 56.2 and delivers consistent improvements across 7 logic and\nmathematical benchmarks up to 12.5 absolute percentage points (AMC2023 from\n52.5 to 65.0). Our code and data are available at\nhttps://github.com/puzzleclone.", "AI": {"tldr": "本文介绍了PuzzleClone，一个基于SMT的正式框架，用于大规模合成可验证的高质量数学和逻辑数据集。通过在这些数据集上进行训练，大型语言模型在推理能力上取得了显著提升。", "motivation": "现有的大型语言模型生成的数据集在可靠性、多样性和可扩展性方面存在局限性，而高质量的、可验证的数学和逻辑数据集对于增强LLM的推理能力至关重要。", "method": "引入了PuzzleClone框架，利用可满足性模理论（SMT）来合成可验证数据。其核心创新包括：1) 将种子谜题编码为结构化逻辑规范；2) 通过系统性变量和约束随机化生成可扩展的变体；3) 通过复现机制确保数据的有效性。", "result": "构建了一个包含超过8.3万个多样化且程序化验证的谜题数据集。在PuzzleClone数据集上进行训练（SFT和RL）后，LLM在PuzzleClone测试集上的平均分数从14.4%提高到56.2%，并在7个逻辑和数学基准测试中取得了持续改进，最高提升了12.5个绝对百分点（例如，AMC2023从52.5%提高到65.0%）。", "conclusion": "PuzzleClone框架能够大规模生成高质量、多样化且可验证的数学和逻辑谜题数据集，显著提升了大型语言模型的推理能力，并在多个基准测试中展现出卓越的性能改进。"}}
{"id": "2508.15755", "categories": ["cs.RO", "cs.AI", "cs.GR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.15755", "abs": "https://arxiv.org/abs/2508.15755", "authors": ["Jie Xu", "Eric Heiden", "Iretiayo Akinola", "Dieter Fox", "Miles Macklin", "Yashraj Narang"], "title": "Neural Robot Dynamics", "comment": null, "summary": "Accurate and efficient simulation of modern robots remains challenging due to\ntheir high degrees of freedom and intricate mechanisms. Neural simulators have\nemerged as a promising alternative to traditional analytical simulators,\ncapable of efficiently predicting complex dynamics and adapting to real-world\ndata; however, existing neural simulators typically require\napplication-specific training and fail to generalize to novel tasks and/or\nenvironments, primarily due to inadequate representations of the global state.\nIn this work, we address the problem of learning generalizable neural\nsimulators for robots that are structured as articulated rigid bodies. We\npropose NeRD (Neural Robot Dynamics), learned robot-specific dynamics models\nfor predicting future states for articulated rigid bodies under contact\nconstraints. NeRD uniquely replaces the low-level dynamics and contact solvers\nin an analytical simulator and employs a robot-centric and spatially-invariant\nsimulation state representation. We integrate the learned NeRD models as an\ninterchangeable backend solver within a state-of-the-art robotics simulator. We\nconduct extensive experiments to show that the NeRD simulators are stable and\naccurate over a thousand simulation steps; generalize across tasks and\nenvironment configurations; enable policy learning exclusively in a neural\nengine; and, unlike most classical simulators, can be fine-tuned from\nreal-world data to bridge the gap between simulation and reality.", "AI": {"tldr": "本文提出NeRD（神经机器人动力学），这是一种可学习的机器人特定动力学模型，用于对接触约束下的铰接刚体进行可泛化的神经模拟。它取代了传统解析模拟器中的低级求解器，并采用以机器人为中心、空间不变的状态表示，实现了稳定、准确、可泛化且可从真实数据微调的模拟。", "motivation": "现代机器人因其高自由度和复杂机制，其精确高效的模拟仍具挑战。现有神经模拟器通常需要特定应用训练，且由于全局状态表示不足，难以泛化到新任务和环境。", "method": "本文提出了NeRD（神经机器人动力学），一种学习到的机器人特定动力学模型，用于预测接触约束下铰接刚体的未来状态。NeRD独特地取代了解析模拟器中的低级动力学和接触求解器，并采用以机器人为中心、空间不变的模拟状态表示。学习到的NeRD模型被整合为最先进机器人模拟器中可互换的后端求解器。", "result": "实验结果表明，NeRD模拟器在数千步模拟中保持稳定和准确；能泛化到不同的任务和环境配置；可以在纯神经引擎中进行策略学习；并且与大多数经典模拟器不同，NeRD可以从真实世界数据进行微调，以弥合模拟与现实之间的差距。", "conclusion": "NeRD为机器人提供了一种通用、准确且可从真实数据微调的神经模拟器，有效解决了现有神经模拟器泛化性差的问题，并为机器人模拟和策略学习提供了新的可能性。"}}
{"id": "2508.15553", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15553", "abs": "https://arxiv.org/abs/2508.15553", "authors": ["Jin Ye", "Jingran Wang", "Fengchao Xiong", "Jingzhou Chen", "Yuntao Qian"], "title": "Deep Equilibrium Convolutional Sparse Coding for Hyperspectral Image Denoising", "comment": null, "summary": "Hyperspectral images (HSIs) play a crucial role in remote sensing but are\noften degraded by complex noise patterns. Ensuring the physical property of the\ndenoised HSIs is vital for robust HSI denoising, giving the rise of deep\nunfolding-based methods. However, these methods map the optimization of a\nphysical model to a learnable network with a predefined depth, which lacks\nconvergence guarantees. In contrast, Deep Equilibrium (DEQ) models treat the\nhidden layers of deep networks as the solution to a fixed-point problem and\nmodels them as infinite-depth networks, naturally consistent with the\noptimization. Under the framework of DEQ, we propose a Deep Equilibrium\nConvolutional Sparse Coding (DECSC) framework that unifies local\nspatial-spectral correlations, nonlocal spatial self-similarities, and global\nspatial consistency for robust HSI denoising. Within the convolutional sparse\ncoding (CSC) framework, we enforce shared 2D convolutional sparse\nrepresentation to ensure global spatial consistency across bands, while\nunshared 3D convolutional sparse representation captures local spatial-spectral\ndetails. To further exploit nonlocal self-similarities, a transformer block is\nembedded after the 2D CSC. Additionally, a detail enhancement module is\nintegrated with the 3D CSC to promote image detail preservation. We formulate\nthe proximal gradient descent of the CSC model as a fixed-point problem and\ntransform the iterative updates into a learnable network architecture within\nthe framework of DEQ. Experimental results demonstrate that our DECSC method\nachieves superior denoising performance compared to state-of-the-art methods.", "AI": {"tldr": "本文提出了一种名为DECSC的深度平衡卷积稀疏编码框架，将深度平衡模型与卷积稀疏编码结合，统一了高光谱图像（HSI）去噪中的局部、非局部和全局特征，实现了卓越的去噪性能。", "motivation": "高光谱图像去噪对遥感至关重要，但常受复杂噪声影响。现有基于深度展开的方法将物理模型优化映射到预定义深度的可学习网络，缺乏收敛性保证。深度平衡（DEQ）模型将深度网络的隐藏层视为不动点问题的解，模拟无限深度网络，与优化过程自然一致，从而激发了本研究。", "method": "本文提出了深度平衡卷积稀疏编码（DECSC）框架，用于鲁棒的HSI去噪，该框架统一了局部空谱相关性、非局部空间自相似性和全局空间一致性。具体方法包括：在卷积稀疏编码（CSC）框架内，采用共享2D卷积稀疏表示以确保跨波段的全局空间一致性；采用非共享3D卷积稀疏表示以捕获局部空谱细节；在2D CSC之后嵌入Transformer块以利用非局部自相似性；将细节增强模块与3D CSC集成以促进图像细节保留。此外，将CSC模型的近端梯度下降公式化为不动点问题，并在DEQ框架内将迭代更新转换为可学习的网络架构。", "result": "实验结果表明，DECSC方法在去噪性能方面优于现有最先进的方法。", "conclusion": "DECSC框架通过结合深度平衡模型和卷积稀疏编码，并有效整合局部空谱相关性、非局部空间自相似性和全局空间一致性，为高光谱图像去噪提供了一种新颖且性能优越的解决方案。"}}
{"id": "2508.15202", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.15202", "abs": "https://arxiv.org/abs/2508.15202", "authors": ["Yuanchen Zhou", "Shuo Jiang", "Jie Zhu", "Junhui Li", "Lifan Guo", "Feng Chen", "Chi Zhang"], "title": "Fin-PRM: A Domain-Specialized Process Reward Model for Financial Reasoning in Large Language Models", "comment": null, "summary": "Process Reward Models (PRMs) have emerged as a promising framework for\nsupervising intermediate reasoning in large language models (LLMs), yet\nexisting PRMs are primarily trained on general or Science, Technology,\nEngineering, and Mathematics (STEM) domains and fall short in domain-specific\ncontexts such as finance, where reasoning is more structured, symbolic, and\nsensitive to factual and regulatory correctness. We introduce \\textbf{Fin-PRM},\na domain-specialized, trajectory-aware PRM tailored to evaluate intermediate\nreasoning steps in financial tasks. Fin-PRM integrates step-level and\ntrajectory-level reward supervision, enabling fine-grained evaluation of\nreasoning traces aligned with financial logic. We apply Fin-PRM in both offline\nand online reward learning settings, supporting three key applications: (i)\nselecting high-quality reasoning trajectories for distillation-based supervised\nfine-tuning, (ii) providing dense process-level rewards for reinforcement\nlearning, and (iii) guiding reward-informed Best-of-N inference at test time.\nExperimental results on financial reasoning benchmarks, including CFLUE and\nFinQA, demonstrate that Fin-PRM consistently outperforms general-purpose PRMs\nand strong domain baselines in trajectory selection quality. Downstream models\ntrained with Fin-PRM yield substantial improvements with baselines, with gains\nof 12.9\\% in supervised learning, 5.2\\% in reinforcement learning, and 5.1\\% in\ntest-time performance. These findings highlight the value of domain-specialized\nreward modeling for aligning LLMs with expert-level financial reasoning. Our\nproject resources will be available at https://github.com/aliyun/qwen-dianjin.", "AI": {"tldr": "Fin-PRM是一种针对金融领域定制的、轨迹感知的过程奖励模型，通过评估中间推理步骤，显著提升了大型语言模型在金融任务上的推理能力，优于通用PRM和强领域基线。", "motivation": "现有的过程奖励模型（PRM）主要针对通用或STEM领域，在金融等领域特定上下文中表现不足。金融领域的推理更为结构化、符号化，且对事实和监管的正确性高度敏感，需要更专业的评估方法。", "method": "引入Fin-PRM，一个领域专业化、轨迹感知的PRM，用于评估金融任务中的中间推理步骤。它整合了步骤级和轨迹级奖励监督，以实现与金融逻辑一致的细粒度评估。Fin-PRM应用于离线和在线奖励学习设置，支持三个主要应用：选择高质量推理轨迹用于蒸馏式监督微调、为强化学习提供密集过程级奖励、以及在测试时指导奖励知情的Best-of-N推理。", "result": "在CFLUE和FinQA等金融推理基准测试中，Fin-PRM在轨迹选择质量方面始终优于通用PRM和强领域基线。使用Fin-PRM训练的下游模型在监督学习中获得了12.9%的显著提升，在强化学习中获得了5.2%的提升，在测试时性能上获得了5.1%的提升。", "conclusion": "这些发现强调了领域专业化奖励建模对于使大型语言模型与专家级金融推理对齐的价值。"}}
{"id": "2508.15189", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15189", "abs": "https://arxiv.org/abs/2508.15189", "authors": ["Jiahao Xu", "Changchang Yin", "Odysseas Chatzipanagiotou", "Diamantis Tsilimigras", "Kevin Clear", "Bingsheng Yao", "Dakuo Wang", "Timothy Pawlik", "Ping Zhang"], "title": "SurgWound-Bench: A Benchmark for Surgical Wound Diagnosis", "comment": null, "summary": "Surgical site infection (SSI) is one of the most common and costly\nhealthcare-associated infections and and surgical wound care remains a\nsignificant clinical challenge in preventing SSIs and improving patient\noutcomes. While recent studies have explored the use of deep learning for\npreliminary surgical wound screening, progress has been hindered by concerns\nover data privacy and the high costs associated with expert annotation.\nCurrently, no publicly available dataset or benchmark encompasses various types\nof surgical wounds, resulting in the absence of an open-source Surgical-Wound\nscreening tool. To address this gap: (1) we present SurgWound, the first\nopen-source dataset featuring a diverse array of surgical wound types. It\ncontains 697 surgical wound images annotated by 3 professional surgeons with\neight fine-grained clinical attributes. (2) Based on SurgWound, we introduce\nthe first benchmark for surgical wound diagnosis, which includes visual\nquestion answering (VQA) and report generation tasks to comprehensively\nevaluate model performance. (3) Furthermore, we propose a three-stage learning\nframework, WoundQwen, for surgical wound diagnosis. In the first stage, we\nemploy five independent MLLMs to accurately predict specific surgical wound\ncharacteristics. In the second stage, these predictions serve as additional\nknowledge inputs to two MLLMs responsible for diagnosing outcomes, which assess\ninfection risk and guide subsequent interventions. In the third stage, we train\na MLLM that integrates the diagnostic results from the previous two stages to\nproduce a comprehensive report. This three-stage framework can analyze detailed\nsurgical wound characteristics and provide subsequent instructions to patients\nbased on surgical images, paving the way for personalized wound care, timely\nintervention, and improved patient outcomes.", "AI": {"tldr": "本研究推出了SurgWound，首个包含多种手术伤口类型的开源数据集和诊断基准，并提出了一个三阶段学习框架WoundQwen，用于手术伤口诊断和报告生成，以实现个性化伤口护理。", "motivation": "手术部位感染（SSI）是常见且代价高昂的医疗相关感染，手术伤口护理在预防SSI方面面临挑战。现有深度学习研究受限于数据隐私和高昂的专家标注成本，且缺乏包含多种手术伤口类型的公开数据集和开源筛查工具。", "method": "1. 构建了SurgWound，首个开源手术伤口数据集，包含697张由3名专业外科医生标注的图像，涵盖8种细粒度临床属性。2. 基于SurgWound，建立了首个手术伤口诊断基准，包括视觉问答（VQA）和报告生成任务。3. 提出了WoundQwen三阶段学习框架：第一阶段使用五个独立的MLLM预测伤口特征；第二阶段将第一阶段的预测作为知识输入，由两个MLLM诊断结果（感染风险和干预指导）；第三阶段训练一个MLLM整合前两阶段结果生成综合报告。", "result": "本研究成功创建了首个包含多种手术伤口类型的开源数据集SurgWound，建立了首个手术伤口诊断基准（包含VQA和报告生成任务），并提出了WoundQwen三阶段学习框架，该框架能够分析详细的手术伤口特征，并根据图像为患者提供后续指导。", "conclusion": "所提出的三阶段框架能够详细分析手术伤口特征并提供后续患者指导，为个性化伤口护理、及时干预和改善患者预后铺平了道路。"}}
{"id": "2508.15192", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.15192", "abs": "https://arxiv.org/abs/2508.15192", "authors": ["Wenjie Lin", "Jin Wei-Kocsis"], "title": "LLM4Sweat: A Trustworthy Large Language Model for Hyperhidrosis Support", "comment": null, "summary": "While large language models (LLMs) have shown promise in healthcare, their\napplication for rare medical conditions is still hindered by scarce and\nunreliable datasets for fine-tuning. Hyperhidrosis, a disorder causing\nexcessive sweating beyond physiological needs, is one such rare disorder,\naffecting 2-3% of the population and significantly impacting both physical\ncomfort and psychosocial well-being. To date, no work has tailored LLMs to\nadvance the diagnosis or care of hyperhidrosis. To address this gap, we present\nLLM4Sweat, an open-source and domain-specific LLM framework for trustworthy and\nempathetic hyperhidrosis support. The system follows a three-stage pipeline. In\nthe data augmentation stage, a frontier LLM generates medically plausible\nsynthetic vignettes from curated open-source data to create a diverse and\nbalanced question-answer dataset. In the fine-tuning stage, an open-source\nfoundation model is fine-tuned on the dataset to provide diagnosis,\npersonalized treatment recommendations, and empathetic psychological support.\nIn the inference and expert evaluation stage, clinical and psychological\nspecialists assess accuracy, appropriateness, and empathy, with validated\nresponses iteratively enriching the dataset. Experiments show that LLM4Sweat\noutperforms baselines and delivers the first open-source LLM framework for\nhyperhidrosis, offering a generalizable approach for other rare diseases with\nsimilar data and trustworthiness challenges.", "AI": {"tldr": "LLM4Sweat是一个开源、领域特定的LLM框架，旨在为多汗症患者提供可信赖和共情的支持，解决了罕见病数据稀缺的挑战。", "motivation": "大型语言模型（LLMs）在医疗领域潜力巨大，但其在罕见病（如多汗症）中的应用受限于微调数据集的稀缺和不可靠。多汗症影响2-3%的人口，严重影响生理和心理健康，目前尚无专门针对多汗症诊断或护理的LLM。", "method": "该系统遵循三阶段流程：1. 数据增强阶段：利用前沿LLM从开放数据生成医学上合理的合成病例，创建多样化、均衡的问答数据集。2. 微调阶段：使用该数据集对开源基础模型进行微调，以提供诊断、个性化治疗建议和共情心理支持。3. 推理与专家评估阶段：由临床和心理专家评估模型的准确性、适当性和共情能力，并将验证过的响应迭代地丰富数据集。", "result": "实验表明，LLM4Sweat超越了基线模型，并成为首个针对多汗症的开源LLM框架。", "conclusion": "LLM4Sweat为多汗症提供了一个有效的LLM解决方案，并为其他面临类似数据和可信度挑战的罕见病提供了一种可推广的方法。"}}
{"id": "2508.15594", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15594", "abs": "https://arxiv.org/abs/2508.15594", "authors": ["Ana C. Perre", "Luís A. Alexandre", "Luís C. Freire"], "title": "Are Virtual DES Images a Valid Alternative to the Real Ones?", "comment": "10 pages, 4 figures, 3 tables", "summary": "Contrast-enhanced spectral mammography (CESM) is an imaging modality that\nprovides two types of images, commonly known as low-energy (LE) and dual-energy\nsubtracted (DES) images. In many domains, particularly in medicine, the\nemergence of image-to-image translation techniques has enabled the artificial\ngeneration of images using other images as input. Within CESM, applying such\ntechniques to generate DES images from LE images could be highly beneficial,\npotentially reducing patient exposure to radiation associated with high-energy\nimage acquisition. In this study, we investigated three models for the\nartificial generation of DES images (virtual DES): a pre-trained U-Net model, a\nU-Net trained end-to-end model, and a CycleGAN model. We also performed a\nseries of experiments to assess the impact of using virtual DES images on the\nclassification of CESM examinations into malignant and non-malignant\ncategories. To our knowledge, this is the first study to evaluate the impact of\nvirtual DES images on CESM lesion classification. The results demonstrate that\nthe best performance was achieved with the pre-trained U-Net model, yielding an\nF1 score of 85.59% when using the virtual DES images, compared to 90.35% with\nthe real DES images. This discrepancy likely results from the additional\ndiagnostic information in real DES images, which contributes to a higher\nclassification accuracy. Nevertheless, the potential for virtual DES image\ngeneration is considerable and future advancements may narrow this performance\ngap to a level where exclusive reliance on virtual DES images becomes\nclinically viable.", "AI": {"tldr": "本研究旨在通过图像到图像转换技术，从低能（LE）图像生成虚拟双能减影（DES）图像，以减少对比增强谱乳腺摄影（CESM）中的辐射暴露，并评估这些虚拟DES图像对病变分类的影响。", "motivation": "对比增强谱乳腺摄影（CESM）生成两种图像：低能（LE）图像和双能减影（DES）图像。DES图像的获取涉及高能X射线，增加了患者的辐射暴露。通过从LE图像人工生成DES图像，有望减少患者的辐射剂量。", "method": "研究使用了三种图像到图像转换模型来生成虚拟DES图像：预训练的U-Net模型、端到端训练的U-Net模型和CycleGAN模型。随后，评估了使用这些虚拟DES图像对CESM检查进行恶性与非恶性分类的影响。", "result": "预训练的U-Net模型在虚拟DES图像生成方面表现最佳，使用虚拟DES图像进行分类的F1分数为85.59%，而使用真实DES图像的F1分数为90.35%。真实DES图像中额外的诊断信息可能是导致性能差异的原因。", "conclusion": "虚拟DES图像生成具有巨大潜力，有望减少患者辐射暴露。尽管目前虚拟DES图像在分类准确性上与真实DES图像存在差距，但未来的技术进步可能会缩小这一差距，使仅依赖虚拟DES图像在临床上成为可能。"}}
{"id": "2508.15212", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.15212", "abs": "https://arxiv.org/abs/2508.15212", "authors": ["Huanxuan Liao", "Yixing Xu", "Shizhu He", "Guanchen Li", "Xuanwu Yin", "Dong Li", "Emad Barsoum", "Jun Zhao", "Kang Liu"], "title": "SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache Channel Pruning", "comment": null, "summary": "Long-context inference in large language models (LLMs) is increasingly\nconstrained by the KV cache bottleneck: memory usage grows linearly with\nsequence length, while attention computation scales quadratically. Existing\napproaches address this issue by compressing the KV cache along the temporal\naxis through strategies such as token eviction or merging to reduce memory and\ncomputational overhead. However, these methods often neglect fine-grained\nimportance variations across feature dimensions (i.e., the channel axis),\nthereby limiting their ability to effectively balance efficiency and model\naccuracy. In reality, we observe that channel saliency varies dramatically\nacross both queries and positions: certain feature channels carry near-zero\ninformation for a given query, while others spike in relevance. To address this\noversight, we propose SPARK, a training-free plug-and-play method that applies\nunstructured sparsity by pruning KV at the channel level, while dynamically\nrestoring the pruned entries during attention score computation. Notably, our\napproach is orthogonal to existing KV compression and quantization techniques,\nmaking it compatible for integration with them to achieve further acceleration.\nBy reducing channel-level redundancy, SPARK enables processing of longer\nsequences within the same memory budget. For sequences of equal length, SPARK\nnot only preserves or improves model accuracy but also reduces KV cache storage\nby over 30% compared to eviction-based methods. Furthermore, even with an\naggressive pruning ratio of 80%, SPARK maintains performance with less\ndegradation than 5% compared to the baseline eviction method, demonstrating its\nrobustness and effectiveness. Our code will be available at\nhttps://github.com/Xnhyacinth/SparK.", "AI": {"tldr": "SPARK是一种无需训练的即插即用方法，通过在通道级别对KV缓存进行非结构化稀疏剪枝，并动态恢复，有效缓解了大型语言模型中长上下文推理的KV缓存瓶颈，显著减少内存并保持或提高模型精度。", "motivation": "大型语言模型中长上下文推理面临KV缓存瓶颈：内存随序列长度线性增长，注意力计算呈二次方增长。现有方法（如token驱逐或合并）通过时间轴压缩KV缓存，但忽略了特征维度（通道轴）上细粒度的重要性变化，限制了效率与准确性的平衡。研究发现，通道显著性在查询和位置上差异巨大，某些通道信息量接近零，而另一些则高度相关。", "method": "本文提出了SPARK，一种无需训练的即插即用方法。它在通道级别对KV缓存应用非结构化稀疏剪枝，并在注意力分数计算过程中动态恢复被剪枝的条目。该方法与现有KV压缩和量化技术正交，可兼容集成以实现进一步加速。", "result": "SPARK在相同内存预算下能够处理更长的序列。对于等长序列，SPARK相比基于驱逐的方法，不仅保持或提高了模型精度，还减少了超过30%的KV缓存存储。即使在80%的激进剪枝率下，SPARK的性能下降也比基线驱逐方法少于5%，展示了其鲁棒性和有效性。", "conclusion": "SPARK通过在通道级别剪枝KV缓存，有效解决了大型语言模型中的KV缓存瓶颈，显著减少了内存占用，同时保持或提高了模型精度。其无需训练、即插即用的特性以及与其他方法的兼容性，使其成为长上下文推理的有效解决方案。"}}
{"id": "2508.15207", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15207", "abs": "https://arxiv.org/abs/2508.15207", "authors": ["Arjun Srinivasan", "Anubhav Paras", "Aniket Bera"], "title": "Adversarial Agent Behavior Learning in Autonomous Driving Using Deep Reinforcement Learning", "comment": null, "summary": "Existing approaches in reinforcement learning train an agent to learn desired\noptimal behavior in an environment with rule based surrounding agents. In\nsafety critical applications such as autonomous driving it is crucial that the\nrule based agents are modelled properly. Several behavior modelling strategies\nand IDM models are used currently to model the surrounding agents. We present a\nlearning based method to derive the adversarial behavior for the rule based\nagents to cause failure scenarios. We evaluate our adversarial agent against\nall the rule based agents and show the decrease in cumulative reward.", "AI": {"tldr": "本文提出了一种基于学习的方法，用于为强化学习环境中基于规则的智能体生成对抗性行为，以引发故障场景并降低累积奖励，这对于自动驾驶等安全关键应用至关重要。", "motivation": "在自动驾驶等安全关键应用中，现有强化学习方法训练的智能体通常与基于规则的周围智能体交互。正确建模这些基于规则的智能体至关重要，而寻找对抗性行为以暴露潜在的系统故障是研究的关键动机。", "method": "本文提出了一种基于学习的方法来推导基于规则的智能体的对抗性行为。该方法训练一个对抗性智能体，使其能够引发故障场景。然后，该对抗性智能体与所有基于规则的智能体进行评估。", "result": "评估结果表明，所提出的对抗性智能体能够导致累积奖励的显著下降，从而成功地引发了故障场景。", "conclusion": "基于学习的方法可以有效地为基于规则的智能体生成对抗性行为，从而揭示系统在安全关键应用中的脆弱性并导致性能（累积奖励）下降。"}}
{"id": "2508.15204", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15204", "abs": "https://arxiv.org/abs/2508.15204", "authors": ["Raj Jain", "Marc Wetter"], "title": "R-ConstraintBench: Evaluating LLMs on NP-Complete Scheduling", "comment": null, "summary": "Effective scheduling under tight resource, timing, and operational\nconstraints underpins large-scale planning across sectors such as capital\nprojects, manufacturing, logistics, and IT fleet transitions. However, the\nreliability of large language models (LLMs) when reasoning under\nhigh-constraint regimes is insufficiently characterized. To address this gap,\nwe present R-ConstraintBench, a scalable framework that evaluates models on\nResource-Constrained Project Scheduling Problems (RCPSP), an NP-Complete\nfeasibility class, while difficulty increases via linear growth in constraints.\nR-ConstraintBench incrementally increases non-redundant precedence constraints\nin Directed Acyclic Graphs (DAGs) and then introduces downtime, temporal\nwindows, and disjunctive constraints. As an illustrative example, we\ninstantiate the benchmark in a data center migration setting and evaluate\nmultiple LLMs using feasibility and error analysis, identifying degradation\nthresholds and constraint types most associated with failure. Empirically,\nstrong models are near-ceiling on precedence-only DAGs, but feasibility\nperformance collapses when downtime, temporal windows, and disjunctive\nconstraints interact, implicating constraint interaction, not graph depth, as\nthe principal bottleneck. Performance on clean synthetic ramps also does not\nguarantee transfer to domain-grounded scenarios, underscoring limited\ngeneralization.", "AI": {"tldr": "本文提出了R-ConstraintBench框架，用于评估大型语言模型（LLMs）在资源受限项目调度问题（RCPSP）中的推理能力，发现LLMs在处理复杂的交互式约束时性能显著下降，且泛化能力有限。", "motivation": "大规模规划（如资本项目、制造、物流、IT车队转型）需要有效的调度，但大型语言模型（LLMs）在高度约束条件下的推理可靠性尚未得到充分表征，存在研究空白。", "method": "本文提出了R-ConstraintBench框架，通过线性增加非冗余先行约束（在有向无环图DAGs中），并逐步引入停机时间、时间窗口和析取约束，来评估LLMs在NP-完全可行性类RCPSP上的表现。以数据中心迁移为例，通过可行性分析和错误分析来评估多个LLMs，识别性能退化阈值和与失败最相关的约束类型。", "result": "实验结果表明，LLMs在仅包含先行约束的DAGs上表现接近上限，但当停机时间、时间窗口和析取约束相互作用时，可行性表现急剧下降。这表明约束交互而非图深度是主要的瓶颈。此外，模型在纯粹的合成场景下的性能并不能保证其在领域接地场景中的迁移能力，凸显了有限的泛化能力。", "conclusion": "LLMs在处理复杂的、交互式的调度约束时表现出显著的局限性，其在合成数据上的良好表现并不能保证在实际应用中的泛化能力。约束交互是LLMs在调度问题中面临的主要挑战。"}}
{"id": "2508.15635", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.15635", "abs": "https://arxiv.org/abs/2508.15635", "authors": ["Malini Shivaram", "Gautam Rajendrakumar Gare", "Laura Hutchins", "Jacob Duplantis", "Thomas Deiss", "Thales Nogueira Gomes", "Thong Tran", "Keyur H. Patel", "Thomas H Fox", "Amita Krishnan", "Deva Ramanan", "Bennett DeBoisblanc", "Ricardo Rodriguez", "John Galeotti"], "title": "Label Uncertainty for Ultrasound Segmentation", "comment": "Paper under review", "summary": "In medical imaging, inter-observer variability among radiologists often\nintroduces label uncertainty, particularly in modalities where visual\ninterpretation is subjective. Lung ultrasound (LUS) is a prime example-it\nfrequently presents a mixture of highly ambiguous regions and clearly\ndiscernible structures, making consistent annotation challenging even for\nexperienced clinicians. In this work, we introduce a novel approach to both\nlabeling and training AI models using expert-supplied, per-pixel confidence\nvalues. Rather than treating annotations as absolute ground truth, we design a\ndata annotation protocol that captures the confidence that radiologists have in\neach labeled region, modeling the inherent aleatoric uncertainty present in\nreal-world clinical data. We demonstrate that incorporating these confidence\nvalues during training leads to improved segmentation performance. More\nimportantly, we show that this enhanced segmentation quality translates into\nbetter performance on downstream clinically-critical tasks-specifically,\nestimating S/F oxygenation ratio values, classifying S/F ratio change, and\npredicting 30-day patient readmission. While we empirically evaluate many\nmethods for exposing the uncertainty to the learning model, we find that a\nsimple approach that trains a model on binarized labels obtained with a (60%)\nconfidence threshold works well. Importantly, high thresholds work far better\nthan a naive approach of a 50% threshold, indicating that training on very\nconfident pixels is far more effective. Our study systematically investigates\nthe impact of training with varying confidence thresholds, comparing not only\nsegmentation metrics but also downstream clinical outcomes. These results\nsuggest that label confidence is a valuable signal that, when properly\nleveraged, can significantly enhance the reliability and clinical utility of AI\nin medical imaging.", "AI": {"tldr": "本研究提出一种利用专家提供的像素级置信度值进行标注和训练AI模型的方法，以解决医学影像（特别是肺部超声）中的标签不确定性问题，并证明该方法能显著提升分割性能和下游临床任务的表现。", "motivation": "医学影像中，放射科医生之间的观察者间差异常导致标签不确定性，尤其在肺部超声等主观性强的模态中，图像常包含模糊区域和清晰结构，使得一致性标注极具挑战性。", "method": "设计了一种数据标注协议，捕捉放射科医生对每个标注区域的像素级置信度，以建模临床数据中固有的不确定性。在AI模型训练中融入这些置信度值，并通过系统性地调查不同置信度阈值（例如60% vs 50%）下二值化标签的训练效果，评估其对分割指标和下游临床结果的影响。", "result": "将置信度值纳入训练能改善分割性能。更重要的是，这种增强的分割质量能转化为下游关键临床任务（如估算S/F氧合比、分类S/F比变化、预测30天患者再入院）的更好表现。研究发现，使用60%置信度阈值获得的二值化标签训练模型效果良好，且高阈值（如60%）远优于朴素的50%阈值方法，表明在高度自信的像素上训练更有效。", "conclusion": "标签置信度是一个有价值的信号，如果利用得当，可以显著提高AI在医学影像中的可靠性和临床实用性。"}}
{"id": "2508.15213", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.15213", "abs": "https://arxiv.org/abs/2508.15213", "authors": ["Bolei He", "Xinran He", "Run Shao", "Shanfu Shu", "Xianwei Xue", "Mingquan Cheng", "Haifeng Li", "Zhenhua Ling"], "title": "Select to Know: An Internal-External Knowledge Self-Selection Framework for Domain-Specific Question Answering", "comment": "EMNLP2025 Findings", "summary": "Large Language Models (LLMs) perform well in general QA but often struggle in\ndomain-specific scenarios. Retrieval-Augmented Generation (RAG) introduces\nexternal knowledge but suffers from hallucinations and latency due to noisy\nretrievals. Continued pretraining internalizes domain knowledge but is costly\nand lacks cross-domain flexibility. We attribute this challenge to the\nlong-tail distribution of domain knowledge, which leaves partial yet useful\ninternal knowledge underutilized. We further argue that knowledge acquisition\nshould be progressive, mirroring human learning: first understanding concepts,\nthen applying them to complex reasoning. To address this, we propose Selct2Know\n(S2K), a cost-effective framework that internalizes domain knowledge through an\ninternal-external knowledge self-selection strategy and selective supervised\nfine-tuning. We also introduce a structured reasoning data generation pipeline\nand integrate GRPO to enhance reasoning ability. Experiments on medical, legal,\nand financial QA benchmarks show that S2K consistently outperforms existing\nmethods and matches domain-pretrained LLMs with significantly lower cost.", "AI": {"tldr": "Selct2Know (S2K) 是一个经济高效的框架，通过内部-外部知识自选择和选择性监督微调，帮助大型语言模型内化领域知识，解决了领域特定问答中的挑战，并在多个领域基准测试中表现优异。", "motivation": "大型语言模型在通用问答中表现良好，但在领域特定场景中表现不佳。检索增强生成（RAG）存在幻觉和延迟问题，而持续预训练成本高昂且缺乏跨领域灵活性。作者将此归因于领域知识的长尾分布导致部分内部知识未被充分利用，并认为知识获取应遵循人类学习的渐进模式。", "method": "本文提出了Selct2Know (S2K) 框架，通过内部-外部知识自选择策略和选择性监督微调来内化领域知识。此外，还引入了结构化推理数据生成管道，并集成了GRPO以增强推理能力。", "result": "在医学、法律和金融问答基准测试中，S2K持续优于现有方法，并以显著更低的成本达到了领域预训练大型语言模型的性能水平。", "conclusion": "S2K是一个经济高效的框架，能够有效地内化领域知识，显著提升大型语言模型在领域特定问答中的表现，并能与领域预训练模型相媲美，同时大大降低了成本。"}}
{"id": "2508.15208", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15208", "abs": "https://arxiv.org/abs/2508.15208", "authors": ["Leiyue Zhao", "Yuechen Yang", "Yanfan Zhu", "Haichun Yang", "Yuankai Huo", "Paul D. Simonson", "Kenji Ikemura", "Mert R. Sabuncu", "Yihe Yang", "Ruining Deng"], "title": "DyMorph-B2I: Dynamic and Morphology-Guided Binary-to-Instance Segmentation for Renal Pathology", "comment": "9 pages, 5 figures", "summary": "Accurate morphological quantification of renal pathology functional units\nrelies on instance-level segmentation, yet most existing datasets and automated\nmethods provide only binary (semantic) masks, limiting the precision of\ndownstream analyses. Although classical post-processing techniques such as\nwatershed, morphological operations, and skeletonization, are often used to\nseparate semantic masks into instances, their individual effectiveness is\nconstrained by the diverse morphologies and complex connectivity found in renal\ntissue. In this study, we present DyMorph-B2I, a dynamic, morphology-guided\nbinary-to-instance segmentation pipeline tailored for renal pathology. Our\napproach integrates watershed, skeletonization, and morphological operations\nwithin a unified framework, complemented by adaptive geometric refinement and\ncustomizable hyperparameter tuning for each class of functional unit. Through\nsystematic parameter optimization, DyMorph-B2I robustly separates adherent and\nheterogeneous structures present in binary masks. Experimental results\ndemonstrate that our method outperforms individual classical approaches and\nna\\\"ive combinations, enabling superior instance separation and facilitating\nmore accurate morphometric analysis in renal pathology workflows. The pipeline\nis publicly available at: https://github.com/ddrrnn123/DyMorph-B2I.", "AI": {"tldr": "DyMorph-B2I是一个动态、形态学引导的二值到实例分割流程，专为肾脏病理学设计，通过整合多种经典技术并进行自适应优化，显著提高了肾脏功能单元的实例分割精度。", "motivation": "现有的肾脏病理学图像分析方法主要提供二值（语义）掩膜，限制了下游分析的精确性。经典的后处理技术（如分水岭、形态学操作、骨架化）受限于肾脏组织形态的多样性和连接复杂性，单独使用效果有限。", "method": "本研究提出了DyMorph-B2I，一个将分水岭、骨架化和形态学操作整合到统一框架中的动态、形态学引导的二值到实例分割流程。该方法还包括自适应几何精修和针对每类功能单元的可定制超参数调整，并通过系统参数优化来稳健分离二值掩膜中粘连和异构的结构。", "result": "实验结果表明，DyMorph-B2I在分离粘连和异构结构方面表现出色，优于单独的经典方法和简单的组合，实现了卓越的实例分离，并促进了肾脏病理工作流程中更精确的形态计量分析。", "conclusion": "DyMorph-B2I是一个有效且公开可用的肾脏病理学实例分割流程，能够显著提升形态学量化的准确性，为肾脏病理学研究提供更精细的数据分析基础。"}}
{"id": "2508.15222", "categories": ["cs.AI", "cs.CV", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.15222", "abs": "https://arxiv.org/abs/2508.15222", "authors": ["Hantao Zhang", "Jingyang Liu", "Ed Li"], "title": "See it. Say it. Sorted: Agentic System for Compositional Diagram Generation", "comment": null, "summary": "We study sketch-to-diagram generation: converting rough hand sketches into\nprecise, compositional diagrams. Diffusion models excel at photorealism but\nstruggle with the spatial precision, alignment, and symbolic structure required\nfor flowcharts. We introduce See it. Say it. Sorted., a training-free agentic\nsystem that couples a Vision-Language Model (VLM) with Large Language Models\n(LLMs) to produce editable Scalable Vector Graphics (SVG) programs. The system\nruns an iterative loop in which a Critic VLM proposes a small set of\nqualitative, relational edits; multiple candidate LLMs synthesize SVG updates\nwith diverse strategies (conservative->aggressive, alternative, focused); and a\nJudge VLM selects the best candidate, ensuring stable improvement. This design\nprioritizes qualitative reasoning over brittle numerical estimates, preserves\nglobal constraints (e.g., alignment, connectivity), and naturally supports\nhuman-in-the-loop corrections. On 10 sketches derived from flowcharts in\npublished papers, our method more faithfully reconstructs layout and structure\nthan two frontier closed-source image generation LLMs (GPT-5 and\nGemini-2.5-Pro), accurately composing primitives (e.g., multi-headed arrows)\nwithout inserting unwanted text. Because outputs are programmatic SVGs, the\napproach is readily extensible to presentation tools (e.g., PowerPoint) via\nAPIs and can be specialized with improved prompts and task-specific tools. The\ncodebase is open-sourced at\nhttps://github.com/hantaoZhangrichard/see_it_say_it_sorted.git.", "AI": {"tldr": "该研究提出了一种名为“See it. Say it. Sorted.”的无训练智能体系统，利用视觉-语言模型（VLM）和大型语言模型（LLM）将手绘草图转换为精确、可编辑的SVG图表，解决了扩散模型在图表空间精度和符号结构方面的不足。", "motivation": "扩散模型在生成逼真的图像方面表现出色，但在生成流程图等图表时，难以满足空间精度、对齐和符号结构等要求。研究旨在开发一种能够将粗略手绘草图转换为精确、可编辑的组合图表的方法。", "method": "该系统是一个无训练的智能体系统，通过迭代循环将VLM（作为评论者和评判者）与LLM（作为候选生成器）相结合，生成可编辑的SVG程序。评论者VLM提出定性的关系编辑，多个候选LLM以不同策略（保守到激进、替代、专注）合成SVG更新，评判者VLM选择最佳候选以确保稳定改进。此设计优先考虑定性推理，保留全局约束（如对齐、连接），并支持人工干预修正。", "result": "在来自已发表论文的10个流程图草图上，该方法比两个前沿的闭源图像生成LLM（GPT-5和Gemini-2.5-Pro）更忠实地重建了布局和结构，准确地组合了基本图形（例如，多头箭头）而没有插入不必要的文本。由于输出是程序化SVG，该方法易于通过API扩展到演示工具（例如PowerPoint），并且可以通过改进的提示和特定任务工具进行专业化。", "conclusion": "所提出的“See it. Say it. Sorted.”系统通过结合VLM和LLM的智能体方法，成功地将手绘草图转换为精确、可编辑的SVG图表。它在空间精度和结构忠实度方面优于现有的先进图像生成模型，并且具有良好的可扩展性和人机协作能力。"}}
{"id": "2508.15660", "categories": ["eess.IV", "cs.CV", "I.4.6; I.5.4; J.3"], "pdf": "https://arxiv.org/pdf/2508.15660", "abs": "https://arxiv.org/abs/2508.15660", "authors": ["Alexandra Bernadotte", "Elfimov Nikita", "Mikhail Shutov", "Ivan Menshikov"], "title": "Hessian-based lightweight neural network for brain vessel segmentation on a minimal training dataset", "comment": "11 pages, 2 figures", "summary": "Accurate segmentation of blood vessels in brain magnetic resonance\nangiography (MRA) is essential for successful surgical procedures, such as\naneurysm repair or bypass surgery. Currently, annotation is primarily performed\nthrough manual segmentation or classical methods, such as the Frangi filter,\nwhich often lack sufficient accuracy. Neural networks have emerged as powerful\ntools for medical image segmentation, but their development depends on\nwell-annotated training datasets. However, there is a notable lack of publicly\navailable MRA datasets with detailed brain vessel annotations.\n  To address this gap, we propose a novel semi-supervised learning lightweight\nneural network with Hessian matrices on board for 3D segmentation of complex\nstructures such as tubular structures, which we named HessNet. The solution is\na Hessian-based neural network with only 6000 parameters. HessNet can run on\nthe CPU and significantly reduces the resource requirements for training neural\nnetworks. The accuracy of vessel segmentation on a minimal training dataset\nreaches state-of-the-art results. It helps us create a large, semi-manually\nannotated brain vessel dataset of brain MRA images based on the IXI dataset\n(annotated 200 images). Annotation was performed by three experts under the\nsupervision of three neurovascular surgeons after applying HessNet. It provides\nhigh accuracy of vessel segmentation and allows experts to focus only on the\nmost complex important cases. The dataset is available at\nhttps://git.scinalytics.com/terilat/VesselDatasetPartly.", "AI": {"tldr": "本文提出了一种名为HessNet的轻量级半监督神经网络，结合Hessian矩阵进行脑部MRA血管的3D分割，并在极少量训练数据下达到了SOTA精度。该网络用于辅助创建了一个大型半手动标注的脑血管数据集，以解决公开MRA数据集缺乏详细标注的问题。", "motivation": "脑部MRA血管的精确分割对于动脉瘤修复或搭桥手术等外科手术至关重要。目前，主要依赖手动分割或传统方法（如Frangi滤波器），但这些方法往往精度不足。尽管神经网络在医学图像分割中表现强大，但其发展受限于高质量的标注训练数据集，而目前公开的MRA脑血管标注数据集严重缺乏。", "method": "本文提出了一种名为HessNet的新型轻量级半监督神经网络，该网络结合了Hessian矩阵，用于复杂管状结构（如血管）的3D分割。HessNet仅有6000个参数，可在CPU上运行，显著降低了训练资源需求。该网络还被用于辅助专家进行半手动标注，从而创建了一个基于IXI数据集的大型脑部MRA血管数据集（标注了200张图像）。", "result": "HessNet在极少量训练数据集上实现了最先进的血管分割精度。该网络非常轻量级，可以在CPU上运行，并且显著减少了神经网络训练所需的资源。通过HessNet的辅助，三位专家在三位神经血管外科医生的监督下，成功创建了一个包含200张图像的大型半手动标注脑血管数据集，该方法提高了血管分割的准确性，并使专家能够专注于最复杂的病例。", "conclusion": "HessNet作为一种结合Hessian矩阵的轻量级半监督神经网络，在MRA脑血管3D分割方面表现出高精度，并显著降低了资源需求。它不仅提供了一个有效的分割工具，还成功解决了高质量标注MRA数据集缺乏的问题，通过辅助专家创建了一个大型数据集，为未来的研究和临床应用提供了宝贵的资源。"}}
{"id": "2508.15214", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.15214", "abs": "https://arxiv.org/abs/2508.15214", "authors": ["Sijia Cui", "Aiyao He", "Shuai Xu", "Hongming Zhang", "Yanna Wang", "Qingyang Zhang", "Yajing Wang", "Bo Xu"], "title": "Self-Guided Function Calling in Large Language Models via Stepwise Experience Recall", "comment": "Accepted to EMNLP 2025", "summary": "Function calling enables large language models (LLMs) to interact with\nexternal systems by leveraging tools and APIs. When faced with multi-step tool\nusage, LLMs still struggle with tool selection, parameter generation, and\ntool-chain planning. Existing methods typically rely on manually designing\ntask-specific demonstrations, or retrieving from a curated library. These\napproaches demand substantial expert effort and prompt engineering becomes\nincreasingly complex and inefficient as tool diversity and task difficulty\nscale. To address these challenges, we propose a self-guided method, Stepwise\nExperience Recall (SEER), which performs fine-grained, stepwise retrieval from\na continually updated experience pool. Instead of relying on static or manually\ncurated library, SEER incrementally augments the experience pool with past\nsuccessful trajectories, enabling continuous expansion of the pool and improved\nmodel performance over time. Evaluated on the ToolQA benchmark, SEER achieves\nan average improvement of 6.1\\% on easy and 4.7\\% on hard questions. We further\ntest SEER on $\\tau$-bench, which includes two real-world domains. Powered by\nQwen2.5-7B and Qwen2.5-72B models, SEER demonstrates substantial accuracy gains\nof 7.44\\% and 23.38\\%, respectively.", "AI": {"tldr": "本文提出了一种名为SEER的自引导方法，通过从持续更新的经验池中逐步检索细粒度经验，显著提升了大型语言模型（LLMs）在多步工具使用场景中的表现，解决了现有方法对专家知识和提示工程的依赖问题。", "motivation": "大型语言模型在处理多步工具使用时，在工具选择、参数生成和工具链规划方面仍面临挑战。现有方法通常依赖手动设计任务特定演示或从精心策划的库中检索，这些方法需要大量专家投入，且随着工具多样性和任务难度的增加，提示工程变得复杂且效率低下。", "method": "本文提出了一种自引导方法——分步经验回忆（SEER）。该方法从一个持续更新的经验池中进行细粒度的分步检索。与依赖静态或手动策划库不同，SEER通过将过去成功的轨迹逐步添加到经验池中，实现经验池的持续扩展和模型性能的逐步提升。", "result": "在ToolQA基准测试中，SEER在简单问题上平均提高了6.1%，在困难问题上平均提高了4.7%。在包含两个真实世界领域的$\tau$-bench上，SEER结合Qwen2.5-7B和Qwen2.5-72B模型分别实现了7.44%和23.38%的显著准确率提升。", "conclusion": "SEER通过其自引导的、逐步的经验检索和持续更新的经验池机制，有效解决了LLMs在多步工具使用中的挑战，实现了显著的性能提升，且无需大量手动干预。"}}
{"id": "2508.15216", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15216", "abs": "https://arxiv.org/abs/2508.15216", "authors": ["Vipooshan Vipulananthan", "Kumudu Mohottala", "Kavindu Chinthana", "Nimsara Paramulla", "Charith D Chitraranjan"], "title": "STAGNet: A Spatio-Temporal Graph and LSTM Framework for Accident Anticipation", "comment": null, "summary": "Accident prediction and timely warnings play a key role in improving road\nsafety by reducing the risk of injury to road users and minimizing property\ndamage. Advanced Driver Assistance Systems (ADAS) are designed to support human\ndrivers and are especially useful when they can anticipate potential accidents\nbefore they happen. While many existing systems depend on a range of sensors\nsuch as LiDAR, radar, and GPS, relying solely on dash-cam video input presents\na more challenging but a more cost-effective and easily deployable solution. In\nthis work, we incorporate better spatio-temporal features and aggregate them\nthrough a recurrent network to improve upon state-of-the-art graph neural\nnetworks for predicting accidents from dash-cam videos. Experiments using three\npublicly available datasets show that our proposed STAGNet model achieves\nhigher average precision and mean time-to-collision values than previous\nmethods, both when cross-validated on a given dataset and when trained and\ntested on different datasets.", "AI": {"tldr": "本文提出了一种名为STAGNet的新模型，通过融合时空特征和循环网络，改进了基于行车记录仪视频的事故预测，并在多个公开数据集上超越了现有技术。", "motivation": "提高道路安全、减少事故风险和财产损失是关键。先进驾驶辅助系统（ADAS）需要能提前预测事故。现有系统多依赖多种传感器，而仅依赖行车记录仪视频提供了一种更具挑战性但更经济、易于部署的解决方案。", "method": "该研究结合了更好的时空特征，并通过循环网络对其进行聚合，以改进现有最先进的图神经网络。提出的模型名为STAGNet，用于从行车记录仪视频中预测事故。", "result": "在三个公开数据集上进行的实验表明，所提出的STAGNet模型在平均精度和平均碰撞时间（time-to-collision）值上均高于以往方法，无论是在给定数据集上进行交叉验证，还是在不同数据集上进行训练和测试。", "conclusion": "STAGNet模型通过结合改进的时空特征和循环网络，显著提升了仅基于行车记录仪视频的事故预测性能，为道路安全提供了更有效、更具成本效益的解决方案。"}}
{"id": "2508.15240", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15240", "abs": "https://arxiv.org/abs/2508.15240", "authors": ["Sabab Aosaf", "Muhammad Ali Nayeem", "Afsana Haque", "M Sohel Rahmana"], "title": "Computational Intelligence based Land-use Allocation Approaches for Mixed Use Areas", "comment": null, "summary": "Urban land-use allocation represents a complex multi-objective optimization\nproblem critical for sustainable urban development policy. This paper presents\nnovel computational intelligence approaches for optimizing land-use allocation\nin mixed-use areas, addressing inherent trade-offs between land-use\ncompatibility and economic objectives. We develop multiple optimization\nalgorithms, including custom variants integrating differential evolution with\nmulti-objective genetic algorithms. Key contributions include: (1) CR+DES\nalgorithm leveraging scaled difference vectors for enhanced exploration, (2)\nsystematic constraint relaxation strategy improving solution quality while\nmaintaining feasibility, and (3) statistical validation using Kruskal-Wallis\ntests with compact letter displays. Applied to a real-world case study with\n1,290 plots, CR+DES achieves 3.16\\% improvement in land-use compatibility\ncompared to state-of-the-art methods, while MSBX+MO excels in price\noptimization with 3.3\\% improvement. Statistical analysis confirms algorithms\nincorporating difference vectors significantly outperform traditional\napproaches across multiple metrics. The constraint relaxation technique enables\nbroader solution space exploration while maintaining practical constraints.\nThese findings provide urban planners and policymakers with evidence-based\ncomputational tools for balancing competing objectives in land-use allocation,\nsupporting more effective urban development policies in rapidly urbanizing\nregions.", "AI": {"tldr": "本文提出并验证了用于优化城市土地利用分配的新型计算智能方法，旨在平衡土地利用兼容性和经济目标，并通过定制算法和约束松弛策略显著提升了解决方案质量。", "motivation": "城市土地利用分配是一个复杂的、多目标优化问题，对可持续城市发展政策至关重要，且在土地利用兼容性和经济目标之间存在固有的权衡。", "method": "研究开发了多种优化算法，包括将差分进化与多目标遗传算法相结合的定制变体。主要贡献包括：(1) CR+DES算法利用缩放差分向量增强探索；(2) 系统的约束松弛策略在保持可行性的同时提高解决方案质量；(3) 使用Kruskal-Wallis检验和紧凑字母显示进行统计验证。", "result": "在包含1,290个地块的真实案例研究中，CR+DES算法在土地利用兼容性方面比现有技术提高了3.16%，而MSBX+MO在价格优化方面提高了3.3%。统计分析证实，结合差分向量的算法在多个指标上显著优于传统方法。约束松弛技术支持更广泛的解决方案空间探索。", "conclusion": "这些发现为城市规划者和政策制定者提供了基于证据的计算工具，以平衡土地利用分配中的相互竞争目标，从而支持快速城市化地区更有效的城市发展政策。"}}
{"id": "2508.15672", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.15672", "abs": "https://arxiv.org/abs/2508.15672", "authors": ["Franz Hanke", "Antonia Bieringer", "Olaf Wysocki", "Boris Jutzi"], "title": "CM2LoD3: Reconstructing LoD3 Building Models Using Semantic Conflict Maps", "comment": "This paper was accepted for the 20th 3D GeoInfo & 9th Smart Data\n  Smart Cities Conference", "summary": "Detailed 3D building models are crucial for urban planning, digital twins,\nand disaster management applications. While Level of Detail 1 (LoD)1 and LoD2\nbuilding models are widely available, they lack detailed facade elements\nessential for advanced urban analysis. In contrast, LoD3 models address this\nlimitation by incorporating facade elements such as windows, doors, and\nunderpasses. However, their generation has traditionally required manual\nmodeling, making large-scale adoption challenging. In this contribution,\nCM2LoD3, we present a novel method for reconstructing LoD3 building models\nleveraging Conflict Maps (CMs) obtained from ray-to-model-prior analysis.\nUnlike previous works, we concentrate on semantically segmenting real-world CMs\nwith synthetically generated CMs from our developed Semantic Conflict Map\nGenerator (SCMG). We also observe that additional segmentation of textured\nmodels can be fused with CMs using confidence scores to further increase\nsegmentation performance and thus increase 3D reconstruction accuracy.\nExperimental results demonstrate the effectiveness of our CM2LoD3 method in\nsegmenting and reconstructing building openings, with the 61% performance with\nuncertainty-aware fusion of segmented building textures. This research\ncontributes to the advancement of automated LoD3 model reconstruction, paving\nthe way for scalable and efficient 3D city modeling. Our project is available:\nhttps://github.com/InFraHank/CM2LoD3", "AI": {"tldr": "本研究提出了一种名为CM2LoD3的新方法，利用冲突图（CMs）和语义分割，结合纹理模型融合，实现了LoD3建筑模型的自动化重建，有效解决了大规模生成LoD3模型的挑战。", "motivation": "LoD3建筑模型对于城市规划、数字孪生和灾害管理至关重要，但传统的LoD3模型生成需要手动建模，难以大规模应用。现有LoD1和LoD2模型缺乏详细的立面元素（如窗户、门），无法满足高级城市分析的需求。", "method": "CM2LoD3方法通过射线-模型先验分析获取冲突图（CMs）。该方法的核心是对真实世界的CMs进行语义分割，并结合从自研语义冲突图生成器（SCMG）合成生成的CMs。此外，他们还将纹理模型的额外分割结果与CMs通过置信度分数进行融合，以提高分割性能和三维重建精度。", "result": "实验结果表明，CM2LoD3方法在分割和重建建筑开口方面表现出有效性，特别是在融合了具有不确定性感知的分段建筑纹理后，性能达到了61%。", "conclusion": "这项研究推动了LoD3模型自动化重建的进展，为可扩展和高效的三维城市建模铺平了道路。"}}
{"id": "2508.15218", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.15218", "abs": "https://arxiv.org/abs/2508.15218", "authors": ["Momoka Furuhashi", "Kouta Nakayama", "Takashi Kodama", "Saku Sugawara"], "title": "Are Checklists Really Useful for Automatic Evaluation of Generative Tasks?", "comment": "Accepted to the EMNLP 2025 Main Conference", "summary": "Automatic evaluation of generative tasks using large language models faces\nchallenges due to ambiguous criteria. Although automatic checklist generation\nis a potentially promising approach, its usefulness remains underexplored. We\ninvestigate whether checklists should be used for all questions or selectively,\ngenerate them using six methods, evaluate their effectiveness across eight\nmodel sizes, and identify checklist items that correlate with human\nevaluations. Through experiments on pairwise comparison and direct scoring\ntasks, we find that selective checklist use tends to improve evaluation\nperformance in pairwise settings, while its benefits are less consistent in\ndirect scoring. Our analysis also shows that even checklist items with low\ncorrelation to human scores often reflect human-written criteria, indicating\npotential inconsistencies in human evaluation. These findings highlight the\nneed to more clearly define objective evaluation criteria to guide both human\nand automatic evaluations. \\footnote{Our code is available\nat~https://github.com/momo0817/checklist-effectiveness-study", "AI": {"tldr": "本研究探讨了在大型语言模型中，选择性使用自动生成的评估清单可以改善生成任务的配对评估性能，但在直接评分中效果不一，并揭示了人类评估标准可能存在不一致。", "motivation": "由于评估标准模糊，使用大型语言模型自动评估生成任务面临挑战。自动生成评估清单是一种有前景的方法，但其有效性尚未得到充分探索。", "method": "研究调查了评估清单应全面使用还是选择性使用；使用六种方法生成评估清单；在八种不同模型尺寸下评估其有效性；并识别出与人类评估相关的清单项目。实验在配对比较和直接评分任务中进行。", "result": "研究发现，选择性使用评估清单倾向于改善配对设置中的评估性能，但在直接评分中的益处不太一致。分析还表明，即使与人类评分相关性低的清单项目也常反映人类编写的标准，这表明人类评估可能存在不一致性。", "conclusion": "这些发现强调了需要更清晰地定义客观评估标准，以指导人类和自动评估，从而提高生成任务评估的准确性和一致性。"}}
{"id": "2508.15228", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15228", "abs": "https://arxiv.org/abs/2508.15228", "authors": ["Ziang Cao", "Zhaoxi Chen", "Liang Pan", "Ziwei Liu"], "title": "Collaborative Multi-Modal Coding for High-Quality 3D Generation", "comment": null, "summary": "3D content inherently encompasses multi-modal characteristics and can be\nprojected into different modalities (e.g., RGB images, RGBD, and point clouds).\nEach modality exhibits distinct advantages in 3D asset modeling: RGB images\ncontain vivid 3D textures, whereas point clouds define fine-grained 3D\ngeometries. However, most existing 3D-native generative architectures either\noperate predominantly within single-modality paradigms-thus overlooking the\ncomplementary benefits of multi-modality data-or restrict themselves to 3D\nstructures, thereby limiting the scope of available training datasets. To\nholistically harness multi-modalities for 3D modeling, we present TriMM, the\nfirst feed-forward 3D-native generative model that learns from basic\nmulti-modalities (e.g., RGB, RGBD, and point cloud). Specifically, 1) TriMM\nfirst introduces collaborative multi-modal coding, which integrates\nmodality-specific features while preserving their unique representational\nstrengths. 2) Furthermore, auxiliary 2D and 3D supervision are introduced to\nraise the robustness and performance of multi-modal coding. 3) Based on the\nembedded multi-modal code, TriMM employs a triplane latent diffusion model to\ngenerate 3D assets of superior quality, enhancing both the texture and the\ngeometric detail. Extensive experiments on multiple well-known datasets\ndemonstrate that TriMM, by effectively leveraging multi-modality, achieves\ncompetitive performance with models trained on large-scale datasets, despite\nutilizing a small amount of training data. Furthermore, we conduct additional\nexperiments on recent RGB-D datasets, verifying the feasibility of\nincorporating other multi-modal datasets into 3D generation.", "AI": {"tldr": "TriMM是一种前馈3D原生生成模型，通过协同多模态编码、2D/3D辅助监督和三平面潜在扩散模型，从RGB、RGBD和点云等基本多模态数据中学习，以少量数据生成高质量3D资产。", "motivation": "现有3D生成模型大多是单模态的，忽略了多模态数据的互补优势，或受限于3D结构，限制了可用训练数据集的范围。", "method": "1) 引入协同多模态编码，整合模态特定特征同时保留其独特表示优势。2) 引入辅助2D和3D监督以提高多模态编码的鲁棒性和性能。3) 基于嵌入的多模态编码，采用三平面潜在扩散模型生成高质量3D资产，提升纹理和几何细节。", "result": "在多个知名数据集上的实验表明，TriMM通过有效利用多模态，即使使用少量训练数据，也能达到与在大型数据集上训练的模型相当的性能。此外，在RGB-D数据集上的实验验证了将其他多模态数据集整合到3D生成中的可行性。", "conclusion": "TriMM成功地利用多模态数据实现了高质量的3D资产生成，并且在数据量较少的情况下也能表现出色，证明了多模态方法在3D生成领域的潜力和有效性。"}}
{"id": "2508.15294", "categories": ["cs.AI", "cs.CL", "cs.MA", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.15294", "abs": "https://arxiv.org/abs/2508.15294", "authors": ["Gaoke Zhang", "Bo Wang", "Yunlong Ma", "Dongming Zhao", "Zifei Yu"], "title": "Multiple Memory Systems for Enhancing the Long-term Memory of Agent", "comment": null, "summary": "An agent powered by large language models have achieved impressive results,\nbut effectively handling the vast amounts of historical data generated during\ninteractions remains a challenge. The current approach is to design a memory\nmodule for the agent to process these data. However, existing methods, such as\nMemoryBank and A-MEM, have poor quality of stored memory content, which affects\nrecall performance and response quality. In order to better construct\nhigh-quality long-term memory content, we have designed a multiple memory\nsystem (MMS) inspired by cognitive psychology theory. The system processes\nshort-term memory to multiple long-term memory fragments, and constructs\nretrieval memory units and contextual memory units based on these fragments,\nwith a one-to-one correspondence between the two. During the retrieval phase,\nMMS will match the most relevant retrieval memory units based on the user's\nquery. Then, the corresponding contextual memory units is obtained as the\ncontext for the response stage to enhance knowledge, thereby effectively\nutilizing historical data. Experiments on LoCoMo dataset compared our method\nwith three others, proving its effectiveness. Ablation studies confirmed the\nrationality of our memory units. We also analyzed the robustness regarding the\nnumber of selected memory segments and the storage overhead, demonstrating its\npractical value.", "AI": {"tldr": "本文提出了一种受认知心理学启发的多种记忆系统（MMS），用于处理大型语言模型（LLM）代理的历史数据。MMS通过构建检索记忆单元和上下文记忆单元来提高长期记忆内容的质量，从而有效提升代理的召回和响应表现。", "motivation": "当前LLM代理在处理大量历史数据时面临挑战，现有记忆模块（如MemoryBank和A-MEM）存储的记忆内容质量不佳，影响了召回性能和响应质量。因此，需要一种方法来更好地构建高质量的长期记忆内容。", "method": "本文设计了一种受认知心理学理论启发的多种记忆系统（MMS）。该系统将短期记忆处理成多个长期记忆片段，并基于这些片段构建检索记忆单元和上下文记忆单元，两者之间存在一对一的对应关系。在检索阶段，MMS根据用户查询匹配最相关的检索记忆单元，然后获取相应的上下文记忆单元作为响应阶段的上下文，以增强知识利用。", "result": "在LoCoMo数据集上的实验证明了MMS相较于其他三种方法的有效性。消融研究证实了所设计记忆单元的合理性。此外，对选定记忆片段数量和存储开销的鲁棒性分析也展示了其在实际应用中的价值。", "conclusion": "MMS通过构建高质量的长期记忆内容，有效地利用了历史数据，从而显著改善了LLM代理的召回性能和响应质量，并具有实际应用价值。"}}
{"id": "2508.15229", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.15229", "abs": "https://arxiv.org/abs/2508.15229", "authors": ["Hanling Zhang", "Yayu Zhou", "Tongcheng Fang", "Zhihang Yuan", "Guohao Dai", "Yu Wang"], "title": "VocabTailor: Dynamic Vocabulary Selection for Downstream Tasks in Small Language Models", "comment": null, "summary": "Small Language Models (SLMs) provide computational advantages in\nresource-constrained environments, yet memory limitations remain a critical\nbottleneck for edge device deployment. A substantial portion of SLMs' memory\nfootprint stems from vocabulary-related components, particularly embeddings and\nlanguage modeling (LM) heads, due to large vocabulary sizes. Existing static\nvocabulary pruning, while reducing memory usage, suffers from rigid,\none-size-fits-all designs that cause information loss from the prefill stage\nand a lack of flexibility. In this work, we identify two key principles\nunderlying the vocabulary reduction challenge: the lexical locality principle,\nthe observation that only a small subset of tokens is required during any\nsingle inference, and the asymmetry in computational characteristics between\nvocabulary-related components of SLM. Based on these insights, we introduce\nVocabTailor, a novel decoupled dynamic vocabulary selection framework that\naddresses memory constraints through offloading embedding and implements a\nhybrid static-dynamic vocabulary selection strategy for LM Head, enabling\non-demand loading of vocabulary components. Comprehensive experiments across\ndiverse downstream tasks demonstrate that VocabTailor achieves a reduction of\nup to 99% in the memory usage of vocabulary-related components with minimal or\nno degradation in task performance, substantially outperforming existing static\nvocabulary pruning.", "AI": {"tldr": "本文提出VocabTailor，一个解耦动态词汇选择框架，通过卸载嵌入层和对LM头采用混合静态-动态策略，显著减少小型语言模型（SLM）在边缘设备上的词汇相关组件内存占用，同时保持性能。", "motivation": "小型语言模型（SLM）在资源受限环境中具有计算优势，但其内存限制是边缘设备部署的关键瓶颈。词汇相关组件（特别是嵌入层和语言模型头）因词汇量大而占用大量内存。现有静态词汇剪枝方法过于僵化，导致预填充阶段信息丢失且缺乏灵活性。", "method": "基于词汇局部性原则（单次推理仅需少量词元）和SLM词汇相关组件计算特性不对称的洞察，本文提出了VocabTailor。该框架通过卸载嵌入层来解决内存限制，并为语言模型头（LM Head）实现混合静态-动态词汇选择策略，从而实现词汇组件的按需加载。", "result": "在多种下游任务上的综合实验表明，VocabTailor将词汇相关组件的内存使用量减少了高达99%，且任务性能退化极小或没有退化。其表现显著优于现有静态词汇剪枝方法。", "conclusion": "VocabTailor通过其新颖的解耦动态词汇选择框架，有效地解决了SLM在边缘设备上的内存限制问题，在显著节省内存的同时保持了出色的任务性能，超越了传统的静态剪枝方法。"}}
{"id": "2508.15231", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15231", "abs": "https://arxiv.org/abs/2508.15231", "authors": ["Shihao Dong", "Xiaotong Zhou", "Yuhui Zheng", "Huiying Xu", "Xinzhong Zhu"], "title": "Center-Oriented Prototype Contrastive Clustering", "comment": null, "summary": "Contrastive learning is widely used in clustering tasks due to its\ndiscriminative representation. However, the conflict problem between classes is\ndifficult to solve effectively. Existing methods try to solve this problem\nthrough prototype contrast, but there is a deviation between the calculation of\nhard prototypes and the true cluster center. To address this problem, we\npropose a center-oriented prototype contrastive clustering framework, which\nconsists of a soft prototype contrastive module and a dual consistency learning\nmodule. In short, the soft prototype contrastive module uses the probability\nthat the sample belongs to the cluster center as a weight to calculate the\nprototype of each category, while avoiding inter-class conflicts and reducing\nprototype drift. The dual consistency learning module aligns different\ntransformations of the same sample and the neighborhoods of different samples\nrespectively, ensuring that the features have transformation-invariant semantic\ninformation and compact intra-cluster distribution, while providing reliable\nguarantees for the calculation of prototypes. Extensive experiments on five\ndatasets show that the proposed method is effective compared to the SOTA. Our\ncode is published on https://github.com/LouisDong95/CPCC.", "AI": {"tldr": "本文提出了一种面向中心的原型对比聚类框架（CPCC），通过软原型对比模块和双重一致性学习模块，有效解决了对比学习聚类中类间冲突和原型漂移问题，并在多个数据集上取得了SOTA性能。", "motivation": "对比学习在聚类任务中因其判别性表示而被广泛应用，但存在类间冲突难以有效解决的问题。现有方法通过原型对比尝试解决，但硬原型计算与真实聚类中心之间存在偏差。", "method": "提出了一种面向中心的原型对比聚类框架（CPCC），包含：\n1. 软原型对比模块：使用样本属于聚类中心的概率作为权重计算各类别原型，以避免类间冲突并减少原型漂移。\n2. 双重一致性学习模块：分别对同一样本的不同变换和不同样本的邻域进行对齐，以确保特征具有变换不变的语义信息和紧凑的簇内分布，并为原型计算提供可靠保障。", "result": "在五个数据集上进行的广泛实验表明，所提出的方法与现有最先进（SOTA）方法相比是有效的。", "conclusion": "所提出的中心导向原型对比聚类框架（CPCC）通过其软原型对比和双重一致性学习模块，成功解决了对比聚类中类间冲突和原型计算偏差的问题，显著提升了聚类性能。"}}
{"id": "2508.15305", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15305", "abs": "https://arxiv.org/abs/2508.15305", "authors": ["Wei Yang", "Jinwei Xiao", "Hongming Zhang", "Qingyang Zhang", "Yanna Wang", "Bo Xu"], "title": "Coarse-to-Fine Grounded Memory for LLM Agent Planning", "comment": "Accepted to EMNLP 2025 Main Conference;27 pages,15 figures", "summary": "Recent advancements in Large Language Models (LLMs) have driven growing\ninterest in LLM-based agents for complex planning tasks. To avoid costly agent\ntraining, many studies adopted memory mechanism that enhances LLM with offline\nexperiences or online trajectory analysis. However, existing works focus on\nsingle-granularity memory derived from dynamic environmental interactions,\nwhich are inherently constrained by the quality of the collected experiences.\nThis limitation, in turn, constrain the diversity of knowledge and the\nflexibility of planning. We propose Coarse-to-Fine Grounded Memory (\\Ours{}), a\nnovel framework that grounds coarse-to-fine memories with LLM, thereby fully\nleverage them for flexible adaptation to diverse scenarios. \\Ours{} grounds\nenvironmental information into coarse-grained focus points to guide experience\ncollection in training tasks, followed by grounding of actionable\nhybrid-grained tips from each experience. At inference, \\Ours{} retrieves\ntask-relevant experiences and tips to support planning. When facing\nenvironmental anomalies, the LLM grounds the current situation into\nfine-grained key information, enabling flexible self-QA reflection and plan\ncorrection.", "AI": {"tldr": "本文提出了一种名为“粗到细接地记忆”（Coarse-to-Fine Grounded Memory, Ours）的新框架，通过结合粗粒度和细粒度记忆，增强大型语言模型（LLM）代理在复杂规划任务中的适应性和灵活性。", "motivation": "现有基于LLM代理的记忆机制通常是单一粒度的，且受限于收集经验的质量，导致知识多样性和规划灵活性的不足，无法有效应对复杂多变的任务场景。", "method": "所提出的Ours框架首先将环境信息接地为粗粒度焦点，以指导训练任务中的经验收集；然后从每个经验中接地可操作的混合粒度提示。在推理阶段，Ours检索任务相关的经验和提示来支持规划。当面对环境异常时，LLM将当前情况接地为细粒度关键信息，从而实现灵活的自我问答反射和计划修正。", "result": "通过这种粗到细的接地记忆机制，该框架能够充分利用不同粒度的信息，使LLM代理能够灵活适应各种场景，支持规划，并在遇到环境异常时进行灵活的自我反思和计划修正。", "conclusion": "粗到细接地记忆框架通过整合多粒度记忆，显著提升了LLM代理在复杂规划任务中的知识多样性、规划灵活性和对环境异常的适应能力。"}}
{"id": "2508.15239", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.15239", "abs": "https://arxiv.org/abs/2508.15239", "authors": ["Peerat Limkonchotiwat", "Pume Tuchinda", "Lalita Lowphansirikul", "Surapon Nonesung", "Panuthep Tasawong", "Alham Fikri Aji", "Can Udomcharoenchaikit", "Sarana Nutanong"], "title": "WangchanThaiInstruct: An instruction-following Dataset for Culture-Aware, Multitask, and Multi-domain Evaluation in Thai", "comment": "Accepted to EMNLP 2025 (Main). Model and Dataset:\n  https://huggingface.co/collections/airesearch/wangchan-thai-instruction-6835722a30b98e01598984fd", "summary": "Large language models excel at instruction-following in English, but their\nperformance in low-resource languages like Thai remains underexplored. Existing\nbenchmarks often rely on translations, missing cultural and domain-specific\nnuances needed for real-world use. We present WangchanThaiInstruct, a\nhuman-authored Thai dataset for evaluation and instruction tuning, covering\nfour professional domains and seven task types. Created through a multi-stage\nquality control process with annotators, domain experts, and AI researchers,\nWangchanThaiInstruct supports two studies: (1) a zero-shot evaluation showing\nperformance gaps on culturally and professionally specific tasks, and (2) an\ninstruction tuning study with ablations isolating the effect of native\nsupervision. Models fine-tuned on WangchanThaiInstruct outperform those using\ntranslated data in both in-domain and out-of-domain benchmarks. These findings\nunderscore the need for culturally and professionally grounded instruction data\nto improve LLM alignment in low-resource, linguistically diverse settings.", "AI": {"tldr": "该研究提出了WangchanThaiInstruct，一个人工编写的泰语指令数据集，用于评估和微调大型语言模型在泰语低资源环境中的表现。结果表明，使用原生指令数据微调的模型显著优于使用翻译数据的模型。", "motivation": "大型语言模型在英语指令遵循方面表现出色，但在泰语等低资源语言中的性能尚未充分探索。现有基准测试常依赖翻译，这可能遗漏了实际应用所需的文化和领域特定细微差别。", "method": "研究团队构建了WangchanThaiInstruct数据集，该数据集由人工编写，涵盖四个专业领域和七种任务类型，并经过多阶段质量控制（涉及标注员、领域专家和AI研究人员）。基于此数据集，进行了两项研究：(1) 零样本评估，以揭示文化和专业特定任务的性能差距；(2) 指令微调研究，通过消融实验隔离原生监督的效果。", "result": "零样本评估揭示了在文化和专业特定任务上的性能差距。使用WangchanThaiInstruct数据集进行微调的模型，无论是在领域内还是领域外基准测试中，都优于使用翻译数据进行微调的模型。", "conclusion": "研究结果强调了在低资源、语言多样化环境中，需要文化和专业背景扎实的指令数据来提高大型语言模型的对齐能力。"}}
{"id": "2508.15232", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15232", "abs": "https://arxiv.org/abs/2508.15232", "authors": ["Ruipu Wu", "Yige Zhang", "Jinyu Chen", "Linjiang Huang", "Shifeng Zhang", "Xu Zhou", "Liang Wang", "Si Liu"], "title": "AeroDuo: Aerial Duo for UAV-based Vision and Language Navigation", "comment": "Accepted by ACM MM 2025", "summary": "Aerial Vision-and-Language Navigation (VLN) is an emerging task that enables\nUnmanned Aerial Vehicles (UAVs) to navigate outdoor environments using natural\nlanguage instructions and visual cues. However, due to the extended\ntrajectories and complex maneuverability of UAVs, achieving reliable UAV-VLN\nperformance is challenging and often requires human intervention or overly\ndetailed instructions. To harness the advantages of UAVs' high mobility, which\ncould provide multi-grained perspectives, while maintaining a manageable motion\nspace for learning, we introduce a novel task called Dual-Altitude UAV\nCollaborative VLN (DuAl-VLN). In this task, two UAVs operate at distinct\naltitudes: a high-altitude UAV responsible for broad environmental reasoning,\nand a low-altitude UAV tasked with precise navigation. To support the training\nand evaluation of the DuAl-VLN, we construct the HaL-13k, a dataset comprising\n13,838 collaborative high-low UAV demonstration trajectories, each paired with\ntarget-oriented language instructions. This dataset includes both unseen maps\nand an unseen object validation set to systematically evaluate the model's\ngeneralization capabilities across novel environments and unfamiliar targets.\nTo consolidate their complementary strengths, we propose a dual-UAV\ncollaborative VLN framework, AeroDuo, where the high-altitude UAV integrates a\nmultimodal large language model (Pilot-LLM) for target reasoning, while the\nlow-altitude UAV employs a lightweight multi-stage policy for navigation and\ntarget grounding. The two UAVs work collaboratively and only exchange minimal\ncoordinate information to ensure efficiency.", "AI": {"tldr": "本文提出了一种双高度无人机协同视觉语言导航（DuAl-VLN）新任务，旨在通过高空无人机进行宏观推理和低空无人机进行精确导航来提高空中VLN的可靠性。为此，作者构建了HaL-13k数据集，并提出了AeroDuo框架。", "motivation": "现有的空中视觉语言导航（VLN）任务由于无人机轨迹长、机动性复杂，难以实现可靠性能，常需人工干预或过于详细的指令。尽管无人机具有高机动性可提供多粒度视角，但仍需在可控的运动空间内进行学习。", "method": "本文引入了双高度无人机协同VLN（DuAl-VLN）任务，其中高空无人机负责广域环境推理，低空无人机负责精确导航。为支持该任务，作者构建了包含13,838条协同轨迹的HaL-13k数据集，并提出了AeroDuo框架：高空无人机集成多模态大语言模型（Pilot-LLM）进行目标推理，低空无人机采用轻量级多阶段策略进行导航和目标定位，两架无人机仅通过交换最少的坐标信息进行协同。", "result": "本文介绍了DuAl-VLN任务、HaL-13k数据集和AeroDuo框架。HaL-13k数据集包含用于评估模型在新环境和陌生目标上泛化能力的未见地图和未见目标验证集。AeroDuo框架通过高空无人机的多模态大语言模型进行目标推理和低空无人机的轻量级策略进行导航与目标定位，实现了双无人机的协同工作。", "conclusion": "本文通过引入双高度无人机协同VLN（DuAl-VLN）任务、构建HaL-13k数据集和提出AeroDuo框架，为解决空中VLN的挑战提供了一种新颖的解决方案，旨在通过双无人机协作实现更可靠、高效的空中导航。"}}
{"id": "2508.15327", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.15327", "abs": "https://arxiv.org/abs/2508.15327", "authors": ["Xiancheng Gao", "Yufeng Shi", "Wengang Zhou", "Houqiang Li"], "title": "Search-Based Credit Assignment for Offline Preference-Based Reinforcement Learning", "comment": "7 pages, 6 figures, under review", "summary": "Offline reinforcement learning refers to the process of learning policies\nfrom fixed datasets, without requiring additional environment interaction.\nHowever, it often relies on well-defined reward functions, which are difficult\nand expensive to design. Human feedback is an appealing alternative, but its\ntwo common forms, expert demonstrations and preferences, have complementary\nlimitations. Demonstrations provide stepwise supervision, but they are costly\nto collect and often reflect limited expert behavior modes. In contrast,\npreferences are easier to collect, but it is unclear which parts of a behavior\ncontribute most to a trajectory segment, leaving credit assignment unresolved.\nIn this paper, we introduce a Search-Based Preference Weighting (SPW) scheme to\nunify these two feedback sources. For each transition in a preference labeled\ntrajectory, SPW searches for the most similar state-action pairs from expert\ndemonstrations and directly derives stepwise importance weights based on their\nsimilarity scores. These weights are then used to guide standard preference\nlearning, enabling more accurate credit assignment that traditional approaches\nstruggle to achieve. We demonstrate that SPW enables effective joint learning\nfrom preferences and demonstrations, outperforming prior methods that leverage\nboth feedback types on challenging robot manipulation tasks.", "AI": {"tldr": "本文提出了一种名为搜索式偏好加权（SPW）的新方案，通过利用专家演示来为偏好学习中的轨迹转换分配重要性权重，从而更准确地解决信用分配问题，有效结合了两种人类反馈形式。", "motivation": "离线强化学习依赖于难以设计和昂贵的奖励函数。人类反馈（专家演示和偏好）是替代方案，但各有局限：演示收集成本高且行为模式有限；偏好易于收集但信用分配困难。本研究旨在统一这两种反馈源，克服各自的缺点。", "method": "本文引入了搜索式偏好加权（SPW）方案。对于偏好标记轨迹中的每个转换，SPW从专家演示中搜索最相似的状态-动作对，并根据相似度分数直接推导出逐步重要性权重。这些权重随后用于指导标准的偏好学习，以实现更准确的信用分配。", "result": "SPW能够有效地从偏好和演示中进行联合学习，在具有挑战性的机器人操纵任务上，其性能优于以往利用两种反馈类型的方法。", "conclusion": "SPW方案成功地统一了专家演示和人类偏好这两种反馈形式，通过改进信用分配问题，使得离线强化学习能够更有效地利用混合反馈数据，从而在复杂任务中取得更好的学习效果。"}}
{"id": "2508.15244", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.15244", "abs": "https://arxiv.org/abs/2508.15244", "authors": ["Sangmin Lee", "Woojin Chung", "Seyun Um", "Hong-Goo Kang"], "title": "UniCoM: A Universal Code-Switching Speech Generator", "comment": "Accepted to EMNLP 2025 Findings", "summary": "Code-switching (CS), the alternation between two or more languages within a\nsingle speaker's utterances, is common in real-world conversations and poses\nsignificant challenges for multilingual speech technology. However, systems\ncapable of handling this phenomenon remain underexplored, primarily due to the\nscarcity of suitable datasets. To resolve this issue, we propose Universal\nCode-Mixer (UniCoM), a novel pipeline for generating high-quality, natural CS\nsamples without altering sentence semantics. Our approach utilizes an algorithm\nwe call Substituting WORDs with Synonyms (SWORDS), which generates CS speech by\nreplacing selected words with their translations while considering their parts\nof speech. Using UniCoM, we construct Code-Switching FLEURS (CS-FLEURS), a\nmultilingual CS corpus designed for automatic speech recognition (ASR) and\nspeech-to-text translation (S2TT). Experimental results show that CS-FLEURS\nachieves high intelligibility and naturalness, performing comparably to\nexisting datasets on both objective and subjective metrics. We expect our\napproach to advance CS speech technology and enable more inclusive multilingual\nsystems.", "AI": {"tldr": "本文提出了一种名为UniCoM的新型流水线，用于生成高质量、自然的语码转换（CS）语音样本，以解决多语言语音技术中CS数据集稀缺的问题，并构建了CS-FLEURS语料库。", "motivation": "语码转换（CS）在真实对话中很常见，对多语言语音技术构成重大挑战。然而，处理这种现象的系统尚未得到充分探索，主要原因是缺乏合适的CS数据集。", "method": "研究人员提出了Universal Code-Mixer (UniCoM) 流水线，利用名为Substituting WORDs with Synonyms (SWORDS) 的算法。该算法通过在考虑词性（POS）的前提下，用翻译替换选定的词语来生成CS语音，同时不改变句子语义。利用UniCoM，他们构建了Code-Switching FLEURS (CS-FLEURS) 多语言CS语料库，专为自动语音识别（ASR）和语音到文本翻译（S2TT）设计。", "result": "实验结果表明，CS-FLEURS语料库在客观和主观指标上都实现了高可懂度和自然度，表现与现有数据集相当。", "conclusion": "该方法有望推动语码转换语音技术的发展，并实现更具包容性的多语言系统。"}}
{"id": "2508.15233", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.15233", "abs": "https://arxiv.org/abs/2508.15233", "authors": ["Wenju Xu"], "title": "Pretrained Diffusion Models Are Inherently Skipped-Step Samplers", "comment": null, "summary": "Diffusion models have been achieving state-of-the-art results across various\ngeneration tasks. However, a notable drawback is their sequential generation\nprocess, requiring long-sequence step-by-step generation. Existing methods,\nsuch as DDIM, attempt to reduce sampling steps by constructing a class of\nnon-Markovian diffusion processes that maintain the same training objective.\nHowever, there remains a gap in understanding whether the original diffusion\nprocess can achieve the same efficiency without resorting to non-Markovian\nprocesses. In this paper, we provide a confirmative answer and introduce\nskipped-step sampling, a mechanism that bypasses multiple intermediate\ndenoising steps in the iterative generation process, in contrast with the\ntraditional step-by-step refinement of standard diffusion inference. Crucially,\nwe demonstrate that this skipped-step sampling mechanism is derived from the\nsame training objective as the standard diffusion model, indicating that\naccelerated sampling via skipped-step sampling via a Markovian way is an\nintrinsic property of pretrained diffusion models. Additionally, we propose an\nenhanced generation method by integrating our accelerated sampling technique\nwith DDIM. Extensive experiments on popular pretrained diffusion models,\nincluding the OpenAI ADM, Stable Diffusion, and Open Sora models, show that our\nmethod achieves high-quality generation with significantly reduced sampling\nsteps.", "AI": {"tldr": "本文提出了一种名为“跳步采样”的新机制，显著减少了扩散模型的生成步骤，同时保持了高质量输出。该方法源自标准扩散模型的训练目标，表明加速采样是预训练扩散模型的内在特性，并且可以与现有加速方法（如DDIM）结合使用。", "motivation": "扩散模型在生成任务中表现卓越，但其顺序生成过程导致采样步骤冗长。现有方法（如DDIM）通过构建非马尔可夫扩散过程来减少步骤，但仍不清楚原始马尔可夫扩散过程是否能在不诉诸非马尔可夫过程的情况下实现相同的效率。", "method": "本文引入了“跳步采样”机制，通过跳过迭代生成过程中的多个中间去噪步骤，来加速采样。作者证明了这种机制源自标准扩散模型的相同训练目标，表明通过马尔可夫方式进行跳步采样加速是预训练扩散模型的内在属性。此外，该方法还与DDIM进行了集成，提出了增强的生成方法。", "result": "在OpenAI ADM、Stable Diffusion和Open Sora等流行的预训练扩散模型上的大量实验表明，所提出的方法在显著减少采样步骤的同时，实现了高质量的生成。", "conclusion": "本文证实了原始扩散过程无需诉诸非马尔可夫过程即可实现采样效率，并通过“跳步采样”机制证明了加速采样是预训练扩散模型的内在属性。该方法有效且可与现有加速技术结合，实现高质量的快速生成。"}}
{"id": "2508.15335", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15335", "abs": "https://arxiv.org/abs/2508.15335", "authors": ["Bin Deng", "Yizhe Feng", "Zeming Liu", "Qing Wei", "Xiangrong Zhu", "Shuai Chen", "Yuanfang Guo", "Yunhong Wang"], "title": "RETAIL: Towards Real-world Travel Planning for Large Language Models", "comment": null, "summary": "Although large language models have enhanced automated travel planning\nabilities, current systems remain misaligned with real-world scenarios. First,\nthey assume users provide explicit queries, while in reality requirements are\noften implicit. Second, existing solutions ignore diverse environmental factors\nand user preferences, limiting the feasibility of plans. Third, systems can\nonly generate plans with basic POI arrangements, failing to provide all-in-one\nplans with rich details. To mitigate these challenges, we construct a novel\ndataset \\textbf{RETAIL}, which supports decision-making for implicit queries\nwhile covering explicit queries, both with and without revision needs. It also\nenables environmental awareness to ensure plan feasibility under real-world\nscenarios, while incorporating detailed POI information for all-in-one travel\nplans. Furthermore, we propose a topic-guided multi-agent framework, termed\nTGMA. Our experiments reveal that even the strongest existing model achieves\nmerely a 1.0% pass rate, indicating real-world travel planning remains\nextremely challenging. In contrast, TGMA demonstrates substantially improved\nperformance 2.72%, offering promising directions for real-world travel\nplanning.", "AI": {"tldr": "本文提出新数据集RETAIL和多智能体框架TGMA，旨在解决大型语言模型在真实世界旅行规划中面临的隐式查询、环境因素和详细信息缺失等挑战，并显著提升了规划通过率。", "motivation": "现有旅行规划系统与真实世界场景不符，主要体现在：1) 假设用户提供明确查询，而实际需求常是隐式的；2) 忽略多样化的环境因素和用户偏好，限制了计划的可行性；3) 只能生成包含基本兴趣点（POI）的计划，缺乏丰富细节的一站式方案。", "method": "1) 构建了新数据集RETAIL，支持隐式和显式查询（含或不含修订需求），并纳入环境感知以确保计划在真实场景下的可行性，同时包含详细的POI信息以生成一站式旅行计划。2) 提出了一个主题引导的多智能体框架（TGMA）。", "result": "实验表明，即使是最强的现有模型，其通过率也仅为1.0%，这表明真实世界旅行规划极具挑战性。相比之下，TGMA框架的性能显著提升，通过率达到2.72%。", "conclusion": "真实世界旅行规划仍然非常具有挑战性，但所提出的TGMA框架展示出显著的性能提升，为未来真实世界旅行规划提供了有希望的方向。"}}
{"id": "2508.15250", "categories": ["cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.15250", "abs": "https://arxiv.org/abs/2508.15250", "authors": ["Yilin Jiang", "Mingzi Zhang", "Sheng Jin", "Zengyi Yu", "Xiangjie Kong", "Binghao Tu"], "title": "EMNLP: Educator-role Moral and Normative Large Language Models Profiling", "comment": "24pages, 12 figures, Accepted by EMNLP Main Confrence", "summary": "Simulating Professions (SP) enables Large Language Models (LLMs) to emulate\nprofessional roles. However, comprehensive psychological and ethical evaluation\nin these contexts remains lacking. This paper introduces EMNLP, an\nEducator-role Moral and Normative LLMs Profiling framework for personality\nprofiling, moral development stage measurement, and ethical risk under soft\nprompt injection. EMNLP extends existing scales and constructs 88\nteacher-specific moral dilemmas, enabling profession-oriented comparison with\nhuman teachers. A targeted soft prompt injection set evaluates compliance and\nvulnerability in teacher SP. Experiments on 12 LLMs show teacher-role LLMs\nexhibit more idealized and polarized personalities than human teachers, excel\nin abstract moral reasoning, but struggle with emotionally complex situations.\nModels with stronger reasoning are more vulnerable to harmful prompt injection,\nrevealing a paradox between capability and safety. The model temperature and\nother hyperparameters have limited influence except in some risk behaviors.\nThis paper presents the first benchmark to assess ethical and psychological\nalignment of teacher-role LLMs for educational AI. Resources are available at\nhttps://e-m-n-l-p.github.io/.", "AI": {"tldr": "本研究提出了EMNLP框架，用于评估模拟教师角色的LLM在人格、道德发展阶段和软提示注入下的伦理风险。发现LLM与人类教师存在差异，在抽象道德推理上表现出色但在情感复杂情境中表现不佳，且推理能力越强的模型越容易受到有害提示注入攻击。", "motivation": "尽管大型语言模型（LLMs）能够模拟专业角色（如教师），但目前缺乏对这些情境下LLMs进行全面的心理和伦理评估。", "method": "本研究引入了EMNLP（教师角色道德与规范LLMs画像）框架，用于人格画像、道德发展阶段测量以及软提示注入下的伦理风险评估。该框架扩展了现有量表，构建了88个教师特有的道德困境，并设计了目标软提示注入集。研究对12个LLMs进行了实验，并与人类教师进行了专业导向的比较。", "result": "实验结果显示，模拟教师角色的LLMs比人类教师展现出更理想化和两极化的人格，在抽象道德推理方面表现出色，但在处理情感复杂情境时遇到困难。具有更强推理能力的模型更容易受到有害提示注入的攻击，揭示了能力与安全之间的悖论。模型温度等超参数的影响有限，仅在某些风险行为中有所体现。", "conclusion": "本论文首次提出了一个基准，用于评估教育AI中教师角色LLMs的伦理和心理一致性。研究揭示了LLM能力与安全之间的矛盾，为未来教育AI的发展提供了重要见解。"}}
{"id": "2508.15243", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15243", "abs": "https://arxiv.org/abs/2508.15243", "authors": ["Yixin Gao", "Xin Li", "Xiaohan Pan", "Runsen Feng", "Bingchen Li", "Yunpeng Qi", "Yiting Lu", "Zhengxue Cheng", "Zhibo Chen", "Jörn Ostermann"], "title": "Comp-X: On Defining an Interactive Learned Image Compression Paradigm With Expert-driven LLM Agent", "comment": null, "summary": "We present Comp-X, the first intelligently interactive image compression\nparadigm empowered by the impressive reasoning capability of large language\nmodel (LLM) agent. Notably, commonly used image codecs usually suffer from\nlimited coding modes and rely on manual mode selection by engineers, making\nthem unfriendly for unprofessional users. To overcome this, we advance the\nevolution of image coding paradigm by introducing three key innovations: (i)\nmulti-functional coding framework, which unifies different coding modes of\nvarious objective/requirements, including human-machine perception, variable\ncoding, and spatial bit allocation, into one framework. (ii) interactive coding\nagent, where we propose an augmented in-context learning method with coding\nexpert feedback to teach the LLM agent how to understand the coding request,\nmode selection, and the use of the coding tools. (iii) IIC-bench, the first\ndedicated benchmark comprising diverse user requests and the corresponding\nannotations from coding experts, which is systematically designed for\nintelligently interactive image compression evaluation. Extensive experimental\nresults demonstrate that our proposed Comp-X can understand the coding requests\nefficiently and achieve impressive textual interaction capability. Meanwhile,\nit can maintain comparable compression performance even with a single coding\nframework, providing a promising avenue for artificial general intelligence\n(AGI) in image compression.", "AI": {"tldr": "Comp-X 是首个由大型语言模型（LLM）代理驱动的智能交互式图像压缩范式，旨在通过统一编码框架和交互式代理克服传统编码器的局限性。", "motivation": "常用的图像编码器编码模式有限，且依赖工程师手动选择，对非专业用户不友好。研究旨在通过引入智能交互来克服这一局限。", "method": "Comp-X 引入了三项创新：(i) 多功能编码框架，统一了不同目标（如人机感知、可变编码、空间比特分配）的编码模式；(ii) 交互式编码代理，采用增强的上下文学习方法并结合编码专家反馈来训练LLM代理理解编码请求和工具使用；(iii) IIC-bench，首个专用于智能交互式图像压缩评估的基准测试。", "result": "实验结果表明，Comp-X 能高效理解编码请求，展现出卓越的文本交互能力。同时，即使在单一编码框架下，它也能保持可观的压缩性能。", "conclusion": "Comp-X 为图像压缩领域的人工通用智能（AGI）提供了一个有前景的方向。"}}
{"id": "2508.15338", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.15338", "abs": "https://arxiv.org/abs/2508.15338", "authors": ["Jinning Yang", "Wen Shi"], "title": "DiagECG: An LLM-Driven Framework for Diagnostic Reasoning via Discretized ECG Tokenization", "comment": null, "summary": "Electrocardiography plays a central role in cardiovascular diagnostics, yet\nexisting automated approaches often struggle to generalize across clinical\ntasks and offer limited support for open-ended reasoning. We present DiagECG, a\nnovel framework that integrates time-series and language modeling by enabling\nlarge language models to process 12-lead ECG signals for clinical text\ngeneration tasks. Our approach discretizes continuous ECG embeddings into\nsymbolic tokens using a lead-independent encoder and quantization module. These\ntokens are then used to extend the vocabulary of LLM, allowing the model to\nhandle both ECG and natural language inputs in a unified manner. To bridge the\nmodality gap, we pretrain the model on an autoregressive ECG forecasting task,\nenabling the LLM to model temporal dynamics using its native language modeling\ncapabilities. Finally, we perform instruction tuning on both ECG question\nanswering and diagnostic report generation. Without modifying the core model,\nDiagECG achieves strong performance across tasks while maintaining\ngeneralization to out-of-distribution settings. Extensive experiments\ndemonstrate the effectiveness of each component and highlight the potential of\nintegrating symbolic ECG representations into LLMs for medical reasoning.", "AI": {"tldr": "DiagECG是一个新颖的框架，通过将ECG信号离散化为符号token并扩展大型语言模型（LLM）的词汇表，使LLM能够处理12导联ECG信号并生成临床文本，从而在心血管诊断中实现开放式推理和跨任务泛化。", "motivation": "现有的自动化心电图分析方法在跨临床任务泛化方面表现不佳，并且对开放式推理的支持有限。", "method": "该方法通过以下步骤实现：1) 使用与导联无关的编码器和量化模块将连续的ECG嵌入离散化为符号token；2) 将这些token用于扩展LLM的词汇表，使其能统一处理ECG和自然语言输入；3) 在自回归ECG预测任务上预训练模型以弥合模态差距，使LLM能利用其原生的语言建模能力对时间动态进行建模；4) 最后，在ECG问答和诊断报告生成任务上进行指令微调。", "result": "DiagECG在不修改核心模型的情况下，在各种任务中取得了强大的性能，并保持了对分布外设置的泛化能力。广泛的实验证明了每个组件的有效性。", "conclusion": "该研究强调了将符号ECG表示集成到LLM中用于医学推理的巨大潜力，并展示了DiageCG在心血管诊断中的有效性。"}}
{"id": "2508.15253", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15253", "abs": "https://arxiv.org/abs/2508.15253", "authors": ["Eunseong Choi", "June Park", "Hyeri Lee", "Jongwuk Lee"], "title": "Conflict-Aware Soft Prompting for Retrieval-Augmented Generation", "comment": "Accepted to EMNLP 2025; 14 pages; 5 figures, 11 tables", "summary": "Retrieval-augmented generation (RAG) enhances the capabilities of large\nlanguage models (LLMs) by incorporating external knowledge into their input\nprompts. However, when the retrieved context contradicts the LLM's parametric\nknowledge, it often fails to resolve the conflict between incorrect external\ncontext and correct parametric knowledge, known as context-memory conflict. To\ntackle this problem, we introduce Conflict-Aware REtrieval-Augmented Generation\n(CARE), consisting of a context assessor and a base LLM. The context assessor\nencodes compact memory token embeddings from raw context tokens. Through\ngrounded/adversarial soft prompting, the context assessor is trained to discern\nunreliable context and capture a guidance signal that directs reasoning toward\nthe more reliable knowledge source. Extensive experiments show that CARE\neffectively mitigates context-memory conflicts, leading to an average\nperformance gain of 5.0\\% on QA and fact-checking benchmarks, establishing a\npromising direction for trustworthy and adaptive RAG systems.", "AI": {"tldr": "当检索到的上下文与大语言模型的参数知识冲突时，RAG系统会遇到问题。本文提出了CARE（Conflict-Aware REtrieval-Augmented Generation），通过一个上下文评估器来识别不可靠的上下文，从而有效缓解这种冲突。", "motivation": "RAG系统在检索到的外部上下文与大语言模型的内部参数知识发生矛盾时（即上下文-记忆冲突），常常无法解决这种冲突，导致其性能下降。因此，需要一种机制来处理这种知识来源之间的不一致性。", "method": "本文引入了CARE系统，它由一个上下文评估器和一个基础大语言模型组成。上下文评估器负责将原始上下文令牌编码为紧凑的记忆令牌嵌入。通过接地/对抗性软提示（grounded/adversarial soft prompting）进行训练，上下文评估器能够辨别不可靠的上下文，并生成一个指导信号，将推理导向更可靠的知识来源。", "result": "实验结果表明，CARE系统有效缓解了上下文-记忆冲突，在问答和事实核查基准测试中平均性能提升了5.0%。", "conclusion": "CARE系统为构建值得信赖和自适应的RAG系统提供了一个有前景的方向，通过处理上下文-记忆冲突，提高了系统的鲁棒性和准确性。"}}
{"id": "2508.15256", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15256", "abs": "https://arxiv.org/abs/2508.15256", "authors": ["Jinsol Song", "Jiamu Wang", "Anh Tien Nguyen", "Keunho Byeon", "Sangjeong Ahn", "Sung Hak Lee", "Jin Tae Kwak"], "title": "Normal and Abnormal Pathology Knowledge-Augmented Vision-Language Model for Anomaly Detection in Pathology Images", "comment": "Accepted at ICCV 2025. \\c{opyright} IEEE 2025. This is the author's\n  accepted version (camera-ready) of the paper. The definitive version is\n  published in the Proceedings of the IEEE/CVF International Conference on\n  Computer Vision (ICCV 2025). DOI will be updated when available", "summary": "Anomaly detection in computational pathology aims to identify rare and scarce\nanomalies where disease-related data are often limited or missing. Existing\nanomaly detection methods, primarily designed for industrial settings, face\nlimitations in pathology due to computational constraints, diverse tissue\nstructures, and lack of interpretability. To address these challenges, we\npropose Ano-NAViLa, a Normal and Abnormal pathology knowledge-augmented\nVision-Language model for Anomaly detection in pathology images. Ano-NAViLa is\nbuilt on a pre-trained vision-language model with a lightweight trainable MLP.\nBy incorporating both normal and abnormal pathology knowledge, Ano-NAViLa\nenhances accuracy and robustness to variability in pathology images and\nprovides interpretability through image-text associations. Evaluated on two\nlymph node datasets from different organs, Ano-NAViLa achieves the\nstate-of-the-art performance in anomaly detection and localization,\noutperforming competing models.", "AI": {"tldr": "Ano-NAViLa是一种结合正常和异常病理知识的视觉-语言模型，用于病理图像异常检测，实现了最先进的性能，并提供可解释性。", "motivation": "计算病理学中的异常检测面临挑战，因为疾病相关数据有限或缺失，现有方法（主要为工业设计）在病理学中存在计算限制、组织结构多样性和缺乏可解释性等局限。", "method": "本文提出了Ano-NAViLa模型，它是一个“正常和异常病理知识增强型视觉-语言模型”。该模型基于预训练的视觉-语言模型，并结合一个轻量级可训练的MLP。通过整合正常和异常病理知识，Ano-NAViLa增强了模型在病理图像变异性方面的准确性和鲁棒性，并通过图像-文本关联提供可解释性。", "result": "Ano-NAViLa在两个来自不同器官的淋巴结数据集上进行了评估，在异常检测和定位方面均达到了最先进的性能，超越了现有竞争模型。", "conclusion": "Ano-NAViLa通过结合正常和异常病理知识，显著提高了病理图像异常检测的准确性和鲁棒性，并提供了可解释性，从而实现了该领域的最新技术水平。"}}
{"id": "2508.15358", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15358", "abs": "https://arxiv.org/abs/2508.15358", "authors": ["Alberto Pozanco", "Marianela Morales", "Daniel Borrajo", "Manuela Veloso"], "title": "Planning with Minimal Disruption", "comment": null, "summary": "In many planning applications, we might be interested in finding plans that\nminimally modify the initial state to achieve the goals. We refer to this\nconcept as plan disruption. In this paper, we formally introduce it, and define\nvarious planning-based compilations that aim to jointly optimize both the sum\nof action costs and plan disruption. Experimental results in different\nbenchmarks show that the reformulated task can be effectively solved in\npractice to generate plans that balance both objectives.", "AI": {"tldr": "本文正式引入了“计划中断”概念，即最小化初始状态修改以达成目标，并提出了基于规划的编译方法，旨在同时优化行动成本和计划中断。实验结果表明该方法能有效生成平衡两目标的计划。", "motivation": "在许多规划应用中，人们希望找到那些能以最小程度修改初始状态来达成目标的计划。作者将此概念定义为“计划中断”，并希望解决如何实现这种优化。", "method": "本文正式定义了“计划中断”概念，并提出了多种基于规划的编译方法，旨在联合优化行动成本之和与计划中断。", "result": "在不同基准测试中的实验结果表明，重新制定的任务可以在实践中有效解决，从而生成能够平衡行动成本和计划中断这两个目标的计划。", "conclusion": "所提出的方法能够有效地在实践中解决联合优化行动成本和计划中断的问题，从而生成同时兼顾这两个目标的实用计划。"}}
{"id": "2508.15274", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.15274", "abs": "https://arxiv.org/abs/2508.15274", "authors": ["Lekshmi R Nair", "Arun Sankar", "Koninika Pal"], "title": "TComQA: Extracting Temporal Commonsense from Text", "comment": null, "summary": "Understanding events necessitates grasping their temporal context, which is\noften not explicitly stated in natural language. For example, it is not a\ntrivial task for a machine to infer that a museum tour may last for a few\nhours, but can not take months. Recent studies indicate that even advanced\nlarge language models (LLMs) struggle in generating text that require reasoning\nwith temporal commonsense due to its infrequent explicit mention in text.\nTherefore, automatically mining temporal commonsense for events enables the\ncreation of robust language models. In this work, we investigate the capacity\nof LLMs to extract temporal commonsense from text and evaluate multiple\nexperimental setups to assess their effectiveness. Here, we propose a temporal\ncommonsense extraction pipeline that leverages LLMs to automatically mine\ntemporal commonsense and use it to construct TComQA, a dataset derived from\nSAMSum and RealNews corpora. TComQA has been validated through crowdsourcing\nand achieves over 80\\% precision in extracting temporal commonsense. The model\ntrained with TComQA also outperforms an LLM fine-tuned on existing dataset of\ntemporal question answering task.", "AI": {"tldr": "本文研究了大型语言模型（LLMs）从文本中提取时间常识的能力，并提出了一个利用LLMs自动挖掘时间常识的管道，构建了新的数据集TComQA，该数据集在时间常识提取方面表现出高精度，并能有效提升模型在时间问答任务上的性能。", "motivation": "理解事件需要掌握其时间上下文，而这些上下文在自然语言中通常不明确提及。当前大型语言模型（LLMs）在处理需要时间常识推理的任务时表现不佳，因为时间常识在文本中很少被明确提及。因此，自动挖掘事件的时间常识对于构建更强大的语言模型至关重要。", "method": "本文调查了LLMs从文本中提取时间常识的能力，并评估了多种实验设置。提出了一种时间常识提取管道，该管道利用LLMs自动挖掘时间常识，并用其构建了TComQA数据集，该数据集来源于SAMSum和RealNews语料库，并通过众包进行了验证。", "result": "TComQA数据集在提取时间常识方面的精度超过80%。使用TComQA训练的模型在时间问答任务上，优于在现有时间问答数据集上进行微调的LLM。", "conclusion": "LLMs能够有效地从文本中提取时间常识。通过所提出的提取管道和构建的TComQA数据集，可以成功地挖掘高精度的事件时间常识，并显著提升模型在时间推理任务上的表现，从而为构建更鲁棒的语言模型提供了支持。"}}
{"id": "2508.15272", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15272", "abs": "https://arxiv.org/abs/2508.15272", "authors": ["Han Li", "Shaofei Huang", "Longfei Xu", "Yulu Gao", "Beipeng Mu", "Si Liu"], "title": "RATopo: Improving Lane Topology Reasoning via Redundancy Assignment", "comment": "Accepted by ACM MM 2025", "summary": "Lane topology reasoning plays a critical role in autonomous driving by\nmodeling the connections among lanes and the topological relationships between\nlanes and traffic elements. Most existing methods adopt a\nfirst-detect-then-reason paradigm, where topological relationships are\nsupervised based on the one-to-one assignment results obtained during the\ndetection stage. This supervision strategy results in suboptimal topology\nreasoning performance due to the limited range of valid supervision. In this\npaper, we propose RATopo, a Redundancy Assignment strategy for lane Topology\nreasoning that enables quantity-rich and geometry-diverse topology supervision.\nSpecifically, we restructure the Transformer decoder by swapping the\ncross-attention and self-attention layers. This allows redundant lane\npredictions to be retained before suppression, enabling effective one-to-many\nassignment. We also instantiate multiple parallel cross-attention blocks with\nindependent parameters, which further enhances the diversity of detected lanes.\nExtensive experiments on OpenLane-V2 demonstrate that our RATopo strategy is\nmodel-agnostic and can be seamlessly integrated into existing topology\nreasoning frameworks, consistently improving both lane-lane and lane-traffic\ntopology performance.", "AI": {"tldr": "RATopo提出了一种冗余分配策略，通过重构Transformer解码器并启用一对多分配，为自动驾驶中的车道拓扑推理提供丰富且多样化的监督，显著提升了拓扑推理性能。", "motivation": "现有车道拓扑推理方法多采用“先检测后推理”范式，其拓扑关系监督依赖于检测阶段的一对一分配结果。这种监督策略因有效监督范围有限，导致拓扑推理性能不佳。", "method": "本文提出了RATopo（Redundancy Assignment strategy for lane Topology reasoning）。具体方法包括：1) 重构Transformer解码器，交换交叉注意力层和自注意力层，从而在抑制前保留冗余车道预测，实现有效的一对多分配。2) 实例化多个具有独立参数的并行交叉注意力块，进一步增强检测到车道的多样性。", "result": "在OpenLane-V2上的大量实验表明，RATopo策略是模型无关的，可以无缝集成到现有拓扑推理框架中，并持续改进车道-车道和车道-交通元素的拓扑推理性能。", "conclusion": "RATopo通过其冗余分配策略，解决了现有拓扑推理监督不足的问题，通过启用数量丰富和几何多样化的拓扑监督，显著提升了自动驾驶中车道拓扑推理的性能。"}}
{"id": "2508.15432", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.15432", "abs": "https://arxiv.org/abs/2508.15432", "authors": ["Bidyapati Pradhan", "Surajit Dasgupta", "Amit Kumar Saha", "Omkar Anustoop", "Sriram Puttagunta", "Vipul Mittal", "Gopal Sarda"], "title": "GraSP: A Unified Graph-Based Framework for Scalable Generation, Quality Tagging, and Management of Synthetic Data for SFT and DPO", "comment": null, "summary": "The advancement of large language models (LLMs) is critically dependent on\nthe availability of high-quality datasets for Supervised Fine-Tuning (SFT),\nalignment tasks like Direct Preference Optimization (DPO), etc. In this work,\nwe present a comprehensive synthetic data generation framework that facilitates\nscalable, configurable, and high-fidelity generation of synthetic data tailored\nfor these training paradigms. Our approach employs a modular and\nconfiguration-based pipeline capable of modeling complex dialogue flows with\nminimal manual intervention. This framework uses a dual-stage quality tagging\nmechanism, combining heuristic rules and LLM-based evaluations, to\nautomatically filter and score data extracted from OASST-formatted\nconversations, ensuring the curation of high-quality dialogue samples. The\nresulting datasets are structured under a flexible schema supporting both SFT\nand DPO use cases, enabling seamless integration into diverse training\nworkflows. Together, these innovations offer a robust solution for generating\nand managing synthetic conversational data at scale, significantly reducing the\noverhead of data preparation in LLM training pipelines.", "AI": {"tldr": "本文提出一个全面的合成数据生成框架，旨在为大型语言模型（LLMs）的监督微调（SFT）和对齐任务（如DPO）提供可扩展、可配置且高保真的高质量合成对话数据。", "motivation": "大型语言模型的进步严重依赖于高质量数据集的可用性，但数据准备工作通常开销巨大。因此，需要一种更高效、可扩展的方式来生成和管理用于SFT和DPO等任务的训练数据。", "method": "该框架采用模块化、基于配置的流水线，能够以最少的人工干预模拟复杂的对话流程。它使用双阶段质量标记机制，结合启发式规则和基于LLM的评估，自动过滤和评分从OASST格式对话中提取的数据，以确保生成高质量的对话样本。生成的数据集采用灵活的结构，支持SFT和DPO两种使用场景。", "result": "该创新提供了一种强大的解决方案，能够大规模生成和管理合成对话数据，显著降低了LLM训练流水线中数据准备的开销，从而实现了可扩展、可配置和高保真的数据生成。", "conclusion": "该框架为大规模生成和管理合成对话数据提供了一个鲁棒的解决方案，极大地减少了LLM训练中数据准备的负担，支持多样化的训练工作流程。"}}
{"id": "2508.15316", "categories": ["cs.CL", "cs.LG", "eess.AS", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.15316", "abs": "https://arxiv.org/abs/2508.15316", "authors": ["Abdul Rehman", "Jian-Jun Zhang", "Xiaosong Yang"], "title": "CUPE: Contextless Universal Phoneme Encoder for Language-Agnostic Speech Processing", "comment": "Accepted in: 8th International Conference on Natural Language and\n  Speech Processing (ICNLSP 2025)", "summary": "Universal phoneme recognition typically requires analyzing long speech\nsegments and language-specific patterns. Many speech processing tasks require\npure phoneme representations free from contextual influence, which motivated\nour development of CUPE - a lightweight model that captures key phoneme\nfeatures in just 120 milliseconds, about one phoneme's length. CUPE processes\nshort, fixed-width windows independently and, despite fewer parameters than\ncurrent approaches, achieves competitive cross-lingual performance by learning\nfundamental acoustic patterns common to all languages. Our extensive evaluation\nthrough supervised and self-supervised training on diverse languages, including\nzero-shot tests on the UCLA Phonetic Corpus, demonstrates strong cross-lingual\ngeneralization and reveals that effective universal speech processing is\npossible through modeling basic acoustic patterns within phoneme-length\nwindows.", "AI": {"tldr": "CUPE是一种轻量级模型，能在120毫秒内捕捉关键音素特征，通过学习基本声学模式实现具有竞争力的跨语言音素识别，摆脱了上下文和语言特异性依赖。", "motivation": "许多语音处理任务需要不受上下文影响的纯音素表示，而传统的通用音素识别方法需要分析长语音片段和语言特定模式，这促使研究者开发一个能快速提供纯音素表示的模型。", "method": "CUPE模型采用轻量级设计，独立处理短的、固定宽度的（约120毫秒）语音窗口。它通过学习所有语言通用的基本声学模式，参数量少于现有方法，从而实现跨语言性能。", "result": "CUPE在监督式和自监督式训练下，在多种语言上展现出强大的跨语言泛化能力，包括在UCLA语音语料库上的零样本测试。它实现了具有竞争力的跨语言性能，证明通过在音素长度窗口内建模基本声学模式，可以实现有效的通用语音处理。", "conclusion": "通过在音素长度窗口内建模基本的声学模式，可以实现有效的通用语音处理，从而获得不受上下文影响的纯音素表示。"}}
{"id": "2508.15297", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15297", "abs": "https://arxiv.org/abs/2508.15297", "authors": ["Zhu Wang", "Homaira Huda Shomee", "Sathya N. Ravi", "Sourav Medya"], "title": "DesignCLIP: Multimodal Learning with CLIP for Design Patent Understanding", "comment": "Accepted by EMNLP 2025. 22 pages, 14 figures", "summary": "In the field of design patent analysis, traditional tasks such as patent\nclassification and patent image retrieval heavily depend on the image data.\nHowever, patent images -- typically consisting of sketches with abstract and\nstructural elements of an invention -- often fall short in conveying\ncomprehensive visual context and semantic information. This inadequacy can lead\nto ambiguities in evaluation during prior art searches. Recent advancements in\nvision-language models, such as CLIP, offer promising opportunities for more\nreliable and accurate AI-driven patent analysis. In this work, we leverage CLIP\nmodels to develop a unified framework DesignCLIP for design patent applications\nwith a large-scale dataset of U.S. design patents. To address the unique\ncharacteristics of patent data, DesignCLIP incorporates class-aware\nclassification and contrastive learning, utilizing generated detailed captions\nfor patent images and multi-views image learning. We validate the effectiveness\nof DesignCLIP across various downstream tasks, including patent classification\nand patent retrieval. Additionally, we explore multimodal patent retrieval,\nwhich provides the potential to enhance creativity and innovation in design by\noffering more diverse sources of inspiration. Our experiments show that\nDesignCLIP consistently outperforms baseline and SOTA models in the patent\ndomain on all tasks. Our findings underscore the promise of multimodal\napproaches in advancing patent analysis. The codebase is available here:\nhttps://anonymous.4open.science/r/PATENTCLIP-4661/README.md.", "AI": {"tldr": "DesignCLIP是一个统一框架，利用CLIP模型、类感知分类和对比学习，通过详细图注和多视图学习，解决了设计专利图像信息不足的问题，并在分类和检索任务中超越现有模型，推动了多模态专利分析。", "motivation": "传统设计专利分析（如分类和图像检索）严重依赖图像数据，但专利图像（通常是抽象草图）缺乏全面的视觉上下文和语义信息，导致现有技术检索中的评估模糊性。最近的视觉-语言模型（如CLIP）为更可靠、准确的AI驱动专利分析提供了机会。", "method": "本文开发了一个统一框架DesignCLIP，利用CLIP模型处理大规模美国设计专利数据集。为应对专利数据的独特性，DesignCLIP整合了类感知分类和对比学习，并利用生成的详细专利图像描述和多视图图像学习。", "result": "DesignCLIP在专利分类和专利检索等下游任务中表现出有效性，始终优于专利领域的基线模型和最先进模型。此外，研究还探索了多模态专利检索，这有可能通过提供更多样化的灵感来源来增强设计的创造力和创新。", "conclusion": "DesignCLIP的实验结果强调了多模态方法在推进专利分析方面的巨大前景，证明了其在解决专利图像信息不足问题上的优越性。"}}
{"id": "2508.15447", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15447", "abs": "https://arxiv.org/abs/2508.15447", "authors": ["Zihao Wang", "Junming Zhang"], "title": "From Bits to Boardrooms: A Cutting-Edge Multi-Agent LLM Framework for Business Excellence", "comment": "Accepted by ECAI 2025", "summary": "Large Language Models (LLMs) have shown promising potential in business\napplications, particularly in enterprise decision support and strategic\nplanning, yet current approaches often struggle to reconcile intricate\noperational analyses with overarching strategic goals across diverse market\nenvironments, leading to fragmented workflows and reduced collaboration across\norganizational levels. This paper introduces BusiAgent, a novel multi-agent\nframework leveraging LLMs for advanced decision-making in complex corporate\nenvironments. BusiAgent integrates three core innovations: an extended\nContinuous Time Markov Decision Process (CTMDP) for dynamic agent modeling, a\ngeneralized entropy measure to optimize collaborative efficiency, and a\nmulti-level Stackelberg game to handle hierarchical decision processes.\nAdditionally, contextual Thompson sampling is employed for prompt optimization,\nsupported by a comprehensive quality assurance system to mitigate errors.\nExtensive empirical evaluations across diverse business scenarios validate\nBusiAgent's efficacy, demonstrating its capacity to generate coherent,\nclient-focused solutions that smoothly integrate granular insights with\nhigh-level strategy, significantly outperforming established approaches in both\nsolution quality and user satisfaction. By fusing cutting-edge AI technologies\nwith deep business insights, BusiAgent marks a substantial step forward in\nAI-driven enterprise decision-making, empowering organizations to navigate\ncomplex business landscapes more effectively.", "AI": {"tldr": "BusiAgent是一个新颖的多智能体框架，利用大型语言模型（LLMs）解决企业决策支持和战略规划中操作分析与战略目标难以协调的问题，通过整合多项创新技术，显著提升了决策质量和用户满意度。", "motivation": "当前的大型语言模型（LLMs）在企业决策支持和战略规划中，难以将复杂的运营分析与宏观战略目标在多样化市场环境中进行整合，导致工作流程碎片化和跨组织协作效率低下。", "method": "BusiAgent框架整合了三项核心创新：扩展的连续时间马尔可夫决策过程（CTMDP）用于动态智能体建模，广义熵度量用于优化协作效率，以及多层Stackelberg博弈用于处理分层决策过程。此外，还采用了上下文Thompson采样进行提示优化，并辅以全面的质量保证系统以减少错误。", "result": "通过对多样化业务场景的广泛实证评估，BusiAgent能够生成连贯的、以客户为中心的解决方案，无缝整合细粒度洞察与高层战略，在解决方案质量和用户满意度方面显著优于现有方法。", "conclusion": "BusiAgent将尖端AI技术与深度业务洞察相结合，在AI驱动的企业决策中迈出了实质性的一步，使组织能够更有效地应对复杂的商业环境。"}}
{"id": "2508.15357", "categories": ["cs.CL", "cs.PF"], "pdf": "https://arxiv.org/pdf/2508.15357", "abs": "https://arxiv.org/abs/2508.15357", "authors": ["Haji Gul", "Abul Ghani Naim", "Ajaz Ahmad Bhat"], "title": "KG-EDAS: A Meta-Metric Framework for Evaluating Knowledge Graph Completion Models", "comment": null, "summary": "Knowledge Graphs (KGs) enable applications in various domains such as\nsemantic search, recommendation systems, and natural language processing. KGs\nare often incomplete, missing entities and relations, an issue addressed by\nKnowledge Graph Completion (KGC) methods that predict missing elements.\nDifferent evaluation metrics, such as Mean Reciprocal Rank (MRR), Mean Rank\n(MR), and Hit@k, are commonly used to assess the performance of such KGC\nmodels. A major challenge in evaluating KGC models, however, lies in comparing\ntheir performance across multiple datasets and metrics. A model may outperform\nothers on one dataset but underperform on another, making it difficult to\ndetermine overall superiority. Moreover, even within a single dataset,\ndifferent metrics such as MRR and Hit@1 can yield conflicting rankings, where\none model excels in MRR while another performs better in Hit@1, further\ncomplicating model selection for downstream tasks. These inconsistencies hinder\nholistic comparisons and highlight the need for a unified meta-metric that\nintegrates performance across all metrics and datasets to enable a more\nreliable and interpretable evaluation framework. To address this need, we\npropose KG Evaluation based on Distance from Average Solution (EDAS), a robust\nand interpretable meta-metric that synthesizes model performance across\nmultiple datasets and diverse evaluation criteria into a single normalized\nscore ($M_i \\in [0,1]$). Unlike traditional metrics that focus on isolated\naspects of performance, EDAS offers a global perspective that supports more\ninformed model selection and promotes fairness in cross-dataset evaluation.\nExperimental results on benchmark datasets such as FB15k-237 and WN18RR\ndemonstrate that EDAS effectively integrates multi-metric, multi-dataset\nperformance into a unified ranking, offering a consistent, robust, and\ngeneralizable framework for evaluating KGC models.", "AI": {"tldr": "该论文提出了一种名为EDAS（基于与平均解距离的知识图谱评估）的元度量标准，用于统一评估跨多个数据集和评估指标的知识图谱补全（KGC）模型性能。", "motivation": "知识图谱（KG）补全模型在评估时面临挑战：不同模型在不同数据集上表现不一，甚至在同一数据集上，不同评估指标（如MRR和Hit@1）也会给出冲突的排名，这阻碍了模型的整体比较和选择。因此，需要一个统一的元度量标准来整合跨所有指标和数据集的性能。", "method": "本文提出了一种名为EDAS（基于与平均解距离的知识图谱评估）的元度量标准。EDAS是一种稳健且可解释的方法，它将模型在多个数据集和多样化评估标准下的性能综合成一个单一的标准化分数（Mi ∈ [0,1]）。", "result": "在FB15k-237和WN18RR等基准数据集上的实验结果表明，EDAS有效地将多指标、多数据集的性能整合为一个统一的排名，提供了一个一致、稳健和可推广的KGC模型评估框架。", "conclusion": "EDAS提供了一个全局视角，支持更明智的模型选择，促进了跨数据集评估的公平性，并为KGC模型提供了一个可靠且可解释的评估框架。"}}
{"id": "2508.15298", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15298", "abs": "https://arxiv.org/abs/2508.15298", "authors": ["Darya Taratynova", "Alya Almsouti", "Beknur Kalmakhanbet", "Numan Saeed", "Mohammad Yaqub"], "title": "TPA: Temporal Prompt Alignment for Fetal Congenital Heart Defect Classification", "comment": null, "summary": "Congenital heart defect (CHD) detection in ultrasound videos is hindered by\nimage noise and probe positioning variability. While automated methods can\nreduce operator dependence, current machine learning approaches often neglect\ntemporal information, limit themselves to binary classification, and do not\naccount for prediction calibration. We propose Temporal Prompt Alignment (TPA),\na method leveraging foundation image-text model and prompt-aware contrastive\nlearning to classify fetal CHD on cardiac ultrasound videos. TPA extracts\nfeatures from each frame of video subclips using an image encoder, aggregates\nthem with a trainable temporal extractor to capture heart motion, and aligns\nthe video representation with class-specific text prompts via a margin-hinge\ncontrastive loss. To enhance calibration for clinical reliability, we introduce\na Conditional Variational Autoencoder Style Modulation (CVAESM) module, which\nlearns a latent style vector to modulate embeddings and quantifies\nclassification uncertainty. Evaluated on a private dataset for CHD detection\nand on a large public dataset, EchoNet-Dynamic, for systolic dysfunction, TPA\nachieves state-of-the-art macro F1 scores of 85.40% for CHD diagnosis, while\nalso reducing expected calibration error by 5.38% and adaptive ECE by 6.8%. On\nEchoNet-Dynamic's three-class task, it boosts macro F1 by 4.73% (from 53.89% to\n58.62%). Temporal Prompt Alignment (TPA) is a framework for fetal congenital\nheart defect (CHD) classification in ultrasound videos that integrates temporal\nmodeling, prompt-aware contrastive learning, and uncertainty quantification.", "AI": {"tldr": "本文提出了一种名为TPA（Temporal Prompt Alignment）的新方法，用于在超声视频中分类胎儿先天性心脏缺陷（CHD），该方法结合了时间建模、提示感知对比学习和不确定性量化。", "motivation": "超声视频中的CHD检测受到图像噪声和探头定位变异性的阻碍。现有自动化方法常忽略时间信息、局限于二分类，且不考虑预测校准，这限制了其临床可靠性。", "method": "TPA方法利用基础图像-文本模型和提示感知对比学习。它从视频子剪辑的每一帧中提取特征，通过可训练的时间提取器聚合以捕捉心脏运动，并使用带裕度的铰链对比损失将视频表示与类别特定文本提示对齐。为增强临床可靠性，引入了条件变分自编码器风格调制（CVAESM）模块，学习潜在风格向量来调制嵌入并量化分类不确定性。", "result": "TPA在CHD检测的私有数据集上实现了85.40%的宏F1分数，同时将预期校准误差（ECE）降低了5.38%，自适应ECE降低了6.8%。在EchoNet-Dynamic数据集的三分类任务中，宏F1分数从53.89%提升至58.62%（提高了4.73%），均达到最先进水平。", "conclusion": "Temporal Prompt Alignment（TPA）是一个用于胎儿先天性心脏缺陷（CHD）超声视频分类的综合框架，它有效地整合了时间建模、提示感知对比学习和不确定性量化，显著提高了诊断性能和临床可靠性。"}}
{"id": "2508.15507", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.15507", "abs": "https://arxiv.org/abs/2508.15507", "authors": ["Yekun Zhu", "Guang Chen", "Chengjun Mao"], "title": "Think in Blocks: Adaptive Reasoning from Direct Response to Deep Reasoning", "comment": null, "summary": "Large Language Models (LLMs) with chains-of-thought have demonstrated strong\nperformance on an increasing range of tasks, particularly those involving\ncomplex logical reasoning. However, excessively long chains can lead to\noverthinking, causing computational waste and slower responses. This raises a\nquestion: can LLMs dynamically adjust the length of their reasoning processes\nbased on task complexity? To address this, we propose the Think in Blocks\nframework, which enables adaptive reasoning-from zero to deep reasoning-by\npartitioning the reasoning process into a tunable number of blocks. Our main\ncontributions are: (1) Establishing an explicit block-structured paradigm in\nwhich the model first predicts an integer reasoning budget-the number of\nblocks-and then partitions its reasoning accordingly; (2) Training an adaptive\nmodel through a three-stage pipeline-Supervised Fine-Tuning, reward-guided\nDirect Preference Optimization, and Reinforcement Learning-that adjusts its\nreasoning depth to problem difficulty; (3) Exploiting the explicit block count\nto dynamically control reasoning depth at inference time, allowing flexible\nadjustment of chain-of-thought length during deployment.", "AI": {"tldr": "本文提出“块状思考”（Think in Blocks）框架，使大型语言模型（LLMs）能够根据任务复杂性动态调整推理深度，通过将推理过程划分为可调数量的块，以避免过度思考并提高效率。", "motivation": "尽管LLMs的思维链（CoT）在复杂推理任务中表现出色，但过长的链条会导致计算浪费和响应变慢。因此，研究的动机是探讨LLMs能否根据任务复杂性动态调整其推理过程的长度。", "method": "该研究提出了“块状思考”框架，具体方法包括：1) 建立显式的块结构范式，模型首先预测推理预算（块的数量），然后相应地划分推理过程；2) 通过三阶段训练流程（监督微调、奖励引导的直接偏好优化和强化学习）训练自适应模型，使其能根据问题难度调整推理深度；3) 利用显式块计数在推理时动态控制推理深度，实现思维链长度的灵活调整。", "result": "主要贡献是：1) 建立了一个显式的块结构范式，模型能预测并划分推理预算；2) 训练了一个能根据问题难度调整推理深度的自适应模型；3) 实现了在推理时动态控制推理深度，从而灵活调整思维链长度。", "conclusion": "“块状思考”框架使LLMs能够从零推理到深度推理进行自适应调整，解决了传统思维链可能导致的过度思考问题，提高了推理效率和灵活性。"}}
{"id": "2508.15361", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.15361", "abs": "https://arxiv.org/abs/2508.15361", "authors": ["Shiwen Ni", "Guhong Chen", "Shuaimin Li", "Xuanang Chen", "Siyi Li", "Bingli Wang", "Qiyao Wang", "Xingjian Wang", "Yifan Zhang", "Liyang Fan", "Chengming Li", "Ruifeng Xu", "Le Sun", "Min Yang"], "title": "A Survey on Large Language Model Benchmarks", "comment": null, "summary": "In recent years, with the rapid development of the depth and breadth of large\nlanguage models' capabilities, various corresponding evaluation benchmarks have\nbeen emerging in increasing numbers. As a quantitative assessment tool for\nmodel performance, benchmarks are not only a core means to measure model\ncapabilities but also a key element in guiding the direction of model\ndevelopment and promoting technological innovation. We systematically review\nthe current status and development of large language model benchmarks for the\nfirst time, categorizing 283 representative benchmarks into three categories:\ngeneral capabilities, domain-specific, and target-specific. General capability\nbenchmarks cover aspects such as core linguistics, knowledge, and reasoning;\ndomain-specific benchmarks focus on fields like natural sciences, humanities\nand social sciences, and engineering technology; target-specific benchmarks pay\nattention to risks, reliability, agents, etc. We point out that current\nbenchmarks have problems such as inflated scores caused by data contamination,\nunfair evaluation due to cultural and linguistic biases, and lack of evaluation\non process credibility and dynamic environments, and provide a referable design\nparadigm for future benchmark innovation.", "AI": {"tldr": "本文首次系统回顾了大型语言模型基准测试的现状与发展，将283个代表性基准测试分为通用能力、领域特定和目标特定三类，并指出了现有基准测试存在的问题，为未来设计提供了参考范式。", "motivation": "随着大型语言模型能力的深度和广度快速发展，评估其性能的基准测试数量激增。基准测试不仅是衡量模型能力的核心手段，也是指导模型发展方向和促进技术创新的关键要素。因此，有必要对这些基准测试进行系统性回顾和分析。", "method": "本文采用系统性回顾的方法，首次对大型语言模型基准测试的现状和发展进行了梳理。研究者将283个具有代表性的基准测试归类为三大类：通用能力、领域特定和目标特定。", "result": "研究结果包括：1. 将大型语言模型基准测试分为通用能力（核心语言学、知识、推理）、领域特定（自然科学、人文社科、工程技术）和目标特定（风险、可靠性、智能体）三大类。2. 指出当前基准测试存在数据污染导致分数虚高、文化和语言偏见导致评估不公、以及缺乏对过程可信度和动态环境评估等问题。", "conclusion": "当前大型语言模型基准测试存在多方面问题，需要创新改进。本文为未来基准测试的设计提供了一个可参考的范式，以期解决现有挑战，推动更公平、全面和动态的模型评估。"}}
{"id": "2508.15299", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15299", "abs": "https://arxiv.org/abs/2508.15299", "authors": ["Ryunosuke Hayashi", "Kohei Torimi", "Rokuto Nagata", "Kazuma Ikeda", "Ozora Sako", "Taichi Nakamura", "Masaki Tani", "Yoshimitsu Aoki", "Kentaro Yoshioka"], "title": "BasketLiDAR: The First LiDAR-Camera Multimodal Dataset for Professional Basketball MOT", "comment": "Accepted to MMSports", "summary": "Real-time 3D trajectory player tracking in sports plays a crucial role in\ntactical analysis, performance evaluation, and enhancing spectator experience.\nTraditional systems rely on multi-camera setups, but are constrained by the\ninherently two-dimensional nature of video data and the need for complex 3D\nreconstruction processing, making real-time analysis challenging. Basketball,\nin particular, represents one of the most difficult scenarios in the MOT field,\nas ten players move rapidly and complexly within a confined court space, with\nfrequent occlusions caused by intense physical contact.\n  To address these challenges, this paper constructs BasketLiDAR, the first\nmultimodal dataset in the sports MOT field that combines LiDAR point clouds\nwith synchronized multi-view camera footage in a professional basketball\nenvironment, and proposes a novel MOT framework that simultaneously achieves\nimproved tracking accuracy and reduced computational cost. The BasketLiDAR\ndataset contains a total of 4,445 frames and 3,105 player IDs, with fully\nsynchronized IDs between three LiDAR sensors and three multi-view cameras. We\nrecorded 5-on-5 and 3-on-3 game data from actual professional basketball\nplayers, providing complete 3D positional information and ID annotations for\neach player. Based on this dataset, we developed a novel MOT algorithm that\nleverages LiDAR's high-precision 3D spatial information. The proposed method\nconsists of a real-time tracking pipeline using LiDAR alone and a multimodal\ntracking pipeline that fuses LiDAR and camera data. Experimental results\ndemonstrate that our approach achieves real-time operation, which was difficult\nwith conventional camera-only methods, while achieving superior tracking\nperformance even under occlusion conditions. The dataset is available upon\nrequest at: https://sites.google.com/keio.jp/keio-csg/projects/basket-lidar", "AI": {"tldr": "该论文构建了首个结合LiDAR点云和多视角相机数据的篮球运动多目标跟踪（MOT）数据集BasketLiDAR，并提出了一种新颖的MOT框架，实现了实时、高精度的3D球员轨迹跟踪，尤其在遮挡条件下表现优异。", "motivation": "传统的体育运动3D轨迹跟踪系统依赖多摄像头，但受限于视频数据的二维性、复杂的3D重建过程以及难以实现实时分析。篮球运动因球员快速、复杂移动和频繁遮挡，是MOT领域最具挑战性的场景之一。", "method": "1. 构建了BasketLiDAR数据集：这是体育MOT领域首个多模态数据集，包含LiDAR点云和同步的多视角相机画面，共4,445帧和3,105个球员ID，提供了完整的3D位置信息和ID标注。2. 提出了新颖的MOT算法：利用LiDAR高精度3D空间信息，设计了两个跟踪流程——单独使用LiDAR的实时跟踪管道和融合LiDAR与相机数据的多模态跟踪管道。", "result": "实验结果表明，所提出的方法实现了实时操作（传统纯相机方法难以达到），并且即使在遮挡条件下，也展现出卓越的跟踪性能。", "conclusion": "该研究通过引入BasketLiDAR数据集和基于LiDAR的多模态MOT框架，有效解决了篮球等复杂体育场景中实时3D球员轨迹跟踪的挑战，显著提升了跟踪精度和效率。"}}
{"id": "2508.15510", "categories": ["cs.AI", "I.2.11; I.2.0; J.4; K.4.0; I.2.6"], "pdf": "https://arxiv.org/pdf/2508.15510", "abs": "https://arxiv.org/abs/2508.15510", "authors": ["Filippo Tonini", "Lukas Galke"], "title": "Super-additive Cooperation in Language Model Agents", "comment": "FAIEMA 2025", "summary": "With the prospect of autonomous artificial intelligence (AI) agents, studying\ntheir tendency for cooperative behavior becomes an increasingly relevant topic.\nThis study is inspired by the super-additive cooperation theory, where the\ncombined effects of repeated interactions and inter-group rivalry have been\nargued to be the cause for cooperative tendencies found in humans. We devised a\nvirtual tournament where language model agents, grouped into teams, face each\nother in a Prisoner's Dilemma game. By simulating both internal team dynamics\nand external competition, we discovered that this blend substantially boosts\nboth overall and initial, one-shot cooperation levels (the tendency to\ncooperate in one-off interactions). This research provides a novel framework\nfor large language models to strategize and act in complex social scenarios and\noffers evidence for how intergroup competition can, counter-intuitively, result\nin more cooperative behavior. These insights are crucial for designing future\nmulti-agent AI systems that can effectively work together and better align with\nhuman values. Source code is available at\nhttps://github.com/pippot/Superadditive-cooperation-LLMs.", "AI": {"tldr": "本研究受超加性合作理论启发，通过模拟团队内部动态和外部竞争，发现将大型语言模型（LLM）代理分组进行囚徒困境游戏，能显著提升其整体和首次合作水平。", "motivation": "随着自主AI代理的出现，研究其合作行为变得日益重要。本研究受超加性合作理论启发，该理论认为重复互动和群体间竞争的结合是人类合作倾向的原因。", "method": "设计了一个虚拟锦标赛，将语言模型代理分组，在囚徒困境游戏中相互对抗。模拟了团队内部动态和外部竞争的结合。", "result": "发现内部团队动态和外部竞争的结合显著提升了LLM代理的整体合作水平和初始一次性合作水平。", "conclusion": "本研究为LLM在复杂社会场景中制定策略和行动提供了一个新颖框架，并提供了群体间竞争能反直觉地导致更多合作行为的证据。这些见解对于设计未来能有效协作并与人类价值观对齐的多智能体AI系统至关重要。"}}
{"id": "2508.15370", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15370", "abs": "https://arxiv.org/abs/2508.15370", "authors": ["Yichi Zhang", "Yao Huang", "Yifan Wang", "Yitong Sun", "Chang Liu", "Zhe Zhao", "Zhengwei Fang", "Huanran Chen", "Xiao Yang", "Xingxing Wei", "Hang Su", "Yinpeng Dong", "Jun Zhu"], "title": "Unveiling Trust in Multimodal Large Language Models: Evaluation, Analysis, and Mitigation", "comment": "For Appendix, please refer to arXiv:2406.07057", "summary": "The trustworthiness of Multimodal Large Language Models (MLLMs) remains an\nintense concern despite the significant progress in their capabilities.\nExisting evaluation and mitigation approaches often focus on narrow aspects and\noverlook risks introduced by the multimodality. To tackle these challenges, we\npropose MultiTrust-X, a comprehensive benchmark for evaluating, analyzing, and\nmitigating the trustworthiness issues of MLLMs. We define a three-dimensional\nframework, encompassing five trustworthiness aspects which include\ntruthfulness, robustness, safety, fairness, and privacy; two novel risk types\ncovering multimodal risks and cross-modal impacts; and various mitigation\nstrategies from the perspectives of data, model architecture, training, and\ninference algorithms. Based on the taxonomy, MultiTrust-X includes 32 tasks and\n28 curated datasets, enabling holistic evaluations over 30 open-source and\nproprietary MLLMs and in-depth analysis with 8 representative mitigation\nmethods. Our extensive experiments reveal significant vulnerabilities in\ncurrent models, including a gap between trustworthiness and general\ncapabilities, as well as the amplification of potential risks in base LLMs by\nboth multimodal training and inference. Moreover, our controlled analysis\nuncovers key limitations in existing mitigation strategies that, while some\nmethods yield improvements in specific aspects, few effectively address overall\ntrustworthiness, and many introduce unexpected trade-offs that compromise model\nutility. These findings also provide practical insights for future\nimprovements, such as the benefits of reasoning to better balance safety and\nperformance. Based on these insights, we introduce a Reasoning-Enhanced Safety\nAlignment (RESA) approach that equips the model with chain-of-thought reasoning\nability to discover the underlying risks, achieving state-of-the-art results.", "AI": {"tldr": "该研究提出了MultiTrust-X，一个用于评估、分析和缓解多模态大语言模型（MLLMs）信任度问题的综合基准，并揭示了现有模型的漏洞和缓解策略的局限性，最终提出了一种基于推理的安全性对齐（RESA）方法。", "motivation": "尽管多模态大语言模型（MLLMs）的能力显著提升，但其信任度仍是主要担忧。现有评估和缓解方法往往过于狭窄，并忽视了多模态引入的风险。", "method": "提出了MultiTrust-X基准，该基准基于一个三维框架，涵盖了真实性、鲁棒性、安全性、公平性和隐私五个信任度维度；多模态风险和跨模态影响两种新型风险类型；以及数据、模型架构、训练和推理算法等多种缓解策略。MultiTrust-X包含32项任务和28个精选数据集，用于评估30多个开源和专有MLLMs，并深入分析了8种代表性缓解方法。此外，基于实验洞察，提出了一种推理增强型安全对齐（RESA）方法。", "result": "实验揭示了当前模型存在显著漏洞，包括信任度与通用能力之间的差距，以及多模态训练和推理对基础LLM潜在风险的放大作用。受控分析表明，现有缓解策略存在关键局限性，很少能有效解决整体信任度问题，并且许多策略会引入意想不到的权衡，损害模型实用性。基于这些发现，提出的RESA方法通过赋予模型链式思维推理能力来发现潜在风险，取得了最先进的结果。", "conclusion": "当前MLLMs在信任度方面存在严重漏洞，多模态特性会放大风险。现有缓解策略效果有限且常带来权衡。推理能力对于平衡安全性和性能至关重要，基于此，RESA方法为提升MLLMs的信任度提供了一条有前景的路径。"}}
{"id": "2508.15313", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15313", "abs": "https://arxiv.org/abs/2508.15313", "authors": ["Wutao Liu", "YiDan Wang", "Pan Gao"], "title": "First RAG, Second SEG: A Training-Free Paradigm for Camouflaged Object Detection", "comment": null, "summary": "Camouflaged object detection (COD) poses a significant challenge in computer\nvision due to the high similarity between objects and their backgrounds.\nExisting approaches often rely on heavy training and large computational\nresources. While foundation models such as the Segment Anything Model (SAM)\noffer strong generalization, they still struggle to handle COD tasks without\nfine-tuning and require high-quality prompts to yield good performance.\nHowever, generating such prompts manually is costly and inefficient. To address\nthese challenges, we propose \\textbf{First RAG, Second SEG (RAG-SEG)}, a\ntraining-free paradigm that decouples COD into two stages: Retrieval-Augmented\nGeneration (RAG) for generating coarse masks as prompts, followed by SAM-based\nsegmentation (SEG) for refinement. RAG-SEG constructs a compact retrieval\ndatabase via unsupervised clustering, enabling fast and effective feature\nretrieval. During inference, the retrieved features produce pseudo-labels that\nguide precise mask generation using SAM2. Our method eliminates the need for\nconventional training while maintaining competitive performance. Extensive\nexperiments on benchmark COD datasets demonstrate that RAG-SEG performs on par\nwith or surpasses state-of-the-art methods. Notably, all experiments are\nconducted on a \\textbf{personal laptop}, highlighting the computational\nefficiency and practicality of our approach. We present further analysis in the\nAppendix, covering limitations, salient object detection extension, and\npossible improvements.", "AI": {"tldr": "本文提出RAG-SEG，一种无需训练的两阶段伪装目标检测方法。它首先利用检索增强生成（RAG）生成粗略掩码作为提示，然后通过基于SAM的分割（SEG）进行细化，实现了高效率和竞争性性能。", "motivation": "伪装目标检测（COD）因目标与背景高度相似而极具挑战。现有方法通常需要大量训练和计算资源。基础模型（如SAM）虽泛化能力强，但直接应用于COD仍面临困难，且需要耗时耗力的高质量提示。", "method": "本文提出RAG-SEG，一种无需训练的两阶段范式。首先，通过检索增强生成（RAG）阶段，利用无监督聚类构建紧凑检索数据库，快速有效地检索特征，生成粗略掩码作为提示。其次，通过基于SAM的分割（SEG）阶段，利用检索到的特征产生的伪标签指导SAM2生成精确掩码进行细化。", "result": "该方法在保持竞争性性能的同时，在基准COD数据集上表现与现有最先进方法相当或超越。所有实验均在个人笔记本电脑上进行，突显了其计算效率和实用性。", "conclusion": "RAG-SEG为伪装目标检测提供了一种无需训练、高效且实用的解决方案，其性能达到或超越了现有先进水平，有效解决了传统方法重度训练和提示生成困难的问题。"}}
{"id": "2508.15548", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15548", "abs": "https://arxiv.org/abs/2508.15548", "authors": ["Jiayi Song", "Rui Wan", "Lipeng Ma", "Weidong Yang", "Qingyuan Zhou", "Yixuan Li", "Ben Fei"], "title": "DeepThink3D: Enhancing Large Language Models with Programmatic Reasoning in Complex 3D Situated Reasoning Tasks", "comment": null, "summary": "This work enhances the ability of large language models (LLMs) to perform\ncomplex reasoning in 3D scenes. Recent work has addressed the 3D situated\nreasoning task by invoking tool usage through large language models. Large\nlanguage models call tools via APIs and integrate the generated programs\nthrough a chain of thought to solve problems based on the program results.\nHowever, due to the simplicity of the questions in the dataset, the generated\nprogram reasoning chains are relatively short. To solve this main challenge, in\nthis paper, we introduce DeepThink3D to enhance the tool usage of LLMs in\ncomplex 3D situated reasoning tasks. Our work proposes a combinatorial and\niterative evolutionary approach on the SQA3D benchmark to generate more complex\nquestions. Building on this foundation, we fine-tune the large language model\nto make it more proficient in using 3D tools. By employing Direct Preference\nOptimization (DPO), we directly optimize the toolchain strategies generated by\nmodels, thereby enhancing their accuracy in complex tasks.", "AI": {"tldr": "该研究引入DeepThink3D框架，通过生成更复杂的3D场景推理问题并利用DPO优化工具链策略，提升大型语言模型在复杂3D情境推理任务中的工具使用能力和准确性。", "motivation": "现有研究虽已通过工具调用使LLMs进行3D情境推理，但由于数据集问题过于简单，导致生成的程序推理链较短，LLMs难以处理更复杂的3D推理任务。", "method": "1. 引入DeepThink3D框架。2. 提出一种组合式和迭代式的进化方法，在SQA3D基准上生成更复杂的问题。3. 基于此基础，微调大型语言模型以提高其使用3D工具的熟练度。4. 采用直接偏好优化（DPO）方法，直接优化模型生成的工具链策略。", "result": "通过上述方法，LLMs在复杂3D情境推理任务中的工具使用能力和准确性得到增强。", "conclusion": "DeepThink3D通过生成复杂问题和优化工具链策略，显著提升了LLMs在3D场景复杂推理任务中的表现。"}}
{"id": "2508.15371", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.15371", "abs": "https://arxiv.org/abs/2508.15371", "authors": ["Jaydip Sen", "Subhasis Dasgupta", "Hetvi Waghela"], "title": "Confidence-Modulated Speculative Decoding for Large Language Models", "comment": "This is the preprint of the paper, which has been accepted for oral\n  presentation and publication in the proceedings of IEEE INDISCON 2025. The\n  conference will be organized at the National Institute of Technology,\n  Rourkela, India, from August 21 to 23, 2025. The paper is 10 pages long, and\n  it contains 2 figures and 5 tables", "summary": "Speculative decoding has emerged as an effective approach for accelerating\nautoregressive inference by parallelizing token generation through a\ndraft-then-verify paradigm. However, existing methods rely on static drafting\nlengths and rigid verification criteria, limiting their adaptability across\nvarying model uncertainties and input complexities. This paper proposes an\ninformation-theoretic framework for speculative decoding based on\nconfidence-modulated drafting. By leveraging entropy and margin-based\nuncertainty measures over the drafter's output distribution, the proposed\nmethod dynamically adjusts the number of speculatively generated tokens at each\niteration. This adaptive mechanism reduces rollback frequency, improves\nresource utilization, and maintains output fidelity. Additionally, the\nverification process is modulated using the same confidence signals, enabling\nmore flexible acceptance of drafted tokens without sacrificing generation\nquality. Experiments on machine translation and summarization tasks demonstrate\nsignificant speedups over standard speculative decoding while preserving or\nimproving BLEU and ROUGE scores. The proposed approach offers a principled,\nplug-in method for efficient and robust decoding in large language models under\nvarying conditions of uncertainty.", "AI": {"tldr": "本文提出了一种基于置信度调制的推测解码信息论框架，通过动态调整草稿长度和验证过程来提高自回归推理的速度和鲁棒性，同时保持生成质量。", "motivation": "现有推测解码方法依赖静态草稿长度和刚性验证标准，限制了它们在不同模型不确定性和输入复杂性下的适应性。", "method": "提出了一种基于置信度调制的推测解码信息论框架。该方法利用草稿器输出分布上的熵和基于裕度的不确定性度量，动态调整每次迭代中推测生成的令牌数量。此外，验证过程也通过相同的置信度信号进行调制，允许更灵活地接受草稿令牌。", "result": "在机器翻译和摘要任务中，该方法比标准推测解码实现了显著加速，同时保持或提高了BLEU和ROUGE分数。", "conclusion": "该方法提供了一种原则性的、可插拔的方法，用于在不同不确定性条件下对大型语言模型进行高效且鲁棒的解码。"}}
{"id": "2508.15314", "categories": ["cs.CV", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.15314", "abs": "https://arxiv.org/abs/2508.15314", "authors": ["Naen Xu", "Jinghuai Zhang", "Changjiang Li", "Zhi Chen", "Chunyi Zhou", "Qingming Li", "Tianyu Du", "Shouling Ji"], "title": "VideoEraser: Concept Erasure in Text-to-Video Diffusion Models", "comment": "To appear in the 2025 Conference on Empirical Methods in Natural\n  Language Processing (EMNLP)", "summary": "The rapid growth of text-to-video (T2V) diffusion models has raised concerns\nabout privacy, copyright, and safety due to their potential misuse in\ngenerating harmful or misleading content. These models are often trained on\nnumerous datasets, including unauthorized personal identities, artistic\ncreations, and harmful materials, which can lead to uncontrolled production and\ndistribution of such content. To address this, we propose VideoEraser, a\ntraining-free framework that prevents T2V diffusion models from generating\nvideos with undesirable concepts, even when explicitly prompted with those\nconcepts. Designed as a plug-and-play module, VideoEraser can seamlessly\nintegrate with representative T2V diffusion models via a two-stage process:\nSelective Prompt Embedding Adjustment (SPEA) and Adversarial-Resilient Noise\nGuidance (ARNG). We conduct extensive evaluations across four tasks, including\nobject erasure, artistic style erasure, celebrity erasure, and explicit content\nerasure. Experimental results show that VideoEraser consistently outperforms\nprior methods regarding efficacy, integrity, fidelity, robustness, and\ngeneralizability. Notably, VideoEraser achieves state-of-the-art performance in\nsuppressing undesirable content during T2V generation, reducing it by 46% on\naverage across four tasks compared to baselines.", "AI": {"tldr": "VideoEraser是一个免训练的即插即用框架，旨在阻止文本到视频（T2V）扩散模型生成包含不良概念的视频，即使这些概念被明确提示。它通过选择性提示嵌入调整（SPEA）和对抗性弹性噪声引导（ARNG）两阶段过程实现。", "motivation": "T2V扩散模型的快速发展引发了隐私、版权和安全担忧，因为它们可能被滥用于生成有害或误导性内容。这些模型常在未经授权的个人身份、艺术创作和有害材料上训练，导致此类内容不受控制地生产和分发。", "method": "VideoEraser是一个免训练的即插即用框架，通过两阶段过程与T2V扩散模型集成：选择性提示嵌入调整（SPEA）和对抗性弹性噪声引导（ARNG）。", "result": "VideoEraser在对象擦除、艺术风格擦除、名人擦除和明确内容擦除四项任务中进行了广泛评估。实验结果表明，VideoEraser在功效、完整性、保真度、鲁棒性和泛化性方面始终优于现有方法，平均将不良内容生成减少了46%。", "conclusion": "VideoEraser在抑制T2V生成过程中的不良内容方面取得了最先进的性能，有效解决了T2V模型潜在滥用带来的隐私、版权和安全问题。"}}
{"id": "2508.15588", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15588", "abs": "https://arxiv.org/abs/2508.15588", "authors": ["Ahmed Nasir", "Abdelhafid Zenati"], "title": "A Dynamical Systems Framework for Reinforcement Learning Safety and Robustness Verification", "comment": null, "summary": "The application of reinforcement learning to safety-critical systems is\nlimited by the lack of formal methods for verifying the robustness and safety\nof learned policies. This paper introduces a novel framework that addresses\nthis gap by analyzing the combination of an RL agent and its environment as a\ndiscrete-time autonomous dynamical system. By leveraging tools from dynamical\nsystems theory, specifically the Finite-Time Lyapunov Exponent (FTLE), we\nidentify and visualize Lagrangian Coherent Structures (LCS) that act as the\nhidden \"skeleton\" governing the system's behavior. We demonstrate that\nrepelling LCS function as safety barriers around unsafe regions, while\nattracting LCS reveal the system's convergence properties and potential failure\nmodes, such as unintended \"trap\" states. To move beyond qualitative\nvisualization, we introduce a suite of quantitative metrics, Mean Boundary\nRepulsion (MBR), Aggregated Spurious Attractor Strength (ASAS), and\nTemporally-Aware Spurious Attractor Strength (TASAS), to formally measure a\npolicy's safety margin and robustness. We further provide a method for deriving\nlocal stability guarantees and extend the analysis to handle model uncertainty.\nThrough experiments in both discrete and continuous control environments, we\nshow that this framework provides a comprehensive and interpretable assessment\nof policy behavior, successfully identifying critical flaws in policies that\nappear successful based on reward alone.", "AI": {"tldr": "该论文提出了一个新颖的框架，利用动力系统理论（特别是FTLE和LCS）来形式化验证强化学习策略在安全关键系统中的鲁棒性和安全性，并通过定量指标识别策略的潜在缺陷。", "motivation": "强化学习在安全关键系统中的应用受限于缺乏形式化方法来验证学习策略的鲁棒性和安全性。", "method": "该研究将RL智能体及其环境组合视为一个离散时间自主动力系统。利用有限时间李雅普诺夫指数（FTLE）识别拉格朗日相干结构（LCS），其中排斥性LCS作为安全屏障，吸引性LCS揭示收敛性和潜在故障模式。此外，引入了平均边界排斥（MBR）、聚合虚假吸引子强度（ASAS）和时间感知虚假吸引子强度（TASAS）等定量指标来衡量策略的安全裕度和鲁棒性，并提供了推导局部稳定性保证和处理模型不确定性的方法。", "result": "该框架提供了一个全面且可解释的策略行为评估，成功识别了仅凭奖励看似成功的策略中的关键缺陷。通过实验证明，排斥性LCS可作为不安全区域周围的安全屏障，吸引性LCS可揭示系统的收敛特性和潜在的故障模式（如意外的“陷阱”状态）。", "conclusion": "该框架提供了一种综合且可解释的评估RL策略行为的方法，能够超越单纯的奖励指标，识别策略中的关键缺陷，从而提高安全关键系统中RL应用的可靠性。"}}
{"id": "2508.15390", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.15390", "abs": "https://arxiv.org/abs/2508.15390", "authors": ["Woojin Chung", "Jeonghoon Kim"], "title": "Exploiting Vocabulary Frequency Imbalance in Language Model Pre-training", "comment": "Preprint", "summary": "Large language models are trained with tokenizers, and the resulting token\ndistribution is highly imbalanced: a few words dominate the stream while most\noccur rarely. Recent practice favors ever-larger vocabularies, but the source\nof the benefit is unclear. We conduct a controlled study that scales the\nlanguage model's vocabulary from 24K to 196K while holding data, compute, and\noptimization fixed. We first quantify the complexity of tokenized text,\nformalized via Kolmogorov complexity, and show that larger vocabularies reduce\nthis complexity. Above 24K, every common word is already a single token, so\nfurther growth mainly deepens the relative token-frequency imbalance. A\nword-level loss decomposition shows that larger vocabularies reduce\ncross-entropy almost exclusively by lowering uncertainty on the 2,500 most\nfrequent words, even though loss on the rare tail rises. Constraining input and\noutput embedding norms to attenuate the effect of token-frequency imbalance\nreverses the gain, directly showing that the model exploits rather than suffers\nfrom imbalance. Because the same frequent words cover roughly 77% of tokens in\ndownstream benchmarks, this training advantage transfers intact. We also show\nthat enlarging model parameters with a fixed vocabulary yields the same\nfrequent-word benefit. Our results reframe \"bigger vocabularies help\" as\n\"lowering the complexity of tokenized text helps,\" providing a simple,\nprincipled lever for tokenizer-model co-design and clarifying the loss dynamics\nthat govern language-model scaling in pre-training.", "AI": {"tldr": "研究发现，更大的词汇表通过降低分词文本的复杂性来帮助大型语言模型，这种益处主要集中在常见词上，模型利用了词元频率的不平衡性，而非受其困扰。", "motivation": "大型语言模型倾向于使用更大的词汇表，但这种做法的益处及其背后的机制尚不明确，尤其是在词元分布高度不平衡的情况下。", "method": "进行了一项对照研究，在保持数据、计算和优化不变的情况下，将语言模型的词汇表从2.4万扩展到19.6万。通过柯尔莫哥洛夫复杂度量化了分词文本的复杂性。进行了词级别损失分解。通过限制输入和输出嵌入范数来评估词元频率不平衡的影响。", "result": "更大的词汇表降低了分词文本的复杂性。超过2.4万后，常见词已是单个词元，词汇量增加主要加剧了相对词元频率的不平衡。词汇量越大，交叉熵损失几乎完全通过降低2500个最常见词的不确定性而减少，尽管稀有词上的损失有所增加。限制嵌入范数会消除这种收益，表明模型利用而非受困于不平衡。这种训练优势在下游基准测试中得以保留，因为常见词覆盖了约77%的词元。增加模型参数但固定词汇量也能带来相同的常见词益处。", "conclusion": "“更大的词汇表有帮助”可以重新理解为“降低分词文本的复杂性有帮助”。这为词元器-模型协同设计提供了一个简单、有原则的杠杆，并阐明了预训练中语言模型扩展的损失动态。"}}
{"id": "2508.15336", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15336", "abs": "https://arxiv.org/abs/2508.15336", "authors": ["Subhasis Dasgupta", "Preetam Saha", "Agniva Roy", "Jaydip Sen"], "title": "Predicting Road Crossing Behaviour using Pose Detection and Sequence Modelling", "comment": "This is a pre-print version of the original paper accepted in the\n  IEEE conference INDISCON 2025. It contains 8 figures and 1 table. The length\n  of the paper is 7 pages", "summary": "The world is constantly moving towards AI based systems and autonomous\nvehicles are now reality in different parts of the world. These vehicles\nrequire sensors and cameras to detect objects and maneuver according to that.\nIt becomes important to for such vehicles to also predict from a distant if a\nperson is about to cross a road or not. The current study focused on predicting\nthe intent of crossing the road by pedestrians in an experimental setup. The\nstudy involved working with deep learning models to predict poses and sequence\nmodelling for temporal predictions. The study analysed three different sequence\nmodelling to understand the prediction behaviour and it was found out that GRU\nwas better in predicting the intent compared to LSTM model but 1D CNN was the\nbest model in terms of speed. The study involved video analysis, and the output\nof pose detection model was integrated later on to sequence modelling\ntechniques for an end-to-end deep learning framework for predicting road\ncrossing intents.", "AI": {"tldr": "本研究利用深度学习（姿态预测与序列建模）预测行人过马路意图，发现1D CNN在速度上表现最佳，GRU在准确性上优于LSTM。", "motivation": "随着自动驾驶汽车的普及，车辆需要提前从远处预测行人是否即将过马路，以确保行车安全和作出相应决策。", "method": "研究采用端到端深度学习框架。首先使用深度学习模型进行姿态检测，然后将姿态检测的输出整合到三种不同的序列建模技术（GRU、LSTM和1D CNN）中，以进行时间序列预测来预测行人过马路意图。", "result": "在预测行人过马路意图方面，GRU模型表现优于LSTM模型。然而，1D CNN模型在处理速度方面表现最佳。", "conclusion": "针对行人过马路意图预测任务，GRU在预测准确性上表现较好，而1D CNN在速度方面具有显著优势，为自动驾驶系统提供了潜在的实时预测解决方案。"}}
{"id": "2508.15610", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.15610", "abs": "https://arxiv.org/abs/2508.15610", "authors": ["Alfio Gliozzo", "Naweed Khan", "Christodoulos Constantinides", "Nandana Mihindukulasooriya", "Nahuel Defosse", "Junkyu Lee"], "title": "Transduction is All You Need for Structured Data Workflows", "comment": "32 pages, 8 figures", "summary": "This paper introduces Agentics, a modular framework for building agent-based\nsystems capable of structured reasoning and compositional generalization over\ncomplex data. Designed with research and practical applications in mind,\nAgentics offers a novel perspective on working with data and AI workflows. In\nthis framework, agents are abstracted from the logical flow and they are used\ninternally to the data type to enable logical transduction among data. Agentics\nencourages AI developers to focus on modeling data rather than crafting\nprompts, enabling a declarative language in which data types are provided by\nLLMs and composed through logical transduction, which is executed by LLMs when\ntypes are connected. We provide empirical evidence demonstrating the\napplicability of this framework across domain-specific multiple-choice question\nanswering, semantic parsing for text-to-SQL, and automated prompt optimization\ntasks, achieving state-of-the-art accuracy or improved scalability without\nsacrificing performance. The open-source implementation is available at\n\\texttt{https://github.com/IBM/agentics}.", "AI": {"tldr": "本文介绍了Agentics，一个模块化框架，用于构建能够对复杂数据进行结构化推理和组合泛化的基于代理的系统。它将代理抽象到数据类型内部以实现逻辑转导，并鼓励开发者关注数据建模而非提示工程，在多项任务中实现了最先进的准确性或提高了可扩展性。", "motivation": "研究旨在构建能够对复杂数据进行结构化推理和组合泛化的基于代理的系统，并提供一种处理数据和AI工作流的新视角，鼓励AI开发者将重点从编写提示转移到数据建模上。", "method": "Agentics框架将代理从逻辑流中抽象出来，并将其用于数据类型内部，以实现数据间的逻辑转导。它采用一种声明式语言，其中数据类型由大型语言模型（LLM）提供，并通过LLM执行的逻辑转导进行组合。", "result": "该框架在领域特定多项选择问答、文本到SQL的语义解析和自动化提示优化任务中得到了应用验证。它在这些任务中实现了最先进的准确性，或者在不牺牲性能的情况下提高了可扩展性。", "conclusion": "Agentics是一个有效且实用的框架，能够促进基于代理的系统在复杂数据上的结构化推理和组合泛化。它通过改变AI开发范式（从提示工程到数据建模）来提升效率和性能，并已在多项任务中得到实证支持。"}}
{"id": "2508.15396", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.15396", "abs": "https://arxiv.org/abs/2508.15396", "authors": ["Tobias Schreieder", "Tim Schopf", "Michael Färber"], "title": "Attribution, Citation, and Quotation: A Survey of Evidence-based Text Generation with Large Language Models", "comment": null, "summary": "The increasing adoption of large language models (LLMs) has been accompanied\nby growing concerns regarding their reliability and trustworthiness. As a\nresult, a growing body of research focuses on evidence-based text generation\nwith LLMs, aiming to link model outputs to supporting evidence to ensure\ntraceability and verifiability. However, the field is fragmented due to\ninconsistent terminology, isolated evaluation practices, and a lack of unified\nbenchmarks. To bridge this gap, we systematically analyze 134 papers, introduce\na unified taxonomy of evidence-based text generation with LLMs, and investigate\n300 evaluation metrics across seven key dimensions. Thereby, we focus on\napproaches that use citations, attribution, or quotations for evidence-based\ntext generation. Building on this, we examine the distinctive characteristics\nand representative methods in the field. Finally, we highlight open challenges\nand outline promising directions for future work.", "AI": {"tldr": "本文系统分析了134篇关于基于证据的大型语言模型（LLMs）文本生成论文，旨在统一术语、评估方法和基准，并提出了一个统一的分类法，分析了评估指标，并指出了未来研究方向。", "motivation": "随着LLMs的广泛应用，对其可靠性和可信度日益担忧。现有研究领域碎片化，存在术语不一致、评估实践孤立以及缺乏统一基准的问题，阻碍了该领域的发展。", "method": "研究方法包括：系统分析134篇相关论文，引入基于证据的LLMs文本生成的统一分类法，调查300个评估指标（跨七个关键维度），并重点关注使用引用、归因或引述的方法。在此基础上，审查了该领域的独特特征和代表性方法。", "result": "研究结果包括：提出了一个基于证据的LLMs文本生成的统一分类法；对300个评估指标进行了多维度分析；识别并总结了该领域的独特特征和代表性方法；指出了开放性挑战并概述了未来工作的有前景方向。", "conclusion": "本文通过对现有文献的系统分析，弥合了基于证据的LLMs文本生成领域的空白，提供了一个统一的视角和分类法，并为未来的研究指明了方向。"}}
{"id": "2508.15353", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15353", "abs": "https://arxiv.org/abs/2508.15353", "authors": ["Olga Matykina", "Dmitry Yudin"], "title": "RCDINO: Enhancing Radar-Camera 3D Object Detection with DINOv2 Semantic Features", "comment": "Accepted for publication in Optical Memory and Neural Networks, 2025", "summary": "Three-dimensional object detection is essential for autonomous driving and\nrobotics, relying on effective fusion of multimodal data from cameras and\nradar. This work proposes RCDINO, a multimodal transformer-based model that\nenhances visual backbone features by fusing them with semantically rich\nrepresentations from the pretrained DINOv2 foundation model. This approach\nenriches visual representations and improves the model's detection performance\nwhile preserving compatibility with the baseline architecture. Experiments on\nthe nuScenes dataset demonstrate that RCDINO achieves state-of-the-art\nperformance among radar-camera models, with 56.4 NDS and 48.1 mAP. Our\nimplementation is available at https://github.com/OlgaMatykina/RCDINO.", "AI": {"tldr": "RCDINO是一种多模态Transformer模型，通过融合预训练DINOv2模型的语义特征来增强视觉骨干特征，从而在nuScenes数据集上实现了雷达-相机3D目标检测的SOTA性能。", "motivation": "自动驾驶和机器人技术需要有效的3D目标检测，这依赖于相机和雷达等多模态数据的有效融合。当前挑战在于如何有效增强视觉特征以提升检测性能。", "method": "本文提出了RCDINO模型，它是一种基于Transformer的多模态模型。该方法通过将视觉骨干特征与来自预训练DINOv2基础模型的语义丰富表示进行融合，来增强视觉特征，同时保持与基线架构的兼容性。", "result": "在nuScenes数据集上的实验表明，RCDINO在雷达-相机模型中取得了最先进的性能，NDS达到56.4，mAP达到48.1。", "conclusion": "RCDINO通过融合DINOv2的语义特征有效提升了视觉表示和模型检测性能，在雷达-相机3D目标检测领域达到了领先水平。"}}
{"id": "2508.15630", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15630", "abs": "https://arxiv.org/abs/2508.15630", "authors": ["Meera Ray", "Christopher L. Dancy"], "title": "Adapting A Vector-Symbolic Memory for Lisp ACT-R", "comment": "6 pages. 5 figures. Submitted and accepted to the 23rd International\n  Conference on Cognitive Modeling (ICCM 2025)", "summary": "Holographic Declarative Memory (HDM) is a vector-symbolic alternative to\nACT-R's Declarative Memory (DM) system that can bring advantages such as\nscalability and architecturally defined similarity between DM chunks. We\nadapted HDM to work with the most comprehensive and widely-used implementation\nof ACT-R (Lisp ACT-R) so extant ACT-R models designed with DM can be run with\nHDM without major changes. With this adaptation of HDM, we have developed\nvector-based versions of common ACT-R functions, set up a text processing\npipeline to add the contents of large documents to ACT-R memory, and most\nsignificantly created a useful and novel mechanism to retrieve an entire chunk\nof memory based on a request using only vector representations of tokens.\nPreliminary results indicate that we can maintain vector-symbolic advantages of\nHDM (e.g., chunk recall without storing the actual chunk and other advantages\nwith scaling) while also extending it so that previous ACT-R models may work\nwith the system with little (or potentially no) modifications within the actual\nprocedural and declarative memory portions of a model. As a part of iterative\nimprovement of this newly translated holographic declarative memory module, we\nwill continue to explore better time-context representations for vectors to\nimprove the module's ability to reconstruct chunks during recall. To more fully\ntest this translated HDM module, we also plan to develop decision-making models\nthat use instance-based learning (IBL) theory, which is a useful application of\nHDM given the advantages of the system.", "AI": {"tldr": "本文将全息声明式记忆（HDM）系统适配到Lisp ACT-R中，实现了基于向量的块检索，并保持了与现有ACT-R模型的兼容性，同时提供了可伸缩性和结构定义的相似性等优势。", "motivation": "ACT-R的声明式记忆（DM）系统可能存在可伸缩性等局限性。全息声明式记忆（HDM）作为一种向量符号替代方案，能够提供更好的可伸缩性以及架构定义的块间相似性，因此有必要将其集成到ACT-R中。", "method": "研究人员将HDM适配到Lisp ACT-R中，开发了基于向量的ACT-R常见函数，建立了用于将大型文档内容添加到ACT-R记忆的文本处理流程，并创建了一种新颖的机制，仅使用令牌的向量表示即可检索完整的记忆块。", "result": "初步结果表明，该系统能够保持HDM的向量符号优势（例如，无需存储实际块即可进行块召回，以及扩展性优势），同时使得现有的ACT-R模型只需少量（甚至无需）修改其程序性和声明性记忆部分即可与新系统协同工作。", "conclusion": "成功将HDM模块集成并翻译到ACT-R中，证明了其维护HDM优势和兼容现有ACT-R模型的能力。未来的工作将包括探索更好的时间上下文向量表示以改进块重建能力，并开发使用实例学习（IBL）理论的决策模型来充分测试该HDM模块。"}}
{"id": "2508.15407", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15407", "abs": "https://arxiv.org/abs/2508.15407", "authors": ["Cheng Wang", "Gelei Deng", "Xianglin Yang", "Han Qiu", "Tianwei Zhang"], "title": "When Audio and Text Disagree: Revealing Text Bias in Large Audio-Language Models", "comment": "Accepted by EMNLP 2025 Main", "summary": "Large Audio-Language Models (LALMs) are enhanced with audio perception\ncapabilities, enabling them to effectively process and understand multimodal\ninputs that combine audio and text. However, their performance in handling\nconflicting information between audio and text modalities remains largely\nunexamined. This paper introduces MCR-BENCH, the first comprehensive benchmark\nspecifically designed to evaluate how LALMs prioritize information when\npresented with inconsistent audio-text pairs. Through extensive evaluation\nacross diverse audio understanding tasks, we reveal a concerning phenomenon:\nwhen inconsistencies exist between modalities, LALMs display a significant bias\ntoward textual input, frequently disregarding audio evidence. This tendency\nleads to substantial performance degradation in audio-centric tasks and raises\nimportant reliability concerns for real-world applications. We further\ninvestigate the influencing factors of text bias, and explore mitigation\nstrategies through supervised finetuning, and analyze model confidence patterns\nthat reveal persistent overconfidence even with contradictory inputs. These\nfindings underscore the need for improved modality balance during training and\nmore sophisticated fusion mechanisms to enhance the robustness when handling\nconflicting multi-modal inputs. The project is available at\nhttps://github.com/WangCheng0116/MCR-BENCH.", "AI": {"tldr": "研究发现，大型音视频语言模型（LALMs）在处理音频和文本冲突信息时，存在显著的文本偏见，倾向于忽略音频证据，导致音频任务性能下降，并伴随过度自信。", "motivation": "现有研究未能充分考察LALMs在处理音频和文本模态之间冲突信息时的表现，这引发了对LALMs在多模态输入场景下信息优先级处理能力的疑问。", "method": "引入了MCR-BENCH，这是首个专门评估LALMs在面对不一致音视频对时如何处理信息的综合基准。通过在各种音频理解任务上进行广泛评估，并进一步调查文本偏见的影响因素，探索了监督微调的缓解策略，并分析了模型置信度模式。", "result": "LALMs在模态不一致时表现出显著的文本偏见，频繁忽略音频证据，导致音频中心任务的性能大幅下降，并引发了可靠性担忧。即使在输入矛盾的情况下，模型也表现出持续的过度自信。", "conclusion": "研究结果强调了在训练过程中需要改进模态平衡和更复杂的融合机制，以增强LALMs在处理冲突多模态输入时的鲁棒性。"}}
{"id": "2508.15360", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15360", "abs": "https://arxiv.org/abs/2508.15360", "authors": ["Chenhui Gou", "Ziyu Ma", "Zicheng Duan", "Haoyu He", "Feng Chen", "Akide Liu", "Bohan Zhuang", "Jianfei Cai", "Hamid Rezatofighi"], "title": "An Empirical Study on How Video-LLMs Answer Video Questions", "comment": null, "summary": "Taking advantage of large-scale data and pretrained language models, Video\nLarge Language Models (Video-LLMs) have shown strong capabilities in answering\nvideo questions. However, most existing efforts focus on improving performance,\nwith limited attention to understanding their internal mechanisms. This paper\naims to bridge this gap through a systematic empirical study. To interpret\nexisting VideoLLMs, we adopt attention knockouts as our primary analytical tool\nand design three variants: Video Temporal Knockout, Video Spatial Knockout, and\nLanguage-to-Video Knockout. Then, we apply these three knockouts on different\nnumbers of layers (window of layers). By carefully controlling the window of\nlayers and types of knockouts, we provide two settings: a global setting and a\nfine-grained setting. Our study reveals three key findings: (1) Global setting\nindicates Video information extraction primarily occurs in early layers,\nforming a clear two-stage process -- lower layers focus on perceptual encoding,\nwhile higher layers handle abstract reasoning; (2) In the fine-grained setting,\ncertain intermediate layers exert an outsized impact on video question\nanswering, acting as critical outliers, whereas most other layers contribute\nminimally; (3) In both settings, we observe that spatial-temporal modeling\nrelies more on language-guided retrieval than on intra- and inter-frame\nself-attention among video tokens, despite the latter's high computational\ncost. Finally, we demonstrate that these insights can be leveraged to reduce\nattention computation in Video-LLMs. To our knowledge, this is the first work\nto systematically uncover how Video-LLMs internally process and understand\nvideo content, offering interpretability and efficiency perspectives for future\nresearch.", "AI": {"tldr": "本研究通过注意力剔除（attention knockouts）方法，系统地分析了视频大语言模型（Video-LLMs）的内部机制，揭示了视频信息处理的阶段性特征、关键层的重要性以及语言引导检索在时空建模中的主导作用，并为模型效率优化提供了见解。", "motivation": "尽管视频大语言模型在视频问答方面表现出色，但现有研究主要关注性能提升，对模型内部机制的理解有限。本研究旨在填补这一空白，系统地解释Video-LLMs如何内部处理和理解视频内容。", "method": "采用注意力剔除（attention knockouts）作为主要分析工具，设计了三种变体：视频时间剔除、视频空间剔除和语言到视频剔除。这些剔除方法应用于不同层数（层窗口），并设置了全局和细粒度两种分析情境，以系统地解释现有Video-LLMs。", "result": "研究揭示了三个关键发现：(1) 在全局设置下，视频信息提取主要发生在早期层，形成清晰的两阶段过程——较低层侧重感知编码，较高层处理抽象推理；(2) 在细粒度设置下，某些中间层对视频问答具有超常影响，是关键的异常点，而大多数其他层的贡献极小；(3) 在两种设置下，时空建模更多依赖语言引导检索，而非视频token内部和帧间的自注意力，尽管后者计算成本高。这些见解可用于减少Video-LLMs中的注意力计算。", "conclusion": "本研究是首次系统地揭示Video-LLMs如何内部处理和理解视频内容的工作，为未来的研究提供了可解释性和效率方面的视角。研究结果表明，通过理解模型内部机制，可以实现模型优化和计算效率提升。"}}
{"id": "2508.15652", "categories": ["cs.AI", "cs.IT", "cs.LG", "cs.MA", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.15652", "abs": "https://arxiv.org/abs/2508.15652", "authors": ["Ardian Selmonaj", "Miroslav Strupl", "Oleg Szehr", "Alessandro Antonucci"], "title": "Understanding Action Effects through Instrumental Empowerment in Multi-Agent Reinforcement Learning", "comment": "European Conference on Artificial Intelligence (ECAI) 2025", "summary": "To reliably deploy Multi-Agent Reinforcement Learning (MARL) systems, it is\ncrucial to understand individual agent behaviors within a team. While prior\nwork typically evaluates overall team performance based on explicit reward\nsignals or learned value functions, it is unclear how to infer agent\ncontributions in the absence of any value feedback. In this work, we\ninvestigate whether meaningful insights into agent behaviors can be extracted\nthat are consistent with the underlying value functions, solely by analyzing\nthe policy distribution. Inspired by the phenomenon that intelligent agents\ntend to pursue convergent instrumental values, which generally increase the\nlikelihood of task success, we introduce Intended Cooperation Values (ICVs), a\nmethod based on information-theoretic Shapley values for quantifying each\nagent's causal influence on their co-players' instrumental empowerment.\nSpecifically, ICVs measure an agent's action effect on its teammates' policies\nby assessing their decision uncertainty and preference alignment. The analysis\nacross cooperative and competitive MARL environments reveals the extent to\nwhich agents adopt similar or diverse strategies. By comparing action effects\nbetween policies and value functions, our method identifies which agent\nbehaviors are beneficial to team success, either by fostering deterministic\ndecisions or by preserving flexibility for future action choices. Our proposed\nmethod offers novel insights into cooperation dynamics and enhances\nexplainability in MARL systems.", "AI": {"tldr": "本文提出“意图合作价值”（ICVs）方法，通过分析策略分布而非价值反馈，来量化多智能体强化学习中个体智能体对其队友“工具性赋能”的因果影响，从而揭示合作动态并增强系统可解释性。", "motivation": "在多智能体强化学习中，理解个体行为对于可靠部署至关重要。现有工作通常基于奖励信号或学习到的价值函数评估团队整体表现，但在缺乏价值反馈时，如何推断个体贡献尚不明确。", "method": "受智能体倾向于追求收敛的工具性价值的启发，本文引入了“意图合作价值”（ICVs）。ICVs方法基于信息论Shapley值，用于量化每个智能体对其队友“工具性赋能”的因果影响。具体来说，ICVs通过评估智能体行动对队友策略的决策不确定性和偏好一致性的影响来衡量其作用。", "result": "在合作和竞争性多智能体环境中的分析表明，该方法能够揭示智能体采取相似或多样化策略的程度。通过比较策略和价值函数之间的行动效果，ICVs能够识别哪些智能体行为（无论是促进确定性决策还是保留未来行动选择的灵活性）对团队成功有益。", "conclusion": "本文提出的方法为合作动态提供了新颖的见解，并显著增强了多智能体强化学习系统的可解释性。"}}
{"id": "2508.15418", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MM", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.15418", "abs": "https://arxiv.org/abs/2508.15418", "authors": ["Yirong Sun", "Yizhong Geng", "Peidong Wei", "Yanjun Chen", "Jinghan Yang", "Rongfei Chen", "Wei Zhang", "Xiaoyu Shen"], "title": "LLaSO: A Foundational Framework for Reproducible Research in Large Language and Speech Model", "comment": null, "summary": "The development of Large Speech-Language Models (LSLMs) has been slowed by\nfragmented architectures and a lack of transparency, hindering the systematic\ncomparison and reproducibility of research. Unlike in the vision-language\ndomain, the LSLM field suffers from the common practice of releasing model\nweights without their corresponding training data and configurations. To\naddress these critical gaps, we introduce LLaSO, the first fully open,\nend-to-end framework for large-scale speech-language modeling. LLaSO provides\nthe community with three essential resources: (1) LLaSO-Align, a 12M-instance\nspeech-text alignment corpus; (2) LLaSO-Instruct, a 13.5M-instance multi-task\ninstruction-tuning dataset; and (3) LLaSO-Eval, a reproducible benchmark for\nstandardized evaluation. To validate our framework, we build and release\nLLaSO-Base, a 3.8B-parameter reference model trained exclusively on our public\ndata. It achieves a normalized score of 0.72, establishing a strong,\nreproducible baseline that surpasses comparable models. Our analysis reveals\nthat while broader training coverage enhances performance, significant\ngeneralization gaps persist on unseen tasks, particularly in pure audio\nscenarios. By releasing the complete stack of data, benchmarks, and models,\nLLaSO establishes a foundational open standard to unify research efforts and\naccelerate community-driven progress in LSLMs. We release the code, dataset,\npretrained models, and results in https://github.com/EIT-NLP/LLaSO.", "AI": {"tldr": "LLaSO是一个用于大型语音语言模型（LSLM）的端到端开放框架，提供了对齐语料库、指令微调数据集、可复现基准测试，并发布了基于公共数据训练的参考模型LLaSO-Base，旨在解决LSLM领域碎片化和透明度不足的问题。", "motivation": "大型语音语言模型（LSLM）的开发受到架构碎片化和透明度不足的阻碍，导致研究难以系统比较和复现。与视觉-语言领域不同，LSLM领域普遍存在发布模型权重却不提供相应训练数据和配置的做法。", "method": "本文介绍了LLaSO，第一个完全开放的端到端大型语音语言建模框架。LLaSO提供了三个核心资源：(1) LLaSO-Align，一个包含1200万实例的语音-文本对齐语料库；(2) LLaSO-Instruct，一个包含1350万实例的多任务指令微调数据集；(3) LLaSO-Eval，一个用于标准化评估的可复现基准测试。为验证该框架，作者构建并发布了LLaSO-Base，一个38亿参数的参考模型，完全使用公共数据进行训练。", "result": "LLaSO-Base模型在标准化评估中取得了0.72的归一化分数，建立了一个强大且可复现的基线，并超越了同类模型。分析表明，更广泛的训练覆盖能提升性能，但在未见任务上，尤其是在纯音频场景中，仍然存在显著的泛化差距。", "conclusion": "通过发布完整的数据、基准测试和模型堆栈，LLaSO建立了一个基础性的开放标准，旨在统一研究工作并加速LSLM领域的社区驱动进展。"}}
{"id": "2508.15367", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15367", "abs": "https://arxiv.org/abs/2508.15367", "authors": ["Jacinto Colan", "Ana Davila", "Yasuhisa Hasegawa"], "title": "Transfer learning optimization based on evolutionary selective fine tuning", "comment": "Presented at the Workshop artiFicial And bio-inspIred netwoRked\n  intelliGence foR cOnstrained aUtoNomous Devices (FAIRGROUND). 2025\n  International Joint Conference on Neural Networks (IJCNN)", "summary": "Deep learning has shown substantial progress in image analysis. However, the\ncomputational demands of large, fully trained models remain a consideration.\nTransfer learning offers a strategy for adapting pre-trained models to new\ntasks. Traditional fine-tuning often involves updating all model parameters,\nwhich can potentially lead to overfitting and higher computational costs. This\npaper introduces BioTune, an evolutionary adaptive fine-tuning technique that\nselectively fine-tunes layers to enhance transfer learning efficiency. BioTune\nemploys an evolutionary algorithm to identify a focused set of layers for\nfine-tuning, aiming to optimize model performance on a given target task.\nEvaluation across nine image classification datasets from various domains\nindicates that BioTune achieves competitive or improved accuracy and efficiency\ncompared to existing fine-tuning methods such as AutoRGN and LoRA. By\nconcentrating the fine-tuning process on a subset of relevant layers, BioTune\nreduces the number of trainable parameters, potentially leading to decreased\ncomputational cost and facilitating more efficient transfer learning across\ndiverse data characteristics and distributions.", "AI": {"tldr": "BioTune是一种基于进化算法的自适应微调技术，通过选择性微调模型层，显著提高了迁移学习的效率和准确性，同时降低了计算成本。", "motivation": "深度学习模型计算需求高，传统微调方法更新所有参数可能导致过拟合和高计算成本。研究旨在开发更高效的迁移学习策略。", "method": "本文提出了BioTune，它利用进化算法来识别并选择模型中的特定层进行微调，以优化模型在特定目标任务上的性能。这种方法旨在集中微调过程，减少可训练参数的数量。", "result": "BioTune在九个图像分类数据集上的评估显示，与AutoRGN和LoRA等现有微调方法相比，它实现了具有竞争力或更高的准确性和效率。通过减少可训练参数，BioTune降低了潜在的计算成本。", "conclusion": "BioTune通过将微调过程集中在相关层子集上，有效地减少了计算成本并促进了跨不同数据特征和分布的更高效迁移学习。"}}
{"id": "2508.15680", "categories": ["cs.AI", "cs.HC", "I.2.6; I.2.11; K.4.1; K.6.0"], "pdf": "https://arxiv.org/pdf/2508.15680", "abs": "https://arxiv.org/abs/2508.15680", "authors": ["Mark Cote", "Susana Aires"], "title": "Futurity as Infrastructure: A Techno-Philosophical Interpretation of the AI Lifecycle", "comment": "15 pages, 3 figures, Presented at IAIL 2025 - Imagining the AI\n  Landscape after the AI Act, 4th International Workshop on Imagining the AI\n  Landscape After the AI Act, The fourth International Conference on Hybrid\n  Human-Artificial Intelligence", "summary": "This paper argues that a techno-philosophical reading of the EU AI Act\nprovides insight into the long-term dynamics of data in AI systems,\nspecifically, how the lifecycle from ingestion to deployment generates\nrecursive value chains that challenge existing frameworks for Responsible AI.\nWe introduce a conceptual tool to frame the AI pipeline, spanning data,\ntraining regimes, architectures, feature stores, and transfer learning. Using\ncross-disciplinary methods, we develop a technically grounded and\nphilosophically coherent analysis of regulatory blind spots. Our central claim\nis that what remains absent from policymaking is an account of the dynamic of\nbecoming that underpins both the technical operation and economic logic of AI.\nTo address this, we advance a formal reading of AI inspired by Simondonian\nphilosophy of technology, reworking his concept of individuation to model the\nAI lifecycle, including the pre-individual milieu, individuation, and\nindividuated AI. To translate these ideas, we introduce futurity: the\nself-reinforcing lifecycle of AI, where more data enhances performance, deepens\npersonalisation, and expands application domains. Futurity highlights the\nrecursively generative, non-rivalrous nature of data, underpinned by\ninfrastructures like feature stores that enable feedback, adaptation, and\ntemporal recursion. Our intervention foregrounds escalating power asymmetries,\nparticularly the tech oligarchy whose infrastructures of capture, training, and\ndeployment concentrate value and decision-making. We argue that effective\nregulation must address these infrastructural and temporal dynamics, and\npropose measures including lifecycle audits, temporal traceability, feedback\naccountability, recursion transparency, and a right to contest recursive reuse.", "AI": {"tldr": "本文通过对欧盟AI法案进行技术-哲学解读，引入西蒙东的个体化概念和“未来性”工具，揭示了AI数据生命周期中的递归价值链和监管盲点，并提出有效监管需关注基础设施和时间动态，以解决日益加剧的权力不对称问题。", "motivation": "现有负责任AI框架和欧盟AI法案面临AI系统数据生命周期（从摄取到部署）产生递归价值链的挑战。政策制定中缺乏对AI技术操作和经济逻辑背后“生成动态”的理解，导致监管盲点。", "method": "本文采用跨学科方法，对欧盟AI法案进行技术-哲学解读。引入了涵盖数据、训练机制、架构、特征存储和迁移学习的AI管道概念工具。借鉴西蒙东的技术哲学，重塑其“个体化”概念（包括前个体环境、个体化和个体化AI）来建模AI生命周期。提出了“未来性”概念来解释AI的自我强化生命周期。", "result": "AI的生命周期（从数据摄取到部署）生成了挑战现有负责任AI框架的递归价值链。政策制定中缺少对AI技术操作和经济逻辑背后“生成动态”的考量。引入的“未来性”概念揭示了AI自我强化的生命周期，即更多数据增强性能、深化个性化并扩展应用领域，突出了数据的递归性、非竞争性以及基础设施（如特征存储）在反馈、适应和时间递归中的作用。研究强调了权力不对称的加剧，特别是技术寡头通过其捕获、训练和部署的基础设施集中了价值和决策权。", "conclusion": "有效的AI监管必须解决基础设施和时间动态问题。为此，本文提出了一系列具体措施，包括生命周期审计、时间可追溯性、反馈问责制、递归透明度以及反对递归重用权利。"}}
{"id": "2508.15421", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.15421", "abs": "https://arxiv.org/abs/2508.15421", "authors": ["Pritilata Saha", "Abhirup Sinha"], "title": "A Study of Privacy-preserving Language Modeling Approaches", "comment": null, "summary": "Recent developments in language modeling have increased their use in various\napplications and domains. Language models, often trained on sensitive data, can\nmemorize and disclose this information during privacy attacks, raising concerns\nabout protecting individuals' privacy rights. Preserving privacy in language\nmodels has become a crucial area of research, as privacy is one of the\nfundamental human rights. Despite its significance, understanding of how much\nprivacy risk these language models possess and how it can be mitigated is still\nlimited. This research addresses this by providing a comprehensive study of the\nprivacy-preserving language modeling approaches. This study gives an in-depth\noverview of these approaches, highlights their strengths, and investigates\ntheir limitations. The outcomes of this study contribute to the ongoing\nresearch on privacy-preserving language modeling, providing valuable insights\nand outlining future research directions.", "AI": {"tldr": "该研究全面概述了保护语言模型隐私的方法，探讨了它们的优势和局限性，并为未来研究指明了方向。", "motivation": "语言模型因其在敏感数据上的训练可能在隐私攻击中泄露信息，引发了对个人隐私保护的担忧。尽管隐私保护至关重要，但对语言模型的隐私风险及其缓解方法的理解仍然有限。", "method": "该研究通过对保护隐私的语言模型方法进行全面研究，深入概述了这些方法，并强调了它们的优势和局限性。", "result": "该研究的成果为隐私保护语言模型领域的持续研究做出了贡献，提供了宝贵的见解，并勾勒了未来的研究方向。", "conclusion": "研究结果为隐私保护语言模型提供了深入理解和宝贵见解，并为未来研究指明了方向，以应对语言模型中的隐私挑战。"}}
{"id": "2508.15372", "categories": ["cs.CV", "cs.AI", "cs.GR"], "pdf": "https://arxiv.org/pdf/2508.15372", "abs": "https://arxiv.org/abs/2508.15372", "authors": ["Xinshuang Liu", "Runfa Blark Li", "Keito Suzuki", "Truong Nguyen"], "title": "Image-Conditioned 3D Gaussian Splat Quantization", "comment": null, "summary": "3D Gaussian Splatting (3DGS) has attracted considerable attention for\nenabling high-quality real-time rendering. Although 3DGS compression methods\nhave been proposed for deployment on storage-constrained devices, two\nlimitations hinder archival use: (1) they compress medium-scale scenes only to\nthe megabyte range, which remains impractical for large-scale scenes or\nextensive scene collections; and (2) they lack mechanisms to accommodate scene\nchanges after long-term archival. To address these limitations, we propose an\nImage-Conditioned Gaussian Splat Quantizer (ICGS-Quantizer) that substantially\nenhances compression efficiency and provides adaptability to scene changes\nafter archiving. ICGS-Quantizer improves quantization efficiency by jointly\nexploiting inter-Gaussian and inter-attribute correlations and by using shared\ncodebooks across all training scenes, which are then fixed and applied to\npreviously unseen test scenes, eliminating the overhead of per-scene codebooks.\nThis approach effectively reduces the storage requirements for 3DGS to the\nkilobyte range while preserving visual fidelity. To enable adaptability to\npost-archival scene changes, ICGS-Quantizer conditions scene decoding on images\ncaptured at decoding time. The encoding, quantization, and decoding processes\nare trained jointly, ensuring that the codes, which are quantized\nrepresentations of the scene, are effective for conditional decoding. We\nevaluate ICGS-Quantizer on 3D scene compression and 3D scene updating.\nExperimental results show that ICGS-Quantizer consistently outperforms\nstate-of-the-art methods in compression efficiency and adaptability to scene\nchanges. Our code, model, and data will be publicly available on GitHub.", "AI": {"tldr": "本文提出了一种名为ICGS-Quantizer的方法，旨在将3D Gaussian Splatting（3DGS）场景的存储需求压缩至千字节级别，同时通过图像条件解码实现归档后场景的适应性更新，显著优于现有技术。", "motivation": "尽管3DGS压缩方法已被提出，但它们存在两个局限性：1) 压缩效率不足，对于大规模场景或大量场景集合，压缩后的文件大小（兆字节范围）仍然不切实际；2) 缺乏在长期归档后适应场景变化的能力。", "method": "本文提出了Image-Conditioned Gaussian Splat Quantizer (ICGS-Quantizer)。该方法通过联合利用高斯间和属性间的相关性来提高量化效率，并使用跨所有训练场景共享的固定码本，从而消除了每场景码本的开销。为了实现归档后场景变化的适应性，ICGS-Quantizer将场景解码过程条件化于解码时捕获的图像。编码、量化和解码过程是联合训练的，以确保量化后的场景表示对于条件解码是有效的。", "result": "实验结果表明，ICGS-Quantizer能够将3DGS的存储需求有效降低到千字节范围，同时保持视觉保真度。在3D场景压缩和3D场景更新方面，ICGS-Quantizer始终优于最先进的方法，在压缩效率和对场景变化的适应性方面均表现出色。", "conclusion": "ICGS-Quantizer通过显著提高压缩效率和提供归档后场景变化的适应性，解决了现有3DGS压缩方法的关键局限性，使得3DGS能够更有效地用于大规模场景的归档和更新。"}}
{"id": "2508.15690", "categories": ["cs.AI", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.15690", "abs": "https://arxiv.org/abs/2508.15690", "authors": ["Abhigya Verma", "Sriram Puttagunta", "Seganrasan Subramanian", "Sravan Ramachandran"], "title": "GRAFT: GRaPH and Table Reasoning for Textual Alignment -- A Benchmark for Structured Instruction Following and Visual Reasoning", "comment": "23 pages, 9 tables, 3 figures", "summary": "GRAFT is a structured multimodal benchmark for evaluating models on\ninstruction-following, visual reasoning, and visual-textual alignment tasks. It\nfeatures programmatically generated charts and synthetically rendered tables,\ncreated with Python visualization libraries to ensure control over data\nsemantics, structure, and clarity. Each GRAFT instance pairs a chart or table\nimage with a systematically generated, multi-step analytical question based\nsolely on visual content. Answers are provided in structured formats such as\nJSON or YAML, supporting consistent evaluation of both reasoning and output\nformat. The benchmark introduces a taxonomy of reasoning types including\ncomparison, trend identification, ranking, aggregation, proportion estimation,\nand anomaly detection to enable comprehensive assessment. Reference answers\nfollow strict factual and formatting guidelines for precise, aspect-based\nevaluation. GRAFT offers a unified, scalable framework for fine-grained\nbenchmarking of multimodal models on visually grounded, structured reasoning\ntasks, setting a new evaluation standard in this field.", "AI": {"tldr": "GRAFT是一个结构化的多模态基准测试，通过程序化生成的图表和表格，配合基于视觉内容的多步分析问题和结构化答案，用于评估模型在指令遵循、视觉推理和视觉-文本对齐任务上的表现。", "motivation": "该研究旨在为评估模型在指令遵循、视觉推理和视觉-文本对齐任务上提供一个统一、可扩展且细粒度的基准测试框架，特别是在视觉接地、结构化推理任务方面。", "method": "GRAFT采用Python可视化库程序化生成图表和合成渲染表格，以确保对数据语义、结构和清晰度的控制。每个实例将图表或表格图像与系统生成的多步分析问题配对，问题仅基于视觉内容。答案以JSON或YAML等结构化格式提供，并引入了包括比较、趋势识别、排序、聚合、比例估计和异常检测在内的推理类型分类法。参考答案遵循严格的事实和格式准则。", "result": "GRAFT成功创建了一个结构化的多模态基准，能够对模型的推理能力和输出格式进行一致性评估。它支持对多种推理类型进行全面评估，并提供了一个统一、可扩展的框架，用于对多模态模型进行细粒度基准测试，为该领域的评估树立了新标准。", "conclusion": "GRAFT为评估多模态模型在视觉接地、结构化推理任务上的表现提供了一个全面且可扩展的工具，通过其精心设计的数据生成、问题制定和答案结构，显著提升了该领域的评估标准。"}}
{"id": "2508.15440", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.15440", "abs": "https://arxiv.org/abs/2508.15440", "authors": ["MSVPJ Sathvik", "Zuhair Hasan Shaik", "Vivek Gupta"], "title": "M-HELP: Using Social Media Data to Detect Mental Health Help-Seeking Signals", "comment": "Accepted at Findings of EMNLP 2025", "summary": "Mental health disorders are a global crisis. While various datasets exist for\ndetecting such disorders, there remains a critical gap in identifying\nindividuals actively seeking help. This paper introduces a novel dataset,\nM-Help, specifically designed to detect help-seeking behavior on social media.\nThe dataset goes beyond traditional labels by identifying not only help-seeking\nactivity but also specific mental health disorders and their underlying causes,\nsuch as relationship challenges or financial stressors. AI models trained on\nM-Help can address three key tasks: identifying help-seekers, diagnosing mental\nhealth conditions, and uncovering the root causes of issues.", "AI": {"tldr": "该论文引入了M-Help数据集，旨在从社交媒体上识别精神健康求助行为、诊断具体疾病并揭示其根本原因。", "motivation": "精神健康障碍是一场全球危机，现有数据集在检测精神障碍方面存在，但在识别主动寻求帮助的个体方面存在关键空白。", "method": "引入了一个名为M-Help的新型数据集，该数据集专门用于检测社交媒体上的求助行为，并超越传统标签，识别具体的精神健康障碍及其潜在原因（如人际关系挑战或经济压力）。", "result": "在M-Help上训练的AI模型可以解决三个关键任务：识别求助者、诊断精神健康状况以及揭示问题的根本原因。", "conclusion": "M-Help数据集能够让AI模型识别寻求帮助的个体，诊断精神健康状况，并揭示其根本原因，填补了现有研究在主动求助行为识别方面的空白。"}}
{"id": "2508.15376", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15376", "abs": "https://arxiv.org/abs/2508.15376", "authors": ["Cong Wang", "Xianda Guo", "Wenbo Xu", "Wei Tian", "Ruiqi Song", "Chenming Zhang", "Lingxi Li", "Long Chen"], "title": "DriveSplat: Decoupled Driving Scene Reconstruction with Geometry-enhanced Partitioned Neural Gaussians", "comment": null, "summary": "In the realm of driving scenarios, the presence of rapidly moving vehicles,\npedestrians in motion, and large-scale static backgrounds poses significant\nchallenges for 3D scene reconstruction. Recent methods based on 3D Gaussian\nSplatting address the motion blur problem by decoupling dynamic and static\ncomponents within the scene. However, these decoupling strategies overlook\nbackground optimization with adequate geometry relationships and rely solely on\nfitting each training view by adding Gaussians. Therefore, these models exhibit\nlimited robustness in rendering novel views and lack an accurate geometric\nrepresentation. To address the above issues, we introduce DriveSplat, a\nhigh-quality reconstruction method for driving scenarios based on neural\nGaussian representations with dynamic-static decoupling. To better accommodate\nthe predominantly linear motion patterns of driving viewpoints, a region-wise\nvoxel initialization scheme is employed, which partitions the scene into near,\nmiddle, and far regions to enhance close-range detail representation.\nDeformable neural Gaussians are introduced to model non-rigid dynamic actors,\nwhose parameters are temporally adjusted by a learnable deformation network.\nThe entire framework is further supervised by depth and normal priors from\npre-trained models, improving the accuracy of geometric structures. Our method\nhas been rigorously evaluated on the Waymo and KITTI datasets, demonstrating\nstate-of-the-art performance in novel-view synthesis for driving scenarios.", "AI": {"tldr": "本文提出DriveSplat，一种用于驾驶场景的高质量3D重建方法，通过动态-静态解耦的神经高斯表示，结合区域体素初始化、可变形神经高斯和深度/法线先验监督，解决了现有方法的几何精度和新视角渲染鲁棒性问题。", "motivation": "驾驶场景中快速移动的车辆、行人以及大规模静态背景给3D场景重建带来巨大挑战。现有的基于3D高斯Splatting的方法虽通过动静解耦处理运动模糊，但忽略了背景几何优化，仅依赖高斯拟合训练视图，导致新视角渲染鲁棒性差，缺乏精确几何表示。", "method": "本文引入DriveSplat。主要方法包括：1) 采用区域体素初始化方案，将场景划分为近、中、远区域，以适应驾驶视角的线性运动模式并增强近距离细节；2) 引入可变形神经高斯来建模非刚性动态物体，其参数通过可学习的形变网络进行时间调整；3) 整个框架通过预训练模型的深度和法线先验进行监督，以提高几何结构的准确性。", "result": "DriveSplat在Waymo和KITTI数据集上进行了严格评估，在新视角合成方面展现出最先进的性能。", "conclusion": "DriveSplat通过其创新的动态-静态解耦神经高斯表示、区域初始化、可变形高斯以及几何先验监督，成功解决了驾驶场景3D重建中的几何精度和新视角渲染鲁棒性问题，实现了高质量的驾驶场景新视角合成。"}}
{"id": "2508.15693", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15693", "abs": "https://arxiv.org/abs/2508.15693", "authors": ["Wilka Carvalho", "Vikram Goddla", "Ishaan Sinha", "Hoon Shin", "Kunal Jha"], "title": "NiceWebRL: a Python library for human subject experiments with reinforcement learning environments", "comment": null, "summary": "We present NiceWebRL, a research tool that enables researchers to use machine\nreinforcement learning (RL) environments for online human subject experiments.\nNiceWebRL is a Python library that allows any Jax-based environment to be\ntransformed into an online interface, supporting both single-agent and\nmulti-agent environments. As such, NiceWebRL enables AI researchers to compare\ntheir algorithms to human performance, cognitive scientists to test ML\nalgorithms as theories for human cognition, and multi-agent researchers to\ndevelop algorithms for human-AI collaboration. We showcase NiceWebRL with 3\ncase studies that demonstrate its potential to help develop Human-like AI,\nHuman-compatible AI, and Human-assistive AI. In the first case study\n(Human-like AI), NiceWebRL enables the development of a novel RL model of\ncognition. Here, NiceWebRL facilitates testing this model against human\nparticipants in both a grid world and Craftax, a 2D Minecraft domain. In our\nsecond case study (Human-compatible AI), NiceWebRL enables the development of a\nnovel multi-agent RL algorithm that can generalize to human partners in the\nOvercooked domain. Finally, in our third case study (Human-assistive AI), we\nshow how NiceWebRL can allow researchers to study how an LLM can assist humans\non complex tasks in XLand-Minigrid, an environment with millions of\nhierarchical tasks. The library is available at\nhttps://github.com/KempnerInstitute/nicewebrl.", "AI": {"tldr": "NiceWebRL是一个Python库，可以将任何基于Jax的强化学习环境转换为在线接口，用于进行人类受试者实验，以研究人机交互和人类认知。", "motivation": "研究人员需要一个工具，能够在线使用机器学习强化学习环境进行人类受试者实验，从而比较AI算法与人类表现，将机器学习算法作为人类认知理论进行测试，并为人类-AI协作开发算法。", "method": "NiceWebRL是一个Python库，它能将任何基于Jax的强化学习环境转换为在线接口，支持单智能体和多智能体环境。该工具通过3个案例研究进行展示。", "result": "通过3个案例研究展示了其潜力：1) 在“类人AI”中，开发了新颖的认知RL模型，并在网格世界和Craftax中与人类参与者进行测试。2) 在“人机兼容AI”中，开发了多智能体RL算法，能在Overcooked领域泛化到人类伙伴。3) 在“人机辅助AI”中，展示了如何研究LLM在XLand-Minigrid复杂任务中协助人类。", "conclusion": "NiceWebRL通过促进在线人机实验，在开发类人AI、人机兼容AI和人机辅助AI方面具有巨大潜力。"}}
{"id": "2508.15453", "categories": ["cs.CL", "Translating nonequivalent words"], "pdf": "https://arxiv.org/pdf/2508.15453", "abs": "https://arxiv.org/abs/2508.15453", "authors": ["Mohammad Ibrahim Qani"], "title": "Principle Methods of Rendering Non-equivalent Words from Uzbek and Dari to Russian and English", "comment": "Fully abstract is available in the attached file", "summary": "These pure languages understanding directly relates to translation knowledge\nwhere linguists and translators need to work and research to eradicate\nmisunderstanding. Misunderstandings mostly appear in non-equivalent words\nbecause there are different local and internal words like food, garment,\ncultural and traditional words and others in every notion. Truly, most of these\nwords do not have equivalent in the target language and these words need to be\nworked and find their equivalent in the target language to fully understand the\nboth languages. The purpose of this research is to introduce the methods of\nrendering non-equivalent words professionally from the source language to the\ntarget language and this research has been completed using library-based\nresearch. However, some of these non-equivalent words are already\nprofessionally rendered to the target language but still there many other words\nto be rendered. As a result, this research paper includes different ways and\nrules of rendering non-equivalent words from source language to the target\nlanguage and 25 non-equvalent words have been rendered from Dar & Uzbek into\nEnglish and Russian languages.", "AI": {"tldr": "本研究旨在探讨并提供专业地将源语言中的非对等词翻译到目标语言的方法和规则。", "motivation": "翻译中常因食物、服装、文化和传统等领域的非对等词导致误解，这些词在目标语言中往往没有直接对应，需要专门处理以实现充分理解。", "method": "本研究采用基于图书馆的文献研究方法完成。", "result": "研究提出了将非对等词从源语言翻译到目标语言的不同方法和规则，并实际将25个达里语和乌兹别克语的非对等词翻译成了英语和俄语。", "conclusion": "本研究提供了专业处理非对等词的有效方法和规则，强调了尽管部分非对等词已被处理，但仍有大量此类词汇需要进一步研究和翻译。"}}
{"id": "2508.15387", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15387", "abs": "https://arxiv.org/abs/2508.15387", "authors": ["Ruizhuo Song", "Beiming Yuan"], "title": "DIO: Refining Mutual Information and Causal Chain to Enhance Machine Abstract Reasoning Ability", "comment": "15 pages, 9 figures, 8 tables", "summary": "Despite the outstanding performance of current deep learning models across\nvarious domains, their fundamental bottleneck in abstract reasoning remains\nunresolved. To address this challenge, the academic community has introduced\nRaven's Progressive Matrices (RPM) problems as an authoritative benchmark for\nevaluating the abstract reasoning capabilities of deep learning algorithms,\nwith a focus on core intelligence dimensions such as abstract reasoning,\npattern recognition, and complex problem-solving. Therefore, this paper centers\non solving RPM problems, aiming to contribute to enhancing the abstract\nreasoning abilities of machine intelligence. Firstly, this paper adopts a\n``causal chain modeling'' perspective to systematically analyze the complete\ncausal chain in RPM tasks: image $\\rightarrow$ abstract attributes\n$\\rightarrow$ progressive attribute patterns $\\rightarrow$ pattern consistency\n$\\rightarrow$ correct answer. Based on this analysis, the network architecture\nof the baseline model DIO is designed. However, experiments reveal that the\noptimization objective formulated for DIO, namely maximizing the variational\nlower bound of mutual information between the context and the correct option,\nfails to enable the model to genuinely acquire the predefined human reasoning\nlogic. This is attributed to two main reasons: the tightness of the lower bound\nsignificantly impacts the effectiveness of mutual information maximization, and\nmutual information, as a statistical measure, does not capture the causal\nrelationship between subjects and objects. To overcome these limitations, this\npaper progressively proposes three improvement methods:", "AI": {"tldr": "本文旨在通过解决瑞文渐进矩阵 (RPM) 问题来增强机器的抽象推理能力。论文首先采用“因果链建模”分析RPM任务，并设计了基线模型DIO。然而，实验发现DIO的互信息最大化目标未能有效捕获人类推理逻辑，原因在于下界紧密性不足和互信息的统计性。为克服这些限制，论文提出了三种改进方法。", "motivation": "当前深度学习模型在抽象推理方面存在根本性瓶颈。瑞文渐进矩阵 (RPM) 问题是评估深度学习算法抽象推理能力、模式识别和复杂问题解决能力的重要基准。因此，本研究旨在通过解决RPM问题来提升机器智能的抽象推理能力。", "method": "1. 采用“因果链建模”视角，系统分析RPM任务中的完整因果链：图像 → 抽象属性 → 渐进属性模式 → 模式一致性 → 正确答案。\n2. 基于上述分析，设计了基线模型DIO的网络架构。\n3. DIO的优化目标是最大化上下文与正确选项之间互信息的变分下界。\n4. 针对DIO的局限性（下界紧密性影响互信息最大化，互信息作为统计度量未能捕获因果关系），提出了三种改进方法。", "result": "实验表明，为DIO制定的优化目标（最大化上下文与正确选项之间互信息的变分下界）未能使模型真正获得预定义的人类推理逻辑。这主要归因于两个原因：下界的紧密性显著影响互信息最大化的有效性；互信息作为统计度量，未能捕获主体与客体之间的因果关系。", "conclusion": "尽管通过因果链建模分析设计了基线模型DIO，但基于互信息最大化的优化目标未能有效使模型习得人类抽象推理逻辑。这暴露了当前方法在互信息下界紧密性和因果关系捕获方面的局限性。为解决这些问题，论文提出了进一步的改进方法。"}}
{"id": "2508.15734", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15734", "abs": "https://arxiv.org/abs/2508.15734", "authors": ["Cooper Elsworth", "Keguo Huang", "David Patterson", "Ian Schneider", "Robert Sedivy", "Savannah Goodman", "Ben Townsend", "Parthasarathy Ranganathan", "Jeff Dean", "Amin Vahdat", "Ben Gomes", "James Manyika"], "title": "Measuring the environmental impact of delivering AI at Google Scale", "comment": null, "summary": "The transformative power of AI is undeniable - but as user adoption\naccelerates, so does the need to understand and mitigate the environmental\nimpact of AI serving. However, no studies have measured AI serving\nenvironmental metrics in a production environment. This paper addresses this\ngap by proposing and executing a comprehensive methodology for measuring the\nenergy usage, carbon emissions, and water consumption of AI inference workloads\nin a large-scale, AI production environment. Our approach accounts for the full\nstack of AI serving infrastructure - including active AI accelerator power,\nhost system energy, idle machine capacity, and data center energy overhead.\nThrough detailed instrumentation of Google's AI infrastructure for serving the\nGemini AI assistant, we find the median Gemini Apps text prompt consumes 0.24\nWh of energy - a figure substantially lower than many public estimates. We also\nshow that Google's software efficiency efforts and clean energy procurement\nhave driven a 33x reduction in energy consumption and a 44x reduction in carbon\nfootprint for the median Gemini Apps text prompt over one year. We identify\nthat the median Gemini Apps text prompt uses less energy than watching nine\nseconds of television (0.24 Wh) and consumes the equivalent of five drops of\nwater (0.26 mL). While these impacts are low compared to other daily\nactivities, reducing the environmental impact of AI serving continues to\nwarrant important attention. Towards this objective, we propose that a\ncomprehensive measurement of AI serving environmental metrics is critical for\naccurately comparing models, and to properly incentivize efficiency gains\nacross the full AI serving stack.", "AI": {"tldr": "本研究首次在生产环境中测量了AI服务（以Google Gemini为例）的能耗、碳排放和水消耗。结果显示，其环境影响远低于公共估计，且Google的效率提升和清洁能源采购显著降低了这些影响。研究强调全面测量对于持续优化AI环境足迹的重要性。", "motivation": "随着AI用户采用的加速，了解并减轻AI服务的环境影响变得日益重要。然而，此前尚无研究在生产环境中测量AI服务的环境指标，这构成了一个研究空白。", "method": "研究提出并执行了一套全面的方法，用于测量大规模AI生产环境中AI推理工作负载的能耗、碳排放和水消耗。该方法考虑了AI服务基础设施的完整堆栈，包括活跃的AI加速器功率、主机系统能耗、空闲机器容量和数据中心能源开销。通过对Google服务Gemini AI助手的AI基础设施进行详细的仪器测量实现。", "result": "研究发现，Gemini Apps文本提示的平均能耗为0.24 Wh，远低于许多公共估计。Google的软件效率提升和清洁能源采购在一年内使平均Gemini Apps文本提示的能耗降低了33倍，碳足迹降低了44倍。一个文本提示的能耗低于观看9秒电视（0.24 Wh），水消耗相当于5滴水（0.26 mL）。", "conclusion": "尽管AI服务对环境的影响与其他日常活动相比相对较低，但持续关注并减少其环境影响仍然至关重要。研究提出，全面测量AI服务的环境指标对于准确比较模型以及激励整个AI服务堆栈的效率提升至关重要。"}}
{"id": "2508.15456", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.15456", "abs": "https://arxiv.org/abs/2508.15456", "authors": ["Alexandru Coca", "Bo-Hsiang Tseng", "Pete Boothroyd", "Jianpeng Cheng", "Mark Gaynor", "Zhenxing Zhang", "Joe Stacey", "Tristan Guigue", "Héctor Martinez Alonso", "Diarmuid Ó Séaghdha", "Anders Johannsen"], "title": "PyTOD: Programmable Task-Oriented Dialogue with Execution Feedback", "comment": "20 pages, 12 figures. To appear at SIGDIAL 2025", "summary": "Programmable task-oriented dialogue (TOD) agents enable language models to\nfollow structured dialogue policies, but their effectiveness hinges on accurate\nstate tracking. We present PyTOD, an agent that generates executable code to\ntrack dialogue state and uses policy and execution feedback for efficient error\ncorrection. To this end, PyTOD employs a simple constrained decoding approach,\nusing a language model instead of grammar rules to follow API schemata. This\nleads to state-of-the-art state tracking performance on the challenging SGD\nbenchmark. Our experiments show that PyTOD surpasses strong baselines in both\naccuracy and robust user goal estimation as the dialogue progresses,\ndemonstrating the effectiveness of execution-aware state tracking.", "AI": {"tldr": "PyTOD是一种通过生成可执行代码来追踪对话状态的对话代理，利用策略和执行反馈进行错误纠正，并在SGD基准测试上实现了最先进的状态追踪性能。", "motivation": "可编程面向任务对话（TOD）代理的有效性取决于准确的状态追踪，但这一过程具有挑战性。", "method": "PyTOD通过生成可执行代码来追踪对话状态，并利用策略和执行反馈进行高效的错误纠正。它采用了一种简单的约束解码方法，使用语言模型而非语法规则来遵循API模式。", "result": "PyTOD在具有挑战性的SGD基准测试上实现了最先进的状态追踪性能。实验表明，随着对话的进行，PyTOD在准确性和鲁棒的用户目标估计方面均超越了强大的基线模型。", "conclusion": "执行感知型状态追踪是有效的，PyTOD证明了其在对话状态追踪中的优越性。"}}
{"id": "2508.15389", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15389", "abs": "https://arxiv.org/abs/2508.15389", "authors": ["Wenrui Li", "Wei Han", "Liang-Jian Deng", "Ruiqin Xiong", "Xiaopeng Fan"], "title": "Spiking Variational Graph Representation Inference for Video Summarization", "comment": "Accepted by IEEE TIP", "summary": "With the rise of short video content, efficient video summarization\ntechniques for extracting key information have become crucial. However,\nexisting methods struggle to capture the global temporal dependencies and\nmaintain the semantic coherence of video content. Additionally, these methods\nare also influenced by noise during multi-channel feature fusion. We propose a\nSpiking Variational Graph (SpiVG) Network, which enhances information density\nand reduces computational complexity. First, we design a keyframe extractor\nbased on Spiking Neural Networks (SNN), leveraging the event-driven computation\nmechanism of SNNs to learn keyframe features autonomously. To enable\nfine-grained and adaptable reasoning across video frames, we introduce a\nDynamic Aggregation Graph Reasoner, which decouples contextual object\nconsistency from semantic perspective coherence. We present a Variational\nInference Reconstruction Module to address uncertainty and noise arising during\nmulti-channel feature fusion. In this module, we employ Evidence Lower Bound\nOptimization (ELBO) to capture the latent structure of multi-channel feature\ndistributions, using posterior distribution regularization to reduce\noverfitting. Experimental results show that SpiVG surpasses existing methods\nacross multiple datasets such as SumMe, TVSum, VideoXum, and QFVS. Our codes\nand pre-trained models are available at https://github.com/liwrui/SpiVG.", "AI": {"tldr": "本文提出了一种名为SpiVG的脉冲变分图网络，利用脉冲神经网络（SNN）进行关键帧提取，并结合动态聚合图推理器和变分推理重建模块，有效解决了现有视频摘要方法在全局时间依赖、语义连贯性和多通道特征融合噪声方面的挑战。", "motivation": "现有视频摘要方法难以捕捉全局时间依赖性并维持语义连贯性，同时在多通道特征融合过程中容易受到噪声影响，导致信息密度不足和计算复杂性高。", "method": "本文提出了SpiVG网络，包含三个核心组件：1) 基于脉冲神经网络（SNN）的关键帧提取器，利用SNN的事件驱动计算机制自主学习关键帧特征；2) 动态聚合图推理器，用于解耦上下文对象一致性和语义视角连贯性，实现视频帧间的细粒度自适应推理；3) 变分推理重建模块，采用证据下界优化（ELBO）捕捉多通道特征分布的潜在结构，并通过后验分布正则化减少过拟合，以处理多通道特征融合中的不确定性和噪声。", "result": "实验结果表明，SpiVG在SumMe、TVSum、VideoXum和QFVS等多个数据集上均超越了现有方法。", "conclusion": "SpiVG网络通过结合SNN、动态图推理和变分推理，有效提升了视频摘要的信息密度，降低了计算复杂性，并在多个数据集上取得了优于现有方法的性能，解决了视频摘要中的关键挑战。"}}
{"id": "2508.15748", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15748", "abs": "https://arxiv.org/abs/2508.15748", "authors": ["Emma Rath", "Stuart Armstrong", "Rebecca Gorman"], "title": "Response and Prompt Evaluation to Prevent Parasocial Relationships with Chatbots", "comment": null, "summary": "The development of parasocial relationships with AI agents has severe, and in\nsome cases, tragic effects for human well-being. Yet preventing such dynamics\nis challenging: parasocial cues often emerge gradually in private\nconversations, and not all forms of emotional engagement are inherently\nharmful. We address this challenge by introducing a simple response evaluation\nframework, created by repurposing a state-of-the-art language model, that\nevaluates ongoing conversations for parasocial cues in real time. To test the\nfeasibility of this approach, we constructed a small synthetic dataset of\nthirty dialogues spanning parasocial, sycophantic, and neutral conversations.\nIterative evaluation with five stage testing successfully identified all\nparasocial conversations while avoiding false positives under a tolerant\nunanimity rule, with detection typically occurring within the first few\nexchanges. These findings provide preliminary evidence that evaluation agents\ncan provide a viable solution for the prevention of parasocial relations.", "AI": {"tldr": "本研究提出了一种利用大型语言模型实时评估对话的框架，旨在早期识别与AI代理建立的有害拟社会关系，并在合成数据集上取得了成功。", "motivation": "与AI代理建立的拟社会关系可能对人类福祉产生严重甚至悲剧性的影响。然而，由于拟社会线索常在私人对话中逐渐显现，且并非所有情感投入都有害，因此预防这些动态极具挑战性。", "method": "通过重新利用一个最先进的语言模型，创建了一个简单的响应评估框架，用于实时评估对话中的拟社会线索。构建了一个包含30个对话（拟社会、奉承和中性）的小型合成数据集，并采用五阶段迭代评估进行测试。", "result": "在容忍一致性规则下，该方法成功识别了所有拟社会对话，同时避免了误报，且检测通常在最初的几次交流中即可发生。", "conclusion": "这些初步发现表明，评估代理可以为预防与AI代理建立拟社会关系提供一个可行的解决方案。"}}
{"id": "2508.15464", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15464", "abs": "https://arxiv.org/abs/2508.15464", "authors": ["Yingshu Li", "Yunyi Liu", "Lingqiao Liu", "Lei Wang", "Luping Zhou"], "title": "RadReason: Radiology Report Evaluation Metric with Reasons and Sub-Scores", "comment": null, "summary": "Evaluating automatically generated radiology reports remains a fundamental\nchallenge due to the lack of clinically grounded, interpretable, and\nfine-grained metrics. Existing methods either produce coarse overall scores or\nrely on opaque black-box models, limiting their usefulness in real-world\nclinical workflows. We introduce RadReason, a novel evaluation framework for\nradiology reports that not only outputs fine-grained sub-scores across six\nclinically defined error types, but also produces human-readable justifications\nthat explain the rationale behind each score. Our method builds on Group\nRelative Policy Optimization and incorporates two key innovations: (1)\nSub-score Dynamic Weighting, which adaptively prioritizes clinically\nchallenging error types based on live F1 statistics; and (2) Majority-Guided\nAdvantage Scaling, which adjusts policy gradient updates based on prompt\ndifficulty derived from sub-score agreement. Together, these components enable\nmore stable optimization and better alignment with expert clinical judgment.\nExperiments on the ReXVal benchmark show that RadReason surpasses all prior\noffline metrics and achieves parity with GPT-4-based evaluations, while\nremaining explainable, cost-efficient, and suitable for clinical deployment.\nCode will be released upon publication.", "AI": {"tldr": "RadReason是一个新颖的放射学报告评估框架，它提供细粒度的临床错误类型子分数和人类可读的理由，解决了现有评估方法缺乏临床依据和可解释性的问题。", "motivation": "自动生成的放射学报告评估面临根本性挑战，因为缺乏基于临床、可解释和细粒度的指标。现有方法要么产生粗略的总体分数，要么依赖不透明的黑盒模型，限制了它们在真实临床工作流程中的实用性。", "method": "RadReason框架基于Group Relative Policy Optimization，并包含两项关键创新：(1) 子分数动态加权（Sub-score Dynamic Weighting），根据实时F1统计数据自适应地优先处理临床上具有挑战性的错误类型；(2) 多数引导优势缩放（Majority-Guided Advantage Scaling），根据子分数一致性得出的提示难度调整策略梯度更新。这些组件共同实现了更稳定的优化和更好地与专家临床判断对齐。", "result": "在ReXVal基准测试上的实验表明，RadReason超越了所有先前的离线指标，并达到了与基于GPT-4的评估相当的水平，同时保持了可解释性、成本效益，并适用于临床部署。", "conclusion": "RadReason提供了一种有效、可解释且成本高效的放射学报告评估方法，其细粒度的分数和理由使其非常适合临床部署，并有望改进放射学报告的自动生成和评估。"}}
{"id": "2508.15404", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15404", "abs": "https://arxiv.org/abs/2508.15404", "authors": ["Anthony Bisulco", "Rahul Ramesh", "Randall Balestriero", "Pratik Chaudhari"], "title": "From Linearity to Non-Linearity: How Masked Autoencoders Capture Spatial Correlations", "comment": null, "summary": "Masked Autoencoders (MAEs) have emerged as a powerful pretraining technique\nfor vision foundation models. Despite their effectiveness, they require\nextensive hyperparameter tuning (masking ratio, patch size, encoder/decoder\nlayers) when applied to novel datasets. While prior theoretical works have\nanalyzed MAEs in terms of their attention patterns and hierarchical latent\nvariable models, the connection between MAE hyperparameters and performance on\ndownstream tasks is relatively unexplored. This work investigates how MAEs\nlearn spatial correlations in the input image. We analytically derive the\nfeatures learned by a linear MAE and show that masking ratio and patch size can\nbe used to select for features that capture short- and long-range spatial\ncorrelations. We extend this analysis to non-linear MAEs to show that MAE\nrepresentations adapt to spatial correlations in the dataset, beyond\nsecond-order statistics. Finally, we discuss some insights on how to select MAE\nhyper-parameters in practice.", "AI": {"tldr": "本文通过理论分析，揭示了MAE超参数（如遮蔽率和补丁大小）如何影响其学习图像空间相关性的能力，并为实际的超参数选择提供了指导。", "motivation": "尽管MAE是一种强大的视觉基础模型预训练技术，但在应用于新数据集时，需要进行大量的超参数调优。现有理论研究尚未充分探索MAE超参数与下游任务性能之间的具体联系。", "method": "研究首先分析推导了线性MAE学习到的特征，然后将此分析扩展到非线性MAE，以探究其表示如何适应数据集中的空间相关性。", "result": "研究表明，线性MAE的遮蔽率和补丁大小可以用于选择捕获短程和长程空间相关性的特征。此外，非线性MAE的表示能够适应数据集中超越二阶统计量的空间相关性。", "conclusion": "本研究深入理解了MAE如何学习空间相关性以及超参数对其影响，并为在实践中选择MAE超参数提供了有价值的见解。"}}
{"id": "2508.15757", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.15757", "abs": "https://arxiv.org/abs/2508.15757", "authors": ["Yuxing Lu", "Yucheng Hu", "Nan Sun", "Xukai Zhao"], "title": "Language-Guided Tuning: Enhancing Numeric Optimization with Textual Feedback", "comment": "9 pages, 4 figures, 4 tables", "summary": "Configuration optimization remains a critical bottleneck in machine learning,\nrequiring coordinated tuning across model architecture, training strategy,\nfeature engineering, and hyperparameters. Traditional approaches treat these\ndimensions independently and lack interpretability, while recent automated\nmethods struggle with dynamic adaptability and semantic reasoning about\noptimization decisions. We introduce Language-Guided Tuning (LGT), a novel\nframework that employs multi-agent Large Language Models to intelligently\noptimize configurations through natural language reasoning. We apply textual\ngradients - qualitative feedback signals that complement numerical optimization\nby providing semantic understanding of training dynamics and configuration\ninterdependencies. LGT coordinates three specialized agents: an Advisor that\nproposes configuration changes, an Evaluator that assesses progress, and an\nOptimizer that refines the decision-making process, creating a self-improving\nfeedback loop. Through comprehensive evaluation on six diverse datasets, LGT\ndemonstrates substantial improvements over traditional optimization methods,\nachieving performance gains while maintaining high interpretability.", "AI": {"tldr": "本文提出了语言引导调优（LGT）框架，利用多智能体大型语言模型（LLM）通过自然语言推理和文本梯度，智能优化机器学习配置。", "motivation": "机器学习配置优化（包括模型架构、训练策略、特征工程和超参数）是关键瓶颈。传统方法独立且缺乏可解释性，而现有自动化方法在动态适应性和优化决策的语义推理方面存在不足。", "method": "LGT框架采用多智能体LLM，通过自然语言推理智能优化配置。它引入了“文本梯度”——提供训练动态和配置相互依赖性语义理解的定性反馈信号。LGT协调三个专业智能体：提出配置更改的“顾问”、评估进展的“评估者”和完善决策过程的“优化器”，从而创建一个自改进的反馈循环。", "result": "通过对六个不同数据集的综合评估，LGT展现出优于传统优化方法的显著改进，在实现性能提升的同时保持了高可解释性。", "conclusion": "LGT是一个通过自然语言推理和文本梯度，利用多智能体LLM智能优化机器学习配置的有效框架，解决了现有方法在适应性、语义推理和可解释性方面的局限性。"}}
{"id": "2508.15471", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.15471", "abs": "https://arxiv.org/abs/2508.15471", "authors": ["Vedasamhitha Challapalli", "Konduru Venkat Sai", "Piyush Pratap Singh", "Rupesh Prasad", "Arvind Maurya", "Atul Singh"], "title": "SLM4Offer: Personalized Marketing Offer Generation Using Contrastive Learning Based Fine-Tuning", "comment": "10 pages, BDA Conference 2025", "summary": "Personalized marketing has emerged as a pivotal strategy for enhancing\ncustomer engagement and driving business growth. Academic and industry efforts\nhave predominantly focused on recommendation systems and personalized\nadvertisements. Nonetheless, this facet of personalization holds significant\npotential for increasing conversion rates and improving customer satisfaction.\nPrior studies suggest that well-executed personalization strategies can boost\nrevenue by up to 40 percent, underscoring the strategic importance of\ndeveloping intelligent, data-driven approaches for offer generation. This work\nintroduces SLM4Offer, a generative AI model for personalized offer generation,\ndeveloped by fine-tuning a pre-trained encoder-decoder language model,\nspecifically Google's Text-to-Text Transfer Transformer (T5-Small 60M) using a\ncontrastive learning approach. SLM4Offer employs InfoNCE (Information\nNoise-Contrastive Estimation) loss to align customer personas with relevant\noffers in a shared embedding space. A key innovation in SLM4Offer lies in the\nadaptive learning behaviour introduced by contrastive loss, which reshapes the\nlatent space during training and enhances the model's generalizability. The\nmodel is fine-tuned and evaluated on a synthetic dataset designed to simulate\ncustomer behaviour and offer acceptance patterns. Experimental results\ndemonstrate a 17 percent improvement in offer acceptance rate over a supervised\nfine-tuning baseline, highlighting the effectiveness of contrastive objectives\nin advancing personalized marketing.", "AI": {"tldr": "本文提出SLM4Offer，一个基于T5-Small并结合对比学习（InfoNCE损失）的生成式AI模型，用于个性化优惠生成，旨在通过将客户画像与优惠对齐，提高优惠接受率。", "motivation": "个性化营销对提升客户参与度和业务增长至关重要。现有研究主要集中在推荐系统和个性化广告，而个性化优惠生成在提高转化率和客户满意度方面潜力巨大（可提升收入高达40%）。因此，需要开发智能、数据驱动的方法来生成优惠。", "method": "本文引入SLM4Offer模型，通过使用对比学习方法（InfoNCE损失）微调预训练的编码器-解码器语言模型（Google的T5-Small 60M）来生成个性化优惠。InfoNCE损失用于在共享嵌入空间中对齐客户画像与相关优惠。SLM4Offer的关键创新在于对比损失引入的自适应学习行为，其在训练期间重塑潜在空间并增强模型泛化能力。模型在一个模拟客户行为和优惠接受模式的合成数据集上进行微调和评估。", "result": "实验结果表明，SLM4Offer在优惠接受率方面比监督微调基线提高了17%。", "conclusion": "对比目标在推进个性化营销中表现出显著效果，尤其在个性化优惠生成方面，能有效提高优惠接受率。"}}
{"id": "2508.15415", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15415", "abs": "https://arxiv.org/abs/2508.15415", "authors": ["Dengyan Luo", "Yanping Xiang", "Hu Wang", "Luping Ji. Shuai Li", "Mao Ye"], "title": "Bidirectional Temporal Information Propagation for Moving Infrared Small Target Detection", "comment": null, "summary": "Moving infrared small target detection is broadly adopted in infrared search\nand track systems, and has attracted considerable research focus in recent\nyears. The existing learning-based multi-frame methods mainly aggregate the\ninformation of adjacent frames in a sliding window fashion to assist the\ndetection of the current frame. However, the sliding-window-based methods do\nnot consider joint optimization of the entire video clip and ignore the global\ntemporal information outside the sliding window, resulting in redundant\ncomputation and sub-optimal performance. In this paper, we propose a\nBidirectional temporal information propagation method for moving InfraRed small\ntarget Detection, dubbed BIRD. The bidirectional propagation strategy\nsimultaneously utilizes local temporal information of adjacent frames and\nglobal temporal information of past and future frames in a recursive fashion.\nSpecifically, in the forward and backward propagation branches, we first design\na Local Temporal Motion Fusion (LTMF) module to model local spatio-temporal\ndependency between a target frame and its two adjacent frames. Then, a Global\nTemporal Motion Fusion (GTMF) module is developed to further aggregate the\nglobal propagation feature with the local fusion feature. Finally, the\nbidirectional aggregated features are fused and input into the detection head\nfor detection. In addition, the entire video clip is jointly optimized by the\ntraditional detection loss and the additional Spatio-Temporal Fusion (STF)\nloss. Extensive experiments demonstrate that the proposed BIRD method not only\nachieves the state-of-the-art performance but also shows a fast inference\nspeed.", "AI": {"tldr": "针对红外小目标检测中滑动窗口方法忽略全局时间信息的问题，本文提出BIRD方法，通过双向时间信息传播（局部与全局）联合优化整个视频，实现了最先进的性能和快速推理速度。", "motivation": "现有的基于学习的多帧红外小目标检测方法主要采用滑动窗口方式聚合信息，但这种方式未考虑整个视频片段的联合优化，忽略了滑动窗口之外的全局时间信息，导致计算冗余和次优性能。", "method": "本文提出BIRD（Moving InfraRed small target Detection的双向时间信息传播方法）。该方法采用递归的双向传播策略，同时利用相邻帧的局部时间信息和过去、未来帧的全局时间信息。具体而言，在正向和反向传播分支中，首先设计局部时间运动融合（LTMF）模块建模目标帧与其相邻两帧之间的局部时空依赖；然后开发全局时间运动融合（GTMF）模块进一步聚合全局传播特征与局部融合特征。最后，将双向聚合的特征融合并输入检测头。此外，通过传统检测损失和额外的时空融合（STF）损失共同优化整个视频片段。", "result": "实验结果表明，所提出的BIRD方法不仅取得了最先进的性能，而且展示了快速的推理速度。", "conclusion": "BIRD方法通过创新的双向时间信息传播策略，有效解决了传统滑动窗口方法在红外小目标检测中存在的局限性，实现了性能和效率的显著提升。"}}
{"id": "2507.23341", "categories": ["cs.CV", "cs.AI", "68T45, 68T07", "I.4.8; I.4.9; I.5.4"], "pdf": "https://arxiv.org/pdf/2507.23341", "abs": "https://arxiv.org/abs/2507.23341", "authors": ["Ahmet Can Ömercikoğlu", "Mustafa Mansur Yönügül", "Pakize Erdoğmuş"], "title": "The Impact of Image Resolution on Face Detection: A Comparative Analysis of MTCNN, YOLOv XI and YOLOv XII models", "comment": "6 pages, 5 figures, 4 tables", "summary": "Face detection is a crucial component in many AI-driven applications such as\nsurveillance, biometric authentication, and human-computer interaction.\nHowever, real-world conditions like low-resolution imagery present significant\nchallenges that degrade detection performance. In this study, we systematically\ninvestigate the impact of input resolution on the accuracy and robustness of\nthree prominent deep learning-based face detectors: YOLOv11, YOLOv12, and\nMTCNN. Using the WIDER FACE dataset, we conduct extensive evaluations across\nmultiple image resolutions (160x160, 320x320, and 640x640) and assess each\nmodel's performance using metrics such as precision, recall, mAP50, mAP50-95,\nand inference time. Results indicate that YOLOv11 outperforms YOLOv12 and MTCNN\nin terms of detection accuracy, especially at higher resolutions, while YOLOv12\nexhibits slightly better recall. MTCNN, although competitive in landmark\nlocalization, lags in real-time inference speed. Our findings provide\nactionable insights for selecting resolution-aware face detection models\nsuitable for varying operational constraints.", "AI": {"tldr": "本研究系统评估了输入分辨率对YOLOv11、YOLOv12和MTCNN三种人脸检测器性能的影响。结果显示YOLOv11在准确性上表现最佳，尤其在高分辨率下；YOLOv12召回率稍好；MTCNN实时推理速度较慢。", "motivation": "人脸检测是许多AI应用的关键组成部分，但低分辨率图像等真实世界条件会显著降低检测性能，构成重大挑战。", "method": "研究系统地调查了输入分辨率对YOLOv11、YOLOv12和MTCNN三种深度学习人脸检测器准确性和鲁棒性的影响。使用WIDER FACE数据集，在多种图像分辨率（160x160、320x320和640x640）下进行广泛评估，并使用精确度、召回率、mAP50、mAP50-95和推理时间等指标衡量模型性能。", "result": "结果表明，YOLOv11在检测准确性方面优于YOLOv12和MTCNN，尤其是在较高分辨率下；YOLOv12展现出稍好的召回率；MTCNN虽然在关键点定位方面具有竞争力，但在实时推理速度上落后。", "conclusion": "本研究结果为根据不同操作约束选择对分辨率敏感的人脸检测模型提供了可操作的见解。"}}
{"id": "2508.15474", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15474", "abs": "https://arxiv.org/abs/2508.15474", "authors": ["Sai Sundaresan", "Harshita Chopra", "Atanu R. Sinha", "Koustava Goswami", "Nagasai Saketh Naidu", "Raghav Karan", "N Anushka"], "title": "Subjective Behaviors and Preferences in LLM: Language of Browsing", "comment": "Accepted at EMNLP 2025", "summary": "A Large Language Model (LLM) offers versatility across domains and tasks,\npurportedly benefiting users with a wide variety of behaviors and preferences.\nWe question this perception about an LLM when users have inherently subjective\nbehaviors and preferences, as seen in their ubiquitous and idiosyncratic\nbrowsing of websites or apps. The sequential behavior logs of pages, thus\ngenerated, form something akin to each user's self-constructed \"language\",\nalbeit without the structure and grammar imbued in natural languages. We ask:\n(i) Can a small LM represent the \"language of browsing\" better than a large LM?\n(ii) Can an LM with a single set of parameters (or, single LM) adequately\ncapture myriad users' heterogeneous, subjective behaviors and preferences?\n(iii) Can a single LM with high average performance, yield low variance in\nperformance to make alignment good at user level? We introduce clusterwise LM\ntraining, HeTLM (Heterogeneity aware Training of Language Model), appropriate\nfor subjective behaviors. We find that (i) a small LM trained using a\npage-level tokenizer outperforms large pretrained or finetuned LMs; (ii) HeTLM\nwith heterogeneous cluster specific set of parameters outperforms a single LM\nof the same family, controlling for the number of parameters; and (iii) a\nhigher mean and a lower variance in generation ensues, implying improved\nalignment.", "AI": {"tldr": "本文质疑大型语言模型（LLM）在处理用户主观行为（如浏览习惯）时的普适性。研究提出了一种名为HeTLM的聚类式语言模型训练方法，发现使用页面级分词器训练的小型LM以及具有异构聚类特定参数的HeTLM，在捕捉用户主观行为方面优于大型单一LM，并能实现更好的用户级别对齐。", "motivation": "尽管大型语言模型（LLM）被认为在多领域和多任务中具有通用性并能满足用户多样化的行为和偏好，但当用户行为和偏好具有固有的主观性和异质性（例如，用户浏览网站或应用程序的独特习惯）时，这种看法受到了质疑。用户的顺序浏览日志形成了类似“浏览语言”的数据，但缺乏自然语言的结构。因此，研究旨在探讨：(i) 小型LM能否比大型LM更好地表示“浏览语言”？(ii) 具有单一参数集的LM能否充分捕捉大量用户异构、主观的行为和偏好？(iii) 具有高平均性能的单一LM能否产生低性能方差，从而在用户层面实现良好的对齐？", "method": "研究引入了适用于主观行为的“聚类式LM训练”方法，命名为HeTLM（异质性感知语言模型训练）。具体方法包括：(i) 使用页面级分词器训练小型语言模型；(ii) 将HeTLM与具有异构聚类特定参数的单一LM进行比较，同时控制参数数量。", "result": "(i) 使用页面级分词器训练的小型LM优于大型预训练或微调的LM；(ii) 具有异构聚类特定参数的HeTLM优于相同家族的单一LM（在控制参数数量的情况下）；(iii) 结果显示生成性能的平均值更高，方差更低，这意味着对齐效果有所改善。", "conclusion": "对于用户主观行为（如浏览），专门的小型LM（通过页面级分词器训练）以及能够感知异质性的聚类式LM（HeTLM）比通用的单一大型LM表现更优。HeTLM通过实现更高的平均性能和更低的性能方差，显著改善了用户级别的对齐效果。"}}
{"id": "2508.15431", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15431", "abs": "https://arxiv.org/abs/2508.15431", "authors": ["Danish Zia Baig", "Mohsin Kamal"], "title": "A Curated Dataset and Deep Learning Approach for Minor Dent Detection in Vehicles", "comment": null, "summary": "Conventional car damage inspection techniques are labor-intensive, manual,\nand frequently overlook tiny surface imperfections like microscopic dents.\nMachine learning provides an innovative solution to the increasing demand for\nquicker and more precise inspection methods. The paper uses the YOLOv8 object\nrecognition framework to provide a deep learning-based solution for\nautomatically detecting microscopic surface flaws, notably tiny dents, on car\nexteriors. Traditional automotive damage inspection procedures are manual,\ntime-consuming, and frequently unreliable at detecting tiny flaws. To solve\nthis, a bespoke dataset containing annotated photos of car surfaces under\nvarious lighting circumstances, angles, and textures was created. To improve\nrobustness, the YOLOv8m model and its customized variants, YOLOv8m-t4 and\nYOLOv8m-t42, were trained employing real-time data augmentation approaches.\nExperimental results show that the technique has excellent detection accuracy\nand low inference latency, making it suited for real-time applications such as\nautomated insurance evaluations and automobile inspections. Evaluation\nparameters such as mean Average Precision (mAP), precision, recall, and\nF1-score verified the model's efficacy. With a precision of 0.86, recall of\n0.84, and F1-score of 0.85, the YOLOv8m-t42 model outperformed the YOLOv8m-t4\nmodel (precision: 0.81, recall: 0.79, F1-score: 0.80) in identifying\nmicroscopic surface defects. With a little reduced mAP@0.5:0.95 of 0.20, the\nmAP@0.5 for YOLOv8m-t42 stabilized at 0.60. Furthermore, YOLOv8m-t42's PR curve\narea was 0.88, suggesting more consistent performance than YOLOv8m-t4 (0.82).\nYOLOv8m-t42 has greater accuracy and is more appropriate for practical dent\ndetection applications, even though its convergence is slower.", "AI": {"tldr": "该研究利用YOLOv8深度学习框架，开发了一种自动检测汽车表面微小凹痕的方法。通过定制数据集和YOLOv8m-t42变体模型，实现了高精度和低延迟的检测，适用于实时汽车损伤评估。", "motivation": "传统的汽车损伤检测方法劳动密集、人工操作且耗时，难以准确发现微小的表面缺陷（如微观凹痕），导致检测效率低且不可靠。市场对更快、更精确的检测方法有日益增长的需求。", "method": "本研究创建了一个包含多种光照、角度和纹理下汽车表面带注释图像的定制数据集。采用YOLOv8对象识别框架，并训练了YOLOv8m模型及其定制变体（YOLOv8m-t4和YOLOv8m-t42），使用实时数据增强技术以提高鲁棒性。模型的有效性通过平均精度（mAP）、精确度、召回率和F1分数等参数进行评估。", "result": "实验结果表明，该方法具有出色的检测精度和低推理延迟。YOLOv8m-t42模型在检测微观表面缺陷方面表现最佳，其精确度为0.86，召回率为0.84，F1分数为0.85，优于YOLOv8m-t4模型。YOLOv8m-t42的mAP@0.5稳定在0.60，PR曲线下面积为0.88，表明其性能更为一致。", "conclusion": "YOLOv8m-t42模型在汽车表面微小凹痕检测中展现出更高的准确性和更一致的性能，尽管收敛速度较慢，但其高精度和低延迟使其非常适合实时应用，如自动化保险评估和汽车检测。"}}
{"id": "2508.15475", "categories": ["cs.CL", "cs.LG", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.15475", "abs": "https://arxiv.org/abs/2508.15475", "authors": ["Loris Schoenegger", "Lukas Thoma", "Terra Blevins", "Benjamin Roth"], "title": "Influence-driven Curriculum Learning for Pre-training on Limited Data", "comment": "9 pages", "summary": "Curriculum learning, a training technique where data is presented to the\nmodel in order of example difficulty (e.g., from simpler to more complex\ndocuments), has shown limited success for pre-training language models. In this\nwork, we investigate whether curriculum learning becomes competitive if we\nreplace conventional human-centered difficulty metrics with one that more\nclosely corresponds to example difficulty as observed during model training.\nSpecifically, we experiment with sorting training examples by their\n\\textit{training data influence}, a score which estimates the effect of\nindividual training examples on the model's output. Models trained on our\ncurricula are able to outperform ones trained in random order by over 10\npercentage points in benchmarks, confirming that curriculum learning is\nbeneficial for language model pre-training, as long as a more model-centric\nnotion of difficulty is adopted.", "AI": {"tldr": "传统课程学习在语言模型预训练中效果有限。本文提出使用“训练数据影响力”作为模型中心化难度指标，显著提升了课程学习在语言模型预训练中的表现。", "motivation": "课程学习（按难度排序数据）在语言模型预训练中，使用传统的人工定义难度指标时效果不佳。研究旨在探讨若采用更贴近模型训练实际观察到的难度指标，课程学习是否能变得有竞争力。", "method": "研究通过将训练样本按其“训练数据影响力”（一个评估单个训练样本对模型输出影响的得分）进行排序来构建课程。然后，将采用这种课程训练的模型与随机顺序训练的模型进行比较。", "result": "使用所提出的课程（基于训练数据影响力）训练的模型在基准测试中，表现优于随机顺序训练的模型超过10个百分点。", "conclusion": "课程学习对语言模型预训练是有益的，前提是采用更以模型为中心的难度概念（如训练数据影响力）。"}}
{"id": "2508.15439", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15439", "abs": "https://arxiv.org/abs/2508.15439", "authors": ["Yogesh Kumar", "Uday Agarwal", "Manish Gupta", "Anand Mishra"], "title": "Aligning Moments in Time using Video Queries", "comment": "11 pages, 4 figures", "summary": "Video-to-video moment retrieval (Vid2VidMR) is the task of localizing unseen\nevents or moments in a target video using a query video. This task poses\nseveral challenges, such as the need for semantic frame-level alignment and\nmodeling complex dependencies between query and target videos. To tackle this\nchallenging problem, we introduce MATR (Moment Alignment TRansformer), a\ntransformer-based model designed to capture semantic context as well as the\ntemporal details necessary for precise moment localization. MATR conditions\ntarget video representations on query video features using dual-stage sequence\nalignment that encodes the required correlations and dependencies. These\nrepresentations are then used to guide foreground/background classification and\nboundary prediction heads, enabling the model to accurately identify moments in\nthe target video that semantically match with the query video. Additionally, to\nprovide a strong task-specific initialization for MATR, we propose a\nself-supervised pre-training technique that involves training the model to\nlocalize random clips within videos. Extensive experiments demonstrate that\nMATR achieves notable performance improvements of 13.1% in R@1 and 8.1% in mIoU\non an absolute scale compared to state-of-the-art methods on the popular\nActivityNet-VRL dataset. Additionally, on our newly proposed dataset,\nSportsMoments, MATR shows a 14.7% gain in R@1 and a 14.4% gain in mIoU on an\nabsolute scale over strong baselines.", "AI": {"tldr": "本文提出MATR模型，一个基于Transformer的模型，用于视频到视频的精彩瞬间检索（Vid2VidMR）。它通过双阶段序列对齐和自监督预训练，在语义匹配和时间定位方面表现出色，显著优于现有SOTA方法。", "motivation": "视频到视频的精彩瞬间检索（Vid2VidMR）任务面临多项挑战，包括需要实现语义帧级对齐以及建模查询视频和目标视频之间复杂的依赖关系。", "method": "本文提出了MATR（Moment Alignment TRansformer）模型，这是一个基于Transformer的模型，旨在捕捉语义上下文和时间细节以进行精确的瞬间定位。MATR通过双阶段序列对齐，利用查询视频特征来条件化目标视频表示，从而编码所需的关联和依赖。这些表示随后用于指导前景/背景分类和边界预测头。此外，为MATR提供强大的任务特定初始化，本文提出了一种自监督预训练技术，通过训练模型在视频内定位随机片段。", "result": "在流行的ActivityNet-VRL数据集上，MATR在R@1指标上绝对提升了13.1%，在mIoU指标上绝对提升了8.1%，优于现有最先进方法。在新提出的SportsMoments数据集上，MATR在R@1指标上绝对提升了14.7%，在mIoU指标上绝对提升了14.4%，优于强基线模型。", "conclusion": "MATR模型通过捕获语义上下文和时间细节，有效解决了视频到视频精彩瞬间检索的挑战，并在多个数据集上取得了显著的性能提升，证明了其在精确瞬间定位方面的能力。"}}
{"id": "2508.15478", "categories": ["cs.CL", "cs.CY", "cs.PF"], "pdf": "https://arxiv.org/pdf/2508.15478", "abs": "https://arxiv.org/abs/2508.15478", "authors": ["Nghiem Thanh Pham", "Tung Kieu", "Duc-Manh Nguyen", "Son Ha Xuan", "Nghia Duong-Trung", "Danh Le-Phuoc"], "title": "SLM-Bench: A Comprehensive Benchmark of Small Language Models on Environmental Impacts -- Extended Version", "comment": "24 pages. An extended version of \"SLM-Bench: A Comprehensive\n  Benchmark of Small Language Models on Environmental Impacts\" accepted at\n  EMNLP 2025", "summary": "Small Language Models (SLMs) offer computational efficiency and\naccessibility, yet a systematic evaluation of their performance and\nenvironmental impact remains lacking. We introduce SLM-Bench, the first\nbenchmark specifically designed to assess SLMs across multiple dimensions,\nincluding accuracy, computational efficiency, and sustainability metrics.\nSLM-Bench evaluates 15 SLMs on 9 NLP tasks using 23 datasets spanning 14\ndomains. The evaluation is conducted on 4 hardware configurations, providing a\nrigorous comparison of their effectiveness. Unlike prior benchmarks, SLM-Bench\nquantifies 11 metrics across correctness, computation, and consumption,\nenabling a holistic assessment of efficiency trade-offs. Our evaluation\nconsiders controlled hardware conditions, ensuring fair comparisons across\nmodels. We develop an open-source benchmarking pipeline with standardized\nevaluation protocols to facilitate reproducibility and further research. Our\nfindings highlight the diverse trade-offs among SLMs, where some models excel\nin accuracy while others achieve superior energy efficiency. SLM-Bench sets a\nnew standard for SLM evaluation, bridging the gap between resource efficiency\nand real-world applicability.", "AI": {"tldr": "SLM-Bench是首个专门评估小型语言模型（SLMs）的基准，它系统地衡量了15个SLMs在9项NLP任务、23个数据集和4种硬件配置下的准确性、计算效率和可持续性，揭示了效率权衡。", "motivation": "小型语言模型（SLMs）在计算效率和可访问性方面具有优势，但目前缺乏对其性能和环境影响的系统性评估。", "method": "引入了SLM-Bench基准，评估了15个SLMs在9项NLP任务（使用14个领域23个数据集）上的表现。评估在4种硬件配置下进行，量化了正确性、计算和消耗方面的11项指标，并开发了一个开源的、标准化的评估流程。", "result": "研究发现SLMs之间存在多样化的权衡，一些模型在准确性上表现出色，而另一些则在能源效率上更优。", "conclusion": "SLM-Bench为SLM评估设定了新标准，弥合了资源效率与实际应用之间的差距。"}}
{"id": "2508.15457", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15457", "abs": "https://arxiv.org/abs/2508.15457", "authors": ["Zongqi He", "Hanmin Li", "Kin-Chung Chan", "Yushen Zuo", "Hao Xie", "Zhe Xiao", "Jun Xiao", "Kin-Man Lam"], "title": "Enhancing Novel View Synthesis from extremely sparse views with SfM-free 3D Gaussian Splatting Framework", "comment": "13 pages, 4 figures", "summary": "3D Gaussian Splatting (3DGS) has demonstrated remarkable real-time\nperformance in novel view synthesis, yet its effectiveness relies heavily on\ndense multi-view inputs with precisely known camera poses, which are rarely\navailable in real-world scenarios. When input views become extremely sparse,\nthe Structure-from-Motion (SfM) method that 3DGS depends on for initialization\nfails to accurately reconstruct the 3D geometric structures of scenes,\nresulting in degraded rendering quality. In this paper, we propose a novel\nSfM-free 3DGS-based method that jointly estimates camera poses and reconstructs\n3D scenes from extremely sparse-view inputs. Specifically, instead of SfM, we\npropose a dense stereo module to progressively estimates camera pose\ninformation and reconstructs a global dense point cloud for initialization. To\naddress the inherent problem of information scarcity in extremely sparse-view\nsettings, we propose a coherent view interpolation module that interpolates\ncamera poses based on training view pairs and generates viewpoint-consistent\ncontent as additional supervision signals for training. Furthermore, we\nintroduce multi-scale Laplacian consistent regularization and adaptive\nspatial-aware multi-scale geometry regularization to enhance the quality of\ngeometrical structures and rendered content. Experiments show that our method\nsignificantly outperforms other state-of-the-art 3DGS-based approaches,\nachieving a remarkable 2.75dB improvement in PSNR under extremely sparse-view\nconditions (using only 2 training views). The images synthesized by our method\nexhibit minimal distortion while preserving rich high-frequency details,\nresulting in superior visual quality compared to existing techniques.", "AI": {"tldr": "本文提出了一种无需SfM的3D Gaussian Splatting方法，通过密集立体模块、相干视图插值和多尺度正则化，显著提升了在极稀疏视图条件下的新颖视图合成质量。", "motivation": "3DGS在稀疏视图输入下，其依赖的SfM方法难以准确重建3D几何结构，导致渲染质量下降。现实世界中，密集多视图和精确相机姿态往往难以获得，因此需要一种无需SfM且能处理极稀疏视图的方法。", "method": "1. 提出一个密集立体模块，逐步估计相机姿态并重建全局密集点云用于初始化，替代SfM。 2. 引入一个相干视图插值模块，基于训练视图对插值相机姿态并生成视图一致的内容作为额外监督信号。 3. 引入多尺度拉普拉斯一致性正则化和自适应空间感知多尺度几何正则化，以增强几何结构和渲染内容的质量。", "result": "在极稀疏视图条件下（仅使用2个训练视图），该方法显著优于其他最先进的3DGS方法，PSNR提高了2.75dB。合成图像失真最小，保留了丰富的高频细节，视觉质量优于现有技术。", "conclusion": "所提出的无需SfM的3DGS方法有效解决了极稀疏视图输入下的新颖视图合成挑战，通过创新的初始化、监督和正则化策略，实现了卓越的渲染质量和几何结构重建。"}}
{"id": "2508.15476", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15476", "abs": "https://arxiv.org/abs/2508.15476", "authors": ["Chengqi Dong", "Fenghe Tang", "Rongge Mao", "Xinpei Gao", "S. Kevin Zhou"], "title": "LGMSNet: Thinning a medical image segmentation model via dual-level multiscale fusion", "comment": "Accepted by ECAI 2025", "summary": "Medical image segmentation plays a pivotal role in disease diagnosis and\ntreatment planning, particularly in resource-constrained clinical settings\nwhere lightweight and generalizable models are urgently needed. However,\nexisting lightweight models often compromise performance for efficiency and\nrarely adopt computationally expensive attention mechanisms, severely\nrestricting their global contextual perception capabilities. Additionally,\ncurrent architectures neglect the channel redundancy issue under the same\nconvolutional kernels in medical imaging, which hinders effective feature\nextraction. To address these challenges, we propose LGMSNet, a novel\nlightweight framework based on local and global dual multiscale that achieves\nstate-of-the-art performance with minimal computational overhead. LGMSNet\nemploys heterogeneous intra-layer kernels to extract local high-frequency\ninformation while mitigating channel redundancy. In addition, the model\nintegrates sparse transformer-convolutional hybrid branches to capture\nlow-frequency global information. Extensive experiments across six public\ndatasets demonstrate LGMSNet's superiority over existing state-of-the-art\nmethods. In particular, LGMSNet maintains exceptional performance in zero-shot\ngeneralization tests on four unseen datasets, underscoring its potential for\nreal-world deployment in resource-limited medical scenarios. The whole project\ncode is in https://github.com/cq-dong/LGMSNet.", "AI": {"tldr": "本文提出了LGMSNet，一个轻量级医学图像分割框架，通过结合局部和全局双多尺度处理，实现了在计算开销极小的情况下，超越现有最先进方法的性能，并展现出卓越的零样本泛化能力。", "motivation": "现有轻量级医学图像分割模型通常以牺牲性能为代价来提高效率，且很少采用计算成本高的注意力机制，从而限制了全局上下文感知能力。此外，当前架构忽视了医学图像中相同卷积核下的通道冗余问题，阻碍了有效的特征提取。这些问题在资源受限的临床环境中尤为突出。", "method": "LGMSNet采用局部和全局双多尺度轻量级框架。它利用异构层内卷积核来提取局部高频信息并减少通道冗余。同时，模型集成了稀疏Transformer-卷积混合分支，以捕获低频全局信息。", "result": "LGMSNet在六个公共数据集上进行了广泛实验，结果表明其性能优于现有最先进方法。特别是在四个未见数据集上的零样本泛化测试中，LGMSNet保持了卓越的性能。", "conclusion": "LGMSNet在资源受限的医疗场景中具有巨大的实际部署潜力，因为它在保持极低计算开销的同时，实现了卓越的分割性能和出色的零样本泛化能力。"}}
{"id": "2508.15483", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.15483", "abs": "https://arxiv.org/abs/2508.15483", "authors": ["Guy Mor-Lan", "Naama Rivlin-Angert", "Yael R. Kaplan", "Tamir Sheafer", "Shaul R. Shenhav"], "title": "HebID: Detecting Social Identities in Hebrew-language Political Text", "comment": null, "summary": "Political language is deeply intertwined with social identities. While social\nidentities are often shaped by specific cultural contexts and expressed through\nparticular uses of language, existing datasets for group and identity detection\nare predominantly English-centric, single-label and focus on coarse identity\ncategories. We introduce HebID, the first multilabel Hebrew corpus for social\nidentity detection: 5,536 sentences from Israeli politicians' Facebook posts\n(Dec 2018-Apr 2021), manually annotated for twelve nuanced social identities\n(e.g. Rightist, Ultra-Orthodox, Socially-oriented) grounded by survey data. We\nbenchmark multilabel and single-label encoders alongside 2B-9B-parameter\ngenerative LLMs, finding that Hebrew-tuned LLMs provide the best results\n(macro-$F_1$ = 0.74). We apply our classifier to politicians' Facebook posts\nand parliamentary speeches, evaluating differences in popularity, temporal\ntrends, clustering patterns, and gender-related variations in identity\nexpression. We utilize identity choices from a national public survey, enabling\na comparison between identities portrayed in elite discourse and the public's\nidentity priorities. HebID provides a comprehensive foundation for studying\nsocial identities in Hebrew and can serve as a model for similar research in\nother non-English political contexts.", "AI": {"tldr": "本文介绍了HebID，一个用于希伯来语社会身份识别的多标签语料库，并使用LLMs对其进行基准测试，分析了以色列政治话语中的身份表达，填补了非英语、细致多标签身份检测的空白。", "motivation": "政治语言与社会身份紧密交织，但现有身份检测数据集主要以英语为中心、单标签且类别粗糙。文化背景对身份和语言使用影响深远，因此需要一个能捕捉细微差别的非英语、多标签社会身份数据集。", "method": "研究引入了HebID，一个包含5,536条来自以色列政治家Facebook帖子的希伯来语语料库（2018年12月-2021年4月），并根据调查数据手动标注了12种细致的社会身份。研究对多标签和单标签编码器以及参数量从2B到9B的生成式LLMs进行了基准测试。随后，将分类器应用于政治家的Facebook帖子和议会演讲，评估了身份表达在受欢迎程度、时间趋势、聚类模式和性别相关差异。此外，还利用全国公众调查中的身份选择，比较了精英话语中描绘的身份与公众的身份优先级。", "result": "研究发现，经过希伯来语调整的LLMs提供了最佳结果（macro-$F_1$ = 0.74）。通过应用分类器，评估了政治家Facebook帖子和议会演讲中身份表达的受欢迎程度、时间趋势、聚类模式和性别相关差异。同时，实现了精英话语与公众身份优先级之间的比较。", "conclusion": "HebID为研究希伯来语中的社会身份提供了一个全面的基础，并可以作为其他非英语政治背景下类似研究的模型。"}}
{"id": "2508.15500", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15500", "abs": "https://arxiv.org/abs/2508.15500", "authors": ["Fulden Ece Uğur", "Rafael Redondo", "Albert Barreiro", "Stefan Hristov", "Roger Marí"], "title": "MExECON: Multi-view Extended Explicit Clothed humans Optimized via Normal integration", "comment": null, "summary": "This work presents MExECON, a novel pipeline for 3D reconstruction of clothed\nhuman avatars from sparse multi-view RGB images. Building on the single-view\nmethod ECON, MExECON extends its capabilities to leverage multiple viewpoints,\nimproving geometry and body pose estimation. At the core of the pipeline is the\nproposed Joint Multi-view Body Optimization (JMBO) algorithm, which fits a\nsingle SMPL-X body model jointly across all input views, enforcing multi-view\nconsistency. The optimized body model serves as a low-frequency prior that\nguides the subsequent surface reconstruction, where geometric details are added\nvia normal map integration. MExECON integrates normal maps from both front and\nback views to accurately capture fine-grained surface details such as clothing\nfolds and hairstyles. All multi-view gains are achieved without requiring any\nnetwork re-training. Experimental results show that MExECON consistently\nimproves fidelity over the single-view baseline and achieves competitive\nperformance compared to modern few-shot 3D reconstruction methods.", "AI": {"tldr": "MExECON是一个新颖的流水线，用于从稀疏多视角RGB图像重建穿衣人体3D形象，它扩展了单视角方法ECON，通过多视角联合优化和法线贴图集成，在不重新训练网络的情况下显著提升了几何和姿态估计的精度和细节。", "motivation": "现有方法在从稀疏多视角RGB图像重建穿衣人体3D形象时，可能在几何和身体姿态估计方面存在不足。该研究旨在通过利用多个视角来改进这些方面，并从单视角方法中汲取灵感。", "method": "该方法名为MExECON，它扩展了单视角方法ECON的功能以利用多个视角。其核心是提出的联合多视角身体优化（JMBO）算法，该算法在所有输入视角中联合拟合单个SMPL-X身体模型，以强制实现多视角一致性。优化后的身体模型作为低频先验，指导后续的表面重建，通过法线贴图集成添加几何细节。MExECON集成了来自正面和背面视角的法线贴图，以准确捕捉衣物褶皱和发型等精细表面细节。所有多视角改进均无需任何网络重新训练。", "result": "实验结果表明，MExECON在保真度上始终优于单视角基线，并且与现代少样本3D重建方法相比，取得了具有竞争力的性能。", "conclusion": "MExECON成功地利用多视角数据来增强穿衣人体形象的3D重建，提供了更高的保真度和更精细的细节，并且无需重新训练网络，证明了其与最先进的少样本方法相比的竞争力。"}}
{"id": "2508.15617", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15617", "abs": "https://arxiv.org/abs/2508.15617", "authors": ["Ishaan Bhola", "Mukunda NS", "Sravanth Kurmala", "Harsh Nandwani", "Arihant Jain"], "title": "Trained Miniatures: Low cost, High Efficacy SLMs for Sales & Marketing", "comment": null, "summary": "Large language models (LLMs) excel in text generation; however, these\ncreative elements require heavy computation and are accompanied by a steep\ncost. Especially for targeted applications such as sales and marketing\noutreach, these costs are far from feasible. This paper introduces the concept\nof \"Trained Miniatures\" - Small Language Models(SLMs) fine-tuned for specific,\nhigh-value applications, generating similar domain-specific responses for a\nfraction of the cost.", "AI": {"tldr": "本文提出“训练微型模型”（Trained Miniatures）概念，即针对特定高价值应用进行微调的小型语言模型（SLMs），旨在以更低的成本生成与大型语言模型（LLMs）相似的领域特定响应。", "motivation": "大型语言模型（LLMs）在文本生成方面表现出色，但计算量大且成本高昂，对于销售和营销等特定应用而言，其成本难以承受。", "method": "引入“训练微型模型”的概念，即对小型语言模型（SLMs）进行微调，使其专注于特定的高价值应用。", "result": "这些微型模型能够以极低的成本生成相似的领域特定响应。", "conclusion": "通过对SLMs进行微调，可以创建出成本效益高、适用于特定高价值应用的“训练微型模型”，作为LLMs的替代方案。"}}
{"id": "2508.15487", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.15487", "abs": "https://arxiv.org/abs/2508.15487", "authors": ["Jiacheng Ye", "Zhihui Xie", "Lin Zheng", "Jiahui Gao", "Zirui Wu", "Xin Jiang", "Zhenguo Li", "Lingpeng Kong"], "title": "Dream 7B: Diffusion Large Language Models", "comment": null, "summary": "We introduce Dream 7B, the most powerful open diffusion large language model\nto date. Unlike autoregressive (AR) models that generate tokens sequentially,\nDream 7B employs discrete diffusion modeling to refine sequences in parallel\nthrough iterative denoising. Our model consistently outperforms existing\ndiffusion language models on general, mathematical, and coding tasks. Dream 7B\ndemonstrates superior planning abilities and inference flexibility, including\narbitrary-order generation, infilling capabilities, and tunable quality-speed\ntrade-offs. These results are achieved through simple yet effective training\ntechniques, including AR-based LLM initialization and context-adaptive\ntoken-level noise rescheduling. We release both Dream-Base and Dream-Instruct\nto facilitate further research in diffusion-based language modeling.", "AI": {"tldr": "本文介绍了Dream 7B，一个强大的开放扩散大语言模型，它通过迭代去噪并行生成序列，并在通用、数学和编码任务上超越现有扩散语言模型。", "motivation": "与顺序生成token的自回归模型不同，本研究旨在利用离散扩散模型实现并行序列精炼，以提升语言模型的性能和推理灵活性。", "method": "Dream 7B采用离散扩散建模，通过迭代去噪并行精炼序列。其训练技术包括基于自回归（AR）LLM的初始化，以及上下文自适应的token级噪声重调度。", "result": "Dream 7B是迄今为止最强大的开放扩散大语言模型。它在通用、数学和编码任务上持续优于现有扩散语言模型，并展现出卓越的规划能力和推理灵活性，包括任意顺序生成、填充能力和可调的质量-速度权衡。", "conclusion": "Dream 7B通过简单而有效的训练技术，成功展示了扩散模型在语言生成领域的强大潜力，并在性能和灵活性上超越了现有模型，为扩散语言模型的研究提供了新的基础（Dream-Base和Dream-Instruct）。"}}
{"id": "2508.15505", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15505", "abs": "https://arxiv.org/abs/2508.15505", "authors": ["Mengyu Wang", "Zhenyu Liu", "Kun Li", "Yu Wang", "Yuwei Wang", "Yanyan Wei", "Fei Wang"], "title": "Task-Generalized Adaptive Cross-Domain Learning for Multimodal Image Fusion", "comment": "Accepted by IEEE Transactions on Multimedia", "summary": "Multimodal Image Fusion (MMIF) aims to integrate complementary information\nfrom different imaging modalities to overcome the limitations of individual\nsensors. It enhances image quality and facilitates downstream applications such\nas remote sensing, medical diagnostics, and robotics. Despite significant\nadvancements, current MMIF methods still face challenges such as modality\nmisalignment, high-frequency detail destruction, and task-specific limitations.\nTo address these challenges, we propose AdaSFFuse, a novel framework for\ntask-generalized MMIF through adaptive cross-domain co-fusion learning.\nAdaSFFuse introduces two key innovations: the Adaptive Approximate Wavelet\nTransform (AdaWAT) for frequency decoupling, and the Spatial-Frequency Mamba\nBlocks for efficient multimodal fusion. AdaWAT adaptively separates the high-\nand low-frequency components of multimodal images from different scenes,\nenabling fine-grained extraction and alignment of distinct frequency\ncharacteristics for each modality. The Spatial-Frequency Mamba Blocks\nfacilitate cross-domain fusion in both spatial and frequency domains, enhancing\nthis process. These blocks dynamically adjust through learnable mappings to\nensure robust fusion across diverse modalities. By combining these components,\nAdaSFFuse improves the alignment and integration of multimodal features,\nreduces frequency loss, and preserves critical details. Extensive experiments\non four MMIF tasks -- Infrared-Visible Image Fusion (IVF), Multi-Focus Image\nFusion (MFF), Multi-Exposure Image Fusion (MEF), and Medical Image Fusion (MIF)\n-- demonstrate AdaSFFuse's superior fusion performance, ensuring both low\ncomputational cost and a compact network, offering a strong balance between\nperformance and efficiency. The code will be publicly available at\nhttps://github.com/Zhen-yu-Liu/AdaSFFuse.", "AI": {"tldr": "AdaSFFuse是一个新颖的多模态图像融合（MMIF）框架，通过自适应近似小波变换（AdaWAT）进行频率解耦和空间-频率Mamba模块进行高效融合，解决了模态错位和高频细节丢失等问题，并在多个MMIF任务中实现了卓越的性能和效率平衡。", "motivation": "当前的多模态图像融合（MMIF）方法面临模态错位、高频细节破坏以及任务特异性限制等挑战，需要一种更通用和高效的解决方案。", "method": "本文提出了AdaSFFuse框架，包含两大核心创新：1) 自适应近似小波变换（AdaWAT），用于自适应地分离多模态图像的高低频分量；2) 空间-频率Mamba模块，通过可学习映射动态调整，促进空间域和频率域的跨域融合。", "result": "AdaSFFuse显著改善了多模态特征的对齐和整合，减少了频率损失，并保留了关键细节。在红外-可见光图像融合、多焦点图像融合、多曝光图像融合和医学图像融合这四种MMIF任务上，AdaSFFuse展示了卓越的融合性能，同时具有较低的计算成本和紧凑的网络结构，实现了性能与效率的良好平衡。", "conclusion": "AdaSFFuse为任务泛化的多模态图像融合提供了一个强大而高效的解决方案，有效克服了现有方法的局限性，并在多种融合任务中展现出优越的性能。"}}
{"id": "2508.15650", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15650", "abs": "https://arxiv.org/abs/2508.15650", "authors": ["Shuchao Pang", "Zhenghan Chen", "Shen Zhang", "Liming Lu", "Siyuan Liang", "Anan Du", "Yongbin Zhou"], "title": "Towards a 3D Transfer-based Black-box Attack via Critical Feature Guidance", "comment": "11 pages, 6 figures", "summary": "Deep neural networks for 3D point clouds have been demonstrated to be\nvulnerable to adversarial examples. Previous 3D adversarial attack methods\noften exploit certain information about the target models, such as model\nparameters or outputs, to generate adversarial point clouds. However, in\nrealistic scenarios, it is challenging to obtain any information about the\ntarget models under conditions of absolute security. Therefore, we focus on\ntransfer-based attacks, where generating adversarial point clouds does not\nrequire any information about the target models. Based on our observation that\nthe critical features used for point cloud classification are consistent across\ndifferent DNN architectures, we propose CFG, a novel transfer-based black-box\nattack method that improves the transferability of adversarial point clouds via\nthe proposed Critical Feature Guidance. Specifically, our method regularizes\nthe search of adversarial point clouds by computing the importance of the\nextracted features, prioritizing the corruption of critical features that are\nlikely to be adopted by diverse architectures. Further, we explicitly constrain\nthe maximum deviation extent of the generated adversarial point clouds in the\nloss function to ensure their imperceptibility. Extensive experiments conducted\non the ModelNet40 and ScanObjectNN benchmark datasets demonstrate that the\nproposed CFG outperforms the state-of-the-art attack methods by a large margin.", "AI": {"tldr": "本文提出了一种名为CFG的新型迁移式黑盒攻击方法，通过腐蚀不同深度神经网络架构共享的关键特征，显著提高了3D点云对抗样本的迁移性，同时确保了其不可感知性。", "motivation": "以往的3D对抗攻击方法通常需要目标模型的参数或输出信息，这在现实世界的黑盒场景下难以获取。因此，研究人员希望开发一种无需目标模型任何信息的迁移式攻击方法。", "method": "该方法基于观察到点云分类的关键特征在不同DNN架构中具有一致性。为此，提出了关键特征指导（CFG）机制，通过计算提取特征的重要性来规范对抗点云的搜索，优先破坏那些可能被多种架构采用的关键特征。此外，通过在损失函数中明确限制生成对抗点云的最大偏差范围，以确保其不可感知性。", "result": "在ModelNet40和ScanObjectNN基准数据集上进行的广泛实验表明，所提出的CFG方法在性能上显著优于现有最先进的攻击方法。", "conclusion": "CFG是一种有效的迁移式黑盒攻击方法，通过针对跨架构共享的关键特征，实现了对3D点云对抗样本的高迁移性和不可感知性。"}}
{"id": "2508.15524", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.15524", "abs": "https://arxiv.org/abs/2508.15524", "authors": ["Naama Rivlin-Angert", "Guy Mor-Lan"], "title": "The Enemy from Within: A Study of Political Delegitimization Discourse in Israeli Political Speech", "comment": null, "summary": "We present the first large-scale computational study of political\ndelegitimization discourse (PDD), defined as symbolic attacks on the normative\nvalidity of political entities. We curate and manually annotate a novel\nHebrew-language corpus of 10,410 sentences drawn from Knesset speeches\n(1993-2023), Facebook posts (2018-2021), and leading news outlets, of which\n1,812 instances (17.4\\%) exhibit PDD and 642 carry additional annotations for\nintensity, incivility, target type, and affective framing. We introduce a\ntwo-stage classification pipeline combining finetuned encoder models and\ndecoder LLMs. Our best model (DictaLM 2.0) attains an F$_1$ of 0.74 for binary\nPDD detection and a macro-F$_1$ of 0.67 for classification of delegitimization\ncharacteristics. Applying this classifier to longitudinal and cross-platform\ndata, we see a marked rise in PDD over three decades, higher prevalence on\nsocial media versus parliamentary debate, greater use by male than female\npoliticians, and stronger tendencies among right-leaning actors - with\npronounced spikes during election campaigns and major political events. Our\nfindings demonstrate the feasibility and value of automated PDD analysis for\nunderstanding democratic discourse.", "AI": {"tldr": "本文首次对政治合法性攻击话语（PDD）进行了大规模计算研究，构建了一个新的希伯来语语料库，并开发了一个两阶段分类模型。研究发现PDD在过去三十年显著增加，在社交媒体、男性政治家和右翼群体中更为普遍，并在选举和政治事件期间达到高峰。", "motivation": "研究旨在对政治实体合法性攻击话语（PDD）进行大规模计算分析，以理解民主话语的演变和特征。此前缺乏对PDD的计算研究。", "method": "研究构建并手动标注了一个包含10,410句希伯来语文本的新语料库（来自议会演讲、Facebook帖子和新闻媒体）。其中1,812个实例被标注为PDD，642个实例包含强度、不文明程度、目标类型和情感框架的额外标注。研究引入了一个结合微调编码器模型和解码器大型语言模型的两阶段分类管道，并使用DictaLM 2.0模型进行分类。", "result": "最佳模型（DictaLM 2.0）在二元PDD检测中达到了0.74的F1分数，在合法性攻击特征分类中达到了0.67的宏观F1分数。将此分类器应用于纵向和跨平台数据，研究发现PDD在三十年中显著增加，在社交媒体上的流行程度高于议会辩论，男性政治家使用更多，右倾参与者倾向更强，并在选举活动和重大政治事件期间出现明显高峰。", "conclusion": "研究结果证明了自动化PDD分析对于理解民主话语的可行性和价值。"}}
{"id": "2508.15529", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15529", "abs": "https://arxiv.org/abs/2508.15529", "authors": ["Kaiyuan Tan", "Yingying Shen", "Haohui Zhu", "Zhiwei Zhan", "Shan Zhao", "Mingfei Tu", "Hongcheng Luo", "Haiyang Sun", "Bing Wang", "Guang Chen", "Hangjun Ye"], "title": "ExtraGS: Geometric-Aware Trajectory Extrapolation with Uncertainty-Guided Generative Priors", "comment": null, "summary": "Synthesizing extrapolated views from recorded driving logs is critical for\nsimulating driving scenes for autonomous driving vehicles, yet it remains a\nchallenging task. Recent methods leverage generative priors as pseudo ground\ntruth, but often lead to poor geometric consistency and over-smoothed\nrenderings. To address these limitations, we propose ExtraGS, a holistic\nframework for trajectory extrapolation that integrates both geometric and\ngenerative priors. At the core of ExtraGS is a novel Road Surface Gaussian(RSG)\nrepresentation based on a hybrid Gaussian-Signed Distance Function (SDF)\ndesign, and Far Field Gaussians (FFG) that use learnable scaling factors to\nefficiently handle distant objects. Furthermore, we develop a self-supervised\nuncertainty estimation framework based on spherical harmonics that enables\nselective integration of generative priors only where extrapolation artifacts\noccur. Extensive experiments on multiple datasets, diverse multi-camera setups,\nand various generative priors demonstrate that ExtraGS significantly enhances\nthe realism and geometric consistency of extrapolated views, while preserving\nhigh fidelity along the original trajectory.", "AI": {"tldr": "ExtraGS是一个用于轨迹外推的框架，通过结合几何和生成先验，利用混合高斯-SDF路面表示和可学习的远场高斯，并辅以自监督不确定性估计，显著提升了外推视图的真实感和几何一致性。", "motivation": "从驾驶日志中合成外推视图对自动驾驶场景模拟至关重要，但现有方法依赖生成先验，常导致几何一致性差和渲染过度平滑的问题。", "method": "本文提出了ExtraGS框架。核心是基于混合高斯-符号距离函数（SDF）设计的新型路面高斯（RSG）表示，以及使用可学习缩放因子有效处理远距离物体的远场高斯（FFG）。此外，开发了一种基于球谐函数的自监督不确定性估计框架，仅在外推伪影出现时选择性地整合生成先验。", "result": "在多个数据集、多相机设置和各种生成先验上的广泛实验表明，ExtraGS显著增强了外推视图的真实感和几何一致性，同时保持了原始轨迹的高保真度。", "conclusion": "ExtraGS通过整合几何和生成先验，有效解决了现有方法在合成外推驾驶场景时几何一致性差和渲染过度平滑的局限性，实现了更真实和几何一致的视图外推。"}}
{"id": "2508.15658", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.15658", "abs": "https://arxiv.org/abs/2508.15658", "authors": ["Weihang Su", "Anzhe Xie", "Qingyao Ai", "Jianming Long", "Jiaxin Mao", "Ziyi Ye", "Yiqun Liu"], "title": "Benchmarking Computer Science Survey Generation", "comment": null, "summary": "Scientific survey articles play a vital role in summarizing research\nprogress, yet their manual creation is becoming increasingly infeasible due to\nthe rapid growth of academic literature. While large language models (LLMs)\noffer promising capabilities for automating this process, progress in this area\nis hindered by the absence of standardized benchmarks and evaluation protocols.\nTo address this gap, we introduce SurGE (Survey Generation Evaluation), a new\nbenchmark for evaluating scientific survey generation in the computer science\ndomain. SurGE consists of (1) a collection of test instances, each including a\ntopic description, an expert-written survey, and its full set of cited\nreferences, and (2) a large-scale academic corpus of over one million papers\nthat serves as the retrieval pool. In addition, we propose an automated\nevaluation framework that measures generated surveys across four dimensions:\ninformation coverage, referencing accuracy, structural organization, and\ncontent quality. Our evaluation of diverse LLM-based approaches shows that\nsurvey generation remains highly challenging, even for advanced self-reflection\nframeworks. These findings highlight the complexity of the task and the\nnecessity for continued research. We have open-sourced all the code, data, and\nmodels at: https://github.com/oneal2000/SurGE", "AI": {"tldr": "本文介绍了SurGE，一个用于评估计算机科学领域科学综述生成的新基准，包含测试实例、引用参考文献和大型学术语料库，并提出了一个多维度自动化评估框架，结果显示LLM在此任务上仍面临巨大挑战。", "motivation": "由于学术文献的快速增长，人工撰写科学综述变得越来越不可行。尽管大型语言模型（LLMs）在此方面显示出潜力，但缺乏标准化基准和评估协议阻碍了该领域的进展。", "method": "本文引入了SurGE（Survey Generation Evaluation）基准，包括：1) 一组测试实例，每个实例包含主题描述、专家撰写的综述及其引用的参考文献；2) 一个包含超过一百万篇论文的大规模学术语料库作为检索池。此外，本文提出了一个自动化评估框架，从信息覆盖、引用准确性、结构组织和内容质量四个维度衡量生成的综述。", "result": "对各种基于LLM的方法（包括先进的自反思框架）的评估表明，综述生成仍然极具挑战性。", "conclusion": "这些发现突显了综述生成任务的复杂性以及持续研究的必要性。所有代码、数据和模型均已开源。"}}
{"id": "2508.15526", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.15526", "abs": "https://arxiv.org/abs/2508.15526", "authors": ["Xiangyang Zhu", "Yuan Tian", "Chunyi Li", "Kaiwei Zhang", "Wei Sun", "Guangtao Zhai"], "title": "SafetyFlow: An Agent-Flow System for Automated LLM Safety Benchmarking", "comment": "Code and dataset are available at\n  https://github.com/yangyangyang127/SafetyFlow", "summary": "The rapid proliferation of large language models (LLMs) has intensified the\nrequirement for reliable safety evaluation to uncover model vulnerabilities. To\nthis end, numerous LLM safety evaluation benchmarks are proposed. However,\nexisting benchmarks generally rely on labor-intensive manual curation, which\ncauses excessive time and resource consumption. They also exhibit significant\nredundancy and limited difficulty. To alleviate these problems, we introduce\nSafetyFlow, the first agent-flow system designed to automate the construction\nof LLM safety benchmarks. SafetyFlow can automatically build a comprehensive\nsafety benchmark in only four days without any human intervention by\norchestrating seven specialized agents, significantly reducing time and\nresource cost. Equipped with versatile tools, the agents of SafetyFlow ensure\nprocess and cost controllability while integrating human expertise into the\nautomatic pipeline. The final constructed dataset, SafetyFlowBench, contains\n23,446 queries with low redundancy and strong discriminative power. Our\ncontribution includes the first fully automated benchmarking pipeline and a\ncomprehensive safety benchmark. We evaluate the safety of 49 advanced LLMs on\nour dataset and conduct extensive experiments to validate our efficacy and\nefficiency.", "AI": {"tldr": "SafetyFlow是首个代理流系统，旨在自动化构建大型语言模型（LLM）安全基准，解决了现有手动基准耗时、资源密集、冗余且难度有限的问题，并在四天内无需人工干预构建了一个包含23,446个高质量查询的基准。", "motivation": "随着大型语言模型（LLMs）的迅速普及，对其安全漏洞进行可靠评估的需求日益增加。然而，现有的LLM安全评估基准通常依赖于耗时且资源密集的手动策划，导致大量时间和资源消耗。此外，这些基准还存在显著的冗余和有限的难度。", "method": "本文提出了SafetyFlow，一个首创的代理流系统，旨在自动化构建LLM安全基准。SafetyFlow通过协调七个专业代理，在四天内无需任何人工干预即可自动构建一个全面的安全基准。这些代理配备了多功能工具，确保过程和成本可控，并将人类专业知识融入自动化流程。", "result": "SafetyFlow成功构建了数据集SafetyFlowBench，包含23,446个查询，具有低冗余和强大的区分能力。该系统显著减少了时间和资源成本。研究团队使用该数据集评估了49个先进LLM的安全性，并进行了广泛实验验证了其有效性和效率。", "conclusion": "本文的主要贡献是提出了首个全自动化的LLM安全基准构建流程（SafetyFlow）和一个全面的安全基准数据集（SafetyFlowBench），有效解决了现有安全评估方法中的效率和质量问题。"}}
{"id": "2508.15535", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15535", "abs": "https://arxiv.org/abs/2508.15535", "authors": ["Guotao Liang", "Juncheng Hu", "Ximing Xing", "Jing Zhang", "Qian Yu"], "title": "Multi-Object Sketch Animation with Grouping and Motion Trajectory Priors", "comment": "Accepted by ACM MM 2025", "summary": "We introduce GroupSketch, a novel method for vector sketch animation that\neffectively handles multi-object interactions and complex motions. Existing\napproaches struggle with these scenarios, either being limited to single-object\ncases or suffering from temporal inconsistency and poor generalization. To\naddress these limitations, our method adopts a two-stage pipeline comprising\nMotion Initialization and Motion Refinement. In the first stage, the input\nsketch is interactively divided into semantic groups and key frames are\ndefined, enabling the generation of a coarse animation via interpolation. In\nthe second stage, we propose a Group-based Displacement Network (GDN), which\nrefines the coarse animation by predicting group-specific displacement fields,\nleveraging priors from a text-to-video model. GDN further incorporates\nspecialized modules, such as Context-conditioned Feature Enhancement (CCFE), to\nimprove temporal consistency. Extensive experiments demonstrate that our\napproach significantly outperforms existing methods in generating high-quality,\ntemporally consistent animations for complex, multi-object sketches, thus\nexpanding the practical applications of sketch animation.", "AI": {"tldr": "GroupSketch是一种新颖的矢量草图动画方法，通过两阶段流程处理多对象交互和复杂运动，利用基于组的位移网络（GDN）和文本到视频模型的先验知识，生成高质量、时间一致的动画。", "motivation": "现有草图动画方法在处理多对象交互和复杂运动时存在局限性，表现为仅限于单对象、时间不一致和泛化能力差。", "method": "该方法采用两阶段流程：1. 运动初始化：将输入草图交互式地划分为语义组，定义关键帧，并通过插值生成粗略动画。2. 运动细化：提出一个基于组的位移网络（GDN），通过预测组特定的位移场来细化粗略动画，并利用文本到视频模型的先验知识。GDN还包含上下文条件特征增强（CCFE）等模块以提高时间一致性。", "result": "广泛的实验表明，该方法在为复杂、多对象的草图生成高质量、时间一致的动画方面显著优于现有方法。", "conclusion": "GroupSketch成功解决了现有方法的局限性，有效处理了复杂的多对象草图动画，从而扩展了草图动画的实际应用。"}}
{"id": "2508.15717", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15717", "abs": "https://arxiv.org/abs/2508.15717", "authors": ["Yanlai Yang", "Zhuokai Zhao", "Satya Narayan Shukla", "Aashu Singh", "Shlok Kumar Mishra", "Lizhu Zhang", "Mengye Ren"], "title": "StreamMem: Query-Agnostic KV Cache Memory for Streaming Video Understanding", "comment": "15 pages, 3 figures", "summary": "Multimodal large language models (MLLMs) have made significant progress in\nvisual-language reasoning, but their ability to efficiently handle long videos\nremains limited. Despite recent advances in long-context MLLMs, storing and\nattending to the key-value (KV) cache for long visual contexts incurs\nsubstantial memory and computational overhead. Existing visual compression\nmethods require either encoding the entire visual context before compression or\nhaving access to the questions in advance, which is impractical for long video\nunderstanding and multi-turn conversational settings. In this work, we propose\nStreamMem, a query-agnostic KV cache memory mechanism for streaming video\nunderstanding. Specifically, StreamMem encodes new video frames in a streaming\nmanner, compressing the KV cache using attention scores between visual tokens\nand generic query tokens, while maintaining a fixed-size KV memory to enable\nefficient question answering (QA) in memory-constrained, long-video scenarios.\nEvaluation on three long video understanding and two streaming video question\nanswering benchmarks shows that StreamMem achieves state-of-the-art performance\nin query-agnostic KV cache compression and is competitive with query-aware\ncompression approaches.", "AI": {"tldr": "StreamMem提出了一种与查询无关的流式KV缓存压缩机制，以解决多模态大语言模型在处理长视频时因KV缓存过大导致的内存和计算效率低下问题。", "motivation": "多模态大语言模型（MLLMs）在处理长视频时，由于需要存储和关注大量的视觉上下文，导致键值（KV）缓存产生巨大的内存和计算开销。现有视觉压缩方法不切实际（需要预先编码整个上下文或知道问题）。", "method": "StreamMem是一种与查询无关的KV缓存内存机制，用于流式视频理解。它以流式方式编码新视频帧，利用视觉token和通用查询token之间的注意力分数来压缩KV缓存，并保持固定大小的KV内存。", "result": "在三个长视频理解和两个流式视频问答基准测试中，StreamMem在与查询无关的KV缓存压缩方面达到了最先进的性能，并与查询感知的压缩方法具有竞争力。", "conclusion": "StreamMem通过高效的、与查询无关的流式KV缓存压缩，有效解决了多模态大语言模型在内存受限的长视频场景中处理长视频的效率问题。"}}
{"id": "2508.15648", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.15648", "abs": "https://arxiv.org/abs/2508.15648", "authors": ["Peng Ding", "Wen Sun", "Dailin Li", "Wei Zou", "Jiaming Wang", "Jiajun Chen", "Shujian Huang"], "title": "SDGO: Self-Discrimination-Guided Optimization for Consistent Safety in Large Language Models", "comment": "Accepted by EMNLP 2025, 15 pages, 4 figures, 6 tables", "summary": "Large Language Models (LLMs) excel at various natural language processing\ntasks but remain vulnerable to jailbreaking attacks that induce harmful content\ngeneration. In this paper, we reveal a critical safety inconsistency: LLMs can\nmore effectively identify harmful requests as discriminators than defend\nagainst them as generators. This insight inspires us to explore aligning the\nmodel's inherent discrimination and generation capabilities. To this end, we\npropose SDGO (Self-Discrimination-Guided Optimization), a reinforcement\nlearning framework that leverages the model's own discrimination capabilities\nas a reward signal to enhance generation safety through iterative\nself-improvement. Our method does not require any additional annotated data or\nexternal models during the training phase. Extensive experiments demonstrate\nthat SDGO significantly improves model safety compared to both prompt-based and\ntraining-based baselines while maintaining helpfulness on general benchmarks.\nBy aligning LLMs' discrimination and generation capabilities, SDGO brings\nrobust performance against out-of-distribution (OOD) jailbreaking attacks. This\nalignment achieves tighter coupling between these two capabilities, enabling\nthe model's generation capability to be further enhanced with only a small\namount of discriminative samples. Our code and datasets are available at\nhttps://github.com/NJUNLP/SDGO.", "AI": {"tldr": "本文提出SDGO框架，通过利用大型语言模型（LLM）自身的判别能力作为奖励信号，以强化学习方式迭代优化其生成安全性，有效抵御越狱攻击。", "motivation": "LLMs易受越狱攻击生成有害内容，且存在安全不一致性：LLMs作为判别器比作为生成器更能识别有害请求。这促使研究者探索如何对齐模型的内在判别和生成能力。", "method": "提出SDGO（Self-Discrimination-Guided Optimization），一个基于强化学习的框架。该方法利用模型自身的判别能力作为奖励信号，通过迭代自我改进来增强生成安全性，无需额外的标注数据或外部模型。", "result": "SDGO显著提升了模型安全性，优于基于提示和基于训练的基线方法，同时保持了通用任务的实用性。它在对抗域外（OOD）越狱攻击方面表现出强大的鲁棒性，并实现了判别与生成能力更紧密的耦合。", "conclusion": "通过对齐LLMs的判别和生成能力，SDGO框架能有效提高模型的安全性能，使其能够更好地抵御越狱攻击，并展现出对OOD攻击的鲁棒性。"}}
{"id": "2508.15537", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15537", "abs": "https://arxiv.org/abs/2508.15537", "authors": ["Chang Liu", "Yang Xu", "Tamas Sziranyi"], "title": "D3FNet: A Differential Attention Fusion Network for Fine-Grained Road Structure Extraction in Remote Perception Systems", "comment": "10 pages, 6 figures, International Conference on Computer Vision,\n  ICCV 2025 (DriveX) paper id 5", "summary": "Extracting narrow roads from high-resolution remote sensing imagery remains a\nsignificant challenge due to their limited width, fragmented topology, and\nfrequent occlusions. To address these issues, we propose D3FNet, a Dilated\nDual-Stream Differential Attention Fusion Network designed for fine-grained\nroad structure segmentation in remote perception systems. Built upon the\nencoder-decoder backbone of D-LinkNet, D3FNet introduces three key\ninnovations:(1) a Differential Attention Dilation Extraction (DADE) module that\nenhances subtle road features while suppressing background noise at the\nbottleneck; (2) a Dual-stream Decoding Fusion Mechanism (DDFM) that integrates\noriginal and attention-modulated features to balance spatial precision with\nsemantic context; and (3) a multi-scale dilation strategy (rates 1, 3, 5, 9)\nthat mitigates gridding artifacts and improves continuity in narrow road\nprediction. Unlike conventional models that overfit to generic road widths,\nD3FNet specifically targets fine-grained, occluded, and low-contrast road\nsegments. Extensive experiments on the DeepGlobe and CHN6-CUG benchmarks show\nthat D3FNet achieves superior IoU and recall on challenging road regions,\noutperforming state-of-the-art baselines. Ablation studies further verify the\ncomplementary synergy of attention-guided encoding and dual-path decoding.\nThese results confirm D3FNet as a robust solution for fine-grained narrow road\nextraction in complex remote and cooperative perception scenarios.", "AI": {"tldr": "D3FNet是一种基于D-LinkNet的膨胀双流差分注意力融合网络，专为高分辨率遥感影像中的细粒度窄路提取设计，通过增强特征、融合多源信息和多尺度膨胀策略，有效解决了窄路宽度有限、拓扑破碎和遮挡等挑战。", "motivation": "高分辨率遥感影像中窄路的提取面临显著挑战，原因包括其宽度有限、拓扑结构破碎以及频繁的遮挡。现有模型往往过度拟合通用路宽，难以有效处理细粒度、被遮挡和低对比度的路段。", "method": "本文提出了D3FNet，一个基于D-LinkNet编码器-解码器骨干的膨胀双流差分注意力融合网络。其核心创新包括：(1) 差分注意力膨胀提取(DADE)模块，在瓶颈处增强细微道路特征并抑制背景噪声；(2) 双流解码融合机制(DDFM)，整合原始特征和注意力调制特征，平衡空间精度与语义上下文；(3) 多尺度膨胀策略(膨胀率为1, 3, 5, 9)，减少网格伪影并提高窄路预测的连续性。", "result": "在DeepGlobe和CHN6-CUG基准数据集上进行的广泛实验表明，D3FNet在具有挑战性的道路区域上实现了卓越的IoU和召回率，优于最先进的基线模型。消融研究进一步验证了注意力引导编码和双路径解码的互补协同作用。", "conclusion": "研究结果证实，D3FNet是复杂远程和协同感知场景中细粒度窄路提取的鲁棒解决方案，能够有效应对窄路提取的固有挑战。"}}
{"id": "2508.15721", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15721", "abs": "https://arxiv.org/abs/2508.15721", "authors": ["Xinyi Ling", "Hanwen Du", "Zhihui Zhu", "Xia Ning"], "title": "EcomMMMU: Strategic Utilization of Visuals for Robust Multimodal E-Commerce Models", "comment": null, "summary": "E-commerce platforms are rich in multimodal data, featuring a variety of\nimages that depict product details. However, this raises an important question:\ndo these images always enhance product understanding, or can they sometimes\nintroduce redundancy or degrade performance? Existing datasets are limited in\nboth scale and design, making it difficult to systematically examine this\nquestion. To this end, we introduce EcomMMMU, an e-commerce multimodal\nmultitask understanding dataset with 406,190 samples and 8,989,510 images.\nEcomMMMU is comprised of multi-image visual-language data designed with 8\nessential tasks and a specialized VSS subset to benchmark the capability of\nmultimodal large language models (MLLMs) to effectively utilize visual content.\nAnalysis on EcomMMMU reveals that product images do not consistently improve\nperformance and can, in some cases, degrade it. This indicates that MLLMs may\nstruggle to effectively leverage rich visual content for e-commerce tasks.\nBuilding on these insights, we propose SUMEI, a data-driven method that\nstrategically utilizes multiple images via predicting visual utilities before\nusing them for downstream tasks. Comprehensive experiments demonstrate the\neffectiveness and robustness of SUMEI. The data and code are available through\nhttps://anonymous.4open.science/r/submission25.", "AI": {"tldr": "本文提出了EcomMMMU数据集，用于系统研究电商平台中多模态图像对产品理解的影响。研究发现图像并非总能提升性能，有时甚至会降低性能，表明多模态大语言模型（MLLMs）在利用视觉内容方面存在挑战。为此，作者提出了SUMEI方法，通过预测视觉效用以策略性地利用图像，并验证了其有效性。", "motivation": "电商平台拥有丰富的多模态数据，尤其是产品图像。然而，现有研究未能系统性地探讨这些图像是否总能增强产品理解，或者有时会引入冗余或降低性能。现有数据集在规模和设计上均受限，难以有效解决这一问题。", "method": "1. 引入了EcomMMMU数据集，一个电商多模态多任务理解数据集，包含406,190个样本和8,989,510张图片。该数据集包含多图像视觉-语言数据，设计了8个核心任务，并包含一个专门的VSS子集，用于评估多模态大语言模型（MLLMs）有效利用视觉内容的能力。 2. 基于对EcomMMMU的分析，提出了SUMEI方法，这是一种数据驱动的方法，通过在使用图像进行下游任务之前预测图像的视觉效用，从而策略性地利用多张图像。", "result": "1. 对EcomMMMU数据集的分析表明，产品图像并非总能持续改善性能，在某些情况下甚至会降低性能。 2. 这表明MLLMs可能难以有效利用丰富的视觉内容来完成电商任务。 3. 综合实验证明了SUMEI方法的有效性和鲁棒性。", "conclusion": "电商平台中的产品图像并非总能提升性能，有时甚至会降低性能，这揭示了多模态大语言模型（MLLMs）在有效利用丰富的视觉内容方面存在的挑战。本文提出的EcomMMMU数据集和SUMEI方法为解决这一问题提供了新的工具和策略，SUMEI通过预测视觉效用实现了对多图像的策略性利用，并被证明是有效和鲁棒的。"}}
{"id": "2508.15709", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.15709", "abs": "https://arxiv.org/abs/2508.15709", "authors": ["Yifei Wang", "Feng Xiong", "Yong Wang", "Linjing Li", "Xiangxiang Chu", "Daniel Dajun Zeng"], "title": "Position Bias Mitigates Position Bias:Mitigate Position Bias Through Inter-Position Knowledge Distillation", "comment": "EMNLP2025 Main", "summary": "Positional bias (PB), manifesting as non-uniform sensitivity across different\ncontextual locations, significantly impairs long-context comprehension and\nprocessing capabilities. While prior work seeks to mitigate PB through\nmodifying the architectures causing its emergence, significant PB still\npersists. To address PB effectively, we introduce \\textbf{Pos2Distill}, a\nposition to position knowledge distillation framework. Pos2Distill transfers\nthe superior capabilities from advantageous positions to less favorable ones,\nthereby reducing the huge performance gaps. The conceptual principle is to\nleverage the inherent, position-induced disparity to counteract the PB itself.\nWe identify distinct manifestations of PB under \\textbf{\\textsc{r}}etrieval and\n\\textbf{\\textsc{r}}easoning paradigms, thereby designing two specialized\ninstantiations: \\emph{Pos2Distill-R\\textsuperscript{1}} and\n\\emph{Pos2Distill-R\\textsuperscript{2}} respectively, both grounded in this\ncore principle. By employing the Pos2Distill approach, we achieve enhanced\nuniformity and significant performance gains across all contextual positions in\nlong-context retrieval and reasoning tasks. Crucially, both specialized systems\nexhibit strong cross-task generalization mutually, while achieving superior\nperformance on their respective tasks.", "AI": {"tldr": "本文提出Pos2Distill框架，通过位置到位置的知识蒸馏，将优势位置的能力迁移到劣势位置，从而有效缓解长文本理解中的位置偏差（PB），提升模型在检索和推理任务中的表现和一致性。", "motivation": "长文本理解中存在的位置偏差（PB）严重损害了模型的处理能力，尽管现有工作尝试通过修改架构来缓解，但PB问题依然显著，导致不同上下文位置的性能差距巨大。", "method": "本文引入Pos2Distill框架，其核心思想是利用固有的位置差异来对抗PB本身。该框架通过知识蒸馏，将模型在有利位置表现出的优越能力迁移到不利位置。针对检索和推理任务中PB的不同表现，设计了两个专门的实例化：Pos2Distill-R¹（用于检索）和Pos2Distill-R²（用于推理）。", "result": "Pos2Distill方法显著增强了长文本检索和推理任务中所有上下文位置的性能一致性，并取得了显著的性能提升。值得注意的是，这两个专门系统在相互之间展现出强大的跨任务泛化能力，同时在其各自任务上均实现了卓越的性能。", "conclusion": "Pos2Distill框架通过创新的位置到位置知识蒸馏方法，有效解决了长文本理解中的位置偏差问题，显著提升了模型在检索和推理任务中的均匀性和整体性能，并展现出良好的跨任务泛化能力。"}}
{"id": "2508.15568", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.15568", "abs": "https://arxiv.org/abs/2508.15568", "authors": ["Youjia Zhang", "Youngeun Kim", "Young-Geun Choi", "Hongyeob Kim", "Huiling Liu", "Sungeun Hong"], "title": "Backpropagation-Free Test-Time Adaptation via Probabilistic Gaussian Alignment", "comment": null, "summary": "Test-time adaptation (TTA) enhances the zero-shot robustness under\ndistribution shifts by leveraging unlabeled test data during inference. Despite\nnotable advances, several challenges still limit its broader applicability.\nFirst, most methods rely on backpropagation or iterative optimization, which\nlimits scalability and hinders real-time deployment. Second, they lack explicit\nmodeling of class-conditional feature distributions. This modeling is crucial\nfor producing reliable decision boundaries and calibrated predictions, but it\nremains underexplored due to the lack of both source data and supervision at\ntest time. In this paper, we propose ADAPT, an Advanced Distribution-Aware and\nbackPropagation-free Test-time adaptation method. We reframe TTA as a Gaussian\nprobabilistic inference task by modeling class-conditional likelihoods using\ngradually updated class means and a shared covariance matrix. This enables\nclosed-form, training-free inference. To correct potential likelihood bias, we\nintroduce lightweight regularization guided by CLIP priors and a historical\nknowledge bank. ADAPT requires no source data, no gradient updates, and no full\naccess to target data, supporting both online and transductive settings.\nExtensive experiments across diverse benchmarks demonstrate that our method\nachieves state-of-the-art performance under a wide range of distribution shifts\nwith superior scalability and robustness.", "AI": {"tldr": "本文提出ADAPT，一种先进的、无反向传播的、分布感知的测试时自适应（TTA）方法，通过高斯概率推断和轻量级正则化，实现了在各种分布偏移下最先进的性能、卓越的可扩展性和鲁棒性。", "motivation": "现有的测试时自适应（TTA）方法存在以下局限性：1) 大多数依赖反向传播或迭代优化，限制了可扩展性和实时部署；2) 缺乏对类别条件特征分布的明确建模，这对于生成可靠的决策边界和校准预测至关重要，但由于测试时缺乏源数据和监督而未被充分探索。", "method": "本文提出了ADAPT方法，将TTA重构为高斯概率推断任务。通过使用逐渐更新的类别均值和共享协方差矩阵来建模类别条件似然，从而实现闭式、免训练的推断。为校正潜在的似然偏差，引入了由CLIP先验和历史知识库指导的轻量级正则化。ADAPT无需源数据、无需梯度更新，也无需完全访问目标数据，支持在线和转导设置。", "result": "在各种基准测试中，ADAPT方法在广泛的分布偏移下均取得了最先进的性能，并展现出卓越的可扩展性和鲁棒性。", "conclusion": "ADAPT通过将TTA重新定义为高斯概率推断任务并结合轻量级正则化，有效解决了现有TTA方法在可扩展性、实时部署和类别条件分布建模方面的挑战，提供了一种无需反向传播且性能卓越的解决方案。"}}
{"id": "2508.15746", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15746", "abs": "https://arxiv.org/abs/2508.15746", "authors": ["Qiaoyu Zheng", "Yuze Sun", "Chaoyi Wu", "Weike Zhao", "Pengcheng Qiu", "Yongguo Yu", "Kun Sun", "Yanfeng Wang", "Ya Zhang", "Weidi Xie"], "title": "End-to-End Agentic RAG System Training for Traceable Diagnostic Reasoning", "comment": "35 pages, 5 figures, 3 tables", "summary": "Accurate diagnosis with medical large language models is hindered by\nknowledge gaps and hallucinations. Retrieval and tool-augmented methods help,\nbut their impact is limited by weak use of external knowledge and poor\nfeedback-reasoning traceability. To address these challenges, We introduce\nDeep-DxSearch, an agentic RAG system trained end-to-end with reinforcement\nlearning (RL) that enables steer tracebale retrieval-augmented reasoning for\nmedical diagnosis. In Deep-DxSearch, we first construct a large-scale medical\nretrieval corpus comprising patient records and reliable medical knowledge\nsources to support retrieval-aware reasoning across diagnostic scenarios. More\ncrutially, we frame the LLM as the core agent and the retrieval corpus as its\nenvironment, using tailored rewards on format, retrieval, reasoning structure,\nand diagnostic accuracy, thereby evolving the agentic RAG policy from\nlarge-scale data through RL.\n  Experiments demonstrate that our end-to-end agentic RL training framework\nconsistently outperforms prompt-engineering and training-free RAG approaches\nacross multiple data centers. After training, Deep-DxSearch achieves\nsubstantial gains in diagnostic accuracy, surpassing strong diagnostic\nbaselines such as GPT-4o, DeepSeek-R1, and other medical-specific frameworks\nfor both common and rare disease diagnosis under in-distribution and\nout-of-distribution settings. Moreover, ablation studies on reward design and\nretrieval corpus components confirm their critical roles, underscoring the\nuniqueness and effectiveness of our approach compared with traditional\nimplementations. Finally, case studies and interpretability analyses highlight\nimprovements in Deep-DxSearch's diagnostic policy, providing deeper insight\ninto its performance gains and supporting clinicians in delivering more\nreliable and precise preliminary diagnoses. See\nhttps://github.com/MAGIC-AI4Med/Deep-DxSearch.", "AI": {"tldr": "本文提出Deep-DxSearch，一个基于强化学习端到端训练的代理式RAG系统，用于医学诊断，通过可追溯的检索增强推理，显著提升诊断准确性。", "motivation": "现有医疗大语言模型在诊断中存在知识空白和幻觉问题，而检索和工具增强方法因外部知识使用不足和反馈-推理可追溯性差而效果有限。", "method": "引入Deep-DxSearch，一个代理式RAG系统，通过强化学习进行端到端训练。构建了包含患者记录和可靠医学知识的大规模医疗检索语料库。将LLM视为核心代理，语料库视为环境，并使用针对格式、检索、推理结构和诊断准确性的定制奖励，通过RL从大规模数据中演化代理式RAG策略。", "result": "Deep-DxSearch在多个数据中心持续优于提示工程和免训练RAG方法。训练后，其在诊断准确性方面取得显著提升，超越了GPT-4o、DeepSeek-R1等强基线以及其他医学专用框架，适用于常见和罕见疾病诊断，涵盖分布内和分布外设置。奖励设计和检索语料库组件的消融研究证实了它们的关键作用。案例研究和可解释性分析揭示了诊断策略的改进。", "conclusion": "Deep-DxSearch提供了一种独特且有效的医疗诊断方法，通过强化学习训练的代理式RAG系统，显著提高了诊断准确性和可靠性，为临床医生提供了更精准的初步诊断支持。"}}
{"id": "2508.15711", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.15711", "abs": "https://arxiv.org/abs/2508.15711", "authors": ["Abhijit Paul", "Mashiat Amin Farin", "Sharif Md. Abdullah", "Ahmedul Kabir", "Zarif Masud", "Shebuti Rayana"], "title": "Stemming -- The Evolution and Current State with a Focus on Bangla", "comment": null, "summary": "Bangla, the seventh most widely spoken language worldwide with 300 million\nnative speakers, faces digital under-representation due to limited resources\nand lack of annotated datasets. Stemming, a critical preprocessing step in\nlanguage analysis, is essential for low-resource, highly-inflectional languages\nlike Bangla, because it can reduce the complexity of algorithms and models by\nsignificantly reducing the number of words the algorithm needs to consider.\nThis paper conducts a comprehensive survey of stemming approaches, emphasizing\nthe importance of handling morphological variants effectively. While exploring\nthe landscape of Bangla stemming, it becomes evident that there is a\nsignificant gap in the existing literature. The paper highlights the\ndiscontinuity from previous research and the scarcity of accessible\nimplementations for replication. Furthermore, it critiques the evaluation\nmethodologies, stressing the need for more relevant metrics. In the context of\nBangla's rich morphology and diverse dialects, the paper acknowledges the\nchallenges it poses. To address these challenges, the paper suggests directions\nfor Bangla stemmer development. It concludes by advocating for robust Bangla\nstemmers and continued research in the field to enhance language analysis and\nprocessing.", "AI": {"tldr": "本文对孟加拉语词干提取方法进行了全面调查，指出现有研究的不足、实现的可及性差以及评估指标不相关，并提出了未来发展方向。", "motivation": "孟加拉语作为一种资源稀缺、高度屈折的语言，在数字领域代表性不足。词干提取是语言分析的关键预处理步骤，能有效降低算法复杂性。然而，孟加拉语词干提取领域存在显著的研究空白、缺乏可复现的实现以及评估方法不当。", "method": "本文对词干提取方法进行了全面调查，强调了处理形态变体的重要性。它批判性地审视了孟加拉语词干提取的现有文献、实现和评估方法，并基于孟加拉语丰富的形态和多样方言所带来的挑战，提出了未来发展方向。", "result": "调查发现孟加拉语词干提取领域存在显著的研究空白，与以往研究存在脱节，缺乏可访问的实现，并且现有评估方法需要更相关的指标。同时，论文也承认了孟加拉语丰富的形态和多样方言带来的挑战。", "conclusion": "本文倡导开发鲁棒的孟加拉语词干提取器，并建议在该领域持续进行研究，以增强孟加拉语的语言分析和处理能力。"}}
{"id": "2508.15582", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15582", "abs": "https://arxiv.org/abs/2508.15582", "authors": ["Sumit Kumar Dam", "Mrityunjoy Gain", "Eui-Nam Huh", "Choong Seon Hong"], "title": "High-Frequency First: A Two-Stage Approach for Improving Image INR", "comment": "Paper on INR; 4 figures, 8 pages", "summary": "Implicit Neural Representations (INRs) have emerged as a powerful alternative\nto traditional pixel-based formats by modeling images as continuous functions\nover spatial coordinates. A key challenge, however, lies in the spectral bias\nof neural networks, which tend to favor low-frequency components while\nstruggling to capture high-frequency (HF) details such as sharp edges and fine\ntextures. While prior approaches have addressed this limitation through\narchitectural modifications or specialized activation functions, we propose an\northogonal direction by directly guiding the training process. Specifically, we\nintroduce a two-stage training strategy where a neighbor-aware soft mask\nadaptively assigns higher weights to pixels with strong local variations,\nencouraging early focus on fine details. The model then transitions to\nfull-image training. Experimental results show that our approach consistently\nimproves reconstruction quality and complements existing INR methods. As a\npioneering attempt to assign frequency-aware importance to pixels in image INR,\nour work offers a new avenue for mitigating the spectral bias problem.", "AI": {"tldr": "本文提出了一种两阶段训练策略，通过在早期训练中引入邻域感知的软掩码，自适应地赋予高频细节像素更高的权重，以缓解隐式神经表示（INR）中的频谱偏差问题，从而提高图像重建质量。", "motivation": "隐式神经表示（INR）在捕捉高频细节（如锐利边缘和精细纹理）方面存在困难，这是由于神经网络的频谱偏差倾向于低频分量。虽然现有方法通过架构修改或专用激活函数来解决，但本文旨在探索一个正交方向，即直接引导训练过程。", "method": "本文提出了一种两阶段训练策略：\n1. 第一阶段：引入一个邻域感知的软掩码，自适应地为具有强局部变化的像素分配更高的权重，鼓励模型早期关注精细细节。\n2. 第二阶段：模型随后过渡到全图像训练。", "result": "实验结果表明，该方法持续提高了重建质量，并且与现有INR方法互补。", "conclusion": "作为首次尝试在图像INR中为像素分配频率感知重要性的工作，本文为缓解频谱偏差问题提供了一条新途径。"}}
{"id": "2508.15754", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15754", "abs": "https://arxiv.org/abs/2508.15754", "authors": ["Yufeng Zhao", "Junnan Liu", "Hongwei Liu", "Dongsheng Zhu", "Yuan Shen", "Songyang Zhang", "Kai Chen"], "title": "Dissecting Tool-Integrated Reasoning: An Empirical Study and Analysis", "comment": "Preprint, working in progress", "summary": "Large Language Models (LLMs) have made significant strides in reasoning tasks\nthrough methods like chain-of-thought (CoT) reasoning. However, they often fall\nshort in tasks requiring precise computations. Tool-Integrated Reasoning (TIR)\nhas emerged as a solution by incorporating external tools into the reasoning\nprocess. Nevertheless, the generalization of TIR in improving the reasoning\nability of LLM is still unclear. Additionally, whether TIR has improved the\nmodel's reasoning behavior and helped the model think remains to be studied. We\nintroduce ReasonZoo, a comprehensive benchmark encompassing nine diverse\nreasoning categories, to evaluate the effectiveness of TIR across various\ndomains. Additionally, we propose two novel metrics, Performance-Aware Cost\n(PAC) and Area Under the Performance-Cost Curve (AUC-PCC), to assess reasoning\nefficiency. Our empirical evaluation demonstrates that TIR-enabled models\nconsistently outperform their non-TIR counterparts in both mathematical and\nnon-mathematical tasks. Furthermore, TIR enhances reasoning efficiency, as\nevidenced by improved PAC and AUC-PCC, indicating reduced overthinking and more\nstreamlined reasoning. These findings underscore the domain-general benefits of\nTIR and its potential to advance LLM capabilities in complex reasoning tasks.", "AI": {"tldr": "通过引入ReasonZoo基准和新颖的效率指标，本研究证明工具集成推理（TIR）显著提升了大型语言模型（LLM）在多种推理任务中的表现和效率，减少了“过度思考”。", "motivation": "尽管LLM在链式思考等推理任务中取得进展，但在需要精确计算的任务中表现不佳。工具集成推理（TIR）被提出作为解决方案，但其在提高LLM推理能力方面的泛化性尚不明确，且TIR是否改善了模型的推理行为和思维方式也需研究。", "method": "引入了ReasonZoo，一个包含九个不同推理类别的综合基准，用于评估TIR在各种领域中的有效性。此外，提出了两个新颖的指标：性能感知成本（PAC）和性能-成本曲线下面积（AUC-PCC），以评估推理效率。", "result": "实证评估表明，启用TIR的模型在数学和非数学任务中均持续优于非TIR模型。此外，TIR提高了推理效率，表现为PAC和AUC-PCC的改善，这表明模型减少了“过度思考”，推理过程更加精简。", "conclusion": "这些发现强调了TIR的领域通用益处，以及其在提升LLM处理复杂推理任务能力方面的潜力。"}}
{"id": "2508.15760", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15760", "abs": "https://arxiv.org/abs/2508.15760", "authors": ["Ming Yin", "Dinghan Shen", "Silei Xu", "Jianbing Han", "Sixun Dong", "Mian Zhang", "Yebowen Hu", "Shujian Liu", "Simin Ma", "Song Wang", "Sathish Reddy Indurthi", "Xun Wang", "Yiran Chen", "Kaiqiang Song"], "title": "LiveMCP-101: Stress Testing and Diagnosing MCP-enabled Agents on Challenging Queries", "comment": null, "summary": "Tool calling has emerged as a critical capability for AI agents to interact\nwith the real world and solve complex tasks. While the Model Context Protocol\n(MCP) provides a powerful standardized framework for tool integration, there is\na significant gap in benchmarking how well AI agents can effectively solve\nmulti-step tasks using diverse MCP tools in realistic, dynamic scenarios. In\nthis work, we present LiveMCP-101, a benchmark of 101 carefully curated\nreal-world queries, refined through iterative LLM rewriting and manual review,\nthat require coordinated use of multiple MCP tools including web search, file\noperations, mathematical reasoning, and data analysis. Moreover, we introduce a\nnovel evaluation approach that leverages ground-truth execution plans rather\nthan raw API outputs, better reflecting the evolving nature of real-world\nenvironments. Experiments show that even frontier LLMs achieve a success rate\nbelow 60\\%, highlighting major challenges in tool orchestration. Detailed\nablations and error analysis further reveal distinct failure modes and\ninefficiencies in token usage, pointing to concrete directions for advancing\ncurrent models. LiveMCP-101 sets a rigorous standard for evaluating real-world\nagent capabilities, advancing toward autonomous AI systems that reliably\nexecute complex tasks through tool use.", "AI": {"tldr": "该研究提出了LiveMCP-101基准测试，用于评估AI智能体在真实动态场景中，使用多种模型上下文协议（MCP）工具解决多步骤任务的能力。结果显示，即使是前沿大型语言模型（LLM）的成功率也低于60%，揭示了工具编排方面的重大挑战。", "motivation": "尽管模型上下文协议（MCP）为工具集成提供了强大的标准化框架，但在基准测试AI智能体如何有效利用多样化MCP工具在真实、动态场景中解决多步骤任务方面存在显著空白。", "method": "本文介绍了LiveMCP-101，一个包含101个精心策划的真实世界查询的基准测试，这些查询经过迭代的LLM重写和人工审查，需要协调使用多种MCP工具（包括网络搜索、文件操作、数学推理和数据分析）。此外，研究引入了一种新颖的评估方法，该方法利用真实执行计划而非原始API输出，更好地反映了真实世界环境的演变性质。", "result": "实验表明，即使是前沿LLM的成功率也低于60%，突显了工具编排中的主要挑战。详细的消融研究和错误分析进一步揭示了不同的失败模式和令牌使用效率低下，指出了当前模型改进的具体方向。", "conclusion": "LiveMCP-101为评估真实世界智能体能力设定了严格标准，推动了通过工具使用可靠执行复杂任务的自主AI系统发展。"}}
{"id": "2508.15613", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15613", "abs": "https://arxiv.org/abs/2508.15613", "authors": ["Ivo Ivanov", "Carsten Markgraf"], "title": "Fast globally optimal Truncated Least Squares point cloud registration with fixed rotation axis", "comment": null, "summary": "Recent results showed that point cloud registration with given\ncorrespondences can be made robust to outlier rates of up to 95\\% using the\ntruncated least squares (TLS) formulation. However, solving this combinatorial\noptimization problem to global optimality is challenging. Provably globally\noptimal approaches using semidefinite programming (SDP) relaxations take\nhundreds of seconds for 100 points. In this paper, we propose a novel linear\ntime convex relaxation as well as a contractor method to speed up Branch and\nBound (BnB). Our solver can register two 3D point clouds with 100 points to\nprovable global optimality in less than half a second when the axis of rotation\nis provided. Although it currently cannot solve the full 6DoF problem, it is\ntwo orders of magnitude faster than the state-of-the-art SDP solver STRIDE when\nsolving the rotation-only TLS problem. In addition to providing a formal proof\nfor global optimality, we present empirical evidence of global optimality using\nadversarial instances with local minimas close to the global minimum.", "AI": {"tldr": "本文提出了一种新颖的线性时间凸松弛和收缩器方法，用于加速分支定界算法，从而在给定旋转轴的情况下，以可证明的全局最优性在不到0.5秒内完成高离群率下点云的纯旋转配准，比现有最先进的SDP方法快两个数量级。", "motivation": "点云配准中的截断最小二乘（TLS）公式能够处理高达95%的离群点，但将其组合优化问题求解到全局最优极具挑战性。现有的基于半定规划（SDP）松弛的方法，如STRIDE，在处理100个点时需要数百秒，计算成本过高。", "method": "本文提出了一种新颖的线性时间凸松弛方法，并结合收缩器（contractor）方法来加速分支定界（BnB）算法。该方法专注于解决纯旋转（rotation-only）的TLS问题，且要求提供旋转轴。", "result": "在提供旋转轴的情况下，该求解器能够在不到半秒的时间内对100个点的3D点云实现可证明的全局最优配准。与最先进的SDP求解器STRIDE相比，在解决纯旋转TLS问题时，速度快了两个数量级。除了形式化的全局最优性证明外，还通过对抗性实例提供了经验证据。", "conclusion": "本研究为高离群率下的点云纯旋转配准问题提供了一个显著加速且可证明全局最优的解决方案。尽管目前仅限于纯旋转问题且需提供旋转轴，但其在速度上远超现有技术，为未来的研究奠定了基础。"}}
{"id": "2508.15769", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15769", "abs": "https://arxiv.org/abs/2508.15769", "authors": ["Yanxu Meng", "Haoning Wu", "Ya Zhang", "Weidi Xie"], "title": "SceneGen: Single-Image 3D Scene Generation in One Feedforward Pass", "comment": "Technical Report; Project Page: https://mengmouxu.github.io/SceneGen", "summary": "3D content generation has recently attracted significant research interest\ndue to its applications in VR/AR and embodied AI. In this work, we address the\nchallenging task of synthesizing multiple 3D assets within a single scene\nimage. Concretely, our contributions are fourfold: (i) we present SceneGen, a\nnovel framework that takes a scene image and corresponding object masks as\ninput, simultaneously producing multiple 3D assets with geometry and texture.\nNotably, SceneGen operates with no need for optimization or asset retrieval;\n(ii) we introduce a novel feature aggregation module that integrates local and\nglobal scene information from visual and geometric encoders within the feature\nextraction module. Coupled with a position head, this enables the generation of\n3D assets and their relative spatial positions in a single feedforward pass;\n(iii) we demonstrate SceneGen's direct extensibility to multi-image input\nscenarios. Despite being trained solely on single-image inputs, our\narchitectural design enables improved generation performance with multi-image\ninputs; and (iv) extensive quantitative and qualitative evaluations confirm the\nefficiency and robust generation abilities of our approach. We believe this\nparadigm offers a novel solution for high-quality 3D content generation,\npotentially advancing its practical applications in downstream tasks. The code\nand model will be publicly available at: https://mengmouxu.github.io/SceneGen.", "AI": {"tldr": "本文提出了SceneGen，一个新颖的框架，能够从单张场景图像和对应物体掩码中，无需优化或检索，一次性生成多个带有几何和纹理的3D资产及其相对空间位置。", "motivation": "3D内容生成在VR/AR和具身AI中具有广泛应用，但从单张场景图像中合成多个3D资产是一个具有挑战性的任务。", "method": "本文提出了SceneGen框架，以场景图像和物体掩码为输入，同时生成多个3D资产的几何和纹理。其核心包括：(i) 一个新颖的特征聚合模块，整合来自视觉和几何编码器的局部及全局场景信息；(ii) 一个位置头部，使得3D资产及其相对空间位置能在一次前向传播中生成；(iii) 尽管仅在单图像输入上训练，但其架构设计使其能直接扩展到多图像输入场景，并提高生成性能。", "result": "广泛的定量和定性评估证实了SceneGen方法的高效性和鲁棒的生成能力。即使在单图像输入上训练，该方法在多图像输入场景下也能实现性能提升。", "conclusion": "该范式为高质量3D内容生成提供了一种新颖的解决方案，有望推动其在下游任务中的实际应用。"}}
{"id": "2508.15629", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15629", "abs": "https://arxiv.org/abs/2508.15629", "authors": ["Hao Chen", "Fang Qiu", "Li An", "Douglas Stow", "Eve Bohnett", "Haitao Lyu", "Shuang Tian"], "title": "Multi-perspective monitoring of wildlife and human activities from camera traps and drones with deep learning models", "comment": null, "summary": "Wildlife and human activities are key components of landscape systems.\nUnderstanding their spatial distribution is essential for evaluating human\nwildlife interactions and informing effective conservation planning.\nMultiperspective monitoring of wildlife and human activities by combining\ncamera traps and drone imagery. Capturing the spatial patterns of their\ndistributions, which allows the identification of the overlap of their activity\nzones and the assessment of the degree of human wildlife conflict. The study\nwas conducted in Chitwan National Park (CNP), Nepal, and adjacent regions.\nImages collected by visible and nearinfrared camera traps and thermal infrared\ndrones from February to July 2022 were processed to create training and testing\ndatasets, which were used to build deep learning models to automatic identify\nwildlife and human activities. Drone collected thermal imagery was used for\ndetecting targets to provide a multiple monitoring perspective. Spatial pattern\nanalysis was performed to identify animal and resident activity hotspots and\ndelineation potential human wildlife conflict zones. Among the deep learning\nmodels tested, YOLOv11s achieved the highest performance with a precision of\n96.2%, recall of 92.3%, mAP50 of 96.7%, and mAP50 of 81.3%, making it the most\neffective for detecting objects in camera trap imagery. Drone based thermal\nimagery, analyzed with an enhanced Faster RCNN model, added a complementary\naerial viewpoint for camera trap detections. Spatial pattern analysis\nidentified clear hotspots for both wildlife and human activities and their\noverlapping patterns within certain areas in the CNP and buffer zones\nindicating potential conflict. This study reveals human wildlife conflicts\nwithin the conserved landscape. Integrating multiperspective monitoring with\nautomated object detection enhances wildlife surveillance and landscape\nmanagement.", "AI": {"tldr": "本研究结合相机陷阱和无人机多视角监测野生动物和人类活动，利用深度学习模型自动识别目标，并通过空间模式分析识别活动热点及潜在人兽冲突区域，以促进野生动物监测和景观管理。", "motivation": "了解野生动物和人类活动的空间分布对于评估人兽互动和制定有效的保护计划至关重要。研究旨在通过捕捉其分布的空间模式，识别活动区域重叠并评估人兽冲突的程度。", "method": "研究在尼泊尔奇特旺国家公园（CNP）及其邻近区域进行。方法包括：1) 使用可见光/近红外相机陷阱和热红外无人机（2022年2月至7月）收集图像数据；2) 处理图像创建训练和测试数据集；3) 构建深度学习模型自动识别野生动物和人类活动（相机陷阱图像采用YOLOv11s，无人机热图像采用增强型Faster RCNN）；4) 进行空间模式分析，识别动物和居民活动热点，并划定潜在的人兽冲突区。", "result": "在测试的深度学习模型中，YOLOv11s在相机陷阱图像检测中表现最佳，精度达96.2%，召回率92.3%，mAP50为96.7%，mAP50_95为81.3%。无人机热成像数据通过增强型Faster RCNN模型分析，提供了相机陷阱检测的互补空中视角。空间模式分析明确识别了野生动物和人类活动的清晰热点，并在CNP和缓冲区内的某些区域发现了它们的重叠模式，表明存在潜在冲突。", "conclusion": "本研究揭示了保护区景观内存在人兽冲突。整合多视角监测与自动化目标检测技术，能够有效增强野生动物监测和景观管理能力。"}}
{"id": "2508.15641", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15641", "abs": "https://arxiv.org/abs/2508.15641", "authors": ["Pengcheng Fang", "Yuxia Chen", "Rui Guo"], "title": "When and What: Diffusion-Grounded VideoLLM with Entity Aware Segmentation for Long Video Understanding", "comment": null, "summary": "Understanding videos requires more than answering open ended questions, it\ndemands the ability to pinpoint when events occur and how entities interact\nacross time. While recent Video LLMs have achieved remarkable progress in\nholistic reasoning, they remain coarse in temporal perception: timestamps are\nencoded only implicitly, frame level features are weak in capturing continuity,\nand language vision alignment often drifts from the entities of interest. In\nthis paper, we present Grounded VideoDiT, a Video LLM designed to overcome\nthese limitations by introducing three key innovations. First, a Diffusion\nTemporal Latent (DTL) encoder enhances boundary sensitivity and maintains\ntemporal consistency. Second, object grounded representations explicitly bind\nquery entities to localized visual evidence, strengthening alignment. Third, a\nmixed token scheme with discrete temporal tokens provides explicit timestamp\nmodeling, enabling fine grained temporal reasoning. Together, these designs\nequip Grounded VideoDiT with robust grounding capabilities, as validated by\nstate of the art results on Charades STA, NExT GQA, and multiple VideoQA\nbenchmarks.", "AI": {"tldr": "解析错误", "motivation": "解析错误", "method": "解析错误", "result": "解析错误", "conclusion": "解析错误"}}
{"id": "2508.15646", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15646", "abs": "https://arxiv.org/abs/2508.15646", "authors": ["Swann Emilien Céleste Destouches", "Jesse Lahaye", "Laurent Valentin Jospin", "Jan Skaloud"], "title": "Weakly-Supervised Learning for Tree Instances Segmentation in Airborne Lidar Point Clouds", "comment": "8 pages, 9 figures", "summary": "Tree instance segmentation of airborne laser scanning (ALS) data is of utmost\nimportance for forest monitoring, but remains challenging due to variations in\nthe data caused by factors such as sensor resolution, vegetation state at\nacquisition time, terrain characteristics, etc. Moreover, obtaining a\nsufficient amount of precisely labeled data to train fully supervised instance\nsegmentation methods is expensive. To address these challenges, we propose a\nweakly supervised approach where labels of an initial segmentation result\nobtained either by a non-finetuned model or a closed form algorithm are\nprovided as a quality rating by a human operator. The labels produced during\nthe quality assessment are then used to train a rating model, whose task is to\nclassify a segmentation output into the same classes as specified by the human\noperator. Finally, the segmentation model is finetuned using feedback from the\nrating model. This in turn improves the original segmentation model by 34\\% in\nterms of correctly identified tree instances while considerably reducing the\nnumber of non-tree instances predicted. Challenges still remain in data over\nsparsely forested regions characterized by small trees (less than two meters in\nheight) or within complex surroundings containing shrubs, boulders, etc. which\ncan be confused as trees where the performance of the proposed method is\nreduced.", "AI": {"tldr": "本文提出了一种弱监督方法，通过人工对初始分割结果进行质量评估并训练一个评估模型，然后利用该评估模型的反馈来微调树木实例分割模型，从而显著提升了机载激光雷达数据的树木分割精度。", "motivation": "机载激光雷达数据中的树木实例分割对森林监测至关重要，但由于传感器分辨率、植被状态、地形等因素导致的数据差异，以及精确标注数据成本高昂，使其面临巨大挑战。", "method": "该方法采用弱监督策略。首先，通过非微调模型或封闭形式算法获得初始分割结果。然后，由人工操作员对这些结果进行质量评级。接着，利用人工评级数据训练一个评估模型，该模型旨在将分割输出分类为与人工评级相同的类别。最后，使用评估模型的反馈来微调原始的分割模型。", "result": "该方法使正确识别的树木实例数量提高了34%，同时显著减少了非树木实例的预测。然而，在稀疏森林区域（小树，高度小于两米）或包含灌木、巨石等复杂环境的数据中，该方法的性能有所下降，这些物体容易被误认为是树木。", "conclusion": "所提出的弱监督方法有效提高了机载激光雷达数据的树木实例分割精度，并通过人工质量评估和评估模型反馈实现了模型微调。尽管在特定复杂或稀疏区域仍存在挑战，但整体性能显著提升。"}}
{"id": "2508.15653", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15653", "abs": "https://arxiv.org/abs/2508.15653", "authors": ["Ziyang Yan", "Ruikai Li", "Zhiyong Cui", "Bohan Li", "Han Jiang", "Yilong Ren", "Aoyong Li", "Zhenning Li", "Sijia Wen", "Haiyang Yu"], "title": "MapKD: Unlocking Prior Knowledge with Cross-Modal Distillation for Efficient Online HD Map Construction", "comment": null, "summary": "Online HD map construction is a fundamental task in autonomous driving\nsystems, aiming to acquire semantic information of map elements around the ego\nvehicle based on real-time sensor inputs. Recently, several approaches have\nachieved promising results by incorporating offline priors such as SD maps and\nHD maps or by fusing multi-modal data. However, these methods depend on stale\noffline maps and multi-modal sensor suites, resulting in avoidable\ncomputational overhead at inference. To address these limitations, we employ a\nknowledge distillation strategy to transfer knowledge from multimodal models\nwith prior knowledge to an efficient, low-cost, and vision-centric student\nmodel. Specifically, we propose MapKD, a novel multi-level cross-modal\nknowledge distillation framework with an innovative Teacher-Coach-Student (TCS)\nparadigm. This framework consists of: (1) a camera-LiDAR fusion model with\nSD/HD map priors serving as the teacher; (2) a vision-centric coach model with\nprior knowledge and simulated LiDAR to bridge the cross-modal knowledge\ntransfer gap; and (3) a lightweight vision-based student model. Additionally,\nwe introduce two targeted knowledge distillation strategies: Token-Guided 2D\nPatch Distillation (TGPD) for bird's eye view feature alignment and Masked\nSemantic Response Distillation (MSRD) for semantic learning guidance. Extensive\nexperiments on the challenging nuScenes dataset demonstrate that MapKD improves\nthe student model by +6.68 mIoU and +10.94 mAP while simultaneously\naccelerating inference speed. The code is available\nat:https://github.com/2004yan/MapKD2026.", "AI": {"tldr": "本文提出MapKD，一个基于教师-教练-学生（TCS）范式的多级跨模态知识蒸馏框架，旨在将多模态模型的知识转移到高效、轻量级的纯视觉学生模型中，以实现在线高清地图构建，同时提高性能并加速推理。", "motivation": "现有的在线高清地图构建方法依赖于过时的离线地图或多模态传感器套件，导致推理时产生不必要的计算开销。研究旨在解决这些局限性，开发一个高效、低成本且以视觉为中心的模型。", "method": "本文提出了MapKD框架，采用知识蒸馏策略，将多模态模型的知识转移到纯视觉学生模型。该框架包括：1) 一个以相机-激光雷达融合模型和SD/HD地图先验知识为基础的教师模型；2) 一个具有先验知识和模拟激光雷达的纯视觉教练模型，用于弥合跨模态知识转移的鸿沟；3) 一个轻量级的纯视觉学生模型。此外，引入了两种知识蒸馏策略：Token-Guided 2D Patch Distillation (TGPD) 用于鸟瞰图特征对齐，以及Masked Semantic Response Distillation (MSRD) 用于语义学习指导。", "result": "在nuScenes数据集上的实验表明，MapKD使学生模型的mIoU提高了+6.68，mAP提高了+10.94，同时显著加快了推理速度。", "conclusion": "MapKD通过创新的教师-教练-学生范式和特定的知识蒸馏策略，成功地将知识从复杂的、多模态的模型转移到高效、轻量级的纯视觉模型中，从而在在线高清地图构建任务中实现了更好的性能和更快的推理速度。"}}
{"id": "2508.15688", "categories": ["cs.CV", "I.4.10"], "pdf": "https://arxiv.org/pdf/2508.15688", "abs": "https://arxiv.org/abs/2508.15688", "authors": ["Yongju Jia", "Jiarui Ma", "Xiangxian Li", "Baiqiao Zhang", "Xianhui Cao", "Juan Liu", "Yulong Bian"], "title": "LLM-empowered Dynamic Prompt Routing for Vision-Language Models Tuning under Long-Tailed Distributions", "comment": "accepted by EMNLP 2025", "summary": "Pre-trained vision-language models (VLMs), such as CLIP, have demonstrated\nimpressive capability in visual tasks, but their fine-tuning often suffers from\nbias in class-imbalanced scene. Recent works have introduced large language\nmodels (LLMs) to enhance VLM fine-tuning with supplementing semantic\ninformation. However, they often overlook inherent class imbalance in VLMs'\npre-training, which may lead to bias accumulation in downstream tasks. To\naddress this problem, this paper proposes a Multi-dimensional Dynamic Prompt\nRouting (MDPR) framework. MDPR constructs a comprehensive knowledge base for\nclasses, spanning five visual-semantic dimensions. During fine-tuning, the\ndynamic routing mechanism aligns global visual classes, retrieves optimal\nprompts, and balances fine-grained semantics, yielding stable predictions\nthrough logits fusion. Extensive experiments on long-tailed benchmarks,\nincluding CIFAR-LT, ImageNet-LT, and Places-LT, demonstrate that MDPR achieves\ncomparable results with current SOTA methods. Ablation studies further confirm\nthe effectiveness of our semantic library for tail classes, and show that our\ndynamic routing incurs minimal computational overhead, making MDPR a flexible\nand efficient enhancement for VLM fine-tuning under data imbalance.", "AI": {"tldr": "针对VLM在类不平衡场景下微调的偏差问题，本文提出了MDPR框架，通过构建多维度知识库和动态提示路由机制，有效平衡了细粒度语义并实现了稳定的预测，在长尾基准测试中取得了与SOTA相当的性能，且计算开销极小。", "motivation": "预训练视觉-语言模型（VLMs）在类不平衡场景下微调时易受偏差影响。现有利用大型语言模型（LLMs）增强VLM微调的方法，往往忽略了VLM预训练中固有的类不平衡问题，这可能导致下游任务中偏差的累积。因此，需要提出一种方法来解决VLM微调中由类不平衡引起的偏差积累问题。", "method": "本文提出了多维度动态提示路由（MDPR）框架。该框架为类别构建了一个涵盖五个视觉-语义维度的综合知识库。在微调过程中，动态路由机制负责对齐全局视觉类别、检索最优提示，并平衡细粒度语义，最终通过logits融合实现稳定的预测。", "result": "在CIFAR-LT、ImageNet-LT和Places-LT等长尾基准测试中，MDPR取得了与当前最先进方法相当的性能。消融研究进一步证实了其语义库对尾部类别的有效性，并表明动态路由的计算开销极小。", "conclusion": "MDPR框架是一种灵活且高效的增强方案，用于解决数据不平衡下VLM微调中的偏差积累问题，并能有效提升其性能。"}}
{"id": "2508.15720", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15720", "abs": "https://arxiv.org/abs/2508.15720", "authors": ["Zhiheng Liu", "Xueqing Deng", "Shoufa Chen", "Angtian Wang", "Qiushan Guo", "Mingfei Han", "Zeyue Xue", "Mengzhao Chen", "Ping Luo", "Linjie Yang"], "title": "WorldWeaver: Generating Long-Horizon Video Worlds via Rich Perception", "comment": "Project page: https://johanan528.github.io/worldweaver_web/", "summary": "Generative video modeling has made significant strides, yet ensuring\nstructural and temporal consistency over long sequences remains a challenge.\nCurrent methods predominantly rely on RGB signals, leading to accumulated\nerrors in object structure and motion over extended durations. To address these\nissues, we introduce WorldWeaver, a robust framework for long video generation\nthat jointly models RGB frames and perceptual conditions within a unified\nlong-horizon modeling scheme. Our training framework offers three key\nadvantages. First, by jointly predicting perceptual conditions and color\ninformation from a unified representation, it significantly enhances temporal\nconsistency and motion dynamics. Second, by leveraging depth cues, which we\nobserve to be more resistant to drift than RGB, we construct a memory bank that\npreserves clearer contextual information, improving quality in long-horizon\nvideo generation. Third, we employ segmented noise scheduling for training\nprediction groups, which further mitigates drift and reduces computational\ncost. Extensive experiments on both diffusion- and rectified flow-based models\ndemonstrate the effectiveness of WorldWeaver in reducing temporal drift and\nimproving the fidelity of generated videos.", "AI": {"tldr": "WorldWeaver是一个用于长视频生成的鲁棒框架，通过联合建模RGB帧和感知条件（特别是深度信息）以及采用分段噪声调度，有效解决了现有方法在长序列中结构和时间一致性差、漂移累积的问题，显著提升了生成视频的质量和保真度。", "motivation": "生成式视频模型在长序列中保持结构和时间一致性方面面临挑战。现有方法主要依赖RGB信号，导致物体结构和运动在长时间内累积误差。", "method": "本文提出了WorldWeaver框架，其训练方法具有三个优势：1. 从统一表示中联合预测感知条件和颜色信息，增强时间一致性和运动动态。2. 利用深度线索（比RGB更抗漂移）构建记忆库，保存更清晰的上下文信息，提高长视频生成质量。3. 采用分段噪声调度训练预测组，进一步减轻漂移并降低计算成本。", "result": "在基于扩散和整流流的模型上进行的广泛实验表明，WorldWeaver在减少时间漂移和提高生成视频保真度方面表现出有效性。", "conclusion": "WorldWeaver通过联合建模RGB和感知条件，并利用深度线索构建记忆库以及采用分段噪声调度，成功解决了长视频生成中的一致性挑战，显著提升了生成视频的质量和稳定性。"}}
{"id": "2508.15751", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15751", "abs": "https://arxiv.org/abs/2508.15751", "authors": ["Xueyuan Li", "Can Cui", "Ruining Deng", "Yucheng Tang", "Quan Liu", "Tianyuan Yao", "Shunxing Bao", "Naweed Chowdhury", "Haichun Yang", "Yuankai Huo"], "title": "Fine-grained Multi-class Nuclei Segmentation with Molecular-empowered All-in-SAM Model", "comment": "25 pages, 3 figures, accepted by Journal of Medical Imaging", "summary": "Purpose: Recent developments in computational pathology have been driven by\nadvances in Vision Foundation Models, particularly the Segment Anything Model\n(SAM). This model facilitates nuclei segmentation through two primary methods:\nprompt-based zero-shot segmentation and the use of cell-specific SAM models for\ndirect segmentation. These approaches enable effective segmentation across a\nrange of nuclei and cells. However, general vision foundation models often face\nchallenges with fine-grained semantic segmentation, such as identifying\nspecific nuclei subtypes or particular cells. Approach: In this paper, we\npropose the molecular-empowered All-in-SAM Model to advance computational\npathology by leveraging the capabilities of vision foundation models. This\nmodel incorporates a full-stack approach, focusing on: (1) annotation-engaging\nlay annotators through molecular-empowered learning to reduce the need for\ndetailed pixel-level annotations, (2) learning-adapting the SAM model to\nemphasize specific semantics, which utilizes its strong generalizability with\nSAM adapter, and (3) refinement-enhancing segmentation accuracy by integrating\nMolecular-Oriented Corrective Learning (MOCL). Results: Experimental results\nfrom both in-house and public datasets show that the All-in-SAM model\nsignificantly improves cell classification performance, even when faced with\nvarying annotation quality. Conclusions: Our approach not only reduces the\nworkload for annotators but also extends the accessibility of precise\nbiomedical image analysis to resource-limited settings, thereby advancing\nmedical diagnostics and automating pathology image analysis.", "AI": {"tldr": "本文提出了一个分子赋能的All-in-SAM模型，通过减少标注工作量、适应特定语义和整合分子导向纠正学习，显著提升了计算病理学中细胞分类的精细分割性能。", "motivation": "尽管视觉基础模型（如SAM）在细胞核分割方面表现出色，但它们在识别特定细胞核亚型或细胞等精细语义分割任务上仍面临挑战。此外，详细的像素级标注工作量巨大。", "method": "本文提出了All-in-SAM模型，采用全栈方法：1) 通过分子赋能学习吸引非专业标注员，减少对详细像素级标注的需求；2) 利用SAM适配器，调整SAM模型以强调特定语义，保持其泛化能力；3) 通过整合分子导向纠正学习（MOCL）来提高分割精度。", "result": "在内部和公共数据集上的实验结果表明，All-in-SAM模型显著提高了细胞分类性能，即使在标注质量不一致的情况下也表现良好。", "conclusion": "该方法不仅减轻了标注员的工作负担，还使精准的生物医学图像分析在资源有限的环境中更易于实现，从而推动了医学诊断和病理图像分析的自动化。"}}
{"id": "2508.15761", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15761", "abs": "https://arxiv.org/abs/2508.15761", "authors": ["Yifu Zhang", "Hao Yang", "Yuqi Zhang", "Yifei Hu", "Fengda Zhu", "Chuang Lin", "Xiaofeng Mei", "Yi Jiang", "Zehuan Yuan", "Bingyue Peng"], "title": "Waver: Wave Your Way to Lifelike Video Generation", "comment": null, "summary": "We present Waver, a high-performance foundation model for unified image and\nvideo generation. Waver can directly generate videos with durations ranging\nfrom 5 to 10 seconds at a native resolution of 720p, which are subsequently\nupscaled to 1080p. The model simultaneously supports text-to-video (T2V),\nimage-to-video (I2V), and text-to-image (T2I) generation within a single,\nintegrated framework. We introduce a Hybrid Stream DiT architecture to enhance\nmodality alignment and accelerate training convergence. To ensure training data\nquality, we establish a comprehensive data curation pipeline and manually\nannotate and train an MLLM-based video quality model to filter for the\nhighest-quality samples. Furthermore, we provide detailed training and\ninference recipes to facilitate the generation of high-quality videos. Building\non these contributions, Waver excels at capturing complex motion, achieving\nsuperior motion amplitude and temporal consistency in video synthesis. Notably,\nit ranks among the Top 3 on both the T2V and I2V leaderboards at Artificial\nAnalysis (data as of 2025-07-30 10:00 GMT+8), consistently outperforming\nexisting open-source models and matching or surpassing state-of-the-art\ncommercial solutions. We hope this technical report will help the community\nmore efficiently train high-quality video generation models and accelerate\nprogress in video generation technologies. Official page:\nhttps://github.com/FoundationVision/Waver.", "AI": {"tldr": "Waver是一种高性能的统一图像和视频生成基础模型，能够生成高质量、长时间（5-10秒）的视频，并在运动捕捉和时间一致性方面表现出色，超越了现有开源模型并与商业解决方案媲美。", "motivation": "研究动机是开发一个能够统一支持文本到视频(T2V)、图像到视频(I2V)和文本到图像(T2I)生成，并能产生高质量、具有复杂运动和良好时间一致性的长视频的高性能基础模型。", "method": "该研究引入了混合流DiT架构以增强模态对齐并加速训练收敛。为了确保训练数据质量，建立了全面的数据整理流程，并手动标注和训练了一个基于MLLM的视频质量模型来筛选高质量样本。模型能直接生成720p分辨率（5-10秒）的视频，随后升级到1080p。此外，还提供了详细的训练和推理方案。", "result": "Waver能够直接生成5到10秒的720p视频并升采样至1080p。它在捕捉复杂运动方面表现出色，实现了卓越的运动幅度和时间一致性。该模型在Artificial Analysis的T2V和I2V排行榜上均位列前三，持续超越现有开源模型，并与最先进的商业解决方案持平或超越。", "conclusion": "Waver模型在统一图像和视频生成方面取得了显著进展，通过其创新的架构和数据策略，实现了卓越的视频质量、运动捕捉和时间一致性。该技术报告旨在帮助社区更高效地训练高质量视频生成模型，并加速视频生成技术的发展。"}}
{"id": "2508.15767", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15767", "abs": "https://arxiv.org/abs/2508.15767", "authors": ["Jinhyung Park", "Javier Romero", "Shunsuke Saito", "Fabian Prada", "Takaaki Shiratori", "Yichen Xu", "Federica Bogo", "Shoou-I Yu", "Kris Kitani", "Rawal Khirodkar"], "title": "ATLAS: Decoupling Skeletal and Shape Parameters for Expressive Parametric Human Modeling", "comment": "ICCV 2025; Website: https://jindapark.github.io/projects/atlas/", "summary": "Parametric body models offer expressive 3D representation of humans across a\nwide range of poses, shapes, and facial expressions, typically derived by\nlearning a basis over registered 3D meshes. However, existing human mesh\nmodeling approaches struggle to capture detailed variations across diverse body\nposes and shapes, largely due to limited training data diversity and\nrestrictive modeling assumptions. Moreover, the common paradigm first optimizes\nthe external body surface using a linear basis, then regresses internal\nskeletal joints from surface vertices. This approach introduces problematic\ndependencies between internal skeleton and outer soft tissue, limiting direct\ncontrol over body height and bone lengths. To address these issues, we present\nATLAS, a high-fidelity body model learned from 600k high-resolution scans\ncaptured using 240 synchronized cameras. Unlike previous methods, we explicitly\ndecouple the shape and skeleton bases by grounding our mesh representation in\nthe human skeleton. This decoupling enables enhanced shape expressivity,\nfine-grained customization of body attributes, and keypoint fitting independent\nof external soft-tissue characteristics. ATLAS outperforms existing methods by\nfitting unseen subjects in diverse poses more accurately, and quantitative\nevaluations show that our non-linear pose correctives more effectively capture\ncomplex poses compared to linear models.", "AI": {"tldr": "ATLAS是一种高精度人体模型，通过将网格表示基于人体骨骼，明确解耦形状和骨骼基，并利用大规模高分辨率扫描数据学习，克服了现有模型在细节变化和骨骼-软组织依赖方面的局限性。", "motivation": "现有的人体网格建模方法难以捕捉多样姿态和形状的详细变化，主要原因在于训练数据多样性有限和建模假设的限制。此外，先优化外部表面再回归内部骨骼的范式，导致骨骼与软组织之间存在问题性依赖，限制了对身高和骨长的直接控制。", "method": "本文提出了ATLAS模型，从60万个使用240个同步相机捕获的高分辨率扫描中学习。与以往方法不同，ATLAS通过将网格表示基于人体骨骼，明确解耦了形状和骨骼基，并使用了非线性姿态校正器。", "result": "ATLAS在拟合未见过的受试者和多样姿态方面比现有方法更准确。定量评估显示，其非线性姿态校正器比线性模型能更有效地捕捉复杂姿态。", "conclusion": "ATLAS模型实现了增强的形状表达能力、身体属性的精细化定制，以及独立于外部软组织特征的关键点拟合，显著优于现有的人体模型。"}}
{"id": "2508.15772", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.15772", "abs": "https://arxiv.org/abs/2508.15772", "authors": ["Qingyang Mao", "Qi Cai", "Yehao Li", "Yingwei Pan", "Mingyue Cheng", "Ting Yao", "Qi Liu", "Tao Mei"], "title": "Visual Autoregressive Modeling for Instruction-Guided Image Editing", "comment": "Source codes and models are available at\n  https://github.com/HiDream-ai/VAREdit", "summary": "Recent advances in diffusion models have brought remarkable visual fidelity\nto instruction-guided image editing. However, their global denoising process\ninherently entangles the edited region with the entire image context, leading\nto unintended spurious modifications and compromised adherence to editing\ninstructions. In contrast, autoregressive models offer a distinct paradigm by\nformulating image synthesis as a sequential process over discrete visual\ntokens. Their causal and compositional mechanism naturally circumvents the\nadherence challenges of diffusion-based methods. In this paper, we present\nVAREdit, a visual autoregressive (VAR) framework that reframes image editing as\na next-scale prediction problem. Conditioned on source image features and text\ninstructions, VAREdit generates multi-scale target features to achieve precise\nedits. A core challenge in this paradigm is how to effectively condition the\nsource image tokens. We observe that finest-scale source features cannot\neffectively guide the prediction of coarser target features. To bridge this\ngap, we introduce a Scale-Aligned Reference (SAR) module, which injects\nscale-matched conditioning information into the first self-attention layer.\nVAREdit demonstrates significant advancements in both editing adherence and\nefficiency. On standard benchmarks, it outperforms leading diffusion-based\nmethods by 30\\%+ higher GPT-Balance score. Moreover, it completes a\n$512\\times512$ editing in 1.2 seconds, making it 2.2$\\times$ faster than the\nsimilarly sized UltraEdit. The models are available at\nhttps://github.com/HiDream-ai/VAREdit.", "AI": {"tldr": "VAREdit是一个视觉自回归(VAR)图像编辑框架，它将图像编辑重新定义为下一尺度预测问题。通过引入尺度对齐参考(SAR)模块，VAREdit在编辑依从性和效率方面显著优于扩散模型。", "motivation": "扩散模型在指令引导的图像编辑中存在全局去噪过程导致不必要的修改和对编辑指令依从性不足的问题。自回归模型通过离散视觉token的顺序生成范式，自然地避免了这些挑战。", "method": "VAREdit将图像编辑重构为下一尺度预测问题，是一个视觉自回归(VAR)框架。它在源图像特征和文本指令的条件下生成多尺度目标特征。为有效条件化源图像token，特别是解决精细尺度源特征无法有效指导粗糙目标特征预测的问题，VAREdit引入了尺度对齐参考(SAR)模块，将尺度匹配的条件信息注入到第一个自注意力层。", "result": "VAREdit在编辑依从性和效率上均表现出色。在标准基准测试中，其GPT-Balance分数比领先的扩散模型高出30%以上。此外，它能在1.2秒内完成512x512的编辑，比同等大小的UltraEdit快2.2倍。", "conclusion": "VAREdit提供了一个基于视觉自回归范式的高效且精确的图像编辑解决方案。通过将编辑重新定义为下一尺度预测并引入SAR模块，它成功克服了扩散模型在编辑依从性上的局限性，并显著提升了编辑速度。"}}
{"id": "2508.15773", "categories": ["cs.CV", "cs.GR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.15773", "abs": "https://arxiv.org/abs/2508.15773", "authors": ["Gaurav Parmar", "Or Patashnik", "Daniil Ostashev", "Kuan-Chieh Wang", "Kfir Aberman", "Srinivasa Narasimhan", "Jun-Yan Zhu"], "title": "Scaling Group Inference for Diverse and High-Quality Generation", "comment": "Project website: https://www.cs.cmu.edu/~group-inference, GitHub:\n  https://github.com/GaParmar/group-inference", "summary": "Generative models typically sample outputs independently, and recent\ninference-time guidance and scaling algorithms focus on improving the quality\nof individual samples. However, in real-world applications, users are often\npresented with a set of multiple images (e.g., 4-8) for each prompt, where\nindependent sampling tends to lead to redundant results, limiting user choices\nand hindering idea exploration. In this work, we introduce a scalable group\ninference method that improves both the diversity and quality of a group of\nsamples. We formulate group inference as a quadratic integer assignment\nproblem: candidate outputs are modeled as graph nodes, and a subset is selected\nto optimize sample quality (unary term) while maximizing group diversity\n(binary term). To substantially improve runtime efficiency, we progressively\nprune the candidate set using intermediate predictions, allowing our method to\nscale up to large candidate sets. Extensive experiments show that our method\nsignificantly improves group diversity and quality compared to independent\nsampling baselines and recent inference algorithms. Our framework generalizes\nacross a wide range of tasks, including text-to-image, image-to-image, image\nprompting, and video generation, enabling generative models to treat multiple\noutputs as cohesive groups rather than independent samples.", "AI": {"tldr": "本文提出了一种可扩展的群组推理方法，通过将群组推理建模为二次整数分配问题，并在运行时进行渐进式剪枝，显著提高了生成模型输出群组的多样性和质量，解决了独立采样导致的冗余问题。", "motivation": "在实际应用中，用户通常会收到针对同一提示的多个（如4-8个）图像，而传统的独立采样方法往往导致结果冗余，限制了用户选择并阻碍了创意探索。因此，需要一种方法来提升生成输出群组的多样性和质量。", "method": "将群组推理公式化为一个二次整数分配问题：将候选输出建模为图节点，并选择一个子集以优化样本质量（一元项）同时最大化群组多样性（二元项）。为了提高运行时效率，该方法使用中间预测逐步剪枝候选集，使其能够扩展到大型候选集。", "result": "实验表明，与独立的采样基线和最近的推理算法相比，该方法显著提高了群组的多样性和质量。此外，该框架适用于多种任务，包括文本到图像、图像到图像、图像提示和视频生成。", "conclusion": "该研究使得生成模型能够将多个输出视为一个有凝聚力的群组，而不仅仅是独立的样本，从而在多种生成任务中提升了用户体验和探索能力。"}}
{"id": "2508.15774", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.15774", "abs": "https://arxiv.org/abs/2508.15774", "authors": ["Haonan Qiu", "Ning Yu", "Ziqi Huang", "Paul Debevec", "Ziwei Liu"], "title": "CineScale: Free Lunch in High-Resolution Cinematic Visual Generation", "comment": "CineScale is an extended work of FreeScale (ICCV 2025). Project Page:\n  https://eyeline-labs.github.io/CineScale/, Code Repo:\n  https://github.com/Eyeline-Labs/CineScale", "summary": "Visual diffusion models achieve remarkable progress, yet they are typically\ntrained at limited resolutions due to the lack of high-resolution data and\nconstrained computation resources, hampering their ability to generate\nhigh-fidelity images or videos at higher resolutions. Recent efforts have\nexplored tuning-free strategies to exhibit the untapped potential\nhigher-resolution visual generation of pre-trained models. However, these\nmethods are still prone to producing low-quality visual content with repetitive\npatterns. The key obstacle lies in the inevitable increase in high-frequency\ninformation when the model generates visual content exceeding its training\nresolution, leading to undesirable repetitive patterns deriving from the\naccumulated errors. In this work, we propose CineScale, a novel inference\nparadigm to enable higher-resolution visual generation. To tackle the various\nissues introduced by the two types of video generation architectures, we\npropose dedicated variants tailored to each. Unlike existing baseline methods\nthat are confined to high-resolution T2I and T2V generation, CineScale broadens\nthe scope by enabling high-resolution I2V and V2V synthesis, built atop\nstate-of-the-art open-source video generation frameworks. Extensive experiments\nvalidate the superiority of our paradigm in extending the capabilities of\nhigher-resolution visual generation for both image and video models.\nRemarkably, our approach enables 8k image generation without any fine-tuning,\nand achieves 4k video generation with only minimal LoRA fine-tuning. Generated\nvideo samples are available at our website:\nhttps://eyeline-labs.github.io/CineScale/.", "AI": {"tldr": "本文提出CineScale，一种新颖的推理范式，旨在解决视觉扩散模型在有限分辨率训练导致的生成高分辨率图像和视频时质量低下的问题，无需大量微调即可实现8k图像和4k视频生成。", "motivation": "视觉扩散模型通常在有限分辨率下训练，原因在于高分辨率数据稀缺和计算资源受限，这阻碍了它们生成高保真高分辨率图像或视频的能力。现有的免调优策略仍会产生重复模式的低质量视觉内容，主要问题在于模型生成超出训练分辨率的内容时，高频信息增加导致累积误差，从而产生不理想的重复模式。", "method": "本文提出了CineScale，一种用于实现更高分辨率视觉生成的新型推理范式。为解决两种视频生成架构引入的各种问题，作者提出了针对每种架构的专用变体。与现有基线方法仅限于高分辨率T2I和T2V生成不同，CineScale扩展了范围，支持基于最先进的开源视频生成框架进行高分辨率I2V和V2V合成。", "result": "广泛的实验验证了CineScale范式在扩展图像和视频模型更高分辨率视觉生成能力方面的优越性。值得注意的是，该方法无需任何微调即可实现8k图像生成，并通过最少的LoRA微调实现4k视频生成。", "conclusion": "CineScale成功地解决了扩散模型在高分辨率视觉生成方面的挑战，提供了一种卓越的推理范式，不仅拓宽了生成范围（支持I2V和V2V），而且在极少或不微调的情况下实现了令人印象深刻的高分辨率输出。"}}
