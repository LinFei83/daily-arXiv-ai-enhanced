{"id": "2508.21083", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.21083", "abs": "https://arxiv.org/abs/2508.21083", "authors": ["Kyohoon Jin", "Juhwan Choi", "Jungmin Yun", "Junho Lee", "Soojin Jang", "Youngbin Kim"], "title": "CoBA: Counterbias Text Augmentation for Mitigating Various Spurious Correlations via Semantic Triples", "comment": "Accepted at EMNLP 2025", "summary": "Deep learning models often learn and exploit spurious correlations in\ntraining data, using these non-target features to inform their predictions.\nSuch reliance leads to performance degradation and poor generalization on\nunseen data. To address these limitations, we introduce a more general form of\ncounterfactual data augmentation, termed counterbias data augmentation, which\nsimultaneously tackles multiple biases (e.g., gender bias, simplicity bias) and\nenhances out-of-distribution robustness. We present CoBA: CounterBias\nAugmentation, a unified framework that operates at the semantic triple level:\nfirst decomposing text into subject-predicate-object triples, then selectively\nmodifying these triples to disrupt spurious correlations. By reconstructing the\ntext from these adjusted triples, CoBA generates counterbias data that\nmitigates spurious patterns. Through extensive experiments, we demonstrate that\nCoBA not only improves downstream task performance, but also effectively\nreduces biases and strengthens out-of-distribution resilience, offering a\nversatile and robust solution to the challenges posed by spurious correlations.", "AI": {"tldr": "本文提出了CoBA（CounterBias Augmentation），一个在语义三元组层面操作的反偏差数据增强框架。它通过分解文本、修改三元组以破坏虚假关联，然后重建文本，从而同时解决多种偏差并增强模型在分布外数据上的鲁棒性。", "motivation": "深度学习模型常利用训练数据中的虚假关联进行预测，导致在未见过的数据上性能下降和泛化能力差。现有方法不足以同时解决多种偏差。", "method": "CoBA框架首先将文本分解为“主语-谓语-宾语”三元组，然后选择性地修改这些三元组以破坏虚假关联。最后，通过调整后的三元组重建文本，生成反偏差数据以缓解虚假模式。", "result": "CoBA不仅提高了下游任务的性能，还有效减少了偏差并增强了模型在分布外数据上的鲁棒性。", "conclusion": "CoBA为解决虚假关联带来的挑战提供了一个多功能且鲁棒的解决方案。"}}
{"id": "2508.21084", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.21084", "abs": "https://arxiv.org/abs/2508.21084", "authors": ["Jan Fillies", "Michael Peter Hoffmann", "Rebecca Reichel", "Roman Salzwedel", "Sven Bodemer", "Adrian Paschke"], "title": "Mapping Toxic Comments Across Demographics: A Dataset from German Public Broadcasting", "comment": "The paper has been accepted to the EMNLP 2025 main track", "summary": "A lack of demographic context in existing toxic speech datasets limits our\nunderstanding of how different age groups communicate online. In collaboration\nwith funk, a German public service content network, this research introduces\nthe first large-scale German dataset annotated for toxicity and enriched with\nplatform-provided age estimates. The dataset includes 3,024 human-annotated and\n30,024 LLM-annotated anonymized comments from Instagram, TikTok, and YouTube.\nTo ensure relevance, comments were consolidated using predefined toxic\nkeywords, resulting in 16.7\\% labeled as problematic. The annotation pipeline\ncombined human expertise with state-of-the-art language models, identifying key\ncategories such as insults, disinformation, and criticism of broadcasting fees.\nThe dataset reveals age-based differences in toxic speech patterns, with\nyounger users favoring expressive language and older users more often engaging\nin disinformation and devaluation. This resource provides new opportunities for\nstudying linguistic variation across demographics and supports the development\nof more equitable and age-aware content moderation systems.", "AI": {"tldr": "该研究引入了首个大规模德语有毒言论数据集，包含平台提供的年龄估算，揭示了不同年龄群体在线交流中的有毒言论模式差异。", "motivation": "现有有毒言论数据集缺乏人口统计学背景（特别是年龄信息），限制了对不同年龄群体在线沟通方式的理解，并阻碍了开发更公平、更具年龄意识的内容审核系统。", "method": "研究与德国公共服务内容网络funk合作，构建了一个包含3,024条人工标注和30,024条LLM标注的匿名评论数据集，数据来源于Instagram、TikTok和YouTube，并富含平台提供的年龄估算。评论通过预定义有毒关键词进行筛选。标注流程结合了人工专业知识和先进语言模型，识别出侮辱、虚假信息和广播费用批评等关键类别。", "result": "数据集中有16.7%的评论被标记为有问题。结果显示，有毒言论模式存在基于年龄的差异：年轻用户倾向于使用表达性语言，而年长用户则更多地参与虚假信息和贬低性言论。", "conclusion": "该数据集为研究跨人口统计学群体的语言变异提供了新机会，并支持开发更公平、更具年龄意识的内容审核系统。"}}
{"id": "2508.21085", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.21085", "abs": "https://arxiv.org/abs/2508.21085", "authors": ["Parul Awasthy", "Aashka Trivedi", "Yulong Li", "Meet Doshi", "Riyaz Bhat", "Vignesh P", "Vishwajeet Kumar", "Yushu Yang", "Bhavani Iyer", "Abraham Daniels", "Rudra Murthy", "Ken Barker", "Martin Franz", "Madison Lee", "Todd Ward", "Salim Roukos", "David Cox", "Luis Lastras", "Jaydeep Sen", "Radu Florian"], "title": "Granite Embedding R2 Models", "comment": null, "summary": "We introduce the Granite Embedding R2 models, a comprehensive family of\nhigh-performance English encoder-based embedding models engineered for\nenterprise-scale dense retrieval applications. Building upon our\nfirst-generation release, these models deliver substantial improvements,\nincluding 16x expanded context length (8,192 tokens), state-of-the-art\nperformance across diverse retrieval domains - text, code, long-document\nsearch, multi-turn conversational, and tabular data - and measurable speed\nadvantages of 19-44\\% over leading competitors while maintaining superior\naccuracy. Our release encompasses both bi-encoder and cross-encoder\narchitectures, featuring a highly effective 22-layer retriever model and its\nefficient 12-layer counterpart, alongside a high-quality reranker model, all\ntrained exclusively on enterprise-appropriate data with comprehensive\ngovernance oversight. The models demonstrate exceptional versatility across\nstandard benchmarks, IBM-developed evaluation suites, and real-world enterprise\nuse cases, establishing new performance standards for open-source embedding\nmodels. In an era where retrieval speed and accuracy are paramount for\ncompetitive advantage, the Granite R2 models deliver a compelling combination\nof cutting-edge performance, enterprise-ready licensing, and transparent data\nprovenance that organizations require for mission-critical deployments. All\nmodels are publicly available under the Apache 2.0 license at\nhttps://huggingface.co/collections/ibm-granite, enabling unrestricted research\nand commercial use.", "AI": {"tldr": "IBM发布了Granite Embedding R2模型家族，这是一套高性能、企业级的英文编码器嵌入模型，专为密集检索应用设计，具有更长的上下文长度、领先的性能、更快的速度和Apache 2.0开源许可。", "motivation": "在当前时代，检索速度和准确性对于企业获得竞争优势至关重要。研究旨在开发一套高性能、企业就绪且具备透明数据溯源的嵌入模型，以满足关键任务部署的需求。", "method": "该研究引入了基于编码器的Granite Embedding R2模型，包括双编码器（bi-encoder）和交叉编码器（cross-encoder）架构。模型包含一个高效的22层检索模型及其12层精简版，以及一个高质量的重排序模型。所有模型均使用符合企业规范的数据进行训练，并进行全面的治理监督。", "result": "Granite R2模型实现了显著改进，包括将上下文长度扩展16倍（达到8,192个token），在文本、代码、长文档搜索、多轮对话和表格数据等多种检索领域达到最先进的性能。与领先的竞争对手相比，模型在保持卓越准确性的同时，速度提升了19-44%。这些模型在标准基准测试、IBM开发的评估套件和实际企业用例中均展现出卓越的通用性，为开源嵌入模型树立了新的性能标准。", "conclusion": "Granite R2模型家族为企业级密集检索应用提供了尖端性能、企业就绪的许可和透明的数据溯源，满足了组织关键任务部署的需求。这些模型通过Apache 2.0许可公开可用，支持无限制的研究和商业用途，为开源嵌入模型设定了新的性能标准。"}}
{"id": "2508.21098", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.21098", "abs": "https://arxiv.org/abs/2508.21098", "authors": ["Zezhong Jin", "Shubhang Desai", "Xu Chen", "Biyi Fang", "Zhuoyi Huang", "Zhe Li", "Chong-Xin Gan", "Xiao Tu", "Man-Wai Mak", "Yan Lu", "Shujie Liu"], "title": "TrInk: Ink Generation with Transformer Network", "comment": "Accepted to EMNLP 2025 Main Conference", "summary": "In this paper, we propose TrInk, a Transformer-based model for ink\ngeneration, which effectively captures global dependencies. To better\nfacilitate the alignment between the input text and generated stroke points, we\nintroduce scaled positional embeddings and a Gaussian memory mask in the\ncross-attention module. Additionally, we design both subjective and objective\nevaluation pipelines to comprehensively assess the legibility and style\nconsistency of the generated handwriting. Experiments demonstrate that our\nTransformer-based model achieves a 35.56\\% reduction in character error rate\n(CER) and an 29.66% reduction in word error rate (WER) on the IAM-OnDB dataset\ncompared to previous methods. We provide an demo page with handwriting samples\nfrom TrInk and baseline models at: https://akahello-a11y.github.io/trink-demo/", "AI": {"tldr": "本文提出了TrInk，一个基于Transformer的墨迹生成模型，通过引入缩放位置嵌入和高斯记忆掩码来有效捕捉全局依赖并改善文本与笔画的对齐。该模型在IAM-OnDB数据集上显著降低了字符错误率和单词错误率。", "motivation": "现有墨迹生成模型在有效捕捉全局依赖以及输入文本与生成笔画点之间的对齐方面可能存在不足。", "method": "本文提出了TrInk模型，这是一个基于Transformer的墨迹生成模型。为了更好地促进输入文本与生成笔画点之间的对齐，该模型在交叉注意力模块中引入了缩放位置嵌入和高斯记忆掩码。此外，还设计了主观和客观评估流程来全面评估生成手写体的可读性和风格一致性。", "result": "实验表明，与现有方法相比，TrInk模型在IAM-OnDB数据集上将字符错误率（CER）降低了35.56%，将单词错误率（WER）降低了29.66%。", "conclusion": "TrInk模型在墨迹生成方面表现出色，通过有效捕捉全局依赖和改进文本与笔画对齐，显著提高了生成手写体的准确性。"}}
{"id": "2508.21263", "categories": ["eess.IV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.21263", "abs": "https://arxiv.org/abs/2508.21263", "authors": ["Roy M. Gabriel", "Mohammadreza Zandehshahvar", "Marly van Assen", "Nattakorn Kittisut", "Kyle Peters", "Carlo N. De Cecco", "Ali Adibi"], "title": "Deep Active Learning for Lung Disease Severity Classification from Chest X-rays: Learning with Less Data in the Presence of Class Imbalance", "comment": null, "summary": "To reduce the amount of required labeled data for lung disease severity\nclassification from chest X-rays (CXRs) under class imbalance, this study\napplied deep active learning with a Bayesian Neural Network (BNN) approximation\nand weighted loss function. This retrospective study collected 2,319 CXRs from\n963 patients (mean age, 59.2 $\\pm$ 16.6 years; 481 female) at Emory Healthcare\naffiliated hospitals between January and November 2020. All patients had\nclinically confirmed COVID-19. Each CXR was independently labeled by 3 to 6\nboard-certified radiologists as normal, moderate, or severe. A deep neural\nnetwork with Monte Carlo Dropout was trained using active learning to classify\ndisease severity. Various acquisition functions were used to iteratively select\nthe most informative samples from an unlabeled pool. Performance was evaluated\nusing accuracy, area under the receiver operating characteristic curve (AU\nROC), and area under the precision-recall curve (AU PRC). Training time and\nacquisition time were recorded. Statistical analysis included descriptive\nmetrics and performance comparisons across acquisition strategies. Entropy\nSampling achieved 93.7% accuracy (AU ROC, 0.91) in binary classification\n(normal vs. diseased) using 15.4% of the training data. In the multi-class\nsetting, Mean STD sampling achieved 70.3% accuracy (AU ROC, 0.86) using 23.1%\nof the labeled data. These methods outperformed more complex and\ncomputationally expensive acquisition functions and significantly reduced\nlabeling needs. Deep active learning with BNN approximation and weighted loss\neffectively reduces labeled data requirements while addressing class imbalance,\nmaintaining or exceeding diagnostic performance.", "AI": {"tldr": "本研究利用深度主动学习结合贝叶斯神经网络（BNN）近似和加权损失函数，在类别不平衡下有效减少了胸部X光片（CXR）肺部疾病严重程度分类所需的标注数据量。", "motivation": "在胸部X光片上进行肺部疾病严重程度分类时，通常需要大量标注数据，并且面临类别不平衡问题。研究旨在减少所需标注数据的数量。", "method": "本研究回顾性收集了2,319张来自963名COVID-19患者的CXR图像，由3至6名放射科医生独立标注为正常、中度或重度。采用带有Monte Carlo Dropout的深度神经网络，通过主动学习进行疾病严重程度分类。使用多种采集函数从无标签池中迭代选择信息量最大的样本。性能评估指标包括准确率、AU ROC和AU PRC，并记录了训练时间和采集时间。统计分析包括描述性指标和不同采集策略的性能比较。", "result": "在二分类（正常 vs. 患病）任务中，熵采样（Entropy Sampling）仅使用15.4%的训练数据就达到了93.7%的准确率（AU ROC为0.91）。在多分类任务中，均值标准差采样（Mean STD sampling）使用23.1%的标注数据达到了70.3%的准确率（AU ROC为0.86）。这些方法优于更复杂、计算成本更高的采集函数，并显著减少了标注需求。", "conclusion": "结合BNN近似和加权损失的深度主动学习方法，能够有效减少标注数据需求，同时解决类别不平衡问题，并保持或超越诊断性能。"}}
{"id": "2508.21138", "categories": ["eess.SY", "cond-mat.stat-mech", "cs.SY", "nlin.CG", "physics.soc-ph"], "pdf": "https://arxiv.org/pdf/2508.21138", "abs": "https://arxiv.org/abs/2508.21138", "authors": ["Yoshiyuki Yajima", "Hemant Prasad", "Daisuke Ikefuji", "Hitoshi Sakurai", "Manabu Otani"], "title": "Traffic State Estimation in Congestion to Extend Applicability of DFOS", "comment": "11 pages, 7 figures, presented in the 31st ITS World Congress", "summary": "This paper presents a traffic state estimation (TSE) method in congestion for\ndistributed fiber-optic sensing (DFOS). DFOS detects vehicle driving vibrations\nalong the optical fiber and obtains their trajectories in the spatiotemporal\nplane. From these trajectories, DFOS provides mean velocities for real-time\nspatially continuous traffic monitoring without dead zones. However, when\nvehicle vibration intensities are insufficiently low due to slow speed,\ntrajectories cannot be obtained, leading to missing values in mean velocity\ndata. It restricts DFOS applicability in severe congestion. Therefore, this\npaper proposes a missing value imputation method based on data assimilation.\nOur proposed method is validated on two expressways in Japan with the reference\ndata. The results show that the mean absolute error (MAE) of the imputed mean\nvelocities to the reference increases only by 1.5 km/h as compared with the MAE\nof non-missing values. This study enhances the wide-range applicability of DFOS\nin practical cases.", "AI": {"tldr": "本文提出了一种基于数据同化的缺失值插补方法，以解决分布式光纤传感（DFOS）在严重拥堵时因车速过慢导致车辆轨迹和平均速度数据缺失的问题，从而提升DFOS在交通状态估计中的适用性。", "motivation": "分布式光纤传感（DFOS）能够提供实时、空间连续的交通监测数据，但在严重拥堵时，车辆速度过慢导致振动强度不足，无法获取车辆轨迹和平均速度，从而产生大量缺失值，限制了DFOS在拥堵情况下的应用。", "method": "提出了一种基于数据同化的缺失值插补方法，用于恢复DFOS在拥堵状态下缺失的平均速度数据。", "result": "在日本两条高速公路上的验证结果显示，插补后的平均速度与参考数据相比，平均绝对误差（MAE）仅比非缺失值的MAE增加了1.5 km/h，表明该方法能有效处理缺失数据。", "conclusion": "本研究通过有效处理缺失值问题，显著增强了DFOS在实际交通状态估计，特别是在拥堵场景下的广泛适用性。"}}
{"id": "2508.21112", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.21112", "abs": "https://arxiv.org/abs/2508.21112", "authors": ["Delin Qu", "Haoming Song", "Qizhi Chen", "Zhaoqing Chen", "Xianqiang Gao", "Xinyi Ye", "Qi Lv", "Modi Shi", "Guanghui Ren", "Cheng Ruan", "Maoqing Yao", "Haoran Yang", "Jiacheng Bao", "Bin Zhao", "Dong Wang"], "title": "EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control", "comment": null, "summary": "The human ability to seamlessly perform multimodal reasoning and physical\ninteraction in the open world is a core goal for general-purpose embodied\nintelligent systems. Recent vision-language-action (VLA) models, which are\nco-trained on large-scale robot and visual-text data, have demonstrated notable\nprogress in general robot control. However, they still fail to achieve\nhuman-level flexibility in interleaved reasoning and interaction. In this work,\nintroduce EO-Robotics, consists of EO-1 model and EO-Data1.5M dataset. EO-1 is\na unified embodied foundation model that achieves superior performance in\nmultimodal embodied reasoning and robot control through interleaved\nvision-text-action pre-training. The development of EO-1 is based on two key\npillars: (i) a unified architecture that processes multimodal inputs\nindiscriminately (image, text, video, and action), and (ii) a massive,\nhigh-quality multimodal embodied reasoning dataset, EO-Data1.5M, which contains\nover 1.5 million samples with emphasis on interleaved vision-text-action\ncomprehension. EO-1 is trained through synergies between auto-regressive\ndecoding and flow matching denoising on EO-Data1.5M, enabling seamless robot\naction generation and multimodal embodied reasoning. Extensive experiments\ndemonstrate the effectiveness of interleaved vision-text-action learning for\nopen-world understanding and generalization, validated through a variety of\nlong-horizon, dexterous manipulation tasks across multiple embodiments. This\npaper details the architecture of EO-1, the data construction strategy of\nEO-Data1.5M, and the training methodology, offering valuable insights for\ndeveloping advanced embodied foundation models.", "AI": {"tldr": "本文介绍了EO-Robotics，包含EO-1模型和EO-Data1.5M数据集，旨在通过统一架构和大规模交错式视觉-文本-动作预训练，实现卓越的多模态具身推理和机器人控制。", "motivation": "当前视觉-语言-动作（VLA）模型在通用机器人控制方面取得进展，但在交错式推理和交互方面仍未能达到人类水平的灵活性，这阻碍了通用具身智能系统在开放世界中无缝执行多模态推理和物理交互。", "method": "本文提出了EO-1模型和EO-Data1.5M数据集。EO-1是一个统一的具身基础模型，采用统一架构无差别处理多模态输入（图像、文本、视频、动作）。EO-Data1.5M是一个大规模、高质量的多模态具身推理数据集，包含超过150万个样本，强调交错式视觉-文本-动作理解。EO-1通过自回归解码和流匹配去噪的协同作用在EO-Data1.5M上进行预训练。", "result": "EO-1在多模态具身推理和机器人控制方面表现出卓越性能，能够实现无缝的机器人动作生成和多模态具身推理。大量实验证明了交错式视觉-文本-动作学习对于开放世界理解和泛化的有效性，并在多种长程、灵巧操作任务中得到验证。", "conclusion": "本文详细介绍了EO-1的架构、EO-Data1.5M的数据构建策略和训练方法，为开发先进的具身基础模型提供了宝贵的见解，展示了交错式视觉-文本-动作学习在开放世界理解和泛化方面的有效性。"}}
{"id": "2508.21080", "categories": ["cs.CV", "cs.RO", "68T45 (Machine vision and scene understanding)", "I.2.10; I.4.8"], "pdf": "https://arxiv.org/pdf/2508.21080", "abs": "https://arxiv.org/abs/2508.21080", "authors": ["Ali K. AlShami", "Ryan Rabinowitz", "Maged Shoman", "Jianwu Fang", "Lukas Picek", "Shao-Yuan Lo", "Steve Cruz", "Khang Nhut Lam", "Nachiket Kamod", "Lei-Lei Li", "Jugal Kalita", "Terrance E. Boult"], "title": "2COOOL: 2nd Workshop on the Challenge Of Out-Of-Label Hazards in Autonomous Driving", "comment": "11 pages, 2 figures, Accepted to ICCV 2025 Workshop on Out-of-Label\n  Hazards in Autonomous Driving (2COOOL)", "summary": "As the computer vision community advances autonomous driving algorithms,\nintegrating vision-based insights with sensor data remains essential for\nimproving perception, decision making, planning, prediction, simulation, and\ncontrol. Yet we must ask: Why don't we have entirely safe self-driving cars\nyet? A key part of the answer lies in addressing novel scenarios, one of the\nmost critical barriers to real-world deployment. Our 2COOOL workshop provides a\ndedicated forum for researchers and industry experts to push the state of the\nart in novelty handling, including out-of-distribution hazard detection,\nvision-language models for hazard understanding, new benchmarking and\nmethodologies, and safe autonomous driving practices. The 2nd Workshop on the\nChallenge of Out-of-Label Hazards in Autonomous Driving (2COOOL) will be held\nat the International Conference on Computer Vision (ICCV) 2025 in Honolulu,\nHawaii, on October 19, 2025. We aim to inspire the development of new\nalgorithms and systems for hazard avoidance, drawing on ideas from anomaly\ndetection, open-set recognition, open-vocabulary modeling, domain adaptation,\nand related fields. Building on the success of its inaugural edition at the\nWinter Conference on Applications of Computer Vision (WACV) 2025, the workshop\nwill feature a mix of academic and industry participation.", "AI": {"tldr": "2COOOL研讨会旨在推动自动驾驶领域对新颖场景和标签外危险的处理能力，以提高安全性并促进实际部署。", "motivation": "尽管计算机视觉和传感器数据融合技术在自动驾驶中至关重要，但自动驾驶汽车仍未完全安全，主要障碍在于处理新颖场景和分布外危险。这促使了对异常处理方法的需求。", "method": "研讨会将探讨和鼓励以下方法：分布外危险检测、用于危险理解的视觉-语言模型、新的基准测试和方法论、安全的自动驾驶实践，并借鉴异常检测、开放集识别、开放词汇建模和域适应等领域的思想。", "result": "研讨会旨在激发开发新的危险规避算法和系统，推动新颖性处理的最新技术发展，并汇集学术界和工业界的参与者。", "conclusion": "解决自动驾驶中的新颖场景和标签外危险是实现完全安全自动驾驶的关键。2COOOL研讨会提供了一个专门的平台，以促进该领域的研究和技术进步。"}}
{"id": "2508.21204", "categories": ["cs.AI", "cs.CL", "I.2.7; I.2.11; I.2.6"], "pdf": "https://arxiv.org/pdf/2508.21204", "abs": "https://arxiv.org/abs/2508.21204", "authors": ["Vanessa Figueiredo"], "title": "Fuzzy, Symbolic, and Contextual: Enhancing LLM Instruction via Cognitive Scaffolding", "comment": null, "summary": "We study how architectural inductive biases influence the cognitive behavior\nof large language models (LLMs) in instructional dialogue. We introduce a\nsymbolic scaffolding mechanism paired with a short-term memory schema designed\nto promote adaptive, structured reasoning in Socratic tutoring. Using\ncontrolled ablation across five system variants, we evaluate model outputs via\nexpert-designed rubrics covering scaffolding, responsiveness, symbolic\nreasoning, and conversational memory. We present preliminary results using an\nLLM-based evaluation framework aligned to a cognitively grounded rubric. This\nenables scalable, systematic comparisons across architectural variants in\nearly-stage experimentation. The preliminary results show that our full system\nconsistently outperforms baseline variants. Analysis reveals that removing\nmemory or symbolic structure degrades key cognitive behaviors, including\nabstraction, adaptive probing, and conceptual continuity. These findings\nsupport a processing-level account in which architectural scaffolds can\nreliably shape emergent instructional strategies in LLMs.", "AI": {"tldr": "本研究探讨了架构归纳偏见如何影响LLM在教学对话中的认知行为，发现符号支架和短期记忆机制能显著提升LLM的苏格拉底式辅导能力。", "motivation": "研究旨在理解架构归纳偏见如何影响大型语言模型（LLMs）在教学对话中的认知行为，特别是为了在苏格拉底式辅导中促进自适应、结构化的推理能力。", "method": "研究引入了符号支架机制和短期记忆方案，以促进LLM的自适应结构化推理。通过对五种系统变体进行受控消融实验，使用专家设计的评分标准（涵盖支架、响应性、符号推理和对话记忆）以及基于LLM的评估框架来评估模型输出。", "result": "初步结果显示，完整系统始终优于基线变体。分析表明，移除记忆或符号结构会损害关键的认知行为，包括抽象、自适应探究和概念连续性。", "conclusion": "这些发现支持了一种处理层面的解释，即架构支架可以可靠地塑造LLM中新兴的教学策略。"}}
{"id": "2508.21137", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.21137", "abs": "https://arxiv.org/abs/2508.21137", "authors": ["Yoshiki Takenami", "Yin Jou Huang", "Yugo Murawaki", "Chenhui Chu"], "title": "How Does Cognitive Bias Affect Large Language Models? A Case Study on the Anchoring Effect in Price Negotiation Simulations", "comment": "work in progress", "summary": "Cognitive biases, well-studied in humans, can also be observed in LLMs,\naffecting their reliability in real-world applications. This paper investigates\nthe anchoring effect in LLM-driven price negotiations. To this end, we\ninstructed seller LLM agents to apply the anchoring effect and evaluated\nnegotiations using not only an objective metric but also a subjective metric.\nExperimental results show that LLMs are influenced by the anchoring effect like\nhumans. Additionally, we investigated the relationship between the anchoring\neffect and factors such as reasoning and personality. It was shown that\nreasoning models are less prone to the anchoring effect, suggesting that the\nlong chain of thought mitigates the effect. However, we found no significant\ncorrelation between personality traits and susceptibility to the anchoring\neffect. These findings contribute to a deeper understanding of cognitive biases\nin LLMs and to the realization of safe and responsible application of LLMs in\nsociety.", "AI": {"tldr": "本文研究了LLM在价格谈判中受锚定效应的影响，发现LLM像人类一样受其影响，推理能力可缓解该效应，但个性特征无显著相关性。", "motivation": "认知偏差在LLM中也存在，影响其在实际应用中的可靠性。本文旨在深入理解LLM中的认知偏差，特别是锚定效应，以实现LLM的安全和负责任应用。", "method": "研究通过指示卖方LLM代理应用锚定效应，在价格谈判中进行评估。评估不仅使用了客观指标，还使用了主观指标。此外，还调查了锚定效应与推理能力和个性特征之间的关系。", "result": "实验结果表明，LLM像人类一样受锚定效应的影响。推理模型更不易受锚定效应影响，表明长链思维可以缓解该效应。然而，个性特征与锚定效应的易感性之间没有发现显著相关性。", "conclusion": "这些发现有助于更深入地理解LLM中的认知偏差，并促进LLM在社会中的安全和负责任应用。"}}
{"id": "2508.21715", "categories": ["cs.CV", "cs.AI", "cs.CR", "cs.IT", "eess.IV", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.21715", "abs": "https://arxiv.org/abs/2508.21715", "authors": ["Amirhossein Nazeri", "Wael Hafez"], "title": "Entropy-Based Non-Invasive Reliability Monitoring of Convolutional Neural Networks", "comment": "8 pages, 3 figures, 2 tables", "summary": "Convolutional Neural Networks (CNNs) have become the foundation of modern\ncomputer vision, achieving unprecedented accuracy across diverse image\nrecognition tasks. While these networks excel on in-distribution data, they\nremain vulnerable to adversarial perturbations imperceptible input\nmodifications that cause misclassification with high confidence. However,\nexisting detection methods either require expensive retraining, modify network\narchitecture, or degrade performance on clean inputs. Here we show that\nadversarial perturbations create immediate, detectable entropy signatures in\nCNN activations that can be monitored without any model modification. Using\nparallel entropy monitoring on VGG-16, we demonstrate that adversarial inputs\nconsistently shift activation entropy by 7% in early convolutional layers,\nenabling 90% detection accuracy with false positives and false negative rates\nbelow 20%. The complete separation between clean and adversarial entropy\ndistributions reveals that CNNs inherently encode distribution shifts in their\nactivation patterns. This work establishes that CNN reliability can be assessed\nthrough activation entropy alone, enabling practical deployment of\nself-diagnostic vision systems that detect adversarial inputs in real-time\nwithout compromising original model performance.", "AI": {"tldr": "该研究提出一种无需修改模型的新方法，通过监测卷积神经网络（CNN）激活层中的熵值变化，实时检测对抗性攻击，且不影响模型在正常数据上的性能。", "motivation": "尽管CNN在图像识别任务中表现出色，但它们容易受到对抗性扰动的攻击。现有的检测方法通常需要昂贵的再训练、修改网络架构或降低在干净输入上的性能，这些缺点促使研究人员寻找更有效的解决方案。", "method": "研究人员发现对抗性扰动会在CNN激活中产生可立即检测到的熵签名。他们使用并行熵监测技术在VGG-16网络上进行了实验，以观察对抗性输入对激活熵的影响。", "result": "实验表明，对抗性输入会使早期卷积层的激活熵持续偏移7%，从而实现90%的检测准确率，同时误报率和漏报率均低于20%。干净和对抗性输入之间的熵分布完全分离，揭示了CNN在激活模式中固有的分布偏移编码能力。", "conclusion": "该工作证明了仅通过激活熵就能评估CNN的可靠性，从而实现实用的自诊断视觉系统。这些系统能够在不损害原始模型性能的情况下，实时检测对抗性输入，具有重要的实际部署价值。"}}
{"id": "2508.21199", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.21199", "abs": "https://arxiv.org/abs/2508.21199", "authors": ["Christopher Martin", "Edward Kim", "Enrique Velasquez", "Wei Li", "Dongmei Chen"], "title": "$H_\\infty$ Performance Analysis for Almost Periodic Piecewise Linear Systems with Application to Roll-to-Roll Manufacturing Control", "comment": "11 pages, 11 figures", "summary": "An almost periodic piecewise linear system (APPLS) is a type of piecewise\nlinear system where the system cyclically switches between different modes,\neach with an uncertain but bounded dwell-time. Process regulation, especially\ndisturbance rejection, is critical to the performance of these advanced\nsystems. However, a method to guarantee disturbance rejection has not been\ndeveloped. The objective of this study is to develop an $H_\\infty$ performance\nanalysis method for APPLSs, building on which an algorithm to synthesize\npractical $H_\\infty$ controllers is proposed. As an application, the developed\nmethods are demonstrated with an advanced manufacturing system -- roll-to-roll\n(R2R) dry transfer of two-dimensional materials and printed flexible\nelectronics. Experimental results show that the proposed method enables a less\nconservative and much better performing $H_\\infty$ controller compared with a\nbaseline $H_\\infty$ controller that does not account for the uncertain system\nswitching structure.", "AI": {"tldr": "本文提出了一种针对几乎周期分段线性系统（APPLS）的H∞性能分析方法和控制器合成算法，以实现扰动抑制，并在卷对卷（R2R）干转移系统中进行了实验验证。", "motivation": "几乎周期分段线性系统（APPLS）的扰动抑制对其性能至关重要，但目前尚缺乏能够保证扰动抑制效果的方法。", "method": "本研究首先开发了一种用于APPLS的H∞性能分析方法，并在此基础上提出了一种合成实用H∞控制器的算法。", "result": "实验结果表明，与未考虑系统切换结构基线H∞控制器相比，所提出的方法能够实现更不保守且性能显著优越的H∞控制器，并在先进制造系统（如R2R干转移）中得到了应用验证。", "conclusion": "所开发的H∞性能分析和控制器合成方法能够有效提高APPLS的扰动抑制能力，并为实际应用提供了更优的控制方案。"}}
{"id": "2508.21163", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.21163", "abs": "https://arxiv.org/abs/2508.21163", "authors": ["Tarek Bouazza", "Soulaimane Berkane", "Minh-Duc Hua", "Tarek Hamel"], "title": "Observer Design for Optical Flow-Based Visual-Inertial Odometry with Almost-Global Convergence", "comment": "8 pages, 6 figures. To appear in IEEE CDC 2025", "summary": "This paper presents a novel cascaded observer architecture that combines\noptical flow and IMU measurements to perform continuous monocular\nvisual-inertial odometry (VIO). The proposed solution estimates body-frame\nvelocity and gravity direction simultaneously by fusing velocity direction\ninformation from optical flow measurements with gyro and accelerometer data.\nThis fusion is achieved using a globally exponentially stable Riccati observer,\nwhich operates under persistently exciting translational motion conditions. The\nestimated gravity direction in the body frame is then employed, along with an\noptional magnetometer measurement, to design a complementary observer on\n$\\mathbf{SO}(3)$ for attitude estimation. The resulting interconnected observer\narchitecture is shown to be almost globally asymptotically stable. To extract\nthe velocity direction from sparse optical flow data, a gradient descent\nalgorithm is developed to solve a constrained minimization problem on the unit\nsphere. The effectiveness of the proposed algorithms is validated through\nsimulation results.", "AI": {"tldr": "本文提出了一种新颖的级联观测器架构，通过融合光流和IMU测量，实现了连续单目视觉惯性里程计（VIO），能够同时估计体坐标系下的速度、重力方向和姿态。", "motivation": "研究动机在于开发一种鲁棒、连续的单目视觉惯性里程计（VIO）解决方案，通过有效融合稀疏光流的速度方向信息与IMU数据，实现对速度、重力方向和姿态的同步高精度估计。", "method": "该方法采用级联观测器架构。首先，一个全局指数稳定的Riccati观测器融合光流测量的速度方向信息、陀螺仪和加速度计数据，在持续激励的平移运动条件下，同时估计体坐标系下的速度和重力方向。然后，利用估计的体坐标系重力方向（以及可选的磁力计测量），设计一个SO(3)上的互补观测器进行姿态估计。此外，还开发了一种梯度下降算法，用于在单位球面上解决约束最小化问题，以从稀疏光流数据中提取速度方向。", "result": "所提出的互连观测器架构被证明是几乎全局渐近稳定的。通过仿真结果验证了所提出算法的有效性。", "conclusion": "本文提出了一种新颖的、几乎全局渐近稳定的级联观测器架构，成功地将光流和IMU测量融合在一起，实现了连续的单目视觉惯性里程计，为同时估计速度、重力方向和姿态提供了一种有效且鲁棒的解决方案。"}}
{"id": "2508.21088", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.21088", "abs": "https://arxiv.org/abs/2508.21088", "authors": ["Alireza Golkarieh", "Kiana Kiashemshaki", "Sajjad Rezvani Boroujeni"], "title": "Advanced Deep Learning Techniques for Classifying Dental Conditions Using Panoramic X-Ray Images", "comment": "14 pages, 8 figures, 8 tables", "summary": "This study investigates deep learning methods for automated classification of\ndental conditions in panoramic X-ray images. A dataset of 1,512 radiographs\nwith 11,137 expert-verified annotations across four conditions fillings,\ncavities, implants, and impacted teeth was used. After preprocessing and class\nbalancing, three approaches were evaluated: a custom convolutional neural\nnetwork (CNN), hybrid models combining CNN feature extraction with traditional\nclassifiers, and fine-tuned pre-trained architectures. Experiments employed 5\nfold cross validation with accuracy, precision, recall, and F1 score as\nevaluation metrics. The hybrid CNN Random Forest model achieved the highest\nperformance with 85.4% accuracy, surpassing the custom CNN baseline of 74.3%.\nAmong pre-trained models, VGG16 performed best at 82.3% accuracy, followed by\nXception and ResNet50. Results show that hybrid models improve discrimination\nof morphologically similar conditions and provide efficient, reliable\nperformance. These findings suggest that combining CNN-based feature extraction\nwith ensemble classifiers offers a practical path toward automated dental\ndiagnostic support, while also highlighting the need for larger datasets and\nfurther clinical validation.", "AI": {"tldr": "本研究评估了深度学习方法在全景X射线图像中自动分类牙齿状况的性能，发现混合模型（CNN特征提取结合传统分类器）表现最佳，其中CNN-Random Forest模型准确率达到85.4%。", "motivation": "本研究旨在开发一种自动化方法，用于全景X射线图像中牙齿状况的分类，以提供高效、可靠的牙科诊断支持。", "method": "研究使用了包含1512张X射线图像和11137个专家验证注释的数据集，涵盖龋齿、填充物、种植体和阻生齿四种牙齿状况。经过预处理和类别平衡后，评估了三种方法：自定义卷积神经网络（CNN）、结合CNN特征提取与传统分类器的混合模型，以及微调的预训练架构。实验采用5折交叉验证，并使用准确率、精确度、召回率和F1分数作为评估指标。", "result": "混合CNN-Random Forest模型表现最佳，准确率达到85.4%，显著优于自定义CNN基线（74.3%）。在预训练模型中，VGG16表现最好，准确率为82.3%，其次是Xception和ResNet50。结果表明，混合模型能提高形态相似状况的鉴别能力。", "conclusion": "结合CNN特征提取与集成分类器的混合模型为自动化牙科诊断支持提供了一条实用的途径，同时也强调了需要更大规模数据集和进一步临床验证的重要性。"}}
{"id": "2508.21238", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.21238", "abs": "https://arxiv.org/abs/2508.21238", "authors": ["Tingxuan Xu", "Jiarui Feng", "Justin Melendez", "Kaleigh Roberts", "Donghong Cai", "Mingfang Zhu", "Donald Elbert", "Yixin Chen", "Randall J. Bateman"], "title": "Addressing accuracy and hallucination of LLMs in Alzheimer's disease research through knowledge graphs", "comment": null, "summary": "In the past two years, large language model (LLM)-based chatbots, such as\nChatGPT, have revolutionized various domains by enabling diverse task\ncompletion and question-answering capabilities. However, their application in\nscientific research remains constrained by challenges such as hallucinations,\nlimited domain-specific knowledge, and lack of explainability or traceability\nfor the response. Graph-based Retrieval-Augmented Generation (GraphRAG) has\nemerged as a promising approach to improving chatbot reliability by integrating\ndomain-specific contextual information before response generation, addressing\nsome limitations of standard LLMs. Despite its potential, there are only\nlimited studies that evaluate GraphRAG on specific domains that require\nintensive knowledge, like Alzheimer's disease or other biomedical domains. In\nthis paper, we assess the quality and traceability of two popular GraphRAG\nsystems. We compile a database of 50 papers and 70 expert questions related to\nAlzheimer's disease, construct a GraphRAG knowledge base, and employ GPT-4o as\nthe LLM for answering queries. We then compare the quality of responses\ngenerated by GraphRAG with those from a standard GPT-4o model. Additionally, we\ndiscuss and evaluate the traceability of several Retrieval-Augmented Generation\n(RAG) and GraphRAG systems. Finally, we provide an easy-to-use interface with a\npre-built Alzheimer's disease database for researchers to test the performance\nof both standard RAG and GraphRAG.", "AI": {"tldr": "本文评估了GraphRAG系统在阿尔茨海默病等知识密集型科学研究领域中的回答质量和可追溯性，并将其与标准LLM（GPT-4o）进行比较。", "motivation": "大型语言模型（LLM）在科学研究中面临幻觉、领域知识有限和缺乏可解释性/可追溯性等挑战。GraphRAG被认为是一种有前途的解决方案，但其在阿尔茨海默病等知识密集型特定领域中的评估研究有限。", "method": "研究编译了一个包含50篇论文和70个专家问题的阿尔茨海默病数据库，构建了GraphRAG知识库，并使用GPT-4o作为LLM。论文比较了GraphRAG与标准GPT-4o模型的回答质量，并讨论和评估了几种RAG和GraphRAG系统的可追溯性。此外，还提供了一个易于使用的界面。", "result": "论文评估了GraphRAG系统在阿尔茨海默病领域的回答质量和可追溯性，并与标准LLM进行了比较。同时，讨论并评估了RAG和GraphRAG系统的可追溯性。", "conclusion": "本文通过对GraphRAG在阿尔茨海默病领域的评估，为研究人员提供了关于其在知识密集型科学研究中应用潜力的见解，并提供了一个测试标准RAG和GraphRAG性能的数据库和界面。"}}
{"id": "2508.21143", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.21143", "abs": "https://arxiv.org/abs/2508.21143", "authors": ["Samrajnee Ghosh", "Naman Agarwal", "Hemanshu Garg", "Chinmay Mittal", "Mausam", "Parag Singla"], "title": "Can Multimodal LLMs Solve the Basic Perception Problems of Percept-V?", "comment": null, "summary": "The reasoning abilities of Multimodal Large Language Models (MLLMs) have\ngarnered a lot of attention in recent times, with advances made in frontiers\nlike coding, mathematics, and science. However, very limited experiments have\nbeen done to assess their performance in simple perception tasks performed over\nuncontaminated, generated images containing basic shapes and structures. To\naddress this issue, the paper introduces a dataset, Percept-V, containing a\ntotal of 7200 program-generated images equally divided into 30 categories, each\ntesting a combination of visual perception skills. Unlike previously proposed\ndatasets, Percept-V comprises very basic tasks of varying complexity that test\nthe perception abilities of MLLMs. This dataset is then tested on\nstate-of-the-art MLLMs like GPT-4o, Gemini, and Claude as well as Large\nReasoning Models (LRMs) like OpenAI o4-mini and DeepSeek R1 to gauge their\nperformance. Contrary to the evidence that MLLMs excel in many complex tasks,\nour experiments show a significant drop in the models' performance with\nincreasing problem complexity across all categories. An analysis of the\nperformances also reveals that the tested MLLMs exhibit a similar trend in\naccuracy across categories, testing a particular cognitive skill and find some\nskills to be more difficult than others.", "AI": {"tldr": "本研究引入了Percept-V数据集，用于评估多模态大语言模型（MLLMs）在基本视觉感知任务上的表现。结果显示，随着问题复杂性增加，MLLMs的性能显著下降，且在不同认知技能上表现出相似的准确性趋势。", "motivation": "尽管MLLMs在编码、数学和科学等复杂推理任务上取得了进展，但很少有实验评估它们在包含基本形状和结构的、无污染生成图像上的简单感知任务表现。", "method": "研究引入了Percept-V数据集，包含7200张程序生成的图像，分为30个类别，每个类别测试不同的视觉感知技能。该数据集包含不同复杂度的基本任务。研究人员使用GPT-4o、Gemini、Claude等最先进的MLLMs以及OpenAI o4-mini、DeepSeek R1等大型推理模型（LRMs）对数据集进行了测试。", "result": "实验发现，与MLLMs在许多复杂任务中表现出色相反，随着问题复杂性的增加，模型在所有类别中的性能都显著下降。对性能的分析还揭示，被测试的MLLMs在测试特定认知技能的类别中表现出相似的准确性趋势，并且发现某些技能比其他技能更难。", "conclusion": "尽管MLLMs在复杂任务上表现出色，但它们在基本的视觉感知任务中，随着问题复杂度的增加，性能会显著下降，这表明它们在基础感知技能方面存在局限性。"}}
{"id": "2508.21316", "categories": ["eess.SY", "cs.RO", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.21316", "abs": "https://arxiv.org/abs/2508.21316", "authors": ["Changheng Wang", "Zhiqing Wei", "Wangjun Jiang", "Haoyue Jiang", "Zhiyong Feng"], "title": "Cooperative Sensing Enhanced UAV Path-Following and Obstacle Avoidance with Variable Formation", "comment": null, "summary": "The high mobility of unmanned aerial vehicles (UAVs) enables them to be used\nin various civilian fields, such as rescue and cargo transport. Path-following\nis a crucial way to perform these tasks while sensing and collision avoidance\nare essential for safe flight. In this paper, we investigate how to efficiently\nand accurately achieve path-following, obstacle sensing and avoidance subtasks,\nas well as their conflict-free fusion scheduling. Firstly, a high precision\ndeep reinforcement learning (DRL)-based UAV formation path-following model is\ndeveloped, and the reward function with adaptive weights is designed from the\nperspective of distance and velocity errors. Then, we use integrated sensing\nand communication (ISAC) signals to detect the obstacle and derive the\nCramer-Rao lower bound (CRLB) for obstacle sensing by information-level fusion,\nbased on which we propose the variable formation enhanced obstacle position\nestimation (VFEO) algorithm. In addition, an online obstacle avoidance scheme\nwithout pretraining is designed to solve the sparse reward. Finally, with the\naid of null space based (NSB) behavioral method, we present a hierarchical\nsubtasks fusion strategy. Simulation results demonstrate the effectiveness and\nsuperiority of the subtask algorithms and the hierarchical fusion strategy.", "AI": {"tldr": "本文提出了一种针对无人机（UAV）的高精度路径跟随、障碍物感知与避障方法，并设计了无冲突的任务融合调度策略，结合了深度强化学习、集成感知与通信技术和分层行为融合。", "motivation": "无人机在高机动性使其在救援和货物运输等民用领域具有广泛应用。为了安全高效地执行任务，路径跟随、感知和避障是至关重要的，同时需要解决这些子任务的冲突融合问题。", "method": "首先，开发了一种基于深度强化学习（DRL）的无人机编队路径跟随模型，并设计了带有自适应权重的奖励函数。其次，利用集成感知与通信（ISAC）信号进行障碍物检测，推导了克拉默-劳下界（CRLB），并提出了可变编队增强障碍物位置估计（VFEO）算法。此外，设计了一种无需预训练的在线避障方案来解决稀疏奖励问题。最后，借助基于零空间（NSB）的行为方法，提出了一种分层子任务融合策略。", "result": "仿真结果表明，所提出的子任务算法和分层融合策略均具有有效性和优越性。", "conclusion": "本文成功地实现了无人机的高效精确路径跟随、障碍物感知与避障，并提出了无冲突的子任务分层融合调度策略，有效解决了无人机安全飞行的关键挑战。"}}
{"id": "2508.21205", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.21205", "abs": "https://arxiv.org/abs/2508.21205", "authors": ["Usman A. Khan", "Mouhacine Benosman", "Wenliang Liu", "Federico Pecora", "Joseph W. Durham"], "title": "Multi-robot Path Planning and Scheduling via Model Predictive Optimal Transport (MPC-OT)", "comment": "2025 IEEE Conference on Decision and Control", "summary": "In this paper, we propose a novel methodology for path planning and\nscheduling for multi-robot navigation that is based on optimal transport theory\nand model predictive control. We consider a setup where $N$ robots are tasked\nto navigate to $M$ targets in a common space with obstacles. Mapping robots to\ntargets first and then planning paths can result in overlapping paths that lead\nto deadlocks. We derive a strategy based on optimal transport that not only\nprovides minimum cost paths from robots to targets but also guarantees\nnon-overlapping trajectories. We achieve this by discretizing the space of\ninterest into $K$ cells and by imposing a ${K\\times K}$ cost structure that\ndescribes the cost of transitioning from one cell to another. Optimal transport\nthen provides \\textit{optimal and non-overlapping} cell transitions for the\nrobots to reach the targets that can be readily deployed without any scheduling\nconsiderations. The proposed solution requires $\\unicode{x1D4AA}(K^3\\log K)$\ncomputations in the worst-case and $\\unicode{x1D4AA}(K^2\\log K)$ for\nwell-behaved problems. To further accommodate potentially overlapping\ntrajectories (unavoidable in certain situations) as well as robot dynamics, we\nshow that a temporal structure can be integrated into optimal transport with\nthe help of \\textit{replans} and \\textit{model predictive control}.", "AI": {"tldr": "本文提出了一种基于最优传输理论和模型预测控制的多机器人路径规划与调度新方法，旨在实现无碰撞的机器人导航。", "motivation": "传统的多机器人路径规划方法（先分配目标再规划路径）可能导致路径重叠和死锁。因此，需要一种能够保证无重叠轨迹的策略。", "method": "研究方法包括：1. 将兴趣空间离散化为K个单元格，并建立K×K的成本结构来描述单元格间的转换成本。2. 利用最优传输理论，从机器人到目标提供最小成本且无重叠的单元格转换。3. 对于可能重叠的轨迹（某些情况不可避免）和机器人动力学，通过“重新规划”和“模型预测控制”将时间结构整合到最优传输中。", "result": "该方法能够提供最优且无重叠的机器人到目标单元格转换。在最坏情况下，计算复杂度为O(K³log K)，对于良好问题为O(K²log K)。通过整合模型预测控制，该方法还能适应潜在的重叠轨迹和机器人动力学。", "conclusion": "本文提出了一种新颖的多机器人导航路径规划和调度方法，该方法基于最优传输理论，能够有效保证机器人轨迹的无重叠性。通过结合模型预测控制，该方案进一步增强了对复杂动态和不可避免的轨迹重叠情况的处理能力。"}}
{"id": "2508.21090", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.21090", "abs": "https://arxiv.org/abs/2508.21090", "authors": ["Namu Kim", "Wonbin Kweon", "Minsoo Kim", "Hwanjo Yu"], "title": "Q-Align: Alleviating Attention Leakage in Zero-Shot Appearance Transfer via Query-Query Alignment", "comment": null, "summary": "We observe that zero-shot appearance transfer with large-scale image\ngeneration models faces a significant challenge: Attention Leakage. This\nchallenge arises when the semantic mapping between two images is captured by\nthe Query-Key alignment. To tackle this issue, we introduce Q-Align, utilizing\nQuery-Query alignment to mitigate attention leakage and improve the semantic\nalignment in zero-shot appearance transfer. Q-Align incorporates three core\ncontributions: (1) Query-Query alignment, facilitating the sophisticated\nspatial semantic mapping between two images; (2) Key-Value rearrangement,\nenhancing feature correspondence through realignment; and (3) Attention\nrefinement using rearranged keys and values to maintain semantic consistency.\nWe validate the effectiveness of Q-Align through extensive experiments and\nanalysis, and Q-Align outperforms state-of-the-art methods in appearance\nfidelity while maintaining competitive structure preservation.", "AI": {"tldr": "针对大规模图像生成模型在零样本外观迁移中出现的“注意力泄漏”问题，本文提出了Q-Align方法，通过查询-查询对齐、键值重排和注意力精炼来缓解泄漏，显著提升外观保真度并保持结构一致性。", "motivation": "大规模图像生成模型在零样本外观迁移中存在“注意力泄漏”的显著挑战，即语义映射被查询-键对齐捕获时导致的问题。", "method": "本文引入了Q-Align方法来解决注意力泄漏问题，其核心贡献包括：1) 查询-查询对齐，以促进两图像间复杂的空间语义映射；2) 键值重排，通过重新对齐增强特征对应；3) 使用重排后的键和值进行注意力精炼，以保持语义一致性。", "result": "通过广泛的实验和分析，Q-Align被验证是有效的，在外观保真度方面超越了现有最先进的方法，同时保持了有竞争力的结构保留能力。", "conclusion": "Q-Align成功解决了零样本外观迁移中的注意力泄漏问题，显著提高了外观保真度，并有效保持了结构一致性。"}}
{"id": "2508.21307", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.21307", "abs": "https://arxiv.org/abs/2508.21307", "authors": ["Sri Ram Macharla", "Sridhar Murthy J", "Anjaneyulu Pasala"], "title": "MultiFluxAI Enhancing Platform Engineering with Advanced Agent-Orchestrated Retrieval Systems", "comment": "Abstract accepted for presentation at ACM ISEC 2025", "summary": "MultiFluxAI is an innovative AI platform developed to address the challenges\nof managing and integrating vast, disparate data sources in product engineering\nacross application domains. It addresses both current and new service related\nqueries that enhance user engagement in the digital ecosystem. This platform\nleverages advanced AI techniques, such as Generative AI, vectorization, and\nagentic orchestration to provide dynamic and context-aware responses to complex\nuser queries.", "AI": {"tldr": "MultiFluxAI是一个创新AI平台，旨在解决产品工程中管理和集成大量异构数据源的挑战，通过先进AI技术提供动态、上下文感知的用户查询响应。", "motivation": "解决产品工程中管理和集成大量、异构数据源的挑战，并增强数字生态系统中的用户参与度。", "method": "利用先进的AI技术，包括生成式AI（Generative AI）、向量化（vectorization）和代理编排（agentic orchestration）。", "result": "能够处理当前和新的服务相关查询，提供动态和上下文感知的响应以解决复杂的用户查询，从而增强数字生态系统中的用户参与度。", "conclusion": "MultiFluxAI通过其创新的AI方法，有效地管理和集成复杂数据，提升了用户在数字生态系统中的交互体验和查询处理能力。"}}
{"id": "2508.21148", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.21148", "abs": "https://arxiv.org/abs/2508.21148", "authors": ["Ming Hu", "Chenglong Ma", "Wei Li", "Wanghan Xu", "Jiamin Wu", "Jucheng Hu", "Tianbin Li", "Guohang Zhuang", "Jiaqi Liu", "Yingzhou Lu", "Ying Chen", "Chaoyang Zhang", "Cheng Tan", "Jie Ying", "Guocheng Wu", "Shujian Gao", "Pengcheng Chen", "Jiashi Lin", "Haitao Wu", "Lulu Chen", "Fengxiang Wang", "Yuanyuan Zhang", "Xiangyu Zhao", "Feilong Tang", "Encheng Su", "Junzhi Ning", "Xinyao Liu", "Ye Du", "Changkai Ji", "Cheng Tang", "Huihui Xu", "Ziyang Chen", "Ziyan Huang", "Jiyao Liu", "Pengfei Jiang", "Yizhou Wang", "Chen Tang", "Jianyu Wu", "Yuchen Ren", "Siyuan Yan", "Zhonghua Wang", "Zhongxing Xu", "Shiyan Su", "Shangquan Sun", "Runkai Zhao", "Zhisheng Zhang", "Yu Liu", "Fudi Wang", "Yuanfeng Ji", "Yanzhou Su", "Hongming Shan", "Chunmei Feng", "Jiahao Xu", "Jiangtao Yan", "Wenhao Tang", "Diping Song", "Lihao Liu", "Yanyan Huang", "Lequan Yu", "Bin Fu", "Shujun Wang", "Xiaomeng Li", "Xiaowei Hu", "Yun Gu", "Ben Fei", "Zhongying Deng", "Benyou Wang", "Yuewen Cao", "Minjie Shen", "Haodong Duan", "Jie Xu", "Yirong Chen", "Fang Yan", "Hongxia Hao", "Jielan Li", "Jiajun Du", "Yanbo Wang", "Imran Razzak", "Chi Zhang", "Lijun Wu", "Conghui He", "Zhaohui Lu", "Jinhai Huang", "Yihao Liu", "Fenghua Ling", "Yuqiang Li", "Aoran Wang", "Qihao Zheng", "Nanqing Dong", "Tianfan Fu", "Dongzhan Zhou", "Yan Lu", "Wenlong Zhang", "Jin Ye", "Jianfei Cai", "Wanli Ouyang", "Yu Qiao", "Zongyuan Ge", "Shixiang Tang", "Junjun He", "Chunfeng Song", "Lei Bai", "Bowen Zhou"], "title": "A Survey of Scientific Large Language Models: From Data Foundations to Agent Frontiers", "comment": null, "summary": "Scientific Large Language Models (Sci-LLMs) are transforming how knowledge is\nrepresented, integrated, and applied in scientific research, yet their progress\nis shaped by the complex nature of scientific data. This survey presents a\ncomprehensive, data-centric synthesis that reframes the development of Sci-LLMs\nas a co-evolution between models and their underlying data substrate. We\nformulate a unified taxonomy of scientific data and a hierarchical model of\nscientific knowledge, emphasizing the multimodal, cross-scale, and\ndomain-specific challenges that differentiate scientific corpora from general\nnatural language processing datasets. We systematically review recent Sci-LLMs,\nfrom general-purpose foundations to specialized models across diverse\nscientific disciplines, alongside an extensive analysis of over 270\npre-/post-training datasets, showing why Sci-LLMs pose distinct demands --\nheterogeneous, multi-scale, uncertainty-laden corpora that require\nrepresentations preserving domain invariance and enabling cross-modal\nreasoning. On evaluation, we examine over 190 benchmark datasets and trace a\nshift from static exams toward process- and discovery-oriented assessments with\nadvanced evaluation protocols. These data-centric analyses highlight persistent\nissues in scientific data development and discuss emerging solutions involving\nsemi-automated annotation pipelines and expert validation. Finally, we outline\na paradigm shift toward closed-loop systems where autonomous agents based on\nSci-LLMs actively experiment, validate, and contribute to a living, evolving\nknowledge base. Collectively, this work provides a roadmap for building\ntrustworthy, continually evolving artificial intelligence (AI) systems that\nfunction as a true partner in accelerating scientific discovery.", "AI": {"tldr": "本调查报告全面审视了科学大型语言模型（Sci-LLMs）的发展，将其视为模型与底层科学数据共同演进的过程。报告提出了科学数据的统一分类法和知识模型，分析了科学数据特有的多模态、跨尺度、领域特异性等挑战，并系统回顾了当前的Sci-LLMs、训练数据集和评估基准，最终展望了迈向闭环自主智能系统的未来。", "motivation": "Sci-LLMs正在变革科学研究，但其进展受限于科学数据的复杂性。本研究旨在通过以数据为中心的方法，综合分析Sci-LLMs的发展，阐明模型与数据之间的共演关系，并识别科学数据在训练和评估Sci-LLMs中带来的独特挑战和需求。", "method": "本研究采用调查综述的方法，具体包括：1) 制定科学数据的统一分类法和科学知识的层级模型；2) 系统回顾从通用到专业化的最新Sci-LLMs；3) 广泛分析超过270个预训练/后训练数据集，强调科学数据（异构、多尺度、不确定性）的特殊要求；4) 审查超过190个基准数据集和评估协议，追踪评估范式的转变；5) 讨论科学数据开发中的持续问题及半自动化标注和专家验证等新兴解决方案。", "result": "Sci-LLMs的发展与底层科学数据共同演进。科学语料库与通用NLP数据集不同，具有多模态、跨尺度、领域特异性、异构、多尺度和不确定性等特点，需要保留领域不变性并实现跨模态推理的表示。评估方式正从静态考试转向以过程和发现为导向的先进评估协议。科学数据开发中存在持续性问题，但半自动化标注流程和专家验证等新兴解决方案正在出现。", "conclusion": "本研究为构建可信赖、持续进化的AI系统以加速科学发现提供了路线图。未来将出现基于Sci-LLMs的自主智能体，形成闭环系统，主动进行实验、验证并贡献于一个活态、演进的知识库，成为科学发现的真正伙伴。"}}
{"id": "2508.21584", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.21584", "abs": "https://arxiv.org/abs/2508.21584", "authors": ["Poulomee Ghosh", "Shubhendu Bhasin"], "title": "State and Input Constrained Model Reference Adaptive Control with Robustness and Feasibility Analysis", "comment": null, "summary": "We propose a model reference adaptive controller (MRAC) for uncertain linear\ntime-invariant (LTI) plants with user-defined state and input constraints in\nthe presence of unmatched bounded disturbances. Unlike popular\noptimization-based approaches for constrained control, such as model predictive\ncontrol (MPC) and control barrier function (CBF) that solve a constrained\noptimization problem at each step using the system model, our approach is\noptimization-free and adaptive; it combines a saturated adaptive controller\nwith a barrier Lyapunov function (BLF)-based design to ensure that the plant\nstate and input always stay within pre-specified bounds despite the presence of\nunmatched disturbances. To the best of our knowledge, this is the first result\nthat considers both state and input constraints for control of uncertain\nsystems with disturbances and provides sufficient feasibility conditions to\ncheck for the existence of an admissible control policy. Simulation results,\nincluding a comparison with a robust MRAC, demonstrate the effectiveness of the\nproposed algorithm.", "AI": {"tldr": "本文提出了一种针对不确定线性时不变(LTI)系统、存在未匹配有界扰动且具有用户定义状态和输入约束的模型参考自适应控制器(MRAC)，该控制器无需优化。", "motivation": "现有的基于优化的约束控制方法（如MPC和CBF）每一步都需要解决一个优化问题。本研究旨在开发一种无需优化且自适应的控制器，能同时处理不确定性、扰动以及状态和输入约束，并且是首次同时考虑这两种约束的方案。", "method": "该方法结合了饱和自适应控制器和基于障碍李雅普诺夫函数(BLF)的设计。它通过这种组合来确保植物状态和输入始终保持在预设范围内。", "result": "该控制器在存在未匹配扰动的情况下，能确保植物状态和输入始终保持在预设范围内。它还提供了足够的判别条件来检查可接受控制策略的存在性。仿真结果（包括与鲁棒MRAC的比较）证明了所提出算法的有效性。", "conclusion": "所提出的MRAC算法是一种有效、无需优化的自适应解决方案，适用于具有状态和输入约束、不确定性以及扰动的LTI系统，并且在保证系统性能方面表现出色。"}}
{"id": "2508.21221", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.21221", "abs": "https://arxiv.org/abs/2508.21221", "authors": ["Fatima Mumtaza Tourk", "Bishoy Galoaa", "Sanat Shajan", "Aaron J. Young", "Michael Everett", "Max K. Shepherd"], "title": "Uncertainty-Aware Ankle Exoskeleton Control", "comment": null, "summary": "Lower limb exoskeletons show promise to assist human movement, but their\nutility is limited by controllers designed for discrete, predefined actions in\ncontrolled environments, restricting their real-world applicability. We present\nan uncertainty-aware control framework that enables ankle exoskeletons to\noperate safely across diverse scenarios by automatically disengaging when\nencountering unfamiliar movements. Our approach uses an uncertainty estimator\nto classify movements as similar (in-distribution) or different\n(out-of-distribution) relative to actions in the training set. We evaluated\nthree architectures (model ensembles, autoencoders, and generative adversarial\nnetworks) on an offline dataset and tested the strongest performing\narchitecture (ensemble of gait phase estimators) online. The online test\ndemonstrated the ability of our uncertainty estimator to turn assistance on and\noff as the user transitioned between in-distribution and out-of-distribution\ntasks (F1: 89.2). This new framework provides a path for exoskeletons to safely\nand autonomously support human movement in unstructured, everyday environments.", "AI": {"tldr": "本文提出了一种不确定性感知控制框架，使踝关节外骨骼能够通过自动识别和脱离不熟悉动作，在多样化场景中安全辅助人体运动。", "motivation": "现有下肢外骨骼控制器仅限于在受控环境中执行离散、预定义的动作，这限制了它们在现实世界中的应用。研究旨在解决这一局限性，使外骨骼能在非结构化日常环境中安全运行。", "method": "该方法使用不确定性估计器将动作分类为已知（分布内）或未知（分布外）。离线评估了三种架构（模型集成、自编码器和生成对抗网络），并在线测试了表现最佳的集成步态相位估计器架构。", "result": "在线测试表明，不确定性估计器能够有效地在用户从分布内任务切换到分布外任务时开启和关闭辅助，F1分数为89.2。", "conclusion": "该框架为外骨骼在非结构化日常环境中安全、自主地辅助人体运动提供了新途径。"}}
{"id": "2508.21091", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.21091", "abs": "https://arxiv.org/abs/2508.21091", "authors": ["Xurui Peng", "Hong Liu", "Chenqian Yan", "Rui Ma", "Fangmin Chen", "Xing Wang", "Zhihua Wu", "Songwei Liu", "Mingbao Lin"], "title": "ERTACache: Error Rectification and Timesteps Adjustment for Efficient Diffusion", "comment": null, "summary": "Diffusion models suffer from substantial computational overhead due to their\ninherently iterative inference process. While feature caching offers a\npromising acceleration strategy by reusing intermediate outputs across\ntimesteps, naive reuse often incurs noticeable quality degradation. In this\nwork, we formally analyze the cumulative error introduced by caching and\ndecompose it into two principal components: feature shift error, caused by\ninaccuracies in cached outputs, and step amplification error, which arises from\nerror propagation under fixed timestep schedules. To address these issues, we\npropose ERTACache, a principled caching framework that jointly rectifies both\nerror types. Our method employs an offline residual profiling stage to identify\nreusable steps, dynamically adjusts integration intervals via a\ntrajectory-aware correction coefficient, and analytically approximates\ncache-induced errors through a closed-form residual linearization model.\nTogether, these components enable accurate and efficient sampling under\naggressive cache reuse. Extensive experiments across standard image and video\ngeneration benchmarks show that ERTACache achieves up to 2x inference speedup\nwhile consistently preserving or even improving visual quality. Notably, on the\nstate-of-the-art Wan2.1 video diffusion model, ERTACache delivers 2x\nacceleration with minimal VBench degradation, effectively maintaining baseline\nfidelity while significantly improving efficiency. The code is available at\nhttps://github.com/bytedance/ERTACache.", "AI": {"tldr": "ERTACache是一种新的缓存框架，旨在通过纠正特征偏移和步长放大错误，显著加速扩散模型的推理过程，同时保持或提高生成质量，最高可达2倍提速。", "motivation": "扩散模型因其迭代推理过程而导致计算开销巨大。虽然特征缓存可以加速，但简单重用中间输出会导致明显的质量下降。研究旨在解决这一问题，同时保持生成质量。", "method": "ERTACache通过正式分析累积误差（特征偏移误差和步长放大误差），并提出一个联合纠正这两种错误类型的框架。具体方法包括：离线残差分析以识别可重用步骤；通过轨迹感知校正系数动态调整积分间隔；以及通过闭式残差线性化模型分析性地近似缓存引起的误差。", "result": "ERTACache在标准图像和视频生成基准上实现了高达2倍的推理加速，同时始终保持甚至提高了视觉质量。特别是在最先进的Wan2.1视频扩散模型上，它实现了2倍加速，而VBench退化最小，有效保持了基线保真度。", "conclusion": "ERTACache是一个原则性的缓存框架，能够通过精确分析和纠正缓存引起的误差，在激进的缓存重用下实现准确高效的采样，显著提高了扩散模型的效率而没有牺牲生成质量。"}}
{"id": "2508.21320", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.21320", "abs": "https://arxiv.org/abs/2508.21320", "authors": ["Mohsen Nayebi Kerdabadi", "Arya Hadizadeh Moghaddam", "Dongjie Wang", "Zijun Yao"], "title": "Multi-Ontology Integration with Dual-Axis Propagation for Medical Concept Representation", "comment": "This work has been accepted as a full research paper at CIKM 2025", "summary": "Medical ontology graphs map external knowledge to medical codes in electronic\nhealth records via structured relationships. By leveraging domain-approved\nconnections (e.g., parent-child), predictive models can generate richer medical\nconcept representations by incorporating contextual information from related\nconcepts. However, existing literature primarily focuses on incorporating\ndomain knowledge from a single ontology system, or from multiple ontology\nsystems (e.g., diseases, drugs, and procedures) in isolation, without\nintegrating them into a unified learning structure. Consequently, concept\nrepresentation learning often remains limited to intra-ontology relationships,\noverlooking cross-ontology connections. In this paper, we propose LINKO, a\nlarge language model (LLM)-augmented integrative ontology learning framework\nthat leverages multiple ontology graphs simultaneously by enabling dual-axis\nknowledge propagation both within and across heterogeneous ontology systems to\nenhance medical concept representation learning. Specifically, LINKO first\nemploys LLMs to provide a graph-retrieval-augmented initialization for ontology\nconcept embedding, through an engineered prompt that includes concept\ndescriptions, and is further augmented with ontology context. Second, our\nmethod jointly learns the medical concepts in diverse ontology graphs by\nperforming knowledge propagation in two axes: (1) intra-ontology vertical\npropagation across hierarchical ontology levels and (2) inter-ontology\nhorizontal propagation within every level in parallel. Last, through extensive\nexperiments on two public datasets, we validate the superior performance of\nLINKO over state-of-the-art baselines. As a plug-in encoder compatible with\nexisting EHR predictive models, LINKO further demonstrates enhanced robustness\nin scenarios involving limited data availability and rare disease prediction.", "AI": {"tldr": "本文提出了LINKO，一个由大型语言模型（LLM）增强的集成本体学习框架，通过在异构本体系统内部和之间进行双轴知识传播，来提升医学概念表示学习。", "motivation": "现有研究主要关注单一本体系统或孤立的多个本体系统（如疾病、药物、手术）中的领域知识整合，导致概念表示学习仅限于本体内部关系，忽略了跨本体连接。", "method": "LINKO框架首先利用LLM提供图检索增强的本体概念嵌入初始化，通过包含概念描述和本体上下文的精心设计的提示词实现。其次，该方法通过双轴知识传播共同学习不同本体图中的医学概念：(1) 本体内垂直传播（跨层级本体级别）和 (2) 本体间水平传播（在每个级别内并行）。", "result": "LINKO在两个公共数据集上的实验结果表明，其性能优于现有最先进的基线方法。作为一个可与现有电子健康记录（EHR）预测模型兼容的插件编码器，LINKO在数据可用性有限和罕见疾病预测场景中也表现出更强的鲁棒性。", "conclusion": "LINKO通过整合多个本体图并利用LLM进行双轴知识传播，有效增强了医学概念表示学习，并在各种预测任务中展示了卓越的性能和鲁棒性。"}}
{"id": "2508.21164", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.21164", "abs": "https://arxiv.org/abs/2508.21164", "authors": ["Muskan Saraf", "Sajjad Rezvani Boroujeni", "Justin Beaudry", "Hossein Abedi", "Tom Bush"], "title": "Quantifying Label-Induced Bias in Large Language Model Self- and Cross-Evaluations", "comment": null, "summary": "Large language models (LLMs) are increasingly used to evaluate outputs, yet\ntheir judgments may be influenced. This study examines bias in self- and\ncross-model evaluations by ChatGPT, Gemini, and Claude under four conditions:\nno labels, true labels, and two false-label scenarios. Blog posts authored by\neach model were evaluated by all three using both overall preference voting and\nquality ratings for Coherence, Informativeness, and Conciseness, with all\nscores expressed as percentages for direct comparison. Results reveal striking\nasymmetries: the \"Claude\" label consistently boosts scores, while the \"Gemini\"\nlabel consistently depresses them, regardless of actual content. False labels\nfrequently reversed rankings, producing shifts of up to 50 percentage points in\npreference votes and up to 12 percentage points in converted quality ratings.\nGemini's self-scores collapsed under true labels, while Claude's\nself-preference intensified. These findings show that perceived model identity\ncan heavily distort high-level judgments and subtly influence detailed quality\nratings, underscoring the need for blind or multimodel evaluation protocols to\nensure fairness in LLM benchmarking.", "AI": {"tldr": "研究发现，大型语言模型在进行自我或跨模型评估时，其判断会受到模型身份标签的显著影响，导致评分和排名出现偏差。", "motivation": "大型语言模型（LLMs）越来越多地被用于评估输出，但它们的判断可能受到偏见的影响。本研究旨在探讨ChatGPT、Gemini和Claude在自我和跨模型评估中存在的偏见。", "method": "研究在四种标签条件下（无标签、真实标签、两种虚假标签情景）对ChatGPT、Gemini和Claude进行了评估。每个模型撰写的博客文章由所有三个模型进行评估，评估方式包括整体偏好投票和针对连贯性、信息量和简洁性的质量评分，所有分数均以百分比表示以便直接比较。", "result": "结果显示出显著的不对称性：\"Claude\"标签始终能提高分数，而\"Gemini\"标签则始终会降低分数，无论实际内容如何。虚假标签经常逆转排名，导致偏好投票中出现高达50个百分点的变化，转换后的质量评分中出现高达12个百分点的变化。在真实标签下，Gemini的自我评分崩溃，而Claude的自我偏好则增强。", "conclusion": "这些发现表明，感知的模型身份会严重扭曲高层判断，并微妙地影响详细的质量评分，强调了在LLM基准测试中需要采用盲测或多模型评估协议以确保公平性。"}}
{"id": "2508.21586", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.21586", "abs": "https://arxiv.org/abs/2508.21586", "authors": ["Poulomee Ghosh", "Shubhendu Bhasin"], "title": "Model Reference Adaptive Control with Time-Varying State and Input Constraints", "comment": null, "summary": "This paper presents a model reference adaptive control (MRAC) framework for\nuncertain linear time-invariant (LTI) systems subject to user-defined,\ntime-varying state and input constraints. The proposed design seamlessly\nintegrates a time-varying barrier Lyapunov function (TVBLF) to enforce state\nconstraints with a time-varying saturation function to handle input limits.\nThese time-varying constraints can be designed as performance functions to\nshape transient and steady-state behaviors for both state and input. A key\ncontribution is the derivation of a verifiable, offline feasibility condition\nto check the existence of a valid control policy for a given set of\nconstraints. To the best of our knowledge, this is the first adaptive control\nmethodology to simultaneously handle both time-varying state and input\nconstraints without resorting to online optimization. Simulation results\nvalidate the efficacy of the proposed constrained MRAC scheme.", "AI": {"tldr": "本文提出了一种模型参考自适应控制（MRAC）框架，用于处理不确定线性时不变（LTI）系统中的时变状态和输入约束，且无需在线优化。", "motivation": "在不确定LTI系统中，同时处理用户定义的时变状态和输入约束是一个挑战，现有自适应控制方法通常需要在线优化来解决此类问题。", "method": "该方法将时变障碍李雅普诺夫函数（TVBLF）与时变饱和函数无缝集成，分别用于强制执行状态约束和处理输入限制。此外，还推导出了一个可验证的离线可行性条件，以检查给定约束下有效控制策略的存在性。", "result": "仿真结果验证了所提出的约束MRAC方案的有效性。据作者所知，这是首个无需在线优化即可同时处理时变状态和输入约束的自适应控制方法。", "conclusion": "该MRAC框架能够有效地在不确定LTI系统中处理用户定义的时变状态和输入约束，并通过离线可行性条件确保控制策略的存在，为相关应用提供了新的解决方案。"}}
{"id": "2508.21260", "categories": ["cs.RO", "eess.SP", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2508.21260", "abs": "https://arxiv.org/abs/2508.21260", "authors": ["Tara Mina", "Lindsey Marinello", "John Christian"], "title": "Remarks on stochastic cloning and delayed-state filtering", "comment": null, "summary": "Many estimation problems in robotics and navigation involve measurements that\ndepend on prior states. A prominent example is odometry, which measures the\nrelative change between states over time. Accurately handling these\ndelayed-state measurements requires capturing their correlations with prior\nstate estimates, and a widely used approach is stochastic cloning (SC), which\naugments the state vector to account for these correlations.\n  This work revisits a long-established but often overlooked alternative--the\ndelayed-state Kalman filter--and demonstrates that a properly derived filter\nyields exactly the same state and covariance update as SC, without requiring\nstate augmentation. Moreover, the generalized Kalman filter formulation\nprovides computational advantages, while also reducing memory requirements for\nhigher-dimensional states.\n  Our findings clarify a common misconception that Kalman filter variants are\ninherently unable to handle correlated delayed-state measurements,\ndemonstrating that an alternative formulation achieves the same results more\nefficiently.", "AI": {"tldr": "本文重新审视了延迟状态卡尔曼滤波器，并证明其在处理延迟状态测量方面与随机克隆方法效果相同，但无需状态增强，且具有计算和内存效率优势，纠正了卡尔曼滤波器无法处理此类测量的常见误解。", "motivation": "机器人和导航中的许多估计问题涉及依赖于先前状态的测量（如里程计）。准确处理这些延迟状态测量需要捕捉它们与先前状态估计之间的相关性。随机克隆（SC）是常用方法，但需要状态增强，这可能带来计算和内存负担。", "method": "本文重新审视并正确推导了延迟状态卡尔曼滤波器（DSKF）。通过与随机克隆（SC）进行比较，分析了DSKF在状态和协方差更新方面的表现。", "result": "研究发现，一个正确推导的延迟状态卡尔曼滤波器（DSKF）能够产生与随机克隆（SC）完全相同的状态和协方差更新。此外，DSKF在不进行状态增强的情况下，提供了计算优势，并减少了高维状态的内存需求。", "conclusion": "卡尔曼滤波器变体并非天生无法处理相关的延迟状态测量。通过适当的公式推导，延迟状态卡尔曼滤波器能够更有效地实现与随机克隆相同的结果，纠正了这一常见误解。"}}
{"id": "2508.21094", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.21094", "abs": "https://arxiv.org/abs/2508.21094", "authors": ["Zheyu Fan", "Jiateng Liu", "Yuji Zhang", "Zihan Wang", "Yi R.", "Fung", "Manling Li", "Heng Ji"], "title": "Video-LLMs with Temporal Visual Screening", "comment": null, "summary": "Humans naturally perform temporal screening by dragging the progress bar and\nfocusing on salient temporal segments, but current Video Large Language Models\n(Video-LLMs) struggle to capture fine-grained temporal semantics due to sparse\nframe sampling and insufficient inter-frame reasoning supervision during their\ntraining. To address this, Inspired by well-established cognitive science\nprinciples, we propose Temporal Visual Screening (TVS), a new task that\nuniversally pre-processes video question answering and instruction tuning data\nby: (1) retaining focus-critical video segments, (2) synchronously\nreconstructing queries to their most direct form while preserving answer\nconsistency, and (3) keeping the invariance and consistency for any possible\nanswer. TVS is formulated as a modular front-end adapter task that can be\nseamlessly integrated into both Video Instruction Tuning (training) and Video\nQuestion Answering (inference) pipelines. TVS optimizes distribution of\nreasoning burden and cognitive load; during training, it aligns queries with\nfocus-critical visual information; at inference, it enables query-aware segment\nfocus and streamlined query representations. In particular, we curate the first\nbenchmark for TVS and propose ReSimplifyIt, a baseline outperforming prior\napproaches on seemingly similar tasks by 0.47 in F-1 score on video trimming\nwhile achieving competitive query rewriting performance. Experiments\ndemonstrate that incorporating TVS yields relative gains of 7.33% (training)\nand 34.6% (inference), demonstrating the effectiveness of temporal information\nscreening for improving video-language understanding.", "AI": {"tldr": "本文提出了一种名为“时序视觉筛选”（TVS）的新任务，旨在通过保留关键视频片段并同步重构查询，解决当前视频大语言模型（Video-LLMs）在捕获细粒度时序语义方面的不足，从而提高视频-语言理解能力。", "motivation": "人类自然地通过拖动进度条和关注显著时序片段来进行时序筛选，但当前的Video-LLMs由于稀疏帧采样和训练期间缺乏足够的帧间推理监督，难以捕捉细粒度时序语义。", "method": "受认知科学原理启发，本文提出了时序视觉筛选（TVS）任务，作为预处理视频问答和指令微调数据的通用方法。TVS通过以下方式实现：1) 保留焦点关键视频片段；2) 同步将查询重构为最直接的形式，同时保持答案一致性；3) 保持任何可能答案的不变性和一致性。TVS被设计为一个模块化的前端适配器任务，可无缝集成到Video指令微调（训练）和Video问答（推理）流程中。本文还策划了第一个TVS基准，并提出了ReSimplifyIt基线模型。", "result": "ReSimplifyIt基线模型在视频剪辑任务上比现有方法高出0.47的F-1分数，并在查询重写方面表现出竞争力。实验表明，整合TVS在训练阶段带来了7.33%的相对增益，在推理阶段带来了34.6%的相对增益。", "conclusion": "时序信息筛选（TVS）对于提高视频-语言理解能力是有效的，通过优化推理负担和认知负荷，显著提升了Video-LLMs的性能。"}}
{"id": "2508.21365", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.21365", "abs": "https://arxiv.org/abs/2508.21365", "authors": ["Yi Liao", "Yu Gu", "Yuan Sui", "Zining Zhu", "Yifan Lu", "Guohua Tang", "Zhongqian Sun", "Wei Yang"], "title": "Think in Games: Learning to Reason in Games via Reinforcement Learning with Large Language Models", "comment": null, "summary": "Large language models (LLMs) excel at complex reasoning tasks such as\nmathematics and coding, yet they frequently struggle with simple interactive\ntasks that young children perform effortlessly. This discrepancy highlights a\ncritical gap between declarative knowledge (knowing about something) and\nprocedural knowledge (knowing how to do something). Although traditional\nreinforcement learning (RL) agents can acquire procedural knowledge through\nenvironmental interaction, they often operate as black boxes and require\nsubstantial training data. In contrast, LLMs possess extensive world knowledge\nand reasoning capabilities, but are unable to effectively convert this static\nknowledge into dynamic decision-making in interactive settings. To address this\nchallenge, we propose Think in Games (TiG), a novel framework that empowers\nLLMs to develop procedural understanding through direct interaction with game\nenvironments, while retaining their inherent reasoning and explanatory\nabilities. Specifically, TiG reformulates RL-based decision-making as a\nlanguage modeling task: LLMs generate language-guided policies, which are\nrefined iteratively through online reinforcement learning based on\nenvironmental feedback. Our experimental results show that TiG successfully\nbridges the gap between declarative and procedural knowledge, achieving\ncompetitive performance with dramatically lower data and computational demands\ncompared to conventional RL methods. Moreover, TiG provides step-by-step\nnatural language explanations for its decisions, greatly improving transparency\nand interpretability in complex interactive tasks.", "AI": {"tldr": "TiG框架使大型语言模型（LLMs）通过与游戏环境的直接交互来学习程序性知识，将强化学习决策重新定义为语言建模任务，从而弥合了声明性知识和程序性知识之间的鸿沟，并提供了可解释的决策。", "motivation": "大型语言模型（LLMs）在复杂推理任务上表现出色，但在简单的交互任务上却很吃力，这凸显了声明性知识和程序性知识之间的关键差距。传统的强化学习（RL）代理虽然能获取程序性知识，但通常是黑箱且需要大量训练数据，而LLMs缺乏将静态知识转化为动态决策的能力。", "method": "提出了“Think in Games (TiG)”框架。TiG将基于RL的决策制定重新构想为语言建模任务：LLMs生成语言引导的策略，这些策略通过基于环境反馈的在线强化学习进行迭代优化。", "result": "TiG成功弥合了声明性知识和程序性知识之间的差距，与传统RL方法相比，以极低的数据和计算需求实现了具有竞争力的性能。此外，TiG为其决策提供了分步的自然语言解释，显著提高了复杂交互任务的透明度和可解释性。", "conclusion": "TiG框架有效地使LLMs通过直接交互获取程序性理解，同时保留其推理和解释能力，从而实现了高效、高性能和可解释的交互式智能体。"}}
{"id": "2508.21184", "categories": ["cs.CL", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.21184", "abs": "https://arxiv.org/abs/2508.21184", "authors": ["Deepro Choudhury", "Sinead Williamson", "Adam Goliński", "Ning Miao", "Freddie Bickford Smith", "Michael Kirchhof", "Yizhe Zhang", "Tom Rainforth"], "title": "BED-LLM: Intelligent Information Gathering with LLMs and Bayesian Experimental Design", "comment": null, "summary": "We propose a general-purpose approach for improving the ability of Large\nLanguage Models (LLMs) to intelligently and adaptively gather information from\na user or other external source using the framework of sequential Bayesian\nexperimental design (BED). This enables LLMs to act as effective multi-turn\nconversational agents and interactively interface with external environments.\nOur approach, which we call BED-LLM (Bayesian Experimental Design with Large\nLanguage Models), is based on iteratively choosing questions or queries that\nmaximize the expected information gain (EIG) about the task of interest given\nthe responses gathered previously. We show how this EIG can be formulated in a\nprincipled way using a probabilistic model derived from the LLM's belief\ndistribution and provide detailed insights into key decisions in its\nconstruction. Further key to the success of BED-LLM are a number of specific\ninnovations, such as a carefully designed estimator for the EIG, not solely\nrelying on in-context updates for conditioning on previous responses, and a\ntargeted strategy for proposing candidate queries. We find that BED-LLM\nachieves substantial gains in performance across a wide range of tests based on\nthe 20-questions game and using the LLM to actively infer user preferences,\ncompared to direct prompting of the LLM and other adaptive design strategies.", "AI": {"tldr": "本文提出了一种名为BED-LLM的通用方法，通过序贯贝叶斯实验设计（BED）框架，显著提升大型语言模型（LLMs）智能且自适应地从用户或外部来源收集信息的能力。", "motivation": "研究动机是使LLMs能够作为高效的多轮对话代理，并与外部环境进行交互，这需要它们能够智能且自适应地收集信息。", "method": "BED-LLM方法基于迭代选择能够最大化预期信息增益（EIG）的问题或查询，该EIG通过LLM的信念分布导出的概率模型进行原理性公式化。关键创新包括精心设计的EIG估计器、不完全依赖上下文更新来条件化先前响应，以及有针对性地提出候选查询的策略。", "result": "BED-LLM在基于“20个问题”游戏和LLM主动推断用户偏好等广泛测试中，相比直接提示LLM和其他自适应设计策略，实现了显著的性能提升。", "conclusion": "BED-LLM通过引入贝叶斯实验设计，显著增强了LLMs智能和自适应信息收集的能力，使其在多轮对话和与外部环境交互方面表现更出色。"}}
{"id": "2508.21610", "categories": ["eess.SY", "cs.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2508.21610", "abs": "https://arxiv.org/abs/2508.21610", "authors": ["Guangdi Hu", "Keyi Liao", "Jian Ye", "Feng Guo"], "title": "Adaptive Dead-Zone Dual Sliding Mode Observer for Reliable Electrochemical Model-Based SOC Estimation", "comment": "36 pages, 5 figures", "summary": "Accurate state of charge (SOC) estimation is critical for ensuring the\nsafety, reliability, and efficiency of lithium-ion batteries in electric\nvehicles and energy storage systems. Electrochemical models provide high\nfidelity for SOC estimation but introduce challenges due to parameter\nvariations, nonlinearities, and computational complexity. To address these\nissues, this paper proposes an adaptive dead-zone dual sliding mode\nobserver(SMO) based on an improved electrochemical single-particle model. The\nalgorithm integrates a state observer for SOC estimation and a parameter\nobserver for online parameter adaptation. A Lyapunov-derived adaptive dead-zone\nis introduced to ensure stability, activating parameter updates only when the\nterminal voltage error lies within a rigorously defined bound. The proposed\nmethod was validated under constant-current and UDDS dynamic conditions.\nResults demonstrate that the adaptive dead-zone dual SMO achieves superior\naccuracy compared with conventional dual SMO and equivalent circuit model-based\nEKF methods, maintaining SOC estimation errors within 0.2% under correct\ninitialization and below 1% under a 30% initial SOC error, with rapid\nconvergence. Computational efficiency analysis further shows that the adaptive\ndead-zone dual sliding mode observer reduces execution time compared with the\nconventional dual SMO by limiting unnecessary parameter updates, highlighting\nits suitability for real-time battery management applications. Moreover,\nrobustness under battery aging was confirmed using a cycle-aging model, where\nthe adaptive dead-zone dual SMO maintained stable SOC estimation despite\nparameter drift. These findings indicate that the proposed method offers a\nreliable, accurate, and computationally efficient solution for SOC estimation.", "AI": {"tldr": "本文提出了一种基于改进电化学单颗粒模型的自适应死区双滑模观测器（SMO），用于锂离子电池荷电状态（SOC）估计，实现了高精度、计算效率和对电池老化的鲁棒性。", "motivation": "准确的SOC估算对于锂离子电池的安全、可靠性和效率至关重要。电化学模型虽然精度高，但面临参数变化、非线性和计算复杂性等挑战，需要解决这些问题以实现实际应用。", "method": "该研究提出了一种基于改进电化学单颗粒模型的自适应死区双滑模观测器。该算法集成了用于SOC估算的状态观测器和用于在线参数自适应的参数观测器。引入了基于Lyapunov导数的自适应死区机制，确保稳定性，并仅当端电压误差在严格定义的范围内时才激活参数更新。该方法在恒流和UDDS动态条件下以及通过循环老化模型进行了验证。", "result": "与传统双SMO和基于等效电路模型的EKF方法相比，所提出的自适应死区双SMO表现出卓越的精度，在正确初始化下SOC估算误差保持在0.2%以内，在30%初始SOC误差下低于1%，且收敛迅速。通过限制不必要的参数更新，该方法还降低了计算执行时间。此外，通过循环老化模型证实了其在电池老化下的鲁棒性，即使参数漂移也能保持稳定的SOC估算。", "conclusion": "所提出的自适应死区双滑模观测器为锂离子电池的SOC估算提供了一种可靠、准确且计算高效的解决方案，非常适用于实时电池管理应用。"}}
{"id": "2508.21271", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.21271", "abs": "https://arxiv.org/abs/2508.21271", "authors": ["Pablo Moraes", "Monica Rodriguez", "Kristofer S. Kappel", "Hiago Sodre", "Santiago Fernandez", "Igor Nunes", "Bruna Guterres", "Ricardo Grando"], "title": "Mini Autonomous Car Driving based on 3D Convolutional Neural Networks", "comment": null, "summary": "Autonomous driving applications have become increasingly relevant in the\nautomotive industry due to their potential to enhance vehicle safety,\nefficiency, and user experience, thereby meeting the growing demand for\nsophisticated driving assistance features. However, the development of reliable\nand trustworthy autonomous systems poses challenges such as high complexity,\nprolonged training periods, and intrinsic levels of uncertainty. Mini\nAutonomous Cars (MACs) are used as a practical testbed, enabling validation of\nautonomous control methodologies on small-scale setups. This simplified and\ncost-effective environment facilitates rapid evaluation and comparison of\nmachine learning models, which is particularly useful for algorithms requiring\nonline training. To address these challenges, this work presents a methodology\nbased on RGB-D information and three-dimensional convolutional neural networks\n(3D CNNs) for MAC autonomous driving in simulated environments. We evaluate the\nproposed approach against recurrent neural networks (RNNs), with architectures\ntrained and tested on two simulated tracks with distinct environmental\nfeatures. Performance was assessed using task completion success, lap-time\nmetrics, and driving consistency. Results highlight how architectural\nmodifications and track complexity influence the models' generalization\ncapability and vehicle control performance. The proposed 3D CNN demonstrated\npromising results when compared with RNNs.", "AI": {"tldr": "本研究提出了一种基于RGB-D信息和3D CNNs的方法，用于模拟环境中的迷你自动驾驶汽车（MACs），并与RNNs进行比较，结果显示3D CNNs表现出更好的前景。", "motivation": "自动驾驶应用对提高车辆安全性、效率和用户体验至关重要，但其开发面临高复杂性、训练周期长和不确定性等挑战。迷你自动驾驶汽车（MACs）提供了一个实用且经济高效的测试平台，用于快速评估和比较机器学习模型，尤其适用于需要在线训练的算法。", "method": "本研究提出了一种基于RGB-D信息和三维卷积神经网络（3D CNNs）的方法，用于模拟环境中的MACs自动驾驶。该方法在两个具有不同环境特征的模拟赛道上进行训练和测试，并与循环神经网络（RNNs）进行对比评估。性能评估指标包括任务完成成功率、圈速和驾驶一致性。", "result": "研究结果表明，架构修改和赛道复杂性会影响模型的泛化能力和车辆控制性能。与RNNs相比，所提出的3D CNN在评估中展现出有前景的结果。", "conclusion": "本研究提出了一种基于RGB-D和3D CNNs的MACs自动驾驶方法，并在模拟环境中展示了其优于RNNs的潜力，为解决自动驾驶系统的复杂性和不确定性挑战提供了新的思路。"}}
{"id": "2508.21096", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.21096", "abs": "https://arxiv.org/abs/2508.21096", "authors": ["Zhe Han", "Charlie Budd", "Gongyu Zhang", "Huanyu Tian", "Christos Bergeles", "Tom Vercauteren"], "title": "ROBUST-MIPS: A Combined Skeletal Pose and Instance Segmentation Dataset for Laparoscopic Surgical Instruments", "comment": null, "summary": "Localisation of surgical tools constitutes a foundational building block for\ncomputer-assisted interventional technologies. Works in this field typically\nfocus on training deep learning models to perform segmentation tasks.\nPerformance of learning-based approaches is limited by the availability of\ndiverse annotated data. We argue that skeletal pose annotations are a more\nefficient annotation approach for surgical tools, striking a balance between\nrichness of semantic information and ease of annotation, thus allowing for\naccelerated growth of available annotated data. To encourage adoption of this\nannotation style, we present, ROBUST-MIPS, a combined tool pose and tool\ninstance segmentation dataset derived from the existing ROBUST-MIS dataset. Our\nenriched dataset facilitates the joint study of these two annotation styles and\nallow head-to-head comparison on various downstream tasks. To demonstrate the\nadequacy of pose annotations for surgical tool localisation, we set up a simple\nbenchmark using popular pose estimation methods and observe high-quality\nresults. To ease adoption, together with the dataset, we release our benchmark\nmodels and custom tool pose annotation software.", "AI": {"tldr": "该论文提出骨骼姿态标注作为手术工具定位的一种高效方法，解决了传统分割方法数据稀缺的问题。为此，作者发布了ROBUST-MIPS数据集、基准模型和标注软件，并展示了姿态标注的有效性。", "motivation": "计算机辅助介入技术中手术工具定位是基础，但当前基于深度学习的分割方法受限于多样化标注数据的可用性。作者认为骨骼姿态标注能更有效地平衡语义信息丰富性和标注便捷性，从而加速标注数据增长。", "method": "提出骨骼姿态标注方法。基于现有ROBUST-MIS数据集，构建了ROBUST-MIPS数据集，结合了工具姿态和工具实例分割。使用流行的姿态估计算法建立了简单的基准测试，以验证姿态标注的有效性。同时发布了数据集、基准模型和自定义工具姿态标注软件。", "result": "在基准测试中，使用姿态标注进行手术工具定位取得了高质量的结果，证明了其充分性。", "conclusion": "骨骼姿态标注是手术工具定位的一种高效且充分的标注方法，它在语义信息丰富性和标注便捷性之间取得了良好平衡，有助于加速可用标注数据的增长。发布的ROBUST-MIPS数据集和相关工具将促进该标注风格的采用和研究。"}}
{"id": "2508.21376", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.21376", "abs": "https://arxiv.org/abs/2508.21376", "authors": ["Tony Lee", "Haoqin Tu", "Chi Heem Wong", "Zijun Wang", "Siwei Yang", "Yifan Mai", "Yuyin Zhou", "Cihang Xie", "Percy Liang"], "title": "AHELM: A Holistic Evaluation of Audio-Language Models", "comment": null, "summary": "Evaluations of audio-language models (ALMs) -- multimodal models that take\ninterleaved audio and text as input and output text -- are hindered by the lack\nof standardized benchmarks; most benchmarks measure only one or two\ncapabilities and omit evaluative aspects such as fairness or safety.\nFurthermore, comparison across models is difficult as separate evaluations test\na limited number of models and use different prompting methods and inference\nparameters. To address these shortfalls, we introduce AHELM, a benchmark that\naggregates various datasets -- including 2 new synthetic audio-text datasets\ncalled PARADE, which evaluates the ALMs on avoiding stereotypes, and\nCoRe-Bench, which measures reasoning over conversational audio through\ninferential multi-turn question answering -- to holistically measure the\nperformance of ALMs across 10 aspects we have identified as important to the\ndevelopment and usage of ALMs: audio perception, knowledge, reasoning, emotion\ndetection, bias, fairness, multilinguality, robustness, toxicity, and safety.\nWe also standardize the prompts, inference parameters, and evaluation metrics\nto ensure equitable comparisons across models. We test 14 open-weight and\nclosed-API ALMs from 3 developers and 3 additional simple baseline systems each\nconsisting of an automatic speech recognizer and a language model. Our results\nshow that while Gemini 2.5 Pro ranks top in 5 out of 10 aspects, it exhibits\ngroup unfairness ($p=0.01$) on ASR tasks whereas most of the other models do\nnot. We also find that the baseline systems perform reasonably well on AHELM,\nwith one ranking 5th overall despite having only speech-to-text capabilities.\nFor transparency, all raw prompts, model generations, and outputs are available\non our website at https://crfm.stanford.edu/helm/audio/v1.0.0. AHELM is\nintended to be a living benchmark and new datasets and models will be added\nover time.", "AI": {"tldr": "AHELM是一个新的多方面基准测试，旨在标准化和全面评估音频语言模型（ALMs），涵盖10个关键方面，并引入新数据集。它揭示了领先模型的公平性问题以及基线系统的意外良好表现。", "motivation": "当前的音频语言模型（ALMs）评估缺乏标准化基准，大多只衡量一两个能力，忽略了公平性或安全性等重要评估维度。此外，由于评估方法、提示和推理参数的不同，模型之间的比较也十分困难。", "method": "本文引入了AHELM基准测试，它整合了多个数据集，包括两个新的合成音频文本数据集：PARADE（评估避免刻板印象）和CoRe-Bench（通过推断性多轮问答测量会话音频推理）。AHELM全面衡量ALMs在10个方面（音频感知、知识、推理、情感检测、偏见、公平性、多语言性、鲁棒性、毒性和安全性）的表现。同时，论文还标准化了提示、推理参数和评估指标，以确保模型间公平比较。研究测试了14个开源和闭源API的ALMs以及3个由自动语音识别器和语言模型组成的简单基线系统。", "result": "研究结果显示，Gemini 2.5 Pro在10个方面中有5个排名第一，但在ASR任务中表现出群体不公平性（p=0.01），而大多数其他模型则没有。此外，基线系统在AHELM上表现良好，其中一个尽管只具备语音转文本能力，但总体排名第五。", "conclusion": "AHELM提供了一个全面的、标准化的方法来评估音频语言模型，揭示了不同模型在多方面（包括公平性）的性能表现。研究发现，即使是领先的模型也可能存在公平性问题，而简单的基线系统也能取得令人惊讶的良好性能。AHELM旨在成为一个持续更新的基准测试。"}}
{"id": "2508.21201", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.21201", "abs": "https://arxiv.org/abs/2508.21201", "authors": ["Arash Ahmadi", "Sarah Sharif", "Yaser Banad"], "title": "Improving Aviation Safety Analysis: Automated HFACS Classification Using Reinforcement Learning with Group Relative Policy Optimization", "comment": null, "summary": "Analyzing the human factors behind aviation accidents is crucial for\npreventing future incidents, yet traditional methods using the Human Factors\nAnalysis and Classification System (HFACS) are limited by scalability and\nconsistency. To address this, we introduce an automated HFACS classification\nframework for aviation safety analysis that utilizes Reinforcement Learning\nwith Group Relative Policy Optimization (GRPO) to fine-tune a Llama-3.1 8B\nlanguage model. Our approach incorporates a multi-component reward system\ntailored for aviation safety analysis and integrates synthetic data generation\nto overcome class imbalance in accident datasets. The resulting GRPO-optimized\nmodel achieved noticeable performance gains, including a 350% increase in exact\nmatch accuracy (from 0.0400 to 0.1800) and an improved partial match accuracy\nof 0.8800. Significantly, our specialized model outperforms state-of-the-art\nLLMs (Large Language Models), including GPT-5-mini and Gemini-2.5-fiash, on key\nmetrics. This research also proposes exact match accuracy in multi-label HFACS\nclassification problem as a new benchmarking methodology to evaluate the\nadvanced reasoning capabilities of language models. Ultimately, our work\nvalidates that smaller, domain-optimized models can provide a computationally\nefficient and better solution for critical safety analysis. This approach makes\npowerful, low-latency deployment on resource-constrained edge devices feasible.", "AI": {"tldr": "本研究引入了一个自动化HFACS分类框架，利用强化学习（GRPO）微调Llama-3.1模型，以解决航空事故分析中传统方法的可扩展性和一致性问题，并在关键指标上超越了现有最先进的大型语言模型，同时适用于资源受限的边缘设备部署。", "motivation": "分析航空事故背后的人为因素对于预防未来事故至关重要，但传统的人为因素分析与分类系统（HFACS）方法在可扩展性和一致性方面存在局限性。", "method": "开发了一个自动化HFACS分类框架，该框架利用带有群组相对策略优化（GRPO）的强化学习来微调Llama-3.1 8B语言模型。该方法结合了针对航空安全分析量身定制的多组件奖励系统，并集成了合成数据生成以克服事故数据集中的类别不平衡问题。", "result": "GRPO优化的模型实现了显著的性能提升，包括精确匹配准确率提高了350%（从0.0400增至0.1800），部分匹配准确率达到0.8800。该专业模型在关键指标上优于现有最先进的LLM（包括GPT-5-mini和Gemini-2.5-flash）。研究还提出了在多标签HFACS分类问题中使用精确匹配准确率作为评估语言模型高级推理能力的新基准方法。", "conclusion": "经验证，更小、领域优化的模型可以为关键安全分析提供计算高效且更优的解决方案。这种方法使得在资源受限的边缘设备上实现强大、低延迟的部署成为可能。"}}
{"id": "2508.21615", "categories": ["eess.SY", "cs.LG", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.21615", "abs": "https://arxiv.org/abs/2508.21615", "authors": ["Fabian Raisch", "Max Langtry", "Felix Koch", "Ruchi Choudhary", "Christoph Goebel", "Benjamin Tischler"], "title": "Adapting to Change: A Comparison of Continual and Transfer Learning for Modeling Building Thermal Dynamics under Concept Drifts", "comment": "Currently under review", "summary": "Transfer Learning (TL) is currently the most effective approach for modeling\nbuilding thermal dynamics when only limited data are available. TL uses a\npretrained model that is fine-tuned to a specific target building. However, it\nremains unclear how to proceed after initial fine-tuning, as more operational\nmeasurement data are collected over time. This challenge becomes even more\ncomplex when the dynamics of the building change, for example, after a retrofit\nor a change in occupancy. In Machine Learning literature, Continual Learning\n(CL) methods are used to update models of changing systems. TL approaches can\nalso address this challenge by reusing the pretrained model at each update step\nand fine-tuning it with new measurement data. A comprehensive study on how to\nincorporate new measurement data over time to improve prediction accuracy and\naddress the challenges of concept drifts (changes in dynamics) for building\nthermal dynamics is still missing.\n  Therefore, this study compares several CL and TL strategies, as well as a\nmodel trained from scratch, for thermal dynamics modeling during building\noperation. The methods are evaluated using 5--7 years of simulated data\nrepresentative of single-family houses in Central Europe, including scenarios\nwith concept drifts from retrofits and changes in occupancy. We propose a CL\nstrategy (Seasonal Memory Learning) that provides greater accuracy improvements\nthan existing CL and TL methods, while maintaining low computational effort.\nSML outperformed the benchmark of initial fine-tuning by 28.1\\% without concept\ndrifts and 34.9\\% with concept drifts.", "AI": {"tldr": "本研究比较了持续学习（CL）和迁移学习（TL）策略，用于在数据持续收集和概念漂移（如改造或占用变化）情况下更新建筑热力学模型，并提出了一种名为季节记忆学习（SML）的CL策略，该策略在准确性方面优于现有方法。", "motivation": "当数据有限时，迁移学习（TL）是建模建筑热力学最有效的方法。然而，随着时间推移收集到更多操作测量数据后，如何继续更新模型，尤其是在建筑动力学发生变化（例如改造或占用变化）时，仍不清楚。目前缺乏一项关于如何随时间整合新数据以提高预测准确性并解决概念漂移挑战的全面研究。", "method": "本研究比较了几种持续学习（CL）和迁移学习（TL）策略，以及从零开始训练的模型，用于建筑运行期间的热力学建模。方法通过5-7年的模拟数据进行评估，这些数据代表了中欧的独栋住宅，包括有概念漂移（改造和占用变化）的场景。研究提出了一种新的CL策略，即季节记忆学习（SML）。", "result": "提出的SML策略比现有的CL和TL方法提供了更大的准确性改进，同时保持了较低的计算量。在没有概念漂移的情况下，SML比初始微调基准提高了28.1%；在有概念漂移的情况下，提高了34.9%。", "conclusion": "SML是一种有效的持续学习策略，能够随着时间的推移不断更新建筑热力学模型，显著提高预测准确性，并更好地应对概念漂移的挑战，优于现有的CL和TL方法。"}}
{"id": "2508.21272", "categories": ["cs.RO", "stat.CO"], "pdf": "https://arxiv.org/pdf/2508.21272", "abs": "https://arxiv.org/abs/2508.21272", "authors": ["Jaehong Oh", "Seungjun Jung", "Sawoong Kim"], "title": "Learning to Assemble the Soma Cube with Legal-Action Masked DQN and Safe ZYZ Regrasp on a Doosan M0609", "comment": "13 figures, 17 pages", "summary": "This paper presents the first comprehensive application of legal-action\nmasked Deep Q-Networks with safe ZYZ regrasp strategies to an underactuated\ngripper-equipped 6-DOF collaborative robot for autonomous Soma cube assembly\nlearning. Our approach represents the first systematic integration of\nconstraint-aware reinforcement learning with singularity-safe motion planning\non a Doosan M0609 collaborative robot. We address critical challenges in\nrobotic manipulation: combinatorial action space explosion, unsafe motion\nplanning, and systematic assembly strategy learning. Our system integrates a\nlegal-action masked DQN with hierarchical architecture that decomposes\nQ-function estimation into orientation and position components, reducing\ncomputational complexity from $O(3,132)$ to $O(116) + O(27)$ while maintaining\nsolution completeness. The robot-friendly reward function encourages\nground-first, vertically accessible assembly sequences aligned with\nmanipulation constraints. Curriculum learning across three progressive\ndifficulty levels (2-piece, 3-piece, 7-piece) achieves remarkable training\nefficiency: 100\\% success rate for Level 1 within 500 episodes, 92.9\\% for\nLevel 2, and 39.9\\% for Level 3 over 105,300 total training episodes.", "AI": {"tldr": "本文首次将合法动作掩码的深度Q网络与安全的ZYZ重抓策略应用于欠驱动夹持器协作机器人，实现了索马立方体自主组装学习，并系统地集成了约束感知强化学习与奇异性安全运动规划。", "motivation": "解决机器人操作中组合动作空间爆炸、不安全运动规划以及系统性组装策略学习等关键挑战。", "method": "采用合法动作掩码的深度Q网络（DQN），结合安全的ZYZ重抓策略。DQN采用分层架构，将Q函数估计分解为姿态和位置两部分，降低了计算复杂度。设计了鼓励“先着地、垂直可达”组装序列的机器人友好奖励函数，并使用了课程学习（2件、3件、7件）来提高训练效率。", "result": "计算复杂度从O(3,132)降低到O(116) + O(27)，同时保持解决方案完整性。在500个回合内，2件组装达到100%成功率；3件组装达到92.9%成功率；在总计105,300个训练回合中，7件组装（索马立方体）达到39.9%成功率。", "conclusion": "该方法成功地将约束感知强化学习与奇异性安全运动规划集成到协作机器人上，实现了索马立方体自主组装学习，显著提高了训练效率，并有效解决了机器人操作中的核心难题。"}}
{"id": "2508.21099", "categories": ["cs.CV", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.21099", "abs": "https://arxiv.org/abs/2508.21099", "authors": ["Xiangtao Meng", "Yingkai Dong", "Ning Yu", "Li Wang", "Zheng Li", "Shanqing Guo"], "title": "Safe-Control: A Safety Patch for Mitigating Unsafe Content in Text-to-Image Generation Models", "comment": null, "summary": "Despite the advancements in Text-to-Image (T2I) generation models, their\npotential for misuse or even abuse raises serious safety concerns. Model\ndevelopers have made tremendous efforts to introduce safety mechanisms that can\naddress these concerns in T2I models. However, the existing safety mechanisms,\nwhether external or internal, either remain susceptible to evasion under\ndistribution shifts or require extensive model-specific adjustments. To address\nthese limitations, we introduce Safe-Control, an innovative plug-and-play\nsafety patch designed to mitigate unsafe content generation in T2I models.\nUsing data-driven strategies and safety-aware conditions, Safe-Control injects\nsafety control signals into the locked T2I model, acting as an update in a\npatch-like manner. Model developers can also construct various safety patches\nto meet the evolving safety requirements, which can be flexibly merged into a\nsingle, unified patch. Its plug-and-play design further ensures adaptability,\nmaking it compatible with other T2I models of similar denoising architecture.\nWe conduct extensive evaluations on six diverse and public T2I models.\nEmpirical results highlight that Safe-Control is effective in reducing unsafe\ncontent generation across six diverse T2I models with similar generative\narchitectures, yet it successfully maintains the quality and text alignment of\nbenign images. Compared to seven state-of-the-art safety mechanisms, including\nboth external and internal defenses, Safe-Control significantly outperforms all\nbaselines in reducing unsafe content generation. For example, it reduces the\nprobability of unsafe content generation to 7%, compared to approximately 20%\nfor most baseline methods, under both unsafe prompts and the latest adversarial\nattacks.", "AI": {"tldr": "Safe-Control是一个即插即用的安全补丁，通过注入安全控制信号，有效减少Text-to-Image (T2I)模型中的不安全内容生成，同时保持图像质量，并显著优于现有安全机制。", "motivation": "尽管Text-to-Image (T2I)模型取得了进步，但其潜在的滥用引发了严重的安全问题。现有安全机制（无论是外部还是内部）在分布偏移下容易被规避，或者需要大量的模型特定调整。", "method": "本文引入了Safe-Control，一个创新的即插即用安全补丁。它采用数据驱动策略和安全感知条件，向锁定的T2I模型注入安全控制信号，以补丁的形式进行更新。该设计允许开发人员构建和合并多个安全补丁，并确保其与具有类似去噪架构的其他T2I模型兼容。", "result": "在六个不同的T2I模型上进行的广泛评估表明，Safe-Control能有效减少不安全内容生成，同时成功保持良性图像的质量和文本对齐。与七种最先进的安全机制相比，Safe-Control在减少不安全内容生成方面显著优于所有基线，例如在不安全提示和最新对抗性攻击下，将不安全内容生成概率降低到7%，而大多数基线方法约为20%。", "conclusion": "Safe-Control提供了一个有效、灵活且可兼容的解决方案，用于减轻T2I模型中的不安全内容生成，克服了现有安全机制的局限性。"}}
{"id": "2508.21394", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.21394", "abs": "https://arxiv.org/abs/2508.21394", "authors": ["Bor-Sung Liang"], "title": "AI Compute Architecture and Evolution Trends", "comment": "29 pages, 26 figures", "summary": "The focus of AI development has shifted from academic research to practical\napplications. However, AI development faces numerous challenges at various\nlevels. This article will attempt to analyze the opportunities and challenges\nof AI from several different perspectives using a structured approach. This\narticle proposes a seven-layer model for AI compute architecture, including\nPhysical Layer, Link Layer, Neural Network Layer, Context Layer, Agent Layer,\nOrchestrator Layer, and Application Layer, from bottom to top. It also explains\nhow AI computing has evolved into this 7-layer architecture through the\nthree-stage evolution on large-scale language models (LLMs). For each layer, we\ndescribe the development trajectory and key technologies. In Layers 1 and 2 we\ndiscuss AI computing issues and the impact of Scale-Up and Scale-Out strategies\non computing architecture. In Layer 3 we explore two different development\npaths for LLMs. In Layer 4 we discuss the impact of contextual memory on LLMs\nand compares it to traditional processor memory. In Layers 5 to 7 we discuss\nthe trends of AI agents and explore the issues in evolution from a single AI\nagent to an AI-based ecosystem, and their impact on the AI industry.\nFurthermore, AI development involves not only technical challenges but also the\neconomic issues to build self-sustainable ecosystem. This article analyzes the\ninternet industry to provide predictions on the future trajectory of AI\ndevelopment.", "AI": {"tldr": "本文提出一个七层AI计算架构模型（物理层到应用层），阐述了其通过大型语言模型（LLMs）演进的过程，并分析了各层级的关键技术、发展趋势以及AI发展面临的技术和经济挑战，并基于互联网行业经验对AI未来轨迹进行预测。", "motivation": "AI发展已从学术研究转向实际应用，但面临多层面挑战。研究旨在通过结构化方法分析AI的机遇与挑战。", "method": "文章提出一个七层AI计算架构模型（物理层、链路层、神经网络层、上下文层、代理层、编排层和应用层），并解释了AI计算如何通过大型语言模型（LLMs）的三阶段演进形成此架构。对每一层都描述了其发展轨迹和关键技术，并分析了互联网行业以预测AI的未来发展。", "result": "提出了一个七层AI计算架构模型；解释了AI计算如何通过LLMs演进至此架构；详细阐述了各层级的关键技术、发展路径（如计算问题、LLMs发展路径、上下文记忆、AI代理和生态系统）；指出了AI发展不仅面临技术挑战，还需解决构建可持续生态系统的经济问题；通过分析互联网行业对AI未来发展轨迹进行了预测。", "conclusion": "AI发展需从多层次架构角度应对技术和经济挑战。所提出的七层模型提供了一个结构化视角，大型语言模型驱动的AI代理和生态系统演进将深刻影响AI产业，其发展轨迹可借鉴互联网行业的经验。"}}
{"id": "2508.21206", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.21206", "abs": "https://arxiv.org/abs/2508.21206", "authors": ["Han Yang", "Jian Lan", "Yihong Liu", "Hinrich Schütze", "Thomas Seidl"], "title": "Enhancing Robustness of Autoregressive Language Models against Orthographic Attacks via Pixel-based Approach", "comment": null, "summary": "Autoregressive language models are vulnerable to orthographic attacks, where\ninput text is perturbed with characters from multilingual alphabets, leading to\nsubstantial performance degradation. This vulnerability primarily stems from\nthe out-of-vocabulary issue inherent in subword tokenizers and their\nembeddings. To address this limitation, we propose a pixel-based generative\nlanguage model that replaces the text-based embeddings with pixel-based\nrepresentations by rendering words as individual images. This design provides\nstronger robustness to noisy inputs, while an extension of compatibility to\nmultilingual text across diverse writing systems. We evaluate the proposed\nmethod on the multilingual LAMBADA dataset, WMT24 dataset and the SST-2\nbenchmark, demonstrating both its resilience to orthographic noise and its\neffectiveness in multilingual settings.", "AI": {"tldr": "该研究提出了一种基于像素的生成式语言模型，通过将单词渲染为图像，以解决自回归语言模型在正字法攻击下的脆弱性，并提高了多语言兼容性和鲁棒性。", "motivation": "自回归语言模型容易受到正字法攻击（即输入文本被多语言字母字符扰动），导致性能显著下降。这种脆弱性主要源于子词分词器及其嵌入固有的词汇外(OOV)问题。", "method": "提出了一种基于像素的生成式语言模型。该模型通过将单词渲染为独立图像，用像素级表示取代了传统的基于文本的嵌入。", "result": "在多语言LAMBADA数据集、WMT24数据集和SST-2基准测试上进行了评估。结果表明，该方法对正字法噪声具有更强的弹性，并在多语言设置中表现出有效性。", "conclusion": "基于像素的语言模型能够有效增强对正字法噪声的鲁棒性，并通过避免子词分词器的OOV问题，扩展了对多种书写系统多语言文本的兼容性。"}}
{"id": "2508.21687", "categories": ["eess.SY", "cs.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2508.21687", "abs": "https://arxiv.org/abs/2508.21687", "authors": ["Tianyang Yi", "D. Adrian Maldonado", "Anirudh Subramanyam"], "title": "Chance-Constrained DC Optimal Power Flow Using Constraint-Informed Statistical Estimation", "comment": null, "summary": "Chance-constrained optimization has emerged as a promising framework for\nmanaging uncertainties in power systems. This work advances its application to\nthe DC Optimal Power Flow (DC-OPF) model, developing a novel approach to\nuncertainty modeling and estimation. Current methods typically tackle these\nproblems by first modeling random nodal injections using high-dimensional\nstatistical distributions that scale with the number of buses, followed by\nderiving deterministic reformulations of the probabilistic constraints. We\npropose an alternative methodology that exploits the constraint structure to\ninform the uncertainties to be estimated, enabling significant dimensionality\nreduction. Rather than learning joint distributions of net-load forecast errors\nacross units, we instead directly model the one-dimensional aggregate system\nforecast error and two-dimensional line errors weighted by power transfer\ndistribution factors. We evaluate our approach under both Gaussian and\nnon-Gaussian distributions on synthetic and real-world datasets, demonstrating\nsignificant improvements in statistical accuracy and optimization performance\ncompared to existing methods.", "AI": {"tldr": "本文提出一种新颖方法，通过利用约束结构对直流最优潮流（DC-OPF）模型中的不确定性进行降维建模与估计，显著提升了统计精度和优化性能。", "motivation": "机会约束优化是管理电力系统不确定性的有效框架，但现有方法在DC-OPF中对随机节点注入进行高维统计分布建模，效率不高。本研究旨在改进DC-OPF中的不确定性建模和估计。", "method": "提出一种替代方法，利用约束结构指导不确定性估计以实现显著降维。具体地，该方法直接建模一维聚合系统预测误差和二维加权电力传输分布因子线路误差，而非学习净负荷预测误差的联合分布。研究在高斯和非高斯分布下，使用合成和真实世界数据集评估了该方法。", "result": "与现有方法相比，在统计精度和优化性能方面均取得了显著改进。", "conclusion": "所提出的不确定性建模和估计方法通过降维和更有效的误差表征，显著提升了DC-OPF在不确定性管理方面的性能。"}}
{"id": "2508.21309", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.21309", "abs": "https://arxiv.org/abs/2508.21309", "authors": ["Seyed Ali Rakhshan", "Mehdi Golestani", "He Kong"], "title": "Observability-driven Assignment of Heterogeneous Sensors for Multi-Target Tracking", "comment": "This paper has been accepted to the 2025 IEEE/RSJ IROS", "summary": "This paper addresses the challenge of assigning heterogeneous sensors (i.e.,\nrobots with varying sensing capabilities) for multi-target tracking. We\nclassify robots into two categories: (1) sufficient sensing robots, equipped\nwith range and bearing sensors, capable of independently tracking targets, and\n(2) limited sensing robots, which are equipped with only range or bearing\nsensors and need to at least form a pair to collaboratively track a target. Our\nobjective is to optimize tracking quality by minimizing uncertainty in target\nstate estimation through efficient robot-to-target assignment. By leveraging\nmatroid theory, we propose a greedy assignment algorithm that dynamically\nallocates robots to targets to maximize tracking quality. The algorithm\nguarantees constant-factor approximation bounds of 1/3 for arbitrary tracking\nquality functions and 1/2 for submodular functions, while maintaining\npolynomial-time complexity. Extensive simulations demonstrate the algorithm's\neffectiveness in accurately estimating and tracking targets over extended\nperiods. Furthermore, numerical results confirm that the algorithm's\nperformance is close to that of the optimal assignment, highlighting its\nrobustness and practical applicability.", "AI": {"tldr": "本文提出了一种基于拟阵理论的贪婪算法，用于解决异构传感器在多目标跟踪中的分配问题，以优化跟踪质量并最小化不确定性。", "motivation": "在多目标跟踪中，如何有效地将具有不同感知能力的异构传感器（机器人）分配给目标，以最大化跟踪质量并最小化目标状态估计的不确定性，是一个重要的挑战。", "method": "将机器人分为两类：(1) 具有测距和测向能力的独立跟踪机器人；(2) 仅具有测距或测向能力，需要配对协作跟踪的机器人。利用拟阵理论，提出了一种贪婪分配算法，动态分配机器人以最大化跟踪质量。", "result": "该算法在多项式时间内运行，对于任意跟踪质量函数提供1/3的常数因子近似界限，对于子模函数提供1/2的常数因子近似界限。广泛仿真表明算法在长时间内有效估计和跟踪目标，并且性能接近最优分配。", "conclusion": "所提出的贪婪算法在异构传感器多目标跟踪中具有鲁棒性和实用性，能够有效提高跟踪质量并接近最优性能。"}}
{"id": "2508.21102", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.21102", "abs": "https://arxiv.org/abs/2508.21102", "authors": ["Kei Katsumata", "Yui Iioka", "Naoki Hosomi", "Teruhisa Misu", "Kentaro Yamada", "Komei Sugiura"], "title": "GENNAV: Polygon Mask Generation for Generalized Referring Navigable Regions", "comment": "Accepted for presentation at CoRL2025", "summary": "We focus on the task of identifying the location of target regions from a\nnatural language instruction and a front camera image captured by a mobility.\nThis task is challenging because it requires both existence prediction and\nsegmentation, particularly for stuff-type target regions with ambiguous\nboundaries. Existing methods often underperform in handling stuff-type target\nregions, in addition to absent or multiple targets. To overcome these\nlimitations, we propose GENNAV, which predicts target existence and generates\nsegmentation masks for multiple stuff-type target regions. To evaluate GENNAV,\nwe constructed a novel benchmark called GRiN-Drive, which includes three\ndistinct types of samples: no-target, single-target, and multi-target. GENNAV\nachieved superior performance over baseline methods on standard evaluation\nmetrics. Furthermore, we conducted real-world experiments with four automobiles\noperated in five geographically distinct urban areas to validate its zero-shot\ntransfer performance. In these experiments, GENNAV outperformed baseline\nmethods and demonstrated its robustness across diverse real-world environments.\nThe project page is available at https://gennav.vercel.app/.", "AI": {"tldr": "该研究提出了GENNAV，一种用于从自然语言指令和车载摄像头图像中识别目标区域（特别是边界模糊的“stuff-type”区域）的方法。GENNAV能预测目标是否存在并生成多个目标区域的分割掩码。通过构建新的GRiN-Drive基准和进行真实世界实验，GENNAV在性能和零样本迁移能力上均优于现有方法。", "motivation": "从自然语言指令和前置摄像头图像中识别目标区域（包括存在预测和分割）是一项具有挑战性的任务，尤其对于边界模糊的“stuff-type”目标区域。现有方法在处理“stuff-type”目标、目标缺失或多个目标时表现不佳，这促使研究者开发更有效的方法。", "method": "本文提出了GENNAV模型，该模型能够预测目标是否存在，并为多个“stuff-type”目标区域生成分割掩码。为了评估GENNAV，研究者构建了一个名为GRiN-Drive的新基准，其中包含无目标、单目标和多目标三种样本类型。此外，还在五个不同地理区域的城市中，使用四辆汽车进行了真实世界实验，以验证其零样本迁移性能。", "result": "GENNAV在GRiN-Drive基准上，使用标准评估指标，取得了优于基线方法的性能。在真实世界实验中，GENNAV同样超越了基线方法，并展示了其在多样化真实世界环境中的鲁棒性。", "conclusion": "GENNAV有效解决了从自然语言指令和图像中识别“stuff-type”目标区域的挑战，并在合成基准和多样化的真实世界环境中均表现出卓越的性能和强大的零样本迁移能力，证明了其在实际应用中的鲁棒性。"}}
{"id": "2508.21411", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.21411", "abs": "https://arxiv.org/abs/2508.21411", "authors": ["Leonard Frank Neis", "Andre Antakli", "Matthias Klusch"], "title": "CARJAN: Agent-Based Generation and Simulation of Traffic Scenarios with AJAN", "comment": null, "summary": "User-friendly modeling and virtual simulation of urban traffic scenarios with\ndifferent types of interacting agents such as pedestrians, cyclists and\nautonomous vehicles remains a challenge. We present CARJAN, a novel tool for\nsemi-automated generation and simulation of such scenarios based on the\nmulti-agent engineering framework AJAN and the driving simulator CARLA. CARJAN\nprovides a visual user interface for the modeling, storage and maintenance of\ntraffic scenario layouts, and leverages SPARQL Behavior Tree-based\ndecision-making and interactions for agents in dynamic scenario simulations in\nCARLA. CARJAN provides a first integrated approach for interactive, intelligent\nagent-based generation and simulation of virtual traffic scenarios in CARLA.", "AI": {"tldr": "CARJAN是一种新工具，用于在CARLA中半自动化生成和模拟包含行人、骑行者和自动驾驶汽车等多种交互代理的城市交通场景，通过可视化用户界面和基于SPARQL行为树的决策实现。", "motivation": "对包含行人、骑行者和自动驾驶汽车等不同类型交互代理的城市交通场景进行用户友好的建模和虚拟仿真仍然是一个挑战。", "method": "该研究提出了CARJAN工具，它基于多代理工程框架AJAN和驾驶模拟器CARLA。CARJAN提供了一个用于交通场景布局建模、存储和维护的可视化用户界面，并利用基于SPARQL行为树的决策和交互来实现CARLA中的动态场景仿真。", "result": "CARJAN提供了一个集成的半自动化工具，用于在CARLA中生成和模拟包含智能代理的虚拟交通场景，支持交互式建模和动态仿真。", "conclusion": "CARJAN首次为CARLA中交互式、基于智能代理的虚拟交通场景生成和仿真提供了一种集成方法。"}}
{"id": "2508.21210", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.21210", "abs": "https://arxiv.org/abs/2508.21210", "authors": ["Yurie Koga", "Shunsuke Kando", "Yusuke Miyao"], "title": "Do Self-Supervised Speech Models Exhibit the Critical Period Effects in Language Acquisition?", "comment": "Accepted to ASRU 2025", "summary": "This paper investigates whether the Critical Period (CP) effects in human\nlanguage acquisition are observed in self-supervised speech models (S3Ms). CP\neffects refer to greater difficulty in acquiring a second language (L2) with\ndelayed L2 exposure onset, and greater retention of their first language (L1)\nwith delayed L1 exposure offset. While previous work has studied these effects\nusing textual language models, their presence in speech models remains\nunderexplored despite the central role of spoken language in human language\nacquisition. We train S3Ms with varying L2 training onsets and L1 training\noffsets on child-directed speech and evaluate their phone discrimination\nperformance. We find that S3Ms do not exhibit clear evidence of either CP\neffects in terms of phonological acquisition. Notably, models with delayed L2\nexposure onset tend to perform better on L2 and delayed L1 exposure offset\nleads to L1 forgetting.", "AI": {"tldr": "本研究发现，自监督语音模型（S3Ms）在语音习得方面不表现出人类语言习得中的“关键期效应”，延迟的二语（L2）暴露反而导致更好的L2表现，而延迟的一语（L1）暴露结束则导致L1遗忘。", "motivation": "人类语言习得中的“关键期效应”（CP effects）在二语习得中表现为延迟暴露导致习得困难，在一语习得中表现为延迟暴露结束导致更好地保留一语。虽然已有研究在文本语言模型中探讨了这些效应，但在语音模型中，尽管口语在人类语言习得中起着核心作用，但这一领域仍未得到充分探索。", "method": "研究人员训练了自监督语音模型（S3Ms），通过改变二语（L2）训练的开始时间（L2 exposure onset）和一语（L1）训练的结束时间（L1 exposure offset），并在儿童导向的语音数据上进行训练。随后，他们评估了这些模型在音素判别任务上的表现。", "result": "研究发现S3Ms没有表现出明显的、与人类相似的语音习得中的关键期效应。具体而言，延迟二语暴露开始的模型在二语表现上反而更好，而延迟一语暴露结束则导致了一语的遗忘。", "conclusion": "自监督语音模型在语音习得方面不展现出与人类语言习得相似的关键期效应。与人类观察到的现象相反，这些模型在延迟二语暴露时表现更佳，且延迟的一语暴露结束可能导致一语遗忘。"}}
{"id": "2508.21760", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.21760", "abs": "https://arxiv.org/abs/2508.21760", "authors": ["Catalin Arghir", "Pieder Jörg", "Silvia Mastellone"], "title": "Transferring the driveshaft inertia to the grid via the DC-link in MV drive systems", "comment": "Submitted for review to IEEE Transactions on Control Systems\n  Technology, complete version, 21 pages", "summary": "This paper investigates a control approach that renders the driveshaft\ninertia completely available on the grid side and enhances the fault\nride-through behavior of medium-voltage (MV) drive systems. Two main\ncontributions are presented. First, we show how the rotational inertia of the\ndriveline shaft can be synchronously coupled to the grid through a modification\nof the speed control reference signal and through an adapted DC-link control\nstrategy. For the latter, we pursue two alternatives: one based on conventional\ncascaded control and another based on synchronous machine (SM) model matching.\nSecond, we demonstrate that both the standard phase-locked loop (PLL) and the\nmatching control approach can be interpreted, via the ray-circle\ncomplementarity, as feedback optimization schemes with distinct steady-state\nmaps. This perspective allows us to revisit matching control, reveal its\nembedded PLL, highlight its current-limiting and tracking capabilities, and\nprovide an extensive simulation study.", "AI": {"tldr": "本文提出了一种控制方法，旨在使中压传动系统中的传动轴惯量完全可用于电网侧，并增强系统的故障穿越能力。", "motivation": "研究动机是提高中压传动系统的故障穿越性能，并使传动轴的惯量能够有效地反馈到电网侧，以增强系统稳定性。", "method": "该方法主要包括两方面：一是通过修改速度控制参考信号和调整直流母线控制策略（包括传统的级联控制和基于同步电机模型匹配的两种方案），将传动轴的旋转惯量同步耦合到电网；二是通过“射线-圆互补性”将标准锁相环（PLL）和模型匹配控制解释为具有不同稳态映射的反馈优化方案，从而揭示模型匹配控制中嵌入的PLL及其限流和跟踪能力。", "result": "研究结果表明，通过所提出的控制方法，传动轴的旋转惯量可以同步耦合到电网。同时，模型匹配控制方法被证明内嵌了锁相环功能，并展示了其电流限制和跟踪能力。通过广泛的仿真研究验证了这些发现。", "conclusion": "该控制方法成功地将传动轴惯量应用于电网侧，显著提升了中压传动系统的故障穿越性能，并为锁相环和模型匹配控制提供了新的解释和理解，尤其突出了模型匹配控制的实用优势。"}}
{"id": "2508.21322", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.21322", "abs": "https://arxiv.org/abs/2508.21322", "authors": ["Haojie Bai", "Yang Wang", "Cong Guo", "Xiongwei Zhao", "Hai Zhu"], "title": "Robust Real-Time Coordination of CAVs: A Distributed Optimization Framework under Uncertainty", "comment": null, "summary": "Achieving both safety guarantees and real-time performance in cooperative\nvehicle coordination remains a fundamental challenge, particularly in dynamic\nand uncertain environments. This paper presents a novel coordination framework\nthat resolves this challenge through three key innovations: 1) direct control\nof vehicles' trajectory distributions during coordination, formulated as a\nrobust cooperative planning problem with adaptive enhanced safety constraints,\nensuring a specified level of safety regarding the uncertainty of the\ninteractive trajectory, 2) a fully parallel ADMM-based distributed trajectory\nnegotiation (ADMM-DTN) algorithm that efficiently solves the optimization\nproblem while allowing configurable negotiation rounds to balance solution\nquality and computational resources, and 3) an interactive attention mechanism\nthat selectively focuses on critical interactive participants to further\nenhance computational efficiency. Both simulation results and practical\nexperiments demonstrate that our framework achieves significant advantages in\nsafety (reducing collision rates by up to 40.79\\% in various scenarios) and\nreal-time performance compared to state-of-the-art methods, while maintaining\nstrong scalability with increasing vehicle numbers. The proposed interactive\nattention mechanism further reduces the computational demand by 14.1\\%. The\nframework's effectiveness is further validated through real-world experiments\nwith unexpected dynamic obstacles, demonstrating robust coordination in complex\nenvironments. The experiment demo could be found at\nhttps://youtu.be/4PZwBnCsb6Q.", "AI": {"tldr": "本文提出了一种新颖的协同车辆协调框架，通过直接控制轨迹分布、分布式轨迹协商算法和交互式注意力机制，在动态不确定环境中同时实现了安全保障和实时性能。", "motivation": "在动态和不确定环境中，实现合作车辆协调中的安全保障和实时性能仍然是一个基本挑战。", "method": "1) 将车辆轨迹分布的直接控制制定为鲁棒的合作规划问题，并采用自适应增强安全约束。2) 提出一种完全并行的基于ADMM的分布式轨迹协商(ADMM-DTN)算法，以平衡解质量和计算资源。3) 引入交互式注意力机制，选择性地关注关键交互参与者以提高计算效率。", "result": "该框架在安全性（在各种场景中碰撞率降低高达40.79%）和实时性能方面优于现有技术，并随着车辆数量的增加保持强大的可扩展性。交互式注意力机制进一步将计算需求降低了14.1%。通过真实世界实验验证了其在复杂环境（包括意外动态障碍物）中的鲁棒协调能力。", "conclusion": "所提出的框架在模拟和实际实验中均表现出在复杂动态环境中实现高安全性、实时性能和强大可扩展性的有效性，并通过交互式注意力机制进一步提升了计算效率和鲁棒性。"}}
{"id": "2508.21113", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.21113", "abs": "https://arxiv.org/abs/2508.21113", "authors": ["Jie Jiang", "Qi Yang", "Bolin Ni", "Shiming Xiang", "Han Hu", "Houwen Peng"], "title": "R-4B: Incentivizing General-Purpose Auto-Thinking Capability in MLLMs via Bi-Mode Annealing and Reinforce Learning", "comment": "20 pages, 14 figures, 5 tables", "summary": "Multimodal Large Language Models (MLLMs) equipped with step-by-step thinking\ncapabilities have demonstrated remarkable performance on complex reasoning\nproblems. However, this thinking process is redundant for simple problems\nsolvable without complex reasoning. To address this inefficiency, we propose\nR-4B, an auto-thinking MLLM, which can adaptively decide when to think based on\nproblem complexity. The central idea of R-4B is to empower the model with both\nthinking and non-thinking capabilities using bi-mode annealing, and apply\nBi-mode Policy Optimization~(BPO) to improve the model's accuracy in\ndetermining whether to activate the thinking process. Specifically, we first\ntrain the model on a carefully curated dataset spanning various topics, which\ncontains samples from both thinking and non-thinking modes. Then it undergoes a\nsecond phase of training under an improved GRPO framework, where the policy\nmodel is forced to generate responses from both modes for each input query.\nExperimental results show that R-4B achieves state-of-the-art performance\nacross 25 challenging benchmarks. It outperforms Qwen2.5-VL-7B in most tasks\nand achieves performance comparable to larger models such as\nKimi-VL-A3B-Thinking-2506 (16B) on reasoning-intensive benchmarks with lower\ncomputational cost.", "AI": {"tldr": "R-4B是一种自适应思考的多模态大语言模型（MLLM），它能根据问题复杂性智能决定是否激活分步思考过程，从而提高效率并保持高性能。", "motivation": "现有MLLM的分步思考能力虽然在复杂推理问题上表现出色，但对于简单问题而言，这种思考过程是冗余且低效的。", "method": "R-4B通过双模态退火（bi-mode annealing）赋予模型思考和非思考两种能力，并采用双模态策略优化（Bi-mode Policy Optimization, BPO）来提高模型决定是否激活思考过程的准确性。具体方法包括：首先在一个包含思考和非思考模式样本的精心策划的数据集上进行训练；然后，在改进的GRPO框架下进行第二阶段训练，强制策略模型为每个输入查询生成两种模式的响应。", "result": "R-4B在25个挑战性基准测试中取得了最先进的性能。它在大多数任务中超越了Qwen2.5-VL-7B，并在推理密集型基准测试上实现了与Kimi-VL-A3B-Thinking-2506（16B）等大型模型相当的性能，同时计算成本更低。", "conclusion": "R-4B成功地解决了MLLM在处理不同复杂性问题时的效率问题，通过自适应地选择思考模式，在保持高推理能力的同时显著降低了计算冗余，展现了高效且强大的性能。"}}
{"id": "2508.21441", "categories": ["cs.AI", "68T30, 68T27"], "pdf": "https://arxiv.org/pdf/2508.21441", "abs": "https://arxiv.org/abs/2508.21441", "authors": ["Christoph Beierle", "Alexander Hahn", "Diana Howey", "Gabriele Kern-Isberner", "Kai Sauerwald"], "title": "A General Framework of Epistemic Forgetting and its Instantiation by Ranking Functions", "comment": null, "summary": "Forgetting as a knowledge management operation deliberately ignores parts of\nthe knowledge and beliefs of an agent, for various reasons. Forgetting has many\nfacets, one may want to forget parts of the syntax, a proposition, or a\nconditional. In the literature, two main operators suitable for performing\nforgetting have been proposed and investigated in depth: First, variable\nelimination is a syntactical method that blends out certain atomic variables to\nfocus on the rest of the language. It has been mainly used in the area of logic\nprogramming and answer set programming. Second, contraction in AGM belief\nrevision theory effectively removes propositions from belief sets under logical\ndeduction. Both operations rely mainly on classical logics. In this article, we\ntake an epistemic perspective and study forgetting operations in epistemic\nstates with richer semantic structures, but with clear links to propositional\nlogic. This allows us to investigate what forgetting in the epistemic\nbackground means, thereby lifting well-known and novel forgetting operations to\nthe epistemic level. We present five general types of epistemic forgetting and\ninstantiate them with seven concrete forgetting operations for Spohn's ranking\nfunctions. We take inspiration from postulates of forgetting both from logic\nprogramming and AGM theory to propose a rich landscape of axioms for evaluating\nforgetting operations. Finally, we evaluate all concrete forgetting operations\naccording to all postulates, leading to a novel comprehensive overview\nhighlighting differences and commonalities among the forgetting operators.", "AI": {"tldr": "本文将遗忘操作从经典逻辑提升到认知层面，提出了五种认知遗忘类型和七种基于Spohn排序函数的具体操作，并根据源自逻辑编程和AGM理论的公理对它们进行了评估。", "motivation": "现有的遗忘操作（如变量消除和收缩）主要基于经典逻辑。研究动机在于探索在具有更丰富语义结构（但与命题逻辑有明确联系）的认知状态中，遗忘意味着什么，从而将遗忘操作提升到认知层面。", "method": "研究方法包括：1) 采用认知视角，在认知状态中研究遗忘操作。2) 提出五种通用类型的认知遗忘。3) 基于Spohn排序函数实例化了七种具体的遗忘操作。4) 借鉴逻辑编程和AGM理论中的遗忘公理，提出了一套丰富的公理体系用于评估遗忘操作。5) 根据这些公理对所有具体的遗忘操作进行评估。", "result": "本文提出了五种通用类型的认知遗忘和七种针对Spohn排序函数的具体遗忘操作。通过对这些具体操作进行全面评估，揭示了不同遗忘操作之间的差异和共性，形成了一个新颖且全面的概览。", "conclusion": "研究成功地将遗忘操作提升到认知层面，为评估认知遗忘操作提供了一个丰富的公理框架，并通过对具体操作的全面评估，深入理解了它们在认知背景下的行为和属性。"}}
{"id": "2508.21228", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.21228", "abs": "https://arxiv.org/abs/2508.21228", "authors": ["Weizhi Gao", "Xiaorui Liu", "Feiyi Wang", "Dan Lu", "Junqi Yin"], "title": "Decoding Memories: An Efficient Pipeline for Self-Consistency Hallucination Detection", "comment": "14 pages, under review", "summary": "Large language models (LLMs) have demonstrated impressive performance in both\nresearch and real-world applications, but they still struggle with\nhallucination. Existing hallucination detection methods often perform poorly on\nsentence-level generation or rely heavily on domain-specific knowledge. While\nself-consistency approaches help address these limitations, they incur high\ncomputational costs due to repeated generation. In this paper, we conduct the\nfirst study on identifying redundancy in self-consistency methods, manifested\nas shared prefix tokens across generations, and observe that non-exact-answer\ntokens contribute minimally to the semantic content. Based on these insights,\nwe propose a novel Decoding Memory Pipeline (DMP) that accelerates generation\nthrough selective inference and annealed decoding. Being orthogonal to the\nmodel, dataset, decoding strategy, and self-consistency baseline, our DMP\nconsistently improves the efficiency of multi-response generation and holds\npromise for extension to alignment and reasoning tasks. Extensive experiments\nshow that our method achieves up to a 3x speedup without sacrificing AUROC\nperformance.", "AI": {"tldr": "本研究首次探讨了自洽性方法中的冗余（表现为生成中共享的前缀标记），并提出了一种名为“解码记忆管道”（DMP）的新型方法，通过选择性推理和退火解码来加速多响应生成，在不牺牲性能的情况下实现了高达3倍的速度提升。", "motivation": "大型语言模型（LLMs）存在幻觉问题。现有的幻觉检测方法在句子级别生成上表现不佳或高度依赖领域知识。自洽性方法虽能解决这些局限性，但由于重复生成而计算成本高昂。", "method": "本研究首先对自洽性方法中的冗余（即生成中共享的前缀标记）进行了首次研究，并观察到非精确答案标记对语义内容的贡献极小。基于这些发现，论文提出了一种新颖的“解码记忆管道”（DMP），通过选择性推理和退火解码来加速生成。DMP独立于模型、数据集、解码策略和自洽性基线。", "result": "DMP持续提高了多响应生成的效率，实现了高达3倍的速度提升，同时没有牺牲AUROC性能。该方法还有望扩展到对齐和推理任务。", "conclusion": "通过识别自洽性方法中的冗余并利用选择性推理和退火解码，所提出的解码记忆管道（DMP）显著提高了多响应生成的效率，在幻觉检测等任务中实现了显著加速，且不影响性能，并具有广泛的应用潜力。"}}
{"id": "2508.21797", "categories": ["eess.SY", "cs.AI", "cs.CR", "cs.LG", "cs.SY", "stat.AP"], "pdf": "https://arxiv.org/pdf/2508.21797", "abs": "https://arxiv.org/abs/2508.21797", "authors": ["Navid Aftabi", "Abhishek Hanchate", "Satish Bukkapatnam", "Dan Li"], "title": "DynaMark: A Reinforcement Learning Framework for Dynamic Watermarking in Industrial Machine Tool Controllers", "comment": null, "summary": "Industry 4.0's highly networked Machine Tool Controllers (MTCs) are prime\ntargets for replay attacks that use outdated sensor data to manipulate\nactuators. Dynamic watermarking can reveal such tampering, but current schemes\nassume linear-Gaussian dynamics and use constant watermark statistics, making\nthem vulnerable to the time-varying, partly proprietary behavior of MTCs. We\nclose this gap with DynaMark, a reinforcement learning framework that models\ndynamic watermarking as a Markov decision process (MDP). It learns an adaptive\npolicy online that dynamically adapts the covariance of a zero-mean Gaussian\nwatermark using available measurements and detector feedback, without needing\nsystem knowledge. DynaMark maximizes a unique reward function balancing control\nperformance, energy consumption, and detection confidence dynamically. We\ndevelop a Bayesian belief updating mechanism for real-time detection confidence\nin linear systems. This approach, independent of specific system assumptions,\nunderpins the MDP for systems with linear dynamics. On a Siemens Sinumerik 828D\ncontroller digital twin, DynaMark achieves a reduction in watermark energy by\n70% while preserving the nominal trajectory, compared to constant variance\nbaselines. It also maintains an average detection delay equivalent to one\nsampling interval. A physical stepper-motor testbed validates these findings,\nrapidly triggering alarms with less control performance decline and exceeding\nexisting benchmarks.", "AI": {"tldr": "DynaMark是一个基于强化学习的自适应动态水印框架，用于保护工业4.0机床控制器免受重放攻击，它能在线优化水印参数以平衡控制性能、能耗和检测置信度，并在数字孪生和物理测试台上展现出卓越的性能提升。", "motivation": "工业4.0中高度网络化的机床控制器（MTCs）容易遭受利用过时传感器数据操纵执行器的重放攻击。现有动态水印方案假设线性高斯动力学并使用恒定水印统计量，这使其无法应对MTCs时变、部分专有的行为。", "method": "本研究提出了DynaMark，一个将动态水印建模为马尔可夫决策过程（MDP）的强化学习框架。它在线学习自适应策略，根据可用测量和检测器反馈动态调整零均值高斯水印的协方差，无需系统知识。DynaMark最大化一个独特的奖励函数，以动态平衡控制性能、能耗和检测置信度。此外，还为线性系统开发了一种用于实时检测置信度的贝叶斯信念更新机制。", "result": "在西门子Sinumerik 828D控制器数字孪生上，与恒定方差基线相比，DynaMark在保持标称轨迹的同时，将水印能耗降低了70%，并保持了相当于一个采样间隔的平均检测延迟。物理步进电机测试台验证了这些发现，实现了更快的警报触发，同时控制性能下降更少，并超越了现有基准。", "conclusion": "DynaMark提供了一种无需特定系统假设的自适应动态水印解决方案，能够有效检测工业4.0机床控制器中的重放攻击，显著降低水印能耗，同时保持或改进控制性能和检测速度，优于现有方法。"}}
{"id": "2508.21364", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.21364", "abs": "https://arxiv.org/abs/2508.21364", "authors": ["Alberto Bertipaglia", "Dariu M. Gavrila", "Barys Shyrokau"], "title": "Multi-Modal Model Predictive Path Integral Control for Collision Avoidance", "comment": "Accepted as an oral presentation at the 29th IAVSD. August 18-22,\n  2025. Shanghai, China", "summary": "This paper proposes a novel approach to motion planning and decision-making\nfor automated vehicles, using a multi-modal Model Predictive Path Integral\ncontrol algorithm. The method samples with Sobol sequences around the prior\ninput and incorporates analytical solutions for collision avoidance. By\nleveraging multiple modes, the multi-modal control algorithm explores diverse\ntrajectories, such as manoeuvring around obstacles or stopping safely before\nthem, mitigating the risk of sub-optimal solutions. A non-linear single-track\nvehicle model with a Fiala tyre serves as the prediction model, and tyre force\nconstraints within the friction circle are enforced to ensure vehicle stability\nduring evasive manoeuvres. The optimised steering angle and longitudinal\nacceleration are computed to generate a collision-free trajectory and to\ncontrol the vehicle. In a high-fidelity simulation environment, we demonstrate\nthat the proposed algorithm can successfully avoid obstacles, keeping the\nvehicle stable while driving a double lane change manoeuvre on high and\nlow-friction road surfaces and occlusion scenarios with moving obstacles,\noutperforming a standard Model Predictive Path Integral approach.", "AI": {"tldr": "本文提出了一种多模态模型预测路径积分（MPPI）控制算法，用于自动驾驶汽车的运动规划和决策。该方法通过Sobol序列采样和解析碰撞避免，探索多样化轨迹，并在高保真模拟中表现出优于标准MPPI的障碍物规避和车辆稳定性。", "motivation": "现有方法可能导致自动驾驶汽车在运动规划和决策中产生次优解决方案，尤其是在需要规避障碍物或安全停车的复杂场景中，因此需要一种能探索多样化轨迹并降低风险的新方法。", "method": "该方法提出了一种多模态模型预测路径积分（MPPI）控制算法。它使用Sobol序列围绕先验输入进行采样，并结合解析解进行碰撞避免。通过利用多模态，算法可以探索多种轨迹（如绕过障碍物或安全停车）。预测模型采用带有Fiala轮胎的非线性单轨车辆模型，并强制执行摩擦圆内的轮胎力约束以确保车辆稳定性。最终计算优化的转向角和纵向加速度以生成无碰撞轨迹。", "result": "在高保真模拟环境中，所提出的算法成功地规避了障碍物，并在高摩擦和低摩擦路面上的双车道变换操作以及移动障碍物的遮挡场景中保持车辆稳定。该算法表现优于标准的模型预测路径积分方法。", "conclusion": "所提出的多模态模型预测路径积分控制算法能够有效地为自动驾驶汽车进行运动规划和决策，在复杂障碍物规避和保持车辆稳定方面表现出色，并优于传统MPPI方法。"}}
{"id": "2508.21135", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.21135", "abs": "https://arxiv.org/abs/2508.21135", "authors": ["Harris Song", "Tuan-Anh Vu", "Sanjith Menon", "Sriram Narasimhan", "M. Khalid Jawed"], "title": "HiddenObject: Modality-Agnostic Fusion for Multimodal Hidden Object Detection", "comment": null, "summary": "Detecting hidden or partially concealed objects remains a fundamental\nchallenge in multimodal environments, where factors like occlusion, camouflage,\nand lighting variations significantly hinder performance. Traditional RGB-based\ndetection methods often fail under such adverse conditions, motivating the need\nfor more robust, modality-agnostic approaches. In this work, we present\nHiddenObject, a fusion framework that integrates RGB, thermal, and depth data\nusing a Mamba-based fusion mechanism. Our method captures complementary signals\nacross modalities, enabling enhanced detection of obscured or camouflaged\ntargets. Specifically, the proposed approach identifies modality-specific\nfeatures and fuses them in a unified representation that generalizes well\nacross challenging scenarios. We validate HiddenObject across multiple\nbenchmark datasets, demonstrating state-of-the-art or competitive performance\ncompared to existing methods. These results highlight the efficacy of our\nfusion design and expose key limitations in current unimodal and na\\\"ive fusion\nstrategies. More broadly, our findings suggest that Mamba-based fusion\narchitectures can significantly advance the field of multimodal object\ndetection, especially under visually degraded or complex conditions.", "AI": {"tldr": "本研究提出了HiddenObject，一个基于Mamba的融合框架，整合RGB、热成像和深度数据，以增强在遮挡、伪装和光照变化等挑战性条件下对隐藏或部分遮蔽物体的检测能力，并取得了最先进或具有竞争力的性能。", "motivation": "在多模态环境中，由于遮挡、伪装和光照变化等因素，检测隐藏或部分遮蔽的物体仍然是一个基本挑战。传统的基于RGB的检测方法在这些不利条件下常常失效，因此需要更鲁棒、与模态无关的方法。", "method": "本研究提出了HiddenObject，一个融合框架，它使用基于Mamba的融合机制整合了RGB、热成像和深度数据。该方法捕获跨模态的互补信号，识别模态特定特征，并将其融合为一个统一的表示，以实现对模糊或伪装目标的增强检测。", "result": "HiddenObject在多个基准数据集上进行了验证，与现有方法相比，展示了最先进或具有竞争力的性能。这些结果突出了其融合设计的有效性，并揭示了当前单模态和朴素融合策略的关键局限性。", "conclusion": "研究结果表明，基于Mamba的融合架构可以显著推动多模态目标检测领域的发展，特别是在视觉退化或复杂条件下。"}}
{"id": "2508.21449", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.21449", "abs": "https://arxiv.org/abs/2508.21449", "authors": ["Niklas Jansen", "Jonas Gösgens", "Hector Geffner"], "title": "Learning Lifted Action Models From Traces of Incomplete Actions and States", "comment": "To be presented at KR 2025", "summary": "Consider the problem of learning a lifted STRIPS model of the sliding-tile\npuzzle from random state-action traces where the states represent the location\nof the tiles only, and the actions are the labels up, down, left, and right,\nwith no arguments. Two challenges are involved in this problem. First, the\nstates are not full STRIPS states, as some predicates are missing, like the\natoms representing the position of the ``blank''. Second, the actions are not\nfull STRIPS either, as they do not reveal all the objects involved in the\nactions effects and preconditions. Previous approaches have addressed different\nversions of this model learning problem, but most assume that actions in the\ntraces are full STRIPS actions or that the domain predicates are all\nobservable. The new setting considered in this work is more ``realistic'', as\nthe atoms observed convey the state of the world but not full STRIPS states,\nand the actions reveal the arguments needed for selecting the action but not\nthe ones needed for modeling it in STRIPS. For formulating and addressing the\nlearning problem, we introduce a variant of STRIPS, which we call STRIPS+,\nwhere certain STRIPS action arguments can be left implicit in preconditions\nwhich can also involve a limited form of existential quantification. The\nlearning problem becomes the problem of learning STRIPS+ models from STRIPS+\nstate-action traces. For this, the proposed learning algorithm, called SYNTH,\nconstructs a stratified sequence (conjunction) of precondition expressions or\n``queries'' for each action, that denote unique objects in the state and ground\nthe implicit action arguments in STRIPS+. The correctness and completeness of\nSYNTH is established, and its scalability is tested on state-action traces\nobtained from STRIPS+ models derived from existing STRIPS domains.", "AI": {"tldr": "本文提出了一种学习滑动拼图游戏提升STRIPS模型的新方法，该模型从不完整的状态-动作轨迹中学习，引入了STRIPS+变体和名为SYNTH的学习算法。", "motivation": "传统的模型学习方法通常假设动作是完整的STRIPS动作或所有领域谓词都是可观察的。本文旨在解决一个更“现实”的问题：状态不包含所有谓词（如“空白”位置），动作也不揭示所有必需的对象参数，这在滑动拼图等问题中尤为明显。", "method": "本文引入了STRIPS+，这是STRIPS的一种变体，其中某些动作参数可以在前置条件中隐式存在，并且前置条件可以包含有限形式的量词。学习问题被重新定义为从STRIPS+状态-动作轨迹中学习STRIPS+模型。提出的学习算法SYNTH为每个动作构建分层的（查询）前置条件表达式序列，以识别状态中的唯一对象并实例化STRIPS+中隐式的动作参数。", "result": "SYNTH算法的正确性和完备性得到了证实。该算法在从现有STRIPS领域导出的STRIPS+模型生成的状态-动作轨迹上进行了可伸缩性测试。", "conclusion": "本文成功地提出了STRIPS+框架和SYNTH算法，以应对从更现实、不完整状态-动作轨迹中学习规划模型的挑战。该方法通过理论证明和实验测试，展示了其在处理隐式信息时的有效性。"}}
{"id": "2508.21290", "categories": ["cs.CL", "cs.AI", "cs.IR", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.21290", "abs": "https://arxiv.org/abs/2508.21290", "authors": ["Daria Kryvosheieva", "Saba Sturua", "Michael Günther", "Scott Martens", "Han Xiao"], "title": "Efficient Code Embeddings from Code Generation Models", "comment": "9 pages, table and evaluations 5-9", "summary": "jina-code-embeddings is a novel code embedding model suite designed to\nretrieve code from natural language queries, perform technical\nquestion-answering, and identify semantically similar code snippets across\nprogramming languages. It makes innovative use of an autoregressive backbone\npre-trained on both text and code, generating embeddings via last-token\npooling. We outline the training recipe and demonstrate state-of-the-art\nperformance despite the relatively small size of the models, validating this\napproach to code embedding model construction.", "AI": {"tldr": "Jina-code-embeddings 是一个新颖的代码嵌入模型套件，利用预训练的自回归骨干网络和末尾标记池化技术，在代码检索、问答和跨语言相似性识别任务上实现了最先进的性能。", "motivation": "研究动机是开发一个能够从自然语言查询中检索代码、执行技术问答以及识别跨编程语言语义相似代码片段的模型。", "method": "该模型创新性地使用了在文本和代码上预训练的自回归骨干网络，并通过末尾标记池化（last-token pooling）生成嵌入。论文概述了其训练方法。", "result": "尽管模型规模相对较小，但Jina-code-embeddings 在各项任务上展现了最先进（state-of-the-art）的性能。", "conclusion": "该研究验证了这种代码嵌入模型构建方法是有效且成功的，能够以较小的模型实现卓越的性能。"}}
{"id": "2508.21635", "categories": ["cs.RO", "cs.CV", "cs.SY", "eess.SY", "I.2.9"], "pdf": "https://arxiv.org/pdf/2508.21635", "abs": "https://arxiv.org/abs/2508.21635", "authors": ["Nicolas Soncini", "Javier Cremona", "Erica Vidal", "Maximiliano García", "Gastón Castro", "Taihú Pire"], "title": "The Rosario Dataset v2: Multimodal Dataset for Agricultural Robotics", "comment": "First published on The International Journal of Robotics Research:\n  https://journals.sagepub.com/doi/10.1177/02783649251368909", "summary": "We present a multi-modal dataset collected in a soybean crop field,\ncomprising over two hours of recorded data from sensors such as stereo infrared\ncamera, color camera, accelerometer, gyroscope, magnetometer, GNSS (Single\nPoint Positioning, Real-Time Kinematic and Post-Processed Kinematic), and wheel\nodometry. This dataset captures key challenges inherent to robotics in\nagricultural environments, including variations in natural lighting, motion\nblur, rough terrain, and long, perceptually aliased sequences. By addressing\nthese complexities, the dataset aims to support the development and\nbenchmarking of advanced algorithms for localization, mapping, perception, and\nnavigation in agricultural robotics. The platform and data collection system is\ndesigned to meet the key requirements for evaluating multi-modal SLAM systems,\nincluding hardware synchronization of sensors, 6-DOF ground truth and loops on\nlong trajectories.\n  We run multimodal state-of-the art SLAM methods on the dataset, showcasing\nthe existing limitations in their application on agricultural settings. The\ndataset and utilities to work with it are released on\nhttps://cifasis.github.io/rosariov2/.", "AI": {"tldr": "本文介绍了一个在大豆田中收集的多模态数据集，包含立体红外相机、彩色相机、IMU、GNSS和轮式里程计等传感器数据，旨在支持农业机器人定位、建图、感知和导航算法的开发与基准测试，并揭示了现有SLAM方法在农业环境中的局限性。", "motivation": "农业机器人面临自然光照变化、运动模糊、崎岖地形和感知混叠等挑战，现有算法在此类环境中表现受限。因此，需要一个专门的数据集来支持和基准测试先进的定位、建图、感知和导航算法，以克服这些复杂性。", "method": "研究人员在大豆田中收集了一个超过两小时的多模态数据集，包含来自立体红外相机、彩色相机、加速度计、陀螺仪、磁力计、GNSS（单点定位、RTK和PPK）以及轮式里程计的数据。数据采集平台设计满足评估多模态SLAM系统的关键要求，包括传感器硬件同步、6-DOF真实值和长轨迹上的闭环。此外，研究人员还在该数据集上运行了多模态最先进的SLAM方法。", "result": "研究成果是一个公开的多模态数据集（RosarioV2），捕获了农业机器人环境中的关键挑战。通过在该数据集上运行最先进的SLAM方法，揭示了这些方法在农业环境应用中存在的局限性。", "conclusion": "该多模态数据集为农业机器人领域的定位、建图、感知和导航算法的开发和基准测试提供了宝贵的资源，并突出了当前最先进SLAM方法在复杂农业环境中的不足之处，为未来的研究指明了方向。"}}
{"id": "2508.21375", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.21375", "abs": "https://arxiv.org/abs/2508.21375", "authors": ["Anuj Pasricha", "Joewie Koh", "Jay Vakil", "Alessandro Roncone"], "title": "Dynamics-Compliant Trajectory Diffusion for Super-Nominal Payload Manipulation", "comment": "Accepted to 2025 Conference on Robot Learning [CoRL]", "summary": "Nominal payload ratings for articulated robots are typically derived from\nworst-case configurations, resulting in uniform payload constraints across the\nentire workspace. This conservative approach severely underutilizes the robot's\ninherent capabilities -- our analysis demonstrates that manipulators can safely\nhandle payloads well above nominal capacity across broad regions of their\nworkspace while staying within joint angle, velocity, acceleration, and torque\nlimits. To address this gap between assumed and actual capability, we propose a\nnovel trajectory generation approach using denoising diffusion models that\nexplicitly incorporates payload constraints into the planning process. Unlike\ntraditional sampling-based methods that rely on inefficient trial-and-error,\noptimization-based methods that are prohibitively slow, or kinodynamic planners\nthat struggle with problem dimensionality, our approach generates dynamically\nfeasible joint-space trajectories in constant time that can be directly\nexecuted on physical hardware without post-processing. Experimental validation\non a 7 DoF Franka Emika Panda robot demonstrates that up to 67.6% of the\nworkspace remains accessible even with payloads exceeding 3 times the nominal\ncapacity. This expanded operational envelope highlights the importance of a\nmore nuanced consideration of payload dynamics in motion planning algorithms.", "AI": {"tldr": "本研究提出了一种基于去噪扩散模型的轨迹生成方法，使机器人能够在保持安全的前提下，携带远超标称容量的有效载荷，显著扩展了工作空间。", "motivation": "传统的机器人标称有效载荷评级过于保守，导致机器人能力被严重低估和利用不足。现有规划方法（如基于采样的、基于优化的或运动学动力学规划器）效率低下或难以处理高维度问题，无法有效利用机器人在实际动态限制下的更高承载能力。", "method": "提出了一种新颖的基于去噪扩散模型的轨迹生成方法。该方法将有效载荷约束明确整合到规划过程中，能够在恒定时间内生成动态可行的关节空间轨迹，无需后处理即可直接在物理硬件上执行。", "result": "在7自由度Franka Emika Panda机器人上的实验验证表明，即使有效载荷超过标称容量的3倍，仍有高达67.6%的工作空间可访问。这显著扩展了机器人的操作范围。", "conclusion": "在运动规划算法中更细致地考虑有效载荷动力学至关重要，因为这能够显著扩大机器人的操作包络，充分发挥其潜在能力。"}}
{"id": "2508.21154", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.21154", "abs": "https://arxiv.org/abs/2508.21154", "authors": ["Ao Shen", "Xueming Fu", "Junfeng Jiang", "Qiang Zeng", "Ye Tang", "Zhengming Chen", "Luming Nong", "Feng Wang", "S. Kevin Zhou"], "title": "RadGS-Reg: Registering Spine CT with Biplanar X-rays via Joint 3D Radiative Gaussians Reconstruction and 3D/3D Registration", "comment": "11 pages, 2 figures", "summary": "Computed Tomography (CT)/X-ray registration in image-guided navigation\nremains challenging because of its stringent requirements for high accuracy and\nreal-time performance. Traditional \"render and compare\" methods, relying on\niterative projection and comparison, suffer from spatial information loss and\ndomain gap. 3D reconstruction from biplanar X-rays supplements spatial and\nshape information for 2D/3D registration, but current methods are limited by\ndense-view requirements and struggles with noisy X-rays. To address these\nlimitations, we introduce RadGS-Reg, a novel framework for vertebral-level\nCT/X-ray registration through joint 3D Radiative Gaussians (RadGS)\nreconstruction and 3D/3D registration. Specifically, our biplanar X-rays\nvertebral RadGS reconstruction module explores learning-based RadGS\nreconstruction method with a Counterfactual Attention Learning (CAL) mechanism,\nfocusing on vertebral regions in noisy X-rays. Additionally, a patient-specific\npre-training strategy progressively adapts the RadGS-Reg from simulated to real\ndata while simultaneously learning vertebral shape prior knowledge. Experiments\non in-house datasets demonstrate the state-of-the-art performance for both\ntasks, surpassing existing methods. The code is available at:\nhttps://github.com/shenao1995/RadGS_Reg.", "AI": {"tldr": "本文提出RadGS-Reg，一个用于椎体级CT/X射线配准的新框架。它通过联合3D辐射高斯（RadGS）重建和3D/3D配准，解决了现有方法在空间信息丢失、域间隙、噪声鲁棒性差和密集视图要求等方面的局限性，并在内部数据集上取得了最先进的性能。", "motivation": "图像引导导航中的CT/X射线配准对精度和实时性要求高，但面临挑战。传统“渲染与比较”方法存在空间信息丢失和域间隙问题。基于双平面X射线的3D重建方法虽能补充信息，但受限于密集视图要求且难以处理噪声X射线。", "method": "本文引入RadGS-Reg框架，通过联合3D辐射高斯（RadGS）重建和3D/3D配准实现椎体级CT/X射线配准。具体方法包括：1) 双平面X射线椎体RadGS重建模块，采用基于学习的RadGS重建和反事实注意力学习（CAL）机制，以关注噪声X射线中的椎体区域。2) 患者特定预训练策略，逐步使RadGS-Reg从模拟数据适应真实数据，同时学习椎体形状先验知识。", "result": "在内部数据集上进行的实验表明，RadGS-Reg在RadGS重建和CT/X射线配准这两项任务中均表现出最先进的性能，超越了现有方法。", "conclusion": "RadGS-Reg通过其创新的联合3D辐射高斯重建和3D/3D配准方法，并结合反事实注意力学习和患者特定预训练策略，有效克服了传统CT/X射线配准的挑战，实现了高精度、鲁棒的椎体级配准。"}}
{"id": "2508.21475", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.21475", "abs": "https://arxiv.org/abs/2508.21475", "authors": ["Xijia Tao", "Yihua Teng", "Xinxing Su", "Xinyu Fu", "Jihao Wu", "Chaofan Tao", "Ziru Liu", "Haoli Bai", "Rui Liu", "Lingpeng Kong"], "title": "MMSearch-Plus: A Simple Yet Challenging Benchmark for Multimodal Browsing Agents", "comment": "Project Page: https://mmsearch-plus.github.io", "summary": "Large multimodal language models (MLLMs) are increasingly deployed as web\nagents, yet many multimodal browsing benchmarks can be solved by shallow, fixed\nworkflows that lean on high-recall image search and nearby text-masking the\ngenuinely multimodal challenges of fine-grained visual reasoning, provenance\nverification, and long-horizon tool use. We introduce MMSearch-Plus, a\nbenchmark of 311 tasks that highly demand multimodal understanding while\npreserving the difficulty profile of strong text-only browsing suites. Each\nitem is constructed to contain multiple weak, localized visual signals that\nmust be extracted, propagated through iterative text-image search, and\ncross-validated under retrieval noise before answering. Our curation procedure,\nSpatial-Temporal Extrapolation, seeds questions whose answers require\nextrapolating from spatial cues (micro-text, part-level appearance, layouts,\nsignage) and temporal traces (broadcast overlays, seasonal context) to\nout-of-image facts such as events, dates, and venues. We provide a\nmodel-agnostic agent framework with browsing tools and evaluate a range of\nclosed and open MLLMs. The strongest agent (o3) attains 15.1% without search\nand 36.0% accuracy with rollout under our framework, while a strong open-source\nmodel (Qwen-2.5-VL-72B-Instruct) achieves 0.0% without search and 6.9% after 20\nrounds of search. Beyond answer accuracy, we assess bounding-box production and\ncropped-image search, and conduct an error analysis that surfaces failures in\nsource verification, part-based reasoning, and long-horizon planning.", "AI": {"tldr": "本文引入了MMSearch-Plus，一个包含311个任务的新基准，旨在挑战大型多模态语言模型（MLLMs）在网页代理场景下进行精细多模态理解、溯源验证和长周期工具使用的能力，并揭示了现有模型的显著不足。", "motivation": "现有的多模态浏览基准可以通过浅层、固定的工作流程（依赖高召回率图像搜索和附近文本掩码）解决，未能真正考验MLLMs在精细视觉推理、溯源验证和长周期工具使用方面的多模态能力。", "method": "研究人员构建了MMSearch-Plus基准，包含311个高度要求多模态理解的任务。每个任务都设计为需要提取、通过迭代文本-图像搜索传播，并在检索噪声下交叉验证多个弱的、局部视觉信号。其策展过程“时空外推法”通过空间线索（微文本、部件级外观、布局、标牌）和时间痕迹（广播叠加、季节背景）来推断图像外事实（如事件、日期、地点）。此外，本文提供了一个模型无关的代理框架和浏览工具，并评估了一系列封闭和开放的MLLMs，同时评估了边界框生成和裁剪图像搜索，并进行了错误分析。", "result": "在所提出的框架下，最强的代理（o3）在没有搜索的情况下准确率为15.1%，在进行搜索后准确率为36.0%。一个强大的开源模型（Qwen-2.5-VL-72B-Instruct）在没有搜索的情况下准确率为0.0%，在20轮搜索后准确率为6.9%。错误分析揭示了模型在来源验证、基于部件的推理和长周期规划方面的失败。", "conclusion": "MMSearch-Plus基准揭示了当前MLLMs在需要精细多模态理解、迭代搜索和交叉验证的复杂网页浏览任务中表现出显著不足，尤其是在来源验证、部件推理和长周期规划方面，为未来MLLM的发展指明了方向。"}}
{"id": "2508.21294", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.21294", "abs": "https://arxiv.org/abs/2508.21294", "authors": ["João Guilherme Alves Santos", "Giovana Kerche Bonás", "Thales Sales Almeida"], "title": "BLUEX Revisited: Enhancing Benchmark Coverage with Automatic Captioning", "comment": "12 pages, 5 figures, 2 tables", "summary": "With the growing capabilities of Large Language Models (LLMs), there is an\nincreasing need for robust evaluation methods, especially in multilingual and\nnon-English contexts. We present an updated version of the BLUEX dataset, now\nincluding 2024-2025 exams and automatically generated image captions using\nstate-of-the-art models, enhancing its relevance for data contamination studies\nin LLM pretraining. Captioning strategies increase accessibility to text-only\nmodels by more than 40%, producing 1,422 usable questions, more than doubling\nthe number in the original BLUEX. We evaluated commercial and open-source LLMs\nand their ability to leverage visual context through captions.", "AI": {"tldr": "本文介绍了BLUEX数据集的更新版本，增加了2024-2025年考试内容和自动生成的图像字幕，以增强其在多语言LLM评估和数据污染研究中的相关性，并通过评估LLM来验证其有效性。", "motivation": "随着大型语言模型（LLM）能力的增长，对鲁棒评估方法的需求也日益增加，尤其是在多语言和非英语环境中。此外，研究LLM预训练中的数据污染问题也是一个重要动机。", "method": "研究人员更新了BLUEX数据集，纳入了2024-2025年的考试内容。他们使用最先进的模型自动生成图像字幕，以提高文本模型对视觉内容的访问能力。随后，评估了商业和开源LLM利用通过字幕提供的视觉上下文的能力。", "result": "字幕策略使文本模型的可访问性提高了40%以上，产生了1,422个可用问题，是原始BLUEX数量的两倍多。这显著增强了数据集对LLM预训练中数据污染研究的相关性。", "conclusion": "更新后的BLUEX数据集（包含最新考试和自动生成的图像字幕）极大地提升了多语言LLM评估和数据污染研究的能力，通过字幕使视觉上下文对纯文本模型更易于访问。"}}
{"id": "2508.21378", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.21378", "abs": "https://arxiv.org/abs/2508.21378", "authors": ["Chenduo Ying", "Linkang Du", "Peng Cheng", "Yuanchao Shu"], "title": "RoboInspector: Unveiling the Unreliability of Policy Code for LLM-enabled Robotic Manipulation", "comment": null, "summary": "Large language models (LLMs) demonstrate remarkable capabilities in reasoning\nand code generation, enabling robotic manipulation to be initiated with just a\nsingle instruction. The LLM carries out various tasks by generating policy code\nrequired to control the robot. Despite advances in LLMs, achieving reliable\npolicy code generation remains a significant challenge due to the diverse\nrequirements of real-world tasks and the inherent complexity of user\ninstructions. In practice, different users may provide distinct instructions to\ndrive the robot for the same task, which may cause the unreliability of policy\ncode generation. To bridge this gap, we design RoboInspector, a pipeline to\nunveil and characterize the unreliability of the policy code for LLM-enabled\nrobotic manipulation from two perspectives: the complexity of the manipulation\ntask and the granularity of the instruction. We perform comprehensive\nexperiments with 168 distinct combinations of tasks, instructions, and LLMs in\ntwo prominent frameworks. The RoboInspector identifies four main unreliable\nbehaviors that lead to manipulation failure. We provide a detailed\ncharacterization of these behaviors and their underlying causes, giving insight\nfor practical development to reduce unreliability. Furthermore, we introduce a\nrefinement approach guided by failure policy code feedback that improves the\nreliability of policy code generation by up to 35% in LLM-enabled robotic\nmanipulation, evaluated in both simulation and real-world environments.", "AI": {"tldr": "本研究设计了RoboInspector，一个用于揭示和分析大语言模型（LLM）驱动机器人操作中策略代码不可靠性的管道，并提出了一种基于失败策略代码反馈的改进方法，将可靠性提高了35%。", "motivation": "尽管LLM在机器人操作中展现出卓越能力，但由于真实世界任务的多样性和用户指令的复杂性，生成可靠的策略代码仍然是一个重大挑战，不同用户对同一任务可能给出不同指令，导致策略代码生成不可靠。", "method": "研究设计了RoboInspector管道，从操作任务的复杂性和指令的粒度两个角度，分析LLM驱动机器人操作中策略代码的不可靠性。通过在两个主流框架中，对168种不同的任务、指令和LLM组合进行综合实验。此外，还引入了一种由失败策略代码反馈指导的改进方法。", "result": "RoboInspector识别出导致操作失败的四种主要不可靠行为，并对其行为及其根本原因进行了详细的表征。改进方法在LLM驱动的机器人操作中，将策略代码生成的可靠性提高了高达35%，并在模拟和真实世界环境中进行了评估。", "conclusion": "本研究揭示并表征了LLM驱动机器人操作中策略代码的不可靠行为及其原因，为实际开发提供了减少不可靠性的见解。同时，提出的改进方法显著提升了策略代码生成的可靠性。"}}
{"id": "2508.21169", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.21169", "abs": "https://arxiv.org/abs/2508.21169", "authors": ["Kevin Mayer", "Alex Vesel", "Xinyi Zhao", "Martin Fischer"], "title": "SYNBUILD-3D: A large, multi-modal, and semantically rich synthetic dataset of 3D building models at Level of Detail 4", "comment": null, "summary": "3D building models are critical for applications in architecture, energy\nsimulation, and navigation. Yet, generating accurate and semantically rich 3D\nbuildings automatically remains a major challenge due to the lack of\nlarge-scale annotated datasets in the public domain. Inspired by the success of\nsynthetic data in computer vision, we introduce SYNBUILD-3D, a large, diverse,\nand multi-modal dataset of over 6.2 million synthetic 3D residential buildings\nat Level of Detail (LoD) 4. In the dataset, each building is represented\nthrough three distinct modalities: a semantically enriched 3D wireframe graph\nat LoD 4 (Modality I), the corresponding floor plan images (Modality II), and a\nLiDAR-like roof point cloud (Modality III). The semantic annotations for each\nbuilding wireframe are derived from the corresponding floor plan images and\ninclude information on rooms, doors, and windows. Through its tri-modal nature,\nfuture work can use SYNBUILD-3D to develop novel generative AI algorithms that\nautomate the creation of 3D building models at LoD 4, subject to predefined\nfloor plan layouts and roof geometries, while enforcing semantic-geometric\nconsistency. Dataset and code samples are publicly available at\nhttps://github.com/kdmayer/SYNBUILD-3D.", "AI": {"tldr": "本文介绍了SYNBUILD-3D，一个包含超过620万个合成LoD 4住宅建筑的大型、多样化、多模态数据集，旨在解决3D建筑模型自动生成中缺乏标注数据的问题。", "motivation": "3D建筑模型在建筑、能源模拟和导航等应用中至关重要。然而，由于公共领域缺乏大规模标注数据集，自动生成精确且语义丰富的3D建筑仍然是一个重大挑战。", "method": "受计算机视觉中合成数据成功的启发，作者引入了SYNBUILD-3D数据集。该数据集包含超过620万个合成的LoD 4住宅建筑，每个建筑通过三种模态表示：语义丰富的LoD 4 3D线框图、相应的平面图图像和类似LiDAR的屋顶点云。语义标注（房间、门、窗）从平面图图像中导出。", "result": "创建了一个大型（超过620万个建筑）、多样化、多模态的合成数据集SYNBUILD-3D。每个建筑都包含LoD 4线框图（带语义标注）、平面图图像和屋顶点云，实现了语义-几何一致性。", "conclusion": "SYNBUILD-3D数据集的三模态特性，将支持未来开发新颖的生成式AI算法，以自动化创建LoD 4 3D建筑模型，这些模型将基于预定义的平面布局和屋顶几何结构，并能强制执行语义-几何一致性。"}}
{"id": "2508.21517", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.21517", "abs": "https://arxiv.org/abs/2508.21517", "authors": ["Sweta Kaman", "Ankita Sharma", "Romi Banerjee"], "title": "Modeling Wise Decision Making: A Z-Number Fuzzy Framework Inspired by Phronesis", "comment": "total 17 pages, main manuscript 12 pages, supplementary 5 pages, 6\n  tables in main manuscript, 5 figures in main manuscript, 2 tables in\n  supplementary, and 3 figures in supplementary", "summary": "Background: Wisdom is a superordinate construct that embraces perspective\ntaking, reflectiveness, prosocial orientation, reflective empathetic action,\nand intellectual humility. Unlike conventional models of reasoning that are\nrigidly bound by binary thinking, wisdom unfolds in shades of ambiguity,\nrequiring both graded evaluation and self-reflective humility. Current measures\ndepend on self-reports and seldom reflect the humility and uncertainty inherent\nin wise reasoning. A computational framework that takes into account both\nmultidimensionality and confidence has the potential to improve psychological\nscience and allow humane AI. Method: We present a fuzzy inference system with Z\nnumbers, each of the decisions being expressed in terms of a wisdom score\n(restriction) and confidence score (certainty). As part of this study,\nparticipants (N = 100) were exposed to culturally neutral pictorial moral\ndilemma tasks to which they generated think-aloud linguistic responses, which\nwere mapped into five theoretically based components of wisdom. The scores of\neach individual component were combined using a base of 21 rules, with\nmembership functions tuned via Gaussian kernel density estimation. Results: In\na proof of concept study, the system produced dual attribute wisdom\nrepresentations that correlated modestly but significantly with established\nscales while showing negligible relations with unrelated traits, supporting\nconvergent and divergent validity. Contribution: The contribution is to\nformalize wisdom as a multidimensional, uncertainty-conscious construct,\noperationalized in the form of Z-numbers. In addition to progressing\nmeasurement in psychology, it calculates how fuzzy Z numbers can provide AI\nsystems with interpretable, confidence-sensitive reasoning that affords a safe,\nmiddle ground between rigorous computation and human-like judgment.", "AI": {"tldr": "该研究提出了一种基于Z数的模糊推理系统，用于计算智慧的双属性表示（智慧得分和置信度），旨在克服传统测量方法的局限性，并为心理学测量和人性化AI提供新的框架。", "motivation": "传统的推理模型过于僵化，采用二元思维，无法捕捉智慧中固有的模糊性、不确定性和谦逊性。现有智慧测量多依赖自我报告，未能充分反映智慧推理中的谦逊和不确定性。因此，需要一个能同时考虑多维度和置信度的计算框架来改进心理学研究并促进人性化AI发展。", "method": "研究采用了一个基于Z数的模糊推理系统，将每个决策表示为智慧得分（限制）和置信度得分（确定性）。100名参与者完成文化中立的图片道德困境任务，并生成有声思维语言回应，这些回应被映射到智慧的五个理论组件。通过21条规则和高斯核密度估计调整的隶属函数，将各个组件得分进行组合。", "result": "概念验证研究表明，该系统生成了双属性的智慧表示，这些表示与现有量表适度但显著相关（支持聚合效度），同时与不相关特质关系微弱（支持区分效度）。", "conclusion": "该研究将智慧形式化为一个多维度、不确定性感知的构建，并以Z数形式进行操作化。除了推动心理学测量进步外，它还展示了模糊Z数如何为AI系统提供可解释、置信度敏感的推理，从而在严格计算和类人判断之间提供一个安全的中间地带。"}}
{"id": "2508.21377", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50, 68T07", "I.2.7; I.2.6; H.3.3"], "pdf": "https://arxiv.org/pdf/2508.21377", "abs": "https://arxiv.org/abs/2508.21377", "authors": ["Shubham Sharma", "Sneha Tuli", "Narendra Badam"], "title": "Challenges and Applications of Large Language Models: A Comparison of GPT and DeepSeek family of models", "comment": "18 pages, 7 figures", "summary": "Large Language Models (LLMs) are transforming AI across industries, but their\ndevelopment and deployment remain complex. This survey reviews 16 key\nchallenges in building and using LLMs and examines how these challenges are\naddressed by two state-of-the-art models with unique approaches: OpenAI's\nclosed source GPT-4o (May 2024 update) and DeepSeek-V3-0324 (March 2025), a\nlarge open source Mixture-of-Experts model. Through this comparison, we\nshowcase the trade-offs between closed source models (robust safety, fine-tuned\nreliability) and open source models (efficiency, adaptability). We also explore\nLLM applications across different domains (from chatbots and coding tools to\nhealthcare and education), highlighting which model attributes are best suited\nfor each use case. This article aims to guide AI researchers, developers, and\ndecision-makers in understanding current LLM capabilities, limitations, and\nbest practices.", "AI": {"tldr": "该调查回顾了构建和使用大型语言模型（LLMs）的16个关键挑战，并通过比较OpenAI的GPT-4o（闭源）和DeepSeek-V3-0324（开源MoE模型）来探讨解决方案、权衡和不同应用场景下的模型适用性。", "motivation": "大型语言模型的开发和部署仍然复杂，需要深入理解其能力、局限性及最佳实践，以指导AI研究人员、开发者和决策者。", "method": "通过审查16个关键挑战，并比较两种最先进的模型（OpenAI的闭源GPT-4o和开源的DeepSeek-V3-0324 MoE模型）如何应对这些挑战，同时探索LLM在不同领域的应用。", "result": "研究展示了闭源模型（鲁棒安全性、精细调优的可靠性）和开源模型（效率、适应性）之间的权衡，并指出哪些模型属性最适合特定的用例，涵盖从聊天机器人到医疗和教育等应用领域。", "conclusion": "本文旨在帮助AI研究人员、开发者和决策者理解当前LLM的能力、局限性及最佳实践，从而更好地应对LLM的开发和部署挑战。"}}
{"id": "2508.21455", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.21455", "abs": "https://arxiv.org/abs/2508.21455", "authors": ["Hariharan Arunachalam", "Phani Teja Singamaneni", "Rachid Alami"], "title": "Assessing Human Cooperation for Enhancing Social Robot Navigation", "comment": null, "summary": "Socially aware robot navigation is a planning paradigm where the robot\nnavigates in human environments and tries to adhere to social constraints while\ninteracting with the humans in the scene. These navigation strategies were\nfurther improved using human prediction models, where the robot takes the\npotential future trajectory of humans while computing its own. Though these\nstrategies significantly improve the robot's behavior, it faces difficulties\nfrom time to time when the human behaves in an unexpected manner. This happens\nas the robot fails to understand human intentions and cooperativeness, and the\nhuman does not have a clear idea of what the robot is planning to do. In this\npaper, we aim to address this gap through effective communication at an\nappropriate time based on a geometric analysis of the context and human\ncooperativeness in head-on crossing scenarios. We provide an assessment\nmethodology and propose some evaluation metrics that could distinguish a\ncooperative human from a non-cooperative one. Further, we also show how\ngeometric reasoning can be used to generate appropriate verbal responses or\nrobot actions.", "AI": {"tldr": "本文提出了一种通过基于几何分析和人类合作性评估的有效沟通策略，来解决社交机器人导航中因人类意外行为而导致的困难，特别是在迎面交叉场景中。", "motivation": "现有的社交感知机器人导航策略，即使结合了人类预测模型，仍难以应对人类的意外行为。这主要是因为机器人无法理解人类的意图和合作性，同时人类也无法清晰了解机器人的规划，从而导致导航困难。", "method": "本文通过在适当时间进行有效沟通来弥补这一缺陷。方法包括：基于上下文的几何分析和人类合作性评估（针对迎面交叉场景），提供评估方法和指标来区分合作性与非合作性人类，并展示如何利用几何推理生成适当的语言响应或机器人动作。", "result": "论文提供了一套评估方法和评估指标，能够区分合作性人类与非合作性人类。此外，还展示了几何推理如何被用于生成恰当的口头响应或机器人动作，以改善人机交互。", "conclusion": "通过在适当时间进行基于几何分析和人类合作性评估的有效沟通，可以有效解决社交机器人导航中因人类意外行为而产生的挑战，提升机器人理解人类意图并做出相应反应的能力。"}}
{"id": "2508.21190", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.21190", "abs": "https://arxiv.org/abs/2508.21190", "authors": ["Mårten Wadenbäck", "Marcus Valtonen Örnhag", "Johan Edstedt"], "title": "Radially Distorted Homographies, Revisited", "comment": null, "summary": "Homographies are among the most prevalent transformations occurring in\ngeometric computer vision and projective geometry, and homography estimation is\nconsequently a crucial step in a wide assortment of computer vision tasks. When\nworking with real images, which are often afflicted with geometric distortions\ncaused by the camera lens, it may be necessary to determine both the homography\nand the lens distortion-particularly the radial component, called radial\ndistortion-simultaneously to obtain anything resembling useful estimates. When\nconsidering a homography with radial distortion between two images, there are\nthree conceptually distinct configurations for the radial distortion; (i)\ndistortion in only one image, (ii) identical distortion in the two images, and\n(iii) independent distortion in the two images. While these cases have been\naddressed separately in the past, the present paper provides a novel and\nunified approach to solve all three cases. We demonstrate how the proposed\napproach can be used to construct new fast, stable, and accurate minimal\nsolvers for radially distorted homographies. In all three cases, our proposed\nsolvers are faster than the existing state-of-the-art solvers while maintaining\nsimilar accuracy. The solvers are tested on well-established benchmarks\nincluding images taken with fisheye cameras. The source code for our solvers\nwill be made available in the event our paper is accepted for publication.", "AI": {"tldr": "本文提出了一种新颖统一的方法，用于同时估计径向畸变同形变换，涵盖了三种不同的畸变配置。该方法构建了更快、更稳定、更准确的最小求解器，并在保持相似精度的前提下，显著提升了现有技术的速度。", "motivation": "同形变换估计是计算机视觉中的关键步骤，但真实图像常受相机镜头几何畸变（特别是径向畸变）影响。为获得有效估计，需要同时确定同形变换和径向畸变。现有方法通常分别处理三种径向畸变配置，缺乏统一方案。", "method": "本文提出了一种新颖且统一的方法来解决径向畸变同形变换的三种概念上不同的配置：（i）仅在一张图像中存在畸变，（ii）两张图像中存在相同畸变，以及（iii）两张图像中存在独立畸变。该方法被用于构建新的快速、稳定、准确的最小求解器。", "result": "在所有三种畸变配置下，本文提出的求解器比现有最先进的求解器更快，同时保持了相似的精度。这些求解器已在包括鱼眼相机图像在内的成熟基准测试中进行了验证。", "conclusion": "本文成功开发了一种统一、快速、稳定且准确的最小求解器，用于处理径向畸变同形变换的估计问题。该求解器在速度上优于现有技术，同时保持了高精度，有效解决了真实图像中同形变换和镜头畸变同时估计的挑战。"}}
{"id": "2508.21521", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.21521", "abs": "https://arxiv.org/abs/2508.21521", "authors": ["Nicola Gigante", "Francesco Leofante", "Andrea Micheli"], "title": "Counterfactual Scenarios for Automated Planning", "comment": "Accepted at the 22nd International Conference on Principles of\n  Knowledge Representation and Reasoning (KR 2025)", "summary": "Counterfactual Explanations (CEs) are a powerful technique used to explain\nMachine Learning models by showing how the input to a model should be minimally\nchanged for the model to produce a different output. Similar proposals have\nbeen made in the context of Automated Planning, where CEs have been\ncharacterised in terms of minimal modifications to an existing plan that would\nresult in the satisfaction of a different goal. While such explanations may\nhelp diagnose faults and reason about the characteristics of a plan, they fail\nto capture higher-level properties of the problem being solved. To address this\nlimitation, we propose a novel explanation paradigm that is based on\ncounterfactual scenarios. In particular, given a planning problem $P$ and an\n\\ltlf formula $\\psi$ defining desired properties of a plan, counterfactual\nscenarios identify minimal modifications to $P$ such that it admits plans that\ncomply with $\\psi$. In this paper, we present two qualitative instantiations of\ncounterfactual scenarios based on an explicit quantification over plans that\nmust satisfy $\\psi$. We then characterise the computational complexity of\ngenerating such counterfactual scenarios when different types of changes are\nallowed on $P$. We show that producing counterfactual scenarios is often only\nas expensive as computing a plan for $P$, thus demonstrating the practical\nviability of our proposal and ultimately providing a framework to construct\npractical algorithms in this area.", "AI": {"tldr": "本文提出了一种基于反事实情景的新型解释范式，通过最小化修改规划问题本身，使其能够产生满足特定高层属性（LTLf公式）的计划，从而克服了传统反事实解释在规划中只关注计划修改的局限性。", "motivation": "现有的自动化规划中的反事实解释（CEs）侧重于对现有计划进行最小修改以达到不同目标，但这未能捕捉到所解决问题的更高层次属性。研究动机是为了解决这一局限性。", "method": "提出了一种基于“反事实情景”的新型解释范式。给定规划问题P和定义期望计划属性的LTLf公式ψ，反事实情景识别对P的最小修改，使得P能够产生符合ψ的计划。论文提出了两种基于显式量化满足ψ的计划的定性反事实情景实例化，并分析了在允许不同类型P修改时生成此类反事实情景的计算复杂性。", "result": "生成反事实情景的计算成本通常与计算P的计划的成本相当，这表明了该提案的实际可行性。", "conclusion": "该研究提供了一个构建该领域实用算法的框架，并证明了其提出的反事实情景解释范式的实际可行性。"}}
{"id": "2508.21382", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.21382", "abs": "https://arxiv.org/abs/2508.21382", "authors": ["Alexandre Kabbach"], "title": "Normality and the Turing Test", "comment": null, "summary": "This paper proposes to revisit the Turing test through the concept of\nnormality. Its core argument is that the statistical interpretation of the\nnormal--understood as the average both in the normative and mathematical sense\nof the term--proves useful for understanding the Turing test in at least two\nways. First, in the sense that the Turing test targets normal/average rather\nthan exceptional human intelligence, so that successfully passing the test\nrequires building machines that \"make mistakes\" and display imperfect behavior\njust like normal/average humans. Second, in the sense that the Turing test is a\nstatistical test where judgments of intelligence are never carried out by a\nsingle \"average\" judge (understood as non-expert) but always by a full jury. As\nsuch, the notion of \"average human interrogator\" that Turing talks about in his\noriginal paper should be understood primarily as referring to a mathematical\nabstraction made of the normalized aggregate of individual judgments of\nmultiple judges. In short, this paper argues that the Turing test is a test of\nnormal intelligence as assessed by a normal judge characterizing the average\njudgment of a pool of human interrogators. Its conclusions are twofold. First,\nit argues that large language models such as ChatGPT are unlikely to pass the\nTuring test as those models precisely target exceptional rather than\nnormal/average human intelligence. As such, they constitute models of what it\nproposes to call artificial smartness rather than artificial intelligence per\nse. Second, it argues that the core question of whether the Turing test can\ncontribute anything to the understanding of human cognition is that of whether\nthe human mind is really reducible to the normal/average mind--a question which\nlargely extends beyond the Turing test itself and questions the conceptual\nunderpinnings of the normalist paradigm it belongs to.", "AI": {"tldr": "本文通过“常态”概念重新审视图灵测试，认为该测试旨在评估机器是否能展现出常态人类智能（包括犯错），并由多名评委的平均判断来衡量。据此，大型语言模型（如ChatGPT）因追求卓越而非常态智能，可能无法通过图灵测试，它们代表的是“人工聪明”而非真正意义上的“人工智能”。", "motivation": "重新审视图灵测试，并提出通过“常态”（在规范和数学意义上理解为平均）概念来深入理解其核心论点，尤其是在当前大型语言模型兴起的背景下。", "method": "概念分析法，通过对“常态”一词的统计学和规范性解释，重新解读图灵测试的两个方面：一是其评估目标是常态人类智能，二是其判断机制是一个统计性测试，由多位评委的平均判断构成。", "result": "1. 图灵测试旨在评估常态/平均人类智能，要求机器能像普通人一样犯错和表现出不完美行为。2. 图灵测试是一个统计性测试，智能判断由一个评委团而非单一“平均”评委完成，“平均人类提问者”指的是多个个体判断的标准化集合。3. 大型语言模型（如ChatGPT）不太可能通过图灵测试，因为它们追求的是卓越而非常态/平均人类智能，因此它们是“人工聪明”而非“人工智能”。4. 图灵测试能否有助于理解人类认知，取决于人类心智是否能简化为常态/平均心智，这超出了图灵测试本身的范畴。", "conclusion": "1. 大型语言模型（如ChatGPT）代表的是“人工聪明”而非“人工智能”，它们不太可能通过基于常态智能评估的图灵测试。2. 图灵测试对人类认知的贡献，最终取决于人类心智是否真的能简化为常态/平均心智，这触及了常态主义范式的概念基础。"}}
{"id": "2508.21501", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.21501", "abs": "https://arxiv.org/abs/2508.21501", "authors": ["Pierrick Lorang", "Hong Lu", "Johannes Huemer", "Patrik Zips", "Matthias Scheutz"], "title": "Few-Shot Neuro-Symbolic Imitation Learning for Long-Horizon Planning and Acting", "comment": "Accepted at CoRL 2025; to appear in PMLR", "summary": "Imitation learning enables intelligent systems to acquire complex behaviors\nwith minimal supervision. However, existing methods often focus on\nshort-horizon skills, require large datasets, and struggle to solve\nlong-horizon tasks or generalize across task variations and distribution\nshifts. We propose a novel neuro-symbolic framework that jointly learns\ncontinuous control policies and symbolic domain abstractions from a few skill\ndemonstrations. Our method abstracts high-level task structures into a graph,\ndiscovers symbolic rules via an Answer Set Programming solver, and trains\nlow-level controllers using diffusion policy imitation learning. A high-level\noracle filters task-relevant information to focus each controller on a minimal\nobservation and action space. Our graph-based neuro-symbolic framework enables\ncapturing complex state transitions, including non-spatial and temporal\nrelations, that data-driven learning or clustering techniques often fail to\ndiscover in limited demonstration datasets. We validate our approach in six\ndomains that involve four robotic arms, Stacking, Kitchen, Assembly, and Towers\nof Hanoi environments, and a distinct Automated Forklift domain with two\nenvironments. The results demonstrate high data efficiency with as few as five\nskill demonstrations, strong zero- and few-shot generalizations, and\ninterpretable decision making.", "AI": {"tldr": "本文提出了一种新颖的神经符号框架，通过少量技能演示共同学习连续控制策略和符号领域抽象，以克服现有模仿学习在长时序任务、泛化性和数据效率上的局限性。", "motivation": "现有的模仿学习方法通常专注于短时序技能，需要大量数据集，难以解决长时序任务，也难以泛化到任务变体和分布变化中。", "method": "该框架将高层任务结构抽象为图，通过Answer Set Programming (ASP) 求解器发现符号规则，并使用扩散策略模仿学习训练低层控制器。一个高层预言机过滤任务相关信息，使每个控制器专注于最小的观察和动作空间。这种基于图的神经符号框架能够捕获复杂的状态转换，包括非空间和时间关系。", "result": "该方法在六个领域（包括四种机械臂环境和自动化叉车环境）中进行了验证。结果表明，该方法具有高数据效率（低至五次技能演示）、强大的零样本和少样本泛化能力，以及可解释的决策制定。", "conclusion": "该神经符号框架通过结合符号推理和扩散策略模仿学习，有效解决了模仿学习在长时序任务、数据效率和泛化性方面的挑战，并提供了可解释的决策过程。"}}
{"id": "2508.21197", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.21197", "abs": "https://arxiv.org/abs/2508.21197", "authors": ["Zhenghao He", "Sanchit Sinha", "Guangzhi Xiong", "Aidong Zhang"], "title": "GCAV: A Global Concept Activation Vector Framework for Cross-Layer Consistency in Interpretability", "comment": null, "summary": "Concept Activation Vectors (CAVs) provide a powerful approach for\ninterpreting deep neural networks by quantifying their sensitivity to\nhuman-defined concepts. However, when computed independently at different\nlayers, CAVs often exhibit inconsistencies, making cross-layer comparisons\nunreliable. To address this issue, we propose the Global Concept Activation\nVector (GCAV), a novel framework that unifies CAVs into a single, semantically\nconsistent representation. Our method leverages contrastive learning to align\nconcept representations across layers and employs an attention-based fusion\nmechanism to construct a globally integrated CAV. By doing so, our method\nsignificantly reduces the variance in TCAV scores while preserving concept\nrelevance, ensuring more stable and reliable concept attributions. To evaluate\nthe effectiveness of GCAV, we introduce Testing with Global Concept Activation\nVectors (TGCAV) as a method to apply TCAV to GCAV-based representations. We\nconduct extensive experiments on multiple deep neural networks, demonstrating\nthat our method effectively mitigates concept inconsistency across layers,\nenhances concept localization, and improves robustness against adversarial\nperturbations. By integrating cross-layer information into a coherent\nframework, our method offers a more comprehensive and interpretable\nunderstanding of how deep learning models encode human-defined concepts. Code\nand models are available at https://github.com/Zhenghao-He/GCAV.", "AI": {"tldr": "本文提出了全局概念激活向量（GCAV），通过对比学习和注意力融合机制，将深度神经网络中不同层的概念激活向量（CAVs）统一为语义一致的表示，从而解决了传统CAVs跨层不一致的问题，提供了更稳定和可靠的概念归因。", "motivation": "传统的概念激活向量（CAVs）在不同层独立计算时常表现出不一致性，导致跨层比较不可靠。这限制了对深度神经网络如何编码人类定义概念的全面理解。", "method": "研究者提出了全局概念激活向量（GCAV）框架，通过以下方式实现：1) 利用对比学习对齐跨层的概念表示；2) 采用注意力机制融合不同层的概念信息，构建一个全局集成的CAV。此外，还引入了TGCAV方法，将TCAV应用于基于GCAV的表示。", "result": "GCAV显著降低了TCAV分数的方差，同时保留了概念相关性，确保了更稳定和可靠的概念归因。实验证明，该方法有效缓解了跨层概念不一致性，增强了概念定位能力，并提高了对抗性扰动的鲁棒性。", "conclusion": "通过将跨层信息整合到一个连贯的框架中，GCAV为理解深度学习模型如何编码人类定义的概念提供了一种更全面、更具解释性的方法。"}}
{"id": "2508.21540", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.21540", "abs": "https://arxiv.org/abs/2508.21540", "authors": ["Eduardo Illueca-Fernandez", "Kaile Chen", "Fernando Seoane", "Farhad Abtahi"], "title": "HealthProcessAI: A Technical Framework and Proof-of-Concept for LLM-Enhanced Healthcare Process Mining", "comment": null, "summary": "Process mining has emerged as a powerful analytical technique for\nunderstanding complex healthcare workflows. However, its application faces\nsignificant barriers, including technical complexity, a lack of standardized\napproaches, and limited access to practical training resources. We introduce\nHealthProcessAI, a GenAI framework designed to simplify process mining\napplications in healthcare and epidemiology by providing a comprehensive\nwrapper around existing Python (PM4PY) and R (bupaR) libraries. To address\nunfamiliarity and improve accessibility, the framework integrates multiple\nLarge Language Models (LLMs) for automated process map interpretation and\nreport generation, helping translate technical analyses into outputs that\ndiverse users can readily understand. We validated the framework using sepsis\nprogression data as a proof-of-concept example and compared the outputs of five\nstate-of-the-art LLM models through the OpenRouter platform. To test its\nfunctionality, the framework successfully processed sepsis data across four\nproof-of-concept scenarios, demonstrating robust technical performance and its\ncapability to generate reports through automated LLM analysis. LLM evaluation\nusing five independent LLMs as automated evaluators revealed distinct model\nstrengths: Claude Sonnet-4 and Gemini 2.5-Pro achieved the highest consistency\nscores (3.79/4.0 and 3.65/4.0) when evaluated by automated LLM assessors. By\nintegrating multiple Large Language Models (LLMs) for automated interpretation\nand report generation, the framework addresses widespread unfamiliarity with\nprocess mining outputs, making them more accessible to clinicians, data\nscientists, and researchers. This structured analytics and AI-driven\ninterpretation combination represents a novel methodological advance in\ntranslating complex process mining results into potentially actionable insights\nfor healthcare applications.", "AI": {"tldr": "HealthProcessAI是一个GenAI框架，旨在通过整合LLMs简化医疗和流行病学中的过程挖掘应用，实现自动化解释和报告生成，使复杂分析结果更易于理解。", "motivation": "过程挖掘在医疗领域应用面临技术复杂性、缺乏标准化方法和培训资源有限等障碍。此外，其技术分析结果难以被不同背景的用户理解。", "method": "引入HealthProcessAI框架，它封装了现有的Python (PM4PY) 和 R (bupaR) 库。该框架集成了多个大型语言模型 (LLMs) 用于自动化过程图解释和报告生成。通过败血症进展数据进行概念验证，并使用OpenRouter平台比较了五种最先进的LLM模型的输出。", "result": "该框架成功处理了四种概念验证场景下的败血症数据，展示了强大的技术性能和通过自动化LLM分析生成报告的能力。LLM评估显示，Claude Sonnet-4和Gemini 2.5-Pro在自动化LLM评估器评估时，分别达到了最高的报告一致性分数（3.79/4.0和3.65/4.0）。", "conclusion": "通过集成多个LLMs进行自动化解释和报告生成，HealthProcessAI框架解决了对过程挖掘输出不熟悉的问题，使其更易于临床医生、数据科学家和研究人员理解。这种结构化分析与AI驱动解释的结合，代表了将复杂过程挖掘结果转化为医疗应用中潜在可操作洞察的新颖方法学进展。"}}
{"id": "2508.21389", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.21389", "abs": "https://arxiv.org/abs/2508.21389", "authors": ["Tanguy Herserant", "Vincent Guigue"], "title": "AllSummedUp: un framework open-source pour comparer les metriques d'evaluation de resume", "comment": "in French language", "summary": "This paper investigates reproducibility challenges in automatic text\nsummarization evaluation. Based on experiments conducted across six\nrepresentative metrics ranging from classical approaches like ROUGE to recent\nLLM-based methods (G-Eval, SEval-Ex), we highlight significant discrepancies\nbetween reported performances in the literature and those observed in our\nexperimental setting. We introduce a unified, open-source framework, applied to\nthe SummEval dataset and designed to support fair and transparent comparison of\nevaluation metrics. Our results reveal a structural trade-off: metrics with the\nhighest alignment with human judgments tend to be computationally intensive and\nless stable across runs. Beyond comparative analysis, this study highlights key\nconcerns about relying on LLMs for evaluation, stressing their randomness,\ntechnical dependencies, and limited reproducibility. We advocate for more\nrobust evaluation protocols including exhaustive documentation and\nmethodological standardization to ensure greater reliability in automatic\nsummarization assessment.", "AI": {"tldr": "本文调查了自动文本摘要评估中的可重现性挑战，发现现有指标（特别是基于LLM的）存在显著差异和稳定性问题，并提出了一个统一的开源框架，呼吁更严格的评估协议。", "motivation": "文献中报告的自动文本摘要评估指标性能与实际观察到的性能之间存在显著差异，尤其是在经典方法和新兴的基于大型语言模型（LLM）的方法之间，这促使研究者寻求更公平透明的比较和对可重现性问题的深入理解。", "method": "研究人员在六种代表性评估指标（从ROUGE等经典方法到G-Eval、SEval-Ex等LLM-based方法）上进行了实验。他们引入了一个统一的开源框架，并将其应用于SummEval数据集，旨在支持评估指标的公平透明比较。", "result": "实验结果显示，文献中报告的性能与实际观察到的性能之间存在显著差异。研究揭示了一个结构性权衡：与人类判断对齐度最高的指标往往计算密集且跨运行稳定性较差。此外，研究强调了依赖LLM进行评估的关键担忧，包括其随机性、技术依赖性和有限的可重现性。", "conclusion": "为了确保自动摘要评估的更高可靠性，研究呼吁采用更稳健的评估协议，包括详尽的文档和方法论标准化。强调了对LLM评估方法可重现性问题的关注，并建议改进实践。"}}
{"id": "2508.21549", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.21549", "abs": "https://arxiv.org/abs/2508.21549", "authors": ["Liding Zhang", "Kuanqi Cai", "Yu Zhang", "Zhenshan Bing", "Chaoqun Wang", "Fan Wu", "Sami Haddadin", "Alois Knoll"], "title": "Estimated Informed Anytime Search for Sampling-Based Planning via Adaptive Sampler", "comment": null, "summary": "Path planning in robotics often involves solving continuously valued,\nhigh-dimensional problems. Popular informed approaches include graph-based\nsearches, such as A*, and sampling-based methods, such as Informed RRT*, which\nutilize informed set and anytime strategies to expedite path optimization\nincrementally. Informed sampling-based planners define informed sets as subsets\nof the problem domain based on the current best solution cost. However, when no\nsolution is found, these planners re-sample and explore the entire\nconfiguration space, which is time-consuming and computationally expensive.\nThis article introduces Multi-Informed Trees (MIT*), a novel planner that\nconstructs estimated informed sets based on prior admissible solution costs\nbefore finding the initial solution, thereby accelerating the initial\nconvergence rate. Moreover, MIT* employs an adaptive sampler that dynamically\nadjusts the sampling strategy based on the exploration process. Furthermore,\nMIT* utilizes length-related adaptive sparse collision checks to guide lazy\nreverse search. These features enhance path cost efficiency and computation\ntimes while ensuring high success rates in confined scenarios. Through a series\nof simulations and real-world experiments, it is confirmed that MIT*\noutperforms existing single-query, sampling-based planners for problems in R^4\nto R^16 and has been successfully applied to real-world robot manipulation\ntasks. A video showcasing our experimental results is available at:\nhttps://youtu.be/30RsBIdexTU", "AI": {"tldr": "MIT*是一种新型机器人路径规划器，通过在找到初始解之前构建估计信息集、采用自适应采样器和稀疏碰撞检测，显著提高了初始收敛速度和整体效率，在R^4到R^16维度问题和实际机器人任务中表现优于现有方法。", "motivation": "现有的基于采样的规划器在未找到初始解时，会重新采样并探索整个配置空间，这既耗时又计算昂贵，从而影响了初始收敛率。", "method": "MIT*提出以下创新点：1. 在找到初始解之前，基于先前的可接受解成本构建估计信息集，以加速初始收敛。2. 采用自适应采样器，根据探索过程动态调整采样策略。3. 利用与长度相关的自适应稀疏碰撞检查来指导惰性逆向搜索。", "result": "MIT*在R^4到R^16维度问题中，性能优于现有的单查询、基于采样的规划器，并在受限场景中保持高成功率。它成功应用于实际机器人操作任务，提高了路径成本效率和计算时间。", "conclusion": "MIT*通过其创新的估计信息集、自适应采样和稀疏碰撞检查机制，显著提升了高维和受限场景下机器人路径规划的初始收敛速度、路径成本效率和计算时间，是一个有效且高效的解决方案。"}}
{"id": "2508.21222", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.21222", "abs": "https://arxiv.org/abs/2508.21222", "authors": ["Zhizhong Huang", "Xiaoming Liu"], "title": "Generalizable Object Re-Identification via Visual In-Context Prompting", "comment": "ICCV 2025", "summary": "Current object re-identification (ReID) methods train domain-specific models\n(e.g., for persons or vehicles), which lack generalization and demand costly\nlabeled data for new categories. While self-supervised learning reduces\nannotation needs by learning instance-wise invariance, it struggles to capture\n\\textit{identity-sensitive} features critical for ReID. This paper proposes\nVisual In-Context Prompting~(VICP), a novel framework where models trained on\nseen categories can directly generalize to unseen novel categories using only\n\\textit{in-context examples} as prompts, without requiring parameter\nadaptation. VICP synergizes LLMs and vision foundation models~(VFM): LLMs infer\nsemantic identity rules from few-shot positive/negative pairs through\ntask-specific prompting, which then guides a VFM (\\eg, DINO) to extract\nID-discriminative features via \\textit{dynamic visual prompts}. By aligning\nLLM-derived semantic concepts with the VFM's pre-trained prior, VICP enables\ngeneralization to novel categories, eliminating the need for dataset-specific\nretraining. To support evaluation, we introduce ShopID10K, a dataset of 10K\nobject instances from e-commerce platforms, featuring multi-view images and\ncross-domain testing. Experiments on ShopID10K and diverse ReID benchmarks\ndemonstrate that VICP outperforms baselines by a clear margin on unseen\ncategories. Code is available at https://github.com/Hzzone/VICP.", "AI": {"tldr": "本文提出了视觉上下文提示（VICP）框架，通过结合大型语言模型（LLM）和视觉基础模型（VFM），仅利用上下文示例就能使模型泛化到未见过的目标类别，无需参数适应或重新训练。", "motivation": "现有目标重识别（ReID）方法通常针对特定领域（如行人或车辆），缺乏泛化能力，且在新类别上需要昂贵的标注数据。自监督学习虽然减少了标注需求，但难以捕获ReID所需的身份敏感特征。", "method": "VICP框架通过以下方式实现：1) LLM利用少量正/负样本对，通过任务特定提示推断语义身份规则；2) 这些规则随后引导VFM（如DINO）通过动态视觉提示提取身份判别特征。通过对齐LLM导出的语义概念与VFM的预训练先验，VICP实现了向新类别的泛化。为支持评估，引入了ShopID10K数据集。", "result": "在ShopID10K和多个ReID基准测试上的实验表明，VICP在未见过的类别上明显优于现有基线方法。", "conclusion": "VICP成功地将LLM和VFM结合起来，无需参数适应或特定数据集的再训练，即可使模型直接泛化到新的目标类别，有效解决了现有ReID方法的泛化性差和数据标注成本高的问题。"}}
{"id": "2508.21564", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.21564", "abs": "https://arxiv.org/abs/2508.21564", "authors": ["Issa Hanou", "Sebastijan Dumančić", "Mathijs de Weerdt"], "title": "Revisiting Landmarks: Learning from Previous Plans to Generalize over Problem Instances", "comment": null, "summary": "We propose a new framework for discovering landmarks that automatically\ngeneralize across a domain. These generalized landmarks are learned from a set\nof solved instances and describe intermediate goals for planning problems where\ntraditional landmark extraction algorithms fall short. Our generalized\nlandmarks extend beyond the predicates of a domain by using state functions\nthat are independent of the objects of a specific problem and apply to all\nsimilar objects, thus capturing repetition. Based on these functions, we\nconstruct a directed generalized landmark graph that defines the landmark\nprogression, including loop possibilities for repetitive subplans. We show how\nto use this graph in a heuristic to solve new problem instances of the same\ndomain. Our results show that the generalized landmark graphs learned from a\nfew small instances are also effective for larger instances in the same domain.\nIf a loop that indicates repetition is identified, we see a significant\nimprovement in heuristic performance over the baseline. Generalized landmarks\ncapture domain information that is interpretable and useful to an automated\nplanner. This information can be discovered from a small set of plans for the\nsame domain.", "AI": {"tldr": "本文提出了一种新的泛化地标框架，通过使用独立于对象的通用状态函数来捕获领域内的重复模式，并构建包含循环的有向地标图。该图可作为启发式函数用于规划，显著提升了对大型问题实例的性能，尤其是在存在重复子计划时。", "motivation": "传统的规划地标提取算法在描述规划问题的中间目标时存在不足，尤其未能有效泛化到整个领域并捕获重复模式，导致在复杂规划问题中表现不佳。", "method": "研究者提出泛化地标框架，通过使用独立于特定对象的状态函数来描述地标，从而捕获领域内的重复模式。基于这些函数，构建了一个有向泛化地标图，该图定义了地标的进展，并包含了重复子计划的循环可能性。最后，将该图应用于启发式函数，以解决同领域的新问题实例。", "result": "结果表明，从少量小实例中学习到的泛化地标图对同领域内的大型实例同样有效。当识别出指示重复的循环时，启发式性能相对于基线有显著提升。泛化地标捕获了可解释且对自动化规划器有用的领域信息。", "conclusion": "该泛化地标框架能够从少量规划中学习到可解释且对自动化规划器有用的领域信息。通过有效捕获重复模式，它显著提升了规划器的启发式性能，为解决复杂规划问题提供了有效途径。"}}
{"id": "2508.21422", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.21422", "abs": "https://arxiv.org/abs/2508.21422", "authors": ["Nils Dycke", "Iryna Gurevych"], "title": "Automatic Reviewers Fail to Detect Faulty Reasoning in Research Papers: A New Counterfactual Evaluation Framework", "comment": null, "summary": "Large Language Models (LLMs) have great potential to accelerate and support\nscholarly peer review and are increasingly used as fully automatic review\ngenerators (ARGs). However, potential biases and systematic errors may pose\nsignificant risks to scientific integrity; understanding the specific\ncapabilities and limitations of state-of-the-art ARGs is essential. We focus on\na core reviewing skill that underpins high-quality peer review: detecting\nfaulty research logic. This involves evaluating the internal consistency\nbetween a paper's results, interpretations, and claims. We present a fully\nautomated counterfactual evaluation framework that isolates and tests this\nskill under controlled conditions. Testing a range of ARG approaches, we find\nthat, contrary to expectation, flaws in research logic have no significant\neffect on their output reviews. Based on our findings, we derive three\nactionable recommendations for future work and release our counterfactual\ndataset and evaluation framework publicly.", "AI": {"tldr": "研究发现，大语言模型（LLMs）作为自动审稿生成器（ARGs）在检测论文研究逻辑缺陷（如结果与解释不一致）方面表现不佳，其输出审稿不受逻辑缺陷影响。", "motivation": "LLMs在学术同行评审中潜力巨大，但作为自动审稿生成器（ARGs）可能引入偏见和系统性错误，对科学诚信构成风险。因此，理解当前ARGs在检测高质量同行评审核心技能——研究逻辑缺陷（即结果、解释和主张之间的内部一致性）方面的能力和局限性至关重要。", "method": "提出了一种全自动的反事实评估框架，在受控条件下隔离并测试了ARGs检测研究逻辑缺陷的能力。该框架通过测试一系列ARG方法来评估其性能。", "result": "与预期相反，研究发现论文中研究逻辑缺陷的存在对ARGs生成的审稿输出没有显著影响，表明ARGs未能有效识别这些缺陷。", "conclusion": "当前ARGs无法有效检测研究逻辑缺陷。基于此发现，论文提出了未来工作的三个可操作建议，并公开发布了反事实数据集和评估框架。"}}
{"id": "2508.21592", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.21592", "abs": "https://arxiv.org/abs/2508.21592", "authors": ["Tianchen Sun", "Bingheng Wang", "Longbin Tang", "Yichao Gao", "Lin Zhao"], "title": "Learning Agile Gate Traversal via Analytical Optimal Policy Gradient", "comment": "8 pages, 8 figures", "summary": "Traversing narrow gates presents a significant challenge and has become a\nstandard benchmark for evaluating agile and precise quadrotor flight.\nTraditional modularized autonomous flight stacks require extensive design and\nparameter tuning, while end-to-end reinforcement learning (RL) methods often\nsuffer from low sample efficiency and limited interpretability. In this work,\nwe present a novel hybrid framework that adaptively fine-tunes model predictive\ncontrol (MPC) parameters online using outputs from a neural network (NN)\ntrained offline. The NN jointly predicts a reference pose and cost-function\nweights, conditioned on the coordinates of the gate corners and the current\ndrone state. To achieve efficient training, we derive analytical policy\ngradients not only for the MPC module but also for an optimization-based gate\ntraversal detection module. Furthermore, we introduce a new formulation of the\nattitude tracking error that admits a simplified representation, facilitating\neffective learning with bounded gradients. Hardware experiments demonstrate\nthat our method enables fast and accurate quadrotor traversal through narrow\ngates in confined environments. It achieves several orders of magnitude\nimprovement in sample efficiency compared to naive end-to-end RL approaches.", "AI": {"tldr": "本文提出一种混合框架，通过离线训练的神经网络在线微调模型预测控制（MPC）参数，实现四旋翼飞行器快速精确地穿过狭窄的门，并在样本效率上显著优于端到端强化学习。", "motivation": "窄门穿越是四旋翼飞行器的重要挑战和基准。传统自主飞行堆栈需要大量设计和参数调整；端到端强化学习（RL）方法通常存在样本效率低和可解释性差的问题。", "method": "本研究提出一种混合框架：离线训练的神经网络（NN）根据门角坐标和当前无人机状态，联合预测参考姿态和成本函数权重，在线微调模型预测控制（MPC）参数。为实现高效训练，推导了MPC模块和基于优化的门穿越检测模块的分析策略梯度。此外，引入了一种新的姿态跟踪误差公式，以简化表示并促进带边界梯度的有效学习。", "result": "硬件实验表明，该方法使四旋翼飞行器能够在受限环境中快速准确地穿过狭窄的门。与朴素的端到端强化学习方法相比，样本效率提高了几个数量级。", "conclusion": "所提出的混合框架通过结合神经网络的在线参数调整和模型预测控制，成功解决了四旋翼飞行器窄门穿越的挑战，实现了高效、精确和敏捷的飞行，并在样本效率上表现出色。"}}
{"id": "2508.21227", "categories": ["cs.CV", "68T07, 68U10", "I.4.6; I.5.4; J.3"], "pdf": "https://arxiv.org/pdf/2508.21227", "abs": "https://arxiv.org/abs/2508.21227", "authors": ["Keshav Jha", "William Sharp", "Dominic LaBella"], "title": "Lightweight MRI-Based Automated Segmentation of Pancreatic Cancer with Auto3DSeg", "comment": "11 pages, 3 figures, 3 tables, MICCAI", "summary": "Accurate delineation of pancreatic tumors is critical for diagnosis,\ntreatment planning, and outcome assessment, yet automated segmentation remains\nchallenging due to anatomical variability and limited dataset availability. In\nthis study, SegResNet models, as part of the Auto3DSeg architecture, were\ntrained and evaluated on two MRI-based pancreatic tumor segmentation tasks as\npart of the 2025 PANTHER Challenge. Algorithm methodology included 5-fold\ncross-validation with STAPLE ensembling after focusing on an anatomically\nrelevant region-of-interest. The Pancreatic Tumor Segmentation on Diagnostic\nMRI task 1 training set included 91 T1-weighted arterial contrast-enhanced MRI\nwith expert annotated pancreas and tumor labels. The Pancreatic Tumor\nSegmentation on MR-Linac task 2 training set used 50 T2-weighted MR-Linac cases\nwith expert annotated pancreas and tumor labels. Algorithm-automated\nsegmentation performance of pancreatic tumor was assessed using Dice Similarity\nCoefficient (DSC), 5 mm DSC, 95th percentile Hausdorff Distance (HD95), Mean\nAverage Surface Distance (MASD), and Root Mean Square Error (RMSE). For Task 1,\nthe algorithm achieved a DSC of 0.56, 5 mm DSC of 0.73, HD95 of 41.1 mm, MASD\nof 26.0 mm, and RMSE of 5164 mm. For Task 2, performance decreased, with a DSC\nof 0.33, 5 mm DSC of 0.50, HD95 of 20.1 mm, MASD of 7.2 mm, and RMSE of 17,203\nmm. These findings illustrate the challenges of MRI-based pancreatic tumor\nsegmentation with small datasets, highlighting variability introduced by\ndifferent MRI sequences. Despite modest performance, the results demonstrate\npotential for automated delineation and emphasize the need for larger,\nstandardized MRI datasets to improve model robustness and clinical utility.", "AI": {"tldr": "本研究在2025年PANTHER挑战赛中，使用SegResNet模型对两种MRI序列（诊断性T1CE和MR-Linac T2）上的胰腺肿瘤进行自动分割。结果显示，由于数据集小和MRI序列差异，分割性能一般，但展示了自动化分割的潜力，并强调需要更大的标准化数据集。", "motivation": "胰腺肿瘤的精确描绘对于诊断、治疗计划和预后评估至关重要，但由于解剖变异性和数据集有限，自动分割仍然充满挑战。", "method": "研究采用Auto3DSeg架构中的SegResNet模型进行训练和评估。方法包括5折交叉验证，结合STAPLE集成，并专注于解剖学相关的感兴趣区域。任务1使用91例T1加权动脉期增强MRI，任务2使用50例T2加权MR-Linac MRI。性能评估指标包括Dice相似系数(DSC)、5毫米DSC、95% Hausdorff距离(HD95)、平均表面距离(MASD)和均方根误差(RMSE)。", "result": "任务1（T1CE MRI）的胰腺肿瘤分割表现为：DSC 0.56，5毫米DSC 0.73，HD95 41.1毫米，MASD 26.0毫米，RMSE 5164毫米。任务2（MR-Linac T2 MRI）的性能有所下降，表现为：DSC 0.33，5毫米DSC 0.50，HD95 20.1毫米，MASD 7.2毫米，RMSE 17203毫米。", "conclusion": "研究结果表明，基于MRI的胰腺肿瘤分割在小数据集下具有挑战性，且不同MRI序列引入了变异性。尽管性能一般，但结果展示了自动描绘的潜力，并强调需要更大、更标准化的MRI数据集以提高模型的鲁棒性和临床实用性。"}}
{"id": "2508.21595", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.21595", "abs": "https://arxiv.org/abs/2508.21595", "authors": ["Yang You", "Alex Schutz", "Zhikun Li", "Bruno Lacerda", "Robert Skilton", "Nick Hawes"], "title": "Scalable Solution Methods for Dec-POMDPs with Deterministic Dynamics", "comment": null, "summary": "Many high-level multi-agent planning problems, including multi-robot\nnavigation and path planning, can be effectively modeled using deterministic\nactions and observations.\n  In this work, we focus on such domains and introduce the class of\nDeterministic Decentralized POMDPs (Det-Dec-POMDPs). This is a subclass of\nDec-POMDPs characterized by deterministic transitions and observations\nconditioned on the state and joint actions.\n  We then propose a practical solver called Iterative Deterministic POMDP\nPlanning (IDPP). This method builds on the classic Joint Equilibrium Search for\nPolicies framework and is specifically optimized to handle large-scale\nDet-Dec-POMDPs that current Dec-POMDP solvers are unable to address\nefficiently.", "AI": {"tldr": "本文提出了一种新的确定性多智能体规划模型——确定性去中心化部分可观测马尔可夫决策过程（Det-Dec-POMDP），并为此类大规模问题设计了一个名为IDPP的迭代求解器。", "motivation": "许多高级多智能体规划问题（如多机器人导航）可以用确定性动作和观测来有效建模。然而，现有的去中心化部分可观测马尔可夫决策过程（Dec-POMDP）求解器无法有效处理这类大规模问题。", "method": "作者引入了Det-Dec-POMDP，这是Dec-POMDP的一个子类，其特点是状态和联合动作决定的确定性转移和观测。然后，提出了一种实用的求解器，名为迭代确定性POMDP规划（IDPP），该方法基于经典的联合均衡策略搜索（JES）框架并进行了优化。", "result": "IDPP方法专门优化，能够处理现有Dec-POMDP求解器无法高效解决的大规模Det-Dec-POMDP问题。", "conclusion": "Det-Dec-POMDP模型和IDPP求解器为具有确定性动态和观测的大规模多智能体规划问题提供了一种有效的建模和求解方案。"}}
{"id": "2508.21430", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.21430", "abs": "https://arxiv.org/abs/2508.21430", "authors": ["Meidan Ding", "Jipeng Zhang", "Wenxuan Wang", "Cheng-Yi Li", "Wei-Chieh Fang", "Hsin-Yu Wu", "Haiqin Zhong", "Wenting Chen", "Linlin Shen"], "title": "Med-RewardBench: Benchmarking Reward Models and Judges for Medical Multimodal Large Language Models", "comment": "19 pages, 5 figures, 3 tables", "summary": "Multimodal large language models (MLLMs) hold significant potential in\nmedical applications, including disease diagnosis and clinical decision-making.\nHowever, these tasks require highly accurate, context-sensitive, and\nprofessionally aligned responses, making reliable reward models and judges\ncritical. Despite their importance, medical reward models (MRMs) and judges\nremain underexplored, with no dedicated benchmarks addressing clinical\nrequirements. Existing benchmarks focus on general MLLM capabilities or\nevaluate models as solvers, neglecting essential evaluation dimensions like\ndiagnostic accuracy and clinical relevance. To address this, we introduce\nMed-RewardBench, the first benchmark specifically designed to evaluate MRMs and\njudges in medical scenarios. Med-RewardBench features a multimodal dataset\nspanning 13 organ systems and 8 clinical departments, with 1,026\nexpert-annotated cases. A rigorous three-step process ensures high-quality\nevaluation data across six clinically critical dimensions. We evaluate 32\nstate-of-the-art MLLMs, including open-source, proprietary, and\nmedical-specific models, revealing substantial challenges in aligning outputs\nwith expert judgment. Additionally, we develop baseline models that demonstrate\nsubstantial performance improvements through fine-tuning.", "AI": {"tldr": "Med-RewardBench是首个专门用于评估医学场景下多模态大语言模型(MLLM)奖励模型和判官的基准，旨在解决现有评估方法在诊断准确性和临床相关性方面的不足。", "motivation": "多模态大语言模型在医疗应用中潜力巨大，但需要高度准确、情境敏感和专业对齐的响应。现有的医学奖励模型和判官研究不足，缺乏专门针对临床需求的基准，无法有效评估诊断准确性和临床相关性。", "method": "引入了Med-RewardBench基准，包含一个涵盖13个器官系统和8个临床科室的多模态数据集，共1,026个专家标注病例。采用严格的三步流程，确保在六个关键临床维度上获得高质量评估数据。评估了32个最先进的MLLM，并开发了通过微调显著提高性能的基线模型。", "result": "评估结果揭示了现有MLLM在将输出与专家判断对齐方面存在巨大挑战。通过微调开发的基线模型展示了显著的性能提升。", "conclusion": "Med-RewardBench填补了医学领域奖励模型和判官评估基准的空白，揭示了当前MLLM在医学对齐方面的不足，并为未来模型的改进提供了方向和基线。"}}
{"id": "2508.21677", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.21677", "abs": "https://arxiv.org/abs/2508.21677", "authors": ["Bernhard Wullt", "Johannes Köhler", "Per Mattsson", "Mikeal Norrlöf", "Thomas B. Schön"], "title": "Robust Convex Model Predictive Control with collision avoidance guarantees for robot manipulators", "comment": null, "summary": "Industrial manipulators are normally operated in cluttered environments,\nmaking safe motion planning important. Furthermore, the presence of\nmodel-uncertainties make safe motion planning more difficult. Therefore, in\npractice the speed is limited in order to reduce the effect of disturbances.\nThere is a need for control methods that can guarantee safe motions that can be\nexecuted fast. We address this need by suggesting a novel model predictive\ncontrol (MPC) solution for manipulators, where our two main components are a\nrobust tube MPC and a corridor planning algorithm to obtain collision-free\nmotion. Our solution results in a convex MPC, which we can solve fast, making\nour method practically useful. We demonstrate the efficacy of our method in a\nsimulated environment with a 6 DOF industrial robot operating in cluttered\nenvironments with uncertainties in model parameters. We outperform benchmark\nmethods, both in terms of being able to work under higher levels of model\nuncertainties, while also yielding faster motion.", "AI": {"tldr": "本文提出了一种新颖的基于鲁棒管MPC和走廊规划的凸MPC解决方案，用于工业机械臂在复杂且存在模型不确定性环境中的快速安全运动规划，并在仿真中验证其在更高不确定性下实现更快运动，优于现有方法。", "motivation": "工业机械臂在杂乱且存在模型不确定性的环境中，安全运动规划至关重要。为应对干扰，实际操作中速度常受限。因此，需要一种能保证快速安全运动的控制方法。", "method": "本文提出了一种新颖的模型预测控制（MPC）解决方案，其主要组成部分包括鲁棒管MPC和用于获得无碰撞运动的走廊规划算法。该解决方案形成一个凸MPC问题，可实现快速求解。", "result": "在具有模型参数不确定性的杂乱环境中，通过6自由度工业机器人的模拟验证，该方法能够工作在更高水平的模型不确定性下，并产生更快的运动，优于基准方法。", "conclusion": "所提出的MPC方法能够有效解决工业机械臂在复杂不确定环境下的快速安全运动规划问题，并在鲁棒性和运动速度方面均优于现有方法，具有实际应用价值。"}}
{"id": "2508.21254", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.21254", "abs": "https://arxiv.org/abs/2508.21254", "authors": ["Yidong Zhao", "Peter Kellman", "Hui Xue", "Tongyun Yang", "Yi Zhang", "Yuchi Han", "Orlando Simonetti", "Qian Tao"], "title": "Reverse Imaging for Wide-spectrum Generalization of Cardiac MRI Segmentation", "comment": null, "summary": "Pretrained segmentation models for cardiac magnetic resonance imaging (MRI)\nstruggle to generalize across different imaging sequences due to significant\nvariations in image contrast. These variations arise from changes in imaging\nprotocols, yet the same fundamental spin properties, including proton density,\nT1, and T2 values, govern all acquired images. With this core principle, we\nintroduce Reverse Imaging, a novel physics-driven method for cardiac MRI data\naugmentation and domain adaptation to fundamentally solve the generalization\nproblem. Our method reversely infers the underlying spin properties from\nobserved cardiac MRI images, by solving ill-posed nonlinear inverse problems\nregularized by the prior distribution of spin properties. We acquire this \"spin\nprior\" by learning a generative diffusion model from the multiparametric\nSAturation-recovery single-SHot acquisition sequence (mSASHA) dataset, which\noffers joint cardiac T1 and T2 maps. Our method enables approximate but\nmeaningful spin-property estimates from MR images, which provide an\ninterpretable \"latent variable\" that lead to highly flexible image synthesis of\narbitrary novel sequences. We show that Reverse Imaging enables highly accurate\nsegmentation across vastly different image contrasts and imaging protocols,\nrealizing wide-spectrum generalization of cardiac MRI segmentation.", "AI": {"tldr": "针对心脏MRI分割模型在不同成像序列间泛化能力差的问题，本文提出“逆向成像”方法，通过反向推断图像的底层自旋特性，实现跨对比度和协议的鲁棒分割。", "motivation": "预训练的心脏MRI分割模型由于成像协议导致的图像对比度显著差异，难以在不同成像序列之间泛化。然而，所有图像都受相同的基本自旋特性（质子密度、T1、T2值）支配。", "method": "引入“逆向成像”方法，这是一种物理驱动的数据增强和域适应技术。它通过解决由自旋特性先验分布正则化的病态非线性逆问题，从观察到的心脏MRI图像中反向推断底层自旋特性。自旋先验通过从多参数mSASHA数据集（提供T1和T2图）学习生成扩散模型获得。", "result": "该方法能够从MR图像中获得近似但有意义的自旋特性估计，这些估计提供了可解释的“潜在变量”，从而可以灵活地合成任意新序列图像。结果表明，“逆向成像”在各种不同的图像对比度和成像协议下都能实现高精度分割，实现了心脏MRI分割的广谱泛化。", "conclusion": "“逆向成像”通过利用物理驱动的自旋特性推断，从根本上解决了心脏MRI分割模型的泛化问题，实现了在不同成像序列和协议下的高准确度分割，具有广泛的应用前景。"}}
{"id": "2508.21622", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.21622", "abs": "https://arxiv.org/abs/2508.21622", "authors": ["Saravanan Venkatachalam"], "title": "Integrating Large Language Models with Network Optimization for Interactive and Explainable Supply Chain Planning: A Real-World Case Study", "comment": null, "summary": "This paper presents an integrated framework that combines traditional network\noptimization models with large language models (LLMs) to deliver interactive,\nexplainable, and role-aware decision support for supply chain planning. The\nproposed system bridges the gap between complex operations research outputs and\nbusiness stakeholder understanding by generating natural language summaries,\ncontextual visualizations, and tailored key performance indicators (KPIs). The\ncore optimization model addresses tactical inventory redistribution across a\nnetwork of distribution centers for multi-period and multi-item, using a\nmixed-integer formulation. The technical architecture incorporates AI agents,\nRESTful APIs, and a dynamic user interface to support real-time interaction,\nconfiguration updates, and simulation-based insights. A case study demonstrates\nhow the system improves planning outcomes by preventing stockouts, reducing\ncosts, and maintaining service levels. Future extensions include integrating\nprivate LLMs, transfer learning, reinforcement learning, and Bayesian neural\nnetworks to enhance explainability, adaptability, and real-time\ndecision-making.", "AI": {"tldr": "该论文提出一个集成框架，结合网络优化模型和大型语言模型（LLMs），为供应链规划提供交互式、可解释且角色感知的决策支持。", "motivation": "研究动机是为了弥合复杂运筹学输出与业务利益相关者理解之间的鸿沟，使决策支持系统更易于理解和使用。", "method": "方法包括：1) 一个核心的混合整数规划模型，用于多周期、多物品、跨配送中心的战术库存再分配；2) 利用LLMs生成自然语言摘要、情境化可视化和定制的关键绩效指标（KPIs）；3) 技术架构包含AI代理、RESTful API和动态用户界面，以支持实时交互、配置更新和基于仿真的洞察。", "result": "案例研究表明，该系统通过防止缺货、降低成本和维持服务水平，显著改善了规划结果。", "conclusion": "该集成系统能有效改善供应链规划成果。未来的工作将包括集成私有LLMs、迁移学习、强化学习和贝叶斯神经网络，以增强解释性、适应性和实时决策能力。"}}
{"id": "2508.21436", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.21436", "abs": "https://arxiv.org/abs/2508.21436", "authors": ["Yunhao Zhang", "Shaonan Wang", "Nan Lin", "Xinyi Dong", "Chong Li", "Chengqing Zong"], "title": "Discovering Semantic Subdimensions through Disentangled Conceptual Representations", "comment": null, "summary": "Understanding the core dimensions of conceptual semantics is fundamental to\nuncovering how meaning is organized in language and the brain. Existing\napproaches often rely on predefined semantic dimensions that offer only broad\nrepresentations, overlooking finer conceptual distinctions. This paper proposes\na novel framework to investigate the subdimensions underlying coarse-grained\nsemantic dimensions. Specifically, we introduce a Disentangled Continuous\nSemantic Representation Model (DCSRM) that decomposes word embeddings from\nlarge language models into multiple sub-embeddings, each encoding specific\nsemantic information. Using these sub-embeddings, we identify a set of\ninterpretable semantic subdimensions. To assess their neural plausibility, we\napply voxel-wise encoding models to map these subdimensions to brain\nactivation. Our work offers more fine-grained interpretable semantic\nsubdimensions of conceptual meaning. Further analyses reveal that semantic\ndimensions are structured according to distinct principles, with polarity\nemerging as a key factor driving their decomposition into subdimensions. The\nneural correlates of the identified subdimensions support their cognitive and\nneuroscientific plausibility.", "AI": {"tldr": "本文提出了一种名为DCSRM的新框架，用于从大型语言模型的词嵌入中解耦出更细粒度的语义子维度，并通过脑激活数据验证了其神经合理性，揭示了极性在语义维度分解中的关键作用。", "motivation": "现有的语义维度方法依赖于预定义且宽泛的表示，忽略了概念上的精细区别。研究的动机在于深入理解语言和大脑中意义的组织方式，并发现粗粒度语义维度下的子维度。", "method": "引入了“解耦连续语义表示模型（Disentangled Continuous Semantic Representation Model, DCSRM）”，该模型将词嵌入分解为多个子嵌入，每个子嵌入编码特定的语义信息。利用这些子嵌入识别可解释的语义子维度。通过体素级编码模型将这些子维度映射到大脑激活，以评估其神经合理性。", "result": "研究识别出更细粒度、可解释的语义子维度。分析显示语义维度根据不同原则构建，其中“极性”是驱动其分解为子维度的关键因素。识别出的子维度的神经关联支持了其认知和神经科学的合理性。", "conclusion": "该工作提供了一个研究概念意义更细粒度可解释语义子维度的新框架，并揭示了语义维度结构的不同原则，特别是极性在维度分解中的重要作用，且这些子维度具有神经合理性。"}}
{"id": "2508.21690", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.21690", "abs": "https://arxiv.org/abs/2508.21690", "authors": ["Olger Siebinga", "David Abbink"], "title": "Can a mobile robot learn from a pedestrian model to prevent the sidewalk salsa?", "comment": null, "summary": "Pedestrians approaching each other on a sidewalk sometimes end up in an\nawkward interaction known as the \"sidewalk salsa\": they both (repeatedly)\ndeviate to the same side to avoid a collision. This provides an interesting use\ncase to study interactions between pedestrians and mobile robots because, in\nthe vast majority of cases, this phenomenon is avoided through a negotiation\nbased on implicit communication. Understanding how it goes wrong and how\npedestrians end up in the sidewalk salsa will therefore provide insight into\nthe implicit communication. This understanding can be used to design safe and\nacceptable robotic behaviour. In a previous attempt to gain this understanding,\na model of pedestrian behaviour based on the Communication-Enabled Interaction\n(CEI) framework was developed that can replicate the sidewalk salsa. However,\nit is unclear how to leverage this model in robotic planning and\ndecision-making since it violates the assumptions of game theory, a much-used\nframework in planning and decision-making. Here, we present a proof-of-concept\nfor an approach where a Reinforcement Learning (RL) agent leverages the model\nto learn how to interact with pedestrians. The results show that a basic RL\nagent successfully learned to interact with the CEI model. Furthermore, a\nrisk-averse RL agent that had access to the perceived risk of the CEI model\nlearned how to effectively communicate its intention through its motion and\nthereby substantially lowered the perceived risk, and displayed effort by the\nmodelled pedestrian. These results show this is a promising approach and\nencourage further exploration.", "AI": {"tldr": "本研究利用强化学习（RL）与一个能模拟“人行道萨尔萨”现象的通信使能交互（CEI）模型，训练机器人学习如何与行人互动，以避免尴尬的碰撞，并展示了风险规避型RL代理在有效沟通意图方面的成功。", "motivation": "行人间的“人行道萨尔萨”现象（双方重复向同一侧避让）是隐性沟通失败的有趣案例。理解这种失败能揭示隐性沟通的机制，从而设计出安全可接受的机器人行为。现有的CEI模型能重现此现象，但因违反博弈论假设，无法直接用于机器人规划，因此需要一种新方法来利用该模型。", "method": "提出了一种概念验证方法，其中一个强化学习（RL）代理利用一个先前开发的、能够重现“人行道萨尔萨”现象的通信使能交互（CEI）模型来学习如何与行人互动。测试了两种RL代理：一个基础RL代理和一个能感知CEI模型感知风险的风险规避型RL代理。", "result": "基础RL代理成功学会了与CEI模型互动。风险规避型RL代理通过其运动学会了有效沟通意图，从而显著降低了CEI模型感知的风险，并显示出模型化行人所付出的努力。", "conclusion": "利用强化学习代理结合CEI模型来学习与行人互动是一个有前景的方法。该方法能够帮助机器人有效沟通意图，降低互动风险，并避免“人行道萨尔萨”等问题，鼓励进一步探索。"}}
{"id": "2508.21257", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.21257", "abs": "https://arxiv.org/abs/2508.21257", "authors": ["Hsuan-I Ho", "Chen Guo", "Po-Chen Wu", "Ivan Shugurov", "Chengcheng Tang", "Abhay Mittal", "Sizhe An", "Manuel Kaufmann", "Linguang Zhang"], "title": "PHD: Personalized 3D Human Body Fitting with Point Diffusion", "comment": "ICCV 2025, 19 pages, 18 figures", "summary": "We introduce PHD, a novel approach for personalized 3D human mesh recovery\n(HMR) and body fitting that leverages user-specific shape information to\nimprove pose estimation accuracy from videos. Traditional HMR methods are\ndesigned to be user-agnostic and optimized for generalization. While these\nmethods often refine poses using constraints derived from the 2D image to\nimprove alignment, this process compromises 3D accuracy by failing to jointly\naccount for person-specific body shapes and the plausibility of 3D poses. In\ncontrast, our pipeline decouples this process by first calibrating the user's\nbody shape and then employing a personalized pose fitting process conditioned\non that shape. To achieve this, we develop a body shape-conditioned 3D pose\nprior, implemented as a Point Diffusion Transformer, which iteratively guides\nthe pose fitting via a Point Distillation Sampling loss. This learned 3D pose\nprior effectively mitigates errors arising from an over-reliance on 2D\nconstraints. Consequently, our approach improves not only pelvis-aligned pose\naccuracy but also absolute pose accuracy -- an important metric often\noverlooked by prior work. Furthermore, our method is highly data-efficient,\nrequiring only synthetic data for training, and serves as a versatile\nplug-and-play module that can be seamlessly integrated with existing 3D pose\nestimators to enhance their performance. Project page:\nhttps://phd-pose.github.io/", "AI": {"tldr": "PHD是一种新颖的个性化3D人体网格恢复方法，它利用用户特定的身体形状和形状条件3D姿态先验来提高从视频中估计姿态的准确性，解决了传统方法过度依赖2D约束导致3D精度不足的问题。", "motivation": "传统的3D人体网格恢复(HMR)方法是用户无关的，旨在泛化，但通过过度依赖2D图像约束来细化姿态，导致在未能同时考虑个体身体形状和3D姿态合理性时，牺牲了3D精度。", "method": "PHD方法首先校准用户的身体形状，然后基于该形状进行个性化的姿态拟合。它开发了一个身体形状条件的3D姿态先验，该先验被实现为一个点扩散Transformer，通过点蒸馏采样损失迭代地指导姿态拟合。该方法仅需合成数据进行训练，并且可以作为即插即用模块集成到现有3D姿态估计器中。", "result": "PHD有效缓解了过度依赖2D约束导致的误差，不仅提高了骨盆对齐的姿态精度，还显著提高了通常被忽视的绝对姿态精度。此外，该方法数据效率高，且作为一个通用的即插即用模块，可以无缝集成以增强现有3D姿态估计器的性能。", "conclusion": "PHD通过引入用户特定的形状信息和形状条件3D姿态先验，为个性化3D人体网格恢复提供了一种新颖且高效的方法，显著改善了3D姿态估计的准确性，特别是在绝对姿态精度方面，并解决了传统方法的局限性。"}}
{"id": "2508.21637", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.21637", "abs": "https://arxiv.org/abs/2508.21637", "authors": ["Ramkumar Natarajan", "Muhammad Suhail Saleem", "William Xiao", "Sandip Aine", "Howie Choset", "Maxim Likhachev"], "title": "A-MHA*: Anytime Multi-Heuristic A*", "comment": null, "summary": "Designing good heuristic functions for graph search requires adequate domain\nknowledge. It is often easy to design heuristics that perform well and\ncorrelate with the underlying true cost-to-go values in certain parts of the\nsearch space but these may not be admissible throughout the domain thereby\naffecting the optimality guarantees of the search. Bounded suboptimal search\nusing several such partially good but inadmissible heuristics was developed in\nMulti-Heuristic A* (MHA*). Although MHA* leverages multiple inadmissible\nheuristics to potentially generate a faster suboptimal solution, the original\nversion does not improve the solution over time. It is a one shot algorithm\nthat requires careful setting of inflation factors to obtain a desired one time\nsolution. In this work, we tackle this issue by extending MHA* to an anytime\nversion that finds a feasible suboptimal solution quickly and continually\nimproves it until time runs out. Our work is inspired from the Anytime\nRepairing A* (ARA*) algorithm. We prove that our precise adaptation of ARA*\nconcepts in the MHA* framework preserves the original suboptimal and\ncompleteness guarantees and enhances MHA* to perform in an anytime fashion.\nFurthermore, we report the performance of A-MHA* in 3-D path planning domain\nand sliding tiles puzzle and compare against MHA* and other anytime algorithms.", "AI": {"tldr": "本文提出了一种名为A-MHA*的算法，它将多启发式A* (MHA*) 扩展为一种随时（anytime）算法，使其能够快速找到次优解并持续改进，同时保留了MHA*的理论保证。", "motivation": "图搜索中设计良好的启发式函数需要领域知识，且启发式函数可能在某些区域表现良好但不全局可采纳。MHA*利用多个不可采纳的启发式函数来加速次优解的生成，但其原始版本是一次性算法，无法随时间推移改进解决方案。", "method": "通过将Anytime Repairing A* (ARA*) 算法的概念精确地应用于MHA*框架，将MHA*扩展为随时版本，命名为A-MHA*。", "result": "A-MHA*在MHA*框架中保留了原始的次优性和完备性保证，并增强了MHA*以随时方式执行。在3D路径规划和滑动拼图领域，A-MHA*与MHA*及其他随时算法进行了性能比较。", "conclusion": "A-MHA*成功地将MHA*扩展为一种随时算法，使其能够快速找到可行次优解并持续改进，同时保持了原有的理论保证，并在实验中展现了其性能。"}}
{"id": "2508.21448", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.21448", "abs": "https://arxiv.org/abs/2508.21448", "authors": ["Shariar Kabir", "Kevin Esterling", "Yue Dong"], "title": "Beyond the Surface: Probing the Ideological Depth of Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated pronounced ideological\nleanings, yet the stability and depth of these positions remain poorly\nunderstood. Surface-level responses can often be manipulated through simple\nprompt engineering, calling into question whether they reflect a coherent\nunderlying ideology. This paper investigates the concept of \"ideological depth\"\nin LLMs, defined as the robustness and complexity of their internal political\nrepresentations. We employ a dual approach: first, we measure the\n\"steerability\" of two well-known open-source LLMs using instruction prompting\nand activation steering. We find that while some models can easily switch\nbetween liberal and conservative viewpoints, others exhibit resistance or an\nincreased rate of refusal, suggesting a more entrenched ideological structure.\nSecond, we probe the internal mechanisms of these models using Sparse\nAutoencoders (SAEs). Preliminary analysis reveals that models with lower\nsteerability possess more distinct and abstract ideological features. Our\nevaluations reveal that one model can contain 7.3x more political features than\nanother model of similar size. This allows targeted ablation of a core\npolitical feature in an ideologically \"deep\" model, leading to consistent,\nlogical shifts in its reasoning across related topics, whereas the same\nintervention in a \"shallow\" model results in an increase in refusal outputs.\nOur findings suggest that ideological depth is a quantifiable property of LLMs\nand that steerability serves as a valuable window into their latent political\narchitecture.", "AI": {"tldr": "本文研究大型语言模型（LLMs）的“意识形态深度”，发现其是可量化的属性，与内部政治表征的鲁棒性和复杂性相关，并可通过可操纵性和内部特征分析进行衡量。", "motivation": "LLMs表现出明显的意识形态倾向，但这些立场的稳定性和深度尚不清楚。表面响应容易通过提示工程操纵，这引发了对其是否反映连贯底层意识形态的质疑。因此，有必要深入理解LLMs的“意识形态深度”。", "method": "本文定义“意识形态深度”为LLMs内部政治表征的鲁棒性和复杂性。研究采用双重方法：1. 测量两个开源LLMs的“可操纵性”，通过指令提示和激活引导实现。2. 使用稀疏自编码器（SAEs）探测模型的内部机制。", "result": "研究发现，一些模型易于在自由派和保守派观点之间切换，而另一些则表现出抵抗或拒绝率增加，表明其意识形态结构更为根深蒂固。初步SAE分析显示，可操纵性较低的模型拥有更独特和抽象的意识形态特征，一个模型比同等大小的另一个模型多7.3倍的政治特征。对“深层”模型的核心政治特征进行靶向消融，可导致其推理在相关主题上发生一致、逻辑性的转变；而对“浅层”模型进行相同干预，则导致拒绝输出增加。", "conclusion": "研究结果表明，意识形态深度是LLMs的一个可量化属性，并且可操纵性是理解其潜在政治架构的宝贵窗口。"}}
{"id": "2508.21542", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.21542", "abs": "https://arxiv.org/abs/2508.21542", "authors": ["Ziwei Liao", "Mohamed Sayed", "Steven L. Waslander", "Sara Vicente", "Daniyar Turmukhambetov", "Michael Firman"], "title": "Complete Gaussian Splats from a Single Image with Denoising Diffusion Models", "comment": "Main paper: 11 pages; Supplementary materials: 7 pages", "summary": "Gaussian splatting typically requires dense observations of the scene and can\nfail to reconstruct occluded and unobserved areas. We propose a latent\ndiffusion model to reconstruct a complete 3D scene with Gaussian splats,\nincluding the occluded parts, from only a single image during inference.\nCompleting the unobserved surfaces of a scene is challenging due to the\nambiguity of the plausible surfaces. Conventional methods use a\nregression-based formulation to predict a single \"mode\" for occluded and\nout-of-frustum surfaces, leading to blurriness, implausibility, and failure to\ncapture multiple possible explanations. Thus, they often address this problem\npartially, focusing either on objects isolated from the background,\nreconstructing only visible surfaces, or failing to extrapolate far from the\ninput views. In contrast, we propose a generative formulation to learn a\ndistribution of 3D representations of Gaussian splats conditioned on a single\ninput image. To address the lack of ground-truth training data, we propose a\nVariational AutoReconstructor to learn a latent space only from 2D images in a\nself-supervised manner, over which a diffusion model is trained. Our method\ngenerates faithful reconstructions and diverse samples with the ability to\ncomplete the occluded surfaces for high-quality 360-degree renderings.", "AI": {"tldr": "该论文提出了一种基于潜在扩散模型的方法，仅通过单张图像即可重建包含遮挡区域的完整3D高斯场场景，解决了传统高斯场重建对密集观测的需求。", "motivation": "传统高斯场重建需要密集的场景观测，并且难以重建遮挡和未观测区域。现有方法通常采用回归方式预测单一“模式”，导致模糊、不真实，且无法捕捉多种可能的解释，限制了其在处理孤立物体、仅重建可见表面或远距离外推方面的能力。", "method": "本文提出了一种生成式方法，学习以单张输入图像为条件的高斯场3D表示分布。为解决缺乏真实训练数据的问题，作者引入了一个变分自重建器（Variational AutoReconstructor），以自监督方式仅从2D图像中学习一个潜在空间，并在此潜在空间上训练扩散模型。", "result": "该方法能够生成忠实的重建结果和多样化的样本，并具备完成遮挡表面以实现高质量360度渲染的能力。", "conclusion": "通过结合潜在扩散模型和自监督学习的变分自重建器，该研究成功实现了从单张图像重建包含遮挡区域的完整、高质量3D高斯场场景，克服了传统方法的局限性，并能生成多样化的场景解释。"}}
{"id": "2508.21363", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.21363", "abs": "https://arxiv.org/abs/2508.21363", "authors": ["Yuquan Bi", "Hongsong Wang", "Xinli Shi", "Zhipeng Gui", "Jie Gui", "Yuan Yan Tang"], "title": "Efficient Diffusion-Based 3D Human Pose Estimation with Hierarchical Temporal Pruning", "comment": null, "summary": "Diffusion models have demonstrated strong capabilities in generating\nhigh-fidelity 3D human poses, yet their iterative nature and multi-hypothesis\nrequirements incur substantial computational cost. In this paper, we propose an\nEfficient Diffusion-Based 3D Human Pose Estimation framework with a\nHierarchical Temporal Pruning (HTP) strategy, which dynamically prunes\nredundant pose tokens across both frame and semantic levels while preserving\ncritical motion dynamics. HTP operates in a staged, top-down manner: (1)\nTemporal Correlation-Enhanced Pruning (TCEP) identifies essential frames by\nanalyzing inter-frame motion correlations through adaptive temporal graph\nconstruction; (2) Sparse-Focused Temporal MHSA (SFT MHSA) leverages the\nresulting frame-level sparsity to reduce attention computation, focusing on\nmotion-relevant tokens; and (3) Mask-Guided Pose Token Pruner (MGPTP) performs\nfine-grained semantic pruning via clustering, retaining only the most\ninformative pose tokens. Experiments on Human3.6M and MPI-INF-3DHP show that\nHTP reduces training MACs by 38.5\\%, inference MACs by 56.8\\%, and improves\ninference speed by an average of 81.1\\% compared to prior diffusion-based\nmethods, while achieving state-of-the-art performance.", "AI": {"tldr": "本文提出了一种名为分层时间剪枝（HTP）的高效扩散模型框架，用于3D人体姿态估计，通过在帧和语义级别剪枝冗余姿态token，显著降低了计算成本，同时保持了最先进的性能。", "motivation": "扩散模型在生成高保真3D人体姿态方面表现出色，但其迭代性质和多假设要求导致了巨大的计算成本。", "method": "该研究提出了一个带有分层时间剪枝（HTP）策略的高效扩散模型框架。HTP以分阶段、自上而下的方式运行：1) 帧间相关增强剪枝（TCEP）通过自适应时间图构建分析帧间运动相关性，识别关键帧；2) 稀疏聚焦时间MHSA（SFT MHSA）利用由此产生的帧级稀疏性减少注意力计算，专注于运动相关token；3) 掩码引导姿态token剪枝器（MGPTP）通过聚类进行细粒度语义剪枝，仅保留信息量最大的姿态token。", "result": "在Human3.6M和MPI-INF-3DHP数据集上的实验表明，与先前的基于扩散的方法相比，HTP将训练MACs减少了38.5%，推理MACs减少了56.8%，推理速度平均提高了81.1%，同时实现了最先进的性能。", "conclusion": "所提出的HTP框架能够显著提高扩散模型在3D人体姿态估计中的计算效率和推理速度，同时保持或超越了最先进的性能，解决了扩散模型计算成本高昂的问题。"}}
{"id": "2508.21648", "categories": ["cs.AI", "68T07, 68T09, 68T20 (Primary) 62P10, 62C20, 62H30 (Secondary)"], "pdf": "https://arxiv.org/pdf/2508.21648", "abs": "https://arxiv.org/abs/2508.21648", "authors": ["Farhad Abtahi", "Mehdi Astaraki", "Fernando Seoane"], "title": "Leveraging Imperfection with MEDLEY A Multi-Model Approach Harnessing Bias in Medical AI", "comment": null, "summary": "Bias in medical artificial intelligence is conventionally viewed as a defect\nrequiring elimination. However, human reasoning inherently incorporates biases\nshaped by education, culture, and experience, suggesting their presence may be\ninevitable and potentially valuable. We propose MEDLEY (Medical Ensemble\nDiagnostic system with Leveraged diversitY), a conceptual framework that\norchestrates multiple AI models while preserving their diverse outputs rather\nthan collapsing them into a consensus. Unlike traditional approaches that\nsuppress disagreement, MEDLEY documents model-specific biases as potential\nstrengths and treats hallucinations as provisional hypotheses for clinician\nverification. A proof-of-concept demonstrator was developed using over 30 large\nlanguage models, creating a minimum viable product that preserved both\nconsensus and minority views in synthetic cases, making diagnostic uncertainty\nand latent biases transparent for clinical oversight. While not yet a validated\nclinical tool, the demonstration illustrates how structured diversity can\nenhance medical reasoning under clinician supervision. By reframing AI\nimperfection as a resource, MEDLEY offers a paradigm shift that opens new\nregulatory, ethical, and innovation pathways for developing trustworthy medical\nAI systems.", "AI": {"tldr": "MEDLEY是一个概念框架，旨在通过整合多个AI模型并保留其多样化输出（包括偏见和幻觉），而非消除它们，来增强医疗诊断，从而将AI的“缺陷”转化为临床监督下的资源。", "motivation": "传统上，医学AI中的偏见被视为必须消除的缺陷。然而，人类推理本身就包含受教育、文化和经验影响的偏见，这表明AI中偏见的存在可能是不可避免且潜在有价值的。", "method": "提出了MEDLEY（利用多样性的医疗集成诊断系统）概念框架。该框架协调多个AI模型，保留其多样化输出而非统一共识，将模型特定偏见视为潜在优势，并将幻觉视为待临床医生验证的临时假设。开发了一个概念验证演示器，使用了30多个大型语言模型，创建了一个最小可行产品。", "result": "该最小可行产品在合成案例中保留了共识和少数观点，使诊断不确定性和潜在偏见对临床监督透明化。尽管尚未经过临床验证，但演示表明结构化的多样性可以在临床医生监督下增强医疗推理。", "conclusion": "MEDLEY通过将AI的不完善之处重新定义为一种资源，提供了一种范式转变，为开发可信赖的医学AI系统开辟了新的监管、伦理和创新途径。结构化的多样性能够在临床医生监督下增强医疗推理。"}}
{"id": "2508.21476", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.21476", "abs": "https://arxiv.org/abs/2508.21476", "authors": ["Xiaolong Wei", "Bo Lu", "Xingyu Zhang", "Zhejun Zhao", "Dongdong Shen", "Long Xia", "Dawei Yin"], "title": "Igniting Creative Writing in Small Language Models: LLM-as-a-Judge versus Multi-Agent Refined Rewards", "comment": "EMNLP 2025 Main", "summary": "Large Language Models (LLMs) have demonstrated remarkable creative writing\ncapabilities, yet their substantial computational demands hinder widespread\nuse. Enhancing Small Language Models (SLMs) offers a promising alternative, but\ncurrent methods like Supervised Fine-Tuning (SFT) struggle with novelty, and\nReinforcement Learning from Human Feedback (RLHF) is costly. This paper\nexplores two distinct AI-driven reward strategies within a Reinforcement\nLearning from AI Feedback (RLAIF) framework to ignite the creative writing of a\n7B-parameter SLM, specifically for generating Chinese greetings. The first\nstrategy employs a RM trained on high-quality preference data curated by a\nnovel multi-agent rejection sampling framework designed for creative tasks. The\nsecond, more novel strategy utilizes a principle-guided LLM-as-a-Judge, whose\nreward function is optimized via an adversarial training scheme with a\nreflection mechanism, to directly provide reward signals. Comprehensive\nexperiments reveal that while both approaches significantly enhance creative\noutput over baselines, the principle-guided LLM-as-a-Judge demonstrably yields\nsuperior generation quality. Furthermore, it offers notable advantages in\ntraining efficiency and reduced dependency on human-annotated data, presenting\na more scalable and effective path towards creative SLMs. Our automated\nevaluation methods also exhibit strong alignment with human judgments. Our code\nand data are publicly available at\nhttps://github.com/weixiaolong94-hub/Igniting-Creative-Writing-in-Small-Language-Models.", "AI": {"tldr": "本研究通过RLAIF框架，探索了两种AI驱动的奖励策略，以提升小型语言模型（SLM）在生成中文问候语方面的创造性，发现基于原则引导的LLM-as-a-Judge方法效果最佳，且效率更高。", "motivation": "大型语言模型（LLMs）的创意写作能力虽强，但计算成本高昂。小型语言模型（SLMs）是替代方案，但现有方法如SFT缺乏新颖性，RLHF成本过高。因此，需要更有效且低成本的方法来激发SLMs的创造力。", "method": "本研究在RLAIF框架下，为7B参数的SLM（用于生成中文问候语）探索了两种AI驱动的奖励策略：1. 使用基于多智能体拒绝采样框架生成的高质量偏好数据训练的奖励模型（RM）。2. 一种更具创新性的策略，利用基于原则引导的LLM-as-a-Judge，其奖励函数通过带有反思机制的对抗性训练方案进行优化，直接提供奖励信号。", "result": "实验结果表明，两种方法都显著提升了基线模型的创意输出。其中，基于原则引导的LLM-as-a-Judge方法在生成质量上表现更优，并在训练效率和对人工标注数据的依赖性方面具有显著优势。此外，其自动化评估方法与人类判断高度一致。", "conclusion": "基于原则引导的LLM-as-a-Judge方法为实现具有创造力的小型语言模型提供了一条更具可扩展性和有效性的途径，它不仅能产出更高质量的内容，还能提高训练效率并减少对人工标注数据的需求。"}}
{"id": "2508.21800", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.21800", "abs": "https://arxiv.org/abs/2508.21800", "authors": ["Hyeonseong Jeon", "Cheolhong Min", "Jaesik Park"], "title": "Tree-Guided Diffusion Planner", "comment": "20 pages, 11 figures, 14 tables (main paper + appendix) / under\n  review / project page will be available after the paper becomes public in\n  arxiv", "summary": "Planning with pretrained diffusion models has emerged as a promising approach\nfor solving test-time guided control problems. However, standard gradient\nguidance typically performs optimally under convex and differentiable reward\nlandscapes, showing substantially reduced effectiveness in real-world scenarios\ninvolving non-convex objectives, non-differentiable constraints, and\nmulti-reward structures. Furthermore, recent supervised planning approaches\nrequire task-specific training or value estimators, which limits test-time\nflexibility and zero-shot generalization. We propose a Tree-guided Diffusion\nPlanner (TDP), a zero-shot test-time planning framework that balances\nexploration and exploitation through structured trajectory generation. We frame\ntest-time planning as a tree search problem using a bi-level sampling process:\n(1) diverse parent trajectories are produced via training-free particle\nguidance to encourage broad exploration, and (2) sub-trajectories are refined\nthrough fast conditional denoising guided by task objectives. TDP addresses the\nlimitations of gradient guidance by exploring diverse trajectory regions and\nharnessing gradient information across this expanded solution space using only\npretrained models and test-time reward signals. We evaluate TDP on three\ndiverse tasks: maze gold-picking, robot arm block manipulation, and AntMaze\nmulti-goal exploration. TDP consistently outperforms state-of-the-art\napproaches on all tasks. The project page can be found at:\ntree-diffusion-planner.github.io.", "AI": {"tldr": "本文提出了一种名为TDP（Tree-guided Diffusion Planner）的零样本测试时规划框架，通过树形搜索和双层采样，利用预训练扩散模型解决非凸、不可微和多奖励等复杂控制问题，并在多个任务上超越了现有SOTA方法。", "motivation": "现有的基于梯度引导的扩散模型规划方法在非凸、不可微奖励函数或多奖励结构等真实世界场景中效果不佳。此外，近期监督规划方法需要任务特定训练或价值估计器，限制了测试时灵活性和零样本泛化能力。", "method": "TDP将测试时规划构建为一个树形搜索问题，采用双层采样过程：1) 通过无训练的粒子引导生成多样化的父轨迹以鼓励广泛探索；2) 通过由任务目标引导的快速条件去噪来细化子轨迹。该方法仅使用预训练模型和测试时奖励信号，平衡了探索与利用。", "result": "TDP在三个不同任务（迷宫捡金币、机械臂积木操作和AntMaze多目标探索）上进行了评估，结果显示它始终优于所有任务上的现有SOTA方法。", "conclusion": "TDP通过探索多样化的轨迹区域并利用扩展解空间中的梯度信息，有效解决了梯度引导在复杂规划场景中的局限性，实现了卓越的零样本性能。"}}
{"id": "2508.21371", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.21371", "abs": "https://arxiv.org/abs/2508.21371", "authors": ["Qingran Miao", "Haixia Wang", "Haohao Sun", "Yilong Zhang"], "title": "Print2Volume: Generating Synthetic OCT-based 3D Fingerprint Volume from 2D Fingerprint Image", "comment": null, "summary": "Optical Coherence Tomography (OCT) enables the acquisition of\nhigh-resolution, three-dimensional fingerprint data, capturing rich subsurface\nstructures for robust biometric recognition. However, the high cost and\ntime-consuming nature of OCT data acquisition have led to a scarcity of\nlarge-scale public datasets, significantly hindering the development of\nadvanced algorithms, particularly data-hungry deep learning models. To address\nthis critical bottleneck, this paper introduces Print2Volume, a novel framework\nfor generating realistic, synthetic OCT-based 3D fingerprints from 2D\nfingerprint image. Our framework operates in three sequential stages: (1) a 2D\nstyle transfer module that converts a binary fingerprint into a grayscale\nimages mimicking the style of a Z-direction mean-projected OCT scan; (2) a 3D\nStructure Expansion Network that extrapolates the 2D im-age into a plausible 3D\nanatomical volume; and (3) an OCT Realism Refiner, based on a 3D GAN, that\nrenders the structural volume with authentic textures, speckle noise, and other\nimaging characteristics. Using Print2Volume, we generated a large-scale\nsynthetic dataset of 420,000 samples. Quantitative experiments demonstrate the\nhigh quality of our synthetic data and its significant impact on recognition\nperformance. By pre-training a recognition model on our synthetic data and\nfine-tuning it on a small real-world dataset, we achieved a remarkable\nreduction in the Equal Error Rate (EER) from 15.62% to 2.50% on the ZJUT-EIFD\nbenchmark, proving the effectiveness of our approach in overcoming data\nscarcity.", "AI": {"tldr": "该论文提出Print2Volume框架，通过三阶段方法从2D指纹图像生成逼真的合成3D OCT指纹数据，以解决真实OCT数据稀缺问题，并显著提升了指纹识别模型的性能。", "motivation": "高分辨率的3D OCT指纹数据能增强生物识别鲁棒性，但其采集成本高、耗时长，导致缺乏大规模公共数据集，严重阻碍了深度学习等先进算法的发展。", "method": "该研究引入了Print2Volume框架，包含三个顺序阶段：1) 2D风格迁移模块，将二值指纹转换为模拟OCT扫描Z轴平均投影的灰度图像；2) 3D结构扩展网络，将2D图像外推为合理的3D解剖体积；3) 基于3D GAN的OCT真实感细化器，为结构体积渲染真实的纹理、散斑噪声及其他成像特征。", "result": "使用Print2Volume生成了包含420,000个样本的大规模合成数据集。定量实验表明，合成数据质量高，对识别性能有显著影响。通过在合成数据上预训练识别模型，并在小型真实世界数据集上进行微调，在ZJUT-EIFD基准测试上，等错误率（EER）从15.62%显著降低至2.50%。", "conclusion": "该方法有效克服了数据稀缺性，显著提升了生物识别模型的性能，证明了其在解决OCT指纹数据不足方面的有效性。"}}
{"id": "2508.21720", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.21720", "abs": "https://arxiv.org/abs/2508.21720", "authors": ["Jiho Choi", "Seojeong Park", "Seongjong Song", "Hyunjung Shim"], "title": "PosterForest: Hierarchical Multi-Agent Collaboration for Scientific Poster Generation", "comment": null, "summary": "We present a novel training-free framework, \\textit{PosterForest}, for\nautomated scientific poster generation. Unlike prior approaches, which largely\nneglect the hierarchical structure of scientific documents and the semantic\nintegration of textual and visual elements, our method addresses both\nchallenges directly. We introduce the \\textit{Poster Tree}, a hierarchical\nintermediate representation that jointly encodes document structure and\nvisual-textual relationships at multiple levels. Our framework employs a\nmulti-agent collaboration strategy, where agents specializing in content\nsummarization and layout planning iteratively coordinate and provide mutual\nfeedback. This approach enables the joint optimization of logical consistency,\ncontent fidelity, and visual coherence. Extensive experiments on multiple\nacademic domains show that our method outperforms existing baselines in both\nqualitative and quantitative evaluations. The resulting posters achieve quality\nclosest to expert-designed ground truth and deliver superior information\npreservation, structural clarity, and user preference.", "AI": {"tldr": "本文提出了一个名为PosterForest的无需训练的自动化科学海报生成框架。该框架通过引入分层中间表示“Poster Tree”和多智能体协作策略，有效解决了科学文档的层级结构和图文语义整合问题，生成的海报质量接近专家水平。", "motivation": "以往的自动化海报生成方法大多忽略了科学文档的层级结构以及文本与视觉元素的语义整合，导致生成的海报在逻辑一致性、内容忠实度和视觉连贯性方面表现不佳。", "method": "该方法引入了“Poster Tree”作为分层的中间表示，共同编码文档结构和多层次的视觉-文本关系。框架采用多智能体协作策略，其中内容摘要和布局规划智能体迭代协调并相互提供反馈，从而联合优化逻辑一致性、内容忠实度和视觉连贯性。", "result": "在多个学术领域的广泛实验表明，PosterForest在定性和定量评估中均优于现有基线。生成的海报质量最接近专家设计，并展现出卓越的信息保留、结构清晰度和用户偏好。", "conclusion": "PosterForest框架通过其创新的分层表示和多智能体协作机制，成功克服了自动化科学海报生成中的核心挑战，能够生成高质量、结构清晰且用户满意度高的海报，其效果与专家设计的海报最为接近。"}}
{"id": "2508.21482", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.21482", "abs": "https://arxiv.org/abs/2508.21482", "authors": ["Sara B. Coutinho", "Rafael M. O. Cruz", "Francimaria R. S. Nascimento", "George D. C. Cavalcanti"], "title": "HSFN: Hierarchical Selection for Fake News Detection building Heterogeneous Ensemble", "comment": "Accepted by IEEE International Conference on Systems, Man, and\n  Cybernetics (SMC) - IEEE SMC 2025", "summary": "Psychological biases, such as confirmation bias, make individuals\nparticularly vulnerable to believing and spreading fake news on social media,\nleading to significant consequences in domains such as public health and\npolitics. Machine learning-based fact-checking systems have been widely studied\nto mitigate this problem. Among them, ensemble methods are particularly\neffective in combining multiple classifiers to improve robustness. However,\ntheir performance heavily depends on the diversity of the constituent\nclassifiers-selecting genuinely diverse models remains a key challenge,\nespecially when models tend to learn redundant patterns. In this work, we\npropose a novel automatic classifier selection approach that prioritizes\ndiversity, also extended by performance. The method first computes pairwise\ndiversity between classifiers and applies hierarchical clustering to organize\nthem into groups at different levels of granularity. A HierarchySelect then\nexplores these hierarchical levels to select one pool of classifiers per level,\neach representing a distinct intra-pool diversity. The most diverse pool is\nidentified and selected for ensemble construction from these. The selection\nprocess incorporates an evaluation metric reflecting each classifiers's\nperformance to ensure the ensemble also generalises well. We conduct\nexperiments with 40 heterogeneous classifiers across six datasets from\ndifferent application domains and with varying numbers of classes. Our method\nis compared against the Elbow heuristic and state-of-the-art baselines. Results\nshow that our approach achieves the highest accuracy on two of six datasets.\nThe implementation details are available on the project's repository:\nhttps://github.com/SaraBCoutinho/HSFN .", "AI": {"tldr": "本文提出了一种名为HierarchySelect的新型自动分类器选择方法，旨在通过优先考虑多样性和性能来改进集成学习系统，以有效应对假新闻检测中的挑战。", "motivation": "心理偏见导致假新闻在社交媒体上广泛传播，对公共卫生和政治产生严重影响。机器学习事实核查系统，尤其是集成方法，在缓解此问题上表现出色，但其性能高度依赖于组成分类器的多样性。选择真正多样化的模型是一个关键挑战，尤其当模型倾向于学习冗余模式时。", "method": "该方法首先计算分类器之间的成对多样性，并应用层次聚类将它们组织成不同粒度级别的组。然后，一个名为HierarchySelect的机制探索这些层次级别，为每个级别选择一个分类器池，每个池代表不同的池内多样性。从中识别并选择最具多样性的池用于集成构建。选择过程还纳入了反映每个分类器性能的评估指标，以确保集成具有良好的泛化能力。", "result": "研究使用来自不同应用领域和不同类别数量的六个数据集，对40个异构分类器进行了实验。与Elbow启发式方法和最先进的基线方法相比，本文提出的方法在六个数据集中的两个上实现了最高的准确率。", "conclusion": "所提出的自动分类器选择方法通过有效结合多样性和性能，显著改进了集成学习系统在假新闻检测中的表现，尤其在处理模型多样性挑战方面展现出优越性。"}}
{"id": "2508.21398", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.21398", "abs": "https://arxiv.org/abs/2508.21398", "authors": ["Andreas Leibetseder", "Sabrina Kletz", "Klaus Schoeffmann", "Simon Keckstein", "Jörg Keckstein"], "title": "GLENDA: Gynecologic Laparoscopy Endometriosis Dataset", "comment": null, "summary": "Gynecologic laparoscopy as a type of minimally invasive surgery (MIS) is\nperformed via a live feed of a patient's abdomen surveying the insertion and\nhandling of various instruments for conducting treatment. Adopting this kind of\nsurgical intervention not only facilitates a great variety of treatments, the\npossibility of recording said video streams is as well essential for numerous\npost-surgical activities, such as treatment planning, case documentation and\neducation. Nonetheless, the process of manually analyzing surgical recordings,\nas it is carried out in current practice, usually proves tediously\ntime-consuming. In order to improve upon this situation, more sophisticated\ncomputer vision as well as machine learning approaches are actively developed.\nSince most of such approaches heavily rely on sample data, which especially in\nthe medical field is only sparsely available, with this work we publish the\nGynecologic Laparoscopy ENdometriosis DAtaset (GLENDA) - an image dataset\ncontaining region-based annotations of a common medical condition named\nendometriosis, i.e. the dislocation of uterine-like tissue. The dataset is the\nfirst of its kind and it has been created in collaboration with leading medical\nexperts in the field.", "AI": {"tldr": "本文发布了GLENDA数据集，这是一个包含子宫内膜异位症区域标注的妇科腹腔镜图像数据集，旨在解决医学领域计算机视觉和机器学习方法中数据稀缺的问题，以改进手术视频的自动化分析。", "motivation": "妇科腹腔镜手术视频的手动分析耗时且繁琐。为了改进这一现状，需要开发更先进的计算机视觉和机器学习方法。然而，这些方法严重依赖样本数据，而医学领域的数据通常非常稀缺。", "method": "本文通过与医学专家合作，创建并发布了妇科腹腔镜子宫内膜异位症数据集（GLENDA）。该数据集包含子宫内膜异位症（一种常见疾病）的区域性标注图像。", "result": "发布了GLENDA数据集，这是首个包含子宫内膜异位症区域标注的妇科腹腔镜图像数据集。该数据集的创建得到了该领域领先医学专家的协作。", "conclusion": "GLENDA数据集的发布填补了医学领域妇科腹腔镜子宫内膜异位症标注数据稀缺的空白，为开发用于治疗计划、病例记录和教育等术后活动的自动化计算机视觉和机器学习方法提供了基础。"}}
{"id": "2508.21730", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.21730", "abs": "https://arxiv.org/abs/2508.21730", "authors": ["Fabrizio Fagiolo", "Nicolo' Vescera"], "title": "Freeze and Conquer: Reusable Ansatz for Solving the Traveling Salesman Problem", "comment": null, "summary": "In this paper we present a variational algorithm for the Traveling Salesman\nProblem (TSP) that combines (i) a compact encoding of permutations, which\nreduces the qubit requirement too, (ii) an optimize-freeze-reuse strategy:\nwhere the circuit topology (``Ansatz'') is first optimized on a training\ninstance by Simulated Annealing (SA), then ``frozen'' and re-used on novel\ninstances, limited to a rapid re-optimization of only the circuit parameters.\nThis pipeline eliminates costly structural research in testing, making the\nprocedure immediately implementable on NISQ hardware.\n  On a set of $40$ randomly generated symmetric instances that span $4 - 7$\ncities, the resulting Ansatz achieves an average optimal trip sampling\nprobability of $100\\%$ for 4 city cases, $90\\%$ for 5 city cases and $80\\%$ for\n6 city cases. With 7 cities the success rate drops markedly to an average of\n$\\sim 20\\%$, revealing the onset of scalability limitations of the proposed\nmethod.\n  The results show robust generalization ability for moderate problem sizes and\nindicate how freezing the Ansatz can dramatically reduce time-to-solution\nwithout degrading solution quality. The paper also discusses scalability\nlimitations, the impact of ``warm-start'' initialization of parameters, and\nprospects for extension to more complex problems, such as Vehicle Routing and\nJob-Shop Scheduling.", "AI": {"tldr": "本文提出了一种针对旅行商问题（TSP）的变分算法，结合了紧凑的排列编码和“优化-冻结-重用”策略，在适中规模问题上表现出良好的泛化能力，但存在可扩展性限制。", "motivation": "为解决旅行商问题，并使其能够在当前的NISQ（噪声中等规模量子）硬件上有效实现，需要一种减少量子比特需求和降低测试阶段计算成本的方法，特别是避免昂贵的结构搜索。", "method": "该方法是一种变分算法，采用：1) 紧凑的排列编码以减少量子比特需求；2) “优化-冻结-重用”策略：首先使用模拟退火（SA）在训练实例上优化电路拓扑（Ansatz），然后“冻结”此Ansatz并在新实例上重复使用，仅需快速重新优化电路参数。", "result": "在4-7个城市的40个随机对称实例上测试：4城市案例的平均最优路径采样概率为100%；5城市案例为90%；6城市案例为80%。7城市案例的成功率显著下降至约20%，揭示了该方法的可扩展性限制。结果表明，对于中等规模问题具有鲁棒的泛化能力，并且冻结Ansatz能显著减少解决时间而不降低解决方案质量。", "conclusion": "该研究表明，所提出的变分算法在适中规模的旅行商问题上具有鲁棒的泛化能力，并且通过冻结Ansatz可以显著减少解决问题所需的时间，使其适用于NISQ硬件。尽管目前存在可扩展性限制，但该方法有望扩展到车辆路径和作业车间调度等更复杂的问题。"}}
{"id": "2508.21569", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.21569", "abs": "https://arxiv.org/abs/2508.21569", "authors": ["Aishwarya Mirashi", "Ananya Joshi", "Raviraj Joshi"], "title": "L3Cube-MahaSTS: A Marathi Sentence Similarity Dataset and Models", "comment": null, "summary": "We present MahaSTS, a human-annotated Sentence Textual Similarity (STS)\ndataset for Marathi, along with MahaSBERT-STS-v2, a fine-tuned Sentence-BERT\nmodel optimized for regression-based similarity scoring. The MahaSTS dataset\nconsists of 16,860 Marathi sentence pairs labeled with continuous similarity\nscores in the range of 0-5. To ensure balanced supervision, the dataset is\nuniformly distributed across six score-based buckets spanning the full 0-5\nrange, thus reducing label bias and enhancing model stability. We fine-tune the\nMahaSBERT model on this dataset and benchmark its performance against other\nalternatives like MahaBERT, MuRIL, IndicBERT, and IndicSBERT. Our experiments\ndemonstrate that MahaSTS enables effective training for sentence similarity\ntasks in Marathi, highlighting the impact of human-curated annotations,\ntargeted fine-tuning, and structured supervision in low-resource settings. The\ndataset and model are publicly shared at\nhttps://github.com/l3cube-pune/MarathiNLP", "AI": {"tldr": "本文提出了MahaSTS，一个用于马拉地语的人工标注句子文本相似度（STS）数据集，以及MahaSBERT-STS-v2，一个针对回归相似度评分优化的微调Sentence-BERT模型。", "motivation": "在低资源语言环境下，需要高质量的句子文本相似度数据集和模型来支持马拉地语的STS任务。", "method": "构建了包含16,860对马拉地语句子的人工标注MahaSTS数据集，相似度分数范围为0-5，并确保了分数在不同区间内的均匀分布。在此数据集上微调了MahaSBERT模型，并将其性能与MahaBERT、MuRIL、IndicBERT和IndicSBERT等其他模型进行了基准测试。", "result": "实验证明，MahaSTS数据集能够有效训练马拉地语的句子相似度任务。研究强调了在低资源环境中，人工标注、有针对性的微调和结构化监督对模型性能的重要性。", "conclusion": "MahaSTS数据集和MahaSBERT-STS-v2模型为马拉地语的句子相似度任务提供了有效的训练资源，并突出了高质量标注和微调在低资源语言处理中的关键作用。"}}
{"id": "2508.21399", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.21399", "abs": "https://arxiv.org/abs/2508.21399", "authors": ["Sabrina Kletz", "Klaus Schoeffmann", "Jenny Benois-Pineau", "Heinrich Husslein"], "title": "Identifying Surgical Instruments in Laparoscopy Using Deep Learning Instance Segmentation", "comment": null, "summary": "Recorded videos from surgeries have become an increasingly important\ninformation source for the field of medical endoscopy, since the recorded\nfootage shows every single detail of the surgery. However, while video\nrecording is straightforward these days, automatic content indexing - the basis\nfor content-based search in a medical video archive - is still a great\nchallenge due to the very special video content. In this work, we investigate\nsegmentation and recognition of surgical instruments in videos recorded from\nlaparoscopic gynecology. More precisely, we evaluate the achievable performance\nof segmenting surgical instruments from their background by using a\nregion-based fully convolutional network for instance-aware (1) instrument\nsegmentation as well as (2) instrument recognition. While the first part\naddresses only binary segmentation of instances (i.e., distinguishing between\ninstrument or background) we also investigate multi-class instrument\nrecognition (i.e., identifying the type of instrument). Our evaluation results\nshow that even with a moderately low number of training examples, we are able\nto localize and segment instrument regions with a pretty high accuracy.\nHowever, the results also reveal that determining the particular instrument is\nstill very challenging, due to the inherently high similarity of surgical\ninstruments.", "AI": {"tldr": "本研究使用区域全卷积网络 (FCN) 对妇科腹腔镜手术视频中的手术器械进行实例分割和识别，发现器械定位和分割精度较高，但器械类型识别仍具挑战性。", "motivation": "手术视频是医疗内窥镜领域日益重要的信息源，但自动内容索引（医学视频档案中基于内容搜索的基础）因其特殊内容而极具挑战性。尽管视频录制已变得简单，但对手术器械的自动识别和分割仍是一个难题。", "method": "本研究采用区域全卷积网络 (FCN) 来实现实例感知的手术器械分割和识别。具体方法包括：1) 器械的二元分割（区分器械与背景），2) 器械的多类别识别（识别器械类型）。", "result": "评估结果表明，即使训练样本数量适中，也能以相当高的精度定位和分割器械区域。然而，结果也揭示，由于手术器械固有的高度相似性，确定特定器械类型仍然非常具有挑战性。", "conclusion": "区域全卷积网络在妇科腹腔镜手术视频中对手术器械的定位和分割表现出较高的准确性。尽管如此，由于器械之间的高度相似性，精确识别具体器械类型仍然是一个亟待解决的难题。"}}
{"id": "2508.21742", "categories": ["cs.AI", "stat.ME"], "pdf": "https://arxiv.org/pdf/2508.21742", "abs": "https://arxiv.org/abs/2508.21742", "authors": ["Timothée Loranchet", "Charles K. Assaad"], "title": "Orientability of Causal Relations in Time Series using Summary Causal Graphs and Faithful Distributions", "comment": null, "summary": "Understanding causal relations between temporal variables is a central\nchallenge in time series analysis, particularly when the full causal structure\nis unknown. Even when the full causal structure cannot be fully specified,\nexperts often succeed in providing a high-level abstraction of the causal\ngraph, known as a summary causal graph, which captures the main causal\nrelations between different time series while abstracting away micro-level\ndetails. In this work, we present conditions that guarantee the orientability\nof micro-level edges between temporal variables given the background knowledge\nencoded in a summary causal graph and assuming having access to a faithful and\ncausally sufficient distribution with respect to the true unknown graph. Our\nresults provide theoretical guarantees for edge orientation at the micro-level,\neven in the presence of cycles or bidirected edges at the macro-level. These\nfindings offer practical guidance for leveraging SCGs to inform causal\ndiscovery in complex temporal systems and highlight the value of incorporating\nexpert knowledge to improve causal inference from observational time series\ndata.", "AI": {"tldr": "本研究提出在已知专家提供的摘要因果图（SCG）的背景下，如何保证时间序列中微观层面因果边的可定向性，即使宏观层面存在循环或双向边。", "motivation": "时间序列中因果关系的理解极具挑战性，尤其当完整的因果结构未知时。专家虽然能提供高层次的摘要因果图，但如何利用这些信息来推断微观层面的因果细节是一个未解决的问题。", "method": "该研究提出了在给定摘要因果图作为背景知识，并假设可获得真实未知图的忠实且因果充分分布的情况下，保证时间变量之间微观层面边缘可定向性的条件。", "result": "研究结果为微观层面的边缘定向提供了理论保证，即使在宏观层面存在循环或双向边的情况下也适用。", "conclusion": "这些发现为利用摘要因果图（SCG）指导复杂时间系统中的因果发现提供了实用指导，并强调了结合专家知识以改进观测时间序列数据因果推断的价值。"}}
{"id": "2508.21587", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.21587", "abs": "https://arxiv.org/abs/2508.21587", "authors": ["Tobias Deußer", "Lorenz Sparrenberg", "Armin Berger", "Max Hahnbück", "Christian Bauckhage", "Rafet Sifa"], "title": "A Survey on Current Trends and Recent Advances in Text Anonymization", "comment": "Accepted at IEEE DSAA 2025", "summary": "The proliferation of textual data containing sensitive personal information\nacross various domains requires robust anonymization techniques to protect\nprivacy and comply with regulations, while preserving data usability for\ndiverse and crucial downstream tasks. This survey provides a comprehensive\noverview of current trends and recent advances in text anonymization\ntechniques. We begin by discussing foundational approaches, primarily centered\non Named Entity Recognition, before examining the transformative impact of\nLarge Language Models, detailing their dual role as sophisticated anonymizers\nand potent de-anonymization threats. The survey further explores\ndomain-specific challenges and tailored solutions in critical sectors such as\nhealthcare, law, finance, and education. We investigate advanced methodologies\nincorporating formal privacy models and risk-aware frameworks, and address the\nspecialized subfield of authorship anonymization. Additionally, we review\nevaluation frameworks, comprehensive metrics, benchmarks, and practical\ntoolkits for real-world deployment of anonymization solutions. This review\nconsolidates current knowledge, identifies emerging trends and persistent\nchallenges, including the evolving privacy-utility trade-off, the need to\naddress quasi-identifiers, and the implications of LLM capabilities, and aims\nto guide future research directions for both academics and practitioners in\nthis field.", "AI": {"tldr": "这篇综述全面概述了文本匿名化技术的当前趋势和最新进展，涵盖了从基础方法到大型语言模型的作用、特定领域挑战、高级方法及评估框架，并指出了未来的研究方向。", "motivation": "随着包含敏感个人信息的文本数据在各领域激增，需要强大的匿名化技术来保护隐私、遵守法规，同时保持数据对下游任务的可用性。", "method": "本研究通过综述的方式，讨论了文本匿名化的基础方法（主要围绕命名实体识别）、大型语言模型（作为匿名器和去匿名化威胁的双重角色）、关键领域的特定挑战和解决方案（如医疗、法律、金融、教育）、结合形式化隐私模型和风险感知框架的先进方法、作者身份匿名化子领域，并回顾了评估框架、指标、基准和实用工具包。", "result": "综述整合了当前知识，识别了新兴趋势和持续挑战，包括不断演变的隐私-效用权衡、处理准标识符的需求以及大型语言模型能力的影响。它还提供了在文本匿名化领域学术界和从业者的未来研究方向。", "conclusion": "本综述旨在通过整合现有知识、识别趋势和挑战，为文本匿名化领域的未来研究提供指导，以应对数据隐私保护和可用性之间的平衡问题。"}}
{"id": "2508.21402", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.21402", "abs": "https://arxiv.org/abs/2508.21402", "authors": ["Jakub Straka", "Ivan Gruber"], "title": "SatDINO: A Deep Dive into Self-Supervised Pretraining for Remote Sensing", "comment": null, "summary": "Self-supervised learning has emerged as a powerful tool for remote sensing,\nwhere large amounts of unlabeled data are available. In this work, we\ninvestigate the use of DINO, a contrastive self-supervised method, for\npretraining on remote sensing imagery. We introduce SatDINO, a model tailored\nfor representation learning in satellite imagery. Through extensive experiments\non multiple datasets in multiple testing setups, we demonstrate that SatDINO\noutperforms other state-of-the-art methods based on much more common masked\nautoencoders (MAE) and achieves competitive results in multiple benchmarks.\n  We also provide a rigorous ablation study evaluating SatDINO's individual\ncomponents. Finally, we propose a few novel enhancements, such as a new way to\nincorporate ground sample distance (GSD) encoding and adaptive view sampling.\nThese enhancements can be used independently on our SatDINO model. Our code and\ntrained models are available at: https://github.com/strakaj/SatDINO.", "AI": {"tldr": "本文提出了SatDINO，一个基于DINO的自监督学习模型，用于遥感图像的表示学习。SatDINO在多个基准测试中超越了基于掩码自编码器(MAE)的SOTA方法，并取得了有竞争力的结果，同时引入了GSD编码和自适应视图采样等新颖增强。", "motivation": "遥感领域存在大量未标注数据，自监督学习是处理这些数据的强大工具。研究旨在为卫星图像开发一种更有效的预训练方法。", "method": "研究了DINO（一种对比自监督方法）在遥感图像预训练中的应用，并引入了为卫星图像表示学习量身定制的SatDINO模型。通过在多个数据集和测试设置上进行大量实验，并进行了严格的消融研究，评估了SatDINO的各个组件。此外，提出了包括GSD编码和自适应视图采样在内的新颖增强。", "result": "SatDINO在多个基准测试中超越了其他基于掩码自编码器（MAE）的SOTA方法，并取得了有竞争力的结果。消融研究验证了SatDINO各个组件的有效性。提出的新颖增强（如GSD编码和自适应视图采样）可以独立应用于SatDINO模型。", "conclusion": "SatDINO是一个在遥感图像表示学习中表现卓越的自监督预训练模型，其性能优于当前流行的MAE方法，并且通过引入新颖增强进一步提升了效果。"}}
{"id": "2508.21803", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.21803", "abs": "https://arxiv.org/abs/2508.21803", "authors": ["Yeawon Lee", "Xiaoyang Wang", "Christopher C. Yang"], "title": "Automated Clinical Problem Detection from SOAP Notes using a Collaborative Multi-Agent LLM Architecture", "comment": "Accepted to The 16th ACM Conference on Bioinformatics, Computational\n  Biology, and Health Informatics (ACM-BCB 2025)(Poster Paper)", "summary": "Accurate interpretation of clinical narratives is critical for patient care,\nbut the complexity of these notes makes automation challenging. While Large\nLanguage Models (LLMs) show promise, single-model approaches can lack the\nrobustness required for high-stakes clinical tasks. We introduce a\ncollaborative multi-agent system (MAS) that models a clinical consultation team\nto address this gap. The system is tasked with identifying clinical problems by\nanalyzing only the Subjective (S) and Objective (O) sections of SOAP notes,\nsimulating the diagnostic reasoning process of synthesizing raw data into an\nassessment. A Manager agent orchestrates a dynamically assigned team of\nspecialist agents who engage in a hierarchical, iterative debate to reach a\nconsensus. We evaluated our MAS against a single-agent baseline on a curated\ndataset of 420 MIMIC-III notes. The dynamic multi-agent configuration\ndemonstrated consistently improved performance in identifying congestive heart\nfailure, acute kidney injury, and sepsis. Qualitative analysis of the agent\ndebates reveals that this structure effectively surfaces and weighs conflicting\nevidence, though it can occasionally be susceptible to groupthink. By modeling\na clinical team's reasoning process, our system offers a promising path toward\nmore accurate, robust, and interpretable clinical decision support tools.", "AI": {"tldr": "该研究引入了一个协作式多智能体系统（MAS），模拟临床会诊团队，用于从SOAP笔记中识别临床问题。该系统在识别充血性心力衰竭、急性肾损伤和败血症方面优于单一智能体基线。", "motivation": "临床叙述的准确解读对患者护理至关重要，但其复杂性使得自动化面临挑战。大型语言模型（LLMs）虽有潜力，但单一模型方法在处理高风险临床任务时缺乏所需的鲁棒性。", "method": "研究引入了一个协作式多智能体系统（MAS），模拟临床会诊团队。该系统通过分析SOAP笔记的主观（S）和客观（O）部分来识别临床问题。一个“经理”智能体协调一个动态分配的专家智能体团队，这些智能体进行分层、迭代的辩论以达成共识。该系统在包含420份MIMIC-III笔记的精选数据集上，与单一智能体基线进行了评估。", "result": "动态多智能体配置在识别充血性心力衰竭、急性肾损伤和败血症方面表现出持续改进的性能。对智能体辩论的定性分析表明，这种结构能有效揭示和权衡冲突证据，尽管偶尔可能容易出现群体思维。", "conclusion": "通过模拟临床团队的推理过程，该系统为开发更准确、鲁棒和可解释的临床决策支持工具提供了一条有前景的途径。"}}
{"id": "2508.21589", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.21589", "abs": "https://arxiv.org/abs/2508.21589", "authors": ["Zinan Tang", "Xin Gao", "Qizhi Pei", "Zhuoshi Pan", "Mengzhang Cai", "Jiang Wu", "Conghui He", "Lijun Wu"], "title": "Middo: Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning", "comment": "Accepted by EMNLP 2025 (main)", "summary": "Supervised Fine-Tuning (SFT) Large Language Models (LLM) fundamentally rely\non high-quality training data. While data selection and data synthesis are two\ncommon strategies to improve data quality, existing approaches often face\nlimitations in static dataset curation that fail to adapt to evolving model\ncapabilities. In this paper, we introduce Middo, a self-evolving Model-informed\ndynamic data optimization framework that uses model-aware data selection and\ncontext-preserving data refinement. Unlike conventional one-off\nfiltering/synthesis methods, our framework establishes a closed-loop\noptimization system: (1) A self-referential diagnostic module proactively\nidentifies suboptimal samples through tri-axial model signals - loss patterns\n(complexity), embedding cluster dynamics (diversity), and self-alignment scores\n(quality); (2) An adaptive optimization engine then transforms suboptimal\nsamples into pedagogically valuable training points while preserving semantic\nintegrity; (3) This optimization process continuously evolves with model\ncapability through dynamic learning principles. Experiments on multiple\nbenchmarks demonstrate that our \\method consistently enhances the quality of\nseed data and boosts LLM's performance with improving accuracy by 7.15% on\naverage while maintaining the original dataset scale. This work establishes a\nnew paradigm for sustainable LLM training through dynamic human-AI co-evolution\nof data and models. Our datasets, models, and code are coming soon.", "AI": {"tldr": "Middo是一个自演进的模型感知动态数据优化框架，通过模型诊断和自适应优化，持续提升SFT大型语言模型的训练数据质量和模型性能。", "motivation": "监督微调（SFT）大型语言模型（LLM）严重依赖高质量训练数据，但现有数据选择和合成方法通常是静态的，无法适应模型能力的变化。", "method": "Middo框架建立了一个闭环优化系统：1) 自我诊断模块通过损失模式（复杂性）、嵌入聚类动态（多样性）和自对齐分数（质量）识别次优样本；2) 自适应优化引擎在保持语义完整性的前提下将次优样本转化为有价值的训练点；3) 优化过程通过动态学习原则随模型能力持续演进。", "result": "实验表明，Middo持续提升了原始数据质量，平均将LLM的准确性提高了7.15%，同时保持了数据集规模。", "conclusion": "该工作为通过数据和模型的动态人机协同演进，建立了可持续LLM训练的新范式。"}}
{"id": "2508.21418", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.21418", "abs": "https://arxiv.org/abs/2508.21418", "authors": ["Gernot Fiala", "Markus Plass", "Robert Harb", "Peter Regitnig", "Kristijan Skok", "Wael Al Zoughbi", "Carmen Zerner", "Paul Torke", "Michaela Kargl", "Heimo Müller", "Tomas Brazdil", "Matej Gallo", "Jaroslav Kubín", "Roman Stoklasa", "Rudolf Nenutil", "Norman Zerbe", "Andreas Holzinger", "Petr Holub"], "title": "Standardized Multi-Layer Tissue Maps for Enhanced Artificial Intelligence Integration and Search in Large-Scale Whole Slide Image Archives", "comment": null, "summary": "A Whole Slide Image (WSI) is a high-resolution digital image created by\nscanning an entire glass slide containing a biological specimen, such as tissue\nsections or cell samples, at multiple magnifications. These images can be\nviewed, analyzed, shared digitally, and are used today for Artificial\nIntelligence (AI) algorithm development. WSIs are used in a variety of fields,\nincluding pathology for diagnosing diseases and oncology for cancer research.\nThey are also utilized in neurology, veterinary medicine, hematology,\nmicrobiology, dermatology, pharmacology, toxicology, immunology, and forensic\nscience.\n  When assembling cohorts for the training or validation of an AI algorithm, it\nis essential to know what is present on such a WSI. However, there is currently\nno standard for this metadata, so such selection has mainly been done through\nmanual inspection, which is not suitable for large collections with several\nmillion objects.\n  We propose a general framework to generate a 2D index map for WSI and a\nprofiling mechanism for specific application domains. We demonstrate this\napproach in the field of clinical pathology, using common syntax and semantics\nto achieve interoperability between different catalogs.\n  Our approach augments each WSI collection with a detailed tissue map that\nprovides fine-grained information about the WSI content. The tissue map is\norganized into three layers: source, tissue type, and pathological alterations,\nwith each layer assigning segments of the WSI to specific classes.\n  We illustrate the advantages and applicability of the proposed standard\nthrough specific examples in WSI catalogs, Machine Learning (ML), and\ngraph-based WSI representations.", "AI": {"tldr": "本文提出了一种通用的框架，用于生成全玻片图像（WSI）的2D索引图和特定应用领域的分析机制，以解决WSI内容元数据标准化缺失和人工检查效率低下的问题，尤其是在临床病理学中。", "motivation": "全玻片图像（WSI）广泛应用于诊断和研究，尤其在AI算法开发中。然而，在为AI算法训练或验证组建队列时，缺乏关于WSI内容的标准元数据。目前主要通过人工检查来筛选WSI，这对于数百万对象的庞大集合来说效率低下且不适用。", "method": "研究者提出了一种通用框架，用于生成WSI的2D索引图，并为特定应用领域提供分析机制。该方法通过详细的组织图增强每个WSI集合，提供细粒度的WSI内容信息。组织图分为三层：来源、组织类型和病理改变，每层将WSI的不同区域分配给特定类别。该方法在临床病理学领域进行了演示，使用通用语法和语义实现不同目录间的互操作性。", "result": "该方法成功为WSI集合提供了详细的组织图，该组织图包含三层（来源、组织类型、病理改变），能够将WSI的不同区域分配到特定类别，从而提供细粒度的内容信息。通过使用通用语法和语义，实现了不同目录间的互操作性。", "conclusion": "所提出的标准框架在WSI目录、机器学习和基于图的WSI表示中具有显著的优势和广泛的适用性，能够有效解决WSI内容元数据标准化和大规模数据处理的挑战。"}}
{"id": "2508.21628", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.21628", "abs": "https://arxiv.org/abs/2508.21628", "authors": ["Sarfaroz Yunusov", "Kaige Chen", "Kazi Nishat Anwar", "Ali Emami"], "title": "Personality Matters: User Traits Predict LLM Preferences in Multi-Turn Collaborative Tasks", "comment": "Accepted to EMNLP 2025 Main Conference", "summary": "As Large Language Models (LLMs) increasingly integrate into everyday\nworkflows, where users shape outcomes through multi-turn collaboration, a\ncritical question emerges: do users with different personality traits\nsystematically prefer certain LLMs over others? We conducted a study with 32\nparticipants evenly distributed across four Keirsey personality types,\nevaluating their interactions with GPT-4 and Claude 3.5 across four\ncollaborative tasks: data analysis, creative writing, information retrieval,\nand writing assistance. Results revealed significant personality-driven\npreferences: Rationals strongly preferred GPT-4, particularly for goal-oriented\ntasks, while idealists favored Claude 3.5, especially for creative and\nanalytical tasks. Other personality types showed task-dependent preferences.\nSentiment analysis of qualitative feedback confirmed these patterns. Notably,\naggregate helpfulness ratings were similar across models, showing how\npersonality-based analysis reveals LLM differences that traditional evaluations\nmiss.", "AI": {"tldr": "本研究发现，用户个性特质（如Keirsey类型）显著影响其对大型语言模型（如GPT-4和Claude 3.5）的偏好，尤其是在不同协作任务中。", "motivation": "随着大型语言模型（LLMs）日益融入日常工作流程，用户通过多轮协作塑造结果，研究者们提出了一个关键问题：不同个性特质的用户是否系统性地偏好某些LLMs而非其他？", "method": "研究招募了32名参与者，他们均匀分布于四种Keirsey个性类型。参与者评估了与GPT-4和Claude 3.5在四种协作任务（数据分析、创意写作、信息检索和写作辅助）中的互动。研究还对定性反馈进行了情感分析。", "result": "结果显示出显著的个性驱动偏好：理性者（Rationals）强烈偏好GPT-4，尤其是在目标导向任务中；理想主义者（Idealists）则偏爱Claude 3.5，特别是在创意和分析任务中。其他个性类型表现出任务依赖的偏好。定性反馈的情感分析证实了这些模式。值得注意的是，模型的总体有用性评分相似，这表明基于个性的分析揭示了传统评估所忽视的LLM差异。", "conclusion": "用户个性特质在LLM偏好中扮演关键角色，基于个性的分析能够揭示传统评估方法无法捕捉到的LLM间差异。"}}
{"id": "2508.21424", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.21424", "abs": "https://arxiv.org/abs/2508.21424", "authors": ["Lucas Rakotoarivony"], "title": "Unsupervised Incremental Learning Using Confidence-Based Pseudo-Labels", "comment": "Submitted to WACV 2026", "summary": "Deep learning models have achieved state-of-the-art performance in many\ncomputer vision tasks. However, in real-world scenarios, novel classes that\nwere unseen during training often emerge, requiring models to acquire new\nknowledge incrementally. Class-Incremental Learning (CIL) methods enable a\nmodel to learn novel classes while retaining knowledge of previous classes.\nHowever, these methods make the strong assumption that the incremental dataset\nis fully labeled, which is unrealistic in practice. In this work, we propose an\nunsupervised Incremental Learning method using Confidence-based Pseudo-labels\n(ICPL), which replaces human annotations with pseudo-labels, enabling\nincremental learning from unlabeled datasets. We integrate these pseudo-labels\ninto various CIL methods with confidence-based selection and evaluate\nperformance degradation on CIFAR100 and ImageNet100. Then, we compare our\napproach to popular Class Incremental Novel Category Discovery (class-iNCD)\nmethods addressing similar challenges. Additionally, we apply our method to\nfine-grained datasets to demonstrate its real-world practicality and measure\nits computational complexity to validate its suitability for\nresource-constrained environments. ICPL achieves competitive results compared\nto supervised methods and outperforms state-of-the-art class-iNCD methods by\nmore than 5% in final accuracy.", "AI": {"tldr": "本文提出了一种名为ICPL的无监督增量学习方法，通过置信度伪标签取代人工标注，使模型能从无标签数据中增量学习新类别，并在多个数据集上取得了优于现有方法的性能。", "motivation": "现有的类增量学习（CIL）方法假设增量数据集是完全标注的，这在实际应用中是不现实的。研究动机是开发一种能够从无标签数据中增量学习新知识的方法。", "method": "该研究提出了一种基于置信度伪标签（Confidence-based Pseudo-labels）的无监督增量学习方法（ICPL）。它用伪标签替代了人工标注，并将其集成到各种CIL方法中，同时采用基于置信度的选择机制。方法在CIFAR100和ImageNet100数据集上评估了性能下降，并与流行的类增量新类别发现（class-iNCD）方法进行了比较。此外，还在细粒度数据集上进行了应用，并测量了计算复杂度。", "result": "ICPL方法与有监督方法相比取得了有竞争力的结果，并且在最终准确率上比最先进的class-iNCD方法高出5%以上。该方法在实际场景中具有实用性，并且其计算复杂度表明它适用于资源受限的环境。", "conclusion": "ICPL提供了一种有效且实用的解决方案，使模型能够从无标签数据中进行无监督增量学习，克服了传统CIL方法对完全标注数据的依赖，并在性能上超越了现有的一些先进方法，尤其适用于资源受限的真实世界场景。"}}
{"id": "2508.21632", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.21632", "abs": "https://arxiv.org/abs/2508.21632", "authors": ["Peng Yu", "En Xu", "Bin Chen", "Haibiao Chen", "Yinfei Xu"], "title": "QZhou-Embedding Technical Report", "comment": null, "summary": "We present QZhou-Embedding, a general-purpose contextual text embedding model\nwith exceptional text representation capabilities. Built upon the\nQwen2.5-7B-Instruct foundation model, we designed a unified multi-task\nframework comprising specialized data transformation and training strategies.\nThe data transformation scheme enables the incorporation of more diverse\ntextual training datasets, while the task-specific training strategies enhance\nmodel learning efficiency. We developed a data synthesis pipeline leveraging\nLLM API, incorporating techniques such as paraphrasing, augmentation, and hard\nnegative example generation to improve the semantic richness and sample\ndifficulty of the training set. Additionally, we employ a two-stage training\nstrategy, comprising initial retrieval-focused pretraining followed by\nfull-task fine-tuning, enabling the embedding model to extend its capabilities\nbased on robust retrieval performance. Our model achieves state-of-the-art\nresults on the MTEB and CMTEB benchmarks, ranking first on both leaderboards\n(August 27 2025), and simultaneously achieves state-of-the-art performance on\ntasks including reranking, clustering, etc. Our findings demonstrate that\nhigher-quality, more diverse data is crucial for advancing retrieval model\nperformance, and that leveraging LLMs generative capabilities can further\noptimize data quality for embedding model breakthroughs. Our model weights are\nreleased on HuggingFace under Apache 2.0 license. For reproducibility, we\nprovide evaluation code and instructions on GitHub.", "AI": {"tldr": "QZhou-Embedding是一种基于Qwen2.5-7B-Instruct的通用上下文文本嵌入模型，通过统一多任务框架、多样化数据转换和两阶段训练策略，在MTEB和CMTEB基准测试中取得了最先进的性能。", "motivation": "开发一个具有卓越文本表示能力的通用上下文文本嵌入模型，并探索高质量、多样化数据对检索模型性能提升的重要性。", "method": "该模型基于Qwen2.5-7B-Instruct基础模型，设计了一个统一的多任务框架，包括专门的数据转换和训练策略。数据转换方案允许整合更多样化的文本训练数据集。利用LLM API开发了数据合成管线，包含释义、增强和难负例生成。采用两阶段训练策略：先进行以检索为重点的预训练，然后进行全任务微调。", "result": "QZhou-Embedding在MTEB和CMTEB基准测试中均取得最先进结果，排名第一（截至2025年8月27日），并在重排序、聚类等任务上同时实现最先进性能。", "conclusion": "更高质量、更多样化的数据对于提升检索模型性能至关重要，并且利用大型语言模型（LLMs）的生成能力可以进一步优化数据质量，从而推动嵌入模型的突破。"}}
{"id": "2508.21435", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.21435", "abs": "https://arxiv.org/abs/2508.21435", "authors": ["Francisco Caetano", "Christiaan Viviers", "Peter H. H. de With", "Fons van der Sommen"], "title": "MedShift: Implicit Conditional Transport for X-Ray Domain Adaptation", "comment": "Accepted at the ICCV 2025 AIM Workshop", "summary": "Synthetic medical data offers a scalable solution for training robust models,\nbut significant domain gaps limit its generalizability to real-world clinical\nsettings. This paper addresses the challenge of cross-domain translation\nbetween synthetic and real X-ray images of the head, focusing on bridging\ndiscrepancies in attenuation behavior, noise characteristics, and soft tissue\nrepresentation. We propose MedShift, a unified class-conditional generative\nmodel based on Flow Matching and Schrodinger Bridges, which enables\nhigh-fidelity, unpaired image translation across multiple domains. Unlike prior\napproaches that require domain-specific training or rely on paired data,\nMedShift learns a shared domain-agnostic latent space and supports seamless\ntranslation between any pair of domains seen during training. We introduce\nX-DigiSkull, a new dataset comprising aligned synthetic and real skull X-rays\nunder varying radiation doses, to benchmark domain translation models.\nExperimental results demonstrate that, despite its smaller model size compared\nto diffusion-based approaches, MedShift offers strong performance and remains\nflexible at inference time, as it can be tuned to prioritize either perceptual\nfidelity or structural consistency, making it a scalable and generalizable\nsolution for domain adaptation in medical imaging. The code and dataset are\navailable at https://caetas.github.io/medshift.html", "AI": {"tldr": "该论文提出了MedShift，一个基于Flow Matching和Schrodinger Bridges的类条件生成模型，用于合成与真实X射线图像之间的高保真、无配对的跨域转换，有效弥合了领域差距，并提供了灵活的性能调整。", "motivation": "合成医疗数据在训练鲁棒模型方面具有可扩展性，但其与真实临床环境之间存在显著的领域差距（如衰减行为、噪声特征和软组织表示），限制了其泛化能力。本研究旨在解决合成与真实头部X射线图像之间的跨域转换挑战，以弥合这些差异。", "method": "本文提出了MedShift，一个统一的类条件生成模型，基于Flow Matching和Schrodinger Bridges。该模型能够实现高保真、无配对的多域图像转换，学习共享的领域无关潜在空间，并支持训练期间任何域对之间的无缝转换。此外，还引入了新的数据集X-DigiSkull，包含不同辐射剂量下的对齐合成和真实颅骨X射线图像，用于基准测试。", "result": "实验结果表明，尽管MedShift的模型尺寸小于基于扩散的方法，但它仍能提供强大的性能，并在推理时保持灵活性，可以根据需求优先考虑感知保真度或结构一致性。这使其成为医学图像领域适应的一种可扩展和可泛化的解决方案。", "conclusion": "MedShift提供了一种可扩展且可泛化的解决方案，用于医学成像中的领域适应，通过其高效的跨域转换能力，有效弥合了合成与真实X射线数据之间的差距，并能根据应用需求调整输出质量。"}}
{"id": "2508.21496", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.21496", "abs": "https://arxiv.org/abs/2508.21496", "authors": ["Hao Lu", "Jiahao Wang", "Yaolun Zhang", "Ruohui Wang", "Xuanyu Zheng", "Yepeng Tang", "Dahua Lin", "Lewei Lu"], "title": "ELV-Halluc: Benchmarking Semantic Aggregation Hallucinations in Long Video Understanding", "comment": null, "summary": "Video multimodal large language models (Video-MLLMs) have achieved remarkable\nprogress in video understanding. However, they remain vulnerable to\nhallucination-producing content inconsistent with or unrelated to video inputs.\nPrevious video hallucination benchmarks primarily focus on short-videos. They\nattribute hallucinations to factors such as strong language priors, missing\nframes, or vision-language biases introduced by the visual encoder. While these\ncauses indeed account for most hallucinations in short videos, they still\noversimplify the cause of hallucinations. Sometimes, models generate incorrect\noutputs but with correct frame-level semantics. We refer to this type of\nhallucination as Semantic Aggregation Hallucination (SAH), which arises during\nthe process of aggregating frame-level semantics into event-level semantic\ngroups. Given that SAH becomes particularly critical in long videos due to\nincreased semantic complexity across multiple events, it is essential to\nseparate and thoroughly investigate the causes of this type of hallucination.\nTo address the above issues, we introduce ELV-Halluc, the first benchmark\ndedicated to long-video hallucination, enabling a systematic investigation of\nSAH. Our experiments confirm the existence of SAH and show that it increases\nwith semantic complexity. Additionally, we find that models are more prone to\nSAH on rapidly changing semantics. Moreover, we discuss potential approaches to\nmitigate SAH. We demonstrate that positional encoding strategy contributes to\nalleviating SAH, and further adopt DPO strategy to enhance the model's ability\nto distinguish semantics within and across events. To support this, we curate a\ndataset of 8K adversarial data pairs and achieve improvements on both\nELV-Halluc and Video-MME, including a substantial 27.7% reduction in SAH ratio.", "AI": {"tldr": "本研究引入了ELV-Halluc，首个针对长视频幻觉的基准，专注于调查“语义聚合幻觉”（SAH），发现SAH随语义复杂性增加，并提出了通过位置编码和DPO策略缓解SAH的方法，实现了显著的SAH降低。", "motivation": "视频多模态大语言模型（Video-MLLMs）在视频理解方面表现出色，但仍易产生与视频内容不符的幻觉。现有幻觉基准主要关注短视频，并简化了幻觉原因。然而，模型有时会生成帧级语义正确但事件级语义错误的输出，即语义聚合幻觉（SAH），这在语义复杂性更高的长视频中尤为关键，需要被独立研究。", "method": "本研究引入了ELV-Halluc，这是首个专门用于长视频幻觉的基准，以系统性地调查SAH。通过实验确认SAH的存在及其特性。此外，研究探讨了缓解SAH的潜在方法，包括采用位置编码策略，并进一步使用DPO（Direct Preference Optimization）策略来增强模型区分事件内外语义的能力。为此，作者还整理了一个包含8K对抗性数据对的数据集。", "result": "实验证实了SAH的存在，并发现其随语义复杂性增加而加剧。模型在语义快速变化的视频上更容易产生SAH。研究表明，位置编码策略有助于缓解SAH，而DPO策略进一步增强了模型区分语义的能力。通过所提出的方法，在ELV-Halluc和Video-MME上均取得了改进，其中SAH率显著降低了27.7%。", "conclusion": "语义聚合幻觉（SAH）是Video-MLLMs在长视频理解中特有且关键的幻觉类型。ELV-Halluc基准能够系统地研究SAH。通过采用位置编码和DPO策略并结合对抗性数据训练，可以有效缓解SAH，显著提升模型在长视频中的语义理解和一致性。"}}
{"id": "2508.21675", "categories": ["cs.CL", "cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2508.21675", "abs": "https://arxiv.org/abs/2508.21675", "authors": ["Jonathan Tonglet", "Jan Zimny", "Tinne Tuytelaars", "Iryna Gurevych"], "title": "Is this chart lying to me? Automating the detection of misleading visualizations", "comment": "Preprint under review. Code and data available at:\n  https://github.com/UKPLab/arxiv2025-misviz", "summary": "Misleading visualizations are a potent driver of misinformation on social\nmedia and the web. By violating chart design principles, they distort data and\nlead readers to draw inaccurate conclusions. Prior work has shown that both\nhumans and multimodal large language models (MLLMs) are frequently deceived by\nsuch visualizations. Automatically detecting misleading visualizations and\nidentifying the specific design rules they violate could help protect readers\nand reduce the spread of misinformation. However, the training and evaluation\nof AI models has been limited by the absence of large, diverse, and openly\navailable datasets. In this work, we introduce Misviz, a benchmark of 2,604\nreal-world visualizations annotated with 12 types of misleaders. To support\nmodel training, we also release Misviz-synth, a synthetic dataset of 81,814\nvisualizations generated using Matplotlib and based on real-world data tables.\nWe perform a comprehensive evaluation on both datasets using state-of-the-art\nMLLMs, rule-based systems, and fine-tuned classifiers. Our results reveal that\nthe task remains highly challenging. We release Misviz, Misviz-synth, and the\naccompanying code.", "AI": {"tldr": "本文介绍了Misviz和Misviz-synth两个大型数据集，用于检测误导性数据可视化，并对现有AI模型进行了评估，结果显示该任务仍具挑战性。", "motivation": "误导性可视化是社交媒体和网络上虚假信息的重要来源，它们通过违反图表设计原则来扭曲数据，导致读者得出不准确的结论。先前的研究表明，人类和多模态大型语言模型（MLLMs）都容易被此类可视化欺骗。然而，由于缺乏大型、多样化且公开可用的数据集，AI模型的训练和评估受到了限制。", "method": "本研究引入了Misviz，一个包含2,604个真实世界可视化并标注了12种误导类型的数据集。为了支持模型训练，还发布了Misviz-synth，一个包含81,814个使用Matplotlib基于真实数据表生成的合成可视化数据集。研究人员使用最先进的MLLMs、基于规则的系统和微调分类器对这两个数据集进行了全面评估。", "result": "评估结果表明，检测误导性可视化并识别其违反的设计规则这一任务仍然极具挑战性。", "conclusion": "本研究发布了Misviz、Misviz-synth数据集及配套代码，旨在推动误导性可视化检测领域的研究进展。"}}
{"id": "2508.21437", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.21437", "abs": "https://arxiv.org/abs/2508.21437", "authors": ["Dimitri Gominski", "Martin Brandt", "Xiaoye Tong", "Siyu Liu", "Maurice Mugabowindekwe", "Sizhuo Li", "Florian Reiner", "Andrew Davies", "Rasmus Fensholt"], "title": "Trees as Gaussians: Large-Scale Individual Tree Mapping", "comment": null, "summary": "Trees are key components of the terrestrial biosphere, playing vital roles in\necosystem function, climate regulation, and the bioeconomy. However,\nlarge-scale monitoring of individual trees remains limited by inadequate\nmodelling. Available global products have focused on binary tree cover or\ncanopy height, which do not explicitely identify trees at individual level. In\nthis study, we present a deep learning approach for detecting large individual\ntrees in 3-m resolution PlanetScope imagery at a global scale. We simulate tree\ncrowns with Gaussian kernels of scalable size, allowing the extraction of crown\ncenters and the generation of binary tree cover maps. Training is based on\nbillions of points automatically extracted from airborne lidar data, enabling\nthe model to successfully identify trees both inside and outside forests. We\ncompare against existing tree cover maps and airborne lidar with\nstate-of-the-art performance (fractional cover R$^2 = 0.81$ against aerial\nlidar), report balanced detection metrics across biomes, and demonstrate how\ndetection can be further improved through fine-tuning with manual labels. Our\nmethod offers a scalable framework for global, high-resolution tree monitoring,\nand is adaptable to future satellite missions offering improved imagery.", "AI": {"tldr": "本研究提出了一种深度学习方法，利用3米分辨率的PlanetScope影像在全球范围内检测大型单棵树，并通过模拟树冠和使用大量激光雷达数据进行训练，实现了高精度和可扩展的树木监测。", "motivation": "树木是地球生物圈的关键组成部分，但在生态系统功能、气候调节和生物经济中的作用监测受限于现有模型。现有全球产品仅关注二元树木覆盖或冠层高度，无法明确识别单棵树木。", "method": "采用深度学习方法，利用3米分辨率的PlanetScope影像进行全球范围内的单棵树检测。通过可伸缩高斯核模拟树冠以提取冠心并生成二元树木覆盖图。训练数据来源于从机载激光雷达数据中自动提取的数十亿个点，使模型能够识别森林内外的树木。", "result": "与现有树木覆盖图和机载激光雷达数据相比，该方法表现出最先进的性能（分数覆盖率R²=0.81）。在不同生物群系中报告了平衡的检测指标，并证明通过手动标签进行微调可以进一步提高检测效果。", "conclusion": "本研究提供了一个可扩展的框架，用于全球高分辨率的树木监测，并可适应未来提供改进影像的卫星任务。"}}
{"id": "2508.21550", "categories": ["cs.CV", "cs.AI", "68T05, 68T09", "I.5.4"], "pdf": "https://arxiv.org/pdf/2508.21550", "abs": "https://arxiv.org/abs/2508.21550", "authors": ["Yujin Park", "Haejun Chung", "Ikbeom Jang"], "title": "EZ-Sort: Efficient Pairwise Comparison via Zero-Shot CLIP-Based Pre-Ordering and Human-in-the-Loop Sorting", "comment": "5 pages, 2 figures, Accepted at CIKM 2025 (ACM International\n  Conference on Information and Knowledge Management)", "summary": "Pairwise comparison is often favored over absolute rating or ordinal\nclassification in subjective or difficult annotation tasks due to its improved\nreliability. However, exhaustive comparisons require a massive number of\nannotations (O(n^2)). Recent work has greatly reduced the annotation burden\n(O(n log n)) by actively sampling pairwise comparisons using a sorting\nalgorithm. We further improve annotation efficiency by (1) roughly pre-ordering\nitems using the Contrastive Language-Image Pre-training (CLIP) model\nhierarchically without training, and (2) replacing easy, obvious human\ncomparisons with automated comparisons. The proposed EZ-Sort first produces a\nCLIP-based zero-shot pre-ordering, then initializes bucket-aware Elo scores,\nand finally runs an uncertainty-guided human-in-the-loop MergeSort. Validation\nwas conducted using various datasets: face-age estimation (FGNET), historical\nimage chronology (DHCI), and retinal image quality assessment (EyePACS). It\nshowed that EZ-Sort reduced human annotation cost by 90.5% compared to\nexhaustive pairwise comparisons and by 19.8% compared to prior work (when n =\n100), while improving or maintaining inter-rater reliability. These results\ndemonstrate that combining CLIP-based priors with uncertainty-aware sampling\nyields an efficient and scalable solution for pairwise ranking.", "AI": {"tldr": "该研究提出了一种名为EZ-Sort的 pairwise 排序方法，通过结合CLIP模型进行零样本预排序和不确定性引导的人机协作MergeSort，显著降低了主观标注任务中的人工标注成本，同时保持或提高了可靠性。", "motivation": "在主观或困难的标注任务中，pairwise 比较因其更高的可靠性而优于绝对评分。然而，穷举比较需要大量的标注（O(n^2)）。尽管现有工作通过主动采样将标注负担降低到O(n log n)，但仍有进一步提高标注效率的需求。", "method": "所提出的EZ-Sort方法通过以下方式提高效率：1) 使用预训练的Contrastive Language-Image Pre-training (CLIP) 模型进行无训练的分层粗略预排序；2) 用自动化比较取代简单、明显的人工比较；3) 初始化桶感知Elo分数；4) 运行不确定性引导的人机协作MergeSort。该方法首先生成基于CLIP的零样本预排序，然后进行后续步骤。", "result": "在FGNET（人脸年龄估计）、DHCI（历史图像年代学）和EyePACS（视网膜图像质量评估）数据集上的验证结果显示，EZ-Sort与穷举pairwise比较相比，人工标注成本降低了90.5%；与现有工作相比（当n=100时），标注成本降低了19.8%，同时改善或保持了评估者间的一致性。", "conclusion": "结合基于CLIP的先验知识与不确定性感知的采样策略，为pairwise排序提供了一种高效且可扩展的解决方案。"}}
{"id": "2508.21741", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.21741", "abs": "https://arxiv.org/abs/2508.21741", "authors": ["Yao Wang", "Di Liang", "Minlong Peng"], "title": "Not All Parameters Are Created Equal: Smart Isolation Boosts Fine-Tuning Performance", "comment": "Accepted to EMNLP 2025 Main Conference", "summary": "Supervised fine-tuning (SFT) is a pivotal approach to adapting large language\nmodels (LLMs) for downstream tasks; however, performance often suffers from the\n``seesaw phenomenon'', where indiscriminate parameter updates yield progress on\ncertain tasks at the expense of others. To address this challenge, we propose a\nnovel \\emph{Core Parameter Isolation Fine-Tuning} (CPI-FT) framework.\nSpecifically, we first independently fine-tune the LLM on each task to identify\nits core parameter regions by quantifying parameter update magnitudes. Tasks\nwith similar core regions are then grouped based on region overlap, forming\nclusters for joint modeling. We further introduce a parameter fusion technique:\nfor each task, core parameters from its individually fine-tuned model are\ndirectly transplanted into a unified backbone, while non-core parameters from\ndifferent tasks are smoothly integrated via Spherical Linear Interpolation\n(SLERP), mitigating destructive interference. A lightweight, pipelined SFT\ntraining phase using mixed-task data is subsequently employed, while freezing\ncore regions from prior tasks to prevent catastrophic forgetting. Extensive\nexperiments on multiple public benchmarks demonstrate that our approach\nsignificantly alleviates task interference and forgetting, consistently\noutperforming vanilla multi-task and multi-stage fine-tuning baselines.", "AI": {"tldr": "针对SFT中存在的“跷跷板现象”，本文提出CPI-FT框架。它通过独立微调识别核心参数区域，对任务进行分组，然后融合参数（核心参数直接移植，非核心参数通过SLERP集成），最后进行轻量级SFT训练并冻结核心区域以防止灾难性遗忘，显著优于现有基线。", "motivation": "监督微调（SFT）在使大型语言模型（LLM）适应下游任务时，常出现“跷跷板现象”，即不加区分的参数更新会导致在某些任务上取得进展，却以牺牲其他任务为代价，从而影响性能。", "method": "本文提出了核心参数隔离微调（CPI-FT）框架：1. 首先独立微调LLM，通过量化参数更新幅度来识别每个任务的核心参数区域。2. 根据核心区域的重叠度对相似任务进行分组，形成联合建模的簇。3. 引入参数融合技术：将每个任务独立微调模型的核参数直接移植到统一骨干网络中，而不同任务的非核参数通过球面线性插值（SLERP）平滑集成，以减轻破坏性干扰。4. 随后采用轻量级、流水线式的SFT训练阶段，使用混合任务数据，并冻结先前任务的核心区域以防止灾难性遗忘。", "result": "在多个公共基准测试上的大量实验表明，该方法显著缓解了任务干扰和遗忘问题，并持续优于传统的多种任务和多阶段微调基线方法。", "conclusion": "CPI-FT框架通过识别、隔离和智能融合核心与非核心参数，有效解决了SFT中的“跷跷板现象”和灾难性遗忘问题，从而在多任务适应中取得了显著的性能提升。"}}
{"id": "2508.21444", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.21444", "abs": "https://arxiv.org/abs/2508.21444", "authors": ["Jiayu Yang", "Weijian Su", "Songqian Zhang", "Yuqi Han", "Jinli Suo", "Qiang Zhang"], "title": "Scale-GS: Efficient Scalable Gaussian Splatting via Redundancy-filtering Training on Streaming Content", "comment": null, "summary": "3D Gaussian Splatting (3DGS) enables high-fidelity real-time rendering, a key\nrequirement for immersive applications. However, the extension of 3DGS to\ndynamic scenes remains limitations on the substantial data volume of dense\nGaussians and the prolonged training time required for each frame. This paper\npresents \\M, a scalable Gaussian Splatting framework designed for efficient\ntraining in streaming tasks. Specifically, Gaussian spheres are hierarchically\norganized by scale within an anchor-based structure. Coarser-level Gaussians\nrepresent the low-resolution structure of the scene, while finer-level\nGaussians, responsible for detailed high-fidelity rendering, are selectively\nactivated by the coarser-level Gaussians. To further reduce computational\noverhead, we introduce a hybrid deformation and spawning strategy that models\nmotion of inter-frame through Gaussian deformation and triggers Gaussian\nspawning to characterize wide-range motion. Additionally, a bidirectional\nadaptive masking mechanism enhances training efficiency by removing static\nregions and prioritizing informative viewpoints. Extensive experiments\ndemonstrate that \\M~ achieves superior visual quality while significantly\nreducing training time compared to state-of-the-art methods.", "AI": {"tldr": "本文提出了一种名为M-GS的可扩展高斯溅射（3DGS）框架，旨在解决动态场景下3DGS数据量大和训练时间长的问题，通过分层结构、混合形变与生成策略以及双向自适应掩码机制，实现了高效训练和高质量渲染。", "motivation": "3D Gaussian Splatting (3DGS) 在沉浸式应用中实现了高保真实时渲染，但将其扩展到动态场景时，面临高斯点数据量庞大以及每帧训练时间过长的限制。", "method": "本文提出了M-GS框架：1) 高斯球体在基于锚点的结构中按比例分层组织，粗级别高斯表示场景低分辨率结构，细级别高斯负责高保真渲染并由粗级别高斯选择性激活。2) 引入混合形变与生成策略，通过高斯形变建模帧间运动，并通过高斯生成来表征大范围运动。3) 采用双向自适应掩码机制，通过移除静态区域和优先处理信息丰富的视角来提高训练效率。", "result": "M-GS在实现卓越视觉质量的同时，与现有最先进方法相比，显著减少了训练时间。", "conclusion": "M-GS框架为动态场景下的高斯溅射提供了一种可扩展且高效的解决方案，有效解决了数据量和训练时间瓶颈，并取得了优异的视觉效果。"}}
{"id": "2508.21693", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.21693", "abs": "https://arxiv.org/abs/2508.21693", "authors": ["Shashank Vempati", "Nishit Anand", "Gaurav Talebailkar", "Arpan Garai", "Chetan Arora"], "title": "Why Stop at Words? Unveiling the Bigger Picture through Line-Level OCR", "comment": "11 pages. Project Website:\n  https://nishitanand.github.io/line-level-ocr-website", "summary": "Conventional optical character recognition (OCR) techniques segmented each\ncharacter and then recognized. This made them prone to error in character\nsegmentation, and devoid of context to exploit language models. Advances in\nsequence to sequence translation in last decade led to modern techniques first\ndetecting words and then inputting one word at a time to a model to directly\noutput full words as sequence of characters. This allowed better utilization of\nlanguage models and bypass error-prone character segmentation step. We observe\nthat the above transition in style has moved the bottleneck in accuracy to word\nsegmentation. Hence, in this paper, we propose a natural and logical\nprogression from word level OCR to line-level OCR. The proposal allows to\nbypass errors in word detection, and provides larger sentence context for\nbetter utilization of language models. We show that the proposed technique not\nonly improves the accuracy but also efficiency of OCR. Despite our thorough\nliterature survey, we did not find any public dataset to train and benchmark\nsuch shift from word to line-level OCR. Hence, we also contribute a\nmeticulously curated dataset of 251 English page images with line-level\nannotations. Our experimentation revealed a notable end-to-end accuracy\nimprovement of 5.4%, underscoring the potential benefits of transitioning\ntowards line-level OCR, especially for document images. We also report a 4\ntimes improvement in efficiency compared to word-based pipelines. With\ncontinuous improvements in large language models, our methodology also holds\npotential to exploit such advances. Project Website:\nhttps://nishitanand.github.io/line-level-ocr-website", "AI": {"tldr": "本文提出了一种行级光学字符识别（OCR）方法，以解决传统词级OCR中词分割错误的问题，并提供更大的上下文来利用语言模型，从而显著提高准确性和效率。此外，还贡献了一个包含行级标注的新数据集。", "motivation": "传统的OCR技术容易出现字符分割错误且缺乏上下文。现代OCR虽然解决了字符分割问题，但将瓶颈转移到了词分割上。为了规避词分割错误并提供更大的句子上下文以更好地利用语言模型，研究者提出从词级OCR向行级OCR的自然演进。", "method": "提出了一种从词级OCR到行级OCR的逻辑演进方法，以绕过词检测错误并提供更大的句子上下文。为此，还创建了一个包含251张带有行级标注的英文页面图像的精心策划的数据集，用于训练和基准测试。", "result": "实验结果显示，所提出的技术不仅将端到端准确率提高了5.4%，而且与基于词的管道相比，效率提高了4倍，尤其适用于文档图像。该方法还具有利用大型语言模型持续改进的潜力。", "conclusion": "行级OCR是OCR技术的一个有前景的自然发展方向，能够显著提高准确性和效率，特别是在文档图像处理中。通过提供更大的上下文，它能更好地利用语言模型，并且随着大型语言模型的进步，其潜力将进一步增强。所贡献的数据集将有助于推动该领域的研究。"}}
{"id": "2508.21762", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.21762", "abs": "https://arxiv.org/abs/2508.21762", "authors": ["Diane Tchuindjo", "Omar Khattab"], "title": "Reasoning-Intensive Regression", "comment": null, "summary": "AI researchers and practitioners increasingly apply large language models\n(LLMs) to what we call reasoning-intensive regression (RiR), i.e. deducing\nsubtle numerical properties from text. Unlike standard language regression\ntasks, e.g. for sentiment or similarity, RiR often appears instead in ad-hoc\nproblems like rubric-based scoring or domain-specific retrieval, where much\ndeeper analysis of text is required while only limited task-specific training\ndata and computation are available. We cast three realistic problems as RiR\ntasks to establish an initial benchmark, and use that to test our hypothesis\nthat prompting frozen LLMs and finetuning Transformer encoders via gradient\ndescent will both often struggle in RiR. We then propose MENTAT, a simple and\nlightweight method that combines batch-reflective prompt optimization with\nneural ensemble learning. MENTAT achieves up to 65% improvement over both\nbaselines, though substantial room remains for future advances in RiR.", "AI": {"tldr": "本文提出了“推理密集型回归”（RiR）任务，并指出LLMs在此类任务中面临的挑战。研究人员建立了一个基准，并提出了MENTAT方法，该方法通过结合批反射提示优化和神经集成学习，显著提升了RiR任务的性能。", "motivation": "AI研究人员越来越多地将大型语言模型（LLMs）应用于推理密集型回归（RiR）任务，即从文本中推断细微的数值属性。与标准语言回归任务不同，RiR通常出现在需要对文本进行更深层次分析的特定问题中（如基于评分标准的评分或领域特定检索），而这些问题往往只有有限的任务特定训练数据和计算资源。研究动机在于，现有的方法（如提示冻结LLMs和通过梯度下降微调Transformer编码器）在RiR任务中表现不佳。", "method": "1. 将三个现实问题转化为RiR任务，建立了一个初始基准。2. 利用该基准测试了现有方法（提示冻结LLMs和微调Transformer编码器）在RiR中表现不佳的假设。3. 提出了MENTAT方法，该方法结合了批反射提示优化（batch-reflective prompt optimization）和神经集成学习（neural ensemble learning）。", "result": "MENTAT方法在RiR任务中，相对于两种基线方法（提示冻结LLMs和微调Transformer编码器）实现了高达65%的性能提升。", "conclusion": "MENTAT方法显著提升了LLMs在推理密集型回归（RiR）任务中的表现，但该领域仍有巨大的进步空间，未来还有待进一步研究和发展。"}}
{"id": "2508.21451", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.21451", "abs": "https://arxiv.org/abs/2508.21451", "authors": ["Junha Song", "Yongsik Jo", "So Yeon Min", "Quanting Xie", "Taehwan Kim", "Yonatan Bisk", "Jaegul Choo"], "title": "One More Glance with Sharp Eyes: Rethinking Lightweight Captioning as a Practical Visual Specialist", "comment": "Project page:\n  https://sites.google.com/view/junha/lightweightcaptioner", "summary": "Image captioning is fundamental for applications like video instruction\nsystems and exploration robots, yet deploying such models on local devices is\nchallenging due to the high computational demands of multimodal large language\nmodels (MLLMs). To address this, we first explore lightweight captioning by\nimplementing a specialist based on a 125M-parameter language model, 56 times\nsmaller than LLaMA-7B, and evaluating its performance on both single-sentence\nand detailed captioning tasks. Surprisingly, we find that our model can achieve\nperformance comparable to large multimodal generalists, suggesting its\npotential to serve as a strong visual specialist for on-device applications.\nWhile promising, our model also exhibits a limitation: like other MLLMs, it\nsuffers from visual blindness, occasionally resulting in semantic captioning\nerrors. We carry out toy experiments and investigate the underlying causes,\nwhere we observe that the problems arise from ineffective attention mechanisms\nand limited visual representations. To alleviate them, we develop a novel\ncaptioning framework, Sharp-Eyed Refinement, which enhances caption quality\nthrough improved visual grounding. At its core, our DeepLens extracts detailed\nvisual representations by concentrating on informative regions identified\nduring the initial glance. Our experiments confirm both the advantages of our\nspecialist over prior small captioning models and large generalists and the\neffectiveness of our framework.", "AI": {"tldr": "本文提出了一种轻量级图像字幕专家模型，其性能可与大型多模态通用模型媲美，解决了设备端部署的计算挑战。针对模型存在的“视觉盲区”问题，通过分析其根本原因并开发了名为“锐眼细化”（Sharp-Eyed Refinement）的新框架，显著提升了视觉接地能力和字幕质量。", "motivation": "由于多模态大型语言模型（MLLMs）计算需求高，在本地设备上部署图像字幕模型极具挑战性。研究旨在探索轻量级解决方案，以实现设备端高效的图像字幕生成。", "method": "首先，实现了一个基于1.25亿参数语言模型的轻量级专家模型（比LLaMA-7B小56倍），并评估了其在单句和详细字幕任务上的表现。其次，通过玩具实验调查了模型视觉盲区（语义字幕错误）的根本原因，发现问题源于注意力机制效率低下和视觉表征有限。最后，开发了一种名为“锐眼细化”（Sharp-Eyed Refinement）的新型字幕框架，其核心是DeepLens模块，通过关注初始识别的信息区域来提取详细的视觉表征，从而增强视觉接地能力。", "result": "研究发现，所提出的轻量级模型能够实现与大型多模态通用模型相当的性能，显示了其作为设备端视觉专家的潜力。尽管如此，模型也表现出视觉盲区，导致语义字幕错误。实验确认这些问题源于注意力机制的低效和视觉表征的局限性。新开发的“锐眼细化”框架和DeepLens模块有效提升了字幕质量，并且该专家模型优于先前的轻量级字幕模型和大型通用模型，证明了框架的有效性。", "conclusion": "轻量级专家模型在设备端图像字幕应用中具有巨大潜力，其性能可与大型通用模型媲美。虽然存在视觉盲区，但通过深入分析其原因并开发“锐眼细化”框架，可以有效解决注意力机制和视觉表征的局限性，显著提升视觉接地能力和字幕质量。"}}
{"id": "2508.21732", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.21732", "abs": "https://arxiv.org/abs/2508.21732", "authors": ["João Valente", "Atabak Dehban", "Rodrigo Ventura"], "title": "CAD2DMD-SET: Synthetic Generation Tool of Digital Measurement Device CAD Model Datasets for fine-tuning Large Vision-Language Models", "comment": null, "summary": "Recent advancements in Large Vision-Language Models (LVLMs) have demonstrated\nimpressive capabilities across various multimodal tasks. They continue,\nhowever, to struggle with trivial scenarios such as reading values from Digital\nMeasurement Devices (DMDs), particularly in real-world conditions involving\nclutter, occlusions, extreme viewpoints, and motion blur; common in\nhead-mounted cameras and Augmented Reality (AR) applications. Motivated by\nthese limitations, this work introduces CAD2DMD-SET, a synthetic data\ngeneration tool designed to support visual question answering (VQA) tasks\ninvolving DMDs. By leveraging 3D CAD models, advanced rendering, and\nhigh-fidelity image composition, our tool produces diverse, VQA-labelled\nsynthetic DMD datasets suitable for fine-tuning LVLMs. Additionally, we present\nDMDBench, a curated validation set of 1,000 annotated real-world images\ndesigned to evaluate model performance under practical constraints.\nBenchmarking three state-of-the-art LVLMs using Average Normalised Levenshtein\nSimilarity (ANLS) and further fine-tuning LoRA's of these models with\nCAD2DMD-SET's generated dataset yielded substantial improvements, with InternVL\nshowcasing a score increase of 200% without degrading on other tasks. This\ndemonstrates that the CAD2DMD-SET training dataset substantially improves the\nrobustness and performance of LVLMs when operating under the previously stated\nchallenging conditions. The CAD2DMD-SET tool is expected to be released as\nopen-source once the final version of this manuscript is prepared, allowing the\ncommunity to add different measurement devices and generate their own datasets.", "AI": {"tldr": "该研究引入了CAD2DMD-SET工具，用于生成合成数据以训练大型视觉-语言模型（LVLMs）从数字测量设备（DMDs）中读取数值，并在真实世界挑战下显著提高了LVLMs的性能和鲁棒性。", "motivation": "尽管大型视觉-语言模型（LVLMs）在多模态任务中表现出色，但它们在读取数字测量设备（DMDs）的简单场景中仍面临困难，尤其是在真实世界中常见的杂乱、遮挡、极端视角和运动模糊等挑战性条件下，这些在头戴式相机和增强现实（AR）应用中尤为普遍。", "method": "本研究引入了CAD2DMD-SET，一个利用3D CAD模型、高级渲染和高保真图像合成技术，生成用于DMD相关视觉问答（VQA）任务的合成VQA标注数据集的工具。此外，还提出了DMDBench，一个包含1,000张真实世界标注图像的验证集，用于评估模型在实际约束下的性能。研究通过使用平均归一化莱文斯坦相似度（ANLS）基准测试了三个最先进的LVLMs，并使用CAD2DMD-SET生成的数据集对这些模型的LoRA进行了微调。", "result": "使用CAD2DMD-SET生成的数据集对模型进行微调后，LVLMs的性能得到了显著提升。例如，InternVL的得分提高了200%，且未在其他任务上出现性能下降。这表明CAD2DMD-SET训练数据集显著提高了LVLMs在先前所述的挑战性条件下操作时的鲁棒性和性能。", "conclusion": "CAD2DMD-SET训练数据集能够显著提高大型视觉-语言模型在处理数字测量设备相关任务时，面对复杂真实世界条件（如杂乱、遮挡、极端视角和运动模糊）时的鲁棒性和性能。该工具预计将开源，以促进社区扩展和生成更多数据集。"}}
{"id": "2508.21787", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.21787", "abs": "https://arxiv.org/abs/2508.21787", "authors": ["Joshua Ong Jun Leang", "Zheng Zhao", "Aryo Pradipta Gema", "Sohee Yang", "Wai-Chung Kwan", "Xuanli He", "Wenda Li", "Pasquale Minervini", "Eleonora Giunchiglia", "Shay B. Cohen"], "title": "PiCSAR: Probabilistic Confidence Selection And Ranking", "comment": null, "summary": "Best-of-n sampling improves the accuracy of large language models (LLMs) and\nlarge reasoning models (LRMs) by generating multiple candidate solutions and\nselecting the one with the highest reward. The key challenge for reasoning\ntasks is designing a scoring function that can identify correct reasoning\nchains without access to ground-truth answers. We propose Probabilistic\nConfidence Selection And Ranking (PiCSAR): a simple, training-free method that\nscores each candidate generation using the joint log-likelihood of the\nreasoning and final answer. The joint log-likelihood of the reasoning and final\nanswer naturally decomposes into reasoning confidence and answer confidence.\nPiCSAR achieves substantial gains across diverse benchmarks (+10.18 on MATH500,\n+9.81 on AIME2025), outperforming baselines with at least 2x fewer samples in\n16 out of 20 comparisons. Our analysis reveals that correct reasoning chains\nexhibit significantly higher reasoning and answer confidence, justifying the\neffectiveness of PiCSAR.", "AI": {"tldr": "PiCSAR是一种无需训练的方法，通过结合推理和最终答案的联合对数似然来为LLM/LRM的最佳n选一采样评分，显著提高了推理任务的准确性，并在多个基准测试中表现优异。", "motivation": "在大型语言模型（LLMs）和大型推理模型（LRMs）的最佳n选一采样中，核心挑战是如何在没有真值答案的情况下，设计一个能够识别正确推理链的评分函数。", "method": "本文提出了概率置信度选择与排序（PiCSAR），这是一种简单、无需训练的方法。它通过使用推理和最终答案的联合对数似然来评估每个候选生成，该联合对数似然自然分解为推理置信度和答案置信度。", "result": "PiCSAR在多个基准测试（如MATH500和AIME2025）上取得了显著提升（MATH500提高10.18，AIME2025提高9.81），在20项比较中的16项中，使用至少两倍少的样本优于基线。分析表明，正确的推理链展现出显著更高的推理和答案置信度。", "conclusion": "PiCSAR的有效性在于，正确的推理链通常表现出更高的推理置信度和答案置信度，这证明了该方法的合理性。"}}
{"id": "2508.21458", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.21458", "abs": "https://arxiv.org/abs/2508.21458", "authors": ["Kaouther Mouheb", "Marawan Elbatel", "Janne Papma", "Geert Jan Biessels", "Jurgen Claassen", "Huub Middelkoop", "Barbara van Munster", "Wiesje van der Flier", "Inez Ramakers", "Stefan Klein", "Esther E. Bron"], "title": "Federated Fine-tuning of SAM-Med3D for MRI-based Dementia Classification", "comment": "Accepted at the MICCAI 2025 Workshop on Distributed, Collaborative\n  and Federated Learning (DeCAF)", "summary": "While foundation models (FMs) offer strong potential for AI-based dementia\ndiagnosis, their integration into federated learning (FL) systems remains\nunderexplored. In this benchmarking study, we systematically evaluate the\nimpact of key design choices: classification head architecture, fine-tuning\nstrategy, and aggregation method, on the performance and efficiency of\nfederated FM tuning using brain MRI data. Using a large multi-cohort dataset,\nwe find that the architecture of the classification head substantially\ninfluences performance, freezing the FM encoder achieves comparable results to\nfull fine-tuning, and advanced aggregation methods outperform standard\nfederated averaging. Our results offer practical insights for deploying FMs in\ndecentralized clinical settings and highlight trade-offs that should guide\nfuture method development.", "AI": {"tldr": "本研究系统评估了联邦学习中基础模型（FM）微调的关键设计选择（分类头架构、微调策略、聚合方法）对基于脑部MRI的痴呆诊断性能和效率的影响。", "motivation": "基础模型在AI辅助痴呆诊断方面潜力巨大，但它们与联邦学习（FL）系统的集成尚未得到充分探索。", "method": "通过一项基准研究，使用大型多队列数据集，系统评估了分类头架构、微调策略（例如冻结编码器与完全微调）和聚合方法（例如高级聚合方法与标准联邦平均）对联邦FM微调性能和效率的影响，数据源为脑部MRI。", "result": "研究发现分类头架构显著影响性能；冻结FM编码器能达到与完全微调相当的结果；高级聚合方法优于标准联邦平均。", "conclusion": "研究结果为在去中心化临床环境中部署基础模型提供了实用见解，并强调了应指导未来方法开发的权衡考量。"}}
{"id": "2508.21773", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.21773", "abs": "https://arxiv.org/abs/2508.21773", "authors": ["Nattapong Kurpukdee", "Adrian G. Bors"], "title": "Unsupervised Video Continual Learning via Non-Parametric Deep Embedded Clustering", "comment": "Accepted to The 36th British Machine Vision Conference (BMVC 2025),\n  Sheffield, UK", "summary": "We propose a realistic scenario for the unsupervised video learning where\nneither task boundaries nor labels are provided when learning a succession of\ntasks. We also provide a non-parametric learning solution for the\nunder-explored problem of unsupervised video continual learning. Videos\nrepresent a complex and rich spatio-temporal media information, widely used in\nmany applications, but which have not been sufficiently explored in\nunsupervised continual learning. Prior studies have only focused on supervised\ncontinual learning, relying on the knowledge of labels and task boundaries,\nwhile having labeled data is costly and not practical. To address this gap, we\nstudy the unsupervised video continual learning (uVCL). uVCL raises more\nchallenges due to the additional computational and memory requirements of\nprocessing videos when compared to images. We introduce a general benchmark\nexperimental protocol for uVCL by considering the learning of unstructured\nvideo data categories during each task. We propose to use the Kernel Density\nEstimation (KDE) of deep embedded video features extracted by unsupervised\nvideo transformer networks as a non-parametric probabilistic representation of\nthe data. We introduce a novelty detection criterion for the incoming new task\ndata, dynamically enabling the expansion of memory clusters, aiming to capture\nnew knowledge when learning a succession of tasks. We leverage the use of\ntransfer learning from the previous tasks as an initial state for the knowledge\ntransfer to the current learning task. We found that the proposed methodology\nsubstantially enhances the performance of the model when successively learning\nmany tasks. We perform in-depth evaluations on three standard video action\nrecognition datasets, including UCF101, HMDB51, and Something-to-Something V2,\nwithout using any labels or class boundaries.", "AI": {"tldr": "本文提出了一种无监督视频持续学习（uVCL）的现实场景和非参数解决方案，无需任务边界和标签。它利用深度视频特征的核密度估计（KDE）进行数据表示，并通过新颖性检测动态扩展知识，显著提升了模型在连续学习多个任务时的性能。", "motivation": "现有的持续学习研究主要集中在有监督场景，依赖于标签和任务边界，但这既昂贵又不切实际。无监督视频持续学习（uVCL）是一个未被充分探索的问题，尤其考虑到视频数据在计算和内存方面的额外挑战，因此需要填补这一空白。", "method": "本文提出了一个通用的uVCL基准实验协议，并引入了一种非参数学习解决方案。该方案使用无监督视频Transformer网络提取的深度嵌入视频特征的核密度估计（KDE）作为数据概率表示。它还引入了新颖性检测准则，用于识别新任务数据并动态扩展内存集群以捕获新知识，并利用先前任务的迁移学习作为当前学习任务的初始状态。", "result": "实验结果表明，所提出的方法在连续学习多个任务时，显著提升了模型的性能。在UCF101、HMDB51和Something-to-Something V2三个标准视频动作识别数据集上进行了深入评估，且全程未使用任何标签或类别边界。", "conclusion": "本文成功地提出了一个针对无监督视频持续学习的现实场景和非参数解决方案，通过结合KDE、新颖性检测和迁移学习，有效解决了在无监督环境下连续学习复杂视频数据的挑战，并取得了显著的性能提升。"}}
{"id": "2508.21788", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.21788", "abs": "https://arxiv.org/abs/2508.21788", "authors": ["Inés Altemir Marinas", "Anastasiia Kucherenko", "Andrei Kucharavy"], "title": "Going over Fine Web with a Fine-Tooth Comb: Technical Report of Indexing Fine Web for Problematic Content Search and Retrieval", "comment": null, "summary": "Large language models (LLMs) rely heavily on web-scale datasets like Common\nCrawl, which provides over 80\\% of training data for some modern models.\nHowever, the indiscriminate nature of web crawling raises challenges in data\nquality, safety, and ethics. Despite the critical importance of training data\nquality, prior research on harmful content has been limited to small samples\ndue to computational constraints. This project presents a framework for\nindexing and analyzing LLM training datasets using an ElasticSearch-based\npipeline. We apply it to SwissAI's FineWeb-2 corpus (1.5TB, four languages),\nachieving fast query performance--most searches in milliseconds, all under 2\nseconds. Our work demonstrates real-time dataset analysis, offering practical\ntools for safer, more accountable AI systems.", "AI": {"tldr": "本文提出了一个基于ElasticSearch的框架，用于索引和分析大型语言模型（LLM）的训练数据集，以解决数据质量、安全和伦理问题，并实现了快速查询性能。", "motivation": "大型语言模型严重依赖网络规模数据集（如Common Crawl），但其无差别抓取导致数据质量、安全和伦理挑战。此前对有害内容的研究受限于计算能力，只能分析小样本，无法满足关键的训练数据质量需求。", "method": "项目开发了一个基于ElasticSearch的管道，用于索引和分析LLM训练数据集。", "result": "将该框架应用于SwissAI的FineWeb-2语料库（1.5TB，四种语言），实现了快速查询性能——大多数搜索在毫秒级完成，所有搜索均在2秒内完成。这证明了实时数据集分析的可行性。", "conclusion": "该工作展示了实时数据集分析能力，为构建更安全、更负责任的AI系统提供了实用工具。"}}
{"id": "2508.21463", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.21463", "abs": "https://arxiv.org/abs/2508.21463", "authors": ["Lucas Rakotoarivony"], "title": "Multi-Method Ensemble for Out-of-Distribution Detection", "comment": "Accepted paper for BMVC 2025", "summary": "Detecting out-of-distribution (OOD) samples is essential for neural networks\noperating in open-world settings, particularly in safety-critical applications.\nExisting methods have improved OOD detection by leveraging two main techniques:\nfeature truncation, which increases the separation between in-distribution (ID)\nand OOD samples, and scoring functions, which assign scores to distinguish\nbetween ID and OOD data. However, most approaches either focus on a single\nfamily of techniques or evaluate their effectiveness on a specific type of OOD\ndataset, overlooking the potential of combining multiple existing solutions.\nMotivated by this observation, we theoretically and empirically demonstrate\nthat state-of-the-art feature truncation and scoring functions can be\neffectively combined. Moreover, we show that aggregating multiple scoring\nfunctions enhances robustness against various types of OOD samples. Based on\nthese insights, we propose the Multi-Method Ensemble (MME) score, which unifies\nstate-of-the-art OOD detectors into a single, more effective scoring function.\nExtensive experiments on both large-scale and small-scale benchmarks, covering\nnear-OOD and far-OOD scenarios, show that MME significantly outperforms recent\nstate-of-the-art methods across all benchmarks. Notably, using the BiT model,\nour method achieves an average FPR95 of 27.57% on the challenging ImageNet-1K\nbenchmark, improving performance by 6% over the best existing baseline.", "AI": {"tldr": "该论文提出了一种名为多方法集成（MME）的新型OOD检测分数，通过理论和实证证明了现有特征截断和评分函数的有效结合，并聚合多个评分函数以提高鲁棒性，在各类基准测试中显著超越了现有最先进的方法。", "motivation": "在开放世界环境中，特别是安全关键应用中，检测分布外（OOD）样本对神经网络至关重要。现有方法主要侧重于单一技术家族或特定OOD数据集类型，忽略了结合多种现有解决方案的潜力，这促使研究者探索更有效的组合方法。", "method": "该研究理论和实证地证明了最先进的特征截断和评分函数可以有效结合。此外，它还展示了聚合多个评分函数能增强对各种OOD样本的鲁棒性。基于这些见解，论文提出了多方法集成（MME）分数，该分数将最先进的OOD检测器统一为一个更有效的评分函数。", "result": "MME在涵盖近OOD和远OOD场景的大规模和小规模基准测试中，显著优于所有基准上的最新最先进方法。值得注意的是，使用BiT模型，该方法在具有挑战性的ImageNet-1K基准上实现了27.57%的平均FPR95，比现有最佳基线提高了6%。", "conclusion": "通过有效结合最先进的特征截断和评分函数，并聚合多个评分函数，所提出的多方法集成（MME）分数能够统一并显著提升OOD检测的性能和鲁棒性，在广泛的OOD场景中超越了现有技术。"}}
{"id": "2508.21777", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.21777", "abs": "https://arxiv.org/abs/2508.21777", "authors": ["Ugur Dinc", "Jibak Sarkar", "Philipp Schubert", "Sabine Semrau", "Thomas Weissmann", "Andre Karius", "Johann Brand", "Bernd-Niklas Axer", "Ahmed Gomaa", "Pluvio Stephan", "Ishita Sheth", "Sogand Beirami", "Annette Schwarz", "Udo Gaipl", "Benjamin Frey", "Christoph Bert", "Stefanie Corradini", "Rainer Fietkau", "Florian Putz"], "title": "Benchmarking GPT-5 in Radiation Oncology: Measurable Gains, but Persistent Need for Expert Oversight", "comment": "Under review in Frontiers in Artificial Intelligence", "summary": "Introduction: Large language models (LLM) have shown great potential in\nclinical decision support. GPT-5 is a novel LLM system that has been\nspecifically marketed towards oncology use.\n  Methods: Performance was assessed using two complementary benchmarks: (i) the\nACR Radiation Oncology In-Training Examination (TXIT, 2021), comprising 300\nmultiple-choice items, and (ii) a curated set of 60 authentic radiation\noncologic vignettes representing diverse disease sites and treatment\nindications. For the vignette evaluation, GPT-5 was instructed to generate\nconcise therapeutic plans. Four board-certified radiation oncologists rated\ncorrectness, comprehensiveness, and hallucinations. Inter-rater reliability was\nquantified using Fleiss' \\k{appa}.\n  Results: On the TXIT benchmark, GPT-5 achieved a mean accuracy of 92.8%,\noutperforming GPT-4 (78.8%) and GPT-3.5 (62.1%). Domain-specific gains were\nmost pronounced in Dose and Diagnosis. In the vignette evaluation, GPT-5's\ntreatment recommendations were rated highly for correctness (mean 3.24/4, 95%\nCI: 3.11-3.38) and comprehensiveness (3.59/4, 95% CI: 3.49-3.69).\nHallucinations were rare with no case reaching majority consensus for their\npresence. Inter-rater agreement was low (Fleiss' \\k{appa} 0.083 for\ncorrectness), reflecting inherent variability in clinical judgment. Errors\nclustered in complex scenarios requiring precise trial knowledge or detailed\nclinical adaptation.\n  Discussion: GPT-5 clearly outperformed prior model variants on the radiation\noncology multiple-choice benchmark. Although GPT-5 exhibited favorable\nperformance in generating real-world radiation oncology treatment\nrecommendations, correctness ratings indicate room for further improvement.\nWhile hallucinations were infrequent, the presence of substantive errors\nunderscores that GPT-5-generated recommendations require rigorous expert\noversight before clinical implementation.", "AI": {"tldr": "GPT-5在放射肿瘤学多项选择题基准测试中表现出色，优于早期模型，并在临床病例推荐中获得较高评价，但仍需专家监督。", "motivation": "大型语言模型（LLM）在临床决策支持方面展现出巨大潜力，而GPT-5是专为肿瘤学应用设计的新型LLM系统。", "method": "研究采用两种互补的基准进行评估：(i) 包含300道多项选择题的ACR放射肿瘤学在职培训考试（TXIT, 2021）；(ii) 包含60个真实放射肿瘤学临床病例的精选集。对于临床病例，GPT-5被要求生成简洁的治疗方案，并由四位获得委员会认证的放射肿瘤学家评估其正确性、全面性和幻觉情况。使用Fleiss' kappa量化评估者间的一致性。", "result": "在TXIT基准测试中，GPT-5的平均准确率为92.8%，优于GPT-4（78.8%）和GPT-3.5（62.1%）。在临床病例评估中，GPT-5的治疗建议在正确性（平均3.24/4）和全面性（3.59/4）方面获得高度评价。幻觉情况罕见。评估者间一致性较低（Fleiss' kappa 0.083），反映了临床判断的固有变异性。错误主要集中在需要精确试验知识或详细临床适应的复杂场景。", "conclusion": "GPT-5在放射肿瘤学多项选择题基准测试中明显优于先前的模型版本。尽管GPT-5在生成真实世界放射肿瘤学治疗建议方面表现良好，但正确性评分表明仍有改进空间。虽然幻觉不常见，但实质性错误的存在强调了GPT-5生成的建议在临床实施前需要严格的专家监督。"}}
{"id": "2508.21472", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.21472", "abs": "https://arxiv.org/abs/2508.21472", "authors": ["Chun Liu", "Panpan Ding", "Zheng Zheng", "Hailong Wang", "Bingqian Zhu", "Tao Xu", "Zhigang Han", "Jiayao Wang"], "title": "Adversarial Patch Attack for Ship Detection via Localized Augmentation", "comment": null, "summary": "Current ship detection techniques based on remote sensing imagery primarily\nrely on the object detection capabilities of deep neural networks (DNNs).\nHowever, DNNs are vulnerable to adversarial patch attacks, which can lead to\nmisclassification by the detection model or complete evasion of the targets.\nNumerous studies have demonstrated that data transformation-based methods can\nimprove the transferability of adversarial examples. However, excessive\naugmentation of image backgrounds or irrelevant regions may introduce\nunnecessary interference, resulting in false detections of the object detection\nmodel. These errors are not caused by the adversarial patches themselves but\nrather by the over-augmentation of background and non-target areas. This paper\nproposes a localized augmentation method that applies augmentation only to the\ntarget regions, avoiding any influence on non-target areas. By reducing\nbackground interference, this approach enables the loss function to focus more\ndirectly on the impact of the adversarial patch on the detection model, thereby\nimproving the attack success rate. Experiments conducted on the HRSC2016\ndataset demonstrate that the proposed method effectively increases the success\nrate of adversarial patch attacks and enhances their transferability.", "AI": {"tldr": "本文提出了一种局部增强方法，通过仅对目标区域应用增强来减少背景干扰，从而提高对抗性补丁攻击对遥感图像舰船检测模型的成功率和可迁移性。", "motivation": "当前基于深度神经网络的舰船检测技术易受对抗性补丁攻击。现有数据转换方法虽然能提高对抗样本的可迁移性，但过度增强图像背景或无关区域会引入不必要的干扰，导致检测模型出现与对抗补丁本身无关的错误检测。因此，需要一种更精确的增强方法来提高攻击效率并避免背景干扰。", "method": "本文提出了一种局部增强方法。该方法仅将增强应用于目标区域，避免对非目标区域产生任何影响。通过减少背景干扰，该方法使损失函数能够更直接地关注对抗性补丁对检测模型的影响。", "result": "在HRSC2016数据集上进行的实验表明，所提出的方法有效地提高了对抗性补丁攻击的成功率，并增强了其可迁移性。", "conclusion": "局部增强方法能够有效提升对抗性补丁攻击对舰船检测模型的成功率和可迁移性，其关键在于减少背景干扰，使攻击更聚焦于目标区域。"}}
{"id": "2508.21795", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.21795", "abs": "https://arxiv.org/abs/2508.21795", "authors": ["Jiawei Liu", "Jiahe Hou", "Wei Wang", "Jinsong Du", "Yang Cong", "Huijie Fan"], "title": "TMUAD: Enhancing Logical Capabilities in Unified Anomaly Detection Models with a Text Memory Bank", "comment": null, "summary": "Anomaly detection, which aims to identify anomalies deviating from normal\npatterns, is challenging due to the limited amount of normal data available.\nUnlike most existing unified methods that rely on carefully designed image\nfeature extractors and memory banks to capture logical relationships between\nobjects, we introduce a text memory bank to enhance the detection of logical\nanomalies. Specifically, we propose a Three-Memory framework for Unified\nstructural and logical Anomaly Detection (TMUAD). First, we build a class-level\ntext memory bank for logical anomaly detection by the proposed logic-aware text\nextractor, which can capture rich logical descriptions of objects from input\nimages. Second, we construct an object-level image memory bank that preserves\ncomplete object contours by extracting features from segmented objects. Third,\nwe employ visual encoders to extract patch-level image features for\nconstructing a patch-level memory bank for structural anomaly detection. These\nthree complementary memory banks are used to retrieve and compare normal images\nthat are most similar to the query image, compute anomaly scores at multiple\nlevels, and fuse them into a final anomaly score. By unifying structural and\nlogical anomaly detection through collaborative memory banks, TMUAD achieves\nstate-of-the-art performance across seven publicly available datasets involving\nindustrial and medical domains. The model and code are available at\nhttps://github.com/SIA-IDE/TMUAD.", "AI": {"tldr": "本文提出了一种名为TMUAD的三记忆框架，用于统一结构和逻辑异常检测。它通过构建类级别文本记忆库、对象级别图像记忆库和补丁级别图像记忆库，整合多模态信息，实现了对工业和医疗领域异常的有效识别。", "motivation": "异常检测面临正常数据量有限的挑战，现有统一方法主要依赖图像特征提取器和记忆库来捕获对象间的逻辑关系。本文旨在通过引入文本记忆库来增强逻辑异常检测能力，并实现结构和逻辑异常的统一检测。", "method": "本文提出了TMUAD框架。首先，通过逻辑感知文本提取器构建类级别文本记忆库，用于逻辑异常检测。其次，通过从分割对象中提取特征，构建对象级别图像记忆库，以保留完整的对象轮廓。最后，利用视觉编码器提取补丁级别图像特征，构建补丁级别记忆库，用于结构异常检测。这三个互补的记忆库用于检索和比较与查询图像最相似的正常图像，计算多层次异常分数，并将其融合为最终异常分数。", "result": "通过协同记忆库统一结构和逻辑异常检测，TMUAD在七个公开的工业和医疗领域数据集中取得了最先进的性能。", "conclusion": "TMUAD框架通过其独特的三记忆库设计，成功地统一了结构和逻辑异常检测，并在多个领域展示了卓越的性能，证明了多模态记忆库协同作用在异常检测中的有效性。"}}
{"id": "2508.21529", "categories": ["cs.CV", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.21529", "abs": "https://arxiv.org/abs/2508.21529", "authors": ["Ronan Docherty", "Antonis Vamvakeros", "Samuel J. Cooper"], "title": "Maybe you don't need a U-Net: convolutional feature upsampling for materials micrograph segmentation", "comment": null, "summary": "Feature foundation models - usually vision transformers - offer rich semantic\ndescriptors of images, useful for downstream tasks such as (interactive)\nsegmentation and object detection. For computational efficiency these\ndescriptors are often patch-based, and so struggle to represent the fine\nfeatures often present in micrographs; they also struggle with the large image\nsizes present in materials and biological image analysis. In this work, we\ntrain a convolutional neural network to upsample low-resolution (i.e, large\npatch size) foundation model features with reference to the input image. We\napply this upsampler network (without any further training) to efficiently\nfeaturise and then segment a variety of microscopy images, including plant\ncells, a lithium-ion battery cathode and organic crystals. The richness of\nthese upsampled features admits separation of hard to segment phases, like\nhairline cracks. We demonstrate that interactive segmentation with these deep\nfeatures produces high-quality segmentations far faster and with far fewer\nlabels than training or finetuning a more traditional convolutional network.", "AI": {"tldr": "本文提出了一种卷积神经网络，用于上采样视觉基础模型在显微图像上的低分辨率特征，以解决现有模型在处理精细特征和大型图像时的不足，从而实现更高效、高质量的显微图像分割。", "motivation": "现有的特征基础模型（通常是视觉Transformer）的描述符是基于图像块的，这使得它们难以表示显微图像中常见的精细特征，并且难以处理材料和生物图像分析中的大型图像，导致计算效率低下。", "method": "研究人员训练了一个卷积神经网络（CNN），以输入图像为参考，上采样低分辨率（即大图像块尺寸）的基础模型特征。这个上采样网络在训练后无需进一步训练，即可用于特征化和分割多种显微图像。", "result": "该上采样网络能够高效地对植物细胞、锂离子电池阴极和有机晶体等多种显微图像进行特征化和分割。上采样特征的丰富性使得难以分割的相（如细微裂纹）也能被有效分离。此外，使用这些深度特征进行交互式分割，与训练或微调传统卷积网络相比，能以更少的标签更快地生成高质量的分割结果。", "conclusion": "所提出的上采样网络有效解决了现有基础模型在处理显微图像精细特征和大型图像时的局限性，显著提高了显微图像分割的效率和质量，尤其在交互式分割任务中表现出色。"}}
{"id": "2508.21816", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.21816", "abs": "https://arxiv.org/abs/2508.21816", "authors": ["Yiming Lin", "Yuchen Niu", "Shang Wang", "Kaizhu Huang", "Qiufeng Wang", "Xiao-Bo Jin"], "title": "The Demon is in Ambiguity: Revisiting Situation Recognition with Single Positive Multi-Label Learning", "comment": "Accepted by ICDM 2025", "summary": "Context recognition (SR) is a fundamental task in computer vision that aims\nto extract structured semantic summaries from images by identifying key events\nand their associated entities. Specifically, given an input image, the model\nmust first classify the main visual events (verb classification), then identify\nthe participating entities and their semantic roles (semantic role labeling),\nand finally localize these entities in the image (semantic role localization).\nExisting methods treat verb classification as a single-label problem, but we\nshow through a comprehensive analysis that this formulation fails to address\nthe inherent ambiguity in visual event recognition, as multiple verb categories\nmay reasonably describe the same image. This paper makes three key\ncontributions: First, we reveal through empirical analysis that verb\nclassification is inherently a multi-label problem due to the ubiquitous\nsemantic overlap between verb categories. Second, given the impracticality of\nfully annotating large-scale datasets with multiple labels, we propose to\nreformulate verb classification as a single positive multi-label learning\n(SPMLL) problem - a novel perspective in SR research. Third, we design a\ncomprehensive multi-label evaluation benchmark for SR that is carefully\ndesigned to fairly evaluate model performance in a multi-label setting. To\naddress the challenges of SPMLL, we futher develop the Graph Enhanced Verb\nMultilayer Perceptron (GE-VerbMLP), which combines graph neural networks to\ncapture label correlations and adversarial training to optimize decision\nboundaries. Extensive experiments on real-world datasets show that our approach\nachieves more than 3\\% MAP improvement while remaining competitive on\ntraditional top-1 and top-5 accuracy metrics.", "AI": {"tldr": "本文指出场景识别（SR）中的动词分类本质上是多标签问题，现有方法将其视为单标签。作者提出了单正多标签学习（SPMLL）范式，并开发了结合图神经网络和对抗训练的GE-VerbMLP模型，同时设计了多标签评估基准，显著提升了多标签性能。", "motivation": "现有场景识别方法将动词分类视为单标签问题，未能解决视觉事件固有的模糊性，即同一图像可能被多个动词合理描述。此外，对大规模数据集进行完全多标签标注不切实际。", "method": "1. 经验性分析揭示动词分类本质上是多标签问题。2. 将动词分类重构为单正多标签学习（SPMLL）问题。3. 设计了一个全面的多标签评估基准。4. 开发了图增强动词多层感知机（GE-VerbMLP），结合图神经网络捕获标签关联性，并利用对抗训练优化决策边界。", "result": "在真实世界数据集上，该方法实现了超过3%的平均精度均值（MAP）提升，同时在传统的Top-1和Top-5准确率指标上保持竞争力。", "conclusion": "动词分类在场景识别中本质上是多标签问题。通过将动词分类重新定义为SPMLL问题，并引入GE-VerbMLP模型以及新的多标签评估基准，能够有效处理视觉事件识别中的多标签模糊性，显著提升多标签性能。"}}
{"id": "2508.21539", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.21539", "abs": "https://arxiv.org/abs/2508.21539", "authors": ["Hao Ruan", "Jinliang Lin", "Yingxin Lai", "Zhiming Luo", "Shaozi Li"], "title": "HCCM: Hierarchical Cross-Granularity Contrastive and Matching Learning for Natural Language-Guided Drones", "comment": "Accepted by ACM MM'25", "summary": "Natural Language-Guided Drones (NLGD) provide a novel paradigm for tasks such\nas target matching and navigation. However, the wide field of view and complex\ncompositional semantics in drone scenarios pose challenges for vision-language\nunderstanding. Mainstream Vision-Language Models (VLMs) emphasize global\nalignment while lacking fine-grained semantics, and existing hierarchical\nmethods depend on precise entity partitioning and strict containment, limiting\neffectiveness in dynamic environments. To address this, we propose the\nHierarchical Cross-Granularity Contrastive and Matching learning (HCCM)\nframework with two components: (1) Region-Global Image-Text Contrastive\nLearning (RG-ITC), which avoids precise scene partitioning and captures\nhierarchical local-to-global semantics by contrasting local visual regions with\nglobal text and vice versa; (2) Region-Global Image-Text Matching (RG-ITM),\nwhich dispenses with rigid constraints and instead evaluates local semantic\nconsistency within global cross-modal representations, enhancing compositional\nreasoning. Moreover, drone text descriptions are often incomplete or ambiguous,\ndestabilizing alignment. HCCM introduces a Momentum Contrast and Distillation\n(MCD) mechanism to improve robustness. Experiments on GeoText-1652 show HCCM\nachieves state-of-the-art Recall@1 of 28.8% (image retrieval) and 14.7% (text\nretrieval). On the unseen ERA dataset, HCCM demonstrates strong zero-shot\ngeneralization with 39.93% mean recall (mR), outperforming fine-tuned\nbaselines.", "AI": {"tldr": "针对自然语言引导无人机（NLGD）中宽视场和复杂组合语义带来的视觉-语言理解挑战，本文提出HCCM框架。该框架通过跨粒度对比与匹配学习，避免精确场景划分，增强组合推理，并引入MCD机制提升鲁棒性，在GeoText-1652和ERA数据集上均达到SOTA性能和强大的零样本泛化能力。", "motivation": "自然语言引导无人机（NLGD）在目标匹配和导航等任务中面临挑战，主要原因包括：无人机场景的宽视场和复杂组合语义给视觉-语言理解带来困难；主流视觉-语言模型（VLM）侧重全局对齐而缺乏细粒度语义；现有分层方法依赖精确实体划分和严格包含，在动态环境中受限；无人机文本描述常不完整或模糊，导致对齐不稳定。", "method": "本文提出了分层跨粒度对比和匹配学习（HCCM）框架，包含两个核心组件：1. 区域-全局图像-文本对比学习（RG-ITC），通过对比局部视觉区域与全局文本（反之亦然）来避免精确场景划分，捕捉分层局部到全局的语义；2. 区域-全局图像-文本匹配（RG-ITM），通过评估全局跨模态表示中的局部语义一致性来增强组合推理，无需严格约束。此外，HCCM还引入了动量对比和蒸馏（MCD）机制，以提高对不完整或模糊文本描述的鲁棒性。", "result": "HCCM在GeoText-1652数据集上取得了最先进的（SOTA）性能：图像检索的Recall@1达到28.8%，文本检索的Recall@1达到14.7%。在未见的ERA数据集上，HCCM展示了强大的零样本泛化能力，平均召回率（mR）为39.93%，优于经过微调的基线模型。", "conclusion": "HCCM框架通过其独特的分层跨粒度对比和匹配学习方法，有效解决了自然语言引导无人机场景中视觉-语言理解的挑战。它在细粒度语义理解、组合推理和鲁棒性方面表现出色，并在基准数据集上取得了最先进的性能和强大的零样本泛化能力。"}}
{"id": "2508.21556", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.21556", "abs": "https://arxiv.org/abs/2508.21556", "authors": ["Ilya A. Petrov", "Vladimir Guzov", "Riccardo Marin", "Emre Aksan", "Xu Chen", "Daniel Cremers", "Thabo Beeler", "Gerard Pons-Moll"], "title": "ECHO: Ego-Centric modeling of Human-Object interactions", "comment": null, "summary": "Modeling human-object interactions (HOI) from an egocentric perspective is a\nlargely unexplored yet important problem due to the increasing adoption of\nwearable devices, such as smart glasses and watches. We investigate how much\ninformation about interaction can be recovered from only head and wrists\ntracking. Our answer is ECHO (Ego-Centric modeling of Human-Object\ninteractions), which, for the first time, proposes a unified framework to\nrecover three modalities: human pose, object motion, and contact from such\nminimal observation. ECHO employs a Diffusion Transformer architecture and a\nunique three-variate diffusion process, which jointly models human motion,\nobject trajectory, and contact sequence, allowing for flexible input\nconfigurations. Our method operates in a head-centric canonical space,\nenhancing robustness to global orientation. We propose a conveyor-based\ninference, which progressively increases the diffusion timestamp with the frame\nposition, allowing us to process sequences of any length. Through extensive\nevaluation, we demonstrate that ECHO outperforms existing methods that do not\noffer the same flexibility, setting a state-of-the-art in egocentric HOI\nreconstruction.", "AI": {"tldr": "本文提出ECHO框架，首次实现仅通过头部和手腕追踪数据，从第一人称视角重建人类姿态、物体运动和接触，并在该领域达到最先进水平。", "motivation": "随着智能眼镜和手表等可穿戴设备的普及，从第一人称视角建模人-物交互（HOI）变得日益重要，但这一领域尚未得到充分探索。研究人员旨在解决如何从最少的信息（仅头部和手腕追踪）中恢复交互细节的问题。", "method": "ECHO是一个统一的框架，利用Diffusion Transformer架构和独特的三变量扩散过程，共同建模人体运动、物体轨迹和接触序列，从而恢复人体姿态、物体运动和接触这三种模态。该方法在以头部为中心的规范空间中操作，增强了对全局方向的鲁棒性，并引入了一种基于传送带的推理机制，以处理任意长度的序列。", "result": "ECHO在第一人称视角HOI重建方面超越了现有方法，提供了更高的灵活性，并在此领域设定了最先进的性能标准。", "conclusion": "ECHO首次提出了一种从极简观察（头部和手腕追踪）中恢复第一人称视角下人-物交互（包括姿态、物体运动和接触）的统一框架，其性能优于现有方法，为该领域树立了新的标杆。"}}
{"id": "2508.21565", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.21565", "abs": "https://arxiv.org/abs/2508.21565", "authors": ["Juneyoung Ro", "Namwoo Kim", "Yoonjin Yoon"], "title": "How Well Do Vision--Language Models Understand Cities? A Comparative Study on Spatial Reasoning from Street-View Images", "comment": "Accepted to ICCV Workshop 2025", "summary": "Effectively understanding urban scenes requires fine-grained spatial\nreasoning about objects, layouts, and depth cues. However, how well current\nvision-language models (VLMs), pretrained on general scenes, transfer these\nabilities to urban domain remains underexplored. To address this gap, we\nconduct a comparative study of three off-the-shelf VLMs-BLIP-2, InstructBLIP,\nand LLaVA-1.5-evaluating both zero-shot performance and the effects of\nfine-tuning with a synthetic VQA dataset specific to urban scenes. We construct\nsuch dataset from segmentation, depth, and object detection predictions of\nstreet-view images, pairing each question with LLM-generated Chain-of-Thought\n(CoT) answers for step-by-step reasoning supervision. Results show that while\nVLMs perform reasonably well in zero-shot settings, fine-tuning with our\nsynthetic CoT-supervised dataset substantially boosts performance, especially\nfor challenging question types such as negation and counterfactuals. This study\nintroduces urban spatial reasoning as a new challenge for VLMs and demonstrates\nsynthetic dataset construction as a practical path for adapting general-purpose\nmodels to specialized domains.", "AI": {"tldr": "本研究评估了通用视觉-语言模型（VLMs）在城市空间推理中的表现，并提出通过合成数据集和思维链（CoT）监督进行微调，可显著提升其在城市场景理解上的性能。", "motivation": "有效理解城市场景需要对物体、布局和深度线索进行细致的空间推理。然而，目前在通用场景上预训练的视觉-语言模型（VLMs）将这些能力转移到城市领域的表现尚未得到充分探索。", "method": "研究对比了BLIP-2、InstructBLIP和LLaVA-1.5三种现成VLM的零样本性能，并评估了使用合成VQA数据集进行微调的效果。该数据集通过街景图像的分割、深度和目标检测预测构建，并为每个问题配对由LLM生成的思维链（CoT）答案以提供分步推理监督。", "result": "结果显示，VLMs在零样本设置下表现尚可，但使用我们合成的、CoT监督的数据集进行微调能显著提升性能，尤其对于否定和反事实等挑战性问题类型。这表明微调对于提升模型在复杂城市推理任务上的能力至关重要。", "conclusion": "本研究将城市空间推理引入为VLM面临的新挑战，并证明了合成数据集的构建是使通用模型适应特定领域的实用途径。通过合成数据和CoT监督，可以有效增强VLM在专业领域，特别是城市场景理解中的推理能力。"}}
{"id": "2508.21580", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.21580", "abs": "https://arxiv.org/abs/2508.21580", "authors": ["Nico Albert Disch", "Yannick Kirchhoff", "Robin Peretzke", "Maximilian Rokuss", "Saikat Roy", "Constantin Ulrich", "David Zimmerer", "Klaus Maier-Hein"], "title": "Temporal Flow Matching for Learning Spatio-Temporal Trajectories in 4D Longitudinal Medical Imaging", "comment": null, "summary": "Understanding temporal dynamics in medical imaging is crucial for\napplications such as disease progression modeling, treatment planning and\nanatomical development tracking. However, most deep learning methods either\nconsider only single temporal contexts, or focus on tasks like classification\nor regression, limiting their ability for fine-grained spatial predictions.\nWhile some approaches have been explored, they are often limited to single\ntimepoints, specific diseases or have other technical restrictions. To address\nthis fundamental gap, we introduce Temporal Flow Matching (TFM), a unified\ngenerative trajectory method that (i) aims to learn the underlying temporal\ndistribution, (ii) by design can fall back to a nearest image predictor, i.e.\npredicting the last context image (LCI), as a special case, and (iii) supports\n$3D$ volumes, multiple prior scans, and irregular sampling. Extensive\nbenchmarks on three public longitudinal datasets show that TFM consistently\nsurpasses spatio-temporal methods from natural imaging, establishing a new\nstate-of-the-art and robust baseline for $4D$ medical image prediction.", "AI": {"tldr": "本文提出Temporal Flow Matching (TFM)，一种统一的生成轨迹方法，用于4D医学图像预测。它通过学习底层时间分布，支持3D体积、多重先验扫描和不规则采样，并在多个纵向数据集上超越现有方法，建立了新的基线。", "motivation": "医学影像中的时间动态对于疾病进展建模、治疗规划和解剖发育跟踪至关重要。然而，现有深度学习方法大多只考虑单一时间背景，或专注于分类/回归等任务，限制了其精细空间预测能力。现有方法常局限于单一时间点、特定疾病或存在技术限制，导致存在根本性空白。", "method": "本文引入Temporal Flow Matching (TFM)，这是一种统一的生成轨迹方法。它旨在学习底层的医学影像时间分布；设计上可退化为最近图像预测器（预测最后一个上下文图像LCI）作为特例；并支持3D体积、多重先验扫描以及不规则采样。", "result": "在三个公共纵向数据集上进行的广泛基准测试表明，TFM持续超越了自然图像领域的时空方法，为4D医学图像预测建立了一个新的最先进且稳健的基线。", "conclusion": "TFM有效解决了医学影像中精细空间预测的时间动态理解方面的根本性空白，为4D医学图像预测提供了一个新的最先进且稳健的解决方案。"}}
{"id": "2508.21581", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.21581", "abs": "https://arxiv.org/abs/2508.21581", "authors": ["Daniël Boeke", "Cedrik Blommestijn", "Rebecca N. Wray", "Kalina Chupetlovska", "Shangqi Gao", "Zeyu Gao", "Regina G. H. Beets-Tan", "Mireia Crispin-Ortuzar", "James O. Jones", "Wilson Silva", "Ines P. Machado"], "title": "Integrating Pathology and CT Imaging for Personalized Recurrence Risk Prediction in Renal Cancer", "comment": "12 pages, 2 figures, 1 table. Accepted at the Multimodal Learning and\n  Fusion Across Scales for Clinical Decision Support (ML-CDS) Workshop, MICCAI\n  2025. This is the submitted version with authors, affiliations, and\n  acknowledgements included; it has not undergone peer review or revisions. The\n  final version will appear in the Springer Lecture Notes in Computer Science\n  (LNCS) proceedings", "summary": "Recurrence risk estimation in clear cell renal cell carcinoma (ccRCC) is\nessential for guiding postoperative surveillance and treatment. The Leibovich\nscore remains widely used for stratifying distant recurrence risk but offers\nlimited patient-level resolution and excludes imaging information. This study\nevaluates multimodal recurrence prediction by integrating preoperative computed\ntomography (CT) and postoperative histopathology whole-slide images (WSIs). A\nmodular deep learning framework with pretrained encoders and Cox-based survival\nmodeling was tested across unimodal, late fusion, and intermediate fusion\nsetups. In a real-world ccRCC cohort, WSI-based models consistently\noutperformed CT-only models, underscoring the prognostic strength of pathology.\nIntermediate fusion further improved performance, with the best model\n(TITAN-CONCH with ResNet-18) approaching the adjusted Leibovich score. Random\ntie-breaking narrowed the gap between the clinical baseline and learned models,\nsuggesting discretization may overstate individualized performance. Using\nsimple embedding concatenation, radiology added value primarily through fusion.\nThese findings demonstrate the feasibility of foundation model-based multimodal\nintegration for personalized ccRCC risk prediction. Future work should explore\nmore expressive fusion strategies, larger multimodal datasets, and\ngeneral-purpose CT encoders to better match pathology modeling capacity.", "AI": {"tldr": "本研究通过整合术前CT和术后病理全切片图像（WSI），利用多模态深度学习框架预测透明细胞肾细胞癌（ccRCC）的复发风险，并证明了其可行性。", "motivation": "Leibovich评分在ccRCC复发风险评估中广泛应用，但其患者个体化分辨率有限且未纳入影像信息，促使研究人员寻求更全面、更个性化的预测方法。", "method": "研究采用了一个模块化的深度学习框架，该框架结合了预训练编码器和基于Cox的生存模型。它在单模态、后期融合和中期融合设置下，评估了术前CT和术后组织病理学WSI的整合效果。", "result": "基于WSI的模型始终优于仅基于CT的模型，凸显了病理学在预后判断中的强大作用。中期融合进一步提升了预测性能，其中最佳模型（TITAN-CONCH结合ResNet-18）的性能接近调整后的Leibovich评分。放射学信息主要通过融合贡献价值。", "conclusion": "研究结果表明，基于基础模型的多模态整合对于个性化ccRCC风险预测是可行的。未来的工作应探索更具表达力的融合策略、更大的多模态数据集和通用CT编码器，以进一步提升模型的性能。"}}
{"id": "2508.21657", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.21657", "abs": "https://arxiv.org/abs/2508.21657", "authors": ["Haomiao Zhang", "Zhangyuan Li", "Yanling Piao", "Zhi Li", "Xiaodong Wang", "Miao Cao", "Xiongfei Su", "Qiang Song", "Xin Yuan"], "title": "Unfolding Framework with Complex-Valued Deformable Attention for High-Quality Computer-Generated Hologram Generation", "comment": null, "summary": "Computer-generated holography (CGH) has gained wide attention with deep\nlearning-based algorithms. However, due to its nonlinear and ill-posed nature,\nchallenges remain in achieving accurate and stable reconstruction.\nSpecifically, ($i$) the widely used end-to-end networks treat the\nreconstruction model as a black box, ignoring underlying physical\nrelationships, which reduces interpretability and flexibility. ($ii$) CNN-based\nCGH algorithms have limited receptive fields, hindering their ability to\ncapture long-range dependencies and global context. ($iii$) Angular spectrum\nmethod (ASM)-based models are constrained to finite near-fields.In this paper,\nwe propose a Deep Unfolding Network (DUN) that decomposes gradient descent into\ntwo modules: an adaptive bandwidth-preserving model (ABPM) and a phase-domain\ncomplex-valued denoiser (PCD), providing more flexibility. ABPM allows for\nwider working distances compared to ASM-based methods. At the same time, PCD\nleverages its complex-valued deformable self-attention module to capture global\nfeatures and enhance performance, achieving a PSNR over 35 dB. Experiments on\nsimulated and real data show state-of-the-art results.", "AI": {"tldr": "本文提出了一种深度展开网络（DUN），通过分解梯度下降为自适应带宽保持模型（ABPM）和相位域复值去噪器（PCD），解决了计算机生成全息图（CGH）中端到端网络、CNN和角谱法（ASM）模型的局限性，实现了更准确和稳定的重建。", "motivation": "深度学习驱动的CGH面临挑战：1) 端到端网络将重建视为黑箱，缺乏可解释性和灵活性；2) 基于CNN的算法感受野有限，难以捕获长距离依赖和全局上下文；3) 基于ASM的模型仅限于有限的近场。", "method": "提出了一种深度展开网络（DUN），将梯度下降分解为两个模块：1) 自适应带宽保持模型（ABPM），允许比ASM更宽的工作距离；2) 相位域复值去噪器（PCD），利用其复值可变形自注意力模块捕获全局特征并增强性能。", "result": "实验在模拟和真实数据上显示了最先进的结果，实现了超过35 dB的PSNR。", "conclusion": "所提出的DUN通过ABPM和PCD有效解决了现有CGH算法的局限性，提供了更灵活、准确且性能优越的全息图重建方法。"}}
{"id": "2508.21680", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.21680", "abs": "https://arxiv.org/abs/2508.21680", "authors": ["Maximilian Rokuss", "Yannick Kirchhoff", "Fabian Isensee", "Klaus H. Maier-Hein"], "title": "Towards Interactive Lesion Segmentation in Whole-Body PET/CT with Promptable Models", "comment": "atuoPET4 Team LesionLocator", "summary": "Whole-body PET/CT is a cornerstone of oncological imaging, yet accurate\nlesion segmentation remains challenging due to tracer heterogeneity,\nphysiological uptake, and multi-center variability. While fully automated\nmethods have advanced substantially, clinical practice benefits from approaches\nthat keep humans in the loop to efficiently refine predicted masks. The\nautoPET/CT IV challenge addresses this need by introducing interactive\nsegmentation tasks based on simulated user prompts. In this work, we present\nour submission to Task 1. Building on the winning autoPET III nnU-Net pipeline,\nwe extend the framework with promptable capabilities by encoding user-provided\nforeground and background clicks as additional input channels. We\nsystematically investigate representations for spatial prompts and demonstrate\nthat Euclidean Distance Transform (EDT) encodings consistently outperform\nGaussian kernels. Furthermore, we propose online simulation of user\ninteractions and a custom point sampling strategy to improve robustness under\nrealistic prompting conditions. Our ensemble of EDT-based models, trained with\nand without external data, achieves the strongest cross-validation performance,\nreducing both false positives and false negatives compared to baseline models.\nThese results highlight the potential of promptable models to enable efficient,\nuser-guided segmentation workflows in multi-tracer, multi-center PET/CT. Code\nis publicly available at https://github.com/MIC-DKFZ/autoPET-interactive", "AI": {"tldr": "本文提出了一种基于nnU-Net的交互式PET/CT病灶分割方法，通过将用户点击（前景/背景）编码为附加输入通道，并采用欧氏距离变换（EDT）编码，实现了高效、用户引导的分割工作流，显著优于基线模型。", "motivation": "全身PET/CT肿瘤影像中，由于示踪剂异质性、生理性摄取和多中心变异性，准确的病灶分割仍具挑战。虽然全自动化方法已取得进展，但临床实践仍受益于允许人类高效精炼预测掩膜的方法。autoPET/CT IV挑战赛旨在通过模拟用户提示的交互式分割任务来解决这一需求。", "method": "本文基于获胜的autoPET III nnU-Net管线，通过将用户提供的前景和背景点击编码为额外的输入通道，扩展了框架的提示能力。系统性地研究了空间提示的表示方法，发现欧氏距离变换（EDT）编码始终优于高斯核。此外，提出了在线模拟用户交互和自定义点采样策略，以提高在实际提示条件下的鲁棒性。最终使用了基于EDT的模型集成，并结合有无外部数据进行训练。", "result": "基于EDT的模型集成，无论是否使用外部数据训练，都取得了最强的交叉验证性能，与基线模型相比，同时减少了假阳性（FP）和假阴性（FN）。", "conclusion": "这些结果突显了可提示模型在多示踪剂、多中心PET/CT中实现高效、用户引导分割工作流的潜力。"}}
{"id": "2508.21689", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.21689", "abs": "https://arxiv.org/abs/2508.21689", "authors": ["Fatih Erdoğan", "Merve Rabia Barın", "Fatma Güney"], "title": "Mapping like a Skeptic: Probabilistic BEV Projection for Online HD Mapping", "comment": "BMVC 2025. GitHub:\n  https://github.com/Fatih-Erdogan/mapping-like-skeptic", "summary": "Constructing high-definition (HD) maps from sensory input requires accurately\nmapping the road elements in image space to the Bird's Eye View (BEV) space.\nThe precision of this mapping directly impacts the quality of the final\nvectorized HD map. Existing HD mapping approaches outsource the projection to\nstandard mapping techniques, such as attention-based ones. However, these\nmethods struggle with accuracy due to generalization problems, often\nhallucinating non-existent road elements. Our key idea is to start with a\ngeometric mapping based on camera parameters and adapt it to the scene to\nextract relevant map information from camera images. To implement this, we\npropose a novel probabilistic projection mechanism with confidence scores to\n(i) refine the mapping to better align with the scene and (ii) filter out\nirrelevant elements that should not influence HD map generation. In addition,\nwe improve temporal processing by using confidence scores to selectively\naccumulate reliable information over time. Experiments on new splits of the\nnuScenes and Argoverse2 datasets demonstrate improved performance over\nstate-of-the-art approaches, indicating better generalization. The improvements\nare particularly pronounced on nuScenes and in the challenging long perception\nrange. Our code and model checkpoints are available at\nhttps://github.com/Fatih-Erdogan/mapping-like-skeptic .", "AI": {"tldr": "本文提出了一种新颖的概率投影机制，结合置信度分数，用于从图像空间到鸟瞰图（BEV）空间的高清地图构建，显著提高了地图绘制的准确性和泛化能力，尤其是在挑战性的感知范围。", "motivation": "现有高清地图绘制方法在将图像空间元素投影到BEV空间时，由于泛化问题，精度不足，常会“幻觉”出不存在的道路元素。这导致最终矢量化高清地图的质量受损。", "method": "核心思想是基于相机参数进行几何映射，并根据场景进行自适应调整，以从图像中提取相关地图信息。具体方法包括：1) 提出一种带有置信度分数的概率投影机制，用于细化映射以更好地与场景对齐，并过滤掉不相关的元素；2) 利用置信度分数选择性地累积可靠信息，从而改进时间处理。", "result": "在nuScenes和Argoverse2数据集的新分割上的实验表明，该方法优于现有最先进的方法，表现出更好的泛化能力。在nuScenes数据集和挑战性的长感知范围下，性能提升尤为显著。", "conclusion": "所提出的概率投影机制能够有效提高高清地图构建的精度和泛化能力，尤其是在复杂场景和远距离感知方面表现出色，解决了现有方法中精度不足和元素“幻觉”的问题。"}}
{"id": "2508.21712", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.21712", "abs": "https://arxiv.org/abs/2508.21712", "authors": ["Alvaro Patricio", "Atabak Dehban", "Rodrigo Ventura"], "title": "FLORA: Efficient Synthetic Data Generation for Object Detection in Low-Data Regimes via finetuning Flux LoRA", "comment": null, "summary": "Recent advances in diffusion-based generative models have demonstrated\nsignificant potential in augmenting scarce datasets for object detection tasks.\nNevertheless, most recent models rely on resource-intensive full fine-tuning of\nlarge-scale diffusion models, requiring enterprise-grade GPUs (e.g., NVIDIA\nV100) and thousands of synthetic images. To address these limitations, we\npropose Flux LoRA Augmentation (FLORA), a lightweight synthetic data generation\npipeline. Our approach uses the Flux 1.1 Dev diffusion model, fine-tuned\nexclusively through Low-Rank Adaptation (LoRA). This dramatically reduces\ncomputational requirements, enabling synthetic dataset generation with a\nconsumer-grade GPU (e.g., NVIDIA RTX 4090). We empirically evaluate our\napproach on seven diverse object detection datasets. Our results demonstrate\nthat training object detectors with just 500 synthetic images generated by our\napproach yields superior detection performance compared to models trained on\n5000 synthetic images from the ODGEN baseline, achieving improvements of up to\n21.3% in mAP@.50:.95. This work demonstrates that it is possible to surpass\nstate-of-the-art performance with far greater efficiency, as FLORA achieves\nsuperior results using only 10% of the data and a fraction of the computational\ncost. This work demonstrates that a quality and efficiency-focused approach is\nmore effective than brute-force generation, making advanced synthetic data\ncreation more practical and accessible for real-world scenarios.", "AI": {"tldr": "本文提出了FLORA，一种轻量级合成数据生成方法，通过使用LoRA对Flux 1.1 Dev扩散模型进行微调，显著降低了计算资源需求，并以更少的数据和计算成本实现了优于现有方法的物体检测性能。", "motivation": "现有的基于扩散模型的物体检测数据增强方法需要大量计算资源（如企业级GPU和数千张合成图像），这限制了其应用。研究旨在解决这一效率和可及性问题。", "method": "本文提出Flux LoRA Augmentation (FLORA) 方法，利用Flux 1.1 Dev扩散模型，并通过低秩适应 (LoRA) 进行独占式微调。这种方法显著降低了计算要求，允许使用消费级GPU（如NVIDIA RTX 4090）生成合成数据集。", "result": "FLORA方法仅使用500张合成图像训练的物体检测器，在七个不同的物体检测数据集上表现优于使用ODGEN基线生成的5000张合成图像训练的模型，mAP@.50:.95最高提升了21.3%。这表明FLORA以十分之一的数据量和更低的计算成本达到了卓越的性能。", "conclusion": "研究表明，相比于蛮力生成，注重质量和效率的方法（如FLORA）在合成数据创建方面更为有效，使得先进的合成数据生成技术在实际场景中更具实用性和可及性。"}}
{"id": "2508.21761", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.21761", "abs": "https://arxiv.org/abs/2508.21761", "authors": ["Xavier Juanola", "Giovana Morais", "Magdalena Fuentes", "Gloria Haro"], "title": "Learning from Silence and Noise for Visual Sound Source Localization", "comment": "10 pages, 2 figures, 4 tables + Supplementary Material", "summary": "Visual sound source localization is a fundamental perception task that aims\nto detect the location of sounding sources in a video given its audio. Despite\nrecent progress, we identify two shortcomings in current methods: 1) most\napproaches perform poorly in cases with low audio-visual semantic\ncorrespondence such as silence, noise, and offscreen sounds, i.e. in the\npresence of negative audio; and 2) most prior evaluations are limited to\npositive cases, where both datasets and metrics convey scenarios with a single\nvisible sound source in the scene. To address this, we introduce three key\ncontributions. First, we propose a new training strategy that incorporates\nsilence and noise, which improves performance in positive cases, while being\nmore robust against negative sounds. Our resulting self-supervised model,\nSSL-SaN, achieves state-of-the-art performance compared to other\nself-supervised models, both in sound localization and cross-modal retrieval.\nSecond, we propose a new metric that quantifies the trade-off between alignment\nand separability of auditory and visual features across positive and negative\naudio-visual pairs. Third, we present IS3+, an extended and improved version of\nthe IS3 synthetic dataset with negative audio.\n  Our data, metrics and code are available on the\nhttps://xavijuanola.github.io/SSL-SaN/.", "AI": {"tldr": "本文提出了一种新的自监督训练策略（SSL-SaN），通过引入静音和噪声来改进视觉声源定位，使其在低音视频语义对应（如负面音频）情况下更鲁棒，并在定位和跨模态检索方面达到SOTA性能。同时，还提出了新的评估指标和包含负面音频的扩展数据集IS3+。", "motivation": "现有视觉声源定位方法存在两个主要缺点：1) 在低音视频语义对应（如静音、噪声、画外音等负面音频）情况下性能不佳；2) 大多数现有评估仅限于正面情况，即数据集中只有一个可见声源的场景。", "method": "1. 提出一种新的训练策略，融合了静音和噪声，以提高正面情况下的性能并增强对负面声音的鲁棒性，从而构建了自监督模型SSL-SaN。2. 提出一种新的度量指标，用于量化听觉和视觉特征在正负音视频对之间对齐和分离的权衡。3. 提出了IS3+，一个扩展并改进的合成数据集，其中包含了负面音频。", "result": "所提出的SSL-SaN模型在声源定位和跨模态检索方面均达到了现有自监督模型的最佳性能，并且对负面声音表现出更强的鲁棒性。", "conclusion": "通过引入新的训练策略、评估指标和扩展数据集，本研究显著提升了视觉声源定位在复杂真实场景（特别是存在负面音频时）的性能和鲁棒性，并为未来的研究提供了更全面的评估工具和数据。"}}
{"id": "2508.21767", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.21767", "abs": "https://arxiv.org/abs/2508.21767", "authors": ["Zhixiong Zeng", "Jing Huang", "Liming Zheng", "Wenkang Han", "Yufeng Zhong", "Lei Chen", "Longrong Yang", "Yingjie Chu", "Yuzhi He", "Lin Ma"], "title": "UItron: Foundational GUI Agent with Advanced Perception and Planning", "comment": "24 pages", "summary": "GUI agent aims to enable automated operations on Mobile/PC devices, which is\nan important task toward achieving artificial general intelligence. The rapid\nadvancement of VLMs accelerates the development of GUI agents, owing to their\npowerful capabilities in visual understanding and task planning. However,\nbuilding a GUI agent remains a challenging task due to the scarcity of\noperation trajectories, the availability of interactive infrastructure, and the\nlimitation of initial capabilities in foundation models. In this work, we\nintroduce UItron, an open-source foundational model for automatic GUI agents,\nfeaturing advanced GUI perception, grounding, and planning capabilities. UItron\nhighlights the necessity of systemic data engineering and interactive\ninfrastructure as foundational components for advancing GUI agent development.\nIt not only systematically studies a series of data engineering strategies to\nenhance training effects, but also establishes an interactive environment\nconnecting both Mobile and PC devices. In training, UItron adopts supervised\nfinetuning over perception and planning tasks in various GUI scenarios, and\nthen develop a curriculum reinforcement learning framework to enable complex\nreasoning and exploration for online environments. As a result, UItron achieves\nsuperior performance in benchmarks of GUI perception, grounding, and planning.\nIn particular, UItron highlights the interaction proficiency with top-tier\nChinese mobile APPs, as we identified a general lack of Chinese capabilities\neven in state-of-the-art solutions. To this end, we manually collect over one\nmillion steps of operation trajectories across the top 100 most popular apps,\nand build the offline and online agent evaluation environments. Experimental\nresults demonstrate that UItron achieves significant progress in Chinese app\nscenarios, propelling GUI agents one step closer to real-world application.", "AI": {"tldr": "UItron是一个开源的通用GUI代理基础模型，通过系统数据工程、交互式基础设施、监督微调和课程强化学习，显著提升了GUI感知、定位和规划能力，尤其在中文APP场景中表现优越。", "motivation": "GUI代理是实现通用人工智能的重要一步，但其发展面临操作轨迹数据稀缺、交互基础设施不足以及基础模型初始能力有限的挑战。尽管VLM加速了发展，但这些挑战依然存在。", "method": "本文提出了UItron，一个开源的GUI代理基础模型。它强调系统性数据工程和交互式基础设施的重要性，并系统研究了一系列数据工程策略。UItron建立了一个连接移动和PC设备的交互式环境，并采用监督微调（针对感知和规划任务）与课程强化学习（用于在线环境中的复杂推理和探索）相结合的训练框架。此外，为解决中文能力不足的问题，UItron手动收集了超过一百万步的中文主流APP操作轨迹数据，并构建了离线和在线评估环境。", "result": "UItron在GUI感知、定位和规划的基准测试中取得了卓越的性能。特别是在与顶级中文移动APP的交互熟练度方面表现突出，填补了现有解决方案中文能力普遍不足的空白。实验结果表明，UItron在中文APP场景中取得了显著进展。", "conclusion": "UItron通过其先进的GUI感知、定位和规划能力，以及对系统数据工程和交互式基础设施的强调，显著推动了GUI代理的发展。尤其在中文APP场景中的卓越表现，使GUI代理离实际应用更近了一步。"}}
{"id": "2508.21769", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.21769", "abs": "https://arxiv.org/abs/2508.21769", "authors": ["Ha Min Son", "Zhe Zhao", "Shahbaz Rezaei", "Xin Liu"], "title": "Domain Generalization in-the-Wild: Disentangling Classification from Domain-Aware Representations", "comment": null, "summary": "Evaluating domain generalization (DG) for foundational models like CLIP is\nchallenging, as web-scale pretraining data potentially covers many existing\nbenchmarks. Consequently, current DG evaluation may neither be sufficiently\nchallenging nor adequately test genuinely unseen data scenarios. To better\nassess the performance of CLIP on DG in-the-wild, a scenario where CLIP\nencounters challenging unseen data, we consider two approaches: (1) evaluating\non 33 diverse datasets with quantified out-of-distribution (OOD) scores after\nfine-tuning CLIP on ImageNet, and (2) using unlearning to make CLIP `forget'\nsome domains as an approximation. We observe that CLIP's performance\ndeteriorates significantly on more OOD datasets. To address this, we present\nCLIP-DCA (Disentangling Classification from enhanced domain Aware\nrepresentations). Our approach is motivated by the observation that while\nstandard domain invariance losses aim to make representations domain-invariant,\nthis can be harmful to foundation models by forcing the discarding of\ndomain-aware representations beneficial for generalization. We instead\nhypothesize that enhancing domain awareness is a prerequisite for effective\ndomain-invariant classification in foundation models. CLIP-DCA identifies and\nenhances domain awareness within CLIP's encoders using a separate domain head\nand synthetically generated diverse domain data. Simultaneously, it encourages\ndomain-invariant classification through disentanglement from the domain\nfeatures. CLIP-DCA shows significant improvements within this challenging\nevaluation compared to existing methods, particularly on datasets that are more\nOOD.", "AI": {"tldr": "现有领域泛化(DG)评估对CLIP等基础模型不够具有挑战性。本文提出新的评估方法，并发现CLIP在域外(OOD)数据集上性能显著下降。为此，提出CLIP-DCA，通过增强领域感知并解耦分类，显著提升了CLIP在挑战性DG场景下的性能。", "motivation": "现有对CLIP等基础模型的DG评估可能不够具有挑战性，无法充分测试真正未见过的数据场景，因为其预训练数据可能已覆盖许多现有基准。此外，标准领域不变性损失可能有害，因为它强制丢弃对泛化有益的领域感知表示。本研究旨在更好地评估CLIP在“野外”DG场景中的性能，并假设增强领域感知是基础模型中有效领域不变分类的先决条件。", "method": "本文采用两种方法评估CLIP的DG性能：1) 在ImageNet上微调CLIP后，使用33个具有量化OOD分数的不同数据集进行评估；2) 使用“遗忘学习”让CLIP“忘记”某些领域作为近似评估。为解决观察到的性能下降，本文提出了CLIP-DCA (Disentangling Classification from enhanced domain Aware representations)。该方法通过独立的领域头部和合成生成的多样领域数据，识别并增强CLIP编码器中的领域感知能力，同时通过与领域特征的解耦来鼓励领域不变的分类。", "result": "实验观察到CLIP在OOD程度更高的数据集上性能显著下降。相比现有方法，CLIP-DCA在本文提出的挑战性评估中显示出显著改进，尤其是在OOD程度更高的数据集上。", "conclusion": "CLIP在面对高度OOD数据时存在显著的领域泛化挑战。CLIP-DCA通过增强基础模型的领域感知能力并将其与分类解耦，有效提升了在复杂“野外”DG场景中的性能，证明了增强领域感知对于实现有效领域不变分类的重要性。"}}
{"id": "2508.21770", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.21770", "abs": "https://arxiv.org/abs/2508.21770", "authors": ["Qiyue Sun", "Qiming Huang", "Yang Yang", "Hongjun Wang", "Jianbo Jiao"], "title": "What Can We Learn from Harry Potter? An Exploratory Study of Visual Representation Learning from Atypical Videos", "comment": null, "summary": "Humans usually show exceptional generalisation and discovery ability in the\nopen world, when being shown uncommon new concepts. Whereas most existing\nstudies in the literature focus on common typical data from closed sets,\nopen-world novel discovery is under-explored in videos. In this paper, we are\ninterested in asking: \\textit{What if atypical unusual videos are exposed in\nthe learning process?} To this end, we collect a new video dataset consisting\nof various types of unusual atypical data (\\eg sci-fi, animation, \\etc). To\nstudy how such atypical data may benefit open-world learning, we feed them into\nthe model training process for representation learning. Focusing on three key\ntasks in open-world learning: out-of-distribution (OOD) detection, novel\ncategory discovery (NCD), and zero-shot action recognition (ZSAR), we found\nthat even straightforward learning approaches with atypical data consistently\nimprove performance across various settings. Furthermore, we found that\nincreasing the categorical diversity of the atypical samples further boosts OOD\ndetection performance. Additionally, in the NCD task, using a smaller yet more\nsemantically diverse set of atypical samples leads to better performance\ncompared to using a larger but more typical dataset. In the ZSAR setting, the\nsemantic diversity of atypical videos helps the model generalise better to\nunseen action classes. These observations in our extensive experimental\nevaluations reveal the benefits of atypical videos for visual representation\nlearning in the open world, together with the newly proposed dataset,\nencouraging further studies in this direction.", "AI": {"tldr": "本研究探讨了利用非典型视频数据（如科幻、动画）来提升开放世界学习（包括OOD检测、NCD和ZSAR）中视觉表征学习的能力，并发现其显著优势。", "motivation": "人类在开放世界中对不常见的新概念表现出卓越的泛化和发现能力，而现有研究大多关注封闭集中的常见典型数据。视频中的开放世界新颖发现仍未充分探索。本研究旨在回答：如果在学习过程中引入非典型、不寻常的视频会怎样？", "method": "1. 收集了一个包含各种非典型不寻常视频（如科幻、动画等）的新数据集。2. 将这些非典型数据用于模型训练，进行表征学习。3. 在开放世界学习的三个关键任务上进行评估：分布外检测（OOD）、新类别发现（NCD）和零样本动作识别（ZSAR）。", "result": "1. 即使是直接的学习方法，引入非典型数据也能持续提升OOD检测、NCD和ZSAR任务的性能。2. 增加非典型样本的类别多样性可以进一步提升OOD检测性能。3. 在NCD任务中，使用更小但语义更多样化的非典型样本集，比使用更大但更典型的样本集表现更好。4. 在ZSAR设置中，非典型视频的语义多样性有助于模型更好地泛化到未见过的动作类别。", "conclusion": "这些广泛的实验评估表明，非典型视频对于开放世界中的视觉表征学习具有显著益处。新提出的数据集以及这些发现鼓励了该方向的进一步研究。"}}
{"id": "2508.21775", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.21775", "abs": "https://arxiv.org/abs/2508.21775", "authors": ["Omer Faruk Durugol", "Maximilian Rokuss", "Yannick Kirchhoff", "Klaus H. Maier-Hein"], "title": "A Multi-Stage Fine-Tuning and Ensembling Strategy for Pancreatic Tumor Segmentation in Diagnostic and Therapeutic MRI", "comment": "11 pages, 1 figure, PANTHER Challenge submission", "summary": "Automated segmentation of Pancreatic Ductal Adenocarcinoma (PDAC) from MRI is\ncritical for clinical workflows but is hindered by poor tumor-tissue contrast\nand a scarcity of annotated data. This paper details our submission to the\nPANTHER challenge, addressing both diagnostic T1-weighted (Task 1) and\ntherapeutic T2-weighted (Task 2) segmentation. Our approach is built upon the\nnnU-Net framework and leverages a deep, multi-stage cascaded pre-training\nstrategy, starting from a general anatomical foundation model and sequentially\nfine-tuning on CT pancreatic lesion datasets and the target MRI modalities.\nThrough extensive five-fold cross-validation, we systematically evaluated data\naugmentation schemes and training schedules. Our analysis revealed a critical\ntrade-off, where aggressive data augmentation produced the highest volumetric\naccuracy, while default augmentations yielded superior boundary precision\n(achieving a state-of-the-art MASD of 5.46 mm and HD95 of 17.33 mm for Task 1).\nFor our final submission, we exploited this finding by constructing custom,\nheterogeneous ensembles of specialist models, essentially creating a mix of\nexperts. This metric-aware ensembling strategy proved highly effective,\nachieving a top cross-validation Tumor Dice score of 0.661 for Task 1 and 0.523\nfor Task 2. Our work presents a robust methodology for developing specialized,\nhigh-performance models in the context of limited data and complex medical\nimaging tasks (Team MIC-DKFZ).", "AI": {"tldr": "该研究提出了一种基于nnU-Net的多阶段级联预训练和度量感知集成策略，用于在数据稀缺和对比度差的条件下，从MRI图像中自动分割胰腺导管腺癌(PDAC)，并取得了高精度。", "motivation": "自动从MRI中分割胰腺导管腺癌(PDAC)对临床工作流程至关重要，但由于肿瘤组织对比度差和带注释数据稀缺而面临挑战。", "method": "该方法基于nnU-Net框架，采用深度多阶段级联预训练策略：从通用解剖基础模型开始，顺序地在CT胰腺病变数据集和目标MRI模态上进行微调。通过广泛的五折交叉验证，系统评估了数据增强方案和训练计划。最终通过构建定制的、异构的专家模型集成（度量感知集成策略）来优化性能。", "result": "分析揭示了一个关键的权衡：激进的数据增强产生了最高的体积准确性，而默认增强则产生了卓越的边界精度（任务1达到了最先进的MASD 5.46毫米和HD95 17.33毫米）。最终的集成策略在任务1中实现了0.661的肿瘤Dice分数，在任务2中实现了0.523的肿瘤Dice分数。", "conclusion": "该工作提出了一种在数据有限和医学成像任务复杂背景下，开发专业化、高性能模型的稳健方法。"}}
{"id": "2508.21809", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.21809", "abs": "https://arxiv.org/abs/2508.21809", "authors": ["Jasper Uijlings", "Xingyi Zhou", "Xiuye Gu", "Arsha Nagrani", "Anurag Arnab", "Alireza Fathi", "David Ross", "Cordelia Schmid"], "title": "VoCap: Video Object Captioning and Segmentation from Any Prompt", "comment": null, "summary": "Understanding objects in videos in terms of fine-grained localization masks\nand detailed semantic properties is a fundamental task in video understanding.\nIn this paper, we propose VoCap, a flexible video model that consumes a video\nand a prompt of various modalities (text, box or mask), and produces a\nspatio-temporal masklet with a corresponding object-centric caption. As such\nour model addresses simultaneously the tasks of promptable video object\nsegmentation, referring expression segmentation, and object captioning. Since\nobtaining data for this task is tedious and expensive, we propose to annotate\nan existing large-scale segmentation dataset (SAV) with pseudo object captions.\nWe do so by preprocessing videos with their ground-truth masks to highlight the\nobject of interest and feed this to a large Vision Language Model (VLM). For an\nunbiased evaluation, we collect manual annotations on the validation set. We\ncall the resulting dataset SAV-Caption. We train our VoCap model at scale on a\nSAV-Caption together with a mix of other image and video datasets. Our model\nyields state-of-the-art results on referring expression video object\nsegmentation, is competitive on semi-supervised video object segmentation, and\nestablishes a benchmark for video object captioning. Our dataset will be made\navailable at https://github.com/google-deepmind/vocap.", "AI": {"tldr": "本文提出了VoCap模型，一个灵活的视频模型，能够根据多模态提示（文本、框、掩码）生成时空掩码和以对象为中心的描述，从而同时解决可提示视频对象分割、指代表达分割和对象描述任务。同时，他们还创建了SAV-Caption数据集。", "motivation": "在视频理解中，以细粒度定位掩码和详细语义属性来理解视频中的对象是一项基础任务。现有的方法可能无法同时满足这些需求，并且获取相关数据成本高昂。", "method": "本文提出了VoCap模型，该模型接收视频和多模态提示（文本、框或掩码），然后输出一个时空掩码（masklet）和相应的以对象为中心的描述。为了解决数据稀缺问题，作者通过使用大型视觉语言模型（VLM）处理现有的大规模分割数据集（SAV）的伪对象描述，创建了SAV-Caption数据集，并在验证集上进行了人工标注以确保无偏评估。VoCap模型在SAV-Caption以及其他图像和视频数据集的混合上进行了大规模训练。", "result": "VoCap模型在指代表达视频对象分割任务上取得了最先进的结果，在半监督视频对象分割任务上表现具有竞争力，并为视频对象描述任务建立了新的基准。所创建的SAV-Caption数据集也将公开发布。", "conclusion": "VoCap是一个多功能的视频模型，能够通过多模态提示实现全面的视频对象理解（包括分割和描述）。同时，新创建的SAV-Caption数据集为这一领域的研究提供了宝贵的资源，并推动了相关任务的性能基准。"}}
{"id": "2508.21824", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.21824", "abs": "https://arxiv.org/abs/2508.21824", "authors": ["Maolin Wei", "Wanzhou Liu", "Eshed Ohn-Bar"], "title": "DriveQA: Passing the Driving Knowledge Test", "comment": "Accepted by ICCV 2025. Project page: https://driveqaiccv.github.io/", "summary": "If a Large Language Model (LLM) were to take a driving knowledge test today,\nwould it pass? Beyond standard spatial and visual question-answering (QA) tasks\non current autonomous driving benchmarks, driving knowledge tests require a\ncomplete understanding of all traffic rules, signage, and right-of-way\nprinciples. To pass this test, human drivers must discern various edge cases\nthat rarely appear in real-world datasets. In this work, we present DriveQA, an\nextensive open-source text and vision-based benchmark that exhaustively covers\ntraffic regulations and scenarios. Through our experiments using DriveQA, we\nshow that (1) state-of-the-art LLMs and Multimodal LLMs (MLLMs) perform well on\nbasic traffic rules but exhibit significant weaknesses in numerical reasoning\nand complex right-of-way scenarios, traffic sign variations, and spatial\nlayouts, (2) fine-tuning on DriveQA improves accuracy across multiple\ncategories, particularly in regulatory sign recognition and intersection\ndecision-making, (3) controlled variations in DriveQA-V provide insights into\nmodel sensitivity to environmental factors such as lighting, perspective,\ndistance, and weather conditions, and (4) pretraining on DriveQA enhances\ndownstream driving task performance, leading to improved results on real-world\ndatasets such as nuScenes and BDD, while also demonstrating that models can\ninternalize text and synthetic traffic knowledge to generalize effectively\nacross downstream QA tasks.", "AI": {"tldr": "本文提出了DriveQA，一个全面的文本和视觉驾驶知识基准，用于评估大型语言模型（LLMs）和多模态LLMs（MLLMs）在交通法规和场景理解方面的能力。研究发现现有模型在复杂推理和边缘案例上存在显著弱点，但通过在DriveQA上进行微调和预训练可以有效提升性能和泛化能力。", "motivation": "目前的自动驾驶基准主要关注空间和视觉问答，但驾驶知识测试需要对所有交通规则、标志和路权原则有完整理解，包括现实世界数据中罕见的各种边缘情况。现有基准不足以评估LLMs能否通过此类测试。", "method": "研究人员创建了DriveQA，一个广泛的开源文本和视觉基准，全面覆盖了交通法规和场景。他们使用DriveQA对最先进的LLMs和MLLMs进行了实验，并在DriveQA上进行了模型微调和预训练，以评估其对模型性能和下游任务的影响。", "result": "1. 最先进的LLMs和MLLMs在基本交通规则上表现良好，但在数值推理、复杂路权场景、交通标志变体和空间布局方面表现出显著弱点。2. 在DriveQA上进行微调可以提高模型在多个类别上的准确性，尤其是在监管标志识别和交叉路口决策方面。3. DriveQA-V中的受控变体揭示了模型对光照、视角、距离和天气条件等环境因素的敏感性。4. 在DriveQA上进行预训练可以增强下游驾驶任务的性能，改善在nuScenes和BDD等真实世界数据集上的结果，并证明模型能够内化文本和合成交通知识以有效泛化到下游问答任务。", "conclusion": "LLMs和MLLMs在理解复杂的驾驶知识和处理边缘案例方面仍有不足。DriveQA基准能够有效识别这些弱点，并通过微调和预训练显著提升模型在驾驶知识理解和下游驾驶任务中的表现，证明模型可以学习并泛化文本和合成交通知识。"}}
