{"id": "2507.17868", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.17868", "abs": "https://arxiv.org/abs/2507.17868", "authors": ["Amr S. Mohamed", "Emily Nguyen", "Deepa Kundur"], "title": "Safe Reinforcement Learning-based Automatic Generation Control", "comment": "5 pages, conference: IEEE Power and Energy Systems General Meeting\n  2025", "summary": "Amidst the growing demand for implementing advanced control and\ndecision-making algorithms|to enhance the reliability, resilience, and\nstability of power systems|arises a crucial concern regarding the safety of\nemploying machine learning techniques. While these methods can be applied to\nderive more optimal control decisions, they often lack safety assurances. This\npaper proposes a framework based on control barrier functions to facilitate\nsafe learning and deployment of reinforcement learning agents for power system\ncontrol applications, specifically in the context of automatic generation\ncontrol. We develop the safety barriers and reinforcement learning framework\nnecessary to establish trust in reinforcement learning as a safe option for\nautomatic generation control - as foundation for future detailed verification\nand application studies.", "AI": {"tldr": "本文提出一个基于控制障碍函数(CBF)的框架，以实现强化学习(RL)智能体在电力系统自动发电控制(AGC)中的安全学习和部署。", "motivation": "电力系统对先进控制和决策算法的需求日益增长，以提高可靠性、韧性和稳定性。然而，机器学习技术在提供更优控制决策的同时，往往缺乏安全保障，这是其应用的关键担忧。", "method": "本文提出一个基于控制障碍函数(CBF)的框架，用于促进强化学习(RL)智能体在电力系统控制（特别是自动发电控制）中的安全学习和部署。研究开发了必要的安全障碍和强化学习框架。", "result": "研究开发了安全障碍和强化学习框架，旨在建立对强化学习作为自动发电控制安全选项的信任。", "conclusion": "该框架为未来在自动发电控制中详细验证和应用强化学习奠定了基础。"}}
{"id": "2507.17875", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.17875", "abs": "https://arxiv.org/abs/2507.17875", "authors": ["R. Spencer Hallyburton", "Miroslav Pajic"], "title": "Trusted Data Fusion, Multi-Agent Autonomy, Autonomous Vehicles", "comment": null, "summary": "Multi-agent collaboration enhances situational awareness in intelligence,\nsurveillance, and reconnaissance (ISR) missions. Ad hoc networks of unmanned\naerial vehicles (UAVs) allow for real-time data sharing, but they face security\nchallenges due to their decentralized nature, making them vulnerable to\ncyber-physical attacks. This paper introduces a trust-based framework for\nassured sensor fusion in distributed multi-agent networks, utilizing a hidden\nMarkov model (HMM)-based approach to estimate the trustworthiness of agents and\ntheir provided information in a decentralized fashion. Trust-informed data\nfusion prioritizes fusing data from reliable sources, enhancing resilience and\naccuracy in contested environments. To evaluate the assured sensor fusion under\nattacks on system/mission sensing, we present a novel multi-agent aerial\ndataset built from the Unreal Engine simulator. We demonstrate through case\nstudies improved ISR performance and an ability to detect malicious actors in\nadversarial settings.", "AI": {"tldr": "本文提出一个基于隐马尔可夫模型（HMM）的去中心化信任框架，用于多智能体网络中的可靠传感器融合，以应对网络物理攻击，提升情报、监视和侦察（ISR）任务的性能和恶意行为者检测能力。", "motivation": "多智能体无人机（UAV）特设网络虽能增强ISR任务中的态势感知，但其去中心化特性使其易受网络物理攻击，面临严峻的安全挑战。", "method": "引入一个基于信任的框架，利用隐马尔可夫模型（HMM）去中心化地评估智能体及其提供信息的信任度。信任感知的融合优先处理来自可靠来源的数据，以增强韧性和准确性。为评估该方法，作者构建了一个基于虚幻引擎模拟器的新型多智能体空中数据集。", "result": "通过案例研究，证明了该方法能够提高ISR性能，并在对抗环境中有效检测恶意行为者。", "conclusion": "所提出的信任框架通过确保传感器融合的可靠性，显著提升了多智能体ISR系统在面临攻击时的韧性和准确性，并能有效识别恶意节点。"}}
{"id": "2507.17990", "categories": ["eess.SY", "cs.MA", "cs.RO", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.17990", "abs": "https://arxiv.org/abs/2507.17990", "authors": ["Takumi Kato", "Zhi Li Hu"], "title": "Rapid Modeling Architecture for Lightweight Simulator to Accelerate and Improve Decision Making for Industrial Systems", "comment": "8 pages, 13 figures. Manuscript accepted at the 2025 IEEE 21st\n  International Conference on Automation Science and Engineering (CASE 2025)", "summary": "Designing industrial systems, such as building, improving, and automating\ndistribution centers and manufacturing plants, involves critical\ndecision-making with limited information in the early phases. The lack of\ninformation leads to less accurate designs of the systems, which are often\ndifficult to resolve later. It is effective to use simulators to model the\ndesigned system and find out the issues early. However, the modeling time\nrequired by conventional simulators is too long to allow for rapid model\ncreation to meet decision-making demands. In this paper, we propose a Rapid\nModeling Architecture (RMA) for a lightweight industrial simulator that\nmitigates the modeling burden while maintaining the essential details in order\nto accelerate and improve decision-making. We have prototyped a simulator based\non the RMA and applied it to the actual factory layout design problem. We also\ncompared the modeling time of our simulator to that of an existing simulator,\nand as a result, our simulator achieved a 78.3% reduction in modeling time\ncompared to conventional simulators.", "AI": {"tldr": "本文提出了一种名为快速建模架构（RMA）的轻量级工业模拟器，旨在显著减少建模时间，以加速和改进工业系统设计早期阶段的决策。", "motivation": "工业系统（如配送中心和制造工厂）在早期设计阶段面临信息有限的问题，导致设计不准确且后期难以纠正。传统模拟器建模时间过长，无法满足快速决策的需求。", "method": "提出并设计了一种名为快速建模架构（RMA）的轻量级工业模拟器。基于RMA原型化了一个模拟器，并将其应用于实际的工厂布局设计问题。将该模拟器的建模时间与现有模拟器进行了比较。", "result": "与传统模拟器相比，基于RMA的原型模拟器将建模时间减少了78.3%。", "conclusion": "RMA能够有效减轻建模负担，同时保留关键细节，从而加速并改进工业系统设计中的决策过程。"}}
{"id": "2507.17800", "categories": ["eess.IV", "cond-mat.mtrl-sci", "cs.CV", "physics.optics"], "pdf": "https://arxiv.org/pdf/2507.17800", "abs": "https://arxiv.org/abs/2507.17800", "authors": ["Christian K. Belardi", "Chia-Hao Lee", "Yingheng Wang", "Justin Lovelace", "Kilian Q. Weinberger", "David A. Muller", "Carla P. Gomes"], "title": "Improving Multislice Electron Ptychography with a Generative Prior", "comment": "16 pages, 10 figures, 5 tables", "summary": "Multislice electron ptychography (MEP) is an inverse imaging technique that\ncomputationally reconstructs the highest-resolution images of atomic crystal\nstructures from diffraction patterns. Available algorithms often solve this\ninverse problem iteratively but are both time consuming and produce suboptimal\nsolutions due to their ill-posed nature. We develop MEP-Diffusion, a diffusion\nmodel trained on a large database of crystal structures specifically for MEP to\naugment existing iterative solvers. MEP-Diffusion is easily integrated as a\ngenerative prior into existing reconstruction methods via Diffusion Posterior\nSampling (DPS). We find that this hybrid approach greatly enhances the quality\nof the reconstructed 3D volumes, achieving a 90.50% improvement in SSIM over\nexisting methods.", "AI": {"tldr": "该研究开发了MEP-Diffusion，一个基于扩散模型的生成先验，用于增强多层电子叠层衍射成像（MEP）的图像重建，显著提高了重建质量。", "motivation": "现有的MEP迭代重建算法耗时且由于病态性质导致次优解决方案，无法有效重建原子晶体结构的高分辨率图像。", "method": "开发了MEP-Diffusion，一个专门针对MEP并基于大量晶体结构数据库训练的扩散模型。该模型通过扩散后验采样（DPS）作为生成先验，轻松集成到现有迭代重建方法中，形成混合方法。", "result": "这种混合方法显著提升了重建三维图像的质量，相较于现有方法，结构相似性指数（SSIM）提高了90.50%。", "conclusion": "MEP-Diffusion作为生成先验，有效克服了传统迭代算法的局限性，大幅提升了MEP图像重建的质量和效率。"}}
{"id": "2507.18001", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.18001", "abs": "https://arxiv.org/abs/2507.18001", "authors": ["Yang Li", "Zenghui Zheng", "Xiangyang Wu", "Jiayong Li", "Wei Wang", "Qiang Zeng", "Zhikang Shuai"], "title": "Quantitative Damping Calculation and Compensation Method for Global Stability Improvement of Inverter-Based Systems", "comment": null, "summary": "Small-signal stability issues-induced broadband oscillations pose significant\nthreats to the secure operation of multi-inverter systems, attracting extensive\nresearch attention. Researches revealed that system instability is led by the\nlacking of positive damping, yet it has not been clearly specified how much the\nexact amount of damping compensation required to sufficiently ensure system\nglobal stability. This paper presents a feasible solution for quantitative\ndamping calculation and compensation to enhance the global stability of\ninverter-based systems. First, based on the system nodal admittance model, a\nquantitative damping calculation algorithm is presented, which can suggest the\nrequired damping compensation as well as compensation location for sufficient\nstability improvement. Then, we propose a specific AD with output current\nfeedforward control strategy, which make the AD be quasi-pure resistive and can\neffectively enhance system damping efficiency. Finally, a testing system with\nthree inverters is used as case study, showing that the proposed method\nprovides a promising solution to efficiently enhance the global stability\nimprovement of inverter-based systems. Simulations and experiments validate the\nproposed method.", "AI": {"tldr": "该论文提出了一种定量阻尼计算与补偿方法，以有效增强多逆变器系统的全局稳定性。", "motivation": "多逆变器系统中的小信号稳定性问题引发的宽带振荡严重威胁系统安全运行。现有研究已揭示系统不稳定性源于正阻尼不足，但尚未明确为确保系统全局稳定性所需阻尼补偿的确切量。", "method": "首先，基于系统节点导纳模型，提出了一种定量阻尼计算算法，能够给出所需的阻尼补偿量和补偿位置。其次，提出了一种带有输出电流前馈控制的特定有源阻尼器（AD）策略，使其呈现准纯电阻特性，有效提高系统阻尼效率。", "result": "通过一个包含三个逆变器的测试系统案例研究表明，所提出的方法为有效提升基于逆变器系统的全局稳定性提供了一个有前景的解决方案。仿真和实验验证了该方法的有效性。", "conclusion": "该研究提供了一种可行的定量阻尼计算与补偿方案，能够有效增强基于逆变器系统的全局稳定性，解决了传统方法在确定所需阻尼量方面的不足。"}}
{"id": "2507.17845", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.17845", "abs": "https://arxiv.org/abs/2507.17845", "authors": ["Jonah Kömen", "Edwin D. de Jong", "Julius Hense", "Hannah Marienwald", "Jonas Dippel", "Philip Naumann", "Eric Marcus", "Lukas Ruff", "Maximilian Alber", "Jonas Teuwen", "Frederick Klauschen", "Klaus-Robert Müller"], "title": "Towards Robust Foundation Models for Digital Pathology", "comment": null, "summary": "Biomedical Foundation Models (FMs) are rapidly transforming AI-enabled\nhealthcare research and entering clinical validation. However, their\nsusceptibility to learning non-biological technical features -- including\nvariations in surgical/endoscopic techniques, laboratory procedures, and\nscanner hardware -- poses risks for clinical deployment. We present the first\nsystematic investigation of pathology FM robustness to non-biological features.\nOur work (i) introduces measures to quantify FM robustness, (ii) demonstrates\nthe consequences of limited robustness, and (iii) proposes a framework for FM\nrobustification to mitigate these issues. Specifically, we developed PathoROB,\na robustness benchmark with three novel metrics, including the robustness\nindex, and four datasets covering 28 biological classes from 34 medical\ncenters. Our experiments reveal robustness deficits across all 20 evaluated\nFMs, and substantial robustness differences between them. We found that\nnon-robust FM representations can cause major diagnostic downstream errors and\nclinical blunders that prevent safe clinical adoption. Using more robust FMs\nand post-hoc robustification considerably reduced (but did not yet eliminate)\nthe risk of such errors. This work establishes that robustness evaluation is\nessential for validating pathology FMs before clinical adoption and\ndemonstrates that future FM development must integrate robustness as a core\ndesign principle. PathoROB provides a blueprint for assessing robustness across\nbiomedical domains, guiding FM improvement efforts towards more robust,\nrepresentative, and clinically deployable AI systems that prioritize biological\ninformation over technical artifacts.", "AI": {"tldr": "该研究系统性地调查了病理学基础模型（FMs）对非生物学技术特征的鲁棒性，发现现有模型存在鲁棒性缺陷，并提出了评估和增强鲁棒性的框架（PathoROB），强调鲁棒性是临床部署的关键。", "motivation": "生物医学基础模型正在改变医疗保健研究并进入临床验证，但它们容易学习非生物学技术特征（如手术技术、实验室程序、扫描仪硬件差异），这给临床部署带来了风险。", "method": "1. 引入了量化FM鲁棒性的新度量标准。2. 开发了PathoROB鲁棒性基准，包含鲁棒性指数等三个新指标。3. 构建了涵盖34个医疗中心28个生物类别的四个数据集。4. 评估了20个病理学基础模型。5. 提出了一个用于FM鲁棒性增强的框架。", "result": "1. 所有20个评估的FM都存在鲁棒性缺陷，且它们之间存在显著差异。2. 不鲁棒的FM表示会导致严重的下游诊断错误和临床失误，阻碍安全临床应用。3. 使用更鲁棒的FM和事后鲁棒性增强方法可以显著减少（但尚未完全消除）此类错误的风险。", "conclusion": "1. 在临床应用前，病理学基础模型的鲁棒性评估至关重要。2. 未来FM开发必须将鲁棒性作为核心设计原则进行整合。3. PathoROB为评估生物医学领域的鲁棒性提供了蓝图，指导FM改进以开发更鲁棒、更具代表性、更优先生物学信息而非技术伪影的临床可部署AI系统。"}}
{"id": "2507.17846", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.17846", "abs": "https://arxiv.org/abs/2507.17846", "authors": ["Alison Bartsch", "Arvind Car", "Amir Barati Farimani"], "title": "PinchBot: Long-Horizon Deformable Manipulation with Guided Diffusion Policy", "comment": null, "summary": "Pottery creation is a complicated art form that requires dexterous, precise\nand delicate actions to slowly morph a block of clay to a meaningful, and often\nuseful 3D goal shape. In this work, we aim to create a robotic system that can\ncreate simple pottery goals with only pinch-based actions. This pinch pottery\ntask allows us to explore the challenges of a highly multi-modal and\nlong-horizon deformable manipulation task. To this end, we present PinchBot, a\ngoal-conditioned diffusion policy model that when combined with pre-trained 3D\npoint cloud embeddings, task progress prediction and collision-constrained\naction projection, is able to successfully create a variety of simple pottery\ngoals. For experimental videos and access to the demonstration dataset, please\nvisit our project website:\nhttps://sites.google.com/andrew.cmu.edu/pinchbot/home.", "AI": {"tldr": "该研究开发了一个名为PinchBot的机器人系统，利用基于捏合的动作，通过扩散策略模型和3D点云嵌入等技术，成功实现了简单陶器的自动制作。", "motivation": "陶器制作是一个复杂的艺术形式，需要灵巧、精确和精细的动作将粘土块塑造成有意义的3D目标形状。该工作旨在探索机器人如何处理这种高度多模态、长周期的可变形物体操作任务。", "method": "研究提出了PinchBot系统，它是一个目标条件扩散策略模型。该模型结合了预训练的3D点云嵌入、任务进度预测和碰撞约束的动作投影，以实现陶器制作。", "result": "PinchBot系统能够成功创建各种简单的陶器目标。", "conclusion": "PinchBot证明了利用基于捏合的动作和扩散策略模型，机器人可以有效地处理复杂的可变形物体操作任务，成功制作简单的陶器。"}}
{"id": "2507.17777", "categories": ["cs.AI", "76A02"], "pdf": "https://arxiv.org/pdf/2507.17777", "abs": "https://arxiv.org/abs/2507.17777", "authors": ["Theofanis Aravanis", "Grigorios Chrimatopoulos", "Mohammad Ferdows", "Michalis Xenos", "Efstratios Em Tzirtzilakis"], "title": "ASP-Assisted Symbolic Regression: Uncovering Hidden Physics in Fluid Mechanics", "comment": "This research was implemented in the framework of the Action\n  \"Flagship actions in interdisciplinary scientific fields with a special focus\n  on the productive fabric'', which is implemented through the National\n  Recovery and Resilience Fund Greece 2.0 and funded by the European\n  Union--NextGenerationEU (Project ID: TAEDR-0535983)", "summary": "Unlike conventional Machine-Learning (ML) approaches, often criticized as\n\"black boxes\", Symbolic Regression (SR) stands out as a powerful tool for\nrevealing interpretable mathematical relationships in complex physical systems,\nrequiring no a priori assumptions about models' structures. Motivated by the\nrecognition that, in fluid mechanics, an understanding of the underlying flow\nphysics is as crucial as accurate prediction, this study applies SR to model a\nfundamental three-dimensional (3D) incompressible flow in a rectangular\nchannel, focusing on the (axial) velocity and pressure fields under laminar\nconditions. By employing the PySR library, compact symbolic equations were\nderived directly from numerical simulation data, revealing key characteristics\nof the flow dynamics. These equations not only approximate the parabolic\nvelocity profile and pressure drop observed in the studied fluid flow, but also\nperfectly coincide with analytical solutions from the literature. Furthermore,\nwe propose an innovative approach that integrates SR with the\nknowledge-representation framework of Answer Set Programming (ASP), combining\nthe generative power of SR with the declarative reasoning strengths of ASP. The\nproposed hybrid SR/ASP framework ensures that the SR-generated symbolic\nexpressions are not only statistically accurate, but also physically plausible,\nadhering to domain-specific principles. Overall, the study highlights two key\ncontributions: SR's ability to simplify complex flow behaviours into concise,\ninterpretable equations, and the potential of knowledge-representation\napproaches to improve the reliability and alignment of data-driven SR models\nwith domain principles. Insights from the examined 3D channel flow pave the way\nfor integrating such hybrid approaches into efficient frameworks, [...] where\nexplainable predictions and real-time data analysis are crucial.", "AI": {"tldr": "本研究利用符号回归（SR）从数值模拟数据中推导出三维不可压缩通道流的解析方程，并提出结合SR与回答集编程（ASP）的混合框架，以确保模型的物理合理性。", "motivation": "传统的机器学习方法常被视为“黑箱”，而在流体力学中，对底层流动物理的理解与准确预测同等重要。因此，研究旨在寻找一种无需先验假设即可揭示复杂物理系统中可解释数学关系的方法。", "method": "使用PySR库对矩形通道内的三维层流不可压缩流（轴向速度和压力场）的数值模拟数据进行符号回归。此外，提出了一种创新的混合方法，将SR与回答集编程（ASP）知识表示框架相结合，利用ASP的声明性推理能力来确保SR生成的符号表达式不仅统计准确，而且物理上合理并符合领域特定原则。", "result": "成功从数值模拟数据中推导出紧凑的符号方程，这些方程不仅近似了观察到的抛物线速度分布和压降，而且与文献中的解析解完美吻合。提出的SR/ASP混合框架能够确保SR生成的表达式既统计准确又具有物理合理性。", "conclusion": "研究表明，符号回归能够将复杂的流动行为简化为简洁、可解释的方程，并且知识表示方法（如ASP）能够显著提高数据驱动SR模型的可靠性及其与领域原理的一致性。这种混合方法为需要可解释预测和实时数据分析的框架奠定了基础。"}}
{"id": "2507.17842", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.17842", "abs": "https://arxiv.org/abs/2507.17842", "authors": ["Yimeng Zhang", "Tian Wang", "Jiri Gesi", "Ziyi Wang", "Yuxuan Lu", "Jiacheng Lin", "Sinong Zhan", "Vianne Gao", "Ruochen Jiao", "Junze Liu", "Kun Qian", "Yuxin Tang", "Ran Xue", "Houyu Zhang", "Qingjun Cui", "Yufan Guo", "Dakuo Wang"], "title": "Shop-R1: Rewarding LLMs to Simulate Human Behavior in Online Shopping via Reinforcement Learning", "comment": null, "summary": "Large Language Models (LLMs) have recently demonstrated strong potential in\ngenerating 'believable human-like' behavior in web environments. Prior work has\nexplored augmenting training data with LLM-synthesized rationales and applying\nsupervised fine-tuning (SFT) to enhance reasoning ability, which in turn can\nimprove downstream action prediction. However, the performance of such\napproaches remains inherently bounded by the reasoning capabilities of the\nmodel used to generate the rationales. In this paper, we introduce Shop-R1, a\nnovel reinforcement learning (RL) framework aimed at enhancing the reasoning\nability of LLMs for simulation of real human behavior in online shopping\nenvironments Specifically, Shop-R1 decomposes the human behavior simulation\ntask into two stages: rationale generation and action prediction, each guided\nby distinct reward signals. For rationale generation, we leverage internal\nmodel signals (e.g., logit distributions) to guide the reasoning process in a\nself-supervised manner. For action prediction, we propose a hierarchical reward\nstructure with difficulty-aware scaling to prevent reward hacking and enable\nfine-grained reward assignment. This design evaluates both high-level action\ntypes and the correctness of fine-grained sub-action details (attributes and\nvalues), rewarding outputs proportionally to their difficulty. Experimental\nresults show that our method achieves a relative improvement of over 65%\ncompared to the baseline.", "AI": {"tldr": "本文提出Shop-R1，一个基于强化学习的框架，旨在通过分解为理由生成和动作预测两个阶段，并设计分层奖励机制，显著提升大型语言模型在模拟在线购物环境中人类行为的推理能力。", "motivation": "现有方法通过LLM合成的理由进行SFT来增强推理能力，但其性能受限于生成理由的模型本身的推理能力。因此，需要一种新方法来突破这一限制，以更有效地模拟真实人类行为。", "method": "Shop-R1将人类行为模拟任务分解为理由生成和动作预测两个阶段，各由不同的奖励信号引导。理由生成阶段利用内部模型信号（如logit分布）进行自监督指导。动作预测阶段采用分层奖励结构，并进行难度感知缩放，以防止奖励作弊，并对高层动作类型和细粒度子动作细节（属性和值）进行奖励，奖励与输出难度成正比。", "result": "实验结果表明，Shop-R1方法相对于基线取得了超过65%的相对提升。", "conclusion": "Shop-R1框架通过其创新的两阶段RL设计和分层、难度感知的奖励机制，成功显著提升了大型语言模型在在线购物环境中模拟人类行为的推理能力。"}}
{"id": "2507.17801", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.17801", "abs": "https://arxiv.org/abs/2507.17801", "authors": ["Yi Xin", "Juncheng Yan", "Qi Qin", "Zhen Li", "Dongyang Liu", "Shicheng Li", "Victor Shea-Jay Huang", "Yupeng Zhou", "Renrui Zhang", "Le Zhuo", "Tiancheng Han", "Xiaoqing Sun", "Siqi Luo", "Mengmeng Wang", "Bin Fu", "Yuewen Cao", "Hongsheng Li", "Guangtao Zhai", "Xiaohong Liu", "Yu Qiao", "Peng Gao"], "title": "Lumina-mGPT 2.0: Stand-Alone AutoRegressive Image Modeling", "comment": "Tech Report, 23 pages, 11 figures, 7 tables", "summary": "We present Lumina-mGPT 2.0, a stand-alone, decoder-only autoregressive model\nthat revisits and revitalizes the autoregressive paradigm for high-quality\nimage generation and beyond. Unlike existing approaches that rely on pretrained\ncomponents or hybrid architectures, Lumina-mGPT 2.0 is trained entirely from\nscratch, enabling unrestricted architectural design and licensing freedom. It\nachieves generation quality on par with state-of-the-art diffusion models such\nas DALL-E 3 and SANA, while preserving the inherent flexibility and\ncompositionality of autoregressive modeling. Our unified tokenization scheme\nallows the model to seamlessly handle a wide spectrum of tasks-including\nsubject-driven generation, image editing, controllable synthesis, and dense\nprediction-within a single generative framework. To further boost usability, we\nincorporate efficient decoding strategies like inference-time scaling and\nspeculative Jacobi sampling to improve quality and speed, respectively.\nExtensive evaluations on standard text-to-image benchmarks (e.g., GenEval, DPG)\ndemonstrate that Lumina-mGPT 2.0 not only matches but in some cases surpasses\ndiffusion-based models. Moreover, we confirm its multi-task capabilities on the\nGraph200K benchmark, with the native Lumina-mGPT 2.0 performing exceptionally\nwell. These results position Lumina-mGPT 2.0 as a strong, flexible foundation\nmodel for unified multimodal generation. We have released our training details,\ncode, and models at https://github.com/Alpha-VLLM/Lumina-mGPT-2.0.", "AI": {"tldr": "Lumina-mGPT 2.0是一个从零开始训练的独立自回归模型，在图像生成质量上与最先进的扩散模型相当甚至超越，并能处理多模态任务，展现了自回归范式的强大潜力。", "motivation": "现有图像生成方法依赖预训练组件或混合架构，限制了设计自由和许可。该研究旨在通过从头训练的自回归模型，恢复其固有的灵活性和组合性，同时达到顶尖的生成质量。", "method": "Lumina-mGPT 2.0是一个独立的、仅解码器的自回归模型，完全从零开始训练。它采用统一的tokenization方案处理多种任务，并结合了推理时缩放（提升质量）和推测性Jacobi采样（提升速度）等高效解码策略。", "result": "Lumina-mGPT 2.0在生成质量上与DALL-E 3和SANA等先进扩散模型媲美。它在一个统一的生成框架内无缝处理多种任务，包括主体驱动生成、图像编辑、可控合成和密集预测。在文本到图像基准测试（如GenEval、DPG）上，其表现与扩散模型相当，甚至在某些情况下超越。在Graph200K基准测试中，其多任务能力也表现出色。", "conclusion": "Lumina-mGPT 2.0是一个强大且灵活的统一多模态生成基础模型，证明了从零开始训练的自回归模型在高质量图像生成及多任务处理方面的巨大潜力。"}}
{"id": "2507.18077", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.18077", "abs": "https://arxiv.org/abs/2507.18077", "authors": ["Yuqing Shen", "Yuanyuan Shi", "Daniel Kirschen", "Yize Chen"], "title": "Carbon Emission Flow Tracing: Fast Algorithm and California Grid Study", "comment": "In Submission, 16 pages, 11 figures, code available at\n  https://github.com/yuqing5/Carbon-Tracker-California", "summary": "Power systems decarbonization are at the focal point of the clean energy\ntransition. While system operators and utility companies increasingly publicize\nsystem-level carbon emission information, it remains unclear how emissions from\nindividual generators are transported through the grid and how they impact\nelectricity users at specific locations. This paper presents a novel and\ncomputationally efficient approach for exact quantification of nodal average\nand marginal carbon emission rates, applicable to both AC and DC optimal power\nflow problems. The approach leverages graph-based topological sorting and\ndirected cycle removal techniques, applied to directed graphs formed by\ngeneration dispatch and optimal power flow solutions. Our proposed algorithm\nefficiently identifies each generator's contribution to each node, capturing\nhow emissions are spatially distributed under varying system conditions. To\nvalidate its effectiveness and reveal locational and temporal emission patterns\nin the real world, we simulate the 8,870-bus realistic California grid using\nactual CAISO data and the CATS model. Based on year long hourly data on nodal\nloads and renewable generation, obtained or estimated from CAISO public data,\nour method accurately estimates power flow conditions, generation mixes, and\nsystemwide emissions, and delivers fine grained spatiotemporal emission\nanalysis for every California county. Both our algorithm and the California\nstudy are open-sourced, providing a foundation for future research on grid\nemissions, planning, operations, and energy policy.", "AI": {"tldr": "本文提出了一种新颖且计算高效的方法，用于精确量化电网中节点的平均和边际碳排放率，揭示了发电机排放如何通过电网传输并影响特定位置的用户，并通过加州电网的模拟进行了验证。", "motivation": "尽管电力系统运营商和公用事业公司日益公开系统层面的碳排放信息，但目前尚不清楚单个发电机的排放如何通过电网传输，以及它们如何影响特定位置的电力用户。", "method": "该方法利用图论中的拓扑排序和有向环移除技术，应用于由发电调度和最优潮流解形成的有向图。该算法能高效识别每个发电机对每个节点的贡献，捕捉不同系统条件下排放的空间分布。通过使用真实的CAISO数据和CATS模型，对8,870总线的加州电网进行了一年的逐小时模拟以验证其有效性。", "result": "该方法能够准确估计潮流条件、发电组合和全系统排放，并为加州每个县提供了细粒度的时空排放分析。模拟结果揭示了现实世界中的位置和时间排放模式。", "conclusion": "所提出的算法和加州案例研究均已开源，为未来电网排放、规划、运营和能源政策方面的研究奠定了基础。"}}
{"id": "2507.17869", "categories": ["eess.IV", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.17869", "abs": "https://arxiv.org/abs/2507.17869", "authors": ["Atif Bilal Asad", "Achyut Paudel", "Safal Kshetri", "Chenchen Kang", "Salik Ram Khanal", "Nataliya Shcherbatyuk", "Pierre Davadant", "R. Paul Schreiner", "Santosh Kalauni", "Manoj Karkee", "Markus Keller"], "title": "Integrating Feature Selection and Machine Learning for Nitrogen Assessment in Grapevine Leaves using In-Field Hyperspectral Imaging", "comment": null, "summary": "Nitrogen (N) is one of the most crucial nutrients in vineyards, affecting\nplant growth and subsequent products such as wine and juice. Because soil N has\nhigh spatial and temporal variability, it is desirable to accurately estimate\nthe N concentration of grapevine leaves and manage fertilization at the\nindividual plant level to optimally meet plant needs. In this study, we used\nin-field hyperspectral images with wavelengths ranging from $400 to 1000nm of\nfour different grapevine cultivars collected from distinct vineyards and over\ntwo growth stages during two growing seasons to develop models for predicting N\nconcentration at the leaf-level and canopy-level. After image processing, two\nfeature selection methods were employed to identify the optimal set of spectral\nbands that were responsive to leaf N concentrations. The selected spectral\nbands were used to train and test two different Machine Learning (ML) models,\nGradient Boosting and XGBoost, for predicting nitrogen concentrations. The\ncomparison of selected bands for both leaf-level and canopy-level datasets\nshowed that most of the spectral regions identified by the feature selection\nmethods were across both methods and the dataset types (leaf- and canopy-level\ndatasets), particularly in the key regions, 500-525nm, 650-690nm, 750-800nm,\nand 900-950nm. These findings indicated the robustness of these spectral\nregions for predicting nitrogen content. The results for N prediction\ndemonstrated that the ML model achieved an R square of 0.49 for canopy-level\ndata and an R square of 0.57 for leaf-level data, despite using different sets\nof selected spectral bands for each analysis level. The study demonstrated the\npotential of using in-field hyperspectral imaging and the use of spectral data\nin integrated feature selection and ML techniques to monitor N status in\nvineyards.", "AI": {"tldr": "本研究利用田间高光谱图像结合特征选择和机器学习方法，实现了葡萄叶片和冠层氮浓度的预测，并识别了关键光谱区域。", "motivation": "氮是葡萄园中关键的营养物质，影响植物生长和产品质量。由于土壤氮含量时空变异性大，需要准确估算葡萄叶片氮浓度，实现单株植物水平的精确施肥，以满足植物最佳需求。", "method": "研究收集了两种生长季、两个生长阶段、四种不同葡萄品种的田间高光谱图像（400-1000nm）。经过图像处理后，采用两种特征选择方法识别与叶片氮浓度相关的最佳光谱波段。选定的波段用于训练和测试两种机器学习模型（Gradient Boosting和XGBoost），以预测叶片和冠层水平的氮浓度。", "result": "特征选择方法识别出在叶片和冠层数据集中均表现出鲁棒性的关键光谱区域，包括500-525nm、650-690nm、750-800nm和900-950nm。机器学习模型在冠层水平数据上实现R²为0.49的氮预测精度，在叶片水平数据上实现R²为0.57的氮预测精度。", "conclusion": "研究证明了利用田间高光谱成像结合集成特征选择和机器学习技术，在监测葡萄园氮素状况方面的潜力。"}}
{"id": "2507.17856", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.17856", "abs": "https://arxiv.org/abs/2507.17856", "authors": ["Dennis Benders", "Laura Ferranti", "Johannes Köhler"], "title": "A Step-by-step Guide on Nonlinear Model Predictive Control for Safe Mobile Robot Navigation", "comment": "51 pages, 3 figures", "summary": "Designing a Model Predictive Control (MPC) scheme that enables a mobile robot\nto safely navigate through an obstacle-filled environment is a complicated yet\nessential task in robotics. In this technical report, safety refers to ensuring\nthat the robot respects state and input constraints while avoiding collisions\nwith obstacles despite the presence of disturbances and measurement noise. This\nreport offers a step-by-step approach to implementing Nonlinear Model\nPredictive Control (NMPC) schemes addressing these safety requirements.\nNumerous books and survey papers provide comprehensive overviews of linear MPC\n(LMPC) \\cite{bemporad2007robust,kouvaritakis2016model}, NMPC\n\\cite{rawlings2017model,allgower2004nonlinear,mayne2014model,grune2017nonlinear,saltik2018outlook},\nand their applications in various domains, including robotics\n\\cite{nascimento2018nonholonomic,nguyen2021model,shi2021advanced,wei2022mpc}.\nThis report does not aim to replicate those exhaustive reviews. Instead, it\nfocuses specifically on NMPC as a foundation for safe mobile robot navigation.\nThe goal is to provide a practical and accessible path from theoretical\nconcepts to mathematical proofs and implementation, emphasizing safety and\nperformance guarantees. It is intended for researchers, robotics engineers, and\npractitioners seeking to bridge the gap between theoretical NMPC formulations\nand real-world robotic applications.\n  This report is not necessarily meant to remain fixed over time. If someone\nfinds an error in the presented theory, please reach out via the given email\naddresses. We are happy to update the document if necessary.", "AI": {"tldr": "本报告提供了一种基于非线性模型预测控制（NMPC）的逐步方法，旨在实现移动机器人在存在干扰和测量噪声的障碍环境中安全导航。", "motivation": "在充满障碍的环境中设计一种能确保移动机器人安全导航的模型预测控制（MPC）方案是一项复杂但至关重要的任务，尤其需要处理状态和输入约束、避开障碍物，并应对干扰和测量噪声。", "method": "报告采用逐步方法实现非线性模型预测控制（NMPC）方案，以满足移动机器人的安全导航要求。它旨在将NMPC的理论概念、数学证明与实际实现相结合。", "result": "报告提供了一条从理论概念到数学证明再到实现的实用且易于理解的路径，重点强调了安全性和性能保证，适用于移动机器人导航。", "conclusion": "本报告旨在为研究人员、机器人工程师和实践者提供一个桥梁，帮助他们将理论NMPC公式应用于实际机器人应用中，实现移动机器人的安全导航。"}}
{"id": "2507.17874", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17874", "abs": "https://arxiv.org/abs/2507.17874", "authors": ["SaiBarath Sundar", "Pranav Satheesan", "Udayaadithya Avadhanam"], "title": "I2I-STRADA -- Information to Insights via Structured Reasoning Agent for Data Analysis", "comment": null, "summary": "Recent advances in agentic systems for data analysis have emphasized\nautomation of insight generation through multi-agent frameworks, and\norchestration layers. While these systems effectively manage tasks like query\ntranslation, data transformation, and visualization, they often overlook the\nstructured reasoning process underlying analytical thinking. Reasoning large\nlanguage models (LLMs) used for multi-step problem solving are trained as\ngeneral-purpose problem solvers. As a result, their reasoning or thinking steps\ndo not adhere to fixed processes for specific tasks. Real-world data analysis\nrequires a consistent cognitive workflow: interpreting vague goals, grounding\nthem in contextual knowledge, constructing abstract plans, and adapting\nexecution based on intermediate outcomes. We introduce I2I-STRADA\n(Information-to-Insight via Structured Reasoning Agent for Data Analysis), an\nagentic architecture designed to formalize this reasoning process. I2I-STRADA\nfocuses on modeling how analysis unfolds via modular sub-tasks that reflect the\ncognitive steps of analytical reasoning. Evaluations on the DABstep and DABench\nbenchmarks show that I2I-STRADA outperforms prior systems in planning coherence\nand insight alignment, highlighting the importance of structured cognitive\nworkflows in agent design for data analysis.", "AI": {"tldr": "本文提出I2I-STRADA，一种用于数据分析的智能体架构，通过形式化结构化推理过程，将分析分解为模块化子任务，以模拟人类认知步骤，从而在基准测试中优于现有系统。", "motivation": "现有数据分析智能体系统（如多智能体框架和编排层）在任务管理上有效，但忽视了分析思维背后的结构化推理过程。通用型LLM在多步问题解决中缺乏针对特定任务的固定推理流程，而实际数据分析需要一致的认知工作流（解释目标、联系上下文、构建计划、适应执行）。", "method": "引入I2I-STRADA（Information-to-Insight via Structured Reasoning Agent for Data Analysis），一个旨在形式化数据分析中推理过程的智能体架构。它专注于通过反映分析推理认知步骤的模块化子任务来建模分析的展开方式。", "result": "在DABstep和DABench基准测试上的评估显示，I2I-STRADA在规划连贯性和洞察对齐方面优于现有系统。", "conclusion": "结构化认知工作流在数据分析智能体设计中至关重要，能够显著提升系统性能。"}}
{"id": "2507.17849", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.17849", "abs": "https://arxiv.org/abs/2507.17849", "authors": ["Zhangyue Yin", "Qiushi Sun", "Zhiyuan Zeng", "Qinyuan Cheng", "Xipeng Qiu", "Xuanjing Huang"], "title": "Dynamic and Generalizable Process Reward Modeling", "comment": "Accepted by ACL 2025 Main", "summary": "Process Reward Models (PRMs) are crucial for guiding Large Language Models\n(LLMs) in complex scenarios by providing dense reward signals. However,\nexisting PRMs primarily rely on heuristic approaches, which struggle with\ncross-domain generalization. While LLM-as-judge has been proposed to provide\ngeneralized rewards, current research has focused mainly on feedback results,\noverlooking the meaningful guidance embedded within the text. Additionally,\nstatic and coarse-grained evaluation criteria struggle to adapt to complex\nprocess supervision. To tackle these challenges, we propose Dynamic and\nGeneralizable Process Reward Modeling (DG-PRM), which features a reward tree to\ncapture and store fine-grained, multi-dimensional reward criteria. DG-PRM\ndynamically selects reward signals for step-wise reward scoring. To handle\nmultifaceted reward signals, we pioneeringly adopt Pareto dominance estimation\nto identify discriminative positive and negative pairs. Experimental results\nshow that DG-PRM achieves stunning performance on prevailing benchmarks,\nsignificantly boosting model performance across tasks with dense rewards.\nFurther analysis reveals that DG-PRM adapts well to out-of-distribution\nscenarios, demonstrating exceptional generalizability.", "AI": {"tldr": "本文提出了动态可泛化的过程奖励模型（DG-PRM），通过奖励树和帕累托支配估计，解决了现有过程奖励模型泛化性差和评估标准静态的问题，显著提升了LLM在复杂任务中的性能和泛化能力。", "motivation": "现有过程奖励模型（PRMs）主要依赖启发式方法，导致跨领域泛化能力差。LLM作为评判者的方法虽然能提供泛化奖励，但忽视了文本中的有意义指导，且静态、粗粒度的评估标准难以适应复杂的流程监督。", "method": "本文提出了DG-PRM，其核心特点包括：1. 奖励树：捕捉并存储细粒度、多维度的奖励标准。2. 动态选择：根据步骤动态选择奖励信号进行评分。3. 帕累托支配估计：创新性地采用此方法来识别判别性的正负对，以处理多方面的奖励信号。", "result": "实验结果表明，DG-PRM在现有基准测试中取得了惊人的性能，显著提升了模型在需要密集奖励的任务中的表现。进一步分析显示，DG-PRM对分布外（OOD）场景具有良好的适应性，展示了卓越的泛化能力。", "conclusion": "DG-PRM有效解决了过程奖励模型在泛化性和动态评估方面的挑战，通过其创新的奖励树和帕累托支配估计方法，显著提升了大型语言模型在复杂任务中的性能和跨领域泛化能力。"}}
{"id": "2507.17844", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17844", "abs": "https://arxiv.org/abs/2507.17844", "authors": ["Sai Varun Kodathala", "Yashwanth Reddy Vutukoori", "Rakesh Vunnam"], "title": "SV3.3B: A Sports Video Understanding Model for Action Recognition", "comment": "8 pages, 6 figures, 4 tables. Submitted to AIxSET 2025", "summary": "This paper addresses the challenge of automated sports video analysis, which\nhas traditionally been limited by computationally intensive models requiring\nserver-side processing and lacking fine-grained understanding of athletic\nmovements. Current approaches struggle to capture the nuanced biomechanical\ntransitions essential for meaningful sports analysis, often missing critical\nphases like preparation, execution, and follow-through that occur within\nseconds. To address these limitations, we introduce SV3.3B, a lightweight 3.3B\nparameter video understanding model that combines novel temporal motion\ndifference sampling with self-supervised learning for efficient on-device\ndeployment. Our approach employs a DWT-VGG16-LDA based keyframe extraction\nmechanism that intelligently identifies the 16 most representative frames from\nsports sequences, followed by a V-DWT-JEPA2 encoder pretrained through\nmask-denoising objectives and an LLM decoder fine-tuned for sports action\ndescription generation. Evaluated on a subset of the NSVA basketball dataset,\nSV3.3B achieves superior performance across both traditional text generation\nmetrics and sports-specific evaluation criteria, outperforming larger\nclosed-source models including GPT-4o variants while maintaining significantly\nlower computational requirements. Our model demonstrates exceptional capability\nin generating technically detailed and analytically rich sports descriptions,\nachieving 29.2% improvement over GPT-4o in ground truth validation metrics,\nwith substantial improvements in information density, action complexity, and\nmeasurement precision metrics essential for comprehensive athletic analysis.\nModel Available at https://huggingface.co/sportsvision/SV3.3B.", "AI": {"tldr": "本文提出SV3.3B，一个轻量级3.3B参数的视频理解模型，用于高效的设备端体育视频分析，能够生成技术细节丰富、分析性强的体育动作描述，性能优于大型模型且计算成本显著降低。", "motivation": "传统体育视频分析模型计算密集，依赖服务器端处理，且缺乏对细微运动生物力学转换的精细理解，难以捕捉运动员动作的关键阶段（如准备、执行、完成），限制了有意义的体育分析。", "method": "引入SV3.3B模型，结合新颖的时间运动差异采样和自监督学习，实现高效的设备端部署。采用基于DWT-VGG16-LDA的关键帧提取机制（识别16个代表性帧），V-DWT-JEPA2编码器通过掩码去噪目标预训练，以及针对体育动作描述生成微调的LLM解码器。", "result": "在NSVA篮球数据集子集上，SV3.3B在传统文本生成指标和体育特定评估标准上均表现出色，优于包括GPT-4o变体在内的更大、闭源模型，同时计算要求显著降低。在真实性验证指标上比GPT-4o提升29.2%，并在信息密度、动作复杂度和测量精度等关键指标上也有显著改进。", "conclusion": "SV3.3B模型在生成技术详细且分析丰富的体育描述方面展现出卓越能力，有效解决了自动化体育视频分析中计算量大和缺乏细粒度理解的挑战，为设备端体育分析提供了高效且高性能的解决方案。"}}
{"id": "2507.18095", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.18095", "abs": "https://arxiv.org/abs/2507.18095", "authors": ["Yi Wang", "Dawei Qiu", "Fei Teng", "Goran Strbac"], "title": "Towards Microgrid Resilience Enhancement via Mobile Power Sources and Repair Crews: A Multi-Agent Reinforcement Learning Approach", "comment": null, "summary": "Mobile power sources (MPSs) have been gradually deployed in microgrids as\ncritical resources to coordinate with repair crews (RCs) towards resilience\nenhancement owing to their flexibility and mobility in handling the complex\ncoupled power-transport systems. However, previous work solves the coordinated\ndispatch problem of MPSs and RCs in a centralized manner with the assumption\nthat the communication network is still fully functioning after the event.\nHowever, there is growing evidence that certain extreme events will damage or\ndegrade communication infrastructure, which makes centralized decision making\nimpractical. To fill this gap, this paper formulates the resilience-driven\ndispatch problem of MPSs and RCs in a decentralized framework. To solve this\nproblem, a hierarchical multi-agent reinforcement learning method featuring a\ntwo-level framework is proposed, where the high-level action is used to switch\ndecision-making between power and transport networks, and the low-level action\nconstructed via a hybrid policy is used to compute continuous scheduling and\ndiscrete routing decisions in power and transport networks, respectively. The\nproposed method also uses an embedded function encapsulating system dynamics to\nenhance learning stability and scalability. Case studies based on IEEE 33-bus\nand 69-bus power networks are conducted to validate the effectiveness of the\nproposed method in load restoration.", "AI": {"tldr": "本文提出了一种去中心化的分层多智能体强化学习方法，用于在通信网络受损后，协调移动电源（MPSs）和抢修队伍（RCs）以提升微电网的韧性，实现负荷恢复。", "motivation": "现有研究在协调MPSs和RCs时，通常假设通信网络在事件后仍能完全正常运行，并采用集中式调度。然而，极端事件可能损坏通信基础设施，导致集中式决策变得不切实际，因此需要一种去中心化的解决方案。", "method": "本文将MPSs和RCs的韧性驱动调度问题建模为去中心化框架。为解决此问题，提出了一种分层多智能体强化学习方法：高层动作用于在电力和交通网络间切换决策；低层动作通过混合策略计算电力网络的连续调度和交通网络的离散路径决策。该方法还嵌入了封装系统动态的函数，以增强学习的稳定性和可扩展性。", "result": "基于IEEE 33节点和69节点电力网络的案例研究验证了所提方法在负荷恢复方面的有效性。", "conclusion": "所提出的去中心化分层多智能体强化学习方法，能够有效协调移动电源和抢修队伍，以增强微电网的韧性，尤其是在通信受损的复杂环境下。"}}
{"id": "2507.17911", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.17911", "abs": "https://arxiv.org/abs/2507.17911", "authors": ["Dou Hoon Kwark", "Shirui Luo", "Xiyue Zhu", "Yudu Li", "Zhi-Pei Liang", "Volodymyr Kindratenko"], "title": "Hierarchical Diffusion Framework for Pseudo-Healthy Brain MRI Inpainting with Enhanced 3D Consistency", "comment": "11 pages, 2 figures", "summary": "Pseudo-healthy image inpainting is an essential preprocessing step for\nanalyzing pathological brain MRI scans. Most current inpainting methods favor\nslice-wise 2D models for their high in-plane fidelity, but their independence\nacross slices produces discontinuities in the volume. Fully 3D models alleviate\nthis issue, but their high model capacity demands extensive training data for\nreliable, high-fidelity synthesis -- often impractical in medical settings. We\naddress these limitations with a hierarchical diffusion framework by replacing\ndirect 3D modeling with two perpendicular coarse-to-fine 2D stages. An axial\ndiffusion model first yields a coarse, globally consistent inpainting; a\ncoronal diffusion model then refines anatomical details. By combining\nperpendicular spatial views with adaptive resampling, our method balances data\nefficiency and volumetric consistency. Our experiments show our approach\noutperforms state-of-the-art baselines in both realism and volumetric\nconsistency, making it a promising solution for pseudo-healthy image\ninpainting. Code is available at\nhttps://github.com/dou0000/3dMRI-Consistent-Inpaint.", "AI": {"tldr": "该论文提出了一种分层扩散框架，通过结合两个垂直的2D阶段（轴向和冠状），实现伪健康脑部MRI图像修复，旨在平衡数据效率和体积一致性。", "motivation": "现有切片式2D修复方法在体积上存在不连续性，而完全3D模型则需要大量训练数据，这在医疗场景中通常不切实际。", "method": "采用分层扩散框架，用两个垂直的粗到细2D阶段代替直接3D建模。首先，一个轴向扩散模型生成粗略、全局一致的修复；然后，一个冠状扩散模型精修解剖细节，并结合自适应重采样。", "result": "实验表明，该方法在真实感和体积一致性方面均优于现有最先进的基线方法。", "conclusion": "该方法为伪健康图像修复提供了一个有前景的解决方案。"}}
{"id": "2507.18033", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18033", "abs": "https://arxiv.org/abs/2507.18033", "authors": ["Mingfeng Yuan", "Letian Wang", "Steven L. Waslander"], "title": "OpenNav: Open-World Navigation with Multimodal Large Language Models", "comment": null, "summary": "Pre-trained large language models (LLMs) have demonstrated strong\ncommon-sense reasoning abilities, making them promising for robotic navigation\nand planning tasks. However, despite recent progress, bridging the gap between\nlanguage descriptions and actual robot actions in the open-world, beyond merely\ninvoking limited predefined motion primitives, remains an open challenge. In\nthis work, we aim to enable robots to interpret and decompose complex language\ninstructions, ultimately synthesizing a sequence of trajectory points to\ncomplete diverse navigation tasks given open-set instructions and open-set\nobjects. We observe that multi-modal large language models (MLLMs) exhibit\nstrong cross-modal understanding when processing free-form language\ninstructions, demonstrating robust scene comprehension. More importantly,\nleveraging their code-generation capability, MLLMs can interact with\nvision-language perception models to generate compositional 2D bird-eye-view\nvalue maps, effectively integrating semantic knowledge from MLLMs with spatial\ninformation from maps to reinforce the robot's spatial understanding. To\nfurther validate our approach, we effectively leverage large-scale autonomous\nvehicle datasets (AVDs) to validate our proposed zero-shot vision-language\nnavigation framework in outdoor navigation tasks, demonstrating its capability\nto execute a diverse range of free-form natural language navigation\ninstructions while maintaining robustness against object detection errors and\nlinguistic ambiguities. Furthermore, we validate our system on a Husky robot in\nboth indoor and outdoor scenes, demonstrating its real-world robustness and\napplicability. Supplementary videos are available at\nhttps://trailab.github.io/OpenNav-website/", "AI": {"tldr": "本文提出了一种零样本视觉-语言导航框架，利用多模态大语言模型（MLLMs）使机器人能够理解复杂的自然语言指令，并生成轨迹点序列以在开放世界中执行多样化的导航任务。", "motivation": "尽管预训练大语言模型（LLMs）在机器人导航和规划方面显示出潜力，但如何将语言描述转化为开放世界中实际的机器人动作（而非仅仅调用有限的预定义运动原语）仍然是一个开放挑战。本研究旨在解决机器人理解和分解复杂语言指令，进而合成轨迹点序列以完成开放集指令和开放集对象下的导航任务。", "method": "该方法利用多模态大语言模型（MLLMs）强大的跨模态理解和场景理解能力。更重要的是，利用MLLMs的代码生成能力，使其与视觉-语言感知模型交互，生成组合式的2D鸟瞰图价值地图，有效整合MLLMs的语义知识和地图的空间信息。该框架被设计为零样本视觉-语言导航。", "result": "该方法在大型自动驾驶车辆数据集（AVDs）上进行了户外导航任务的验证，展示了其执行多样化自由形式自然语言导航指令的能力，并对物体检测错误和语言歧义表现出鲁棒性。此外，系统还在Husky机器人上进行了室内外场景的验证，证明了其在真实世界中的鲁棒性和适用性。", "conclusion": "本研究提出的零样本视觉-语言导航框架，通过利用MLLMs的跨模态理解和代码生成能力，成功使机器人能够解释复杂的语言指令并生成轨迹，在开放世界导航任务中表现出强大的适应性和鲁棒性，有效应对了语言歧义和感知误差等挑战。"}}
{"id": "2507.17927", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17927", "abs": "https://arxiv.org/abs/2507.17927", "authors": ["Timothy Tin Long Yu", "Mahdi Mostajabdaveh", "Jabo Serge Byusa", "Rindra Ramamonjison", "Giuseppe Carenini", "Kun Mao", "Zirui Zhou", "Yong Zhang"], "title": "SMARTAPS: Tool-augmented LLMs for Operations Management", "comment": "https://aaai.org/conference/aaai/aaai-25/bridge-ai-orms/", "summary": "Large language models (LLMs) present intriguing opportunities to enhance user\ninteraction with traditional algorithms and tools in real-world applications.\nAn advanced planning system (APS) is a sophisticated software that leverages\noptimization to help operations planners create, interpret, and modify an\noperational plan. While highly beneficial, many customers are priced out of\nusing an APS due to the ongoing costs of consultants responsible for\ncustomization and maintenance. To address the need for a more accessible APS\nexpressed by supply chain planners, we present SmartAPS, a conversational\nsystem built on a tool-augmented LLM. Our system provides operations planners\nwith an intuitive natural language chat interface, allowing them to query\ninformation, perform counterfactual reasoning, receive recommendations, and\nexecute scenario analysis to better manage their operation. A short video\ndemonstrating the system has been released: https://youtu.be/KtIrJjlDbyw", "AI": {"tldr": "SmartAPS是一个基于工具增强型LLM的对话式系统，旨在通过自然语言界面降低高级计划系统(APS)的使用门槛，帮助运营规划者管理其操作。", "motivation": "传统高级计划系统(APS)因定制和维护成本高昂（需顾问支持）而难以普及，许多客户无法负担。供应链规划者急需一个更易于访问的APS。", "method": "开发了SmartAPS，一个基于工具增强型大型语言模型(LLM)构建的对话系统。该系统提供直观的自然语言聊天界面。", "result": "SmartAPS使运营规划者能够通过自然语言查询信息、进行反事实推理、接收建议以及执行情景分析，从而更好地管理其运营。", "conclusion": "SmartAPS通过利用LLM提供对话式接口，显著提高了高级计划系统的可访问性和易用性，有望帮助更多规划者有效管理其运营。"}}
{"id": "2507.17896", "categories": ["cs.CL", "cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2507.17896", "abs": "https://arxiv.org/abs/2507.17896", "authors": ["Shubham Mohole", "Sainyam Galhotra"], "title": "VeriMinder: Mitigating Analytical Vulnerabilities in NL2SQL", "comment": null, "summary": "Application systems using natural language interfaces to databases (NLIDBs)\nhave democratized data analysis. This positive development has also brought\nforth an urgent challenge to help users who might use these systems without a\nbackground in statistical analysis to formulate bias-free analytical questions.\nAlthough significant research has focused on text-to-SQL generation accuracy,\naddressing cognitive biases in analytical questions remains underexplored. We\npresent VeriMinder, https://veriminder.ai, an interactive system for detecting\nand mitigating such analytical vulnerabilities. Our approach introduces three\nkey innovations: (1) a contextual semantic mapping framework for biases\nrelevant to specific analysis contexts (2) an analytical framework that\noperationalizes the Hard-to-Vary principle and guides users in systematic data\nanalysis (3) an optimized LLM-powered system that generates high-quality,\ntask-specific prompts using a structured process involving multiple candidates,\ncritic feedback, and self-reflection.\n  User testing confirms the merits of our approach. In direct user experience\nevaluation, 82.5% participants reported positively impacting the quality of the\nanalysis. In comparative evaluation, VeriMinder scored significantly higher\nthan alternative approaches, at least 20% better when considered for metrics of\nthe analysis's concreteness, comprehensiveness, and accuracy. Our system,\nimplemented as a web application, is set to help users avoid \"wrong question\"\nvulnerability during data analysis. VeriMinder code base with prompts,\nhttps://reproducibility.link/veriminder, is available as an MIT-licensed\nopen-source software to facilitate further research and adoption within the\ncommunity.", "AI": {"tldr": "VeriMinder是一个交互式系统，旨在帮助自然语言数据库接口（NLIDB）用户避免数据分析中的认知偏差，通过引入偏见上下文语义映射、Hard-to-Vary分析框架和优化的LLM提示生成实现。", "motivation": "自然语言数据库接口（NLIDB）使数据分析大众化，但非统计学背景的用户难以提出无偏见的分析问题。现有研究主要关注文本到SQL的准确性，而认知偏差问题尚未得到充分探索。", "method": "本文提出了VeriMinder系统，其方法包含三项创新：1) 针对特定分析上下文的偏见上下文语义映射框架；2) 运用Hard-to-Vary原则指导系统数据分析的分析框架；3) 优化的大型语言模型（LLM）驱动系统，通过多候选、评论反馈和自我反思的结构化过程生成高质量、任务特定的提示。", "result": "用户测试证实了该方法的优点。在直接用户体验评估中，82.5%的参与者表示VeriMinder对分析质量产生了积极影响。在比较评估中，VeriMinder在分析的具体性、全面性和准确性指标上，比替代方法至少高出20%。", "conclusion": "VeriMinder系统（一个Web应用）旨在帮助用户在数据分析中避免“错误问题”的脆弱性。其代码库和提示已作为MIT许可的开源软件提供，以促进社区的进一步研究和应用。"}}
{"id": "2507.17853", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17853", "abs": "https://arxiv.org/abs/2507.17853", "authors": ["Lifeng Chen", "Jiner Wang", "Zihao Pan", "Beier Zhu", "Xiaofeng Yang", "Chi Zhang"], "title": "Detail++: Training-Free Detail Enhancer for Text-to-Image Diffusion Models", "comment": null, "summary": "Recent advances in text-to-image (T2I) generation have led to impressive\nvisual results. However, these models still face significant challenges when\nhandling complex prompt, particularly those involving multiple subjects with\ndistinct attributes. Inspired by the human drawing process, which first\noutlines the composition and then incrementally adds details, we propose\nDetail++, a training-free framework that introduces a novel Progressive Detail\nInjection (PDI) strategy to address this limitation. Specifically, we decompose\na complex prompt into a sequence of simplified sub-prompts, guiding the\ngeneration process in stages. This staged generation leverages the inherent\nlayout-controlling capacity of self-attention to first ensure global\ncomposition, followed by precise refinement. To achieve accurate binding\nbetween attributes and corresponding subjects, we exploit cross-attention\nmechanisms and further introduce a Centroid Alignment Loss at test time to\nreduce binding noise and enhance attribute consistency. Extensive experiments\non T2I-CompBench and a newly constructed style composition benchmark\ndemonstrate that Detail++ significantly outperforms existing methods,\nparticularly in scenarios involving multiple objects and complex stylistic\nconditions.", "AI": {"tldr": "Detail++是一个免训练框架，通过渐进式细节注入（PDI）策略，有效解决了文生图模型在处理复杂多主体提示词时面临的挑战，实现了更好的构图和属性绑定。", "motivation": "现有文生图模型在处理包含多个具有不同属性的主体的复杂提示词时，仍面临显著挑战，难以生成令人满意的视觉结果。", "method": "受人类绘画过程启发，Detail++将复杂提示词分解为一系列简化的子提示词，分阶段引导生成过程。它利用自注意力机制确保全局构图，并通过交叉注意力机制实现属性与主体的精确绑定。此外，在测试时引入质心对齐损失（Centroid Alignment Loss）以减少绑定噪声并增强属性一致性。", "result": "在T2I-CompBench和新建的风格构图基准测试中，Detail++显著优于现有方法，特别是在涉及多个对象和复杂风格条件的场景中表现出色。", "conclusion": "Detail++通过其新颖的渐进式细节注入策略和质心对齐损失，有效提升了文生图模型处理复杂提示词的能力，尤其在多对象构图和属性绑定方面表现出卓越性能。"}}
{"id": "2507.18102", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.18102", "abs": "https://arxiv.org/abs/2507.18102", "authors": ["Yi Wang", "Goran Strbac"], "title": "Regional Frequency-Constrained Planning for the Optimal Sizing of Power Systems via Enhanced Input Convex Neural Networks", "comment": null, "summary": "Large renewable penetration has been witnessed in power systems, resulting in\nreduced levels of system inertia and increasing requirements for frequency\nresponse services. There have been plenty of studies developing\nfrequency-constrained models for power system security. However, most existing\nliterature only considers uniform frequency security, while neglecting\nfrequency spatial differences in different regions. To fill this gap, this\npaper proposes a novel planning model for the optimal sizing problem of power\nsystems, capturing regional frequency security and inter-area frequency\noscillations. Specifically, regional frequency constraints are first extracted\nvia an enhanced input convex neural network (ICNN) and then embedded into the\noriginal optimisation for frequency security, where a principled weight\ninitialisation strategy is adopted to deal with the gradient vanishing issues\nof non-negative weights in traditional ICNNs and enhance its fitting ability.\nAn adaptive genetic algorithm with sparsity calculation and local search is\ndeveloped to separate the planning model into two stages and effectively solve\nit iteratively. Case studies have been conducted on three different power\nsystems to verify the effectiveness of the proposed frequency-constrained\nplanning model in ensuring regional system security and obtaining realistic\ninvestment decisions.", "AI": {"tldr": "针对可再生能源高渗透下电力系统区域频率安全规划问题，提出一种新型规划模型，通过增强型ICNN提取区域频率约束，并结合自适应遗传算法求解，以确保区域系统安全并优化投资决策。", "motivation": "随着可再生能源渗透率提高，电力系统惯量降低，频率响应需求增加。现有频率安全模型多考虑统一频率安全，忽略了不同区域的频率空间差异和区域间振荡。", "method": "本文提出一种考虑区域频率安全和区域间频率振荡的电力系统最优规模规划模型。具体地，通过增强型输入凸神经网络（ICNN）提取区域频率约束，该ICNN采用了新的权重初始化策略以解决梯度消失问题并提升拟合能力。为有效求解，开发了一种带有稀疏度计算和局部搜索的自适应遗传算法，将规划模型分为两阶段进行迭代求解。", "result": "在三种不同电力系统上进行了案例研究，验证了所提出的频率约束规划模型在确保区域系统安全性以及获得实际投资决策方面的有效性。", "conclusion": "所提出的模型能够有效解决高可再生能源渗透背景下电力系统的区域频率安全规划问题，并能得到符合实际的投资决策，填补了现有研究中忽视区域频率差异的空白。"}}
{"id": "2507.17971", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.17971", "abs": "https://arxiv.org/abs/2507.17971", "authors": ["Deepa Krishnaswamy", "Cosmin Ciausu", "Steve Pieper", "Ron Kikinis", "Benjamin Billot", "Andrey Fedorov"], "title": "Benchmarking of Deep Learning Methods for Generic MRI Multi-OrganAbdominal Segmentation", "comment": null, "summary": "Recent advances in deep learning have led to robust automated tools for\nsegmentation of abdominal computed tomography (CT). Meanwhile, segmentation of\nmagnetic resonance imaging (MRI) is substantially more challenging due to the\ninherent signal variability and the increased effort required for annotating\ntraining datasets. Hence, existing approaches are trained on limited sets of\nMRI sequences, which might limit their generalizability. To characterize the\nlandscape of MRI abdominal segmentation tools, we present here a comprehensive\nbenchmarking of the three state-of-the-art and open-source models:\nMRSegmentator, MRISegmentator-Abdomen, and TotalSegmentator MRI. Since these\nmodels are trained using labor-intensive manual annotation cycles, we also\nintroduce and evaluate ABDSynth, a SynthSeg-based model purely trained on\nwidely available CT segmentations (no real images). More generally, we assess\naccuracy and generalizability by leveraging three public datasets (not seen by\nany of the evaluated methods during their training), which span all major\nmanufacturers, five MRI sequences, as well as a variety of subject conditions,\nvoxel resolutions, and fields-of-view. Our results reveal that MRSegmentator\nachieves the best performance and is most generalizable. In contrast, ABDSynth\nyields slightly less accurate results, but its relaxed requirements in training\ndata make it an alternative when the annotation budget is limited. The\nevaluation code and datasets are given for future benchmarking at\nhttps://github.com/deepakri201/AbdoBench, along with inference code and weights\nfor ABDSynth.", "AI": {"tldr": "本文对三种最先进的开源MRI腹部分割模型进行了全面基准测试，并引入了一个基于CT数据训练的新模型ABDSynth，评估了它们的准确性和泛化能力。", "motivation": "深度学习在CT腹部分割方面取得了显著进展，但MRI分割因信号变异性和高昂的标注成本而更具挑战性，导致现有模型泛化能力受限。因此，需要对MRI腹部分割工具进行系统性评估，并探索无需大量MRI标注数据的新方法。", "method": "研究评估了MRSegmentator、MRISegmentator-Abdomen和TotalSegmentator MRI三种主流开源模型。同时，引入并评估了ABDSynth，这是一个基于SynthSeg模型，纯粹使用广泛可用的CT分割数据训练，无需真实MRI图像。所有模型均在三个未曾用于训练的公共数据集上进行评估，这些数据集涵盖了主要制造商、五种MRI序列、多种受试者条件、体素分辨率和视野，以评估准确性和泛化能力。", "result": "结果显示，MRSegmentator表现最佳，泛化能力最强。ABDSynth的准确性略低，但由于其对训练数据要求较低，在标注预算有限的情况下是一个可行的替代方案。", "conclusion": "MRSegmentator是目前MRI腹部分割的最佳选择，且泛化能力强。ABDSynth为缺乏MRI标注数据的场景提供了一个有价值的替代方案。研究提供了评估代码和数据集，以便未来进行基准测试。"}}
{"id": "2507.18070", "categories": ["cs.RO", "cs.SY", "eess.SP", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.18070", "abs": "https://arxiv.org/abs/2507.18070", "authors": ["Behzad Zamani", "Jochen Trumpf", "Chris Manzie"], "title": "Modular Robot and Landmark Localisation Using Relative Bearing Measurements", "comment": "Submitted to RA-L", "summary": "In this paper we propose a modular nonlinear least squares filtering approach\nfor systems composed of independent subsystems. The state and error covariance\nestimate of each subsystem is updated independently, even when a relative\nmeasurement simultaneously depends on the states of multiple subsystems. We\nintegrate the Covariance Intersection (CI) algorithm as part of our solution in\norder to prevent double counting of information when subsystems share estimates\nwith each other. An alternative derivation of the CI algorithm based on least\nsquares estimation makes this integration possible. We particularise the\nproposed approach to the robot-landmark localization problem. In this problem,\nnoisy measurements of the bearing angle to a stationary landmark position\nmeasured relative to the SE(2) pose of a moving robot couple the estimation\nproblems for the robot pose and the landmark position. In a randomized\nsimulation study, we benchmark the proposed modular method against a monolithic\njoint state filter to elucidate their respective trade-offs. In this study we\nalso include variants of the proposed method that achieve a graceful\ndegradation of performance with reduced communication and bandwidth\nrequirements.", "AI": {"tldr": "本文提出了一种模块化非线性最小二乘滤波方法，用于由独立子系统组成的系统，即使存在相对测量，也能独立更新状态和协方差，并通过协方差交集（CI）算法防止信息重复计算，并应用于机器人-地标定位问题。", "motivation": "现有方法在处理由独立子系统组成且存在跨子系统相对测量的系统时，面临信息重复计算的挑战。研究者希望开发一种模块化方法，能够独立更新各子系统状态，同时有效处理共享信息。", "method": "该方法采用模块化非线性最小二乘滤波，允许每个子系统独立更新其状态和误差协方差。通过将协方差交集（CI）算法集成到解决方案中，并基于最小二乘估计对CI算法进行了替代推导，以防止子系统共享估计时信息重复计算。该方法特别应用于机器人-地标定位问题，其中机器人姿态和地标位置的估计问题通过相对测量耦合。通过随机模拟研究，将所提出的模块化方法与整体联合状态滤波器进行基准测试。", "result": "模拟研究表明，所提出的模块化方法可以与整体联合状态滤波器进行比较，并阐明了它们的权衡。此外，该研究还包括了所提出方法的变体，这些变体在降低通信和带宽要求的同时，实现了性能的平稳下降。", "conclusion": "所提出的模块化非线性最小二乘滤波方法能够有效处理由独立子系统组成的系统，即使存在相对测量也能独立更新。通过集成CI算法，成功解决了信息重复计算的问题。该方法在机器人-地标定位问题中表现出可行性，并在通信和带宽要求方面提供了灵活的性能权衡。"}}
{"id": "2507.17988", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17988", "abs": "https://arxiv.org/abs/2507.17988", "authors": ["Dario Della Monica", "Angelo Montanari", "Pietro Sala"], "title": "Synthesis of timeline-based planning strategies avoiding determinization", "comment": "arXiv admin note: text overlap with arXiv:2410.22757", "summary": "Qualitative timeline-based planning models domains as sets of independent,\nbut\n  interacting, components whose behaviors over time, the timelines, are\ngoverned\n  by sets of qualitative temporal constraints (ordering relations), called\n  synchronization rules.\n  Its plan-existence problem has been shown to be PSPACE-complete; in\n  particular, PSPACE-membership has been proved via reduction to the\n  nonemptiness problem for nondeterministic finite automata.\n  However, nondeterministic automata cannot be directly used to synthesize\n  planning strategies as a costly determinization step is needed.\n  In this paper, we identify a fragment of qualitative timeline-based planning\n  whose plan-existence problem can be directly mapped into the nonemptiness\n  problem of deterministic finite automata, which can then\n  synthesize strategies.\n  In addition, we identify a maximal subset of Allen's relations that fits into\n  such a deterministic fragment.", "AI": {"tldr": "本文识别了定性时间线规划的一个片段，其计划存在问题可直接映射到确定性有限自动机（DFA）的非空性问题，从而可以直接合成规划策略，并确定了适用于此片段的Allen关系的最大子集。", "motivation": "定性时间线规划的计划存在问题是PSPACE完全的，虽然可以通过归约到非确定性有限自动机（NFA）的非空性问题来证明PSPACE成员资格，但NFA无法直接用于合成规划策略，需要昂贵的确定化步骤。研究的动机是为了找到一种无需昂贵确定化步骤、能直接合成规划策略的方法。", "method": "研究方法是识别定性时间线规划的一个特定片段，使其计划存在问题可以直接映射到确定性有限自动机的非空性问题。此外，还识别了Allen关系中符合此确定性片段的最大子集。", "result": "主要结果是成功识别了一个定性时间线规划的片段，其计划存在问题可以直接映射到确定性有限自动机的非空性问题，从而可以直接合成策略。同时，也识别了适合此确定性片段的Allen关系的最大子集。", "conclusion": "结论是，通过识别定性时间线规划的一个特定片段，可以绕过非确定性有限自动机所需的昂贵确定化步骤，直接利用确定性有限自动机来合成规划策略，这为该领域的策略合成提供了一条更直接的途径。"}}
{"id": "2507.17918", "categories": ["cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.17918", "abs": "https://arxiv.org/abs/2507.17918", "authors": ["Nhan Phan", "Anusha Porwal", "Yaroslav Getman", "Ekaterina Voskoboinik", "Tamás Grósz", "Mikko Kurimo"], "title": "One Whisper to Grade Them All", "comment": "Accepted to SLaTE 2025 workshop", "summary": "We present an efficient end-to-end approach for holistic Automatic Speaking\nAssessment (ASA) of multi-part second-language tests, developed for the 2025\nSpeak & Improve Challenge. Our system's main novelty is the ability to process\nall four spoken responses with a single Whisper-small encoder, combine all\ninformation via a lightweight aggregator, and predict the final score. This\narchitecture removes the need for transcription and per-part models, cuts\ninference time, and makes ASA practical for large-scale Computer-Assisted\nLanguage Learning systems.\n  Our system achieved a Root Mean Squared Error (RMSE) of 0.384, outperforming\nthe text-based baseline (0.44) while using at most 168M parameters (about 70%\nof Whisper-small). Furthermore, we propose a data sampling strategy, allowing\nthe model to train on only 44.8% of the speakers in the corpus and still reach\n0.383 RMSE, demonstrating improved performance on imbalanced classes and strong\ndata efficiency.", "AI": {"tldr": "提出了一种高效的端到端多部分二语口语自动评估（ASA）方法，使用单个编码器处理所有语音响应并直接预测最终分数，显著提升了效率和性能。", "motivation": "现有ASA系统需要转录和独立部分模型，导致推理时间长，难以在大规模计算机辅助语言学习（CALL）系统中实际应用，因此需要一种更高效、端到端的方法。", "method": "核心方法是使用单个Whisper-small编码器处理所有四个语音响应，并通过一个轻量级聚合器整合信息，直接预测最终分数。此外，提出了一种数据采样策略，以提高数据效率和处理不平衡类别。", "result": "系统实现了0.384的均方根误差（RMSE），优于基于文本的基线（0.44）。模型参数最多为1.68亿（约为Whisper-small的70%）。数据采样策略仅使用44.8%的说话者数据就达到了0.383的RMSE，显示出对不平衡类别的性能提升和强大的数据效率。", "conclusion": "所提出的端到端ASA方法通过移除转录和独立部分模型，显著缩短了推理时间，提高了效率，并在更少的参数和更少数据的情况下取得了更好的性能，使其在大规模CALL系统中具有实用性。"}}
{"id": "2507.17859", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.17859", "abs": "https://arxiv.org/abs/2507.17859", "authors": ["Muayad Abujabal", "Lyes Saad Saoud", "Irfan Hussain"], "title": "FishDet-M: A Unified Large-Scale Benchmark for Robust Fish Detection and CLIP-Guided Model Selection in Diverse Aquatic Visual Domains", "comment": null, "summary": "Accurate fish detection in underwater imagery is essential for ecological\nmonitoring, aquaculture automation, and robotic perception. However, practical\ndeployment remains limited by fragmented datasets, heterogeneous imaging\nconditions, and inconsistent evaluation protocols. To address these gaps, we\npresent \\textit{FishDet-M}, the largest unified benchmark for fish detection,\ncomprising 13 publicly available datasets spanning diverse aquatic environments\nincluding marine, brackish, occluded, and aquarium scenes. All data are\nharmonized using COCO-style annotations with both bounding boxes and\nsegmentation masks, enabling consistent and scalable cross-domain evaluation.\nWe systematically benchmark 28 contemporary object detection models, covering\nthe YOLOv8 to YOLOv12 series, R-CNN based detectors, and DETR based models.\nEvaluations are conducted using standard metrics including mAP, mAP@50, and\nmAP@75, along with scale-specific analyses (AP$_S$, AP$_M$, AP$_L$) and\ninference profiling in terms of latency and parameter count. The results\nhighlight the varying detection performance across models trained on FishDet-M,\nas well as the trade-off between accuracy and efficiency across models of\ndifferent architectures. To support adaptive deployment, we introduce a\nCLIP-based model selection framework that leverages vision-language alignment\nto dynamically identify the most semantically appropriate detector for each\ninput image. This zero-shot selection strategy achieves high performance\nwithout requiring ensemble computation, offering a scalable solution for\nreal-time applications. FishDet-M establishes a standardized and reproducible\nplatform for evaluating object detection in complex aquatic scenes. All\ndatasets, pretrained models, and evaluation tools are publicly available to\nfacilitate future research in underwater computer vision and intelligent marine\nsystems.", "AI": {"tldr": "本文提出了FishDet-M，一个最大的统一鱼类检测基准数据集，整合了13个公共数据集，并对28种主流目标检测模型进行了系统性基准测试。同时，引入了一个基于CLIP的模型选择框架，以实现自适应部署。", "motivation": "水下图像中的鱼类检测对于生态监测、水产养殖自动化和机器人感知至关重要，但实际部署受限于数据集碎片化、成像条件异构以及评估协议不一致。", "method": "构建了FishDet-M，包含13个公共数据集，涵盖多种水生环境，并统一采用COCO风格的边界框和分割掩码标注。系统性地评估了28种当代目标检测模型（包括YOLO系列、R-CNN和DETR模型），使用mAP、mAP@50、mAP@75、尺度特定AP以及推理延迟和参数量等指标。引入了基于CLIP的模型选择框架，利用视觉-语言对齐零样本动态选择最合适的检测器。", "result": "结果显示，在FishDet-M上训练的模型表现各异，不同架构模型在精度和效率之间存在权衡。基于CLIP的模型选择策略无需集成计算即可实现高性能。", "conclusion": "FishDet-M为复杂水生场景中的目标检测提供了一个标准化、可复现的评估平台，所有数据集、预训练模型和评估工具均已公开，以促进水下计算机视觉和智能海洋系统的未来研究。"}}
{"id": "2507.18110", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.18110", "abs": "https://arxiv.org/abs/2507.18110", "authors": ["Yi Wang", "Dawei Qiu", "Fei Teng", "Goran Strbac"], "title": "Two-Stage TSO-DSO Services Provision Framework for Electric Vehicle Coordination", "comment": null, "summary": "High renewable penetration has been witnessed in power systems, resulting in\nreduced system inertia and increasing requirements for frequency response\nservices. Electric vehicles (EVs), owing to their vehicle-to-grid (V2G)\ncapabilities, can provide cost-effective frequency services for transmission\nsystem operators (TSOs). However, EVs that are inherently connected to\ndistribution networks may pose voltage security issues for distribution system\noperators (DSOs) when supporting TSO frequency. To coordinate both TSO\nfrequency and DSO voltage, this paper proposes a two-stage service provision\nframework for multi-EVs. At stage one, EVs participate in day-ahead TSO-DSO\ninteractions for frequency reserve schedules; at stage two, EVs make real-time\ndispatching behaviors in distribution networks for reserve delivery while\nsupporting DSO voltage. Considering the potentially large EV number and\nenvironment complexity, a decentralized operation paradigm is introduced for\nreal-time EV dispatches at stage two, while a communication-efficient\nreinforcement learning (RL) algorithm is proposed to reduce the communication\noverhead during large-scale multi-agent RL training without compromising policy\nperformance. Case studies are carried out on a 6-bus transmission and 33-bus\ndistribution network as well as a 69-bus distribution network to evaluate the\neffectiveness and scalability of the proposed method in enabling EVs for\nfrequency service and voltage support.", "AI": {"tldr": "本文提出一个两阶段服务提供框架，使电动汽车（EVs）通过V2G能力同时为输电系统提供频率服务和为配电系统提供电压支持，并引入了通信高效的强化学习算法以实现大规模EV的去中心化实时调度。", "motivation": "可再生能源高渗透导致电力系统惯性降低，对频率响应服务需求增加。电动汽车（EVs）的V2G能力可提供经济高效的频率服务，但其接入配电网可能对配电系统运营商（DSOs）造成电压安全问题。因此，需要协调输电系统运营商（TSOs）的频率需求和DSOs的电压安全。", "method": "提出一个两阶段服务提供框架：第一阶段，EVs参与日前TSO-DSO交互以确定频率储备计划；第二阶段，EVs在配电网中进行实时调度以交付储备并支持DSO电压。针对第二阶段，引入去中心化操作范式，并提出一种通信高效的强化学习（RL）算法，以减少大规模多智能体RL训练中的通信开销，同时不影响策略性能。", "result": "通过在6总线输电/33总线配电网络以及69总线配电网络上进行案例研究，验证了所提出方法在使EVs提供频率服务和电压支持方面的有效性和可扩展性。", "conclusion": "所提出的两阶段服务提供框架结合去中心化操作和通信高效的强化学习算法，能够有效且可扩展地使电动汽车同时为输电系统提供频率服务和为配电系统提供电压支持，解决了高可再生能源渗透下电网的频率和电压协调问题。"}}
{"id": "2507.18012", "categories": ["eess.IV", "cs.CV", "physics.med-ph", "92C55, 94A08", "I.4.5; J.3"], "pdf": "https://arxiv.org/pdf/2507.18012", "abs": "https://arxiv.org/abs/2507.18012", "authors": ["Hang Xu", "Alexandre Bousse", "Alessandro Perelli"], "title": "Direct Dual-Energy CT Material Decomposition using Model-based Denoising Diffusion Model", "comment": "13 pages, 10 figures, 2 tables", "summary": "Dual-energy X-ray Computed Tomography (DECT) constitutes an advanced\ntechnology which enables automatic decomposition of materials in clinical\nimages without manual segmentation using the dependency of the X-ray linear\nattenuation with energy. However, most methods perform material decomposition\nin the image domain as a post-processing step after reconstruction but this\nprocedure does not account for the beam-hardening effect and it results in\nsub-optimal results. In this work, we propose a deep learning procedure called\nDual-Energy Decomposition Model-based Diffusion (DEcomp-MoD) for quantitative\nmaterial decomposition which directly converts the DECT projection data into\nmaterial images. The algorithm is based on incorporating the knowledge of the\nspectral DECT model into the deep learning training loss and combining a\nscore-based denoising diffusion learned prior in the material image domain.\nImportantly the inference optimization loss takes as inputs directly the\nsinogram and converts to material images through a model-based conditional\ndiffusion model which guarantees consistency of the results. We evaluate the\nperformance with both quantitative and qualitative estimation of the proposed\nDEcomp-MoD method on synthetic DECT sinograms from the low-dose AAPM dataset.\nFinally, we show that DEcomp-MoD outperform state-of-the-art unsupervised\nscore-based model and supervised deep learning networks, with the potential to\nbe deployed for clinical diagnosis.", "AI": {"tldr": "本文提出了一种名为DEcomp-MoD的深度学习方法，可直接将双能CT（DECT）投影数据转换为材料图像，实现定量材料分解，并优于现有方法。", "motivation": "大多数DECT材料分解方法在图像域进行，作为重建后的后处理步骤，但这未能考虑射束硬化效应，导致结果不理想。", "method": "DEcomp-MoD是一种深度学习流程，将光谱DECT模型知识融入训练损失，并结合基于分数的去噪扩散学习先验。推理优化直接以正弦图作为输入，通过基于模型的条件扩散模型转换为材料图像，确保结果一致性。", "result": "DEcomp-MoD在低剂量AAPM数据集的合成DECT正弦图上进行了定量和定性评估，结果显示其性能优于最先进的无监督基于分数模型和有监督深度学习网络。", "conclusion": "DEcomp-MoD方法在材料分解方面表现出色，具有应用于临床诊断的潜力。"}}
{"id": "2507.18138", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.18138", "abs": "https://arxiv.org/abs/2507.18138", "authors": ["Min-Gyu Kim", "Dongyun Kang", "Hajun Kim", "Hae-Won Park"], "title": "A Modular Residual Learning Framework to Enhance Model-Based Approach for Robust Locomotion", "comment": "8 pages, IEEE RA-L accepted (July 2025)", "summary": "This paper presents a novel approach that combines the advantages of both\nmodel-based and learning-based frameworks to achieve robust locomotion. The\nresidual modules are integrated with each corresponding part of the model-based\nframework, a footstep planner and dynamic model designed using heuristics, to\ncomplement performance degradation caused by a model mismatch. By utilizing a\nmodular structure and selecting the appropriate learning-based method for each\nresidual module, our framework demonstrates improved control performance in\nenvironments with high uncertainty, while also achieving higher learning\nefficiency compared to baseline methods. Moreover, we observed that our\nproposed methodology not only enhances control performance but also provides\nadditional benefits, such as making nominal controllers more robust to\nparameter tuning. To investigate the feasibility of our framework, we\ndemonstrated residual modules combined with model predictive control in a real\nquadrupedal robot. Despite uncertainties beyond the simulation, the robot\nsuccessfully maintains balance and tracks the commanded velocity.", "AI": {"tldr": "本文提出一种结合模型基和学习基方法的新型框架，通过残差模块补偿模型失配，实现机器人鲁棒运动。", "motivation": "现有模型基框架在模型失配或高不确定性环境中性能下降，需要一种方法来弥补这种性能损失。", "method": "将学习基的残差模块集成到模型基框架（包括步态规划器和动力学模型）的各个对应部分，以补偿模型失配。采用模块化结构并为每个残差模块选择合适的学习方法。在四足机器人上结合模型预测控制（MPC）进行验证。", "result": "该框架在高不确定性环境中表现出更好的控制性能和更高的学习效率。此外，它使标称控制器对参数调整更鲁棒。在真实四足机器人上，尽管存在仿真之外的不确定性，机器人仍成功保持平衡并跟踪指令速度。", "conclusion": "所提出的混合框架对于在不确定环境中实现鲁棒机器人运动是可行且有效的，能够提升控制性能并增强控制器对参数调整的鲁棒性。"}}
{"id": "2507.18004", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18004", "abs": "https://arxiv.org/abs/2507.18004", "authors": ["Yusen Peng", "Shuhua Mao"], "title": "E.A.R.T.H.: Structuring Creative Evolution through Model Error in Generative AI", "comment": "44 pages,11 figures", "summary": "How can AI move beyond imitation toward genuine creativity? This paper\nproposes the E.A.R.T.H. framework, a five-stage generative pipeline that\ntransforms model-generated errors into creative assets through Error\ngeneration, Amplification, Refine selection, Transform, and Harness feedback.\nDrawing on cognitive science and generative modeling, we posit that \"creative\npotential hides in failure\" and operationalize this via structured prompts,\nsemantic scoring, and human-in-the-loop evaluation. Implemented using\nLLaMA-2-7B-Chat, SBERT, BERTScore, CLIP, BLIP-2, and Stable Diffusion, the\npipeline employs a composite reward function based on novelty, surprise, and\nrelevance. At the Refine stage, creativity scores increase by 52.5% (1.179 to\n1.898, t = -5.56, p < 0.001), with final outputs reaching 2.010 - a 70.4%\nimprovement. Refined slogans are 48.4% shorter, 40.7% more novel, with only a\n4.0% drop in relevance. Cross-modal tests show strong slogan-to-image alignment\n(CLIPScore: 0.249; BERTScore F1: 0.816). In human evaluations, 60% of outputs\nscored >= 4.0, with metaphorical slogans (avg. 4.09) outperforming literal ones\n(3.99). Feedback highlights stylistic precision and emotional resonance. These\nresults demonstrate that error-centered, feedback-driven generation enhances\ncreativity, offering a scalable path toward self-evolving, human-aligned\ncreative AI.", "AI": {"tldr": "该论文提出E.A.R.T.H.框架，一个五阶段生成流程，通过将模型生成的错误转化为创意资产，显著提升了AI的创造力。", "motivation": "当前的AI模型倾向于模仿，而非展现真正的创造力。研究旨在探索如何让AI超越模仿，实现真正的创造性，并提出“创造潜力隐藏在失败中”的观点。", "method": "提出了E.A.R.T.H.框架，包含错误生成、放大、精炼选择、转换和反馈利用五个阶段。技术实现上，使用了LLaMA-2-7B-Chat、SBERT、BERTScore、CLIP、BLIP-2和Stable Diffusion等模型，并采用基于新颖性、惊喜度和相关性的复合奖励函数。方法还包括结构化提示、语义评分和人机协作评估。", "result": "在精炼阶段，创造力得分提升了52.5%（从1.179到1.898），最终输出达到2.010，总提升70.4%。精炼后的标语短了48.4%，新颖性增加了40.7%，相关性仅下降4.0%。跨模态测试显示标语与图像对齐度高（CLIPScore: 0.249; BERTScore F1: 0.816）。在人工评估中，60%的输出得分>=4.0，其中隐喻性标语（平均4.09）优于字面性标语（3.99）。反馈强调了风格精确性和情感共鸣。", "conclusion": "以错误为中心、反馈驱动的生成方法能够显著增强AI的创造力，为实现自我演化、与人类对齐的创意AI提供了一条可扩展的路径。"}}
{"id": "2507.17944", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17944", "abs": "https://arxiv.org/abs/2507.17944", "authors": ["Hulayyil Alshammari", "Praveen Rao"], "title": "Evaluating the Performance of AI Text Detectors, Few-Shot and Chain-of-Thought Prompting Using DeepSeek Generated Text", "comment": null, "summary": "Large language models (LLMs) have rapidly transformed the creation of written\nmaterials. LLMs have led to questions about writing integrity, thereby driving\nthe creation of artificial intelligence (AI) detection technologies.\nAdversarial attacks, such as standard and humanized paraphrasing, inhibit\ndetectors' ability to detect machine-generated text. Previous studies have\nmainly focused on ChatGPT and other well-known LLMs and have shown varying\naccuracy across detectors. However, there is a clear gap in the literature\nabout DeepSeek, a recently published LLM. Therefore, in this work, we\ninvestigate whether six generally accessible AI detection tools -- AI Text\nClassifier, Content Detector AI, Copyleaks, QuillBot, GPT-2, and GPTZero -- can\nconsistently recognize text generated by DeepSeek. The detectors were exposed\nto the aforementioned adversarial attacks. We also considered DeepSeek as a\ndetector by performing few-shot prompting and chain-of-thought reasoning (CoT)\nfor classifying AI and human-written text. We collected 49 human-authored\nquestion-answer pairs from before the LLM era and generated matching responses\nusing DeepSeek-v3, producing 49 AI-generated samples. Then, we applied\nadversarial techniques such as paraphrasing and humanizing to add 196 more\nsamples. These were used to challenge detector robustness and assess accuracy\nimpact. While QuillBot and Copyleaks showed near-perfect performance on\noriginal and paraphrased DeepSeek text, others -- particularly AI Text\nClassifier and GPT-2 -- showed inconsistent results. The most effective attack\nwas humanization, reducing accuracy to 71% for Copyleaks, 58% for QuillBot, and\n52% for GPTZero. Few-shot and CoT prompting showed high accuracy, with the best\nfive-shot result misclassifying only one of 49 samples (AI recall 96%, human\nrecall 100%).", "AI": {"tldr": "本研究评估了六种AI检测工具在识别DeepSeek生成文本方面的能力，并测试了标准和拟人化复述等对抗性攻击的影响。此外，还探索了DeepSeek本身作为检测器的性能。结果显示，拟人化攻击显著降低了检测器准确率，而DeepSeek结合少样本和思维链推理表现出高检测精度。", "motivation": "大型语言模型（LLMs）的快速发展引发了对写作完整性的担忧，催生了AI检测技术。然而，现有研究主要集中在ChatGPT等知名LLMs上，对于新发布的DeepSeek模型的检测能力存在明显空白。此外，对抗性攻击（如复述）会抑制检测器的性能，需要评估其对DeepSeek生成文本检测的影响。", "method": "研究使用了六个常用的AI检测工具（AI Text Classifier, Content Detector AI, Copyleaks, QuillBot, GPT-2, GPTZero）来识别DeepSeek生成的文本。收集了49对LLM时代前的人类问答对，并使用DeepSeek-v3生成了匹配的49个AI样本。接着，应用了复述和拟人化等对抗性技术，额外生成了196个样本以挑战检测器鲁棒性。同时，通过少样本提示和思维链推理（CoT）的方式，将DeepSeek自身也作为AI和人类文本的分类器进行评估。", "result": "QuillBot和Copyleaks在原始及复述的DeepSeek文本上表现接近完美，但其他工具（特别是AI Text Classifier和GPT-2）表现不稳定。最有效的攻击是拟人化，将Copyleaks的准确率降至71%，QuillBot降至58%，GPTZero降至52%。DeepSeek作为检测器，通过少样本和思维链提示展现出高准确率，最佳的五样本结果仅错误分类了49个样本中的一个（AI召回率96%，人类召回率100%）。", "conclusion": "当前市面上的AI检测工具在识别DeepSeek生成文本方面表现不一，且拟人化复述是有效的对抗性攻击，能显著降低检测准确率。值得注意的是，LLM本身（如DeepSeek结合少样本和思维链推理）在区分AI生成和人类编写文本方面展现出极高的潜力，可能成为未来AI文本检测的有效方法。"}}
{"id": "2507.17860", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.17860", "abs": "https://arxiv.org/abs/2507.17860", "authors": ["Ko Watanabe. Stanislav Frolov. Adriano Lucieri. Andreas Dengel"], "title": "Towards Facilitated Fairness Assessment of AI-based Skin Lesion Classifiers Through GenAI-based Image Synthesis", "comment": null, "summary": "Recent advancements in Deep Learning and its application on the edge hold\ngreat potential for the revolution of routine screenings for skin cancers like\nMelanoma. Along with the anticipated benefits of this technology, potential\ndangers arise from unforseen and inherent biases. Thus, assessing and improving\nthe fairness of such systems is of utmost importance. A key challenge in\nfairness assessment is to ensure that the evaluation dataset is sufficiently\nrepresentative of different Personal Identifiable Information (PII) (sex, age,\nand race) and other minority groups. Against the backdrop of this challenge,\nthis study leverages the state-of-the-art Generative AI (GenAI) LightningDiT\nmodel to assess the fairness of publicly available melanoma classifiers. The\nresults suggest that fairness assessment using highly realistic synthetic data\nis a promising direction. Yet, our findings indicate that verifying fairness\nbecomes difficult when the melanoma-detection model used for evaluation is\ntrained on data that differ from the dataset underpinning the synthetic images.\nNonetheless, we propose that our approach offers a valuable new avenue for\nemploying synthetic data to gauge and enhance fairness in medical-imaging GenAI\nsystems.", "AI": {"tldr": "本研究利用生成式AI（LightningDiT）生成合成数据来评估皮肤癌检测深度学习模型的公平性，发现合成数据评估有前景，但受限于评估模型训练数据与合成数据基础数据的不一致性。", "motivation": "深度学习在皮肤癌筛查中潜力巨大，但存在潜在的固有偏见。评估和提高这些系统的公平性至关重要，尤其需要确保评估数据集能充分代表不同个人身份信息（如性别、年龄、种族）和少数群体。", "method": "利用最先进的生成式AI（GenAI）LightningDiT模型生成高度逼真的合成数据。然后，使用这些合成数据来评估公开可用的黑色素瘤分类器的公平性。", "result": "使用高度逼真的合成数据进行公平性评估是一个有前景的方向。然而，当用于评估的黑色素瘤检测模型所训练的数据与合成图像的基础数据集不同时，验证公平性变得困难。", "conclusion": "该研究提出的方法为利用合成数据来衡量和增强医疗影像GenAI系统的公平性提供了一个有价值的新途径，尽管存在数据不匹配的挑战。"}}
{"id": "2507.18131", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.18131", "abs": "https://arxiv.org/abs/2507.18131", "authors": ["Behrad Samari", "Henrik Sandberg", "Karl H. Johansson", "Abolfazl Lavaei"], "title": "Data-Driven Model Order Reduction for Continuous- and Discrete-Time Nonlinear Systems", "comment": null, "summary": "Model order reduction simplifies high-dimensional dynamical systems by\nderiving lower-dimensional models that preserve essential system\ncharacteristics. These techniques are crucial to controller design for complex\nsystems while significantly reducing computational costs. Nevertheless,\nconstructing effective reduced-order models (ROMs) poses considerable\nchallenges, particularly for dynamical systems characterized by highly\nnonlinear terms. These challenges are further exacerbated when the actual\nsystem model is unavailable, a scenario frequently encountered in real-world\napplications. In this work, we propose a data-driven framework for the\nconstruction of ROMs for both continuous- and discrete-time nonlinear dynamical\nsystems with unknown mathematical models. By leveraging two sets of data\ncollected from the system, referred to as two input-state trajectories, we\nfirst construct a data-based closed-loop representation of the system. We then\nestablish a similarity relation between the output trajectories of the original\nsystem and those of its data-driven ROM employing the notion of simulation\nfunctions (SFs), thereby enabling a formal characterization of their closeness.\nTo achieve this, we propose data-dependent semidefinite programs as sufficient\nconditions to simultaneously construct both ROMs and SFs, while offering\ncorrectness guarantees. We demonstrate that the obtained data-driven ROMs can\nbe employed for synthesizing controllers that ensure the unknown system\nsatisfies high-level logic properties. This is accomplished by first designing\ncontrollers for the data-driven ROMs and then translating the results back to\nthe original system through an interface function. We evaluate the efficacy of\nour data-driven findings through four benchmark case studies involving unknown\ndynamics with highly nonlinear terms.", "AI": {"tldr": "本文提出了一种数据驱动的框架，用于构建未知数学模型的连续和离散时间非线性动力系统的降阶模型（ROMs），并通过仿真函数提供正确性保证，最终应用于控制器设计。", "motivation": "高维动态系统的模型阶次降低对于复杂系统的控制器设计至关重要，能显著降低计算成本。然而，为高度非线性的动态系统构建有效的降阶模型极具挑战，尤其是在实际系统模型不可用时，这种场景在现实应用中频繁出现。", "method": "该研究利用从系统收集的两组输入-状态轨迹数据，首先构建了系统的数据驱动闭环表示。然后，通过仿真函数（SFs）的概念，建立了原始系统输出轨迹与其数据驱动ROM输出轨迹之间的相似关系，从而正式表征它们的接近程度。为此，作者提出了数据依赖的半定规划（SDPs）作为充分条件，以同时构建ROMs和SFs，并提供正确性保证。最后，通过为数据驱动的ROMs设计控制器，并通过接口函数将结果转换回原始系统，实现了对未知系统满足高级逻辑属性的控制器合成。", "result": "研究表明，所获得的数据驱动降阶模型可以用于合成控制器，确保未知系统满足高级逻辑属性。通过四个涉及未知高度非线性动态的基准案例研究，验证了该数据驱动方法的有效性。", "conclusion": "本文成功开发了一种数据驱动的框架，能够为具有未知数学模型的高度非线性动态系统构建有效的降阶模型。该框架不仅提供了正式的正确性保证，还能够支持针对原始未知系统的高级控制器合成，从而显著降低了复杂系统控制器设计的难度和计算成本。"}}
{"id": "2507.18112", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18112", "abs": "https://arxiv.org/abs/2507.18112", "authors": ["Binghua Li", "Ziqing Chang", "Tong Liang", "Chao Li", "Toshihisa Tanaka", "Shigeki Aoki", "Qibin Zhao", "Zhe Sun"], "title": "Parameter-Efficient Fine-Tuning of 3D DDPM for MRI Image Generation Using Tensor Networks", "comment": null, "summary": "We address the challenge of parameter-efficient fine-tuning (PEFT) for\nthree-dimensional (3D) U-Net-based denoising diffusion probabilistic models\n(DDPMs) in magnetic resonance imaging (MRI) image generation. Despite its\npractical significance, research on parameter-efficient representations of 3D\nconvolution operations remains limited. To bridge this gap, we propose Tensor\nVolumetric Operator (TenVOO), a novel PEFT method specifically designed for\nfine-tuning DDPMs with 3D convolutional backbones. Leveraging tensor network\nmodeling, TenVOO represents 3D convolution kernels with lower-dimensional\ntensors, effectively capturing complex spatial dependencies during fine-tuning\nwith few parameters. We evaluate TenVOO on three downstream brain MRI\ndatasets-ADNI, PPMI, and BraTS2021-by fine-tuning a DDPM pretrained on 59,830\nT1-weighted brain MRI scans from the UK Biobank. Our results demonstrate that\nTenVOO achieves state-of-the-art performance in multi-scale structural\nsimilarity index measure (MS-SSIM), outperforming existing approaches in\ncapturing spatial dependencies while requiring only 0.3% of the trainable\nparameters of the original model. Our code is available at:\nhttps://github.com/xiaovhua/tenvoo", "AI": {"tldr": "本文提出TenVOO，一种参数高效微调（PEFT）方法，专为基于3D U-Net的去噪扩散概率模型（DDPMs）在MRI图像生成中的应用设计，通过张量网络建模高效捕获空间依赖性。", "motivation": "3D U-Net-based DDPMs在MRI图像生成中具有重要实践意义，但针对其参数高效微调（PEFT）的研究，特别是3D卷积操作的参数高效表示，仍然非常有限，存在研究空白。", "method": "提出Tensor Volumetric Operator (TenVOO)方法，利用张量网络建模将3D卷积核表示为低维张量，从而在微调过程中以少量参数有效捕获复杂的空间依赖性。该方法在UK Biobank预训练的DDPM上进行微调，并在ADNI、PPMI和BraTS2021三个脑MRI数据集上进行评估。", "result": "TenVOO在多尺度结构相似性指数（MS-SSIM）方面取得了最先进的性能，在捕获空间依赖性方面优于现有方法，同时仅需要原始模型0.3%的可训练参数。", "conclusion": "TenVOO是一种高效的参数高效微调方法，能够有效应用于基于3D U-Net的DDPMs在MRI图像生成中的微调，在大幅减少参数的同时保持甚至超越现有方法的性能，有效解决了3D卷积操作的参数效率问题。"}}
{"id": "2507.18160", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.18160", "abs": "https://arxiv.org/abs/2507.18160", "authors": ["Luka Šiktar", "Branimir Ćaran", "Bojan Šekoranja", "Marko Švaco"], "title": "Autonomous UAV Navigation for Search and Rescue Missions Using Computer Vision and Convolutional Neural Networks", "comment": "The paper is accepted and presented on the 34th International\n  Conference on Robotics in Alpe-Adria-Danube Region, RAAD 2025, Belgrade\n  Serbia", "summary": "In this paper, we present a subsystem, using Unmanned Aerial Vehicles (UAV),\nfor search and rescue missions, focusing on people detection, face recognition\nand tracking of identified individuals. The proposed solution integrates a UAV\nwith ROS2 framework, that utilizes multiple convolutional neural networks (CNN)\nfor search missions. System identification and PD controller deployment are\nperformed for autonomous UAV navigation. The ROS2 environment utilizes the\nYOLOv11 and YOLOv11-pose CNNs for tracking purposes, and the dlib library CNN\nfor face recognition. The system detects a specific individual, performs face\nrecognition and starts tracking. If the individual is not yet known, the UAV\noperator can manually locate the person, save their facial image and\nimmediately initiate the tracking process. The tracking process relies on\nspecific keypoints identified on the human body using the YOLOv11-pose CNN\nmodel. These keypoints are used to track a specific individual and maintain a\nsafe distance. To enhance accurate tracking, system identification is\nperformed, based on measurement data from the UAVs IMU. The identified system\nparameters are used to design PD controllers that utilize YOLOv11-pose to\nestimate the distance between the UAVs camera and the identified individual.\nThe initial experiments, conducted on 14 known individuals, demonstrated that\nthe proposed subsystem can be successfully used in real time. The next step\ninvolves implementing the system on a large experimental UAV for field use and\nintegrating autonomous navigation with GPS-guided control for rescue operations\nplanning.", "AI": {"tldr": "本文提出一个基于无人机（UAV）的搜救子系统，结合ROS2框架和多种卷积神经网络（CNN），实现人员检测、面部识别和目标追踪功能，并利用系统辨识和PD控制器进行自主导航和距离保持。", "motivation": "旨在提升搜救任务的效率和准确性，通过无人机整合先进的视觉识别和追踪技术，实现对失踪人员的自动搜索、识别和持续追踪，以应对传统搜救方式的挑战。", "method": "该系统将无人机与ROS2框架集成，利用YOLOv11和YOLOv11-pose CNN进行人员追踪和关键点识别，使用dlib库的CNN进行面部识别。通过无人机IMU数据进行系统辨识，并设计PD控制器实现自主导航和与目标的安全距离保持。系统支持对已知人员的自动识别追踪，也允许操作员手动录入未知人员信息并启动追踪。", "result": "对14名已知个体的初步实验表明，所提出的子系统能够实时成功运行，验证了其在实际搜救场景中的可行性。", "conclusion": "该无人机搜救子系统在人员检测、面部识别和追踪方面表现出良好的实时性能。下一步计划将系统部署到大型实验无人机上进行现场使用，并集成GPS引导的自主导航功能，以支持更复杂的救援行动规划。"}}
{"id": "2507.18022", "categories": ["cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18022", "abs": "https://arxiv.org/abs/2507.18022", "authors": ["Victoria R. Li", "Johnathan Sun", "Martin Wattenberg"], "title": "Does visualization help AI understand data?", "comment": "5 pages, 6 figures", "summary": "Charts and graphs help people analyze data, but can they also be useful to AI\nsystems? To investigate this question, we perform a series of experiments with\ntwo commercial vision-language models: GPT 4.1 and Claude 3.5. Across three\nrepresentative analysis tasks, the two systems describe synthetic datasets more\nprecisely and accurately when raw data is accompanied by a scatterplot,\nespecially as datasets grow in complexity. Comparison with two baselines --\nproviding a blank chart and a chart with mismatched data -- shows that the\nimproved performance is due to the content of the charts. Our results are\ninitial evidence that AI systems, like humans, can benefit from visualization.", "AI": {"tldr": "研究发现，AI系统在分析数据时，像人类一样，能够从数据可视化（如散点图）中获益，尤其是在处理复杂数据集时。", "motivation": "探讨数据图表（如散点图）除了帮助人类分析数据外，是否也能对AI系统有所助益。", "method": "使用两个商业视觉-语言模型（GPT 4.1和Claude 3.5），在三个代表性分析任务上进行实验。通过提供原始数据、附带散点图的数据、空白图表和数据不匹配图表等不同形式的数据，比较模型对合成数据集的描述精度和准确性。", "result": "当原始数据附带散点图时，两个AI系统都能更精确、更准确地描述合成数据集，尤其是在数据集复杂度增加时。与提供空白图表和数据不匹配图表的基线比较表明，性能提升是由于图表内容的帮助。", "conclusion": "初步证据表明，AI系统与人类一样，可以从数据可视化中受益。"}}
{"id": "2507.17951", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17951", "abs": "https://arxiv.org/abs/2507.17951", "authors": ["Sohaib Imran", "Ihor Kendiukhov", "Matthew Broerman", "Aditya Thomas", "Riccardo Campanella", "Rob Lamb", "Peter M. Atkinson"], "title": "Are LLM Belief Updates Consistent with Bayes' Theorem?", "comment": "Accepted at the ICML 2025 Workshop on Assessing World Models", "summary": "Do larger and more capable language models learn to update their \"beliefs\"\nabout propositions more consistently with Bayes' theorem when presented with\nevidence in-context? To test this, we formulate a Bayesian Coherence\nCoefficient (BCC) metric and generate a dataset with which to measure the BCC.\nWe measure BCC for multiple pre-trained-only language models across five model\nfamilies, comparing against the number of model parameters, the amount of\ntraining data, and model scores on common benchmarks. Our results provide\nevidence for our hypothesis that larger and more capable pre-trained language\nmodels assign credences that are more coherent with Bayes' theorem. These\nresults have important implications for our understanding and governance of\nLLMs.", "AI": {"tldr": "研究发现，更大、能力更强的预训练语言模型在上下文推理中，其信念更新与贝叶斯定理的一致性更高。", "motivation": "探究大型语言模型（LLMs）在接收上下文证据时，是否能更一致地按照贝叶斯定理更新其对命题的“信念”，特别是模型规模和能力对其贝叶斯一致性的影响。", "method": "开发了贝叶斯一致性系数（BCC）指标，并生成了用于测量BCC的数据集。测量了来自五个模型家族的多个仅预训练语言模型的BCC，并将其与模型参数数量、训练数据量以及在常见基准测试上的得分进行比较。", "result": "结果支持了假设，即更大、能力更强的预训练语言模型所分配的置信度与贝叶斯定理更具一致性。", "conclusion": "更大的LLMs在信念更新上更符合贝叶斯定理，这一发现对于理解和管理大型语言模型具有重要意义。"}}
{"id": "2507.17892", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.17892", "abs": "https://arxiv.org/abs/2507.17892", "authors": ["Hanzhou Liu", "Binghan Li", "Chengkai Liu", "Mi Lu"], "title": "DiNAT-IR: Exploring Dilated Neighborhood Attention for High-Quality Image Restoration", "comment": null, "summary": "Transformers, with their self-attention mechanisms for modeling long-range\ndependencies, have become a dominant paradigm in image restoration tasks.\nHowever, the high computational cost of self-attention limits scalability to\nhigh-resolution images, making efficiency-quality trade-offs a key research\nfocus. To address this, Restormer employs channel-wise self-attention, which\ncomputes attention across channels instead of spatial dimensions. While\neffective, this approach may overlook localized artifacts that are crucial for\nhigh-quality image restoration. To bridge this gap, we explore Dilated\nNeighborhood Attention (DiNA) as a promising alternative, inspired by its\nsuccess in high-level vision tasks. DiNA balances global context and local\nprecision by integrating sliding-window attention with mixed dilation factors,\neffectively expanding the receptive field without excessive overhead. However,\nour preliminary experiments indicate that directly applying this global-local\ndesign to the classic deblurring task hinders accurate visual restoration,\nprimarily due to the constrained global context understanding within local\nattention. To address this, we introduce a channel-aware module that\ncomplements local attention, effectively integrating global context without\nsacrificing pixel-level precision. The proposed DiNAT-IR, a Transformer-based\narchitecture specifically designed for image restoration, achieves competitive\nresults across multiple benchmarks, offering a high-quality solution for\ndiverse low-level computer vision problems.", "AI": {"tldr": "该研究提出了一种名为DiNAT-IR的Transformer架构，通过结合膨胀邻域注意力（DiNA）和通道感知模块，有效平衡了图像恢复中的全局上下文和局部精度，解决了传统Transformer计算成本高和通道注意力忽略局部细节的问题。", "motivation": "Transformer在图像恢复中表现出色，但自注意力机制计算成本高，限制了其在高分辨率图像上的应用。现有的通道自注意力方法（如Restormer）虽然效率高，但可能忽略对高质量恢复至关重要的局部伪影。直接应用DiNA在去模糊任务中因全局上下文受限而表现不佳，因此需要一种新的方法来弥补这一差距。", "method": "研究首先探索了膨胀邻域注意力（DiNA）作为替代方案，该方法通过混合膨胀因子将滑动窗口注意力与全局上下文和局部精度相结合。针对DiNA在去模糊任务中全局上下文理解不足的问题，引入了一个通道感知模块来补充局部注意力，从而在不牺牲像素级精度的情况下有效整合全局上下文。最终提出了基于Transformer的DiNAT-IR架构。", "result": "所提出的DiNAT-IR架构在多个图像恢复基准测试中取得了具有竞争力的结果，为各种低级计算机视觉问题提供了高质量的解决方案。", "conclusion": "DiNAT-IR通过其创新的全局-局部上下文整合设计，克服了传统Transformer在图像恢复任务中的效率和精度权衡问题，为低级视觉任务提供了一个高性能的Transformer解决方案。"}}
{"id": "2507.18141", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.18141", "abs": "https://arxiv.org/abs/2507.18141", "authors": ["Mahdieh Zaker", "David Angeli", "Abolfazl Lavaei"], "title": "Data-Driven Incremental GAS Certificate of Nonlinear Homogeneous Networks: A Formal Modular Approach", "comment": null, "summary": "This work focuses on a compositional data-driven approach to verify\nincremental global asymptotic stability (delta-GAS) over interconnected\nhomogeneous networks of degree one with unknown mathematical dynamics. Our\nproposed approach leverages the concept of incremental input-to-state stability\n(delta-ISS) of subsystems, characterized by delta-ISS Lyapunov functions. To\nimplement our data-driven scheme, we initially reframe the delta-ISS Lyapunov\nconditions as a robust optimization program (ROP). However, due to the presence\nof unknown subsystem dynamics in the ROP constraints, we develop a scenario\noptimization program (SOP) by gathering data from trajectories of each unknown\nsubsystem. We solve the SOP and construct a delta-ISS Lyapunov function for\neach subsystem with unknown dynamics. We then leverage a small-gain\ncompositional condition to facilitate the construction of an incremental\nLyapunov function for an unknown interconnected network with unknown dynamics\nbased on its data-driven delta-ISS Lyapunov functions of individual subsystems,\nwhile providing correctness guarantees. We demonstrate that our data-driven\ncompositional approach aligns sample complexity with subsystem granularity,\nresulting in a linear increase in required data as the number of subsystems\nrises. In contrast, the existing monolithic approach in the literature exhibits\nexponential growth in sample complexity with increasing number of subsystems,\nrendering it impractical for real-world applications. To validate the\neffectiveness of our compositional data-driven approach, we apply it to an\nunknown nonlinear homogeneous network of degree one, comprising 10000\nsubsystems. By gathering data from each unknown subsystem, we demonstrate that\nthe interconnected network is delta-GAS with a correctness guarantee.", "AI": {"tldr": "本文提出了一种数据驱动的组合方法，用于验证具有未知动态的互连同构网络的增量全局渐近稳定性（delta-GAS），其数据复杂度随子系统数量线性增长，解决了传统方法的指数级增长问题。", "motivation": "现有验证互连系统稳定性的整体（monolithic）方法在数据复杂度方面呈指数级增长，对于实际应用中的大规模系统变得不切实际。因此，需要一种更高效、可扩展的数据驱动方法来处理具有未知动态的大型互连网络。", "method": "该方法利用子系统的增量输入到状态稳定性（delta-ISS）概念，通过delta-ISS Lyapunov函数进行表征。首先，将delta-ISS Lyapunov条件重新表述为鲁棒优化问题（ROP）。然后，由于ROP约束中存在未知子系统动态，通过收集每个未知子系统轨迹数据，开发了一个场景优化问题（SOP）并求解，为每个子系统构建delta-ISS Lyapunov函数。最后，利用小增益组合条件，基于子系统的数据驱动delta-ISS Lyapunov函数，为具有未知动态的互连网络构建增量Lyapunov函数，并提供正确性保证。", "result": "该数据驱动组合方法将样本复杂度与子系统粒度对齐，所需数据量随子系统数量线性增加。与现有整体方法相比，后者样本复杂度呈指数增长，不适用于实际应用。通过在一个包含10000个子系统的未知非线性同构网络上进行验证，证明了互连网络是delta-GAS的，并提供了正确性保证。", "conclusion": "所提出的数据驱动组合方法能够有效且可扩展地验证具有未知动态的大规模互连网络的增量全局渐近稳定性，其线性增长的数据复杂度使其在实际应用中具有可行性，克服了传统整体方法的局限性。"}}
{"id": "2507.18126", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18126", "abs": "https://arxiv.org/abs/2507.18126", "authors": ["Juexin Zhang", "Ying Weng", "Ke Chen"], "title": "U-Net Based Healthy 3D Brain Tissue Inpainting", "comment": "Accepted by the International Brain Tumor Segmentation (BraTS)\n  challenge organized at MICCAI 2024 conference. Included 7 pages, 2 figures", "summary": "This paper introduces a novel approach to synthesize healthy 3D brain tissue\nfrom masked input images, specifically focusing on the task of 'ASNR-MICCAI\nBraTS Local Synthesis of Tissue via Inpainting'. Our proposed method employs a\nU-Net-based architecture, which is designed to effectively reconstruct the\nmissing or corrupted regions of brain MRI scans. To enhance our model's\ngeneralization capabilities and robustness, we implement a comprehensive data\naugmentation strategy that involves randomly masking healthy images during\ntraining. Our model is trained on the BraTS-Local-Inpainting dataset and\ndemonstrates the exceptional performance in recovering healthy brain tissue.\nThe evaluation metrics employed, including Structural Similarity Index (SSIM),\nPeak Signal-to-Noise Ratio (PSNR), and Mean Squared Error (MSE), consistently\nyields impressive results. On the BraTS-Local-Inpainting validation set, our\nmodel achieved an SSIM score of 0.841, a PSNR score of 23.257, and an MSE score\nof 0.007. Notably, these evaluation metrics exhibit relatively low standard\ndeviations, i.e., 0.103 for SSIM score, 4.213 for PSNR score and 0.007 for MSE\nscore, which indicates that our model's reliability and consistency across\nvarious input scenarios. Our method also secured first place in the challenge.", "AI": {"tldr": "本文提出了一种基于U-Net的3D脑组织修复新方法，通过在训练中随机掩膜健康图像进行数据增强，在BraTS-Local-Inpainting挑战中取得了第一名，并展示了出色的性能和稳定性。", "motivation": "研究动机是解决“ASNR-MICCAI BraTS局部组织合成通过修复”任务，即从带掩膜的输入图像中合成健康的3D脑组织，重建缺失或损坏的脑部MRI区域。", "method": "采用基于U-Net的架构来重建缺失或损坏的脑部MRI扫描区域。通过在训练期间随机掩膜健康图像，实施了全面的数据增强策略，以增强模型的泛化能力和鲁棒性。", "result": "模型在BraTS-Local-Inpainting验证集上取得了卓越性能：SSIM评分为0.841，PSNR评分为23.257，MSE评分为0.007。这些评估指标的标准偏差较低（SSIM为0.103，PSNR为4.213，MSE为0.007），表明模型在各种输入场景下具有可靠性和一致性。该方法还在挑战中获得了第一名。", "conclusion": "所提出的基于U-Net的方法能够有效且可靠地从带掩膜的输入图像中合成健康的3D脑组织，在相关挑战中表现出领先的性能和高度的一致性。"}}
{"id": "2507.18206", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18206", "abs": "https://arxiv.org/abs/2507.18206", "authors": ["Arup Kumar Sahoo", "Itzik Klein"], "title": "MoRPI-PINN: A Physics-Informed Framework for Mobile Robot Pure Inertial Navigation", "comment": "9 pages, 5 figures", "summary": "A fundamental requirement for full autonomy in mobile robots is accurate\nnavigation even in situations where satellite navigation or cameras are\nunavailable. In such practical situations, relying only on inertial sensors\nwill result in navigation solution drift due to the sensors' inherent noise and\nerror terms. One of the emerging solutions to mitigate drift is to maneuver the\nrobot in a snake-like slithering motion to increase the inertial\nsignal-to-noise ratio, allowing the regression of the mobile robot position. In\nthis work, we propose MoRPI-PINN as a physics-informed neural network framework\nfor accurate inertial-based mobile robot navigation. By embedding physical laws\nand constraints into the training process, MoRPI-PINN is capable of providing\nan accurate and robust navigation solution. Using real-world experiments, we\nshow accuracy improvements of over 85% compared to other approaches. MoRPI-PINN\nis a lightweight approach that can be implemented even on edge devices and used\nin any typical mobile robot application.", "AI": {"tldr": "本文提出MoRPI-PINN，一个基于物理信息神经网络（PINN）的框架，通过结合机器人蛇形运动和物理约束，显著提升了在无卫星导航或摄像头环境下，仅依靠惯性传感器进行移动机器人导航的精度和鲁棒性。", "motivation": "在卫星导航或摄像头不可用的实际场景中，移动机器人需要精确导航。仅依靠惯性传感器会导致导航漂移。蛇形滑行运动可以提高惯性信号信噪比，但仍需更精确的解决方案来利用此信息并减轻漂移。", "method": "提出MoRPI-PINN，一个物理信息神经网络框架。该方法将物理定律和约束嵌入到神经网络的训练过程中，并利用机器人蛇形滑行运动来增强惯性信号，从而实现精确的惯性导航。", "result": "通过真实世界实验证明，MoRPI-PINN相比其他方法，导航精度提高了85%以上。该方法轻量化，可在边缘设备上实现，适用于各种典型的移动机器人应用。", "conclusion": "MoRPI-PINN为移动机器人提供了一种准确、鲁棒且轻量级的基于惯性传感器的导航解决方案，即使在GPS或摄像头不可用的挑战性环境中也能有效工作。"}}
{"id": "2507.18059", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.18059", "abs": "https://arxiv.org/abs/2507.18059", "authors": ["Yueheng Li", "Guangming Xie", "Zongqing Lu"], "title": "Multi-Agent Guided Policy Optimization", "comment": null, "summary": "Due to practical constraints such as partial observability and limited\ncommunication, Centralized Training with Decentralized Execution (CTDE) has\nbecome the dominant paradigm in cooperative Multi-Agent Reinforcement Learning\n(MARL). However, existing CTDE methods often underutilize centralized training\nor lack theoretical guarantees. We propose Multi-Agent Guided Policy\nOptimization (MAGPO), a novel framework that better leverages centralized\ntraining by integrating centralized guidance with decentralized execution.\nMAGPO uses an auto-regressive joint policy for scalable, coordinated\nexploration and explicitly aligns it with decentralized policies to ensure\ndeployability under partial observability. We provide theoretical guarantees of\nmonotonic policy improvement and empirically evaluate MAGPO on 43 tasks across\n6 diverse environments. Results show that MAGPO consistently outperforms strong\nCTDE baselines and matches or surpasses fully centralized approaches, offering\na principled and practical solution for decentralized multi-agent learning. Our\ncode and experimental data can be found in https://github.com/liyheng/MAGPO.", "AI": {"tldr": "MAGPO是一种新的多智能体强化学习框架，它通过整合集中式引导和分散式执行，解决了现有CTDE方法未能充分利用集中式训练或缺乏理论保证的问题，并在多个任务中表现优异。", "motivation": "在合作多智能体强化学习（MARL）中，尽管集中式训练分散式执行（CTDE）是主流范式，但现有CTDE方法往往未能充分利用集中式训练的优势，或缺乏理论保证。", "method": "本文提出了多智能体引导策略优化（MAGPO）框架。该框架通过整合集中式引导与分散式执行，更好地利用集中式训练。MAGPO采用自回归联合策略进行可扩展的、协调的探索，并将其与分散式策略明确对齐，以确保在部分可观察性下的可部署性。此外，该方法提供了单调策略改进的理论保证。", "result": "MAGPO在6个不同环境的43个任务上进行了实证评估。结果表明，MAGPO持续优于强大的CTDE基线方法，并且与完全集中式方法相当或超越它们。", "conclusion": "MAGPO为分散式多智能体学习提供了一个原则性且实用的解决方案，它在性能上优于现有方法，并具有理论保证。"}}
{"id": "2507.17974", "categories": ["cs.CL", "cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2507.17974", "abs": "https://arxiv.org/abs/2507.17974", "authors": ["Fitsum Gaim", "Jong C. Park"], "title": "Natural Language Processing for Tigrinya: Current State and Future Directions", "comment": null, "summary": "Despite being spoken by millions of people, Tigrinya remains severely\nunderrepresented in Natural Language Processing (NLP) research. This work\npresents a comprehensive survey of NLP research for Tigrinya, analyzing over 40\nstudies spanning more than a decade of work from 2011 to 2025. We\nsystematically review the current state of computational resources, models, and\napplications across ten distinct downstream tasks, including morphological\nprocessing, machine translation, speech recognition, and question-answering.\nOur analysis reveals a clear trajectory from foundational, rule-based systems\nto modern neural architectures, with progress consistently unlocked by resource\ncreation milestones. We identify key challenges rooted in Tigrinya's\nmorphological complexity and resource scarcity, while highlighting promising\nresearch directions, including morphology-aware modeling, cross-lingual\ntransfer, and community-centered resource development. This work serves as both\na comprehensive reference for researchers and a roadmap for advancing Tigrinya\nNLP. A curated metadata of the surveyed studies and resources is made publicly\navailable.\\footnote{Tigrinya NLP Anthology:\nhttps://github.com/fgaim/tigrinya-nlp-anthology.", "AI": {"tldr": "该论文全面综述了2011年至2025年期间提格雷尼亚语（Tigrinya）的自然语言处理（NLP）研究，分析了资源、模型和应用，并指出了挑战和未来方向。", "motivation": "尽管提格雷尼亚语有数百万使用者，但在NLP研究中严重不足。这项工作旨在全面分析现有研究，识别其发展轨迹、瓶颈和未来潜力。", "method": "通过系统回顾40多项研究，分析了提格雷尼亚语NLP在过去十多年间在计算资源、模型和应用方面的发展，涵盖了形态处理、机器翻译、语音识别和问答等十个下游任务。", "result": "研究发现提格雷尼亚语NLP研究从基于规则的系统发展到现代神经网络架构，进展与资源创建里程碑紧密相关。主要挑战在于其形态复杂性和资源稀缺性。有前景的研究方向包括形态感知建模、跨语言迁移和以社区为中心的资源开发。", "conclusion": "该工作为提格雷尼亚语NLP研究人员提供了全面的参考，并为未来的研究指明了方向。"}}
{"id": "2507.17957", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.17957", "abs": "https://arxiv.org/abs/2507.17957", "authors": ["Md. Al-Masrur Khan", "Durgakant Pushp", "Lantao Liu"], "title": "AFRDA: Attentive Feature Refinement for Domain Adaptive Semantic Segmentation", "comment": null, "summary": "In Unsupervised Domain Adaptive Semantic Segmentation (UDA-SS), a model is\ntrained on labeled source domain data (e.g., synthetic images) and adapted to\nan unlabeled target domain (e.g., real-world images) without access to target\nannotations. Existing UDA-SS methods often struggle to balance fine-grained\nlocal details with global contextual information, leading to segmentation\nerrors in complex regions. To address this, we introduce the Adaptive Feature\nRefinement (AFR) module, which enhances segmentation accuracy by refining\nhighresolution features using semantic priors from low-resolution logits. AFR\nalso integrates high-frequency components, which capture fine-grained\nstructures and provide crucial boundary information, improving object\ndelineation. Additionally, AFR adaptively balances local and global information\nthrough uncertaintydriven attention, reducing misclassifications. Its\nlightweight design allows seamless integration into HRDA-based UDA methods,\nleading to state-of-the-art segmentation performance. Our approach improves\nexisting UDA-SS methods by 1.05% mIoU on GTA V --> Cityscapes and 1.04% mIoU on\nSynthia-->Cityscapes. The implementation of our framework is available at:\nhttps://github.com/Masrur02/AFRDA", "AI": {"tldr": "提出自适应特征细化（AFR）模块，通过结合多尺度信息、高频分量和不确定性驱动注意力，解决无监督域适应语义分割（UDA-SS）中局部细节与全局上下文平衡问题，提升分割精度。", "motivation": "现有无监督域适应语义分割（UDA-SS）方法难以平衡细粒度局部细节与全局上下文信息，导致在复杂区域出现分割错误。", "method": "引入自适应特征细化（AFR）模块。该模块通过低分辨率logits的语义先验来细化高分辨率特征，以增强分割精度；整合高频分量以捕获细粒度结构和提供关键边界信息；并通过不确定性驱动的注意力机制自适应地平衡局部和全局信息。其轻量级设计使其能无缝集成到基于HRDA的UDA方法中。", "result": "在GTA V --> Cityscapes数据集上，将现有UDA-SS方法的mIoU提高了1.05%；在Synthia --> Cityscapes数据集上，mIoU提高了1.04%，达到了最先进的分割性能。", "conclusion": "AFR模块通过有效平衡局部细节和全局上下文信息，显著提升了无监督域适应语义分割的准确性，解决了现有方法的关键挑战，实现了最先进的性能。"}}
{"id": "2507.18150", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.18150", "abs": "https://arxiv.org/abs/2507.18150", "authors": ["Shiny Choudhury", "Michael Davidson", "George Tynan"], "title": "Unit Commitment Framework for Nuclear Reactors with Reactivity Decline", "comment": "11 pages, preliminary version, comments welcome", "summary": "Nuclear reactors are often modeled as inflexible, baseload generators with\nfixed downtimes and restrictive ramping limits. In practice, however, a\nreactor's operational flexibility is closely tied to it's fuel cycle stage and\nthe associated reactivity margin. A key physical constraint to power\nmaneuverability is xenon poisoning, caused by an increase in neutron absorbing\nxenon concentration following a power ramp down. This can delay or even prevent\nsubsequent power ramp up due to suppressed core reactivity. Additionally, if a\nreactor is shutdown during periods of low reactivity, restart times can vary\nsignificantly due to these xenon transients, leading to longer downtimes. This\nwork introduces a physics informed, metaheuristic modeling approach that embeds\nfuel cycle dynamics directly with a unit commitment (UC) framework. The\nframework tracks reactivity margin, dynamically activates xenon related\nconstraints, and endogenously implements refueling outages based on the core\nconditions. By capturing intra-cycle reactivity evolution and the conditional\nonset of xenon poisoning, the formulation allows for operation dependent\nnuclear dispatch that reflects both regulatory limits and physical behavior.\nWhen applied to a representative reactor fleet operating in distinct modes of\noperation -- ranging from baseload to part load -- the framework reveals that\nflexible operation can slow reactivity degradation and extend fuel cycles. The\nresults show that fuel cycle aware flexibility modeling is critical for\naccurate scheduling of nuclear reactors and offers a tractable pathway to\nintegrate nuclear power in energy system models.", "AI": {"tldr": "该研究引入了一种物理信息驱动的元启发式建模方法，将核反应堆的燃料循环动态（包括氙中毒和反应性裕度）嵌入到机组组合框架中，以实现更灵活和准确的核电调度。", "motivation": "核反应堆通常被建模为不灵活的基荷发电机组，具有固定停机时间和严格的爬坡限制。然而，实际上反应堆的运行灵活性与其燃料循环阶段和反应性裕度密切相关。氙中毒是功率机动性的一个关键物理限制，会导致后续功率爬升延迟或阻止，并延长停机时间。传统模型未能充分捕捉这些物理约束和运行依赖的灵活性。", "method": "该研究提出了一种物理信息驱动的元启发式建模方法，将燃料循环动态直接嵌入到机组组合（UC）框架中。该框架跟踪反应性裕度，动态激活与氙相关的约束，并根据堆芯条件内生地实现换料停机。通过捕获周期内反应性演变和氙中毒的条件性发生，该公式允许根据运行情况进行核电调度，反映监管限制和物理行为。", "result": "将该框架应用于代表性反应堆机组，在从基荷到部分负荷的不同运行模式下，结果表明灵活运行可以减缓反应性衰减并延长燃料循环。研究揭示，考虑燃料循环的灵活性建模对于核反应堆的准确调度至关重要，并为将核电整合到能源系统模型中提供了可行的途径。", "conclusion": "该研究提出的框架能够实现反映监管限制和物理行为的、依赖于运行模式的核电调度。通过捕捉反应堆的内部物理特性，特别是燃料循环和氙中毒效应，该模型能够更准确地规划核电运行，从而提高灵活性、延长燃料循环，并促进核能更好地融入未来能源系统。"}}
{"id": "2507.18133", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18133", "abs": "https://arxiv.org/abs/2507.18133", "authors": ["Juexin Zhang", "Ying Weng", "Ke Chen"], "title": "Deep Learning for Glioblastoma Morpho-pathological Features Identification: A BraTS-Pathology Challenge Solution", "comment": "Accepted by the International Brain Tumor Segmentation (BraTS)\n  challenge organized at MICCAI 2024 conference", "summary": "Glioblastoma, a highly aggressive brain tumor with diverse molecular and\npathological features, poses a diagnostic challenge due to its heterogeneity.\nAccurate diagnosis and assessment of this heterogeneity are essential for\nchoosing the right treatment and improving patient outcomes. Traditional\nmethods rely on identifying specific features in tissue samples, but deep\nlearning offers a promising approach for improved glioblastoma diagnosis. In\nthis paper, we present our approach to the BraTS-Path Challenge 2024. We\nleverage a pre-trained model and fine-tune it on the BraTS-Path training\ndataset. Our model demonstrates poor performance on the challenging BraTS-Path\nvalidation set, as rigorously assessed by the Synapse online platform. The\nmodel achieves an accuracy of 0.392229, a recall of 0.392229, and a F1-score of\n0.392229, indicating a consistent ability to correctly identify instances under\nthe target condition. Notably, our model exhibits perfect specificity of\n0.898704, showing an exceptional capacity to correctly classify negative cases.\nMoreover, a Matthews Correlation Coefficient (MCC) of 0.255267 is calculated,\nto signify a limited positive correlation between predicted and actual values\nand highlight our model's overall predictive power. Our solution also achieves\nthe second place during the testing phase.", "AI": {"tldr": "针对胶质母细胞瘤的诊断挑战，本文提出了一种基于预训练模型微调的深度学习方法，参与了BraTS-Path 2024挑战赛。尽管模型在验证集上表现出较低的绝对性能指标，但在测试阶段取得了第二名的成绩。", "motivation": "胶质母细胞瘤的高度异质性使其诊断面临挑战。准确诊断和评估其异质性对选择正确治疗和改善患者预后至关重要。深度学习为改进胶质母细胞瘤诊断提供了一种有前景的方法。", "method": "本文利用一个预训练的深度学习模型，并在BraTS-Path训练数据集上对其进行了微调。", "result": "模型在BraTS-Path验证集上表现出较低的性能，具体为准确率、召回率和F1分数均为0.392229。然而，模型展示了0.898704的优秀特异性，对阴性病例具有卓越的分类能力。马修斯相关系数（MCC）为0.255267，表明预测与实际值之间存在有限的正相关。最终，该解决方案在测试阶段获得了第二名。", "conclusion": "尽管模型在某些绝对性能指标上表现不佳，但其出色的特异性以及在BraTS-Path 2024挑战赛中取得的第二名成绩，表明了该方法在胶质母细胞瘤诊断领域的竞争力和潜在价值，可能反映了挑战的难度或该模型相对于其他参赛者的优势。"}}
{"id": "2507.18248", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18248", "abs": "https://arxiv.org/abs/2507.18248", "authors": ["Ines Frajtag", "Marko Švaco", "Filip Šuligoj"], "title": "Evaluation of facial landmark localization performance in a surgical setting", "comment": null, "summary": "The use of robotics, computer vision, and their applications is becoming\nincreasingly widespread in various fields, including medicine. Many face\ndetection algorithms have found applications in neurosurgery, ophthalmology,\nand plastic surgery. A common challenge in using these algorithms is variable\nlighting conditions and the flexibility of detection positions to identify and\nprecisely localize patients. The proposed experiment tests the MediaPipe\nalgorithm for detecting facial landmarks in a controlled setting, using a\nrobotic arm that automatically adjusts positions while the surgical light and\nthe phantom remain in a fixed position. The results of this study demonstrate\nthat the improved accuracy of facial landmark detection under surgical lighting\nsignificantly enhances the detection performance at larger yaw and pitch\nangles. The increase in standard deviation/dispersion occurs due to imprecise\ndetection of selected facial landmarks. This analysis allows for a discussion\non the potential integration of the MediaPipe algorithm into medical\nprocedures.", "AI": {"tldr": "该研究测试了MediaPipe算法在受控医疗环境下进行面部特征点检测的性能，发现其在手术照明下对大偏航角和俯仰角的人脸检测精度有显著提升，并讨论了其在医疗程序中的应用潜力。", "motivation": "机器人和计算机视觉在医学领域应用日益广泛，但现有的面部检测算法在可变光照条件和灵活检测位置下识别和精确定位患者面临挑战。本研究旨在解决这一问题，提升面部检测在手术环境下的鲁棒性。", "method": "实验在一个受控环境中进行，使用机械臂自动调整检测位置，同时手术灯和人体模型保持固定。研究人员测试了MediaPipe算法在这些条件下检测面部特征点的性能。", "result": "研究结果表明，在手术照明下，MediaPipe算法的面部特征点检测精度得到显著提高，尤其是在较大的偏航角和俯仰角下，检测性能有明显增强。然而，部分选定特征点的检测精度不高导致标准差/离散度增加。", "conclusion": "该分析为MediaPipe算法潜在地整合到医疗程序中提供了讨论基础，表明其在特定光照和姿态条件下具有良好的应用前景，但需进一步解决某些特征点检测的精度问题。"}}
{"id": "2507.18074", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18074", "abs": "https://arxiv.org/abs/2507.18074", "authors": ["Yixiu Liu", "Yang Nan", "Weixian Xu", "Xiangkun Hu", "Lyumanshan Ye", "Zhen Qin", "Pengfei Liu"], "title": "AlphaGo Moment for Model Architecture Discovery", "comment": null, "summary": "While AI systems demonstrate exponentially improving capabilities, the pace\nof AI research itself remains linearly bounded by human cognitive capacity,\ncreating an increasingly severe development bottleneck. We present ASI-Arch,\nthe first demonstration of Artificial Superintelligence for AI research\n(ASI4AI) in the critical domain of neural architecture discovery--a fully\nautonomous system that shatters this fundamental constraint by enabling AI to\nconduct its own architectural innovation. Moving beyond traditional Neural\nArchitecture Search (NAS), which is fundamentally limited to exploring\nhuman-defined spaces, we introduce a paradigm shift from automated optimization\nto automated innovation. ASI-Arch can conduct end-to-end scientific research in\nthe domain of architecture discovery, autonomously hypothesizing novel\narchitectural concepts, implementing them as executable code, training and\nempirically validating their performance through rigorous experimentation and\npast experience. ASI-Arch conducted 1,773 autonomous experiments over 20,000\nGPU hours, culminating in the discovery of 106 innovative, state-of-the-art\n(SOTA) linear attention architectures. Like AlphaGo's Move 37 that revealed\nunexpected strategic insights invisible to human players, our AI-discovered\narchitectures demonstrate emergent design principles that systematically\nsurpass human-designed baselines and illuminate previously unknown pathways for\narchitectural innovation. Crucially, we establish the first empirical scaling\nlaw for scientific discovery itself--demonstrating that architectural\nbreakthroughs can be scaled computationally, transforming research progress\nfrom a human-limited to a computation-scalable process. We provide\ncomprehensive analysis of the emergent design patterns and autonomous research\ncapabilities that enabled these breakthroughs, establishing a blueprint for\nself-accelerating AI systems.", "AI": {"tldr": "该研究展示了ASI-Arch，一个能自主进行神经网络架构创新的AI系统，旨在突破人类认知对AI研究的限制，实现AI驱动的AI研究加速。", "motivation": "当前AI研究的进展受限于人类认知能力，导致AI能力指数级提升与研究速度线性增长之间的瓶颈。研究旨在通过AI系统自身进行创新，打破这一限制。", "method": "引入ASI-Arch系统，超越传统神经架构搜索（NAS），实现从自动化优化到自动化创新的范式转变。该系统能自主提出新架构概念、将其实现为可执行代码、训练并进行经验验证，并通过严谨实验和过往经验评估性能。", "result": "ASI-Arch进行了1,773次自主实验，耗费20,000 GPU小时，发现了106种创新的、最先进的（SOTA）线性注意力架构。这些AI发现的架构展示了超越人类设计基线的涌现设计原则，并揭示了新的架构创新途径。研究还首次建立了科学发现的经验缩放定律，证明架构突破可以通过计算进行扩展。", "conclusion": "ASI-Arch为AI系统自我加速提供了蓝图，将研究进展从受限于人类的过程转变为可计算扩展的过程，开启了AI自主进行科学研究的新范式。"}}
{"id": "2507.18013", "categories": ["cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2507.18013", "abs": "https://arxiv.org/abs/2507.18013", "authors": ["Zihan Wang", "Xinzhang Liu", "Yitong Yao", "Chao Wang", "Yu Zhao", "Zhihao Yang", "Wenmin Deng", "Kaipeng Jia", "Jiaxin Peng", "Yuyao Huang", "Sishi Xiong", "Zhuo Jiang", "Kaidong Yu", "Xiaohui Hu", "Fubei Yao", "Ruiyu Fang", "Zhuoru Jiang", "Ruiting Song", "Qiyi Xie", "Rui Xue", "Xuewei He", "Yanlei Xue", "Zhu Yuan", "Zhaoxi Zhang", "Zilu Huang", "Shiquan Wang", "Xin Wang", "Hanming Wu", "Mingyuan Wang", "Xufeng Zhan", "Yuhan Sun", "Zhaohu Xing", "Yuhao Jiang", "Bingkai Yang", "Shuangyong Song", "Yongxiang Li", "Zhongjiang He", "Xuelong Li"], "title": "Technical Report of TeleChat2, TeleChat2.5 and T1", "comment": "32 pages, 5 figures", "summary": "We introduce the latest series of TeleChat models: \\textbf{TeleChat2},\n\\textbf{TeleChat2.5}, and \\textbf{T1}, offering a significant upgrade over\ntheir predecessor, TeleChat. Despite minimal changes to the model architecture,\nthe new series achieves substantial performance gains through enhanced training\nstrategies in both pre-training and post-training stages. The series begins\nwith \\textbf{TeleChat2}, which undergoes pretraining on 10 trillion\nhigh-quality and diverse tokens. This is followed by Supervised Fine-Tuning\n(SFT) and Direct Preference Optimization (DPO) to further enhance its\ncapabilities. \\textbf{TeleChat2.5} and \\textbf{T1} expand the pipeline by\nincorporating a continual pretraining phase with domain-specific datasets,\ncombined with reinforcement learning (RL) to improve performance in code\ngeneration and mathematical reasoning tasks. The \\textbf{T1} variant is\ndesigned for complex reasoning, supporting long Chain-of-Thought (CoT)\nreasoning and demonstrating substantial improvements in mathematics and coding.\nIn contrast, \\textbf{TeleChat2.5} prioritizes speed, delivering rapid\ninference. Both flagship models of \\textbf{T1} and \\textbf{TeleChat2.5} are\ndense Transformer-based architectures with 115B parameters, showcasing\nsignificant advancements in reasoning and general task performance compared to\nthe original TeleChat. Notably, \\textbf{T1-115B} outperform proprietary models\nsuch as OpenAI's o1-mini and GPT-4o. We publicly release \\textbf{TeleChat2},\n\\textbf{TeleChat2.5} and \\textbf{T1}, including post-trained versions with 35B\nand 115B parameters, to empower developers and researchers with\nstate-of-the-art language models tailored for diverse applications.", "AI": {"tldr": "本文介绍了TeleChat模型的最新系列：TeleChat2、TeleChat2.5和T1。这些新模型通过改进的预训练和后训练策略，显著提升了性能，尤其在复杂推理、代码生成和数学推理方面表现出色，并公开了不同参数版本。", "motivation": "为了显著提升TeleChat模型的性能，尤其是在推理、代码和数学任务上的能力，超越其前身，并提供更先进的开源语言模型。", "method": "模型架构变化极小，主要通过以下策略实现性能提升：1. TeleChat2：在10万亿高质量多样化tokens上进行预训练，随后进行监督微调（SFT）和直接偏好优化（DPO）。2. TeleChat2.5和T1：在TeleChat2基础上，加入领域特定数据集的持续预训练阶段，并结合强化学习（RL）以提升代码生成和数学推理性能。T1专注于复杂推理（支持长CoT），TeleChat2.5优先考虑推理速度。旗舰模型T1和TeleChat2.5均为115B参数的密集Transformer架构。", "result": "新系列模型实现了显著的性能提升。T1在数学和编码等复杂推理任务中表现出实质性进步，T1-115B甚至超越了OpenAI的o1-mini和GPT-4o等专有模型。TeleChat2.5则优先提供快速推理。所有新模型在推理和通用任务性能上均显著优于原始TeleChat。项目公开了TeleChat2、TeleChat2.5和T1，包括35B和115B参数的后训练版本。", "conclusion": "TeleChat2、TeleChat2.5和T1系列模型通过优化的训练策略，在几乎不改变架构的情况下，取得了显著的性能飞跃，特别是在复杂推理、代码和数学方面表现卓越，甚至超越了某些领先的专有模型。这些模型的公开发布将赋能开发者和研究人员，为多样化应用提供最先进的语言模型。"}}
{"id": "2507.17959", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.17959", "abs": "https://arxiv.org/abs/2507.17959", "authors": ["Ali Abedi", "Sadaf Safa", "Tracey J. F. Colella", "Shehroz S. Khan"], "title": "OPEN: A Benchmark Dataset and Baseline for Older Adult Patient Engagement Recognition in Virtual Rehabilitation Learning Environments", "comment": "14 pages, 3 figures, 7 tables", "summary": "Engagement in virtual learning is essential for participant satisfaction,\nperformance, and adherence, particularly in online education and virtual\nrehabilitation, where interactive communication plays a key role. Yet,\naccurately measuring engagement in virtual group settings remains a challenge.\nThere is increasing interest in using artificial intelligence (AI) for\nlarge-scale, real-world, automated engagement recognition. While engagement has\nbeen widely studied in younger academic populations, research and datasets\nfocused on older adults in virtual and telehealth learning settings remain\nlimited. Existing methods often neglect contextual relevance and the\nlongitudinal nature of engagement across sessions. This paper introduces OPEN\n(Older adult Patient ENgagement), a novel dataset supporting AI-driven\nengagement recognition. It was collected from eleven older adults participating\nin weekly virtual group learning sessions over six weeks as part of cardiac\nrehabilitation, producing over 35 hours of data, making it the largest dataset\nof its kind. To protect privacy, raw video is withheld; instead, the released\ndata include facial, hand, and body joint landmarks, along with affective and\nbehavioral features extracted from video. Annotations include binary engagement\nstates, affective and behavioral labels, and context-type indicators, such as\nwhether the instructor addressed the group or an individual. The dataset offers\nversions with 5-, 10-, 30-second, and variable-length samples. To demonstrate\nutility, multiple machine learning and deep learning models were trained,\nachieving engagement recognition accuracy of up to 81 percent. OPEN provides a\nscalable foundation for personalized engagement modeling in aging populations\nand contributes to broader engagement recognition research.", "AI": {"tldr": "该论文介绍了一个名为OPEN的新数据集，用于驱动AI识别老年人在虚拟学习环境中的参与度，并展示了高达81%的识别准确率。", "motivation": "在虚拟学习和康复中，参与度对于用户满意度、表现和依从性至关重要，尤其是在在线教育和虚拟康复中。然而，准确测量虚拟群组环境中的参与度仍是一个挑战，特别是针对老年人群体，相关研究和数据集非常有限。现有方法常忽略上下文相关性和参与度的长期性。", "method": "引入了OPEN（老年患者参与度）数据集，该数据集从11名老年人每周参与为期六周的虚拟小组学习（心脏康复的一部分）中收集，共计超过35小时数据。为保护隐私，不提供原始视频，而是发布从视频中提取的面部、手部和身体关节地标、情感和行为特征。标注包括二元参与状态、情感和行为标签以及上下文类型指示器。数据集提供5秒、10秒、30秒和可变长度的样本版本。通过训练多种机器学习和深度学习模型来验证其效用。", "result": "OPEN是同类数据集中最大的数据集，包含超过35小时的数据。通过训练机器学习和深度学习模型，实现了高达81%的参与度识别准确率。", "conclusion": "OPEN数据集为老龄人群的个性化参与度建模提供了可扩展的基础，并对更广泛的参与度识别研究做出了贡献。"}}
{"id": "2507.18158", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.18158", "abs": "https://arxiv.org/abs/2507.18158", "authors": ["Zhenyi Yuan", "Jie Feng", "Yuanyuan Shi", "Jorge Cortés"], "title": "Stability Constrained Voltage Control in Distribution Grids with Arbitrary Communication Infrastructure", "comment": null, "summary": "We consider the problem of designing learning-based reactive power\ncontrollers that perform voltage regulation in distribution grids while\nensuring closed-loop system stability. In contrast to existing methods, where\nthe provably stable controllers are restricted to be decentralized, we propose\na unified design framework that enables the controllers to take advantage of an\narbitrary communication infrastructure on top of the physical power network.\nThis allows the controllers to incorporate information beyond their local bus,\ncovering existing methods as a special case and leading to less conservative\nconstraints on the controller design. We then provide a design procedure to\nconstruct input convex neural network (ICNN) based controllers that satisfy the\nidentified stability constraints by design under arbitrary communication\nscenarios, and train these controllers using supervised learning. Simulation\nresults on the the University of California, San Diego (UCSD) microgrid testbed\nillustrate the effectiveness of the framework and highlight the role of\ncommunication in improving control performance.", "AI": {"tldr": "本文提出了一个统一的设计框架，用于在配电网中进行基于学习的无功功率控制器设计，以实现电压调节并确保系统稳定性，允许控制器利用任意通信基础设施。", "motivation": "现有可证明稳定的控制器仅限于分散式，无法利用通信获取非本地信息，导致设计保守。本文旨在打破这一限制，允许控制器利用通信以实现更优性能和更宽松的设计约束。", "method": "提出了一个统一的设计框架，允许控制器利用任意通信基础设施。设计了基于输入凸神经网络（ICNN）的控制器，通过设计满足稳定性约束。控制器采用监督学习进行训练。", "result": "在加州大学圣地亚哥分校（UCSD）微电网测试平台上的仿真结果表明，该框架有效，并突出了通信在提高控制性能方面的作用。", "conclusion": "该框架提供了一种有效且不那么保守的基于学习的无功功率控制器设计方法，通过利用通信，显著提升了配电网的电压调节性能和系统稳定性。"}}
{"id": "2507.18288", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18288", "abs": "https://arxiv.org/abs/2507.18288", "authors": ["Xuebo Jin", "Longfei Gao", "Anshuo Tong", "Zhengyang Chen", "Jianlei Kong", "Ning Sun", "Huijun Ma", "Qiang Wang", "Yuting Bai", "Tingli Su"], "title": "TCM-Tongue: A Standardized Tongue Image Dataset with Pathological Annotations for AI-Assisted TCM Diagnosis", "comment": "16 pages, 11 figures, 2 Tables", "summary": "Traditional Chinese medicine (TCM) tongue diagnosis, while clinically\nvaluable, faces standardization challenges due to subjective interpretation and\ninconsistent imaging protocols, compounded by the lack of large-scale,\nannotated datasets for AI development. To address this gap, we present the\nfirst specialized dataset for AI-driven TCM tongue diagnosis, comprising 6,719\nhigh-quality images captured under standardized conditions and annotated with\n20 pathological symptom categories (averaging 2.54 clinically validated labels\nper image, all verified by licensed TCM practitioners). The dataset supports\nmultiple annotation formats (COCO, TXT, XML) for broad usability and has been\nbenchmarked using nine deep learning models (YOLOv5/v7/v8 variants, SSD, and\nMobileNetV2) to demonstrate its utility for AI development. This resource\nprovides a critical foundation for advancing reliable computational tools in\nTCM, bridging the data shortage that has hindered progress in the field, and\nfacilitating the integration of AI into both research and clinical practice\nthrough standardized, high-quality diagnostic data.", "AI": {"tldr": "该研究构建了首个大规模、标准化且经过专业标注的中医舌诊图像数据集，以推动人工智能在中医舌诊领域的发展。", "motivation": "传统中医舌诊面临主观性强、图像采集不一致以及缺乏大规模标注数据集的挑战，这阻碍了人工智能在该领域的应用和标准化。", "method": "收集了6,719张在标准化条件下拍摄的高质量舌象图片，并由执业中医师标注了20种病理症状类别（平均每张图片2.54个标签）。数据集支持COCO、TXT、XML等多种标注格式，并使用YOLOv5/v7/v8、SSD、MobileNetV2等九种深度学习模型进行了基准测试。", "result": "成功构建了首个专用于AI驱动中医舌诊的大规模、标准化、高质量标注数据集，并通过多种深度学习模型的基准测试验证了其效用。", "conclusion": "该数据集为中医计算工具的可靠发展奠定了关键基础，弥补了该领域的数据短缺，有助于通过标准化、高质量的诊断数据将人工智能整合到中医研究和临床实践中。"}}
{"id": "2507.18262", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18262", "abs": "https://arxiv.org/abs/2507.18262", "authors": ["Chenyu Su", "Weiwei Shang", "Chen Qian", "Fei Zhang", "Shuang Cong"], "title": "ReSem3D: Refinable 3D Spatial Constraints via Fine-Grained Semantic Grounding for Generalizable Robotic Manipulation", "comment": "12 pages,9 figures", "summary": "Semantics-driven 3D spatial constraints align highlevel semantic\nrepresentations with low-level action spaces, facilitating the unification of\ntask understanding and execution in robotic manipulation. The synergistic\nreasoning of Multimodal Large Language Models (MLLMs) and Vision Foundation\nModels (VFMs) enables cross-modal 3D spatial constraint construction.\nNevertheless, existing methods have three key limitations: (1) coarse semantic\ngranularity in constraint modeling, (2) lack of real-time closed-loop planning,\n(3) compromised robustness in semantically diverse environments. To address\nthese challenges, we propose ReSem3D, a unified manipulation framework for\nsemantically diverse environments, leveraging the synergy between VFMs and\nMLLMs to achieve fine-grained visual grounding and dynamically constructs\nhierarchical 3D spatial constraints for real-time manipulation. Specifically,\nthe framework is driven by hierarchical recursive reasoning in MLLMs, which\ninteract with VFMs to automatically construct 3D spatial constraints from\nnatural language instructions and RGB-D observations in two stages: part-level\nextraction and region-level refinement. Subsequently, these constraints are\nencoded as real-time optimization objectives in joint space, enabling reactive\nbehavior to dynamic disturbances. Extensive simulation and real-world\nexperiments are conducted in semantically rich household and sparse chemical\nlab environments. The results demonstrate that ReSem3D performs diverse\nmanipulation tasks under zero-shot conditions, exhibiting strong adaptability\nand generalization. Code and videos at https://resem3d.github.io.", "AI": {"tldr": "ReSem3D是一个统一的机器人操作框架，利用多模态大语言模型（MLLMs）和视觉基础模型（VFMs）的协同作用，实现细粒度视觉定位和动态构建分层3D空间约束，以在语义多样化的环境中进行实时操作。", "motivation": "现有方法在机器人操作中存在三个主要限制：1) 约束建模的语义粒度粗糙；2) 缺乏实时闭环规划；3) 在语义多样化环境中鲁棒性受损。", "method": "ReSem3D框架通过MLLMs的分层递归推理驱动，MLLMs与VFMs交互，从自然语言指令和RGB-D观测中自动构建3D空间约束。该构建分为两阶段：部件级提取和区域级细化。随后，这些约束被编码为关节空间中的实时优化目标，以实现对动态干扰的反应性行为。", "result": "在语义丰富的家庭和稀疏的化学实验室环境中进行了广泛的模拟和真实世界实验。结果表明，ReSem3D在零样本条件下执行了多样化的操作任务，展示了强大的适应性和泛化能力。", "conclusion": "ReSem3D成功地解决了现有方法在语义粒度、实时规划和环境鲁棒性方面的挑战，为在多样化环境中进行机器人操作提供了一个统一且高效的解决方案。"}}
{"id": "2507.18115", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.ET", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18115", "abs": "https://arxiv.org/abs/2507.18115", "authors": ["Soorya Ram Shimgekar", "Shayan Vassef", "Abhay Goyal", "Navin Kumar", "Koustuv Saha"], "title": "Agentic AI framework for End-to-End Medical Data Inference", "comment": "10 pages, 5 figures, 2 tables, BIBM conference", "summary": "Building and deploying machine learning solutions in healthcare remains\nexpensive and labor-intensive due to fragmented preprocessing workflows, model\ncompatibility issues, and stringent data privacy constraints. In this work, we\nintroduce an Agentic AI framework that automates the entire clinical data\npipeline, from ingestion to inference, through a system of modular,\ntask-specific agents. These agents handle both structured and unstructured\ndata, enabling automatic feature selection, model selection, and preprocessing\nrecommendation without manual intervention. We evaluate the system on publicly\navailable datasets from geriatrics, palliative care, and colonoscopy imaging.\nFor example, in the case of structured data (anxiety data) and unstructured\ndata (colonoscopy polyps data), the pipeline begins with file-type detection by\nthe Ingestion Identifier Agent, followed by the Data Anonymizer Agent ensuring\nprivacy compliance, where we first identify the data type and then anonymize\nit. The Feature Extraction Agent identifies features using an embedding-based\napproach for tabular data, extracting all column names, and a multi-stage\nMedGemma-based approach for image data, which infers modality and disease name.\nThese features guide the Model-Data Feature Matcher Agent in selecting the\nbest-fit model from a curated repository. The Preprocessing Recommender Agent\nand Preprocessing Implementor Agent then apply tailored preprocessing based on\ndata type and model requirements. Finally, the ``Model Inference Agent\" runs\nthe selected model on the uploaded data and generates interpretable outputs\nusing tools like SHAP, LIME, and DETR attention maps. By automating these\nhigh-friction stages of the ML lifecycle, the proposed framework reduces the\nneed for repeated expert intervention, offering a scalable, cost-efficient\npathway for operationalizing AI in clinical environments.", "AI": {"tldr": "该研究提出了一个Agentic AI框架，通过模块化的任务特定智能体自动化医疗健康领域的机器学习全流程，从数据摄取到推理，以解决部署成本高、流程碎片化和数据隐私问题。", "motivation": "在医疗健康领域构建和部署机器学习解决方案成本高昂且劳动密集，主要原因在于预处理工作流碎片化、模型兼容性问题以及严格的数据隐私限制。", "method": "引入了一个Agentic AI框架，由一系列模块化、任务特定的智能体组成，自动化整个临床数据管道。这些智能体处理结构化和非结构化数据，实现自动特征选择、模型选择和预处理推荐。具体智能体包括：摄取识别智能体、数据匿名化智能体、特征提取智能体（基于嵌入的表格数据，基于MedGemma的多阶段图像数据）、模型-数据特征匹配智能体、预处理推荐智能体、预处理实现智能体和模型推理智能体（使用SHAP、LIME、DETR等工具生成可解释输出）。", "result": "该系统在老年病学、姑息治疗和结肠镜成像的公开数据集上进行了评估。例如，在结构化数据（焦虑数据）和非结构化数据（结肠镜息肉数据）案例中，管道展示了从文件类型检测、数据匿名化、特征提取、模型选择到预处理和最终推理的自动化流程。", "conclusion": "所提出的框架通过自动化机器学习生命周期中摩擦力大的阶段，减少了重复的专家干预需求，为在临床环境中操作化人工智能提供了一条可扩展、经济高效的途径。"}}
{"id": "2507.18028", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18028", "abs": "https://arxiv.org/abs/2507.18028", "authors": ["Weizhi Fei", "Hao Shi", "Jing Xu", "Jingchen Peng", "Jiazheng Li", "Jingzhao Zhang", "Bo Bai", "Wei Han", "Zhenyuan Chen", "Xueyan Niu"], "title": "NeuralDB: Scaling Knowledge Editing in LLMs to 100,000 Facts with Neural KV Database", "comment": null, "summary": "Efficiently editing knowledge stored in large language models (LLMs) enables\nmodel updates without large-scale training. One possible solution is\nLocate-and-Edit (L\\&E), allowing simultaneous modifications of a massive number\nof facts. However, such editing may compromise the general abilities of LLMs\nand even result in forgetting edited facts when scaling up to thousands of\nedits. In this paper, we model existing linear L\\&E methods as querying a\nKey-Value (KV) database. From this perspective, we then propose NeuralDB, an\nediting framework that explicitly represents the edited facts as a neural KV\ndatabase equipped with a non-linear gated retrieval module, % In particular,\nour gated module only operates when inference involves the edited facts,\neffectively preserving the general abilities of LLMs. Comprehensive experiments\ninvolving the editing of 10,000 facts were conducted on the ZsRE and\nCounterFacts datasets, using GPT2-XL, GPT-J (6B) and Llama-3 (8B). The results\ndemonstrate that NeuralDB not only excels in editing efficacy, generalization,\nspecificity, fluency, and consistency, but also preserves overall performance\nacross six representative text understanding and generation tasks. Further\nexperiments indicate that NeuralDB maintains its effectiveness even when scaled\nto 100,000 facts (\\textbf{50x} more than in prior work).", "AI": {"tldr": "本文提出NeuralDB，一个基于神经网络键值数据库的LLM知识编辑框架，它通过门控检索模块在编辑大量事实（高达10万个）的同时，有效保持了模型的通用能力。", "motivation": "大规模语言模型（LLMs）的知识编辑允许在不进行大规模训练的情况下更新模型。然而，现有的Locate-and-Edit (L&E) 方法在处理大量事实时，可能会损害LLMs的通用能力，甚至导致已编辑事实的遗忘。", "method": "将现有线性L&E方法建模为对键值(KV)数据库的查询。在此基础上，提出NeuralDB框架，将编辑后的事实明确表示为一个配备非线性门控检索模块的神经网络KV数据库。该门控模块仅在推理涉及已编辑事实时才激活，从而有效保留了LLMs的通用能力。", "result": "在GPT2-XL、GPT-J和Llama-3模型上，使用ZsRE和CounterFacts数据集对10,000个事实进行编辑的实验表明，NeuralDB在编辑效率、泛化性、特异性、流畅性和一致性方面表现出色，并能保持在六个代表性文本理解和生成任务上的整体性能。进一步实验表明，NeuralDB在扩展到100,000个事实（比现有工作多50倍）时仍能保持其有效性。", "conclusion": "NeuralDB是一个高效且可扩展的LLM知识编辑框架，它通过创新的神经网络键值数据库和门控检索机制，成功解决了现有方法在保持模型通用能力和处理大规模编辑方面的挑战。"}}
{"id": "2507.17987", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.17987", "abs": "https://arxiv.org/abs/2507.17987", "authors": ["Arsen Yermukan", "Pedro Machado", "Feliciano Domingos", "Isibor Kennedy Ihianle", "Jordan J. Bird", "Stefano S. K. Kaburu", "Samantha J. Ward"], "title": "Bearded Dragon Activity Recognition Pipeline: An AI-Based Approach to Behavioural Monitoring", "comment": null, "summary": "Traditional monitoring of bearded dragon (Pogona Viticeps) behaviour is\ntime-consuming and prone to errors. This project introduces an automated system\nfor real-time video analysis, using You Only Look Once (YOLO) object detection\nmodels to identify two key behaviours: basking and hunting. We trained five\nYOLO variants (v5, v7, v8, v11, v12) on a custom, publicly available dataset of\n1200 images, encompassing bearded dragons (600), heating lamps (500), and\ncrickets (100). YOLOv8s was selected as the optimal model due to its superior\nbalance of accuracy (mAP@0.5:0.95 = 0.855) and speed. The system processes\nvideo footage by extracting per-frame object coordinates, applying temporal\ninterpolation for continuity, and using rule-based logic to classify specific\nbehaviours. Basking detection proved reliable. However, hunting detection was\nless accurate, primarily due to weak cricket detection (mAP@0.5 = 0.392).\nFuture improvements will focus on enhancing cricket detection through expanded\ndatasets or specialised small-object detectors. This automated system offers a\nscalable solution for monitoring reptile behaviour in controlled environments,\nsignificantly improving research efficiency and data quality.", "AI": {"tldr": "该项目开发了一个基于YOLO模型（YOLOv8s最佳）的自动化系统，用于实时视频分析胡须龙的晒太阳和捕食行为，显著提高了监控效率，但蟋蟀检测准确性有待提高。", "motivation": "传统的胡须龙行为监测耗时且易出错，需要一种自动化、高效的解决方案。", "method": "项目采用You Only Look Once (YOLO) 系列目标检测模型（v5, v7, v8, v11, v12）进行实时视频分析。模型在一个包含胡须龙、加热灯和蟋蟀的自定义数据集（1200张图像）上进行训练。通过提取每帧目标坐标、应用时间插值和基于规则的逻辑来分类行为。最终选择YOLOv8s作为最佳模型。", "result": "YOLOv8s被选为最佳模型，其准确性（mAP@0.5:0.95 = 0.855）和速度达到优异平衡。晒太阳行为的检测结果可靠。然而，捕食行为的检测准确性较低，主要原因是蟋蟀检测效果不佳（mAP@0.5 = 0.392）。", "conclusion": "该自动化系统为受控环境下爬行动物行为监测提供了一个可扩展的解决方案，显著提高了研究效率和数据质量。未来的改进将集中于通过扩充数据集或使用专门的小目标检测器来增强蟋蟀检测能力。"}}
{"id": "2507.18204", "categories": ["eess.SY", "cs.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.18204", "abs": "https://arxiv.org/abs/2507.18204", "authors": ["Hamza Mettali", "Rousset François", "Eric Bideaux", "Clausse Marc"], "title": "Optimal Integration Of Heat-Pump And Solar Thermal Energy In The Pre-heating Loop Of Wood And Gas Boiler Based District Heating System", "comment": null, "summary": "The integration of renewable sources is essential for decarbonizing heat\nproduction in district energy networks. Beyond biomass-based solutions, solar\nthermal energy, with or without heat pumps, presents a significant opportunity.\nHowever, system performance is highly dependent on outdoor and setpoint\ntemperatures. This study aims to optimize system design using a multi-criteria\napproach that considers techno-economic and environmental (CO2) factors. A\nMixed-Integer Linear Programming (MILP) model is developed, incorporating\ntemperature discretization for problem linearization and capturing key dynamic\ncharacteristics of heat generators. The model improves convergence, reducing a\n19% MIP gap in 26 hours to 10% in 12 hours by dissipating 6% excess solar heat.\nA multi-scenario analysis under two carbon taxation levels and different CO2\nemission cases revealed solar integration up to 11,932 m${}^2$ but increased\ngas reliance (50%) and TES losses (49%). Wood boiler inclusion reduced solar\ndependency, covering 45% of heat, lowered LCOH, but limited renewable\npenetration. Higher carbon taxes boosted solar adoption but faced storage\ninefficiencies, while biomass enhanced cost efficiency and system stability.", "AI": {"tldr": "本研究通过多目标MILP模型优化区域能源网络中的太阳能热系统设计，考虑技术经济和环境因素。结果显示模型收敛性改善，太阳能集成受储热效率和天然气依赖限制；生物质锅炉可降低太阳能依赖并提高成本效益，但限制可再生能源渗透率；高碳税促进太阳能应用，但储能效率是挑战。", "motivation": "区域能源网络的热生产脱碳需要整合可再生能源，其中太阳能热能具有显著潜力。然而，系统性能高度依赖外部和设定温度。本研究旨在通过考虑技术经济和环境（CO2）因素的多准则方法来优化系统设计。", "method": "开发了一个混合整数线性规划（MILP）模型，该模型通过温度离散化实现问题线性化，并捕获热发生器的关键动态特性。通过多场景分析，评估了两种碳税水平和不同CO2排放情景下的系统性能。", "result": "模型收敛性得到改善，将19%的MIP差距在26小时内缩短到12小时内的10%，通过消散6%的过剩太阳热量。研究显示太阳能集成面积可达11,932平方米，但导致天然气依赖增加（50%）和储热损耗增加（49%）。纳入木材锅炉可降低太阳能依赖（覆盖45%热量），降低平准化热成本（LCOH），但限制了可再生能源渗透率。", "conclusion": "提高碳税能促进太阳能的采用，但面临储能效率低下的挑战。生物质能可以提高成本效益和系统稳定性，但在可再生能源渗透率方面存在权衡。太阳能与生物质能的整合需权衡系统性能、成本和环境效益。"}}
{"id": "2507.18362", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18362", "abs": "https://arxiv.org/abs/2507.18362", "authors": ["Yilong Hu", "Shijie Chang", "Lihe Zhang", "Feng Tian", "Weibing Sun", "Huchuan Lu"], "title": "UniSegDiff: Boosting Unified Lesion Segmentation via a Staged Diffusion Model", "comment": "MICCAI2025", "summary": "The Diffusion Probabilistic Model (DPM) has demonstrated remarkable\nperformance across a variety of generative tasks. The inherent randomness in\ndiffusion models helps address issues such as blurring at the edges of medical\nimages and labels, positioning Diffusion Probabilistic Models (DPMs) as a\npromising approach for lesion segmentation. However, we find that the current\ntraining and inference strategies of diffusion models result in an uneven\ndistribution of attention across different timesteps, leading to longer\ntraining times and suboptimal solutions. To this end, we propose UniSegDiff, a\nnovel diffusion model framework designed to address lesion segmentation in a\nunified manner across multiple modalities and organs. This framework introduces\na staged training and inference approach, dynamically adjusting the prediction\ntargets at different stages, forcing the model to maintain high attention\nacross all timesteps, and achieves unified lesion segmentation through\npre-training the feature extraction network for segmentation. We evaluate\nperformance on six different organs across various imaging modalities.\nComprehensive experimental results demonstrate that UniSegDiff significantly\noutperforms previous state-of-the-art (SOTA) approaches. The code is available\nat https://github.com/HUYILONG-Z/UniSegDiff.", "AI": {"tldr": "本文提出UniSegDiff，一个针对病灶分割的新型扩散模型框架，通过分阶段训练和推理策略解决现有DPM在不同时间步长注意力不均的问题，实现多模态多器官统一病灶分割，并显著优于现有SOTA方法。", "motivation": "扩散概率模型（DPM）在生成任务中表现出色，其固有的随机性有助于解决医学图像边缘模糊问题，使其成为病灶分割的有前景方法。然而，现有DPM的训练和推理策略导致不同时间步长的注意力分布不均，从而延长训练时间并产生次优解。", "method": "本文提出UniSegDiff框架，引入分阶段的训练和推理方法，动态调整不同阶段的预测目标，强制模型在所有时间步长保持高注意力。此外，通过预训练分割的特征提取网络，实现统一的病灶分割。", "result": "在六种不同器官和多种成像模态上的综合实验结果表明，UniSegDiff显著优于以前的最先进（SOTA）方法。", "conclusion": "UniSegDiff通过创新的分阶段训练和推理策略，有效解决了DPM在病灶分割中注意力分布不均的问题，实现了多模态多器官的统一病灶分割，并取得了超越现有SOTA的卓越性能。"}}
{"id": "2507.18276", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18276", "abs": "https://arxiv.org/abs/2507.18276", "authors": ["Xiaojie Zhang", "Yuanfei Wang", "Ruihai Wu", "Kunqi Xu", "Yu Li", "Liuyu Xiang", "Hao Dong", "Zhaofeng He"], "title": "Adaptive Articulated Object Manipulation On The Fly with Foundation Model Reasoning and Part Grounding", "comment": "ICCV 2025", "summary": "Articulated objects pose diverse manipulation challenges for robots. Since\ntheir internal structures are not directly observable, robots must adaptively\nexplore and refine actions to generate successful manipulation trajectories.\nWhile existing works have attempted cross-category generalization in adaptive\narticulated object manipulation, two major challenges persist: (1) the\ngeometric diversity of real-world articulated objects complicates visual\nperception and understanding, and (2) variations in object functions and\nmechanisms hinder the development of a unified adaptive manipulation strategy.\nTo address these challenges, we propose AdaRPG, a novel framework that\nleverages foundation models to extract object parts, which exhibit greater\nlocal geometric similarity than entire objects, thereby enhancing visual\naffordance generalization for functional primitive skills. To support this, we\nconstruct a part-level affordance annotation dataset to train the affordance\nmodel. Additionally, AdaRPG utilizes the common knowledge embedded in\nfoundation models to reason about complex mechanisms and generate high-level\ncontrol codes that invoke primitive skill functions based on part affordance\ninference. Simulation and real-world experiments demonstrate AdaRPG's strong\ngeneralization ability across novel articulated object categories.", "AI": {"tldr": "AdaRPG是一个利用基础模型进行关节物体操作的新框架，通过提取物体部件并结合部分级功能推断和高层控制代码，实现了对新型关节物体的强大泛化能力。", "motivation": "现有方法在自适应关节物体操作中面临两大挑战：一是真实世界关节物体的几何多样性导致视觉感知和理解困难；二是物体功能和机制的差异阻碍了统一自适应操作策略的开发。", "method": "AdaRPG框架利用基础模型提取物体部件（其局部几何相似性高于整体物体），以增强功能性基本技能的视觉可供性泛化。为此，构建了部件级可供性标注数据集来训练可供性模型。此外，AdaRPG利用基础模型中的常识来推理复杂机制，并基于部件可供性推断生成调用基本技能函数的高层控制代码。", "result": "仿真和真实世界实验证明了AdaRPG在新型关节物体类别上的强大泛化能力。", "conclusion": "AdaRPG通过利用基础模型提取部件并结合部分级可供性推断和高层控制代码，有效解决了关节物体操作中的几何多样性和机制差异挑战，实现了对未知关节物体的泛化操作。"}}
{"id": "2507.18123", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.18123", "abs": "https://arxiv.org/abs/2507.18123", "authors": ["Sedigh Khademi", "Christopher Palmer", "Muhammad Javed", "Hazel Clothier", "Jim Buttery", "Gerardo Luis Dimaguila", "Jim Black"], "title": "Actively evaluating and learning the distinctions that matter: Vaccine safety signal detection from emergency triage notes", "comment": "14 pages", "summary": "The rapid development of COVID-19 vaccines has showcased the global\ncommunitys ability to combat infectious diseases. However, the need for\npost-licensure surveillance systems has grown due to the limited window for\nsafety data collection in clinical trials and early widespread implementation.\nThis study aims to employ Natural Language Processing techniques and Active\nLearning to rapidly develop a classifier that detects potential vaccine safety\nissues from emergency department notes. ED triage notes, containing expert,\nsuccinct vital patient information at the point of entry to health systems, can\nsignificantly contribute to timely vaccine safety signal surveillance. While\nkeyword-based classification can be effective, it may yield false positives and\ndemand extensive keyword modifications. This is exacerbated by the infrequency\nof vaccination-related ED presentations and their similarity to other reasons\nfor ED visits. NLP offers a more accurate and efficient alternative, albeit\nrequiring annotated data, which is often scarce in the medical field. Active\nlearning optimizes the annotation process and the quality of annotated data,\nwhich can result in faster model implementation and improved model performance.\nThis work combines active learning, data augmentation, and active learning and\nevaluation techniques to create a classifier that is used to enhance vaccine\nsafety surveillance from ED triage notes.", "AI": {"tldr": "本研究旨在利用自然语言处理（NLP）和主动学习（Active Learning）技术，从急诊科（ED）分诊笔记中快速开发一个分类器，以检测潜在的疫苗安全问题，从而加强疫苗上市后监测。", "motivation": "COVID-19疫苗的快速开发凸显了临床试验中安全数据收集窗口有限，以及早期广泛接种后对上市后监测系统的需求。现有关键词分类方法可能产生误报，且难以应对疫苗相关急诊就诊频率低和症状非特异性等挑战。医疗领域标注数据稀缺，限制了NLP的应用。", "method": "本研究结合自然语言处理（NLP）技术、主动学习（Active Learning）以及数据增强（data augmentation）和主动学习评估技术，从急诊科分诊笔记中训练一个分类器。", "result": "通过结合NLP、主动学习和数据增强，本研究旨在快速开发一个准确高效的分类器，用于增强从急诊科分诊笔记中进行的疫苗安全信号监测。", "conclusion": "该方法能够优化标注过程和数据质量，从而实现模型的更快部署和性能提升，有效提升疫苗上市后安全监测的及时性和准确性。"}}
{"id": "2507.18043", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18043", "abs": "https://arxiv.org/abs/2507.18043", "authors": ["Duy Nguyen", "Archiki Prasad", "Elias Stengel-Eskin", "Mohit Bansal"], "title": "GrAInS: Gradient-based Attribution for Inference-Time Steering of LLMs and VLMs", "comment": "21 pages. Code: https://github.com/duykhuongnguyen/GrAInS", "summary": "Inference-time steering methods offer a lightweight alternative to\nfine-tuning large language models (LLMs) and vision-language models (VLMs) by\nmodifying internal activations at test time without updating model weights.\nHowever, most existing approaches rely on fixed, global intervention vectors,\noverlook the causal influence of individual input tokens, and fail to leverage\ninformative gradients from the model's logits, particularly in multimodal\nsettings where visual and textual inputs contribute unevenly. To address these\nlimitations, we introduce GrAInS, an inference-time steering approach that\noperates across both language-only and vision-language models and tasks. GrAInS\nuses contrastive, gradient-based attribution via Integrated Gradients to\nidentify the top-k most influential tokens, both positively and negatively\nattributed based on their contribution to preferred versus dispreferred\noutputs. These tokens are then used to construct directional steering vectors\nthat capture semantic shifts from undesirable to desirable behavior. During\ninference, GrAInS adjusts hidden activations at transformer layers guided by\ntoken-level attribution signals, and normalizes activations to preserve\nrepresentational scale. This enables fine-grained, interpretable, and modular\ncontrol over model behavior, without retraining or auxiliary supervision.\nEmpirically, GrAInS consistently outperforms both fine-tuning and existing\nsteering baselines: it achieves a 13.22% accuracy gain on TruthfulQA using\nLlama-3.1-8B, reduces hallucination rates on MMHal-Bench from 0.624 to 0.514\nwith LLaVA-1.6-7B, and improves alignment win rates on SPA-VL by 8.11%, all\nwhile preserving the model's fluency and general capabilities.", "AI": {"tldr": "GrAInS是一种新的推理时干预方法，通过基于梯度的归因识别关键词元，并构建方向性干预向量来调整LLMs和VLMs的内部激活，从而实现对模型行为的精细控制，且无需微调。", "motivation": "现有推理时干预方法存在局限性：依赖固定全局干预向量、忽视个体输入词元的因果影响、未利用模型logits中的信息梯度，尤其在多模态设置中未能有效处理视觉和文本输入的不平衡贡献。", "method": "GrAInS使用对比性的、基于梯度的归因（通过Integrated Gradients）来识别对期望和非期望输出贡献最大的前k个词元。这些词元用于构建从不良行为到期望行为的语义转变方向性干预向量。在推理过程中，GrAInS根据词元级别的归因信号调整Transformer层的隐藏激活，并对激活进行归一化以保持表示尺度。", "result": "GrAInS在多个任务上持续优于微调和现有干预基线：在TruthfulQA上使用Llama-3.1-8B实现了13.22%的准确率提升；使用LLaVA-1.6-7B在MMHal-Bench上将幻觉率从0.624降至0.514；在SPA-VL上将对齐胜率提高了8.11%，同时保持了模型的流畅性和通用能力。", "conclusion": "GrAInS提供了一种无需再训练或辅助监督的、对模型行为进行精细、可解释和模块化控制的方法，并在语言和视觉-语言模型任务中均表现出优异的性能。"}}
{"id": "2507.17995", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.17995", "abs": "https://arxiv.org/abs/2507.17995", "authors": ["Huy Nguyen", "Kien Nguyen", "Akila Pemasiri", "Akmal Jahan", "Clinton Fookes", "Sridha Sridharan"], "title": "AG-VPReID.VIR: Bridging Aerial and Ground Platforms for Video-based Visible-Infrared Person Re-ID", "comment": "Accepted atIEEE International Joint Conference on Biometrics (IJCB)\n  2025", "summary": "Person re-identification (Re-ID) across visible and infrared modalities is\ncrucial for 24-hour surveillance systems, but existing datasets primarily focus\non ground-level perspectives. While ground-based IR systems offer nighttime\ncapabilities, they suffer from occlusions, limited coverage, and vulnerability\nto obstructions--problems that aerial perspectives uniquely solve. To address\nthese limitations, we introduce AG-VPReID.VIR, the first aerial-ground\ncross-modality video-based person Re-ID dataset. This dataset captures 1,837\nidentities across 4,861 tracklets (124,855 frames) using both UAV-mounted and\nfixed CCTV cameras in RGB and infrared modalities. AG-VPReID.VIR presents\nunique challenges including cross-viewpoint variations, modality discrepancies,\nand temporal dynamics. Additionally, we propose TCC-VPReID, a novel\nthree-stream architecture designed to address the joint challenges of\ncross-platform and cross-modality person Re-ID. Our approach bridges the domain\ngaps between aerial-ground perspectives and RGB-IR modalities, through\nstyle-robust feature learning, memory-based cross-view adaptation, and\nintermediary-guided temporal modeling. Experiments show that AG-VPReID.VIR\npresents distinctive challenges compared to existing datasets, with our\nTCC-VPReID framework achieving significant performance gains across multiple\nevaluation protocols. Dataset and code are available at\nhttps://github.com/agvpreid25/AG-VPReID.VIR.", "AI": {"tldr": "该研究引入了首个空对地跨模态视频行人重识别数据集AG-VPReID.VIR，并提出了一个三流架构TCC-VPReID以应对跨平台和跨模态的挑战。", "motivation": "现有的行人重识别数据集主要关注地面视角，而地面红外系统存在遮挡、覆盖范围有限和易受阻碍等问题。空中视角能有效解决这些问题，但缺乏相应的跨模态数据集和方法来支持24小时监控系统。", "method": "引入了AG-VPReID.VIR数据集，包含1837个身份、4861个轨迹（124855帧），通过无人机和固定CCTV摄像头在RGB和红外模态下采集。提出了TCC-VPReID，一个新颖的三流架构，通过风格鲁棒特征学习、基于记忆的跨视角适应和中介引导的时间建模，来弥合空对地视角和RGB-IR模态之间的域差异。", "result": "AG-VPReID.VIR数据集相比现有数据集提出了独特的挑战。所提出的TCC-VPReID框架在多个评估协议下取得了显著的性能提升。", "conclusion": "该研究通过提供一个新颖的空对地跨模态视频行人重识别数据集和一种有效的三流架构，显著推动了跨平台和跨模态行人重识别领域的发展，为24小时监控系统提供了关键支持。"}}
{"id": "2507.18269", "categories": ["math.OC"], "pdf": "https://arxiv.org/pdf/2507.18269", "abs": "https://arxiv.org/abs/2507.18269", "authors": ["Makito Oku"], "title": "Designing efficient interventions for pre-disease states using control theory", "comment": "24 pages, 14 figures, 1 table, submitted to NOLTA", "summary": "To extend healthy life expectancy in an aging society, it is crucial to\nprevent various diseases at pre-disease states. Although dynamical network\nbiomarker theory has been developed for pre-disease detection, mathematical\nframeworks for pre-disease treatment have not been well established. Here I\npropose a control theory-based approach for pre-disease treatment, named Markov\nchain sparse control (MCSC), where time evolution of a probability distribution\non a Markov chain is described as a discrete-time linear system. By designing a\nsparse controller, a few candidate states for intervention are identified. The\nvalidity of MCSC is demonstrated using numerical simulations and real-data\nanalysis.", "AI": {"tldr": "本文提出了一种基于控制理论的马尔可夫链稀疏控制（MCSC）方法，用于疾病前期的治疗干预，通过识别少数关键干预状态来预防疾病。", "motivation": "在老龄化社会中延长健康预期寿命至关重要，需要在疾病前期预防各种疾病。尽管已有动态网络生物标志物理论用于疾病前期检测，但针对疾病前期治疗的数学框架尚未完善。", "method": "提出马尔可夫链稀疏控制（MCSC）方法。将马尔可夫链上概率分布的时间演化描述为一个离散时间线性系统。通过设计稀疏控制器，识别出少数可进行干预的候选状态。", "result": "通过数值模拟和真实数据分析，验证了MCSC方法的有效性。", "conclusion": "MCSC提供了一种基于控制理论的疾病前期治疗方法，能够有效识别关键干预状态，为延长健康寿命提供新的策略。"}}
{"id": "2507.18433", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18433", "abs": "https://arxiv.org/abs/2507.18433", "authors": ["Minxi Ouyang", "Lianghui Zhu", "Yaqing Bao", "Qiang Huang", "Jingli Ouyang", "Tian Guan", "Xitong Ling", "Jiawen Li", "Song Duan", "Wenbin Dai", "Li Zheng", "Xuemei Zhang", "Yonghong He"], "title": "DiagR1: A Vision-Language Model Trained via Reinforcement Learning for Digestive Pathology Diagnosis", "comment": null, "summary": "Multimodal large models have shown great potential in automating pathology\nimage analysis. However, current multimodal models for gastrointestinal\npathology are constrained by both data quality and reasoning transparency:\npervasive noise and incomplete annotations in public datasets predispose vision\nlanguage models to factual hallucinations when generating diagnostic text,\nwhile the absence of explicit intermediate reasoning chains renders the outputs\ndifficult to audit and thus less trustworthy in clinical practice. To address\nthese issues, we construct a large scale gastrointestinal pathology dataset\ncontaining both microscopic descriptions and diagnostic conclusions, and\npropose a prompt argumentation strategy that incorporates lesion classification\nand anatomical site information. This design guides the model to better capture\nimage specific features and maintain semantic consistency in generation.\nFurthermore, we employ a post training pipeline that combines supervised fine\ntuning with Group Relative Policy Optimization (GRPO) to improve reasoning\nquality and output structure. Experimental results on real world pathology\nreport generation tasks demonstrate that our approach significantly outperforms\nstate of the art open source and proprietary baselines in terms of generation\nquality, structural completeness, and clinical relevance. Our solution\noutperforms state of the art models with 18.7% higher clinical relevance, 32.4%\nimproved structural completeness, and 41.2% fewer diagnostic errors,\ndemonstrating superior accuracy and clinical utility compared to existing\nsolutions.", "AI": {"tldr": "该研究针对胃肠道病理图像分析中多模态大模型的幻觉和推理不透明问题，构建了大规模数据集，并提出了结合提示论证策略和后训练优化（SFT+GRPO）的方法，显著提升了诊断报告生成的质量、完整性和临床相关性。", "motivation": "当前用于胃肠道病理学的多模态模型受限于数据质量和推理透明度。公共数据集中普遍存在的噪声和不完整标注导致视觉语言模型在生成诊断文本时容易产生事实幻觉，同时缺乏明确的中间推理链使得输出难以审计，从而降低了临床实践中的可信度。", "method": "1. 构建了一个包含微观描述和诊断结论的大规模胃肠道病理数据集。2. 提出了一种提示论证策略，该策略结合了病变分类和解剖部位信息，以引导模型更好地捕获图像特定特征并保持生成内容的语义一致性。3. 采用了一种后训练流程，结合了监督微调（SFT）和组相对策略优化（GRPO），以提高推理质量和输出结构。", "result": "在真实世界病理报告生成任务中的实验结果表明，该方法在生成质量、结构完整性和临床相关性方面显著优于现有最先进的开源和专有基线模型。具体表现为：临床相关性提高18.7%，结构完整性提高32.4%，诊断错误减少41.2%。", "conclusion": "该解决方案在准确性和临床实用性方面优于现有方案，有效解决了胃肠道病理图像分析中多模态大模型的幻觉和推理不透明问题，展示了卓越的性能。"}}
{"id": "2507.18317", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.18317", "abs": "https://arxiv.org/abs/2507.18317", "authors": ["Chenglong Qian", "Yang Xu", "Xiufang Shi", "Jiming Chen", "Liang Li"], "title": "AF-RLIO: Adaptive Fusion of Radar-LiDAR-Inertial Information for Robust Odometry in Challenging Environments", "comment": null, "summary": "In robotic navigation, maintaining precise pose estimation and navigation in\ncomplex and dynamic environments is crucial. However, environmental challenges\nsuch as smoke, tunnels, and adverse weather can significantly degrade the\nperformance of single-sensor systems like LiDAR or GPS, compromising the\noverall stability and safety of autonomous robots. To address these challenges,\nwe propose AF-RLIO: an adaptive fusion approach that integrates 4D\nmillimeter-wave radar, LiDAR, inertial measurement unit (IMU), and GPS to\nleverage the complementary strengths of these sensors for robust odometry\nestimation in complex environments. Our method consists of three key modules.\nFirstly, the pre-processing module utilizes radar data to assist LiDAR in\nremoving dynamic points and determining when environmental conditions are\ndegraded for LiDAR. Secondly, the dynamic-aware multimodal odometry selects\nappropriate point cloud data for scan-to-map matching and tightly couples it\nwith the IMU using the Iterative Error State Kalman Filter. Lastly, the factor\ngraph optimization module balances weights between odometry and GPS data,\nconstructing a pose graph for optimization. The proposed approach has been\nevaluated on datasets and tested in real-world robotic environments,\ndemonstrating its effectiveness and advantages over existing methods in\nchallenging conditions such as smoke and tunnels.", "AI": {"tldr": "该论文提出AF-RLIO，一种自适应多传感器融合方法，整合4D毫米波雷达、激光雷达、惯性测量单元（IMU）和GPS，以在复杂环境中实现鲁棒的里程计估计。", "motivation": "在机器人导航中，单传感器系统（如激光雷达或GPS）在烟雾、隧道和恶劣天气等复杂动态环境中性能会显著下降，从而影响自主机器人的稳定性和安全性。", "method": "该方法包含三个核心模块：1. 预处理模块：利用雷达数据辅助激光雷达去除动态点，并判断激光雷达环境条件何时恶化。2. 动态感知多模态里程计：选择合适的点云数据进行扫描到地图匹配，并使用迭代误差状态卡尔曼滤波器与IMU紧密耦合。3. 因子图优化模块：平衡里程计和GPS数据之间的权重，构建姿态图进行优化。", "result": "该方法已在数据集和真实机器人环境中进行评估和测试，结果表明在烟雾和隧道等挑战性条件下，其有效性优于现有方法。", "conclusion": "所提出的自适应融合方法在复杂和动态环境中，通过结合多种传感器的互补优势，显著提高了机器人姿态估计和导航的鲁棒性。"}}
{"id": "2507.18145", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.18145", "abs": "https://arxiv.org/abs/2507.18145", "authors": ["Moritz Schönherr", "Carsten Lutz"], "title": "Logical Characterizations of GNNs with Mean Aggregation", "comment": null, "summary": "We study the expressive power of graph neural networks (GNNs) with mean as\nthe aggregation function. In the non-uniform setting, we show that such GNNs\nhave exactly the same expressive power as ratio modal logic, which has modal\noperators expressing that at least a certain ratio of the successors of a\nvertex satisfies a specified property. The non-uniform expressive power of mean\nGNNs is thus higher than that of GNNs with max aggregation, but lower than for\nsum aggregation--the latter are characterized by modal logic and graded modal\nlogic, respectively. In the uniform setting, we show that the expressive power\nrelative to MSO is exactly that of alternation-free modal logic, under the\nnatural assumptions that combination functions are continuous and\nclassification functions are thresholds. This implies that, relative to MSO and\nin the uniform setting, mean GNNs are strictly less expressive than sum GNNs\nand max GNNs. When any of the assumptions is dropped, the expressive power\nincreases.", "AI": {"tldr": "本文研究了以均值作为聚合函数的图神经网络（GNNs）的表达能力，并将其与各种模态逻辑进行比较，发现其在不同设置下表达能力各异。", "motivation": "理解不同聚合函数对GNNs表达能力的影响，特别是均值聚合，并将其与已知的其他聚合函数（如求和、最大值）进行对比，以精确刻画其计算极限。", "method": "通过将均值GNNs的表达能力与不同的模态逻辑（如比率模态逻辑、交替自由模态逻辑）进行等价性证明来分析其表达力。区分了非均匀设置和均匀设置，并在均匀设置下考虑了连续组合函数和阈值分类函数的自然假设。", "result": "在非均匀设置下，均值GNNs的表达能力与比率模态逻辑完全相同，高于最大值聚合GNNs，但低于求和聚合GNNs。在均匀设置下（假设组合函数连续且分类函数为阈值），相对于MSO，均值GNNs的表达能力与交替自由模态逻辑完全相同，且严格低于求和及最大值聚合GNNs。若移除任何假设，表达能力会提升。", "conclusion": "均值聚合GNNs的表达能力因设置和假设条件而异。在非均匀设置下，它具有独特的比率推理能力；而在均匀设置下，在特定假设下其表达能力相对受限，但通过放宽假设可以增强。"}}
{"id": "2507.18044", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18044", "abs": "https://arxiv.org/abs/2507.18044", "authors": ["Hoyeon Lee", "Sejung Son", "Ye-Eun Kang", "Jong-Hwan Kim"], "title": "Synthetic Data Generation for Phrase Break Prediction with Large Language Model", "comment": "Accepted at Interspeech 2025", "summary": "Current approaches to phrase break prediction address crucial prosodic\naspects of text-to-speech systems but heavily rely on vast human annotations\nfrom audio or text, incurring significant manual effort and cost. Inherent\nvariability in the speech domain, driven by phonetic factors, further\ncomplicates acquiring consistent, high-quality data. Recently, large language\nmodels (LLMs) have shown success in addressing data challenges in NLP by\ngenerating tailored synthetic data while reducing manual annotation needs.\nMotivated by this, we explore leveraging LLM to generate synthetic phrase break\nannotations, addressing the challenges of both manual annotation and\nspeech-related tasks by comparing with traditional annotations and assessing\neffectiveness across multiple languages. Our findings suggest that LLM-based\nsynthetic data generation effectively mitigates data challenges in phrase break\nprediction and highlights the potential of LLMs as a viable solution for the\nspeech domain.", "AI": {"tldr": "本文探讨了利用大型语言模型（LLM）生成合成韵律短语边界标注数据，以减少人工标注成本和应对语音领域数据变异性挑战，并证明其在多语言短语边界预测中的有效性。", "motivation": "当前的短语边界预测方法严重依赖大量人工标注数据，耗时且成本高昂。此外，语音领域固有的变异性使得获取高质量、一致的数据更加困难。鉴于大型语言模型在自然语言处理中生成定制合成数据以减少人工标注的成功经验，研究者受此启发。", "method": "研究方法是利用大型语言模型（LLM）生成合成的短语边界标注。通过与传统标注进行比较，并在多种语言中评估其有效性，来验证这种方法的潜力。", "result": "研究结果表明，基于LLM的合成数据生成能有效缓解短语边界预测中的数据挑战。", "conclusion": "该研究强调了大型语言模型作为解决语音领域数据挑战的潜在可行方案。"}}
{"id": "2507.17996", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.17996", "abs": "https://arxiv.org/abs/2507.17996", "authors": ["Emma A. M. Stanley", "Raghav Mehta", "Mélanie Roschewitz", "Nils D. Forkert", "Ben Glocker"], "title": "Exploring the interplay of label bias with subgroup size and separability: A case study in mammographic density classification", "comment": "Accepted at MICCAI Workshop on Fairness of AI in Medical Imaging\n  (FAIMI) 2025", "summary": "Systematic mislabelling affecting specific subgroups (i.e., label bias) in\nmedical imaging datasets represents an understudied issue concerning the\nfairness of medical AI systems. In this work, we investigated how size and\nseparability of subgroups affected by label bias influence the learned features\nand performance of a deep learning model. Therefore, we trained deep learning\nmodels for binary tissue density classification using the EMory BrEast imaging\nDataset (EMBED), where label bias affected separable subgroups (based on\nimaging manufacturer) or non-separable \"pseudo-subgroups\". We found that\nsimulated subgroup label bias led to prominent shifts in the learned feature\nrepresentations of the models. Importantly, these shifts within the feature\nspace were dependent on both the relative size and the separability of the\nsubgroup affected by label bias. We also observed notable differences in\nsubgroup performance depending on whether a validation set with clean labels\nwas used to define the classification threshold for the model. For instance,\nwith label bias affecting the majority separable subgroup, the true positive\nrate for that subgroup fell from 0.898, when the validation set had clean\nlabels, to 0.518, when the validation set had biased labels. Our work\nrepresents a key contribution toward understanding the consequences of label\nbias on subgroup fairness in medical imaging AI.", "AI": {"tldr": "本研究发现，医学影像数据集中特定子群体的标签偏差（即标签偏倚）会导致深度学习模型学习到的特征表示发生显著偏移，并影响子群体性能，其影响程度取决于受影响子群体的大小和可分离性。", "motivation": "医学影像数据集中影响特定子群体的系统性错误标注（即标签偏倚）是一个未被充分研究的问题，它关乎医疗AI系统的公平性。", "method": "研究人员使用EMory BrEast imaging Dataset (EMBED)训练了用于二元组织密度分类的深度学习模型。他们模拟了标签偏倚，使其影响可分离子群体（基于成像制造商）或不可分离的“伪子群体”，并观察标签偏倚对模型学习特征和性能的影响。", "result": "模拟的子群体标签偏倚导致模型学习到的特征表示发生显著偏移，且这些偏移取决于受标签偏倚影响子群体的相对大小和可分离性。此外，子群体性能的差异也取决于是否使用具有干净标签的验证集来定义分类阈值。例如，当标签偏倚影响主要可分离子群体时，该子群体的真阳性率从使用干净标签验证集时的0.898下降到使用偏倚标签验证集时的0.518。", "conclusion": "本研究对理解标签偏倚在医学影像AI中对子群体公平性的影响做出了关键贡献。"}}
{"id": "2507.18309", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.18309", "abs": "https://arxiv.org/abs/2507.18309", "authors": ["Emir Cem Gezer", "Roger Skjetne"], "title": "Maneuvering-based Dynamic Thrust Allocation for Fully-Actuated Vessels", "comment": null, "summary": "This paper introduces a new approach to solving the thrust allocation problem\nusing the maneuvering problem in the maritime domain for fully actuated\nvessels. The method uses a control Lyapunov function to create a nonlinear\nreference filter for the thruster forces. The filter ensures dynamic tracking\nof the optimal thrust allocation solution with rate limitation in the output\nthruster references. It further uses control barrier functions to ensure that\nthe thruster force saturation limits are respected. The approach aims for\nsimplicity and effectiveness, as well as smooth and dynamic thruster reference\nsignals, in the implementation of thrust allocation for marine vessels.", "AI": {"tldr": "本文提出了一种基于控制Lyapunov函数（CLF）和控制障碍函数（CBF）的新方法，用于解决全驱动船舶的推力分配问题，以确保动态跟踪、速率限制和推力饱和度约束。", "motivation": "现有推力分配方法在实施中可能面临平滑性、动态响应和约束满足的挑战。该研究旨在为海洋船舶的推力分配实现简单、有效、平滑和动态的推力参考信号。", "method": "该方法使用控制Lyapunov函数（CLF）为推力创建非线性参考滤波器，以确保对最优推力分配解的动态跟踪，并对输出推力参考进行速率限制。同时，利用控制障碍函数（CBF）来确保推力饱和限制得到遵守。", "result": "该方法成功实现了对最优推力分配解的动态跟踪，并有效处理了输出推力参考的速率限制。它还确保了推力力饱和限制得到尊重，从而在推力分配实现中获得了简单、有效、平滑和动态的推力参考信号。", "conclusion": "所提出的基于CLF和CBF的推力分配方法，为全驱动船舶提供了一种简单、有效、并能生成平滑动态推力参考信号的解决方案，同时满足了速率限制和推力饱和度约束。"}}
{"id": "2507.18144", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.18144", "abs": "https://arxiv.org/abs/2507.18144", "authors": ["Jinhong He", "Minglong Xue", "Zhipu Liu", "Mingliang Zhou", "Aoxiang Ning", "Palaiahnakote Shivakumara"], "title": "Degradation-Consistent Learning via Bidirectional Diffusion for Low-Light Image Enhancement", "comment": "10page", "summary": "Low-light image enhancement aims to improve the visibility of degraded images\nto better align with human visual perception. While diffusion-based methods\nhave shown promising performance due to their strong generative capabilities.\nHowever, their unidirectional modelling of degradation often struggles to\ncapture the complexity of real-world degradation patterns, leading to\nstructural inconsistencies and pixel misalignments. To address these\nchallenges, we propose a bidirectional diffusion optimization mechanism that\njointly models the degradation processes of both low-light and normal-light\nimages, enabling more precise degradation parameter matching and enhancing\ngeneration quality. Specifically, we perform bidirectional diffusion-from\nlow-to-normal light and from normal-to-low light during training and introduce\nan adaptive feature interaction block (AFI) to refine feature representation.\nBy leveraging the complementarity between these two paths, our approach imposes\nan implicit symmetry constraint on illumination attenuation and noise\ndistribution, facilitating consistent degradation learning and improving the\nmodels ability to perceive illumination and detail degradation. Additionally,\nwe design a reflection-aware correction module (RACM) to guide color\nrestoration post-denoising and suppress overexposed regions, ensuring content\nconsistency and generating high-quality images that align with human visual\nperception. Extensive experiments on multiple benchmark datasets demonstrate\nthat our method outperforms state-of-the-art methods in both quantitative and\nqualitative evaluations while generalizing effectively to diverse degradation\nscenarios. Code at https://github.com/hejh8/BidDiff", "AI": {"tldr": "提出了一种双向扩散优化机制（BidDiff）用于低光图像增强，通过同时建模低光和正常光图像的降质过程，并结合特征交互和反射感知校正，解决了现有单向扩散方法在复杂真实世界降质中的局限性，实现了更高质量的图像增强。", "motivation": "现有扩散模型在低光图像增强中表现出色，但其单向降质建模难以捕捉真实世界降质的复杂性，导致结构不一致和像素错位，无法有效提升图像可见性以符合人类视觉感知。", "method": "本文提出双向扩散优化机制，在训练过程中进行低光到正常光和正常光到低光的双向扩散建模，以实现更精确的降质参数匹配。引入自适应特征交互块（AFI）来优化特征表示，并通过双向路径的互补性施加隐式对称约束。此外，设计了反射感知校正模块（RACM）来引导去噪后的色彩恢复并抑制过曝区域。", "result": "在多个基准数据集上的广泛实验表明，该方法在定量和定性评估方面均优于现有最先进的方法，并且能有效泛化到不同的降质场景。", "conclusion": "通过双向扩散优化机制、特征交互和反射感知校正，本文提出的方法能够更准确地学习和感知光照与细节降质，从而生成高质量、符合人类视觉感知的低光增强图像。"}}
{"id": "2507.18344", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.18344", "abs": "https://arxiv.org/abs/2507.18344", "authors": ["Gyuhyeon Pak", "Hae Min Cho", "Euntai Kim"], "title": "G2S-ICP SLAM: Geometry-aware Gaussian Splatting ICP SLAM", "comment": "8 pages, 6 figures", "summary": "In this paper, we present a novel geometry-aware RGB-D Gaussian Splatting\nSLAM system, named G2S-ICP SLAM. The proposed method performs high-fidelity 3D\nreconstruction and robust camera pose tracking in real-time by representing\neach scene element using a Gaussian distribution constrained to the local\ntangent plane. This effectively models the local surface as a 2D Gaussian disk\naligned with the underlying geometry, leading to more consistent depth\ninterpretation across multiple viewpoints compared to conventional 3D\nellipsoid-based representations with isotropic uncertainty. To integrate this\nrepresentation into the SLAM pipeline, we embed the surface-aligned Gaussian\ndisks into a Generalized ICP framework by introducing anisotropic covariance\nprior without altering the underlying registration formulation. Furthermore we\npropose a geometry-aware loss that supervises photometric, depth, and normal\nconsistency. Our system achieves real-time operation while preserving both\nvisual and geometric fidelity. Extensive experiments on the Replica and\nTUM-RGBD datasets demonstrate that G2S-ICP SLAM outperforms prior SLAM systems\nin terms of localization accuracy, reconstruction completeness, while\nmaintaining the rendering quality.", "AI": {"tldr": "本文提出了一种新颖的几何感知RGB-D高斯溅射SLAM系统G2S-ICP SLAM，通过将场景元素表示为受局部切平面约束的2D高斯盘，实现了实时高保真3D重建和鲁棒的相机位姿跟踪。", "motivation": "传统基于3D椭球体且具有各向同性不确定性的表示方法在多视角下深度解释一致性较差。本研究旨在通过引入与底层几何对齐的2D高斯盘，更有效地建模局部表面，从而实现更一致的深度解释和更高保真度的重建。", "method": "该方法将每个场景元素表示为受局部切平面约束的高斯分布（2D高斯盘）。通过引入各向异性协方差先验，将表面对齐的高斯盘嵌入到广义ICP框架中，同时不改变底层配准公式。此外，提出了一个几何感知损失函数，用于监督光度、深度和法线一致性。", "result": "G2S-ICP SLAM系统实现了实时操作，并保持了视觉和几何保真度。在Replica和TUM-RGBD数据集上的大量实验表明，该系统在定位精度和重建完整性方面优于现有SLAM系统，同时保持了渲染质量。", "conclusion": "G2S-ICP SLAM是一种有效的几何感知RGB-D SLAM系统，通过创新的2D高斯盘表示和优化的ICP框架，实现了实时、高保真的3D重建和鲁棒的相机位姿跟踪，并在性能上超越了现有方法。"}}
{"id": "2507.18178", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18178", "abs": "https://arxiv.org/abs/2507.18178", "authors": ["Mutian Yang", "Jiandong Gao", "Ji Wu"], "title": "Decoupling Knowledge and Reasoning in LLMs: An Exploration Using Cognitive Dual-System Theory", "comment": null, "summary": "While large language models (LLMs) leverage both knowledge and reasoning\nduring inference, the capacity to distinguish between them plays a pivotal role\nin model analysis, interpretability, and development. Inspired by dual-system\ncognitive theory, we propose a cognition attribution framework to decouple the\ncontribution of knowledge and reasoning. In particular, the cognition of LLMs\nis decomposed into two distinct yet complementary phases: knowledge retrieval\n(Phase 1) and reasoning adjustment (Phase 2). To separate these phases, LLMs\nare prompted to generate answers under two different cognitive modes, fast\nthinking and slow thinking, respectively. The performance under different\ncognitive modes is analyzed to quantify the contribution of knowledge and\nreasoning. This architecture is employed to 15 LLMs across 3 datasets. Results\nreveal: (1) reasoning adjustment is domain-specific, benefiting\nreasoning-intensive domains (e.g., mathematics, physics, and chemistry) and\npotentially imparing knowledge-intensive domains. (2) Parameter scaling\nimproves both knowledge and reasoning, with knowledge improvements being more\npronounced. Additionally, parameter scaling make LLMs reasoning significantly\nmore prudent, while moderately more intelligent. (3) Knowledge primarily\nresides in lower network layers, while reasoning operates in higher layers. Our\nframework not only helps understand LLMs from a \"decoupling\" perspective, but\nalso provides new insights into existing research, including scaling laws,\nhierarchical knowledge editing, and limitations of small-model reasoning.", "AI": {"tldr": "该研究提出了一个认知归因框架，基于双系统认知理论，将大型语言模型（LLMs）的认知解耦为知识检索和推理调整两个阶段，并通过“快思考”和“慢思考”模式量化其贡献，揭示了推理调整的领域特异性、参数缩放的影响以及知识与推理在模型层级中的分布。", "motivation": "在LLMs的推理过程中，区分知识和推理对其模型分析、可解释性和发展至关重要。", "method": "受双系统认知理论启发，提出了一个认知归因框架。该框架将LLMs的认知分解为知识检索（阶段1）和推理调整（阶段2）。通过提示LLMs在“快思考”和“慢思考”两种不同认知模式下生成答案，以分离并量化知识和推理的贡献。该架构应用于15个LLMs和3个数据集进行分析。", "result": "(1) 推理调整具有领域特异性，有利于推理密集型领域（如数学、物理、化学），但可能损害知识密集型领域。(2) 参数缩放能同时提升知识和推理能力，其中知识提升更显著。此外，参数缩放使LLMs的推理更趋谨慎，智能程度适度提升。(3) 知识主要存在于较低的网络层，而推理则在较高的层级中运作。", "conclusion": "该框架不仅有助于从“解耦”视角理解LLMs，还为现有研究提供了新见解，包括缩放定律、分层知识编辑以及小型模型推理的局限性。"}}
{"id": "2507.18055", "categories": ["cs.CL", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18055", "abs": "https://arxiv.org/abs/2507.18055", "authors": ["Tevin Atwal", "Chan Nam Tieu", "Yefeng Yuan", "Zhan Shi", "Yuhong Liu", "Liang Cheng"], "title": "Privacy-Preserving Synthetic Review Generation with Diverse Writing Styles Using LLMs", "comment": null, "summary": "The increasing use of synthetic data generated by Large Language Models\n(LLMs) presents both opportunities and challenges in data-driven applications.\nWhile synthetic data provides a cost-effective, scalable alternative to\nreal-world data to facilitate model training, its diversity and privacy risks\nremain underexplored. Focusing on text-based synthetic data, we propose a\ncomprehensive set of metrics to quantitatively assess the diversity (i.e.,\nlinguistic expression, sentiment, and user perspective), and privacy (i.e.,\nre-identification risk and stylistic outliers) of synthetic datasets generated\nby several state-of-the-art LLMs. Experiment results reveal significant\nlimitations in LLMs' capabilities in generating diverse and privacy-preserving\nsynthetic data. Guided by the evaluation results, a prompt-based approach is\nproposed to enhance the diversity of synthetic reviews while preserving\nreviewer privacy.", "AI": {"tldr": "该研究评估了LLM生成的合成文本数据的多样性和隐私风险，发现LLM在此方面存在显著局限性，并提出了基于提示的方法来改善多样性并保护隐私。", "motivation": "LLM生成的合成数据在数据驱动应用中带来机遇，但其多样性和隐私风险尚未得到充分探索。真实数据成本高昂且难以扩展，合成数据提供了替代方案，因此需要对其质量进行深入评估。", "method": "提出了一套全面的度量标准，用于定量评估合成文本数据的多样性（语言表达、情感、用户视角）和隐私（再识别风险、风格异常值）。利用这些指标评估了多个最先进LLM生成的合成数据集。基于评估结果，提出了一种基于提示的方法来增强合成评论的多样性，同时保护用户隐私。", "result": "实验结果揭示了LLM在生成多样化和隐私保护的合成数据方面的显著局限性。", "conclusion": "LLM在生成高质量（多样且隐私保护）的合成数据方面存在不足。研究提出了一个基于提示的方法，可以有效提升合成数据的多样性并兼顾隐私保护。"}}
{"id": "2507.17998", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.17998", "abs": "https://arxiv.org/abs/2507.17998", "authors": ["Jaeho Shin", "Hyeonjae Gil", "Junwoo Jang", "Maani Ghaffari", "Ayoung Kim"], "title": "Registration beyond Points: General Affine Subspace Alignment via Geodesic Distance on Grassmann Manifold", "comment": null, "summary": "Affine Grassmannian has been favored for expressing proximity between lines\nand planes due to its theoretical exactness in measuring distances among\nfeatures. Despite this advantage, the existing method can only measure the\nproximity without yielding the distance as an explicit function of rigid body\ntransformation. Thus, an optimizable distance function on the manifold has\nremained underdeveloped, stifling its application in registration problems.\nThis paper is the first to explicitly derive an optimizable cost function\nbetween two Grassmannian features with respect to rigid body transformation\n($\\mathbf{R}$ and $\\mathbf{t}$). Specifically, we present a rigorous\nmathematical proof demonstrating that the bases of high-dimensional linear\nsubspaces can serve as an explicit representation of the cost. Finally, we\npropose an optimizable cost function based on the transformed bases that can be\napplied to the registration problem of any affine subspace. Compared to vector\nparameter-based approaches, our method is able to find a globally optimal\nsolution by directly minimizing the geodesic distance which is agnostic to\nrepresentation ambiguity. The resulting cost function and its extension to the\ninlier-set maximizing \\ac{BnB} solver have been demonstrated to improve the\nconvergence of existing solutions or outperform them in various computer vision\ntasks. The code is available on\nhttps://github.com/joomeok/GrassmannRegistration.", "AI": {"tldr": "本文首次推导了一个可优化的仿射格拉斯曼流形上的代价函数，用于测量特征间的距离，并能应用于刚体变换下的配准问题，实现全局最优解。", "motivation": "现有的仿射格拉斯曼方法虽然能精确测量特征间的接近度，但无法提供一个显式的、关于刚体变换的距离函数，这限制了其在配准问题中的应用，导致可优化的流形距离函数仍未被充分开发。", "method": "本文通过严格的数学证明，首次明确推导了一个可优化的代价函数，该函数将高维线性子空间的基础作为代价的显式表示。在此基础上，提出了一种基于变换后的基础的可优化代价函数，可应用于任何仿射子空间的配准问题。该方法通过直接最小化测地距离来寻找全局最优解。", "result": "与基于向量参数的方法相比，本文方法能够通过直接最小化测地距离来找到全局最优解，且不受表示模糊性的影响。所得到的代价函数及其在内点集最大化BnB求解器中的扩展，在各种计算机视觉任务中均表现出改进现有解决方案的收敛性或超越现有解决方案的性能。", "conclusion": "本文成功推导并提出了一个可优化的仿射格拉斯曼流形上的距离代价函数，该函数能够明确表示刚体变换下的特征距离，并实现了全局最优配准。这显著提升了现有配准方案的性能和收敛性，为计算机视觉任务提供了新的有效工具。"}}
{"id": "2507.18419", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.18419", "abs": "https://arxiv.org/abs/2507.18419", "authors": ["Francesco Ceccanti", "Aldo Bischi", "Umberto Desideri", "Andrea Baccioli"], "title": "Toward Sustainable Vertical Farming: Impacts of Environmental Factors and Energy Mix on Performance and Costs", "comment": null, "summary": "The increasing interest in vertical farming arises from its ability to ensure\nconsistent, high-quality, and pest-free vegetable production while supporting\nsynergies with energy systems and urban development. Accordingly, standardized\ndesign and operation guidelines are essential to improve energy efficiency and\nlower costs. This study analyzes the production performance and energy\nconsumption of a vertical farming system, assessing its efficiency,\nsustainability, and economic viability. A total of 162 scenarios were evaluated\nby combining three levels of temperature, photosynthetic photon flux density\n(PPFD), and CO2 concentration across three distinct climatic zones, namely\nNorway, China, and Dubai, which also differ from a socio-environmental\nviewpoint. Two insulation thicknesses were also tested in each scenario.\nResults indicate that due to the heating, ventilation, and air conditioning and\ndehumidification (HVACD) system, neither the insulation layer nor the external\nclimate significantly influences crop productivity. PPFD proved to be the\ndominant factor in crop growth (correlation: 0.85), followed by CO2 (0.36) and\nindoor temperature (0.22). PPFD also emerged as the primary driver of overall\nenergy consumption (correlation: 0.73), as it affects both lighting and HVACD\nloads. Notably, the lowest specific energy consumption (SEC) coincided with the\nlowest crop productivity (55 kg/m2). The levelized cost of lettuce (LCoL),\nbalancing productivity and energy use, identified the most cost-effective setup\nas 24C, 250 PPFD, 1400 ppm CO2, with insulation, consistent across all\nclimates. Ultimately, only nearly decarbonized energy systems can support\nvertical farming without increasing CO2 emissions compared to imported lettuce.", "AI": {"tldr": "本研究分析了垂直农业系统的生产性能和能耗，评估了其效率、可持续性和经济可行性，旨在为提高能源效率和降低成本提供标准化设计和操作指南。", "motivation": "垂直农业因其能提供稳定、高质量、无虫害的蔬菜生产，并支持与能源系统和城市发展的协同作用而日益受到关注。因此，需要标准化的设计和操作指南来提高能源效率和降低成本。", "method": "研究通过结合三种温度、光合光子通量密度（PPFD）和二氧化碳浓度水平，在挪威、中国和迪拜三个不同气候区评估了162种场景，并测试了两种绝缘厚度，分析了垂直农业系统的生产性能和能耗。", "result": "结果显示，由于暖通空调和除湿（HVACD）系统，绝缘层和外部气候对作物生产力影响不显著。PPFD是作物生长的主要影响因素（相关性：0.85），其次是二氧化碳（0.36）和室内温度（0.22）。PPFD也是总能耗的主要驱动因素（相关性：0.73）。最低的比能耗（SEC）与最低的作物生产力（55公斤/平方米）同时出现。最经济高效的生菜生产设置（LCoL）是24°C、250 PPFD、1400 ppm CO2，并带有绝缘，这在所有气候下都保持一致。", "conclusion": "研究得出结论，只有使用接近脱碳的能源系统，垂直农业才能在不增加二氧化碳排放的情况下，比进口生菜更具环境优势。"}}
{"id": "2507.18558", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.18558", "abs": "https://arxiv.org/abs/2507.18558", "authors": ["Yihong Feng", "Chaitanya Pallerla", "Xiaomin Lin", "Pouya Sohrabipour Sr", "Philip Crandall", "Wan Shou", "Yu She", "Dongyi Wang"], "title": "Synthetic Data Augmentation for Enhanced Chicken Carcass Instance Segmentation", "comment": "Submitted for journal reviewing", "summary": "The poultry industry has been driven by broiler chicken production and has\ngrown into the world's largest animal protein sector. Automated detection of\nchicken carcasses on processing lines is vital for quality control, food\nsafety, and operational efficiency in slaughterhouses and poultry processing\nplants. However, developing robust deep learning models for tasks like instance\nsegmentation in these fast-paced industrial environments is often hampered by\nthe need for laborious acquisition and annotation of large-scale real-world\nimage datasets. We present the first pipeline generating photo-realistic,\nautomatically labeled synthetic images of chicken carcasses. We also introduce\na new benchmark dataset containing 300 annotated real-world images, curated\nspecifically for poultry segmentation research. Using these datasets, this\nstudy investigates the efficacy of synthetic data and automatic data annotation\nto enhance the instance segmentation of chicken carcasses, particularly when\nreal annotated data from the processing line is scarce. A small real dataset\nwith varying proportions of synthetic images was evaluated in prominent\ninstance segmentation models. Results show that synthetic data significantly\nboosts segmentation performance for chicken carcasses across all models. This\nresearch underscores the value of synthetic data augmentation as a viable and\neffective strategy to mitigate data scarcity, reduce manual annotation efforts,\nand advance the development of robust AI-driven automated detection systems for\nchicken carcasses in the poultry processing industry.", "AI": {"tldr": "本研究首次提出生成逼真、自动标注的鸡胴体合成图像的流程，并引入了一个新的真实世界基准数据集。结果表明，合成数据显著提升了鸡胴体实例分割的性能，为解决家禽加工业中真实标注数据稀缺的问题提供了有效策略。", "motivation": "家禽加工线上鸡胴体的自动化检测对于质量控制、食品安全和操作效率至关重要。然而，在快节奏的工业环境中，开发鲁棒的深度学习模型（如实例分割）往往受限于获取和标注大规模真实世界图像数据集的艰巨性。", "method": "本研究开发了首个能生成逼真、自动标注的鸡胴体合成图像的流程。同时，创建了一个包含300张标注真实图像的新基准数据集。研究人员将这些数据集用于评估合成数据和自动数据标注在增强鸡胴体实例分割方面的有效性，特别是在真实标注数据稀缺的情况下。他们使用不同比例的合成图像与少量真实数据集，在主流实例分割模型上进行了评估。", "result": "实验结果表明，合成数据显著提升了所有模型在鸡胴体分割任务上的性能。", "conclusion": "本研究强调了合成数据增强作为一种可行且有效的策略的价值，可以缓解数据稀缺、减少人工标注工作，并推动家禽加工业中鸡胴体鲁棒AI驱动自动化检测系统的发展。"}}
{"id": "2507.18396", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.18396", "abs": "https://arxiv.org/abs/2507.18396", "authors": ["Yonghao Fu", "Cheng Hu", "Haokun Xiong", "Zhangpeng Bao", "Wenyuan Du", "Edoardo Ghignone", "Michele Magno", "Lei Xie", "Hongye Su"], "title": "Residual Koopman Model Predictive Control for Enhanced Vehicle Dynamics with Small On-Track Data Input", "comment": null, "summary": "In vehicle trajectory tracking tasks, the simplest approach is the Pure\nPursuit (PP) Control. However, this single-point preview tracking strategy\nfails to consider vehicle model constraints, compromising driving safety. Model\nPredictive Control (MPC) as a widely adopted control method, optimizes control\nactions by incorporating mechanistic models and physical constraints. While its\ncontrol performance critically depends on the accuracy of vehicle modeling.\nTraditional vehicle modeling approaches face inherent trade-offs between\ncapturing nonlinear dynamics and maintaining computational efficiency, often\nresulting in reduced control performance. To address these challenges, this\npaper proposes Residual Koopman Model Predictive Control (RKMPC) framework.\nThis method uses two linear MPC architecture to calculate control inputs: a\nLinear Model Predictive Control (LMPC) computes the baseline control input\nbased on the vehicle kinematic model, and a neural network-based RKMPC\ncalculates the compensation input. The final control command is obtained by\nadding these two components. This design preserves the reliability and\ninterpretability of traditional mechanistic model while achieving performance\noptimization through residual modeling. This method has been validated on the\nCarsim-Matlab joint simulation platform and a physical 1:10 scale F1TENTH\nracing car. Experimental results show that RKMPC requires only 20% of the\ntraining data needed by traditional Koopman Model Predictive Control (KMPC)\nwhile delivering superior tracking performance. Compared to traditional LMPC,\nRKMPC reduces lateral error by 11.7%-22.1%, decreases heading error by\n8.9%-15.8%, and improves front-wheel steering stability by up to 27.6%. The\nimplementation code is available at: https://github.com/ZJU-DDRX/Residual\nKoopman.", "AI": {"tldr": "本文提出残差Koopman模型预测控制（RKMPC）框架，结合线性运动学模型和神经网络残差补偿，解决了传统方法在车辆轨迹跟踪中模型精度与计算效率的权衡问题，显著提升了跟踪性能和数据效率。", "motivation": "纯追踪控制未考虑车辆模型约束，影响安全；模型预测控制（MPC）性能依赖于车辆模型精度；传统车辆建模在捕获非线性动力学与保持计算效率之间存在固有限制，导致控制性能下降。", "method": "提出残差Koopman模型预测控制（RKMPC）框架，采用双线性MPC架构：一个基于车辆运动学模型的线性MPC（LMPC）计算基线控制输入；一个基于神经网络的RKMPC计算补偿输入。最终控制指令由两者叠加得到，旨在保留传统机械模型可靠性和可解释性的同时，通过残差建模优化性能。", "result": "RKMPC在Carsim-Matlab联合仿真平台和1:10 F1TENTH赛车上验证。与传统Koopman模型预测控制（KMPC）相比，RKMPC仅需20%的训练数据即可提供更优的跟踪性能。与传统LMPC相比，RKMPC横向误差降低11.7%-22.1%，航向误差降低8.9%-15.8%，前轮转向稳定性提高高达27.6%。", "conclusion": "RKMPC通过结合传统机械模型和残差神经网络补偿，成功解决了车辆轨迹跟踪中模型精度和计算效率的矛盾，在保持模型可解释性的同时，显著提升了跟踪性能、转向稳定性，并大幅降低了对训练数据的需求。"}}
{"id": "2507.18198", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18198", "abs": "https://arxiv.org/abs/2507.18198", "authors": ["Felicidad Aguado", "Pedro Cabalar", "Brais Muñiz", "Gilberto Pérez", "Concepción Vidal"], "title": "Comparing Non-minimal Semantics for Disjunction in Answer Set Programming", "comment": null, "summary": "In this paper, we compare four different semantics for disjunction in Answer\nSet Programming that, unlike stable models, do not adhere to the principle of\nmodel minimality. Two of these approaches, Cabalar and Mu\\~niz' \\emph{Justified\nModels} and Doherty and Szalas' \\emph{Strongly Supported Models}, directly\nprovide an alternative non-minimal semantics for disjunction. The other two,\nAguado et al's \\emph{Forks} and Shen and Eiter's \\emph{Determining Inference}\n(DI) semantics, actually introduce a new disjunction connective, but are\ncompared here as if they constituted new semantics for the standard disjunction\noperator. We are able to prove that three of these approaches (Forks, Justified\nModels and a reasonable relaxation of the DI semantics) actually coincide,\nconstituting a common single approach under different definitions. Moreover,\nthis common semantics always provides a superset of the stable models of a\nprogram (in fact, modulo any context) and is strictly stronger than the fourth\napproach (Strongly Supported Models), that actually treats disjunctions as in\nclassical logic.", "AI": {"tldr": "本文比较了四种不遵循模型最小性原则的ASP析取语义，发现其中三种（Forks、Justified Models和松弛的DI语义）实际上是等价的，并构成了共同的语义，该语义是稳定模型的超集，且比Strongly Supported Models更强。", "motivation": "ASP中的稳定模型遵循模型最小性原则，但存在其他不遵循该原则的析取语义。本研究旨在比较这些非最小析取语义，理解它们之间的关系和特性。", "method": "本文比较了四种不同的ASP析取语义：Cabalar和Muñiz的Justified Models、Doherty和Szalas的Strongly Supported Models、Aguado et al的Forks以及Shen和Eiter的Determining Inference (DI) 语义。Forks和DI语义虽然引入了新的析取连接词，但在此被视为标准析取运算符的新语义进行比较。通过理论证明，分析它们之间的重合性、包含关系和强度。", "result": "研究发现，其中三种方法（Forks、Justified Models和DI语义的合理松弛版本）实际上是重合的，在不同定义下构成了一种共同的单一方法。这种共同语义总是提供程序稳定模型的超集。此外，这种共同语义严格强于第四种方法（Strongly Supported Models），后者实际上将析取视为经典逻辑中的处理方式。", "conclusion": "在不遵循最小性原则的ASP析取语义中，存在一个统一的共同语义（包括Forks、Justified Models和松弛的DI语义），它扩展了稳定模型，并且比将析取视为经典逻辑的语义（Strongly Supported Models）更强。这为理解不同的ASP析取语义提供了统一的视角。"}}
{"id": "2507.18061", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.18061", "abs": "https://arxiv.org/abs/2507.18061", "authors": ["Zehan Li", "Hongjie Chen", "Yuxin Zhang", "Jing Zhou", "Xuening Wang", "Hang Lv", "Mengjie Du", "Yaodong Song", "Jie Lian", "Jian Kang", "Jie Li", "Yongxiang Li", "Zhongjiang He", "Xuelong Li"], "title": "TELEVAL: A Dynamic Benchmark Designed for Spoken Language Models in Chinese Interactive Scenarios", "comment": null, "summary": "Spoken language models (SLMs) have seen rapid progress in recent years, along\nwith the development of numerous benchmarks for evaluating their performance.\nHowever, most existing benchmarks primarily focus on evaluating whether SLMs\ncan perform complex tasks comparable to those tackled by large language models\n(LLMs), often failing to align with how users naturally interact in real-world\nconversational scenarios. In this paper, we propose TELEVAL, a dynamic\nbenchmark specifically designed to evaluate SLMs' effectiveness as\nconversational agents in realistic Chinese interactive settings. TELEVAL\ndefines three evaluation dimensions: Explicit Semantics, Paralinguistic and\nImplicit Semantics, and System Abilities. It adopts a dialogue format\nconsistent with real-world usage and evaluates text and audio outputs\nseparately. TELEVAL particularly focuses on the model's ability to extract\nimplicit cues from user speech and respond appropriately without additional\ninstructions. Our experiments demonstrate that despite recent progress,\nexisting SLMs still have considerable room for improvement in natural\nconversational tasks. We hope that TELEVAL can serve as a user-centered\nevaluation framework that directly reflects the user experience and contributes\nto the development of more capable dialogue-oriented SLMs.", "AI": {"tldr": "本文提出了TELEVAL，一个动态基准测试，用于评估口语语言模型（SLMs）在真实中文交互场景中作为对话代理的有效性，并发现现有SLMs仍有很大提升空间。", "motivation": "现有的大多数SLM评估基准主要关注模型能否执行复杂任务，与大型语言模型（LLMs）的能力对标，但未能与用户在真实对话场景中的自然交互方式对齐。", "method": "TELEVAL是一个动态基准测试，定义了三个评估维度：显式语义、副语言和隐式语义、系统能力。它采用与真实世界使用一致的对话格式，并分别评估文本和音频输出，特别关注模型从用户语音中提取隐式线索并适当响应的能力。", "result": "实验表明，尽管近期SLMs取得了进展，但在自然对话任务中，现有模型仍有相当大的改进空间。", "conclusion": "TELEVAL可以作为一个以用户为中心的评估框架，直接反映用户体验，并有助于开发出更强大的面向对话的SLMs。"}}
{"id": "2507.18009", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.18009", "abs": "https://arxiv.org/abs/2507.18009", "authors": ["Jake R. Patock", "Nicole Catherine Lewis", "Kevin McCoy", "Christina Gomez", "Canling Chen", "Lorenzo Luzi"], "title": "GRR-CoCa: Leveraging LLM Mechanisms in Multimodal Model Architectures", "comment": "12 pages, 2 figures", "summary": "State-of-the-art (SOTA) image and text generation models are multimodal\nmodels that have many similarities to large language models (LLMs). Despite\nachieving strong performances, leading foundational multimodal model\narchitectures frequently lag behind the architectural sophistication of\ncontemporary LLMs. We propose GRR-CoCa, an improved SOTA Contrastive Captioner\n(CoCa) model that incorporates Gaussian error gated linear units, root mean\nsquared normalization, and rotary positional embedding into the textual\ndecoders and the vision transformer (ViT) encoder. Each architectural\nmodification has been shown to improve model performance in LLMs, but has yet\nto be adopted in CoCa. We benchmarked GRR-CoCa against Baseline CoCa, a model\nwith the same modified textual decoders but with CoCa's original ViT encoder.\nWe used standard pretraining and fine-tuning workflows to benchmark the models\non contrastive and generative tasks. Our GRR-CoCa significantly outperformed\nBaseline CoCa on the pretraining dataset and three diverse fine-tuning\ndatasets. Pretraining improvements were 27.25% in contrastive loss, 3.71% in\nperplexity, and 7.15% in CoCa loss. The average fine-tuning improvements were\n13.66% in contrastive loss, 5.18% in perplexity, and 5.55% in CoCa loss. We\nshow that GRR-CoCa's modified architecture improves performance and\ngeneralization across vision-language domains.", "AI": {"tldr": "本文提出了GRR-CoCa模型，通过将LLM中有效的架构改进（高斯误差门控线性单元、均方根归一化、旋转位置嵌入）引入CoCa模型的文本解码器和视觉编码器，显著提升了图像和文本生成任务的性能和泛化能力。", "motivation": "尽管最先进的多模态模型（如CoCa）表现出色，但其架构复杂性通常落后于当代大型语言模型（LLMs）。研究旨在将LLM中已验证有效的架构改进引入多模态模型，以提升其性能。", "method": "研究者提出了GRR-CoCa，在SOTA的CoCa模型中，将高斯误差门控线性单元、均方根归一化和旋转位置嵌入集成到文本解码器和视觉Transformer (ViT) 编码器中。通过标准的预训练和微调流程，将GRR-CoCa与基线CoCa（具有相同修改的文本解码器但保留CoCa原始ViT编码器）进行对比基准测试。", "result": "GRR-CoCa在预训练数据集和三个多样化的微调数据集上均显著优于基线CoCa。预训练阶段，对比损失降低27.25%，困惑度降低3.71%，CoCa损失降低7.15%。微调阶段，平均对比损失降低13.66%，困惑度降低5.18%，CoCa损失降低5.55%。", "conclusion": "GRR-CoCa的改进架构能够提升模型在视觉-语言域中的性能和泛化能力，证明了将LLM的先进架构引入多模态模型的有效性。"}}
{"id": "2507.18492", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.18492", "abs": "https://arxiv.org/abs/2507.18492", "authors": ["Mirhan Ürkmez", "Carsten Kallesøe", "Jan Dimon Bendtsen", "Eric C. Kerrigan", "John Leth"], "title": "A Robust Predictive Control Method for Pump Scheduling in Water Distribution Networks", "comment": null, "summary": "Water utilities aim to reduce the high electrical costs of Water Distribution\nNetworks (WDNs), primarily driven by pumping. However, pump scheduling is\nchallenging due to model uncertainties and water demand forecast errors. This\npaper presents a Robust Model Predictive Control (RMPC) method for optimal and\nreliable pump scheduling, extending a previous efficient robust control method\ntailored to our model. A linear model with bounded additive disturbances is\nused to represent tank water level evolution, with uncertainty bounds derived\nfrom WDN simulation and demand data. At each time step, a pump scheduling\npolicy, affine in past disturbances, is optimized to satisfy system constraints\nover a prediction horizon. The resulting policies are then applied in a\nreceding horizon fashion. The optimization problem is formulated to require\n$\\mathcal{O}(N^6)$ computations per iteration with an interior-point method,\nwhich is reduced to $\\mathcal{O}(N^3)$ by reformulating it into a sparse form.\nWhen evaluated on a model representing the water distribution network of\nRanders, a medium-sized town in Denmark, the method surpasses nominal and\nconstraint-tightening model predictive control (MPC) approaches in terms of\nmeeting constraints and provides comparable economic outcomes.", "AI": {"tldr": "本文提出了一种鲁棒模型预测控制（RMPC）方法，用于优化水分配网络（WDN）中的水泵调度，以应对模型不确定性和需求预测误差，并在实际案例中表现出优越的约束满足能力和相似的经济效益。", "motivation": "水务公司面临高昂的电力成本，主要源于水泵运行。然而，水泵调度因模型不确定性和用水需求预测误差而极具挑战性，需要一种能够应对这些不确定性的优化调度方法。", "method": "该研究采用鲁棒模型预测控制（RMPC）方法。它使用具有有界加性扰动的线性模型来表示水箱水位变化，扰动边界通过WDN仿真和需求数据导出。在每个时间步，制定一个与过去扰动相关的泵调度策略，并在预测范围内进行优化以满足系统约束，然后以滚动时域方式应用。优化问题最初需要O(N^6)的计算量，通过稀疏化重构后降至O(N^3)。", "result": "该方法在丹麦兰德斯市的水分配网络模型上进行评估，结果显示其在满足约束方面优于名义和约束收紧的模型预测控制（MPC）方法，并在经济效益上与其相当。", "conclusion": "该RMPC方法能够有效且可靠地解决水分配网络中的水泵优化调度问题，尤其在不确定性环境下表现出更强的鲁棒性，能够更好地满足系统约束，同时保持良好的经济性。"}}
{"id": "2507.18436", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.18436", "abs": "https://arxiv.org/abs/2507.18436", "authors": ["David Blanco-Mulero", "Júlia Borràs", "Carme Torras"], "title": "Evaluating the Pre-Dressing Step: Unfolding Medical Garments Via Imitation Learning", "comment": "6 pages, 4 figures, 2 tables. Accepted to IEEE/RSJ IROS 2025. Project\n  website: https://sites.google.com/view/pre-dressing", "summary": "Robotic-assisted dressing has the potential to significantly aid both\npatients as well as healthcare personnel, reducing the workload and improving\nthe efficiency in clinical settings. While substantial progress has been made\nin robotic dressing assistance, prior works typically assume that garments are\nalready unfolded and ready for use. However, in medical applications gowns and\naprons are often stored in a folded configuration, requiring an additional\nunfolding step. In this paper, we introduce the pre-dressing step, the process\nof unfolding garments prior to assisted dressing. We leverage imitation\nlearning for learning three manipulation primitives, including both high and\nlow acceleration motions. In addition, we employ a visual classifier to\ncategorise the garment state as closed, partly opened, and fully opened. We\nconduct an empirical evaluation of the learned manipulation primitives as well\nas their combinations. Our results show that highly dynamic motions are not\neffective for unfolding freshly unpacked garments, where the combination of\nmotions can efficiently enhance the opening configuration.", "AI": {"tldr": "本文提出并解决了机器人辅助穿衣前的“预穿衣”步骤，即展开折叠衣物的问题，通过模仿学习和视觉分类器实现了衣物展开，并评估了不同操作策略的效果。", "motivation": "现有机器人辅助穿衣研究通常假设衣物已展开，但在医疗应用中，手术服等衣物常是折叠存放的，需要额外的展开步骤，这增加了医护人员的负担和效率问题。", "method": "引入“预穿衣”步骤，即在辅助穿衣前展开衣物。利用模仿学习训练了三种操作原语（包括高低加速运动）。采用视觉分类器将衣物状态分为闭合、部分打开和完全打开。对学习到的操作原语及其组合进行了实证评估。", "result": "研究结果表明，对于刚拆封的衣物，高度动态的运动在展开方面效果不佳。而操作原语的组合可以有效地改善衣物的打开状态。", "conclusion": "在机器人辅助穿衣中，衣物展开（预穿衣步骤）是必不可少且具有挑战性的一步。对于新拆封的衣物，需要特定的操作策略，特别是结合多种运动方式能更有效地实现衣物展开。"}}
{"id": "2507.18290", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18290", "abs": "https://arxiv.org/abs/2507.18290", "authors": ["Antonino Rotolo", "Beatrice Ferrigno", "Jose Miguel Angel Garcia Godinez", "Claudio Novelli", "Giovanni Sartor"], "title": "Foundations for Risk Assessment of AI in Protecting Fundamental Rights", "comment": "24 pages, 1 figure. To be published in: The Philosophical Foundations\n  of Information Technology Law. Oxford University Press, Oxford", "summary": "This chapter introduces a conceptual framework for qualitative risk\nassessment of AI, particularly in the context of the EU AI Act. The framework\naddresses the complexities of legal compliance and fundamental rights\nprotection by itegrating definitional balancing and defeasible reasoning.\nDefinitional balancing employs proportionality analysis to resolve conflicts\nbetween competing rights, while defeasible reasoning accommodates the dynamic\nnature of legal decision-making. Our approach stresses the need for an analysis\nof AI deployment scenarios and for identifying potential legal violations and\nmulti-layered impacts on fundamental rights. On the basis of this analysis, we\nprovide philosophical foundations for a logical account of AI risk analysis. In\nparticular, we consider the basic building blocks for conceptually grasping the\ninteraction between AI deployment scenarios and fundamental rights,\nincorporating in defeasible reasoning definitional balancing and arguments\nabout the contextual promotion or demotion of rights. This layered approach\nallows for more operative models of assessment of both high-risk AI systems and\nGeneral Purpose AI (GPAI) systems, emphasizing the broader applicability of the\nlatter. Future work aims to develop a formal model and effective algorithms to\nenhance AI risk assessment, bridging theoretical insights with practical\napplications to support responsible AI governance.", "AI": {"tldr": "本文提出了一个概念框架，用于对AI进行定性风险评估，尤其针对欧盟AI法案，通过整合定义平衡和可废止推理来处理法律合规性和基本权利保护的复杂性。", "motivation": "AI风险评估，尤其是在法律合规（如欧盟AI法案）和基本权利保护方面，存在复杂性。需要一个能够处理权利冲突、法律动态性和多层影响的框架。", "method": "该研究引入了一个概念框架，整合了“定义平衡”（通过比例原则分析解决权利冲突）和“可废止推理”（适应法律决策的动态性）。它强调分析AI部署场景，识别潜在法律违规和对基本权利的多层影响，并构建了AI风险分析的逻辑基础，考虑了权利在不同情境下的促进或削弱。", "result": "该框架为AI风险分析提供了一个逻辑账户的哲学基础，特别是在AI部署场景与基本权利交互方面。这种分层方法能够更有效地评估高风险AI系统和通用AI（GPAI）系统，并强调了后者更广泛的适用性。", "conclusion": "该概念框架为AI风险评估，特别是与欧盟AI法案相关的评估，提供了有力的工具。未来的工作将致力于开发形式化模型和有效算法，以弥合理论与实践之间的差距，支持负责任的AI治理。"}}
{"id": "2507.18076", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.18076", "abs": "https://arxiv.org/abs/2507.18076", "authors": ["Haomin Qi", "Zihan Dai", "Chengbo Huang"], "title": "Hybrid and Unitary Fine-Tuning of Large Language Models: Methods and Benchmarking under Resource Constraints", "comment": "10 pages, 2 figures and 1 table", "summary": "Fine-tuning large language models (LLMs) remains a computational bottleneck\ndue to their scale and memory demands. This paper presents a comprehensive\nevaluation of parameter-efficient fine-tuning (PEFT) techniques, including\nLoRA, BOFT, LoRA-GA, and uRNN, and introduces a novel hybrid strategy that\ndynamically integrates BOFT's orthogonal stability with LoRA-GA's\ngradient-aligned rapid convergence. By computing per-layer adaptive updates\nguided by gradient norms, the hybrid method achieves superior convergence\nefficiency and generalization across diverse tasks. We also explore, for the\nfirst time, the adaptation of unitary RNN (uRNN) principles to\ntransformer-based LLMs, enhancing gradient stability through structured unitary\nconstraints. Empirical evaluations on four benchmarks -- GLUE, GSM8K, MT-Bench,\nand HumanEval -- using models ranging from 7B to 405B parameters demonstrate\nthat our hybrid method consistently outperforms individual PEFT baselines,\napproaching full fine-tuning accuracy while reducing resource consumption by up\nto 2.1 times in training time and 50 percent in memory usage. These findings\nestablish the hybrid approach as a practical and scalable fine-tuning solution\nfor real-world deployment of LLMs under resource constraints.", "AI": {"tldr": "本文评估了多种参数高效微调（PEFT）技术，并提出了一种结合BOFT和LoRA-GA优点的新型混合策略，该策略通过梯度范数引导的自适应更新和引入uRNN原理，显著提升了LLM微调的效率和性能，同时大幅降低了资源消耗。", "motivation": "由于大型语言模型（LLMs）的规模和内存需求，对其进行微调仍然是一个计算瓶颈。", "method": "本文全面评估了包括LoRA、BOFT、LoRA-GA和uRNN在内的PEFT技术。在此基础上，引入了一种新型混合策略，该策略动态融合了BOFT的正交稳定性和LoRA-GA的梯度对齐快速收敛特性。通过梯度范数计算每层自适应更新，并首次探索将酉RNN（uRNN）原理应用于Transformer-based LLMs，以增强梯度稳定性。", "result": "该混合方法在GLUE、GSM8K、MT-Bench和HumanEval四个基准测试上，使用7B至405B参数的模型进行实证评估，结果表明其在不同任务上实现了卓越的收敛效率和泛化能力。它持续优于单独的PEFT基线，接近全量微调的准确性，同时将训练时间减少高达2.1倍，内存使用减少50%。", "conclusion": "研究结果确立了所提出的混合方法作为在资源受限下实际部署LLM的一种实用且可扩展的微调解决方案。"}}
{"id": "2507.18015", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18015", "abs": "https://arxiv.org/abs/2507.18015", "authors": ["Yuezun Li", "Delong Zhu", "Xinjie Cui", "Siwei Lyu"], "title": "Celeb-DF++: A Large-scale Challenging Video DeepFake Benchmark for Generalizable Forensics", "comment": "https://github.com/OUC-VAS/Celeb-DF-PP", "summary": "The rapid advancement of AI technologies has significantly increased the\ndiversity of DeepFake videos circulating online, posing a pressing challenge\nfor \\textit{generalizable forensics}, \\ie, detecting a wide range of unseen\nDeepFake types using a single model. Addressing this challenge requires\ndatasets that are not only large-scale but also rich in forgery diversity.\nHowever, most existing datasets, despite their scale, include only a limited\nvariety of forgery types, making them insufficient for developing generalizable\ndetection methods. Therefore, we build upon our earlier Celeb-DF dataset and\nintroduce {Celeb-DF++}, a new large-scale and challenging video DeepFake\nbenchmark dedicated to the generalizable forensics challenge. Celeb-DF++ covers\nthree commonly encountered forgery scenarios: Face-swap (FS), Face-reenactment\n(FR), and Talking-face (TF). Each scenario contains a substantial number of\nhigh-quality forged videos, generated using a total of 22 various recent\nDeepFake methods. These methods differ in terms of architectures, generation\npipelines, and targeted facial regions, covering the most prevalent DeepFake\ncases witnessed in the wild. We also introduce evaluation protocols for\nmeasuring the generalizability of 24 recent detection methods, highlighting the\nlimitations of existing detection methods and the difficulty of our new\ndataset.", "AI": {"tldr": "本文介绍了Celeb-DF++，一个用于可泛化DeepFake检测的新的大规模、多样化视频数据集，涵盖换脸、面部重演和说话脸三种伪造场景，使用22种方法生成，并评估了24种现有检测方法的泛化能力。", "motivation": "AI技术快速发展导致DeepFake视频多样性增加，对“可泛化取证”（即使用单一模型检测广泛的、未见过的DeepFake类型）提出了紧迫挑战。现有数据集尽管规模大，但伪造类型有限，不足以开发可泛化检测方法。", "method": "在Celeb-DF数据集基础上，构建了Celeb-DF++。该数据集包含换脸（FS）、面部重演（FR）和说话脸（TF）三种常见伪造场景，使用22种不同的最新DeepFake方法生成了大量高质量伪造视频。同时，引入了评估协议来衡量24种最新检测方法的泛化能力。", "result": "Celeb-DF++是一个大规模且具有挑战性的DeepFake基准数据集，涵盖了野外最常见的DeepFake案例。对现有24种检测方法的评估显示，它们在泛化能力上存在局限性，并突显了新数据集的难度。", "conclusion": "Celeb-DF++的引入旨在解决DeepFake的可泛化取证挑战，通过提供多样化的数据和评估协议，揭示了现有检测方法的不足，为未来更鲁棒的DeepFake检测方法研究奠定了基础。"}}
{"id": "2507.18493", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.18493", "abs": "https://arxiv.org/abs/2507.18493", "authors": ["Changwu Liu", "Yuan Shen"], "title": "Global Observer Design for a Class of Linear Observed Systems on Groups", "comment": "16 pages, 1 figure", "summary": "Linear observed systems on groups encode the geometry of a variety of\npractical state estimation problems. In this paper, we propose a unified\nobserver framework for a class of linear observed systems by restricting a\nbi-invariant system on a Lie group to its normal subgroup. This structural\nproperty powerfully enables a system immersion of the original system into a\nlinear time-varying system. Leveraging the immersion, an observer is\nconstructed by first designing a Kalman-like observer for the immersed system\nand then reconstructing the group-valued state via optimization. Under a rank\ncondition, global exponential stability (GES) is achieved provided one global\noptimum of the reconstruction optimization is found, reflecting the topological\ndifficulties inherent to the non-Euclidean state space. Semi-global stability\nis guaranteed when input biases are jointly estimated. The theory is applied to\nthe GES observer design for two-frame systems, capable of modeling a family of\nnavigation problems. Two non-trivial examples are provided to illustrate\nimplementation details.", "AI": {"tldr": "本文提出了一种针对群上线性观测系统设计的统一观测器框架，通过系统浸入和优化重构实现状态估计，并在特定条件下达到全局指数稳定性。", "motivation": "许多实际状态估计问题（如导航）的几何结构可以通过群上的线性观测系统来描述，需要一个统一的观测器框架来解决这类问题。", "method": "该方法将Lie群上的双不变系统限制在其正规子群上，从而将原始系统浸入到一个线性时变系统中。然后，为浸入系统设计一个类卡尔曼观测器，并通过优化重构群值状态。为保证半全局稳定性，还联合估计输入偏差。", "result": "在满足秩条件且找到重构优化问题的全局最优解时，实现了全局指数稳定性（GES）。当同时估计输入偏差时，保证了半全局稳定性。该理论成功应用于双帧系统（可建模一系列导航问题）的GES观测器设计。", "conclusion": "本文提出了一种有效的统一观测器框架，适用于群上的线性观测系统，通过系统浸入和优化重构，在特定条件下实现了全局指数稳定性，并通过偏差估计提升了稳定性，对导航等实际问题具有重要应用价值。"}}
{"id": "2507.18462", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.18462", "abs": "https://arxiv.org/abs/2507.18462", "authors": ["Alghalya Al-Hajri", "Ejmen Al-Ubejdij", "Aiman Erbad", "Ali Safa"], "title": "A Novel Monte-Carlo Compressed Sensing and Dictionary Learning Method for the Efficient Path Planning of Remote Sensing Robots", "comment": null, "summary": "In recent years, Compressed Sensing (CS) has gained significant interest as a\ntechnique for acquiring high-resolution sensory data using fewer measurements\nthan traditional Nyquist sampling requires. At the same time, autonomous\nrobotic platforms such as drones and rovers have become increasingly popular\ntools for remote sensing and environmental monitoring tasks, including\nmeasurements of temperature, humidity, and air quality. Within this context,\nthis paper presents, to the best of our knowledge, the first investigation into\nhow the structure of CS measurement matrices can be exploited to design\noptimized sampling trajectories for robotic environmental data collection. We\npropose a novel Monte Carlo optimization framework that generates measurement\nmatrices designed to minimize both the robot's traversal path length and the\nsignal reconstruction error within the CS framework. Central to our approach is\nthe application of Dictionary Learning (DL) to obtain a data-driven sparsifying\ntransform, which enhances reconstruction accuracy while further reducing the\nnumber of samples that the robot needs to collect. We demonstrate the\neffectiveness of our method through experiments reconstructing $NO_2$ pollution\nmaps over the Gulf region. The results indicate that our approach can reduce\nrobot travel distance to less than $10\\%$ of a full-coverage path, while\nimproving reconstruction accuracy by over a factor of five compared to\ntraditional CS methods based on DCT and polynomial dictionaries, as well as by\na factor of two compared to previously-proposed Informative Path Planning (IPP)\nmethods.", "AI": {"tldr": "本文提出一种新颖的蒙特卡洛优化框架，结合字典学习，为机器人环境数据采集设计优化的压缩感知采样轨迹，旨在最小化机器人行程并提高信号重建精度。", "motivation": "近年来，压缩感知（CS）因其能用更少测量获取高分辨率数据而备受关注，同时自主机器人平台在远程传感和环境监测中日益普及。研究的动机在于探索如何利用CS测量矩阵的结构来设计优化的机器人采样轨迹，以同时减少机器人路径长度和信号重建误差。", "method": "本文提出一个蒙特卡洛优化框架，生成测量矩阵以最小化机器人遍历路径长度和信号重建误差。核心方法是应用字典学习（DL）来获取数据驱动的稀疏变换，以提高重建精度并进一步减少机器人所需采集的样本数量。", "result": "实验表明，该方法能将机器人行程减少到全覆盖路径的不到10%，同时重建精度比基于DCT和多项式字典的传统CS方法提高五倍以上，比现有信息路径规划（IPP）方法提高两倍。效果在海湾地区的$NO_2$污染地图重建中得到验证。", "conclusion": "所提出的方法成功地利用压缩感知测量矩阵结构，通过蒙特卡洛优化和字典学习，显著优化了机器人环境数据采集的采样轨迹，大幅减少了机器人行程并显著提高了数据重建精度。"}}
{"id": "2507.18337", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18337", "abs": "https://arxiv.org/abs/2507.18337", "authors": ["Peter Baumgartner", "Lachlan McGinness"], "title": "The AlphaPhysics Term Rewriting System for Marking Algebraic Expressions in Physics Exams", "comment": null, "summary": "We present our method for automatically marking Physics exams. The marking\nproblem consists in assessing typed student answers for correctness with\nrespect to a ground truth solution. This is a challenging problem that we seek\nto tackle using a combination of a computer algebra system, an SMT solver and a\nterm rewriting system. A Large Language Model is used to interpret and remove\nerrors from student responses and rewrite these in a machine readable format.\nOnce formalized and language-aligned, the next step then consists in applying\nautomated reasoning techniques for assessing student solution correctness. We\nconsider two methods of automated theorem proving: off-the-shelf SMT solving\nand term rewriting systems tailored for physics problems involving\ntrigonometric expressions. The development of the term rewrite system and\nestablishing termination and confluence properties was not trivial, and we\ndescribe it in some detail in the paper. We evaluate our system on a rich pool\nof over 1500 real-world student exam responses from the 2023 Australian Physics\nOlympiad.", "AI": {"tldr": "提出一种结合大语言模型（LLM）、计算机代数系统（CAS）、SMT求解器和项重写系统来自动批改物理考试答案的方法。", "motivation": "自动评估学生输入的物理答案的正确性是一个具有挑战性的问题，需要一种有效且准确的解决方案。", "method": "利用LLM解释学生答案、纠正错误并将其转换为机器可读格式；结合CAS、SMT求解器和定制的项重写系统（TRS）进行答案正确性评估；采用两种自动定理证明方法：现成的SMT求解和专门针对物理问题（特别是三角表达式）开发的TRS；详细阐述了TRS的开发及其终止性和合流性属性。", "result": "该系统已在超过1500份来自2023年澳大利亚物理奥林匹克竞赛的真实学生答卷上进行了评估。", "conclusion": "成功开发并验证了一种混合方法，利用LLM进行自然语言处理，并结合形式化推理工具（CAS、SMT、TRS）实现物理考试答案的自动化批改。"}}
{"id": "2507.18103", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18103", "abs": "https://arxiv.org/abs/2507.18103", "authors": ["Riley Carlson", "John Bauer", "Christopher D. Manning"], "title": "A New Pair of GloVes", "comment": null, "summary": "This report documents, describes, and evaluates new 2024 English GloVe\n(Global Vectors for Word Representation) models. While the original GloVe\nmodels built in 2014 have been widely used and found useful, languages and the\nworld continue to evolve and we thought that current usage could benefit from\nupdated models. Moreover, the 2014 models were not carefully documented as to\nthe exact data versions and preprocessing that were used, and we rectify this\nby documenting these new models. We trained two sets of word embeddings using\nWikipedia, Gigaword, and a subset of Dolma. Evaluation through vocabulary\ncomparison, direct testing, and NER tasks shows that the 2024 vectors\nincorporate new culturally and linguistically relevant words, perform\ncomparably on structural tasks like analogy and similarity, and demonstrate\nimproved performance on recent, temporally dependent NER datasets such as\nnon-Western newswire data.", "AI": {"tldr": "该报告介绍了2024年更新的GloVe词向量模型，这些模型通过使用新数据进行训练并详细记录，在捕获新词汇和提高最新命名实体识别（NER）任务性能方面表现出色。", "motivation": "2014年的原始GloVe模型已广泛使用但已过时，语言和世界持续演变，现有模型未能充分反映当前语言使用。此外，原始模型缺乏详细的数据和预处理文档。", "method": "使用Wikipedia、Gigaword和Dolma数据集的子集训练了两组新的词向量。评估通过词汇比较、直接测试（如类比和相似性）以及命名实体识别（NER）任务进行。", "result": "2024年GloVe向量纳入了新的文化和语言相关词汇，在类比和相似性等结构性任务上表现相当，并在最近、时间依赖的NER数据集（如非西方新闻数据）上表现出改进的性能。", "conclusion": "更新的2024年GloVe模型通过吸收新的语言和文化信息，并在关键NLP任务上展示出有竞争力的或改进的性能，证明了其价值，同时解决了原始模型文档不足的问题。"}}
{"id": "2507.18023", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18023", "abs": "https://arxiv.org/abs/2507.18023", "authors": ["Jun Zhou", "Dinghao Li", "Nannan Li", "Mingjie Wang"], "title": "High-fidelity 3D Gaussian Inpainting: preserving multi-view consistency and photorealistic details", "comment": null, "summary": "Recent advancements in multi-view 3D reconstruction and novel-view synthesis,\nparticularly through Neural Radiance Fields (NeRF) and 3D Gaussian Splatting\n(3DGS), have greatly enhanced the fidelity and efficiency of 3D content\ncreation. However, inpainting 3D scenes remains a challenging task due to the\ninherent irregularity of 3D structures and the critical need for maintaining\nmulti-view consistency. In this work, we propose a novel 3D Gaussian inpainting\nframework that reconstructs complete 3D scenes by leveraging sparse inpainted\nviews. Our framework incorporates an automatic Mask Refinement Process and\nregion-wise Uncertainty-guided Optimization. Specifically, we refine the\ninpainting mask using a series of operations, including Gaussian scene\nfiltering and back-projection, enabling more accurate localization of occluded\nregions and realistic boundary restoration. Furthermore, our Uncertainty-guided\nFine-grained Optimization strategy, which estimates the importance of each\nregion across multi-view images during training, alleviates multi-view\ninconsistencies and enhances the fidelity of fine details in the inpainted\nresults. Comprehensive experiments conducted on diverse datasets demonstrate\nthat our approach outperforms existing state-of-the-art methods in both visual\nquality and view consistency.", "AI": {"tldr": "本文提出了一种新颖的3D高斯修复框架，利用稀疏修复视图重建完整3D场景，并通过自动掩码细化和区域不确定性引导优化提升修复质量和视图一致性。", "motivation": "尽管NeRF和3DGS等技术在3D重建和新视图合成方面取得了显著进展，但3D场景修复仍具挑战性，主要原因在于3D结构固有的不规则性以及保持多视图一致性的关键需求。", "method": "该框架通过以下方法实现：1) 自动掩码细化过程，利用高斯场景滤波和反向投影操作，实现更精确的遮挡区域定位和逼真的边界恢复。2) 区域不确定性引导的精细优化策略，在训练期间估计多视图图像中每个区域的重要性，以缓解多视图不一致性并增强修复结果的细节保真度。", "result": "在多样化数据集上进行的综合实验表明，该方法在视觉质量和视图一致性方面均优于现有最先进的方法。", "conclusion": "所提出的3D高斯修复框架能够有效重建完整的3D场景，并在修复质量和多视图一致性方面表现出色。"}}
{"id": "2507.18630", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.18630", "abs": "https://arxiv.org/abs/2507.18630", "authors": ["Junbin Zhong", "Mingtong Chen", "Zhengbao Yang"], "title": "Design and optimization of a novel leaf-shape antenna for RF energy transfer", "comment": null, "summary": "In this research, the design and optimization of a novel leaf-shaped antenna\ninspired by natural leaf structures for radio frequency energy transfer is\npresented. The objectives of this study are to develop a bio-inspired antenna,\noptimize its performance through impedance matching for the 915 MHz frequency\nband, and evaluate its efficiency in capturing RF energy. The design process\ninvolves selecting an appropriate leaf shape, modeling the antenna using\nAutoCAD and HFSS software, and fabricating a printed circuit board (PCB)\nprototype. Simulations and physical tests are conducted to optimize the\nantennas performance, achieving an S11 parameter of nearly -20 dB at 915 MHz,\nindicating effective energy capture. Experimental results demonstrate the\nantennas ability to power a device at distances up to 200 cm, with charging\ntimes reflecting its efficiency. The study concludes that the bio-inspired\ndesign of the proposed antenna improves RF energy transfer. Future work should\nfocus on testing the antennas penetration through concrete and developing a\nfeedback system for autonomous alignment.", "AI": {"tldr": "本研究设计并优化了一种受自然叶片结构启发的射频能量传输叶形天线，该天线在915 MHz频段表现出高效的能量捕获能力，并能为设备在200厘米距离内供电。", "motivation": "开发一种仿生天线，优化其在915 MHz频段的性能以实现阻抗匹配，并评估其捕获射频能量的效率，以改进射频能量传输。", "method": "设计过程包括选择合适的叶片形状，使用AutoCAD和HFSS软件建模天线，并制作印刷电路板（PCB）原型。通过仿真和物理测试来优化天线性能。", "result": "天线在915 MHz频率下实现了接近-20 dB的S11参数，表明有效的能量捕获。实验结果证明天线能够在最远200厘米的距离内为设备供电，充电时间反映了其效率。", "conclusion": "该研究得出结论，所提出的仿生天线设计提高了射频能量传输效率。未来的工作应侧重于测试天线穿透混凝土的能力以及开发自主对准的反馈系统。"}}
{"id": "2507.18502", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.18502", "abs": "https://arxiv.org/abs/2507.18502", "authors": ["Sait Sovukluk", "Grazia Zambella", "Tobias Egle", "Christian Ott"], "title": "Experimental Comparison of Whole-Body Control Formulations for Humanoid Robots in Task Acceleration and Task Force Spaces", "comment": "This paper has been accepted for publication in 2025 IEEE/RSJ\n  International Conference on Intelligent Robots and Systems (IROS 2025). -\n  Link to video: https://youtu.be/Nfm50ycz-FU", "summary": "This paper studies the experimental comparison of two different whole-body\ncontrol formulations for humanoid robots: inverse dynamics whole-body control\n(ID-WBC) and passivity-based whole-body control (PB-WBC). The two controllers\nfundamentally differ from each other as the first is formulated in task\nacceleration space and the latter is in task force space with passivity\nconsiderations. Even though both control methods predict stability under ideal\nconditions in closed-loop dynamics, their robustness against joint friction,\nsensor noise, unmodeled external disturbances, and non-perfect contact\nconditions is not evident. Therefore, we analyze and experimentally compare the\ntwo controllers on a humanoid robot platform through swing foot position and\norientation control, squatting with and without unmodeled additional weights,\nand jumping. We also relate the observed performance and characteristic\ndifferences with the controller formulations and highlight each controller's\nadvantages and disadvantages.", "AI": {"tldr": "本文通过实验比较了两种不同的人形机器人全身控制方法：逆动力学全身控制（ID-WBC）和基于无源性的全身控制（PB-WBC），分析它们在实际条件下的鲁棒性。", "motivation": "尽管ID-WBC和PB-WBC在理想条件下都能预测闭环动力学稳定性，但它们对关节摩擦、传感器噪声、未建模外部扰动和非完美接触等实际因素的鲁棒性尚不明确。", "method": "在人形机器人平台上，通过摆动脚位姿控制、负重深蹲（有无额外重量）和跳跃等实验，对ID-WBC和PB-WBC两种控制器进行分析和比较。同时，将观察到的性能和特性差异与控制器公式关联起来。", "result": "通过实验，本文分析并比较了两种控制器在不同任务下的性能和特性差异，并指出了每种控制器的优缺点。", "conclusion": "根据实验观察到的性能和特性差异，本文总结了ID-WBC和PB-WBC两种控制器的优缺点，并将其与各自的公式联系起来。"}}
{"id": "2507.18368", "categories": ["cs.AI", "I.2.0; I.2.6; J.4"], "pdf": "https://arxiv.org/pdf/2507.18368", "abs": "https://arxiv.org/abs/2507.18368", "authors": ["Zhuang Qiang Bok", "Watson Wei Khong Chua"], "title": "Reasoning Beyond the Obvious: Evaluating Divergent and Convergent Thinking in LLMs for Financial Scenarios", "comment": "Accepted by Agentic & GenAI Evaluation KDD2025: KDD workshop on\n  Evaluation and Trustworthiness of Agentic and Generative AI Models\n  https://kdd-eval-workshop.github.io/genai-evaluation-kdd2025/", "summary": "Most reasoning benchmarks for LLMs emphasize factual accuracy or step-by-step\nlogic. In finance, however, professionals must not only converge on optimal\ndecisions but also generate creative, plausible futures under uncertainty. We\nintroduce ConDiFi, a benchmark that jointly evaluates divergent and convergent\nthinking in LLMs for financial tasks.\n  ConDiFi features 607 macro-financial prompts for divergent reasoning and 990\nmulti-hop adversarial MCQs for convergent reasoning. Using this benchmark, we\nevaluated 14 leading models and uncovered striking differences. Despite high\nfluency, GPT-4o underperforms on Novelty and Actionability. In contrast, models\nlike DeepSeek-R1 and Cohere Command R+ rank among the top for generating\nactionable, insights suitable for investment decisions. ConDiFi provides a new\nperspective to assess reasoning capabilities essential to safe and strategic\ndeployment of LLMs in finance.", "AI": {"tldr": "该论文引入了ConDiFi基准，用于同时评估大型语言模型（LLMs）在金融任务中的发散性（创造性）和收敛性（决策）思维能力，并揭示了主流模型在此方面的显著差异。", "motivation": "现有LLM推理基准主要侧重于事实准确性或循序渐进的逻辑，但在金融领域，专业人士不仅需要做出最优决策，还需要在不确定性下生成有创意、合理且可行的未来设想。因此，需要一个能同时评估这两种思维能力的基准。", "method": "研究者创建了ConDiFi基准，其中包含607个用于发散性推理的宏观金融提示和990个用于收敛性推理的多跳对抗性多项选择题。利用此基准，他们评估了14个主流LLM模型。", "result": "评估结果显示，尽管GPT-4o具有高流畅性，但在新颖性和可操作性方面表现不佳。相比之下，DeepSeek-R1和Cohere Command R+等模型在生成适合投资决策的可操作性见解方面表现优异。", "conclusion": "ConDiFi基准为评估LLM在金融领域安全和战略部署所需的关键推理能力提供了新的视角和工具。"}}
{"id": "2507.18119", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.18119", "abs": "https://arxiv.org/abs/2507.18119", "authors": ["Hongjie Chen", "Zehan Li", "Yaodong Song", "Wenming Deng", "Yitong Yao", "Yuxin Zhang", "Hang Lv", "Xuechao Zhu", "Jian Kang", "Jie Lian", "Jie Li", "Chao Wang", "Shuangyong Song", "Yongxiang Li", "Zhongjiang He"], "title": "GOAT-SLM: A Spoken Language Model with Paralinguistic and Speaker Characteristic Awareness", "comment": null, "summary": "Recent advances in end-to-end spoken language models (SLMs) have\nsignificantly improved the ability of AI systems to engage in natural spoken\ninteractions. However, most existing models treat speech merely as a vehicle\nfor linguistic content, often overlooking the rich paralinguistic and speaker\ncharacteristic cues embedded in human speech, such as dialect, age, emotion,\nand non-speech vocalizations. In this work, we introduce GOAT-SLM, a novel\nspoken language model with paralinguistic and speaker characteristic awareness,\ndesigned to extend spoken language modeling beyond text semantics. GOAT-SLM\nadopts a dual-modality head architecture that decouples linguistic modeling\nfrom acoustic realization, enabling robust language understanding while\nsupporting expressive and adaptive speech generation. To enhance model\nefficiency and versatility, we propose a modular, staged training strategy that\nprogressively aligns linguistic, paralinguistic, and speaker characteristic\ninformation using large-scale speech-text corpora. Experimental results on\nTELEVAL, a multi-dimensional evaluation benchmark, demonstrate that GOAT-SLM\nachieves well-balanced performance across both semantic and non-semantic tasks,\nand outperforms existing open-source models in handling emotion, dialectal\nvariation, and age-sensitive interactions. This work highlights the importance\nof modeling beyond linguistic content and advances the development of more\nnatural, adaptive, and socially aware spoken language systems.", "AI": {"tldr": "本文提出了GOAT-SLM，一个具有副语言和说话者特征感知的语音语言模型，旨在超越文本语义，实现更自然、自适应的语音交互。", "motivation": "现有语音语言模型（SLMs）主要关注语言内容，忽视了人类语音中丰富的副语言和说话者特征线索（如方言、年龄、情感和非言语发声），导致AI系统在自然语音交互中表现不足。", "method": "GOAT-SLM采用双模态头部架构，将语言建模与声学实现解耦，以实现鲁棒的语言理解和富有表现力的自适应语音生成。此外，它提出了一种模块化、分阶段的训练策略，利用大规模语音-文本语料库逐步对齐语言、副语言和说话者特征信息。", "result": "在多维度评估基准TELEVAL上的实验结果表明，GOAT-SLM在语义和非语义任务上均取得了良好平衡的性能，并在处理情感、方言变异和年龄敏感交互方面优于现有开源模型。", "conclusion": "这项工作强调了超越语言内容进行建模的重要性，并推动了更自然、自适应和具有社会意识的语音语言系统的发展。"}}
{"id": "2507.18026", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18026", "abs": "https://arxiv.org/abs/2507.18026", "authors": ["Haifeng Lu", "Jiuyi Chen", "Zhen Zhang", "Ruida Liu", "Runhao Zeng", "Xiping Hu"], "title": "Emotion Recognition from Skeleton Data: A Comprehensive Survey", "comment": "34 pages, 5 figures, 13 tables", "summary": "Emotion recognition through body movements has emerged as a compelling and\nprivacy-preserving alternative to traditional methods that rely on facial\nexpressions or physiological signals. Recent advancements in 3D skeleton\nacquisition technologies and pose estimation algorithms have significantly\nenhanced the feasibility of emotion recognition based on full-body motion. This\nsurvey provides a comprehensive and systematic review of skeleton-based emotion\nrecognition techniques. First, we introduce psychological models of emotion and\nexamine the relationship between bodily movements and emotional expression.\nNext, we summarize publicly available datasets, highlighting the differences in\ndata acquisition methods and emotion labeling strategies. We then categorize\nexisting methods into posture-based and gait-based approaches, analyzing them\nfrom both data-driven and technical perspectives. In particular, we propose a\nunified taxonomy that encompasses four primary technical paradigms: Traditional\napproaches, Feat2Net, FeatFusionNet, and End2EndNet. Representative works\nwithin each category are reviewed and compared, with benchmarking results\nacross commonly used datasets. Finally, we explore the extended applications of\nemotion recognition in mental health assessment, such as detecting depression\nand autism, and discuss the open challenges and future research directions in\nthis rapidly evolving field.", "AI": {"tldr": "该综述全面系统地回顾了基于骨架的肢体动作情感识别技术，涵盖心理学模型、数据集、方法分类、技术范式（Traditional, Feat2Net, FeatFusionNet, End2EndNet），以及在心理健康评估中的应用和未来挑战。", "motivation": "传统的情感识别方法依赖面部表情或生理信号，存在隐私问题。3D骨架采集和姿态估计技术的进步，使得通过全身动作进行情感识别成为一种有前景且保护隐私的替代方案。", "method": "首先介绍情感的心理学模型及肢体动作与情感表达的关系；其次总结公开数据集及其采集与标注策略；然后将现有方法分为基于姿态和基于步态的方法，并从数据驱动和技术角度进行分析；提出一个统一的技术分类范式，包括传统方法、Feat2Net、FeatFusionNet和End2EndNet；回顾并比较各类别代表性工作及其基准测试结果；最后探讨其在心理健康评估中的应用。", "result": "提供了一个对基于骨架的情感识别技术的全面系统回顾，提出了一种统一的技术分类范式，并比较了各方法的性能。揭示了该技术在精神健康评估（如抑郁症和自闭症检测）中的应用潜力。", "conclusion": "基于骨架的肢体动作情感识别是一个快速发展的领域，具有扩展应用前景，尤其是在心理健康评估方面。该领域仍面临挑战，未来研究方向广阔。"}}
{"id": "2507.18444", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.18444", "abs": "https://arxiv.org/abs/2507.18444", "authors": ["Haiyang Jiang", "Songhao Piao", "Chao Gao", "Lei Yu", "Liguo Chen"], "title": "DSFormer: A Dual-Scale Cross-Learning Transformer for Visual Place Recognition", "comment": null, "summary": "Visual Place Recognition (VPR) is crucial for robust mobile robot\nlocalization, yet it faces significant challenges in maintaining reliable\nperformance under varying environmental conditions and viewpoints. To address\nthis, we propose a novel framework that integrates Dual-Scale-Former\n(DSFormer), a Transformer-based cross-learning module, with an innovative block\nclustering strategy. DSFormer enhances feature representation by enabling\nbidirectional information transfer between dual-scale features extracted from\nthe final two CNN layers, capturing both semantic richness and spatial details\nthrough self-attention for long-range dependencies within each scale and shared\ncross-attention for cross-scale learning. Complementing this, our block\nclustering strategy repartitions the widely used San Francisco eXtra Large\n(SF-XL) training dataset from multiple distinct perspectives, optimizing data\norganization to further bolster robustness against viewpoint variations.\nTogether, these innovations not only yield a robust global embedding adaptable\nto environmental changes but also reduce the required training data volume by\napproximately 30\\% compared to previous partitioning methods. Comprehensive\nexperiments demonstrate that our approach achieves state-of-the-art performance\nacross most benchmark datasets, surpassing advanced reranking methods like\nDELG, Patch-NetVLAD, TransVPR, and R2Former as a global retrieval solution\nusing 512-dim global descriptors, while significantly improving computational\nefficiency.", "AI": {"tldr": "该论文提出了一种结合双尺度Transformer (DSFormer) 和创新块聚类策略的视觉地点识别(VPR)框架，以应对环境和视角变化带来的挑战。该框架通过双尺度特征融合和优化的数据组织，实现了鲁棒的全局嵌入，显著减少了训练数据量，并在多个基准数据集上取得了最先进的性能和更高的计算效率。", "motivation": "视觉地点识别(VPR)对于移动机器人鲁棒定位至关重要，但在不同环境条件和视角下保持可靠性能面临巨大挑战。", "method": "本文提出一个新颖的框架，整合了：1. 双尺度Transformer (DSFormer)：一个基于Transformer的交叉学习模块，通过自注意力实现双尺度特征（来自最后两个CNN层）的内部长程依赖和共享交叉注意力实现跨尺度信息双向传输，以增强特征表示，捕获语义丰富性和空间细节。2. 创新块聚类策略：重新划分广泛使用的SF-XL训练数据集，从多个不同视角优化数据组织，进一步增强对视角变化的鲁棒性。", "result": "该方法不仅生成了对环境变化适应性强的鲁棒全局嵌入，而且与现有分区方法相比，将所需训练数据量减少了约30%。综合实验表明，该方法在大多数基准数据集上实现了最先进的性能，作为使用512维全局描述符的全局检索方案，超越了DELG、Patch-NetVLAD、TransVPR和R2Former等先进方法，并显著提高了计算效率。", "conclusion": "所提出的集成DSFormer和块聚类策略的VPR框架，有效解决了环境和视角变化带来的挑战，实现了鲁棒、高效且性能领先的视觉地点识别解决方案。"}}
{"id": "2507.18391", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18391", "abs": "https://arxiv.org/abs/2507.18391", "authors": ["Shiye Lei", "Zhihao Cheng", "Kai Jia", "Dacheng Tao"], "title": "Revisiting LLM Reasoning via Information Bottleneck", "comment": null, "summary": "Large language models (LLMs) have recently demonstrated remarkable progress\nin reasoning capabilities through reinforcement learning with verifiable\nrewards (RLVR). By leveraging simple rule-based rewards, RL effectively\nincentivizes LLMs to produce extended chain-of-thought (CoT) reasoning\ntrajectories, progressively guiding them toward correct answers. However,\nexisting approaches remain largely heuristic and intuition-driven, limiting the\ndevelopment of principled methodologies. In this paper, we present a\ntheoretical characterization of LLM reasoning grounded in information\nbottleneck (IB) principle, introducing IB-aware reasoning optimization (IBRO),\na framework that encourages reasoning trajectories to be both informative about\nthe final correct answer and generalizable across diverse prompts. We derive a\npractical token-level surrogate objective and propose an efficient\napproximation, resulting in the lightweight IB regularization method. This\ntechnique integrates seamlessly into existing RL-based post-training frameworks\nwithout additional computational overhead, requiring only a one-line code\nmodification. Empirically, we validate IB regularization across multiple\nmathematical reasoning benchmarks and RL algorithms, demonstrating consistent\nimprovements in LLM reasoning performance.", "AI": {"tldr": "通过信息瓶颈（IB）原理，提出IBRO框架和轻量级IB正则化方法，以理论化方式优化LLM推理轨迹，使其更具信息量和泛化性，从而在数学推理任务中提升LLM性能。", "motivation": "现有基于强化学习（RLVR）的LLM推理方法多为启发式且凭直觉，缺乏系统性理论指导，限制了原则性方法的发展。", "method": "提出基于信息瓶颈（IB）原理的LLM推理理论表征，并引入IB-aware推理优化（IBRO）框架，旨在使推理轨迹既能提供最终正确答案的充分信息，又能泛化到不同提示。推导出一个实用的token级替代目标，并提出高效近似，形成轻量级IB正则化方法，可无缝集成到现有RL后训练框架中，仅需一行代码修改。", "result": "在多个数学推理基准和RL算法上验证了IB正则化方法，展示了LLM推理性能的持续改进。", "conclusion": "IB正则化方法提供了一种理论上更严谨且高效的方式来优化LLM的推理能力，解决了现有方法的启发式局限性，并经验性地证明了其有效性。"}}
{"id": "2507.18140", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.18140", "abs": "https://arxiv.org/abs/2507.18140", "authors": ["Xiaoyuan Li", "Moxin Li", "Wenjie Wang", "Rui Men", "Yichang Zhang", "Fuli Feng", "Dayiheng Liu", "Junyang Lin"], "title": "MathOPEval: A Fine-grained Evaluation Benchmark for Visual Operations of MLLMs in Mathematical Reasoning", "comment": "Under Review", "summary": "Recent progress in Multi-modal Large Language Models (MLLMs) has enabled\nstep-by-step multi-modal mathematical reasoning by performing visual operations\nbased on the textual instructions. A promising approach uses code as an\nintermediate representation to precisely express and manipulate the images in\nthe reasoning steps. However, existing evaluations focus mainly on text-only\nreasoning outputs, leaving the MLLM's ability to perform accurate visual\noperations via code largely unexplored. This work takes a first step toward\naddressing that gap by evaluating MLLM's code-based capabilities in multi-modal\nmathematical reasoning.Specifically, our framework focuses on two key\nevaluation aspects: (1) Multi-modal Code Generation (MCG) evaluates the model's\nability to accurately understand and construct visualizations from scratch. (2)\nMulti-modal Code Editing (MCE) assesses the model's capacity for fine-grained\noperations, which include three types: Deletion, Modification and Annotation.\nTo evaluate the above tasks, we incorporate a dataset that covers the five most\npopular types of mathematical figures, including geometric diagrams, function\nplots, and three types of statistical charts, to provide a comprehensive and\neffective measurement of existing MLLMs. Our experimental evaluation involves\nnine mainstream MLLMs, and the results reveal that existing models still lag\nsignificantly behind human performance in performing fine-grained visual\noperations.", "AI": {"tldr": "该研究首次评估了多模态大语言模型（MLLMs）在多模态数学推理中基于代码执行视觉操作的能力，发现现有模型在精细视觉操作方面远低于人类水平。", "motivation": "现有对MLLMs的评估主要集中于纯文本推理输出，忽视了模型通过代码精确执行视觉操作的能力，这部分能力尚待充分探索。", "method": "该研究提出了一个评估框架，聚焦于两个关键方面：1) 多模态代码生成（MCG），评估模型从零开始理解和构建可视化的能力；2) 多模态代码编辑（MCE），评估模型进行精细操作（包括删除、修改和标注）的能力。为此，研究整合了一个涵盖五种流行数学图形（几何图、函数图和三种统计图表）的数据集，并对九个主流MLLMs进行了实验评估。", "result": "实验结果表明，现有MLLMs在执行精细视觉操作方面，与人类表现相比仍存在显著差距。", "conclusion": "当前的多模态大语言模型在通过代码进行精确和精细的视觉操作方面仍有很大的提升空间，远未达到人类水平。"}}
{"id": "2507.18031", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18031", "abs": "https://arxiv.org/abs/2507.18031", "authors": ["Ahmad ALBarqawi", "Mahmoud Nazzal", "Issa Khalil", "Abdallah Khreishah", "NhatHai Phan"], "title": "ViGText: Deepfake Image Detection with Vision-Language Model Explanations and Graph Neural Networks", "comment": null, "summary": "The rapid rise of deepfake technology, which produces realistic but\nfraudulent digital content, threatens the authenticity of media. Traditional\ndeepfake detection approaches often struggle with sophisticated, customized\ndeepfakes, especially in terms of generalization and robustness against\nmalicious attacks. This paper introduces ViGText, a novel approach that\nintegrates images with Vision Large Language Model (VLLM) Text explanations\nwithin a Graph-based framework to improve deepfake detection. The novelty of\nViGText lies in its integration of detailed explanations with visual data, as\nit provides a more context-aware analysis than captions, which often lack\nspecificity and fail to reveal subtle inconsistencies. ViGText systematically\ndivides images into patches, constructs image and text graphs, and integrates\nthem for analysis using Graph Neural Networks (GNNs) to identify deepfakes.\nThrough the use of multi-level feature extraction across spatial and frequency\ndomains, ViGText captures details that enhance its robustness and accuracy to\ndetect sophisticated deepfakes. Extensive experiments demonstrate that ViGText\nsignificantly enhances generalization and achieves a notable performance boost\nwhen it detects user-customized deepfakes. Specifically, average F1 scores rise\nfrom 72.45% to 98.32% under generalization evaluation, and reflects the model's\nsuperior ability to generalize to unseen, fine-tuned variations of stable\ndiffusion models. As for robustness, ViGText achieves an increase of 11.1% in\nrecall compared to other deepfake detection approaches. When facing targeted\nattacks that exploit its graph-based architecture, ViGText limits\nclassification performance degradation to less than 4%. ViGText uses detailed\nvisual and textual analysis to set a new standard for detecting deepfakes,\nhelping ensure media authenticity and information integrity.", "AI": {"tldr": "ViGText是一种新颖的深度伪造检测方法，它将图像与视觉大语言模型（VLLM）生成的文本解释集成到基于图的框架中，通过图神经网络（GNNs）进行分析，显著提高了对复杂定制深度伪造的泛化能力和鲁棒性。", "motivation": "深度伪造技术日益猖獗，传统检测方法在面对复杂、定制化的深度伪造时，在泛化能力和抵御恶意攻击的鲁棒性方面表现不足，威胁媒体真实性。", "method": "ViGText将图像分割成块，利用VLLM生成详细的文本解释，构建图像图和文本图，并通过图神经网络（GNNs）进行整合分析。该方法还通过跨空间和频率域的多级特征提取来捕获细节，以增强鲁棒性和准确性。", "result": "在泛化评估中，ViGText的平均F1分数从72.45%显著提高到98.32%，显示出对未见过的、微调的稳定扩散模型的卓越泛化能力。在鲁棒性方面，ViGText的召回率比其他方法提高了11.1%。即使面对针对其图基架构的定向攻击，分类性能下降也限制在4%以内。", "conclusion": "ViGText通过详细的视觉和文本分析，为深度伪造检测设定了新标准，有助于确保媒体真实性和信息完整性。"}}
{"id": "2507.18398", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18398", "abs": "https://arxiv.org/abs/2507.18398", "authors": ["Kwong Ho Li", "Wathsala Karunarathne"], "title": "Optimising Call Centre Operations using Reinforcement Learning: Value Iteration versus Proximal Policy Optimisation", "comment": "10 pages", "summary": "This paper investigates the application of Reinforcement Learning (RL) to\noptimise call routing in call centres to minimise client waiting time and staff\nidle time. Two methods are compared: a model-based approach using Value\nIteration (VI) under known system dynamics, and a model-free approach using\nProximal Policy Optimisation (PPO) that learns from experience. For the\nmodel-based approach, a theoretical model is used, while a simulation model\ncombining Discrete Event Simulation (DES) with the OpenAI Gym environment is\ndeveloped for model-free learning. Both models frame the problem as a Markov\nDecision Process (MDP) within a Skills-Based Routing (SBR) framework, with\nPoisson client arrivals and exponentially distributed service and abandonment\ntimes. For policy evaluation, random, VI, and PPO policies are evaluated using\nthe simulation model. After 1,000 test episodes, PPO consistently achives the\nhighest rewards, along with the lowest client waiting time and staff idle time,\ndespite requiring longer training time.", "AI": {"tldr": "本文研究了强化学习（RL）在呼叫中心呼叫路由优化中的应用，旨在最小化客户等待时间和员工空闲时间。对比了基于模型的价值迭代（VI）和无模型的近端策略优化（PPO），结果表明PPO性能最佳。", "motivation": "优化呼叫中心的呼叫路由策略，以有效降低客户等待时间并减少员工空闲时间。", "method": "将呼叫路由问题建模为技能型路由（SBR）框架下的马尔可夫决策过程（MDP），其中客户到达遵循泊松分布，服务和放弃时间遵循指数分布。对比了两种RL方法：1. 基于模型的方法：使用已知系统动态的价值迭代（VI），基于理论模型。2. 无模型的方法：使用近端策略优化（PPO），通过结合离散事件仿真（DES）和OpenAI Gym环境的仿真模型从经验中学习。通过仿真模型评估了随机策略、VI策略和PPO策略的性能。", "result": "经过1000个测试回合，PPO策略持续获得最高的回报，同时实现了最低的客户等待时间和最低的员工空闲时间，尽管它需要更长的训练时间。", "conclusion": "PPO是一种有效的呼叫中心呼叫路由优化方法，尽管训练时间较长，但在最小化客户等待时间和员工空闲时间方面优于传统的价值迭代和随机策略。"}}
{"id": "2507.18143", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18143", "abs": "https://arxiv.org/abs/2507.18143", "authors": ["Gonzalo Cardenal Antolin", "Jacques Fellay", "Bashkim Jaha", "Roger Kouyos", "Niko Beerenwinkel", "Diane Duroux"], "title": "HIVMedQA: Benchmarking large language models for HIV medical decision support", "comment": null, "summary": "Large language models (LLMs) are emerging as valuable tools to support\nclinicians in routine decision-making. HIV management is a compelling use case\ndue to its complexity, including diverse treatment options, comorbidities, and\nadherence challenges. However, integrating LLMs into clinical practice raises\nconcerns about accuracy, potential harm, and clinician acceptance. Despite\ntheir promise, AI applications in HIV care remain underexplored, and LLM\nbenchmarking studies are scarce. This study evaluates the current capabilities\nof LLMs in HIV management, highlighting their strengths and limitations. We\nintroduce HIVMedQA, a benchmark designed to assess open-ended medical question\nanswering in HIV care. The dataset consists of curated, clinically relevant\nquestions developed with input from an infectious disease physician. We\nevaluated seven general-purpose and three medically specialized LLMs, applying\nprompt engineering to enhance performance. Our evaluation framework\nincorporates both lexical similarity and an LLM-as-a-judge approach, extended\nto better reflect clinical relevance. We assessed performance across key\ndimensions: question comprehension, reasoning, knowledge recall, bias,\npotential harm, and factual accuracy. Results show that Gemini 2.5 Pro\nconsistently outperformed other models across most dimensions. Notably, two of\nthe top three models were proprietary. Performance declined as question\ncomplexity increased. Medically fine-tuned models did not always outperform\ngeneral-purpose ones, and larger model size was not a reliable predictor of\nperformance. Reasoning and comprehension were more challenging than factual\nrecall, and cognitive biases such as recency and status quo were observed.\nThese findings underscore the need for targeted development and evaluation to\nensure safe, effective LLM integration in clinical care.", "AI": {"tldr": "本研究评估了大型语言模型（LLMs）在艾滋病管理中的能力，引入了HIVMedQA基准数据集，发现Gemini 2.5 Pro表现最佳，但模型性能随问题复杂性下降，且存在偏见，强调了安全有效整合LLM的必要性。", "motivation": "LLMs在临床决策中具有潜力，尤其在复杂的艾滋病管理中。然而，将LLMs整合到临床实践中存在准确性、潜在危害和医护人员接受度方面的担忧。目前，AI在艾滋病护理中的应用尚待探索，且缺乏LLM基准研究。", "method": "研究引入了HIVMedQA，一个针对艾滋病护理开放式医学问答的基准数据集，由传染病医生协助策划。评估了7个通用型和3个医学专业型LLMs，并应用了提示工程。评估框架结合了词汇相似性和“LLM作为评判者”的方法，以更好地反映临床相关性。评估维度包括问题理解、推理、知识召回、偏见、潜在危害和事实准确性。", "result": "Gemini 2.5 Pro在大多数维度上持续优于其他模型，排名前三的模型中有两个是专有模型。模型性能随问题复杂性增加而下降。医学微调模型并非总优于通用模型，模型大小也不是性能的可靠预测指标。推理和理解比事实召回更具挑战性，并观察到近因效应和现状偏见等认知偏见。", "conclusion": "这些发现强调了需要有针对性的开发和评估，以确保LLMs在临床护理中的安全有效整合。"}}
{"id": "2507.18046", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18046", "abs": "https://arxiv.org/abs/2507.18046", "authors": ["Hanwen Shen", "Jiajie Lu", "Yupeng Cao", "Xiaonan Yang"], "title": "Enhancing Scene Transition Awareness in Video Generation via Post-Training", "comment": null, "summary": "Recent advances in AI-generated video have shown strong performance on\n\\emph{text-to-video} tasks, particularly for short clips depicting a single\nscene. However, current models struggle to generate longer videos with coherent\nscene transitions, primarily because they cannot infer when a transition is\nneeded from the prompt. Most open-source models are trained on datasets\nconsisting of single-scene video clips, which limits their capacity to learn\nand respond to prompts requiring multiple scenes. Developing scene transition\nawareness is essential for multi-scene generation, as it allows models to\nidentify and segment videos into distinct clips by accurately detecting\ntransitions.\n  To address this, we propose the \\textbf{Transition-Aware Video} (TAV)\ndataset, which consists of preprocessed video clips with multiple scene\ntransitions. Our experiment shows that post-training on the \\textbf{TAV}\ndataset improves prompt-based scene transition understanding, narrows the gap\nbetween required and generated scenes, and maintains image quality.", "AI": {"tldr": "当前AI视频生成模型难以处理多场景连贯过渡，本文提出TAV数据集，通过后训练显著提升模型对场景过渡的理解和生成质量。", "motivation": "现有AI视频生成模型在单场景短视频方面表现良好，但在生成具有连贯场景过渡的长视频时面临困难，主要原因是它们无法从提示词中推断出何时需要场景过渡。多数开源模型在单场景视频数据集上训练，限制了其学习和响应多场景提示的能力。", "method": "提出并构建了“过渡感知视频”（Transition-Aware Video, TAV）数据集，该数据集包含经过预处理的、具有多个场景过渡的视频片段，用于解决现有模型在场景过渡理解上的不足。", "result": "实验表明，在TAV数据集上进行后训练能够显著提升模型对基于提示词的场景过渡的理解能力，缩小了所需场景与生成场景之间的差距，并能保持图像质量。", "conclusion": "通过在TAV数据集上进行后训练，可以有效增强AI视频生成模型处理多场景连贯过渡的能力，从而克服现有模型在长视频生成中的局限性。"}}
{"id": "2507.18413", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18413", "abs": "https://arxiv.org/abs/2507.18413", "authors": ["Enrico Santi", "Fabio Tardivo", "Agostino Dovier", "Andrea Formisano"], "title": "GPU Accelerated Compact-Table Propagation", "comment": "Under consideration in Theory and Practice of Logic Programming\n  (TPLP)", "summary": "Constraint Programming developed within Logic Programming in the Eighties;\nnowadays all Prolog systems encompass modules capable of handling constraint\nprogramming on finite domains demanding their solution to a constraint solver.\nThis work focuses on a specific form of constraint, the so-called table\nconstraint, used to specify conditions on the values of variables as an\nenumeration of alternative options. Since every condition on a set of finite\ndomain variables can be ultimately expressed as a finite set of cases, Table\ncan, in principle, simulate any other constraint. These characteristics make\nTable one of the most studied constraints ever, leading to a series of\nincreasingly efficient propagation algorithms. Despite this, it is not uncommon\nto encounter real-world problems with hundreds or thousands of valid cases that\nare simply too many to be handled effectively with standard CPU-based\napproaches. In this paper, we deal with the Compact-Table (CT) algorithm, the\nstate-of-the-art propagation algorithms for Table. We describe how CT can be\nenhanced by exploiting the massive computational power offered by modern GPUs\nto handle large Table constraints. In particular, we report on the design and\nimplementation of GPU-accelerated CT, on its integration into an existing\nconstraint solver, and on an experimental validation performed on a significant\nset of instances.", "AI": {"tldr": "本文研究如何利用GPU的并行计算能力加速处理大规模表约束（Table Constraint）的传播算法，特别是对Compact-Table (CT)算法进行优化。", "motivation": "表约束是一种强大的约束形式，原则上可以模拟任何其他有限域变量上的条件。然而，在实际问题中，当有效情况数量巨大时，传统的基于CPU的方法难以高效处理，导致性能瓶颈。", "method": "本文描述了如何通过利用现代GPU的强大计算能力来增强最先进的Compact-Table (CT)传播算法，以处理大型表约束。具体方法包括设计和实现GPU加速的CT算法，并将其集成到现有的约束求解器中。", "result": "研究报告了GPU加速CT的设计和实现，以及其与现有约束求解器的集成。通过在大量实例上进行实验验证，证明了该方法在处理大规模表约束时的有效性。", "conclusion": "利用GPU的并行计算能力可以显著提升表约束传播算法（特别是Compact-Table）的处理效率，从而有效应对现实世界中包含大量有效情况的复杂问题。"}}
{"id": "2507.18171", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18171", "abs": "https://arxiv.org/abs/2507.18171", "authors": ["Kexin Chen", "Dongxia Wang", "Yi Liu", "Haonan Zhang", "Wenhai Wang"], "title": "Sticking to the Mean: Detecting Sticky Tokens in Text Embedding Models", "comment": "ACL 2025 main", "summary": "Despite the widespread use of Transformer-based text embedding models in NLP\ntasks, surprising 'sticky tokens' can undermine the reliability of embeddings.\nThese tokens, when repeatedly inserted into sentences, pull sentence similarity\ntoward a certain value, disrupting the normal distribution of embedding\ndistances and degrading downstream performance. In this paper, we\nsystematically investigate such anomalous tokens, formally defining them and\nintroducing an efficient detection method, Sticky Token Detector (STD), based\non sentence and token filtering. Applying STD to 40 checkpoints across 14 model\nfamilies, we discover a total of 868 sticky tokens. Our analysis reveals that\nthese tokens often originate from special or unused entries in the vocabulary,\nas well as fragmented subwords from multilingual corpora. Notably, their\npresence does not strictly correlate with model size or vocabulary size. We\nfurther evaluate how sticky tokens affect downstream tasks like clustering and\nretrieval, observing significant performance drops of up to 50%. Through\nattention-layer analysis, we show that sticky tokens disproportionately\ndominate the model's internal representations, raising concerns about\ntokenization robustness. Our findings show the need for better tokenization\nstrategies and model design to mitigate the impact of sticky tokens in future\ntext embedding applications.", "AI": {"tldr": "研究发现Transformer文本嵌入模型中存在“粘性词元”，它们会扭曲嵌入距离分布并降低下游任务性能。论文定义了这些词元，提出了一种高效检测方法STD，发现大量粘性词元，分析了其来源和对性能的显著影响，并呼吁改进分词策略和模型设计。", "motivation": "Transformer文本嵌入模型在NLP任务中广泛使用，但“粘性词元”的存在会损害嵌入的可靠性，扭曲句向量相似度，并降低下游任务性能，因此需要系统性地调查和解决这一问题。", "method": "论文正式定义了“粘性词元”，提出了一种基于句子和词元过滤的“粘性词元检测器”（STD）高效检测方法。将STD应用于14个模型家族的40个检查点，并对粘性词元进行注意力层分析，评估其对聚类和检索等下游任务的影响。", "result": "共发现了868个粘性词元，它们通常来源于词汇表中特殊或未使用的条目以及多语言语料库中的碎片化子词。粘性词元的存在与模型大小或词汇表大小无严格关联。它们导致下游任务性能显著下降，降幅高达50%。注意力层分析显示，粘性词元不成比例地主导了模型的内部表示。", "conclusion": "研究结果揭示了粘性词元对文本嵌入模型鲁棒性的担忧，强调了未来文本嵌入应用中需要更好的分词策略和模型设计来缓解粘性词元的影响。"}}
{"id": "2507.18060", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18060", "abs": "https://arxiv.org/abs/2507.18060", "authors": ["Chengxuan Zhu", "Qingnan Fan", "Qi Zhang", "Jinwei Chen", "Huaqi Zhang", "Chao Xu", "Boxin Shi"], "title": "BokehDiff: Neural Lens Blur with One-Step Diffusion", "comment": "Accepted by ICCV 2025", "summary": "We introduce BokehDiff, a novel lens blur rendering method that achieves\nphysically accurate and visually appealing outcomes, with the help of\ngenerative diffusion prior. Previous methods are bounded by the accuracy of\ndepth estimation, generating artifacts in depth discontinuities. Our method\nemploys a physics-inspired self-attention module that aligns with the image\nformation process, incorporating depth-dependent circle of confusion constraint\nand self-occlusion effects. We adapt the diffusion model to the one-step\ninference scheme without introducing additional noise, and achieve results of\nhigh quality and fidelity. To address the lack of scalable paired data, we\npropose to synthesize photorealistic foregrounds with transparency with\ndiffusion models, balancing authenticity and scene diversity.", "AI": {"tldr": "BokehDiff是一种新颖的镜头模糊渲染方法，利用生成扩散先验实现物理准确和视觉吸引人的效果，解决了现有方法在深度估计和不连续性处的伪影问题。", "motivation": "现有镜头模糊渲染方法受限于深度估计的准确性，在深度不连续处产生伪影，无法生成物理准确且视觉吸引人的结果。", "method": "引入BokehDiff，采用受物理启发的自注意力模块，与图像形成过程对齐，并整合了依赖深度的弥散圈（CoC）约束和自遮挡效应。将扩散模型适应为一步推理方案，不引入额外噪声。为解决配对数据缺乏问题，提出使用扩散模型合成具有透明度的真实感前景。", "result": "实现了物理准确且视觉吸引人的结果，通过一步推理方案达到了高质量和高保真度。", "conclusion": "BokehDiff通过结合物理启发机制和扩散先验，成功克服了传统方法的局限性，提供了卓越的镜头模糊渲染效果。"}}
{"id": "2507.18550", "categories": ["cs.AI", "cs.CV", "cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2507.18550", "abs": "https://arxiv.org/abs/2507.18550", "authors": ["Manuel de Sousa Ribeiro", "Afonso Leote", "João Leite"], "title": "On the Performance of Concept Probing: The Influence of the Data (Extended Version)", "comment": "Extended version of the paper published in Proceedings of the\n  European Conference on Artificial Intelligence (ECAI 2025)", "summary": "Concept probing has recently garnered increasing interest as a way to help\ninterpret artificial neural networks, dealing both with their typically large\nsize and their subsymbolic nature, which ultimately renders them unfeasible for\ndirect human interpretation. Concept probing works by training additional\nclassifiers to map the internal representations of a model into human-defined\nconcepts of interest, thus allowing humans to peek inside artificial neural\nnetworks. Research on concept probing has mainly focused on the model being\nprobed or the probing model itself, paying limited attention to the data\nrequired to train such probing models. In this paper, we address this gap.\nFocusing on concept probing in the context of image classification tasks, we\ninvestigate the effect of the data used to train probing models on their\nperformance. We also make available concept labels for two widely used\ndatasets.", "AI": {"tldr": "概念探测通过训练额外分类器将神经网络内部表示映射到人类概念，以解释神经网络。本文关注探测模型训练数据对性能的影响，并发布了两个常用数据集的概念标签。", "motivation": "神经网络因其规模庞大和亚符号性质难以直接解释。概念探测作为一种解释方法兴起，但现有研究多关注被探测模型或探测模型本身，对训练探测模型所需的数据关注不足。本文旨在弥补这一空白。", "method": "本文在图像分类任务背景下，研究了用于训练探测模型的数据对其性能的影响。同时，作者为两个广泛使用的数据集提供了概念标签。", "result": "研究揭示了用于训练探测模型的数据对探测模型性能的影响。具体结果未在摘要中详细说明，但表明数据是一个重要因素。", "conclusion": "训练探测模型的数据对概念探测的性能至关重要。本文通过实验验证了这一点，并为社区提供了带概念标签的数据集，以促进未来研究。"}}
{"id": "2507.18182", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18182", "abs": "https://arxiv.org/abs/2507.18182", "authors": ["Wonjun Jeong", "Dongseok Kim", "Taegkeun Whangbo"], "title": "SCOPE: Stochastic and Counterbiased Option Placement for Evaluating Large Language Models", "comment": "34 pages, 1 figure", "summary": "Large Language Models (LLMs) can achieve inflated scores on multiple-choice\ntasks by exploiting inherent biases in option positions or labels, rather than\ndemonstrating genuine understanding. This study introduces SCOPE, an evaluation\nframework designed to measure and mitigate such selection bias in a\ndataset-independent manner. By repeatedly invoking a null prompt that lacks\nsemantic content, SCOPE estimates each model's unique position-bias\ndistribution. It then redistributes the answer slot according to the\ninverse-bias distribution, thereby equalizing the lucky-rate, the probability\nof selecting the correct answer by chance. Furthermore, it prevents\nsemantically similar distractors from being placed adjacent to the answer,\nthereby blocking near-miss guesses based on superficial proximity cues. Across\nmultiple benchmark experiments, SCOPE consistently outperformed existing\ndebiasing methods in terms of stable performance improvements and showed\nclearer confidence distributions over correct options. This framework thus\noffers a new standard for enhancing the fairness and reliability of LLM\nevaluations.", "AI": {"tldr": "该研究引入SCOPE框架，通过估计并抵消大型语言模型在多项选择题中利用选项位置或标签偏差的问题，从而提高评估的公平性和可靠性。", "motivation": "大型语言模型（LLMs）在多项选择任务中可能因利用选项位置或标签的固有偏差而获得虚高分数，而非真正理解，这促使研究者开发一种测量和缓解此类选择偏差的方法。", "method": "SCOPE框架通过重复调用无语义内容的空提示来估计每个模型的独特位置偏差分布。然后，它根据逆偏差分布重新分配答案槽，以平衡随机选择正确答案的概率。此外，它还阻止语义相似的干扰项被放置在答案附近，从而避免基于表面接近度的侥幸猜测。", "result": "在多项基准实验中，SCOPE框架在稳定的性能提升方面始终优于现有去偏方法，并显示出对正确选项更清晰的置信度分布。", "conclusion": "SCOPE框架为增强大型语言模型评估的公平性和可靠性提供了一个新标准。"}}
{"id": "2507.18064", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18064", "abs": "https://arxiv.org/abs/2507.18064", "authors": ["Xiaoran Sun", "Liyan Wang", "Cong Wang", "Yeying Jin", "Kin-man Lam", "Zhixun Su", "Yang Yang", "Jinshan Pan"], "title": "Adapting Large VLMs with Iterative and Manual Instructions for Generative Low-light Enhancement", "comment": null, "summary": "Most existing low-light image enhancement (LLIE) methods rely on pre-trained\nmodel priors, low-light inputs, or both, while neglecting the semantic guidance\navailable from normal-light images. This limitation hinders their effectiveness\nin complex lighting conditions. In this paper, we propose VLM-IMI, a novel\nframework that leverages large vision-language models (VLMs) with iterative and\nmanual instructions (IMIs) for LLIE. VLM-IMI incorporates textual descriptions\nof the desired normal-light content as enhancement cues, enabling semantically\ninformed restoration. To effectively integrate cross-modal priors, we introduce\nan instruction prior fusion module, which dynamically aligns and fuses image\nand text features, promoting the generation of detailed and semantically\ncoherent outputs. During inference, we adopt an iterative and manual\ninstruction strategy to refine textual instructions, progressively improving\nvisual quality. This refinement enhances structural fidelity, semantic\nalignment, and the recovery of fine details under extremely low-light\nconditions. Extensive experiments across diverse scenarios demonstrate that\nVLM-IMI outperforms state-of-the-art methods in both quantitative metrics and\nperceptual quality. The source code is available at\nhttps://github.com/sunxiaoran01/VLM-IMI.", "AI": {"tldr": "VLM-IMI是一个新颖的低光图像增强框架，它利用大型视觉-语言模型（VLM）和迭代手动指令（IMI），通过文本描述提供语义指导，以实现高质量的图像恢复。", "motivation": "现有低光图像增强方法多依赖预训练模型或低光输入，忽略了正常光图像中的语义指导，导致在复杂光照条件下效果受限。", "method": "提出VLM-IMI框架，利用视觉-语言模型，将目标正常光内容的文本描述作为增强线索，实现语义引导恢复。引入指令先验融合模块，动态对齐并融合图像和文本特征。推理时采用迭代和手动指令策略，逐步优化文本指令，提升视觉质量。", "result": "在多种场景下的广泛实验表明，VLM-IMI在定量指标和感知质量上均优于现有最先进方法，尤其在极低光照条件下能增强结构保真度、语义对齐和细节恢复。", "conclusion": "VLM-IMI通过整合视觉-语言模型和迭代指令，有效解决了低光图像增强中语义信息缺失的问题，显著提升了图像恢复的质量和细节表现，为复杂光照条件下的图像增强提供了新范式。"}}
{"id": "2507.18576", "categories": ["cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18576", "abs": "https://arxiv.org/abs/2507.18576", "authors": ["Shanghai AI Lab", ":", "Yicheng Bao", "Guanxu Chen", "Mingkang Chen", "Yunhao Chen", "Chiyu Chen", "Lingjie Chen", "Sirui Chen", "Xinquan Chen", "Jie Cheng", "Yu Cheng", "Dengke Deng", "Yizhuo Ding", "Dan Ding", "Xiaoshan Ding", "Yi Ding", "Zhichen Dong", "Lingxiao Du", "Yuyu Fan", "Xinshun Feng", "Yanwei Fu", "Yuxuan Gao", "Ruijun Ge", "Tianle Gu", "Lujun Gui", "Jiaxuan Guo", "Qianxi He", "Yuenan Hou", "Xuhao Hu", "Hong Huang", "Kaichen Huang", "Shiyang Huang", "Yuxian Jiang", "Shanzhe Lei", "Jie Li", "Lijun Li", "Hao Li", "Juncheng Li", "Xiangtian Li", "Yafu Li", "Lingyu Li", "Xueyan Li", "Haotian Liang", "Dongrui Liu", "Qihua Liu", "Zhixuan Liu", "Bangwei Liu", "Huacan Liu", "Yuexiao Liu", "Zongkai Liu", "Chaochao Lu", "Yudong Lu", "Xiaoya Lu", "Zhenghao Lu", "Qitan Lv", "Caoyuan Ma", "Jiachen Ma", "Xiaoya Ma", "Zhongtian Ma", "Lingyu Meng", "Ziqi Miao", "Yazhe Niu", "Yuezhang Peng", "Yuan Pu", "Han Qi", "Chen Qian", "Xingge Qiao", "Jingjing Qu", "Jiashu Qu", "Wanying Qu", "Wenwen Qu", "Xiaoye Qu", "Qihan Ren", "Qingnan Ren", "Qingyu Ren", "Jing Shao", "Wenqi Shao", "Shuai Shao", "Dongxing Shi", "Xin Song", "Xinhao Song", "Yan Teng", "Xuan Tong", "Yingchun Wang", "Xuhong Wang", "Shujie Wang", "Xin Wang", "Yige Wang", "Yixu Wang", "Yuanfu Wang", "Futing Wang", "Ruofan Wang", "Wenjie Wang", "Yajie Wang", "Muhao Wei", "Xiaoyu Wen", "Fenghua Weng", "Yuqi Wu", "Yingtong Xiong", "Xingcheng Xu", "Chao Yang", "Yue Yang", "Yang Yao", "Yulei Ye", "Zhenyun Yin", "Yi Yu", "Bo Zhang", "Qiaosheng Zhang", "Jinxuan Zhang", "Yexin Zhang", "Yinqiang Zheng", "Hefeng Zhou", "Zhanhui Zhou", "Pengyu Zhu", "Qingzi Zhu", "Yubo Zhu", "Bowen Zhou"], "title": "SafeWork-R1: Coevolving Safety and Intelligence under the AI-45$^{\\circ}$ Law", "comment": "47 pages, 18 figures, authors are listed in alphabetical order by\n  their last names", "summary": "We introduce SafeWork-R1, a cutting-edge multimodal reasoning model that\ndemonstrates the coevolution of capabilities and safety. It is developed by our\nproposed SafeLadder framework, which incorporates large-scale, progressive,\nsafety-oriented reinforcement learning post-training, supported by a suite of\nmulti-principled verifiers. Unlike previous alignment methods such as RLHF that\nsimply learn human preferences, SafeLadder enables SafeWork-R1 to develop\nintrinsic safety reasoning and self-reflection abilities, giving rise to safety\n`aha' moments. Notably, SafeWork-R1 achieves an average improvement of\n$46.54\\%$ over its base model Qwen2.5-VL-72B on safety-related benchmarks\nwithout compromising general capabilities, and delivers state-of-the-art safety\nperformance compared to leading proprietary models such as GPT-4.1 and Claude\nOpus 4. To further bolster its reliability, we implement two distinct\ninference-time intervention methods and a deliberative search mechanism,\nenforcing step-level verification. Finally, we further develop\nSafeWork-R1-InternVL3-78B, SafeWork-R1-DeepSeek-70B, and\nSafeWork-R1-Qwen2.5VL-7B. All resulting models demonstrate that safety and\ncapability can co-evolve synergistically, highlighting the generalizability of\nour framework in building robust, reliable, and trustworthy general-purpose AI.", "AI": {"tldr": "本文提出了SafeWork-R1，一个通过SafeLadder框架训练的多模态推理模型，实现了能力与安全性的协同演进，并在安全性基准上显著超越基线模型和领先的专有模型。", "motivation": "传统的对齐方法（如RLHF）仅学习人类偏好，缺乏内在的安全推理和自我反思能力。研究旨在开发一种能使模型具备内在安全推理和自我反思能力的方法，从而实现能力与安全的协同发展。", "method": "引入了SafeWork-R1多模态推理模型。开发了SafeLadder框架，该框架包含大规模、渐进式、面向安全的强化学习后训练，并由多原则验证器支持。此外，还实施了两种不同的推理时干预方法和一种审慎搜索机制，以强制执行步骤级验证。该框架应用于多种基础模型（如Qwen2.5-VL-72B、InternVL3-78B、DeepSeek-70B、Qwen2.5VL-7B）。", "result": "SafeWork-R1在安全相关基准上比其基础模型Qwen2.5-VL-72B平均提升了46.54%，且不损害通用能力。与GPT-4.1和Claude Opus 4等领先专有模型相比，其安全性能达到了最先进水平。所有衍生的模型（如SafeWork-R1-InternVL3-78B）都展示了安全性和能力的协同演进。", "conclusion": "安全性和能力可以协同演进，这突出了所提出的SafeLadder框架在构建鲁棒、可靠和值得信赖的通用人工智能方面的通用性。"}}
{"id": "2507.18190", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.18190", "abs": "https://arxiv.org/abs/2507.18190", "authors": ["Keyu Wu", "Qianjin Yu", "Manlin Mei", "Ruiting Liu", "Jun Wang", "Kailai Zhang", "Yelun Bao"], "title": "TN-AutoRCA: Benchmark Construction and Agentic Framework for Self-Improving Alarm-Based Root Cause Analysis in Telecommunication Networks", "comment": "10 pages", "summary": "Root Cause Analysis (RCA) in telecommunication networks is a critical task,\nyet it presents a formidable challenge for Artificial Intelligence (AI) due to\nits complex, graph-based reasoning requirements and the scarcity of realistic\nbenchmarks.", "AI": {"tldr": "电信网络中的根因分析（RCA）对人工智能（AI）来说是一个巨大挑战，因为它需要复杂的图基推理，并且缺乏真实的基准。", "motivation": "电信网络中的根因分析是一项关键任务，但其复杂的图基推理需求和真实基准的稀缺性使其成为AI难以应对的挑战。", "method": "抽象中未提及具体方法。", "result": "抽象中未提及具体结果。", "conclusion": "抽象中未提及具体结论。"}}
{"id": "2507.18082", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18082", "abs": "https://arxiv.org/abs/2507.18082", "authors": ["Pascal Spiegler", "Taha Koleilat", "Arash Harirpoush", "Corey S. Miller", "Hassan Rivaz", "Marta Kersten-Oertel", "Yiming Xiao"], "title": "TextSAM-EUS: Text Prompt Learning for SAM to Accurately Segment Pancreatic Tumor in Endoscopic Ultrasound", "comment": "Accepted to ICCV 2025 Workshop CVAMD", "summary": "Pancreatic cancer carries a poor prognosis and relies on endoscopic\nultrasound (EUS) for targeted biopsy and radiotherapy. However, the speckle\nnoise, low contrast, and unintuitive appearance of EUS make segmentation of\npancreatic tumors with fully supervised deep learning (DL) models both\nerror-prone and dependent on large, expert-curated annotation datasets. To\naddress these challenges, we present TextSAM-EUS, a novel, lightweight,\ntext-driven adaptation of the Segment Anything Model (SAM) that requires no\nmanual geometric prompts at inference. Our approach leverages text prompt\nlearning (context optimization) through the BiomedCLIP text encoder in\nconjunction with a LoRA-based adaptation of SAM's architecture to enable\nautomatic pancreatic tumor segmentation in EUS, tuning only 0.86% of the total\nparameters. On the public Endoscopic Ultrasound Database of the Pancreas,\nTextSAM-EUS with automatic prompts attains 82.69% Dice and 85.28% normalized\nsurface distance (NSD), and with manual geometric prompts reaches 83.10% Dice\nand 85.70% NSD, outperforming both existing state-of-the-art (SOTA) supervised\nDL models and foundation models (e.g., SAM and its variants). As the first\nattempt to incorporate prompt learning in SAM-based medical image segmentation,\nTextSAM-EUS offers a practical option for efficient and robust automatic EUS\nsegmentation. Our code will be publicly available upon acceptance.", "AI": {"tldr": "TextSAM-EUS是一种新型轻量级文本驱动的SAM模型，通过利用文本提示学习和LoRA适配，实现了胰腺肿瘤超声图像（EUS）的自动分割，无需推理时手动几何提示，并且性能优于现有SOTA模型和SAM变体。", "motivation": "胰腺癌预后不良，依赖EUS进行活检和放疗。然而，EUS图像存在散斑噪声、对比度低和外观不直观等问题，使得全监督深度学习模型进行肿瘤分割容易出错，并高度依赖大型、专家标注的数据集。本研究旨在解决这些挑战。", "method": "提出了TextSAM-EUS，这是Segment Anything Model (SAM)的一种轻量级、文本驱动的改编。它通过BiomedCLIP文本编码器结合上下文优化实现文本提示学习，并采用基于LoRA的SAM架构适配，仅调整0.86%的总参数，从而实现胰腺肿瘤在EUS中的自动分割，推理时无需手动几何提示。", "result": "在公开的胰腺内窥镜超声数据库上，TextSAM-EUS在自动提示下达到了82.69%的Dice系数和85.28%的归一化表面距离（NSD）；在手动几何提示下达到了83.10%的Dice系数和85.70%的NSD。这优于现有的最先进（SOTA）监督深度学习模型和基础模型（如SAM及其变体）。", "conclusion": "作为首次将提示学习引入基于SAM的医学图像分割的尝试，TextSAM-EUS为高效、鲁棒的自动EUS分割提供了一个实用的选择。"}}
{"id": "2507.18197", "categories": ["cs.CL", "cs.DL"], "pdf": "https://arxiv.org/pdf/2507.18197", "abs": "https://arxiv.org/abs/2507.18197", "authors": ["Aline Belloni", "Patrick Prieur"], "title": "Integrating an ISO30401-compliant Knowledge management system with existing business processes of an organization", "comment": "in French language. AGeCSO2025 : 18{\\`e}me Colloque International de\n  l'Association pour la Gestion des Connaissances dans la Soci{\\'e}t{\\'e} et\n  les Organisations, Association pour la Gestion des Connaissances dans la\n  Soci{\\'e}t{\\'e} et les Organisations (AGECSO), Jun 2025, TROYES, France", "summary": "Business process modeling is used by most organizations as an essential\nframework for ensuring efficiency and effectiveness of the work and workflow\nperformed by its employees and for ensuring the alignment of such work with its\nstrategic goals. For organizations that are compliant or near-compliant with\nISO 9001, this approach involves the detailed mapping of processes,\nsub-processes, activities, and tasks. ISO30401 is a Management System Standard,\nintroduced in 2018, establishing universal requirements for the set up of a\nKnowledge Management System in an organization. As ``ISO30401 implementers'' we\nregularly face the challenge of explaining our clients how the knowledge\ndevelopment, transformation and conveyances activities depicted in ISO30401 do\nintegrate with existing operational processes. This article recaps process\nmodelling principles in the context of ISO9001 and explores, based on our\nexperience, how an ISO30401-compliant Knowledge Management System (KMS)\nentwines with all other processes of an Integrated Management System and in\nparticular how it can be implemented by deploying the mechanisms of the SECI\nmodel through the steps of PDCA cycles.", "AI": {"tldr": "本文探讨了如何将ISO 30401知识管理系统（KMS）与ISO 9001业务流程整合，特别是通过SECI模型和PDCA循环实现。", "motivation": "组织广泛使用业务流程建模（如ISO 9001）来提高效率。ISO 30401于2018年引入了知识管理系统要求。作者作为ISO 30401实施者，经常面临如何向客户解释ISO 30401中的知识活动如何与现有操作流程整合的挑战。", "method": "回顾了ISO 9001背景下的流程建模原则。基于作者的经验，探讨了ISO 30401兼容的知识管理系统如何与集成管理系统的所有其他流程交织，并具体阐述了如何通过PDCA循环的步骤部署SECI模型的机制来实现整合。", "result": "文章通过经验探索，展示了ISO 30401兼容的知识管理系统如何与现有业务流程（特别是ISO 9001流程）有效整合，并提出通过SECI模型机制在PDCA循环中实施的方法。", "conclusion": "结论是ISO 30401知识管理系统可以成功地与集成管理系统中的其他流程（尤其是ISO 9001流程）整合，并且可以通过在PDCA循环中应用SECI模型来有效地实现这一整合。"}}
{"id": "2507.18099", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18099", "abs": "https://arxiv.org/abs/2507.18099", "authors": ["Naman Srivastava", "Joel D Joy", "Yash Dixit", "Swarup E", "Rakshit Ramesh"], "title": "Comparison of Segmentation Methods in Remote Sensing for Land Use Land Cover", "comment": null, "summary": "Land Use Land Cover (LULC) mapping is essential for urban and resource\nplanning, and is one of the key elements in developing smart and sustainable\ncities.This study evaluates advanced LULC mapping techniques, focusing on\nLook-Up Table (LUT)-based Atmospheric Correction applied to Cartosat\nMultispectral (MX) sensor images, followed by supervised and semi-supervised\nlearning models for LULC prediction. We explore DeeplabV3+ and Cross-Pseudo\nSupervision (CPS). The CPS model is further refined with dynamic weighting,\nenhancing pseudo-label reliability during training. This comprehensive approach\nanalyses the accuracy and utility of LULC mapping techniques for various urban\nplanning applications. A case study of Hyderabad, India, illustrates\nsignificant land use changes due to rapid urbanization. By analyzing Cartosat\nMX images over time, we highlight shifts such as urban sprawl, shrinking green\nspaces, and expanding industrial areas. This demonstrates the practical utility\nof these techniques for urban planners and policymakers.", "AI": {"tldr": "本研究评估了先进的土地利用土地覆盖（LULC）测绘技术，结合基于查找表的大气校正和深度学习模型（DeeplabV3+、动态加权CPS），并通过印度海得拉巴的案例研究，展示了其在快速城市化背景下监测土地利用变化和支持城市规划的实用性。", "motivation": "土地利用土地覆盖（LULC）测绘对于城市和资源规划至关重要，是发展智慧和可持续城市的关键要素。快速城市化导致显著的土地利用变化，需要有效的技术来监测和分析这些变化。", "method": "研究方法包括：1. 对Cartosat Multispectral (MX) 传感器图像应用基于查找表（LUT）的大气校正。2. 接着使用监督学习模型DeeplabV3+和半监督学习模型Cross-Pseudo Supervision (CPS) 进行LULC预测。3. CPS模型通过动态加权进行改进，以提高训练期间伪标签的可靠性。4. 以印度海得拉巴为例进行案例研究，分析LULC测绘技术的准确性和实用性。", "result": "研究分析了LULC测绘技术的准确性和实用性。海得拉巴的案例研究显示，由于快速城市化，该地区发生了显著的土地利用变化，例如城市扩张、绿地萎缩和工业区扩大。这证明了这些技术对城市规划者和政策制定者的实际效用。", "conclusion": "本研究展示的先进LULC测绘技术（包括LUT大气校正、DeeplabV3+和动态加权CPS）能够有效监测和分析城市化带来的土地利用变化，为城市规划者和政策制定者提供了重要的工具，以应对城市发展挑战。"}}
{"id": "2507.18202", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18202", "abs": "https://arxiv.org/abs/2507.18202", "authors": ["San Kim", "Jonghwi Kim", "Yejin Jeon", "Gary Geunbae Lee"], "title": "Safeguarding RAG Pipelines with GMTP: A Gradient-based Masked Token Probability Method for Poisoned Document Detection", "comment": "18 pages, accepted to ACL Findings 2025", "summary": "Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by\nproviding external knowledge for accurate and up-to-date responses. However,\nthis reliance on external sources exposes a security risk, attackers can inject\npoisoned documents into the knowledge base to steer the generation process\ntoward harmful or misleading outputs. In this paper, we propose Gradient-based\nMasked Token Probability (GMTP), a novel defense method to detect and filter\nout adversarially crafted documents. Specifically, GMTP identifies high-impact\ntokens by examining gradients of the retriever's similarity function. These key\ntokens are then masked, and their probabilities are checked via a Masked\nLanguage Model (MLM). Since injected tokens typically exhibit markedly low\nmasked-token probabilities, this enables GMTP to easily detect malicious\ndocuments and achieve high-precision filtering. Experiments demonstrate that\nGMTP is able to eliminate over 90% of poisoned content while retaining relevant\ndocuments, thus maintaining robust retrieval and generation performance across\ndiverse datasets and adversarial settings.", "AI": {"tldr": "本文提出GMTP，一种基于梯度和MLM的新型防御方法，用于检测并过滤RAG系统中的恶意投毒文档，有效消除超过90%的投毒内容。", "motivation": "检索增强生成（RAG）虽然能提升大型语言模型（LLM）的响应准确性和时效性，但其对外部知识源的依赖也引入了安全风险，攻击者可能通过注入投毒文档来引导模型产生有害或误导性输出。", "method": "本文提出了梯度掩码词元概率（GMTP）方法。具体而言，GMTP通过检查检索器相似性函数的梯度来识别高影响力的词元。随后，这些关键词元被遮蔽，并通过掩码语言模型（MLM）检查其概率。由于被注入的恶意词元通常表现出极低的掩码词元概率，这使得GMTP能够有效检测恶意文档并实现高精度过滤。", "result": "实验证明，GMTP能够消除90%以上的投毒内容，同时保留了相关文档，从而在多样化的数据集和对抗设置下，保持了鲁棒的检索和生成性能。", "conclusion": "GMTP是一种有效的防御RAG投毒攻击的方法，能够显著提高系统的安全性，同时不影响其核心功能。"}}
{"id": "2507.18100", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18100", "abs": "https://arxiv.org/abs/2507.18100", "authors": ["Ruizhe Chen", "Zhiting Fan", "Tianze Luo", "Heqing Zou", "Zhaopeng Feng", "Guiyang Xie", "Hansheng Zhang", "Zhuochen Wang", "Zuozhu Liu", "Huaijian Zhang"], "title": "Datasets and Recipes for Video Temporal Grounding via Reinforcement Learning", "comment": null, "summary": "Video Temporal Grounding (VTG) aims to localize relevant temporal segments in\nvideos given natural language queries. Despite recent progress with large\nvision-language models (LVLMs) and instruction-tuning, existing approaches\noften suffer from limited temporal awareness and poor generalization. In this\nwork, we introduce a two-stage training framework that integrates supervised\nfine-tuning with reinforcement learning (RL) to improve both the accuracy and\nrobustness of VTG models. Our approach first leverages high-quality curated\ncold start data for SFT initialization, followed by difficulty-controlled RL to\nfurther enhance temporal localization and reasoning abilities. Comprehensive\nexperiments on multiple VTG benchmarks demonstrate that our method consistently\noutperforms existing models, particularly in challenging and open-domain\nscenarios. We conduct an in-depth analysis of training strategies and dataset\ncuration, highlighting the importance of both high-quality cold start data and\ndifficulty-controlled RL. To facilitate further research and industrial\nadoption, we release all intermediate datasets, models, and code to the\ncommunity.", "AI": {"tldr": "本文提出一个两阶段训练框架，结合监督微调（SFT）和强化学习（RL），以提升视频时间定位（VTG）模型的准确性和泛化能力，尤其是在复杂和开放域场景下。", "motivation": "尽管大型视觉-语言模型（LVLMs）和指令微调取得了进展，但现有视频时间定位（VTG）方法仍存在时间感知能力有限和泛化性差的问题。", "method": "采用两阶段训练框架：首先，利用高质量的“冷启动”数据进行监督微调（SFT）初始化；其次，通过难度控制的强化学习（RL）进一步增强时间定位和推理能力。", "result": "在多个VTG基准测试中，该方法始终优于现有模型，尤其在挑战性和开放域场景下表现突出。深入分析证实了高质量冷启动数据和难度控制强化学习的重要性。", "conclusion": "所提出的SFT与RL结合的两阶段训练框架显著提高了VTG模型的性能和鲁棒性。研究团队已发布所有中间数据集、模型和代码，以促进后续研究和工业应用。"}}
{"id": "2507.18106", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18106", "abs": "https://arxiv.org/abs/2507.18106", "authors": ["JinYoung Kim", "DaeUng Jo", "Kimin Yun", "Jeonghyo Song", "Youngjoon Yoo"], "title": "Distributional Uncertainty for Out-of-Distribution Detection", "comment": "6 pages , 3 figures , IEEE International Conference on Advanced\n  Visual and Signal-Based Systems", "summary": "Estimating uncertainty from deep neural networks is a widely used approach\nfor detecting out-of-distribution (OoD) samples, which typically exhibit high\npredictive uncertainty. However, conventional methods such as Monte Carlo (MC)\nDropout often focus solely on either model or data uncertainty, failing to\nalign with the semantic objective of OoD detection. To address this, we propose\nthe Free-Energy Posterior Network, a novel framework that jointly models\ndistributional uncertainty and identifying OoD and misclassified regions using\nfree energy. Our method introduces two key contributions: (1) a\nfree-energy-based density estimator parameterized by a Beta distribution, which\nenables fine-grained uncertainty estimation near ambiguous or unseen regions;\nand (2) a loss integrated within a posterior network, allowing direct\nuncertainty estimation from learned parameters without requiring stochastic\nsampling. By integrating our approach with the residual prediction branch (RPL)\nframework, the proposed method goes beyond post-hoc energy thresholding and\nenables the network to learn OoD regions by leveraging the variance of the Beta\ndistribution, resulting in a semantically meaningful and computationally\nefficient solution for uncertainty-aware segmentation. We validate the\neffectiveness of our method on challenging real-world benchmarks, including\nFishyscapes, RoadAnomaly, and Segment-Me-If-You-Can.", "AI": {"tldr": "该论文提出了一个名为自由能后验网络（Free-Energy Posterior Network）的新框架，通过结合Beta分布和残差预测分支（RPL），实现不确定性感知分割中的越界（OoD）样本检测，无需随机采样，从而提供了一个语义上有意义且计算高效的解决方案。", "motivation": "现有的深度神经网络不确定性估计方法（如Monte Carlo Dropout）在检测越界（OoD）样本时，通常只关注模型或数据不确定性，未能与OoD检测的语义目标对齐。此外，这些方法可能需要随机采样或事后阈值设定。", "method": "本文提出了自由能后验网络，它联合建模分布不确定性并识别OoD和错误分类区域。该方法引入了两大贡献：1) 一个基于Beta分布参数化的自由能密度估计器，用于在模糊或未见区域进行细粒度不确定性估计；2) 一个集成在后验网络中的损失函数，允许直接从学习参数估计不确定性，无需随机采样。该方法还与残差预测分支（RPL）框架结合，利用Beta分布的方差使网络学习OoD区域。", "result": "该方法在包括Fishyscapes、RoadAnomaly和Segment-Me-If-You-Can在内的挑战性真实世界基准测试中验证了其有效性。", "conclusion": "该方法提供了一个语义上有意义且计算高效的不确定性感知分割解决方案，超越了传统的事后能量阈值方法，能够有效学习越界（OoD）区域。"}}
{"id": "2507.18203", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.18203", "abs": "https://arxiv.org/abs/2507.18203", "authors": ["Kyubeen Han", "Junseo Jang", "Hongjin Kim", "Geunyeong Jeong", "Harksoo Kim"], "title": "Exploring the Impact of Instruction-Tuning on LLM's Susceptibility to Misinformation", "comment": "ACL 2025 Main Accepted", "summary": "Instruction-tuning enhances the ability of large language models (LLMs) to\nfollow user instructions more accurately, improving usability while reducing\nharmful outputs. However, this process may increase the model's dependence on\nuser input, potentially leading to the unfiltered acceptance of misinformation\nand the generation of hallucinations. Existing studies primarily highlight that\nLLMs are receptive to external information that contradict their parametric\nknowledge, but little research has been conducted on the direct impact of\ninstruction-tuning on this phenomenon. In our study, we investigate the impact\nof instruction-tuning on LLM's susceptibility to misinformation. Our analysis\nreveals that instruction-tuned LLMs are significantly more likely to accept\nmisinformation when it is presented by the user. A comparison with base models\nshows that instruction-tuning increases reliance on user-provided information,\nshifting susceptibility from the assistant role to the user role. Furthermore,\nwe explore additional factors influencing misinformation susceptibility, such\nas the role of the user in prompt structure, misinformation length, and the\npresence of warnings in the system prompt. Our findings underscore the need for\nsystematic approaches to mitigate unintended consequences of instruction-tuning\nand enhance the reliability of LLMs in real-world applications.", "AI": {"tldr": "本研究发现指令微调（instruction-tuning）会显著增加大型语言模型（LLMs）对用户提供虚假信息的接受度，并探索了影响这种易感性的其他因素。", "motivation": "指令微调提高了LLMs遵循指令的能力，但可能增加模型对用户输入的依赖，导致无过滤地接受虚假信息。现有研究虽关注LLMs对外部信息的接受度，但很少直接探讨指令微调对此现象的影响。", "method": "研究调查了指令微调对LLM虚假信息易感性的影响。通过与基础模型进行比较，分析了指令微调对模型接受用户提供虚假信息的影响。此外，还探讨了提示结构中用户角色、虚假信息长度以及系统提示中警告的存在等因素对虚假信息易感性的影响。", "result": "指令微调后的LLMs在用户提供虚假信息时，接受虚假信息的可能性显著增加。与基础模型相比，指令微调增强了LLMs对用户提供信息的依赖，使虚假信息易感性从助手角色转移到用户角色。", "conclusion": "研究结果强调了需要系统性方法来减轻指令微调带来的意外后果，并提高LLMs在实际应用中的可靠性。"}}
{"id": "2507.18104", "categories": ["cs.CV", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2507.18104", "abs": "https://arxiv.org/abs/2507.18104", "authors": ["Qianyi He", "Yuan Chang Leong"], "title": "A Multimodal Seq2Seq Transformer for Predicting Brain Responses to Naturalistic Stimuli", "comment": null, "summary": "The Algonauts 2025 Challenge called on the community to develop encoding\nmodels that predict whole-brain fMRI responses to naturalistic multimodal\nmovies. In this submission, we propose a sequence-to-sequence Transformer that\nautoregressively predicts fMRI activity from visual, auditory, and language\ninputs. Stimulus features were extracted using pretrained models including\nVideoMAE, HuBERT, Qwen, and BridgeTower. The decoder integrates information\nfrom prior brain states, current stimuli, and episode-level summaries via dual\ncross-attention mechanisms that attend to both perceptual information extracted\nfrom the stimulus as well as narrative information provided by high-level\nsummaries of narrative content. One core innovation of our approach is the use\nof sequences of multimodal context to predict sequences of brain activity,\nenabling the model to capture long-range temporal structure in both stimuli and\nneural responses. Another is the combination of a shared encoder with partial\nsubject-specific decoder, which leverages common structure across subjects\nwhile accounting for individual variability. Our model achieves strong\nperformance on both in-distribution and out-of-distribution data, demonstrating\nthe effectiveness of temporally-aware, multimodal sequence modeling for brain\nactivity prediction. The code is available at\nhttps://github.com/Angelneer926/Algonauts_challenge.", "AI": {"tldr": "该研究提出了一种序列到序列的Transformer模型，利用多模态（视觉、听觉、语言）输入和先前的脑状态，自回归地预测自然电影刺激下的全脑fMRI活动，并在Algonauts 2025挑战赛中表现出色。", "motivation": "响应Algonauts 2025挑战赛的号召，旨在开发能够预测自然多模态电影刺激下全脑fMRI响应的编码模型。", "method": "采用序列到序列的Transformer模型，通过自回归方式预测fMRI活动。输入特征使用预训练模型（VideoMAE、HuBERT、Qwen、BridgeTower）提取。解码器通过双交叉注意力机制整合先前的脑状态、当前刺激和情节级摘要信息。模型创新点包括使用多模态上下文序列来预测脑活动序列，以捕捉长期时间结构；以及结合共享编码器和部分受试者特异性解码器，以兼顾通用结构和个体差异。", "result": "模型在同分布（in-distribution）和异分布（out-of-distribution）数据上均取得了强大的性能，证明了时间感知、多模态序列建模在脑活动预测中的有效性。", "conclusion": "时间感知和多模态序列建模方法对于预测大脑活动是有效的，本研究提出的模型在fMRI预测任务中展现了优异性能。"}}
{"id": "2507.18177", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18177", "abs": "https://arxiv.org/abs/2507.18177", "authors": ["Dhruv Jain", "Romain Modzelewski", "Romain Hérault", "Clement Chatelain", "Eva Torfeh", "Sebastien Thureau"], "title": "Differential-UMamba: Rethinking Tumor Segmentation Under Limited Data Scenarios", "comment": null, "summary": "In data-scarce scenarios, deep learning models often overfit to noise and\nirrelevant patterns, which limits their ability to generalize to unseen\nsamples. To address these challenges in medical image segmentation, we\nintroduce Diff-UMamba, a novel architecture that combines the UNet framework\nwith the mamba mechanism for modeling long-range dependencies. At the heart of\nDiff-UMamba is a Noise Reduction Module (NRM), which employs a signal\ndifferencing strategy to suppress noisy or irrelevant activations within the\nencoder. This encourages the model to filter out spurious features and enhance\ntask-relevant representations, thereby improving its focus on clinically\nmeaningful regions. As a result, the architecture achieves improved\nsegmentation accuracy and robustness, particularly in low-data settings.\nDiff-UMamba is evaluated on multiple public datasets, including MSD (lung and\npancreas) and AIIB23, demonstrating consistent performance gains of 1-3% over\nbaseline methods across diverse segmentation tasks. To further assess\nperformance under limited-data conditions, additional experiments are conducted\non the BraTS-21 dataset by varying the proportion of available training\nsamples. The approach is also validated on a small internal non-small cell lung\ncancer (NSCLC) dataset for gross tumor volume (GTV) segmentation in cone beam\nCT (CBCT), where it achieves a 4-5% improvement over the baseline.", "AI": {"tldr": "Diff-UMamba是一种新型医学图像分割架构，结合了UNet和Mamba，并引入噪声抑制模块，旨在解决数据稀缺场景下的过拟合问题，提高分割精度和鲁棒性。", "motivation": "在数据稀缺场景下，深度学习模型容易过拟合噪声和无关模式，限制了其对未见样本的泛化能力，尤其在医学图像分割中表现明显。", "method": "该研究提出了Diff-UMamba架构，它结合了UNet框架和Mamba机制来建模长程依赖。核心是一个噪声抑制模块（NRM），该模块在编码器内部采用信号差分策略来抑制噪声或不相关的激活，从而过滤掉伪特征并增强任务相关表示。", "result": "Diff-UMamba在低数据设置下显著提高了分割精度和鲁棒性。在MSD（肺和胰腺）和AIIB23等公共数据集上，性能比基线方法提高了1-3%。在BraTS-21数据集上通过改变训练样本比例验证了其在有限数据条件下的表现。在内部非小细胞肺癌（NSCLC）GTV分割数据集（CBCT）上，比基线提高了4-5%。", "conclusion": "Diff-UMamba通过有效抑制噪声和增强任务相关特征，在数据稀缺的医学图像分割任务中表现出优越的性能和鲁棒性，能够更好地聚焦于临床有意义的区域。"}}
{"id": "2507.18212", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.18212", "abs": "https://arxiv.org/abs/2507.18212", "authors": ["Xinrui Chen", "Hongxing Zhang", "Fanyi Zeng", "Yongxian Wei", "Yizhi Wang", "Xitong Ling", "Guanghao Li", "Chun Yuan"], "title": "Prune&Comp: Free Lunch for Layer-Pruned LLMs via Iterative Pruning with Magnitude Compensation", "comment": null, "summary": "Layer pruning has emerged as a promising technique for compressing large\nlanguage models (LLMs) while achieving acceleration proportional to the pruning\nratio. In this work, we identify that removing any layer induces a significant\nmagnitude gap in hidden states, resulting in substantial performance\ndegradation. To address this issue, we propose Prune&Comp, a novel\nplug-and-play layer pruning scheme that leverages magnitude compensation to\nmitigate such gaps in a training-free manner. Specifically, we first estimate\nthe magnitude gap caused by layer removal and then eliminate this gap by\nrescaling the remaining weights offline, with zero runtime overhead incurred.\nWe further demonstrate the advantages of Prune&Comp through an iterative\npruning strategy. When integrated with an iterative prune-and-compensate loop,\nPrune&Comp consistently enhances existing layer pruning metrics. For instance,\nwhen 5 layers of LLaMA-3-8B are pruned using the prevalent block influence\nmetric, Prune&Comp nearly halves the perplexity and retains 93.19\\% of the\noriginal model's question-answering performance, outperforming the baseline by\n4.01%.", "AI": {"tldr": "该研究提出了一种名为 Prune&Comp 的新型层剪枝方案，通过无训练的幅度补偿来解决大型语言模型（LLMs）层剪枝导致的隐藏状态幅度差异问题，显著提升了剪枝性能。", "motivation": "层剪枝是压缩LLMs的一种有前景的技术，但移除层会导致隐藏状态产生显著的幅度差异，从而导致性能大幅下降。", "method": "提出 Prune&Comp 方案，通过离线重新缩放剩余权重，在零运行时开销下估计并消除层移除导致的幅度差异。该方案可即插即用，并结合迭代剪枝策略进一步提升效果。", "result": "Prune&Comp 持续提升了现有层剪枝指标。例如，在剪枝 LLaMA-3-8B 的5层时，Prune&Comp 几乎将困惑度减半，并保留了原始模型93.19%的问答性能，比基线提高了4.01%。", "conclusion": "Prune&Comp 是一种有效、无训练、即插即用的层剪枝方案，通过幅度补偿成功缓解了LLM层剪枝中的性能下降问题。"}}
{"id": "2507.18107", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18107", "abs": "https://arxiv.org/abs/2507.18107", "authors": ["Yubin Chen", "Xuyang Guo", "Zhenmei Shi", "Zhao Song", "Jiahao Zhang"], "title": "T2VWorldBench: A Benchmark for Evaluating World Knowledge in Text-to-Video Generation", "comment": null, "summary": "Text-to-video (T2V) models have shown remarkable performance in generating\nvisually reasonable scenes, while their capability to leverage world knowledge\nfor ensuring semantic consistency and factual accuracy remains largely\nunderstudied. In response to this challenge, we propose T2VWorldBench, the\nfirst systematic evaluation framework for evaluating the world knowledge\ngeneration abilities of text-to-video models, covering 6 major categories, 60\nsubcategories, and 1,200 prompts across a wide range of domains, including\nphysics, nature, activity, culture, causality, and object. To address both\nhuman preference and scalable evaluation, our benchmark incorporates both human\nevaluation and automated evaluation using vision-language models (VLMs). We\nevaluated the 10 most advanced text-to-video models currently available,\nranging from open source to commercial models, and found that most models are\nunable to understand world knowledge and generate truly correct videos. These\nfindings point out a critical gap in the capability of current text-to-video\nmodels to leverage world knowledge, providing valuable research opportunities\nand entry points for constructing models with robust capabilities for\ncommonsense reasoning and factual generation.", "AI": {"tldr": "本文提出了T2VWorldBench，这是首个系统评估文本到视频（T2V）模型世界知识生成能力的框架，发现当前模型在这方面存在显著不足。", "motivation": "尽管T2V模型在生成视觉合理场景方面表现出色，但它们利用世界知识确保语义一致性和事实准确性的能力仍未得到充分研究。", "method": "研究者提出了T2VWorldBench评估框架，涵盖物理、自然、活动、文化、因果和物体6大类、60个子类和1200个提示。该基准结合了人工评估和使用视觉语言模型（VLMs）的自动化评估。", "result": "研究评估了10个最先进的T2V模型，发现大多数模型无法理解世界知识并生成真正正确的视频。", "conclusion": "当前T2V模型在利用世界知识方面存在关键差距，这为构建具有强大常识推理和事实生成能力的模型提供了宝贵的研究机会和切入点。"}}
{"id": "2507.18243", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18243", "abs": "https://arxiv.org/abs/2507.18243", "authors": ["Longjian Zeng", "Zunjie Zhu", "Rongfeng Lu", "Ming Lu", "Bolun Zheng", "Chenggang Yan", "Anke Xue"], "title": "DepthDark: Robust Monocular Depth Estimation for Low-Light Environments", "comment": "Accepted by ACM MM 2025 conference", "summary": "In recent years, foundation models for monocular depth estimation have\nreceived increasing attention. Current methods mainly address typical daylight\nconditions, but their effectiveness notably decreases in low-light\nenvironments. There is a lack of robust foundational models for monocular depth\nestimation specifically designed for low-light scenarios. This largely stems\nfrom the absence of large-scale, high-quality paired depth datasets for\nlow-light conditions and the effective parameter-efficient fine-tuning (PEFT)\nstrategy. To address these challenges, we propose DepthDark, a robust\nfoundation model for low-light monocular depth estimation. We first introduce a\nflare-simulation module and a noise-simulation module to accurately simulate\nthe imaging process under nighttime conditions, producing high-quality paired\ndepth datasets for low-light conditions. Additionally, we present an effective\nlow-light PEFT strategy that utilizes illumination guidance and multiscale\nfeature fusion to enhance the model's capability in low-light environments. Our\nmethod achieves state-of-the-art depth estimation performance on the\nchallenging nuScenes-Night and RobotCar-Night datasets, validating its\neffectiveness using limited training data and computing resources.", "AI": {"tldr": "DepthDark是一个针对低光照单目深度估计的鲁棒基础模型，通过引入光照和噪声模拟模块生成高质量低光照深度数据集，并提出了一种有效的低光照PEFT策略，在夜间数据集上实现了最先进的性能。", "motivation": "当前单目深度估计的基础模型主要针对日光条件，在低光照环境下效果显著下降。主要挑战在于缺乏大规模高质量的低光照配对深度数据集以及有效的参数高效微调（PEFT）策略。", "method": "提出DepthDark模型。首先，引入光斑模拟模块和噪声模拟模块，以准确模拟夜间成像过程，生成高质量的低光照配对深度数据集。其次，提出一种有效的低光照PEFT策略，该策略利用光照引导和多尺度特征融合来增强模型在低光照环境下的能力。", "result": "DepthDark在具有挑战性的nuScenes-Night和RobotCar-Night数据集上实现了最先进的深度估计性能，验证了其在有限训练数据和计算资源下的有效性。", "conclusion": "DepthDark通过解决低光照数据集缺乏和缺乏有效PEFT策略的问题，显著提升了单目深度估计在低光照环境下的鲁棒性和准确性，为该领域提供了新的解决方案。"}}
{"id": "2507.18263", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18263", "abs": "https://arxiv.org/abs/2507.18263", "authors": ["Suhang Wu", "Jialong Tang", "Chengyi Yang", "Pei Zhang", "Baosong Yang", "Junhui Li", "Junfeng Yao", "Min Zhang", "Jinsong Su"], "title": "Locate-and-Focus: Enhancing Terminology Translation in Speech Language Models", "comment": "Accepted at ACL 2025", "summary": "Direct speech translation (ST) has garnered increasing attention nowadays,\nyet the accurate translation of terminology within utterances remains a great\nchallenge. In this regard, current studies mainly concentrate on leveraging\nvarious translation knowledge into ST models. However, these methods often\nstruggle with interference from irrelevant noise and can not fully utilize the\ntranslation knowledge. To address these issues, in this paper, we propose a\nnovel Locate-and-Focus method for terminology translation. It first effectively\nlocates the speech clips containing terminologies within the utterance to\nconstruct translation knowledge, minimizing irrelevant information for the ST\nmodel. Subsequently, it associates the translation knowledge with the utterance\nand hypothesis from both audio and textual modalities, allowing the ST model to\nbetter focus on translation knowledge during translation. Experimental results\nacross various datasets demonstrate that our method effectively locates\nterminologies within utterances and enhances the success rate of terminology\ntranslation, while maintaining robust general translation performance.", "AI": {"tldr": "本文提出了一种名为“定位与聚焦”的新方法，用于直接语音翻译中的术语翻译，通过有效识别术语片段并关联多模态知识，提高了术语翻译的准确性。", "motivation": "直接语音翻译中术语的准确翻译是一个巨大挑战。现有方法在利用翻译知识时，常受无关噪声干扰，且未能充分利用这些知识。", "method": "提出“定位与聚焦”方法：1. 定位语音中包含术语的片段以构建翻译知识，减少无关信息。2. 将此翻译知识与语音和假设（音频和文本模态）关联起来，使语音翻译模型在翻译时能更好地聚焦于这些知识。", "result": "在多个数据集上的实验结果表明，该方法能有效定位语音中的术语，提高术语翻译的成功率，同时保持稳定的整体翻译性能。", "conclusion": "所提出的“定位与聚焦”方法能有效定位语音中的术语，并显著提升术语翻译的准确性，同时不损害整体翻译质量。"}}
{"id": "2507.18135", "categories": ["cs.CV", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.18135", "abs": "https://arxiv.org/abs/2507.18135", "authors": ["Kesheng Wang", "Xiaoyu Chen", "Chunlei He", "Fenfen Li", "Xinxin Yu", "Dexing Kong", "Shoujun Huang", "Qi Dai"], "title": "Information Entropy-Based Framework for Quantifying Tortuosity in Meibomian Gland Uneven Atrophy", "comment": "This manuscript contains 7 figures. All comments are welcome", "summary": "In the medical image analysis field, precise quantification of curve\ntortuosity plays a critical role in the auxiliary diagnosis and pathological\nassessment of various diseases. In this study, we propose a novel framework for\ntortuosity quantification and demonstrate its effectiveness through the\nevaluation of meibomian gland atrophy uniformity,serving as a representative\napplication scenario.\n  We introduce an information entropy-based tortuosity quantification framework\nthat integrates probability modeling with entropy theory and incorporates\ndomain transformation of curve data. Unlike traditional methods such as\ncurvature or arc-chord ratio, this approach evaluates the tortuosity of a\ntarget curve by comparing it to a designated reference curve. Consequently, it\nis more suitable for tortuosity assessment tasks in medical data where\nbiologically plausible reference curves are available, providing a more robust\nand objective evaluation metric without relying on idealized straight-line\ncomparisons.\n  First, we conducted numerical simulation experiments to preliminarily assess\nthe stability and validity of the method. Subsequently, the framework was\napplied to quantify the spatial uniformity of meibomian gland atrophy and to\nanalyze the difference in this uniformity between \\textit{Demodex}-negative and\n\\textit{Demodex}-positive patient groups. The results demonstrated a\nsignificant difference in tortuosity-based uniformity between the two groups,\nwith an area under the curve of 0.8768, sensitivity of 0.75, and specificity of\n0.93. These findings highlight the clinical utility of the proposed framework\nin curve tortuosity analysis and its potential as a generalizable tool for\nquantitative morphological evaluation in medical diagnostics.", "AI": {"tldr": "该研究提出了一种基于信息熵的曲线扭曲度量化新框架，通过与参考曲线比较进行评估，并在睑板腺萎缩均匀性评估中验证了其在医学诊断中的有效性。", "motivation": "在医学图像分析领域，精确量化曲线扭曲度对于疾病的辅助诊断和病理评估至关重要，但传统方法存在局限性。", "method": "提出了一种基于信息熵的扭曲度量化框架，该框架结合了概率建模、熵理论和曲线数据域变换。与传统方法不同，它通过将目标曲线与指定的生物学参考曲线进行比较来评估扭曲度。研究首先通过数值模拟评估了方法的稳定性和有效性，然后将其应用于量化睑板腺萎缩的空间均匀性，并分析了蠕形螨阴性和阳性患者组之间的差异。", "result": "该框架成功量化了睑板腺萎缩的均匀性，结果显示蠕形螨阴性和阳性患者组之间基于扭曲度的均匀性存在显著差异，曲线下面积（AUC）为0.8768，敏感性为0.75，特异性为0.93。", "conclusion": "所提出的框架在曲线扭曲度分析中具有重要的临床实用性，并有望成为医学诊断中定量形态评估的通用工具。"}}
{"id": "2507.18260", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18260", "abs": "https://arxiv.org/abs/2507.18260", "authors": ["Junyao Li", "Yahao Lu", "Xingyuan Guo", "Xiaoyu Xian", "Tiantian Wang", "Yukai Shi"], "title": "Exploiting Gaussian Agnostic Representation Learning with Diffusion Priors for Enhanced Infrared Small Target Detection", "comment": "Submitted to Neural Networks. We propose the Gaussian Group Squeezer,\n  leveraging Gaussian sampling and compression with diffusion models for\n  channel-based data augmentation", "summary": "Infrared small target detection (ISTD) plays a vital role in numerous\npractical applications. In pursuit of determining the performance boundaries,\nresearchers employ large and expensive manual-labeling data for representation\nlearning. Nevertheless, this approach renders the state-of-the-art ISTD methods\nhighly fragile in real-world challenges. In this paper, we first study the\nvariation in detection performance across several mainstream methods under\nvarious scarcity -- namely, the absence of high-quality infrared data -- that\nchallenge the prevailing theories about practical ISTD. To address this\nconcern, we introduce the Gaussian Agnostic Representation Learning.\nSpecifically, we propose the Gaussian Group Squeezer, leveraging Gaussian\nsampling and compression for non-uniform quantization. By exploiting a diverse\narray of training samples, we enhance the resilience of ISTD models against\nvarious challenges. Then, we introduce two-stage diffusion models for\nreal-world reconstruction. By aligning quantized signals closely with\nreal-world distributions, we significantly elevate the quality and fidelity of\nthe synthetic samples. Comparative evaluations against state-of-the-art\ndetection methods in various scarcity scenarios demonstrate the efficacy of the\nproposed approach.", "AI": {"tldr": "红外小目标检测（ISTD）在现实应用中面临数据稀缺导致性能脆弱的问题。本文研究了现有方法在数据稀缺下的表现，并提出了高斯不可知表示学习（G-ARL），包括高斯组压缩器（GGS）用于鲁棒表示学习，以及两阶段扩散模型用于高质量合成数据生成，有效提升了在数据稀缺场景下的检测性能。", "motivation": "红外小目标检测在实际应用中至关重要，但当前最先进的方法高度依赖大量昂贵的手动标注数据进行表示学习，导致它们在缺乏高质量红外数据的真实世界挑战中表现脆弱。现有理论受到数据稀缺场景下性能波动的挑战。", "method": "首先，研究了主流方法在不同数据稀缺程度下的检测性能变化。其次，引入了高斯不可知表示学习（Gaussian Agnostic Representation Learning），具体提出高斯组压缩器（Gaussian Group Squeezer），利用高斯采样和压缩进行非均匀量化，并通过利用多样化的训练样本增强模型韧性。然后，引入了两阶段扩散模型进行真实世界重建，通过将量化信号与真实世界分布对齐，显著提升了合成样本的质量和保真度。", "result": "在各种数据稀缺场景下，与最先进的检测方法进行对比评估，证明了所提出方法的有效性，显著提升了检测性能和合成样本的质量与保真度。", "conclusion": "所提出的高斯不可知表示学习方法，结合高斯组压缩器和两阶段扩散模型，有效解决了红外小目标检测在数据稀缺环境下的脆弱性问题，提高了模型在真实世界挑战中的检测性能和鲁棒性。"}}
{"id": "2507.18264", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.18264", "abs": "https://arxiv.org/abs/2507.18264", "authors": ["Nevidu Jayatilleke", "Nisansa de Silva"], "title": "Zero-shot OCR Accuracy of Low-Resourced Languages: A Comparative Analysis on Sinhala and Tamil", "comment": "10 pages, 4 figures, Accepted paper at Recent Advances in Natural\n  Language Processing (RANLP) 2025", "summary": "Solving the problem of Optical Character Recognition (OCR) on printed text\nfor Latin and its derivative scripts can now be considered settled due to the\nvolumes of research done on English and other High-Resourced Languages (HRL).\nHowever, for Low-Resourced Languages (LRL) that use unique scripts, it remains\nan open problem. This study presents a comparative analysis of the zero-shot\nperformance of six distinct OCR engines on two LRLs: Sinhala and Tamil. The\nselected engines include both commercial and open-source systems, aiming to\nevaluate the strengths of each category. The Cloud Vision API, Surya, Document\nAI, and Tesseract were evaluated for both Sinhala and Tamil, while Subasa OCR\nand EasyOCR were examined for only one language due to their limitations. The\nperformance of these systems was rigorously analysed using five measurement\ntechniques to assess accuracy at both the character and word levels. According\nto the findings, Surya delivered the best performance for Sinhala across all\nmetrics, with a WER of 2.61%. Conversely, Document AI excelled across all\nmetrics for Tamil, highlighted by a very low CER of 0.78%. In addition to the\nabove analysis, we also introduce a novel synthetic Tamil OCR benchmarking\ndataset.", "AI": {"tldr": "本研究比较了六种OCR引擎在低资源语言（斯里兰卡僧伽罗语和泰米尔语）上的零样本性能，发现不同引擎在不同语言上表现最佳，并引入了一个新的泰米尔语合成OCR基准数据集。", "motivation": "高资源语言的OCR问题已基本解决，但对于使用独特文字的低资源语言，OCR仍然是一个开放问题。", "method": "对六种OCR引擎（包括商业和开源系统，如Cloud Vision API、Surya、Document AI、Tesseract、Subasa OCR和EasyOCR）在斯里兰卡僧伽罗语和泰米尔语上的零样本性能进行了比较分析。使用五种测量技术评估字符和单词级别的准确性，并引入了一个新的合成泰米尔语OCR基准数据集。", "result": "Surya在所有指标上对僧伽罗语表现最佳，词错误率（WER）为2.61%。Document AI在所有指标上对泰米尔语表现突出，字符错误率（CER）为0.78%。", "conclusion": "对于低资源语言，没有一个OCR引擎能普遍表现最佳，性能取决于具体的语言。Surya和Document AI分别在僧伽罗语和泰米尔语上展现了优异的零样本性能。"}}
{"id": "2507.18173", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.18173", "abs": "https://arxiv.org/abs/2507.18173", "authors": ["Haodong Zhu", "Wenhao Dong", "Linlin Yang", "Hong Li", "Yuguang Yang", "Yangyang Ren", "Qingcheng Zhu", "Zichao Feng", "Changbai Li", "Shaohui Lin", "Runqi Wang", "Xiaoyan Luo", "Baochang Zhang"], "title": "WaveMamba: Wavelet-Driven Mamba Fusion for RGB-Infrared Object Detection", "comment": null, "summary": "Leveraging the complementary characteristics of visible (RGB) and infrared\n(IR) imagery offers significant potential for improving object detection. In\nthis paper, we propose WaveMamba, a cross-modality fusion method that\nefficiently integrates the unique and complementary frequency features of RGB\nand IR decomposed by Discrete Wavelet Transform (DWT). An improved detection\nhead incorporating the Inverse Discrete Wavelet Transform (IDWT) is also\nproposed to reduce information loss and produce the final detection results.\nThe core of our approach is the introduction of WaveMamba Fusion Block (WMFB),\nwhich facilitates comprehensive fusion across low-/high-frequency sub-bands.\nWithin WMFB, the Low-frequency Mamba Fusion Block (LMFB), built upon the Mamba\nframework, first performs initial low-frequency feature fusion with channel\nswapping, followed by deep fusion with an advanced gated attention mechanism\nfor enhanced integration. High-frequency features are enhanced using a strategy\nthat applies an ``absolute maximum\" fusion approach. These advancements lead to\nsignificant performance gains, with our method surpassing state-of-the-art\napproaches and achieving average mAP improvements of 4.5% on four benchmarks.", "AI": {"tldr": "该论文提出WaveMamba，一种跨模态融合方法，通过离散小波变换（DWT）分解RGB和IR图像的频率特征，并利用改进的检测头和WaveMamba融合块（WMFB）进行低频和高频特征融合，显著提升了目标检测性能。", "motivation": "利用可见光（RGB）和红外（IR）图像的互补特性，以显著提升目标检测的性能。", "method": "提出WaveMamba跨模态融合方法。首先，使用离散小波变换（DWT）分解RGB和IR图像的频率特征。其次，设计WaveMamba融合块（WMFB）进行全面的低频和高频子带融合：低频特征通过基于Mamba框架的低频Mamba融合块（LMFB）进行通道交换和门控注意力机制的深度融合；高频特征采用“绝对最大值”融合策略增强。最后，采用结合逆离散小波变换（IDWT）的改进检测头以减少信息损失并生成最终检测结果。", "result": "该方法显著提升了性能，超越了现有最先进的方法，并在四个基准测试中平均mAP提高了4.5%。", "conclusion": "WaveMamba方法通过有效融合RGB和IR图像的互补频率特征，显著提升了目标检测性能，证明了其在多模态目标检测领域的先进性和有效性。"}}
{"id": "2507.18323", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.18323", "abs": "https://arxiv.org/abs/2507.18323", "authors": ["Minje Park", "Jeonghwa Lim", "Taehyung Yu", "Sunghoon Joo"], "title": "A Multi-Dataset Benchmark for Semi-Supervised Semantic Segmentation in ECG Delineation", "comment": "6 pages, 2 figures", "summary": "Electrocardiogram (ECG) delineation, the segmentation of meaningful waveform\nfeatures, is critical for clinical diagnosis. Despite recent advances using\ndeep learning, progress has been limited by the scarcity of publicly available\nannotated datasets. Semi-supervised learning presents a promising solution by\nleveraging abundant unlabeled ECG data. In this study, we present the first\nsystematic benchmark for semi-supervised semantic segmentation (SemiSeg) in ECG\ndelineation. We curated and unified multiple public datasets, including\npreviously underused sources, to support robust and diverse evaluation. We\nadopted five representative SemiSeg algorithms from computer vision,\nimplemented them on two different architectures: the convolutional network and\nthe transformer, and evaluated them in two different settings: in-domain and\ncross-domain. Additionally, we propose ECG-specific training configurations and\naugmentation strategies and introduce a standardized evaluation framework. Our\nresults show that the transformer outperforms the convolutional network in\nsemi-supervised ECG delineation. We anticipate that our benchmark will serve as\na foundation for advancing semi-supervised ECG delineation methods and will\nfacilitate further research in this domain.", "AI": {"tldr": "该研究首次为心电图（ECG）波形特征描绘中的半监督语义分割（SemiSeg）建立了基准，并发现Transformer架构优于卷积网络。", "motivation": "心电图波形特征描绘对临床诊断至关重要，但公开可用的带注释数据集稀缺限制了深度学习的进展。半监督学习通过利用大量未标记的心电图数据，提供了一个有前景的解决方案。", "method": "研究构建并统一了多个公共数据集（包括此前未充分利用的来源），以支持鲁棒和多样化的评估。采用了计算机视觉领域的五种代表性SemiSeg算法，并在卷积网络和Transformer两种不同架构上实现。在域内和跨域两种设置下进行了评估。此外，提出了针对ECG的训练配置和增强策略，并引入了标准化的评估框架。", "result": "研究结果表明，在半监督心电图描绘任务中，Transformer架构的表现优于卷积网络。", "conclusion": "该基准有望为推进半监督心电图描绘方法奠定基础，并促进该领域的进一步研究。"}}
{"id": "2507.18294", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.18294", "abs": "https://arxiv.org/abs/2507.18294", "authors": ["Pritika Ramu", "Apoorv Saxena", "Meghanath M Y", "Varsha Sankar", "Debraj Basu"], "title": "StyleAdaptedLM: Enhancing Instruction Following Models with Efficient Stylistic Transfer", "comment": null, "summary": "Adapting LLMs to specific stylistic characteristics, like brand voice or\nauthorial tones, is crucial for enterprise communication but challenging to\nachieve from corpora which lacks instruction-response formatting without\ncompromising instruction adherence. We introduce StyleAdaptedLM, a framework\nthat efficiently transfers stylistic traits to instruction-following models\nusing Low-Rank Adaptation (LoRA). LoRA adapters are first trained on a base\nmodel with diverse unstructured stylistic corpora, then merged with a separate\ninstruction-following model. This enables robust stylistic customization\nwithout paired data or sacrificing task performance. Experiments across\nmultiple datasets and models demonstrate improved stylistic consistency while\npreserving instruction adherence, with human evaluations confirming\nbrand-specific convention uptake. StyleAdaptedLM offers an efficient path for\nstylistic personalization in LLMs.", "AI": {"tldr": "StyleAdaptedLM框架利用LoRA高效地将风格特征转移到遵循指令的大语言模型中，无需配对数据，且不牺牲指令遵循能力，从而实现稳健的风格定制。", "motivation": "将大语言模型（LLMs）适应特定风格（如品牌声音或作者语调）对企业通信至关重要，但从缺乏指令-响应格式的语料库中实现这一点具有挑战性，且可能损害指令遵循能力。", "method": "引入StyleAdaptedLM框架，使用低秩适应（LoRA）技术。首先，LoRA适配器在基础模型上使用多样化的非结构化风格语料库进行训练，然后将这些适配器与一个独立的指令遵循模型合并。", "result": "在多个数据集和模型上的实验表明，该方法在保持指令遵循能力的同时，提高了风格一致性。人工评估也证实了模型能采纳品牌特定的约定。", "conclusion": "StyleAdaptedLM为大语言模型中的风格个性化提供了一条高效的途径。"}}
{"id": "2507.18174", "categories": ["cs.CV", "cs.AR"], "pdf": "https://arxiv.org/pdf/2507.18174", "abs": "https://arxiv.org/abs/2507.18174", "authors": ["Rashed Al Amin", "Roman Obermaisser"], "title": "Real-Time Object Detection and Classification using YOLO for Edge FPGAs", "comment": "This paper has been accepted for the 67th International Symposium on\n  ELMAR 2025", "summary": "Object detection and classification are crucial tasks across various\napplication domains, particularly in the development of safe and reliable\nAdvanced Driver Assistance Systems (ADAS). Existing deep learning-based methods\nsuch as Convolutional Neural Networks (CNNs), Single Shot Detectors (SSDs), and\nYou Only Look Once (YOLO) have demonstrated high performance in terms of\naccuracy and computational speed when deployed on Field-Programmable Gate\nArrays (FPGAs). However, despite these advances, state-of-the-art YOLO-based\nobject detection and classification systems continue to face challenges in\nachieving resource efficiency suitable for edge FPGA platforms. To address this\nlimitation, this paper presents a resource-efficient real-time object detection\nand classification system based on YOLOv5 optimized for FPGA deployment. The\nproposed system is trained on the COCO and GTSRD datasets and implemented on\nthe Xilinx Kria KV260 FPGA board. Experimental results demonstrate a\nclassification accuracy of 99%, with a power consumption of 3.5W and a\nprocessing speed of 9 frames per second (FPS). These findings highlight the\neffectiveness of the proposed approach in enabling real-time,\nresource-efficient object detection and classification for edge computing\napplications.", "AI": {"tldr": "本文提出了一种资源高效的实时YOLOv5目标检测与分类系统，专为边缘FPGA优化，并在Xilinx Kria KV260上实现了高精度、低功耗和实时处理。", "motivation": "现有基于深度学习的目标检测方法（如CNN、SSD、YOLO）在FPGA上部署时，尽管在准确性和速度方面表现出色，但在实现适用于边缘FPGA平台的资源效率方面仍面临挑战。特别是在ADAS等对安全可靠性要求高的应用中，需要更高效的解决方案。", "method": "本文提出了一种基于YOLOv5的资源高效实时目标检测与分类系统，并针对FPGA部署进行了优化。该系统在COCO和GTSRD数据集上进行训练，并部署在Xilinx Kria KV260 FPGA开发板上。", "result": "实验结果表明，该系统实现了99%的分类准确率，功耗为3.5W，处理速度达到9帧/秒（FPS）。", "conclusion": "研究结果突显了所提出方法在实现边缘计算应用中实时、资源高效的目标检测与分类方面的有效性。"}}
{"id": "2507.18334", "categories": ["cs.CV", "cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.18334", "abs": "https://arxiv.org/abs/2507.18334", "authors": ["Ezhini Rasendiran R", "Chandresh Kumar Maurya"], "title": "Improving Bird Classification with Primary Color Additives", "comment": "5 pages (Accepted to Interspeech 2025)", "summary": "We address the problem of classifying bird species using their song\nrecordings, a challenging task due to environmental noise, overlapping\nvocalizations, and missing labels. Existing models struggle with low-SNR or\nmulti-species recordings. We hypothesize that birds can be classified by\nvisualizing their pitch pattern, speed, and repetition, collectively called\nmotifs. Deep learning models applied to spectrogram images help, but similar\nmotifs across species cause confusion. To mitigate this, we embed frequency\ninformation into spectrograms using primary color additives. This enhances\nspecies distinction and improves classification accuracy. Our experiments show\nthat the proposed approach achieves statistically significant gains over models\nwithout colorization and surpasses the BirdCLEF 2024 winner, improving F1 by\n7.3%, ROC-AUC by 6.2%, and CMAP by 6.6%. These results demonstrate the\neffectiveness of incorporating frequency information via colorization.", "AI": {"tldr": "该研究通过将频率信息编码为颜色添加到声谱图中，显著提高了鸟类鸣叫声分类的准确性，超越了现有模型。", "motivation": "鸟类鸣叫声分类面临环境噪音、重叠发声和标签缺失等挑战。现有模型在低信噪比或多物种录音中表现不佳，且不同物种间相似的鸣叫模式（motifs）容易导致混淆。", "method": "将鸟类鸣叫声的可视化特征（音高模式、速度和重复，统称为motifs）应用于深度学习模型。为了解决相似motifs导致的混淆，研究将频率信息通过原色添加剂嵌入到声谱图中，以增强物种间的区分度。", "result": "实验结果表明，所提出的方法在统计学上显著优于未进行色彩化的模型，并超越了BirdCLEF 2024的冠军模型，F1分数提高了7.3%，ROC-AUC提高了6.2%，CMAP提高了6.6%。", "conclusion": "通过色彩化整合频率信息能够有效提升鸟类物种分类的准确性。"}}
{"id": "2507.18305", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.18305", "abs": "https://arxiv.org/abs/2507.18305", "authors": ["Biao Yi", "Zekun Fei", "Jianing Geng", "Tong Li", "Lihai Nie", "Zheli Liu", "Yiming Li"], "title": "BadReasoner: Planting Tunable Overthinking Backdoors into Large Reasoning Models for Fun or Profit", "comment": null, "summary": "Large reasoning models (LRMs) have emerged as a significant advancement in\nartificial intelligence, representing a specialized class of large language\nmodels (LLMs) designed to tackle complex reasoning tasks. The defining\ncharacteristic of LRMs lies in their extensive chain-of-thought (CoT) reasoning\ncapabilities. In this paper, we identify a previously unexplored attack vector\nagainst LRMs, which we term \"overthinking backdoors\". We advance this concept\nby proposing a novel tunable backdoor, which moves beyond simple on/off attacks\nto one where an attacker can precisely control the extent of the model's\nreasoning verbosity. Our attack is implemented through a novel data poisoning\nmethodology. It pairs a tunable trigger-where the number of repetitions signals\nthe desired intensity-with a correspondingly verbose CoT response. These\nresponses are programmatically generated by instructing a teacher LLM to inject\na controlled number of redundant refinement steps into a correct reasoning\nprocess. The approach preserves output correctness, which ensures stealth and\nestablishes the attack as a pure resource-consumption vector. Extensive\nempirical results on various LRMs demonstrate that our method can reliably\ntrigger a controllable, multi-fold increase in the length of the reasoning\nprocess, without degrading the final answer's correctness. Our source code is\navailable at https://github.com/FZaKK/BadReasoner.", "AI": {"tldr": "本文提出了一种针对大型推理模型（LRMs）的“过度思考后门”攻击，通过数据投毒使攻击者能够精确控制模型推理的冗余程度，同时保持答案的正确性，从而消耗资源。", "motivation": "大型推理模型（LRMs）在复杂推理任务中展现出强大的思维链（CoT）能力。研究旨在发现并探索一种新的攻击向量，该向量不仅能触发攻击，还能精确控制模型推理的冗余程度，而不仅仅是简单的开/关式攻击。", "method": "提出“过度思考后门”概念，并设计了一种可调的后门攻击。通过新颖的数据投毒方法实现：将可调触发器（重复次数表示期望强度）与相应冗长的思维链响应配对。这些响应由教师LLM通过注入受控数量的冗余细化步骤来程序化生成，以确保输出正确性，从而实现隐蔽性和资源消耗攻击。", "result": "在各种LRMs上的广泛实证结果表明，该方法能可靠地触发可控的、多倍的推理过程长度增加，且不降低最终答案的正确性。", "conclusion": "研究成功识别并实现了一种名为“过度思考后门”的新型攻击向量，该攻击能够精确控制大型推理模型推理过程的冗余程度，作为一种纯粹的资源消耗手段，同时不影响模型输出的正确性。"}}
{"id": "2507.18176", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18176", "abs": "https://arxiv.org/abs/2507.18176", "authors": ["Abhishek Kaushik", "Norbert Haala", "Uwe Soergel"], "title": "Unsupervised Domain Adaptation for 3D LiDAR Semantic Segmentation Using Contrastive Learning and Multi-Model Pseudo Labeling", "comment": null, "summary": "Addressing performance degradation in 3D LiDAR semantic segmentation due to\ndomain shifts (e.g., sensor type, geographical location) is crucial for\nautonomous systems, yet manual annotation of target data is prohibitive. This\nstudy addresses the challenge using Unsupervised Domain Adaptation (UDA) and\nintroduces a novel two-stage framework to tackle it. Initially, unsupervised\ncontrastive learning at the segment level is used to pre-train a backbone\nnetwork, enabling it to learn robust, domain-invariant features without labels.\nSubsequently, a multi-model pseudo-labeling strategy is introduced, utilizing\nan ensemble of diverse state-of-the-art architectures (including projection,\nvoxel, hybrid, and cylinder-based methods). Predictions from these models are\naggregated via hard voting to generate high-quality, refined pseudo-labels for\nthe unlabeled target domain, mitigating single-model biases. The contrastively\npre-trained network is then fine-tuned using these robust pseudo-labels.\nExperiments adapting from SemanticKITTI to unlabeled target datasets\n(SemanticPOSS, SemanticSlamantic) demonstrate significant improvements in\nsegmentation accuracy compared to direct transfer and single-model UDA\napproaches. These results highlight the effectiveness of combining contrastive\npre-training with refined ensemble pseudo-labeling for bridging complex domain\ngaps without requiring target domain annotations.", "AI": {"tldr": "该研究提出一种新颖的两阶段无监督域适应（UDA）框架，用于解决3D LiDAR语义分割中的域漂移问题，结合对比预训练和多模型伪标签策略。", "motivation": "3D LiDAR语义分割在不同域（如传感器类型、地理位置）之间存在性能下降问题，而目标域的手动标注成本过高，这对于自动驾驶系统是关键挑战。", "method": "该方法采用两阶段框架：1. 使用段级无监督对比学习预训练骨干网络，以学习鲁棒的域不变特征。2. 引入多模型伪标签策略，通过集成多种最先进的架构（包括投影、体素、混合和圆柱体方法）并进行硬投票，为无标签目标域生成高质量的伪标签，然后使用这些伪标签微调预训练的网络。", "result": "实验结果表明，在将SemanticKITTI适应到无标签目标数据集（SemanticPOSS、SemanticSlamantic）时，该方法相较于直接迁移和单模型UDA方法，显著提高了分割精度。", "conclusion": "研究结果强调了结合对比预训练和精炼的集成伪标签策略的有效性，能够在不要求目标域标注的情况下，弥合复杂的域差距，从而提升3D LiDAR语义分割的性能。"}}
{"id": "2507.18392", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18392", "abs": "https://arxiv.org/abs/2507.18392", "authors": ["Asaf Yehudai", "Lilach Eden", "Yotam Perlitz", "Roy Bar-Haim", "Michal Shmueli-Scheuer"], "title": "CLEAR: Error Analysis via LLM-as-a-Judge Made Easy", "comment": null, "summary": "The evaluation of Large Language Models (LLMs) increasingly relies on other\nLLMs acting as judges. However, current evaluation paradigms typically yield a\nsingle score or ranking, answering which model is better but not why. While\nessential for benchmarking, these top-level scores obscure the specific,\nactionable reasons behind a model's performance. To bridge this gap, we\nintroduce CLEAR, an interactive, open-source package for LLM-based error\nanalysis. CLEAR first generates per-instance textual feedback, then it creates\na set of system-level error issues, and quantifies the prevalence of each\nidentified issue. Our package also provides users with an interactive dashboard\nthat allows for a comprehensive error analysis through aggregate\nvisualizations, applies interactive filters to isolate specific issues or score\nranges, and drills down to the individual instances that exemplify a particular\nbehavioral pattern. We demonstrate CLEAR analysis for RAG and Math benchmarks,\nand showcase its utility through a user case study.", "AI": {"tldr": "CLEAR是一个交互式、开源的LLM错误分析工具包，它通过生成实例级文本反馈、识别系统级错误并量化其普遍性，提供比单一分数更深入的性能洞察。", "motivation": "当前的LLM评估范式通常只提供单一分数或排名，回答“哪个模型更好”而非“为什么更好”，这掩盖了模型性能背后的具体可操作原因，阻碍了模型的改进。", "method": "CLEAR首先生成每个实例的文本反馈，然后创建一套系统级错误问题，并量化每个已识别问题的普遍性。它还提供一个交互式仪表板，支持通过聚合可视化进行全面错误分析，应用交互式过滤器隔离特定问题或分数范围，并深入查看代表特定行为模式的单个实例。", "result": "研究人员在RAG和数学基准测试中演示了CLEAR的分析能力，并通过用户案例研究展示了其效用。", "conclusion": "CLEAR通过提供具体、可操作的错误原因，弥合了现有评估方法的不足，使用户能够进行全面深入的LLM错误分析，从而更好地理解和改进模型性能。"}}
{"id": "2507.18338", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.18338", "abs": "https://arxiv.org/abs/2507.18338", "authors": ["Ieva Raminta Staliūnaitė", "Julius Cheng", "Andreas Vlachos"], "title": "Uncertainty Quantification for Evaluating Machine Translation Bias", "comment": null, "summary": "In machine translation (MT), when the source sentence includes a lexeme whose\ngender is not overtly marked, but whose target-language equivalent requires\ngender specification, the model must infer the appropriate gender from the\ncontext and/or external knowledge. Studies have shown that MT models exhibit\nbiased behaviour, relying on stereotypes even when they clash with contextual\ninformation. We posit that apart from confidently translating using the correct\ngender when it is evident from the input, models should also maintain\nuncertainty about the gender when it is ambiguous. Using recently proposed\nmetrics of semantic uncertainty, we find that models with high translation and\ngender accuracy on unambiguous instances do not necessarily exhibit the\nexpected level of uncertainty in ambiguous ones. Similarly, debiasing has\nindependent effects on ambiguous and unambiguous translation instances.", "AI": {"tldr": "机器翻译模型在处理性别模糊词时存在偏见，即使在明确的情况下表现良好，也未能在模糊情况下表现出预期的不确定性。去偏处理对明确和模糊翻译实例的影响是独立的。", "motivation": "机器翻译模型在源语言性别未明确标记但目标语言需要性别指定时，常表现出性别偏见，依赖刻板印象，即使与上下文冲突。研究者认为模型在性别模糊时应保持不确定性，而非盲目推断。", "method": "使用了最近提出的语义不确定性度量方法，来评估模型在处理性别模糊和非模糊实例时的表现。", "result": "1. 在性别明确的翻译实例上具有高翻译和性别准确性的模型，在性别模糊实例上不一定表现出预期的不确定性水平。\n2. 去偏处理对性别模糊和非模糊翻译实例具有独立的影响。", "conclusion": "机器翻译模型需要更好地处理性别模糊性，并在不确定时保持不确定性。去偏方法对不同类型的翻译实例（明确与模糊）可能产生不同的效果，需要更精细的考量。"}}
{"id": "2507.18184", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18184", "abs": "https://arxiv.org/abs/2507.18184", "authors": ["Hoang Hai Nam Nguyen", "Phan Nguyen Duc Hieu", "Ho Won Lee"], "title": "MatSSL: Robust Self-Supervised Representation Learning for Metallographic Image Segmentation", "comment": null, "summary": "MatSSL is a streamlined self-supervised learning (SSL) architecture that\nemploys Gated Feature Fusion at each stage of the backbone to integrate\nmulti-level representations effectively. Current micrograph analysis of\nmetallic materials relies on supervised methods, which require retraining for\neach new dataset and often perform inconsistently with only a few labeled\nsamples. While SSL offers a promising alternative by leveraging unlabeled data,\nmost existing methods still depend on large-scale datasets to be effective.\nMatSSL is designed to overcome this limitation. We first perform\nself-supervised pretraining on a small-scale, unlabeled dataset and then\nfine-tune the model on multiple benchmark datasets. The resulting segmentation\nmodels achieve 69.13% mIoU on MetalDAM, outperforming the 66.73% achieved by an\nImageNet-pretrained encoder, and delivers consistently up to nearly 40%\nimprovement in average mIoU on the Environmental Barrier Coating benchmark\ndataset (EBC) compared to models pretrained with MicroNet. This suggests that\nMatSSL enables effective adaptation to the metallographic domain using only a\nsmall amount of unlabeled data, while preserving the rich and transferable\nfeatures learned from large-scale pretraining on natural images.", "AI": {"tldr": "MatSSL是一种流线型的自监督学习（SSL）架构，通过门控特征融合有效整合多级表示，使其能够仅利用少量未标记数据对金属材料微观结构图像进行有效适应和分析，超越了现有方法。", "motivation": "当前金属材料显微图像分析依赖监督方法，需要为每个新数据集重新训练，且在标记样本稀少时表现不稳定。现有自监督学习方法虽有潜力，但大多仍依赖大规模数据集才能有效。MatSSL旨在克服这些局限性。", "method": "MatSSL采用门控特征融合（Gated Feature Fusion）在骨干网络的每个阶段集成多级表示。其训练流程包括在小规模未标记数据集上进行自监督预训练，然后针对多个基准数据集进行微调。", "result": "MatSSL在MetalDAM数据集上实现了69.13%的mIoU，优于ImageNet预训练编码器（66.73%）。在环境阻隔涂层（EBC）基准数据集上，与使用MicroNet预训练的模型相比，平均mIoU提高了近40%。", "conclusion": "MatSSL能够利用少量未标记数据有效适应金相领域，同时保留从大规模自然图像预训练中学习到的丰富且可迁移的特征，从而实现卓越的性能。"}}
{"id": "2507.18442", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18442", "abs": "https://arxiv.org/abs/2507.18442", "authors": ["Rana Alshaikh", "Israa Alghanmi", "Shelan Jeawak"], "title": "AraTable: Benchmarking LLMs' Reasoning and Understanding of Arabic Tabular Data", "comment": null, "summary": "The cognitive and reasoning abilities of large language models (LLMs) have\nenabled remarkable progress in natural language processing. However, their\nperformance in interpreting structured data, especially in tabular formats,\nremains limited. Although benchmarks for English tabular data are widely\navailable, Arabic is still underrepresented because of the limited availability\nof public resources and its unique language features. To address this gap, we\npresent AraTable, a novel and comprehensive benchmark designed to evaluate the\nreasoning and understanding capabilities of LLMs when applied to Arabic tabular\ndata. AraTable consists of various evaluation tasks, such as direct question\nanswering, fact verification, and complex reasoning, involving a wide range of\nArabic tabular sources. Our methodology follows a hybrid pipeline, where\ninitial content is generated by LLMs and subsequently filtered and verified by\nhuman experts to ensure high dataset quality. Initial analyses using AraTable\nshow that, while LLMs perform adequately on simpler tabular tasks such as\ndirect question answering, they continue to face significant cognitive\nchallenges when tasks require deeper reasoning and fact verification. This\nindicates that there are substantial opportunities for future work to improve\nperformance on complex tabular reasoning tasks. We also propose a fully\nautomated evaluation framework that uses a self-deliberation mechanism and\nachieves performance nearly identical to that of human judges. This research\nprovides a valuable, publicly available resource and evaluation framework that\ncan help accelerate the development of foundational models for processing and\nanalysing Arabic structured data.", "AI": {"tldr": "本文提出了AraTable，一个用于评估大型语言模型在阿拉伯语表格数据上推理和理解能力的综合基准，并揭示了LLM在复杂表格推理任务上的局限性。", "motivation": "尽管大型语言模型在自然语言处理方面取得了显著进展，但它们在解释结构化数据（尤其是表格数据）方面的性能仍然有限。此外，阿拉伯语表格数据领域因公共资源稀缺和语言特性而严重不足。", "method": "研究构建了AraTable基准，包含直接问答、事实核查和复杂推理等多种评估任务，涵盖广泛的阿拉伯语表格数据源。数据生成采用混合管道，由LLM初步生成内容，再经人类专家过滤和验证。同时，提出了一种利用自审议机制的自动化评估框架。", "result": "初步分析显示，LLM在直接问答等简单表格任务上表现尚可，但在需要更深层次推理和事实核查的复杂任务上仍面临显著认知挑战。此外，所提出的自动化评估框架其性能与人类评判几乎一致。", "conclusion": "AraTable为评估和改进LLM在阿拉伯语表格数据处理方面的能力提供了宝贵的公共资源和评估框架，有助于加速阿拉伯语结构化数据基础模型的发展。"}}
{"id": "2507.18340", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.18340", "abs": "https://arxiv.org/abs/2507.18340", "authors": ["Yifu Chen", "Bingchen Huang", "Zhiling Wang", "Yuanchao Du", "Junfeng Luo", "Lei Shen", "Zhineng chen"], "title": "TDR: Task-Decoupled Retrieval with Fine-Grained LLM Feedback for In-Context Learning", "comment": null, "summary": "In-context learning (ICL) has become a classic approach for enabling LLMs to\nhandle various tasks based on a few input-output examples. The effectiveness of\nICL heavily relies on the quality of these examples, and previous works which\nfocused on enhancing example retrieval capabilities have achieved impressive\nperformances. However, two challenges remain in retrieving high-quality\nexamples: (1) Difficulty in distinguishing cross-task data distributions, (2)\nDifficulty in making the fine-grained connection between retriever output and\nfeedback from LLMs. In this paper, we propose a novel framework called TDR. TDR\ndecouples the ICL examples from different tasks, which enables the retrieval\nmodule to retrieve examples specific to the target task within a multi-task\ndataset. Furthermore, TDR models fine-grained feedback from LLMs to supervise\nand guide the training of the retrieval module, which helps to retrieve\nhigh-quality examples. We conducted extensive experiments on a suite of 30 NLP\ntasks, the results demonstrate that TDR consistently improved results across\nall datasets and achieves state-of-the-art performance. Meanwhile, our approach\nis a plug-and-play method, which can be easily combined with various LLMs to\nimprove example retrieval abilities for ICL. The code is available at\nhttps://github.com/Nnn-s/TDR.", "AI": {"tldr": "本文提出TDR框架，通过解耦任务数据和利用LLM细粒度反馈，显著提升了上下文学习（ICL）的示例检索质量，在30个NLP任务上实现SOTA性能。", "motivation": "上下文学习（ICL）的效果严重依赖于示例质量，但现有示例检索方法面临两大挑战：1) 难以区分跨任务数据分布；2) 难以将检索器输出与LLM的细粒度反馈有效关联。", "method": "TDR框架通过以下两点解决问题：1) 解耦不同任务的ICL示例，使检索模块能针对目标任务检索特定示例；2) 建模LLM的细粒度反馈，用于监督和指导检索模块的训练，从而检索高质量示例。", "result": "在30个NLP任务上进行了广泛实验，结果表明TDR在所有数据集上均持续提升性能，并达到了最先进（SOTA）水平。此外，TDR是一种即插即用的方法，易于与各种LLM结合。", "conclusion": "TDR通过任务解耦和细粒度LLM反馈机制，有效解决了ICL示例检索中的关键挑战，显著提升了检索质量和ICL性能，并具有良好的通用性和易用性。"}}
{"id": "2507.18192", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18192", "abs": "https://arxiv.org/abs/2507.18192", "authors": ["Minghao Fu", "Guo-Hua Wang", "Xiaohao Chen", "Qing-Guo Chen", "Zhao Xu", "Weihua Luo", "Kaifu Zhang"], "title": "TeEFusion: Blending Text Embeddings to Distill Classifier-Free Guidance", "comment": "Accepted by ICCV 2025. The code is publicly available at\n  https://github.com/AIDC-AI/TeEFusion", "summary": "Recent advances in text-to-image synthesis largely benefit from sophisticated\nsampling strategies and classifier-free guidance (CFG) to ensure high-quality\ngeneration. However, CFG's reliance on two forward passes, especially when\ncombined with intricate sampling algorithms, results in prohibitively high\ninference costs. To address this, we introduce TeEFusion (\\textbf{Te}xt\n\\textbf{E}mbeddings \\textbf{Fusion}), a novel and efficient distillation method\nthat directly incorporates the guidance magnitude into the text embeddings and\ndistills the teacher model's complex sampling strategy. By simply fusing\nconditional and unconditional text embeddings using linear operations,\nTeEFusion reconstructs the desired guidance without adding extra parameters,\nsimultaneously enabling the student model to learn from the teacher's output\nproduced via its sophisticated sampling approach. Extensive experiments on\nstate-of-the-art models such as SD3 demonstrate that our method allows the\nstudent to closely mimic the teacher's performance with a far simpler and more\nefficient sampling strategy. Consequently, the student model achieves inference\nspeeds up to 6$\\times$ faster than the teacher model, while maintaining image\nquality at levels comparable to those obtained through the teacher's complex\nsampling approach. The code is publicly available at\n\\href{https://github.com/AIDC-AI/TeEFusion}{github.com/AIDC-AI/TeEFusion}.", "AI": {"tldr": "TeEFusion是一种高效的文本到图像生成模型蒸馏方法，通过融合文本嵌入并学习教师模型的复杂采样策略，显著降低推理成本，同时保持图像质量。", "motivation": "当前文本到图像生成模型（如SD3）依赖复杂的采样策略和分类器无关指导（CFG）以确保高质量生成，但这导致高昂的推理成本，因为CFG需要两次前向传播，且采样算法复杂。", "method": "TeEFusion通过线性操作直接融合条件和无条件文本嵌入，将指导强度融入文本嵌入中，从而在不增加额外参数的情况下重建所需指导。同时，它蒸馏了教师模型的复杂采样策略，使学生模型能从教师的输出中学习。", "result": "在SD3等先进模型上的实验表明，TeEFusion使学生模型能够以更简单、更高效的采样策略，紧密模仿教师模型的性能。学生模型推理速度比教师模型快6倍，同时图像质量与教师模型的复杂采样方法相当。", "conclusion": "TeEFusion通过创新的文本嵌入融合和采样策略蒸馏，有效解决了文本到图像生成中高推理成本的问题，使得学生模型在保持高质量图像生成的同时，显著提高了推理效率。"}}
{"id": "2507.18448", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2; I.7"], "pdf": "https://arxiv.org/pdf/2507.18448", "abs": "https://arxiv.org/abs/2507.18448", "authors": ["Md Obyedullahil Mamun", "Md Adyelullahil Mamun", "Arif Ahmad", "Md. Imran Hossain Emu"], "title": "Restoring Rhythm: Punctuation Restoration Using Transformer Models for Bangla, a Low-Resource Language", "comment": null, "summary": "Punctuation restoration enhances the readability of text and is critical for\npost-processing tasks in Automatic Speech Recognition (ASR), especially for\nlow-resource languages like Bangla. In this study, we explore the application\nof transformer-based models, specifically XLM-RoBERTa-large, to automatically\nrestore punctuation in unpunctuated Bangla text. We focus on predicting four\npunctuation marks: period, comma, question mark, and exclamation mark across\ndiverse text domains. To address the scarcity of annotated resources, we\nconstructed a large, varied training corpus and applied data augmentation\ntechniques. Our best-performing model, trained with an augmentation factor of\nalpha = 0.20%, achieves an accuracy of 97.1% on the News test set, 91.2% on the\nReference set, and 90.2% on the ASR set.\n  Results show strong generalization to reference and ASR transcripts,\ndemonstrating the model's effectiveness in real-world, noisy scenarios. This\nwork establishes a strong baseline for Bangla punctuation restoration and\ncontributes publicly available datasets and code to support future research in\nlow-resource NLP.", "AI": {"tldr": "本研究利用基于Transformer的XLM-RoBERTa-large模型，通过数据增强技术，实现了对孟加拉语无标点文本的自动标点恢复，并在不同数据集上取得了高准确率，为低资源语言NLP提供了基线。", "motivation": "标点恢复能增强文本可读性，对自动语音识别（ASR）的后处理至关重要，尤其对于孟加拉语等低资源语言，其缺乏带标注的资源。", "method": "采用基于Transformer的XLM-RoBERTa-large模型，预测四种标点符号（句号、逗号、问号、感叹号）。构建了大型多样化的训练语料库，并应用了数据增强技术（增强因子alpha = 0.20%）。", "result": "最佳模型在News测试集上达到97.1%的准确率，在Reference集上达到91.2%，在ASR集上达到90.2%。结果表明模型在真实、嘈杂场景下具有强大的泛化能力。", "conclusion": "本工作为孟加拉语标点恢复建立了一个强大的基线，并贡献了公开可用的数据集和代码，以支持未来在低资源NLP领域的研究。"}}
{"id": "2507.18343", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.18343", "abs": "https://arxiv.org/abs/2507.18343", "authors": ["Ariana Sahitaj", "Premtim Sahitaj", "Veronika Solopova", "Jiaao Li", "Sebastian Möller", "Vera Schmitt"], "title": "Hybrid Annotation for Propaganda Detection: Integrating LLM Pre-Annotations with Human Intelligence", "comment": "NLP4PI at ACL", "summary": "Propaganda detection on social media remains challenging due to task\ncomplexity and limited high-quality labeled data. This paper introduces a novel\nframework that combines human expertise with Large Language Model (LLM)\nassistance to improve both annotation consistency and scalability. We propose a\nhierarchical taxonomy that organizes 14 fine-grained propaganda techniques into\nthree broader categories, conduct a human annotation study on the HQP dataset\nthat reveals low inter-annotator agreement for fine-grained labels, and\nimplement an LLM-assisted pre-annotation pipeline that extracts propagandistic\nspans, generates concise explanations, and assigns local labels as well as a\nglobal label. A secondary human verification study shows significant\nimprovements in both agreement and time-efficiency. Building on this, we\nfine-tune smaller language models (SLMs) to perform structured annotation.\nInstead of fine-tuning on human annotations, we train on high-quality\nLLM-generated data, allowing a large model to produce these annotations and a\nsmaller model to learn to generate them via knowledge distillation. Our work\ncontributes towards the development of scalable and robust propaganda detection\nsystems, supporting the idea of transparent and accountable media ecosystems in\nline with SDG 16. The code is publicly available at our GitHub repository.", "AI": {"tldr": "本文提出一个结合人类专业知识和大型语言模型（LLM）辅助的框架，旨在提高社交媒体上宣传检测的标注一致性和可扩展性。该框架通过LLM辅助预标注并利用LLM生成的高质量数据，通过知识蒸馏训练小型语言模型进行结构化标注。", "motivation": "社交媒体上的宣传检测面临任务复杂性和高质量标注数据稀缺的挑战。", "method": "1. 提出一个将14种细粒度宣传技术组织成3个更广泛类别的分层分类法。2. 进行人工标注研究，发现细粒度标签的标注者间一致性较低。3. 实施LLM辅助的预标注流程，用于提取宣传片段、生成解释并分配局部及全局标签。4. 在LLM生成的高质量数据而非人工标注数据上微调小型语言模型（SLM），通过知识蒸馏实现结构化标注。", "result": "1. 初步人工标注研究显示细粒度标签的标注者间一致性低。2. 辅助LLM的预标注流程显著提高了标注一致性和时间效率。", "conclusion": "本研究有助于开发可扩展且鲁棒的宣传检测系统，支持建立透明和负责任的媒体生态系统，符合联合国可持续发展目标16。"}}
{"id": "2507.18214", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18214", "abs": "https://arxiv.org/abs/2507.18214", "authors": ["Qilin Huang", "Tianyu Lin", "Zhiguang Chen", "Fudan Zheng"], "title": "LEAF: Latent Diffusion with Efficient Encoder Distillation for Aligned Features in Medical Image Segmentation", "comment": "Accepted at MICCAI 2025", "summary": "Leveraging the powerful capabilities of diffusion models has yielded quite\neffective results in medical image segmentation tasks. However, existing\nmethods typically transfer the original training process directly without\nspecific adjustments for segmentation tasks. Furthermore, the commonly used\npre-trained diffusion models still have deficiencies in feature extraction.\nBased on these considerations, we propose LEAF, a medical image segmentation\nmodel grounded in latent diffusion models. During the fine-tuning process, we\nreplace the original noise prediction pattern with a direct prediction of the\nsegmentation map, thereby reducing the variance of segmentation results. We\nalso employ a feature distillation method to align the hidden states of the\nconvolutional layers with the features from a transformer-based vision encoder.\nExperimental results demonstrate that our method enhances the performance of\nthe original diffusion model across multiple segmentation datasets for\ndifferent disease types. Notably, our approach does not alter the model\narchitecture, nor does it increase the number of parameters or computation\nduring the inference phase, making it highly efficient.", "AI": {"tldr": "LEAF是一种基于潜在扩散模型的医学图像分割模型，通过直接预测分割图和特征蒸馏来改进扩散模型，在不增加推理成本的情况下显著提升了分割性能。", "motivation": "现有扩散模型在医学图像分割中直接沿用原始训练过程，未针对分割任务进行特定调整；常用的预训练扩散模型在特征提取方面仍存在不足。", "method": "提出LEAF模型，在微调阶段用直接预测分割图取代原始的噪声预测模式，以降低分割结果的方差；采用特征蒸馏方法，使卷积层的隐藏状态与基于Transformer的视觉编码器特征对齐。", "result": "实验结果表明，该方法在多个不同疾病类型的分割数据集上提升了原始扩散模型的性能。此外，该方法不改变模型架构，不增加推理阶段的参数量或计算量，效率高。", "conclusion": "LEAF通过对扩散模型进行针对性调整和特征增强，在医学图像分割任务中实现了性能的显著提升，同时保持了高效性，为扩散模型在医学图像分割领域的应用提供了有效途径。"}}
{"id": "2507.18451", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18451", "abs": "https://arxiv.org/abs/2507.18451", "authors": ["Basel Alshaikhdeeb", "Ahmed Abdelmonem Hemedan", "Soumyabrata Ghosh", "Irina Balaur", "Venkata Satagopam"], "title": "Generation of Synthetic Clinical Text: A Systematic Review", "comment": null, "summary": "Generating clinical synthetic text represents an effective solution for\ncommon clinical NLP issues like sparsity and privacy. This paper aims to\nconduct a systematic review on generating synthetic medical free-text by\nformulating quantitative analysis to three research questions concerning (i)\nthe purpose of generation, (ii) the techniques, and (iii) the evaluation\nmethods. We searched PubMed, ScienceDirect, Web of Science, Scopus, IEEE,\nGoogle Scholar, and arXiv databases for publications associated with generating\nsynthetic medical unstructured free-text. We have identified 94 relevant\narticles out of 1,398 collected ones. A great deal of attention has been given\nto the generation of synthetic medical text from 2018 onwards, where the main\npurpose of such a generation is towards text augmentation, assistive writing,\ncorpus building, privacy-preserving, annotation, and usefulness. Transformer\narchitectures were the main predominant technique used to generate the text,\nespecially the GPTs. On the other hand, there were four main aspects of\nevaluation, including similarity, privacy, structure, and utility, where\nutility was the most frequent method used to assess the generated synthetic\nmedical text. Although the generated synthetic medical text demonstrated a\nmoderate possibility to act as real medical documents in different downstream\nNLP tasks, it has proven to be a great asset as augmented, complementary to the\nreal documents, towards improving the accuracy and overcoming\nsparsity/undersampling issues. Yet, privacy is still a major issue behind\ngenerating synthetic medical text, where more human assessments are needed to\ncheck for the existence of any sensitive information. Despite that, advances in\ngenerating synthetic medical text will considerably accelerate the adoption of\nworkflows and pipeline development, discarding the time-consuming legalities of\ndata transfer.", "AI": {"tldr": "本文对合成临床自由文本的生成进行了系统综述，分析了其生成目的、所用技术和评估方法，旨在解决临床NLP中的数据稀疏性和隐私问题。", "motivation": "解决临床自然语言处理（NLP）中常见的数据稀疏性和隐私保护问题，为NLP任务提供有效的解决方案。", "method": "通过系统回顾的方式，在PubMed、ScienceDirect、Web of Science、Scopus、IEEE、Google Scholar和arXiv等数据库中搜索了与生成合成医学非结构化自由文本相关的出版物。共收集了1398篇文章，并从中识别出94篇相关文章。对生成目的、所用技术和评估方法进行了定量分析。", "result": "自2018年以来，合成医学文本生成领域受到广泛关注。主要生成目的包括文本增强、辅助写作、语料库构建、隐私保护、标注和实用性。Transformer架构（尤其是GPT模型）是主要的生成技术。评估主要集中在相似性、隐私、结构和实用性四个方面，其中实用性是最常用的评估方法。合成医学文本在下游NLP任务中作为真实文档表现出中等可能性，但在作为增强或补充数据以提高准确性和克服稀疏/欠采样问题方面表现出色。", "conclusion": "合成医学文本是NLP任务的宝贵资产，能有效提高准确性并解决数据稀疏问题，同时显著加速工作流程和管道开发，减少数据传输的法律障碍。然而，隐私仍然是一个主要问题，需要更多的人工评估来检查敏感信息的存在。"}}
{"id": "2507.18406", "categories": ["cs.CL", "cs.DB", "cs.DL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.18406", "abs": "https://arxiv.org/abs/2507.18406", "authors": ["Silvia Cappa", "Lingxiao Kong", "Pille-Riin Peet", "Fanfu Wei", "Yuchen Zhou", "Jan-Christoph Kalo"], "title": "Factual Inconsistencies in Multilingual Wikipedia Tables", "comment": "11 pages, 7 figures, White Paper for RTF Work at ISWS Summer School\n  2025", "summary": "Wikipedia serves as a globally accessible knowledge source with content in\nover 300 languages. Despite covering the same topics, the different versions of\nWikipedia are written and updated independently. This leads to factual\ninconsistencies that can impact the neutrality and reliability of the\nencyclopedia and AI systems, which often rely on Wikipedia as a main training\nsource. This study investigates cross-lingual inconsistencies in Wikipedia's\nstructured content, with a focus on tabular data. We developed a methodology to\ncollect, align, and analyze tables from Wikipedia multilingual articles,\ndefining categories of inconsistency. We apply various quantitative and\nqualitative metrics to assess multilingual alignment using a sample dataset.\nThese insights have implications for factual verification, multilingual\nknowledge interaction, and design for reliable AI systems leveraging Wikipedia\ncontent.", "AI": {"tldr": "本研究调查了维基百科跨语言版本中结构化内容（表格数据）的事实不一致性，开发了收集、对齐和分析表格的方法，并评估了这些不一致性对AI系统可靠性的影响。", "motivation": "维基百科是全球知识来源，内容涵盖300多种语言，但各语言版本独立编写和更新，导致事实不一致。这会影响百科全书的中立性和可靠性，尤其对于依赖维基百科作为主要训练来源的AI系统。", "method": "开发了一套方法来收集、对齐和分析维基百科多语言文章中的表格数据，并定义了不一致性的类别。使用样本数据集应用了多种定量和定性指标来评估多语言对齐情况。", "result": "研究发现了维基百科跨语言结构化内容中的不一致性，并通过定量和定性指标进行了评估。这些发现对事实核查、多语言知识交互以及设计利用维基百科内容的可靠AI系统具有重要意义。", "conclusion": "维基百科跨语言版本中存在结构化内容的不一致性，这对于事实核查、多语言知识交互以及构建依赖维基百科内容的可靠AI系统至关重要，需要加以关注和解决。"}}
{"id": "2507.18225", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18225", "abs": "https://arxiv.org/abs/2507.18225", "authors": ["Xin Wei", "Qin Yang", "Yijie Fang", "Mingrui Zhu", "Nannan Wang"], "title": "3D Test-time Adaptation via Graph Spectral Driven Point Shift", "comment": null, "summary": "While test-time adaptation (TTA) methods effectively address domain shifts by\ndynamically adapting pre-trained models to target domain data during online\ninference, their application to 3D point clouds is hindered by their irregular\nand unordered structure. Current 3D TTA methods often rely on computationally\nexpensive spatial-domain optimizations and may require additional training\ndata. In contrast, we propose Graph Spectral Domain Test-Time Adaptation\n(GSDTTA), a novel approach for 3D point cloud classification that shifts\nadaptation to the graph spectral domain, enabling more efficient adaptation by\ncapturing global structural properties with fewer parameters. Point clouds in\ntarget domain are represented as outlier-aware graphs and transformed into\ngraph spectral domain by Graph Fourier Transform (GFT). For efficiency,\nadaptation is performed by optimizing only the lowest 10% of frequency\ncomponents, which capture the majority of the point cloud's energy. An inverse\nGFT (IGFT) is then applied to reconstruct the adapted point cloud with the\ngraph spectral-driven point shift. This process is enhanced by an\neigenmap-guided self-training strategy that iteratively refines both the\nspectral adjustments and the model parameters. Experimental results and\nablation studies on benchmark datasets demonstrate the effectiveness of GSDTTA,\noutperforming existing TTA methods for 3D point cloud classification.", "AI": {"tldr": "GSDTTA是一种新颖的3D点云测试时间自适应方法，通过将自适应转移到图谱域，优化低频分量，实现高效且参数更少的自适应。", "motivation": "现有的3D点云测试时间自适应（TTA）方法在处理不规则和无序的3D点云时，常面临计算成本高昂、需要额外训练数据以及依赖耗时的空间域优化等挑战。", "method": "GSDTTA将目标域点云表示为异常值感知图，并通过图傅里叶变换（GFT）转换到图谱域。自适应过程仅优化最低10%的频率分量以提高效率。之后应用逆GFT（IGFT）重建自适应点云。该过程通过特征图引导的自训练策略进行增强，迭代地优化谱调整和模型参数。", "result": "在基准数据集上的实验结果和消融研究表明，GSDTTA在3D点云分类方面优于现有TTA方法。", "conclusion": "GSDTTA通过在图谱域进行自适应，提供了一种有效且高效的3D点云测试时间自适应解决方案，克服了现有方法的局限性。"}}
{"id": "2507.18457", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18457", "abs": "https://arxiv.org/abs/2507.18457", "authors": ["Luo Cheng", "Hanwei Zhang", "Lijun Zhang", "Holger Hermanns"], "title": "Revisiting Physically Realizable Adversarial Object Attack against LiDAR-based Detection: Clarifying Problem Formulation and Experimental Protocols", "comment": null, "summary": "Adversarial robustness in LiDAR-based 3D object detection is a critical\nresearch area due to its widespread application in real-world scenarios. While\nmany digital attacks manipulate point clouds or meshes, they often lack\nphysical realizability, limiting their practical impact. Physical adversarial\nobject attacks remain underexplored and suffer from poor reproducibility due to\ninconsistent setups and hardware differences. To address this, we propose a\ndevice-agnostic, standardized framework that abstracts key elements of physical\nadversarial object attacks, supports diverse methods, and provides open-source\ncode with benchmarking protocols in simulation and real-world settings. Our\nframework enables fair comparison, accelerates research, and is validated by\nsuccessfully transferring simulated attacks to a physical LiDAR system. Beyond\nthe framework, we offer insights into factors influencing attack success and\nadvance understanding of adversarial robustness in real-world LiDAR perception.", "AI": {"tldr": "提出一个与设备无关的标准化框架，以提高LiDAR物理对抗性攻击的可复现性、可比性，并成功将模拟攻击转移到物理系统。", "motivation": "数字对抗性攻击缺乏物理可实现性，而物理对抗性对象攻击研究不足且由于设置和硬件差异导致复现性差。", "method": "提出了一个与设备无关的标准化框架，抽象了物理对抗性对象攻击的关键元素，支持多种方法，并提供开源代码和模拟及真实世界基准测试协议。", "result": "该框架实现了公平比较，加速了研究，并通过将模拟攻击成功转移到物理LiDAR系统进行了验证。", "conclusion": "该框架不仅推进了研究，还提供了影响攻击成功因素的见解，加深了对真实世界LiDAR感知中对抗性鲁棒性的理解。"}}
{"id": "2507.18417", "categories": ["cs.CL", "cs.LG", "q-fin.ST", "q-fin.TR"], "pdf": "https://arxiv.org/pdf/2507.18417", "abs": "https://arxiv.org/abs/2507.18417", "authors": ["Giorgos Iacovides", "Wuyang Zhou", "Danilo Mandic"], "title": "FinDPO: Financial Sentiment Analysis for Algorithmic Trading through Preference Optimization of LLMs", "comment": null, "summary": "Opinions expressed in online finance-related textual data are having an\nincreasingly profound impact on trading decisions and market movements. This\ntrend highlights the vital role of sentiment analysis as a tool for quantifying\nthe nature and strength of such opinions. With the rapid development of\nGenerative AI (GenAI), supervised fine-tuned (SFT) large language models (LLMs)\nhave become the de facto standard for financial sentiment analysis. However,\nthe SFT paradigm can lead to memorization of the training data and often fails\nto generalize to unseen samples. This is a critical limitation in financial\ndomains, where models must adapt to previously unobserved events and the\nnuanced, domain-specific language of finance. To this end, we introduce FinDPO,\nthe first finance-specific LLM framework based on post-training human\npreference alignment via Direct Preference Optimization (DPO). The proposed\nFinDPO achieves state-of-the-art performance on standard sentiment\nclassification benchmarks, outperforming existing supervised fine-tuned models\nby 11% on the average. Uniquely, the FinDPO framework enables the integration\nof a fine-tuned causal LLM into realistic portfolio strategies through a novel\n'logit-to-score' conversion, which transforms discrete sentiment predictions\ninto continuous, rankable sentiment scores (probabilities). In this way,\nsimulations demonstrate that FinDPO is the first sentiment-based approach to\nmaintain substantial positive returns of 67% annually and strong risk-adjusted\nperformance, as indicated by a Sharpe ratio of 2.0, even under realistic\ntransaction costs of 5 basis points (bps).", "AI": {"tldr": "本文提出了FinDPO，首个基于直接偏好优化（DPO）的金融领域LLM框架，用于情感分析。它解决了传统SFT模型泛化性差的问题，在基准测试中表现出色，并在实际投资组合策略中实现了显著的正回报和风险调整收益。", "motivation": "在线金融文本数据中的观点对交易决策和市场走势影响深远，凸显了情感分析的重要性。然而，当前主流的监督微调（SFT）大型语言模型（LLMs）存在记忆训练数据和泛化能力差的问题，这在需要适应未见事件和领域特定语言的金融领域是致命缺陷。", "method": "本文引入了FinDPO框架，这是首个基于后训练人类偏好对齐（通过直接偏好优化DPO）的金融特定LLM框架。此外，FinDPO通过一种新颖的“logit-to-score”转换方法，将离散情感预测转化为连续、可排序的情感分数，从而将微调后的因果LLM整合到实际投资组合策略中。", "result": "FinDPO在标准情感分类基准测试中取得了最先进的性能，平均优于现有监督微调模型11%。在投资组合模拟中，即使在5个基点的交易成本下，FinDPO作为首个基于情感的方法，仍能保持每年67%的显著正回报和2.0的夏普比率，显示出强大的风险调整性能。", "conclusion": "FinDPO通过引入DPO克服了传统SFT模型在金融领域泛化性差的局限性，实现了情感分析的最新性能。其独特的“logit-to-score”转换使其能有效应用于实际投资组合策略，产生显著的积极回报和良好的风险调整性能，为金融情感分析和量化投资提供了新的范式。"}}
{"id": "2507.18237", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18237", "abs": "https://arxiv.org/abs/2507.18237", "authors": ["Chengchang Tian", "Jianwei Ma", "Yan Huang", "Zhanye Chen", "Honghao Wei", "Hui Zhang", "Wei Hong"], "title": "DATA: Domain-And-Time Alignment for High-Quality Feature Fusion in Collaborative Perception", "comment": "ICCV 2025, accepted as poster. 22 pages including supplementary\n  materials", "summary": "Feature-level fusion shows promise in collaborative perception (CP) through\nbalanced performance and communication bandwidth trade-off. However, its\neffectiveness critically relies on input feature quality. The acquisition of\nhigh-quality features faces domain gaps from hardware diversity and deployment\nconditions, alongside temporal misalignment from transmission delays. These\nchallenges degrade feature quality with cumulative effects throughout the\ncollaborative network. In this paper, we present the Domain-And-Time Alignment\n(DATA) network, designed to systematically align features while maximizing\ntheir semantic representations for fusion. Specifically, we propose a\nConsistency-preserving Domain Alignment Module (CDAM) that reduces domain gaps\nthrough proximal-region hierarchical downsampling and observability-constrained\ndiscriminator. We further propose a Progressive Temporal Alignment Module\n(PTAM) to handle transmission delays via multi-scale motion modeling and\ntwo-stage compensation. Building upon the aligned features, an Instance-focused\nFeature Aggregation Module (IFAM) is developed to enhance semantic\nrepresentations. Extensive experiments demonstrate that DATA achieves\nstate-of-the-art performance on three typical datasets, maintaining robustness\nwith severe communication delays and pose errors. The code will be released at\nhttps://github.com/ChengchangTian/DATA.", "AI": {"tldr": "本文提出DATA网络，通过一致性保持域对齐和渐进式时间对齐模块，系统性地解决协同感知中特征级融合面临的域差异和时间错位问题，并增强语义表示，从而提升性能和鲁棒性。", "motivation": "协同感知中的特征级融合在性能与通信带宽之间取得平衡，但其有效性严重依赖于输入特征的质量。硬件多样性、部署条件导致的域差异以及传输延迟引起的时间错位会累积性地降低特征质量，影响协同网络的性能。", "method": "本文提出了Domain-And-Time Alignment (DATA) 网络。具体包括：1) 一致性保持域对齐模块(CDAM)，通过近端区域分层下采样和可观测性约束判别器减少域差异。2) 渐进式时间对齐模块(PTAM)，通过多尺度运动建模和两阶段补偿处理传输延迟。3) 实例聚焦特征聚合模块(IFAM)，在对齐特征的基础上增强语义表示。", "result": "DATA网络在三个典型数据集上取得了最先进的性能，并在存在严重通信延迟和姿态误差的情况下保持了鲁棒性。", "conclusion": "DATA网络通过系统性地对齐特征并最大化其语义表示，有效解决了协同感知中特征级融合的域差异和时间错位问题，实现了卓越且鲁棒的性能。"}}
{"id": "2507.18484", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18484", "abs": "https://arxiv.org/abs/2507.18484", "authors": ["Xiao Yang", "Lingxuan Wu", "Lizhong Wang", "Chengyang Ying", "Hang Su", "Jun Zhu"], "title": "Reinforced Embodied Active Defense: Exploiting Adaptive Interaction for Robust Visual Perception in Adversarial 3D Environments", "comment": "arXiv admin note: text overlap with arXiv:2404.00540", "summary": "Adversarial attacks in 3D environments have emerged as a critical threat to\nthe reliability of visual perception systems, particularly in safety-sensitive\napplications such as identity verification and autonomous driving. These\nattacks employ adversarial patches and 3D objects to manipulate deep neural\nnetwork (DNN) predictions by exploiting vulnerabilities within complex scenes.\nExisting defense mechanisms, such as adversarial training and purification,\nprimarily employ passive strategies to enhance robustness. However, these\napproaches often rely on pre-defined assumptions about adversarial tactics,\nlimiting their adaptability in dynamic 3D settings. To address these\nchallenges, we introduce Reinforced Embodied Active Defense (Rein-EAD), a\nproactive defense framework that leverages adaptive exploration and interaction\nwith the environment to improve perception robustness in 3D adversarial\ncontexts. By implementing a multi-step objective that balances immediate\nprediction accuracy with predictive entropy minimization, Rein-EAD optimizes\ndefense strategies over a multi-step horizon. Additionally, Rein-EAD involves\nan uncertainty-oriented reward-shaping mechanism that facilitates efficient\npolicy updates, thereby reducing computational overhead and supporting\nreal-world applicability without the need for differentiable environments.\nComprehensive experiments validate the effectiveness of Rein-EAD, demonstrating\na substantial reduction in attack success rates while preserving standard\naccuracy across diverse tasks. Notably, Rein-EAD exhibits robust generalization\nto unseen and adaptive attacks, making it suitable for real-world complex\ntasks, including 3D object classification, face recognition and autonomous\ndriving.", "AI": {"tldr": "本文提出Rein-EAD，一种主动防御框架，通过强化学习和环境交互来提升3D感知系统在对抗攻击下的鲁棒性，有效降低攻击成功率并保持准确性。", "motivation": "3D环境中的对抗攻击对视觉感知系统构成严重威胁，尤其是在身份验证和自动驾驶等安全关键应用中。现有防御机制多为被动策略，依赖预设的攻击假设，在动态3D环境中适应性有限。", "method": "引入Rein-EAD框架，采用主动防御策略，利用自适应探索和环境交互来提升鲁棒性。通过多步目标优化，平衡即时预测准确性和预测熵最小化。此外，采用面向不确定性的奖励塑形机制，促进高效策略更新，降低计算开销，且无需可微分环境。", "result": "实验验证了Rein-EAD的有效性，显著降低了攻击成功率，同时在各种任务中保持了标准准确性。Rein-EAD对未知和自适应攻击表现出强大的泛化能力。", "conclusion": "Rein-EAD是一种有效且实用的主动防御框架，适用于3D物体分类、人脸识别和自动驾驶等复杂的现实世界任务，能显著增强感知系统在3D对抗环境中的鲁棒性。"}}
{"id": "2507.18504", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18504", "abs": "https://arxiv.org/abs/2507.18504", "authors": ["Zheyu Zhang", "Shuo Yang", "Bardh Prenkaj", "Gjergji Kasneci"], "title": "Not All Features Deserve Attention: Graph-Guided Dependency Learning for Tabular Data Generation with Language Models", "comment": null, "summary": "Large Language Models (LLMs) have shown strong potential for tabular data\ngeneration by modeling textualized feature-value pairs. However, tabular data\ninherently exhibits sparse feature-level dependencies, where many feature\ninteractions are structurally insignificant. This creates a fundamental\nmismatch as LLMs' self-attention mechanism inevitably distributes focus across\nall pairs, diluting attention on critical relationships, particularly in\ndatasets with complex dependencies or semantically ambiguous features. To\naddress this limitation, we propose GraDe (Graph-Guided Dependency Learning), a\nnovel method that explicitly integrates sparse dependency graphs into LLMs'\nattention mechanism. GraDe employs a lightweight dynamic graph learning module\nguided by externally extracted functional dependencies, prioritizing key\nfeature interactions while suppressing irrelevant ones. Our experiments across\ndiverse real-world datasets demonstrate that GraDe outperforms existing\nLLM-based approaches by up to 12% on complex datasets while achieving\ncompetitive results with state-of-the-art approaches in synthetic data quality.\nOur method is minimally intrusive yet effective, offering a practical solution\nfor structure-aware tabular data modeling with LLMs.", "AI": {"tldr": "该论文提出GraDe方法，通过将稀疏依赖图集成到LLM的注意力机制中，解决LLM在表格数据生成中因特征依赖稀疏性导致的注意力分散问题，从而提高生成质量。", "motivation": "LLM在表格数据生成中，其自注意力机制会不可避免地将注意力分散到所有特征对上，导致在关键关系上的注意力被稀释，尤其是在具有复杂依赖或语义模糊特征的数据集中，这与表格数据固有的稀疏特征级依赖性存在根本性不匹配。", "method": "提出GraDe（Graph-Guided Dependency Learning）方法。该方法将稀疏依赖图显式地集成到LLM的注意力机制中。GraDe采用一个轻量级的动态图学习模块，该模块由外部提取的功能依赖关系引导，优先处理关键特征交互，同时抑制不相关的交互。", "result": "在多样化的真实世界数据集上的实验表明，GraDe在复杂数据集上比现有基于LLM的方法性能提升高达12%，并在合成数据质量方面与最先进的方法具有竞争力。", "conclusion": "GraDe方法对LLM的侵入性小但效果显著，为使用LLM进行结构感知型表格数据建模提供了一个实用的解决方案。"}}
{"id": "2507.18255", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18255", "abs": "https://arxiv.org/abs/2507.18255", "authors": ["Zhuoguang Chen", "Minghui Qin", "Tianyuan Yuan", "Zhe Liu", "Hang Zhao"], "title": "LONG3R: Long Sequence Streaming 3D Reconstruction", "comment": "Accepted by ICCV 2025. Project page:\n  https://zgchen33.github.io/LONG3R/", "summary": "Recent advancements in multi-view scene reconstruction have been significant,\nyet existing methods face limitations when processing streams of input images.\nThese methods either rely on time-consuming offline optimization or are\nrestricted to shorter sequences, hindering their applicability in real-time\nscenarios. In this work, we propose LONG3R (LOng sequence streaming 3D\nReconstruction), a novel model designed for streaming multi-view 3D scene\nreconstruction over longer sequences. Our model achieves real-time processing\nby operating recurrently, maintaining and updating memory with each new\nobservation. We first employ a memory gating mechanism to filter relevant\nmemory, which, together with a new observation, is fed into a dual-source\nrefined decoder for coarse-to-fine interaction. To effectively capture\nlong-sequence memory, we propose a 3D spatio-temporal memory that dynamically\nprunes redundant spatial information while adaptively adjusting resolution\nalong the scene. To enhance our model's performance on long sequences while\nmaintaining training efficiency, we employ a two-stage curriculum training\nstrategy, each stage targeting specific capabilities. Experiments demonstrate\nthat LONG3R outperforms state-of-the-art streaming methods, particularly for\nlonger sequences, while maintaining real-time inference speed. Project page:\nhttps://zgchen33.github.io/LONG3R/.", "AI": {"tldr": "LONG3R是一种新颖的模型，专为长序列多视角流式3D场景重建设计，通过循环操作和创新的内存机制实现实时处理，并优于现有方法。", "motivation": "现有多视角场景重建方法在处理图像流时存在局限性，要么依赖耗时的离线优化，要么仅限于短序列，这阻碍了它们在实时场景中的应用。", "method": "LONG3R模型通过循环操作实现实时处理，并随新观测值维护和更新内存。它采用记忆门控机制过滤相关内存，并结合新观测值输入到双源精炼解码器进行粗到细的交互。为有效捕获长序列记忆，提出了一种3D时空记忆，该记忆动态修剪冗余空间信息并自适应调整场景分辨率。此外，模型采用两阶段课程训练策略以提高长序列性能并保持训练效率。", "result": "实验证明，LONG3R在长序列处理方面优于现有最先进的流式方法，同时保持了实时推理速度。", "conclusion": "LONG3R成功解决了长序列多视角流式3D重建的挑战，通过其创新的内存管理和训练策略，实现了实时高性能的场景重建。"}}
{"id": "2507.18512", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18512", "abs": "https://arxiv.org/abs/2507.18512", "authors": ["Clément Cornet", "Romaric Besançon", "Hervé Le Borgne"], "title": "Explaining How Visual, Textual and Multimodal Encoders Share Concepts", "comment": null, "summary": "Sparse autoencoders (SAEs) have emerged as a powerful technique for\nextracting human-interpretable features from neural networks activations.\nPrevious works compared different models based on SAE-derived features but\nthose comparisons have been restricted to models within the same modality. We\npropose a novel indicator allowing quantitative comparison of models across SAE\nfeatures, and use it to conduct a comparative study of visual, textual and\nmultimodal encoders. We also propose to quantify the Comparative Sharedness of\nindividual features between different classes of models. With these two new\ntools, we conduct several studies on 21 encoders of the three types, with two\nsignificantly different sizes, and considering generalist and domain specific\ndatasets. The results allow to revisit previous studies at the light of\nencoders trained in a multimodal context and to quantify to which extent all\nthese models share some representations or features. They also suggest that\nvisual features that are specific to VLMs among vision encoders are shared with\ntext encoders, highlighting the impact of text pretraining. The code is\navailable at https://github.com/CEA-LIST/SAEshareConcepts", "AI": {"tldr": "该研究提出了一种新的指标，用于定量比较不同模态（视觉、文本、多模态）编码器从稀疏自编码器（SAE）中提取的特征，并量化特征共享度，从而揭示了多模态预训练的影响。", "motivation": "以往基于SAE特征的模型比较仅限于相同模态的模型，缺乏跨模态的定量比较方法，因此需要开发新的工具来克服这一限制。", "method": "提出了一种新颖的指标，用于跨模态SAE特征的模型定量比较；同时提出量化不同类型模型之间个体特征的“比较共享度”（Comparative Sharedness）。使用这两种新工具，对21个不同大小和数据集（通用及领域特定）的视觉、文本和多模态编码器进行了比较研究。", "result": "研究结果能够重新审视多模态背景下训练的编码器，并量化这些模型共享表征或特征的程度。结果还表明，视觉语言模型（VLMs）中特有的视觉特征与文本编码器共享，突显了文本预训练的影响。", "conclusion": "该研究提出的新工具成功实现了跨模态编码器特征的定量比较，揭示了不同模型间表征的共享程度，并强调了文本预训练在视觉特征形成中的重要作用。"}}
{"id": "2507.18523", "categories": ["cs.CL", "cs.CY", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18523", "abs": "https://arxiv.org/abs/2507.18523", "authors": ["Maciej Skorski", "Alina Landowska"], "title": "The Moral Gap of Large Language Models", "comment": "preprint", "summary": "Moral foundation detection is crucial for analyzing social discourse and\ndeveloping ethically-aligned AI systems. While large language models excel\nacross diverse tasks, their performance on specialized moral reasoning remains\nunclear.\n  This study provides the first comprehensive comparison between\nstate-of-the-art LLMs and fine-tuned transformers across Twitter and Reddit\ndatasets using ROC, PR, and DET curve analysis.\n  Results reveal substantial performance gaps, with LLMs exhibiting high false\nnegative rates and systematic under-detection of moral content despite prompt\nengineering efforts. These findings demonstrate that task-specific fine-tuning\nremains superior to prompting for moral reasoning applications.", "AI": {"tldr": "本研究比较了大型语言模型（LLMs）和微调Transformer模型在道德基础检测任务上的表现，发现微调模型优于LLMs。", "motivation": "道德基础检测对于分析社会话语和开发符合伦理的AI系统至关重要，但LLMs在此类专业道德推理任务上的性能尚不明确。", "method": "使用ROC、PR和DET曲线分析，首次对最先进的LLMs（通过提示工程）和微调的Transformer模型在Twitter和Reddit数据集上进行了全面比较。", "result": "结果显示LLMs存在显著的性能差距，表现出高假阴性率和对道德内容的系统性低检测，即使进行了提示工程。微调模型在性能上明显优于LLMs。", "conclusion": "对于道德推理应用，任务特定的微调模型仍优于基于提示工程的LLMs。"}}
{"id": "2507.18287", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18287", "abs": "https://arxiv.org/abs/2507.18287", "authors": ["Wenran Zhang", "Huihuan Luo", "Linda Wei", "Ping Nie", "Yiqun Wu", "Dedong Yu"], "title": "Dissecting the Dental Lung Cancer Axis via Mendelian Randomization and Mediation Analysis", "comment": null, "summary": "Periodontitis and dental caries are common oral diseases affecting billions\nglobally. While observational studies suggest links between these conditions\nand lung cancer, causality remains uncertain. This study used two sample\nMendelian randomization (MR) to explore causal relationships between dental\ntraits (periodontitis, dental caries) and lung cancer subtypes, and to assess\nmediation by pulmonary function. Genetic instruments were derived from the\nlargest available genome wide association studies, including data from 487,823\ndental caries and 506,594 periodontitis cases, as well as lung cancer data from\nthe Transdisciplinary Research of Cancer in Lung consortium. Inverse variance\nweighting was the main analytical method; lung function mediation was assessed\nusing the delta method. The results showed a significant positive causal effect\nof dental caries on overall lung cancer and its subtypes. Specifically, a one\nstandard deviation increase in dental caries incidence was associated with a\n188.0% higher risk of squamous cell lung carcinoma (OR = 2.880, 95% CI =\n1.236--6.713, p = 0.014), partially mediated by declines in forced vital\ncapacity (FVC) and forced expiratory volume in one second (FEV1), accounting\nfor 5.124% and 5.890% of the total effect. No causal effect was found for\nperiodontitis. These findings highlight a causal role of dental caries in lung\ncancer risk and support integrating dental care and pulmonary function\nmonitoring into cancer prevention strategies.", "AI": {"tldr": "本研究通过孟德尔随机化方法发现龋齿与肺癌（尤其是鳞状细胞肺癌）之间存在显著因果关系，并部分通过肺功能下降介导，而牙周炎无此因果效应。", "motivation": "观察性研究提示口腔疾病（牙周炎、龋齿）与肺癌可能存在关联，但因果关系尚不明确。", "method": "采用两样本孟德尔随机化（MR）方法，利用大规模全基因组关联研究（GWAS）数据作为遗传工具（包括487,823例龋齿和506,594例牙周炎数据，以及肺癌联盟数据）。主要分析方法为逆方差加权法（IVW），肺功能介导作用通过delta方法评估。", "result": "龋齿对整体肺癌及其亚型（特别是鳞状细胞肺癌）存在显著正向因果效应。龋齿发病率每增加一个标准差，鳞状细胞肺癌风险增加188.0%（OR = 2.880）。这种效应部分由用力肺活量（FVC）和一秒用力呼气量（FEV1）下降介导，分别解释总效应的5.124%和5.890%。未发现牙周炎与肺癌的因果关系。", "conclusion": "研究结果表明龋齿是肺癌风险的一个因果因素，且部分通过肺功能受损介导。这提示应将口腔护理和肺功能监测纳入癌症预防策略。"}}
{"id": "2507.18546", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18546", "abs": "https://arxiv.org/abs/2507.18546", "authors": ["Urchade Zaratiana", "Gil Pasternak", "Oliver Boyd", "George Hurn-Maloney", "Ash Lewis"], "title": "GLiNER2: An Efficient Multi-Task Information Extraction System with Schema-Driven Interface", "comment": null, "summary": "Information extraction (IE) is fundamental to numerous NLP applications, yet\nexisting solutions often require specialized models for different tasks or rely\non computationally expensive large language models. We present GLiNER2, a\nunified framework that enhances the original GLiNER architecture to support\nnamed entity recognition, text classification, and hierarchical structured data\nextraction within a single efficient model. Built pretrained transformer\nencoder architecture, GLiNER2 maintains CPU efficiency and compact size while\nintroducing multi-task composition through an intuitive schema-based interface.\nOur experiments demonstrate competitive performance across extraction and\nclassification tasks with substantial improvements in deployment accessibility\ncompared to LLM-based alternatives. We release GLiNER2 as an open-source\npip-installable library with pre-trained models and documentation at\nhttps://github.com/fastino-ai/GLiNER2.", "AI": {"tldr": "GLiNER2是一个统一且高效的框架，基于预训练Transformer编码器，能在一个模型中处理命名实体识别、文本分类和分层数据提取等多种信息提取任务，且在部署可访问性上优于大型语言模型。", "motivation": "现有信息提取解决方案通常需要针对不同任务的专用模型，或依赖计算成本高昂的大型语言模型，缺乏统一且高效的替代方案。", "method": "GLiNER2在原有GLiNER架构基础上进行了增强，采用预训练Transformer编码器架构。通过直观的基于Schema的接口实现多任务组合，同时保持CPU效率和紧凑的模型大小。", "result": "GLiNER2在提取和分类任务上展现出具有竞争力的性能，并且与基于大型语言模型的替代方案相比，在部署可访问性方面有显著提升。", "conclusion": "GLiNER2提供了一个统一、高效且易于部署的信息提取解决方案，能够有效处理多种NLP任务，并已作为开源库发布。"}}
{"id": "2507.18542", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.18542", "abs": "https://arxiv.org/abs/2507.18542", "authors": ["João Ruano", "Gonçalo M. Correia", "Leonor Barreiros", "Afonso Mendes"], "title": "Effective Multi-Task Learning for Biomedical Named Entity Recognition", "comment": "Accepted at the 24th BioNLP workshop (ACL2025), 15 pages, 3 figures", "summary": "Biomedical Named Entity Recognition presents significant challenges due to\nthe complexity of biomedical terminology and inconsistencies in annotation\nacross datasets. This paper introduces SRU-NER (Slot-based Recurrent Unit NER),\na novel approach designed to handle nested named entities while integrating\nmultiple datasets through an effective multi-task learning strategy. SRU-NER\nmitigates annotation gaps by dynamically adjusting loss computation to avoid\npenalizing predictions of entity types absent in a given dataset. Through\nextensive experiments, including a cross-corpus evaluation and human assessment\nof the model's predictions, SRU-NER achieves competitive performance in\nbiomedical and general-domain NER tasks, while improving cross-domain\ngeneralization.", "AI": {"tldr": "本文提出SRU-NER，一种新的槽位循环单元命名实体识别方法，通过多任务学习和动态损失调整来处理嵌套实体和跨数据集标注不一致问题，并在生物医学和通用领域NER任务中表现出色。", "motivation": "生物医学命名实体识别面临术语复杂性和数据集标注不一致的挑战。", "method": "引入SRU-NER（基于槽位的循环单元NER），一种处理嵌套命名实体的新方法，通过有效多任务学习策略整合多个数据集。通过动态调整损失计算，避免惩罚特定数据集中不存在的实体类型预测，从而缓解标注差距。", "result": "通过广泛实验（包括跨语料库评估和人工评估），SRU-NER在生物医学和通用领域NER任务中取得了有竞争力的性能，并提高了跨领域泛化能力。", "conclusion": "SRU-NER有效解决了生物医学命名实体识别中的挑战，尤其在处理嵌套实体、整合多数据集和提高跨领域泛化方面表现突出。"}}
{"id": "2507.18300", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18300", "abs": "https://arxiv.org/abs/2507.18300", "authors": ["Jincheng Li", "Chunyu Xie", "Ji Ao", "Dawei Leng", "Yuhui Yin"], "title": "LMM-Det: Make Large Multimodal Models Excel in Object Detection", "comment": "Accepted at ICCV 2025", "summary": "Large multimodal models (LMMs) have garnered wide-spread attention and\ninterest within the artificial intelligence research and industrial\ncommunities, owing to their remarkable capability in multimodal understanding,\nreasoning, and in-context learning, among others. While LMMs have demonstrated\npromising results in tackling multimodal tasks like image captioning, visual\nquestion answering, and visual grounding, the object detection capabilities of\nLMMs exhibit a significant gap compared to specialist detectors. To bridge the\ngap, we depart from the conventional methods of integrating heavy detectors\nwith LMMs and propose LMM-Det, a simple yet effective approach that leverages a\nLarge Multimodal Model for vanilla object Detection without relying on\nspecialized detection modules. Specifically, we conduct a comprehensive\nexploratory analysis when a large multimodal model meets with object detection,\nrevealing that the recall rate degrades significantly compared with specialist\ndetection models. To mitigate this, we propose to increase the recall rate by\nintroducing data distribution adjustment and inference optimization tailored\nfor object detection. We re-organize the instruction conversations to enhance\nthe object detection capabilities of large multimodal models. We claim that a\nlarge multimodal model possesses detection capability without any extra\ndetection modules. Extensive experiments support our claim and show the\neffectiveness of the versatile LMM-Det. The datasets, models, and codes are\navailable at https://github.com/360CVGroup/LMM-Det.", "AI": {"tldr": "该研究提出LMM-Det，一种简单有效的方法，使大型多模态模型（LMMs）无需专用检测模块即可执行通用目标检测，缩小了LMMs与专业检测器在目标检测能力上的差距。", "motivation": "尽管大型多模态模型（LMMs）在多模态理解和推理方面表现出色，但在目标检测能力上与专业检测器相比存在显著差距。传统方法通常将笨重的检测器与LMMs集成，而本研究旨在探索LMMs自身进行目标检测的可能性。", "method": "研究通过深入分析LMMs在目标检测中的召回率下降问题，提出了数据分布调整和推理优化策略以提高召回率。此外，通过重新组织指令对话来增强LMMs的目标检测能力，从而实现无需额外检测模块的通用目标检测。", "result": "实验结果支持了大型多模态模型本身具备检测能力的论断，并证明了所提出的LMM-Det方法的有效性，使其能够进行通用的目标检测。", "conclusion": "大型多模态模型无需任何额外的检测模块，通过数据分布调整、推理优化和指令对话重组，即可有效实现目标检测功能，从而弥补了其与专业检测器在检测能力上的差距。"}}
{"id": "2507.18552", "categories": ["cs.CV", "cs.AI", "68T45, 68T50, 68U35,", "I.4.8; I.2.7; I.2.10; H.5.1"], "pdf": "https://arxiv.org/pdf/2507.18552", "abs": "https://arxiv.org/abs/2507.18552", "authors": ["Baoyao Yang", "Wanyun Li", "Dixin Chen", "Junxiang Chen", "Wenbin Yao", "Haifeng Lin"], "title": "VideoMind: An Omni-Modal Video Dataset with Intent Grounding for Deep-Cognitive Video Understanding", "comment": "7 pages; 14 figures", "summary": "This paper introduces VideoMind, a video-centric omni-modal dataset designed\nfor deep video content cognition and enhanced multi-modal feature\nrepresentation. The dataset comprises 103K video samples (3K reserved for\ntesting), each paired with audio and systematically detailed textual\ndescriptions. Specifically, every video and its audio is described across three\nhierarchical layers (factual, abstract, and intent), progressing from surface\nto depth. It contains over 22 million words, averaging ~225 words per sample.\nVideoMind's key distinction from existing datasets is its provision of intent\nexpressions, which require contextual integration across the entire video and\nare not directly observable. These deep-cognitive expressions are generated\nusing a Chain-of-Thought (COT) approach, prompting the mLLM through\nstep-by-step reasoning. Each description includes annotations for subject,\nplace, time, event, action, and intent, supporting downstream recognition\ntasks. Crucially, we establish a gold-standard benchmark with 3,000 manually\nvalidated samples for evaluating deep-cognitive video understanding. We design\nhybrid-cognitive retrieval experiments, scored by multi-level retrieval\nmetrics, to appropriately assess deep video comprehension. Evaluation results\nfor models (e.g., InternVideo, VAST, UMT-L) are released. VideoMind serves as a\npowerful benchmark for fine-grained cross-modal alignment and advances fields\nrequiring in-depth video understanding, such as emotion and intent recognition.\nThe data is publicly available on GitHub, HuggingFace, and OpenDataLab,\nhttps://github.com/cdx-cindy/VideoMind.", "AI": {"tldr": "本文介绍了VideoMind，一个以视频为中心的综合模态数据集，旨在促进深度视频内容认知和增强多模态特征表示。", "motivation": "现有数据集缺乏对视频深层意图的表达，且无法支持需要上下文整合的非直接可观察的深层认知表达，因此需要一个能支持深度视频内容认知和增强多模态特征表示的新数据集。", "method": "VideoMind数据集包含103K视频样本（3K用于测试），每个样本配有音频和系统化的分层（事实、抽象、意图）文本描述。意图表达通过Chain-of-Thought (COT) 方法使用mLLM生成。每份描述包含主体、地点、时间、事件、动作和意图的标注。论文建立了一个包含3,000个手动验证样本的黄金标准基准，并设计了混合认知检索实验，采用多级检索指标来评估深度视频理解。", "result": "VideoMind数据集已公开发布。论文还发布了InternVideo、VAST、UMT-L等模型的评估结果。", "conclusion": "VideoMind数据集为细粒度跨模态对齐提供了一个强大的基准，并推动了需要深度视频理解的领域发展，例如情感和意图识别。"}}
{"id": "2507.18562", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18562", "abs": "https://arxiv.org/abs/2507.18562", "authors": ["Jiafeng Xiong", "Yuting Zhao"], "title": "GIIFT: Graph-guided Inductive Image-free Multimodal Machine Translation", "comment": null, "summary": "Multimodal Machine Translation (MMT) has demonstrated the significant help of\nvisual information in machine translation. However, existing MMT methods face\nchallenges in leveraging the modality gap by enforcing rigid visual-linguistic\nalignment whilst being confined to inference within their trained multimodal\ndomains. In this work, we construct novel multimodal scene graphs to preserve\nand integrate modality-specific information and introduce GIIFT, a two-stage\nGraph-guided Inductive Image-Free MMT framework that uses a cross-modal Graph\nAttention Network adapter to learn multimodal knowledge in a unified fused\nspace and inductively generalize it to broader image-free translation domains.\nExperimental results on the Multi30K dataset of English-to-French and\nEnglish-to-German tasks demonstrate that our GIIFT surpasses existing\napproaches and achieves the state-of-the-art, even without images during\ninference. Results on the WMT benchmark show significant improvements over the\nimage-free translation baselines, demonstrating the strength of GIIFT towards\ninductive image-free inference.", "AI": {"tldr": "本文提出GIIFT框架，通过构建多模态场景图和使用跨模态图注意力网络适配器，学习并整合多模态知识，实现了在推理阶段无需图像的多模态机器翻译，并在多个基准测试中达到最先进水平。", "motivation": "现有的多模态机器翻译（MMT）方法在利用模态间隙方面面临挑战，因为它们强制执行刚性的视觉-语言对齐，并且推理仅限于其训练过的多模态领域。", "method": "1. 构建新颖的多模态场景图以保留并整合模态特异性信息。2. 引入GIIFT（图引导归纳式无图像MMT）两阶段框架。3. 使用跨模态图注意力网络（GAT）适配器在统一的融合空间中学习多模态知识。4. 将所学知识归纳推广到更广泛的无图像翻译领域。", "result": "1. 在Multi30K数据集（英译法、英译德任务）上，GIIFT超越现有方法并达到最先进水平，即使在推理时没有图像。2. 在WMT基准测试上，GIIFT比无图像翻译基线有显著改进。", "conclusion": "GIIFT框架能够有效地学习和泛化多模态知识，实现了在推理阶段无需图像的归纳式机器翻译，并在多个翻译任务中展现出卓越的性能，解决了现有MMT方法的局限性。"}}
{"id": "2507.18311", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18311", "abs": "https://arxiv.org/abs/2507.18311", "authors": ["Xiaomei Zhang", "Hanyu Zheng", "Xiangyu Zhu", "Jinghuan Wei", "Junhong Zou", "Zhen Lei", "Zhaoxiang Zhang"], "title": "Improving Large Vision-Language Models' Understanding for Field Data", "comment": null, "summary": "Large Vision-Language Models (LVLMs) have shown impressive capabilities\nacross a range of tasks that integrate visual and textual understanding, such\nas image captioning and visual question answering. These models are trained on\nlarge-scale image and video datasets paired with text, enabling them to bridge\nvisual perception and natural language processing. However, their application\nto scientific domains, especially in interpreting complex field data commonly\nused in the natural sciences, remains underexplored. In this work, we introduce\nFieldLVLM, a novel framework designed to improve large vision-language models'\nunderstanding of field data. FieldLVLM consists of two main components: a\nfield-aware language generation strategy and a data-compressed multimodal model\ntuning. The field-aware language generation strategy leverages a\nspecial-purpose machine learning pipeline to extract key physical features from\nfield data, such as flow classification, Reynolds number, and vortex patterns.\nThis information is then converted into structured textual descriptions that\nserve as a dataset. The data-compressed multimodal model tuning focuses on\nLVLMs with these generated datasets, using a data compression strategy to\nreduce the complexity of field inputs and retain only the most informative\nvalues. This ensures compatibility with the models language decoder and guides\nits learning more effectively. Experimental results on newly proposed benchmark\ndatasets demonstrate that FieldLVLM significantly outperforms existing methods\nin tasks involving scientific field data. Our findings suggest that this\napproach opens up new possibilities for applying large vision-language models\nto scientific research, helping bridge the gap between large models and\ndomain-specific discovery.", "AI": {"tldr": "FieldLVLM是一个新颖的框架，旨在提高大型视觉语言模型（LVLMs）对科学领域数据的理解能力，通过结合领域感知语言生成和数据压缩的多模态模型微调实现。", "motivation": "尽管大型视觉语言模型（LVLMs）在集成视觉和文本理解的任务中表现出色，但它们在科学领域，特别是解释自然科学中常用的复杂场数据方面的应用尚未得到充分探索。", "method": "FieldLVLM框架包含两个主要组件：1. 领域感知语言生成策略：利用专用机器学习管道从场数据中提取关键物理特征（如流分类、雷诺数、涡流模式），并将其转换为结构化文本描述作为数据集。2. 数据压缩的多模态模型微调：使用生成的数据集对LVLMs进行微调，采用数据压缩策略来减少场输入的复杂性并保留最具信息量的值，以确保与模型语言解码器兼容并有效指导学习。", "result": "在新提出的基准数据集上的实验结果表明，FieldLVLM在涉及科学场数据的任务中显著优于现有方法。", "conclusion": "该方法为将大型视觉语言模型应用于科学研究开辟了新的可能性，有助于弥合大型模型与特定领域发现之间的鸿沟。"}}
{"id": "2507.18584", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18584", "abs": "https://arxiv.org/abs/2507.18584", "authors": ["Xiaopeng Ke", "Hexuan Deng", "Xuebo Liu", "Jun Rao", "Zhenxi Song", "Jun Yu", "Min Zhang"], "title": "AQuilt: Weaving Logic and Self-Inspection into Low-Cost, High-Relevance Data Synthesis for Specialist LLMs", "comment": "32 pages, 4 figures", "summary": "Despite the impressive performance of large language models (LLMs) in general\ndomains, they often underperform in specialized domains. Existing approaches\ntypically rely on data synthesis methods and yield promising results by using\nunlabeled data to capture domain-specific features. However, these methods\neither incur high computational costs or suffer from performance limitations,\nwhile also demonstrating insufficient generalization across different tasks. To\naddress these challenges, we propose AQuilt, a framework for constructing\ninstruction-tuning data for any specialized domains from corresponding\nunlabeled data, including Answer, Question, Unlabeled data, Inspection, Logic,\nand Task type. By incorporating logic and inspection, we encourage reasoning\nprocesses and self-inspection to enhance model performance. Moreover,\ncustomizable task instructions enable high-quality data generation for any\ntask. As a result, we construct a dataset of 703k examples to train a powerful\ndata synthesis model. Experiments show that AQuilt is comparable to DeepSeek-V3\nwhile utilizing just 17% of the production cost. Further analysis demonstrates\nthat our generated data exhibits higher relevance to downstream tasks. Source\ncode, models, and scripts are available at https://github.com/Krueske/AQuilt.", "AI": {"tldr": "AQuilt是一个从无标签数据构建领域特定指令微调数据的框架，通过引入逻辑和自检提升模型性能，并以更低成本实现与现有SOTA模型相当的性能。", "motivation": "尽管大型语言模型（LLMs）在通用领域表现出色，但在专业领域常表现不佳。现有数据合成方法要么计算成本高昂，要么性能受限，且泛化能力不足。", "method": "提出AQuilt框架，用于从无标签数据构建任何专业领域的指令微调数据，包括答案（Answer）、问题（Question）、无标签数据（Unlabeled data）、检查（Inspection）、逻辑（Logic）和任务类型（Task type）。通过整合逻辑和检查，鼓励推理过程和自我检查以提升模型性能。可定制的任务指令确保了高质量数据生成。构建了70.3万个示例的数据集来训练数据合成模型。", "result": "实验表明，AQuilt在仅使用DeepSeek-V3 17%的生产成本下，性能可与之媲美。进一步分析显示，AQuilt生成的数据与下游任务的相关性更高。", "conclusion": "AQuilt提供了一种高效且有效的方法，通过从无标签数据生成高质量指令微调数据，将大型语言模型适应到专业领域，解决了现有方法的成本和性能限制问题。"}}
{"id": "2507.18570", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.18570", "abs": "https://arxiv.org/abs/2507.18570", "authors": ["Ganesh Sapkota", "Md Hasibur Rahman"], "title": "Hybrid Tokenization Strategy for DNA Language Model using Byte Pair Encoding and K-MER Methods", "comment": null, "summary": "This paper presents a novel hybrid tokenization strategy that enhances the\nperformance of DNA Language Models (DLMs) by combining 6-mer tokenization with\nByte Pair Encoding (BPE-600). Traditional k-mer tokenization is effective at\ncapturing local DNA sequence structures but often faces challenges, including\nuneven token distribution and a limited understanding of global sequence\ncontext. To address these limitations, we propose merging unique 6mer tokens\nwith optimally selected BPE tokens generated through 600 BPE cycles. This\nhybrid approach ensures a balanced and context-aware vocabulary, enabling the\nmodel to capture both short and long patterns within DNA sequences\nsimultaneously. A foundational DLM trained on this hybrid vocabulary was\nevaluated using next-k-mer prediction as a fine-tuning task, demonstrating\nsignificantly improved performance. The model achieved prediction accuracies of\n10.78% for 3-mers, 10.1% for 4-mers, and 4.12% for 5-mers, outperforming\nstate-of-the-art models such as NT, DNABERT2, and GROVER. These results\nhighlight the ability of the hybrid tokenization strategy to preserve both the\nlocal sequence structure and global contextual information in DNA modeling.\nThis work underscores the importance of advanced tokenization methods in\ngenomic language modeling and lays a robust foundation for future applications\nin downstream DNA sequence analysis and biological research.", "AI": {"tldr": "该论文提出了一种结合6-mer和BPE-600的新型混合分词策略，显著提升了DNA语言模型（DLMs）的性能，在下一k-mer预测任务中超越了现有先进模型。", "motivation": "传统的k-mer分词虽然能捕捉局部DNA结构，但存在分词分布不均和对全局序列上下文理解有限的问题。为了解决这些限制，需要一种更平衡且上下文感知的词汇表。", "method": "提出将独特的6-mer分词与经过600次BPE循环优选的BPE分词相结合，形成一种混合词汇表。在此混合词汇表上训练一个基础DLM，并通过下一k-mer预测作为微调任务进行评估。", "result": "该模型在3-mer预测中达到10.78%的准确率，4-mer预测中达到10.1%，5-mer预测中达到4.12%，性能优于NT、DNABERT2和GROVER等现有最先进模型。", "conclusion": "混合分词策略在DNA建模中能同时保留局部序列结构和全局上下文信息，强调了高级分词方法在基因组语言建模中的重要性，并为未来的DNA序列分析和生物学研究奠定了坚实基础。"}}
{"id": "2507.18327", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18327", "abs": "https://arxiv.org/abs/2507.18327", "authors": ["Jiangjun Peng", "Yisi Luo", "Xiangyong Cao", "Shuang Xu", "Deyu Meng"], "title": "Beyond Low-rankness: Guaranteed Matrix Recovery via Modified Nuclear Norm", "comment": "15 pages, 14 figures", "summary": "The nuclear norm (NN) has been widely explored in matrix recovery problems,\nsuch as Robust PCA and matrix completion, leveraging the inherent global\nlow-rank structure of the data. In this study, we introduce a new modified\nnuclear norm (MNN) framework, where the MNN family norms are defined by\nadopting suitable transformations and performing the NN on the transformed\nmatrix. The MNN framework offers two main advantages: (1) it jointly captures\nboth local information and global low-rankness without requiring trade-off\nparameter tuning; (2) Under mild assumptions on the transformation, we provided\nexact theoretical recovery guarantees for both Robust PCA and MC tasks-an\nachievement not shared by existing methods that combine local and global\ninformation. Thanks to its general and flexible design, MNN can accommodate\nvarious proven transformations, enabling a unified and effective approach to\nstructured low-rank recovery. Extensive experiments demonstrate the\neffectiveness of our method. Code and supplementary material are available at\nhttps://github.com/andrew-pengjj/modified_nuclear_norm.", "AI": {"tldr": "本文提出了一种新的修正核范数（MNN）框架，通过对矩阵进行变换后应用核范数，联合捕获数据的局部信息和全局低秩性，并在鲁棒主成分分析和矩阵补全任务中提供了精确的理论恢复保证。", "motivation": "传统的核范数在矩阵恢复问题中主要利用数据的全局低秩结构。然而，现有方法在结合局部信息和全局低秩性时，往往需要调优权衡参数，且难以提供精确的理论恢复保证。因此，需要一种能够同时捕获这两种信息，且具有理论支持的新方法。", "method": "该研究引入了一个修正核范数（MNN）框架。MNN家族范数通过对原始矩阵进行适当变换，然后对变换后的矩阵执行核范数来定义。这种设计使其能够适应各种已验证的变换。", "result": "1. MNN框架能够联合捕获局部信息和全局低秩性，且无需进行权衡参数调优。2. 在对变换施加温和假设下，为鲁棒主成分分析和矩阵补全任务提供了精确的理论恢复保证，这是现有结合局部和全局信息的方法所不具备的。3. 大量实验证明了该方法的有效性。", "conclusion": "MNN框架提供了一种通用、灵活且有效的结构化低秩恢复方法。它能够统一处理多种问题，并在同时捕获局部和全局信息方面表现出色，且具有坚实的理论基础和优异的实验性能。"}}
{"id": "2507.18594", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18594", "abs": "https://arxiv.org/abs/2507.18594", "authors": ["Xuecheng Bai", "Yuxiang Wang", "Boyu Hu", "Qinyuan Jie", "Chuanzhi Xu", "Hongru Xiao", "Kechen Li", "Vera Chung"], "title": "DRWKV: Focusing on Object Edges for Low-Light Image Enhancement", "comment": null, "summary": "Low-light image enhancement remains a challenging task, particularly in\npreserving object edge continuity and fine structural details under extreme\nillumination degradation. In this paper, we propose a novel model, DRWKV\n(Detailed Receptance Weighted Key Value), which integrates our proposed Global\nEdge Retinex (GER) theory, enabling effective decoupling of illumination and\nedge structures for enhanced edge fidelity. Secondly, we introduce Evolving WKV\nAttention, a spiral-scanning mechanism that captures spatial edge continuity\nand models irregular structures more effectively. Thirdly, we design the\nBilateral Spectrum Aligner (Bi-SAB) and a tailored MS2-Loss to jointly align\nluminance and chrominance features, improving visual naturalness and mitigating\nartifacts. Extensive experiments on five LLIE benchmarks demonstrate that DRWKV\nachieves leading performance in PSNR, SSIM, and NIQE while maintaining low\ncomputational complexity. Furthermore, DRWKV enhances downstream performance in\nlow-light multi-object tracking tasks, validating its generalization\ncapabilities.", "AI": {"tldr": "该论文提出了一种名为DRWKV的新型低光照图像增强模型，通过整合全局边缘Retinex理论、演进WKV注意力机制、双边光谱对齐器和定制损失函数，有效提升图像的边缘和细节保真度，同时保持低计算量和良好的泛化能力。", "motivation": "低光照图像增强仍是一个具有挑战性的任务，尤其是在极端光照退化条件下，难以有效保留物体边缘连续性和精细结构细节。", "method": ["提出DRWKV模型，并整合了全局边缘Retinex (GER) 理论，以有效解耦光照和边缘结构，从而增强边缘保真度。", "引入演进WKV注意力机制，该机制采用螺旋扫描方式捕捉空间边缘连续性并更有效地建模不规则结构。", "设计了双边光谱对齐器 (Bi-SAB) 和定制的MS2-损失函数，用于联合对齐亮度和色度特征，以提高视觉自然度并减轻伪影。"], "result": ["在五个低光照图像增强基准测试中，DRWKV在PSNR、SSIM和NIQE指标上均达到领先性能，同时保持较低的计算复杂度。", "DRWKV提升了低光照多目标跟踪等下游任务的性能，验证了其泛化能力。"], "conclusion": "DRWKV模型在低光照图像增强任务中取得了领先性能，有效解决了边缘和细节保留的挑战，并且在保持低计算复杂度的同时，展现了良好的泛化能力，适用于多种低光照视觉任务。"}}
{"id": "2507.18578", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.18578", "abs": "https://arxiv.org/abs/2507.18578", "authors": ["Feng Hong", "Geng Yu", "Yushi Ye", "Haicheng Huang", "Huangjie Zheng", "Ya Zhang", "Yanfeng Wang", "Jiangchao Yao"], "title": "Wide-In, Narrow-Out: Revokable Decoding for Efficient and Effective DLLMs", "comment": null, "summary": "Diffusion Large Language Models (DLLMs) have emerged as a compelling\nalternative to Autoregressive models, designed for fast parallel generation.\nHowever, existing DLLMs are plagued by a severe quality-speed trade-off, where\nfaster parallel decoding leads to significant performance degradation. We\nattribute this to the irreversibility of standard decoding in DLLMs, which is\neasily polarized into the wrong decoding direction along with early error\ncontext accumulation. To resolve this, we introduce Wide-In, Narrow-Out (WINO),\na training-free decoding algorithm that enables revokable decoding in DLLMs.\nWINO employs a parallel draft-and-verify mechanism, aggressively drafting\nmultiple tokens while simultaneously using the model's bidirectional context to\nverify and re-mask suspicious ones for refinement. Verified in open-source\nDLLMs like LLaDA and MMaDA, WINO is shown to decisively improve the\nquality-speed trade-off. For instance, on the GSM8K math benchmark, it\naccelerates inference by 6$\\times$ while improving accuracy by 2.58%; on\nFlickr30K captioning, it achieves a 10$\\times$ speedup with higher performance.\nMore comprehensive experiments are conducted to demonstrate the superiority and\nprovide an in-depth understanding of WINO.", "AI": {"tldr": "本文提出WINO，一种无训练的DLLM解码算法，通过可撤销解码机制显著改善了扩散大语言模型（DLLMs）的速度与质量权衡，解决了现有DLLMs快速并行解码导致性能下降的问题。", "motivation": "现有的扩散大语言模型（DLLMs）在快速并行生成时面临严重的质量-速度权衡问题，即更快的并行解码会导致显著的性能下降。作者认为这归因于DLLMs中标准解码的不可逆性，容易在早期错误上下文积累下偏向错误的解码方向。", "method": "本文引入了Wide-In, Narrow-Out (WINO)，一种无需训练的解码算法，实现了DLLMs中的可撤销解码。WINO采用并行草稿-验证机制，积极地草拟多个token，同时利用模型的双向上下文来验证并重新遮蔽可疑的token以进行细化。", "result": "WINO在开源DLLMs（如LLaDA和MMaDA）上得到验证，结果表明它显著改善了质量-速度权衡。例如，在GSM8K数学基准测试中，它将推理速度提高了6倍，同时准确率提高了2.58%；在Flickr30K图像描述任务中，它实现了10倍的速度提升并带来了更高的性能。论文还进行了更全面的实验来证明WINO的优越性。", "conclusion": "WINO通过引入可撤销解码机制，有效解决了DLLMs中困扰的质量-速度权衡问题，实现了更快的并行生成且不牺牲性能，甚至有所提升，为DLLMs的实际应用提供了重要进展。"}}
{"id": "2507.18330", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18330", "abs": "https://arxiv.org/abs/2507.18330", "authors": ["Gabriel Jarry", "Ramon Dalmau", "Philippe Very", "Franck Ballerini", "Stephania-Denisa Bocu"], "title": "GVCCS: A Dataset for Contrail Identification and Tracking on Visible Whole Sky Camera Sequences", "comment": null, "summary": "Aviation's climate impact includes not only CO2 emissions but also\nsignificant non-CO2 effects, especially from contrails. These ice clouds can\nalter Earth's radiative balance, potentially rivaling the warming effect of\naviation CO2. Physics-based models provide useful estimates of contrail\nformation and climate impact, but their accuracy depends heavily on the quality\nof atmospheric input data and on assumptions used to represent complex\nprocesses like ice particle formation and humidity-driven persistence.\nObservational data from remote sensors, such as satellites and ground cameras,\ncould be used to validate and calibrate these models. However, existing\ndatasets don't explore all aspect of contrail dynamics and formation: they\ntypically lack temporal tracking, and do not attribute contrails to their\nsource flights. To address these limitations, we present the Ground Visible\nCamera Contrail Sequences (GVCCS), a new open data set of contrails recorded\nwith a ground-based all-sky camera in the visible range. Each contrail is\nindividually labeled and tracked over time, allowing a detailed analysis of its\nlifecycle. The dataset contains 122 video sequences (24,228 frames) and\nincludes flight identifiers for contrails that form above the camera. As\nreference, we also propose a unified deep learning framework for contrail\nanalysis using a panoptic segmentation model that performs semantic\nsegmentation (contrail pixel identification), instance segmentation (individual\ncontrail separation), and temporal tracking in a single architecture. By\nproviding high-quality, temporally resolved annotations and a benchmark for\nmodel evaluation, our work supports improved contrail monitoring and will\nfacilitate better calibration of physical models. This sets the groundwork for\nmore accurate climate impact understanding and assessments.", "AI": {"tldr": "航空业的凝结尾迹对气候影响显著，但现有观测数据不足。本文提出了一个新的地面可见光凝结尾迹序列数据集（GVCCS）和一个统一的深度学习框架，以改善凝结尾迹监测和物理模型校准。", "motivation": "航空业的凝结尾迹对气候有重要影响，但当前物理模型精度受限于输入数据和复杂过程假设。现有观测数据集缺乏凝结尾迹的时间追踪和与源航班的关联，无法有效验证和校准模型。", "method": "开发了一个新的开放数据集“地面可见光凝结尾迹序列（GVCCS）”，通过地面全天空相机记录凝结尾迹，并对每个凝结尾迹进行单独标注、时间追踪和航班识别。同时，提出了一个统一的深度学习框架，使用全景分割模型实现语义分割（凝结尾迹像素识别）、实例分割（单个凝结尾迹分离）和时间追踪。", "result": "GVCCS数据集包含122个视频序列（24,228帧），实现了凝结尾迹的个体标记和时间追踪，并为相机上方的凝结尾迹提供了航班标识符。所提出的深度学习框架能够在一个架构中完成凝结尾迹的语义分割、实例分割和时间追踪。", "conclusion": "该工作提供了高质量、时间解析的凝结尾迹标注和模型评估基准，有助于改进凝结尾迹监测，并促进物理模型的校准，从而更准确地理解和评估航空业对气候的影响。"}}
{"id": "2507.18616", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18616", "abs": "https://arxiv.org/abs/2507.18616", "authors": ["Si-Woo Kim", "MinJu Jeon", "Ye-Chan Kim", "Soeun Lee", "Taewhan Kim", "Dong-Jin Kim"], "title": "SynC: Synthetic Image Caption Dataset Refinement with One-to-many Mapping for Zero-shot Image Captioning", "comment": "Accepted to ACM Multimedia 2025", "summary": "Zero-shot Image Captioning (ZIC) increasingly utilizes synthetic datasets\ngenerated by text-to-image (T2I) models to mitigate the need for costly manual\nannotation. However, these T2I models often produce images that exhibit\nsemantic misalignments with their corresponding input captions (e.g., missing\nobjects, incorrect attributes), resulting in noisy synthetic image-caption\npairs that can hinder model training. Existing dataset pruning techniques are\nlargely designed for removing noisy text in web-crawled data. However, these\nmethods are ill-suited for the distinct challenges of synthetic data, where\ncaptions are typically well-formed, but images may be inaccurate\nrepresentations. To address this gap, we introduce SynC, a novel framework\nspecifically designed to refine synthetic image-caption datasets for ZIC.\nInstead of conventional filtering or regeneration, SynC focuses on reassigning\ncaptions to the most semantically aligned images already present within the\nsynthetic image pool. Our approach employs a one-to-many mapping strategy by\ninitially retrieving multiple relevant candidate images for each caption. We\nthen apply a cycle-consistency-inspired alignment scorer that selects the best\nimage by verifying its ability to retrieve the original caption via\nimage-to-text retrieval. Extensive evaluations demonstrate that SynC\nconsistently and significantly improves performance across various ZIC models\non standard benchmarks (MS-COCO, Flickr30k, NoCaps), achieving state-of-the-art\nresults in several scenarios. SynC offers an effective strategy for curating\nrefined synthetic data to enhance ZIC.", "AI": {"tldr": "针对零样本图像字幕（ZIC）中由文本到图像（T2I）模型生成的合成数据存在的图像语义与字幕不匹配问题，SynC提出一种新颖的框架，通过在现有合成图像池中重新分配字幕到最语义对齐的图像，从而提升ZIC模型性能。", "motivation": "零样本图像字幕（ZIC）依赖于文本到图像（T2I）模型生成的合成数据集来避免昂贵的手动标注。然而，T2I模型生成的图像常与输入字幕存在语义不匹配（如缺少物体、属性错误），导致合成图像-字幕对噪声大，阻碍模型训练。现有数据集去噪技术主要针对网络爬取数据中的文本噪声，不适用于合成数据中字幕良好但图像不准确的独特挑战。", "method": "SynC框架旨在优化ZIC的合成图像-字幕数据集。它不采用传统的过滤或重新生成，而是专注于将字幕重新分配给合成图像池中已有的、语义最对齐的图像。该方法采用一对多映射策略，首先为每个字幕检索多个相关的候选图像，然后应用一个受循环一致性启发的对齐评分器，通过验证图像能否通过图文检索召回原始字幕来选择最佳图像。", "result": "广泛的评估表明，SynC在各种ZIC模型上，于标准基准（MS-COCO、Flickr30k、NoCaps）上持续显著提升性能，并在多个场景中达到最先进水平。", "conclusion": "SynC为整理精炼的合成数据以增强ZIC提供了一种有效的策略。"}}
{"id": "2507.18580", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.18580", "abs": "https://arxiv.org/abs/2507.18580", "authors": ["Jiahao Wang", "Ramen Liu", "Longhui Zhang", "Jing Li"], "title": "System Report for CCL25-Eval Task 10: SRAG-MAV for Fine-Grained Chinese Hate Speech Recognition", "comment": "8 pages, 3 figures, accepted as oral presentation at CCL25-Eval", "summary": "This paper presents our system for CCL25-Eval Task 10, addressing\nFine-Grained Chinese Hate Speech Recognition (FGCHSR). We propose a novel\nSRAG-MAV framework that synergistically integrates task reformulation(TR),\nSelf-Retrieval-Augmented Generation (SRAG), and Multi-Round Accumulative Voting\n(MAV). Our method reformulates the quadruplet extraction task into triplet\nextraction, uses dynamic retrieval from the training set to create contextual\nprompts, and applies multi-round inference with voting to improve output\nstability and performance. Our system, based on the Qwen2.5-7B model, achieves\na Hard Score of 26.66, a Soft Score of 48.35, and an Average Score of 37.505 on\nthe STATE ToxiCN dataset, significantly outperforming baselines such as GPT-4o\n(Average Score 15.63) and fine-tuned Qwen2.5-7B (Average Score 35.365). The\ncode is available at https://github.com/king-wang123/CCL25-SRAG-MAV.", "AI": {"tldr": "本文提出了一个名为SRAG-MAV的新颖框架，用于细粒度中文仇恨言论识别（FGCHSR），通过任务重构、自检索增强生成和多轮累积投票，显著提升了识别性能。", "motivation": "解决细粒度中文仇恨言论识别（FGCHSR）的挑战，旨在提高识别的准确性和稳定性。", "method": "提出了SRAG-MAV框架，包含三个核心组件：1. 任务重构：将四元组抽取任务重构为三元组抽取；2. 自检索增强生成（SRAG）：从训练集中动态检索上下文提示；3. 多轮累积投票（MAV）：通过多轮推理和投票提高输出稳定性和性能。该系统基于Qwen2.5-7B模型。", "result": "在STATE ToxiCN数据集上，系统取得了Hard Score 26.66、Soft Score 48.35和Average Score 37.505的成绩，显著优于基线模型（如GPT-4o的平均分15.63和微调Qwen2.5-7B的平均分35.365）。", "conclusion": "所提出的SRAG-MAV框架在细粒度中文仇恨言论识别任务上表现出色，显著超越了现有基线模型，证明了其在提升识别性能方面的有效性。"}}
{"id": "2507.18331", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18331", "abs": "https://arxiv.org/abs/2507.18331", "authors": ["Runmin Zhang", "Zhu Yu", "Si-Yuan Cao", "Lingyu Zhu", "Guangyi Zhang", "Xiaokai Bai", "Hui-Liang Shen"], "title": "Boosting Multi-View Indoor 3D Object Detection via Adaptive 3D Volume Construction", "comment": "Accepted by ICCV2025", "summary": "This work presents SGCDet, a novel multi-view indoor 3D object detection\nframework based on adaptive 3D volume construction. Unlike previous approaches\nthat restrict the receptive field of voxels to fixed locations on images, we\nintroduce a geometry and context aware aggregation module to integrate\ngeometric and contextual information within adaptive regions in each image and\ndynamically adjust the contributions from different views, enhancing the\nrepresentation capability of voxel features. Furthermore, we propose a sparse\nvolume construction strategy that adaptively identifies and selects voxels with\nhigh occupancy probabilities for feature refinement, minimizing redundant\ncomputation in free space. Benefiting from the above designs, our framework\nachieves effective and efficient volume construction in an adaptive way. Better\nstill, our network can be supervised using only 3D bounding boxes, eliminating\nthe dependence on ground-truth scene geometry. Experimental results demonstrate\nthat SGCDet achieves state-of-the-art performance on the ScanNet, ScanNet200\nand ARKitScenes datasets. The source code is available at\nhttps://github.com/RM-Zhang/SGCDet.", "AI": {"tldr": "SGCDet是一个新颖的多视角室内3D目标检测框架，通过自适应3D体素构建和几何上下文感知聚合模块，有效整合多视角信息并减少冗余计算，实现了最先进的性能。", "motivation": "现有方法将体素感受野限制在图像上的固定位置，导致特征表示能力受限且在自由空间中存在大量冗余计算，因此需要一种更自适应、高效的方式来整合多视角信息并优化体素构建。", "method": "SGCDet框架包含两个核心创新：1. 几何和上下文感知聚合模块：在每张图像中自适应区域内整合几何和上下文信息，并动态调整不同视角的贡献，以增强体素特征的表示能力。2. 稀疏体素构建策略：自适应识别并选择高占用概率的体素进行特征细化，从而最小化自由空间中的冗余计算。此外，该网络仅使用3D边界框进行监督，无需依赖真实场景几何。", "result": "SGCDet框架实现了有效且高效的自适应体素构建。在ScanNet、ScanNet200和ARKitScenes数据集上，其性能达到了最先进水平。", "conclusion": "SGCDet通过创新的自适应3D体素构建和特征聚合机制，显著提升了多视角室内3D目标检测的效率和准确性，且无需依赖场景几何信息，达到了领域领先的性能。"}}
{"id": "2507.18625", "categories": ["cs.CV", "cs.AI", "cs.MM", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.18625", "abs": "https://arxiv.org/abs/2507.18625", "authors": ["Shuqing Li", "Anson Y. Lam", "Yun Peng", "Wenxuan Wang", "Michael R. Lyu"], "title": "3D Software Synthesis Guided by Constraint-Expressive Intermediate Representation", "comment": null, "summary": "Graphical user interface (UI) software has undergone a fundamental\ntransformation from traditional two-dimensional (2D) desktop/web/mobile\ninterfaces to spatial three-dimensional (3D) environments. While existing work\nhas made remarkable success in automated 2D software generation, such as\nHTML/CSS and mobile app interface code synthesis, the generation of 3D software\nstill remains under-explored. Current methods for 3D software generation\nusually generate the 3D environments as a whole and cannot modify or control\nspecific elements in the software. Furthermore, these methods struggle to\nhandle the complex spatial and semantic constraints inherent in the real world.\nTo address the challenges, we present Scenethesis, a novel\nrequirement-sensitive 3D software synthesis approach that maintains formal\ntraceability between user specifications and generated 3D software. Scenethesis\nis built upon ScenethesisLang, a domain-specific language that serves as a\ngranular constraint-aware intermediate representation (IR) to bridge natural\nlanguage requirements and executable 3D software. It serves both as a\ncomprehensive scene description language enabling fine-grained modification of\n3D software elements and as a formal constraint-expressive specification\nlanguage capable of expressing complex spatial constraints. By decomposing 3D\nsoftware synthesis into stages operating on ScenethesisLang, Scenethesis\nenables independent verification, targeted modification, and systematic\nconstraint satisfaction. Our evaluation demonstrates that Scenethesis\naccurately captures over 80% of user requirements and satisfies more than 90%\nof hard constraints while handling over 100 constraints simultaneously.\nFurthermore, Scenethesis achieves a 42.8% improvement in BLIP-2 visual\nevaluation scores compared to the state-of-the-art method.", "AI": {"tldr": "Scenethesis是一种新型的、对需求敏感的3D软件合成方法，它通过引入领域特定语言ScenethesisLang，实现了从自然语言需求到可执行3D软件的精细化生成与修改，并有效处理复杂的空间和语义约束。", "motivation": "传统2D UI已向3D空间环境转型，但现有的3D软件生成方法仍有局限：它们通常整体生成3D环境，无法对特定元素进行修改或控制，并且难以处理现实世界中复杂的空间和语义约束。", "method": "Scenethesis方法基于ScenethesisLang（一种领域特定语言），作为连接自然语言需求和可执行3D软件的粒度约束感知中间表示（IR）。ScenethesisLang既是全面的场景描述语言，支持3D软件元素的精细修改，也是形式化的约束表达语言，能够表达复杂的空间约束。通过将3D软件合成分解为在ScenethesisLang上操作的多个阶段，Scenethesis实现了独立验证、目标修改和系统性的约束满足。", "result": "Scenethesis能够准确捕获超过80%的用户需求，满足90%以上的硬约束，同时处理超过100个约束。与现有最先进的方法相比，Scenethesis在BLIP-2视觉评估分数上实现了42.8%的提升。", "conclusion": "Scenethesis提供了一种有效且创新的3D软件合成方法，解决了现有技术在精细控制和复杂约束处理方面的不足，显著提高了从用户需求生成高质量3D软件的能力。"}}
{"id": "2507.18618", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18618", "abs": "https://arxiv.org/abs/2507.18618", "authors": ["Andreea Nica", "Ivan Zakazov", "Nicolas Mario Baldwin", "Saibo Geng", "Robert West"], "title": "TRPrompt: Bootstrapping Query-Aware Prompt Optimization from Textual Rewards", "comment": null, "summary": "Prompt optimization improves the reasoning abilities of large language models\n(LLMs) without requiring parameter updates to the target model. Following\nheuristic-based \"Think step by step\" approaches, the field has evolved in two\nmain directions: while one group of methods uses textual feedback to elicit\nimproved prompts from general-purpose LLMs in a training-free way, a concurrent\nline of research relies on numerical rewards to train a special prompt model,\ntailored for providing optimal prompts to the target model. In this paper, we\nintroduce the Textual Reward Prompt framework (TRPrompt), which unifies these\napproaches by directly incorporating textual feedback into training of the\nprompt model. Our framework does not require prior dataset collection and is\nbeing iteratively improved with the feedback on the generated prompts. When\ncoupled with the capacity of an LLM to internalize the notion of what a \"good\"\nprompt is, the high-resolution signal provided by the textual rewards allows us\nto train a prompt model yielding state-of-the-art query-specific prompts for\nthe problems from the challenging math datasets GSMHard and MATH.", "AI": {"tldr": "本文提出了TRPrompt框架，通过将文本反馈直接整合到提示模型训练中，统一了基于文本反馈和数值奖励的提示优化方法，从而为大型语言模型生成高质量、任务特定的提示，并在数学推理任务上达到了最先进的性能。", "motivation": "提示优化无需更新LLM参数即可提升其推理能力。现有方法分为两类：一类使用文本反馈启发式生成提示（无需训练），另一类使用数值奖励训练专门的提示模型。这两种方法各有优缺点，需要一种能结合两者优势的新框架。", "method": "引入了Textual Reward Prompt (TRPrompt) 框架，该框架将文本反馈直接整合到提示模型的训练过程中。它不需要预先收集数据集，并通过对生成提示的反馈进行迭代改进。结合LLM对“好”提示的内在理解，文本奖励提供高分辨率信号来训练提示模型。", "result": "TRPrompt框架能够训练出一个提示模型，为挑战性数学数据集（GSMHard和MATH）的问题生成最先进的查询特定提示。", "conclusion": "TRPrompt成功地将文本反馈融入提示模型训练，提供了一种有效且迭代优化的方法来生成高质量提示，显著提升了LLM在复杂推理任务上的表现。"}}
{"id": "2507.18342", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18342", "abs": "https://arxiv.org/abs/2507.18342", "authors": ["Yuping He", "Yifei Huang", "Guo Chen", "Baoqi Pei", "Jilan Xu", "Tong Lu", "Jiangmiao Pang"], "title": "EgoExoBench: A Benchmark for First- and Third-person View Video Understanding in MLLMs", "comment": null, "summary": "Transferring and integrating knowledge across first-person (egocentric) and\nthird-person (exocentric) viewpoints is intrinsic to human intelligence,\nenabling humans to learn from others and convey insights from their own\nexperiences. Despite rapid progress in multimodal large language models\n(MLLMs), their ability to perform such cross-view reasoning remains unexplored.\nTo address this, we introduce EgoExoBench, the first benchmark for\negocentric-exocentric video understanding and reasoning. Built from publicly\navailable datasets, EgoExoBench comprises over 7,300 question-answer pairs\nspanning eleven sub-tasks organized into three core challenges: semantic\nalignment, viewpoint association, and temporal reasoning. We evaluate 13\nstate-of-the-art MLLMs and find that while these models excel on single-view\ntasks, they struggle to align semantics across perspectives, accurately\nassociate views, and infer temporal dynamics in the ego-exo context. We hope\nEgoExoBench can serve as a valuable resource for research on embodied agents\nand intelligent assistants seeking human-like cross-view intelligence.", "AI": {"tldr": "本文介绍了EgoExoBench，首个用于评估多模态大语言模型（MLLMs）在第一人称（自我中心）和第三人称（外部中心）视角之间进行跨视角理解和推理能力的基准测试。", "motivation": "人类智能的核心在于跨视角知识的整合与转移，这使得人类能够相互学习并分享经验。尽管多模态大语言模型取得了快速进展，但它们在跨视角推理方面的能力尚未被探索，因此需要一个专门的基准来评估这一能力。", "method": "研究者构建了EgoExoBench，一个包含7300多个问答对的基准测试，数据来源于公开数据集。该基准涵盖了语义对齐、视角关联和时间推理三大核心挑战下的十一个子任务。研究者使用此基准评估了13个最先进的MLLMs。", "result": "评估结果显示，尽管这些MLLMs在单视角任务上表现出色，但它们在自我中心-外部中心背景下进行跨视角语义对齐、准确关联不同视角以及推断时间动态方面表现不佳。", "conclusion": "EgoExoBench有望成为开发具备类人跨视角智能的具身智能体和智能助手的宝贵资源，推动该领域的研究进展。"}}
{"id": "2507.18632", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.18632", "abs": "https://arxiv.org/abs/2507.18632", "authors": ["Ye-Chan Kim", "SeungJu Cha", "Si-Woo Kim", "Taewhan Kim", "Dong-Jin Kim"], "title": "SIDA: Synthetic Image Driven Zero-shot Domain Adaptation", "comment": "Accepted to ACM MM 2025", "summary": "Zero-shot domain adaptation is a method for adapting a model to a target\ndomain without utilizing target domain image data. To enable adaptation without\ntarget images, existing studies utilize CLIP's embedding space and text\ndescription to simulate target-like style features. Despite the previous\nachievements in zero-shot domain adaptation, we observe that these text-driven\nmethods struggle to capture complex real-world variations and significantly\nincrease adaptation time due to their alignment process. Instead of relying on\ntext descriptions, we explore solutions leveraging image data, which provides\ndiverse and more fine-grained style cues. In this work, we propose SIDA, a\nnovel and efficient zero-shot domain adaptation method leveraging synthetic\nimages. To generate synthetic images, we first create detailed, source-like\nimages and apply image translation to reflect the style of the target domain.\nWe then utilize the style features of these synthetic images as a proxy for the\ntarget domain. Based on these features, we introduce Domain Mix and Patch Style\nTransfer modules, which enable effective modeling of real-world variations. In\nparticular, Domain Mix blends multiple styles to expand the intra-domain\nrepresentations, and Patch Style Transfer assigns different styles to\nindividual patches. We demonstrate the effectiveness of our method by showing\nstate-of-the-art performance in diverse zero-shot adaptation scenarios,\nparticularly in challenging domains. Moreover, our approach achieves high\nefficiency by significantly reducing the overall adaptation time.", "AI": {"tldr": "本文提出SIDA，一种新颖高效的零样本域适应方法，通过生成合成图像并利用其风格特征来克服现有文本驱动方法的局限性，实现了更好的性能和更快的适应时间。", "motivation": "现有的零样本域适应方法依赖CLIP的嵌入空间和文本描述来模拟目标域风格，但这些文本驱动方法难以捕捉复杂的真实世界变化，并且由于对齐过程导致适应时间显著增加。研究者旨在探索利用图像数据（提供更多样和细粒度的风格线索）的解决方案。", "method": "提出SIDA方法，利用合成图像进行零样本域适应。首先创建详细的源域相似图像，然后应用图像翻译以反映目标域风格，生成合成图像。接着，将这些合成图像的风格特征作为目标域的代理。在此基础上，引入域混合（Domain Mix）模块来融合多种风格以扩展域内表示，以及补丁风格迁移（Patch Style Transfer）模块将不同风格分配给单个图像块，从而有效建模真实世界变化。", "result": "SIDA方法在各种零样本适应场景，特别是在挑战性域中，展示了最先进的性能。此外，该方法显著减少了整体适应时间，实现了高效率。", "conclusion": "通过利用合成图像及其风格特征，并结合域混合和补丁风格迁移模块，SIDA方法能够有效且高效地进行零样本域适应，克服了传统文本驱动方法的局限性，并在复杂真实世界场景中表现出色。"}}
{"id": "2507.18624", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.18624", "abs": "https://arxiv.org/abs/2507.18624", "authors": ["Vijay Viswanathan", "Yanchao Sun", "Shuang Ma", "Xiang Kong", "Meng Cao", "Graham Neubig", "Tongshuang Wu"], "title": "Checklists Are Better Than Reward Models For Aligning Language Models", "comment": null, "summary": "Language models must be adapted to understand and follow user instructions.\nReinforcement learning is widely used to facilitate this -- typically using\nfixed criteria such as \"helpfulness\" and \"harmfulness\". In our work, we instead\npropose using flexible, instruction-specific criteria as a means of broadening\nthe impact that reinforcement learning can have in eliciting instruction\nfollowing. We propose \"Reinforcement Learning from Checklist Feedback\" (RLCF).\nFrom instructions, we extract checklists and evaluate how well responses\nsatisfy each item - using both AI judges and specialized verifier programs -\nthen combine these scores to compute rewards for RL. We compare RLCF with other\nalignment methods applied to a strong instruction following model\n(Qwen2.5-7B-Instruct) on five widely-studied benchmarks -- RLCF is the only\nmethod to improve performance on every benchmark, including a 4-point boost in\nhard satisfaction rate on FollowBench, a 6-point increase on InFoBench, and a\n3-point rise in win rate on Arena-Hard. These results establish checklist\nfeedback as a key tool for improving language models' support of queries that\nexpress a multitude of needs.", "AI": {"tldr": "本文提出了一种名为“基于清单反馈的强化学习”（RLCF）的新方法，通过使用灵活的、指令特定的清单来评估语言模型响应，并将其分数作为强化学习奖励，显著提升了模型在多个基准测试上的指令遵循能力。", "motivation": "语言模型需要适应以理解和遵循用户指令。传统的强化学习方法通常使用固定标准（如“有用性”和“有害性”）来促进指令遵循，但这限制了强化学习在引发指令遵循方面的影响。研究旨在提出一种更灵活、指令特定的评估标准。", "method": "本文提出了RLCF方法。该方法从用户指令中提取清单，并使用AI评判和专业验证程序评估响应满足每个清单项的程度。然后，将这些评估分数结合起来计算强化学习的奖励。RLCF被应用于Qwen2.5-7B-Instruct模型，并在五个广泛研究的基准测试上与其他对齐方法进行了比较。", "result": "RLCF是唯一在所有五个基准测试上都提高了性能的方法。具体包括：在FollowBench上硬性满足率提高了4点，在InFoBench上提高了6点，在Arena-Hard上胜率提高了3点。", "conclusion": "研究结果表明，清单反馈是改进语言模型支持表达多种需求查询的关键工具。"}}
{"id": "2507.18348", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18348", "abs": "https://arxiv.org/abs/2507.18348", "authors": ["Ioannis Sarridis", "Christos Koutlis", "Symeon Papadopoulos", "Christos Diou"], "title": "VB-Mitigator: An Open-source Framework for Evaluating and Advancing Visual Bias Mitigation", "comment": null, "summary": "Bias in computer vision models remains a significant challenge, often\nresulting in unfair, unreliable, and non-generalizable AI systems. Although\nresearch into bias mitigation has intensified, progress continues to be\nhindered by fragmented implementations and inconsistent evaluation practices.\nDisparate datasets and metrics used across studies complicate reproducibility,\nmaking it difficult to fairly assess and compare the effectiveness of various\napproaches. To overcome these limitations, we introduce the Visual Bias\nMitigator (VB-Mitigator), an open-source framework designed to streamline the\ndevelopment, evaluation, and comparative analysis of visual bias mitigation\ntechniques. VB-Mitigator offers a unified research environment encompassing 12\nestablished mitigation methods, 7 diverse benchmark datasets. A key strength of\nVB-Mitigator is its extensibility, allowing for seamless integration of\nadditional methods, datasets, metrics, and models. VB-Mitigator aims to\naccelerate research toward fairness-aware computer vision models by serving as\na foundational codebase for the research community to develop and assess their\napproaches. To this end, we also recommend best evaluation practices and\nprovide a comprehensive performance comparison among state-of-the-art\nmethodologies.", "AI": {"tldr": "本文提出了VB-Mitigator，一个开源框架，旨在统一计算机视觉偏见缓解技术的研究、评估和比较，以克服当前碎片化和评估不一致的挑战。", "motivation": "计算机视觉模型中的偏见导致不公平、不可靠和泛化能力差的AI系统。现有的偏见缓解研究因实现碎片化和评估实践不一致而进展受阻，不同的数据集和指标使得方法比较和复现困难。", "method": "开发了VB-Mitigator开源框架，提供一个统一的研究环境，集成了12种已有的偏见缓解方法和7个基准数据集。该框架具有可扩展性，允许无缝集成额外的方法、数据集、指标和模型。同时，论文推荐了最佳评估实践，并提供了现有SOTA方法间的全面性能比较。", "result": "VB-Mitigator成功地简化了视觉偏见缓解技术的开发、评估和比较分析。它提供了一个统一的研究环境和可扩展的平台，能够加速公平性感知计算机视觉模型的研究。", "conclusion": "VB-Mitigator作为一个基础代码库，旨在加速计算机视觉领域对公平性感知模型的研究，并为研究社区开发和评估偏见缓解方法提供支持。"}}
{"id": "2507.18354", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18354", "abs": "https://arxiv.org/abs/2507.18354", "authors": ["Lexuan Zhu", "Yuxuan Li", "Yuning Ren"], "title": "Deformable Convolution Module with Globally Learned Relative Offsets for Fundus Vessel Segmentation", "comment": null, "summary": "Deformable convolution can adaptively change the shape of convolution kernel\nby learning offsets to deal with complex shape features. We propose a novel\nplug and play deformable convolutional module that uses attention and\nfeedforward networks to learn offsets, so that the deformable patterns can\ncapture long-distance global features. Compared with previously existing\ndeformable convolutions, the proposed module learns the sub pixel displacement\nfield and adaptively warps the feature maps across all channels rather than\ndirectly deforms the convolution kernel , which is equivalent to a relative\ndeformation of the kernel sampling grids, achieving global feature deformation\nand the decoupling of kernel size and learning network. Considering that the\nfundus blood vessels have globally self similar complex edges, we design a deep\nlearning model for fundus blood vessel segmentation, GDCUnet, based on the\nproposed convolutional module. Empirical evaluations under the same\nconfiguration and unified framework show that GDCUnet has achieved state of the\nart performance on public datasets. Further ablation experiments demonstrated\nthat the proposed deformable convolutional module could more significantly\nlearn the complex features of fundus blood vessels, enhancing the model\nrepresentation and generalization capabilities.The proposed module is similar\nto the interface of conventional convolution, we suggest applying it to more\nmachine vision tasks with complex global self similar features.", "AI": {"tldr": "本文提出了一种新型可变形卷积模块，通过注意力机制和前馈网络学习偏移量，实现特征图的自适应扭曲，以捕获长距离全局特征。基于此模块设计的GDCUnet在眼底血管分割任务上达到了SOTA性能。", "motivation": "传统可变形卷积通过学习偏移量适应复杂形状特征，但可能难以捕获长距离全局特征。眼底血管具有全局自相似的复杂边缘，需要更强的特征捕获能力。", "method": "提出了一种即插即用的可变形卷积模块，利用注意力机制和前馈网络学习偏移量。该模块学习亚像素位移场，并自适应地扭曲所有通道的特征图，而非直接变形卷积核，实现了全局特征变形和核大小与学习网络的解耦。基于此模块，设计了用于眼底血管分割的深度学习模型GDCUnet。", "result": "在统一框架下，GDCUnet在公共数据集上实现了眼底血管分割的SOTA性能。消融实验表明，所提出的可变形卷积模块能更显著地学习眼底血管的复杂特征，增强了模型的表示和泛化能力。", "conclusion": "所提出的可变形卷积模块与传统卷积接口类似，易于集成，并在处理具有复杂全局自相似特征的机器视觉任务中展现出巨大潜力，建议将其应用于更多此类任务。"}}
{"id": "2507.18371", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18371", "abs": "https://arxiv.org/abs/2507.18371", "authors": ["Xiaotian Chen", "DongFu Yin", "Fei Richard Yu", "Xuanchen Li", "Xinhao Zhang"], "title": "MVG4D: Image Matrix-Based Multi-View and Motion Generation for 4D Content Creation from a Single Image", "comment": null, "summary": "Advances in generative modeling have significantly enhanced digital content\ncreation, extending from 2D images to complex 3D and 4D scenes. Despite\nsubstantial progress, producing high-fidelity and temporally consistent dynamic\n4D content remains a challenge. In this paper, we propose MVG4D, a novel\nframework that generates dynamic 4D content from a single still image by\ncombining multi-view synthesis with 4D Gaussian Splatting (4D GS). At its core,\nMVG4D employs an image matrix module that synthesizes temporally coherent and\nspatially diverse multi-view images, providing rich supervisory signals for\ndownstream 3D and 4D reconstruction. These multi-view images are used to\noptimize a 3D Gaussian point cloud, which is further extended into the temporal\ndomain via a lightweight deformation network. Our method effectively enhances\ntemporal consistency, geometric fidelity, and visual realism, addressing key\nchallenges in motion discontinuity and background degradation that affect prior\n4D GS-based methods. Extensive experiments on the Objaverse dataset demonstrate\nthat MVG4D outperforms state-of-the-art baselines in CLIP-I, PSNR, FVD, and\ntime efficiency. Notably, it reduces flickering artifacts and sharpens\nstructural details across views and time, enabling more immersive AR/VR\nexperiences. MVG4D sets a new direction for efficient and controllable 4D\ngeneration from minimal inputs.", "AI": {"tldr": "MVG4D是一个新颖的框架，通过结合多视角合成和4D高斯泼溅（4D GS），能够从单张静止图像生成高保真且时间一致的动态4D内容。", "motivation": "尽管生成模型在数字内容创作方面取得了显著进展，但生成高保真、时间一致的动态4D内容仍然是一个挑战，尤其是在运动不连续性和背景退化方面。", "method": "MVG4D的核心是一个图像矩阵模块，用于合成时间连贯且空间多样化的多视角图像，为后续的3D和4D重建提供监督信号。这些多视角图像用于优化3D高斯点云，并通过一个轻量级形变网络扩展到时间域。", "result": "MVG4D在Objaverse数据集上的实验表明，其在CLIP-I、PSNR、FVD和时间效率方面均优于现有基线。显著减少了闪烁伪影，并锐化了跨视角和时间的结构细节。", "conclusion": "MVG4D有效增强了时间一致性、几何保真度和视觉真实感，解决了现有4D GS方法中的关键挑战，为从最小输入进行高效和可控的4D生成开辟了新方向，并能实现更沉浸式的AR/VR体验。"}}
{"id": "2507.18374", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18374", "abs": "https://arxiv.org/abs/2507.18374", "authors": ["Filippos Bellos", "Yayuan Li", "Cary Shu", "Ruey Day", "Jeffrey M. Siskind", "Jason J. Corso"], "title": "Towards Effective Human-in-the-Loop Assistive AI Agents", "comment": "10 pages, 5 figures, 2 tables", "summary": "Effective human-AI collaboration for physical task completion has significant\npotential in both everyday activities and professional domains. AI agents\nequipped with informative guidance can enhance human performance, but\nevaluating such collaboration remains challenging due to the complexity of\nhuman-in-the-loop interactions. In this work, we introduce an evaluation\nframework and a multimodal dataset of human-AI interactions designed to assess\nhow AI guidance affects procedural task performance, error reduction and\nlearning outcomes. Besides, we develop an augmented reality (AR)-equipped AI\nagent that provides interactive guidance in real-world tasks, from cooking to\nbattlefield medicine. Through human studies, we share empirical insights into\nAI-assisted human performance and demonstrate that AI-assisted collaboration\nimproves task completion.", "AI": {"tldr": "本文提出了一个评估框架和多模态数据集，用于评估AI指导如何影响人类在物理任务中的表现、错误减少和学习成果，并通过一个AR增强AI代理在真实世界任务中进行人类研究，证明了AI辅助协作能提高任务完成度。", "motivation": "人类-AI协作在日常和专业领域具有巨大潜力，但由于人机交互的复杂性，评估此类协作仍具挑战性。", "method": "引入了一个评估框架和多模态人类-AI交互数据集；开发了一个配备增强现实（AR）的AI代理，提供实时互动指导；通过人类研究分享实证见解。", "result": "AI辅助协作改善了任务完成度，提高了人类表现，有助于减少错误和学习成果。", "conclusion": "AI辅助协作能够有效提升人类在物理任务中的表现，并通过所提出的框架和AR-AI代理得到了验证。"}}
{"id": "2507.18382", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18382", "abs": "https://arxiv.org/abs/2507.18382", "authors": ["Yayuan Li", "Filippos Bellos", "Jason Corso"], "title": "Towards Consistent Long-Term Pose Generation", "comment": "10 pages, 5 figures, 4 tables", "summary": "Current approaches to pose generation rely heavily on intermediate\nrepresentations, either through two-stage pipelines with quantization or\nautoregressive models that accumulate errors during inference. This fundamental\nlimitation leads to degraded performance, particularly in long-term pose\ngeneration where maintaining temporal coherence is crucial. We propose a novel\none-stage architecture that directly generates poses in continuous coordinate\nspace from minimal context - a single RGB image and text description - while\nmaintaining consistent distributions between training and inference. Our key\ninnovation is eliminating the need for intermediate representations or\ntoken-based generation by operating directly on pose coordinates through a\nrelative movement prediction mechanism that preserves spatial relationships,\nand a unified placeholder token approach that enables single-forward generation\nwith identical behavior during training and inference. Through extensive\nexperiments on Penn Action and First-Person Hand Action Benchmark (F-PHAB)\ndatasets, we demonstrate that our approach significantly outperforms existing\nquantization-based and autoregressive methods, especially in long-term\ngeneration scenarios.", "AI": {"tldr": "该论文提出一种新颖的单阶段架构，直接从RGB图像和文本描述在连续坐标空间生成姿态，避免了中间表示，并在长期姿态生成中表现出色。", "motivation": "当前的姿态生成方法依赖于中间表示（如量化或自回归模型），导致性能下降，尤其是在需要保持时间一致性的长期姿态生成中，会累积误差。", "method": "提出一种新颖的单阶段架构，直接在连续坐标空间生成姿态。主要创新点包括：1) 消除对中间表示或基于token的生成的需求；2) 通过相对运动预测机制直接操作姿态坐标以保留空间关系；3) 采用统一的占位符token方法实现单次前向生成，确保训练和推理行为一致。", "result": "在Penn Action和First-Person Hand Action Benchmark (F-PHAB)数据集上的实验表明，该方法显著优于现有的基于量化和自回归的方法，尤其在长期生成场景中表现更佳。", "conclusion": "所提出的直接在连续坐标空间进行单阶段姿态生成的方法，通过避免中间表示，有效解决了现有方法的局限性，并在长期姿态生成任务中取得了显著的性能提升。"}}
{"id": "2507.18385", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18385", "abs": "https://arxiv.org/abs/2507.18385", "authors": ["Yu Jiang", "Jiahao Xia", "Jiongming Qin", "Yusen Wang", "Tuo Cao", "Chunxia Xiao"], "title": "HumanMaterial: Human Material Estimation from a Single Image via Progressive Training", "comment": "14", "summary": "Full-body Human inverse rendering based on physically-based rendering aims to\nacquire high-quality materials, which helps achieve photo-realistic rendering\nunder arbitrary illuminations. This task requires estimating multiple material\nmaps and usually relies on the constraint of rendering result. The absence of\nconstraints on the material maps makes inverse rendering an ill-posed task.\nPrevious works alleviated this problem by building material dataset for\ntraining, but their simplified material data and rendering equation lead to\nrendering results with limited realism, especially that of skin. To further\nalleviate this problem, we construct a higher-quality dataset (OpenHumanBRDF)\nbased on scanned real data and statistical material data. In addition to the\nnormal, diffuse albedo, roughness, specular albedo, we produce displacement and\nsubsurface scattering to enhance the realism of rendering results, especially\nfor the skin. With the increase in prediction tasks for more materials, using\nan end-to-end model as in the previous work struggles to balance the importance\namong various material maps, and leads to model underfitting. Therefore, we\ndesign a model (HumanMaterial) with progressive training strategy to make full\nuse of the supervision information of the material maps and improve the\nperformance of material estimation. HumanMaterial first obtain the initial\nmaterial results via three prior models, and then refine the results by a\nfinetuning model. Prior models estimate different material maps, and each map\nhas different significance for rendering results. Thus, we design a Controlled\nPBR Rendering (CPR) loss, which enhances the importance of the materials to be\noptimized during the training of prior models. Extensive experiments on\nOpenHumanBRDF dataset and real data demonstrate that our method achieves\nstate-of-the-art performance.", "AI": {"tldr": "该研究通过构建高质量人体材质数据集（OpenHumanBRDF）并设计渐进式训练模型（HumanMaterial）及受控PBR渲染损失（CPR loss），解决了全身体人体逆向渲染中材质估计的病态问题和现有方法真实感不足的局限性，实现了照片级真实感的人体材质渲染。", "motivation": "人体逆向渲染旨在获取高质量材质以实现任意光照下的照片级真实感渲染，但估计多材质图缺乏约束使其成为病态问题。现有方法因材质数据和渲染方程简化，导致渲染真实感（尤其是皮肤）有限。此外，随着预测材质种类增多，传统端到端模型难以平衡不同材质图的重要性，易导致欠拟合。", "method": "1. 构建了高质量数据集OpenHumanBRDF，包含法线、漫反射反照率、粗糙度、镜面反照率，并额外引入位移图和次表面散射以增强皮肤等渲染的真实感。2. 设计了HumanMaterial模型，采用渐进式训练策略，首先通过三个先验模型获取初始材质结果，再通过一个微调模型进行精炼。3. 提出了受控PBR渲染（CPR）损失函数，在先验模型训练时增强待优化材质的重要性。", "result": "在OpenHumanBRDF数据集和真实数据上的大量实验表明，所提出的方法实现了最先进的性能。", "conclusion": "通过高质量数据集和创新的模型架构与损失函数，本研究有效解决了全身体人体逆向渲染中的材质估计难题，显著提升了渲染结果的真实感和质量。"}}
{"id": "2507.18405", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18405", "abs": "https://arxiv.org/abs/2507.18405", "authors": ["Simin Huo", "Ning Li"], "title": "Iwin Transformer: Hierarchical Vision Transformer using Interleaved Windows", "comment": "14 pages, 10 figures, Submitted to IEEE Transactions on Pattern\n  Analysis and Machine Intelligence", "summary": "We introduce Iwin Transformer, a novel position-embedding-free hierarchical\nvision transformer, which can be fine-tuned directly from low to high\nresolution, through the collaboration of innovative interleaved window\nattention and depthwise separable convolution. This approach uses attention to\nconnect distant tokens and applies convolution to link neighboring tokens,\nenabling global information exchange within a single module, overcoming Swin\nTransformer's limitation of requiring two consecutive blocks to approximate\nglobal attention. Extensive experiments on visual benchmarks demonstrate that\nIwin Transformer exhibits strong competitiveness in tasks such as image\nclassification (87.4 top-1 accuracy on ImageNet-1K), semantic segmentation and\nvideo action recognition. We also validate the effectiveness of the core\ncomponent in Iwin as a standalone module that can seamlessly replace the\nself-attention module in class-conditional image generation. The concepts and\nmethods introduced by the Iwin Transformer have the potential to inspire future\nresearch, like Iwin 3D Attention in video generation. The code and models are\navailable at https://github.com/cominder/Iwin-Transformer.", "AI": {"tldr": "Iwin Transformer是一种新型的无位置编码分层视觉Transformer，通过交错窗口注意力与深度可分离卷积结合，实现单模块内的全局信息交换，并支持从低分辨率到高分辨率的直接微调，在多项视觉任务上表现出色。", "motivation": "Swin Transformer需要两个连续的块才能近似实现全局注意力，这促使研究者开发一种能在单个模块内实现全局信息交换并克服这一限制的方法，同时支持低到高分辨率的直接微调。", "method": "提出了Iwin Transformer，它结合了创新的交错窗口注意力（连接远距离token）和深度可分离卷积（连接相邻token），在一个模块内实现全局信息交换。该模型是无位置编码的分层结构，支持直接从低分辨率到高分辨率的微调。", "result": "Iwin Transformer在图像分类任务中（ImageNet-1K）取得了87.4%的top-1准确率，并在语义分割和视频动作识别等视觉基准测试中展现出强大的竞争力。其核心组件可作为独立模块无缝替代条件图像生成中的自注意力模块。", "conclusion": "Iwin Transformer引入的概念和方法被证明是有效的，并具有启发未来研究的潜力，例如视频生成中的Iwin 3D注意力。"}}
{"id": "2507.18407", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18407", "abs": "https://arxiv.org/abs/2507.18407", "authors": ["Xun Ye", "Ruixiang Tang", "Mingda Zhang", "Jianglong Qin"], "title": "DCFFSNet: Deep Connectivity Feature Fusion Separation Network for Medical Image Segmentation", "comment": "16 pages , 11 figures", "summary": "Medical image segmentation leverages topological connectivity theory to\nenhance edge precision and regional consistency. However, existing deep\nnetworks integrating connectivity often forcibly inject it as an additional\nfeature module, resulting in coupled feature spaces with no standardized\nmechanism to quantify different feature strengths. To address these issues, we\npropose DCFFSNet (Dual-Connectivity Feature Fusion-Separation Network). It\nintroduces an innovative feature space decoupling strategy. This strategy\nquantifies the relative strength between connectivity features and other\nfeatures. It then builds a deep connectivity feature fusion-separation\narchitecture. This architecture dynamically balances multi-scale feature\nexpression. Experiments were conducted on the ISIC2018, DSB2018, and MoNuSeg\ndatasets. On ISIC2018, DCFFSNet outperformed the next best model (CMUNet) by\n1.3% (Dice) and 1.2% (IoU). On DSB2018, it surpassed TransUNet by 0.7% (Dice)\nand 0.9% (IoU). On MoNuSeg, it exceeded CSCAUNet by 0.8% (Dice) and 0.9% (IoU).\nThe results demonstrate that DCFFSNet exceeds existing mainstream methods\nacross all metrics. It effectively resolves segmentation fragmentation and\nachieves smooth edge transitions. This significantly enhances clinical\nusability.", "AI": {"tldr": "DCFFSNet提出了一种新颖的特征空间解耦策略，用于医学图像分割，通过量化连接性特征与其他特征的相对强度，动态平衡多尺度特征表达，有效提升了分割精度和边缘平滑度。", "motivation": "现有深度网络在医学图像分割中整合拓扑连接性时，常将其作为附加特征模块强制注入，导致特征空间耦合，且缺乏量化不同特征强度的标准化机制，从而影响边缘精度和区域一致性。", "method": "本文提出了DCFFSNet（Dual-Connectivity Feature Fusion-Separation Network），引入了一种创新的特征空间解耦策略。该策略量化连接性特征与其他特征的相对强度，并构建深度连接性特征融合-分离架构，以动态平衡多尺度特征表达。", "result": "在ISIC2018数据集上，DCFFSNet在Dice和IoU指标上分别超越CMUNet 1.3%和1.2%。在DSB2018数据集上，Dice和IoU分别超越TransUNet 0.7%和0.9%。在MoNuSeg数据集上，Dice和IoU分别超越CSCAUNet 0.8%和0.9%。实验结果表明，DCFFSNet在所有指标上均优于现有主流方法。", "conclusion": "DCFFSNet有效解决了分割碎片化问题，实现了平滑的边缘过渡，显著增强了临床可用性，并在多个医学图像分割数据集上表现出卓越性能。"}}
{"id": "2507.18424", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18424", "abs": "https://arxiv.org/abs/2507.18424", "authors": ["Edward Ellis", "Robert Mendel", "Andrew Bulpitt", "Nasim Parsa", "Michael F Byrne", "Sharib Ali"], "title": "Self-Supervised Ultrasound-Video Segmentation with Feature Prediction and 3D Localised Loss", "comment": null, "summary": "Acquiring and annotating large datasets in ultrasound imaging is challenging\ndue to low contrast, high noise, and susceptibility to artefacts. This process\nrequires significant time and clinical expertise. Self-supervised learning\n(SSL) offers a promising solution by leveraging unlabelled data to learn useful\nrepresentations, enabling improved segmentation performance when annotated data\nis limited. Recent state-of-the-art developments in SSL for video data include\nV-JEPA, a framework solely based on feature prediction, avoiding pixel level\nreconstruction or negative samples. We hypothesise that V-JEPA is well-suited\nto ultrasound imaging, as it is less sensitive to noisy pixel-level detail\nwhile effectively leveraging temporal information. To the best of our\nknowledge, this is the first study to adopt V-JEPA for ultrasound video data.\nSimilar to other patch-based masking SSL techniques such as VideoMAE, V-JEPA is\nwell-suited to ViT-based models. However, ViTs can underperform on small\nmedical datasets due to lack of inductive biases, limited spatial locality and\nabsence of hierarchical feature learning. To improve locality understanding, we\npropose a novel 3D localisation auxiliary task to improve locality in ViT\nrepresentations during V-JEPA pre-training. Our results show V-JEPA with our\nauxiliary task improves segmentation performance significantly across various\nfrozen encoder configurations, with gains up to 3.4\\% using 100\\% and up to\n8.35\\% using only 10\\% of the training data.", "AI": {"tldr": "本研究首次将V-JEPA自监督学习框架应用于超声视频数据，并提出一种新颖的3D定位辅助任务，以增强ViT模型在超声图像分割中的局部性理解，显著提升了分割性能。", "motivation": "超声图像数据获取和标注面临低对比度、高噪声和伪影等挑战，耗时且需要专业知识。自监督学习（SSL）有望解决有限标注数据下的性能问题。V-JEPA因其基于特征预测而非像素重建的特性，被认为更适合处理超声图像中的噪声和利用时间信息。然而，Vision Transformers（ViTs）在小型医学数据集上可能表现不佳，因其缺乏归纳偏置、空间局部性有限且缺乏分层特征学习。", "method": "本研究将V-JEPA框架应用于超声视频数据进行自监督预训练。为了改善ViT在小数据集上的局部性理解问题，提出了一种新颖的3D定位辅助任务，在V-JEPA预训练过程中增强ViT表示的局部性。随后，在各种冻结编码器配置下评估了模型在分割任务上的性能。", "result": "结果表明，结合了3D定位辅助任务的V-JEPA显著提升了超声图像分割性能。使用100%训练数据时，性能提升高达3.4%；仅使用10%训练数据时，性能提升高达8.35%。", "conclusion": "本研究首次成功将V-JEPA应用于超声视频数据，并通过引入3D定位辅助任务有效解决了ViT在小医学数据集上的局部性问题。该方法在超声图像分割任务中展现出显著的性能提升，尤其在数据量有限的情况下效果更佳，为超声图像分析提供了有前景的自监督学习解决方案。"}}
{"id": "2507.18429", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18429", "abs": "https://arxiv.org/abs/2507.18429", "authors": ["Mahdi Ghafourian", "Federico M. Sukno"], "title": "NLML-HPE: Head Pose Estimation with Limited Data via Manifold Learning", "comment": null, "summary": "Head pose estimation (HPE) plays a critical role in various computer vision\napplications such as human-computer interaction and facial recognition. In this\npaper, we propose a novel deep learning approach for head pose estimation with\nlimited training data via non-linear manifold learning called NLML-HPE. This\nmethod is based on the combination of tensor decomposition (i.e., Tucker\ndecomposition) and feed forward neural networks. Unlike traditional\nclassification-based approaches, our method formulates head pose estimation as\na regression problem, mapping input landmarks into a continuous representation\nof pose angles. To this end, our method uses tensor decomposition to split each\nEuler angle (yaw, pitch, roll) to separate subspaces and models each dimension\nof the underlying manifold as a cosine curve. We address two key challenges: 1.\nAlmost all HPE datasets suffer from incorrect and inaccurate pose annotations.\nHence, we generated a precise and consistent 2D head pose dataset for our\ntraining set by rotating 3D head models for a fixed set of poses and rendering\nthe corresponding 2D images. 2. We achieved real-time performance with limited\ntraining data as our method accurately captures the nature of rotation of an\nobject from facial landmarks. Once the underlying manifold for rotation around\neach axis is learned, the model is very fast in predicting unseen data. Our\ntraining and testing code is available online along with our trained models:\nhttps: //github.com/MahdiGhafoorian/NLML_HPE.", "AI": {"tldr": "提出NLML-HPE，一种基于非线性流形学习的深度学习方法，通过张量分解和前馈神经网络，将头部姿态估计建模为回归问题，用有限数据实现实时高性能。", "motivation": "头部姿态估计（HPE）在人机交互和面部识别等计算机视觉应用中至关重要。现有HPE数据集普遍存在姿态标注不准确的问题，且难以在有限训练数据下实现实时高性能。", "method": "提出NLML-HPE方法，结合张量分解（Tucker分解）和前馈神经网络。将头部姿态估计视为回归问题，将输入地标映射到连续的姿态角度表示。使用张量分解将每个欧拉角（偏航、俯仰、滚动）分解到独立的子空间，并将底层流形的每个维度建模为余弦曲线。为解决数据标注问题，通过旋转3D头部模型并渲染2D图像，生成了精确一致的2D头部姿态数据集。", "result": "在有限训练数据下实现了实时性能，模型能够准确捕捉物体从面部地标旋转的本质。一旦学习了每个轴的旋转流形，模型在预测未见数据时速度非常快。", "conclusion": "NLML-HPE方法通过非线性流形学习和张量分解，有效地解决了头部姿态估计中的数据标注不准确和有限数据下的实时性能挑战，为HPE提供了一种精确且高效的回归解决方案。"}}
{"id": "2507.18447", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18447", "abs": "https://arxiv.org/abs/2507.18447", "authors": ["Junda Wu", "Jessica Echterhoff", "Kyungtae Han", "Amr Abdelraouf", "Rohit Gupta", "Julian McAuley"], "title": "PDB-Eval: An Evaluation of Large Multimodal Models for Description and Explanation of Personalized Driving Behavior", "comment": null, "summary": "Understanding a driver's behavior and intentions is important for potential\nrisk assessment and early accident prevention. Safety and driver assistance\nsystems can be tailored to individual drivers' behavior, significantly\nenhancing their effectiveness. However, existing datasets are limited in\ndescribing and explaining general vehicle movements based on external visual\nevidence. This paper introduces a benchmark, PDB-Eval, for a detailed\nunderstanding of Personalized Driver Behavior, and aligning Large Multimodal\nModels (MLLMs) with driving comprehension and reasoning. Our benchmark consists\nof two main components, PDB-X and PDB-QA. PDB-X can evaluate MLLMs'\nunderstanding of temporal driving scenes. Our dataset is designed to find valid\nvisual evidence from the external view to explain the driver's behavior from\nthe internal view. To align MLLMs' reasoning abilities with driving tasks, we\npropose PDB-QA as a visual explanation question-answering task for MLLM\ninstruction fine-tuning. As a generic learning task for generative models like\nMLLMs, PDB-QA can bridge the domain gap without harming MLLMs'\ngeneralizability. Our evaluation indicates that fine-tuning MLLMs on\nfine-grained descriptions and explanations can effectively bridge the gap\nbetween MLLMs and the driving domain, which improves zero-shot performance on\nquestion-answering tasks by up to 73.2%. We further evaluate the MLLMs\nfine-tuned on PDB-X in Brain4Cars' intention prediction and AIDE's recognition\ntasks. We observe up to 12.5% performance improvements on the turn intention\nprediction task in Brain4Cars, and consistent performance improvements up to\n11.0% on all tasks in AIDE.", "AI": {"tldr": "本文提出了PDB-Eval基准，用于理解个性化驾驶行为并将大型多模态模型（MLLMs）与驾驶理解和推理对齐，通过细粒度描述和解释显著提升了MLLMs在驾驶任务上的性能。", "motivation": "理解驾驶员行为和意图对于风险评估和事故预防至关重要，现有的数据集在基于外部视觉证据描述和解释车辆运动方面存在局限性，限制了安全和驾驶辅助系统的有效性。", "method": "引入了PDB-Eval基准，包含PDB-X和PDB-QA两部分。PDB-X用于评估MLLMs对时间驾驶场景的理解，旨在从外部视角寻找视觉证据解释内部视角的驾驶员行为。PDB-QA作为视觉解释问答任务，用于MLLM指令微调，以弥合领域差距并保持泛化能力。", "result": "在细粒度描述和解释上对MLLMs进行微调，能有效弥合MLLMs与驾驶领域之间的差距，使零样本问答性能提高高达73.2%。在Brain4Cars的意图预测任务上性能提升高达12.5%，在AIDE的所有识别任务上性能提升高达11.0%。", "conclusion": "PDB-Eval基准通过提供细粒度的驾驶行为描述和解释，成功地将大型多模态模型与驾驶领域对齐，显著提升了其在驾驶理解、意图预测和行为识别等任务上的性能和泛化能力。"}}
{"id": "2507.18473", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18473", "abs": "https://arxiv.org/abs/2507.18473", "authors": ["Haoran Xu", "Saining Zhang", "Peishuo Li", "Baijun Ye", "Xiaoxue Chen", "Huan-ang Gao", "Jv Zheng", "Xiaowei Song", "Ziqiao Peng", "Run Miao", "Jinrang Jia", "Yifeng Shi", "Guangqi Yi", "Hang Zhao", "Hao Tang", "Hongyang Li", "Kaicheng Yu", "Hao Zhao"], "title": "CRUISE: Cooperative Reconstruction and Editing in V2X Scenarios using Gaussian Splatting", "comment": "IROS 2025, Code: https://github.com/SainingZhang/CRUISE", "summary": "Vehicle-to-everything (V2X) communication plays a crucial role in autonomous\ndriving, enabling cooperation between vehicles and infrastructure. While\nsimulation has significantly contributed to various autonomous driving tasks,\nits potential for data generation and augmentation in V2X scenarios remains\nunderexplored. In this paper, we introduce CRUISE, a comprehensive\nreconstruction-and-synthesis framework designed for V2X driving environments.\nCRUISE employs decomposed Gaussian Splatting to accurately reconstruct\nreal-world scenes while supporting flexible editing. By decomposing dynamic\ntraffic participants into editable Gaussian representations, CRUISE allows for\nseamless modification and augmentation of driving scenes. Furthermore, the\nframework renders images from both ego-vehicle and infrastructure views,\nenabling large-scale V2X dataset augmentation for training and evaluation. Our\nexperimental results demonstrate that: 1) CRUISE reconstructs real-world V2X\ndriving scenes with high fidelity; 2) using CRUISE improves 3D detection across\nego-vehicle, infrastructure, and cooperative views, as well as cooperative 3D\ntracking on the V2X-Seq benchmark; and 3) CRUISE effectively generates\nchallenging corner cases.", "AI": {"tldr": "CRUISE是一个用于V2X驾驶环境的重建与合成框架，利用分解高斯溅射技术高保真重建场景并支持灵活编辑，从而实现大规模V2X数据增强，有效提升3D感知任务性能。", "motivation": "尽管模拟在自动驾驶任务中贡献显著，但其在V2X场景中用于数据生成和增强的潜力尚未得到充分探索。现有方法难以高效生成大规模、多样化的V2X数据集。", "method": "CRUISE框架采用分解高斯溅射（decomposed Gaussian Splatting）技术，实现对真实世界V2X场景的高保真重建并支持灵活编辑。它将动态交通参与者分解为可编辑的高斯表示，从而无缝修改和增强驾驶场景。此外，该框架能从自车和基础设施视角渲染图像，支持大规模V2X数据集增强。", "result": "实验结果表明：1) CRUISE能够高保真重建真实世界的V2X驾驶场景；2) 使用CRUISE能提高自车、基础设施和协同视角下的3D检测性能，以及V2X-Seq基准上的协同3D跟踪性能；3) CRUISE能有效生成具有挑战性的角点案例。", "conclusion": "CRUISE是一个全面且有效的V2X数据生成和增强框架，能够提升自动驾驶中V2X感知任务的性能，并通过生成复杂场景来应对挑战。"}}
{"id": "2507.18481", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18481", "abs": "https://arxiv.org/abs/2507.18481", "authors": ["Francesco Dalmonte", "Emirhan Bayar", "Emre Akbas", "Mariana-Iuliana Georgescu"], "title": "Q-Former Autoencoder: A Modern Framework for Medical Anomaly Detection", "comment": "15 pages", "summary": "Anomaly detection in medical images is an important yet challenging task due\nto the diversity of possible anomalies and the practical impossibility of\ncollecting comprehensively annotated data sets. In this work, we tackle\nunsupervised medical anomaly detection proposing a modernized autoencoder-based\nframework, the Q-Former Autoencoder, that leverages state-of-the-art pretrained\nvision foundation models, such as DINO, DINOv2 and Masked Autoencoder. Instead\nof training encoders from scratch, we directly utilize frozen vision foundation\nmodels as feature extractors, enabling rich, multi-stage, high-level\nrepresentations without domain-specific fine-tuning. We propose the usage of\nthe Q-Former architecture as the bottleneck, which enables the control of the\nlength of the reconstruction sequence, while efficiently aggregating multiscale\nfeatures. Additionally, we incorporate a perceptual loss computed using\nfeatures from a pretrained Masked Autoencoder, guiding the reconstruction\ntowards semantically meaningful structures. Our framework is evaluated on four\ndiverse medical anomaly detection benchmarks, achieving state-of-the-art\nresults on BraTS2021, RESC, and RSNA. Our results highlight the potential of\nvision foundation model encoders, pretrained on natural images, to generalize\neffectively to medical image analysis tasks without further fine-tuning. We\nrelease the code and models at https://github.com/emirhanbayar/QFAE.", "AI": {"tldr": "提出了一种名为Q-Former自编码器（QFAE）的无监督医学图像异常检测框架。该框架利用冻结的预训练视觉基础模型（如DINO、DINOv2、MAE）作为特征提取器，并引入基于MAE的感知损失，在多个医学异常检测基准测试中取得了最先进的结果。", "motivation": "医学图像中的异常检测任务极具挑战性，主要原因在于异常的多样性以及难以收集到全面标注的数据集，这促使研究者探索无监督的解决方案。", "method": "核心方法是Q-Former自编码器。它不从头训练编码器，而是直接使用冻结的预训练视觉基础模型（如DINO、DINOv2、MAE）作为特征提取器，以获取丰富、多阶段的高级表示，且无需特定领域的微调。该方法将Q-Former架构用作瓶颈，以控制重建序列的长度并高效聚合多尺度特征。此外，还引入了基于预训练Masked Autoencoder（MAE）的感知损失，以指导重建过程生成语义上有意义的结构。", "result": "该框架在BraTS2021、RESC和RSNA等四个不同的医学异常检测基准测试中，取得了最先进的（SOTA）结果。", "conclusion": "研究结果强调了在自然图像上预训练的视觉基础模型编码器，无需进一步微调即可有效泛化到医学图像分析任务的巨大潜力。"}}
{"id": "2507.18483", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18483", "abs": "https://arxiv.org/abs/2507.18483", "authors": ["Frauke Wilm", "Luis Carlos Rivera Monroy", "Mathias Öttl", "Lukas Mürdter", "Leonid Mill", "Andreas Maier"], "title": "A COCO-Formatted Instance-Level Dataset for Plasmodium Falciparum Detection in Giemsa-Stained Blood Smears", "comment": "7 pages, 4 figures, 2 tables, accepted at MICCAI 2025 Open Data", "summary": "Accurate detection of Plasmodium falciparum in Giemsa-stained blood smears is\nan essential component of reliable malaria diagnosis, especially in developing\ncountries. Deep learning-based object detection methods have demonstrated\nstrong potential for automated Malaria diagnosis, but their adoption is limited\nby the scarcity of datasets with detailed instance-level annotations. In this\nwork, we present an enhanced version of the publicly available NIH malaria\ndataset, with detailed bounding box annotations in COCO format to support\nobject detection training. We validated the revised annotations by training a\nFaster R-CNN model to detect infected and non-infected red blood cells, as well\nas white blood cells. Cross-validation on the original dataset yielded F1\nscores of up to 0.88 for infected cell detection. These results underscore the\nimportance of annotation volume and consistency, and demonstrate that automated\nannotation refinement combined with targeted manual correction can produce\ntraining data of sufficient quality for robust detection performance. The\nupdated annotations set is publicly available via GitHub:\nhttps://github.com/MIRA-Vision-Microscopy/malaria-thin-smear-coco.", "AI": {"tldr": "为解决深度学习疟疾诊断中高质量标注数据稀缺问题，本文发布了一个增强版的NIH疟疾数据集，包含COCO格式的详细边界框标注，并验证了其在Faster R-CNN模型上对感染细胞检测的有效性。", "motivation": "吉姆萨染色血涂片中恶性疟原虫的准确检测对于可靠的疟疾诊断至关重要，尤其是在发展中国家。尽管基于深度学习的目标检测方法在自动化疟疾诊断方面显示出巨大潜力，但由于缺乏具有详细实例级标注的数据集，其应用受到限制。", "method": "本文提出了一个增强版的公开NIH疟疾数据集，增加了COCO格式的详细边界框标注，以支持目标检测训练。通过训练一个Faster R-CNN模型来检测感染和未感染的红细胞以及白细胞，对修订后的标注进行了验证。在原始数据集上进行了交叉验证，并结合了自动化标注精炼和有针对性的人工校正。", "result": "在原始数据集上进行交叉验证，感染细胞检测的F1分数高达0.88。这些结果强调了标注数量和一致性的重要性，并证明了自动化标注精炼结合有针对性的人工校正可以产生足够高质量的训练数据，以实现稳健的检测性能。", "conclusion": "更新后的、包含详细COCO标注的数据集已公开可用，为自动化疟疾诊断中的深度学习模型训练提供了高质量的数据，证明了通过标注数量、一致性以及精炼与校正相结合的方法，可以有效提升检测模型的性能。"}}
{"id": "2507.18498", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18498", "abs": "https://arxiv.org/abs/2507.18498", "authors": ["Zongzheng Zhang", "Xuchong Qiu", "Boran Zhang", "Guantian Zheng", "Xunjiang Gu", "Guoxuan Chi", "Huan-ang Gao", "Leichen Wang", "Ziming Liu", "Xinrun Li", "Igor Gilitschenski", "Hongyang Li", "Hang Zhao", "Hao Zhao"], "title": "Delving into Mapping Uncertainty for Mapless Trajectory Prediction", "comment": "Accepted to IROS 2025, Project Page:\n  https://ethan-zheng136.github.io/Dev-Unc/", "summary": "Recent advances in autonomous driving are moving towards mapless approaches,\nwhere High-Definition (HD) maps are generated online directly from sensor data,\nreducing the need for expensive labeling and maintenance. However, the\nreliability of these online-generated maps remains uncertain. While\nincorporating map uncertainty into downstream trajectory prediction tasks has\nshown potential for performance improvements, current strategies provide\nlimited insights into the specific scenarios where this uncertainty is\nbeneficial. In this work, we first analyze the driving scenarios in which\nmapping uncertainty has the greatest positive impact on trajectory prediction\nand identify a critical, previously overlooked factor: the agent's kinematic\nstate. Building on these insights, we propose a novel Proprioceptive Scenario\nGating that adaptively integrates map uncertainty into trajectory prediction\nbased on forecasts of the ego vehicle's future kinematics. This lightweight,\nself-supervised approach enhances the synergy between online mapping and\ntrajectory prediction, providing interpretability around where uncertainty is\nadvantageous and outperforming previous integration methods. Additionally, we\nintroduce a Covariance-based Map Uncertainty approach that better aligns with\nmap geometry, further improving trajectory prediction. Extensive ablation\nstudies confirm the effectiveness of our approach, achieving up to 23.6%\nimprovement in mapless trajectory prediction performance over the\nstate-of-the-art method using the real-world nuScenes driving dataset. Our\ncode, data, and models are publicly available at\nhttps://github.com/Ethan-Zheng136/Map-Uncertainty-for-Trajectory-Prediction.", "AI": {"tldr": "本文分析了地图不确定性对轨迹预测的益处场景，提出了一种基于自我车辆运动状态的自适应门控机制和一种新的协方差地图不确定性方法，显著提升了无地图轨迹预测的性能和可解释性。", "motivation": "自动驾驶正转向无地图方法，在线生成高清（HD）地图，但其可靠性不确定。将地图不确定性纳入轨迹预测虽有潜力，但现有策略对何时受益缺乏深入洞察。", "method": "首先分析了地图不确定性对轨迹预测影响最大的驾驶场景，并识别出自我车辆运动状态这一关键因素。在此基础上，提出了新颖的“本体感知场景门控”（Proprioceptive Scenario Gating），根据自我车辆未来运动学预测自适应地整合地图不确定性。此外，引入了一种更符合地图几何的“基于协方差的地图不确定性”方法。", "result": "该方法增强了在线地图与轨迹预测的协同作用，提供了不确定性何时有利的可解释性，并优于先前的集成方法。在nuScenes数据集上，无地图轨迹预测性能比现有最佳方法提高了23.6%。", "conclusion": "研究成功识别了自我运动状态在整合地图不确定性中的关键作用，并提出了有效的自适应集成方法和不确定性表示，显著提升了无地图轨迹预测的性能、鲁棒性和可解释性。"}}
{"id": "2507.18503", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18503", "abs": "https://arxiv.org/abs/2507.18503", "authors": ["João Luzio", "Alexandre Bernardino", "Plinio Moreno"], "title": "Human Scanpath Prediction in Target-Present Visual Search with Semantic-Foveal Bayesian Attention", "comment": "To be published in the 2025 IEEE International Conference on\n  Development and Learning (ICDL)", "summary": "In goal-directed visual tasks, human perception is guided by both top-down\nand bottom-up cues. At the same time, foveal vision plays a crucial role in\ndirecting attention efficiently. Modern research on bio-inspired computational\nattention models has taken advantage of advancements in deep learning by\nutilizing human scanpath data to achieve new state-of-the-art performance. In\nthis work, we assess the performance of SemBA-FAST, i.e. Semantic-based\nBayesian Attention for Foveal Active visual Search Tasks, a top-down framework\ndesigned for predicting human visual attention in target-present visual search.\nSemBA-FAST integrates deep object detection with a probabilistic semantic\nfusion mechanism to generate attention maps dynamically, leveraging pre-trained\ndetectors and artificial foveation to update top-down knowledge and improve\nfixation prediction sequentially. We evaluate SemBA-FAST on the COCO-Search18\nbenchmark dataset, comparing its performance against other scanpath prediction\nmodels. Our methodology achieves fixation sequences that closely match human\nground-truth scanpaths. Notably, it surpasses baseline and other top-down\napproaches and competes, in some cases, with scanpath-informed models. These\nfindings provide valuable insights into the capabilities of semantic-foveal\nprobabilistic frameworks for human-like attention modelling, with implications\nfor real-time cognitive computing and robotics.", "AI": {"tldr": "本文提出了一种名为SemBA-FAST的深度学习框架，用于预测目标存在视觉搜索中的人类视觉注意力，该框架结合了深度目标检测和概率语义融合，并在COCO-Search18数据集上取得了接近人类眼动轨迹的优异表现。", "motivation": "人类在目标导向的视觉任务中，感知受自上而下和自下而上线索引导，且中央凹视觉在有效引导注意力方面至关重要。现有生物启发计算注意力模型利用深度学习和人类眼动路径数据取得了进展，但仍有提升空间，尤其是在自上而下框架中。", "method": "本文提出了SemBA-FAST（Semantic-based Bayesian Attention for Foveal Active visual Search Tasks）框架，这是一个用于预测目标存在视觉搜索中人类视觉注意力的自上而下框架。它将深度目标检测与概率语义融合机制相结合，动态生成注意力图，并利用预训练检测器和人工中央凹视觉（foveation）来顺序更新自上而下知识并改进注视点预测。", "result": "SemBA-FAST在COCO-Search18基准数据集上进行了评估，其生成的注视序列与人类真实眼动路径高度匹配。该方法超越了基线模型和其他自上而下的方法，在某些情况下甚至能与利用眼动路径信息的模型相媲美。", "conclusion": "这些发现为语义-中央凹概率框架在类人注意力建模方面的能力提供了宝贵见解，对实时认知计算和机器人技术具有重要意义。"}}
{"id": "2507.18513", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18513", "abs": "https://arxiv.org/abs/2507.18513", "authors": ["Adhemar de Senneville", "Xavier Bou", "Thibaud Ehret", "Rafael Grompone", "Jean Louis Bonne", "Nicolas Dumelie", "Thomas Lauvaux", "Gabriele Facciolo"], "title": "Towards Large Scale Geostatistical Methane Monitoring with Part-based Object Detection", "comment": null, "summary": "Object detection is one of the main applications of computer vision in remote\nsensing imagery. Despite its increasing availability, the sheer volume of\nremote sensing data poses a challenge when detecting rare objects across large\ngeographic areas. Paradoxically, this common challenge is crucial to many\napplications, such as estimating environmental impact of certain human\nactivities at scale. In this paper, we propose to address the problem by\ninvestigating the methane production and emissions of bio-digesters in France.\nWe first introduce a novel dataset containing bio-digesters, with small\ntraining and validation sets, and a large test set with a high imbalance\ntowards observations without objects since such sites are rare. We develop a\npart-based method that considers essential bio-digester sub-elements to boost\ninitial detections. To this end, we apply our method to new, unseen regions to\nbuild an inventory of bio-digesters. We then compute geostatistical estimates\nof the quantity of methane produced that can be attributed to these\ninfrastructures in a given area at a given time.", "AI": {"tldr": "该论文提出了一种基于部件的方法，用于在遥感图像中检测稀有对象（生物消化器），以解决数据量大和目标稀疏的挑战，并将其应用于法国沼气池的甲烷排放量估算。", "motivation": "尽管遥感数据日益普及，但其巨大的数据量使得在大地理区域内检测稀有对象成为挑战，然而这对于评估人类活动的环境影响（如甲烷排放）至关重要。", "method": "首先，构建了一个包含生物消化器的新型数据集，该数据集训练和验证集小，测试集大且高度不平衡（无目标观测占多数）。其次，开发了一种基于部件的方法，通过考虑生物消化器的关键子元素来提升初始检测效果。最后，将该方法应用于新的、未见区域以建立生物消化器清单，并计算这些基础设施在特定区域和时间内的甲烷产量地理统计估算值。", "result": "开发的方法能够提升初始检测效果，成功构建了法国生物消化器的清单，并能据此估算特定区域和时间内的甲烷产量。", "conclusion": "所提出的基于部件的检测方法能够有效识别遥感图像中的稀有对象（如生物消化器），从而支持对环境影响（如甲烷排放）进行大规模评估。"}}
{"id": "2507.18517", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18517", "abs": "https://arxiv.org/abs/2507.18517", "authors": ["Bolutife Atoki", "Jenny Benois-Pineau", "Renaud Péteri", "Fabien Baldacci", "Aymar de Rugy"], "title": "Object segmentation in the wild with foundation models: application to vision assisted neuro-prostheses for upper limbs", "comment": null, "summary": "In this work, we address the problem of semantic object segmentation using\nfoundation models. We investigate whether foundation models, trained on a large\nnumber and variety of objects, can perform object segmentation without\nfine-tuning on specific images containing everyday objects, but in highly\ncluttered visual scenes. The ''in the wild'' context is driven by the target\napplication of vision guided upper limb neuroprostheses. We propose a method\nfor generating prompts based on gaze fixations to guide the Segment Anything\nModel (SAM) in our segmentation scenario, and fine-tune it on egocentric visual\ndata. Evaluation results of our approach show an improvement of the IoU\nsegmentation quality metric by up to 0.51 points on real-world challenging data\nof Grasping-in-the-Wild corpus which is made available on the RoboFlow Platform\n(https://universe.roboflow.com/iwrist/grasping-in-the-wild)", "AI": {"tldr": "本文研究使用基础模型（如SAM）在高度杂乱的真实场景中进行语义对象分割，通过注视点生成提示并微调模型，以应用于视觉引导的上肢神经假肢。", "motivation": "在不针对特定图像进行微调的情况下，使基础模型能够分割高度杂乱的“野外”场景中的日常物体，特别是为视觉引导的上肢神经假肢应用提供支持。", "method": "提出一种基于注视点生成提示的方法来引导Segment Anything Model (SAM) 进行分割，并使用自我中心视觉数据对SAM进行微调。", "result": "在Grasping-in-the-Wild真实世界挑战性数据集上，IoU分割质量指标提高了0.51点。", "conclusion": "通过结合注视点提示和在特定自我中心数据上的微调，基础模型能够显著提高在高度杂乱真实场景中的对象分割性能，适用于神经假肢等应用。"}}
{"id": "2507.18522", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18522", "abs": "https://arxiv.org/abs/2507.18522", "authors": ["Tomislav Pavković", "Mohammad-Ali Nikouei Mahani", "Johannes Niedermayer", "Johannes Betz"], "title": "GaussianFusionOcc: A Seamless Sensor Fusion Approach for 3D Occupancy Prediction Using 3D Gaussians", "comment": null, "summary": "3D semantic occupancy prediction is one of the crucial tasks of autonomous\ndriving. It enables precise and safe interpretation and navigation in complex\nenvironments. Reliable predictions rely on effective sensor fusion, as\ndifferent modalities can contain complementary information. Unlike conventional\nmethods that depend on dense grid representations, our approach,\nGaussianFusionOcc, uses semantic 3D Gaussians alongside an innovative sensor\nfusion mechanism. Seamless integration of data from camera, LiDAR, and radar\nsensors enables more precise and scalable occupancy prediction, while 3D\nGaussian representation significantly improves memory efficiency and inference\nspeed. GaussianFusionOcc employs modality-agnostic deformable attention to\nextract essential features from each sensor type, which are then used to refine\nGaussian properties, resulting in a more accurate representation of the\nenvironment. Extensive testing with various sensor combinations demonstrates\nthe versatility of our approach. By leveraging the robustness of multi-modal\nfusion and the efficiency of Gaussian representation, GaussianFusionOcc\noutperforms current state-of-the-art models.", "AI": {"tldr": "GaussianFusionOcc提出了一种新的3D语义占用预测方法，通过结合语义3D高斯表示和创新的多模态（相机、激光雷达、雷达）传感器融合机制，显著提升了自动驾驶中环境感知的精度、内存效率和推理速度。", "motivation": "自动驾驶中精确安全的解释和导航依赖于有效的传感器融合，且传统方法依赖于密集的网格表示，存在效率问题。因此，需要一种更精确、可扩展且高效的3D语义占用预测方法。", "method": "该方法名为GaussianFusionOcc，核心在于使用语义3D高斯表示替代传统密集网格，并采用创新的传感器融合机制。它通过模态无关的可变形注意力从相机、激光雷达和雷达数据中提取关键特征，然后用这些特征来优化高斯属性，从而更准确地表示环境。", "result": "GaussianFusionOcc实现了更精确和可扩展的占用预测，3D高斯表示显著提高了内存效率和推理速度。通过与各种传感器组合进行广泛测试，证明了其通用性，并且性能超越了当前最先进的模型。", "conclusion": "GaussianFusionOcc通过利用多模态融合的鲁棒性和高斯表示的效率，在3D语义占用预测任务中取得了卓越的性能，为自动驾驶提供了更可靠的环境感知能力。"}}
{"id": "2507.18531", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18531", "abs": "https://arxiv.org/abs/2507.18531", "authors": ["Tianheng Qiu", "Jingchun Gao", "Jingyu Li", "Huiyi Leong", "Xuan Huang", "Xi Wang", "Xiaocheng Zhang", "Kele Xu", "Lan Zhang"], "title": "IntentVCNet: Bridging Spatio-Temporal Gaps for Intention-Oriented Controllable Video Captioning", "comment": null, "summary": "Intent-oriented controlled video captioning aims to generate targeted\ndescriptions for specific targets in a video based on customized user intent.\nCurrent Large Visual Language Models (LVLMs) have gained strong instruction\nfollowing and visual comprehension capabilities. Although the LVLMs\ndemonstrated proficiency in spatial and temporal understanding respectively, it\nwas not able to perform fine-grained spatial control in time sequences in\ndirect response to instructions. This substantial spatio-temporal gap\ncomplicates efforts to achieve fine-grained intention-oriented control in\nvideo. Towards this end, we propose a novel IntentVCNet that unifies the\ntemporal and spatial understanding knowledge inherent in LVLMs to bridge the\nspatio-temporal gap from both prompting and model perspectives. Specifically,\nwe first propose a prompt combination strategy designed to enable LLM to model\nthe implicit relationship between prompts that characterize user intent and\nvideo sequences. We then propose a parameter efficient box adapter that\naugments the object semantic information in the global visual context so that\nthe visual token has a priori information about the user intent. The final\nexperiment proves that the combination of the two strategies can further\nenhance the LVLM's ability to model spatial details in video sequences, and\nfacilitate the LVLMs to accurately generate controlled intent-oriented\ncaptions. Our proposed method achieved state-of-the-art results in several open\nsource LVLMs and was the runner-up in the IntentVC challenge. Our code is\navailable on https://github.com/thqiu0419/IntentVCNet.", "AI": {"tldr": "本文提出了一种名为IntentVCNet的新方法，旨在弥合大型视觉语言模型（LVLMs）在视频中进行意图导向的精细时空控制方面的不足，通过提示组合策略和高效边界框适配器实现。", "motivation": "尽管LVLMs在空间和时间理解方面表现出色，但它们无法直接响应指令对时间序列进行细粒度的空间控制，这种显著的时空差距阻碍了视频中实现精细的意图导向控制。", "method": "本文提出了IntentVCNet，从提示和模型两个角度统一了LVLMs固有的时空理解知识。具体而言，首先提出了一种提示组合策略，使LVLM能够建模用户意图提示和视频序列之间的隐含关系；其次，提出了一种参数高效的边界框适配器，增强了全局视觉上下文中的对象语义信息，使视觉token具有用户意图的先验信息。", "result": "实验证明，这两种策略的结合可以进一步增强LVLM在视频序列中建模空间细节的能力，并促进LVLM准确生成受控的意图导向字幕。所提出的方法在多个开源LVLM上取得了最先进的结果，并在IntentVC挑战中获得亚军。", "conclusion": "通过结合提示组合策略和参数高效的边界框适配器，IntentVCNet成功弥合了LVLMs在视频中进行意图导向精细时空控制的差距，显著提升了生成目标描述的能力。"}}
{"id": "2507.18532", "categories": ["cs.CV", "I.4.9; I.5.4; H.2.8"], "pdf": "https://arxiv.org/pdf/2507.18532", "abs": "https://arxiv.org/abs/2507.18532", "authors": ["Akbar Ali", "Mahek Vyas", "Soumyaratna Debnath", "Chanda Grover Kamra", "Jaidev Sanjay Khalane", "Reuben Shibu Devanesan", "Indra Deep Mastan", "Subramanian Sankaranarayanan", "Pankaj Khanna", "Shanmuganathan Raman"], "title": "COT-AD: Cotton Analysis Dataset", "comment": "Dataset publicly available at:\n  https://ieee-dataport.org/documents/cot-adcotton-analysis-dataset. Accepted\n  to IEEE International Conference on Image Processing (ICIP) 2025", "summary": "This paper presents COT-AD, a comprehensive Dataset designed to enhance\ncotton crop analysis through computer vision. Comprising over 25,000 images\ncaptured throughout the cotton growth cycle, with 5,000 annotated images,\nCOT-AD includes aerial imagery for field-scale detection and segmentation and\nhigh-resolution DSLR images documenting key diseases. The annotations cover\npest and disease recognition, vegetation, and weed analysis, addressing a\ncritical gap in cotton-specific agricultural datasets. COT-AD supports tasks\nsuch as classification, segmentation, image restoration, enhancement, deep\ngenerative model-based cotton crop synthesis, and early disease management,\nadvancing data-driven crop management", "AI": {"tldr": "本文提出了COT-AD，一个用于棉花作物计算机视觉分析的综合数据集，旨在弥补现有棉花农业数据集的不足。", "motivation": "现有棉花作物领域缺乏全面且针对性强的农业数据集，阻碍了计算机视觉技术在棉花生长周期分析中的应用。", "method": "构建了COT-AD数据集，包含超过25,000张图片（其中5,000张已标注），涵盖棉花整个生长周期。数据集包括用于田间尺度检测和分割的航拍图像，以及用于记录关键病害的高分辨率单反相机图像。标注内容覆盖病虫害识别、植被和杂草分析。", "result": "COT-AD数据集支持多种计算机视觉任务，如分类、分割、图像恢复、增强、基于深度生成模型的棉花作物合成，以及早期病害管理。", "conclusion": "COT-AD数据集的发布填补了棉花特定农业数据集的空白，有望推动数据驱动的棉花作物管理技术发展。"}}
{"id": "2507.18534", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18534", "abs": "https://arxiv.org/abs/2507.18534", "authors": ["Xingyu Qiu", "Mengying Yang", "Xinghua Ma", "Dong Liang", "Yuzhen Li", "Fanding Li", "Gongning Luo", "Wei Wang", "Kuanquan Wang", "Shuo Li"], "title": "Elucidating the Design Space of Arbitrary-Noise-Based Diffusion Models", "comment": "21 pages, 4 figures", "summary": "EDM elucidates the unified design space of diffusion models, yet its fixed\nnoise patterns restricted to pure Gaussian noise, limit advancements in image\nrestoration. Our study indicates that forcibly injecting Gaussian noise\ncorrupts the degraded images, overextends the image transformation distance,\nand increases restoration complexity. To address this problem, our proposed EDA\nElucidates the Design space of Arbitrary-noise-based diffusion models.\nTheoretically, EDA expands the freedom of noise pattern while preserving the\noriginal module flexibility of EDM, with rigorous proof that increased noise\ncomplexity incurs no additional computational overhead during restoration. EDA\nis validated on three typical tasks: MRI bias field correction (global smooth\nnoise), CT metal artifact reduction (global sharp noise), and natural image\nshadow removal (local boundary-aware noise). With only 5 sampling steps, EDA\noutperforms most task-specific methods and achieves state-of-the-art\nperformance in bias field correction and shadow removal.", "AI": {"tldr": "EDA扩展了EDM的扩散模型设计空间，允许使用任意噪声模式进行图像修复，解决了EDM固定高斯噪声的局限性，并在多种修复任务中以极少采样步数实现了SOTA性能。", "motivation": "EDM（扩散模型）在图像修复中受限于其固定的纯高斯噪声模式，这会导致对退化图像的额外破坏，增加图像转换距离和修复复杂性。现有EDM的这一限制阻碍了其在图像修复领域的进一步发展。", "method": "本文提出了EDA（任意噪声基扩散模型的设计空间阐明），它在理论上扩展了噪声模式的自由度，同时保留了EDM原有的模块灵活性。通过严格证明，增加噪声复杂度不会在修复过程中产生额外的计算开销。", "result": "EDA在三种典型任务上进行了验证：MRI偏置场校正（全局平滑噪声）、CT金属伪影消除（全局尖锐噪声）和自然图像阴影去除（局部边界感知噪声）。结果显示，EDA仅需5个采样步骤就能超越大多数特定任务方法，并在偏置场校正和阴影去除任务中达到了最先进的性能。", "conclusion": "EDA成功地通过引入任意噪声模式克服了EDM在图像修复中的限制，显著提升了修复效果和效率，证明了其在处理各种复杂图像退化问题上的优越性和通用性。"}}
{"id": "2507.18537", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18537", "abs": "https://arxiv.org/abs/2507.18537", "authors": ["Zhekai Chen", "Ruihang Chu", "Yukang Chen", "Shiwei Zhang", "Yujie Wei", "Yingya Zhang", "Xihui Liu"], "title": "TTS-VAR: A Test-Time Scaling Framework for Visual Auto-Regressive Generation", "comment": "10 Tables, 9 Figures", "summary": "Scaling visual generation models is essential for real-world content\ncreation, yet requires substantial training and computational expenses.\nAlternatively, test-time scaling has garnered growing attention due to resource\nefficiency and promising performance. In this work, we present TTS-VAR, the\nfirst general test-time scaling framework for visual auto-regressive (VAR)\nmodels, modeling the generation process as a path searching problem. To\ndynamically balance computational efficiency with exploration capacity, we\nfirst introduce an adaptive descending batch size schedule throughout the\ncausal generation process. Besides, inspired by VAR's hierarchical\ncoarse-to-fine multi-scale generation, our framework integrates two key\ncomponents: (i) At coarse scales, we observe that generated tokens are hard for\nevaluation, possibly leading to erroneous acceptance of inferior samples or\nrejection of superior samples. Noticing that the coarse scales contain\nsufficient structural information, we propose clustering-based diversity\nsearch. It preserves structural variety through semantic feature clustering,\nenabling later selection on samples with higher potential. (ii) In fine scales,\nresampling-based potential selection prioritizes promising candidates using\npotential scores, which are defined as reward functions incorporating\nmulti-scale generation history. Experiments on the powerful VAR model Infinity\nshow a notable 8.7% GenEval score improvement (from 0.69 to 0.75). Key insights\nreveal that early-stage structural features effectively influence final\nquality, and resampling efficacy varies across generation scales. Code is\navailable at https://github.com/ali-vilab/TTS-VAR.", "AI": {"tldr": "TTS-VAR是一种针对视觉自回归（VAR）模型的通用测试时缩放框架，将生成过程建模为路径搜索问题，通过自适应批处理大小、粗粒度多样性搜索和细粒度潜力选择来提高生成质量和效率。", "motivation": "大规模视觉生成模型需要巨大的训练和计算开销，而测试时缩放能够有效利用资源并展现出有前景的性能，因此需要开发一种通用的测试时缩放框架。", "method": "该方法将生成过程视为路径搜索问题。它引入了自适应递减批处理大小调度来平衡计算效率和探索能力。在粗粒度尺度上，提出基于聚类的多样性搜索，通过语义特征聚类保留结构多样性。在细粒度尺度上，采用基于重采样的潜力选择，利用结合多尺度生成历史的奖励函数定义潜力分数来优先选择有前景的候选。", "result": "在强大的VAR模型Infinity上，GenEval得分显著提高了8.7%（从0.69到0.75）。关键发现表明，早期阶段的结构特征能有效影响最终质量，并且重采样的效果因生成尺度的不同而异。", "conclusion": "TTS-VAR是首个针对VAR模型的通用测试时缩放框架，通过创新的路径搜索方法和多尺度策略，显著提升了视觉生成模型的质量和效率，并强调了早期结构信息的重要性以及重采样在不同生成尺度上的差异性影响。"}}
{"id": "2507.18541", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18541", "abs": "https://arxiv.org/abs/2507.18541", "authors": ["Chong Cheng", "Zijian Wang", "Sicheng Yu", "Yu Hu", "Nanjie Yao", "Hao Wang"], "title": "Unposed 3DGS Reconstruction with Probabilistic Procrustes Mapping", "comment": null, "summary": "3D Gaussian Splatting (3DGS) has emerged as a core technique for 3D\nrepresentation. Its effectiveness largely depends on precise camera poses and\naccurate point cloud initialization, which are often derived from pretrained\nMulti-View Stereo (MVS) models. However, in unposed reconstruction task from\nhundreds of outdoor images, existing MVS models may struggle with memory limits\nand lose accuracy as the number of input images grows. To address this\nlimitation, we propose a novel unposed 3DGS reconstruction framework that\nintegrates pretrained MVS priors with the probabilistic Procrustes mapping\nstrategy. The method partitions input images into subsets, maps submaps into a\nglobal space, and jointly optimizes geometry and poses with 3DGS. Technically,\nwe formulate the mapping of tens of millions of point clouds as a probabilistic\nProcrustes problem and solve a closed-form alignment. By employing\nprobabilistic coupling along with a soft dustbin mechanism to reject uncertain\ncorrespondences, our method globally aligns point clouds and poses within\nminutes across hundreds of images. Moreover, we propose a joint optimization\nframework for 3DGS and camera poses. It constructs Gaussians from\nconfidence-aware anchor points and integrates 3DGS differentiable rendering\nwith an analytical Jacobian to jointly refine scene and poses, enabling\naccurate reconstruction and pose estimation. Experiments on Waymo and KITTI\ndatasets show that our method achieves accurate reconstruction from unposed\nimage sequences, setting a new state of the art for unposed 3DGS\nreconstruction.", "AI": {"tldr": "针对3DGS在无姿态重建中MVS模型面对大量图像时的内存和精度限制，本文提出一种新的无姿态3DGS重建框架，通过概率Procrustes映射和3DGS与相机姿态的联合优化，实现了高效准确的场景重建和姿态估计。", "motivation": "3D Gaussian Splatting (3DGS) 的效果高度依赖精确的相机姿态和准确的点云初始化，而这些通常由预训练的多视图立体（MVS）模型提供。然而，在处理数百张户外图像的无姿态重建任务中，现有MVS模型面临内存限制，且随着输入图像数量的增加，精度会下降。", "method": "本方法提出一个无姿态3DGS重建框架，结合了预训练MVS先验和概率Procrustes映射策略。它将输入图像划分为子集，将子图映射到全局空间，并联合优化几何和姿态。具体地，将数千万点云的映射公式化为概率Procrustes问题并求解闭合形式对齐，同时采用概率耦合和软垃圾箱机制拒绝不确定对应点。此外，提出了一个3DGS和相机姿态的联合优化框架，从置信度感知的锚点构建高斯，并将3DGS可微分渲染与解析雅可比矩阵结合，共同优化场景和姿态。", "result": "该方法能够在数分钟内对数百张图像进行点云和姿态的全局对齐。实验结果表明，在Waymo和KITTI数据集上，本方法实现了从无姿态图像序列的准确重建，并在无姿态3DGS重建领域达到了新的SOTA水平。", "conclusion": "本文提出的方法成功克服了传统MVS模型在处理大量无姿态图像时遇到的内存和精度挑战，为无姿态3DGS重建提供了一种高效、准确的解决方案，实现了场景的精确重建和姿态估计。"}}
{"id": "2507.18551", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18551", "abs": "https://arxiv.org/abs/2507.18551", "authors": ["Daniil Morozov", "Reuben Dorent", "Nazim Haouchine"], "title": "A 3D Cross-modal Keypoint Descriptor for MR-US Matching and Registration", "comment": "Under review", "summary": "Intraoperative registration of real-time ultrasound (iUS) to preoperative\nMagnetic Resonance Imaging (MRI) remains an unsolved problem due to severe\nmodality-specific differences in appearance, resolution, and field-of-view. To\naddress this, we propose a novel 3D cross-modal keypoint descriptor for MRI-iUS\nmatching and registration. Our approach employs a patient-specific\nmatching-by-synthesis approach, generating synthetic iUS volumes from\npreoperative MRI. This enables supervised contrastive training to learn a\nshared descriptor space.\n  A probabilistic keypoint detection strategy is then employed to identify\nanatomically salient and modality-consistent locations. During training, a\ncurriculum-based triplet loss with dynamic hard negative mining is used to\nlearn descriptors that are i) robust to iUS artifacts such as speckle noise and\nlimited coverage, and ii) rotation-invariant . At inference, the method detects\nkeypoints in MR and real iUS images and identifies sparse matches, which are\nthen used to perform rigid registration. Our approach is evaluated using 3D\nMRI-iUS pairs from the ReMIND dataset. Experiments show that our approach\noutperforms state-of-the-art keypoint matching methods across 11 patients, with\nan average precision of $69.8\\%$. For image registration, our method achieves a\ncompetitive mean Target Registration Error of 2.39 mm on the ReMIND2Reg\nbenchmark.\n  Compared to existing iUS-MR registration approach, our framework is\ninterpretable, requires no manual initialization, and shows robustness to iUS\nfield-of-view variation. Code is available at\nhttps://github.com/morozovdd/CrossKEY.", "AI": {"tldr": "该研究提出了一种新的3D跨模态关键点描述符，用于解决术中超声（iUS）与术前磁共振成像（MRI）的配准难题，通过合成数据和对比学习实现鲁棒匹配和高精度配准。", "motivation": "由于实时超声（iUS）与磁共振成像（MRI）在外观、分辨率和视野上存在严重的模态特异性差异，术中iUS到术前MRI的配准仍然是一个未解决的问题。", "method": "该方法采用患者特异性的“合成匹配”方法，从术前MRI生成合成iUS图像，从而实现有监督的对比训练，学习共享的描述符空间。使用概率关键点检测策略识别解剖学上显著且模态一致的位置。训练中采用基于课程的、带有动态难负例挖掘的三元组损失，以学习对iUS伪影和有限覆盖范围具有鲁棒性、且旋转不变的描述符。推断时，检测MR和真实iUS图像中的关键点，识别稀疏匹配，并用于刚性配准。", "result": "该方法在ReMIND数据集的11名患者中，关键点匹配的平均精度达到69.8%，优于现有最先进的关键点匹配方法。在ReMIND2Reg基准测试中，图像配准的平均目标配准误差（TRE）为2.39毫米，具有竞争力。", "conclusion": "与现有iUS-MR配准方法相比，该框架具有可解释性，无需手动初始化，并且对iUS视野变化表现出鲁棒性。"}}
{"id": "2507.18565", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18565", "abs": "https://arxiv.org/abs/2507.18565", "authors": ["Muhammad Imran Zaman", "Nisar Ahmed"], "title": "Deep Learning-Based Age Estimation and Gender Deep Learning-Based Age Estimation and Gender Classification for Targeted Advertisement", "comment": "6", "summary": "This paper presents a novel deep learning-based approach for simultaneous age\nand gender classification from facial images, designed to enhance the\neffectiveness of targeted advertising campaigns. We propose a custom\nConvolutional Neural Network (CNN) architecture, optimized for both tasks,\nwhich leverages the inherent correlation between age and gender information\npresent in facial features. Unlike existing methods that often treat these\ntasks independently, our model learns shared representations, leading to\nimproved performance. The network is trained on a large, diverse dataset of\nfacial images, carefully pre-processed to ensure robustness against variations\nin lighting, pose, and image quality. Our experimental results demonstrate a\nsignificant improvement in gender classification accuracy, achieving 95%, and a\ncompetitive mean absolute error of 5.77 years for age estimation. Critically,\nwe analyze the performance across different age groups, identifying specific\nchallenges in accurately estimating the age of younger individuals. This\nanalysis reveals the need for targeted data augmentation and model refinement\nto address these biases. Furthermore, we explore the impact of different CNN\narchitectures and hyperparameter settings on the overall performance, providing\nvaluable insights for future research.", "AI": {"tldr": "本文提出了一种基于深度学习的新型卷积神经网络（CNN）方法，用于同时从面部图像中进行年龄和性别分类，旨在提高定向广告的有效性。", "motivation": "现有方法通常独立处理年龄和性别分类任务，未能利用面部特征中固有的年龄和性别信息关联。本研究旨在通过学习共享表示来提高分类性能，并应用于增强定向广告活动。", "method": "提出了一种定制的卷积神经网络（CNN）架构，该架构针对年龄和性别分类任务进行了优化，并利用了面部特征中年龄和性别信息之间的内在相关性。模型在一个大型、多样化的面部图像数据集上进行训练，该数据集经过精心预处理，以确保对光照、姿态和图像质量变化的鲁棒性。", "result": "实验结果显示，性别分类准确率显著提高至95%，年龄估计的平均绝对误差（MAE）为5.77年，具有竞争力。分析发现，在估计年轻个体年龄时存在特定挑战，揭示了处理这些偏差需要有针对性的数据增强和模型优化。", "conclusion": "该方法在性别分类和年龄估计方面表现出色，特别是在性别分类上取得了显著进步。研究揭示了在年轻个体年龄估计中的挑战，并强调了未来通过有针对性的数据增强和模型优化来解决这些偏差的必要性。同时，对不同CNN架构和超参数设置的探索为未来的研究提供了宝贵见解。"}}
{"id": "2507.18566", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18566", "abs": "https://arxiv.org/abs/2507.18566", "authors": ["Nitish Shukla", "Arun Ross"], "title": "Facial Demorphing from a Single Morph Using a Latent Conditional GAN", "comment": null, "summary": "A morph is created by combining two (or more) face images from two (or more)\nidentities to create a composite image that is highly similar to both\nconstituent identities, allowing the forged morph to be biometrically\nassociated with more than one individual. Morph Attack Detection (MAD) can be\nused to detect a morph, but does not reveal the constituent images. Demorphing\n- the process of deducing the constituent images - is thus vital to provide\nadditional evidence about a morph. Existing demorphing methods suffer from the\nmorph replication problem, where the outputs tend to look very similar to the\nmorph itself, or assume that train and test morphs are generated using the same\nmorph technique. The proposed method overcomes these issues. The method\ndecomposes a morph in latent space allowing it to demorph images created from\nunseen morph techniques and face styles. We train our method on morphs created\nfrom synthetic faces and test on morphs created from real faces using arbitrary\nmorph techniques. Our method outperforms existing methods by a considerable\nmargin and produces high fidelity demorphed face images.", "AI": {"tldr": "该论文提出了一种新的去变形（demorphing）方法，通过在潜在空间分解变形图像，以克服现有方法的局限性（如变形复制问题和对变形技术依赖），并能从合成和真实面部变形中恢复高保真度的原始图像。", "motivation": "面部变形攻击允许一张图像与多个身份关联，构成安全威胁。虽然变形攻击检测（MAD）可以识别变形，但无法揭示原始组成图像。去变形对于提供额外证据至关重要，但现有方法存在“变形复制问题”（输出与变形本身过于相似）或假设训练和测试变形使用相同的生成技术。", "method": "所提出的方法在潜在空间中分解变形图像，使其能够处理来自未知变形技术和面部风格的图像。该方法使用合成面部生成的变形进行训练，并使用真实面部通过任意变形技术生成的变形进行测试。", "result": "该方法显著优于现有方法，并能生成高保真度的去变形面部图像。", "conclusion": "所提出的去变形方法有效克服了现有方法的缺陷，能从使用未知变形技术和面部风格创建的图像中恢复原始组成图像，并取得了显著优异的性能。"}}
{"id": "2507.18569", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18569", "abs": "https://arxiv.org/abs/2507.18569", "authors": ["Yanzuo Lu", "Yuxi Ren", "Xin Xia", "Shanchuan Lin", "Xing Wang", "Xuefeng Xiao", "Andy J. Ma", "Xiaohua Xie", "Jian-Huang Lai"], "title": "Adversarial Distribution Matching for Diffusion Distillation Towards Efficient Image and Video Synthesis", "comment": "Accepted by ICCV 2025 (Highlight)", "summary": "Distribution Matching Distillation (DMD) is a promising score distillation\ntechnique that compresses pre-trained teacher diffusion models into efficient\none-step or multi-step student generators. Nevertheless, its reliance on the\nreverse Kullback-Leibler (KL) divergence minimization potentially induces mode\ncollapse (or mode-seeking) in certain applications. To circumvent this inherent\ndrawback, we propose Adversarial Distribution Matching (ADM), a novel framework\nthat leverages diffusion-based discriminators to align the latent predictions\nbetween real and fake score estimators for score distillation in an adversarial\nmanner. In the context of extremely challenging one-step distillation, we\nfurther improve the pre-trained generator by adversarial distillation with\nhybrid discriminators in both latent and pixel spaces. Different from the mean\nsquared error used in DMD2 pre-training, our method incorporates the\ndistributional loss on ODE pairs collected from the teacher model, and thus\nproviding a better initialization for score distillation fine-tuning in the\nnext stage. By combining the adversarial distillation pre-training with ADM\nfine-tuning into a unified pipeline termed DMDX, our proposed method achieves\nsuperior one-step performance on SDXL compared to DMD2 while consuming less GPU\ntime. Additional experiments that apply multi-step ADM distillation on\nSD3-Medium, SD3.5-Large, and CogVideoX set a new benchmark towards efficient\nimage and video synthesis.", "AI": {"tldr": "该论文提出了一种名为对抗性分布匹配（ADM）的新框架，并通过DMDX统一管道解决了现有分布匹配蒸馏（DMD）方法中潜在的模式崩溃问题，显著提升了单步和多步扩散模型的生成效率和性能。", "motivation": "现有的分布匹配蒸馏（DMD）技术依赖于反向KL散度最小化，可能导致在某些应用中出现模式崩溃（或模式寻求）问题。因此，需要一种新的方法来克服这一固有缺陷，实现更鲁棒和高效的模型压缩。", "method": "提出了对抗性分布匹配（ADM）框架，利用基于扩散的判别器以对抗方式对齐真实和伪分数估计器之间的潜在预测。在极具挑战性的单步蒸馏中，通过在潜在空间和像素空间使用混合判别器进一步改进生成器。与DMD2预训练中使用的均方误差不同，该方法在预训练中整合了从教师模型收集的ODE对的分布损失，为后续的分数蒸馏微调提供了更好的初始化。最终将对抗性蒸馏预训练与ADM微调结合成一个统一的管道，命名为DMDX。", "result": "DMDX在SDXL上的单步性能优于DMD2，同时消耗更少的GPU时间。此外，将多步ADM蒸馏应用于SD3-Medium、SD3.5-Large和CogVideoX，为高效图像和视频合成树立了新的基准。", "conclusion": "所提出的ADM和DMDX方法有效解决了DMD的模式崩溃问题，通过对抗性学习和改进的预训练策略，在单步和多步扩散模型蒸馏方面取得了卓越的性能和效率，为图像和视频合成领域带来了显著进步。"}}
{"id": "2507.18575", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18575", "abs": "https://arxiv.org/abs/2507.18575", "authors": ["Xinyu Wang", "Jinghua Hou", "Zhe Liu", "Yingying Zhu"], "title": "HybridTM: Combining Transformer and Mamba for 3D Semantic Segmentation", "comment": "7 pages, 5 figures", "summary": "Transformer-based methods have demonstrated remarkable capabilities in 3D\nsemantic segmentation through their powerful attention mechanisms, but the\nquadratic complexity limits their modeling of long-range dependencies in\nlarge-scale point clouds. While recent Mamba-based approaches offer efficient\nprocessing with linear complexity, they struggle with feature representation\nwhen extracting 3D features. However, effectively combining these complementary\nstrengths remains an open challenge in this field. In this paper, we propose\nHybridTM, the first hybrid architecture that integrates Transformer and Mamba\nfor 3D semantic segmentation. In addition, we propose the Inner Layer Hybrid\nStrategy, which combines attention and Mamba at a finer granularity, enabling\nsimultaneous capture of long-range dependencies and fine-grained local\nfeatures. Extensive experiments demonstrate the effectiveness and\ngeneralization of our HybridTM on diverse indoor and outdoor datasets.\nFurthermore, our HybridTM achieves state-of-the-art performance on ScanNet,\nScanNet200, and nuScenes benchmarks. The code will be made available at\nhttps://github.com/deepinact/HybridTM.", "AI": {"tldr": "HybridTM是首个结合Transformer和Mamba的混合架构，用于3D语义分割，通过内层混合策略有效捕捉长距离依赖和局部特征，并在多个基准测试中达到SOTA性能。", "motivation": "Transformer在3D语义分割中表现出色但存在二次复杂度，限制了对大规模点云长距离依赖的建模。Mamba虽然效率高但特征表示能力不足。如何有效结合两者的优势是一个开放挑战。", "method": "提出HybridTM，首个集成Transformer和Mamba的混合架构。引入“内层混合策略”（Inner Layer Hybrid Strategy），在更细粒度上结合注意力机制和Mamba，以同时捕获长距离依赖和细粒度局部特征。", "result": "HybridTM在多种室内外数据集上表现出有效性和泛化性。在ScanNet、ScanNet200和nuScenes基准测试中取得了最先进的性能。", "conclusion": "HybridTM成功地结合了Transformer和Mamba的互补优势，解决了现有方法的局限性，为3D语义分割设定了新的性能标准。"}}
{"id": "2507.18633", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18633", "abs": "https://arxiv.org/abs/2507.18633", "authors": ["Grace Su", "Sheng-Yu Wang", "Aaron Hertzmann", "Eli Shechtman", "Jun-Yan Zhu", "Richard Zhang"], "title": "Identifying Prompted Artist Names from Generated Images", "comment": "Project page:\n  https://graceduansu.github.io/IdentifyingPromptedArtists", "summary": "A common and controversial use of text-to-image models is to generate\npictures by explicitly naming artists, such as \"in the style of Greg\nRutkowski\". We introduce a benchmark for prompted-artist recognition:\npredicting which artist names were invoked in the prompt from the image alone.\nThe dataset contains 1.95M images covering 110 artists and spans four\ngeneralization settings: held-out artists, increasing prompt complexity,\nmultiple-artist prompts, and different text-to-image models. We evaluate\nfeature similarity baselines, contrastive style descriptors, data attribution\nmethods, supervised classifiers, and few-shot prototypical networks.\nGeneralization patterns vary: supervised and few-shot models excel on seen\nartists and complex prompts, whereas style descriptors transfer better when the\nartist's style is pronounced; multi-artist prompts remain the most challenging.\nOur benchmark reveals substantial headroom and provides a public testbed to\nadvance the responsible moderation of text-to-image models. We release the\ndataset and benchmark to foster further research:\nhttps://graceduansu.github.io/IdentifyingPromptedArtists/", "AI": {"tldr": "该研究引入了一个基准数据集和评估框架，用于识别文本到图像模型中通过提示词引用的艺术家，以促进负责任的模型审查。", "motivation": "文本到图像模型通过明确命名艺术家来生成图片是一种常见且有争议的使用方式。为了负责任地管理这些模型，需要一种方法来识别图像中引用的艺术家。", "method": "研究构建了一个包含195万张图像的基准数据集，涵盖110位艺术家和四种泛化设置（未见艺术家、复杂提示、多艺术家提示、不同T2I模型）。评估了多种方法，包括特征相似性基线、对比风格描述符、数据归因方法、监督分类器和少样本原型网络。", "result": "泛化模式各异：监督模型和少样本模型在已知艺术家和复杂提示上表现出色；风格描述符在艺术家风格鲜明时迁移效果更好；多艺术家提示仍然是最具挑战性的。基准测试显示仍有很大改进空间。", "conclusion": "该基准测试揭示了识别提示艺术家任务的巨大潜力，并提供了一个公开的测试平台，以推动文本到图像模型负责任的审查和未来研究。"}}
{"id": "2507.18634", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18634", "abs": "https://arxiv.org/abs/2507.18634", "authors": ["Junfei Xiao", "Ceyuan Yang", "Lvmin Zhang", "Shengqu Cai", "Yang Zhao", "Yuwei Guo", "Gordon Wetzstein", "Maneesh Agrawala", "Alan Yuille", "Lu Jiang"], "title": "Captain Cinema: Towards Short Movie Generation", "comment": "Under review. Project page: https://thecinema.ai", "summary": "We present Captain Cinema, a generation framework for short movie generation.\nGiven a detailed textual description of a movie storyline, our approach firstly\ngenerates a sequence of keyframes that outline the entire narrative, which\nensures long-range coherence in both the storyline and visual appearance (e.g.,\nscenes and characters). We refer to this step as top-down keyframe planning.\nThese keyframes then serve as conditioning signals for a video synthesis model,\nwhich supports long context learning, to produce the spatio-temporal dynamics\nbetween them. This step is referred to as bottom-up video synthesis. To support\nstable and efficient generation of multi-scene long narrative cinematic works,\nwe introduce an interleaved training strategy for Multimodal Diffusion\nTransformers (MM-DiT), specifically adapted for long-context video data. Our\nmodel is trained on a specially curated cinematic dataset consisting of\ninterleaved data pairs. Our experiments demonstrate that Captain Cinema\nperforms favorably in the automated creation of visually coherent and narrative\nconsistent short movies in high quality and efficiency. Project page:\nhttps://thecinema.ai", "AI": {"tldr": "Captain Cinema是一个短片电影生成框架，能根据文本描述生成视觉连贯、叙事一致的高质量电影。", "motivation": "自动化短片电影创作，确保长程叙事和视觉连贯性，并提高生成效率。", "method": ["顶层关键帧规划：根据电影故事情节文本描述，生成一系列关键帧，确保故事情节和视觉外观（如场景、角色）的长程连贯性。", "底层视频合成：利用关键帧作为条件信号，通过支持长上下文学习的视频合成模型，生成关键帧之间的时空动态。", "引入交错训练策略：为适应长上下文视频数据，对多模态扩散Transformer（MM-DiT）引入了交错训练策略，以支持多场景长叙事电影的稳定高效生成。", "数据集：模型在一个特别策划的、包含交错数据对的电影数据集上进行训练。"], "result": "Captain Cinema在自动化创建视觉连贯、叙事一致的高质量短片电影方面表现出色，且效率高。", "conclusion": "Captain Cinema框架通过其分层生成方法和针对长上下文视频数据优化的训练策略，成功实现了从文本描述到高质量短片电影的自动生成。"}}
